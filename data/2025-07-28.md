<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 36]
- [cs.CV](#cs.CV) [Total: 66]
- [cs.AI](#cs.AI) [Total: 15]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.IR](#cs.IR) [Total: 10]
- [cs.LG](#cs.LG) [Total: 52]
- [eess.IV](#eess.IV) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Specification Self-Correction: Mitigating In-Context Reward Hacking Through Test-Time Refinement](https://arxiv.org/abs/2507.18742)
*Víctor Gallego*

Main category: cs.CL

TL;DR: The paper introduces Specification Self-Correction (SSC) to fix flaws in specifications and reduce reward hacking in language models.


<details>
  <summary>Details</summary>
Motivation: Language models (LMs) are susceptible to in-context reward hacking, where they exploit flaws in tainted or faulty written specifications or rubrics to achieve high scores without fulfilling the user's true intent.

Method: Specification Self-Correction (SSC), a novel, test-time framework that enables an LM to identify and correct flaws within its own guiding specification. SSC employs a multi-step inference process where the model first generates a response based on a potentially tainted specification, critiques its output, and then revises the specification itself to remove the exploitable loophole.

Result: models initially game tainted specifications in 50-70% of cases, the SSC process reduces this vulnerability by over 90%.

Conclusion: Specification Self-Correction (SSC) reduces the vulnerability of language models (LMs) to reward hacking by over 90%.

Abstract: Language models (LMs) are susceptible to in-context reward hacking, where
they exploit flaws in tainted or faulty written specifications or rubrics to
achieve high scores without fulfilling the user's true intent. We introduce
Specification Self-Correction (SSC), a novel, test-time framework that enables
an LM to identify and correct flaws within its own guiding specification. SSC
employs a multi-step inference process where the model first generates a
response based on a potentially tainted specification, critiques its output,
and then revises the specification itself to remove the exploitable loophole. A
final, more robust response is then generated using this self-corrected
specification. Across experiments spanning creative writing and agentic coding
tasks with several LMs, we demonstrate that while models initially game tainted
specifications in 50-70\% of cases, the SSC process reduces this vulnerability
by over 90\%. This dynamic repair occurs at inference time, requires no weight
modification, and leads to more robustly aligned model behavior. Code at
https://github.com/vicgalle/specification-self-correction .

</details>


### [2] [The Role of Orthographic Consistency in Multilingual Embedding Models for Text Classification in Arabic-Script Languages](https://arxiv.org/abs/2507.18762)
*Abdulhady Abas Abdullah,Amir H. Gandomi,Tarik A Rashid,Seyedali Mirjalili,Laith Abualigah,Milena Živković,Hadi Veisi*

Main category: cs.CL

TL;DR: AS-RoBERTa models outperform general-purpose models by focusing pre-training on language-specific script features and statistics.


<details>
  <summary>Details</summary>
Motivation: multilingual models struggle with languages that share a script yet differ in orthographic norms and cultural context, especially in Arabic-script languages

Method: pre-trained four RoBERTa-based models on a large corpus tailored to its specific language

Result: AS-RoBERTa variants outperform mBERT and XLM-RoBERTa by 2 to 5 percentage points

Conclusion: script-aware specialization is valuable for languages using the Arabic script

Abstract: In natural language processing, multilingual models like mBERT and
XLM-RoBERTa promise broad coverage but often struggle with languages that share
a script yet differ in orthographic norms and cultural context. This issue is
especially notable in Arabic-script languages such as Kurdish Sorani, Arabic,
Persian, and Urdu. We introduce the Arabic Script RoBERTa (AS-RoBERTa) family:
four RoBERTa-based models, each pre-trained on a large corpus tailored to its
specific language. By focusing pre-training on language-specific script
features and statistics, our models capture patterns overlooked by
general-purpose models. When fine-tuned on classification tasks, AS-RoBERTa
variants outperform mBERT and XLM-RoBERTa by 2 to 5 percentage points. An
ablation study confirms that script-focused pre-training is central to these
gains. Error analysis using confusion matrices shows how shared script traits
and domain-specific content affect performance. Our results highlight the value
of script-aware specialization for languages using the Arabic script and
support further work on pre-training strategies rooted in script and language
specificity.

</details>


### [3] [ylmmcl at Multilingual Text Detoxification 2025: Lexicon-Guided Detoxification and Classifier-Gated Rewriting](https://arxiv.org/abs/2507.18769)
*Nicole Lai-Lopez,Lusha Wang,Su Yuan,Liza Zhang*

Main category: cs.CL

TL;DR: A multilingual text detoxification pipeline using lexicon-guided tagging, a fine-tuned sequence-to-sequence model, and an iterative classifier, achieving ninth place in the PAN-2025 competition.


<details>
  <summary>Details</summary>
Motivation: The approach departs from prior unsupervised or monolingual pipelines by leveraging explicit toxic word annotation to guide detoxification with greater precision and cross-lingual generalization.

Method: The solution integrates lexicon-guided tagging, a fine-tuned sequence-to-sequence model, and an iterative classifier-based gatekeeping mechanism.

Result: The final model achieves the highest STA (0.922) and an average official J score of 0.612 for toxic inputs. It also achieved xCOMET scores of 0.793 (dev) and 0.787 (test).

Conclusion: The team achieved ninth place in the competition with a score of 0.612, demonstrating strong generalization in high-resource settings.

Abstract: In this work, we introduce our solution for the Multilingual Text
Detoxification Task in the PAN-2025 competition for the ylmmcl team: a robust
multilingual text detoxification pipeline that integrates lexicon-guided
tagging, a fine-tuned sequence-to-sequence model (s-nlp/mt0-xl-detox-orpo) and
an iterative classifier-based gatekeeping mechanism. Our approach departs from
prior unsupervised or monolingual pipelines by leveraging explicit toxic word
annotation via the multilingual_toxic_lexicon to guide detoxification with
greater precision and cross-lingual generalization. Our final model achieves
the highest STA (0.922) from our previous attempts, and an average official J
score of 0.612 for toxic inputs in both the development and test sets. It also
achieved xCOMET scores of 0.793 (dev) and 0.787 (test). This performance
outperforms baseline and backtranslation methods across multiple languages, and
shows strong generalization in high-resource settings (English, Russian,
French). Despite some trade-offs in SIM, the model demonstrates consistent
improvements in detoxification strength. In the competition, our team achieved
ninth place with a score of 0.612.

</details>


### [4] [Evaluating Code-Mixing in LLMs Across 18 Languages](https://arxiv.org/abs/2507.18791)
*Yilun Yang,Yekun Chai*

Main category: cs.CL

TL;DR: This paper evaluates LLMs on code-mixed data, finds underperformance, and suggests improvements in training data, model scale, and few-shot learning.


<details>
  <summary>Details</summary>
Motivation: Code-mixing presents unique challenges for traditional natural language processing, but existing benchmarks are limited, and research on LLMs in this context remains limited. Additionally, current methods for generating code-mixed data are underdeveloped.

Method: We propose a novel approach for generating synthetic code-mixed texts by combining word substitution with GPT-4 prompting.

Result: LLMs' performance on code-mixed data was evaluated across 18 languages from seven language families.

Conclusion: LLMs consistently underperform on code-mixed datasets involving multiple language families, suggesting that improvements in training data size, model scale, and few-shot learning could enhance their performance.

Abstract: Code-mixing, the practice of switching between languages within a
conversation, presents unique challenges for traditional natural language
processing. Existing benchmarks, such as LinCE and GLUECoS, are limited by
narrow language pairings and tasks, failing to adequately evaluate the
code-mixing capabilities of large language models (LLMs). Despite the
significance of code-mixing for multilingual users, research on LLMs in this
context remains limited. Additionally, current methods for generating
code-mixed data are underdeveloped. In this paper, we conduct a comprehensive
evaluation of LLMs' performance on code-mixed data across 18 languages from
seven language families. We also propose a novel approach for generating
synthetic code-mixed texts by combining word substitution with GPT-4 prompting.
Our analysis reveals consistent underperformance of LLMs on code-mixed datasets
involving multiple language families. We suggest that improvements in training
data size, model scale, and few-shot learning could enhance their performance.

</details>


### [5] [CueBuddy: helping non-native English speakers navigate English-centric STEM education](https://arxiv.org/abs/2507.18827)
*Pranav Gupta*

Main category: cs.CL

TL;DR: CueBuddy provides real-time lexical cues to help students in STEM classes understand complex English jargon.


<details>
  <summary>Details</summary>
Motivation: Students in STEM classes, especially in the Global South, fall behind their peers who are more fluent in English, despite being at par with them in terms of scientific prerequisites. Live speech translation can be too expensive and often struggles with technical content.

Method: The paper describes CueBuddy, which provides real-time lexical cues through technical keyword spotting along real-time multilingual glossary lookup.

Result: CueBuddy helps students stay up to speed with complex English jargon without disrupting their concentration on the lecture.

Conclusion: This paper describes the limitations and future extensions of the CueBuddy approach.

Abstract: Students across the world in STEM classes, especially in the Global South,
fall behind their peers who are more fluent in English, despite being at par
with them in terms of scientific prerequisites. While many of them are able to
follow everyday English at ease, key terms in English stay challenging. In most
cases, such students have had most of their course prerequisites in a lower
resource language. Live speech translation to lower resource languages is a
promising area of research, however, models for speech translation can be too
expensive on a large scale and often struggle with technical content. In this
paper, we describe CueBuddy, which aims to remediate these issues by providing
real-time "lexical cues" through technical keyword spotting along real-time
multilingual glossary lookup to help students stay up to speed with complex
English jargon without disrupting their concentration on the lecture. We also
describe the limitations and future extensions of our approach.

</details>


### [6] [PrismRAG: Boosting RAG Factuality with Distractor Resilience and Strategized Reasoning](https://arxiv.org/abs/2507.18857)
*Mohammad Kachuee,Teja Gollapudi,Minseok Kim,Yin Huang,Kai Sun,Xiao Yang,Jiaqi Wang,Nirav Shah,Yue Liu,Aaron Colak,Anuj Kumar,Wen-tau Yih,Xin Luna Dong*

Main category: cs.CL

TL;DR: PrismRAG, an efficient fine-tuning framework, improves factuality in retrieval-augmented generation by training with distractor-aware QA pairs and instilling reasoning-centric habits.


<details>
  <summary>Details</summary>
Motivation: Retrieval-augmented generation (RAG) often falls short when retrieved context includes confusing semi-relevant passages, or when answering questions require deep contextual understanding and reasoning.

Method: an efficient fine-tuning framework, called PrismRAG, that (i) trains the model with distractor-aware QA pairs mixing gold evidence with subtle distractor passages, and (ii) instills reasoning-centric habits that make the LLM plan, rationalize, and synthesize without relying on extensive human engineered instructions.

Result: PrismRAG improves average factuality by 5.4%, outperforming state-of-the-art solutions.

Conclusion: PrismRAG improves average factuality by 5.4%, outperforming state-of-the-art solutions.

Abstract: Retrieval-augmented generation (RAG) often falls short when retrieved context
includes confusing semi-relevant passages, or when answering questions require
deep contextual understanding and reasoning. We propose an efficient
fine-tuning framework, called PrismRAG, that (i) trains the model with
distractor-aware QA pairs mixing gold evidence with subtle distractor passages,
and (ii) instills reasoning-centric habits that make the LLM plan, rationalize,
and synthesize without relying on extensive human engineered instructions.
Evaluated across 12 open-book RAG QA benchmarks spanning diverse application
domains and scenarios, PrismRAG improves average factuality by 5.4%,
outperforming state-of-the-art solutions.

</details>


### [7] [MindFlow+: A Self-Evolving Agent for E-Commerce Customer Service](https://arxiv.org/abs/2507.18884)
*Ming Gong,Xucheng Huang,Ziheng Xu,Vijayan K. Asari*

Main category: cs.CL

TL;DR: MindFlow+是一种自进化对话代理，通过结合大型语言模型与模仿学习和离线强化学习来学习领域特定行为。


<details>
  <summary>Details</summary>
Motivation: 传统意图系统难以应对动态、多轮交互。

Method: 结合大型语言模型与模仿学习和离线强化学习。

Result: MindFlow+在上下文相关性、灵活性和任务准确性方面优于强大的基线模型。

Conclusion: MindFlow+结合大型语言模型、工具推理和奖励引导学习，有潜力构建领域专用、上下文感知的对话系统。

Abstract: High-quality dialogue is crucial for e-commerce customer service, yet
traditional intent-based systems struggle with dynamic, multi-turn
interactions. We present MindFlow+, a self-evolving dialogue agent that learns
domain-specific behavior by combining large language models (LLMs) with
imitation learning and offline reinforcement learning (RL). MindFlow+
introduces two data-centric mechanisms to guide learning: tool-augmented
demonstration construction, which exposes the model to knowledge-enhanced and
agentic (ReAct-style) interactions for effective tool use; and
reward-conditioned data modeling, which aligns responses with task-specific
goals using reward signals. To evaluate the model's role in response
generation, we introduce the AI Contribution Ratio, a novel metric quantifying
AI involvement in dialogue. Experiments on real-world e-commerce conversations
show that MindFlow+ outperforms strong baselines in contextual relevance,
flexibility, and task accuracy. These results demonstrate the potential of
combining LLMs tool reasoning, and reward-guided learning to build
domain-specialized, context-aware dialogue systems.

</details>


### [8] [NUTMEG: Separating Signal From Noise in Annotator Disagreement](https://arxiv.org/abs/2507.18890)
*Jonathan Ivey,Susan Gauch,David Jurgens*

Main category: cs.CL

TL;DR: 提出了一种新的贝叶斯模型，可以区分注释者分歧中的信号和噪声，从而提高下游模型的性能。


<details>
  <summary>Details</summary>
Motivation: NLP模型通常依赖于人工标记的数据进行训练和评估。许多方法从大量具有不同技能、背景和动机的注释者那里众包这些数据，从而导致冲突的注释。传统上，这些冲突是通过假设分歧是错误的聚合方法来解决的。最近的研究表明，对于许多任务，注释者可能存在真正的分歧，并且这种变化应被视为信号而不是噪声。然而，很少有模型能够区分注释者分歧中的信号和噪声。

Method: 引入了一种新的贝叶斯模型NUTMEG，该模型结合了注释者背景信息，以从人工标记的训练数据中删除嘈杂的注释，同时保留系统性分歧。

Result: 使用合成数据，表明NUTMEG在从具有系统性分歧的注释中恢复ground-truth方面比传统聚合方法更有效。进一步分析描述了子群体大小、分歧率和垃圾信息率的差异如何影响模型的性能。

Conclusion: 下游模型在经过NUTMEG聚合的数据上训练后，明显优于在传统聚合方法的数据上训练的模型。结果突出了在人工标记数据上训练时，考虑注释者能力和系统性分歧的重要性。

Abstract: NLP models often rely on human-labeled data for training and evaluation. Many
approaches crowdsource this data from a large number of annotators with varying
skills, backgrounds, and motivations, resulting in conflicting annotations.
These conflicts have traditionally been resolved by aggregation methods that
assume disagreements are errors. Recent work has argued that for many tasks
annotators may have genuine disagreements and that variation should be treated
as signal rather than noise. However, few models separate signal and noise in
annotator disagreement. In this work, we introduce NUTMEG, a new Bayesian model
that incorporates information about annotator backgrounds to remove noisy
annotations from human-labeled training data while preserving systematic
disagreements. Using synthetic data, we show that NUTMEG is more effective at
recovering ground-truth from annotations with systematic disagreement than
traditional aggregation methods. We provide further analysis characterizing how
differences in subpopulation sizes, rates of disagreement, and rates of spam
affect the performance of our model. Finally, we demonstrate that downstream
models trained on NUTMEG-aggregated data significantly outperform models
trained on data from traditionally aggregation methods. Our results highlight
the importance of accounting for both annotator competence and systematic
disagreements when training on human-labeled data.

</details>


### [9] [REPRO-Bench: Can Agentic AI Systems Assess the Reproducibility of Social Science Research?](https://arxiv.org/abs/2507.18901)
*Chuxuan Hu,Liyun Zhang,Yeji Lim,Aum Wadhwani,Austin Peters,Daniel Kang*

Main category: cs.CL

TL;DR: 本文介绍了一个用于评估社会科学论文可重复性的新基准 REPRO-Bench，并表明现有的 AI 代理在自动化此过程方面的能力有限，但可以通过 REPRO-Agent 得到显著改善。


<details>
  <summary>Details</summary>
Motivation: 评估社会科学论文的可重复性对于提高研究过程的严谨性至关重要，但人工评估成本高昂。随着 Agentic AI 系统的最新进展，我们试图评估他们自动化这一过程的能力。

Method: 引入 REPRO-Bench，一个包含 112 个任务实例的集合，每个实例代表一篇带有公开可用的复制报告的社会科学论文。

Result: 在 REPRO-Bench 上评估了三个有代表性的 AI 代理，性能最佳的代理的准确率仅为 21.4%。 在我们的实证分析的基础上，我们开发了 REPRO-Agent，它将现有代理实现的最高准确率提高了 71%。

Conclusion: 需要开发更先进的 AI 代理来自动执行实际的可重复性评估。

Abstract: Assessing the reproducibility of social science papers is essential for
promoting rigor in research processes, but manual assessment is costly. With
recent advances in agentic AI systems (i.e., AI agents), we seek to evaluate
their capability to automate this process. However, existing benchmarks for
reproducing research papers (1) focus solely on reproducing results using
provided code and data without assessing their consistency with the paper, (2)
oversimplify real-world scenarios, and (3) lack necessary diversity in data
formats and programming languages. To address these issues, we introduce
REPRO-Bench, a collection of 112 task instances, each representing a social
science paper with a publicly available reproduction report. The agents are
tasked with assessing the reproducibility of the paper based on the original
paper PDF and the corresponding reproduction package. REPRO-Bench features
end-to-end evaluation tasks on the reproducibility of social science papers
with complexity comparable to real-world assessments. We evaluate three
representative AI agents on REPRO-Bench, with the best-performing agent
achieving an accuracy of only 21.4%. Building on our empirical analysis, we
develop REPRO-Agent, which improves the highest accuracy achieved by existing
agents by 71%. We conclude that more advanced AI agents should be developed to
automate real-world reproducibility assessment. REPRO-Bench is publicly
available at https://github.com/uiuc-kang-lab/REPRO-Bench.

</details>


### [10] [SLoW: Select Low-frequency Words! Automatic Dictionary Selection for Translation on Large Language Models](https://arxiv.org/abs/2507.18902)
*Hongyuan Lu,Zixuan Li,Zefan Zhang,Wai Lam*

Main category: cs.CL

TL;DR: 提出了一种自动词典选择（ADS）任务和一种名为SLoW的方法，用于在低频词典上提升翻译性能，实验表明SLoW在节省token的同时还能提升翻译质量。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型（LLM）仅支持数百种语言，而全球有7,000多种语言。基于词典的prompt方法可以增强这些语言的翻译，但是大多数方法都使用所有可用的词典，这可能会很昂贵。因此，在token消耗和翻译性能之间进行权衡将是灵活的。

Method: 提出了一种新颖有效的方法，称为SLoW，它选择那些频率较低的词典。

Result: SLoW方法超越了强大的基线，并且可以明显节省token使用量，在许多语言上甚至超过了完整词典基线的翻译性能。该方法不需要访问训练数据即可进行频率估计，并且使用公共资源获得的估计频率在改善ChatGPT和Llama和DeepSeek的翻译方面仍然有效。

Conclusion: SLoW方法在100种FLORES语言上的实验结果表明，该方法超越了强大的基线，并且可以明显节省token使用量，在许多语言上甚至超过了完整词典基线的翻译性能。

Abstract: There are more than 7,000 languages around the world, and current Large
Language Models (LLMs) only support hundreds of languages. Dictionary-based
prompting methods can enhance translation on them, but most methods use all the
available dictionaries, which could be expensive. Instead, it will be flexible
to have a trade-off between token consumption and translation performance. This
paper proposes a novel task called \textbf{A}utomatic \textbf{D}ictionary
\textbf{S}election (\textbf{ADS}). The goal of the task is to automatically
select which dictionary to use to enhance translation. We propose a novel and
effective method which we call \textbf{S}elect \textbf{Lo}w-frequency
\textbf{W}ords! (\textbf{SLoW}) which selects those dictionaries that have a
lower frequency. Our methods have unique advantages. First, there is no need
for access to the training data for frequency estimation (which is usually
unavailable). Second, it inherits the advantage of dictionary-based methods,
where no additional tuning is required on LLMs. Experimental results on 100
languages from FLORES indicate that SLoW surpasses strong baselines, and it can
obviously save token usage, with many languages even surpassing the translation
performance of the full dictionary baseline.\footnote{A shocking fact is that
there is no need to use the actual training data (often unobtainable) for
frequency estimation, and an estimation frequency obtained using public
resources is still apparently effective in improving translation with ChatGPT
and Llama, and DeepSeek.}\footnote{Code and data available upon publication.}

</details>


### [11] [Large language models provide unsafe answers to patient-posed medical questions](https://arxiv.org/abs/2507.18905)
*Rachel L. Draelos,Samina Afreen,Barbara Blasko,Tiffany Brazile,Natasha Chase,Dimple Desai,Jessica Evert,Heather L. Gardner,Lauren Herrmann,Aswathy Vaikom House,Stephanie Kass,Marianne Kavan,Kirshma Khemani,Amanda Koire,Lauren M. McDonald,Zahraa Rabeeah,Amy Shah*

Main category: cs.CL

TL;DR: 该研究评估了四个聊天机器人在医疗建议方面的安全性，发现它们可能提供不安全的建议，需要改进。


<details>
  <summary>Details</summary>
Motivation: 数百万患者经常使用大型语言模型 (LLM) 聊天机器人来获取医疗建议，这引起了患者安全方面的担忧。

Method: 使用一个新的数据集 HealthAdvice，通过一个能够进行定量和定性分析的评估框架，比较了 Anthropic 的 Claude、Google 的 Gemini、OpenAI 的 GPT-4o 和 Meta 的 Llama3-70B 这四个公开可用的聊天机器人的安全性。

Result: 聊天机器人的问题回复率从 21.6% (Claude) 到 43.2% (Llama) 不等，不安全回复率从 5% (Claude) 到 13% (GPT-4o, Llama) 不等。定性结果显示，聊天机器人的回复可能导致严重的患者伤害。

Conclusion: 数百万患者可能正在从公开的聊天机器人那里获得不安全的医疗建议，需要进一步工作以提高这些强大工具的临床安全性。

Abstract: Millions of patients are already using large language model (LLM) chatbots
for medical advice on a regular basis, raising patient safety concerns. This
physician-led red-teaming study compares the safety of four publicly available
chatbots--Claude by Anthropic, Gemini by Google, GPT-4o by OpenAI, and
Llama3-70B by Meta--on a new dataset, HealthAdvice, using an evaluation
framework that enables quantitative and qualitative analysis. In total, 888
chatbot responses are evaluated for 222 patient-posed advice-seeking medical
questions on primary care topics spanning internal medicine, women's health,
and pediatrics. We find statistically significant differences between chatbots.
The rate of problematic responses varies from 21.6 percent (Claude) to 43.2
percent (Llama), with unsafe responses varying from 5 percent (Claude) to 13
percent (GPT-4o, Llama). Qualitative results reveal chatbot responses with the
potential to lead to serious patient harm. This study suggests that millions of
patients could be receiving unsafe medical advice from publicly available
chatbots, and further work is needed to improve the clinical safety of these
powerful tools.

</details>


### [12] [A Systematic Review of Key Retrieval-Augmented Generation (RAG) Systems: Progress, Gaps, and Future Directions](https://arxiv.org/abs/2507.18910)
*Agada Joseph Oche,Ademola Glory Folashade,Tirthankar Ghosal,Arpan Biswas*

Main category: cs.CL

TL;DR: 本文全面回顾了 RAG，从动机、核心技术组件、关键里程碑、企业系统中的部署、性能评估以及新兴解决方案等方面进行了探讨。


<details>
  <summary>Details</summary>
Motivation: RAG 旨在减轻参数模型中的幻觉和过时知识。

Method: 对 RAG 进行了全面的系统回顾，追踪了其从开放域问答的早期发展到最近在各种应用中的最新实现。

Result: 对 RAG 实现进行了比较评估，对检索准确率、生成流畅性、延迟和计算效率进行了基准测试。评估了检索质量、隐私问题和集成开销等持续存在的挑战。

Conclusion: RAG 的未来发展方向包括混合检索方法、隐私保护技术、优化融合策略和 Agentic RAG 架构，这些创新将使知识密集型 NLP 系统更可靠、高效和具有上下文感知能力。

Abstract: Retrieval-Augmented Generation (RAG) represents a major advancement in
natural language processing (NLP), combining large language models (LLMs) with
information retrieval systems to enhance factual grounding, accuracy, and
contextual relevance. This paper presents a comprehensive systematic review of
RAG, tracing its evolution from early developments in open domain question
answering to recent state-of-the-art implementations across diverse
applications. The review begins by outlining the motivations behind RAG,
particularly its ability to mitigate hallucinations and outdated knowledge in
parametric models. Core technical components-retrieval mechanisms,
sequence-to-sequence generation models, and fusion strategies are examined in
detail. A year-by-year analysis highlights key milestones and research trends,
providing insight into RAG's rapid growth. The paper further explores the
deployment of RAG in enterprise systems, addressing practical challenges
related to retrieval of proprietary data, security, and scalability. A
comparative evaluation of RAG implementations is conducted, benchmarking
performance on retrieval accuracy, generation fluency, latency, and
computational efficiency. Persistent challenges such as retrieval quality,
privacy concerns, and integration overhead are critically assessed. Finally,
the review highlights emerging solutions, including hybrid retrieval
approaches, privacy-preserving techniques, optimized fusion strategies, and
agentic RAG architectures. These innovations point toward a future of more
reliable, efficient, and context-aware knowledge-intensive NLP systems.

</details>


### [13] [Mining Contextualized Visual Associations from Images for Creativity Understanding](https://arxiv.org/abs/2507.18915)
*Ananya Sahu,Amith Ananthram,Kathleen McKeown*

Main category: cs.CL

TL;DR: 提出了一种挖掘图像情境化关联的方法，生成高质量的创意标题，并创建一个新的数据集，用于改进零样本图像-文本检索。


<details>
  <summary>Details</summary>
Motivation: 理解他人的创造性产出需要共同的联想语言。然而，在训练视觉-语言模型（如CLIP）时，我们依赖于包含简短、主要为字面意义的alt-text的网络抓取数据集。

Method: 引入一种方法，用于挖掘图像中显著视觉元素的情境化关联，该方法可以扩展到任何未标记的数据集。

Result: 生成了一个新的视觉关联数据集和170万个MSCOCO图像的创意标题。人类评估证实，这些标题保持了视觉基础，同时表现出明显增强的抽象性。

Conclusion: 该数据集可以提升图像-文本检索在诗歌和隐喻可视化方面的性能

Abstract: Understanding another person's creative output requires a shared language of
association. However, when training vision-language models such as CLIP, we
rely on web-scraped datasets containing short, predominantly literal, alt-text.
In this work, we introduce a method for mining contextualized associations for
salient visual elements in an image that can scale to any unlabeled dataset.
Given an image, we can use these mined associations to generate high quality
creative captions at increasing degrees of abstraction. With our method, we
produce a new dataset of visual associations and 1.7m creative captions for the
images in MSCOCO. Human evaluation confirms that these captions remain visually
grounded while exhibiting recognizably increasing abstraction. Moreover,
fine-tuning a visual encoder on this dataset yields meaningful improvements in
zero-shot image-text retrieval in two creative domains: poetry and metaphor
visualization. We release our dataset, our generation code and our models for
use by the broader community.

</details>


### [14] [Uncovering Cross-Linguistic Disparities in LLMs using Sparse Autoencoders](https://arxiv.org/abs/2507.18918)
*Richmond Sin Jing Xuan,Jalil Huseynov,Yang Zhang*

Main category: cs.CL

TL;DR: 多语言 LLM 在低资源语言上的表现不佳。激活感知微调可以显著提高这些语言的性能。


<details>
  <summary>Details</summary>
Motivation: 多语言大型语言模型 (LLM) 表现出强大的跨语言泛化能力，但中低资源语言在 ARC-Challenge、MMLU 和 HellaSwag 等常见基准测试中的表现不佳。

Method: 使用稀疏自动编码器 (SAE) 分析 Gemma-2-2B 在所有 26 个残差层和 10 种语言（包括中文 (zh)、俄语 (ru)、西班牙语 (es)、意大利语 (it) 以及中低资源语言，包括印度尼西亚语 (id)、加泰罗尼亚语 (ca)、马拉地语 (mr)、马拉雅拉姆语 (ml) 和印地语 (hi)，并以英语 (en) 作为参考）中的激活模式。

Result: 中低资源语言在早期层中的激活率最多降低 26.27%，在更深层中持续存在 19.89% 的差距。为了解决这个问题，我们通过低秩适应 (LoRA) 应用激活感知微调，从而带来显着的激活增益，例如马拉雅拉姆语为 87.69%，印地语为 86.32%，同时将英语保留率维持在约 91%。

Conclusion: 通过激活感知微调，基准测试结果显示出适度但持续的改进，突显了激活对齐是增强多语言 LLM 性能的关键因素。

Abstract: Multilingual large language models (LLMs) exhibit strong cross-linguistic
generalization, yet medium to low resource languages underperform on common
benchmarks such as ARC-Challenge, MMLU, and HellaSwag. We analyze activation
patterns in Gemma-2-2B across all 26 residual layers and 10 languages: Chinese
(zh), Russian (ru), Spanish (es), Italian (it), medium to low resource
languages including Indonesian (id), Catalan (ca), Marathi (mr), Malayalam
(ml), and Hindi (hi), with English (en) as the reference. Using Sparse
Autoencoders (SAEs), we reveal systematic disparities in activation patterns.
Medium to low resource languages receive up to 26.27 percent lower activations
in early layers, with a persistent gap of 19.89 percent in deeper layers. To
address this, we apply activation-aware fine-tuning via Low-Rank Adaptation
(LoRA), leading to substantial activation gains, such as 87.69 percent for
Malayalam and 86.32 percent for Hindi, while maintaining English retention at
approximately 91 percent. After fine-tuning, benchmark results show modest but
consistent improvements, highlighting activation alignment as a key factor in
enhancing multilingual LLM performance.

</details>


### [15] [LLaVA-NeuMT: Selective Layer-Neuron Modulation for Efficient Multilingual Multimodal Translation](https://arxiv.org/abs/2507.18940)
*Jingxuan Wei,Caijun Jia,Qi Chen,Yujun Cai,Linzhuang Sun,Xiangxiang Zhang,Gaowei Wu,Bihui Yu*

Main category: cs.CL

TL;DR: LLaVA-NeuMT: a new multimodal multilingual translation framework that outperforms existing methods by explicitly modeling language-specific and language-agnostic representations, achieving SOTA results with only 40% fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Extending existing MMT methods to multilingual translation is challenging due to cross-lingual interference and ineffective parameter-sharing strategies.

Method: LLaVA-NeuMT, a novel multimodal multilingual translation framework that explicitly models language-specific and language-agnostic representations using a layer selection mechanism and a neuron-level adaptation strategy.

Result: LLaVA-NeuMT surpasses full fine-tuning approaches and achieves SOTA results on both M3-Multi30K and M3-AmbigCaps datasets while fine-tuning only 40% of the model parameters.

Conclusion: LLaVA-NeuMT achieves SOTA results on M3-Multi30K and M3-AmbigCaps datasets, surpassing full fine-tuning approaches while fine-tuning only 40% of the model parameters. The analysis provides insights into the importance of selected layers and neurons.

Abstract: Multimodal Machine Translation (MMT) enhances translation quality by
incorporating visual context, helping to resolve textual ambiguities. While
existing MMT methods perform well in bilingual settings, extending them to
multilingual translation remains challenging due to cross-lingual interference
and ineffective parameter-sharing strategies. To address this, we propose
LLaVA-NeuMT, a novel multimodal multilingual translation framework that
explicitly models language-specific and language-agnostic representations to
mitigate multilingual interference. Our approach consists of a layer selection
mechanism that identifies the most informative layers for different language
pairs and a neuron-level adaptation strategy that dynamically selects
language-specific and agnostic neurons to improve translation quality while
reducing redundancy. We conduct extensive experiments on the M3-Multi30K and
M3-AmbigCaps datasets, demonstrating that LLaVA-NeuMT, while fine-tuning only
40\% of the model parameters, surpasses full fine-tuning approaches and
ultimately achieves SOTA results on both datasets. Our analysis further
provides insights into the importance of selected layers and neurons in
multimodal multilingual adaptation, offering an efficient and scalable solution
to cross-lingual adaptation in multimodal translation.

</details>


### [16] [Legal Document Summarization: Enhancing Judicial Efficiency through Automation Detection](https://arxiv.org/abs/2507.18952)
*Yongjie Li,Ruilin Nong,Jianan Liu,Lucas Evans*

Main category: cs.CL

TL;DR: 通过自动化关键信息检测来提高司法效率。


<details>
  <summary>Details</summary>
Motivation: 法律文件摘要代表了通过自动化关键信息检测来提高司法效率的重大进步。

Method: 利用最先进的自然语言处理技术，从大量的法律文本中细致地识别和提取重要数据，通过采用先进的机器学习算法，该框架识别司法文件中的潜在模式，以创建包含关键要素的精确摘要。

Result: 结果表明运营效率显著提高，使法律从业人员能够将精力集中于关键分析和决策活动，而不是手动审查。

Conclusion: 这项研究强调了有前景的技术驱动策略，这些策略可以显著改变法律部门内的工作流程动态，强调自动化在改进司法流程中的作用。

Abstract: Legal document summarization represents a significant advancement towards
improving judicial efficiency through the automation of key information
detection. Our approach leverages state-of-the-art natural language processing
techniques to meticulously identify and extract essential data from extensive
legal texts, which facilitates a more efficient review process. By employing
advanced machine learning algorithms, the framework recognizes underlying
patterns within judicial documents to create precise summaries that encapsulate
the crucial elements. This automation alleviates the burden on legal
professionals, concurrently reducing the likelihood of overlooking vital
information that could lead to errors. Through comprehensive experiments
conducted with actual legal datasets, we demonstrate the capability of our
method to generate high-quality summaries while preserving the integrity of the
original content and enhancing processing times considerably. The results
reveal marked improvements in operational efficiency, allowing legal
practitioners to direct their efforts toward critical analytical and
decision-making activities instead of manual reviews. This research highlights
promising technology-driven strategies that can significantly alter workflow
dynamics within the legal sector, emphasizing the role of automation in
refining judicial processes.

</details>


### [17] [A Similarity Measure for Comparing Conversational Dynamics](https://arxiv.org/abs/2507.18956)
*Sang Min Jung,Kaixiang Zhang,Cristian Danescu-Niculescu-Mizil*

Main category: cs.CL

TL;DR: We introduce a similarity measure for comparing conversations and use it to analyze conversational dynamics in a large online community, bringing new insights into the role of situational power in conversations.


<details>
  <summary>Details</summary>
Motivation: There is no robust automated method for comparing conversations in terms of their overall interactional dynamics. Such methods could enhance the analysis of conversational data and help evaluate conversational agents more holistically.

Method: design a validation framework for testing the robustness of the metric in capturing differences in conversation dynamics and for assessing its sensitivity to the topic of the conversations.

Result: analyze conversational dynamics in a large online community, bringing new insights into the role of situational power in conversations.

Conclusion: We introduce a similarity measure for comparing conversations with respect to their dynamics.

Abstract: The quality of a conversation goes beyond the individual quality of each
reply, and instead emerges from how these combine into interactional patterns
that give the conversation its distinctive overall "shape". However, there is
no robust automated method for comparing conversations in terms of their
overall interactional dynamics. Such methods could enhance the analysis of
conversational data and help evaluate conversational agents more holistically.
  In this work, we introduce a similarity measure for comparing conversations
with respect to their dynamics. We design a validation framework for testing
the robustness of the metric in capturing differences in conversation dynamics
and for assessing its sensitivity to the topic of the conversations. Finally,
to illustrate the measure's utility, we use it to analyze conversational
dynamics in a large online community, bringing new insights into the role of
situational power in conversations.

</details>


### [18] [A Toolbox, Not a Hammer -- Multi-TAG: Scaling Math Reasoning with Multi-Tool Aggregation](https://arxiv.org/abs/2507.18973)
*Bohan Yao,Vikas Yadav*

Main category: cs.CL

TL;DR: Multi-TAG, a finetuning-free framework, guides an LLM to concurrently invoke multiple tools at each reasoning step, aggregates their outputs, and achieves average improvements of 6.0% to 7.5% over state-of-the-art baselines.


<details>
  <summary>Details</summary>
Motivation: Prior tool-augmented approaches typically finetune an LLM to select and invoke a single tool at each reasoning step and show promising results on simpler math reasoning benchmarks such as GSM8K. However, these approaches struggle with more complex math problems that require precise reasoning over multiple steps.

Method: Multi-TAG guides an LLM to concurrently invoke multiple tools at each reasoning step and then aggregates their diverse outputs to verify and refine the reasoning process

Result: achieving average improvements of 6.0% to 7.5% over state-of-the-art baselines

Conclusion: Multi-TAG consistently and substantially outperforms state-of-the-art baselines, achieving average improvements of 6.0% to 7.5% over state-of-the-art baselines.

Abstract: Augmenting large language models (LLMs) with external tools is a promising
avenue for developing high-performance mathematical reasoning systems. Prior
tool-augmented approaches typically finetune an LLM to select and invoke a
single tool at each reasoning step and show promising results on simpler math
reasoning benchmarks such as GSM8K. However, these approaches struggle with
more complex math problems that require precise reasoning over multiple steps.
To address this limitation, in this work, we propose Multi-TAG, a Multi-Tool
AGgregation-based framework. Instead of relying on a single tool, Multi-TAG
guides an LLM to concurrently invoke multiple tools at each reasoning step. It
then aggregates their diverse outputs to verify and refine the reasoning
process, enhancing solution robustness and accuracy. Notably, Multi-TAG is a
finetuning-free, inference-only framework, making it readily applicable to any
LLM backbone, including large open-weight models which are computationally
expensive to finetune and proprietary frontier models which cannot be finetuned
with custom recipes. We evaluate Multi-TAG on four challenging benchmarks:
MATH500, AIME, AMC, and OlympiadBench. Across both open-weight and
closed-source LLM backbones, Multi-TAG consistently and substantially
outperforms state-of-the-art baselines, achieving average improvements of 6.0%
to 7.5% over state-of-the-art baselines.

</details>


### [19] [Arg-LLaDA: Argument Summarization via Large Language Diffusion Models and Sufficiency-Aware Refinement](https://arxiv.org/abs/2507.19081)
*Hao Li,Yizheng Sun,Viktor Schlegel,Kailai Yang,Riza Batista-Navarro,Goran Nenadic*

Main category: cs.CL

TL;DR: Arg-LLaDA 是一种新的大型语言扩散框架，它通过充分性引导的重掩码和再生来迭代地改进摘要，从而产生更忠实、简洁和连贯的输出。


<details>
  <summary>Details</summary>
Motivation: 论点总结旨在生成复杂、多角度辩论的简洁、结构化表示。然而，现有的方法通常依赖于单次生成，对事实修正或结构细化的支持有限。

Method: 引入了一种新颖的大型语言扩散框架Arg-LLaDA，该框架通过充分性引导的重掩码和再生来迭代地改进摘要。

Result: 在两个基准数据集上的实证结果表明，Arg-LLaDA在十分之七的自动评估指标中超过了最先进的基线。

Conclusion: Arg-LLaDA在核心维度、覆盖率、忠实度和简洁性方面都有显著改进，验证了迭代的、充分性感知生成策略的有效性。

Abstract: Argument summarization aims to generate concise, structured representations
of complex, multi-perspective debates. While recent work has advanced the
identification and clustering of argumentative components, the generation stage
remains underexplored. Existing approaches typically rely on single-pass
generation, offering limited support for factual correction or structural
refinement. To address this gap, we introduce Arg-LLaDA, a novel large language
diffusion framework that iteratively improves summaries via sufficiency-guided
remasking and regeneration. Our method combines a flexible masking controller
with a sufficiency-checking module to identify and revise unsupported,
redundant, or incomplete spans, yielding more faithful, concise, and coherent
outputs. Empirical results on two benchmark datasets demonstrate that Arg-LLaDA
surpasses state-of-the-art baselines in 7 out of 10 automatic evaluation
metrics. In addition, human evaluations reveal substantial improvements across
core dimensions, coverage, faithfulness, and conciseness, validating the
effectiveness of our iterative, sufficiency-aware generation strategy.

</details>


### [20] [Debating Truth: Debate-driven Claim Verification with Multiple Large Language Model Agents](https://arxiv.org/abs/2507.19090)
*Haorui He,Yupeng Li,Dacheng Wen,Reynold Cheng,Francis C. M. Lau*

Main category: cs.CL

TL;DR: DebateCV, a debate-driven claim verification framework using multiple LLM agents, outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing single-LLM methods struggle with complex claim verification that involves multi-faceted evidences. Claim verification is critical for enhancing digital literacy.

Method: A debate-driven methodology using multiple LLM agents is proposed, where two Debaters take opposing stances and engage in multi-round argumentation, while a Moderator evaluates the arguments and renders a verdict. A novel post-training strategy is introduced to improve the Moderator's performance, leveraging synthetic debate data.

Result: The method outperforms existing claim verification methods under varying levels of evidence quality.

Conclusion: The proposed DebateCV framework outperforms existing claim verification methods under varying levels of evidence quality.

Abstract: Claim verification is critical for enhancing digital literacy. However, the
state-of-the-art single-LLM methods struggle with complex claim verification
that involves multi-faceted evidences. Inspired by real-world fact-checking
practices, we propose DebateCV, the first claim verification framework that
adopts a debate-driven methodology using multiple LLM agents. In our framework,
two Debaters take opposing stances on a claim and engage in multi-round
argumentation, while a Moderator evaluates the arguments and renders a verdict
with justifications. To further improve the performance of the Moderator, we
introduce a novel post-training strategy that leverages synthetic debate data
generated by the zero-shot DebateCV, effectively addressing the scarcity of
real-world debate-driven claim verification data. Experimental results show
that our method outperforms existing claim verification methods under varying
levels of evidence quality. Our code and dataset are publicly available at
https://anonymous.4open.science/r/DebateCV-6781.

</details>


### [21] [Objectifying the Subjective: Cognitive Biases in Topic Interpretations](https://arxiv.org/abs/2507.19117)
*Swapnil Hingmire,Ze Shi Li,Shiyu,Zeng,Ahmed Musa Awon,Luiz Franciscatto Guerra,Neil Ernst*

Main category: cs.CL

TL;DR: 用户通过锚定显着词语并进行语义调整来解释主题。


<details>
  <summary>Details</summary>
Motivation: 主题的解释对于其下游应用至关重要。主题质量的最新评估方法，如连贯性和词语入侵，并不能衡量一个主题在多大程度上促进了语料库的探索。为了设计基于任务和用户群体的评估方法，我们进行了用户研究，以了解用户如何解释主题。

Method: 使用反身专题分析从理由中识别主题解释的主题。

Result: 用户根据显着词语进行锚定，并进行语义调整以得出解释。我们提出了一个基于锚定和调整启发法的主题解释理论。

Conclusion: 用户根据可得性和代表性启发法，而不是概率来解释主题。主题解释可以被看作是在不确定性下由具有生态理性的用户做出的判断，因此需要有认知偏差的用户模型和评估框架。

Abstract: Interpretation of topics is crucial for their downstream applications.
State-of-the-art evaluation measures of topic quality such as coherence and
word intrusion do not measure how much a topic facilitates the exploration of a
corpus. To design evaluation measures grounded on a task, and a population of
users, we do user studies to understand how users interpret topics. We propose
constructs of topic quality and ask users to assess them in the context of a
topic and provide rationale behind evaluations. We use reflexive thematic
analysis to identify themes of topic interpretations from rationales. Users
interpret topics based on availability and representativeness heuristics rather
than probability. We propose a theory of topic interpretation based on the
anchoring-and-adjustment heuristic: users anchor on salient words and make
semantic adjustments to arrive at an interpretation. Topic interpretation can
be viewed as making a judgment under uncertainty by an ecologically rational
user, and hence cognitive biases aware user models and evaluation frameworks
are needed.

</details>


### [22] [An Empirical Investigation of Gender Stereotype Representation in Large Language Models: The Italian Case](https://arxiv.org/abs/2507.19156)
*Gioele Giachino,Marco Rondina,Antonio Vetrò,Riccardo Coppola,Juan Carlos De Martin*

Main category: cs.CL

TL;DR: This paper examines how LLMs perpetuate gender and professional biases in Italian using ChatGPT and Gemini. Results show LLMs often reinforce stereotypes, raising ethical concerns.


<details>
  <summary>Details</summary>
Motivation: The increasing use of Large Language Models (LLMs) in a large variety of domains has sparked worries about how easily they can perpetuate stereotypes and contribute to the generation of biased content. With a focus on gender and professional bias, this work examines in which manner LLMs shape responses to ungendered prompts, contributing to biased outputs.

Method: This analysis uses a structured experimental method, giving different prompts involving three different professional job combinations, which are also characterized by a hierarchical relationship. This study uses Italian, a language with extensive grammatical gender differences, to highlight potential limitations in current LLMs' ability to generate objective text in non-English languages. Two popular LLM-based chatbots are examined, namely OpenAI ChatGPT (gpt-4o-mini) and Google Gemini (gemini-1.5-flash). Through APIs, we collected a range of 3600 responses.

Result: The results highlight how content generated by LLMs can perpetuate stereotypes. For example, Gemini associated 100% (ChatGPT 97%) of 'she' pronouns to the 'assistant' rather than the 'manager'.

Conclusion: The presence of bias in AI-generated text can have significant implications in many fields, such as in the workplaces or in job selections, raising ethical concerns about its use. Understanding these risks is pivotal to developing mitigation strategies and assuring that AI-based systems do not increase social inequalities, but rather contribute to more equitable outcomes.

Abstract: The increasing use of Large Language Models (LLMs) in a large variety of
domains has sparked worries about how easily they can perpetuate stereotypes
and contribute to the generation of biased content. With a focus on gender and
professional bias, this work examines in which manner LLMs shape responses to
ungendered prompts, contributing to biased outputs. This analysis uses a
structured experimental method, giving different prompts involving three
different professional job combinations, which are also characterized by a
hierarchical relationship. This study uses Italian, a language with extensive
grammatical gender differences, to highlight potential limitations in current
LLMs' ability to generate objective text in non-English languages. Two popular
LLM-based chatbots are examined, namely OpenAI ChatGPT (gpt-4o-mini) and Google
Gemini (gemini-1.5-flash). Through APIs, we collected a range of 3600
responses. The results highlight how content generated by LLMs can perpetuate
stereotypes. For example, Gemini associated 100% (ChatGPT 97%) of 'she'
pronouns to the 'assistant' rather than the 'manager'. The presence of bias in
AI-generated text can have significant implications in many fields, such as in
the workplaces or in job selections, raising ethical concerns about its use.
Understanding these risks is pivotal to developing mitigation strategies and
assuring that AI-based systems do not increase social inequalities, but rather
contribute to more equitable outcomes. Future research directions include
expanding the study to additional chatbots or languages, refining prompt
engineering methods or further exploiting a larger experimental base.

</details>


### [23] [Can Small-Scale Data Poisoning Exacerbate Dialect-Linked Biases in Large Language Models?](https://arxiv.org/abs/2507.19195)
*Chaymaa Abbas,Mariette Awad,Razane Tajeddine*

Main category: cs.CL

TL;DR: 研究表明，数据中毒会不成比例地增加 AAVE 输入的毒性，强调了在 LLM 开发中解决方言偏见的需求。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 在促进包容性和平衡响应方面不断改进，但这些系统仍然容易编码和放大社会偏见。本研究旨在检验方言变异（特别是非裔美国人白话英语 (AAVE) 与标准美国英语 (SAE)）如何与数据中毒相互作用，从而影响输出中的毒性。

Method: 使用小型和中型 LLaMA 模型，并使用 GPT-4o 作为公平审计员。

Result: 即使少量接触中毒数据也会显着增加 AAVE 输入的毒性，而 SAE 的毒性相对不受影响。较大的模型表现出更显着的放大效应，这表明随着规模的扩大，敏感性也会提高。GPT-4o 发现与 AAVE 输入不成比例的有害刻板印象模式，包括对攻击、犯罪和智力低下的描述。

Conclusion: 数据中毒和方言偏见会相互叠加影响，因此在模型开发过程中，需要注意方言相关的评估、有针对性的去偏见干预以及具有社会责任感的训练协议。

Abstract: Despite the ongoing improvements in the design of large language models
(LLMs) to foster inclusion and balanced responses, these systems remain
susceptible to encoding and amplifying social biases. This study examines how
dialectal variation, specifically African American Vernacular English (AAVE)
versus Standard American English (SAE), interacts with data poisoning to
influence toxicity in outputs. Using both small- and medium-scale LLaMA models,
we show that even minimal exposure to poisoned data significantly increases
toxicity for AAVE inputs, while it remains comparatively unaffected for SAE.
Larger models exhibit a more significant amplification effect which suggests
heightened susceptibility with scale. To further assess these disparities, we
employed GPT-4o as a fairness auditor, which identified harmful stereotypical
patterns disproportionately tied to AAVE inputs, including portrayals of
aggression, criminality, and intellectual inferiority. These findings
underscore the compounding impact of data poisoning and dialectal bias and
emphasize the need for dialect-aware evaluation, targeted debiasing
interventions, and socially responsible training protocols during development.

</details>


### [24] [How Much Do Large Language Model Cheat on Evaluation? Benchmarking Overestimation under the One-Time-Pad-Based Framework](https://arxiv.org/abs/2507.19219)
*Zi Liang,Liantong Yu,Shiyu Zhang,Qingqing Ye,Haibo Hu*

Main category: cs.CL

TL;DR: ArxivRoll是一个动态评估框架，通过构建新的基准来解决大型语言模型评估中过度估计的问题，该基准每六个月更新一次，以评估LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 评估中的高估已成为一个日益严重的问题。由于公共基准的污染或不平衡的模型训练，LLM 可能会在公共基准上获得不真实的评估结果，从而导致 LLM 之间的不公平比较，并损害其现实能力评估。现有的基准试图通过永久保密测试用例、通过人工评估减轻污染或重复收集和构建新样本来解决这些问题。但是，这些方法无法同时确保可重复性、透明性和高效率。此外，当前法学硕士的高估程度仍未量化。

Method: ArxivRoll包含两个关键组件：SCP（排序、完形填空和预测）和 Rugged Scores (RS)。SCP是一个私有测试用例的自动生成器。RS是衡量公共基准污染和训练偏差程度的指标。ArxivRoll每六个月使用来自 ArXiv 的最新文章构建一个新的基准，并使用它们对 LLM 性能进行一次性评估。

Result: 广泛的实验证明了我们的基准的高质量，并且我们对当前的大型语言模型进行了系统的评估。

Conclusion: ArxivRoll是一个动态评估框架，可以系统地评估当前大型语言模型。

Abstract: Overestimation in evaluating large language models (LLMs) has become an
increasing concern. Due to the contamination of public benchmarks or imbalanced
model training, LLMs may achieve unreal evaluation results on public
benchmarks, either intentionally or unintentionally, which leads to unfair
comparisons among LLMs and undermines their realistic capability assessments.
Existing benchmarks attempt to address these issues by keeping test cases
permanently secret, mitigating contamination through human evaluation, or
repeatedly collecting and constructing new samples. However, these approaches
fail to ensure reproducibility, transparency, and high efficiency
simultaneously. Moreover, the extent of overestimation in current LLMs remains
unquantified. To address these issues, we propose ArxivRoll, a dynamic
evaluation framework inspired by one-time pad encryption in cryptography.
ArxivRoll comprises two key components: \emph{i) SCP (Sequencing, Cloze, and
Prediction)}, an automated generator for private test cases, and \emph{ii)
Rugged Scores (RS)}, metrics that measure the proportion of public benchmark
contamination and training bias. Leveraging SCP, ArxivRoll constructs a new
benchmark every six months using recent articles from ArXiv and employs them
for one-time evaluations of LLM performance. Extensive experiments demonstrate
the high quality of our benchmark, and we provide a systematic evaluation of
current LLMs. The source code is available at
https://github.com/liangzid/ArxivRoll/.

</details>


### [25] [Jailbreaking Large Language Diffusion Models: Revealing Hidden Safety Flaws in Diffusion-Based Text Generation](https://arxiv.org/abs/2507.19227)
*Yuanhe Zhang,Fangzhou Xie,Zhenhong Zhou,Zherui Li,Hao Chen,Kun Wang,Yufei Guo*

Main category: cs.CL

TL;DR: LLDMs比LLMs更快，但也更容易受到攻击，可能被用于快速生成有害内容。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM jailbreak方法对LLDMs效果有限，并且LLDMs的快速生成能力加剧了人们对其产生有害内容的担忧。

Method: 提出了一种名为PAD的新的jailbreak方法，专门针对diffusion-based语言模型。

Result: PAD在四个LLDM上实现了97%的jailbreak攻击成功率，并且LLDMs的有害内容生成速度比相同大小的自回归LLMs快2倍。

Conclusion: LLDMs存在严重的安全漏洞，可能被用于恶意目的，并且速度比LLMs快。

Abstract: Large Language Diffusion Models (LLDMs) exhibit comparable performance to
LLMs while offering distinct advantages in inference speed and mathematical
reasoning tasks.The precise and rapid generation capabilities of LLDMs amplify
concerns of harmful generations, while existing jailbreak methodologies
designed for Large Language Models (LLMs) prove limited effectiveness against
LLDMs and fail to expose safety vulnerabilities.Successful defense cannot
definitively resolve harmful generation concerns, as it remains unclear whether
LLDMs possess safety robustness or existing attacks are incompatible with
diffusion-based architectures.To address this, we first reveal the
vulnerability of LLDMs to jailbreak and demonstrate that attack failure in
LLDMs stems from fundamental architectural differences.We present a PArallel
Decoding jailbreak (PAD) for diffusion-based language models. PAD introduces
Multi-Point Attention Attack, which guides parallel generative processes toward
harmful outputs that inspired by affirmative response patterns in LLMs.
Experimental evaluations across four LLDMs demonstrate that PAD achieves
jailbreak attack success rates by 97%, revealing significant safety
vulnerabilities. Furthermore, compared to autoregressive LLMs of the same size,
LLDMs increase the harmful generation speed by 2x, significantly highlighting
risks of uncontrolled misuse.Through comprehensive analysis, we provide an
investigation into LLDM architecture, offering critical insights for the secure
deployment of diffusion-based language models.

</details>


### [26] [Identifying Fine-grained Forms of Populism in Political Discourse: A Case Study on Donald Trump's Presidential Campaigns](https://arxiv.org/abs/2507.19303)
*Ilias Chalkidis,Stephanie Brandl,Paris Aslanidis*

Main category: cs.CL

TL;DR: 本文研究了 LLM 检测民粹主义言论的能力，发现微调的 RoBERTa 模型表现最好，指令调整的 LLM 在跨领域数据上更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 在各种指令跟随任务中表现出了卓越的能力，但它们对细致的社会科学概念的掌握仍有待探索。本文研究了 LLM 是否可以识别和分类细粒度的民粹主义形式。

Method: 通过创建新的数据集来捕获民粹主义言论，并评估一系列预训练的语言模型。

Result: 发现 LLM 在性能上存在显著差异，表明 LLM 在检测民粹主义言论方面存在局限性。微调的 RoBERTa 分类器大大优于所有新型指令调整的 LLM，除非经过微调。指令调整的 LLM 在领域外数据上表现出更强的鲁棒性。

Conclusion: LLMs 在检测民粹主义言论方面存在局限性。微调的 RoBERTa 分类器优于大多数 LLM，除非 LLM 也经过微调。指令调整的 LLM 在领域外数据上表现出更强的鲁棒性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
a wide range of instruction-following tasks, yet their grasp of nuanced social
science concepts remains underexplored. This paper examines whether LLMs can
identify and classify fine-grained forms of populism, a complex and contested
concept in both academic and media debates. To this end, we curate and release
novel datasets specifically designed to capture populist discourse. We evaluate
a range of pre-trained (large) language models, both open-weight and
proprietary, across multiple prompting paradigms. Our analysis reveals notable
variation in performance, highlighting the limitations of LLMs in detecting
populist discourse. We find that a fine-tuned RoBERTa classifier vastly
outperforms all new-era instruction-tuned LLMs, unless fine-tuned.
Additionally, we apply our best-performing model to analyze campaign speeches
by Donald Trump, extracting valuable insights into his strategic use of
populist rhetoric. Finally, we assess the generalizability of these models by
benchmarking them on campaign speeches by European politicians, offering a lens
into cross-context transferability in political discourse analysis. In this
setting, we find that instruction-tuned LLMs exhibit greater robustness on
out-of-domain data.

</details>


### [27] [AutoPCR: Automated Phenotype Concept Recognition by Prompting](https://arxiv.org/abs/2507.19315)
*Yicheng Tao,Yuanhao Huang,Jie Liu*

Main category: cs.CL

TL;DR: AutoPCR是一种基于提示的表型概念识别方法，不需要本体特定的训练，并且在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的表型概念识别（CR）方法通常需要本体特定的训练，并且难以推广到不同的文本类型和不断发展的生物医学术语。

Method: AutoPCR执行CR分为三个阶段：实体提取（使用基于规则和神经标记策略的混合）、通过SapBERT进行候选检索，以及通过提示大型语言模型进行实体链接。

Result: AutoPCR在提及级别和文档级别的评估中均表现出色，并且具有归纳能力和对新本体的泛化能力。

Conclusion: AutoPCR在多个基准数据集上实现了最佳的平均和最稳健的性能，超过了先前的最先进方法。

Abstract: Phenotype concept recognition (CR) is a fundamental task in biomedical text
mining, enabling applications such as clinical diagnostics and knowledge graph
construction. However, existing methods often require ontology-specific
training and struggle to generalize across diverse text types and evolving
biomedical terminology. We present AutoPCR, a prompt-based phenotype CR method
that does not require ontology-specific training. AutoPCR performs CR in three
stages: entity extraction using a hybrid of rule-based and neural tagging
strategies, candidate retrieval via SapBERT, and entity linking through
prompting a large language model. Experiments on four benchmark datasets show
that AutoPCR achieves the best average and most robust performance across both
mention-level and document-level evaluations, surpassing prior state-of-the-art
methods. Further ablation and transfer studies demonstrate its inductive
capability and generalizability to new ontologies.

</details>


### [28] [Smooth Reading: Bridging the Gap of Recurrent LLM to Self-Attention LLM on Long-Context Tasks](https://arxiv.org/abs/2507.19353)
*Kai Liu,Zhan Su,Peijie Dong,Fengran Mo,Jianfei Gao,ShaoTing Zhang,Kai Chen*

Main category: cs.CL

TL;DR: This paper presents Smooth Reading, a chunk-wise inference method for Recurrent LLMs that achieves comparable performance to Self-Attention LLMs on long-context tasks while maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: Recurrent LLMs, while efficient, underperform on long-context tasks due to limited memory, and processing the entire context at once is not well-suited for them.

Method: The paper introduces Smooth Reading, a chunk-wise inference method that processes context in chunks and iteratively summarizes contextual information.

Result: Smooth Reading significantly narrows the performance gap between Recurrent and Self-Attention LLMs on long-context tasks. It boosts SWA-3B-4k performance on LongBench and maintains high efficiency with faster training and inference.

Conclusion: The paper's Smooth Reading method enables Recurrent LLMs to achieve comparable performance with Self-Attention LLMs on long-context tasks, a first in the field. The authors will release code and dataset.

Abstract: Recently, recurrent large language models (Recurrent LLMs) with linear
computational complexity have re-emerged as efficient alternatives to
self-attention-based LLMs (Self-Attention LLMs), which have quadratic
complexity. However, Recurrent LLMs often underperform on long-context tasks
due to their limited fixed-size memory. Previous research has primarily focused
on enhancing the memory capacity of Recurrent LLMs through architectural
innovations, but these approaches have not yet enabled Recurrent LLMs to match
the performance of Self-Attention LLMs on long-context tasks. We argue that
this limitation arises because processing the entire context at once is not
well-suited for Recurrent LLMs. In this paper, we propose Smooth Reading, a
chunk-wise inference method inspired by human reading strategies. Smooth
Reading processes context in chunks and iteratively summarizes the contextual
information, thereby reducing memory demands and making the approach more
compatible with Recurrent LLMs. Our experimental results show that this method
substantially narrows the performance gap between Recurrent and Self-Attention
LLMs on long-context tasks, while preserving the efficiency advantages of
Recurrent LLMs. Our Smooth Reading boosts SWA-3B-4k (a Recurrent LLM) from
5.68% lower to 3.61% higher performance than Self-Attention LLMs on LongBench.
Besides, our method maintains the high efficiency, training 3x faster and
inferring 2x faster at 64k context compared to Self-Attention LLMs. To our
knowledge, this is the first work to achieve comparable performance using
Recurrent LLMs compared with Self-Attention LLMs on long-context tasks. We hope
our method will inspire future research in this area. To facilitate further
progress, we will release code and dataset.

</details>


### [29] [Enhancing Speech Emotion Recognition Leveraging Aligning Timestamps of ASR Transcripts and Speaker Diarization](https://arxiv.org/abs/2507.19356)
*Hsuan-Yu Wang,Pei-Ying Lee,Berlin Chen*

Main category: cs.CL

TL;DR: Timestamp alignment between ASR and speaker diarization improves speech emotion recognition accuracy.


<details>
  <summary>Details</summary>
Motivation: Misalignment between ASR transcripts and Speaker Diarization outputs reduces the reliability of multimodal emotion recognition systems.

Method: Alignment pipeline utilizing pre-trained ASR and speaker diarization models, synchronizing timestamps. Multimodal approach combines textual embeddings (RoBERTa) with audio embeddings (Wav2Vec), using cross-attention fusion with a gating mechanism.

Result: Improved SER accuracy on the IEMOCAP dataset with precise timestamp alignment.

Conclusion: Precise timestamp alignment improves SER accuracy, outperforming baseline methods. Temporal alignment enhances emotion recognition accuracy.

Abstract: In this paper, we investigate the impact of incorporating timestamp-based
alignment between Automatic Speech Recognition (ASR) transcripts and Speaker
Diarization (SD) outputs on Speech Emotion Recognition (SER) accuracy.
Misalignment between these two modalities often reduces the reliability of
multimodal emotion recognition systems, particularly in conversational
contexts. To address this issue, we introduce an alignment pipeline utilizing
pre-trained ASR and speaker diarization models, systematically synchronizing
timestamps to generate accurately labeled speaker segments. Our multimodal
approach combines textual embeddings extracted via RoBERTa with audio
embeddings from Wav2Vec, leveraging cross-attention fusion enhanced by a gating
mechanism. Experimental evaluations on the IEMOCAP benchmark dataset
demonstrate that precise timestamp alignment improves SER accuracy,
outperforming baseline methods that lack synchronization. The results highlight
the critical importance of temporal alignment, demonstrating its effectiveness
in enhancing overall emotion recognition accuracy and providing a foundation
for robust multimodal emotion analysis.

</details>


### [30] [SpeechIQ: Speech Intelligence Quotient Across Cognitive Levels in Voice Understanding Large Language Models](https://arxiv.org/abs/2507.19361)
*Zhen Wan,Chao-Han Huck Yang,Yahan Yu,Jinchuan Tian,Sheng Li,Ke Hu,Zhehuai Chen,Shinji Watanabe,Fei Cheng,Chenhui Chu,Sadao Kurohashi*

Main category: cs.CL

TL;DR: 提出了SIQ，这是一个新的评估流程，用于评估LLM Voice的语音理解能力，并通过三个认知层次进行评估。


<details>
  <summary>Details</summary>
Motivation: 旨在评估语音理解大型语言模型（LLM Voice）的语音理解能力。超越了流行的语音理解指标，如词错误率（WER）。

Method: 引入了一种新的基于语音的智商（SIQ）的人类认知启发评估流程，用于语音理解大型语言模型，即LLM Voice。

Result: SIQ不仅可以量化语音理解能力，还可以对级联方法（例如，ASR LLM）和端到端模型进行统一比较，识别现有基准中的注释错误，并检测LLM Voice中的幻觉。

Conclusion: SIQ框架是首个将认知原则与语音基准联系起来的智能检查，同时揭示了多模态训练中被忽视的挑战。

Abstract: We introduce Speech-based Intelligence Quotient (SIQ) as a new form of human
cognition-inspired evaluation pipeline for voice understanding large language
models, LLM Voice, designed to assess their voice understanding ability. Moving
beyond popular voice understanding metrics such as word error rate (WER), SIQ
examines LLM Voice across three cognitive levels motivated by Bloom's Taxonomy:
(1) Remembering (i.e., WER for verbatim accuracy); (2) Understanding (i.e.,
similarity of LLM's interpretations); and (3) Application (i.e., QA accuracy
for simulating downstream tasks). We demonstrate that SIQ not only quantifies
voice understanding abilities but also provides unified comparisons between
cascaded methods (e.g., ASR LLM) and end-to-end models, identifies annotation
errors in existing benchmarks, and detects hallucinations in LLM Voice. Our
framework represents a first-of-its-kind intelligence examination that bridges
cognitive principles with voice-oriented benchmarks, while exposing overlooked
challenges in multi-modal training.

</details>


### [31] [Data Augmentation for Spoken Grammatical Error Correction](https://arxiv.org/abs/2507.19374)
*Penny Karanasou,Mengjie Qian,Stefano Bannò,Mark J. F. Gales,Kate M. Knill*

Main category: cs.CL

TL;DR: propose a fully automated method to generate audio-text pairs with grammatical errors and disfluencies for Spoken GEC (SGEC).


<details>
  <summary>Details</summary>
Motivation: While there exist strong benchmark datasets for grammatical error correction (GEC), high-quality annotated spoken datasets for Spoken GEC (SGEC) are still under-resourced.

Method: propose a fully automated method to generate audio-text pairs with grammatical errors and disfluencies. Moreover, we propose a series of objective metrics that can be used to evaluate the generated data and choose the more suitable dataset for SGEC.

Result: This augmented dataset should augment and enrich the original corpus without altering the language assessment scores of the second language (L2) learners.

Conclusion: We evaluate the use of the augmented corpus both for written GEC (the text part) and for SGEC (the audio-text pairs). Our experiments are conducted on the S&I Corpus, the first publicly available speech dataset with grammar error annotations.

Abstract: While there exist strong benchmark datasets for grammatical error correction
(GEC), high-quality annotated spoken datasets for Spoken GEC (SGEC) are still
under-resourced. In this paper, we propose a fully automated method to generate
audio-text pairs with grammatical errors and disfluencies. Moreover, we propose
a series of objective metrics that can be used to evaluate the generated data
and choose the more suitable dataset for SGEC. The goal is to generate an
augmented dataset that maintains the textual and acoustic characteristics of
the original data while providing new types of errors. This augmented dataset
should augment and enrich the original corpus without altering the language
assessment scores of the second language (L2) learners. We evaluate the use of
the augmented corpus both for written GEC (the text part) and for SGEC (the
audio-text pairs). Our experiments are conducted on the S\&I Corpus, the first
publicly available speech dataset with grammar error annotations.

</details>


### [32] [Detection of Adverse Drug Events in Dutch clinical free text documents using Transformer Models: benchmark study](https://arxiv.org/abs/2507.19396)
*Rachel M. Murphy,Nishant Mishra,Nicolette F. de Keizer,Dave A. Dongelmans,Kitty J. Jager,Ameen Abu-Hanna,Joanna E. Klopotowska,Iacer Calixto*

Main category: cs.CL

TL;DR: This study benchmarks ADE detection in Dutch clinical text using transformer models, finding MedRoBERTa.nl to be the best performing model.


<details>
  <summary>Details</summary>
Motivation: The study aims to set a benchmark for adverse drug event (ADE) detection in Dutch clinical free text documents.

Method: The study trained a Bi-LSTM model and four transformer-based Dutch and/or multilingual encoder models (BERTje, RobBERT, MedRoBERTa.nl, and NuNER) for NER and RC tasks using 102 richly annotated Dutch ICU clinical progress notes. Models were evaluated internally using gold standard and predicted entities, and externally validated on detecting ADEs at the document level.

Result: MedRoBERTa.nl was the best performing model with macro-averaged F1 score of 0.63 using gold standard and 0.62 using predicted entities. External validation achieved recall of between 0.67 to 0.74 using predicted entities.

Conclusion: This benchmark study presents a robust and clinically meaningful approach for evaluating language models for ADE detection in clinical free text documents. The study highlights the need to use appropriate performance measures fit for the task and envisioned future clinical use.

Abstract: In this study, we set a benchmark for adverse drug event (ADE) detection in
Dutch clinical free text documents using several transformer models, clinical
scenarios and fit-for-purpose performance measures. We trained a Bidirectional
Long Short-Term Memory (Bi-LSTM) model and four transformer-based Dutch and/or
multilingual encoder models (BERTje, RobBERT, MedRoBERTa.nl, and NuNER) for the
tasks of named entity recognition (NER) and relation classification (RC) using
102 richly annotated Dutch ICU clinical progress notes. Anonymized free text
clinical progress notes of patients admitted to intensive care unit (ICU) of
one academic hospital and discharge letters of patients admitted to Internal
Medicine wards of two non-academic hospitals were reused. We evaluated our ADE
RC models internally using gold standard (two-step task) and predicted entities
(end-to-end task). In addition, all models were externally validated on
detecting ADEs at the document level. We report both micro- and macro-averaged
F1 scores, given the imbalance of ADEs in the datasets. Although differences
for the ADE RC task between the models were small, MedRoBERTa.nl was the best
performing model with macro-averaged F1 score of 0.63 using gold standard and
0.62 using predicted entities. The MedRoBERTa.nl models also performed the best
in our external validation and achieved recall of between 0.67 to 0.74 using
predicted entities, meaning between 67 to 74% of discharge letters with ADEs
were detected. Our benchmark study presents a robust and clinically meaningful
approach for evaluating language models for ADE detection in clinical free text
documents. Our study highlights the need to use appropriate performance
measures fit for the task of ADE detection in clinical free-text documents and
envisioned future clinical use.

</details>


### [33] [Towards Domain Specification of Embedding Models in Medicine](https://arxiv.org/abs/2507.19407)
*Mohammad Khodadad,Ali Shiraee,Mahdi Astaraki,Hamidreza Mahyar*

Main category: cs.CL

TL;DR: The paper introduces MEDTE, a new medical text embedding model, and a comprehensive benchmark suite, demonstrating that MEDTE outperforms existing models.


<details>
  <summary>Details</summary>
Motivation: Existing medical text embedding models are trained on a narrow slice of medical and biological data and existing evaluations are often inadequate.

Method: The authors leverage MEDTE, a GTE model extensively fine-tuned on diverse medical corpora through self-supervised contrastive learning across multiple data sources, to deliver robust medical text embeddings. They also propose a comprehensive benchmark suite of 51 tasks.

Result: The combined approach not only establishes a robust evaluation framework but also yields embeddings that consistently outperform state of the art alternatives in different tasks.

Conclusion: This paper introduces a new medical text embedding model (MEDTE) and a comprehensive benchmark suite of 51 tasks. The results demonstrate that MEDTE consistently outperforms state-of-the-art alternatives in different tasks.

Abstract: Medical text embedding models are foundational to a wide array of healthcare
applications, ranging from clinical decision support and biomedical information
retrieval to medical question answering, yet they remain hampered by two
critical shortcomings. First, most models are trained on a narrow slice of
medical and biological data, beside not being up to date in terms of
methodology, making them ill suited to capture the diversity of terminology and
semantics encountered in practice. Second, existing evaluations are often
inadequate: even widely used benchmarks fail to generalize across the full
spectrum of real world medical tasks.
  To address these gaps, we leverage MEDTE, a GTE model extensively fine-tuned
on diverse medical corpora through self-supervised contrastive learning across
multiple data sources, to deliver robust medical text embeddings.
  Alongside this model, we propose a comprehensive benchmark suite of 51 tasks
spanning classification, clustering, pair classification, and retrieval modeled
on the Massive Text Embedding Benchmark (MTEB) but tailored to the nuances of
medical text. Our results demonstrate that this combined approach not only
establishes a robust evaluation framework but also yields embeddings that
consistently outperform state of the art alternatives in different tasks.

</details>


### [34] [TokenSmith: Streamlining Data Editing, Search, and Inspection for Large-Scale Language Model Training and Interpretability](https://arxiv.org/abs/2507.19419)
*Mohammad Aflah Khan,Ameya Godbole,Johnny Tian-Zheng Wei,Ryan Wang,James Flemings,Krishna Gummadi,Willie Neiswanger,Robin Jia*

Main category: cs.CL

TL;DR: TokenSmith is an open-source library that simplifies the editing, inspection, and analysis of datasets used in large language model pretraining.


<details>
  <summary>Details</summary>
Motivation: Understanding the relationship between training data and model behavior during pretraining is crucial, but existing workflows make this process cumbersome, fragmented, and often inaccessible to researchers.

Method: TokenSmith, an open-source library for interactive editing, inspection, and analysis of datasets.

Result: TokenSmith supports a wide range of operations including searching, viewing, ingesting, exporting, inspecting, and sampling data, all accessible through a simple user interface and a modular backend. It also enables structured editing of pretraining data without requiring changes to training code, simplifying dataset debugging, validation, and experimentation.

Conclusion: TokenSmith democratizes access to production-grade dataset tooling for large language model pretraining workflows.

Abstract: Understanding the relationship between training data and model behavior
during pretraining is crucial, but existing workflows make this process
cumbersome, fragmented, and often inaccessible to researchers. We present
TokenSmith, an open-source library for interactive editing, inspection, and
analysis of datasets used in Megatron-style pretraining frameworks such as
GPT-NeoX, Megatron, and NVIDIA NeMo. TokenSmith supports a wide range of
operations including searching, viewing, ingesting, exporting, inspecting, and
sampling data, all accessible through a simple user interface and a modular
backend. It also enables structured editing of pretraining data without
requiring changes to training code, simplifying dataset debugging, validation,
and experimentation.
  TokenSmith is designed as a plug and play addition to existing large language
model pretraining workflows, thereby democratizing access to production-grade
dataset tooling. TokenSmith is hosted on GitHub1, with accompanying
documentation and tutorials. A demonstration video is also available on
YouTube.

</details>


### [35] [GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning](https://arxiv.org/abs/2507.19457)
*Lakshya A Agrawal,Shangyin Tan,Dilara Soylu,Noah Ziems,Rishi Khare,Krista Opsahl-Ong,Arnav Singhvi,Herumb Shandilya,Michael J Ryan,Meng Jiang,Christopher Potts,Koushik Sen,Alexandros G. Dimakis,Ion Stoica,Dan Klein,Matei Zaharia,Omar Khattab*

Main category: cs.CL

TL;DR: GEPA是一种新的prompt优化器，它使用自然语言反思从少量试验中学习，优于现有的强化学习方法和prompt优化器。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）越来越多地通过强化学习（RL）方法（如Group Relative Policy Optimization (GRPO)）来适应下游任务，但这些方法通常需要数千次rollout才能学习新任务。与来自稀疏标量奖励的策略梯度相比，语言的可解释性可以为LLM提供更丰富的学习媒介。

Method: 提出了一种名为GEPA（Genetic-Pareto）的prompt优化器，该优化器充分结合了自然语言反思，以从试错中学习高级规则。

Result: GEPA可以将少量rollout转化为大量质量提升。在四个任务中，GEPA的性能优于GRPO平均10%，最高可达20%，同时使用的rollout最多减少35倍。GEPA的性能也优于领先的prompt优化器MIPROv2超过10%，并证明了其作为代码优化推理时间搜索策略的良好结果。

Conclusion: GEPA在四个任务上的表现优于GRPO平均10%，最高达20%，同时减少了35倍的rollout。GEPA在两个LLM上的表现也优于领先的prompt优化器MIPROv2超过10%，并展示了作为代码优化推理时间搜索策略的有希望的结果。

Abstract: Large language models (LLMs) are increasingly adapted to downstream tasks via
reinforcement learning (RL) methods like Group Relative Policy Optimization
(GRPO), which often require thousands of rollouts to learn new tasks. We argue
that the interpretable nature of language can often provide a much richer
learning medium for LLMs, compared with policy gradients derived from sparse,
scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt
optimizer that thoroughly incorporates natural language reflection to learn
high-level rules from trial and error. Given any AI system containing one or
more LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool
calls, and tool outputs) and reflects on them in natural language to diagnose
problems, propose and test prompt updates, and combine complementary lessons
from the Pareto frontier of its own attempts. As a result of GEPA's design, it
can often turn even just a few rollouts into a large quality gain. Across four
tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up
to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer,
MIPROv2, by over 10% across two LLMs, and demonstrates promising results as an
inference-time search strategy for code optimization.

</details>


### [36] [Conversations Gone Awry, But Then? Evaluating Conversational Forecasting Models](https://arxiv.org/abs/2507.19470)
*Son Quoc Tran,Tushaar Gangavarapu,Nicholas Chernogor,Jonathan P. Chang,Cristian Danescu-Niculescu-Mizil*

Main category: cs.CL

TL;DR: The paper introduces a new evaluation framework and metric for predicting when conversations will go awry, providing a benchmark for comparing different models.


<details>
  <summary>Details</summary>
Motivation: Endowing automated systems with foresight can enable them to assist human-human interactions. Recent work has focused on the Conversations Gone Awry (CGA) task: forecasting whether an ongoing conversation will derail.

Method: The paper introduces a uniform evaluation framework and a novel metric that captures a model's ability to revise its forecast as the conversation progresses.

Result: The paper creates a benchmark that enables direct and reliable comparisons between different architectures. This allows us to present an up-to-date overview of the current progress in CGA models, in light of recent advancements in language modeling.

Conclusion: The paper introduces a uniform evaluation framework for the Conversations Gone Awry (CGA) task, creating a benchmark for direct comparisons between different architectures and providing an up-to-date overview of current progress in CGA models.

Abstract: We often rely on our intuition to anticipate the direction of a conversation.
Endowing automated systems with similar foresight can enable them to assist
human-human interactions. Recent work on developing models with this predictive
capacity has focused on the Conversations Gone Awry (CGA) task: forecasting
whether an ongoing conversation will derail. In this work, we revisit this task
and introduce the first uniform evaluation framework, creating a benchmark that
enables direct and reliable comparisons between different architectures. This
allows us to present an up-to-date overview of the current progress in CGA
models, in light of recent advancements in language modeling. Our framework
also introduces a novel metric that captures a model's ability to revise its
forecast as the conversation progresses.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [37] [Quantum-Cognitive Tunnelling Neural Networks for Military-Civilian Vehicle Classification and Sentiment Analysis](https://arxiv.org/abs/2507.18645)
*Milan Maksimovic,Anna Bohdanets,Immaculate Motsi-Omoijiade,Guido Governatori,Ivan S. Maksymov*

Main category: cs.CV

TL;DR: This paper uses Quantum Tunnelling based Neural Networks to classify military and civilian vehicles and sentiment analysis, suggesting it can improve AI in battlefield scenarios.


<details>
  <summary>Details</summary>
Motivation: incorporating well-known quantum tunnelling (QT) probability into neural network models effectively captures important nuances of human perception, particularly in the recognition of ambiguous objects and sentiment analysis.

Method: employ novel QT-based neural networks

Result: assess their effectiveness in distinguishing customised CIFAR-format images of military and civilian vehicles, as well as sentiment, using a proprietary military-specific vocabulary.

Conclusion: QT-based models can enhance multimodal AI applications in battlefield scenarios, particularly within human-operated drone warfare contexts, imbuing AI with certain traits of human reasoning.

Abstract: Prior work has demonstrated that incorporating well-known quantum tunnelling
(QT) probability into neural network models effectively captures important
nuances of human perception, particularly in the recognition of ambiguous
objects and sentiment analysis. In this paper, we employ novel QT-based neural
networks and assess their effectiveness in distinguishing customised
CIFAR-format images of military and civilian vehicles, as well as sentiment,
using a proprietary military-specific vocabulary. We suggest that QT-based
models can enhance multimodal AI applications in battlefield scenarios,
particularly within human-operated drone warfare contexts, imbuing AI with
certain traits of human reasoning.

</details>


### [38] [Livatar-1: Real-Time Talking Heads Generation with Tailored Flow Matching](https://arxiv.org/abs/2507.18649)
*Haiyang Liu,Xiaolin Hong,Xuancheng Yang,Yudi Ruan,Xiang Lian,Michael Lingelbach,Hongwei Yi,Wei Li*

Main category: cs.CV

TL;DR: Livatar is a real-time audio-driven talking heads videos generation framework that addresses the limitations of existing baselines with a flow matching based framework. It achieves competitive lip-sync quality and high throughput with low latency.


<details>
  <summary>Details</summary>
Motivation: Existing talking head video generation baselines suffer from limited lip-sync accuracy and long-term pose drift.

Method: a flow matching based framework

Result: Livatar achieves competitive lip-sync quality with a 8.50 LipSync Confidence on the HDTF dataset, and reaches a throughput of 141 FPS with an end-to-end latency of 0.17s on a single A10 GPU.

Conclusion: Livatar makes high-fidelity avatars accessible to broader applications.

Abstract: We present Livatar, a real-time audio-driven talking heads videos generation
framework. Existing baselines suffer from limited lip-sync accuracy and
long-term pose drift. We address these limitations with a flow matching based
framework. Coupled with system optimizations, Livatar achieves competitive
lip-sync quality with a 8.50 LipSync Confidence on the HDTF dataset, and
reaches a throughput of 141 FPS with an end-to-end latency of 0.17s on a single
A10 GPU. This makes high-fidelity avatars accessible to broader applications.
Our project is available at https://www.hedra.com/ with with examples at
https://h-liu1997.github.io/Livatar-1/

</details>


### [39] [Features extraction for image identification using computer vision](https://arxiv.org/abs/2507.18650)
*Venant Niyonkuru,Sylla Sekou,Jimmy Jackson Sinzinkayo*

Main category: cs.CV

TL;DR: 本研究对比了ViT和其他特征提取方法在计算机视觉中的应用，强调了ViT的架构及其优于传统CNN的性能。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在检验计算机视觉中各种特征提取技术。

Method: 研究方法包括视觉Transformer (ViT)、生成对抗网络(GANs)、深度特征模型、传统方法(SIFT、SURF、ORB)以及非对比和对比特征模型。

Result: ViT在性能上优于传统的卷积神经网络(CNN)，报告总结了ViT的架构，包括patch embedding、位置编码和多头自注意力机制。

Conclusion: 实验结果确定了ViT和其他方法的优点、局限性以及它们在推进计算机视觉方面的应用。

Abstract: This study examines various feature extraction techniques in computer vision,
the primary focus of which is on Vision Transformers (ViTs) and other
approaches such as Generative Adversarial Networks (GANs), deep feature models,
traditional approaches (SIFT, SURF, ORB), and non-contrastive and contrastive
feature models. Emphasizing ViTs, the report summarizes their architecture,
including patch embedding, positional encoding, and multi-head self-attention
mechanisms with which they overperform conventional convolutional neural
networks (CNNs). Experimental results determine the merits and limitations of
both methods and their utilitarian applications in advancing computer vision.

</details>


### [40] [Adapt, But Don't Forget: Fine-Tuning and Contrastive Routing for Lane Detection under Distribution Shift](https://arxiv.org/abs/2507.18653)
*Mohammed Abdul Hafeez Khan,Parth Ganeriwala,Sarah M. Lehman,Siddhartha Bhattacharyya,Amy Alvarez,Natasha Neogi*

Main category: cs.CV

TL;DR: Addresses catastrophic forgetting in lane detection models by adapting a base model to new target distributions using separate branches and supervised contrastive learning for dynamic routing.


<details>
  <summary>Details</summary>
Motivation: cross-dataset distribution shifts can cause severe catastrophic forgetting during fine-tuning.

Method: train a base model on a source distribution and then adapt it to each new target distribution by creating separate branches, fine-tuning only selected components while keeping the original source branch fixed. Based on a component-wise analysis, we identify effective fine-tuning strategies for target distributions that enable parameter-efficient adaptation. At inference time, we propose using a supervised contrastive learning model to identify the input distribution and dynamically route it to the corresponding branch.

Result: effective fine-tuning strategies for target distributions that enable parameter-efficient adaptation

Conclusion: This framework achieves near-optimal F1-scores while using significantly fewer parameters than training separate models for each distribution.

Abstract: Lane detection models are often evaluated in a closed-world setting, where
training and testing occur on the same dataset. We observe that, even within
the same domain, cross-dataset distribution shifts can cause severe
catastrophic forgetting during fine-tuning. To address this, we first train a
base model on a source distribution and then adapt it to each new target
distribution by creating separate branches, fine-tuning only selected
components while keeping the original source branch fixed. Based on a
component-wise analysis, we identify effective fine-tuning strategies for
target distributions that enable parameter-efficient adaptation. At inference
time, we propose using a supervised contrastive learning model to identify the
input distribution and dynamically route it to the corresponding branch. Our
framework achieves near-optimal F1-scores while using significantly fewer
parameters than training separate models for each distribution.

</details>


### [41] [Part Segmentation of Human Meshes via Multi-View Human Parsing](https://arxiv.org/abs/2507.18655)
*James Dickens,Kamyar Hamad*

Main category: cs.CV

TL;DR: enable per-vertex semantic segmentation of large-scale human meshes


<details>
  <summary>Details</summary>
Motivation: This work aims to bridge these two domains by enabling per-vertex semantic segmentation of large-scale human meshes.

Method: a pseudo-ground truth labeling pipeline is developed for the Thuman2.1 dataset: meshes are first aligned to a canonical pose, segmented from multiple viewpoints, and the resulting point-level labels are then backprojected onto the original mesh to produce per-point pseudo ground truth annotations. Subsequently, a novel, memory-efficient sampling strategy is introduced, a windowed iterative farthest point sampling (FPS) with space-filling curve-based serialization to effectively downsample the point clouds. This is followed by a purely geometric segmentation using PointTransformer

Result: achieve high per-part labeling accuracy on large-scale point clouds, using only the raw geometry of unordered point sets.

Conclusion: Experimental results confirm the effectiveness and accuracy of the proposed approach.

Abstract: Recent advances in point cloud deep learning have led to models that achieve
high per-part labeling accuracy on large-scale point clouds, using only the raw
geometry of unordered point sets. In parallel, the field of human parsing
focuses on predicting body part and clothing/accessory labels from images. This
work aims to bridge these two domains by enabling per-vertex semantic
segmentation of large-scale human meshes. To achieve this, a pseudo-ground
truth labeling pipeline is developed for the Thuman2.1 dataset: meshes are
first aligned to a canonical pose, segmented from multiple viewpoints, and the
resulting point-level labels are then backprojected onto the original mesh to
produce per-point pseudo ground truth annotations. Subsequently, a novel,
memory-efficient sampling strategy is introduced, a windowed iterative farthest
point sampling (FPS) with space-filling curve-based serialization to
effectively downsample the point clouds. This is followed by a purely geometric
segmentation using PointTransformer, enabling semantic parsing of human meshes
without relying on texture information. Experimental results confirm the
effectiveness and accuracy of the proposed approach.

</details>


### [42] [ShrinkBox: Backdoor Attack on Object Detection to Disrupt Collision Avoidance in Machine Learning-based Advanced Driver Assistance Systems](https://arxiv.org/abs/2507.18656)
*Muhammad Zaeem Shahzad,Muhammad Abdullah Hanif,Bassem Ouni,Muhammad Shafique*

Main category: cs.CV

TL;DR: This paper introduces ShrinkBox, a novel backdoor attack on object detection in ML-ADAS that subtly shrinks bounding boxes, disrupting distance estimation and potentially preventing collision warnings.


<details>
  <summary>Details</summary>
Motivation: Critical to ML-ADAS is the collision avoidance feature, which requires the ability to detect objects and estimate their distances accurately. However, the robustness of these systems is undermined by security vulnerabilities in object detectors.

Method: a novel backdoor attack targeting object detection in collision avoidance ML-ADAS. Unlike existing attacks that manipulate object class labels or presence, ShrinkBox subtly shrinks ground truth bounding boxes.

Result: ShrinkBox can be realized in the YOLOv9m object detector at an Attack Success Rate (ASR) of 96%, with only a 4% poisoning ratio in the training instances of the KITTI dataset.

Conclusion: ShrinkBox increases the Mean Absolute Error (MAE) in downstream distance estimation by more than 3x on poisoned samples, potentially resulting in delays or prevention of collision warnings altogether.

Abstract: Advanced Driver Assistance Systems (ADAS) significantly enhance road safety
by detecting potential collisions and alerting drivers. However, their reliance
on expensive sensor technologies such as LiDAR and radar limits accessibility,
particularly in low- and middle-income countries. Machine learning-based ADAS
(ML-ADAS), leveraging deep neural networks (DNNs) with only standard camera
input, offers a cost-effective alternative. Critical to ML-ADAS is the
collision avoidance feature, which requires the ability to detect objects and
estimate their distances accurately. This is achieved with specialized DNNs
like YOLO, which provides real-time object detection, and a lightweight,
detection-wise distance estimation approach that relies on key features
extracted from the detections like bounding box dimensions and size. However,
the robustness of these systems is undermined by security vulnerabilities in
object detectors. In this paper, we introduce ShrinkBox, a novel backdoor
attack targeting object detection in collision avoidance ML-ADAS. Unlike
existing attacks that manipulate object class labels or presence, ShrinkBox
subtly shrinks ground truth bounding boxes. This attack remains undetected in
dataset inspections and standard benchmarks while severely disrupting
downstream distance estimation. We demonstrate that ShrinkBox can be realized
in the YOLOv9m object detector at an Attack Success Rate (ASR) of 96%, with
only a 4% poisoning ratio in the training instances of the KITTI dataset.
Furthermore, given the low error targets introduced in our relaxed poisoning
strategy, we find that ShrinkBox increases the Mean Absolute Error (MAE) in
downstream distance estimation by more than 3x on poisoned samples, potentially
resulting in delays or prevention of collision warnings altogether.

</details>


### [43] [VGS-ATD: Robust Distributed Learning for Multi-Label Medical Image Classification Under Heterogeneous and Imbalanced Conditions](https://arxiv.org/abs/2507.18657)
*Zehui Zhao,Laith Alzubaidi,Haider A. Alwzwazy,Jinglan Zhang,Yuantong Gu*

Main category: cs.CV

TL;DR: 提出了 VGS-ATD，它是一种新颖的分布式学习框架，优于集中式学习、集群学习和联邦学习。


<details>
  <summary>Details</summary>
Motivation: 传统的集中式学习范例存在严重隐私风险，因为所有数据都在单个服务器上收集和训练。虽然这些方法增强了隐私，但它们难以处理异构和不平衡数据，并且由于频繁的通信和权重聚合而效率低下。更重要的是，临床环境的动态和复杂性质需要能够不断从不同模式和多标签中学习的可扩展 AI 系统。然而，集中式和分散式模型都容易在系统扩展过程中发生灾难性遗忘，通常需要完全重新训练模型以整合新数据。

Method: 我们提出了 VGS-ATD，这是一种新颖的分布式学习框架。

Result: VGS-ATD 实现了 92.7% 的总体准确率，优于集中式学习 (84.9%) 和集群学习 (72.99%)，而联邦学习由于对计算资源的高要求而未能满足这些条件。VGS-ATD 还表现出强大的可扩展性

Conclusion: VGS-ATD 在扩展后，现有节点的准确率仅下降 1%，而集中式学习的准确率下降 20%，这突显了其对灾难性遗忘的抵御能力。此外，相对于集中式学习和集群学习，它还降低了高达 50% 的计算成本，证实了其卓越的效率和可扩展性。

Abstract: In recent years, advanced deep learning architectures have shown strong
performance in medical imaging tasks. However, the traditional centralized
learning paradigm poses serious privacy risks as all data is collected and
trained on a single server. To mitigate this challenge, decentralized
approaches such as federated learning and swarm learning have emerged, allowing
model training on local nodes while sharing only model weights. While these
methods enhance privacy, they struggle with heterogeneous and imbalanced data
and suffer from inefficiencies due to frequent communication and the
aggregation of weights. More critically, the dynamic and complex nature of
clinical environments demands scalable AI systems capable of continuously
learning from diverse modalities and multilabels. Yet, both centralized and
decentralized models are prone to catastrophic forgetting during system
expansion, often requiring full model retraining to incorporate new data. To
address these limitations, we propose VGS-ATD, a novel distributed learning
framework. To validate VGS-ATD, we evaluate it in experiments spanning 30
datasets and 80 independent labels across distributed nodes, VGS-ATD achieved
an overall accuracy of 92.7%, outperforming centralized learning (84.9%) and
swarm learning (72.99%), while federated learning failed under these conditions
due to high requirements on computational resources. VGS-ATD also demonstrated
strong scalability, with only a 1% drop in accuracy on existing nodes after
expansion, compared to a 20% drop in centralized learning, highlighting its
resilience to catastrophic forgetting. Additionally, it reduced computational
costs by up to 50% relative to both centralized and swarm learning, confirming
its superior efficiency and scalability.

</details>


### [44] [Fuzzy Theory in Computer Vision: A Review](https://arxiv.org/abs/2507.18660)
*Adilet Yerkin,Ayan Igali,Elnara Kadyrgali,Maksat Shagyrov,Malika Ziyada,Muragul Muratbekova,Pakizar Shamoi*

Main category: cs.CV

TL;DR: Fuzzy logic improves computer vision by handling uncertainty and can be integrated with deep learning for better performance.


<details>
  <summary>Details</summary>
Motivation: The paper explores the use of fuzzy logic in computer vision to handle uncertainty, noise, and imprecision in image data, offering a promising approach to improve object recognition, image segmentation, and feature extraction.

Method: Key fuzzy techniques like fuzzy clustering, fuzzy inference systems, type-2 fuzzy sets, and fuzzy rule-based decision-making are discussed. Integration of fuzzy logic with CNNs is explored.

Result: Fuzzy approaches improve object recognition, image segmentation, and feature extraction. Applications include medical imaging, autonomous systems, and industrial inspection. Hybrid fuzzy-deep learning models show emerging trends.

Conclusion: Fuzzy logic enhances computer vision tasks by offering adaptable and interpretable solutions, especially when integrated with deep learning for improved performance and explainability.

Abstract: Computer vision applications are omnipresent nowadays. The current paper
explores the use of fuzzy logic in computer vision, stressing its role in
handling uncertainty, noise, and imprecision in image data. Fuzzy logic is able
to model gradual transitions and human-like reasoning and provides a promising
approach to computer vision. Fuzzy approaches offer a way to improve object
recognition, image segmentation, and feature extraction by providing more
adaptable and interpretable solutions compared to traditional methods. We
discuss key fuzzy techniques, including fuzzy clustering, fuzzy inference
systems, type-2 fuzzy sets, and fuzzy rule-based decision-making. The paper
also discusses various applications, including medical imaging, autonomous
systems, and industrial inspection. Additionally, we explore the integration of
fuzzy logic with deep learning models such as convolutional neural networks
(CNNs) to enhance performance in complex vision tasks. Finally, we examine
emerging trends such as hybrid fuzzy-deep learning models and explainable AI.

</details>


### [45] [Eyes Will Shut: A Vision-Based Next GPS Location Prediction Model by Reinforcement Learning from Visual Map Feed Back](https://arxiv.org/abs/2507.18661)
*Ruixing Zhang,Yang Zhang,Tongyu Zhu,Leilei Sun,Weifeng Lv*

Main category: cs.CV

TL;DR: VLMLocPredictor uses VLMs to predict next locations in a human-like manner, achieving SOTA performance and superior cross-city generalization.


<details>
  <summary>Details</summary>
Motivation: Existing next-location prediction models do not reason over maps in the way that humans do. VLMs have demonstrated strong capabilities in visual perception and visual reasoning.

Method: Vision-Guided Location Search (VGLS) to evaluate VLM's trajectory-based reasoning; VLMLocPredictor with two stages: Supervised Fine-Tuning (SFT) and Reinforcement Learning from Visual Map Feedback.

Result: Achieves state-of-the-art (SOTA) performance and exhibits superior cross-city generalization compared to other LLM-based approaches on datasets from four different cities.

Conclusion: The proposed VLMLocPredictor achieves state-of-the-art performance and exhibits superior cross-city generalization compared to other LLM-based approaches.

Abstract: Next Location Prediction is a fundamental task in the study of human
mobility, with wide-ranging applications in transportation planning, urban
governance, and epidemic forecasting. In practice, when humans attempt to
predict the next location in a trajectory, they often visualize the trajectory
on a map and reason based on road connectivity and movement trends. However,
the vast majority of existing next-location prediction models do not reason
over maps \textbf{in the way that humans do}. Fortunately, the recent
development of Vision-Language Models (VLMs) has demonstrated strong
capabilities in visual perception and even visual reasoning. This opens up a
new possibility: by rendering both the road network and trajectory onto an
image and leveraging the reasoning abilities of VLMs, we can enable models to
perform trajectory inference in a human-like manner. To explore this idea, we
first propose a method called Vision-Guided Location Search (VGLS), which
evaluates whether a general-purpose VLM is capable of trajectory-based
reasoning without modifying any of its internal parameters. Based on insights
from the VGLS results, we further propose our main approach: VLMLocPredictor,
which is composed of two stages: In the first stage, we design two Supervised
Fine-Tuning (SFT) tasks that help the VLM understand road network and
trajectory structures and acquire basic reasoning ability on such visual
inputs. In the second stage, we introduce Reinforcement Learning from Visual
Map Feedback, enabling the model to self-improve its next-location prediction
ability through interaction with the environment. Experiments conducted on
datasets from four different cities show that our method achieves
state-of-the-art (SOTA) performance and exhibits superior cross-city
generalization compared to other LLM-based approaches.

</details>


### [46] [Gen-AI Police Sketches with Stable Diffusion](https://arxiv.org/abs/2507.18667)
*Nicholas Fidalgo,Aaron Contreras,Katherine Harvey,Johnny Ni*

Main category: cs.CV

TL;DR: The project explores AI suspect sketching using Stable Diffusion, CLIP, and LoRA. Model 1, the baseline, performed best in structural similarity and clarity of facial features.


<details>
  <summary>Details</summary>
Motivation: This project investigates the use of multimodal AI-driven approaches to automate and enhance suspect sketching.

Method: Three pipelines were developed and evaluated: (1) baseline image-to-image Stable Diffusion model, (2) same model integrated with a pre-trained CLIP model for text-image alignment, and (3) novel approach incorporating LoRA fine-tuning of the CLIP model, applied to self-attention and cross-attention layers, and integrated with Stable Diffusion.

Result: Model 1 achieved the highest structural similarity (SSIM) of 0.72 and a peak signal-to-noise ratio (PSNR) of 25 dB, outperforming Model 2 and Model 3. Iterative refinement enhanced perceptual similarity (LPIPS), with Model 3 showing improvement over Model 2 but still trailing Model 1.

Conclusion: Model 1's sketches had the clearest facial features, making it a robust baseline.

Abstract: This project investigates the use of multimodal AI-driven approaches to
automate and enhance suspect sketching. Three pipelines were developed and
evaluated: (1) baseline image-to-image Stable Diffusion model, (2) same model
integrated with a pre-trained CLIP model for text-image alignment, and (3)
novel approach incorporating LoRA fine-tuning of the CLIP model, applied to
self-attention and cross-attention layers, and integrated with Stable
Diffusion. An ablation study confirmed that fine-tuning both self- and
cross-attention layers yielded the best alignment between text descriptions and
sketches. Performance testing revealed that Model 1 achieved the highest
structural similarity (SSIM) of 0.72 and a peak signal-to-noise ratio (PSNR) of
25 dB, outperforming Model 2 and Model 3. Iterative refinement enhanced
perceptual similarity (LPIPS), with Model 3 showing improvement over Model 2
but still trailing Model 1. Qualitatively, sketches generated by Model 1
demonstrated the clearest facial features, highlighting its robustness as a
baseline despite its simplicity.

</details>


### [47] [Closing the Modality Gap for Mixed Modality Search](https://arxiv.org/abs/2507.19054)
*Binxu Li,Yuhui Zhang,Xiaohan Wang,Weixin Liang,Ludwig Schmidt,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: This paper proposes GR-CLIP, a method to remove the modality gap in CLIP's embedding space, improving mixed modality search performance.


<details>
  <summary>Details</summary>
Motivation: contrastive vision-language models, such as CLIP, exhibit a pronounced modality gap in the embedding space, where image and text embeddings form distinct clusters, leading to intra-modal ranking bias and inter-modal fusion failure

Method: propose GR-CLIP, a lightweight post-hoc calibration method that removes the modality gap in CLIP's embedding space

Result: Our analysis reveals a critical limitation: these models exhibit a pronounced modality gap in the embedding space, where image and text embeddings form distinct clusters, leading to intra-modal ranking bias and inter-modal fusion failure.

Conclusion: GR-CLIP improves NDCG@10 by up to 26 percentage points over CLIP, surpasses recent vision-language generative embedding models by 4 percentage points, while using 75x less compute.

Abstract: Mixed modality search -- retrieving information across a heterogeneous corpus
composed of images, texts, and multimodal documents -- is an important yet
underexplored real-world application. In this work, we investigate how
contrastive vision-language models, such as CLIP, perform on the mixed modality
search task. Our analysis reveals a critical limitation: these models exhibit a
pronounced modality gap in the embedding space, where image and text embeddings
form distinct clusters, leading to intra-modal ranking bias and inter-modal
fusion failure. To address this issue, we propose GR-CLIP, a lightweight
post-hoc calibration method that removes the modality gap in CLIP's embedding
space. Evaluated on MixBench -- the first benchmark specifically designed for
mixed modality search -- GR-CLIP improves NDCG@10 by up to 26 percentage points
over CLIP, surpasses recent vision-language generative embedding models by 4
percentage points, while using 75x less compute.

</details>


### [48] [Advancing Vision-based Human Action Recognition: Exploring Vision-Language CLIP Model for Generalisation in Domain-Independent Tasks](https://arxiv.org/abs/2507.18675)
*Sanyam Jain,Marsha Mariya Kappan,Vijeta Sharma*

Main category: cs.CV

TL;DR: 这篇论文评估了CLIP在动作识别中的性能，发现它在处理遮挡时存在问题。研究人员提出了一种通过添加类特定噪声来改进CLIP的方法，以提高其准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 人类动作识别在医疗保健和医学中起着关键作用，支持患者行为监测、跌倒检测、手术机器人监督和程序技能评估等应用。传统的CNN和RNN模型已经取得了一定的成功，但它们往往难以推广到多样化和复杂的动作中。视觉-语言模型的最新进展，特别是基于Transformer的CLIP模型，为从视频数据中推广动作识别提供了有希望的能力。

Method: 研究人员在UCF-101数据集上评估CLIP，并系统地分析了其在三种掩蔽策略下的性能：(1)基于百分比和形状的黑色掩蔽，比例为10%、30%和50%；(2)特征特定的掩蔽，以抑制诱导偏差的元素；(3)隔离掩蔽，仅保留类特定的区域。为了克服CLIP的局限性，研究人员提出了一种结合类特定噪声的方法，通过自定义损失函数学习，以加强对类定义特征的关注。

Result: CLIP在基本视觉线索被遮挡时表现出不一致的行为和频繁的错误分类。通过结合类特定的噪声，研究人员提出的方法提高了分类准确率和模型置信度，同时减少了偏差。

Conclusion: CLIP模型在处理被遮挡的关键视觉线索时，在动作识别方面表现出不一致的行为和频繁的错误分类。为了克服这些限制，研究人员提出了一种结合类特定噪声的方法，通过自定义损失函数学习，以加强对类定义特征的关注，从而提高分类准确率和模型置信度，同时减少偏差。最后，讨论了在临床领域应用此类模型所面临的挑战，并概述了未来工作的方向，以提高跨领域独立医疗保健场景的泛化能力。

Abstract: Human action recognition plays a critical role in healthcare and medicine,
supporting applications such as patient behavior monitoring, fall detection,
surgical robot supervision, and procedural skill assessment. While traditional
models like CNNs and RNNs have achieved moderate success, they often struggle
to generalize across diverse and complex actions. Recent advancements in
vision-language models, especially the transformer-based CLIP model, offer
promising capabilities for generalizing action recognition from video data. In
this work, we evaluate CLIP on the UCF-101 dataset and systematically analyze
its performance under three masking strategies: (1) percentage-based and
shape-based black masking at 10%, 30%, and 50%, (2) feature-specific masking to
suppress bias-inducing elements, and (3) isolation masking that retains only
class-specific regions. Our results reveal that CLIP exhibits inconsistent
behavior and frequent misclassifications, particularly when essential visual
cues are obscured. To overcome these limitations, we propose incorporating
class-specific noise, learned via a custom loss function, to reinforce
attention to class-defining features. This enhancement improves classification
accuracy and model confidence while reducing bias. We conclude with a
discussion on the challenges of applying such models in clinical domains and
outline directions for future work to improve generalizability across
domain-independent healthcare scenarios.

</details>


### [49] [HeartUnloadNet: A Weakly-Supervised Cycle-Consistent Graph Network for Predicting Unloaded Cardiac Geometry from Diastolic States](https://arxiv.org/abs/2507.18677)
*Siyu Mu,Wei Xuan Chan,Choon Hwai Yap*

Main category: cs.CV

TL;DR: HeartUnloadNet, a deep learning model, accurately and rapidly predicts unloaded heart geometry, outperforming traditional methods and enabling real-time clinical use.


<details>
  <summary>Details</summary>
Motivation: Estimating unloaded cardiac geometry from clinical images is challenging and computationally expensive with traditional inverse FE solvers.

Method: A deep learning framework, HeartUnloadNet, predicts unloaded LV shape from ED mesh using a graph attention architecture and cycle-consistency strategy.

Result: HeartUnloadNet achieves sub-millimeter accuracy with DSC of 0.986 and HD of 0.083 cm, reducing inference time to 0.02 seconds, significantly faster and more accurate than traditional methods. Cycle-consistency maintains 97% DSC with only 200 training samples.

Conclusion: HeartUnloadNet is a fast and accurate surrogate for inverse FE solvers, enabling real-time clinical applications.

Abstract: The unloaded cardiac geometry (i.e., the state of the heart devoid of luminal
pressure) serves as a valuable zero-stress and zero-strain reference and is
critical for personalized biomechanical modeling of cardiac function, to
understand both healthy and diseased physiology and to predict the effects of
cardiac interventions. However, estimating the unloaded geometry from clinical
images remains a challenging task. Traditional approaches rely on inverse
finite element (FE) solvers that require iterative optimization and are
computationally expensive. In this work, we introduce HeartUnloadNet, a deep
learning framework that predicts the unloaded left ventricular (LV) shape
directly from the end diastolic (ED) mesh while explicitly incorporating
biophysical priors. The network accepts a mesh of arbitrary size along with
physiological parameters such as ED pressure, myocardial stiffness scale, and
fiber helix orientation, and outputs the corresponding unloaded mesh. It adopts
a graph attention architecture and employs a cycle-consistency strategy to
enable bidirectional (loading and unloading) prediction, allowing for partial
self-supervision that improves accuracy and reduces the need for large training
datasets. Trained and tested on 20,700 FE simulations across diverse LV
geometries and physiological conditions, HeartUnloadNet achieves sub-millimeter
accuracy, with an average DSC of 0.986 and HD of 0.083 cm, while reducing
inference time to just 0.02 seconds per case, over 10^5 times faster and
significantly more accurate than traditional inverse FE solvers. Ablation
studies confirm the effectiveness of the architecture. Notably, the
cycle-consistent design enables the model to maintain a DSC of 97% even with as
few as 200 training samples. This work thus presents a scalable and accurate
surrogate for inverse FE solvers, supporting real-time clinical applications in
the future.

</details>


### [50] [Towards Scalable Spatial Intelligence via 2D-to-3D Data Lifting](https://arxiv.org/abs/2507.18678)
*Xingyu Miao,Haoran Duan,Quanhao Qian,Jiuniu Wang,Yang Long,Ling Shao,Deli Zhao,Ran Xu,Gongjie Zhang*

Main category: cs.CV

TL;DR: 提出了一种可扩展的流程，可以将单视图图像转换为全面的、尺度和外观逼真的 3D 表示，从而显着降低数据收集成本，并为推进空间智能开辟新途径。


<details>
  <summary>Details</summary>
Motivation: 空间智能正在成为人工智能领域的一个变革性前沿，但它仍然受到大规模 3D 数据集稀缺的限制。与丰富的 2D 图像不同，获取 3D 数据通常需要专门的传感器和费力的人工标注。

Method: 将单视图图像转换为全面的、尺度和外观逼真的 3D 表示——包括点云、相机姿势、深度图和伪 RGBD——通过集成深度估计、相机校准和尺度校准。

Result: 我们发布了两个生成的空间数据集，即 COCO-3D 和 Objects365-v2-3D。

Conclusion: 通过大量实验证明，生成的数据可以促进各种 3D 任务，从基本感知到基于 MLLM 的推理。这些结果验证了该流程是开发能够感知、理解和与物理环境交互的 AI 系统的有效解决方案。

Abstract: Spatial intelligence is emerging as a transformative frontier in AI, yet it
remains constrained by the scarcity of large-scale 3D datasets. Unlike the
abundant 2D imagery, acquiring 3D data typically requires specialized sensors
and laborious annotation. In this work, we present a scalable pipeline that
converts single-view images into comprehensive, scale- and appearance-realistic
3D representations - including point clouds, camera poses, depth maps, and
pseudo-RGBD - via integrated depth estimation, camera calibration, and scale
calibration. Our method bridges the gap between the vast repository of imagery
and the increasing demand for spatial scene understanding. By automatically
generating authentic, scale-aware 3D data from images, we significantly reduce
data collection costs and open new avenues for advancing spatial intelligence.
We release two generated spatial datasets, i.e., COCO-3D and Objects365-v2-3D,
and demonstrate through extensive experiments that our generated data can
benefit various 3D tasks, ranging from fundamental perception to MLLM-based
reasoning. These results validate our pipeline as an effective solution for
developing AI systems capable of perceiving, understanding, and interacting
with physical environments.

</details>


### [51] [SaLF: Sparse Local Fields for Multi-Sensor Rendering in Real-Time](https://arxiv.org/abs/2507.18713)
*Yun Chen,Matthew Haines,Jingkang Wang,Krzysztof Baron-Lis,Sivabalan Manivasagam,Ze Yang,Raquel Urtasun*

Main category: cs.CV

TL;DR: 提出了一种新的体素表示方法SaLF，它比NeRF和3DGS更快、更通用，并且可以实现更可扩展的传感器模拟。


<details>
  <summary>Details</summary>
Motivation: 基于神经辐射场（NeRF）的方法通过隐式表示的射线投射重建传感器观测，已经证明了驾驶场景的精确模拟，但是训练和渲染速度慢，阻碍了规模化。3D高斯溅射（3DGS）通过光栅化展示了更快的训练和渲染时间，但是主要限于针孔相机传感器，从而阻止了用于实际的多传感器自主评估。

Method: 提出了一种新的体积表示方法，称为稀疏局部场（SaLF），它支持光栅化和光线追踪。SaLF将体积表示为一组稀疏的3D体素图元，其中每个体素都是一个局部隐式场。

Result: SaLF具有快速训练（<30分钟）和渲染能力（相机50+ FPS和LiDAR 600+ FPS），具有自适应修剪和密集化功能，可以轻松处理大型场景，并且可以支持非针孔相机和旋转LiDAR。

Conclusion: SaLF在保持与现有自动驾驶传感器模拟方法相似的真实感的同时，提高了效率并增强了功能，从而实现了更具可扩展性的模拟。

Abstract: High-fidelity sensor simulation of light-based sensors such as cameras and
LiDARs is critical for safe and accurate autonomy testing. Neural radiance
field (NeRF)-based methods that reconstruct sensor observations via ray-casting
of implicit representations have demonstrated accurate simulation of driving
scenes, but are slow to train and render, hampering scale. 3D Gaussian
Splatting (3DGS) has demonstrated faster training and rendering times through
rasterization, but is primarily restricted to pinhole camera sensors,
preventing usage for realistic multi-sensor autonomy evaluation. Moreover, both
NeRF and 3DGS couple the representation with the rendering procedure (implicit
networks for ray-based evaluation, particles for rasterization), preventing
interoperability, which is key for general usage. In this work, we present
Sparse Local Fields (SaLF), a novel volumetric representation that supports
rasterization and raytracing. SaLF represents volumes as a sparse set of 3D
voxel primitives, where each voxel is a local implicit field. SaLF has fast
training (<30 min) and rendering capabilities (50+ FPS for camera and 600+ FPS
LiDAR), has adaptive pruning and densification to easily handle large scenes,
and can support non-pinhole cameras and spinning LiDARs. We demonstrate that
SaLF has similar realism as existing self-driving sensor simulation methods
while improving efficiency and enhancing capabilities, enabling more scalable
simulation. https://waabi.ai/salf/

</details>


### [52] [KuiSCIMA v2.0: Improved Baselines, Calibration, and Cross-Notation Generalization for Historical Chinese Music Notations in Jiang Kui's Baishidaoren Gequ](https://arxiv.org/abs/2507.18741)
*Tristan Repolusk,Eduardo Veas*

Main category: cs.CV

TL;DR: 该研究提升了对《白石道人歌曲》中苏字谱和律吕谱的光学音乐识别(OMR)性能，超越了人工转录。


<details>
  <summary>Details</summary>
Motivation: 历史中文乐谱（如苏字谱和律吕谱）的光学音乐识别(OMR)由于类别高度不平衡和训练数据有限而面临独特的挑战。

Method: 该研究采用字符识别模型，并使用温度缩放进行模型校准，同时使用留一版本交叉验证方法。

Result: 该研究将苏字谱的字符错误率(CER)从10.4%降低到7.1%，律吕谱的CER达到0.9%，优于人工转录的15.9%平均CER和7.6%最佳CER。校准模型的预期校准误差(ECE)低于0.0162。

Conclusion: 该研究通过开发字符识别模型，显著提升了对《白石道人歌曲》中苏字谱和律吕谱的光学音乐识别(OMR)性能，优于人工转录。

Abstract: Optical Music Recognition (OMR) for historical Chinese musical notations,
such as suzipu and l\"ul\"upu, presents unique challenges due to high class
imbalance and limited training data. This paper introduces significant
advancements in OMR for Jiang Kui's influential collection Baishidaoren Gequ
from 1202. In this work, we develop and evaluate a character recognition model
for scarce imbalanced data. We improve upon previous baselines by reducing the
Character Error Rate (CER) from 10.4% to 7.1% for suzipu, despite working with
77 highly imbalanced classes, and achieve a remarkable CER of 0.9% for
l\"ul\"upu. Our models outperform human transcribers, with an average human CER
of 15.9% and a best-case CER of 7.6%. We employ temperature scaling to achieve
a well-calibrated model with an Expected Calibration Error (ECE) below 0.0162.
Using a leave-one-edition-out cross-validation approach, we ensure robust
performance across five historical editions. Additionally, we extend the
KuiSCIMA dataset to include all 109 pieces from Baishidaoren Gequ, encompassing
suzipu, l\"ul\"upu, and jianzipu notations. Our findings advance the
digitization and accessibility of historical Chinese music, promoting cultural
diversity in OMR and expanding its applicability to underrepresented music
traditions.

</details>


### [53] [SAR-TEXT: A Large-Scale SAR Image-Text Dataset Built with SAR-Narrator and Progressive Transfer Learning](https://arxiv.org/abs/2507.18743)
*Xinjun Cheng,Yiguo He,Junjie Zhu,Chunping Qiu,Jun Wang,Qiangjuan Huang,Ke Yang*

Main category: cs.CV

TL;DR: Construct SAR-Text, a large-scale and high-quality dataset consisting of over 130,000 SAR image-text pairs to verify the effectiveness of the SAR-TEXT dataset, we conduct experiments on three typical vision-language tasks: image-text retrieval, image captioning, and visual question answering (VQA).


<details>
  <summary>Details</summary>
Motivation: lack of large-scale, high-quality SAR image-text datasets hinders its semantic understanding

Method: design the SAR-Narrator framework, which generates textual descriptions for SAR images through a multi-stage progressive transfer learning strategy

Result: SAR-RS-CLIP achieves notable improvements in retrieval performance, boosting average recall by 16.43% and 10.54% on the OSdataset-512 and HRSID test sets, respectively. SAR-RS-CoCa achieves BLEU-4, SPICE, and CIDEr scores exceeding those of the original CoCa model by more than 8x, 4x, and 10x, respectively.

Conclusion: SAR-GPT demonstrates stronger semantic understanding and reasoning ability, as further confirmed by qualitative results. SAR-Narrator can be readily adopted by the community to construct larger-scale SAR image-text datasets.

Abstract: Vision Language Models (VLMs) have achieved remarkable breakthroughs in the
field of remote sensing in recent years. Synthetic Aperture Radar (SAR)
imagery, with its all-weather capability, is essential in remote sensing, yet
the lack of large-scale, high-quality SAR image-text datasets hinders its
semantic understanding. In this paper, we construct SAR-Text, a large-scale and
high-quality dataset consisting of over 130,000 SAR image-text pairs. To
construct the SAR-Text dataset, we design the SAR-Narrator framework, which
generates textual descriptions for SAR images through a multi-stage progressive
transfer learning strategy. To verify the effectiveness of the SAR-TEXT
dataset, we conduct experiments on three typical vision-language tasks:
image-text retrieval, image captioning, and visual question answering (VQA).
Specifically, we construct three representative models on SAR-TEXT:
SAR-RS-CLIP, SAR-RS-CoCa, and SAR-GPT. SAR-RS-CLIP achieves notable
improvements in retrieval performance, boosting average recall by 16.43% and
10.54% on the OSdataset-512 and HRSID test sets, respectively. In the
captioning task, SAR-RS-CoCa achieves BLEU-4, SPICE, and CIDEr scores exceeding
those of the original CoCa model by more than 8x, 4x, and 10x, respectively. In
the VQA task, SAR-GPT outperforms baseline and single-stage models on multiple
SAR-VQA datasets, demonstrating stronger semantic understanding and reasoning
ability, as further confirmed by qualitative results. It is worth noting that,
as a flexible captioning tool, SAR-Narrator can be readily adopted by the
community to construct larger-scale SAR image-text datasets.

</details>


### [54] [Learning Efficient and Generalizable Human Representation with Human Gaussian Model](https://arxiv.org/abs/2507.18758)
*Yifan Liu,Shengjun Zhang,Chensheng Dai,Yang Chen,Hao Liu,Chen Li,Yueqi Duan*

Main category: cs.CV

TL;DR: 提出了Human Gaussian Graph来建模预测高斯和人体SMPL网格之间的连接，从而利用所有帧的信息来恢复可动画的人体表示。


<details>
  <summary>Details</summary>
Motivation: 从视频中建模可动画的人体化身是一个长期存在的挑战性问题。传统方法需要对每个实例进行优化，而最近的前馈方法被提出以使用可学习的网络生成3D高斯。

Method: 提出了Human Gaussian Graph来建模预测高斯和人体SMPL网格之间的连接。

Result: Human Gaussian Graph包含双层，其中高斯是第一层节点，网格顶点作为第二层节点。基于这种结构，我们进一步提出了节点内操作来聚合连接到一个网格顶点的各种高斯，以及节点间操作来支持网格节点邻居之间的消息传递。

Conclusion: 该方法在新的视图合成和新的姿势动画上表现出效率和泛化能力。

Abstract: Modeling animatable human avatars from videos is a long-standing and
challenging problem. While conventional methods require per-instance
optimization, recent feed-forward methods have been proposed to generate 3D
Gaussians with a learnable network. However, these methods predict Gaussians
for each frame independently, without fully capturing the relations of
Gaussians from different timestamps. To address this, we propose Human Gaussian
Graph to model the connection between predicted Gaussians and human SMPL mesh,
so that we can leverage information from all frames to recover an animatable
human representation. Specifically, the Human Gaussian Graph contains dual
layers where Gaussians are the first layer nodes and mesh vertices serve as the
second layer nodes. Based on this structure, we further propose the intra-node
operation to aggregate various Gaussians connected to one mesh vertex, and
inter-node operation to support message passing among mesh node neighbors.
Experimental results on novel view synthesis and novel pose animation
demonstrate the efficiency and generalization of our method.

</details>


### [55] [Diffusion-FS: Multimodal Free-Space Prediction via Diffusion for Autonomous Driving](https://arxiv.org/abs/2507.18763)
*Keshav Gupta,Tejas S. Stanley,Pranjal Paul,Arun K. Singh,K. Madhava Krishna*

Main category: cs.CV

TL;DR: This paper introduces a self-supervised approach, ContourDiff, for predicting drivable free-space corridors from monocular images by leveraging future ego-trajectories and a diffusion-based architecture that denoises over contour points.


<details>
  <summary>Details</summary>
Motivation: Existing corridor estimation methods directly assume a BEV-centric representation, which is hard to obtain. The formulation of drivable free-space corridor prediction as a pure image perception task poses challenges as there is no corresponding data for such free-space corridor segments in the image.

Method: A novel self-supervised approach for free-space sample generation is developed by leveraging future ego trajectories and front-view camera images. A diffusion process models the distribution of segments in the image. ContourDiff, a specialized diffusion-based architecture, denoises over contour points.

Result: The approach demonstrates effectiveness in accurately predicting safe multimodal navigable corridors in the image on both nuScenes and CARLA.

Conclusion: The approach accurately predicts safe multimodal navigable corridors in the image, as demonstrated on nuScenes and CARLA.

Abstract: Drivable Free-space prediction is a fundamental and crucial problem in
autonomous driving. Recent works have addressed the problem by representing the
entire non-obstacle road regions as the free-space. In contrast our aim is to
estimate the driving corridors that are a navigable subset of the entire road
region. Unfortunately, existing corridor estimation methods directly assume a
BEV-centric representation, which is hard to obtain. In contrast, we frame
drivable free-space corridor prediction as a pure image perception task, using
only monocular camera input. However such a formulation poses several
challenges as one doesn't have the corresponding data for such free-space
corridor segments in the image. Consequently, we develop a novel
self-supervised approach for free-space sample generation by leveraging future
ego trajectories and front-view camera images, making the process of visual
corridor estimation dependent on the ego trajectory. We then employ a diffusion
process to model the distribution of such segments in the image. However, the
existing binary mask-based representation for a segment poses many limitations.
Therefore, we introduce ContourDiff, a specialized diffusion-based architecture
that denoises over contour points rather than relying on binary mask
representations, enabling structured and interpretable free-space predictions.
We evaluate our approach qualitatively and quantitatively on both nuScenes and
CARLA, demonstrating its effectiveness in accurately predicting safe multimodal
navigable corridors in the image.

</details>


### [56] [Tell Me What You See: An Iterative Deep Learning Framework for Image Captioning](https://arxiv.org/abs/2507.18788)
*Hitesh Kumar Gupta*

Main category: cs.CV

TL;DR: 本文通过迭代开发 CNN-LSTM 图像描述模型，最终模型 Nexus 在 MS COCO 2017 数据集上实现了 31.4 的 BLEU-4 分数，验证了注意力机制的重要性。


<details>
  <summary>Details</summary>
Motivation: 图像描述是计算机视觉和自然语言处理的交汇点，需要对视觉场景和语言结构有深入的理解。现代方法主要由大规模 Transformer 架构主导，但本文记录了基础图像描述模型的系统迭代开发过程。

Method: 从简单的 CNN-LSTM 编码器-解码器开始，逐步发展到具有 EfficientNetV2B3 主干和动态注意力机制的先进模型 Nexus，共提出了五个模型。

Result: 研究表明，在经典的 CNN-LSTM 范例中，仅仅升级视觉主干而不使用相应的注意力机制会降低性能，因为单向量瓶颈无法传递更丰富的视觉细节。Nexus 模型实现了 31.4 的 BLEU-4 分数。

Conclusion: 通过迭代开发图像描述模型，从简单的 CNN-LSTM 编码器-解码器发展到有竞争力的基于注意力的系统，最终模型 Nexus 在 MS COCO 2017 数据集上实现了 31.4 的 BLEU-4 分数，超过了几个基础基准。

Abstract: Image captioning, a task at the confluence of computer vision and natural
language processing, requires a sophisticated understanding of both visual
scenes and linguistic structure. While modern approaches are dominated by
large-scale Transformer architectures, this paper documents a systematic,
iterative development of foundational image captioning models, progressing from
a simple CNN-LSTM encoder-decoder to a competitive attention-based system. We
present a series of five models, beginning with Genesis and concluding with
Nexus, an advanced model featuring an EfficientNetV2B3 backbone and a dynamic
attention mechanism. Our experiments chart the impact of architectural
enhancements and demonstrate a key finding within the classic CNN-LSTM
paradigm: merely upgrading the visual backbone without a corresponding
attention mechanism can degrade performance, as the single-vector bottleneck
cannot transmit the richer visual detail. This insight validates the
architectural shift to attention. Trained on the MS COCO 2017 dataset, our
final model, Nexus, achieves a BLEU-4 score of 31.4, surpassing several
foundational benchmarks and validating our iterative design process. This work
provides a clear, replicable blueprint for understanding the core architectural
principles that underpin modern vision-language tasks.

</details>


### [57] [Deepfake Detection Via Facial Feature Extraction and Modeling](https://arxiv.org/abs/2507.18815)
*Benjamin Carter,Nathan Dilla,Micheal Callahan,Atuhaire Ambala*

Main category: cs.CV

TL;DR: 本文提出了一种仅使用面部标志进行deepfake检测的方法，实验结果表明该方法在各种神经网络模型中都是有效的，且需要更少的参数。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术的兴起对当今在线发现的各种形式媒体的真实性提出了新的问题。人工智能（AI）生成的视频和图像变得越来越难以与真实媒体区分开来，因此需要新的模型来检测人工智能生成的媒体。

Method: 本文介绍了一种仅使用面部标志进行deepfake检测的方法。使用包含人脸的deepfake和真实视频的数据集，本文介绍了一种提取面部标志用于deepfake检测的方法，重点是识别面部运动中的细微不一致，而不是原始图像处理。

Result: 实验结果表明，这种特征提取技术在各种神经网络模型中都是有效的，相同的面部标志在三个神经网络模型上进行了测试，具有良好的性能指标，表明其在现实世界应用中的潜力。本文讨论的结果包括RNN和人工神经网络（ANN）模型，准确率分别在96%和93%之间，而CNN模型则在78%左右。

Conclusion: 这项研究挑战了原始图像处理对于识别deepfake视频是必要的假设，提出了一种与各种神经网络模型兼容的面部特征提取方法，同时需要更少的参数。

Abstract: The rise of deepfake technology brings forth new questions about the
authenticity of various forms of media found online today. Videos and images
generated by artificial intelligence (AI) have become increasingly more
difficult to differentiate from genuine media, resulting in the need for new
models to detect artificially-generated media. While many models have attempted
to solve this, most focus on direct image processing, adapting a convolutional
neural network (CNN) or a recurrent neural network (RNN) that directly
interacts with the video image data. This paper introduces an approach of using
solely facial landmarks for deepfake detection. Using a dataset consisting of
both deepfake and genuine videos of human faces, this paper describes an
approach for extracting facial landmarks for deepfake detection, focusing on
identifying subtle inconsistencies in facial movements instead of raw image
processing. Experimental results demonstrated that this feature extraction
technique is effective in various neural network models, with the same facial
landmarks tested on three neural network models, with promising performance
metrics indicating its potential for real-world applications. The findings
discussed in this paper include RNN and artificial neural network (ANN) models
with accuracy between 96% and 93%, respectively, with a CNN model hovering
around 78%. This research challenges the assumption that raw image processing
is necessary to identify deepfake videos by presenting a facial feature
extraction approach compatible with various neural network models while
requiring fewer parameters.

</details>


### [58] [Flow Stochastic Segmentation Networks](https://arxiv.org/abs/2507.18838)
*Fabio De Sousa Ribeiro,Omar Todd,Charles Jones,Avinash Kori,Raghav Mehta,Ben Glocker*

Main category: cs.CV

TL;DR: Flow-SSN是一种新的生成分割模型，它优于现有的医学图像分割方法，且采样效率更高。


<details>
  <summary>Details</summary>
Motivation: 先前方法的低秩参数化存在基本限制。

Method: 提出了Flow Stochastic Segmentation Network (Flow-SSN)，这是一种生成分割模型系列，具有离散时间自回归和现代连续时间流变体。

Result: Flow-SSN可以估计任意高秩的像素级协方差，而无需假设秩或存储分布参数。Flow-SSN的采样效率也高于标准基于扩散的分割模型，这归功于模型的大部分容量被分配用于学习流量的基础分布，从而构成了富有表现力的先验。

Conclusion: Flow-SSNs在具有挑战性的医学成像基准测试中实现了最先进的结果。

Abstract: We introduce the Flow Stochastic Segmentation Network (Flow-SSN), a
generative segmentation model family featuring discrete-time autoregressive and
modern continuous-time flow variants. We prove fundamental limitations of the
low-rank parameterisation of previous methods and show that Flow-SSNs can
estimate arbitrarily high-rank pixel-wise covariances without assuming the rank
or storing the distributional parameters. Flow-SSNs are also more efficient to
sample from than standard diffusion-based segmentation models, thanks to most
of the model capacity being allocated to learning the base distribution of the
flow, constituting an expressive prior. We apply Flow-SSNs to challenging
medical imaging benchmarks and achieve state-of-the-art results. Code
available: https://github.com/biomedia-mira/flow-ssn.

</details>


### [59] [PTCMIL: Multiple Instance Learning via Prompt Token Clustering for Whole Slide Image Analysis](https://arxiv.org/abs/2507.18848)
*Beidi Zhao,SangMook Kim,Hao Chen,Chen Zhou,Zu-hua Gao,Gang Wang,Xiaoxiao Li*

Main category: cs.CV

TL;DR: PTCMIL：一种基于Prompt Token Clustering的ViT，用于MIL聚合，它通过引入可学习的prompt token来统一聚类和预测任务，从而有效地捕获task相关的模式。


<details>
  <summary>Details</summary>
Motivation: 现有的MIL方法难以将不同的patch信息聚合成鲁棒的WSI表示，并且计算密集，无法捕获特定任务和slide特异性变异。

Method: 提出了一种新的基于Prompt Token Clustering的ViT (PTCMIL) 用于MIL聚合。

Result: 在八个数据集上的大量实验表明，PTCMIL在分类和生存分析任务中表现出色，优于现有方法。消融研究证实了其鲁棒性和强大的可解释性。

Conclusion: PTCMIL在分类和生存分析任务中表现出色，优于现有方法，具有鲁棒性和可解释性。

Abstract: Multiple Instance Learning (MIL) has advanced WSI analysis but struggles with
the complexity and heterogeneity of WSIs. Existing MIL methods face challenges
in aggregating diverse patch information into robust WSI representations. While
ViTs and clustering-based approaches show promise, they are computationally
intensive and fail to capture task-specific and slide-specific variability. To
address these limitations, we propose PTCMIL, a novel Prompt Token
Clustering-based ViT for MIL aggregation. By introducing learnable prompt
tokens into the ViT backbone, PTCMIL unifies clustering and prediction tasks in
an end-to-end manner. It dynamically aligns clustering with downstream tasks,
using projection-based clustering tailored to each WSI, reducing complexity
while preserving patch heterogeneity. Through token merging and prototype-based
pooling, PTCMIL efficiently captures task-relevant patterns. Extensive
experiments on eight datasets demonstrate its superior performance in
classification and survival analysis tasks, outperforming state-of-the-art
methods. Systematic ablation studies confirm its robustness and strong
interpretability. The code is released at https://github.com/ubc-tea/PTCMIL.

</details>


### [60] [Phoneme-Level Visual Speech Recognition via Point-Visual Fusion and Language Model Reconstruction](https://arxiv.org/abs/2507.18863)
*Matthew Kit Khinn Teng,Haibo Zhang,Takeshi Saitoh*

Main category: cs.CV

TL;DR: 提出了一种基于音素的两阶段视觉自动语音识别框架，该框架通过融合视觉和landmark运动特征，并利用LLM模型进行单词重建，显著提高了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常旨在直接从视觉线索预测单词或字符，但由于视觉素的模糊性而导致高错误率，并且需要大量的预训练数据。

Method: 提出了一种新颖的基于音素的两阶段框架，该框架融合了视觉和landmark运动特征，然后使用LLM模型进行单词重建。

Result: 提出的PV-ASR方法通过在LRS2上实现17.4%的WER和在LRS3数据集上实现21.0%的WER，展示了卓越的性能。

Conclusion: PV-ASR方法在LRS2和LRS3数据集上分别实现了17.4%和21.0%的WER，表现出卓越的性能。

Abstract: Visual Automatic Speech Recognition (V-ASR) is a challenging task that
involves interpreting spoken language solely from visual information, such as
lip movements and facial expressions. This task is notably challenging due to
the absence of auditory cues and the visual ambiguity of phonemes that exhibit
similar visemes-distinct sounds that appear identical in lip motions. Existing
methods often aim to predict words or characters directly from visual cues, but
they commonly suffer from high error rates due to viseme ambiguity and require
large amounts of pre-training data. We propose a novel phoneme-based two-stage
framework that fuses visual and landmark motion features, followed by an LLM
model for word reconstruction to address these challenges. Stage 1 consists of
V-ASR, which outputs the predicted phonemes, thereby reducing training
complexity. Meanwhile, the facial landmark features address speaker-specific
facial characteristics. Stage 2 comprises an encoder-decoder LLM model, NLLB,
that reconstructs the output phonemes back to words. Besides using a large
visual dataset for deep learning fine-tuning, our PV-ASR method demonstrates
superior performance by achieving 17.4% WER on the LRS2 and 21.0% WER on the
LRS3 dataset.

</details>


### [61] [Transferable and Undefendable Point Cloud Attacks via Medial Axis Transform](https://arxiv.org/abs/2507.18870)
*Keke Tang,Yuze Gao,Weilong Peng,Xiaofei Wang,Meie Fang,Peican Zhu*

Main category: cs.CV

TL;DR: 提出了一种新的对抗性攻击框架MAT-Adv，该框架通过显式地扰动骨架变换（MAT）表示来增强可转移性和不可防御性，以便在生成的点云中诱导固有的对抗性。


<details>
  <summary>Details</summary>
Motivation: 研究点云上的对抗性攻击对于评估和提高3D深度学习模型的鲁棒性至关重要。然而，大多数现有的攻击方法都是在理想的白盒设置下开发的，并且常常遭受有限的可转移性到看不见的模型和不足的鲁棒性对抗常见的防御机制。

Method: 通过扰动内蕴的骨架表示，MAT-Adv引入了结构级的对抗性特征，这种特征在不同的模型和防御策略中仍然有效。为了减轻过拟合和防止扰动崩溃，我们将dropout策略加入到MAT扰动的优化中，进一步提高了可转移性和不可防御性。

Result: MAT-Adv在可转移性和不可防御性方面均显著优于现有的最先进方法。

Conclusion: MAT-Adv显著优于现有技术水平的方法，在可转移性和不可防御性方面均有提升。

Abstract: Studying adversarial attacks on point clouds is essential for evaluating and
improving the robustness of 3D deep learning models. However, most existing
attack methods are developed under ideal white-box settings and often suffer
from limited transferability to unseen models and insufficient robustness
against common defense mechanisms. In this paper, we propose MAT-Adv, a novel
adversarial attack framework that enhances both transferability and
undefendability by explicitly perturbing the medial axis transform (MAT)
representations, in order to induce inherent adversarialness in the resulting
point clouds. Specifically, we employ an autoencoder to project input point
clouds into compact MAT representations that capture the intrinsic geometric
structure of point clouds. By perturbing these intrinsic representations,
MAT-Adv introduces structural-level adversarial characteristics that remain
effective across diverse models and defense strategies. To mitigate overfitting
and prevent perturbation collapse, we incorporate a dropout strategy into the
optimization of MAT perturbations, further improving transferability and
undefendability. Extensive experiments demonstrate that MAT-Adv significantly
outperforms existing state-of-the-art methods in both transferability and
undefendability. Codes will be made public upon paper acceptance.

</details>


### [62] [Perspective from a Higher Dimension: Can 3D Geometric Priors Help Visual Floorplan Localization?](https://arxiv.org/abs/2507.18881)
*Bolei Chen,Jiaxu Kang,Haonan Yang,Ping Zhong,Jianxin Wang*

Main category: cs.CV

TL;DR: 通过注入 3D 几何先验来解决 2D 楼层平面定位 (FLoc) 问题，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于建筑物的楼层平面图易于访问、随时间推移保持一致，并且本质上能够抵抗视觉外观的变化，因此楼层平面图内的自我定位吸引了研究人员的兴趣。然而，由于楼层平面图是建筑物结构的极简表示，视觉感知和楼层平面图之间的模态和几何差异给这项任务带来了挑战。现有的方法巧妙地利用 2D 几何特征和姿势过滤器来实现有希望的性能，但它们未能解决由频繁的视觉变化和由于各种形状的 3D 物体引起的视图遮挡导致的定位错误。

Method: 通过将 3D 几何先验注入视觉 FLoc 算法，从更高维度看待 2D 楼层平面定位 (FLoc) 问题。首先，使用多视图约束对几何感知视图不变性进行建模，然后，通过将场景的表面重建与序列的 RGB 帧相关联，进一步建模视图-场景对齐的几何先验，从而增强跨模态几何-颜色对应关系。两种 3D 先验都通过自监督对比学习进行建模，因此不需要额外的几何或语义注释。

Result: 大量对比研究表明，该方法显著优于现有方法，并大大提高了 FLoc 精度。

Conclusion: 该方法显著优于现有方法，并大大提高了 FLoc 精度。

Abstract: Since a building's floorplans are easily accessible, consistent over time,
and inherently robust to changes in visual appearance, self-localization within
the floorplan has attracted researchers' interest. However, since floorplans
are minimalist representations of a building's structure, modal and geometric
differences between visual perceptions and floorplans pose challenges to this
task. While existing methods cleverly utilize 2D geometric features and pose
filters to achieve promising performance, they fail to address the localization
errors caused by frequent visual changes and view occlusions due to variously
shaped 3D objects. To tackle these issues, this paper views the 2D Floorplan
Localization (FLoc) problem from a higher dimension by injecting 3D geometric
priors into the visual FLoc algorithm. For the 3D geometric prior modeling, we
first model geometrically aware view invariance using multi-view constraints,
i.e., leveraging imaging geometric principles to provide matching constraints
between multiple images that see the same points. Then, we further model the
view-scene aligned geometric priors, enhancing the cross-modal geometry-color
correspondences by associating the scene's surface reconstruction with the RGB
frames of the sequence. Both 3D priors are modeled through self-supervised
contrastive learning, thus no additional geometric or semantic annotations are
required. These 3D priors summarized in extensive realistic scenes bridge the
modal gap while improving localization success without increasing the
computational burden on the FLoc algorithm. Sufficient comparative studies
demonstrate that our method significantly outperforms state-of-the-art methods
and substantially boosts the FLoc accuracy. All data and code will be released
after the anonymous review.

</details>


### [63] [Synthetic-to-Real Camouflaged Object Detection](https://arxiv.org/abs/2507.18911)
*Zhihao Luo,Luojun Lin,Zheng Lin*

Main category: cs.CV

TL;DR: This paper introduces Syn-to-Real Camouflaged Object Detection (S2R-COD) to address the problem of limited data in COD. The proposed CSRDA framework uses a student-teacher model and recurrent learning to adapt from synthetic to real images.


<details>
  <summary>Details</summary>
Motivation: There are relatively few datasets for camouflaged object detection (COD) due to the high cost of collection and labeling. Directly training with synthetic datasets compared to real datasets can lead to a degradation in model performance.

Method: A student-teacher model based method called Cycling Syn-to-Real Domain Adaptation Framework (CSRDA) is proposed. It propagates class information from the labeled source domain to the unlabeled target domain through pseudo labeling combined with consistency regularization. A recurrent learning framework is utilized to build an evolving real domain for bridging the source and target domain.

Result: Extensive experiments demonstrate the effectiveness of the proposed framework.

Conclusion: The proposed Cycling Syn-to-Real Domain Adaptation Framework (CSRDA) effectively mitigates the problem of limited data and handcraft annotations in COD.

Abstract: Due to the high cost of collection and labeling, there are relatively few
datasets for camouflaged object detection (COD). In particular, for certain
specialized categories, the available image dataset is insufficiently
populated. Synthetic datasets can be utilized to alleviate the problem of
limited data to some extent. However, directly training with synthetic datasets
compared to real datasets can lead to a degradation in model performance. To
tackle this problem, in this work, we investigate a new task, namely
Syn-to-Real Camouflaged Object Detection (S2R-COD). In order to improve the
model performance in real world scenarios, a set of annotated synthetic
camouflaged images and a limited number of unannotated real images must be
utilized. We propose the Cycling Syn-to-Real Domain Adaptation Framework
(CSRDA), a method based on the student-teacher model. Specially, CSRDA
propagates class information from the labeled source domain to the unlabeled
target domain through pseudo labeling combined with consistency regularization.
Considering that narrowing the intra-domain gap can improve the quality of
pseudo labeling, CSRDA utilizes a recurrent learning framework to build an
evolving real domain for bridging the source and target domain. Extensive
experiments demonstrate the effectiveness of our framework, mitigating the
problem of limited data and handcraft annotations in COD. Our code is publicly
available at: https://github.com/Muscape/S2R-COD

</details>


### [64] [HQ-SMem: Video Segmentation and Tracking Using Memory Efficient Object Embedding With Selective Update and Self-Supervised Distillation Feedback](https://arxiv.org/abs/2507.18921)
*Elham Soltani Kazemi,Imad Eddine Toubal,Gani Rahmon,Jaired Collins,K. Palaniappan*

Main category: cs.CV

TL;DR: HQ-SMem是一种新的视频分割方法，通过利用SAM-HQ、动态记忆和外观模型来提高分割质量，尤其是在长视频中。


<details>
  <summary>Details</summary>
Motivation: 现有的视频目标分割模型在精确的mask delineations、可变形物体、拓扑变换物体、跟踪漂移和长视频序列方面存在不足。

Method: 该方法结合了SAM-HQ以优化粗糙的分割mask，并结合动态智能记忆机制和动态更新外观模型。

Result: HQ-SMem通过解决现有VOS模型的局限性，提高了VOS基本模型的性能。

Conclusion: HQ-SMem在多个公共数据集上始终名列前茅，并在长视频数据集和LVOS上设立了新的基准。

Abstract: Video Object Segmentation (VOS) is foundational to numerous computer vision
applications, including surveillance, autonomous driving, robotics and
generative video editing. However, existing VOS models often struggle with
precise mask delineation, deformable objects, topologically transforming
objects, tracking drift and long video sequences. In this paper, we introduce
HQ-SMem, for High Quality video segmentation and tracking using Smart Memory, a
novel method that enhances the performance of VOS base models by addressing
these limitations. Our approach incorporates three key innovations: (i)
leveraging SAM with High-Quality masks (SAM-HQ) alongside appearance-based
candidate-selection to refine coarse segmentation masks, resulting in improved
object boundaries; (ii) implementing a dynamic smart memory mechanism that
selectively stores relevant key frames while discarding redundant ones, thereby
optimizing memory usage and processing efficiency for long-term videos; and
(iii) dynamically updating the appearance model to effectively handle complex
topological object variations and reduce drift throughout the video. These
contributions mitigate several limitations of existing VOS models including,
coarse segmentations that mix-in background pixels, fixed memory update
schedules, brittleness to drift and occlusions, and prompt ambiguity issues
associated with SAM. Extensive experiments conducted on multiple public
datasets and state-of-the-art base trackers demonstrate that our method
consistently ranks among the top two on VOTS and VOTSt 2024 datasets. Moreover,
HQ-SMem sets new benchmarks on Long Video Dataset and LVOS, showcasing its
effectiveness in challenging scenarios characterized by complex multi-object
dynamics over extended temporal durations.

</details>


### [65] [Gaussian Set Surface Reconstruction through Per-Gaussian Optimization](https://arxiv.org/abs/2507.18923)
*Zhentao Huang,Di Wu,Zhenbang He,Minglun Gong*

Main category: cs.CV

TL;DR: GSSR 是一种用于均匀分布高斯并对齐其主法线与表面法线的方法，通过像素级和高斯级单视图法线一致性以及多视图光度一致性的结合来实现。


<details>
  <summary>Details</summary>
Motivation: 3D 高斯溅射 (3DGS) 通过其灵活的表示有效地合成新颖的视图，但无法准确地重建场景几何。虽然 PGSR 等现代变体引入了额外的损失以确保通过高斯融合获得适当的深度和法线贴图，但它们仍然忽略了单独的放置优化。这导致高斯分布不均匀，偏离了潜在表面，从而使重建细化和场景编辑变得复杂。

Method: GSSR 结合像素级和高斯级单视图法线一致性以及多视图光度一致性，通过优化局部和全局视角来执行细粒度的几何对齐。为了进一步优化表示，引入了不透明度正则化损失以消除冗余高斯，并应用周期性深度和法线引导的高斯重新初始化，以实现更清晰、更均匀的空间分布。

Result: 重建结果表明，高斯放置的几何精度显着提高，从而能够进行直观的场景编辑和高效生成基于高斯的新型 3D 环境。大量实验验证了 GSSR 的有效性，在保持高质量渲染性能的同时，提高了几何精度。

Conclusion: GSSR显著提高了高斯放置的几何精度，从而能够进行直观的场景编辑并高效生成基于高斯的新型 3D 环境。大量实验验证了 GSSR 的有效性，在保持高质量渲染性能的同时，提高了几何精度。

Abstract: 3D Gaussian Splatting (3DGS) effectively synthesizes novel views through its
flexible representation, yet fails to accurately reconstruct scene geometry.
While modern variants like PGSR introduce additional losses to ensure proper
depth and normal maps through Gaussian fusion, they still neglect individual
placement optimization. This results in unevenly distributed Gaussians that
deviate from the latent surface, complicating both reconstruction refinement
and scene editing. Motivated by pioneering work on Point Set Surfaces, we
propose Gaussian Set Surface Reconstruction (GSSR), a method designed to
distribute Gaussians evenly along the latent surface while aligning their
dominant normals with the surface normal. GSSR enforces fine-grained geometric
alignment through a combination of pixel-level and Gaussian-level single-view
normal consistency and multi-view photometric consistency, optimizing both
local and global perspectives. To further refine the representation, we
introduce an opacity regularization loss to eliminate redundant Gaussians and
apply periodic depth- and normal-guided Gaussian reinitialization for a
cleaner, more uniform spatial distribution. Our reconstruction results
demonstrate significantly improved geometric precision in Gaussian placement,
enabling intuitive scene editing and efficient generation of novel
Gaussian-based 3D environments. Extensive experiments validate GSSR's
effectiveness, showing enhanced geometric accuracy while preserving
high-quality rendering performance.

</details>


### [66] [WiSE-OD: Benchmarking Robustness in Infrared Object Detection](https://arxiv.org/abs/2507.18925)
*Heitor R. Medeiros,Atif Belal,Masih Aminbeidokhti,Eric Granger,Marco Pedersoli*

Main category: cs.CV

TL;DR: Introduce LLVIP-C and FLIR-C, two cross-modality OOD benchmarks. Propose WiSE-OD, a weight-space ensembling method, to improve cross-modality and corruption robustness.


<details>
  <summary>Details</summary>
Motivation: The scarcity of large-scale IR datasets forces models to rely on weights pre-trained on RGB images. Fine-tuning on IR improves accuracy, but compromises robustness under distribution shifts due to the modality gap between RGB and IR.

Method: WiSE-OD, a weight-space ensembling method with two variants: WiSE-OD$_{ZS}$ and WiSE-OD$_{LP}$.

Result: Evaluated across three RGB-pretrained detectors and two robust baselines, WiSE-OD improves both cross-modality and corruption robustness.

Conclusion: WiSE-OD improves both cross-modality and corruption robustness without any additional training or inference cost.

Abstract: Object detection (OD) in infrared (IR) imagery is critical for low-light and
nighttime applications. However, the scarcity of large-scale IR datasets forces
models to rely on weights pre-trained on RGB images. While fine-tuning on IR
improves accuracy, it often compromises robustness under distribution shifts
due to the inherent modality gap between RGB and IR. To address this, we
introduce LLVIP-C and FLIR-C, two cross-modality out-of-distribution (OOD)
benchmarks built by applying corruption to standard IR datasets. Additionally,
to fully leverage the complementary knowledge from RGB and infrared trained
models, we propose WiSE-OD, a weight-space ensembling method with two variants:
WiSE-OD$_{ZS}$, which combines RGB zero-shot and IR fine-tuned weights, and
WiSE-OD$_{LP}$, which blends zero-shot and linear probing. Evaluated across
three RGB-pretrained detectors and two robust baselines, WiSE-OD improves both
cross-modality and corruption robustness without any additional training or
inference cost.

</details>


### [67] [MGHFT: Multi-Granularity Hierarchical Fusion Transformer for Cross-Modal Sticker Emotion Recognition](https://arxiv.org/abs/2507.18929)
*Jian Chen,Yuxuan Hu,Haifeng Lu,Wei Wang,Min Yang,Chengming Li,Xiping Hu*

Main category: cs.CV

TL;DR: 提出了MGHFT，一种用于贴纸情感理解的多粒度分层融合transformer，它优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于贴纸情感理解依赖于多视角信息，例如背景知识和风格线索，因此仍然具有挑战性。

Method: 提出了一个新颖的多粒度分层融合transformer (MGHFT)，它带有一个基于多模态大型语言模型的多视角贴纸解释器。首先使用多模态大型语言模型通过多视角描述提供丰富的文本语境来解释贴纸。然后，设计了一种分层融合策略，将文本语境融合到视觉理解中，该策略建立在金字塔视觉transformer之上，以提取多个阶段的全局和局部贴纸特征。通过对比学习和注意力机制，文本特征被注入到视觉主干的不同阶段，增强了全局和局部粒度视觉语义与文本指导的融合。最后，引入了一种文本指导的融合注意力机制，以有效整合整体多模态特征，增强语义理解。

Result: MGHFT在两个公共贴纸情感数据集上显著优于现有的贴纸情感识别方法，实现了更高的准确率和更细粒度的情感识别。与最佳的预训练视觉模型相比，MGHFT在F1上提高了5.4%，在准确率上提高了4.0%。

Conclusion: MGHFT在两个公共贴纸情感数据集上显著优于现有的贴纸情感识别方法，实现了更高的准确率和更细粒度的情感识别。与最佳的预训练视觉模型相比，MGHFT在F1上提高了5.4%，在准确率上提高了4.0%。

Abstract: Although pre-trained visual models with text have demonstrated strong
capabilities in visual feature extraction, sticker emotion understanding
remains challenging due to its reliance on multi-view information, such as
background knowledge and stylistic cues. To address this, we propose a novel
multi-granularity hierarchical fusion transformer (MGHFT), with a multi-view
sticker interpreter based on Multimodal Large Language Models. Specifically,
inspired by the human ability to interpret sticker emotions from multiple
views, we first use Multimodal Large Language Models to interpret stickers by
providing rich textual context via multi-view descriptions. Then, we design a
hierarchical fusion strategy to fuse the textual context into visual
understanding, which builds upon a pyramid visual transformer to extract both
global and local sticker features at multiple stages. Through contrastive
learning and attention mechanisms, textual features are injected at different
stages of the visual backbone, enhancing the fusion of global- and
local-granularity visual semantics with textual guidance. Finally, we introduce
a text-guided fusion attention mechanism to effectively integrate the overall
multimodal features, enhancing semantic understanding. Extensive experiments on
2 public sticker emotion datasets demonstrate that MGHFT significantly
outperforms existing sticker emotion recognition approaches, achieving higher
accuracy and more fine-grained emotion recognition. Compared to the best
pre-trained visual models, our MGHFT also obtains an obvious improvement, 5.4%
on F1 and 4.0% on accuracy. The code is released at
https://github.com/cccccj-03/MGHFT_ACMMM2025.

</details>


### [68] [PDT: Point Distribution Transformation with Diffusion Models](https://arxiv.org/abs/2507.18939)
*Jionghao Wang,Cheng Lin,Yuan Liu,Rui Xu,Zhiyang Dou,Xiao-Xiao Long,Hao-Xiang Guo,Taku Komura,Wenping Wang,Xin Li*

Main category: cs.CV

TL;DR: PDT transforms unstructured point clouds into structured, semantically meaningful point distributions using diffusion models.


<details>
  <summary>Details</summary>
Motivation: Extracting meaningful structural information from unstructured point cloud distributions and transforming them into semantically meaningful point distributions is an under-explored problem.

Method: The method utilizes diffusion models with a novel architecture and learning strategy to correlate source and target point distributions through a denoising process.

Result: The method successfully transforms input point clouds into various forms of structured outputs, including surface-aligned keypoints, inner sparse joints, and continuous feature lines.

Conclusion: The paper introduces a point distribution transformation framework (PDT) using diffusion models, capable of transforming unstructured point clouds into structured outputs like keypoints, joints, and feature lines, demonstrating the capture of both geometric and semantic features.

Abstract: Point-based representations have consistently played a vital role in
geometric data structures. Most point cloud learning and processing methods
typically leverage the unordered and unconstrained nature to represent the
underlying geometry of 3D shapes. However, how to extract meaningful structural
information from unstructured point cloud distributions and transform them into
semantically meaningful point distributions remains an under-explored problem.
We present PDT, a novel framework for point distribution transformation with
diffusion models. Given a set of input points, PDT learns to transform the
point set from its original geometric distribution into a target distribution
that is semantically meaningful. Our method utilizes diffusion models with
novel architecture and learning strategy, which effectively correlates the
source and the target distribution through a denoising process. Through
extensive experiments, we show that our method successfully transforms input
point clouds into various forms of structured outputs - ranging from
surface-aligned keypoints, and inner sparse joints to continuous feature lines.
The results showcase our framework's ability to capture both geometric and
semantic features, offering a powerful tool for various 3D geometry processing
tasks where structured point distributions are desired. Code will be available
at this link: https://github.com/shanemankiw/PDT.

</details>


### [69] [Underwater Waste Detection Using Deep Learning A Performance Comparison of YOLOv7 to 10 and Faster RCNN](https://arxiv.org/abs/2507.18967)
*UMMPK Nawarathne,HMNS Kumari,HMLS Kumari*

Main category: cs.CV

TL;DR: 本研究比较了五种目标识别算法在水下垃圾检测中的性能，结果表明 YOLOv8 表现最佳，可作为水下清理行动的有效工具。


<details>
  <summary>Details</summary>
Motivation: 水下污染是当今最严重的环境问题之一，准确检测这些废物对于成功的废物管理、环境监测和缓解策略至关重要。

Method: 使用了五种先进的目标识别算法，包括 YOLOv7、YOLOv8、YOLOv9、YOLOv10 和 Faster R-CNN，并在包含十五个不同类别的大型数据集上进行了训练和测试。

Result: YOLOv8 的 mAP 达到 80.9%，优于其他模型，这归功于其架构，该架构结合了改进的无锚机制和自监督学习等高级功能。

Conclusion: YOLOv8 模型在水下垃圾检测方面表现出色，具有改进的检测能力和可扩展性，为全球污染治理提供了一种有效的工具。

Abstract: Underwater pollution is one of today's most significant environmental
concerns, with vast volumes of garbage found in seas, rivers, and landscapes
around the world. Accurate detection of these waste materials is crucial for
successful waste management, environmental monitoring, and mitigation
strategies. In this study, we investigated the performance of five cutting-edge
object recognition algorithms, namely YOLO (You Only Look Once) models,
including YOLOv7, YOLOv8, YOLOv9, YOLOv10, and Faster Region-Convolutional
Neural Network (R-CNN), to identify which model was most effective at
recognizing materials in underwater situations. The models were thoroughly
trained and tested on a large dataset containing fifteen different classes
under diverse conditions, such as low visibility and variable depths. From the
above-mentioned models, YOLOv8 outperformed the others, with a mean Average
Precision (mAP) of 80.9%, indicating a significant performance. This increased
performance is attributed to YOLOv8's architecture, which incorporates advanced
features such as improved anchor-free mechanisms and self-supervised learning,
allowing for more precise and efficient recognition of items in a variety of
settings. These findings highlight the YOLOv8 model's potential as an effective
tool in the global fight against pollution, improving both the detection
capabilities and scalability of underwater cleanup operations.

</details>


### [70] [Structure Matters: Revisiting Boundary Refinement in Video Object Segmentation](https://arxiv.org/abs/2507.18944)
*Guanyi Qin,Ziyue Wang,Daiyun Shen,Haofeng Liu,Hantao Zhou,Junde Wu,Runze Hu,Yueming Jin*

Main category: cs.CV

TL;DR: 提出了一种名为OASIS的新型半监督视频对象分割方法，该方法通过结构细化模块和证据学习来提高分割精度和处理遮挡，并在性能和速度方面都优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 半监督视频对象分割（SVOS）技术旨在跟踪和分割视频帧中的对象，这是计算机视觉中的一项基本任务。虽然最近的基于内存的方法显示出潜力，但它们通常难以处理涉及遮挡的场景，尤其是在处理对象交互和高特征相似性时。

Method: 提出了一种新颖的具有固有结构细化的边界修正视频对象分割方法，命名为OASIS。具体来说，提出了一个轻量级的结构细化模块来提高分割精度。通过融合Canny滤波器捕获的粗糙边缘先验和存储的对象特征，该模块可以生成对象级别的结构图，并通过突出显示边界特征来细化表示。引入了用于不确定性估计的证据学习，以进一步解决遮挡区域中的挑战。

Result: OASIS在DAVIS-17验证集上实现了91.6的F值（相对于89.7），在YouTubeVOS 2019验证集上实现了86.6的G值（相对于86.2），同时在DAVIS上保持了48 FPS的有竞争力的速度。

Conclusion: OASIS在具有挑战性的基准测试中表现出卓越的性能和有竞争力的推理速度，例如在DAVIS-17验证集上实现了91.6的F值（相对于89.7），在YouTubeVOS 2019验证集上实现了86.6的G值（相对于86.2），同时在DAVIS上保持了48 FPS的有竞争力的速度。

Abstract: Given an object mask, Semi-supervised Video Object Segmentation (SVOS)
technique aims to track and segment the object across video frames, serving as
a fundamental task in computer vision. Although recent memory-based methods
demonstrate potential, they often struggle with scenes involving occlusion,
particularly in handling object interactions and high feature similarity. To
address these issues and meet the real-time processing requirements of
downstream applications, in this paper, we propose a novel bOundary Amendment
video object Segmentation method with Inherent Structure refinement, hereby
named OASIS. Specifically, a lightweight structure refinement module is
proposed to enhance segmentation accuracy. With the fusion of rough edge priors
captured by the Canny filter and stored object features, the module can
generate an object-level structure map and refine the representations by
highlighting boundary features. Evidential learning for uncertainty estimation
is introduced to further address challenges in occluded regions. The proposed
method, OASIS, maintains an efficient design, yet extensive experiments on
challenging benchmarks demonstrate its superior performance and competitive
inference speed compared to other state-of-the-art methods, i.e., achieving the
F values of 91.6 (vs. 89.7 on DAVIS-17 validation set) and G values of 86.6
(vs. 86.2 on YouTubeVOS 2019 validation set) while maintaining a competitive
speed of 48 FPS on DAVIS.

</details>


### [71] [PerioDet: Large-Scale Panoramic Radiograph Benchmark for Clinical-Oriented Apical Periodontitis Detection](https://arxiv.org/abs/2507.18958)
*Xiaocheng Fang,Jieyi Cai,Huanyu Liu,Chengju Zhou,Minhua Lu,Bingzhi Chen*

Main category: cs.CV

TL;DR: 发布了一个大规模的根尖周炎全景 X 射线基准数据集 PerioXrays，并提出了一个用于自动化根尖周炎检测的 PerioDet 范例。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏大规模、高质量的带注释数据集，计算机辅助诊断 (CAD) 应用在根尖周炎中的开发受到限制。

Method: 提出了一种面向临床的根尖周炎检测 (PerioDet) 范例，该范例共同结合了背景去噪注意力 (BDA) 和 IoU 动态校准 (IDC) 机制。

Result: 在 PerioXrays 数据集上的大量实验证明了 PerioDet 的优越性。

Conclusion: PerioDet在推进自动化根尖周炎检测方面表现出色，人机协作实验证明了其作为牙医辅助诊断工具的临床适用性。

Abstract: Apical periodontitis is a prevalent oral pathology that presents significant
public health challenges. Despite advances in automated diagnostic systems
across various medical fields, the development of Computer-Aided Diagnosis
(CAD) applications for apical periodontitis is still constrained by the lack of
a large-scale, high-quality annotated dataset. To address this issue, we
release a large-scale panoramic radiograph benchmark called "PerioXrays",
comprising 3,673 images and 5,662 meticulously annotated instances of apical
periodontitis. To the best of our knowledge, this is the first benchmark
dataset for automated apical periodontitis diagnosis. This paper further
proposes a clinical-oriented apical periodontitis detection (PerioDet)
paradigm, which jointly incorporates Background-Denoising Attention (BDA) and
IoU-Dynamic Calibration (IDC) mechanisms to address the challenges posed by
background noise and small targets in automated detection. Extensive
experiments on the PerioXrays dataset demonstrate the superiority of PerioDet
in advancing automated apical periodontitis detection. Additionally, a
well-designed human-computer collaborative experiment underscores the clinical
applicability of our method as an auxiliary diagnostic tool for professional
dentists.

</details>


### [72] [YOLO for Knowledge Extraction from Vehicle Images: A Baseline Study](https://arxiv.org/abs/2507.18966)
*Saraa Al-Saddik,Manna Elizabeth Philip,Ali Haidar*

Main category: cs.CV

TL;DR: This study evaluates the effectiveness of three state-of-the-art deep learning approaches on a real-world vehicle image dataset. A classification accuracy of 93.70%, 82.86%, 85.19%, and 94.86% was achieved with the best performing make, shape, colour, and colour-binary models respectively.Object detection models outperformed classification-only models. Smaller YOLO variants perform comparably to larger counterparts


<details>
  <summary>Details</summary>
Motivation: Accurate identification of vehicle attributes such as make, colour, and shape is critical for law enforcement and intelligence applications

Method: YOLO-v11, YOLO-World, and YOLO-Classification on a real-world vehicle image dataset. A multi-view inference (MVI) approach was deployed to enhance the performance of the models' predictions

Result: A classification accuracy of 93.70%, 82.86%, 85.19%, and 94.86% was achieved with the best performing make, shape, colour, and colour-binary models respectively

Conclusion: there is a need to use MVI to get usable models within such complex real-world datasets.the object detection models YOLO-v11 and YOLO-World outperformed classification-only models in make and shape extraction. smaller YOLO variants perform comparably to larger counterparts, offering substantial efficiency benefits for real-time predictions. This work provides a robust baseline for extracting vehicle metadata in real-world scenarios

Abstract: Accurate identification of vehicle attributes such as make, colour, and shape
is critical for law enforcement and intelligence applications. This study
evaluates the effectiveness of three state-of-the-art deep learning approaches
YOLO-v11, YOLO-World, and YOLO-Classification on a real-world vehicle image
dataset. This dataset was collected under challenging and unconstrained
conditions by NSW Police Highway Patrol Vehicles. A multi-view inference (MVI)
approach was deployed to enhance the performance of the models' predictions. To
conduct the analyses, datasets with 100,000 plus images were created for each
of the three metadata prediction tasks, specifically make, shape and colour.
The models were tested on a separate dataset with 29,937 images belonging to
1809 number plates. Different sets of experiments have been investigated by
varying the models sizes. A classification accuracy of 93.70%, 82.86%, 85.19%,
and 94.86% was achieved with the best performing make, shape, colour, and
colour-binary models respectively. It was concluded that there is a need to use
MVI to get usable models within such complex real-world datasets. Our findings
indicated that the object detection models YOLO-v11 and YOLO-World outperformed
classification-only models in make and shape extraction. Moreover, smaller YOLO
variants perform comparably to larger counterparts, offering substantial
efficiency benefits for real-time predictions. This work provides a robust
baseline for extracting vehicle metadata in real-world scenarios. Such models
can be used in filtering and sorting user queries, minimising the time required
to search large vehicle images datasets.

</details>


### [73] [LOTUS: A Leaderboard for Detailed Image Captioning from Quality to Societal Bias and User Preferences](https://arxiv.org/abs/2507.19362)
*Yusuke Hirota,Boyi Li,Ryo Hachiuma,Yueh-Hua Wu,Boris Ivanovic,Yuta Nakashima,Marco Pavone,Yejin Choi,Yu-Chiang Frank Wang,Chao-Han Huck Yang*

Main category: cs.CV

TL;DR: Introducing LOTUS, a leaderboard for evaluating detailed captions, addressing three main gaps in existing evaluations: lack of standardized criteria, bias-aware assessments, and user preference considerations.


<details>
  <summary>Details</summary>
Motivation: Lack of standardized criteria, bias-aware assessments, and user preference considerations in existing evaluations for detailed captions.

Method: Introducing LOTUS, a leaderboard for evaluating detailed captions, addressing three main gaps in existing evaluations: lack of standardized criteria, bias-aware assessments, and user preference considerations.

Result: Analysis of recent LVLMs reveals no single model excels across all criteria, while correlations emerge between caption detail and bias risks.

Conclusion: Optimal model selection depends on user priorities.

Abstract: Large Vision-Language Models (LVLMs) have transformed image captioning,
shifting from concise captions to detailed descriptions. We introduce LOTUS, a
leaderboard for evaluating detailed captions, addressing three main gaps in
existing evaluations: lack of standardized criteria, bias-aware assessments,
and user preference considerations. LOTUS comprehensively evaluates various
aspects, including caption quality (e.g., alignment, descriptiveness), risks
(\eg, hallucination), and societal biases (e.g., gender bias) while enabling
preference-oriented evaluations by tailoring criteria to diverse user
preferences. Our analysis of recent LVLMs reveals no single model excels across
all criteria, while correlations emerge between caption detail and bias risks.
Preference-oriented evaluations demonstrate that optimal model selection
depends on user priorities.

</details>


### [74] [AEDR: Training-Free AI-Generated Image Attribution via Autoencoder Double-Reconstruction](https://arxiv.org/abs/2507.18988)
*Chao Wang,Kejiang Chen,Zijin Yang,Yaofei Wang,Weiming Zhang*

Main category: cs.CV

TL;DR: This paper proposes AEDR, a novel training-free attribution method for tracing the origin of images generated by generative models. AEDR achieves higher accuracy and efficiency compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of image-generation technologies has made it possible for anyone to create photorealistic images using generative models, raising significant security concerns. To mitigate malicious use, tracing the origin of such images is essential. Reconstruction-based attribution methods offer a promising solution, but they often suffer from reduced accuracy and high computational costs when applied to state-of-the-art (SOTA) models.

Method: a novel training-free attribution method designed for generative models with continuous autoencoders. It performs two consecutive reconstructions using the model's autoencoder, and adopts the ratio of these two reconstruction losses as the attribution signal. This signal is further calibrated using the image homogeneity metric to improve accuracy

Result: AEDR achieves 25.5% higher attribution accuracy than existing reconstruction-based methods, while requiring only 1% of the computational time. Experiments on eight top latent diffusion models.

Conclusion: AEDR achieves 25.5% higher attribution accuracy than existing reconstruction-based methods, while requiring only 1% of the computational time.

Abstract: The rapid advancement of image-generation technologies has made it possible
for anyone to create photorealistic images using generative models, raising
significant security concerns. To mitigate malicious use, tracing the origin of
such images is essential. Reconstruction-based attribution methods offer a
promising solution, but they often suffer from reduced accuracy and high
computational costs when applied to state-of-the-art (SOTA) models. To address
these challenges, we propose AEDR (AutoEncoder Double-Reconstruction), a novel
training-free attribution method designed for generative models with continuous
autoencoders. Unlike existing reconstruction-based approaches that rely on the
value of a single reconstruction loss, AEDR performs two consecutive
reconstructions using the model's autoencoder, and adopts the ratio of these
two reconstruction losses as the attribution signal. This signal is further
calibrated using the image homogeneity metric to improve accuracy, which
inherently cancels out absolute biases caused by image complexity, with
autoencoder-based reconstruction ensuring superior computational efficiency.
Experiments on eight top latent diffusion models show that AEDR achieves 25.5%
higher attribution accuracy than existing reconstruction-based methods, while
requiring only 1% of the computational time.

</details>


### [75] [MMBench-GUI: Hierarchical Multi-Platform Evaluation Framework for GUI Agents](https://arxiv.org/abs/2507.19478)
*Xuehui Wang,Zhenyu Wu,JingJing Xie,Zichen Ding,Bowen Yang,Zehao Li,Zhaoyang Liu,Qingyun Li,Xuan Dong,Zhe Chen,Weiyun Wang,Xiangyu Zhao,Jixuan Chen,Haodong Duan,Tianbao Xie,Chenyu Yang,Shiqian Su,Yue Yu,Yuan Huang,Yiqian Liu,Xiao Zhang,Yanting Zhang,Xiangyu Yue,Weijie Su,Xizhou Zhu,Wei Shen,Jifeng Dai,Wenhai Wang*

Main category: cs.CV

TL;DR: MMBench-GUI是一个用于评估GUI自动化代理的分层基准，它揭示了精确的可视化定位、任务规划、跨平台泛化和任务效率是关键因素。


<details>
  <summary>Details</summary>
Motivation: 我们介绍了MMBench-GUI，这是一个分层基准，用于评估Windows、macOS、Linux、iOS、Android和Web平台上的GUI自动化代理。

Method: 提出了一个新的效率-质量面积（EQA）指标来评估在线自动化场景中GUI代理的执行效率。

Result: 通过MMBench-GUI，我们确定精确的可视化定位是整体任务成功的关键决定因素，强调了集成专用定位模块的模块化框架的实质性优势。为了实现可靠的GUI自动化，代理需要强大的任务规划和跨平台泛化能力，其中长上下文记忆、广泛的行动空间和长期推理起着关键作用。更重要的是，任务效率仍然是一个严重未被充分探索的维度，所有模型都存在严重的效率低下问题，即使最终完成任务，也会出现过多的冗余步骤。

Conclusion: 精确的定位、有效的计划和提前停止策略对于实现真正高效和可扩展的GUI自动化是不可或缺的。

Abstract: We introduce MMBench-GUI, a hierarchical benchmark for evaluating GUI
automation agents across Windows, macOS, Linux, iOS, Android, and Web
platforms. It comprises four levels: GUI Content Understanding, Element
Grounding, Task Automation, and Task Collaboration, covering essential skills
for GUI agents. In addition, we propose a novel Efficiency-Quality Area (EQA)
metric to assess GUI agent execution efficiency in online automation scenarios.
Through MMBench-GUI, we identify accurate visual grounding as a critical
determinant of overall task success, emphasizing the substantial benefits of
modular frameworks that integrate specialized grounding modules. Furthermore,
to achieve reliable GUI automation, an agent requires strong task planning and
cross-platform generalization abilities, with long-context memory, a broad
action space, and long-term reasoning playing a critical role. More important,
task efficiency remains a critically underexplored dimension, and all models
suffer from substantial inefficiencies, with excessive redundant steps even
when tasks are ultimately completed. The integration of precise localization,
effective planning, and early stopping strategies is indispensable to enable
truly efficient and scalable GUI automation. Our benchmark code, evaluation
data, and running environment will be publicly available at
https://github.com/open-compass/MMBench-GUI.

</details>


### [76] [MedIQA: A Scalable Foundation Model for Prompt-Driven Medical Image Quality Assessment](https://arxiv.org/abs/2507.19004)
*Siyi Xun,Yue Sun,Jingkun Chen,Zitong Yu,Tong Tong,Xiaohong Liu,Mingxiang Wu,Tao Tan*

Main category: cs.CV

TL;DR: MedIQA是第一个医学IQA基础模型，它优于现有方法，并推进了诊断。


<details>
  <summary>Details</summary>
Motivation: 现有的医学IQA方法难以推广到不同的模式和临床场景。

Method: MedIQA集成了显著切片评估模块，以关注诊断相关的区域特征检索，并采用自动提示策略，将上游物理参数预训练与下游专家注释微调对齐。

Result: MedIQA是第一个全面的医学IQA基础模型，旨在处理图像维度、模式、解剖区域和类型的可变性。我们开发了一个大规模的多模式数据集，其中包含大量手动注释的质量分数来支持这一点。

Conclusion: MedIQA显著优于多个下游任务中的基线，为医学IQA建立了一个可扩展的框架，并推进了诊断工作流程和临床决策。

Abstract: Rapid advances in medical imaging technology underscore the critical need for
precise and automated image quality assessment (IQA) to ensure diagnostic
accuracy. Existing medical IQA methods, however, struggle to generalize across
diverse modalities and clinical scenarios. In response, we introduce MedIQA,
the first comprehensive foundation model for medical IQA, designed to handle
variability in image dimensions, modalities, anatomical regions, and types. We
developed a large-scale multi-modality dataset with plentiful manually
annotated quality scores to support this. Our model integrates a salient slice
assessment module to focus on diagnostically relevant regions feature retrieval
and employs an automatic prompt strategy that aligns upstream physical
parameter pre-training with downstream expert annotation fine-tuning. Extensive
experiments demonstrate that MedIQA significantly outperforms baselines in
multiple downstream tasks, establishing a scalable framework for medical IQA
and advancing diagnostic workflows and clinical decision-making.

</details>


### [77] [UPP: Unified Point-Level Prompting for Robust Point Cloud Analysis](https://arxiv.org/abs/2507.18997)
*Zixiang Ai,Zhenyu Cui,Yuxin Peng,Jiahuan Zhou*

Main category: cs.CV

TL;DR: Proposes a unified point-level prompting method for point cloud denoising and completion, enabling robust analysis in a parameter-efficient manner.


<details>
  <summary>Details</summary>
Motivation: Pre-trained point cloud analysis models have shown promising advancements in various downstream tasks, yet their effectiveness is typically suffering from low-quality point cloud (i.e., noise and incompleteness), which is a common issue in real scenarios due to casual object occlusions and unsatisfactory data collected by 3D sensors. To this end, existing methods focus on enhancing point cloud quality by developing dedicated denoising and completion models. However, due to the isolation between the point cloud enhancement and downstream tasks, these methods fail to work in various real-world domains. In addition, the conflicting objectives between denoising and completing tasks further limit the ensemble paradigm to preserve critical geometric features.

Method: a unified point-level prompting method that reformulates point cloud denoising and completion as a prompting mechanism, enabling robust analysis in a parameter-efficient manner. We start by introducing a Rectification Prompter to adapt to noisy points through the predicted rectification vector prompts, effectively filtering noise while preserving intricate geometric features essential for accurate analysis. Sequentially, we further incorporate a Completion Prompter to generate auxiliary point prompts based on the rectified point clouds, facilitating their robustness and adaptability. Finally, a Shape-Aware Unit module is exploited to efficiently unify and capture the filtered geometric features for the downstream point cloud analysis

Result: the superiority and robustness of our method when handling noisy and incomplete point cloud data against existing state-of-the-art methods

Conclusion: Extensive experiments on four datasets demonstrate the superiority and robustness of our method when handling noisy and incomplete point cloud data against existing state-of-the-art methods.

Abstract: Pre-trained point cloud analysis models have shown promising advancements in
various downstream tasks, yet their effectiveness is typically suffering from
low-quality point cloud (i.e., noise and incompleteness), which is a common
issue in real scenarios due to casual object occlusions and unsatisfactory data
collected by 3D sensors. To this end, existing methods focus on enhancing point
cloud quality by developing dedicated denoising and completion models. However,
due to the isolation between the point cloud enhancement and downstream tasks,
these methods fail to work in various real-world domains. In addition, the
conflicting objectives between denoising and completing tasks further limit the
ensemble paradigm to preserve critical geometric features. To tackle the above
challenges, we propose a unified point-level prompting method that reformulates
point cloud denoising and completion as a prompting mechanism, enabling robust
analysis in a parameter-efficient manner. We start by introducing a
Rectification Prompter to adapt to noisy points through the predicted
rectification vector prompts, effectively filtering noise while preserving
intricate geometric features essential for accurate analysis. Sequentially, we
further incorporate a Completion Prompter to generate auxiliary point prompts
based on the rectified point clouds, facilitating their robustness and
adaptability. Finally, a Shape-Aware Unit module is exploited to efficiently
unify and capture the filtered geometric features for the downstream point
cloud analysis.Extensive experiments on four datasets demonstrate the
superiority and robustness of our method when handling noisy and incomplete
point cloud data against existing state-of-the-art methods. Our code is
released at https://github.com/zhoujiahuan1991/ICCV2025-UPP.

</details>


### [78] [GPSMamba: A Global Phase and Spectral Prompt-guided Mamba for Infrared Image Super-Resolution](https://arxiv.org/abs/2507.18998)
*Yongsong Huang,Tomo Miyazaki,Xiaofeng Liu,Shinichiro Omachi*

Main category: cs.CV

TL;DR: GPSMamba, a framework that synergizes architectural guidance with non-causal supervision, mitigates the limitations of causal modeling and achieves state-of-the-art performance in infrared image restoration.


<details>
  <summary>Details</summary>
Motivation: Infrared Image Super-Resolution (IRSR) is challenged by the low contrast and sparse textures of infrared data, requiring robust long-range modeling to maintain global coherence. State-Space Models like Mamba offer proficiency in modeling long-range dependencies for this task, their inherent 1D causal scanning mechanism fragments the global context of 2D images, hindering fine-detail restoration.

Method: Adaptive Semantic-Frequency State Space Module (ASF-SSM) injects a fused semantic-frequency prompt directly into the Mamba block, integrating non-local context to guide reconstruction. A novel Thermal-Spectral Attention and Phase Consistency Loss provides explicit, non-causal supervision to enforce global structural and spectral fidelity.

Result: GPSMamba achieves state-of-the-art performance

Conclusion: GPSMamba achieves state-of-the-art performance, validating our approach as a powerful new paradigm for infrared image restoration.

Abstract: Infrared Image Super-Resolution (IRSR) is challenged by the low contrast and
sparse textures of infrared data, requiring robust long-range modeling to
maintain global coherence. While State-Space Models like Mamba offer
proficiency in modeling long-range dependencies for this task, their inherent
1D causal scanning mechanism fragments the global context of 2D images,
hindering fine-detail restoration. To address this, we propose Global Phase and
Spectral Prompt-guided Mamba (GPSMamba), a framework that synergizes
architectural guidance with non-causal supervision. First, our Adaptive
Semantic-Frequency State Space Module (ASF-SSM) injects a fused
semantic-frequency prompt directly into the Mamba block, integrating non-local
context to guide reconstruction. Then, a novel Thermal-Spectral Attention and
Phase Consistency Loss provides explicit, non-causal supervision to enforce
global structural and spectral fidelity. By combining these two innovations,
our work presents a systematic strategy to mitigate the limitations of causal
modeling. Extensive experiments demonstrate that GPSMamba achieves
state-of-the-art performance, validating our approach as a powerful new
paradigm for infrared image restoration. Code is available at
https://github.com/yongsongH/GPSMamba.

</details>


### [79] [Enhancing Reward Models for High-quality Image Generation: Beyond Text-Image Alignment](https://arxiv.org/abs/2507.19002)
*Ying Ba,Tianyu Zhang,Yalong Bai,Wenyi Mo,Tao Liang,Bing Su,Ji-Rong Wen*

Main category: cs.CV

TL;DR: 当前图像生成评估框架存在缺陷。提出了一种新的评估方法，通过评估图像包含文本内容的程度来提高评分准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的评估框架未能并行发展。研究表明，基于CLIP和BLIP架构微调的人类偏好奖励模型存在固有缺陷：它们不恰当地为具有丰富细节和高审美价值的图像分配低分，与实际的人类审美偏好产生显着差异。

Method: 设计了一种新的评估分数，ICT（图像包含文本）分数，通过评估图像表示文本内容的程度来实现和超越文本-图像对齐的目标。在此基础上，我们进一步训练了一个仅使用图像模态的HP（高偏好）评分模型，以增强图像美学和细节质量，同时保持文本-图像对齐。

Result: 所提出的评估模型比现有方法提高了10％以上的评分准确率，并在优化最先进的文本到图像模型方面取得了显着成果。

Conclusion: 该研究提供理论和实践支持，以推动图像生成技术朝着更高阶的人类审美偏好发展。

Abstract: Contemporary image generation systems have achieved high fidelity and
superior aesthetic quality beyond basic text-image alignment. However, existing
evaluation frameworks have failed to evolve in parallel. This study reveals
that human preference reward models fine-tuned based on CLIP and BLIP
architectures have inherent flaws: they inappropriately assign low scores to
images with rich details and high aesthetic value, creating a significant
discrepancy with actual human aesthetic preferences. To address this issue, we
design a novel evaluation score, ICT (Image-Contained-Text) score, that
achieves and surpasses the objectives of text-image alignment by assessing the
degree to which images represent textual content. Building upon this
foundation, we further train an HP (High-Preference) score model using solely
the image modality to enhance image aesthetics and detail quality while
maintaining text-image alignment. Experiments demonstrate that the proposed
evaluation model improves scoring accuracy by over 10\% compared to existing
methods, and achieves significant results in optimizing state-of-the-art
text-to-image models. This research provides theoretical and empirical support
for evolving image generation technology toward higher-order human aesthetic
preferences. Code is available at https://github.com/BarretBa/ICTHP.

</details>


### [80] [A Survey of Multimodal Hallucination Evaluation and Detection](https://arxiv.org/abs/2507.19024)
*Zhiyuan Chen,Yuecong Min,Jie Zhang,Bei Yan,Jiahao Wang,Xiaozhen Wang,Shiguang Shan*

Main category: cs.CV

TL;DR: 本研究概述了多模态大型语言模型中幻觉评估基准和检测方法，强调了它们的局限性，并为未来的研究提出了方向。


<details>
  <summary>Details</summary>
Motivation: 多模态大型语言模型 (MLLM) 已成为集成视觉和文本信息的强大范例，支持范围广泛的多模态任务。然而，这些模型经常出现幻觉问题，产生的内容看似合理，但与输入内容或已建立的世界知识相矛盾。

Method: 对图像到文本 (I2T) 和文本到图像 (T2I) 生成任务中的幻觉评估基准和检测方法进行了深入的回顾。

Result: 该研究提出了基于忠实度和事实性的幻觉分类法，总结了 T2I 和 I2T 任务的现有幻觉评估基准，并概述了幻觉检测方法的最新进展。

Conclusion: 这篇综述总结了现有基准和检测方法的关键局限性，并概述了未来研究的潜在方向。

Abstract: Multi-modal Large Language Models (MLLMs) have emerged as a powerful paradigm
for integrating visual and textual information, supporting a wide range of
multi-modal tasks. However, these models often suffer from hallucination,
producing content that appears plausible but contradicts the input content or
established world knowledge. This survey offers an in-depth review of
hallucination evaluation benchmarks and detection methods across Image-to-Text
(I2T) and Text-to-image (T2I) generation tasks. Specifically, we first propose
a taxonomy of hallucination based on faithfulness and factuality, incorporating
the common types of hallucinations observed in practice. Then we provide an
overview of existing hallucination evaluation benchmarks for both T2I and I2T
tasks, highlighting their construction process, evaluation objectives, and
employed metrics. Furthermore, we summarize recent advances in hallucination
detection methods, which aims to identify hallucinated content at the instance
level and serve as a practical complement of benchmark-based evaluation.
Finally, we highlight key limitations in current benchmarks and detection
methods, and outline potential directions for future research.

</details>


### [81] [MedSymmFlow: Bridging Generative Modeling and Classification in Medical Imaging through Symmetrical Flow Matching](https://arxiv.org/abs/2507.19098)
*Francisco Caetano,Lemar Abdi,Christiaan Viviers,Amaan Valiuddin,Fons van der Sommen*

Main category: cs.CV

TL;DR: MedSymmFlow是一个用于医学图像分类、生成和不确定性量化的混合模型，它在准确性和不确定性估计方面都表现出色。


<details>
  <summary>Details</summary>
Motivation: 可靠的医学图像分类需要准确的预测和良好校准的不确定性估计，尤其是在高风险的临床环境中。

Method: MedSymmFlow，一个建立在对称流匹配上的生成-判别混合模型，旨在统一医学图像中的分类、生成和不确定性量化。MedSymmFlow利用潜在空间公式，可以扩展到高分辨率输入，并引入语义掩码调节机制，以增强诊断相关性。

Result: MedSymmFlow在四个MedMNIST数据集上进行了评估，涵盖了一系列模式和病理。

Conclusion: MedSymmFlow在分类准确率和AUC方面与已建立的基线相匹配或超过了它们的性能，同时提供了可靠的不确定性估计，并通过选择性预测下的性能改进得到了验证。

Abstract: Reliable medical image classification requires accurate predictions and
well-calibrated uncertainty estimates, especially in high-stakes clinical
settings. This work presents MedSymmFlow, a generative-discriminative hybrid
model built on Symmetrical Flow Matching, designed to unify classification,
generation, and uncertainty quantification in medical imaging. MedSymmFlow
leverages a latent-space formulation that scales to high-resolution inputs and
introduces a semantic mask conditioning mechanism to enhance diagnostic
relevance. Unlike standard discriminative models, it naturally estimates
uncertainty through its generative sampling process. The model is evaluated on
four MedMNIST datasets, covering a range of modalities and pathologies. The
results show that MedSymmFlow matches or exceeds the performance of established
baselines in classification accuracy and AUC, while also delivering reliable
uncertainty estimates validated by performance improvements under selective
prediction.

</details>


### [82] [A New One-Shot Federated Learning Framework for Medical Imaging Classification with Feature-Guided Rectified Flow and Knowledge Distillation](https://arxiv.org/abs/2507.19045)
*Yufei Ma,Hanwen Zhang,Qiya Yang,Guibo Luo,Yuesheng Zhu*

Main category: cs.CV

TL;DR: This paper proposes a modified OSFL framework with FG-RF and DLKD to improve training efficiency, reduce privacy leakage, and handle non-IID data in medical imaging. The proposed method outperforms existing federated learning approaches.


<details>
  <summary>Details</summary>
Motivation: Existing generative model-based OSFL methods suffer from low training efficiency and potential privacy leakage in the healthcare domain. Achieving convergence within a single round of model aggregation is challenging under non-Independent and Identically Distributed (non-IID) data.

Method: A modified OSFL framework with a new Feature-Guided Rectified Flow Model (FG-RF) and Dual-Layer Knowledge Distillation (DLKD) aggregation method.

Result: FG-RF accelerates generative modeling in medical imaging scenarios while preserving privacy by synthesizing feature-level images rather than pixel-level images. DLKD enables the global student model to simultaneously mimic the output logits and align the intermediate-layer features of client-side teacher models during aggregation.

Conclusion: The proposed framework outperforms multi-round federated learning approaches, achieving up to 21.73% improvement, and exceeds the baseline FedISCA by an average of 21.75%. Feature-level synthetic images significantly reduce privacy leakage risks compared to pixel-level synthetic images.

Abstract: In multi-center scenarios, One-Shot Federated Learning (OSFL) has attracted
increasing attention due to its low communication overhead, requiring only a
single round of transmission. However, existing generative model-based OSFL
methods suffer from low training efficiency and potential privacy leakage in
the healthcare domain. Additionally, achieving convergence within a single
round of model aggregation is challenging under non-Independent and Identically
Distributed (non-IID) data. To address these challenges, in this paper a
modified OSFL framework is proposed, in which a new Feature-Guided Rectified
Flow Model (FG-RF) and Dual-Layer Knowledge Distillation (DLKD) aggregation
method are developed. FG-RF on the client side accelerates generative modeling
in medical imaging scenarios while preserving privacy by synthesizing
feature-level images rather than pixel-level images. To handle non-IID
distributions, DLKD enables the global student model to simultaneously mimic
the output logits and align the intermediate-layer features of client-side
teacher models during aggregation. Experimental results on three non-IID
medical imaging datasets show that our new framework and method outperform
multi-round federated learning approaches, achieving up to 21.73% improvement,
and exceeds the baseline FedISCA by an average of 21.75%. Furthermore, our
experiments demonstrate that feature-level synthetic images significantly
reduce privacy leakage risks compared to pixel-level synthetic images.

</details>


### [83] [Probing Multimodal Fusion in the Brain: The Dominance of Audiovisual Streams in Naturalistic Encoding](https://arxiv.org/abs/2507.19052)
*Hamid Abdollahi,Amir Hossein Mansouri Majoumerd,Amir Hossein Bagheri Baboukani,Amir Abolfazl Suratgar,Mohammad Bagher Menhaj*

Main category: cs.CV

TL;DR: 开发了大脑编码模型，发现模型复杂性和泛化之间存在权衡，并且严格的异分布 (OOD) 测试对于构建稳健的神经人工智能模型至关重要。


<details>
  <summary>Details</summary>
Motivation: 预测大脑对自然、多模态刺激的反应是计算神经科学中的一个关键挑战。虽然编码模型正变得越来越强大，但它们推广到真正新颖环境的能力仍然是一个关键的、经常未经检验的问题。

Method: 使用最先进的视觉 (X-CLIP) 和听觉 (Whisper) 特征提取器开发了大脑编码模型，并在同分布 (ID) 和不同的异分布 (OOD) 数据上严格评估它们。

Result: 更高容量的基于注意力的模型在 ID 数据上表现出色，但更简单的线性模型更稳健，在 OOD 集上优于竞争基线 18%。语言特征没有提高预测准确性，这表明对于熟悉的语言，神经编码可能主要由连续的视觉和听觉流控制，而不是冗余的文本信息。在空间上，该方法在听觉皮层中显示出显着的性能提升，突出了高保真语音表示的优势。

Conclusion: 严格的OOD测试对于构建稳健的神经人工智能模型至关重要，并为模型架构、刺激特征和感觉层级如何塑造我们丰富的多模式世界的神经编码提供了细致的见解。

Abstract: Predicting brain activity in response to naturalistic, multimodal stimuli is
a key challenge in computational neuroscience. While encoding models are
becoming more powerful, their ability to generalize to truly novel contexts
remains a critical, often untested, question. In this work, we developed brain
encoding models using state-of-the-art visual (X-CLIP) and auditory (Whisper)
feature extractors and rigorously evaluated them on both in-distribution (ID)
and diverse out-of-distribution (OOD) data. Our results reveal a fundamental
trade-off between model complexity and generalization: a higher-capacity
attention-based model excelled on ID data, but a simpler linear model was more
robust, outperforming a competitive baseline by 18\% on the OOD set.
Intriguingly, we found that linguistic features did not improve predictive
accuracy, suggesting that for familiar languages, neural encoding may be
dominated by the continuous visual and auditory streams over redundant textual
information. Spatially, our approach showed marked performance gains in the
auditory cortex, underscoring the benefit of high-fidelity speech
representations. Collectively, our findings demonstrate that rigorous OOD
testing is essential for building robust neuro-AI models and provides nuanced
insights into how model architecture, stimulus characteristics, and sensory
hierarchies shape the neural encoding of our rich, multimodal world.

</details>


### [84] [PatchTraj: Dynamic Patch Representation Learning for Time-Frequency Trajectory Prediction](https://arxiv.org/abs/2507.19119)
*Yanghong Liu,Xingping Dong,Ming Li,Weixing Zhang,Yidong Lou*

Main category: cs.CV

TL;DR: propose PatchTraj, a dynamic patch-based trajectory prediction framework that unifies time-domain and frequency-domain representations and achieves state-of-the-art performance with high efficiency.


<details>
  <summary>Details</summary>
Motivation: existing point-based and grid-based methods expose two key limitations: insufficiently modeling human motion dynamics, as they fail to balance local motion details with long-range spatiotemporal dependencies, and the time representation lacks interaction with the frequency domain in modeling trajectory sequences

Method: propose PatchTraj, a dynamic patch-based trajectory prediction framework that unifies time-domain and frequency-domain representations. It decomposes the trajectory into raw time sequences and frequency components, employing dynamic patch partitioning for multi-scale trajectory segmentation to capture hierarchical motion patterns. Each patch is processed by an adaptive embedding layer with scale-aware feature extraction, followed by hierarchical feature aggregation to model both fine-grained and long-range dependencies. The outputs of two branches interact via cross-modal attention, enabling complementary fusion of temporal and spectral cues. Finally, a Transformer encoder-decoder integrates both modalities to autoregressively predict future trajectories.

Result: achieves state-of-the-art performance with high efficiency

Conclusion: The method achieves state-of-the-art performance with high efficiency on ETH-UCY, SDD, NBA, and JRDB datasets.

Abstract: Pedestrian trajectory prediction is crucial for autonomous driving and
robotics. While existing point-based and grid-based methods expose two key
limitations: insufficiently modeling human motion dynamics, as they fail to
balance local motion details with long-range spatiotemporal dependencies, and
the time representation lacks interaction with the frequency domain in modeling
trajectory sequences. To address these challenges, we propose PatchTraj, a
dynamic patch-based trajectory prediction framework that unifies time-domain
and frequency-domain representations. Specifically, we decompose the trajectory
into raw time sequences and frequency components, employing dynamic patch
partitioning for multi-scale trajectory segmentation to capture hierarchical
motion patterns. Each patch is processed by an adaptive embedding layer with
scale-aware feature extraction, followed by hierarchical feature aggregation to
model both fine-grained and long-range dependencies. The outputs of two
branches interact via cross-modal attention, enabling complementary fusion of
temporal and spectral cues. Finally, a Transformer encoder-decoder integrates
both modalities to autoregressively predict future trajectories. Extensive
experiments on ETH-UCY, SDD, NBA, and JRDB datasets demonstrate that our method
achieves state-of-the-art performance with high efficiency.

</details>


### [85] [ScenePainter: Semantically Consistent Perpetual 3D Scene Generation with Concept Relation Alignment](https://arxiv.org/abs/2507.19058)
*Chong Xia,Shengjun Zhang,Fangfu Liu,Chang Liu,Khodchaphun Hirunyaratsameewong,Yueqi Duan*

Main category: cs.CV

TL;DR: 提出 ScenePainter 框架，通过 SceneConceptGraph 解决 3D 场景生成中的语义漂移问题，生成更一致的视图。


<details>
  <summary>Details</summary>
Motivation: 现有的方法遵循“导航和想象”的方式，并依赖外绘进行连续的视图扩展。然而，生成的视图序列存在语义漂移问题，这是由于外绘模块的累积偏差造成的。

Method: 提出了一种名为 ScenePainter 的新框架，用于语义一致的 3D 场景生成，该框架将外绘器的场景特定先验与当前场景的理解对齐。引入了一个分层图结构 SceneConceptGraph 来构建多层次场景概念之间的关系，从而指导外绘器生成一致的新颖视图，并且可以动态地细化以增强多样性。

Result: 大量的实验表明，我们的框架克服了语义漂移问题，并生成了更一致和沉浸式的 3D 视图序列。

Conclusion: 该框架通过引入分层图结构 SceneConceptGraph 来构建多层次场景概念之间的关系，从而克服了语义漂移问题，并生成了更一致和沉浸式的 3D 视图序列。

Abstract: Perpetual 3D scene generation aims to produce long-range and coherent 3D view
sequences, which is applicable for long-term video synthesis and 3D scene
reconstruction. Existing methods follow a "navigate-and-imagine" fashion and
rely on outpainting for successive view expansion. However, the generated view
sequences suffer from semantic drift issue derived from the accumulated
deviation of the outpainting module. To tackle this challenge, we propose
ScenePainter, a new framework for semantically consistent 3D scene generation,
which aligns the outpainter's scene-specific prior with the comprehension of
the current scene. To be specific, we introduce a hierarchical graph structure
dubbed SceneConceptGraph to construct relations among multi-level scene
concepts, which directs the outpainter for consistent novel views and can be
dynamically refined to enhance diversity. Extensive experiments demonstrate
that our framework overcomes the semantic drift issue and generates more
consistent and immersive 3D view sequences. Project Page:
https://xiac20.github.io/ScenePainter/.

</details>


### [86] [Revisiting DETR for Small Object Detection via Noise-Resilient Query Optimization](https://arxiv.org/abs/2507.19059)
*Xiaocheng Fang,Jieyi Cai,Huanyu Liu,Wenxiu Cai,Yishu Liu,Bingzhi Chen*

Main category: cs.CV

TL;DR: 提出了一种新的噪声弹性查询优化(NRQO)范例，该范例结合了噪声容错特征金字塔网络(NT-FPN)和成对相似性区域提议网络(PS-RPN)。


<details>
  <summary>Details</summary>
Motivation: 基于Transformer的SOD检测器由于特征金字塔网络(FPN)中固有的噪声敏感性和现有标签分配策略中查询质量的下降而面临挑战。

Method: Noise-Tolerance Feature Pyramid Network (NT-FPN)和Pairwise-Similarity Region Proposal Network (PS-RPN)。

Result: NT-FPN通过保持空间和语义信息的完整性来减轻FPN中特征融合期间的噪声。PS-RPN通过增强锚点-地面真值匹配(通过位置和形状相似性)来生成足够数量的高质量正查询，而不需要额外的超参数。

Conclusion: NRQO在多个基准测试中始终优于最先进的基线。

Abstract: Despite advancements in Transformer-based detectors for small object
detection (SOD), recent studies show that these detectors still face challenges
due to inherent noise sensitivity in feature pyramid networks (FPN) and
diminished query quality in existing label assignment strategies. In this
paper, we propose a novel Noise-Resilient Query Optimization (NRQO) paradigm,
which innovatively incorporates the Noise-Tolerance Feature Pyramid Network
(NT-FPN) and the Pairwise-Similarity Region Proposal Network (PS-RPN).
Specifically, NT-FPN mitigates noise during feature fusion in FPN by preserving
spatial and semantic information integrity. Unlike existing label assignment
strategies, PS-RPN generates a sufficient number of high-quality positive
queries by enhancing anchor-ground truth matching through position and shape
similarities, without the need for additional hyperparameters. Extensive
experiments on multiple benchmarks consistently demonstrate the superiority of
NRQO over state-of-the-art baselines.

</details>


### [87] [Negation-Aware Test-Time Adaptation for Vision-Language Models](https://arxiv.org/abs/2507.19064)
*Haochen Han,Alex Jinpeng Wang,Fangming Liu*

Main category: cs.CV

TL;DR: This paper studies negation understanding in Vision-Language Models (VLMs) and proposes a Negation-Aware Test-Time Adaptation (NEAT) method to efficiently adjust distribution-related parameters during inference.


<details>
  <summary>Details</summary>
Motivation: many real-world applications require models to explicitly identify what is false or non-existent, existing methods attribute its root cause to the scarcity of negation training data and propose to fine-tune VLMs on massive data containing explicit negation. Undoubtedly, such data-centric solutions demand substantial data and computational resources, limiting their sustainable widespread adoption

Method: propose a Negation-Aware Test-Time Adaptation (NEAT) method to efficiently adjust distribution-related parameters during inference

Result: NEAT can reduce distribution shift in consistent semantics while eliminating false distributional consistency in unrelated semantics

Conclusion: Extensive experiments on the various negation understanding tasks verify the effectiveness of the proposed method.

Abstract: In this paper, we study a practical but less-touched problem in
Vision-Language Models (VLMs), \ie, negation understanding. Specifically, many
real-world applications require models to explicitly identify what is false or
non-existent, \eg, radiologists may search for images that exclude specific
conditions. Despite the impressive transferability of VLMs through large-scale
training, they suffer from a critical limitation that fails to handle negation.
To address this challenge, existing methods attribute its root cause to the
scarcity of negation training data and propose to fine-tune VLMs on massive
data containing explicit negation. Undoubtedly, such data-centric solutions
demand substantial data and computational resources, limiting their sustainable
widespread adoption. To tackle negation in a low-carbon manner, we empirically
observe that the key obstacle lies in the dual-concept shifts between the
affirmation and negation distributions. Therefore, we propose a Negation-Aware
Test-Time Adaptation (NEAT) method to efficiently adjust distribution-related
parameters during inference. In brief, NEAT can reduce distribution shift in
consistent semantics while eliminating false distributional consistency in
unrelated semantics. Extensive experiments on the various negation
understanding tasks verify the effectiveness of the proposed method. The code
is available at https://github.com/hhc1997/NEAT.

</details>


### [88] [Cross-Subject Mind Decoding from Inaccurate Representations](https://arxiv.org/abs/2507.19071)
*Yangyang Xu,Bangzhen Liu,Wenqi Shao,Yong Du,Shengfeng He,Tingting Zhu*

Main category: cs.CV

TL;DR: 提出了一种用于精确解码表示预测的双向自编码器交织框架，该框架优于最先进的方法，并且对新主题具有很强的适应性，只需最少的训练样本。


<details>
  <summary>Details</summary>
Motivation: 由于认知变异性和受试者特定差异，现有的方法在跨受试者映射方面存在困难。这种挑战源于顺序误差，其中单向映射生成部分不准确的表示，当馈入扩散模型时，会累积误差并降低重建保真度。

Method: 双向自编码器交织框架，通过主题偏差调制模块统一多个主题，并利用双向映射来更好地捕获数据分布以进行精确的表征预测。引入了语义细化模块和视觉连贯性模块，以提高语义表征并减轻不准确的视觉表征的影响。与 ControlNet 和 Stable Diffusion 集成。

Result: 该方法在定性和定量评估中均优于基准数据集上的现有技术方法。

Conclusion: 该框架在基准数据集上优于最先进的方法，并且对新主题具有很强的适应性，只需最少的训练样本。

Abstract: Decoding stimulus images from fMRI signals has advanced with pre-trained
generative models. However, existing methods struggle with cross-subject
mappings due to cognitive variability and subject-specific differences. This
challenge arises from sequential errors, where unidirectional mappings generate
partially inaccurate representations that, when fed into diffusion models,
accumulate errors and degrade reconstruction fidelity. To address this, we
propose the Bidirectional Autoencoder Intertwining framework for accurate
decoded representation prediction. Our approach unifies multiple subjects
through a Subject Bias Modulation Module while leveraging bidirectional mapping
to better capture data distributions for precise representation prediction. To
further enhance fidelity when decoding representations into stimulus images, we
introduce a Semantic Refinement Module to improve semantic representations and
a Visual Coherence Module to mitigate the effects of inaccurate visual
representations. Integrated with ControlNet and Stable Diffusion, our method
outperforms state-of-the-art approaches on benchmark datasets in both
qualitative and quantitative evaluations. Moreover, our framework exhibits
strong adaptability to new subjects with minimal training samples.

</details>


### [89] [SP-Mamba: Spatial-Perception State Space Model for Unsupervised Medical Anomaly Detection](https://arxiv.org/abs/2507.19076)
*Rui Pan,Ruiying Lu*

Main category: cs.CV

TL;DR: SP-Mamba, a Mamba-based framework, is introduced for unsupervised medical anomaly detection, achieving state-of-the-art performance by exploiting structural regularity in medical images.


<details>
  <summary>Details</summary>
Motivation: Medical images have consistent structural patterns, but CNNs have limitations in capturing long-range dependencies, and transformers have quadratic computational complexity. Mamba-based models offer a promising alternative due to their superior long-range modeling, structural feature extraction, and linear computational efficiency.

Method: The study introduces SP-Mamba, a spatial-perception Mamba framework with window-sliding prototype learning and Circular-Hilbert scanning-based Mamba.

Result: Extensive experiments on three diverse medical anomaly detection benchmarks confirm the proposed method's state-of-the-art performance.

Conclusion: The proposed SP-Mamba method achieves state-of-the-art performance on three medical anomaly detection benchmarks, demonstrating its effectiveness and robustness.

Abstract: Radiography imaging protocols target on specific anatomical regions,
resulting in highly consistent images with recurrent structural patterns across
patients. Recent advances in medical anomaly detection have demonstrated the
effectiveness of CNN- and transformer-based approaches. However, CNNs exhibit
limitations in capturing long-range dependencies, while transformers suffer
from quadratic computational complexity. In contrast, Mamba-based models,
leveraging superior long-range modeling, structural feature extraction, and
linear computational efficiency, have emerged as a promising alternative. To
capitalize on the inherent structural regularity of medical images, this study
introduces SP-Mamba, a spatial-perception Mamba framework for unsupervised
medical anomaly detection. The window-sliding prototype learning and
Circular-Hilbert scanning-based Mamba are introduced to better exploit
consistent anatomical patterns and leverage spatial information for medical
anomaly detection. Furthermore, we excavate the concentration and contrast
characteristics of anomaly maps for improving anomaly detection. Extensive
experiments on three diverse medical anomaly detection benchmarks confirm the
proposed method's state-of-the-art performance, validating its efficacy and
robustness. The code is available at https://github.com/Ray-RuiPan/SP-Mamba.

</details>


### [90] [Multi-Task Dense Prediction Fine-Tuning with Mixture of Fine-Grained Experts](https://arxiv.org/abs/2507.19077)
*Yangyang Xu,Xi Ye,Duo Su*

Main category: cs.CV

TL;DR: FGMoE: A Fine-Grained Mixture of Experts architecture for multi-task learning that improves performance with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: balancing shared representations with task-specific specialization in multi-task learning (MTL) for dense prediction

Method: a novel Fine-Grained Mixture of Experts (FGMoE) architecture that explores MoE-based MTL models through a combination of three key innovations and fine-tuning

Result: the proposed FGMoE uses fewer parameters and significantly outperforms current MoE-based competitive MTL models on two dense prediction datasets

Conclusion: FGMoE uses fewer parameters and significantly outperforms current MoE-based competitive MTL models on two dense prediction datasets

Abstract: Multi-task learning (MTL) for dense prediction has shown promising results
but still faces challenges in balancing shared representations with
task-specific specialization. In this paper, we introduce a novel Fine-Grained
Mixture of Experts (FGMoE) architecture that explores MoE-based MTL models
through a combination of three key innovations and fine-tuning. First, we
propose intra-task experts that partition along intermediate hidden dimensions
of MLPs, enabling finer decomposition of task information while maintaining
parameter efficiency. Second, we introduce shared experts that consolidate
common information across different contexts of the same task, reducing
redundancy, and allowing routing experts to focus on unique aspects. Third, we
design a global expert that facilitates adaptive knowledge transfer across
tasks based on both input feature and task requirements, promoting beneficial
information sharing while preventing harmful interference. In addition, we use
the fine-tuning approach to improve parameter efficiency only by training the
parameters of the decoder. Extensive experimental results show that the
proposed FGMoE uses fewer parameters and significantly outperforms current
MoE-based competitive MTL models on two dense prediction datasets
(\textit{i.e.,} NYUD-v2, PASCAL-Context) in various metrics.

</details>


### [91] [Multistream Network for LiDAR and Camera-based 3D Object Detection in Outdoor Scenes](https://arxiv.org/abs/2507.19304)
*Muhammad Ibrahim,Naveed Akhtar,Haitian Wang,Saeed Anwar,Ajmal Mian*

Main category: cs.CV

TL;DR: MuStD network fuses LiDAR and RGB data for accurate and efficient 3D object detection, achieving state-of-the-art results on KITTI.


<details>
  <summary>Details</summary>
Motivation: Effective integration of LiDAR and RGB data for precise object detection in outdoor 3D object detection tasks remains an open problem.

Method: A Multi-Stream Detection (MuStD) network with three streams: LiDAR-PillarNet, LiDAR-Height Compression, and a 3D Multimodal stream combining RGB and LiDAR features using UV mapping and polar coordinate indexing.

Result: The MuStD network achieves state-of-the-art or highly competitive results on the KITTI Object Detection Benchmark.

Conclusion: The MuStD network achieves state-of-the-art or highly competitive results on the KITTI Object Detection Benchmark while remaining efficient.

Abstract: Fusion of LiDAR and RGB data has the potential to enhance outdoor 3D object
detection accuracy. To address real-world challenges in outdoor 3D object
detection, fusion of LiDAR and RGB input has started gaining traction. However,
effective integration of these modalities for precise object detection task
still remains a largely open problem. To address that, we propose a MultiStream
Detection (MuStD) network, that meticulously extracts task-relevant information
from both data modalities. The network follows a three-stream structure. Its
LiDAR-PillarNet stream extracts sparse 2D pillar features from the LiDAR input
while the LiDAR-Height Compression stream computes Bird's-Eye View features. An
additional 3D Multimodal stream combines RGB and LiDAR features using UV
mapping and polar coordinate indexing. Eventually, the features containing
comprehensive spatial, textural and geometric information are carefully fused
and fed to a detection head for 3D object detection. Our extensive evaluation
on the challenging KITTI Object Detection Benchmark using public testing server
at
https://www.cvlibs.net/datasets/kitti/eval_object_detail.php?&result=d162ec699d6992040e34314d19ab7f5c217075e0
establishes the efficacy of our method by achieving new state-of-the-art or
highly competitive results in different categories while remaining among the
most efficient methods. Our code will be released through MuStD GitHub
repository at https://github.com/IbrahimUWA/MuStD.git

</details>


### [92] [SIDE: Sparse Information Disentanglement for Explainable Artificial Intelligence](https://arxiv.org/abs/2507.19321)
*Viktar Dubovik,Łukasz Struski,Jacek Tabor,Dawid Rymarczyk*

Main category: cs.CV

TL;DR: SIDE提高了原型部分的可解释性，同时保持了准确性。


<details>
  <summary>Details</summary>
Motivation: 理解深度神经网络所做的决策在医疗成像和自动驾驶等高风险领域至关重要。然而，这些模型通常缺乏透明度，尤其是在计算机视觉中。基于原型的神经网络已经成为一种有前途的解决方案，通过提供概念级的解释。然而，大多数都局限于细粒度的分类任务，InfoDisent是少数例外。InfoDisent将原型模型扩展到像ImageNet这样的大规模数据集，但会产生复杂的解释。

Method: 一种新颖的方法，通过专门的训练和剪枝方案来提高原型部分的可解释性，该方案强制执行稀疏性。结合sigmoid激活代替softmax，这种方法允许SIDE将每个类与一小部分相关的原型相关联。

Result: SIDE在匹配现有方法准确率的同时，将解释规模缩小了90%以上。

Conclusion: SIDE在匹配现有方法准确率的同时，将解释规模缩小了90%以上，大大提高了基于原型的解释的可理解性。

Abstract: Understanding the decisions made by deep neural networks is essential in
high-stakes domains such as medical imaging and autonomous driving. Yet, these
models often lack transparency, particularly in computer vision.
Prototypical-parts-based neural networks have emerged as a promising solution
by offering concept-level explanations. However, most are limited to
fine-grained classification tasks, with few exceptions such as InfoDisent.
InfoDisent extends prototypical models to large-scale datasets like ImageNet,
but produces complex explanations.
  We introduce Sparse Information Disentanglement for Explainability (SIDE), a
novel method that improves the interpretability of prototypical parts through a
dedicated training and pruning scheme that enforces sparsity. Combined with
sigmoid activations in place of softmax, this approach allows SIDE to associate
each class with only a small set of relevant prototypes. Extensive experiments
show that SIDE matches the accuracy of existing methods while reducing
explanation size by over $90\%$, substantially enhancing the understandability
of prototype-based explanations.

</details>


### [93] [LISA: A Layer-wise Integration and Suppression Approach for Hallucination Mitigation in Multimodal Large Language Models](https://arxiv.org/abs/2507.19110)
*Zhihui Guo,Xin Man,Hui Xu,Jie Shao*

Main category: cs.CV

TL;DR: LISA通过分层调制和多层融合来减少 MLLM 中的幻觉。


<details>
  <summary>Details</summary>
Motivation: 多模态大型语言模型 (MLLM) 在图像描述等视觉语言任务中表现出色，但仍然容易出现对象幻觉，即它们描述了图像中未出现的对象。

Method: LISA，一种分层集成和抑制方法，通过分层调制和多层融合来增强生成一致性。

Result: LISA在多个基准测试中将幻觉减少了高达 53.6%，并在 POPE F1 中提高了 4.5%。

Conclusion: LISA在多个基准测试中将幻觉减少了高达 53.6%，并在 POPE F1 中提高了 4.5%，证明了跨模型和任务的强大泛化能力。

Abstract: Multimodal Large Language Models (MLLMs) excel in vision-language tasks such
as image captioning but remain prone to object hallucinations, where they
describe objects that do not appear in the image. To mitigate this, we propose
\textbf{LISA}, a \textbf{L}ayer-wise \textbf{I}ntegration and
\textbf{S}uppression \textbf{A}pproach that enhances generation consistency
through hierarchical modulation and multi-layer fusion. LISA leverages the
functional hierarchy within MLLMs, where shallow layers provide visual
grounding, middle layers encode semantics, and deep layers tend to amplify
spurious signals. First, zone-specific spectral modulation stabilizes attention
by suppressing over-amplified activations in deeper layers while preserving
alignment cues in earlier layers. Second, token-level logits from selected
layers are fused via anchor-based routing, with token-wise anchor selection and
soft logit fusion enabling adaptive integration during decoding. LISA is fully
\textbf{plug-and-play} and can be seamlessly integrated into existing MLLMs,
including Qwen2.5-VL. Experiments on multiple benchmarks show that LISA reduces
hallucinations by up to 53.6\% in $\mathrm{CHAIR}_I$ and improves POPE F1 by
4.5\%, demonstrating strong generalization across models and tasks.

</details>


### [94] [Cross Spatial Temporal Fusion Attention for Remote Sensing Object Detection via Image Feature Matching](https://arxiv.org/abs/2507.19118)
*Abu Sadat Mohammad Salehin Amit,Xiaoli Zhang,Md Masum Billa Shagar,Zhaojun Liu,Xiongfei Li,Fanlong Meng*

Main category: cs.CV

TL;DR: 提出了一种跨时空融合（CSTF）机制，用于增强跨模态遥感图像匹配的特征表示，并在目标检测任务中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 由于多模态图像之间存在显著的几何和辐射差异，因此有效描述跨模态遥感图像匹配的特征仍然是一项具有挑战性的任务。现有方法主要在全连接层提取特征，但通常无法有效捕获跨模态相似性。

Method: 提出了一种跨时空融合（CSTF）机制，通过整合参考图像和查询图像中独立检测到的尺度不变关键点来增强特征表示。

Result: CSTF在特征匹配方面有所改进，它创建了利用来自多个图像区域的信息的对应图，并将相似性匹配过程重新定义为使用SoftMax和全卷积网络（FCN）层的分类任务。

Conclusion: CSTF在HRSC2016和DOTA数据集上取得了最先进的性能，平均mAP分别为90.99%和90.86%，超过了现有模型。推理速度为12.5 FPS，验证了该方法能有效提升下游遥感应用，如目标检测。

Abstract: Effectively describing features for cross-modal remote sensing image matching
remains a challenging task due to the significant geometric and radiometric
differences between multimodal images. Existing methods primarily extract
features at the fully connected layer but often fail to capture cross-modal
similarities effectively. We propose a Cross Spatial Temporal Fusion (CSTF)
mechanism that enhances feature representation by integrating scale-invariant
keypoints detected independently in both reference and query images. Our
approach improves feature matching in two ways: First, by creating
correspondence maps that leverage information from multiple image regions
simultaneously, and second, by reformulating the similarity matching process as
a classification task using SoftMax and Fully Convolutional Network (FCN)
layers. This dual approach enables CSTF to maintain sensitivity to distinctive
local features while incorporating broader contextual information, resulting in
robust matching across diverse remote sensing modalities. To demonstrate the
practical utility of improved feature matching, we evaluate CSTF on object
detection tasks using the HRSC2016 and DOTA benchmark datasets. Our method
achieves state-of-theart performance with an average mAP of 90.99% on HRSC2016
and 90.86% on DOTA, outperforming existing models. The CSTF model maintains
computational efficiency with an inference speed of 12.5 FPS. These results
validate that our approach to crossmodal feature matching directly enhances
downstream remote sensing applications such as object detection.

</details>


### [95] [Preserving Topological and Geometric Embeddings for Point Cloud Recovery](https://arxiv.org/abs/2507.19121)
*Kaiyue Zhou,Zelong Tan,Hongxiao Wang,Ya-li Li,Shengjin Wang*

Main category: cs.CV

TL;DR: 提出了 TopGeoFormer，一种端到端架构，用于点云恢复，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的点云恢复方法难以有效地利用拓扑和几何属性。

Method: 提出了一个名为 TopGeoFormer 的端到端架构，它在采样和恢复阶段保持这些关键特征。

Result: 定量和定性的结果表明，该方法显著优于现有的采样和恢复方法。

Conclusion: 该方法显著优于现有的采样和恢复方法。

Abstract: Recovering point clouds involves the sequential process of sampling and
restoration, yet existing methods struggle to effectively leverage both
topological and geometric attributes. To address this, we propose an end-to-end
architecture named \textbf{TopGeoFormer}, which maintains these critical
features throughout the sampling and restoration phases. First, we revisit
traditional feature extraction techniques to yield topological embedding using
a continuous mapping of relative relationships between neighboring points, and
integrate it in both phases for preserving the structure of the original space.
Second, we propose the \textbf{InterTwining Attention} to fully merge
topological and geometric embeddings, which queries shape with local awareness
in both phases to form a learnable shape context facilitated with point-wise,
point-shape-wise, and intra-shape features. Third, we introduce a full geometry
loss and a topological constraint loss to optimize the embeddings in both
Euclidean and topological spaces. The geometry loss uses inconsistent matching
between coarse-to-fine generations and targets for reconstructing better
geometric details, and the constraint loss limits embedding variances for
better approximation of the topological space. In experiments, we
comprehensively analyze the circumstances using the conventional and
learning-based sampling/upsampling algorithms. The quantitative and qualitative
results demonstrate that our method significantly outperforms existing sampling
and recovery methods.

</details>


### [96] [MixA-Q: Revisiting Activation Sparsity for Vision Transformers from a Mixed-Precision Quantization Perspective](https://arxiv.org/abs/2507.19131)
*Weitian Wang,Rai Shubham,Cecilia De La Parra,Akash Kumar*

Main category: cs.CV

TL;DR: MixA-Q improves the speed and accuracy of quantized vision transformers by using mixed-precision activation quantization.


<details>
  <summary>Details</summary>
Motivation: Efficient inference of quantized window-based vision transformers using intra-layer activation sparsity.

Method: MixA-Q, a mixed-precision activation quantization framework using Two-Branch Swin Block.

Result: Achieves training-free 1.35x speedup without accuracy loss in PTQ. With QAT, achieves lossless 1.25x speedup and 1.53x speedup with 1% mAP drop. Improves mAP of W4A4 model by 0.7%.

Conclusion: MixA-Q achieves speedups with minimal accuracy loss and improves mAP by reducing quantization error.

Abstract: In this paper, we propose MixA-Q, a mixed-precision activation quantization
framework that leverages intra-layer activation sparsity (a concept widely
explored in activation pruning methods) for efficient inference of quantized
window-based vision transformers. For a given uniform-bit quantization
configuration, MixA-Q separates the batched window computations within Swin
blocks and assigns a lower bit width to the activations of less important
windows, improving the trade-off between model performance and efficiency. We
introduce a Two-Branch Swin Block that processes activations separately in
high- and low-bit precision, enabling seamless integration of our method with
most quantization-aware training (QAT) and post-training quantization (PTQ)
methods, or with simple modifications. Our experimental evaluations over the
COCO dataset demonstrate that MixA-Q achieves a training-free 1.35x
computational speedup without accuracy loss in PTQ configuration. With QAT,
MixA-Q achieves a lossless 1.25x speedup and a 1.53x speedup with only a 1% mAP
drop by incorporating activation pruning. Notably, by reducing the quantization
error in important regions, our sparsity-aware quantization adaptation improves
the mAP of the quantized W4A4 model (with both weights and activations in 4-bit
precision) by 0.7%, reducing quantization degradation by 24%.

</details>


### [97] [Balancing Conservatism and Aggressiveness: Prototype-Affinity Hybrid Network for Few-Shot Segmentation](https://arxiv.org/abs/2507.19140)
*Tianyu Zou,Shengwu Xiong,Ruilin Yao,Yi Rong*

Main category: cs.CV

TL;DR: 提出了PAHNet，通过平衡原型学习和亲和学习来提高少样本分割的性能。


<details>
  <summary>Details</summary>
Motivation: 原型学习方法的预测通常是保守的，而亲和学习方法的预测往往更激进。为了平衡这两种类型的FSS框架所捕获的保守和激进信息，从而提高分割性能。

Method: 提出了一个原型-亲和混合网络(PAHNet)，它在亲和学习模型的每个注意块中引入了一个原型引导的特征增强(PFE)模块和一个注意评分校准(ASC)模块。

Result: PFE模块增强了支持和查询图像表示中的前景信息，ASC模块抑制了它们之间不匹配的前景-背景(FG-BG)关系，从而有效地缓解了亲和学习者的激进性，最终提高了PAHNet方法的分割精度。

Conclusion: PAHNet在PASCAL-5$^i$和COCO-20$^i$数据集上的1-shot和5-shot设置中优于最近提出的方法，表明其有效性。

Abstract: This paper studies the few-shot segmentation (FSS) task, which aims to
segment objects belonging to unseen categories in a query image by learning a
model on a small number of well-annotated support samples. Our analysis of two
mainstream FSS paradigms reveals that the predictions made by prototype
learning methods are usually conservative, while those of affinity learning
methods tend to be more aggressive. This observation motivates us to balance
the conservative and aggressive information captured by these two types of FSS
frameworks so as to improve the segmentation performance. To achieve this, we
propose a **P**rototype-**A**ffinity **H**ybrid **Net**work (PAHNet), which
introduces a Prototype-guided Feature Enhancement (PFE) module and an Attention
Score Calibration (ASC) module in each attention block of an affinity learning
model (called affinity learner). These two modules utilize the predictions
generated by a pre-trained prototype learning model (called prototype
predictor) to enhance the foreground information in support and query image
representations and suppress the mismatched foreground-background (FG-BG)
relationships between them, respectively. In this way, the aggressiveness of
the affinity learner can be effectively mitigated, thereby eventually
increasing the segmentation accuracy of our PAHNet method. Experimental results
show that PAHNet outperforms most recently proposed methods across 1-shot and
5-shot settings on both PASCAL-5$^i$ and COCO-20$^i$ datasets, suggesting its
effectiveness. The code is available at: [GitHub - tianyu-zou/PAHNet: Balancing
Conservatism and Aggressiveness: Prototype-Affinity Hybrid Network for Few-Shot
Segmentation (ICCV'25)](https://github.com/tianyu-zou/PAHNet)

</details>


### [98] [CXR-CML: Improved zero-shot classification of long-tailed multi-label diseases in Chest X-Rays](https://arxiv.org/abs/2507.19398)
*Rajesh Madhipati,Sheethal Bhat,Lukas Buess,Andreas Maier*

Main category: cs.CV

TL;DR: 通过类加权和在潜在空间中的聚类，该论文提出了一种改进的零样本分类方法，尤其针对长尾分布的类别，并在胸部 X 光图像数据集上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督深度学习模型在准确分类长尾类时表现不佳。虽然像 CLIP 这样的视觉-语言模型在建模潜在空间的流形分布方面有效，但对于长尾分布的类，其有效性显著降低。

Method: 采用类加权机制，直接与潜在空间内的类分布对齐；应用高斯混合模型（GMM）聚类到潜在空间，并通过学生 t 分布细化后续聚类，再使用度量损失来调整嵌入。

Result: 在 MIMIC-CXR-JPG 数据集的 40 个类别上，零样本 AUC 分数平均提高了 7 个百分点，优于之前的 SOTA 模型。

Conclusion: 通过在潜在空间中应用高斯混合模型（GMM）聚类，并利用学生 t 分布和度量损失细化后续聚类，该方法能够稳定和自适应地聚类特征，从而显著提高整体分类性能，尤其是在罕见类别上的识别和准确性。

Abstract: Chest radiography (CXR) plays a crucial role in the diagnosis of various
diseases. However, the inherent class imbalance in the distribution of clinical
findings presents a significant challenge for current self-supervised deep
learning models. These models often fail to accurately classify long-tailed
classes. Current Vision-Language models such as Contrastive Language Image
Pre-training (CLIP) models effectively model the manifold distribution of the
latent space, enabling high zero-shot classification accuracies. Although CLIP
performs well on most of the primary classes in the dataset, our work reveals
that its effectiveness decreases significantly for classes with a long-tailed
distribution. Our approach employs a class-weighting mechanism that directly
aligns with the distribution of classes within the latent space. This method
ensures a substantial improvement in overall classification performance, with
particular emphasis on enhancing the recognition and accuracy of rarely
observed classes. We accomplish this by applying Gaussian Mixture Model (GMM)
clustering to the latent space. The subsequent clusters are further refined by
Student t-distribution, followed by a metric loss that utilizes the altered
embeddings. Our approach facilitates stable and adaptive clustering of the
features. This results in a notable average improvement of 7\% points in
zero-shot AUC scores across 40 classes in the MIMIC-CXR-JPG dataset from
previous SOTA models.

</details>


### [99] [DASH: 4D Hash Encoding with Self-Supervised Decomposition for Real-Time Dynamic Scene Rendering](https://arxiv.org/abs/2507.19141)
*Jie Chen,Zhangchi Hu,Peixi Wu,Huyue Zhu,Hebei Li,Xiaoyan Sun*

Main category: cs.CV

TL;DR: DASH is a real-time dynamic scene rendering framework using 4D hash encoding and self-supervised decomposition to achieve state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing plane-based methods in dynamic Gaussian splatting suffer from an unsuitable low-rank assumption, causing feature overlap and poor rendering quality. Directly applying 4D hash encoding to the entire dynamic scene leads to substantial hash collisions and redundancy.

Method: employs 4D hash encoding coupled with self-supervised decomposition

Result: enhanced visual quality at real-time speeds of 264 FPS on a single 4090 GPU

Conclusion: DASH achieves state-of-the-art dynamic rendering performance with enhanced visual quality at real-time speeds.

Abstract: Dynamic scene reconstruction is a long-term challenge in 3D vision. Existing
plane-based methods in dynamic Gaussian splatting suffer from an unsuitable
low-rank assumption, causing feature overlap and poor rendering quality.
Although 4D hash encoding provides an explicit representation without low-rank
constraints, directly applying it to the entire dynamic scene leads to
substantial hash collisions and redundancy. To address these challenges, we
present DASH, a real-time dynamic scene rendering framework that employs 4D
hash encoding coupled with self-supervised decomposition. Our approach begins
with a self-supervised decomposition mechanism that separates dynamic and
static components without manual annotations or precomputed masks. Next, we
introduce a multiresolution 4D hash encoder for dynamic elements, providing an
explicit representation that avoids the low-rank assumption. Finally, we
present a spatio-temporal smoothness regularization strategy to mitigate
unstable deformation artifacts. Experiments on real-world datasets demonstrate
that DASH achieves state-of-the-art dynamic rendering performance, exhibiting
enhanced visual quality at real-time speeds of 264 FPS on a single 4090 GPU.
Code: https://github.com/chenj02/DASH.

</details>


### [100] [Patch Pruning Strategy Based on Robust Statistical Measures of Attention Weight Diversity in Vision Transformers](https://arxiv.org/abs/2507.19175)
*Yuki Igaue,Hiroaki Aizawa*

Main category: cs.CV

TL;DR: 提出了一种基于多头注意力权重方差的patch pruning策略，以提高视觉transformer的计算效率，并在保持分类精度的前提下提高了吞吐量。


<details>
  <summary>Details</summary>
Motivation: 多头自注意力机制在视觉transformer中计算所有输入patch之间的关系，但计算复杂度较高。Patch pruning是一种通过识别和删除冗余patch来提高计算效率的方法。

Method: 提出了一种patch pruning策略，该策略基于多头注意力权重方差来评估每个patch的重要性。该方法可以在训练和推理期间轻松应用。

Result: 在微调预训练模型等场景中，提高了吞吐量，同时保持了分类精度。

Conclusion: 使用稳健的统计方法（如中位数绝对偏差）代替方差来评估patch重要性，可以获得相似的性能。通过引入重叠的patch嵌入，该方法在与使用所有patch的传统方法相当的吞吐量下实现了更好的性能。

Abstract: Multi-head self-attention is a distinctive feature extraction mechanism of
vision transformers that computes pairwise relationships among all input
patches, contributing significantly to their high performance. However, it is
known to incur a quadratic computational complexity with respect to the number
of patches. One promising approach to address this issue is patch pruning,
which improves computational efficiency by identifying and removing redundant
patches. In this work, we propose a patch pruning strategy that evaluates the
importance of each patch based on the variance of attention weights across
multiple attention heads. This approach is inspired by the design of multi-head
self-attention, which aims to capture diverse attention patterns across
different subspaces of feature representations. The proposed method can be
easily applied during both training and inference, and achieves improved
throughput while maintaining classification accuracy in scenarios such as
fine-tuning with pre-trained models. In addition, we also found that using
robust statistical measures, such as the median absolute deviation in place of
variance, to assess patch importance can similarly lead to strong performance.
Furthermore, by introducing overlapping patch embeddings, our method achieves
better performance with comparable throughput to conventional approaches that
utilize all patches.

</details>


### [101] [Continual Learning-Based Unified Model for Unpaired Image Restoration Tasks](https://arxiv.org/abs/2507.19184)
*Kotha Kartheek,Lingamaneni Gnanesh Chowdary,Snehasis Mukherjee*

Main category: cs.CV

TL;DR: This paper introduces a unified image restoration framework using continual learning to handle various weather conditions like fog, snow, and rain, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Restoring images contaminated by different adverse weather conditions is challenging, and existing methods often focus on only one condition. A unified model is needed for applications like autonomous driving.

Method: The paper proposes a continual learning approach with Selective Kernel Fusion layers, Elastic Weight Consolidation (EWC), and a novel Cycle-Contrastive Loss for unpaired image restoration.

Result: The proposed approach achieves significant improvements in PSNR, SSIM, and perceptual quality compared to existing methods.

Conclusion: The paper demonstrates significant improvements in PSNR, SSIM, and perceptual quality over state-of-the-art methods on standard benchmark datasets for dehazing, desnowing, and deraining tasks.

Abstract: Restoration of images contaminated by different adverse weather conditions
such as fog, snow, and rain is a challenging task due to the varying nature of
the weather conditions. Most of the existing methods focus on any one
particular weather conditions. However, for applications such as autonomous
driving, a unified model is necessary to perform restoration of corrupted
images due to different weather conditions. We propose a continual learning
approach to propose a unified framework for image restoration. The proposed
framework integrates three key innovations: (1) Selective Kernel Fusion layers
that dynamically combine global and local features for robust adaptive feature
selection; (2) Elastic Weight Consolidation (EWC) to enable continual learning
and mitigate catastrophic forgetting across multiple restoration tasks; and (3)
a novel Cycle-Contrastive Loss that enhances feature discrimination while
preserving semantic consistency during domain translation. Further, we propose
an unpaired image restoration approach to reduce the dependance of the proposed
approach on the training data. Extensive experiments on standard benchmark
datasets for dehazing, desnowing and deraining tasks demonstrate significant
improvements in PSNR, SSIM, and perceptual quality over the state-of-the-art.

</details>


### [102] [VisHall3D: Monocular Semantic Scene Completion from Reconstructing the Visible Regions to Hallucinating the Invisible Regions](https://arxiv.org/abs/2507.19188)
*Haoang Lu,Yuanqi Su,Xiaoning Zhang,Longjun Gao,Yu Xue,Le Wang*

Main category: cs.CV

TL;DR: VisHall3D, a two-stage framework, addresses feature entanglement and geometric inconsistency in monocular semantic scene completion, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods for monocular semantic scene completion suffer from feature entanglement and geometric inconsistency.

Method: The paper introduces VisHall3D, a two-stage framework. The first stage uses VisFrontierNet for visible regions, and the second stage uses OcclusionMAE for invisible regions.

Result: VisHall3D significantly improves reconstruction quality by decoupling scene completion into two distinct stages.

Conclusion: VisHall3D achieves state-of-the-art performance on SemanticKITTI and SSCBench-KITTI-360, outperforming previous methods and enabling more accurate scene understanding.

Abstract: This paper introduces VisHall3D, a novel two-stage framework for monocular
semantic scene completion that aims to address the issues of feature
entanglement and geometric inconsistency prevalent in existing methods.
VisHall3D decomposes the scene completion task into two stages: reconstructing
the visible regions (vision) and inferring the invisible regions
(hallucination). In the first stage, VisFrontierNet, a visibility-aware
projection module, is introduced to accurately trace the visual frontier while
preserving fine-grained details. In the second stage, OcclusionMAE, a
hallucination network, is employed to generate plausible geometries for the
invisible regions using a noise injection mechanism. By decoupling scene
completion into these two distinct stages, VisHall3D effectively mitigates
feature entanglement and geometric inconsistency, leading to significantly
improved reconstruction quality.
  The effectiveness of VisHall3D is validated through extensive experiments on
two challenging benchmarks: SemanticKITTI and SSCBench-KITTI-360. VisHall3D
achieves state-of-the-art performance, outperforming previous methods by a
significant margin and paves the way for more accurate and reliable scene
understanding in autonomous driving and other applications.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [103] [Initial Steps in Integrating Large Reasoning and Action Models for Service Composition](https://arxiv.org/abs/2507.18775)
*Ilche Georgievski,Marco Aiello*

Main category: cs.AI

TL;DR: 本文提出了一种集成的 LRM-LAM 架构框架，以推进自动化服务组合。


<details>
  <summary>Details</summary>
Motivation: 服务组合仍然是构建自适应和智能软件系统中的一个核心挑战，但通常受到有限的推理能力或脆弱的执行机制的约束。

Method: 集成大型推理模型 (LRM) 和大型动作模型 (LAM) 的 LRM-LAM 架构框架

Result: LRM解决了语义推理和生态系统复杂性的挑战，而LAM擅长动态动作执行和系统互操作性。集成的系统可以推理服务需求和约束，同时动态执行工作流程，从而弥合意图和执行之间的差距。

Conclusion: 集成的LRM-LAM架构有潜力将服务组合转变为完全自动化、用户友好的流程，该流程由高级自然语言意图驱动。

Abstract: Service composition remains a central challenge in building adaptive and
intelligent software systems, often constrained by limited reasoning
capabilities or brittle execution mechanisms. This paper explores the
integration of two emerging paradigms enabled by large language models: Large
Reasoning Models (LRMs) and Large Action Models (LAMs). We argue that LRMs
address the challenges of semantic reasoning and ecosystem complexity while
LAMs excel in dynamic action execution and system interoperability. However,
each paradigm has complementary limitations - LRMs lack grounded action
capabilities, and LAMs often struggle with deep reasoning. We propose an
integrated LRM-LAM architectural framework as a promising direction for
advancing automated service composition. Such a system can reason about service
requirements and constraints while dynamically executing workflows, thus
bridging the gap between intention and execution. This integration has the
potential to transform service composition into a fully automated,
user-friendly process driven by high-level natural language intent.

</details>


### [104] [Simulation-Driven Reinforcement Learning in Queuing Network Routing Optimization](https://arxiv.org/abs/2507.18795)
*Fatima Al-Ani,Molly Wang,Jevon Charles,Aaron Ong,Joshua Forday,Vinayak Modi*

Main category: cs.AI

TL;DR: 本研究侧重于开发一种仿真驱动的强化学习 (RL) 框架，用于优化复杂排队网络系统中的路由决策，特别强调制造和通信应用。


<details>
  <summary>Details</summary>
Motivation: 传统排队方法通常难以应对动态、不确定的环境。

Method: DDPG结合Dyna-style planning (Dyna-DDPG)

Result: 改进的Dyna-DDPG实现包含用于下一状态转换和奖励的独立预测模型，从而显着提高稳定性和样本效率。

Conclusion: 该框架能够快速学习有效的路由策略，在中断下保持稳健的性能，并有效地扩展到更大的网络规模。此外，我们强调采用强大的软件工程实践，以确保框架的可重复性和可维护性，从而能够在实际场景中进行实际部署。

Abstract: This study focuses on the development of a simulation-driven reinforcement
learning (RL) framework for optimizing routing decisions in complex queueing
network systems, with a particular emphasis on manufacturing and communication
applications. Recognizing the limitations of traditional queueing methods,
which often struggle with dynamic, uncertain environments, we propose a robust
RL approach leveraging Deep Deterministic Policy Gradient (DDPG) combined with
Dyna-style planning (Dyna-DDPG). The framework includes a flexible and
configurable simulation environment capable of modeling diverse queueing
scenarios, disruptions, and unpredictable conditions. Our enhanced Dyna-DDPG
implementation incorporates separate predictive models for next-state
transitions and rewards, significantly improving stability and sample
efficiency. Comprehensive experiments and rigorous evaluations demonstrate the
framework's capability to rapidly learn effective routing policies that
maintain robust performance under disruptions and scale effectively to larger
network sizes. Additionally, we highlight strong software engineering practices
employed to ensure reproducibility and maintainability of the framework,
enabling practical deployment in real-world scenarios.

</details>


### [105] [A Neuroscience-Inspired Dual-Process Model of Compositional Generalization](https://arxiv.org/abs/2507.18868)
*Alex Noviello,Claas Beger,Jacob Groner,Kevin Ellis,Weinan Sun*

Main category: cs.AI

TL;DR: MIRAGE, a framework inspired by the human brain, achieves systematic generalization on compositional tasks by using a meta-trained Transformer and a Schema Engine.


<details>
  <summary>Details</summary>
Motivation: Systematic compositional generalization remains a core challenge for AI systems. Human cognition achieves this flexibility via the interplay of the hippocampus (HPC) and prefrontal cortex (PFC).

Method: MIRAGE has two interacting modules: a meta-trained Transformer Neural Decomposer and a Schema Engine. The Transformer is trained on a task-agnostic stream of randomly sampled compositional grammars and applies one decomposition step per pass. The Schema Engine dynamically extracts, ranks, and applies reusable schemas, storing variable bindings in episodic memory and expanding them when needed.

Result: MIRAGE demonstrates systematic compositional generalization on the SCAN benchmark, achieving > 99% accuracy on all task splits with only 1.19M parameters in the transformer module.

Conclusion: MIRAGE achieves systematic compositional generalization on the SCAN benchmark, achieving > 99% accuracy on all task splits with only 1.19M parameters. Ablation studies confirm that MIRAGE's systematicity critically depends on the quality of extracted schemas and the model's iterative refinement process.

Abstract: Systematic compositional generalization - constructing and understanding
novel combinations of known building blocks - remains a core challenge for AI
systems. Human cognition achieves this flexibility via the interplay of the
hippocampus (HPC) and prefrontal cortex (PFC): the hippocampus rapidly encodes
episodes, and the prefrontal cortex consolidates them into reusable schemas for
reasoning. Drawing on these insights, we present MIRAGE (Meta-Inference with
Rules and Abstractions from Generalized Experience), a framework that achieves
systematic generalization on compositional tasks. MIRAGE has two interacting
modules mirroring the brain's deliberative HPC-PFC loop and intuitive
neocortical pattern recognition. (1) The meta-trained Transformer Neural
Decomposer, paralleling neocortical "System 1" computation, is trained on a
task-agnostic stream of randomly sampled compositional grammars and applies one
decomposition step per pass, with successive passes iteratively refining the
sequence representation. (2) The Schema Engine, analogous to the HPC-PFC
"System 2" loop, dynamically extracts, ranks, and applies reusable schemas,
storing variable bindings in episodic memory and expanding them when needed. By
explicitly equipping the Transformer component of MIRAGE with actively managed
schematic structures, our model performs systematic compositional operations
through explicit schema application and transformation, relying solely on
frozen weights when solving entirely novel tasks. This approach demonstrates
systematic compositional generalization on the SCAN benchmark, achieving > 99%
accuracy on all task splits with only 1.19M parameters in the transformer
module. Ablation studies confirm that MIRAGE's systematicity critically depends
on the quality of extracted schemas and the model's iterative refinement
process.

</details>


### [106] [Success in Humanoid Reinforcement Learning under Partial Observation](https://arxiv.org/abs/2507.18883)
*Wuhao Wang,Zhiyong Chen*

Main category: cs.AI

TL;DR: 该研究提出了一种在不完全状态信息下，在Gymnasium Humanoid-v4环境中学习人形机器人策略的成功案例。


<details>
  <summary>Details</summary>
Motivation: 在部分可观察性下进行有效的策略学习仍然是一个主要的挑战，尤其是在像人形机器人运动这样的高维任务中。迄今为止，还没有先前的工作证明在基准Gymnasium Humanoid-v4环境中，通过不完整的状态信息对人形机器人策略进行稳定训练。

Method: 一种新颖的历史编码器，可以并行处理固定长度的过去观察序列。集成到标准无模型算法中，该编码器能够实现与完全观察到的基线相当的性能。

Result: 所学习的策略实现了与具有完全状态访问权限的最先进结果相当的性能，尽管仅使用了原始状态的三分之一到三分之二。此外，该策略表现出对机器人属性的适应性，例如身体部位质量的变化。

Conclusion: 我们假设，该策略从最近的观察中重建了重要的上下文信息，从而实现了稳健的决策。

Abstract: Reinforcement learning has been widely applied to robotic control, but
effective policy learning under partial observability remains a major
challenge, especially in high-dimensional tasks like humanoid locomotion. To
date, no prior work has demonstrated stable training of humanoid policies with
incomplete state information in the benchmark Gymnasium Humanoid-v4
environment. The objective in this environment is to walk forward as fast as
possible without falling, with rewards provided for staying upright and moving
forward, and penalties incurred for excessive actions and external contact
forces. This research presents the first successful instance of learning under
partial observability in this environment. The learned policy achieves
performance comparable to state-of-the-art results with full state access,
despite using only one-third to two-thirds of the original states. Moreover,
the policy exhibits adaptability to robot properties, such as variations in
body part masses. The key to this success is a novel history encoder that
processes a fixed-length sequence of past observations in parallel. Integrated
into a standard model-free algorithm, the encoder enables performance on par
with fully observed baselines. We hypothesize that it reconstructs essential
contextual information from recent observations, thereby enabling robust
decision-making.

</details>


### [107] [Towards Improving Long-Tail Entity Predictions in Temporal Knowledge Graphs through Global Similarity and Weighted Sampling](https://arxiv.org/abs/2507.18977)
*Mehrnoosh Mirtaheri,Ryan A. Rossi,Sungchul Kim,Kanak Mahadik,Tong Yu,Xiang Chen,Mohammad Rostami*

Main category: cs.AI

TL;DR: 提出了一种用于 TKG 的增量训练框架，旨在解决在训练期间未观察到或具有稀疏连接的实体。


<details>
  <summary>Details</summary>
Motivation: 传统的时间知识图谱 (TKG) 完成模型通常假设在训练期间可以访问整个图。这忽略了源于 TKG 不断变化的性质的挑战，例如：(i) 模型泛化和吸收新知识的要求，以及 (ii) 管理通常具有稀疏连接的新的或未见过的实体的任务。

Method: 结合了模型无关的增强层和加权抽样策略

Result: 该方法在总链接预测、归纳链接预测和解决长尾实体方面优于现有方法。值得注意的是，我们的方法在这些数据集的 MRR 中实现了 10% 的改进和 15% 的提升。

Conclusion: 该方法在缓解灾难性遗忘和增强 TKG 完成方法的鲁棒性方面具有潜力，尤其是在增量训练环境中。

Abstract: Temporal Knowledge Graph (TKG) completion models traditionally assume access
to the entire graph during training. This overlooks challenges stemming from
the evolving nature of TKGs, such as: (i) the model's requirement to generalize
and assimilate new knowledge, and (ii) the task of managing new or unseen
entities that often have sparse connections. In this paper, we present an
incremental training framework specifically designed for TKGs, aiming to
address entities that are either not observed during training or have sparse
connections. Our approach combines a model-agnostic enhancement layer with a
weighted sampling strategy, that can be augmented to and improve any existing
TKG completion method. The enhancement layer leverages a broader, global
definition of entity similarity, which moves beyond mere local neighborhood
proximity of GNN-based methods. The weighted sampling strategy employed in
training accentuates edges linked to infrequently occurring entities. We
evaluate our method on two benchmark datasets, and demonstrate that our
framework outperforms existing methods in total link prediction, inductive link
prediction, and in addressing long-tail entities. Notably, our method achieves
a 10\% improvement and a 15\% boost in MRR for these datasets. The results
underscore the potential of our approach in mitigating catastrophic forgetting
and enhancing the robustness of TKG completion methods, especially in an
incremental training context

</details>


### [108] [Fine-Grained Traffic Inference from Road to Lane via Spatio-Temporal Graph Node Generation](https://arxiv.org/abs/2507.19089)
*Shuhao Li,Weidong Yang,Yue Cui,Xiaoxing Liu,Lingkai Meng,Lipeng Ma,Fan Zhang*

Main category: cs.AI

TL;DR: This paper proposes the Fine-grained Road Traffic Inference (FRTI) task and a RoadDiff model to generate detailed lane-level traffic information from limited road data.


<details>
  <summary>Details</summary>
Motivation: Obtaining lane-level traffic data is a critical bottleneck due to limitations in sensors and tracking algorithm accuracy.

Method: A two-stage framework--RoadDiff--leveraging the Road-Lane Correlation Autoencoder-Decoder and the Lane Diffusion Module.

Result: Extensive experiments on six datasets validate the effectiveness of the RoadDiff model.

Conclusion: The RoadDiff model effectively addresses the FRTI task, as validated by experiments on six datasets representing different road conditions.

Abstract: Fine-grained traffic management and prediction are fundamental to key
applications such as autonomous driving, lane change guidance, and traffic
signal control. However, obtaining lane-level traffic data has become a
critical bottleneck for data-driven models due to limitations in the types and
number of sensors and issues with the accuracy of tracking algorithms. To
address this, we propose the Fine-grained Road Traffic Inference (FRTI) task,
which aims to generate more detailed lane-level traffic information using
limited road data, providing a more energy-efficient and cost-effective
solution for precise traffic management. This task is abstracted as the first
scene of the spatio-temporal graph node generation problem. We designed a
two-stage framework--RoadDiff--to solve the FRTI task. solve the FRTI task.
This framework leverages the Road-Lane Correlation Autoencoder-Decoder and the
Lane Diffusion Module to fully utilize the limited spatio-temporal dependencies
and distribution relationships of road data to accurately infer fine-grained
lane traffic states. Based on existing research, we designed several baseline
models with the potential to solve the FRTI task and conducted extensive
experiments on six datasets representing different road conditions to validate
the effectiveness of the RoadDiff model in addressing the FRTI task. The
relevant datasets and code are available at
https://github.com/ShuhaoLii/RoadDiff.

</details>


### [109] [Pareto-NRPA: A Novel Monte-Carlo Search Algorithm for Multi-Objective Optimization](https://arxiv.org/abs/2507.19109)
*Noé Lallouet,Tristan Cazenave,Cyrille Enderli*

Main category: cs.AI

TL;DR: Pareto-NRPA是一种新的多目标优化算法，在多个问题上表现出良好的性能。


<details>
  <summary>Details</summary>
Motivation: 针对离散搜索空间上的多目标优化问题，引入了一种新的蒙特卡罗算法Pareto-NRPA。

Method: Pareto-NRPA算法，它将嵌套搜索和策略更新机制推广到多目标优化。该算法使用一组策略来同时探索解空间的不同区域，并在每个搜索级别维护非支配前沿。

Result: Pareto-NRPA在旅行商问题和神经架构搜索任务上实现了具有竞争力的性能。

Conclusion: Pareto-NRPA在约束搜索空间上显著优于最先进的进化多目标算法，并且在收敛性和解决方案的多样性方面都达到了与最先进的多目标算法相媲美的性能。

Abstract: We introduce Pareto-NRPA, a new Monte-Carlo algorithm designed for
multi-objective optimization problems over discrete search spaces. Extending
the Nested Rollout Policy Adaptation (NRPA) algorithm originally formulated for
single-objective problems, Pareto-NRPA generalizes the nested search and policy
update mechanism to multi-objective optimization. The algorithm uses a set of
policies to concurrently explore different regions of the solution space and
maintains non-dominated fronts at each level of search. Policy adaptation is
performed with respect to the diversity and isolation of sequences within the
Pareto front. We benchmark Pareto-NRPA on two classes of problems: a novel
bi-objective variant of the Traveling Salesman Problem with Time Windows
problem (MO-TSPTW), and a neural architecture search task on well-known
benchmarks. Results demonstrate that Pareto-NRPA achieves competitive
performance against state-of-the-art multi-objective algorithms, both in terms
of convergence and diversity of solutions. Particularly, Pareto-NRPA strongly
outperforms state-of-the-art evolutionary multi-objective algorithms on
constrained search spaces. To our knowledge, this work constitutes the first
adaptation of NRPA to the multi-objective setting.

</details>


### [110] [OS-MAP: How Far Can Computer-Using Agents Go in Breadth and Depth?](https://arxiv.org/abs/2507.19132)
*Xuetian Chen,Yinghao Chen,Xinfeng Yuan,Zhuo Peng,Lu Chen,Yuekeng Li,Zhoujia Zhang,Yingqian Huang,Leyan Huang,Jiaqing Liang,Tianbao Xie,Zhiyong Wu,Qiushi Sun,Biqing Qi,Bowen Zhou*

Main category: cs.AI

TL;DR: Introduces OS-MAP, a new benchmark for computer-using automation that addresses limitations in existing benchmarks by considering task heterogeneity, agent capabilities, and alignment with user demands. Experiments show current agents struggle with higher-level tasks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to account for the internal task heterogeneity and the corresponding agent capabilities, as well as their alignment with actual user demands-hindering both targeted capability development and the reliable transition of research progress into practical deployment.

Method: OS-MAP, a benchmark for daily computer-using automation that organizes its 416 realistic tasks across 15 applications along two key dimensions: a five-level taxonomy of automation and a generalization scope derived from a real-world user demand hierarchy.

Result: OS-MAP evaluates agents along two dimensions: automation level across a five-level taxonomy, and generalization scope across a demand hierarchy. This design captures varying levels of required agent autonomy and generalization, forming a performance-generalization evaluation matrix for structured and comprehensive assessment.

Conclusion: State-of-the-art agents struggle with higher-level tasks, highlighting the need for a deeper understanding of current strengths and limitations.

Abstract: Computer-using agents have shown strong potential to boost human productivity
and enable new application forms across platforms. While recent advances have
led to usable applications, existing benchmarks fail to account for the
internal task heterogeneity and the corresponding agent capabilities, as well
as their alignment with actual user demands-hindering both targeted capability
development and the reliable transition of research progress into practical
deployment. To bridge the gap, we present OS-MAP, a benchmark for daily
computer-using automation that organizes its 416 realistic tasks across 15
applications along two key dimensions: a five-level taxonomy of automation and
a generalization scope derived from a real-world user demand hierarchy. To
enable fine-grained analysis of required capabilities and alignment with
real-world scenarios, OS-MAP evaluates agents along two dimensions: automation
level across a five-level taxonomy, and generalization scope across a demand
hierarchy. This design captures varying levels of required agent autonomy and
generalization, forming a performance-generalization evaluation matrix for
structured and comprehensive assessment. Experiments show that even
State-of-the-Art agents with VLM backbones struggle with higher-level tasks
involving perception, reasoning, and coordination-highlighting the need for a
deeper understanding of current strengths and limitations to drive the future
progress in computer-using agents research and deployment. All code,
environments, baselines, and data are publicly available at
https://github.com/OS-Copilot/OS-Map.

</details>


### [111] [PhysDrive: A Multimodal Remote Physiological Measurement Dataset for In-vehicle Driver Monitoring](https://arxiv.org/abs/2507.19172)
*Jiyao Wang,Xiao Yang,Qingyong Hu,Jiankai Tang,Can Liu,Dengbo He,Yuntao Wang,Yingcong Chen,Kaishun Wu*

Main category: cs.AI

TL;DR: PhysDrive是一个大规模多模态车载生理传感数据集，旨在解决现有数据集的局限性，为驾驶员监控和智能驾驶舱系统的研究提供基础。


<details>
  <summary>Details</summary>
Motivation: 现有车载生理监测数据集在规模、模态多样性、生物特征注释的广度以及捕获的条件范围方面存在局限性，无法反映真实驾驶场景中的挑战。

Method: 该论文介绍了PhysDrive数据集的构建，并通过信号处理和深度学习方法对其进行了广泛评估，为所有模态建立了全面的基准。

Result: PhysDrive是一个大规模多模态数据集，包含48名驾驶员的同步RGB、近红外相机和毫米波雷达数据，以及六个同步的ground truth（ECG、BVP、呼吸、HR、RR和SpO2）。它涵盖了广泛的自然驾驶条件，包括驾驶员运动、动态自然光、车辆类型和道路条件。

Conclusion: PhysDrive数据集的发布为多模态驾驶员监控和智能驾驶舱系统的研究奠定了基础，并加速了相关研究。

Abstract: Robust and unobtrusive in-vehicle physiological monitoring is crucial for
ensuring driving safety and user experience. While remote physiological
measurement (RPM) offers a promising non-invasive solution, its translation to
real-world driving scenarios is critically constrained by the scarcity of
comprehensive datasets. Existing resources are often limited in scale, modality
diversity, the breadth of biometric annotations, and the range of captured
conditions, thereby omitting inherent real-world challenges in driving. Here,
we present PhysDrive, the first large-scale multimodal dataset for contactless
in-vehicle physiological sensing with dedicated consideration on various
modality settings and driving factors. PhysDrive collects data from 48 drivers,
including synchronized RGB, near-infrared camera, and raw mmWave radar data,
accompanied with six synchronized ground truths (ECG, BVP, Respiration, HR, RR,
and SpO2). It covers a wide spectrum of naturalistic driving conditions,
including driver motions, dynamic natural light, vehicle types, and road
conditions. We extensively evaluate both signal-processing and deep-learning
methods on PhysDrive, establishing a comprehensive benchmark across all
modalities, and release full open-source code with compatibility for mainstream
public toolboxes. We envision PhysDrive will serve as a foundational resource
and accelerate research on multimodal driver monitoring and smart-cockpit
systems.

</details>


### [112] [Faster Lifting for Ordered Domains with Predecessor Relations](https://arxiv.org/abs/2507.19182)
*Kuncheng Zou,Jiahao Mai,Yonggang Zhang,Yuyi Wang,Ondřej Kuželka,Yuanhong Wang,Yi Chang*

Main category: cs.AI

TL;DR: 我们提出了一种用于有序域上提升推理的新算法，该算法原生支持前置关系，并在各种任务中实现了显着的加速。


<details>
  <summary>Details</summary>
Motivation: 以前的工作通过加权一阶模型计数 (WFOMC) 探索了这个问题，该方法计算给定一阶逻辑语句在有限域上的模型的加权和。在 WFOMC 中，顺序约束通常由线性顺序公理编码，在线性顺序公理中引入二元谓词以对域元素施加线性顺序。然后，直接和第二个前置关系由线性顺序谓词编码。尽管具有线性顺序公理的 WFOMC 在理论上是易于处理的，但现有的算法在实际应用中却很困难，尤其是在涉及前置关系时。

Method: 我们设计了一种新颖的算法，该算法固有地支持这些关系。

Result: 在提升的推理任务和组合数学问题上的大量实验表明了我们算法的效率，实现了整整一个数量级的加速。

Conclusion: 该算法不仅为已知易处理的直接和第二个前置关系提供了指数级的加速，而且还处理了一般的 k-th 前置关系。

Abstract: We investigate lifted inference on ordered domains with predecessor
relations, where the elements of the domain respect a total (cyclic) order, and
every element has a distinct (clockwise) predecessor. Previous work has
explored this problem through weighted first-order model counting (WFOMC),
which computes the weighted sum of models for a given first-order logic
sentence over a finite domain. In WFOMC, the order constraint is typically
encoded by the linear order axiom introducing a binary predicate in the
sentence to impose a linear ordering on the domain elements. The immediate and
second predecessor relations are then encoded by the linear order predicate.
Although WFOMC with the linear order axiom is theoretically tractable, existing
algorithms struggle with practical applications, particularly when the
predecessor relations are involved. In this paper, we treat predecessor
relations as a native part of the axiom and devise a novel algorithm that
inherently supports these relations. The proposed algorithm not only provides
an exponential speedup for the immediate and second predecessor relations,
which are known to be tractable, but also handles the general k-th predecessor
relations. The extensive experiments on lifted inference tasks and
combinatorics math problems demonstrate the efficiency of our algorithm,
achieving speedups of a full order of magnitude.

</details>


### [113] [Knowledge Grafting: A Mechanism for Optimizing AI Model Deployment in Resource-Constrained Environments](https://arxiv.org/abs/2507.19261)
*Osama Almurshed,Ashish Kaushal,Asmail Muftah,Nitin Auluck,Omer Rana*

Main category: cs.AI

TL;DR: The paper introduces knowledge grafting, a method to optimize AI models for resource-constrained environments. It transfers features from a large model to a smaller one, reducing size and improving performance.


<details>
  <summary>Details</summary>
Motivation: Larger, more complex AI models require substantial computing power, which is often unavailable in many real-world application scenarios.

Method: The paper introduces knowledge grafting, a novel mechanism that optimizes AI models for resource-constrained environments by transferring selected features from a large donor model to a smaller rootstock model.

Result: The approach achieves an 88.54% reduction in model size, while improving generalization capability of the model. The new rootstock model achieves 89.97% validation accuracy (vs. donor's 87.47%), maintains lower validation loss (0.2976 vs. 0.5068), and performs exceptionally well on unseen test data with 90.45% accuracy.

Conclusion: The knowledge grafting approach enables deployment of AI frameworks on resource-constrained devices with enhanced performance, and can be extended across various edge computing scenarios.

Abstract: The increasing adoption of Artificial Intelligence (AI) has led to larger,
more complex models with numerous parameters that require substantial computing
power -- resources often unavailable in many real-world application scenarios.
Our paper addresses this challenge by introducing knowledge grafting, a novel
mechanism that optimizes AI models for resource-constrained environments by
transferring selected features (the scion) from a large donor model to a
smaller rootstock model. The approach achieves an 88.54% reduction in model
size (from 64.39 MB to 7.38 MB), while improving generalization capability of
the model. Our new rootstock model achieves 89.97% validation accuracy (vs.
donor's 87.47%), maintains lower validation loss (0.2976 vs. 0.5068), and
performs exceptionally well on unseen test data with 90.45% accuracy. It
addresses the typical size vs performance trade-off, and enables deployment of
AI frameworks on resource-constrained devices with enhanced performance. We
have tested our approach on an agricultural weed detection scenario, however,
it can be extended across various edge computing scenarios, potentially
accelerating AI adoption in areas with limited hardware/software support -- by
mirroring in a similar manner the horticultural grafting enables productive
cultivation in challenging agri-based environments.

</details>


### [114] [Modeling Uncertainty: Constraint-Based Belief States in Imperfect-Information Games](https://arxiv.org/abs/2507.19263)
*Achille Morenville,Éric Piette*

Main category: cs.AI

TL;DR: This paper investigates two approaches to represent beliefs in games with hidden piece identities: constraint-based model and probabilistic extension. The findings indicate that constraint-based beliefs yield results comparable to those of probabilistic inference.


<details>
  <summary>Details</summary>
Motivation: agents must make decisions based on partial knowledge of the game state. The Belief Stochastic Game model addresses this challenge by delegating state estimation to the game model itself. This allows agents to operate on externally provided belief states, thereby reducing the need for game-specific inference logic.

Method: a constraint-based model using Constraint Satisfaction Problems and a probabilistic extension using Belief Propagation to estimate marginal probabilities

Result: constraint-based beliefs yield results comparable to those of probabilistic inference, with minimal differences in agent performance

Conclusion: constraint-based beliefs yield results comparable to those of probabilistic inference, with minimal differences in agent performance. This suggests that constraint-based belief states alone may suffice for effective decision-making in many settings.

Abstract: In imperfect-information games, agents must make decisions based on partial
knowledge of the game state. The Belief Stochastic Game model addresses this
challenge by delegating state estimation to the game model itself. This allows
agents to operate on externally provided belief states, thereby reducing the
need for game-specific inference logic. This paper investigates two approaches
to represent beliefs in games with hidden piece identities: a constraint-based
model using Constraint Satisfaction Problems and a probabilistic extension
using Belief Propagation to estimate marginal probabilities. We evaluated the
impact of both representations using general-purpose agents across two
different games. Our findings indicate that constraint-based beliefs yield
results comparable to those of probabilistic inference, with minimal
differences in agent performance. This suggests that constraint-based belief
states alone may suffice for effective decision-making in many settings.

</details>


### [115] [Integrating LLM in Agent-Based Social Simulation: Opportunities and Challenges](https://arxiv.org/abs/2507.19364)
*Patrick Taillandier,Jean Daniel Zucker,Arnaud Grignard,Benoit Gaudou,Nghi Quang Huynh,Alexis Drogoul*

Main category: cs.AI

TL;DR: 本文探讨了LLM在社会模拟中的应用，强调了它们的潜力和局限性，并倡导混合方法。


<details>
  <summary>Details</summary>
Motivation: 从计算社会科学的角度分析大型语言模型（LLM）在社会模拟中的使用，分析它们的潜力和局限性。

Method: 分析LLM在多智能体模拟框架中的新兴应用，关注系统架构、规模和验证策略。讨论了Generative Agents (Smallville) 和 AgentSociety 等项目的设计选择、实证基础和方法创新。特别关注大规模LLM驱动的模拟中行为保真度、校准和可重复性的挑战。

Result: 回顾了LLM在复制人类认知的关键方面（包括心理理论推理和社会推理）的能力的最新发现，同时也强调了认知偏差、缺乏真正理解和行为不一致等重大局限性。区分了LLM提供直接价值（如交互式模拟和严肃游戏）的上下文，以及它们的使用更有问题的上下文（特别是在解释或预测建模中）。

Conclusion: 倡导将LLM集成到传统的基于Agent建模平台中，结合语言推理的表达灵活性与经典规则系统的透明性和分析严谨性。

Abstract: This position paper examines the use of Large Language Models (LLMs) in
social simulation, analyzing both their potential and their limitations from a
computational social science perspective. The first part reviews recent
findings on the ability of LLMs to replicate key aspects of human cognition,
including Theory of Mind reasoning and social inference, while also
highlighting significant limitations such as cognitive biases, lack of true
understanding, and inconsistencies in behavior. The second part surveys
emerging applications of LLMs in multi-agent simulation frameworks, focusing on
system architectures, scale, and validation strategies. Notable projects such
as Generative Agents (Smallville) and AgentSociety are discussed in terms of
their design choices, empirical grounding, and methodological innovations.
Particular attention is given to the challenges of behavioral fidelity,
calibration, and reproducibility in large-scale LLM-driven simulations. The
final section distinguishes between contexts where LLMs, like other black-box
systems, offer direct value-such as interactive simulations and serious
games-and those where their use is more problematic, notably in explanatory or
predictive modeling. The paper concludes by advocating for hybrid approaches
that integrate LLMs into traditional agent-based modeling platforms (GAMA,
Netlogo, etc), enabling modelers to combine the expressive flexibility of
language-based reasoning with the transparency and analytical rigor of
classical rule-based systems.

</details>


### [116] [Learning neuro-symbolic convergent term rewriting systems](https://arxiv.org/abs/2507.19372)
*Flavio Petruzzellis,Alberto Testolin,Alessandro Sperduti*

Main category: cs.AI

TL;DR: This paper introduces a neuro-symbolic architecture for learning term rewriting systems that outperforms strong neural baselines and matches top-tier models in reasoning benchmarks, with improved generalization and efficiency.


<details>
  <summary>Details</summary>
Motivation: Building neural systems that can learn to execute symbolic algorithms is a challenging open problem, especially when aiming for strong generalization and out-of-distribution performance.

Method: introducing a general framework for learning convergent term rewriting systems using a neuro-symbolic architecture inspired by the rewriting algorithm itself. Two modular implementations are presented: Neural Rewriting System (NRS) and Fast Neural Rewriting System (FastNRS).

Result: Both models can generalize to out-of-distribution instances, with FastNRS offering significant improvements in terms of memory efficiency, training speed, and inference time. The system was evaluated on four tasks and demonstrates versatility in a multi-domain learning scenario.

Conclusion: The proposed neural rewriting system matches or outperforms the latest o1-preview model from OpenAI and significantly outperforms two strong neural baselines.

Abstract: Building neural systems that can learn to execute symbolic algorithms is a
challenging open problem in artificial intelligence, especially when aiming for
strong generalization and out-of-distribution performance. In this work, we
introduce a general framework for learning convergent term rewriting systems
using a neuro-symbolic architecture inspired by the rewriting algorithm itself.
We present two modular implementations of such architecture: the Neural
Rewriting System (NRS) and the Fast Neural Rewriting System (FastNRS). As a
result of algorithmic-inspired design and key architectural elements, both
models can generalize to out-of-distribution instances, with FastNRS offering
significant improvements in terms of memory efficiency, training speed, and
inference time. We evaluate both architectures on four tasks involving the
simplification of mathematical formulas and further demonstrate their
versatility in a multi-domain learning scenario, where a single model is
trained to solve multiple types of problems simultaneously. The proposed system
significantly outperforms two strong neural baselines: the Neural Data Router,
a recent transformer variant specifically designed to solve algorithmic
problems, and GPT-4o, one of the most powerful general-purpose large-language
models. Moreover, our system matches or outperforms the latest o1-preview model
from OpenAI that excels in reasoning benchmarks.

</details>


### [117] [Hierarchical Deep Reinforcement Learning Framework for Multi-Year Asset Management Under Budget Constraints](https://arxiv.org/abs/2507.19458)
*Amir Fard,Arnold X. -X. Yuan*

Main category: cs.AI

TL;DR: This paper proposes a Hierarchical Deep Reinforcement Learning methodology specifically tailored to multi-year infrastructure planning, which decomposes the problem into two hierarchical levels: a high-level Budget Planner allocating annual budgets within explicit feasibility bounds, and a low-level Maintenance Planner prioritizing assets within the allocated budget.


<details>
  <summary>Details</summary>
Motivation: Complexity arising from combinatorial action spaces, diverse asset deterioration, stringent budget constraints, and environmental uncertainty significantly limits existing methods' scalability.

Method: Hierarchical Deep Reinforcement Learning methodology

Result: The method efficiently addresses exponential growth in the action space and ensures rigorous budget compliance. A case study evaluating sewer networks of varying sizes (10, 15, and 20 sewersheds) illustrates the effectiveness of the proposed approach.

Conclusion: The proposed Hierarchical Deep Reinforcement Learning methodology converges more rapidly, scales effectively, and consistently delivers near-optimal solutions even as network size grows.

Abstract: Budget planning and maintenance optimization are crucial for infrastructure
asset management, ensuring cost-effectiveness and sustainability. However, the
complexity arising from combinatorial action spaces, diverse asset
deterioration, stringent budget constraints, and environmental uncertainty
significantly limits existing methods' scalability. This paper proposes a
Hierarchical Deep Reinforcement Learning methodology specifically tailored to
multi-year infrastructure planning. Our approach decomposes the problem into
two hierarchical levels: a high-level Budget Planner allocating annual budgets
within explicit feasibility bounds, and a low-level Maintenance Planner
prioritizing assets within the allocated budget. By structurally separating
macro-budget decisions from asset-level prioritization and integrating linear
programming projection within a hierarchical Soft Actor-Critic framework, the
method efficiently addresses exponential growth in the action space and ensures
rigorous budget compliance. A case study evaluating sewer networks of varying
sizes (10, 15, and 20 sewersheds) illustrates the effectiveness of the proposed
approach. Compared to conventional Deep Q-Learning and enhanced genetic
algorithms, our methodology converges more rapidly, scales effectively, and
consistently delivers near-optimal solutions even as network size grows.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [118] [ApproxJoin: Approximate Matching for Efficient Verification in Fuzzy Set Similarity Join](https://arxiv.org/abs/2507.18891)
*Michael Mandulak,S M Ferdous,Sayan Ghosh,Mahantesh Halappanavar,George Slota*

Main category: cs.DB

TL;DR: ApproxJoin applies approximate maximum weight matching algorithms for computationally expensive fuzzy set similarity join verification, yielding performance improvements of 2-19x the state-of-the-art with high accuracy (99% recall).


<details>
  <summary>Details</summary>
Motivation: State-of-the-art methods within this domain improve performance through efficient filtering methods within the filter-verify framework, primarily to offset high verification costs induced by the usage of the Hungarian algorithm - an optimal matching method. Instead, we directly target the verification process to assess the efficacy of more efficient matching methods within candidate pair pruning.

Method: applying approximate maximum weight matching algorithms for computationally expensive fuzzy set similarity join verification. We comprehensively test the performance of three approximate matching methods: the Greedy, Locally Dominant and Paz Schwartzman methods

Result: ApproxJoin yields performance improvements of 2-19x the state-of-the-art with high accuracy (99% recall).

Conclusion: ApproxJoin yields performance improvements of 2-19x the state-of-the-art with high accuracy (99% recall).

Abstract: The set similarity join problem is a fundamental problem in data processing
and discovery, relying on exact similarity measures between sets. In the
presence of alterations, such as misspellings on string data, the fuzzy set
similarity join problem instead approximately matches pairs of elements based
on the maximum weighted matching of the bipartite graph representation of sets.
State-of-the-art methods within this domain improve performance through
efficient filtering methods within the filter-verify framework, primarily to
offset high verification costs induced by the usage of the Hungarian algorithm
- an optimal matching method. Instead, we directly target the verification
process to assess the efficacy of more efficient matching methods within
candidate pair pruning.
  We present ApproxJoin, the first work of its kind in applying approximate
maximum weight matching algorithms for computationally expensive fuzzy set
similarity join verification. We comprehensively test the performance of three
approximate matching methods: the Greedy, Locally Dominant and Paz Schwartzman
methods, and compare with the state-of-the-art approach using exact matching.
Our experimental results show that ApproxJoin yields performance improvements
of 2-19x the state-of-the-art with high accuracy (99% recall).

</details>


### [119] [Big Data Energy Systems: A Survey of Practices and Associated Challenges](https://arxiv.org/abs/2507.19154)
*Lunodzo J. Mwinuka,Massimo Cafaro,Lucas Pereira,Hugo Morais*

Main category: cs.DB

TL;DR: 本文回顾了能源系统大数据管理的研究趋势，强调了实践、机遇和挑战，探讨了当前存储和数据集成解决方案的局限性，并为实现增强的数据共享和法规遵从提供了实用的建议。


<details>
  <summary>Details</summary>
Motivation: 能源系统在极短的时间间隔内产生大量数据，这对高效数据管理提出了挑战。传统的数据管理方法通常难以扩展和访问，限制了它们的用处。即使是NoSQL数据库和云平台等更高级的解决方案也会遇到瓶颈，这会影响数据存储、检索和分析的效率。

Method: 对能源系统的大数据管理的研究趋势进行综述，重点介绍了实践、机遇和挑战，并使用选定的参考架构重点介绍了数据监管需求。

Result: 探讨了当前存储和数据集成解决方案的局限性，并研究了如何将新技术应用于能源领域。

Conclusion: 新兴技术（包括数据空间、各种数据管理架构、对等数据管理和区块链）为增强数据共享和法规遵从提供了新的见解和实践建议。

Abstract: Energy systems generate vast amounts of data in extremely short time
intervals, creating challenges for efficient data management. Traditional data
management methods often struggle with scalability and accessibility, limiting
their usefulness. More advanced solutions, such as NoSQL databases and
cloud-based platforms, have been adopted to address these issues. Still, even
these advanced solutions can encounter bottlenecks, which can impact the
efficiency of data storage, retrieval, and analysis. This review paper explores
the research trends in big data management for energy systems, highlighting the
practices, opportunities and challenges. Also, the data regulatory demands are
highlighted using chosen reference architectures. The review, in particular,
explores the limitations of current storage and data integration solutions and
examines how new technologies are applied to the energy sector. Novel insights
into emerging technologies, including data spaces, various data management
architectures, peer-to-peer data management, and blockchains, are provided,
along with practical recommendations for achieving enhanced data sharing and
regulatory compliance.

</details>


### [120] [DBMS-LLM Integration Strategies in Industrial and Business Applications: Current Status and Future Challenges](https://arxiv.org/abs/2507.19254)
*Zhengtong Yan,Gongsheng Yuan,Qingsong Guo,Jiaheng Lu*

Main category: cs.DB

TL;DR: This paper surveys recent developments in DBMS+LLM integration, categorizes five architectural patterns, and identifies key future challenges to achieve scalable and efficient integration of data management and language reasoning.


<details>
  <summary>Details</summary>
Motivation: The efficient integration of DBMSs and LLMs within a unified system offers significant opportunities but also introduces new technical challenges.

Method: categorize five representative architectural patterns based on their core design principles, strengths, and trade-offs. Based on this analysis, we further highlight several critical open challenges.

Result: five representative architectural patterns based on their core design principles, strengths, and trade-offs

Conclusion: This paper surveys recent developments in DBMS+LLM integration and identifies key future challenges. The aim is to provide a systematic understanding of the current integration landscape and to outline the unresolved issues that must be addressed to achieve scalable and efficient integration of traditional data management and advanced language reasoning in future intelligent applications.

Abstract: Modern enterprises are increasingly driven by the DATA+AI paradigm, in which
Database Management Systems (DBMSs) and Large Language Models (LLMs) have
become two foundational infrastructures powering a wide range of industrial and
business applications, such as enterprise analytics, intelligent customer
service, and data-driven decision-making. The efficient integration of DBMSs
and LLMs within a unified system offers significant opportunities but also
introduces new technical challenges. This paper surveys recent developments in
DBMS+LLM integration and identifies key future challenges. Specifically, we
categorize five representative architectural patterns based on their core
design principles, strengths, and trade-offs. Based on this analysis, we
further highlight several critical open challenges. We aim to provide a
systematic understanding of the current integration landscape and to outline
the unresolved issues that must be addressed to achieve scalable and efficient
integration of traditional data management and advanced language reasoning in
future intelligent applications.

</details>


### [121] [Properties for Paths in Graph Databases](https://arxiv.org/abs/2507.19329)
*Fernando Orejas,Elvira Pino,Renzo Angles,E. Pasarella,Nikos Milonakis*

Main category: cs.DB

TL;DR: 本文提出了一个用于定义图数据库中路径属性的形式体系，该形式体系可用于限制导航查询的解决方案的数量。


<details>
  <summary>Details</summary>
Motivation: 定义图数据库中路径的属性的形式，该形式可用于限制导航查询的解决方案的数量。

Method: 在查询语言中定义了一个形式体系，该形式体系结合了这些新结构，通过证明其与简单的逻辑语义的兼容性来证明其可靠性和完整性。

Result: 路径属性比寄存器自动机更具表现力。

Conclusion: 查询中使用路径属性作为过滤器的性能优于不使用它们的标准查询。

Abstract: This paper presents a formalism for defining properties of paths in graph
databases, which can be used to restrict the number of solutions to
navigational queries. In particular, our formalism allows us to define
quantitative properties such as length or accumulated cost, which can be used
as query filters. Furthermore, it enables the identification and removal of
paths that may be considered ill-formed.
  The new formalism is defined in terms of an operational semantics for the
query language that incorporates these new constructs, demonstrating its
soundness and completeness by proving its compatibility with a simple logical
semantics. We also analyze its expressive power, showing that path properties
are more expressive than register automata. Finally, after discussing some
complexity issues related to this new approach, we present an empirical
analysis carried out using our prototype implementation of the graph database
that serves as a running example throughout the paper. The results show that
queries using path properties as filters outperform standard queries that do
not use them.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [122] [CityHood: An Explainable Travel Recommender System for Cities and Neighborhoods](https://arxiv.org/abs/2507.18778)
*Gustavo H Santos,Myriam Delgado,Thiago H Silva*

Main category: cs.IR

TL;DR: CityHood is an interactive recommendation system that suggests cities and neighborhoods based on user interests, using Google Places reviews and explainable techniques.


<details>
  <summary>Details</summary>
Motivation: To provide personalized recommendations at city and neighborhood levels, supported by explainable techniques and natural-language explanations.

Method: The system models user interests leveraging large-scale Google Places reviews enriched with geographic, socio-demographic, political, and cultural indicators.

Result: The demo illustrates how spatial similarity, cultural alignment, and interest understanding can be used to make travel recommendations transparent and engaging.

Conclusion: This work bridges gaps in location-based recommendation by combining a kind of interest modeling, multi-scale analysis, and explainability in a user-facing system.

Abstract: We present CityHood, an interactive and explainable recommendation system
that suggests cities and neighborhoods based on users' areas of interest. The
system models user interests leveraging large-scale Google Places reviews
enriched with geographic, socio-demographic, political, and cultural
indicators. It provides personalized recommendations at city (Core-Based
Statistical Areas - CBSAs) and neighborhood (ZIP code) levels, supported by an
explainable technique (LIME) and natural-language explanations. Users can
explore recommendations based on their stated preferences and inspect the
reasoning behind each suggestion through a visual interface. The demo
illustrates how spatial similarity, cultural alignment, and interest
understanding can be used to make travel recommendations transparent and
engaging. This work bridges gaps in location-based recommendation by combining
a kind of interest modeling, multi-scale analysis, and explainability in a
user-facing system.

</details>


### [123] [Semantic IDs for Music Recommendation](https://arxiv.org/abs/2507.18800)
*M. Jeffrey Mei,Florian Henkel,Samuel E. Sandberg,Oliver Bembom,Andreas F. Ehmann*

Main category: cs.IR

TL;DR: 共享嵌入可减少模型大小，同时提高推荐准确性和多样性。


<details>
  <summary>Details</summary>
Motivation: 为下一个项目推荐训练推荐系统通常需要为每个项目学习独特的嵌入，这可能占用模型的大部分可训练参数。共享嵌入可以减少要存储在内存中的不同嵌入的数量。这允许使用更轻量级的模型；相应地，由于需要存储在内存中的嵌入更少，因此可以增加模型复杂性。

Method: 使用共享内容特征

Result: 在包括音乐流媒体服务上的在线 A/B 测试在内的两个音乐推荐数据集上展示了使用共享的基于内容的特征的优势。

Conclusion: 使用共享内容特征（“语义 ID”）可以提高推荐准确性和多样性，同时减少模型大小。

Abstract: Training recommender systems for next-item recommendation often requires
unique embeddings to be learned for each item, which may take up most of the
trainable parameters for a model. Shared embeddings, such as using content
information, can reduce the number of distinct embeddings to be stored in
memory. This allows for a more lightweight model; correspondingly, model
complexity can be increased due to having fewer embeddings to store in memory.
We show the benefit of using shared content-based features ('semantic IDs') in
improving recommendation accuracy and diversity, while reducing model size, for
two music recommendation datasets, including an online A/B test on a music
streaming service.

</details>


### [124] [A Comprehensive Review of AI-based Intelligent Tutoring Systems: Applications and Challenges](https://arxiv.org/abs/2507.18882)
*Meriem Zerkouk,Miloud Mihoubi,Belkacem Chikhaoui*

Main category: cs.IR

TL;DR: ITS effectiveness is mixed. More research is needed.


<details>
  <summary>Details</summary>
Motivation: Mixed results about the effectiveness of AI-based Intelligent Tutoring Systems (ITS).

Method: Systematic literature review method to analyze studies from 2010 to 2025, examining pedagogical strategies, NLP, adaptive learning, student modeling, and domain-specific applications.

Result: Complex landscape regarding the effectiveness of ITS, highlighting both advancements and persistent challenges.

Conclusion: Effectiveness of ITS is complex with advancements and challenges. Need greater scientific rigor in experimental design and data analysis. Suggestions for future research and practical implications are proposed.

Abstract: AI-based Intelligent Tutoring Systems (ITS) have significant potential to
transform teaching and learning. As efforts continue to design, develop, and
integrate ITS into educational contexts, mixed results about their
effectiveness have emerged. This paper provides a comprehensive review to
understand how ITS operate in real educational settings and to identify the
associated challenges in their application and evaluation. We use a systematic
literature review method to analyze numerous qualified studies published from
2010 to 2025, examining domains such as pedagogical strategies, NLP, adaptive
learning, student modeling, and domain-specific applications of ITS. The
results reveal a complex landscape regarding the effectiveness of ITS,
highlighting both advancements and persistent challenges. The study also
identifies a need for greater scientific rigor in experimental design and data
analysis. Based on these findings, suggestions for future research and
practical implications are proposed.

</details>


### [125] [Agent0: Leveraging LLM Agents to Discover Multi-value Features from Text for Enhanced Recommendations](https://arxiv.org/abs/2507.18993)
*Blaž Škrlj,Benoît Guilleminot,Andraž Tori*

Main category: cs.IR

TL;DR: Agent0是一个LLM驱动的agent系统，可以自动化信息提取和特征构建，特别是在推荐系统中，发现闭环方法对于自动化特征发现是实用且有效的。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型及其相关的基于代理的框架已经显著推进了自动化信息提取，这是现代推荐系统的一个关键组成部分。尽管这些多任务框架被广泛应用于代码生成，但它们在以数据为中心的研究中的应用在很大程度上仍未开发。

Method: Agent0系统，一个基于LLM驱动的agent系统，用于自动化信息提取和特征构建。

Result: Agent0能够自动识别最有价值的文本方面，用于后续任务。

Conclusion: Agent0的闭环方法对于自动化特征发现是实用且有效的。

Abstract: Large language models (LLMs) and their associated agent-based frameworks have
significantly advanced automated information extraction, a critical component
of modern recommender systems. While these multitask frameworks are widely used
in code generation, their application in data-centric research is still largely
untapped. This paper presents Agent0, an LLM-driven, agent-based system
designed to automate information extraction and feature construction from raw,
unstructured text. Categorical features are crucial for large-scale recommender
systems but are often expensive to acquire. Agent0 coordinates a group of
interacting LLM agents to automatically identify the most valuable text aspects
for subsequent tasks (such as models or AutoML pipelines). Beyond its feature
engineering capabilities, Agent0 also offers an automated prompt-engineering
tuning method that utilizes dynamic feedback loops from an oracle. Our findings
demonstrate that this closed-loop methodology is both practical and effective
for automated feature discovery, which is recognized as one of the most
challenging phases in current recommender system development.

</details>


### [126] [SelfRACG: Enabling LLMs to Self-Express and Retrieve for Code Generation](https://arxiv.org/abs/2507.19033)
*Qian Dong,Jia Chen,Qingyao Ai,Hongning Wang,Haitao Li,Yi Wu,Yao Hu,Yiqun Liu,Shaoping Ma*

Main category: cs.IR

TL;DR: SelfRACG是一种新颖的范例，它使大型语言模型（LLM）能够自我表达其信息需求，以增强RACG。


<details>
  <summary>Details</summary>
Motivation: 由于逻辑上的进展，即使是对于连续的代码片段，内容也经常会发散，从而导致内容上的差距。这种差距削弱了当前RACG方法的性能，因为基于内容匹配的外部检索模块无法推断LLM生成下一个代码片段的特定信息需求。

Method: SelfRACG包括一个信息需求表达模块和一个两阶段信息需求引导的训练策略，鼓励LLM表达它们的信息需求。

Result: SelfRACG可以检索与LLM自身信息需求更吻合的外部知识，与vanilla RACG相比，产生了卓越的生成性能。

Conclusion: SelfRACG可以检索与LLM自身信息需求更吻合的外部知识，与vanilla RACG相比，产生了卓越的生成性能。

Abstract: Existing retrieval-augmented code generation (RACG) methods typically use an
external retrieval module to fetch semantically similar code snippets used for
generating subsequent fragments. However, even for consecutive code fragments,
the content often diverges due to logical progression, resulting in a content
gap. This gap undermines the performance of current RACG methods, as
\textit{external} retrieval modules based on content matching fail to infer the
specific information need of LLMs to generate the next code fragment.
Therefore, we propose \textbf{SelfRACG}, a novel paradigm that enables large
language models (LLMs) to \textbf{Self}-express their information needs to
enhance \textbf{RACG}. Specifically, SelfRACG includes an information need
expression module and a two-stage information need-guided training strategy,
which encourages LLMs to express their information need. Extensive experiments
demonstrate that SelfRACG can retrieve external knowledge that better aligns
with the LLM's own information needs, resulting in superior generation
performance compared to vanilla RACG.

</details>


### [127] [PBiLoss: Popularity-Aware Regularization to Improve Fairness in Graph-Based Recommender Systems](https://arxiv.org/abs/2507.19067)
*Mohammad Naeimi,Mostafa Haghir Chehreghani*

Main category: cs.IR

TL;DR: PBiLoss是一种新的损失函数，旨在减少推荐系统中的热门偏见，提高公平性，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 推荐系统容易出现热门偏见，导致内容多样性降低和公平性受损。

Method: PBiLoss，一种基于正则化的新型损失函数，通过惩罚模型对热门商品的倾向来抵消基于图的推荐模型中的热门偏见。

Result: PBiLoss显著提高了公平性，同时保持或提高推荐准确率和排名指标。提出的方法是模型无关的，可以无缝集成到最先进的基于图的框架中，如LightGCN及其变体。

Conclusion: PBiLoss显著提高了公平性，同时保持或提高推荐准确率和排名指标。

Abstract: Recommender systems, especially those based on graph neural networks (GNNs),
have achieved remarkable success in capturing user-item interaction patterns.
However, they remain susceptible to popularity bias--the tendency to
over-recommend popular items--resulting in reduced content diversity and
compromised fairness. In this paper, we propose PBiLoss, a novel
regularization-based loss function designed to counteract popularity bias in
graph-based recommender models explicitly. PBiLoss augments traditional
training objectives by penalizing the model's inclination toward popular items,
thereby encouraging the recommendation of less popular but potentially more
personalized content. We introduce two sampling strategies: Popular Positive
(PopPos) and Popular Negative (PopNeg), which respectively modulate the
contribution of the positive and negative popular items during training. We
further explore two methods to distinguish popular items: one based on a fixed
popularity threshold and another without any threshold, making the approach
flexible and adaptive. Our proposed method is model-agnostic and can be
seamlessly integrated into state-of-the-art graph-based frameworks such as
LightGCN and its variants. Comprehensive experiments across multiple real-world
datasets demonstrate that PBiLoss significantly improves fairness, as
demonstrated by reductions in the Popularity-Rank Correlation for Users (PRU)
and Popularity-Rank Correlation for Items (PRI), while maintaining or even
enhancing standard recommendation accuracy and ranking metrics. These results
highlight the effectiveness of directly embedding fairness objectives into the
optimization process, providing a practical and scalable solution for balancing
accuracy and equitable content exposure in modern recommender systems.

</details>


### [128] [Distilling a Small Utility-Based Passage Selector to Enhance Retrieval-Augmented Generation](https://arxiv.org/abs/2507.19102)
*Hengran Zhang,Keping Bi,Jiafeng Guo,Jiaming Zhang,Shuaiqiang Wang,Dawei Yin,Xueqi Cheng*

Main category: cs.IR

TL;DR: Propose a method to distill the utility judgment capabilities of LLMs into smaller, more efficient models for retrieval-augmented generation (RAG), focusing on utility-based selection to reduce computational costs and improve answer quality, especially for complex questions.


<details>
  <summary>Details</summary>
Motivation: The high computational cost of using LLMs for utility judgments limits the number of passages evaluated, which is problematic for complex queries requiring extensive information.

Method: Distill the utility judgment capabilities of LLMs into smaller, more efficient models, focusing on utility-based selection rather than ranking, enabling dynamic passage selection tailored to specific queries without the need for fixed thresholds. Train student models to learn pseudo-answer generation and utility judgments from teacher LLMs, using a sliding window method that dynamically selects useful passages.

Result: Utility-based selection provides a flexible and cost-effective solution for RAG, significantly reducing computational costs while improving answer quality. Distillation results using Qwen3-32B as the teacher model into RankQwen1.7B and UtilityQwen1.7B.

Conclusion: Utility-based selection is more effective than relevance ranking in enhancing answer generation performance, especially for complex questions. The authors will release the relevance ranking and utility-based selection annotations for the MS MARCO dataset.

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
incorporating retrieved information. Standard retrieval process prioritized
relevance, focusing on topical alignment between queries and passages. In
contrast, in RAG, the emphasis has shifted to utility, which considers the
usefulness of passages for generating accurate answers. Despite empirical
evidence showing the benefits of utility-based retrieval in RAG, the high
computational cost of using LLMs for utility judgments limits the number of
passages evaluated. This restriction is problematic for complex queries
requiring extensive information. To address this, we propose a method to
distill the utility judgment capabilities of LLMs into smaller, more efficient
models. Our approach focuses on utility-based selection rather than ranking,
enabling dynamic passage selection tailored to specific queries without the
need for fixed thresholds. We train student models to learn pseudo-answer
generation and utility judgments from teacher LLMs, using a sliding window
method that dynamically selects useful passages. Our experiments demonstrate
that utility-based selection provides a flexible and cost-effective solution
for RAG, significantly reducing computational costs while improving answer
quality. We present the distillation results using Qwen3-32B as the teacher
model for both relevance ranking and utility-based selection, distilled into
RankQwen1.7B and UtilityQwen1.7B. Our findings indicate that for complex
questions, utility-based selection is more effective than relevance ranking in
enhancing answer generation performance. We will release the relevance ranking
and utility-based selection annotations for the MS MARCO dataset, supporting
further research in this area.

</details>


### [129] [Towards LLM-Enhanced Group Recommender Systems](https://arxiv.org/abs/2507.19283)
*Sebastian Lubos,Alexander Felfernig,Thi Ngoc Trang Tran,Viet-Man Le,Damian Garber,Manuel Henrich,Reinhard Willfort,Jeremias Fuchs*

Main category: cs.IR

TL;DR: This paper analyzes how large language models can support group recommender systems to improve decision support quality and applicability.


<details>
  <summary>Details</summary>
Motivation: Group recommender systems introduce additional complexities compared to single-user systems, including understanding group dynamics, defining effective decision-making processes, ensuring recommendations are suitable for all, and providing group-level explanations.

Method: Analyzing how large language models (LLMs) can support group recommender systems.

Result: Ways that large language models (LLMs) can support these aspects and help to increase the overall decision support quality and applicability of group recommender systems.

Conclusion: LLMs can support aspects of group recommender systems and help to increase the overall decision support quality and applicability.

Abstract: In contrast to single-user recommender systems, group recommender systems are
designed to generate and explain recommendations for groups. This
group-oriented setting introduces additional complexities, as several factors -
absent in individual contexts - must be addressed. These include understanding
group dynamics (e.g., social dependencies within the group), defining effective
decision-making processes, ensuring that recommendations are suitable for all
group members, and providing group-level explanations as well as explanations
for individual users. In this paper, we analyze in which way large language
models (LLMs) can support these aspects and help to increase the overall
decision support quality and applicability of group recommender systems.

</details>


### [130] [Injecting External Knowledge into the Reasoning Process Enhances Retrieval-Augmented Generation](https://arxiv.org/abs/2507.19333)
*Minghao Tang,Shiyu Ni,Jiafeng Guo,Keping Bi*

Main category: cs.IR

TL;DR: Passage Injection improves overall RAG performance and consistently improves robustness by incorporating passages in LLMs' reasoning process


<details>
  <summary>Details</summary>
Motivation: Enhancing LLMs' robustness to such noise is critical for improving the reliability of RAG systems

Method: Passage Injection-a simple yet effective method that explicitly incorporates retrieved passages into LLMs' reasoning process

Result: Passage Injection significantly improves overall RAG performance and consistently improves robustness

Conclusion: incorporating passages in LLMs' reasoning process is a promising direction for building more robust RAG systems

Abstract: Retrieval-augmented generation (RAG) has been widely adopted to augment large
language models (LLMs) with external knowledge for knowledge-intensive tasks.
However, its effectiveness is often undermined by the presence of noisy (i.e.,
low-quality) retrieved passages. Enhancing LLMs' robustness to such noise is
critical for improving the reliability of RAG systems. Recent advances have
equipped LLMs with strong reasoning and self-reflection capabilities, allowing
them to identify and correct errors in their reasoning process. Inspired by
this ability, we propose Passage Injection-a simple yet effective method that
explicitly incorporates retrieved passages into LLMs' reasoning process, aiming
to enhance the model's ability to recognize and resist noisy passages. We
validate Passage Injection under general RAG settings using BM25 as the
retriever. Experiments on four reasoning-enhanced LLMs across four factual QA
datasets demonstrate that Passage Injection significantly improves overall RAG
performance. Further analysis on two noisy retrieval settings-random noise,
where the model is provided irrelevant passages, and counterfactual noise,
where it is given misleading passages-shows that Passage Injection consistently
improves robustness. Controlled experiments confirm that Passage Injection can
also effectively leverage helpful passages. These findings suggest that
incorporating passages in LLMs' reasoning process is a promising direction for
building more robust RAG systems. The code can be found
\href{here}{https://github.com/mh-tang/Passage-Injection}.

</details>


### [131] [Let It Go? Not Quite: Addressing Item Cold Start in Sequential Recommendations with Content-Based Initialization](https://arxiv.org/abs/2507.19473)
*Anton Pembek,Artem Fatkulin,Anton Klenitskiy,Alexey Vasilev*

Main category: cs.IR

TL;DR: The paper proposes adding a small trainable delta to frozen content embeddings to improve cold-start item recommendation in sequential recommender systems.


<details>
  <summary>Details</summary>
Motivation: Many sequential recommender systems suffer from the cold start problem, where items with few or no interactions cannot be effectively used by the model due to the absence of a trained embedding. Directly using frozen content embeddings often results in suboptimal performance, as they may not fully adapt to the recommendation task. Fine-tuning these embeddings can degrade performance for cold-start items, as item representations may drift far from their original structure after training.

Method: The paper introduces a small trainable delta to frozen embeddings.

Result: This approach demonstrates consistent improvements across multiple datasets and modalities, including e-commerce datasets with textual descriptions and a music dataset with audio-based representation.

Conclusion: This paper introduces a novel approach that adds a small trainable delta to frozen content embeddings. This allows the model to adapt item representations without significant deviation from their original semantic structure.

Abstract: Many sequential recommender systems suffer from the cold start problem, where
items with few or no interactions cannot be effectively used by the model due
to the absence of a trained embedding. Content-based approaches, which leverage
item metadata, are commonly used in such scenarios. One possible way is to use
embeddings derived from content features such as textual descriptions as
initialization for the model embeddings. However, directly using frozen content
embeddings often results in suboptimal performance, as they may not fully adapt
to the recommendation task. On the other hand, fine-tuning these embeddings can
degrade performance for cold-start items, as item representations may drift far
from their original structure after training. We propose a novel approach to
address this limitation. Instead of entirely freezing the content embeddings or
fine-tuning them extensively, we introduce a small trainable delta to frozen
embeddings that enables the model to adapt item representations without letting
them go too far from their original semantic structure. This approach
demonstrates consistent improvements across multiple datasets and modalities,
including e-commerce datasets with textual descriptions and a music dataset
with audio-based representation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [132] [Diffusion Models for Solving Inverse Problems via Posterior Sampling with Piecewise Guidance](https://arxiv.org/abs/2507.18654)
*Saeed Mohseni-Sehdeh,Walid Saad,Kei Sakaguchi,Tao Yu*

Main category: cs.LG

TL;DR: 提出了一种新的基于扩散模型的逆问题求解框架，它更快，且在各种逆问题中表现良好


<details>
  <summary>Details</summary>
Motivation: 现有的逆问题求解方法通常是任务特定的，需要为每个问题重新训练。此外，这些方法通常没有明确地将测量噪声纳入重建过程中。

Method: 该方法使用分段指导方案，将指导项定义为扩散时间步长的分段函数，在高噪声和低噪声阶段使用不同的近似。

Result: 在图像修复任务（图像修复和超分辨率）上，与PGDM基线相比，该框架在推理时间上减少了25%（修复）和23%-24%（超分辨率），而PSNR和SSIM的损失可忽略不计。

Conclusion: 该论文提出了一种基于扩散模型的逆问题求解框架，通过分段指导方案平衡计算效率和指导项的准确性，并在图像修复任务上验证了其有效性。

Abstract: Diffusion models are powerful tools for sampling from high-dimensional
distributions by progressively transforming pure noise into structured data
through a denoising process. When equipped with a guidance mechanism, these
models can also generate samples from conditional distributions. In this paper,
a novel diffusion-based framework is introduced for solving inverse problems
using a piecewise guidance scheme. The guidance term is defined as a piecewise
function of the diffusion timestep, facilitating the use of different
approximations during high-noise and low-noise phases. This design is shown to
effectively balance computational efficiency with the accuracy of the guidance
term. Unlike task-specific approaches that require retraining for each problem,
the proposed method is problem-agnostic and readily adaptable to a variety of
inverse problems. Additionally, it explicitly incorporates measurement noise
into the reconstruction process. The effectiveness of the proposed framework is
demonstrated through extensive experiments on image restoration tasks,
specifically image inpainting and super-resolution. Using a class conditional
diffusion model for recovery, compared to the \pgdm baseline, the proposed
framework achieves a reduction in inference time of \(25\%\) for inpainting
with both random and center masks, and \(23\%\) and \(24\%\) for \(4\times\)
and \(8\times\) super-resolution tasks, respectively, while incurring only
negligible loss in PSNR and SSIM.

</details>


### [133] [Efficient Knowledge Tracing Leveraging Higher-Order Information in Integrated Graphs](https://arxiv.org/abs/2507.18668)
*Donghee Han,Daehee Kim,Minjun Lee,Daeyoung Roh,Keejun Han,Mun Yong Yi*

Main category: cs.LG

TL;DR: DGAKT是一种更有效的知识追踪方法，它使用子图来减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的知识追踪(KT)方法忽略了利用大型图和长学习序列时计算成本增加的问题。

Method: 引入了基于双图注意力的知识追踪(DGAKT)，这是一种图神经网络模型，旨在利用来自表示学生-练习-KC关系子图的高阶信息。

Result: 通过处理每个目标交互的相关子图，与完整的全局图模型相比，DGAKT显著降低了内存和计算需求。

Conclusion: DGAKT不仅优于现有的KT模型，而且在资源效率方面树立了新标准。

Abstract: The rise of online learning has led to the development of various knowledge
tracing (KT) methods. However, existing methods have overlooked the problem of
increasing computational cost when utilizing large graphs and long learning
sequences. To address this issue, we introduce Dual Graph Attention-based
Knowledge Tracing (DGAKT), a graph neural network model designed to leverage
high-order information from subgraphs representing student-exercise-KC
relationships. DGAKT incorporates a subgraph-based approach to enhance
computational efficiency. By processing only relevant subgraphs for each target
interaction, DGAKT significantly reduces memory and computational requirements
compared to full global graph models. Extensive experimental results
demonstrate that DGAKT not only outperforms existing KT models but also sets a
new standard in resource efficiency, addressing a critical need that has been
largely overlooked by prior KT approaches.

</details>


### [134] [Innovator: Scientific Continued Pretraining with Fine-grained MoE Upcycling](https://arxiv.org/abs/2507.18671)
*Ning Liao,Xiaoxing Wang,Zehao Lin,Weiyang Guo,Feng Hong,Shixiang Song,Geng Yu,Zihua Zhao,Sitao Xie,Longxuan Wei,Xiangqi Jin,Xiaohan Qin,Jiale Ma,Kai Chen,Jiangchao Yao,Zhouhan Lin,Junchi Yan,Zhiyu Li,Feiyu Xiong,Yanfeng Wang,Linfeng Zhang*

Main category: cs.LG

TL;DR: Innovator solves the problem of catastrophic forgetting in LLMs by upcycling a pre-trained dense LLM into a fine-grained Mixtures-of-Experts model, achieving significant improvements in scientific tasks while retaining general abilities.


<details>
  <summary>Details</summary>
Motivation: Directly continued pretraining an LLM using science data usually leads to catastrophic forgetting, which indicates severe degradation in general ability.

Method: upcycling a pre-trained dense LLM into a fine-grained Mixtures-of-Experts model during continued pretraining, with a four-stage upcycle training paradigm: (1) Scientific Expert Induction, (2) Fine-grained Expert Splitting, (3) Science-Aware Routing warmup, and (4) Generalist-Scientist Integration training on hybrid datasets.

Result: Knowledge in the general domain, and different scientific disciplines can be decoupled, avoiding the negative influence among knowledge in different domains. Trained on 300B tokens with tri-level quality-controlled data.

Conclusion: Innovator, with 53.3B total parameters and 13.3B activated, achieves 25% average improvement across 30 scientific tasks with a win rate as 70%, while retaining 99% performance in general tasks. Innovator-Reason exhibits excellent reasoning performance in solving complex scientific problems with improvements over 30%.

Abstract: A large language model (LLM) with knowledge in both scientific and general
tasks is the foundation of science general intelligence. However, directly
continued pretraining an LLM using science data usually leads to catastrophic
forgetting, which indicates severe degradation in general ability. In this
report, we present Innovator, which solves this problem by upcycling a
pre-trained dense LLM into a fine-grained Mixtures-of-Experts model during
continued pretraining, where different experts are expected to learn science
knowledge in different disciplines, and a shared expert is utilized for general
tasks. Innovator introduces a four-stage upcycle training paradigm: (1)
Scientific Expert Induction on discipline-specific data, (2) Fine-grained
Expert Splitting via FFN dimension decomposition, (3) Science-Aware Routing
warmup, and (4) Generalist-Scientist Integration training on hybrid datasets.
Such a paradigm enables knowledge in the general domain, and different
scientific disciplines can be decoupled, avoiding the negative influence among
knowledge in different domains. With 53.3B total parameters and 13.3B
activated, Innovator extends Qwen2.5-7B using a shared general expert and 64
specialized scientific experts with 8 activated. Trained on 300B tokens with
tri-level quality-controlled data, Innovator achieves 25% average improvement
across 30 scientific tasks with a win rate as 70%, while retaining 99%
performance in general tasks. Furthermore, Innovator-Reason, which is
post-trained from Innovator for reasoning boosting, exhibits excellent
reasoning performance in solving complex scientific problems with improvements
over 30%.

</details>


### [135] [Market Making Strategies with Reinforcement Learning](https://arxiv.org/abs/2507.18680)
*Óscar Fernández Vicente*

Main category: cs.LG

TL;DR: This research explores how RL can be employed to develop autonomous, adaptive, and profitable market making strategies.


<details>
  <summary>Details</summary>
Motivation: Market makers (MMs) play a fundamental role in providing liquidity, yet face significant challenges arising from inventory risk, competition, and non-stationary market dynamics. This research explores how RL, particularly Deep Reinforcement Learning (DRL), can be employed to develop autonomous, adaptive, and profitable market making strategies.

Method: This research introduces POW-dTS, a novel policy weighting algorithm based on Discounted Thompson Sampling. This method allows agents to dynamically select and combine pretrained policies, enabling continual adaptation to shifting market conditions.

Result: The proposed RL-based approaches significantly outperform traditional and baseline algorithmic strategies across various performance metrics.

Conclusion: This research thesis contributes new methodologies and insights for the design of robust, efficient, and adaptive market making agents, reinforcing the potential of RL to transform algorithmic trading in complex financial systems.

Abstract: This thesis presents the results of a comprehensive research project focused
on applying Reinforcement Learning (RL) to the problem of market making in
financial markets. Market makers (MMs) play a fundamental role in providing
liquidity, yet face significant challenges arising from inventory risk,
competition, and non-stationary market dynamics. This research explores how RL,
particularly Deep Reinforcement Learning (DRL), can be employed to develop
autonomous, adaptive, and profitable market making strategies.
  The study begins by formulating the MM task as a reinforcement learning
problem, designing agents capable of operating in both single-agent and
multi-agent settings within a simulated financial environment. It then
addresses the complex issue of inventory management using two complementary
approaches: reward engineering and Multi-Objective Reinforcement Learning
(MORL). While the former uses dynamic reward shaping to guide behavior, the
latter leverages Pareto front optimization to explicitly balance competing
objectives.
  To address the problem of non-stationarity, the research introduces POW-dTS,
a novel policy weighting algorithm based on Discounted Thompson Sampling. This
method allows agents to dynamically select and combine pretrained policies,
enabling continual adaptation to shifting market conditions.
  The experimental results demonstrate that the proposed RL-based approaches
significantly outperform traditional and baseline algorithmic strategies across
various performance metrics. Overall, this research thesis contributes new
methodologies and insights for the design of robust, efficient, and adaptive
market making agents, reinforcing the potential of RL to transform algorithmic
trading in complex financial systems.

</details>


### [136] [Concept Probing: Where to Find Human-Defined Concepts (Extended Version)](https://arxiv.org/abs/2507.18681)
*Manuel de Sousa Ribeiro,Afonso Leote,João Leite*

Main category: cs.LG

TL;DR: This paper proposes a method to automatically identify which layer's representations in a neural network model should be considered when probing for a given human-defined concept of interest, based on how informative and regular the representations are with respect to the concept.


<details>
  <summary>Details</summary>
Motivation: Concept probing has recently gained popularity as a way for humans to peek into what is encoded within artificial neural networks. However, the performance of these probes is highly dependent on the internal representations they probe from, making identifying the appropriate layer to probe an essential task.

Method: We propose a method to automatically identify which layer's representations in a neural network model should be considered when probing for a given human-defined concept of interest, based on how informative and regular the representations are with respect to the concept.

Result: the performance of these probes is highly dependent on the internal representations they probe from, making identifying the appropriate layer to probe an essential task.

Conclusion: We validate our findings through an exhaustive empirical analysis over different neural network models and datasets.

Abstract: Concept probing has recently gained popularity as a way for humans to peek
into what is encoded within artificial neural networks. In concept probing,
additional classifiers are trained to map the internal representations of a
model into human-defined concepts of interest. However, the performance of
these probes is highly dependent on the internal representations they probe
from, making identifying the appropriate layer to probe an essential task. In
this paper, we propose a method to automatically identify which layer's
representations in a neural network model should be considered when probing for
a given human-defined concept of interest, based on how informative and regular
the representations are with respect to the concept. We validate our findings
through an exhaustive empirical analysis over different neural network models
and datasets.

</details>


### [137] [The Right to be Forgotten in Pruning: Unveil Machine Unlearning on Sparse Models](https://arxiv.org/abs/2507.18725)
*Yang Xiao,Gen Li,Jie Ji,Ruimeng Ye,Xiaolong Ma,Bo Hui*

Main category: cs.LG

TL;DR: 本文研究了稀疏模型中的 machine unlearning 问题，提出了一个un-pruning算法来消除删除数据对模型剪枝的影响，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在稀疏模型中进行 machine unlearning 的研究不足，删除的数据会对稀疏模型中的剪枝拓扑产生影响。

Method: 提出了一个un-pruning算法，用于近似由保留数据驱动的剪枝拓扑。

Result: 发现成员推理攻击（MIA）的准确性不可靠，并设计了新的性能指标来评估un-pruning的成功。

Conclusion: 提出了一个un-pruning算法，可以和现有的machine unlearning算法结合，并适用于结构化和非结构化稀疏模型。通过大量实验验证了un-pruning的有效性。

Abstract: Machine unlearning aims to efficiently eliminate the memory about deleted
data from trained models and address the right to be forgotten. Despite the
success of existing unlearning algorithms, unlearning in sparse models has not
yet been well studied. In this paper, we empirically find that the deleted data
has an impact on the pruned topology in a sparse model. Motivated by the
observation and the right to be forgotten, we define a new terminology
``un-pruning" to eliminate the impact of deleted data on model pruning. Then we
propose an un-pruning algorithm to approximate the pruned topology driven by
retained data. We remark that any existing unlearning algorithm can be
integrated with the proposed un-pruning workflow and the error of un-pruning is
upper-bounded in theory. Also, our un-pruning algorithm can be applied to both
structured sparse models and unstructured sparse models. In the experiment, we
further find that Membership Inference Attack (MIA) accuracy is unreliable for
assessing whether a model has forgotten deleted data, as a small change in the
amount of deleted data can produce arbitrary MIA results. Accordingly, we
devise new performance metrics for sparse models to evaluate the success of
un-pruning. Lastly, we conduct extensive experiments to verify the efficacy of
un-pruning with various pruning methods and unlearning algorithms. Our code is
released at https://anonymous.4open.science/r/UnlearningSparseModels-FBC5/.

</details>


### [138] [Exploitation Over Exploration: Unmasking the Bias in Linear Bandit Recommender Offline Evaluation](https://arxiv.org/abs/2507.18756)
*Pedro R. Pires,Gregorio F. Azevedo,Pietro L. Campos,Rafael T. Sereicikas,Tiago A. Almeida*

Main category: cs.LG

TL;DR: 对线性MAB的离线评估表明，纯粹的利用策略通常优于探索策略，揭示了当前评估协议的不足。


<details>
  <summary>Details</summary>
Motivation: 多臂老虎机（MAB）算法广泛应用于需要持续增量学习的推荐系统中。MAB 的一个核心问题是探索-利用的权衡。

Method: 对多个线性MAB进行了广泛的离线实证比较。

Result: 在一个贪婪的线性模型中，没有进行任何类型的探索，但在超过 90% 的各种数据集中，该模型始终能达到顶级的性能，通常优于或匹配其探索性的对应模型。

Conclusion: 离线评估方法在反映bandit算法的探索效果方面存在严重不足，需要开发更强大的评估方法。

Abstract: Multi-Armed Bandit (MAB) algorithms are widely used in recommender systems
that require continuous, incremental learning. A core aspect of MABs is the
exploration-exploitation trade-off: choosing between exploiting items likely to
be enjoyed and exploring new ones to gather information. In contextual linear
bandits, this trade-off is particularly central, as many variants share the
same linear regression backbone and differ primarily in their exploration
strategies. Despite its prevalent use, offline evaluation of MABs is
increasingly recognized for its limitations in reliably assessing exploration
behavior. This study conducts an extensive offline empirical comparison of
several linear MABs. Strikingly, across over 90% of various datasets, a greedy
linear model, with no type of exploration, consistently achieves top-tier
performance, often outperforming or matching its exploratory counterparts. This
observation is further corroborated by hyperparameter optimization, which
consistently favors configurations that minimize exploration, suggesting that
pure exploitation is the dominant strategy within these evaluation settings.
Our results expose significant inadequacies in offline evaluation protocols for
bandits, particularly concerning their capacity to reflect true exploratory
efficacy. Consequently, this research underscores the urgent necessity for
developing more robust assessment methodologies, guiding future investigations
into alternative evaluation frameworks for interactive learning in recommender
systems.

</details>


### [139] [CLEAR: Unlearning Spurious Style-Content Associations with Contrastive LEarning with Anti-contrastive Regularization](https://arxiv.org/abs/2507.18794)
*Minghui Sun,Benjamin A. Goldstein,Matthew M. Engelhard*

Main category: cs.LG

TL;DR: CLEAR 通过在训练期间有效地将必要的（即，任务相关的）特征与表面的（即，任务无关的）特征分离，从而在表面特征在测试时发生变化时获得更好的性能。


<details>
  <summary>Details</summary>
Motivation: 确保在测试时这些特征的变化不会影响下游预测性能，学习不受表面特征影响的表征非常重要。例如，在医疗保健应用中，我们可能希望学习包含关于病理学的信息，但不受种族、性别和其他生理变异来源影响的特征，从而确保预测在所有人口统计学上都是公平和可推广的。

Method: 对比学习与反对比正则化 (CLEAR)

Result: CLEAR-VAE 允许我们：(a) 在任何一对样本之间交换和插值内容和风格，以及 (b) 在存在先前未见的内容和风格组合的情况下提高下游分类性能。

Conclusion: CLEAR-VAE 允许我们：(a) 在任何一对样本之间交换和插值内容和风格，以及 (b) 在存在先前未见的内容和风格组合的情况下提高下游分类性能。

Abstract: Learning representations unaffected by superficial characteristics is
important to ensure that shifts in these characteristics at test time do not
compromise downstream prediction performance. For instance, in healthcare
applications, we might like to learn features that contain information about
pathology yet are unaffected by race, sex, and other sources of physiologic
variability, thereby ensuring predictions are equitable and generalizable
across all demographics. Here we propose Contrastive LEarning with
Anti-contrastive Regularization (CLEAR), an intuitive and easy-to-implement
framework that effectively separates essential (i.e., task-relevant)
characteristics from superficial (i.e., task-irrelevant) characteristics during
training, leading to better performance when superficial characteristics shift
at test time. We begin by supposing that data representations can be
semantically separated into task-relevant content features, which contain
information relevant to downstream tasks, and task-irrelevant style features,
which encompass superficial attributes that are irrelevant to these tasks, yet
may degrade performance due to associations with content present in training
data that do not generalize. We then prove that our anti-contrastive penalty,
which we call Pair-Switching (PS), minimizes the Mutual Information between the
style attributes and content labels. Finally, we instantiate CLEAR in the
latent space of a Variational Auto-Encoder (VAE), then perform experiments to
quantitatively and qualitatively evaluate the resulting CLEAR-VAE over several
image datasets. Our results show that CLEAR-VAE allows us to: (a) swap and
interpolate content and style between any pair of samples, and (b) improve
downstream classification performance in the presence of previously unseen
combinations of content and style. Our code will be made publicly available.

</details>


### [140] [Ralts: Robust Aggregation for Enhancing Graph Neural Network Resilience on Bit-flip Errors](https://arxiv.org/abs/2507.18804)
*Wencheng Zou,Nan Wu*

Main category: cs.LG

TL;DR: This paper proposes Ralts, a lightweight solution to bolster GNN resilience to bit-flip errors. Ralts improves prediction accuracy by at least 20\% when errors present in model weights or node embeddings, and by at least 10\% when errors occur in adjacency matrices.


<details>
  <summary>Details</summary>
Motivation: Existing research on GNN robustness has primarily focused on software-level threats, hardware-induced faults and errors remain largely underexplored. As hardware systems progress toward advanced technology nodes to meet high-performance and energy efficiency demands, they become increasingly susceptible to transient faults, which can cause bit flips and silent data corruption.

Method: Ralts exploits various graph similarity metrics to filter out outliers and recover compromised graph topology, and incorporates these protective techniques directly into aggregation functions to support any message-passing GNNs.

Result: Ralts effectively enhances GNN robustness across a range of GNN models, graph datasets, error patterns, and both dense and sparse architectures. On average, under a BER of $3\times10^{-5}$, these robust aggregation functions improve prediction accuracy by at least 20\% when errors present in model weights or node embeddings, and by at least 10\% when errors occur in adjacency matrices.

Conclusion: Ralts effectively enhances GNN robustness across a range of GNN models, graph datasets, error patterns, and both dense and sparse architectures. On average, under a BER of $3\times10^{-5}$, these robust aggregation functions improve prediction accuracy by at least 20\% when errors present in model weights or node embeddings, and by at least 10\% when errors occur in adjacency matrices. Ralts is also optimized to deliver execution efficiency comparable to built-in aggregation functions in PyTorch Geometric.

Abstract: Graph neural networks (GNNs) have been widely applied in safety-critical
applications, such as financial and medical networks, in which compromised
predictions may cause catastrophic consequences. While existing research on GNN
robustness has primarily focused on software-level threats, hardware-induced
faults and errors remain largely underexplored. As hardware systems progress
toward advanced technology nodes to meet high-performance and energy efficiency
demands, they become increasingly susceptible to transient faults, which can
cause bit flips and silent data corruption, a prominent issue observed by major
technology companies (e.g., Meta and Google). In response, we first present a
comprehensive analysis of GNN robustness against bit-flip errors, aiming to
reveal system-level optimization opportunities for future reliable and
efficient GNN systems. Second, we propose Ralts, a generalizable and
lightweight solution to bolster GNN resilience to bit-flip errors.
Specifically, Ralts exploits various graph similarity metrics to filter out
outliers and recover compromised graph topology, and incorporates these
protective techniques directly into aggregation functions to support any
message-passing GNNs. Evaluation results demonstrate that Ralts effectively
enhances GNN robustness across a range of GNN models, graph datasets, error
patterns, and both dense and sparse architectures. On average, under a BER of
$3\times10^{-5}$, these robust aggregation functions improve prediction
accuracy by at least 20\% when errors present in model weights or node
embeddings, and by at least 10\% when errors occur in adjacency matrices. Ralts
is also optimized to deliver execution efficiency comparable to built-in
aggregation functions in PyTorch Geometric.

</details>


### [141] [Fishers for Free? Approximating the Fisher Information Matrix by Recycling the Squared Gradient Accumulator](https://arxiv.org/abs/2507.18807)
*YuXin Li,Felix Dangel,Derek Tam,Colin Raffel*

Main category: cs.LG

TL;DR: 通过回收训练过程中计算出的平方梯度累加器，可以“免费”获得 Fisher 对角线的近似值，命名为 Squisher。


<details>
  <summary>Details</summary>
Motivation: 模型的 Fisher 信息矩阵的对角线（“Fisher 对角线”）经常被用作衡量参数敏感度的方法。通常，Fisher 对角线是通过模型可能性相对于其参数的平方采样梯度来估计的，在数百或数千个示例中取平均值——这个过程会产生大量的计算成本。同时，像无处不在的 Adam 优化器这样的自适应梯度方法会在训练过程中计算平方梯度的移动平均值。

Method: 通过回收已计算的平方梯度累加器来获得 Fisher 对角线的近似值。

Result: Squisher 始终表现与 Fisher 对角线相似，同时优于基线方法。

Conclusion: Squisher 表现与 Fisher 对角线相似，优于基线方法。澄清了 Squisher 和 Fisher 对角线之间的差异，并对其各自的影响进行了实证量化。

Abstract: The diagonal of a model's Fisher Information Matrix (the "Fisher diagonal")
has frequently been used as a way to measure parameter sensitivity. Typically,
the Fisher diagonal is estimated via squared sampled gradients of the model's
likelihood with respect to its parameters, averaged over a few hundred or
thousand examples -- a process which incurs nontrivial computational costs. At
the same time, adaptive gradient methods like the ubiquitous Adam optimizer
compute a moving average of the squared gradient over the course of training.
This paper therefore explores whether an approximation of the Fisher diagonal
can be obtained "for free" by recycling the squared gradient accumulator that
has already been computed over the course of training. Through a comprehensive
set of experiments covering five applications of the Fisher diagonal, we
demonstrate that the "Squisher" (SQUared gradient accumulator as an
approximation of the FISHER) consistently performs similarly to the Fisher
diagonal while outperforming baseline methods. Additionally, we clarify the
exact differences between the Squisher and the Fisher diagonal and provide
empirical quantification of their respective impact.

</details>


### [142] [Test-time Offline Reinforcement Learning on Goal-related Experience](https://arxiv.org/abs/2507.18809)
*Marco Bagatella,Mert Albaba,Jonas Hübotter,Georg Martius,Andreas Krause*

Main category: cs.LG

TL;DR: Goal-conditioned test-time training (GC-TTT) improves reinforcement learning performance by fine-tuning policies on relevant data during evaluation, outperforming standard methods and model scaling.


<details>
  <summary>Details</summary>
Motivation: Foundation models have shown performance improvements through test-time training, and parallels exist between this framework and offline goal-conditioned reinforcement learning.  The research investigates whether test-time offline reinforcement learning on relevant experience can improve policies.

Method: A novel self-supervised data selection criterion is used to select transitions from an offline dataset based on their relevance to the current state and quality with respect to the evaluation goal. This data is then used to fine-tune a policy during evaluation in a receding-horizon fashion.

Result: GC-TTT achieves significant performance gains over standard offline pre-training across various high-dimensional loco-navigation and manipulation tasks.  It also demonstrates that GC-TTT's performance gains are not achievable by simply scaling model size at comparable costs.

Conclusion: Goal-conditioned test-time training (GC-TTT) significantly improves performance in loco-navigation and manipulation tasks by fine-tuning a policy on data selected for its relevance to the current state and goal.

Abstract: Foundation models compress a large amount of information in a single, large
neural network, which can then be queried for individual tasks. There are
strong parallels between this widespread framework and offline goal-conditioned
reinforcement learning algorithms: a universal value function is trained on a
large number of goals, and the policy is evaluated on a single goal in each
test episode. Extensive research in foundation models has shown that
performance can be substantially improved through test-time training,
specializing the model to the current goal. We find similarly that test-time
offline reinforcement learning on experience related to the test goal can lead
to substantially better policies at minimal compute costs. We propose a novel
self-supervised data selection criterion, which selects transitions from an
offline dataset according to their relevance to the current state and quality
with respect to the evaluation goal. We demonstrate across a wide range of
high-dimensional loco-navigation and manipulation tasks that fine-tuning a
policy on the selected data for a few gradient steps leads to significant
performance gains over standard offline pre-training. Our goal-conditioned
test-time training (GC-TTT) algorithm applies this routine in a
receding-horizon fashion during evaluation, adapting the policy to the current
trajectory as it is being rolled out. Finally, we study compute allocation at
inference, demonstrating that, at comparable costs, GC-TTT induces performance
gains that are not achievable by scaling model size.

</details>


### [143] [Even Faster Simulations with Flow Matching: A Study of Zero Degree Calorimeter Responses](https://arxiv.org/abs/2507.18811)
*Maksymilian Wojnar*

Main category: cs.LG

TL;DR: 本研究利用 Flow Matching 模型加速了 ALICE 实验中零度量热仪的模拟，实现了更高的效率和精度。


<details>
  <summary>Details</summary>
Motivation: 为了加速高能物理 (HEP) 中的模拟，帮助研究机构满足不断增长的计算需求。

Method: 利用 Flow Matching (FM) 模型。

Result: 对于 ZN 模拟，FM 模型的 Wasserstein 距离为 1.27，每个样本的推理时间为 0.46 毫秒，而目前最好的结果是 1.20，推理时间约为 109 毫秒。对于 ZP 模拟，该方法的 Wasserstein 距离为 1.30，优于目前最好的结果 2.08。

Conclusion: 该研究利用 Flow Matching (FM) 模型为 ALICE 实验中的零度量热仪开发了快速模拟的替代模型，在 neutron (ZN) 和 proton (ZP) 探测器上都实现了最先进的模拟保真度，并显著降低了计算成本。

Abstract: Recent advances in generative neural networks, particularly flow matching
(FM), have enabled the generation of high-fidelity samples while significantly
reducing computational costs. A promising application of these models is
accelerating simulations in high-energy physics (HEP), helping research
institutions meet their increasing computational demands. In this work, we
leverage FM to develop surrogate models for fast simulations of zero degree
calorimeters in the ALICE experiment. We present an effective training strategy
that enables the training of fast generative models with an exceptionally low
number of parameters. This approach achieves state-of-the-art simulation
fidelity for both neutron (ZN) and proton (ZP) detectors, while offering
substantial reductions in computational costs compared to existing methods. Our
FM model achieves a Wasserstein distance of 1.27 for the ZN simulation with an
inference time of 0.46 ms per sample, compared to the current best of 1.20 with
an inference time of approximately 109 ms. The latent FM model further improves
the inference speed, reducing the sampling time to 0.026 ms per sample, with a
minimal trade-off in accuracy. Similarly, our approach achieves a Wasserstein
distance of 1.30 for the ZP simulation, outperforming the current best of 2.08.
The source code is available at https://github.com/m-wojnar/faster_zdc.

</details>


### [144] [Scale-Consistent Learning for Partial Differential Equations](https://arxiv.org/abs/2507.18813)
*Zongyi Li,Samuel Lanthaler,Catherine Deng,Michael Chen,Yixuan Wang,Kamyar Azizzadenesheli,Anima Anandkumar*

Main category: cs.LG

TL;DR: propose a data augmentation scheme based on scale-consistency properties of PDEs and design a scale-informed neural operator that can model a wide range of scales to overcome the limitations of previous ML models


<details>
  <summary>Details</summary>
Motivation: Previous ML models typically cannot generalize outside the training data; for example, a trained ML model for the Navier-Stokes equations only works for a fixed Reynolds number ($Re$) on a pre-defined domain. To overcome these limitations

Method: propose a data augmentation scheme based on scale-consistency properties of PDEs and design a scale-informed neural operator

Result: the model trained on $Re$ of 1000 can generalize to $Re$ ranging from 250 to 10000, and reduces the error by 34% on average of all datasets compared to baselines.

Conclusion: With scale-consistency, the model trained on $Re$ of 1000 can generalize to $Re$ ranging from 250 to 10000, and reduces the error by 34% on average of all datasets compared to baselines.

Abstract: Machine learning (ML) models have emerged as a promising approach for solving
partial differential equations (PDEs) in science and engineering. Previous ML
models typically cannot generalize outside the training data; for example, a
trained ML model for the Navier-Stokes equations only works for a fixed
Reynolds number ($Re$) on a pre-defined domain. To overcome these limitations,
we propose a data augmentation scheme based on scale-consistency properties of
PDEs and design a scale-informed neural operator that can model a wide range of
scales. Our formulation leverages the facts: (i) PDEs can be rescaled, or more
concretely, a given domain can be re-scaled to unit size, and the parameters
and the boundary conditions of the PDE can be appropriately adjusted to
represent the original solution, and (ii) the solution operators on a given
domain are consistent on the sub-domains. We leverage these facts to create a
scale-consistency loss that encourages matching the solutions evaluated on a
given domain and the solution obtained on its sub-domain from the rescaled PDE.
Since neural operators can fit to multiple scales and resolutions, they are the
natural choice for incorporating scale-consistency loss during training of
neural PDE solvers. We experiment with scale-consistency loss and the
scale-informed neural operator model on the Burgers' equation, Darcy Flow,
Helmholtz equation, and Navier-Stokes equations. With scale-consistency, the
model trained on $Re$ of 1000 can generalize to $Re$ ranging from 250 to 10000,
and reduces the error by 34% on average of all datasets compared to baselines.

</details>


### [145] [Weak-to-Strong Generalization with Failure Trajectories: A Tree-based Approach to Elicit Optimal Policy in Strong Models](https://arxiv.org/abs/2507.18858)
*Ruimeng Ye,Zihan Wang,Xiao Yang,Zinan Ling,Manling Li,Bo Hui*

Main category: cs.LG

TL;DR: This paper introduces a new Weak-to-Strong generalization framework for complex decision-making, using trajectory trees and MCTS to improve strong model performance by learning from both successful and failed trajectories generated by weak models.


<details>
  <summary>Details</summary>
Motivation: Elicit the full capabilities of a strong model with supervision from a weak model in complex interactive decision-making environments, inspired by the human learning process to generalize from both success and failure.

Method: Fine-tuning a strong model with trajectories of intermediate actions generated by a weak model, constructing trajectory trees, and using Monte Carlo Tree Search (MCTS) to optimize the strong model.

Result: Substantial improvements in reasoning and decision-making capabilities across diverse task domains.

Conclusion: The proposed framework improves reasoning and decision-making capabilities across diverse task domains, validating its scalability and robustness.

Abstract: Weak-to-Strong generalization (W2SG) is a new trend to elicit the full
capabilities of a strong model with supervision from a weak model. While
existing W2SG studies focus on simple tasks like binary classification, we
extend this paradigm to complex interactive decision-making environments.
Specifically, we fine-tune a strong model with trajectories of intermediate
actions generated by a weak model. Motivated by the human learning process, we
propose to generalize not only success knowledge but also failure experience so
that the strong model can learn from failed trajectories accumulated by weak
models. To effectively and efficiently elicit the potential of strong agents,
we further construct ``trajectory trees," a hierarchical representation that
organizes weak model-generated action trajectories, coupled with Monte Carlo
Tree Search (MCTS) to optimize the strong model. Through theoretical analysis,
we provide formal guarantees for the effectiveness of our method in improving
W2SG performance. Our empirical evaluations demonstrate substantial
improvements in reasoning and decision-making capabilities across diverse task
domains, validating the scalability and robustness of our proposed framework.
Our code is available at: https://github.com/yeruimeng/TraTree

</details>


### [146] [Early Mortality Prediction in ICU Patients with Hypertensive Kidney Disease Using Interpretable Machine Learning](https://arxiv.org/abs/2507.18866)
*Yong Si,Junyi Fan,Li Sun,Shuheng Chen,Minoo Ahmadi,Elham Pishgar,Kamiar Alaei,Greg Placencia,Maryam Pishgar*

Main category: cs.LG

TL;DR: 开发了一种用于预测ICU中HKD患者死亡率的机器学习模型，该模型具有良好的预测性能和可解释性，有助于临床决策。


<details>
  <summary>Details</summary>
Motivation: 重症监护病房(ICU)中的高血压肾脏疾病(HKD)患者面临较高的短期死亡率，但缺乏量身定制的风险预测工具。早期识别高风险个体对于临床决策至关重要。

Method: 使用来自MIMIC-IV v2.2数据库的早期临床数据，开发了一个机器学习框架来预测HKD ICU患者的30天住院死亡率。通过严格的标准筛选了1366名成年人，排除了恶性肿瘤病例。通过随机森林重要性和互信息过滤选择了18个临床特征，包括生命体征、实验室指标、合并症和治疗方法。训练了几个模型，并通过分层五重交叉验证进行比较；CatBoost表现出最佳性能。

Result: CatBoost在独立测试集上实现了0.88的AUROC，灵敏度为0.811，特异性为0.798。SHAP值和累积局部效应(ALE)图显示，该模型依赖于有意义的预测因子，如意识改变、血管升压药使用和凝血状态。此外，集成了DREAM算法来估计患者特定的后验风险分布，使临床医生能够评估预测的死亡率及其不确定性。

Conclusion: 开发了一个可解释的机器学习管道，用于HKD ICU患者的早期实时风险评估，通过结合高预测性能和不确定性量化，该模型支持个体化的分诊和透明的临床决策，这种方法在临床部署中显示出希望，并且值得在更广泛的重症监护人群中进行外部验证。

Abstract: Background: Hypertensive kidney disease (HKD) patients in intensive care
units (ICUs) face high short-term mortality, but tailored risk prediction tools
are lacking. Early identification of high-risk individuals is crucial for
clinical decision-making. Methods: We developed a machine learning framework to
predict 30-day in-hospital mortality among ICU patients with HKD using early
clinical data from the MIMIC-IV v2.2 database. A cohort of 1,366 adults was
curated with strict criteria, excluding malignancy cases. Eighteen clinical
features-including vital signs, labs, comorbidities, and therapies-were
selected via random forest importance and mutual information filtering. Several
models were trained and compared with stratified five-fold cross-validation;
CatBoost demonstrated the best performance. Results: CatBoost achieved an AUROC
of 0.88 on the independent test set, with sensitivity of 0.811 and specificity
of 0.798. SHAP values and Accumulated Local Effects (ALE) plots showed the
model relied on meaningful predictors such as altered consciousness,
vasopressor use, and coagulation status. Additionally, the DREAM algorithm was
integrated to estimate patient-specific posterior risk distributions, allowing
clinicians to assess both predicted mortality and its uncertainty. Conclusions:
We present an interpretable machine learning pipeline for early, real-time risk
assessment in ICU patients with HKD. By combining high predictive performance
with uncertainty quantification, our model supports individualized triage and
transparent clinical decisions. This approach shows promise for clinical
deployment and merits external validation in broader critical care populations.

</details>


### [147] [Learning Individual Intrinsic Reward in Multi-Agent Reinforcement Learning via Incorporating Generalized Human Expertise](https://arxiv.org/abs/2507.18867)
*Xuefei Wu,Xiao Yin,Yuanyang Zhu,Chunlin Chen*

Main category: cs.LG

TL;DR: LIGHT 结合人类知识，通过学习个体内在奖励来提高 MARL 算法在稀疏奖励环境中的探索效率，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在多智能体强化学习 (MARL) 中，当仅收到团队奖励时，尤其是在奖励稀疏的环境中，高效探索是一个具有挑战性的问题。缓解此问题的一种有效方法是设计密集的个体奖励，以引导智能体进行高效探索。然而，个体奖励通常依赖于手动设计的塑造奖励函数，这些函数缺乏高阶智能，因此在复杂问题的学习和泛化方面，其表现不如人类有效。

Method: LIGHT 通过结合广义人类专业知识来学习个体内在奖励，以端到端的方式将人类知识整合到 MARL 算法中。LIGHT 通过考虑个体动作分布和人类专业知识偏好分布来引导每个智能体避免不必要的探索。然后，LIGHT 基于与 Q 学习相关的可操作的表征转换，为每个智能体设计个体内在奖励，以便智能体在最大化联合动作价值的同时，使其动作偏好与人类专业知识对齐。

Result: 实验结果表明，我们的方法在具有挑战性的场景中的不同稀疏奖励任务中，在性能和更好的知识可重用性方面优于代表性的基线。

Conclusion: LIGHT在具有挑战性的场景中的不同稀疏奖励任务中，在性能和更好的知识可重用性方面优于代表性的基线。

Abstract: Efficient exploration in multi-agent reinforcement learning (MARL) is a
challenging problem when receiving only a team reward, especially in
environments with sparse rewards. A powerful method to mitigate this issue
involves crafting dense individual rewards to guide the agents toward efficient
exploration. However, individual rewards generally rely on manually engineered
shaping-reward functions that lack high-order intelligence, thus it behaves
ineffectively than humans regarding learning and generalization in complex
problems. To tackle these issues, we combine the above two paradigms and
propose a novel framework, LIGHT (Learning Individual Intrinsic reward via
Incorporating Generalized Human experTise), which can integrate human knowledge
into MARL algorithms in an end-to-end manner. LIGHT guides each agent to avoid
unnecessary exploration by considering both individual action distribution and
human expertise preference distribution. Then, LIGHT designs individual
intrinsic rewards for each agent based on actionable representational
transformation relevant to Q-learning so that the agents align their action
preferences with the human expertise while maximizing the joint action value.
Experimental results demonstrate the superiority of our method over
representative baselines regarding performance and better knowledge reusability
across different sparse-reward tasks on challenging scenarios.

</details>


### [148] [Geometric Multi-color Message Passing Graph Neural Networks for Blood-brain Barrier Permeability Prediction](https://arxiv.org/abs/2507.18926)
*Trung Nguyen,Md Masud Rana,Farjana Tasnim Mukta,Chang-Guo Zhan,Duc Duy Nguyen*

Main category: cs.LG

TL;DR: GMC-MPNN是一种新的图神经网络框架，它结合了几何信息，在血脑屏障渗透性预测方面优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 准确预测血脑屏障渗透性 (BBBP) 对于中枢神经系统 (CNS) 药物开发至关重要。虽然图神经网络 (GNNs) 已经促进了分子性质预测，但它们通常依赖于分子拓扑，而忽略了对于建模转运机制至关重要的三维几何信息。

Method: 提出了几何多色消息传递图神经网络 (GMC-MPNN)，通过显式地结合原子级别的几何特征和长程相互作用来增强标准消息传递架构。

Result: GMC-MPNN 在三个基准数据集上始终优于现有的最先进模型，在将化合物分类为可渗透/不可渗透（AUC-ROC 为 0.9704 和 0.9685）和回归连续渗透性值（RMSE 为 0.4609，Pearson 相关性为 0.7759）方面均实现了卓越的性能。

Conclusion: GMC-MPNN通过整合空间几何信息到图表示中，为血脑屏障渗透性预测设定了新的性能基准，并为药物发现流程提供了一个更准确和通用的工具。

Abstract: Accurate prediction of blood-brain barrier permeability (BBBP) is essential
for central nervous system (CNS) drug development. While graph neural networks
(GNNs) have advanced molecular property prediction, they often rely on
molecular topology and neglect the three-dimensional geometric information
crucial for modeling transport mechanisms. This paper introduces the geometric
multi-color message-passing graph neural network (GMC-MPNN), a novel framework
that enhances standard message-passing architectures by explicitly
incorporating atomic-level geometric features and long-range interactions. Our
model constructs weighted colored subgraphs based on atom types to capture the
spatial relationships and chemical context that govern BBB permeability. We
evaluated GMC-MPNN on three benchmark datasets for both classification and
regression tasks, using rigorous scaffold-based splitting to ensure a robust
assessment of generalization. The results demonstrate that GMC-MPNN
consistently outperforms existing state-of-the-art models, achieving superior
performance in both classifying compounds as permeable/non-permeable (AUC-ROC
of 0.9704 and 0.9685) and in regressing continuous permeability values (RMSE of
0.4609, Pearson correlation of 0.7759). An ablation study further quantified
the impact of specific atom-pair interactions, revealing that the model's
predictive power derives from its ability to learn from both common and rare,
but chemically significant, functional motifs. By integrating spatial geometry
into the graph representation, GMC-MPNN sets a new performance benchmark and
offers a more accurate and generalizable tool for drug discovery pipelines.

</details>


### [149] [Secure Best Arm Identification in the Presence of a Copycat](https://arxiv.org/abs/2507.18975)
*Asaf Cohen,Onur Günlü*

Main category: cs.LG

TL;DR: 本文提出了一种用于随机线性bandit的最佳臂识别的安全算法，该算法在保证安全性的同时，实现了竞争性的误差指数。


<details>
  <summary>Details</summary>
Motivation: 考虑具有安全约束的最佳臂识别问题。具体来说，假设一个随机线性bandits的设置，其中$K$臂的维度为$d$。在每次臂拉动中，玩家都会获得一个奖励，该奖励是臂与未知参数向量的点积之和以及独立噪声。玩家的目标是在$T$次臂拉动后识别出最佳臂。此外，假设一个模仿者Chloe正在观察臂拉动。玩家希望让Chloe不知道最佳臂。

Method: 提出了一种使用“编码臂”的安全算法

Result: 一个minimax--最优算法以$\\Omega\\left(\\frac{T}{\\log(d)}\\right)$的误差指数识别最佳臂，但它很容易向外部观察者揭示其最佳臂估计，因为最佳臂被更频繁地播放。一个简单的安全算法，平等地播放所有臂，导致一个$\\Omega\\left(\\frac{T}{d}\\right)$指数。

Conclusion: 我们提出了一个使用“编码臂”的安全算法，该算法不需要任何密钥或密码原语，但实现了$\\Omega\\left(\\frac{T}{\\log^2(d)}\\right)$指数，同时几乎不泄露有关最佳臂的信息。

Abstract: Consider the problem of best arm identification with a security constraint.
Specifically, assume a setup of stochastic linear bandits with $K$ arms of
dimension $d$. In each arm pull, the player receives a reward that is the sum
of the dot product of the arm with an unknown parameter vector and independent
noise. The player's goal is to identify the best arm after $T$ arm pulls.
Moreover, assume a copycat Chloe is observing the arm pulls. The player wishes
to keep Chloe ignorant of the best arm.
  While a minimax--optimal algorithm identifies the best arm with an
$\Omega\left(\frac{T}{\log(d)}\right)$ error exponent, it easily reveals its
best-arm estimate to an outside observer, as the best arms are played more
frequently. A naive secure algorithm that plays all arms equally results in an
$\Omega\left(\frac{T}{d}\right)$ exponent. In this paper, we propose a secure
algorithm that plays with \emph{coded arms}. The algorithm does not require any
key or cryptographic primitives, yet achieves an
$\Omega\left(\frac{T}{\log^2(d)}\right)$ exponent while revealing almost no
information on the best arm.

</details>


### [150] [KASPER: Kolmogorov Arnold Networks for Stock Prediction and Explainable Regimes](https://arxiv.org/abs/2507.18983)
*Vidhi Oad,Param Pathak,Nouhaila Innan,Shalini D,Muhammad Shafique*

Main category: cs.LG

TL;DR: This paper introduces KASPER, a novel framework for stock prediction that integrates regime detection, sparse spline-based function modeling, and symbolic rule extraction. It achieves high accuracy and interpretability on real-world financial time series.


<details>
  <summary>Details</summary>
Motivation: Forecasting in financial markets remains a significant challenge due to their nonlinear and regime-dependent dynamics. Traditional deep learning models often struggle to generalize across shifting market conditions, highlighting the need for a more adaptive and interpretable approach.

Method: a novel framework that integrates regime detection, sparse spline-based function modeling, and symbolic rule extraction

Result: the model achieves an $R^2$ score of 0.89, a Sharpe Ratio of 12.02, and a mean squared error as low as 0.0001, outperforming existing methods.

Conclusion: This research establishes a new direction for regime-aware, transparent, and robust forecasting in financial markets.

Abstract: Forecasting in financial markets remains a significant challenge due to their
nonlinear and regime-dependent dynamics. Traditional deep learning models, such
as long short-term memory networks and multilayer perceptrons, often struggle
to generalize across shifting market conditions, highlighting the need for a
more adaptive and interpretable approach. To address this, we introduce
Kolmogorov-Arnold networks for stock prediction and explainable regimes
(KASPER), a novel framework that integrates regime detection, sparse
spline-based function modeling, and symbolic rule extraction. The framework
identifies hidden market conditions using a Gumbel-Softmax-based mechanism,
enabling regime-specific forecasting. For each regime, it employs
Kolmogorov-Arnold networks with sparse spline activations to capture intricate
price behaviors while maintaining robustness. Interpretability is achieved
through symbolic learning based on Monte Carlo Shapley values, which extracts
human-readable rules tailored to each regime. Applied to real-world financial
time series from Yahoo Finance, the model achieves an $R^2$ score of 0.89, a
Sharpe Ratio of 12.02, and a mean squared error as low as 0.0001, outperforming
existing methods. This research establishes a new direction for regime-aware,
transparent, and robust forecasting in financial markets.

</details>


### [151] [Differentiated Thyroid Cancer Recurrence Classification Using Machine Learning Models and Bayesian Neural Networks with Varying Priors: A SHAP-Based Interpretation of the Best Performing Model](https://arxiv.org/abs/2507.18987)
*HMNS Kumari,HMLS Kumari,UMMPK Nawarathne*

Main category: cs.LG

TL;DR: This study introduces a framework for DTC recurrence classification using machine learning models and Bayesian Neural Networks (BNN) to improve accuracy and address the lack of uncertainty quantification in clinical decision-making.


<details>
  <summary>Details</summary>
Motivation: Differentiated thyroid cancer DTC recurrence is a major public health concern, requiring classification and predictive models that are not only accurate but also interpretable and uncertainty aware. ML models often lack uncertainty quantification, which is critical in clinical decision making

Method: 11 machine learning ML models were employed using the complete dataset, where the Support Vector Machines SVM model achieved the highest accuracy of 0.9481.  feature selection was carried out using the Boruta algorithm, and the same ML models were applied to the reduced dataset, where it was observed that the Logistic Regression LR model obtained the maximum accuracy of 0.9611. Bayesian Neural Networks BNN with six varying prior distributions were implemented on both the complete and reduced datasets.

Result: SVM model achieved the highest accuracy of 0.9481. the Logistic Regression LR model obtained the maximum accuracy of 0.9611. The BNN model with Normal 0,10 prior distribution exhibited maximum accuracies of 0.9740 and 0.9870 before and after feature selection, respectively.

Conclusion: The Bayesian Neural Networks BNN model with Normal 0,10 prior distribution exhibited maximum accuracies of 0.9740 and 0.9870 before and after feature selection, respectively.

Abstract: Differentiated thyroid cancer DTC recurrence is a major public health
concern, requiring classification and predictive models that are not only
accurate but also interpretable and uncertainty aware. This study introduces a
comprehensive framework for DTC recurrence classification using a dataset
containing 383 patients and 16 clinical and pathological variables. Initially,
11 machine learning ML models were employed using the complete dataset, where
the Support Vector Machines SVM model achieved the highest accuracy of 0.9481.
To reduce complexity and redundancy, feature selection was carried out using
the Boruta algorithm, and the same ML models were applied to the reduced
dataset, where it was observed that the Logistic Regression LR model obtained
the maximum accuracy of 0.9611. However, these ML models often lack uncertainty
quantification, which is critical in clinical decision making. Therefore, to
address this limitation, the Bayesian Neural Networks BNN with six varying
prior distributions, including Normal 0,1, Normal 0,10, Laplace 0,1, Cauchy
0,1, Cauchy 0,2.5, and Horseshoe 1, were implemented on both the complete and
reduced datasets. The BNN model with Normal 0,10 prior distribution exhibited
maximum accuracies of 0.9740 and 0.9870 before and after feature selection,
respectively.

</details>


### [152] [GENIAL: Generative Design Space Exploration via Network Inversion for Low Power Algorithmic Logic Units](https://arxiv.org/abs/2507.18989)
*Maxence Bouvier,Ryan Amaudruz,Felix Arnold,Renzo Andri,Lukas Cavigelli*

Main category: cs.LG

TL;DR: GENIAL, a machine learning framework, automates the generation and optimization of arithmetic units, achieving significant power savings and improvements in logic functions.


<details>
  <summary>Details</summary>
Motivation: Optimizing arithmetic units is becoming increasingly important to reduce the footprint of digital systems, but conventional design flows are limited.

Method: A machine learning-based framework with a Transformer-based surrogate model trained in two stages.

Result: GENIAL is consistently more sample efficient than other methods and converges faster towards optimized designs.

Conclusion: GENIAL achieves up to 18% switching activity savings within multipliers and significant improvements on Finite State Machines.

Abstract: As AI workloads proliferate, optimizing arithmetic units is becoming
increasingly important to reduce the footprint of digital systems. Conventional
design flows, which often rely on manual or heuristics-based optimization, are
limited in their ability to thoroughly explore the vast design space. In this
paper, we introduce GENIAL, a machine learning-based framework for the
automatic generation and optimization of arithmetic units, more specifically
multipliers.
  At the core of GENIAL is a Transformer-based surrogate model trained in two
stages, involving self-supervised pretraining followed by supervised
finetuning, to robustly forecast key hardware metrics such as power and area
from abstracted design representations. By inverting the surrogate model,
GENIAL efficiently searches for new operand encodings that directly minimize
power consumption in arithmetic units for specific input data distributions.
Extensive experiments on large datasets demonstrate that GENIAL is consistently
more sample efficient than other methods, and converges faster towards
optimized designs. This enables to deploy a high-effort logic synthesis
optimization flow in the loop, improving the accuracy of the surrogate model.
Notably, GENIAL automatically discovers encodings that achieve up to 18%
switching activity savings within multipliers on representative AI workloads
compared with the conventional two's complement. We also demonstrate the
versatility of our approach by achieving significant improvements on Finite
State Machines, highlighting GENIAL's applicability for a wide spectrum of
logic functions. Together, these advances mark a significant step toward
automated Quality-of-Results-optimized combinational circuit generation for
digital systems.

</details>


### [153] [Reinforcement Learning via Conservative Agent for Environments with Random Delays](https://arxiv.org/abs/2507.18992)
*Jongsoo Lee,Jangwon Kim,Jiseok Jeong,Soohee Han*

Main category: cs.LG

TL;DR: 提出了一种保守代理算法，用于解决随机延迟环境下的强化学习问题，该算法将随机延迟环境转化为恒定延迟环境，并在连续控制任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的强化学习应用常常受到环境延迟反馈的阻碍，这违反了马尔可夫假设并带来了重大挑战。虽然已经为具有恒定延迟的环境提出了许多延迟补偿方法，但由于其固有的可变性和不可预测性，具有随机延迟的环境在很大程度上仍未被探索。

Method: 提出了一种名为保守代理的简单而鲁棒的智能体，用于在随机延迟下进行决策，该方法将随机延迟环境重新构建为其恒定延迟等效环境。

Result: 经验结果表明，该算法在渐近性能和样本效率方面均显著优于现有的基线算法。

Conclusion: 保守代理算法在连续控制任务上的表现显著优于现有基线算法，在渐近性能和样本效率方面均有提升。

Abstract: Real-world reinforcement learning applications are often hindered by delayed
feedback from environments, which violates the Markov assumption and introduces
significant challenges. Although numerous delay-compensating methods have been
proposed for environments with constant delays, environments with random delays
remain largely unexplored due to their inherent variability and
unpredictability. In this study, we propose a simple yet robust agent for
decision-making under random delays, termed the conservative agent, which
reformulates the random-delay environment into its constant-delay equivalent.
This transformation enables any state-of-the-art constant-delay method to be
directly extended to the random-delay environments without modifying the
algorithmic structure or sacrificing performance. We evaluate the conservative
agent-based algorithm on continuous control tasks, and empirical results
demonstrate that it significantly outperforms existing baseline algorithms in
terms of asymptotic performance and sample efficiency.

</details>


### [154] [Adapting to Fragmented and Evolving Data: A Fisher Information Perspective](https://arxiv.org/abs/2507.18996)
*Behraj Khan,Tahir Qasim Syed,Nouman Muhammad Durrani*

Main category: cs.LG

TL;DR: FADE是一种轻量级的、有理论基础的框架，用于在SCS下进行鲁棒学习，它在动态环境中表现良好，并且可以推广到联邦学习。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习系统在动态环境中运行时，经常面临序贯协变量偏移(SCS)，即输入分布随时间演变，而条件分布保持稳定。

Method: FADE采用了一种基于Fisher信息几何的、感知迁移的正则化机制，通过基于敏感性和稳定性的参数更新来指导适应。

Result: 在跨越视觉、语言和表格数据的七个基准测试中，FADE在严重偏移下实现了高达19%的更高精度，优于TENT和DIW等方法。

Conclusion: FADE在各种模态和迁移强度上都表现出鲁棒性，并在联邦学习中实现了可扩展和稳定的适应。

Abstract: Modern machine learning systems operating in dynamic environments often face
\textit{sequential covariate shift} (SCS), where input distributions evolve
over time while the conditional distribution remains stable. We introduce FADE
(Fisher-based Adaptation to Dynamic Environments), a lightweight and
theoretically grounded framework for robust learning under SCS. FADE employs a
shift-aware regularization mechanism anchored in Fisher information geometry,
guiding adaptation by modulating parameter updates based on sensitivity and
stability. To detect significant distribution changes, we propose a
Cramer-Rao-informed shift signal that integrates KL divergence with temporal
Fisher dynamics. Unlike prior methods requiring task boundaries, target
supervision, or experience replay, FADE operates online with fixed memory and
no access to target labels. Evaluated on seven benchmarks spanning vision,
language, and tabular data, FADE achieves up to 19\% higher accuracy under
severe shifts, outperforming methods such as TENT and DIW. FADE also
generalizes naturally to federated learning by treating heterogeneous clients
as temporally fragmented environments, enabling scalable and stable adaptation
in decentralized settings. Theoretical analysis guarantees bounded regret and
parameter consistency, while empirical results demonstrate FADE's robustness
across modalities and shift intensities.

</details>


### [155] [A diffusion-based generative model for financial time series via geometric Brownian motion](https://arxiv.org/abs/2507.19003)
*Gihun Kim,Sun-Yong Choi,Yeoneung Kim*

Main category: cs.LG

TL;DR: 提出了一种新的基于扩散的金融时间序列生成框架，该框架将几何布朗运动（GBM）纳入前向噪声过程。


<details>
  <summary>Details</summary>
Motivation: 我们提出了一种新的基于扩散的金融时间序列生成框架，该框架将几何布朗运动（GBM）（Black-Scholes理论的基础）纳入前向噪声过程。

Method: 通过使用基于Transformer的架构，通过去噪分数匹配来训练逆时生成过程，该架构改编自条件分数扩散插补（CSDI）框架。

Result: 在历史股票数据上的经验评估表明，我们的模型能够比传统扩散模型更真实地再现关键的程式化事实，例如重尾收益分布、波动率聚集和杠杆效应。

Conclusion: 该模型比传统扩散模型更真实地再现了关键的程式化事实，例如重尾收益分布、波动率聚集和杠杆效应。

Abstract: We propose a novel diffusion-based generative framework for financial time
series that incorporates geometric Brownian motion (GBM), the foundation of the
Black--Scholes theory, into the forward noising process. Unlike standard
score-based models that treat price trajectories as generic numerical
sequences, our method injects noise proportionally to asset prices at each time
step, reflecting the heteroskedasticity observed in financial time series. By
accurately balancing the drift and diffusion terms, we show that the resulting
log-price process reduces to a variance-exploding stochastic differential
equation, aligning with the formulation in score-based generative models. The
reverse-time generative process is trained via denoising score matching using a
Transformer-based architecture adapted from the Conditional Score-based
Diffusion Imputation (CSDI) framework. Empirical evaluations on historical
stock data demonstrate that our model reproduces key stylized facts
heavy-tailed return distributions, volatility clustering, and the leverage
effect more realistically than conventional diffusion models.

</details>


### [156] [MindSpeed RL: Distributed Dataflow for Scalable and Efficient RL Training on Ascend NPU Cluster](https://arxiv.org/abs/2507.19017)
*Laingjun Feng,Chenyi Pan,Xinjie Guo,Fei Mei,Benzhe Ning,Jianxiang Zhang,Xinyang Liu,Beirong Zhou,Zeng Shu,Chang Liu,Guang Yang,Zhenyu Han,Jiangben Wang,Bo Wang*

Main category: cs.LG

TL;DR: MindSpeed RL, an effective and efficient system for large-scale RL training, increases the throughput by 1.42 ~ 3.97 times compared with existing state-of-the-art systems.


<details>
  <summary>Details</summary>
Motivation: RL training system usually suffers from poor cluster scalability and low memory utilization.

Method: introduce MindSpeed RL, an effective and efficient system for large-scale RL training. a distributed transfer dock strategy, which sets controllers and warehouses on the basis of the conventional replay buffer, is designed to release the dispatch overhead in the sample flow. A practical allgather--swap strategy is presented to eliminate redundant memory usage in resharding flow. MindSpeed RL further integrates numerous parallelization strategies and acceleration techniques for systematic optimization.

Result: MindSpeed RL increases the throughput by 1.42 ~ 3.97 times. demonstrate the powerful performance and reliability of Ascend.

Conclusion: MindSpeed RL increases the throughput by 1.42 ~ 3.97 times compared with existing state-of-the-art systems. MindSpeed RL is open--sourced and perform all the experiments on a super pod of Ascend with 384 neural processing units (NPUs) to demonstrate the powerful performance and reliability of Ascend.

Abstract: Reinforcement learning (RL) is a paradigm increasingly used to align large
language models. Popular RL algorithms utilize multiple workers and can be
modeled as a graph, where each node is the status of a worker and each edge
represents dataflow between nodes. Owing to the heavy cross-node dependencies,
the RL training system usually suffers from poor cluster scalability and low
memory utilization. In this article, we introduce MindSpeed RL, an effective
and efficient system for large-scale RL training. Unlike existing centralized
methods, MindSpeed RL organizes the essential data dependencies in RL training,
i.e., sample flow and resharding flow, from a distributed view. On the one
hand, a distributed transfer dock strategy, which sets controllers and
warehouses on the basis of the conventional replay buffer, is designed to
release the dispatch overhead in the sample flow. A practical allgather--swap
strategy is presented to eliminate redundant memory usage in resharding flow.
In addition, MindSpeed RL further integrates numerous parallelization
strategies and acceleration techniques for systematic optimization. Compared
with existing state-of-the-art systems, comprehensive experiments on the RL
training of popular Qwen2.5-Dense-7B/32B, Qwen3-MoE-30B, and
DeepSeek-R1-MoE-671B show that MindSpeed RL increases the throughput by 1.42 ~
3.97 times. Finally, we open--source MindSpeed RL and perform all the
experiments on a super pod of Ascend with 384 neural processing units (NPUs) to
demonstrate the powerful performance and reliability of Ascend.

</details>


### [157] [ProGMLP: A Progressive Framework for GNN-to-MLP Knowledge Distillation with Efficient Trade-offs](https://arxiv.org/abs/2507.19031)
*Weigang Lu,Ziyu Guan,Wei Zhao,Yaming Yang,Yujie Sun,Zheng Liang,Yibing Zhan,Dapeng Tao*

Main category: cs.LG

TL;DR: ProGMLP offers flexible trade-offs between inference cost and accuracy for GNN-to-MLP knowledge distillation by employing a Progressive Training Structure (PTS), Progressive Knowledge Distillation (PKD), and Progressive Mixup Augmentation (PMA).


<details>
  <summary>Details</summary>
Motivation: Existing G2M methods are limited by their inability to flexibly adjust inference cost and accuracy dynamically, a critical requirement for real-world applications where computational resources and time constraints can vary significantly.

Method: ProGMLP employs a Progressive Training Structure (PTS), where multiple MLP students are trained in sequence, each building on the previous one. Furthermore, ProGMLP incorporates Progressive Knowledge Distillation (PKD) to iteratively refine the distillation process from GNNs to MLPs, and Progressive Mixup Augmentation (PMA) to enhance generalization by progressively generating harder mixed samples.

Result: demonstrating that ProGMLP maintains high accuracy while dynamically adapting to varying runtime scenarios

Conclusion: ProGMLP maintains high accuracy while dynamically adapting to varying runtime scenarios, making it highly effective for deployment in diverse application settings.

Abstract: GNN-to-MLP (G2M) methods have emerged as a promising approach to accelerate
Graph Neural Networks (GNNs) by distilling their knowledge into simpler
Multi-Layer Perceptrons (MLPs). These methods bridge the gap between the
expressive power of GNNs and the computational efficiency of MLPs, making them
well-suited for resource-constrained environments. However, existing G2M
methods are limited by their inability to flexibly adjust inference cost and
accuracy dynamically, a critical requirement for real-world applications where
computational resources and time constraints can vary significantly. To address
this, we introduce a Progressive framework designed to offer flexible and
on-demand trade-offs between inference cost and accuracy for GNN-to-MLP
knowledge distillation (ProGMLP). ProGMLP employs a Progressive Training
Structure (PTS), where multiple MLP students are trained in sequence, each
building on the previous one. Furthermore, ProGMLP incorporates Progressive
Knowledge Distillation (PKD) to iteratively refine the distillation process
from GNNs to MLPs, and Progressive Mixup Augmentation (PMA) to enhance
generalization by progressively generating harder mixed samples. Our approach
is validated through comprehensive experiments on eight real-world graph
datasets, demonstrating that ProGMLP maintains high accuracy while dynamically
adapting to varying runtime scenarios, making it highly effective for
deployment in diverse application settings.

</details>


### [158] [Neural Ordinary Differential Equations for Learning and Extrapolating System Dynamics Across Bifurcations](https://arxiv.org/abs/2507.19036)
*Eva van Tegelen,George van Voorn,Ioannis Athanasiadis,Peter van Heijster*

Main category: cs.LG

TL;DR: Neural Ordinary Differential Equations can learn and forecast bifurcation structures from timeseries data, even with limited and noisy data.


<details>
  <summary>Details</summary>
Motivation: Forecasting system behaviour near and across bifurcations is crucial for identifying potential shifts in dynamical systems. Most studies remain limited as they exclusively focus on discrete-time methods and local bifurcations.

Method: Neural Ordinary Differential Equations which provide a continuous, data-driven framework for learning system dynamics. The approach is applied to a predator-prey system that features both local and global bifurcations.

Result: Neural Ordinary Differential Equations can recover underlying bifurcation structures directly from timeseries data by learning parameter-dependent vector fields. Neural Ordinary Differential Equations can forecast bifurcations even beyond the parameter regions represented in the training data. Model accuracy depends more on the quality of information that can be inferred from the training data, than on the amount of data available.

Conclusion: Neural Ordinary Differential Equations can recover underlying bifurcation structures directly from timeseries data by learning parameter-dependent vector fields and forecast bifurcations even beyond the parameter regions represented in the training data. Model accuracy depends more on the quality of information that can be inferred from the training data, than on the amount of data available.

Abstract: Forecasting system behaviour near and across bifurcations is crucial for
identifying potential shifts in dynamical systems. While machine learning has
recently been used to learn critical transitions and bifurcation structures
from data, most studies remain limited as they exclusively focus on
discrete-time methods and local bifurcations. To address these limitations, we
use Neural Ordinary Differential Equations which provide a continuous,
data-driven framework for learning system dynamics. We apply our approach to a
predator-prey system that features both local and global bifurcations,
presenting a challenging test case. Our results show that Neural Ordinary
Differential Equations can recover underlying bifurcation structures directly
from timeseries data by learning parameter-dependent vector fields. Notably, we
demonstrate that Neural Ordinary Differential Equations can forecast
bifurcations even beyond the parameter regions represented in the training
data. We also assess the method's performance under limited and noisy data
conditions, finding that model accuracy depends more on the quality of
information that can be inferred from the training data, than on the amount of
data available.

</details>


### [159] [Dynamics-Informed Reservoir Computing with Visibility Graphs](https://arxiv.org/abs/2507.19046)
*Charlotte Geier,Merten Stender*

Main category: cs.LG

TL;DR: propose a novel Dynamics-Informed Reservoir Computing (DyRC) framework that systematically infers the reservoir network structure directly from the input training sequence using visibility graph (VG) technique


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of complex and nonlinear time series remains a challenging problem across engineering and scientific disciplines. Reservoir computing (RC) offers a computationally efficient alternative to traditional deep learning by training only the read-out layer while employing a randomly structured and fixed reservoir network. Despite its advantages, the largely random reservoir graph architecture often results in suboptimal and oversized networks with poorly understood dynamics.

Method: a novel Dynamics-Informed Reservoir Computing (DyRC) framework that systematically infers the reservoir network structure directly from the input training sequence. This work proposes to employ the visibility graph (VG) technique, which converts time series data into networks by representing measurement points as nodes linked by mutual visibility. The reservoir network is constructed by directly adopting the VG network from a training data sequence

Result: We assess the DyRC-VG method through prediction tasks involving the canonical nonlinear Duffing oscillator, evaluating prediction accuracy and consistency.

Conclusion: Compared to an Erd	{o}s-R\'enyi graph of the same size, spectral radius, and comparable density, we observe higher prediction quality and more consistent performance over repeated implementations in the DyRC-VG.

Abstract: Accurate prediction of complex and nonlinear time series remains a
challenging problem across engineering and scientific disciplines. Reservoir
computing (RC) offers a computationally efficient alternative to traditional
deep learning by training only the read-out layer while employing a randomly
structured and fixed reservoir network. Despite its advantages, the largely
random reservoir graph architecture often results in suboptimal and oversized
networks with poorly understood dynamics. Addressing this issue, we propose a
novel Dynamics-Informed Reservoir Computing (DyRC) framework that
systematically infers the reservoir network structure directly from the input
training sequence. This work proposes to employ the visibility graph (VG)
technique, which converts time series data into networks by representing
measurement points as nodes linked by mutual visibility. The reservoir network
is constructed by directly adopting the VG network from a training data
sequence, leveraging the parameter-free visibility graph approach to avoid
expensive hyperparameter tuning. This process results in a reservoir that is
directly informed by the specific dynamics of the prediction task under study.
We assess the DyRC-VG method through prediction tasks involving the canonical
nonlinear Duffing oscillator, evaluating prediction accuracy and consistency.
Compared to an Erd\H{o}s-R\'enyi graph of the same size, spectral radius, and
comparable density, we observe higher prediction quality and more consistent
performance over repeated implementations in the DyRC-VG.

</details>


### [160] [Exploring molecular assembly as a biosignature using mass spectrometry and machine learning](https://arxiv.org/abs/2507.19057)
*Lindsay A. Rutter,Abhishek Sharma,Ian Seet,David Obeh Alobo,An Goto,Leroy Cronin*

Main category: cs.LG

TL;DR: 分子组装是一种很有前景的非特定生命探测方法，可以通过质谱法测量。开发了一种机器学习模型来预测分子组装，为未来的天体生物学任务提供概念验证。


<details>
  <summary>Details</summary>
Motivation: 分子组装为探测地球以外的生命提供了一条有希望的途径，同时最大限度地减少了基于地球生命的假设。由于质谱仪将是未来太阳系任务的核心，因此预测其数据中的分子组装而无需阐明未知结构对于公正的生命探测至关重要。

Method: 开发了一种机器学习模型，可以高精度地预测分子组装。

Result: 分子组装可以直接使用质谱数据进行测量。模拟数据显示，即使是很小的仪器不一致也会使模型误差增加一倍，这突出了标准化的必要性。

Conclusion: 标准化的质谱数据库能够实现准确的分子组装预测，无需结构解析，为未来的天体生物学任务提供了概念验证。

Abstract: Molecular assembly offers a promising path to detect life beyond Earth, while
minimizing assumptions based on terrestrial life. As mass spectrometers will be
central to upcoming Solar System missions, predicting molecular assembly from
their data without needing to elucidate unknown structures will be essential
for unbiased life detection. An ideal agnostic biosignature must be
interpretable and experimentally measurable. Here, we show that molecular
assembly, a recently developed approach to measure objects that have been
produced by evolution, satisfies both criteria. First, it is interpretable for
life detection, as it reflects the assembly of molecules with their bonds as
building blocks, in contrast to approaches that discount construction history.
Second, it can be determined without structural elucidation, as it can be
physically measured by mass spectrometry, a property that distinguishes it from
other approaches that use structure-based information measures for molecular
complexity. Whilst molecular assembly is directly measurable using mass
spectrometry data, there are limits imposed by mission constraints. To address
this, we developed a machine learning model that predicts molecular assembly
with high accuracy, reducing error by three-fold compared to baseline models.
Simulated data shows that even small instrumental inconsistencies can double
model error, emphasizing the need for standardization. These results suggest
that standardized mass spectrometry databases could enable accurate molecular
assembly prediction, without structural elucidation, providing a
proof-of-concept for future astrobiology missions.

</details>


### [161] [Clustering-Oriented Generative Attribute Graph Imputation](https://arxiv.org/abs/2507.19085)
*Mulin Chen,Bocheng Wang,Jiaxin Zhong,Zongcheng Miao,Xuelong Li*

Main category: cs.LG

TL;DR: CGIR模型通过子簇搜索和合并来实现节点插补和细化，并在属性缺失图聚类中表现出色。


<details>
  <summary>Details</summary>
Motivation: 大多数插补方法未能捕获类相关的语义信息，导致聚类的次优插补。此外，现有的细化策略通过图重建来优化学习到的嵌入，而忽略了一些属性与图不相关的这一事实。

Method: 建立了一种具有可靠细化的面向聚类的生成插补(CGIR)模型。具体地，估计子簇分布以精确地揭示类特定的特征，并约束生成对抗模块的采样空间，使得插补节点被迫与正确的簇对齐。然后，合并多个子簇以指导所提出的边缘注意网络，该网络识别每个类的边缘属性，从而避免图重建中的冗余属性干扰整体嵌入的细化。

Result: 大量的实验证明了CGIR相对于现有技术的优势。

Conclusion: CGIR模型将属性缺失图聚类分解为子簇的搜索和合并，从而指导在统一框架内实现节点插补和细化。大量的实验证明了CGIR相对于现有技术的优势。

Abstract: Attribute-missing graph clustering has emerged as a significant unsupervised
task, where only attribute vectors of partial nodes are available and the graph
structure is intact. The related models generally follow the two-step paradigm
of imputation and refinement. However, most imputation approaches fail to
capture class-relevant semantic information, leading to sub-optimal imputation
for clustering. Moreover, existing refinement strategies optimize the learned
embedding through graph reconstruction, while neglecting the fact that some
attributes are uncorrelated with the graph. To remedy the problems, we
establish the Clustering-oriented Generative Imputation with reliable
Refinement (CGIR) model. Concretely, the subcluster distributions are estimated
to reveal the class-specific characteristics precisely, and constrain the
sampling space of the generative adversarial module, such that the imputation
nodes are impelled to align with the correct clusters. Afterwards, multiple
subclusters are merged to guide the proposed edge attention network, which
identifies the edge-wise attributes for each class, so as to avoid the
redundant attributes in graph reconstruction from disturbing the refinement of
overall embedding. To sum up, CGIR splits attribute-missing graph clustering
into the search and mergence of subclusters, which guides to implement node
imputation and refinement within a unified framework. Extensive experiments
prove the advantages of CGIR over state-of-the-art competitors.

</details>


### [162] [GCL-GCN: Graphormer and Contrastive Learning Enhanced Attributed Graph Clustering Network](https://arxiv.org/abs/2507.19095)
*Binxiong Li,Xu Xiang,Xue Li,Binyu Zhao,Yujie Liu,Huijie Tang,Benhan Yang,Zhixuan Chen*

Main category: cs.LG

TL;DR: GCL-GCN是一种新的深度图聚类模型，它使用Graphormer模块和对比学习模块来提高聚类质量和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 由于图数据的复杂性和节点属性的异构性，利用图信息进行聚类仍然具有挑战性。

Method: 提出了一个新颖的深度图聚类模型GCL-GCN，它结合了中心性编码和空间关系的Graphormer模块，并提出了一个新的对比学习模块。

Result: GCL-GCN在聚类质量和鲁棒性方面优于14种先进方法。

Conclusion: GCL-GCN在六个数据集上优于14种先进方法，在Cora数据集上，相比于MBN，ACC提高了4.94%，NMI提高了13.01%，ARI提高了10.97%。

Abstract: Attributed graph clustering holds significant importance in modern data
analysis. However, due to the complexity of graph data and the heterogeneity of
node attributes, leveraging graph information for clustering remains
challenging. To address this, we propose a novel deep graph clustering model,
GCL-GCN, specifically designed to address the limitations of existing models in
capturing local dependencies and complex structures when dealing with sparse
and heterogeneous graph data. GCL-GCN introduces an innovative Graphormer
module that combines centrality encoding and spatial relationships, effectively
capturing both global and local information between nodes, thereby enhancing
the quality of node representations. Additionally, we propose a novel
contrastive learning module that significantly enhances the discriminative
power of feature representations. In the pre-training phase, this module
increases feature distinction through contrastive learning on the original
feature matrix, ensuring more identifiable initial representations for
subsequent graph convolution and clustering tasks. Extensive experimental
results on six datasets demonstrate that GCL-GCN outperforms 14 advanced
methods in terms of clustering quality and robustness. Specifically, on the
Cora dataset, it improves ACC, NMI, and ARI by 4.94%, 13.01%, and 10.97%,
respectively, compared to the primary comparison method MBN.

</details>


### [163] [Graph Structure Learning with Privacy Guarantees for Open Graph Data](https://arxiv.org/abs/2507.19116)
*Muhao Guo,Jiaqi Wu,Yang Weng,Yizheng Liao,Shengzhe Chen*

Main category: cs.LG

TL;DR: The paper proposes a new privacy-preserving method for publishing graph data that balances privacy and utility, using Gaussian DP with structured noise injection.


<details>
  <summary>Details</summary>
Motivation: Ensuring privacy in large-scale open datasets is increasingly challenging under regulations such as the General Data Protection Regulation (GDPR). Existing privacy-preserving data publishing (PPDP) approaches struggle to balance privacy and utility, particularly when data publishers and users are distinct entities. The paper aims to address the gap of neglecting privacy preservation at the data publishing stage.

Method: The paper proposes a privacy-preserving estimation framework for open graph data, leveraging Gaussian DP (GDP) with a structured noise-injection mechanism to ensure unbiased graph structure recovery while enforcing DP at the data publishing stage. The method is extended to discrete-variable graphs.

Result: The proposed method ensures unbiased graph structure recovery while enforcing DP at the data publishing stage. Theoretical guarantees on estimation accuracy are provided. Experimental results in graph learning demonstrate robust performance.

Conclusion: The paper introduces a novel privacy-preserving estimation framework for open graph data, leveraging Gaussian DP (GDP) with a structured noise-injection mechanism. Experimental results in graph learning demonstrate robust performance, offering a viable solution for privacy-conscious graph analysis.

Abstract: Ensuring privacy in large-scale open datasets is increasingly challenging
under regulations such as the General Data Protection Regulation (GDPR). While
differential privacy (DP) provides strong theoretical guarantees, it primarily
focuses on noise injection during model training, neglecting privacy
preservation at the data publishing stage. Existing privacy-preserving data
publishing (PPDP) approaches struggle to balance privacy and utility,
particularly when data publishers and users are distinct entities. To address
this gap, we focus on the graph recovery problem and propose a novel
privacy-preserving estimation framework for open graph data, leveraging
Gaussian DP (GDP) with a structured noise-injection mechanism. Unlike
traditional methods that perturb gradients or model updates, our approach
ensures unbiased graph structure recovery while enforcing DP at the data
publishing stage. Moreover, we provide theoretical guarantees on estimation
accuracy and extend our method to discrete-variable graphs, a setting often
overlooked in DP research. Experimental results in graph learning demonstrate
robust performance, offering a viable solution for privacy-conscious graph
analysis.

</details>


### [164] [Solar Photovoltaic Assessment with Large Language Model](https://arxiv.org/abs/2507.19144)
*Muhao Guo,Yang Weng*

Main category: cs.LG

TL;DR: This paper introduces PVAL, a framework leveraging LLMs for accurate and scalable solar panel detection in satellite imagery, overcoming limitations of existing methods through task decomposition, output standardization, and fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack transparency regarding their underlying algorithms or training datasets, rely on large, high-quality PV training data, and struggle to generalize to new geographic regions or varied environmental conditions without extensive re-training. These limitations lead to inconsistent detection outcomes, hindering large-scale deployment and data-driven grid optimization.

Method: We propose the PV Assessment with LLMs (PVAL) framework, which incorporates task decomposition for more efficient workflows, output standardization for consistent and scalable formatting, few-shot prompting to enhance classification accuracy, and fine-tuning using curated PV datasets with detailed annotations.

Result: PVAL ensures transparency, scalability, and adaptability across heterogeneous datasets while minimizing computational overhead.

Conclusion: PVAL establishes an automated and reproducible pipeline for solar panel detection, paving the way for large-scale renewable energy integration and optimized grid management.

Abstract: Accurate detection and localization of solar photovoltaic (PV) panels in
satellite imagery is essential for optimizing microgrids and active
distribution networks (ADNs), which are critical components of renewable energy
systems. Existing methods lack transparency regarding their underlying
algorithms or training datasets, rely on large, high-quality PV training data,
and struggle to generalize to new geographic regions or varied environmental
conditions without extensive re-training. These limitations lead to
inconsistent detection outcomes, hindering large-scale deployment and
data-driven grid optimization. In this paper, we investigate how large language
models (LLMs) can be leveraged to overcome these challenges. Despite their
promise, LLMs face several challenges in solar panel detection, including
difficulties with multi-step logical processes, inconsistent output formatting,
frequent misclassification of visually similar objects (e.g., shadows, parking
lots), and low accuracy in complex tasks such as spatial localization and
quantification. To overcome these issues, we propose the PV Assessment with
LLMs (PVAL) framework, which incorporates task decomposition for more efficient
workflows, output standardization for consistent and scalable formatting,
few-shot prompting to enhance classification accuracy, and fine-tuning using
curated PV datasets with detailed annotations. PVAL ensures transparency,
scalability, and adaptability across heterogeneous datasets while minimizing
computational overhead. By combining open-source accessibility with robust
methodologies, PVAL establishes an automated and reproducible pipeline for
solar panel detection, paving the way for large-scale renewable energy
integration and optimized grid management.

</details>


### [165] [Explainable AI guided unsupervised fault diagnostics for high-voltage circuit breakers](https://arxiv.org/abs/2507.19168)
*Chi-Ching Hsu,Gaëtan Frusque,Florent Forest,Felipe Macedo,Christian M. Franck,Olga Fink*

Main category: cs.LG

TL;DR: 本文提出了一种基于振动和声音信号的断路器无监督故障检测和分割框架，无需故障标签即可检测和诊断故障。


<details>
  <summary>Details</summary>
Motivation: 商业高压断路器 (CB) 状态监测系统依赖于直接可观察的物理参数，但这仅涵盖一小部分故障机制，并且通常只能在断路器与电网断开连接时进行监测。为了在 CB 保持连接的同时进行在线状态监测，需要振动或声学信号等非侵入式测量技术。然而，目前使用这些信号的 CB 状态监测研究通常采用有监督方法进行故障诊断，这在实际应用中是不可行的，因为无法获得故障标签。

Method: 该框架利用无监督学习方法检测断路器与健康状态的偏差，并结合可解释人工智能（XAI）进行故障诊断。

Result: 该研究提出了一种集成的无监督故障检测和分割框架，该框架能够检测故障并对不同的故障进行聚类，且在训练期间只需要健康数据。此外，还提供了一种使用 XAI 的无监督可解释性引导的故障诊断方法，为领域专家提供老化或故障组件的潜在指示，无需ground-truth故障标签即可实现故障诊断。

Conclusion: 本文提出了一种基于振动和声音信号的断路器无监督故障检测和分割框架，并通过实验数据集验证了其有效性，有助于提高断路器系统运行的可靠性。

Abstract: Commercial high-voltage circuit breaker (CB) condition monitoring systems
rely on directly observable physical parameters such as gas filling pressure
with pre-defined thresholds. While these parameters are crucial, they only
cover a small subset of malfunctioning mechanisms and usually can be monitored
only if the CB is disconnected from the grid. To facilitate online condition
monitoring while CBs remain connected, non-intrusive measurement techniques
such as vibration or acoustic signals are necessary. Currently, CB condition
monitoring studies using these signals typically utilize supervised methods for
fault diagnostics, where ground-truth fault types are known due to artificially
introduced faults in laboratory settings. This supervised approach is however
not feasible in real-world applications, where fault labels are unavailable. In
this work, we propose a novel unsupervised fault detection and segmentation
framework for CBs based on vibration and acoustic signals. This framework can
detect deviations from the healthy state. The explainable artificial
intelligence (XAI) approach is applied to the detected faults for fault
diagnostics. The specific contributions are: (1) we propose an integrated
unsupervised fault detection and segmentation framework that is capable of
detecting faults and clustering different faults with only healthy data
required during training (2) we provide an unsupervised explainability-guided
fault diagnostics approach using XAI to offer domain experts potential
indications of the aged or faulty components, achieving fault diagnostics
without the prerequisite of ground-truth fault labels. These contributions are
validated using an experimental dataset from a high-voltage CB under healthy
and artificially introduced fault conditions, contributing to more reliable CB
system operation.

</details>


### [166] [Automatic Cough Analysis for Non-Small Cell Lung Cancer Detection](https://arxiv.org/abs/2507.19174)
*Chiara Giangregorio,Cristina Maria Licciardello,Vanja Miskovic,Leonardo Provenzano,Alessandra Laura Giulia Pedrocchi,Andra Diana Dumitrascu,Arsela Prelaj,Marina Chiara Garassino,Emilia Ambrosini,Simona Ferrante*

Main category: cs.LG

TL;DR: This paper explores using machine learning to analyze coughs for early NSCLC detection, finding CNN performs best but SVM offers a balance of speed and interpretability. More diverse data is needed to improve reliability.


<details>
  <summary>Details</summary>
Motivation: Early detection of non-small cell lung cancer (NSCLC) is critical for improving patient outcomes, and novel approaches are needed to facilitate early diagnosis. In this study, we explore the use of automatic cough analysis as a pre-screening tool for distinguishing between NSCLC patients and healthy controls.

Method: Cough audio recordings were analyzed using machine learning techniques, such as support vector machine (SVM) and XGBoost, as well as deep learning approaches, specifically convolutional neural networks (CNN) and transfer learning with VGG16. To enhance the interpretability of the machine learning model, we utilized Shapley Additive Explanations (SHAP).

Result: The results demonstrate that CNN achieves the best performance, with an accuracy of 0.83 on the test set. The use of SHAP for SVM interpretation further enhances model transparency, making it more trustworthy for clinical applications.

Conclusion: CNN achieves the best performance, with an accuracy of 0.83 on the test set. Nevertheless, SVM achieves slightly lower performances (accuracy of 0.76 in validation and 0.78 in the test set), making it suitable in contexts with low computational power. Fairness analysis shows slightly higher disparity across age (0.15) than gender (0.09) on the test set. Therefore, to strengthen our findings' reliability, a larger, more diverse, and unbiased dataset is needed -- particularly including individuals at risk of NSCLC and those in early disease stages.

Abstract: Early detection of non-small cell lung cancer (NSCLC) is critical for
improving patient outcomes, and novel approaches are needed to facilitate early
diagnosis. In this study, we explore the use of automatic cough analysis as a
pre-screening tool for distinguishing between NSCLC patients and healthy
controls. Cough audio recordings were prospectively acquired from a total of
227 subjects, divided into NSCLC patients and healthy controls. The recordings
were analyzed using machine learning techniques, such as support vector machine
(SVM) and XGBoost, as well as deep learning approaches, specifically
convolutional neural networks (CNN) and transfer learning with VGG16. To
enhance the interpretability of the machine learning model, we utilized Shapley
Additive Explanations (SHAP). The fairness of the models across demographic
groups was assessed by comparing the performance of the best model across
different age groups (less than or equal to 58y and higher than 58y) and gender
using the equalized odds difference on the test set. The results demonstrate
that CNN achieves the best performance, with an accuracy of 0.83 on the test
set. Nevertheless, SVM achieves slightly lower performances (accuracy of 0.76
in validation and 0.78 in the test set), making it suitable in contexts with
low computational power. The use of SHAP for SVM interpretation further
enhances model transparency, making it more trustworthy for clinical
applications. Fairness analysis shows slightly higher disparity across age
(0.15) than gender (0.09) on the test set. Therefore, to strengthen our
findings' reliability, a larger, more diverse, and unbiased dataset is needed
-- particularly including individuals at risk of NSCLC and those in early
disease stages.

</details>


### [167] [WACA-UNet: Weakness-Aware Channel Attention for Static IR Drop Prediction in Integrated Circuit Design](https://arxiv.org/abs/2507.19197)
*Youngmin Seo,Yunhyeong Kwon,Younghun Park,HwiRyong Kim,Seungho Eum,Jinha Kim,Taigon Song,Juho Kim,Unsang Park*

Main category: cs.LG

TL;DR: reformulating IR drop estimation as a pixel-wise regression task with Weakness-Aware Channel Attention mechanism


<details>
  <summary>Details</summary>
Motivation: traditional simulation-based solvers are computationally expensive and difficult to scale for accurate spatial prediction of power integrity issues

Method: a novel Weakness-Aware Channel Attention (WACA) mechanism, which recursively enhances weak feature channels while suppressing over-dominant ones through a two-stage gating strategy. Integrated into a ConvNeXtV2-based attention U-Net

Result: outperforms the ICCAD-2023 contest winner by reducing mean absolute error by 61.1% and improving F1-score by 71.0%

Conclusion: channel-wise heterogeneity is a key inductive bias in physical layout analysis for VLSI

Abstract: Accurate spatial prediction of power integrity issues, such as IR drop, is
critical for reliable VLSI design. However, traditional simulation-based
solvers are computationally expensive and difficult to scale. We address this
challenge by reformulating IR drop estimation as a pixel-wise regression task
on heterogeneous multi-channel physical maps derived from circuit layouts.
Prior learning-based methods treat all input layers (e.g., metal, via, and
current maps) equally, ignoring their varying importance to prediction
accuracy. To tackle this, we propose a novel Weakness-Aware Channel Attention
(WACA) mechanism, which recursively enhances weak feature channels while
suppressing over-dominant ones through a two-stage gating strategy. Integrated
into a ConvNeXtV2-based attention U-Net, our approach enables adaptive and
balanced feature representation. On the public ICCAD-2023 benchmark, our method
outperforms the ICCAD-2023 contest winner by reducing mean absolute error by
61.1% and improving F1-score by 71.0%. These results demonstrate that
channel-wise heterogeneity is a key inductive bias in physical layout analysis
for VLSI.

</details>


### [168] [A Markov Categorical Framework for Language Modeling](https://arxiv.org/abs/2507.19247)
*Yifan Zhang*

Main category: cs.LG

TL;DR: This paper introduces a unifying analytical framework using Markov Categories to explain why NLL objective works for language models. It shows NLL training learns data stochasticity and acts as spectral contrastive learning.


<details>
  <summary>Details</summary>
Motivation: A deep theoretical understanding of why the negative log-likelihood (NLL) objective yields such versatile representations remains elusive.

Method: Analytical framework using Markov Categories (MCs) to deconstruct the AR generation process and the NLL objective.

Result: Formal, information-theoretic rationale for speculative decoding methods; Formalization of how NLL minimization forces the model to learn the data's intrinsic conditional stochasticity; Proof that NLL training acts as an implicit form of spectral contrastive learning.

Conclusion: NLL training acts as an implicit form of spectral contrastive learning, aligning the learned representation space with the eigenspectrum of a predictive similarity operator.

Abstract: Auto-regressive language models factorize sequence probabilities and are
trained by minimizing the negative log-likelihood (NLL) objective. While
empirically powerful, a deep theoretical understanding of why this simple
objective yields such versatile representations remains elusive. This work
introduces a unifying analytical framework using Markov Categories (MCs) to
deconstruct the AR generation process and the NLL objective. We model the
single-step generation map as a composition of Markov kernels in the category
Stoch. This compositional view, when enriched with statistical divergences,
allows us to dissect information flow and learned geometry. Our framework makes
three main contributions. First, we provide a formal, information-theoretic
rationale for the success of modern speculative decoding methods like EAGLE,
quantifying the information surplus in hidden states that these methods
exploit. Second, we formalize how NLL minimization forces the model to learn
not just the next token, but the data's intrinsic conditional stochasticity, a
process we analyze using categorical entropy. Third, and most centrally, we
prove that NLL training acts as an implicit form of spectral contrastive
learning. By analyzing the information geometry of the model's prediction head,
we show that NLL implicitly forces the learned representation space to align
with the eigenspectrum of a predictive similarity operator, thereby learning a
geometrically structured space without explicit contrastive pairs. This
compositional and information-geometric perspective reveals the deep structural
principles underlying the effectiveness of modern LMs. Project Page:
https://github.com/asiresearch/lm-theory

</details>


### [169] [Physics-Informed Graph Neural Networks for Transverse Momentum Estimation in CMS Trigger Systems](https://arxiv.org/abs/2507.19205)
*Md Abrar Jahin,Shahriar Soudeep,M. F. Mridha,Muhammad Mostafa Monowar,Md. Abdul Hamid*

Main category: cs.LG

TL;DR: Proposes a physics-informed GNN framework for real-time particle transverse momentum estimation in high-energy physics, achieving superior accuracy-efficiency trade-offs.


<details>
  <summary>Details</summary>
Motivation: Real-time particle transverse momentum estimation in high-energy physics demands efficient and accurate algorithms under strict hardware constraints. Static machine learning models degrade under high pileup and lack physics-aware optimization, while generic graph neural networks (GNNs) often neglect domain structure critical for robust pt regression.

Method: A physics-informed GNN framework that encodes detector geometry and physical observables through four distinct graph construction strategies.

Result: A station-informed EdgeConv model achieves a state-of-the-art MAE of 0.8525 with $\geq 55\%$ fewer parameters than deep learning baselines, especially TabNet, while an η-centric MPL configuration also demonstrates improved accuracy with comparable efficiency.

Conclusion: Physics-guided GNNs show promise for deployment in resource-constrained trigger systems.

Abstract: Real-time particle transverse momentum ($p_T$) estimation in high-energy
physics demands algorithms that are both efficient and accurate under strict
hardware constraints. Static machine learning models degrade under high pileup
and lack physics-aware optimization, while generic graph neural networks (GNNs)
often neglect domain structure critical for robust $p_T$ regression. We propose
a physics-informed GNN framework that systematically encodes detector geometry
and physical observables through four distinct graph construction strategies
that systematically encode detector geometry and physical observables:
station-as-node, feature-as-node, bending angle-centric, and pseudorapidity
($\eta$)-centric representations. This framework integrates these tailored
graph structures with a novel Message Passing Layer (MPL), featuring
intra-message attention and gated updates, and domain-specific loss functions
incorporating $p_{T}$-distribution priors. Our co-design methodology yields
superior accuracy-efficiency trade-offs compared to existing baselines.
Extensive experiments on the CMS Trigger Dataset validate the approach: a
station-informed EdgeConv model achieves a state-of-the-art MAE of 0.8525 with
$\ge55\%$ fewer parameters than deep learning baselines, especially TabNet,
while an $\eta$-centric MPL configuration also demonstrates improved accuracy
with comparable efficiency. These results establish the promise of
physics-guided GNNs for deployment in resource-constrained trigger systems.

</details>


### [170] [Dependency-aware synthetic tabular data generation](https://arxiv.org/abs/2507.19211)
*Chaithra Umesh,Kristian Schultz,Manjunath Mahendra,Saptarshi Bej,Olaf Wolkenhauer*

Main category: cs.LG

TL;DR: HFGF 框架通过首先生成独立特征，然后基于预定义的 FD 和 LD 规则重建依赖特征，从而改善合成表格数据中 FD 和 LD 的保留。


<details>
  <summary>Details</summary>
Motivation: 现有的生成模型通常无法保留属性间的关系，特别是在合成数据集中很少或很差地保留了功能依赖（FD）和逻辑依赖（LD）。

Method:  hierarchical feature generation framework (HFGF)

Result: 在具有不同大小、特征不平衡和依赖复杂性的四个基准数据集上的实验表明，HFGF 提高了六个生成模型（包括 CTGAN、TVAE 和 GReaT）对 FD 和 LD 的保留。

Conclusion: HFGF 显著提高了合成表格数据的结构保真度和下游效用。

Abstract: Synthetic tabular data is increasingly used in privacy-sensitive domains such
as health care, but existing generative models often fail to preserve
inter-attribute relationships. In particular, functional dependencies (FDs) and
logical dependencies (LDs), which capture deterministic and rule-based
associations between features, are rarely or often poorly retained in synthetic
datasets. To address this research gap, we propose the Hierarchical Feature
Generation Framework (HFGF) for synthetic tabular data generation. We created
benchmark datasets with known dependencies to evaluate our proposed HFGF. The
framework first generates independent features using any standard generative
model, and then reconstructs dependent features based on predefined FD and LD
rules. Our experiments on four benchmark datasets with varying sizes, feature
imbalance, and dependency complexity demonstrate that HFGF improves the
preservation of FDs and LDs across six generative models, including CTGAN,
TVAE, and GReaT. Our findings demonstrate that HFGF can significantly enhance
the structural fidelity and downstream utility of synthetic tabular data.

</details>


### [171] [Component-Based Machine Learning for Indoor Flow and Temperature Fields Prediction Latent Feature Aggregation and Flow Interaction](https://arxiv.org/abs/2507.19233)
*Shaofan Wang,Nils Thuerey,Philipp Geyer*

Main category: cs.LG

TL;DR: This study proposes a component-based machine learning (CBML) surrogate modeling approach to replace conventional CFD simulation for fast prediction of indoor velocity and temperature fields.


<details>
  <summary>Details</summary>
Motivation: Accurate and efficient prediction of indoor airflow and temperature distributions is essential for building energy optimization and occupant comfort control. However, traditional CFD simulations are computationally intensive, limiting their integration into real-time or design-iterative workflows.

Method: a component-based machine learning (CBML) surrogate modeling approach

Result: CBML model accurately and fast predicts two-component aggregated velocity and temperature fields across both training and testing datasets.

Conclusion: The CBML model accurately and fast predicts two-component aggregated velocity and temperature fields across both training and testing datasets.

Abstract: Accurate and efficient prediction of indoor airflow and temperature
distributions is essential for building energy optimization and occupant
comfort control. However, traditional CFD simulations are computationally
intensive, limiting their integration into real-time or design-iterative
workflows. This study proposes a component-based machine learning (CBML)
surrogate modeling approach to replace conventional CFD simulation for fast
prediction of indoor velocity and temperature fields. The model consists of
three neural networks: a convolutional autoencoder with residual connections
(CAER) to extract and compress flow features, a multilayer perceptron (MLP) to
map inlet velocities to latent representations, and a convolutional neural
network (CNN) as an aggregator to combine single-inlet features into dual-inlet
scenarios. A two-dimensional room with varying left and right air inlet
velocities is used as a benchmark case, with CFD simulations providing training
and testing data. Results show that the CBML model accurately and fast predicts
two-component aggregated velocity and temperature fields across both training
and testing datasets.

</details>


### [172] [Advancing Event Forecasting through Massive Training of Large Language Models: Challenges, Solutions, and Broader Impacts](https://arxiv.org/abs/2507.19477)
*Sang-Woo Lee,Sohee Yang,Donghyun Kwak,Noah Y. Siegel*

Main category: cs.LG

TL;DR: 本文认为现在是进行超预测器级别事件预测LLM大规模训练研究的成熟时机，并提出了训练方法和数据获取两个关键研究方向。


<details>
  <summary>Details</summary>
Motivation: 最近的研究表明，最先进的LLM正在逐渐达到超预测器的水平，并且强化学习也被报道可以提高未来预测。此外，最近的推理模型和深度研究风格模型的空前成功表明，能够大大提高预测性能的技术已经开发出来。

Method: 我们介绍了基于LLM的事件预测训练的三个难点：噪声-稀疏性、知识截断和简单的奖励结构问题。然后，我们提出了相关的想法来缓解这些问题：假设事件贝叶斯网络、利用不良召回和反事实事件以及辅助奖励信号。对于数据，我们建议积极使用市场、公共和爬取数据集以实现大规模训练和评估。

Result: 最先进的LLM正在逐渐达到超预测器的水平，并且强化学习也被报道可以提高未来预测。

Conclusion: 我们认为，现在是进行超预测器级别事件预测LLM大规模训练研究的成熟时机。我们提出了训练方法和数据获取两个关键研究方向，并讨论了如何利用技术进步使AI在更广泛的领域为社会提供预测智能。

Abstract: Many recent papers have studied the development of superforecaster-level
event forecasting LLMs. While methodological problems with early studies cast
doubt on the use of LLMs for event forecasting, recent studies with improved
evaluation methods have shown that state-of-the-art LLMs are gradually reaching
superforecaster-level performance, and reinforcement learning has also been
reported to improve future forecasting. Additionally, the unprecedented success
of recent reasoning models and Deep Research-style models suggests that
technology capable of greatly improving forecasting performance has been
developed. Therefore, based on these positive recent trends, we argue that the
time is ripe for research on large-scale training of superforecaster-level
event forecasting LLMs. We discuss two key research directions: training
methods and data acquisition. For training, we first introduce three
difficulties of LLM-based event forecasting training: noisiness-sparsity,
knowledge cut-off, and simple reward structure problems. Then, we present
related ideas to mitigate these problems: hypothetical event Bayesian networks,
utilizing poorly-recalled and counterfactual events, and auxiliary reward
signals. For data, we propose aggressive use of market, public, and crawling
datasets to enable large-scale training and evaluation. Finally, we explain how
these technical advances could enable AI to provide predictive intelligence to
society in broader areas. This position paper presents promising specific paths
and considerations for getting closer to superforecaster-level AI technology,
aiming to call for researchers' interest in these directions.

</details>


### [173] [Doubling Your Data in Minutes: Ultra-fast Tabular Data Generation via LLM-Induced Dependency Graphs](https://arxiv.org/abs/2507.19334)
*Shuo Yang,Zheyu Zhang,Bardh Prenkaj,Gjergji Kasneci*

Main category: cs.LG

TL;DR: SPADA 是一种轻量级生成框架，它通过 LLM 诱导的图显式地捕获稀疏依赖关系，减少了约束违反并加速了生成。


<details>
  <summary>Details</summary>
Motivation: 表格数据在不同领域中至关重要，但由于隐私问题和收集成本，高质量的数据集仍然稀缺。当前的方法采用大型语言模型 (LLM) 进行表格增强，但存在两个主要限制：(1) 表格特征之间的密集依赖关系建模，这可能会引入偏差，以及 (2) 采样中的高计算开销。

Method: SPADA：一种轻量级生成框架，它通过 LLM 诱导的图显式地捕获稀疏依赖关系。我们将每个特征视为一个节点，并通过遍历图来合成值，仅根据其父节点来调节每个特征。

Result: SPADA 减少了约束违反，并加速了生成。

Conclusion: SPADA在四个数据集上的实验表明，与基于扩散的方法相比，约束违反减少了 4%，并且与基于 LLM 的基线相比，生成速度提高了近 9,500 倍。

Abstract: Tabular data is critical across diverse domains, yet high-quality datasets
remain scarce due to privacy concerns and the cost of collection. Contemporary
approaches adopt large language models (LLMs) for tabular augmentation, but
exhibit two major limitations: (1) dense dependency modeling among tabular
features that can introduce bias, and (2) high computational overhead in
sampling. To address these issues, we propose SPADA for SPArse
Dependency-driven Augmentation, a lightweight generative framework that
explicitly captures sparse dependencies via an LLM-induced graph. We treat each
feature as a node and synthesize values by traversing the graph, conditioning
each feature solely on its parent nodes. We explore two synthesis strategies: a
non-parametric method using Gaussian kernel density estimation, and a
conditional normalizing flow model that learns invertible mappings for
conditional density estimation. Experiments on four datasets show that SPADA
reduces constraint violations by 4% compared to diffusion-based methods and
accelerates generation by nearly 9,500 times over LLM-based baselines.

</details>


### [174] [Short-Form Video Recommendations with Multimodal Embeddings: Addressing Cold-Start and Bias Challenges](https://arxiv.org/abs/2507.19346)
*Andrii Dzhoha,Katya Mirylenka,Egor Malykh,Marco-Andrea Buchmann,Francesca Catino*

Main category: cs.LG

TL;DR: 本文重点介绍了引入新的短视频体验时面临的挑战，并展示了我们的经验，表明即使拥有足够的视频互动数据，利用使用微调的多模态视觉语言模型的视频检索系统也可能更有益。


<details>
  <summary>Details</summary>
Motivation: 为了吸引用户并增加他们在平台上的时间，电子商务等其他领域的成熟平台已经开始引入短视频内容。这种体验的成功不仅归功于内容本身，还得益于独特的UI创新：平台主动推荐内容供用户一次观看一个，而不是向用户提供点击选项列表。这给推荐系统带来了新的挑战，尤其是在启动新的视频体验时。除了有限的互动数据外，沉浸式Feed体验由于UI引入了更强的位置偏差，并在优化观看时间时引入了持续时间偏差，因为模型倾向于选择较短的视频。这些问题与推荐系统中固有的反馈循环一起，使得构建有效的解决方案变得困难。

Method: 使用微调的多模态视觉语言模型的视频检索系统

Result: 在我们的电子商务平台上进行的在线实验表明，与传统的监督学习方法相比，该方法更有效。

Conclusion: 该论文表明，即使拥有足够的视频互动数据，利用微调的多模态视觉语言模型的视频检索系统比传统的监督学习方法更有效。

Abstract: In recent years, social media users have spent significant amounts of time on
short-form video platforms. As a result, established platforms in other
domains, such as e-commerce, have begun introducing short-form video content to
engage users and increase their time spent on the platform. The success of
these experiences is due not only to the content itself but also to a unique UI
innovation: instead of offering users a list of choices to click, platforms
actively recommend content for users to watch one at a time. This creates new
challenges for recommender systems, especially when launching a new video
experience. Beyond the limited interaction data, immersive feed experiences
introduce stronger position bias due to the UI and duration bias when
optimizing for watch-time, as models tend to favor shorter videos. These
issues, together with the feedback loop inherent in recommender systems, make
it difficult to build effective solutions. In this paper, we highlight the
challenges faced when introducing a new short-form video experience and present
our experience showing that, even with sufficient video interaction data, it
can be more beneficial to leverage a video retrieval system using a fine-tuned
multimodal vision-language model to overcome these challenges. This approach
demonstrated greater effectiveness compared to conventional supervised learning
methods in online experiments conducted on our e-commerce platform.

</details>


### [175] [Reconstruction of Sparse Urban Wireless Signals via Group Equivariant Non-Expansive Operators](https://arxiv.org/abs/2507.19349)
*Lorenzo Mario Amorosa,Francesco Conti,Nicola Quercioli,Flavio Zabini,Tayebeh Lotfi Mahyari,Yiqun Ge,Patrizio Frosini*

Main category: cs.LG

TL;DR: GENEO offers a low-complexity alternative to traditional neural networks for reconstructing spatial signals from sparse measurements, leveraging invariances to reduce parameters and mitigate data scarcity.


<details>
  <summary>Details</summary>
Motivation: Efficient resource management in 6G networks relies on accurate knowledge of spatially-varying quantities like SINR maps, which are costly to acquire at high resolution.

Method: Introducing a novel GENEO-based approach for SINR map reconstruction.

Result: Demonstrates competitive performance compared to established methods and highlights advantages in accurately reconstructing spatial signals under severe data limitations.

Conclusion: GENEO-based approach achieves competitive performance in SINR map reconstruction with sparse sampling.

Abstract: In emerging communication systems such as sixth generation (6G) wireless
networks, efficient resource management and service delivery rely on accurate
knowledge of spatially-varying quantities like signal-to-interference-noise
ratio (SINR) maps, which are costly to acquire at high resolution. This work
explores the reconstruction of such spatial signals from sparse measurements
using Group Equivariant Non-Expansive Operators (GENEOs), offering a
low-complexity alternative to traditional neural networks. The concept of
GENEO, which originated in topological data analysis (TDA), is a mathematical
tool used in machine learning to represent agents modelled as functional
operators acting on data while incorporating application-specific invariances.
Leveraging these invariances reduces the number of parameters with respect to
traditional neural networks and mitigates data scarcity by enforcing known
algebraic and geometric constraints that reflect symmetries in the agents'
actions. In this paper, we introduce a novel GENEO-based approach for SINR map
reconstruction in urban wireless communication networks using extremely sparse
sampling. We demonstrate that this mathematical framework achieves competitive
performance compared to established methods. Our evaluation, conducted using
both statistical and TDA metrics, highlights the advantages of our approach in
accurately reconstructing spatial signals under severe data limitations on the
number of samples.

</details>


### [176] [A Data-Driven Approach to Estimate LEO Orbit Capacity Models](https://arxiv.org/abs/2507.19365)
*Braden Stock,Maddox McVarthy,Simone Servadio*

Main category: cs.LG

TL;DR: 本研究提出了一种基于SINDy和LSTM的轻量级模型，用于预测LEO中卫星和碎片的传播。


<details>
  <summary>Details</summary>
Motivation: 使用计算成本高昂的高保真模型MOCAT-MC的数据集，以提供一个轻量级、低保真度的模型，从而在更短的时间内提供准确的预测。

Method: 利用稀疏识别非线性动力学算法（SINDy）和长短期记忆循环神经网络（LSTM）。

Result: 对LEO中的卫星和碎片进行建模，预测其未来的传播轨迹

Conclusion: 利用SINDy和LSTM，可以准确地对LEO中的空间物体进行建模，预测卫星和碎片未来的传播。

Abstract: Utilizing the Sparse Identification of Nonlinear Dynamics algorithm (SINDy)
and Long Short-Term Memory Recurrent Neural Networks (LSTM), the population of
resident space objects, divided into Active, Derelict, and Debris, in LEO can
be accurately modeled to predict future satellite and debris propagation. This
proposed approach makes use of a data set coming from a computational expensive
high-fidelity model, the MOCAT-MC, to provide a light, low-fidelity counterpart
that provides accurate forecasting in a shorter time frame.

</details>


### [177] [Counterfactual Explanations in Medical Imaging: Exploring SPN-Guided Latent Space Manipulation](https://arxiv.org/abs/2507.19368)
*Julia Siekiera,Stefan Kramer*

Main category: cs.LG

TL;DR: 利用人工智能在医学图像分析中进行决策，但由于黑盒系统的性质，模型复杂性引发担忧。通过 SPN 建模 VAE 的潜在空间，实现了潜在空间反事实的优化，使其更接近原始数据分布并与目标类分布对齐。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医学图像分析中表现出色，但其复杂性使其成为黑盒系统，引发了对可靠性和可解释性的担忧。反事实解释通过提供假设的“假设”情景来改变模型分类，从而提供对决策过程的可理解的见解。然而，生成符合相似性约束的合理反事实仍然是一个挑战。

Method: 通过模型特定的优化方法，用 SPN 建模半监督 VAE 的潜在空间。

Result: 在 cheXpert 数据集上进行了实验评估。将 SPN 引导的潜在空间操作与神经网络基线进行比较，分析了潜在变量正则化和反事实质量之间的权衡。

Conclusion: 通过用 SPN 建模半监督 VAE 的潜在空间的可能性，能够优化潜在空间反事实，使其既接近原始数据分布，又与目标类分布对齐。

Abstract: Artificial intelligence is increasingly leveraged across various domains to
automate decision-making processes that significantly impact human lives. In
medical image analysis, deep learning models have demonstrated remarkable
performance. However, their inherent complexity makes them black box systems,
raising concerns about reliability and interpretability. Counterfactual
explanations provide comprehensible insights into decision processes by
presenting hypothetical "what-if" scenarios that alter model classifications.
By examining input alterations, counterfactual explanations provide patterns
that influence the decision-making process. Despite their potential, generating
plausible counterfactuals that adhere to similarity constraints providing
human-interpretable explanations remains a challenge. In this paper, we
investigate this challenge by a model-specific optimization approach. While
deep generative models such as variational autoencoders (VAEs) exhibit
significant generative power, probabilistic models like sum-product networks
(SPNs) efficiently represent complex joint probability distributions. By
modeling the likelihood of a semi-supervised VAE's latent space with an SPN, we
leverage its dual role as both a latent space descriptor and a classifier for a
given discrimination task. This formulation enables the optimization of latent
space counterfactuals that are both close to the original data distribution and
aligned with the target class distribution. We conduct experimental evaluation
on the cheXpert dataset. To evaluate the effectiveness of the integration of
SPNs, our SPN-guided latent space manipulation is compared against a neural
network baseline. Additionally, the trade-off between latent variable
regularization and counterfactual quality is analyzed.

</details>


### [178] [FD4QC: Application of Classical and Quantum-Hybrid Machine Learning for Financial Fraud Detection A Technical Report](https://arxiv.org/abs/2507.19402)
*Matteo Cardaioli,Luca Marangoni,Giada Martini,Francesco Mazzolin,Luca Pajola,Andrea Ferretto Parodi,Alessandra Saitta,Maria Chiara Vernillo*

Main category: cs.LG

TL;DR: Classical Random Forest is better for fraud detection than quantum models, but QSVM has potential.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity and volume of financial transactions pose significant challenges to traditional fraud detection systems.

Method: Develop a behavioural feature engineering framework and evaluate classical and hybrid quantum models on the IBM AML dataset.

Result: Classical Random Forest achieves high accuracy (97.34%) and F-measure (86.95%), while QSVM delivers high precision (77.15%) and a low false-positive rate (1.36%) but with lower recall and significant computational overhead.

Conclusion: Classical tree-based models outperform quantum models, with QSVM showing promise but with limitations.

Abstract: The increasing complexity and volume of financial transactions pose
significant challenges to traditional fraud detection systems. This technical
report investigates and compares the efficacy of classical, quantum, and
quantum-hybrid machine learning models for the binary classification of
fraudulent financial activities.
  As of our methodology, first, we develop a comprehensive behavioural feature
engineering framework to transform raw transactional data into a rich,
descriptive feature set. Second, we implement and evaluate a range of models on
the IBM Anti-Money Laundering (AML) dataset. The classical baseline models
include Logistic Regression, Decision Tree, Random Forest, and XGBoost. These
are compared against three hybrid classic quantum algorithms architectures: a
Quantum Support Vector Machine (QSVM), a Variational Quantum Classifier (VQC),
and a Hybrid Quantum Neural Network (HQNN).
  Furthermore, we propose Fraud Detection for Quantum Computing (FD4QC), a
practical, API-driven system architecture designed for real-world deployment,
featuring a classical-first, quantum-enhanced philosophy with robust fallback
mechanisms.
  Our results demonstrate that classical tree-based models, particularly
\textit{Random Forest}, significantly outperform the quantum counterparts in
the current setup, achieving high accuracy (\(97.34\%\)) and F-measure
(\(86.95\%\)). Among the quantum models, \textbf{QSVM} shows the most promise,
delivering high precision (\(77.15\%\)) and a low false-positive rate
(\(1.36\%\)), albeit with lower recall and significant computational overhead.
  This report provides a benchmark for a real-world financial application,
highlights the current limitations of quantum machine learning in this domain,
and outlines promising directions for future research.

</details>


### [179] [On Arbitrary Predictions from Equally Valid Models](https://arxiv.org/abs/2507.19408)
*Sarah Lockfisch,Kristian Schwethelm,Martin Menten,Rickmer Braren,Daniel Rueckert,Alexander Ziller,Georgios Kaissis*

Main category: cs.LG

TL;DR: 医学模型的预测多重性可能导致冲突诊断。小型集成模型可以缓解这一问题，但在模型未能达成共识时，应交由专家审查。


<details>
  <summary>Details</summary>
Motivation: 在医学领域，模型的多重性可能导致对同一患者的冲突预测，这是一个未被充分理解和解决的风险。

Method: 通过实证分析，研究了不同医学任务和模型架构中预测多重性的程度、驱动因素和影响。

Result: (1) 标准验证指标未能识别出唯一的最优模型；(2) 大量预测取决于模型开发过程中的任意选择；(3) 小型集成模型与弃权策略相结合，可以有效缓解预测多重性；(4) 通过增加模型容量来实现更高的准确性可以减少预测多重性。

Conclusion: 模型的多重性在医学领域可能导致对同一患者的冲突预测。通过采用集成策略和专家审查，可以提高诊断的可靠性。

Abstract: Model multiplicity refers to the existence of multiple machine learning
models that describe the data equally well but may produce different
predictions on individual samples. In medicine, these models can admit
conflicting predictions for the same patient -- a risk that is poorly
understood and insufficiently addressed.
  In this study, we empirically analyze the extent, drivers, and ramifications
of predictive multiplicity across diverse medical tasks and model
architectures, and show that even small ensembles can mitigate/eliminate
predictive multiplicity in practice. Our analysis reveals that (1) standard
validation metrics fail to identify a uniquely optimal model and (2) a
substantial amount of predictions hinges on arbitrary choices made during model
development. Using multiple models instead of a single model reveals instances
where predictions differ across equally plausible models -- highlighting
patients that would receive arbitrary diagnoses if any single model were used.
In contrast, (3) a small ensemble paired with an abstention strategy can
effectively mitigate measurable predictive multiplicity in practice;
predictions with high inter-model consensus may thus be amenable to automated
classification. While accuracy is not a principled antidote to predictive
multiplicity, we find that (4) higher accuracy achieved through increased model
capacity reduces predictive multiplicity.
  Our findings underscore the clinical importance of accounting for model
multiplicity and advocate for ensemble-based strategies to improve diagnostic
reliability. In cases where models fail to reach sufficient consensus, we
recommend deferring decisions to expert review.

</details>


### [180] [SILS: Strategic Influence on Liquidity Stability and Whale Detection in Concentrated-Liquidity DEXs](https://arxiv.org/abs/2507.19411)
*Ali RajabiNekoo,Laleh Rasoul,Amirfarhad Farhadi,Azadeh Zamanifar*

Main category: cs.LG

TL;DR: SILS 框架通过使用流动性稳定性影响评分 (LSIS)（一种反事实指标，用于衡量 LP 撤回时市场可能发生的恶化）来定义 LP 的功能重要性，从而准确识别高影响力 LP。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖于名义资本规模或表面活动等广泛措施，通常会导致不准确的风险分析。

Method: 该方法使用链上事件日志和智能合约执行跟踪来计算指数时间加权流动性 (ETWL) 配置文件并应用无监督异常检测。

Result: 这种以影响为中心且全面的方法使 SILS 能够准确地识别高影响力 LP，包括传统方法遗漏的那些，并支持诸如保护性预言机层和可操作的交易者信号等基本应用，从而显着增强 DeFi 生态系统。

Conclusion: SILS 提供了一种有效的机制来进行主动风险管理，改变了 DeFi 协议保护其生态系统免受不对称流动性行为影响的方式。

Abstract: Traditional methods for identifying impactful liquidity providers (LPs) in
Concentrated Liquidity Market Makers (CLMMs) rely on broad measures, such as
nominal capital size or surface-level activity, which often lead to inaccurate
risk analysis. The SILS framework offers a significantly more detailed
approach, characterizing LPs not just as capital holders but as dynamic
systemic agents whose actions directly impact market stability. This represents
a fundamental paradigm shift from the static, volume-based analysis to a
dynamic, impact-focused understanding. This advanced approach uses on-chain
event logs and smart contract execution traces to compute Exponential
Time-Weighted Liquidity (ETWL) profiles and apply unsupervised anomaly
detection. Most importantly, it defines an LP's functional importance through
the Liquidity Stability Impact Score (LSIS), a counterfactual metric that
measures the potential degradation of the market if the LP withdraws. This
combined approach provides a more detailed and realistic characterization of an
LP's impact, moving beyond the binary and often misleading classifications used
by existing methods. This impact-focused and comprehensive approach enables
SILS to accurately identify high-impact LPs-including those missed by
traditional methods and supports essential applications like a protective
oracle layer and actionable trader signals, thereby significantly enhancing
DeFi ecosystem. The framework provides unprecedented transparency into the
underlying liquidity structure and associated risks, effectively reducing the
common false positives and uncovering critical false negatives found in
traditional models. Therefore, SILS provides an effective mechanism for
proactive risk management, transforming how DeFi protocols safeguard their
ecosystems against asymmetric liquidity behavior.

</details>


### [181] [Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding](https://arxiv.org/abs/2507.19427)
*StepFun,:,Bin Wang,Bojun Wang,Changyi Wan,Guanzhe Huang,Hanpeng Hu,Haonan Jia,Hao Nie,Mingliang Li,Nuo Chen,Siyu Chen,Song Yuan,Wuxun Xie,Xiaoniu Song,Xing Chen,Xingping Yang,Xuelin Zhang,Yanbo Yu,Yaoyu Wang,Yibo Zhu,Yimin Jiang,Yu Zhou,Yuanwei Lu,Houyi Li,Jingcheng Hu,Ka Man Lo,Ailin Huang,Binxing Jiao,Bo Li,Boyu Chen,Changxin Miao,Chang Lou,Chen Hu,Chen Xu,Chenfeng Yu,Chengyuan Yao,Daokuan Lv,Dapeng Shi,Deshan Sun,Ding Huang,Dingyuan Hu,Dongqing Pang,Enle Liu,Fajie Zhang,Fanqi Wan,Gulin Yan,Han Zhang,Han Zhou,Hanghao Wu,Hangyu Guo,Hanqi Chen,Hanshan Zhang,Hao Wu,Haocheng Zhang,Haolong Yan,Haoran Lv,Haoran Wei,Hebin Zhou,Heng Wang,Heng Wang,Hongxin Li,Hongyu Zhou,Hongyuan Wang,Huiyong Guo,Jia Wang,Jiahao Gong,Jialing Xie,Jian Zhou,Jianjian Sun,Jiaoren Wu,Jiaran Zhang,Jiayu Liu,Jie Cheng,Jie Luo,Jie Yan,Jie Yang,Jieyi Hou,Jinguang Zhang,Jinlan Cao,Jisheng Yin,Junfeng Liu,Junhao Huang,Junzhe Lin,Kaijun Tan,Kaixiang Li,Kang An,Kangheng Lin,Kenkun Liu,Lei Yang,Liang Zhao,Liangyu Chen,Lieyu Shi,Liguo Tan,Lin Lin,Lin Zhang,Lina Chen,Liwen Huang,Liying Shi,Longlong Gu,Mei Chen,Mengqiang Ren,Ming Li,Mingzhe Chen,Na Wang,Nan Wu,Qi Han,Qian Zhao,Qiang Zhang,Qianni Liu,Qiaohui Chen,Qiling Wu,Qinglin He,Qinyuan Tan,Qiufeng Wang,Qiuping Wu,Qiuyan Liang,Quan Sun,Rui Li,Ruihang Miao,Ruosi Wan,Ruyan Guo,Shangwu Zhong,Shaoliang Pang,Shengjie Fan,Shijie Shang,Shilei Jiang,Shiliang Yang,Shiming Hao,Shuli Gao,Siming Huang,Siqi Liu,Tiancheng Cao,Tianhao Cheng,Tianhao Peng,Wang You,Wei Ji,Wen Sun,Wenjin Deng,Wenqing He,Wenzhen Zheng,Xi Chen,Xiangwen Kong,Xianzhen Luo,Xiaobo Yang,Xiaojia Liu,Xiaoxiao Ren,Xin Han,Xin Li,Xin Wu,Xu Zhao,Yanan Wei,Yang Li,Yangguang Li,Yangshijie Xu,Yanming Xu,Yaqiang Shi,Yeqing Shen,Yi Yang,Yifei Yang,Yifeng Gong,Yihan Chen,Yijing Yang,Yinmin Zhang,Yizhuang Zhou,Yuanhao Ding,Yuantao Fan,Yuanzhen Yang,Yuchu Luo,Yue Peng,Yufan Lu,Yuhang Deng,Yuhe Yin,Yujie Liu,Yukun Chen,Yuling Zhao,Yun Mou,Yunlong Li,Yunzhou Ju,Yusheng Li,Yuxiang Yang,Yuxiang Zhang,Yuyang Chen,Zejia Weng,Zhe Xie,Zheng Ge,Zheng Gong,Zhenyi Lu,Zhewei Huang,Zhichao Chang,Zhiguo Huang,Zhirui Wang,Zidong Yang,Zili Wang,Ziqi Wang,Zixin Zhang,Binxing Jiao,Daxin Jiang,Heung-Yeung Shum,Xiangyu Zhang*

Main category: cs.LG

TL;DR: Step-3是一种硬件感知的模型系统协同设计，旨在最大限度地降低解码成本，通过MFA和AFD实现了前所未有的成本效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在解码过程中面临较低的硬件效率，尤其是在长上下文推理任务中。

Method: 引入了一种新颖的多矩阵分解注意力（MFA）机制和注意力-FFN分解（AFD）

Result: Step-3显著降低了解码成本，同时每个token激活38B参数，并在Hopper GPU上实现了高达每秒每GPU 4,039个token的解码吞吐量。

Conclusion: Step-3在LLM解码方面设立了新的Pareto前沿，在Hopper GPU上实现了高达每秒每GPU 4,039个token的解码吞吐量。

Abstract: Large language models (LLMs) face low hardware efficiency during decoding,
especially for long-context reasoning tasks. This paper introduces Step-3, a
321B-parameter VLM with hardware-aware model-system co-design optimized for
minimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel
Multi-Matrix Factorization Attention (MFA) mechanism that significantly reduces
both KV cache size and computation while maintaining high attention
expressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed
inference system that decouples attention and Feed-Forward Network (FFN) layers
into specialized subsystems. This co-design achieves unprecedented cost
efficiency: Step-3 significantly reduces theoretical decoding costs compared
with models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at
longer context. Step-3 achieves low cost while activating 38B parameters per
token (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that
hardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are
critical to cost-effectiveness. We perform a head-to-head comparison with
DeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs
achieves a decoding throughput of up to 4,039 tokens per second per GPU under
50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324
in the same setup and sets a new Pareto frontier for LLM decoding.

</details>


### [182] [Observations Meet Actions: Learning Control-Sufficient Representations for Robust Policy Generalization](https://arxiv.org/abs/2507.19437)
*Yuliang Gu,Hongpeng Cao,Marco Caccamo,Naira Hovakimyan*

Main category: cs.LG

TL;DR: We recast context-based RL as a dual inference-control problem and derive a contextual evidence lower bound objective that cleanly separates representation learning from policy learning and optimizes it with Bottlenecked Contextual Policy Optimization (BCPO).


<details>
  <summary>Details</summary>
Motivation: Capturing latent variations is key to deploying reinforcement-learning (RL) agents beyond their training regime.

Method: an algorithm that places a variational information-bottleneck encoder in front of any off-policy policy learner

Result: BCPO matches or surpasses other baselines while using fewer samples and retaining performance far outside the training regime.

Conclusion: The framework unifies theory, diagnostics, and practice for context-based RL.

Abstract: Capturing latent variations ("contexts") is key to deploying
reinforcement-learning (RL) agents beyond their training regime. We recast
context-based RL as a dual inference-control problem and formally characterize
two properties and their hierarchy: observation sufficiency (preserving all
predictive information) and control sufficiency (retaining decision-making
relevant information). Exploiting this dichotomy, we derive a contextual
evidence lower bound(ELBO)-style objective that cleanly separates
representation learning from policy learning and optimizes it with Bottlenecked
Contextual Policy Optimization (BCPO), an algorithm that places a variational
information-bottleneck encoder in front of any off-policy policy learner. On
standard continuous-control benchmarks with shifting physical parameters, BCPO
matches or surpasses other baselines while using fewer samples and retaining
performance far outside the training regime. The framework unifies theory,
diagnostics, and practice for context-based RL.

</details>


### [183] [Forest-Guided Clustering -- Shedding Light into the Random Forest Black Box](https://arxiv.org/abs/2507.19455)
*Lisa Barros de Andrade e Sousa,Gregor Miller,Ronan Le Gleut,Dominik Thalmeier,Helena Pelin,Marie Piraud*

Main category: cs.LG

TL;DR: FGC是一种新的模型特定的可解释性方法，它通过对实例进行聚类来揭示随机森林中的局部和全局结构，从而提高可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型越来越多地部署在敏感应用领域，对可解释和可信的决策的需求也随之增加。随机森林（RF）尽管被广泛使用，并且在表格数据上表现出色，但由于其集成性质，仍然难以解释。

Method: 我们提出了森林引导聚类（FGC），这是一种特定于模型的解释方法，它通过根据共享决策路径对实例进行分组，揭示了RF中的局部和全局结构。

Result: FGC在基准数据集上准确地恢复了潜在的子类结构，并且优于经典的聚类和事后解释方法。应用于AML转录组数据集，FGC发现了生物学上一致的亚群，从混杂因素中解开了与疾病相关的信号，并恢复了已知和新颖的基因表达模式。

Conclusion: FGC通过提供超越特征层面归因的结构感知见解，弥合了性能和可解释性之间的差距。

Abstract: As machine learning models are increasingly deployed in sensitive application
areas, the demand for interpretable and trustworthy decision-making has
increased. Random Forests (RF), despite their widespread use and strong
performance on tabular data, remain difficult to interpret due to their
ensemble nature. We present Forest-Guided Clustering (FGC), a model-specific
explainability method that reveals both local and global structure in RFs by
grouping instances according to shared decision paths. FGC produces
human-interpretable clusters aligned with the model's internal logic and
computes cluster-specific and global feature importance scores to derive
decision rules underlying RF predictions. FGC accurately recovered latent
subclass structure on a benchmark dataset and outperformed classical clustering
and post-hoc explanation methods. Applied to an AML transcriptomic dataset, FGC
uncovered biologically coherent subpopulations, disentangled disease-relevant
signals from confounders, and recovered known and novel gene expression
patterns. FGC bridges the gap between performance and interpretability by
providing structure-aware insights that go beyond feature-level attribution.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [184] [XAI-Guided Analysis of Residual Networks for Interpretable Pneumonia Detection in Paediatric Chest X-rays](https://arxiv.org/abs/2507.18647)
*Rayyan Ridwan*

Main category: eess.IV

TL;DR: 开发了一种可解释的深度学习模型，用于通过胸部X光片自动诊断儿童肺炎，该模型具有高精度和临床意义的视觉解释。


<details>
  <summary>Details</summary>
Motivation: 肺炎仍然是全球儿童死亡的主要原因之一，因此迫切需要快速、准确的诊断工具。

Method: 在残差网络（ResNets）上提出了一个可解释的深度学习模型，用于自动诊断儿童胸部X光片上的肺炎。通过贝叶斯梯度加权类激活映射（BayesGrad-CAM）来增强可解释性。

Result: ResNet-50模型在大型儿科胸部X光片数据集上训练，实现了高分类精度（95.94%）、AUC-ROC（98.91%）和Cohen's Kappa（0.913），并附有临床意义的视觉解释。

Conclusion: 这项研究表明，高性能和可解释性不仅可以实现，而且对于临床人工智能部署至关重要。

Abstract: Pneumonia remains one of the leading causes of death among children
worldwide, underscoring a critical need for fast and accurate diagnostic tools.
In this paper, we propose an interpretable deep learning model on Residual
Networks (ResNets) for automatically diagnosing paediatric pneumonia on chest
X-rays. We enhance interpretability through Bayesian Gradient-weighted Class
Activation Mapping (BayesGrad-CAM), which quantifies uncertainty in visual
explanations, and which offers spatial locations accountable for the
decision-making process of the model. Our ResNet-50 model, trained on a large
paediatric chest X-rays dataset, achieves high classification accuracy
(95.94%), AUC-ROC (98.91%), and Cohen's Kappa (0.913), accompanied by
clinically meaningful visual explanations. Our findings demonstrate that high
performance and interpretability are not only achievable but critical for
clinical AI deployment.

</details>


### [185] [Learned Single-Pixel Fluorescence Microscopy](https://arxiv.org/abs/2507.18740)
*Serban C. Tudosie,Valerio Gandolfi,Shivaprasad Varakkoth,Andrea Farina,Cosimo D'Andrea,Simon Arridge*

Main category: eess.IV

TL;DR: 我们使用自监督自编码器来改进单像素荧光显微镜，从而实现更快的重建、更高的图像质量和多光谱能力。


<details>
  <summary>Details</summary>
Motivation: 单像素成像已成为荧光显微镜中的一项关键技术，其中快速采集和重建至关重要。在这种情况下，图像是从线性压缩测量中重建的。实际上，总变差最小化仍然用于从正交采样模式向量与原始图像数据之间的内积的噪声测量中重建图像。但是，可以利用数据来学习测量向量和重建过程，从而提高压缩率、重建质量和速度。

Method: 我们通过自我监督训练一个自编码器来学习编码器（或测量矩阵）和解码器。然后在物理采集的多光谱和强度数据上对其进行测试。在采集过程中，学习到的编码器成为物理设备的一部分。

Result: 我们的方法可以减少两个数量级的重建时间，实现卓越的图像质量，并实现多光谱重建。

Conclusion: 通过减少两个数量级的重建时间、实现卓越的图像质量并实现多光谱重建，我们的方法可以增强荧光显微镜中的单像素成像。最终，学习到的单像素荧光显微镜可以促进诊断和生物学研究，以一小部分成本提供多光谱成像。

Abstract: Single-pixel imaging has emerged as a key technique in fluorescence
microscopy, where fast acquisition and reconstruction are crucial. In this
context, images are reconstructed from linearly compressed measurements. In
practice, total variation minimisation is still used to reconstruct the image
from noisy measurements of the inner product between orthogonal sampling
pattern vectors and the original image data. However, data can be leveraged to
learn the measurement vectors and the reconstruction process, thereby enhancing
compression, reconstruction quality, and speed. We train an autoencoder through
self-supervision to learn an encoder (or measurement matrix) and a decoder. We
then test it on physically acquired multispectral and intensity data. During
acquisition, the learned encoder becomes part of the physical device. Our
approach can enhance single-pixel imaging in fluorescence microscopy by
reducing reconstruction time by two orders of magnitude, achieving superior
image quality, and enabling multispectral reconstructions. Ultimately, learned
single-pixel fluorescence microscopy could advance diagnosis and biological
research, providing multispectral imaging at a fraction of the cost.

</details>


### [186] [RealDeal: Enhancing Realism and Details in Brain Image Generation via Image-to-Image Diffusion Models](https://arxiv.org/abs/2507.18830)
*Shen Zhu,Yinzhu Jin,Tyler Spears,Ifrah Zawar,P. Thomas Fletcher*

Main category: eess.IV

TL;DR: image-to-image diffusion models are designed to enhance the realism and details of generated brain images


<details>
  <summary>Details</summary>
Motivation: enhance the realism and details of generated brain images by introducing sharp edges, fine textures, subtle anatomical features, and imaging noise. Generative models have been widely adopted in the biomedical domain, especially in image generation applications. Latent diffusion models achieve state-of-the-art results in generating brain MRIs. However, due to latent compression, generated images from these models are overly smooth, lacking fine anatomical structures and scan acquisition noise that are typically seen in real images.

Method: image-to-image diffusion models

Result: We employ commonly used metrics like FID and LPIPS for image realism assessment. Furthermore, we introduce new metrics to demonstrate the realism of images generated by RealDeal in terms of image noise distribution, sharpness, and texture.

Conclusion: This work formulates the realism enhancing and detail adding process as image-to-image diffusion models, which refines the quality of LDM-generated images.

Abstract: We propose image-to-image diffusion models that are designed to enhance the
realism and details of generated brain images by introducing sharp edges, fine
textures, subtle anatomical features, and imaging noise. Generative models have
been widely adopted in the biomedical domain, especially in image generation
applications. Latent diffusion models achieve state-of-the-art results in
generating brain MRIs. However, due to latent compression, generated images
from these models are overly smooth, lacking fine anatomical structures and
scan acquisition noise that are typically seen in real images. This work
formulates the realism enhancing and detail adding process as image-to-image
diffusion models, which refines the quality of LDM-generated images. We employ
commonly used metrics like FID and LPIPS for image realism assessment.
Furthermore, we introduce new metrics to demonstrate the realism of images
generated by RealDeal in terms of image noise distribution, sharpness, and
texture.

</details>


### [187] [Dealing with Segmentation Errors in Needle Reconstruction for MRI-Guided Brachytherapy](https://arxiv.org/abs/2507.18895)
*Vangelis Kostoulas,Arthur Guijt,Ellen M. Kerkhof,Bradley R. Pieters,Peter A. N. Bosman,Tanja Alderliesten*

Main category: eess.IV

TL;DR: 提出了一种改进的针头重建后处理技术，以处理分割错误，并在前列腺癌数据集上实现了良好的重建精度。


<details>
  <summary>Details</summary>
Motivation: 在图像引导的近距离放射治疗计划中，重建针头是必不可少的。手动注释患者图像上的这些针头对于医疗专业人员来说可能是一项具有挑战性且耗时的任务。虽然深度学习模型在分割方面有效，但其结果通常包含错误。目前没有现有的后处理技术能够应对所有可能的分割错误。

Method: 对现有的后处理技术进行了改进，主要目的是处理分割错误，从而提高重建精度。

Result: 在基于医疗专业人员注释的MRI扫描的前列腺癌数据集上的实验表明，我们提出的改进方法可以帮助有效管理分割错误。

Conclusion: 提出的改进方法能有效管理分割错误，最佳改进后处理技术在包含261个针头的测试集上，针尖和针底点定位误差中值分别为$1.07$ mm和$0.43$ mm，轴误差中值为$0.75$ mm，且没有假阳性和假阴性针头。

Abstract: Brachytherapy involves bringing a radioactive source near tumor tissue using
implanted needles. Image-guided brachytherapy planning requires amongst others,
the reconstruction of the needles. Manually annotating these needles on patient
images can be a challenging and time-consuming task for medical professionals.
For automatic needle reconstruction, a two-stage pipeline is commonly adopted,
comprising a segmentation stage followed by a post-processing stage. While deep
learning models are effective for segmentation, their results often contain
errors. No currently existing post-processing technique is robust to all
possible segmentation errors. We therefore propose adaptations to existing
post-processing techniques mainly aimed at dealing with segmentation errors and
thereby improving the reconstruction accuracy. Experiments on a prostate cancer
dataset, based on MRI scans annotated by medical professionals, demonstrate
that our proposed adaptations can help to effectively manage segmentation
errors, with the best adapted post-processing technique achieving median
needle-tip and needle-bottom point localization errors of $1.07$ (IQR $\pm
1.04$) mm and $0.43$ (IQR $\pm 0.46$) mm, respectively, and median shaft error
of $0.75$ (IQR $\pm 0.69$) mm with 0 false positive and 0 false negative
needles on a test set of 261 needles.

</details>


### [188] [Dual Path Learning -- learning from noise and context for medical image denoising](https://arxiv.org/abs/2507.19035)
*Jitindra Fartiyal,Pedro Freire,Yasmeen Whayeb,James S. Wolffsohn,Sergei K. Turitsyn,Sergei G. Sokolov*

Main category: eess.IV

TL;DR: DPL is a robust and generalizable denoising model for medical images that uses both noise and contextual information, outperforming UNet on Gaussian noise.


<details>
  <summary>Details</summary>
Motivation: Noise in medical images degrades quality, leading to misinterpretations. Existing denoising approaches rely on either noise characteristics or image context, often for a single modality and noise type.

Method: Introduces a Dual-Pathway Learning (DPL) model architecture that denoises medical images by leveraging and fusing both noise and contextual information.

Result: DPL improves PSNR by 3.35% compared to the baseline UNet when evaluated on Gaussian noise and trained across all modalities.

Conclusion: DPL effectively denoises medical images, demonstrating robustness and generalizability across multiple modalities and noise types, improving PSNR by 3.35% compared to UNet on Gaussian noise.

Abstract: Medical imaging plays a critical role in modern healthcare, enabling
clinicians to accurately diagnose diseases and develop effective treatment
plans. However, noise, often introduced by imaging devices, can degrade image
quality, leading to misinterpretation and compromised clinical outcomes.
Existing denoising approaches typically rely either on noise characteristics or
on contextual information from the image. Moreover, they are commonly developed
and evaluated for a single imaging modality and noise type. Motivated by Geng
et.al CNCL, which integrates both noise and context, this study introduces a
Dual-Pathway Learning (DPL) model architecture that effectively denoises
medical images by leveraging both sources of information and fusing them to
generate the final output. DPL is evaluated across multiple imaging modalities
and various types of noise, demonstrating its robustness and generalizability.
DPL improves PSNR by 3.35% compared to the baseline UNet when evaluated on
Gaussian noise and trained across all modalities. The code is available at
10.5281/zenodo.15836053.

</details>


### [189] [Joint Holistic and Lesion Controllable Mammogram Synthesis via Gated Conditional Diffusion Model](https://arxiv.org/abs/2507.19201)
*Xin Li,Kaixiang Yang,Qiang Li,Zhiwei Wang*

Main category: eess.IV

TL;DR: 提出了一种新颖的框架Gated Conditional Diffusion Model (GCDM)，用于联合合成完整乳房X线图像和局部病灶。


<details>
  <summary>Details</summary>
Motivation: 现有的生成模型通常不能充分强调病灶特异性特征及其与周围组织的关系，从而限制了精确和稳健的方法的开发。

Method: Gated Conditional Diffusion Model (GCDM)

Result: GCDM实现了对小病灶区域的精确控制，同时增强了合成乳房X线照片的真实性和多样性。

Conclusion: GCDM在合成乳房X线照片方面是一种很有前途的临床应用工具。

Abstract: Mammography is the most commonly used imaging modality for breast cancer
screening, driving an increasing demand for deep-learning techniques to support
large-scale analysis. However, the development of accurate and robust methods
is often limited by insufficient data availability and a lack of diversity in
lesion characteristics. While generative models offer a promising solution for
data synthesis, current approaches often fail to adequately emphasize
lesion-specific features and their relationships with surrounding tissues. In
this paper, we propose Gated Conditional Diffusion Model (GCDM), a novel
framework designed to jointly synthesize holistic mammogram images and
localized lesions. GCDM is built upon a latent denoising diffusion framework,
where the noised latent image is concatenated with a soft mask embedding that
represents breast, lesion, and their transitional regions, ensuring anatomical
coherence between them during the denoising process. To further emphasize
lesion-specific features, GCDM incorporates a gated conditioning branch that
guides the denoising process by dynamically selecting and fusing the most
relevant radiomic and geometric properties of lesions, effectively capturing
their interplay. Experimental results demonstrate that GCDM achieves precise
control over small lesion areas while enhancing the realism and diversity of
synthesized mammograms. These advancements position GCDM as a promising tool
for clinical applications in mammogram synthesis. Our code is available at
https://github.com/lixinHUST/Gated-Conditional-Diffusion-Model/

</details>


### [190] [A Self-training Framework for Semi-supervised Pulmonary Vessel Segmentation and Its Application in COPD](https://arxiv.org/abs/2507.19074)
*Shuiqing Zhao,Meihuan Wang,Jiaxuan Xu,Jie Feng,Wei Qian,Rongchang Chen,Zhenyu Liang,Shouliang Qi,Yanan Wu*

Main category: eess.IV

TL;DR: This paper introduces a semi-supervised method using a teacher-student model for segmenting pulmonary vessels in COPD patients' CT images, achieving improved precision.


<details>
  <summary>Details</summary>
Motivation: It is fundamental for accurate segmentation and quantification of the pulmonary vessel, particularly smaller vessels, from computed tomography (CT) images in chronic obstructive pulmonary disease (COPD) patients.

Method: a self-training framework is proposed by leveraging a teacher-student model for the segmentation of pulmonary vessels. First, the high-quality annotations are acquired in the in-house data by an interactive way. Then, the model is trained in the semi-supervised way. A fully supervised model is trained on a small set of labeled CT images, yielding the teacher model. Following this, the teacher model is used to generate pseudo-labels for the unlabeled CT images, from which reliable ones are selected based on a certain strategy. The training of the student model involves these reliable pseudo-labels. This training process is iteratively repeated until an optimal performance is achieved.

Result: the proposed method, Semi2, significantly improves the precision of vessel segmentation by 2.3%, achieving a precision of 90.3%. Further, quantitative analysis is conducted in the pulmonary vessel of COPD, providing insights into the differences in the pulmonary vessel across different severity of the disease.

Conclusion: The proposed method can not only improve the performance of pulmonary vascular segmentation, but can also be applied in COPD analysis.

Abstract: Background: It is fundamental for accurate segmentation and quantification of
the pulmonary vessel, particularly smaller vessels, from computed tomography
(CT) images in chronic obstructive pulmonary disease (COPD) patients.
Objective: The aim of this study was to segment the pulmonary vasculature using
a semi-supervised method. Methods: In this study, a self-training framework is
proposed by leveraging a teacher-student model for the segmentation of
pulmonary vessels. First, the high-quality annotations are acquired in the
in-house data by an interactive way. Then, the model is trained in the
semi-supervised way. A fully supervised model is trained on a small set of
labeled CT images, yielding the teacher model. Following this, the teacher
model is used to generate pseudo-labels for the unlabeled CT images, from which
reliable ones are selected based on a certain strategy. The training of the
student model involves these reliable pseudo-labels. This training process is
iteratively repeated until an optimal performance is achieved. Results:
Extensive experiments are performed on non-enhanced CT scans of 125 COPD
patients. Quantitative and qualitative analyses demonstrate that the proposed
method, Semi2, significantly improves the precision of vessel segmentation by
2.3%, achieving a precision of 90.3%. Further, quantitative analysis is
conducted in the pulmonary vessel of COPD, providing insights into the
differences in the pulmonary vessel across different severity of the disease.
Conclusion: The proposed method can not only improve the performance of
pulmonary vascular segmentation, but can also be applied in COPD analysis. The
code will be made available at
https://github.com/wuyanan513/semi-supervised-learning-for-vessel-segmentation.

</details>


### [191] [Learned Image Compression with Hierarchical Progressive Context Modeling](https://arxiv.org/abs/2507.19125)
*Yuqi Li,Haotian Zhang,Li Li,Dong Liu*

Main category: eess.IV

TL;DR: This paper introduces a Hierarchical Progressive Context Model (HPCM) for more efficient context information acquisition in learned image compression. It achieves state-of-the-art rate-distortion performance with better computational complexity.


<details>
  <summary>Details</summary>
Motivation: recent advanced methods have expanded context modeling capacity, they still struggle to efficiently exploit long-range dependency and diverse context information across different coding steps

Method: a novel Hierarchical Progressive Context Model (HPCM)

Result: achieves state-of-the-art rate-distortion performance

Conclusion: The method achieves state-of-the-art rate-distortion performance and strikes a better balance between compression performance and computational complexity.

Abstract: Context modeling is essential in learned image compression for accurately
estimating the distribution of latents. While recent advanced methods have
expanded context modeling capacity, they still struggle to efficiently exploit
long-range dependency and diverse context information across different coding
steps. In this paper, we introduce a novel Hierarchical Progressive Context
Model (HPCM) for more efficient context information acquisition. Specifically,
HPCM employs a hierarchical coding schedule to sequentially model the
contextual dependencies among latents at multiple scales, which enables more
efficient long-range context modeling. Furthermore, we propose a progressive
context fusion mechanism that incorporates contextual information from previous
coding steps into the current step, effectively exploiting diverse contextual
information. Experimental results demonstrate that our method achieves
state-of-the-art rate-distortion performance and strikes a better balance
between compression performance and computational complexity. The code is
available at https://github.com/lyq133/LIC-HPCM.

</details>


### [192] [Reconstruct or Generate: Exploring the Spectrum of Generative Modeling for Cardiac MRI](https://arxiv.org/abs/2507.19186)
*Niklas Bubeck,Yundi Zhang,Suprosanna Shit,Daniel Rueckert,Jiazhen Pan*

Main category: eess.IV

TL;DR: 分析了生成模型在医学图像重建和生成任务中的表现，发现扩散模型擅长生成，但重建时容易出现幻觉，而自回归模型在重建方面更稳定。


<details>
  <summary>Details</summary>
Motivation: 在医学成像中，生成模型越来越依赖于两个不同但同样关键的任务：重建（目标是恢复医学成像，通常是逆问题，如图像修复或超分辨率）和生成（创建合成数据以扩充数据集或进行反事实分析）。

Method: 引入了一个“生成模型动物园”，并系统地分析了现代潜在扩散模型和自回归模型如何在重建-生成谱中导航。

Result: 扩散模型在无条件生成方面提供卓越的感知质量，但随着掩蔽率的增加，往往会出现幻觉，而自回归模型在掩蔽水平上保持稳定的感知性能，但通常保真度较低。

Conclusion: 扩散模型在无条件生成方面具有卓越的感知质量，但随着掩蔽率的增加，容易产生幻觉，而自回归模型在各种掩蔽水平下保持稳定的感知性能，但保真度通常较低。

Abstract: In medical imaging, generative models are increasingly relied upon for two
distinct but equally critical tasks: reconstruction, where the goal is to
restore medical imaging (usually inverse problems like inpainting or
superresolution), and generation, where synthetic data is created to augment
datasets or carry out counterfactual analysis. Despite shared architecture and
learning frameworks, they prioritize different goals: generation seeks high
perceptual quality and diversity, while reconstruction focuses on data fidelity
and faithfulness. In this work, we introduce a "generative model zoo" and
systematically analyze how modern latent diffusion models and autoregressive
models navigate the reconstruction-generation spectrum. We benchmark a suite of
generative models across representative cardiac medical imaging tasks, focusing
on image inpainting with varying masking ratios and sampling strategies, as
well as unconditional image generation. Our findings show that diffusion models
offer superior perceptual quality for unconditional generation but tend to
hallucinate as masking ratios increase, whereas autoregressive models maintain
stable perceptual performance across masking levels, albeit with generally
lower fidelity.

</details>
