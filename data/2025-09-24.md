<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 44]
- [cs.CV](#cs.CV) [Total: 46]
- [cs.AI](#cs.AI) [Total: 43]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.IR](#cs.IR) [Total: 7]
- [cs.LG](#cs.LG) [Total: 44]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Dynamic Prompt Fusion for Multi-Task and Cross-Domain Adaptation in LLMs](https://arxiv.org/abs/2509.18113)
*Xin Hu,Yue Kang,Guanzi Yao,Tianze Kang,Mengjie Wang,Heyao Liu*

Main category: cs.CL

TL;DR: 提出了一种具有动态提示调度机制的统一多任务学习框架，以解决大型语言模型在多任务和跨域设置下的泛化局限性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多任务和跨域设置下存在泛化局限性。

Method: 引入提示池和任务感知调度策略，动态组合和对齐不同任务的提示；使用任务嵌入和门控机制来精细控制提示信号；采用联合多任务学习的优化目标，并结合自动学习策略来调度权重。

Result: 实验结果表明，该提示调度方法显著提高了模型在一系列语言理解和知识推理任务上的性能。

Conclusion: 该方法在统一多任务建模和跨域适应方面具有适用性和有效性。

Abstract: This study addresses the generalization limitations commonly observed in
large language models under multi-task and cross-domain settings. Unlike prior
methods such as SPoT, which depends on fixed prompt templates, our study
introduces a unified multi-task learning framework with dynamic prompt
scheduling mechanism. By introducing a prompt pool and a task-aware scheduling
strategy, the method dynamically combines and aligns prompts for different
tasks. This enhances the model's ability to capture semantic differences across
tasks. During prompt fusion, the model uses task embeddings and a gating
mechanism to finely control the prompt signals. This ensures alignment between
prompt content and task-specific demands. At the same time, it builds flexible
sharing pathways across tasks. In addition, the proposed optimization objective
centers on joint multi-task learning. It incorporates an automatic learning
strategy for scheduling weights, which effectively mitigates task interference
and negative transfer. To evaluate the effectiveness of the method, a series of
sensitivity experiments were conducted. These experiments examined the impact
of prompt temperature parameters and task number variation. The results confirm
the advantages of the proposed mechanism in maintaining model stability and
enhancing transferability. Experimental findings show that the prompt
scheduling method significantly improves performance on a range of language
understanding and knowledge reasoning tasks. These results fully demonstrate
its applicability and effectiveness in unified multi-task modeling and
cross-domain adaptation.

</details>


### [2] [GAUSS: Benchmarking Structured Mathematical Skills for Large Language Models](https://arxiv.org/abs/2509.18122)
*Yue Zhang,Jiaxin Zhang,Qiuyu Ren,Tahsin Saffat,Xiaoxuan Liu,Zitong Yang,Banghua Zhu,Yi Ma*

Main category: cs.CL

TL;DR: GAUSS是一个评估LLM数学能力的基准，它将数学能力分为知识理解、问题解决与交流、元技能与创造力三个领域，共十二个核心技能维度。


<details>
  <summary>Details</summary>
Motivation: 为了全面、细致、可解释地分析LLM的数学能力，并忠实地反映其潜在的数学智能。

Method: GAUSS通过对问题进行认知技能分类，并设计能够分离特定能力的任务来实现。

Result: 通过GAUSS基准，我们导出了GPT-5-thinking的技能概况，揭示了其优势和劣势，以及它与o4-mini-high的差异。

Conclusion: GAUSS基准证明了多维度、基于技能的评估的价值。

Abstract: We introduce \textbf{GAUSS} (\textbf{G}eneral \textbf{A}ssessment of
\textbf{U}nderlying \textbf{S}tructured \textbf{S}kills in Mathematics), a
benchmark that evaluates LLMs' mathematical abilities across twelve core skill
dimensions, grouped into three domains: knowledge and understanding, problem
solving and communication, and meta-skills and creativity. By categorizing
problems according to cognitive skills and designing tasks that isolate
specific abilities, GAUSS constructs comprehensive, fine-grained, and
interpretable profiles of models' mathematical abilities. These profiles
faithfully represent their underlying mathematical intelligence. To exemplify
how to use the \textsc{GAUSS} benchmark, we have derived the skill profile of
\textsc{GPT-5-thinking}, revealing its strengths and weaknesses as well as its
differences relative to \textsc{o4-mini-high}, thereby underscoring the value
of multidimensional, skill-based evaluation.

</details>


### [3] [Event Causality Identification with Synthetic Control](https://arxiv.org/abs/2509.18156)
*Haoyu Wang,Fengze Liu,Jiayao Zhang,Dan Roth,Kyle Richardson*

Main category: cs.CL

TL;DR: 本文提出了一种新的事件因果关系识别方法，该方法采用 Rubin 因果模型，通过生成“孪生”主角来模拟干预，从而更可靠地识别因果关系。


<details>
  <summary>Details</summary>
Motivation: 传统 ECI 方法容易受到语言模式和多跳关系推理的误导，导致错误识别因果关系。

Method: 本文采用 Rubin 因果模型，将第一个事件视为处理，第二个事件视为观察到的结果。通过使用合成控制方法生成“孪生”主角来模拟干预，并估计结果概率的变化。

Result: 该方法在因果关系基准 COPES-hard 上表现优于以往的方法，包括 GPT-4。

Conclusion: 本文提出的方法能够比以前的方法更稳健地识别因果关系。

Abstract: Event causality identification (ECI), a process that extracts causal
relations between events from text, is crucial for distinguishing causation
from correlation. Traditional approaches to ECI have primarily utilized
linguistic patterns and multi-hop relational inference, risking false causality
identification due to informal usage of causality and specious graphical
inference. In this paper, we adopt the Rubin Causal Model to identify event
causality: given two temporally ordered events, we see the first event as the
treatment and the second one as the observed outcome. Determining their
causality involves manipulating the treatment and estimating the resultant
change in the likelihood of the outcome. Given that it is only possible to
implement manipulation conceptually in the text domain, as a work-around, we
try to find a twin for the protagonist from existing corpora. This twin should
have identical life experiences with the protagonist before the treatment but
undergoes an intervention of treatment. However, the practical difficulty of
locating such a match limits its feasibility. Addressing this issue, we use the
synthetic control method to generate such a twin' from relevant historical
data, leveraging text embedding synthesis and inversion techniques. This
approach allows us to identify causal relations more robustly than previous
methods, including GPT-4, which is demonstrated on a causality benchmark,
COPES-hard.

</details>


### [4] [ZERA: Zero-init Instruction Evolving Refinement Agent - From Zero Instructions to Structured Prompts via Principle-based Optimization](https://arxiv.org/abs/2509.18158)
*Seungyoun Yi,Minsoo Khang,Sungrae Park*

Main category: cs.CL

TL;DR: ZERA jointly optimizes system and user prompts for LLMs with structured feedback, achieving fast convergence and high performance.


<details>
  <summary>Details</summary>
Motivation: Existing APO methods are costly and brittle due to focusing only on user prompts, using unstructured feedback, and requiring large sample sizes.

Method: ZERA scores prompts using eight generalizable criteria with automatically inferred weights and revises prompts based on these structured critiques.

Result: ZERA consistently outperforms strong baselines across five LLMs and nine diverse datasets.

Conclusion: ZERA enables fast convergence to high-quality prompts using minimal examples and short iteration cycles, demonstrating the contribution of each component to effective prompt construction.

Abstract: Automatic Prompt Optimization (APO) improves large language model (LLM)
performance by refining prompts for specific tasks. However, prior APO methods
typically focus only on user prompts, rely on unstructured feedback, and
require large sample sizes and long iteration cycles-making them costly and
brittle. We propose ZERA (Zero-init Instruction Evolving Refinement Agent), a
novel framework that jointly optimizes both system and user prompts through
principled, low-overhead refinement. ZERA scores prompts using eight
generalizable criteria with automatically inferred weights, and revises prompts
based on these structured critiques. This enables fast convergence to
high-quality prompts using minimal examples and short iteration cycles. We
evaluate ZERA across five LLMs and nine diverse datasets spanning reasoning,
summarization, and code generation tasks. Experimental results demonstrate
consistent improvements over strong baselines. Further ablation studies
highlight the contribution of each component to more effective prompt
construction. Our implementation including all prompts is publicly available at
https://github.com/younatics/zera-agent.

</details>


### [5] [Thinking in a Crowd: How Auxiliary Information Shapes LLM Reasoning](https://arxiv.org/abs/2509.18163)
*Haodong Zhao,Chenyan Zhao,Yansi Li,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.CL

TL;DR: 大型语言模型（LLM）的推理能力对于其在复杂、知识密集型领域的应用至关重要。本文研究了外部辅助信息对具有显式逐步思考能力的LLM推理过程的因果影响，发现LLM在思考模式下，有用的信息会提高准确性，但误导性信息会导致性能急剧下降。挑战在于如何使模型评估其推理所依据的信息。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在实际场景中，外部信息（可能有用、无关或误导）对其推理过程的影响。

Method: 构建了一个名为SciAux的新数据集，该数据集源自ScienceQA，用于系统地测试模型对这些类型信息的鲁棒性。

Result: 发现LLM的“思考模式”是一把双刃剑。有用的上下文可以提高准确性，但误导性信息会导致性能的灾难性下降，并且思考过程会放大这种影响。当提供错误信息时，思考会强化错误的程度。

Conclusion: 挑战不仅仅在于让模型“思考”，更在于赋予它们评估其推理所依据信息的重要能力。

Abstract: The capacity of Large Language Models (LLMs) to reason is fundamental to
their application in complex, knowledge-intensive domains. In real-world
scenarios, LLMs are often augmented with external information that can be
helpful, irrelevant, or even misleading. This paper investigates the causal
impact of such auxiliary information on the reasoning process of LLMs with
explicit step-by-step thinking capabilities. We introduce SciAux, a new dataset
derived from ScienceQA, to systematically test the robustness of the model
against these types of information. Our findings reveal a critical
vulnerability: the model's deliberative "thinking mode" is a double-edged
sword. While helpful context improves accuracy, misleading information causes a
catastrophic drop in performance, which is amplified by the thinking process.
Instead of conferring robustness, thinking reinforces the degree of error when
provided with misinformation. This highlights that the challenge is not merely
to make models "think", but to endow them with the critical faculty to evaluate
the information upon which their reasoning is based. The SciAux dataset is
available at https://huggingface.co/datasets/billhdzhao/SciAux.

</details>


### [6] [SIRAG: Towards Stable and Interpretable RAG with A Process-Supervised Multi-Agent Framework](https://arxiv.org/abs/2509.18167)
*Junlin Wang,Zehao Wu,Shaowei Lu,Yanlan Li,Xinghao Huang*

Main category: cs.CL

TL;DR: 提出了一种流程监督的多代理框架，以弥合检索器和生成器之间的差距，从而提升检索增强生成 (RAG) 的效果。


<details>
  <summary>Details</summary>
Motivation: RAG 的有效性依赖于检索器和生成器之间的协调，但由于这两个组件是独立开发的，因此它们的交互通常不是最优的。

Method: 引入两个轻量级代理：决策者（决定何时继续检索或停止以生成答案）和知识选择器（过滤检索到的文档以仅保留最有用的证据）。使用 LLM-as-a-Judge 来评估每个中间动作，并采用树状 rollout 策略来探索不同的推理路径，并以端到端的方式使用近端策略优化 (PPO) 训练两个代理。

Result: 在单跳和多跳问答基准测试中，该方法实现了更高的准确率、更稳定的收敛性，并产生了更易于解释的推理轨迹。

Conclusion: 该框架是模块化的和即插即用的，无需修改检索器或生成器，使其适用于实际的 RAG 应用。

Abstract: Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to
access external knowledge sources, but the effectiveness of RAG relies on the
coordination between the retriever and the generator. Since these components
are developed independently, their interaction is often suboptimal: the
retriever may return irrelevant or redundant documents, while the generator may
fail to fully leverage retrieved evidence. In this work, we propose a
process-supervised multi-agent framework to bridge the gap between retriever
and generator. The framework introduces two lightweight agents: a Decision
Maker, which determines when to continue retrieval or stop for answer
generation, and a Knowledge Selector, which filters retrieved documents to
retain only the most useful evidence. To provide fine-grained supervision, we
employ an LLM-as-a-Judge that evaluates each intermediate action with
process-level rewards, ensuring more accurate credit assignment than relying
solely on final answer correctness. We further adopt a tree-structured rollout
strategy to explore diverse reasoning paths, and train both agents with
Proximal Policy Optimization (PPO) in an end-to-end manner. Experiments on
single-hop and multi-hop question answering benchmarks show that our approach
achieves higher accuracy, more stable convergence, and produces more
interpretable reasoning trajectories compared with standard RAG baselines.
Importantly, the proposed framework is modular and plug-and-play, requiring no
modification to the retriever or generator, making it practical for real-world
RAG applications.

</details>


### [7] [ERFC: Happy Customers with Emotion Recognition and Forecasting in Conversation in Call Centers](https://arxiv.org/abs/2509.18175)
*Aditi Debsharma,Bhushan Jagyasi,Surajit Sen,Priyanka Pandey,Devicharith Dovari,Yuvaraj V. C,Rosalin Parida,Gopali Contractor*

Main category: cs.CL

TL;DR: 本文提出了一种新的对话情感识别和预测架构（ERFC），旨在预测未来话语的情感，从而提升客户体验。


<details>
  <summary>Details</summary>
Motivation: 在呼叫中心等场景中，座席不仅要接听电话，还要通过保持中立和积极的情绪来提供良好的客户体验。预测未来话语的情感有助于座席在正确的时间提供正确的解决方案，从而改善客户体验。

Method: 提出的ERFC架构考虑了多模态、情感的不同属性、上下文以及对话中说话者话语的相互依赖性。

Result: 在IEMOCAP数据集上的大量实验表明了所提出的ERFC的可行性。

Conclusion: 该方法为呼叫中心等应用提供了巨大的商业价值，在这些应用中，客户的满意度至关重要。

Abstract: Emotion Recognition in Conversation has been seen to be widely applicable in
call center analytics, opinion mining, finance, retail, healthcare, and other
industries. In a call center scenario, the role of the call center agent is not
just confined to receiving calls but to also provide good customer experience
by pacifying the frustration or anger of the customers. This can be achieved by
maintaining neutral and positive emotion from the agent. As in any
conversation, the emotion of one speaker is usually dependent on the emotion of
other speaker. Hence the positive emotion of an agent, accompanied with the
right resolution will help in enhancing customer experience. This can change an
unhappy customer to a happy one. Imparting the right resolution at right time
becomes easier if the agent has the insight of the emotion of future
utterances. To predict the emotions of the future utterances we propose a novel
architecture, Emotion Recognition and Forecasting in Conversation. Our proposed
ERFC architecture considers multi modalities, different attributes of emotion,
context and the interdependencies of the utterances of the speakers in the
conversation. Our intensive experiments on the IEMOCAP dataset have shown the
feasibility of the proposed ERFC. This approach can provide a tremendous
business value for the applications like call center, where the happiness of
customer is utmost important.

</details>


### [8] [Are Smaller Open-Weight LLMs Closing the Gap to Proprietary Models for Biomedical Question Answering?](https://arxiv.org/abs/2509.18843)
*Damian Stachura,Joanna Konieczna,Artur Nowak*

Main category: cs.CL

TL;DR: 本研究探讨了小型开源LLM是否能有效替代大型闭源模型，特别是在生物医学问答领域。


<details>
  <summary>Details</summary>
Motivation: 目前开源LLM迅速发展，性能与闭源模型相当。研究关注小型开源LLM在生物医学问答中替代闭源模型的潜力。

Method: 通过参加BioASQ挑战赛，比较多个开源模型与GPT-4o等顶级系统的性能，采用检索相关片段、上下文学习和结构化输出等技术，并使用集成方法。

Result: 开源LLM与闭源LLM具有可比性，在某些情况下，特别是采用集成策略时，开源LLM甚至超过了闭源模型。

Conclusion: 开源LLM在生物医学问答领域具有替代闭源模型的潜力，尤其是在采用集成策略时。

Abstract: Open-weight versions of large language models (LLMs) are rapidly advancing,
with state-of-the-art models like DeepSeek-V3 now performing comparably to
proprietary LLMs. This progression raises the question of whether small
open-weight LLMs are capable of effectively replacing larger closed-source
models. We are particularly interested in the context of biomedical
question-answering, a domain we explored by participating in Task 13B Phase B
of the BioASQ challenge. In this work, we compare several open-weight models
against top-performing systems such as GPT-4o, GPT-4.1, Claude 3.5 Sonnet, and
Claude 3.7 Sonnet. To enhance question answering capabilities, we use various
techniques including retrieving the most relevant snippets based on embedding
distance, in-context learning, and structured outputs. For certain submissions,
we utilize ensemble approaches to leverage the diverse outputs generated by
different models for exact-answer questions. Our results demonstrate that
open-weight LLMs are comparable to proprietary ones. In some instances,
open-weight LLMs even surpassed their closed counterparts, particularly when
ensembling strategies were applied. All code is publicly available at
https://github.com/evidenceprime/BioASQ-13b.

</details>


### [9] [Evaluating Large Language Models for Detecting Antisemitism](https://arxiv.org/abs/2509.18293)
*Jay Patel,Hrudayangam Mehta,Jeremy Blackburn*

Main category: cs.CL

TL;DR: 本文评估了八个开源LLM检测反犹太内容的能力，利用上下文定义作为策略指导。


<details>
  <summary>Details</summary>
Motivation: 检测仇恨内容是一个具有挑战性和重要性的问题，需要不断训练机器学习模型以适应不断变化的社交媒体环境。

Method: 探索各种prompt技术，并设计了一种新的CoT-like prompt，即Guided-CoT。

Result: Guided-CoT能很好地处理上下文策略，提高了所有评估模型的性能。Llama 3.1 70B优于微调的GPT-3.5。

Conclusion: 我们的实验突出了不同LLM在效用性、可解释性和可靠性方面的差异。

Abstract: Detecting hateful content is a challenging and important problem. Automated
tools, like machine-learning models, can help, but they require continuous
training to adapt to the ever-changing landscape of social media. In this work,
we evaluate eight open-source LLMs' capability to detect antisemitic content,
specifically leveraging in-context definition as a policy guideline. We explore
various prompting techniques and design a new CoT-like prompt, Guided-CoT.
Guided-CoT handles the in-context policy well, increasing performance across
all evaluated models, regardless of decoding configuration, model sizes, or
reasoning capability. Notably, Llama 3.1 70B outperforms fine-tuned GPT-3.5.
Additionally, we examine LLM errors and introduce metrics to quantify semantic
divergence in model-generated rationales, revealing notable differences and
paradoxical behaviors among LLMs. Our experiments highlight the differences
observed across LLMs' utility, explainability, and reliability.

</details>


### [10] [Exploiting Tree Structure for Credit Assignment in RL Training of LLMs](https://arxiv.org/abs/2509.18314)
*Hieu Tran,Zonghai Yao,Hong Yu*

Main category: cs.CL

TL;DR: 提出了一种名为TEMPO的无评论员算法，用于解决LLM推理中token级别的信用分配问题。该算法通过将多个响应转换为前缀树，并结合GRPO的优势，在分支token上提供精确的token级别信用，从而在数学和医学QA等任务中优于PPO和GRPO。


<details>
  <summary>Details</summary>
Motivation: 强化学习可以提高LLM的推理能力，但长序列中稀疏的延迟奖励使得token级别的信用分配成为关键瓶颈。现有的PPO算法训练复杂且容易过拟合，GRPO算法忽略了分支。

Method: 提出Prefix-to-Tree (P2T)方法，将一组响应转换为前缀树，并通过聚合后代结果计算非参数前缀值。基于P2T，提出TEMPO算法，利用树导出的分支门控时间差校正来增强GRPO的组相对结果信号。

Result: 在Qwen3-1.7B/4B上，TEMPO在同分布和异分布基准测试中优于PPO和GRPO，并在大致相同的运行时间内达到更高的验证精度。

Conclusion: TEMPO算法有效地解决了LLM推理中token级别的信用分配问题，并在多个基准测试中取得了优越的性能。

Abstract: Reinforcement learning improves LLM reasoning, yet sparse delayed reward over
long sequences makes token-level credit assignment the key bottleneck. We study
the verifiable-reward setting, where the final answer is checkable and multiple
responses can be drawn per prompt. Reasoning tasks in math and medical QA align
with this setup, where only a few decision tokens significantly impact the
outcome. PPO offers token-level advantages with a learned value model, but it
is complex to train both the actor and critic models simultaneously, and it is
not easily generalizable, as the token-level values from the critic model can
make training prone to overfitting. GRPO is critic-free and supports verifiable
rewards, but spreads a single sequence-level return across tokens and ignores
branching. We introduce \textbf{Prefix-to-Tree (P2T)}, a simple procedure that
converts a group of responses into a prefix tree and computes
\emph{nonparametric} prefix values \(V(s)\) by aggregating descendant outcomes.
Built on P2T, we propose \textbf{TEMPO} (\emph{\textbf{T}ree-\textbf{E}stimated
\textbf{M}ean Prefix Value for \textbf{P}olicy \textbf{O}ptimization}), a
critic-free algorithm that augments the group-relative outcome signal of GRPO
with \emph{branch-gated} temporal-difference corrections derived from the tree.
At non-branch tokens, the temporal-difference (TD) term is zero, so TEMPO
reduces to GRPO; at branching tokens, it supplies precise token-level credit
without a learned value network or extra judges/teachers. On Qwen3-1.7B/4B,
TEMPO outperforms PPO and GRPO on in-distribution (MATH, MedQA) and
out-of-distribution (GSM-HARD, AMC23, MedMCQA, MMLU-Medical) benchmarks, and
reaches higher validation accuracy with roughly the same wall-clock time.

</details>


### [11] [Pathways of Thoughts: Multi-Directional Thinking for Long-form Personalized Question Answering](https://arxiv.org/abs/2509.19094)
*Alireza Salemi,Cheng Li,Mingyang Zhang,Qiaozhu Mei,Zhuowan Li,Spurthi Amba Hombaiah,Weize Kong,Tao Chen,Hamed Zamani,Michael Bendersky*

Main category: cs.CL

TL;DR: 提出了一种名为Pathways of Thoughts (PoT) 的推理阶段方法，用于个性化问答，无需特定任务的微调。


<details>
  <summary>Details</summary>
Motivation: 个性化对于使问答 (QA) 系统适应用户特定的信息需求至关重要，从而提高准确性和用户满意度。由于从冗长、嘈杂和隐式的上下文中推断偏好，以及生成同时正确、在上下文中适当且与用户期望和背景知识相符的响应等挑战，个性化 QA 仍有待探索。

Method: 将 LLM 的推理建模为一个迭代决策过程，模型在推理、修正、个性化和澄清等认知操作之间动态选择。然后，PoT 根据推断的用户偏好聚合和重新加权这些候选对象，从而产生最终的个性化响应，该响应受益于不同推理路径的互补优势。

Result: 在 LaMP-QA 基准测试中进行的个性化 QA 实验表明，PoT 始终优于竞争基线，实现了高达 13.1% 的相对改进。人工评估证实了这些结果，注释者在 66% 的情况下更喜欢 PoT 的输出，并且仅在 15% 的情况下报告平局。

Conclusion: 提出的 Pathways of Thoughts (PoT) 方法在个性化问答方面优于其他方法，并且更受用户青睐

Abstract: Personalization is essential for adapting question answering (QA) systems to
user-specific information needs, thereby improving both accuracy and user
satisfaction. However, personalized QA remains relatively underexplored due to
challenges such as inferring preferences from long, noisy, and implicit
contexts, and generating responses that are simultaneously correct,
contextually appropriate, and aligned with user expectations and background
knowledge. To address these challenges, we propose Pathways of Thoughts (PoT),
an inference-stage method that applies to any large language model (LLM)
without requiring task-specific fine-tuning. The approach models the reasoning
of an LLM as an iterative decision process, where the model dynamically selects
among cognitive operations such as reasoning, revision, personalization, and
clarification. This enables exploration of multiple reasoning trajectories,
producing diverse candidate responses that capture different perspectives. PoT
then aggregates and reweights these candidates according to inferred user
preferences, yielding a final personalized response that benefits from the
complementary strengths of diverse reasoning paths. Experiments on the LaMP-QA
benchmark for personalized QA show that PoT consistently outperforms
competitive baselines, achieving up to a 13.1% relative improvement. Human
evaluation corroborates these results, with annotators preferring outputs from
PoT in 66% of cases and reporting ties in only 15% of cases.

</details>


### [12] [Brittleness and Promise: Knowledge Graph Based Reward Modeling for Diagnostic Reasoning](https://arxiv.org/abs/2509.18316)
*Saksham Khatwani,He Cheng,Majid Afshar,Dmitriy Dligach,Yanjun Gao*

Main category: cs.CL

TL;DR: 本文提出了一种新的范式，将大型语言模型（LLM）视为知识图谱（KG）推理路径的奖励模型，以判断候选路径是否能得出正确的诊断。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在诊断推理方面显示出潜力，但通常缺乏可靠的、基于知识的推理。知识图谱（KG）提供了结构化的生物医学知识，可以支持可信的推理。

Method: 将LLM视为KG推理路径的奖励模型，模型学习判断候选路径是否能得出给定患者输入的正确诊断。系统地评估了五种知识路径判断的任务公式和八种训练范式。

Result: 特定的奖励优化和提炼可以带来强大的路径判断性能，但下游任务的可转移性仍然较弱。

Conclusion: 本文对临床KG上的“奖励模型风格”推理进行了首次系统评估，为结构化、基于奖励的监督如何影响医疗GenAI系统中的诊断推理提供了见解。

Abstract: Large language models (LLMs) show promise for diagnostic reasoning but often
lack reliable, knowledge grounded inference. Knowledge graphs (KGs), such as
the Unified Medical Language System (UMLS), offer structured biomedical
knowledge that can support trustworthy reasoning. Prior approaches typically
integrate KGs via retrieval augmented generation or fine tuning, inserting KG
content into prompts rather than enabling structured reasoning. We explore an
alternative paradigm: treating the LLM as a reward model of KG reasoning paths,
where the model learns to judge whether a candidate path leads to correct
diagnosis for a given patient input. This approach is inspired by recent work
that leverages reward training to enhance model reasoning abilities, and
grounded in computational theory, which suggests that verifying a solution is
often easier than generating one from scratch. It also parallels physicians'
diagnostic assessment, where they judge which sequences of findings and
intermediate conditions most plausibly support a diagnosis. We first
systematically evaluate five task formulation for knowledge path judging and
eight training paradigm. Second, we test whether the path judging abilities
generalize to downstream diagnostic tasks, including diagnosis summarization
and medical question answering. Experiments with three open source
instruct-tuned LLMs reveal both promise and brittleness: while specific reward
optimization and distillation lead to strong path-judging performance, the
transferability to downstream tasks remain weak. Our finding provides the first
systematic assessment of "reward model style" reasoning over clinical KGs,
offering insights into how structured, reward-based supervision influences
diagnostic reasoning in GenAI systems for healthcare.

</details>


### [13] [Speculate Deep and Accurate: Lossless and Training-Free Acceleration for Offloaded LLMs via Substitute Speculative Decoding](https://arxiv.org/abs/2509.18344)
*Pei-Shuo Wang,Jian-Jia Chen,Chun-Che Yang,Chi-Chih Chang,Ning-Chi Huang,Mohamed S. Abdelfattah,Kai-Chiang Wu*

Main category: cs.CL

TL;DR: SubSpec is a plug-and-play, lossless, and training-free method for accelerating parameter offloading in large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of deploying large language models (LLMs) on memory-limited consumer GPUs, where model compression degrades quality and offloading suffers from slow inference.

Method: SubSpec constructs a highly aligned draft model by generating low-bit quantized substitute layers from offloaded target LLM portions. It shares the remaining GPU-resident layers and the KV-Cache.

Result: SubSpec achieves a high average acceptance length, delivering 9.1x speedup for Qwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for Qwen2.5 32B on popular generation benchmarks (24GB VRAM limit).

Conclusion: SubSpec is a promising method for accelerating parameter offloading that is lossless and training-free.

Abstract: The immense model sizes of large language models (LLMs) challenge deployment
on memory-limited consumer GPUs. Although model compression and parameter
offloading are common strategies to address memory limitations, compression can
degrade quality, and offloading maintains quality but suffers from slow
inference. Speculative decoding presents a promising avenue to accelerate
parameter offloading, utilizing a fast draft model to propose multiple draft
tokens, which are then verified by the target LLM in parallel with a single
forward pass. This method reduces the time-consuming data transfers in forward
passes that involve offloaded weight transfers. Existing methods often rely on
pretrained weights of the same family, but require additional training to align
with custom-trained models. Moreover, approaches that involve draft model
training usually yield only modest speedups. This limitation arises from
insufficient alignment with the target model, preventing higher token
acceptance lengths. To address these challenges and achieve greater speedups,
we propose SubSpec, a plug-and-play method to accelerate parameter offloading
that is lossless and training-free. SubSpec constructs a highly aligned draft
model by generating low-bit quantized substitute layers from offloaded target
LLM portions. Additionally, our method shares the remaining GPU-resident layers
and the KV-Cache, further reducing memory overhead and enhance alignment.
SubSpec achieves a high average acceptance length, delivering 9.1x speedup for
Qwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for
Qwen2.5 32B on popular generation benchmarks (24GB VRAM limit).

</details>


### [14] [Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents](https://arxiv.org/abs/2509.18360)
*Chutong Meng,Philipp Koehn*

Main category: cs.CL

TL;DR: Speech Vecalign: a parallel speech document alignment method that monotonically aligns speech segment embeddings and does not depend on text transcriptions.


<details>
  <summary>Details</summary>
Motivation: Compared to the baseline method Global Mining, a variant of speech mining, Speech Vecalign produces longer speech-to-speech alignments. It also demonstrates greater robustness than Local Mining, another speech mining variant, as it produces less noise.

Method: We applied Speech Vecalign to 3,000 hours of unlabeled parallel English-German (En-De) speech documents from VoxPopuli, yielding about 1,000 hours of high-quality alignments. We then trained En-De speech-to-speech translation models on the aligned data.

Result: Speech Vecalign improves the En-to-De and De-to-En performance over Global Mining by 0.37 and 0.18 ASR-BLEU, respectively. Moreover, our models match or outperform SpeechMatrix model performance, despite using 8 times fewer raw speech documents.

Conclusion: Speech Vecalign is effective for parallel speech document alignment and improves speech-to-speech translation performance.

Abstract: We present Speech Vecalign, a parallel speech document alignment method that
monotonically aligns speech segment embeddings and does not depend on text
transcriptions. Compared to the baseline method Global Mining, a variant of
speech mining, Speech Vecalign produces longer speech-to-speech alignments. It
also demonstrates greater robustness than Local Mining, another speech mining
variant, as it produces less noise. We applied Speech Vecalign to 3,000 hours
of unlabeled parallel English-German (En-De) speech documents from VoxPopuli,
yielding about 1,000 hours of high-quality alignments. We then trained En-De
speech-to-speech translation models on the aligned data. Speech Vecalign
improves the En-to-De and De-to-En performance over Global Mining by 0.37 and
0.18 ASR-BLEU, respectively. Moreover, our models match or outperform
SpeechMatrix model performance, despite using 8 times fewer raw speech
documents.

</details>


### [15] [Interactive Real-Time Speaker Diarization Correction with Human Feedback](https://arxiv.org/abs/2509.18377)
*Xinlu He,Yiwen Guan,Badrivishal Paurana,Zilin Dai,Jacob Whitehill*

Main category: cs.CL

TL;DR: 提出了一种LLM辅助的说话人日志校正系统，允许用户实时修正说话人归属错误。


<details>
  <summary>Details</summary>
Motivation: 大多数自动语音处理系统在“开环”模式下运行，没有用户关于谁说了什么的反馈；然而，人机协作工作流程有可能实现更高的准确性。

Method: 该pipeline执行流式ASR和日志，使用LLM向用户提供简洁的摘要，并接受立即合并的简短口头反馈，而不会中断交互。此外，我们还开发了使工作流程更有效的技术：首先，一种split-when-merged (SWM)技术检测并分割ASR错误地归因于单个说话人的多说话人片段。其次，在线说话人注册是基于用户的日志校正收集的，从而有助于防止未来发生说话人日志错误。

Result: 在AMI测试集上进行的llm驱动的模拟表明，我们的系统大大降低了9.92%的DER和44.23%的说话人混淆错误。

Conclusion: 我们进一步分析了不同设置下的校正效果，包括摘要与完整抄本显示、在线注册数量限制和校正频率。

Abstract: Most automatic speech processing systems operate in "open loop" mode without
user feedback about who said what; yet, human-in-the-loop workflows can
potentially enable higher accuracy. We propose an LLM-assisted speaker
diarization correction system that lets users fix speaker attribution errors in
real time. The pipeline performs streaming ASR and diarization, uses an LLM to
deliver concise summaries to the users, and accepts brief verbal feedback that
is immediately incorporated without disrupting interactions. Moreover, we
develop techniques to make the workflow more effective: First, a
split-when-merged (SWM) technique detects and splits multi-speaker segments
that the ASR erroneously attributes to just a single speaker. Second, online
speaker enrollments are collected based on users' diarization corrections, thus
helping to prevent speaker diarization errors from occurring in the future.
LLM-driven simulations on the AMI test set indicate that our system
substantially reduces DER by 9.92% and speaker confusion error by 44.23%. We
further analyze correction efficacy under different settings, including summary
vs full transcript display, the number of online enrollments limitation, and
correction frequency.

</details>


### [16] [NormGenesis: Multicultural Dialogue Generation via Exemplar-Guided Social Norm Modeling and Violation Recovery](https://arxiv.org/abs/2509.18395)
*Minki Hong,Jangho Choi,Jihie Kim*

Main category: cs.CL

TL;DR: 本文提出了NormGenesis，一个用于生成和注释跨英语、中文和韩语的社会对话的多文化框架。


<details>
  <summary>Details</summary>
Motivation: 为了在静态规范分类之外对社会互动的动态进行建模。

Method: 我们提出了一种新的对话类型，违反到解决(V2R)，它通过识别和适当的社会修复来模拟违反规范后的对话进展。为了提高代表性不足的语言中的语用一致性，我们在对话合成过程的早期实现了一个基于范例的迭代细化。

Result: 人类和基于llm的评估表明，NormGenesis在细化质量、对话自然性和泛化性能方面显著优于现有数据集。我们表明，在我们的v2r增强数据上训练的模型在伦理敏感的上下文中表现出改进的语用能力。

Conclusion: 我们的工作为文化适应性对话建模建立了一个新的基准，并为跨语言和文化多样化语言的规范感知生成提供了一个可扩展的方法。

Abstract: Social norms govern culturally appropriate behavior in communication,
enabling dialogue systems to produce responses that are not only coherent but
also socially acceptable. We present NormGenesis, a multicultural framework for
generating and annotating socially grounded dialogues across English, Chinese,
and Korean. To model the dynamics of social interaction beyond static norm
classification, we propose a novel dialogue type, Violation-to-Resolution
(V2R), which models the progression of conversations following norm violations
through recognition and socially appropriate repair. To improve pragmatic
consistency in underrepresented languages, we implement an exemplar-based
iterative refinement early in the dialogue synthesis process. This design
introduces alignment with linguistic, emotional, and sociocultural expectations
before full dialogue generation begins. Using this framework, we construct a
dataset of 10,800 multi-turn dialogues annotated at the turn level for norm
adherence, speaker intent, and emotional response. Human and LLM-based
evaluations demonstrate that NormGenesis significantly outperforms existing
datasets in refinement quality, dialogue naturalness, and generalization
performance. We show that models trained on our V2R-augmented data exhibit
improved pragmatic competence in ethically sensitive contexts. Our work
establishes a new benchmark for culturally adaptive dialogue modeling and
provides a scalable methodology for norm-aware generation across linguistically
and culturally diverse languages.

</details>


### [17] [Evaluating the Creativity of LLMs in Persian Literary Text Generation](https://arxiv.org/abs/2509.18401)
*Armin Tourajmehr,Mohammad Reza Modarres,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: 评估大型语言模型 (LLM) 生成波斯文学文本的能力，重点关注创造力和文化相关性。


<details>
  <summary>Details</summary>
Motivation: 先前研究主要集中在英语文学文本的生成，对非英语文学传统的研究有限，缺乏评估创造力的标准化方法。

Method: 1. 构建包含 20 个不同主题的波斯文学数据集；2. 调整 Torrance 创造性思维测试，从原创性、流畅性、灵活性和精细性四个维度评估模型输出；3. 采用 LLM 作为评估者进行自动评分，并通过类内相关系数验证其相对于人工判断的可靠性；4. 分析模型理解和运用四种核心文学手段（明喻、隐喻、夸张和对偶）的能力。

Result: 结果突出了 LLM 在波斯文学文本生成中的优势和局限性。

Conclusion: 表明需要进一步改进。

Abstract: Large language models (LLMs) have demonstrated notable creative abilities in
generating literary texts, including poetry and short stories. However, prior
research has primarily centered on English, with limited exploration of
non-English literary traditions and without standardized methods for assessing
creativity. In this paper, we evaluate the capacity of LLMs to generate Persian
literary text enriched with culturally relevant expressions. We build a dataset
of user-generated Persian literary spanning 20 diverse topics and assess model
outputs along four creativity dimensions-originality, fluency, flexibility, and
elaboration-by adapting the Torrance Tests of Creative Thinking. To reduce
evaluation costs, we adopt an LLM as a judge for automated scoring and validate
its reliability against human judgments using intraclass correlation
coefficients, observing strong agreement. In addition, we analyze the models'
ability to understand and employ four core literary devices: simile, metaphor,
hyperbole, and antithesis. Our results highlight both the strengths and
limitations of LLMs in Persian literary text generation, underscoring the need
for further refinement.

</details>


### [18] [Developing an AI framework to automatically detect shared decision-making in patient-doctor conversations](https://arxiv.org/abs/2509.18439)
*Oscar J. Ponce-Ponte,David Toro-Tobon,Luis F. Figueroa,Michael Gionfriddo,Megan Branda,Victor M. Montori,Saturnino Luz,Juan P. Brito*

Main category: cs.CL

TL;DR: 本研究旨在开发一种自动测量共享决策 (SDM) 的方法，通过使用语言建模和对话对齐 (CA) 分数。


<details>
  <summary>Details</summary>
Motivation: 目前没有方法可以大规模自动测量 SDM，而 SDM 对于实现以患者为中心的护理至关重要。

Method: 使用来自患者-医生对话的转录文本，训练深度学习 (DL) 模型和微调 BERT 模型，并计算 CA 分数。评估 CA 分数与 SDM 结果之间的关联。

Result: DL 模型和微调的 BERT 模型在 CA 分数上有所不同，某些 CA 分数与 SDM 结果相关。

Conclusion: 本研究介绍了一种自动、可扩展的方法，通过可解释的 CA 分数来衡量患者-医生对话中的 SDM，并有可能大规模评估 SDM 策略。

Abstract: Shared decision-making (SDM) is necessary to achieve patient-centred care.
Currently no methodology exists to automatically measure SDM at scale. This
study aimed to develop an automated approach to measure SDM by using language
modelling and the conversational alignment (CA) score. A total of 157
video-recorded patient-doctor conversations from a randomized multi-centre
trial evaluating SDM decision aids for anticoagulation in atrial fibrillations
were transcribed and segmented into 42,559 sentences. Context-response pairs
and negative sampling were employed to train deep learning (DL) models and
fine-tuned BERT models via the next sentence prediction (NSP) task. Each
top-performing model was used to calculate four types of CA scores. A
random-effects analysis by clinician, adjusting for age, sex, race, and trial
arm, assessed the association between CA scores and SDM outcomes: the
Decisional Conflict Scale (DCS) and the Observing Patient Involvement in
Decision-Making 12 (OPTION12) scores. p-values were corrected for multiple
comparisons with the Benjamini-Hochberg method. Among 157 patients (34% female,
mean age 70 SD 10.8), clinicians on average spoke more words than patients
(1911 vs 773). The DL model without the stylebook strategy achieved a recall@1
of 0.227, while the fine-tuned BERTbase (110M) achieved the highest recall@1
with 0.640. The AbsMax (18.36 SE7.74 p=0.025) and Max CA (21.02 SE7.63 p=0.012)
scores generated with the DL without stylebook were associated with OPTION12.
The Max CA score generated with the fine-tuned BERTbase (110M) was associated
with the DCS score (-27.61 SE12.63 p=0.037). BERT model sizes did not have an
impact the association between CA scores and SDM. This study introduces an
automated, scalable methodology to measure SDM in patient-doctor conversations
through explainable CA scores, with potential to evaluate SDM strategies at
scale.

</details>


### [19] [CogniLoad: A Synthetic Natural Language Reasoning Benchmark With Tunable Length, Intrinsic Difficulty, and Distractor Density](https://arxiv.org/abs/2509.18458)
*Daniel Kaiser,Arnoldo Frigessi,Ali Ramezani-Kebrya,Benjamin Ricaud*

Main category: cs.CL

TL;DR: 提出了一个名为 CogniLoad 的新的合成基准，用于评估大型语言模型 (LLM) 中的长上下文推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文推理基准模糊了内在任务复杂性、干扰因素和任务长度等关键因素，为了能够进行更精确的失败分析。

Method: CogniLoad 基于认知负荷理论 (CLT) 生成自然语言逻辑谜题，其参数可独立调整，以反映 CLT 的核心维度：内在难度 (d)、干扰信号比 (ρ) 和任务长度 (N)。

Result: CogniLoad 揭示了不同的性能敏感性，确定任务长度是主要的约束，并揭示了对内在复杂性的不同容忍度和对干扰因素比率的 U 形响应。

Conclusion: CogniLoad 提供了一种可重现、可扩展且具有诊断意义的工具，用于剖析 LLM 推理的局限性并指导未来的模型开发。

Abstract: Current benchmarks for long-context reasoning in Large Language Models (LLMs)
often blur critical factors like intrinsic task complexity, distractor
interference, and task length. To enable more precise failure analysis, we
introduce CogniLoad, a novel synthetic benchmark grounded in Cognitive Load
Theory (CLT). CogniLoad generates natural-language logic puzzles with
independently tunable parameters that reflect CLT's core dimensions: intrinsic
difficulty ($d$) controls intrinsic load; distractor-to-signal ratio ($\rho$)
regulates extraneous load; and task length ($N$) serves as an operational proxy
for conditions demanding germane load. Evaluating 22 SotA reasoning LLMs,
CogniLoad reveals distinct performance sensitivities, identifying task length
as a dominant constraint and uncovering varied tolerances to intrinsic
complexity and U-shaped responses to distractor ratios. By offering systematic,
factorial control over these cognitive load dimensions, CogniLoad provides a
reproducible, scalable, and diagnostically rich tool for dissecting LLM
reasoning limitations and guiding future model development.

</details>


### [20] [LAWCAT: Efficient Distillation from Quadratic to Linear Attention with Convolution across Tokens for Long Context Modeling](https://arxiv.org/abs/2509.18467)
*Zeyu Liu,Souvik Kundu,Lianghao Jiang,Anni Li,Srikanth Ronanki,Sravan Bodapati,Gourav Datta,Peter A. Beerel*

Main category: cs.CL

TL;DR: 提出了一种名为LAWCAT的新框架，旨在有效地将预训练transformers的能力转移到高性能线性注意力架构中。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在各个领域都取得了最先进的性能，但其相对于序列长度的二次计算复杂度仍然是一个显着的瓶颈，特别是对于延迟敏感的长上下文应用。

Method: LAWCAT集成了因果Conv1D层以增强局部依赖性建模，并采用归一化门控线性注意力以提高在不同上下文长度上的泛化能力。

Result: 使用仅有1K长度序列的Mistral-7B进行提炼，可以实现高达22K tokens的超过90％的密码检索准确率，从而显着扩展了其有效上下文窗口。Llama3.2-1B LAWCAT变体在S-NIAH 1＆2＆3任务（1K-8K上下文长度）和BABILong基准（QA2＆QA3，0K-16K上下文长度）上实现了具有竞争力的性能，与预训练模型相比，所需的预训练tokens少于0.1％。对于超过8K tokens的序列，LAWCAT的预填充速度比FlashAttention-2更快。

Conclusion: LAWCAT提供了一条通往高性能，长上下文线性模型的有效途径，适用于边缘部署，减少了对大量长序列训练数据和计算资源的依赖。

Abstract: Although transformer architectures have achieved state-of-the-art performance
across diverse domains, their quadratic computational complexity with respect
to sequence length remains a significant bottleneck, particularly for
latency-sensitive long-context applications. While recent linear-complexity
alternatives are increasingly powerful, effectively training them from scratch
is still resource-intensive. To overcome these limitations, we propose LAWCAT
(Linear Attention with Convolution Across Time), a novel linearization
framework designed to efficiently transfer the capabilities of pre-trained
transformers into a performant linear attention architecture. LAWCAT integrates
causal Conv1D layers to enhance local dependency modeling and employs
normalized gated linear attention to improve generalization across varying
context lengths. Our comprehensive evaluations demonstrate that, distilling
Mistral-7B with only 1K-length sequences yields over 90\% passkey retrieval
accuracy up to 22K tokens, significantly extending its effective context
window. Similarly, Llama3.2-1B LAWCAT variant achieves competitive performance
on S-NIAH 1\&2\&3 tasks (1K-8K context length) and BABILong benchmark
(QA2\&QA3, 0K-16K context length), requiring less than 0.1\% pre-training
tokens compared with pre-training models. Furthermore, LAWCAT exhibits faster
prefill speeds than FlashAttention-2 for sequences exceeding 8K tokens. LAWCAT
thus provides an efficient pathway to high-performance, long-context linear
models suitable for edge deployment, reducing reliance on extensive
long-sequence training data and computational resources.

</details>


### [21] [Actions Speak Louder than Prompts: A Large-Scale Study of LLMs for Graph Inference](https://arxiv.org/abs/2509.18487)
*Ben Finkelshtein,Silviu Cucerzan,Sujay Kumar Jauhar,Ryen White*

Main category: cs.CL

TL;DR: 本文对大型语言模型（LLM）在基于文本的图机器学习任务中的能力进行了大规模、受控的评估，涵盖了LLM-图交互模式、数据集领域、结构机制、特征和模型配置等多个关键维度。


<details>
  <summary>Details</summary>
Motivation: 虽然人们对LLM与图数据的交互越来越感兴趣，但该领域缺乏对LLM能力的系统性理解。

Method: 通过比较prompting、工具使用和代码生成等LLM-图交互模式，跨越引用、网络链接、电子商务和社交网络等数据集领域，对比同质图和异质图等结构机制，涉及短文本和长文本节点属性等特征，以及不同LLM大小和推理能力的模型配置，进行大规模评估。

Result: LLM作为代码生成器在图数据上实现了最强大的整体性能，尤其是在长文本或高阶图上，prompting很快就会超过token预算。所有交互策略在异质图上仍然有效。代码生成能够灵活地调整其对结构、特征或标签的依赖性，以利用信息量最大的输入类型。

Conclusion: 这些发现全面地展示了当前LLM-图交互模式的优势和局限性，并强调了未来方法设计的关键原则。

Abstract: Large language models (LLMs) are increasingly used for text-rich graph
machine learning tasks such as node classification in high-impact domains like
fraud detection and recommendation systems. Yet, despite a surge of interest,
the field lacks a principled understanding of the capabilities of LLMs in their
interaction with graph data. In this work, we conduct a large-scale, controlled
evaluation across several key axes of variability to systematically assess the
strengths and weaknesses of LLM-based graph reasoning methods in text-based
applications. The axes include the LLM-graph interaction mode, comparing
prompting, tool-use, and code generation; dataset domains, spanning citation,
web-link, e-commerce, and social networks; structural regimes contrasting
homophilic and heterophilic graphs; feature characteristics involving both
short- and long-text node attributes; and model configurations with varying LLM
sizes and reasoning capabilities. We further analyze dependencies by
methodically truncating features, deleting edges, and removing labels to
quantify reliance on input types. Our findings provide practical and actionable
guidance. (1) LLMs as code generators achieve the strongest overall performance
on graph data, with especially large gains on long-text or high-degree graphs
where prompting quickly exceeds the token budget. (2) All interaction
strategies remain effective on heterophilic graphs, challenging the assumption
that LLM-based methods collapse under low homophily. (3) Code generation is
able to flexibly adapt its reliance between structure, features, or labels to
leverage the most informative input type. Together, these findings provide a
comprehensive view of the strengths and limitations of current LLM-graph
interaction modes and highlight key design principles for future approaches.

</details>


### [22] [A Rhythm-Aware Phrase Insertion for Classical Arabic Poetry Composition](https://arxiv.org/abs/2509.18514)
*Mohamad Elzohbi,Richard Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种使用 ByT5 模型在阿拉伯语诗歌中插入短语以符合特定节奏的方法。


<details>
  <summary>Details</summary>
Motivation: 目标是让插入的短语符合特定节奏，解决阿拉伯语诗歌创作中节奏对齐的问题。

Method: 该方法包括：1) 基于规则的字形到节拍转换；2) 使用条件去噪目标微调 ByT5 模型，使其重建被屏蔽的词以匹配目标节奏；3) 采用课程学习策略，先在通用阿拉伯语数据集上进行预训练，然后在诗歌数据集上进行微调；4) 探索从英语到阿拉伯语的跨语言迁移。

Result: 实验结果表明，该模型在保持语义连贯性的同时，实现了高节奏对齐。

Conclusion: 该模型有潜力用于创作古典阿拉伯语诗歌的协同创作应用。

Abstract: This paper presents a methodology for inserting phrases in Arabic poems to
conform to a specific rhythm using ByT5, a byte-level multilingual
transformer-based model. Our work discusses a rule-based grapheme-to-beat
transformation tailored for extracting the rhythm from fully diacritized Arabic
script. Our approach employs a conditional denoising objective to fine-tune
ByT5, where the model reconstructs masked words to match a target rhythm. We
adopt a curriculum learning strategy, pre-training on a general Arabic dataset
before fine-tuning on poetic dataset, and explore cross-lingual transfer from
English to Arabic. Experimental results demonstrate that our models achieve
high rhythmic alignment while maintaining semantic coherence. The proposed
model has the potential to be used in co-creative applications in the process
of composing classical Arabic poems.

</details>


### [23] [Trace Is In Sentences: Unbiased Lightweight ChatGPT-Generated Text Detector](https://arxiv.org/abs/2509.18535)
*Mo Mu,Dianqiao Lei,Chang Li*

Main category: cs.CL

TL;DR: 这篇论文提出了一种新的框架，用于检测原始的和经过释义修改的AI生成的文本。


<details>
  <summary>Details</summary>
Motivation: 当前基于词级的AI生成文本检测器容易受到释义或简单提示的攻击，并且会受到ChatGPT的词级模式和训练数据内容引起的偏差的影响，在修改后的文本上表现不佳，并且通常需要大型模型或在线LLM交互。

Method: 该方法基于文本的内部结构对文本进行分类，该结构在词级更改下保持不变。该方法对来自预训练语言模型的句子嵌入进行编码，并通过注意力机制对它们的关系进行建模。采用对比学习来减轻来自自回归生成的嵌入偏差，并结合使用带有反事实方法的因果图，以将结构特征与主题相关的偏差隔离开。

Result: 在包括抽象比较和修订后的生活常见问题解答在内的两个精选数据集上的实验验证了该方法的有效性。

Conclusion: 该论文提出了一种轻量级框架，可以有效地检测原始的和经过释义修改的AI生成的文本，并且该框架能够减轻偏差并隔离结构特征。

Abstract: The widespread adoption of ChatGPT has raised concerns about its misuse,
highlighting the need for robust detection of AI-generated text. Current
word-level detectors are vulnerable to paraphrasing or simple prompts (PSP),
suffer from biases induced by ChatGPT's word-level patterns (CWP) and training
data content, degrade on modified text, and often require large models or
online LLM interaction. To tackle these issues, we introduce a novel task to
detect both original and PSP-modified AI-generated texts, and propose a
lightweight framework that classifies texts based on their internal structure,
which remains invariant under word-level changes. Our approach encodes sentence
embeddings from pre-trained language models and models their relationships via
attention. We employ contrastive learning to mitigate embedding biases from
autoregressive generation and incorporate a causal graph with counterfactual
methods to isolate structural features from topic-related biases. Experiments
on two curated datasets, including abstract comparisons and revised life FAQs,
validate the effectiveness of our method.

</details>


### [24] [CCQA: Generating Question from Solution Can Improve Inference-Time Reasoning in SLMs](https://arxiv.org/abs/2509.18536)
*Jin Young Kim,Ji Won Yoon*

Main category: cs.CL

TL;DR: 本文提出了一种名为循环一致性问答 (CCQA) 的新型推理方法，该方法通过生成问题并评估其与原始问题的相似度来改进小型语言模型 (SLM) 的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有推理策略在大型语言模型 (LLM) 上表现良好，但在小型模型上的效果尚不清楚，并且传统方法通常无法提高小型模型的性能。

Method: CCQA 从每个推理路径和答案生成问题，通过其与原始问题的相似性来评估每个问题，然后选择具有最高相似性得分的候选解决方案作为最终答案。使用轻量级的 Flan-T5 模型生成问题。

Result: 实验结果表明，在数学和常识推理基准测试中，CCQA 在八个模型上始终优于现有的最先进 (SOTA) 方法。

Conclusion: CCQA 为 SLM 中的高效推理建立了一个新的实用基线。

Abstract: Recently, inference-time reasoning strategies have further improved the
accuracy of large language models (LLMs), but their effectiveness on smaller
models remains unclear. Based on the observation that conventional approaches
often fail to improve performance in this context, we propose
\textbf{C}ycle-\textbf{C}onsistency in \textbf{Q}uestion \textbf{A}nswering
(CCQA), a novel reasoning method that can be effectively applied to SLMs.
Inspired by cycle consistency, CCQA generates a question from each reasoning
path and answer, evaluates each by its similarity to the original question, and
then selects the candidate solution with the highest similarity score as the
final response. Since conventional SLMs struggle to generate accurate questions
from their own reasoning paths and answers, we employ a lightweight Flan-T5
model specialized for question generation to support this process efficiently.
From the experimental results, it is verified that CCQA consistently
outperforms existing state-of-the-art (SOTA) methods across eight models on
mathematical and commonsense reasoning benchmarks. Furthermore, our method
establishes a new practical baseline for efficient reasoning in SLMs. Source
code can be found at https://github.com/scai-research/ccqa_official.

</details>


### [25] [Prior-based Noisy Text Data Filtering: Fast and Strong Alternative For Perplexity](https://arxiv.org/abs/2509.18577)
*Yeongbin Seo,Gayoung Kim,Jaehyung Kim,Jinyoung Yeo*

Main category: cs.CL

TL;DR: 提出了一种基于先验的数据过滤方法，该方法使用语料库级别的术语频率统计来估计token先验，并根据token先验的均值和标准差来过滤文档。


<details>
  <summary>Details</summary>
Motivation: 为了确保有效和高效的学习，对数据进行仔细选择至关重要。基于困惑度（PPL）的过滤虽然表现出强大的性能，但存在缺点：大量的时间成本以及模型在处理嘈杂或分布外样本时的固有不可靠性。

Method: 提出了一种基于先验的数据过滤方法，该方法使用语料库级别的术语频率统计来估计token先验，并根据token先验的均值和标准差来过滤文档，作为PPL的快速代理，而无需模型推理。

Result: 基于先验的过滤器在20个下游基准测试中实现了最高的平均性能，同时与基于PPL的过滤相比，时间成本降低了1000倍以上。证明了其对代码和数学等符号语言的适用性，以及其对多语言语料库的动态适应性，无需监督。

Conclusion: 该论文提出了一种简单但强大的替代方案，即基于先验的数据过滤方法，该方法可以有效地过滤数据，同时降低时间成本。

Abstract: As large language models (LLMs) are pretrained on massive web corpora,
careful selection of data becomes essential to ensure effective and efficient
learning. While perplexity (PPL)-based filtering has shown strong performance,
it suffers from drawbacks: substantial time costs and inherent unreliability of
the model when handling noisy or out-of-distribution samples. In this work, we
propose a simple yet powerful alternative: a prior-based data filtering method
that estimates token priors using corpus-level term frequency statistics,
inspired by linguistic insights on word roles and lexical density. Our approach
filters documents based on the mean and standard deviation of token priors,
serving as a fast proxy to PPL while requiring no model inference. Despite its
simplicity, the prior-based filter achieves the highest average performance
across 20 downstream benchmarks, while reducing time cost by over 1000x
compared to PPL-based filtering. We further demonstrate its applicability to
symbolic languages such as code and math, and its dynamic adaptability to
multilingual corpora without supervision

</details>


### [26] [TsqLoRA: Towards Sensitivity and Quality Low-Rank Adaptation for Efficient Fine-Tuning](https://arxiv.org/abs/2509.18585)
*Yu Chen,Yifei Han,Long Zhang,Yue Du,Bin Li*

Main category: cs.CL

TL;DR: 这篇论文提出了一种名为TsqLoRA的新方法，旨在提高大型预训练模型在下游任务中的微调效率，同时保持或提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效微调方法忽略了不同模型层的不同敏感性和训练数据的重要性，尤其是在资源受限的环境中，完全微调所有模型参数的计算成本很高且内存密集。

Method: 该方法结合了数据质量驱动的选择和敏感性感知的低秩适应，包括一个用于选择信息量最大的训练数据的质量感知采样机制，以及一个基于每层对参数更新的敏感性来调整每层秩的动态秩分配模块。

Result: 实验结果表明，TsqLoRA提高了微调效率，同时在各种NLP任务中保持甚至提高了性能。

Conclusion: TsqLoRA 是一种很有前途的参数高效微调方法。

Abstract: Fine-tuning large pre-trained models for downstream tasks has become a
fundamental approach in natural language processing. Fully fine-tuning all
model parameters is computationally expensive and memory-intensive, especially
in resource-constrained environments. Existing parameter-efficient fine-tuning
methods reduce the number of trainable parameters but typically overlook the
varying sensitivity of different model layers and the importance of training
data. In this work, we propose TsqLoRA, a novel method that integrates
data-quality-driven selection with sensitivity-aware low-rank adaptation,
consisted of two main components: a quality-aware sampling mechanism for
selecting the most informative training data, and a dynamic rank allocation
module that adjusts the rank of each layer based on its sensitivity to
parameter updates. The experimental results demonstrate that TsqLoRA improves
fine-tuning efficiency while maintaining or even improving performance on a
variety of NLP tasks. Our code will be available at
https://github.com/Benjamin-Ricky/TsqLoRA.

</details>


### [27] [UniECG: Understanding and Generating ECG in One Unified Model](https://arxiv.org/abs/2509.18588)
*Jiarui Jin,Haoyu Wang,Xiang Lan,Jun Li,Gaofeng Cheng,Hongyan Li,Shenda Hong*

Main category: cs.CL

TL;DR: UniECG is a unified model for ECG that can perform both ECG interpretation and generation.


<details>
  <summary>Details</summary>
Motivation: Existing unified models struggle with ECG signals and medical diagnoses.

Method: A decoupled two-stage training approach is used, first learning interpretation (ECG-to-Text) and then generation (Text-to-ECG) via latent space alignment.

Result: UniECG can autonomously interpret or generate ECGs based on user input.

Conclusion: UniECG extends the capabilities of current ECG models.

Abstract: Recent unified models such as GPT-5 have achieved encouraging progress on
vision-language tasks. However, these unified models typically fail to
correctly understand ECG signals and provide accurate medical diagnoses, nor
can they correctly generate ECG signals. To address these limitations, we
propose UniECG, the first unified model for ECG capable of concurrently
performing evidence-based ECG interpretation and text-conditioned ECG
generation tasks. Through a decoupled two-stage training approach, the model
first learns evidence-based interpretation skills (ECG-to-Text), and then
injects ECG generation capabilities (Text-to-ECG) via latent space alignment.
UniECG can autonomously choose to interpret or generate an ECG based on user
input, significantly extending the capability boundaries of current ECG models.
Our code and checkpoints will be made publicly available at
https://github.com/PKUDigitalHealth/UniECG upon acceptance.

</details>


### [28] [A Good Plan is Hard to Find: Aligning Models with Preferences is Misaligned with What Helps Users](https://arxiv.org/abs/2509.18632)
*Nishant Balepur,Matthew Shu,Yoo Yeon Sung,Seraphina Goldfarb-Tarrant,Shi Feng,Fumeng Yang,Rachel Rudinger,Jordan Lee Boyd-Graber*

Main category: cs.CL

TL;DR: 大型语言模型（LLM）生成的计划旨在帮助用户完成复杂任务，但研究表明，用户偏好和模型偏好并不总是与计划的实际帮助程度相符。因此，常见的对齐反馈可能会导致LLM在有用性方面出现偏差。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于检验当前LLM对齐方法中，基于用户偏好进行训练和评估的假设是否成立，即用户偏好能够反映计划的实际帮助程度。研究发现用户/模型偏好和agent成功率无法准确预测哪些计划对用户有帮助，因此常见的对齐反馈可能会与有用性不符。

Method: 研究使用了Planorama界面，邀请126名用户参与，回答300个多步骤问题，并使用LLM计划。收集了4388个计划执行数据和5584个比较数据，以衡量计划的帮助程度（QA成功率）和用户对计划的偏好。同时，在agent和奖励模型中重现该设置，以评估它们是否能模拟或偏好对用户有帮助的计划。

Result: 研究结果表明：1）用户/模型偏好和agent成功率不能准确预测哪些计划对用户有帮助；2）这种差距并非源于用户特定偏好；3）简洁性和问题相似性等表面线索与偏好密切相关，但这些偏差无法预测帮助程度。

Conclusion: 研究结论强调，对齐有用的LLM需要来自真实用户交互的反馈，而不仅仅是看起来有用的偏好。

Abstract: To assist users in complex tasks, LLMs generate plans: step-by-step
instructions towards a goal. While alignment methods aim to ensure LLM plans
are helpful, they train (RLHF) or evaluate (ChatbotArena) on what users prefer,
assuming this reflects what helps them. We test this with Planorama: an
interface where 126 users answer 300 multi-step questions with LLM plans. We
get 4388 plan executions and 5584 comparisons to measure plan helpfulness (QA
success) and user preferences on plans, and recreate the setup in agents and
reward models to see if they simulate or prefer what helps users. We expose: 1)
user/model preferences and agent success do not accurately predict which plans
help users, so common alignment feedback can misalign with helpfulness; 2) this
gap is not due to user-specific preferences, as users are similarly successful
when using plans they prefer/disprefer; 3) surface-level cues like brevity and
question similarity strongly link to preferences, but such biases fail to
predict helpfulness. In all, we argue aligning helpful LLMs needs feedback from
real user interactions, not just preferences of what looks helpful, so we
discuss the plan NLP researchers can execute to solve this problem.

</details>


### [29] [Consistency-Aware Parameter-Preserving Knowledge Editing Framework for Multi-Hop Question Answering](https://arxiv.org/abs/2509.18655)
*Lingwen Deng,Yifei Han,Long Zhang,Yue Du,Bin Li*

Main category: cs.CL

TL;DR: CAPE-KG: A consistency-aware framework for parameter-preserving knowledge editing on multi-hop question answering (MHQA).


<details>
  <summary>Details</summary>
Motivation: Existing PPKE methods lack consistency, leading to knowledge contamination and unstable updates in multi-hop reasoning.

Method: Proposes CAPE-KG, a framework ensuring KG construction, update, and retrieval are aligned with MHQA task requirements.

Result: Experiments on MQuAKE show accuracy improvements in PPKE performance for MHQA.

Conclusion: Addressing consistency in PPKE improves performance in multi-hop question answering.

Abstract: Parameter-Preserving Knowledge Editing (PPKE) enables updating models with
new or corrected information without retraining or parameter adjustment. Recent
PPKE approaches based on knowledge graphs (KG) to extend knowledge editing (KE)
capabilities to multi-hop question answering (MHQA). However, these methods
often lack consistency, leading to knowledge contamination, unstable updates,
and retrieval behaviors that fail to reflect the intended edits. Such
inconsistencies undermine the reliability of PPKE in multi- hop reasoning. We
present CAPE-KG, Consistency-Aware Parameter-Preserving Editing with Knowledge
Graphs, a novel consistency-aware framework for PPKE on MHQA. CAPE-KG ensures
KG construction, update, and retrieval are always aligned with the requirements
of the MHQA task, maintaining coherent reasoning over both unedited and edited
knowledge. Extensive experiments on the MQuAKE benchmark show accuracy
improvements in PPKE performance for MHQA, demonstrating the effectiveness of
addressing consistency in PPKE.

</details>


### [30] [Analyzing Uncertainty of LLM-as-a-Judge: Interval Evaluations with Conformal Prediction](https://arxiv.org/abs/2509.18658)
*Huanxin Sheng,Xinyi Liu,Hangfeng He,Jieyu Zhao,Jian Kang*

Main category: cs.CL

TL;DR: 本研究探讨了使用大型语言模型（LLM）评估自然语言生成（NLG）任务时评估结果的不确定性问题，并提出了一个基于共形预测的框架来分析这种不确定性，为LLM评分提供预测区间。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型作为评估自然语言生成任务的评判标准，其评估结果的不确定性尚未被充分研究，这限制了它在许多应用中的部署。

Method: 该研究提出了一个基于共形预测的框架，通过构建连续的预测区间来分析LLM评分的不确定性，并为离散评级任务设计了有序边界调整。此外，还提出了一种基于区间中点的评分方法，作为原始模型评分和加权平均的低偏差替代方案。

Result: 实验结果表明，共形预测可以提供有效的、具有覆盖保证的预测区间。研究还探讨了区间中点和法官重新提示对于改善判断的有效性。

Conclusion: 该研究首次提出了一个分析LLM评估NLG任务不确定性的框架，并验证了其有效性，为提高LLM评估的可靠性提供了新思路。

Abstract: LLM-as-a-judge has become a promising paradigm for using large language
models (LLMs) to evaluate natural language generation (NLG), but the
uncertainty of its evaluation remains underexplored. This lack of reliability
may limit its deployment in many applications. This work presents the first
framework to analyze the uncertainty by offering a prediction interval of
LLM-based scoring via conformal prediction. Conformal prediction constructs
continuous prediction intervals from a single evaluation run, and we design an
ordinal boundary adjustment for discrete rating tasks. We also suggest a
midpoint-based score within the interval as a low-bias alternative to raw model
score and weighted average. We perform extensive experiments and analysis,
which show that conformal prediction can provide valid prediction interval with
coverage guarantees. We also explore the usefulness of interval midpoint and
judge reprompting for better judgment.

</details>


### [31] [MemOrb: A Plug-and-Play Verbal-Reinforcement Memory Layer for E-Commerce Customer Service](https://arxiv.org/abs/2509.18713)
*Yizhe Huang,Yang Liu,Ruiyu Zhao,Xiaolong Zhong,Xingming Yue,Ling Jiang*

Main category: cs.CL

TL;DR: LLM-based agents in customer service are unreliable due to forgetting, repeating errors, and lacking self-improvement. This paper introduces MemOrb, a memory layer that distills interactions into strategy reflections for improved reliability and consistency.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based agents in customer service lack reliability in dynamic settings due to forgetting, repeating errors and lacking self-improvement.

Method: Proposes MemOrb, a lightweight, plug-and-play verbal reinforcement memory layer that distills multi-turn interactions into compact strategy reflections, stored in a shared memory bank and retrieved to guide decision-making without fine-tuning.

Result: MemOrb significantly improves both success rate and stability, achieving up to a 63 percentage-point gain in multi-turn success rate and delivering more consistent performance across repeated trials.

Conclusion: Structured reflection is a powerful mechanism for enhancing long-term reliability of frozen LLM agents in customer service scenarios.

Abstract: Large Language Model-based agents(LLM-based agents) are increasingly deployed
in customer service, yet they often forget across sessions, repeat errors, and
lack mechanisms for continual self-improvement. This makes them unreliable in
dynamic settings where stability and consistency are critical. To better
evaluate these properties, we emphasize two indicators: task success rate as a
measure of overall effectiveness, and consistency metrics such as Pass$^k$ to
capture reliability across multiple trials. To address the limitations of
existing approaches, we propose MemOrb, a lightweight and plug-and-play verbal
reinforcement memory layer that distills multi-turn interactions into compact
strategy reflections. These reflections are stored in a shared memory bank and
retrieved to guide decision-making, without requiring any fine-tuning.
Experiments show that MemOrb significantly improves both success rate and
stability, achieving up to a 63 percentage-point gain in multi-turn success
rate and delivering more consistent performance across repeated trials. Our
results demonstrate that structured reflection is a powerful mechanism for
enhancing long-term reliability of frozen LLM agents in customer service
scenarios.

</details>


### [32] [LOTUSDIS: A Thai far-field meeting corpus for robust conversational ASR](https://arxiv.org/abs/2509.18722)
*Pattara Tipaksorn,Sumonmas Thatphithakkul,Vataya Chunwijitra,Kwanchiva Thangthai*

Main category: cs.CL

TL;DR: LOTUSDIS：一个公开的泰语会议语料库，用于提升远场会话ASR。


<details>
  <summary>Details</summary>
Motivation: 现有的语音识别模型在远场泰语语音识别方面表现不佳，因为预训练数据与实际的泰语远场语音存在不匹配。

Method: 构建了一个包含114小时的自发、无脚本对话的泰语会议语料库，使用多种麦克风在不同距离录制，并提供了训练、开发和测试集。

Result: 在LOTUSDIS上微调Whisper模型后，整体WER从64.3降至38.3，远场WER从81.6降至49.5，尤其是在最远距离的麦克风上获得了显著的提升。

Conclusion: 距离多样化的训练数据对于提高ASR的鲁棒性至关重要。该语料库和基线系统已公开发布，以促进该领域的可复现研究。

Abstract: We present LOTUSDIS, a publicly available Thai meeting corpus designed to
advance far-field conversational ASR. The dataset comprises 114 hours of
spontaneous, unscripted dialogue collected in 15-20 minute sessions with three
participants, where overlapping speech is frequent and natural. Speech was
recorded simultaneously by nine independent single-channel devices spanning six
microphone types at distances from 0.12 m to 10 m, preserving the authentic
effects of reverberation, noise, and device coloration without relying on
microphone arrays. We provide standard train, dev, test splits and release a
reproducible baseline system. We benchmarked several Whisper variants under
zero-shot and fine-tuned conditions. Off-the-shelf models showed strong
degradation with distance, confirming a mismatch between pre-training data and
Thai far-field speech. Fine-tuning on LOTUSDIS dramatically improved
robustness: a Thai Whisper baseline reduced overall WER from 64.3 to 38.3 and
far-field WER from 81.6 to 49.5, with especially large gains on the most
distant microphones. These results underscore the importance of
distance-diverse training data for robust ASR. The corpus is available under
CC-BY-SA 4.0. We also release training and evaluation scripts as a baseline
system to promote reproducible research in this field.

</details>


### [33] [Global-Recent Semantic Reasoning on Dynamic Text-Attributed Graphs with Large Language Models](https://arxiv.org/abs/2509.18742)
*Yunan Wang,Jianxin Li,Ziwei Zhang*

Main category: cs.CL

TL;DR: 提出了一种名为DyGRASP的新方法，利用LLM和时间GNN来有效地推理DyTAG。


<details>
  <summary>Details</summary>
Motivation: 现有的方法主要集中在静态TAG上，忽略了最近的全局时间语义：交互文本之间最近的语义依赖关系和节点随时间的全局语义演变。此外，将LLM应用于DyTAG中丰富且不断发展的文本面临效率问题。

Method: 设计了一种以节点为中心的隐式推理方法，结合滑动窗口机制来有效地捕获最近的时间语义。此外，为了捕获节点的全局语义动态，利用带有定制提示的显式推理和类似RNN的链结构来推断长期语义。最后，使用更新和合并层复杂地整合了最近和全局时间语义以及动态图结构信息。

Result: 在DyTAG基准测试中进行了广泛的实验，证明了DyGRASP的优越性，在目标节点检索任务中，Hit@10的性能提高了34%。此外，DyGRASP在不同的时间GNN和LLM中表现出强大的泛化能力。

Conclusion: DyGRASP是一种有效且高效的DyTAG推理方法。

Abstract: Dynamic Text-Attribute Graphs (DyTAGs), characterized by time-evolving graph
interactions and associated text attributes, are prevalent in real-world
applications. Existing methods, such as Graph Neural Networks (GNNs) and Large
Language Models (LLMs), mostly focus on static TAGs. Extending these existing
methods to DyTAGs is challenging as they largely neglect the recent-global
temporal semantics: the recent semantic dependencies among interaction texts
and the global semantic evolution of nodes over time. Furthermore, applying
LLMs to the abundant and evolving text in DyTAGs faces efficiency issues. To
tackle these challenges, we propose Dynamic Global-Recent Adaptive Semantic
Processing (DyGRASP), a novel method that leverages LLMs and temporal GNNs to
efficiently and effectively reason on DyTAGs. Specifically, we first design a
node-centric implicit reasoning method together with a sliding window mechanism
to efficiently capture recent temporal semantics. In addition, to capture
global semantic dynamics of nodes, we leverage explicit reasoning with tailored
prompts and an RNN-like chain structure to infer long-term semantics. Lastly,
we intricately integrate the recent and global temporal semantics as well as
the dynamic graph structural information using updating and merging layers.
Extensive experiments on DyTAG benchmarks demonstrate DyGRASP's superiority,
achieving up to 34% improvement in Hit@10 for destination node retrieval task.
Besides, DyGRASP exhibits strong generalization across different temporal GNNs
and LLMs.

</details>


### [34] [False Friends Are Not Foes: Investigating Vocabulary Overlap in Multilingual Language Models](https://arxiv.org/abs/2509.18750)
*Julie Kallini,Dan Jurafsky,Christopher Potts,Martijn Bartelds*

Main category: cs.CL

TL;DR: 研究了多语种语料库训练的子词分词器中token重叠对跨语言迁移的影响。


<details>
  <summary>Details</summary>
Motivation: 先前研究结果不一，存在多种设置和混淆因素，如token频率或子词分割粒度。

Method: 在多个语言对上训练双语自回归模型，系统地改变词汇重叠设置，并探索共享token的语义相似性。

Result: 发现任何形式的重叠都会创建捕捉跨语言语义关系的嵌入空间，重叠模型在XNLI和XQuAD上优于不重叠模型，迁移性能通常随着重叠的增加而提高。

Conclusion: token重叠在多语种模型中具有优势，大量共享词汇仍然是多语种分词器的有益设计选择。

Abstract: Subword tokenizers trained on multilingual corpora naturally produce
overlapping tokens across languages. Does token overlap facilitate
cross-lingual transfer or instead introduce interference between languages?
Prior work offers mixed evidence, partly due to varied setups and confounders,
such as token frequency or subword segmentation granularity. To address this
question, we devise a controlled experiment where we train bilingual
autoregressive models on multiple language pairs under systematically varied
vocabulary overlap settings. Crucially, we explore a new dimension to
understanding how overlap affects transfer: the semantic similarity of tokens
shared across languages. We first analyze our models' hidden representations
and find that overlap of any kind creates embedding spaces that capture
cross-lingual semantic relationships, while this effect is much weaker in
models with disjoint vocabularies. On XNLI and XQuAD, we find that models with
overlap outperform models with disjoint vocabularies, and that transfer
performance generally improves as overlap increases. Overall, our findings
highlight the advantages of token overlap in multilingual models and show that
substantial shared vocabulary remains a beneficial design choice for
multilingual tokenizers.

</details>


### [35] [When Long Helps Short: How Context Length in Supervised Fine-tuning Affects Behavior of Large Language Models](https://arxiv.org/abs/2509.18762)
*Yingming Zheng,Hanqi Li,Kai Yu,Lu Chen*

Main category: cs.CL

TL;DR: 研究了长文本SFT如何影响LLM在短文本任务上的表现，发现长文本SFT可以提高短文本性能。


<details>
  <summary>Details</summary>
Motivation: 现实应用需要更长的上下文窗口，因此在长文本数据上进行持续预训练和SFT变得普遍。但是，数据长度对SFT的影响尚不清楚。

Method: 系统地研究了SFT数据长度如何影响LLM在短文本任务上的行为。解耦并分析了MHA和FFN，并研究了它们的相互作用。

Result: 长文本SFT可以提高短文本性能。长文本SFT促进上下文知识，而短文本SFT偏爱参数知识。混合训练可以减轻这种偏差。

Conclusion: 长文本SFT可以提高短文本性能，但过度依赖长文本SFT并非最优。混合训练可以减轻知识偏好偏差，为微调LLM提供了解释性指导。

Abstract: Large language models (LLMs) have achieved impressive performance across
natural language processing (NLP) tasks. As real-world applications
increasingly demand longer context windows, continued pretraining and
supervised fine-tuning (SFT) on long-context data has become a common approach.
While the effects of data length in continued pretraining have been extensively
studied, their implications for SFT remain unclear. In this work, we
systematically investigate how SFT data length influences LLM behavior on
short-context tasks. Counterintuitively, we find that long-context SFT improves
short-context performance, contrary to the commonly observed degradation from
long-context pretraining. To uncover the underlying mechanisms of this
phenomenon, we first decouple and analyze two key components, Multi-Head
Attention (MHA) and Feed-Forward Network (FFN), and show that both
independently benefit from long-context SFT. We further study their interaction
and reveal a knowledge preference bias: long-context SFT promotes contextual
knowledge, while short-context SFT favors parametric knowledge, making
exclusive reliance on long-context SFT suboptimal. Finally, we demonstrate that
hybrid training mitigates this bias, offering explainable guidance for
fine-tuning LLMs.

</details>


### [36] [Financial Risk Relation Identification through Dual-view Adaptation](https://arxiv.org/abs/2509.18775)
*Wei-Ning Chiu,Yu-Hsiang Wang,Andy Hsiao,Yu-Shiang Huang,Chuan-Ju Wang*

Main category: cs.CL

TL;DR: 本文提出了一种从Form 10-K文件中提取公司间风险关系的方法。


<details>
  <summary>Details</summary>
Motivation: 识别公司间风险关系对于投资组合管理和投资策略等应用至关重要。传统方法依赖于专家判断和人工分析，但存在主观、劳动密集和难以扩展的问题。

Method: 利用自然语言处理的最新进展，该方法通过基于备案中时间和词汇模式的无监督微调来捕捉隐式和抽象的风险联系。这使得能够开发具有更深层次上下文理解的领域特定金融编码器，并引入用于透明、可解释分析的定量风险关系评分。

Result: 广泛的实验表明，该方法在多个评估设置中优于强大的基线。

Conclusion: 该方法提供了一种系统、可扩展且更客观的方式来识别公司间的风险关系。

Abstract: A multitude of interconnected risk events -- ranging from regulatory changes
to geopolitical tensions -- can trigger ripple effects across firms.
Identifying inter-firm risk relations is thus crucial for applications like
portfolio management and investment strategy. Traditionally, such assessments
rely on expert judgment and manual analysis, which are, however, subjective,
labor-intensive, and difficult to scale. To address this, we propose a
systematic method for extracting inter-firm risk relations using Form 10-K
filings -- authoritative, standardized financial documents -- as our data
source. Leveraging recent advances in natural language processing, our approach
captures implicit and abstract risk connections through unsupervised
fine-tuning based on chronological and lexical patterns in the filings. This
enables the development of a domain-specific financial encoder with a deeper
contextual understanding and introduces a quantitative risk relation score for
transparency, interpretable analysis. Extensive experiments demonstrate that
our method outperforms strong baselines across multiple evaluation settings.

</details>


### [37] [AECBench: A Hierarchical Benchmark for Knowledge Evaluation of Large Language Models in the AEC Field](https://arxiv.org/abs/2509.18776)
*Chen Liang,Zhaoqi Huang,Haofen Wang,Fu Chai,Chunying Yu,Huanhuan Wei,Zhengjie Liu,Yanpeng Li,Hongjun Wang,Ruifeng Luo,Xianzhong Zhao*

Main category: cs.CL

TL;DR: 本文构建了一个名为AECBench的综合基准，用于评估大型语言模型（LLM）在建筑、工程和建造（AEC）领域的性能。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在这个专业和安全关键领域的稳健性和可靠性。

Method: 通过定义一个包含23个代表性任务的五级认知评估框架，并构建一个包含4800个问题的多样化数据集，引入LLM-as-a-Judge方法。

Result: LLM在知识记忆和理解等基础任务中表现出能力，但在解释建筑规范中的表格知识、执行复杂推理和计算以及生成领域特定文档方面表现出明显的性能缺陷。

Conclusion: 为未来在安全关键工程实践中稳健和可靠地集成LLM奠定了基础。

Abstract: Large language models (LLMs), as a novel information technology, are seeing
increasing adoption in the Architecture, Engineering, and Construction (AEC)
field. They have shown their potential to streamline processes throughout the
building lifecycle. However, the robustness and reliability of LLMs in such a
specialized and safety-critical domain remain to be evaluated. To address this
challenge, this paper establishes AECBench, a comprehensive benchmark designed
to quantify the strengths and limitations of current LLMs in the AEC domain.
The benchmark defines 23 representative tasks within a five-level
cognition-oriented evaluation framework encompassing Knowledge Memorization,
Understanding, Reasoning, Calculation, and Application. These tasks were
derived from authentic AEC practice, with scope ranging from codes retrieval to
specialized documents generation. Subsequently, a 4,800-question dataset
encompassing diverse formats, including open-ended questions, was crafted
primarily by engineers and validated through a two-round expert review.
Furthermore, an LLM-as-a-Judge approach was introduced to provide a scalable
and consistent methodology for evaluating complex, long-form responses
leveraging expert-derived rubrics. Through the evaluation of nine LLMs, a clear
performance decline across five cognitive levels was revealed. Despite
demonstrating proficiency in foundational tasks at the Knowledge Memorization
and Understanding levels, the models showed significant performance deficits,
particularly in interpreting knowledge from tables in building codes, executing
complex reasoning and calculation, and generating domain-specific documents.
Consequently, this study lays the groundwork for future research and
development aimed at the robust and reliable integration of LLMs into
safety-critical engineering practices.

</details>


### [38] [Beyond the Leaderboard: Understanding Performance Disparities in Large Language Models via Model Diffing](https://arxiv.org/abs/2509.18792)
*Sabri Boughorbel,Fahim Dalvi,Nadir Durrani,Majd Hawasly*

Main category: cs.CL

TL;DR: 本文运用模型差异分析方法，对比了Gemma-2-9b-it模型及其SimPO增强变体的具体能力差异。


<details>
  <summary>Details</summary>
Motivation: 传统基准测试无法解释模型性能差异的原因，因此需要理解微调过程中发生的改变。

Method: 使用crosscoders识别并分类区分两个模型的潜在表征。

Result: SimPO主要提升了安全性、多语言能力和指令遵循能力，同时减少了模型自我参照和幻觉管理。

Conclusion: 模型差异分析能够提供比排行榜指标更细粒度的见解，并将性能差距归因于具体的能力。

Abstract: As fine-tuning becomes the dominant paradigm for improving large language
models (LLMs), understanding what changes during this process is increasingly
important. Traditional benchmarking often fails to explain why one model
outperforms another. In this work, we use model diffing, a mechanistic
interpretability approach, to analyze the specific capability differences
between Gemma-2-9b-it and a SimPO-enhanced variant. Using crosscoders, we
identify and categorize latent representations that differentiate the two
models. We find that SimPO acquired latent concepts predominantly enhance
safety mechanisms (+32.8%), multilingual capabilities (+43.8%), and
instruction-following (+151.7%), while its additional training also reduces
emphasis on model self-reference (-44.1%) and hallucination management
(-68.5%). Our analysis shows that model diffing can yield fine-grained insights
beyond leaderboard metrics, attributing performance gaps to concrete
mechanistic capabilities. This approach offers a transparent and targeted
framework for comparing LLMs.

</details>


### [39] [MAPEX: A Multi-Agent Pipeline for Keyphrase Extraction](https://arxiv.org/abs/2509.18813)
*Liting Zhang,Shiwan Zhao,Aobo Kong,Qicheng Li*

Main category: cs.CL

TL;DR: 提出了MAPEX，一个多代理协作框架，用于关键短语提取。


<details>
  <summary>Details</summary>
Motivation: 现有无监督的基于提示的大型语言模型（LLM）方法通常依赖于单阶段推理管道和统一提示，而不管文档长度或LLM骨干如何。这种“一刀切”的设计阻碍了LLM推理和生成能力的充分发挥，尤其是在跨不同场景的关键短语提取的复杂性方面。

Method: MAPEX通过专家招募、候选提取、主题指导、知识增强和后处理等模块协调基于LLM的代理。双路径策略动态适应文档长度：用于短文本的知识驱动提取和用于长文本的主题指导提取。

Result: 在三个不同LLM的六个基准数据集上的大量实验表明，MAPEX具有很强的泛化性和通用性，在F1@5上平均比最先进的无监督方法高2.44%，比标准LLM基线高4.01%。

Conclusion: MAPEX框架有效地提升了关键短语提取的性能，具有良好的泛化性和通用性。

Abstract: Keyphrase extraction is a fundamental task in natural language processing.
However, existing unsupervised prompt-based methods for Large Language Models
(LLMs) often rely on single-stage inference pipelines with uniform prompting,
regardless of document length or LLM backbone. Such one-size-fits-all designs
hinder the full exploitation of LLMs' reasoning and generation capabilities,
especially given the complexity of keyphrase extraction across diverse
scenarios. To address these challenges, we propose MAPEX, the first framework
that introduces multi-agent collaboration into keyphrase extraction. MAPEX
coordinates LLM-based agents through modules for expert recruitment, candidate
extraction, topic guidance, knowledge augmentation, and post-processing. A
dual-path strategy dynamically adapts to document length: knowledge-driven
extraction for short texts and topic-guided extraction for long texts.
Extensive experiments on six benchmark datasets across three different LLMs
demonstrate its strong generalization and universality, outperforming the
state-of-the-art unsupervised method by 2.44\% and standard LLM baselines by
4.01\% in F1@5 on average. Code is available at
https://github.com/NKU-LITI/MAPEX.

</details>


### [40] [Multi-Hierarchical Feature Detection for Large Language Model Generated Text](https://arxiv.org/abs/2509.18862)
*Luyan Zhang,Xinyu Xie*

Main category: cs.CL

TL;DR: 本文研究了多特征融合方法在AI文本检测中的应用，发现其性能提升有限，但计算成本显著增加。


<details>
  <summary>Details</summary>
Motivation: 研究动机是检验多特征融合方法是否能显著提高AI文本检测的性能，弥补单一神经模型的不足。之前的研究缺乏对现代LLM生成文本的严格测试。

Method: 本文提出了MHFD方法，该方法集成了基于DeBERTa的语义分析、句法分析和统计概率特征，并通过自适应融合进行集成。

Result: 实验结果表明，多特征融合方法对性能的提升很小(0.4-0.5%)，但计算成本却大幅增加(4.2x)。MHFD方法在同域检测中达到89.7%的准确率，在跨域检测中保持84.2%的稳定性能，相比现有方法略有提升(0.4-2.6%)。

Conclusion: 多特征融合方法在AI文本检测中的收益有限，现代神经语言模型可能已经有效地捕捉到了最相关的检测信号。

Abstract: With the rapid advancement of large language model technology, there is
growing interest in whether multi-feature approaches can significantly improve
AI text detection beyond what single neural models achieve. While intuition
suggests that combining semantic, syntactic, and statistical features should
provide complementary signals, this assumption has not been rigorously tested
with modern LLM-generated text. This paper provides a systematic empirical
investigation of multi-hierarchical feature integration for AI text detection,
specifically testing whether the computational overhead of combining multiple
feature types is justified by performance gains. We implement MHFD
(Multi-Hierarchical Feature Detection), integrating DeBERTa-based semantic
analysis, syntactic parsing, and statistical probability features through
adaptive fusion. Our investigation reveals important negative results: despite
theoretical expectations, multi-feature integration provides minimal benefits
(0.4-0.5% improvement) while incurring substantial computational costs (4.2x
overhead), suggesting that modern neural language models may already capture
most relevant detection signals efficiently. Experimental results on multiple
benchmark datasets demonstrate that the MHFD method achieves 89.7% accuracy in
in-domain detection and maintains 84.2% stable performance in cross-domain
detection, showing modest improvements of 0.4-2.6% over existing methods.

</details>


### [41] [Diversity Boosts AI-Generated Text Detection](https://arxiv.org/abs/2509.18880)
*Advik Raj Basani,Pin-Yu Chen*

Main category: cs.CL

TL;DR: 提出了一种名为DivEye的新框架，用于检测AI生成的文本，该框架通过基于惊奇度的特征捕捉文本中不可预测性的波动。


<details>
  <summary>Details</summary>
Motivation: 检测AI生成文本的需求日益增加，但现有检测器在高Generations质量面前表现不佳，且缺乏可解释性。

Method: 利用人类作者的文本在词汇和结构不可预测性方面比LLM输出表现出更丰富的可变性这一观察结果，DivEye通过一组可解释的统计特征来捕捉这种信号。

Result: DivEye优于现有的zero-shot检测器高达33.2%，并在多个基准测试中实现了与微调基线相当的性能。DivEye对释义和对抗性攻击具有鲁棒性，在不同领域和模型中具有良好的泛化能力，并且当用作辅助信号时，可以将现有检测器的性能提高高达18.7%。

Conclusion: DivEye不仅可以检测AI生成的文本，还可以深入了解文本被标记的原因，指出节奏不可预测性是LLM检测的一个强大且未被充分探索的信号。

Abstract: Detecting AI-generated text is an increasing necessity to combat misuse of
LLMs in education, business compliance, journalism, and social media, where
synthetic fluency can mask misinformation or deception. While prior detectors
often rely on token-level likelihoods or opaque black-box classifiers, these
approaches struggle against high-quality generations and offer little
interpretability. In this work, we propose DivEye, a novel detection framework
that captures how unpredictability fluctuates across a text using
surprisal-based features. Motivated by the observation that human-authored text
exhibits richer variability in lexical and structural unpredictability than LLM
outputs, DivEye captures this signal through a set of interpretable statistical
features. Our method outperforms existing zero-shot detectors by up to 33.2%
and achieves competitive performance with fine-tuned baselines across multiple
benchmarks. DivEye is robust to paraphrasing and adversarial attacks,
generalizes well across domains and models, and improves the performance of
existing detectors by up to 18.7% when used as an auxiliary signal. Beyond
detection, DivEye provides interpretable insights into why a text is flagged,
pointing to rhythmic unpredictability as a powerful and underexplored signal
for LLM detection.

</details>


### [42] [Extractive Fact Decomposition for Interpretable Natural Language Inference in one Forward Pass](https://arxiv.org/abs/2509.18901)
*Nicholas Popovič,Michael Färber*

Main category: cs.CL

TL;DR: 提出了一种名为JEDI的仅编码器架构，用于执行可解释的推理和抽取式原子事实分解，而无需在推理过程中使用生成模型。


<details>
  <summary>Details</summary>
Motivation: 现有的自然语言推理(NLI)和相关任务(如自动事实核查)依赖于资源密集型的生成大型语言模型(llm)来执行分解，以提高可解释性和鲁棒性。

Method: 提出JEDI，一种仅编码器的架构，可以联合执行抽取式原子事实分解和可解释推理，而无需在推理过程中使用生成模型。为了方便训练，我们生成了一个大型的合成理由语料库，涵盖多个NLI基准。

Result: 实验结果表明，JEDI在分布内的准确率具有竞争力，并且在分布外和对抗性环境中，相对于仅基于抽取式理由监督的模型，JEDI的鲁棒性显著提高。

Conclusion: 我们的研究结果表明，使用仅编码器的架构和合成理由可以实现NLI中的可解释性和鲁棒的泛化。

Abstract: Recent works in Natural Language Inference (NLI) and related tasks, such as
automated fact-checking, employ atomic fact decomposition to enhance
interpretability and robustness. For this, existing methods rely on
resource-intensive generative large language models (LLMs) to perform
decomposition. We propose JEDI, an encoder-only architecture that jointly
performs extractive atomic fact decomposition and interpretable inference
without requiring generative models during inference. To facilitate training,
we produce a large corpus of synthetic rationales covering multiple NLI
benchmarks. Experimental results demonstrate that JEDI achieves competitive
accuracy in distribution and significantly improves robustness out of
distribution and in adversarial settings over models based solely on extractive
rationale supervision. Our findings show that interpretability and robust
generalization in NLI can be realized using encoder-only architectures and
synthetic rationales. Code and data available at https://jedi.nicpopovic.com

</details>


### [43] [DTW-Align: Bridging the Modality Gap in End-to-End Speech Translation with Dynamic Time Warping Alignment](https://arxiv.org/abs/2509.18987)
*Abderrahmane Issam,Yusuf Can Semerci,Jan Scholtes,Gerasimos Spanakis*

Main category: cs.CL

TL;DR: 本文提出了一种新的端到端语音翻译方法，通过动态时间规整（DTW）对齐语音和文本嵌入，从而弥合模态差距。


<details>
  <summary>Details</summary>
Motivation: 现有端到端语音翻译方法需要在词或token级别对齐语音和文本表示，这需要对齐工具，而并非所有语言都可用。虽然可以使用最近邻相似性搜索对齐语音和文本嵌入，但这并不准确。

Method: 本文采用动态时间规整（DTW）在训练期间对齐语音和文本嵌入。

Result: 实验表明，该方法能更准确地对齐语音和文本，并在端到端语音翻译中取得可比的结果，同时速度更快。在低资源环境下，该方法在6个语言方向中的5个方向上优于之前的工作。

Conclusion: 本文提出的方法有效地弥合了端到端语音翻译中的模态差距，并在低资源环境下表现出色。

Abstract: End-to-End Speech Translation (E2E-ST) is the task of translating source
speech directly into target text bypassing the intermediate transcription step.
The representation discrepancy between the speech and text modalities has
motivated research on what is known as bridging the modality gap.
State-of-the-art methods addressed this by aligning speech and text
representations on the word or token level. Unfortunately, this requires an
alignment tool that is not available for all languages. Although this issue has
been addressed by aligning speech and text embeddings using nearest-neighbor
similarity search, it does not lead to accurate alignments. In this work, we
adapt Dynamic Time Warping (DTW) for aligning speech and text embeddings during
training. Our experiments demonstrate the effectiveness of our method in
bridging the modality gap in E2E-ST. Compared to previous work, our method
produces more accurate alignments and achieves comparable E2E-ST results while
being significantly faster. Furthermore, our method outperforms previous work
in low resource settings on 5 out of 6 language directions.

</details>


### [44] [Investigating Test-Time Scaling with Reranking for Machine Translation](https://arxiv.org/abs/2509.19020)
*Shaomu Tan,Ryosuke Mitani,Ritvik Choudhary,Toshiyuki Sekiya*

Main category: cs.CL

TL;DR: 本文研究了机器翻译中的测试时间缩放 (TTS) 方法，通过在推理时分配更多计算资源来提高翻译质量。


<details>
  <summary>Details</summary>
Motivation: 扩展模型参数是改进 NLP 系统的常见策略，但计算成本很高。测试时间缩放 (TTS) 提供了一种替代方案，通过在推理时分配更多计算资源来生成多个候选并选择最佳方案。

Method: 本文在 WMT24 基准测试中，对一个简单但实用的 best-of-N 框架进行了研究，涵盖了六个高资源和一个低资源语言对，五个模型大小 (3B-72B) 以及各种 TTS 计算预算（N 高达 1024）。

Result: 对于高资源语言，TTS 通常可以提高翻译质量，人工评估证实了这些收益。用大 N 增强的小模型可以匹配或超过 N=1 时更大的模型，但计算成本更高。在固定计算预算下，较大的模型通常更有效，并且由于低资源案例中的指标盲点，TTS 可能会降低质量。

Conclusion: TTS 是一种提高机器翻译质量的方法，但需要仔细考虑计算成本和模型大小，尤其是在低资源情况下。

Abstract: Scaling model parameters has become the de facto strategy for improving NLP
systems, but it comes with substantial computational costs. Test-Time Scaling
(TTS) offers an alternative by allocating more computation at inference:
generating multiple candidates and selecting the best. While effective in tasks
such as mathematical reasoning, TTS has not been systematically explored for
machine translation (MT). In this paper, we present the first systematic study
of TTS for MT, investigating a simple but practical best-of-N framework on
WMT24 benchmarks. Our experiments cover six high-resource and one low-resource
language pairs, five model sizes (3B-72B), and various TTS compute budget (N up
to 1024). Our results show that a) For high-resource languages, TTS generally
improves translation quality according to multiple neural MT evaluation
metrics, and our human evaluation confirms these gains; b) Augmenting smaller
models with large $N$ can match or surpass larger models at $N{=}1$ with more
compute cost; c) Under fixed compute budgets, larger models are typically more
efficient, and TTS can degrade quality due to metric blind spots in
low-resource cases.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [45] [PolypSeg-GradCAM: Towards Explainable Computer-Aided Gastrointestinal Disease Detection Using U-Net Based Segmentation and Grad-CAM Visualization on the Kvasir Dataset](https://arxiv.org/abs/2509.18159)
*Akwasi Asare,Ulas Bagci*

Main category: cs.CV

TL;DR: 提出了一种可解释的深度学习框架PolypSeg-GradCAM，用于透明的息肉分割。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌是全球癌症相关发病率和死亡率的主要原因之一，胃肠道息肉是关键的癌前病变。在结肠镜检查中早期和准确地分割息肉对于降低CRC进展至关重要，但手动描绘劳动强度大且容易出现观察者差异。深度学习方法在自动化息肉分析方面显示出强大的潜力，但其有限的可解释性仍然是临床应用的一个障碍。

Method: 该框架集成了U-Net架构和梯度加权类激活映射（Grad-CAM）。

Result: 在Kvasir-SEG数据集上进行了训练和评估，在测试集上实现了0.9257的平均交并比（IoU），并且在训练和验证集上实现了持续较高的Dice系数（F-score > 0.96）。Grad-CAM可视化进一步证实，预测是由临床相关区域指导的，从而提高了模型决策的透明度和信任度。

Conclusion: PolypSeg-GradCAM通过将高分割精度与可解释性相结合，代表了迈向可靠、值得信赖的AI辅助结肠镜检查和改善早期结直肠癌预防的一步。

Abstract: Colorectal cancer (CRC) remains one of the leading causes of cancer-related
morbidity and mortality worldwide, with gastrointestinal (GI) polyps serving as
critical precursors according to the World Health Organization (WHO). Early and
accurate segmentation of polyps during colonoscopy is essential for reducing
CRC progression, yet manual delineation is labor-intensive and prone to
observer variability. Deep learning methods have demonstrated strong potential
for automated polyp analysis, but their limited interpretability remains a
barrier to clinical adoption. In this study, we present PolypSeg-GradCAM, an
explainable deep learning framework that integrates the U-Net architecture with
Gradient-weighted Class Activation Mapping (Grad-CAM) for transparent polyp
segmentation. The model was trained and evaluated on the Kvasir-SEG dataset of
1000 annotated endoscopic images. Experimental results demonstrate robust
segmentation performance, achieving a mean Intersection over Union (IoU) of
0.9257 on the test set and consistently high Dice coefficients (F-score > 0.96)
on training and validation sets. Grad-CAM visualizations further confirmed that
predictions were guided by clinically relevant regions, enhancing transparency
and trust in the model's decisions. By coupling high segmentation accuracy with
interpretability, PolypSeg-GradCAM represents a step toward reliable,
trustworthy AI-assisted colonoscopy and improved early colorectal cancer
prevention.

</details>


### [46] [PerceptronCARE: A Deep Learning-Based Intelligent Teleopthalmology Application for Diabetic Retinopathy Diagnosis](https://arxiv.org/abs/2509.18160)
*Akwasi Asare,Isaac Baffour Senkyire,Emmanuel Freeman,Simon Hilary Ayinedenaba Aluze-Ele,Kelvin Kwao*

Main category: cs.CV

TL;DR: PerceptronCARE is a deep learning telemedicine app for diabetic retinopathy detection.


<details>
  <summary>Details</summary>
Motivation: Diabetic retinopathy is a major cause of vision loss, especially in underserved regions.

Method: Developed and evaluated using CNNs like ResNet-18, EfficientNet-B0, and SqueezeNet.

Result: Achieved 85.4% accuracy in classifying disease severity.

Conclusion: AI telemedicine can expand access to diabetic retinopathy screening, especially in remote areas.

Abstract: Diabetic retinopathy is a leading cause of vision loss among adults and a
major global health challenge, particularly in underserved regions. This study
presents PerceptronCARE, a deep learning-based teleophthalmology application
designed for automated diabetic retinopathy detection using retinal images. The
system was developed and evaluated using multiple convolutional neural
networks, including ResNet-18, EfficientNet-B0, and SqueezeNet, to determine
the optimal balance between accuracy and computational efficiency. The final
model classifies disease severity with an accuracy of 85.4%, enabling real-time
screening in clinical and telemedicine settings. PerceptronCARE integrates
cloud-based scalability, secure patient data management, and a multi-user
framework, facilitating early diagnosis, improving doctor-patient interactions,
and reducing healthcare costs. This study highlights the potential of AI-driven
telemedicine solutions in expanding access to diabetic retinopathy screening,
particularly in remote and resource-constrained environments.

</details>


### [47] [Self Identity Mapping](https://arxiv.org/abs/2509.18165)
*Xiuding Cai,Yaoyao Zhu,Linjie Fu,Dong Miao,Yu Yao*

Main category: cs.CV

TL;DR: 提出了自身份映射（SIM）作为一种新的正则化框架，通过逆映射机制增强表示学习，并减少信息损失。


<details>
  <summary>Details</summary>
Motivation: 传统正则化技术依赖启发式方法，在不同设置下的可靠性和有效性较低。SIM旨在解决这个问题。

Method: 通过从变换后的输出重建输入，SIM减少前向传播中的信息损失，并促进更平滑的梯度流动。为了解决计算效率问题，SIM通过结合patch-level特征采样和基于投影的方法来重建潜在特征，从而有效地降低了复杂性。

Result: 在图像分类、少样本提示学习和域泛化三个任务中，实验结果表明，与基线方法相比，性能得到了一致的提高，这突出了SIM在各种任务中增强表示学习的能力。SIM与现有的正则化方法是正交的，可以提高它们的有效性。SIM在密集到密集任务（如语义分割和图像翻译）以及非视觉领域（包括音频分类和时间序列异常检测）中有效地保留了语义信息并提高了性能。

Conclusion: SIM作为一种模型无关、任务无关的正则化器，可以无缝集成作为即插即用模块，适用于不同的网络架构和任务。

Abstract: Regularization is essential in deep learning to enhance generalization and
mitigate overfitting. However, conventional techniques often rely on
heuristics, making them less reliable or effective across diverse settings. We
propose Self Identity Mapping (SIM), a simple yet effective, data-intrinsic
regularization framework that leverages an inverse mapping mechanism to enhance
representation learning. By reconstructing the input from its transformed
output, SIM reduces information loss during forward propagation and facilitates
smoother gradient flow. To address computational inefficiencies, We instantiate
SIM as $ \rho\text{SIM} $ by incorporating patch-level feature sampling and
projection-based method to reconstruct latent features, effectively lowering
complexity. As a model-agnostic, task-agnostic regularizer, SIM can be
seamlessly integrated as a plug-and-play module, making it applicable to
different network architectures and tasks.
  We extensively evaluate $\rho\text{SIM}$ across three tasks: image
classification, few-shot prompt learning, and domain generalization.
Experimental results show consistent improvements over baseline methods,
highlighting $\rho\text{SIM}$'s ability to enhance representation learning
across various tasks. We also demonstrate that $\rho\text{SIM}$ is orthogonal
to existing regularization methods, boosting their effectiveness. Moreover, our
results confirm that $\rho\text{SIM}$ effectively preserves semantic
information and enhances performance in dense-to-dense tasks, such as semantic
segmentation and image translation, as well as in non-visual domains including
audio classification and time series anomaly detection. The code is publicly
available at https://github.com/XiudingCai/SIM-pytorch.

</details>


### [48] [MAGIA: Sensing Per-Image Signals from Single-Round Averaged Gradients for Label-Inference-Free Gradient Inversion](https://arxiv.org/abs/2509.18170)
*Zhanting Zhou,Jinbo Wang,Zeqin Wu,Fengli Zhang*

Main category: cs.CV

TL;DR: 提出了一种新的梯度反演攻击方法MAGIA，可以在单轮平均梯度SAG机制下，从单个批次平均梯度中解耦每个样本的线索。


<details>
  <summary>Details</summary>
Motivation: 在单轮平均梯度SAG机制中，每个样本的线索都纠缠在单个批次平均梯度中，这使得梯度反演具有挑战性。

Method: MAGIA是一种基于动量的自适应校正梯度反演攻击，它通过探测随机数据子集来感知潜在的每张图像信号。MAGIA目标集成了两个核心创新：1）闭式组合重缩放，创建了可证明的更严格的优化边界；2）基于动量的整批和子集损失混合，以确保重建的鲁棒性。

Result: MAGIA显著优于先进方法，在大批量场景中实现了高保真多图像重建，而先前的工作失败了。

Conclusion: MAGIA在计算 footprint 与标准求解器相当的情况下，无需任何辅助信息即可实现这一目标。

Abstract: We study gradient inversion in the challenging single round averaged gradient
SAG regime where per sample cues are entangled within a single batch mean
gradient. We introduce MAGIA a momentum based adaptive correction on gradient
inversion attack a novel label inference free framework that senses latent per
image signals by probing random data subsets. MAGIA objective integrates two
core innovations 1 a closed form combinatorial rescaling that creates a
provably tighter optimization bound and 2 a momentum based mixing of whole
batch and subset losses to ensure reconstruction robustness. Extensive
experiments demonstrate that MAGIA significantly outperforms advanced methods
achieving high fidelity multi image reconstruction in large batch scenarios
where prior works fail. This is all accomplished with a computational footprint
comparable to standard solvers and without requiring any auxiliary information.

</details>


### [49] [Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR](https://arxiv.org/abs/2509.18174)
*Khalil Hennara,Muhammad Hreden,Mohamed Motasim Hamed,Ahmad Bastati,Zeina Aldallal,Sara Chrouf,Safwan AlModhayan*

Main category: cs.CV

TL;DR: 本文介绍了一个专门为阿拉伯语文档 OCR 微调的视觉语言模型 Baseer，它优于现有的解决方案。


<details>
  <summary>Details</summary>
Motivation: 尽管现代多模态大型语言模型 (MLLM) 提高了对高资源语言的文档理解，但它们在阿拉伯语方面的性能仍然有限。这项工作旨在提高阿拉伯语文档 OCR 的性能。

Method: 利用结合了合成文档和真实文档的大规模数据集，使用仅解码器微调策略训练 Baseer，以适应预训练的 MLLM，同时保留一般视觉特征。还提出了 Misraj-DocOCR，这是一个高质量、经过专家验证的基准，专为阿拉伯语 OCR 系统的严格评估而设计。

Result: Baseer 显著优于现有的开源和商业解决方案，实现了 0.25 的 WER，并在阿拉伯语文档 OCR 领域建立了新的最先进水平。

Conclusion: 研究结果强调了通用 MLLM 领域特定适应的优势，并为阿拉伯语等形态丰富的语言建立了一个强大的高精度 OCR 基线。

Abstract: Arabic document OCR remains a challenging task due to the language's cursive
script, diverse fonts, diacritics, and right-to-left orientation. While modern
Multimodal Large Language Models (MLLMs) have advanced document understanding
for high-resource languages, their performance on Arabic remains limited. In
this work, we introduce Baseer, a vision-language model fine- tuned
specifically for Arabic document OCR. Leveraging a large-scale dataset
combining synthetic and real-world documents, Baseer is trained using a
decoder-only fine-tuning strategy to adapt a pre-trained MLLM while preserving
general visual features. We also present Misraj-DocOCR, a high-quality,
expert-verified benchmark designed for rigorous evaluation of Arabic OCR
systems. Our experiments show that Baseer significantly outperforms existing
open-source and commercial solutions, achieving a WER of 0.25 and establishing
a new state-of-the-art in the domain of Arabic document OCR. Our results
highlight the benefits of domain-specific adaptation of general-purpose MLLMs
and establish a strong baseline for high-accuracy OCR on morphologically rich
languages like Arabic.

</details>


### [50] [A Deep Learning Approach for Spatio-Temporal Forecasting of InSAR Ground Deformation in Eastern Ireland](https://arxiv.org/abs/2509.18176)
*Wendong Yao,Saeed Azadnejad,Binhua Huang,Shane Donohue,Soumyabrata Dev*

Main category: cs.CV

TL;DR: 本文提出了一种新的深度学习框架，用于从稀疏的InSAR时间序列数据预测地面沉降。


<details>
  <summary>Details</summary>
Motivation: 监测地面沉降对城市基础设施稳定和减轻地质灾害至关重要。然而，从稀疏的干涉合成孔径雷达(InSAR)时间序列数据预测未来形变仍然是一个重大挑战。

Method: 该方法将稀疏点测量转换为密集时空张量，并设计了一种混合卷积神经网络和长短期记忆(CNN-LSTM)模型，用于学习空间模式和时间依赖性。

Result: 在爱尔兰东部的Sentinel-1数据上的实验结果表明，该模型提供了更准确和空间连贯的预测，优于Light Gradient Boosting Machine和LASSO回归等基线模型。

Conclusion: 研究结果证实了时空深度学习在高分辨率形变预测中的有效性和潜力。

Abstract: Monitoring ground displacement is crucial for urban infrastructure stability
and mitigating geological hazards. However, forecasting future deformation from
sparse Interferometric Synthetic Aperture Radar (InSAR) time-series data
remains a significant challenge. This paper introduces a novel deep learning
framework that transforms these sparse point measurements into a dense
spatio-temporal tensor. This methodological shift allows, for the first time,
the direct application of advanced computer vision architectures to this
forecasting problem. We design and implement a hybrid Convolutional Neural
Network and Long-Short Term Memory (CNN-LSTM) model, specifically engineered to
simultaneously learn spatial patterns and temporal dependencies from the
generated data tensor. The model's performance is benchmarked against powerful
machine learning baselines, Light Gradient Boosting Machine and LASSO
regression, using Sentinel-1 data from eastern Ireland. Results demonstrate
that the proposed architecture provides significantly more accurate and
spatially coherent forecasts, establishing a new performance benchmark for this
task. Furthermore, an interpretability analysis reveals that baseline models
often default to simplistic persistence patterns, highlighting the necessity of
our integrated spatio-temporal approach to capture the complex dynamics of
ground deformation. Our findings confirm the efficacy and potential of
spatio-temporal deep learning for high-resolution deformation forecasting.

</details>


### [51] [A Framework for Generating Artificial Datasets to Validate Absolute and Relative Position Concepts](https://arxiv.org/abs/2509.18177)
*George Corrêa de Araújo,Helena de Almeida Maia,Helio Pedrini*

Main category: cs.CV

TL;DR: 提出了一个名为 Scrapbook 的框架，用于生成大规模数据集，以探测 AI 模型学习到的概念。


<details>
  <summary>Details</summary>
Motivation: 旨在验证模型在处理更复杂任务之前，对物体识别、位置和属性识别等基本要素的理解。

Method: 通过生成大量关于单个概念的问题和广泛的语言变体的数据集。

Result: 实验表明，现有模型在识别和枚举对象方面表现出色，但在理解位置信息和回答具有额外约束的查询时面临挑战。MobileVLM-V2 模型显示出显着的答案不一致和看似合理的错误答案，而其他模型则表现出对肯定答案的偏见，并且难以回答涉及几何形状和位置信息的问题。

Conclusion: 该框架为生成多样化和全面的数据集提供了一种有价值的工具，可用于系统地评估和提高 AI 模型的性能。

Abstract: In this paper, we present the Scrapbook framework, a novel methodology
designed to generate extensive datasets for probing the learned concepts of
artificial intelligence (AI) models. The framework focuses on fundamental
concepts such as object recognition, absolute and relative positions, and
attribute identification. By generating datasets with a large number of
questions about individual concepts and a wide linguistic variation, the
Scrapbook framework aims to validate the model's understanding of these basic
elements before tackling more complex tasks. Our experimental findings reveal
that, while contemporary models demonstrate proficiency in recognizing and
enumerating objects, they encounter challenges in comprehending positional
information and addressing inquiries with additional constraints. Specifically,
the MobileVLM-V2 model showed significant answer disagreements and plausible
wrong answers, while other models exhibited a bias toward affirmative answers
and struggled with questions involving geometric shapes and positional
information, indicating areas for improvement in understanding and consistency.
The proposed framework offers a valuable instrument for generating diverse and
comprehensive datasets, which can be utilized to systematically assess and
enhance the performance of AI models.

</details>


### [52] [The Describe-Then-Generate Bottleneck: How VLM Descriptions Alter Image Generation Outcomes](https://arxiv.org/abs/2509.18179)
*Sai Varun Kodathala,Rakesh Vunnam*

Main category: cs.CV

TL;DR: 本文研究了视觉-语言-视觉管道中的信息损失问题，特别是“描述-生成”瓶颈。


<details>
  <summary>Details</summary>
Motivation: 量化当视觉内容通过文本中介时发生的退化仍然不足。

Method: 通过“描述-生成”管道生成150个图像对，并应用LPIPS、SSIM和颜色距离等指标来测量跨感知、结构和色度维度上的信息保存。

Result: 99.3%的样本表现出显著的感知退化，91.5%的样本表现出显著的结构信息损失。

Conclusion: “描述-生成”瓶颈是当代多模态系统中一个可测量且一致的限制。

Abstract: With the increasing integration of multimodal AI systems in creative
workflows, understanding information loss in vision-language-vision pipelines
has become important for evaluating system limitations. However, the
degradation that occurs when visual content passes through textual
intermediation remains poorly quantified. In this work, we provide empirical
analysis of the describe-then-generate bottleneck, where natural language
serves as an intermediate representation for visual information. We generated
150 image pairs through the describe-then-generate pipeline and applied
existing metrics (LPIPS, SSIM, and color distance) to measure information
preservation across perceptual, structural, and chromatic dimensions. Our
evaluation reveals that 99.3% of samples exhibit substantial perceptual
degradation and 91.5% demonstrate significant structural information loss,
providing empirical evidence that the describe-then-generate bottleneck
represents a measurable and consistent limitation in contemporary multimodal
systems.

</details>


### [53] [AI-Derived Structural Building Intelligence for Urban Resilience: An Application in Saint Vincent and the Grenadines](https://arxiv.org/abs/2509.18182)
*Isabelle Tingzon,Yoji Toriumi,Caroline Gevaert*

Main category: cs.CV

TL;DR: 本文提出了一种基于人工智能的工作流程，用于从高分辨率卫星图像中自动推断屋顶属性，以解决气候脆弱地区小岛屿发展中国家（SIDS）缺乏详细结构建筑信息的问题。


<details>
  <summary>Details</summary>
Motivation: 在城市复原力规划和降低灾害风险方面，详细的结构建筑信息至关重要。然而，许多气候脆弱地区的SIDS缺乏这些信息。

Method: 本文比较了地理空间基础模型与浅层分类器以及微调的深度学习模型在屋顶分类中的效用。此外，评估了纳入来自邻近SIDS的额外训练数据对改善模型性能的影响。

Result: 最好的模型在屋顶坡度和屋顶材料分类方面分别实现了0.88和0.83的F1分数。

Conclusion: 本研究旨在为SIDS提供利用人工智能和地球观测（EO）数据的新能力，以实现更高效、基于证据的城市治理。

Abstract: Detailed structural building information is used to estimate potential damage
from hazard events like cyclones, floods, and landslides, making them critical
for urban resilience planning and disaster risk reduction. However, such
information is often unavailable in many small island developing states (SIDS)
in climate-vulnerable regions like the Caribbean. To address this data gap, we
present an AI-driven workflow to automatically infer rooftop attributes from
high-resolution satellite imagery, with Saint Vincent and the Grenadines as our
case study. Here, we compare the utility of geospatial foundation models
combined with shallow classifiers against fine-tuned deep learning models for
rooftop classification. Furthermore, we assess the impact of incorporating
additional training data from neighboring SIDS to improve model performance.
Our best models achieve F1 scores of 0.88 and 0.83 for roof pitch and roof
material classification, respectively. Combined with local capacity building,
our work aims to provide SIDS with novel capabilities to harness AI and Earth
Observation (EO) data to enable more efficient, evidence-based urban
governance.

</details>


### [54] [VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation](https://arxiv.org/abs/2509.18183)
*Jinyue Bian,Zhaoxing Zhang,Zhengyu Liang,Shiwei Zheng,Shengtao Zhang,Rong Shen,Chen Yang,Anzhou Hou*

Main category: cs.CV

TL;DR: VLA模型在不同视角下的泛化性受限，本文提出VLA-LPAF模块以提高视角适应性。


<details>
  <summary>Details</summary>
Motivation: VLA模型在不同环境下的视觉特征存在显著差异，视角异构性限制了模型的泛化性。

Method: 提出轻量级模块VLA-LPAF，利用单视角图像微调，并在潜在空间融合多视角观测。

Result: RoboFlamingo-LPAF在CALVIN、LIBERO和自定义模拟基准测试中，任务成功率分别提高了8%、15%和30%。

Conclusion: 提出的RoboFlamingo-LPAF具有视角自适应性，并在真实世界任务中得到了验证。

Abstract: The Visual-Language-Action (VLA) models can follow text instructions
according to visual observations of the surrounding environment. This ability
to map multimodal inputs to actions is derived from the training of the VLA
model on extensive standard demonstrations. These visual observations captured
by third-personal global and in-wrist local cameras are inevitably varied in
number and perspective across different environments, resulting in significant
differences in the visual features. This perspective heterogeneity constrains
the generality of VLA models. In light of this, we first propose the
lightweight module VLA-LPAF to foster the perspective adaptivity of VLA models
using only 2D data. VLA-LPAF is finetuned using images from a single view and
fuses other multiview observations in the latent space, which effectively and
efficiently bridge the gap caused by perspective inconsistency. We instantiate
our VLA-LPAF framework with the VLA model RoboFlamingo to construct
RoboFlamingo-LPAF. Experiments show that RoboFlamingo-LPAF averagely achieves
around 8% task success rate improvement on CALVIN, 15% on LIBERO, and 30% on a
customized simulation benchmark. We also demonstrate the developed viewadaptive
characteristics of the proposed RoboFlamingo-LPAF through real-world tasks.

</details>


### [55] [URNet: Uncertainty-aware Refinement Network for Event-based Stereo Depth Estimation](https://arxiv.org/abs/2509.18184)
*Yifeng Cheng,Alois Knoll,Hu Cao*

Main category: cs.CV

TL;DR: URNet: Uncertainty-aware refinement network for event-based stereo depth estimation.


<details>
  <summary>Details</summary>
Motivation: Event cameras offer advantages over conventional cameras.

Method: Local-global refinement module and KL divergence-based uncertainty modeling.

Result: Outperforms state-of-the-art methods on the DSEC dataset.

Conclusion: URNet enhances prediction reliability in event-based stereo depth estimation.

Abstract: Event cameras provide high temporal resolution, high dynamic range, and low
latency, offering significant advantages over conventional frame-based cameras.
In this work, we introduce an uncertainty-aware refinement network called URNet
for event-based stereo depth estimation. Our approach features a local-global
refinement module that effectively captures fine-grained local details and
long-range global context. Additionally, we introduce a Kullback-Leibler (KL)
divergence-based uncertainty modeling method to enhance prediction reliability.
Extensive experiments on the DSEC dataset demonstrate that URNet consistently
outperforms state-of-the-art (SOTA) methods in both qualitative and
quantitative evaluations.

</details>


### [56] [Visionerves: Automatic and Reproducible Hybrid AI for Peripheral Nervous System Recognition Applied to Endometriosis Cases](https://arxiv.org/abs/2509.18185)
*Giammarco La Barbera,Enzo Bonnot,Thomas Isla,Juan Pablo de la Plata,Joy-Rose Dunoyer de Segonzac,Jennifer Attali,Cécile Lozach,Alexandre Bellucci,Louis Marcellin,Laure Fournier,Sabine Sarnacki,Pietro Gori,Isabelle Bloch*

Main category: cs.CV

TL;DR: Visionerves是一种用于识别周围神经系统的新型混合AI框架，它通过模糊空间关系编码解剖学知识，无需手动选择感兴趣区域。


<details>
  <summary>Details</summary>
Motivation: 子宫内膜异位症常导致慢性盆腔疼痛和可能的神经受累，但周围神经的成像仍然是一个挑战。

Method: 该流程包括两个阶段：(A)使用深度学习模型自动分割解剖结构，以及(B)通过符号空间推理进行tractography和神经识别。

Result: 在10名患有（已确诊或疑似）子宫内膜异位症的女性的腰骶丛中应用Visionerves，与标准tractography相比，Dice评分提高了25%，空间误差减少到5 mm以下。

Conclusion: 这种自动且可重复的方法能够进行详细的神经分析，并为子宫内膜异位症相关神经病变以及其他神经受累疾病的非侵入性诊断铺平了道路。

Abstract: Endometriosis often leads to chronic pelvic pain and possible nerve
involvement, yet imaging the peripheral nerves remains a challenge. We
introduce Visionerves, a novel hybrid AI framework for peripheral nervous
system recognition from multi-gradient DWI and morphological MRI data. Unlike
conventional tractography, Visionerves encodes anatomical knowledge through
fuzzy spatial relationships, removing the need for selection of manual ROIs.
The pipeline comprises two phases: (A) automatic segmentation of anatomical
structures using a deep learning model, and (B) tractography and nerve
recognition by symbolic spatial reasoning. Applied to the lumbosacral plexus in
10 women with (confirmed or suspected) endometriosis, Visionerves demonstrated
substantial improvements over standard tractography, with Dice score
improvements of up to 25% and spatial errors reduced to less than 5 mm. This
automatic and reproducible approach enables detailed nerve analysis and paves
the way for non-invasive diagnosis of endometriosis-related neuropathy, as well
as other conditions with nerve involvement.

</details>


### [57] [V-SenseDrive: A Privacy-Preserving Road Video and In-Vehicle Sensor Fusion Framework for Road Safety & Driver Behaviour Modelling](https://arxiv.org/abs/2509.18187)
*Muhammad Naveed,Nazia Perwaiz,Sidra Sultana,Mohaira Ahmad,Muhammad Moazam Fraz*

Main category: cs.CV

TL;DR: 提出了V-SenseDrive数据集，用于解决在巴基斯坦等道路状况复杂的国家中，驾驶行为检测和道路安全问题。


<details>
  <summary>Details</summary>
Motivation: 现有数据集主要来自发达国家，无法代表新兴经济体的行为多样性，且驾驶员面部记录侵犯隐私。

Method: 通过智能手机收集惯性、GPS传感器数据和同步道路视频，记录正常、激进和危险三种驾驶行为。

Result: 构建了一个包含原始数据、处理后数据和语义层的数据集，适用于驾驶行为分类、交通安全分析和ADAS开发。

Conclusion: V-SenseDrive填补了全球驾驶行为数据集的空白，为情境感知智能交通解决方案奠定了基础。

Abstract: Road traffic accidents remain a major public health challenge, particularly
in countries with heterogeneous road conditions, mixed traffic flow, and
variable driving discipline, such as Pakistan. Reliable detection of unsafe
driving behaviours is a prerequisite for improving road safety, enabling
advanced driver assistance systems (ADAS), and supporting data driven decisions
in insurance and fleet management. Most of existing datasets originate from the
developed countries with limited representation of the behavioural diversity
observed in emerging economies and the driver's face recording voilates the
privacy preservation. We present V-SenseDrive, the first privacy-preserving
multimodal driver behaviour dataset collected entirely within the Pakistani
driving environment. V-SenseDrive combines smartphone based inertial and GPS
sensor data with synchronized road facing video to record three target driving
behaviours (normal, aggressive, and risky) on multiple types of roads,
including urban arterials, secondary roads, and motorways. Data was gathered
using a custom Android application designed to capture high frequency
accelerometer, gyroscope, and GPS streams alongside continuous video, with all
sources precisely time aligned to enable multimodal analysis. The focus of this
work is on the data acquisition process, covering participant selection,
driving scenarios, environmental considerations, and sensor video
synchronization techniques. The dataset is structured into raw, processed, and
semantic layers, ensuring adaptability for future research in driver behaviour
classification, traffic safety analysis, and ADAS development. By representing
real world driving in Pakistan, V-SenseDrive fills a critical gap in the global
landscape of driver behaviour datasets and lays the groundwork for context
aware intelligent transportation solutions.

</details>


### [58] [Qianfan-VL: Domain-Enhanced Universal Vision-Language Models](https://arxiv.org/abs/2509.18189)
*Daxiang Dong,Mingming Zheng,Dong Xu,Bairong Zhuang,Wenyu Zhang,Chunhua Luo,Haoran Wang,Zijian Zhao,Jie Li,Yuxuan Li,Hanjun Zhong,Mengyue Liu,Jieting Chen,Shupeng Li,Lun Tian,Yaping Feng,Xin Li,Donggang Jiang,Yong Chen,Yehua Xu,Duohao Qin,Chen Feng,Dan Wang,Henghua Zhang,Jingjing Ha,Jinhui He,Yanfeng Zhai,Chengxin Zheng,Jiayi Mao,Jiacheng Chen,Ruchang Yao,Ziye Yuan,Jianmin Wu,Guangjun Xie,Dou Shen*

Main category: cs.CV

TL;DR: Qianfan-VL is a series of multimodal large language models (3B-70B) excelling through domain enhancement.


<details>
  <summary>Details</summary>
Motivation: Enhance domain-specific capabilities while maintaining strong general performance.

Method: Multi-stage progressive training and high-precision data synthesis pipelines.

Result: State-of-the-art performance on CCBench, SEEDBench IMG, ScienceQA, MMStar, OCRBench, DocVQA, and MathVista. Achieved over 90% scaling efficiency on 5000 chips.

Conclusion: Establishes an effective methodology for developing domain-enhanced multimodal models for diverse enterprise deployment scenarios.

Abstract: We present Qianfan-VL, a series of multimodal large language models ranging
from 3B to 70B parameters, achieving state-of-the-art performance through
innovative domain enhancement techniques. Our approach employs multi-stage
progressive training and high-precision data synthesis pipelines, which prove
to be critical technologies for enhancing domain-specific capabilities while
maintaining strong general performance. Qianfan-VL achieves comparable results
to leading open-source models on general benchmarks, with state-of-the-art
performance on benchmarks such as CCBench, SEEDBench IMG, ScienceQA, and
MMStar. The domain enhancement strategy delivers significant advantages in OCR
and document understanding, validated on both public benchmarks (OCRBench 873,
DocVQA 94.75%) and in-house evaluations. Notably, Qianfan-VL-8B and 70B
variants incorporate long chain-of-thought capabilities, demonstrating superior
performance on mathematical reasoning (MathVista 78.6%) and logical inference
tasks. All models are trained entirely on Baidu's Kunlun P800 chips, validating
the capability of large-scale AI infrastructure to train SOTA-level multimodal
models with over 90% scaling efficiency on 5000 chips for a single task. This
work establishes an effective methodology for developing domain-enhanced
multimodal models suitable for diverse enterprise deployment scenarios.

</details>


### [59] [HazeFlow: Revisit Haze Physical Model as ODE and Non-Homogeneous Haze Generation for Real-World Dehazing](https://arxiv.org/abs/2509.18190)
*Junseong Shin,Seungwoo Chung,Yunjeong Yang,Tae Hyun Kim*

Main category: cs.CV

TL;DR: HazeFlow: An ODE-based framework for real-world image dehazing, enhanced by a Markov Chain Brownian Motion (MCBM) haze generation method.


<details>
  <summary>Details</summary>
Motivation: Deep learning dehazing methods struggle with real-world generalization due to lack of paired real data and the limitations of the Atmospheric Scattering Model (ASM).

Method: Reformulates ASM as an ODE and learns an optimal trajectory to map hazy to clean images. Introduces MCBM for non-homogeneous haze generation.

Result: Achieves state-of-the-art performance on real-world dehazing benchmarks.

Conclusion: HazeFlow effectively addresses real-world dehazing by combining physics-grounded learning with a novel haze simulation technique.

Abstract: Dehazing involves removing haze or fog from images to restore clarity and
improve visibility by estimating atmospheric scattering effects. While deep
learning methods show promise, the lack of paired real-world training data and
the resulting domain gap hinder generalization to real-world scenarios. In this
context, physics-grounded learning becomes crucial; however, traditional
methods based on the Atmospheric Scattering Model (ASM) often fall short in
handling real-world complexities and diverse haze patterns. To solve this
problem, we propose HazeFlow, a novel ODE-based framework that reformulates ASM
as an ordinary differential equation (ODE). Inspired by Rectified Flow (RF),
HazeFlow learns an optimal ODE trajectory to map hazy images to clean ones,
enhancing real-world dehazing performance with only a single inference step.
Additionally, we introduce a non-homogeneous haze generation method using
Markov Chain Brownian Motion (MCBM) to address the scarcity of paired
real-world data. By simulating realistic haze patterns through MCBM, we enhance
the adaptability of HazeFlow to diverse real-world scenarios. Through extensive
experiments, we demonstrate that HazeFlow achieves state-of-the-art performance
across various real-world dehazing benchmark datasets.

</details>


### [60] [TinyEcoWeedNet: Edge Efficient Real-Time Aerial Agricultural Weed Detection](https://arxiv.org/abs/2509.18193)
*Omar H. Khater,Abdul Jabbar Siddiqui,Aiman El-Maleh,M. Shamim Hossain*

Main category: cs.CV

TL;DR: 本文提出了一种压缩版的EcoWeedNet，通过结构化通道剪枝、量化感知训练（QAT）以及在Jetson Orin Nano上使用NVIDIA的TensorRT进行加速，从而实现在资源有限的边缘设备上部署深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 在农业中部署深度学习模型很困难，因为边缘设备资源有限。

Method: 使用结构化通道剪枝、量化感知训练（QAT）和NVIDIA的TensorRT加速技术压缩EcoWeedNet模型。

Result: 模型尺寸减少高达68.5%，计算量减少3.2 GFLOPs，FP16下的推理速度达到184 FPS，比基线快28.7%。在CottonWeedDet12数据集上，剪枝率为39.5%的EcoWeedNet优于YOLO11n和YOLO12n（仅剪枝20%），实现了83.7%的精度，77.5%的召回率和85.9%的mAP50。

Conclusion: 证明该方法对于精准农业既高效又有效。

Abstract: Deploying deep learning models in agriculture is difficult because edge
devices have limited resources, but this work presents a compressed version of
EcoWeedNet using structured channel pruning, quantization-aware training (QAT),
and acceleration with NVIDIA's TensorRT on the Jetson Orin Nano. Despite the
challenges of pruning complex architectures with residual shortcuts, attention
mechanisms, concatenations, and CSP blocks, the model size was reduced by up to
68.5% and computations by 3.2 GFLOPs, while inference speed reached 184 FPS at
FP16, 28.7% faster than the baseline. On the CottonWeedDet12 dataset, the
pruned EcoWeedNet with a 39.5% pruning ratio outperformed YOLO11n and YOLO12n
(with only 20% pruning), achieving 83.7% precision, 77.5% recall, and 85.9%
mAP50, proving it to be both efficient and effective for precision agriculture.

</details>


### [61] [Learning Contrastive Multimodal Fusion with Improved Modality Dropout for Disease Detection and Prediction](https://arxiv.org/abs/2509.18284)
*Yi Gu,Kuniaki Saito,Jiaxin Ma*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态学习框架，该框架集成了增强的模态 dropout 和对比学习，以解决模态不平衡和缺失等实际限制。


<details>
  <summary>Details</summary>
Motivation: 医学诊断越来越多地利用多模态数据，机器学习模型需要有效地融合异构信息，同时对缺失模态保持鲁棒性。

Method: 该方法引入了可学习的模态 tokens，以改进对模态缺失的感知融合，并通过融合的多模态表示增强传统的单模态对比目标。

Result: 在用于疾病检测和预测任务的大规模临床数据集上验证了该框架，包括视觉和表格模态。实验结果表明，该方法实现了最先进的性能，特别是在具有挑战性和实际的场景中，在这些场景中，只有一种模态可用。此外，通过与最近的 CT 基础模型的成功集成，展示了其适应性。

Conclusion: 研究结果强调了该方法在多模态学习中的有效性、效率和通用性，提供了一种可扩展、低成本的解决方案，具有在现实世界临床应用中的巨大潜力。

Abstract: As medical diagnoses increasingly leverage multimodal data, machine learning
models are expected to effectively fuse heterogeneous information while
remaining robust to missing modalities. In this work, we propose a novel
multimodal learning framework that integrates enhanced modalities dropout and
contrastive learning to address real-world limitations such as modality
imbalance and missingness. Our approach introduces learnable modality tokens
for improving missingness-aware fusion of modalities and augments conventional
unimodal contrastive objectives with fused multimodal representations. We
validate our framework on large-scale clinical datasets for disease detection
and prediction tasks, encompassing both visual and tabular modalities.
Experimental results demonstrate that our method achieves state-of-the-art
performance, particularly in challenging and practical scenarios where only a
single modality is available. Furthermore, we show its adaptability through
successful integration with a recent CT foundation model. Our findings
highlight the effectiveness, efficiency, and generalizability of our approach
for multimodal learning, offering a scalable, low-cost solution with
significant potential for real-world clinical applications. The code is
available at https://github.com/omron-sinicx/medical-modality-dropout.

</details>


### [62] [Rethinking Pulmonary Embolism Segmentation: A Study of Current Approaches and Challenges with an Open Weight Model](https://arxiv.org/abs/2509.18308)
*Yixin Zhang,Ryan Chamberlain,Lawrance Ngo,Kevin Kramer,Maciej A. Mazurowski*

Main category: cs.CV

TL;DR: 本研究整理了一个包含 490 个 CTPA 扫描的密集注释内部数据集，并在统一的测试框架下，系统地评估了来自 CNN 和 Vision Transformer (ViT) 系列的九个广泛使用的分割架构，这些架构使用预训练或随机权重进行初始化，作为性能审计。


<details>
  <summary>Details</summary>
Motivation: 研究目的是评估不同的分割架构在肺栓塞 (PE) 分割任务中的性能，并深入了解它们的优势和局限性。

Method: 使用内部数据集系统地评估了九个广泛使用的分割架构，包括 CNN 和 Vision Transformer (ViT) 系列，使用预训练或随机权重进行初始化。

Result: (1) 带有 ResNet 编码器的 3D U-Net 仍然是 PE 分割的一种非常有效的架构；(2) 3D 模型特别适合这项任务，因为栓塞的形态学特征；(3) 基于 CNN 的模型通常比它们在 PE 分割中基于 ViT 的模型产生更好的性能；(4) 基于分类的预训练，即使在大型 PE 数据集上，与从头开始训练相比，也会对分割性能产生不利影响；(5) 当在相同的数据上训练时，不同的模型架构显示出高度一致的分割性能模式；(6) 虽然中心和大栓塞可以以令人满意的精度分割，但由于任务复杂性和高质量数据集的稀缺性，远端栓塞仍然具有挑战性。

Conclusion: 最佳模型在分割方面实现了 0.7131 的平均 Dice 分数。它从 60 个内部测试扫描中检测到 181 个栓塞，有 49 个假阳性和 28 个假阴性。它的通用性在公共数据集上得到了进一步验证。

Abstract: In this study, we curated a densely annotated in-house dataset comprising 490
CTPA scans. Using this dataset, we systematically evaluated nine widely used
segmentation architectures from both the CNN and Vision Transformer (ViT)
families, initialized with either pretrained or random weights, under a unified
testing framework as a performance audit. Our study leads to several important
observations: (1) 3D U-Net with a ResNet encoder remains a highly effective
architecture for PE segmentation; (2) 3D models are particularly well-suited to
this task given the morphological characteristics of emboli; (3) CNN-based
models generally yield superior performance compared to their ViT-based
counterparts in PE segmentation; (4) classification-based pretraining, even on
large PE datasets, can adversely impact segmentation performance compared to
training from scratch, suggesting that PE classification and segmentation may
rely on different sets of discriminative features; (5) different model
architectures show a highly consistent pattern of segmentation performance when
trained on the same data; and (6) while central and large emboli can be
segmented with satisfactory accuracy, distal emboli remain challenging due to
both task complexity and the scarcity of high-quality datasets. Besides these
findings, our best-performing model achieves a mean Dice score of 0.7131 for
segmentation. It detects 181 emboli with 49 false positives and 28 false
negatives from 60 in-house testing scans. Its generalizability is further
validated on public datasets.

</details>


### [63] [Improving Handshape Representations for Sign Language Processing: A Graph Neural Network Approach](https://arxiv.org/abs/2509.18309)
*Alessa Carbo,Eric Nalisnick*

Main category: cs.CV

TL;DR: 本文提出了一种新的图神经网络，用于从静态手势配置中分离时间动态。


<details>
  <summary>Details</summary>
Motivation: 现有的计算方法很少明确地对手势进行建模，限制了识别准确率和语言分析。

Method: 该方法结合了解剖结构的图结构和对比学习，以解决在手势识别中的关键挑战，包括细微的类间区分和时间变化。

Result: 在签名序列的结构化手势识别方面，建立了第一个基准，在 37 个手势类别中实现了 46% 的准确率（基线方法实现了 25%）。

Conclusion: 该研究为手势识别建立了一个新的基准，并提出了一种新的图神经网络方法，该方法在手势识别方面取得了显著的成果。

Abstract: Handshapes serve a fundamental phonological role in signed languages, with
American Sign Language employing approximately 50 distinct shapes.
However,computational approaches rarely model handshapes explicitly, limiting
both recognition accuracy and linguistic analysis.We introduce a novel graph
neural network that separates temporal dynamics from static handshape
configurations. Our approach combines anatomically-informed graph structures
with contrastive learning to address key challenges in handshape recognition,
including subtle interclass distinctions and temporal variations. We establish
the first benchmark for structured handshape recognition in signing sequences,
achieving 46% accuracy across 37 handshape classes (with baseline methods
achieving 25%).

</details>


### [64] [Influence of Classification Task and Distribution Shift Type on OOD Detection in Fetal Ultrasound](https://arxiv.org/abs/2509.18326)
*Chun Kit Wong,Anders N. Christensen,Cosmin I. Bercea,Julia A. Schnabel,Martin G. Tolsgaard,Aasa Feragen*

Main category: cs.CV

TL;DR: 可靠的OOD检测对于深度学习模型在胎儿超声中的安全部署非常重要。本文研究了分类任务本身对OOD检测的影响，通过实验发现OOD检测性能随任务变化显著，最佳任务取决于ID-OOD标准。


<details>
  <summary>Details</summary>
Motivation: 在异构图像特征和临床环境中，深度学习模型需要可靠的OOD检测来保证安全部署。现有的研究主要集中在不确定性量化方法上，但是分类任务本身的影响没有被充分研究。

Method: 通过八种不确定性量化方法在四个分类任务上进行实验，来研究分类任务对OOD检测的影响。

Result: OOD检测性能随任务变化显著，最佳任务取决于ID-OOD标准，即OOD样本是由于图像特征偏移还是解剖特征偏移。此外，优秀的OOD检测并不保证最佳的拒绝预测。

Conclusion: 任务选择和不确定性策略需要与医学图像分析中特定的下游应用对齐。

Abstract: Reliable out-of-distribution (OOD) detection is important for safe deployment
of deep learning models in fetal ultrasound amidst heterogeneous image
characteristics and clinical settings. OOD detection relies on estimating a
classification model's uncertainty, which should increase for OOD samples.
While existing research has largely focused on uncertainty quantification
methods, this work investigates the impact of the classification task itself.
Through experiments with eight uncertainty quantification methods across four
classification tasks, we demonstrate that OOD detection performance
significantly varies with the task, and that the best task depends on the
defined ID-OOD criteria; specifically, whether the OOD sample is due to: i) an
image characteristic shift or ii) an anatomical feature shift. Furthermore, we
reveal that superior OOD detection does not guarantee optimal abstained
prediction, underscoring the necessity to align task selection and uncertainty
strategies with the specific downstream application in medical image analysis.

</details>


### [65] [OrthoLoC: UAV 6-DoF Localization and Calibration Using Orthographic Geodata](https://arxiv.org/abs/2509.18350)
*Oussema Dhaouadi,Riccardo Marin,Johannes Meier,Jacques Kaiser,Daniel Cremers*

Main category: cs.CV

TL;DR: 本文提出了OrthoLoC，一个包含来自德国和美国的16,425张无人机图像的大规模数据集，用于解决在资源受限情况下，利用正射地理数据进行精确视觉定位的问题。


<details>
  <summary>Details</summary>
Motivation: 在许多场景中，视觉定位系统需要在资源有限的情况下进行高精度定位，而大型图像数据库或重型3D模型并不实用。正射地理数据作为一种轻量级的替代方案，越来越容易通过政府机构免费获取，但很少受到关注。

Method: 本文构建了OrthoLoC数据集，该数据集包含多种模态，并针对无人机图像和地理空间数据之间的域偏移问题进行了处理。此外，本文还提出了一种名为AdHoP的改进技术，可以与任何特征匹配器集成，从而提高匹配效果。

Result: 通过全面的评估，本文研究了域偏移、数据分辨率和共视性对定位精度的影响。AdHoP技术可以将匹配效果提高高达95%，并将平移误差降低高达63%。

Conclusion: 本文填补了利用正射地理数据进行视觉定位的研究空白，并为未来的研究提供了基准数据集和改进技术。

Abstract: Accurate visual localization from aerial views is a fundamental problem with
applications in mapping, large-area inspection, and search-and-rescue
operations. In many scenarios, these systems require high-precision
localization while operating with limited resources (e.g., no internet
connection or GNSS/GPS support), making large image databases or heavy 3D
models impractical. Surprisingly, little attention has been given to leveraging
orthographic geodata as an alternative paradigm, which is lightweight and
increasingly available through free releases by governmental authorities (e.g.,
the European Union). To fill this gap, we propose OrthoLoC, the first
large-scale dataset comprising 16,425 UAV images from Germany and the United
States with multiple modalities. The dataset addresses domain shifts between
UAV imagery and geospatial data. Its paired structure enables fair benchmarking
of existing solutions by decoupling image retrieval from feature matching,
allowing isolated evaluation of localization and calibration performance.
Through comprehensive evaluation, we examine the impact of domain shifts, data
resolutions, and covisibility on localization accuracy. Finally, we introduce a
refinement technique called AdHoP, which can be integrated with any feature
matcher, improving matching by up to 95% and reducing translation error by up
to 63%. The dataset and code are available at:
https://deepscenario.github.io/OrthoLoC.

</details>


### [66] [A Single Image Is All You Need: Zero-Shot Anomaly Localization Without Training Data](https://arxiv.org/abs/2509.18354)
*Mehrdad Moradi,Shengzhe Chen,Hao Yan,Kamran Paynabar*

Main category: cs.CV

TL;DR: 提出了一种名为SSDnet的单图像异常定位方法，该方法利用卷积神经网络的归纳偏置，灵感来自深度图像先验(DIP)，用于解决零样本异常检测问题。


<details>
  <summary>Details</summary>
Motivation: 在许多现实场景中，通常无法获得训练数据，只能提供测试图像本身。因此，本文旨在解决零样本环境下的图像异常检测问题。

Method: 该方法设计了一个基于patch的训练框架，其中输入图像直接输入网络进行自重构，而不是像DIP那样将随机噪声映射到图像。为了避免模型简单地学习恒等映射，采用了掩蔽、patch shuffling和小高斯噪声。此外，还使用基于内积相似性的感知损失来捕获像素保真度之外的结构。

Result: SSDnet在MVTec-AD上实现了0.99 AUROC和0.60 AUPRC，在fabric数据集上实现了0.98 AUROC和0.67 AUPRC，优于现有技术水平的方法。

Conclusion: 该方法不需要外部训练数据、标签或参考，并且在存在噪声或缺失像素的情况下仍然保持稳健。

Abstract: Anomaly detection in images is typically addressed by learning from
collections of training data or relying on reference samples. In many
real-world scenarios, however, such training data may be unavailable, and only
the test image itself is provided. We address this zero-shot setting by
proposing a single-image anomaly localization method that leverages the
inductive bias of convolutional neural networks, inspired by Deep Image Prior
(DIP). Our method is named Single Shot Decomposition Network (SSDnet). Our key
assumption is that natural images often exhibit unified textures and patterns,
and that anomalies manifest as localized deviations from these repetitive or
stochastic patterns. To learn the deep image prior, we design a patch-based
training framework where the input image is fed directly into the network for
self-reconstruction, rather than mapping random noise to the image as done in
DIP. To avoid the model simply learning an identity mapping, we apply masking,
patch shuffling, and small Gaussian noise. In addition, we use a perceptual
loss based on inner-product similarity to capture structure beyond pixel
fidelity. Our approach needs no external training data, labels, or references,
and remains robust in the presence of noise or missing pixels. SSDnet achieves
0.99 AUROC and 0.60 AUPRC on MVTec-AD and 0.98 AUROC and 0.67 AUPRC on the
fabric dataset, outperforming state-of-the-art methods. The implementation code
will be released at https://github.com/mehrdadmoradi124/SSDnet

</details>


### [67] [Align Where the Words Look: Cross-Attention-Guided Patch Alignment with Contrastive and Transport Regularization for Bengali Captioning](https://arxiv.org/abs/2509.18369)
*Riad Ahmed Anonto,Sardar Md. Saffat Zabin,M. Saifur Rahman*

Main category: cs.CV

TL;DR: 该论文提出了一种针对低资源语言的视觉-语言模型，通过计算感知的孟加拉语图像描述流程来提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型在低资源语言中表现不佳，因为缺乏配对数据、翻译偏差以及以英语为中心的预训练忽略了目标语言的语义。

Method: 该方法使用冻结的MaxViT提取视觉特征，孟加拉语原生的mBART-50解码，并通过三损失目标进行优化：Patch-Alignment Loss (PAL)、InfoNCE和Sinkhorn-based OT。

Result: 在Flickr30k-1k和MSCOCO-1k数据集上取得了显著的性能提升，缩小了真实图像和合成图像之间的差距。

Conclusion: 该方法有效地提高了低资源语言中视觉-语言模型的grounding能力，减少了虚假匹配。

Abstract: Grounding vision--language models in low-resource languages remains
challenging, as they often produce fluent text about the wrong objects. This
stems from scarce paired data, translation pivots that break alignment, and
English-centric pretraining that ignores target-language semantics. We address
this with a compute-aware Bengali captioning pipeline trained on LaBSE-verified
EN--BN pairs and 110k bilingual-prompted synthetic images. A frozen MaxViT
yields stable visual patches, a Bengali-native mBART-50 decodes, and a
lightweight bridge links the modalities. Our core novelty is a tri-loss
objective: Patch-Alignment Loss (PAL) aligns real and synthetic patch
descriptors using decoder cross-attention, InfoNCE enforces global
real--synthetic separation, and Sinkhorn-based OT ensures balanced fine-grained
patch correspondence. This PAL+InfoNCE+OT synergy improves grounding, reduces
spurious matches, and drives strong gains on Flickr30k-1k (BLEU-4 12.29, METEOR
27.98, BERTScore-F1 71.20) and MSCOCO-1k (BLEU-4 12.00, METEOR 28.14,
BERTScore-F1 75.40), outperforming strong CE baselines and narrowing the
real--synthetic centroid gap by 41%.

</details>


### [68] [TinyBEV: Cross Modal Knowledge Distillation for Efficient Multi Task Bird's Eye View Perception and Planning](https://arxiv.org/abs/2509.18372)
*Reeshad Khan,John Gauch*

Main category: cs.CV

TL;DR: TinyBEV是一个仅使用摄像头的BEV框架，它将大型规划导向教师模型UniAD的完整功能提炼成一个紧凑的实时学生模型。


<details>
  <summary>Details</summary>
Motivation: 先前的仅使用摄像头的BEV框架计算效率不高，TinyBEV旨在解决这个问题。

Method: 使用模型无关的多阶段蒸馏策略，结合特征级别、输出级别和自适应区域感知监督，将高容量多模态知识有效地转移到轻量级BEV表示。

Result: 在nuScenes上，Tiny-BEV在检测方面实现了39.0 mAP，在运动预测方面实现了1.08 minADE，碰撞率为0.32，同时运行速度提高了5倍（11 FPS），并且只需要摄像头输入。

Conclusion: TinyBEV证明了全栈驾驶智能可以保留在资源受限的环境中，弥合了大规模多模态感知规划模型和可部署的实时自主性之间的差距。

Abstract: We present TinyBEV, a unified, camera only Bird's Eye View (BEV) framework
that distills the full-stack capabilities of a large planning-oriented teacher
(UniAD [19]) into a compact, real-time student model. Unlike prior efficient
camera only baselines such as VAD[23] and VADv2[7], TinyBEV supports the
complete autonomy stack 3D detection, HD-map segmentation, motion forecasting,
occupancy prediction, and goal-directed planning within a streamlined
28M-parameter backbone, achieving a 78% reduction in parameters over UniAD
[19]. Our model-agnostic, multi-stage distillation strategy combines
feature-level, output-level, and adaptive region-aware supervision to
effectively transfer high-capacity multi-modal knowledge to a lightweight BEV
representation. On nuScenes[4], Tiny-BEV achieves 39.0 mAP for detection, 1.08
minADE for motion forecasting, and a 0.32 collision rate, while running 5x
faster (11 FPS) and requiring only camera input. These results demonstrate that
full-stack driving intelligence can be retained in resource-constrained
settings, bridging the gap between large-scale, multi-modal perception-planning
models and deployment-ready real-time autonomy.

</details>


### [69] [BlurBall: Joint Ball and Motion Blur Estimation for Table Tennis Ball Tracking](https://arxiv.org/abs/2509.18387)
*Thomas Gossard,Filip Radovic,Andreas Ziegler,Andrea Zell*

Main category: cs.CV

TL;DR: 本文提出了一种新的运动模糊球检测数据集和模型，通过将球标记在模糊条纹的中心并显式注释模糊属性，提高了检测性能和轨迹预测的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的标记方法将球标记在模糊的前沿，引入了不对称性，忽略了与速度相关的运动线索。这给快速移动物体的检测带来了挑战，特别是在球拍运动中。

Method: 本文提出了一种新的标记策略，将球放置在模糊条纹的中心，并明确注释模糊属性。此外，还引入了一个名为BlurBall的模型，该模型联合估计球的位置和运动模糊属性，并结合了多帧输入的注意力机制。

Result: 新的标记方法和BlurBall模型在球检测方面取得了最新的成果，提高了检测精度。

Conclusion: 利用模糊不仅提高了检测精度，而且使轨迹预测更加可靠，有益于实时体育分析。

Abstract: Motion blur reduces the clarity of fast-moving objects, posing challenges for
detection systems, especially in racket sports, where balls often appear as
streaks rather than distinct points. Existing labeling conventions mark the
ball at the leading edge of the blur, introducing asymmetry and ignoring
valuable motion cues correlated with velocity. This paper introduces a new
labeling strategy that places the ball at the center of the blur streak and
explicitly annotates blur attributes. Using this convention, we release a new
table tennis ball detection dataset. We demonstrate that this labeling approach
consistently enhances detection performance across various models. Furthermore,
we introduce BlurBall, a model that jointly estimates ball position and motion
blur attributes. By incorporating attention mechanisms such as
Squeeze-and-Excitation over multi-frame inputs, we achieve state-of-the-art
results in ball detection. Leveraging blur not only improves detection accuracy
but also enables more reliable trajectory prediction, benefiting real-time
sports analytics.

</details>


### [70] [MVP: Motion Vector Propagation for Zero-Shot Video Object Detection](https://arxiv.org/abs/2509.18388)
*Binhua Huang,Ni Wang,Wendong Yao,Soumyabrata Dev*

Main category: cs.CV

TL;DR: 提出了一种无需训练的流水线，通过在固定间隔的关键帧上调用 OWLv2 并使用压缩域运动矢量 (MV) 将检测结果传播到中间帧来运行大型开放词汇 (Open-vocab) 检测器。


<details>
  <summary>Details</summary>
Motivation: 在每个视频帧上运行大型开放词汇检测器是准确的，但成本很高。

Method: 该方法在固定间隔的关键帧上调用 OWLv2，并使用压缩域运动矢量将检测结果传播到中间帧。运动矢量的简单 3x3 网格聚合提供平移和均匀比例更新，并辅以面积增长检查和可选的单类切换。该方法不需要标签，不需要微调，并且对所有开放词汇方法使用相同的提示列表。

Result: 在 ILSVRC2015-VID（验证数据集）上，我们的方法 (MVP) 达到 mAP@0.5=0.609 和 mAP@[0.5:0.95]=0.316。在宽松的 intersection-over-union (IoU) 阈值下，它仍然接近逐帧 OWLv2-Large（在 0.2/0.3 时分别为 0.747/0.721，而后者为 0.784/0.780），这反映了粗略的定位在很大程度上得到了保留。

Conclusion: 压缩域传播是减少检测器调用的同时保持视频中强大的零样本覆盖率的一种实用方法。

Abstract: Running a large open-vocabulary (Open-vocab) detector on every video frame is
accurate but expensive. We introduce a training-free pipeline that invokes
OWLv2 only on fixed-interval keyframes and propagates detections to
intermediate frames using compressed-domain motion vectors (MV). A simple 3x3
grid aggregation of motion vectors provides translation and uniform-scale
updates, augmented with an area-growth check and an optional single-class
switch. The method requires no labels, no fine-tuning, and uses the same prompt
list for all open-vocabulary methods. On ILSVRC2015-VID (validation dataset),
our approach (MVP) attains mAP@0.5=0.609 and mAP@[0.5:0.95]=0.316. At loose
intersection-over-union (IoU) thresholds it remains close to framewise
OWLv2-Large (0.747/0.721 at 0.2/0.3 versus 0.784/0.780), reflecting that coarse
localization is largely preserved. Under the same keyframe schedule, MVP
outperforms tracker-based propagation (MOSSE, KCF, CSRT) at mAP@0.5. A
supervised reference (YOLOv12x) reaches 0.631 at mAP@0.5 but requires labeled
training, whereas our method remains label-free and open-vocabulary. These
results indicate that compressed-domain propagation is a practical way to
reduce detector invocations while keeping strong zero-shot coverage in videos.
Our code and models are available at https://github.com/microa/MVP.

</details>


### [71] [Improving the color accuracy of lighting estimation models](https://arxiv.org/abs/2509.18390)
*Zitian Zhang,Joshua Urban Davis,Jeanne Phuong Anh Vu,Jiangtao Kuang,Jean-François Lalonde*

Main category: cs.CV

TL;DR: 本研究探讨了单张图像高动态范围(HDR)光照估计的色彩鲁棒性问题，这对于增强现实(AR)应用至关重要。


<details>
  <summary>Details</summary>
Motivation: 现有光照估计方法在色彩方面存在不足，而色彩是实现视觉真实感的关键因素。研究着重关注色彩，并探究简单的色彩适应技术是否可以提高现有模型的色彩准确性。

Method: 使用包含多种照明颜色的新型HDR数据集，系统地评估了几种颜色适应策略。其中，使用预训练的白平衡网络预处理输入图像。

Result: 结果表明，使用预训练的白平衡网络预处理输入图像可以提高色彩鲁棒性，优于所有测试场景中的其他策略。而且，这种方法不需要重新训练光照估计模型。

Conclusion: 通过将该技术应用于最近文献中的三种最先进的光照估计方法，进一步验证了该发现的普遍性。

Abstract: Advances in high dynamic range (HDR) lighting estimation from a single image
have opened new possibilities for augmented reality (AR) applications.
Predicting complex lighting environments from a single input image allows for
the realistic rendering and compositing of virtual objects. In this work, we
investigate the color robustness of such methods -- an often overlooked yet
critical factor for achieving visual realism. While most evaluations conflate
color with other lighting attributes (e.g., intensity, direction), we isolate
color as the primary variable of interest. Rather than introducing a new
lighting estimation algorithm, we explore whether simple adaptation techniques
can enhance the color accuracy of existing models. Using a novel HDR dataset
featuring diverse lighting colors, we systematically evaluate several
adaptation strategies. Our results show that preprocessing the input image with
a pre-trained white balance network improves color robustness, outperforming
other strategies across all tested scenarios. Notably, this approach requires
no retraining of the lighting estimation model. We further validate the
generality of this finding by applying the technique to three state-of-the-art
lighting estimation methods from recent literature.

</details>


### [72] [Check Field Detection Agent (CFD-Agent) using Multimodal Large Language and Vision Language Models](https://arxiv.org/abs/2509.18405)
*Sourav Halder,Jinjun Tong,Xinyu Wu*

Main category: cs.CV

TL;DR: 提出了一种新的、无需训练的支票字段自动检测框架，利用视觉语言模型 (VLM) 和多模态大型语言模型 (MLLM) 实现支票组件的零样本检测。


<details>
  <summary>Details</summary>
Motivation: 支票欺诈仍然是一个问题，需要强大的支票欺诈检测机制。准确识别和定位关键字段（如签名、MICR 线、礼貌金额、法律金额、收款人和付款人）至关重要，但传统上依赖于在大型、多样化和精心标记的数据集上训练的对象检测模型，而这种资源由于所有权和隐私问题而稀缺。

Method: 利用视觉语言模型 (VLM) 和多模态大型语言模型 (MLLM) 的力量，实现支票组件的零样本检测。

Result: 在包含多种格式和布局的 110 张支票的手动管理数据集上对模型进行定量评估，证明了强大的性能和泛化能力。

Conclusion: 该框架可以作为生成高质量标记数据集的引导机制，从而能够开发专为机构需求量身定制的专用实时对象检测模型。

Abstract: Checks remain a foundational instrument in the financial ecosystem,
facilitating substantial transaction volumes across institutions. However,
their continued use also renders them a persistent target for fraud,
underscoring the importance of robust check fraud detection mechanisms. At the
core of such systems lies the accurate identification and localization of
critical fields, such as the signature, magnetic ink character recognition
(MICR) line, courtesy amount, legal amount, payee, and payer, which are
essential for subsequent verification against reference checks belonging to the
same customer. This field-level detection is traditionally dependent on object
detection models trained on large, diverse, and meticulously labeled datasets,
a resource that is scarce due to proprietary and privacy concerns. In this
paper, we introduce a novel, training-free framework for automated check field
detection, leveraging the power of a vision language model (VLM) in conjunction
with a multimodal large language model (MLLM). Our approach enables zero-shot
detection of check components, significantly lowering the barrier to deployment
in real-world financial settings. Quantitative evaluation of our model on a
hand-curated dataset of 110 checks spanning multiple formats and layouts
demonstrates strong performance and generalization capability. Furthermore,
this framework can serve as a bootstrap mechanism for generating high-quality
labeled datasets, enabling the development of specialized real-time object
detection models tailored to institutional needs.

</details>


### [73] [Losing the Plot: How VLM responses degrade on imperfect charts](https://arxiv.org/abs/2509.18425)
*Philip Wootaek Shin,Jack Sampson,Vijaykrishnan Narayanan,Andres Marquez,Mahantesh Halappanavar*

Main category: cs.CV

TL;DR: 现有的视觉语言模型在图表理解方面表现出色，但面对现实世界中扭曲和遮挡的图表时，性能会急剧下降，并出现幻觉问题。本文提出了 CHART NOISe 数据集，用于评估模型在噪声和遮挡输入下的图表理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准假设图表是干净的，查询是基于事实的，而现实世界的图表通常包含扭曲，并且需要超出简单匹配的推理能力。

Method: 通过引入 CHART NOISe 数据集，该数据集结合了图表损坏、遮挡和受韩国 CSAT 英语部分启发的考试风格多项选择题，并使用 prompt reverse inconsistency 方法来检测模型是否会自相矛盾。

Result: 发现 ChatGPT 4o、Claude Sonnet 4 和 Gemini 2.5 Pro 在损坏或遮挡的情况下性能急剧下降，并且幻觉现象（如价值捏造、趋势误解和实体混淆）变得更加频繁。模型在降级设置中仍然过于自信，产生看似合理但没有根据的解释。

Conclusion: 本文通过基准测试、发布 CHART NOISe 数据集以及提出诸如质量过滤和遮挡检测等基线缓解策略，为提高图表理解的鲁棒性和可靠性建立了一个严格的测试平台。

Abstract: Vision language models (VLMs) show strong results on chart understanding, yet
existing benchmarks assume clean figures and fact based queries. Real world
charts often contain distortions and demand reasoning beyond simple matching.
We evaluate ChatGPT 4o, Claude Sonnet 4, and Gemini 2.5 Pro, finding sharp
performance drops under corruption or occlusion, with hallucinations such as
value fabrication, trend misinterpretation, and entity confusion becoming more
frequent. Models remain overconfident in degraded settings, generating
plausible but unsupported explanations.
  To address this gap, we introduce CHART NOISe(Chart Hallucinations, Answers,
and Reasoning Testing on Noisy and Occluded Input Selections), a dataset
combining chart corruptions, occlusions, and exam style multiple choice
questions inspired by Korea's CSAT English section. A key innovation is prompt
reverse inconsistency, where models contradict themselves when asked to confirm
versus deny the same statement. Our contributions are threefold: (1)
benchmarking state of the art VLMs, exposing systematic vulnerabilities in
chart reasoning; (2) releasing CHART NOISe, the first dataset unifying
corruption, occlusion, and reverse inconsistency; and (3) proposing baseline
mitigation strategies such as quality filtering and occlusion detection.
Together, these efforts establish a rigorous testbed for advancing robustness
and reliability in chart understanding.

</details>


### [74] [CPT-4DMR: Continuous sPatial-Temporal Representation for 4D-MRI Reconstruction](https://arxiv.org/abs/2509.18427)
*Xinyang Wu,Muheng Li,Xia Li,Orso Pusterla,Sairos Safai,Philippe C. Cattin,Antony J. Lomax,Ye Zhang*

Main category: cs.CV

TL;DR: 提出了一种新的基于神经表示的4D-MRI重建框架，该框架能够捕捉呼吸引起的运动，并且比传统方法更有效率。


<details>
  <summary>Details</summary>
Motivation: 传统的4D重建方法难以捕捉时间变异性，流程复杂，计算负担重。

Method: 该方法通过两个协同网络融合运动建模和图像重建：空间解剖网络（SAN）编码连续的3D解剖表示，而时间运动网络（TMN）在Transformer衍生的呼吸信号的引导下，产生时间上一致的变形场。

Result: 在19名志愿者的自由呼吸数据集上的评估表明，该方法能够准确捕捉规则和不规则的呼吸模式，同时保持血管和支气管的连续性，具有很高的解剖保真度。处理时间从传统方法的约5小时减少到仅15分钟的训练时间。此外，它能够在不到一秒钟的时间内推断出每个3D体积。

Conclusion: 该框架能够准确地重建任何呼吸状态下的3D图像，与传统方法相比具有更优越的性能，并且在4D放射治疗计划和实时自适应治疗中具有强大的应用潜力。

Abstract: Four-dimensional MRI (4D-MRI) is an promising technique for capturing
respiratory-induced motion in radiation therapy planning and delivery.
Conventional 4D reconstruction methods, which typically rely on phase binning
or separate template scans, struggle to capture temporal variability,
complicate workflows, and impose heavy computational loads. We introduce a
neural representation framework that considers respiratory motion as a smooth,
continuous deformation steered by a 1D surrogate signal, completely replacing
the conventional discrete sorting approach. The new method fuses motion
modeling with image reconstruction through two synergistic networks: the
Spatial Anatomy Network (SAN) encodes a continuous 3D anatomical
representation, while a Temporal Motion Network (TMN), guided by
Transformer-derived respiratory signals, produces temporally consistent
deformation fields. Evaluation using a free-breathing dataset of 19 volunteers
demonstrates that our template- and phase-free method accurately captures both
regular and irregular respiratory patterns, while preserving vessel and
bronchial continuity with high anatomical fidelity. The proposed method
significantly improves efficiency, reducing the total processing time from
approximately five hours required by conventional discrete sorting methods to
just 15 minutes of training. Furthermore, it enables inference of each 3D
volume in under one second. The framework accurately reconstructs 3D images at
any respiratory state, achieves superior performance compared to conventional
methods, and demonstrates strong potential for application in 4D radiation
therapy planning and real-time adaptive treatment.

</details>


### [75] [An Analysis of Kalman Filter based Object Tracking Methods for Fast-Moving Tiny Objects](https://arxiv.org/abs/2509.18451)
*Prithvi Raj Singh,Raju Gottumukkala,Anthony Maida*

Main category: cs.CV

TL;DR: 评估了五种基于卡尔曼滤波器的跟踪方法在快速移动微小物体（如壁球）上的性能，发现它们存在跟踪漂移，需要改进。


<details>
  <summary>Details</summary>
Motivation: 在体育机器人应用中，精确跟踪快速移动的微小物体（如壁球）是一个挑战，因为这可以提高机器人感知和规划能力。

Method: 使用包含10,000个带注释的壁球帧的自定义数据集，评估了五种最先进的基于卡尔曼滤波器的跟踪方法（OCSORT、DeepOCSORT、ByteTrack、BoTSORT和StrongSORT）的性能。

Result: DeepOCSORT实现了最低的跟踪误差（平均ADE为31.15像素），而ByteTrack的平均推理时间最快（26.6ms）。但是，所有基于卡尔曼滤波器的跟踪器都表现出显著的跟踪漂移（空间误差范围为3-11厘米）。

Conclusion: 当前的跟踪方法需要大幅改进，误差率比标准对象跟踪基准高3-4倍，这表明需要专门的方法来跟踪快速移动的微小物体。

Abstract: Unpredictable movement patterns and small visual mark make precise tracking
of fast-moving tiny objects like a racquetball one of the challenging problems
in computer vision. This challenge is particularly relevant for sport robotics
applications, where lightweight and accurate tracking systems can improve robot
perception and planning capabilities. While Kalman filter-based tracking
methods have shown success in general object tracking scenarios, their
performance degrades substantially when dealing with rapidly moving objects
that exhibit irregular bouncing behavior. In this study, we evaluate the
performance of five state-of-the-art Kalman filter-based tracking
methods-OCSORT, DeepOCSORT, ByteTrack, BoTSORT, and StrongSORT-using a custom
dataset containing 10,000 annotated racquetball frames captured at 720p-1280p
resolution. We focus our analysis on two critical performance factors:
inference speed and update frequency per image, examining how these parameters
affect tracking accuracy and reliability for fast-moving tiny objects. Our
experimental evaluation across four distinct scenarios reveals that DeepOCSORT
achieves the lowest tracking error with an average ADE of 31.15 pixels compared
to ByteTrack's 114.3 pixels, while ByteTrack demonstrates the fastest
processing at 26.6ms average inference time versus DeepOCSORT's 26.8ms.
However, our results show that all Kalman filter-based trackers exhibit
significant tracking drift with spatial errors ranging from 3-11cm (ADE values:
31-114 pixels), indicating fundamental limitations in handling the
unpredictable motion patterns of fast-moving tiny objects like racquetballs.
Our analysis demonstrates that current tracking approaches require substantial
improvements, with error rates 3-4x higher than standard object tracking
benchmarks, highlighting the need for specialized methodologies for fast-moving
tiny object tracking applications.

</details>


### [76] [MoCrop: Training Free Motion Guided Cropping for Efficient Video Action Recognition](https://arxiv.org/abs/2509.18473)
*Binhua Huang,Wendong Yao,Shaowu Chen,Guoxin Wang,Qingyuan Wang,Soumyabrata Dev*

Main category: cs.CV

TL;DR: MoCrop是一个运动感知自适应裁剪模块，用于压缩域中高效的视频动作识别。


<details>
  <summary>Details</summary>
Motivation: 在压缩域中进行高效视频动作识别。

Method: 利用H.264视频中可用的运动向量来定位运动密集区域，并生成应用于所有I帧的单个剪辑级别裁剪。

Result: 在UCF101上，MoCrop提高了准确性或减少了计算量。在CoViAR上，它在原始成本下达到89.2%的Top-1准确率，并在减少计算量的情况下达到88.5%的Top-1准确率。

Conclusion: MoCrop具有很强的通用性，适用于压缩域中的实时部署。

Abstract: We introduce MoCrop, a motion-aware adaptive cropping module for efficient
video action recognition in the compressed domain. MoCrop uses motion vectors
that are available in H.264 video to locate motion-dense regions and produces a
single clip-level crop that is applied to all I-frames at inference. The module
is training free, adds no parameters, and can be plugged into diverse
backbones. A lightweight pipeline that includes denoising & merge (DM), Monte
Carlo sampling (MCS), and adaptive cropping (AC) via a motion-density submatrix
search yields robust crops with negligible overhead. On UCF101, MoCrop improves
accuracy or reduces compute. With ResNet-50, it delivers +3.5% Top-1 accuracy
at equal FLOPs (attention setting), or +2.4% Top-1 accuracy with 26.5% fewer
FLOPs (efficiency setting). Applied to CoViAR, it reaches 89.2% Top-1 accuracy
at the original cost and 88.5% Top-1 accuracy while reducing compute from 11.6
to 8.5 GFLOPs. Consistent gains on MobileNet-V3, EfficientNet-B1, and Swin-B
indicate strong generality and make MoCrop practical for real-time deployment
in the compressed domain. Our code and models are available at
https://github.com/microa/MoCrop.

</details>


### [77] [Codebook-Based Adaptive Feature Compression With Semantic Enhancement for Edge-Cloud Systems](https://arxiv.org/abs/2509.18481)
*Xinyu Wang,Zikun Zhou,Yingjian Li,Xin An,Hongpeng Wang*

Main category: cs.CV

TL;DR: 提出了一种名为 CAFC-SE 的基于码本的自适应特征压缩框架，通过向量量化将连续视觉特征映射到离散索引，以在低比特率条件下保持更多信息。


<details>
  <summary>Details</summary>
Motivation: 在低比特率条件下，现有图像编解码方法和基于熵模型的特征压缩方法表现不佳，因为它们保留了许多冗余细节或学习了过度集中的符号分布。

Method: 通过向量量化 (VQ) 将连续视觉特征映射到离散索引，并有选择地将它们传输到云端。VQ 操作将特征向量投影到最近的视觉原语上。

Result: 大量实验表明，该方法在速率和准确性方面都具有优越性。

Conclusion: 该方法在低比特率条件下能够保持更多信息，因而不易受到低比特率条件的影响。

Abstract: Coding images for machines with minimal bitrate and strong analysis
performance is key to effective edge-cloud systems. Several approaches deploy
an image codec and perform analysis on the reconstructed image. Other methods
compress intermediate features using entropy models and subsequently perform
analysis on the decoded features. Nevertheless, these methods both perform
poorly under low-bitrate conditions, as they retain many redundant details or
learn over-concentrated symbol distributions. In this paper, we propose a
Codebook-based Adaptive Feature Compression framework with Semantic
Enhancement, named CAFC-SE. It maps continuous visual features to discrete
indices with a codebook at the edge via Vector Quantization (VQ) and
selectively transmits them to the cloud. The VQ operation that projects feature
vectors onto the nearest visual primitives enables us to preserve more
informative visual patterns under low-bitrate conditions. Hence, CAFC-SE is
less vulnerable to low-bitrate conditions. Extensive experiments demonstrate
the superiority of our method in terms of rate and accuracy.

</details>


### [78] [MK-UNet: Multi-kernel Lightweight CNN for Medical Image Segmentation](https://arxiv.org/abs/2509.18493)
*Md Mostafijur Rahman,Radu Marculescu*

Main category: cs.CV

TL;DR: 提出了一个超轻量级的多核U型CNN，名为MK-UNet，专为医学图像分割设计。


<details>
  <summary>Details</summary>
Motivation: 在资源有限的环境中，实现实时、高保真的医学诊断。

Method: 设计了多核深度卷积块（MKDC），通过多个内核巧妙地处理图像，同时捕获复杂的多分辨率空间关系。MK-UNet还通过复杂的注意力机制（包括通道、空间和分组门控注意力）来强调图像的显著特征。

Result: 在六个二元医学成像基准测试中，MK-UNet以仅0.316M参数和0.314G FLOPs的适度计算占用空间，不仅代表着一个非常轻量级的解决方案，而且还提供了显着改进的分割解决方案，该解决方案比最先进（SOTA）的方法具有更高的准确性。

Conclusion: MK-UNet在性能上的飞跃，加上计算上的大幅提升，使其成为资源有限环境（如即时护理设备）中实时、高保真医学诊断的无与伦比的解决方案。

Abstract: In this paper, we introduce MK-UNet, a paradigm shift towards
ultra-lightweight, multi-kernel U-shaped CNNs tailored for medical image
segmentation. Central to MK-UNet is the multi-kernel depth-wise convolution
block (MKDC) we design to adeptly process images through multiple kernels,
while capturing complex multi-resolution spatial relationships. MK-UNet also
emphasizes the images salient features through sophisticated attention
mechanisms, including channel, spatial, and grouped gated attention. Our
MK-UNet network, with a modest computational footprint of only 0.316M
parameters and 0.314G FLOPs, represents not only a remarkably lightweight, but
also significantly improved segmentation solution that provides higher accuracy
over state-of-the-art (SOTA) methods across six binary medical imaging
benchmarks. Specifically, MK-UNet outperforms TransUNet in DICE score with
nearly 333$\times$ and 123$\times$ fewer parameters and FLOPs, respectively.
Similarly, when compared against UNeXt, MK-UNet exhibits superior segmentation
performance, improving the DICE score up to 6.7% margins while operating with
4.7$\times$ fewer #Params. Our MK-UNet also outperforms other recent
lightweight networks, such as MedT, CMUNeXt, EGE-UNet, and Rolling-UNet, with
much lower computational resources. This leap in performance, coupled with
drastic computational gains, positions MK-UNet as an unparalleled solution for
real-time, high-fidelity medical diagnostics in resource-limited settings, such
as point-of-care devices. Our implementation is available at
https://github.com/SLDGroup/MK-UNet.

</details>


### [79] [BridgeSplat: Bidirectionally Coupled CT and Non-Rigid Gaussian Splatting for Deformable Intraoperative Surgical Navigation](https://arxiv.org/abs/2509.18501)
*Maximilian Fehrentz,Alexander Winkler,Thomas Heiliger,Nazim Haouchine,Christian Heiliger,Nassir Navab*

Main category: cs.CV

TL;DR: BridgeSplat: 使用术中3D重建和术前CT数据，通过将3D高斯函数绑定到CT网格，实现可变形的手术导航。


<details>
  <summary>Details</summary>
Motivation: 弥合手术视频和体积患者数据之间的差距。

Method: 将3D高斯函数绑定到CT网格，通过光度监督联合优化高斯参数和网格变形。每个高斯函数都相对于其父网格三角形进行参数化，从而加强高斯函数和网格之间的对齐。

Result: 在内脏猪手术和人体肝脏的合成数据上，展示了BridgeSplat的有效性，显示了单眼RGB数据上术前CT的合理变形。

Conclusion: BridgeSplat实现了可变形的手术导航，有效弥合了手术视频和体积患者数据之间的差距。

Abstract: We introduce BridgeSplat, a novel approach for deformable surgical navigation
that couples intraoperative 3D reconstruction with preoperative CT data to
bridge the gap between surgical video and volumetric patient data. Our method
rigs 3D Gaussians to a CT mesh, enabling joint optimization of Gaussian
parameters and mesh deformation through photometric supervision. By
parametrizing each Gaussian relative to its parent mesh triangle, we enforce
alignment between Gaussians and mesh and obtain deformations that can be
propagated back to update the CT. We demonstrate BridgeSplat's effectiveness on
visceral pig surgeries and synthetic data of a human liver under simulation,
showing sensible deformations of the preoperative CT on monocular RGB data.
Code, data, and additional resources can be found at
https://maxfehrentz.github.io/ct-informed-splatting/ .

</details>


### [80] [Source-Free Domain Adaptive Semantic Segmentation of Remote Sensing Images with Diffusion-Guided Label Enrichment](https://arxiv.org/abs/2509.18502)
*Wenjie Liu,Hongmin Liu,Lixin Zhang,Bin Fan*

Main category: cs.CV

TL;DR: 提出了一种新的伪标签优化框架，名为扩散引导标签富集 (DGLE)，该框架从一些容易获得的高质量伪标签开始，并将它们传播到完整的伪标签集，同时确保新生成的标签的质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法优化整个伪标签集以获得更多监督信息。然而，由于伪标签集通常包含大量噪声，因此同时优化所有标签具有挑战性。这种限制削弱了优化方法的有效性，从而限制了自我训练的性能。

Method: 首先，提出了一种基于置信度过滤和超分辨率增强的伪标签融合方法，该方法利用细节和上下文信息的交叉验证来获得少量高质量的伪标签作为初始种子。然后，我们利用扩散模型来传播不完整的种子伪标签，由于其强大的去噪能力（针对随机分布的噪声）和强大的复杂分布建模能力，从而生成完整且高质量的伪标签。

Result: 该方法有效地避免了直接优化完整伪标签集的困难，显着提高了伪标签的质量，从而提高了模型在目标域中的性能。

Conclusion: 提出了一种新的伪标签优化框架，名为扩散引导标签富集 (DGLE)，该框架从一些容易获得的高质量伪标签开始，并将它们传播到完整的伪标签集，同时确保新生成的标签的质量。

Abstract: Research on unsupervised domain adaptation (UDA) for semantic segmentation of
remote sensing images has been extensively conducted. However, research on how
to achieve domain adaptation in practical scenarios where source domain data is
inaccessible namely, source-free domain adaptation (SFDA) remains limited.
Self-training has been widely used in SFDA, which requires obtaining as many
high-quality pseudo-labels as possible to train models on target domain data.
Most existing methods optimize the entire pseudo-label set to obtain more
supervisory information. However, as pseudo-label sets often contain
substantial noise, simultaneously optimizing all labels is challenging. This
limitation undermines the effectiveness of optimization approaches and thus
restricts the performance of self-training. To address this, we propose a novel
pseudo-label optimization framework called Diffusion-Guided Label Enrichment
(DGLE), which starts from a few easily obtained high-quality pseudo-labels and
propagates them to a complete set of pseudo-labels while ensuring the quality
of newly generated labels. Firstly, a pseudo-label fusion method based on
confidence filtering and super-resolution enhancement is proposed, which
utilizes cross-validation of details and contextual information to obtain a
small number of high-quality pseudo-labels as initial seeds. Then, we leverage
the diffusion model to propagate incomplete seed pseudo-labels with irregular
distributions due to its strong denoising capability for randomly distributed
noise and powerful modeling capacity for complex distributions, thereby
generating complete and high-quality pseudo-labels. This method effectively
avoids the difficulty of directly optimizing the complete set of pseudo-labels,
significantly improves the quality of pseudo-labels, and thus enhances the
model's performance in the target domain.

</details>


### [81] [Hyperbolic Coarse-to-Fine Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2509.18504)
*Jiaxin Dai,Xiang Xiang*

Main category: cs.CV

TL;DR: 该论文提出了一种基于双曲空间的粗到细少样本类增量学习 (C2FSCIL) 方法，通过将特征提取器嵌入到双曲空间中，并引入双曲对比损失和全连接层，以及最大熵分布来增强少样本条件下的性能。


<details>
  <summary>Details</summary>
Motivation: 传统欧几里得空间在表示分层数据方面不如双曲空间。该研究着眼于粗到细少样本类增量学习 (C2FSCIL) 任务。

Method: 该方法将特征提取器嵌入到双曲空间，使用 Poincaré 球模型，并引入双曲对比损失、双曲全连接层以及双曲空间中的最大熵分布来估计细类特征向量的概率分布，从而生成增强特征。

Result: 在 C2FSCIL 基准测试上的实验表明，该方法有效提高了粗类和细类的准确性。

Conclusion: 该研究提出了一种有效的基于双曲空间的 C2FSCIL 方法，并在实验中验证了其性能。

Abstract: In the field of machine learning, hyperbolic space demonstrates superior
representation capabilities for hierarchical data compared to conventional
Euclidean space. This work focuses on the Coarse-To-Fine Few-Shot
Class-Incremental Learning (C2FSCIL) task. Our study follows the Knowe
approach, which contrastively learns coarse class labels and subsequently
normalizes and freezes the classifier weights of learned fine classes in the
embedding space. To better interpret the "coarse-to-fine" paradigm, we propose
embedding the feature extractor into hyperbolic space. Specifically, we employ
the Poincar\'e ball model of hyperbolic space, enabling the feature extractor
to transform input images into feature vectors within the Poincar\'e ball
instead of Euclidean space. We further introduce hyperbolic contrastive loss
and hyperbolic fully-connected layers to facilitate model optimization and
classification in hyperbolic space. Additionally, to enhance performance under
few-shot conditions, we implement maximum entropy distribution in hyperbolic
space to estimate the probability distribution of fine-class feature vectors.
This allows generation of augmented features from the distribution to mitigate
overfitting during training with limited samples. Experiments on C2FSCIL
benchmarks show that our method effectively improves both coarse and fine class
accuracies.

</details>


### [82] [GeoRemover: Removing Objects and Their Causal Visual Artifacts](https://arxiv.org/abs/2509.18538)
*Zixin Zhu,Haoxiang Li,Xuelu Feng,He Wu,Chunming Qiao,Junsong Yuan*

Main category: cs.CV

TL;DR: 提出了一种几何感知图像编辑框架，用于移除图像中的物体及其因果视觉伪影（如阴影和反射）。


<details>
  <summary>Details</summary>
Motivation: 现有的基于图像外观的方法要么严格遵循mask对齐训练，无法移除未明确mask的因果效应，要么采用松散的mask对齐策略，缺乏可控性，可能无意中过度擦除其他物体。这些局限性源于忽略了物体几何存在与其视觉效果之间的因果关系。

Method: 提出了一个几何感知的两阶段框架，将物体移除分解为 (1) 几何移除和 (2) 外观渲染。第一阶段，使用严格的mask对齐监督从几何（例如，深度）中直接移除物体，从而实现具有强几何约束的结构感知编辑。在第二阶段，我们根据更新后的几何渲染逼真的RGB图像，其中因果视觉效果被隐式地视为修改后的3D几何的结果。为了指导几何移除阶段的学习，我们引入了一种基于正负样本对的偏好驱动目标，鼓励模型移除物体及其因果视觉伪影，同时避免新的结构插入。

Result: 在两个流行的基准测试中，该方法在移除物体及其相关伪影方面取得了最先进的性能。

Conclusion: 该论文提出了一种新的图像编辑框架，通过解耦几何移除和外观渲染，有效地移除了物体及其因果视觉伪影。

Abstract: Towards intelligent image editing, object removal should eliminate both the
target object and its causal visual artifacts, such as shadows and reflections.
However, existing image appearance-based methods either follow strictly
mask-aligned training and fail to remove these causal effects which are not
explicitly masked, or adopt loosely mask-aligned strategies that lack
controllability and may unintentionally over-erase other objects. We identify
that these limitations stem from ignoring the causal relationship between an
object's geometry presence and its visual effects. To address this limitation,
we propose a geometry-aware two-stage framework that decouples object removal
into (1) geometry removal and (2) appearance rendering. In the first stage, we
remove the object directly from the geometry (e.g., depth) using strictly
mask-aligned supervision, enabling structure-aware editing with strong
geometric constraints. In the second stage, we render a photorealistic RGB
image conditioned on the updated geometry, where causal visual effects are
considered implicitly as a result of the modified 3D geometry. To guide
learning in the geometry removal stage, we introduce a preference-driven
objective based on positive and negative sample pairs, encouraging the model to
remove objects as well as their causal visual artifacts while avoiding new
structural insertions. Extensive experiments demonstrate that our method
achieves state-of-the-art performance in removing both objects and their
associated artifacts on two popular benchmarks. The code is available at
https://github.com/buxiangzhiren/GeoRemover.

</details>


### [83] [SEGA: A Transferable Signed Ensemble Gaussian Black-Box Attack against No-Reference Image Quality Assessment Models](https://arxiv.org/abs/2509.18546)
*Yujia Liu,Dingquan Li,Tiejun Huang*

Main category: cs.CV

TL;DR: 本文提出了一种新的黑盒攻击方法，用于评估无参考图像质量评估 (NR-IQA) 模型在对抗攻击下的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有的白盒攻击方法在黑盒场景下迁移性较差，无法有效攻击未知的 NR-IQA 模型。

Method: 提出了一种可迁移的 Signed Ensemble Gaussian 黑盒攻击 (SEGA) 方法，该方法通过对源模型应用高斯平滑并集成其平滑梯度来近似目标模型的梯度，并使用专门设计的扰动滤波器掩模来消除不适当的扰动，以确保对抗性扰动的不可察觉性。

Result: 在 CLIVE 数据集上的实验结果表明，SEGA 具有优越的可迁移性。

Conclusion: SEGA 能够成功地实现基于迁移的黑盒攻击，从而验证了其有效性。

Abstract: No-Reference Image Quality Assessment (NR-IQA) models play an important role
in various real-world applications. Recently, adversarial attacks against
NR-IQA models have attracted increasing attention, as they provide valuable
insights for revealing model vulnerabilities and guiding robust system design.
Some effective attacks have been proposed against NR-IQA models in white-box
settings, where the attacker has full access to the target model. However,
these attacks often suffer from poor transferability to unknown target models
in more realistic black-box scenarios, where the target model is inaccessible.
This work makes the first attempt to address the challenge of low
transferability in attacking NR-IQA models by proposing a transferable Signed
Ensemble Gaussian black-box Attack (SEGA). The main idea is to approximate the
gradient of the target model by applying Gaussian smoothing to source models
and ensembling their smoothed gradients. To ensure the imperceptibility of
adversarial perturbations, SEGA further removes inappropriate perturbations
using a specially designed perturbation filter mask. Experimental results on
the CLIVE dataset demonstrate the superior transferability of SEGA, validating
its effectiveness in enabling successful transfer-based black-box attacks
against NR-IQA models.

</details>


### [84] [HadaSmileNet: Hadamard fusion of handcrafted and deep-learning features for enhancing facial emotion recognition of genuine smiles](https://arxiv.org/abs/2509.18550)
*Mohammad Junayed Hasan,Nabeel Mohammed,Shafin Rahman,Philipp Koehn*

Main category: cs.CV

TL;DR: 提出了一种新的特征融合框架HadaSmileNet，用于微笑面部表情识别，该框架通过参数自由的乘法交互将基于transformer的表示与生理学D-Marker直接集成。


<details>
  <summary>Details</summary>
Motivation: 区分真假情绪是模式识别中的一项基本挑战，对社会科学、医疗保健和人机交互中的数据挖掘应用具有重要意义。现有的多任务学习框架在结合深度学习架构和手工制作的D-Marker特征方面表现出计算效率低下。

Method: 该论文提出HadaSmileNet，一种新颖的特征融合框架，通过参数自由的乘法交互直接整合基于transformer的表示与生理学D-Markers。通过对15种融合策略的系统评估，证明了Hadamard乘法融合通过实现直接特征交互同时保持计算效率，从而实现了最佳性能。

Result: 在四个基准数据集UvA-NEMO、MMI、SPOS和BBC上，该方法取得了新的state-of-the-art结果。计算分析表明，与多任务替代方案相比，参数减少了26%，并且简化了训练。特征可视化证明了通过直接领域知识集成增强了判别能力。

Conclusion: 该框架的效率和有效性使其特别适用于需要实时情感计算能力的多媒体数据挖掘应用中的实际部署。

Abstract: The distinction between genuine and posed emotions represents a fundamental
pattern recognition challenge with significant implications for data mining
applications in social sciences, healthcare, and human-computer interaction.
While recent multi-task learning frameworks have shown promise in combining
deep learning architectures with handcrafted D-Marker features for smile facial
emotion recognition, these approaches exhibit computational inefficiencies due
to auxiliary task supervision and complex loss balancing requirements. This
paper introduces HadaSmileNet, a novel feature fusion framework that directly
integrates transformer-based representations with physiologically grounded
D-Markers through parameter-free multiplicative interactions. Through
systematic evaluation of 15 fusion strategies, we demonstrate that Hadamard
multiplicative fusion achieves optimal performance by enabling direct feature
interactions while maintaining computational efficiency. The proposed approach
establishes new state-of-the-art results for deep learning methods across four
benchmark datasets: UvA-NEMO (88.7 percent, +0.8), MMI (99.7 percent), SPOS
(98.5 percent, +0.7), and BBC (100 percent, +5.0). Comprehensive computational
analysis reveals 26 percent parameter reduction and simplified training
compared to multi-task alternatives, while feature visualization demonstrates
enhanced discriminative power through direct domain knowledge integration. The
framework's efficiency and effectiveness make it particularly suitable for
practical deployment in multimedia data mining applications that require
real-time affective computing capabilities.

</details>


### [85] [Event-guided 3D Gaussian Splatting for Dynamic Human and Scene Reconstruction](https://arxiv.org/abs/2509.18566)
*Xiaoting Yin,Hao Shi,Kailun Yang,Jiajun Zhai,Shangwei Guo,Lin Wang,Kaiwei Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的事件引导的人体场景重建框架，该框架通过 3D 高斯溅射从单个单目事件相机中共同建模人体和场景。


<details>
  <summary>Details</summary>
Motivation: 从单目视频重建动态人和静态场景仍然很困难，尤其是在快速运动下，RGB 帧会受到运动模糊的影响。事件相机具有独特的优势，例如微秒级的时间分辨率，使其成为动态人体重建的绝佳传感选择。

Method: 该方法使用一组统一的 3D 高斯来携带可学习的语义属性；只有被分类为人类的高斯才会发生变形以进行动画，而场景高斯则保持静态。为了对抗模糊，该方法提出了一种事件引导的损失，该损失将连续渲染之间的模拟亮度变化与事件流相匹配，从而提高了快速移动区域的局部保真度。

Result: 在两个基准数据集 ZJU-MoCap-Blur 和 MMHPSD-Blur 上，该方法提供了最先进的人体场景重建，与强大的基线相比，在 PSNR/SSIM 方面取得了显着 gains，并降低了 LPIPS，尤其是在高速 subject 方面。

Conclusion: 该方法无需外部人体 mask，并简化了管理单独的高斯 sets。

Abstract: Reconstructing dynamic humans together with static scenes from monocular
videos remains difficult, especially under fast motion, where RGB frames suffer
from motion blur. Event cameras exhibit distinct advantages, e.g., microsecond
temporal resolution, making them a superior sensing choice for dynamic human
reconstruction. Accordingly, we present a novel event-guided human-scene
reconstruction framework that jointly models human and scene from a single
monocular event camera via 3D Gaussian Splatting. Specifically, a unified set
of 3D Gaussians carries a learnable semantic attribute; only Gaussians
classified as human undergo deformation for animation, while scene Gaussians
stay static. To combat blur, we propose an event-guided loss that matches
simulated brightness changes between consecutive renderings with the event
stream, improving local fidelity in fast-moving regions. Our approach removes
the need for external human masks and simplifies managing separate Gaussian
sets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers
state-of-the-art human-scene reconstruction, with notable gains over strong
baselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.

</details>


### [86] [Live-E2T: Real-time Threat Monitoring in Video via Deduplicated Event Reasoning and Chain-of-Thought](https://arxiv.org/abs/2509.18571)
*Yuhan Wang,Cheng Liu,Zihan Zhao,Weichao Wu*

Main category: cs.CV

TL;DR: 提出了一种名为 Live-E2T 的新框架，用于实时威胁监控，该框架能够兼顾实时性能和决策可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于监督学习或生成模型的方法难以同时满足实时性能和决策可解释性的需求。

Method: 该框架通过三个协同机制实现：将视频帧解构为结构化的语义元组；提出一种高效的在线事件去重和更新机制；使用 Chain-of-Thought 策略微调大型语言模型，使其能够对事件序列进行透明和逻辑的推理，以生成连贯的威胁评估报告。

Result: 在基准数据集上的大量实验表明，Live-E2T 在威胁检测准确性、实时效率和可解释性方面均优于现有技术。

Conclusion: Live-E2T 显著优于现有技术。

Abstract: Real-time threat monitoring identifies threatening behaviors in video streams
and provides reasoning and assessment of threat events through explanatory
text. However, prevailing methodologies, whether based on supervised learning
or generative models, struggle to concurrently satisfy the demanding
requirements of real-time performance and decision explainability. To bridge
this gap, we introduce Live-E2T, a novel framework that unifies these two
objectives through three synergistic mechanisms. First, we deconstruct video
frames into structured Human-Object-Interaction-Place semantic tuples. This
approach creates a compact, semantically focused representation, circumventing
the information degradation common in conventional feature compression. Second,
an efficient online event deduplication and updating mechanism is proposed to
filter spatio-temporal redundancies, ensuring the system's real time
responsiveness. Finally, we fine-tune a Large Language Model using a
Chain-of-Thought strategy, endow it with the capability for transparent and
logical reasoning over event sequences to produce coherent threat assessment
reports. Extensive experiments on benchmark datasets, including XD-Violence and
UCF-Crime, demonstrate that Live-E2T significantly outperforms state-of-the-art
methods in terms of threat detection accuracy, real-time efficiency, and the
crucial dimension of explainability.

</details>


### [87] [The Photographer Eye: Teaching Multimodal Large Language Models to See and Critique like Photographers](https://arxiv.org/abs/2509.18582)
*Daiqing Qi,Handong Zhao,Jing Shi,Simon Jenni,Yifei Fan,Franck Dernoncourt,Scott Cohen,Sheng Li*

Main category: cs.CV

TL;DR: 本文介绍了一种新的数据集、模型和基准，以提高 MLLM 的美学理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有的 MLLM 在理解图像美学方面存在差距，尤其是在需要专业知识的现实场景中。

Method: 本文提出了 PhotoCritique 数据集，并提出了 PhotoEye 模型，该模型具有语言引导的多视图视觉融合机制。此外，本文还提出了 PhotoBench 基准。

Result: PhotoEye 模型在现有基准和 PhotoBench 上都优于现有模型。

Conclusion: 本文提出的方法可以有效提高 MLLM 的美学理解能力。

Abstract: While editing directly from life, photographers have found it too difficult
to see simultaneously both the blue and the sky. Photographer and curator,
Szarkowski insightfully revealed one of the notable gaps between general and
aesthetic visual understanding: while the former focuses on identifying the
factual element in an image (sky), the latter transcends such object
identification, viewing it instead as an aesthetic component--a pure color
block (blue). Such fundamental distinctions between general (detection,
localization, etc.) and aesthetic (color, lighting, composition, etc.) visual
understanding present a significant challenge for Multimodal Large Language
Models (MLLMs). Although some recent works have made initial explorations, they
are often limited to general and basic aesthetic commonsense. As a result, they
frequently fall short in real-world scenarios (Fig. 1), which require extensive
expertise--including photographic techniques, photo pre/post-processing
knowledge, and more, to provide a detailed analysis and description. To
fundamentally enhance the aesthetics understanding of MLLMs, we first introduce
a novel dataset, PhotoCritique, derived from extensive discussions among
professional photographers and enthusiasts, and characterized by the large
scale, expertise, and diversity. Then, to better learn visual aesthetics from
PhotoCritique, we furthur propose a novel model, PhotoEye, featuring a
languageguided multi-view vision fusion mechanism to understand image
aesthetics from multiple perspectives. Finally, we present a novel benchmark,
PhotoBench, a comprehensive and professional benchmark for aesthetic visual
understanding. On existing benchmarks and PhotoBench, our model demonstrates
clear advantages over existing models.

</details>


### [88] [Enhancing Video Object Segmentation in TrackRAD Using XMem Memory Network](https://arxiv.org/abs/2509.18591)
*Pengchao Deng,Shengqi Chen*

Main category: cs.CV

TL;DR: 本文提出了一种用于实时MRI引导放射治疗的先进肿瘤分割框架，专为TrackRAD2025挑战赛设计。


<details>
  <summary>Details</summary>
Motivation: 在MRI引导放射治疗中，精确的肿瘤追踪至关重要，有助于提高癌症治疗的准确性和安全性。

Method: 该方法利用XMem模型（一种记忆增强架构）来分割长电影MRI序列中的肿瘤，并有效整合记忆机制以实时跟踪肿瘤运动。

Result: 基于XMem的框架表现出合理的分割性能，并满足临床实时要求。但由于详细的实验记录丢失，目前无法报告精确的定量结果。

Conclusion: 该研究有助于提高MRI引导放射治疗期间肿瘤追踪的精度。

Abstract: This paper presents an advanced tumor segmentation framework for real-time
MRI-guided radiotherapy, designed for the TrackRAD2025 challenge. Our method
leverages the XMem model, a memory-augmented architecture, to segment tumors
across long cine-MRI sequences. The proposed system efficiently integrates
memory mechanisms to track tumor motion in real-time, achieving high
segmentation accuracy even under challenging conditions with limited annotated
data. Unfortunately, the detailed experimental records have been lost,
preventing us from reporting precise quantitative results at this stage.
Nevertheless, From our preliminary impressions during development, the
XMem-based framework demonstrated reasonable segmentation performance and
satisfied the clinical real-time requirement. Our work contributes to improving
the precision of tumor tracking during MRI-guided radiotherapy, which is
crucial for enhancing the accuracy and safety of cancer treatments.

</details>


### [89] [SSCM: A Spatial-Semantic Consistent Model for Multi-Contrast MRI Super-Resolution](https://arxiv.org/abs/2509.18593)
*Xiaoman Wu,Lubin Gan,Siying Wu,Jing Zhang,Yunwei Ou,Xiaoyan Sun*

Main category: cs.CV

TL;DR: 本文提出了一种用于多对比度MRI超分辨率重建的空间语义一致模型（SSCM），旨在提高低分辨率图像的分辨率，同时保持空间和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在建模空间语义一致性方面不足，且未充分利用频域信息，导致精细对齐效果差，高频细节恢复不足。

Method: 该模型集成了动态空间扭曲模块、语义感知令牌聚合块和空间频率融合块。

Result: 在公共和私有数据集上的实验表明，SSCM以更少的参数实现了最先进的性能。

Conclusion: SSCM能够确保空间和语义一致的重建结果。

Abstract: Multi-contrast Magnetic Resonance Imaging super-resolution (MC-MRI SR) aims
to enhance low-resolution (LR) contrasts leveraging high-resolution (HR)
references, shortening acquisition time and improving imaging efficiency while
preserving anatomical details. The main challenge lies in maintaining
spatial-semantic consistency, ensuring anatomical structures remain
well-aligned and coherent despite structural discrepancies and motion between
the target and reference images. Conventional methods insufficiently model
spatial-semantic consistency and underuse frequency-domain information, which
leads to poor fine-grained alignment and inadequate recovery of high-frequency
details. In this paper, we propose the Spatial-Semantic Consistent Model
(SSCM), which integrates a Dynamic Spatial Warping Module for inter-contrast
spatial alignment, a Semantic-Aware Token Aggregation Block for long-range
semantic consistency, and a Spatial-Frequency Fusion Block for fine structure
restoration. Experiments on public and private datasets show that SSCM achieves
state-of-the-art performance with fewer parameters while ensuring spatially and
semantically consistent reconstructions.

</details>


### [90] [OraPO: Oracle-educated Reinforcement Learning for Data-efficient and Factual Radiology Report Generation](https://arxiv.org/abs/2509.18600)
*Zhuoxiao Chen,Hongyang Yu,Ying Xu,Yadan Luo,Long Duong,Yuan-Fang Li*

Main category: cs.CV

TL;DR: 提出了一种名为 Oracle-educated GRPO (OraPO) 的新方法，结合 FactScore-based reward (FactS)，以在有限的预算下解决 RRG 任务。


<details>
  <summary>Details</summary>
Motivation: 现有RRG方法需要大量配对语料库和超大骨干网络进行多阶段训练，数据和计算密集度高。

Method: OraPO 通过轻量级的 oracle 步骤将 GRPO 在罕见或困难研究中的失败探索转化为直接偏好监督，实现单阶段、纯 RL 训练。FactS 通过提取原子临床事实并检查与真实标签的蕴含关系，产生密集的、可解释的句子级奖励，从而将学习建立在诊断证据的基础上。

Result: 在 CheXpert Plus 数据集上实现了新的 SOTA 性能 (F1 值为 0.341)，与使用小型基础 VLM 在适度硬件上进行训练相比，训练数据减少了 2-3 个数量级。

Conclusion: OraPO 和 FactS 共同创建了一个紧凑而强大的框架，显著提高了临床挑战性案例的学习效率。

Abstract: Radiology report generation (RRG) aims to automatically produce clinically
faithful reports from chest X-ray images. Prevailing work typically follows a
scale-driven paradigm, by multi-stage training over large paired corpora and
oversized backbones, making pipelines highly data- and compute-intensive. In
this paper, we propose Oracle-educated GRPO {OraPO) with a FactScore-based
reward (FactS) to tackle the RRG task under constrained budgets. OraPO enables
single-stage, RL-only training by converting failed GRPO explorations on rare
or difficult studies into direct preference supervision via a lightweight
oracle step. FactS grounds learning in diagnostic evidence by extracting atomic
clinical facts and checking entailment against ground-truth labels, yielding
dense, interpretable sentence-level rewards. Together, OraPO and FactS create a
compact and powerful framework that significantly improves learning efficiency
on clinically challenging cases, setting the new SOTA performance on the
CheXpert Plus dataset (0.341 in F1) with 2--3 orders of magnitude less training
data using a small base VLM on modest hardware.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [91] [A Cost-Benefit Analysis of On-Premise Large Language Model Deployment: Breaking Even with Commercial LLM Services](https://arxiv.org/abs/2509.18101)
*Guanzhong Pan,Haibo Wang*

Main category: cs.AI

TL;DR: 该论文提出了一个成本效益分析框架，帮助组织确定何时本地部署LLM在经济上可行。


<details>
  <summary>Details</summary>
Motivation: 推动本地部署开源模型的因素包括数据隐私问题、更换服务提供商的困难以及长期运营成本。

Method: 该研究考虑了最新的开源模型（包括Qwen、Llama、Mistral等）的硬件要求、运营费用和性能基准，并将本地部署的总成本与主要云提供商的订阅费用进行比较。

Result: 该研究提供了一个基于使用水平和性能需求的盈亏平衡点估算。

Conclusion: 这些结果为组织规划其LLM战略提供了一个实用的框架。

Abstract: Large language models (LLMs) are becoming increasingly widespread.
Organizations that want to use AI for productivity now face an important
decision. They can subscribe to commercial LLM services or deploy models on
their own infrastructure. Cloud services from providers such as OpenAI,
Anthropic, and Google are attractive because they provide easy access to
state-of-the-art models and are easy to scale. However, concerns about data
privacy, the difficulty of switching service providers, and long-term operating
costs have driven interest in local deployment of open-source models. This
paper presents a cost-benefit analysis framework to help organizations
determine when on-premise LLM deployment becomes economically viable compared
to commercial subscription services. We consider the hardware requirements,
operational expenses, and performance benchmarks of the latest open-source
models, including Qwen, Llama, Mistral, and etc. Then we compare the total cost
of deploying these models locally with the major cloud providers subscription
fee. Our findings provide an estimated breakeven point based on usage levels
and performance needs. These results give organizations a practical framework
for planning their LLM strategies.

</details>


### [92] [SPADE: A Large Language Model Framework for Soil Moisture Pattern Recognition and Anomaly Detection in Precision Agriculture](https://arxiv.org/abs/2509.18123)
*Yeonju Lee,Rui Qi Chen,Joseph Oboamah,Po Nien Su,Wei-zhen Liang,Yeyin Shi,Lu Gan,Yongsheng Chen,Xin Qiao,Jing Li*

Main category: cs.AI

TL;DR: 本研究提出了一种名为SPADE的框架，该框架利用大型语言模型（LLM）来联合检测土壤湿度时间序列数据中的灌溉模式和异常。


<details>
  <summary>Details</summary>
Motivation: 现有的土壤湿度时间序列分析方法要么依赖于基于阈值的规则，要么依赖于数据密集型的机器学习或深度学习模型，这些模型在适应性和可解释性方面受到限制。

Method: SPADE 将时间序列数据转换为文本表示，并设计领域相关的提示模板，从而识别灌溉事件，估计净灌溉增益，检测和分类异常，并生成结构化的、可解释的报告。

Result: SPADE 在异常检测方面优于现有方法，实现了更高的召回率和 F1 分数，并准确地对异常类型进行分类。此外，SPADE 在检测灌溉事件方面实现了高精度和召回率，表明其具有很强的准确捕获灌溉模式的能力。

Conclusion: 本研究强调了 LLM 作为可扩展的、适应性强的精准农业工具的潜力，它能够整合定性知识和数据驱动的推理，从而为准确的土壤湿度监测和改进的灌溉计划提供可操作的见解。

Abstract: Accurate interpretation of soil moisture patterns is critical for irrigation
scheduling and crop management, yet existing approaches for soil moisture
time-series analysis either rely on threshold-based rules or data-hungry
machine learning or deep learning models that are limited in adaptability and
interpretability. In this study, we introduce SPADE (Soil moisture Pattern and
Anomaly DEtection), an integrated framework that leverages large language
models (LLMs) to jointly detect irrigation patterns and anomalies in soil
moisture time-series data. SPADE utilizes ChatGPT-4.1 for its advanced
reasoning and instruction-following capabilities, enabling zero-shot analysis
without requiring task-specific annotation or fine-tuning. By converting
time-series data into a textual representation and designing domain-informed
prompt templates, SPADE identifies irrigation events, estimates net irrigation
gains, detects, classifies anomalies, and produces structured, interpretable
reports. Experiments were conducted on real-world soil moisture sensor data
from commercial and experimental farms cultivating multiple crops across the
United States. Results demonstrate that SPADE outperforms the existing method
in anomaly detection, achieving higher recall and F1 scores and accurately
classifying anomaly types. Furthermore, SPADE achieved high precision and
recall in detecting irrigation events, indicating its strong capability to
capture irrigation patterns accurately. SPADE's reports provide
interpretability and usability of soil moisture analytics. This study
highlights the potential of LLMs as scalable, adaptable tools for precision
agriculture, which is capable of integrating qualitative knowledge and
data-driven reasoning to produce actionable insights for accurate soil moisture
monitoring and improved irrigation scheduling from soil moisture time-series
data.

</details>


### [93] [Position Paper: Integrating Explainability and Uncertainty Estimation in Medical AI](https://arxiv.org/abs/2509.18132)
*Xiuyi Fan*

Main category: cs.AI

TL;DR: 医学人工智能系统在量化和交流不确定性方面存在不足，而这与临床推理不符。


<details>
  <summary>Details</summary>
Motivation: 现有的XAI工作侧重于解释模型预测，但没有捕捉到这些预测的置信度或可靠性。不确定性估计（UE）技术提供置信度测量，但缺乏直观的解释。这两个领域之间的脱节限制了人工智能在医学中的应用。

Method: 我们系统地将医学不确定性映射到人工智能不确定性概念，并确定了实施XUE的关键挑战。我们概述了推进XUE的技术方向，包括多模态不确定性量化、模型无关的可视化技术和不确定性感知决策支持系统。

Result: 我们的分析强调，人工智能系统不仅需要生成可靠的预测，还需要以临床上有意义的方式表达置信水平。

Conclusion: 这项工作通过弥合可解释性和不确定性，为开发值得信赖的医学人工智能做出了贡献，为与现实世界临床复杂性相一致的人工智能系统铺平了道路。

Abstract: Uncertainty is a fundamental challenge in medical practice, but current
medical AI systems fail to explicitly quantify or communicate uncertainty in a
way that aligns with clinical reasoning. Existing XAI works focus on
interpreting model predictions but do not capture the confidence or reliability
of these predictions. Conversely, uncertainty estimation (UE) techniques
provide confidence measures but lack intuitive explanations. The disconnect
between these two areas limits AI adoption in medicine. To address this gap, we
propose Explainable Uncertainty Estimation (XUE) that integrates explainability
with uncertainty quantification to enhance trust and usability in medical AI.
We systematically map medical uncertainty to AI uncertainty concepts and
identify key challenges in implementing XUE. We outline technical directions
for advancing XUE, including multimodal uncertainty quantification,
model-agnostic visualization techniques, and uncertainty-aware decision support
systems. Lastly, we propose guiding principles to ensure effective XUE
realisation. Our analysis highlights the need for AI systems that not only
generate reliable predictions but also articulate confidence levels in a
clinically meaningful way. This work contributes to the development of
trustworthy medical AI by bridging explainability and uncertainty, paving the
way for AI systems that are aligned with real-world clinical complexities.

</details>


### [94] [HSGM: Hierarchical Segment-Graph Memory for Scalable Long-Text Semantics](https://arxiv.org/abs/2509.18168)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: 本文介绍了一种新的框架，用于长文档的语义解析，名为分层段图记忆 (HSGM)。


<details>
  <summary>Details</summary>
Motivation: 长文档的语义解析面临着两两组合的二次增长和内存需求的问题。

Method: 该方法将长度为 N 的输入分解为 M 个有意义的段，在每个段上构建局部语义图，并提取紧凑的摘要节点以形成全局图记忆。HSGM 支持增量更新和分层查询处理。

Result: 在三个基准测试中，HSGM 实现了 2-4 倍的推理加速，峰值内存减少了 >60%，并且达到了基线准确率的 >=95%。

Conclusion: HSGM 能够为超长文本实现可扩展、准确的语义建模，从而实现实时和资源受限的 NLP 应用。

Abstract: Semantic parsing of long documents remains challenging due to quadratic
growth in pairwise composition and memory requirements. We introduce
\textbf{Hierarchical Segment-Graph Memory (HSGM)}, a novel framework that
decomposes an input of length $N$ into $M$ meaningful segments, constructs
\emph{Local Semantic Graphs} on each segment, and extracts compact
\emph{summary nodes} to form a \emph{Global Graph Memory}. HSGM supports
\emph{incremental updates} -- only newly arrived segments incur local graph
construction and summary-node integration -- while \emph{Hierarchical Query
Processing} locates relevant segments via top-$K$ retrieval over summary nodes
and then performs fine-grained reasoning within their local graphs.
  Theoretically, HSGM reduces worst-case complexity from $O(N^2)$ to
$O\!\left(N\,k + (N/k)^2\right)$, with segment size $k \ll N$, and we derive
Frobenius-norm bounds on the approximation error introduced by node
summarization and sparsification thresholds. Empirically, on three benchmarks
-- long-document AMR parsing, segment-level semantic role labeling (OntoNotes),
and legal event extraction -- HSGM achieves \emph{2--4$\times$ inference
speedup}, \emph{$>60\%$ reduction} in peak memory, and \emph{$\ge 95\%$} of
baseline accuracy. Our approach unlocks scalable, accurate semantic modeling
for ultra-long texts, enabling real-time and resource-constrained NLP
applications.

</details>


### [95] [Foam-Agent: An End-to-End Composable Multi-Agent Framework for Automating CFD Simulation in OpenFOAM](https://arxiv.org/abs/2509.18178)
*Ling Yue,Nithin Somasekharan,Tingwen Zhang,Yadi Cao,Shaowu Pan*

Main category: cs.AI

TL;DR: Foam-Agent automates the OpenFOAM workflow from a natural language prompt, lowering the barrier to entry for CFD.


<details>
  <summary>Details</summary>
Motivation: The steep learning curve and complex manual setup of CFD create significant barriers for engineers.

Method: A multi-agent framework automates the entire end-to-end OpenFOAM workflow, featuring a versatile Meshing Agent, automatic HPC submission script generation, and post-simulation visualization.

Result: Foam-Agent achieves an 88.2% success rate on a benchmark of 110 simulation tasks, outperforming existing frameworks.

Conclusion: Foam-Agent demonstrates how specialized multi-agent systems can democratize complex scientific computing.

Abstract: Computational Fluid Dynamics (CFD) is an essential simulation tool in
engineering, yet its steep learning curve and complex manual setup create
significant barriers. To address these challenges, we introduce Foam-Agent, a
multi-agent framework that automates the entire end-to-end OpenFOAM workflow
from a single natural language prompt. Our key innovations address critical
gaps in existing systems: 1. An Comprehensive End-to-End Simulation Automation:
Foam-Agent is the first system to manage the full simulation pipeline,
including advanced pre-processing with a versatile Meshing Agent capable of
handling external mesh files and generating new geometries via Gmsh, automatic
generation of HPC submission scripts, and post-simulation visualization via
ParaView. 2. Composable Service Architecture: Going beyond a monolithic agent,
the framework uses Model Context Protocol (MCP) to expose its core functions as
discrete, callable tools. This allows for flexible integration and use by other
agentic systems, such as Claude-code, for more exploratory workflows. 3.
High-Fidelity Configuration Generation: We achieve superior accuracy through a
Hierarchical Multi-Index RAG for precise context retrieval and a
dependency-aware generation process that ensures configuration consistency.
Evaluated on a benchmark of 110 simulation tasks, Foam-Agent achieves an 88.2%
success rate with Claude 3.5 Sonnet, significantly outperforming existing
frameworks (55.5% for MetaOpenFOAM). Foam-Agent dramatically lowers the
expertise barrier for CFD, demonstrating how specialized multi-agent systems
can democratize complex scientific computing. The code is public at
https://github.com/csml-rpi/Foam-Agent.

</details>


### [96] [Large Language Models and Operations Research: A Structured Survey](https://arxiv.org/abs/2509.18180)
*Yang Wang,Kai Li*

Main category: cs.AI

TL;DR: 本研究探讨了大型语言模型（LLM）在运筹学（OR）中的应用，旨在克服传统方法在处理大规模、动态和多约束问题时面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统运筹学方法依赖于专家建模和手动参数调整，难以应对大规模、动态和多约束问题。大型语言模型（LLM）通过语义理解、结构化生成和推理控制，为解决这些局限性提供了潜力。

Method: 本文对LLM与OR集成的最新进展进行了综述，将方法组织为三个主要方向：自动建模、辅助优化和直接求解。此外，还回顾了评估基准和特定领域的应用。

Result: 总结了关键的开放性问题，如不稳定的语义到结构映射、分散的研究进展、有限的泛化和不足的评估系统。

Conclusion: 概述了推进LLM在OR中作用的可能研究途径。

Abstract: Operations research (OR) provides fundamental methodologies for complex
system decision-making, with established applications in transportation, supply
chain management, and production scheduling. Traditional approaches, which
depend on expert-based modeling and manual parameter adjustment, often face
challenges in handling large-scale, dynamic, and multi-constraint problems.
Recently, large language models (LLMs) have shown potential to address these
limitations through semantic understanding, structured generation, and
reasoning control. LLMs can translate natural language descriptions into
mathematical models or executable code, generate heuristics, evolve algorithms,
and directly tackle optimization tasks. This paper surveys recent progress on
the integration of LLMs into OR, organizing methods into three main directions:
automatic modeling, auxiliary optimization, and direct solving. It further
reviews evaluation benchmarks and domain-specific applications, and summarizes
key open issues such as unstable semantic-to-structure mapping, fragmented
research progress, limited generalization, and insufficient evaluation systems.
Finally, the survey outlines possible research avenues for advancing the role
of LLMs in OR.

</details>


### [97] [Memory-QA: Answering Recall Questions Based on Multimodal Memories](https://arxiv.org/abs/2509.18436)
*Hongda Jiang,Xinyuan Zhang,Siddhant Garg,Rishab Arora,Shiun-Zu Kuo,Jiayang Xu,Christopher Brossman,Yue Liu,Aaron Colak,Ahmed Aly,Anuj Kumar,Xin Luna Dong*

Main category: cs.AI

TL;DR: Memory-QA：一个新的现实世界任务，涉及回答关于先前存储的多模态记忆中的视觉内容的回忆问题。


<details>
  <summary>Details</summary>
Motivation: 这项任务提出了独特的挑战，包括创建面向任务的记忆，有效利用记忆中的时间和位置信息，以及利用多个记忆来回答一个回忆问题。

Method: 为了应对这些挑战，我们提出了一个全面的流程 Pensieve，集成了特定于记忆的增强、时间和位置感知的多信号检索以及多记忆 QA 微调。

Result: 我们创建了一个多模态基准来展示此任务中的各种实际挑战，并表明 Pensieve 优于最先进的解决方案（QA 准确率高达 14%）。

Conclusion: 结论：Pensieve 在 Memory-QA 任务上表现出色，优于现有技术。

Abstract: We introduce Memory-QA, a novel real-world task that involves answering
recall questions about visual content from previously stored multimodal
memories. This task poses unique challenges, including the creation of
task-oriented memories, the effective utilization of temporal and location
information within memories, and the ability to draw upon multiple memories to
answer a recall question. To address these challenges, we propose a
comprehensive pipeline, Pensieve, integrating memory-specific augmentation,
time- and location-aware multi-signal retrieval, and multi-memory QA
fine-tuning. We created a multimodal benchmark to illustrate various real
challenges in this task, and show the superior performance of Pensieve over
state-of-the-art solutions (up to 14% on QA accuracy).

</details>


### [98] [Synthesizing Attitudes, Predicting Actions (SAPA): Behavioral Theory-Guided LLMs for Ridesourcing Mode Choice Modeling](https://arxiv.org/abs/2509.18181)
*Mustafa Sameen,Xiaojian Zhang,Xilei Zhao*

Main category: cs.AI

TL;DR: 提出SAPA框架，利用大型语言模型合成潜在态度来预测网约车选择，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有网约车模式选择模型预测精度有限，无法捕捉关键心理因素，且面临严重的类别不平衡问题。

Method: 首先，利用大型语言模型从原始出行调查数据中生成定性旅行者角色，然后训练倾向得分模型。接下来，大型语言模型为理论驱动的潜在变量分配定量分数，并使用分类器整合倾向得分、潜在变量分数和可观察的出行属性来预测网约车模式选择。

Result: 在大型多年出行调查中，SAPA的性能显著优于现有基线，在PR-AUC方面，网约车选择预测提高了75.9%。

Conclusion: SAPA为准确预测网约车模式选择提供了一个强大的工具，并且该方法可以很容易地转移到各种应用中。

Abstract: Accurate modeling of ridesourcing mode choices is essential for designing and
implementing effective traffic management policies for reducing congestion,
improving mobility, and allocating resources more efficiently. Existing models
for predicting ridesourcing mode choices often suffer from limited predictive
accuracy due to their inability to capture key psychological factors, and are
further challenged by severe class imbalance, as ridesourcing trips comprise
only a small fraction of individuals' daily travel. To address these
limitations, this paper introduces the Synthesizing Attitudes, Predicting
Actions (SAPA) framework, a hierarchical approach that uses Large Language
Models (LLMs) to synthesize theory-grounded latent attitudes to predict
ridesourcing choices. SAPA first uses an LLM to generate qualitative traveler
personas from raw travel survey data and then trains a propensity-score model
on demographic and behavioral features, enriched by those personas, to produce
an individual-level score. Next, the LLM assigns quantitative scores to
theory-driven latent variables (e.g., time and cost sensitivity), and a final
classifier integrates the propensity score, latent-variable scores (with their
interaction terms), and observable trip attributes to predict ridesourcing mode
choice. Experiments on a large-scale, multi-year travel survey show that SAPA
significantly outperforms state-of-the-art baselines, improving ridesourcing
choice predictions by up to 75.9% in terms of PR-AUC on a held-out test set.
This study provides a powerful tool for accurately predicting ridesourcing mode
choices, and provides a methodology that is readily transferable to various
applications.

</details>


### [99] [An Outcome-Based Educational Recommender System](https://arxiv.org/abs/2509.18186)
*Nursultan Askarbekuly,Timur Fayzrakhmanov,Sladjan Babarogić,Ivan Luković*

Main category: cs.AI

TL;DR: 介绍了一种名为 OBER 的基于结果的教育推荐系统，该系统将学习成果和评估项目直接嵌入到数据模式中，因此任何算法都可以根据其培养的掌握程度进行评估。


<details>
  <summary>Details</summary>
Motivation: 大多数教育推荐系统都基于点击或基于评级的相关性进行调整和判断，但其真正的教学影响尚不清楚。因此，需要一种能够评估教学效果的推荐系统。

Method: OBER 使用极简的实体关系模型、日志驱动的掌握公式和插件架构。通过为期两周的随机分组测试，在非正式领域的电子学习系统中集成，对超过 5700 名学习者进行了评估，采用了三种方法：固定专家轨迹、协同过滤 (CF) 和基于知识 (KB) 的过滤。

Result: CF 最大化了保留率，但固定路径实现了最高的掌握度。

Conclusion: OBER 从相同的日志中得出业务、相关性和学习指标，因此从业者可以在没有额外测试开销的情况下，权衡相关性和参与度与结果掌握度。该框架与方法无关，并且可以轻松扩展到未来的自适应或上下文感知推荐器。

Abstract: Most educational recommender systems are tuned and judged on click- or
rating-based relevance, leaving their true pedagogical impact unclear. We
introduce OBER-an Outcome-Based Educational Recommender that embeds learning
outcomes and assessment items directly into the data schema, so any algorithm
can be evaluated on the mastery it fosters. OBER uses a minimalist
entity-relation model, a log-driven mastery formula, and a plug-in
architecture. Integrated into an e-learning system in non-formal domain, it was
evaluated trough a two-week randomized split test with over 5 700 learners
across three methods: fixed expert trajectory, collaborative filtering (CF),
and knowledge-based (KB) filtering. CF maximized retention, but the fixed path
achieved the highest mastery. Because OBER derives business, relevance, and
learning metrics from the same logs, it lets practitioners weigh relevance and
engagement against outcome mastery with no extra testing overhead. The
framework is method-agnostic and readily extensible to future adaptive or
context-aware recommenders.

</details>


### [100] [From latent factors to language: a user study on LLM-generated explanations for an inherently interpretable matrix-based recommender system](https://arxiv.org/abs/2509.18980)
*Maxime Manderlier,Fabian Lecron,Olivier Vu Thanh,Nicolas Gillis*

Main category: cs.AI

TL;DR: 研究大型语言模型 (LLM) 是否可以从数学上可解释的推荐模型生成有效的、面向用户的解释。


<details>
  <summary>Details</summary>
Motivation: 许多可解释人工智能 (XAI) 工作依赖于自动评估指标，但这些指标通常无法捕捉用户的实际需求和看法。为了弥补这一缺陷，本文采用以用户为中心的方法。

Method: 使用精心设计的 LLM 提示将该结构转换为自然语言解释。进行了一项有 326 名参与者的研究，他们评估了解释在五个关键维度（透明度、有效性、说服力、信任度和满意度）以及推荐本身方面的质量。

Result: 所有解释类型都普遍受到好评，策略之间的统计差异不大。用户评论进一步强调了参与者如何对每种类型的解释做出反应，提供了定量结果之外的补充见解。

Conclusion: LLM 可以从数学上可解释的推荐模型生成有效的、面向用户的解释。

Abstract: We investigate whether large language models (LLMs) can generate effective,
user-facing explanations from a mathematically interpretable recommendation
model. The model is based on constrained matrix factorization, where user types
are explicitly represented and predicted item scores share the same scale as
observed ratings, making the model's internal representations and predicted
scores directly interpretable. This structure is translated into natural
language explanations using carefully designed LLM prompts. Many works in
explainable AI rely on automatic evaluation metrics, which often fail to
capture users' actual needs and perceptions. In contrast, we adopt a
user-centered approach: we conduct a study with 326 participants who assessed
the quality of the explanations across five key dimensions-transparency,
effectiveness, persuasion, trust, and satisfaction-as well as the
recommendations themselves.To evaluate how different explanation strategies are
perceived, we generate multiple explanation types from the same underlying
model, varying the input information provided to the LLM. Our analysis reveals
that all explanation types are generally well received, with moderate
statistical differences between strategies. User comments further underscore
how participants react to each type of explanation, offering complementary
insights beyond the quantitative results.

</details>


### [101] [MMCD: Multi-Modal Collaborative Decision-Making for Connected Autonomy with Knowledge Distillation](https://arxiv.org/abs/2509.18198)
*Rui Liu,Zikang Wang,Peng Gao,Yu Shen,Pratap Tokekar,Ming Lin*

Main category: cs.AI

TL;DR: 提出了一种新的多模态协同决策框架（MMCD），用于提高互联自动驾驶在复杂环境中的安全性。


<details>
  <summary>Details</summary>
Motivation: 在事故多发环境中，单个车辆的传感器范围有限，容易发生事故。现有的多车辆互联系统和多模态方法通常假设所有数据模态和互联车辆在训练和测试期间都可用，这在实际中是不切实际的。

Method: 该框架融合了来自自我车辆和协作车辆的多模态观测，以增强在挑战性条件下的决策能力。为了确保在测试期间某些数据模态不可用时也能保持鲁棒性，提出了一种基于交叉模态知识蒸馏的师生模型结构。

Result: 在地面车辆互联自动驾驶和无人机-地面车辆协作实验中，该方法将驾驶安全性提高了20.7%，超过了现有最佳基线。

Conclusion: 该方法能够有效地检测潜在事故并做出安全的驾驶决策，提高了互联自动驾驶在复杂环境中的安全性。

Abstract: Autonomous systems have advanced significantly, but challenges persist in
accident-prone environments where robust decision-making is crucial. A single
vehicle's limited sensor range and obstructed views increase the likelihood of
accidents. Multi-vehicle connected systems and multi-modal approaches,
leveraging RGB images and LiDAR point clouds, have emerged as promising
solutions. However, existing methods often assume the availability of all data
modalities and connected vehicles during both training and testing, which is
impractical due to potential sensor failures or missing connected vehicles. To
address these challenges, we introduce a novel framework MMCD (Multi-Modal
Collaborative Decision-making) for connected autonomy. Our framework fuses
multi-modal observations from ego and collaborative vehicles to enhance
decision-making under challenging conditions. To ensure robust performance when
certain data modalities are unavailable during testing, we propose an approach
based on cross-modal knowledge distillation with a teacher-student model
structure. The teacher model is trained with multiple data modalities, while
the student model is designed to operate effectively with reduced modalities.
In experiments on $\textit{connected autonomous driving with ground vehicles}$
and $\textit{aerial-ground vehicles collaboration}$, our method improves
driving safety by up to ${\it 20.7}\%$, surpassing the best-existing baseline
in detecting potential accidents and making safe driving decisions. More
information can be found on our website https://ruiiu.github.io/mmcd.

</details>


### [102] [Change in Quantitative Bipolar Argumentation: Sufficient, Necessary, and Counterfactual Explanations](https://arxiv.org/abs/2509.18215)
*Timotheus Kampik,Kristijonas Čyras,José Ruiz Alarcón*

Main category: cs.AI

TL;DR: 本文提出了一种形式化的方法来解释定量双极论证框架（QBAF）中推理的变化。


<details>
  <summary>Details</summary>
Motivation: 在从 QBAF 中得出结论并更新 QBAF 以再次得出结论（等等）时，我们的方法会追踪变化——我们称之为强度不一致——语义在一些感兴趣的论点（称为主题论点）上建立的论点强度上的偏序。

Method: 我们追踪强度不一致的原因到特定的论点，然后这些论点作为解释。我们确定了强度不一致的充分、必要和反事实解释，并表明强度不一致解释的存在当且仅当更新导致强度不一致时。

Result: 我们定义了一种基于启发式的方法来促进寻找强度不一致解释，为此我们还提供了一个实现。

Conclusion: 强度不一致解释的存在当且仅当更新导致强度不一致时。

Abstract: This paper presents a formal approach to explaining change of inference in
Quantitative Bipolar Argumentation Frameworks (QBAFs). When drawing conclusions
from a QBAF and updating the QBAF to then again draw conclusions (and so on),
our approach traces changes -- which we call strength inconsistencies -- in the
partial order over argument strengths that a semantics establishes on some
arguments of interest, called topic arguments. We trace the causes of strength
inconsistencies to specific arguments, which then serve as explanations. We
identify sufficient, necessary, and counterfactual explanations for strength
inconsistencies and show that strength inconsistency explanations exist if and
only if an update leads to strength inconsistency. We define a heuristic-based
approach to facilitate the search for strength inconsistency explanations, for
which we also provide an implementation.

</details>


### [103] [nDNA -- the Semantic Helix of Artificial Cognition](https://arxiv.org/abs/2509.18216)
*Amitava Das*

Main category: cs.AI

TL;DR: 这篇论文提出了 Neural DNA (nDNA) 的概念，它是一种语义基因型表示，通过信念的内在几何结构来捕捉模型的潜在认知特征。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能基础模型能力的增长，模型内部认知特征的塑造因素成为一个更深层次的问题。传统的基准测试只能衡量模型的行为，而模型的本质在于其潜在的几何结构。

Method: 该论文从潜在几何的三个维度合成 nDNA：谱曲率（揭示跨层概念流的曲率）、热力学长度（量化通过层进行表征转换所需的语义努力）和信念向量场（描绘引导模型信念方向的语义扭转场）。

Result: nDNA 可以编码祖先、突变和语义继承，这些信息可以在微调和对齐的痕迹、文化印记和架构漂移中找到。

Conclusion: 该论文开创了一个新的领域：神经基因组学，将模型视为具有可追溯内部认知的数字语义有机体。通过 nDNA，可以追踪模型的谱系，测量检查点之间的继承，检测漂移，并研究人工认知的演变，从而比较模型、诊断风险并管理变化。

Abstract: As AI foundation models grow in capability, a deeper question emerges: What
shapes their internal cognitive identity -- beyond fluency and output?
Benchmarks measure behavior, but the soul of a model resides in its latent
geometry. In this work, we propose Neural DNA (nDNA) as a semantic-genotypic
representation that captures this latent identity through the intrinsic
geometry of belief. At its core, nDNA is synthesized from three principled and
indispensable dimensions of latent geometry: spectral curvature, which reveals
the curvature of conceptual flow across layers; thermodynamic length, which
quantifies the semantic effort required to traverse representational
transitions through layers; and belief vector field, which delineates the
semantic torsion fields that guide a model's belief directional orientations.
Like biological DNA, it encodes ancestry, mutation, and semantic inheritance,
found in finetuning and alignment scars, cultural imprints, and architectural
drift. In naming it, we open a new field: Neural Genomics, where models are not
just tools, but digital semantic organisms with traceable inner cognition.
  Modeling statement. We read AI foundation models as semantic fluid--dynamics:
meaning is transported through layers like fluid in a shaped conduit; nDNA is
the physics-grade readout of that flow -- a geometry-first measure of how
meaning is bent, paid for, and pushed -- yielding a stable, coordinate-free
neural DNA fingerprint tied to on-input behavior; with this fingerprint we
cross into biology: tracing lineages across pretraining, fine-tuning,
alignment, pruning, distillation, and merges; measuring inheritance between
checkpoints; detecting drift as traits shift under new data or objectives; and,
ultimately, studying the evolution of artificial cognition to compare models,
diagnose risks, and govern change over time.

</details>


### [104] [Similarity Field Theory: A Mathematical Framework for Intelligence](https://arxiv.org/abs/2509.18218)
*Kei-Sing Ng*

Main category: cs.AI

TL;DR: Similarity Field Theory: a mathematical framework for formalizing similarity among entities and their evolution.


<details>
  <summary>Details</summary>
Motivation: To establish that persisting and transforming similarity relations form the structural basis of any comprehensible dynamic system.

Method: The paper defines similarity fields, system evolution, concepts as entities inducing fibers, and a generative operator. It formalizes a generative definition of intelligence within this framework.

Result: The paper proves two theorems: (i) asymmetry blocks mutual inclusion; and (ii) stability requires either an anchor coordinate or eventual confinement within a level set of f.

Conclusion: Similarity Field Theory offers a foundational language for characterizing, comparing, and constructing intelligent systems. The framework allows us to interpret large language models and use them as experimental probes into societal cognition.

Abstract: We posit that persisting and transforming similarity relations form the
structural basis of any comprehensible dynamic system. This paper introduces
Similarity Field Theory, a mathematical framework that formalizes the
principles governing similarity values among entities and their evolution. We
define: (1) a similarity field $S: U \times U \to [0,1]$ over a universe of
entities $U$, satisfying reflexivity $S(E,E)=1$ and treated as a directed
relational field (asymmetry and non-transitivity are allowed); (2) the
evolution of a system through a sequence $Z_p = (X_p, S^{(p)})$ indexed by
$p=0,1,2,\ldots$; (3) concepts $K$ as entities that induce fibers
$F_{\alpha}(K) = { E \in U \mid S(E,K) \ge \alpha }$, i.e., superlevel sets of
the unary map $S_K(E) := S(E,K)$; and (4) a generative operator $G$ that
produces new entities. Within this framework, we formalize a generative
definition of intelligence: an operator $G$ is intelligent with respect to a
concept $K$ if, given a system containing entities belonging to the fiber of
$K$, it generates new entities that also belong to that fiber. Similarity Field
Theory thus offers a foundational language for characterizing, comparing, and
constructing intelligent systems. We prove two theorems: (i) asymmetry blocks
mutual inclusion; and (ii) stability requires either an anchor coordinate or
eventual confinement within a level set of $f$. These results ensure that the
evolution of similarity fields is both constrained and interpretable,
culminating in an exploration of how the framework allows us to interpret large
language models and use them as experimental probes into societal cognition.

</details>


### [105] [Multimodal Health Risk Prediction System for Chronic Diseases via Vision-Language Fusion and Large Language Models](https://arxiv.org/abs/2509.18221)
*Dingxin Lu,Shurui Wu,Xinyi Huang*

Main category: cs.AI

TL;DR: 提出了VL-RiskFormer，一个用于预测个体健康风险的统一多模态AI框架。


<details>
  <summary>Details</summary>
Motivation: 随着慢性疾病的全球负担日益加重，以及多模态和异构临床数据（医学影像、自由文本记录、可穿戴传感器流等）的出现，迫切需要一个能够主动预测个体健康风险的统一多模态AI框架。

Method: VL-RiskFormer是一个分层堆叠的视觉-语言多模态Transformer，其顶层嵌入了一个大型语言模型（LLM）推理头。该系统基于现有视觉-语言模型的双流架构，具有四个关键创新：(i) 使用动量更新编码器和去偏InfoNCE损失，对放射影像、眼底图和可穿戴设备照片与相应的临床叙述进行跨模态比较和细粒度对齐的预训练；(ii) 通过自适应时间间隔位置编码，将不规则访问序列集成到因果Transformer解码器中的时间融合块；(iii) 一种疾病本体图适配器，将ICD-10代码注入到视觉和文本通道的层中，并在图注意力机制的帮助下推断合并症模式。

Result: 在MIMIC-IV纵向队列上，VL-RiskFormer实现了0.90的平均AUROC，预期校准误差为2.7%。

Conclusion: VL-RiskFormer是一个有效预测个体健康风险的统一多模态AI框架。

Abstract: With the rising global burden of chronic diseases and the multimodal and
heterogeneous clinical data (medical imaging, free-text recordings, wearable
sensor streams, etc.), there is an urgent need for a unified multimodal AI
framework that can proactively predict individual health risks. We propose
VL-RiskFormer, a hierarchical stacked visual-language multimodal Transformer
with a large language model (LLM) inference head embedded in its top layer. The
system builds on the dual-stream architecture of existing visual-linguistic
models (e.g., PaLM-E, LLaVA) with four key innovations: (i) pre-training with
cross-modal comparison and fine-grained alignment of radiological images,
fundus maps, and wearable device photos with corresponding clinical narratives
using momentum update encoders and debiased InfoNCE losses; (ii) a time fusion
block that integrates irregular visit sequences into the causal Transformer
decoder through adaptive time interval position coding; (iii) a disease
ontology map adapter that injects ICD-10 codes into visual and textual channels
in layers and infers comorbid patterns with the help of a graph attention
mechanism. On the MIMIC-IV longitudinal cohort, VL-RiskFormer achieved an
average AUROC of 0.90 with an expected calibration error of 2.7 percent.

</details>


### [106] [From "What to Eat?" to Perfect Recipe: ChefMind's Chain-of-Exploration for Ambiguous User Intent in Recipe Recommendation](https://arxiv.org/abs/2509.18226)
*Yu Fu,Linyue Cai,Ruoyu Wu,Yong Zhao*

Main category: cs.AI

TL;DR: ChefMind is a new recipe recommendation system that combines multiple techniques to improve accuracy, relevance, and clarity.


<details>
  <summary>Details</summary>
Motivation: Personalized recipe recommendation systems struggle with fuzzy user intent, semantic accuracy, and detail coverage.

Method: ChefMind uses a hybrid architecture combining Chain of Exploration (CoE), Knowledge Graph (KG), Retrieval-Augmented Generation (RAG), and a Large Language Model (LLM).

Result: ChefMind outperforms LLM-only, KG-only, and RAG-only baselines in accuracy, relevance, completeness, and clarity, achieving an average score of 8.7 versus 6.4-6.7 for ablation models. It also reduces unprocessed queries to 1.6%.

Conclusion: ChefMind is a robust and effective recipe recommendation system that can handle fuzzy demands and provide accurate, relevant, and clear recommendations.

Abstract: Personalized recipe recommendation faces challenges in handling fuzzy user
intent, ensuring semantic accuracy, and providing sufficient detail coverage.
We propose ChefMind, a hybrid architecture combining Chain of Exploration
(CoE), Knowledge Graph (KG), Retrieval-Augmented Generation (RAG), and a Large
Language Model (LLM). CoE refines ambiguous queries into structured conditions,
KG offers semantic reasoning and interpretability, RAG supplements contextual
culinary details, and LLM integrates outputs into coherent recommendations. We
evaluate ChefMind on the Xiachufang dataset and manually annotated queries,
comparing it with LLM-only, KG-only, and RAG-only baselines. Results show that
ChefMind achieves superior performance in accuracy, relevance, completeness,
and clarity, with an average score of 8.7 versus 6.4-6.7 for ablation models.
Moreover, it reduces unprocessed queries to 1.6%, demonstrating robustness in
handling fuzzy demands.

</details>


### [107] [An N-Plus-1 GPT Agency for Critical Solution of Mechanical Engineering Analysis Problems](https://arxiv.org/abs/2509.18229)
*Anthony Patera,Rohan Abeyaratne*

Main category: cs.AI

TL;DR: GPT 在机械工程分析问题中可能产生有缺陷的解决方案，因此引入 N-Plus-1 GPT 机构以提高可靠性。


<details>
  <summary>Details</summary>
Motivation: GPT 在解决机械工程问题时存在不稳定性，成功率仅为 85%，不适合直接应用于教育或工程实践。

Method: 提出一种“N+1”GPT 机构，通过启动 N 个 Agent Solve 实例生成多个问题解决方案，然后使用 Agent Compare 总结和比较这些方案，并提供推荐的解决方案。

Result: 对于单次求解成功概率大于 1/2 的问题，该机构的主要解决方案很可能对应于正确的解决方案。与 Grok Heavy 相比，设计和性能相似，但更注重透明性和教学价值。

Conclusion: N+1 GPT 机构可以提高机械工程问题解决方案的可靠性，并具有透明性和教学价值。

Abstract: Generative AI, and specifically GPT, can produce a remarkable solution to a
mechanical engineering analysis problem - but also, on occasion, a flawed
solution. For example, an elementary mechanics problem is solved flawlessly in
one GPT instance and incorrectly in a subsequent GPT instance, with a success
probability of only 85%. This unreliability renders "out-of-the-box" GPT
unsuitable for deployment in education or engineering practice. We introduce an
"N-Plus-1" GPT Agency for Initial (Low-Cost) Analysis of mechanical engineering
Problem Statements. Agency first launches N instantiations of Agent Solve to
yield N independent Proposed Problem Solution Realizations; Agency then invokes
Agent Compare to summarize and compare the N Proposed Problem Solution
Realizations and to provide a Recommended Problem Solution. We argue from
Condorcet's Jury Theorem that, for a Problem Statement characterized by
per-Solve success probability greater than 1/2 (and N sufficiently large), the
Predominant (Agent Compare) Proposed Problem Solution will, with high
probability, correspond to a Correct Proposed Problem Solution. Furthermore,
Agent Compare can also incorporate aspects of Secondary (Agent Compare)
Proposed Problem Solutions, in particular when the latter represent alternative
Problem Statement interpretations - different Mathematical Models - or
alternative Mathematical Solution Procedures. Comparisons to Grok Heavy, a
commercial multi-agent model, show similarities in design and performance, but
also important differences in emphasis: our Agency focuses on transparency and
pedagogical value.

</details>


### [108] [Towards General Computer Control with Hierarchical Agents and Multi-Level Action Spaces](https://arxiv.org/abs/2509.18230)
*Zihan Dong,Xinyu Fan,Zixiang Tang,Yunqing Li*

Main category: cs.AI

TL;DR: 提出了一种轻量级的分层强化学习框架ComputerAgent，用于控制桌面应用程序。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大型语言模型（MLLMs）在控制桌面应用程序时存在推理延迟高、样本效率低、无法在设备上部署等问题。

Method: 该框架将操作系统控制建模为两级选项过程（管理器和子策略），采用三模态状态编码器处理视觉和上下文多样性，集成元动作和早停机制以减少浪费的交互，并使用紧凑的视觉骨干网络和小型策略网络。

Result: 在135个真实桌面任务中，ComputerAgent在简单任务上达到92.1%的成功率，在困难任务上达到58.8%的成功率，与200B参数的MLLM基线相当，同时模型尺寸减少了四个数量级，推理时间减半。

Conclusion: 分层强化学习为基于MLLM的计算机控制自动化提供了一种实用且可扩展的替代方案。

Abstract: Controlling desktop applications via software remains a fundamental yet
under-served problem. Existing multi-modal large language models (MLLMs) ingest
screenshots and task instructions to generate keystrokes and mouse events, but
they suffer from prohibitive inference latency, poor sample efficiency on
long-horizon sparse-reward tasks, and infeasible on-device deployment. We
introduce a lightweight hierarchical reinforcement learning framework,
ComputerAgent, that formulates OS control as a two-level option process
(manager and subpolicy), employs a triple-modal state encoder (screenshot, task
ID, numeric state) to handle visual and contextual diversity, integrates
meta-actions with an early-stop mechanism to reduce wasted interactions, and
uses a compact vision backbone plus small policy networks for on-device
inference (15M parameters). On a suite of 135 real-world desktop tasks,
ComputerAgent attains 92.1% success on simple tasks (<8 steps) and 58.8% on
hard tasks (>=8 steps), matching or exceeding 200B-parameter MLLM baselines on
simple scenarios while reducing model size by over four orders of magnitude and
halving inference time. These results demonstrate that hierarchical RL offers a
practical, scalable alternative to monolithic MLLM-based automation for
computer control.

</details>


### [109] [The Illusion of Readiness: Stress Testing Large Frontier Models on Multimodal Medical Benchmarks](https://arxiv.org/abs/2509.18234)
*Yu Gu,Jingjing Fu,Xiaodong Liu,Jeya Maria Jose Valanarasu,Noel Codella,Reuben Tan,Qianchu Liu,Ying Jin,Sheng Zhang,Jinyu Wang,Rui Wang,Lei Song,Guanghui Qin,Naoto Usuyama,Cliff Wong,Cheng Hao,Hohin Lee,Praneeth Sanapathi,Sarah Hilado,Bian Jiang,Javier Alvarez-Valle,Mu Wei,Jianfeng Gao,Eric Horvitz,Matt Lungren,Hoifung Poon,Paul Vozila*

Main category: cs.AI

TL;DR: 大型前沿模型在医疗基准测试中取得了很高的分数，但压力测试表明，它们在缺乏关键输入时也能猜对答案，在提示发生微小变化时会改变答案，并且会编造看似合理但有缺陷的推理。这些问题暴露了当前的基准测试奖励的是应试技巧，而不是医学理解。


<details>
  <summary>Details</summary>
Motivation: 评估六个旗舰模型在六个广泛使用的基准测试中的表现，发现高分掩盖了脆弱性和捷径学习。

Method: 通过临床医生指导的评估标准评估，表明基准测试在真正衡量的内容方面差异很大，但可以互换使用，掩盖了失败模式。

Result: 医疗基准测试分数不能直接反映现实世界的准备情况。

Conclusion: 如果我们希望人工智能赢得医疗保健领域的信任，我们必须要求更多，而不能仅仅是排行榜上的胜利，并且必须让系统对稳健性、合理的推理以及与实际医疗需求的协调负责。

Abstract: Large frontier models like GPT-5 now achieve top scores on medical
benchmarks. But our stress tests tell a different story. Leading systems often
guess correctly even when key inputs like images are removed, flip answers
under trivial prompt changes, and fabricate convincing yet flawed reasoning.
These aren't glitches; they expose how today's benchmarks reward test-taking
tricks over medical understanding. We evaluate six flagship models across six
widely used benchmarks and find that high leaderboard scores hide brittleness
and shortcut learning. Through clinician-guided rubric evaluation, we show that
benchmarks vary widely in what they truly measure yet are treated
interchangeably, masking failure modes. We caution that medical benchmark
scores do not directly reflect real-world readiness. If we want AI to earn
trust in healthcare, we must demand more than leaderboard wins and must hold
systems accountable for robustness, sound reasoning, and alignment with real
medical demands.

</details>


### [110] [Evaluating the Safety and Skill Reasoning of Large Reasoning Models Under Compute Constraints](https://arxiv.org/abs/2509.18382)
*Adarsha Balaji,Le Chen,Rajeev Thakur,Franck Cappello,Sandeep Madireddy*

Main category: cs.AI

TL;DR: 本文探讨了在推理语言模型中，通过限制推理长度和模型量化来降低计算需求，并研究了这些方法对模型安全性的影响。


<details>
  <summary>Details</summary>
Motivation: 提高推理语言模型的性能通常需要生成更长的CoT序列，但这会显著增加计算成本。因此，本文旨在研究如何在计算资源有限的情况下，优化推理模型的性能和安全性。

Method: 本文研究了两种计算约束策略：(1) 推理长度约束，通过基于LCPO的强化学习方法微调模型，以满足用户定义的CoT推理长度；(2) 模型量化，以在用户定义的计算约束内最大化CoT序列的生成。

Result: 本文研究了计算效率和模型安全性之间的权衡。

Conclusion: 通过计算约束策略，可以在一定程度上降低推理语言模型的计算需求，并在计算效率和模型安全性之间取得平衡。

Abstract: Test-time compute scaling has demonstrated the ability to improve the
performance of reasoning language models by generating longer chain-of-thought
(CoT) sequences. However, this increase in performance comes with a significant
increase in computational cost. In this work, we investigate two compute
constraint strategies: (1) reasoning length constraint and (2) model
quantization, as methods to reduce the compute demand of reasoning models and
study their impact on their safety performance. Specifically, we explore two
approaches to apply compute constraints to reasoning models: (1) fine-tuning
reasoning models using a length controlled policy optimization (LCPO) based
reinforcement learning method to satisfy a user-defined CoT reasoning length,
and (2) applying quantization to maximize the generation of CoT sequences
within a user-defined compute constraint. Furthermore, we study the trade-off
between the computational efficiency and the safety of the model.

</details>


### [111] [Gödel Test: Can Large Language Models Solve Easy Conjectures?](https://arxiv.org/abs/2509.18383)
*Moran Feldman,Amin Karbasi*

Main category: cs.AI

TL;DR: 评估大型语言模型解决高级数学领域中新的、简单的猜想的能力。提出了哥德尔测试，通过评估模型是否能为先前未解决的简单猜想生成正确的证明。使用GPT-5对组合优化中的五个猜想进行了测试。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在高中和本科数学竞赛中表现出强大的结果，但尚不清楚它们是否能解决更高级数学领域中新的、简单的猜想。

Method: 使用GPT-5对组合优化中的五个猜想进行了测试。对于每个问题，提供了猜想产生的一到两篇来源论文，隐瞒了自己的猜想，然后详细评估了模型的推理。

Result: 在三个较容易的问题上，GPT-5产生了几乎正确的解决方案；对于问题2，它甚至推导出了不同的近似保证，经检查，这反驳了我们的猜想，同时提供了有效的解决方案。该模型在问题4上失败了，该问题需要结合两篇论文的结果。在问题5上，GPT-5提出了与我们相同的算法，但在分析中失败了，这表明证明比预期的更具挑战性。

Conclusion: 结果表明，在常规推理方面取得了有意义的进展，偶尔会有原创性的闪光，并且在需要跨论文综合时存在明显的局限性。GPT-5可能代表了前沿模型最终通过哥德尔测试的早期步骤。

Abstract: Recent announcements from frontier AI model labs have highlighted strong
results on high-school and undergraduate math competitions. Yet it remains
unclear whether large language models can solve new, simple conjectures in more
advanced areas of mathematics. We propose the G\"odel Test: evaluating whether
a model can produce correct proofs for very simple, previously unsolved
conjectures. To this end, we study the performance of GPT-5 on five conjectures
in combinatorial optimization. For each problem, we provided one or two source
papers from which the conjecture arose, withheld our own conjecture, and then
assessed the model's reasoning in detail. On the three easier problems, GPT-5
produced nearly correct solutions; for Problem 2 it even derived a different
approximation guarantee that, upon checking, refuted our conjecture while
providing a valid solution. The model failed on Problem 4, which required
combining results from two papers. On Problem 5, a harder case without a
validated conjecture, GPT-5 proposed the same algorithm we had in mind but
failed in the analysis, suggesting the proof is more challenging than expected.
Although our sample is small, the results point to meaningful progress on
routine reasoning, occasional flashes of originality, and clear limitations
when cross-paper synthesis is required. GPT-5 may represent an early step
toward frontier models eventually passing the G\"odel Test.

</details>


### [112] [ATLAS: Benchmarking and Adapting LLMs for Global Trade via Harmonized Tariff Code Classification](https://arxiv.org/abs/2509.18400)
*Pritish Yuvraj,Siva Devarakonda*

Main category: cs.AI

TL;DR: 本文提出了一个协调关税表 (HTS) 代码分类的新基准，该基准对于全球贸易至关重要但很少受到机器学习界的关注。作者使用美国海关裁决在线搜索系统 (CROSS) 导出的数据，并评估了领先的 LLM。结果表明，经过微调的 Atlas 模型在 10 位数分类中达到 40% 的完全正确率，在 6 位数分类中达到 57.5% 的正确率，优于 GPT-5-Thinking 和 Gemini-2.5-Pro-Thinking。Atlas 模型也更便宜且可以自托管。


<details>
  <summary>Details</summary>
Motivation: HTS 代码的准确分类是全球贸易中的关键瓶颈，但机器学习界对此关注不足。错误分类可能导致货物运输中断。因此，建立一个 HTS 代码分类的基准非常重要。

Method: 本文从美国海关裁决在线搜索系统 (CROSS) 中提取数据，构建了 HTS 代码分类的基准数据集。然后，评估了领先的 LLM 在该基准上的性能，并通过微调 Atlas 模型来提高准确率。

Result: 经过微调的 Atlas 模型在 10 位数分类中达到 40% 的完全正确率，在 6 位数分类中达到 57.5% 的正确率，比 GPT-5-Thinking 提高了 15 个百分点，比 Gemini-2.5-Pro-Thinking 提高了 27.5 个百分点。此外，Atlas 的成本也远低于 GPT-5-Thinking 和 Gemini-2.5-Pro-Thinking。

Conclusion: 本文提出了 HTS 代码分类的新基准，并发布了数据集和模型，旨在将 HTS 分类定位为一个新的社区基准任务，并邀请未来的研究关注检索、推理和对齐。

Abstract: Accurate classification of products under the Harmonized Tariff Schedule
(HTS) is a critical bottleneck in global trade, yet it has received little
attention from the machine learning community. Misclassification can halt
shipments entirely, with major postal operators suspending deliveries to the
U.S. due to incomplete customs documentation. We introduce the first benchmark
for HTS code classification, derived from the U.S. Customs Rulings Online
Search System (CROSS). Evaluating leading LLMs, we find that our fine-tuned
Atlas model (LLaMA-3.3-70B) achieves 40 percent fully correct 10-digit
classifications and 57.5 percent correct 6-digit classifications, improvements
of 15 points over GPT-5-Thinking and 27.5 points over Gemini-2.5-Pro-Thinking.
Beyond accuracy, Atlas is roughly five times cheaper than GPT-5-Thinking and
eight times cheaper than Gemini-2.5-Pro-Thinking, and can be self-hosted to
guarantee data privacy in high-stakes trade and compliance workflows. While
Atlas sets a strong baseline, the benchmark remains highly challenging, with
only 40 percent 10-digit accuracy. By releasing both dataset and model, we aim
to position HTS classification as a new community benchmark task and invite
future work in retrieval, reasoning, and alignment.

</details>


### [113] [Instruction-Following Evaluation in Function Calling for Large Language Models](https://arxiv.org/abs/2509.18420)
*Nikolai Skripko*

Main category: cs.AI

TL;DR: IFEval-FC是一个新的基准，用于评估大型语言模型在函数调用中对格式指令的遵循程度。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试评估参数的正确性，但没有测试对参数描述中嵌入的格式指令的遵守情况。

Method: IFEval-FC通过在JSON模式描述中直接编码可验证的格式来评估函数调用中精确的指令遵循，例如指定一个值不能包含标点符号。

Result: 结果表明，即使是最先进的专有模型，包括GPT-5和Claude 4.1 Opus，也经常未能遵循基本的格式规则。

Conclusion: IFEval-FC揭示了现有模型在函数调用中遵循格式指令方面的不足，这对于实际的agent系统是一个重要的局限性。

Abstract: Function calling is a core capability of large language models, essential for
AI agents. Existing benchmarks such as the Berkeley Function Calling
Leaderboard (BFCL), tau^2-Bench (arXiv:2506.07982), and ACEBench
(arXiv:2501.12851) evaluate argument correctness but do not test adherence to
format instructions embedded in parameter descriptions, such as enclosing
values in double quotes or using ISO date formats.
  We introduce IFEval-FC, a benchmark inspired by IFEval (arXiv:2311.07911)
that assesses precise instruction following in function calling. IFEval-FC
encodes verifiable formats directly within JSON schema descriptions, for
example specifying that a value must not contain punctuation. It includes 750
test cases, each consisting of a function with an embedded format for one of
its input parameters and a corresponding user query. Evaluation is fully
algorithmic, ensuring objectivity, reproducibility, and scalability.
  Our results show that even state-of-the-art proprietary models, including
GPT-5 and Claude 4.1 Opus, frequently fail to follow basic formatting rules,
highlighting a practical limitation for real-world agent systems. The complete
codebase and data are publicly available at
https://github.com/Skripkon/IFEval-FC.

</details>


### [114] [FERA: Foil Fencing Referee Assistant Using Pose-Based Multi-Label Move Recognition and Rule Reasoning](https://arxiv.org/abs/2509.18527)
*Ziwen Chen,Zhong Wang*

Main category: cs.AI

TL;DR: 提出了一种用于击剑运动的AI裁判原型系统，名为FERA，旨在解决击剑裁判中的主观判断、人为错误和资源限制等问题。


<details>
  <summary>Details</summary>
Motivation: 解决击剑裁判中主观判断、人为错误、资源限制等问题。

Method: 利用姿势识别技术从视频中提取2D关节位置，计算运动学特征，并使用Transformer模型进行多标签动作和剑刃分类。同时，使用精馏语言模型编码优先权规则，以确定优先级和评分。

Result: 在有限的手动标记数据下，5折交叉验证实现了0.549的平均macro-F1 score，优于多个基线模型。

Conclusion: 结果表明，该系统在自动化击剑裁判辅助方面具有潜力，并为AI在击剑领域的应用（如教练）提供了新的机会。

Abstract: The sport of fencing, like many other sports, faces challenges in refereeing:
subjective calls, human errors, bias, and limited availability in practice
environments. We present FERA (Fencing Referee Assistant), a prototype AI
referee for foil fencing which integrates pose-based multi-label action
recognition and rule-based reasoning. FERA extracts 2D joint positions from
video, normalizes them, computes a 101-dimensional kinematic feature set, and
applies a Transformer for multi-label move and blade classification. To
determine priority and scoring, FERA applies a distilled language model with
encoded right-of-way rules, producing both a decision and an explanation for
each exchange. With limited hand-labeled data, a 5-fold cross-validation
achieves an average macro-F1 score of 0.549, outperforming multiple baselines,
including a Temporal Convolutional Network (TCN), BiLSTM, and a vanilla
Transformer. While not ready for deployment, these results demonstrate a
promising path towards automated referee assistance in foil fencing and new
opportunities for AI applications, such as coaching in the field of fencing.

</details>


### [115] [LLMZ+: Contextual Prompt Whitelist Principles for Agentic LLMs](https://arxiv.org/abs/2509.18557)
*Tom Pawelek,Raj Patel,Charlotte Crowell,Noorbakhsh Amiri,Sudip Mittal,Shahram Rahimi,Andy Perkins*

Main category: cs.AI

TL;DR: 本文提出了一种名为LLMZ+的新方法，通过实施提示白名单来增强Agentic LLM的安全性，只允许上下文相关的安全消息与Agentic LLM交互。


<details>
  <summary>Details</summary>
Motivation: 传统的Agentic AI由于拥有对数据源和API工具的特权访问，因此成为潜在攻击者的高价值目标。现有的防御机制主要依赖于检测恶意意图，但本文提出了一种替代方法。

Method: 本文提出LLMZ+方法，通过提示白名单机制，保证所有外部用户和LLM之间的交流都符合预定义的使用场景和操作边界。

Result: 实验结果表明，LLMZ+能够有效抵抗常见的越狱提示，同时不中断合法的业务通信，并且可以将假阳性和假阴性率都降低到0。

Conclusion: LLMZ+通过简化安全框架，增强长期弹性，并减少维持LLM信息安全所需的资源，从而提供强大的安全性。

Abstract: Compared to traditional models, agentic AI represents a highly valuable
target for potential attackers as they possess privileged access to data
sources and API tools, which are traditionally not incorporated into classical
agents. Unlike a typical software application residing in a Demilitarized Zone
(DMZ), agentic LLMs consciously rely on nondeterministic behavior of the AI
(only defining a final goal, leaving the path selection to LLM). This
characteristic introduces substantial security risk to both operational
security and information security. Most common existing defense mechanism rely
on detection of malicious intent and preventing it from reaching the LLM agent,
thus protecting against jailbreak attacks such as prompt injection. In this
paper, we present an alternative approach, LLMZ+, which moves beyond
traditional detection-based approaches by implementing prompt whitelisting.
Through this method, only contextually appropriate and safe messages are
permitted to interact with the agentic LLM. By leveraging the specificity of
context, LLMZ+ guarantees that all exchanges between external users and the LLM
conform to predefined use cases and operational boundaries. Our approach
streamlines the security framework, enhances its long-term resilience, and
reduces the resources required for sustaining LLM information security. Our
empirical evaluation demonstrates that LLMZ+ provides strong resilience against
the most common jailbreak prompts. At the same time, legitimate business
communications are not disrupted, and authorized traffic flows seamlessly
between users and the agentic LLM. We measure the effectiveness of approach
using false positive and false negative rates, both of which can be reduced to
0 in our experimental setting.

</details>


### [116] [Solving Math Word Problems Using Estimation Verification and Equation Generation](https://arxiv.org/abs/2509.18565)
*Mitchell Piehl,Dillon Wilson,Ananya Kalita,Jugal Kalita*

Main category: cs.AI

TL;DR: LLMs struggle with Math Word Problems (MWPs). This paper introduces a new method to improve LLM's performance on MWPs.


<details>
  <summary>Details</summary>
Motivation: LLMs often find Math Word Problems (MWPs) challenging because solving them requires a range of reasoning and mathematical abilities with which LLMs seem to struggle.

Method: The method prompts an LLM to create equations, uses an external solver, and verifies the answer by estimating the correct answer and comparing it to the generated answer. If verification fails, an iterative rectification process is employed.

Result: This approach achieves new state-of-the-art results on datasets used by prior published research on numeric and algebraic MWPs, improving the previous best results by nearly two percent on average. In addition, the approach obtains satisfactory results on trigonometric MWPs, a task not previously attempted to the authors' best knowledge. This study also introduces two new datasets, SVAMPClean and Trig300

Conclusion: The proposed approach improves LLMs' performance on MWPs and introduces new datasets for further research.

Abstract: Large Language Models (LLMs) excel at various tasks, including
problem-solving and question-answering. However, LLMs often find Math Word
Problems (MWPs) challenging because solving them requires a range of reasoning
and mathematical abilities with which LLMs seem to struggle. Recent efforts
have helped LLMs solve more complex MWPs with improved prompts. This study
proposes a novel method that initially prompts an LLM to create equations from
a decomposition of the question, followed by using an external symbolic
equation solver to produce an answer. To ensure the accuracy of the obtained
answer, inspired by an established recommendation of math teachers, the LLM is
instructed to solve the MWP a second time, but this time with the objective of
estimating the correct answer instead of solving it exactly. The estimation is
then compared to the generated answer to verify. If verification fails, an
iterative rectification process is employed to ensure the correct answer is
eventually found. This approach achieves new state-of-the-art results on
datasets used by prior published research on numeric and algebraic MWPs,
improving the previous best results by nearly two percent on average. In
addition, the approach obtains satisfactory results on trigonometric MWPs, a
task not previously attempted to the authors' best knowledge. This study also
introduces two new datasets, SVAMPClean and Trig300, to further advance the
testing of LLMs' reasoning abilities.

</details>


### [117] [Adaptive Learning in Spatial Agent-Based Models for Climate Risk Assessment: A Geospatial Framework with Evolutionary Economic Agents](https://arxiv.org/abs/2509.18633)
*Yara Mohajerani*

Main category: cs.AI

TL;DR: 这篇论文提出了一个新颖的地理空间Agent-Based模型，该模型结合了气候灾害数据和经济主体的进化学习。


<details>
  <summary>Details</summary>
Motivation: 气候风险评估需要对空间异质性灾害和适应性经济系统之间的复杂交互进行建模。

Method: 该框架结合了基于Mesa的空间建模和CLIMADA气候影响评估，引入了自适应学习行为，允许企业通过基于适应度的选择和变异来进化预算分配、定价、工资和风险适应策略。

Result: 使用RCP8.5情景下的河流洪水预测到2100年，结果表明，进化适应使企业能够在因气候压力中断数十年后，收敛到基线（无灾害）生产水平。结果还揭示了系统性风险，即使未直接暴露于洪水的Agent也会因供应链中断而面临影响，在RCP8.5情景下，到本世纪末，商品平均价格比基线高5.6%。

Conclusion: 该开源框架为金融机构和公司提供了量化直接和级联气候风险以及评估具有成本效益的适应策略的工具。

Abstract: Climate risk assessment requires modelling complex interactions between
spatially heterogeneous hazards and adaptive economic systems. We present a
novel geospatial agent-based model that integrates climate hazard data with
evolutionary learning for economic agents. Our framework combines Mesa-based
spatial modelling with CLIMADA climate impact assessment, introducing adaptive
learning behaviours that allow firms to evolve strategies for budget
allocation, pricing, wages, and risk adaptation through fitness-based selection
and mutation. We demonstrate the framework using riverine flood projections
under RCP8.5 until 2100, showing that evolutionary adaptation enables firms to
converge with baseline (no hazard) production levels after decades of
disruption due to climate stress. Our results reveal systemic risks where even
agents that are not directly exposed to floods face impacts through supply
chain disruptions, with the end-of-century average price of goods 5.6% higher
under RCP8.5 compared to the baseline. This open-source framework provides
financial institutions and companies with tools to quantify both direct and
cascading climate risks while evaluating cost-effective adaptation strategies.

</details>


### [118] [TERAG: Token-Efficient Graph-Based Retrieval-Augmented Generation](https://arxiv.org/abs/2509.18667)
*Qiao Xiao,Hong Ting Tsang,Jiaxin Bai*

Main category: cs.AI

TL;DR: TERAG: A cost-effective graph-based RAG framework.


<details>
  <summary>Details</summary>
Motivation: Existing graph-based RAG systems are expensive due to LLM token usage during graph construction, limiting their scalability.

Method: Propose TERAG, a framework that builds informative graphs at a lower cost, incorporating Personalized PageRank (PPR) during retrieval.

Result: Achieves at least 80% of the accuracy of widely used graph-based RAG methods while consuming only 3%-11% of the output tokens.

Conclusion: TERAG is a simple and effective framework for building informative graphs at a significantly lower cost.

Abstract: Graph-based Retrieval-augmented generation (RAG) has become a widely studied
approach for improving the reasoning, accuracy, and factuality of Large
Language Models. However, many existing graph-based RAG systems overlook the
high cost associated with LLM token usage during graph construction, hindering
large-scale adoption. To address this, we propose TERAG, a simple yet effective
framework designed to build informative graphs at a significantly lower cost.
Inspired by HippoRAG, we incorporate Personalized PageRank (PPR) during the
retrieval phase, and we achieve at least 80% of the accuracy of widely used
graph-based RAG methods while consuming only 3%-11% of the output tokens.

</details>


### [119] [Implementation of airborne ML models with semantics preservation](https://arxiv.org/abs/2509.18681)
*Nicolas Valot,Louis Fabre,Benjamin Lesage,Ammar Mechouche,Claire Pagetti*

Main category: cs.AI

TL;DR: 这篇论文旨在明确机器学习模型（ML模型）与其对应的明确描述（MLMD）之间的区别，并完善语义保留的概念，以确保模型的准确复制。


<details>
  <summary>Details</summary>
Motivation: 机器学习（ML）可能为机载系统提供新的能力。但是，与任何机载系统一样，基于ML的系统也需要保证其安全运行。因此，必须证明它们的发展符合适当的指导。

Method: 论文提炼了语义保留的基本概念，以确保模型的准确复制。然后，将研究成果应用于多个工业用例，以构建和比较多个目标模型。

Result: 论文构建并比较了多个目标模型。

Conclusion: 论文明确了ML模型和MLMD之间的区别，并完善了语义保留的概念。

Abstract: Machine Learning (ML) may offer new capabilities in airborne systems.
However, as any piece of airborne systems, ML-based systems will be required to
guarantee their safe operation. Thus, their development will have to be
demonstrated to be compliant with the adequate guidance. So far, the European
Union Aviation Safety Agency (EASA) has published a concept paper and an
EUROCAE/SAE group is preparing ED-324. Both approaches delineate high-level
objectives to confirm the ML model achieves its intended function and maintains
training performance in the target environment. The paper aims to clarify the
difference between an ML model and its corresponding unambiguous description,
referred to as the Machine Learning Model Description (MLMD). It then refines
the essential notion of semantics preservation to ensure the accurate
replication of the model. We apply our contributions to several industrial use
cases to build and compare several target models.

</details>


### [120] [Advances in Large Language Models for Medicine](https://arxiv.org/abs/2509.18690)
*Zhiyu Kan,Wensheng Gan,Zhenlian Qi,Philip S. Yu*

Main category: cs.AI

TL;DR: 本文综述了大型语言模型（LLM）在医学领域的最新研究进展，并根据训练方法创新性地将医学LLM分为三类，根据评估方法将其分为两类。最后，该研究针对现有挑战提出了解决方案，并概述了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 强调开发医学LLM的必要性，并更深入地了解它们当前的发展状态。

Method: 系统地回顾医学领域中LLM的最新研究进展，深入分析大型医学模型的训练技术、在医疗保健环境中的应用、相关应用以及它们的优势和局限性。

Result: 根据训练方法将医学LLM分为三类，根据评估方法将其分为两类。

Conclusion: 为后续研究提供明确的指导。

Abstract: Artificial intelligence (AI) technology has advanced rapidly in recent years,
with large language models (LLMs) emerging as a significant breakthrough. LLMs
are increasingly making an impact across various industries, with the medical
field standing out as the most prominent application area. This paper
systematically reviews the up-to-date research progress of LLMs in the medical
field, providing an in-depth analysis of training techniques for large medical
models, their adaptation in healthcare settings, related applications, as well
as their strengths and limitations. Furthermore, it innovatively categorizes
medical LLMs into three distinct types based on their training methodologies
and classifies their evaluation approaches into two categories. Finally, the
study proposes solutions to existing challenges and outlines future research
directions based on identified issues in the field of medical LLMs. By
systematically reviewing previous and advanced research findings, we aim to
highlight the necessity of developing medical LLMs, provide a deeper
understanding of their current state of development, and offer clear guidance
for subsequent research.

</details>


### [121] [Autonomous Data Agents: A New Opportunity for Smart Data](https://arxiv.org/abs/2509.18710)
*Yanjie Fu,Dongjie Wang,Wangyang Ying,Xiangliang Zhang,Huan Liu,Jian Pei*

Main category: cs.AI

TL;DR: 自主数据代理 (DataAgents) 通过整合大型语言模型 (LLM) 推理与任务分解、行动推理和 grounding 以及工具调用，能够自主地解释数据任务描述，将任务分解为子任务，对行动进行推理，将行动 grounding 为 python 代码或工具调用，并执行操作。


<details>
  <summary>Details</summary>
Motivation: 随着数据在规模和复杂性上不断增长，准备、转换和分析数据仍然是劳动密集型、重复性的且难以扩展。由于数据包含知识，而人工智能 (AI) 从数据中学习知识，因此 AI 和数据之间的对齐至关重要。然而，数据的结构通常并非针对 AI 利用进行优化。

Method: DataAgents 整合了 LLM 推理与任务分解、行动推理和 grounding 以及工具调用。

Result: DataAgents 能够处理收集、整合、预处理、选择、转换、重新加权、增强、重编程、修复和检索。通过这些能力，DataAgents 将复杂且非结构化的数据转换为连贯且可操作的知识。

Conclusion: DataAgents 代表着向自主数据到知识系统的范式转变。

Abstract: As data continues to grow in scale and complexity, preparing, transforming,
and analyzing it remains labor-intensive, repetitive, and difficult to scale.
Since data contains knowledge and AI learns knowledge from it, the alignment
between AI and data is essential. However, data is often not structured in ways
that are optimal for AI utilization. Moreover, an important question arises:
how much knowledge can we pack into data through intensive data operations?
Autonomous data agents (DataAgents), which integrate LLM reasoning with task
decomposition, action reasoning and grounding, and tool calling, can
autonomously interpret data task descriptions, decompose tasks into subtasks,
reason over actions, ground actions into python code or tool calling, and
execute operations. Unlike traditional data management and engineering tools,
DataAgents dynamically plan workflows, call powerful tools, and adapt to
diverse data tasks at scale. This report argues that DataAgents represent a
paradigm shift toward autonomous data-to-knowledge systems. DataAgents are
capable of handling collection, integration, preprocessing, selection,
transformation, reweighing, augmentation, reprogramming, repairs, and
retrieval. Through these capabilities, DataAgents transform complex and
unstructured data into coherent and actionable knowledge. We first examine why
the convergence of agentic AI and data-to-knowledge systems has emerged as a
critical trend. We then define the concept of DataAgents and discuss their
architectural design, training strategies, as well as the new skills and
capabilities they enable. Finally, we call for concerted efforts to advance
action workflow optimization, establish open datasets and benchmark ecosystems,
safeguard privacy, balance efficiency with scalability, and develop trustworthy
DataAgent guardrails to prevent malicious actions.

</details>


### [122] [Experience Scaling: Post-Deployment Evolution For Large Language Models](https://arxiv.org/abs/2509.18771)
*Xingkun Yin,Kaibin Huang,Dong In Kim,Hongyang Du*

Main category: cs.AI

TL;DR: 本文提出了一种名为“经验缩放”的框架，用于在部署后持续改进大型语言模型 (LLM)。


<details>
  <summary>Details</summary>
Motivation: 随着人类生成的文本耗尽，扩展模型大小、训练数据和计算能力的方法正变得饱和，收益递减。

Method: 该框架通过与环境的自主交互和协作共享累积的经验来实现LLM的持续演进，它捕获原始交互，将其提炼成紧凑、可重用的知识，并定期改进存储的内容以保持相关性和效率。

Result: 在涉及泛化到以前未见过但相关的任务、重复查询和过度饱和的知识存储的模拟真实场景中，验证了该框架。在所有设置中，经验缩放提高了准确性，随着时间的推移保持了性能，并在应用于新情况时保持了收益。

Conclusion: 结构化的部署后学习可以将LLM的能力扩展到静态人类生成数据的限制之外，从而为持续的智能进步提供可扩展的途径。

Abstract: Scaling model size, training data, and compute power have driven advances in
large language models (LLMs), but these approaches are reaching saturation as
human-generated text is exhausted and further gains diminish. We propose
experience scaling, a framework for continuous post-deployment evolution for
LLMs through autonomous interaction with the environment and collaborative
sharing of accumulated experience. The framework captures raw interactions,
distills them into compact, reusable knowledge, and periodically refines stored
content to preserve relevance and efficiency. We validate the framework in
simulated real-world scenarios involving generalization to previously unseen
but related tasks, repetitive queries, and over-saturated knowledge stores.
Across all settings, experience scaling improves accuracy, sustains performance
over time, and maintains gains when applied to novel situations. These results
demonstrate that structured post-deployment learning can extend LLM
capabilities beyond the limits of static human-generated data, offering a
scalable path for continued intelligence progress.

</details>


### [123] [The AGNTCY Agent Directory Service: Architecture and Implementation](https://arxiv.org/abs/2509.18787)
*Luca Muscariello,Vijoy Pandey,Ramiz Polic*

Main category: cs.AI

TL;DR: Agent Directory Service (ADS) is a distributed directory for discovering AI agent capabilities.


<details>
  <summary>Details</summary>
Motivation: Enable efficient, verifiable, and multi-dimensional discovery across heterogeneous Multi-Agent Systems (MAS).

Method: Leverages content-addressed storage, hierarchical taxonomies, and cryptographic signing; decouples capability indexing from content location through a two-level mapping realized over a Kademlia-based Distributed Hash Table (DHT).

Result: Reuses mature OCI / ORAS infrastructure for artifact distribution, integrates Sigstore for provenance, and supports schema-driven extensibility for emerging agent modalities.

Conclusion: This paper formalizes the architectural model, describes storage and discovery layers, explains security and performance properties, and positions ADS within the broader landscape of emerging agent registry and interoperability initiatives.

Abstract: The Agent Directory Service (ADS) is a distributed directory for the
discovery of AI agent capabilities, metadata, and provenance. It leverages
content-addressed storage, hierarchical taxonomies, and cryptographic signing
to enable efficient, verifiable, and multi-dimensional discovery across
heterogeneous Multi-Agent Systems (MAS). Built on the Open Agentic Schema
Framework (OASF), ADS decouples capability indexing from content location
through a two-level mapping realized over a Kademlia-based Distributed Hash
Table (DHT). It reuses mature OCI / ORAS infrastructure for artifact
distribution, integrates Sigstore for provenance, and supports schema-driven
extensibility for emerging agent modalities (LLM prompt agents, MCP servers,
A2A-enabled components). This paper formalizes the architectural model,
describes storage and discovery layers, explains security and performance
properties, and positions ADS within the broader landscape of emerging agent
registry and interoperability initiatives.

</details>


### [124] [Bounded PCTL Model Checking of Large Language Model Outputs](https://arxiv.org/abs/2509.18836)
*Dennis Gross,Helge Spieker,Arnaud Gotlieb*

Main category: cs.AI

TL;DR: LLMCHECKER introduces a model-checking method to verify PCTL properties in LLM text generation.


<details>
  <summary>Details</summary>
Motivation: Verifying the consistency of LLM text generation processes using PCTL model checking.

Method: The method narrows focus to the α maximal cumulative probability on the top-k tokens at each step and accommodates diverse text quantification methods.

Result: The method's applicability is demonstrated in several LLMs, including Llama, Gemma, Mistral, Genstruct, and BERT.

Conclusion: This is the first time PCTL-based model checking has been used to check the consistency of LLM text generation.

Abstract: In this paper, we introduce LLMCHECKER, a model-checking-based verification
method to verify the probabilistic computation tree logic (PCTL) properties of
an LLM text generation process. We empirically show that only a limited number
of tokens are typically chosen during text generation, which are not always the
same. This insight drives the creation of $\alpha$-$k$-bounded text generation,
narrowing the focus to the $\alpha$ maximal cumulative probability on the
top-$k$ tokens at every step of the text generation process. Our verification
method considers an initial string and the subsequent top-$k$ tokens while
accommodating diverse text quantification methods, such as evaluating text
quality and biases. The threshold $\alpha$ further reduces the selected tokens,
only choosing those that exceed or meet it in cumulative probability.
LLMCHECKER then allows us to formally verify the PCTL properties of
$\alpha$-$k$-bounded LLMs. We demonstrate the applicability of our method in
several LLMs, including Llama, Gemma, Mistral, Genstruct, and BERT. To our
knowledge, this is the first time PCTL-based model checking has been used to
check the consistency of the LLM text generation process.

</details>


### [125] [Model selection meets clinical semantics: Optimizing ICD-10-CM prediction via LLM-as-Judge evaluation, redundancy-aware sampling, and section-aware fine-tuning](https://arxiv.org/abs/2509.18846)
*Hong-Jie Dai,Zheng-Hao Li,An-Tai Lu,Bo-Tsz Shain,Ming-Ta Li,Tatheer Hussain Mir,Kuang-Te Wang,Min-I Su,Pei-Kang Liu,Ming-Ju Tsai*

Main category: cs.AI

TL;DR: 本文提出了一种用于ICD-10-CM编码预测的模块化框架，该框架通过模型选择、数据采样和结构化输入设计来解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: ICD编码对于临床文档、计费和医疗保健分析至关重要，但它仍然是一项劳动密集且容易出错的任务。大型语言模型在自动化ICD编码方面显示出潜力，但其在基础模型选择、输入语境化和训练数据冗余方面的挑战限制了其有效性。

Method: 该框架集成了LLM-as-judge评估协议与Plackett-Luce聚合，以评估和排序开源LLM。引入了基于嵌入的相似性度量，一种冗余感知采样策略来删除语义重复的出院总结。利用来自台湾医院的结构化出院总结来评估上下文效应，并检查通用和特定部分建模范式下的分段内容包含。

Result: 在两个机构数据集上的实验表明，微调后选择的基础模型在内部和外部评估中始终优于基线LLM。纳入更多临床部分始终提高预测性能。

Conclusion: 本研究使用开源LLM来建立一种实用且有原则的ICD-10-CM代码预测方法。该框架通过结合知情的模型选择、高效的数据提炼和上下文感知的提示，为自动化医疗编码系统的实际部署提供了一种可扩展的、机构就绪的解决方案。

Abstract: Accurate International Classification of Diseases (ICD) coding is critical
for clinical documentation, billing, and healthcare analytics, yet it remains a
labour-intensive and error-prone task. Although large language models (LLMs)
show promise in automating ICD coding, their challenges in base model
selection, input contextualization, and training data redundancy limit their
effectiveness. We propose a modular framework for ICD-10 Clinical Modification
(ICD-10-CM) code prediction that addresses these challenges through principled
model selection, redundancy-aware data sampling, and structured input design.
The framework integrates an LLM-as-judge evaluation protocol with Plackett-Luce
aggregation to assess and rank open-source LLMs based on their intrinsic
comprehension of ICD-10-CM code definitions. We introduced embedding-based
similarity measures, a redundancy-aware sampling strategy to remove
semantically duplicated discharge summaries. We leverage structured discharge
summaries from Taiwanese hospitals to evaluate contextual effects and examine
section-wise content inclusion under universal and section-specific modelling
paradigms. Experiments across two institutional datasets demonstrate that the
selected base model after fine-tuning consistently outperforms baseline LLMs in
internal and external evaluations. Incorporating more clinical sections
consistently improves prediction performance. This study uses open-source LLMs
to establish a practical and principled approach to ICD-10-CM code prediction.
The proposed framework provides a scalable, institution-ready solution for
real-world deployment of automated medical coding systems by combining informed
model selection, efficient data refinement, and context-aware prompting.

</details>


### [126] [MAPO: Mixed Advantage Policy Optimization](https://arxiv.org/abs/2509.18849)
*Wenke Huang,Quan Zhang,Yiyang Fang,Jian Liang,Xuankun Rong,Huanjin Yao,Guancheng Wan,Ke Liang,Wenwen He,Mingjun Li,Leszek Rutkowski,Mang Ye,Bo Du,Dacheng Tao*

Main category: cs.AI

TL;DR: 提出了一种新的GRPO策略，称为混合优势策略优化(MAPO)，以解决现有探索中遇到的优势反转和优势镜像问题。


<details>
  <summary>Details</summary>
Motivation: 现有的GRPO方法在不同查询样本中的优势分配不合理。

Method: 提出了优势百分比偏差来处理高确定性轨迹的样本，并动态地重新加权优势函数来处理不同轨迹确定性的样本。

Result: 与最先进的方法比较以及对不同优势变体的消融研究验证了该方法的有效性。

Conclusion: 该方法有效地解决了优势反转和优势镜像问题，提高了foundation model在推理任务上的性能。

Abstract: Recent advances in reinforcement learning for foundation models, such as
Group Relative Policy Optimization (GRPO), have significantly improved the
performance of foundation models on reasoning tasks. Notably, the advantage
function serves as a central mechanism in GRPO for ranking the trajectory
importance. However, existing explorations encounter both advantage reversion
and advantage mirror problems, which hinder the reasonable advantage allocation
across different query samples. In this work, we propose an easy but effective
GRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the
trajectory appears with different certainty and propose the advantage percent
deviation for samples with high-certainty trajectories. Furthermore, we
dynamically reweight the advantage function for samples with varying trajectory
certainty, thereby adaptively configuring the advantage function to account for
sample-specific characteristics. Comparison with related state-of-the-art
methods, along with ablation studies on different advantage variants, validates
the effectiveness of our approach.

</details>


### [127] [Conf-Profile: A Confidence-Driven Reasoning Paradigm for Label-Free User Profiling](https://arxiv.org/abs/2509.18864)
*Yingxin Li,Jianbo Zhao,Xueyu Ren,Jie Tang,Wangjie You,Xu Chen,Kan Zhou,Chao Feng,Jiao Ran,Yuan Meng,Zhi Wang*

Main category: cs.AI

TL;DR: 本文提出了ProfileBench，一个来自真实视频平台的工业benchmark，用于用户画像。同时，提出了一个名为Conf-Profile的框架，用于解决无标签和可靠的用户画像问题。


<details>
  <summary>Details</summary>
Motivation: 现有用户画像研究缺乏全面的benchmark，且大规模ground-truth标签难以收集，异构和噪声用户信息会影响LLM的可靠性。

Method: 提出了一个两阶段的框架Conf-Profile。首先，利用带有置信度提示的先进LLM合成高质量标签，然后通过置信度加权投票提高准确性，并通过置信度校准实现平衡分布。此外，通过置信度引导的无监督强化学习来提高推理能力。

Result: Conf-Profile通过两阶段训练，在Qwen3-8B上将F1提高了13.97。

Conclusion: 本文提出的ProfileBench和Conf-Profile框架在用户画像任务中表现出色，为该领域的研究提供了新的方向。

Abstract: User profiling, as a core technique for user understanding, aims to infer
structural attributes from user information. Large Language Models (LLMs)
provide a promising avenue for user profiling, yet the progress is hindered by
the lack of comprehensive benchmarks. To bridge this gap, we propose
ProfileBench, an industrial benchmark derived from a real-world video platform,
encompassing heterogeneous user data and a well-structured profiling taxonomy.
However, the profiling task remains challenging due to the difficulty of
collecting large-scale ground-truth labels, and the heterogeneous and noisy
user information can compromise the reliability of LLMs. To approach label-free
and reliable user profiling, we propose a Confidence-driven Profile reasoning
framework Conf-Profile, featuring a two-stage paradigm. We first synthesize
high-quality labels by leveraging advanced LLMs with confidence hints, followed
by confidence-weighted voting for accuracy improvement and confidence
calibration for a balanced distribution. The multiple profile results,
rationales, and confidence scores are aggregated and distilled into a
lightweight LLM. We further enhance the reasoning ability via confidence-guided
unsupervised reinforcement learning, which exploits confidence for difficulty
filtering, quasi-ground truth voting, and reward weighting. Experimental
results demonstrate that Conf-Profile delivers substantial performance through
the two-stage training, improving F1 by 13.97 on Qwen3-8B.

</details>


### [128] [Memory in Large Language Models: Mechanisms, Evaluation and Evolution](https://arxiv.org/abs/2509.18868)
*Dianxing Zhang,Wendong Li,Kani Song,Jiaye Lu,Gang Li,Liuchun Yang,Sheng Li*

Main category: cs.AI

TL;DR: 这篇论文提出了一个关于大型语言模型（LLM）记忆的统一框架，包括定义、分类、评估和治理方案，旨在实现可重现、可比较和可治理的研究与部署。


<details>
  <summary>Details</summary>
Motivation: 为了避免在异构设置中进行扭曲的比较，并建立一个更清晰的LLM记忆研究基础。

Method: 论文提出了一个四部分的分类法（参数、上下文、外部、程序/情景）和一个记忆四元组（位置、持久性、写入/访问路径、可控性）。此外，还提出了一个三设置协议（仅参数、离线检索、在线检索）和一个分层评估体系。

Result: 论文构建了一个分层评估体系，涵盖参数、上下文、外部和程序/情景记忆。同时，提出了DMM Gov，一个协调更新和遗忘的框架，以及四个可测试的主张。

Conclusion: 该框架为LLM记忆的研究和部署提供了一个可重现、可比较和可治理的坐标系统。

Abstract: Under a unified operational definition, we define LLM memory as a persistent
state written during pretraining, finetuning, or inference that can later be
addressed and that stably influences outputs. We propose a four-part taxonomy
(parametric, contextual, external, procedural/episodic) and a memory quadruple
(location, persistence, write/access path, controllability). We link mechanism,
evaluation, and governance via the chain write -> read -> inhibit/update. To
avoid distorted comparisons across heterogeneous setups, we adopt a
three-setting protocol (parametric only, offline retrieval, online retrieval)
that decouples capability from information availability on the same data and
timeline. On this basis we build a layered evaluation: parametric (closed-book
recall, edit differential, memorization/privacy), contextual (position curves
and the mid-sequence drop), external (answer correctness vs snippet
attribution/faithfulness), and procedural/episodic (cross-session consistency
and timeline replay, E MARS+). The framework integrates temporal governance and
leakage auditing (freshness hits, outdated answers, refusal slices) and
uncertainty reporting via inter-rater agreement plus paired tests with
multiple-comparison correction. For updating and forgetting, we present DMM
Gov: coordinating DAPT/TAPT, PEFT, model editing (ROME, MEND, MEMIT, SERAC),
and RAG to form an auditable loop covering admission thresholds, rollout,
monitoring, rollback, and change audits, with specs for timeliness, conflict
handling, and long-horizon consistency. Finally, we give four testable
propositions: minimum identifiability; a minimal evaluation card; causally
constrained editing with verifiable forgetting; and when retrieval with
small-window replay outperforms ultra-long-context reading. This yields a
reproducible, comparable, and governable coordinate system for research and
deployment.

</details>


### [129] [LongCat-Flash-Thinking Technical Report](https://arxiv.org/abs/2509.18883)
*Meituan LongCat Team,Anchun Gui,Bei Li,Bingyang Tao,Bole Zhou,Borun Chen,Chao Zhang,Chao Zhang,Chengcheng Han,Chenhui Yang,Chi Zhang,Chong Peng,Chuyu Zhang,Cong Chen,Fengcun Li,Gang Xu,Guoyuan Lin,Hao Jiang,Hao Liang,Haomin Fu,Haoxiang Ma,Hong Liu,Hongyan Hao,Hongyin Tang,Hongyu Zang,Hongzhi Ni,Hui Su,Jiahao Liu,Jiahuan Li,Jialin Liu,Jianfei Zhang,Jianhao Xu,Jianing Wang,Jiaqi Sun,Jiaqi Zhang,Jiarong Shi,Jiawei Yang,Jingang Wang,Jinrui Ding,Jun Kuang,Jun Xu,Ke He,Kefeng Zhang,Keheng Wang,Keqing He,Li Wei,Liang Shi,Lin Qiu,Lingbin Kong,Lingchuan Liu,Linsen Guo,Longfei An,Mai Xia,Meng Zhou,Mengshen Zhu,Peng Pei,Pengcheng Jia,Qi Gu,Qi Guo,Qiong Huang,Quan Chen,Quanchi Weng,Rongxiang Weng,Ruichen Shao,Rumei Li,Shanglin Lei,Shuai Du,Shuaikang Liu,Shuang Zhou,Shuhao Hu,Siyu Xu,Songshan Gong,Tao Liang,Tianhao Hu,Wei He,Wei Shi,Wei Wang,Wei Wu,Wei Zhuo,Weifeng Tang,Wenjie Shi,Wenlong Zhu,Xi Su,Xiangcheng Liu,Xiangyu Xi,Xiangzhou Huang,Xiao Liu,Xiaochen Jiang,Xiaowei Shi,Xiaowen Shi,Xiaoyu Li,Xin Chen,Xinyue Zhao,Xuan Huang,Xuemiao Zhang,Xuezhi Cao,Xunliang Cai,Yajie Zhang,Yang Chen,Yang Liu,Yang Liu,Yang Zheng,Yaoming Wang,Yaqi Huo,Yerui Sun,Yifan Lu,Yiyang Li,Youshao Xiao,Yuanzhe Lei,Yuchen Xie,Yueqing Sun,Yufei Zhang,Yuhuai Wei,Yulei Qian,Yunke Zhao,Yuqing Ding,Yuwei Jiang,Zhaohua Yang,Zhengyu Chen,Zhijian Liu,Zhikang Xia,Zhongda Su,Ziran Li,Ziwen Wang,Ziyuan Zhuang,Zongyu Wang,Zunyuan Yang*

Main category: cs.AI

TL;DR: LongCat-Flash-Thinking是一个高效的5600亿参数开源MoE推理模型，通过长CoT数据冷启动和大规模RL训练而成。


<details>
  <summary>Details</summary>
Motivation: 构建一个高效且强大的开源推理模型，以推动推理系统和Agentic AI研究的进展。

Method: 采用精心设计的训练流程，包括长CoT数据冷启动、领域并行训练方案和DORA系统加速的大规模RL框架。

Result: LongCat-Flash-Thinking在复杂推理任务上达到了开源模型的最佳性能，并在Agentic推理中显著降低了token消耗。

Conclusion: 发布LongCat-Flash-Thinking模型，旨在促进推理系统和Agentic AI研究的进一步发展。

Abstract: We present LongCat-Flash-Thinking, an efficient 560-billion-parameter
open-source Mixture-of-Experts (MoE) reasoning model. Its advanced capabilities
are cultivated through a meticulously crafted training process, beginning with
long Chain-of-Thought (CoT) data cold-start and culminating in large-scale
Reinforcement Learning (RL). We first employ a well-designed cold-start
training strategy, which significantly enhances the reasoning potential and
equips the model with specialized skills in both formal and agentic reasoning.
Then, a core innovation is our domain-parallel training scheme, which decouples
optimization across distinct domains (e.g., STEM, Code, Agentic) and
subsequently fuses the resulting expert models into a single, nearly
Pareto-optimal model. This entire process is powered by our Dynamic
ORchestration for Asynchronous rollout (DORA) system, a large-scale RL
framework that delivers a greater than threefold training speedup over
synchronous methods on tens of thousands of accelerators. As a result,
LongCat-Flash-Thinking achieves state-of-the-art performance among open-source
models on a suite of complex reasoning tasks. The model exhibits exceptional
efficiency in agentic reasoning, reducing average token consumption by 64.5%
(from 19, 653 to 6, 965) on AIME-25, without degrading task accuracy. We
release LongCat-Flash-Thinking to promote further advances in reasoning systems
and agentic AI research.

</details>


### [130] [How Far are VLMs from Visual Spatial Intelligence? A Benchmark-Driven Perspective](https://arxiv.org/abs/2509.18905)
*Songsong Yu,Yuxin Chen,Hao Ju,Lianjie Jia,Fuxi Zhang,Shaofei Huang,Yuhan Wu,Rundi Cui,Binghao Ran,Zaibin Zhang,Zhedong Zheng,Zhipeng Zhang,Yifan Wang,Lin Song,Lijun Wang,Yanwei Li,Ying Shan,Huchuan Lu*

Main category: cs.AI

TL;DR: 本文对视觉语言模型(VLM)中的视觉空间推理(VSR)进行了系统研究，包括回顾现有方法、对空间智能进行分类并建立基准测试。结果表明，VLM在感知任务上表现良好，但在理解和规划任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型取得了进展，但由于表示和推理三维空间的复杂性，实现人类水平的视觉空间推理仍然具有挑战性。

Method: 本文对视觉语言模型中的视觉空间推理进行了系统研究，包括回顾跨输入模态、模型架构、训练策略和推理机制的现有方法。此外，本文将空间智能分为三个能力级别，即基本感知、空间理解、空间规划，并创建了包含近20个跨23个任务设置的开源数据集的空间智能基准测试SIBench。

Result: 最先进的视觉语言模型的实验表明，感知和推理之间存在明显差距，因为模型在基本感知任务中表现出能力，但在理解和规划任务中始终表现不佳，尤其是在数值估计、多视图推理、时间动态和空间想象方面。

Conclusion: 在实现空间智能方面仍然存在重大挑战，同时提供了一个系统的路线图和一个全面的基准来推动该领域未来的研究。

Abstract: Visual Spatial Reasoning (VSR) is a core human cognitive ability and a
critical requirement for advancing embodied intelligence and autonomous
systems. Despite recent progress in Vision-Language Models (VLMs), achieving
human-level VSR remains highly challenging due to the complexity of
representing and reasoning over three-dimensional space. In this paper, we
present a systematic investigation of VSR in VLMs, encompassing a review of
existing methodologies across input modalities, model architectures, training
strategies, and reasoning mechanisms. Furthermore, we categorize spatial
intelligence into three levels of capability, ie, basic perception, spatial
understanding, spatial planning, and curate SIBench, a spatial intelligence
benchmark encompassing nearly 20 open-source datasets across 23 task settings.
Experiments with state-of-the-art VLMs reveal a pronounced gap between
perception and reasoning, as models show competence in basic perceptual tasks
but consistently underperform in understanding and planning tasks, particularly
in numerical estimation, multi-view reasoning, temporal dynamics, and spatial
imagination. These findings underscore the substantial challenges that remain
in achieving spatial intelligence, while providing both a systematic roadmap
and a comprehensive benchmark to drive future research in the field. The
related resources of this study are accessible at
https://sibench.github.io/Awesome-Visual-Spatial-Reasoning/.

</details>


### [131] [Data Efficient Adaptation in Large Language Models via Continuous Low-Rank Fine-Tuning](https://arxiv.org/abs/2509.18942)
*Xiao Han,Zimo Zhao,Wanyu Wang,Maolin Wang,Zitao Liu,Yi Chang,Xiangyu Zhao*

Main category: cs.AI

TL;DR: 本文提出了一种名为DEAL的新框架，该框架集成了低秩适应（LoRA）与连续微调策略，以解决传统微调方法中的灾难性遗忘和数据效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 传统微调（FT）方法存在灾难性遗忘和数据效率低下的问题，限制了其在现实世界中的应用。

Method: 该文提出DEAL框架，通过整合低秩适应（LoRA）与连续微调策略，并结合知识保留和自适应参数更新模块来缓解现有FT方法的局限性。

Result: 在15个不同的数据集上的实验表明，DEAL始终优于基线方法，在任务准确性和资源效率方面产生了显着提高。

Conclusion: 该研究结果表明，该方法有潜力通过提高任务性能和资源效率来推进LLM中的持续适应。

Abstract: Recent advancements in Large Language Models (LLMs) have emphasized the
critical role of fine-tuning (FT) techniques in adapting LLMs to specific
tasks, especially when retraining from scratch is computationally infeasible.
Fine-tuning enables LLMs to leverage task- or domain-specific data, producing
models that more effectively meet the requirements of targeted applications.
However, con- ventional FT approaches often suffer from catastrophic forgetting
and suboptimal data efficiency, limiting their real-world applicability. To
address these challenges, this paper proposes DEAL, a novel framework that
integrates Low-Rank Adapta- tion (LoRA) with a continuous fine-tuning strategy.
By incorporating knowledge retention and adaptive parameter update modules, the
framework mitigates the lim- itations of existing FT methods while maintaining
efficiency in privacy-preserving settings. Experiments on 15 diverse datasets
show that DEAL consistently outper- forms baseline methods, yielding
substantial gains in task accuracy and resource efficiency. These findings
demonstrate the potential of our approach to advance continual adaptation in
LLMs by enhancing task performance while improving resource efficiency.

</details>


### [132] [LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy, Methods, and Directions](https://arxiv.org/abs/2509.18970)
*Xixun Lin,Yucheng Ning,Jingwen Zhang,Yan Dong,Yilong Liu,Yongxuan Wu,Xiaohua Qi,Nan Sun,Yanmin Shang,Pengfei Cao,Lixin Zou,Xu Chen,Chuan Zhou,Jia Wu,Shirui Pan,Bin Wang,Yanan Cao,Kai Chen,Songlin Hu,Li Guo*

Main category: cs.AI

TL;DR: 本研究对基于大型语言模型（LLM）的代理中的幻觉问题进行了全面的调查。


<details>
  <summary>Details</summary>
Motivation: 尽管基于LLM的代理在各种实际应用中具有显著潜力，但它们仍然容易出现幻觉问题，这可能导致错误的 Task 执行并损害整体系统设计的可靠性。解决这个关键挑战需要深入理解和系统地巩固基于LLM的代理的最新进展。

Method: 通过仔细分析代理的完整工作流程，我们提出了一种新的分类法，该分类法确定了在不同阶段发生的 Agent 幻觉的不同类型。此外，我们深入研究了导致 Agent 幻觉出现的 18 个触发原因。通过详细回顾大量现有研究，我们总结了幻觉缓解和检测的方法，并强调了未来研究的有希望的方向。

Result: 总结了幻觉缓解和检测的方法，并强调了未来研究的有希望的方向。

Conclusion: 我们希望这项调查将激发更多努力来解决基于LLM的代理中的幻觉问题，最终有助于开发更强大和可靠的代理系统。

Abstract: Driven by the rapid advancements of Large Language Models (LLMs), LLM-based
agents have emerged as powerful intelligent systems capable of human-like
cognition, reasoning, and interaction. These agents are increasingly being
deployed across diverse real-world applications, including student education,
scientific research, and financial analysis. However, despite their remarkable
potential, LLM-based agents remain vulnerable to hallucination issues, which
can result in erroneous task execution and undermine the reliability of the
overall system design. Addressing this critical challenge requires a deep
understanding and a systematic consolidation of recent advances on LLM-based
agents. To this end, we present the first comprehensive survey of
hallucinations in LLM-based agents. By carefully analyzing the complete
workflow of agents, we propose a new taxonomy that identifies different types
of agent hallucinations occurring at different stages. Furthermore, we conduct
an in-depth examination of eighteen triggering causes underlying the emergence
of agent hallucinations. Through a detailed review of a large number of
existing studies, we summarize approaches for hallucination mitigation and
detection, and highlight promising directions for future research. We hope this
survey will inspire further efforts toward addressing hallucinations in
LLM-based agents, ultimately contributing to the development of more robust and
reliable agent systems.

</details>


### [133] [Remaining Time Prediction in Outbound Warehouse Processes: A Case Study (Short Paper)](https://arxiv.org/abs/2509.18986)
*Erik Penther,Michael Grohs,Jana-Rebecca Rehse*

Main category: cs.AI

TL;DR: 本文比较了四种预测剩余时间的方法，发现在航空业务的物流公司真实出库仓库流程中，深度学习模型精度最高，但传统 boosting 技术等浅层方法也具有竞争力，且计算资源需求更少。


<details>
  <summary>Details</summary>
Motivation: 预测流程执行的剩余时间是流程挖掘的一个子领域，对于正在进行的流程执行的未来进行预测。

Method: 比较了四种不同的剩余时间预测方法。

Result: 深度学习模型精度最高，但传统 boosting 技术等浅层方法也具有竞争力。

Conclusion: 浅层方法在计算资源有限的情况下，也能实现与深度学习模型相近的预测精度。

Abstract: Predictive process monitoring is a sub-domain of process mining which aims to
forecast the future of ongoing process executions. One common prediction target
is the remaining time, meaning the time that will elapse until a process
execution is completed. In this paper, we compare four different remaining time
prediction approaches in a real-life outbound warehouse process of a logistics
company in the aviation business. For this process, the company provided us
with a novel and original event log with 169,523 traces, which we can make
publicly available. Unsurprisingly, we find that deep learning models achieve
the highest accuracy, but shallow methods like conventional boosting techniques
achieve competitive accuracy and require significantly fewer computational
resources.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [134] [ExtGraph: A Fast Extraction Method of User-intended Graphs from a Relational Database](https://arxiv.org/abs/2509.18534)
*Jeongho Park,Geonho Lee,Min-Soo Kim*

Main category: cs.DB

TL;DR: 这篇论文提出了一种名为ExtGraph的高效图提取方法，用于从关系数据库中提取用户想要的图。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常需要复杂的连接查询处理，无法提取用户想要的图。

Method: 该方法通过外连接和物化视图的混合查询处理来实现高效的图提取。

Result: 在TPC-DS、DBLP和IMDB数据集上的实验表明，ExtGraph在图提取时间方面优于现有方法，最高可达2.78倍。

Conclusion: ExtGraph是一种高效的图提取方法。

Abstract: Graph analytics is widely used in many fields to analyze various complex
patterns. However, in most cases, important data in companies is stored in
RDBMS's, and so, it is necessary to extract graphs from relational databases to
perform graph analysis. Most of the existing methods do not extract a
user-intended graph since it typically requires complex join query processing.
We propose an efficient graph extraction method, \textit{ExtGraph}, which can
extract user-intended graphs efficiently by hybrid query processing of outer
join and materialized view. Through experiments using the TPC-DS, DBLP, and
IMDB datasets, we have shown that \textit{ExtGraph} outperforms the
state-of-the-art methods up to by 2.78x in terms of graph extraction time.

</details>


### [135] [CALL: Context-Aware Low-Latency Retrieval in Disk-Based Vector Databases](https://arxiv.org/abs/2509.18670)
*Yeonwoo Jeong,Hyunji Cho,Kyuri Park,Youngjae Kim,Sungyong Park*

Main category: cs.DB

TL;DR: CALL通过组织基于共享簇访问模式的查询来减少缓存未命中惩罚，并结合组感知预取方法和延迟感知簇加载来最小化搜索延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的方法优化了单个查询，但忽略了簇访问模式的影响，未能考虑到访问相似簇的查询的局部性效应，从而增加了缓存未命中惩罚。

Method: CALL是一种上下文感知的查询分组机制，它基于共享簇访问模式组织查询。此外，CALL还包含一个组感知预取方法，以最小化查询组之间转换期间的缓存未命中，以及延迟感知簇加载。

Result: 实验结果表明，CALL将第99个百分位的尾部延迟降低了高达33%，同时始终保持较高的缓存命中率，从而大大降低了搜索延迟。

Conclusion: CALL有效地减少了缓存未命中惩罚，并降低了搜索延迟。

Abstract: Embedding models capture both semantic and syntactic structures of queries,
often mapping different queries to similar regions in vector space. This
results in non-uniform cluster access patterns in modern disk-based vector
databases. While existing approaches optimize individual queries, they overlook
the impact of cluster access patterns, failing to account for the locality
effects of queries that access similar clusters. This oversight increases cache
miss penalty. To minimize the cache miss penalty, we propose CALL, a
context-aware query grouping mechanism that organizes queries based on shared
cluster access patterns. Additionally, CALL incorporates a group-aware
prefetching method to minimize cache misses during transitions between query
groups and latency-aware cluster loading. Experimental results show that CALL
reduces the 99th percentile tail latency by up to 33% while consistently
maintaining a higher cache hit ratio, substantially reducing search latency.

</details>


### [136] [Teaching RDM in a smart advanced inorganic lab course and its provision in the DALIA platform](https://arxiv.org/abs/2509.18902)
*Alexander Hoffmann,Jochen Ortmeyer,Fabian Fink,Charles Tapley Hoyt,Jonathan D. Geiger,Paul Kehrein,Torsten Schrade,Sonja Herres-Pawlis*

Main category: cs.DB

TL;DR: 本研究介绍了一种在本科化学课程中教授研究数据管理(RDM)的方法，利用Chemotion电子实验笔记本和DALIA平台。


<details>
  <summary>Details</summary>
Motivation: 传统化学研究数据管理方式不利于数据重用和机器学习应用，学生需要掌握FAIR数据原则等RDM技能。

Method: 在课程中引入Chemotion电子实验笔记本，学生使用其计划、记录和评估实验，并通过研讨会和在线培训视频加强RDM。

Result: Chemotion的界面和存储库促进了可持续的数据共享。

Conclusion: 强调使用DALIA平台作为学生的发现工具。

Abstract: Research data management (RDM) is a key data literacy skill that chemistry
students must acquire. Concepts such as the FAIR data principles (Findable,
Accessible, Interoperable, Reusable) should be taught and applied in
undergraduate studies already. Traditionally, research data from labs, theses,
and internships were handwritten and stored in inaccessible formats such as
PDFs, limiting reuse and machine learning applications. At RWTH Aachen
University, a fifth-semester lab course introduces students to the electronic
laboratory notebook (ELN) Chemotion, an open-source DFG-funded tool linked to
the national NFDI4Chem initiative. Students plan, document, and evaluate
experiments digitally, ensuring metadata and analysis are captured for
long-term reuse. Chemotion's intuitive interface and repository enable
sustainable data sharing. To reinforce RDM, students receive a seminar and
access to online training videos with interactive Moodle elements. Herein we
highlight the use of the DALIA platform as a discovery tool for the students.

</details>


### [137] [A decentralized future for the open-science databases](https://arxiv.org/abs/2509.19206)
*Gaurav Sharma,Viorel Munteanu,Nika Mansouri Ghiasi,Jineta Banerjee,Susheel Varma,Luca Foschini,Kyle Ellrott,Onur Mutlu,Dumitru Ciorbă,Roel A. Ophoff,Viorel Bostan,Christopher E Mason,Jason H. Moore,Despoina Sousoni,Arunkumar Krishnan,Christopher E. Mason,Mihai Dimian,Gustavo Stolovitzky,Fabio G. Liberante,Taras K. Oleksyk,Serghei Mangul*

Main category: cs.DB

TL;DR: 集中式生物数据存储库容易受到单点故障的影响，而联邦和分散式架构可以增强科学数据基础设施的弹性。


<details>
  <summary>Details</summary>
Motivation: 集中式存储库容易受到网络攻击、技术故障、自然灾害或资金和政治不确定性的影响，从而导致数据不可用、数据丢失和完整性受损，最终阻碍科学进步。因此，需要重新评估集中式模型的可持续性。

Method: 检查集中式存储库的结构限制，评估联邦和分散式模型，并提出一个用于弹性、FAIR 和可持续科学数据管理的混合框架。

Result: 与集中式存储库相比，混合框架显著降低了治理不稳定、基础设施脆弱性和资金波动的影响，并促进了公平性和全球可访问性。

Conclusion: 开放科学的未来取决于整合互补的方法，以建立一个全球分布、经济上可持续且机构上强大的基础设施，从而保护科学数据作为公共产品，并确保持续的可访问性、互操作性和为后代保存。

Abstract: Continuous and reliable access to curated biological data repositories is
indispensable for accelerating rigorous scientific inquiry and fostering
reproducible research. Centralized repositories, though widely used, are
vulnerable to single points of failure arising from cyberattacks, technical
faults, natural disasters, or funding and political uncertainties. This can
lead to widespread data unavailability, data loss, integrity compromises, and
substantial delays in critical research, ultimately impeding scientific
progress. Centralizing essential scientific resources in a single geopolitical
or institutional hub is inherently dangerous, as any disruption can paralyze
diverse ongoing research. The rapid acceleration of data generation, combined
with an increasingly volatile global landscape, necessitates a critical
re-evaluation of the sustainability of centralized models. Implementing
federated and decentralized architectures presents a compelling and
future-oriented pathway to substantially strengthen the resilience of
scientific data infrastructures, thereby mitigating vulnerabilities and
ensuring the long-term integrity of data. Here, we examine the structural
limitations of centralized repositories, evaluate federated and decentralized
models, and propose a hybrid framework for resilient, FAIR, and sustainable
scientific data stewardship. Such an approach offers a significant reduction in
exposure to governance instability, infrastructural fragility, and funding
volatility, and also fosters fairness and global accessibility. The future of
open science depends on integrating these complementary approaches to establish
a globally distributed, economically sustainable, and institutionally robust
infrastructure that safeguards scientific data as a public good, further
ensuring continued accessibility, interoperability, and preservation for
generations to come.

</details>


### [138] [Gate-Based and Annealing-Based Quantum Algorithms for the Maximum K-Plex Problem](https://arxiv.org/abs/2509.19214)
*Xiaofan Li,Gao Cong,Rui Zhou*

Main category: cs.DB

TL;DR: 本文研究了最大k-plex问题（MKP），并提出了两种量子算法：基于量子门的qTKP和qMKP，以及基于量子退火的qaMKP。实验结果表明，这些算法在解决MKP问题上具有潜力。


<details>
  <summary>Details</summary>
Motivation: 现实世界的图数据通常包含噪声和不完美数据，传统的clique模型不适用。k-plex模型作为clique的扩展，更适合分析这些真实世界的图数据。最大k-plex问题在社交网络分析、社区发现、恐怖分子网络识别和图聚类等领域越来越受到关注。

Method: 本文提出了两种量子算法：qTKP通过结合量子搜索与图编码、度数计算、度数比较和大小确定来寻找给定大小的k-plex；qMKP使用二分搜索来逐步确定最大解。此外，通过将MKP重新表述为二次无约束二元优化问题，提出了首个基于量子退火的近似算法qaMKP。

Result: qTKP和qMKP实现了O*(1.42^n)的时间复杂度。通过在IBM量子模拟器和D-Wave量子计算机上进行实验，验证了算法的实际性能。

Conclusion: 本文提出的量子算法在解决最大k-plex问题上具有潜力，并且可以应用于各种clique扩展问题，例如n-clan和n-club。

Abstract: The $ k $-plex model, which allows each vertex to miss connections with up to
$ k $ neighbors, serves as a relaxation of the clique. Its adaptability makes
it more suitable for analyzing real-world graphs where noise and imperfect data
are common and the ideal clique model is often impractical. The problem of
identifying the maximum $ k $-plex (MKP, which is NP-hard) is gaining attention
in fields such as social network analysis, community detection, terrorist
network identification, and graph clustering. Recent works have focused on
optimizing the time complexity of MKP algorithms. The state-of-the-art has
reduced the complexity from a trivial $ O^*(2^n) $ to $ O^*(c_k^n) $, with $
c_k > 1.94 $ for $ k \geq 3 $, where $ n $ denotes the vertex number. This
paper investigates the MKP using two quantum models: gate-based model and
annealing-based model. Two gate-based algorithms, qTKP and qMKP, are proposed
to achieve $ O^*(1.42^n) $ time complexity. qTKP integrates quantum search with
graph encoding, degree counting, degree comparison, and size determination to
find a $ k $-plex of a given size; qMKP uses binary search to progressively
identify the maximum solution. Furthermore, by reformulating MKP as a quadratic
unconstrained binary optimization problem, we propose qaMKP, the first
annealing-based approximation algorithm, which utilizes qubit resources more
efficiently than gate-based algorithms. To validate the practical performance,
proof-of-principle experiments were conducted using the latest IBM gate-based
quantum simulator and D-Wave adiabatic quantum computer. This work holds
potential to be applied to a wide range of clique relaxations, e.g., $ n $-clan
and $ n $-club.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [139] [Understand your Users, An Ensemble Learning Framework for Natural Noise Filtering in Recommender Systems](https://arxiv.org/abs/2509.18560)
*Clarita Hawat,Wissam Al Jurdi,Jacques Bou Abdo,Jacques Demerjian,Abdallah Makhoul*

Main category: cs.IR

TL;DR: 本文提出了一种识别噪声评分的新框架，旨在为推荐系统提供更清晰的训练数据集，从而提高用户满意度和参与度。


<details>
  <summary>Details</summary>
Motivation: 网络内容的指数增长是推荐系统成功的关键，但用户偏好和行为的差异性带来了噪声问题。论文旨在解决定义噪声的挑战，区分了影响用户情绪的外部因素、意外偏好和偶然交互等现象。

Method: 该框架是模块化的，由三个层次组成：用于项目分类的已知自然噪声算法、用于改进项目评估的集成学习模型和基于签名的噪声识别。

Result: 论文进一步提倡定量评估偶然性和群体验证的指标，从而提供更高的推荐准确性。

Conclusion: 该方法旨在提供更清晰的训练数据集，从而提高用户满意度和推荐系统的参与度。

Abstract: The exponential growth of web content is a major key to the success for
Recommender Systems. This paper addresses the challenge of defining noise,
which is inherently related to variability in human preferences and behaviors.
In classifying changes in user tendencies, we distinguish three kinds of
phenomena: external factors that directly influence users' sentiment,
serendipity causing unexpected preference, and incidental interaction perceived
as noise. To overcome these problems, we present a new framework that
identifies noisy ratings. In this context, the proposed framework is modular,
consisting of three layers: known natural noise algorithms for item
classification, an Ensemble learning model for refined evaluation of the items
and signature-based noise identification. We further advocate the metrics that
quantitatively assess serendipity and group validation, offering higher
robustness in recommendation accuracy. Our approach aims to provide a cleaner
training dataset that would inherently improve user satisfaction and engagement
with Recommender Systems.

</details>


### [140] [The Ranking Blind Spot: Decision Hijacking in LLM-based Text Ranking](https://arxiv.org/abs/2509.18575)
*Yaoyao Qian,Yifan Zeng,Yuchao Jiang,Chelsi Jain,Huazheng Wang*

Main category: cs.IR

TL;DR: 大型语言模型在信息检索任务中表现出色。研究发现LLM在多文档比较任务中存在“排序盲点”，恶意内容提供者可利用此弱点影响LLM排序系统，使其倾向于特定内容，从而增加曝光度。实验证明攻击对多种LLM有效，且更强大的LLM更容易受到攻击。


<details>
  <summary>Details</summary>
Motivation: 研究LLM中指令跟随能力如何与多文档比较任务相互作用，并识别LLM决策过程中的“排序盲点”。

Method: 通过决策目标劫持和决策标准劫持两种方法，分析排序盲点如何影响LLM评估系统。通过改变成对排序系统中的评估目标和修改排序方案中的相关性标准来进行分析。

Result: 提出的攻击在各种LLM中有效，可以推广到多种排序方案。更强大的LLM更容易受到这些攻击。

Conclusion: 恶意内容提供者可以利用LLM的排序盲点，通过攻击排序器来获得额外的曝光。

Abstract: Large Language Models (LLMs) have demonstrated strong performance in
information retrieval tasks like passage ranking. Our research examines how
instruction-following capabilities in LLMs interact with multi-document
comparison tasks, identifying what we term the "Ranking Blind Spot", a
characteristic of LLM decision processes during comparative evaluation. We
analyze how this ranking blind spot affects LLM evaluation systems through two
approaches: Decision Objective Hijacking, which alters the evaluation goal in
pairwise ranking systems, and Decision Criteria Hijacking, which modifies
relevance standards across ranking schemes. These approaches demonstrate how
content providers could potentially influence LLM-based ranking systems to
affect document positioning. These attacks aim to force the LLM ranker to
prefer a specific passage and rank it at the top. Malicious content providers
can exploit this weakness, which helps them gain additional exposure by
attacking the ranker. In our experiment, We empirically show that the proposed
attacks are effective in various LLMs and can be generalized to multiple
ranking schemes. We apply these attack to realistic examples to show their
effectiveness. We also found stronger LLMs are more vulnerable to these
attacks. Our code is available at:
https://github.com/blindspotorg/RankingBlindSpot

</details>


### [141] [Agentic AutoSurvey: Let LLMs Survey LLMs](https://arxiv.org/abs/2509.18661)
*Yixin Liu,Yonghui Wu,Denghui Zhang,Lichao Sun*

Main category: cs.IR

TL;DR: Agentic AutoSurvey是一个用于自动生成文献综述的多代理框架，通过四个专业代理协同工作，在综合质量上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 科研文献的指数增长给研究人员综合快速发展的领域知识带来了前所未有的挑战。

Method: 采用多代理框架，包含论文搜索专家、主题挖掘与聚类、学术综述作者和质量评估员四个专业代理协同工作。

Result: 在COLM 2024的六个代表性LLM研究主题上进行的实验表明，我们的多代理方法相比现有基线取得了显著改进，评分从AutoSurvey的4.77/10提高到8.18/10。

Conclusion: 多代理架构代表了在快速发展的科学领域中自动文献综述生成的一个有意义的进步。

Abstract: The exponential growth of scientific literature poses unprecedented
challenges for researchers attempting to synthesize knowledge across rapidly
evolving fields. We present \textbf{Agentic AutoSurvey}, a multi-agent
framework for automated survey generation that addresses fundamental
limitations in existing approaches. Our system employs four specialized agents
(Paper Search Specialist, Topic Mining \& Clustering, Academic Survey Writer,
and Quality Evaluator) working in concert to generate comprehensive literature
surveys with superior synthesis quality. Through experiments on six
representative LLM research topics from COLM 2024 categories, we demonstrate
that our multi-agent approach achieves significant improvements over existing
baselines, scoring 8.18/10 compared to AutoSurvey's 4.77/10. The multi-agent
architecture processes 75--443 papers per topic (847 total across six topics)
while targeting high citation coverage (often $\geq$80\% on 75--100-paper sets;
lower on very large sets such as RLHF) through specialized agent orchestration.
Our 12-dimension evaluation captures organization, synthesis integration, and
critical analysis beyond basic metrics. These findings demonstrate that
multi-agent architectures represent a meaningful advancement for automated
literature survey generation in rapidly evolving scientific domains.

</details>


### [142] [Robust Denoising Neural Reranker for Recommender Systems](https://arxiv.org/abs/2509.18736)
*Wenyu Mao,Shuchang Liu,Hailan Yang,Xiaobei Wang,Xiaoyu Yang,Xu Gao,Xiang Li,Lantao Hu,Han Li,Kun Gai,An Zhang,Xiang Wang*

Main category: cs.IR

TL;DR: 提出了一种对抗框架DNR，用于解决多阶段推荐系统中检索器分数利用不足的问题。


<details>
  <summary>Details</summary>
Motivation: 检索器分数在重排序阶段的重要性被限制性地探索，而检索器分数是有信息的。

Method: 设计了一个对抗框架DNR，将去噪重排序器与精心设计的噪声生成模块相关联。通过三个增强目标扩展了传统的得分误差最小化项：1) 去噪目标，旨在去噪噪声检索器分数以与用户反馈对齐；2) 对抗性检索器分数生成目标，提高了检索器分数空间中的探索；3) 分布正则化项，旨在使生成的噪声检索器分数的分布与真实分布对齐。

Result: 在三个公共数据集上进行了广泛的实验，并提供了分析支持，验证了所提出的DNR的有效性。

Conclusion: DNR有效解决了多阶段推荐系统中检索器分数利用不足的问题。

Abstract: For multi-stage recommenders in industry, a user request would first trigger
a simple and efficient retriever module that selects and ranks a list of
relevant items, then calls a slower but more sophisticated deep reranking model
that refines the item arrangement before exposure to the user. The latter model
typically reranks the item list conditioned on the user's history content and
the initial ranking from retrievers. Although this two-stage retrieval-ranking
framework demonstrates practical effectiveness, the significance of retriever
scores from the previous stage has been limitedly explored, which is
informative. In this work, we first theoretically analyze the limitations of
using retriever scores as the rerankers' input directly and argue that the
reranking task is essentially a noise reduction problem from the retriever
scores. Following this notion, we derive an adversarial framework, DNR, that
associates the denoising reranker with a carefully designed noise generation
module. We extend the conventional score error minimization term with three
augmented objectives, including: 1) a denoising objective that aims to denoise
the noisy retriever scores to align with the user feedback; 2) an adversarial
retriever score generation objective that improves the exploration in the
retriever score space; and 3) a distribution regularization term that aims to
align the distribution of generated noisy retriever scores with the real ones.
Extensive experiments are conducted on three public datasets, together with
analytical support, validating the effectiveness of the proposed DNR.

</details>


### [143] [Single-Branch Network Architectures to Close the Modality Gap in Multimodal Recommendation](https://arxiv.org/abs/2509.18807)
*Christian Ganhör,Marta Moscati,Anna Hausberger,Shah Nawaz,Markus Schedl*

Main category: cs.IR

TL;DR: 本文提出了一种单分支神经网络，通过权重共享、模态抽样和对比损失来弥合模态差距，从而在缺失模态的情况下提供准确的推荐。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统依赖协同过滤，但在冷启动情况下效果不佳。混合推荐系统（HRSs）虽然可以结合协同信息和侧信息，但在模态缺失时质量下降。

Method: 利用配备权重共享、模态抽样和对比损失的单分支神经网络。

Result: 单分支网络在 warm-start 场景中表现出竞争性，在缺失模态设置中明显更好。该方法可以缩小嵌入空间中项目模态之间的差距。

Conclusion: 单分支网络可以有效弥合模态差距，在缺失模态场景中提供高质量推荐。

Abstract: Traditional recommender systems rely on collaborative filtering, using past
user-item interactions to help users discover new items in a vast collection.
In cold start, i.e., when interaction histories of users or items are not
available, content-based recommender systems use side information instead.
Hybrid recommender systems (HRSs) often employ multimodal learning to combine
collaborative and side information, which we jointly refer to as modalities.
Though HRSs can provide recommendations when some modalities are missing, their
quality degrades. In this work, we utilize single-branch neural networks
equipped with weight sharing, modality sampling, and contrastive loss to
provide accurate recommendations even in missing modality scenarios by
narrowing the modality gap. We compare these networks with multi-branch
alternatives and conduct extensive experiments on three datasets. Six
accuracy-based and four beyond-accuracy-based metrics help assess the
recommendation quality for the different training paradigms and their
hyperparameters in warm-start and missing modality scenarios. We quantitatively
and qualitatively study the effects of these different aspects on bridging the
modality gap. Our results show that single-branch networks achieve competitive
performance in warm-start scenarios and are significantly better in missing
modality settings. Moreover, our approach leads to closer proximity of an
item's modalities in the embedding space. Our full experimental setup is
available at https://github.com/hcai-mms/single-branch-networks.

</details>


### [144] [RELATE: Relation Extraction in Biomedical Abstracts with LLMs and Ontology Constraints](https://arxiv.org/abs/2509.19057)
*Olawumi Olasunkanmi,Mathew Satursky,Hong Yi,Chris Bizon,Harlin Lee,Stanley Ahalt*

Main category: cs.IR

TL;DR: RELATE是一个三阶段流程，它将LLM提取的关系映射到标准本体谓词，以实现生物医学知识图谱的构建。


<details>
  <summary>Details</summary>
Motivation: 生物医学知识图谱对于药物发现和临床决策支持至关重要，但仍不完整。大型语言模型擅长提取生物医学关系，但其输出缺乏标准化和与本体的对齐，限制了KG集成。

Method: RELATE包含三个阶段：(1) 使用谓词嵌入进行本体预处理，(2) 利用SapBERT增强的基于相似性的检索，(3) 基于LLM的重排序，并显式处理否定。

Result: 在ChemProt基准测试中，RELATE实现了52%的精确匹配和94%的accuracy@10。在2,400个HEAL项目摘要中，它有效地拒绝了不相关的关联（0.4%），并识别了否定的断言。

Conclusion: 通过结合向量搜索和上下文LLM推理，RELATE提供了一个可扩展、语义准确的框架，用于将非结构化的生物医学文献转换为标准化的KG。

Abstract: Biomedical knowledge graphs (KGs) are vital for drug discovery and clinical
decision support but remain incomplete. Large language models (LLMs) excel at
extracting biomedical relations, yet their outputs lack standardization and
alignment with ontologies, limiting KG integration. We introduce RELATE, a
three-stage pipeline that maps LLM-extracted relations to standardized ontology
predicates using ChemProt and the Biolink Model. The pipeline includes: (1)
ontology preprocessing with predicate embeddings, (2) similarity-based
retrieval enhanced with SapBERT, and (3) LLM-based reranking with explicit
negation handling. This approach transforms relation extraction from free-text
outputs to structured, ontology-constrained representations. On the ChemProt
benchmark, RELATE achieves 52% exact match and 94% accuracy@10, and in 2,400
HEAL Project abstracts, it effectively rejects irrelevant associations (0.4%)
and identifies negated assertions. RELATE captures nuanced biomedical
relationships while ensuring quality for KG augmentation. By combining vector
search with contextual LLM reasoning, RELATE provides a scalable, semantically
accurate framework for converting unstructured biomedical literature into
standardized KGs.

</details>


### [145] [A Knowledge Graph and a Tripartite Evaluation Framework Make Retrieval-Augmented Generation Scalable and Transparent](https://arxiv.org/abs/2509.19209)
*Olalekan K. Akindele,Bhupesh Kumar Mishra,Kenneth Y. Wertheim*

Main category: cs.IR

TL;DR: 本研究提出了一种RAG聊天机器人，该机器人利用知识图谱和向量搜索检索，以提供精确的、上下文丰富的响应，并最大限度地减少了文档分块的需要。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(llm)显著增强了会话人工智能(ai)聊天机器人;然而，特定领域的准确性和避免事实不一致仍然是紧迫的挑战，特别是对于大型数据集。

Method: 本研究提出了一种检索增强生成(RAG)聊天机器人，该机器人利用知识图谱和向量搜索检索。此外，还引入了RAG评估(RAG-Eval)，这是一个新颖的基于思维链LLM的三方评估框架，专门用于评估RAG应用程序。

Result: 实验比较表明，该方法在摘要评估任务中与BERTScore和g-eval相比是有效的，并且经验分析表明，RAG-Eval能够可靠地检测到事实差距和查询不匹配。

Conclusion: 研究结果强调了一条可扩展的路径，用于开发准确的、用户可验证的聊天机器人，弥合了高级对话流畅性和事实准确性之间的差距。

Abstract: Large Language Models (LLMs) have significantly enhanced conversational
Artificial Intelligence(AI) chatbots; however, domain-specific accuracy and the
avoidance of factual inconsistencies remain pressing challenges, particularly
for large datasets. Designing an effective chatbot with appropriate methods and
evaluating its effectiveness is among the challenges in this domain. This study
presents a Retrieval Augmented Generation (RAG) chatbot that harnesses a
knowledge graph and vector search retrieval to deliver precise, context-rich
responses in an exemplary use case from over high-volume engineering
project-related emails, thereby minimising the need for document chunking. A
central innovation of this work is the introduction of RAG Evaluation
(RAG-Eval), a novel chain-of-thought LLM-based tripartite evaluation framework
specifically developed to assess RAG applications. This framework operates in
parallel with the chatbot, jointly assessing the user's query, the retrieved
document, and the generated response, enabling a holistic evaluation across
multiple quality metrics like query relevance, factual accuracy, coverage,
coherence and fluency. The resulting scoring system is provided directly to
users as a confidence score (1 to 100%), enabling quick identification of
possible misaligned or incomplete answers. This proposed approach promotes
transparency and rapid verification by incorporating metadata email IDs,
timestamps into responses. Experimental comparisons against BERTScore and
G-EVAL for summarisation evaluation tasks confirm its effectiveness, and
empirical analysis also shows RAG-Eval reliably detects factual gaps and query
mismatches, thereby fostering trust in high demand, data centric environments.
These findings highlight a scalable path for developing accurate,
user-verifiable chatbots that bridge the gap between high-level conversational
fluency and factual accuracy.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [146] [Machine Learnability as a Measure of Order in Aperiodic Sequences](https://arxiv.org/abs/2509.18103)
*Jennifer Dodgson,Michael Joedhitya,Adith Ramdas,Surender Suresh Kumar,Adarsh Singh Chauhan,Akira Rafhael,Wang Mingshu,Nordine Lotfi*

Main category: cs.LG

TL;DR: 使用图像聚焦机器学习模型来测量特定区域 Ulam 螺旋中素数域的比较规律性。


<details>
  <summary>Details</summary>
Motivation: 素数分布的研究表明，它具有双重特征：确定性的定义，但表现出类似于随机过程的统计行为。

Method: 使用图像聚焦机器学习模型。

Result: 在纯粹的准确性方面，在螺旋区域附近提取的块上训练的模型优于在代表小于 25m 的整数的区域提取的块上训练的模型。模型在螺旋的不同区域采用不同的分类方法：在较低的数字中，模型更侧重于识别素数模式，而在较高的数字中，模型更侧重于消除合数。

Conclusion: 机器学习可以作为数论的一种新的实验工具。该方法显示出研究密码学中强素数和弱素数模式的潜力。

Abstract: Research on the distribution of prime numbers has revealed a dual character:
deterministic in definition yet exhibiting statistical behavior reminiscent of
random processes. In this paper we show that it is possible to use an
image-focused machine learning model to measure the comparative regularity of
prime number fields at specific regions of an Ulam spiral. Specifically, we
demonstrate that in pure accuracy terms, models trained on blocks extracted
from regions of the spiral in the vicinity of 500m outperform models trained on
blocks extracted from the region representing integers lower than 25m. This
implies existence of more easily learnable order in the former region than in
the latter. Moreover, a detailed breakdown of precision and recall scores seem
to imply that the model is favouring a different approach to classification in
different regions of the spiral, focusing more on identifying prime patterns at
lower numbers and more on eliminating composites at higher numbers. This aligns
with number theory conjectures suggesting that at higher orders of magnitude we
should see diminishing noise in prime number distributions, with averages
(density, AP equidistribution) coming to dominate, while local randomness
regularises after scaling by log x. Taken together, these findings point toward
an interesting possibility: that machine learning can serve as a new
experimental instrument for number theory. Notably, the method shows potential
1 for investigating the patterns in strong and weak primes for cryptographic
purposes.

</details>


### [147] [Data Valuation and Selection in a Federated Model Marketplace](https://arxiv.org/abs/2509.18104)
*Wenqian Li,Youjia Yang,Ruoxi Jia,Yan Pang*

Main category: cs.LG

TL;DR: 本文提出了一种基于Wasserstein的联邦学习框架，用于解决数据市场中数据估值和选择的问题，从而建立值得信赖的数据市场。


<details>
  <summary>Details</summary>
Motivation: 在人工智能时代，数据市场对于促进数据共享至关重要。联邦学习为建立值得信赖的数据市场提供了一个有前景的范例，但联邦学习设置中异构数据的有效估值和选择仍然是关键挑战。

Method: 本文提出了一个基于Wasserstein的估计器，用于预测模型在未见数据组合上的性能，并揭示数据异构性和联邦学习聚合算法之间的兼容性。此外，还提出了一种分布式方法来近似Wasserstein距离，并在神经缩放定律下推断模型性能。

Result: 在标签倾斜、错误标记和未标记源等多种场景下的大量实验表明，该方法能够持续识别高性能的数据组合。

Conclusion: 该方法为更可靠的基于联邦学习的模型市场铺平了道路。

Abstract: In the era of Artificial Intelligence (AI), marketplaces have become
essential platforms for facilitating the exchange of data products to foster
data sharing. Model transactions provide economic solutions in data
marketplaces that enhance data reusability and ensure the traceability of data
ownership. To establish trustworthy data marketplaces, Federated Learning (FL)
has emerged as a promising paradigm to enable collaborative learning across
siloed datasets while safeguarding data privacy. However, effective data
valuation and selection from heterogeneous sources in the FL setup remain key
challenges. This paper introduces a comprehensive framework centered on a
Wasserstein-based estimator tailored for FL. The estimator not only predicts
model performance across unseen data combinations but also reveals the
compatibility between data heterogeneity and FL aggregation algorithms. To
ensure privacy, we propose a distributed method to approximate Wasserstein
distance without requiring access to raw data. Furthermore, we demonstrate that
model performance can be reliably extrapolated under the neural scaling law,
enabling effective data selection without full-scale training. Extensive
experiments across diverse scenarios, such as label skew, mislabeled, and
unlabeled sources, show that our approach consistently identifies
high-performing data combinations, paving the way for more reliable FL-based
model marketplaces.

</details>


### [148] [BULL-ODE: Bullwhip Learning with Neural ODEs and Universal Differential Equations under Stochastic Demand](https://arxiv.org/abs/2509.18105)
*Nachiket N. Naik,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: 该论文研究了随机需求下连续时间库存动态的学习，并量化了结构化方法在预测牛鞭效应方面的帮助或阻碍作用。


<details>
  <summary>Details</summary>
Motivation: 研究领域中，经典供应链模型通过控制/预测选择和信息共享来解释牛鞭效应，而最近的物理信息和神经微分方程方法将领域约束与学习组件相结合。目前尚不清楚结构偏差在不同需求 regime 下是否有助于或阻碍预测。

Method: 该研究使用单级测试平台，采用三种需求 regime - AR(1)（自相关）、i.i.d. 高斯和重尾对数正态分布。在每个 trajectory 的不同 fraction 上进行训练，然后评估库存 I、订单率 O 和需求 D 的 multi-step 预测。

Result: 在结构化 regime 中，UDE 始终具有更好的泛化能力：在 90% 的训练 horizon 下，在 AR(1) 下，库存 RMSE 从 4.92 (NODE) 降至 0.26 (UDE)，在高斯需求下，库存 RMSE 从 5.96 降至 0.95。在重尾对数正态冲击下，NODE 的灵活性更好。

Conclusion: 当噪声为 light-tailed 或时间相关时，强制执行结构；当极端事件占主导地位时，放松结构。

Abstract: We study learning of continuous-time inventory dynamics under stochastic
demand and quantify when structure helps or hurts forecasting of the bullwhip
effect. BULL-ODE compares a fully learned Neural ODE (NODE) that models the
entire right-hand side against a physics-informed Universal Differential
Equation (UDE) that preserves conservation and order-up-to structure while
learning a small residual policy term. Classical supply chain models explain
the bullwhip through control/forecasting choices and information sharing, while
recent physics-informed and neural differential equation methods blend domain
constraints with learned components. It is unclear whether structural bias
helps or hinders forecasting under different demand regimes. We address this by
using a single-echelon testbed with three demand regimes - AR(1)
(autocorrelated), i.i.d. Gaussian, and heavy-tailed lognormal. Training is done
on varying fractions of each trajectory, followed by evaluation of multi-step
forecasts for inventory I, order rate O, and demand D. Across the structured
regimes, UDE consistently generalizes better: with 90% of the training horizon,
inventory RMSE drops from 4.92 (NODE) to 0.26 (UDE) under AR(1) and from 5.96
to 0.95 under Gaussian demand. Under heavy-tailed lognormal shocks, the
flexibility of NODE is better. These trends persist as train18 ing data
shrinks, with NODE exhibiting phase drift in extrapolation while UDE remains
stable but underreacts to rare spikes. Our results provide concrete guidance:
enforce structure when noise is light-tailed or temporally correlated; relax
structure when extreme events dominate. Beyond inventory control, the results
offer guidance for hybrid modeling in scientific and engineering systems:
enforce known structure when conservation laws and modest noise dominate, and
relax structure to capture extremes in settings where rare events drive
dynamics.

</details>


### [149] [Model-Based Transfer Learning for Real-Time Damage Assessment of Bridge Networks](https://arxiv.org/abs/2509.18106)
*Elisa Tomassini,Enrique García-Macías,Filippo Ubertini*

Main category: cs.LG

TL;DR: 提出了一种基于模型迁移学习的方法，使用神经网络代理模型，使得在一个桥梁上训练的模型能够适应于另一个具有相似特征的桥梁。


<details>
  <summary>Details</summary>
Motivation: 永久监测系统的日益普及增加了数据的可用性，为结构评估提供了新的机会，但也带来了可扩展性挑战，尤其是在大型桥梁网络中。管理多个结构需要有效地跟踪和比较长期行为。为了解决这个问题，相似结构之间的知识转移变得至关重要。

Method: 提出了一种基于模型迁移学习的方法，使用神经网络代理模型，该模型经过在一个桥梁上的训练，可以适应于另一个具有相似特征的桥梁。这些模型捕获了共享的损伤机制，支持可扩展和通用的监测框架。该方法使用来自两座桥梁的真实数据进行了验证。将转移模型集成到贝叶斯推理框架中，以基于监测数据中的模态特征进行连续损伤评估。

Result: 结果表明，该方法对损伤位置、严重程度和范围具有很高的敏感性。

Conclusion: 这种方法增强了实时监测，实现了跨结构知识转移，从而在网络层面促进了智能监测策略和提高的弹性。

Abstract: The growing use of permanent monitoring systems has increased data
availability, offering new opportunities for structural assessment but also
posing scalability challenges, especially across large bridge networks.
Managing multiple structures requires tracking and comparing long-term
behaviour efficiently. To address this, knowledge transfer between similar
structures becomes essential. This study proposes a model-based transfer
learning approach using neural network surrogate models, enabling a model
trained on one bridge to be adapted to another with similar characteristics.
These models capture shared damage mechanisms, supporting a scalable and
generalizable monitoring framework. The method was validated using real data
from two bridges. The transferred model was integrated into a Bayesian
inference framework for continuous damage assessment based on modal features
from monitoring data. Results showed high sensitivity to damage location,
severity, and extent. This approach enhances real-time monitoring and enables
cross-structure knowledge transfer, promoting smart monitoring strategies and
improved resilience at the network level.

</details>


### [150] [AdaMixT: Adaptive Weighted Mixture of Multi-Scale Expert Transformers for Time Series Forecasting](https://arxiv.org/abs/2509.18107)
*Huanyao Zhang,Jiaye Lin,Wentao Zhang,Haitao Yuan,Guoliang Li*

Main category: cs.LG

TL;DR: 提出了一种名为 AdaMixT 的新架构，用于多变量时间序列预测，通过自适应多尺度融合实现更准确的预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖于预定义的单尺度patches或缺乏有效的多尺度特征融合机制，无法充分捕捉时间序列中固有的复杂模式，导致性能受限和泛化性不足。

Method: AdaMixT 引入了各种patches，并利用通用预训练模型 (GPM) 和特定领域模型 (DSM) 进行多尺度特征提取。为了适应时间特征的异质性，AdaMixT 包含一个门控网络，该网络在不同的专家之间动态分配权重，从而通过自适应多尺度融合实现更准确的预测。

Result: 在八个广泛使用的基准测试（包括 Weather、Traffic、Electricity、ILI 和四个 ETT 数据集）上进行的综合实验一致证明了 AdaMixT 在实际场景中的有效性。

Conclusion: AdaMixT 架构在多变量时间序列预测中表现出色，并在实际场景中具有有效性。

Abstract: Multivariate time series forecasting involves predicting future values based
on historical observations. However, existing approaches primarily rely on
predefined single-scale patches or lack effective mechanisms for multi-scale
feature fusion. These limitations hinder them from fully capturing the complex
patterns inherent in time series, leading to constrained performance and
insufficient generalizability. To address these challenges, we propose a novel
architecture named Adaptive Weighted Mixture of Multi-Scale Expert Transformers
(AdaMixT). Specifically, AdaMixT introduces various patches and leverages both
General Pre-trained Models (GPM) and Domain-specific Models (DSM) for
multi-scale feature extraction. To accommodate the heterogeneity of temporal
features, AdaMixT incorporates a gating network that dynamically allocates
weights among different experts, enabling more accurate predictions through
adaptive multi-scale fusion. Comprehensive experiments on eight widely used
benchmarks, including Weather, Traffic, Electricity, ILI, and four ETT
datasets, consistently demonstrate the effectiveness of AdaMixT in real-world
scenarios.

</details>


### [151] [Solve it with EASE](https://arxiv.org/abs/2509.18108)
*Adam Viktorin,Tomas Kadavy,Jozef Kovac,Michal Pluhacek,Roman Senkerik*

Main category: cs.LG

TL;DR: EASE是一个开源的、完全模块化的框架，用于迭代算法解决方案生成，利用大型语言模型（LLM）。


<details>
  <summary>Details</summary>
Motivation: EASE集成了生成、测试、分析和评估到一个可重复的反馈循环中，让用户完全控制错误处理、分析和质量评估。

Method: EASE的架构支持在互补角色中协调多个LLM，例如生成器、分析器和评估器。

Result: 通过抽象提示设计和模型管理的复杂性，EASE为研究人员和从业人员提供了一个透明和可扩展的平台，用于在不同领域共同设计算法和其他生成解决方案。

Conclusion: EASE是一个很有潜力的框架，它简化了算法设计的过程，并为研究人员和从业人员提供了更多的控制和灵活性。

Abstract: This paper presents EASE (Effortless Algorithmic Solution Evolution), an
open-source and fully modular framework for iterative algorithmic solution
generation leveraging large language models (LLMs). EASE integrates generation,
testing, analysis, and evaluation into a reproducible feedback loop, giving
users full control over error handling, analysis, and quality assessment. Its
architecture supports the orchestration of multiple LLMs in complementary
roles-such as generator, analyst, and evaluator. By abstracting the complexity
of prompt design and model management, EASE provides a transparent and
extensible platform for researchers and practitioners to co-design algorithms
and other generative solutions across diverse domains.

</details>


### [152] [Individualized non-uniform quantization for vector search](https://arxiv.org/abs/2509.18471)
*Mariano Tepper,Ted Willke*

Main category: cs.LG

TL;DR: 提出了一种新的向量压缩技术NVQ（非均匀向量量化），在高保真状态下具有计算和空间效率。


<details>
  <summary>Details</summary>
Motivation: 高维向量的大小给现代向量搜索技术带来了问题：从内存/存储中检索大型向量的成本很高，而且占用空间很大。

Method: 使用新的简约和计算高效的非线性来构建非均匀向量量化器, 并且这些量化器是为每个索引向量单独学习的。

Result: NVQ 表现出比现有技术更高的准确性，且计算成本最低。

Conclusion: NVQ是一种有效的向量压缩技术，在高保真状态下具有计算和空间效率，优于现有技术。

Abstract: Embedding vectors are widely used for representing unstructured data and
searching through it for semantically similar items. However, the large size of
these vectors, due to their high-dimensionality, creates problems for modern
vector search techniques: retrieving large vectors from memory/storage is
expensive and their footprint is costly. In this work, we present NVQ
(non-uniform vector quantization), a new vector compression technique that is
computationally and spatially efficient in the high-fidelity regime. The core
in NVQ is to use novel parsimonious and computationally efficient
nonlinearities for building non-uniform vector quantizers. Critically, these
quantizers are \emph{individually} learned for each indexed vector. Our
experimental results show that NVQ exhibits improved accuracy compared to the
state of the art with a minimal computational cost.

</details>


### [153] [Machine Learning-Based Classification of Vessel Types in Straits Using AIS Tracks](https://arxiv.org/abs/2509.18109)
*Jonatan Katz Nielsen*

Main category: cs.LG

TL;DR: 本文提出了一种基于AIS数据的海峡尺度船舶类型识别机器学习流程，用于安全监管和打击非法活动。


<details>
  <summary>Details</summary>
Motivation: 准确识别船舶类型对于安全监管和打击非法、未报告和无管制的活动至关重要。

Method: 该方法使用来自波罗的海Bornholm海峡的AIS数据，经过数据预处理和特征提取后，使用基于树的模型进行分类，并采用分组训练/测试分割和分层5折交叉验证以避免数据泄露。

Result: 随机森林模型在测试集上达到92.15%的准确率（macro-precision 94.11%, macro-recall 92.51%, macro-F1 93.27%），并且bridge-position ratio和最大SOG是最具区分性的特征。

Conclusion: 结果表明，AIS轨迹上的轻量级特征能够实现在海峡中进行实时船舶类型分类。

Abstract: Accurate recognition of vessel types from Automatic Identification System
(AIS) tracks is essential for safety oversight and combating illegal,
unreported, and unregulated (IUU) activity. This paper presents a strait-scale,
machine-learning pipeline that classifies moving vessels using only AIS data.
We analyze eight days of historical AIS from the Danish Maritime Authority
covering the Bornholm Strait in the Baltic Sea (January 22-30, 2025). After
forward/backward filling voyage records, removing kinematic and geospatial
outliers, and segmenting per-MMSI tracks while excluding stationary periods
($\ge 1$ h), we derive 31 trajectory-level features spanning kinematics (e.g.,
SOG statistics), temporal, geospatial (Haversine distances, spans), and
ship-shape attributes computed from AIS A/B/C/D reference points (length,
width, aspect ratio, bridge-position ratio). To avoid leakage, we perform
grouped train/test splits by MMSI and use stratified 5-fold cross-validation.
Across five classes (cargo, tanker, passenger, high-speed craft, fishing;
N=1{,}910 trajectories; test=382), tree-based models dominate: a Random Forest
with SMOTE attains 92.15% accuracy (macro-precision 94.11%, macro-recall
92.51%, macro-F1 93.27%) on the held-out test set, while a tuned RF reaches
one-vs-rest ROC-AUC up to 0.9897. Feature-importance analysis highlights the
bridge-position ratio and maximum SOG as the most discriminative signals;
principal errors occur between cargo and tanker, reflecting similar transit
behavior. We demonstrate operational value by backfilling missing ship types on
unseen data and discuss improvements such as DBSCAN based trip segmentation and
gradient-boosted ensembles to handle frequent-stop ferries and further lift
performance. The results show that lightweight features over AIS trajectories
enable real-time vessel type classification in straits.

</details>


### [154] [Localized PCA-Net Neural Operators for Scalable Solution Reconstruction of Elliptic PDEs](https://arxiv.org/abs/2509.18110)
*Mrigank Dhingra,Romit Maulik,Adil Rasheed,Omer San*

Main category: cs.LG

TL;DR: 提出了一种基于patch的PCA-Net框架，用于解决偏微分方程（PDEs）的神经算子学习问题，旨在降低计算复杂度并保持高精度。


<details>
  <summary>Details</summary>
Motivation: 在高维解域中应用主成分分析（PCA）会产生巨大的计算开销。

Method: 将解域分解为更小的patch，在每个patch中应用PCA，并在降维的PCA空间中训练神经算子。研究了两种不同的基于patch的方法：局部到全局patch PCA和局部到局部patch PCA。此外，还探索了两种改进计算效率最高的方法的策略：引入带有平滑滤波器的重叠patch和采用带有卷积神经网络（CNN）的两步细化过程。

Result: 基于patch的PCA显著降低了计算复杂度，同时保持了高精度，与全局PCA相比，端到端pipeline处理时间减少了3.7到4倍。

Conclusion: 基于patch的PCA是一种有前景的有效算子学习技术，适用于基于PDE的系统。

Abstract: Neural operator learning has emerged as a powerful approach for solving
partial differential equations (PDEs) in a data-driven manner. However,
applying principal component analysis (PCA) to high-dimensional solution fields
incurs significant computational overhead. To address this, we propose a
patch-based PCA-Net framework that decomposes the solution fields into smaller
patches, applies PCA within each patch, and trains a neural operator in the
reduced PCA space. We investigate two different patch-based approaches that
balance computational efficiency and reconstruction accuracy: (1)
local-to-global patch PCA, and (2) local-to-local patch PCA. The trade-off
between computational cost and accuracy is analyzed, highlighting the
advantages and limitations of each approach. Furthermore, within each approach,
we explore two refinements for the most computationally efficient method: (i)
introducing overlapping patches with a smoothing filter and (ii) employing a
two-step process with a convolutional neural network (CNN) for refinement. Our
results demonstrate that patch-based PCA significantly reduces computational
complexity while maintaining high accuracy, reducing end-to-end pipeline
processing time by a factor of 3.7 to 4 times compared to global PCA, thefore
making it a promising technique for efficient operator learning in PDE-based
systems.

</details>


### [155] [Prompt Optimization Meets Subspace Representation Learning for Few-shot Out-of-Distribution Detection](https://arxiv.org/abs/2509.18111)
*Faizul Rakib Sayem,Shahana Ibrahim*

Main category: cs.LG

TL;DR: 本文提出了一种基于CoOp的框架，该框架集成了子空间表示学习与提示调整，以提高ID-OOD的可分离性。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示学习的OOD方法仅依赖于softmax概率，忽略了在数百万个样本上训练的VLM学习到的特征嵌入的丰富判别潜力。

Method: 该方法通过将ID特征投影到提示向量所跨越的子空间中，同时将ID不相关的特征投影到正交零空间中，从而提高ID-OOD的可分离性。此外，还设计了一种易于处理的端到端学习准则，以确保强大的OOD检测性能以及较高的ID分类精度。

Result: 在真实世界数据集上的实验表明了该方法的有效性。

Conclusion: 该文提出了一种新颖的基于CoOp的框架，可以有效提高ID-OOD的可分离性，并在真实世界数据集上取得了良好的效果。

Abstract: The reliability of artificial intelligence (AI) systems in open-world
settings depends heavily on their ability to flag out-of-distribution (OOD)
inputs unseen during training. Recent advances in large-scale vision-language
models (VLMs) have enabled promising few-shot OOD detection frameworks using
only a handful of in-distribution (ID) samples. However, existing prompt
learning-based OOD methods rely solely on softmax probabilities, overlooking
the rich discriminative potential of the feature embeddings learned by VLMs
trained on millions of samples. To address this limitation, we propose a novel
context optimization (CoOp)-based framework that integrates subspace
representation learning with prompt tuning. Our approach improves ID-OOD
separability by projecting the ID features into a subspace spanned by prompt
vectors, while projecting ID-irrelevant features into an orthogonal null space.
To train such OOD detection framework, we design an easy-to-handle end-to-end
learning criterion that ensures strong OOD detection performance as well as
high ID classification accuracy. Experiments on real-world datasets showcase
the effectiveness of our approach.

</details>


### [156] [Large language models surpass domain-specific architectures for antepartum electronic fetal monitoring analysis](https://arxiv.org/abs/2509.18112)
*Sheng Wong,Ravi Shankar,Beth Albert,Gabriel Davis Jones*

Main category: cs.LG

TL;DR: 本研究探索了大型语言模型（LLM）在电子胎儿监护（EFM）/胎心宫缩图（CTG）分析中的潜力，这是一项评估胎儿健康的关键技术。


<details>
  <summary>Details</summary>
Motivation: 由于胎儿心率（FHR）模式和子宫活动的复杂性，产前CTG解释提出了独特的挑战，并且CTG的评估主要基于主观的临床解释，通常导致诊断准确性的变化和偏离及时的妊娠护理。

Method: 我们系统地比较了时间序列FMs和LLMs与已建立的CTG特定架构。

Result: 微调的LLM在性能上优于基础模型和特定领域的方法，为临床CTG解释提供了一条有希望的替代途径。

Conclusion: 这些发现为不同AI方法在胎儿监护应用中的相对优势提供了重要的见解，并为未来产前护理中的临床AI发展奠定了基础。

Abstract: Foundation models (FMs) and large language models (LLMs) demonstrate
remarkable capabilities across diverse domains through training on massive
datasets. These models have demonstrated exceptional performance in healthcare
applications, yet their potential for electronic fetal monitoring
(EFM)/cardiotocography (CTG) analysis, a critical technology for evaluating
fetal well-being, remains largely underexplored. Antepartum CTG interpretation
presents unique challenges due to the complex nature of fetal heart rate (FHR)
patterns and uterine activity, requiring sophisticated analysis of long
time-series data. The assessment of CTG is heavily based on subjective clinical
interpretation, often leading to variability in diagnostic accuracy and
deviation from timely pregnancy care. This study presents the first
comprehensive comparison of state-of-the-art AI approaches for automated
antepartum CTG analysis. We systematically compare time-series FMs and LLMs
against established CTG-specific architectures. Our evaluation encompasses over
500 CTG recordings of varying durations reflecting real-world clinical
recordings, providing robust performance benchmarks across different modelling
paradigms. Our results demonstrate that fine-tuned LLMs achieve superior
performance compared to both foundation models and domain-specific approaches,
offering a promising alternative pathway for clinical CTG interpretation. These
findings provide critical insights into the relative strengths of different AI
methodologies for fetal monitoring applications and establish a foundation for
future clinical AI development in prenatal care.

</details>


### [157] [A Study of Skews, Imbalances, and Pathological Conditions in LLM Inference Deployment on GPU Clusters detectable from DPU](https://arxiv.org/abs/2509.18114)
*Javed I. Khan an Henry Uwabor Moye*

Main category: cs.LG

TL;DR: 本研究探讨了大型语言模型（LLM）中由于GPU负载不平衡导致的运行效率问题，并提出了一种基于DPU辅助的框架，用于实时检测和缓解多节点张量并行推理中的负载不平衡。


<details>
  <summary>Details</summary>
Motivation: 大型Transformer语言模型在自回归推理时面临运行效率的挑战，尤其是在解码阶段，GPU负载不平衡会导致吞吐量下降和延迟峰值。

Method: 利用BlueField-3数据处理单元的DPU辅助框架，通过将监控任务卸载到DPU，并分析GPU遥测和节点间通信模式，从而实现实时检测和缓解负载不平衡。

Result: 该系统能够为推理控制器和调度器提供可操作的反馈。

Conclusion: 本研究旨在识别LLM张量计算在多GPU执行中出现的偏差/不平衡/病态条件，评估它们对计算性能的影响，并 критически 评估是否可以从DPU网络跟踪这些条件以进行潜在的缓解。

Abstract: Autoregressive inference in large transformer-based language models (LLMs)
presents significant challenges for runtime efficiency, particularly during the
decode phase where load imbalance across GPU shards can cause throughput
degradation and latency spikes. A DPU-assisted framework leveraged by
BlueField-3 Data Processing Units can enable real-time detection and mitigation
of load imbalance in multi-node tensor-parallel inference. By offloading
monitoring tasks to the DPU and analyzing GPU telemetry and inter-node
communication patterns, the resulting system can provide actionable feedback to
inference controllers and schedulers. The goal of this study is three-fold i)
identify the reported skews/imbalances/pathological conditions that arise in
muti-GPU execution of a) LLM tensor computing (both during training and
inference), b) identify their impact on computational performance, and c) make
a critical assessment if those can be tracked for potential mitigation from a
DPU network.

</details>


### [158] [Towards Scalable and Structured Spatiotemporal Forecasting](https://arxiv.org/abs/2509.18115)
*Hongyi Chen,Xiucheng Li,Xinyang Chen,Jing Li,Kehai Chen,Liqiang Nie*

Main category: cs.LG

TL;DR: 本文提出了一种新的空间平衡注意力块，用于时空预测。


<details>
  <summary>Details</summary>
Motivation: 为了在服从空间邻近性和捕获全局相关性之间取得平衡。

Method: 将空间图划分为一组子图，并实例化子图内注意力以学习每个子图内的局部空间相关性；为了捕获全局空间相关性，进一步聚合节点以产生子图表示，并通过子图间注意力实现子图之间的消息传递。在此基础上，通过逐步增加子图尺度，开发了一个多尺度时空预测模型。

Result: 在真实世界的、中到大型的时空数据集上，针对现有模型评估了该模型的有效性和效率。实验结果表明，在较低的运行成本下，该模型可以实现高达7.7%的性能提升。

Conclusion: 该模型具有可扩展性，能够产生结构化的空间相关性，同时易于实现。

Abstract: In this paper, we propose a novel Spatial Balance Attention block for
spatiotemporal forecasting. To strike a balance between obeying spatial
proximity and capturing global correlation, we partition the spatial graph into
a set of subgraphs and instantiate Intra-subgraph Attention to learn local
spatial correlation within each subgraph; to capture the global spatial
correlation, we further aggregate the nodes to produce subgraph representations
and achieve message passing among the subgraphs via Inter-subgraph Attention.
Building on the proposed Spatial Balance Attention block, we develop a
multiscale spatiotemporal forecasting model by progressively increasing the
subgraph scales. The resulting model is both scalable and able to produce
structured spatial correlation, and meanwhile, it is easy to implement. We
evaluate its efficacy and efficiency against the existing models on real-world
spatiotemporal datasets from medium to large sizes. The experimental results
show that it can achieve performance improvements up to 7.7% over the baseline
methods at low running costs.

</details>


### [159] [Amortized Latent Steering: Low-Cost Alternative to Test-Time Optimization](https://arxiv.org/abs/2509.18116)
*Nathan Egbuna,Saatvik Gaur,Sunishchal Dev,Ashwinee Panda,Maheep Chaudhary*

Main category: cs.LG

TL;DR: 提出了一种名为Amortized Latent Steering (ALS) 的新方法，该方法通过离线计算向量来引导隐藏表示，从而在推理时以恒定成本应用，加速了测试时优化。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时优化方法（如迭代优化）计算成本过高，而潜空间方法（如LatentSeek）仍然需要昂贵的单查询优化循环。

Method: ALS计算成功和不成功生成的隐藏状态之间的平均差异，并使用该方向来校准模型的隐藏表示，将迭代优化折叠成单个离线计算的向量。

Result: 在GSM8K和MATH-500基准测试中，ALS比迭代方法快2-5倍，同时匹配或超过了贪婪的Chain-of-Thought (CoT) 和Self-Consistency基线，效率-准确性权衡提高了高达101%。

Conclusion: 离线捕获潜在优化的大部分好处是可行的，这使得复杂的推理技术适用于生产部署。

Abstract: Test-time optimization remains impractical at scale due to prohibitive
inference costs\textemdash techniques like iterative refinement and multi-step
verification can require $10$--$100\times$ more compute per query than standard
decoding. Latent space test-time optimization methods like LatentSeek offer a
more direct approach by steering hidden representations, but still demand
expensive per-query optimization loops with multiple backward passes. We
propose Amortized Latent Steering (ALS), which collapses this iterative
optimization into a single offline-computed vector applied at constant cost
during inference. ALS computes the mean difference between hidden states from
successful versus unsuccessful generations, then uses this direction to
calibrate the model's hidden representations: when decoding drifts away from
the success manifold, ALS nudges activations back toward it. Across GSM8K and
MATH-$500$ benchmarks, ALS achieves $2$--$5\times$ speedup over iterative
methods while matching or surpassing greedy Chain-of-Thought (CoT) and
Self-Consistency baselines, yielding up to 101\% improvement in
efficiency--accuracy trade-off. These results show that much of latent
optimization's benefit can be captured offline, making sophisticated reasoning
techniques viable for production deployment. Code is available
at~\href{https://anonymous.4open.science/r/steering-17F2}{https://anonymous.4open.science/r/steering-17F2}

</details>


### [160] [Robust and continuous machine learning of usage habits to adapt digital interfaces to user needs](https://arxiv.org/abs/2509.18117)
*Eric Petit,Denis Chêne*

Main category: cs.LG

TL;DR: 本文提出了一种机器学习方法，用于设计能够动态适应不同用户和使用策略的数字界面。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机在于创建能够根据个人用户行为习惯进行调整的界面，从而提升用户体验。

Method: 该算法使用贝叶斯统计来建模用户的浏览行为，侧重于个人习惯而非群体偏好，并采用在线增量学习。

Result: 仿真结果表明该方法在静态和非静态环境中均有效。

Conclusion: 该研究为自适应系统铺平了道路，这些系统通过帮助用户更好地导航和操作界面来改善用户体验。

Abstract: The paper presents a machine learning approach to design digital interfaces
that can dynamically adapt to different users and usage strategies. The
algorithm uses Bayesian statistics to model users' browsing behavior, focusing
on their habits rather than group preferences. It is distinguished by its
online incremental learning, allowing reliable predictions even with little
data and in the case of a changing environment. This inference method generates
a task model, providing a graphical representation of navigation with the usage
statistics of the current user. The algorithm learns new tasks while preserving
prior knowledge. The theoretical framework is described, and simulations show
the effectiveness of the approach in stationary and non-stationary
environments. In conclusion, this research paves the way for adaptive systems
that improve the user experience by helping them to better navigate and act on
their interface.

</details>


### [161] [Decentor-V: Lightweight ML Training on Low-Power RISC-V Edge Devices](https://arxiv.org/abs/2509.18118)
*Marcelo Ribeiro,Diogo Costa,Gonçalo Moreira,Sandro Pinto,Tiago Gomes*

Main category: cs.LG

TL;DR: 本研究针对RISC-V MCU上缺乏设备端训练支持的问题，通过L-SGD算法的改进，实现了高效的联邦学习。


<details>
  <summary>Details</summary>
Motivation: 物联网设备依赖机器学习进行本地数据处理，但设备算力不足和隐私问题限制了应用。联邦学习为此提供了一种解决方案，但在资源受限的设备上需要高效的优化算法。

Method: 将L-SGD算法扩展到RISC-V MCU，并提出了一种8位量化版本的L-SGD算法。

Result: 8位量化L-SGD算法在RISC-V平台上实现了近4倍的内存减少和2.2倍的训练加速，且精度损失可忽略不计。

Conclusion: 该研究成功地在RISC-V MCU上实现了高效的设备端训练，为联邦学习在资源受限的物联网设备上的应用提供了可能。

Abstract: Modern IoT devices increasingly rely on machine learning solutions to process
data locally. However, the lack of graphics processing units (GPUs) or
dedicated accelerators on most platforms makes on-device training largely
infeasible, often requiring cloud-based services to perform this task. This
procedure often raises privacy-related concerns, and creates dependency on
reliable and always-on connectivity. Federated Learning (FL) is a new trend
that addresses these issues by enabling decentralized and collaborative
training directly on devices, but it requires highly efficient optimization
algorithms. L-SGD, a lightweight variant of stochastic gradient descent, has
enabled neural network training on Arm Cortex-M Microcontroller Units (MCUs).
This work extends L-SGD to RISC-V-based MCUs, an open and emerging architecture
that still lacks robust support for on-device training. L-SGD was evaluated on
both Arm and RISC-V platforms using 32-bit floating-point arithmetic,
highlighting the performance impact of the absence of Floating-Point Units
(FPUs) in RISC-V MCUs. To mitigate these limitations, we introduce an 8-bit
quantized version of L-SGD for RISC-V, which achieves nearly 4x reduction in
memory usage and a 2.2x speedup in training time, with negligible accuracy
degradation.

</details>


### [162] [MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents](https://arxiv.org/abs/2509.18119)
*Yifan Xu,Xiao Liu,Xinghan Liu,Jiaqi Fu,Hanchen Zhang,Bohao Jing,Shudan Zhang,Yuting Wang,Wenyi Zhao,Yuxiao Dong*

Main category: cs.LG

TL;DR: MOBILERL: 使用难度自适应 GRPO 算法，通过难度自适应正向回放、失败课程过滤以及最短路径奖励调整，来提升移动 GUI 代理的性能，并在 AndroidWorld 和 AndroidLab 上取得了 SOTA 结果。


<details>
  <summary>Details</summary>
Motivation: 现有的基于强化学习的移动 GUI 代理在任务难度分布不均和环境采样效率低下面临挑战。

Method: 提出了一种在线代理强化学习框架 MOBILERL，其核心是难度自适应 GRPO (ADAGRPO) 算法，包含难度自适应正向回放、失败课程过滤和最短路径奖励调整策略。

Result: MOBILERL-9B 模型在 AndroidWorld (75.8%) 和 AndroidLab (46.8%) 上取得了 state-of-the-art 的成功率。

Conclusion: MOBILERL 框架已被 AutoGLM 产品采用，并已开源。

Abstract: Building general-purpose graphical user interface (GUI) agents has become
increasingly promising with the progress in vision language models. However,
developing effective mobile GUI agents with reinforcement learning (RL) remains
challenging due to the heavy-tailed distribution of task difficulty and the
inefficiency of large-scale environment sampling. We present an online agentic
reinforcement learning framework MOBILERL to enhance GUI agents in mobile
environments. Its core component is the Difficulty-Adaptive GRPO (ADAGRPO)
algorithm. In ADAGRPO, we design difficulty-adaptive positive replay and
failure curriculum filtering to adapt the model to different task difficulties.
We introduce the shortest path reward adjustment strategy to reshape rewards
concerning the task length in multi-turn agentic tasks. Those strategies
jointly stabilize RL training, improve sample efficiency, and generate strong
performance across diverse mobile apps and tasks. We apply MOBILERL to two open
models (Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base). The resultant MOBILERL-9B
model achieves state-of-the-art results in terms of success rates on both
AndroidWorld (75.8%) and AndroidLab (46.8%). The MOBILERL framework is adopted
in the AutoGLM products, and also open-sourced at
https://github.com/THUDM/MobileRL.

</details>


### [163] [A Coopetitive-Compatible Data Generation Framework for Cross-silo Federated Learning](https://arxiv.org/abs/2509.18120)
*Thanh Linh Nguyen,Quoc-Viet Pham*

Main category: cs.LG

TL;DR: 提出CoCoGen，一个兼容合作竞争的数据生成框架，利用生成人工智能和潜在博弈论来建模、分析和优化异构和竞争环境下的协同学习。


<details>
  <summary>Details</summary>
Motivation: 跨机构联邦学习使组织能够在保护数据隐私的同时协作训练人工智能模型，但经济竞争使组织不愿参与联合训练，并且统计异构性和组织间竞争对组织行为和社会福利的综合影响仍未得到充分探索。

Method: 通过学习性能和基于效用的公式来表征竞争和统计异构性，并将每个训练轮次建模为加权势博弈。然后，我们推导出基于 GenAI 的数据生成策略，以最大化社会福利。

Result: 在 Fashion-MNIST 数据集上的实验结果表明，不同的异构性和竞争水平如何影响组织行为，并证明 CoCoGen 始终优于基线方法。

Conclusion: CoCoGen 框架在异构和竞争环境中，通过生成对抗网络进行数据生成，可以有效提升联邦学习的社会福利。

Abstract: Cross-silo federated learning (CFL) enables organizations (e.g., hospitals or
banks) to collaboratively train artificial intelligence (AI) models while
preserving data privacy by keeping data local. While prior work has primarily
addressed statistical heterogeneity across organizations, a critical challenge
arises from economic competition, where organizations may act as market rivals,
making them hesitant to participate in joint training due to potential utility
loss (i.e., reduced net benefit). Furthermore, the combined effects of
statistical heterogeneity and inter-organizational competition on
organizational behavior and system-wide social welfare remain underexplored. In
this paper, we propose CoCoGen, a coopetitive-compatible data generation
framework, leveraging generative AI (GenAI) and potential game theory to model,
analyze, and optimize collaborative learning under heterogeneous and
competitive settings. Specifically, CoCoGen characterizes competition and
statistical heterogeneity through learning performance and utility-based
formulations and models each training round as a weighted potential game. We
then derive GenAI-based data generation strategies that maximize social
welfare. Experimental results on the Fashion-MNIST dataset reveal how varying
heterogeneity and competition levels affect organizational behavior and
demonstrate that CoCoGen consistently outperforms baseline methods.

</details>


### [164] [Prediction of Coffee Ratings Based On Influential Attributes Using SelectKBest and Optimal Hyperparameters](https://arxiv.org/abs/2509.18124)
*Edmund Agyemang,Lawrence Agbota,Vincent Agbenyeavu,Peggy Akabuah,Bismark Bimpong,Christopher Attafuah*

Main category: cs.LG

TL;DR: 本研究利用监督学习算法，通过用户评论中的文本和数值属性预测咖啡评分。


<details>
  <summary>Details</summary>
Motivation: 旨在通过数据驱动方法补充传统的咖啡品尝，为感官产品评价构建强大的预测系统。

Method: 使用TF-IDF进行特征提取，SelectKBest进行特征选择，并训练六个模型（决策树、K近邻、多层感知器、随机森林、Extra Trees和XGBoost）。

Result: 集成方法（Extra Trees、随机森林和XGBoost）以及多层感知器在F1分数、G-mean和AUC等指标上优于简单的分类器（决策树和K近邻）。

Conclusion: 强调了严格的特征选择和超参数调整在构建稳健的感官产品评价预测系统中的重要性。

Abstract: This study explores the application of supervised machine learning algorithms
to predict coffee ratings based on a combination of influential textual and
numerical attributes extracted from user reviews. Through careful data
preprocessing including text cleaning, feature extraction using TF-IDF, and
selection with SelectKBest, the study identifies key factors contributing to
coffee quality assessments. Six models (Decision Tree, KNearest Neighbors,
Multi-layer Perceptron, Random Forest, Extra Trees, and XGBoost) were trained
and evaluated using optimized hyperparameters. Model performance was assessed
primarily using F1-score, Gmean, and AUC metrics. Results demonstrate that
ensemble methods (Extra Trees, Random Forest, and XGBoost), as well as
Multi-layer Perceptron, consistently outperform simpler classifiers (Decision
Trees and K-Nearest Neighbors) in terms of evaluation metrics such as F1
scores, G-mean and AUC. The findings highlight the essence of rigorous feature
selection and hyperparameter tuning in building robust predictive systems for
sensory product evaluation, offering a data driven approach to complement
traditional coffee cupping by expertise of trained professionals.

</details>


### [165] [NurseSchedRL: Attention-Guided Reinforcement Learning for Nurse-Patient Assignment](https://arxiv.org/abs/2509.18125)
*Harsha Koduri*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的护士-病人分配框架NurseSchedRL，以优化医疗资源分配。


<details>
  <summary>Details</summary>
Motivation: 医疗系统面临越来越大的压力，需要在考虑技能异质性、患者敏锐度、员工疲劳和护理连续性的同时，有效地分配有限的护理资源。传统的优化和启发式调度方法难以捕捉这些动态的、多约束的环境。

Method: NurseSchedRL集成了结构化状态编码、约束动作掩码和基于注意力的技能、疲劳和地理环境表示。NurseSchedRL使用近端策略优化（PPO）与可行性掩码，以确保分配尊重现实世界的约束，同时动态适应患者的到来和不同的护士可用性。

Result: 在具有真实护士和患者数据的模拟中，与基线启发式和无约束的RL方法相比，NurseSchedRL实现了更高的调度效率，更好地将技能与患者需求对齐，并减少了疲劳。

Conclusion: 这些结果突出了强化学习在复杂、高风险的医疗 workforce 管理中对决策支持的潜力。

Abstract: Healthcare systems face increasing pressure to allocate limited nursing
resources efficiently while accounting for skill heterogeneity, patient acuity,
staff fatigue, and continuity of care. Traditional optimization and heuristic
scheduling methods struggle to capture these dynamic, multi-constraint
environments. I propose NurseSchedRL, a reinforcement learning framework for
nurse-patient assignment that integrates structured state encoding, constrained
action masking, and attention-based representations of skills, fatigue, and
geographical context. NurseSchedRL uses Proximal Policy Optimization (PPO) with
feasibility masks to ensure assignments respect real-world constraints, while
dynamically adapting to patient arrivals and varying nurse availability. In
simulation with realistic nurse and patient data, NurseSchedRL achieves
improved scheduling efficiency, better alignment of skills to patient needs,
and reduced fatigue compared to baseline heuristic and unconstrained RL
approaches. These results highlight the potential of reinforcement learning for
decision support in complex, high-stakes healthcare workforce management.

</details>


### [166] [Anomaly Detection in Electric Vehicle Charging Stations Using Federated Learning](https://arxiv.org/abs/2509.18126)
*Bishal K C,Amr Hilal,Pawan Thapa*

Main category: cs.LG

TL;DR: 联邦学习 (FL) 是一种去中心化的训练框架，通过将原始数据保留在本地来保护隐私，使其成为物联网 (IoT) 的理想选择。本文针对电动汽车充电站 (EVCS) 的网络威胁，提出了一种基于 FL 的入侵检测系统 (IDS)。


<details>
  <summary>Details</summary>
Motivation: 集中式入侵检测系统 (IDS) 会引发隐私问题，而当前的基于 FL 的 IDS 评估忽略了系统异构和非独立同分布 (Non-IID) 数据等实际挑战。

Method: 使用 FedAvg 和 FedAvgM 两种优化方法，评估了联邦学习在系统和数据异构情况下对电动汽车充电站异常检测的性能。

Result: 在独立同分布 (IID) 设置下，FedAvg 的性能优于使用相同神经网络的集中式模型。但是，在非独立同分布 (Non-IID) 数据和系统异构情况下，性能会下降。FedAvgM 在异构设置中始终优于 FedAvg，表现出更好的收敛性和更高的异常检测准确性。

Conclusion: 研究结果表明，FL 可以处理基于物联网的 EVCS 中的异构性，而不会显着降低性能，FedAvgM 是一种有前途的解决方案，可用于实现稳健、保护隐私的 EVCS 安全。

Abstract: Federated Learning (FL) is a decentralized training framework widely used in
IoT ecosystems that preserves privacy by keeping raw data local, making it
ideal for IoT-enabled cyber-physical systems with sensing and communication
like Smart Grids (SGs), Connected and Automated Vehicles (CAV), and Electric
Vehicle Charging Stations (EVCS). With the rapid expansion of electric vehicle
infrastructure, securing these IoT-based charging stations against cyber
threats has become critical. Centralized Intrusion Detection Systems (IDS)
raise privacy concerns due to sensitive network and user data, making FL a
promising alternative. However, current FL-based IDS evaluations overlook
practical challenges such as system heterogeneity and non-IID data. To address
these challenges, we conducted experiments to evaluate the performance of
federated learning for anomaly detection in EV charging stations under system
and data heterogeneity. We used FedAvg and FedAvgM, widely studied optimization
approaches, to analyze their effectiveness in anomaly detection. Under IID
settings, FedAvg achieves superior performance to centralized models using the
same neural network. However, performance degrades with non-IID data and system
heterogeneity. FedAvgM consistently outperforms FedAvg in heterogeneous
settings, showing better convergence and higher anomaly detection accuracy. Our
results demonstrate that FL can handle heterogeneity in IoT-based EVCS without
significant performance loss, with FedAvgM as a promising solution for robust,
privacy-preserving EVCS security.

</details>


### [167] [Safe-SAIL: Towards a Fine-grained Safety Landscape of Large Language Models via Sparse Autoencoder Interpretation Framework](https://arxiv.org/abs/2509.18127)
*Jiaqi Weng,Han Zheng,Hanyu Zhang,Qinqin He,Jialing Tao,Hui Xue,Zhixuan Chu,Xiting Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为Safe-SAIL的框架，用于解释LLM中SAE的特征，以促进对安全领域中机制的理解。


<details>
  <summary>Details</summary>
Motivation: 现有的安全研究主要集中在评估LLM输出或特定安全任务，但无法解决更广泛、未定义的风险。之前的SAE应用没有用细粒度的安全相关概念来解释特征，因此不足以解决安全关键行为。

Method: 该方法系统地识别具有最佳概念特定可解释性的SAE，解释安全相关神经元，并引入有效的策略来扩大解释过程。

Result: 发布了一个综合工具包，包括SAE检查点和人类可读的神经元解释，支持对安全风险的实证分析，以促进LLM安全研究。

Conclusion: 为了进行严格的安全分析，我们必须提取一组丰富多样的安全相关特征，以有效捕获这些高风险行为。

Abstract: Increasing deployment of large language models (LLMs) in real-world
applications raises significant safety concerns. Most existing safety research
focuses on evaluating LLM outputs or specific safety tasks, limiting their
ability to ad- dress broader, undefined risks. Sparse Autoencoders (SAEs)
facilitate interpretability research to clarify model behavior by explaining
single-meaning atomic features decomposed from entangled signals. jHowever,
prior applications on SAEs do not interpret features with fine-grained
safety-related con- cepts, thus inadequately addressing safety-critical
behaviors, such as generating toxic responses and violating safety regu-
lations. For rigorous safety analysis, we must extract a rich and diverse set
of safety-relevant features that effectively capture these high-risk behaviors,
yet face two challenges: identifying SAEs with the greatest potential for
generating safety concept-specific neurons, and the prohibitively high cost of
detailed feature explanation. In this paper, we pro- pose Safe-SAIL, a
framework for interpreting SAE features within LLMs to advance mechanistic
understanding in safety domains. Our approach systematically identifies SAE
with best concept-specific interpretability, explains safety-related neurons,
and introduces efficient strategies to scale up the in- terpretation process.
We will release a comprehensive toolkit including SAE checkpoints and
human-readable neuron ex- planations, which supports empirical analysis of
safety risks to promote research on LLM safety.

</details>


### [168] [Accounting for Uncertainty in Machine Learning Surrogates: A Gauss-Hermite Quadrature Approach to Reliability Analysis](https://arxiv.org/abs/2509.18128)
*Amirreza Tootchi,Xiaoping Du*

Main category: cs.LG

TL;DR: 提出了一种使用高斯-埃尔米特求积法来分离不确定性的方法，以提高可靠性分析的准确性。


<details>
  <summary>Details</summary>
Motivation: 机器学习代理越来越多地用于替代基于物理的可靠性分析中的计算模型，但它们的使用会引入模型近似误差的认知不确定性，并与模型输入中的偶然不确定性相结合，从而可能损害可靠性预测的准确性。

Method: 该方法使用一阶和二阶可靠性方法评估偶然不确定性下的条件失效概率，然后将这些概率与认知不确定性的实现相结合。

Result: 三个例子表明，与忽略模型不确定性的传统方法相比，该方法在保持计算效率的同时，产生了更可信的预测。

Conclusion: 该研究提出了一种高斯-埃尔米特求积法，用于解耦嵌套的不确定性，从而实现更准确的可靠性分析。

Abstract: Machine learning surrogates are increasingly employed to replace expensive
computational models for physics-based reliability analysis. However, their use
introduces epistemic uncertainty from model approximation errors, which couples
with aleatory uncertainty in model inputs, potentially compromising the
accuracy of reliability predictions. This study proposes a Gauss-Hermite
quadrature approach to decouple these nested uncertainties and enable more
accurate reliability analysis. The method evaluates conditional failure
probabilities under aleatory uncertainty using First and Second Order
Reliability Methods and then integrates these probabilities across realizations
of epistemic uncertainty. Three examples demonstrate that the proposed approach
maintains computational efficiency while yielding more trustworthy predictions
than traditional methods that ignore model uncertainty.

</details>


### [169] [Research on Metro Transportation Flow Prediction Based on the STL-GRU Combined Model](https://arxiv.org/abs/2509.18130)
*Zijie Zhou,Huichen Ma*

Main category: cs.LG

TL;DR: 提出了一种结合STL分解和GRU的地铁换乘客流预测模型，提高了预测精度。


<details>
  <summary>Details</summary>
Motivation: 为了优化运营计划和提高运输效率，改进地铁内部换乘客流预测理论，为智能运营决策提供更可靠的支持。

Method: 首先使用STL分解将换乘客流时间序列分解为趋势、周期和残余成分，然后使用GRU模型进行预测。

Result: 该模型在工作日（不包括星期五）、星期五和休息日的预测精度均优于LSTM、GRU和STL-LSTM模型，MAPE至少分别降低了2.3、1.36和6.42个百分点。

Conclusion: 验证了该模型的有效性，表明其能显著提高换乘客流的预测精度。

Abstract: In the metro intelligent transportation system, accurate transfer passenger
flow prediction is a key link in optimizing operation plans and improving
transportation efficiency. To further improve the theory of metro internal
transfer passenger flow prediction and provide more reliable support for
intelligent operation decisions, this paper innovatively proposes a metro
transfer passenger flow prediction model that integrates the Seasonal and Trend
decomposition using Loess (STL) method and Gated Recurrent Unit (GRU).In
practical application, the model first relies on the deep learning library
Keras to complete the construction and training of the GRU model, laying the
foundation for subsequent prediction; then preprocesses the original metro card
swiping data, uses the graph-based depth-first search algorithm to identify
passengers' travel paths, and further constructs the transfer passenger flow
time series; subsequently adopts the STL time series decomposition algorithm to
decompose the constructed transfer passenger flow time series into trend
component, periodic component and residual component, and uses the 3{\sigma}
principle to eliminate and fill the outliers in the residual component, and
finally completes the transfer passenger flow prediction.Taking the transfer
passenger flow data of a certain metro station as the research sample, the
validity of the model is verified. The results show that compared with Long
Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and the combined model of
STL time series decomposition method and Long Short-Term Memory (STL-LSTM), the
STL-GRU combined prediction model significantly improves the prediction
accuracy of transfer passenger flow on weekdays (excluding Fridays), Fridays
and rest days, with the mean absolute percentage error (MAPE) of the prediction
results reduced by at least 2.3, 1.36 and 6.42 percentage points respectively.

</details>


### [170] [Two ways to knowledge?](https://arxiv.org/abs/2509.18131)
*Jean-Michel Tucny,Abhisek Ganguly,Santosh Ansumali,Sauro Succi*

Main category: cs.LG

TL;DR: Transformer模型的权重矩阵表现出随机性，与物理问题的结构没有直接联系。


<details>
  <summary>Details</summary>
Motivation: 研究基于Transformer的机器学习应用在解决物理问题时，其权重矩阵的特性。

Method: 分析Transformer模型在解决两个代表性物理应用时的权重矩阵。

Result: 权重矩阵表现出随机性，与物理问题的物理和数学结构没有直接联系。

Conclusion: 机器学习和科学方法可能是获得知识的两种不同且互补的途径，但网络参数与物理结构之间的直接对应关系可能难以实现。

Abstract: It is shown that the weight matrices of transformer-based machine learning
applications to the solution of two representative physical applications show a
random-like character which bears no directly recognizable link to the physical
and mathematical structure of the physical problem under study. This suggests
that machine learning and the scientific method may represent two distinct and
potentially complementary paths to knowledge, even though a strict notion of
explainability in terms of direct correspondence between network parameters and
physical structures may remain out of reach. It is also observed that drawing a
parallel between transformer operation and (generalized) path-integration
techniques may account for the random-like nature of the weights, but still
does not resolve the tension with explainability. We conclude with some general
comments on the hazards of gleaning knowledge without the benefit of Insight.

</details>


### [171] [Self-Evolving LLMs via Continual Instruction Tuning](https://arxiv.org/abs/2509.18133)
*Le Huang,Jiazheng Kang,Cheng Hou,Zhe Zhao,Zhenxiang Yan,Chuan Shi,Ting Bai*

Main category: cs.LG

TL;DR: 提出了一种名为 MoE-CL 的参数高效对抗混合专家框架，用于 LLM 的工业级、自演进持续指令调整。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 必须不断学习以跟上各种不断发展的任务，从而需要自我进化以在动态数据分布下改进知识。然而，现有的持续学习 (CL) 方法通常会遭受灾难性遗忘。

Method: MoE-CL 使用双重专家设计：(1) 每个任务一个专用的 LoRA 专家，以通过参数独立性来保留特定于任务的知识，从而减轻遗忘；(2) 一个共享的 LoRA 专家，以实现跨任务转移。为了防止通过共享路径传递与任务无关的噪声，我们在 GAN 中集成了一个任务感知鉴别器。鉴别器鼓励共享专家在顺序训练期间仅传递与任务对齐的信息。

Result: 在公共 MTL5 基准和工业 Tencent3 基准上进行的大量实验验证了 MoE-CL 对持续指令调整的有效性。在腾讯视频平台上进行内容合规性审查的真实世界 A/B 测试中，MoE-CL 降低了 15.3% 的人工审查成本。

Conclusion: MoE-CL 适用于持续适应和稳定转移至关重要的大规模工业部署。

Abstract: In real-world industrial settings, large language models (LLMs) must learn
continually to keep pace with diverse and evolving tasks, requiring
self-evolution to refine knowledge under dynamic data distributions. However,
existing continual learning (CL) approaches, such as replay and parameter
isolation, often suffer from catastrophic forgetting: training on new tasks
degrades performance on earlier ones by overfitting to the new distribution and
weakening generalization.We propose MoE-CL, a parameter-efficient adversarial
mixture-of-experts framework for industrial-scale, self-evolving continual
instruction tuning of LLMs. MoE-CL uses a dual-expert design: (1) a dedicated
LoRA expert per task to preserve task-specific knowledge via parameter
independence, mitigating forgetting; and (2) a shared LoRA expert to enable
cross-task transfer. To prevent transferring task-irrelevant noise through the
shared pathway, we integrate a task-aware discriminator within a GAN. The
discriminator encourages the shared expert to pass only task-aligned
information during sequential training. Through adversarial learning, the
shared expert acquires generalized representations that mimic the
discriminator, while dedicated experts retain task-specific details, balancing
knowledge retention and cross-task generalization and thereby supporting
self-evolution.Extensive experiments on the public MTL5 benchmark and an
industrial Tencent3 benchmark validate the effectiveness of MoE-CL for
continual instruction tuning. In real-world A/B testing for content compliance
review on the Tencent Video platform, MoE-CL reduced manual review costs by
15.3%. These results demonstrate that MoE-CL is practical for large-scale
industrial deployment where continual adaptation and stable transfer are
critical.

</details>


### [172] [A Weighted Gradient Tracking Privacy-Preserving Method for Distributed Optimization](https://arxiv.org/abs/2509.18134)
*Furan Xie,Bing Liu,Li Chai*

Main category: cs.LG

TL;DR: 本文研究了保护隐私的分布式优化问题，旨在保护智能体的隐私信息免受潜在攻击者的攻击。


<details>
  <summary>Details</summary>
Motivation: 揭示了梯度跟踪中固有的隐私泄露风险。

Method: 提出了一种加权梯度跟踪分布式隐私保护算法，利用衰减权重因子消除了梯度跟踪中的隐私泄露风险。

Result: 在时变异构步长下，表征了所提算法的收敛性。证明了在温和假设下，该算法能够精确收敛到最优解。

Conclusion: 通过一个经典的分布式估计问题和一个卷积神经网络的分布式训练，数值仿真验证了该算法的有效性。

Abstract: This paper investigates the privacy-preserving distributed optimization
problem, aiming to protect agents' private information from potential attackers
during the optimization process. Gradient tracking, an advanced technique for
improving the convergence rate in distributed optimization, has been applied to
most first-order algorithms in recent years. We first reveal the inherent
privacy leakage risk associated with gradient tracking. Building upon this
insight, we propose a weighted gradient tracking distributed privacy-preserving
algorithm, eliminating the privacy leakage risk in gradient tracking using
decaying weight factors. Then, we characterize the convergence of the proposed
algorithm under time-varying heterogeneous step sizes. We prove the proposed
algorithm converges precisely to the optimal solution under mild assumptions.
Finally, numerical simulations validate the algorithm's effectiveness through a
classical distributed estimation problem and the distributed training of a
convolutional neural network.

</details>


### [173] [SDGF: Fusing Static and Multi-Scale Dynamic Correlations for Multivariate Time Series Forecasting](https://arxiv.org/abs/2509.18135)
*Shaoxun Wang,Xingjun Zhang,Qianyang Li,Jiawei Cao,Zhendong Tan*

Main category: cs.LG

TL;DR: 提出了一种新的静态-动态图融合网络（SDGF）用于多元时间序列预测。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在建模多尺度依赖关系方面存在局限性，难以捕捉其复杂和演变的性质。

Method: 该模型利用基于先验知识的静态图来锚定长期稳定的依赖关系，同时采用多层次小波分解提取多尺度特征，构建自适应学习的动态图来捕捉不同尺度的关联。设计了一个注意力门控模块来智能地融合这两种互补的信息来源，然后使用多核扩张卷积网络来加深对时间模式的理解。

Result: 在多个广泛使用的真实世界基准数据集上的综合实验表明了我们提出的模型的有效性。

Conclusion: 该模型在多元时间序列预测任务上表现出色。

Abstract: Inter-series correlations are crucial for accurate multivariate time series
forecasting, yet these relationships often exhibit complex dynamics across
different temporal scales. Existing methods are limited in modeling these
multi-scale dependencies and struggle to capture their intricate and evolving
nature. To address this challenge, this paper proposes a novel Static-Dynamic
Graph Fusion network (SDGF), whose core lies in capturing multi-scale
inter-series correlations through a dual-path graph structure learning
approach. Specifically, the model utilizes a static graph based on prior
knowledge to anchor long-term, stable dependencies, while concurrently
employing Multi-level Wavelet Decomposition to extract multi-scale features for
constructing an adaptively learned dynamic graph to capture associations at
different scales. We design an attention-gated module to fuse these two
complementary sources of information intelligently, and a multi-kernel dilated
convolutional network is then used to deepen the understanding of temporal
patterns. Comprehensive experiments on multiple widely used real-world
benchmark datasets demonstrate the effectiveness of our proposed model.

</details>


### [174] [From Parameters to Performance: A Data-Driven Study on LLM Structure and Development](https://arxiv.org/abs/2509.18136)
*Suqing Wang,Zuchao Li,Luohe Shi,Bo Du,Hai Zhao,Yun Li,Qianren Wang*

Main category: cs.LG

TL;DR: 本文研究了大型语言模型（LLM）的结构配置如何影响其性能，旨在为未来模型的开发提供指导。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在模型规模和能力上迅速增长，但关于结构配置如何影响性能的系统性、数据驱动的研究仍然稀缺。

Method: 本文构建了一个包含各种开源LLM结构及其在多个基准测试中性能的大规模数据集，并进行系统的数据挖掘驱动分析。

Result: 本文验证并量化了结构配置与性能之间的关系，并使用机械可解释性技术进一步证实了研究结果。

Conclusion: 本文通过提供数据驱动的LLM优化见解，旨在指导未来模型的有针对性开发和应用。

Abstract: Large language models (LLMs) have achieved remarkable success across various
domains, driving significant technological advancements and innovations.
Despite the rapid growth in model scale and capability, systematic, data-driven
research on how structural configurations affect performance remains scarce. To
address this gap, we present a large-scale dataset encompassing diverse
open-source LLM structures and their performance across multiple benchmarks.
Leveraging this dataset, we conduct a systematic, data mining-driven analysis
to validate and quantify the relationship between structural configurations and
performance. Our study begins with a review of the historical development of
LLMs and an exploration of potential future trends. We then analyze how various
structural choices impact performance across benchmarks and further corroborate
our findings using mechanistic interpretability techniques. By providing
data-driven insights into LLM optimization, our work aims to guide the targeted
development and application of future models. We will release our dataset at
https://huggingface.co/datasets/DX0369/LLM-Structure-Performance-Dataset

</details>


### [175] [LoRALib: A Standardized Benchmark for Evaluating LoRA-MoE Methods](https://arxiv.org/abs/2509.18137)
*Shaoheng Wang,Yao Lu,Yuqi Li,Yaxin Gao,Jiaqi Nie,Shanqing Yu,Yingli Tian,Qi Xuan*

Main category: cs.LG

TL;DR: 提出了一个统一的LoRA-MoE基准测试框架LoRALib，用于评估不同LoRA-MoE方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的LoRA-MoE方法缺乏统一的标准，难以进行公平比较。

Method: 构建了一个包含40个下游任务和17个模型架构的LoRA库，并使用OpenCompass进行大规模实验。

Result: LoRAMoE表现最佳，且优先选择与目标任务相关的LoRA可以进一步提高MoE的性能。

Conclusion: 该研究提供了一个统一的评估框架，并通过实验揭示了LoRA-MoE方法的有效性以及LoRA选择的重要性。

Abstract: As a parameter efficient fine-tuning (PEFT) method, low-rank adaptation
(LoRA) can save significant costs in storage and computing, but its strong
adaptability to a single task is often accompanied by insufficient cross-task
generalization capabilities. To improve this, existing work combines LoRA with
mixture-of-experts (MoE) to enhance the model's adaptability through expert
modules and routing mechanisms. However, existing LoRA-MoE methods lack unified
standards in models, datasets, hyperparameters, and evaluation methods, making
it difficult to conduct fair comparisons between different methods. To this
end, we proposed a unified benchmark named LoRALib. Specifically, we
standardized datasets from $40$ downstream tasks into a unified format,
fine-tuned them using the same hyperparameters and obtained $680$ LoRA modules
across $17$ model architectures. Based on this LoRA library, we conduct
large-scale experiments on $3$ representative LoRA-MoE methods and different
LoRA selection mechanisms using the open-sourced testing tool OpenCompass.
Extensive experiments show that LoRAMoE performs best, and that prioritizing
LoRAs relevant to the target task can further improve the performance of MoE.
We hope these findings will inspire future work. Our datasets and LoRA library
are available at https://huggingface.co/datasets/YaoLuzjut/LoRAOcean_dataset
and https://huggingface.co/YaoLuzjut/models.

</details>


### [176] [Rank-Induced PL Mirror Descent: A Rank-Faithful Second-Order Algorithm for Sleeping Experts](https://arxiv.org/abs/2509.18138)
*Tiantian Zhang*

Main category: cs.LG

TL;DR: 提出了一种新的算法RIPLM，它直接在rank-induced PL参数化中更新，保持与rank benchmark的等价性。


<details>
  <summary>Details</summary>
Motivation: 利用了rank benchmark和distributional benchmark之间的结构等价性。

Method: RIPLM直接在rank-induced Plackett--Luce (PL)参数化中更新。

Result: RIPLM是第一个在sleeping experts setting中既是rank-faithful又是variance-adaptive的算法。

Conclusion: RIPLM算法保持了与rank benchmark的等价性，并且在sleeping experts setting中表现出色。

Abstract: We introduce a new algorithm, \emph{Rank-Induced Plackett--Luce Mirror
Descent (RIPLM)}, which leverages the structural equivalence between the
\emph{rank benchmark} and the \emph{distributional benchmark} established in
\citet{BergamOzcanHsu2022}. Unlike prior approaches that operate on expert
identities, RIPLM updates directly in the \emph{rank-induced Plackett--Luce
(PL)} parameterization. This ensures that the algorithm's played distributions
remain within the class of rank-induced distributions at every round,
preserving the equivalence with the rank benchmark. To our knowledge, RIPLM is
the first algorithm that is both (i) \emph{rank-faithful} and (ii)
\emph{variance-adaptive} in the sleeping experts setting.

</details>


### [177] [Comparative Analysis of FOLD-SE vs. FOLD-R++ in Binary Classification and XGBoost in Multi-Category Classification](https://arxiv.org/abs/2509.18139)
*Akshay Murthy,Shawn Sebastian,Manil Shangle,Huaduo Wang,Sopam Dasgupta,Gopal Gupta*

Main category: cs.LG

TL;DR: This study compares the performance of FOLD-SE, a rule-based classifier, against FOLD-R++ and XGBoost in binary and multi-category classification tasks.


<details>
  <summary>Details</summary>
Motivation: The need for ML models that balance accuracy, efficiency, and interpretability, addressing the trade-off between accuracy and explainability in traditional models.

Method: Comparing FOLD-SE and FOLD-R++ in binary classification and evaluating FOLD-SE against XGBoost in multi-category classification, using accuracy, F1 scores, and processing time as performance measures.

Result: FOLD-SE outperforms FOLD-R++ in binary classification with fewer rules, and is more precise and efficient than XGBoost in multi-category classification.

Conclusion: FOLD-SE bridges the gap between explainability and performance, serving as a viable alternative to black-box models in diverse classification tasks.

Abstract: Recently, the demand for Machine Learning (ML) models that can balance
accuracy, efficiency, and interpreability has grown significantly.
Traditionally, there has been a tradeoff between accuracy and explainability in
predictive models, with models such as Neural Networks achieving high accuracy
on complex datasets while sacrificing internal transparency. As such, new
rule-based algorithms such as FOLD-SE have been developed that provide tangible
justification for predictions in the form of interpretable rule sets. The
primary objective of this study was to compare FOLD-SE and FOLD-R++, both
rule-based classifiers, in binary classification and evaluate how FOLD-SE
performs against XGBoost, a widely used ensemble classifier, when applied to
multi-category classification. We hypothesized that because FOLD-SE can
generate a condensed rule set in a more explainable manner, it would lose
upwards of an average of 3 percent in accuracy and F1 score when compared with
XGBoost and FOLD-R++ in multiclass and binary classification, respectively. The
research used data collections for classification, with accuracy, F1 scores,
and processing time as the primary performance measures. Outcomes show that
FOLD-SE is superior to FOLD-R++ in terms of binary classification by offering
fewer rules but losing a minor percentage of accuracy and efficiency in
processing time; in tasks that involve multi-category classifications, FOLD-SE
is more precise and far more efficient compared to XGBoost, in addition to
generating a comprehensible rule set. The results point out that FOLD-SE is a
better choice for both binary tasks and classifications with multiple
categories. Therefore, these results demonstrate that rule-based approaches
like FOLD-SE can bridge the gap between explainability and performance,
highlighting their potential as viable alternatives to black-box models in
diverse classification tasks.

</details>


### [178] [A Machine Learning Framework for Pathway-Driven Therapeutic Target Discovery in Metabolic Disorders](https://arxiv.org/abs/2509.18140)
*Iram Wajahat,Amritpal Singh,Fazel Keshtkar,Syed Ahmad Chan Bukhari*

Main category: cs.LG

TL;DR: 本研究提出了一种新的机器学习框架，该框架集成了预测建模和基因无关的通路映射，以识别高危人群并发现潜在的治疗靶点。


<details>
  <summary>Details</summary>
Motivation: 代谢紊乱，特别是2型糖尿病（T2DM），代表着一项重大的全球健康负担，对诸如皮马印第安人（亚利桑那州中南部的一个美洲原住民部落）等具有遗传易感性的人群的影响尤为严重。

Method: 使用Pima Indian数据集，应用Logistic回归和t检验来识别T2DM的关键预测因子，总体模型准确率为78.43%。开发了一种通路映射策略，该策略将已识别的预测因子与关键信号网络（包括胰岛素信号传导，AMPK和PPAR通路）相关联。

Result: Logistic回归和t检验确定了T2DM的关键预测因子，总体模型准确率为78.43%。通路映射将预测因子与胰岛素信号传导，AMPK和PPAR通路等关键信号网络相关联。提出了诸如双重GLP-1 / GIP受体激动剂，AMPK激活剂，SIRT1调节剂和植物化学物质等治疗策略，并通过通路富集分析进一步验证。

Conclusion: 该框架通过为代谢紊乱的早期检测和靶向干预提供可解释且可扩展的解决方案，从而推进了精准医学。

Abstract: Metabolic disorders, particularly type 2 diabetes mellitus (T2DM), represent
a significant global health burden, disproportionately impacting genetically
predisposed populations such as the Pima Indians (a Native American tribe from
south central Arizona). This study introduces a novel machine learning (ML)
framework that integrates predictive modeling with gene-agnostic pathway
mapping to identify high-risk individuals and uncover potential therapeutic
targets. Using the Pima Indian dataset, logistic regression and t-tests were
applied to identify key predictors of T2DM, yielding an overall model accuracy
of 78.43%. To bridge predictive analytics with biological relevance, we
developed a pathway mapping strategy that links identified predictors to
critical signaling networks, including insulin signaling, AMPK, and PPAR
pathways. This approach provides mechanistic insights without requiring direct
molecular data. Building upon these connections, we propose therapeutic
strategies such as dual GLP-1/GIP receptor agonists, AMPK activators, SIRT1
modulators, and phytochemical, further validated through pathway enrichment
analyses. Overall, this framework advances precision medicine by offering
interpretable and scalable solutions for early detection and targeted
intervention in metabolic disorders. The key contributions of this work are:
(1) development of an ML framework combining logistic regression and principal
component analysis (PCA) for T2DM risk prediction; (2) introduction of a
gene-agnostic pathway mapping approach to generate mechanistic insights; and
(3) identification of novel therapeutic strategies tailored for high-risk
populations.

</details>


### [179] [KM-GPT: An Automated Pipeline for Reconstructing Individual Patient Data from Kaplan-Meier Plots](https://arxiv.org/abs/2509.18141)
*Yao Zhao,Haoyue Sun,Yantian Ding,Yanxun Xu*

Main category: cs.LG

TL;DR: KM-GPT: 自动化地从 Kaplan-Meier 图中重建个体患者数据 (IPD)，无需手动干预，提高了准确性和可重复性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于手动数字化 KM 图，容易出错且缺乏可扩展性。

Method: 开发了 KM-GPT，一个全自动、AI 驱动的流程，集成了图像预处理、GPT-5 驱动的多模态推理和迭代重建算法。

Result: KM-GPT 在合成和真实世界数据集上表现出卓越的准确性。应用于胃癌免疫治疗试验的 Meta 分析，重建 IPD 以促进证据合成和基于生物标志物的亚组分析。

Conclusion: KM-GPT 通过自动化手动过程并提供可扩展的 Web 解决方案，转变了临床研究，利用重建的 IPD 来实现更明智的下游分析，支持循证决策。

Abstract: Reconstructing individual patient data (IPD) from Kaplan-Meier (KM) plots
provides valuable insights for evidence synthesis in clinical research.
However, existing approaches often rely on manual digitization, which is
error-prone and lacks scalability. To address these limitations, we develop
KM-GPT, the first fully automated, AI-powered pipeline for reconstructing IPD
directly from KM plots with high accuracy, robustness, and reproducibility.
KM-GPT integrates advanced image preprocessing, multi-modal reasoning powered
by GPT-5, and iterative reconstruction algorithms to generate high-quality IPD
without manual input or intervention. Its hybrid reasoning architecture
automates the conversion of unstructured information into structured data flows
and validates data extraction from complex KM plots. To improve accessibility,
KM-GPT is equipped with a user-friendly web interface and an integrated AI
assistant, enabling researchers to reconstruct IPD without requiring
programming expertise. KM-GPT was rigorously evaluated on synthetic and
real-world datasets, consistently demonstrating superior accuracy. To
illustrate its utility, we applied KM-GPT to a meta-analysis of gastric cancer
immunotherapy trials, reconstructing IPD to facilitate evidence synthesis and
biomarker-based subgroup analyses. By automating traditionally manual processes
and providing a scalable, web-based solution, KM-GPT transforms clinical
research by leveraging reconstructed IPD to enable more informed downstream
analyses, supporting evidence-based decision-making.

</details>


### [180] [AdaSTI: Conditional Diffusion Models with Adaptive Dependency Modeling for Spatio-Temporal Imputation](https://arxiv.org/abs/2509.18144)
*Yubo Yang,Yichen Zhu,Bo Jiang*

Main category: cs.LG

TL;DR: 本文提出了一种基于条件扩散模型的时空数据插补方法 AdaSTI，该方法在三个真实世界的数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 时空数据中普遍存在缺失值，现有的基于扩散模型的方法在提取和利用时空依赖关系作为条件信息时存在误差累积，忽略了不同扩散步骤中噪声数据依赖关系的可变性。

Method: 提出了 AdaSTI，其中包含用于预插补的 BiS4PI 网络（基于双向 S4 模型）以及用于提取条件信息的时空条件器 (STC) 网络，并提出了具有门控注意力机制的噪声感知时空 (NAST) 网络，以捕获跨扩散步骤的不同依赖关系。

Result: 在三个真实世界的数据集上的大量实验表明，AdaSTI 在所有设置中均优于现有方法，插补误差最多可减少 46.4%。

Conclusion: AdaSTI 是一种有效的时空数据插补方法，它通过自适应地建模时空依赖关系，并在扩散过程中考虑噪声的影响，显著提高了插补精度。

Abstract: Spatio-temporal data abounds in domain like traffic and environmental
monitoring. However, it often suffers from missing values due to sensor
malfunctions, transmission failures, etc. Recent years have seen continued
efforts to improve spatio-temporal data imputation performance. Recently
diffusion models have outperformed other approaches in various tasks, including
spatio-temporal imputation, showing competitive performance. Extracting and
utilizing spatio-temporal dependencies as conditional information is vital in
diffusion-based methods. However, previous methods introduce error accumulation
in this process and ignore the variability of the dependencies in the noisy
data at different diffusion steps. In this paper, we propose AdaSTI (Adaptive
Dependency Model in Diffusion-based Spatio-Temporal Imputation), a novel
spatio-temporal imputation approach based on conditional diffusion model.
Inside AdaSTI, we propose a BiS4PI network based on a bi-directional S4 model
for pre-imputation with the imputed result used to extract conditional
information by our designed Spatio-Temporal Conditionalizer (STC)network. We
also propose a Noise-Aware Spatio-Temporal (NAST) network with a gated
attention mechanism to capture the variant dependencies across diffusion steps.
Extensive experiments on three real-world datasets show that AdaSTI outperforms
existing methods in all the settings, with up to 46.4% reduction in imputation
error.

</details>


### [181] [Early Prediction of Multi-Label Care Escalation Triggers in the Intensive Care Unit Using Electronic Health Records](https://arxiv.org/abs/2509.18145)
*Syed Ahmad Chan Bukhari,Amritpal Singh,Shifath Hossain,Iram Wajahat*

Main category: cs.LG

TL;DR: 该研究提出了一种多标签分类框架，使用 ICU 前 24 小时的数据来预测护理升级触发因素 (CET)，包括呼吸衰竭、血流动力学不稳定、肾脏受损和神经功能恶化。


<details>
  <summary>Details</summary>
Motivation: 传统的早期预警系统 (如 SOFA 或 MEWS) 仅关注单一结果，无法捕捉临床衰退的多维性质。

Method: 使用 MIMIC-IV 数据库，通过基于规则的标准将 CET 定义应用于 24 至 72 小时的数据。从前 24 小时提取特征，包括生命体征聚集、实验室值和静态人口统计数据。在 85,242 个 ICU 住院队列上训练和评估多个分类模型 (80% 训练：68,193；20% 测试：17,049)。

Result: XGBoost 是性能最佳的模型，在呼吸、血流动力学、肾脏和神经功能恶化方面的 F1 分数分别为 0.66、0.72、0.76 和 0.62，优于基线模型。

Conclusion: 该框架展示了早期、可解释的临床警报的实际潜力，无需复杂的时间序列建模或自然语言处理。

Abstract: Intensive Care Unit (ICU) patients often present with complex, overlapping
signs of physiological deterioration that require timely escalation of care.
Traditional early warning systems, such as SOFA or MEWS, are limited by their
focus on single outcomes and fail to capture the multi-dimensional nature of
clinical decline. This study proposes a multi-label classification framework to
predict Care Escalation Triggers (CETs), including respiratory failure,
hemodynamic instability, renal compromise, and neurological deterioration,
using the first 24 hours of ICU data. Using the MIMIC-IV database, CETs are
defined through rule-based criteria applied to data from hours 24 to 72 (for
example, oxygen saturation below 90, mean arterial pressure below 65 mmHg,
creatinine increase greater than 0.3 mg/dL, or a drop in Glasgow Coma Scale
score greater than 2). Features are extracted from the first 24 hours and
include vital sign aggregates, laboratory values, and static demographics. We
train and evaluate multiple classification models on a cohort of 85,242 ICU
stays (80 percent training: 68,193; 20 percent testing: 17,049). Evaluation
metrics include per-label precision, recall, F1-score, and Hamming loss.
XGBoost, the best performing model, achieves F1-scores of 0.66 for respiratory,
0.72 for hemodynamic, 0.76 for renal, and 0.62 for neurologic deterioration,
outperforming baseline models. Feature analysis shows that clinically relevant
parameters such as respiratory rate, blood pressure, and creatinine are the
most influential predictors, consistent with the clinical definitions of the
CETs. The proposed framework demonstrates practical potential for early,
interpretable clinical alerts without requiring complex time-series modeling or
natural language processing.

</details>


### [182] [ConceptFlow: Hierarchical and Fine-grained Concept-Based Explanation for Convolutional Neural Networks](https://arxiv.org/abs/2509.18147)
*Xinyu Mu,Hui Dou,Furao Shen,Jian Zhao*

Main category: cs.LG

TL;DR: ConceptFlow simulates CNN's thinking by tracing concept evolution across layers.


<details>
  <summary>Details</summary>
Motivation: Existing concept-based methods ignore semantic roles of filters and concept propagation.

Method: ConceptFlow uses concept attentions to link filters to concepts and conceptual pathways to track concept transitions.

Result: ConceptFlow provides semantically meaningful insights into model reasoning.

Conclusion: ConceptFlow offers deeper understanding of CNN logic and generates faithful explanations.

Abstract: Concept-based interpretability for Convolutional Neural Networks (CNNs) aims
to align internal model representations with high-level semantic concepts, but
existing approaches largely overlook the semantic roles of individual filters
and the dynamic propagation of concepts across layers. To address these
limitations, we propose ConceptFlow, a concept-based interpretability framework
that simulates the internal "thinking path" of a model by tracing how concepts
emerge and evolve across layers. ConceptFlow comprises two key components: (i)
concept attentions, which associate each filter with relevant high-level
concepts to enable localized semantic interpretation, and (ii) conceptual
pathways, derived from a concept transition matrix that quantifies how concepts
propagate and transform between filters. Together, these components offer a
unified and structured view of internal model reasoning. Experimental results
demonstrate that ConceptFlow yields semantically meaningful insights into model
reasoning, validating the effectiveness of concept attentions and conceptual
pathways in explaining decision behavior. By modeling hierarchical conceptual
pathways, ConceptFlow provides deeper insight into the internal logic of CNNs
and supports the generation of more faithful and human-aligned explanations.

</details>


### [183] [Sparse Training Scheme for Multimodal LLM](https://arxiv.org/abs/2509.18150)
*Kean Shi,Liang Chen,Haozhe Zhao,Baobao Chang*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的基于稀疏表示的训练高效框架（STS），以解决多模态大型语言模型（MLLM）训练效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 多模态数据引入的输入序列过长和层间计算利用率低导致 MLLM 训练效率低下。

Method: 该框架包含两个关键组件：视觉 Token 压缩器（用于压缩视觉 tokens 以减少信息负载）和层动态跳过器（用于在正向和反向传播过程中动态跳过语言模型中不必要的层，以减少计算开销）。

Result: 该方法已在多个基准测试中进行了广泛评估，证明了其有效性和效率。

Conclusion: 该方法广泛适用于不同的 MLLM 架构。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated outstanding
performance across a variety of domains. However, training MLLMs is often
inefficient due to the significantly longer input sequences introduced by
multimodal data and the low utilization of inter-layer computations. To address
this challenge, we shift the focus to the training process itself and propose a
novel training-efficient framework based on sparse representations, termed the
Sparse Training Scheme (STS). This scheme consists of two key components: the
Visual Token Compressor, which reduces the information load by compressing
visual tokens, and the Layer Dynamic Skipper, which mitigates the computational
overhead by dynamically skipping unnecessary layers in the language model
during both forward and backward passes. Our approach is broadly applicable to
diverse MLLM architectures and has been extensively evaluated on multiple
benchmarks, demonstrating its effectiveness and efficiency.

</details>


### [184] [HyperNAS: Enhancing Architecture Representation for NAS Predictor via Hypernetwork](https://arxiv.org/abs/2509.18151)
*Jindi Lv,Yuhao Zhou,Yuxin Tian,Qing Ye,Wentao Feng,Jiancheng Lv*

Main category: cs.LG

TL;DR: HyperNAS: A new neural predictor paradigm for NAS that enhances architecture representation learning.


<details>
  <summary>Details</summary>
Motivation: Time-intensive performance evaluations impede progress in NAS. Existing predictors have poor generalization due to their limited ability to capture intricate relationships among architectures.

Method: A global encoding scheme captures macro-structure information, and a shared hypernetwork enhances the investigation of inter-architecture patterns. A dynamic adaptive multi-task loss ensures training stability.

Result: Achieves state-of-the-art results on CIFAR-10 (97.60% top-1 accuracy) and ImageNet (82.4% top-1 accuracy) using at least 5.0x fewer samples.

Conclusion: HyperNAS demonstrates advantages, particularly in few-shot scenarios, across five representative search spaces, including ViTs.

Abstract: Time-intensive performance evaluations significantly impede progress in
Neural Architecture Search (NAS). To address this, neural predictors leverage
surrogate models trained on proxy datasets, allowing for direct performance
predictions for new architectures. However, these predictors often exhibit poor
generalization due to their limited ability to capture intricate relationships
among various architectures. In this paper, we propose HyperNAS, a novel neural
predictor paradigm for enhancing architecture representation learning. HyperNAS
consists of two primary components: a global encoding scheme and a shared
hypernetwork. The global encoding scheme is devised to capture the
comprehensive macro-structure information, while the shared hypernetwork serves
as an auxiliary task to enhance the investigation of inter-architecture
patterns. To ensure training stability, we further develop a dynamic adaptive
multi-task loss to facilitate personalized exploration on the Pareto front.
Extensive experiments across five representative search spaces, including ViTs,
demonstrate the advantages of HyperNAS, particularly in few-shot scenarios. For
instance, HyperNAS strikes new state-of-the-art results, with 97.60\% top-1
accuracy on CIFAR-10 and 82.4\% top-1 accuracy on ImageNet, using at least
5.0$\times$ fewer samples.

</details>


### [185] [WLFM: A Well-Logs Foundation Model for Multi-Task and Cross-Well Geological Interpretation](https://arxiv.org/abs/2509.18152)
*Zhenyu Qi,Qing Yu,Jichen Wang,Yun-Bo Zhao,Zerui Li,Wenjun Lv*

Main category: cs.LG

TL;DR: WLFM是一个在大量测井数据上预训练的基础模型，用于提高测井解释的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的测井解释方法受到工具响应差异、噪声信号和有限标签的限制。

Method: 该模型包含三个阶段：测井片段的token化、使用掩码token建模和地层对比学习的自监督预训练、以及使用少量样本微调的多任务适应。

Result: WLFM在孔隙度估计和岩性分类任务中均优于现有方法，且表现出层位感知能力，学习了可重用的地质词汇，并能以合理的保真度重建掩码曲线。

Conclusion: WLFM是一个可扩展、可解释和可转移的地质AI骨干模型，具有多模态集成潜力。

Abstract: Well-log interpretation is fundamental for subsurface characterization but
remains challenged by heterogeneous tool responses, noisy signals, and limited
labels. We propose WLFM, a foundation model pretrained on multi-curve logs from
1200 wells, comprising three stages: tokenization of log patches into
geological tokens, self-supervised pretraining with masked-token modeling and
stratigraphy-aware contrastive learning, and multi-task adaptation with
few-shot fine-tuning. WLFM consistently outperforms state-of-the-art baselines,
achieving 0.0041 MSE in porosity estimation and 74.13\% accuracy in lithology
classification, while WLFM-Finetune further improves to 0.0038 MSE and 78.10\%
accuracy. Beyond predictive accuracy, WLFM exhibits emergent layer-awareness,
learns a reusable geological vocabulary, and reconstructs masked curves with
reasonable fidelity, though systematic offsets are observed in shallow and
ultra-deep intervals. Although boundary detection is not explicitly evaluated
here, clustering analyses suggest strong potential for future extension. These
results establish WLFM as a scalable, interpretable, and transferable backbone
for geological AI, with implications for multi-modal integration of logs,
seismic, and textual data.

</details>


### [186] [A deep reinforcement learning platform for antibiotic discovery](https://arxiv.org/abs/2509.18153)
*Hanqun Cao,Marcelo D. T. Torres,Jingjie Zhang,Zijun Gao,Fang Wu,Chunbin Gu,Jure Leskovec,Yejin Choi,Cesar de la Fuente-Nunez,Guangyong Chen,Pheng-Ann Heng*

Main category: cs.LG

TL;DR: 这篇论文介绍了一种名为ApexAmphion的深度学习框架，用于de novo设计抗生素。


<details>
  <summary>Details</summary>
Motivation: 对抗生素耐药性(AMR)预计到2050年每年将导致高达1000万人死亡，因此迫切需要新的抗生素。

Method: 该模型首先在经过整理的肽数据上进行微调，以捕获抗菌序列的规律性，然后使用近端策略优化，根据学习到的最小抑菌浓度(MIC)分类器的预测结果与可区分的物理化学目标相结合的综合奖励进行优化。

Result: 对100个设计的肽的体外评估表明，所有候选肽都具有较低的MIC值(在某些情况下为纳摩尔范围)(100%命中率)。此外，100个化合物中有99个对至少两种临床相关细菌表现出广谱抗菌活性。主要的分子主要通过有效地靶向细胞质膜来杀死细菌。

Conclusion: 通过在单个管道中通过深度强化学习统一生成、评分和多目标优化，我们的方法可以快速生成各种有效的候选药物，从而为肽抗生素提供可扩展的途径，并为在数小时内迭代引导至效力和可开发性提供平台。

Abstract: Antimicrobial resistance (AMR) is projected to cause up to 10 million deaths
annually by 2050, underscoring the urgent need for new antibiotics. Here we
present ApexAmphion, a deep-learning framework for de novo design of
antibiotics that couples a 6.4-billion-parameter protein language model with
reinforcement learning. The model is first fine-tuned on curated peptide data
to capture antimicrobial sequence regularities, then optimised with proximal
policy optimization against a composite reward that combines predictions from a
learned minimum inhibitory concentration (MIC) classifier with differentiable
physicochemical objectives. In vitro evaluation of 100 designed peptides showed
low MIC values (nanomolar range in some cases) for all candidates (100% hit
rate). Moreover, 99 our of 100 compounds exhibited broad-spectrum antimicrobial
activity against at least two clinically relevant bacteria. The lead molecules
killed bacteria primarily by potently targeting the cytoplasmic membrane. By
unifying generation, scoring and multi-objective optimization with deep
reinforcement learning in a single pipeline, our approach rapidly produces
diverse, potent candidates, offering a scalable route to peptide antibiotics
and a platform for iterative steering toward potency and developability within
hours.

</details>


### [187] [MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe](https://arxiv.org/abs/2509.18154)
*Tianyu Yu,Zefan Wang,Chongyi Wang,Fuwei Huang,Wenshuo Ma,Zhihui He,Tianchi Cai,Weize Chen,Yuxiang Huang,Yuanqian Zhao,Bokai Xu,Junbo Cui,Yingjing Xu,Liqing Ruan,Luoyuan Zhang,Hanyu Liu,Jingkun Tang,Hongyuan Liu,Qining Guo,Wenhao Hu,Bingxiang He,Jie Zhou,Jie Cai,Ji Qi,Zonghao Guo,Chi Chen,Guoyang Zeng,Yuxuan Li,Ganqu Cui,Ning Ding,Xu Han,Yuan Yao,Zhiyuan Liu,Maosong Sun*

Main category: cs.LG

TL;DR: MiniCPM-V 4.5 is an 8B parameter MLLM designed for high efficiency and strong performance.


<details>
  <summary>Details</summary>
Motivation: Training and inference efficiency are core bottlenecks in making MLLMs more accessible and scalable.

Method: A unified 3D-Resampler model architecture, a unified learning paradigm, and a hybrid reinforcement learning strategy.

Result: MiniCPM-V 4.5 surpasses GPT-4o-latest and Qwen2.5-VL 72B with remarkable efficiency.

Conclusion: MiniCPM-V 4.5 achieves state-of-the-art performance with significantly reduced GPU memory cost and inference time.

Abstract: Multimodal Large Language Models (MLLMs) are undergoing rapid progress and
represent the frontier of AI development. However, their training and inference
efficiency have emerged as a core bottleneck in making MLLMs more accessible
and scalable. To address the challenges, we present MiniCPM-V 4.5, an 8B
parameter model designed for high efficiency and strong performance. We
introduce three core improvements in model architecture, data strategy and
training method: a unified 3D-Resampler model architecture for highly compact
encoding over images and videos, a unified learning paradigm for document
knowledge and text recognition without heavy data engineering, and a hybrid
reinforcement learning strategy for proficiency in both short and long
reasoning modes. Comprehensive experimental results in OpenCompass evaluation
show that MiniCPM-V 4.5 surpasses widely used proprietary models such as
GPT-4o-latest, and significantly larger open-source models such as Qwen2.5-VL
72B. Notably, the strong performance is achieved with remarkable efficiency.
For example, on the widely adopted VideoMME benchmark, MiniCPM-V 4.5 achieves
state-of-the-art performance among models under 30B size, using just 46.7\% GPU
memory cost and 8.7\% inference time of Qwen2.5-VL 7B.

</details>


### [188] [Developing Training Procedures for Piecewise-linear Spline Activation Functions in Neural Networks](https://arxiv.org/abs/2509.18161)
*William H Patty*

Main category: cs.LG

TL;DR: 该论文研究了通过优化神经网络中激活函数的形状来提高模型性能的方法。


<details>
  <summary>Details</summary>
Motivation: 传统的神经网络激活函数通常选择ReLU、tanh或sigmoid等静态函数，但这些函数可能不是最优的。

Method: 论文提出了9种训练方法，用于探索具有参数化线性B样条激活函数的神经网络中的双重优化动态。

Result: 实验结果表明，与传统的基于ReLU的模型相比，该方法在FNN中可降低高达94%的最终模型错误率，在CNN中可降低高达51%。

Conclusion: 通过优化激活函数可以提高模型性能，但会增加开发和训练的复杂性以及最终模型的延迟。

Abstract: Activation functions in neural networks are typically selected from a set of
empirically validated, commonly used static functions such as ReLU, tanh, or
sigmoid. However, by optimizing the shapes of a network's activation functions,
we can train models that are more parameter-efficient and accurate by assigning
more optimal activations to the neurons. In this paper, I present and compare 9
training methodologies to explore dual-optimization dynamics in neural networks
with parameterized linear B-spline activation functions. The experiments
realize up to 94% lower end model error rates in FNNs and 51% lower rates in
CNNs compared to traditional ReLU-based models. These gains come at the cost of
additional development and training complexity as well as end model latency.

</details>


### [189] [A Simple and Reproducible Hybrid Solver for a Truck-Drone VRP with Recharge](https://arxiv.org/abs/2509.18162)
*Meraryslan Meraliyev,Cemil Turan,Shirali Kadyrov*

Main category: cs.LG

TL;DR: 研究了在显式电池管理下，使用一辆卡车和一个无人机进行最后一英里交付的问题。无人机的飞行速度是卡车的两倍；每次飞行必须满足续航预算；每次交付后，无人机在下次发射前在卡车上充电。


<details>
  <summary>Details</summary>
Motivation: 旨在解决最后一英里交付问题，通过优化卡车和无人机的协同工作来最小化完成时间。

Method: 提出了一种混合强化学习（RL）求解器，该求解器将基于ALNS的卡车路线（具有2/3-opt和Or-opt）与一个小的指针/注意力策略相结合，该策略用于调度无人机的飞行。该策略解码发射-服务-交汇三元组，并具有用于续航和交付后充电的硬可行性掩码；快速、精确的时间线模拟器强制执行发射/回收处理，并计算掩蔽贪婪/波束解码使用的真实完工时间。

Result: 在N=50，E=0.7和R=0.1的欧几里德实例上，该方法实现了平均完工时间5.203±0.093，而ALNS为5.349±0.038，NN为5.208±0.124，即平均比ALNS好2.73％，并且在NN的0.10％之内。每个种子，RL调度程序在同一实例上从不逊于ALNS，并且在三个种子中的两个上与NN打成平手或击败NN。

Conclusion: 结果表明，所提出的强化学习调度器能够有效地平衡卡车等待和无人机飞行之间的权衡，从而最小化总完成时间。

Abstract: We study last-mile delivery with one truck and one drone under explicit
battery management: the drone flies at twice the truck speed; each sortie must
satisfy an endurance budget; after every delivery the drone recharges on the
truck before the next launch. We introduce a hybrid reinforcement learning (RL)
solver that couples an ALNS-based truck tour (with 2/3-opt and Or-opt) with a
small pointer/attention policy that schedules drone sorties. The policy decodes
launch--serve--rendezvous triplets with hard feasibility masks for endurance
and post-delivery recharge; a fast, exact timeline simulator enforces
launch/recovery handling and computes the true makespan used by masked
greedy/beam decoding. On Euclidean instances with $N{=}50$, $E{=}0.7$, and
$R{=}0.1$, the method achieves an average makespan of \textbf{5.203}$\pm$0.093,
versus \textbf{5.349}$\pm$0.038 for ALNS and \textbf{5.208}$\pm$0.124 for NN --
i.e., \textbf{2.73\%} better than ALNS on average and within \textbf{0.10\%} of
NN. Per-seed, the RL scheduler never underperforms ALNS on the same instance
and ties or beats NN on two of three seeds. A decomposition of the makespan
shows the expected truck--wait trade-off across heuristics; the learned
scheduler balances both to minimize the total completion time. We provide a
config-first implementation with plotting and significance-test utilities to
support replication.

</details>
