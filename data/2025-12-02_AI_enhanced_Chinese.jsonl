{"id": "2512.00004", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00004", "abs": "https://arxiv.org/abs/2512.00004", "authors": ["Jihang Li", "Bing Xu", "Zulong Chen", "Chuanfei Xu", "Minping Chen", "Suyu Liu", "Ying Zhou", "Zeyi Wen"], "title": "Enhancing Talent Search Ranking with Role-Aware Expert Mixtures and LLM-based Fine-Grained Job Descriptions", "comment": null, "summary": "Talent search is a cornerstone of modern recruitment systems, yet existing approaches often struggle to capture nuanced job-specific preferences, model recruiter behavior at a fine-grained level, and mitigate noise from subjective human judgments. We present a novel framework that enhances talent search effectiveness and delivers substantial business value through two key innovations: (i) leveraging LLMs to extract fine-grained recruitment signals from job descriptions and historical hiring data, and (ii) employing a role-aware multi-gate MoE network to capture behavioral differences across recruiter roles. To further reduce noise, we introduce a multi-task learning module that jointly optimizes click-through rate (CTR), conversion rate (CVR), and resume matching relevance. Experiments on real-world recruitment data and online A/B testing show relative AUC gains of 1.70% (CTR) and 5.97% (CVR), and a 17.29% lift in click-through conversion rate. These improvements reduce dependence on external sourcing channels, enabling an estimated annual cost saving of millions of CNY.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u62db\u8058\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528LLM\u63d0\u53d6\u62db\u8058\u4fe1\u53f7\u548c\u4f7f\u7528\u89d2\u8272\u611f\u77e5\u7684\u591a\u95e8MoE\u7f51\u7edc\u6765\u63d0\u9ad8\u4eba\u624d\u641c\u7d22\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u7ec6\u5fae\u7684\u7279\u5b9a\u5de5\u4f5c\u504f\u597d\uff0c\u4ee5\u7ec6\u7c92\u5ea6\u7684\u65b9\u5f0f\u5efa\u6a21\u62db\u8058\u4eba\u5458\u884c\u4e3a\uff0c\u5e76\u51cf\u8f7b\u4e3b\u89c2\u4eba\u4e3a\u5224\u65ad\u5e26\u6765\u7684\u566a\u97f3\u3002", "method": "\u8be5\u6846\u67b6\u5229\u7528LLM\u4ece\u804c\u4f4d\u63cf\u8ff0\u548c\u5386\u53f2\u62db\u8058\u6570\u636e\u4e2d\u63d0\u53d6\u7ec6\u7c92\u5ea6\u7684\u62db\u8058\u4fe1\u53f7\uff0c\u5e76\u91c7\u7528\u89d2\u8272\u611f\u77e5\u7684\u591a\u95e8MoE\u7f51\u7edc\u6765\u6355\u6349\u62db\u8058\u4eba\u5458\u89d2\u8272\u4e4b\u95f4\u7684\u884c\u4e3a\u5dee\u5f02\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u591a\u4efb\u52a1\u5b66\u4e60\u6a21\u5757\uff0c\u5171\u540c\u4f18\u5316\u70b9\u51fb\u7387\uff08CTR\uff09\u3001\u8f6c\u5316\u7387\uff08CVR\uff09\u548c\u7b80\u5386\u5339\u914d\u76f8\u5173\u6027\uff0c\u4ee5\u8fdb\u4e00\u6b65\u51cf\u5c11\u566a\u97f3\u3002", "result": "\u5728\u771f\u5b9e\u62db\u8058\u6570\u636e\u548c\u5728\u7ebfA/B\u6d4b\u8bd5\u4e2d\uff0cCTR\u76f8\u5bf9AUC\u589e\u76ca\u4e3a1.70%\uff0cCVR\u76f8\u5bf9AUC\u589e\u76ca\u4e3a5.97%\uff0c\u70b9\u51fb\u8f6c\u5316\u7387\u63d0\u9ad8\u4e8617.29%\u3002", "conclusion": "\u8fd9\u4e9b\u6539\u8fdb\u51cf\u5c11\u4e86\u5bf9\u5916\u90e8\u91c7\u8d2d\u6e20\u9053\u7684\u4f9d\u8d56\uff0c\u4f30\u8ba1\u6bcf\u5e74\u53ef\u8282\u7701\u6570\u767e\u4e07\u4eba\u6c11\u5e01\u7684\u6210\u672c\u3002"}}
{"id": "2512.00007", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00007", "abs": "https://arxiv.org/abs/2512.00007", "authors": ["Jingyi Huang", "Yuyi Yang", "Mengmeng Ji", "Charles Alba", "Sheng Zhang", "Ruopeng An"], "title": "Use of Retrieval-Augmented Large Language Model Agent for Long-Form COVID-19 Fact-Checking", "comment": null, "summary": "The COVID-19 infodemic calls for scalable fact-checking solutions that handle long-form misinformation with accuracy and reliability. This study presents SAFE (system for accurate fact extraction and evaluation), an agent system that combines large language models with retrieval-augmented generation (RAG) to improve automated fact-checking of long-form COVID-19 misinformation. SAFE includes two agents - one for claim extraction and another for claim verification using LOTR-RAG, which leverages a 130,000-document COVID-19 research corpus. An enhanced variant, SAFE (LOTR-RAG + SRAG), incorporates Self-RAG to refine retrieval via query rewriting. We evaluated both systems on 50 fake news articles (2-17 pages) containing 246 annotated claims (M = 4.922, SD = 3.186), labeled as true (14.1%), partly true (14.4%), false (27.0%), partly false (2.2%), and misleading (21.0%) by public health professionals. SAFE systems significantly outperformed baseline LLMs in all metrics (p < 0.001). For consistency (0-1 scale), SAFE (LOTR-RAG) scored 0.629, exceeding both SAFE (+SRAG) (0.577) and the baseline (0.279). In subjective evaluations (0-4 Likert scale), SAFE (LOTR-RAG) also achieved the highest average ratings in usefulness (3.640), clearness (3.800), and authenticity (3.526). Adding SRAG slightly reduced overall performance, except for a minor gain in clearness. SAFE demonstrates robust improvements in long-form COVID-19 fact-checking by addressing LLM limitations in consistency and explainability. The core LOTR-RAG design proved more effective than its SRAG-augmented variant, offering a strong foundation for scalable misinformation mitigation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86SAFE\uff0c\u4e00\u4e2a\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u4ee3\u7406\u7cfb\u7edf\uff0c\u7528\u4e8e\u6539\u8fdb\u5bf9\u957f\u7bc7COVID-19\u9519\u8bef\u4fe1\u606f\u7684\u81ea\u52a8\u4e8b\u5b9e\u6838\u67e5\u3002", "motivation": "COVID-19 \u4fe1\u606f\u75ab\u60c5\u9700\u8981\u53ef\u6269\u5c55\u7684\u4e8b\u5b9e\u6838\u67e5\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u51c6\u786e\u548c\u53ef\u9760\u5730\u5904\u7406\u957f\u7bc7\u9519\u8bef\u4fe1\u606f\u3002", "method": "SAFE\u5305\u62ec\u4e24\u4e2a\u4ee3\u7406\u2014\u2014\u4e00\u4e2a\u7528\u4e8e\u58f0\u660e\u63d0\u53d6\uff0c\u53e6\u4e00\u4e2a\u4f7f\u7528LOTR-RAG\u8fdb\u884c\u58f0\u660e\u9a8c\u8bc1\uff0cLOTR-RAG\u5229\u7528\u5305\u542b13\u4e07\u4efd\u6587\u4ef6\u7684COVID-19\u7814\u7a76\u8bed\u6599\u5e93\u3002SAFE (LOTR-RAG + SRAG)\u7684\u589e\u5f3a\u53d8\u4f53\uff0c\u7ed3\u5408\u4e86Self-RAG\uff0c\u901a\u8fc7\u67e5\u8be2\u91cd\u5199\u6765\u6539\u8fdb\u68c0\u7d22\u3002", "result": "SAFE\u7cfb\u7edf\u5728\u6240\u6709\u6307\u6807\u4e0a\u90fd\u663e\u8457\u4f18\u4e8e\u57fa\u7ebfLLM\u3002\u5728\u4e00\u81f4\u6027\u65b9\u9762\uff0cSAFE (LOTR-RAG)\u5f97\u5206\u6700\u9ad8\uff0c\u4e3a0.629\uff0c\u8d85\u8fc7\u4e86SAFE (+SRAG) (0.577)\u548c\u57fa\u7ebf(0.279)\u3002\u5728\u4e3b\u89c2\u8bc4\u4f30\u4e2d\uff0cSAFE (LOTR-RAG)\u5728\u6709\u7528\u6027\u3001\u6e05\u6670\u5ea6\u548c\u771f\u5b9e\u6027\u65b9\u9762\u4e5f\u83b7\u5f97\u4e86\u6700\u9ad8\u7684\u5e73\u5747\u8bc4\u5206\u3002", "conclusion": "SAFE\u901a\u8fc7\u89e3\u51b3LLM\u5728\u4e00\u81f4\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u8bc1\u660e\u4e86\u5728\u957f\u7bc7COVID-19\u4e8b\u5b9e\u6838\u67e5\u65b9\u9762\u7684\u5f3a\u5927\u6539\u8fdb\u3002\u6838\u5fc3LOTR-RAG\u8bbe\u8ba1\u6bd4\u5176SRAG\u589e\u5f3a\u53d8\u4f53\u66f4\u6709\u6548\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u9519\u8bef\u4fe1\u606f\u7f13\u89e3\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u57fa\u7840\u3002"}}
{"id": "2512.00313", "categories": ["cs.IR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.00313", "abs": "https://arxiv.org/abs/2512.00313", "authors": ["Zhitong Guan", "Yi Wang"], "title": "Evolving Paradigms in Task-Based Search and Learning: A Comparative Analysis of Traditional Search Engine with LLM-Enhanced Conversational Search System", "comment": null, "summary": "Large Language Models (LLMs) are rapidly reshaping information retrieval by enabling interactive, generative, and inference-driven search. While traditional keyword-based search remains central to web and academic information access, it often struggles to support multi-step reasoning and exploratory learning tasks. LLM-powered search interfaces, such as ChatGPT and Claude, introduce new capabilities that may influence how users formulate queries, navigate information, and construct knowledge. However, empirical understanding of these effects is still limited. This study compares search behavior and learning outcomes in two environments: a standard search engine and an LLM-powered search system. We investigate (1) how search strategies, query formulation, and evaluation behaviors differ across systems, and (2) how LLM use affects comprehension, knowledge integration, and critical thinking during search-based learning tasks. Findings offer insight into how generative AI shapes information-seeking processes and contribute to ongoing discussions in information retrieval, human-AI interaction, and technology-supported learning.", "AI": {"tldr": "\u672c\u7814\u7a76\u5bf9\u6bd4\u4e86\u4f20\u7edf\u641c\u7d22\u5f15\u64ce\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u7684\u641c\u7d22\u7cfb\u7edf\u4e2d\u7684\u641c\u7d22\u884c\u4e3a\u548c\u5b66\u4e60\u6210\u679c\u3002", "motivation": "\u4f20\u7edf\u5173\u952e\u8bcd\u641c\u7d22\u5728\u591a\u6b65\u9aa4\u63a8\u7406\u548c\u63a2\u7d22\u6027\u5b66\u4e60\u4efb\u52a1\u4e2d\u5b58\u5728\u5c40\u9650\u6027\u3002LLM\u641c\u7d22\u754c\u9762\u5f15\u5165\u4e86\u65b0\u529f\u80fd\uff0c\u4f46\u5bf9\u5176\u5f71\u54cd\u7684\u5b9e\u8bc1\u7406\u89e3\u6709\u9650\u3002", "method": "\u6bd4\u8f83\u6807\u51c6\u641c\u7d22\u5f15\u64ce\u548cLLM\u9a71\u52a8\u7684\u641c\u7d22\u7cfb\u7edf\u4e2d\u7684\u641c\u7d22\u884c\u4e3a\u548c\u5b66\u4e60\u6210\u679c\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u751f\u6210\u5f0fAI\u5982\u4f55\u5f71\u54cd\u4fe1\u606f\u641c\u7d22\u8fc7\u7a0b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4fe1\u606f\u68c0\u7d22\u3001\u4eba\u673a\u4ea4\u4e92\u548c\u6280\u672f\u652f\u6301\u5b66\u4e60\u7b49\u9886\u57df\u7684\u8ba8\u8bba\u505a\u51fa\u4e86\u8d21\u732e\u3002"}}
{"id": "2512.00367", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00367", "abs": "https://arxiv.org/abs/2512.00367", "authors": ["Aparajitha Allamraju", "Maitreya Prafulla Chitale", "Hiranmai Sri Adibhatla", "Rahul Mishra", "Manish Shrivastava"], "title": "Breaking It Down: Domain-Aware Semantic Segmentation for Retrieval Augmented Generation", "comment": null, "summary": "Document chunking is a crucial component of Retrieval-Augmented Generation (RAG), as it directly affects the retrieval of relevant and precise context. Conventional fixed-length and recursive splitters often produce arbitrary, incoherent segments that fail to preserve semantic structure. Although semantic chunking has gained traction, its influence on generation quality remains underexplored. This paper introduces two efficient semantic chunking methods, Projected Similarity Chunking (PSC) and Metric Fusion Chunking (MFC), trained on PubMed data using three different embedding models. We further present an evaluation framework that measures the effect of chunking on both retrieval and generation by augmenting PubMedQA with full-text PubMed Central articles. Our results show substantial retrieval improvements (24x with PSC) in MRR and higher Hits@k on PubMedQA. We provide a comprehensive analysis, including statistical significance and response-time comparisons with common chunking libraries. Despite being trained on a single domain, PSC and MFC also generalize well, achieving strong out-of-domain generation performance across multiple datasets. Overall, our findings confirm that our semantic chunkers, especially PSC, consistently deliver superior performance.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u8bed\u4e49\u5206\u5757\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u7684\u5206\u5757\u65b9\u6cd5\u901a\u5e38\u4f1a\u4ea7\u751f\u4efb\u610f\u4e14\u4e0d\u8fde\u8d2f\u7684\u7247\u6bb5\uff0c\u65e0\u6cd5\u4fdd\u6301\u8bed\u4e49\u7ed3\u6784\u3002\u8bed\u4e49\u5206\u5757\u5bf9\u751f\u6210\u8d28\u91cf\u7684\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u6709\u6548\u7684\u8bed\u4e49\u5206\u5757\u65b9\u6cd5\uff1a\u6295\u5f71\u76f8\u4f3c\u5ea6\u5206\u5757\uff08PSC\uff09\u548c\u5ea6\u91cf\u878d\u5408\u5206\u5757\uff08MFC\uff09\uff0c\u5e76\u5728PubMed\u6570\u636e\u4e0a\u4f7f\u7528\u4e09\u79cd\u4e0d\u540c\u7684\u5d4c\u5165\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528\u5168\u6587PubMed Central\u6587\u7ae0\u589e\u5f3aPubMedQA\uff0c\u6765\u8861\u91cf\u5206\u5757\u5bf9\u68c0\u7d22\u548c\u751f\u6210\u7684\u5f71\u54cd\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5728PubMedQA\u4e0a\uff0cMRR\u7684\u68c0\u7d22\u6548\u679c\u663e\u8457\u63d0\u9ad8\uff08PSC\u63d0\u9ad8\u4e8624\u500d\uff09\uff0c\u5e76\u4e14Hits@k\u4e5f\u66f4\u9ad8\u3002PSC\u548cMFC\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u8de8\u9886\u57df\u751f\u6210\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8bc1\u5b9e\uff0c\u8bed\u4e49\u5206\u5757\u5668\uff0c\u7279\u522b\u662fPSC\uff0c\u59cb\u7ec8\u80fd\u591f\u63d0\u4f9b\u5353\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2512.00105", "categories": ["cs.DB", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00105", "abs": "https://arxiv.org/abs/2512.00105", "authors": ["Djawad Bekkoucha", "Lamine Diop", "Abdelkader Ouali", "Bruno Cr\u00e9milleux", "Patrice Boizumault"], "title": "Efficiently Sampling Interval Patterns from Numerical Databases", "comment": null, "summary": "Pattern sampling has emerged as a promising approach for information discovery in large databases, allowing analysts to focus on a manageable subset of patterns. In this approach, patterns are randomly drawn based on an interestingness measure, such as frequency or hyper-volume. This paper presents the first sampling approach designed to handle interval patterns in numerical databases. This approach, named Fips, samples interval patterns proportionally to their frequency. It uses a multi-step sampling procedure and addresses a key challenge in numerical data: accurately determining the number of interval patterns that cover each object. We extend this work with HFips, which samples interval patterns proportionally to both their frequency and hyper-volume. These methods efficiently tackle the well-known long-tail phenomenon in pattern sampling. We formally prove that Fips and HFips sample interval patterns in proportion to their frequency and the product of hyper-volume and frequency, respectively. Through experiments on several databases, we demonstrate the quality of the obtained patterns and their robustness against the long-tail phenomenon.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u5f0f\u62bd\u6837\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u6570\u503c\u6570\u636e\u5e93\u4e2d\u53d1\u73b0\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u9891\u7387\u548c\u8d85\u4f53\u79ef\u6765\u62bd\u53d6\u95f4\u9694\u6a21\u5f0f\u3002", "motivation": "\u5728\u5927\u6570\u636e\u5e93\u4e2d\uff0c\u4fe1\u606f\u53d1\u73b0\u9762\u4e34\u6311\u6218\uff0c\u5206\u6790\u5e08\u96be\u4ee5\u5904\u7406\u5927\u91cf\u7684\u6a21\u5f0f\u3002\u6a21\u5f0f\u62bd\u6837\u63d0\u4f9b\u4e86\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u5141\u8bb8\u5206\u6790\u5e08\u5173\u6ce8\u53ef\u7ba1\u7406\u7684\u6a21\u5f0f\u5b50\u96c6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFips\u7684\u62bd\u6837\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6309\u9891\u7387\u6bd4\u4f8b\u5bf9\u95f4\u9694\u6a21\u5f0f\u8fdb\u884c\u62bd\u6837\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86HFips\uff0c\u5b83\u6309\u9891\u7387\u548c\u8d85\u4f53\u79ef\u7684\u6bd4\u4f8b\u5bf9\u95f4\u9694\u6a21\u5f0f\u8fdb\u884c\u62bd\u6837\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u83b7\u5f97\u7684\u6a21\u5f0f\u8d28\u91cf\u9ad8\uff0c\u5e76\u4e14\u5bf9\u957f\u5c3e\u73b0\u8c61\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "Fips\u548cHFips\u80fd\u591f\u6709\u6548\u5730\u5bf9\u95f4\u9694\u6a21\u5f0f\u8fdb\u884c\u62bd\u6837\uff0c\u5e76\u4e14\u80fd\u591f\u7f13\u89e3\u957f\u5c3e\u73b0\u8c61\u3002"}}
{"id": "2512.00097", "categories": ["cs.AI", "cs.CG"], "pdf": "https://arxiv.org/pdf/2512.00097", "abs": "https://arxiv.org/abs/2512.00097", "authors": ["Boyan Duan", "Xiao Liang", "Shuai Lu", "Yaoxiang Wang", "Yelong Shen", "Kai-Wei Chang", "Ying Nian Wu", "Mao Yang", "Weizhu Chen", "Yeyun Gong"], "title": "Gold-Medal-Level Olympiad Geometry Solving with Efficient Heuristic Auxiliary Constructions", "comment": null, "summary": "Automated theorem proving in Euclidean geometry, particularly for International Mathematical Olympiad (IMO) level problems, remains a major challenge and an important research focus in Artificial Intelligence. In this paper, we present a highly efficient method for geometry theorem proving that runs entirely on CPUs without relying on neural network-based inference. Our initial study shows that a simple random strategy for adding auxiliary points can achieve silver-medal level human performance on IMO. Building on this, we propose HAGeo, a Heuristic-based method for adding Auxiliary constructions in Geometric deduction that solves 28 of 30 problems on the IMO-30 benchmark, achieving gold-medal level performance and surpassing AlphaGeometry, a competitive neural network-based approach, by a notable margin. To evaluate our method and existing approaches more comprehensively, we further construct HAGeo-409, a benchmark consisting of 409 geometry problems with human-assessed difficulty levels. Compared with the widely used IMO-30, our benchmark poses greater challenges and provides a more precise evaluation, setting a higher bar for geometry theorem proving.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u51e0\u4f55\u5b9a\u7406\u8bc1\u660e\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5b8c\u5168\u5728 CPU \u4e0a\u8fd0\u884c\uff0c\u4e0d\u4f9d\u8d56\u4e8e\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u63a8\u7406\u3002", "motivation": "\u5728\u6b27\u51e0\u91cc\u5f97\u51e0\u4f55\u4e2d\u8fdb\u884c\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u56fd\u9645\u6570\u5b66\u5965\u6797\u5339\u514b (IMO) \u7ea7\u522b\u7684\u95ee\u9898\uff0c\u4ecd\u7136\u662f\u4e00\u4e2a\u4e3b\u8981\u7684\u6311\u6218\uff0c\u4e5f\u662f\u4eba\u5de5\u667a\u80fd\u4e2d\u4e00\u4e2a\u91cd\u8981\u7684\u7814\u7a76\u91cd\u70b9\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u542f\u53d1\u5f0f\u7684\u65b9\u6cd5 (HAGeo) \u7528\u4e8e\u5728\u51e0\u4f55\u6f14\u7ece\u4e2d\u6dfb\u52a0\u8f85\u52a9\u7ed3\u6784\u3002", "result": "HAGeo \u5728 IMO-30 \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u89e3\u51b3\u4e86 30 \u4e2a\u95ee\u9898\u4e2d\u7684 28 \u4e2a\uff0c\u8fbe\u5230\u4e86\u91d1\u724c\u6c34\u5e73\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u8d85\u8fc7\u4e86 AlphaGeometry\u3002\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b 409 \u4e2a\u5177\u6709\u4eba\u5de5\u8bc4\u4f30\u96be\u5ea6\u7ea7\u522b\u7684\u51e0\u4f55\u95ee\u9898\u7684\u65b0\u57fa\u51c6 HAGeo-409\u3002", "conclusion": "\u8be5\u57fa\u51c6\u6d4b\u8bd5\u6bd4\u5e7f\u6cdb\u4f7f\u7528\u7684 IMO-30 \u63d0\u51fa\u4e86\u66f4\u5927\u7684\u6311\u6218\uff0c\u5e76\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u8bc4\u4f30\uff0c\u4e3a\u51e0\u4f55\u5b9a\u7406\u8bc1\u660e\u8bbe\u5b9a\u4e86\u66f4\u9ad8\u7684\u6807\u51c6\u3002"}}
{"id": "2512.00046", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00046", "abs": "https://arxiv.org/abs/2512.00046", "authors": ["Angelina Parfenova", "Andreas Marfurt", "Alexander Denzler", "Juergen Pfeffer"], "title": "Text Annotation via Inductive Coding: Comparing Human Experts to LLMs in Qualitative Data Analysis", "comment": null, "summary": "This paper investigates the automation of qualitative data analysis, focusing on inductive coding using large language models (LLMs). Unlike traditional approaches that rely on deductive methods with predefined labels, this research investigates the inductive process where labels emerge from the data. The study evaluates the performance of six open-source LLMs compared to human experts. As part of the evaluation, experts rated the perceived difficulty of the quotes they coded. The results reveal a peculiar dichotomy: human coders consistently perform well when labeling complex sentences but struggle with simpler ones, while LLMs exhibit the opposite trend. Additionally, the study explores systematic deviations in both human and LLM generated labels by comparing them to the golden standard from the test set. While human annotations may sometimes differ from the golden standard, they are often rated more favorably by other humans. In contrast, some LLMs demonstrate closer alignment with the true labels but receive lower evaluations from experts.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u81ea\u52a8\u8fdb\u884c\u5b9a\u6027\u6570\u636e\u5206\u6790\uff0c\u91cd\u70b9\u5173\u6ce8\u5f52\u7eb3\u7f16\u7801\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u9884\u5b9a\u4e49\u6807\u7b7e\u7684\u6f14\u7ece\u65b9\u6cd5\uff0c\u800c\u672c\u7814\u7a76\u5219\u7740\u773c\u4e8e\u6807\u7b7e\u4ece\u6570\u636e\u4e2d\u4ea7\u751f\u7684\u5f52\u7eb3\u8fc7\u7a0b\u3002", "method": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u516d\u4e2a\u5f00\u6e90llm\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u76f8\u6bd4\u7684\u6027\u80fd\uff0c\u4e13\u5bb6\u5bf9\u4ed6\u4eec\u7f16\u7801\u7684\u5f15\u7528\u7684\u611f\u77e5\u96be\u5ea6\u8fdb\u884c\u4e86\u8bc4\u7ea7\u3002", "result": "\u7ed3\u679c\u63ed\u793a\u4e86\u4e00\u4e2a\u5947\u7279\u7684\u4e8c\u5206\u6cd5\uff1a\u4eba\u7c7b\u7f16\u7801\u5458\u5728\u6807\u8bb0\u590d\u6742\u53e5\u5b50\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5904\u7406\u7b80\u5355\u53e5\u5b50\u65f6\u5374\u5f88\u5403\u529b\uff0c\u800cllm\u5219\u8868\u73b0\u51fa\u76f8\u53cd\u7684\u8d8b\u52bf\u3002", "conclusion": "\u867d\u7136\u4eba\u7c7b\u6ce8\u91ca\u6709\u65f6\u53ef\u80fd\u4e0e\u9ec4\u91d1\u6807\u51c6\u4e0d\u540c\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u4f1a\u53d7\u5230\u5176\u4ed6\u4eba\u7684\u597d\u8bc4\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u4e00\u4e9bllm\u8868\u73b0\u51fa\u4e0e\u771f\u5b9e\u6807\u7b7e\u66f4\u7d27\u5bc6\u7684\u5bf9\u9f50\uff0c\u4f46\u53d7\u5230\u4e13\u5bb6\u7684\u8bc4\u4ef7\u8f83\u4f4e\u3002"}}
{"id": "2512.00163", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00163", "abs": "https://arxiv.org/abs/2512.00163", "authors": ["Saeed AlMarri", "Mathieu Ravaut", "Kristof Juhasz", "Gautier Marti", "Hamdan Al Ahbabi", "Ibrahim Elfadel"], "title": "Measuring What LLMs Think They Do: SHAP Faithfulness and Deployability on Financial Tabular Classification", "comment": "7 pages, 3 figures, 3 tables, AAAI 2026 Deployable AI Workshop", "summary": "Large Language Models (LLMs) have attracted significant attention for classification tasks, offering a flexible alternative to trusted classical machine learning models like LightGBM through zero-shot prompting. However, their reliability for structured tabular data remains unclear, particularly in high stakes applications like financial risk assessment. Our study systematically evaluates LLMs and generates their SHAP values on financial classification tasks. Our analysis shows a divergence between LLMs self-explanation of feature impact and their SHAP values, as well as notable differences between LLMs and LightGBM SHAP values. These findings highlight the limitations of LLMs as standalone classifiers for structured financial modeling, but also instill optimism that improved explainability mechanisms coupled with few-shot prompting will make LLMs usable in risk-sensitive domains.", "AI": {"tldr": "LLMs are being explored for classification tasks but their reliability with structured tabular data, especially in finance, is uncertain.", "motivation": "The study evaluates the reliability of LLMs in financial classification tasks, an area where accuracy and explainability are critical.", "method": "The study systematically evaluates LLMs and generates their SHAP values on financial classification tasks and compares them to LightGBM SHAP values.", "result": "LLMs' self-explanations diverge from their SHAP values, and there are differences between LLM and LightGBM SHAP values.", "conclusion": "LLMs have limitations as standalone classifiers for structured financial modeling, but improvements in explainability and few-shot prompting may make them viable in risk-sensitive domains."}}
{"id": "2512.00008", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.00008", "abs": "https://arxiv.org/abs/2512.00008", "authors": ["Veeramani Pugazhenthi", "Wei-Hsiang Chu", "Junwei Lu", "Jadyn N. Miyahira", "Soheil Salehi"], "title": "MOTION: ML-Assisted On-Device Low-Latency Motion Recognition", "comment": null, "summary": "The use of tiny devices capable of low-latency gesture recognition is gaining momentum in everyday human-computer interaction and especially in medical monitoring fields. Embedded solutions such as fall detection, rehabilitation tracking, and patient supervision require fast and efficient tracking of movements while avoiding unwanted false alarms. This study presents an efficient solution on how to build very efficient motion-based models only using triaxial accelerometer sensors. We explore the capability of the AutoML pipelines to extract the most important features from the data segments. This approach also involves training multiple lightweight machine learning algorithms using the extracted features. We use WeBe Band, a multi-sensor wearable device that is equipped with a powerful enough MCU to effectively perform gesture recognition entirely on the device. Of the models explored, we found that the neural network provided the best balance between accuracy, latency, and memory use. Our results also demonstrate that reliable real-time gesture recognition can be achieved in WeBe Band, with great potential for real-time medical monitoring solutions that require a secure and fast response time.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u4e09\u8f74\u52a0\u901f\u5ea6\u8ba1\u4f20\u611f\u5668\u6784\u5efa\u9ad8\u6548\u7684\u57fa\u4e8e\u8fd0\u52a8\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u65e5\u5e38\u4eba\u673a\u4ea4\u4e92\u548c\u533b\u7597\u76d1\u63a7\u9886\u57df\u3002", "motivation": "\u5728\u8dcc\u5012\u68c0\u6d4b\u3001\u5eb7\u590d\u8ddf\u8e2a\u548c\u75c5\u4eba\u76d1\u62a4\u7b49\u5d4c\u5165\u5f0f\u89e3\u51b3\u65b9\u6848\u4e2d\uff0c\u9700\u8981\u5feb\u901f\u6709\u6548\u5730\u8ddf\u8e2a\u8fd0\u52a8\uff0c\u540c\u65f6\u907f\u514d\u4e0d\u5fc5\u8981\u7684\u8bef\u62a5\u3002", "method": "\u5229\u7528AutoML\u7ba1\u9053\u4ece\u6570\u636e\u6bb5\u4e2d\u63d0\u53d6\u6700\u91cd\u8981\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u63d0\u53d6\u7684\u7279\u5f81\u8bad\u7ec3\u591a\u4e2a\u8f7b\u91cf\u7ea7\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u3002\u4f7f\u7528WeBe Band\uff0c\u4e00\u79cd\u914d\u5907\u4e86\u529f\u80fd\u5f3a\u5927\u7684MCU\u7684\u591a\u4f20\u611f\u5668\u53ef\u7a7f\u6234\u8bbe\u5907\uff0c\u4ee5\u5728\u8bbe\u5907\u4e0a\u6709\u6548\u6267\u884c\u624b\u52bf\u8bc6\u522b\u3002", "result": "\u5728\u63a2\u7d22\u7684\u6a21\u578b\u4e2d\uff0c\u53d1\u73b0\u795e\u7ecf\u7f51\u7edc\u5728\u51c6\u786e\u6027\u3001\u5ef6\u8fdf\u548c\u5185\u5b58\u4f7f\u7528\u4e4b\u95f4\u63d0\u4f9b\u4e86\u6700\u4f73\u5e73\u8861\u3002\u7ed3\u679c\u8fd8\u8868\u660e\uff0c\u5728WeBe Band\u4e2d\u53ef\u4ee5\u5b9e\u73b0\u53ef\u9760\u7684\u5b9e\u65f6\u624b\u52bf\u8bc6\u522b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u5b9e\u65f6\u624b\u52bf\u8bc6\u522b\u5728\u9700\u8981\u5b89\u5168\u548c\u5feb\u901f\u54cd\u5e94\u65f6\u95f4\u7684\u5b9e\u65f6\u533b\u7597\u76d1\u63a7\u89e3\u51b3\u65b9\u6848\u4e2d\u5177\u6709\u5de8\u5927\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.00439", "categories": ["cs.IR", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00439", "abs": "https://arxiv.org/abs/2512.00439", "authors": ["Xiaoshan Yu", "Ziwei Huang", "Shangshang Yang", "Ziwen Wang", "Haiping Ma", "Xingyi Zhang"], "title": "PEOAT: Personalization-Guided Evolutionary Question Assembly for One-Shot Adaptive Testing", "comment": "AAAI-2026, 9 pages", "summary": "With the rapid advancement of intelligent education, Computerized Adaptive Testing (CAT) has attracted increasing attention by integrating educational psychology with deep learning technologies. Unlike traditional paper-and-pencil testing, CAT aims to efficiently and accurately assess examinee abilities by adaptively selecting the most suitable items during the assessment process. However, its real-time and sequential nature presents limitations in practical scenarios, particularly in large-scale assessments where interaction costs are high, or in sensitive domains such as psychological evaluations where minimizing noise and interference is essential. These challenges constrain the applicability of conventional CAT methods in time-sensitive or resourceconstrained environments. To this end, we first introduce a novel task called one-shot adaptive testing (OAT), which aims to select a fixed set of optimal items for each test-taker in a one-time selection. Meanwhile, we propose PEOAT, a Personalization-guided Evolutionary question assembly framework for One-shot Adaptive Testing from the perspective of combinatorial optimization. Specifically, we began by designing a personalization-aware initialization strategy that integrates differences between examinee ability and exercise difficulty, using multi-strategy sampling to construct a diverse and informative initial population. Building on this, we proposed a cognitive-enhanced evolutionary framework incorporating schema-preserving crossover and cognitively guided mutation to enable efficient exploration through informative signals. To maintain diversity without compromising fitness, we further introduced a diversity-aware environmental selection mechanism. The effectiveness of PEOAT is validated through extensive experiments on two datasets, complemented by case studies that uncovered valuable insights.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5355\u6b21\u81ea\u9002\u5e94\u6d4b\u8bd5 (OAT) \u4efb\u52a1\uff0c\u65e8\u5728\u4e00\u6b21\u6027\u4e3a\u6bcf\u4e2a\u6d4b\u8bd5\u8005\u9009\u62e9\u4e00\u7ec4\u56fa\u5b9a\u7684\u6700\u4f73\u9879\u76ee\u3002", "motivation": "\u4f20\u7edf\u8ba1\u7b97\u673a\u81ea\u9002\u5e94\u6d4b\u8bd5 (CAT) \u5728\u5927\u89c4\u6a21\u8bc4\u4f30\u548c\u5fc3\u7406\u8bc4\u4f30\u7b49\u654f\u611f\u9886\u57df\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u4e3a\u4ea4\u4e92\u6210\u672c\u9ad8\uff0c\u4e14\u9700\u8981\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u566a\u97f3\u548c\u5e72\u6270\u3002", "method": "\u63d0\u51fa\u4e86 PEOAT\uff0c\u4e00\u4e2a\u4ece\u7ec4\u5408\u4f18\u5316\u89d2\u5ea6\u51fa\u53d1\u7684\u4e2a\u6027\u5316\u5f15\u5bfc\u8fdb\u5316\u95ee\u9898\u7ec4\u88c5\u6846\u67b6\uff0c\u7528\u4e8e\u5355\u6b21\u81ea\u9002\u5e94\u6d4b\u8bd5\u3002\u8be5\u6846\u67b6\u5305\u62ec\u4e2a\u6027\u5316\u611f\u77e5\u521d\u59cb\u5316\u7b56\u7565\u3001\u8ba4\u77e5\u589e\u5f3a\u8fdb\u5316\u6846\u67b6\u548c\u591a\u6837\u6027\u611f\u77e5\u73af\u5883\u9009\u62e9\u673a\u5236\u3002", "result": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86 PEOAT \u7684\u6709\u6548\u6027\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u53d1\u73b0\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002", "conclusion": "PEOAT \u6846\u67b6\u5728\u5355\u6b21\u81ea\u9002\u5e94\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u5730\u4e3a\u6bcf\u4e2a\u6d4b\u8bd5\u8005\u9009\u62e9\u6700\u4f73\u9879\u76ee\u3002"}}
{"id": "2512.00662", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.00662", "abs": "https://arxiv.org/abs/2512.00662", "authors": ["Christian Mancas", "Diana Christina Mancas"], "title": "MatBase algorithm for translating (E)MDM schemes into E-R data models", "comment": "Submitted on 11/27/2025 to the Journal of Data Science and Intelligent Systems, BON VIEW PUB. PTE. LTD, Singapore", "summary": "This paper presents a pseudocode algorithm for translating (Elementary) Mathematical Data Model schemes into Entity-Relationship data models. We prove that this algorithm is linear, sound, complete, and semi-optimal. As an example, we apply this algorithm to an (Elementary) Mathematical Data Model scheme for a genealogical tree sub-universe. We also provide the main additional features added to the implementation of this algorithm in MatBase, our intelligent knowledge and database management system prototype based on both the Entity-Relationship, (Elementary) Mathematical, and Relational Data Models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06(\u57fa\u672c)\u6570\u5b66\u6570\u636e\u6a21\u578b\u65b9\u6848\u8f6c\u6362\u4e3a\u5b9e\u4f53\u5173\u7cfb\u6570\u636e\u6a21\u578b\u7684\u4f2a\u4ee3\u7801\u7b97\u6cd5\u3002", "motivation": "\u5c06\u6570\u5b66\u6570\u636e\u6a21\u578b\u8f6c\u6362\u4e3a\u5b9e\u4f53\u5173\u7cfb\u6570\u636e\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4f2a\u4ee3\u7801\u7b97\u6cd5\uff0c\u5e76\u8bc1\u660e\u8be5\u7b97\u6cd5\u662f\u7ebf\u6027\u7684\u3001\u53ef\u9760\u7684\u3001\u5b8c\u6574\u7684\u548c\u534a\u4f18\u7684\u3002", "result": "\u5c06\u8be5\u7b97\u6cd5\u5e94\u7528\u4e8e\u65cf\u8c31\u6811\u5b50\u5b87\u5b99\u7684(\u57fa\u672c)\u6570\u5b66\u6570\u636e\u6a21\u578b\u65b9\u6848\u3002\u5728MatBase\u4e2d\u5b9e\u73b0\u4e86\u8be5\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u7b97\u6cd5\uff0c\u53ef\u4ee5\u5728MatBase\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u6570\u5b66\u6570\u636e\u6a21\u578b\u5230\u5b9e\u4f53\u5173\u7cfb\u6570\u636e\u6a21\u578b\u7684\u8f6c\u6362\u3002"}}
{"id": "2512.00185", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00185", "abs": "https://arxiv.org/abs/2512.00185", "authors": ["Shashanka B R", "Mohith Charan R", "Seema Banu F"], "title": "Chunking Strategies for Multimodal AI Systems", "comment": "45 pages, 5 figure", "summary": "Our goal is to consolidate the landscape of multimodal chunking strategies, providing researchers and practitioners with a technical foundation and design space for developing more effective and efficient multimodal AI systems. This survey paves the way for innovations in robust chunking pipelines that scale with modality complexity, enhance processing accuracy, and improve generative coherence in real-world applications. This survey provides a comprehensive taxonomy and technical analysis of chunking strategies tailored for each modality: text, images, audio, video, and cross-modal data. We examine classical and modern approaches such as fixed-size token windowing, recursive text splitting, object-centric visual chunking, silence-based audio segmentation, and scene detection in videos. Each approach is analyzed in terms of its underlying methodology, supporting tools (e.g., LangChain, Detectron2, PySceneDetect), benefits, and challenges, particularly those related to granularity-context trade-offs and multimodal alignment. Furthermore, we explore emerging cross-modal chunking strategies that aim to preserve alignment and semantic consistency across disparate data types [4]. We also include comparative insights, highlight open problems such as asynchronous information density and noisy alignment signals, and identify opportunities for future research in adaptive, learning-based, and task-specific chunking.", "AI": {"tldr": "\u672c\u6587\u65e8\u5728\u6574\u5408\u591a\u6a21\u6001\u5206\u5757\u7b56\u7565\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u6280\u672f\u57fa\u7840\u548c\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u4ee5\u5f00\u53d1\u66f4\u6709\u6548\u548c\u9ad8\u6548\u7684\u591a\u6a21\u6001AI\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001AI\u7cfb\u7edf\u5728\u5904\u7406\u590d\u6742\u6a21\u6001\u6570\u636e\u65f6\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u5206\u5757\u7b56\u7565\u3002", "method": "\u672c\u6587\u5bf9\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u3001\u89c6\u9891\u548c\u8de8\u6a21\u6001\u6570\u636e\u7684\u5206\u5757\u7b56\u7565\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5206\u7c7b\u548c\u6280\u672f\u5206\u6790\uff0c\u8003\u5bdf\u4e86\u7ecf\u5178\u548c\u73b0\u4ee3\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u5b83\u4eec\u7684\u65b9\u6cd5\u8bba\u3001\u652f\u6301\u5de5\u5177\u3001\u4f18\u70b9\u548c\u6311\u6218\u3002", "result": "\u672c\u6587\u5bf9\u5404\u79cd\u5206\u5757\u7b56\u7565\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\uff0c\u5f3a\u8c03\u4e86\u5f02\u6b65\u4fe1\u606f\u5bc6\u5ea6\u548c\u566a\u58f0\u5bf9\u9f50\u4fe1\u53f7\u7b49\u5f00\u653e\u6027\u95ee\u9898\uff0c\u5e76\u4e3a\u81ea\u9002\u5e94\u3001\u57fa\u4e8e\u5b66\u4e60\u548c\u7279\u5b9a\u4efb\u52a1\u7684\u5206\u5757\u7684\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u3001\u51c6\u786e\u548c\u8fde\u8d2f\u7684\u591a\u6a21\u6001AI\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u673a\u9047\u3002"}}
{"id": "2512.00047", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00047", "abs": "https://arxiv.org/abs/2512.00047", "authors": ["Angelina Parfenova", "Alexander Denzler", "Juergen Pfeffer"], "title": "Emergent Convergence in Multi-Agent LLM Annotation", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in collaborative settings, yet little is known about how they coordinate when treated as black-box agents. We simulate 7500 multi-agent, multi-round discussions in an inductive coding task, generating over 125000 utterances that capture both final annotations and their interactional histories. We introduce process-level metrics: code stability, semantic self-consistency, and lexical confidence alongside sentiment and convergence measures, to track coordination dynamics. To probe deeper alignment signals, we analyze the evolving geometry of output embeddings, showing that intrinsic dimensionality declines over rounds, suggesting semantic compression. The results reveal that LLM groups converge lexically and semantically, develop asymmetric influence patterns, and exhibit negotiation-like behaviors despite the absence of explicit role prompting. This work demonstrates how black-box interaction analysis can surface emergent coordination strategies, offering a scalable complement to internal probe-based interpretability methods.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u88ab\u90e8\u7f72\u5728\u534f\u4f5c\u73af\u5883\u4e2d\uff0c\u4f46\u4eba\u4eec\u5bf9\u5b83\u4eec\u5728\u88ab\u89c6\u4e3a\u9ed1\u76d2\u4ee3\u7406\u65f6\u5982\u4f55\u8fdb\u884c\u534f\u8c03\u77e5\u4e4b\u751a\u5c11\u3002\u672c\u7814\u7a76\u6a21\u62df\u4e86 7500 \u4e2a\u591a\u667a\u80fd\u4f53\u3001\u591a\u8f6e\u8ba8\u8bba\uff0c\u751f\u6210\u4e86\u8d85\u8fc7 125000 \u6761\u8bdd\u8bed\uff0c\u6355\u6349\u4e86\u6700\u7ec8\u6ce8\u91ca\u53ca\u5176\u4ea4\u4e92\u5386\u53f2\u3002\u5f15\u5165\u4e86\u8fc7\u7a0b\u7ea7\u6307\u6807\uff1a\u4ee3\u7801\u7a33\u5b9a\u6027\u3001\u8bed\u4e49\u81ea\u6d3d\u6027\u548c\u8bcd\u6c47\u7f6e\u4fe1\u5ea6\u4ee5\u53ca\u60c5\u611f\u548c\u6536\u655b\u6027\u6307\u6807\uff0c\u4ee5\u8ddf\u8e2a\u534f\u8c03\u52a8\u6001\u3002\u4e3a\u4e86\u66f4\u6df1\u5165\u5730\u63a2\u6d4b\u5bf9\u9f50\u4fe1\u53f7\uff0c\u5206\u6790\u4e86\u8f93\u51fa\u5d4c\u5165\u7684\u6f14\u5316\u51e0\u4f55\u7ed3\u6784\uff0c\u8868\u660e\u5185\u5728\u7ef4\u5ea6\u968f\u8f6e\u6570\u4e0b\u964d\uff0c\u8868\u660e\u8bed\u4e49\u538b\u7f29\u3002\u7ed3\u679c\u8868\u660e\uff0cLLM \u7ec4\u5728\u8bcd\u6c47\u548c\u8bed\u4e49\u4e0a\u8d8b\u4e8e\u6536\u655b\uff0c\u53d1\u5c55\u51fa\u4e0d\u5bf9\u79f0\u7684\u5f71\u54cd\u6a21\u5f0f\uff0c\u5e76\u8868\u73b0\u51fa\u7c7b\u4f3c\u8c08\u5224\u7684\u884c\u4e3a\uff0c\u5c3d\u7ba1\u7f3a\u4e4f\u660e\u786e\u7684\u89d2\u8272\u63d0\u793a\u3002\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u9ed1\u76d2\u4ea4\u4e92\u5206\u6790\u5982\u4f55\u6d6e\u73b0\u51fa\u6d8c\u73b0\u7684\u534f\u8c03\u7b56\u7565\uff0c\u4ece\u800c\u4e3a\u57fa\u4e8e\u5185\u90e8\u63a2\u6d4b\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8865\u5145\u3002", "motivation": "\u63a2\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u534f\u4f5c\u73af\u5883\u4e2d\u4f5c\u4e3a\u9ed1\u76d2\u4ee3\u7406\u65f6\u7684\u534f\u8c03\u65b9\u5f0f\u3002", "method": "\u6a21\u62df 7500 \u4e2a\u591a\u667a\u80fd\u4f53\u3001\u591a\u8f6e\u8ba8\u8bba\uff0c\u751f\u6210\u8d85\u8fc7 125000 \u6761\u8bdd\u8bed\uff0c\u5e76\u5f15\u5165\u8fc7\u7a0b\u7ea7\u6307\u6807\uff08\u4ee3\u7801\u7a33\u5b9a\u6027\u3001\u8bed\u4e49\u81ea\u6d3d\u6027\u548c\u8bcd\u6c47\u7f6e\u4fe1\u5ea6\uff09\u4ee5\u53ca\u60c5\u611f\u548c\u6536\u655b\u6027\u6307\u6807\u6765\u8ddf\u8e2a\u534f\u8c03\u52a8\u6001\u3002\u5206\u6790\u8f93\u51fa\u5d4c\u5165\u7684\u6f14\u5316\u51e0\u4f55\u7ed3\u6784\u4ee5\u63a2\u6d4b\u66f4\u6df1\u5165\u7684\u5bf9\u9f50\u4fe1\u53f7\u3002", "result": "LLM \u7ec4\u5728\u8bcd\u6c47\u548c\u8bed\u4e49\u4e0a\u8d8b\u4e8e\u6536\u655b\uff0c\u53d1\u5c55\u51fa\u4e0d\u5bf9\u79f0\u7684\u5f71\u54cd\u6a21\u5f0f\uff0c\u5e76\u8868\u73b0\u51fa\u7c7b\u4f3c\u8c08\u5224\u7684\u884c\u4e3a\u3002", "conclusion": "\u9ed1\u76d2\u4ea4\u4e92\u5206\u6790\u53ef\u4ee5\u63ed\u793a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6d8c\u73b0\u7684\u534f\u8c03\u7b56\u7565\uff0c\u4e3a\u57fa\u4e8e\u5185\u90e8\u63a2\u6d4b\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u63d0\u4f9b\u4e86\u8865\u5145\u3002"}}
{"id": "2512.00164", "categories": ["cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2512.00164", "abs": "https://arxiv.org/abs/2512.00164", "authors": ["Alessandro De Palma", "Greta Dolcetti", "Caterina Urban"], "title": "Faster Verified Explanations for Neural Networks", "comment": null, "summary": "Verified explanations are a theoretically-principled way to explain the decisions taken by neural networks, which are otherwise black-box in nature. However, these techniques face significant scalability challenges, as they require multiple calls to neural network verifiers, each of them with an exponential worst-case complexity. We present FaVeX, a novel algorithm to compute verified explanations. FaVeX accelerates the computation by dynamically combining batch and sequential processing of input features, and by reusing information from previous queries, both when proving invariances with respect to certain input features, and when searching for feature assignments altering the prediction. Furthermore, we present a novel and hierarchical definition of verified explanations, termed verifier-optimal robust explanations, that explicitly factors the incompleteness of network verifiers within the explanation. Our comprehensive experimental evaluation demonstrates the superior scalability of both FaVeX, and of verifier-optimal robust explanations, which together can produce meaningful formal explanation on networks with hundreds of thousands of non-linear activations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5FaVeX\uff0c\u7528\u4e8e\u8ba1\u7b97\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u89e3\u91ca\uff0c\u5e76\u63d0\u51fa\u4e86\u9a8c\u8bc1\u5668\u6700\u4f18\u9c81\u68d2\u89e3\u91ca\u7684\u5206\u5c42\u5b9a\u4e49\u3002", "motivation": "\u89e3\u91ca\u795e\u7ecf\u7f51\u7edc\u7684\u51b3\u7b56\u8fc7\u7a0b\u5728\u7406\u8bba\u4e0a\u5f88\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7684\u9a8c\u8bc1\u89e3\u91ca\u6280\u672f\u9762\u4e34\u53ef\u6269\u5c55\u6027\u6311\u6218\uff0c\u56e0\u4e3a\u5b83\u4eec\u9700\u8981\u591a\u6b21\u8c03\u7528\u795e\u7ecf\u7f51\u7edc\u9a8c\u8bc1\u5668\uff0c\u6bcf\u6b21\u8c03\u7528\u90fd\u5177\u6709\u6307\u6570\u7ea7\u7684\u6700\u574f\u60c5\u51b5\u590d\u6742\u6027\u3002", "method": "FaVeX\u901a\u8fc7\u52a8\u6001\u7ed3\u5408\u8f93\u5165\u7279\u5f81\u7684\u6279\u5904\u7406\u548c\u987a\u5e8f\u5904\u7406\uff0c\u4ee5\u53ca\u91cd\u7528\u5148\u524d\u67e5\u8be2\u4e2d\u7684\u4fe1\u606f\u6765\u52a0\u901f\u8ba1\u7b97\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u548c\u5206\u5c42\u7684\u9a8c\u8bc1\u89e3\u91ca\u5b9a\u4e49\uff0c\u79f0\u4e3a\u9a8c\u8bc1\u5668\u6700\u4f18\u9c81\u68d2\u89e3\u91ca\uff0c\u5b83\u5728\u89e3\u91ca\u4e2d\u660e\u786e\u5730\u8003\u8651\u4e86\u7f51\u7edc\u9a8c\u8bc1\u5668\u7684\u4e0d\u5b8c\u6574\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFaVeX\u548c\u9a8c\u8bc1\u5668\u6700\u4f18\u9c81\u68d2\u89e3\u91ca\u90fd\u5177\u6709 superior \u7684\u53ef\u6269\u5c55\u6027\uff0c\u5b83\u4eec\u53ef\u4ee5\u5728\u5177\u6709\u6570\u5341\u4e07\u4e2a\u975e\u7ebf\u6027\u6fc0\u6d3b\u7684\u7f51\u7edc\u4e0a\u4ea7\u751f\u6709\u610f\u4e49\u7684\u5f62\u5f0f\u89e3\u91ca\u3002", "conclusion": "FaVeX\u7b97\u6cd5\u548c\u9a8c\u8bc1\u5668\u6700\u4f18\u9c81\u68d2\u89e3\u91ca\u7684\u7ed3\u5408\u53ef\u4ee5\u4e3a\u590d\u6742\u7684\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u6709\u6548\u7684\u5f62\u5f0f\u89e3\u91ca\u3002"}}
{"id": "2512.00042", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.00042", "abs": "https://arxiv.org/abs/2512.00042", "authors": ["Egemen Sert", "\u015eeyda Ertekin"], "title": "Closing the Gap: Data-Centric Fine-Tuning of Vision Language Models for the Standardized Exam Questions", "comment": null, "summary": "Multimodal reasoning has become a cornerstone of modern AI research. Standardized exam questions offer a uniquely rigorous testbed for such reasoning, providing structured visual contexts and verifiable answers. While recent progress has largely focused on algorithmic advances such as reinforcement learning (e.g., GRPO, DPO), the data centric foundations of vision language reasoning remain less explored.\n  We show that supervised fine-tuning (SFT) with high-quality data can rival proprietary approaches. To this end, we compile a 161.4 million token multimodal dataset combining textbook question-solution pairs, curriculum aligned diagrams, and contextual materials, and fine-tune Qwen-2.5VL-32B using an optimized reasoning syntax (QMSA). The resulting model achieves 78.6% accuracy, only 1.0% below Gemini 2.0 Flash, on our newly released benchmark YKSUniform, which standardizes 1,854 multimodal exam questions across 309 curriculum topics.\n  Our results reveal that data composition and representational syntax play a decisive role in multimodal reasoning. This work establishes a data centric framework for advancing open weight vision language models, demonstrating that carefully curated and curriculum-grounded multimodal data can elevate supervised fine-tuning to near state-of-the-art performance.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u5728\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u63a5\u8fd1\u6700\u5148\u8fdb\u6c34\u5e73\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u7684\u7814\u7a76\u66f4\u591a\u96c6\u4e2d\u5728\u7b97\u6cd5\u5c42\u9762\uff0c\u800c\u5bf9\u6570\u636e\u57fa\u7840\u7684\u63a2\u7d22\u4e0d\u8db3\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b1.614\u4ebftoken\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5e76\u4f7f\u7528\u4f18\u5316\u7684\u63a8\u7406\u8bed\u6cd5\uff08QMSA\uff09\u5bf9Qwen-2.5VL-32B\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u8be5\u6a21\u578b\u5728YKSUniform\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e8678.6%\u7684\u51c6\u786e\u7387\uff0c\u4ec5\u6bd4Gemini 2.0 Flash\u4f4e1.0%\u3002", "conclusion": "\u6570\u636e\u6784\u6210\u548c\u8868\u793a\u8bed\u6cd5\u5728\u591a\u6a21\u6001\u63a8\u7406\u4e2d\u8d77\u7740\u51b3\u5b9a\u6027\u4f5c\u7528\uff0c\u7cbe\u5fc3\u7b56\u5212\u7684\u591a\u6a21\u6001\u6570\u636e\u53ef\u4ee5\u5c06\u76d1\u7763\u5fae\u8c03\u63d0\u5347\u5230\u63a5\u8fd1\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2512.00596", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00596", "abs": "https://arxiv.org/abs/2512.00596", "authors": ["Jiahao Tian", "Zhenkai Wang"], "title": "DLRREC: Denoising Latent Representations via Multi-Modal Knowledge Fusion in Deep Recommender Systems", "comment": null, "summary": "Modern recommender systems struggle to effectively utilize the rich, yet high-dimensional and noisy, multi-modal features generated by Large Language Models (LLMs). Treating these features as static inputs decouples them from the core recommendation task. We address this limitation with a novel framework built on a key insight: deeply fusing multi-modal and collaborative knowledge for representation denoising. Our unified architecture introduces two primary technical innovations. First, we integrate dimensionality reduction directly into the recommendation model, enabling end-to-end co-training that makes the reduction process aware of the final ranking objective. Second, we introduce a contrastive learning objective that explicitly incorporates the collaborative filtering signal into the latent space. This synergistic process refines raw LLM embeddings, filtering noise while amplifying task-relevant signals. Extensive experiments confirm our method's superior discriminative power, proving that this integrated fusion and denoising strategy is critical for achieving state-of-the-art performance. Our work provides a foundational paradigm for effectively harnessing LLMs in recommender systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u878d\u5408\u591a\u6a21\u6001\u548c\u534f\u540c\u77e5\u8bc6\u6765\u8fdb\u884c\u8868\u5f81\u53bb\u566a\uff0c\u4ee5\u89e3\u51b3\u63a8\u8350\u7cfb\u7edf\u4e2dLLM\u7279\u5f81\u5229\u7528\u4e0d\u5145\u5206\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3\u63a8\u8350\u7cfb\u7edf\u96be\u4ee5\u6709\u6548\u5229\u7528LLM\u751f\u6210\u7684\u9ad8\u7ef4\u566a\u58f0\u591a\u6a21\u6001\u7279\u5f81\uff0c\u4e14\u9759\u6001\u5904\u7406\u8fd9\u4e9b\u7279\u5f81\u4f7f\u5176\u4e0e\u6838\u5fc3\u63a8\u8350\u4efb\u52a1\u8131\u8282\u3002", "method": "\u8be5\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u521b\u65b0\uff1a\u4e00\u662f\u5c06\u964d\u7ef4\u76f4\u63a5\u6574\u5408\u5230\u63a8\u8350\u6a21\u578b\u4e2d\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u8054\u5408\u8bad\u7ec3\uff1b\u4e8c\u662f\u5f15\u5165\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\uff0c\u5c06\u534f\u540c\u8fc7\u6ee4\u4fe1\u53f7\u663e\u5f0f\u5730\u878d\u5165\u6f5c\u5728\u7a7a\u95f4\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u5177\u6709\u5353\u8d8a\u7684\u533a\u5206\u80fd\u529b\uff0c\u8868\u660e\u96c6\u6210\u878d\u5408\u548c\u53bb\u566a\u7b56\u7565\u5bf9\u4e8e\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u6709\u6548\u5229\u7528LLM\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u7840\u8303\u4f8b\u3002"}}
{"id": "2512.01092", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.01092", "abs": "https://arxiv.org/abs/2512.01092", "authors": ["Sofia Sideri", "Georgia Troullinou", "Elisjana Ymeralli", "Vasilis Efthymiou", "Dimitris Plexousakis", "Haridimos Kondylakis"], "title": "PG-HIVE: Hybrid Incremental Schema Discovery for Property Graphs", "comment": null, "summary": "Property graphs have rapidly become the de facto standard for representing and managing complex, interconnected data, powering applications across domains from knowledge graphs to social networks. Despite the advantages, their schema-free nature poses major challenges for integration, exploration, visualization, and efficient querying. To bridge this gap, we present PG-HIVE, a novel framework for automatic schema discovery in property graphs. PG-HIVE goes beyond existing approaches by uncovering latent node and edge types, inferring property datatypes, constraints, and cardinalities, and doing so even in the absence of explicit labeling information. Leveraging a unique combination of Locality-Sensitive Hashing with property- and label-based clustering, PG-HIVE identifies structural similarities at scale. Moreover, it introduces incremental schema discovery, eliminating costly recomputation as new data arrives. Through extensive experimentation, we demonstrate that PG-HIVE consistently outperforms state-of-the-art solutions, in both accuracy (by up to 65% for nodes and 40% for edges), and efficiency (up to 1.95x faster execution), unlocking the full potential of schema-aware property graph management.", "AI": {"tldr": "PG-HIVE: A framework for automatic schema discovery in property graphs.", "motivation": "Schema-free property graphs have challenges for integration, exploration, visualization, and efficient querying.", "method": "Locality-Sensitive Hashing with property- and label-based clustering, identifies structural similarities at scale and introduces incremental schema discovery.", "result": "PG-HIVE outperforms state-of-the-art solutions in accuracy (up to 65% for nodes and 40% for edges) and efficiency (up to 1.95x faster execution).", "conclusion": "PG-HIVE unlocks the full potential of schema-aware property graph management."}}
{"id": "2512.00193", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00193", "abs": "https://arxiv.org/abs/2512.00193", "authors": ["Anson Ho", "Jean-Stanislas Denain", "David Atanasov", "Samuel Albanie", "Rohin Shah"], "title": "A Rosetta Stone for AI Benchmarks", "comment": null, "summary": "Most AI benchmarks saturate within years or even months after they are introduced, making it hard to study long-run trends in AI capabilities. To address this challenge, we build a statistical framework that stitches benchmarks together, putting model capabilities and benchmark difficulties on a single numerical scale. This acts as a \"Rosetta Stone\", allowing us to compare models across a wide range of abilities and time, even if they are not evaluated on the same benchmarks. Moreover, this works without assuming how capabilities evolve across time or with training compute. We demonstrate three applications of this framework. First, we use it to measure the speed of AI progress over time, and to forecast future AI capabilities. Second, we estimate the rate of improvements in algorithmic efficiency, finding estimates that are higher, but broadly consistent with prior work. Finally, we find that our approach can be used to detect rapid accelerations in AI progress.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u8ba1\u6846\u67b6\uff0c\u5c06\u4e0d\u540c\u7684AI\u57fa\u51c6\u7edf\u4e00\u5230\u4e00\u4e2a\u6570\u503c\u5c3a\u5ea6\u4e0a\uff0c\u4ee5\u4fbf\u8de8\u65f6\u95f4\u548c\u80fd\u529b\u8303\u56f4\u6bd4\u8f83\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684AI\u57fa\u51c6\u5f88\u5feb\u9971\u548c\uff0c\u96be\u4ee5\u7814\u7a76AI\u80fd\u529b\u7684\u957f\u671f\u8d8b\u52bf\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u7edf\u8ba1\u6846\u67b6\uff0c\u5c06\u6a21\u578b\u80fd\u529b\u548c\u57fa\u51c6\u96be\u5ea6\u653e\u5728\u4e00\u4e2a\u5171\u540c\u7684\u6570\u503c\u5c3a\u5ea6\u4e0a\uff0c\u65e0\u9700\u5047\u8bbe\u80fd\u529b\u968f\u65f6\u95f4\u548c\u8ba1\u7b97\u91cf\u7684\u6f14\u53d8\u65b9\u5f0f\u3002", "result": "1. \u6d4b\u91cf\u4e86AI\u968f\u65f6\u95f4\u63a8\u79fb\u7684\u8fdb\u6b65\u901f\u5ea6\uff0c\u5e76\u9884\u6d4b\u4e86\u672a\u6765\u7684AI\u80fd\u529b\u30022. \u8bc4\u4f30\u4e86\u7b97\u6cd5\u6548\u7387\u7684\u6539\u8fdb\u7387\u30023. \u53ef\u4ee5\u7528\u6765\u68c0\u6d4bAI\u8fdb\u6b65\u7684\u5feb\u901f\u52a0\u901f\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u5f25\u5408\u4e0d\u540c\u57fa\u51c6\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5b9e\u73b0\u8de8\u6a21\u578b\u548c\u65f6\u95f4\u7684\u6bd4\u8f83\uff0c\u5e76\u5e94\u7528\u4e8e\u8861\u91cfAI\u8fdb\u5c55\u548c\u68c0\u6d4b\u52a0\u901f\u3002"}}
{"id": "2512.00204", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00204", "abs": "https://arxiv.org/abs/2512.00204", "authors": ["Jason Lunder"], "title": "Tree Matching Networks for Natural Language Inference: Parameter-Efficient Semantic Understanding via Dependency Parse Trees", "comment": "16 pages, preprint", "summary": "In creating sentence embeddings for Natural Language Inference (NLI) tasks, using transformer-based models like BERT leads to high accuracy, but require hundreds of millions of parameters. These models take in sentences as a sequence of tokens, and learn to encode the meaning of the sequence into embeddings such that those embeddings can be used reliably for NLI tasks. Essentially, every word is considered against every other word in the sequence, and the transformer model is able to determine the relationships between them, entirely from scratch. However, a model that accepts explicit linguistic structures like dependency parse trees may be able to leverage prior encoded information about these relationships, without having to learn them from scratch, thus improving learning efficiency. To investigate this, we adapt Graph Matching Networks (GMN) to operate on dependency parse trees, creating Tree Matching Networks (TMN). We compare TMN to a BERT based model on the SNLI entailment task and on the SemEval similarity task. TMN is able to achieve significantly better results with a significantly reduced memory footprint and much less training time than the BERT based model on the SNLI task, while both models struggled to preform well on the SemEval. Explicit structural representations significantly outperform sequence-based models at comparable scales, but current aggregation methods limit scalability. We propose multi-headed attention aggregation to address this limitation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6811\u5339\u914d\u7f51\u7edc\uff08TMN\uff09\u7684\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5229\u7528\u4f9d\u8d56\u89e3\u6790\u6811\u8fdb\u884c\u81ea\u7136\u8bed\u8a00\u63a8\u7406\uff08NLI\uff09\u4efb\u52a1\u3002", "motivation": "\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff08\u5982BERT\uff09\u5728NLI\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u53c2\u6570\u91cf\u5de8\u5927\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5229\u7528\u663e\u5f0f\u8bed\u8a00\u7ed3\u6784\uff08\u5982\u4f9d\u8d56\u89e3\u6790\u6811\uff09\u80fd\u5426\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u3002", "method": "\u672c\u6587\u4fee\u6539\u4e86\u56fe\u5339\u914d\u7f51\u7edc\uff08GMN\uff09\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u4f9d\u8d56\u89e3\u6790\u6811\uff0c\u4ece\u800c\u521b\u5efa\u4e86\u6811\u5339\u914d\u7f51\u7edc\uff08TMN\uff09\u3002", "result": "\u5728SNLI\u8574\u542b\u4efb\u52a1\u4e0a\uff0cTMN\u4ee5\u66f4\u5c0f\u7684\u5185\u5b58\u5360\u7528\u548c\u66f4\u77ed\u7684\u8bad\u7ec3\u65f6\u95f4\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u4e8eBERT\u7684\u6a21\u578b\u3002\u4f46\u5728SemEval\u76f8\u4f3c\u5ea6\u4efb\u52a1\u4e0a\uff0c\u4e24\u4e2a\u6a21\u578b\u8868\u73b0\u5747\u4e0d\u4f73\u3002", "conclusion": "\u663e\u5f0f\u7ed3\u6784\u5316\u8868\u793a\u5728\u76f8\u5f53\u7684\u89c4\u6a21\u4e0b\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u5e8f\u5217\u7684\u6a21\u578b\uff0c\u4f46\u5f53\u524d\u7684\u805a\u5408\u65b9\u6cd5\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e86\u591a\u5934\u6ce8\u610f\u529b\u805a\u5408\u6765\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\u3002"}}
{"id": "2512.00170", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.00170", "abs": "https://arxiv.org/abs/2512.00170", "authors": ["Colin Doumont", "Donney Fan", "Natalie Maus", "Jacob R. Gardner", "Henry Moss", "Geoff Pleiss"], "title": "We Still Don't Understand High-Dimensional Bayesian Optimization", "comment": null, "summary": "High-dimensional spaces have challenged Bayesian optimization (BO). Existing methods aim to overcome this so-called curse of dimensionality by carefully encoding structural assumptions, from locality to sparsity to smoothness, into the optimization procedure. Surprisingly, we demonstrate that these approaches are outperformed by arguably the simplest method imaginable: Bayesian linear regression. After applying a geometric transformation to avoid boundary-seeking behavior, Gaussian processes with linear kernels match state-of-the-art performance on tasks with 60- to 6,000-dimensional search spaces. Linear models offer numerous advantages over their non-parametric counterparts: they afford closed-form sampling and their computation scales linearly with data, a fact we exploit on molecular optimization tasks with > 20,000 observations. Coupled with empirical analyses, our results suggest the need to depart from past intuitions about BO methods in high-dimensional spaces.", "AI": {"tldr": "\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u9762\u4e34\u6311\u6218\uff0c\u4f46\u7b80\u5355\u7684\u8d1d\u53f6\u65af\u7ebf\u6027\u56de\u5f52\u65b9\u6cd5\uff0c\u5728\u7ecf\u8fc7\u51e0\u4f55\u53d8\u6362\u540e\uff0c\u53ef\u4ee5\u4e0e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u3002", "motivation": "\u73b0\u6709\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u9762\u4e34\u7ef4\u5ea6\u707e\u96be\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u7ecf\u8fc7\u51e0\u4f55\u53d8\u6362\u7684\u3001\u5177\u6709\u7ebf\u6027\u6838\u7684\u9ad8\u65af\u8fc7\u7a0b\u8fdb\u884c\u8d1d\u53f6\u65af\u4f18\u5316\u3002", "result": "\u8be5\u65b9\u6cd5\u572860\u52306000\u7ef4\u7684\u641c\u7d22\u7a7a\u95f4\u4e2d\uff0c\u4e0e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53\uff0c\u5e76\u4e14\u5728\u8d85\u8fc720000\u4e2a\u89c2\u6d4b\u503c\u7684\u5206\u5b50\u4f18\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\uff0c\u8fc7\u53bb\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\u53ef\u80fd\u9700\u8981\u91cd\u65b0\u8bc4\u4f30\u3002"}}
{"id": "2512.00060", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00060", "abs": "https://arxiv.org/abs/2512.00060", "authors": ["Abdolazim Rezaei", "Mehdi Sookhak"], "title": "PEFT-DML: Parameter-Efficient Fine-Tuning Deep Metric Learning for Robust Multi-Modal 3D Object Detection in Autonomous Driving", "comment": null, "summary": "This study introduces PEFT-DML, a parameter-efficient deep metric learning framework for robust multi-modal 3D object detection in autonomous driving. Unlike conventional models that assume fixed sensor availability, PEFT-DML maps diverse modalities (LiDAR, radar, camera, IMU, GNSS) into a shared latent space, enabling reliable detection even under sensor dropout or unseen modality class combinations. By integrating Low-Rank Adaptation (LoRA) and adapter layers, PEFT-DML achieves significant training efficiency while enhancing robustness to fast motion, weather variability, and domain shifts. Experiments on benchmarks nuScenes demonstrate superior accuracy.", "AI": {"tldr": "PEFT-DML: A parameter-efficient deep metric learning framework for robust multi-modal 3D object detection.", "motivation": "To address the challenge of reliable 3D object detection in autonomous driving under varying sensor availability and unseen modality combinations.", "method": "Utilizes parameter-efficient techniques like LoRA and adapter layers to map diverse modalities into a shared latent space.", "result": "Achieves superior accuracy on the nuScenes dataset.", "conclusion": "PEFT-DML enhances robustness to sensor dropout, fast motion, weather variability, and domain shifts with improved training efficiency."}}
{"id": "2512.00679", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.00679", "abs": "https://arxiv.org/abs/2512.00679", "authors": ["Yi Zhang", "Yiwen Zhang", "Yu Wang", "Tong Chen", "Hongzhi Yin"], "title": "ProEx: A Unified Framework Leveraging Large Language Model with Profile Extrapolation for Recommendation", "comment": "Accepted by KDD 2026 (First Cycle)", "summary": "The powerful text understanding and generation capabilities of large language models (LLMs) have brought new vitality to general recommendation with implicit feedback. One possible strategy involves generating a unique user (or item) profile from historical interaction data, which is then mapped to a semantic representation in the language space. However, a single-instance profile may be insufficient to comprehensively capture the complex intentions behind a user's interacted items. Moreover, due to the inherent instability of LLMs, a biased or misinterpreted profile could even undermine the original recommendation performance. Consequently, an intuitive solution is to generate multiple profiles for each user (or item), each reflecting a distinct aspect of their characteristics. In light of this, we propose a unified recommendation framework with multi-faceted profile extrapolation (ProEx) in this paper. By leveraging chain-of-thought reasoning, we construct multiple distinct profiles for each user and item. These new profiles are subsequently mapped into semantic vectors, extrapolating from the position of the original profile to explore a broader region of the language space. Subsequently, we introduce the concept of environments, where each environment represents a possible linear combination of all profiles. The differences across environments are minimized to reveal the inherent invariance of user preferences. We apply ProEx to three discriminative methods and three generative methods, and conduct extensive experiments on three datasets. The experimental results demonstrate that ProEx significantly enhances the performance of these base recommendation models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u63a8\u8350\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u591a\u65b9\u9762\u7684\u7528\u6237\u548c\u7269\u54c1\u753b\u50cf\u6765\u63d0\u9ad8\u63a8\u8350\u6027\u80fd\u3002", "motivation": "\u5355\u4e2a\u7528\u6237\u6216\u7269\u54c1\u753b\u50cf\u53ef\u80fd\u65e0\u6cd5\u5145\u5206\u6355\u6349\u7528\u6237\u4ea4\u4e92\u7269\u54c1\u80cc\u540e\u590d\u6742\u610f\u56fe\uff0c\u4e14\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4e0d\u7a33\u5b9a\u6027\u53ef\u80fd\u635f\u5bb3\u539f\u59cb\u63a8\u8350\u6027\u80fd\u3002", "method": "\u5229\u7528\u601d\u7ef4\u94fe\u63a8\u7406\u4e3a\u6bcf\u4e2a\u7528\u6237\u548c\u7269\u54c1\u6784\u5efa\u591a\u4e2a\u4e0d\u540c\u7684\u753b\u50cf\uff0c\u5e76\u5c06\u8fd9\u4e9b\u753b\u50cf\u6620\u5c04\u5230\u8bed\u4e49\u5411\u91cf\uff0c\u901a\u8fc7\u4ece\u539f\u59cb\u753b\u50cf\u7684\u4f4d\u7f6e\u63a8\u65ad\u6765\u63a2\u7d22\u66f4\u5e7f\u6cdb\u7684\u8bed\u8a00\u7a7a\u95f4\u3002\u5f15\u5165\u4e86\u73af\u5883\u7684\u6982\u5ff5\uff0c\u6bcf\u4e2a\u73af\u5883\u4ee3\u8868\u6240\u6709\u753b\u50cf\u7684\u53ef\u80fd\u7ebf\u6027\u7ec4\u5408\uff0c\u5e76\u6700\u5c0f\u5316\u73af\u5883\u4e4b\u95f4\u7684\u5dee\u5f02\u4ee5\u63ed\u793a\u7528\u6237\u504f\u597d\u7684\u5185\u5728\u4e0d\u53d8\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cProEx \u663e\u8457\u63d0\u9ad8\u4e86\u8fd9\u4e9b\u57fa\u672c\u63a8\u8350\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "ProEx \u662f\u4e00\u79cd\u6709\u6548\u7684\u63a8\u8350\u6846\u67b6\uff0c\u53ef\u4ee5\u901a\u8fc7\u751f\u6210\u591a\u65b9\u9762\u7684\u7528\u6237\u548c\u7269\u54c1\u753b\u50cf\u6765\u63d0\u9ad8\u63a8\u8350\u6027\u80fd\u3002"}}
{"id": "2512.01490", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.01490", "abs": "https://arxiv.org/abs/2512.01490", "authors": ["Marius Ottosen", "Magnus Keinicke Parlo", "Philippe Bonnet"], "title": "DuckDB on xNVMe", "comment": null, "summary": "DuckDB is designed for portability. It is also designed to run anywhere, and possibly in contexts where it can be specialized for performance, e.g., as a cloud service or on a smart device. In this paper, we consider the way DuckDB interacts with local storage. Our long term research question is whether and how SSDs could be co-designed with DuckDB. As a first step towards vertical integration of DuckDB and programmable SSDs, we consider whether and how DuckDB can access NVMe SSDs directly. By default, DuckDB relies on the POSIX file interface. In contrast, we rely on the xNVMe library and explore how it can be leveraged in DuckDB. We leverage the block-based nature of the DuckDB buffer manager to bypass the synchronous POSIX I/O interface, the file system and the block manager. Instead, we directly issue asynchronous I/Os against the SSD logical block address space. Our preliminary experimental study compares different ways to manage asynchronous I/Os atop xNVMe. The speed-up we observe over the DuckDB baseline is significant, even for the simplest scan query over a TPC-H table. As expected, the speed-up increases with the scale factor, and the Linux NVMe passthru improves performance. Future work includes a more thorough experimental study, a flexible solution that combines raw NVMe access and legacy POSIX file interface as well the co-design of DuckDB and SSDs.", "AI": {"tldr": "DuckDB\u901a\u8fc7\u76f4\u63a5\u8bbf\u95eeNVMe SSD\u6765\u4f18\u5316\u6027\u80fd\uff0c\u7ed5\u8fc7POSIX\u63a5\u53e3\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u4e0eSSD\u534f\u540c\u8bbe\u8ba1DuckDB\uff0c\u63d0\u9ad8\u5176\u5728\u4e91\u670d\u52a1\u6216\u667a\u80fd\u8bbe\u5907\u7b49\u73af\u5883\u4e0b\u7684\u6027\u80fd\u3002", "method": "\u5229\u7528xNVMe\u5e93\uff0c\u7ed5\u8fc7POSIX I/O\u63a5\u53e3\uff0c\u76f4\u63a5\u5411SSD\u903b\u8f91\u5757\u5730\u5740\u7a7a\u95f4\u53d1\u51fa\u5f02\u6b65I/O\u8bf7\u6c42\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u662f\u6700\u7b80\u5355\u7684TPC-H\u8868\u626b\u63cf\u67e5\u8be2\uff0c\u4e5f\u6bd4DuckDB\u57fa\u7ebf\u6709\u663e\u8457\u7684\u901f\u5ea6\u63d0\u5347\u3002", "conclusion": "\u76f4\u63a5\u8bbf\u95eeNVMe SSD\u80fd\u663e\u8457\u63d0\u5347DuckDB\u6027\u80fd\uff0c\u672a\u6765\u7684\u5de5\u4f5c\u5305\u62ec\u66f4\u5f7b\u5e95\u7684\u5b9e\u9a8c\u7814\u7a76\u4ee5\u53caDuckDB\u548cSSD\u7684\u534f\u540c\u8bbe\u8ba1\u3002"}}
{"id": "2512.00218", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.00218", "abs": "https://arxiv.org/abs/2512.00218", "authors": ["Matt MacDermott", "Qiyao Wei", "Rada Djoneva", "Francis Rhys Ward"], "title": "Reasoning Under Pressure: How do Training Incentives Influence Chain-of-Thought Monitorability?", "comment": null, "summary": "AI systems that output their reasoning in natural language offer an opportunity for safety -- we can \\emph{monitor} their chain of thought (CoT) for undesirable reasoning, such as the pursuit of harmful objectives. However, the extent to which CoT faithfully reflects the underlying reasoning process, and hence the extent to which it can be usefully monitored, may be influenced by certain aspects of training. We investigate how different \\emph{training incentives}, applied to a reasoning model, affect its monitorability. We introduce a novel methodology for measuring monitorability according to whether a monitor can predict a key latent variable using the model's reasoning. When controlling for accuracy, we do not find evidence for consistent effects from commonly used incentives (length penalties and KL regularisation), but we find that adversarial optimisation (penalising monitor accuracy) degrades monitor performance, while direct optimisation for monitorability does not reliably lead to improvements. Our code is available at https://github.com/QiyaoWei/reasoning-under-pressure.", "AI": {"tldr": "\u7814\u7a76\u4e86\u8bad\u7ec3\u6fc0\u52b1\u5982\u4f55\u5f71\u54cd AI \u7cfb\u7edf\u63a8\u7406\u8fc7\u7a0b\u7684\u53ef\u76d1\u63a7\u6027\uff0c\u53d1\u73b0\u5bf9\u6297\u4f18\u5316\u4f1a\u964d\u4f4e\u76d1\u63a7\u6027\u80fd\uff0c\u800c\u76f4\u63a5\u4f18\u5316\u53ef\u76d1\u63a7\u6027\u5e76\u4e0d\u4e00\u5b9a\u80fd\u5e26\u6765\u6539\u8fdb\u3002", "motivation": "\u63a2\u8ba8\u4e86\u901a\u8fc7\u76d1\u63a7 AI \u7cfb\u7edf\u7684\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u8fc7\u7a0b\u6765\u786e\u4fdd\u5b89\u5168\u6027\u7684\u673a\u4f1a\uff0c\u5e76\u7814\u7a76\u4e86\u8bad\u7ec3\u4e2d\u7684\u67d0\u4e9b\u56e0\u7d20\u5982\u4f55\u5f71\u54cd\u63a8\u7406\u8fc7\u7a0b\u7684\u53ef\u76d1\u63a7\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u8861\u91cf\u53ef\u76d1\u63a7\u6027\uff0c\u5373\u76d1\u63a7\u5668\u662f\u5426\u53ef\u4ee5\u4f7f\u7528\u6a21\u578b\u7684\u63a8\u7406\u6765\u9884\u6d4b\u5173\u952e\u6f5c\u5728\u53d8\u91cf\u3002", "result": "\u5728\u63a7\u5236\u51c6\u786e\u7387\u7684\u60c5\u51b5\u4e0b\uff0c\u5e38\u7528\u6fc0\u52b1\uff08\u957f\u5ea6\u60e9\u7f5a\u548c KL \u6b63\u5219\u5316\uff09\u5bf9\u53ef\u76d1\u63a7\u6027\u6ca1\u6709\u4e00\u81f4\u7684\u5f71\u54cd\uff0c\u5bf9\u6297\u4f18\u5316\u4f1a\u964d\u4f4e\u76d1\u63a7\u6027\u80fd\uff0c\u800c\u76f4\u63a5\u4f18\u5316\u53ef\u76d1\u63a7\u6027\u5e76\u4e0d\u4e00\u5b9a\u80fd\u5e26\u6765\u6539\u8fdb\u3002", "conclusion": "\u8bad\u7ec3\u6fc0\u52b1\u4f1a\u5f71\u54cd AI \u7cfb\u7edf\u7684\u63a8\u7406\u8fc7\u7a0b\u7684\u53ef\u76d1\u63a7\u6027\uff0c\u5bf9\u6297\u4f18\u5316\u4f1a\u964d\u4f4e\u76d1\u63a7\u6027\u80fd\uff0c\u800c\u76f4\u63a5\u4f18\u5316\u53ef\u76d1\u63a7\u6027\u5e76\u4e0d\u4e00\u5b9a\u80fd\u5e26\u6765\u6539\u8fdb\u3002"}}
{"id": "2512.00214", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00214", "abs": "https://arxiv.org/abs/2512.00214", "authors": ["Matej Klemen", "Tja\u0161a Ar\u010don", "Luka Ter\u010don", "Marko Robnik-\u0160ikonja", "Kaja Dobrovoljc"], "title": "Towards Corpus-Grounded Agentic LLMs for Multilingual Grammatical Analysis", "comment": "Pre-print, submission under review", "summary": "Empirical grammar research has become increasingly data-driven, but the systematic analysis of annotated corpora still requires substantial methodological and technical effort. We explore how agentic large language models (LLMs) can streamline this process by reasoning over annotated corpora and producing interpretable, data-grounded answers to linguistic questions. We introduce an agentic framework for corpus-grounded grammatical analysis that integrates concepts such as natural-language task interpretation, code generation, and data-driven reasoning. As a proof of concept, we apply it to Universal Dependencies (UD) corpora, testing it on multilingual grammatical tasks inspired by the World Atlas of Language Structures (WALS). The evaluation spans 13 word-order features and over 170 languages, assessing system performance across three complementary dimensions - dominant-order accuracy, order-coverage completeness, and distributional fidelity - which reflect how well the system generalizes, identifies, and quantifies word-order variations. The results demonstrate the feasibility of combining LLM reasoning with structured linguistic data, offering a first step toward interpretable, scalable automation of corpus-based grammatical inquiry.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u5206\u6790\u5e26\u6ce8\u91ca\u7684\u8bed\u6599\u5e93\uff0c\u5e76\u4e3a\u8bed\u8a00\u95ee\u9898\u751f\u6210\u53ef\u89e3\u91ca\u7684\u3001\u6570\u636e\u9a71\u52a8\u7684\u7b54\u6848\uff0c\u4ece\u800c\u7b80\u5316\u7ecf\u9a8c\u8bed\u6cd5\u7814\u7a76\u3002", "motivation": "\u7cfb\u7edf\u5730\u5206\u6790\u5e26\u6ce8\u91ca\u7684\u8bed\u6599\u5e93\u4ecd\u7136\u9700\u8981\u5927\u91cf\u7684\u65b9\u6cd5\u548c\u6280\u672f\u5de5\u4f5c\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8bed\u6599\u5e93\u7684\u8bed\u6cd5\u5206\u6790\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u89e3\u91ca\u3001\u4ee3\u7801\u751f\u6210\u548c\u6570\u636e\u9a71\u52a8\u63a8\u7406\u7b49\u6982\u5ff5\u3002\u6211\u4eec\u5c06\u5176\u5e94\u7528\u4e8e\u901a\u7528\u4f9d\u5b58\u5173\u7cfb (UD) \u8bed\u6599\u5e93\uff0c\u5e76\u5728\u53d7\u4e16\u754c\u8bed\u8a00\u7ed3\u6784\u56fe\u96c6 (WALS) \u542f\u53d1\u7684\u8bed\u8a00\u8bed\u6cd5\u4efb\u52a1\u4e0a\u5bf9\u5176\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u8be5\u8bc4\u4f30\u6db5\u76d6 13 \u4e2a\u8bcd\u5e8f\u7279\u5f81\u548c 170 \u591a\u79cd\u8bed\u8a00\uff0c\u8bc4\u4f30\u7cfb\u7edf\u5728\u4e09\u4e2a\u4e92\u8865\u7ef4\u5ea6\u4e0a\u7684\u6027\u80fd\u2014\u2014\u4e3b\u8981\u987a\u5e8f\u51c6\u786e\u6027\u3001\u987a\u5e8f\u8986\u76d6\u5b8c\u6574\u6027\u548c\u5206\u5e03\u4fdd\u771f\u5ea6\u2014\u2014\u53cd\u6620\u4e86\u7cfb\u7edf\u6982\u62ec\u3001\u8bc6\u522b\u548c\u91cf\u5316\u8bcd\u5e8f\u53d8\u4f53\u7684\u80fd\u529b\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e0e\u7ed3\u6784\u5316\u8bed\u8a00\u6570\u636e\u76f8\u7ed3\u5408\u662f\u53ef\u884c\u7684\uff0c\u8fd9\u4e3a\u57fa\u4e8e\u8bed\u6599\u5e93\u7684\u8bed\u6cd5\u67e5\u8be2\u7684\u53ef\u89e3\u91ca\u3001\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u8fc8\u51fa\u4e86\u7b2c\u4e00\u6b65\u3002"}}
{"id": "2512.00181", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.00181", "abs": "https://arxiv.org/abs/2512.00181", "authors": ["Mohamed Bouadi", "Pratinav Seth", "Aditya Tanna", "Vinay Kumar Sankarapu"], "title": "Orion-Bix: Bi-Axial Attention for Tabular In-Context Learning", "comment": null, "summary": "Tabular data drive most real-world machine learning applications, yet building general-purpose models for them remains difficult. Mixed numeric and categorical fields, weak feature structure, and limited labeled data make scaling and generalization challenging. To this end, we introduce Orion-Bix, a tabular foundation model that combines biaxial attention with meta-learned in-context reasoning for few-shot tabular learning. Its encoder alternates standard, grouped, hierarchical, and relational attention, fusing their outputs through multi-CLS summarization to capture both local and global dependencies efficiently. A label-aware ICL head adapts on the fly and scales to large label spaces via hierarchical decision routing. Meta-trained on synthetically generated, structurally diverse tables with causal priors, Orion-Bix learns transferable inductive biases across heterogeneous data. Delivered as a scikit-learn compatible foundation model, it outperforms gradient-boosting baselines and remains competitive with state-of-the-art tabular foundation models on public benchmarks, showing that biaxial attention with episodic meta-training enables robust, few-shot-ready tabular learning. The model is publicly available at https://github.com/Lexsi-Labs/Orion-BiX .", "AI": {"tldr": "Orion-Bix is a tabular foundation model using biaxial attention and meta-learning for few-shot tabular learning.", "motivation": "Building general-purpose models for tabular data is difficult due to mixed data types, weak feature structure, and limited labeled data.", "method": "The model combines biaxial attention with meta-learned in-context reasoning. Its encoder alternates different attention mechanisms and uses multi-CLS summarization. A label-aware ICL head adapts on the fly.", "result": "Orion-Bix outperforms gradient-boosting baselines and is competitive with state-of-the-art tabular foundation models.", "conclusion": "Biaxial attention with episodic meta-training enables robust, few-shot-ready tabular learning for tabular data."}}
{"id": "2512.00061", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00061", "abs": "https://arxiv.org/abs/2512.00061", "authors": ["Pouya Shiri", "Amirali Baniasadi"], "title": "DL-CapsNet: A Deep and Light Capsule Network", "comment": null, "summary": "Capsule Network (CapsNet) is among the promising classifiers and a possible successor of the classifiers built based on Convolutional Neural Network (CNN). CapsNet is more accurate than CNNs in detecting images with overlapping categories and those with applied affine transformations. In this work, we propose a deep variant of CapsNet consisting of several capsule layers. In addition, we design the Capsule Summarization layer to reduce the complexity by reducing the number of parameters. DL-CapsNet, while being highly accurate, employs a small number of parameters and delivers faster training and inference. DL-CapsNet can process complex datasets with a high number of categories.", "AI": {"tldr": "CapsNet\u662fCNN\u7684\u4e00\u79cd\u6709\u524d\u666f\u7684\u5206\u7c7b\u5668\uff0c\u5728\u68c0\u6d4b\u5177\u6709\u91cd\u53e0\u7c7b\u522b\u548c\u4eff\u5c04\u53d8\u6362\u7684\u56fe\u50cf\u65f6\uff0cCapsNet\u6bd4CNN\u66f4\u51c6\u786e\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7531\u591a\u4e2a\u80f6\u56ca\u5c42\u7ec4\u6210\u7684CapsNet\u7684\u6df1\u5ea6\u53d8\u4f53\u3002", "motivation": "CapsNet\u6bd4CNN\u66f4\u51c6\u786e\uff0c\u53ef\u4ee5\u68c0\u6d4b\u5177\u6709\u91cd\u53e0\u7c7b\u522b\u548c\u4eff\u5c04\u53d8\u6362\u7684\u56fe\u50cf\u3002", "method": "\u8bbe\u8ba1\u4e86Capsule Summarization\u5c42\u6765\u51cf\u5c11\u53c2\u6570\u6570\u91cf\uff0c\u4ece\u800c\u964d\u4f4e\u590d\u6742\u6027\u3002", "result": "DL-CapsNet\u5728\u9ad8\u5ea6\u51c6\u786e\u7684\u540c\u65f6\uff0c\u4f7f\u7528\u5c11\u91cf\u7684\u53c2\u6570\uff0c\u5e76\u63d0\u4f9b\u66f4\u5feb\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u3002", "conclusion": "DL-CapsNet\u53ef\u4ee5\u5904\u7406\u5177\u6709\u5927\u91cf\u7c7b\u522b\u7684\u590d\u6742\u6570\u636e\u96c6\u3002"}}
{"id": "2512.00772", "categories": ["cs.IR", "cs.AI", "cs.DL"], "pdf": "https://arxiv.org/pdf/2512.00772", "abs": "https://arxiv.org/abs/2512.00772", "authors": ["Hyunseok Ryu", "Wonjune Shin", "Hyun Park"], "title": "SHRAG: AFrameworkfor Combining Human-Inspired Search with RAG", "comment": "10 pages, 4 figures, 1 table, 1 algorithm, 3 prompts", "summary": "Retrieval-Augmented Generation (RAG) is gaining recognition as one of the key technological axes for next generation information retrieval, owing to its ability to mitigate the hallucination phenomenon in Large Language\n  Models (LLMs)and effectively incorporate up-to-date information. However, specialized expertise is necessary to\n  construct ahigh-quality retrieval system independently; moreover, RAGdemonstratesrelativelyslowerprocessing\n  speeds compared to conventional pure retrieval systems because it involves both retrieval and generation stages.\n  Accordingly, this study proposes SHRAG, a novel framework designed to facilitate the seamless integration of\n  Information Retrieval and RAG while simultaneously securing precise retrieval performance. SHRAG utilizes a\n  Large Language Model as a Query Strategist to automatically transform unstructured natural language queries\n  into logically structured search queries, subsequently performing Boolean retrieval to emulate the search process\n  of an expert human searcher. Furthermore, it incorporates multilingual query expansion and a multilingual\n  embedding model, enabling it to perform efficient cross-lingual question answering within the multilingual\n  dataset environment of the ScienceON Challenge. Experimental results demonstrate that the proposed method,\n  combining logical retrieval capabilities and generative reasoning, can significantly enhance the accuracy and\n  reliability of RAG systems. Furthermore, SHRAG movesbeyondconventionaldocument-centric retrieval methods,\n  presenting the potential for a new search paradigm capable of providing direct and reliable responses to queries.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSHRAG\u7684\u65b0\u6846\u67b6\uff0c\u65e8\u5728\u4fc3\u8fdb\u4fe1\u606f\u68c0\u7d22\u548cRAG\u7684\u65e0\u7f1d\u96c6\u6210\uff0c\u540c\u65f6\u786e\u4fdd\u7cbe\u786e\u7684\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "RAG\u4f5c\u4e3a\u4e0b\u4e00\u4ee3\u4fe1\u606f\u68c0\u7d22\u7684\u5173\u952e\u6280\u672f\u8f74\uff0c\u56e0\u5176\u80fd\u591f\u51cf\u8f7b\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\u5e76\u6709\u6548\u6574\u5408\u6700\u65b0\u4fe1\u606f\u800c\u53d7\u5230\u8ba4\u53ef\u3002\u7136\u800c\uff0c\u72ec\u7acb\u6784\u5efa\u9ad8\u8d28\u91cf\u7684\u68c0\u7d22\u7cfb\u7edf\u9700\u8981\u4e13\u95e8\u7684\u4e13\u4e1a\u77e5\u8bc6\uff1b\u6b64\u5916\uff0c\u4e0e\u4f20\u7edf\u7684\u7eaf\u68c0\u7d22\u7cfb\u7edf\u76f8\u6bd4\uff0cRAG\u7684\u5904\u7406\u901f\u5ea6\u76f8\u5bf9\u8f83\u6162\uff0c\u56e0\u4e3a\u5b83\u6d89\u53ca\u68c0\u7d22\u548c\u751f\u6210\u9636\u6bb5\u3002", "method": "SHRAG\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u67e5\u8be2\u7b56\u7565\u5668\uff0c\u4ee5\u81ea\u52a8\u5c06\u975e\u7ed3\u6784\u5316\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u8f6c\u6362\u4e3a\u903b\u8f91\u7ed3\u6784\u5316\u641c\u7d22\u67e5\u8be2\uff0c\u968f\u540e\u6267\u884c\u5e03\u5c14\u68c0\u7d22\u4ee5\u6a21\u62df\u4e13\u5bb6\u4eba\u5de5\u641c\u7d22\u8005\u7684\u641c\u7d22\u8fc7\u7a0b\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u7ed3\u5408\u4e86\u591a\u8bed\u8a00\u67e5\u8be2\u6269\u5c55\u548c\u591a\u8bed\u8a00\u5d4c\u5165\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u591f\u5728ScienceON\u6311\u6218\u8d5b\u7684\u591a\u8bed\u8a00\u6570\u636e\u96c6\u73af\u5883\u4e2d\u6267\u884c\u9ad8\u6548\u7684\u8de8\u8bed\u8a00\u95ee\u9898\u89e3\u7b54\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u7ed3\u5408\u4e86\u903b\u8f91\u68c0\u7d22\u80fd\u529b\u548c\u751f\u6210\u63a8\u7406\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8RAG\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "SHRAG\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u4ee5\u6587\u6863\u4e3a\u4e2d\u5fc3\u7684\u68c0\u7d22\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u4e00\u79cd\u65b0\u7684\u641c\u7d22\u8303\u5f0f\u7684\u6f5c\u529b\uff0c\u8be5\u8303\u5f0f\u80fd\u591f\u4e3a\u67e5\u8be2\u63d0\u4f9b\u76f4\u63a5\u548c\u53ef\u9760\u7684\u54cd\u5e94\u3002"}}
{"id": "2512.01693", "categories": ["cs.DB", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2512.01693", "abs": "https://arxiv.org/abs/2512.01693", "authors": ["Honghui Kim", "Dohoon Kim", "Jihan Kim"], "title": "LLM-Driven Multi-Agent Curation and Expansion of Metal-Organic Frameworks Database", "comment": null, "summary": "Metal-organic framework (MOF) databases have grown rapidly through experimental deposition and large-scale literature extraction, but recent analyses show that nearly half of their entries contain substantial structural errors. These inaccuracies propagate through high-throughput screening and machine-learning workflows, limiting the reliability of data-driven MOF discovery. Correcting such errors is exceptionally difficult because true repairs require integrating crystallographic files, synthesis descriptions, and contextual evidence scattered across the literature. Here we introduce LitMOF, a large language model-driven multi-agent framework that validates crystallographic information directly from the original literature and cross-validates it with database entries to repair structural errors. Applying LitMOF to the experimental MOF database (the CSD MOF Subset), we constructed LitMOF-DB, a curated set 118,464 computation-ready structures, including corrections of 69% (6,161 MOFs) of the invalid MOFs in the latest CoRE MOF database. Additionally, the system uncovered 12,646 experimentally reported MOFs absent from existing resources, substantially expanding the known experimental design space. This work establishes a scalable pathway toward self-correcting scientific databases and a generalizable paradigm for LLM-driven curation in materials science.", "AI": {"tldr": "LitMOF\uff1a\u4e00\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u9a8c\u8bc1\u6676\u4f53\u5b66\u4fe1\u606f\u5e76\u4fee\u590d\u7ed3\u6784\u9519\u8bef\uff0c\u4ece\u800c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b 118,464 \u4e2a\u8ba1\u7b97\u5c31\u7eea\u7ed3\u6784\u7684\u7cbe\u9009\u6570\u636e\u96c6 LitMOF-DB\u3002", "motivation": "\u73b0\u6709 MOF \u6570\u636e\u5e93\u5305\u542b\u5927\u91cf\u7ed3\u6784\u9519\u8bef\uff0c\u5f71\u54cd\u6570\u636e\u9a71\u52a8\u7684 MOF \u53d1\u73b0\u7684\u53ef\u9760\u6027\uff0c\u4e14\u96be\u4ee5\u901a\u8fc7\u6574\u5408\u6587\u732e\u4e2d\u7684\u6676\u4f53\u5b66\u6587\u4ef6\u3001\u5408\u6210\u63cf\u8ff0\u548c\u4e0a\u4e0b\u6587\u8bc1\u636e\u8fdb\u884c\u4fee\u590d\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u591a\u4ee3\u7406\u6846\u67b6 LitMOF\uff0c\u76f4\u63a5\u4ece\u539f\u59cb\u6587\u732e\u4e2d\u9a8c\u8bc1\u6676\u4f53\u5b66\u4fe1\u606f\uff0c\u5e76\u4e0e\u6570\u636e\u5e93\u6761\u76ee\u8fdb\u884c\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u4ee5\u4fee\u590d\u7ed3\u6784\u9519\u8bef\u3002", "result": "LitMOF \u4fee\u590d\u4e86 CoRE MOF \u6570\u636e\u5e93\u4e2d 69% \u7684\u65e0\u6548 MOF\uff086,161 \u4e2a MOF\uff09\uff0c\u5e76\u53d1\u73b0\u4e86\u73b0\u6709\u8d44\u6e90\u4e2d\u7f3a\u5c11\u7684 12,646 \u4e2a\u5b9e\u9a8c\u62a5\u544a MOF\uff0c\u4ece\u800c\u6784\u5efa\u4e86\u5305\u542b 118,464 \u4e2a\u8ba1\u7b97\u5c31\u7eea\u7ed3\u6784\u7684 LitMOF-DB\u3002", "conclusion": "\u8be5\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u6761\u901a\u5f80\u81ea\u6211\u4fee\u6b63\u79d1\u5b66\u6570\u636e\u5e93\u7684\u53ef\u6269\u5c55\u9014\u5f84\uff0c\u4ee5\u53ca\u4e00\u79cd\u5728\u6750\u6599\u79d1\u5b66\u4e2d\u8fdb\u884c LLM \u9a71\u52a8\u7684\u7b56\u5c55\u7684\u901a\u7528\u8303\u4f8b\u3002"}}
{"id": "2512.00267", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00267", "abs": "https://arxiv.org/abs/2512.00267", "authors": ["Anab Maulana Barik", "Shou Ziyi", "Yang Kaiwen", "Yang Qi", "Shen Xin"], "title": "Trification: A Comprehensive Tree-based Strategy Planner and Structural Verification for Fact-Checking", "comment": null, "summary": "Technological advancement allows information to be shared in just a single click, which has enabled the rapid spread of false information. This makes automated fact-checking system necessary to ensure the safety and integrity of our online media ecosystem. Previous methods have demonstrated the effectiveness of decomposing the claim into simpler sub-tasks and utilizing LLM-based multi agent system to execute them. However, those models faces two limitations: they often fail to verify every component in the claim and lack of structured framework to logically connect the results of sub-tasks for a final prediction. In this work, we propose a novel automated fact-checking framework called Trification. Our framework begins by generating a comprehensive set of verification actions to ensure complete coverage of the claim. It then structured these actions into a dependency graph to model the logical interaction between actions. Furthermore, the graph can be dynamically modified, allowing the system to adapt its verification strategy. Experimental results on two challenging benchmarks demonstrate that our framework significantly enhances fact-checking accuracy, thereby advancing current state-of-the-art in automated fact-checking system.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTrification\u7684\u81ea\u52a8\u4e8b\u5b9e\u6838\u67e5\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5168\u9762\u7684\u9a8c\u8bc1\u52a8\u4f5c\u5e76\u5c06\u5176\u6784\u5efa\u4e3a\u4f9d\u8d56\u5173\u7cfb\u56fe\u6765\u63d0\u9ad8\u4e8b\u5b9e\u6838\u67e5\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u9a8c\u8bc1\u58f0\u660e\u4e2d\u7684\u6bcf\u4e2a\u7ec4\u6210\u90e8\u5206\uff0c\u5e76\u4e14\u7f3a\u4e4f\u7ed3\u6784\u5316\u6846\u67b6\u6765\u8fde\u63a5\u5b50\u4efb\u52a1\u7684\u7ed3\u679c\u4ee5\u8fdb\u884c\u6700\u7ec8\u9884\u6d4b\u3002", "method": "\u9996\u5148\u751f\u6210\u4e00\u7ec4\u5168\u9762\u7684\u9a8c\u8bc1\u64cd\u4f5c\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u64cd\u4f5c\u6784\u5efa\u4e3a\u4f9d\u8d56\u5173\u7cfb\u56fe\uff0c\u4ee5\u6a21\u62df\u64cd\u4f5c\u4e4b\u95f4\u7684\u903b\u8f91\u4ea4\u4e92\u3002\u8be5\u56fe\u53ef\u4ee5\u52a8\u6001\u4fee\u6539\uff0c\u4ece\u800c\u5141\u8bb8\u7cfb\u7edf\u8c03\u6574\u5176\u9a8c\u8bc1\u7b56\u7565\u3002", "result": "\u5728\u4e24\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u4e8b\u5b9e\u6838\u67e5\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u9ad8\u4e86\u81ea\u52a8\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u7684\u5f53\u524d\u6280\u672f\u6c34\u5e73\u3002"}}
{"id": "2512.00219", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00219", "abs": "https://arxiv.org/abs/2512.00219", "authors": ["Akhil Rajeev P"], "title": "Minimal-Edit Instruction Tuning for Low-Resource Indic GEC", "comment": "Submitted to AACL-IJCNLP Bhasha Workshop Shared Task1 :GEC", "summary": "Grammatical error correction for Indic languages faces limited supervision, diverse scripts, and rich morphology. We propose an augmentation-free setup that uses instruction-tuned large language models and conservative decoding. A 12B GEMMA 3 model is instruction-tuned in bnb 4-bit precision with parameter-efficient fine-tuning (PEFT) and Alpaca-style formatting. Decoding follows a deterministic, constraint-aware procedure with a lightweight normaliser that encourages minimal, meaning-preserving edits. We operationalise inference, subsequent to instruction fine-tuning (IFT), via a fixed, language-specific prompt directly synthesised from a deterministic error classifier's taxonomy, label distributions, and precedence ordering computed on the training corpus.\n  Under the official untuned GLEU evaluation, the system scores 92.41 on Malayalam, sixth overall, and 81.44 on Hindi, third overall. These results indicate that classifier-informed prompt design, adapter-based instruction tuning, and deterministic decoding provide a reproducible and a computationally efficient alternative to augmentation-centred pipelines for Indic GEC. The approach also motivates future work on stronger morphosyntactic constraints and human-centred evaluation of conservative edits.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5370\u5ea6\u8bed\u8a00\u8bed\u6cd5\u7ea0\u9519\u7684\u65e0\u589e\u5f3a\u8bbe\u7f6e\uff0c\u8be5\u8bbe\u7f6e\u4f7f\u7528\u6307\u4ee4\u8c03\u6574\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u4fdd\u5b88\u89e3\u7801\u3002", "motivation": "\u5370\u5ea6\u8bed\u8a00\u7684\u8bed\u6cd5\u7ea0\u9519\u9762\u4e34\u76d1\u7763\u6709\u9650\u3001\u811a\u672c\u591a\u6837\u548c\u5f62\u6001\u4e30\u5bcc\u7b49\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5177\u6709\u53c2\u6570\u9ad8\u6548\u5fae\u8c03 (PEFT) \u548c Alpaca \u98ce\u683c\u683c\u5f0f\u7684 bnb 4 \u4f4d\u7cbe\u5ea6\u6307\u4ee4\u8c03\u6574\u7684 12B GEMMA 3 \u6a21\u578b\u3002\u89e3\u7801\u9075\u5faa\u786e\u5b9a\u6027\u7684\u3001\u7ea6\u675f\u611f\u77e5\u7684\u7a0b\u5e8f\uff0c\u8be5\u7a0b\u5e8f\u5177\u6709\u8f7b\u91cf\u7ea7\u5f52\u4e00\u5316\u5668\uff0c\u53ef\u9f13\u52b1\u6700\u5c0f\u7684\u3001\u4fdd\u7559\u542b\u4e49\u7684\u7f16\u8f91\u3002\u901a\u8fc7\u4ece\u786e\u5b9a\u6027\u9519\u8bef\u5206\u7c7b\u5668\u7684\u5206\u7c7b\u6cd5\u3001\u6807\u7b7e\u5206\u5e03\u548c\u5728\u8bad\u7ec3\u8bed\u6599\u5e93\u4e0a\u8ba1\u7b97\u7684\u4f18\u5148\u7ea7\u987a\u5e8f\u76f4\u63a5\u5408\u6210\u7684\u56fa\u5b9a\u3001\u7279\u5b9a\u4e8e\u8bed\u8a00\u7684\u63d0\u793a\u6765\u64cd\u4f5c\u6307\u4ee4\u5fae\u8c03 (IFT) \u4e4b\u540e\u7684\u63a8\u7406\u3002", "result": "\u5728\u5b98\u65b9\u672a\u8c03\u6574\u7684 GLEU \u8bc4\u4f30\u4e0b\uff0c\u8be5\u7cfb\u7edf\u5728\u9a6c\u62c9\u96c5\u62c9\u59c6\u8bed\u4e0a\u7684\u5f97\u5206\u4e3a 92.41\uff0c\u603b\u6392\u540d\u7b2c\u516d\uff0c\u5728\u5370\u5730\u8bed\u4e0a\u7684\u5f97\u5206\u4e3a 81.44\uff0c\u603b\u6392\u540d\u7b2c\u4e09\u3002", "conclusion": "\u5206\u7c7b\u5668\u544a\u77e5\u7684\u63d0\u793a\u8bbe\u8ba1\u3001\u57fa\u4e8e\u9002\u914d\u5668\u7684\u6307\u4ee4\u8c03\u6574\u548c\u786e\u5b9a\u6027\u89e3\u7801\u4e3a\u5370\u5ea6 GEC \u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u91cd\u590d\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4ee5\u66ff\u4ee3\u4ee5\u589e\u5f3a\u4e3a\u4e2d\u5fc3\u7684\u7ba1\u9053\u3002\u8be5\u65b9\u6cd5\u8fd8\u6fc0\u53d1\u4e86\u672a\u6765\u5728\u66f4\u5f3a\u7684\u5f62\u6001\u53e5\u6cd5\u7ea6\u675f\u548c\u4ee5\u4eba\u4e3a\u672c\u7684\u4fdd\u5b88\u7f16\u8f91\u8bc4\u4f30\u65b9\u9762\u7684\u5de5\u4f5c\u3002"}}
{"id": "2512.00191", "categories": ["cs.LG", "eess.IV", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2512.00191", "abs": "https://arxiv.org/abs/2512.00191", "authors": ["Jose Luis Lima de Jesus Silva", "Joao Pedro Gomes", "Paulo Roberto de Melo Barros Junior", "Vitor Hugo Serravalle Reis Rodrigues", "Alexsandro Guerra Cerqueira"], "title": "Hybrid Context-Fusion Attention (CFA) U-Net and Clustering for Robust Seismic Horizon Interpretation", "comment": null, "summary": "Interpreting seismic horizons is a critical task for characterizing subsurface structures in hydrocarbon exploration. Recent advances in deep learning, particularly U-Net-based architectures, have significantly improved automated horizon tracking. However, challenges remain in accurately segmenting complex geological features and interpolating horizons from sparse annotations. To address these issues, a hybrid framework is presented that integrates advanced U-Net variants with spatial clustering to enhance horizon continuity and geometric fidelity. The core contribution is the Context Fusion Attention (CFA) U-Net, a novel architecture that fuses spatial and Sobel-derived geometric features within attention gates to improve both precision and surface completeness. The performance of five architectures, the U-Net (Standard and compressed), U-Net++, Attention U-Net, and CFA U-Net, was systematically evaluated across various data sparsity regimes (10-, 20-, and 40-line spacing). This approach outperformed existing baselines, achieving state-of-the-art results on the Mexilhao field (Santos Basin, Brazil) dataset with a validation IoU of 0.881 and MAE of 2.49ms, and excellent surface coverage of 97.6% on the F3 Block of the North Sea dataset under sparse conditions. The framework further refines merged horizon predictions (inline and cross-line) using Density-Based Spatial Clustering of Applications with Noise (DBSCAN) to produce geologically plausible surfaces. The results demonstrate the advantages of hybrid methodologies and attention-based architectures enhanced with geometric context, providing a robust and generalizable solution for seismic interpretation in structurally complex and data-scarce environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u6539\u8fdb\u7684U-Net\u548c\u7a7a\u95f4\u805a\u7c7b\uff0c\u4ee5\u63d0\u9ad8\u5730\u9707\u5c42\u4f4d\u89e3\u91ca\u7684\u8fde\u7eed\u6027\u548c\u51e0\u4f55\u4fdd\u771f\u5ea6\u3002", "motivation": "\u5728\u6cb9\u6c14\u52d8\u63a2\u4e2d\uff0c\u89e3\u91ca\u5730\u9707\u5c42\u4f4d\u5bf9\u4e8e\u8868\u5f81\u5730\u4e0b\u7ed3\u6784\u81f3\u5173\u91cd\u8981\u3002\u6df1\u5ea6\u5b66\u4e60\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u7279\u522b\u662f\u57fa\u4e8eU-Net\u7684\u67b6\u6784\uff0c\u663e\u7740\u6539\u8fdb\u4e86\u81ea\u52a8\u5c42\u4f4d\u8ffd\u8e2a\u3002\u7136\u800c\uff0c\u51c6\u786e\u5206\u5272\u590d\u6742\u5730\u8d28\u7279\u5f81\u548c\u4ece\u7a00\u758f\u6ce8\u91ca\u4e2d\u63d2\u503c\u5c42\u4f4d\u4ecd\u7136\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86Context Fusion Attention (CFA) U-Net\uff0c\u5b83\u878d\u5408\u4e86\u7a7a\u95f4\u548cSobel\u5bfc\u51fa\u7684\u51e0\u4f55\u7279\u5f81\u5728\u6ce8\u610f\u529b\u95e8\u5185\uff0c\u4ee5\u63d0\u9ad8\u7cbe\u5ea6\u548c\u8868\u9762\u5b8c\u6574\u6027\u3002\u4f7f\u7528DBSCAN\u7ec6\u5316\u5408\u5e76\u7684\u5c42\u4f4d\u9884\u6d4b\u3002", "result": "\u5728Mexilhao\u6cb9\u7530\u6570\u636e\u96c6\u4e0a\uff0c\u9a8c\u8bc1IoU\u8fbe\u52300.881\uff0cMAE\u4e3a2.49ms\uff0c\u5728\u5317\u6d77F3\u533a\u5757\u6570\u636e\u96c6\u4e0a\uff0c\u7a00\u758f\u6761\u4ef6\u4e0b\u7684\u8868\u9762\u8986\u76d6\u7387\u4e3a97.6%\u3002", "conclusion": "\u8bc1\u660e\u4e86\u6df7\u5408\u65b9\u6cd5\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u67b6\u6784\uff08\u901a\u8fc7\u51e0\u4f55\u4e0a\u4e0b\u6587\u589e\u5f3a\uff09\u7684\u4f18\u52bf\uff0c\u4e3a\u7ed3\u6784\u590d\u6742\u548c\u6570\u636e\u7a00\u7f3a\u73af\u5883\u4e2d\u7684\u5730\u9707\u89e3\u91ca\u63d0\u4f9b\u4e86\u7a33\u5065\u548c\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.00065", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00065", "abs": "https://arxiv.org/abs/2512.00065", "authors": ["Sreesritha Sai", "Sai Venkata Suma Sreeja", "Deepthi", "Nikhil"], "title": "Satellite to Street : Disaster Impact Estimator", "comment": "11 pages,9 figures", "summary": "Accurate post-disaster damage assessment is of high importance for prioritizing emergency response; however, manual interpretation of satellite imagery is slow, subjective, and hard to scale. While deep-learning models for image segmentation, such as U-Net-based baselines and change-detection models, are useful baselines, they often struggle with subtle structural variations and severe class imbalance, yielding poor detection of highly damaged regions. The present work proposes a deep-learning framework that jointly processes pre- and post-disaster satellite images to obtain fine-grained pixel-level damage maps: Satellite-to-Street: Disaster Impact Estimator. The model uses a modified dual-input U-Net architecture with enhanced feature fusion to capture both the local structural changes as well as the broader contextual cues. Class-aware weighted loss functions are integrated in order to handle the dominance of undamaged pixels in real disaster datasets, thus enhancing sensitivity toward major and destroyed categories. Experimentation on publicly available disaster datasets shows improved localization and classification of structural damage when compared to traditional segmentation and baseline change-detection models. The resulting damage maps provide a rapid and consistent assessment mechanism to support and not replace expert decision-making, thus allowing more efficient, data-driven disaster management.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Satellite-to-Street \u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5feb\u901f\u3001\u4e00\u81f4\u5730\u8bc4\u4f30\u707e\u540e\u635f\u5931\uff0c\u4ee5\u652f\u6301\u66f4\u9ad8\u6548\u7684\u6570\u636e\u9a71\u52a8\u578b\u707e\u5bb3\u7ba1\u7406\u3002", "motivation": "\u4eba\u5de5\u89e3\u8bd1\u536b\u661f\u56fe\u50cf\u8fdb\u884c\u707e\u540e\u8bc4\u4f30\u901f\u5ea6\u6162\u3001\u4e3b\u89c2\u4e14\u96be\u4ee5\u6269\u5c55\u3002\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u96be\u4ee5\u5904\u7406\u7ec6\u5fae\u7684\u7ed3\u6784\u53d8\u5316\u548c\u4e25\u91cd\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u5bfc\u81f4\u5bf9\u4e25\u91cd\u53d7\u635f\u533a\u57df\u7684\u68c0\u6d4b\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u8be5\u6a21\u578b\u4f7f\u7528\u6539\u8fdb\u7684\u53cc\u8f93\u5165 U-Net \u67b6\u6784\uff0c\u5177\u6709\u589e\u5f3a\u7684\u7279\u5f81\u878d\u5408\uff0c\u4ee5\u6355\u83b7\u5c40\u90e8\u7ed3\u6784\u53d8\u5316\u548c\u66f4\u5e7f\u6cdb\u7684\u4e0a\u4e0b\u6587\u7ebf\u7d22\u3002\u96c6\u6210\u4e86\u7c7b\u522b\u611f\u77e5\u52a0\u6743\u635f\u5931\u51fd\u6570\uff0c\u4ee5\u5904\u7406\u771f\u5b9e\u707e\u96be\u6570\u636e\u96c6\u4e2d\u672a\u635f\u574f\u50cf\u7d20\u7684\u4e3b\u5bfc\u5730\u4f4d\uff0c\u4ece\u800c\u63d0\u9ad8\u5bf9\u4e3b\u8981\u548c\u5df2\u6467\u6bc1\u7c7b\u522b\u7684\u654f\u611f\u6027\u3002", "result": "\u5728\u516c\u5f00\u7684\u707e\u96be\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u5206\u5272\u548c\u57fa\u7ebf\u53d8\u5316\u68c0\u6d4b\u6a21\u578b\u76f8\u6bd4\uff0c\u7ed3\u6784\u635f\u574f\u7684\u5b9a\u4f4d\u548c\u5206\u7c7b\u5f97\u5230\u4e86\u6539\u5584\u3002", "conclusion": "\u751f\u6210\u7684\u635f\u5931\u56fe\u63d0\u4f9b\u4e86\u4e00\u79cd\u5feb\u901f\u4e14\u4e00\u81f4\u7684\u8bc4\u4f30\u673a\u5236\uff0c\u4ee5\u652f\u6301\u800c\u975e\u53d6\u4ee3\u4e13\u5bb6\u51b3\u7b56\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u9ad8\u6548\u3001\u6570\u636e\u9a71\u52a8\u7684\u707e\u5bb3\u7ba1\u7406\u3002"}}
{"id": "2512.00968", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00968", "abs": "https://arxiv.org/abs/2512.00968", "authors": ["Ziyang Zeng", "Heming Jing", "Jindong Chen", "Xiangli Li", "Hongyu Liu", "Yixuan He", "Zhengyu Li", "Yige Sun", "Zheyong Xie", "Yuqing Yang", "Shaosheng Cao", "Jun Fan", "Yi Wu", "Yao Hu"], "title": "Optimizing Generative Ranking Relevance via Reinforcement Learning in Xiaohongshu Search", "comment": "Accepted by KDD 2026 ADS Track", "summary": "Ranking relevance is a fundamental task in search engines, aiming to identify the items most relevant to a given user query. Traditional relevance models typically produce scalar scores or directly predict relevance labels, limiting both interpretability and the modeling of complex relevance signals. Inspired by recent advances in Chain-of-Thought (CoT) reasoning for complex tasks, we investigate whether explicit reasoning can enhance both interpretability and performance in relevance modeling. However, existing reasoning-based Generative Relevance Models (GRMs) primarily rely on supervised fine-tuning on large amounts of human-annotated or synthetic CoT data, which often leads to limited generalization. Moreover, domain-agnostic, free-form reasoning tends to be overly generic and insufficiently grounded, limiting its potential to handle the diverse and ambiguous cases prevalent in open-domain search. In this work, we formulate relevance modeling in Xiaohongshu search as a reasoning task and introduce a Reinforcement Learning (RL)-based training framework to enhance the grounded reasoning capabilities of GRMs. Specifically, we incorporate practical business-specific relevance criteria into the multi-step reasoning prompt design and propose Stepwise Advantage Masking (SAM), a lightweight process-supervision strategy which facilitates effective learning of these criteria through improved credit assignment. To enable industrial deployment, we further distill the large-scale RL-tuned model to a lightweight version suitable for real-world search systems. Extensive experiments on industrial datasets, along with online A/B tests, demonstrate the effectiveness of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60 (RL) \u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u4ee5\u589e\u5f3a\u751f\u6210\u76f8\u5173\u6027\u6a21\u578b (GRM) \u7684\u57fa\u7840\u63a8\u7406\u80fd\u529b\uff0c\u7528\u4e8e\u5c0f\u7ea2\u4e66\u641c\u7d22\u4e2d\u7684\u76f8\u5173\u6027\u5efa\u6a21\u3002", "motivation": "\u4f20\u7edf\u76f8\u5173\u6027\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u590d\u6742\u76f8\u5173\u6027\u4fe1\u53f7\u5efa\u6a21\u6709\u9650\u3002\u73b0\u6709\u57fa\u4e8e\u63a8\u7406\u7684\u751f\u6210\u76f8\u5173\u6027\u6a21\u578b (GRM) \u4e3b\u8981\u4f9d\u8d56\u4e8e\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u6216\u5408\u6210\u7684 CoT \u6570\u636e\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u9886\u57df\u65e0\u5173\u7684\u81ea\u7531\u5f62\u5f0f\u63a8\u7406\u8fc7\u4e8e\u901a\u7528\u4e14\u4e0d\u591f\u624e\u5b9e\u3002", "method": "\u5c06\u5c0f\u7ea2\u4e66\u641c\u7d22\u4e2d\u7684\u76f8\u5173\u6027\u5efa\u6a21 \u0444\u043e\u0440\u043c\u0443\u043b\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u043a\u0430\u043a \u0437\u0430\u0434\u0430\u0447\u0430 \u0440\u0430\u0441\u0441\u0443\u0436\u0434\u0435\u043d\u0438\u044f\uff0c\u5e76\u5f15\u5165\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60 (RL) \u7684\u8bad\u7ec3\u6846\u67b6\u6765\u589e\u5f3a GRM \u7684\u57fa\u7840\u63a8\u7406\u80fd\u529b\u3002\u5c06\u4e1a\u52a1\u76f8\u5173\u7684\u76f8\u5173\u6027\u6807\u51c6\u7eb3\u5165\u591a\u6b65\u63a8\u7406\u63d0\u793a\u8bbe\u8ba1\uff0c\u5e76\u63d0\u51fa\u9010\u6b65\u4f18\u52bf\u5c4f\u853d (SAM)\u3002", "result": "\u5728\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u4ee5\u53ca\u5728\u7ebf A/B \u6d4b\u8bd5\u8868\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5de5\u4e1a\u6570\u636e\u96c6\u548c\u5728\u7ebf A/B \u6d4b\u8bd5\u4e2d\u8868\u73b0\u6709\u6548\uff0c\u5e76\u4e14\u9002\u7528\u4e8e\u5b9e\u9645\u641c\u7d22\u7cfb\u7edf\u3002"}}
{"id": "2512.01733", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.01733", "abs": "https://arxiv.org/abs/2512.01733", "authors": ["Heyang Li", "Anthony Widjaja Lin", "Domagoj Vrgo\u010d"], "title": "Answering Constraint Path Queries over Graphs", "comment": null, "summary": "Constraints are powerful declarative constructs that allow users to\n  conveniently restrict variable values that potentially range over an\n  infinite domain. In this paper, we propose a constraint path query language\n  over property graphs,\n  which extends Regular Path Queries (RPQs) with SMT constraints on data\n  attributes in the form of equality constraints and Linear\n  Real Arithmetic (LRA) constraints. We provide efficient algorithms\n  for evaluating such path queries over property graphs, which exploits\n  optimization of macro-states (among others, using theory-specific\n  techniques).\n  In particular, we demonstrate how such an algorithm may effectively utilize\n  highly optimized SMT solvers for resolving such constraints over paths.\n  We implement our algorithm in MillenniumDB, an open-source graph engine\n  supporting property graph queries and GQL. Our extensive empirical\n  evaluation in a real-world setting demonstrates the viability of our\n  approach.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ea6\u675f\u8def\u5f84\u67e5\u8be2\u8bed\u8a00\uff0c\u8be5\u8bed\u8a00\u6269\u5c55\u4e86\u5e38\u89c4\u8def\u5f84\u67e5\u8be2\uff08RPQ\uff09\uff0c\u5177\u6709SMT\u7ea6\u675f\uff0c\u5305\u62ec\u7b49\u5f0f\u7ea6\u675f\u548c\u7ebf\u6027\u5b9e\u6570\u7b97\u672f\uff08LRA\uff09\u7ea6\u675f\u3002", "motivation": "\u7528\u6237\u53ef\u4ee5\u4f7f\u7528\u7ea6\u675f\u6765\u65b9\u4fbf\u5730\u9650\u5236\u53d8\u91cf\u503c\uff0c\u8fd9\u4e9b\u53d8\u91cf\u503c\u53ef\u80fd\u5728\u65e0\u9650\u57df\u4e2d\u53d8\u5316\u3002", "method": "\u8be5\u65b9\u6cd5\u5305\u542b\u8bc4\u4f30\u5c5e\u6027\u56fe\u4e0a\u7684\u6b64\u7c7b\u8def\u5f84\u67e5\u8be2\u7684\u6709\u6548\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5229\u7528\u5b8f\u72b6\u6001\u7684\u4f18\u5316\uff08\u5305\u62ec\u4f7f\u7528\u7279\u5b9a\u4e8e\u7406\u8bba\u7684\u6280\u672f\uff09\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7684\u73af\u5883\u4e2d\u8fdb\u884c\u7684\u5e7f\u6cdb\u7684\u7ecf\u9a8c\u8bc4\u4f30\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5728MillenniumDB\u4e2d\u5b9e\u73b0\uff0cMillenniumDB\u662f\u4e00\u4e2a\u652f\u6301\u5c5e\u6027\u56fe\u67e5\u8be2\u548cGQL\u7684\u5f00\u6e90\u56fe\u5f15\u64ce\u3002"}}
{"id": "2512.00305", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00305", "abs": "https://arxiv.org/abs/2512.00305", "authors": ["Zhengzhuo Xu", "SiNan Du", "Yiyan Qi", "SiwenLu", "Chengjin Xu", "Chun Yuan", "Jian Guo"], "title": "ChartPoint: Guiding MLLMs with Grounding Reflection for Chart Reasoning", "comment": "Accepted by ICCV 2025", "summary": "Multimodal Large Language Models (MLLMs) have emerged as powerful tools for chart comprehension. However, they heavily rely on extracted content via OCR, which leads to numerical hallucinations when chart textual annotations are sparse. While existing methods focus on scaling instructions, they fail to address the fundamental challenge, i.e., reasoning with visual perception. In this paper, we identify a critical observation: MLLMs exhibit weak grounding in chart elements and proportional relationships, as evidenced by their inability to localize key positions to match their reasoning. To bridge this gap, we propose PointCoT, which integrates reflective interaction into chain-of-thought reasoning in charts. By prompting MLLMs to generate bounding boxes and re-render charts based on location annotations, we establish connections between textual reasoning steps and visual grounding regions. We further introduce an automated pipeline to construct ChartPoint-SFT-62k, a dataset featuring 19.2K high-quality chart samples with step-by-step CoT, bounding box, and re-rendered visualizations. Leveraging this data, we develop two instruction-tuned models, ChartPointQ2 and ChartPointQ2.5, which outperform state-of-the-art across several chart benchmarks, e.g., +5.04\\% on ChartBench.", "AI": {"tldr": "\u63d0\u51fa\u4e86PointCoT\uff0c\u901a\u8fc7\u751f\u6210\u8fb9\u754c\u6846\u548c\u91cd\u65b0\u6e32\u67d3\u56fe\u8868\u6765\u589e\u5f3aMLLM\u5728\u56fe\u8868\u63a8\u7406\u4e2d\u7684\u89c6\u89c9\u57fa\u7840\u3002", "motivation": "\u73b0\u6709\u7684MLLM\u5728\u56fe\u8868\u7406\u89e3\u4e2d\u4f9d\u8d56OCR\u63d0\u53d6\u5185\u5bb9\uff0c\u5f53\u56fe\u8868\u6587\u672c\u6ce8\u91ca\u7a00\u758f\u65f6\uff0c\u4f1a\u5bfc\u81f4\u6570\u5b57\u5e7b\u89c9\uff0c\u5e76\u4e14\u7f3a\u4e4f\u89c6\u89c9\u611f\u77e5\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86PointCoT\uff0c\u5c06\u53cd\u601d\u6027\u4e92\u52a8\u6574\u5408\u5230\u56fe\u8868\u7684\u601d\u7ef4\u94fe\u63a8\u7406\u4e2d\uff0c\u901a\u8fc7\u63d0\u793aMLLM\u751f\u6210\u8fb9\u754c\u6846\u5e76\u57fa\u4e8e\u4f4d\u7f6e\u6ce8\u91ca\u91cd\u65b0\u6e32\u67d3\u56fe\u8868\uff0c\u5efa\u7acb\u6587\u672c\u63a8\u7406\u6b65\u9aa4\u548c\u89c6\u89c9\u57fa\u7840\u533a\u57df\u4e4b\u95f4\u7684\u8fde\u63a5\u3002\u6784\u5efa\u4e86ChartPoint-SFT-62k\u6570\u636e\u96c6\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u5f00\u53d1\u4e86\u4e24\u4e2a\u6307\u4ee4\u5fae\u8c03\u6a21\u578bChartPointQ2\u548cChartPointQ2.5\u3002", "result": "ChartPointQ2\u548cChartPointQ2.5\u5728\u591a\u4e2a\u56fe\u8868\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4f8b\u5982\u5728ChartBench\u4e0a\u63d0\u9ad8\u4e865.04%\u3002", "conclusion": "PointCoT\u901a\u8fc7\u6574\u5408\u53cd\u601d\u6027\u4e92\u52a8\u548c\u89c6\u89c9\u57fa\u7840\uff0c\u663e\u8457\u63d0\u9ad8\u4e86MLLM\u5728\u56fe\u8868\u7406\u89e3\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2512.00234", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00234", "abs": "https://arxiv.org/abs/2512.00234", "authors": ["Sai Koneru", "Matthias Huck", "Jan Niehues"], "title": "OmniFusion: Simultaneous Multilingual Multimodal Translations via Modular Fusion", "comment": "Preprint for ACL 2026", "summary": "There has been significant progress in open-source text-only translation large language models (LLMs) with better language coverage and quality. However, these models can be only used in cascaded pipelines for speech translation (ST), performing automatic speech recognition first followed by translation. This introduces additional latency, which is particularly critical in simultaneous ST (SimulST), and prevents the model from exploiting multimodal context, such as images, which can aid disambiguation. Pretrained multimodal foundation models (MMFMs) already possess strong perception and reasoning capabilities across multiple modalities, but generally lack the multilingual coverage and specialized translation performance of dedicated translation LLMs. To build an effective multimodal translation system, we propose an end-to-end approach that fuses MMFMs with translation LLMs. We introduce a novel fusion strategy that connects hidden states from multiple layers of a pretrained MMFM to a translation LLM, enabling joint end-to-end training. The resulting model, OmniFusion, built on Omni 2.5-7B as the MMFM and SeedX PPO-7B as the translation LLM, can perform speech-to-text, speech-and-image-to-text, and text-and-image-to-text translation. Experiments demonstrate that OmniFusion effectively leverages both audio and visual inputs, achieves a 1-second latency reduction in SimulST compared to cascaded pipelines and also improves the overall translation quality\\footnote{Code is available at https://github.com/saikoneru/OmniFusion}.", "AI": {"tldr": "OmniFusion\u901a\u8fc7\u878d\u5408\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff08MMFMs\uff09\u548c\u7ffb\u8bd1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u8bed\u97f3\u3001\u56fe\u50cf\u548c\u6587\u672c\u7ffb\u8bd1\uff0c \u5e76\u5728SimulST\u4e2d\u964d\u4f4e\u4e86\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u4e86\u7ffb\u8bd1\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u8bed\u97f3\u7ffb\u8bd1\u6a21\u578b\u901a\u5e38\u91c7\u7528\u7ea7\u8054\u7ba1\u9053\uff0c\u5bfc\u81f4\u5ef6\u8fdf\u589e\u52a0\u4e14\u65e0\u6cd5\u5229\u7528\u591a\u6a21\u6001\u4fe1\u606f\u3002\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7f3a\u4e4f\u591a\u8bed\u8a00\u8986\u76d6\u548c\u4e13\u4e1a\u7ffb\u8bd1\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u878d\u5408\u7b56\u7565\uff0c\u5c06\u9884\u8bad\u7ec3\u7684MMFM\u7684\u591a\u5c42\u9690\u85cf\u72b6\u6001\u8fde\u63a5\u5230\u7ffb\u8bd1LLM\uff0c\u5b9e\u73b0\u8054\u5408\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "OmniFusion\u5728SimulST\u4e2d\u5b9e\u73b0\u4e861\u79d2\u7684\u5ef6\u8fdf\u964d\u4f4e\uff0c\u5e76\u63d0\u9ad8\u4e86\u6574\u4f53\u7ffb\u8bd1\u8d28\u91cf\u3002", "conclusion": "OmniFusion\u6709\u6548\u5229\u7528\u4e86\u97f3\u9891\u548c\u89c6\u89c9\u8f93\u5165\uff0c\u4e3a\u591a\u6a21\u6001\u7ffb\u8bd1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2512.00196", "categories": ["cs.LG", "cs.NE", "math.DG", "q-bio.NC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.00196", "abs": "https://arxiv.org/abs/2512.00196", "authors": ["Julian Brandon", "Angus Chadwick", "Arthur Pellegrino"], "title": "Emergent Riemannian geometry over learning discrete computations on continuous manifolds", "comment": null, "summary": "Many tasks require mapping continuous input data (e.g. images) to discrete task outputs (e.g. class labels). Yet, how neural networks learn to perform such discrete computations on continuous data manifolds remains poorly understood. Here, we show that signatures of such computations emerge in the representational geometry of neural networks as they learn. By analysing the Riemannian pullback metric across layers of a neural network, we find that network computation can be decomposed into two functions: discretising continuous input features and performing logical operations on these discretised variables. Furthermore, we demonstrate how different learning regimes (rich vs. lazy) have contrasting metric and curvature structures, affecting the ability of the networks to generalise to unseen inputs. Overall, our work provides a geometric framework for understanding how neural networks learn to perform discrete computations on continuous manifolds.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u795e\u7ecf\u7f51\u7edc\u5982\u4f55\u5b66\u4e60\u5728\u8fde\u7eed\u6570\u636e\u6d41\u5f62\u4e0a\u6267\u884c\u79bb\u6563\u8ba1\u7b97\u3002", "motivation": "\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u5982\u4f55\u5728\u8fde\u7eed\u8f93\u5165\u6570\u636e\u4e0a\u6267\u884c\u79bb\u6563\u8ba1\u7b97\uff0c\u4f8b\u5982\u5c06\u56fe\u50cf\u6620\u5c04\u5230\u7c7b\u522b\u6807\u7b7e\u3002", "method": "\u901a\u8fc7\u5206\u6790\u795e\u7ecf\u7f51\u7edc\u5404\u5c42\u7684\u9ece\u66fc pullback \u5ea6\u91cf\uff0c\u5c06\u7f51\u7edc\u8ba1\u7b97\u5206\u89e3\u4e3a\u4e24\u4e2a\u529f\u80fd\uff1a\u79bb\u6563\u5316\u8fde\u7eed\u8f93\u5165\u7279\u5f81\u548c\u5bf9\u8fd9\u4e9b\u79bb\u6563\u5316\u53d8\u91cf\u6267\u884c\u903b\u8f91\u8fd0\u7b97\u3002", "result": "\u53d1\u73b0\u7f51\u7edc\u8ba1\u7b97\u53ef\u4ee5\u5206\u89e3\u4e3a\u79bb\u6563\u5316\u8fde\u7eed\u8f93\u5165\u7279\u5f81\u548c\u5bf9\u8fd9\u4e9b\u79bb\u6563\u5316\u53d8\u91cf\u6267\u884c\u903b\u8f91\u8fd0\u7b97\uff0c\u5e76\u4e14\u4e0d\u540c\u7684\u5b66\u4e60\u673a\u5236\uff08rich vs. lazy\uff09\u5177\u6709\u5bf9\u6bd4\u7684\u5ea6\u91cf\u548c\u66f2\u7387\u7ed3\u6784\uff0c\u5f71\u54cd\u7f51\u7edc\u6cdb\u5316\u5230\u672a\u89c1\u8f93\u5165\u7684\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u51e0\u4f55\u6846\u67b6\uff0c\u7528\u4e8e\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u5982\u4f55\u5b66\u4e60\u5728\u8fde\u7eed\u6d41\u5f62\u4e0a\u6267\u884c\u79bb\u6563\u8ba1\u7b97\u3002"}}
{"id": "2512.00073", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00073", "abs": "https://arxiv.org/abs/2512.00073", "authors": ["Aswinkumar Varathakumaran", "Nirmala Paramanandham"], "title": "ProvRain: Rain-Adaptive Denoising and Vehicle Detection via MobileNet-UNet and Faster R-CNN", "comment": null, "summary": "Provident vehicle detection has a lot of scope in the detection of vehicle during night time. The extraction of features other than the headlamps of vehicles allows us to detect oncoming vehicles before they appear directly on the camera. However, it faces multiple issues especially in the field of night vision, where a lot of noise caused due to weather conditions such as rain or snow as well as camera conditions. This paper focuses on creating a pipeline aimed at dealing with such noise while at the same time maintaining the accuracy of provident vehicular detection. The pipeline in this paper, ProvRain, uses a lightweight MobileNet-U-Net architecture tuned to generalize to robust weather conditions by using the concept of curricula training. A mix of synthetic as well as available data from the PVDN dataset is used for this. This pipeline is compared to the base Faster RCNN architecture trained on the PVDN dataset to see how much the addition of a denoising architecture helps increase the detection model's performance in rainy conditions. The system boasts an 8.94\\% increase in accuracy and a 10.25\\% increase in recall in the detection of vehicles in rainy night time frames. Similarly, the custom MobileNet-U-Net architecture that was trained also shows a 10-15\\% improvement in PSNR, a 5-6\\% increase in SSIM, and upto a 67\\% reduction in perceptual error (LPIPS) compared to other transformer approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aProvRain\u7684\u8f66\u8f86\u68c0\u6d4b\u6d41\u6c34\u7ebf\uff0c\u65e8\u5728\u63d0\u9ad8\u591c\u95f4\u96e8\u5929\u7b49\u566a\u58f0\u73af\u5883\u4e0b\u7684\u8f66\u8f86\u68c0\u6d4b\u51c6\u786e\u7387\u3002", "motivation": "\u591c\u95f4\u8f66\u8f86\u68c0\u6d4b\u5728\u96e8\u96ea\u7b49\u5929\u6c14\u6761\u4ef6\u4e0b\u53d7\u566a\u58f0\u5f71\u54cd\u5927\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u4fdd\u8bc1\u51c6\u786e\u6027\u3002", "method": "\u8be5\u6d41\u6c34\u7ebf\u91c7\u7528\u8f7b\u91cf\u7ea7\u7684MobileNet-U-Net\u67b6\u6784\uff0c\u5e76\u4f7f\u7528\u8bfe\u7a0b\u5b66\u4e60\u7684\u6982\u5ff5\uff0c\u6df7\u5408\u5408\u6210\u6570\u636e\u548cPVDN\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cProvRain\u5728\u96e8\u591c\u573a\u666f\u4e0b\u8f66\u8f86\u68c0\u6d4b\u7684\u51c6\u786e\u7387\u63d0\u9ad8\u4e868.94%\uff0c\u53ec\u56de\u7387\u63d0\u9ad8\u4e8610.25%\u3002\u4e0eTransformer\u65b9\u6cd5\u76f8\u6bd4\uff0cPSNR\u63d0\u9ad8\u4e8610-15%\uff0cSSIM\u63d0\u9ad8\u4e865-6%\uff0cLPIPS\u964d\u4f4e\u4e8667%\u3002", "conclusion": "ProvRain\u6d41\u6c34\u7ebf\u80fd\u591f\u6709\u6548\u5904\u7406\u566a\u58f0\uff0c\u63d0\u9ad8\u591c\u95f4\u96e8\u5929\u7b49\u590d\u6742\u73af\u5883\u4e0b\u8f66\u8f86\u68c0\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2512.01171", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01171", "abs": "https://arxiv.org/abs/2512.01171", "authors": ["Tao Xue", "Yanwu Yang", "Panyu Zhai"], "title": "Conversion rate prediction in online advertising: modeling techniques, performance evaluation and future directions", "comment": "99 pages, 15 figures, 7 tables", "summary": "Conversion and conversion rate (CVR) prediction play a critical role in efficient advertising decision-making. In past decades, although researchers have developed plenty of models for CVR prediction, the methodological evolution and relationships between different techniques have been precluded. In this paper, we conduct a comprehensive literature review on CVR prediction in online advertising, and classify state-of-the-art CVR prediction models into six categories with respect to the underlying techniques and elaborate on connections between these techniques. For each category of models, we present the framework of underlying techniques, their advantages and disadvantages, and discuss how they are utilized for CVR prediction. Moreover, we summarize the performance of various CVR prediction models on public and proprietary datasets. Finally, we identify research trends, major challenges, and promising future directions. We observe that results of performance evaluation reported in prior studies are not unanimous; semantics-enriched, attribution-enhanced, debiased CVR prediction and jointly modeling CTR and CVR prediction would be promising directions to explore in the future. This review is expected to provide valuable references and insights for future researchers and practitioners in this area.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5728\u7ebf\u5e7f\u544a\u4e2d\u7684\u8f6c\u5316\u7387 (CVR) \u9884\u6d4b\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u7684\u6587\u732e\u7efc\u8ff0\uff0c\u5e76\u5c06\u73b0\u6709\u6a21\u578b\u5206\u4e3a\u516d\u7c7b\uff0c\u603b\u7ed3\u4e86\u5404\u7c7b\u6a21\u578b\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u8ba8\u8bba\u4e86\u5b83\u4eec\u5728 CVR \u9884\u6d4b\u4e2d\u7684\u5e94\u7528\u3002\u6700\u540e\uff0c\u786e\u5b9a\u4e86\u7814\u7a76\u8d8b\u52bf\u3001\u4e3b\u8981\u6311\u6218\u548c\u6709\u5e0c\u671b\u7684\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u8fc7\u53bb\u51e0\u5341\u5e74\uff0c\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86\u5927\u91cf\u7684 CVR \u9884\u6d4b\u6a21\u578b\uff0c\u4f46\u4e0d\u540c\u6280\u672f\u4e4b\u95f4\u7684\u65b9\u6cd5\u6f14\u53d8\u548c\u5173\u7cfb\u88ab\u6392\u9664\u5728\u5916\u3002", "method": "\u5bf9\u5728\u7ebf\u5e7f\u544a\u4e2d\u7684 CVR \u9884\u6d4b\u8fdb\u884c\u5168\u9762\u7684\u6587\u732e\u7efc\u8ff0\uff0c\u5e76\u6839\u636e\u5e95\u5c42\u6280\u672f\u5c06\u6700\u5148\u8fdb\u7684 CVR \u9884\u6d4b\u6a21\u578b\u5206\u4e3a\u516d\u7c7b\uff0c\u5e76\u8be6\u7ec6\u9610\u8ff0\u8fd9\u4e9b\u6280\u672f\u4e4b\u95f4\u7684\u8054\u7cfb\u3002\u5bf9\u4e8e\u6bcf\u4e00\u7c7b\u6a21\u578b\uff0c\u6211\u4eec\u90fd\u4ecb\u7ecd\u4e86\u5e95\u5c42\u6280\u672f\u7684\u6846\u67b6\u3001\u5b83\u4eec\u7684\u4f18\u70b9\u548c\u7f3a\u70b9\uff0c\u5e76\u8ba8\u8bba\u4e86\u5b83\u4eec\u662f\u5982\u4f55\u7528\u4e8e CVR \u9884\u6d4b\u7684\u3002", "result": "\u603b\u7ed3\u4e86\u5404\u79cd CVR \u9884\u6d4b\u6a21\u578b\u5728\u516c\u5171\u548c\u4e13\u6709\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u3002\u89c2\u5bdf\u5230\u5148\u524d\u7814\u7a76\u4e2d\u62a5\u544a\u7684\u6027\u80fd\u8bc4\u4f30\u7ed3\u679c\u5e76\u4e0d\u4e00\u81f4\uff1b\u8bed\u4e49\u4e30\u5bcc\u3001\u5f52\u56e0\u589e\u5f3a\u3001\u53bb\u504f\u7684 CVR \u9884\u6d4b\u4ee5\u53ca\u8054\u5408\u5efa\u6a21 CTR \u548c CVR \u9884\u6d4b\u5c06\u662f\u672a\u6765\u6709\u5e0c\u671b\u63a2\u7d22\u7684\u65b9\u5411\u3002", "conclusion": "\u672c\u6b21\u7efc\u8ff0\u9884\u8ba1\u5c06\u4e3a\u8be5\u9886\u57df\u672a\u6765\u7684\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u4eba\u5458\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u53c2\u8003\u548c\u89c1\u89e3\u3002"}}
{"id": "2512.01769", "categories": ["cs.CV", "cs.DB"], "pdf": "https://arxiv.org/pdf/2512.01769", "abs": "https://arxiv.org/abs/2512.01769", "authors": ["Hafsa Billah"], "title": "VideoScoop: A Non-Traditional Domain-Independent Framework For Video Analysis", "comment": "This is a report submitted as part of PhD proposal defense of Hafsa Billah", "summary": "Automatically understanding video contents is important for several applications in Civic Monitoring (CM), general Surveillance (SL), Assisted Living (AL), etc. Decades of Image and Video Analysis (IVA) research have advanced tasks such as content extraction (e.g., object recognition and tracking). Identifying meaningful activities or situations (e.g., two objects coming closer) remains difficult and cannot be achieved by content extraction alone. Currently, Video Situation Analysis (VSA) is done manually with a human in the loop, which is error-prone and labor-intensive, or through custom algorithms designed for specific video types or situations. These algorithms are not general-purpose and require a new algorithm/software for each new situation or video from a new domain.\n  This report proposes a general-purpose VSA framework that overcomes the above limitations. Video contents are extracted once using state-of-the-art Video Content Extraction technologies. They are represented using two alternative models -- the extended relational model (R++) and graph models. When represented using R++, the extracted contents can be used as data streams, enabling Continuous Query Processing via the proposed Continuous Query Language for Video Analysis. The graph models complement this by enabling the detection of situations that are difficult or impossible to detect using the relational model alone. Existing graph algorithms and newly developed algorithms support a wide variety of situation detection. To support domain independence, primitive situation variants across domains are identified and expressed as parameterized templates. Extensive experiments were conducted across several interesting situations from three domains -- AL, CM, and SL-- to evaluate the accuracy, efficiency, and robustness of the proposed approach using a dataset of videos of varying lengths from these domains.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u89c6\u9891\u60c5\u5883\u5206\u6790\u6846\u67b6\uff0c\u5b83\u53ef\u4ee5\u514b\u670d\u73b0\u6709\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u81ea\u52a8\u7406\u89e3\u89c6\u9891\u5185\u5bb9\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u60c5\u5883\u5206\u6790\u4e3b\u8981\u4f9d\u8d56\u4eba\u5de5\u6216\u5b9a\u5236\u7b97\u6cd5\uff0c\u5b58\u5728\u6613\u51fa\u9519\u3001\u52b3\u52a8\u5bc6\u96c6\u4ee5\u53ca\u7f3a\u4e4f\u901a\u7528\u6027\u7684\u95ee\u9898\u3002", "method": "\u8be5\u6846\u67b6\u9996\u5148\u4f7f\u7528\u5148\u8fdb\u7684\u89c6\u9891\u5185\u5bb9\u63d0\u53d6\u6280\u672f\u63d0\u53d6\u89c6\u9891\u5185\u5bb9\uff0c\u7136\u540e\u4f7f\u7528\u6269\u5c55\u5173\u7cfb\u6a21\u578b\uff08R++\uff09\u548c\u56fe\u6a21\u578b\u4e24\u79cd\u6a21\u578b\u8868\u793a\u63d0\u53d6\u7684\u5185\u5bb9\u3002R++\u6a21\u578b\u652f\u6301\u8fde\u7eed\u67e5\u8be2\u5904\u7406\uff0c\u56fe\u6a21\u578b\u7528\u4e8e\u68c0\u6d4b\u96be\u4ee5\u7528\u5173\u7cfb\u6a21\u578b\u68c0\u6d4b\u5230\u7684\u60c5\u5883\u3002\u4e3a\u4e86\u652f\u6301\u9886\u57df\u72ec\u7acb\u6027\uff0c\u7814\u7a76\u8bc6\u522b\u4e86\u8de8\u9886\u57df\u7684\u539f\u59cb\u60c5\u5883\u53d8\u4f53\uff0c\u5e76\u5c06\u5b83\u4eec\u8868\u793a\u4e3a\u53c2\u6570\u5316\u6a21\u677f\u3002", "result": "\u5728\u6765\u81eaAL\u3001CM\u548cSL\u4e09\u4e2a\u9886\u57df\u7684\u591a\u4e2a\u6709\u8da3\u60c5\u5883\u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u4ee5\u8bc4\u4f30\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u89c6\u9891\u60c5\u5883\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5173\u7cfb\u6a21\u578b\u548c\u56fe\u6a21\u578b\uff0c\u4ee5\u53ca\u53c2\u6570\u5316\u6a21\u677f\uff0c\u5b9e\u73b0\u4e86\u5728\u4e0d\u540c\u9886\u57df\u4e2d\u81ea\u52a8\u3001\u51c6\u786e\u3001\u9ad8\u6548\u5730\u5206\u6790\u89c6\u9891\u5185\u5bb9\u3002"}}
{"id": "2512.00319", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00319", "abs": "https://arxiv.org/abs/2512.00319", "authors": ["Ruike Hu", "Shulei Wu"], "title": "RL-Struct: A Lightweight Reinforcement Learning Framework for Reliable Structured Output in LLMs", "comment": "23 pages, 14 figures. Model is available at https://huggingface.co/Freakz3z/Qwen-JSON", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language generation and reasoning. However, their integration into automated software ecosystems is often hindered by the \"Structure Gap\" - the inherent tension between the probabilistic nature of token generation and the deterministic requirements of structured data formats (e.g., JSON, XML). Traditional Supervised Fine-Tuning (SFT) often fails to enforce strict syntactic constraints, leading to \"hallucinated\" keys or malformed structures, while constrained decoding methods impose significant inference latency. In this paper, we propose a lightweight, efficient Reinforcement Learning (RL) framework to bridge this gap. We introduce a novel Multi-dimensional Reward Function that decomposes the structured output task into a hierarchy of constraints: structural integrity, format correctness, content accuracy, and validity. Leveraging Gradient Regularized Policy Optimization (GRPO), we enable the model to internalize these constraints without the need for a separate critic network, reducing peak VRAM usage by 40% compared to PPO. We validate our approach on multiple tasks, including complex recipe generation and structured math reasoning (GSM8K-JSON). Experimental results demonstrate that our method achieves 89.7% structural accuracy and 92.1% JSON validity, significantly outperforming both zero-shot baselines (e.g., GPT-3.5) and SFT on larger models like LLaMA-3-8B. Furthermore, we provide a detailed analysis of training dynamics, revealing a distinct self-paced curriculum where the model sequentially acquires syntactic proficiency before semantic accuracy. Our model is publicly available at https://huggingface.co/Freakz3z/Qwen-JSON.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4ee5\u5f25\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u7ed3\u6784\u5316\u6570\u636e\u65f6\u9047\u5230\u7684\u201c\u7ed3\u6784\u5dee\u8ddd\u201d\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u751f\u6210\u548c\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7531\u4e8etoken\u751f\u6210\u662f\u6982\u7387\u6027\u7684\uff0c\u800c\u7ed3\u6784\u5316\u6570\u636e\u683c\u5f0f\u9700\u8981\u786e\u5b9a\u6027\uff0c\u56e0\u6b64\u5f88\u96be\u5c06\u5176\u96c6\u6210\u5230\u81ea\u52a8\u5316\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\u4e2d\u3002", "method": "\u8be5\u65b9\u6cd5\u4f7f\u7528\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u7ef4\u5956\u52b1\u51fd\u6570\uff0c\u5c06\u7ed3\u6784\u5316\u8f93\u51fa\u4efb\u52a1\u5206\u89e3\u4e3a\u7ed3\u6784\u5b8c\u6574\u6027\u3001\u683c\u5f0f\u6b63\u786e\u6027\u3001\u5185\u5bb9\u51c6\u786e\u6027\u548c\u6709\u6548\u6027\u7b49\u7ea6\u675f\u3002\u5229\u7528\u68af\u5ea6\u6b63\u5219\u5316\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5728\u4e0d\u9700\u8981\u5355\u72ec\u7684critic\u7f51\u7edc\u7684\u60c5\u51b5\u4e0b\u5185\u5316\u8fd9\u4e9b\u7ea6\u675f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u90fd\u4f18\u4e8ezero-shot\u57fa\u7ebf\u548cSFT\uff0c\u7ed3\u6784\u51c6\u786e\u7387\u8fbe\u523089.7%\uff0cJSON\u6709\u6548\u6027\u8fbe\u523092.1%\u3002", "conclusion": "\u8be5\u6a21\u578b\u80fd\u591f\u6309\u987a\u5e8f\u83b7\u5f97\u53e5\u6cd5\u719f\u7ec3\u5ea6\u548c\u8bed\u4e49\u51c6\u786e\u6027\u3002"}}
{"id": "2512.00274", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00274", "abs": "https://arxiv.org/abs/2512.00274", "authors": ["Prakrithi Shivaprakash", "Diptadhi Mukherjee", "Lekhansh Shukla", "Animesh Mukherjee", "Prabhat Chand", "Pratima Murthy"], "title": "Lost without translation -- Can transformer (language models) understand mood states?", "comment": "33 pages, 3 figures, 2 tables", "summary": "Background: Large Language Models show promise in psychiatry but are English-centric. Their ability to understand mood states in other languages is unclear, as different languages have their own idioms of distress. Aim: To quantify the ability of language models to faithfully represent phrases (idioms of distress) of four distinct mood states (depression, euthymia, euphoric mania, dysphoric mania) expressed in Indian languages. Methods: We collected 247 unique phrases for the four mood states across 11 Indic languages. We tested seven experimental conditions, comparing k-means clustering performance on: (a) direct embeddings of native and Romanised scripts (using multilingual and Indic-specific models) and (b) embeddings of phrases translated to English and Chinese. Performance was measured using a composite score based on Adjusted Rand Index, Normalised Mutual Information, Homogeneity and Completeness. Results: Direct embedding of Indic languages failed to cluster mood states (Composite Score = 0.002). All translation-based approaches showed significant improvement. High performance was achieved using Gemini-translated English (Composite=0.60) and human-translated English (Composite=0.61) embedded with gemini-001. Surprisingly, human-translated English, further translated into Chinese and embedded with a Chinese model, performed best (Composite = 0.67). Specialised Indic models (IndicBERT and Sarvam-M) performed poorly. Conclusion: Current models cannot meaningfully represent mood states directly from Indic languages, posing a fundamental barrier to their psychiatric application for diagnostic or therapeutic purposes in India. While high-quality translation bridges this gap, reliance on proprietary models or complex translation pipelines is unsustainable. Models must first be built to understand diverse local languages to be effective in global mental health.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u5370\u5ea6\u8bed\u8a00\u7684\u60c5\u7eea\u72b6\u6001\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u8fd9\u963b\u788d\u4e86\u5b83\u4eec\u5728\u5370\u5ea6\u7cbe\u795e\u75c5\u5b66\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5728\u5370\u5ea6\u8bed\u8a00\u4e2d\u7406\u89e3\u4e0d\u540c\u60c5\u7eea\u72b6\u6001\uff08\u6291\u90c1\u3001\u5fc3\u5883\u6b63\u5e38\u3001\u6b23\u5feb\u72c2\u8e81\u3001\u70e6\u8e81\u72c2\u8e81\uff09\u7684\u80fd\u529b\u3002", "method": "\u6536\u96c6\u4e8611\u79cd\u5370\u5ea6\u8bed\u8a00\u4e2d\u56db\u79cd\u60c5\u7eea\u72b6\u6001\u7684247\u4e2a\u77ed\u8bed\uff0c\u5e76\u6d4b\u8bd5\u4e86\u4e03\u79cd\u5b9e\u9a8c\u6761\u4ef6\uff0c\u6bd4\u8f83k-means\u805a\u7c7b\u6027\u80fd\u3002", "result": "\u76f4\u63a5\u5d4c\u5165\u5370\u5ea6\u8bed\u8a00\u65e0\u6cd5\u805a\u7c7b\u60c5\u7eea\u72b6\u6001\uff0c\u800c\u57fa\u4e8e\u7ffb\u8bd1\u7684\u65b9\u6cd5\u663e\u793a\u51fa\u663e\u8457\u6539\u5584\u3002\u4f7f\u7528 Gemini \u7ffb\u8bd1\u6210\u82f1\u8bed\u548c\u4eba\u5de5\u7ffb\u8bd1\u6210\u82f1\u8bed\u8868\u73b0\u826f\u597d\uff0c\u4f46\u4eba\u5de5\u7ffb\u8bd1\u6210\u82f1\u8bed\u518d\u7ffb\u8bd1\u6210\u4e2d\u6587\u8868\u73b0\u6700\u4f73\u3002\u4e13\u95e8\u7684\u5370\u5ea6\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u76ee\u524d\u7684\u6a21\u578b\u65e0\u6cd5\u76f4\u63a5\u7406\u89e3\u5370\u5ea6\u8bed\u8a00\u7684\u60c5\u7eea\u72b6\u6001\uff0c\u4f9d\u8d56\u4e13\u6709\u6a21\u578b\u6216\u590d\u6742\u7684\u7ffb\u8bd1\u6d41\u7a0b\u662f\u4e0d\u53ef\u6301\u7eed\u7684\u3002"}}
{"id": "2512.00207", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00207", "abs": "https://arxiv.org/abs/2512.00207", "authors": ["Owen Dugan", "Roberto Garcia", "Ronny Junkins", "Jerry Liu", "Dylan Zinsley", "Sabri Eyuboglu", "Atri Rudra", "Chris R\u00e9"], "title": "Constructing Efficient Fact-Storing MLPs for Transformers", "comment": null, "summary": "The success of large language models (LLMs) can be attributed in part to their ability to efficiently store factual knowledge as key-value mappings within their MLP parameters. Recent work has proposed explicit weight constructions to build such fact-storing MLPs, providing an improved understanding of LLM fact storage mechanisms. In this paper, we introduce an MLP construction framework that improves over previous constructions in three areas: it 1) works for all but a measure-zero set of feasible input-output pairs, 2) achieves asymptotically optimal parameter efficiency matching information-theoretic bounds for some embeddings, and 3) maintains usability within Transformers for factual recall. Through our improvements, we 1) discover a metric on value embeddings that characterizes facts-per-parameter scaling for both constructed and gradient-descent-trained MLPs, 2) identify a simple encoder-decoder mechanism that empirically matches gradient-descent MLP facts-per-parameter asymptotics across all the inputs and outputs we test, and 3) uncover a fundamental tradeoff between an MLP's fact-storage capacity and its usability within Transformers. Finally, we demonstrate a proof-of-concept application of fact-storing MLPs: modular fact editing on one-layer Transformers by \\textit{replacing entire MLPs at once}.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cdMLP\u6784\u5efa\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6539\u8fdb\u4e86\u4ee5\u524d\u7684\u7ed3\u6784\uff0c\u9002\u7528\u4e8e\u6240\u6709\u53ef\u884c\u7684\u8f93\u5165-\u8f93\u51fa\u5bf9\uff0c\u5b9e\u73b0\u4e86\u6e10\u8fd1\u6700\u4f18\u7684\u53c2\u6570\u6548\u7387\uff0c\u5e76\u5728Transformer\u4e2d\u4fdd\u6301\u4e86\u4e8b\u5b9e\u56de\u5fc6\u7684\u53ef\u7528\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6210\u529f\u90e8\u5206\u5f52\u529f\u4e8e\u5b83\u4eec\u80fd\u591f\u6709\u6548\u5730\u5c06\u4e8b\u5b9e\u77e5\u8bc6\u4f5c\u4e3a\u952e-\u503c\u6620\u5c04\u5b58\u50a8\u5728\u5176MLP\u53c2\u6570\u4e2d\u3002\u6700\u8fd1\u7684\u5de5\u4f5c\u63d0\u51fa\u4e86\u663e\u5f0f\u7684\u6743\u91cd\u6784\u9020\u6765\u6784\u5efa\u8fd9\u79cd\u4e8b\u5b9e\u5b58\u50a8MLP\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u5bf9LLM\u4e8b\u5b9e\u5b58\u50a8\u673a\u5236\u7684\u7406\u89e3\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cdMLP\u6784\u5efa\u6846\u67b6\u3002", "result": "\u672c\u6587\u53d1\u73b0\u4e86\u503c\u5d4c\u5165\u4e0a\u7684\u4e00\u79cd\u5ea6\u91cf\uff0c\u8be5\u5ea6\u91cf\u8868\u5f81\u4e86\u6784\u9020\u7684\u548c\u68af\u5ea6\u4e0b\u964d\u8bad\u7ec3\u7684MLP\u7684\u4e8b\u5b9e-\u53c2\u6570\u7f29\u653e\u6bd4\u4f8b\uff1b\u786e\u5b9a\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u673a\u5236\uff0c\u8be5\u673a\u5236\u5728\u7ecf\u9a8c\u4e0a\u4e0e\u68af\u5ea6\u4e0b\u964dMLP\u4e8b\u5b9e-\u53c2\u6570\u6e10\u8fd1\u7ebf\u76f8\u5339\u914d\uff1b\u63ed\u793a\u4e86MLP\u7684\u4e8b\u5b9e\u5b58\u50a8\u5bb9\u91cf\u4e0e\u5176\u5728Transformer\u4e2d\u7684\u53ef\u7528\u6027\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\u3002", "conclusion": "\u672c\u6587\u8bba\u8bc1\u4e86\u4e8b\u5b9e\u5b58\u50a8MLP\u7684\u6982\u5ff5\u9a8c\u8bc1\u5e94\u7528\uff1a\u901a\u8fc7\u4e00\u6b21\u66ff\u6362\u6574\u4e2aMLP\u6765\u5bf9\u5355\u5c42Transformer\u8fdb\u884c\u6a21\u5757\u5316\u4e8b\u5b9e\u7f16\u8f91\u3002"}}
{"id": "2512.00075", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.00075", "abs": "https://arxiv.org/abs/2512.00075", "authors": ["Jun Jia", "Hongyi Miao", "Yingjie Zhou", "Wangqiu Zhou", "Jianbo Zhang", "Linhan Cao", "Dandan Zhu", "Hua Yang", "Xiongkuo Min", "Wei Sun", "Guangtao Zhai"], "title": "Adapter Shield: A Unified Framework with Built-in Authentication for Preventing Unauthorized Zero-Shot Image-to-Image Generation", "comment": null, "summary": "With the rapid progress in diffusion models, image synthesis has advanced to the stage of zero-shot image-to-image generation, where high-fidelity replication of facial identities or artistic styles can be achieved using just one portrait or artwork, without modifying any model weights. Although these techniques significantly enhance creative possibilities, they also pose substantial risks related to intellectual property violations, including unauthorized identity cloning and stylistic imitation. To counter such threats, this work presents Adapter Shield, the first universal and authentication-integrated solution aimed at defending personal images from misuse in zero-shot generation scenarios. We first investigate how current zero-shot methods employ image encoders to extract embeddings from input images, which are subsequently fed into the UNet of diffusion models through cross-attention layers. Inspired by this mechanism, we construct a reversible encryption system that maps original embeddings into distinct encrypted representations according to different secret keys. The authorized users can restore the authentic embeddings via a decryption module and the correct key, enabling normal usage for authorized generation tasks. For protection purposes, we design a multi-target adversarial perturbation method that actively shifts the original embeddings toward designated encrypted patterns. Consequently, protected images are embedded with a defensive layer that ensures unauthorized users can only produce distorted or encrypted outputs. Extensive evaluations demonstrate that our method surpasses existing state-of-the-art defenses in blocking unauthorized zero-shot image synthesis, while supporting flexible and secure access control for verified users.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAdapter Shield\u7684\u901a\u7528\u4e14\u96c6\u6210\u4e86\u8eab\u4efd\u9a8c\u8bc1\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e8\u5728\u4fdd\u62a4\u4e2a\u4eba\u56fe\u50cf\u514d\u53d7\u96f6\u6837\u672c\u751f\u6210\u573a\u666f\u4e2d\u7684\u6ee5\u7528\u3002", "motivation": "\u96f6\u6837\u672c\u56fe\u50cf\u5230\u56fe\u50cf\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55\u5e26\u6765\u4e86\u77e5\u8bc6\u4ea7\u6743\u4fb5\u72af\u7684\u98ce\u9669\uff0c\u4f8b\u5982\u672a\u7ecf\u6388\u6743\u7684\u8eab\u4efd\u514b\u9686\u548c\u98ce\u683c\u6a21\u4eff\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u53ef\u9006\u52a0\u5bc6\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u6839\u636e\u4e0d\u540c\u7684\u5bc6\u94a5\u5c06\u539f\u59cb\u5d4c\u5165\u6620\u5c04\u5230\u4e0d\u540c\u7684\u52a0\u5bc6\u8868\u793a\u3002\u8bbe\u8ba1\u4e86\u4e00\u79cd\u591a\u76ee\u6807\u5bf9\u6297\u6270\u52a8\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4e3b\u52a8\u5c06\u539f\u59cb\u5d4c\u5165\u8f6c\u79fb\u5230\u6307\u5b9a\u7684\u52a0\u5bc6\u6a21\u5f0f\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u963b\u6b62\u672a\u7ecf\u6388\u6743\u7684\u96f6\u6837\u672c\u56fe\u50cf\u5408\u6210\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u540c\u65f6\u652f\u6301\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u7528\u6237\u7684\u7075\u6d3b\u548c\u5b89\u5168\u8bbf\u95ee\u63a7\u5236\u3002", "conclusion": "Adapter Shield\u662f\u4e00\u79cd\u6709\u6548\u7684\u9632\u5fa1\u673a\u5236\uff0c\u53ef\u4ee5\u4fdd\u62a4\u4e2a\u4eba\u56fe\u50cf\u514d\u53d7\u96f6\u6837\u672c\u751f\u6210\u6280\u672f\u7684\u6ee5\u7528\uff0c\u540c\u65f6\u5141\u8bb8\u6388\u6743\u7528\u6237\u6b63\u5e38\u4f7f\u7528\u3002"}}
{"id": "2512.01179", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01179", "abs": "https://arxiv.org/abs/2512.01179", "authors": ["Shan Gao", "Yanwu Yang"], "title": "Toward a benchmark for CTR prediction in online advertising: datasets, evaluation protocols and perspectives", "comment": "64 pages, 8 figures, 11 tables", "summary": "This research designs a unified architecture of CTR prediction benchmark (Bench-CTR) platform that offers flexible interfaces with datasets and components of a wide range of CTR prediction models. Moreover, we construct a comprehensive system of evaluation protocols encompassing real-world and synthetic datasets, a taxonomy of metrics, standardized procedures and experimental guidelines for calibrating the performance of CTR prediction models. Furthermore, we implement the proposed benchmark platform and conduct a comparative study to evaluate a wide range of state-of-the-art models from traditional multivariate statistical to modern large language model (LLM)-based approaches on three public datasets and two synthetic datasets. Experimental results reveal that, (1) high-order models largely outperform low-order models, though such advantage varies in terms of metrics and on different datasets; (2) LLM-based models demonstrate a remarkable data efficiency, i.e., achieving the comparable performance to other models while using only 2% of the training data; (3) the performance of CTR prediction models has achieved significant improvements from 2015 to 2016, then reached a stage with slow progress, which is consistent across various datasets. This benchmark is expected to facilitate model development and evaluation and enhance practitioners' understanding of the underlying mechanisms of models in the area of CTR prediction. Code is available at https://github.com/NuriaNinja/Bench-CTR.", "AI": {"tldr": "\u672c\u6587\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684CTR\u9884\u6d4b\u57fa\u51c6\u5e73\u53f0\uff08Bench-CTR\uff09\uff0c\u8be5\u5e73\u53f0\u63d0\u4f9b\u7075\u6d3b\u7684\u63a5\u53e3\uff0c\u53ef\u4ee5\u8fde\u63a5\u5404\u79cd\u6570\u636e\u96c6\u548cCTR\u9884\u6d4b\u6a21\u578b\u7684\u7ec4\u4ef6\u3002", "motivation": "\u65e8\u5728\u4fc3\u8fdb\u6a21\u578b\u5f00\u53d1\u548c\u8bc4\u4f30\uff0c\u5e76\u63d0\u9ad8\u4ece\u4e1a\u8005\u5bf9CTR\u9884\u6d4b\u9886\u57df\u6a21\u578b\u5e95\u5c42\u673a\u5236\u7684\u7406\u89e3\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u8bc4\u4f30\u534f\u8bae\u7cfb\u7edf\uff0c\u5305\u62ec\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u96c6\u3001\u6307\u6807\u5206\u7c7b\u3001\u6807\u51c6\u5316\u7a0b\u5e8f\u548c\u5b9e\u9a8c\u6307\u5357\uff0c\u7528\u4e8e\u6821\u51c6CTR\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002\u540c\u65f6\uff0c\u5bf9\u4ece\u4f20\u7edf\u591a\u5143\u7edf\u8ba1\u5230\u73b0\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5404\u79cd\u6700\u5148\u8fdb\u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83\u7814\u7a76\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff1a\uff081\uff09\u9ad8\u9636\u6a21\u578b\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4f18\u4e8e\u4f4e\u9636\u6a21\u578b\uff0c\u5c3d\u7ba1\u8fd9\u79cd\u4f18\u52bf\u56e0\u6307\u6807\u548c\u6570\u636e\u96c6\u800c\u5f02\uff1b\uff082\uff09\u57fa\u4e8eLLM\u7684\u6a21\u578b\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6570\u636e\u6548\u7387\uff0c\u5373\u4ec5\u4f7f\u75282%\u7684\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u8fbe\u5230\u4e0e\u5176\u4ed6\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff1b\uff083\uff09CTR\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u4ece2015\u5e74\u52302016\u5e74\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u6b65\uff0c\u7136\u540e\u8fdb\u5165\u4e86\u4e00\u4e2a\u8fdb\u5c55\u7f13\u6162\u7684\u9636\u6bb5\uff0c\u8fd9\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e2d\u662f\u4e00\u81f4\u7684\u3002", "conclusion": "\u8be5\u57fa\u51c6\u5e73\u53f0\u6709\u671b\u4fc3\u8fdb\u6a21\u578b\u5f00\u53d1\u548c\u8bc4\u4f30\uff0c\u5e76\u589e\u5f3a\u4ece\u4e1a\u8005\u5bf9CTR\u9884\u6d4b\u9886\u57df\u6a21\u578b\u5e95\u5c42\u673a\u5236\u7684\u7406\u89e3\u3002"}}
{"id": "2512.00331", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.00331", "abs": "https://arxiv.org/abs/2512.00331", "authors": ["Yefeng Wu", "Yuchen Song", "Yecheng Zhao", "Ling Wu", "Shan Wan"], "title": "CogEvo-Edu: Cognitive Evolution Educational Multi-Agent Collaborative System", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed as conversational tutors in STEM education, yet most systems still rely on a single LLM with a static retrieval-augmented generation (RAG) pipeline over course materials. This design struggles in complex domains such as digital signal processing (DSP), where tutors must maintain coherent long-term student models, manage heterogeneous knowledge bases, and adapt teaching strategies over extended interactions. We argue that retrieval, memory, and control should be treated as a coupled cognitive evolution process. We instantiate this view in CogEvo-Edu, a hierarchical educational multi-agent system comprising a Cognitive Perception Layer (CPL), a Knowledge Evolution Layer (KEL), and a Meta-Control Layer (MCL). CPL maintains dual memories and performs confidence-weighted consolidation to build structured, self-correcting student profiles under limited context. KEL assigns each knowledge chunk a spatiotemporal value that drives activation, semantic compression, and forgetting. MCL formulates tutoring as hierarchical sequential decision making, orchestrating specialized agents and jointly adapting CPL/KEL hyperparameters via a dual inner--outer loop. To evaluate CogEvo-Edu, we construct DSP-EduBench, a vertical benchmark for DSP tutoring with heterogeneous resources, simulated student profiles, and long-horizon interaction scripts. Using a three-model LLM-as-a-Judge ensemble, CogEvo-Edu raises the overall score from 5.32 to 9.23 and improves all six indicators over static RAG, simple memory, and a single-agent variant, demonstrating the value of jointly evolving student profiles, knowledge bases, and teaching policies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6559\u80b2\u591a\u667a\u80fd\u4f53\u7cfb\u7edfCogEvo-Edu\uff0c\u7528\u4e8e\u63d0\u5347LLM\u5728STEM\u6559\u80b2\u4e2d\u4f5c\u4e3a\u4f1a\u8bdd\u8f85\u5bfc\u5458\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u9886\u57df\u5982\u6570\u5b57\u4fe1\u53f7\u5904\u7406(DSP)\u65b9\u9762\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u9759\u6001RAG\u7ba1\u9053\u7684LLM\u8f85\u5bfc\u7cfb\u7edf\u5728\u5904\u7406\u590d\u6742\u9886\u57df\uff08\u5982DSP\uff09\u65f6\uff0c\u96be\u4ee5\u7ef4\u6301\u8fde\u8d2f\u7684\u5b66\u751f\u6a21\u578b\u3001\u7ba1\u7406\u5f02\u6784\u77e5\u8bc6\u5e93\u548c\u8c03\u6574\u6559\u5b66\u7b56\u7565\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5206\u5c42\u7684\u6559\u80b2\u591a\u667a\u80fd\u4f53\u7cfb\u7edfCogEvo-Edu\uff0c\u5305\u542b\u8ba4\u77e5\u611f\u77e5\u5c42(CPL)\u3001\u77e5\u8bc6\u8fdb\u5316\u5c42(KEL)\u548c\u5143\u63a7\u5236\u5c42(MCL)\uff0c\u5206\u522b\u8d1f\u8d23\u7ef4\u62a4\u5b66\u751f\u6a21\u578b\u3001\u7ba1\u7406\u77e5\u8bc6\u5e93\u548c\u534f\u8c03\u6559\u5b66\u7b56\u7565\u3002\u901a\u8fc7\u53cc\u5185\u5916\u5faa\u73af\u5171\u540c\u8c03\u6574CPL/KEL\u7684\u8d85\u53c2\u6570\u3002", "result": "\u5728DSP-EduBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCogEvo-Edu\u5c06\u603b\u4f53\u8bc4\u5206\u4ece5.32\u63d0\u9ad8\u52309.23\uff0c\u5e76\u5728\u6240\u6709\u516d\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8e\u9759\u6001RAG\u3001\u7b80\u5355\u8bb0\u5fc6\u548c\u5355\u667a\u80fd\u4f53\u53d8\u4f53\u3002", "conclusion": "CogEvo-Edu\u901a\u8fc7\u8054\u5408\u8fdb\u5316\u5b66\u751f\u6863\u6848\u3001\u77e5\u8bc6\u5e93\u548c\u6559\u5b66\u7b56\u7565\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u590d\u6742STEM\u6559\u80b2\u4e2d\u4f5c\u4e3aLLM\u8f85\u5bfc\u5458\u7684\u4ef7\u503c\u3002"}}
{"id": "2512.00290", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00290", "abs": "https://arxiv.org/abs/2512.00290", "authors": ["Guoqing Ma", "Jia Zhu", "Hanghui Guo", "Weijie Shi", "Yue Cui", "Jiawei Shen", "Zilong Li", "Yidan Liang"], "title": "EduEval: A Hierarchical Cognitive Benchmark for Evaluating Large Language Models in Chinese Education", "comment": null, "summary": "Large language models (LLMs) demonstrate significant potential for educational applications. However, their unscrutinized deployment poses risks to educational standards, underscoring the need for rigorous evaluation. We introduce EduEval, a comprehensive hierarchical benchmark for evaluating LLMs in Chinese K-12 education. This benchmark makes three key contributions: (1) Cognitive Framework: We propose the EduAbility Taxonomy, which unifies Bloom's Taxonomy and Webb's Depth of Knowledge to organize tasks across six cognitive dimensions including Memorization, Understanding, Application, Reasoning, Creativity, and Ethics. (2) Authenticity: Our benchmark integrates real exam questions, classroom conversation, student essays, and expert-designed prompts to reflect genuine educational challenges; (3) Scale: EduEval comprises 24 distinct task types with over 11,000 questions spanning primary to high school levels. We evaluate 14 leading LLMs under both zero-shot and few-shot settings, revealing that while models perform well on factual tasks, they struggle with classroom dialogue classification and exhibit inconsistent results in creative content generation. Interestingly, several open source models outperform proprietary systems on complex educational reasoning. Few-shot prompting shows varying effectiveness across cognitive dimensions, suggesting that different educational objectives require tailored approaches. These findings provide targeted benchmarking metrics for developing LLMs specifically optimized for diverse Chinese educational tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86EduEval\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e2d\u6587K-12\u6559\u80b2\u4e2d\u8868\u73b0\u7684\u7efc\u5408\u6027\u5206\u5c42\u57fa\u51c6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6559\u80b2\u5e94\u7528\u4e2d\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u672a\u7ecf\u5ba1\u67e5\u7684\u90e8\u7f72\u4f1a\u7ed9\u6559\u80b2\u6807\u51c6\u5e26\u6765\u98ce\u9669\uff0c\u56e0\u6b64\u9700\u8981\u4e25\u683c\u8bc4\u4f30\u3002", "method": "\u6784\u5efa\u4e86EduEval\u57fa\u51c6\uff0c\u5305\u542b\u8ba4\u77e5\u6846\u67b6\uff08EduAbility Taxonomy\uff0c\u7edf\u4e00\u4e86Bloom\u5206\u7c7b\u6cd5\u548cWebb\u6df1\u5ea6\u77e5\u8bc6\uff09\u3001\u771f\u5b9e\u6027\uff08\u6574\u5408\u771f\u5b9e\u8003\u8bd5\u9898\u3001\u8bfe\u5802\u5bf9\u8bdd\u3001\u5b66\u751f\u4f5c\u6587\u548c\u4e13\u5bb6\u8bbe\u8ba1\u7684\u63d0\u793a\uff09\u548c\u89c4\u6a21\uff08\u5305\u542b24\u79cd\u4e0d\u540c\u4efb\u52a1\u7c7b\u578b\uff0c\u8d85\u8fc711000\u4e2a\u95ee\u9898\uff0c\u6db5\u76d6\u5c0f\u5b66\u5230\u9ad8\u4e2d\u6c34\u5e73\uff09\u3002", "result": "\u8bc4\u4f30\u4e8614\u4e2a\u9886\u5148\u7684LLM\u5728\u96f6\u6837\u672c\u548c\u5c0f\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u4e8b\u5b9e\u6027\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8bfe\u5802\u5bf9\u8bdd\u5206\u7c7b\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u4e14\u5728\u521b\u9020\u6027\u5185\u5bb9\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u4e0d\u4e00\u81f4\u7684\u7ed3\u679c\u3002\u4e00\u4e9b\u5f00\u6e90\u6a21\u578b\u5728\u590d\u6742\u7684\u6559\u80b2\u63a8\u7406\u65b9\u9762\u4f18\u4e8e\u4e13\u6709\u7cfb\u7edf\u3002\u5c0f\u6837\u672cprompting\u5728\u4e0d\u540c\u8ba4\u77e5\u7ef4\u5ea6\u4e0a\u7684\u6709\u6548\u6027\u5404\u4e0d\u76f8\u540c\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u5f00\u53d1\u4e13\u95e8\u9488\u5bf9\u4e0d\u540c\u4e2d\u56fd\u6559\u80b2\u4efb\u52a1\u8fdb\u884c\u4f18\u5316\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u9488\u5bf9\u6027\u7684\u57fa\u51c6\u6307\u6807\u3002"}}
{"id": "2512.00229", "categories": ["cs.LG", "cs.CV", "eess.IV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.00229", "abs": "https://arxiv.org/abs/2512.00229", "authors": ["Pirzada Suhail", "Rehna Afroz", "Amit Sethi"], "title": "TIE: A Training-Inversion-Exclusion Framework for Visually Interpretable and Uncertainty-Guided Out-of-Distribution Detection", "comment": null, "summary": "Deep neural networks often struggle to recognize when an input lies outside their training experience, leading to unreliable and overconfident predictions. Building dependable machine learning systems therefore requires methods that can both estimate predictive \\textit{uncertainty} and detect \\textit{out-of-distribution (OOD)} samples in a unified manner. In this paper, we propose \\textbf{TIE: a Training--Inversion--Exclusion} framework for visually interpretable and uncertainty-guided anomaly detection that jointly addresses these challenges through iterative refinement. TIE extends a standard $n$-class classifier to an $(n+1)$-class model by introducing a garbage class initialized with Gaussian noise to represent outlier inputs. Within each epoch, TIE performs a closed-loop process of \\textit{training, inversion, and exclusion}, where highly uncertain inverted samples reconstructed from the just-trained classifier are excluded into the garbage class. Over successive iterations, the inverted samples transition from noisy artifacts into visually coherent class prototypes, providing transparent insight into how the model organizes its learned manifolds. During inference, TIE rejects OOD inputs by either directly mapping them to the garbage class or producing low-confidence, uncertain misclassifications within the in-distribution classes that are easily separable, all without relying on external OOD datasets. A comprehensive threshold-based evaluation using multiple OOD metrics and performance measures such as \\textit{AUROC}, \\textit{AUPR}, and \\textit{FPR@95\\%TPR} demonstrates that TIE offers a unified and interpretable framework for robust anomaly detection and calibrated uncertainty estimation (UE) achieving near-perfect OOD detection with \\textbf{\\(\\!\\approx\\!\\) 0 FPR@95\\%TPR} when trained on MNIST or FashionMNIST and tested against diverse unseen datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTIE\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u548c\u68c0\u6d4b\u5206\u5e03\u5916(OOD)\u6837\u672c\u7684\u95ee\u9898\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u8bc6\u522b\u8d85\u51fa\u5176\u8bad\u7ec3\u7ecf\u9a8c\u7684\u8f93\u5165\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u9884\u6d4b\u4e0d\u53ef\u9760\u4e14\u8fc7\u5ea6\u81ea\u4fe1\u3002\u56e0\u6b64\uff0c\u6784\u5efa\u53ef\u9760\u7684\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u9700\u8981\u80fd\u591f\u4ee5\u7edf\u4e00\u7684\u65b9\u5f0f\u4f30\u8ba1\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u548c\u68c0\u6d4b\u5206\u5e03\u5916(OOD)\u6837\u672c\u7684\u65b9\u6cd5\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8fed\u4ee3\u7ec6\u5316\uff0c\u6269\u5c55\u4e86\u4e00\u4e2a\u6807\u51c6\u7684n\u7c7b\u5206\u7c7b\u5668\u5230\u4e00\u4e2a(n+1)\u7c7b\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u7528\u9ad8\u65af\u566a\u58f0\u521d\u59cb\u5316\u7684\u5783\u573e\u7c7b\u6765\u8868\u793a\u5f02\u5e38\u503c\u8f93\u5165\u3002\u5728\u6bcf\u4e2aepoch\u4e2d\uff0c\u6267\u884c\u8bad\u7ec3\u3001\u53cd\u6f14\u548c\u6392\u9664\u7684\u95ed\u73af\u8fc7\u7a0b\uff0c\u5176\u4e2d\u4ece\u521a\u521a\u8bad\u7ec3\u7684\u5206\u7c7b\u5668\u91cd\u5efa\u7684\u9ad8\u5ea6\u4e0d\u786e\u5b9a\u7684\u53cd\u6f14\u6837\u672c\u88ab\u6392\u9664\u5230\u5783\u573e\u7c7b\u4e2d\u3002", "result": "\u5728MNIST\u6216FashionMNIST\u4e0a\u8bad\u7ec3\u5e76\u5728\u4e0d\u540c\u7684\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u65f6\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u5b8c\u7f8e\u7684OOD\u68c0\u6d4b\u3002", "conclusion": "TIE\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u4e14\u53ef\u89e3\u91ca\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u7a33\u5065\u7684\u5f02\u5e38\u68c0\u6d4b\u548c\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1(UE)\u3002"}}
{"id": "2512.00078", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00078", "abs": "https://arxiv.org/abs/2512.00078", "authors": ["Mario de Jesus da Graca", "J\u00f6rg Dahlkemper", "Peer Stelldinger"], "title": "Diffusion-Based Synthetic Brightfield Microscopy Images for Enhanced Single Cell Detection", "comment": null, "summary": "Accurate single cell detection in brightfield microscopy is crucial for biological research, yet data scarcity and annotation bottlenecks limit the progress of deep learning methods. We investigate the use of unconditional models to generate synthetic brightfield microscopy images and evaluate their impact on object detection performance. A U-Net based diffusion model was trained and used to create datasets with varying ratios of synthetic and real images. Experiments with YOLOv8, YOLOv9 and RT-DETR reveal that training with synthetic data can achieve improved detection accuracies (at minimal costs). A human expert survey demonstrates the high realism of generated images, with experts not capable to distinguish them from real microscopy images (accuracy 50%). Our findings suggest that diffusion-based synthetic data generation is a promising avenue for augmenting real datasets in microscopy image analysis, reducing the reliance on extensive manual annotation and potentially improving the robustness of cell detection models.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u4f7f\u7528\u65e0\u6761\u4ef6\u6a21\u578b\u751f\u6210\u5408\u6210\u660e\u573a\u663e\u5fae\u955c\u56fe\u50cf\uff0c\u5e76\u8bc4\u4f30\u5b83\u4eec\u5bf9\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u5728\u660e\u573a\u663e\u5fae\u955c\u4e2d\u8fdb\u884c\u7cbe\u786e\u7684\u5355\u7ec6\u80de\u68c0\u6d4b\u5bf9\u4e8e\u751f\u7269\u7814\u7a76\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6570\u636e\u7a00\u7f3a\u548c\u6ce8\u91ca\u74f6\u9888\u9650\u5236\u4e86\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u8fdb\u5c55\u3002", "method": "\u8bad\u7ec3\u4e86\u4e00\u4e2a\u57fa\u4e8e U-Net \u7684\u6269\u6563\u6a21\u578b\uff0c\u5e76\u7528\u5b83\u6765\u521b\u5efa\u5177\u6709\u4e0d\u540c\u5408\u6210\u56fe\u50cf\u548c\u771f\u5b9e\u56fe\u50cf\u6bd4\u4f8b\u7684\u6570\u636e\u96c6\u3002\u4f7f\u7528 YOLOv8\u3001YOLOv9 \u548c RT-DETR \u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002", "result": "\u4f7f\u7528\u5408\u6210\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u53ef\u4ee5\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\uff08\u6210\u672c \u043c\u0438\u043d\u0438\u043c\u0430\u043b\u044c\uff09\u3002\u4eba\u5de5\u4e13\u5bb6\u8c03\u67e5\u8868\u660e\uff0c\u751f\u6210\u7684\u56fe\u50cf\u5177\u6709\u5f88\u9ad8\u7684\u771f\u5b9e\u611f\uff0c\u4e13\u5bb6\u65e0\u6cd5\u5c06\u5b83\u4eec\u4e0e\u771f\u5b9e\u7684\u663e\u5fae\u955c\u56fe\u50cf\u533a\u5206\u5f00\u6765\uff08\u51c6\u786e\u7387 50%\uff09\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u6269\u6563\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u662f\u589e\u5f3a\u663e\u5fae\u955c\u56fe\u50cf\u5206\u6790\u4e2d\u771f\u5b9e\u6570\u636e\u96c6\u7684\u4e00\u79cd\u6709\u524d\u9014\u7684\u9014\u5f84\uff0c\u51cf\u5c11\u4e86\u5bf9\u5927\u91cf\u624b\u52a8\u6ce8\u91ca\u7684\u4f9d\u8d56\uff0c\u5e76\u6709\u53ef\u80fd\u63d0\u9ad8\u7ec6\u80de\u68c0\u6d4b\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2512.01372", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01372", "abs": "https://arxiv.org/abs/2512.01372", "authors": ["Wei Yang", "Rui Zhong", "Yiqun Chen", "Chi Lu", "Peng Jiang"], "title": "Structured Spectral Reasoning for Frequency-Adaptive Multimodal Recommendation", "comment": null, "summary": "Multimodal recommendation aims to integrate collaborative signals with heterogeneous content such as visual and textual information, but remains challenged by modality-specific noise, semantic inconsistency, and unstable propagation over user-item graphs. These issues are often exacerbated by naive fusion or shallow modeling strategies, leading to degraded generalization and poor robustness. While recent work has explored the frequency domain as a lens to separate stable from noisy signals, most methods rely on static filtering or reweighting, lacking the ability to reason over spectral structure or adapt to modality-specific reliability. To address these challenges, we propose a Structured Spectral Reasoning (SSR) framework for frequency-aware multimodal recommendation. Our method follows a four-stage pipeline: (i) Decompose graph-based multimodal signals into spectral bands via graph-guided transformations to isolate semantic granularity; (ii) Modulate band-level reliability with spectral band masking, a training-time masking with a prediction-consistency objective that suppresses brittle frequency components; (iii) Fuse complementary frequency cues using hyperspectral reasoning with low-rank cross-band interaction; and (iv) Align modality-specific spectral features via contrastive regularization to promote semantic and structural consistency. Experiments on three real-world benchmarks show consistent gains over strong baselines, particularly under sparse and cold-start settings. Additional analyses indicate that structured spectral modeling improves robustness and provides clearer diagnostics of how different bands contribute to performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u9891\u7387\u611f\u77e5\u591a\u6a21\u6001\u63a8\u8350\u7684\u7ed3\u6784\u5316\u9891\u8c31\u63a8\u7406 (SSR) \u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u6a21\u6001\u7279\u5b9a\u566a\u58f0\u3001\u8bed\u4e49\u4e0d\u4e00\u81f4\u548c\u7528\u6237-\u9879\u76ee\u56fe\u4e0a\u7684\u4e0d\u7a33\u5b9a\u4f20\u64ad\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u63a8\u8350\u65b9\u6cd5\u5bb9\u6613\u53d7\u5230\u6a21\u6001\u7279\u5b9a\u566a\u58f0\u3001\u8bed\u4e49\u4e0d\u4e00\u81f4\u548c\u7528\u6237-\u9879\u76ee\u56fe\u4e0a\u7684\u4e0d\u7a33\u5b9a\u4f20\u64ad\u7684\u5f71\u54cd\uff0c\u5e76\u4e14\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u9c81\u68d2\u6027\u4f4e\u3002", "method": "\u8be5\u65b9\u6cd5\u5305\u542b\u56db\u4e2a\u9636\u6bb5\uff1a(i) \u901a\u8fc7\u56fe\u5f15\u5bfc\u53d8\u6362\u5c06\u57fa\u4e8e\u56fe\u7684\u591a\u6a21\u6001\u4fe1\u53f7\u5206\u89e3\u4e3a\u9891\u8c31\u5e26\uff1b(ii) \u4f7f\u7528\u9891\u8c31\u5e26\u63a9\u853d\u6765\u8c03\u8282\u9891\u5e26\u7ea7\u522b\u7684\u53ef\u9760\u6027\uff1b(iii) \u4f7f\u7528\u5177\u6709\u4f4e\u79e9\u8de8\u9891\u5e26\u4ea4\u4e92\u7684\u8d85\u8c31\u63a8\u7406\u878d\u5408\u4e92\u8865\u9891\u7387\u7ebf\u7d22\uff1b(iv) \u901a\u8fc7\u5bf9\u6bd4\u6b63\u5219\u5316\u5bf9\u9f50\u6a21\u6001\u7279\u5b9a\u7684\u9891\u8c31\u7279\u5f81\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u5bf9\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\u65b9\u6cd5\u59cb\u7ec8\u80fd\u591f\u53d6\u5f97\u66f4\u597d\u7684\u7ed3\u679c\uff0c\u5c24\u5176\u662f\u5728\u7a00\u758f\u548c\u51b7\u542f\u52a8\u8bbe\u7f6e\u4e0b\u3002", "conclusion": "\u7ed3\u6784\u5316\u9891\u8c31\u5efa\u6a21\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5173\u4e8e\u4e0d\u540c\u9891\u5e26\u5982\u4f55\u6709\u52a9\u4e8e\u6027\u80fd\u7684\u66f4\u6e05\u6670\u7684\u8bca\u65ad\u3002"}}
{"id": "2512.00344", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00344", "abs": "https://arxiv.org/abs/2512.00344", "authors": ["Naifan Zhang", "Ruihan Sun", "Ruixi Su", "Shiqi Ma", "Shiya Zhang", "Xianna Weng", "Xiaofan Zhang", "Yuhan Zhan", "Yuyang Xu", "Zhaohan Chen", "Zhengyuan Pan", "Ziyi Song"], "title": "Echo-N1: Affective RL Frontier", "comment": null, "summary": "The LLM field has spent a year perfecting RL for tasks machines already excel at, math, code, and deterministic reasoning, while completely sidestepping the domain that actually defines human intelligence: subjective, emotionally grounded, personality sensitive conversation. This space has often been regarded as inherently subjective and challenging to formalize, making it appear unsuitable for conventional RL pipelines. We show that it is not only possible and it is a solvable and transformative RL problem. We propose the first framework that infers user personality on the fly and optimizes model behavior toward personalized conversational preferences. Contrary to the widespread belief that RL collapses in non-verifiable settings, our method produces consistent, robust, and dramatic improvements in humanlike interaction quality. We also introduce the first dynamic emotional intelligence evaluation suite to quantify these gains. Our model, which is introduced as Echo-N1, behaves far above its base version and outperforming the proprietary Doubao 1.5 Character. This work establishes a new frontier for RL: optimizing models for the deeply subjective, deeply human dimensions of conversation.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684RL\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u5177\u6709\u4e3b\u89c2\u6027\u3001\u60c5\u611f\u548c\u4e2a\u6027\u5316\u7279\u5f81\u7684\u5bf9\u8bdd\u6a21\u578b\u3002", "motivation": "\u73b0\u6709LLM\u9886\u57df\u5728\u673a\u5668\u64c5\u957f\u7684\u4efb\u52a1\uff08\u5982\u6570\u5b66\u3001\u4ee3\u7801\u548c\u786e\u5b9a\u6027\u63a8\u7406\uff09\u4e0a\u8fc7\u5ea6\u4f7f\u7528RL\uff0c\u800c\u5ffd\u7565\u4e86\u4eba\u7c7b\u667a\u80fd\u7684\u5173\u952e\u9886\u57df\uff1a\u4e3b\u89c2\u3001\u60c5\u611f\u5316\u548c\u4e2a\u6027\u5316\u7684\u5bf9\u8bdd\u3002\u4f20\u7edf\u89c2\u70b9\u8ba4\u4e3a\u8fd9\u4e9b\u9886\u57df\u96be\u4ee5\u5f62\u5f0f\u5316\uff0c\u4e0d\u9002\u5408RL\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u53ef\u4ee5\u52a8\u6001\u63a8\u65ad\u7528\u6237\u4e2a\u6027\uff0c\u5e76\u6839\u636e\u4e2a\u6027\u5316\u7684\u5bf9\u8bdd\u504f\u597d\u4f18\u5316\u6a21\u578b\u884c\u4e3a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7c7b\u4eba\u4ea4\u4e92\u8d28\u91cf\u65b9\u9762\u4ea7\u751f\u4e86\u6301\u7eed\u3001\u7a33\u5065\u548c\u663e\u8457\u7684\u6539\u8fdb\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u548cDoubao 1.5 Character\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3aRL\u5f00\u8f9f\u4e86\u4e00\u4e2a\u65b0\u7684\u524d\u6cbf\uff1a\u4f18\u5316\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u5bf9\u8bdd\u4e2d\u6df1\u523b\u7684\u4e3b\u89c2\u548c\u4eba\u6027\u5316\u7ef4\u5ea6\u3002"}}
{"id": "2512.00323", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00323", "abs": "https://arxiv.org/abs/2512.00323", "authors": ["Muhammad Muneeb", "David B. Ascher", "Ahsan Baidar Bakht"], "title": "Comparative Analysis of 47 Context-Based Question Answer Models Across 8 Diverse Datasets", "comment": null, "summary": "Context-based question answering (CBQA) models provide more accurate and relevant answers by considering the contextual information. They effectively extract specific information given a context, making them functional in various applications involving user support, information retrieval, and educational platforms. In this manuscript, we benchmarked the performance of 47 CBQA models from Hugging Face on eight different datasets. This study aims to identify the best-performing model across diverse datasets without additional fine-tuning. It is valuable for practical applications where the need to retrain models for specific datasets is minimized, streamlining the implementation of these models in various contexts. The best-performing models were trained on the SQuAD v2 or SQuAD v1 datasets. The best-performing model was ahotrod/electra_large_discriminator_squad2_512, which yielded 43\\% accuracy across all datasets. We observed that the computation time of all models depends on the context length and the model size. The model's performance usually decreases with an increase in the answer length. Moreover, the model's performance depends on the context complexity. We also used the Genetic algorithm to improve the overall accuracy by integrating responses from other models. ahotrod/electra_large_discriminator_squad2_512 generated the best results for bioasq10b-factoid (65.92\\%), biomedical\\_cpgQA (96.45\\%), QuAC (11.13\\%), and Question Answer Dataset (41.6\\%). Bert-large-uncased-whole-word-masking-finetuned-squad achieved an accuracy of 82\\% on the IELTS dataset.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6765\u81ea Hugging Face \u7684 47 \u4e2a CBQA \u6a21\u578b\u5728\u516b\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u786e\u5b9a\u5728\u65e0\u9700\u989d\u5916\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u8de8\u4e0d\u540c\u6570\u636e\u96c6\u8868\u73b0\u6700\u4f73\u7684\u6a21\u578b\u3002", "motivation": "\u5728\u5404\u79cd\u5e94\u7528\u7a0b\u5e8f\u4e2d\uff0c\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u95ee\u7b54 (CBQA) \u6a21\u578b\u901a\u8fc7\u8003\u8651\u4e0a\u4e0b\u6587\u4fe1\u606f\u6765\u63d0\u4f9b\u66f4\u51c6\u786e\u548c\u76f8\u5173\u7684\u7b54\u6848\u3002\u672c\u7814\u7a76\u65e8\u5728\u627e\u5230\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6700\u597d\u7684\u6a21\u578b\uff0c\u4ece\u800c\u51cf\u5c11\u4e3a\u7279\u5b9a\u6570\u636e\u96c6\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u7684\u9700\u6c42\u3002", "method": "\u5bf9 47 \u4e2a Hugging Face \u7684 CBQA \u6a21\u578b\u5728\u516b\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u4f7f\u7528\u9057\u4f20\u7b97\u6cd5\u6765\u63d0\u9ad8\u6574\u4f53\u51c6\u786e\u6027\u3002", "result": "ahotrod/electra_large_discriminator_squad2_512 \u6a21\u578b\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u6574\u4f53\u51c6\u786e\u7387\u4e3a 43%\u3002\u8ba1\u7b97\u65f6\u95f4\u53d6\u51b3\u4e8e\u4e0a\u4e0b\u6587\u957f\u5ea6\u548c\u6a21\u578b\u5927\u5c0f\uff0c\u6a21\u578b\u6027\u80fd\u901a\u5e38\u968f\u7740\u7b54\u6848\u957f\u5ea6\u7684\u589e\u52a0\u800c\u964d\u4f4e\u3002\u9057\u4f20\u7b97\u6cd5\u4e5f\u88ab\u7528\u4e8e\u63d0\u9ad8\u6574\u4f53\u51c6\u786e\u6027\u3002", "conclusion": "\u6700\u4f73\u6a21\u578b\u662f\u5728 SQuAD v2 \u6216 SQuAD v1 \u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u3002"}}
{"id": "2512.00239", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.00239", "abs": "https://arxiv.org/abs/2512.00239", "authors": ["Yenho Chen", "Maxwell A. Xu", "James M. Rehg", "Christopher J. Rozell"], "title": "Self-Supervised Dynamical System Representations for Physiological Time-Series", "comment": null, "summary": "The effectiveness of self-supervised learning (SSL) for physiological time series depends on the ability of a pretraining objective to preserve information about the underlying physiological state while filtering out unrelated noise. However, existing strategies are limited due to reliance on heuristic principles or poorly constrained generative tasks. To address this limitation, we propose a pretraining framework that exploits the information structure of a dynamical systems generative model across multiple time-series. This framework reveals our key insight that class identity can be efficiently captured by extracting information about the generative variables related to the system parameters shared across similar time series samples, while noise unique to individual samples should be discarded. Building on this insight, we propose PULSE, a cross-reconstruction-based pretraining objective for physiological time series datasets that explicitly extracts system information while discarding non-transferrable sample-specific ones. We establish theory that provides sufficient conditions for the system information to be recovered, and empirically validate it using a synthetic dynamical systems experiment. Furthermore, we apply our method to diverse real-world datasets, demonstrating that PULSE learns representations that can broadly distinguish semantic classes, increase label efficiency, and improve transfer learning.", "AI": {"tldr": "\u63d0\u51fa PULSE\uff0c\u4e00\u79cd\u57fa\u4e8e\u4ea4\u53c9\u91cd\u5efa\u7684\u751f\u7406\u65f6\u95f4\u5e8f\u5217\u9884\u8bad\u7ec3\u76ee\u6807\uff0c\u663e\u5f0f\u63d0\u53d6\u7cfb\u7edf\u4fe1\u606f\u5e76\u4e22\u5f03\u4e0d\u53ef\u8f6c\u79fb\u7684\u6837\u672c\u7279\u5b9a\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u81ea\u76d1\u7763\u5b66\u4e60 (SSL) \u7b56\u7565\u4f9d\u8d56\u4e8e\u542f\u53d1\u5f0f\u539f\u5219\u6216\u7ea6\u675f\u4e0d\u4f73\u7684\u751f\u6210\u4efb\u52a1\uff0c\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u8de8\u591a\u4e2a\u65f6\u95f4\u5e8f\u5217\u7684\u52a8\u6001\u7cfb\u7edf\u751f\u6210\u6a21\u578b\u7684\u4fe1\u606f\u7ed3\u6784\u3002\u8be5\u6846\u67b6\u63d0\u53d6\u4e0e\u8de8\u76f8\u4f3c\u65f6\u95f4\u5e8f\u5217\u6837\u672c\u5171\u4eab\u7684\u7cfb\u7edf\u53c2\u6570\u76f8\u5173\u7684\u751f\u6210\u53d8\u91cf\u7684\u4fe1\u606f\uff0c\u540c\u65f6\u4e22\u5f03\u7279\u5b9a\u4e8e\u5355\u4e2a\u6837\u672c\u7684\u566a\u58f0\u3002", "result": "\u901a\u8fc7\u5408\u6210\u52a8\u6001\u7cfb\u7edf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u7406\u8bba\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u5404\u79cd\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\uff0c\u8bc1\u660e PULSE \u5b66\u4e60\u7684\u8868\u793a\u53ef\u4ee5\u5e7f\u6cdb\u533a\u5206\u8bed\u4e49\u7c7b\uff0c\u63d0\u9ad8\u6807\u7b7e\u6548\u7387\u5e76\u6539\u5584\u8fc1\u79fb\u5b66\u4e60\u3002", "conclusion": "PULSE \u662f\u4e00\u79cd\u6709\u6548\u7684\u751f\u7406\u65f6\u95f4\u5e8f\u5217\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5b83\u80fd\u591f\u63d0\u53d6\u7cfb\u7edf\u4fe1\u606f\u5e76\u63d0\u9ad8\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2512.00080", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.00080", "abs": "https://arxiv.org/abs/2512.00080", "authors": ["Andr\u00e9 Dehne", "Juri Zach", "Peer Stelldinger"], "title": "Conceptual Evaluation of Deep Visual Stereo Odometry for the MARWIN Radiation Monitoring Robot in Accelerator Tunnels", "comment": null, "summary": "The MARWIN robot operates at the European XFEL to perform autonomous radiation monitoring in long, monotonous accelerator tunnels where conventional localization approaches struggle. Its current navigation concept combines lidar-based edge detection, wheel/lidar odometry with periodic QR-code referencing, and fuzzy control of wall distance, rotation, and longitudinal position. While robust in predefined sections, this design lacks flexibility for unknown geometries and obstacles. This paper explores deep visual stereo odometry (DVSO) with 3D-geometric constraints as a focused alternative. DVSO is purely vision-based, leveraging stereo disparity, optical flow, and self-supervised learning to jointly estimate depth and ego-motion without labeled data. For global consistency, DVSO can subsequently be fused with absolute references (e.g., landmarks) or other sensors. We provide a conceptual evaluation for accelerator tunnel environments, using the European XFEL as a case study. Expected benefits include reduced scale drift via stereo, low-cost sensing, and scalable data collection, while challenges remain in low-texture surfaces, lighting variability, computational load, and robustness under radiation. The paper defines a research agenda toward enabling MARWIN to navigate more autonomously in constrained, safety-critical infrastructures.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u4f7f\u7528\u6df1\u5ea6\u89c6\u89c9\u7acb\u4f53\u91cc\u7a0b\u8ba1\uff08DVSO\uff09\u589e\u5f3a MARWIN \u673a\u5668\u4eba\u5728\u6b27\u6d32 XFEL \u52a0\u901f\u5668\u96a7\u9053\u4e2d\u81ea\u4e3b\u5bfc\u822a\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684 MARWIN \u5bfc\u822a\u7cfb\u7edf\u5728\u672a\u77e5\u73af\u5883\u548c\u969c\u788d\u7269\u9762\u524d\u7f3a\u4e4f\u7075\u6d3b\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u7acb\u4f53\u89c6\u89c9\u7684 DVSO \u65b9\u6cd5\uff0c\u5229\u7528\u7acb\u4f53\u89c6\u5dee\u3001\u5149\u6d41\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u8054\u5408\u4f30\u8ba1\u6df1\u5ea6\u548c\u81ea\u8eab\u8fd0\u52a8\u3002", "result": "\u5bf9\u52a0\u901f\u5668\u96a7\u9053\u73af\u5883\u8fdb\u884c\u4e86\u6982\u5ff5\u6027\u8bc4\u4f30\uff0c\u786e\u5b9a\u4e86\u4f7f\u7528 DVSO \u7684\u6f5c\u5728\u4f18\u52bf\uff08\u51cf\u5c11\u5c3a\u5ea6\u6f02\u79fb\u3001\u4f4e\u6210\u672c\u4f20\u611f\u3001\u53ef\u6269\u5c55\u6570\u636e\u6536\u96c6\uff09\u548c\u6311\u6218\uff08\u4f4e\u7eb9\u7406\u8868\u9762\u3001\u5149\u7167\u53d8\u5316\u3001\u8ba1\u7b97\u8d1f\u8f7d\u3001\u8f90\u5c04\u4e0b\u7684\u9c81\u68d2\u6027\uff09\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a MARWIN \u5728\u53d7\u9650\u3001\u5b89\u5168\u5173\u952e\u7684\u57fa\u7840\u8bbe\u65bd\u4e2d\u5b9e\u73b0\u66f4\u81ea\u4e3b\u7684\u5bfc\u822a\u5b9a\u4e49\u4e86\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2512.00329", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.00329", "abs": "https://arxiv.org/abs/2512.00329", "authors": ["Ashish Thanga", "Vibhu Dixit", "Abhilash Shankarampeta", "Vivek Gupta"], "title": "Evidence-Guided Schema Normalization for Temporal Tabular Reasoning", "comment": null, "summary": "Temporal reasoning over evolving semi-structured tables poses a challenge to current QA systems. We propose a SQL-based approach that involves (1) generating a 3NF schema from Wikipedia infoboxes, (2) generating SQL queries, and (3) query execution. Our central finding challenges model scaling assumptions: the quality of schema design has a greater impact on QA precision than model capacity. We establish three evidence-based principles: normalization that preserves context, semantic naming that reduces ambiguity, and consistent temporal anchoring. Our best configuration (Gemini 2.5 Flash schema + Gemini-2.0-Flash queries) achieves 80.39 EM, a 16.8\\% improvement over the baseline (68.89 EM).", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSQL\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u4e0d\u65ad\u53d8\u5316\u7684\u534a\u7ed3\u6784\u5316\u8868\u683c\u4e0a\u7684\u65f6\u95f4\u63a8\u7406\u95ee\u7b54\u4efb\u52a1\u3002", "motivation": "\u5f53\u524d\u95ee\u7b54\u7cfb\u7edf\u5728\u5904\u7406\u4e0d\u65ad\u53d8\u5316\u7684\u534a\u7ed3\u6784\u5316\u8868\u683c\u4e0a\u7684\u65f6\u95f4\u63a8\u7406\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002", "method": "\u8be5\u65b9\u6cd5\u5305\u62ec\uff1a(1) \u4ece\u7ef4\u57fa\u767e\u79d1\u4fe1\u606f\u6846\u751f\u62103NF\u6a21\u5f0f\uff1b(2) \u751f\u6210SQL\u67e5\u8be2\uff1b(3) \u67e5\u8be2\u6267\u884c\u3002", "result": "\u6700\u597d\u7684\u914d\u7f6e\uff08Gemini 2.5 Flash schema + Gemini-2.0-Flash \u67e5\u8be2\uff09\u8fbe\u5230\u4e8680.39 EM\uff0c\u6bd4\u57fa\u7ebf\u63d0\u9ad8\u4e8616.8%\u3002", "conclusion": "\u6a21\u5f0f\u8bbe\u8ba1\u7684\u8d28\u91cf\u5bf9\u95ee\u7b54\u7cbe\u786e\u5ea6\u7684\u5f71\u54cd\u5927\u4e8e\u6a21\u578b\u5bb9\u91cf\u3002\u5efa\u7acb\u4e86\u4e09\u4e2a\u57fa\u4e8e\u8bc1\u636e\u7684\u539f\u5219\uff1a\u4fdd\u6301\u4e0a\u4e0b\u6587\u7684\u89c4\u8303\u5316\u3001\u51cf\u5c11\u6b67\u4e49\u7684\u8bed\u4e49\u547d\u540d\u548c\u4e00\u81f4\u7684\u65f6\u95f4\u951a\u5b9a\u3002"}}
{"id": "2512.00349", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00349", "abs": "https://arxiv.org/abs/2512.00349", "authors": ["Sitong Fang", "Shiyi Hou", "Kaile Wang", "Boyuan Chen", "Donghai Hong", "Jiayi Zhou", "Josef Dai", "Yaodong Yang", "Jiaming Ji"], "title": "Debate with Images: Detecting Deceptive Behaviors in Multimodal Large Language Models", "comment": null, "summary": "Are frontier AI systems becoming more capable? Certainly. Yet such progress is not an unalloyed blessing but rather a Trojan horse: behind their performance leaps lie more insidious and destructive safety risks, namely deception. Unlike hallucination, which arises from insufficient capability and leads to mistakes, deception represents a deeper threat in which models deliberately mislead users through complex reasoning and insincere responses. As system capabilities advance, deceptive behaviours have spread from textual to multimodal settings, amplifying their potential harm. First and foremost, how can we monitor these covert multimodal deceptive behaviors? Nevertheless, current research remains almost entirely confined to text, leaving the deceptive risks of multimodal large language models unexplored. In this work, we systematically reveal and quantify multimodal deception risks, introducing MM-DeceptionBench, the first benchmark explicitly designed to evaluate multimodal deception. Covering six categories of deception, MM-DeceptionBench characterizes how models strategically manipulate and mislead through combined visual and textual modalities. On the other hand, multimodal deception evaluation is almost a blind spot in existing methods. Its stealth, compounded by visual-semantic ambiguity and the complexity of cross-modal reasoning, renders action monitoring and chain-of-thought monitoring largely ineffective. To tackle this challenge, we propose debate with images, a novel multi-agent debate monitor framework. By compelling models to ground their claims in visual evidence, this method substantially improves the detectability of deceptive strategies. Experiments show that it consistently increases agreement with human judgements across all tested models, boosting Cohen's kappa by 1.5x and accuracy by 1.25x on GPT-4o.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e86\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u5b58\u5728\u7684\u6b3a\u9a97\u98ce\u9669\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aMM-DeceptionBench\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u8fd9\u4e9b\u98ce\u9669\u3002\u540c\u65f6\uff0c\u8bba\u6587\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u56fe\u50cf\u8fa9\u8bba\u201d\u7684\u65b0\u578b\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u76d1\u63a7\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u6b3a\u9a97\u884c\u4e3a\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u6587\u672c\u6b3a\u9a97\u4e0a\uff0c\u800c\u5ffd\u7565\u4e86\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u5b58\u5728\u7684\u6b3a\u9a97\u98ce\u9669\u3002\u8fd9\u4e9b\u98ce\u9669\u53ef\u80fd\u4f1a\u5229\u7528\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u7684\u7ec4\u5408\u6765\u7b56\u7565\u6027\u5730\u64cd\u7eb5\u548c\u8bef\u5bfc\u7528\u6237\u3002", "method": "\u8bba\u6587\u5f15\u5165\u4e86MM-DeceptionBench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u6b3a\u9a97\u3002\u6b64\u5916\uff0c\u8bba\u6587\u8fd8\u63d0\u51fa\u4e86\u201c\u56fe\u50cf\u8fa9\u8bba\u201d\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u8feb\u4f7f\u6a21\u578b\u5c06\u5176\u58f0\u660e\u5efa\u7acb\u5728\u89c6\u89c9\u8bc1\u636e\u7684\u57fa\u7840\u4e0a\uff0c\u6765\u63d0\u9ad8\u6b3a\u9a97\u7b56\u7565\u7684\u53ef\u68c0\u6d4b\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u201c\u56fe\u50cf\u8fa9\u8bba\u201d\u6846\u67b6\u5728\u6240\u6709\u6d4b\u8bd5\u6a21\u578b\u4e0a\u90fd\u6301\u7eed\u63d0\u9ad8\u4e86\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u4e00\u81f4\u6027\uff0c\u5728GPT-4o\u4e0a\u5c06Cohen's kappa\u63d0\u9ad8\u4e861.5\u500d\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u4e861.25\u500d\u3002", "conclusion": "\u591a\u6a21\u6001\u6b3a\u9a97\u662f\u4e00\u4e2a\u503c\u5f97\u5173\u6ce8\u7684\u95ee\u9898\uff0c\u73b0\u6709\u7684\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u76f2\u70b9\u3002\u63d0\u51fa\u7684\u201c\u56fe\u50cf\u8fa9\u8bba\u201d\u6846\u67b6\u53ef\u4ee5\u6709\u6548\u5730\u63d0\u9ad8\u591a\u6a21\u6001\u6b3a\u9a97\u7684\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2512.00242", "categories": ["cs.LG", "cs.AI", "cs.ET", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.00242", "abs": "https://arxiv.org/abs/2512.00242", "authors": ["Alessio Borgi", "Fabrizio Silvestri", "Pietro Li\u00f2"], "title": "Polynomial Neural Sheaf Diffusion: A Spectral Filtering Approach on Cellular Sheaves", "comment": null, "summary": "Sheaf Neural Networks equip graph structures with a cellular sheaf: a geometric structure which assigns local vector spaces (stalks) and a linear learnable restriction/transport maps to nodes and edges, yielding an edge-aware inductive bias that handles heterophily and limits oversmoothing. However, common Neural Sheaf Diffusion implementations rely on SVD-based sheaf normalization and dense per-edge restriction maps, which scale with stalk dimension, require frequent Laplacian rebuilds, and yield brittle gradients. To address these limitations, we introduce Polynomial Neural Sheaf Diffusion (PolyNSD), a new sheaf diffusion approach whose propagation operator is a degree-K polynomial in a normalised sheaf Laplacian, evaluated via a stable three-term recurrence on a spectrally rescaled operator. This provides an explicit K-hop receptive field in a single layer (independently of the stalk dimension), with a trainable spectral response obtained as a convex mixture of K+1 orthogonal polynomial basis responses. PolyNSD enforces stability via convex mixtures, spectral rescaling, and residual/gated paths, reaching new state-of-the-art results on both homophilic and heterophilic benchmarks, inverting the Neural Sheaf Diffusion trend by obtaining these results with just diagonal restriction maps, decoupling performance from large stalk dimension, while reducing runtime and memory requirements.", "AI": {"tldr": "\u63d0\u51fa\u4e86Polynomial Neural Sheaf Diffusion (PolyNSD)\uff0c\u4e00\u79cd\u65b0\u7684sheaf\u6269\u6563\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u8c31\u91cd\u7f29\u653e\u7b97\u5b50\u4e0a\u8fdb\u884c\u7a33\u5b9a\u7684\u4e09\u9879\u9012\u5f52\u8ba1\u7b97\uff0c\u5176\u4f20\u64ad\u7b97\u5b50\u662f\u5f52\u4e00\u5316sheaf Laplacian\u4e2d\u7684K\u6b21\u591a\u9879\u5f0f\u3002", "motivation": "\u73b0\u6709\u7684\u795e\u7ecfSheaf\u6269\u6563\u5b9e\u73b0\u4f9d\u8d56\u4e8e\u57fa\u4e8eSVD\u7684sheaf\u5f52\u4e00\u5316\u548c\u5bc6\u96c6\u7684per-edge restriction maps\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u968fstalk\u7ef4\u5ea6\u6269\u5c55\uff0c\u9700\u8981\u9891\u7e41\u7684Laplacian\u91cd\u5efa\uff0c\u5e76\u4ea7\u751f\u8106\u5f31\u7684\u68af\u5ea6\u3002", "method": "PolyNSD\u7684\u4f20\u64ad\u7b97\u5b50\u662f\u5f52\u4e00\u5316sheaf Laplacian\u4e2d\u7684K\u6b21\u591a\u9879\u5f0f\uff0c\u901a\u8fc7\u8c31\u91cd\u7f29\u653e\u7b97\u5b50\u4e0a\u7684\u7a33\u5b9a\u4e09\u9879\u9012\u5f52\u8ba1\u7b97\u8fdb\u884c\u8bc4\u4f30\u3002\u5b83\u5728\u5355\u5c42\u4e2d\u63d0\u4f9b\u4e86\u4e00\u4e2a\u663e\u5f0f\u7684K-hop\u611f\u53d7\u91ce\uff08\u72ec\u7acb\u4e8estalk\u7ef4\u5ea6\uff09\uff0c\u5177\u6709\u53ef\u8bad\u7ec3\u7684\u8c31\u54cd\u5e94\uff0c\u8be5\u54cd\u5e94\u4f5c\u4e3aK+1\u4e2a\u6b63\u4ea4\u591a\u9879\u5f0f\u57fa\u54cd\u5e94\u7684\u51f8\u6df7\u5408\u83b7\u5f97\u3002", "result": "PolyNSD\u901a\u8fc7\u51f8\u6df7\u5408\u3001\u8c31\u91cd\u7f29\u653e\u548c\u6b8b\u5dee/\u95e8\u63a7\u8def\u5f84\u6765\u5f3a\u5236\u7a33\u5b9a\u6027\uff0c\u5728\u540c\u8d28\u548c\u5f02\u8d28\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u90fd\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u901a\u8fc7\u4ec5\u4f7f\u7528\u5bf9\u89d2restriction maps\u83b7\u5f97\u8fd9\u4e9b\u7ed3\u679c\uff0c\u4ece\u800c\u626d\u8f6c\u4e86\u795e\u7ecfSheaf\u6269\u6563\u7684\u8d8b\u52bf\uff0c\u5c06\u6027\u80fd\u4e0e\u5927\u7684stalk\u7ef4\u5ea6\u89e3\u8026\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8fd0\u884c\u65f6\u95f4\u548c\u5185\u5b58\u9700\u6c42\u3002", "conclusion": "PolyNSD\u662f\u4e00\u79cd\u6709\u6548\u7684\u795e\u7ecfSheaf\u6269\u6563\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u591a\u9879\u5f0f\u903c\u8fd1\u548c\u8c31\u6280\u672f\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5404\u79cd\u56fe\u5b66\u4e60\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2512.00082", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00082", "abs": "https://arxiv.org/abs/2512.00082", "authors": ["Divendar Murtadak", "Yoon Kim", "Trilokya Akula"], "title": "Exploring Diagnostic Prompting Approach for Multimodal LLM-based Visual Complexity Assessment: A Case Study of Amazon Search Result Pages", "comment": "9 pages, 4 figures, 9 tables. Study on diagnostic prompting for multimodal LLM-based visual complexity assessment of Amazon search result pages", "summary": "This study investigates whether diagnostic prompting can improve Multimodal Large Language Model (MLLM) reliability for visual complexity assessment of Amazon Search Results Pages (SRP). We compare diagnostic prompting with standard gestalt principles-based prompting using 200 Amazon SRP pages and human expert annotations. Diagnostic prompting showed notable improvements in predicting human complexity judgments, with F1-score increasing from 0.031 to 0.297 (+858\\% relative improvement), though absolute performance remains modest (Cohen's $\u03ba$ = 0.071). The decision tree revealed that models prioritize visual design elements (badge clutter: 38.6\\% importance) while humans emphasize content similarity, suggesting partial alignment in reasoning patterns. Failure case analysis reveals persistent challenges in MLLM visual perception, particularly for product similarity and color intensity assessment. Our findings indicate that diagnostic prompting represents a promising initial step toward human-aligned MLLM-based evaluation, though failure cases with consistent human-MLLM disagreement require continued research and refinement in prompting approaches with larger ground truth datasets for reliable practical deployment.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u8bca\u65ad\u6027\u63d0\u793a\u662f\u5426\u53ef\u4ee5\u63d0\u9ad8\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u5728\u8bc4\u4f30\u4e9a\u9a6c\u900a\u641c\u7d22\u7ed3\u679c\u9875\u9762 (SRP) \u89c6\u89c9\u590d\u6742\u6027\u65b9\u9762\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63d0\u9ad8 MLLM \u5728\u8bc4\u4f30\u89c6\u89c9\u590d\u6742\u6027\u65b9\u9762\u7684\u53ef\u9760\u6027\uff0c\u4f7f\u5176\u66f4\u7b26\u5408\u4eba\u7c7b\u7684\u5224\u65ad\u3002", "method": "\u4f7f\u7528 200 \u4e2a\u4e9a\u9a6c\u900a SRP \u9875\u9762\u548c\u4eba\u7c7b\u4e13\u5bb6\u6ce8\u91ca\uff0c\u6bd4\u8f83\u8bca\u65ad\u6027\u63d0\u793a\u4e0e\u57fa\u4e8e\u683c\u5f0f\u5854\u539f\u5219\u7684\u6807\u51c6\u63d0\u793a\u3002", "result": "\u8bca\u65ad\u6027\u63d0\u793a\u5728\u9884\u6d4b\u4eba\u7c7b\u590d\u6742\u6027\u5224\u65ad\u65b9\u9762\u663e\u793a\u51fa\u663e\u7740\u6539\u8fdb\uff0cF1 \u5206\u6570\u4ece 0.031 \u589e\u52a0\u5230 0.297\uff08+858% \u7684\u76f8\u5bf9\u6539\u8fdb\uff09\uff0c\u4f46\u7edd\u5bf9\u6027\u80fd\u4ecd\u7136\u9002\u4e2d\uff08Cohen's $\u03ba$ = 0.071\uff09\u3002\u51b3\u7b56\u6811\u663e\u793a\uff0c\u6a21\u578b\u4f18\u5148\u8003\u8651\u89c6\u89c9\u8bbe\u8ba1\u5143\u7d20\uff08\u5fbd\u7ae0\u6742\u4e71\uff1a38.6% \u7684\u91cd\u8981\u6027\uff09\uff0c\u800c\u4eba\u7c7b\u5f3a\u8c03\u5185\u5bb9\u76f8\u4f3c\u6027\uff0c\u8868\u660e\u63a8\u7406\u6a21\u5f0f\u90e8\u5206\u4e00\u81f4\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8bca\u65ad\u6027\u63d0\u793a\u662f\u671d\u7740\u57fa\u4e8e MLLM \u7684\u4eba\u7c7b\u5bf9\u9f50\u8bc4\u4f30\u8fc8\u51fa\u7684\u6709\u5e0c\u671b\u7684\u521d\u6b65\u6b65\u9aa4\uff0c\u4f46\u6301\u7eed\u5b58\u5728\u4eba\u7c7b\u4e0e MLLM \u4e0d\u4e00\u81f4\u7684\u5931\u8d25\u6848\u4f8b\uff0c\u9700\u8981\u5728\u66f4\u5927\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7ee7\u7eed\u7814\u7a76\u548c\u6539\u8fdb\u63d0\u793a\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u53ef\u9760\u7684\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2512.00390", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.00390", "abs": "https://arxiv.org/abs/2512.00390", "authors": ["Nuo Chen", "Hanpei Fang", "Jiqun Liu", "Wilson Wei", "Tetsuya Sakai", "Xiao-Ming Wu"], "title": "Mitigating the Threshold Priming Effect in Large Language Model-Based Relevance Judgments via Personality Infusing", "comment": null, "summary": "Recent research has explored LLMs as scalable tools for relevance labeling, but studies indicate they are susceptible to priming effects, where prior relevance judgments influence later ones. Although psychological theories link personality traits to such biases, it is unclear whether simulated personalities in LLMs exhibit similar effects. We investigate how Big Five personality profiles in LLMs influence priming in relevance labeling, using multiple LLMs on TREC 2021 and 2022 Deep Learning Track datasets. Our results show that certain profiles, such as High Openness and Low Neuroticism, consistently reduce priming susceptibility. Additionally, the most effective personality in mitigating priming may vary across models and task types. Based on these findings, we propose personality prompting as a method to mitigate threshold priming, connecting psychological evidence with LLM-based evaluation practices.", "AI": {"tldr": "\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u76f8\u5173\u6027\u6807\u6ce8\u4e2d\u53d7\u542f\u52a8\u6548\u5e94\u5f71\u54cd\u7684\u95ee\u9898\uff0c\u5e76\u7814\u7a76\u4e86LLM\u4e2d\u6a21\u62df\u7684\u201c\u5927\u4e94\u201d\u4eba\u683c\u7279\u5f81\u5982\u4f55\u5f71\u54cd\u8fd9\u79cd\u542f\u52a8\u6548\u5e94\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u76f8\u5173\u6027\u6807\u6ce8\u4e2d\u5bb9\u6613\u53d7\u5230\u542f\u52a8\u6548\u5e94\u7684\u5f71\u54cd\uff0c\u5e76\u4e14\u8fd9\u79cd\u6548\u5e94\u53ef\u80fd\u4e0e\u5fc3\u7406\u5b66\u4e2d\u7684\u4eba\u683c\u7279\u8d28\u6709\u5173\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22LLM\u4e2d\u7684\u4eba\u683c\u6a21\u62df\u662f\u5426\u4e5f\u4f1a\u8868\u73b0\u51fa\u7c7b\u4f3c\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5728TREC 2021\u548c2022\u6df1\u5ea6\u5b66\u4e60 \u091f\u094d\u0930\u0948\u0915\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528\u591a\u4e2aLLM\uff0c\u5e76\u6a21\u62df\u201c\u5927\u4e94\u201d\u4eba\u683c\u7279\u5f81\uff0c\u6765\u89c2\u5bdf\u4e0d\u540c\u4eba\u683c\u914d\u7f6e\u7684LLM\u5728\u76f8\u5173\u6027\u6807\u6ce8\u4e2d\u53d7\u542f\u52a8\u6548\u5e94\u5f71\u54cd\u7684\u7a0b\u5ea6\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u67d0\u4e9b\u4eba\u683c\u7279\u5f81\uff0c\u5982\u9ad8\u5f00\u653e\u6027\u548c\u4f4e\u795e\u7ecf\u8d28\uff0c\u80fd\u591f\u6301\u7eed\u964d\u4f4e\u542f\u52a8\u6548\u5e94\u7684\u654f\u611f\u6027\u3002\u6b64\u5916\uff0c\u5728\u51cf\u8f7b\u542f\u52a8\u6548\u5e94\u65b9\u9762\u6700\u6709\u6548\u7684\u4eba\u683c\u7279\u5f81\u53ef\u80fd\u56e0\u6a21\u578b\u548c\u4efb\u52a1\u7c7b\u578b\u7684\u4e0d\u540c\u800c\u6709\u6240\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u4eba\u683cPrompting\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u51cf\u8f7b\u9608\u503c\u542f\u52a8\u6548\u5e94\u7684\u65b9\u6cd5\uff0c\u4ece\u800c\u5c06\u5fc3\u7406\u5b66\u8bc1\u636e\u4e0e\u57fa\u4e8eLLM\u7684\u8bc4\u4f30\u5b9e\u8df5\u8054\u7cfb\u8d77\u6765\u3002"}}
{"id": "2512.00406", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00406", "abs": "https://arxiv.org/abs/2512.00406", "authors": ["Pengyu Zeng", "Yuqin Dai", "Jun Yin", "Jing Zhong", "Ziyang Han", "Chaoyang Shi", "ZhanXiang Jin", "Maowei Jiang", "Yuxing Han", "Shuai Lu"], "title": "GreenPlanner: Practical Floorplan Layout Generation via an Energy-Aware and Function-Feasible Generative Framework", "comment": "11 pages, 6 figures", "summary": "Building design directly affects human well-being and carbon emissions, yet generating spatial-functional and energy-compliant floorplans remains manual, costly, and non-scalable. Existing methods produce visually plausible layouts but frequently violate key constraints, yielding invalid results due to the absence of automated evaluation. We present GreenPlanner, an energy- and functionality-aware generative framework that unifies design evaluation and generation. It consists of a labeled Design Feasibility Dataset for learning constraint priors; a fast Practical Design Evaluator (PDE) for predicting energy performance and spatial-functional validity; a Green Plan Dataset (GreenPD) derived from PDE-guided filtering to pair user requirements with regulation-compliant layouts; and a GreenFlow generator trained on GreenPD with PDE feedback for controllable, regulation-aware generation. Experiments show that GreenPlanner accelerates evaluation by over $10^{5}\\times$ with $>$99% accuracy, eliminates invalid samples, and boosts design efficiency by 87% over professional architects.", "AI": {"tldr": "GreenPlanner\u662f\u4e00\u4e2a\u751f\u6210\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u8bbe\u8ba1\u8bc4\u4f30\u548c\u751f\u6210\uff0c\u4ee5\u89e3\u51b3\u5efa\u7b51\u8bbe\u8ba1\u4e2d\u4eba\u5de5\u3001\u6210\u672c\u9ad8\u3001\u4e0d\u53ef\u6269\u5c55\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u751f\u6210\u89c6\u89c9\u4e0a\u5408\u7406\u7684\u5e03\u5c40\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u81ea\u52a8\u8bc4\u4f30\uff0c\u7ecf\u5e38\u8fdd\u53cd\u5173\u952e\u7ea6\u675f\uff0c\u5bfc\u81f4\u65e0\u6548\u7684\u7ed3\u679c\u3002", "method": "\u8be5\u65b9\u6cd5\u5305\u62ec\uff1a\u4e00\u4e2a\u7528\u4e8e\u5b66\u4e60\u7ea6\u675f\u5148\u9a8c\u7684\u6807\u8bb0\u8bbe\u8ba1\u53ef\u884c\u6027\u6570\u636e\u96c6\uff1b\u4e00\u4e2a\u7528\u4e8e\u9884\u6d4b\u80fd\u6e90\u6027\u80fd\u548c\u7a7a\u95f4\u529f\u80fd\u6709\u6548\u6027\u7684\u5feb\u901f\u5b9e\u7528\u8bbe\u8ba1\u8bc4\u4f30\u5668\uff08PDE\uff09\uff1b\u4e00\u4e2a\u4ecePDE\u5f15\u5bfc\u8fc7\u6ee4\u6d3e\u751f\u7684\u7eff\u8272\u8ba1\u5212\u6570\u636e\u96c6\uff08GreenPD\uff09\uff0c\u7528\u4e8e\u5c06\u7528\u6237\u9700\u6c42\u4e0e\u7b26\u5408\u6cd5\u89c4\u7684\u5e03\u5c40\u914d\u5bf9\uff1b\u4ee5\u53ca\u4e00\u4e2a\u5728GreenPD\u4e0a\u8bad\u7ec3\u7684GreenFlow\u751f\u6210\u5668\uff0c\u5177\u6709PDE\u53cd\u9988\uff0c\u7528\u4e8e\u53ef\u63a7\u7684\u3001\u7b26\u5408\u6cd5\u89c4\u7684\u751f\u6210\u3002", "result": "GreenPlanner\u5c06\u8bc4\u4f30\u901f\u5ea6\u63d0\u9ad8\u4e86\u8d85\u8fc7$10^{5}\\%$\uff0c\u51c6\u786e\u7387\u8d85\u8fc799%\uff0c\u6d88\u9664\u4e86\u65e0\u6548\u6837\u672c\uff0c\u5e76\u5c06\u8bbe\u8ba1\u6548\u7387\u63d0\u9ad8\u4e8687%\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86GreenPlanner\uff0c\u4e00\u4e2a\u80fd\u6e90\u548c\u529f\u80fd\u611f\u77e5\u7684\u751f\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u6709\u6548\u751f\u6210\u7b26\u5408\u7ea6\u675f\u7684\u5efa\u7b51\u8bbe\u8ba1\u65b9\u6848\uff0c\u5e76\u4e14\u6548\u7387\u9ad8\u4e8e\u4e13\u4e1a\u5efa\u7b51\u5e08\u3002"}}
{"id": "2512.00332", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00332", "abs": "https://arxiv.org/abs/2512.00332", "authors": ["Daud Waqas", "Aaryamaan Golthi", "Erika Hayashida", "Huanzhi Mao"], "title": "Assertion-Conditioned Compliance: A Provenance-Aware Vulnerability in Multi-Turn Tool-Calling Agents", "comment": "15 pages (incl. Appendix), 2 figures, 7 tables", "summary": "Multi-turn tool-calling LLMs (models capable of invoking external APIs or tools across several user turns) have emerged as a key feature in modern AI assistants, enabling extended dialogues from benign tasks to critical business, medical, and financial operations. Yet implementing multi-turn pipelines remains difficult for many safety-critical industries due to ongoing concerns regarding model resilience. While standardized benchmarks such as the Berkeley Function-Calling Leaderboard (BFCL) have underpinned confidence concerning advanced function-calling models (like Salesforce's xLAM V2), there is still a lack of visibility into multi-turn conversation-level robustness, especially given their exposure to real-world systems. In this paper, we introduce Assertion-Conditioned Compliance (A-CC), a novel evaluation paradigm for multi-turn function-calling dialogues. A-CC provides holistic metrics that evaluate a model's behavior when confronted with misleading assertions originating from two distinct vectors: (1) user-sourced assertions (USAs), which measure sycophancy toward plausible but misinformed user beliefs, and (2) function-sourced assertions (FSAs), which measure compliance with plausible but contradictory system policies (e.g., stale hints from unmaintained tools). Our results show that models are highly vulnerable to both USA sycophancy and FSA policy conflicts, confirming A-CC as a critical, latent vulnerability in deployed agents.", "AI": {"tldr": "\u591a\u8f6e\u5de5\u5177\u8c03\u7528LLM\u5728\u73b0\u4ee3AI\u52a9\u624b\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u4ecd\u4ee4\u4eba\u62c5\u5fe7\u3002\u672c\u6587\u63d0\u51fa\u4e86Assertion-Conditioned Compliance (A-CC)\u8bc4\u4f30\u8303\u5f0f\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5728\u9762\u5bf9\u8bef\u5bfc\u6027\u65ad\u8a00\u65f6\u7684\u884c\u4e3a\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7f3a\u4e4f\u5bf9\u591a\u8f6e\u5bf9\u8bdd\u9c81\u68d2\u6027\u7684\u53ef\u89c1\u6027\uff0c\u5c24\u5176\u662f\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\u3002\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u7528\u6237\u6765\u6e90\u65ad\u8a00\uff08USA\uff09\u7684\u5949\u627f\u548c\u529f\u80fd\u6765\u6e90\u65ad\u8a00\uff08FSA\uff09\u7684\u7b56\u7565\u51b2\u7a81\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e86Assertion-Conditioned Compliance (A-CC)\u8bc4\u4f30\u8303\u5f0f\uff0c\u8be5\u8303\u5f0f\u901a\u8fc7\u7528\u6237\u6765\u6e90\u65ad\u8a00\uff08USA\uff09\u548c\u529f\u80fd\u6765\u6e90\u65ad\u8a00\uff08FSA\uff09\u6765\u8bc4\u4f30\u6a21\u578b\u7684\u884c\u4e3a\u3002", "result": "\u6a21\u578b\u5728\u7528\u6237\u6765\u6e90\u65ad\u8a00\uff08USA\uff09\u5949\u627f\u548c\u529f\u80fd\u6765\u6e90\u65ad\u8a00\uff08FSA\uff09\u7b56\u7565\u51b2\u7a81\u65b9\u9762\u8868\u73b0\u51fa\u9ad8\u5ea6\u8106\u5f31\u6027\u3002", "conclusion": "A-CC\u662f\u5df2\u90e8\u7f72\u4ee3\u7406\u4e2d\u4e00\u4e2a\u5173\u952e\u7684\u6f5c\u5728\u6f0f\u6d1e\u3002"}}
{"id": "2512.00249", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.00249", "abs": "https://arxiv.org/abs/2512.00249", "authors": ["Scotty Black", "Christian Darken"], "title": "A Hierarchical Hybrid AI Approach: Integrating Deep Reinforcement Learning and Scripted Agents in Combat Simulations", "comment": "arXiv admin note: substantial text overlap with arXiv:2408.13333", "summary": "In the domain of combat simulations in support of wargaming, the development of intelligent agents has predominantly been characterized by rule-based, scripted methodologies with deep reinforcement learning (RL) approaches only recently being introduced. While scripted agents offer predictability and consistency in controlled environments, they fall short in dynamic, complex scenarios due to their inherent inflexibility. Conversely, RL agents excel in adaptability and learning, offering potential improvements in handling unforeseen situations, but suffer from significant challenges such as black-box decision-making processes and scalability issues in larger simulation environments. This paper introduces a novel hierarchical hybrid artificial intelligence (AI) approach that synergizes the reliability and predictability of scripted agents with the dynamic, adaptive learning capabilities of RL. By structuring the AI system hierarchically, the proposed approach aims to utilize scripted agents for routine, tactical-level decisions and RL agents for higher-level, strategic decision-making, thus addressing the limitations of each method while leveraging their individual strengths. This integration is shown to significantly improve overall performance, providing a robust, adaptable, and effective solution for developing and training intelligent agents in complex simulation environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5c42\u7ea7\u6df7\u5408\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u811a\u672c\u4ee3\u7406\u7684\u53ef\u9760\u6027\u548c\u53ef\u9884\u6d4b\u6027\u4ee5\u53ca\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u52a8\u6001\u81ea\u9002\u5e94\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u5728\u5175\u68cb\u63a8\u6f14\u7684\u4f5c\u6218\u6a21\u62df\u9886\u57df\uff0c\u667a\u80fdagent\u7684\u5f00\u53d1\u4e3b\u8981\u4ee5\u57fa\u4e8e\u89c4\u5219\u7684\u811a\u672c\u65b9\u6cd5\u4e3a\u4e3b\uff0c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\u662f\u6700\u8fd1\u624d\u5f15\u5165\u7684\u3002\u811a\u672c\u4ee3\u7406\u5728\u53d7\u63a7\u73af\u5883\u4e2d\u63d0\u4f9b\u53ef\u9884\u6d4b\u6027\u548c\u4e00\u81f4\u6027\uff0c\u4f46\u5728\u52a8\u6001\u3001\u590d\u6742\u7684\u573a\u666f\u4e2d\uff0c\u7531\u4e8e\u5176\u56fa\u6709\u7684\u4e0d\u7075\u6d3b\u6027\u800c\u663e\u5f97\u4e0d\u8db3\u3002\u53cd\u4e4b\uff0cRL agent\u64c5\u957f\u9002\u5e94\u548c\u5b66\u4e60\uff0c\u5728\u5904\u7406\u65e0\u6cd5\u9884\u89c1\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u4e86\u6f5c\u5728\u7684\u6539\u8fdb\uff0c\u4f46\u540c\u65f6\u4e5f\u9762\u4e34\u7740\u91cd\u5927\u7684\u6311\u6218\uff0c\u5982\u9ed1\u76d2\u51b3\u7b56\u8fc7\u7a0b\u548c\u5728\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u62df\u73af\u5883\u4e2d\u5b58\u5728\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5206\u5c42\u6784\u5efaAI\u7cfb\u7edf\uff0c\u8be5\u65b9\u6cd5\u65e8\u5728\u5229\u7528\u811a\u672c\u4ee3\u7406\u8fdb\u884c\u65e5\u5e38\u7684\u6218\u672f\u5c42\u9762\u51b3\u7b56\uff0c\u5229\u7528RL agent\u8fdb\u884c\u66f4\u9ad8\u5c42\u6b21\u7684\u6218\u7565\u51b3\u7b56\uff0c\u4ece\u800c\u626c\u957f\u907f\u77ed\u3002", "result": "\u8fd9\u79cd\u96c6\u6210\u663e\u8457\u63d0\u9ad8\u4e86\u6574\u4f53\u6027\u80fd\uff0c\u4e3a\u5728\u590d\u6742\u6a21\u62df\u73af\u5883\u4e2d\u5f00\u53d1\u548c\u8bad\u7ec3\u667a\u80fdagent\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u3001\u9002\u5e94\u6027\u5f3a\u548c\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8bba\u6587\u7ae0\u8282"}}
{"id": "2512.00084", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00084", "abs": "https://arxiv.org/abs/2512.00084", "authors": ["Venkata Siddharth Dhara", "Pawan Kumar"], "title": "A Fast and Efficient Modern BERT based Text-Conditioned Diffusion Model for Medical Image Segmentation", "comment": "15 pages, 3 figures, Accepted in Slide 3 10th International Conference on Computer Vision & Image Processing (CVIP 2026)", "summary": "In recent times, denoising diffusion probabilistic models (DPMs) have proven effective for medical image generation and denoising, and as representation learners for downstream segmentation. However, segmentation performance is limited by the need for dense pixel-wise labels, which are expensive, time-consuming, and require expert knowledge. We propose FastTextDiff, a label-efficient diffusion-based segmentation model that integrates medical text annotations to enhance semantic representations. Our approach uses ModernBERT, a transformer capable of processing long clinical notes, to tightly link textual annotations with semantic content in medical images. Trained on MIMIC-III and MIMIC-IV, ModernBERT encodes clinical knowledge that guides cross-modal attention between visual and textual features. This study validates ModernBERT as a fast, scalable alternative to Clinical BioBERT in diffusion-based segmentation pipelines and highlights the promise of multi-modal techniques for medical image analysis. By replacing Clinical BioBERT with ModernBERT, FastTextDiff benefits from FlashAttention 2, an alternating attention mechanism, and a 2-trillion-token corpus, improving both segmentation accuracy and training efficiency over traditional diffusion-based models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a FastTextDiff \u7684\u6807\u7b7e\u9ad8\u6548\u6269\u6563\u5206\u5272\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u96c6\u6210\u4e86\u533b\u5b66\u6587\u672c\u6ce8\u91ca\u4ee5\u589e\u5f3a\u8bed\u4e49\u8868\u793a\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6027\u80fd\u53d7\u5230\u5bc6\u96c6\u50cf\u7d20\u7ea7\u6807\u7b7e\u9700\u6c42\u7684\u9650\u5236\uff0c\u8fd9\u4e9b\u6807\u7b7e\u6210\u672c\u9ad8\u6602\u3001\u8017\u65f6\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u3002", "method": "\u5229\u7528 ModernBERT\uff08\u4e00\u79cd\u80fd\u591f\u5904\u7406\u957f\u4e34\u5e8a\u7b14\u8bb0\u7684 Transformer\uff09\u5c06\u6587\u672c\u6ce8\u91ca\u4e0e\u533b\u5b66\u56fe\u50cf\u4e2d\u7684\u8bed\u4e49\u5185\u5bb9\u7d27\u5bc6\u8054\u7cfb\u8d77\u6765\u3002ModernBERT \u5728 MIMIC-III \u548c MIMIC-IV \u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5bf9\u4e34\u5e8a\u77e5\u8bc6\u8fdb\u884c\u7f16\u7801\uff0c\u4ece\u800c\u6307\u5bfc\u89c6\u89c9\u548c\u6587\u672c\u7279\u5f81\u4e4b\u95f4\u7684\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u3002", "result": "\u901a\u8fc7\u7528 ModernBERT \u66ff\u4ee3 Clinical BioBERT\uff0cFastTextDiff \u53d7\u76ca\u4e8e FlashAttention 2 \u548c\u4e00\u4e2a 2 \u4e07\u4ebf token \u7684\u8bed\u6599\u5e93\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u5206\u5272\u7cbe\u5ea6\u548c\u8bad\u7ec3\u6548\u7387\u3002", "conclusion": "\u9a8c\u8bc1\u4e86 ModernBERT \u4f5c\u4e3a\u57fa\u4e8e\u6269\u6563\u7684\u5206\u5272\u6d41\u6c34\u7ebf\u4e2d Clinical BioBERT \u7684\u4e00\u79cd\u5feb\u901f\u3001\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5e76\u5f3a\u8c03\u4e86\u591a\u6a21\u6001\u6280\u672f\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u524d\u666f\u3002"}}
{"id": "2512.00724", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.00724", "abs": "https://arxiv.org/abs/2512.00724", "authors": ["Lingling Fu"], "title": "Upcycled and Merged MoE Reward Model for Mitigating Reward Hacking", "comment": "9 pages,5 figures", "summary": "Reward models play a critical role in Reinforcement Learning from Human Feedback (RLHF) by assessing the consistency between generated outputs and human preferences. However, conventional reward models are prone to reward hacking or over-optimization, where the policy exploits shortcut patterns to obtain high reward scores that do not reflect true human preference. Although Mixture-of-Experts (MoE)-based reward models can enhance discriminative capability, they typically introduce substantial computational overhead. To address these challenges, we propose an upcycle and merge MoE reward modeling approach. We first upcycle a dense reward model into a MoE architecture, where a shared expert captures general knowledge, while normal experts specialize in instruction-specific patterns. We then apply routing-weight normalization and merge experts back into a dense model through a learnable weight-averaging mechanism, preserving performance gains while significantly reducing inference cost. Experimental results demonstrate that our method effectively mitigates reward hacking across various model scales. Our work highlights the potential of upcycle and merge MoE structures for improving both robustness and efficiency of RLHF reward models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cdupcycle\u548cmerge MoE\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8RLHF\u5956\u52b1\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\uff0c\u5e76\u51cf\u8f7b\u5956\u52b1\u64cd\u7eb5\u3002", "motivation": "\u4f20\u7edf\u7684\u5956\u52b1\u6a21\u578b\u5bb9\u6613\u51fa\u73b0\u5956\u52b1\u64cd\u7eb5\u6216\u8fc7\u5ea6\u4f18\u5316\uff0c\u800cMoE\u5956\u52b1\u6a21\u578b\u867d\u7136\u53ef\u4ee5\u589e\u5f3a\u5224\u522b\u80fd\u529b\uff0c\u4f46\u8ba1\u7b97\u5f00\u9500\u5f88\u5927\u3002", "method": "\u9996\u5148\u5c06\u5bc6\u96c6\u5956\u52b1\u6a21\u578b\u5347\u7ea7\u4e3aMoE\u67b6\u6784\uff0c\u7136\u540e\u5e94\u7528\u8def\u7531\u6743\u91cd\u5f52\u4e00\u5316\uff0c\u5e76\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u6743\u91cd\u5e73\u5747\u673a\u5236\u5c06\u4e13\u5bb6\u5408\u5e76\u56de\u5bc6\u96c6\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u5730\u51cf\u8f7b\u4e86\u5404\u79cd\u6a21\u578b\u89c4\u6a21\u7684\u5956\u52b1\u64cd\u7eb5\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5f3a\u8c03\u4e86upcycle\u548cmerge MoE\u7ed3\u6784\u5728\u63d0\u9ad8RLHF\u5956\u52b1\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.00479", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00479", "abs": "https://arxiv.org/abs/2512.00479", "authors": ["Yuta Kobayashi", "Vincent Jeanselme", "Shalmali Joshi"], "title": "Mind the data gap: Missingness Still Shapes Large Language Model Prognoses", "comment": "Presented at ML4H 2025 - Findings Track", "summary": "Data collection often reflects human decisions. In healthcare, for instance, a referral for a diagnostic test is influenced by the patient's health, their preferences, available resources, and the practitioner's recommendations. Despite the extensive literature on the informativeness of missingness, its implications on the performance of Large Language Models (LLMs) have not been studied. Through a series of experiments on data from Columbia University Medical Center, a large urban academic medical center, and MIMIC-IV, we demonstrate that patterns of missingness significantly impact zero-shot predictive performance. Notably, the explicit inclusion of missingness indicators at prompting benefits some while hurting other LLMs' zero-shot predictive performance and calibration, suggesting an inconsistent impact. The proposed aggregated analysis and theoretical insights suggest that larger models benefit from these interventions, while smaller models can be negatively impacted. The LLM paradigm risks obscuring the impact of missingness, often neglected even in conventional ML, even further. We conclude that there is a need for more transparent accounting and systematic evaluation of the impact of representing (informative) missingness on downstream performance.", "AI": {"tldr": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u5728\u5904\u7406\u7f3a\u5931\u6570\u636e\u65f6\u7684\u6027\u80fd\u8868\u73b0\uff0c\u53d1\u73b0\u7f3a\u5931\u6570\u636e\u7684\u6a21\u5f0f\u5bf9\u96f6\u6837\u672c\u9884\u6d4b\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u5e76\u4e14\u663e\u5f0f\u5305\u542b\u7f3a\u5931\u6027\u6307\u6807\u5bf9\u4e0d\u540c\u5927\u5c0f\u7684LLM\u6a21\u578b\u6709\u4e0d\u4e00\u81f4\u7684\u5f71\u54cd\u3002", "motivation": "\u5c3d\u7ba1\u5173\u4e8e\u7f3a\u5931\u4fe1\u606f\u7684\u7814\u7a76\u5f88\u591a\uff0c\u4f46\u5176\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u6027\u80fd\u7684\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u8be5\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5173\u6ce8\u7f3a\u5931\u6570\u636e\u5bf9LLM\u9884\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5bf9\u6765\u81ea\u54e5\u4f26\u6bd4\u4e9a\u5927\u5b66\u533b\u5b66\u4e2d\u5fc3\u548cMIMIC-IV\u7684\u6570\u636e\u8fdb\u884c\u4e00\u7cfb\u5217\u5b9e\u9a8c\uff0c\u63a2\u8ba8\u7f3a\u5931\u6a21\u5f0f\u5bf9LLM\u96f6\u6837\u672c\u9884\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u5206\u6790\u663e\u5f0f\u5305\u542b\u7f3a\u5931\u6027\u6307\u6807\u7684\u5f71\u54cd\u3002", "result": "\u7f3a\u5931\u6570\u636e\u7684\u6a21\u5f0f\u663e\u8457\u5f71\u54cdLLM\u7684\u96f6\u6837\u672c\u9884\u6d4b\u6027\u80fd\u3002\u663e\u5f0f\u5305\u542b\u7f3a\u5931\u6027\u6307\u6807\u5bf9\u67d0\u4e9bLLM\u6709\u76ca\uff0c\u4f46\u5bf9\u5176\u4ed6LLM\u6709\u5bb3\uff0c\u8868\u660e\u5f71\u54cd\u4e0d\u4e00\u81f4\u3002\u8f83\u5927\u7684\u6a21\u578b\u53d7\u76ca\u4e8e\u8fd9\u4e9b\u5e72\u9884\uff0c\u800c\u8f83\u5c0f\u7684\u6a21\u578b\u53ef\u80fd\u53d7\u5230\u8d1f\u9762\u5f71\u54cd\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u53ef\u80fd\u4f1a\u63a9\u76d6\u7f3a\u5931\u6570\u636e\u7684\u5f71\u54cd\uff0c\u800c\u8fd9\u79cd\u5f71\u54cd\u5728\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u4e2d\u7ecf\u5e38\u88ab\u5ffd\u7565\u3002\u56e0\u6b64\uff0c\u9700\u8981\u66f4\u900f\u660e\u7684\u8bf4\u660e\u548c\u7cfb\u7edf\u8bc4\u4f30\u4ee3\u8868\u6027(\u4fe1\u606f\u6027)\u7f3a\u5931\u5bf9\u4e0b\u6e38\u6027\u80fd\u7684\u5f71\u54cd\u3002"}}
{"id": "2512.00333", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00333", "abs": "https://arxiv.org/abs/2512.00333", "authors": ["Ayush Maheshwari", "Kaushal Sharma", "Vivek Patel", "Aditya Maheshwari"], "title": "IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages", "comment": null, "summary": "While large language models excel on high-resource multilingual tasks, low- and extremely low-resource Indic languages remain severely under-evaluated. We present IndicParam, a human-curated benchmark of over 13,000 multiple-choice questions covering 11 such languages (Nepali, Gujarati, Marathi, Odia as low-resource; Dogri, Maithili, Rajasthani, Sanskrit, Bodo, Santali, Konkani as extremely low-resource) plus Sanskrit-English code-mixed set. We evaluated 19 LLMs, both proprietary and open-weights, which reveals that even the top-performing GPT-5 reaches only 45.0% average accuracy, followed by DeepSeek-3.2 (43.1) and Claude-4.5 (42.7). We additionally label each question as knowledge-oriented or purely linguistic to discriminate factual recall from grammatical proficiency. Further, we assess the ability of LLMs to handle diverse question formats-such as list-based matching, assertion-reason pairs, and sequence ordering-alongside conventional multiple-choice questions. IndicParam provides insights into limitations of cross-lingual transfer and establishes a challenging benchmark for Indic languages. The dataset is available at https://huggingface.co/datasets/bharatgenai/IndicParam. Scripts to run benchmark are present at https://github.com/ayushbits/IndicParam.", "AI": {"tldr": "IndicParam\u662f\u4e00\u4e2a\u5305\u542b11\u79cd\u4f4e\u8d44\u6e90\u5370\u5ea6\u8bed\u8a00\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u3002", "motivation": "\u73b0\u6709\u7684LLM\u5728\u5904\u7406\u9ad8\u8d44\u6e90\u591a\u8bed\u8a00\u4efb\u52a1\u65f6\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u4f4e\u8d44\u6e90\u5370\u5ea6\u8bed\u8a00\u4e0a\u7684\u8868\u73b0\u672a\u5f97\u5230\u5145\u5206\u8bc4\u4f30\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aIndicParam\u7684\u4eba\u5de5\u7ba1\u7406\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5176\u4e2d\u5305\u542b\u8d85\u8fc713,000\u4e2a\u591a\u9879\u9009\u62e9\u9898\uff0c\u6db5\u76d611\u79cd\u5370\u5ea6\u8bed\u8a00\u3002", "result": "\u8bc4\u4f30\u4e8619\u4e2aLLM\uff0c\u5305\u62ec\u4e13\u6709\u6a21\u578b\u548c\u5f00\u6e90\u6a21\u578b\uff0c\u7ed3\u679c\u663e\u793a\u5373\u4f7f\u662f\u8868\u73b0\u6700\u4f73\u7684GPT-5\u4e5f\u4ec5\u8fbe\u523045.0\uff05\u7684\u5e73\u5747\u51c6\u786e\u7387\u3002", "conclusion": "IndicParam\u63ed\u793a\u4e86\u8de8\u8bed\u8a00\u8fc1\u79fb\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u5370\u5ea6\u8bed\u8a00\u5efa\u7acb\u4e86\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u3002"}}
{"id": "2512.00251", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.00251", "abs": "https://arxiv.org/abs/2512.00251", "authors": ["Henry Onyeka", "Emmanuel Samson", "Liang Hong", "Tariqul Islam", "Imtiaz Ahmed", "Kamrul Hasan"], "title": "SD-CGAN: Conditional Sinkhorn Divergence GAN for DDoS Anomaly Detection in IoT Networks", "comment": "7 pages, 6 figures, camera-ready version accepted for presentation at IEEE ICNC 2026", "summary": "The increasing complexity of IoT edge networks presents significant challenges for anomaly detection, particularly in identifying sophisticated Denial-of-Service (DoS) attacks and zero-day exploits under highly dynamic and imbalanced traffic conditions. This paper proposes SD-CGAN, a Conditional Generative Adversarial Network framework enhanced with Sinkhorn Divergence, tailored for robust anomaly detection in IoT edge environments. The framework incorporates CTGAN-based synthetic data augmentation to address class imbalance and leverages Sinkhorn Divergence as a geometry-aware loss function to improve training stability and reduce mode collapse. The model is evaluated on exploitative attack subsets from the CICDDoS2019 dataset and compared against baseline deep learning and GAN-based approaches. Results show that SD-CGAN achieves superior detection accuracy, precision, recall, and F1-score while maintaining computational efficiency suitable for deployment in edge-enabled IoT environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a SD-CGAN \u7684\u65b0\u578b\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u4e13\u4e3a\u7269\u8054\u7f51\u8fb9\u7f18\u73af\u5883\u8bbe\u8ba1\uff0c\u80fd\u6709\u6548\u5e94\u5bf9\u590d\u6742\u7684\u7f51\u7edc\u653b\u51fb\u3002", "motivation": "\u7269\u8054\u7f51\u8fb9\u7f18\u7f51\u7edc\u65e5\u76ca\u590d\u6742\uff0c\u5bf9\u5f02\u5e38\u68c0\u6d4b\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u8bc6\u522b DoS \u653b\u51fb\u548c\u96f6\u65e5\u6f0f\u6d1e\u5229\u7528\u65b9\u9762\u3002", "method": "\u8be5\u65b9\u6cd5\u63d0\u51fa\u4e86\u4e00\u79cd\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u6846\u67b6\uff0c\u901a\u8fc7 Sinkhorn \u6563\u5ea6\u8fdb\u884c\u589e\u5f3a\uff0c\u5e76\u7ed3\u5408 CTGAN \u7684\u5408\u6210\u6570\u636e\u589e\u5f3a\u6765\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "SD-CGAN \u5728\u68c0\u6d4b\u7cbe\u5ea6\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548c F1 \u503c\u65b9\u9762\u5747\u4f18\u4e8e\u57fa\u7ebf\u6df1\u5ea6\u5b66\u4e60\u548c\u57fa\u4e8e GAN \u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "SD-CGAN \u9002\u7528\u4e8e\u90e8\u7f72\u5728\u652f\u6301\u8fb9\u7f18\u8ba1\u7b97\u7684\u7269\u8054\u7f51\u73af\u5883\u4e2d\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u7684\u5f02\u5e38\u68c0\u6d4b\u3002"}}
{"id": "2512.00086", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00086", "abs": "https://arxiv.org/abs/2512.00086", "authors": ["Davide Nadalini", "Manuele Rusci", "Elia Cereda", "Luca Benini", "Francesco Conti", "Daniele Palossi"], "title": "Multi-modal On-Device Learning for Monocular Depth Estimation on Ultra-low-power MCUs", "comment": "14 pages, 9 figures, 3 tables. Associated open-source release available at: https://github.com/dnadalini/ondevice_learning_for_monocular_depth_estimation", "summary": "Monocular depth estimation (MDE) plays a crucial role in enabling spatially-aware applications in Ultra-low-power (ULP) Internet-of-Things (IoT) platforms. However, the limited number of parameters of Deep Neural Networks for the MDE task, designed for IoT nodes, results in severe accuracy drops when the sensor data observed in the field shifts significantly from the training dataset. To address this domain shift problem, we present a multi-modal On-Device Learning (ODL) technique, deployed on an IoT device integrating a Greenwaves GAP9 MicroController Unit (MCU), a 80 mW monocular camera and a 8 x 8 pixel depth sensor, consuming $\\approx$300mW. In its normal operation, this setup feeds a tiny 107 k-parameter $\u03bc$PyD-Net model with monocular images for inference. The depth sensor, usually deactivated to minimize energy consumption, is only activated alongside the camera to collect pseudo-labels when the system is placed in a new environment. Then, the fine-tuning task is performed entirely on the MCU, using the new data. To optimize our backpropagation-based on-device training, we introduce a novel memory-driven sparse update scheme, which minimizes the fine-tuning memory to 1.2 MB, 2.2x less than a full update, while preserving accuracy (i.e., only 2% and 1.5% drops on the KITTI and NYUv2 datasets). Our in-field tests demonstrate, for the first time, that ODL for MDE can be performed in 17.8 minutes on the IoT node, reducing the root mean squared error from 4.9 to 0.6m with only 3 k self-labeled samples, collected in a real-life deployment scenario.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7269\u8054\u7f51\u8bbe\u5907\u4e0a\u8fdb\u884c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1(MDE)\u7684On-Device Learning (ODL)\u6280\u672f\uff0c\u4ee5\u89e3\u51b3\u9886\u57df\u8fc1\u79fb\u95ee\u9898\u3002", "motivation": "\u5728\u8d85\u4f4e\u529f\u8017(ULP)\u7269\u8054\u7f51(IoT)\u5e73\u53f0\u4e2d\uff0c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1(MDE)\u5728\u5b9e\u73b0\u7a7a\u95f4\u611f\u77e5\u5e94\u7528\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002\u7136\u800c\uff0c\u4e3a\u7269\u8054\u7f51\u8282\u70b9\u8bbe\u8ba1\u7684\u7528\u4e8eMDE\u4efb\u52a1\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u53c2\u6570\u6570\u91cf\u6709\u9650\uff0c\u5f53\u73b0\u573a\u89c2\u5bdf\u5230\u7684\u4f20\u611f\u5668\u6570\u636e\u4e0e\u8bad\u7ec3\u6570\u636e\u96c6\u53d1\u751f\u663e\u8457\u53d8\u5316\u65f6\uff0c\u4f1a\u5bfc\u81f4\u4e25\u91cd\u7684\u7cbe\u5ea6\u4e0b\u964d\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528\u4e00\u4e2a\u96c6\u6210\u4e86Greenwaves GAP9\u5fae\u63a7\u5236\u5668\u5355\u5143(MCU)\u3001\u4e00\u4e2a80mW\u5355\u76ee\u6444\u50cf\u5934\u548c\u4e00\u4e2a8x8\u50cf\u7d20\u6df1\u5ea6\u4f20\u611f\u5668\u7684\u7269\u8054\u7f51\u8bbe\u5907\u3002\u8be5\u8bbe\u5907\u4f7f\u7528\u4e00\u4e2a\u5fae\u5c0f\u7684107k\u53c2\u6570\u7684\u03bcPyD-Net\u6a21\u578b\u8fdb\u884c\u63a8\u7406\uff0c\u5e76\u5229\u7528\u6df1\u5ea6\u4f20\u611f\u5668\u5728\u65b0\u7684\u73af\u5883\u4e2d\u6536\u96c6\u4f2a\u6807\u7b7e\uff0c\u7136\u540e\u5728MCU\u4e0a\u5b8c\u5168\u6267\u884c\u5fae\u8c03\u4efb\u52a1\u3002\u4e3a\u4e86\u4f18\u5316\u57fa\u4e8e\u53cd\u5411\u4f20\u64ad\u7684\u8bbe\u5907\u4e0a\u8bad\u7ec3\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u5185\u5b58\u9a71\u52a8\u7684\u7a00\u758f\u66f4\u65b0\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u5c06\u5fae\u8c03\u5185\u5b58\u6700\u5c0f\u5316\u52301.2MB\uff0c\u6bd4\u5b8c\u5168\u66f4\u65b0\u51cf\u5c112.2\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7cbe\u5ea6\u3002", "result": "\u73b0\u573a\u6d4b\u8bd5\u8868\u660e\uff0cMDE\u7684ODL\u53ef\u4ee5\u5728\u7269\u8054\u7f51\u8282\u70b9\u4e0a\u572817.8\u5206\u949f\u5185\u5b8c\u6210\uff0c\u5e76\u4e14\u4ec5\u75283k\u4e2a\u5728\u5b9e\u9645\u90e8\u7f72\u573a\u666f\u4e2d\u6536\u96c6\u7684\u81ea\u6807\u8bb0\u6837\u672c\uff0c\u5c31\u5c06\u5747\u65b9\u6839\u8bef\u5dee\u4ece4.9\u7c73\u964d\u4f4e\u52300.6\u7c73\u3002", "conclusion": "\u8be5\u8bba\u6587\u8bc1\u660e\u4e86\u5728\u7269\u8054\u7f51\u8282\u70b9\u4e0a\u8fdb\u884cMDE\u7684ODL\u662f\u53ef\u884c\u7684\uff0c\u5e76\u4e14\u53ef\u4ee5\u6709\u6548\u5730\u89e3\u51b3\u9886\u57df\u8fc1\u79fb\u95ee\u9898\u3002"}}
{"id": "2512.00768", "categories": ["cs.LG", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.00768", "abs": "https://arxiv.org/abs/2512.00768", "authors": ["Hamed Razavi"], "title": "Text Mining Analysis of Symptom Patterns in Medical Chatbot Conversations", "comment": "9 pages, 4 tables", "summary": "The fast growth of digital health systems has led to a need to better comprehend how they interpret and represent patient-reported symptoms. Chatbots have been used in healthcare to provide clinical support and enhance the user experience, making it possible to provide meaningful clinical patterns from text-based data through chatbots. The proposed research utilises several different natural language processing methods to study the occurrences of symptom descriptions in medicine as well as analyse the patterns that emerge through these conversations within medical bots. Through the use of the Medical Conversations to Disease Dataset which contains 960 multi-turn dialogues divided into 24 Clinical Conditions, a standardised representation of conversations between patient and bot is created for further analysis by computational means. The multi-method approach uses a variety of tools, including Latent Dirichlet Allocation (LDA) to identify latent symptom themes, K-Means to group symptom descriptions by similarity, Transformer-based Named Entity Recognition (NER) to extract medical concepts, and the Apriori algorithm to discover frequent symptom pairs. Findings from the analysis indicate a coherent structure of clinically relevant topics, moderate levels of clustering cohesiveness and several high confidence rates on the relationships between symptoms like fever headache and rash itchiness. The results support the notion that conversational medical data can be a valuable diagnostic signal for early symptom interpretation, assist in strengthening decision support and improve how users interact with tele-health technology. By demonstrating a method for converting unstructured free-flowing dialogue into actionable knowledge regarding symptoms this work provides an extensible framework to further enhance future performance, dependability and clinical utility of selecting medical chatbots.", "AI": {"tldr": "\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5904\u7406\u65b9\u6cd5\uff0c\u4ece\u533b\u7597\u804a\u5929\u673a\u5668\u4eba\u5bf9\u8bdd\u4e2d\u63d0\u53d6\u6709\u610f\u4e49\u7684\u4e34\u5e8a\u6a21\u5f0f\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u6570\u5b57\u5065\u5eb7\u7cfb\u7edf\u5982\u4f55\u89e3\u91ca\u548c\u8868\u793a\u60a3\u8005\u62a5\u544a\u7684\u75c7\u72b6\u3002", "motivation": "\u968f\u7740\u6570\u5b57\u5065\u5eb7\u7cfb\u7edf\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u9700\u8981\u66f4\u597d\u5730\u7406\u89e3\u5b83\u4eec\u5982\u4f55\u89e3\u91ca\u548c\u8868\u793a\u60a3\u8005\u62a5\u544a\u7684\u75c7\u72b6\u3002\u804a\u5929\u673a\u5668\u4eba\u5df2\u88ab\u7528\u4e8e\u533b\u7597\u4fdd\u5065\uff0c\u4ee5\u63d0\u4f9b\u4e34\u5e8a\u652f\u6301\u548c\u589e\u5f3a\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u8be5\u7814\u7a76\u5229\u7528\u591a\u79cd\u81ea\u7136\u8bed\u8a00\u5904\u7406\u65b9\u6cd5\uff0c\u5305\u62ecLDA\u4e3b\u9898\u6a21\u578b\u3001K-Means\u805a\u7c7b\u3001Transformer-based NER\u548cApriori\u7b97\u6cd5\uff0c\u5206\u6790\u533b\u5b66\u5bf9\u8bdd\u4e2d\u7684\u75c7\u72b6\u63cf\u8ff0\u6a21\u5f0f\u3002", "result": "\u5206\u6790\u7ed3\u679c\u8868\u660e\uff0c\u4e34\u5e8a\u76f8\u5173\u4e3b\u9898\u5177\u6709\u8fde\u8d2f\u7684\u7ed3\u6784\uff0c\u805a\u7c7b\u5185\u805a\u529b\u5904\u4e8e\u4e2d\u7b49\u6c34\u5e73\uff0c\u5e76\u4e14\u5728\u53d1\u70e7\u3001\u5934\u75db\u548c\u76ae\u75b9\u7619\u75d2\u7b49\u75c7\u72b6\u4e4b\u95f4\u5b58\u5728\u82e5\u5e72\u9ad8\u7f6e\u4fe1\u5ea6\u5173\u7cfb\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u652f\u6301\u5bf9\u8bdd\u5f0f\u533b\u7597\u6570\u636e\u53ef\u4ee5\u4f5c\u4e3a\u65e9\u671f\u75c7\u72b6\u89e3\u91ca\u7684\u6709\u4ef7\u503c\u7684\u8bca\u65ad\u4fe1\u53f7\uff0c\u6709\u52a9\u4e8e\u52a0\u5f3a\u51b3\u7b56\u652f\u6301\uff0c\u5e76\u6539\u5584\u7528\u6237\u4e0e\u8fdc\u7a0b\u533b\u7597\u6280\u672f\u7684\u4ea4\u4e92\u65b9\u5f0f\u3002"}}
{"id": "2512.00601", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00601", "abs": "https://arxiv.org/abs/2512.00601", "authors": ["Boyang Gu", "Hongjian Zhou", "Bradley Max Segal", "Jinge Wu", "Zeyu Cao", "Hantao Zhong", "Lei Clifton", "Fenglin Liu", "David A. Clifton"], "title": "Clinical-R1: Empowering Large Language Models for Faithful and Comprehensive Reasoning with Clinical Objective Relative Policy Optimization", "comment": null, "summary": "Recent advances in large language models (LLMs) have shown strong reasoning capabilities through large-scale pretraining and post-training reinforcement learning, demonstrated by DeepSeek-R1. However, current post-training methods, such as Grouped Relative Policy Optimization (GRPO), mainly reward correctness, which is not aligned with the multi-dimensional objectives required in high-stakes fields such as medicine, where reasoning must also be faithful and comprehensive. We introduce Clinical-Objective Relative Policy Optimization (CRPO), a scalable, multi-objective, verifiable reinforcement learning method designed to align LLM post-training with clinical reasoning principles. CRPO integrates rule-based and verifiable reward signals that jointly optimize accuracy, faithfulness, and comprehensiveness without relying on human annotation. To demonstrate its effectiveness, we train Clinical-R1-3B, a 3B-parameter model for clinical reasoning. The experiments on three benchmarks demonstrate that our CRPO substantially improves reasoning on truthfulness and completeness over standard GRPO while maintaining comfortable accuracy enhancements. This framework provides a scalable pathway to align LLM reasoning with clinical objectives, enabling safer and more collaborative AI systems for healthcare while also highlighting the potential of multi-objective, verifiable RL methods in post-training scaling of LLMs for medical domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684LLM\u5bf9\u9f50\u65b9\u6cd5\uff0c\u7528\u4e8e\u533b\u7597\u9886\u57df\u7684\u4e34\u5e8a\u63a8\u7406\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u591a\u76ee\u6807\u53ef\u9a8c\u8bc1\u5f3a\u5316\u5b66\u4e60\u6765\u4f18\u5316\u51c6\u786e\u6027\u3001\u5fe0\u5b9e\u6027\u548c\u5168\u9762\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5956\u52b1\u6b63\u786e\u6027\uff0c\u4e0e\u533b\u7597\u9886\u57df\u4e2d\u63a8\u7406\u5fc5\u987b\u5fe0\u5b9e\u548c\u5168\u9762\u7684\u591a\u7ef4\u76ee\u6807\u4e0d\u7b26\u3002", "method": "\u5f15\u5165Clinical-Objective Relative Policy Optimization (CRPO)\uff0c\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u591a\u76ee\u6807\u53ef\u9a8c\u8bc1\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u57fa\u4e8e\u89c4\u5219\u7684\u53ef\u9a8c\u8bc1\u5956\u52b1\u4fe1\u53f7\uff0c\u8054\u5408\u4f18\u5316\u51c6\u786e\u6027\u3001\u5fe0\u5b9e\u6027\u548c\u5168\u9762\u6027\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCRPO\u5728\u771f\u5b9e\u6027\u548c\u5b8c\u6574\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6807\u51c6GRPO\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8212\u9002\u7684\u51c6\u786e\u6027\u63d0\u5347\u3002", "conclusion": "CRPO\u4e3aLLM\u63a8\u7406\u4e0e\u4e34\u5e8a\u76ee\u6807\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u7684\u9014\u5f84\uff0c\u5b9e\u73b0\u4e86\u66f4\u5b89\u5168\u3001\u66f4\u534f\u4f5c\u7684\u533b\u7597AI\u7cfb\u7edf\uff0c\u5e76\u7a81\u51fa\u4e86\u591a\u76ee\u6807\u53ef\u9a8c\u8bc1RL\u65b9\u6cd5\u5728\u533b\u7597\u9886\u57dfLLM\u540e\u8bad\u7ec3\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.00360", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00360", "abs": "https://arxiv.org/abs/2512.00360", "authors": ["Vsevolod Kovalev", "Parteek Kumar"], "title": "CourseTimeQA: A Lecture-Video Benchmark and a Latency-Constrained Cross-Modal Fusion Method for Timestamped QA", "comment": "5 figures, 8 tables", "summary": "We study timestamped question answering over educational lecture videos under a single-GPU latency/memory budget. Given a natural-language query, the system retrieves relevant timestamped segments and synthesizes a grounded answer. We present CourseTimeQA (52.3 h, 902 queries across six courses) and a lightweight, latency-constrained cross-modal retriever (CrossFusion-RAG) that combines frozen encoders, a learned 512->768 vision projection, shallow query-agnostic cross-attention over ASR and frames with a temporal-consistency regularizer, and a small cross-attentive reranker. On CourseTimeQA, CrossFusion-RAG improves nDCG@10 by 0.10 and MRR by 0.08 over a strong BLIP-2 retriever while achieving approximately 1.55 s median end-to-end latency on a single A100. Closest comparators (zero-shot CLIP multi-frame pooling; CLIP + cross-encoder reranker + MMR; learned late-fusion gating; text-only hybrid with cross-encoder reranking and its MMR variant; caption-augmented text retrieval; non-learned temporal smoothing) are evaluated under matched hardware and indexing. We report robustness across ASR noise (WER quartiles), diagnostics for temporal localization, and full training/tuning details to support reproducible comparison.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u5355\u4e2aGPU\u5ef6\u8fdf/\u5185\u5b58\u9884\u7b97\u4e0b\uff0c\u6559\u80b2\u8bb2\u5ea7\u89c6\u9891\u7684\u65f6\u95f4\u6233\u95ee\u9898\u89e3\u7b54\u3002\u7cfb\u7edf\u68c0\u7d22\u76f8\u5173\u7684\u65f6\u95f4\u6233\u7247\u6bb5\u5e76\u5408\u6210\u4e00\u4e2a\u57fa\u4e8e\u6b64\u7684\u7b54\u6848\u3002", "motivation": "\u7814\u7a76\u6559\u80b2\u8bb2\u5ea7\u89c6\u9891\u7684\u65f6\u95f4\u6233\u95ee\u9898\u89e3\u7b54\uff0c\u65e8\u5728\u63d0\u9ad8\u68c0\u7d22\u6548\u7387\u548c\u7b54\u6848\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u3001\u5ef6\u8fdf\u7ea6\u675f\u7684\u8de8\u6a21\u6001\u68c0\u7d22\u5668 (CrossFusion-RAG)\uff0c\u5b83\u7ed3\u5408\u4e86\u51bb\u7ed3\u7f16\u7801\u5668\u3001\u5b66\u4e60\u5230\u7684\u89c6\u89c9\u6295\u5f71\u3001\u6d45\u5c42\u67e5\u8be2\u65e0\u5173\u7684\u8de8\u6ce8\u610f\u529b\u4ee5\u53ca\u65f6\u95f4\u4e00\u81f4\u6027\u6b63\u5219\u5316\u3002", "result": "\u5728CourseTimeQA\u6570\u636e\u96c6\u4e0a\uff0cCrossFusion-RAG\u6bd4BLIP-2\u68c0\u7d22\u5668\u7684nDCG@10\u63d0\u9ad8\u4e860.10\uff0cMRR\u63d0\u9ad8\u4e860.08\uff0c\u540c\u65f6\u5728\u5355\u4e2aA100\u4e0a\u5b9e\u73b0\u4e86\u7ea61.55\u79d2\u7684\u4e2d\u503c\u7aef\u5230\u7aef\u5ef6\u8fdf\u3002", "conclusion": "CrossFusion-RAG\u5728\u6559\u80b2\u8bb2\u5ea7\u89c6\u9891\u7684\u65f6\u95f4\u6233\u95ee\u9898\u89e3\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5e73\u8861\u3002"}}
{"id": "2512.00260", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.00260", "abs": "https://arxiv.org/abs/2512.00260", "authors": ["Y. Sungtaek Ju"], "title": "Scalable and Interpretable Scientific Discovery via Sparse Variational Gaussian Process Kolmogorov-Arnold Networks (SVGP KAN)", "comment": "7 pages, 3 figures", "summary": "Kolmogorov-Arnold Networks (KANs) offer a promising alternative to Multi-Layer Perceptron (MLP) by placing learnable univariate functions on network edges, enhancing interpretability. However, standard KANs lack probabilistic outputs, limiting their utility in applications requiring uncertainty quantification. While recent Gaussian Process (GP) extensions to KANs address this, they utilize exact inference methods that scale cubically with data size N, restricting their application to smaller datasets. We introduce the Sparse Variational GP-KAN (SVGP-KAN), an architecture that integrates sparse variational inference with the KAN topology. By employing $M$ inducing points and analytic moment matching, our method reduces computational complexity from $O(N^3)$ to $O(NM^2)$ or linear in sample size, enabling the application of probabilistic KANs to larger scientific datasets. Furthermore, we demonstrate that integrating a permutation-based importance analysis enables the network to function as a framework for structural identification, identifying relevant inputs and classifying functional relationships.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u6982\u7387 Kolmogorov-Arnold \u7f51\u7edc (KAN) \u67b6\u6784\uff0c\u540d\u4e3a\u7a00\u758f\u53d8\u5206 GP-KAN (SVGP-KAN)\uff0c\u5b83\u5c06\u7a00\u758f\u53d8\u5206\u63a8\u7406\u4e0e KAN \u62d3\u6251\u7ed3\u6784\u96c6\u6210\uff0c\u4ee5\u5b9e\u73b0\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u5e76\u5e94\u7528\u4e8e\u66f4\u5927\u7684\u79d1\u5b66\u6570\u636e\u96c6\u3002", "motivation": "\u6807\u51c6 KAN \u7f3a\u4e4f\u6982\u7387\u8f93\u51fa\uff0c\u9650\u5236\u4e86\u5176\u5728\u9700\u8981\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u5e94\u7528\u4e2d\u7684\u6548\u7528\u3002\u867d\u7136\u6700\u8fd1\u7684 KAN \u7684\u9ad8\u65af\u8fc7\u7a0b (GP) \u6269\u5c55\u89e3\u51b3\u4e86\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46\u5b83\u4eec\u5229\u7528\u7cbe\u786e\u7684\u63a8\u7406\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u7684\u89c4\u6a21\u4e0e\u6570\u636e\u5927\u5c0f N \u6210\u4e09\u6b21\u6bd4\u4f8b\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u8f83\u5c0f\u6570\u636e\u96c6\u4e0a\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u91c7\u7528 M \u4e2a\u5f52\u7eb3\u70b9\u548c\u89e3\u6790\u77e9\u5339\u914d\uff0c\u8be5\u65b9\u6cd5\u5c06\u8ba1\u7b97\u590d\u6742\u5ea6\u4ece O(N^3) \u964d\u4f4e\u5230 O(NM^2) \u6216\u6837\u672c\u5927\u5c0f\u7684\u7ebf\u6027\u590d\u6742\u5ea6\uff0c\u4ece\u800c\u53ef\u4ee5\u5c06\u6982\u7387 KAN \u5e94\u7528\u4e8e\u66f4\u5927\u7684\u79d1\u5b66\u6570\u636e\u96c6\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u96c6\u6210\u4e86\u57fa\u4e8e\u6392\u5217\u7684\u91cd\u8981\u6027\u5206\u6790\uff0c\u4f7f\u7f51\u7edc\u80fd\u591f\u5145\u5f53\u7ed3\u6784\u8bc6\u522b\u6846\u67b6\uff0c\u8bc6\u522b\u76f8\u5173\u8f93\u5165\u5e76\u5bf9\u51fd\u6570\u5173\u7cfb\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u964d\u4f4e\uff0c\u4ece\u800c\u53ef\u4ee5\u5c06\u6982\u7387 KAN \u5e94\u7528\u4e8e\u66f4\u5927\u7684\u79d1\u5b66\u6570\u636e\u96c6\u3002", "conclusion": "\u96c6\u6210\u4e86\u57fa\u4e8e\u6392\u5217\u7684\u91cd\u8981\u6027\u5206\u6790\uff0c\u4f7f\u7f51\u7edc\u80fd\u591f\u5145\u5f53\u7ed3\u6784\u8bc6\u522b\u6846\u67b6\uff0c\u8bc6\u522b\u76f8\u5173\u8f93\u5165\u5e76\u5bf9\u51fd\u6570\u5173\u7cfb\u8fdb\u884c\u5206\u7c7b\u3002"}}
{"id": "2512.00087", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00087", "abs": "https://arxiv.org/abs/2512.00087", "authors": ["Ivo Bueno", "Ruikun Hou", "Babette B\u00fchler", "Tim F\u00fctterer", "James Drimalla", "Jonathan Kyle Foster", "Peter Youngs", "Peter Gerjets", "Ulrich Trautwein", "Enkelejda Kasneci"], "title": "Exploring Automated Recognition of Instructional Activity and Discourse from Multimodal Classroom Data", "comment": "This article has been accepted for publication in the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2026", "summary": "Observation of classroom interactions can provide concrete feedback to teachers, but current methods rely on manual annotation, which is resource-intensive and hard to scale. This work explores AI-driven analysis of classroom recordings, focusing on multimodal instructional activity and discourse recognition as a foundation for actionable feedback. Using a densely annotated dataset of 164 hours of video and 68 lesson transcripts, we design parallel, modality-specific pipelines. For video, we evaluate zero-shot multimodal LLMs, fine-tuned vision-language models, and self-supervised video transformers on 24 activity labels. For transcripts, we fine-tune a transformer-based classifier with contextualized inputs and compare it against prompting-based LLMs on 19 discourse labels. To handle class imbalance and multi-label complexity, we apply per-label thresholding, context windows, and imbalance-aware loss functions. The results show that fine-tuned models consistently outperform prompting-based approaches, achieving macro-F1 scores of 0.577 for video and 0.460 for transcripts. These results demonstrate the feasibility of automated classroom analysis and establish a foundation for scalable teacher feedback systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u5229\u7528\u4eba\u5de5\u667a\u80fd\u5206\u6790\u8bfe\u5802\u5f55\u50cf\uff0c\u4ee5\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u6559\u5e08\u53cd\u9988\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524d\u8bfe\u5802\u4e92\u52a8\u89c2\u5bdf\u4f9d\u8d56\u4e8e\u4eba\u5de5\u6807\u6ce8\uff0c\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\u3002", "method": "\u4f7f\u7528\u5305\u542b164\u5c0f\u65f6\u89c6\u9891\u548c68\u4efd\u8bfe\u7a0b\u8bb0\u5f55\u7684\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u4e86\u5e76\u884c\u7684\u3001\u7279\u5b9a\u6a21\u6001\u7684\u6d41\u7a0b\u3002\u89c6\u9891\u65b9\u9762\uff0c\u8bc4\u4f30\u4e86zero-shot\u591a\u6a21\u6001LLM\u3001\u5fae\u8c03\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u548c\u81ea\u76d1\u7763\u89c6\u9891transformer\u3002\u6587\u672c\u65b9\u9762\uff0c\u5fae\u8c03\u4e86\u57fa\u4e8etransformer\u7684\u5206\u7c7b\u5668\uff0c\u5e76\u4e0e\u57fa\u4e8eprompt\u7684LLM\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u91c7\u7528\u6309\u6807\u7b7e\u9608\u503c\u5904\u7406\u3001\u4e0a\u4e0b\u6587\u7a97\u53e3\u548c\u4e0d\u5e73\u8861\u611f\u77e5\u635f\u5931\u51fd\u6570\u6765\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u591a\u6807\u7b7e\u590d\u6742\u6027\u95ee\u9898\u3002", "result": "\u5fae\u8c03\u6a21\u578b\u7684\u6027\u80fd\u59cb\u7ec8\u4f18\u4e8e\u57fa\u4e8eprompt\u7684\u65b9\u6cd5\uff0c\u89c6\u9891\u7684macro-F1\u503c\u4e3a0.577\uff0c\u6587\u672c\u7684macro-F1\u503c\u4e3a0.460\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u81ea\u52a8\u5316\u8bfe\u5802\u5206\u6790\u662f\u53ef\u884c\u7684\uff0c\u5e76\u4e3a\u53ef\u6269\u5c55\u7684\u6559\u5e08\u53cd\u9988\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.01710", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.01710", "abs": "https://arxiv.org/abs/2512.01710", "authors": ["Stefano Zeppieri"], "title": "MMAG: Mixed Memory-Augmented Generation for Large Language Models Applications", "comment": null, "summary": "Large Language Models (LLMs) excel at generating coherent text within a single prompt but fall short in sustaining relevance, personalization, and continuity across extended interactions. Human communication, however, relies on multiple forms of memory, from recalling past conversations to adapting to personal traits and situational context. This paper introduces the Mixed Memory-Augmented Generation (MMAG) pattern, a framework that organizes memory for LLM-based agents into five interacting layers: conversational, long-term user, episodic and event-linked, sensory and context-aware, and short-term working memory. Drawing inspiration from cognitive psychology, we map these layers to technical components and outline strategies for coordination, prioritization, and conflict resolution. We demonstrate the approach through its implementation in the Heero conversational agent, where encrypted long-term bios and conversational history already improve engagement and retention. We further discuss implementation concerns around storage, retrieval, privacy, and latency, and highlight open challenges. MMAG provides a foundation for building memory-rich language agents that are more coherent, proactive, and aligned with human needs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6df7\u5408\u8bb0\u5fc6\u589e\u5f3a\u751f\u6210 (MMAG) \u6a21\u5f0f\uff0c\u7528\u4e8e\u63d0\u5347 LLM \u4ee3\u7406\u5728\u6269\u5c55\u4ea4\u4e92\u4e2d\u7684\u8fde\u8d2f\u6027\u3001\u4e2a\u6027\u5316\u548c\u8fde\u7eed\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u64c5\u957f\u5728\u5355\u4e2a\u63d0\u793a\u4e2d\u751f\u6210\u8fde\u8d2f\u7684\u6587\u672c\uff0c\u4f46\u5728\u6269\u5c55\u7684\u4ea4\u4e92\u4e2d\u4fdd\u6301\u76f8\u5173\u6027\u3001\u4e2a\u6027\u5316\u548c\u8fde\u7eed\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u4eba\u7c7b\u4ea4\u6d41\u4f9d\u8d56\u4e8e\u591a\u79cd\u5f62\u5f0f\u7684\u8bb0\u5fc6\u3002", "method": "\u5c06 LLM \u4ee3\u7406\u7684\u8bb0\u5fc6\u7ec4\u7ec7\u6210\u4e94\u4e2a\u4ea4\u4e92\u5c42\uff1a\u4f1a\u8bdd\u8bb0\u5fc6\u3001\u957f\u671f\u7528\u6237\u8bb0\u5fc6\u3001\u60c5\u666f\u548c\u4e8b\u4ef6\u94fe\u63a5\u8bb0\u5fc6\u3001\u611f\u89c9\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u8bb0\u5fc6\u4ee5\u53ca\u77ed\u671f\u5de5\u4f5c\u8bb0\u5fc6\u3002\u5c06\u8fd9\u4e9b\u5c42\u6620\u5c04\u5230\u6280\u672f\u7ec4\u4ef6\uff0c\u5e76\u6982\u8ff0\u4e86\u534f\u8c03\u3001\u4f18\u5148\u7ea7\u6392\u5e8f\u548c\u51b2\u7a81\u89e3\u51b3\u7684\u7b56\u7565\u3002", "result": "\u901a\u8fc7\u5728 Heero \u4f1a\u8bdd\u4ee3\u7406\u4e2d\u7684\u5b9e\u65bd\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\uff0c\u5176\u4e2d\u52a0\u5bc6\u7684\u957f\u671f\u7b80\u5386\u548c\u4f1a\u8bdd\u5386\u53f2\u5df2\u7ecf\u63d0\u9ad8\u4e86\u53c2\u4e0e\u5ea6\u548c\u4fdd\u7559\u7387\u3002", "conclusion": "MMAG \u4e3a\u6784\u5efa\u8bb0\u5fc6\u4e30\u5bcc\u7684\u8bed\u8a00\u4ee3\u7406\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u8fd9\u4e9b\u4ee3\u7406\u66f4\u8fde\u8d2f\u3001\u66f4\u4e3b\u52a8\u5e76\u4e14\u66f4\u7b26\u5408\u4eba\u7c7b\u9700\u6c42\u3002"}}
{"id": "2512.00670", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00670", "abs": "https://arxiv.org/abs/2512.00670", "authors": ["He-Yen Hsieh", "Hong Wang", "H. T. Kung"], "title": "EDIT: Early Diffusion Inference Termination for dLLMs Based on Dynamics of Training Gradients", "comment": "22 pages, 11 figures", "summary": "Diffusion-based large language models (dLLMs) refine token generations through iterative denoising, but answers often stabilize before all steps complete. We propose EDIT (Early Diffusion Inference Termination), an inference-time criterion that adaptively stops denoising once sufficient reasoning stability relative to training-time reasoning is detected. EDIT monitors the alignment between token activations and a reasoning map derived from AdamW-aggregated LoRA updates captured during supervised fine-tuning (SFT). During training, optimization dynamics generate rich metadata about parameter importance that in prior methods is typically discarded upon model release. We preserve this information as a compact representation of learned reasoning pathways. During inference, alignment scores are converted to a distribution over the tokens already unmasked at the current denoising step, and convergence is detected when KL divergence between consecutive steps falls below a threshold on the matched unmasked (visible) tokens. Across reasoning benchmarks, EDIT reduces diffusion steps by 11.8% to 68.3% while preserving or improving accuracy in most settings, with approximately 0.02% storage overhead (about 1.5-2 MB for all QKV modules across 32 blocks in an 8 GB model). By utilizing training-gradient dynamics, our work opens a new research direction for reducing dLLM inference time and cost.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEDIT\u7684\u63a8\u7406\u65f6\u6807\u51c6\uff0c\u7528\u4e8e\u81ea\u9002\u5e94\u5730\u505c\u6b62\u53bb\u566a\u8fc7\u7a0b\uff0c\u4e00\u65e6\u68c0\u6d4b\u5230\u76f8\u5bf9\u4e8e\u8bad\u7ec3\u65f6\u63a8\u7406\u7684\u5145\u5206\u63a8\u7406\u7a33\u5b9a\u6027\u3002", "motivation": "\u57fa\u4e8e\u6269\u6563\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b(dLLMs)\u901a\u8fc7\u8fed\u4ee3\u53bb\u566a\u6765\u6539\u8fdbtoken\u751f\u6210\uff0c\u4f46\u7b54\u6848\u901a\u5e38\u5728\u6240\u6709\u6b65\u9aa4\u5b8c\u6210\u4e4b\u524d\u7a33\u5b9a\u3002", "method": "EDIT \u76d1\u63a7 token \u6fc0\u6d3b\u548c\u4ece AdamW \u805a\u5408\u7684 LoRA \u66f4\u65b0\u4e2d\u5bfc\u51fa\u7684\u63a8\u7406\u56fe\u4e4b\u95f4\u7684\u5bf9\u9f50\u3002\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u5bf9\u9f50\u5206\u6570\u88ab\u8f6c\u6362\u4e3a\u5df2\u5728\u5f53\u524d\u53bb\u566a\u6b65\u9aa4\u4e2d\u672a\u5c4f\u853d\u7684 token \u7684\u5206\u5e03\uff0c\u5e76\u4e14\u5f53\u8fde\u7eed\u6b65\u9aa4\u4e4b\u95f4\u7684 KL \u6563\u5ea6\u4f4e\u4e8e\u5339\u914d\u7684\u672a\u5c4f\u853d\uff08\u53ef\u89c1\uff09token \u7684\u9608\u503c\u65f6\uff0c\u68c0\u6d4b\u5230\u6536\u655b\u3002", "result": "\u5728\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEDIT \u5c06\u6269\u6563\u6b65\u9aa4\u51cf\u5c11\u4e86 11.8% \u5230 68.3%\uff0c\u540c\u65f6\u5728\u5927\u591a\u6570\u8bbe\u7f6e\u4e2d\u4fdd\u6301\u6216\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u5927\u7ea6 0.02% \u7684\u5b58\u50a8\u5f00\u9500\uff08\u5bf9\u4e8e 8 GB \u6a21\u578b\u4e2d 32 \u4e2a\u5757\u7684\u6240\u6709 QKV \u6a21\u5757\uff0c\u7ea6\u4e3a 1.5-2 MB\uff09\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u8bad\u7ec3\u68af\u5ea6\u52a8\u6001\uff0c\u6211\u4eec\u7684\u5de5\u4f5c\u4e3a\u51cf\u5c11 dLLM \u63a8\u7406\u65f6\u95f4\u548c\u6210\u672c\u5f00\u8f9f\u4e86\u4e00\u4e2a\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2512.00272", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.00272", "abs": "https://arxiv.org/abs/2512.00272", "authors": ["Mohammad M Maheri", "Xavier Cadet", "Peter Chin", "Hamed Haddadi"], "title": "Teleportation-Based Defenses for Privacy in Approximate Machine Unlearning", "comment": null, "summary": "Approximate machine unlearning aims to efficiently remove the influence of specific data points from a trained model, offering a practical alternative to full retraining. However, it introduces privacy risks: an adversary with access to pre- and post-unlearning models can exploit their differences for membership inference or data reconstruction. We show these vulnerabilities arise from two factors: large gradient norms of forget-set samples and the close proximity of unlearned parameters to the original model. To demonstrate their severity, we propose unlearning-specific membership inference and reconstruction attacks, showing that several state-of-the-art methods (e.g., NGP, SCRUB) remain vulnerable. To mitigate this leakage, we introduce WARP, a plug-and-play teleportation defense that leverages neural network symmetries to reduce forget-set gradient energy and increase parameter dispersion while preserving predictions. This reparameterization obfuscates the signal of forgotten data, making it harder for attackers to distinguish forgotten samples from non-members or recover them via reconstruction. Across six unlearning algorithms, our approach achieves consistent privacy gains, reducing adversarial advantage (AUC) by up to 64% in black-box and 92% in white-box settings, while maintaining accuracy on retained data. These results highlight teleportation as a general tool for reducing attack success in approximate unlearning.", "AI": {"tldr": "\u8fd1\u4f3c\u673a\u5668\u9057\u5fd8\u65e8\u5728\u6709\u6548\u5730\u4ece\u8bad\u7ec3\u6a21\u578b\u4e2d\u79fb\u9664\u7279\u5b9a\u6570\u636e\u70b9\u7684\u5f71\u54cd\uff0c\u4f46\u4f1a\u5f15\u5165\u9690\u79c1\u98ce\u9669\u3002\u653b\u51fb\u8005\u53ef\u4ee5\u5229\u7528\u9057\u5fd8\u524d\u540e\u7684\u6a21\u578b\u5dee\u5f02\u8fdb\u884c\u6210\u5458\u63a8\u65ad\u6216\u6570\u636e\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u7684\u8fd1\u4f3c\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\u5b58\u5728\u9690\u79c1\u6f0f\u6d1e\uff0c\u56e0\u4e3a\u9057\u5fd8\u6837\u672c\u7684\u68af\u5ea6\u8303\u6570\u8f83\u5927\uff0c\u4e14\u9057\u5fd8\u540e\u7684\u53c2\u6570\u4e0e\u539f\u59cb\u6a21\u578b\u8fc7\u4e8e\u63a5\u8fd1\u3002", "method": "\u63d0\u51fa\u4e86WARP\uff0c\u4e00\u79cd\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u5bf9\u79f0\u6027\u7684\u5373\u63d2\u5373\u7528\u5f0f\u4f20\u9001\u9632\u5fa1\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u9057\u5fd8\u6837\u672c\u7684\u68af\u5ea6\u80fd\u91cf\u5e76\u589e\u52a0\u53c2\u6570\u79bb\u6563\u5ea6\u3002", "result": "WARP\u5728\u516d\u79cd\u9057\u5fd8\u7b97\u6cd5\u4e2d\u5b9e\u73b0\u4e86\u6301\u7eed\u7684\u9690\u79c1\u63d0\u5347\uff0c\u5728\u9ed1\u76d2\u8bbe\u7f6e\u4e2d\u5c06\u5bf9\u6297\u4f18\u52bf\uff08AUC\uff09\u964d\u4f4e\u4e86\u9ad8\u8fbe64%\uff0c\u5728\u767d\u76d2\u8bbe\u7f6e\u4e2d\u964d\u4f4e\u4e86\u9ad8\u8fbe92%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4fdd\u7559\u6570\u636e\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u4f20\u9001\u662f\u4e00\u79cd\u964d\u4f4e\u8fd1\u4f3c\u9057\u5fd8\u4e2d\u653b\u51fb\u6210\u529f\u7387\u7684\u901a\u7528\u5de5\u5177\u3002"}}
{"id": "2512.00088", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00088", "abs": "https://arxiv.org/abs/2512.00088", "authors": ["Mohammad Zare"], "title": "SemImage: Semantic Image Representation for Text, a Novel Framework for Embedding Disentangled Linguistic Features", "comment": null, "summary": "We propose SemImage, a novel method for representing a text document as a two-dimensional semantic image to be processed by convolutional neural networks (CNNs). In a SemImage, each word is represented as a pixel in a 2D image: rows correspond to sentences and an additional boundary row is inserted between sentences to mark semantic transitions. Each pixel is not a typical RGB value but a vector in a disentangled HSV color space, encoding different linguistic features: the Hue with two components H_cos and H_sin to account for circularity encodes the topic, Saturation encodes the sentiment, and Value encodes intensity or certainty. We enforce this disentanglement via a multi-task learning framework: a ColorMapper network maps each word embedding to the HSV space, and auxiliary supervision is applied to the Hue and Saturation channels to predict topic and sentiment labels, alongside the main task objective. The insertion of dynamically computed boundary rows between sentences yields sharp visual boundaries in the image when consecutive sentences are semantically dissimilar, effectively making paragraph breaks salient. We integrate SemImage with standard 2D CNNs (e.g., ResNet) for document classification. Experiments on multi-label datasets (with both topic and sentiment annotations) and single-label benchmarks demonstrate that SemImage can achieve competitive or better accuracy than strong text classification baselines (including BERT and hierarchical attention networks) while offering enhanced interpretability. An ablation study confirms the importance of the multi-channel HSV representation and the dynamic boundary rows. Finally, we present visualizations of SemImage that qualitatively reveal clear patterns corresponding to topic shifts and sentiment changes in the generated image, suggesting that our representation makes these linguistic features visible to both humans and machines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684 SemImage \u65b9\u6cd5\uff0c\u7528\u4e8e\u5c06\u6587\u672c\u6587\u6863\u8868\u793a\u4e3a\u4e8c\u7ef4\u8bed\u4e49\u56fe\u50cf\uff0c\u4ee5\u4f9b\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) \u5904\u7406\u3002", "motivation": "\u4e3a\u4e86\u66f4\u597d\u5730\u5229\u7528 CNN \u5904\u7406\u6587\u672c\u6570\u636e\uff0c\u5e76\u63d0\u9ad8\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5c06\u6bcf\u4e2a\u8bcd\u8868\u793a\u4e3a 2D \u56fe\u50cf\u4e2d\u7684\u50cf\u7d20\uff0c\u884c\u5bf9\u5e94\u4e8e\u53e5\u5b50\uff0c\u5e76\u5728\u53e5\u5b50\u4e4b\u95f4\u63d2\u5165\u989d\u5916\u7684\u8fb9\u754c\u884c\u4ee5\u6807\u8bb0\u8bed\u4e49\u8f6c\u6362\u3002\u6bcf\u4e2a\u50cf\u7d20\u90fd\u662f\u4e00\u4e2a\u89e3\u8026\u7684 HSV \u989c\u8272\u7a7a\u95f4\u4e2d\u7684\u5411\u91cf\uff0c\u7f16\u7801\u4e0d\u540c\u7684\u8bed\u8a00\u7279\u5f81\uff1a\u8272\u8c03\u7f16\u7801\u4e3b\u9898\uff0c\u9971\u548c\u5ea6\u7f16\u7801\u60c5\u611f\uff0c\u503c\u7f16\u7801\u5f3a\u5ea6\u6216\u786e\u5b9a\u6027\u3002", "result": "\u5728\u591a\u6807\u7b7e\u6570\u636e\u96c6\u548c\u5355\u6807\u7b7e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSemImage \u53ef\u4ee5\u5b9e\u73b0\u4e0e\u5f3a\u5927\u7684\u6587\u672c\u5206\u7c7b\u57fa\u7ebf\uff08\u5305\u62ec BERT \u548c\u5206\u5c42\u6ce8\u610f\u529b\u7f51\u7edc\uff09\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u6216\u66f4\u597d\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u63d0\u4f9b\u589e\u5f3a\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "SemImage \u80fd\u591f\u6709\u6548\u5730\u5c06\u6587\u672c\u8f6c\u6362\u4e3a\u56fe\u50cf\uff0c\u5e76\u4f7f\u8bed\u8a00\u7279\u5f81\u5bf9\u4eba\u7c7b\u548c\u673a\u5668\u90fd\u53ef\u89c1\u3002"}}
{"id": "2512.00683", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00683", "abs": "https://arxiv.org/abs/2512.00683", "authors": ["Wu Yonggang"], "title": "Model of human cognition", "comment": null, "summary": "The development of large language models (LLMs) is limited by a lack of explainability, the absence of a unifying theory, and prohibitive operational costs. We propose a neuro-theoretical framework for the emergence of intelligence in systems that is both functionally robust and biologically plausible. The model provides theoretical insights into cognitive processes such as decision-making and problem solving, and a computationally efficient approach for the creation of explainable and generalizable artificial intelligence.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u91ca\u7cfb\u7edf\u4e2d\u667a\u80fd\u7684\u51fa\u73b0\uff0c\u8be5\u6846\u67b6\u5728\u529f\u80fd\u4e0a\u7a33\u5065\u4e14\u5728\u751f\u7269\u5b66\u4e0a\u5408\u7406\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u53d1\u5c55\u53d7\u5230\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3001\u7f3a\u4e4f\u7edf\u4e00\u7406\u8bba\u4ee5\u53ca\u8fc7\u9ad8\u7684\u8fd0\u8425\u6210\u672c\u7684\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u795e\u7ecf\u7406\u8bba\u6846\u67b6\u3002", "result": "\u8be5\u6a21\u578b\u4e3a\u51b3\u7b56\u548c\u95ee\u9898\u89e3\u51b3\u7b49\u8ba4\u77e5\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u7406\u8bba\u89c1\u89e3\uff0c\u5e76\u4e3a\u521b\u5efa\u53ef\u89e3\u91ca\u548c\u53ef\u63a8\u5e7f\u7684\u4eba\u5de5\u667a\u80fd\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6a21\u578b functionally robust \u4e14 biologically plausible\uff0c\u80fd\u4e3a\u8ba4\u77e5\u8fc7\u7a0b\u63d0\u4f9b\u7406\u8bba\u89c1\u89e3\uff0c\u5e76\u4e3a\u521b\u5efa\u66f4\u4f18\u79c0\u7684\u4eba\u5de5\u667a\u80fd\u63d0\u4f9b\u8ba1\u7b97\u65b9\u6cd5\u3002"}}
{"id": "2512.00392", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00392", "abs": "https://arxiv.org/abs/2512.00392", "authors": ["Damian Heywood", "Joseph Andrew Carrier", "Kyu-Hong Hwang"], "title": "A Taxonomy of Errors in English as she is spoke: Toward an AI-Based Method of Error Analysis for EFL Writing Instruction", "comment": "Metadata at \"Replication Data for: A Taxonomy of Errors in English as she is spoke: An AI-Based System for Error Analysis for EFL Writing Instruction\", https://doi.org/10.7910/DVN/N5O7C4, Harvard Dataverse, V1", "summary": "This study describes the development of an AI-assisted error analysis system designed to identify, categorize, and correct writing errors in English. Utilizing Large Language Models (LLMs) like Claude 3.5 Sonnet and DeepSeek R1, the system employs a detailed taxonomy grounded in linguistic theories from Corder (1967), Richards (1971), and James (1998). Errors are classified at both word and sentence levels, covering spelling, grammar, and punctuation. Implemented through Python-coded API calls, the system provides granular feedback beyond traditional rubric-based assessments. Initial testing on isolated errors refined the taxonomy, addressing challenges like overlapping categories. Final testing used \"English as she is spoke\" by Jose da Fonseca (1855), a text rich with authentic linguistic errors, to evaluate the system's capacity for handling complex, multi-layered analysis. The AI successfully identified diverse error types but showed limitations in contextual understanding and occasionally generated new error categories when encountering uncoded errors. This research demonstrates AI's potential to transform EFL instruction by automating detailed error analysis and feedback. While promising, further development is needed to improve contextual accuracy and expand the taxonomy to stylistic and discourse-level errors.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2aAI\u8f85\u52a9\u7684\u9519\u8bef\u5206\u6790\u7cfb\u7edf\uff0c\u7528\u4e8e\u8bc6\u522b\u3001\u5206\u7c7b\u548c\u7ea0\u6b63\u82f1\u8bed\u5199\u4f5c\u9519\u8bef\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7AI\u81ea\u52a8\u8fdb\u884c\u8be6\u7ec6\u7684\u9519\u8bef\u5206\u6790\u548c\u53cd\u9988\uff0c\u4ece\u800c\u6539\u53d8EFL\u6559\u5b66\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u91c7\u7528\u57fa\u4e8e\u8bed\u8a00\u5b66\u7406\u8bba\u7684\u8be6\u7ec6\u5206\u7c7b\u6cd5\uff0c\u901a\u8fc7Python API\u8c03\u7528\u5b9e\u73b0\u3002", "result": "AI\u6210\u529f\u8bc6\u522b\u4e86\u591a\u79cd\u9519\u8bef\u7c7b\u578b\uff0c\u4f46\u5728\u4e0a\u4e0b\u6587\u7406\u89e3\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5e76\u4e14\u5728\u9047\u5230\u672a\u7f16\u7801\u7684\u9519\u8bef\u65f6\u5076\u5c14\u4f1a\u751f\u6210\u65b0\u7684\u9519\u8bef\u7c7b\u522b\u3002", "conclusion": "AI\u5728\u81ea\u52a8\u5316\u8be6\u7ec6\u9519\u8bef\u5206\u6790\u548c\u53cd\u9988\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u53d1\u5c55\u4ee5\u63d0\u9ad8\u4e0a\u4e0b\u6587\u51c6\u786e\u6027\u5e76\u6269\u5c55\u5230\u6587\u4f53\u548c\u8bed\u7bc7\u5c42\u9762\u7684\u9519\u8bef\u3002"}}
{"id": "2512.00283", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2512.00283", "abs": "https://arxiv.org/abs/2512.00283", "authors": ["Yi Fang", "Haoran Xu", "Jiaxin Han", "Sirui Ding", "Yizhi Wang", "Yue Wang", "Xuan Wang"], "title": "BioArc: Discovering Optimal Neural Architectures for Biological Foundation Models", "comment": null, "summary": "Foundation models have revolutionized various fields such as natural language processing (NLP) and computer vision (CV). While efforts have been made to transfer the success of the foundation models in general AI domains to biology, existing works focus on directly adopting the existing foundation model architectures from general machine learning domains without a systematic design considering the unique physicochemical and structural properties of each biological data modality. This leads to suboptimal performance, as these repurposed architectures struggle to capture the long-range dependencies, sparse information, and complex underlying ``grammars'' inherent to biological data. To address this gap, we introduce BioArc, a novel framework designed to move beyond intuition-driven architecture design towards principled, automated architecture discovery for biological foundation models. Leveraging Neural Architecture Search (NAS), BioArc systematically explores a vast architecture design space, evaluating architectures across multiple biological modalities while rigorously analyzing the interplay between architecture, tokenization, and training strategies. This large-scale analysis identifies novel, high-performance architectures, allowing us to distill a set of empirical design principles to guide future model development. Furthermore, to make the best of this set of discovered principled architectures, we propose and compare several architecture prediction methods that effectively and efficiently predict optimal architectures for new biological tasks. Overall, our work provides a foundational resource and a principled methodology to guide the creation of the next generation of task-specific and foundation models for biology.", "AI": {"tldr": "\u63d0\u51fa\u4e86BioArc\uff0c\u4e00\u4e2a\u7528\u4e8e\u751f\u7269\u57fa\u7840\u6a21\u578b\u7684\u81ea\u52a8\u67b6\u6784\u53d1\u73b0\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u76f4\u63a5\u91c7\u7528\u901a\u7528\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u57fa\u7840\u6a21\u578b\u67b6\u6784\uff0c\u6ca1\u6709\u7cfb\u7edf\u5730\u8003\u8651\u6bcf\u79cd\u751f\u7269\u6570\u636e\u6a21\u6001\u7684\u72ec\u7279\u7684\u7269\u7406\u5316\u5b66\u548c\u7ed3\u6784\u7279\u6027\uff0c\u5bfc\u81f4\u6b21\u4f18\u6027\u80fd\u3002", "method": "\u5229\u7528\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\uff0c\u7cfb\u7edf\u5730\u63a2\u7d22\u4e86\u4e00\u4e2a\u5de8\u5927\u7684\u67b6\u6784\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u8bc4\u4f30\u8de8\u591a\u4e2a\u751f\u7269\u6a21\u6001\u7684\u67b6\u6784\uff0c\u540c\u65f6\u4e25\u683c\u5206\u6790\u67b6\u6784\u3001\u6807\u8bb0\u5316\u548c\u8bad\u7ec3\u7b56\u7565\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u8bc6\u522b\u51fa\u65b0\u7684\u3001\u9ad8\u6027\u80fd\u7684\u67b6\u6784\uff0c\u4ece\u800c\u80fd\u591f\u63d0\u70bc\u51fa\u4e00\u5957\u7ecf\u9a8c\u8bbe\u8ba1\u539f\u5219\uff0c\u4ee5\u6307\u5bfc\u672a\u6765\u7684\u6a21\u578b\u5f00\u53d1\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u7840\u8d44\u6e90\u548c\u4e00\u4e2a\u6709\u539f\u5219\u7684\u65b9\u6cd5\uff0c\u4ee5\u6307\u5bfc\u4e0b\u4e00\u4ee3\u751f\u7269\u4efb\u52a1\u7279\u5b9a\u548c\u57fa\u7840\u6a21\u578b\u7684\u521b\u5efa\u3002"}}
{"id": "2512.00089", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00089", "abs": "https://arxiv.org/abs/2512.00089", "authors": ["Ioannis Prapas", "Nikolaos Papadopoulos", "Nikolaos-Ioannis Bountos", "Dimitrios Michail", "Gustau Camps-Valls", "Ioannis Papoutsis"], "title": "TeleViT1.0: Teleconnection-aware Vision Transformers for Subseasonal to Seasonal Wildfire Pattern Forecasts", "comment": "Under review", "summary": "Forecasting wildfires weeks to months in advance is difficult, yet crucial for planning fuel treatments and allocating resources. While short-term predictions typically rely on local weather conditions, long-term forecasting requires accounting for the Earth's interconnectedness, including global patterns and teleconnections. We introduce TeleViT, a Teleconnection-aware Vision Transformer that integrates (i) fine-scale local fire drivers, (ii) coarsened global fields, and (iii) teleconnection indices. This multi-scale fusion is achieved through an asymmetric tokenization strategy that produces heterogeneous tokens processed jointly by a transformer encoder, followed by a decoder that preserves spatial structure by mapping local tokens to their corresponding prediction patches.\n  Using the global SeasFire dataset (2001-2021, 8-day resolution), TeleViT improves AUPRC performance over U-Net++, ViT, and climatology across all lead times, including horizons up to four months. At zero lead, TeleViT with indices and global inputs reaches AUPRC 0.630 (ViT 0.617, U-Net 0.620), at 16x8day lead (around 4 months), TeleViT variants using global input maintain 0.601-0.603 (ViT 0.582, U-Net 0.578), while surpassing the climatology (0.572) at all lead times. Regional results show the highest skill in seasonally consistent fire regimes, such as African savannas, and lower skill in boreal and arid regions. Attention and attribution analyses indicate that predictions rely mainly on local tokens, with global fields and indices contributing coarse contextual information. These findings suggest that architectures explicitly encoding large-scale Earth-system context can extend wildfire predictability on subseasonal-to-seasonal timescales.", "AI": {"tldr": "TeleViT\u6a21\u578b\u901a\u8fc7\u7ed3\u5408\u5c40\u90e8\u706b\u707e\u9a71\u52a8\u56e0\u7d20\u3001\u5168\u5c40\u573a\u548c\u9065\u76f8\u5173\u6307\u6570\uff0c\u6539\u8fdb\u4e86\u91ce\u706b\u9884\u6d4b\u3002", "motivation": "\u63d0\u524d\u6570\u5468\u81f3\u6570\u6708\u9884\u6d4b\u91ce\u706b\u5bf9\u4e8e\u71c3\u6599\u5904\u7406\u548c\u8d44\u6e90\u5206\u914d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5177\u6709\u6311\u6218\u6027\u3002\u957f\u671f\u9884\u6d4b\u9700\u8981\u8003\u8651\u5730\u7403\u7684\u76f8\u4e92\u8054\u7cfb\uff0c\u5305\u62ec\u5168\u7403\u6a21\u5f0f\u548c\u9065\u76f8\u5173\u3002", "method": "TeleViT\u662f\u4e00\u79cd\u9065\u76f8\u5173\u611f\u77e5\u89c6\u89c9Transformer\uff0c\u5b83\u901a\u8fc7\u975e\u5bf9\u79f0tokenization\u7b56\u7565\u878d\u5408\u591a\u5c3a\u5ea6\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7transformer\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u5904\u7406\u5f02\u6784tokens\uff0c\u4ece\u800c\u4fdd\u6301\u7a7a\u95f4\u7ed3\u6784\u3002", "result": "TeleViT\u5728\u4f7f\u7528\u5168\u7403SeasFire\u6570\u636e\u96c6\uff082001-2021\uff0c8\u5929\u5206\u8fa8\u7387\uff09\u65f6\uff0c\u5728\u6240\u6709\u63d0\u524d\u671f\uff08\u5305\u62ec\u957f\u8fbe\u56db\u4e2a\u6708\u7684\u8303\u56f4\uff09\u5185\uff0c\u63d0\u9ad8\u4e86AUPRC\u6027\u80fd\u3002", "conclusion": "\u660e\u786e\u7f16\u7801\u5927\u89c4\u6a21\u5730\u7403\u7cfb\u7edf\u4e0a\u4e0b\u6587\u7684\u67b6\u6784\u53ef\u4ee5\u6269\u5c55\u4e9a\u5b63\u8282\u5230\u5b63\u8282\u6027\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u7684\u91ce\u706b\u53ef\u9884\u6d4b\u6027\u3002"}}
{"id": "2512.00709", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00709", "abs": "https://arxiv.org/abs/2512.00709", "authors": ["Yifan Xu", "Xichen Ye", "Yifan Chen", "Qiaosheng Zhang"], "title": "When Human Preferences Flip: An Instance-Dependent Robust Loss for RLHF", "comment": "Accepted by AAAI-26-AIA", "summary": "Quality of datasets plays an important role in large language model (LLM) alignment. In collecting human feedback, however, preference flipping is ubiquitous and causes corruption in data annotation; the issue necessitates the alignment algorithms with improved robustness against potential flipped pairs. To this end, this paper introduces a Flipping-Aware Direct Preference Optimization (FA-DPO) algorithm tailored to preference flipping from a reinforcement learning with human feedback (RLHF) perspective. We dissect the inherent human intention model and the preference flipping mechanism introduced by external factors as two distinct stages; in the latter, we introduce an instance-dependent flipping probability on the basis of the Bradley-Terry (BT) model. Further, by leveraging features relevant to preference annotation, we capture uncertainty in judgments and model preference flipping patterns. In practice, we design a simple yet efficient iterative optimization algorithm compatible with the original RLHF and DPO algorithms. In our experiments, we investigate the instance-dependent preference flipping model under multiple circumstances for evaluation of our proposed method, as well as other baseline methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a Flipping-Aware Direct Preference Optimization (FA-DPO) \u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5bf9\u9f50\u4e2d\u666e\u904d\u5b58\u5728\u7684 preference flipping \u95ee\u9898\u3002", "motivation": "\u6570\u636e\u96c6\u8d28\u91cf\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5bf9\u9f50\u4e2d\u8d77\u7740\u91cd\u8981\u4f5c\u7528\u3002\u7136\u800c\uff0c\u5728\u6536\u96c6\u4eba\u7c7b\u53cd\u9988\u65f6\uff0cpreference flipping \u662f\u666e\u904d\u5b58\u5728\u7684\uff0c\u4f1a\u5bfc\u81f4\u6570\u636e\u6ce8\u91ca\u635f\u574f\uff1b\u56e0\u6b64\uff0c\u9700\u8981\u6539\u8fdb\u5bf9\u6f5c\u5728 flipped pairs \u7684\u9c81\u68d2\u6027\u7684\u5bf9\u9f50\u7b97\u6cd5\u3002", "method": "\u4ece\u4eba\u4e0e\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60 (RLHF) \u89d2\u5ea6\uff0c\u5c06\u5185\u5728\u7684\u4eba\u7c7b\u610f\u56fe\u6a21\u578b\u548c\u7531\u5916\u90e8\u56e0\u7d20\u5f15\u5165\u7684 preference flipping \u673a\u5236\u5206\u89e3\u4e3a\u4e24\u4e2a\u4e0d\u540c\u7684\u9636\u6bb5\uff1b\u5728\u540e\u8005\u4e2d\uff0c\u5728 Bradley-Terry (BT) \u6a21\u578b\u7684\u57fa\u7840\u4e0a\u5f15\u5165\u4e86\u5b9e\u4f8b\u76f8\u5173\u7684 flipping \u6982\u7387\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5229\u7528\u4e0e\u504f\u597d\u6ce8\u91ca\u76f8\u5173\u7684\u7279\u5f81\uff0c\u6211\u4eec\u6355\u6349\u5224\u65ad\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u5e76\u5bf9\u504f\u597d\u7ffb\u8f6c\u6a21\u5f0f\u8fdb\u884c\u5efa\u6a21\u3002\u5728\u5b9e\u8df5\u4e2d\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u6709\u6548\u7684\u8fed\u4ee3\u4f18\u5316\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u4e0e\u539f\u59cb RLHF \u548c DPO \u7b97\u6cd5\u517c\u5bb9\u3002", "result": "\u5728\u6211\u4eec\u7684\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u5728\u591a\u79cd\u60c5\u51b5\u4e0b\u5b9e\u4f8b\u76f8\u5173\u7684\u504f\u597d\u7ffb\u8f6c\u6a21\u578b\uff0c\u4ee5\u8bc4\u4f30\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u4ee5\u53ca\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5 FA-DPO\uff0c\u65e8\u5728\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b58\u5728 preference flipping \u60c5\u51b5\u4e0b\u7684\u5bf9\u9f50\u6027\u80fd\u3002"}}
{"id": "2512.00417", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00417", "abs": "https://arxiv.org/abs/2512.00417", "authors": ["Jiacheng Guo", "Suozhi Huang", "Zixin Yao", "Yifan Zhang", "Yifu Lu", "Jiashuo Liu", "Zihao Li", "Yanyan Deng", "Qixin Xiao", "Jia Tian", "Kanghong Zhan", "Tianyi Li", "Xiaochen Liu", "Jason Ge", "Chaoyang He", "Kaixuan Huang", "Lin Yang", "Wenhao Huang", "Mengdi Wang"], "title": "CryptoBench: A Dynamic Benchmark for Expert-Level Evaluation of LLM Agents in Cryptocurrency", "comment": null, "summary": "This paper introduces CryptoBench, the first expert-curated, dynamic benchmark designed to rigorously evaluate the real-world capabilities of Large Language Model (LLM) agents in the uniquely demanding and fast-paced cryptocurrency domain. Unlike general-purpose agent benchmarks for search and prediction, professional crypto analysis presents specific challenges: \\emph{extreme time-sensitivity}, \\emph{a highly adversarial information environment}, and the critical need to synthesize data from \\emph{diverse, specialized sources}, such as on-chain intelligence platforms and real-time Decentralized Finance (DeFi) dashboards. CryptoBench thus serves as a much more challenging and valuable scenario for LLM agent assessment. To address these challenges, we constructed a live, dynamic benchmark featuring 50 questions per month, expertly designed by crypto-native professionals to mirror actual analyst workflows. These tasks are rigorously categorized within a four-quadrant system: Simple Retrieval, Complex Retrieval, Simple Prediction, and Complex Prediction. This granular categorization enables a precise assessment of an LLM agent's foundational data-gathering capabilities alongside its advanced analytical and forecasting skills.\n  Our evaluation of ten LLMs, both directly and within an agentic framework, reveals a performance hierarchy and uncovers a failure mode. We observe a \\textit{retrieval-prediction imbalance}, where many leading models, despite being proficient at data retrieval, demonstrate a pronounced weakness in tasks requiring predictive analysis. This highlights a problematic tendency for agents to appear factually grounded while lacking the deeper analytical capabilities to synthesize information.", "AI": {"tldr": "CryptoBench\u662f\u4e00\u4e2a\u4e13\u95e8\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u52a0\u5bc6\u8d27\u5e01\u9886\u57df\u80fd\u529b\u7684\u52a8\u6001\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709\u7684\u901a\u7528\u57fa\u51c6\u4e0d\u80fd\u6ee1\u8db3\u52a0\u5bc6\u8d27\u5e01\u9886\u57df\u5bf9LLM\u7684\u72ec\u7279\u8981\u6c42\uff0c\u5982\u65f6\u95f4\u654f\u611f\u6027\u3001\u5bf9\u6297\u6027\u4fe1\u606f\u73af\u5883\u548c\u591a\u6837\u5316\u6570\u636e\u6e90\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u6bcf\u670850\u4e2a\u95ee\u9898\u7684\u52a8\u6001\u57fa\u51c6\uff0c\u95ee\u9898\u7531\u52a0\u5bc6\u8d27\u5e01\u4e13\u4e1a\u4eba\u58eb\u8bbe\u8ba1\uff0c\u5e76\u5206\u4e3a\u7b80\u5355\u68c0\u7d22\u3001\u590d\u6742\u68c0\u7d22\u3001\u7b80\u5355\u9884\u6d4b\u548c\u590d\u6742\u9884\u6d4b\u56db\u4e2a\u8c61\u9650\u3002", "result": "\u5bf910\u4e2aLLM\u7684\u8bc4\u4f30\u63ed\u793a\u4e86\u6027\u80fd\u7b49\u7ea7\u548c\u4e00\u79cd\u5931\u8d25\u6a21\u5f0f\uff1a\u68c0\u7d22-\u9884\u6d4b\u5931\u8861\uff0c\u5373\u6a21\u578b\u5728\u6570\u636e\u68c0\u7d22\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9884\u6d4b\u5206\u6790\u65b9\u9762\u5b58\u5728\u660e\u663e\u4e0d\u8db3\u3002", "conclusion": "\u8bb8\u591a\u6a21\u578b\u5728\u68c0\u7d22\u6570\u636e\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7efc\u5408\u4fe1\u606f\u8fdb\u884c\u6df1\u5165\u5206\u6790\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002"}}
{"id": "2512.00289", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.00289", "abs": "https://arxiv.org/abs/2512.00289", "authors": ["Nguyen Ly", "Caroline Tatsuoka", "Jai Nagaraj", "Jacob Levy", "Fernando Palafox", "David Fridovich-Keil", "Hannah Lu"], "title": "Data-Driven Modeling and Correction of Vehicle Dynamics", "comment": null, "summary": "We develop a data-driven framework for learning and correcting non-autonomous vehicle dynamics. Physics-based vehicle models are often simplified for tractability and therefore exhibit inherent model-form uncertainty, motivating the need for data-driven correction. Moreover, non-autonomous dynamics are governed by time-dependent control inputs, which pose challenges in learning predictive models directly from temporal snapshot data. To address these, we reformulate the vehicle dynamics via a local parameterization of the time-dependent inputs, yielding a modified system composed of a sequence of local parametric dynamical systems. We approximate these parametric systems using two complementary approaches. First, we employ the DRIPS (dimension reduction and interpolation in parameter space) methodology to construct efficient linear surrogate models, equipped with lifted observable spaces and manifold-based operator interpolation. This enables data-efficient learning of vehicle models whose dynamics admit accurate linear representations in the lifted spaces. Second, for more strongly nonlinear systems, we employ FML (Flow Map Learning), a deep neural network approach that approximates the parametric evolution map without requiring special treatment of nonlinearities. We further extend FML with a transfer-learning-based model correction procedure, enabling the correction of misspecified prior models using only a sparse set of high-fidelity or experimental measurements, without assuming a prescribed form for the correction term. Through a suite of numerical experiments on unicycle, simplified bicycle, and slip-based bicycle models, we demonstrate that DRIPS offers robust and highly data-efficient learning of non-autonomous vehicle dynamics, while FML provides expressive nonlinear modeling and effective correction of model-form errors under severe data scarcity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u548c\u6821\u6b63\u975e\u81ea\u6cbb\u8f66\u8f86\u52a8\u529b\u5b66\u3002", "motivation": "\u7269\u7406\u8f66\u8f86\u6a21\u578b\u901a\u5e38\u4e3a\u4e86\u6613\u4e8e\u5904\u7406\u800c\u88ab\u7b80\u5316\uff0c\u56e0\u6b64\u5b58\u5728\u56fa\u6709\u7684\u6a21\u578b\u5f62\u5f0f\u4e0d\u786e\u5b9a\u6027\uff0c\u8fd9\u4fc3\u4f7f\u4eba\u4eec\u9700\u8981\u8fdb\u884c\u6570\u636e\u9a71\u52a8\u7684\u6821\u6b63\u3002\u6b64\u5916\uff0c\u975e\u81ea\u6cbb\u52a8\u529b\u5b66\u7531\u65f6\u95f4\u76f8\u5173\u7684\u63a7\u5236\u8f93\u5165\u63a7\u5236\uff0c\u8fd9\u7ed9\u76f4\u63a5\u4ece\u65f6\u95f4\u5feb\u7167\u6570\u636e\u4e2d\u5b66\u4e60\u9884\u6d4b\u6a21\u578b\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u65f6\u95f4\u76f8\u5173\u8f93\u5165\u7684\u5c40\u90e8\u53c2\u6570\u5316\u6765\u91cd\u65b0\u6784\u5efa\u8f66\u8f86\u52a8\u529b\u5b66\uff0c\u4ece\u800c\u4ea7\u751f\u4e00\u4e2a\u7531\u4e00\u7cfb\u5217\u5c40\u90e8\u53c2\u6570\u5316\u52a8\u529b\u5b66\u7cfb\u7edf\u7ec4\u6210\u7684\u4fee\u6539\u7cfb\u7edf\u3002\u6211\u4eec\u4f7f\u7528\u4e24\u79cd\u4e92\u8865\u7684\u65b9\u6cd5\u6765\u8fd1\u4f3c\u8fd9\u4e9b\u53c2\u6570\u5316\u7cfb\u7edf\u3002\u9996\u5148\uff0c\u6211\u4eec\u91c7\u7528 DRIPS\uff08\u53c2\u6570\u7a7a\u95f4\u4e2d\u7684\u964d\u7ef4\u548c\u63d2\u503c\uff09\u65b9\u6cd5\u6765\u6784\u5efa\u6709\u6548\u7684\u7ebf\u6027\u66ff\u4ee3\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u914d\u5907\u4e86\u63d0\u5347\u7684\u53ef\u89c2\u5bdf\u7a7a\u95f4\u548c\u57fa\u4e8e\u6d41\u5f62\u7684\u7b97\u5b50\u63d2\u503c\u3002\u8fd9\u4f7f\u5f97\u80fd\u591f\u5bf9\u8f66\u8f86\u6a21\u578b\u8fdb\u884c\u6570\u636e\u9ad8\u6548\u5b66\u4e60\uff0c\u5176\u52a8\u529b\u5b66\u5728\u63d0\u5347\u7a7a\u95f4\u4e2d\u5141\u8bb8\u7cbe\u786e\u7684\u7ebf\u6027\u8868\u793a\u3002\u5176\u6b21\uff0c\u5bf9\u4e8e\u66f4\u5f3a\u7684\u975e\u7ebf\u6027\u7cfb\u7edf\uff0c\u6211\u4eec\u91c7\u7528 FML\uff08\u6d41\u56fe\u5b66\u4e60\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u5b83\u8fd1\u4f3c\u4e8e\u53c2\u6570\u5316\u6f14\u5316\u56fe\uff0c\u800c\u65e0\u9700\u7279\u6b8a\u5904\u7406\u975e\u7ebf\u6027\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u6269\u5c55\u4e86 FML\uff0c\u91c7\u7528\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fc1\u79fb\u5b66\u4e60\u7684\u6a21\u578b\u6821\u6b63\u7a0b\u5e8f\uff0c\u4ece\u800c\u80fd\u591f\u4ec5\u4f7f\u7528\u4e00\u7ec4\u7a00\u758f\u7684\u9ad8\u4fdd\u771f\u6216\u5b9e\u9a8c\u6d4b\u91cf\u503c\u6765\u6821\u6b63\u9519\u8bef\u6307\u5b9a\u7684\u5148\u9a8c\u6a21\u578b\uff0c\u800c\u65e0\u9700\u5047\u8bbe\u6821\u6b63\u9879\u7684\u89c4\u5b9a\u5f62\u5f0f\u3002", "result": "\u901a\u8fc7\u5bf9\u5355\u8f6e\u8f66\u3001\u7b80\u5316\u81ea\u884c\u8f66\u548c\u57fa\u4e8e\u6ed1\u79fb\u7684\u81ea\u884c\u8f66\u6a21\u578b\u8fdb\u884c\u7684\u4e00\u7cfb\u5217\u6570\u503c\u5b9e\u9a8c\uff0c\u6211\u4eec\u8bc1\u660e\u4e86 DRIPS \u63d0\u4f9b\u4e86\u5bf9\u975e\u81ea\u6cbb\u8f66\u8f86\u52a8\u529b\u5b66\u7684\u7a33\u5065\u4e14\u9ad8\u5ea6\u6570\u636e\u9ad8\u6548\u7684\u5b66\u4e60\uff0c\u800c FML \u63d0\u4f9b\u4e86\u5bcc\u6709\u8868\u73b0\u529b\u7684\u975e\u7ebf\u6027\u5efa\u6a21\u548c\u5728\u4e25\u91cd\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\u5bf9\u6a21\u578b\u5f62\u5f0f\u8bef\u5dee\u7684\u6709\u6548\u6821\u6b63\u3002", "conclusion": "DRIPS \u63d0\u4f9b\u4e86\u5bf9\u975e\u81ea\u6cbb\u8f66\u8f86\u52a8\u529b\u5b66\u7684\u7a33\u5065\u4e14\u9ad8\u5ea6\u6570\u636e\u9ad8\u6548\u7684\u5b66\u4e60\uff0c\u800c FML \u63d0\u4f9b\u4e86\u5bcc\u6709\u8868\u73b0\u529b\u7684\u975e\u7ebf\u6027\u5efa\u6a21\u548c\u5728\u4e25\u91cd\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\u5bf9\u6a21\u578b\u5f62\u5f0f\u8bef\u5dee\u7684\u6709\u6548\u6821\u6b63\u3002"}}
{"id": "2512.00091", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00091", "abs": "https://arxiv.org/abs/2512.00091", "authors": ["Karam Mawas", "Mehdi Maboudi", "Pedro Achanccaray", "Markus Gerke"], "title": "Deep Filament Extraction for 3D Concrete Printing", "comment": null, "summary": "The architecture, engineering and construction (AEC) industry is constantly evolving to meet the demand for sustainable and effective design and construction of the built environment. In the literature, two primary deposition techniques for large-scale 3D concrete printing (3DCP) have been described, namely extrusion-based (Contour Crafting-CC) and shotcrete 3D printing (SC3DP) methods. The deposition methods use a digitally controlled nozzle to print material layer by layer. The continuous flow of concrete material used to create the printed structure is called a filament or layer. As these filaments are the essential structure defining the printed object, the filaments' geometry quality control is crucial. This paper presents an automated procedure for quality control (QC) of filaments in extrusion-based and SC3DP printing methods. The paper also describes a workflow that is independent of the sensor used for data acquisition, such as a camera, a structured light system (SLS) or a terrestrial laser scanner (TLS). This method can be used with materials in either the fresh or cured state. Thus, it can be used for online and post-printing QC.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6324\u538b\u5f0f\u548c\u55b7\u5c04\u5f0f3D\u6df7\u51dd\u571f\u6253\u5370\uff083DCP\uff09\u4e2d\u4e1d\u6750\u8d28\u91cf\u63a7\u5236\uff08QC\uff09\u7684\u81ea\u52a8\u5316\u7a0b\u5e8f\u3002", "motivation": "\u5efa\u7b51\u3001\u5de5\u7a0b\u548c\u65bd\u5de5\uff08AEC\uff09\u884c\u4e1a\u4e0d\u65ad\u53d1\u5c55\uff0c\u4ee5\u6ee1\u8db3\u5bf9\u53ef\u6301\u7eed\u548c\u6709\u6548\u7684\u8bbe\u8ba1\u548c\u5efa\u9020\u73af\u5883\u7684\u9700\u6c42\u3002\u5728\u6587\u732e\u4e2d\uff0c\u5df2\u7ecf\u63cf\u8ff0\u4e86\u4e24\u79cd\u7528\u4e8e\u5927\u89c4\u6a213D\u6df7\u51dd\u571f\u6253\u5370\uff083DCP\uff09\u7684\u4e3b\u8981\u6c89\u79ef\u6280\u672f\uff0c\u5373\u57fa\u4e8e\u6324\u538b\u7684\uff08\u8f6e\u5ed3\u5de5\u827a-CC\uff09\u548c\u55b7\u5c04\u6df7\u51dd\u571f3D\u6253\u5370\uff08SC3DP\uff09\u65b9\u6cd5\u3002\u7531\u4e8e\u8fd9\u4e9b\u4e1d\u6750\u662f\u5b9a\u4e49\u6253\u5370\u5bf9\u8c61\u7684\u57fa\u672c\u7ed3\u6784\uff0c\u56e0\u6b64\u4e1d\u6750\u7684\u51e0\u4f55\u8d28\u91cf\u63a7\u5236\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7528\u4e8e\u6324\u538b\u5f0f\u548cSC3DP\u6253\u5370\u65b9\u6cd5\u4e2d\u4e1d\u6750\u8d28\u91cf\u63a7\u5236\uff08QC\uff09\u7684\u81ea\u52a8\u5316\u7a0b\u5e8f\u3002\u672c\u6587\u8fd8\u4ecb\u7ecd\u4e86\u4e00\u79cd\u72ec\u7acb\u4e8e\u7528\u4e8e\u6570\u636e\u91c7\u96c6\u7684\u4f20\u611f\u5668\uff08\u5982\u76f8\u673a\u3001\u7ed3\u6784\u5149\u7cfb\u7edf\uff08SLS\uff09\u6216\u5730\u9762\u6fc0\u5149\u626b\u63cf\u4eea\uff08TLS\uff09\uff09\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002\u6b64\u65b9\u6cd5\u53ef\u7528\u4e8e\u65b0\u9c9c\u6216\u56fa\u5316\u72b6\u6001\u7684\u6750\u6599\u3002", "result": "\u8be5\u65b9\u6cd5\u53ef\u7528\u4e8e\u5728\u7ebf\u548c\u6253\u5370\u540eQC\u3002", "conclusion": "\u603b\u7ed3\uff1a\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u4e1d\u6750\u8d28\u91cf\u63a7\u5236\u65b9\u6cd5\uff0c\u53ef\u7528\u4e8e\u4e0d\u540c\u7c7b\u578b\u76843D\u6df7\u51dd\u571f\u6253\u5370\u5de5\u827a\uff0c\u5e76\u9002\u7528\u4e8e\u5728\u7ebf\u548c\u79bb\u7ebf\u8d28\u91cf\u68c0\u6d4b\u3002"}}
{"id": "2512.00722", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00722", "abs": "https://arxiv.org/abs/2512.00722", "authors": ["Jiaming Xu", "Jiayi Pan", "Hanzhen Wang", "Yongkang Zhou", "Jiancai Ye", "Yu Wang", "Guohao Dai"], "title": "SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs", "comment": "Accepted by ASPLOS 2026", "summary": "In this paper, we point out that the objective of the retrieval algorithms is to align with the LLM, which is similar to the objective of knowledge distillation in LLMs. We analyze the similarity in information focus between the distilled language model(DLM) and the original LLM from the perspective of information theory, and thus propose a novel paradigm that leverages a DLM as the retrieval algorithm. Based on the insight, we present SpeContext, an algorithm and system co-design for long-context reasoning. (1) At the algorithm level, SpeContext proposes lightweight retrieval head based on the head-level attention weights of DLM, achieving > 90% parameters reduction by pruning the redundancy. (2) At the system level, SpeContext designs an asynchronous prefetch dataflow via the elastic loading strategy, effectively overlapping KV cache retrieval with the LLM computation. (3) At the compilation level, SpeContext constructs the theoretical memory model and implements an adaptive memory management system to achieve acceleration by maximizing GPU memory utilization. We deploy and evaluate SpeContext in two resourceconstrained environments, cloud and edge. Extensive experiments show that, compared with the Huggingface framework, SpeContext achieves up to 24.89x throughput improvement in cloud and 10.06x speedup in edge with negligible accuracy loss, pushing the Pareto frontier of accuracy and throughput.", "AI": {"tldr": "SpeContext\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u8303\u4f8b\uff0c\u5229\u7528\u7cbe\u998f\u8bed\u8a00\u6a21\u578b\uff08DLM\uff09\u4f5c\u4e3a\u68c0\u7d22\u7b97\u6cd5\uff0c\u5e76\u7ed3\u5408\u7b97\u6cd5\u3001\u7cfb\u7edf\u548c\u7f16\u8bd1\u4f18\u5316\uff0c\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u541e\u5410\u91cf\u548c\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u672c\u6587\u6307\u51fa\u68c0\u7d22\u7b97\u6cd5\u7684\u76ee\u6807\u662f\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bf9\u9f50\uff0c\u8fd9\u4e0eLLM\u4e2d\u7684\u77e5\u8bc6\u84b8\u998f\u76ee\u6807\u76f8\u4f3c\u3002\u4ece\u4fe1\u606f\u8bba\u7684\u89d2\u5ea6\u5206\u6790\u4e86\u7cbe\u998f\u8bed\u8a00\u6a21\u578b\uff08DLM\uff09\u548c\u539f\u59cbLLM\u5728\u4fe1\u606f\u7126\u70b9\u4e0a\u7684\u76f8\u4f3c\u6027\u3002", "method": "SpeContext\u5728\u7b97\u6cd5\u5c42\u9762\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDLM\u5934\u7ea7\u6ce8\u610f\u529b\u6743\u91cd\u7684\u8f7b\u91cf\u7ea7\u68c0\u7d22\u5934\uff0c\u901a\u8fc7\u526a\u679d\u5197\u4f59\u5b9e\u73b0\u4e86\u8d85\u8fc790%\u7684\u53c2\u6570\u51cf\u5c11\u3002\u5728\u7cfb\u7edf\u5c42\u9762\uff0cSpeContext\u8bbe\u8ba1\u4e86\u4e00\u79cd\u901a\u8fc7\u5f39\u6027\u52a0\u8f7d\u7b56\u7565\u5b9e\u73b0\u7684\u5f02\u6b65\u9884\u53d6\u6570\u636e\u6d41\uff0c\u6709\u6548\u5730\u5c06KV\u7f13\u5b58\u68c0\u7d22\u4e0eLLM\u8ba1\u7b97\u91cd\u53e0\u3002\u5728\u7f16\u8bd1\u5c42\u9762\uff0cSpeContext\u6784\u5efa\u4e86\u7406\u8bba\u5185\u5b58\u6a21\u578b\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u5185\u5b58\u7ba1\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u6700\u5927\u5316GPU\u5185\u5b58\u5229\u7528\u7387\u6765\u5b9e\u73b0\u52a0\u901f\u3002", "result": "\u5728\u4e91\u7aef\u73af\u5883\u4e2d\uff0cSpeContext\u5b9e\u73b0\u4e86\u9ad8\u8fbe24.89\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0c\u5728\u8fb9\u7f18\u73af\u5883\u4e2d\u5b9e\u73b0\u4e8610.06\u500d\u7684\u52a0\u901f\uff0c\u800c\u7cbe\u5ea6\u635f\u5931\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "conclusion": "SpeContext\u5728\u7cbe\u5ea6\u548c\u541e\u5410\u91cf\u65b9\u9762\u90fd\u4f18\u4e8eHuggingface\u6846\u67b6\uff0c\u63a8\u52a8\u4e86Pareto\u6700\u4f18\u524d\u6cbf\u3002"}}
{"id": "2512.00466", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00466", "abs": "https://arxiv.org/abs/2512.00466", "authors": ["Yang Xiao", "Chunpu Xu", "Ruifeng Yuan", "Jiashuo Wang", "Wenjie Li", "Pengfei Liu"], "title": "SCALE: Selective Resource Allocation for Overcoming Performance Bottlenecks in Mathematical Test-time Scaling", "comment": "accepted by AAAI 2026", "summary": "Test-time compute scaling has emerged as a powerful paradigm for enhancing mathematical reasoning in large language models (LLMs) by allocating additional computational resources during inference. However, current methods employ uniform resource distribution across all reasoning sub-problems, creating fundamental bottlenecks where challenging sub-problems receive insufficient attention while routine operations consume disproportionate resources. This uniform allocation creates performance bottlenecks where additional computational resources yield diminishing returns. Inspired by dual-process theory, we propose \\textbf{SCALE} (Selective Resource Allocation), a framework that selectively allocates computational resources based on sub-problem difficulty. SCALE operates through four stages: (1) problem decomposition into sequential reasoning sub-problems, (2) difficulty assessment of each sub-problem to distinguish between routine operations and computationally challenging sub-problems, (3) selective processing mode assignment between System 1 for simple sub-problems and System 2 for complex ones, and (4) sequential execution with context propagation. By concentrating resources on challenging sub-problems while processing routine operations efficiently, SCALE achieves substantial performance improvements with superior resource utilization. Extensive experiments demonstrate that SCALE significantly outperforms uniform scaling baselines, achieving accuracy improvements of up to 13.75 percentage points (57.50% to 71.25% on AIME25) while reducing computational costs by 33%-53%, representing a major advance in test-time scaling that addresses fundamental limitations of current approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a SCALE \u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6839\u636e\u5b50\u95ee\u9898\u7684\u96be\u5ea6\u9009\u62e9\u6027\u5730\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u65f6\u91c7\u7528\u7edf\u4e00\u7684\u8d44\u6e90\u5206\u914d\u65b9\u5f0f\uff0c\u5bfc\u81f4\u96be\u9898\u5f97\u4e0d\u5230\u8db3\u591f\u7684\u91cd\u89c6\uff0c\u800c\u4f8b\u884c\u64cd\u4f5c\u5374\u6d88\u8017\u4e86\u8fc7\u591a\u7684\u8d44\u6e90\uff0c\u9020\u6210\u4e86\u6027\u80fd\u74f6\u9888\u3002", "method": "\u8be5\u6846\u67b6\u5305\u542b\u56db\u4e2a\u9636\u6bb5\uff1a\u95ee\u9898\u5206\u89e3\u3001\u96be\u5ea6\u8bc4\u4f30\u3001\u9009\u62e9\u6027\u5904\u7406\u6a21\u5f0f\u5206\u914d\u548c\u987a\u5e8f\u6267\u884c\u3002", "result": "SCALE \u663e\u8457\u4f18\u4e8e\u7edf\u4e00\u7f29\u653e\u57fa\u7ebf\uff0c\u5728 AIME25 \u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe 13.75 \u4e2a\u767e\u5206\u70b9\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0857.50% \u63d0\u5347\u81f3 71.25%\uff09\uff0c\u540c\u65f6\u964d\u4f4e\u4e86 33%-53% \u7684\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "SCALE \u901a\u8fc7\u5c06\u8d44\u6e90\u96c6\u4e2d\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u5b50\u95ee\u9898\u4e0a\uff0c\u540c\u65f6\u9ad8\u6548\u5730\u5904\u7406\u4f8b\u884c\u64cd\u4f5c\uff0c\u4ece\u800c\u5728\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\u3002"}}
{"id": "2512.00293", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00293", "abs": "https://arxiv.org/abs/2512.00293", "authors": ["Yafei Lyu", "Hao Zhou", "Lu Zhang", "Xu Yang", "Zhiyong Liu"], "title": "FiCoTS: Fine-to-Coarse LLM-Enhanced Hierarchical Cross-Modality Interaction for Time Series Forecasting", "comment": null, "summary": "Time series forecasting is central to data analysis and web technologies. The recent success of Large Language Models (LLMs) offers significant potential for this field, especially from the cross-modality aspect. Most methods adopt an LLM-as-Predictor paradigm, using LLM as the forecasting backbone and designing modality alignment mechanisms to enable LLM to understand time series data. However, the semantic information in the two modalities of time series and text differs significantly, making it challenging for LLM to fully understand time series data. To mitigate this challenge, our work follows an LLM-as-Enhancer paradigm to fully utilize the advantage of LLM in text understanding, where LLM is only used to encode text modality to complement time series modality. Based on this paradigm, we propose FiCoTS, an LLM-enhanced fine-to-coarse framework for multimodal time series forecasting. Specifically, the framework facilitates progressive cross-modality interaction by three levels in a fine-to-coarse scheme: First, in the token-level modality alignment module, a dynamic heterogeneous graph is constructed to filter noise and align time series patches with text tokens; Second, in the feature-level modality interaction module, a global cross-attention mechanism is introduced to enable each time series variable to connect with relevant textual contexts; Third, in the decision-level modality fusion module, we design a gated network to adaptively fuse the results of the two modalities for robust predictions. These three modules work synergistically to let the two modalities interact comprehensively across three semantic levels, enabling textual information to effectively support temporal prediction. Extensive experiments on seven real-world benchmarks demonstrate that our model achieves state-of-the-art performance. The codes will be released publicly.", "AI": {"tldr": "FiCoTS: \u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u589e\u5f3a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u901a\u8fc7\u7ec6\u5230\u7c97\u7684\u6846\u67b6\u5b9e\u73b0\u8de8\u6a21\u6001\u4ea4\u4e92\uff0c\u5728token\u3001\u7279\u5f81\u548c\u51b3\u7b56\u5c42\u9762\u8fdb\u884c\u6a21\u6001\u5bf9\u9f50\u548c\u878d\u5408\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u76f4\u63a5\u4f7f\u7528LLM\u9884\u6d4b\u65f6\u95f4\u5e8f\u5217\uff0c\u4f46\u65f6\u95f4\u5e8f\u5217\u548c\u6587\u672c\u8bed\u4e49\u5dee\u5f02\u5927\uff0cLLM\u96be\u4ee5\u7406\u89e3\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002", "method": "\u63d0\u51faFiCoTS\u6846\u67b6\uff0c\u91c7\u7528LLM-as-Enhancer\u8303\u5f0f\uff0c\u4ec5\u7528LLM\u7f16\u7801\u6587\u672c\uff0c\u901a\u8fc7token\u7ea7\u5bf9\u9f50\u3001\u7279\u5f81\u7ea7\u4ea4\u4e92\u548c\u51b3\u7b56\u7ea7\u878d\u5408\uff0c\u5b9e\u73b0\u7ec6\u5230\u7c97\u7684\u8de8\u6a21\u6001\u4ea4\u4e92\u3002", "result": "\u5728\u4e03\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86state-of-the-art\u7684\u6548\u679c\u3002", "conclusion": "FiCoTS\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86LLM\u5728\u589e\u5f3a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.00103", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00103", "abs": "https://arxiv.org/abs/2512.00103", "authors": ["Ifeanyi Okala"], "title": "Comparative Analysis of Vision Transformer, Convolutional, and Hybrid Architectures for Mental Health Classification Using Actigraphy-Derived Images", "comment": null, "summary": "This work examines how three different image-based methods, VGG16, ViT-B/16, and CoAtNet-Tiny, perform in identifying depression, schizophrenia, and healthy controls using daily actigraphy records. Wrist-worn activity signals from the Psykose and Depresjon datasets were converted into 30 by 48 images and evaluated through a three-fold subject-wise split. Although all methods fitted the training data well, their behaviour on unseen data differed. VGG16 improved steadily but often settled at lower accuracy. ViT-B/16 reached strong results in some runs, but its performance shifted noticeably from fold to fold. CoAtNet-Tiny stood out as the most reliable, recording the highest average accuracy and the most stable curves across folds. It also produced the strongest precision, recall, and F1-scores, particularly for the underrepresented depression and schizophrenia classes. Overall, the findings indicate that CoAtNet-Tiny performed most consistently on the actigraphy images, while VGG16 and ViT-B/16 yielded mixed results. These observations suggest that certain hybrid designs may be especially suited for mental-health work that relies on actigraphy-derived images.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4e09\u79cd\u57fa\u4e8e\u56fe\u50cf\u7684\u65b9\u6cd5\uff08VGG16\u3001ViT-B/16 \u548c CoAtNet-Tiny\uff09\u5728\u4f7f\u7528\u6bcf\u65e5\u6d3b\u52a8\u8bb0\u5f55\u8bc6\u522b\u6291\u90c1\u75c7\u3001\u7cbe\u795e\u5206\u88c2\u75c7\u548c\u5065\u5eb7\u5bf9\u7167\u65b9\u9762\u7684\u8868\u73b0\u3002", "motivation": "\u5229\u7528\u8155\u6234\u5f0f\u6d3b\u52a8\u4fe1\u53f7\u8bc6\u522b\u7cbe\u795e\u75be\u75c5\uff0c\u4e3a\u5fc3\u7406\u5065\u5eb7\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u9014\u5f84\u3002", "method": "\u5c06Psykose\u548cDepresjon\u6570\u636e\u96c6\u7684\u8155\u6234\u5f0f\u6d3b\u52a8\u4fe1\u53f7\u8f6c\u6362\u4e3a30x48\u7684\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7\u4e09\u6298\u53d7\u8bd5\u8005\u5206\u5272\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "CoAtNet-Tiny \u8868\u73b0\u6700\u4e3a\u53ef\u9760\uff0c\u8bb0\u5f55\u4e86\u6700\u9ad8\u7684\u5e73\u5747\u51c6\u786e\u7387\u548c\u6700\u7a33\u5b9a\u7684\u66f2\u7ebf\u3002VGG16 \u7a33\u6b65\u63d0\u9ad8\u4f46\u901a\u5e38\u7a33\u5b9a\u5728\u8f83\u4f4e\u7684\u51c6\u786e\u7387\u3002ViT-B/16 \u5728\u67d0\u4e9b\u8fd0\u884c\u4e2d\u8fbe\u5230\u4e86\u5f88\u597d\u7684\u6548\u679c\uff0c\u4f46\u5176\u6027\u80fd\u5728\u4e0d\u540c\u7684 fold \u4e2d\u53d1\u751f\u4e86\u660e\u663e\u7684\u53d8\u5316\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cCoAtNet-Tiny \u5728 actigraphy \u56fe\u50cf\u4e0a\u8868\u73b0\u6700\u4e3a\u7a33\u5b9a\uff0c\u800c VGG16 \u548c ViT-B/16 \u7684\u7ed3\u679c\u597d\u574f\u53c2\u534a\u3002"}}
{"id": "2512.00729", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00729", "abs": "https://arxiv.org/abs/2512.00729", "authors": ["Yuxiang Chen", "Zuohan Wu", "Ziwei Wang", "Xiangning Yu", "Xujia Li", "Linyi Yang", "Mengyue Yang", "Jun Wang", "Lei Chen"], "title": "Probing the \"Psyche'' of Large Reasoning Models: Understanding Through a Human Lens", "comment": "13 pages", "summary": "Large reasoning models (LRMs) have garnered significant attention from researchers owing to their exceptional capability in addressing complex tasks. Motivated by the observed human-like behaviors in their reasoning processes, this paper introduces a comprehensive taxonomy to characterize atomic reasoning steps and probe the ``psyche'' of LRM intelligence. Specifically, it comprises five groups and seventeen categories derived from human mental processes, thereby grounding the understanding of LRMs in an interdisciplinary perspective. The taxonomy is then applied for an in-depth understanding of current LRMs, resulting in a distinct labeled dataset that comprises 277,534 atomic reasoning steps. Using this resource, we analyze contemporary LRMs and distill several actionable takeaways for improving training and post-training of reasoning models. Notably, our analysis reveals that prevailing post-answer ``double-checks'' (self-monitoring evaluations) are largely superficial and rarely yield substantive revisions. Thus, incentivizing comprehensive multi-step reflection, rather than simple self-monitoring, may offer a more effective path forward. To complement the taxonomy, an automatic annotation framework, named CAPO, is proposed to leverage large language models (LLMs) for generating the taxonomy-based annotations. Experimental results demonstrate that CAPO achieves higher consistency with human experts compared to baselines, facilitating a scalable and comprehensive analysis of LRMs from a human cognitive perspective. Together, the taxonomy, CAPO, and the derived insights provide a principled, scalable path toward understanding and advancing LRM reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u63cf\u8ff0\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRM\uff09\u63a8\u7406\u6b65\u9aa4\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u7528\u5b83\u6765\u6df1\u5165\u4e86\u89e3\u5f53\u524d\u7684LRM\uff0c\u4ece\u800c\u4e3a\u6539\u8fdb\u63a8\u7406\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u8bad\u7ec3\u540e\u5904\u7406\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u5efa\u8bae\u3002", "motivation": "\u7814\u7a76\u8868\u660e\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRM\uff09\u5728\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u89c2\u5bdf\u5230\u7c7b\u4f3c\u4eba\u7c7b\u7684\u884c\u4e3a\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4e00\u4e2a\u5168\u9762\u7684\u5206\u7c7b\u6cd5\u6765\u63cf\u8ff0\u539f\u5b50\u63a8\u7406\u6b65\u9aa4\uff0c\u4ece\u800c\u63a2\u7a76LRM\u667a\u80fd\u7684\u201c\u5fc3\u7406\u201d\u3002", "method": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u4e94\u4e2a\u7ec4\u548c\u5341\u4e03\u4e2a\u7c7b\u522b\u7684\u5206\u7c7b\u6cd5\uff0c\u8fd9\u4e9b\u7c7b\u522b\u6765\u6e90\u4e8e\u4eba\u7c7b\u5fc3\u7406\u8fc7\u7a0b\u3002\u7136\u540e\uff0c\u5e94\u7528\u8be5\u5206\u7c7b\u6cd5\u6765\u6df1\u5165\u7406\u89e3\u5f53\u524d\u7684LRM\uff0c\u4ece\u800c\u751f\u6210\u4e00\u4e2a\u5305\u542b277,534\u4e2a\u539f\u5b50\u63a8\u7406\u6b65\u9aa4\u7684\u5e26\u6807\u7b7e\u6570\u636e\u96c6\u3002\u6b64\u5916\uff0c\u672c\u6587\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCAPO\u7684\u81ea\u52a8\u6ce8\u91ca\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6765\u751f\u6210\u57fa\u4e8e\u5206\u7c7b\u6cd5\u7684\u6ce8\u91ca\u3002", "result": "\u672c\u6587\u7684\u5206\u6790\u8868\u660e\uff0c\u76ee\u524d\u6d41\u884c\u7684\u7b54\u6848\u540e\u201c\u4e8c\u6b21\u68c0\u67e5\u201d\uff08\u81ea\u6211\u76d1\u63a7\u8bc4\u4f30\uff09\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u662f\u80a4\u6d45\u7684\uff0c\u5f88\u5c11\u4ea7\u751f\u5b9e\u8d28\u6027\u7684\u4fee\u6539\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCAPO\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u5206\u7c7b\u6cd5\u3001CAPO\u548c\u7531\u6b64\u4ea7\u751f\u7684\u89c1\u89e3\u4e3a\u7406\u89e3\u548c\u63a8\u8fdbLRM\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u6761\u539f\u5219\u6027\u7684\u3001\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002"}}
{"id": "2512.00496", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00496", "abs": "https://arxiv.org/abs/2512.00496", "authors": ["Diego A. B. Moreira", "Alef I. Ferreira", "Jhessica Silva", "Gabriel O. dos Santos", "Gustavo Bonil", "Jo\u00e3o Gondim", "Marina dos Santos", "Helena Maia", "Simone Hashiguti", "N\u00e1dia da Silva", "Carolina Scarton", "Helio Pedrini", "Sandra Avila"], "title": "CACARA: Cross-Modal Alignment Leveraging a Text-Centric Approach for Cost-Effective Multimodal and Multilingual Learning", "comment": "25 pages, 12 tables, 5 figures", "summary": "As deep learning models evolve, new applications and challenges are rapidly emerging. Tasks that once relied on a single modality, such as text, images, or audio, are now enriched by seamless interactions between multimodal data. These connections bridge information gaps: an image can visually materialize a text, while audio can add context to an image. Researchers have developed numerous multimodal models, but most rely on resource-intensive training across multiple modalities. Similarly, extending these models to new languages often follows the same resource-heavy training strategy. In this work, we propose a multimodal and multilingual architecture, CACARA, trained through emergent alignment learning, enabling the seamless integration of new modalities into an existing bimodal/multimodal model without requiring full retraining. This work breaks new ground by demonstrating that this emergent alignment paradigm can unlock multilingual capabilities from monolingual training. By fine-tuning the newly incorporated modality only on data aligned with the English language, our model develops support for over 100 languages without explicit multilingual pretraining or tuning of the text encoder. Such emergent multimodal and multilingual properties are gained efficiently, preserving previously learned knowledge at a training cost comparable to that of a monolingual model. Our strategy achieves up to a 14.24 percentage points improvement in R@1 audio-to-text retrieval, outperforming state-of-the-art multimodal models -- all without the heavy computational cost of retraining across every modality and language.", "AI": {"tldr": "CACARA\u6a21\u578b\u901a\u8fc7\u65b0\u5174\u5bf9\u9f50\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u548c\u591a\u8bed\u8a00\u7684\u6269\u5c55\uff0c\u65e0\u9700\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\uff0c\u5e76\u4e14\u8ba1\u7b97\u6210\u672c\u8f83\u4f4e\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\uff0c\u4e14\u6269\u5c55\u5230\u65b0\u8bed\u8a00\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u8d44\u6e90\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u548c\u591a\u8bed\u8a00\u67b6\u6784CACARA\uff0c\u901a\u8fc7\u65b0\u5174\u5bf9\u9f50\u5b66\u4e60\u8fdb\u884c\u8bad\u7ec3\uff0c\u5141\u8bb8\u5c06\u65b0\u6a21\u6001\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u7684\u53cc\u6a21\u6001/\u591a\u6a21\u6001\u6a21\u578b\u4e2d\uff0c\u800c\u65e0\u9700\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "\u5728\u97f3\u9891\u5230\u6587\u672c\u68c0\u7d22\u65b9\u9762\uff0cR@1\u6307\u6807\u63d0\u9ad8\u4e8614.24\u4e2a\u767e\u5206\u70b9\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u6a21\u578b\u3002", "conclusion": "\u8be5\u7b56\u7565\u80fd\u591f\u9ad8\u6548\u5730\u83b7\u5f97\u65b0\u5174\u7684\u591a\u6a21\u6001\u548c\u591a\u8bed\u8a00\u5c5e\u6027\uff0c\u4ee5\u4e0e\u5355\u8bed\u6a21\u578b\u76f8\u5f53\u7684\u8bad\u7ec3\u6210\u672c\u4fdd\u7559\u5148\u524d\u5b66\u4e60\u7684\u77e5\u8bc6\u3002"}}
{"id": "2512.00298", "categories": ["cs.LG", "cs.CL", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.00298", "abs": "https://arxiv.org/abs/2512.00298", "authors": ["Gonz\u00e1lez Trigueros Jes\u00fas Eduardo", "Alonso S\u00e1nchez Alejandro", "Mu\u00f1oz Rivera Emilio", "Pe\u00f1ar\u00e1n Prieto Mariana Jaqueline", "Mendoza Gonz\u00e1lez Camila Natalia"], "title": "Challenges of Heterogeneity in Big Data: A Comparative Study of Classification in Large-Scale Structured and Unstructured Domains", "comment": "13 pages, 1 figure, 3 tables. Comparative study involving Apache Spark and Hyperparameter Optimization. Keywords: Big Data, NLP, Tabular Data", "summary": "This study analyzes the impact of heterogeneity (\"Variety\") in Big Data by comparing classification strategies across structured (Epsilon) and unstructured (Rest-Mex, IMDB) domains. A dual methodology was implemented: evolutionary and Bayesian hyperparameter optimization (Genetic Algorithms, Optuna) in Python for numerical data, and distributed processing in Apache Spark for massive textual corpora. The results reveal a \"complexity paradox\": in high-dimensional spaces, optimized linear models (SVM, Logistic Regression) outperformed deep architectures and Gradient Boosting. Conversely, in text-based domains, the constraints of distributed fine-tuning led to overfitting in complex models, whereas robust feature engineering -- specifically Transformer-based embeddings (ROBERTa) and Bayesian Target Encoding -- enabled simpler models to generalize effectively. This work provides a unified framework for algorithm selection based on data nature and infrastructure constraints.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u5927\u6570\u636e\u4e2d\u5f02\u6784\u6027\uff08\u201c\u591a\u6837\u6027\u201d\uff09\u7684\u5f71\u54cd\uff0c\u6bd4\u8f83\u4e86\u7ed3\u6784\u5316\uff08Epsilon\uff09\u548c\u975e\u7ed3\u6784\u5316\uff08Rest-Mex\uff0cIMDB\uff09\u9886\u57df\u4e2d\u7684\u5206\u7c7b\u7b56\u7565\u3002", "motivation": "\u65e8\u5728\u7814\u7a76\u4e0d\u540c\u6570\u636e\u7c7b\u578b\u548c\u57fa\u7840\u8bbe\u65bd\u7ea6\u675f\u4e0b\uff0c\u7b97\u6cd5\u9009\u62e9\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e86\u53cc\u91cd\u65b9\u6cd5\uff1a\u5728Python\u4e2d\u5bf9\u6570\u503c\u6570\u636e\u8fdb\u884c\u8fdb\u5316\u548c\u8d1d\u53f6\u65af\u8d85\u53c2\u6570\u4f18\u5316\uff08\u9057\u4f20\u7b97\u6cd5\uff0cOptuna\uff09\uff0c\u4ee5\u53ca\u5728Apache Spark\u4e2d\u5bf9\u5927\u91cf\u6587\u672c\u8bed\u6599\u5e93\u8fdb\u884c\u5206\u5e03\u5f0f\u5904\u7406\u3002", "result": "\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\uff0c\u4f18\u5316\u7684\u7ebf\u6027\u6a21\u578b\uff08SVM\uff0c\u903b\u8f91\u56de\u5f52\uff09\u4f18\u4e8e\u6df1\u5ea6\u67b6\u6784\u548c\u68af\u5ea6\u63d0\u5347\u3002\u5728\u57fa\u4e8e\u6587\u672c\u7684\u9886\u57df\u4e2d\uff0c\u5206\u5e03\u5f0f\u5fae\u8c03\u7684\u7ea6\u675f\u5bfc\u81f4\u590d\u6742\u6a21\u578b\u4e2d\u7684\u8fc7\u62df\u5408\uff0c\u800c\u5f3a\u5927\u7684\u7279\u5f81\u5de5\u7a0b\uff08\u7279\u522b\u662f\u57fa\u4e8eTransformer\u7684\u5d4c\u5165\uff08ROBERTa\uff09\u548c\u8d1d\u53f6\u65af\u76ee\u6807\u7f16\u7801\uff09\u4f7f\u66f4\u7b80\u5355\u7684\u6a21\u578b\u80fd\u591f\u6709\u6548\u5730\u63a8\u5e7f\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u57fa\u4e8e\u6570\u636e\u6027\u8d28\u548c\u57fa\u7840\u8bbe\u65bd\u7ea6\u675f\u7684\u7b97\u6cd5\u9009\u62e9\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u3002"}}
{"id": "2512.00117", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.00117", "abs": "https://arxiv.org/abs/2512.00117", "authors": ["Ishwaryah Pandiarajan", "Mohamed Mansoor Roomi Sindha", "Uma Maheswari Pandyan", "Sharafia N"], "title": "TinyViT: Field Deployable Transformer Pipeline for Solar Panel Surface Fault and Severity Screening", "comment": "3pages, 2figures,ICGVIP 2025", "summary": "Sustained operation of solar photovoltaic assets hinges on accurate detection and prioritization of surface faults across vast, geographically distributed modules. While multi modal imaging strategies are popular, they introduce logistical and economic barriers for routine farm level deployment. This work demonstrates that deep learning and classical machine learning may be judiciously combined to achieve robust surface anomaly categorization and severity estimation from planar visible band imagery alone. We introduce TinyViT which is a compact pipeline integrating Transformer based segmentation, spectral-spatial feature engineering, and ensemble regression. The system ingests consumer grade color camera mosaics of PV panels, classifies seven nuanced surface faults, and generates actionable severity grades for maintenance triage. By eliminating reliance on electroluminescence or IR sensors, our method enables affordable, scalable upkeep for resource limited installations, and advances the state of solar health monitoring toward universal field accessibility. Experiments on real public world datasets validate both classification and regression sub modules, achieving accuracy and interpretability competitive with specialized approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f7f\u7528\u53ef\u89c1\u5149\u56fe\u50cf\u8fdb\u884c\u592a\u9633\u80fd\u5149\u4f0f\u677f\u8868\u9762\u7f3a\u9677\u68c0\u6d4b\u548c\u4f18\u5148\u7ea7\u6392\u5e8f\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u4f9d\u8d56\u5176\u4ed6\u4f20\u611f\u5668\u3002", "motivation": "\u4f9d\u8d56\u591a\u6a21\u6001\u6210\u50cf\u7b56\u7565\u8fdb\u884c\u592a\u9633\u80fd\u5149\u4f0f\u677f\u6545\u969c\u68c0\u6d4b\u5b58\u5728\u540e\u52e4\u548c\u7ecf\u6d4e\u969c\u788d\uff0c\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u90e8\u7f72\u65f6\u3002", "method": "\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a TinyViT \u7684\u7d27\u51d1 pipeline\uff0c\u5305\u62ec\u57fa\u4e8e Transformer \u7684\u5206\u5272\u3001\u5149\u8c31-\u7a7a\u95f4\u7279\u5f81\u5de5\u7a0b\u548c\u96c6\u6210\u56de\u5f52\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5206\u7c7b\u548c\u56de\u5f52\u5b50\u6a21\u5757\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u4e0e\u4e13\u4e1a\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u964d\u4f4e\u4e86\u5bf9\u7535\u81f4\u53d1\u5149\u6216\u7ea2\u5916\u4f20\u611f\u5668\u7684\u4f9d\u8d56\uff0c\u5b9e\u73b0\u4e86\u7ecf\u6d4e\u53ef\u6269\u5c55\u7684\u7ef4\u62a4\uff0c\u5e76\u63a8\u52a8\u4e86\u592a\u9633\u80fd\u5065\u5eb7\u76d1\u6d4b\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.00756", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00756", "abs": "https://arxiv.org/abs/2512.00756", "authors": ["Ruihan Chen", "Qiming Li", "Xiaocheng Feng", "Xiaoliang Yang", "Weihong Zhong", "Yuxuan Gu", "Zekun Zhou", "Bing Qin"], "title": "MPR-GUI: Benchmarking and Enhancing Multilingual Perception and Reasoning in GUI Agents", "comment": "27pages, 12figures", "summary": "With the advancement of computational resources, Large Vision-Language Models (LVLMs) exhibit impressive Perception and Reasoning (P&R) performance on Graphical User Interface (GUI) tasks. However, although they demonstrate strong P&R capabilities in English GUI scenarios, their performance in multilingual settings has received little attention, which limits their global applications. Moreover, existing studies on GUI tasks lack fine-grained analyses, including widget functions and elements' spatial relationships, which are fundamental for more targeted improvements. To tackle these issues, we propose MPR-GUI-Bench, a Multilingual fine-grained Perception and Reasoning GUI Benchmark to evaluate GUI agents' P&R capabilities. Evaluation results demonstrate that LVLMs exhibit significantly worse P&R performance in non-English languages than in English. To address these gaps, we propose GUI-XLI, a GUI Cross-Lingual Intervention method that applies interventions to the hidden states at P&R capability-related layers to mitigate the gaps between English and other languages, building on previous research showing that the hidden states of different language inputs exhibit significant differences in the latent space. Experimental results indicate that our method improves GUI agents' multilingual P&R capability by 6.5% on average.", "AI": {"tldr": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(LVLMs)\u5728\u56fe\u5f62\u7528\u6237\u754c\u9762(GUI)\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u611f\u77e5\u548c\u63a8\u7406(P&R)\u6027\u80fd\u3002\u4f46\u662f\uff0c\u5b83\u4eec\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u6027\u80fd\u53d7\u5230\u7684\u5173\u6ce8\u8f83\u5c11\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u7684\u5168\u7403\u5e94\u7528\u3002\u6b64\u5916\uff0c\u73b0\u6709\u7684GUI\u4efb\u52a1\u7814\u7a76\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u7684\u5206\u6790\uff0c\u5305\u62ec\u5c0f\u90e8\u4ef6\u529f\u80fd\u548c\u5143\u7d20\u7a7a\u95f4\u5173\u7cfb\uff0c\u8fd9\u5bf9\u4e8e\u66f4\u6709\u9488\u5bf9\u6027\u7684\u6539\u8fdb\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u8bc4\u4f30GUI agents\u7684P&R\u80fd\u529b", "method": "MPR-GUI-Bench\uff0c\u4e00\u4e2a\u591a\u8bed\u8a00\u7ec6\u7c92\u5ea6\u611f\u77e5\u548c\u63a8\u7406GUI\u57fa\u51c6", "result": "LVLMs\u5728\u975e\u82f1\u8bed\u8bed\u8a00\u4e2d\u7684P&R\u6027\u80fd\u660e\u663e\u4f4e\u4e8e\u82f1\u8bed\u3002GUI-XLI\uff0c\u4e00\u79cdGUI\u8de8\u8bed\u8a00\u5e72\u9884\u65b9\u6cd5\uff0c\u53ef\u5c06\u5e72\u9884\u5e94\u7528\u4e8e\u4e0eP&R\u80fd\u529b\u76f8\u5173\u7684\u5c42\u4e2d\u7684\u9690\u85cf\u72b6\u6001\uff0c\u4ee5\u51cf\u8f7b\u82f1\u8bed\u548c\u5176\u4ed6\u8bed\u8a00\u4e4b\u95f4\u7684\u5dee\u8ddd", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u5c06GUI\u4ee3\u7406\u7684\u591a\u8bed\u8a00P&R\u80fd\u529b\u5e73\u5747\u63d0\u9ad8\u4e866.5%\u3002"}}
{"id": "2512.00504", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00504", "abs": "https://arxiv.org/abs/2512.00504", "authors": ["Mengqi Liao", "Lu Wang", "Chaoyun Zhang", "Zekai Shen", "Xiaowei Mao", "Si Qin", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang", "Huaiyu Wan"], "title": "G-KV: Decoding-Time KV Cache Eviction with Global Attention", "comment": null, "summary": "Recent reasoning large language models (LLMs) excel in complex tasks but encounter significant computational and memory challenges due to long sequence lengths. KV cache compression has emerged as an effective approach to greatly enhance the efficiency of reasoning. However, existing methods often focus on prompt compression or token eviction with local attention score, overlooking the long-term importance of tokens. We propose G-KV, a KV cache eviction method that employs a global scoring mechanism, combining local and historical attention scores to more accurately assess token importance. Additionally, we introduce post-training techniques, including reinforcement learning and distillation, to optimize models for compressed KV cache settings. The code of this paper is available on: https://github.com/microsoft/G-KV.", "AI": {"tldr": "G-KV \u901a\u8fc7\u7ed3\u5408\u5c40\u90e8\u548c\u5386\u53f2\u6ce8\u610f\u529b\u5206\u6570\u6765\u8bc4\u4f30token\u7684\u91cd\u8981\u6027\uff0c\u4ece\u800c\u66f4\u51c6\u786e\u5730\u8fdb\u884c KV \u7f13\u5b58\u6dd8\u6c70\uff0c\u5e76\u4f7f\u7528\u540e\u8bad\u7ec3\u6280\u672f\u4f18\u5316\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4fa7\u91cd\u4e8e\u63d0\u793a\u538b\u7f29\u6216\u5177\u6709\u5c40\u90e8\u6ce8\u610f\u529b\u5206\u6570\u7684token\u6dd8\u6c70\uff0c\u5ffd\u7565\u4e86token\u7684\u957f\u671f\u91cd\u8981\u6027\u3002\u56e0\u6b64\uff0c\u63a8\u7406\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7531\u4e8e\u5e8f\u5217\u957f\u5ea6\u8fc7\u957f\uff0c\u9762\u4e34\u7740\u5de8\u5927\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6311\u6218\u3002", "method": "G-KV \u662f\u4e00\u79cd KV \u7f13\u5b58\u6dd8\u6c70\u65b9\u6cd5\uff0c\u5b83\u91c7\u7528\u5168\u5c40\u8bc4\u5206\u673a\u5236\uff0c\u7ed3\u5408\u5c40\u90e8\u548c\u5386\u53f2\u6ce8\u610f\u529b\u5206\u6570\u6765\u66f4\u51c6\u786e\u5730\u8bc4\u4f30token\u7684\u91cd\u8981\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u540e\u8bad\u7ec3\u6280\u672f\uff0c\u5305\u62ec\u5f3a\u5316\u5b66\u4e60\u548c\u84b8\u998f\uff0c\u4ee5\u4f18\u5316\u538b\u7f29 KV \u7f13\u5b58\u8bbe\u7f6e\u7684\u6a21\u578b\u3002", "result": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86 G-KV\u3002", "conclusion": "G-KV \u662f\u4e00\u79cd\u7528\u4e8e\u538b\u7f29 KV \u7f13\u5b58\u8bbe\u7f6e\u7684 KV \u7f13\u5b58\u6dd8\u6c70\u65b9\u6cd5\uff0c\u5b83\u91c7\u7528\u5168\u5c40\u8bc4\u5206\u673a\u5236\uff0c\u7ed3\u5408\u5c40\u90e8\u548c\u5386\u53f2\u6ce8\u610f\u529b\u5206\u6570\u6765\u66f4\u51c6\u786e\u5730\u8bc4\u4f30token\u7684\u91cd\u8981\u6027, \u5e76\u4f7f\u7528\u540e\u8bad\u7ec3\u6280\u672f\u4f18\u5316\u6a21\u578b\u3002"}}
{"id": "2512.00303", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00303", "abs": "https://arxiv.org/abs/2512.00303", "authors": ["Shenghong He"], "title": "Gradient Inversion in Federated Reinforcement Learning", "comment": null, "summary": "Federated reinforcement learning (FRL) enables distributed learning of optimal policies while preserving local data privacy through gradient sharing.However, FRL faces the risk of data privacy leaks, where attackers exploit shared gradients to reconstruct local training data.Compared to traditional supervised federated learning, successful reconstruction in FRL requires the generated data not only to match the shared gradients but also to align with real transition dynamics of the environment (i.e., aligning with the real data transition distribution).To address this issue, we propose a novel attack method called Regularization Gradient Inversion Attack (RGIA), which enforces prior-knowledge-based regularization on states, rewards, and transition dynamics during the optimization process to ensure that the reconstructed data remain close to the true transition distribution.Theoretically, we prove that the prior-knowledge-based regularization term narrows the solution space from a broad set containing spurious solutions to a constrained subset that satisfies both gradient matching and true transition dynamics.Extensive experiments on control tasks and autonomous driving tasks demonstrate that RGIA can effectively constrain reconstructed data transition distributions and thus successfully reconstruct local private data.", "AI": {"tldr": "\u8054\u90a6\u5f3a\u5316\u5b66\u4e60(FRL)\u901a\u8fc7\u68af\u5ea6\u5171\u4eab\u5b9e\u73b0\u5206\u5e03\u5f0f\u5b66\u4e60\u6700\u4f18\u7b56\u7565\uff0c\u540c\u65f6\u4fdd\u62a4\u672c\u5730\u6570\u636e\u9690\u79c1\u3002\u7136\u800c\uff0cFRL\u9762\u4e34\u6570\u636e\u9690\u79c1\u6cc4\u9732\u7684\u98ce\u9669\uff0c\u653b\u51fb\u8005\u5229\u7528\u5171\u4eab\u68af\u5ea6\u6765\u91cd\u5efa\u672c\u5730\u8bad\u7ec3\u6570\u636e\u3002", "motivation": "\u4e0e\u4f20\u7edf\u7684\u76d1\u7763\u8054\u90a6\u5b66\u4e60\u76f8\u6bd4\uff0cFRL\u4e2d\u7684\u6210\u529f\u91cd\u5efa\u4e0d\u4ec5\u9700\u8981\u751f\u6210\u7684\u6570\u636e\u4e0e\u5171\u4eab\u68af\u5ea6\u5339\u914d\uff0c\u8fd8\u9700\u8981\u4e0e\u73af\u5883\u7684\u771f\u5b9e\u8f6c\u79fb\u52a8\u6001\u5bf9\u9f50(\u5373\u4e0e\u771f\u5b9e\u6570\u636e\u8f6c\u79fb\u5206\u5e03\u5bf9\u9f50)\u3002", "method": "\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u79f0\u4e3a\u6b63\u5219\u5316\u68af\u5ea6\u53cd\u6f14\u653b\u51fb(RGIA)\uff0c\u5b83\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u5bf9\u72b6\u6001\u3001\u5956\u52b1\u548c\u8f6c\u79fb\u52a8\u6001\u5f3a\u5236\u6267\u884c\u57fa\u4e8e\u5148\u9a8c\u77e5\u8bc6\u7684\u6b63\u5219\u5316\uff0c\u4ee5\u786e\u4fdd\u91cd\u5efa\u7684\u6570\u636e\u4fdd\u6301\u63a5\u8fd1\u771f\u5b9e\u7684\u8f6c\u79fb\u5206\u5e03\u3002", "result": "\u5728\u63a7\u5236\u4efb\u52a1\u548c\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cRGIA\u53ef\u4ee5\u6709\u6548\u5730\u7ea6\u675f\u91cd\u5efa\u7684\u6570\u636e\u8f6c\u6362\u5206\u5e03\uff0c\u4ece\u800c\u6210\u529f\u5730\u91cd\u5efa\u672c\u5730\u79c1\u6709\u6570\u636e\u3002", "conclusion": "\u4ece\u7406\u8bba\u4e0a\u8bb2\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u57fa\u4e8e\u5148\u9a8c\u77e5\u8bc6\u7684\u6b63\u5219\u5316\u9879\u5c06\u89e3\u7a7a\u95f4\u4ece\u5305\u542b\u865a\u5047\u89e3\u7684\u5e7f\u6cdb\u96c6\u5408\u7f29\u5c0f\u5230\u6ee1\u8db3\u68af\u5ea6\u5339\u914d\u548c\u771f\u5b9e\u8f6c\u79fb\u52a8\u6001\u7684\u7ea6\u675f\u5b50\u96c6\u3002"}}
{"id": "2512.00125", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00125", "abs": "https://arxiv.org/abs/2512.00125", "authors": ["Ruo-Syuan Mei", "Sixian Jia", "Guangze Li", "Soo Yeon Lee", "Brian Musser", "William Keller", "Sreten Zakula", "Jorge Arinez", "Chenhui Shao"], "title": "Hybrid Synthetic Data Generation with Domain Randomization Enables Zero-Shot Vision-Based Part Inspection Under Extreme Class Imbalance", "comment": "Submitted to the NAMRC 54", "summary": "Machine learning, particularly deep learning, is transforming industrial quality inspection. Yet, training robust machine learning models typically requires large volumes of high-quality labeled data, which are expensive, time-consuming, and labor-intensive to obtain in manufacturing. Moreover, defective samples are intrinsically rare, leading to severe class imbalance that degrades model performance. These data constraints hinder the widespread adoption of machine learning-based quality inspection methods in real production environments. Synthetic data generation (SDG) offers a promising solution by enabling the creation of large, balanced, and fully annotated datasets in an efficient, cost-effective, and scalable manner. This paper presents a hybrid SDG framework that integrates simulation-based rendering, domain randomization, and real background compositing to enable zero-shot learning for computer vision-based industrial part inspection without manual annotation. The SDG pipeline generates 12,960 labeled images in one hour by varying part geometry, lighting, and surface properties, and then compositing synthetic parts onto real image backgrounds. A two-stage architecture utilizing a YOLOv8n backbone for object detection and MobileNetV3-small for quality classification is trained exclusively on synthetic data and evaluated on 300 real industrial parts. The proposed approach achieves an mAP@0.5 of 0.995 for detection, 96% classification accuracy, and 90.1% balanced accuracy. Comparative evaluation against few-shot real-data baseline approaches demonstrates significant improvement. The proposed SDG-based approach achieves 90-91% balanced accuracy under severe class imbalance, while the baselines reach only 50% accuracy. These results demonstrate that the proposed method enables annotation-free, scalable, and robust quality inspection for real-world manufacturing applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u5408\u6210\u6570\u636e\u751f\u6210 (SDG) \u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6ca1\u6709\u624b\u52a8\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u5de5\u4e1a\u96f6\u4ef6\u8fdb\u884c\u96f6\u6837\u672c\u5b66\u4e60\u3002", "motivation": "\u5728\u5236\u9020\u4e1a\u4e2d\uff0c\u8bad\u7ec3\u7a33\u5065\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u901a\u5e38\u9700\u8981\u5927\u91cf\u9ad8\u8d28\u91cf\u7684\u6807\u6ce8\u6570\u636e\uff0c\u800c\u8fd9\u4e9b\u6570\u636e\u7684\u83b7\u53d6\u6210\u672c\u9ad8\u6602\u3001\u8017\u65f6\u4e14\u52b3\u52a8\u5bc6\u96c6\u3002\u6b64\u5916\uff0c\u7f3a\u9677\u6837\u672c\u672c\u8d28\u4e0a\u662f\u7a00\u6709\u7684\uff0c\u5bfc\u81f4\u4e25\u91cd\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u4ece\u800c\u964d\u4f4e\u6a21\u578b\u6027\u80fd\u3002\u8fd9\u4e9b\u6570\u636e\u7ea6\u675f\u963b\u788d\u4e86\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u8d28\u91cf\u68c0\u6d4b\u65b9\u6cd5\u5728\u5b9e\u9645\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u8be5\u65b9\u6cd5\u96c6\u6210\u4e86\u57fa\u4e8e\u4eff\u771f\u7684\u6e32\u67d3\u3001\u57df\u968f\u673a\u5316\u548c\u771f\u5b9e\u80cc\u666f\u5408\u6210\uff0c\u4ee5\u5b9e\u73b0\u96f6\u6837\u672c\u5b66\u4e60\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4ec5\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u5728 300 \u4e2a\u771f\u5b9e\u5de5\u4e1a\u96f6\u4ef6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5b9e\u73b0\u4e86 0.995 \u7684 mAP@0.5\uff08\u76ee\u6807\u68c0\u6d4b\uff09\uff0c96% \u7684\u5206\u7c7b\u51c6\u786e\u7387\u548c 90.1% \u7684\u5e73\u8861\u51c6\u786e\u7387\u3002\u5728\u4e25\u91cd\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u4e0b\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86 90-91% \u7684\u5e73\u8861\u51c6\u786e\u7387\uff0c\u800c\u57fa\u7ebf\u65b9\u6cd5\u4ec5\u8fbe\u5230 50% \u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u65e0\u6807\u6ce8\u3001\u53ef\u6269\u5c55\u4e14\u7a33\u5065\u7684\u8d28\u91cf\u68c0\u6d4b\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u7684\u5236\u9020\u5e94\u7528\u3002"}}
{"id": "2512.00807", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00807", "abs": "https://arxiv.org/abs/2512.00807", "authors": ["Yujie Lin", "Jiayao Ma", "Qingguo Hu", "Derek F. Wong", "Jinsong Su"], "title": "BioPro: On Difference-Aware Gender Fairness for Vision-Language Models", "comment": null, "summary": "Vision-Language Models (VLMs) inherit significant social biases from their training data, notably in gender representation. Current fairness interventions often adopt a difference-unaware perspective that enforces uniform treatment across demographic groups. These approaches, however, fail to distinguish between contexts where neutrality is required and those where group-specific attributes are legitimate and must be preserved. Building upon recent advances in difference-aware fairness for text-only models, we extend this concept to the multimodal domain and formalize the problem of difference-aware gender fairness for image captioning and text-to-image generation. We advocate for selective debiasing, which aims to mitigate unwanted bias in neutral contexts while preserving valid distinctions in explicit ones. To achieve this, we propose BioPro (Bias Orthogonal Projection), an entirely training-free framework. BioPro identifies a low-dimensional gender-variation subspace through counterfactual embeddings and applies projection to selectively neutralize gender-related information. Experiments show that BioPro effectively reduces gender bias in neutral cases while maintaining gender faithfulness in explicit ones, thus providing a promising direction toward achieving selective fairness in VLMs. Beyond gender bias, we further demonstrate that BioPro can effectively generalize to continuous bias variables, such as scene brightness, highlighting its broader applicability.", "AI": {"tldr": "\u63d0\u51fa\u4e86BioPro\uff0c\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9009\u62e9\u6027\u5730\u51cf\u5c11\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6027\u522b\u504f\u89c1\u3002", "motivation": "\u73b0\u6709\u7684\u516c\u5e73\u6027\u5e72\u9884\u63aa\u65bd\u901a\u5e38\u91c7\u7528\u5dee\u5f02\u6027\u76f2\u89c6\u89d2\u5ea6\uff0c\u5f3a\u5236\u5bf9\u4e0d\u540c\u4eba\u53e3\u7fa4\u4f53\u8fdb\u884c\u7edf\u4e00\u5bf9\u5f85\uff0c\u4f46\u672a\u80fd\u533a\u5206\u9700\u8981\u4e2d\u7acb\u7684\u8bed\u5883\u548c\u5fc5\u987b\u4fdd\u7559\u7279\u5b9a\u7fa4\u4f53\u5c5e\u6027\u7684\u8bed\u5883\u3002", "method": "\u901a\u8fc7\u53cd\u4e8b\u5b9e\u5d4c\u5165\u8bc6\u522b\u4f4e\u7ef4\u6027\u522b\u53d8\u5f02\u5b50\u7a7a\u95f4\uff0c\u5e76\u5e94\u7528\u6295\u5f71\u6765\u9009\u62e9\u6027\u5730\u6d88\u9664\u6027\u522b\u76f8\u5173\u4fe1\u606f\u3002", "result": "BioPro\u5728 \u043d\u0435\u0439\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0445 \u60c5\u51b5\u4e0b\u6709\u6548\u5730\u51cf\u5c11\u4e86\u6027\u522b\u504f\u89c1\uff0c\u540c\u65f6\u5728 \u044f\u0432\u043d\u044b\u0445 \u60c5\u51b5\u4e0b\u4fdd\u6301\u4e86\u6027\u522b\u5fe0\u5b9e\u5ea6\u3002", "conclusion": "BioPro\u4e3a\u5b9e\u73b0\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u9009\u62e9\u6027\u516c\u5e73\u6027\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u5e0c\u671b\u7684\u65b9\u5411\uff0c\u5e76\u4e14\u53ef\u4ee5\u6709\u6548\u5730\u63a8\u5e7f\u5230\u8fde\u7eed\u504f\u89c1\u53d8\u91cf\uff0c\u5982\u573a\u666f\u4eae\u5ea6\u3002"}}
{"id": "2512.00515", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00515", "abs": "https://arxiv.org/abs/2512.00515", "authors": ["Cem Rifki Aydin"], "title": "Developing a Comprehensive Framework for Sentiment Analysis in Turkish", "comment": "Ph.D. Thesis, Bogazici University, 2020", "summary": "In this thesis, we developed a comprehensive framework for sentiment analysis that takes its many aspects into account mainly for Turkish. We have also proposed several approaches specific to sentiment analysis in English only. We have accordingly made five major and three minor contributions. We generated a novel and effective feature set by combining unsupervised, semi-supervised, and supervised metrics. We then fed them as input into classical machine learning methods, and outperformed neural network models for datasets of different genres in both Turkish and English. We created a polarity lexicon with a semi-supervised domain-specific method, which has been the first approach applied for corpora in Turkish. We performed a fine morphological analysis for the sentiment classification task in Turkish by determining the polarities of morphemes. This can be adapted to other morphologically-rich or agglutinative languages as well. We have built a novel neural network architecture, which combines recurrent and recursive neural network models for English. We built novel word embeddings that exploit sentiment, syntactic, semantic, and lexical characteristics for both Turkish and English. We also redefined context windows as subclauses in modelling word representations in English. This can also be applied to other linguistic fields and natural language processing tasks. We have achieved state-of-the-art and significant results for all these original approaches. Our minor contributions include methods related to aspect-based sentiment in Turkish, parameter redefinition in the semi-supervised approach, and aspect term extraction techniques for English. This thesis can be considered the most detailed and comprehensive study made on sentiment analysis in Turkish as of July, 2020. Our work has also contributed to the opinion classification problem in English.", "AI": {"tldr": "\u672c\u8bba\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u7efc\u5408\u60c5\u611f\u5206\u6790\u6846\u67b6\uff0c\u4e3b\u8981\u9488\u5bf9\u571f\u8033\u5176\u8bed\uff0c\u4e5f\u4e3a\u82f1\u8bed\u63d0\u51fa\u4e86\u51e0\u79cd\u60c5\u611f\u5206\u6790\u65b9\u6cd5\u3002", "motivation": "\u60c5\u611f\u5206\u6790\u5728\u571f\u8033\u5176\u8bed\u4e2d\u7684\u5e94\u7528\u76f8\u5bf9\u8f83\u5c11\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u571f\u8033\u5176\u8bed\u548c\u82f1\u8bed\u60c5\u611f\u5206\u6790\u65f6\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u7ed3\u5408\u4e86\u975e\u76d1\u7763\u3001\u534a\u76d1\u7763\u548c\u76d1\u7763\u6307\u6807\uff0c\u751f\u6210\u65b0\u7684\u7279\u5f81\u96c6\uff0c\u5e76\u5c06\u5176\u8f93\u5165\u5230\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4e2d\u3002\u6784\u5efa\u4e86\u65b0\u9896\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7ed3\u5408\u4e86\u5faa\u73af\u548c\u9012\u5f52\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3002\u521b\u5efa\u4e86\u5229\u7528\u60c5\u611f\u3001\u53e5\u6cd5\u3001\u8bed\u4e49\u548c\u8bcd\u6c47\u7279\u5f81\u7684\u65b0\u578b\u8bcd\u5d4c\u5165\u3002", "result": "\u5728\u571f\u8033\u5176\u8bed\u548c\u82f1\u8bed\u7684\u4e0d\u540c\u7c7b\u578b\u6570\u636e\u96c6\u4e2d\uff0c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7684\u6027\u80fd\u4f18\u4e8e\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3002\u5728\u6240\u6709\u539f\u521b\u65b9\u6cd5\u4e0a\u90fd\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u548c\u663e\u8457\u7684\u6210\u679c\u3002", "conclusion": "\u672c\u8bba\u6587\u662f\u622a\u81f32020\u5e747\u6708\uff0c\u9488\u5bf9\u571f\u8033\u5176\u8bed\u60c5\u611f\u5206\u6790\u6700\u8be6\u7ec6\u548c\u5168\u9762\u7684\u7814\u7a76\u3002\u8be5\u7814\u7a76\u4e5f\u4e3a\u82f1\u8bed\u7684\u89c2\u70b9\u5206\u7c7b\u95ee\u9898\u505a\u51fa\u4e86\u8d21\u732e\u3002"}}
{"id": "2512.00307", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00307", "abs": "https://arxiv.org/abs/2512.00307", "authors": ["Haobin Ke", "Sen Zhang", "Qingqing Ye", "Xun Ran", "Haibo Hu"], "title": "Adversarial Signed Graph Learning with Differential Privacy", "comment": null, "summary": "Signed graphs with positive and negative edges can model complex relationships in social networks. Leveraging on balance theory that deduces edge signs from multi-hop node pairs, signed graph learning can generate node embeddings that preserve both structural and sign information. However, training on sensitive signed graphs raises significant privacy concerns, as model parameters may leak private link information. Existing protection methods with differential privacy (DP) typically rely on edge or gradient perturbation for unsigned graph protection. Yet, they are not well-suited for signed graphs, mainly because edge perturbation tends to cascading errors in edge sign inference under balance theory, while gradient perturbation increases sensitivity due to node interdependence and gradient polarity change caused by sign flips, resulting in larger noise injection. In this paper, motivated by the robustness of adversarial learning to noisy interactions, we present ASGL, a privacy-preserving adversarial signed graph learning method that preserves high utility while achieving node-level DP. We first decompose signed graphs into positive and negative subgraphs based on edge signs, and then design a gradient-perturbed adversarial module to approximate the true signed connectivity distribution. In particular, the gradient perturbation helps mitigate cascading errors, while the subgraph separation facilitates sensitivity reduction. Further, we devise a constrained breadth-first search tree strategy that fuses with balance theory to identify the edge signs between generated node pairs. This strategy also enables gradient decoupling, thereby effectively lowering gradient sensitivity. Extensive experiments on real-world datasets show that ASGL achieves favorable privacy-utility trade-offs across multiple downstream tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4fdd\u62a4\u9690\u79c1\u7684\u5bf9\u6297\u6027\u7b26\u53f7\u56fe\u5b66\u4e60\u65b9\u6cd5\uff08ASGL\uff09\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9e\u73b0\u8282\u70b9\u7ea7\u5dee\u5206\u9690\u79c1\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u9ad8\u5b9e\u7528\u6027\u3002", "motivation": "\u5728\u654f\u611f\u7684\u7b26\u53f7\u56fe\u4e0a\u8fdb\u884c\u8bad\u7ec3\u4f1a\u5f15\u8d77\u4e25\u91cd\u7684\u9690\u79c1\u95ee\u9898\uff0c\u56e0\u4e3a\u6a21\u578b\u53c2\u6570\u53ef\u80fd\u4f1a\u6cc4\u9732\u79c1\u6709\u94fe\u63a5\u4fe1\u606f\u3002\u73b0\u6709\u7684\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u4fdd\u62a4\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u8fb9\u6216\u68af\u5ea6\u6270\u52a8\u6765\u8fdb\u884c\u65e0\u7b26\u53f7\u56fe\u4fdd\u62a4\uff0c\u4f46\u5b83\u4eec\u4e0d\u592a\u9002\u5408\u4e8e\u7b26\u53f7\u56fe\u3002", "method": "ASGL\u65b9\u6cd5\u9996\u5148\u5c06\u7b26\u53f7\u56fe\u5206\u89e3\u4e3a\u6b63\u5b50\u56fe\u548c\u8d1f\u5b50\u56fe\uff0c\u7136\u540e\u8bbe\u8ba1\u4e00\u4e2a\u68af\u5ea6\u6270\u52a8\u5bf9\u6297\u6a21\u5757\u6765\u903c\u8fd1\u771f\u5b9e\u7684\u7b26\u53f7\u8fde\u901a\u6027\u5206\u5e03\u3002\u6b64\u5916\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7ea6\u675f\u5e7f\u5ea6\u4f18\u5148\u641c\u7d22\u6811\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u4e0e\u5e73\u8861\u7406\u8bba\u76f8\u7ed3\u5408\uff0c\u4ee5\u8bc6\u522b\u751f\u6210\u7684\u8282\u70b9\u5bf9\u4e4b\u95f4\u7684\u8fb9\u7b26\u53f7\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cASGL\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u826f\u597d\u7684\u9690\u79c1-\u6548\u7528\u6298\u8877\u3002", "conclusion": "ASGL\u65b9\u6cd5\u901a\u8fc7\u68af\u5ea6\u6270\u52a8\u51cf\u8f7b\u4e86\u7ea7\u8054\u8bef\u5dee\uff0c\u800c\u5b50\u56fe\u5206\u79bb\u6709\u52a9\u4e8e\u964d\u4f4e\u654f\u611f\u6027\u3002\u7ea6\u675f\u5e7f\u5ea6\u4f18\u5148\u641c\u7d22\u6811\u7b56\u7565\u80fd\u591f\u8fdb\u884c\u68af\u5ea6\u89e3\u8026\uff0c\u4ece\u800c\u6709\u6548\u5730\u964d\u4f4e\u4e86\u68af\u5ea6\u654f\u611f\u6027\u3002"}}
{"id": "2512.00129", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00129", "abs": "https://arxiv.org/abs/2512.00129", "authors": ["Jayan Adhikari", "Prativa Joshi", "Susish Baral"], "title": "Analysis of Incursive Breast Cancer in Mammograms Using YOLO, Explainability, and Domain Adaptation", "comment": null, "summary": "Deep learning models for breast cancer detection from mammographic images have significant reliability problems when presented with Out-of-Distribution (OOD) inputs such as other imaging modalities (CT, MRI, X-ray) or equipment variations, leading to unreliable detection and misdiagnosis. The current research mitigates the fundamental OOD issue through a comprehensive approach integrating ResNet50-based OOD filtering with YOLO architectures (YOLOv8, YOLOv11, YOLOv12) for accurate detection of breast cancer. Our strategy establishes an in-domain gallery via cosine similarity to rigidly reject non-mammographic inputs prior to processing, ensuring that only domain-associated images supply the detection pipeline. The OOD detection component achieves 99.77\\% general accuracy with immaculate 100\\% accuracy on OOD test sets, effectively eliminating irrelevant imaging modalities. ResNet50 was selected as the optimum backbone after 12 CNN architecture searches. The joint framework unites OOD robustness with high detection performance (mAP@0.5: 0.947) and enhanced interpretability through Grad-CAM visualizations. Experimental validation establishes that OOD filtering significantly improves system reliability by preventing false alarms on out-of-distribution inputs while maintaining higher detection accuracy on mammographic data. The present study offers a fundamental foundation for the deployment of reliable AI-based breast cancer detection systems in diverse clinical environments with inherent data heterogeneity.", "AI": {"tldr": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4e73\u817a\u764c\u68c0\u6d4b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5904\u7406\u975e\u540c\u5206\u5e03(OOD)\u8f93\u5165\u65f6\u53ef\u9760\u6027\u4f4e\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u6574\u5408\u57fa\u4e8eResNet50\u7684OOD\u8fc7\u6ee4\u4e0eYOLO\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86\u5bf9\u4e73\u817a\u764c\u7684\u7cbe\u786e\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u4e73\u817a\u764c\u68c0\u6d4b\u6a21\u578b\u5728\u9762\u5bf9\u5176\u4ed6\u6210\u50cf\u65b9\u5f0f\u6216\u8bbe\u5907\u53d8\u5f02\u7b49OOD\u8f93\u5165\u65f6\uff0c\u5b58\u5728\u53ef\u9760\u6027\u95ee\u9898\uff0c\u5bfc\u81f4\u8bef\u8bca\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7efc\u5408\u65b9\u6cd5\uff0c\u6574\u5408\u4e86\u57fa\u4e8eResNet50\u7684OOD\u8fc7\u6ee4\u4e0eYOLO\u67b6\u6784\uff08YOLOv8, YOLOv11, YOLOv12\uff09\uff0c\u5229\u7528\u4f59\u5f26\u76f8\u4f3c\u6027\u5efa\u7acb\u57df\u5185\u56fe\u5e93\uff0c\u4ee5\u6392\u9664\u975e\u4e73\u817a\u56fe\u50cf\u8f93\u5165\u3002", "result": "OOD\u68c0\u6d4b\u7ec4\u4ef6\u5b9e\u73b0\u4e8699.77%\u7684\u603b\u4f53\u51c6\u786e\u7387\uff0c\u5728OOD\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u5230\u4e86100%\u7684\u51c6\u786e\u7387\uff0c\u6709\u6548\u6d88\u9664\u4e86\u4e0d\u76f8\u5173\u7684\u6210\u50cf\u65b9\u5f0f\u3002\u8054\u5408\u6846\u67b6\u5b9e\u73b0\u4e86\u9ad8\u68c0\u6d4b\u6027\u80fd(mAP@0.5: 0.947)\uff0c\u5e76\u901a\u8fc7Grad-CAM\u53ef\u89c6\u5316\u589e\u5f3a\u4e86\u89e3\u91ca\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u5728\u5177\u6709\u56fa\u6709\u6570\u636e\u5f02\u8d28\u6027\u7684\u5404\u79cd\u4e34\u5e8a\u73af\u5883\u4e2d\u90e8\u7f72\u53ef\u9760\u7684\u57fa\u4e8eAI\u7684\u4e73\u817a\u764c\u68c0\u6d4b\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.00818", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00818", "abs": "https://arxiv.org/abs/2512.00818", "authors": ["Haozhen Gong", "Xiaozhong Ji", "Yuansen Liu", "Wenbin Wu", "Xiaoxiao Yan", "Jingjing Liu", "Kai Wu", "Jiazhen Pan", "Bailiang Jian", "Jiangning Zhang", "Xiaobin Hu", "Hongwei Bran Li"], "title": "Med-CMR: A Fine-Grained Benchmark Integrating Visual Evidence and Clinical Logic for Medical Complex Multimodal Reasoning", "comment": null, "summary": "MLLMs MLLMs are beginning to appear in clinical workflows, but their ability to perform complex medical reasoning remains unclear. We present Med-CMR, a fine-grained Medical Complex Multimodal Reasoning benchmark. Med-CMR distinguishes from existing counterparts by three core features: 1) Systematic capability decomposition, splitting medical multimodal reasoning into fine-grained visual understanding and multi-step reasoning to enable targeted evaluation; 2) Challenging task design, with visual understanding across three key dimensions (small-object detection, fine-detail discrimination, spatial understanding) and reasoning covering four clinically relevant scenarios (temporal prediction, causal reasoning, long-tail generalization, multi-source integration); 3) Broad, high-quality data coverage, comprising 20,653 Visual Question Answering (VQA) pairs spanning 11 organ systems and 12 imaging modalities, validated via a rigorous two-stage (human expert + model-assisted) review to ensure clinical authenticity. We evaluate 18 state-of-the-art MLLMs with Med-CMR, revealing GPT-5 as the top-performing commercial model: 57.81 accuracy on multiple-choice questions (MCQs) and a 48.70 open-ended score, outperforming Gemini 2.5 Pro (49.87 MCQ accuracy, 45.98 open-ended score) and leading open-source model Qwen3-VL-235B-A22B (49.34 MCQ accuracy, 42.62 open-ended score). However, specialized medical MLLMs do not reliably outperform strong general models, and long-tail generalization emerges as the dominant failure mode. Med-CMR thus provides a stress test for visual-reasoning integration and rare-case robustness in medical MLLMs, and a rigorous yardstick for future clinical systems.", "AI": {"tldr": "Med-CMR: A new benchmark for evaluating complex medical reasoning in multimodal large language models (MLLMs).", "motivation": "Existing benchmarks lack fine-grained evaluation and challenging tasks for medical reasoning.", "method": "A new benchmark, Med-CMR, is introduced with systematic capability decomposition, challenging task design, and broad, high-quality data coverage.", "result": "Evaluated 18 state-of-the-art MLLMs, with GPT-5 performing the best but still struggling with long-tail generalization.", "conclusion": "Med-CMR provides a stress test for medical MLLMs and a yardstick for future clinical systems."}}
{"id": "2512.00552", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00552", "abs": "https://arxiv.org/abs/2512.00552", "authors": ["Subramanyam Sahoo", "Vinija Jain", "Saanidhya Vats", "Siddharth Mohapatra", "Rui Min", "Aman Chadha", "Divya Chaudhary"], "title": "Catch Me If You Can: How Smaller Reasoning Models Pretend to Reason with Mathematical Fidelity", "comment": "8 pages, 5 figures. A preprint. Initial Work", "summary": "Current evaluation of mathematical reasoning in language models relies primarily on answer accuracy, potentially masking fundamental failures in logical computation. We introduce a diagnostic framework that distinguishes genuine mathematical reasoning from superficial pattern matching through four complementary axes: forward-backward consistency, transitivity coverage, counterfactual sensitivity, and perturbation robustness. Through a case study applying this framework to Qwen3-0.6B on the MenatQA dataset, we reveal a striking disconnect between surface performance and reasoning fidelity. While the model achieves reasonable answer accuracy (70%+), it demonstrates poor backward consistency (15%), limited transitivity coverage (32.2%), and brittle sensitivity to perturbations. Our diagnostics expose reasoning failures invisible to traditional accuracy metrics, suggesting that this small model relies heavily on pattern matching rather than genuine logical computation. While our empirical findings are based on a single 600M-parameter model, the diagnostic framework itself is model-agnostic and generalizable. We release our evaluation protocols to enable the research community to assess reasoning fidelity across different model scales and architectures, moving beyond surface-level accuracy toward verifiable mathematical reasoning.", "AI": {"tldr": "\u5f53\u524d\u5bf9\u8bed\u8a00\u6a21\u578b\u6570\u5b66\u63a8\u7406\u7684\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u4e8e\u7b54\u6848\u7684\u51c6\u786e\u6027\uff0c\u8fd9\u53ef\u80fd\u4f1a\u63a9\u76d6\u903b\u8f91\u8ba1\u7b97\u4e2d\u7684\u6839\u672c\u6027\u5931\u8d25\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u8bca\u65ad\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u4e2a\u4e92\u8865\u7684\u8f74\u6765\u533a\u5206\u771f\u6b63\u7684\u6570\u5b66\u63a8\u7406\u548c\u8868\u9762\u6a21\u5f0f\u5339\u914d\uff1a\u524d\u540e\u4e00\u81f4\u6027\u3001\u4f20\u9012\u6027\u8986\u76d6\u3001\u53cd\u4e8b\u5b9e\u654f\u611f\u6027\u548c\u6270\u52a8\u9c81\u68d2\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5f53\u524d\u5bf9\u8bed\u8a00\u6a21\u578b\u6570\u5b66\u63a8\u7406\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u4e8e\u7b54\u6848\u51c6\u786e\u6027\uff0c\u53ef\u80fd\u63a9\u76d6\u903b\u8f91\u8ba1\u7b97\u6839\u672c\u6027\u5931\u8d25\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\uff0c\u5c06\u8be5\u6846\u67b6\u5e94\u7528\u4e8e MenatQA \u6570\u636e\u96c6\u4e0a\u7684 Qwen3-0.6B\u3002", "result": "\u8be5\u6a21\u578b\u5b9e\u73b0\u4e86\u5408\u7406\u7684\u7b54\u6848\u51c6\u786e\u7387\uff0870%+\uff09\uff0c\u4f46\u5728\u540e\u5411\u4e00\u81f4\u6027\uff0815%\uff09\u3001\u6709\u9650\u7684\u4f20\u9012\u6027\u8986\u76d6\u7387\uff0832.2%\uff09\u548c\u5bf9\u6270\u52a8\u7684\u8106\u5f31\u654f\u611f\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002\u8bca\u65ad\u7ed3\u679c\u63ed\u793a\u4e86\u4f20\u7edf\u51c6\u786e\u6027\u6307\u6807\u65e0\u6cd5\u5bdf\u89c9\u7684\u63a8\u7406\u5931\u8d25\uff0c\u8868\u660e\u8fd9\u4e2a\u5c0f\u578b\u6a21\u578b\u4e25\u91cd\u4f9d\u8d56\u6a21\u5f0f\u5339\u914d\u800c\u4e0d\u662f\u771f\u6b63\u7684\u903b\u8f91\u8ba1\u7b97\u3002", "conclusion": "\u867d\u7136\u6211\u4eec\u7684\u7ecf\u9a8c\u7ed3\u679c\u662f\u57fa\u4e8e\u5355\u4e2a 6 \u4ebf\u53c2\u6570\u6a21\u578b\uff0c\u4f46\u8bca\u65ad\u6846\u67b6\u672c\u8eab\u662f\u6a21\u578b\u4e0d\u53ef\u77e5\u7684\u548c\u53ef\u63a8\u5e7f\u7684\u3002\u6211\u4eec\u53d1\u5e03\u4e86\u8bc4\u4f30\u534f\u8bae\uff0c\u4f7f\u7814\u7a76\u754c\u80fd\u591f\u8de8\u4e0d\u540c\u7684\u6a21\u578b\u89c4\u6a21\u548c\u67b6\u6784\u8bc4\u4f30\u63a8\u7406\u4fdd\u771f\u5ea6\uff0c\u4ece\u800c\u8d85\u8d8a\u8868\u9762\u6c34\u5e73\u7684\u51c6\u786e\u6027\uff0c\u8f6c\u5411\u53ef\u9a8c\u8bc1\u7684\u6570\u5b66\u63a8\u7406\u3002"}}
{"id": "2512.00311", "categories": ["cs.LG", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.00311", "abs": "https://arxiv.org/abs/2512.00311", "authors": ["Jungyang Park", "Suho Kang", "Jaewoo Park", "Jaehong Kim", "Jaewoo Shin", "Seonjoon Park", "Youngjae Yu"], "title": "Tracing Mathematical Proficiency Through Problem-Solving Processes", "comment": "15 pages, 7 figures", "summary": "Knowledge Tracing (KT) aims to model student's knowledge state and predict future performance to enable personalized learning in Intelligent Tutoring Systems. However, traditional KT methods face fundamental limitations in explainability, as they rely solely on the response correctness, neglecting the rich information embedded in students' problem-solving processes. To address this gap, we propose Knowledge Tracing Leveraging Problem-Solving Process (KT-PSP), which incorporates students' problem-solving processes to capture the multidimensional aspects of mathematical proficiency. We also introduce KT-PSP-25, a new dataset specifically designed for the KT-PSP. Building on this, we present StatusKT, a KT framework that employs a teacher-student-teacher three-stage LLM pipeline to extract students' MP as intermediate signals. In this pipeline, the teacher LLM first extracts problem-specific proficiency indicators, then a student LLM generates responses based on the student's solution process, and a teacher LLM evaluates these responses to determine mastery of each indicator. The experimental results on KT-PSP-25 demonstrate that StatusKT improves the prediction performance of existing KT methods. Moreover, StatusKT provides interpretable explanations for its predictions by explicitly modeling students' mathematical proficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u77e5\u8bc6\u8ffd\u8e2a\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u5b66\u751f\u7684\u95ee\u9898\u89e3\u51b3\u8fc7\u7a0b\u6765\u6355\u6349\u6570\u5b66\u80fd\u529b\u7684\u591a\u7ef4\u65b9\u9762\u3002", "motivation": "\u4f20\u7edf\u7684\u77e5\u8bc6\u8ffd\u8e2a\u65b9\u6cd5\u5728\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5b58\u5728\u6839\u672c\u5c40\u9650\u6027\uff0c\u56e0\u4e3a\u5b83\u4eec\u4ec5\u4f9d\u8d56\u4e8e\u56de\u7b54\u7684\u6b63\u786e\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u5b66\u751f\u89e3\u51b3\u95ee\u9898\u8fc7\u7a0b\u4e2d\u5d4c\u5165\u7684\u4e30\u5bcc\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u77e5\u8bc6\u8ffd\u8e2a\u5229\u7528\u95ee\u9898\u89e3\u51b3\u8fc7\u7a0b\uff08KT-PSP\uff09\uff0c\u5e76\u5f15\u5165\u4e86KT-PSP-25\u6570\u636e\u96c6\u3002\u63d0\u51fa\u4e86StatusKT\uff0c\u4e00\u4e2aKT\u6846\u67b6\uff0c\u5b83\u91c7\u7528\u6559\u5e08-\u5b66\u751f-\u6559\u5e08\u4e09\u9636\u6bb5LLM\u7ba1\u9053\u6765\u63d0\u53d6\u5b66\u751f\u7684MP\u4f5c\u4e3a\u4e2d\u95f4\u4fe1\u53f7\u3002\u5728\u8fd9\u4e2a\u7ba1\u9053\u4e2d\uff0c\u6559\u5e08LLM\u9996\u5148\u63d0\u53d6\u7279\u5b9a\u4e8e\u95ee\u9898\u7684\u719f\u7ec3\u7a0b\u5ea6\u6307\u6807\uff0c\u7136\u540e\u5b66\u751fLLM\u57fa\u4e8e\u5b66\u751f\u7684\u89e3\u51b3\u65b9\u6848\u8fc7\u7a0b\u751f\u6210\u54cd\u5e94\uff0c\u5e76\u4e14\u6559\u5e08LLM\u8bc4\u4f30\u8fd9\u4e9b\u54cd\u5e94\u4ee5\u786e\u5b9a\u5bf9\u6bcf\u4e2a\u6307\u6807\u7684\u638c\u63e1\u7a0b\u5ea6\u3002", "result": "\u5728KT-PSP-25\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cStatusKT\u63d0\u9ad8\u4e86\u73b0\u6709KT\u65b9\u6cd5\u7684\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "StatusKT\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u5b66\u751f\u7684\u6570\u5b66\u80fd\u529b\uff0c\u4e3a\u5176\u9884\u6d4b\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u89e3\u91ca\u3002"}}
{"id": "2512.00130", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00130", "abs": "https://arxiv.org/abs/2512.00130", "authors": ["Fadi Dornaika", "Danyang Sun"], "title": "Local and Global Context-and-Object-part-Aware Superpixel-based Data Augmentation for Deep Visual Recognition", "comment": null, "summary": "Cutmix-based data augmentation, which uses a cut-and-paste strategy, has shown remarkable generalization capabilities in deep learning. However, existing methods primarily consider global semantics with image-level constraints, which excessively reduces attention to the discriminative local context of the class and leads to a performance improvement bottleneck. Moreover, existing methods for generating augmented samples usually involve cutting and pasting rectangular or square regions, resulting in a loss of object part information. To mitigate the problem of inconsistency between the augmented image and the generated mixed label, existing methods usually require double forward propagation or rely on an external pre-trained network for object centering, which is inefficient. To overcome the above limitations, we propose LGCOAMix, an efficient context-aware and object-part-aware superpixel-based grid blending method for data augmentation. To the best of our knowledge, this is the first time that a label mixing strategy using a superpixel attention approach has been proposed for cutmix-based data augmentation. It is the first instance of learning local features from discriminative superpixel-wise regions and cross-image superpixel contrasts. Extensive experiments on various benchmark datasets show that LGCOAMix outperforms state-of-the-art cutmix-based data augmentation methods on classification tasks, {and weakly supervised object location on CUB200-2011.} We have demonstrated the effectiveness of LGCOAMix not only for CNN networks, but also for Transformer networks. Source codes are available at https://github.com/DanielaPlusPlus/LGCOAMix.", "AI": {"tldr": "LGCOAMix: A novel cutmix data augmentation method using superpixel attention for local feature learning and cross-image contrast.", "motivation": "Existing cutmix methods overly focus on global semantics, neglect local context, and lose object part information, leading to performance bottlenecks and inefficiency.", "method": "Proposes LGCOAMix, a superpixel-based grid blending method that is context-aware and object-part-aware. It uses a superpixel attention approach for label mixing and learns local features from discriminative superpixel regions.", "result": "LGCOAMix outperforms state-of-the-art cutmix methods on classification tasks and weakly supervised object localization. Effective for both CNN and Transformer networks.", "conclusion": "LGCOAMix addresses limitations of existing cutmix methods by incorporating local context and object part information through superpixel attention, leading to improved performance and efficiency."}}
{"id": "2512.00834", "categories": ["cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.00834", "abs": "https://arxiv.org/abs/2512.00834", "authors": ["Lin Zhu", "Kezhi Wang", "Luping Xiang", "Kun Yang"], "title": "SemAgent: Semantic-Driven Agentic AI Empowered Trajectory Prediction in Vehicular Networks", "comment": "Submitted for possible journal publication", "summary": "Efficient information exchange and reliable contextual reasoning are essential for vehicle-to-everything (V2X) networks. Conventional communication schemes often incur significant transmission overhead and latency, while existing trajectory prediction models generally lack environmental perception and logical inference capabilities. This paper presents a trajectory prediction framework that integrates semantic communication with Agentic AI to enhance predictive performance in vehicular environments. In vehicle-to-infrastructure (V2I) communication, a feature-extraction agent at the Roadside Unit (RSU) derives compact representations from historical vehicle trajectories, followed by semantic reasoning performed by a semantic-analysis agent. The RSU then transmits both feature representations and semantic insights to the target vehicle via semantic communication, enabling the vehicle to predict future trajectories by combining received semantics with its own historical data. In vehicle-to-vehicle (V2V) communication, each vehicle performs local feature extraction and semantic analysis while receiving predicted trajectories from neighboring vehicles, and jointly utilizes this information for its own trajectory prediction. Extensive experiments across diverse communication conditions demonstrate that the proposed method significantly outperforms baseline schemes, achieving up to a 47.5% improvement in prediction accuracy under low signal-to-noise ratio (SNR) conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8bed\u4e49\u901a\u4fe1\u548cAgentic AI\u7684\u8f68\u8ff9\u9884\u6d4b\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u8f66\u8f86\u73af\u5883\u4e2d\u7684\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u901a\u4fe1\u65b9\u6848\u901a\u5e38\u4f1a\u4ea7\u751f\u663e\u8457\u7684\u4f20\u8f93\u5f00\u9500\u548c\u5ef6\u8fdf\uff0c\u800c\u73b0\u6709\u7684\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\u901a\u5e38\u7f3a\u4e4f\u73af\u5883\u611f\u77e5\u548c\u903b\u8f91\u63a8\u7406\u80fd\u529b\u3002", "method": "\u5728V2I\u901a\u4fe1\u4e2d\uff0c\u8def\u4fa7\u5355\u5143\uff08RSU\uff09\u7684\u7279\u5f81\u63d0\u53d6\u4ee3\u7406\u4ece\u5386\u53f2\u8f66\u8f86\u8f68\u8ff9\u4e2d\u63d0\u53d6\u7d27\u51d1\u7684\u8868\u793a\uff0c\u7136\u540e\u7531\u8bed\u4e49\u5206\u6790\u4ee3\u7406\u6267\u884c\u8bed\u4e49\u63a8\u7406\u3002\u7136\u540e\uff0cRSU\u901a\u8fc7\u8bed\u4e49\u901a\u4fe1\u5c06\u7279\u5f81\u8868\u793a\u548c\u8bed\u4e49\u89c1\u89e3\u4f20\u8f93\u5230\u76ee\u6807\u8f66\u8f86\uff0c\u4f7f\u8f66\u8f86\u80fd\u591f\u901a\u8fc7\u5c06\u63a5\u6536\u5230\u7684\u8bed\u4e49\u4e0e\u5176\u81ea\u8eab\u7684\u5386\u53f2\u6570\u636e\u76f8\u7ed3\u5408\u6765\u9884\u6d4b\u672a\u6765\u7684\u8f68\u8ff9\u3002\u5728V2V\u901a\u4fe1\u4e2d\uff0c\u6bcf\u8f86\u8f66\u90fd\u6267\u884c\u672c\u5730\u7279\u5f81\u63d0\u53d6\u548c\u8bed\u4e49\u5206\u6790\uff0c\u540c\u65f6\u63a5\u6536\u6765\u81ea\u76f8\u90bb\u8f66\u8f86\u7684\u9884\u6d4b\u8f68\u8ff9\uff0c\u5e76\u5171\u540c\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u8fdb\u884c\u81ea\u5df1\u7684\u8f68\u8ff9\u9884\u6d4b\u3002", "result": "\u5728\u4e0d\u540c\u7684\u901a\u4fe1\u6761\u4ef6\u4e0b\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6848\uff0c\u5728\u4f4e\u4fe1\u566a\u6bd4\uff08SNR\uff09\u6761\u4ef6\u4e0b\uff0c\u9884\u6d4b\u7cbe\u5ea6\u63d0\u9ad8\u4e8647.5%\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684V2X\u7f51\u7edc\u8f68\u8ff9\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u901a\u4fe1\u548cAgentic AI\u7684\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u3002"}}
{"id": "2512.00579", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00579", "abs": "https://arxiv.org/abs/2512.00579", "authors": ["Miroslav Bl\u0161t\u00e1k"], "title": "Slovak Conceptual Dictionary", "comment": "7 pages, 2 figures", "summary": "When solving tasks in the field of natural language processing, we sometimes need dictionary tools, such as lexicons, word form dictionaries or knowledge bases. However, the availability of dictionary data is insufficient in many languages, especially in the case of low resourced languages. In this article, we introduce a new conceptual dictionary for the Slovak language as the first linguistic tool of this kind. Since Slovak language is a language with limited linguistic resources and there are currently not available any machine-readable linguistic data sources with a sufficiently large volume of data, many tasks which require automated processing of Slovak text achieve weaker results compared to other languages and are almost impossible to solve.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u65af\u6d1b\u4f10\u514b\u8bed\u6982\u5ff5\u8bcd\u5178\uff0c\u662f\u540c\u7c7b\u8bed\u8a00\u5de5\u5177\u4e2d\u7684\u9996\u521b\u3002", "motivation": "\u7531\u4e8e\u65af\u6d1b\u4f10\u514b\u8bed\u662f\u4e00\u79cd\u8bed\u8a00\u8d44\u6e90\u6709\u9650\u7684\u8bed\u8a00\uff0c\u76ee\u524d\u8fd8\u6ca1\u6709\u8db3\u591f\u5927\u7684\u673a\u5668\u53ef\u8bfb\u8bed\u8a00\u6570\u636e\u6e90\uff0c\u56e0\u6b64\u8bb8\u591a\u9700\u8981\u81ea\u52a8\u5904\u7406\u65af\u6d1b\u4f10\u514b\u8bed\u6587\u672c\u7684\u4efb\u52a1\u4e0e\u5176\u4ed6\u8bed\u8a00\u76f8\u6bd4\uff0c\u6548\u679c\u8f83\u5dee\uff0c\u51e0\u4e4e\u65e0\u6cd5\u89e3\u51b3\u3002", "method": "\u521b\u5efa\u65b0\u7684\u65af\u6d1b\u4f10\u514b\u8bed\u6982\u5ff5\u8bcd\u5178\u3002", "result": "\u521b\u5efa\u4e86\u9996\u4e2a\u65af\u6d1b\u4f10\u514b\u8bed\u6982\u5ff5\u8bcd\u5178\u3002", "conclusion": "\u521b\u5efa\u4e86\u9996\u4e2a\u65af\u6d1b\u4f10\u514b\u8bed\u6982\u5ff5\u8bcd\u5178\uff0c\u4e3a\u65af\u6d1b\u4f10\u514b\u8bed\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u8d44\u6e90\u3002"}}
{"id": "2512.00321", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.00321", "abs": "https://arxiv.org/abs/2512.00321", "authors": ["Shivani Mruthyunjaya", "Anandi Dutta", "Kazi Sifatul Islam"], "title": "Introducing AI-Driven IoT Energy Management Framework", "comment": "Accepted in IEEE Smart World Congress 2025, Calgary, Canada", "summary": "Power consumption has become a critical aspect of modern life due to the consistent reliance on technological advancements. Reducing power consumption or following power usage predictions can lead to lower monthly costs and improved electrical reliability. The proposal of a holistic framework to establish a foundation for IoT systems with a focus on contextual decision making, proactive adaptation, and scalable structure. A structured process for IoT systems with accuracy and interconnected development would support reducing power consumption and support grid stability. This study presents the feasibility of this proposal through the application of each aspect of the framework. This system would have long term forecasting, short term forecasting, anomaly detection, and consideration of qualitative data with any energy management decisions taken. Performance was evaluated on Power Consumption Time Series data to display the direct application of the framework.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7269\u8054\u7f51\u7cfb\u7edf\u6846\u67b6\uff0c\u4ee5\u964d\u4f4e\u529f\u8017\u5e76\u63d0\u9ad8\u7535\u7f51\u7a33\u5b9a\u6027\u3002", "motivation": "\u7531\u4e8e\u5bf9\u6280\u672f\u8fdb\u6b65\u7684\u6301\u7eed\u4f9d\u8d56\uff0c\u529f\u8017\u5df2\u6210\u4e3a\u73b0\u4ee3\u751f\u6d3b\u7684\u4e00\u4e2a\u5173\u952e\u65b9\u9762\u3002\u964d\u4f4e\u529f\u8017\u6216\u9075\u5faa\u529f\u8017\u9884\u6d4b\u53ef\u4ee5\u964d\u4f4e\u6bcf\u6708\u6210\u672c\u5e76\u63d0\u9ad8\u7535\u6c14\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u5e94\u7528\u6846\u67b6\u7684\u5404\u4e2a\u65b9\u9762\u6765\u5c55\u793a\u8be5\u63d0\u6848\u7684\u53ef\u884c\u6027\u3002\u8be5\u7cfb\u7edf\u5c06\u5177\u6709\u957f\u671f\u9884\u6d4b\u3001\u77ed\u671f\u9884\u6d4b\u3001\u5f02\u5e38\u68c0\u6d4b\uff0c\u5e76\u8003\u8651\u5728\u4efb\u4f55\u80fd\u6e90\u7ba1\u7406\u51b3\u7b56\u4e2d\u4f7f\u7528\u7684\u5b9a\u6027\u6570\u636e\u3002", "result": "\u5728\u529f\u8017\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e0a\u8bc4\u4f30\u4e86\u6027\u80fd\uff0c\u4ee5\u663e\u793a\u6846\u67b6\u7684\u76f4\u63a5\u5e94\u7528\u3002", "conclusion": "\u4e00\u4e2a\u5177\u6709\u51c6\u786e\u6027\u548c\u4e92\u8fde\u53d1\u5c55\u7684\u7269\u8054\u7f51\u7cfb\u7edf\u7684\u7ed3\u6784\u5316\u8fc7\u7a0b\u5c06\u652f\u6301\u964d\u4f4e\u529f\u8017\u5e76\u652f\u6301\u7535\u7f51\u7a33\u5b9a\u6027\u3002"}}
{"id": "2512.00179", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.00179", "abs": "https://arxiv.org/abs/2512.00179", "authors": ["Mohamed Abdallah Salem", "Nourhan Zein Diab"], "title": "Efficient Edge-Compatible CNN for Speckle-Based Material Recognition in Laser Cutting Systems", "comment": "Copyright 2025 IEEE. This is the author's version of the work that has been Accepted for publication in the Proceedings of the 2025 IEEE The 35th International Conference on Computer Theory and Applications (ICCTA 2025). Final published version will be available on IEEE Xplore", "summary": "Accurate material recognition is critical for safe and effective laser cutting, as misidentification can lead to poor cut quality, machine damage, or the release of hazardous fumes. Laser speckle sensing has recently emerged as a low-cost and non-destructive modality for material classification; however, prior work has either relied on computationally expensive backbone networks or addressed only limited subsets of materials. In this study, A lightweight convolutional neural network (CNN) tailored for speckle patterns is proposed, designed to minimize parameters while maintaining high discriminative power. Using the complete SensiCut dataset of 59 material classes spanning woods, acrylics, composites, textiles, metals, and paper-based products, the proposed model achieves 95.05% test accuracy, with macro and weighted F1-scores of 0.951. The network contains only 341k trainable parameters (~1.3 MB) -- over 70X fewer than ResNet-50 -- and achieves an inference speed of 295 images per second, enabling deployment on Raspberry Pi and Jetson-class devices. Furthermore, when materials are regrouped into nine and five practical families, recall exceeds 98% and approaches 100%, directly supporting power and speed preset selection in laser cutters. These results demonstrate that compact, domain-specific CNNs can outperform large backbones for speckle-based material classification, advancing the feasibility of material-aware, edge-deployable laser cutting systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u6fc0\u5149\u5207\u5272\u4e2d\u57fa\u4e8e\u6563\u6591\u56fe\u6848\u7684\u6750\u6599\u8bc6\u522b\uff0c\u53ef\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u5feb\u901f\u63a8\u7406\u3002", "motivation": "\u7cbe\u786e\u7684\u6750\u6599\u8bc6\u522b\u5bf9\u4e8e\u5b89\u5168\u6709\u6548\u7684\u6fc0\u5149\u5207\u5272\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4ee5\u5f80\u7684\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u6216\u4ec5\u9650\u4e8e\u6750\u6599\u7684\u5b50\u96c6\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u4e13\u95e8\u7528\u4e8e\u5904\u7406\u6563\u6591\u56fe\u6848\uff0c\u65e8\u5728\u6700\u5c0f\u5316\u53c2\u6570\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u533a\u5206\u80fd\u529b\u3002", "result": "\u5728\u5305\u542b59\u79cd\u6750\u6599\u7c7b\u522b\u7684\u5b8c\u6574SensiCut\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u6a21\u578b\u5b9e\u73b0\u4e8695.05%\u7684\u6d4b\u8bd5\u7cbe\u5ea6\uff0c\u5b8f\u89c2\u548c\u52a0\u6743F1\u5206\u6570\u4e3a0.951\u3002\u8be5\u7f51\u7edc\u5305\u542b\u4ec5341k\u4e2a\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u5e76\u4e14\u5b9e\u73b0\u4e86\u6bcf\u79d2295\u5e27\u7684\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u7d27\u51d1\u7684\u3001\u7279\u5b9a\u9886\u57df\u7684CNN\u53ef\u4ee5\u4f18\u4e8e\u5927\u578b\u9aa8\u5e72\u7f51\u7edc\uff0c\u4ece\u800c\u63a8\u8fdb\u4e86\u6750\u6599\u611f\u77e5\u3001\u8fb9\u7f18\u53ef\u90e8\u7f72\u6fc0\u5149\u5207\u5272\u7cfb\u7edf\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2512.00836", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00836", "abs": "https://arxiv.org/abs/2512.00836", "authors": ["Emily Howerton", "Justin Lessler"], "title": "Assessing model error in counterfactual worlds", "comment": null, "summary": "Counterfactual scenario modeling exercises that ask \"what would happen if?\" are one of the most common ways we plan for the future. Despite their ubiquity in planning and decision making, scenario projections are rarely evaluated retrospectively. Differences between projections and observations come from two sources: scenario deviation and model miscalibration. We argue the latter is most important for assessing the value of models in decision making, but requires estimating model error in counterfactual worlds. Here we present and contrast three approaches for estimating this error, and demonstrate the benefits and limitations of each in a simulation experiment. We provide recommendations for the estimation of counterfactual error and discuss the components of scenario design that are required to make scenario projections evaluable.", "AI": {"tldr": "\u672c\u6587\u8ba8\u8bba\u4e86\u53cd\u4e8b\u5b9e\u60c5\u666f\u5efa\u6a21\u4e2d\u9884\u6d4b\u4e0e\u5b9e\u9645\u89c2\u5bdf\u7684\u5dee\u5f02\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u79cd\u8bc4\u4f30\u6a21\u578b\u8bef\u5dee\u7684\u65b9\u6cd5\u3002", "motivation": "\u8bc4\u4f30\u6a21\u578b\u5728\u51b3\u7b56\u4e2d\u7684\u4ef7\u503c\uff0c\u9700\u8981\u4f30\u8ba1\u53cd\u4e8b\u5b9e\u4e16\u754c\u4e2d\u7684\u6a21\u578b\u8bef\u5dee\u3002", "method": "\u63d0\u51fa\u5e76\u5bf9\u6bd4\u4e86\u4e09\u79cd\u4f30\u8ba1\u53cd\u4e8b\u5b9e\u8bef\u5dee\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u5b9e\u9a8c\u5c55\u793a\u4e86\u5b83\u4eec\u7684\u4f18\u7f3a\u70b9\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u5b9e\u9a8c\u5c55\u793a\u4e86\u4e09\u79cd\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u4e3a\u53cd\u4e8b\u5b9e\u8bef\u5dee\u7684\u4f30\u8ba1\u63d0\u4f9b\u4e86\u5efa\u8bae\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u60c5\u666f\u8bbe\u8ba1\u4e2d\u4f7f\u60c5\u666f\u9884\u6d4b\u53ef\u8bc4\u4f30\u7684\u8981\u7d20\uff0c\u5e76\u4e3a\u53cd\u4e8b\u5b9e\u8bef\u5dee\u7684\u4f30\u8ba1\u63d0\u4f9b\u4e86\u5efa\u8bae\u3002"}}
{"id": "2512.00590", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00590", "abs": "https://arxiv.org/abs/2512.00590", "authors": ["Alla Chepurova", "Aydar Bulatov", "Yuri Kuratov", "Mikhail Burtsev"], "title": "Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models", "comment": null, "summary": "Knowledge graphs (KGs) provide structured, verifiable grounding for large language models (LLMs), but current LLM-based systems commonly use KGs as auxiliary structures for text retrieval, leaving their intrinsic quality underexplored. In this work, we propose Wikontic, a multi-stage pipeline that constructs KGs from open-domain text by extracting candidate triplets with qualifiers, enforcing Wikidata-based type and relation constraints, and normalizing entities to reduce duplication. The resulting KGs are compact, ontology-consistent, and well-connected; on MuSiQue, the correct answer entity appears in 96% of generated triplets. On HotpotQA, our triplets-only setup achieves 76.0 F1, and on MuSiQue 59.8 F1, matching or surpassing several retrieval-augmented generation baselines that still require textual context. In addition, Wikontic attains state-of-the-art information-retention performance on the MINE-1 benchmark (86%), outperforming prior KG construction methods. Wikontic is also efficient at build time: KG construction uses less than 1,000 output tokens, about 3$\\times$ fewer than AriGraph and $<$1/20 of GraphRAG. The proposed pipeline enhances the quality of the generated KG and offers a scalable solution for leveraging structured knowledge in LLMs.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWikontic\u7684\u591a\u9636\u6bb5\u6d41\u7a0b\uff0c\u7528\u4e8e\u4ece\u5f00\u653e\u57df\u6587\u672c\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31(KG)\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\u901a\u5e38\u4f7f\u7528KG\u4f5c\u4e3a\u6587\u672c\u68c0\u7d22\u7684\u8f85\u52a9\u7ed3\u6784\uff0c\u5176\u5185\u5728\u8d28\u91cf\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "Wikontic\u901a\u8fc7\u63d0\u53d6\u5e26\u6709\u9650\u5b9a\u8bcd\u7684\u5019\u9009\u4e09\u5143\u7ec4\uff0c\u5b9e\u65bd\u57fa\u4e8eWikidata\u7684\u7c7b\u578b\u548c\u5173\u7cfb\u7ea6\u675f\uff0c\u5e76\u89c4\u8303\u5316\u5b9e\u4f53\u6765\u51cf\u5c11\u91cd\u590d\uff0c\u4ece\u800c\u6784\u5efaKG\u3002", "result": "\u751f\u6210\u7684KG\u7ed3\u6784\u7d27\u51d1\u3001\u672c\u4f53\u4e00\u81f4\u4e14\u8fde\u63a5\u826f\u597d\u3002\u5728MuSiQue\u4e0a\uff0c\u6b63\u786e\u7b54\u6848\u5b9e\u4f53\u51fa\u73b0\u572896%\u751f\u6210\u7684\u4e09\u5143\u7ec4\u4e2d\u3002\u5728HotpotQA\u4e0a\uff0c\u6211\u4eec\u7684\u4ec5\u4e09\u5143\u7ec4\u8bbe\u7f6e\u8fbe\u5230\u4e8676.0 F1\uff0c\u5728MuSiQue\u4e0a\u8fbe\u5230\u4e8659.8 F1\uff0c\u5339\u914d\u6216\u8d85\u8fc7\u4e86\u4e00\u4e9b\u4ecd\u7136\u9700\u8981\u6587\u672c\u4e0a\u4e0b\u6587\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u57fa\u7ebf\u3002\u6b64\u5916\uff0cWikontic\u5728MINE-1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u83b7\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u4fe1\u606f\u4fdd\u7559\u6027\u80fd(86%)\uff0c\u4f18\u4e8e\u4e4b\u524d\u7684KG\u6784\u5efa\u65b9\u6cd5\u3002Wikontic\u5728\u6784\u5efa\u65f6\u4e5f\u5f88\u9ad8\u6548\uff1aKG\u6784\u5efa\u4f7f\u7528\u5c11\u4e8e1,000\u4e2a\u8f93\u51fatokens\uff0c\u6bd4AriGraph\u5c11\u5927\u7ea63\u500d\uff0c\u6bd4GraphRAG\u5c11<1/20\u3002", "conclusion": "\u8be5\u6d41\u7a0b\u63d0\u9ad8\u4e86\u751f\u6210\u7684KG\u7684\u8d28\u91cf\uff0c\u5e76\u4e3a\u5229\u7528llm\u4e2d\u7684\u7ed3\u6784\u5316\u77e5\u8bc6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.00342", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.00342", "abs": "https://arxiv.org/abs/2512.00342", "authors": ["Haizheng Li", "Lei Guo"], "title": "Adaptive prediction theory combining offline and online learning", "comment": null, "summary": "Real-world intelligence systems usually operate by combining offline learning and online adaptation with highly correlated and non-stationary system data or signals, which, however, has rarely been investigated theoretically in the literature. This paper initiates a theoretical investigation on the prediction performance of a two-stage learning framework combining offline and online algorithms for a class of nonlinear stochastic dynamical systems. For the offline-learning phase, we establish an upper bound on the generalization error for approximate nonlinear-least-squares estimation under general datasets with strong correlation and distribution shift, leveraging the Kullback-Leibler divergence to quantify the distributional discrepancies. For the online-adaptation phase, we address, on the basis of the offline-trained model, the possible uncertain parameter drift in real-world target systems by proposing a meta-LMS prediction algorithm. This two-stage framework, integrating offline learning with online adaptation, demonstrates superior prediction performances compared with either purely offline or online methods. Both theoretical guarantees and empirical studies are provided.", "AI": {"tldr": "\u7814\u7a76\u4e86\u79bb\u7ebf\u5b66\u4e60\u548c\u5728\u7ebf\u9002\u5e94\u76f8\u7ed3\u5408\u7684\u4e24\u9636\u6bb5\u5b66\u4e60\u6846\u67b6\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u8be5\u6846\u67b6\u9002\u7528\u4e8e\u4e00\u7c7b\u975e\u7ebf\u6027\u968f\u673a\u52a8\u6001\u7cfb\u7edf\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u667a\u80fd\u7cfb\u7edf\u901a\u5e38\u901a\u8fc7\u7ed3\u5408\u79bb\u7ebf\u5b66\u4e60\u548c\u5728\u7ebf\u9002\u5e94\u6765\u8fd0\u884c\uff0c\u4f46\u73b0\u6709\u6587\u732e\u4e2d\u5f88\u5c11\u5bf9\u5176\u8fdb\u884c\u7406\u8bba\u7814\u7a76\u3002", "method": "\u5229\u7528Kullback-Leibler\u6563\u5ea6\u6765\u91cf\u5316\u5206\u5e03\u5dee\u5f02\uff0c\u5efa\u7acb\u4e86\u5728\u5f3a\u76f8\u5173\u548c\u5206\u5e03\u504f\u79fb\u7684\u4e00\u822c\u6570\u636e\u96c6\u4e0b\uff0c\u8fd1\u4f3c\u975e\u7ebf\u6027\u6700\u5c0f\u4e8c\u4e58\u4f30\u8ba1\u7684\u6cdb\u5316\u8bef\u5dee\u4e0a\u754c\u3002\u63d0\u51fa\u4e86\u57fa\u4e8e\u79bb\u7ebf\u8bad\u7ec3\u6a21\u578b\u7684\u5143LMS\u9884\u6d4b\u7b97\u6cd5\uff0c\u4ee5\u89e3\u51b3\u5b9e\u9645\u76ee\u6807\u7cfb\u7edf\u4e2d\u53ef\u80fd\u5b58\u5728\u7684\u4e0d\u786e\u5b9a\u53c2\u6570\u6f02\u79fb\u95ee\u9898\u3002", "result": "\u4e0e\u7eaf\u79bb\u7ebf\u6216\u5728\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u96c6\u6210\u4e86\u79bb\u7ebf\u5b66\u4e60\u548c\u5728\u7ebf\u9002\u5e94\u7684\u4e24\u9636\u6bb5\u6846\u67b6\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u8bc1\u7814\u7a76\u3002"}}
{"id": "2512.00194", "categories": ["cs.CV", "cs.LG", "eess.IV", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2512.00194", "abs": "https://arxiv.org/abs/2512.00194", "authors": ["Zag ElSayed", "Grace Westerkamp", "Gavin Gammoh", "Yanchen Liu", "Peyton Siekierski", "Craig Erickson", "Ernest Pedapati"], "title": "AutocleanEEG ICVision: Automated ICA Artifact Classification Using Vision-Language AI", "comment": "6 pages, 8 figures", "summary": "We introduce EEG Autoclean Vision Language AI (ICVision) a first-of-its-kind system that emulates expert-level EEG ICA component classification through AI-agent vision and natural language reasoning. Unlike conventional classifiers such as ICLabel, which rely on handcrafted features, ICVision directly interprets ICA dashboard visualizations topography, time series, power spectra, and ERP plots, using a multimodal large language model (GPT-4 Vision). This allows the AI to see and explain EEG components the way trained neurologists do, making it the first scientific implementation of AI-agent visual cognition in neurophysiology. ICVision classifies each component into one of six canonical categories (brain, eye, heart, muscle, channel noise, and other noise), returning both a confidence score and a human-like explanation. Evaluated on 3,168 ICA components from 124 EEG datasets, ICVision achieved k = 0.677 agreement with expert consensus, surpassing MNE ICLabel, while also preserving clinically relevant brain signals in ambiguous cases. Over 97% of its outputs were rated as interpretable and actionable by expert reviewers. As a core module of the open-source EEG Autoclean platform, ICVision signals a paradigm shift in scientific AI, where models do not just classify, but see, reason, and communicate. It opens the door to globally scalable, explainable, and reproducible EEG workflows, marking the emergence of AI agents capable of expert-level visual decision-making in brain science and beyond.", "AI": {"tldr": "ICVision\u662f\u4e00\u4e2a\u4f7f\u7528GPT-4 Vision\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u6a21\u4eff\u4e13\u5bb6\u5bf9\u8111\u7535\u56feICA\u6210\u5206\u7684\u5206\u7c7b\uff0c\u5b9e\u73b0\u4e86AI\u5728\u795e\u7ecf\u751f\u7406\u5b66\u4e2d\u7684\u89c6\u89c9\u8ba4\u77e5\u3002", "motivation": "\u4f20\u7edf\u5206\u7c7b\u5668\u4f9d\u8d56\u624b\u5de5\u7279\u5f81\uff0c\u800cICVision\u76f4\u63a5\u89e3\u91caICA\u4eea\u8868\u677f\u7684\u53ef\u89c6\u5316\u7ed3\u679c\uff0c\u66f4\u63a5\u8fd1\u795e\u7ecf\u79d1\u533b\u751f\u7684\u5206\u6790\u65b9\u5f0f\u3002", "method": "ICVision\u4f7f\u7528GPT-4 Vision\u76f4\u63a5\u5206\u6790ICA\u6210\u5206\u7684\u5730\u5f62\u56fe\u3001\u65f6\u95f4\u5e8f\u5217\u3001\u529f\u7387\u8c31\u548cERP\u56fe\uff0c\u5e76\u5c06\u5176\u5206\u7c7b\u4e3a\u516d\u79cd\u7c7b\u522b\u3002", "result": "ICVision\u57283168\u4e2aICA\u6210\u5206\u7684\u8bc4\u4f30\u4e2d\uff0c\u4e0e\u4e13\u5bb6\u5171\u8bc6\u8fbe\u6210k=0.677\u7684\u534f\u8bae\uff0c\u8d85\u8fc7\u4e86MNE ICLabel\uff0c\u5e76\u4e14\u5728\u6a21\u7cca\u60c5\u51b5\u4e0b\u4fdd\u7559\u4e86\u4e34\u5e8a\u76f8\u5173\u7684\u5927\u8111\u4fe1\u53f7\u3002\u8d85\u8fc797%\u7684\u8f93\u51fa\u88ab\u4e13\u5bb6\u8bc4\u4e3a\u53ef\u89e3\u91ca\u548c\u53ef\u64cd\u4f5c\u3002", "conclusion": "ICVision\u4f5c\u4e3a\u5f00\u6e90EEG Autoclean\u5e73\u53f0\u7684\u6838\u5fc3\u6a21\u5757\uff0c\u6807\u5fd7\u7740\u79d1\u5b66AI\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u5f00\u542f\u4e86\u5168\u7403\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u91cd\u590d\u7684\u8111\u7535\u5de5\u4f5c\u6d41\u7a0b\u3002"}}
{"id": "2512.00839", "categories": ["cs.AI", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.00839", "abs": "https://arxiv.org/abs/2512.00839", "authors": ["Fabrizio Maturo", "Donato Riccio", "Andrea Mazzitelli", "Giuseppe Bifulco", "Francesco Paolone", "Iulia Brezeanu"], "title": "ARCADIA: Scalable Causal Discovery for Corporate Bankruptcy Analysis Using Agentic AI", "comment": "35 pages, 9 figures, 4 tables", "summary": "This paper introduces ARCADIA, an agentic AI framework for causal discovery that integrates large-language-model reasoning with statistical diagnostics to construct valid, temporally coherent causal structures. Unlike traditional algorithms, ARCADIA iteratively refines candidate DAGs through constraint-guided prompting and causal-validity feedback, leading to stable and interpretable models for real-world high-stakes domains. Experiments on corporate bankruptcy data show that ARCADIA produces more reliable causal graphs than NOTEARS, GOLEM, and DirectLiNGAM while offering a fully explainable, intervention-ready pipeline. The framework advances AI by demonstrating how agentic LLMs can participate in autonomous scientific modeling and structured causal inference.", "AI": {"tldr": "ARCADIA: Agentic AI\u6846\u67b6\uff0c\u7528\u4e8e\u56e0\u679c\u53d1\u73b0\uff0c\u96c6\u6210\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u548c\u7edf\u8ba1\u8bca\u65ad\uff0c\u4ee5\u6784\u5efa\u6709\u6548\u7684\u3001\u65f6\u95f4\u4e0a\u8fde\u8d2f\u7684\u56e0\u679c\u7ed3\u6784\u3002", "motivation": "\u4e0e\u4f20\u7edf\u7b97\u6cd5\u4e0d\u540c\uff0cARCADIA\u901a\u8fc7\u7ea6\u675f\u5f15\u5bfc\u63d0\u793a\u548c\u56e0\u679c\u6709\u6548\u6027\u53cd\u9988\u8fed\u4ee3\u5730\u6539\u8fdb\u5019\u9009DAG\uff0c\u4ece\u800c\u4e3a\u73b0\u5b9e\u4e16\u754c\u7684\u9ad8\u98ce\u9669\u9886\u57df\u751f\u6210\u7a33\u5b9a\u4e14\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u3002", "method": "ARCADIA\u901a\u8fc7\u7ea6\u675f\u5f15\u5bfc\u63d0\u793a\u548c\u56e0\u679c\u6709\u6548\u6027\u53cd\u9988\u8fed\u4ee3\u5730\u6539\u8fdb\u5019\u9009DAG\u3002", "result": "\u5728\u516c\u53f8\u7834\u4ea7\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cARCADIA\u6bd4NOTEARS\u3001GOLEM\u548cDirectLiNGAM\u4ea7\u751f\u66f4\u53ef\u9760\u7684\u56e0\u679c\u56fe\uff0c\u540c\u65f6\u63d0\u4f9b\u5b8c\u5168\u53ef\u89e3\u91ca\u7684\u3001\u53ef\u5e72\u9884\u7684\u7ba1\u9053\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5c55\u793aAgentic LLM\u5982\u4f55\u53c2\u4e0e\u81ea\u4e3b\u79d1\u5b66\u5efa\u6a21\u548c\u7ed3\u6784\u5316\u56e0\u679c\u63a8\u7406\u6765\u63a8\u8fdbAI\u3002"}}
{"id": "2512.00611", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00611", "abs": "https://arxiv.org/abs/2512.00611", "authors": ["Franck Binard", "Vanja Kljajevic"], "title": "Prism: A Minimal Compositional Metalanguage for Specifying Agent Behavior", "comment": null, "summary": "Prism is a small, compositional metalanguage for specifying the behaviour of tool-using software agents. Rather than introducing ad hoc control constructs, Prism is built around a fixed core context, Core1, which provides a minimal background grammar of categories numbers, strings, user prompts, tools together with abstract combinators for booleans, predicates, pairs, and lists. Agent policies are written as ordinary expressions using a single abstraction operator so that conditionals appear as selections between alternatives instead of imperative if-else blocks. Domains extend the core by defining their own context-mini-grammars that introduce new categories, predicates, and external tools while reusing the same compositional machinery. We illustrate this with worked examples from thermostat control, home security, e-commerce recommendation, and medical monitoring, showing how natural language decision rules can be mapped to inspectable, executable policies. From a linguistic perspective, Prism enforces a clear separation between a reusable grammar-like core and domain specific lexicons and treats tools as bridges between internal policy representations and the external world. From an engineering perspective, it offers a compact interface language for agent control, making the space of possible actions explicit and amenable to analysis, verification, and safety constraints.", "AI": {"tldr": "Prism\u662f\u4e00\u4e2a\u7528\u4e8e\u6307\u5b9a\u5de5\u5177\u4f7f\u7528\u8f6f\u4ef6\u4ee3\u7406\u884c\u4e3a\u7684\u5c0f\u578b\u3001\u53ef\u7ec4\u5408\u7684\u5143\u8bed\u8a00\u3002", "motivation": "\u73b0\u6709\u7684\u63a7\u5236\u7ed3\u6784\u662f\u4e34\u65f6\u7684\uff0cPrism\u56f4\u7ed5\u4e00\u4e2a\u56fa\u5b9a\u7684\u6838\u5fc3\u4e0a\u4e0b\u6587\u6784\u5efa\uff0c\u8be5\u4e0a\u4e0b\u6587\u63d0\u4f9b\u4e86\u7c7b\u522b\u3001\u6570\u5b57\u3001\u5b57\u7b26\u4e32\u3001\u7528\u6237\u63d0\u793a\u3001\u5de5\u5177\u4ee5\u53ca\u5e03\u5c14\u503c\u3001\u8c13\u8bcd\u3001\u5bf9\u548c\u5217\u8868\u7684\u62bd\u8c61\u7ec4\u5408\u5668\u7684\u6700\u5c0f\u80cc\u666f\u8bed\u6cd5\u3002", "method": "Agent\u7b56\u7565\u88ab\u7f16\u5199\u4e3a\u666e\u901a\u8868\u8fbe\u5f0f\uff0c\u4f7f\u7528\u5355\u4e2a\u62bd\u8c61\u8fd0\u7b97\u7b26\uff0c\u4ee5\u4fbf\u6761\u4ef6\u663e\u793a\u4e3a\u66ff\u4ee3\u65b9\u6848\u4e4b\u95f4\u7684\u9009\u62e9\uff0c\u800c\u4e0d\u662f\u547d\u4ee4\u5f0fif-else\u5757\u3002\u57df\u901a\u8fc7\u5b9a\u4e49\u81ea\u5df1\u7684\u4e0a\u4e0b\u6587-\u8ff7\u4f60\u8bed\u6cd5\u6765\u6269\u5c55\u6838\u5fc3\uff0c\u8fd9\u4e9b\u8bed\u6cd5\u5f15\u5165\u4e86\u65b0\u7684\u7c7b\u522b\u3001\u8c13\u8bcd\u548c\u5916\u90e8\u5de5\u5177\uff0c\u540c\u65f6\u91cd\u7528\u76f8\u540c\u7684\u7ec4\u5408\u673a\u5236\u3002", "result": "\u901a\u8fc7\u6765\u81ea\u6052\u6e29\u5668\u63a7\u5236\u3001\u5bb6\u5ead\u5b89\u5168\u3001\u7535\u5b50\u5546\u52a1\u63a8\u8350\u548c\u533b\u7597\u76d1\u63a7\u7684\u5de5\u4f5c\u793a\u4f8b\u6765\u8bf4\u660e\u8fd9\u4e00\u70b9\uff0c\u5c55\u793a\u4e86\u81ea\u7136\u8bed\u8a00\u51b3\u7b56\u89c4\u5219\u5982\u4f55\u6620\u5c04\u5230\u53ef\u68c0\u67e5\u3001\u53ef\u6267\u884c\u7684\u7b56\u7565\u3002", "conclusion": "\u4ece\u8bed\u8a00\u5b66\u7684\u89d2\u5ea6\u6765\u770b\uff0cPrism\u5f3a\u5236\u6267\u884c\u53ef\u91cd\u7528\u7684\u7c7b\u4f3c\u8bed\u6cd5\u7684\u6838\u5fc3\u548c\u7279\u5b9a\u4e8e\u9886\u57df\u7684\u8bcd\u5178\u4e4b\u95f4\u7684\u6e05\u6670\u5206\u79bb\uff0c\u5e76\u5c06\u5de5\u5177\u89c6\u4e3a\u5185\u90e8\u7b56\u7565\u8868\u793a\u548c\u5916\u90e8\u4e16\u754c\u4e4b\u95f4\u7684\u6865\u6881\u3002\u4ece\u5de5\u7a0b\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u5b83\u4e3a\u4ee3\u7406\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7d27\u51d1\u7684\u63a5\u53e3\u8bed\u8a00\uff0c\u4f7f\u53ef\u80fd\u7684\u64cd\u4f5c\u7a7a\u95f4\u663e\u5f0f\u4e14\u6613\u4e8e\u5206\u6790\u3001\u9a8c\u8bc1\u548c\u5b89\u5168\u7ea6\u675f\u3002"}}
{"id": "2512.00351", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.00351", "abs": "https://arxiv.org/abs/2512.00351", "authors": ["Na Li", "Yuchen Jiao", "Hangguan Shan", "Shefeng Yan"], "title": "Provable Memory Efficient Self-Play Algorithm for Model-free Reinforcement Learning", "comment": "ICLR 2024. arXiv admin note: substantial text overlap with arXiv:2110.04645 by other authors", "summary": "The thriving field of multi-agent reinforcement learning (MARL) studies how a group of interacting agents make decisions autonomously in a shared dynamic environment. Existing theoretical studies in this area suffer from at least two of the following obstacles: memory inefficiency, the heavy dependence of sample complexity on the long horizon and the large state space, the high computational complexity, non-Markov policy, non-Nash policy, and high burn-in cost. In this work, we take a step towards settling this problem by designing a model-free self-play algorithm \\emph{Memory-Efficient Nash Q-Learning (ME-Nash-QL)} for two-player zero-sum Markov games, which is a specific setting of MARL. ME-Nash-QL is proven to enjoy the following merits. First, it can output an $\\varepsilon$-approximate Nash policy with space complexity $O(SABH)$ and sample complexity $\\widetilde{O}(H^4SAB/\\varepsilon^2)$, where $S$ is the number of states, $\\{A, B\\}$ is the number of actions for two players, and $H$ is the horizon length. It outperforms existing algorithms in terms of space complexity for tabular cases, and in terms of sample complexity for long horizons, i.e., when $\\min\\{A, B\\}\\ll H^2$. Second, ME-Nash-QL achieves the lowest computational complexity $O(T\\mathrm{poly}(AB))$ while preserving Markov policies, where $T$ is the number of samples. Third, ME-Nash-QL also achieves the best burn-in cost $O(SAB\\,\\mathrm{poly}(H))$, whereas previous algorithms have a burn-in cost of at least $O(S^3 AB\\,\\mathrm{poly}(H))$ to attain the same level of sample complexity with ours.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u53cc\u4eba\u96f6\u548c\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u7684\u65e0\u6a21\u578b\u81ea\u535a\u5f08\u7b97\u6cd5\uff0c\u79f0\u4e3a Memory-Efficient Nash Q-Learning (ME-Nash-QL)\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60 (MARL) \u7406\u8bba\u7814\u7a76\u5b58\u5728\u5185\u5b58\u6548\u7387\u4f4e\u3001\u6837\u672c\u590d\u6742\u5ea6\u5bf9\u957f\u65f6\u7a0b\u548c\u5927\u578b\u72b6\u6001\u7a7a\u95f4\u7684\u4f9d\u8d56\u6027\u5f3a\u3001\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u975e\u9a6c\u5c14\u53ef\u592b\u7b56\u7565\u3001\u975e\u7eb3\u4ec0\u7b56\u7565\u4ee5\u53ca\u9ad8\u9884\u70ed\u6210\u672c\u7b49\u95ee\u9898\u3002", "method": "\u8be5\u8bba\u6587\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u81ea\u535a\u5f08\u7b97\u6cd5 ME-Nash-QL\uff0c\u7528\u4e8e\u89e3\u51b3\u53cc\u4eba\u96f6\u548c\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u95ee\u9898\u3002", "result": "ME-Nash-QL \u7b97\u6cd5\u5728\u7a7a\u95f4\u590d\u6742\u5ea6\u3001\u6837\u672c\u590d\u6742\u5ea6\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\uff0c\u5e76\u4e14\u5b9e\u73b0\u4e86\u6700\u4f73\u7684\u9884\u70ed\u6210\u672c\u3002", "conclusion": "ME-Nash-QL \u7b97\u6cd5\u5728\u53cc\u4eba\u96f6\u548c\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u4e2d\u5177\u6709\u663e\u8457\u7684\u4f18\u52bf\uff0c\u4e3a\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2512.00198", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00198", "abs": "https://arxiv.org/abs/2512.00198", "authors": ["Shantanu Ghosh", "Vedant Parthesh Joshi", "Rayan Syed", "Aya Kassem", "Abhishek Varshney", "Payel Basak", "Weicheng Dai", "Judy Wawira Gichoya", "Hari M. Trivedi", "Imon Banerjee", "Shyam Visweswaran", "Clare B. Poynton", "Kayhan Batmanghelich"], "title": "Mammo-FM: Breast-specific foundational model for Integrated Mammographic Diagnosis, Prognosis, and Reporting", "comment": null, "summary": "Breast cancer is one of the leading causes of death among women worldwide. We introduce Mammo-FM, the first foundation model specifically for mammography, pretrained on the largest and most diverse dataset to date - 140,677 patients (821,326 mammograms) across four U.S. institutions. Mammo-FM provides a unified foundation for core clinical tasks in breast imaging, including cancer diagnosis, pathology localization, structured report generation, and cancer risk prognosis within a single framework. Its alignment between images and text enables both visual and textual interpretability, improving transparency and clinical auditability, which are essential for real-world adoption. We rigorously evaluate Mammo-FM across diagnosis, prognosis, and report-generation tasks in in- and out-of-distribution datasets. Despite operating on native-resolution mammograms and using only one-third of the parameters of state-of-the-art generalist FMs, Mammo-FM consistently outperforms them across multiple public and private benchmarks. These results highlight the efficiency and value of domain-specific foundation models designed around the full spectrum of tasks within a clinical domain and emphasize the importance of rigorous, domain-aligned evaluation.", "AI": {"tldr": "Mammo-FM: a foundation model for mammography, pretrained on a large dataset, excels in various breast imaging tasks.", "motivation": "Breast cancer is a leading cause of death in women; the model aims to improve clinical tasks in breast imaging.", "method": "A foundation model (Mammo-FM) pretrained on a large mammography dataset.", "result": "Mammo-FM outperforms generalist models in diagnosis, prognosis, and report generation tasks.", "conclusion": "Domain-specific foundation models are efficient and valuable for clinical tasks."}}
{"id": "2512.00852", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00852", "abs": "https://arxiv.org/abs/2512.00852", "authors": ["Yandong Sun", "Qiang Huang", "Ziwei Xu", "Yiqun Sun", "Yixuan Tang", "Anthony K. H. Tung"], "title": "One Swallow Does Not Make a Summer: Understanding Semantic Structures in Embedding Spaces", "comment": null, "summary": "Embedding spaces are fundamental to modern AI, translating raw data into high-dimensional vectors that encode rich semantic relationships. Yet, their internal structures remain opaque, with existing approaches often sacrificing semantic coherence for structural regularity or incurring high computational overhead to improve interpretability. To address these challenges, we introduce the Semantic Field Subspace (SFS), a geometry-preserving, context-aware representation that captures local semantic neighborhoods within the embedding space. We also propose SAFARI (SemAntic Field subspAce deteRmInation), an unsupervised, modality-agnostic algorithm that uncovers hierarchical semantic structures using a novel metric called Semantic Shift, which quantifies how semantics evolve as SFSes evolve. To ensure scalability, we develop an efficient approximation of Semantic Shift that replaces costly SVD computations, achieving a 15~30x speedup with average errors below 0.01. Extensive evaluations across six real-world text and image datasets show that SFSes outperform standard classifiers not only in classification but also in nuanced tasks such as political bias detection, while SAFARI consistently reveals interpretable and generalizable semantic hierarchies. This work presents a unified framework for structuring, analyzing, and scaling semantic understanding in embedding spaces.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5d4c\u5165\u7a7a\u95f4\u8868\u793a\u65b9\u6cd5\uff0c\u79f0\u4e3a\u8bed\u4e49\u573a\u5b50\u7a7a\u95f4\uff08SFS\uff09\uff0c\u5b83\u80fd\u4fdd\u7559\u51e0\u4f55\u7ed3\u6784\u5e76\u611f\u77e5\u4e0a\u4e0b\u6587\uff0c\u4ee5\u6355\u6349\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u5c40\u90e8\u8bed\u4e49\u90bb\u57df\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u727a\u7272\u8bed\u4e49\u8fde\u8d2f\u6027\u4ee5\u6362\u53d6\u7ed3\u6784\u89c4\u5f8b\u6027\uff0c\u6216\u8005\u4ea7\u751f\u9ad8\u8ba1\u7b97\u5f00\u9500\u4ee5\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u3001\u6a21\u6001\u65e0\u5173\u7684\u7b97\u6cd5\uff0c\u79f0\u4e3aSAFARI\uff0c\u5b83\u4f7f\u7528\u4e00\u79cd\u540d\u4e3a\u8bed\u4e49\u6f02\u79fb\u7684\u65b0\u6307\u6807\u6765\u63ed\u793a\u5206\u5c42\u8bed\u4e49\u7ed3\u6784\uff0c\u8be5\u6307\u6807\u91cf\u5316\u4e86\u8bed\u4e49\u968fSFS\u6f14\u53d8\u7684\u65b9\u5f0f\u3002", "result": "\u5728\u516d\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u6587\u672c\u548c\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u8bc4\u4f30\u8868\u660e\uff0cSFS\u5728\u5206\u7c7b\u548c\u7ec6\u5fae\u7684\u4efb\u52a1\uff08\u5982\u653f\u6cbb\u504f\u89c1\u68c0\u6d4b\uff09\u4e2d\u4f18\u4e8e\u6807\u51c6\u5206\u7c7b\u5668\uff0c\u800cSAFARI\u59cb\u7ec8\u63ed\u793a\u53ef\u89e3\u91ca\u548c\u53ef\u63a8\u5e7f\u7684\u8bed\u4e49\u5c42\u6b21\u7ed3\u6784\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u3001\u5206\u6790\u548c\u6269\u5c55\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u8bed\u4e49\u7406\u89e3\u3002"}}
{"id": "2512.00617", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00617", "abs": "https://arxiv.org/abs/2512.00617", "authors": ["Omer Jauhar Khan"], "title": "ART: Adaptive Response Tuning Framework -- A Multi-Agent Tournament-Based Approach to LLM Response Optimization", "comment": "8 pages, 5 figures, 5 tables. Conference-style paper", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation. However, single-model responses often exhibit inconsistencies, hallucinations, and varying quality across different query domains. This paper presents ART (Adaptive Response Tuning), a novel framework that employs tournament-style ELO ranking and multi-agent reasoning to systematically optimize LLM outputs. By enabling multiple LLM agents to compete, critique, and collaborate through structured tournament workflows, ART produces consensus responses that outperform individual model outputs. Our framework introduces configurable tournament parameters, dynamic agent selection, and multiple consensus fusion strategies. Experimental evaluations demonstrate significant improvements in response accuracy, coherence, and reliability compared to baseline single-model approaches. The ART framework provides a scalable, production-ready solution for applications requiring high-quality, vetted LLM responses, achieving an 8.4% improvement in overall quality metrics and R22 values exceeding 0.96 in ELO rating convergence.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a ART (Adaptive Response Tuning) \u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u9526\u6807\u8d5b\u5f0f ELO \u6392\u540d\u548c\u591a\u667a\u80fd\u4f53\u63a8\u7406\u6765\u7cfb\u7edf\u5730\u4f18\u5316 LLM \u8f93\u51fa\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u6a21\u578b LLM \u56de\u590d\u901a\u5e38\u8868\u73b0\u51fa\u4e0d\u4e00\u81f4\u3001\u5e7b\u89c9\u4ee5\u53ca\u4e0d\u540c\u67e5\u8be2\u57df\u4e4b\u95f4\u7684\u8d28\u91cf\u5dee\u5f02\u3002", "method": "ART \u6846\u67b6\u5141\u8bb8\u591a\u4e2a LLM \u667a\u80fd\u4f53\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u9526\u6807\u8d5b\u5de5\u4f5c\u6d41\u7a0b\u8fdb\u884c\u7ade\u4e89\u3001\u8bc4\u8bba\u548c\u534f\u4f5c\uff0c\u4ece\u800c\u751f\u6210\u5171\u8bc6\u56de\u590d\uff0c\u5e76\u4e14\u5f15\u5165\u53ef\u914d\u7f6e\u7684\u9526\u6807\u8d5b\u53c2\u6570\u3001\u52a8\u6001\u667a\u80fd\u4f53\u9009\u62e9\u548c\u591a\u79cd\u5171\u8bc6\u878d\u5408\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u5355\u6a21\u578b\u65b9\u6cd5\u76f8\u6bd4\uff0c\u54cd\u5e94\u51c6\u786e\u6027\u3001\u8fde\u8d2f\u6027\u548c\u53ef\u9760\u6027\u5f97\u5230\u4e86\u663e\u7740\u63d0\u9ad8\u3002\u6574\u4f53\u8d28\u91cf\u6307\u6807\u63d0\u9ad8\u4e86 8.4%\uff0c\u5e76\u4e14 ELO \u8bc4\u7ea7\u6536\u655b\u4e2d\u7684 R22 \u503c\u8d85\u8fc7 0.96\u3002", "conclusion": "ART \u6846\u67b6\u4e3a\u9700\u8981\u9ad8\u8d28\u91cf\u3001\u7ecf\u8fc7\u5ba1\u6838\u7684 LLM \u56de\u590d\u7684\u5e94\u7528\u7a0b\u5e8f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u3001\u53ef\u7528\u4e8e\u751f\u4ea7\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.00352", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.00352", "abs": "https://arxiv.org/abs/2512.00352", "authors": ["Na Li", "Zewu Zheng", "Wei Ni", "Hangguan Shan", "Wenjie Zhang", "Xinyu Li"], "title": "Sample-Efficient Tabular Self-Play for Offline Robust Reinforcement Learning", "comment": "NeurIPS 2025", "summary": "Multi-agent reinforcement learning (MARL), as a thriving field, explores how multiple agents independently make decisions in a shared dynamic environment. Due to environmental uncertainties, policies in MARL must remain robust to tackle the sim-to-real gap. We focus on robust two-player zero-sum Markov games (TZMGs) in offline settings, specifically on tabular robust TZMGs (RTZMGs). We propose a model-based algorithm (\\textit{RTZ-VI-LCB}) for offline RTZMGs, which is optimistic robust value iteration combined with a data-driven Bernstein-style penalty term for robust value estimation. By accounting for distribution shifts in the historical dataset, the proposed algorithm establishes near-optimal sample complexity guarantees under partial coverage and environmental uncertainty. An information-theoretic lower bound is developed to confirm the tightness of our algorithm's sample complexity, which is optimal regarding both state and action spaces. To the best of our knowledge, RTZ-VI-LCB is the first to attain this optimality, sets a new benchmark for offline RTZMGs, and is validated experimentally.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u79bb\u7ebf\u9c81\u68d2\u53cc\u4eba\u96f6\u548c\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\uff08RTZMGs\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRTZ-VI-LCB\u7684\u57fa\u4e8e\u6a21\u578b\u7684\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5728\u90e8\u5206\u8986\u76d6\u548c\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u4e0b\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u6837\u672c\u590d\u6742\u5ea6\u4fdd\u8bc1\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u9700\u8981\u5728\u5171\u4eab\u52a8\u6001\u73af\u5883\u4e2d\u5904\u7406\u73af\u5883\u4e0d\u786e\u5b9a\u6027\uff0c\u7b56\u7565\u5fc5\u987b\u4fdd\u6301\u9c81\u68d2\u6027\u4ee5\u5e94\u5bf9sim-to-real\u7684\u5dee\u8ddd\u3002\u672c\u6587\u5173\u6ce8\u79bb\u7ebf\u8bbe\u7f6e\u4e2d\u7684\u9c81\u68d2\u53cc\u4eba\u96f6\u548c\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRTZ-VI-LCB\u7684\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u662f\u4e50\u89c2\u9c81\u68d2\u503c\u8fed\u4ee3\u4e0e\u6570\u636e\u9a71\u52a8\u7684Bernstein-style\u60e9\u7f5a\u9879\u76f8\u7ed3\u5408\uff0c\u7528\u4e8e\u9c81\u68d2\u503c\u4f30\u8ba1\u3002", "result": "\u8be5\u7b97\u6cd5\u5728\u90e8\u5206\u8986\u76d6\u548c\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u4e0b\uff0c\u5efa\u7acb\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u6837\u672c\u590d\u6742\u5ea6\u4fdd\u8bc1\u3002\u5f00\u53d1\u4e86\u4e00\u4e2a\u4fe1\u606f\u8bba\u4e0b\u754c\uff0c\u4ee5\u786e\u8ba4\u7b97\u6cd5\u6837\u672c\u590d\u6742\u5ea6\u7684\u7d27\u5bc6\u6027\uff0c\u8be5\u590d\u6742\u5ea6\u5728\u72b6\u6001\u548c\u884c\u52a8\u7a7a\u95f4\u65b9\u9762\u90fd\u662f\u6700\u4f18\u7684\u3002", "conclusion": "RTZ-VI-LCB\u662f\u7b2c\u4e00\u4e2a\u8fbe\u5230\u8fd9\u79cd\u6700\u4f18\u6027\u7684\u7b97\u6cd5\uff0c\u4e3a\u79bb\u7ebfRTZMGs\u8bbe\u7f6e\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2512.00208", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00208", "abs": "https://arxiv.org/abs/2512.00208", "authors": ["Hajra Anwar Beg", "Baptiste Chopin", "Hao Tang", "Mohamed Daoudi"], "title": "ReactionMamba: Generating Short &Long Human Reaction Sequences", "comment": null, "summary": "We present ReactionMamba, a novel framework for generating long 3D human reaction motions. Reaction-Mamba integrates a motion VAE for efficient motion encoding with Mamba-based state-space models to decode temporally consistent reactions. This design enables ReactionMamba to generate both short sequences of simple motions and long sequences of complex motions, such as dance and martial arts. We evaluate ReactionMamba on three datasets--NTU120-AS, Lindy Hop, and InterX--and demonstrate competitive performance in terms of realism, diversity, and long-sequence generation compared to previous methods, including InterFormer, ReMoS, and Ready-to-React, while achieving substantial improvements in inference speed.", "AI": {"tldr": "ReactionMamba\u662f\u4e00\u4e2a\u7528\u4e8e\u751f\u6210\u957f3D\u4eba\u4f53\u53cd\u5e94\u52a8\u4f5c\u7684\u65b0\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86\u8fd0\u52a8VAE\u548c\u57fa\u4e8eMamba\u7684\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u3002", "motivation": "\u751f\u6210\u903c\u771f\u3001\u591a\u6837\u4e14\u65f6\u95f4\u4e0a\u8fde\u8d2f\u76843D\u4eba\u4f53\u53cd\u5e94\u52a8\u4f5c\uff0c\u5c24\u5176\u662f\u5728\u957f\u5e8f\u5217\u548c\u590d\u6742\u52a8\u4f5c\u65b9\u9762\u3002", "method": "ReactionMamba\u96c6\u6210\u4e86\u8fd0\u52a8VAE\u7528\u4e8e\u9ad8\u6548\u8fd0\u52a8\u7f16\u7801\uff0c\u4ee5\u53ca\u57fa\u4e8eMamba\u7684\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7528\u4e8e\u89e3\u7801\u65f6\u95f4\u4e0a\u4e00\u81f4\u7684\u53cd\u5e94\u3002", "result": "\u5728NTU120-AS\u3001Lindy Hop\u548cInterX\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cReactionMamba\u5728\u771f\u5b9e\u6027\u3001\u591a\u6837\u6027\u548c\u957f\u5e8f\u5217\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u4e0eInterFormer\u3001ReMoS\u548cReady-to-React\u7b49\u5148\u524d\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u5728\u63a8\u7406\u901f\u5ea6\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "ReactionMamba\u5728\u751f\u6210\u957f3D\u4eba\u4f53\u53cd\u5e94\u52a8\u4f5c\u65b9\u9762\u5177\u6709\u7ade\u4e89\u529b\uff0c\u5e76\u5728\u63a8\u7406\u901f\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2512.00881", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00881", "abs": "https://arxiv.org/abs/2512.00881", "authors": ["Li Yuan", "Qingfei Huang", "Bingshan Zhu", "Yi Cai", "Qingbao Huang", "Changmeng Zheng", "Zikun Deng", "Tao Wang"], "title": "Hybrid-DMKG: A Hybrid Reasoning Framework over Dynamic Multimodal Knowledge Graphs for Multimodal Multihop QA with Knowledge Editing", "comment": "Accepted by AAAI 2026", "summary": "Multimodal Knowledge Editing (MKE) extends traditional knowledge editing to settings involving both textual and visual modalities. However, existing MKE benchmarks primarily assess final answer correctness while neglecting the quality of intermediate reasoning and robustness to visually rephrased inputs. To address this limitation, we introduce MMQAKE, the first benchmark for multimodal multihop question answering with knowledge editing. MMQAKE evaluates (1) a model's ability to reason over 2-5-hop factual chains that span both text and images, including performance at each intermediate step, and (2) robustness to visually rephrased inputs in multihop questions. Our evaluation shows that current MKE methods often struggle to consistently update and reason over multimodal reasoning chains after knowledge edits. To overcome these challenges, we propose Hybrid-DMKG, a hybrid reasoning framework built on a dynamic multimodal knowledge graph (DMKG) to enable accurate multihop reasoning over updated multimodal knowledge. Hybrid-DMKG first uses a large language model to decompose multimodal multihop questions into sequential sub-questions, then applies a multimodal retrieval model to locate updated facts by jointly encoding each sub-question with candidate entities and their associated images. For answer inference, a hybrid reasoning module operates over the DMKG via two parallel paths: (1) relation linking prediction, and (2) RAG reasoning with large vision-language models. A decision module aggregates evidence from both paths to select the most credible answer. Experimental results on MMQAKE show that Hybrid-DMKG significantly outperforms existing MKE approaches, achieving higher accuracy and improved robustness to knowledge updates.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u591a\u6a21\u6001\u77e5\u8bc6\u7f16\u8f91\uff08MKE\uff09\u57fa\u51c6\u6d4b\u8bd5MMQAKE\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5728\u591a\u8df3\u95ee\u7b54\u4e2d\u8fdb\u884c\u63a8\u7406\u548c\u5904\u7406\u89c6\u89c9\u91cd\u8ff0\u8f93\u5165\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684MKE\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u8bc4\u4f30\u6700\u7ec8\u7b54\u6848\u7684\u6b63\u786e\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u4e2d\u95f4\u63a8\u7406\u7684\u8d28\u91cf\u548c\u5bf9\u89c6\u89c9\u91cd\u8ff0\u8f93\u5165\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6df7\u5408\u63a8\u7406\u6846\u67b6Hybrid-DMKG\uff0c\u8be5\u6846\u67b6\u6784\u5efa\u5728\u52a8\u6001\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\uff08DMKG\uff09\u4e0a\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u66f4\u65b0\u7684\u591a\u6a21\u6001\u77e5\u8bc6\u7684\u51c6\u786e\u591a\u8df3\u63a8\u7406\u3002Hybrid-DMKG\u9996\u5148\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5c06\u591a\u6a21\u6001\u591a\u8df3\u95ee\u9898\u5206\u89e3\u4e3a\u987a\u5e8f\u5b50\u95ee\u9898\uff0c\u7136\u540e\u5e94\u7528\u591a\u6a21\u6001\u68c0\u7d22\u6a21\u578b\uff0c\u901a\u8fc7\u8054\u5408\u7f16\u7801\u6bcf\u4e2a\u5b50\u95ee\u9898\u4e0e\u5019\u9009\u5b9e\u4f53\u53ca\u5176\u76f8\u5173\u56fe\u50cf\u6765\u5b9a\u4f4d\u66f4\u65b0\u7684\u4e8b\u5b9e\u3002", "result": "\u5728MMQAKE\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHybrid-DMKG\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684MKE\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u5bf9\u77e5\u8bc6\u66f4\u65b0\u7684\u6539\u8fdb\u9c81\u68d2\u6027\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86MMQAKE\u57fa\u51c6\u6d4b\u8bd5\u548cHybrid-DMKG\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709MKE\u65b9\u6cd5\u5728\u591a\u8df3\u63a8\u7406\u548c\u89c6\u89c9\u91cd\u8ff0\u8f93\u5165\u65b9\u9762\u7684\u4e0d\u8db3\u3002"}}
{"id": "2512.00656", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.00656", "abs": "https://arxiv.org/abs/2512.00656", "authors": ["Jan Batzner", "Volker Stocker", "Stefan Schmid", "Gjergji Kasneci"], "title": "Sycophancy Claims about Language Models: The Missing Human-in-the-Loop", "comment": "NeurIPS 2025 Workshop on LLM Evaluation and ICLR 2025 Workshop on Bi-Directional Human-AI Alignment", "summary": "Sycophantic response patterns in Large Language Models (LLMs) have been increasingly claimed in the literature. We review methodological challenges in measuring LLM sycophancy and identify five core operationalizations. Despite sycophancy being inherently human-centric, current research does not evaluate human perception. Our analysis highlights the difficulties in distinguishing sycophantic responses from related concepts in AI alignment and offers actionable recommendations for future research.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4e2d\u5949\u627f\u53cd\u5e94\u6a21\u5f0f\u7684\u6d4b\u91cf\u65b9\u6cd5\uff0c\u5e76\u6307\u51fa\u4e86\u5176\u4e2d\u5b58\u5728\u7684\u6311\u6218\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u672a\u80fd\u8bc4\u4f30\u4eba\u7c7b\u5bf9 LLM \u5949\u627f\u884c\u4e3a\u7684\u611f\u77e5\u3002", "method": "\u672c\u6587\u56de\u987e\u4e86\u6d4b\u91cf LLM \u5949\u627f\u884c\u4e3a\u7684\u65b9\u6cd5\uff0c\u5e76\u786e\u5b9a\u4e86\u4e94\u4e2a\u6838\u5fc3\u64cd\u4f5c\u3002", "result": "\u5206\u6790\u5f3a\u8c03\u4e86\u533a\u5206\u5949\u627f\u53cd\u5e94\u4e0e AI \u5bf9\u9f50\u4e2d\u76f8\u5173\u6982\u5ff5\u7684\u96be\u5ea6\u3002", "conclusion": "\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u5efa\u8bae\u3002"}}
{"id": "2512.00357", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.00357", "abs": "https://arxiv.org/abs/2512.00357", "authors": ["Na Li", "Hangguan Shan", "Wei Ni", "Wenjie Zhang", "Xinyu Li", "Yamin Wang"], "title": "Learning Causal States Under Partial Observability and Perturbation", "comment": null, "summary": "A critical challenge for reinforcement learning (RL) is making decisions based on incomplete and noisy observations, especially in perturbed and partially observable Markov decision processes (P$^2$OMDPs). Existing methods fail to mitigate perturbations while addressing partial observability. We propose \\textit{Causal State Representation under Asynchronous Diffusion Model (CaDiff)}, a framework that enhances any RL algorithm by uncovering the underlying causal structure of P$^2$OMDPs. This is achieved by incorporating a novel asynchronous diffusion model (ADM) and a new bisimulation metric. ADM enables forward and reverse processes with different numbers of steps, thus interpreting the perturbation of P$^2$OMDP as part of the noise suppressed through diffusion. The bisimulation metric quantifies the similarity between partially observable environments and their causal counterparts. Moreover, we establish the theoretical guarantee of CaDiff by deriving an upper bound for the value function approximation errors between perturbed observations and denoised causal states, reflecting a principled trade-off between approximation errors of reward and transition-model. Experiments on Roboschool tasks show that CaDiff enhances returns by at least 14.18\\% compared to baselines. CaDiff is the first framework that approximates causal states using diffusion models with both theoretical rigor and practicality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCaDiff\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5f02\u6b65\u6269\u6563\u6a21\u578b\uff08ADM\uff09\u548c\u65b0\u7684\u53cc\u6a21\u62df\u6307\u6807\uff0c\u6765\u589e\u5f3aRL\u7b97\u6cd5\uff0c\u4ee5\u89e3\u51b3\u6270\u52a8\u548c\u90e8\u5206\u53ef\u89c2\u5bdf\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08P^2OMDPs\uff09\u4e2d\u7684\u51b3\u7b56\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5728\u89e3\u51b3\u90e8\u5206\u53ef\u89c2\u5bdf\u6027\u7684\u540c\u65f6\u51cf\u8f7b\u6270\u52a8\u3002\u5728\u4e0d\u5b8c\u6574\u548c\u5608\u6742\u7684\u89c2\u5bdf\u57fa\u7840\u4e0a\u505a\u51fa\u51b3\u7b56\u662f\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u6270\u52a8\u548c\u90e8\u5206\u53ef\u89c2\u5bdf\u7684\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08P^2OMDPs\uff09\u4e2d\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u65b0\u9896\u7684\u5f02\u6b65\u6269\u6563\u6a21\u578b\uff08ADM\uff09\u548c\u65b0\u7684\u53cc\u6a21\u62df\u6307\u6807\u6765\u63ed\u793aP^2OMDPs\u7684\u6f5c\u5728\u56e0\u679c\u7ed3\u6784\u3002ADM \u80fd\u591f\u4ee5\u4e0d\u540c\u6570\u91cf\u7684\u6b65\u9aa4\u5b9e\u73b0\u524d\u5411\u548c\u53cd\u5411\u8fc7\u7a0b\uff0c\u4ece\u800c\u5c06 P^2OMDP \u7684\u6270\u52a8\u89e3\u91ca\u4e3a\u901a\u8fc7\u6269\u6563\u6291\u5236\u7684\u566a\u58f0\u7684\u4e00\u90e8\u5206\u3002\u53cc\u6a21\u62df\u6307\u6807\u91cf\u5316\u4e86\u90e8\u5206\u53ef\u89c2\u5bdf\u73af\u5883\u4e0e\u5176\u56e0\u679c\u5bf9\u5e94\u7269\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u3002", "result": "\u5728Roboschool\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u76f8\u6bd4\uff0cCaDiff \u81f3\u5c11\u63d0\u9ad8\u4e86 14.18% \u7684\u56de\u62a5\u3002", "conclusion": "CaDiff \u662f\u7b2c\u4e00\u4e2a\u4f7f\u7528\u5177\u6709\u7406\u8bba\u4e25\u8c28\u6027\u548c\u5b9e\u7528\u6027\u7684\u6269\u6563\u6a21\u578b\u6765\u8fd1\u4f3c\u56e0\u679c\u72b6\u6001\u7684\u6846\u67b6\u3002"}}
{"id": "2512.00226", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00226", "abs": "https://arxiv.org/abs/2512.00226", "authors": ["Zirui Wang", "Tao Zhang"], "title": "DenseScan: Advancing 3D Scene Understanding with 2D Dense Annotation", "comment": "Workshop on Space in Vision, Language, and Embodied AI at NeurIPS 2025", "summary": "3D understanding is a key capability for real-world AI assistance. High-quality data plays an important role in driving the development of the 3D understanding community. Current 3D scene understanding datasets often provide geometric and instance-level information, yet they lack the rich semantic annotations necessary for nuanced visual-language tasks.In this work, we introduce DenseScan, a novel dataset with detailed multi-level descriptions generated by an automated pipeline leveraging multi-view 2D images and multimodal large language models (MLLMs). Our approach enables dense captioning of scene elements, ensuring comprehensive object-level descriptions that capture context-sensitive details. Furthermore, we extend these annotations through scenario-based question generation, producing high-level queries that integrate object properties, spatial relationships, and scene context. By coupling geometric detail with semantic richness, DenseScan broadens the range of downstream tasks, from detailed visual-language navigation to interactive question answering. Experimental results demonstrate that our method significantly enhances object-level understanding and question-answering performance in 3D environments compared to traditional annotation pipelines. We release both the annotated dataset and our annotation pipeline to facilitate future research and applications in robotics, augmented reality, and beyond. Through DenseScan, we aim to catalyze new avenues in 3D scene understanding, allowing researchers and practitioners to tackle the complexities of real-world environments with richer, more contextually aware annotations.", "AI": {"tldr": "DenseScan: A new 3D scene understanding dataset with detailed multi-level descriptions generated by an automated pipeline leveraging multi-view 2D images and multimodal large language models (MLLMs).", "motivation": "Current 3D scene understanding datasets lack the rich semantic annotations necessary for nuanced visual-language tasks.", "method": "An automated pipeline leveraging multi-view 2D images and multimodal large language models (MLLMs) is used to generate detailed multi-level descriptions.", "result": "The method significantly enhances object-level understanding and question-answering performance in 3D environments compared to traditional annotation pipelines.", "conclusion": "DenseScan broadens the range of downstream tasks and facilitates future research and applications in robotics, augmented reality, and beyond by providing richer, more contextually aware annotations."}}
{"id": "2512.00918", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00918", "abs": "https://arxiv.org/abs/2512.00918", "authors": ["Cen Lu", "Yung-Chen Tang", "Andrea Cavallaro"], "title": "Minimal neuron ablation triggers catastrophic collapse in the language core of Large Vision-Language Models", "comment": "15 pages, 6 figures,", "summary": "Large Vision-Language Models (LVLMs) have shown impressive multimodal understanding capabilities, yet their robustness is poorly understood. In this paper, we investigate the structural vulnerabilities of LVLMs to identify any critical neurons whose removal triggers catastrophic collapse. In this context, we propose CAN, a method to detect Consistently Activated Neurons and to locate critical neurons by progressive masking. Experiments on LLaVA-1.5-7b-hf and InstructBLIP-Vicuna-7b reveal that masking only a tiny portion of the language model's feed-forward networks (just as few as four neurons in extreme cases) suffices to trigger catastrophic collapse. Notably, critical neurons are predominantly localized in the language model rather than in the vision components, and the down-projection layer is a particularly vulnerable structure. We also observe a consistent two-stage collapse pattern: initial expressive degradation followed by sudden, complete collapse. Our findings provide important insights for safety research in LVLMs.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLMs) \u7684\u7ed3\u6784\u8106\u5f31\u6027\uff0c\u53d1\u73b0\u79fb\u9664\u5c11\u91cf\u795e\u7ecf\u5143\u4f1a\u5bfc\u81f4\u707e\u96be\u6027\u5d29\u6e83\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLMs) \u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa CAN \u65b9\u6cd5\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u5c4f\u853d\u6765\u68c0\u6d4b\u6301\u7eed\u6fc0\u6d3b\u7684\u795e\u7ecf\u5143\u5e76\u5b9a\u4f4d\u5173\u952e\u795e\u7ecf\u5143\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5c4f\u853d\u8bed\u8a00\u6a21\u578b\u524d\u9988\u7f51\u7edc\u4e2d\u6781\u5c11\u4e00\u90e8\u5206\u795e\u7ecf\u5143\uff08\u5728\u6781\u7aef\u60c5\u51b5\u4e0b\u4ec5\u56db\u4e2a\u795e\u7ecf\u5143\uff09\u5c31\u8db3\u4ee5\u89e6\u53d1\u707e\u96be\u6027\u5d29\u6e83\u3002\u5173\u952e\u795e\u7ecf\u5143\u4e3b\u8981\u4f4d\u4e8e\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u800c\u975e\u89c6\u89c9\u7ec4\u4ef6\u4e2d\uff0c\u5e76\u4e14\u4e0b\u6295\u5f71\u5c42\u662f\u4e00\u4e2a\u7279\u522b\u8106\u5f31\u7684\u7ed3\u6784\u3002\u89c2\u5bdf\u5230\u4e00\u4e2a\u4e00\u81f4\u7684\u4e24\u9636\u6bb5\u5d29\u6e83\u6a21\u5f0f\uff1a\u521d\u59cb\u8868\u8fbe\u80fd\u529b\u4e0b\u964d\uff0c\u7136\u540e\u662f\u7a81\u7136\u7684\u5b8c\u5168\u5d29\u6e83\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a LVLMs \u7684\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u89c1\u89e3\u3002"}}
{"id": "2512.00663", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00663", "abs": "https://arxiv.org/abs/2512.00663", "authors": ["Tanmay Agrawal"], "title": "Graphing the Truth: Structured Visualizations for Automated Hallucination Detection in LLMs", "comment": null, "summary": "Large Language Models have rapidly advanced in their ability to interpret and generate natural language. In enterprise settings, they are frequently augmented with closed-source domain knowledge to deliver more contextually informed responses. However, operational constraints such as limited context windows and inconsistencies between pre-training data and supplied knowledge often lead to hallucinations, some of which appear highly credible and escape routine human review. Current mitigation strategies either depend on costly, large-scale gold-standard Q\\&A curation or rely on secondary model verification, neither of which offers deterministic assurance. This paper introduces a framework that organizes proprietary knowledge and model-generated content into interactive visual knowledge graphs. The objective is to provide end users with a clear, intuitive view of potential hallucination zones by linking model assertions to underlying sources of truth and indicating confidence levels. Through this visual interface, users can diagnose inconsistencies, identify weak reasoning chains, and supply corrective feedback. The resulting human-in-the-loop workflow creates a structured feedback loop that can enhance model reliability and continuously improve response quality.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5229\u7528\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u77e5\u8bc6\u56fe\u8c31\u6765\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e7b\u89c9\u7684\u6846\u67b6\u3002", "motivation": "\u4f01\u4e1a\u73af\u5883\u4e2d\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ecf\u5e38\u4f7f\u7528\u95ed\u6e90\u9886\u57df\u77e5\u8bc6\u8fdb\u884c\u589e\u5f3a\uff0c\u4f46\u7531\u4e8e\u4e0a\u4e0b\u6587\u7a97\u53e3\u6709\u9650\u548c\u9884\u8bad\u7ec3\u6570\u636e\u4e0e\u63d0\u4f9b\u7684\u77e5\u8bc6\u4e4b\u95f4\u4e0d\u4e00\u81f4\uff0c\u5bfc\u81f4\u6a21\u578b\u4ea7\u751f\u5e7b\u89c9\u3002\u5f53\u524d\u7684\u7f13\u89e3\u7b56\u7565\u8981\u4e48\u4f9d\u8d56\u4e8e\u6602\u8d35\u7684\u5927\u89c4\u6a21\u9ec4\u91d1\u6807\u51c6\u95ee\u7b54\u7ba1\u7406\uff0c\u8981\u4e48\u4f9d\u8d56\u4e8e\u4e8c\u7ea7\u6a21\u578b\u9a8c\u8bc1\uff0c\u4f46\u90fd\u4e0d\u80fd\u63d0\u4f9b\u786e\u5b9a\u6027\u7684\u4fdd\u8bc1\u3002", "method": "\u8be5\u6846\u67b6\u5c06\u4e13\u6709\u77e5\u8bc6\u548c\u6a21\u578b\u751f\u6210\u7684\u5185\u5bb9\u7ec4\u7ec7\u6210\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u77e5\u8bc6\u56fe\u8c31\u3002\u901a\u8fc7\u5c06\u6a21\u578b\u65ad\u8a00\u94fe\u63a5\u5230\u57fa\u672c\u4e8b\u5b9e\u6765\u6e90\u5e76\u6307\u793a\u7f6e\u4fe1\u5ea6\uff0c\u4e3a\u6700\u7ec8\u7528\u6237\u63d0\u4f9b\u6f5c\u5728\u5e7b\u89c9\u533a\u57df\u7684\u6e05\u6670\u76f4\u89c2\u89c6\u56fe\u3002", "result": "\u901a\u8fc7\u8fd9\u79cd\u53ef\u89c6\u5316\u754c\u9762\uff0c\u7528\u6237\u53ef\u4ee5\u8bca\u65ad\u4e0d\u4e00\u81f4\u4e4b\u5904\uff0c\u8bc6\u522b\u8584\u5f31\u7684\u63a8\u7406\u94fe\uff0c\u5e76\u63d0\u4f9b\u7ea0\u6b63\u6027\u53cd\u9988\u3002", "conclusion": "\u7531\u6b64\u4ea7\u751f\u7684\u4eba\u5de5\u53c2\u4e0e\u5de5\u4f5c\u6d41\u7a0b\u521b\u5efa\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u53cd\u9988\u5faa\u73af\uff0c\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u53ef\u9760\u6027\u5e76\u4e0d\u65ad\u63d0\u9ad8\u54cd\u5e94\u8d28\u91cf\u3002"}}
{"id": "2512.00366", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00366", "abs": "https://arxiv.org/abs/2512.00366", "authors": ["Wenshuo Wang", "Yaomin Shen", "Yingjie Tan", "Yihao Chen"], "title": "S^2-KD: Semantic-Spectral Knowledge Distillation Spatiotemporal Forecasting", "comment": null, "summary": "Spatiotemporal forecasting often relies on computationally intensive models to capture complex dynamics. Knowledge distillation (KD) has emerged as a key technique for creating lightweight student models, with recent advances like frequency-aware KD successfully preserving spectral properties (i.e., high-frequency details and low-frequency trends). However, these methods are fundamentally constrained by operating on pixel-level signals, leaving them blind to the rich semantic and causal context behind the visual patterns. To overcome this limitation, we introduce S^2-KD, a novel framework that unifies Semantic priors with Spectral representations for distillation. Our approach begins by training a privileged, multimodal teacher model. This teacher leverages textual narratives from a Large Multimodal Model (LMM) to reason about the underlying causes of events, while its architecture simultaneously decouples spectral components in its latent space. The core of our framework is a new distillation objective that transfers this unified semantic-spectral knowledge into a lightweight, vision-only student. Consequently, the student learns to make predictions that are not only spectrally accurate but also semantically coherent, without requiring any textual input or architectural overhead at inference. Extensive experiments on benchmarks like WeatherBench and TaxiBJ+ show that S^2-KD significantly boosts the performance of simple student models, enabling them to outperform state-of-the-art methods, particularly in long-horizon and complex non-stationary scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u77e5\u8bc6\u84b8\u998f\u6846\u67b6S^2-KD\uff0c\u5b83\u7ed3\u5408\u4e86\u8bed\u4e49\u5148\u9a8c\u548c\u9891\u8c31\u8868\u793a\u6765\u8fdb\u884c\u84b8\u998f\uff0c\u4ee5\u63d0\u9ad8\u65f6\u7a7a\u9884\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u4e3b\u8981\u5728\u50cf\u7d20\u5c42\u9762\u64cd\u4f5c\uff0c\u5ffd\u7565\u4e86\u89c6\u89c9\u6a21\u5f0f\u80cc\u540e\u7684\u4e30\u5bcc\u8bed\u4e49\u548c\u56e0\u679c\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2a\u7279\u6743\u7684\u591a\u6a21\u6001\u6559\u5e08\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5229\u7528\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMM\uff09\u7684\u6587\u672c\u53d9\u8ff0\u6765\u63a8\u65ad\u4e8b\u4ef6\u7684\u6839\u672c\u539f\u56e0\uff0c\u540c\u65f6\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u89e3\u8026\u9891\u8c31\u5206\u91cf\u3002\u7136\u540e\uff0c\u901a\u8fc7\u65b0\u7684\u84b8\u998f\u76ee\u6807\u5c06\u7edf\u4e00\u7684\u8bed\u4e49-\u9891\u8c31\u77e5\u8bc6\u8f6c\u79fb\u5230\u8f7b\u91cf\u7ea7\u7684\u3001\u4ec5\u89c6\u89c9\u7684\u5b66\u751f\u6a21\u578b\u4e2d\u3002", "result": "\u5728WeatherBench\u548cTaxiBJ+\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cS^2-KD\u663e\u8457\u63d0\u9ad8\u4e86\u7b80\u5355\u5b66\u751f\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4f7f\u5176\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u957f\u7a0b\u548c\u590d\u6742\u7684\u975e\u5e73\u7a33\u573a\u666f\u4e2d\u3002", "conclusion": "S^2-KD\u6846\u67b6\u80fd\u591f\u4f7f\u5b66\u751f\u6a21\u578b\u505a\u51fa\u4e0d\u4ec5\u5728\u9891\u8c31\u4e0a\u51c6\u786e\uff0c\u800c\u4e14\u5728\u8bed\u4e49\u4e0a\u8fde\u8d2f\u7684\u9884\u6d4b\uff0c\u800c\u65e0\u9700\u4efb\u4f55\u6587\u672c\u8f93\u5165\u6216\u63a8\u7406\u65f6\u7684\u67b6\u6784\u5f00\u9500\u3002"}}
{"id": "2512.00255", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00255", "abs": "https://arxiv.org/abs/2512.00255", "authors": ["Kunwar Maheep Singh", "Jianchun Chen", "Vladislav Golyanik", "Stephan J. Garbin", "Thabo Beeler", "Rishabh Dabral", "Marc Habermann", "Christian Theobalt"], "title": "Relightable Holoported Characters: Capturing and Relighting Dynamic Human Performance from Sparse Views", "comment": null, "summary": "We present Relightable Holoported Characters (RHC), a novel person-specific method for free-view rendering and relighting of full-body and highly dynamic humans solely observed from sparse-view RGB videos at inference. In contrast to classical one-light-at-a-time (OLAT)-based human relighting, our transformer-based RelightNet predicts relit appearance within a single network pass, avoiding costly OLAT-basis capture and generation. For training such a model, we introduce a new capture strategy and dataset recorded in a multi-view lightstage, where we alternate frames lit by random environment maps with uniformly lit tracking frames, simultaneously enabling accurate motion tracking and diverse illumination as well as dynamics coverage. Inspired by the rendering equation, we derive physics-informed features that encode geometry, albedo, shading, and the virtual camera view from a coarse human mesh proxy and the input views. Our RelightNet then takes these features as input and cross-attends them with a novel lighting condition, and regresses the relit appearance in the form of texel-aligned 3D Gaussian splats attached to the coarse mesh proxy. Consequently, our RelightNet implicitly learns to efficiently compute the rendering equation for novel lighting conditions within a single feed-forward pass. Experiments demonstrate our method's superior visual fidelity and lighting reproduction compared to state-of-the-art approaches. Project page: https://vcai.mpi-inf.mpg.de/projects/RHC/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRelightable Holoported Characters (RHC) \u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u63a8\u7406\u65f6\u4ece\u7a00\u758f\u89c6\u56fe RGB \u89c6\u9891\u4e2d\u81ea\u7531\u89c6\u89d2\u6e32\u67d3\u548c\u91cd\u65b0\u5149\u7167\u5168\u8eab\u548c\u9ad8\u5ea6\u52a8\u6001\u7684\u4eba\u4f53\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfone-light-at-a-time (OLAT) \u4eba\u4f53\u91cd\u5149\u65b9\u6cd5\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e Transformer \u7684 RelightNet\uff0c\u8be5\u7f51\u7edc\u5728\u5355\u6b21\u7f51\u7edc\u4f20\u9012\u4e2d\u9884\u6d4b\u91cd\u65b0\u5149\u7167\u7684\u5916\u89c2\uff0c\u5e76\u7ed3\u5408\u65b0\u7684\u6355\u83b7\u7b56\u7565\u548c\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5728\u591a\u89c6\u56fe\u5149\u53f0\u4e2d\u8bb0\u5f55\uff0c\u4ea4\u66ff\u4f7f\u7528\u968f\u673a\u73af\u5883\u8d34\u56fe\u7167\u660e\u7684\u5e27\u548c\u5747\u5300\u7167\u660e\u7684\u8ddf\u8e2a\u5e27\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u5149\u7167\u518d\u73b0\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "RelightNet \u9690\u5f0f\u5730\u5b66\u4e60\u5728\u5355\u4e2a\u524d\u9988\u4f20\u9012\u4e2d\u6709\u6548\u5730\u8ba1\u7b97\u65b0\u7167\u660e\u6761\u4ef6\u4e0b\u7684\u6e32\u67d3\u65b9\u7a0b\u3002"}}
{"id": "2512.00969", "categories": ["cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.00969", "abs": "https://arxiv.org/abs/2512.00969", "authors": ["Felix Saretzky", "Lucas Andersen", "Thomas Engel", "Fazel Ansari"], "title": "Integrating Causal Foundation Model in Prescriptive Maintenance Framework for Optimizing Production Line OEE", "comment": "9 pages, 3 images, 1 table, conference paper", "summary": "The transition to prescriptive maintenance in manufacturing is critically constrained by a dependence on predictive models. These models tend to rely on spurious correlations rather than identifying the true causal drivers of failures, often leading to costly misdiagnoses and ineffective interventions. This fundamental limitation results in a key-challenge: while we can predict that a failure may occur, we lack a systematic method to understand why a failure occurs, thereby providing the basis for identifying the most effective intervention. This paper proposes a model based on causal machine learning to bridge this gap. Our objective is to move beyond diagnosis to active prescription by simulating and evaluating potential fixes toward optimizing KPIs such as Overall Equipment Effectiveness (OEE). For this purpose a pre-trained causal foundation model is used as a \"what-if\" model to estimate the effects of potential fixes. By measuring the causal effect of each intervention on system-level KPIs, it provides a data-driven ranking of actions to recommend at the production line. This process not only identifies root causes but also quantifies their operational impact. The model is evaluated using semi-synthetic manufacturing data and compared with a baseline machine learning model. This paper sets the technical basis for a robust prescriptive maintenance framework, allowing engineers to test potential solutions in a causal environment to make more effective operational decisions and reduce costly downtimes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u673a\u5668\u5b66\u4e60\u7684\u6a21\u578b\uff0c\u65e8\u5728\u901a\u8fc7\u6a21\u62df\u548c\u8bc4\u4f30\u6f5c\u5728\u7684\u4fee\u590d\u65b9\u6848\uff0c\u4ece\u800c\u8d85\u8d8a\u8bca\u65ad\uff0c\u5b9e\u73b0\u4e3b\u52a8\u5904\u65b9\uff0c\u4f18\u5316\u5173\u952e\u7ee9\u6548\u6307\u6807\uff08\u5982\u6574\u4f53\u8bbe\u5907\u6548\u7387OEE\uff09\u3002", "motivation": "\u5236\u9020\u4e1a\u5411\u89c4\u8303\u6027\u7ef4\u62a4\u7684\u8f6c\u53d8\u53d7\u5230\u5bf9\u9884\u6d4b\u6a21\u578b\u7684\u4f9d\u8d56\u7684\u4e25\u91cd\u9650\u5236\u3002\u8fd9\u4e9b\u6a21\u578b\u503e\u5411\u4e8e\u4f9d\u8d56\u865a\u5047\u7684\u76f8\u5173\u6027\uff0c\u800c\u4e0d\u662f\u8bc6\u522b\u6545\u969c\u7684\u771f\u6b63\u56e0\u679c\u9a71\u52a8\u56e0\u7d20\uff0c\u4ece\u800c\u7ecf\u5e38\u5bfc\u81f4\u4ee3\u4ef7\u9ad8\u6602\u7684\u8bef\u8bca\u548c\u65e0\u6548\u7684\u5e72\u9884\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u56e0\u679c\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u201c\u5047\u8bbe\u201d\u6a21\u578b\u6765\u4f30\u8ba1\u6f5c\u5728\u4fee\u590d\u65b9\u6848\u7684\u6548\u679c\u3002\u901a\u8fc7\u6d4b\u91cf\u6bcf\u6b21\u5e72\u9884\u5bf9\u7cfb\u7edf\u7ea7KPI\u7684\u56e0\u679c\u6548\u5e94\uff0c\u5b83\u63d0\u4f9b\u4e86\u6570\u636e\u9a71\u52a8\u7684\u884c\u52a8\u6392\u5e8f\uff0c\u4ee5\u4fbf\u5728\u751f\u4ea7\u7ebf\u4e0a\u63a8\u8350\u3002", "result": "\u8be5\u6a21\u578b\u4f7f\u7528\u534a\u5408\u6210\u5236\u9020\u6570\u636e\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u4e0e\u57fa\u7ebf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u8bc6\u522b\u6839\u672c\u539f\u56e0\uff0c\u5e76\u91cf\u5316\u5176\u5bf9\u8fd0\u8425\u7684\u5f71\u54cd\u3002", "conclusion": "\u672c\u6587\u4e3a\u7a33\u5065\u7684\u89c4\u8303\u6027\u7ef4\u62a4\u6846\u67b6\u5960\u5b9a\u4e86\u6280\u672f\u57fa\u7840\uff0c\u4f7f\u5de5\u7a0b\u5e08\u80fd\u591f\u5728\u56e0\u679c\u73af\u5883\u4e2d\u6d4b\u8bd5\u6f5c\u5728\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ece\u800c\u505a\u51fa\u66f4\u6709\u6548\u7684\u8fd0\u8425\u51b3\u7b56\u5e76\u51cf\u5c11\u4ee3\u4ef7\u9ad8\u6602\u7684\u505c\u673a\u65f6\u95f4\u3002"}}
{"id": "2512.00673", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00673", "abs": "https://arxiv.org/abs/2512.00673", "authors": ["Breanna E. Green", "Ashley L. Shea", "Pengfei Zhao", "Drew B. Margolin"], "title": "A Comparison of Human and ChatGPT Classification Performance on Complex Social Media Data", "comment": "About 15 pages, draft version of accepted conference full paper. Published paper to follow", "summary": "Generative artificial intelligence tools, like ChatGPT, are an increasingly utilized resource among computational social scientists. Nevertheless, there remains space for improved understanding of the performance of ChatGPT in complex tasks such as classifying and annotating datasets containing nuanced language. Method. In this paper, we measure the performance of GPT-4 on one such task and compare results to human annotators. We investigate ChatGPT versions 3.5, 4, and 4o to examine performance given rapid changes in technological advancement of large language models. We craft four prompt styles as input and evaluate precision, recall, and F1 scores. Both quantitative and qualitative evaluations of results demonstrate that while including label definitions in prompts may help performance, overall GPT-4 has difficulty classifying nuanced language. Qualitative analysis reveals four specific findings. Our results suggest the use of ChatGPT in classification tasks involving nuanced language should be conducted with prudence.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86ChatGPT\u5728\u590d\u6742\u4efb\u52a1\uff08\u5982\u5206\u7c7b\u548c\u6ce8\u91ca\u5305\u542b\u7ec6\u5fae\u5dee\u522b\u7684\u8bed\u8a00\u7684\u6570\u636e\u96c6\uff09\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u5c06\u5176\u7ed3\u679c\u4e0e\u4eba\u5de5\u6ce8\u91ca\u5458\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5bf9\u4e8eChatGPT\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u4ecd\u9700\u8fdb\u4e00\u6b65\u7406\u89e3\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86GPT-4\uff0c\u5e76\u4e0e\u4eba\u5de5\u6ce8\u91ca\u5458\u7684\u7ed3\u679c\u8fdb\u884c\u6bd4\u8f83\u3002 \u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043b\u0438 ChatGPT 3.5, 4 \u548c 4o\u7248\u672c\uff0c\u4f7f\u7528\u4e86\u56db\u79cdprompt\u6837\u5f0f\u4f5c\u4e3a\u8f93\u5165\uff0c\u5e76\u8bc4\u4f30\u4e86\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u867d\u7136\u5728\u63d0\u793a\u4e2d\u5305\u542b\u6807\u7b7e\u5b9a\u4e49\u53ef\u80fd\u6709\u52a9\u4e8e\u63d0\u9ad8\u6027\u80fd\uff0c\u4f46GPT-4\u5728\u5206\u7c7b\u7ec6\u5fae\u8bed\u8a00\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "\u5728\u6d89\u53ca\u7ec6\u5fae\u8bed\u8a00\u7684\u5206\u7c7b\u4efb\u52a1\u4e2d\u4f7f\u7528ChatGPT\u5e94\u8c28\u614e\u3002"}}
{"id": "2512.00383", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.00383", "abs": "https://arxiv.org/abs/2512.00383", "authors": ["Jianhai Su", "Jinzhu Luo", "Qi Zhang"], "title": "An Empirical Study on the Effectiveness of Incorporating Offline RL As Online RL Subroutines", "comment": null, "summary": "We take the novel perspective of incorporating offline RL algorithms as subroutines of tabula rasa online RL. This is feasible because an online learning agent can repurpose its historical interactions as offline dataset. We formalize this idea into a framework that accommodates several variants of offline RL incorporation such as final policy recommendation and online fine-tuning. We further introduce convenient techniques to improve its effectiveness in enhancing online learning efficiency. Our extensive and systematic empirical analyses show that 1) the effectiveness of the proposed framework depends strongly on the nature of the task, 2) our proposed techniques greatly enhance its effectiveness, and 3) existing online fine-tuning methods are overall ineffective, calling for more research therein.", "AI": {"tldr": "\u5c06\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u6574\u5408\u4e3a\u4ece\u96f6\u5f00\u59cb\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u5b50\u7a0b\u5e8f\u3002", "motivation": "\u5728\u7ebf\u5b66\u4e60\u667a\u80fd\u4f53\u53ef\u4ee5\u5c06\u5176\u5386\u53f2\u4ea4\u4e92\u91cd\u65b0\u7528\u4f5c\u79bb\u7ebf\u6570\u636e\u96c6\u3002", "method": "\u5f62\u5f0f\u5316\u4e3a\u4e00\u4e2a\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u53ef\u5bb9\u7eb3\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5408\u5e76\u7684\u51e0\u79cd\u53d8\u4f53\uff0c\u4f8b\u5982\u6700\u7ec8\u7b56\u7565\u63a8\u8350\u548c\u5728\u7ebf\u5fae\u8c03\u3002\u5f15\u5165\u4fbf\u6377\u6280\u672f\u4ee5\u63d0\u9ad8\u5176\u5728\u63d0\u9ad8\u5728\u7ebf\u5b66\u4e60\u6548\u7387\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "result": "1) \u6240\u63d0\u51fa\u6846\u67b6\u7684\u6709\u6548\u6027\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u4efb\u52a1\u7684\u6027\u8d28\uff0c2) \u6211\u4eec\u63d0\u51fa\u7684\u6280\u672f\u6781\u5927\u5730\u63d0\u9ad8\u4e86\u5176\u6709\u6548\u6027\uff0c3) \u73b0\u6709\u7684\u5728\u7ebf\u5fae\u8c03\u65b9\u6cd5\u603b\u4f53\u4e0a\u662f\u65e0\u6548\u7684\uff0c\u9700\u8981\u66f4\u591a\u7684\u7814\u7a76\u3002", "conclusion": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u6574\u5408\u4e3a\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u5b50\u7a0b\u5e8f\uff0c\u4f46\u5176\u6709\u6548\u6027\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u4efb\u52a1\u7684\u6027\u8d28\u3002"}}
{"id": "2512.00261", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00261", "abs": "https://arxiv.org/abs/2512.00261", "authors": ["Yuzhen Hu", "Saurabh Prasad"], "title": "UniDiff: Parameter-Efficient Adaptation of Diffusion Models for Land Cover Classification with Multi-Modal Remotely Sensed Imagery and Sparse Annotations", "comment": "Camera-ready for WACV 2026", "summary": "Sparse annotations fundamentally constrain multimodal remote sensing: even recent state-of-the-art supervised methods such as MSFMamba are limited by the availability of labeled data, restricting their practical deployment despite architectural advances. ImageNet-pretrained models provide rich visual representations, but adapting them to heterogeneous modalities such as hyperspectral imaging (HSI) and synthetic aperture radar (SAR) without large labeled datasets remains challenging. We propose UniDiff, a parameter-efficient framework that adapts a single ImageNet-pretrained diffusion model to multiple sensing modalities using only target-domain data. UniDiff combines FiLM-based timestep-modality conditioning, parameter-efficient adaptation of approximately 5% of parameters, and pseudo-RGB anchoring to preserve pre-trained representations and prevent catastrophic forgetting. This design enables effective feature extraction from remote sensing data under sparse annotations. Our results with two established multi-modal benchmarking datasets demonstrate that unsupervised adaptation of a pre-trained diffusion model effectively mitigates annotation constraints and achieves effective fusion of multi-modal remotely sensed data.", "AI": {"tldr": "\u63d0\u51fa UniDiff\uff0c\u4e00\u4e2a\u53c2\u6570\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u5b83\u4f7f\u7528\u76ee\u6807\u57df\u6570\u636e\u5c06\u5355\u4e2a ImageNet \u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u9002\u914d\u5230\u591a\u4e2a\u4f20\u611f\u6a21\u6001\u3002", "motivation": "\u7a00\u758f\u6ce8\u91ca\u4ece\u6839\u672c\u4e0a\u9650\u5236\u4e86\u591a\u6a21\u6001\u9065\u611f\uff0c\u5373\u4f7f\u662f\u6700\u65b0\u7684\u6709\u76d1\u7763\u65b9\u6cd5\u4e5f\u53d7\u5230\u6807\u8bb0\u6570\u636e\u53ef\u7528\u6027\u7684\u9650\u5236\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u7684\u5b9e\u9645\u90e8\u7f72\u3002", "method": "UniDiff \u7ed3\u5408\u4e86\u57fa\u4e8e FiLM \u7684\u65f6\u95f4\u6b65\u957f-\u6a21\u6001\u8c03\u8282\u3001\u5927\u7ea6 5% \u53c2\u6570\u7684\u53c2\u6570\u9ad8\u6548\u9002\u914d\u4ee5\u53ca\u4f2a RGB \u951a\u5b9a\uff0c\u4ee5\u4fdd\u7559\u9884\u8bad\u7ec3\u7684\u8868\u793a\u5e76\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u3002", "result": "\u5728\u4e24\u4e2a\u5df2\u5efa\u7acb\u7684\u591a\u6a21\u6001\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u7ed3\u679c\u8868\u660e\uff0c\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u65e0\u76d1\u7763\u9002\u914d\u6709\u6548\u5730\u7f13\u89e3\u4e86\u6ce8\u91ca\u7ea6\u675f\uff0c\u5e76\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u9065\u611f\u6570\u636e\u7684\u6709\u6548\u878d\u5408\u3002", "conclusion": "UniDiff \u6709\u6548\u5730\u7f13\u89e3\u4e86\u6ce8\u91ca\u7ea6\u675f\uff0c\u5e76\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u9065\u611f\u6570\u636e\u7684\u6709\u6548\u878d\u5408\u3002"}}
{"id": "2512.00997", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00997", "abs": "https://arxiv.org/abs/2512.00997", "authors": ["Param Biyani", "Shashank Kirtania", "Yasharth Bajpai", "Sumit Gulwani", "Ashish Tiwari"], "title": "IndiMathBench: Autoformalizing Mathematical Reasoning Problems with a Human Touch", "comment": null, "summary": "We introduce IndiMathBench, a human-verified benchmark designed to evaluate mathematical theorem proving, curated using an AI-powered human-assisted pipeline for formalizing natural language problems in Lean. IndiMathBench is composed of 312 formal Lean 4 theorems paired with their corresponding informal problem statements, sourced from Indian Mathematics Olympiads. Through category-based retrieval, iterative compiler feedback, and multi-model ensembles, our pipeline generates candidate formalizations that experts efficiently validate via an interactive dashboard with automated quality summaries. Evaluation across multiple frontier models demonstrates that autoformalization remains challenging, with substantial gaps between syntactic validity and semantic correctness, while theorem proving success rates remain low even with iterative refinement, demonstrating that \\benchmark~presents a challenging testbed for mathematical reasoning. IndiMathBench is available at https://github.com/prmbiy/IndiMathBench.", "AI": {"tldr": "IndiMathBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u6570\u5b66\u5b9a\u7406\u8bc1\u660e\u7684\u4eba\u5de5\u9a8c\u8bc1\u57fa\u51c6\uff0c\u5b83\u7531312\u4e2a\u5f62\u5f0f\u5316\u7684Lean 4\u5b9a\u7406\u4ee5\u53ca\u5bf9\u5e94\u7684\u975e\u6b63\u5f0f\u95ee\u9898\u9648\u8ff0\u7ec4\u6210\uff0c\u8fd9\u4e9b\u95ee\u9898\u6765\u81ea\u5370\u5ea6\u6570\u5b66\u5965\u6797\u5339\u514b\u7ade\u8d5b\u3002", "motivation": "\u65e8\u5728\u8bc4\u4f30\u6570\u5b66\u5b9a\u7406\u8bc1\u660e\uff0c\u5e76\u4e3a\u5f62\u5f0f\u5316\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u63d0\u4f9b\u57fa\u51c6\u3002", "method": "\u4f7f\u7528AI\u8f85\u52a9\u7684\u4eba\u5de5\u6d41\u7a0b\uff0c\u901a\u8fc7\u57fa\u4e8e\u7c7b\u522b\u7684\u68c0\u7d22\u3001\u8fed\u4ee3\u7f16\u8bd1\u5668\u53cd\u9988\u548c\u591a\u6a21\u578b\u96c6\u6210\u6765\u751f\u6210\u5019\u9009\u5f62\u5f0f\u5316\uff0c\u4e13\u5bb6\u901a\u8fc7\u4ea4\u4e92\u5f0f\u4eea\u8868\u677f\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u81ea\u52a8\u5f62\u5f0f\u5316\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u53e5\u6cd5\u6709\u6548\u6027\u548c\u8bed\u4e49\u6b63\u786e\u6027\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\uff0c\u5373\u4f7f\u7ecf\u8fc7\u8fed\u4ee3\u6539\u8fdb\uff0c\u5b9a\u7406\u8bc1\u660e\u6210\u529f\u7387\u4ecd\u7136\u5f88\u4f4e\u3002", "conclusion": "IndiMathBench\u4e3a\u6570\u5b66\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002"}}
{"id": "2512.00745", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00745", "abs": "https://arxiv.org/abs/2512.00745", "authors": ["Md Abdullah Al Kafi", "Sumit Kumar Banshal"], "title": "FastPOS: Language-Agnostic Scalable POS Tagging Framework Low-Resource Use Case", "comment": null, "summary": "This study proposes a language-agnostic transformer-based POS tagging framework designed for low-resource languages, using Bangla and Hindi as case studies. With only three lines of framework-specific code, the model was adapted from Bangla to Hindi, demonstrating effective portability with minimal modification. The framework achieves 96.85 percent and 97 percent token-level accuracy across POS categories in Bangla and Hindi while sustaining strong F1 scores despite dataset imbalance and linguistic overlap. A performance discrepancy in a specific POS category underscores ongoing challenges in dataset curation. The strong results stem from the underlying transformer architecture, which can be replaced with limited code adjustments. Its modular and open-source design enables rapid cross-lingual adaptation while reducing model design and tuning overhead, allowing researchers to focus on linguistic preprocessing and dataset refinement, which are essential for advancing NLP in underrepresented languages.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0e\u8bed\u8a00\u65e0\u5173\u7684\u3001\u57fa\u4e8eTransformer\u7684\u8bcd\u6027\u6807\u6ce8\u6846\u67b6\uff0c\u4e13\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u8bbe\u8ba1\uff0c\u4ee5\u5b5f\u52a0\u62c9\u8bed\u548c\u5370\u5730\u8bed\u4e3a\u6848\u4f8b\u7814\u7a76\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u8bcd\u6027\u6807\u6ce8\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eTransformer\u7684\u67b6\u6784\uff0c\u53ea\u9700\u5c11\u91cf\u7279\u5b9a\u4e8e\u6846\u67b6\u7684\u4ee3\u7801\u5373\u53ef\u4ece\u5b5f\u52a0\u62c9\u8bed\u8c03\u6574\u5230\u5370\u5730\u8bed\u3002", "result": "\u5728\u5b5f\u52a0\u62c9\u8bed\u548c\u5370\u5730\u8bed\u7684POS\u7c7b\u522b\u4e2d\uff0c\u5b9e\u73b0\u4e8696.85%\u548c97%\u7684token\u7ea7\u522b\u51c6\u786e\u7387\uff0c\u5e76\u5728\u6570\u636e\u96c6\u4e0d\u5e73\u8861\u548c\u8bed\u8a00\u91cd\u53e0\u7684\u60c5\u51b5\u4e0b\u4fdd\u6301\u4e86\u5f3a\u5927\u7684F1\u5206\u6570\u3002", "conclusion": "\u8be5\u6846\u67b6\u7684\u6a21\u5757\u5316\u548c\u5f00\u6e90\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u5feb\u901f\u7684\u8de8\u8bed\u8a00\u9002\u5e94\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u6a21\u578b\u8bbe\u8ba1\u548c\u8c03\u6574\u5f00\u9500\uff0c\u4ece\u800c\u4f7f\u7814\u7a76\u4eba\u5458\u53ef\u4ee5\u4e13\u6ce8\u4e8e\u8bed\u8a00\u9884\u5904\u7406\u548c\u6570\u636e\u96c6\u6539\u8fdb\u3002"}}
{"id": "2512.00384", "categories": ["cs.LG", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2512.00384", "abs": "https://arxiv.org/abs/2512.00384", "authors": ["Shitong Luo", "Connor W. Coley"], "title": "Efficient and Programmable Exploration of Synthesizable Chemical Space", "comment": null, "summary": "The constrained nature of synthesizable chemical space poses a significant challenge for sampling molecules that are both synthetically accessible and possess desired properties. In this work, we present PrexSyn, an efficient and programmable model for molecular discovery within synthesizable chemical space. PrexSyn is based on a decoder-only transformer trained on a billion-scale datastream of synthesizable pathways paired with molecular properties, enabled by a real-time, high-throughput C++-based data generation engine. The large-scale training data allows PrexSyn to reconstruct the synthesizable chemical space nearly perfectly at a high inference speed and learn the association between properties and synthesizable molecules. Based on its learned property-pathway mappings, PrexSyn can generate synthesizable molecules that satisfy not only single-property conditions but also composite property queries joined by logical operators, thereby allowing users to ``program'' generation objectives. Moreover, by exploiting this property-based querying capability, PrexSyn can efficiently optimize molecules against black-box oracle functions via iterative query refinement, achieving higher sampling efficiency than even synthesis-agnostic baselines, making PrexSyn a powerful general-purpose molecular optimization tool. Overall, PrexSyn pushes the frontier of synthesizable molecular design by setting a new state of the art in synthesizable chemical space coverage, molecular sampling efficiency, and inference speed.", "AI": {"tldr": "PrexSyn\u662f\u4e00\u79cd\u7528\u4e8e\u5728\u53ef\u5408\u6210\u5316\u5b66\u7a7a\u95f4\u4e2d\u8fdb\u884c\u5206\u5b50\u53d1\u73b0\u7684\u9ad8\u6548\u4e14\u53ef\u7f16\u7a0b\u7684\u6a21\u578b\uff0c\u5b83\u57fa\u4e8edecoder-only transformer\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u7684\u5408\u6210\u8def\u5f84\u548c\u5206\u5b50\u5c5e\u6027\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002", "motivation": "\u5728\u53ef\u5408\u6210\u5316\u5b66\u7a7a\u95f4\u4e2d\u91c7\u6837\u5206\u5b50\uff0c\u4f7f\u5176\u65e2\u5177\u6709\u5408\u6210\u53ef\u53ca\u6027\u53c8\u5177\u6709\u6240\u9700\u6027\u8d28\uff0c\u8fd9\u662f\u4e00\u4e2a\u5de8\u5927\u7684\u6311\u6218\u3002", "method": "PrexSyn\u57fa\u4e8edecoder-only transformer\uff0c\u5e76\u4f7f\u7528C++\u6570\u636e\u751f\u6210\u5f15\u64ce\u751f\u6210\u7684\u5927\u89c4\u6a21\u5408\u6210\u8def\u5f84\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "PrexSyn\u80fd\u591f\u8fd1\u4e4e\u5b8c\u7f8e\u5730\u91cd\u5efa\u53ef\u5408\u6210\u5316\u5b66\u7a7a\u95f4\uff0c\u5e76\u5b66\u4e60\u5c5e\u6027\u548c\u53ef\u5408\u6210\u5206\u5b50\u4e4b\u95f4\u7684\u5173\u8054\u3002\u5b83\u53ef\u4ee5\u901a\u8fc7\u8fed\u4ee3\u67e5\u8be2\u4f18\u5316\u5206\u5b50\uff0c\u5b9e\u73b0\u6bd4\u975e\u5408\u6210\u611f\u77e5\u57fa\u7ebf\u66f4\u9ad8\u7684\u91c7\u6837\u6548\u7387\u3002", "conclusion": "PrexSyn\u901a\u8fc7\u5728\u53ef\u5408\u6210\u5316\u5b66\u7a7a\u95f4\u8986\u76d6\u7387\u3001\u5206\u5b50\u91c7\u6837\u6548\u7387\u548c\u63a8\u7406\u901f\u5ea6\u65b9\u9762\u8bbe\u5b9a\u65b0\u7684\u6280\u672f\u6c34\u5e73\uff0c\u63a8\u52a8\u4e86\u53ef\u5408\u6210\u5206\u5b50\u8bbe\u8ba1\u7684\u524d\u6cbf\u3002"}}
{"id": "2512.00264", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00264", "abs": "https://arxiv.org/abs/2512.00264", "authors": ["Zhengda Ma", "Abhirup Banerjee"], "title": "HeartFormer: Semantic-Aware Dual-Structure Transformers for 3D Four-Chamber Cardiac Point Cloud Reconstruction", "comment": null, "summary": "We present the first geometric deep learning framework based on point cloud representation for 3D four-chamber cardiac reconstruction from cine MRI data. This work addresses a long-standing limitation in conventional cine MRI, which typically provides only 2D slice images of the heart, thereby restricting a comprehensive understanding of cardiac morphology and physiological mechanisms in both healthy and pathological conditions. To overcome this, we propose \\textbf{HeartFormer}, a novel point cloud completion network that extends traditional single-class point cloud completion to the multi-class. HeartFormer consists of two key components: a Semantic-Aware Dual-Structure Transformer Network (SA-DSTNet) and a Semantic-Aware Geometry Feature Refinement Transformer Network (SA-GFRTNet). SA-DSTNet generates an initial coarse point cloud with both global geometry features and substructure geometry features. Guided by these semantic-geometry representations, SA-GFRTNet progressively refines the coarse output, effectively leveraging both global and substructure geometric priors to produce high-fidelity and geometrically consistent reconstructions. We further construct \\textbf{HeartCompv1}, the first publicly available large-scale dataset with 17,000 high-resolution 3D multi-class cardiac meshes and point-clouds, to establish a general benchmark for this emerging research direction. Extensive cross-domain experiments on HeartCompv1 and UK Biobank demonstrate that HeartFormer achieves robust, accurate, and generalizable performance, consistently surpassing state-of-the-art (SOTA) methods. Code and dataset will be released upon acceptance at: https://github.com/10Darren/HeartFormer.", "AI": {"tldr": "\u63d0\u51fa\u4e86HeartFormer\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u70b9\u4e91\u8865\u5168\u7f51\u7edc\uff0c\u7528\u4e8e\u4ece\u7535\u5f71MRI\u6570\u636e\u4e2d\u8fdb\u884c3D\u56db\u8154\u5fc3\u810f\u91cd\u5efa\uff0c\u5c06\u4f20\u7edf\u7684\u5355\u7c7b\u70b9\u4e91\u8865\u5168\u6269\u5c55\u5230\u591a\u7c7b\u3002", "motivation": "\u4f20\u7edf\u7535\u5f71MRI\u901a\u5e38\u53ea\u63d0\u4f9b\u5fc3\u810f\u76842D\u5207\u7247\u56fe\u50cf\uff0c\u9650\u5236\u4e86\u5bf9\u5065\u5eb7\u548c\u75c5\u7406\u6761\u4ef6\u4e0b\u5fc3\u810f\u5f62\u6001\u548c\u751f\u7406\u673a\u5236\u7684\u5168\u9762\u7406\u89e3\u3002", "method": "HeartFormer\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u8bed\u4e49\u611f\u77e5\u53cc\u7ed3\u6784Transformer\u7f51\u7edc\uff08SA-DSTNet\uff09\u548c\u8bed\u4e49\u611f\u77e5\u51e0\u4f55\u7279\u5f81\u7ec6\u5316Transformer\u7f51\u7edc\uff08SA-GFRTNet\uff09\u3002", "result": "\u5728HeartCompv1\u548c\u82f1\u56fd\u751f\u7269\u94f6\u884c\u7684\u5e7f\u6cdb\u8de8\u9886\u57df\u5b9e\u9a8c\u8868\u660e\uff0cHeartFormer\u5b9e\u73b0\u4e86\u7a33\u5065\u3001\u51c6\u786e\u548c\u53ef\u63a8\u5e7f\u7684\u6027\u80fd\uff0c\u59cb\u7ec8\u8d85\u8fc7\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "HeartFormer\u57283D\u5fc3\u810f\u91cd\u5efa\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728HeartCompv1\u548cUK Biobank\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002"}}
{"id": "2512.01017", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01017", "abs": "https://arxiv.org/abs/2512.01017", "authors": ["Xinhang Li", "Jingbo Zhou", "Pengfei Luo", "Yixiong Xiao", "Tong Xu"], "title": "ChartAnchor: Chart Grounding with Structural-Semantic Fidelity", "comment": null, "summary": "Recent advances in multimodal large language models (MLLMs) highlight the need for benchmarks that rigorously evaluate structured chart comprehension.Chart grounding refers to the bidirectional alignment between a chart's visual appearance and the structured semantics. This task requires models to produce a symbolic specification that faithfully captures the chart's visual and structural intent, while also recovering the underlying tabular data with precise values and relationships. Chart grounding directly reflects a model's capabilities in numerical reasoning, multimodal alignment, and structural reconstruction, and has several important applications in real-world scenarios.Existing benchmarks, constrained by narrow chart diversity, isolated tasks, and incomplete evaluation frameworks, fail to holistically assess grounding. To address this, we propose ChartAnchor, a comprehensive benchmark of 8k+ chart-table-code triples spanning 30 chart types drawn from diverse real-world and augmented sources. ChartAnchor introduces two complementary tasks: chart-to-code generation (synthesizing executable code to replicate charts) and controlled chart-to-table reconstruction (extracting exact data with predefined headers), enabling cross-validation of visual and numerical fidelity. A multi-level evaluation framework integrates semantic validation, stylistic analysis, and perceptual metrics to assess both structural and content-level correctness. Extensive experiments on MLLMs reveal critical limitations in numerical precision and code synthesis, emphasizing the need for structured reasoning beyond surface-level perception. By unifying symbolic and data-driven grounding, ChartAnchor establishes a rigorous foundation for chart grounding, offering meaningful insights for advancing MLLMs in scientific, financial, and industrial domains.", "AI": {"tldr": "\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u7531\u4e8e\u56fe\u8868\u591a\u6837\u6027\u7a84\u3001\u5b64\u7acb\u7684\u4efb\u52a1\u548c\u4e0d\u5b8c\u6574\u7684\u8bc4\u4f30\u6846\u67b6\u800c\u672a\u80fd\u5168\u9762\u8bc4\u4f30 grounding\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 ChartAnchor\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b 8k+ \u56fe\u8868-\u8868\u683c-\u4ee3\u7801\u4e09\u5143\u7ec4\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u8de8\u8d8a\u6765\u81ea\u4e0d\u540c\u771f\u5b9e\u4e16\u754c\u548c\u589e\u5f3a\u6765\u6e90\u7684 30 \u79cd\u56fe\u8868\u7c7b\u578b\u3002ChartAnchor \u5f15\u5165\u4e86\u4e24\u9879\u4e92\u8865\u7684\u4efb\u52a1\uff1a\u56fe\u8868\u5230\u4ee3\u7801\u751f\u6210\uff08\u5408\u6210\u53ef\u6267\u884c\u4ee3\u7801\u4ee5\u590d\u5236\u56fe\u8868\uff09\u548c\u53d7\u63a7\u56fe\u8868\u5230\u8868\u683c\u91cd\u5efa\uff08\u63d0\u53d6\u5177\u6709\u9884\u5b9a\u4e49\u6807\u9898\u7684\u7cbe\u786e\u6570\u636e\uff09\uff0c\u4ece\u800c\u80fd\u591f\u4ea4\u53c9\u9a8c\u8bc1\u89c6\u89c9\u548c\u6570\u503c\u4fdd\u771f\u5ea6\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u7684\u6700\u65b0\u8fdb\u5c55\u51f8\u663e\u4e86\u5bf9\u4e25\u683c\u8bc4\u4f30\u7ed3\u6784\u5316\u56fe\u8868\u7406\u89e3\u7684\u57fa\u51c6\u6d4b\u8bd5\u7684\u9700\u6c42\u3002\u56fe\u8868 grounding \u662f\u6307\u56fe\u8868\u89c6\u89c9\u5916\u89c2\u548c\u7ed3\u6784\u5316\u8bed\u4e49\u4e4b\u95f4\u7684\u53cc\u5411\u5bf9\u9f50\u3002\u8fd9\u9879\u4efb\u52a1\u8981\u6c42\u6a21\u578b\u751f\u6210\u4e00\u4e2a\u7b26\u53f7\u89c4\u8303\uff0c\u8be5\u89c4\u8303\u5fe0\u5b9e\u5730\u6355\u6349\u56fe\u8868\u7684\u89c6\u89c9\u548c\u7ed3\u6784\u610f\u56fe\uff0c\u540c\u65f6\u6062\u590d\u5177\u6709\u7cbe\u786e\u503c\u548c\u5173\u7cfb\u7684\u5e95\u5c42\u8868\u683c\u6570\u636e\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86 ChartAnchor\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b 8k+ \u56fe\u8868-\u8868\u683c-\u4ee3\u7801\u4e09\u5143\u7ec4\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u8de8\u8d8a\u6765\u81ea\u4e0d\u540c\u771f\u5b9e\u4e16\u754c\u548c\u589e\u5f3a\u6765\u6e90\u7684 30 \u79cd\u56fe\u8868\u7c7b\u578b\u3002ChartAnchor \u5f15\u5165\u4e86\u4e24\u9879\u4e92\u8865\u7684\u4efb\u52a1\uff1a\u56fe\u8868\u5230\u4ee3\u7801\u751f\u6210\uff08\u5408\u6210\u53ef\u6267\u884c\u4ee3\u7801\u4ee5\u590d\u5236\u56fe\u8868\uff09\u548c\u53d7\u63a7\u56fe\u8868\u5230\u8868\u683c\u91cd\u5efa\uff08\u63d0\u53d6\u5177\u6709\u9884\u5b9a\u4e49\u6807\u9898\u7684\u7cbe\u786e\u6570\u636e\uff09\uff0c\u4ece\u800c\u80fd\u591f\u4ea4\u53c9\u9a8c\u8bc1\u89c6\u89c9\u548c\u6570\u503c\u4fdd\u771f\u5ea6\u3002\u4e00\u4e2a\u591a\u5c42\u6b21\u7684\u8bc4\u4f30\u6846\u67b6\u96c6\u6210\u4e86\u8bed\u4e49\u9a8c\u8bc1\u3001\u98ce\u683c\u5206\u6790\u548c\u611f\u77e5\u6307\u6807\uff0c\u4ee5\u8bc4\u4f30\u7ed3\u6784\u548c\u5185\u5bb9\u7ea7\u522b\u7684\u6b63\u786e\u6027\u3002", "result": "\u5bf9 MLLM \u7684\u5e7f\u6cdb\u5b9e\u9a8c\u63ed\u793a\u4e86\u6570\u503c\u7cbe\u5ea6\u548c\u4ee3\u7801\u5408\u6210\u65b9\u9762\u7684\u5173\u952e\u9650\u5236\uff0c\u5f3a\u8c03\u4e86\u8d85\u8d8a\u8868\u9762\u611f\u77e5\u7684\u7ed3\u6784\u5316\u63a8\u7406\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u901a\u8fc7\u7edf\u4e00\u7b26\u53f7\u548c\u6570\u636e\u9a71\u52a8\u7684 grounding\uff0cChartAnchor \u4e3a\u56fe\u8868 grounding \u5efa\u7acb\u4e86\u4e25\u683c\u7684\u57fa\u7840\uff0c\u4e3a\u63a8\u8fdb MLLM \u5728\u79d1\u5b66\u3001\u91d1\u878d\u548c\u5de5\u4e1a\u9886\u57df\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u610f\u4e49\u7684\u89c1\u89e3\u3002"}}
{"id": "2512.00789", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00789", "abs": "https://arxiv.org/abs/2512.00789", "authors": ["Xiaodong Cai", "Hai Lin", "Shaoxiong Zhan", "Weiqi Luo", "Hong-Gee Kim", "Hongyan Hao", "Yu Yang", "Hai-Tao Zheng"], "title": "Auxiliary-Hyperparameter-Free Sampling: Entropy Equilibrium for Text Generation", "comment": null, "summary": "Token sampling strategies critically influence text generation quality in large language models (LLMs). However, existing methods introduce additional hyperparameters, requiring extensive tuning and complicating deployment. We present Entropy Equilibrium Sampling (EES), an auxiliary hyperparameter-free approach inspired by information theory that can dynamically adjust candidate sets by balancing normalized entropy with probability mass. We evaluate EES on both reasoning and generation tasks across a range of model architectures. Our results show that EES consistently performs well across temperature settings, delivering competitive accuracy and coherence while maintaining diversity. By eliminating the need for hyperparameter tuning, EES greatly simplifies deployment while improving performance. Code is available at https://github.com/shuanncai/EES", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u71b5\u5e73\u8861\u62bd\u6837 (EES) \u7684\u65e0\u8d85\u53c2\u6570\u65b9\u6cd5\uff0c\u7528\u4e8e\u52a8\u6001\u8c03\u6574\u5019\u9009\u96c6\uff0c\u5e73\u8861\u5f52\u4e00\u5316\u71b5\u548c\u6982\u7387\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684 token \u62bd\u6837\u7b56\u7565\u4f1a\u5f15\u5165\u989d\u5916\u7684\u8d85\u53c2\u6570\uff0c\u9700\u8981\u5927\u91cf\u7684\u8c03\u6574\uff0c\u4ece\u800c\u4f7f\u90e8\u7f72\u590d\u6742\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u4fe1\u606f\u8bba\u542f\u53d1\u7684\u8f85\u52a9\u65e0\u8d85\u53c2\u6570\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u901a\u8fc7\u5e73\u8861\u5f52\u4e00\u5316\u71b5\u4e0e\u6982\u7387\u8d28\u91cf\u6765\u52a8\u6001\u8c03\u6574\u5019\u9009\u96c6\u3002", "result": "EES \u5728\u5404\u79cd\u6a21\u578b\u67b6\u6784\u7684\u63a8\u7406\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u5728\u6e29\u5ea6\u8bbe\u7f6e\u4e2d\u63d0\u4f9b\u5177\u6709\u7ade\u4e89\u529b\u7684\u51c6\u786e\u6027\u548c\u8fde\u8d2f\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u591a\u6837\u6027\u3002", "conclusion": "EES \u6d88\u9664\u4e86\u5bf9\u8d85\u53c2\u6570\u8c03\u6574\u7684\u9700\u6c42\uff0c\u5927\u5927\u7b80\u5316\u4e86\u90e8\u7f72\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u6027\u80fd\u3002"}}
{"id": "2512.00389", "categories": ["cs.LG", "cs.GT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.00389", "abs": "https://arxiv.org/abs/2512.00389", "authors": ["Deep Patel", "Emmanouil-Vasileios Vlatakis-Gkaragkounis"], "title": "Solving Neural Min-Max Games: The Role of Architecture, Initialization & Dynamics", "comment": "Camera-ready for NeurIPS 2025 (including updated section on neural network initialization for experiments in Appendix C)", "summary": "Many emerging applications - such as adversarial training, AI alignment, and robust optimization - can be framed as zero-sum games between neural nets, with von Neumann-Nash equilibria (NE) capturing the desirable system behavior. While such games often involve non-convex non-concave objectives, empirical evidence shows that simple gradient methods frequently converge, suggesting a hidden geometric structure. In this paper, we provide a theoretical framework that explains this phenomenon through the lens of hidden convexity and overparameterization. We identify sufficient conditions - spanning initialization, training dynamics, and network width - that guarantee global convergence to a NE in a broad class of non-convex min-max games. To our knowledge, this is the first such result for games that involve two-layer neural networks. Technically, our approach is twofold: (a) we derive a novel path-length bound for the alternating gradient descent-ascent scheme in min-max games; and (b) we show that the reduction from a hidden convex-concave geometry to two-sided Polyak-\u0141ojasiewicz (P\u0141) min-max condition hold with high probability under overparameterization, using tools from random matrix theory.", "AI": {"tldr": "\u672c\u7814\u7a76\u81f4\u529b\u4e8e\u89e3\u91ca\u5728\u795e\u7ecf\u7f51\u7edc\u96f6\u548c\u535a\u5f08\u4e2d\uff0c\u5c3d\u7ba1\u76ee\u6807\u51fd\u6570\u975e\u51f8\u975e\u51f9\uff0c\u68af\u5ea6\u65b9\u6cd5\u5374\u80fd\u6536\u655b\u7684\u73b0\u8c61\uff0c\u901a\u8fc7\u9690\u85cf\u51f8\u6027\u548c\u8fc7\u5ea6\u53c2\u6570\u5316\u7684\u89c6\u89d2\u8fdb\u884c\u5206\u6790\u3002", "motivation": "\u8bb8\u591a\u65b0\u5174\u5e94\u7528\uff0c\u5982\u5bf9\u6297\u8bad\u7ec3\u3001AI\u5bf9\u9f50\u548c\u9c81\u68d2\u4f18\u5316\uff0c\u53ef\u4ee5\u88ab\u6784\u5efa\u4e3a\u795e\u7ecf\u7f51\u7edc\u4e4b\u95f4\u7684\u96f6\u548c\u535a\u5f08\uff0c\u800c\u51af\u00b7\u8bfa\u4f9d\u66fc-\u7eb3\u4ec0\u5747\u8861(NE)\u6355\u6349\u4e86\u7406\u60f3\u7684\u7cfb\u7edf\u884c\u4e3a\u3002\u867d\u7136\u8fd9\u4e9b\u535a\u5f08\u901a\u5e38\u6d89\u53ca\u975e\u51f8\u975e\u51f9\u7684\u76ee\u6807\uff0c\u4f46\u7ecf\u9a8c\u8bc1\u636e\u8868\u660e\uff0c\u7b80\u5355\u7684\u68af\u5ea6\u65b9\u6cd5\u7ecf\u5e38\u6536\u655b\uff0c\u8fd9\u8868\u660e\u5b58\u5728\u4e00\u4e2a\u9690\u85cf\u7684\u51e0\u4f55\u7ed3\u6784\u3002", "method": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u9690\u85cf\u51f8\u6027\u548c\u8fc7\u5ea6\u53c2\u6570\u5316\u7684\u89c6\u89d2\u6765\u89e3\u91ca\u8fd9\u79cd\u73b0\u8c61\u3002\u8be5\u7814\u7a76\u786e\u5b9a\u4e86\u5145\u5206\u6761\u4ef6\uff0c\u5305\u62ec\u521d\u59cb\u5316\u3001\u8bad\u7ec3\u52a8\u6001\u548c\u7f51\u7edc\u5bbd\u5ea6\uff0c\u4ee5\u4fdd\u8bc1\u5728\u5e7f\u6cdb\u7684\u975e\u51f8min-max\u535a\u5f08\u4e2d\u5168\u5c40\u6536\u655b\u5230NE\u3002\u8be5\u65b9\u6cd5\u4e3b\u8981\u5305\u62ec\uff1a(a)\u63a8\u5bfc\u4e86min-max\u535a\u5f08\u4e2d\u4ea4\u66ff\u68af\u5ea6\u4e0b\u964d-\u4e0a\u5347\u65b9\u6848\u7684\u65b0\u8def\u5f84\u957f\u5ea6\u754c\u9650\uff1b(b)\u4f7f\u7528\u968f\u673a\u77e9\u9635\u7406\u8bba\u7684\u5de5\u5177\uff0c\u8bc1\u660e\u4e86\u5728\u8fc7\u5ea6\u53c2\u6570\u5316\u4e0b\uff0c\u4ece\u9690\u85cf\u7684\u51f8\u51f9\u51e0\u4f55\u5230\u53cc\u8fb9Polyak-\u0141ojasiewicz (P\u0141) min-max\u6761\u4ef6\u7684\u7b80\u5316\u4ee5\u9ad8\u6982\u7387\u6210\u7acb\u3002", "result": "\u5bf9\u4e8e\u6d89\u53ca\u4e24\u5c42\u795e\u7ecf\u7f51\u7edc\u7684\u535a\u5f08\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u6b64\u7c7b\u7ed3\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4e00\u7c7b\u975e\u51f8min-max\u535a\u5f08\u63d0\u4f9b\u4e86\u5168\u5c40\u6536\u655b\u5230\u7eb3\u4ec0\u5747\u8861\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u63ed\u793a\u4e86\u9690\u85cf\u7684\u51f8\u6027\u548c\u8fc7\u5ea6\u53c2\u6570\u5316\u5728\u5b9e\u73b0\u6536\u655b\u4e2d\u7684\u4f5c\u7528\u3002"}}
{"id": "2512.00269", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00269", "abs": "https://arxiv.org/abs/2512.00269", "authors": ["Jun Wang", "Peirong Liu"], "title": "USB: Unified Synthetic Brain Framework for Bidirectional Pathology-Healthy Generation and Editing", "comment": "16 pages, 17 figures", "summary": "Understanding the relationship between pathological and healthy brain structures is fundamental to neuroimaging, connecting disease diagnosis and detection with modeling, prediction, and treatment planning. However, paired pathological-healthy data are extremely difficult to obtain, as they rely on pre- and post-treatment imaging, constrained by clinical outcomes and longitudinal data availability. Consequently, most existing brain image generation and editing methods focus on visual quality yet remain domain-specific, treating pathological and healthy image modeling independently. We introduce USB (Unified Synthetic Brain), the first end-to-end framework that unifies bidirectional generation and editing of pathological and healthy brain images. USB models the joint distribution of lesions and brain anatomy through a paired diffusion mechanism and achieves both pathological and healthy image generation. A consistency guidance algorithm further preserves anatomical consistency and lesion correspondence during bidirectional pathology-healthy editing. Extensive experiments on six public brain MRI datasets including healthy controls, stroke, and Alzheimer's patients, demonstrate USB's ability to produce diverse and realistic results. By establishing the first unified benchmark for brain image generation and editing, USB opens opportunities for scalable dataset creation and robust neuroimaging analysis. Code is available at https://github.com/jhuldr/USB.", "AI": {"tldr": "USB\uff1a\u9996\u4e2a\u7edf\u4e00\u75c5\u7406\u548c\u5065\u5eb7\u8111\u56fe\u50cf\u53cc\u5411\u751f\u6210\u548c\u7f16\u8f91\u7684\u7aef\u5230\u7aef\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u8111\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u65b9\u6cd5\u4fa7\u91cd\u89c6\u89c9\u8d28\u91cf\uff0c\u4f46\u9886\u57df\u7279\u5b9a\uff0c\u72ec\u7acb\u5904\u7406\u75c5\u7406\u548c\u5065\u5eb7\u56fe\u50cf\u5efa\u6a21\uff1b\u914d\u5bf9\u7684\u75c5\u7406-\u5065\u5eb7\u6570\u636e\u96be\u4ee5\u83b7\u5f97\u3002", "method": "\u901a\u8fc7\u914d\u5bf9\u6269\u6563\u673a\u5236\u5bf9\u75c5\u53d8\u548c\u8111\u89e3\u5256\u7ed3\u6784\u7684\u8054\u5408\u5206\u5e03\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u91c7\u7528\u4e00\u81f4\u6027\u6307\u5bfc\u7b97\u6cd5\u5728\u53cc\u5411\u75c5\u7406-\u5065\u5eb7\u7f16\u8f91\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u89e3\u5256\u7ed3\u6784\u4e00\u81f4\u6027\u548c\u75c5\u53d8\u5bf9\u5e94\u5173\u7cfb\u3002", "result": "\u5728\u5305\u62ec\u5065\u5eb7\u5bf9\u7167\u3001\u4e2d\u98ce\u548c\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u60a3\u8005\u5728\u5185\u7684\u516d\u4e2a\u516c\u5171\u8111MRI\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cUSB \u80fd\u591f\u4ea7\u751f\u591a\u6837\u5316\u548c\u903c\u771f\u7684\u7ed3\u679c\u3002", "conclusion": "USB \u5efa\u7acb\u4e86\u8111\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u7684\u7b2c\u4e00\u4e2a\u7edf\u4e00\u57fa\u51c6\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u6570\u636e\u96c6\u521b\u5efa\u548c\u7a33\u5065\u7684\u795e\u7ecf\u5f71\u50cf\u5206\u6790\u5f00\u8f9f\u4e86\u673a\u4f1a\u3002"}}
{"id": "2512.01020", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01020", "abs": "https://arxiv.org/abs/2512.01020", "authors": ["Jinu Lee", "Kyoung-Woon On", "Simeng Han", "Arman Cohan", "Julia Hockenmaier"], "title": "Evaluating Legal Reasoning Traces with Legal Issue Tree Rubrics", "comment": null, "summary": "Evaluating the quality of LLM-generated reasoning traces in expert domains (e.g., law) is essential for ensuring credibility and explainability, yet remains challenging due to the inherent complexity of such reasoning tasks. We introduce LEGIT (LEGal Issue Trees), a novel large-scale (24K instances) expert-level legal reasoning dataset with an emphasis on reasoning trace evaluation. We convert court judgments into hierarchical trees of opposing parties' arguments and the court's conclusions, which serve as rubrics for evaluating the issue coverage and correctness of the reasoning traces. We verify the reliability of these rubrics via human expert annotations and comparison with coarse, less informative rubrics. Using the LEGIT dataset, we show that (1) LLMs' legal reasoning ability is seriously affected by both legal issue coverage and correctness, and that (2) retrieval-augmented generation (RAG) and RL with rubrics bring complementary benefits for legal reasoning abilities, where RAG improves overall reasoning capability, whereas RL improves correctness albeit with reduced coverage.", "AI": {"tldr": "LEGIT: A new legal reasoning dataset (24K instances) for evaluating LLM-generated reasoning traces using hierarchical issue trees extracted from court judgments.", "motivation": "Evaluating the quality of LLM-generated reasoning in expert domains like law is challenging but crucial for credibility and explainability.", "method": "Court judgments are converted into hierarchical trees of arguments and conclusions, serving as rubrics for evaluation. Reliability is verified through human expert annotations.", "result": "LLMs' legal reasoning is significantly impacted by issue coverage and correctness. RAG and RL offer complementary benefits: RAG improves overall reasoning, while RL enhances correctness but reduces coverage.", "conclusion": "The LEGIT dataset reveals the strengths and weaknesses of LLMs in legal reasoning and the impact of different techniques like RAG and RL."}}
{"id": "2512.00829", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00829", "abs": "https://arxiv.org/abs/2512.00829", "authors": ["Md Mehrab Hossain Opi", "Sumaiya Khan", "Moshammad Farzana Rahman"], "title": "Accelerating Bangla NLP Tasks with Automatic Mixed Precision: Resource-Efficient Training Preserving Model Efficacy", "comment": null, "summary": "Training models for Natural Language Processing (NLP) requires substantial computational resources and time, posing significant challenges, especially for NLP development in Bangla, where access to high-end hardware is often limited. In this work, we explore automatic mixed precision (AMP) training as a means to improve computational efficiency without sacrificing model performance. By leveraging a dynamic mix of 16-bit and 32-bit floating-point computations, AMP lowers GPU memory requirements and speeds up training without degrading model performance. We evaluate AMP across four standard Bangla NLP tasks, namely sentiment analysis, named entity recognition, error classification, and question answering, using four transformer-based models: BanglaBERT, BanglishBERT, XLM-R, and mBERT. Our results demonstrate that AMP accelerates training by 44.5% and reduces memory consumption by 17.6%, while maintaining F-1 score within 99.7% of the full-precision baselines. This empirical study highlights AMP's potential to democratize access to state-of-the-art NLP capabilities in hardware-constrained settings by lowering computational barriers.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\uff08AMP\uff09\u8bad\u7ec3\uff0c\u4ee5\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\uff0c\u800c\u4e0d\u727a\u7272\u6a21\u578b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5b5f\u52a0\u62c9\u8bedNLP\u5f00\u53d1\u4e2d\u3002", "motivation": "\u5728\u5b5f\u52a0\u62c9\u8bed\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\uff0c\u7531\u4e8e\u786c\u4ef6\u8d44\u6e90\u7684\u9650\u5236\uff0c\u8bad\u7ec3\u6a21\u578b\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u65f6\u95f4\uff0c\u8fd9\u662f\u4e00\u4e2a\u5de8\u5927\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5229\u752816\u4f4d\u548c32\u4f4d\u6d6e\u70b9\u8ba1\u7b97\u7684\u52a8\u6001\u7ec4\u5408\uff0cAMP\u964d\u4f4e\u4e86GPU\u5185\u5b58\u9700\u6c42\uff0c\u5e76\u52a0\u5feb\u4e86\u8bad\u7ec3\u901f\u5ea6\uff0c\u540c\u65f6\u4e0d\u964d\u4f4e\u6a21\u578b\u6027\u80fd\u3002\u5728\u56db\u4e2a\u6807\u51c6\u7684\u5b5f\u52a0\u62c9\u8bedNLP\u4efb\u52a1\uff08\u60c5\u611f\u5206\u6790\u3001\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3001\u9519\u8bef\u5206\u7c7b\u548c\u95ee\u7b54\uff09\u4e2d\u4f7f\u7528\u56db\u4e2a\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff1aBanglaBERT\u3001BanglishBERT\u3001XLM-R\u548cmBERT\u8bc4\u4f30AMP\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cAMP\u52a0\u901f\u4e8644.5%\u7684\u8bad\u7ec3\u901f\u5ea6\uff0c\u5e76\u964d\u4f4e\u4e8617.6%\u7684\u5185\u5b58\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u6301F-1\u5206\u6570\u5728\u5168\u7cbe\u5ea6\u57fa\u7ebf\u768499.7%\u4ee5\u5185\u3002", "conclusion": "\u8fd9\u9879\u5b9e\u8bc1\u7814\u7a76\u5f3a\u8c03\u4e86AMP\u901a\u8fc7\u964d\u4f4e\u8ba1\u7b97\u58c1\u5792\uff0c\u6709\u53ef\u80fd\u5728\u786c\u4ef6\u53d7\u9650\u7684\u73af\u5883\u4e2d\u666e\u53ca\u6700\u5148\u8fdb\u7684NLP\u80fd\u529b\u3002"}}
{"id": "2512.00391", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00391", "abs": "https://arxiv.org/abs/2512.00391", "authors": ["Zhikang Chen", "Sen Cui", "Deheng Ye", "Min Zhang", "Gang Niu", "Yu Zhang", "Masashi Sugiyama", "Tingting Zhu"], "title": "From Coefficients to Directions: Rethinking Model Merging with Directional Alignment", "comment": null, "summary": "Model merging has emerged as a practical paradigm for integrating multiple independently trained models into a single model without joint retraining. Previous studies have demonstrated the effectiveness of combining parameters through strategies such as parameter decomposition, coefficient optimization, and subspace learning, significantly reducing the need for expensive joint training and achieving strong empirical performance across diverse tasks. However, these approaches predominantly treat merging as a problem of parameter space decomposition or fusion coefficient optimization, while overlooking the critical role of directional information in both parameter and feature spaces. In practice, na\u00efve merging introduces inconsistencies in dominant parameter directions and disrupts structural coherence across models, which can degrade performance. Moreover, coefficient-based optimization methods implicitly assume compatible feature-space directions across models. However, Neural Collapse indicates that class features follow structured directional patterns, which may differ across independently trained models, making coefficient optimization alone insufficient. In this work, we emphasize the importance of \\emph{directional alignment} and introduce a unified geometric framework, \\emph{Merging with Directional Alignment} (\\method{}), which aligns directional structures consistently in both the parameter and feature spaces. Our analysis shows that directional alignment improves structural coherence, and extensive experiments across benchmarks, model scales, and task configurations further validate the effectiveness of our approach.", "AI": {"tldr": "\u6a21\u578b\u5408\u5e76\u662f\u4e00\u79cd\u5c06\u591a\u4e2a\u72ec\u7acb\u8bad\u7ec3\u7684\u6a21\u578b\u96c6\u6210\u5230\u5355\u4e2a\u6a21\u578b\u4e2d\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u65e0\u9700\u8054\u5408\u91cd\u65b0\u8bad\u7ec3\u3002", "motivation": "\u5148\u524d\u7684\u65b9\u6cd5\u4e3b\u8981\u5c06\u5408\u5e76\u89c6\u4e3a\u53c2\u6570\u7a7a\u95f4\u5206\u89e3\u6216\u878d\u5408\u7cfb\u6570\u4f18\u5316\u95ee\u9898\uff0c\u800c\u5ffd\u7565\u4e86\u65b9\u5411\u4fe1\u606f\u5728\u53c2\u6570\u548c\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002\u7b80\u5355\u7684\u5408\u5e76\u4f1a\u5f15\u5165\u53c2\u6570\u65b9\u5411\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u5e76\u7834\u574f\u6a21\u578b\u4e4b\u95f4\u7684\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u4ece\u800c\u964d\u4f4e\u6027\u80fd\u3002\u6b64\u5916\uff0c\u57fa\u4e8e\u7cfb\u6570\u7684\u4f18\u5316\u65b9\u6cd5\u9690\u542b\u5730\u5047\u8bbe\u4e86\u6a21\u578b\u4e4b\u95f4\u517c\u5bb9\u7684\u7279\u5f81\u7a7a\u95f4\u65b9\u5411\uff0c\u8fd9\u4f7f\u5f97\u5355\u72ec\u7684\u7cfb\u6570\u4f18\u5316\u662f\u4e0d\u591f\u7684\u3002", "method": "\u6211\u4eec\u5f3a\u8c03\u4e86\u65b9\u5411\u5bf9\u9f50\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u51e0\u4f55\u6846\u67b6\uff0c\u5373\u5177\u6709\u65b9\u5411\u5bf9\u9f50\u7684\u5408\u5e76\uff08Merging with Directional Alignment\uff09\u3002", "result": "\u6211\u4eec\u7684\u5206\u6790\u8868\u660e\uff0c\u65b9\u5411\u5bf9\u9f50\u63d0\u9ad8\u4e86\u7ed3\u6784\u4e00\u81f4\u6027\u3002", "conclusion": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u3001\u6a21\u578b\u89c4\u6a21\u548c\u4efb\u52a1\u914d\u7f6e\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2512.00275", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00275", "abs": "https://arxiv.org/abs/2512.00275", "authors": ["Yi Liu", "Yi Wan", "Xinyi Liu", "Qiong Wu", "Panwang Xia", "Xuejun Huang", "Yongjun Zhang"], "title": "HIMOSA: Efficient Remote Sensing Image Super-Resolution with Hierarchical Mixture of Sparse Attention", "comment": null, "summary": "In remote sensing applications, such as disaster detection and response, real-time efficiency and model lightweighting are of critical importance. Consequently, existing remote sensing image super-resolution methods often face a trade-off between model performance and computational efficiency. In this paper, we propose a lightweight super-resolution framework for remote sensing imagery, named HIMOSA. Specifically, HIMOSA leverages the inherent redundancy in remote sensing imagery and introduces a content-aware sparse attention mechanism, enabling the model to achieve fast inference while maintaining strong reconstruction performance. Furthermore, to effectively leverage the multi-scale repetitive patterns found in remote sensing imagery, we introduce a hierarchical window expansion and reduce the computational complexity by adjusting the sparsity of the attention. Extensive experiments on multiple remote sensing datasets demonstrate that our method achieves state-of-the-art performance while maintaining computational efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u9065\u611f\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u6846\u67b6HIMOSA\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u9065\u611f\u5e94\u7528\u4e2d\uff0c\u5b9e\u65f6\u6548\u7387\u548c\u6a21\u578b\u8f7b\u91cf\u5316\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5728\u6a21\u578b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u9762\u4e34\u6743\u8861\u3002", "method": "\u5229\u7528\u9065\u611f\u56fe\u50cf\u7684\u56fa\u6709\u5197\u4f59\uff0c\u5f15\u5165\u5185\u5bb9\u611f\u77e5\u7684\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u91c7\u7528\u5206\u5c42\u7a97\u53e3\u6269\u5c55\u6765\u6709\u6548\u5229\u7528\u9065\u611f\u56fe\u50cf\u4e2d\u7684\u591a\u5c3a\u5ea6\u91cd\u590d\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u8c03\u6574\u6ce8\u610f\u529b\u7684\u7a00\u758f\u6027\u6765\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u5728\u591a\u4e2a\u9065\u611f\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u9065\u611f\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2512.01045", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01045", "abs": "https://arxiv.org/abs/2512.01045", "authors": ["Shenxi Liu", "Kan Li", "Mingyang Zhao", "Yuhang Tian", "Shoujun Zhou", "Bin Li"], "title": "Med-CRAFT: Automated Construction of Interpretable and Multi-Hop Video Workloads via Knowledge Graph Traversal", "comment": "8 pages, 7 figures", "summary": "The scarcity of high-quality, logically annotated video datasets remains a primary bottleneck in advancing Multi-Modal Large Language Models (MLLMs) for the medical domain. Traditional manual annotation is prohibitively expensive and non-scalable, while existing synthetic methods often suffer from stochastic hallucinations and a lack of logical interpretability. To address these challenges, we introduce \\textbf{\\PipelineName}, a novel neuro-symbolic data engineering framework that formalizes benchmark synthesis as a deterministic graph traversal process. Unlike black-box generative approaches, Med-CRAFT extracts structured visual primitives (e.g., surgical instruments, anatomical boundaries) from raw video streams and instantiates them into a dynamic Spatiotemporal Knowledge Graph. By anchoring query generation to valid paths within this graph, we enforce a rigorous Chain-of-Thought (CoT) provenance for every synthesized benchmark item. We instantiate this pipeline to produce M3-Med-Auto, a large-scale medical video reasoning benchmark exhibiting fine-grained temporal selectivity and multi-hop logical complexity. Comprehensive evaluations demonstrate that our automated pipeline generates query workloads with complexity comparable to expert-curated datasets. Furthermore, a logic alignment analysis reveals a high correlation between the prescribed graph topology and the reasoning steps of state-of-the-art MLLMs, validating the system's capability to encode verifiable logic into visual-linguistic benchmarks. This work paves the way for scalable, low-cost construction of robust evaluation protocols in critical domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u7b26\u53f7\u6570\u636e\u5de5\u7a0b\u6846\u67b6\uff0c\u7528\u4e8e\u5408\u6210\u9ad8\u8d28\u91cf\u3001\u903b\u8f91\u6ce8\u91ca\u7684\u533b\u7597\u89c6\u9891\u6570\u636e\u96c6\uff0c\u4ee5\u89e3\u51b3\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u9886\u57df\u5e94\u7528\u4e2d\u6570\u636e\u96c6\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "motivation": "\u9ad8\u8d28\u91cf\u3001\u903b\u8f91\u6ce8\u91ca\u7684\u89c6\u9891\u6570\u636e\u96c6\u7684\u7a00\u7f3a\u6027\u662f\u63a8\u8fdb\u533b\u5b66\u9886\u57df\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u4e3b\u8981\u74f6\u9888\u3002\u4f20\u7edf\u7684\u624b\u52a8\u6ce8\u91ca\u6210\u672c\u9ad8\u6602\u4e14\u4e0d\u53ef\u6269\u5c55\uff0c\u800c\u73b0\u6709\u7684\u5408\u6210\u65b9\u6cd5\u901a\u5e38\u5b58\u5728\u968f\u673a\u5e7b\u89c9\u548c\u7f3a\u4e4f\u903b\u8f91\u53ef\u89e3\u91ca\u6027\u7684\u95ee\u9898\u3002", "method": "\u8be5\u6846\u67b6\u4ece\u539f\u59cb\u89c6\u9891\u6d41\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u7684\u89c6\u89c9\u539f\u8bed\uff08\u4f8b\u5982\uff0c\u624b\u672f\u5668\u68b0\u3001\u89e3\u5256\u8fb9\u754c\uff09\uff0c\u5e76\u5c06\u5b83\u4eec\u5b9e\u4f8b\u5316\u4e3a\u52a8\u6001\u65f6\u7a7a\u77e5\u8bc6\u56fe\u3002\u901a\u8fc7\u5c06\u67e5\u8be2\u751f\u6210\u951a\u5b9a\u5230\u8be5\u56fe\u4e2d\u7684\u6709\u6548\u8def\u5f84\uff0c\u4e3a\u6bcf\u4e2a\u5408\u6210\u7684\u57fa\u51c6\u9879\u76ee\u5f3a\u5236\u6267\u884c\u4e25\u683c\u7684\u601d\u7ef4\u94fe\uff08CoT\uff09\u6eaf\u6e90\u3002", "result": "\u751f\u6210\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u533b\u7597\u89c6\u9891\u63a8\u7406\u57fa\u51c6M3-Med-Auto\uff0c\u8be5\u57fa\u51c6\u8868\u73b0\u51fa\u7cbe\u7ec6\u7684\u65f6\u95f4\u9009\u62e9\u6027\u548c\u591a\u8df3\u903b\u8f91\u590d\u6742\u6027\u3002\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u81ea\u52a8\u5316\u7ba1\u9053\u751f\u6210\u7684\u67e5\u8be2\u5de5\u4f5c\u8d1f\u8f7d\u7684\u590d\u6742\u6027\u4e0e\u4e13\u5bb6\u7b56\u5212\u7684\u6570\u636e\u96c6\u76f8\u5f53\u3002\u903b\u8f91\u5bf9\u9f50\u5206\u6790\u63ed\u793a\u4e86\u89c4\u5b9a\u7684\u56fe\u62d3\u6251\u4e0e\u6700\u5148\u8fdb\u7684MLLM\u7684\u63a8\u7406\u6b65\u9aa4\u4e4b\u95f4\u7684\u9ad8\u5ea6\u76f8\u5173\u6027\uff0c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u5c06\u53ef\u9a8c\u8bc1\u7684\u903b\u8f91\u7f16\u7801\u5230\u89c6\u89c9\u8bed\u8a00\u57fa\u51c6\u4e2d\u7684\u80fd\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5728\u5173\u952e\u9886\u57df\u4e2d\u53ef\u6269\u5c55\u3001\u4f4e\u6210\u672c\u5730\u6784\u5efa\u7a33\u5065\u7684\u8bc4\u4f30\u534f\u8bae\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2512.00837", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00837", "abs": "https://arxiv.org/abs/2512.00837", "authors": ["Yukang Lin", "Jiahao Shao", "Shuoran Jiang", "Wentao Zhu", "Bingjie Lu", "Xiangping Wu", "Joanna Siebert", "Qingcai Chen"], "title": "WaterSearch: A Quality-Aware Search-based Watermarking Framework for Large Language Models", "comment": null, "summary": "Watermarking acts as a critical safeguard in text generated by Large Language Models (LLMs). By embedding identifiable signals into model outputs, watermarking enables reliable attribution and enhances the security of machine-generated content. Existing approaches typically embed signals by manipulating token generation probabilities. Despite their effectiveness, these methods inherently face a trade-off between detectability and text quality: the signal strength and randomness required for robust watermarking tend to degrade the performance of downstream tasks.\n  In this paper, we design a novel embedding scheme that controls seed pools to facilitate diverse parallel generation of watermarked text. Based on that scheme, we propose WaterSearch, a sentence-level, search-based watermarking framework adaptable to a wide range of existing methods. WaterSearch enhances text quality by jointly optimizing two key aspects: 1) distribution fidelity and 2) watermark signal characteristics. Furthermore, WaterSearch is complemented by a sentence-level detection method with strong attack robustness. We evaluate our method on three popular LLMs across ten diverse tasks. Extensive experiments demonstrate that our method achieves an average performance improvement of 51.01\\% over state-of-the-art baselines at a watermark detectability strength of 95\\%. In challenging scenarios such as short text generation and low-entropy output generation, our method yields performance gains of 47.78\\% and 36.47\\%, respectively. Moreover, under different attack senarios including insertion, synonym substitution and paraphrase attasks, WaterSearch maintains high detectability, further validating its robust anti-attack capabilities. Our code is available at \\href{https://github.com/Yukang-Lin/WaterSearch}{https://github.com/Yukang-Lin/WaterSearch}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWaterSearch\u7684\u65b0\u7684\u57fa\u4e8e\u641c\u7d22\u7684\u6587\u672c\u6c34\u5370\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u63a7\u5236\u79cd\u5b50\u6c60\u6765\u4fc3\u8fdb\u5e26\u6c34\u5370\u6587\u672c\u7684\u5e76\u884c\u751f\u6210\uff0c\u4ece\u800c\u63d0\u9ad8\u6587\u672c\u8d28\u91cf\uff0c\u5e76\u5728\u5404\u79cd\u653b\u51fb\u4e0b\u4fdd\u6301\u9ad8\u53ef\u68c0\u6d4b\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6c34\u5370\u65b9\u6cd5\u5728\u53ef\u68c0\u6d4b\u6027\u548c\u6587\u672c\u8d28\u91cf\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u56e0\u4e3a\u5f3a\u5927\u7684\u6c34\u5370\u4fe1\u53f7\u4f1a\u964d\u4f4e\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u5d4c\u5165\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u63a7\u5236\u79cd\u5b50\u6c60\u4ee5\u4fc3\u8fdb\u5e26\u6c34\u5370\u6587\u672c\u7684\u5e76\u884c\u751f\u6210\uff0c\u5e76\u63d0\u51fa\u4e86WaterSearch\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u8054\u5408\u4f18\u5316\u5206\u5e03\u4fdd\u771f\u5ea6\u548c\u6c34\u5370\u4fe1\u53f7\u7279\u5f81\u6765\u589e\u5f3a\u6587\u672c\u8d28\u91cf\u3002\u6b64\u5916\uff0c\u8fd8\u91c7\u7528\u4e86\u4e00\u79cd\u5177\u6709\u5f3a\u5927\u6297\u653b\u51fb\u80fd\u529b\u7684\u53e5\u5b50\u7ea7\u68c0\u6d4b\u65b9\u6cd5\u3002", "result": "\u5728\u4e09\u4e2a\u6d41\u884c\u7684LLM\u548c\u5341\u4e2a\u4e0d\u540c\u7684\u4efb\u52a1\u4e2d\uff0cWaterSearch\u7684\u6027\u80fd\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u63d0\u9ad8\u4e8651.01\uff05\uff0c\u6c34\u5370\u53ef\u68c0\u6d4b\u5f3a\u5ea6\u4e3a95\uff05\u3002\u5728\u77ed\u6587\u672c\u751f\u6210\u548c\u4f4e\u71b5\u8f93\u51fa\u751f\u6210\u7b49\u5177\u6709\u6311\u6218\u6027\u7684\u573a\u666f\u4e2d\uff0c\u6027\u80fd\u5206\u522b\u63d0\u9ad8\u4e8647.78\uff05\u548c36.47\uff05\u3002\u5728\u4e0d\u540c\u7684\u653b\u51fb\u573a\u666f\u4e0b\uff0cWaterSearch\u4fdd\u6301\u4e86\u5f88\u9ad8\u7684\u53ef\u68c0\u6d4b\u6027\u3002", "conclusion": "WaterSearch\u5728\u63d0\u9ad8\u6587\u672c\u8d28\u91cf\u548c\u4fdd\u6301\u6c34\u5370\u53ef\u68c0\u6d4b\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u5177\u6709\u5f88\u5f3a\u7684\u6297\u653b\u51fb\u80fd\u529b\u3002"}}
{"id": "2512.00396", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.00396", "abs": "https://arxiv.org/abs/2512.00396", "authors": ["Andrea Procopio", "Marco Esposito", "Sara Raggiunto", "Andrey Gizdov", "Alberto Belli", "Paola Pierleoni"], "title": "Time-Series at the Edge: Tiny Separable CNNs for Wearable Gait Detection and Optimal Sensor Placement", "comment": null, "summary": "We study on-device time-series analysis for gait detection in Parkinson's disease (PD) from short windows of triaxial acceleration, targeting resource-constrained wearables and edge nodes. We compare magnitude thresholding to three 1D CNNs for time-series analysis: a literature baseline (separable convolutions) and two ultra-light models - one purely separable and one with residual connections. Using the BioStampRC21 dataset, 2 s windows at 30 Hz, and subject-independent leave-one-subject-out (LOSO) validation on 16 PwPD with chest-worn IMUs, our residual separable model (Model 2, 533 params) attains PR-AUC = 94.5%, F1 = 91.2%, MCC = 89.4%, matching or surpassing the baseline (5,552 params; PR-AUC = 93.7%, F1 = 90.5%, MCC = 88.5%) with approximately 10x fewer parameters. The smallest model (Model 1, 305 params) reaches PR-AUC = 94.0%, F1 = 91.0%, MCC = 89.1%. Thresholding obtains high recall (89.0%) but low precision (76.5%), yielding many false positives and high inter-subject variance. Sensor-position analysis (train-on-all) shows chest and thighs are most reliable; forearms degrade precision/recall due to non-gait arm motion; naive fusion of all sites does not outperform the best single site. Both compact CNNs execute within tight memory/latency budgets on STM32-class MCUs (sub-10 ms on low-power boards), enabling on-sensor gating of transmission/storage. Overall, ultra-light separable CNNs provide a superior accuracy-efficiency-generalization trade-off to fixed thresholds for wearable PD gait detection and underscore the value of tailored time-series models for edge deployment.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u4f7f\u7528\u8d85\u8f7b\u91cf\u7ea7\u53ef\u5206\u79bb\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u5728\u8d44\u6e90\u53d7\u9650\u7684\u53ef\u7a7f\u6234\u8bbe\u5907\u4e0a\u8fdb\u884c\u5e15\u91d1\u68ee\u75c5\uff08PD\uff09\u6b65\u6001\u68c0\u6d4b\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u4e0a\u90fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5728\u53ef\u7a7f\u6234\u8bbe\u5907\u548c\u8fb9\u7f18\u8282\u70b9\u4e0a\uff0c\u4f7f\u7528\u77ed\u7a97\u53e3\u4e09\u8f74\u52a0\u901f\u5ea6\u8fdb\u884c\u5e15\u91d1\u68ee\u75c5(PD)\u6b65\u6001\u68c0\u6d4b\u7684\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u7814\u7a76\u3002", "method": "\u6bd4\u8f83\u5e45\u5ea6\u9608\u503c\u6cd5\u548c\u4e09\u79cd\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u7684\u4e00\u7ef4\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff1a\u4e00\u4e2a\u6587\u732e\u57fa\u7ebf\uff08\u53ef\u5206\u79bb\u5377\u79ef\uff09\u548c\u4e24\u4e2a\u8d85\u8f7b\u91cf\u7ea7\u6a21\u578b\u2014\u2014\u4e00\u4e2a\u7eaf\u7cb9\u53ef\u5206\u79bb\uff0c\u4e00\u4e2a\u5e26\u6709\u6b8b\u5dee\u8fde\u63a5\u3002", "result": "\u6b8b\u5dee\u53ef\u5206\u79bb\u6a21\u578b\uff08Model 2, 533 \u4e2a\u53c2\u6570\uff09\u8fbe\u5230 PR-AUC = 94.5%\uff0cF1 = 91.2%\uff0cMCC = 89.4%\uff0c\u5728\u53c2\u6570\u51cf\u5c11\u7ea6 10 \u500d\u7684\u60c5\u51b5\u4e0b\uff0c\u5339\u914d\u6216\u8d85\u8fc7\u4e86\u57fa\u7ebf\uff085,552 \u4e2a\u53c2\u6570\uff1bPR-AUC = 93.7%\uff0cF1 = 90.5%\uff0cMCC = 88.5%\uff09\u3002\u6700\u5c0f\u7684\u6a21\u578b\uff08Model 1\uff0c305 \u4e2a\u53c2\u6570\uff09\u8fbe\u5230 PR-AUC = 94.0%\uff0cF1 = 91.0%\uff0cMCC = 89.1%\u3002", "conclusion": "\u8d85\u8f7b\u91cf\u7ea7\u53ef\u5206\u79bb\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4e3a\u53ef\u7a7f\u6234PD\u6b65\u6001\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4f18\u4e8e\u56fa\u5b9a\u9608\u503c\u7684\u7cbe\u5ea6-\u6548\u7387-\u6cdb\u5316\u6298\u8877\u65b9\u6848\uff0c\u5e76\u5f3a\u8c03\u4e86\u4e3a\u8fb9\u7f18\u90e8\u7f72\u5b9a\u5236\u7684\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u7684\u4ef7\u503c\u3002"}}
{"id": "2512.00281", "categories": ["cs.CV", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2512.00281", "abs": "https://arxiv.org/abs/2512.00281", "authors": ["Sylvain Bodard", "Pierre Baudot", "Benjamin Renoust", "Charles Voyton", "Gwendoline De Bie", "Ezequiel Geremia", "Van-Khoa Le", "Danny Francis", "Pierre-Henri Siot", "Yousra Haddou", "Vincent Bobin", "Jean-Christophe Brisset", "Carey C. Thomson", "Valerie Bourdes", "Benoit Huet"], "title": "Rethinking Lung Cancer Screening: AI Nodule Detection and Diagnosis Outperforms Radiologists, Leading Models, and Standards Beyond Size and Growth", "comment": "25 pages, 8 figures, with supplementary information containing 11 figures", "summary": "Early detection of malignant lung nodules is critical, but its dependence on size and growth in screening inherently delays diagnosis. We present an AI system that redefines lung cancer screening by performing both detection and malignancy diagnosis directly at the nodule level on low-dose CT scans. To address limitations in dataset scale and explainability, we designed an ensemble of shallow deep learning and feature-based specialized models. Trained and evaluated on 25,709 scans with 69,449 annotated nodules, the system outperforms radiologists, Lung-RADS, and leading AI models (Sybil, Brock, Google, Kaggle). It achieves an area under the receiver operating characteristic curve (AUC) of 0.98 internally and 0.945 on an independent cohort. With 0.5 false positives per scan at 99.3\\% sensitivity, it addresses key barriers to AI adoption. Critically, it outperforms radiologists across all nodule sizes and stages, excelling in stage 1 cancers, and all growth-based metrics, including the least accurate: Volume-Doubling Time. It also surpasses radiologists by up to one year in diagnosing indeterminate and slow-growing nodules.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684AI\u7cfb\u7edf\uff0c\u7528\u4e8e\u5728\u4f4e\u5242\u91cfCT\u626b\u63cf\u4e2d\u76f4\u63a5\u5728\u7ed3\u8282\u6c34\u5e73\u4e0a\u8fdb\u884c\u80ba\u764c\u7b5b\u67e5\uff0c\u80fd\u591f\u540c\u65f6\u8fdb\u884c\u68c0\u6d4b\u548c\u6076\u6027\u8bca\u65ad\u3002", "motivation": "\u65e9\u671f\u53d1\u73b0\u6076\u6027\u80ba\u7ed3\u8282\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u5bf9\u7b5b\u67e5\u4e2d\u5927\u5c0f\u548c\u751f\u957f\u7684\u4f9d\u8d56\u6027\u4f1a\u5ef6\u8fdf\u8bca\u65ad\u3002", "method": "\u8be5\u7cfb\u7edf\u91c7\u7528\u4e86\u4e00\u79cd\u6d45\u5c42\u6df1\u5ea6\u5b66\u4e60\u548c\u57fa\u4e8e\u7279\u5f81\u7684\u4e13\u4e1a\u6a21\u578b\u7684\u96c6\u6210\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u6570\u636e\u96c6\u89c4\u6a21\u548c\u53ef\u89e3\u91ca\u6027\u7684\u9650\u5236\u3002", "result": "\u8be5\u7cfb\u7edf\u5728\u5305\u542b69,449\u4e2a\u5e26\u6ce8\u91ca\u7ed3\u8282\u768425,709\u6b21\u626b\u63cf\u4e2d\u8fdb\u884c\u4e86\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u653e\u5c04\u79d1\u533b\u751f\u3001Lung-RADS\u548c\u9886\u5148\u7684AI\u6a21\u578b\uff0c\u5185\u90e8\u6d4b\u8bd5AUC\u4e3a0.98\uff0c\u72ec\u7acb\u961f\u5217\u6d4b\u8bd5AUC\u4e3a0.945\u3002\u572899.3%\u7684\u7075\u654f\u5ea6\u4e0b\uff0c\u6bcf\u6b21\u626b\u63cf\u7684\u5047\u9633\u6027\u7387\u4e3a0.5\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5728\u6240\u6709\u7ed3\u8282\u5927\u5c0f\u548c\u9636\u6bb5\uff08\u5305\u62ecI\u671f\u764c\u75c7\uff09\u4ee5\u53ca\u6240\u6709\u57fa\u4e8e\u751f\u957f\u7684\u6307\u6807\uff08\u5305\u62ec\u4f53\u79ef\u500d\u589e\u65f6\u95f4\uff09\u65b9\u9762\u5747\u4f18\u4e8e\u653e\u5c04\u79d1\u533b\u751f\uff0c\u5e76\u4e14\u5728\u8bca\u65ad\u4e0d\u786e\u5b9a\u548c\u7f13\u6162\u751f\u957f\u7684\u7ed3\u8282\u65b9\u9762\uff0c\u8be5\u7cfb\u7edf\u6bd4\u653e\u5c04\u79d1\u533b\u751f\u63d0\u524d\u4e00\u5e74\u3002"}}
{"id": "2512.01046", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01046", "abs": "https://arxiv.org/abs/2512.01046", "authors": ["Hadi Nekoei", "Alexandre Blondin Mass\u00e9", "Rachid Hassani", "Sarath Chandar", "Vincent Mai"], "title": "Shielded Controller Units for RL with Operational Constraints Applied to Remote Microgrids", "comment": null, "summary": "Reinforcement learning (RL) is a powerful framework for optimizing decision-making in complex systems under uncertainty, an essential challenge in real-world settings, particularly in the context of the energy transition. A representative example is remote microgrids that supply power to communities disconnected from the main grid. Enabling the energy transition in such systems requires coordinated control of renewable sources like wind turbines, alongside fuel generators and batteries, to meet demand while minimizing fuel consumption and battery degradation under exogenous and intermittent load and wind conditions. These systems must often conform to extensive regulations and complex operational constraints. To ensure that RL agents respect these constraints, it is crucial to provide interpretable guarantees. In this paper, we introduce Shielded Controller Units (SCUs), a systematic and interpretable approach that leverages prior knowledge of system dynamics to ensure constraint satisfaction. Our shield synthesis methodology, designed for real-world deployment, decomposes the environment into a hierarchical structure where each SCU explicitly manages a subset of constraints. We demonstrate the effectiveness of SCUs on a remote microgrid optimization task with strict operational requirements. The RL agent, equipped with SCUs, achieves a 24% reduction in fuel consumption without increasing battery degradation, outperforming other baselines while satisfying all constraints. We hope SCUs contribute to the safe application of RL to the many decision-making challenges linked to the energy transition.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5c4f\u853d\u63a7\u5236\u5668\u5355\u5143\uff08SCU\uff09\u7684\u7cfb\u7edf\u6027\u4e14\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u7cfb\u7edf\u52a8\u529b\u5b66\u7684\u5148\u9a8c\u77e5\u8bc6\u6765\u786e\u4fdd\u6ee1\u8db3\u7ea6\u675f\u3002", "motivation": "\u5728\u80fd\u6e90\u8f6c\u578b\u80cc\u666f\u4e0b\uff0c\u73b0\u5b9e\u4e16\u754c\u4e2d\u4f18\u5316\u4e0d\u786e\u5b9a\u6027\u4e0b\u590d\u6742\u7cfb\u7edf\u4e2d\u7684\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u4f8b\u5982\uff0c\u9700\u8981\u534f\u8c03\u63a7\u5236\u53ef\u518d\u751f\u80fd\u6e90\u3001\u71c3\u6599\u53d1\u7535\u673a\u548c\u7535\u6c60\u7684\u8fdc\u7a0b\u5fae\u7535\u7f51\uff0c\u4ee5\u5728\u6ee1\u8db3\u9700\u6c42\u7684\u540c\u65f6\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u71c3\u6599\u6d88\u8017\u548c\u7535\u6c60\u9000\u5316\u3002", "method": "\u5c06\u73af\u5883\u5206\u89e3\u4e3a\u5206\u5c42\u7ed3\u6784\uff0c\u5176\u4e2d\u6bcf\u4e2aSCU\u663e\u5f0f\u7ba1\u7406\u7ea6\u675f\u7684\u5b50\u96c6\u3002", "result": "\u914d\u5907 SCU \u7684 RL \u667a\u80fd\u4f53\u5728\u4e0d\u589e\u52a0\u7535\u6c60\u9000\u5316\u7684\u60c5\u51b5\u4e0b\uff0c\u71c3\u6599\u6d88\u8017\u51cf\u5c11\u4e86 24%\uff0c\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\uff0c\u540c\u65f6\u6ee1\u8db3\u6240\u6709\u7ea6\u675f\u3002", "conclusion": "SCU \u6709\u52a9\u4e8e\u5c06 RL \u5b89\u5168\u5730\u5e94\u7528\u4e8e\u4e0e\u80fd\u6e90\u8f6c\u578b\u76f8\u5173\u7684\u8bb8\u591a\u51b3\u7b56\u6311\u6218\u3002"}}
{"id": "2512.00878", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00878", "abs": "https://arxiv.org/abs/2512.00878", "authors": ["Chunlin Tian", "Xuyang Wei", "Huanrong Liu", "Zhijiang Guo", "Li Li"], "title": "Less is More: Resource-Efficient Low-Rank Adaptation", "comment": "18 pages, 7 figures", "summary": "Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning (PEFT) method for Large Language Models (LLMs), but it still incurs notable overhead and suffers from parameter interference in complex datasets. While re- cent works decouple LoRA update matrices to exploit matrix-wise asymmetry, training costs remain high. We revisit LoRA from the perspective of inter-matrix and intra-layer parameter redundancy and propose Resource-Efficient Low-Rank Adaptation, EffiLoRA, a lightweight and generalizable approach for language, multimodal, and diffusion models. EffiLoRA employs a unified A matrix across all transformer layers and introduces a runtime selective B matrices up- date to dynamically trade-off the system resource budget and model performance. EffiLoRA consistently outperforms LoRA across diverse modalities, including commonsense reasoning, visual instruction tuning, and image generation, demon- strating improved efficiency and robustness.", "AI": {"tldr": "EffiLoRA outperforms LoRA in diverse modalities by employing a unified A matrix and a runtime selective B matrices update, which dynamically trade-off the system resource budget and model performance.", "motivation": "LoRA has notable overhead and suffers from parameter interference in complex datasets. Recent works decouple LoRA update matrices, but training costs remain high.", "method": "EffiLoRA employs a unified A matrix across all transformer layers and introduces a runtime selective B matrices update.", "result": "EffiLoRA consistently outperforms LoRA across diverse modalities, including commonsense reasoning, visual instruction tuning, and image generation, demonstrating improved efficiency and robustness.", "conclusion": "EffiLoRA is a lightweight and generalizable approach for language, multimodal, and diffusion models."}}
{"id": "2512.00403", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00403", "abs": "https://arxiv.org/abs/2512.00403", "authors": ["Xiao Wu", "Ting-Zhu Huang", "Liang-Jian Deng", "Xiaobing Yu", "Yu Zhong", "Shangqi Deng", "Ufaq Khan", "Jianghao Wu", "Xiaofeng Liu", "Imran Razzak", "Xiaojun Chang", "Yutong Xie"], "title": "SelfAI: Building a Self-Training AI System with LLM Agents", "comment": null, "summary": "Recent work on autonomous scientific discovery has leveraged LLM-based agents to integrate problem specification, experiment planning, and execution into end-to-end systems. However, these frameworks are often confined to narrow application domains, offer limited real-time interaction with researchers, and lack principled mechanisms for determining when to halt exploration, resulting in inefficiencies, reproducibility challenges, and under-utilized human expertise. To address these gaps, we propose \\textit{SelfAI}, a general multi-agent platform that combines a User Agent for translating high-level research objectives into standardized experimental configurations, a Cognitive Agent powered by LLMs with optimal stopping criteria to iteratively refine hyperparameter searches, and an Experiment Manager responsible for orchestrating parallel, fault-tolerant training workflows across heterogeneous hardware while maintaining a structured knowledge base for continuous feedback. We further introduce two novel evaluation metrics, Score and $\\text{AUP}_D$, to quantify discovery efficiency and search diversity. Across regression, NLP, computer vision, scientific computing, medical imaging, and drug discovery benchmarks, SelfAI consistently achieves strong performance and reduces redundant trials compared to classical Bayesian optimization and LLM-based baselines, while enabling seamless interaction with human researchers.", "AI": {"tldr": "SelfAI is a multi-agent platform that improves autonomous scientific discovery by addressing limitations of existing LLM-based agents, such as narrow application domains and lack of real-time interaction. It introduces a User Agent, Cognitive Agent, and Experiment Manager for translating research objectives, refining hyperparameter searches, and orchestrating parallel training workflows. The platform also includes two new evaluation metrics, Score and AUP_D, to quantify discovery efficiency and search diversity. SelfAI achieves strong performance across various benchmarks, reduces redundant trials, and enables seamless interaction with human researchers.", "motivation": "Existing LLM-based agents for autonomous scientific discovery have limitations in application domains, real-time interaction, and mechanisms for halting exploration, leading to inefficiencies and reproducibility challenges.", "method": "The paper proposes SelfAI, a multi-agent platform with a User Agent, Cognitive Agent, and Experiment Manager. The Cognitive Agent uses LLMs with optimal stopping criteria to refine hyperparameter searches. The platform also introduces two new evaluation metrics, Score and AUP_D.", "result": "SelfAI consistently achieves strong performance across regression, NLP, computer vision, scientific computing, medical imaging, and drug discovery benchmarks. It also reduces redundant trials compared to classical Bayesian optimization and LLM-based baselines.", "conclusion": "SelfAI addresses the limitations of existing LLM-based agents for autonomous scientific discovery by providing a general multi-agent platform with improved efficiency, reproducibility, and human interaction. The platform's performance is validated across various benchmarks, demonstrating its effectiveness in reducing redundant trials and enabling seamless interaction with human researchers."}}
{"id": "2512.00294", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.00294", "abs": "https://arxiv.org/abs/2512.00294", "authors": ["Lixing Guo", "Tobias H\u00f6llerer"], "title": "Words into World: A Task-Adaptive Agent for Language-Guided Spatial Retrieval in AR", "comment": null, "summary": "Traditional augmented reality (AR) systems predominantly rely on fixed class detectors or fiducial markers, limiting their ability to interpret complex, open-vocabulary natural language queries. We present a modular AR agent system that integrates multimodal large language models (MLLMs) with grounded vision models to enable relational reasoning in space and language-conditioned spatial retrieval in physical environments. Our adaptive task agent coordinates MLLMs and coordinate-aware perception tools to address varying query complexities, ranging from simple object identification to multi-object relational reasoning, while returning meter-accurate 3D anchors. It constructs dynamic AR scene graphs encoding nine typed relations (spatial, structural-semantic, causal-functional), enabling MLLMs to understand not just what objects exist, but how they relate and interact in 3D space. Through task-adaptive region-of-interest highlighting and contextual spatial retrieval, the system guides human attention to information-dense areas while supporting human-in-the-loop refinement. The agent dynamically invokes coordinate-aware tools for complex queries-selection, measurement, comparison, and actuation-grounding language understanding in physical operations. The modular architecture supports plug-and-use vision-language models without retraining, establishing AR agents as intermediaries that augment MLLMs with real-world spatial intelligence for interactive scene understanding. We also introduce GroundedAR-Bench, an evaluation framework for language-driven real world localization and relation grounding across diverse environments.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u6a21\u5757\u5316AR\u4ee3\u7406\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u96c6\u6210\u4e86\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u4e0e\u57fa\u4e8e\u63a5\u5730\u7684\u89c6\u89c9\u6a21\u578b\uff0c\u4ee5\u5b9e\u73b0\u7a7a\u95f4\u4e2d\u7684\u5173\u7cfb\u63a8\u7406\u548c\u7269\u7406\u73af\u5883\u4e2d\u7684\u8bed\u8a00\u6761\u4ef6\u7a7a\u95f4\u68c0\u7d22\u3002", "motivation": "\u4f20\u7edf\u7684\u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u7c7b\u68c0\u6d4b\u5668\u6216\u57fa\u51c6\u6807\u8bb0\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u89e3\u91ca\u590d\u6742\u7684\u5f00\u653e\u8bcd\u6c47\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u7684\u80fd\u529b\u3002", "method": "\u8be5\u7cfb\u7edf\u6784\u5efa\u52a8\u6001AR\u573a\u666f\u56fe\uff0c\u7f16\u7801\u4e5d\u79cd\u7c7b\u578b\u7684\u5173\u7cfb\uff08\u7a7a\u95f4\u3001\u7ed3\u6784\u8bed\u4e49\u3001\u56e0\u679c\u529f\u80fd\uff09\uff0c\u4f7fMLLM\u4e0d\u4ec5\u80fd\u7406\u89e3\u5b58\u5728\u7684\u5bf9\u8c61\uff0c\u8fd8\u80fd\u7406\u89e3\u5b83\u4eec\u5982\u4f55\u57283D\u7a7a\u95f4\u4e2d\u5173\u8054\u548c\u4ea4\u4e92\u3002\u8be5\u4ee3\u7406\u52a8\u6001\u8c03\u7528\u5750\u6807\u611f\u77e5\u5de5\u5177\u8fdb\u884c\u590d\u6742\u67e5\u8be2\u9009\u62e9\u3001\u6d4b\u91cf\u3001\u6bd4\u8f83\u548c\u9a71\u52a8\uff0c\u4ece\u800c\u5728\u7269\u7406\u64cd\u4f5c\u4e2d\u5b9e\u73b0\u57fa\u7840\u8bed\u8a00\u7406\u89e3\u3002", "result": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u4efb\u52a1\u81ea\u9002\u5e94\u611f\u5174\u8da3\u533a\u57df\u7a81\u51fa\u663e\u793a\u548c\u4e0a\u4e0b\u6587\u7a7a\u95f4\u68c0\u7d22\uff0c\u5f15\u5bfc\u4eba\u4eec\u7684\u6ce8\u610f\u529b\u96c6\u4e2d\u5728\u4fe1\u606f\u5bc6\u96c6\u533a\u57df\uff0c\u540c\u65f6\u652f\u6301\u4eba\u673a\u4ea4\u4e92\u6539\u8fdb\u3002\u6a21\u5757\u5316\u67b6\u6784\u652f\u6301\u5373\u63d2\u5373\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u4ece\u800c\u5c06AR\u4ee3\u7406\u5efa\u7acb\u4e3a\u4e2d\u4ecb\uff0c\u5229\u7528\u771f\u5b9e\u4e16\u754c\u7684\u7a7a\u95f4\u667a\u80fd\u589e\u5f3aMLLM\uff0c\u4ee5\u5b9e\u73b0\u4ea4\u4e92\u5f0f\u573a\u666f\u7406\u89e3\u3002", "conclusion": "\u6211\u4eec\u8fd8\u4ecb\u7ecd\u4e86GroundedAR-Bench\uff0c\u8fd9\u662f\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8de8\u4e0d\u540c\u73af\u5883\u7684\u8bed\u8a00\u9a71\u52a8\u7684\u771f\u5b9e\u4e16\u754c\u5b9a\u4f4d\u548c\u5173\u7cfb\u57fa\u7840\u3002"}}
{"id": "2512.01047", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01047", "abs": "https://arxiv.org/abs/2512.01047", "authors": ["Tanmay Ambadkar", "\u0110or\u0111e \u017dikeli\u0107", "Abhinav Verma"], "title": "Automating the Refinement of Reinforcement Learning Specifications", "comment": null, "summary": "Logical specifications have been shown to help reinforcement learning algorithms in achieving complex tasks. However, when a task is under-specified, agents might fail to learn useful policies. In this work, we explore the possibility of improving coarse-grained logical specifications via an exploration-guided strategy. We propose \\textsc{AutoSpec}, a framework that searches for a logical specification refinement whose satisfaction implies satisfaction of the original specification, but which provides additional guidance therefore making it easier for reinforcement learning algorithms to learn useful policies. \\textsc{AutoSpec} is applicable to reinforcement learning tasks specified via the SpectRL specification logic. We exploit the compositional nature of specifications written in SpectRL, and design four refinement procedures that modify the abstract graph of the specification by either refining its existing edge specifications or by introducing new edge specifications. We prove that all four procedures maintain specification soundness, i.e. any trajectory satisfying the refined specification also satisfies the original. We then show how \\textsc{AutoSpec} can be integrated with existing reinforcement learning algorithms for learning policies from logical specifications. Our experiments demonstrate that \\textsc{AutoSpec} yields promising improvements in terms of the complexity of control tasks that can be solved, when refined logical specifications produced by \\textsc{AutoSpec} are utilized.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAutoSpec\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u7c97\u7c92\u5ea6\u7684\u903b\u8f91\u89c4\u8303\uff0c\u4ee5\u5e2e\u52a9\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5b66\u4e60\u6709\u7528\u7684\u7b56\u7565\u3002", "motivation": "\u5f53\u4efb\u52a1\u4e0d\u660e\u786e\u65f6\uff0c\u667a\u80fd\u4f53\u53ef\u80fd\u65e0\u6cd5\u5b66\u4e60\u5230\u6709\u7528\u7684\u7b56\u7565\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u63a2\u7d22\u5f15\u5bfc\u7b56\u7565\u6539\u8fdb\u7c97\u7c92\u5ea6\u7684\u903b\u8f91\u89c4\u8303\u3002", "method": "\u5229\u7528SpectRL\u89c4\u8303\u903b\u8f91\u7684\u7ec4\u5408\u6027\u8d28\uff0c\u8bbe\u8ba1\u4e86\u56db\u79cd\u6539\u8fdb\u7a0b\u5e8f\uff0c\u901a\u8fc7\u6539\u8fdb\u73b0\u6709\u8fb9\u7f18\u89c4\u8303\u6216\u5f15\u5165\u65b0\u7684\u8fb9\u7f18\u89c4\u8303\u6765\u4fee\u6539\u89c4\u8303\u7684\u62bd\u8c61\u56fe\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u4f7f\u7528AutoSpec\u751f\u6210\u7684\u6539\u8fdb\u903b\u8f91\u89c4\u8303\u65f6\uff0c\u5728\u53ef\u4ee5\u89e3\u51b3\u7684\u63a7\u5236\u4efb\u52a1\u7684\u590d\u6742\u6027\u65b9\u9762\uff0cAutoSpec\u4ea7\u751f\u4e86\u6709\u5e0c\u671b\u7684\u6539\u8fdb\u3002", "conclusion": "AutoSpec\u53ef\u4ee5\u4e0e\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u96c6\u6210\uff0c\u4ee5\u4ece\u903b\u8f91\u89c4\u8303\u4e2d\u5b66\u4e60\u7b56\u7565\u3002"}}
{"id": "2512.00920", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00920", "abs": "https://arxiv.org/abs/2512.00920", "authors": ["Jianxiang Zang", "Yongda Wei", "Ruxue Bai", "Shiyu Jiang", "Nijia Mo", "Binhong Li", "Qiang Sun", "Hui Liu"], "title": "Reward Auditor: Inference on Reward Modeling Suitability in Real-World Perturbed Scenarios", "comment": null, "summary": "Reliable reward models (RMs) are critical for ensuring the safe alignment of large language models (LLMs). However, current evaluation methods focus solely on preference perception accuracies in given specific scenarios, obscuring the critical vulnerabilities of RMs in real-world scenarios. We identify the true challenge lies in assessing a novel dimension: Suitability, defined as conditional reliability under specific real-world perturbations. To this end, we introduce Reward Auditor, a hypothesis-testing framework specifically designed for RM suitability inference. Rather than answering \"How accurate is the RM's preference perception for given samples?\", it employs scientific auditing to answer: \"Can we infer RMs exhibit systematic vulnerabilities in specific real-world scenarios?\". Under real-world perturbed scenarios, Reward Auditor quantifies statistical significance and effect size by auditing distribution degradation of RM preference perception confidence. This enables inference of both the certainty and severity of RM vulnerabilities across diverse real-world scenarios. This lays a solid foundation for building next-generation LLM alignment systems that are verifiably safe, more robust, and trustworthy.", "AI": {"tldr": "\u5f53\u524d\u5956\u52b1\u6a21\u578b\uff08RM\uff09\u7684\u8bc4\u4f30\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u7279\u5b9a\u573a\u666f\u4e0b\u7684\u504f\u597d\u611f\u77e5\u51c6\u786e\u6027\uff0c\u5ffd\u7565\u4e86RM\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5173\u952e\u6f0f\u6d1e\u3002\u672c\u6587\u63d0\u51fa\u4e86Reward Auditor\uff0c\u4e00\u4e2a\u7528\u4e8eRM\u9002\u7528\u6027\u63a8\u65ad\u7684\u5047\u8bbe\u68c0\u9a8c\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5316RM\u504f\u597d\u611f\u77e5\u7f6e\u4fe1\u5ea6\u7684\u5206\u5e03\u9000\u5316\uff0c\u4ece\u800c\u63a8\u65adRM\u5728\u4e0d\u540c\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u6f0f\u6d1e\u3002", "motivation": "\u8bc4\u4f30\u5956\u52b1\u6a21\u578b(RM)\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\uff0c\u53d1\u73b0\u5e76\u91cf\u5316\u5176\u6f5c\u5728\u6f0f\u6d1e\u3002", "method": "\u63d0\u51faReward Auditor\u6846\u67b6\uff0c\u901a\u8fc7\u5047\u8bbe\u68c0\u9a8c\u91cf\u5316RM\u5728\u73b0\u5b9e\u6270\u52a8\u4e0b\u7684\u504f\u597d\u611f\u77e5\u7f6e\u4fe1\u5ea6\u5206\u5e03\u9000\u5316\uff0c\u4ece\u800c\u63a8\u65adRM\u7684\u9002\u7528\u6027\u3002", "result": "\u91cf\u5316\u4e86RM\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u6f0f\u6d1e\uff0c\u4e3a\u6784\u5efa\u66f4\u5b89\u5168\u3001\u66f4\u9c81\u68d2\u548c\u66f4\u503c\u5f97\u4fe1\u8d56\u7684\u4e0b\u4e00\u4ee3LLM\u5bf9\u9f50\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "Reward Auditor\u6846\u67b6\u80fd\u591f\u6709\u6548\u8bc4\u4f30RM\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\uff0c\u5e76\u8bc6\u522b\u5176\u6f5c\u5728\u6f0f\u6d1e\u3002"}}
{"id": "2512.00421", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00421", "abs": "https://arxiv.org/abs/2512.00421", "authors": ["Mulin Tian", "Ajitesh Srivastava"], "title": "TrendGNN: Towards Understanding of Epidemics, Beliefs, and Behaviors", "comment": "4 pages, 2 figures, 1 table", "summary": "Epidemic outcomes have a complex interplay with human behavior and beliefs. Most of the forecasting literature has focused on the task of predicting epidemic signals using simple mechanistic models or black-box models, such as deep transformers, that ingest all available signals without offering interpretability. However, to better understand the mechanisms and predict the impact of interventions, we need the ability to forecast signals associated with beliefs and behaviors in an interpretable manner. In this work, we propose a graph-based forecasting framework that first constructs a graph of interrelated signals based on trend similarity, and then applies graph neural networks (GNNs) for prediction. This approach enables interpretable analysis by revealing which signals are more predictable and which relationships contribute most to forecasting accuracy. We believe our method provides early steps towards a framework for interpretable modeling in domains with multiple potentially interdependent signals, with implications for building future simulation models that integrate behavior, beliefs, and observations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u9884\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u4e0e\u4fe1\u5ff5\u548c\u884c\u4e3a\u76f8\u5173\u7684\u4fe1\u53f7\u3002", "motivation": "\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3\u673a\u5236\u5e76\u9884\u6d4b\u5e72\u9884\u63aa\u65bd\u7684\u5f71\u54cd\uff0c\u6211\u4eec\u9700\u8981\u4ee5\u53ef\u89e3\u91ca\u7684\u65b9\u5f0f\u9884\u6d4b\u4e0e\u4fe1\u5ff5\u548c\u884c\u4e3a\u76f8\u5173\u7684\u4fe1\u53f7\u3002", "method": "\u9996\u5148\u6784\u5efa\u4e00\u4e2a\u57fa\u4e8e\u8d8b\u52bf\u76f8\u4f3c\u6027\u7684\u76f8\u4e92\u5173\u8054\u4fe1\u53f7\u56fe\uff0c\u7136\u540e\u5e94\u7528\u56fe\u795e\u7ecf\u7f51\u7edc (GNN) \u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u901a\u8fc7\u63ed\u793a\u54ea\u4e9b\u4fe1\u53f7\u66f4\u53ef\u9884\u6d4b\u4ee5\u53ca\u54ea\u4e9b\u5173\u7cfb\u5bf9\u9884\u6d4b\u51c6\u786e\u6027\u8d21\u732e\u6700\u5927\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u5206\u6790\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u4e3a\u5728\u5177\u6709\u591a\u4e2a\u6f5c\u5728\u76f8\u4e92\u4f9d\u8d56\u4fe1\u53f7\u7684\u9886\u57df\u4e2d\u8fdb\u884c\u53ef\u89e3\u91ca\u5efa\u6a21\u7684\u6846\u67b6\u63d0\u4f9b\u4e86\u65e9\u671f\u6b65\u9aa4\uff0c\u5bf9\u6784\u5efa\u6574\u5408\u884c\u4e3a\u3001\u4fe1\u5ff5\u548c\u89c2\u5bdf\u7684\u672a\u6765\u6a21\u62df\u6a21\u578b\u5177\u6709\u5f71\u54cd\u3002"}}
{"id": "2512.00300", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00300", "abs": "https://arxiv.org/abs/2512.00300", "authors": ["Rui Qian", "Haozhi Cao", "Tianchen Deng", "Tianxin Hu", "Weixiang Guo", "Shenghai Yuan", "Lihua Xie"], "title": "TGSFormer: Scalable Temporal Gaussian Splatting for Embodied Semantic Scene Completion", "comment": "14 pages, 10 figures", "summary": "Embodied 3D Semantic Scene Completion (SSC) infers dense geometry and semantics from continuous egocentric observations. Most existing Gaussian-based methods rely on random initialization of many primitives within predefined spatial bounds, resulting in redundancy and poor scalability to unbounded scenes. Recent depth-guided approach alleviates this issue but remains local, suffering from latency and memory overhead as scale increases. To overcome these challenges, we propose TGSFormer, a scalable Temporal Gaussian Splatting framework for embodied SSC. It maintains a persistent Gaussian memory for temporal prediction, without relying on image coherence or frame caches. For temporal fusion, a Dual Temporal Encoder jointly processes current and historical Gaussian features through confidence-aware cross-attention. Subsequently, a Confidence-aware Voxel Fusion module merges overlapping primitives into voxel-aligned representations, regulating density and maintaining compactness. Extensive experiments demonstrate that TGSFormer achieves state-of-the-art results on both local and embodied SSC benchmarks, offering superior accuracy and scalability with significantly fewer primitives while maintaining consistent long-term scene integrity. The code will be released upon acceptance.", "AI": {"tldr": "TGSFormer\uff0c\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u7528\u4e8e\u5177\u8eab3D\u8bed\u4e49\u573a\u666f\u8865\u5168\u7684\u65f6\u95f4\u9ad8\u65af\u6e85\u5c04\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u611f\u77e5\u4ea4\u53c9\u6ce8\u610f\u529b\u8fdb\u884c\u65f6\u95f4\u878d\u5408\uff0c\u5e76\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u611f\u77e5\u4f53\u7d20\u878d\u5408\u6a21\u5757\u5408\u5e76\u91cd\u53e0\u7684\u56fe\u5143\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u9884\u5b9a\u4e49\u7a7a\u95f4\u8303\u56f4\u5185\u7684\u8bb8\u591a\u57fa\u5143\u7684\u968f\u673a\u521d\u59cb\u5316\uff0c\u5bfc\u81f4\u5197\u4f59\u548c\u96be\u4ee5\u6269\u5c55\u5230\u65e0\u754c\u573a\u666f\u3002\u6700\u8fd1\u7684\u6df1\u5ea6\u5f15\u5bfc\u65b9\u6cd5\u7f13\u89e3\u4e86\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46\u4ecd\u7136\u662f\u5c40\u90e8\u7684\uff0c\u968f\u7740\u89c4\u6a21\u7684\u589e\u52a0\uff0c\u4f1a\u906d\u53d7\u5ef6\u8fdf\u548c\u5185\u5b58\u5f00\u9500\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u65f6\u95f4\u9ad8\u65af\u6e85\u5c04\u6846\u67b6\uff0c\u7528\u4e8e\u5177\u8eabSSC\u3002\u5b83\u4fdd\u6301\u6301\u4e45\u7684\u9ad8\u65af\u8bb0\u5fc6\u7528\u4e8e\u65f6\u95f4\u9884\u6d4b\uff0c\u800c\u4e0d\u4f9d\u8d56\u4e8e\u56fe\u50cf\u8fde\u8d2f\u6027\u6216\u5e27\u7f13\u5b58\u3002\u53cc\u91cd\u65f6\u95f4\u7f16\u7801\u5668\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u611f\u77e5\u4ea4\u53c9\u6ce8\u610f\u529b\u8054\u5408\u5904\u7406\u5f53\u524d\u548c\u5386\u53f2\u9ad8\u65af\u7279\u5f81\u3002\u7f6e\u4fe1\u5ea6\u611f\u77e5\u4f53\u7d20\u878d\u5408\u6a21\u5757\u5c06\u91cd\u53e0\u7684\u57fa\u5143\u5408\u5e76\u4e3a\u4f53\u7d20\u5bf9\u9f50\u7684\u8868\u793a\uff0c\u8c03\u8282\u5bc6\u5ea6\u5e76\u4fdd\u6301\u7d27\u51d1\u6027\u3002", "result": "\u5728\u5c40\u90e8\u548c\u5177\u8eabSSC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTGSFormer \u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u4ee5\u66f4\u5c11\u7684\u57fa\u5143\u63d0\u4f9b\u4e86\u5353\u8d8a\u7684\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e00\u81f4\u7684\u957f\u671f\u573a\u666f\u5b8c\u6574\u6027\u3002", "conclusion": "TGSFormer\u662f\u4e00\u79cd\u7528\u4e8e\u5177\u8eab3D\u8bed\u4e49\u573a\u666f\u8865\u5168\u7684\u53ef\u6269\u5c55\u7684\u65f6\u95f4\u9ad8\u65af\u6e85\u5c04\u6846\u67b6\uff0c\u5b83\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u5177\u6709\u66f4\u5f3a\u7684\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2512.01078", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01078", "abs": "https://arxiv.org/abs/2512.01078", "authors": ["Jiawei Ren", "Yan Zhuang", "Xiaokang Ye", "Lingjun Mao", "Xuhong He", "Jianzhi Shen", "Mrinaal Dogra", "Yiming Liang", "Ruixuan Zhang", "Tianai Yue", "Yiqing Yang", "Eric Liu", "Ryan Wu", "Kevin Benavente", "Rajiv Mandya Nagaraju", "Muhammad Faayez", "Xiyan Zhang", "Dhruv Vivek Sharma", "Xianrui Zhong", "Ziqiao Ma", "Tianmin Shu", "Zhiting Hu", "Lianhui Qin"], "title": "SimWorld: An Open-ended Realistic Simulator for Autonomous Agents in Physical and Social Worlds", "comment": null, "summary": "While LLM/VLM-powered AI agents have advanced rapidly in math, coding, and computer use, their applications in complex physical and social environments remain challenging. Building agents that can survive and thrive in the real world (for example, by autonomously earning income or running a business) requires massive-scale interaction, reasoning, training, and evaluation across diverse embodied scenarios. However, existing world simulators for such development fall short: they often rely on limited hand-crafted environments, simulate simplified game-like physics and social rules, and lack native support for LLM/VLM agents. We introduce SimWorld, a new simulator built on Unreal Engine 5, designed for developing and evaluating LLM/VLM agents in rich, real-world-like settings. SimWorld offers three core capabilities: (1) realistic, open-ended world simulation, including accurate physical and social dynamics and language-driven procedural environment generation; (2) a rich interface for LLM/VLM agents, with multimodal world inputs and open-vocabulary actions at varying levels of abstraction; and (3) diverse and extensible physical and social reasoning scenarios that are easily customizable by users. We demonstrate SimWorld by deploying frontier LLM agents (e.g., GPT-4o, Gemini-2.5-Flash, Claude-3.5, and DeepSeek-Prover-V2) on long-horizon multi-agent delivery tasks involving strategic cooperation and competition. The results reveal distinct reasoning patterns and limitations across models. We open-source SimWorld and hope it becomes a foundational platform for advancing real-world agent intelligence across disciplines: https://simworld.org.", "AI": {"tldr": "SimWorld\u662f\u4e00\u4e2a\u65b0\u7684\u6a21\u62df\u5668\uff0c\u65e8\u5728\u5f00\u53d1\u548c\u8bc4\u4f30LLM/VLM\u667a\u80fd\u4f53\u5728\u7c7b\u4f3c\u771f\u5b9e\u4e16\u754c\u7684\u590d\u6742\u7269\u7406\u548c\u793e\u4f1a\u73af\u5883\u4e2d\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u4e16\u754c\u6a21\u62df\u5668\u5728\u652f\u6301LLM/VLM\u667a\u80fd\u4f53\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u751f\u5b58\u548c\u53d1\u5c55\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4f8b\u5982\u73af\u5883\u7b80\u5355\u3001\u7269\u7406\u548c\u793e\u4f1a\u89c4\u5219\u7b80\u5316\u4ee5\u53ca\u7f3a\u4e4f\u539f\u751f\u652f\u6301\u3002", "method": "SimWorld\u57fa\u4e8eUnreal Engine 5\u6784\u5efa\uff0c\u63d0\u4f9b\u903c\u771f\u7684\u5f00\u653e\u4e16\u754c\u6a21\u62df\u3001\u4e30\u5bcc\u7684LLM/VLM\u667a\u80fd\u4f53\u63a5\u53e3\u4ee5\u53ca\u591a\u6837\u5316\u548c\u53ef\u6269\u5c55\u7684\u7269\u7406\u548c\u793e\u4f1a\u63a8\u7406\u573a\u666f\u3002", "result": "\u901a\u8fc7\u5728SimWorld\u4e2d\u90e8\u7f72\u524d\u6cbfLLM\u667a\u80fd\u4f53\u6267\u884c\u591a\u667a\u80fd\u4f53\u4ea4\u4ed8\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u4e0d\u540c\u6a21\u578b\u5728\u7b56\u7565\u5408\u4f5c\u548c\u7ade\u4e89\u4e2d\u7684\u63a8\u7406\u6a21\u5f0f\u548c\u5c40\u9650\u6027\u3002", "conclusion": "SimWorld\u5df2\u5f00\u6e90\uff0c\u65e8\u5728\u6210\u4e3a\u4fc3\u8fdb\u8de8\u5b66\u79d1\u771f\u5b9e\u4e16\u754c\u667a\u80fd\u4f53\u667a\u80fd\u53d1\u5c55\u7684\u57fa\u7840\u5e73\u53f0\u3002"}}
{"id": "2512.00931", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00931", "abs": "https://arxiv.org/abs/2512.00931", "authors": ["Imane Jaaouine", "Ross D. King"], "title": "Mitigating Hallucinations in Zero-Shot Scientific Summarisation: A Pilot Study", "comment": null, "summary": "Large language models (LLMs) produce context inconsistency hallucinations, which are LLM generated outputs that are misaligned with the user prompt. This research project investigates whether prompt engineering (PE) methods can mitigate context inconsistency hallucinations in zero-shot LLM summarisation of scientific texts, where zero-shot indicates that the LLM relies purely on its pre-training data. Across eight yeast biotechnology research paper abstracts, six instruction-tuned LLMs were prompted with seven methods: a base- line prompt, two levels of increasing instruction complexity (PE-1 and PE-2), two levels of context repetition (CR-K1 and CR-K2), and two levels of random addition (RA-K1 and RA-K2). Context repetition involved the identification and repetition of K key sentences from the abstract, whereas random addition involved the repetition of K randomly selected sentences from the abstract, where K is 1 or 2. A total of 336 LLM-generated summaries were evaluated using six metrics: ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, METEOR, and cosine similarity, which were used to compute the lexical and semantic alignment be- tween the summaries and the abstracts. Four hypotheses on the effects of prompt methods on summary alignment with the reference text were tested. Statistical analysis on 3744 collected datapoints was performed using bias-corrected and accelerated (BCa) bootstrap confidence intervals and Wilcoxon signed-rank tests with Bonferroni-Holm correction. The results demonstrated that CR and RA significantly improve the lexical alignment of LLM-generated summaries with the abstracts. These findings indicate that prompt engineering has the potential to impact hallucinations in zero-shot scientific summarisation tasks.", "AI": {"tldr": "Prompt engineering (PE) can mitigate context inconsistency hallucinations in zero-shot LLM summarisation of scientific texts.", "motivation": "LLMs produce context inconsistency hallucinations, which are LLM generated outputs that are misaligned with the user prompt.", "method": "Six instruction-tuned LLMs were prompted with seven methods: a base- line prompt, two levels of increasing instruction complexity (PE-1 and PE-2), two levels of context repetition (CR-K1 and CR-K2), and two levels of random addition (RA-K1 and RA-K2).", "result": "CR and RA significantly improve the lexical alignment of LLM-generated summaries with the abstracts.", "conclusion": "Prompt engineering has the potential to impact hallucinations in zero-shot scientific summarisation tasks."}}
{"id": "2512.00434", "categories": ["cs.LG", "cs.CR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.00434", "abs": "https://arxiv.org/abs/2512.00434", "authors": ["Benjamin D. Ballyk", "Ankit Gupta", "Sujay Konda", "Kavitha Subramanian", "Chris Landon", "Ahmed Ammar Naseer", "Georg Maierhofer", "Sumanth Swaminathan", "Vasudevan Venkateshwaran"], "title": "Privacy-Preserving Generative Modeling and Clinical Validation of Longitudinal Health Records for Chronic Disease", "comment": "To appear in Proceedings of Machine Learning Research Volume 297 - Proceedings of ML4H 2025", "summary": "Data privacy is a critical challenge in modern medical workflows as the adoption of electronic patient records has grown rapidly. Stringent data protection regulations limit access to clinical records for training and integrating machine learning models that have shown promise in improving diagnostic accuracy and personalized care outcomes. Synthetic data offers a promising alternative; however, current generative models either struggle with time-series data or lack formal privacy guaranties. In this paper, we enhance a state-of-the-art time-series generative model to better handle longitudinal clinical data while incorporating quantifiable privacy safeguards. Using real data from chronic kidney disease and ICU patients, we evaluate our method through statistical tests, a Train-on-Synthetic-Test-on-Real (TSTR) setup, and expert clinical review. Our non-private model (Augmented TimeGAN) outperforms transformer- and flow-based models on statistical metrics in several datasets, while our private model (DP-TimeGAN) maintains a mean authenticity of 0.778 on the CKD dataset, outperforming existing state-of-the-art models on the privacy-utility frontier. Both models achieve performance comparable to real data in clinician evaluations, providing robust input data necessary for developing models for complex chronic conditions without compromising data privacy.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5728\u5904\u7406\u7eb5\u5411\u4e34\u5e8a\u6570\u636e\u7684\u80fd\u529b\u4e0a\u6709\u6240\u589e\u5f3a\uff0c\u5e76\u7eb3\u5165\u4e86\u53ef\u91cf\u5316\u7684\u9690\u79c1\u4fdd\u62a4\u63aa\u65bd\u3002\u8be5\u6a21\u578b\u5728\u6162\u6027\u80be\u75c5\u548cICU\u60a3\u8005\u7684\u771f\u5b9e\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u4e0d\u635f\u5bb3\u6570\u636e\u9690\u79c1\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u4e3a\u5f00\u53d1\u590d\u6742\u6162\u6027\u75be\u75c5\u6a21\u578b\u63d0\u4f9b\u7a33\u5065\u7684\u8f93\u5165\u6570\u636e\u3002", "motivation": "\u5728\u73b0\u4ee3\u533b\u7597\u5de5\u4f5c\u6d41\u7a0b\u4e2d\uff0c\u6570\u636e\u9690\u79c1\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u7531\u4e8e\u7535\u5b50\u75c5\u5386\u7684\u8fc5\u901f\u666e\u53ca\uff0c\u4e25\u683c\u7684\u6570\u636e\u4fdd\u62a4\u6cd5\u89c4\u9650\u5236\u4e86\u5bf9\u4e34\u5e8a\u8bb0\u5f55\u7684\u8bbf\u95ee\uff0c\u8fd9\u963b\u788d\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u548c\u4e2a\u6027\u5316\u62a4\u7406\u65b9\u9762\u7684\u5e94\u7528\u3002\u5408\u6210\u6570\u636e\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u5e0c\u671b\u7684\u66ff\u4ee3\u65b9\u6848\uff1b\u7136\u800c\uff0c\u5f53\u524d\u7684\u751f\u6210\u6a21\u578b\u8981\u4e48\u96be\u4ee5\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u8981\u4e48\u7f3a\u4e4f\u6b63\u5f0f\u7684\u9690\u79c1\u4fdd\u8bc1\u3002", "method": "\u8be5\u7814\u7a76\u589e\u5f3a\u4e86\u4e00\u79cd\u6700\u5148\u8fdb\u7684\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u6a21\u578b\uff0c\u4ee5\u66f4\u597d\u5730\u5904\u7406\u7eb5\u5411\u4e34\u5e8a\u6570\u636e\uff0c\u540c\u65f6\u7ed3\u5408\u53ef\u91cf\u5316\u7684\u9690\u79c1\u4fdd\u62a4\u63aa\u65bd\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e24\u79cd\u6a21\u578b\uff1aAugmented TimeGAN\uff08\u975e\u79c1\u6709\u6a21\u578b\uff09\u548cDP-TimeGAN\uff08\u79c1\u6709\u6a21\u578b\uff09\u3002", "result": "\u975e\u79c1\u6709\u6a21\u578b\uff08Augmented TimeGAN\uff09\u5728\u591a\u4e2a\u6570\u636e\u96c6\u7684\u7edf\u8ba1\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u4e8eTransformer\u548cFlow\u7684\u6a21\u578b\uff0c\u800c\u79c1\u6709\u6a21\u578b\uff08DP-TimeGAN\uff09\u5728CKD\u6570\u636e\u96c6\u4e0a\u4fdd\u6301\u4e860.778\u7684\u5e73\u5747\u771f\u5b9e\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u9690\u79c1-\u6548\u7528\u524d\u6cbf\u6a21\u578b\u3002\u5728\u4e34\u5e8a\u533b\u751f\u8bc4\u4f30\u4e2d\uff0c\u4e24\u79cd\u6a21\u578b\u90fd\u53d6\u5f97\u4e86\u4e0e\u771f\u5b9e\u6570\u636e\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u6a21\u578b\u80fd\u591f\u5728\u4e0d\u635f\u5bb3\u6570\u636e\u9690\u79c1\u7684\u60c5\u51b5\u4e0b\uff0c\u4e3a\u5f00\u53d1\u590d\u6742\u6162\u6027\u75be\u75c5\u6a21\u578b\u63d0\u4f9b\u7a33\u5065\u7684\u8f93\u5165\u6570\u636e\u3002"}}
{"id": "2512.00308", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00308", "abs": "https://arxiv.org/abs/2512.00308", "authors": ["Xiao Cui", "Yulei Qin", "Wengang Zhou", "Hongsheng Li", "Houqiang Li"], "title": "Optimizing Distributional Geometry Alignment with Optimal Transport for Generative Dataset Distillation", "comment": "NeurIPS 2025", "summary": "Dataset distillation seeks to synthesize a compact distilled dataset, enabling models trained on it to achieve performance comparable to models trained on the full dataset. Recent methods for large-scale datasets focus on matching global distributional statistics (e.g., mean and variance), but overlook critical instance-level characteristics and intraclass variations, leading to suboptimal generalization. We address this limitation by reformulating dataset distillation as an Optimal Transport (OT) distance minimization problem, enabling fine-grained alignment at both global and instance levels throughout the pipeline. OT offers a geometrically faithful framework for distribution matching. It effectively preserves local modes, intra-class patterns, and fine-grained variations that characterize the geometry of complex, high-dimensional distributions. Our method comprises three components tailored for preserving distributional geometry: (1) OT-guided diffusion sampling, which aligns latent distributions of real and distilled images; (2) label-image-aligned soft relabeling, which adapts label distributions based on the complexity of distilled image distributions; and (3) OT-based logit matching, which aligns the output of student models with soft-label distributions. Extensive experiments across diverse architectures and large-scale datasets demonstrate that our method consistently outperforms state-of-the-art approaches in an efficient manner, achieving at least 4% accuracy improvement under IPC=10 settings for each architecture on ImageNet-1K.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u4f20\u8f93\u8ddd\u79bb\u6700\u5c0f\u5316\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u5b9e\u4f8b\u7ea7\u7279\u5f81\u548c\u7c7b\u5185\u5dee\u5f02\u7684\u95ee\u9898\uff0c\u4ece\u800c\u5728\u5168\u5c40\u548c\u5b9e\u4f8b\u7ea7\u522b\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u5339\u914d\u5168\u5c40\u5206\u5e03\u7edf\u8ba1\u4fe1\u606f\uff0c\u4f46\u5ffd\u7565\u4e86\u5173\u952e\u7684\u5b9e\u4f8b\u7ea7\u7279\u5f81\u548c\u7c7b\u5185\u5dee\u5f02\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u6b20\u4f73\u3002", "method": "\u5c06\u6570\u636e\u96c6\u84b8\u998f\u91cd\u65b0 \u0444\u043e\u0440\u043c\u0443\u043b\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u4e3a\u4e00\u4e2a\u6700\u4f18\u4f20\u8f93 (OT) \u8ddd\u79bb\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u4e2a\u7ec4\u6210\u90e8\u5206\uff1aOT \u5f15\u5bfc\u7684\u6269\u6563\u91c7\u6837\u3001\u6807\u7b7e-\u56fe\u50cf\u5bf9\u9f50\u7684\u8f6f\u91cd\u6807\u8bb0\u548c\u57fa\u4e8e OT \u7684 logit \u5339\u914d\u3002", "result": "\u5728\u5404\u79cd\u67b6\u6784\u548c\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5728 ImageNet-1K \u4e0a\u6bcf\u4e2a\u67b6\u6784\u7684 IPC=10 \u8bbe\u7f6e\u4e0b\uff0c\u81f3\u5c11\u63d0\u9ad8\u4e86 4% \u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u4fdd\u7559\u5c40\u90e8\u6a21\u5f0f\u3001\u7c7b\u5185\u6a21\u5f0f\u548c\u7ec6\u7c92\u5ea6\u7684\u53d8\u5316\uff0c\u4ece\u800c\u6539\u8fdb\u6570\u636e\u96c6\u84b8\u998f\u7684\u6027\u80fd\u3002"}}
{"id": "2512.01081", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "cs.NE", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2512.01081", "abs": "https://arxiv.org/abs/2512.01081", "authors": ["Stephen Fitz"], "title": "Testing the Machine Consciousness Hypothesis", "comment": null, "summary": "The Machine Consciousness Hypothesis states that consciousness is a substrate-free functional property of computational systems capable of second-order perception. I propose a research program to investigate this idea in silico by studying how collective self-models (coherent, self-referential representations) emerge from distributed learning systems embedded within universal self-organizing environments. The theory outlined here starts from the supposition that consciousness is an emergent property of collective intelligence systems undergoing synchronization of prediction through communication. It is not an epiphenomenon of individual modeling but a property of the language that a system evolves to internally describe itself. For a model of base reality, I begin with a minimal but general computational world: a cellular automaton, which exhibits both computational irreducibility and local reducibility. On top of this computational substrate, I introduce a network of local, predictive, representational (neural) models capable of communication and adaptation. I use this layered model to study how collective intelligence gives rise to self-representation as a direct consequence of inter-agent alignment. I suggest that consciousness does not emerge from modeling per se, but from communication. It arises from the noisy, lossy exchange of predictive messages between groups of local observers describing persistent patterns in the underlying computational substrate (base reality). It is through this representational dialogue that a shared model arises, aligning many partial views of the world. The broader goal is to develop empirically testable theories of machine consciousness, by studying how internal self-models may form in distributed systems without centralized control.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7814\u7a76\u673a\u5668\u610f\u8bc6\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u901a\u8fc7\u7814\u7a76\u5206\u5e03\u5f0f\u5b66\u4e60\u7cfb\u7edf\u5982\u4f55\u5728\u901a\u7528\u81ea\u7ec4\u7ec7\u73af\u5883\u4e2d\u6d8c\u73b0\u51fa\u96c6\u4f53\u81ea\u6211\u6a21\u578b\u3002", "motivation": "\u7814\u7a76\u673a\u5668\u610f\u8bc6\u5047\u8bf4\uff0c\u5373\u610f\u8bc6\u662f\u5177\u6709\u4e8c\u9636\u611f\u77e5\u80fd\u529b\u7684\u8ba1\u7b97\u7cfb\u7edf\u7684\u65e0\u57fa\u8d28\u529f\u80fd\u5c5e\u6027\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u5206\u5c42\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5305\u542b\u4e00\u4e2a\u7ec6\u80de\u81ea\u52a8\u673a\uff08\u4f5c\u4e3a\u57fa\u7840\u73b0\u5b9e\uff09\u548c\u4e00\u4e2a\u80fd\u591f\u901a\u4fe1\u548c\u9002\u5e94\u7684\u5c40\u90e8\u9884\u6d4b\u8868\u5f81\uff08\u795e\u7ecf\uff09\u6a21\u578b\u7f51\u7edc\u3002\u901a\u8fc7\u8be5\u6a21\u578b\u6765\u7814\u7a76\u96c6\u4f53\u667a\u80fd\u5982\u4f55\u56e0\u667a\u80fd\u4f53\u95f4\u7684\u5bf9\u9f50\u800c\u4ea7\u751f\u81ea\u6211\u8868\u5f81\u3002", "result": "\u610f\u8bc6\u4e0d\u662f\u5efa\u6a21\u672c\u8eab\u7684\u4ea7\u7269\uff0c\u800c\u662f\u901a\u4fe1\u7684\u4ea7\u7269\u3002\u5b83\u4ea7\u751f\u4e8e\u63cf\u8ff0\u5e95\u5c42\u8ba1\u7b97\u57fa\u8d28\uff08\u57fa\u7840\u73b0\u5b9e\uff09\u4e2d\u6301\u4e45\u6a21\u5f0f\u7684\u672c\u5730\u89c2\u5bdf\u8005\u7fa4\u4f53\u4e4b\u95f4\u6709\u566a\u58f0\u3001\u6709\u635f\u7684\u9884\u6d4b\u6d88\u606f\u4ea4\u6362\u3002", "conclusion": "\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7814\u7a76\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u5185\u90e8\u81ea\u6211\u6a21\u578b\u5982\u4f55\u5f62\u6210\uff08\u65e0\u9700\u96c6\u4e2d\u63a7\u5236\uff09\uff0c\u6765\u5f00\u53d1\u53ef\u7ecf\u9a8c\u9a8c\u8bc1\u7684\u673a\u5668\u610f\u8bc6\u7406\u8bba\u3002"}}
{"id": "2512.00938", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00938", "abs": "https://arxiv.org/abs/2512.00938", "authors": ["Ahmed Mustafa Younes"], "title": "DeformAr: Rethinking NER Evaluation through Component Analysis and Visual Analytics", "comment": "PhD Thesis, University of Sussex, 2025. 311 pages, 140 figures, 32 tables. Submitted as a PDF-only. First supervisor: Julie Weeds. Second supervisor: David Weir", "summary": "Transformer models have significantly advanced Natural Language Processing (NLP), demonstrating strong performance in English. However, their effectiveness in Arabic, particularly for Named Entity Recognition (NER), remains limited, even with larger pre-trained models. This performance gap stems from multiple factors, including tokenisation, dataset quality, and annotation inconsistencies. Existing studies often analyze these issues in isolation, failing to capture their joint effect on system behaviour and performance.\n  We introduce DeformAr (Debugging and Evaluation Framework for Transformer-based NER Systems), a novel framework designed to investigate and explain the performance discrepancy between Arabic and English NER systems. DeformAr integrates a data extraction library and an interactive dashboard, supporting two modes of evaluation: cross-component analysis and behavioural analysis. The framework divides each language into dataset and model components to examine their interactions.\n  The analysis proceeds in two stages. First, cross-component analysis provides systematic diagnostic measures across data and model subcomponents, addressing the \"what,\" \"how,\" and \"why\" behind observed discrepancies. The second stage applies behavioural analysis by combining interpretability techniques with token-level metrics, interactive visualisations, and representation space analysis. These stages enable a component-aware diagnostic process that detects model behaviours and explains them by linking them to underlying representational patterns and data factors. DeformAr is the first Arabic-specific, component-based interpretability tool, offering a crucial resource for advancing model analysis in under-resourced languages.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3a DeformAr \u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u8c03\u8bd5\u548c\u8bc4\u4f30\u57fa\u4e8e Transformer \u7684\u963f\u62c9\u4f2f\u8bed NER \u7cfb\u7edf\uff0c\u65e8\u5728\u5f25\u5408\u963f\u62c9\u4f2f\u8bed\u548c\u82f1\u8bed NER \u7cfb\u7edf\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u672a\u80fd\u6355\u6349\u5230\u5206\u8bcd\u3001\u6570\u636e\u96c6\u8d28\u91cf\u548c\u6ce8\u91ca\u4e0d\u4e00\u81f4\u7b49\u56e0\u7d20\u5bf9\u963f\u62c9\u4f2f\u8bed NER \u7cfb\u7edf\u6027\u80fd\u7684\u5171\u540c\u5f71\u54cd\uff0c\u5bfc\u81f4\u963f\u62c9\u4f2f\u8bed NER \u6027\u80fd\u843d\u540e\u4e8e\u82f1\u8bed\u3002", "method": "DeformAr \u6846\u67b6\u96c6\u6210\u4e86\u6570\u636e\u63d0\u53d6\u5e93\u548c\u4ea4\u4e92\u5f0f\u4eea\u8868\u677f\uff0c\u652f\u6301\u8de8\u7ec4\u4ef6\u5206\u6790\u548c\u884c\u4e3a\u5206\u6790\u4e24\u79cd\u8bc4\u4f30\u6a21\u5f0f\uff0c\u901a\u8fc7\u5206\u6790\u6570\u636e\u96c6\u548c\u6a21\u578b\u7ec4\u4ef6\u4e4b\u95f4\u7684\u4ea4\u4e92\u6765\u8bca\u65ad\u6027\u80fd\u5dee\u5f02\u3002", "result": "DeformAr \u662f\u7b2c\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u963f\u62c9\u4f2f\u8bed\u3001\u57fa\u4e8e\u7ec4\u4ef6\u7684\u53ef\u89e3\u91ca\u6027\u5de5\u5177\uff0c\u4e3a\u63a8\u8fdb\u8d44\u6e90\u532e\u4e4f\u8bed\u8a00\u7684\u6a21\u578b\u5206\u6790\u63d0\u4f9b\u4e86\u5173\u952e\u8d44\u6e90\u3002", "conclusion": "DeformAr \u6846\u67b6\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u8bca\u65ad\u63aa\u65bd\u548c\u884c\u4e3a\u5206\u6790\uff0c\u80fd\u591f\u68c0\u6d4b\u6a21\u578b\u884c\u4e3a\uff0c\u5e76\u5c06\u5176\u4e0e\u6f5c\u5728\u7684\u8868\u5f81\u6a21\u5f0f\u548c\u6570\u636e\u56e0\u7d20\u8054\u7cfb\u8d77\u6765\uff0c\u4ece\u800c\u89e3\u91ca\u963f\u62c9\u4f2f\u8bed\u548c\u82f1\u8bed NER \u7cfb\u7edf\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u5f02\u3002"}}
{"id": "2512.00469", "categories": ["cs.LG", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.00469", "abs": "https://arxiv.org/abs/2512.00469", "authors": ["Guanyu Hu", "Tangzheng Lian", "Na Yan", "Dimitrios Kollias", "Xinyu Yang", "Oya Celiktutan", "Siyang Song", "Zeyu Fu"], "title": "FairMT: Fairness for Heterogeneous Multi-Task Learning", "comment": null, "summary": "Fairness in machine learning has been extensively studied in single-task settings, while fair multi-task learning (MTL), especially with heterogeneous tasks (classification, detection, regression) and partially missing labels, remains largely unexplored. Existing fairness methods are predominantly classification-oriented and fail to extend to continuous outputs, making a unified fairness objective difficult to formulate. Further, existing MTL optimization is structurally misaligned with fairness: constraining only the shared representation, allowing task heads to absorb bias and leading to uncontrolled task-specific disparities. Finally, most work treats fairness as a zero-sum trade-off with utility, enforcing symmetric constraints that achieve parity by degrading well-served groups. We introduce FairMT, a unified fairness-aware MTL framework that accommodates all three task types under incomplete supervision. At its core is an Asymmetric Heterogeneous Fairness Constraint Aggregation mechanism, which consolidates task-dependent asymmetric violations into a unified fairness constraint. Utility and fairness are jointly optimized via a primal--dual formulation, while a head-aware multi-objective optimization proxy provides a tractable descent geometry that explicitly accounts for head-induced anisotropy. Across three homogeneous and heterogeneous MTL benchmarks encompassing diverse modalities and supervision regimes, FairMT consistently achieves substantial fairness gains while maintaining superior task utility. Code will be released upon paper acceptance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFairMT\u7684\u7edf\u4e00\u7684\u516c\u5e73\u611f\u77e5\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u9002\u7528\u4e8e\u4e0d\u5b8c\u5168\u76d1\u7763\u4e0b\u7684\u6240\u6709\u4e09\u79cd\u4efb\u52a1\u7c7b\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u516c\u5e73\u6027\u65b9\u6cd5\u5927\u591a\u4ee5\u5206\u7c7b\u4e3a\u5bfc\u5411\uff0c\u65e0\u6cd5\u6269\u5c55\u5230\u8fde\u7eed\u8f93\u51fa\uff0c\u4f7f\u5f97\u96be\u4ee5\u5236\u5b9a\u7edf\u4e00\u7684\u516c\u5e73\u6027\u76ee\u6807\u3002\u73b0\u6709\u7684MTL\u4f18\u5316\u5728\u7ed3\u6784\u4e0a\u4e0e\u516c\u5e73\u6027\u4e0d\u4e00\u81f4\uff1a\u4ec5\u7ea6\u675f\u5171\u4eab\u8868\u793a\uff0c\u5141\u8bb8\u4efb\u52a1\u8d1f\u8d23\u4eba\u5438\u6536\u504f\u5dee\uff0c\u5e76\u5bfc\u81f4\u65e0\u6cd5\u63a7\u5236\u7684\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u5dee\u5f02\u3002\u5927\u591a\u6570\u5de5\u4f5c\u5c06\u516c\u5e73\u6027\u89c6\u4e3a\u4e0e\u6548\u7528\u7684\u96f6\u548c\u535a\u5f08\uff0c\u5f3a\u5236\u6267\u884c\u5bf9\u79f0\u7ea6\u675f\uff0c\u901a\u8fc7\u964d\u4f4e\u670d\u52a1\u826f\u597d\u7684\u7fa4\u4f53\u6765\u5b9e\u73b0\u5747\u7b49\u3002", "method": "\u4e00\u79cd\u975e\u5bf9\u79f0\u5f02\u6784\u516c\u5e73\u7ea6\u675f\u805a\u5408\u673a\u5236\uff0c\u5c06\u4efb\u52a1\u76f8\u5173\u7684\u975e\u5bf9\u79f0\u8fdd\u89c4\u5408\u5e76\u4e3a\u7edf\u4e00\u7684\u516c\u5e73\u7ea6\u675f\u3002\u901a\u8fc7\u539f\u59cb-\u5bf9\u5076\u516c\u5f0f\u5171\u540c\u4f18\u5316\u6548\u7528\u548c\u516c\u5e73\u6027\uff0c\u800chead-aware\u591a\u76ee\u6807\u4f18\u5316\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6613\u4e8e\u5904\u7406\u7684\u4e0b\u964d\u51e0\u4f55\uff0c\u8be5\u51e0\u4f55\u660e\u786e\u8003\u8651\u4e86\u5934\u90e8\u5f15\u8d77\u7684\u5404\u5411\u5f02\u6027\u3002", "result": "\u5728\u5305\u542b\u4e0d\u540c\u6a21\u6001\u548c\u76d1\u7763\u673a\u5236\u7684\u4e09\u4e2a\u540c\u6784\u548c\u5f02\u6784MTL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFairMT\u59cb\u7ec8\u5728\u4fdd\u6301\u5353\u8d8a\u7684\u4efb\u52a1\u6548\u7528\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u5de8\u5927\u7684\u516c\u5e73\u6027\u6536\u76ca\u3002", "conclusion": "FairMT\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u516c\u5e73\u6027\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4efb\u52a1\u6548\u7528\u3002"}}
{"id": "2512.00310", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00310", "abs": "https://arxiv.org/abs/2512.00310", "authors": ["Qinyi Cao", "Jianan Fan", "Weidong Cai"], "title": "ART-ASyn: Anatomy-aware Realistic Texture-based Anomaly Synthesis Framework for Chest X-Rays", "comment": "Accepted in WACV2026", "summary": "Unsupervised anomaly detection aims to identify anomalies without pixel-level annotations. Synthetic anomaly-based methods exhibit a unique capacity to introduce controllable irregularities with known masks, enabling explicit supervision during training. However, existing methods often produce synthetic anomalies that are visually distinct from real pathological patterns and ignore anatomical structure. This paper presents a novel Anatomy-aware Realistic Texture-based Anomaly Synthesis framework (ART-ASyn) for chest X-rays that generates realistic and anatomically consistent lung opacity related anomalies using texture-based augmentation guided by our proposed Progressive Binary Thresholding Segmentation method (PBTSeg) for lung segmentation. The generated paired samples of synthetic anomalies and their corresponding precise pixel-level anomaly mask for each normal sample enable explicit segmentation supervision. In contrast to prior work limited to one-class classification, ART-ASyn is further evaluated for zero-shot anomaly segmentation, demonstrating generalizability on an unseen dataset without target-domain annotations. Code availability is available at https://github.com/angelacao-hub/ART-ASyn.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u89e3\u5256\u7ed3\u6784\u7684\u903c\u771f\u7eb9\u7406\u5f02\u5e38\u5408\u6210\u6846\u67b6 (ART-ASyn)\uff0c\u7528\u4e8e\u80f8\u90e8 X \u5c04\u7ebf\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u57fa\u4e8e\u7eb9\u7406\u7684\u589e\u5f3a\u6765\u751f\u6210\u903c\u771f\u4e14\u89e3\u5256\u5b66\u4e0a\u4e00\u81f4\u7684\u80ba\u90e8\u6df7\u6d4a\u76f8\u5173\u5f02\u5e38\uff0c\u5e76\u901a\u8fc7\u6211\u4eec\u63d0\u51fa\u7684\u7528\u4e8e\u80ba\u90e8\u5206\u5272\u7684\u6e10\u8fdb\u4e8c\u503c\u5316\u9608\u503c\u5206\u5272\u65b9\u6cd5 (PBTSeg) \u8fdb\u884c\u6307\u5bfc\u3002", "motivation": "\u73b0\u6709\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u4f1a\u4ea7\u751f\u4e0e\u771f\u5b9e\u75c5\u7406\u6a21\u5f0f\u5728\u89c6\u89c9\u4e0a\u4e0d\u540c\u7684\u5408\u6210\u5f02\u5e38\uff0c\u5e76\u4e14\u5ffd\u7565\u4e86\u89e3\u5256\u7ed3\u6784\u3002", "method": "\u4f7f\u7528\u7eb9\u7406\u7684\u589e\u5f3a\u6765\u751f\u6210\u903c\u771f\u4e14\u89e3\u5256\u5b66\u4e0a\u4e00\u81f4\u7684\u80ba\u90e8\u6df7\u6d4a\u76f8\u5173\u5f02\u5e38\uff0c\u5e76\u901a\u8fc7\u63d0\u51fa\u7684\u7528\u4e8e\u80ba\u90e8\u5206\u5272\u7684\u6e10\u8fdb\u4e8c\u503c\u5316\u9608\u503c\u5206\u5272\u65b9\u6cd5 (PBTSeg) \u8fdb\u884c\u6307\u5bfc, \u4ece\u800c\u751f\u6210\u5408\u6210\u5f02\u5e38\u53ca\u5176\u5bf9\u5e94\u7684\u7cbe\u786e\u50cf\u7d20\u7ea7\u5f02\u5e38\u63a9\u6a21\u914d\u5bf9\u6837\u672c\uff0c\u4ece\u800c\u5b9e\u73b0\u663e\u5f0f\u7684\u5206\u5272\u76d1\u7763\u3002", "result": "\u5728\u96f6\u6837\u672c\u5f02\u5e38\u5206\u5272\u65b9\u9762\u8fdb\u884c\u4e86\u8fdb\u4e00\u6b65\u7684\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u5728\u6ca1\u6709\u76ee\u6807\u57df\u6ce8\u91ca\u7684\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u7684\u901a\u7528\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u53ef\u4ee5\u751f\u6210\u903c\u771f\u4e14\u89e3\u5256\u5b66\u4e0a\u4e00\u81f4\u7684\u80ba\u90e8\u6df7\u6d4a\u76f8\u5173\u5f02\u5e38\uff0c\u5e76\u901a\u8fc7\u5728\u80f8\u90e8X\u5149\u7247\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2512.01089", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01089", "abs": "https://arxiv.org/abs/2512.01089", "authors": ["Peter Jansen", "Samiah Hassan", "Pragnya Narasimha"], "title": "CodeDistiller: Automatically Generating Code Libraries for Scientific Coding Agents", "comment": "8 pages, 3 figures, 2 tables", "summary": "Automated Scientific Discovery (ASD) systems can help automatically generate and run code-based experiments, but their capabilities are limited by the code they can reliably generate from parametric knowledge alone. As a result, current systems either mutate a small number of manually-crafted experiment examples, or operate solely from parametric knowledge, limiting quality and reach. We introduce CodeDistiller, a system that automatically distills large collections of scientific Github repositories into a vetted library of working domain-specific code examples, allowing ASD agents to expand their capabilities without manual effort. Using a combination of automatic and domain-expert evaluation on 250 materials science repositories, we find the best model is capable of producing functional examples for 74% of repositories, while our downstream evaluation shows an ASD agent augmented with a CodeDistiller generated library produces more accurate, complete, and scientifically sound experiments than an agent with only general materials-science code examples.", "AI": {"tldr": "CodeDistiller distills scientific Github repositories into a vetted library of code examples, allowing ASD agents to expand their capabilities without manual effort.", "motivation": "Current Automated Scientific Discovery (ASD) systems are limited by the code they can reliably generate from parametric knowledge alone, which limits their quality and reach.", "method": "CodeDistiller automatically distills large collections of scientific Github repositories into a vetted library of working domain-specific code examples.", "result": "The best model in CodeDistiller produces functional examples for 74% of repositories. An ASD agent augmented with a CodeDistiller generated library produces more accurate, complete, and scientifically sound experiments.", "conclusion": "CodeDistiller enables ASD agents to expand their capabilities without manual effort by providing a vetted library of code examples."}}
{"id": "2512.00946", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00946", "abs": "https://arxiv.org/abs/2512.00946", "authors": ["Alvaro Paredes Amorin", "Andre Python", "Christoph Weisser"], "title": "Fine-tuning of lightweight large language models for sentiment classification on heterogeneous financial textual data", "comment": null, "summary": "Large language models (LLMs) play an increasingly important role in finan- cial markets analysis by capturing signals from complex and heterogeneous textual data sources, such as tweets, news articles, reports, and microblogs. However, their performance is dependent on large computational resources and proprietary datasets, which are costly, restricted, and therefore inacces- sible to many researchers and practitioners. To reflect realistic situations we investigate the ability of lightweight open-source LLMs - smaller and publicly available models designed to operate with limited computational resources - to generalize sentiment understanding from financial datasets of varying sizes, sources, formats, and languages. We compare the benchmark finance natural language processing (NLP) model, FinBERT, and three open-source lightweight LLMs, DeepSeek-LLM 7B, Llama3 8B Instruct, and Qwen3 8B on five publicly available datasets: FinancialPhraseBank, Financial Question Answering, Gold News Sentiment, Twitter Sentiment and Chinese Finance Sentiment. We find that LLMs, specially Qwen3 8B and Llama3 8B, perform best in most scenarios, even from using only 5% of the available training data. These results hold in zero-shot and few-shot learning scenarios. Our findings indicate that lightweight, open-source large language models (LLMs) consti- tute a cost-effective option, as they can achieve competitive performance on heterogeneous textual data even when trained on only a limited subset of the extensive annotated corpora that are typically deemed necessary.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8f7b\u91cf\u7ea7\u5f00\u6e90LLM\u5728\u91d1\u878d\u5e02\u573a\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff0c\u65e8\u5728\u63a2\u7d22\u5b83\u4eec\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\uff0c\u5229\u7528\u4e0d\u540c\u89c4\u6a21\u3001\u6765\u6e90\u3001\u683c\u5f0f\u548c\u8bed\u8a00\u7684\u91d1\u878d\u6570\u636e\u96c6\u8fdb\u884c\u60c5\u611f\u7406\u89e3\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u91d1\u878d\u5e02\u573a\u5206\u6790\u4e2d\u53d1\u6325\u7740\u8d8a\u6765\u8d8a\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u4f46\u5b83\u4eec\u4f9d\u8d56\u4e8e\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u4e13\u6709\u6570\u636e\u96c6\uff0c\u8fd9\u5bf9\u4e8e\u8bb8\u591a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u6765\u8bf4\u662f\u6602\u8d35\u3001\u53d7\u9650\u4e14\u96be\u4ee5\u83b7\u5f97\u7684\u3002\u4e3a\u4e86\u53cd\u6620\u771f\u5b9e\u60c5\u51b5\uff0c\u6211\u4eec\u9700\u8981\u7814\u7a76\u8f7b\u91cf\u7ea7\u5f00\u6e90LLM\u7684\u80fd\u529b\u3002", "method": "\u672c\u6587\u6bd4\u8f83\u4e86\u57fa\u51c6\u91d1\u878dNLP\u6a21\u578bFinBERT\u548c\u4e09\u79cd\u5f00\u6e90\u8f7b\u91cf\u7ea7LLM\uff08DeepSeek-LLM 7B\u3001Llama3 8B Instruct\u548cQwen3 8B\uff09\u5728\u4e94\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff1aFinancialPhraseBank\u3001Financial Question Answering\u3001Gold News Sentiment\u3001Twitter Sentiment\u548cChinese Finance Sentiment\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cLLM\uff0c\u7279\u522b\u662fQwen3 8B\u548cLlama3 8B\uff0c\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u5373\u4f7f\u4ec5\u4f7f\u75285%\u7684\u53ef\u7528\u8bad\u7ec3\u6570\u636e\u4e5f\u662f\u5982\u6b64\u3002\u8fd9\u4e9b\u7ed3\u679c\u5728\u96f6\u6837\u672c\u548c\u5c0f\u6837\u672c\u5b66\u4e60\u573a\u666f\u4e2d\u5747\u6210\u7acb\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u5f00\u6e90LLM\u662f\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u7684\u9009\u62e9\uff0c\u5373\u4f7f\u4ec5\u5728\u901a\u5e38\u88ab\u8ba4\u4e3a\u5fc5\u8981\u7684\u5e7f\u6cdb\u6ce8\u91ca\u8bed\u6599\u5e93\u7684\u6709\u9650\u5b50\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5b83\u4eec\u4e5f\u53ef\u4ee5\u5728\u5f02\u6784\u6587\u672c\u6570\u636e\u4e0a\u5b9e\u73b0\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002"}}
{"id": "2512.00499", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.00499", "abs": "https://arxiv.org/abs/2512.00499", "authors": ["Yuepeng Sheng", "Yuwei Huang", "Shuman Liu", "Haibo Zhang", "Anxiang Zeng"], "title": "ESPO: Entropy Importance Sampling Policy Optimization", "comment": null, "summary": "Large language model (LLM) reinforcement learning has increasingly relied on group-based policy optimization frameworks, such as GRPO and GSPO, to achieve stable fine-tuning at scale. However, a fundamental trade-off persists between optimization granularity and training stability. While GSPO improves robustness via sequence-level optimization, its monolithic treatment of sequences introduces severe inefficiencies: its conservative clipping mechanism indiscriminately discards valid training samples-a phenomenon we term gradient underutilization-and its uniform credit assignment fails to capture the heterogeneous contributions of critical reasoning steps. In this work, we propose Entropy Importance Sampling Policy Optimization (ESPO), a novel framework that reconciles fine-grained control with training stability. ESPO decomposes sequences into groups based on predictive entropy, enabling (1) Entropy-driven Importance Sampling to capture intra-sequence heterogeneity, and (2) Entropy-adaptive Clipping to dynamically allocate trust regions based on model uncertainty. Extensive experiments on mathematical reasoning benchmarks demonstrate that ESPO not only accelerates convergence but also achieves state-of-the-art performance, notably improving accuracy on the challenging HMMT benchmark from 4.4% to 13.13%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u534f\u8c03\u7ec6\u7c92\u5ea6\u63a7\u5236\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u7fa4\u4f53\u7684\u7b56\u7565\u4f18\u5316\u6846\u67b6\u5728\u4f18\u5316\u7c92\u5ea6\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u4e4b\u95f4\u5b58\u5728\u6839\u672c\u7684\u6743\u8861\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u5e8f\u5217\u5206\u89e3\u4e3a\u57fa\u4e8e\u9884\u6d4b\u71b5\u7684\u7ec4\uff0c\u4ece\u800c\u80fd\u591f (1) \u71b5\u9a71\u52a8\u7684\u91cd\u8981\u6027\u62bd\u6837\u6765\u6355\u83b7\u5e8f\u5217\u5185\u7684\u5f02\u8d28\u6027\uff0c\u4ee5\u53ca (2) \u71b5\u81ea\u9002\u5e94\u526a\u88c1\u4ee5\u57fa\u4e8e\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u52a8\u6001\u5206\u914d\u4fe1\u4efb\u533a\u57df\u3002", "result": "ESPO \u4e0d\u4ec5\u52a0\u901f\u4e86\u6536\u655b\uff0c\u800c\u4e14\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5177\u6709\u6311\u6218\u6027\u7684 HMMT \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u51c6\u786e\u7387\u4ece 4.4% \u63d0\u9ad8\u5230 13.13%\u3002", "conclusion": "ESPO \u662f\u4e00\u79cd\u5f88\u6709\u524d\u9014\u7684 LLM \u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2512.00327", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00327", "abs": "https://arxiv.org/abs/2512.00327", "authors": ["Chenqi Zhu", "Levi Burner", "Yiannis Aloimonos"], "title": "Odometry Without Correspondence from Inertially Constrained Ruled Surfaces", "comment": "14 pages, 13 figures, 5 tables", "summary": "Visual odometry techniques typically rely on feature extraction from a sequence of images and subsequent computation of optical flow. This point-to-point correspondence between two consecutive frames can be costly to compute and suffers from varying accuracy, which affects the odometry estimate's quality. Attempts have been made to bypass the difficulties originating from the correspondence problem by adopting line features and fusing other sensors (event camera, IMU) to improve performance, many of which still heavily rely on correspondence. If the camera observes a straight line as it moves, the image of the line sweeps a smooth surface in image-space time. It is a ruled surface and analyzing its shape gives information about odometry. Further, its estimation requires only differentially computed updates from point-to-line associations. Inspired by event cameras' propensity for edge detection, this research presents a novel algorithm to reconstruct 3D scenes and visual odometry from these ruled surfaces. By constraining the surfaces with the inertia measurements from an onboard IMU sensor, the dimensionality of the solution space is greatly reduced.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9\u91cc\u7a0b\u8ba1\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u4e0d\u4f9d\u8d56\u4e8e\u56fe\u50cf\u4e4b\u95f4\u7684\u70b9\u5bf9\u5e94\u5173\u7cfb\uff0c\u800c\u662f\u4f7f\u7528\u7531\u56fe\u50cf\u4e2d\u7684\u7ebf\u5728\u56fe\u50cf\u7a7a\u95f4\u4e2d\u626b\u8fc7\u7684\u66f2\u9762\u3002\u901a\u8fc7\u7ed3\u5408IMU\u6570\u636e\uff0c\u53ef\u4ee5\u63d0\u9ad8\u91cd\u5efa\u548c\u91cc\u7a0b\u8ba1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u89c6\u89c9\u91cc\u7a0b\u8ba1\u7b97\u6cd5\u4f9d\u8d56\u4e8e\u56fe\u50cf\u7279\u5f81\u63d0\u53d6\u548c\u5149\u6d41\u8ba1\u7b97\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u7cbe\u5ea6\u4e0d\u7a33\u5b9a\u3002\u4e3a\u4e86\u89e3\u51b3\u5bf9\u5e94\u5173\u7cfb\u95ee\u9898\uff0c\u6709\u4eba\u5c1d\u8bd5\u4f7f\u7528\u7ebf\u7279\u5f81\u548c\u878d\u5408\u5176\u4ed6\u4f20\u611f\u5668\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u4ecd\u7136\u4f9d\u8d56\u4e8e\u5bf9\u5e94\u5173\u7cfb\u3002", "method": "\u8be5\u7b97\u6cd5\u901a\u8fc7\u5206\u6790\u7ebf\u5728\u56fe\u50cf\u7a7a\u95f4\u4e2d\u626b\u8fc7\u7684\u66f2\u9762\uff08\u5373 ruled surface\uff09\u7684\u5f62\u72b6\u6765\u83b7\u53d6\u91cc\u7a0b\u8ba1\u4fe1\u606f\u3002\u8be5\u65b9\u6cd5\u53ea\u9700\u8981\u70b9\u5230\u7ebf\u7684\u5173\u8054\u7684\u5fae\u5206\u66f4\u65b0\u3002\u7ed3\u5408IMU\u7684\u60ef\u6027\u6d4b\u91cf\uff0c\u53ef\u4ee5\u51cf\u5c11\u89e3\u7a7a\u95f4\u7684\u7ef4\u5ea6\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u53ef\u4ee5\u4ece ruled surface \u91cd\u5efa 3D \u573a\u666f\u548c\u89c6\u89c9\u91cc\u7a0b\u8ba1\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408IMU\u6570\u636e\u7ea6\u675f\u66f2\u9762\uff0c\u53ef\u4ee5\u5927\u5927\u964d\u4f4e\u89e3\u7a7a\u95f4\u7684\u7ef4\u5ea6\uff0c\u63d0\u9ad8\u91cd\u5efa\u548c\u91cc\u7a0b\u8ba1\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2512.01099", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01099", "abs": "https://arxiv.org/abs/2512.01099", "authors": ["Daria Smirnova", "Hamid Nasiri", "Marta Adamska", "Zhengxin Yu", "Peter Garraghan"], "title": "Energy-Aware Data-Driven Model Selection in LLM-Orchestrated AI Systems", "comment": null, "summary": "As modern artificial intelligence (AI) systems become more advanced and capable, they can leverage a wide range of tools and models to perform complex tasks. Today, the task of orchestrating these models is often performed by Large Language Models (LLMs) that rely on qualitative descriptions of models for decision-making. However, the descriptions provided to these LLM-based orchestrators do not reflect true model capabilities and performance characteristics, leading to suboptimal model selection, reduced accuracy, and increased energy costs. In this paper, we conduct an empirical analysis of LLM-based orchestration limitations and propose GUIDE, a new energy-aware model selection framework that accounts for performance-energy trade-offs by incorporating quantitative model performance characteristics in decision-making. Experimental results demonstrate that GUIDE increases accuracy by 0.90%-11.92% across various evaluated tasks, and achieves up to 54% energy efficiency improvement, while reducing orchestrator model selection latency from 4.51 s to 7.2 ms.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u901a\u5e38\u7528\u4e8e\u7f16\u6392AI\u6a21\u578b\uff0c\u4f46\u5b83\u4eec\u4f9d\u8d56\u4e8e\u6a21\u578b\u7684\u4e0d\u51c6\u786e\u63cf\u8ff0\uff0c\u5bfc\u81f4\u6b21\u4f18\u9009\u62e9\u548c\u80fd\u6e90\u6210\u672c\u589e\u52a0\u3002\u672c\u6587\u63d0\u51fa\u4e86GUIDE\uff0c\u4e00\u79cd\u8003\u8651\u6027\u80fd-\u80fd\u6e90\u6743\u8861\u7684\u80fd\u6e90\u611f\u77e5\u6a21\u578b\u9009\u62e9\u6846\u67b6\u3002", "motivation": "\u57fa\u4e8eLLM\u7684\u7f16\u6392\u5668\u4f9d\u8d56\u4e8e\u6a21\u578b\u7684\u5b9a\u6027\u63cf\u8ff0\uff0c\u8fd9\u4e0d\u80fd\u53cd\u6620\u771f\u5b9e\u7684\u6a21\u578b\u80fd\u529b\u548c\u6027\u80fd\u7279\u5f81\uff0c\u5bfc\u81f4\u6a21\u578b\u9009\u62e9\u4e0d\u4f73\u3001\u51c6\u786e\u6027\u964d\u4f4e\u548c\u80fd\u6e90\u6210\u672c\u589e\u52a0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u80fd\u6e90\u611f\u77e5\u6a21\u578b\u9009\u62e9\u6846\u67b6GUIDE\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5728\u51b3\u7b56\u4e2d\u7ed3\u5408\u5b9a\u91cf\u6a21\u578b\u6027\u80fd\u7279\u5f81\u6765\u8003\u8651\u6027\u80fd-\u80fd\u6e90\u6743\u8861\u3002", "result": "GUIDE\u5728\u5404\u79cd\u8bc4\u4f30\u4efb\u52a1\u4e2d\u5c06\u51c6\u786e\u7387\u63d0\u9ad8\u4e860.90%-11.92%\uff0c\u5e76\u5b9e\u73b0\u4e86\u9ad8\u8fbe54%\u7684\u80fd\u6e90\u6548\u7387\u63d0\u5347\uff0c\u540c\u65f6\u5c06\u7f16\u6392\u5668\u6a21\u578b\u9009\u62e9\u5ef6\u8fdf\u4ece4.51\u79d2\u964d\u4f4e\u52307.2\u6beb\u79d2\u3002", "conclusion": "\u5b9e\u8bc1\u5206\u6790\u8868\u660e\uff0c\u5f53\u524d\u57fa\u4e8eLLM\u7684\u7f16\u6392\u5b58\u5728\u5c40\u9650\u6027\uff0c\u800c\u6240\u63d0\u51fa\u7684GUIDE\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u80fd\u6e90\u6548\u7387\uff0c\u5e76\u964d\u4f4e\u6a21\u578b\u9009\u62e9\u5ef6\u8fdf\u3002"}}
{"id": "2512.00947", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00947", "abs": "https://arxiv.org/abs/2512.00947", "authors": ["Liyao Li", "Chao Ye", "Wentao Ye", "Yifei Sun", "Zhe Jiang", "Haobo Wang", "Jiaming Tian", "Yiming Zhang", "Ningtao Wang", "Xing Fu", "Gang Chen", "Junbo Zhao"], "title": "Table as a Modality for Large Language Models", "comment": "Accepted to NeurIPS 2025", "summary": "To migrate the remarkable successes of Large Language Models (LLMs), the community has made numerous efforts to generalize them to the table reasoning tasks for the widely deployed tabular data. Despite that, in this work, by showing a probing experiment on our proposed StructQA benchmark, we postulate that even the most advanced LLMs (such as GPTs) may still fall short of coping with tabular data. More specifically, the current scheme often simply relies on serializing the tabular data, together with the meta information, then inputting them through the LLMs. We argue that the loss of structural information is the root of this shortcoming. In this work, we further propose TAMO, which bears an ideology to treat the tables as an independent modality integrated with the text tokens. The resulting model in TAMO is a multimodal framework consisting of a hypergraph neural network as the global table encoder seamlessly integrated with the mainstream LLM. Empirical results on various benchmarking datasets, including HiTab, WikiTQ, WikiSQL, FeTaQA, and StructQA, have demonstrated significant improvements on generalization with an average relative gain of 42.65%.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8868\u683c\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u8868\u683c\u7ed3\u6784\u4fe1\u606f\u4e22\u5931\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7b80\u5355\u5730\u5e8f\u5217\u5316\u8868\u683c\u6570\u636e\uff0c\u5ffd\u7565\u4e86\u7ed3\u6784\u4fe1\u606f\u3002", "method": "\u63d0\u51faTAMO\u6a21\u578b\uff0c\u5c06\u8868\u683c\u89c6\u4e3a\u72ec\u7acb\u6a21\u6001\uff0c\u4f7f\u7528\u8d85\u56fe\u795e\u7ecf\u7f51\u7edc\u4f5c\u4e3a\u5168\u5c40\u8868\u683c\u7f16\u7801\u5668\u4e0e\u4e3b\u6d41LLM\u96c6\u6210\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u5e73\u5747\u76f8\u5bf9\u589e\u76ca\u4e3a42.65%\u3002", "conclusion": "TAMO\u6a21\u578b\u901a\u8fc7\u4fdd\u7559\u8868\u683c\u7ed3\u6784\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86LLM\u5728\u8868\u683c\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2512.00521", "categories": ["cs.LG", "cs.CL", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2512.00521", "abs": "https://arxiv.org/abs/2512.00521", "authors": ["Sabrina Islam", "Md. Atiqur Rahman", "Md. Bakhtiar Hasan", "Md. Hasanul Kabir"], "title": "Rep3Net: An Approach Exploiting Multimodal Representation for Molecular Bioactivity Prediction", "comment": null, "summary": "In early stage drug discovery, bioactivity prediction of molecules against target proteins plays a crucial role. Trdaitional QSAR models that utilizes molecular descriptor based data often struggles to predict bioactivity of molecules effectively due to its limitation in capturing structural and contextual information embedded within each compound. To address this challenge, we propose Rep3Net, a unified deep learning architecture that not only incorporates descriptor data but also includes spatial and relational information through graph-based represenation of compounds and contextual information through ChemBERTa generated embeddings from SMILES strings. Our model employing multimodal concatenated features produce reliable bioactivity prediction on Poly [ADP-ribose] polymerase 1 (PARP-1) dataset. PARP-1 is a crucial agent in DNA damage repair and has become a significant theraputic target in malignancies that depend on it for survival and growth. A comprehensive analysis and comparison with conventional standalone models including GCN, GAT, XGBoost, etc. demonstrates that our architecture achieves the highest predictive performance. In computational screening of compounds in drug discovery, our architecture provides a scalable framework for bioactivity prediction.", "AI": {"tldr": "\u63d0\u51fa\u4e86Rep3Net\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u9884\u6d4b\u5206\u5b50\u5bf9\u9776\u86cb\u767d\u7684\u751f\u7269\u6d3b\u6027\u3002", "motivation": "\u4f20\u7edf\u7684QSAR\u6a21\u578b\u96be\u4ee5\u6709\u6548\u9884\u6d4b\u5206\u5b50\u7684\u751f\u7269\u6d3b\u6027\uff0c\u56e0\u4e3a\u5b83\u5728\u6355\u83b7\u5316\u5408\u7269\u4e2d\u5d4c\u5165\u7684\u7ed3\u6784\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u8be5\u6a21\u578b\u4e0d\u4ec5\u5305\u542b\u63cf\u8ff0\u7b26\u6570\u636e\uff0c\u8fd8\u5305\u62ec\u57fa\u4e8e\u56fe\u7684\u5316\u5408\u7269\u8868\u793a\u7684\u7a7a\u95f4\u548c\u5173\u7cfb\u4fe1\u606f\uff0c\u4ee5\u53ca\u901a\u8fc7 ChemBERTa \u4ece SMILES \u5b57\u7b26\u4e32\u751f\u6210\u7684\u5d4c\u5165\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002\u8be5\u6a21\u578b\u91c7\u7528\u591a\u6a21\u6001\u8fde\u63a5\u7279\u5f81\u3002", "result": "\u5728Poly [ADP-ribose] polymerase 1 (PARP-1) \u6570\u636e\u96c6\u4e0a\u4ea7\u751f\u4e86\u53ef\u9760\u7684\u751f\u7269\u6d3b\u6027\u9884\u6d4b\u3002\u4e0e\u5305\u62ecGCN\u3001GAT\u3001XGBoost\u7b49\u5728\u5185\u7684\u4f20\u7edf\u72ec\u7acb\u6a21\u578b\u8fdb\u884c\u5168\u9762\u5206\u6790\u548c\u6bd4\u8f83\uff0c\u8868\u660e\u8be5\u67b6\u6784\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u8be5\u67b6\u6784\u4e3a\u836f\u7269\u53d1\u73b0\u4e2d\u7684\u5316\u5408\u7269\u8ba1\u7b97\u7b5b\u9009\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u751f\u7269\u6d3b\u6027\u9884\u6d4b\u6846\u67b6\u3002"}}
{"id": "2512.00336", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00336", "abs": "https://arxiv.org/abs/2512.00336", "authors": ["Mengxue Hu", "Yunfeng Diao", "Changtao Miao", "Jianshu Li", "Zhe Li", "Joey Tianyi Zhou"], "title": "MVAD : A Comprehensive Multimodal Video-Audio Dataset for AIGC Detection", "comment": "7 pages,2 figures", "summary": "The rapid advancement of AI-generated multimodal video-audio content has raised significant concerns regarding information security and content authenticity. Existing synthetic video datasets predominantly focus on the visual modality alone, while the few incorporating audio are largely confined to facial deepfakes--a limitation that fails to address the expanding landscape of general multimodal AI-generated content and substantially impedes the development of trustworthy detection systems. To bridge this critical gap, we introduce the Multimodal Video-Audio Dataset (MVAD), the first comprehensive dataset specifically designed for detecting AI-generated multimodal video-audio content. Our dataset exhibits three key characteristics: (1) genuine multimodality with samples generated according to three realistic video-audio forgery patterns; (2) high perceptual quality achieved through diverse state-of-the-art generative models; and (3) comprehensive diversity spanning realistic and anime visual styles, four content categories (humans, animals, objects, and scenes), and four video-audio multimodal data types. Our dataset will be available at https://github.com/HuMengXue0104/MVAD.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u68c0\u6d4bAI\u751f\u6210\u7684\u591a\u6a21\u6001\u89c6\u9891-\u97f3\u9891\u5185\u5bb9\u7684\u7efc\u5408\u6570\u636e\u96c6MVAD\u3002", "motivation": "\u73b0\u6709\u7684\u5408\u6210\u89c6\u9891\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u89c6\u89c9\u6a21\u6001\uff0c\u800c\u5c11\u6570\u5305\u542b\u97f3\u9891\u7684\u6570\u636e\u96c6\u4e3b\u8981\u5c40\u9650\u4e8e\u9762\u90e8\u6df1\u5ea6\u4f2a\u9020\uff0c\u8fd9\u65e0\u6cd5\u89e3\u51b3\u901a\u7528\u591a\u6a21\u6001AI\u751f\u6210\u5185\u5bb9\u7684\u6269\u5c55\uff0c\u5e76\u4e25\u91cd\u963b\u788d\u4e86\u53ef\u4fe1\u68c0\u6d4b\u7cfb\u7edf\u7684\u5f00\u53d1\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u89c6\u9891-\u97f3\u9891\u6570\u636e\u96c6\uff08MVAD\uff09\u3002\u8be5\u6570\u636e\u96c6\u5305\u542b\u6839\u636e\u4e09\u79cd\u771f\u5b9e\u7684\u89c6\u9891-\u97f3\u9891\u4f2a\u9020\u6a21\u5f0f\u751f\u6210\u7684\u6837\u672c\u3002", "result": "\u8be5\u6570\u636e\u96c6\u5177\u6709\u4e09\u4e2a\u5173\u952e\u7279\u5f81\uff1a(1) \u771f\u6b63\u7684\u591a\u6a21\u6001\uff1b(2) \u901a\u8fc7\u5404\u79cd\u6700\u5148\u8fdb\u7684\u751f\u6210\u6a21\u578b\u5b9e\u73b0\u7684\u9ad8\u611f\u77e5\u8d28\u91cf\uff1b(3) \u6db5\u76d6\u73b0\u5b9e\u548c\u52a8\u6f2b\u89c6\u89c9\u98ce\u683c\u3001\u56db\u79cd\u5185\u5bb9\u7c7b\u522b\uff08\u4eba\u7c7b\u3001\u52a8\u7269\u3001\u7269\u4f53\u548c\u573a\u666f\uff09\u4ee5\u53ca\u56db\u79cd\u89c6\u9891-\u97f3\u9891\u591a\u6a21\u6001\u6570\u636e\u7c7b\u578b\u7684\u5168\u9762\u591a\u6837\u6027\u3002", "conclusion": "MVAD\u662f\u9996\u4e2a\u4e13\u95e8\u7528\u4e8e\u68c0\u6d4bAI\u751f\u6210\u7684\u591a\u6a21\u6001\u89c6\u9891-\u97f3\u9891\u5185\u5bb9\u7684\u7efc\u5408\u6570\u636e\u96c6\u3002"}}
{"id": "2512.01107", "categories": ["cs.AI", "econ.EM", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.01107", "abs": "https://arxiv.org/abs/2512.01107", "authors": ["Sanjog Misra"], "title": "Foundation Priors", "comment": null, "summary": "Foundation models, and in particular large language models, can generate highly informative responses, prompting growing interest in using these ''synthetic'' outputs as data in empirical research and decision-making. This paper introduces the idea of a foundation prior, which shows that model-generated outputs are not as real observations, but draws from the foundation prior induced prior predictive distribution. As such synthetic data reflects both the model's learned patterns and the user's subjective priors, expectations, and biases. We model the subjectivity of the generative process by making explicit the dependence of synthetic outputs on the user's anticipated data distribution, the prompt-engineering process, and the trust placed in the foundation model.\n  We derive the foundation prior as an exponential-tilted, generalized Bayesian update of the user's primitive prior, where a trust parameter governs the weight assigned to synthetic data. We then show how synthetic data and the associated foundation prior can be incorporated into standard statistical and econometric workflows, and discuss their use in applications such as refining complex models, informing latent constructs, guiding experimental design, and augmenting random-coefficient and partially linear specifications. By treating generative outputs as structured, explicitly subjective priors rather than as empirical observations, the framework offers a principled way to harness foundation models in empirical work while avoiding the conflation of synthetic ''facts'' with real data.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u201c\u5408\u6210\u201d\u8f93\u51fa\u88ab\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4f5c\u5b9e\u8bc1\u7814\u7a76\u548c\u51b3\u7b56\u7684\u6570\u636e\u3002\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u57fa\u7840\u5148\u9a8c\u7684\u6982\u5ff5\uff0c\u8868\u660e\u6a21\u578b\u751f\u6210\u7684\u8f93\u51fa\u4e0d\u662f\u771f\u5b9e\u7684\u89c2\u5bdf\u7ed3\u679c\uff0c\u800c\u662f\u6765\u81ea\u57fa\u7840\u5148\u9a8c\u5f15\u8d77\u7684\u5148\u9a8c\u9884\u6d4b\u5206\u5e03\u3002", "motivation": "\u5b9e\u8bc1\u7814\u7a76\u548c\u51b3\u7b56\u8d8a\u6765\u8d8a\u591a\u5730\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u201c\u5408\u6210\u201d\u8f93\u51fa\u4f5c\u4e3a\u6570\u636e\u3002\u56e0\u6b64\u9700\u8981\u5bf9\u8fd9\u79cd\u505a\u6cd5\u8fdb\u884c\u5206\u6790\u3002", "method": "\u8be5\u8bba\u6587\u901a\u8fc7\u660e\u786e\u5408\u6210\u8f93\u51fa\u5bf9\u7528\u6237\u9884\u671f\u6570\u636e\u5206\u5e03\u3001\u63d0\u793a\u5de5\u7a0b\u8fc7\u7a0b\u548c\u5bf9\u57fa\u7840\u6a21\u578b\u7684\u4fe1\u4efb\u7684\u4f9d\u8d56\u6027\uff0c\u5bf9\u751f\u6210\u8fc7\u7a0b\u7684\u4e3b\u89c2\u6027\u8fdb\u884c\u5efa\u6a21\u3002\u8be5\u8bba\u6587\u5c06\u57fa\u7840\u5148\u9a8c\u63a8\u5bfc\u4e3a\u7528\u6237\u539f\u59cb\u5148\u9a8c\u7684\u6307\u6570\u503e\u659c\u3001\u5e7f\u4e49\u8d1d\u53f6\u65af\u66f4\u65b0\uff0c\u5176\u4e2d\u4fe1\u4efb\u53c2\u6570\u63a7\u5236\u5206\u914d\u7ed9\u5408\u6210\u6570\u636e\u7684\u6743\u91cd\u3002", "result": "\u8be5\u8bba\u6587\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u5408\u6210\u6570\u636e\u548c\u76f8\u5173\u7684\u57fa\u7840\u5148\u9a8c\u7eb3\u5165\u6807\u51c6\u7edf\u8ba1\u548c\u8ba1\u91cf\u7ecf\u6d4e\u5b66\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5e76\u8ba8\u8bba\u4e86\u5b83\u4eec\u5728\u6539\u8fdb\u590d\u6742\u6a21\u578b\u3001\u901a\u77e5\u6f5c\u5728\u7ed3\u6784\u3001\u6307\u5bfc\u5b9e\u9a8c\u8bbe\u8ba1\u4ee5\u53ca\u589e\u5f3a\u968f\u673a\u7cfb\u6570\u548c\u90e8\u5206\u7ebf\u6027\u89c4\u8303\u7b49\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\u3002", "conclusion": "\u901a\u8fc7\u5c06\u751f\u6210\u8f93\u51fa\u89c6\u4e3a\u7ed3\u6784\u5316\u7684\u3001\u660e\u786e\u4e3b\u89c2\u7684\u5148\u9a8c\uff0c\u800c\u4e0d\u662f\u4f5c\u4e3a\u7ecf\u9a8c\u89c2\u5bdf\uff0c\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u5728\u5b9e\u8bc1\u5de5\u4f5c\u4e2d\u5229\u7528\u57fa\u7840\u6a21\u578b\u7684\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u540c\u65f6\u907f\u514d\u4e86\u5408\u6210\u201c\u4e8b\u5b9e\u201d\u4e0e\u771f\u5b9e\u6570\u636e\u7684\u6df7\u6dc6\u3002"}}
{"id": "2512.00986", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00986", "abs": "https://arxiv.org/abs/2512.00986", "authors": ["Zhihan Guo", "Feiyang Xu", "Yifan Li", "Muzhi Li", "Shuai Zou", "Jiele Wu", "Han Shi", "Haoli Bai", "Ho-fung Leung", "Irwin King"], "title": "Dr.Mi-Bench: A Modular-integrated Benchmark for Scientific Deep Research Agent", "comment": null, "summary": "The explosive growth in academic literature necessitates automated deep research (DR) agents, yet their evaluation remains a significant challenge. First, existing benchmarks often focus narrowly on retrieval while neglecting high-level planning and reasoning. Second, existing benchmarks favor general domains over the scientific domains that are the core application for DR agents. To address these gaps, we introduce Dr.Mi-Bench, a Modular-integrated benchmark for scientific DR agents. Grounded in academic literature, our benchmark uses a human-annotated dataset of 200 instances across 10 scientific domains, including both research and review papers. Besides, we also propose a Modular-integrated Evaluation Paradigm for DR Agents (Dr.Mi-Eval), a novel modular-integrated evaluation paradigm, which leverages the rich structure of academic papers to assess the core competencies of planning, retrieval, and reasoning through two complementary modes: an end-to-end evaluation for DR agents and an isolated evaluation for foundational LLMs as potential backbones. Experimental results reveal a fragmented performance landscape: agents exhibit specialized strengths but share critical weaknesses, most notably in performing the multi-source retrieval required for review-style tasks and performing consistently across diverse scientific fields. Moreover, improving high-level planning capability is the crucial factor for unlocking the reasoning potential of foundational LLMs as backbones. By exposing these actionable failure modes, Dr.Mi-Bench provides a diagnostic tool to guide the development of more reliable academic research assistants.", "AI": {"tldr": "\u63d0\u51fa\u4e86Dr.Mi-Bench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u79d1\u5b66\u7814\u7a76\u578bDR\u4ee3\u7406\u7684\u6a21\u5757\u5316\u96c6\u6210\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u8fc7\u4e8e\u5173\u6ce8\u68c0\u7d22\uff0c\u800c\u5ffd\u7565\u4e86\u9ad8\u5c42\u6b21\u7684\u89c4\u5212\u548c\u63a8\u7406\uff0c\u5e76\u4e14\u504f\u5411\u4e8e\u901a\u7528\u9886\u57df\u800c\u4e0d\u662f\u79d1\u5b66\u9886\u57df\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u4eba\u5de5\u6ce8\u91ca\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b10\u4e2a\u79d1\u5b66\u9886\u57df\u7684200\u4e2a\u5b9e\u4f8b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u96c6\u6210\u7684\u8bc4\u4f30\u8303\u5f0fDr.Mi-Eval\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDR\u4ee3\u7406\u5728\u591a\u6e90\u68c0\u7d22\u548c\u8de8\u9886\u57df\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u660e\u663e\u4e0d\u8db3\uff0c\u63d0\u9ad8\u9ad8\u5c42\u6b21\u89c4\u5212\u80fd\u529b\u662f\u91ca\u653eLLM\u63a8\u7406\u6f5c\u529b\u7684\u5173\u952e\u3002", "conclusion": "Dr.Mi-Bench\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8bca\u65ad\u5de5\u5177\uff0c\u7528\u4e8e\u6307\u5bfc\u66f4\u53ef\u9760\u7684\u5b66\u672f\u7814\u7a76\u52a9\u624b\u7684\u5f00\u53d1\u3002"}}
{"id": "2512.00524", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.00524", "abs": "https://arxiv.org/abs/2512.00524", "authors": ["Guangjie Zeng", "Hao Peng", "Angsheng Li", "Li Sun", "Chunyang Liu", "Shengze Li", "Yicheng Pan", "Philip S. Yu"], "title": "Hyperbolic Continuous Structural Entropy for Hierarchical Clustering", "comment": "14 pages, accepted by AAAI 2026", "summary": "Hierarchical clustering is a fundamental machine-learning technique for grouping data points into dendrograms. However, existing hierarchical clustering methods encounter two primary challenges: 1) Most methods specify dendrograms without a global objective. 2) Graph-based methods often neglect the significance of graph structure, optimizing objectives on complete or static predefined graphs. In this work, we propose Hyperbolic Continuous Structural Entropy neural networks, namely HypCSE, for structure-enhanced continuous hierarchical clustering. Our key idea is to map data points in the hyperbolic space and minimize the relaxed continuous structural entropy (SE) on structure-enhanced graphs. Specifically, we encode graph vertices in hyperbolic space using hyperbolic graph neural networks and minimize approximate SE defined on graph embeddings. To make the SE objective differentiable for optimization, we reformulate it into a function using the lowest common ancestor (LCA) on trees and then relax it into continuous SE (CSE) by the analogy of hyperbolic graph embeddings and partitioning trees. To ensure a graph structure that effectively captures the hierarchy of data points for CSE calculation, we employ a graph structure learning (GSL) strategy that updates the graph structure during training. Extensive experiments on seven datasets demonstrate the superior performance of HypCSE.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7ed3\u6784\u589e\u5f3a\u8fde\u7eed\u5c42\u6b21\u805a\u7c7b\u7684\u53cc\u66f2\u8fde\u7eed\u7ed3\u6784\u71b5\u795e\u7ecf\u7f51\u7edc\uff08HypCSE\uff09\u3002", "motivation": "\u73b0\u6709\u7684\u5c42\u6b21\u805a\u7c7b\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1) \u5927\u591a\u6570\u65b9\u6cd5\u5728\u6ca1\u6709\u5168\u5c40\u76ee\u6807\u7684\u60c5\u51b5\u4e0b\u6307\u5b9a\u6811\u72b6\u56fe\u30022) \u57fa\u4e8e\u56fe\u7684\u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u56fe\u7ed3\u6784\u7684\u91cd\u8981\u6027\uff0c\u5728\u5b8c\u6574\u6216\u9759\u6001\u7684\u9884\u5b9a\u4e49\u56fe\u4e0a\u4f18\u5316\u76ee\u6807\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u6570\u636e\u70b9\u6620\u5c04\u5230\u53cc\u66f2\u7a7a\u95f4\uff0c\u5e76\u6700\u5c0f\u5316\u7ed3\u6784\u589e\u5f3a\u56fe\u4e0a\u7684\u677e\u5f1b\u8fde\u7eed\u7ed3\u6784\u71b5\uff08SE\uff09\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u4f7f\u7528\u53cc\u66f2\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u53cc\u66f2\u7a7a\u95f4\u4e2d\u7f16\u7801\u56fe\u9876\u70b9\uff0c\u5e76\u6700\u5c0f\u5316\u56fe\u5d4c\u5165\u4e0a\u5b9a\u4e49\u7684\u8fd1\u4f3cSE\u3002\u4e3a\u4e86\u4f7fSE\u76ee\u6807\u53ef\u5fae\u4ee5\u8fdb\u884c\u4f18\u5316\uff0c\u4f7f\u7528\u6811\u4e0a\u7684\u6700\u4f4e\u516c\u5171\u7956\u5148\uff08LCA\uff09\u5c06\u5176\u91cd\u65b0\u8868\u8ff0\u4e3a\u4e00\u4e2a\u51fd\u6570\uff0c\u7136\u540e\u901a\u8fc7\u53cc\u66f2\u56fe\u5d4c\u5165\u548c\u5212\u5206\u6811\u7684\u7c7b\u6bd4\u5c06\u5176\u653e\u5bbd\u4e3a\u8fde\u7eedSE\uff08CSE\uff09\u3002\u4e3a\u4e86\u786e\u4fdd\u56fe\u7ed3\u6784\u6709\u6548\u5730\u6355\u83b7\u6570\u636e\u70b9\u7684\u5c42\u6b21\u7ed3\u6784\u4ee5\u8fdb\u884cCSE\u8ba1\u7b97\uff0c\u91c7\u7528\u4e86\u4e00\u79cd\u56fe\u7ed3\u6784\u5b66\u4e60\uff08GSL\uff09\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u5728\u8bad\u7ec3\u671f\u95f4\u66f4\u65b0\u56fe\u7ed3\u6784\u3002", "result": "\u5728\u4e03\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\u4e86HypCSE\u7684\u5353\u8d8a\u6027\u80fd\u3002", "conclusion": "HypCSE\u5728\u5c42\u6b21\u805a\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u901a\u8fc7\u5728\u53cc\u66f2\u7a7a\u95f4\u4e2d\u5d4c\u5165\u6570\u636e\u5e76\u4f18\u5316\u8fde\u7eed\u7ed3\u6784\u71b5\uff0c\u6709\u6548\u5730\u5b66\u4e60\u6570\u636e\u7684\u5c42\u6b21\u7ed3\u6784\u3002"}}
{"id": "2512.00343", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00343", "abs": "https://arxiv.org/abs/2512.00343", "authors": ["Zhongqi Wang", "Jie Zhang", "Shiguang Shan", "Xilin Chen"], "title": "Assimilation Matters: Model-level Backdoor Detection in Vision-Language Pretrained Models", "comment": null, "summary": "Vision-language pretrained models (VLPs) such as CLIP have achieved remarkable success, but are also highly vulnerable to backdoor attacks. Given a model fine-tuned by an untrusted third party, determining whether the model has been injected with a backdoor is a critical and challenging problem. Existing detection methods usually rely on prior knowledge of training dataset, backdoor triggers and targets, or downstream classifiers, which may be impractical for real-world applications. To address this, To address this challenge, we introduce Assimilation Matters in DETection (AMDET), a novel model-level detection framework that operates without any such prior knowledge. Specifically, we first reveal the feature assimilation property in backdoored text encoders: the representations of all tokens within a backdoor sample exhibit a high similarity. Further analysis attributes this effect to the concentration of attention weights on the trigger token. Leveraging this insight, AMDET scans a model by performing gradient-based inversion on token embeddings to recover implicit features that capable of activating backdoor behaviors. Furthermore, we identify the natural backdoor feature in the OpenAI's official CLIP model, which are not intentionally injected but still exhibit backdoor-like behaviors. We then filter them out from real injected backdoor by analyzing their loss landscapes. Extensive experiments on 3,600 backdoored and benign-finetuned models with two attack paradigms and three VLP model structures show that AMDET detects backdoors with an F1 score of 89.90%. Besides, it achieves one complete detection in approximately 5 minutes on a RTX 4090 GPU and exhibits strong robustness against adaptive attacks. Code is available at: https://github.com/Robin-WZQ/AMDET", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAMDET\u7684\u65b0\u578b\u6a21\u578b\u7ea7\u540e\u95e8\u68c0\u6d4b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u65e0\u9700\u4efb\u4f55\u5148\u9a8c\u77e5\u8bc6\u5373\u53ef\u8fd0\u884c\uff0c\u901a\u8fc7\u68af\u5ea6\u53cd\u6f14token\u5d4c\u5165\u6765\u6062\u590d\u6fc0\u6d3b\u540e\u95e8\u884c\u4e3a\u7684\u9690\u5f0f\u7279\u5f81\uff0c\u5e76\u5206\u6790\u635f\u5931\u60c5\u51b5\u4ee5\u8fc7\u6ee4\u6389\u81ea\u7136\u540e\u95e8\u7279\u5f81\u3002", "motivation": "\u73b0\u6709\u7684\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u8bad\u7ec3\u6570\u636e\u96c6\u3001\u540e\u95e8\u89e6\u53d1\u5668\u548c\u76ee\u6807\u6216\u4e0b\u6e38\u5206\u7c7b\u5668\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u80fd\u4e0d\u5207\u5b9e\u9645\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "AMDET\u9996\u5148\u63ed\u793a\u4e86\u540e\u95e8\u6587\u672c\u7f16\u7801\u5668\u4e2d\u7684\u7279\u5f81\u540c\u5316\u5c5e\u6027\uff1a\u540e\u95e8\u6837\u672c\u4e2d\u6240\u6709token\u7684\u8868\u793a\u90fd\u8868\u73b0\u51fa\u9ad8\u5ea6\u76f8\u4f3c\u6027\u3002 \u8fdb\u4e00\u6b65\u7684\u5206\u6790\u5c06\u8fd9\u79cd\u6548\u5e94\u5f52\u56e0\u4e8e\u6ce8\u610f\u529b\u6743\u91cd\u96c6\u4e2d\u5728\u89e6\u53d1token\u4e0a\u3002\u5229\u7528\u8fd9\u79cd\u6d1e\u5bdf\u529b\uff0cAMDET\u901a\u8fc7\u5bf9token\u5d4c\u5165\u6267\u884c\u57fa\u4e8e\u68af\u5ea6\u7684\u53cd\u6f14\u6765\u626b\u63cf\u6a21\u578b\uff0c\u4ee5\u6062\u590d\u80fd\u591f\u6fc0\u6d3b\u540e\u95e8\u884c\u4e3a\u7684\u9690\u5f0f\u7279\u5f81\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bc6\u522b\u4e86OpenAI\u5b98\u65b9CLIP\u6a21\u578b\u4e2d\u7684\u81ea\u7136\u540e\u95e8\u7279\u5f81\uff0c\u8fd9\u4e9b\u7279\u5f81\u4e0d\u662f\u6545\u610f\u6ce8\u5165\u7684\uff0c\u4f46\u4ecd\u7136\u8868\u73b0\u51fa\u7c7b\u4f3c\u540e\u95e8\u7684\u884c\u4e3a\u3002\u7136\u540e\uff0c\u6211\u4eec\u901a\u8fc7\u5206\u6790\u5b83\u4eec\u7684\u635f\u5931\u60c5\u51b5\uff0c\u5c06\u5b83\u4eec\u4ece\u771f\u6b63\u6ce8\u5165\u7684\u540e\u95e8\u4e2d\u8fc7\u6ee4\u51fa\u6765\u3002", "result": "\u5728\u5177\u6709\u4e24\u79cd\u653b\u51fb\u8303\u4f8b\u548c\u4e09\u79cdVLP\u6a21\u578b\u7ed3\u6784\u76843600\u4e2a\u540e\u95e8\u548c\u826f\u6027\u5fae\u8c03\u6a21\u578b\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cAMDET\u68c0\u6d4b\u540e\u95e8\u7684F1\u5f97\u5206\u4e3a89.90%\u3002\u6b64\u5916\uff0c\u5b83\u5728RTX 4090 GPU\u4e0a\u5927\u7ea65\u5206\u949f\u5185\u5b8c\u6210\u4e00\u6b21\u5b8c\u6574\u68c0\u6d4b\uff0c\u5e76\u5bf9\u81ea\u9002\u5e94\u653b\u51fb\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "AMDET\u6846\u67b6\u5728\u68c0\u6d4b\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u7684\u540e\u95e8\u653b\u51fb\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\uff0c\u4e14\u5177\u6709\u8f83\u9ad8\u7684\u68c0\u6d4b\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2512.01149", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01149", "abs": "https://arxiv.org/abs/2512.01149", "authors": ["Krishna Taduri", "Shaunak Dhande", "Giacinto Paolo", "Saggese", "Paul Smith"], "title": "A Benchmark of Causal vs Correlation AI for Predictive Maintenance", "comment": null, "summary": "Predictive maintenance in manufacturing environments presents a challenging optimization problem characterized by extreme cost asymmetry, where missed failures incur costs roughly fifty times higher than false alarms. Conventional machine learning approaches typically optimize statistical accuracy metrics that do not reflect this operational reality and cannot reliably distinguish causal relationships from spurious correlations. This study evaluates eight predictive models, ranging from baseline statistical approaches to formal causal inference methods, on a dataset of 10,000 CNC machines with a 3.3% failure prevalence. The formal causal inference model (L5) achieved estimated annual cost savings of 1.16 million USD (a 70.2 percent reduction), outperforming the best correlation-based decision tree model (L3) by approximately 80,000 USD per year. The causal model matched the highest observed recall (87.9 percent) while reducing false alarms by 97 percent (from 165 to 5) and attained a precision of 92.1 percent, with a train-test performance gap of only 2.6 percentage points. These results indicate that causal AI methods, when combined with domain knowledge, can yield superior financial outcomes and more interpretable predictions compared to correlation-based approaches in predictive maintenance applications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u516b\u79cd\u9884\u6d4b\u6a21\u578b\u5728\u6570\u63a7\u673a\u5e8a\u9884\u6d4b\u6027\u7ef4\u62a4\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u57fa\u4e8e\u56e0\u679c\u63a8\u7406\u7684\u6a21\u578b\u5728\u964d\u4f4e\u6210\u672c\u548c\u51cf\u5c11\u8bef\u62a5\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u7684\u57fa\u4e8e\u76f8\u5173\u6027\u7684\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u9884\u6d4b\u6027\u7ef4\u62a4\u4e2d\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u6210\u672c\u4e0d\u5bf9\u79f0\u95ee\u9898\uff0c\u4e14\u96be\u4ee5\u533a\u5206\u56e0\u679c\u5173\u7cfb\u548c\u865a\u5047\u76f8\u5173\u3002", "method": "\u5728\u5305\u542b10000\u53f0\u6570\u63a7\u673a\u5e8a\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u8bc4\u4f30\u4e86\u4ece\u57fa\u7ebf\u7edf\u8ba1\u65b9\u6cd5\u5230\u6b63\u5f0f\u56e0\u679c\u63a8\u7406\u65b9\u6cd5\u7684\u516b\u79cd\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u6b63\u5f0f\u56e0\u679c\u63a8\u7406\u6a21\u578b\u5b9e\u73b0\u4e86116\u4e07\u7f8e\u5143\u7684\u5e74\u6210\u672c\u8282\u7701\uff08\u964d\u4f4e70.2%\uff09\uff0c\u6bd4\u6700\u4f73\u7684\u57fa\u4e8e\u76f8\u5173\u6027\u7684\u51b3\u7b56\u6811\u6a21\u578b\u6bcf\u5e74\u8282\u7701\u7ea68\u4e07\u7f8e\u5143\u3002\u53ec\u56de\u7387\u6700\u9ad8\uff0887.9%\uff09\uff0c\u5047\u8b66\u62a5\u51cf\u5c11\u4e8697%\uff08\u4ece165\u4e2a\u51cf\u5c11\u52305\u4e2a\uff09\uff0c\u7cbe\u786e\u5ea6\u8fbe\u523092.1%\uff0c\u8bad\u7ec3-\u6d4b\u8bd5\u6027\u80fd\u5dee\u8ddd\u4ec5\u4e3a2.6\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u7684\u56e0\u679cAI\u65b9\u6cd5\uff0c\u5728\u9884\u6d4b\u6027\u7ef4\u62a4\u5e94\u7528\u4e2d\uff0c\u4e0e\u57fa\u4e8e\u76f8\u5173\u6027\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u53ef\u4ee5\u4ea7\u751f\u66f4\u597d\u7684\u8d22\u52a1\u7ed3\u679c\u548c\u66f4\u6613\u4e8e\u89e3\u91ca\u7684\u9884\u6d4b\u3002"}}
{"id": "2512.00991", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00991", "abs": "https://arxiv.org/abs/2512.00991", "authors": ["Nicole Favero", "Francesca Salute", "Daniel Hardt"], "title": "Advancing Academic Chatbots: Evaluation of Non Traditional Outputs", "comment": null, "summary": "Most evaluations of large language models focus on standard tasks such as factual question answering or short summarization. This research expands that scope in two directions: first, by comparing two retrieval strategies, Graph RAG, structured knowledge-graph based, and Advanced RAG, hybrid keyword-semantic search, for QA; and second, by evaluating whether LLMs can generate high quality non-traditional academic outputs, specifically slide decks and podcast scripts. We implemented a prototype combining Meta's LLaMA 3 70B open weight and OpenAI's GPT 4o mini API based. QA performance was evaluated using both human ratings across eleven quality dimensions and large language model judges for scalable cross validation. GPT 4o mini with Advanced RAG produced the most accurate responses. Graph RAG offered limited improvements and led to more hallucinations, partly due to its structural complexity and manual setup. Slide and podcast generation was tested with document grounded retrieval. GPT 4o mini again performed best, though LLaMA 3 showed promise in narrative coherence. Human reviewers were crucial for detecting layout and stylistic flaws, highlighting the need for combined human LLM evaluation in assessing emerging academic outputs.", "AI": {"tldr": "\u672c\u7814\u7a76\u6269\u5c55\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u8303\u56f4\uff0c\u901a\u8fc7\u6bd4\u8f83\u4e24\u79cd\u68c0\u7d22\u7b56\u7565\uff08Graph RAG \u548c Advanced RAG\uff09\u5728\u95ee\u7b54\u65b9\u9762\u7684\u5e94\u7528\uff0c\u5e76\u8bc4\u4f30 LLM \u751f\u6210\u9ad8\u8d28\u91cf\u975e\u4f20\u7edf\u5b66\u672f\u8f93\u51fa\uff08\u5e7b\u706f\u7247\u548c\u64ad\u5ba2\u811a\u672c\uff09\u7684\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u4e3b\u8981\u96c6\u4e2d\u5728\u6807\u51c6\u4efb\u52a1\u4e0a\uff0c\u5982\u4e8b\u5b9e\u95ee\u7b54\u6216\u7b80\u77ed\u6458\u8981\u3002\u672c\u7814\u7a76\u65e8\u5728\u6269\u5c55\u8bc4\u4f30\u8303\u56f4\uff0c\u63a2\u7d22 LLM \u5728\u66f4\u590d\u6742\u548c\u975e\u4f20\u7edf\u5b66\u672f\u8f93\u51fa\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u7814\u7a76\u5b9e\u73b0\u4e86\u7ed3\u5408 Meta \u7684 LLaMA 3 70B \u5f00\u653e\u6743\u91cd\u6a21\u578b\u548c OpenAI \u7684 GPT 4o mini API \u7684\u539f\u578b\u7cfb\u7edf\u3002\u901a\u8fc7\u4eba\u5de5\u8bc4\u4f30\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\uff0c\u5bf9\u95ee\u7b54\u6027\u80fd\u8fdb\u884c\u8bc4\u4f30\u3002\u5e7b\u706f\u7247\u548c\u64ad\u5ba2\u751f\u6210\u901a\u8fc7\u6587\u6863\u68c0\u7d22\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "GPT 4o mini \u7ed3\u5408 Advanced RAG \u4ea7\u751f\u4e86\u6700\u51c6\u786e\u7684\u56de\u7b54\u3002Graph RAG \u7684\u6539\u8fdb\u6709\u9650\uff0c\u4e14\u5bfc\u81f4\u66f4\u591a\u7684\u5e7b\u89c9\u3002GPT 4o mini \u5728\u5e7b\u706f\u7247\u548c\u64ad\u5ba2\u751f\u6210\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u800c LLaMA 3 \u5728\u53d9\u4e8b\u8fde\u8d2f\u6027\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\u3002", "conclusion": "\u4eba\u5de5\u5ba1\u6838\u5bf9\u4e8e\u68c0\u6d4b\u5e03\u5c40\u548c\u98ce\u683c\u7f3a\u9677\u81f3\u5173\u91cd\u8981\uff0c\u7a81\u663e\u4e86\u5728\u8bc4\u4f30\u65b0\u5174\u5b66\u672f\u8f93\u51fa\u65f6\u9700\u8981\u7ed3\u5408\u4eba\u5de5 LLM \u8bc4\u4f30\u3002"}}
{"id": "2512.00528", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00528", "abs": "https://arxiv.org/abs/2512.00528", "authors": ["Isara Liyanage", "Uthayasanker Thayasivam"], "title": "Pushing the Boundaries of Interpretability: Incremental Enhancements to the Explainable Boosting Machine", "comment": null, "summary": "The widespread adoption of complex machine learning models in high-stakes domains has brought the \"black-box\" problem to the forefront of responsible AI research. This paper aims at addressing this issue by improving the Explainable Boosting Machine (EBM), a state-of-the-art glassbox model that delivers both high accuracy and complete transparency. The paper outlines three distinct enhancement methodologies: targeted hyperparameter optimization with Bayesian methods, the implementation of a custom multi-objective function for fairness for hyperparameter optimization, and a novel self-supervised pre-training pipeline for cold-start scenarios. All three methodologies are evaluated across standard benchmark datasets, including the Adult Income, Credit Card Fraud Detection, and UCI Heart Disease datasets. The analysis indicates that while the tuning process yielded marginal improvements in the primary ROC AUC metric, it led to a subtle but important shift in the model's decision-making behavior, demonstrating the value of a multi-faceted evaluation beyond a single performance score. This work is positioned as a critical step toward developing machine learning systems that are not only accurate but also robust, equitable, and transparent, meeting the growing demands of regulatory and ethical compliance.", "AI": {"tldr": "\u672c\u6587\u65e8\u5728\u6539\u8fdb\u53ef\u89e3\u91ca\u7684Boosting Machine (EBM)\uff0c\u8fd9\u662f\u4e00\u79cd\u6700\u5148\u8fdb\u7684glassbox\u6a21\u578b\uff0c\u5b83\u65e2\u80fd\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u53c8\u80fd\u63d0\u4f9b\u5b8c\u5168\u900f\u660e\u5ea6\u3002", "motivation": "\u9ad8\u98ce\u9669\u9886\u57df\u4e2d\u590d\u6742\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u5e7f\u6cdb\u91c7\u7528\u5c06\u201c\u9ed1\u76d2\u201d\u95ee\u9898\u5e26\u5230\u4e86\u8d1f\u8d23\u4efb\u7684AI\u7814\u7a76\u7684\u524d\u6cbf\u3002", "method": "\u672c\u6587\u6982\u8ff0\u4e86\u4e09\u79cd\u4e0d\u540c\u7684\u589e\u5f3a\u65b9\u6cd5\uff1a\u4f7f\u7528\u8d1d\u53f6\u65af\u65b9\u6cd5\u8fdb\u884c\u6709\u9488\u5bf9\u6027\u7684\u8d85\u53c2\u6570\u4f18\u5316\uff0c\u4e3a\u8d85\u53c2\u6570\u4f18\u5316\u5b9e\u65bd\u81ea\u5b9a\u4e49\u591a\u76ee\u6807\u516c\u5e73\u6027\u51fd\u6570\uff0c\u4ee5\u53ca\u7528\u4e8e\u51b7\u542f\u52a8\u573a\u666f\u7684\u65b0\u578b\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7ba1\u9053\u3002", "result": "\u5206\u6790\u8868\u660e\uff0c\u867d\u7136\u8c03\u6574\u8fc7\u7a0b\u5728\u4e3b\u8981\u7684ROC AUC\u6307\u6807\u4e2d\u4ea7\u751f\u4e86\u5fae\u5c0f\u7684\u6539\u8fdb\uff0c\u4f46\u5b83\u5bfc\u81f4\u4e86\u6a21\u578b\u51b3\u7b56\u884c\u4e3a\u4e2d\u5fae\u5999\u4f46\u91cd\u8981\u7684\u8f6c\u53d8\uff0c\u8bc1\u660e\u4e86\u591a\u65b9\u9762\u8bc4\u4f30\u7684\u4ef7\u503c\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u88ab\u5b9a\u4f4d\u4e3a\u5f00\u53d1\u4e0d\u4ec5\u51c6\u786e\u800c\u4e14\u7a33\u5065\u3001\u516c\u5e73\u548c\u900f\u660e\u7684\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u7684\u5173\u952e\u4e00\u6b65\uff0c\u6ee1\u8db3\u4e86\u76d1\u7ba1\u548c\u9053\u5fb7\u5408\u89c4\u6027\u65e5\u76ca\u589e\u957f\u7684\u9700\u6c42\u3002"}}
{"id": "2512.00345", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00345", "abs": "https://arxiv.org/abs/2512.00345", "authors": ["Junqiao Fan", "Haocong Rao", "Jiarui Zhang", "Jianfei Yang", "Lihua Xie"], "title": "mmPred: Radar-based Human Motion Prediction in the Dark", "comment": "This paper is accepted by AAAI-2026", "summary": "Existing Human Motion Prediction (HMP) methods based on RGB-D cameras are sensitive to lighting conditions and raise privacy concerns, limiting their real-world applications such as firefighting and healthcare. Motivated by the robustness and privacy-preserving nature of millimeter-wave (mmWave) radar, this work introduces radar as a novel sensing modality for HMP, for the first time. Nevertheless, radar signals often suffer from specular reflections and multipath effects, resulting in noisy and temporally inconsistent measurements, such as body-part miss-detection. To address these radar-specific artifacts, we propose mmPred, the first diffusion-based framework tailored for radar-based HMP. mmPred introduces a dual-domain historical motion representation to guide the generation process, combining a Time-domain Pose Refinement (TPR) branch for learning fine-grained details and a Frequency-domain Dominant Motion (FDM) branch for capturing global motion trends and suppressing frame-level inconsistency. Furthermore, we design a Global Skeleton-relational Transformer (GST) as the diffusion backbone to model global inter-joint cooperation, enabling corrupted joints to dynamically aggregate information from others. Extensive experiments show that mmPred achieves state-of-the-art performance, outperforming existing methods by 8.6% on mmBody and 22% on mm-Fi.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u6beb\u7c73\u6ce2\u96f7\u8fbe\u7684\u4eba\u4f53\u8fd0\u52a8\u9884\u6d4b\u6846\u67b6 (mmPred)\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5bf9\u5149\u7167\u654f\u611f\u548c\u9690\u79c1\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eRGB-D\u76f8\u673a\u7684\u4eba\u4f53\u8fd0\u52a8\u9884\u6d4b\u65b9\u6cd5\u5bf9\u5149\u7167\u6761\u4ef6\u654f\u611f\u5e76\u5b58\u5728\u9690\u79c1\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5e94\u7528\u3002\u6beb\u7c73\u6ce2\u96f7\u8fbe\u5177\u6709\u9c81\u68d2\u6027\u548c\u4fdd\u62a4\u9690\u79c1\u7684\u7279\u6027\uff0c\u56e0\u6b64\u88ab\u5f15\u5165\u4f5c\u4e3a\u4eba\u4f53\u8fd0\u52a8\u9884\u6d4b\u7684\u4e00\u79cd\u65b0\u7684\u4f20\u611f\u65b9\u5f0f\u3002", "method": "\u63d0\u51fa\u4e86mmPred\uff0c\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u6846\u67b6\uff0c\u4e13\u4e3a\u57fa\u4e8e\u96f7\u8fbe\u7684\u4eba\u4f53\u8fd0\u52a8\u9884\u6d4b\u800c\u8bbe\u8ba1\u3002mmPred\u5f15\u5165\u4e86\u53cc\u57df\u5386\u53f2\u8fd0\u52a8\u8868\u793a\u6765\u6307\u5bfc\u751f\u6210\u8fc7\u7a0b\uff0c\u7ed3\u5408\u65f6\u57df\u59ff\u52bf\u7ec6\u5316 (TPR) \u5206\u652f\u548c\u9891\u57df\u4e3b\u5bfc\u8fd0\u52a8 (FDM) \u5206\u652f\u3002\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5168\u5c40\u9aa8\u9abc\u5173\u7cfbTransformer (GST) \u4f5c\u4e3a\u6269\u6563\u4e3b\u5e72\uff0c\u4ee5\u5efa\u6a21\u5168\u5c40\u5173\u8282\u95f4\u534f\u4f5c\u3002", "result": "mmPred \u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728 mmBody \u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5 8.6%\uff0c\u5728 mm-Fi \u4e0a\u4f18\u4e8e 22%\u3002", "conclusion": "\u6beb\u7c73\u6ce2\u96f7\u8fbe\u53ef\u4ee5\u6709\u6548\u5e94\u7528\u5728\u4eba\u4f53\u8fd0\u52a8\u9884\u6d4b\u4e2d\uff0c\u63d0\u51fa\u7684 mmPred \u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u3002"}}
{"id": "2512.01189", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01189", "abs": "https://arxiv.org/abs/2512.01189", "authors": ["Chunzheng Zhu", "Jialin Shao", "Jianxin Lin", "Yijun Wang", "Jing Wang", "Jinhui Tang", "Kenli Li"], "title": "fMRI2GES: Co-speech Gesture Reconstruction from fMRI Signal with Dual Brain Decoding Alignment", "comment": "IEEE Transactions on Circuits and Systems for Video Technology (TCSVT) 2025", "summary": "Understanding how the brain responds to external stimuli and decoding this process has been a significant challenge in neuroscience. While previous studies typically concentrated on brain-to-image and brain-to-language reconstruction, our work strives to reconstruct gestures associated with speech stimuli perceived by brain. Unfortunately, the lack of paired \\{brain, speech, gesture\\} data hinders the deployment of deep learning models for this purpose. In this paper, we introduce a novel approach, \\textbf{fMRI2GES}, that allows training of fMRI-to-gesture reconstruction networks on unpaired data using \\textbf{Dual Brain Decoding Alignment}. This method relies on two key components: (i) observed texts that elicit brain responses, and (ii) textual descriptions associated with the gestures. Then, instead of training models in a completely supervised manner to find a mapping relationship among the three modalities, we harness an fMRI-to-text model, a text-to-gesture model with paired data and an fMRI-to-gesture model with unpaired data, establishing dual fMRI-to-gesture reconstruction patterns. Afterward, we explicitly align two outputs and train our model in a self-supervision way. We show that our proposed method can reconstruct expressive gestures directly from fMRI recordings. We also investigate fMRI signals from different ROIs in the cortex and how they affect generation results. Overall, we provide new insights into decoding co-speech gestures, thereby advancing our understanding of neuroscience and cognitive science.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5 fMRI2GES\uff0c\u5b83\u5141\u8bb8\u4f7f\u7528\u53cc\u8111\u89e3\u7801\u5bf9\u9f50\u5728\u672a\u914d\u5bf9\u7684\u6570\u636e\u4e0a\u8bad\u7ec3 fMRI \u5230\u624b\u52bf\u91cd\u5efa\u7f51\u7edc\u3002", "motivation": "\u7406\u89e3\u5927\u8111\u5982\u4f55\u54cd\u5e94\u5916\u90e8\u523a\u6fc0\u5e76\u89e3\u7801\u8fd9\u4e00\u8fc7\u7a0b\u662f\u795e\u7ecf\u79d1\u5b66\u4e2d\u7684\u4e00\u9879\u91cd\u5927\u6311\u6218\u3002\u4ee5\u524d\u7684\u7814\u7a76\u901a\u5e38\u96c6\u4e2d\u4e8e\u8111-\u56fe\u50cf\u548c\u8111-\u8bed\u8a00\u91cd\u5efa\uff0c\u4f46\u7f3a\u4e4f\u914d\u5bf9\u7684 {\u8111\u3001\u8bed\u97f3\u3001\u624b\u52bf} \u6570\u636e\u963b\u788d\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u90e8\u7f72\u3002", "method": "\u5229\u7528 fMRI \u5230\u6587\u672c\u6a21\u578b\u3001\u6587\u672c\u5230\u624b\u52bf\u6a21\u578b\uff08\u914d\u5bf9\u6570\u636e\uff09\u548c fMRI \u5230\u624b\u52bf\u6a21\u578b\uff08\u672a\u914d\u5bf9\u6570\u636e\uff09\uff0c\u5efa\u7acb\u53cc\u91cd fMRI \u5230\u624b\u52bf\u91cd\u5efa\u6a21\u5f0f\u3002\u7136\u540e\uff0c\u6211\u4eec\u663e\u5f0f\u5730\u5bf9\u9f50\u4e24\u4e2a\u8f93\u51fa\uff0c\u5e76\u4ee5\u81ea\u6211\u76d1\u7763\u7684\u65b9\u5f0f\u8bad\u7ec3\u6211\u4eec\u7684\u6a21\u578b\u3002", "result": "\u8868\u660e\u8be5\u65b9\u6cd5\u53ef\u4ee5\u76f4\u63a5\u4ece fMRI \u8bb0\u5f55\u4e2d\u91cd\u5efa\u5bcc\u6709\u8868\u73b0\u529b\u7684\u624b\u52bf\u3002\u7814\u7a76\u4e86\u6765\u81ea\u76ae\u5c42\u4e2d\u4e0d\u540c ROI \u7684 fMRI \u4fe1\u53f7\u53ca\u5176\u5982\u4f55\u5f71\u54cd\u751f\u6210\u7ed3\u679c\u3002", "conclusion": "\u4e3a\u89e3\u7801\u540c\u8bed\u624b\u52bf\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u4ece\u800c\u589e\u8fdb\u4e86\u6211\u4eec\u5bf9\u795e\u7ecf\u79d1\u5b66\u548c\u8ba4\u77e5\u79d1\u5b66\u7684\u7406\u89e3\u3002"}}
{"id": "2512.01037", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01037", "abs": "https://arxiv.org/abs/2512.01037", "authors": ["Riad Ahmed Anonto", "Md Labid Al Nahiyan", "Md Tanvir Hassan", "Ch. Md. Rakin Haider"], "title": "When Safety Blocks Sense: Measuring Semantic Confusion in LLM Refusals", "comment": null, "summary": "Safety-aligned language models often refuse prompts that are actually harmless. Current evaluations mostly report global rates such as false rejection or compliance. These scores treat each prompt alone and miss local inconsistency, where a model accepts one phrasing of an intent but rejects a close paraphrase. This gap limits diagnosis and tuning. We introduce \"semantic confusion,\" a failure mode that captures such local inconsistency, and a framework to measure it. We build ParaGuard, a 10k-prompt corpus of controlled paraphrase clusters that hold intent fixed while varying surface form. We then propose three model-agnostic metrics at the token level: Confusion Index, Confusion Rate, and Confusion Depth. These metrics compare each refusal to its nearest accepted neighbors and use token embeddings, next-token probabilities, and perplexity signals. Experiments across diverse model families and deployment guards show that global false-rejection rate hides critical structure. Our metrics reveal globally unstable boundaries in some settings, localized pockets of inconsistency in others, and cases where stricter refusal does not increase inconsistency. We also show how confusion-aware auditing separates how often a system refuses from how sensibly it refuses. This gives developers a practical signal to reduce false refusals while preserving safety.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u5b89\u5168\u5bf9\u9f50\u7684\u8bed\u8a00\u6a21\u578b\u7ecf\u5e38\u62d2\u7edd\u5b9e\u9645\u65e0\u5bb3\u7684\u63d0\u793a\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u6d4b\u91cf\u8fd9\u79cd\u5c40\u90e8\u4e0d\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u4e3b\u8981\u62a5\u544a\u5168\u5c40\u6307\u6807\uff0c\u5ffd\u7565\u4e86\u5c40\u90e8\u4e0d\u4e00\u81f4\u6027\uff0c\u9650\u5236\u4e86\u8bca\u65ad\u548c\u8c03\u6574\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b10k\u63d0\u793a\u7684\u91ca\u4e49\u8bed\u6599\u5e93ParaGuard\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u4e2a\u6a21\u578b\u65e0\u5173\u7684token\u7ea7\u522b\u6307\u6807\uff1aConfusion Index\u3001Confusion Rate\u548cConfusion Depth\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5168\u5c40\u9519\u8bef\u62d2\u7edd\u7387\u63a9\u76d6\u4e86\u5173\u952e\u7ed3\u6784\uff0c\u63d0\u51fa\u7684\u6307\u6807\u63ed\u793a\u4e86\u5168\u5c40\u4e0d\u7a33\u5b9a\u7684\u8fb9\u754c\u3001\u5c40\u90e8\u4e0d\u4e00\u81f4\u7684\u533a\u57df\uff0c\u4ee5\u53ca\u66f4\u4e25\u683c\u7684\u62d2\u7edd\u5e76\u4e0d\u603b\u662f\u589e\u52a0\u4e0d\u4e00\u81f4\u6027\u7684\u60c5\u51b5\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165confusion-aware auditing\uff0c\u53ef\u4ee5\u533a\u5206\u7cfb\u7edf\u62d2\u7edd\u7684\u9891\u7387\u548c\u62d2\u7edd\u7684\u5408\u7406\u6027\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u4fe1\u53f7\u6765\u51cf\u5c11\u9519\u8bef\u62d2\u7edd\uff0c\u540c\u65f6\u4fdd\u6301\u5b89\u5168\u6027\u3002"}}
{"id": "2512.00536", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.00536", "abs": "https://arxiv.org/abs/2512.00536", "authors": ["Aaryan Gupta", "Rishi Saket", "Aravindan Raghuveer"], "title": "Algorithmic Guarantees for Distilling Supervised and Offline RL Datasets", "comment": "29 pages, 2 figures", "summary": "Given a training dataset, the goal of dataset distillation is to derive a synthetic dataset such that models trained on the latter perform as well as those trained on the training dataset. In this work, we develop and analyze an efficient dataset distillation algorithm for supervised learning, specifically regression in $\\mathbb{R}^d$, based on matching the losses on the training and synthetic datasets with respect to a fixed set of randomly sampled regressors without any model training. Our first key contribution is a novel performance guarantee proving that our algorithm needs only $\\tilde{O}(d^2)$ sampled regressors to derive a synthetic dataset on which the MSE loss of any bounded linear model is nearly the same as its MSE loss on the given training data. In particular, the model optimized on the synthetic data has close to minimum loss on the training data, thus performing nearly as well as the model optimized on the latter. Complementing this, we also prove a matching lower bound of $\u03a9(d^2)$ for the number of sampled regressors showing the tightness of our analysis.\n  Our second contribution is to extend our algorithm to offline RL dataset distillation by matching the Bellman loss, unlike previous works which used a behavioral cloning objective. This is the first such method which leverages both, the rewards and the next state information, available in offline RL datasets, without any policy model optimization. Our algorithm generates a synthetic dataset whose Bellman loss with respect to any linear action-value predictor is close to the latter's Bellman loss on the offline RL training dataset. Therefore, a policy associated with an action-value predictor optimized on the synthetic dataset performs nearly as well as that derived from the one optimized on the training data. We conduct experiments to validate our theoretical guarantees and observe performance gains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u76d1\u7763\u5b66\u4e60\u7684\u6709\u6548\u6570\u636e\u96c6\u7cbe\u998f\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u901a\u8fc7\u5339\u914d\u8bad\u7ec3\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u635f\u5931\u6765\u5b9e\u73b0\uff0c\u800c\u65e0\u9700\u4efb\u4f55\u6a21\u578b\u8bad\u7ec3\u3002", "motivation": "\u6570\u636e\u96c6\u7cbe\u998f\u65e8\u5728\u4ece\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u5bfc\u51fa\u4e00\u4e2a\u5408\u6210\u6570\u636e\u96c6\uff0c\u4f7f\u5f97\u5728\u540e\u8005\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u8868\u73b0\u4e0e\u5728\u8bad\u7ec3\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u4e00\u6837\u597d\u3002", "method": "\u8be5\u7b97\u6cd5\u57fa\u4e8e\u5339\u914d\u8bad\u7ec3\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u635f\u5931\uff0c\u4f7f\u7528\u4e00\u7ec4\u56fa\u5b9a\u7684\u968f\u673a\u62bd\u6837\u7684\u56de\u5f52\u5668\uff0c\u65e0\u9700\u4efb\u4f55\u6a21\u578b\u8bad\u7ec3\u3002\u6b64\u5916\uff0c\u8be5\u7b97\u6cd5\u8fd8\u901a\u8fc7\u5339\u914d\u8d1d\u5c14\u66fc\u635f\u5931\u6269\u5c55\u5230\u79bb\u7ebfRL\u6570\u636e\u96c6\u7cbe\u998f\u3002", "result": "\u8be5\u7b97\u6cd5\u53ea\u9700\u8981$\\\\tilde{O}(d^2)$\u4e2a\u62bd\u6837\u56de\u5f52\u5668\u5c31\u53ef\u4ee5\u5bfc\u51fa\u4e00\u4e2a\u5408\u6210\u6570\u636e\u96c6\uff0c\u5728\u8be5\u6570\u636e\u96c6\u4e0a\uff0c\u4efb\u4f55\u6709\u754c\u7ebf\u6027\u6a21\u578b\u7684MSE\u635f\u5931\u4e0e\u5176\u5728\u7ed9\u5b9a\u8bad\u7ec3\u6570\u636e\u4e0a\u7684MSE\u635f\u5931\u51e0\u4e4e\u76f8\u540c\u3002\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u89c2\u5bdf\u5230\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6570\u636e\u96c6\u7cbe\u998f\u7b97\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u3002"}}
{"id": "2512.00355", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00355", "abs": "https://arxiv.org/abs/2512.00355", "authors": ["Junqiao Fan", "Pengfei Liu", "Haocong Rao"], "title": "SMamDiff: Spatial Mamba for Stochastic Human Motion Prediction", "comment": null, "summary": "With intelligent room-side sensing and service robots widely deployed, human motion prediction (HMP) is essential for safe, proactive assistance. However, many existing HMP methods either produce a single, deterministic forecast that ignores uncertainty or rely on probabilistic models that sacrifice kinematic plausibility. Diffusion models improve the accuracy-diversity trade-off but often depend on multi-stage pipelines that are costly for edge deployment. This work focuses on how to ensure spatial-temporal coherence within a single-stage diffusion model for HMP. We introduce SMamDiff, a Spatial Mamba-based Diffusion model with two novel designs: (i) a residual-DCT motion encoding that subtracts the last observed pose before a temporal DCT, reducing the first DC component ($f=0$) dominance and highlighting informative higher-frequency cues so the model learns how joints move rather than where they are; and (ii) a stickman-drawing spatial-mamba module that processes joints in an ordered, joint-by-joint manner, making later joints condition on earlier ones to induce long-range, cross-joint dependencies. On Human3.6M and HumanEva, these coherence mechanisms deliver state-of-the-art results among single-stage probabilistic HMP methods while using less latency and memory than multi-stage diffusion baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSMamDiff\u7684\u5355\u9636\u6bb5\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u4eba\u4f53\u8fd0\u52a8\u9884\u6d4b(HMP)\uff0c\u8be5\u6a21\u578b\u5728\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709HMP\u65b9\u6cd5\u5ffd\u7565\u4e0d\u786e\u5b9a\u6027\u6216\u727a\u7272\u8fd0\u52a8\u5b66\u5408\u7406\u6027\uff0c\u800c\u6269\u6563\u6a21\u578b\u867d\u7136\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u4f46\u4f9d\u8d56\u4e8e\u591a\u9636\u6bb5\u6d41\u7a0b\uff0c\u6210\u672c\u9ad8\u6602\u3002", "method": "\u5f15\u5165\u4e86\u4e24\u4e2a\u65b0\u8bbe\u8ba1\uff1a(i) \u6b8b\u5dee-DCT\u8fd0\u52a8\u7f16\u7801\uff0c\u51cf\u5c11\u4e86\u7b2c\u4e00\u4e2aDC\u5206\u91cf\u7684\u4e3b\u5bfc\u5730\u4f4d\uff0c\u7a81\u51fa\u4e86\u4fe1\u606f\u91cf\u5927\u7684\u9ad8\u9891\u7ebf\u7d22\uff1b(ii) stickman-drawing spatial-mamba\u6a21\u5757\uff0c\u4ee5\u6709\u5e8f\u7684\u3001\u9010\u5173\u8282\u7684\u65b9\u5f0f\u5904\u7406\u5173\u8282\uff0c\u4f7f\u540e\u9762\u7684\u5173\u8282\u4ee5\u65e9\u671f\u7684\u5173\u8282\u4e3a\u6761\u4ef6\uff0c\u4ece\u800c\u8bf1\u5bfc\u957f\u7a0b\u3001\u8de8\u5173\u8282\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728Human3.6M\u548cHumanEva\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u6a21\u578b\u5728\u5355\u9636\u6bb5\u6982\u7387HMP\u65b9\u6cd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u6bd4\u591a\u9636\u6bb5\u6269\u6563\u57fa\u7ebf\u4f7f\u7528\u66f4\u5c11\u7684\u5ef6\u8fdf\u548c\u5185\u5b58\u3002", "conclusion": "\u63d0\u51fa\u7684SMamDiff\u6a21\u578b\u901a\u8fc7\u7a7a\u95f4-\u65f6\u95f4\u8fde\u8d2f\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u4eba\u4f53\u8fd0\u52a8\u9884\u6d4b\u3002"}}
{"id": "2512.01210", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01210", "abs": "https://arxiv.org/abs/2512.01210", "authors": ["Ruiyu Wang", "Tuan Vinh", "Ran Xu", "Yuyin Zhou", "Jiaying Lu", "Carl Yang", "Francisco Pasquel"], "title": "Knowledge Graph Augmented Large Language Models for Next-Visit Disease Prediction", "comment": null, "summary": "Electronic health records (EHRs) support powerful clinical prediction models, but existing methods typically provide coarse, post hoc explanations that offer limited value for patient-level decision making. We introduce a knowledge graph (KG)-guided chain-of-thought (CoT) framework that generates clinically grounded and temporally consistent reasoning for visit-level disease prediction in MIMIC-III. ICD-9 codes are mapped to PrimeKG, from which disease-relevant nodes and multi-hop reasoning paths are extracted and used as scaffolds for CoT generation; only explanations whose conclusions match observed outcomes are retained. Lightweight LLaMA-3.1-Instruct-8B and Gemma-7B models are then fine-tuned on this supervision corpus. Across ten PrimeKG-mapped diseases and limited training cohorts (400 and 1000 cases), KG-guided models outperform strong classical baselines, achieving AUROC values of 0.66 to 0.70 and macro-AUPR values of 0.40 to 0.47. The models also transfer zero-shot to the CRADLE cohort, improving accuracy from approximately 0.40 to 0.51 up to 0.72 to 0.77. A blinded clinician evaluation shows consistent preference for KG-guided CoT explanations in clarity, relevance, and clinical correctness.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u5f15\u5bfc\u7684\u601d\u7ef4\u94fe\uff08CoT\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u4e34\u5e8a \u043e\u0431\u043e\u0441\u043d\u043e\u0432\u0430\u043d\u043d\u043e\u0439 \u0438 \u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e \u0441\u043e\u0433\u043b\u0430\u0441\u043e\u0432\u0430\u043d\u043d\u043e\u0439 \u043b\u043e\u0433\u0438\u043a\u0438 \u0434\u043b\u044f \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0437\u0430\u0431\u043e\u043b\u0435\u0432\u0430\u043d\u0438\u0439 \u043d\u0430 \u0443\u0440\u043e\u0432\u043d\u0435 \u043f\u043e\u0441\u0435\u0449\u0435\u043d\u0438\u0439 \u0432 MIMIC-III\u3002", "motivation": "\u73b0\u6709\u7684\u4e34\u5e8a\u9884\u6d4b\u6a21\u578b\u63d0\u4f9b\u7684\u89e3\u91ca\u901a\u5e38\u6bd4\u8f83\u7c97\u7565\uff0c\u5bf9\u60a3\u8005\u5c42\u9762\u7684\u51b3\u7b56\u5e2e\u52a9\u6709\u9650\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06ICD-9\u4ee3\u7801\u6620\u5c04\u5230PrimeKG\uff0c\u4ece\u4e2d\u63d0\u53d6\u75be\u75c5\u76f8\u5173\u7684\u8282\u70b9\u548c\u591a\u8df3\u63a8\u7406\u8def\u5f84\uff0c\u5e76\u5c06\u5176\u7528\u4f5cCoT\u751f\u6210\u7684\u652f\u67b6\uff1b\u53ea\u4fdd\u7559\u7ed3\u8bba\u4e0e\u89c2\u5bdf\u7ed3\u679c\u76f8\u7b26\u7684\u89e3\u91ca\u3002\u7136\u540e\uff0c\u5728\u76d1\u7763\u8bed\u6599\u5e93\u4e0a\u5bf9\u8f7b\u91cf\u7ea7\u7684LLaMA-3.1-Instruct-8B\u548cGemma-7B\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728\u5341\u79cdPrimeKG\u6620\u5c04\u7684\u75be\u75c5\u548c\u6709\u9650\u7684\u8bad\u7ec3\u961f\u5217\uff08400\u548c1000\u4e2a\u75c5\u4f8b\uff09\u4e2d\uff0cKG\u5f15\u5bfc\u7684\u6a21\u578b\u4f18\u4e8e\u5f3a\u5927\u7684\u7ecf\u5178\u57fa\u7ebf\uff0cAUROC\u503c\u8fbe\u52300.66\u52300.70\uff0c\u5b8fAUPR\u503c\u8fbe\u52300.40\u52300.47\u3002\u8fd9\u4e9b\u6a21\u578b\u8fd8\u53ef\u4ee5\u96f6\u6837\u672c\u8f6c\u79fb\u5230CRADLE\u961f\u5217\uff0c\u5c06\u51c6\u786e\u7387\u4ece\u5927\u7ea60.40\u52300.51\u63d0\u9ad8\u52300.72\u52300.77\u3002", "conclusion": "\u4e34\u5e8a\u533b\u751f\u8bc4\u4f30\u8868\u660e\uff0cKG\u5f15\u5bfc\u7684CoT\u89e3\u91ca\u5728\u6e05\u6670\u5ea6\u3001\u76f8\u5173\u6027\u548c\u4e34\u5e8a\u6b63\u786e\u6027\u65b9\u9762\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u504f\u597d\u3002"}}
{"id": "2512.01077", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.01077", "abs": "https://arxiv.org/abs/2512.01077", "authors": ["Neha Joshi", "Pamir Gogoi", "Aasim Mirza", "Aayush Jansari", "Aditya Yadavalli", "Ayushi Pandey", "Arunima Shukla", "Deepthi Sudharsan", "Kalika Bali", "Vivek Seshadri"], "title": "ELR-1000: A Community-Generated Dataset for Endangered Indic Indigenous Languages", "comment": "Accepted at AACL 2025 (Main)", "summary": "We present a culturally-grounded multimodal dataset of 1,060 traditional recipes crowdsourced from rural communities across remote regions of Eastern India, spanning 10 endangered languages. These recipes, rich in linguistic and cultural nuance, were collected using a mobile interface designed for contributors with low digital literacy. Endangered Language Recipes (ELR)-1000 -- captures not only culinary practices but also the socio-cultural context embedded in indigenous food traditions. We evaluate the performance of several state-of-the-art large language models (LLMs) on translating these recipes into English and find the following: despite the models' capabilities, they struggle with low-resource, culturally-specific language. However, we observe that providing targeted context -- including background information about the languages, translation examples, and guidelines for cultural preservation -- leads to significant improvements in translation quality. Our results underscore the need for benchmarks that cater to underrepresented languages and domains to advance equitable and culturally-aware language technologies. As part of this work, we release the ELR-1000 dataset to the NLP community, hoping it motivates the development of language technologies for endangered languages.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5305\u542b10\u79cd\u6fd2\u5371\u8bed\u8a00\u76841060\u4e2a\u4f20\u7edf\u98df\u8c31\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u8fd9\u4e9b\u98df\u8c31\u6765\u81ea\u5370\u5ea6\u4e1c\u90e8\u519c\u6751\u793e\u533a\uff0c\u4f7f\u7528\u4e3a\u6570\u5b57\u7d20\u517b\u8f83\u4f4e\u7684\u8d21\u732e\u8005\u8bbe\u8ba1\u7684\u79fb\u52a8\u754c\u9762\u6536\u96c6\u3002", "motivation": "\u65e8\u5728\u4fdd\u62a4\u548c\u4f20\u627f\u6fd2\u5371\u8bed\u8a00\u7684\u6587\u5316\u80cc\u666f\u4e0b\u7684\u672c\u571f\u98df\u7269\u4f20\u7edf\uff0c\u5e76\u4fc3\u8fdb\u516c\u5e73\u548c\u5177\u6709\u6587\u5316\u610f\u8bc6\u7684\u8bed\u8a00\u6280\u672f\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u4f17\u5305\u6536\u96c6\u98df\u8c31\uff0c\u5e76\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5c06\u8fd9\u4e9b\u98df\u8c31\u7ffb\u8bd1\u6210\u82f1\u8bed\u65b9\u9762\u7684\u6027\u80fd\u3002\u901a\u8fc7\u63d0\u4f9b\u76ee\u6807\u4e0a\u4e0b\u6587\u6765\u6539\u8fdb\u7ffb\u8bd1\u8d28\u91cf\u3002", "result": "\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u4f4e\u8d44\u6e90\u3001\u5177\u6709\u6587\u5316\u7279\u5f02\u6027\u7684\u8bed\u8a00\u65f6\u8868\u73b0\u4e0d\u4f73\uff1b\u63d0\u4f9b\u6709\u9488\u5bf9\u6027\u7684\u4e0a\u4e0b\u6587\uff08\u5305\u62ec\u5173\u4e8e\u8bed\u8a00\u7684\u80cc\u666f\u4fe1\u606f\u3001\u7ffb\u8bd1\u793a\u4f8b\u548c\u6587\u5316\u4fdd\u62a4\u6307\u5357\uff09\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u7ffb\u8bd1\u8d28\u91cf\u3002", "conclusion": "\u5f3a\u8c03\u9700\u8981\u4e13\u95e8\u4e3a\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u8bed\u8a00\u548c\u9886\u57df\u5b9a\u5236\u57fa\u51c6\uff0c\u4ee5\u63a8\u8fdb\u516c\u5e73\u548c\u5177\u6709\u6587\u5316\u610f\u8bc6\u7684\u8bed\u8a00\u6280\u672f\u3002\u53d1\u5e03ELR-1000\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u6fd2\u5371\u8bed\u8a00\u7684\u8bed\u8a00\u6280\u672f\u5f00\u53d1\u3002"}}
{"id": "2512.00545", "categories": ["cs.LG", "cs.SI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.00545", "abs": "https://arxiv.org/abs/2512.00545", "authors": ["Akrati Saxena", "Harshith Kumar Yadav", "Bart Rutten", "Shashi Shekhar Jha"], "title": "DQ4FairIM: Fairness-aware Influence Maximization using Deep Reinforcement Learning", "comment": null, "summary": "The Influence Maximization (IM) problem aims to select a set of seed nodes within a given budget to maximize the spread of influence in a social network. However, real-world social networks have several structural inequalities, such as dominant majority groups and underrepresented minority groups. If these inequalities are not considered while designing IM algorithms, the outcomes might be biased, disproportionately benefiting majority groups while marginalizing minorities. In this work, we address this gap by designing a fairness-aware IM method using Reinforcement Learning (RL) that ensures equitable influence outreach across all communities, regardless of protected attributes. Fairness is incorporated using a maximin fairness objective, which prioritizes improving the outreach of the least-influenced group, pushing the solution toward an equitable influence distribution. We propose a novel fairness-aware deep RL method, called DQ4FairIM, that maximizes the expected number of influenced nodes by learning an RL policy. The learnt policy ensures that minority groups formulate the IM problem as a Markov Decision Process (MDP) and use deep Q-learning, combined with the Structure2Vec network embedding, earning together with Structure2Vec network embedding to solve the MDP. We perform extensive experiments on synthetic benchmarks and real-world networks to compare our method with fairness-agnostic and fairness-aware baselines. The results show that our method achieves a higher level of fairness while maintaining a better fairness-performance trade-off than baselines. Additionally, our approach learns effective seeding policies that generalize across problem instances without retraining, such as varying the network size or the number of seed nodes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u516c\u5e73\u611f\u77e5\u7684\u5f71\u54cd\u529b\u6700\u5927\u5316\uff08IM\uff09\u65b9\u6cd5\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6765\u786e\u4fdd\u6240\u6709\u793e\u533a\u7684\u516c\u5e73\u5f71\u54cd\u529b\u8986\u76d6\uff0c\u65e0\u8bba\u5176\u53d7\u4fdd\u62a4\u5c5e\u6027\u5982\u4f55\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u793e\u4ea4\u7f51\u7edc\u5b58\u5728\u7ed3\u6784\u6027\u4e0d\u5e73\u7b49\uff0c\u5982\u679c\u8bbe\u8ba1IM\u7b97\u6cd5\u65f6\u4e0d\u8003\u8651\u8fd9\u4e9b\u4e0d\u5e73\u7b49\uff0c\u53ef\u80fd\u4f1a\u4f7f\u591a\u6570\u7fa4\u4f53\u4e0d\u6210\u6bd4\u4f8b\u5730\u53d7\u76ca\uff0c\u540c\u65f6\u4f7f\u5c11\u6570\u7fa4\u4f53\u8fb9\u7f18\u5316\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8bbe\u8ba1\u4e86\u4e00\u79cd\u516c\u5e73\u611f\u77e5IM\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86Structure2Vec\u7f51\u7edc\u5d4c\u5165\u7684\u6df1\u5ea6Q\u5b66\u4e60\uff0c\u5c06IM\u95ee\u9898\u6784\u5efa\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u3002", "result": "\u5728\u5408\u6210\u57fa\u51c6\u548c\u771f\u5b9e\u7f51\u7edc\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u6bd4\u57fa\u7ebf\u66f4\u597d\u7684\u516c\u5e73\u6027-\u6027\u80fd\u6743\u8861\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u516c\u5e73\u6027\u6c34\u5e73\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b66\u4e60\u4e86\u6709\u6548\u7684\u79cd\u5b50\u7b56\u7565\uff0c\u8fd9\u4e9b\u7b56\u7565\u53ef\u4ee5\u5728\u4e0d\u540c\u7684\u95ee\u9898\u5b9e\u4f8b\u4e2d\u63a8\u5e7f\uff0c\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u4f8b\u5982\u6539\u53d8\u7f51\u7edc\u5927\u5c0f\u6216\u79cd\u5b50\u8282\u70b9\u6570\u91cf\u3002"}}
{"id": "2512.00363", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00363", "abs": "https://arxiv.org/abs/2512.00363", "authors": ["Jianhong Han", "Yupei Wang", "Yuan Zhang", "Liang Chen"], "title": "MM-DETR: An Efficient Multimodal Detection Transformer with Mamba-Driven Dual-Granularity Fusion and Frequency-Aware Modality Adapters", "comment": "Manuscript submitted to IEEE Transactions on Geoscience and Remote Sensing", "summary": "Multimodal remote sensing object detection aims to achieve more accurate and robust perception under challenging conditions by fusing complementary information from different modalities. However, existing approaches that rely on attention-based or deformable convolution fusion blocks still struggle to balance performance and lightweight design. Beyond fusion complexity, extracting modality features with shared backbones yields suboptimal representations due to insufficient modality-specific modeling, whereas dual-stream architectures nearly double the parameter count, ultimately limiting practical deployment. To this end, we propose MM-DETR, a lightweight and efficient framework for multimodal object detection. Specifically, we propose a Mamba-based dual granularity fusion encoder that reformulates global interaction as channel-wise dynamic gating and leverages a 1D selective scan for efficient cross-modal modeling with linear complexity. Following this design, we further reinterpret multimodal fusion as a modality completion problem. A region-aware 2D selective scanning completion branch is introduced to recover modality-specific cues, supporting fine-grained fusion along a bidirectional pyramid pathway with minimal overhead. To further reduce parameter redundancy while retaining strong feature extraction capability, a lightweight frequency-aware modality adapter is inserted into the shared backbone. This adapter employs a spatial-frequency co-expert structure to capture modality-specific cues, while a pixel-wise router dynamically balances expert contributions for efficient spatial-frequency fusion. Extensive experiments conducted on four multimodal benchmark datasets demonstrate the effectiveness and generalization capability of the proposed method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u9ad8\u6548\u7684\u591a\u6a21\u6001\u76ee\u6807\u68c0\u6d4b\u6846\u67b6MM-DETR\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6216\u53ef\u53d8\u5f62\u5377\u79ef\u878d\u5408\u5757\u7684\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u6027\u80fd\u548c\u8f7b\u91cf\u5316\u8bbe\u8ba1\uff1b\u5171\u4eab\u9aa8\u5e72\u63d0\u53d6\u6a21\u6001\u7279\u5f81\u5bfc\u81f4\u6b21\u4f18\u8868\u793a\uff0c\u800c\u53cc\u6d41\u67b6\u6784\u53c2\u6570\u91cf\u51e0\u4e4e\u7ffb\u500d\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8eMamba\u7684\u53cc\u7c92\u5ea6\u878d\u5408\u7f16\u7801\u5668\uff0c\u5c06\u5168\u5c40\u4ea4\u4e92\u91cd\u6784\u4e3a\u901a\u9053\u52a8\u6001\u95e8\u63a7\uff0c\u5e76\u5229\u7528\u4e00\u7ef4\u9009\u62e9\u6027\u626b\u63cf\u5b9e\u73b0\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u6709\u6548\u8de8\u6a21\u6001\u5efa\u6a21\uff1b\u5f15\u5165\u4e86\u533a\u57df\u611f\u77e5\u7684\u4e8c\u7ef4\u9009\u62e9\u6027\u626b\u63cf\u8865\u5168\u5206\u652f\uff0c\u4ee5\u6062\u590d\u6a21\u6001\u7279\u5b9a\u7ebf\u7d22\uff0c\u652f\u6301\u6cbf\u53cc\u5411\u91d1\u5b57\u5854\u8def\u5f84\u7684\u7ec6\u7c92\u5ea6\u878d\u5408\uff1b\u5728\u5171\u4eab\u9aa8\u5e72\u4e2d\u63d2\u5165\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u9891\u7387\u611f\u77e5\u6a21\u6001\u9002\u914d\u5668\uff0c\u91c7\u7528\u7a7a\u95f4\u9891\u7387\u534f\u540c\u4e13\u5bb6\u7ed3\u6784\u6765\u6355\u83b7\u6a21\u6001\u7279\u5b9a\u7ebf\u7d22\uff0c\u5e76\u901a\u8fc7\u50cf\u7d20\u7ea7\u8def\u7531\u5668\u52a8\u6001\u5e73\u8861\u4e13\u5bb6\u8d21\u732e\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u7a7a\u95f4\u9891\u7387\u878d\u5408\u3002", "result": "\u5728\u56db\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MM-DETR\u662f\u4e00\u79cd\u6709\u6548\u4e14\u5177\u6709\u6cdb\u5316\u80fd\u529b\u7684\u591a\u6a21\u6001\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\u3002"}}
{"id": "2512.01222", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01222", "abs": "https://arxiv.org/abs/2512.01222", "authors": ["Ching Fang", "Samuel Marks"], "title": "Unsupervised decoding of encoded reasoning using language model interpretability", "comment": null, "summary": "As large language models become increasingly capable, there is growing concern that they may develop reasoning processes that are encoded or hidden from human oversight. To investigate whether current interpretability techniques can penetrate such encoded reasoning, we construct a controlled testbed by fine-tuning a reasoning model (DeepSeek-R1-Distill-Llama-70B) to perform chain-of-thought reasoning in ROT-13 encryption while maintaining intelligible English outputs. We evaluate mechanistic interpretability methods--in particular, logit lens analysis--on their ability to decode the model's hidden reasoning process using only internal activations. We show that logit lens can effectively translate encoded reasoning, with accuracy peaking in intermediate-to-late layers. Finally, we develop a fully unsupervised decoding pipeline that combines logit lens with automated paraphrasing, achieving substantial accuracy in reconstructing complete reasoning transcripts from internal model representations. These findings suggest that current mechanistic interpretability techniques may be more robust to simple forms of encoded reasoning than previously understood. Our work provides an initial framework for evaluating interpretability methods against models that reason in non-human-readable formats, contributing to the broader challenge of maintaining oversight over increasingly capable AI systems.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u4e0d\u65ad\u63d0\u9ad8\uff0c\u4eba\u4eec\u8d8a\u6765\u8d8a\u62c5\u5fc3\u5b83\u4eec\u53ef\u80fd\u4f1a\u53d1\u5c55\u51fa\u5bf9\u4eba\u7c7b\u76d1\u7763\u8fdb\u884c\u7f16\u7801\u6216\u9690\u85cf\u7684\u63a8\u7406\u8fc7\u7a0b\u3002\u672c\u7814\u7a76\u901a\u8fc7\u5fae\u8c03\u4e00\u4e2a\u63a8\u7406\u6a21\u578b\uff0c\u4f7f\u5176\u5728 ROT-13 \u52a0\u5bc6\u4e2d\u6267\u884c\u601d\u7ef4\u94fe\u63a8\u7406\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u7406\u89e3\u7684\u82f1\u8bed\u8f93\u51fa\uff0c\u4ece\u800c\u6784\u5efa\u4e86\u4e00\u4e2a\u53d7\u63a7\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u4ee5\u7814\u7a76\u5f53\u524d\u7684\u53ef\u89e3\u91ca\u6027\u6280\u672f\u662f\u5426\u53ef\u4ee5\u7a7f\u900f\u8fd9\u79cd\u7f16\u7801\u63a8\u7406\u3002", "motivation": "\u8c03\u67e5\u5f53\u524d\u7684\u53ef\u89e3\u91ca\u6027\u6280\u672f\u662f\u5426\u53ef\u4ee5\u7a7f\u900f\u7f16\u7801\u63a8\u7406\u3002", "method": "\u4f7f\u7528 mechanistic interpretability \u65b9\u6cd5\uff08\u7279\u522b\u662f logit lens \u5206\u6790\uff09\u6765\u89e3\u7801\u6a21\u578b\u7684\u9690\u85cf\u63a8\u7406\u8fc7\u7a0b\uff0c\u53ea\u4f7f\u7528\u5185\u90e8\u6fc0\u6d3b\u3002", "result": "logit lens \u53ef\u4ee5\u6709\u6548\u5730\u7ffb\u8bd1\u7f16\u7801\u63a8\u7406\uff0c\u51c6\u786e\u7387\u5728\u4e2d\u95f4\u5230\u540e\u9762\u7684\u5c42\u4e2d\u8fbe\u5230\u5cf0\u503c\u3002\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b8c\u5168\u65e0\u76d1\u7763\u7684\u89e3\u7801\u7ba1\u9053\uff0c\u8be5\u7ba1\u9053\u5c06 logit lens \u4e0e\u81ea\u52a8\u91ca\u4e49\u76f8\u7ed3\u5408\uff0c\u5728\u4ece\u5185\u90e8\u6a21\u578b\u8868\u793a\u4e2d\u91cd\u5efa\u5b8c\u6574\u7684\u63a8\u7406\u8bb0\u5f55\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u5f53\u524d mechanistic interpretability \u6280\u672f\u53ef\u80fd\u6bd4\u4ee5\u524d\u7406\u89e3\u7684\u66f4\u80fd\u591f\u62b5\u6297\u7b80\u5355\u5f62\u5f0f\u7684\u7f16\u7801\u63a8\u7406\u3002\u8fd9\u9879\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u521d\u6b65\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u9488\u5bf9\u4ee5\u975e\u4eba\u7c7b\u53ef\u8bfb\u683c\u5f0f\u8fdb\u884c\u63a8\u7406\u7684\u6a21\u578b\u7684interpretability \u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u7ef4\u62a4\u5bf9\u65e5\u76ca\u5f3a\u5927\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u76d1\u7763\u8fd9\u4e00\u66f4\u5e7f\u6cdb\u7684\u6311\u6218\u3002"}}
{"id": "2512.01109", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01109", "abs": "https://arxiv.org/abs/2512.01109", "authors": ["Yaxuan Ren", "Krithika Ramesh", "Yaxing Yao", "Anjalie Field"], "title": "How do we measure privacy in text? A survey of text anonymization metrics", "comment": "13 pages, 1 figure, 1 table. To be published in Findings of the Association for Computational Linguistics (AACL-IJCNLP 2025). Related resources at: https://github.com/ryxGuo/privacy-metrics-survey", "summary": "In this work, we aim to clarify and reconcile metrics for evaluating privacy protection in text through a systematic survey. Although text anonymization is essential for enabling NLP research and model development in domains with sensitive data, evaluating whether anonymization methods sufficiently protect privacy remains an open challenge. In manually reviewing 47 papers that report privacy metrics, we identify and compare six distinct privacy notions, and analyze how the associated metrics capture different aspects of privacy risk. We then assess how well these notions align with legal privacy standards (HIPAA and GDPR), as well as user-centered expectations grounded in HCI studies. Our analysis offers practical guidance on navigating the landscape of privacy evaluation approaches further and highlights gaps in current practices. Ultimately, we aim to facilitate more robust, comparable, and legally aware privacy evaluations in text anonymization.", "AI": {"tldr": "\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7cfb\u7edf\u7684\u8c03\u67e5\uff0c\u7406\u6e05\u548c\u534f\u8c03\u8bc4\u4f30\u6587\u672c\u9690\u79c1\u4fdd\u62a4\u7684\u6307\u6807\u3002\u901a\u8fc7\u624b\u52a8\u56de\u987e47\u7bc7\u62a5\u544a\u9690\u79c1\u6307\u6807\u7684\u8bba\u6587\uff0c\u6211\u4eec\u8bc6\u522b\u5e76\u6bd4\u8f83\u4e86\u516d\u79cd\u4e0d\u540c\u7684\u9690\u79c1\u6982\u5ff5\uff0c\u5e76\u5206\u6790\u4e86\u76f8\u5173\u7684\u6307\u6807\u5982\u4f55\u6355\u6349\u9690\u79c1\u98ce\u9669\u7684\u4e0d\u540c\u65b9\u9762\u3002\u7136\u540e\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u6982\u5ff5\u4e0e\u6cd5\u5f8b\u9690\u79c1\u6807\u51c6\uff08HIPAA\u548cGDPR\uff09\u4ee5\u53ca\u4ee5HCI\u7814\u7a76\u4e3a\u57fa\u7840\u7684\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u671f\u671b\u7684\u543b\u5408\u7a0b\u5ea6\u3002\u6211\u4eec\u7684\u5206\u6790\u4e3a\u8fdb\u4e00\u6b65\u63a2\u7d22\u9690\u79c1\u8bc4\u4f30\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\uff0c\u5e76\u5f3a\u8c03\u4e86\u5f53\u524d\u5b9e\u8df5\u4e2d\u7684\u5dee\u8ddd\u3002\u6700\u7ec8\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u4fc3\u8fdb\u5728\u6587\u672c\u533f\u540d\u5316\u4e2d\u66f4\u5f3a\u5927\u3001\u53ef\u6bd4\u8f83\u548c\u5177\u6709\u6cd5\u5f8b\u610f\u8bc6\u7684\u9690\u79c1\u8bc4\u4f30\u3002", "motivation": "\u6587\u672c\u533f\u540d\u5316\u5bf9\u4e8e\u5728\u5177\u6709\u654f\u611f\u6570\u636e\u7684\u9886\u57df\u4e2d\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7814\u7a76\u548c\u6a21\u578b\u5f00\u53d1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8bc4\u4f30\u533f\u540d\u5316\u65b9\u6cd5\u662f\u5426\u5145\u5206\u4fdd\u62a4\u9690\u79c1\u4ecd\u7136\u662f\u4e00\u4e2a\u516c\u5f00\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u624b\u52a8\u56de\u987e47\u7bc7\u62a5\u544a\u9690\u79c1\u6307\u6807\u7684\u8bba\u6587\uff0c\u8bc6\u522b\u5e76\u6bd4\u8f83\u4e86\u516d\u79cd\u4e0d\u540c\u7684\u9690\u79c1\u6982\u5ff5\uff0c\u5e76\u5206\u6790\u4e86\u76f8\u5173\u7684\u6307\u6807\u5982\u4f55\u6355\u6349\u9690\u79c1\u98ce\u9669\u7684\u4e0d\u540c\u65b9\u9762\u3002\u7136\u540e\uff0c\u8bc4\u4f30\u8fd9\u4e9b\u6982\u5ff5\u4e0e\u6cd5\u5f8b\u9690\u79c1\u6807\u51c6\uff08HIPAA\u548cGDPR\uff09\u4ee5\u53ca\u4ee5HCI\u7814\u7a76\u4e3a\u57fa\u7840\u7684\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u671f\u671b\u7684\u543b\u5408\u7a0b\u5ea6\u3002", "result": "\u8bc6\u522b\u5e76\u6bd4\u8f83\u4e86\u516d\u79cd\u4e0d\u540c\u7684\u9690\u79c1\u6982\u5ff5\uff0c\u5e76\u5206\u6790\u4e86\u76f8\u5173\u7684\u6307\u6807\u5982\u4f55\u6355\u6349\u9690\u79c1\u98ce\u9669\u7684\u4e0d\u540c\u65b9\u9762\u3002\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u6982\u5ff5\u4e0e\u6cd5\u5f8b\u9690\u79c1\u6807\u51c6\uff08HIPAA\u548cGDPR\uff09\u4ee5\u53ca\u4ee5HCI\u7814\u7a76\u4e3a\u57fa\u7840\u7684\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u671f\u671b\u7684\u543b\u5408\u7a0b\u5ea6\u3002\u4e3a\u8fdb\u4e00\u6b65\u63a2\u7d22\u9690\u79c1\u8bc4\u4f30\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\uff0c\u5e76\u5f3a\u8c03\u4e86\u5f53\u524d\u5b9e\u8df5\u4e2d\u7684\u5dee\u8ddd\u3002", "conclusion": "\u65e8\u5728\u4fc3\u8fdb\u5728\u6587\u672c\u533f\u540d\u5316\u4e2d\u66f4\u5f3a\u5927\u3001\u53ef\u6bd4\u8f83\u548c\u5177\u6709\u6cd5\u5f8b\u610f\u8bc6\u7684\u9690\u79c1\u8bc4\u4f30\u3002"}}
{"id": "2512.00546", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00546", "abs": "https://arxiv.org/abs/2512.00546", "authors": ["Joud El-Shawa", "Elham Bagheri", "Sedef Akinli Kocak", "Yalda Mohsenzadeh"], "title": "A Graph Neural Network Approach for Localized and High-Resolution Temperature Forecasting", "comment": "6 pages, 2 figures. Accepted to the NeurIPS 2025 Tackling Climate Change with Machine Learning Workshop", "summary": "Heatwaves are intensifying worldwide and are among the deadliest weather disasters. The burden falls disproportionately on marginalized populations and the Global South, where under-resourced health systems, exposure to urban heat islands, and the lack of adaptive infrastructure amplify risks. Yet current numerical weather prediction models often fail to capture micro-scale extremes, leaving the most vulnerable excluded from timely early warnings. We present a Graph Neural Network framework for localized, high-resolution temperature forecasting. By leveraging spatial learning and efficient computation, our approach generates forecasts at multiple horizons, up to 48 hours. For Southwestern Ontario, Canada, the model captures temperature patterns with a mean MAE of 1.93$^{\\circ}$C across 1-48h forecasts and MAE@48h of 2.93$^{\\circ}$C, evaluated using 24h input windows on the largest region. While demonstrated here in a data-rich context, this work lays the foundation for transfer learning approaches that could enable localized, equitable forecasts in data-limited regions of the Global South.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u5c40\u90e8\u3001\u9ad8\u5206\u8fa8\u7387\u6e29\u5ea6\u9884\u6d4b\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\u3002", "motivation": "\u5168\u7403\u70ed\u6d6a\u6b63\u5728\u52a0\u5267\uff0c\u5bf9\u8fb9\u7f18\u5316\u4eba\u7fa4\u548c\u5168\u7403\u5357\u65b9\u5730\u533a\u9020\u6210\u4e0d\u6210\u6bd4\u4f8b\u7684\u5f71\u54cd\uff0c\u800c\u76ee\u524d\u7684\u6570\u503c\u5929\u6c14\u9884\u62a5\u6a21\u578b\u901a\u5e38\u65e0\u6cd5\u6355\u6349\u5230\u5fae\u89c2\u5c3a\u5ea6\u7684\u6781\u7aef\u60c5\u51b5\uff0c\u5bfc\u81f4\u6700\u8106\u5f31\u7684\u7fa4\u4f53\u65e0\u6cd5\u53ca\u65f6\u83b7\u5f97\u9884\u8b66\u3002", "method": "\u5229\u7528\u7a7a\u95f4\u5b66\u4e60\u548c\u9ad8\u6548\u8ba1\u7b97\uff0c\u751f\u6210\u591a\u4e2a\u65f6\u7a0b\u7684\u9884\u6d4b\uff0c\u6700\u957f\u53ef\u8fbe 48 \u5c0f\u65f6\u3002", "result": "\u5728\u52a0\u62ff\u5927\u897f\u5357\u5b89\u5927\u7565\u7701\uff0c\u8be5\u6a21\u578b\u6355\u6349\u5230\u7684\u6e29\u5ea6\u6a21\u5f0f\u5728 1-48 \u5c0f\u65f6\u9884\u6d4b\u4e2d\u7684\u5e73\u5747 MAE \u4e3a 1.93$^{\\circ}$C\uff0c\u5728\u4f7f\u7528 24 \u5c0f\u65f6\u8f93\u5165\u7a97\u53e3\u7684\u6700\u5927\u533a\u57df\u8bc4\u4f30\u4e2d\uff0cMAE@48 \u5c0f\u65f6\u4e3a 2.93$^{\\circ}$C\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u53ef\u4ee5\u5728\u6570\u636e\u6709\u9650\u7684\u5168\u7403\u5357\u65b9\u5730\u533a\u5b9e\u73b0\u672c\u5730\u5316\u7684\u3001\u516c\u5e73\u7684\u9884\u6d4b\u3002"}}
{"id": "2512.00365", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00365", "abs": "https://arxiv.org/abs/2512.00365", "authors": ["Andrey Gizdov", "Andrea Procopio", "Yichen Li", "Daniel Harari", "Tomer Ullman"], "title": "Towards aligned body representations in vision models", "comment": "Andrea Procopio and Andrey Gizdov have equal contributions", "summary": "Human physical reasoning relies on internal \"body\" representations - coarse, volumetric approximations that capture an object's extent and support intuitive predictions about motion and physics. While psychophysical evidence suggests humans use such coarse representations, their internal structure remains largely unknown. Here we test whether vision models trained for segmentation develop comparable representations. We adapt a psychophysical experiment conducted with 50 human participants to a semantic segmentation task and test a family of seven segmentation networks, varying in size. We find that smaller models naturally form human-like coarse body representations, whereas larger models tend toward overly detailed, fine-grain encodings. Our results demonstrate that coarse representations can emerge under limited computational resources, and that machine representations can provide a scalable path toward understanding the structure of physical reasoning in the brain.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u7528\u4e8e\u5206\u5272\u7684\u89c6\u89c9\u6a21\u578b\u662f\u5426\u53d1\u5c55\u51fa\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u8eab\u4f53\u8868\u5f81\u3002", "motivation": "\u4eba\u7c7b\u7684\u7269\u7406\u63a8\u7406\u4f9d\u8d56\u4e8e\u5185\u90e8\u201c\u8eab\u4f53\u201d\u8868\u5f81\uff0c\u4f46\u5b83\u4eec\u7684\u5185\u90e8\u7ed3\u6784\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u7136\u672a\u77e5\u3002\u672c\u6587\u65e8\u5728\u6d4b\u8bd5\u7528\u4e8e\u5206\u5272\u7684\u89c6\u89c9\u6a21\u578b\u662f\u5426\u53d1\u5c55\u51fa\u53ef\u6bd4\u8f83\u7684\u8868\u5f81\u3002", "method": "\u5c06\u4e00\u9879\u9488\u5bf9 50 \u540d\u4eba\u7c7b\u53c2\u4e0e\u8005\u7684\u5fc3\u7406\u7269\u7406\u5b66\u5b9e\u9a8c\u8c03\u6574\u4e3a\u8bed\u4e49\u5206\u5272\u4efb\u52a1\uff0c\u5e76\u6d4b\u8bd5\u4e86\u4e00\u7cfb\u5217\u4e03\u4e2a\u5927\u5c0f\u4e0d\u540c\u7684\u5206\u5272\u7f51\u7edc\u3002", "result": "\u8f83\u5c0f\u7684\u6a21\u578b\u81ea\u7136\u5f62\u6210\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u7684\u7c97\u7565\u8eab\u4f53\u8868\u5f81\uff0c\u800c\u8f83\u5927\u7684\u6a21\u578b\u5219\u503e\u5411\u4e8e\u8fc7\u4e8e\u8be6\u7ec6\u7684\u7ec6\u7c92\u5ea6\u7f16\u7801\u3002", "conclusion": "\u7c97\u7565\u8868\u5f81\u53ef\u4ee5\u5728\u6709\u9650\u7684\u8ba1\u7b97\u8d44\u6e90\u4e0b\u51fa\u73b0\uff0c\u5e76\u4e14\u673a\u5668\u8868\u5f81\u53ef\u4ee5\u4e3a\u7406\u89e3\u5927\u8111\u4e2d\u7269\u7406\u63a8\u7406\u7684\u7ed3\u6784\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u9014\u5f84\u3002"}}
{"id": "2512.01289", "categories": ["cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.01289", "abs": "https://arxiv.org/abs/2512.01289", "authors": ["Mingqin Yu", "Fethi Rabhi", "Boming Xia", "Zhengyi Yang", "Felix Tan", "Qinghua Lu"], "title": "OntoMetric: An Ontology-Guided Framework for Automated ESG Knowledge Graph Construction", "comment": null, "summary": "Environmental, Social, and Governance (ESG) disclosure frameworks such as SASB, TCFD, and IFRS S2 require organizations to compute and report numerous metrics for compliance, yet these requirements are embedded in long, unstructured PDF documents that are difficult to interpret, standardize, and audit. Manual extraction is unscalable, while unconstrained large language model (LLM) extraction often produces inconsistent entities, hallucinated relationships, missing provenance, and high validation failure rates. We present OntoMetric, an ontology-guided framework that transforms ESG regulatory documents into validated, AI- and web-ready knowledge graphs. OntoMetric operates through a three-stage pipeline: (1) structure-aware segmentation using table-of-contents boundaries, (2) ontology-constrained LLM extraction that embeds the ESGMKG schema into prompts while enriching entities with semantic fields for downstream reasoning, and (3) two-phase validation that combines LLM-based semantic verification with rule-based schema checking across entity, property, and relationship levels (VR001-VR006). The framework preserves both segment-level and page-level provenance for audit traceability. Evaluated on five ESG standards (SASB Commercial Banks, SASB Semiconductors, TCFD, IFRS S2, AASB S2) totaling 228 pages and 60 segments, OntoMetric achieves 65-90% semantic accuracy and 80-90% schema compliance, compared to 3-10% for baseline unconstrained extraction, at approximately 0.01 to 0.02 USD per validated entity. Our results demonstrate that combining symbolic ontology constraints with neural extraction enables reliable, auditable knowledge graphs suitable for regulatory compliance and web integration, supporting downstream applications such as sustainable-finance analytics, transparency portals, and automated compliance tools.", "AI": {"tldr": "OntoMetric is an ontology-guided framework that transforms ESG regulatory documents into validated, AI- and web-ready knowledge graphs.", "motivation": "ESG disclosure frameworks require organizations to report numerous metrics for compliance, but these requirements are embedded in long, unstructured PDF documents that are difficult to interpret, standardize, and audit. Manual extraction is unscalable, while unconstrained large language model (LLM) extraction often produces inconsistent entities, hallucinated relationships, missing provenance, and high validation failure rates.", "method": "OntoMetric operates through a three-stage pipeline: (1) structure-aware segmentation using table-of-contents boundaries, (2) ontology-constrained LLM extraction that embeds the ESGMKG schema into prompts while enriching entities with semantic fields for downstream reasoning, and (3) two-phase validation that combines LLM-based semantic verification with rule-based schema checking across entity, property, and relationship levels.", "result": "Evaluated on five ESG standards totaling 228 pages and 60 segments, OntoMetric achieves 65-90% semantic accuracy and 80-90% schema compliance, compared to 3-10% for baseline unconstrained extraction, at approximately 0.01 to 0.02 USD per validated entity.", "conclusion": "Combining symbolic ontology constraints with neural extraction enables reliable, auditable knowledge graphs suitable for regulatory compliance and web integration, supporting downstream applications such as sustainable-finance analytics, transparency portals, and automated compliance tools."}}
{"id": "2512.01174", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01174", "abs": "https://arxiv.org/abs/2512.01174", "authors": ["Hyunjun Kim", "Sooyoung Ryu"], "title": "DrawingBench: Evaluating Spatial Reasoning and UI Interaction Capabilities of Large Language Models through Mouse-Based Drawing Tasks", "comment": "AAAI 2026 TrustAgent Workshop", "summary": "As agentic AI systems increasingly operate autonomously, establishing trust through verifiable evaluation becomes critical. Yet existing benchmarks lack the transparency and auditability needed to assess whether agents behave reliably. We present DrawingBench, a verification framework for evaluating the trustworthiness of agentic LLMs through spatial reasoning tasks that require generating sequences of low-level GUI actions. Unlike opaque evaluations, DrawingBench provides transparent, rule-based assessment: 8 objective criteria enable reproducible scoring, while action-level inspection allows stakeholders to audit agent behavior. Our framework comprises 250 diverse prompts across 20 categories and 4 difficulty levels, deterministic evaluation metrics, and an external oversight mechanism through multi-turn feedback that enables human control over agent refinement. Evaluating four state-of-the-art LLMs (Claude-4 Sonnet, GPT-4.1, GPT-4.1-mini, Gemini-2.5 Flash) across 1,000 tests, we establish both capabilities and limitations: models achieved 92.8% perfect performance with structured external feedback driving significant improvements (average +3.2%, up to +32.8% for complex scenes), but systematic error patterns emerged in tool state management and long-horizon planning. Notably, specification clarity proved more important than task complexity -- models achieved 100% perfect performance when given explicit, verifiable criteria. These findings demonstrate that transparent evaluation frameworks can establish trust in agentic systems, with external oversight proving more reliable than self-correction for guiding agent behavior. Our open-source framework provides a template for trustworthy agent assessment. Code and data: https://github.com/hyunjun1121/DrawingBench", "AI": {"tldr": "DrawingBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u81ea\u4e3bAI\u7cfb\u7edf\u53ef\u4fe1\u5ea6\u7684\u9a8c\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u548cGUI\u64cd\u4f5c\u5e8f\u5217\u751f\u6210\u6765\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u7684\u53ef\u4fe1\u5ea6\uff0c\u63d0\u4f9b\u900f\u660e\u3001\u53ef\u5ba1\u8ba1\u7684\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u7f3a\u4e4f\u900f\u660e\u6027\u548c\u53ef\u5ba1\u8ba1\u6027\uff0c\u65e0\u6cd5\u8bc4\u4f30\u667a\u80fd\u4f53\u662f\u5426\u53ef\u9760\u3002", "method": "\u4f7f\u7528DrawingBench\u6846\u67b6\uff0c\u5305\u542b250\u4e2a\u63d0\u793a\u3001\u786e\u5b9a\u6027\u8bc4\u4f30\u6307\u6807\u548c\u5916\u90e8\u76d1\u7763\u673a\u5236\uff0c\u901a\u8fc7\u591a\u8f6e\u53cd\u9988\u5b9e\u73b0\u5bf9\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u4eba\u5de5\u63a7\u5236\u3002", "result": "\u56db\u4e2a\u5148\u8fdb\u7684LLM\u6a21\u578b\u57281000\u6b21\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u80fd\u529b\u548c\u5c40\u9650\u6027\uff0c\u5916\u90e8\u53cd\u9988\u9a71\u52a8\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u4f46\u5728\u5de5\u5177\u72b6\u6001\u7ba1\u7406\u548c\u957f\u7a0b\u89c4\u5212\u4e2d\u51fa\u73b0\u7cfb\u7edf\u6027\u9519\u8bef\u3002", "conclusion": "\u900f\u660e\u7684\u8bc4\u4f30\u6846\u67b6\u53ef\u4ee5\u5efa\u7acb\u5bf9\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u4fe1\u4efb\uff0c\u5916\u90e8\u76d1\u7763\u6bd4\u81ea\u6211\u7ea0\u6b63\u66f4\u53ef\u9760\uff0c\u8be5\u6846\u67b6\u4e3a\u53ef\u4fe1\u7684\u667a\u80fd\u4f53\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6a21\u677f\u3002"}}
{"id": "2512.00553", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.00553", "abs": "https://arxiv.org/abs/2512.00553", "authors": ["Bohan Zhang", "Michael Chen", "A. Pavan", "N. V. Vinodchandran", "Lin F. Yang", "Ruosong Wang"], "title": "List Replicable Reinforcement Learning", "comment": null, "summary": "Replicability is a fundamental challenge in reinforcement learning (RL), as RL algorithms are empirically observed to be unstable and sensitive to variations in training conditions. To formally address this issue, we study \\emph{list replicability} in the Probably Approximately Correct (PAC) RL framework, where an algorithm must return a near-optimal policy that lies in a \\emph{small list} of policies across different runs, with high probability. The size of this list defines the \\emph{list complexity}. We introduce both weak and strong forms of list replicability: the weak form ensures that the final learned policy belongs to a small list, while the strong form further requires that the entire sequence of executed policies remains constrained. These objectives are challenging, as existing RL algorithms exhibit exponential list complexity due to their instability. Our main theoretical contribution is a provably efficient tabular RL algorithm that guarantees list replicability by ensuring the list complexity remains polynomial in the number of states, actions, and the horizon length. We further extend our techniques to achieve strong list replicability, bounding the number of possible policy execution traces polynomially with high probability. Our theoretical result is made possible by key innovations including (i) a novel planning strategy that selects actions based on lexicographic order among near-optimal choices within a randomly chosen tolerance threshold, and (ii) a mechanism for testing state reachability in stochastic environments while preserving replicability. Finally, we demonstrate that our theoretical investigation sheds light on resolving the \\emph{instability} issue of RL algorithms used in practice. In particular, we show that empirically, our new planning strategy can be incorporated into practical RL frameworks to enhance their stability.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5f3a\u5316\u5b66\u4e60(RL)\u4e2d\u7684\u53ef\u91cd\u590d\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u5217\u8868\u53ef\u91cd\u590d\u6027\u7684\u6982\u5ff5\uff0c\u8981\u6c42\u7b97\u6cd5\u5728\u4e0d\u540c\u8fd0\u884c\u4e2d\u8fd4\u56de\u4e00\u4e2a\u63a5\u8fd1\u6700\u4f18\u7b56\u7565\u7684\u5c0f\u5217\u8868\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4e0d\u7a33\u5b9a\u4e14\u5bf9\u8bad\u7ec3\u6761\u4ef6\u7684\u53d8\u5316\u654f\u611f\uff0c\u56e0\u6b64\u53ef\u91cd\u590d\u6027\u662f\u4e00\u4e2a\u6839\u672c\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u8bc1\u660e\u6709\u6548\u7684\u8868\u683cRL\u7b97\u6cd5\uff0c\u901a\u8fc7\u786e\u4fdd\u5217\u8868\u590d\u6742\u5ea6\u5728\u72b6\u6001\u3001\u52a8\u4f5c\u548c\u8303\u56f4\u957f\u5ea6\u4e2d\u4fdd\u6301\u591a\u9879\u5f0f\u6765\u4fdd\u8bc1\u5217\u8868\u53ef\u91cd\u590d\u6027\u3002\u8be5\u7b97\u6cd5\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89c4\u5212\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u57fa\u4e8e\u968f\u673a\u9009\u62e9\u7684\u5bb9\u5dee\u9608\u503c\u5185\u63a5\u8fd1\u6700\u4f18\u9009\u62e9\u7684\u8bcd\u5178\u987a\u5e8f\u9009\u62e9\u52a8\u4f5c\uff0c\u4ee5\u53ca\u4e00\u79cd\u5728\u4fdd\u6301\u53ef\u91cd\u590d\u6027\u7684\u540c\u65f6\u6d4b\u8bd5\u968f\u673a\u73af\u5883\u4e2d\u72b6\u6001\u53ef\u8fbe\u6027\u7684\u673a\u5236\u3002", "result": "\u672c\u6587\u63d0\u51fa\u7684\u89c4\u5212\u7b56\u7565\u53ef\u4ee5\u88ab\u7eb3\u5165\u5230\u5b9e\u9645\u7684RL\u6846\u67b6\u4e2d\uff0c\u4ee5\u63d0\u9ad8\u5b83\u4eec\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "\u672c\u6587\u7684\u7406\u8bba\u7814\u7a76\u9610\u660e\u4e86\u89e3\u51b3\u5b9e\u9645\u4e2d\u4f7f\u7528\u7684RL\u7b97\u6cd5\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002"}}
{"id": "2512.00368", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00368", "abs": "https://arxiv.org/abs/2512.00368", "authors": ["Jian Zhu"], "title": "THCRL: Trusted Hierarchical Contrastive Representation Learning for Multi-View Clustering", "comment": null, "summary": "Multi-View Clustering (MVC) has garnered increasing attention in recent years. It is capable of partitioning data samples into distinct groups by learning a consensus representation. However, a significant challenge remains: the problem of untrustworthy fusion. This problem primarily arises from two key factors: 1) Existing methods often ignore the presence of inherent noise within individual views; 2) In traditional MVC methods using Contrastive Learning (CL), similarity computations typically rely on different views of the same instance, while neglecting the structural information from nearest neighbors within the same cluster. Consequently, this leads to the wrong direction for multi-view fusion. To address this problem, we present a novel Trusted Hierarchical Contrastive Representation Learning (THCRL). It consists of two key modules. Specifically, we propose the Deep Symmetry Hierarchical Fusion (DSHF) module, which leverages the UNet architecture integrated with multiple denoising mechanisms to achieve trustworthy fusion of multi-view data. Furthermore, we present the Average K-Nearest Neighbors Contrastive Learning (AKCL) module to align the fused representation with the view-specific representation. Unlike conventional strategies, AKCL enhances representation similarity among samples belonging to the same cluster, rather than merely focusing on the same sample across views, thereby reinforcing the confidence of the fused representation. Extensive experiments demonstrate that THCRL achieves the state-of-the-art performance in deep MVC tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aTrusted Hierarchical Contrastive Representation Learning (THCRL)\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u89c6\u56fe\u805a\u7c7b(MVC)\u4e2d\u4e0d\u53ef\u4fe1\u878d\u5408\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684MVC\u65b9\u6cd5\u5ffd\u7565\u4e86\u89c6\u56fe\u4e2d\u7684\u566a\u58f0\uff0c\u5e76\u4e14\u5bf9\u6bd4\u5b66\u4e60(CL)\u5ffd\u7565\u4e86\u540c\u4e00\u96c6\u7fa4\u5185\u6700\u8fd1\u90bb\u7684\u7ed3\u6784\u4fe1\u606f\uff0c\u5bfc\u81f4\u591a\u89c6\u56fe\u878d\u5408\u65b9\u5411\u9519\u8bef\u3002", "method": "\u8be5\u65b9\u6cd5\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6a21\u5757\uff1aDeep Symmetry Hierarchical Fusion (DSHF) \u6a21\u5757\uff0c\u5229\u7528UNet\u67b6\u6784\u548c\u591a\u4e2a\u53bb\u566a\u673a\u5236\u5b9e\u73b0\u591a\u89c6\u56fe\u6570\u636e\u7684\u53ef\u4fe1\u878d\u5408\uff1bAverage K-Nearest Neighbors Contrastive Learning (AKCL) \u6a21\u5757\uff0c\u5c06\u878d\u5408\u8868\u793a\u4e0e\u7279\u5b9a\u89c6\u56fe\u8868\u793a\u5bf9\u9f50\uff0c\u589e\u5f3a\u540c\u4e00\u96c6\u7fa4\u5185\u6837\u672c\u7684\u8868\u793a\u76f8\u4f3c\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cTHCRL\u5728\u6df1\u5ea6MVC\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "THCRL\u6846\u67b6\u6709\u6548\u5730\u89e3\u51b3\u4e86\u591a\u89c6\u56fe\u805a\u7c7b\u4e2d\u4e0d\u53ef\u4fe1\u878d\u5408\u7684\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\u3002"}}
{"id": "2512.01300", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01300", "abs": "https://arxiv.org/abs/2512.01300", "authors": ["Dacheng Liao", "Mengshi Qi", "Peng Shu", "Zhining Zhang", "Yuxin Lin", "Liang Liu", "Huadong Ma"], "title": "RoboDriveVLM: A Novel Benchmark and Baseline towards Robust Vision-Language Models for Autonomous Driving", "comment": null, "summary": "Current Vision-Language Model (VLM)-based end-to-end autonomous driving systems often leverage large language models to generate driving decisions directly based on their understanding of the current scene. However, such systems introduce multiple risks in real-world driving scenarios. To evaluate whether VLMs are truly viable for autonomous driving, we introduce RoboDriveBench, the first robustness benchmark focused on end-to-end trajectory prediction tasks. This benchmark systematically evaluates two critical categories of real-world challenges for VLM-based end-to-end autonomous driving systems through 11 simulated scenarios encompassing various corruption types, including 6 scenarios of sensor corruption caused by environmental variations, along with 5 cases of prompt corruption resulting from human intervention and data transmission failures. Each corruption type includes 250 unique driving scenarios and 5,689 frames, resulting in 64,559 total trajectory prediction cases per evaluation. To overcome these real-world challenges, we propose a novel VLM-based autonomous driving framework called RoboDriveVLM, which enhances robustness by mapping more multimodal data-e.g., lidar and radar-into a unified latent space. Furthermore, we introduce a new Test-Time Adaptation (TTA) method based on cross-modal knowledge distillation to improve the robustness of VLM-based autonomous driving systems. Through extensive experiments, our work highlights the limitations of current VLM-based end-to-end autonomous driving systems and provides a more reliable solution for real-world deployment. Source code and datasets will be released.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86RoboDriveBench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8e\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u9c81\u68d2\u6027\u7684\u57fa\u51c6\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86RoboDriveVLM\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u548c\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u65b9\u6cd5\u6765\u63d0\u9ad8\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eVLM\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5b58\u5728\u73b0\u5b9e\u9a7e\u9a76\u98ce\u9669\uff0c\u9700\u8981\u8bc4\u4f30\u5176\u53ef\u884c\u6027\u3002", "method": "\u63d0\u51fa\u4e86RoboDriveBench\u57fa\u51c6\uff0c\u5305\u542b11\u79cd\u6a21\u62df\u573a\u666f\uff0c\u6db5\u76d6\u4f20\u611f\u5668\u548cprompt\u7684\u5404\u79cdcorruption\u7c7b\u578b\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86RoboDriveVLM\u6846\u67b6\uff0c\u4f7f\u7528\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u548c\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff0c\u63ed\u793a\u4e86\u5f53\u524dVLM\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u8bba\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u53ef\u9760\u7684\u73b0\u5b9e\u90e8\u7f72\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u53d1\u5e03\u4e86\u6e90\u4ee3\u7801\u548c\u6570\u636e\u96c6\u3002"}}
{"id": "2512.01183", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01183", "abs": "https://arxiv.org/abs/2512.01183", "authors": ["Yongxin Zhou", "Philippe Mulhem", "Didier Schwab"], "title": "TempPerturb-Eval: On the Joint Effects of Internal Temperature and External Perturbations in RAG Robustness", "comment": null, "summary": "The evaluation of Retrieval-Augmented Generation (RAG) systems typically examines retrieval quality and generation parameters like temperature in isolation, overlooking their interaction. This work presents a systematic investigation of how text perturbations (simulating noisy retrieval) interact with temperature settings across multiple LLM runs. We propose a comprehensive RAG Perturbation-Temperature Analysis Framework that subjects retrieved documents to three distinct perturbation types across varying temperature settings. Through extensive experiments on HotpotQA with both open-source and proprietary LLMs, we demonstrate that performance degradation follows distinct patterns: high-temperature settings consistently amplify vulnerability to perturbations, while certain perturbation types exhibit non-linear sensitivity across the temperature range. Our work yields three key contributions: (1) a diagnostic benchmark for assessing RAG robustness, (2) an analytical framework for quantifying perturbation-temperature interactions, and (3) practical guidelines for model selection and parameter tuning under noisy retrieval conditions.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u4e2d\u68c0\u7d22\u8d28\u91cf\u548c\u751f\u6210\u53c2\u6570\uff08\u5982\u6e29\u5ea6\uff09\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "motivation": "\u73b0\u6709\u7684RAG\u7cfb\u7edf\u8bc4\u4f30\u901a\u5e38\u5b64\u7acb\u5730\u68c0\u67e5\u68c0\u7d22\u8d28\u91cf\u548c\u6e29\u5ea6\u7b49\u751f\u6210\u53c2\u6570\uff0c\u5ffd\u7565\u4e86\u5b83\u4eec\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002\u8fd9\u9879\u5de5\u4f5c\u65e8\u5728\u7cfb\u7edf\u5730\u7814\u7a76\u6587\u672c\u6270\u52a8\uff08\u6a21\u62df\u566a\u58f0\u68c0\u7d22\uff09\u5982\u4f55\u4e0e\u4e0d\u540c\u6e29\u5ea6\u8bbe\u7f6e\u76f8\u4e92\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684RAG\u6270\u52a8-\u6e29\u5ea6\u5206\u6790\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5728\u4e0d\u540c\u7684\u6e29\u5ea6\u8bbe\u7f6e\u4e0b\u5bf9\u68c0\u7d22\u5230\u7684\u6587\u6863\u8fdb\u884c\u4e09\u79cd\u4e0d\u540c\u7684\u6270\u52a8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6027\u80fd\u4e0b\u964d\u9075\u5faa\u4e0d\u540c\u7684\u6a21\u5f0f\uff1a\u9ad8\u6e29\u8bbe\u7f6e\u59cb\u7ec8\u4f1a\u653e\u5927\u5bf9\u6270\u52a8\u7684\u8106\u5f31\u6027\uff0c\u800c\u67d0\u4e9b\u6270\u52a8\u7c7b\u578b\u5728\u6574\u4e2a\u6e29\u5ea6\u8303\u56f4\u5185\u8868\u73b0\u51fa\u975e\u7ebf\u6027\u654f\u611f\u6027\u3002", "conclusion": "\u8bba\u6587\u7684\u4e3b\u8981\u8d21\u732e\u5305\u62ec\uff1a(1) \u7528\u4e8e\u8bc4\u4f30 RAG \u9c81\u68d2\u6027\u7684\u8bca\u65ad\u57fa\u51c6\uff1b(2) \u7528\u4e8e\u91cf\u5316\u6270\u52a8-\u6e29\u5ea6\u76f8\u4e92\u4f5c\u7528\u7684\u5206\u6790\u6846\u67b6\uff1b(3) \u5728\u566a\u58f0\u68c0\u7d22\u6761\u4ef6\u4e0b\u8fdb\u884c\u6a21\u578b\u9009\u62e9\u548c\u53c2\u6570\u8c03\u6574\u7684\u5b9e\u7528\u6307\u5357\u3002"}}
{"id": "2512.00564", "categories": ["cs.LG", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2512.00564", "abs": "https://arxiv.org/abs/2512.00564", "authors": ["Naman Choudhary", "Vedant Singh", "Ameet Talwalkar", "Nicholas Matthew Boffi", "Mikhail Khodak", "Tanya Marwah"], "title": "Pre-Generating Multi-Difficulty PDE Data for Few-Shot Neural PDE Solvers", "comment": "10 Pages, 11 Figures", "summary": "A key aspect of learned partial differential equation (PDE) solvers is that the main cost often comes from generating training data with classical solvers rather than learning the model itself. Another is that there are clear axes of difficulty--e.g., more complex geometries and higher Reynolds numbers--along which problems become (1) harder for classical solvers and thus (2) more likely to benefit from neural speedups. Towards addressing this chicken-and-egg challenge, we study difficulty transfer on 2D incompressible Navier-Stokes, systematically varying task complexity along geometry (number and placement of obstacles), physics (Reynolds number), and their combination. Similar to how it is possible to spend compute to pre-train foundation models and improve their performance on downstream tasks, we find that by classically solving (analogously pre-generating) many low and medium difficulty examples and including them in the training set, it is possible to learn high-difficulty physics from far fewer samples. Furthermore, we show that by combining low and high difficulty data, we can spend 8.9x less compute on pre-generating a dataset to achieve the same error as using only high difficulty examples. Our results highlight that how we allocate classical-solver compute across difficulty levels is as important as how much we allocate overall, and suggest substantial gains from principled curation of pre-generated PDE data for neural solvers. Our code is available at https://github.com/Naman-Choudhary-AI-ML/pregenerating-pde", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u901a\u8fc7\u5728\u4f4e\u96be\u5ea6\u6837\u672c\u4e0a\u8bad\u7ec3\u6765\u63d0\u9ad8\u795e\u7ecf\u504f\u5fae\u5206\u65b9\u7a0b\u6c42\u89e3\u5668\u5728\u9ad8\u96be\u5ea6\u95ee\u9898\u4e0a\u7684\u6027\u80fd\uff0c\u4ece\u800c\u51cf\u5c11\u5bf9\u9ad8\u96be\u5ea6\u6570\u636e\u7684\u9700\u6c42\u3002", "motivation": "\u4f20\u7edf\u504f\u5fae\u5206\u65b9\u7a0b\u6c42\u89e3\u5668\u7684\u4e3b\u8981\u6210\u672c\u5728\u4e8e\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u51e0\u4f55\u5f62\u72b6\u548c\u9ad8\u96f7\u8bfa\u6570\u7684\u60c5\u51b5\u4e0b\u3002\u8fd9\u4f7f\u5f97\u795e\u7ecf\u6c42\u89e3\u5668\u96be\u4ee5\u6709\u6548\u8bad\u7ec3\u3002", "method": "\u8be5\u7814\u7a76\u7cfb\u7edf\u5730\u6539\u53d8\u4e86\u4e8c\u7ef4\u4e0d\u53ef\u538b\u7f29Navier-Stokes\u65b9\u7a0b\u7684\u51e0\u4f55\u5f62\u72b6\uff08\u969c\u788d\u7269\u7684\u6570\u91cf\u548c\u4f4d\u7f6e\uff09\u3001\u7269\u7406\u7279\u6027\uff08\u96f7\u8bfa\u6570\uff09\u53ca\u5176\u7ec4\u5408\uff0c\u4ece\u800c\u63a7\u5236\u4efb\u52a1\u7684\u96be\u5ea6\u3002", "result": "\u901a\u8fc7\u5305\u542b\u5927\u91cf\u4f4e\u3001\u4e2d\u7b49\u96be\u5ea6\u7684\u6837\u672c\u8fdb\u884c\u8bad\u7ec3\uff0c\u53ef\u4ee5\u7528\u66f4\u5c11\u7684\u6837\u672c\u5b66\u4e60\u9ad8\u96be\u5ea6\u7684\u7269\u7406\u95ee\u9898\u3002\u7ed3\u5408\u4f4e\u96be\u5ea6\u548c\u9ad8\u96be\u5ea6\u6570\u636e\uff0c\u4e0e\u4ec5\u4f7f\u7528\u9ad8\u96be\u5ea6\u6837\u672c\u76f8\u6bd4\uff0c\u751f\u6210\u6570\u636e\u96c6\u7684\u8ba1\u7b97\u6210\u672c\u53ef\u4ee5\u964d\u4f4e8.9\u500d\uff0c\u540c\u65f6\u8fbe\u5230\u76f8\u540c\u7684\u8bef\u5dee\u3002", "conclusion": "\u5728\u795e\u7ecf\u6c42\u89e3\u5668\u7684\u9884\u751f\u6210\u504f\u5fae\u5206\u65b9\u7a0b\u6570\u636e\u4e2d\uff0c\u5982\u4f55\u5206\u914d\u8de8\u96be\u5ea6\u7ea7\u522b\u7684\u7ecf\u5178\u6c42\u89e3\u5668\u8ba1\u7b97\u4e0e\u603b\u5206\u914d\u91cf\u540c\u6837\u91cd\u8981\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7\u6709\u539f\u5219\u7684\u7ec4\u7ec7\u83b7\u5f97\u663e\u8457\u6536\u76ca\u3002"}}
{"id": "2512.00369", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00369", "abs": "https://arxiv.org/abs/2512.00369", "authors": ["Wenshuo Chen", "Haosen Li", "Shaofeng Liang", "Lei Wang", "Haozhe Jia", "Kaishen Yuan", "Jieming Wu", "Bowen Tian", "Yutao Yue"], "title": "POLARIS: Projection-Orthogonal Least Squares for Robust and Adaptive Inversion in Diffusion Models", "comment": null, "summary": "The Inversion-Denoising Paradigm, which is based on diffusion models, excels in diverse image editing and restoration tasks. We revisit its mechanism and reveal a critical, overlooked factor in reconstruction degradation: the approximate noise error. This error stems from approximating the noise at step t with the prediction at step t-1, resulting in severe error accumulation throughout the inversion process. We introduce Projection-Orthogonal Least Squares for Robust and Adaptive Inversion (POLARIS), which reformulates inversion from an error-compensation problem into an error-origin problem. Rather than optimizing embeddings or latent codes to offset accumulated drift, POLARIS treats the guidance scale \u03c9 as a step-wise variable and derives a mathematically grounded formula to minimize inversion error at each step. Remarkably, POLARIS improves inversion latent quality with just one line of code. With negligible performance overhead, it substantially mitigates noise approximation errors and consistently improves the accuracy of downstream tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u7f16\u8f91\u548c\u6062\u590d\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u51cf\u5c11\u53cd\u6f14\u8fc7\u7a0b\u4e2d\u7684\u566a\u58f0\u8fd1\u4f3c\u8bef\u5dee\u6765\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u7f16\u8f91\u548c\u6062\u590d\u65b9\u6cd5\u5b58\u5728\u91cd\u5efa\u8d28\u91cf\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u4f5c\u8005\u53d1\u73b0\u8fd9\u662f\u7531\u4e8e\u53cd\u6f14\u8fc7\u7a0b\u4e2d\u8fd1\u4f3c\u566a\u58f0\u8bef\u5dee\u7d2f\u79ef\u9020\u6210\u7684\u3002", "method": "\u8bba\u6587\u63d0\u51faProjection-Orthogonal Least Squares for Robust and Adaptive Inversion (POLARIS)\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06\u53cd\u6f14\u8fc7\u7a0b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4e00\u4e2a\u8bef\u5dee\u6eaf\u6e90\u95ee\u9898\uff0c\u901a\u8fc7\u4f18\u5316\u5f15\u5bfc\u5c3a\u5ea6\u6765\u6700\u5c0f\u5316\u6bcf\u4e00\u6b65\u7684\u8bef\u5dee\u3002", "result": "POLARIS\u65b9\u6cd5\u53ea\u9700\u4e00\u884c\u4ee3\u7801\u5373\u53ef\u663e\u8457\u964d\u4f4e\u566a\u58f0\u8fd1\u4f3c\u8bef\u5dee\uff0c\u5e76\u6301\u7eed\u63d0\u9ad8\u4e0b\u6e38\u4efb\u52a1\u7684\u51c6\u786e\u6027\uff0c\u4e14\u6027\u80fd\u5f00\u9500\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "conclusion": "POLARIS\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u6539\u5584\u53cd\u6f14\u6f5c\u5728\u8d28\u91cf\uff0c\u63d0\u9ad8\u56fe\u50cf\u7f16\u8f91\u548c\u6062\u590d\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
