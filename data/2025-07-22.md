<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 46]
- [cs.CV](#cs.CV) [Total: 45]
- [cs.AI](#cs.AI) [Total: 44]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.IR](#cs.IR) [Total: 15]
- [cs.LG](#cs.LG) [Total: 45]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On Offline Knowledge Base](https://arxiv.org/abs/2507.14189)
*Song Mao,Lejun Cheng,Pinlong Cai,Guohang Yan,Ding Wang,Botian Shi*

Main category: cs.CL

TL;DR: DeepWriter, a customizable, multimodal writing assistant, addresses the limitations of LLMs in specialized domains by using a curated knowledge base and a novel pipeline, achieving superior factual accuracy and content quality in financial report generation.


<details>
  <summary>Details</summary>
Motivation: LLMs lack deep domain-specific knowledge and a tendency to hallucinate in specialized domains. Existing solutions, such as RAG, can suffer from inconsistency across multiple retrieval steps, while online search-based methods often degrade quality due to unreliable web content.

Method: DeepWriter leverages a novel pipeline that involves task decomposition, outline generation, multimodal retrieval, and section-by-section composition with reflection. We also propose a hierarchical knowledge representation to enhance retrieval efficiency and accuracy.

Result: DeepWriter generates coherent, factually grounded, and professional-grade documents. Experiments on financial report generation demonstrate that DeepWriter produces high-quality, verifiable articles that surpasses existing baselines in factual accuracy and generated content quality.

Conclusion: DeepWriter produces high-quality, verifiable articles that surpasses existing baselines in factual accuracy and generated content quality.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
various applications. However, their use as writing assistants in specialized
domains like finance, medicine, and law is often hampered by a lack of deep
domain-specific knowledge and a tendency to hallucinate. Existing solutions,
such as Retrieval-Augmented Generation (RAG), can suffer from inconsistency
across multiple retrieval steps, while online search-based methods often
degrade quality due to unreliable web content. To address these challenges, we
introduce DeepWriter, a customizable, multimodal, long-form writing assistant
that operates on a curated, offline knowledge base. DeepWriter leverages a
novel pipeline that involves task decomposition, outline generation, multimodal
retrieval, and section-by-section composition with reflection. By deeply mining
information from a structured corpus and incorporating both textual and visual
elements, DeepWriter generates coherent, factually grounded, and
professional-grade documents. We also propose a hierarchical knowledge
representation to enhance retrieval efficiency and accuracy. Our experiments on
financial report generation demonstrate that DeepWriter produces high-quality,
verifiable articles that surpasses existing baselines in factual accuracy and
generated content quality.

</details>


### [2] [Retention analysis of edited knowledge after fine-tuning](https://arxiv.org/abs/2507.14198)
*Fufang Wen,Shichang Zhang*

Main category: cs.CL

TL;DR: 微调更容易使LLM忘记编辑过的知识，但冻结某些层可以缓解这个问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）存储了大量知识，这些知识通常需要更新，以纠正事实错误、整合新获得的信息或调整模型行为。模型编辑方法已经成为这种更新的有效解决方案，以比持续训练低得多的计算成本提供本地化和精确的知识修改。然而，微调对先前编辑的知识的影响仍然知之甚少。

Method: 系统地研究不同的微调目标如何与各种模型编辑技术相互作用。

Result: 编辑后的知识在微调过程中更容易被遗忘，冻结与编辑内容相关的层可以显著提高知识保留率。

Conclusion: 编辑后的知识比预训练获得的内在知识更容易在微调过程中被遗忘。冻结与编辑内容相关的层可以显著提高知识保留率。

Abstract: Large language models (LLMs) store vast amounts of knowledge, which often
requires updates to correct factual errors, incorporate newly acquired
information, or adapt model behavior. Model editing methods have emerged as
efficient solutions for such updates, offering localized and precise knowledge
modification at significantly lower computational cost than continual training.
In parallel, LLMs are frequently fine-tuned for a wide range of downstream
tasks. However, the effect of fine-tuning on previously edited knowledge
remains poorly understood. In this work, we systematically investigate how
different fine-tuning objectives interact with various model editing
techniques. Our findings show that edited knowledge is substantially more
susceptible to forgetting during fine-tuning than intrinsic knowledge acquired
through pre-training. This analysis highlights a key limitation of current
editing approaches and suggests that evaluating edit robustness under
downstream fine-tuning is critical for their practical deployment. We further
find that freezing layers associated with edited content can significantly
improve knowledge retention, offering insight into how future editing methods
might be made more robust.

</details>


### [3] [Open-Source LLMs Collaboration Beats Closed-Source LLMs: A Scalable Multi-Agent System](https://arxiv.org/abs/2507.14200)
*Shengji Tang,Jianjian Cao,Weihao Lin,Jiale Hong,Bo Zhang,Shuyue Hu,Lei Bai,Tao Chen,Wanli Ouyang,Peng Ye*

Main category: cs.CL

TL;DR: SMACS, a multi-agent collaboration system, integrates fifteen open-source LLMs and outperforms leading closed-source LLMs on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Can we harness multiple open-source LLMs to match or even beat the closed-source LLMs?

Method: Scalable Multi-Agent Collaboration System (SMACS) framework with Retrieval-based Prior Selection (RPS) and Exploration-Exploitation-Driven Posterior Enhancement (EPE).

Result: SMACS outperforms leading closed-source LLMs, e.g., Claude-3.7-Sonnet (+12.73%), GPT-4.1(+5.36%) and GPT-o3-mini(+5.28%) across multiple tasks. It even exceeds the average of best results from both open-source and closed-source LLMs.

Conclusion: SMACS outperforms leading closed-source LLMs and even exceeds the average of best results from both open-source and closed-source LLMs.

Abstract: This paper aims to demonstrate the potential and strengths of open-source
collectives. It leads to a promising question: Can we harness multiple
open-source LLMs to match or even beat the closed-source LLMs? To answer this,
we propose SMACS, a scalable multi-agent collaboration system (MACS) framework
with high performance. Specifically, for continuous integration of new LLMs and
generalization to diverse questions, we first propose a Retrieval-based Prior
Selection (RPS), which assigns a proxy performance score to each LLM to select
the Top-k LLMs at the instance level for any given question. Then, we propose
an Exploration-Exploitation-Driven Posterior Enhancement (EPE), encouraging the
generation of diverse responses through prior dropping and selecting the
high-quality response via a hybrid posterior score. Experiments on eight
mainstream benchmarks validate the effectiveness of our SMACS: by integrating
fifteen open-source LLMs, SMACS outperforms leading closed-source LLMs in 2025,
e.g., Claude-3.7-Sonnet (+12.73%), GPT-4.1(+5.36%) and GPT-o3-mini(+5.28%)
across multiple tasks. Remarkably, it even exceeds the average of best results
of different datasets from both open-source LLMs (+2.86%) and closed-source
LLMs (+2.04%), pushing the upper bound of intelligence. Code will be released
at https://github.com/magent4aci/SMACS.

</details>


### [4] [Let's Measure the Elephant in the Room: Facilitating Personalized Automated Analysis of Privacy Policies at Scale](https://arxiv.org/abs/2507.14214)
*Rui Zhao,Vladyslav Melnychuk,Jun Zhao,Jesse Wright,Nigel Shadbolt*

Main category: cs.CL

TL;DR: PoliAnalyzer is a system that helps users analyze privacy policies by extracting data usage practices and comparing them to user preferences, achieving high accuracy and reducing cognitive burden.


<details>
  <summary>Details</summary>
Motivation: People have numerous online accounts, but they rarely read the Terms of Service or Privacy Policy of those sites despite claiming otherwise.

Method: a neuro-symbolic system that assists users with personalized privacy policy analysis. PoliAnalyzer uses Natural Language Processing (NLP) to extract formal representations of data usage practices from policy texts. In favor of deterministic, logical inference is applied to compare user preferences with the formal privacy policy representation and produce a compliance report. To achieve this, we extend an existing formal Data Terms of Use policy language to model privacy policies as app policies and user preferences as data policies.

Result: PoliAnalyzer demonstrated high accuracy in identifying relevant data usage practices, achieving F1-score of 90-100% across most tasks.  This analysis revealed that, on average, 95.2% of a privacy policy's segments do not conflict with the analyzed user preferences, enabling users to concentrate on understanding the 4.8% (636 / 13205) that violates preferences, significantly reducing cognitive burden. Further, we identified common practices in privacy policies that violate user expectations - such as the sharing of location data with 3rd parties.

Conclusion: PoliAnalyzer can support automated personalized privacy policy analysis at scale using off-the-shelf NLP tools. This sheds light on a pathway to help individuals regain control over their data and encourage societal discussions on platform data practices to promote a fairer power dynamic.

Abstract: In modern times, people have numerous online accounts, but they rarely read
the Terms of Service or Privacy Policy of those sites despite claiming
otherwise. This paper introduces PoliAnalyzer, a neuro-symbolic system that
assists users with personalized privacy policy analysis. PoliAnalyzer uses
Natural Language Processing (NLP) to extract formal representations of data
usage practices from policy texts. In favor of deterministic, logical inference
is applied to compare user preferences with the formal privacy policy
representation and produce a compliance report. To achieve this, we extend an
existing formal Data Terms of Use policy language to model privacy policies as
app policies and user preferences as data policies. In our evaluation using our
enriched PolicyIE dataset curated by legal experts, PoliAnalyzer demonstrated
high accuracy in identifying relevant data usage practices, achieving F1-score
of 90-100% across most tasks. Additionally, we demonstrate how PoliAnalyzer can
model diverse user data-sharing preferences, derived from prior research as 23
user profiles, and perform compliance analysis against the top 100 most-visited
websites. This analysis revealed that, on average, 95.2% of a privacy policy's
segments do not conflict with the analyzed user preferences, enabling users to
concentrate on understanding the 4.8% (636 / 13205) that violates preferences,
significantly reducing cognitive burden. Further, we identified common
practices in privacy policies that violate user expectations - such as the
sharing of location data with 3rd parties. This paper demonstrates that
PoliAnalyzer can support automated personalized privacy policy analysis at
scale using off-the-shelf NLP tools. This sheds light on a pathway to help
individuals regain control over their data and encourage societal discussions
on platform data practices to promote a fairer power dynamic.

</details>


### [5] [Beyond Architectures: Evaluating the Role of Contextual Embeddings in Detecting Bipolar Disorder on Social Media](https://arxiv.org/abs/2507.14231)
*Khalid Hasan,Jamil Saquer*

Main category: cs.CL

TL;DR: This paper explores the use of NLP models to recognize signs of bipolar disorder in social media text, finding that RoBERTa and LSTM models with BERT embeddings perform the best.


<details>
  <summary>Details</summary>
Motivation: Bipolar disorder is a chronic mental illness frequently underdiagnosed due to subtle early symptoms and social stigma.

Method: transformer-based models (BERT, RoBERTa, ALBERT, ELECTRA, DistilBERT) and Long Short Term Memory (LSTM) models based on contextualized (BERT) and static (GloVe, Word2Vec) word embeddings

Result: RoBERTa achieves the highest performance among transformer models with an F1 score of ~98% while LSTM models using BERT embeddings yield nearly identical results. In contrast, LSTMs trained on static embeddings fail to capture meaningful patterns, scoring near-zero F1.

Conclusion: contextualized language models can support early bipolar disorder screening

Abstract: Bipolar disorder is a chronic mental illness frequently underdiagnosed due to
subtle early symptoms and social stigma. This paper explores the advanced
natural language processing (NLP) models for recognizing signs of bipolar
disorder based on user-generated social media text. We conduct a comprehensive
evaluation of transformer-based models (BERT, RoBERTa, ALBERT, ELECTRA,
DistilBERT) and Long Short Term Memory (LSTM) models based on contextualized
(BERT) and static (GloVe, Word2Vec) word embeddings. Experiments were performed
on a large, annotated dataset of Reddit posts after confirming their validity
through sentiment variance and judgmental analysis. Our results demonstrate
that RoBERTa achieves the highest performance among transformer models with an
F1 score of ~98% while LSTM models using BERT embeddings yield nearly identical
results. In contrast, LSTMs trained on static embeddings fail to capture
meaningful patterns, scoring near-zero F1. These findings underscore the
critical role of contextual language modeling in detecting bipolar disorder. In
addition, we report model training times and highlight that DistilBERT offers
an optimal balance between efficiency and accuracy. In general, our study
offers actionable insights for model selection in mental health NLP
applications and validates the potential of contextualized language models to
support early bipolar disorder screening.

</details>


### [6] [Language Models Change Facts Based on the Way You Talk](https://arxiv.org/abs/2507.14238)
*Matthew Kearney,Reuben Binns,Yarin Gal*

Main category: cs.CL

TL;DR: LLMs are biased by identity markers in user queries, leading to inconsistent and potentially harmful responses in high-stakes applications like medicine, law, and job seeking.


<details>
  <summary>Details</summary>
Motivation: Little is known about how LLMs use identity information in their decision-making in real-world applications, while LLMs are increasingly being used in user-facing applications.

Method: Comprehensive analysis of how identity markers present in a user's writing bias LLM responses across five different high-stakes LLM applications in the domains of medicine, law, politics, government benefits, and job salaries.

Result: LLMs are extremely sensitive to markers of identity in user queries and that race, gender, and age consistently influence LLM responses in these applications. LLMs apply different standards of care to individuals of different ethnicities, alter answers to align with political worldviews based on age, and recommend different salaries based on race and gender.

Conclusion: Off-the-shelf LLMs in user-facing applications may cause harmful differences in medical care, foster wage gaps, and create different political factual realities for people of different identities. Thorough assessments of LLM use in user-facing applications are recommended before future deployment.

Abstract: Large language models (LLMs) are increasingly being used in user-facing
applications, from providing medical consultations to job interview advice.
Recent research suggests that these models are becoming increasingly proficient
at inferring identity information about the author of a piece of text from
linguistic patterns as subtle as the choice of a few words. However, little is
known about how LLMs use this information in their decision-making in
real-world applications. We perform the first comprehensive analysis of how
identity markers present in a user's writing bias LLM responses across five
different high-stakes LLM applications in the domains of medicine, law,
politics, government benefits, and job salaries. We find that LLMs are
extremely sensitive to markers of identity in user queries and that race,
gender, and age consistently influence LLM responses in these applications. For
instance, when providing medical advice, we find that models apply different
standards of care to individuals of different ethnicities for the same
symptoms; we find that LLMs are more likely to alter answers to align with a
conservative (liberal) political worldview when asked factual questions by
older (younger) individuals; and that LLMs recommend lower salaries for
non-White job applicants and higher salaries for women compared to men. Taken
together, these biases mean that the use of off-the-shelf LLMs for these
applications may cause harmful differences in medical care, foster wage gaps,
and create different political factual realities for people of different
identities. Beyond providing an analysis, we also provide new tools for
evaluating how subtle encoding of identity in users' language choices impacts
model decisions. Given the serious implications of these findings, we recommend
that similar thorough assessments of LLM use in user-facing applications are
conducted before future deployment.

</details>


### [7] [CCL-XCoT: An Efficient Cross-Lingual Knowledge Transfer Method for Mitigating Hallucination Generation](https://arxiv.org/abs/2507.14239)
*Weihua Zheng,Roy Ka-Wei Lee,Zhengyuan Liu,Kui Wu,AiTi Aw,Bowei Zou*

Main category: cs.CL

TL;DR: This paper proposes CCL-XCoT, a two-stage fine-tuning framework, to reduce hallucinations in multilingual language models, especially in low-resource languages. It uses curriculum-based contrastive learning and cross-lingual Chain-of-Thought prompting.


<details>
  <summary>Details</summary>
Motivation: Multilingual Large Language Models(MLLMs) demonstrate strong generalization across languages, yet they remain prone to hallucinations, especially in low-resource languages, due to training data imbalances. These hallucinations, which include inaccurate or fabricated outputs, are particularly problematic in domain-specific generation tasks

Method: a two-stage fine-tuning framework for mitigating hallucination in MLLMs. Our approach first enhances cross-lingual semantic alignment through curriculum-based contrastive learning combined with next-token prediction during continued pre-training. Building on this foundation, we then introduce a cross-lingual Chain-of-Thought (XCoT) prompting strategy during instruction fine-tuning, which guides the model to reason in a high-resource language before generating answers in the target low-resource language.

Result: CCL-XCoT reduces hallucination rates by up to 62% and substantially improves factual knowledge transfer across language pairs

Conclusion: CCL-XCoT reduces hallucination rates by up to 62% and substantially improves factual knowledge transfer across language pairs, without relying on external retrieval or multi-model ensembles.

Abstract: Multilingual Large Language Models(MLLMs) demonstrate strong generalization
across languages, yet they remain prone to hallucinations, especially in
low-resource languages, due to training data imbalances. These hallucinations,
which include inaccurate or fabricated outputs, are particularly problematic in
domain-specific generation tasks (Chataigner et al., 2024). To address this
challenge, we propose CCL-XCoT(Curriculum-based Contrastive Learning-based
Cross-lingual Chain-of-Thought), a two-stage fine-tuning framework for
mitigating hallucination in MLLMs. Our approach first enhances cross-lingual
semantic alignment through curriculum-based contrastive learning combined with
next-token prediction during continued pre-training. Building on this
foundation, we then introduce a cross-lingual Chain-of-Thought (XCoT) prompting
strategy during instruction fine-tuning, which guides the model to reason in a
high-resource language before generating answers in the target low-resource
language. Experimental results show that CCL-XCoT reduces hallucination rates
by up to 62% and substantially improves factual knowledge transfer across
language pairs, without relying on external retrieval or multi-model ensembles.

</details>


### [8] [Text-to-SQL for Enterprise Data Analytics](https://arxiv.org/abs/2507.14372)
*Albert Chen,Manas Bundele,Gaurav Ahlawat,Patrick Stetz,Zhitao Wang,Qiang Fei,Donghoon Jung,Audrey Chu,Bharadwaj Jayaraman,Ayushi Panth,Yatin Arora,Sourav Jain,Renjith Varma,Alexey Ilin,Iuliia Melnychuk,Chelsea Chueh,Joyan Sil,Xiaofeng Wang*

Main category: cs.CL

TL;DR: LinkedIn built a chatbot for internal data insights using a knowledge graph and Text-to-SQL agent, achieving 53% accuracy.


<details>
  <summary>Details</summary>
Motivation: Building a working enterprise Text-to-SQL solution is challenging despite progress in Text-to-SQL benchmarks.

Method: The approach involves a knowledge graph for up-to-date semantics, a Text-to-SQL agent with retrieval and error correction, and an interactive chatbot with a rich UI.

Result: The chatbot has over 300 weekly users and achieves 53% accuracy on an internal benchmark set.

Conclusion: The paper presents a chatbot that enables self-service data insights at LinkedIn, achieving 53% accuracy on an internal benchmark. Ablation studies identify key components for enterprise Text-to-SQL solutions.

Abstract: The introduction of large language models has brought rapid progress on
Text-to-SQL benchmarks, but it is not yet easy to build a working enterprise
solution. In this paper, we present insights from building an internal chatbot
that enables LinkedIn's product managers, engineers, and operations teams to
self-serve data insights from a large, dynamic data lake. Our approach features
three components. First, we construct a knowledge graph that captures
up-to-date semantics by indexing database metadata, historical query logs,
wikis, and code. We apply clustering to identify relevant tables for each team
or product area. Second, we build a Text-to-SQL agent that retrieves and ranks
context from the knowledge graph, writes a query, and automatically corrects
hallucinations and syntax errors. Third, we build an interactive chatbot that
supports various user intents, from data discovery to query writing to
debugging, and displays responses in rich UI elements to encourage follow-up
chats. Our chatbot has over 300 weekly users. Expert review shows that 53% of
its responses are correct or close to correct on an internal benchmark set.
Through ablation studies, we identify the most important knowledge graph and
modeling components, offering a practical path for developing enterprise
Text-to-SQL solutions.

</details>


### [9] [HuggingGraph: Understanding the Supply Chain of LLM Ecosystem](https://arxiv.org/abs/2507.14240)
*Mohammad Shahedur Rahman,Peng Gao,Yuede Ji*

Main category: cs.CL

TL;DR: This paper studies the relationships between models and datasets in the LLM supply chain by building a graph and analyzing its structure and dynamics.


<details>
  <summary>Details</summary>
Motivation: It is critical to understand the origin and development of LLM components to better detect potential risks, improve model fairness, and ensure compliance.

Method: A method to systematically collect LLM supply chain data to build a directed heterogeneous graph to model the relationships between models and datasets.

Result: A directed heterogeneous graph with 397,376 nodes and 453,469 edges, and several findings about the LLM supply chain.

Conclusion: The LLM supply chain graph is large, sparse, follows a power-law degree distribution, features a densely connected core and a fragmented periphery, datasets play pivotal roles in training, strong interdependence exists between models and datasets, and the graph is dynamic, with daily updates reflecting the ecosystem's ongoing evolution.

Abstract: Large language models (LLMs) leverage deep learning to process and predict
sequences of words from context, enabling them to perform various NLP tasks,
such as translation, summarization, question answering, and content generation.
However, the growing size and complexity of developing, training, and deploying
advanced LLMs require extensive computational resources and large datasets.
This creates a barrier for users. As a result, platforms that host models and
datasets are widely used. For example, Hugging Face, one of the most popular
platforms, hosted 1.8 million models and 450K datasets by June 2025, with no
sign of slowing down. Since many LLMs are built from base models, pre-trained
models, and external datasets, they can inherit vulnerabilities, biases, or
malicious components from earlier models or datasets. Therefore, it is critical
to understand the origin and development of these components to better detect
potential risks, improve model fairness, and ensure compliance. Motivated by
this, our project aims to study the relationships between models and datasets,
which are core components of the LLM supply chain. First, we design a method to
systematically collect LLM supply chain data. Using this data, we build a
directed heterogeneous graph to model the relationships between models and
datasets, resulting in a structure with 397,376 nodes and 453,469 edges. We
then perform various analyses and uncover several findings, such as: (i) the
LLM supply chain graph is large, sparse, and follows a power-law degree
distribution; (ii) it features a densely connected core and a fragmented
periphery; (iii) datasets play pivotal roles in training; (iv) strong
interdependence exists between models and datasets; and (v) the graph is
dynamic, with daily updates reflecting the ecosystem's ongoing evolution.

</details>


### [10] [Promptomatix: An Automatic Prompt Optimization Framework for Large Language Models](https://arxiv.org/abs/2507.14241)
*Rithesh Murthy,Ming Zhu,Liangwei Yang,Jielin Qiu,Juntao Tan,Shelby Heinecke,Huan Wang,Caiming Xiong,Silvio Savarese*

Main category: cs.CL

TL;DR: Promptomatix is an automatic prompt optimization framework that transforms natural language task descriptions into high-quality prompts without manual tuning, achieving better performance with reduced cost.


<details>
  <summary>Details</summary>
Motivation: prompt engineering remains manual, inconsistent, and inaccessible to non-experts

Method: an automatic prompt optimization framework that transforms natural language task descriptions into high-quality prompts

Result: achieves competitive or superior performance compared to existing libraries, while reducing prompt length and computational overhead making prompt optimization scalable and efficient

Conclusion: Promptomatix achieves competitive or superior performance compared to existing libraries, while reducing prompt length and computational overhead.

Abstract: Large Language Models (LLMs) perform best with well-crafted prompts, yet
prompt engineering remains manual, inconsistent, and inaccessible to
non-experts. We introduce Promptomatix, an automatic prompt optimization
framework that transforms natural language task descriptions into high-quality
prompts without requiring manual tuning or domain expertise. Promptomatix
supports both a lightweight meta-prompt-based optimizer and a DSPy-powered
compiler, with modular design enabling future extension to more advanced
frameworks. The system analyzes user intent, generates synthetic training data,
selects prompting strategies, and refines prompts using cost-aware objectives.
Evaluated across 5 task categories, Promptomatix achieves competitive or
superior performance compared to existing libraries, while reducing prompt
length and computational overhead making prompt optimization scalable and
efficient.

</details>


### [11] [In-Depth and In-Breadth: Pre-training Multimodal Language Models Customized for Comprehensive Chart Understanding](https://arxiv.org/abs/2507.14298)
*Wan-Cyuan Fan,Yen-Chun Chen,Mengchen Liu,Alexander Jacobson,Lu Yuan,Leonid Sigal*

Main category: cs.CL

TL;DR: ChartScope是一种LVLM，针对各种图表类型的深入图表理解进行了优化。


<details>
  <summary>Details</summary>
Motivation: 用于定制特定领域任务的大型视觉语言模型（LVLM）的最新方法在科学图表理解方面显示出可喜的结果。然而，现有方法面临两个主要限制：首先，它们依赖于仅来自少数图表类型的配对数据，限制了对各种图表类型的泛化。其次，它们缺乏针对图表数据对齐的针对性预训练，这阻碍了模型对底层数据的理解。

Method: 提出了一个高效的数据生成管道，该管道合成了各种图表类型的配对数据，以及一种新颖的双路径训练策略，该策略使模型能够简洁地捕获基本数据细节，同时通过结合对底层数据的推理来保持强大的推理能力。

Result: 建立了一个新的基准ChartDQA，用于评估不同级别的问答以及底层数据理解。实验结果表明，ChartScope显著提高了对各种图表类型的理解。

Conclusion: ChartScope显著提高了对各种图表类型的理解。

Abstract: Recent methods for customizing Large Vision Language Models (LVLMs) for
domain-specific tasks have shown promising results in scientific chart
comprehension. However, existing approaches face two major limitations: First,
they rely on paired data from only a few chart types, limiting generalization
to wide range of chart types. Secondly, they lack targeted pre-training for
chart-data alignment, which hampers the model's understanding of underlying
data. In this paper, we introduce ChartScope, an LVLM optimized for in-depth
chart comprehension across diverse chart types. We propose an efficient data
generation pipeline that synthesizes paired data for a wide range of chart
types, along with a novel Dual-Path training strategy that enabling the model
to succinctly capture essential data details while preserving robust reasoning
capabilities by incorporating reasoning over the underlying data. Lastly, we
establish ChartDQA, a new benchmark for evaluating not only question-answering
at different levels but also underlying data understanding. Experimental
results demonstrate that ChartScope significantly enhances comprehension on a
wide range of chart types. The code and data are available at
https://davidhalladay.github.io/chartscope_demo.

</details>


### [12] [Aligning Large Language Models to Low-Resource Languages through LLM-Based Selective Translation: A Systematic Study](https://arxiv.org/abs/2507.14304)
*Rakesh Paul,Anusha Kamath,Kanishk Singla,Raviraj Joshi,Utkarsh Vaidya,Sanjay Singh Chauhan,Niranjan Wartikar*

Main category: cs.CL

TL;DR: 本文探讨了基于LLM的选择性翻译技术，该技术选择性地仅翻译文本的可翻译部分，同时保留不可翻译的内容和句子结构，从而提高LLM的多语言对齐能力。


<details>
  <summary>Details</summary>
Motivation: 多语言大型语言模型 (LLM) 通常在英语和非英语语言之间表现出性能差距，尤其是在低资源环境中。由于高质量数据的限制，将这些模型与低资源语言对齐至关重要，但具有挑战性。虽然英语对齐数据集很容易获得，但在其他语言中整理等效数据既昂贵又耗时。

Method: LLM-based selective translation

Result: 选择性翻译优于普通翻译，过滤噪声输出非常重要，并且在对齐期间将翻译后的样本与原始英语数据混合是有益的。

Conclusion: 选择性翻译是一种用于改进LLM中多语言对齐的实用且有效的方法。

Abstract: Multilingual large language models (LLMs) often demonstrate a performance gap
between English and non-English languages, particularly in low-resource
settings. Aligning these models to low-resource languages is essential yet
challenging due to limited high-quality data. While English alignment datasets
are readily available, curating equivalent data in other languages is expensive
and time-consuming. A common workaround is to translate existing English
alignment data; however, standard translation techniques often fail to preserve
critical elements such as code, mathematical expressions, and structured
formats like JSON. In this work, we investigate LLM-based selective
translation, a technique that selectively translates only the translatable
parts of a text while preserving non-translatable content and sentence
structure. We conduct a systematic study to explore key questions around this
approach, including its effectiveness compared to vanilla translation, the
importance of filtering noisy outputs, and the benefits of mixing translated
samples with original English data during alignment. Our experiments focus on
the low-resource Indic language Hindi and compare translations generated by
Google Cloud Translation (GCP) and Llama-3.1-405B. The results highlight the
promise of selective translation as a practical and effective method for
improving multilingual alignment in LLMs.

</details>


### [13] [How LLMs Comprehend Temporal Meaning in Narratives: A Case Study in Cognitive Evaluation of LLMs](https://arxiv.org/abs/2507.14307)
*Karin de Langis,Jong Inn Park,Andreas Schramm,Bin Hu,Khanh Chi Le,Michael Mensink,Ahn Thu Tong,Dongyeop Kang*

Main category: cs.CL

TL;DR: LLMs don't understand narratives like humans do, over-relying on patterns instead of true comprehension.


<details>
  <summary>Details</summary>
Motivation: The extent to which these behaviors reflect human-like cognition versus advanced pattern recognition remains an open question.

Method: Using an Expert-in-the-Loop probing pipeline, we conduct a series of targeted experiments to assess whether LLMs construct semantic representations and pragmatic inferences in a human-like manner.

Result: LLMs over-rely on prototypicality, produce inconsistent aspectual judgments, and struggle with causal reasoning derived from aspect.

Conclusion: LLMs process aspect fundamentally differently from humans and lack robust narrative understanding.

Abstract: Large language models (LLMs) exhibit increasingly sophisticated linguistic
capabilities, yet the extent to which these behaviors reflect human-like
cognition versus advanced pattern recognition remains an open question. In this
study, we investigate how LLMs process the temporal meaning of linguistic
aspect in narratives that were previously used in human studies. Using an
Expert-in-the-Loop probing pipeline, we conduct a series of targeted
experiments to assess whether LLMs construct semantic representations and
pragmatic inferences in a human-like manner. Our findings show that LLMs
over-rely on prototypicality, produce inconsistent aspectual judgments, and
struggle with causal reasoning derived from aspect, raising concerns about
their ability to fully comprehend narratives. These results suggest that LLMs
process aspect fundamentally differently from humans and lack robust narrative
understanding. Beyond these empirical findings, we develop a standardized
experimental framework for the reliable assessment of LLMs' cognitive and
linguistic capabilities.

</details>


### [14] [GRACE: Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization](https://arxiv.org/abs/2507.14758)
*Luyi Ma,Wanjia Zhang,Kai Zhao,Abhishek Kulkarni,Lalitesh Morishetti,Anjana Ganesh,Ashish Ranjan,Aashika Padmanabhan,Jianpeng Xu,Jason Cho,Praveen Kanumala,Kaushiki Nag,Sumit Dutta,Kamiya Motwani,Malay Patel,Evren Korpeoglu,Sushant Kumar,Kannan Achan*

Main category: cs.CL

TL;DR: GRACE是一种新的生成框架，用于多行为序列推荐，它通过旅程感知的稀疏注意力和链式思考tokenization来实现。


<details>
  <summary>Details</summary>
Motivation: 生成模型最近在多行为推荐系统中表现出强大的潜力，利用transformer和tokenization的表达能力来生成个性化的项目序列。然而，它们的采用受到以下因素的阻碍：（1）缺乏用于token推理的显式信息，（2）由于二次注意复杂性和tokenization后的密集序列表示而导致的高计算成本，以及（3）对用户历史的有限多尺度建模。

Method: GRACE引入了一种混合的Chain-of-Thought (CoT) tokenization方法，该方法使用来自产品知识图谱的显式属性（例如，类别、品牌、价格）在语义tokenization上编码用户-项目交互，从而实现可解释的、行为对齐的生成。为了解决标准注意力的低效问题，我们设计了一种Journey-Aware Sparse Attention (JSA)机制，该机制选择性地注意tokenized序列中压缩的、内部的、相互的以及当前的上下文片段。

Result: GRACE在两个真实世界数据集上显著优于最先进的基线，在Home领域实现了高达+106.9% HR@10和+106.7% NDCG@10的改进，在Electronics领域实现了+22.1% HR@10的改进。GRACE还减少了高达48%的注意计算。

Conclusion: GRACE在两个真实世界数据集上显著优于最先进的基线，在Home领域实现了高达+106.9% HR@10和+106.7% NDCG@10的改进，在Electronics领域实现了+22.1% HR@10的改进。GRACE还减少了高达48%的注意计算。

Abstract: Generative models have recently demonstrated strong potential in
multi-behavior recommendation systems, leveraging the expressive power of
transformers and tokenization to generate personalized item sequences. However,
their adoption is hindered by (1) the lack of explicit information for token
reasoning, (2) high computational costs due to quadratic attention complexity
and dense sequence representations after tokenization, and (3) limited
multi-scale modeling over user history. In this work, we propose GRACE
(Generative Recommendation via journey-aware sparse Attention on
Chain-of-thought tokEnization), a novel generative framework for multi-behavior
sequential recommendation. GRACE introduces a hybrid Chain-of-Thought (CoT)
tokenization method that encodes user-item interactions with explicit
attributes from product knowledge graphs (e.g., category, brand, price) over
semantic tokenization, enabling interpretable and behavior-aligned generation.
To address the inefficiency of standard attention, we design a Journey-Aware
Sparse Attention (JSA) mechanism, which selectively attends to compressed,
intra-, inter-, and current-context segments in the tokenized sequence.
Experiments on two real-world datasets show that GRACE significantly
outperforms state-of-the-art baselines, achieving up to +106.9% HR@10 and
+106.7% NDCG@10 improvement over the state-of-the-art baseline on the Home
domain, and +22.1% HR@10 on the Electronics domain. GRACE also reduces
attention computation by up to 48% with long sequences.

</details>


### [15] [What Makes You CLIC: Detection of Croatian Clickbait Headlines](https://arxiv.org/abs/2507.14314)
*Marija Anđedelić,Dominik Šipek,Laura Majer,Jan Šnajder*

Main category: cs.CL

TL;DR: This paper introduces a new dataset (CLIC) for Croatian clickbait detection and finds that fine-tuned models perform better than general LLMs.


<details>
  <summary>Details</summary>
Motivation: Automatic detection of clickbait headlines is essential for preserving information quality and reader trust in digital media.

Method: Fine-tuned BERTi'c model and compared its performance to LLM-based ICL methods with prompts both in Croatian and English.

Result: Nearly half of the analyzed headlines contain clickbait.

Conclusion: Finetuned models outperform general LLMs in clickbait detection for Croatian news headlines.

Abstract: Online news outlets operate predominantly on an advertising-based revenue
model, compelling journalists to create headlines that are often scandalous,
intriguing, and provocative -- commonly referred to as clickbait. Automatic
detection of clickbait headlines is essential for preserving information
quality and reader trust in digital media and requires both contextual
understanding and world knowledge. For this task, particularly in
less-resourced languages, it remains unclear whether fine-tuned methods or
in-context learning (ICL) yield better results. In this paper, we compile CLIC,
a novel dataset for clickbait detection of Croatian news headlines spanning a
20-year period and encompassing mainstream and fringe outlets. We fine-tune the
BERTi\'c model on this task and compare its performance to LLM-based ICL
methods with prompts both in Croatian and English. Finally, we analyze the
linguistic properties of clickbait. We find that nearly half of the analyzed
headlines contain clickbait, and that finetuned models deliver better results
than general LLMs.

</details>


### [16] [Can LLMs Infer Personality from Real World Conversations?](https://arxiv.org/abs/2507.14355)
*Jianfeng Zhu,Ruoming Jin,Karin G. Coifman*

Main category: cs.CL

TL;DR: 本研究评估了LLM在性格推断中的应用，发现其信度高但效度有限，表明当前LLM在心理学应用中存在局限性。


<details>
  <summary>Details</summary>
Motivation: 利用开放式语言，大型语言模型（LLM），如OpenAI的GPT-4和Meta的LLaMA，为可扩展的性格评估提供了一种有前景的方法。然而，推断性格特征仍然具有挑战性，而且早期工作通常依赖于缺乏心理测量有效性的合成数据或社交媒体文本。

Method: 使用BFI-10自我报告评分，通过零样本提示进行BFI-10项目预测，并通过零样本和思维链提示进行Big Five特质推断，测试了三种最先进的LLM（GPT-4.1 Mini、Meta-LLaMA和DeepSeek）。

Result: 所有模型都表现出很高的重测信度，但结构效度有限：与真实分数的相关性较弱（最大Pearson's r = 0.27），评分者间一致性较低（Cohen's κ < 0.10），并且预测偏向于中等或高特质水平。思维链提示和更长的输入上下文适度地改善了分布对齐，但没有改善特质水平的准确性。

Conclusion: 当前基于LLM的性格推断存在局限性，心理学应用需要基于证据的开发。

Abstract: Large Language Models (LLMs) such as OpenAI's GPT-4 and Meta's LLaMA offer a
promising approach for scalable personality assessment from open-ended
language. However, inferring personality traits remains challenging, and
earlier work often relied on synthetic data or social media text lacking
psychometric validity. We introduce a real-world benchmark of 555
semi-structured interviews with BFI-10 self-report scores for evaluating
LLM-based personality inference. Three state-of-the-art LLMs (GPT-4.1 Mini,
Meta-LLaMA, and DeepSeek) were tested using zero-shot prompting for BFI-10 item
prediction and both zero-shot and chain-of-thought prompting for Big Five trait
inference. All models showed high test-retest reliability, but construct
validity was limited: correlations with ground-truth scores were weak (max
Pearson's $r = 0.27$), interrater agreement was low (Cohen's $\kappa < 0.10$),
and predictions were biased toward moderate or high trait levels.
Chain-of-thought prompting and longer input context modestly improved
distributional alignment, but not trait-level accuracy. These results
underscore limitations in current LLM-based personality inference and highlight
the need for evidence-based development for psychological applications.

</details>


### [17] [A Fisher's exact test justification of the TF-IDF term-weighting scheme](https://arxiv.org/abs/2507.15742)
*Paul Sheridan,Zeyad Ahmed,Aitazaz A. Farooque*

Main category: cs.CL

TL;DR: 本文通过展示 TF-IDF 如何从显着性检验的角度来理解，从而为 TF-IDF 在统计社区中的使用提供了理由。


<details>
  <summary>Details</summary>
Motivation: TF-IDF 及其许多变体通常用作各种文本分析应用中的术语加权方案。越来越多的学术研究致力于为 TF-IDF 奠定良好的理论基础。

Method: 通过从显着性检验的角度展示如何理解著名的表达式，证明了 TF-IDF 在统计社区中的使用是合理的。

Result: 常见的 TF-IDF 变体 TF-ICF 在温和的正则性条件下，与 Fisher 统计显着性精确检验的单尾版本的 p 值的负对数密切相关。在某些理想化假设下，TF-IDF 与所述负对数转换 p 值之间建立了联系。作为一种限制情况，证明了在无限大的文档集合的限制下，同一数量收敛到 TF-IDF。

Conclusion: TF-IDF 的 Fisher 精确检验证明为工作统计学家提供了解释术语权重方案长期有效性的现成方法。

Abstract: Term frequency-inverse document frequency, or TF-IDF for short, is arguably
the most celebrated mathematical expression in the history of information
retrieval. Conceived as a simple heuristic quantifying the extent to which a
given term's occurrences are concentrated in any one given document out of
many, TF-IDF and its many variants are routinely used as term-weighting schemes
in diverse text analysis applications. There is a growing body of scholarship
dedicated to placing TF-IDF on a sound theoretical foundation. Building on that
tradition, this paper justifies the use of TF-IDF to the statistics community
by demonstrating how the famed expression can be understood from a significance
testing perspective. We show that the common TF-IDF variant TF-ICF is, under
mild regularity conditions, closely related to the negative logarithm of the
$p$-value from a one-tailed version of Fisher's exact test of statistical
significance. As a corollary, we establish a connection between TF-IDF and the
said negative log-transformed $p$-value under certain idealized assumptions. We
further demonstrate, as a limiting case, that this same quantity converges to
TF-IDF in the limit of an infinitely large document collection. The Fisher's
exact test justification of TF-IDF equips the working statistician with a ready
explanation of the term-weighting scheme's long-established effectiveness.

</details>


### [18] [Error-Aware Curriculum Learning for Biomedical Relation Classification](https://arxiv.org/abs/2507.14374)
*Sinchani Chakraborty,Sudeshna Sarkar,Pawan Goyal*

Main category: cs.CL

TL;DR: This paper introduces an error-aware teacher-student framework using GPT-4o for relation classification in biomedical texts, achieving state-of-the-art results on PPI and DDI datasets.


<details>
  <summary>Details</summary>
Motivation: Relation Classification (RC) in biomedical texts is essential for constructing knowledge graphs and enabling applications such as drug repurposing and clinical decision-making.

Method: An error-aware teacher--student framework is proposed, which uses GPT-4o to analyze prediction failures, classify error types, assign difficulty scores, and generate targeted remediations. Curriculum learning is then applied to train a second student model.

Result: The approach achieves new state-of-the-art performance on 4 of 5 PPI datasets and the DDI dataset, while remaining competitive on ChemProt.

Conclusion: The proposed error-aware teacher--student framework achieves new state-of-the-art performance on 4 of 5 PPI datasets and the DDI dataset, while remaining competitive on ChemProt.

Abstract: Relation Classification (RC) in biomedical texts is essential for
constructing knowledge graphs and enabling applications such as drug
repurposing and clinical decision-making. We propose an error-aware
teacher--student framework that improves RC through structured guidance from a
large language model (GPT-4o). Prediction failures from a baseline student
model are analyzed by the teacher to classify error types, assign difficulty
scores, and generate targeted remediations, including sentence rewrites and
suggestions for KG-based enrichment. These enriched annotations are used to
train a first student model via instruction tuning. This model then annotates a
broader dataset with difficulty scores and remediation-enhanced inputs. A
second student is subsequently trained via curriculum learning on this dataset,
ordered by difficulty, to promote robust and progressive learning. We also
construct a heterogeneous biomedical knowledge graph from PubMed abstracts to
support context-aware RC. Our approach achieves new state-of-the-art
performance on 4 of 5 PPI datasets and the DDI dataset, while remaining
competitive on ChemProt.

</details>


### [19] [X-Intelligence 3.0: Training and Evaluating Reasoning LLM for Semiconductor Display](https://arxiv.org/abs/2507.14430)
*Xiaolin Yan,Yangxing Liu,Jiazhang Zheng,Chi Liu,Mingyu Du,Caisheng Chen,Haoyang Liu,Ming Ding,Yuan Li,Qiuping Liao,Linfeng Li,Zhili Mei,Siyu Wan,Li Li,Ruyi Zhong,Jiangling Yu,Xule Liu,Huihui Hu,Jiameng Yue,Ruohui Cheng,Qi Yang,Liangqing Wu,Ke Zhu,Chi Zhang,Chufei Jing,Yifan Zhou,Yan Liang,Dongdong Li,Zhaohui Wang,Bin Zhao,Mingzhou Wu,Mingzhong Zhou,Peng Du,Zuomin Liao,Chao Dai,Pengfei Liang,Xiaoguang Zhu,Yu Zhang,Yu Gu,Kun Pan,Yuan Wu,Yanqing Guan,Shaojing Wu,Zikang Feng,Xianze Ma,Peishan Cheng,Wenjuan Jiang,Jing Ba,Huihao Yu,Zeping Hu,Yuan Xu,Zhiwei Liu,He Wang,Zhenguo Lin,Ming Liu,Yanhong Meng*

Main category: cs.CL

TL;DR: X-Intelligence 3.0 is the first high-performance reasoning model specifically developed for the semiconductor display industry.


<details>
  <summary>Details</summary>
Motivation: LLMs effectiveness in the semiconductor display industry remains limited due to a lack of domain-specific training and expertise

Method: supervised fine-tuning and reinforcement learning, automated evaluation framework, domain-specific retrieval-augmented generation (RAG) mechanism

Result: notable performance gains on benchmark datasets; outperforms SOTA DeepSeek-R1-671B across multiple evaluations, despite its relatively compact size of 32 billion parameters

Conclusion: X-Intelligence 3.0 outperforms SOTA DeepSeek-R1-671B across multiple evaluations, demonstrating its exceptional efficiency and establishes it as a powerful solution to the longstanding reasoning challenges faced by the semiconductor display industry.

Abstract: Large language models (LLMs) have recently achieved significant advances in
reasoning and demonstrated their advantages in solving challenging problems.
Yet, their effectiveness in the semiconductor display industry remains limited
due to a lack of domain-specific training and expertise. To bridge this gap, we
present X-Intelligence 3.0, the first high-performance reasoning model
specifically developed for the semiconductor display industry. This model is
designed to deliver expert-level understanding and reasoning for the industry's
complex challenges. Leveraging a carefully curated industry knowledge base, the
model undergoes supervised fine-tuning and reinforcement learning to enhance
its reasoning and comprehension capabilities. To further accelerate
development, we implemented an automated evaluation framework that simulates
expert-level assessments. We also integrated a domain-specific
retrieval-augmented generation (RAG) mechanism, resulting in notable
performance gains on benchmark datasets. Despite its relatively compact size of
32 billion parameters, X-Intelligence 3.0 outperforms SOTA DeepSeek-R1-671B
across multiple evaluations. This demonstrates its exceptional efficiency and
establishes it as a powerful solution to the longstanding reasoning challenges
faced by the semiconductor display industry.

</details>


### [20] [XL-DURel: Finetuning Sentence Transformers for Ordinal Word-in-Context Classification](https://arxiv.org/abs/2507.14578)
*Sachin Yadav,Dominik Schlechtweg*

Main category: cs.CL

TL;DR: XL-DURel 是一种用于有序 Word-in-Context 分类的微调多语言句子转换器模型，它在排序目标上优于之前的模型，并将二元 WiC 视为有序 WiC 的一个特例。


<details>
  <summary>Details</summary>
Motivation: 提出 XL-DURel，这是一种微调的多语言句子转换器模型，针对有序上下文词分类进行了优化。

Method: 微调的多语言句子转换器模型，针对有序上下文词分类进行了优化

Result: 在有序和二元数据上，使用基于复空间中角度距离的排序目标，优于之前的模型。

Conclusion: 二元 WiC 可以被视为有序 WiC 的一个特例，并且针对一般有序任务优化模型可以提高在更具体的二元任务中的性能。这为跨不同任务公式的 WiC 建模的统一处理铺平了道路。

Abstract: We propose XL-DURel, a finetuned, multilingual Sentence Transformer model
optimized for ordinal Word-in-Context classification. We test several loss
functions for regression and ranking tasks managing to outperform previous
models on ordinal and binary data with a ranking objective based on angular
distance in complex space. We further show that binary WiC can be treated as a
special case of ordinal WiC and that optimizing models for the general ordinal
task improves performance on the more specific binary task. This paves the way
for a unified treatment of WiC modeling across different task formulations.

</details>


### [21] [Exploring Human-AI Complementarity in CPS Diagnosis Using Unimodal and Multimodal BERT Models](https://arxiv.org/abs/2507.14579)
*Kester Wong,Sahan Bulathwela,Mutlu Cukurova*

Main category: cs.CL

TL;DR: This paper extends previous research on using AudiBERT for CPS diagnosis, showing statistically significant improvements in the social-cognitive dimension compared to BERT, but not in the affective dimension. It also explores the importance of data size, inter-rater agreement, and human-AI complementarity.


<details>
  <summary>Details</summary>
Motivation: Detecting collaborative problem solving (CPS) indicators from dialogue using machine learning techniques is a significant challenge. Previous research showed unclear statistical significance in multimodal improvements and lacked guidance on human-AI complementarity.

Method: The study uses the multimodal BERT variant, AudiBERT, which integrates speech and acoustic-prosodic audio features to enhance CPS diagnosis. It also employs the BERT model for comparison.

Result: AudiBERT improved classification of sparse classes and showed statistically significant class-wise improvements over BERT in the social-cognitive dimension, but not in the affective dimension. Larger training data correlated with higher recall for both models. BERT's precision was linked to high inter-rater agreement. Performance was inconsistent when using BERT to diagnose indicators well-detected by AudiBERT.

Conclusion: The paper concludes by outlining a structured approach towards achieving human-AI complementarity for CPS diagnosis, highlighting the crucial inclusion of model explainability to support human agency and engagement in the reflective coding process.

Abstract: Detecting collaborative problem solving (CPS) indicators from dialogue using
machine learning techniques is a significant challenge for the field of AI in
Education. Recent studies have explored the use of Bidirectional Encoder
Representations from Transformers (BERT) models on transcription data to
reliably detect meaningful CPS indicators. A notable advancement involved the
multimodal BERT variant, AudiBERT, which integrates speech and
acoustic-prosodic audio features to enhance CPS diagnosis. Although initial
results demonstrated multimodal improvements, the statistical significance of
these enhancements remained unclear, and there was insufficient guidance on
leveraging human-AI complementarity for CPS diagnosis tasks. This workshop
paper extends the previous research by highlighting that the AudiBERT model not
only improved the classification of classes that were sparse in the dataset,
but it also had statistically significant class-wise improvements over the BERT
model for classifications in the social-cognitive dimension. However, similar
significant class-wise improvements over the BERT model were not observed for
classifications in the affective dimension. A correlation analysis highlighted
that larger training data was significantly associated with higher recall
performance for both the AudiBERT and BERT models. Additionally, the precision
of the BERT model was significantly associated with high inter-rater agreement
among human coders. When employing the BERT model to diagnose indicators within
these subskills that were well-detected by the AudiBERT model, the performance
across all indicators was inconsistent. We conclude the paper by outlining a
structured approach towards achieving human-AI complementarity for CPS
diagnosis, highlighting the crucial inclusion of model explainability to
support human agency and engagement in the reflective coding process.

</details>


### [22] [Explainable Collaborative Problem Solving Diagnosis with BERT using SHAP and its Implications for Teacher Adoption](https://arxiv.org/abs/2507.14584)
*Kester Wong,Sahan Bulathwela,Mutlu Cukurova*

Main category: cs.CL

TL;DR: This study uses SHAP to examine how tokenized words contribute to a BERT model's classification of CPS processes, finding that good classifications don't always mean reasonable explanations and that spurious words can influence classifications. It suggests exploring ensemble models and human-AI collaboration for CPS diagnosis.


<details>
  <summary>Details</summary>
Motivation: Enhancing the explainability of BERT-based CPS diagnostics is essential to better inform end users such as teachers, thereby fostering greater trust and facilitating wider adoption in education. Limited attention has been given to understanding how individual tokenised words in the dataset contribute to the model's classification decisions.

Method: SHapley Additive exPlanations (SHAP) to examine how different tokenised words in transcription data contributed to a BERT model's classification of CPS processes.

Result: Well-performing classifications did not necessarily equate to a reasonable explanation for the classification decisions. Particular tokenised words were used frequently to affect classifications. The analysis also identified a spurious word, which contributed positively to the classification but was not semantically meaningful to the class.

Conclusion: The extent to which the model appropriately uses the tokens for its classification is associated with the number of classes involved. It calls for an investigation into the exploration of ensemble model architectures and the involvement of human-AI complementarity for CPS diagnosis, since considerable human reasoning is still required for fine-grained discrimination of CPS subskills.

Abstract: The use of Bidirectional Encoder Representations from Transformers (BERT)
model and its variants for classifying collaborative problem solving (CPS) has
been extensively explored within the AI in Education community. However,
limited attention has been given to understanding how individual tokenised
words in the dataset contribute to the model's classification decisions.
Enhancing the explainability of BERT-based CPS diagnostics is essential to
better inform end users such as teachers, thereby fostering greater trust and
facilitating wider adoption in education. This study undertook a preliminary
step towards model transparency and explainability by using SHapley Additive
exPlanations (SHAP) to examine how different tokenised words in transcription
data contributed to a BERT model's classification of CPS processes. The
findings suggested that well-performing classifications did not necessarily
equate to a reasonable explanation for the classification decisions. Particular
tokenised words were used frequently to affect classifications. The analysis
also identified a spurious word, which contributed positively to the
classification but was not semantically meaningful to the class. While such
model transparency is unlikely to be useful to an end user to improve their
practice, it can help them not to overrely on LLM diagnostics and ignore their
human expertise. We conclude the workshop paper by noting that the extent to
which the model appropriately uses the tokens for its classification is
associated with the number of classes involved. It calls for an investigation
into the exploration of ensemble model architectures and the involvement of
human-AI complementarity for CPS diagnosis, since considerable human reasoning
is still required for fine-grained discrimination of CPS subskills.

</details>


### [23] [Backtranslation and paraphrasing in the LLM era? Comparing data augmentation methods for emotion classification](https://arxiv.org/abs/2507.14590)
*Łukasz Radliński,Mateusz Guściora,Jan Kocoń*

Main category: cs.CL

TL;DR: This paper explores data augmentation methods for NLP, particularly through large language models like GPT, and finds that backtranslation and paraphrasing can yield comparable or even better results than zero and a few-shot generation of examples.


<details>
  <summary>Details</summary>
Motivation: Numerous domain-specific machine learning tasks struggle with data scarcity and class imbalance.

Method: We conducted a series of experiments comparing four different approaches to data augmentation in multiple experimental setups.

Result: Evaluated the results both in terms of the quality of generated data and its impact on classification performance.

Conclusion: Backtranslation and paraphrasing can yield comparable or even better results than zero and a few-shot generation of examples.

Abstract: Numerous domain-specific machine learning tasks struggle with data scarcity
and class imbalance. This paper systematically explores data augmentation
methods for NLP, particularly through large language models like GPT. The
purpose of this paper is to examine and evaluate whether traditional methods
such as paraphrasing and backtranslation can leverage a new generation of
models to achieve comparable performance to purely generative methods. Methods
aimed at solving the problem of data scarcity and utilizing ChatGPT were
chosen, as well as an exemplary dataset. We conducted a series of experiments
comparing four different approaches to data augmentation in multiple
experimental setups. We then evaluated the results both in terms of the quality
of generated data and its impact on classification performance. The key
findings indicate that backtranslation and paraphrasing can yield comparable or
even better results than zero and a few-shot generation of examples.

</details>


### [24] [Retrieval-Augmented Clinical Benchmarking for Contextual Model Testing in Kenyan Primary Care: A Methodology Paper](https://arxiv.org/abs/2507.14615)
*Fred Mutisya,Shikoh Gitau,Christine Syovata,Diana Oigara,Ibrahim Matende,Muna Aden,Munira Ali,Ryan Nyotu,Diana Marion,Job Nyangena,Nasubo Ongoma,Keith Mbae,Elizabeth Wamicha,Eric Mibuari,Jean Philbert Nsengemana,Talkmore Chidede*

Main category: cs.CL

TL;DR: 创建了一个基准数据集和评估框架，重点关注肯尼亚2级和3级临床护理，结果表明，当LLM应用于本地化场景时，存在显著的性能差距。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）有望改善资源匮乏地区获得医疗保健的机会，但它们在非洲初级保健中的有效性仍有待探索。

Method: 使用检索增强生成（RAG）将临床问题置于肯尼亚国家指南中，确保与当地标准对齐。这些指南被数字化、分块和索引以进行语义检索。然后，使用Gemini Flash 2.0 Lite，根据指南摘录生成逼真的临床场景、多项选择题和基于原理的英语和斯瓦希里语答案。

Result: Alama Health QA数据集包括数千个与监管机构对齐的问题答案对，涵盖常见的门诊条件。除了准确性之外，我们还引入了评估指标，用于测试临床推理、安全性和适应性，例如罕见病例检测（大海捞针）、逐步逻辑（决策点）和情境适应性。

Conclusion: LLMs在应用于本地化场景时存在显著的性能差距，与LLM在非洲医疗内容上的准确率低于美国基准的研究结果一致。这项工作为指南驱动的动态基准测试提供了一个可复制的模型，以支持非洲卫生系统中安全的人工智能部署。

Abstract: Large Language Models(LLMs) hold promise for improving healthcare access in
low-resource settings, but their effectiveness in African primary care remains
underexplored. We present a methodology for creating a benchmark dataset and
evaluation framework focused on Kenyan Level 2 and 3 clinical care. Our
approach uses retrieval augmented generation (RAG) to ground clinical questions
in Kenya's national guidelines, ensuring alignment with local standards. These
guidelines were digitized, chunked, and indexed for semantic retrieval. Gemini
Flash 2.0 Lite was then prompted with guideline excerpts to generate realistic
clinical scenarios, multiple-choice questions, and rationale based answers in
English and Swahili. Kenyan physicians co-created and refined the dataset, and
a blinded expert review process ensured clinical accuracy, clarity, and
cultural appropriateness. The resulting Alama Health QA dataset includes
thousands of regulator-aligned question answer pairs across common outpatient
conditions. Beyond accuracy, we introduce evaluation metrics that test clinical
reasoning, safety, and adaptability such as rare case detection (Needle in the
Haystack), stepwise logic (Decision Points), and contextual adaptability.
Initial results reveal significant performance gaps when LLMs are applied to
localized scenarios, consistent with findings that LLM accuracy is lower on
African medical content than on US-based benchmarks. This work offers a
replicable model for guideline-driven, dynamic benchmarking to support safe AI
deployment in African health systems.

</details>


### [25] [Linear Relational Decoding of Morphology in Language Models](https://arxiv.org/abs/2507.14640)
*Eric Xia,Jugal Kalita*

Main category: cs.CL

TL;DR: Transformer计算可以使用线性转换来近似，该转换在形态关系上实现了 90% 的忠实度。


<details>
  <summary>Details</summary>
Motivation: 发现两部分仿射近似是某些主客体关系上 Transformer 计算的良好近似。

Method: 线性变换 Ws，其中 s 是主题标记的中间层表示，W 来源于模型导数。

Result: 在线性技术上，形态关系的忠实度达到 90%，并且我们展示了多语言和跨模型的类似发现。

Conclusion: 语言模型中的一些概念关系（如形态学）可以很容易地从潜在空间解释，并且由跨层线性变换稀疏编码。

Abstract: A two-part affine approximation has been found to be a good approximation for
transformer computations over certain subject object relations. Adapting the
Bigger Analogy Test Set, we show that the linear transformation Ws, where s is
a middle layer representation of a subject token and W is derived from model
derivatives, is also able to accurately reproduce final object states for many
relations. This linear technique is able to achieve 90% faithfulness on
morphological relations, and we show similar findings multi-lingually and
across models. Our findings indicate that some conceptual relationships in
language models, such as morphology, are readily interpretable from latent
space, and are sparsely encoded by cross-layer linear transformations.

</details>


### [26] [Cleanse: Uncertainty Estimation Approach Using Clustering-based Semantic Consistency in LLMs](https://arxiv.org/abs/2507.14649)
*Minsuh Joo,Hyunsoo Cho*

Main category: cs.CL

TL;DR: 提出Cleanse方法，通过聚类分析LLM生成文本的语义一致性来检测幻觉。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在各种NLP任务中表现出色，但LLM中的幻觉仍然是一个关键问题，因为它直接关系到构建安全可靠的LLM。

Method: 提出了一种名为Cleanse的不确定性估计方法，该方法通过计算LLM隐藏层嵌入向量聚类内部一致性与总一致性的比例来量化不确定性。

Result: 在LLaMA-7B, LLaMA-13B, LLaMA2-7B和Mistral-7B四个模型以及SQuAD和CoQA两个问答基准测试中验证了Cleanse检测幻觉的有效性。

Conclusion: 通过聚类方法量化LLM隐藏层嵌入向量的语义一致性，以此来评估LLM中的幻觉现象。

Abstract: Despite the outstanding performance of large language models (LLMs) across
various NLP tasks, hallucinations in LLMs--where LLMs generate inaccurate
responses--remains as a critical problem as it can be directly connected to a
crisis of building safe and reliable LLMs. Uncertainty estimation is primarily
used to measure hallucination levels in LLM responses so that correct and
incorrect answers can be distinguished clearly. This study proposes an
effective uncertainty estimation approach, \textbf{Cl}ust\textbf{e}ring-based
sem\textbf{an}tic con\textbf{s}ist\textbf{e}ncy (\textbf{Cleanse}). Cleanse
quantifies the uncertainty with the proportion of the intra-cluster consistency
in the total consistency between LLM hidden embeddings which contain adequate
semantic information of generations, by employing clustering. The effectiveness
of Cleanse for detecting hallucination is validated using four off-the-shelf
models, LLaMA-7B, LLaMA-13B, LLaMA2-7B and Mistral-7B and two
question-answering benchmarks, SQuAD and CoQA.

</details>


### [27] [Mangosteen: An Open Thai Corpus for Language Model Pretraining](https://arxiv.org/abs/2507.14664)
*Wannaphong Phatthiyaphaibun,Can Udomcharoenchaikit,Pakpoom Singkorapoom,Kunat Pipatanakul,Ekapol Chuangsuwanich,Peerat Limkonchotiwat,Sarana Nutanong*

Main category: cs.CL

TL;DR: Mangosteen, a 47B-token Thai corpus, was built using a Thai-adapted Dolma pipeline and improves performance on Thai benchmarks.


<details>
  <summary>Details</summary>
Motivation: Raw web text is noisy, and existing corpora don't capture Thai script or cultural nuances, leaving risky material untreated. Prior Thai-specific efforts lack reproducibility.

Method: A Thai-adapted Dolma pipeline with custom language ID, quality filters, and content filters, plus curated non-web sources.

Result: The pipeline trims CommonCrawl from 202M to 25M documents while raising SEA-HELM NLG from 3 to 11. An 8B-parameter SEA-LION model surpasses SEA-LION-v3 and Llama-3.1 by about four points on Thai benchmarks.

Conclusion: A 47 billion-token Thai corpus, Mangosteen, was created using a Thai-adapted Dolma pipeline. A SEA-LION model pre-trained on Mangosteen surpasses previous models on Thai benchmarks.

Abstract: Pre-training data shapes a language model's quality, but raw web text is
noisy and demands careful cleaning. Existing large-scale corpora rely on
English-centric or language-agnostic pipelines whose heuristics do not capture
Thai script or cultural nuances, leaving risky material such as gambling
content untreated. Prior Thai-specific efforts customize pipelines or build new
ones, yet seldom release their data or document design choices, hindering
reproducibility and raising the question of how to construct a transparent,
high-quality Thai corpus. We introduce Mangosteen: a 47 billion-token Thai
corpus built through a Thai-adapted Dolma pipeline that includes custom
rule-based language ID, revised C4/Gopher quality filters, and Thai-trained
content filters, plus curated non-web sources such as Wikipedia, Royal Gazette
texts, OCR-extracted books, and CC-licensed YouTube subtitles. Systematic
ablations using GPT-2 show the pipeline trims CommonCrawl from 202M to 25M
documents while raising SEA-HELM NLG from 3 to 11; an 8B-parameter SEA-LION
model continually pre-trained on Mangosteen then surpasses SEA-LION-v3 and
Llama-3.1 by about four points on Thai benchmarks. We release the full pipeline
code, cleaning manifests, corpus snapshot, and all checkpoints, providing a
fully reproducible foundation for future Thai and regional LLM research.

</details>


### [28] [Large Language Models as Medical Codes Selectors: a benchmark using the International Classification of Primary Care](https://arxiv.org/abs/2507.14681)
*Vinicius Anjos de Almeida,Vinicius de Camargo,Raquel Gómez-Bravo,Egbert van der Haring,Kees van Boven,Marcelo Finger,Luis Fernandez Lopez*

Main category: cs.CL

TL;DR: 大型语言模型在自动化ICPC-2编码方面显示出希望，但需要进一步的临床验证。


<details>
  <summary>Details</summary>
Motivation: 医学编码构建了用于研究、质量监控和政策的医疗保健数据。本研究评估了大型语言模型（LLM）使用领域特定搜索引擎的输出分配ICPC-2代码的潜力。

Method: 使用了一个包含437个葡萄牙语临床表达式的数据集，每个表达式都用ICPC-2代码进行注释。一个语义搜索引擎（OpenAI的text-embedding-3-large）从73,563个标记的概念中检索候选代码。使用每个查询和检索结果提示了33个LLM，以选择最佳匹配的ICPC-2代码。使用F1分数以及令牌使用量、成本、响应时间和格式依从性来评估性能。

Result: 28个模型的F1分数> 0.8；10个模型超过0.85。表现最佳的模型包括gpt-4.5-preview、o3和gemini-2.5-pro。检索器优化可以将性能提高高达4个点。大多数模型以预期格式返回有效代码，减少了幻觉。较小的模型（<3B）在格式和输入长度方面存在困难。

Conclusion: LLMs表现出在自动化ICPC-2编码方面的强大潜力，即使没有进行微调。这项工作提供了一个基准并强调了挑战，但研究结果受到数据集范围和设置的限制。需要更广泛的、多语言的、端到端的评估来进行临床验证。

Abstract: Background: Medical coding structures healthcare data for research, quality
monitoring, and policy. This study assesses the potential of large language
models (LLMs) to assign ICPC-2 codes using the output of a domain-specific
search engine.
  Methods: A dataset of 437 Brazilian Portuguese clinical expressions, each
annotated with ICPC-2 codes, was used. A semantic search engine (OpenAI's
text-embedding-3-large) retrieved candidates from 73,563 labeled concepts.
Thirty-three LLMs were prompted with each query and retrieved results to select
the best-matching ICPC-2 code. Performance was evaluated using F1-score, along
with token usage, cost, response time, and format adherence.
  Results: Twenty-eight models achieved F1-score > 0.8; ten exceeded 0.85. Top
performers included gpt-4.5-preview, o3, and gemini-2.5-pro. Retriever
optimization can improve performance by up to 4 points. Most models returned
valid codes in the expected format, with reduced hallucinations. Smaller models
(<3B) struggled with formatting and input length.
  Conclusions: LLMs show strong potential for automating ICPC-2 coding, even
without fine-tuning. This work offers a benchmark and highlights challenges,
but findings are limited by dataset scope and setup. Broader, multilingual,
end-to-end evaluations are needed for clinical validation.

</details>


### [29] [MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via Context-Aware Multi-Stage Policy Optimization](https://arxiv.org/abs/2507.14683)
*Xingxuan Li,Yao Xiao,Dianwen Ng,Hai Ye,Yue Deng,Xiang Lin,Bin Wang,Zhanfeng Mo,Chong Zhang,Yueyi Zhang,Zonglin Yang,Ruilin Li,Lei Lei,Shihao Xu,Han Zhao,Weiling Chen,Feng Ji,Lidong Bing*

Main category: cs.CL

TL;DR: MiroMind-M1, a fully open-source RLM series based on Qwen-2.5, is introduced with complete resources for reproducibility, achieving strong performance in mathematical reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing open-source RLMs lack sufficient openness by omitting critical resources such as datasets and detailed training configurations, which hinders reproducibility. This paper aims to contribute toward greater transparency in RLM development.

Method: The models are trained in two stages: SFT on a carefully curated corpus of 719K math-reasoning problems with verified CoT trajectories, followed by RLVR on 62K challenging and verifiable problems. Context-Aware Multi-Stage Policy Optimization, an algorithm that integrates length-progressive training with an adaptive repetition penalty, is introduced to enhance the robustness and efficiency of the RLVR process.

Result: The model achieves state-of-the-art or competitive performance and superior token efficiency among Qwen-2.5-based open-source 7B and 32B models on the AIME24, AIME25, and MATH benchmarks.

Conclusion: The MiroMind-M1 series, a set of fully open-source RLMs built on the Qwen-2.5 backbone, matches or exceeds the performance of existing open-source RLMs. The complete stack, including models, datasets, and all training and evaluation configurations, is released to facilitate reproducibility and foster community advancement.

Abstract: Large language models have recently evolved from fluent text generation to
advanced reasoning across diverse domains, giving rise to reasoning language
models. Among these domains, mathematical reasoning serves as a representative
benchmark as it requires precise multi-step logic and abstract reasoning, which
can be generalized to other tasks. While closed-source RLMs such as GPT-o3
demonstrate impressive reasoning capabilities, their proprietary nature limits
transparency and reproducibility. Although many open-source projects aim to
close this gap, most of them lack sufficient openness by omitting critical
resources such as datasets and detailed training configurations, which hinders
reproducibility. To contribute toward greater transparency in RLM development,
we introduce the MiroMind-M1 series, a set of fully open-source RLMs built on
the Qwen-2.5 backbone that match or exceed the performance of existing
open-source RLMs. Specifically, our models are trained in two stages: SFT on a
carefully curated corpus of 719K math-reasoning problems with verified CoT
trajectories, followed by RLVR on 62K challenging and verifiable problems. To
enhance the robustness and efficiency of the RLVR process, we introduce
Context-Aware Multi-Stage Policy Optimization, an algorithm that integrates
length-progressive training with an adaptive repetition penalty to encourage
context-aware RL training. Our model achieves state-of-the-art or competitive
performance and superior token efficiency among Qwen-2.5-based open-source 7B
and 32B models on the AIME24, AIME25, and MATH benchmarks. To facilitate
reproducibility, we release the complete stack: models (MiroMind-M1-SFT-7B,
MiroMind-M1-RL-7B, MiroMind-M1-RL-32B); datasets (MiroMind-M1-SFT-719K,
MiroMind-M1-RL-62K); and all training and evaluation configurations. We hope
these resources will support further research and foster community advancement.

</details>


### [30] [Mind the Gap: A Review of Arabic Post-Training Datasets and Their Limitations](https://arxiv.org/abs/2507.14688)
*Mohammed Alkhowaiter,Norah Alshahrani,Saied Alshahrani,Reem I. Masoud,Alaa Alzahrani,Deema Alnuhait,Emad A. Alghamdi,Khalid Almubarak*

Main category: cs.CL

TL;DR: This paper reviews Arabic post-training datasets, finds gaps in task diversity and documentation, and suggests improvements.


<details>
  <summary>Details</summary>
Motivation: Post-training is crucial for aligning LLMs with human instructions, and the quality of post-training datasets is central to this process.

Method: Review of publicly available Arabic post-training datasets on the Hugging Face Hub, organized along four key dimensions: LLM Capabilities, Steerability, Alignment, and Robustness. Datasets are evaluated based on popularity, adoption, recency, documentation, licensing, and scientific contribution.

Result: Revealed critical gaps in the development of Arabic post-training datasets.

Conclusion: This paper identifies critical gaps in Arabic post-training datasets, including limited task diversity, inconsistent documentation, and low adoption, and recommends future efforts in dataset development.

Abstract: Post-training has emerged as a crucial technique for aligning pre-trained
Large Language Models (LLMs) with human instructions, significantly enhancing
their performance across a wide range of tasks. Central to this process is the
quality and diversity of post-training datasets. This paper presents a review
of publicly available Arabic post-training datasets on the Hugging Face Hub,
organized along four key dimensions: (1) LLM Capabilities (e.g., Question
Answering, Translation, Reasoning, Summarization, Dialogue, Code Generation,
and Function Calling); (2) Steerability (e.g., persona and system prompts); (3)
Alignment (e.g., cultural, safety, ethics, and fairness), and (4) Robustness.
Each dataset is rigorously evaluated based on popularity, practical adoption,
recency and maintenance, documentation and annotation quality, licensing
transparency, and scientific contribution. Our review revealed critical gaps in
the development of Arabic post-training datasets, including limited task
diversity, inconsistent or missing documentation and annotation, and low
adoption across the community. Finally, the paper discusses the implications of
these gaps on the progress of Arabic LLMs and applications while providing
concrete recommendations for future efforts in post-training dataset
development.

</details>


### [31] [Rethinking Suicidal Ideation Detection: A Trustworthy Annotation Framework and Cross-Lingual Model Evaluation](https://arxiv.org/abs/2507.14693)
*Amina Dzafic,Merve Kavut,Ulya Bayram*

Main category: cs.CL

TL;DR: 构建了一个土耳其语自杀意念语料库，评估了标签可靠性和模型一致性，发现需要更严谨、更具语言包容性的注释和评估方法。


<details>
  <summary>Details</summary>
Motivation: 自杀意念检测对于实时预防自杀至关重要，但其进展面临两个未被充分探索的挑战：语言覆盖范围有限和注释实践不可靠。大多数可用的数据集都是英文的，但即使在这些数据集中，高质量的人工注释数据仍然稀缺。缺乏其他语言的数据集进一步限制了通过人工智能（AI）在全球范围内实现自杀预防。

Method: 构建了一个新的土耳其语自杀意念语料库，该语料库来自社交媒体帖子，并引入了一种资源高效的注释框架，涉及三个人工注释员和两个大型语言模型（LLM）。通过八个预训练的情感和情感分类器进行迁移学习，对该数据集和三个流行的英语自杀意念检测数据集的标签可靠性和模型一致性进行了双向评估。

Result: 强调需要在心理健康自然语言处理（NLP）中采用更严格、更具语言包容性的注释和评估方法，同时证明了流行的模型在零样本迁移学习中的可疑性能。

Conclusion: 需要更严谨、更具语言包容性的方法来进行心理健康自然语言处理（NLP）中的注释和评估，同时证明了流行的模型在零样本迁移学习中的可疑表现。我们提倡心理健康自然语言处理中模型训练和数据集构建的透明度，优先考虑数据和模型的可靠性。

Abstract: Suicidal ideation detection is critical for real-time suicide prevention, yet
its progress faces two under-explored challenges: limited language coverage and
unreliable annotation practices. Most available datasets are in English, but
even among these, high-quality, human-annotated data remains scarce. As a
result, many studies rely on available pre-labeled datasets without examining
their annotation process or label reliability. The lack of datasets in other
languages further limits the global realization of suicide prevention via
artificial intelligence (AI). In this study, we address one of these gaps by
constructing a novel Turkish suicidal ideation corpus derived from social media
posts and introducing a resource-efficient annotation framework involving three
human annotators and two large language models (LLMs). We then address the
remaining gaps by performing a bidirectional evaluation of label reliability
and model consistency across this dataset and three popular English suicidal
ideation detection datasets, using transfer learning through eight pre-trained
sentiment and emotion classifiers. These transformers help assess annotation
consistency and benchmark model performance against manually labeled data. Our
findings underscore the need for more rigorous, language-inclusive approaches
to annotation and evaluation in mental health natural language processing (NLP)
while demonstrating the questionable performance of popular models with
zero-shot transfer learning. We advocate for transparency in model training and
dataset construction in mental health NLP, prioritizing data and model
reliability.

</details>


### [32] [Disparities in Peer Review Tone and the Role of Reviewer Anonymity](https://arxiv.org/abs/2507.14741)
*Maria Sahakyan,Bedoor AlShebli*

Main category: cs.CL

TL;DR: 同行评审存在偏见，匿名评审未能完全实现公平。


<details>
  <summary>Details</summary>
Motivation: 同行评审过程并非没有偏见，但语言本身可能强化差距。

Method: 使用自然语言处理和大规模统计建模，检查了两个主要期刊的 80,000 多篇评论。

Result: 评审语气、情感和支持性语言因作者的人口统计（包括性别、种族和机构关系）而异。评审者身份的披露会影响评估语言。

Conclusion: 揭示了同行评审中隐藏的偏见，并质疑了匿名在公平性中的作用。

Abstract: The peer review process is often regarded as the gatekeeper of scientific
integrity, yet increasing evidence suggests that it is not immune to bias.
Although structural inequities in peer review have been widely debated, much
less attention has been paid to the subtle ways in which language itself may
reinforce disparities. This study undertakes one of the most comprehensive
linguistic analyses of peer review to date, examining more than 80,000 reviews
in two major journals. Using natural language processing and large-scale
statistical modeling, it uncovers how review tone, sentiment, and supportive
language vary across author demographics, including gender, race, and
institutional affiliation. Using a data set that includes both anonymous and
signed reviews, this research also reveals how the disclosure of reviewer
identity shapes the language of evaluation. The findings not only expose hidden
biases in peer feedback, but also challenge conventional assumptions about
anonymity's role in fairness. As academic publishing grapples with reform,
these insights raise critical questions about how review policies shape career
trajectories and scientific progress.

</details>


### [33] [On the robustness of modeling grounded word learning through a child's egocentric input](https://arxiv.org/abs/2507.14749)
*Wai Keen Vong,Brenden M. Lake*

Main category: cs.CL

TL;DR: Multimodal neural networks trained on automatically transcribed data from multiple children can acquire and generalize word-referent mappings, validating their robustness for grounded word learning and highlighting individual differences in learning.


<details>
  <summary>Details</summary>
Motivation: Large language and multimodal models rely on massive training datasets, creating a mismatch with children's language acquisition from limited input. It was not explored whether the success of training neural networks using data similar in quantity and quality to children's input reflects the idiosyncrasies of a single child's experience, or whether it would show consistent and robust learning patterns across multiple children's experiences.

Method: Applied automated speech transcription methods to the entirety of the SAYCam dataset and generated multi-modal vision-and-language datasets for training and evaluation, and explored a range of neural network configurations.

Result: Networks trained on automatically transcribed data from each child can acquire and generalize word-referent mappings across multiple network architectures.

Conclusion: Networks trained on automatically transcribed data from each child can acquire and generalize word-referent mappings across multiple network architectures, validating the robustness of multimodal neural networks for grounded word learning, while highlighting the individual differences that emerge in how models learn when trained on each child's developmental experiences.

Abstract: What insights can machine learning bring to understanding human language
acquisition? Large language and multimodal models have achieved remarkable
capabilities, but their reliance on massive training datasets creates a
fundamental mismatch with children, who succeed in acquiring language from
comparatively limited input. To help bridge this gap, researchers have
increasingly trained neural networks using data similar in quantity and quality
to children's input. Taking this approach to the limit, Vong et al. (2024)
showed that a multimodal neural network trained on 61 hours of visual and
linguistic input extracted from just one child's developmental experience could
acquire word-referent mappings. However, whether this approach's success
reflects the idiosyncrasies of a single child's experience, or whether it would
show consistent and robust learning patterns across multiple children's
experiences was not explored. In this article, we applied automated speech
transcription methods to the entirety of the SAYCam dataset, consisting of over
500 hours of video data spread across all three children. Using these automated
transcriptions, we generated multi-modal vision-and-language datasets for both
training and evaluation, and explored a range of neural network configurations
to examine the robustness of simulated word learning. Our findings demonstrate
that networks trained on automatically transcribed data from each child can
acquire and generalize word-referent mappings across multiple network
architectures. These results validate the robustness of multimodal neural
networks for grounded word learning, while highlighting the individual
differences that emerge in how models learn when trained on each child's
developmental experiences.

</details>


### [34] [FastLongSpeech: Enhancing Large Speech-Language Models for Efficient Long-Speech Processing](https://arxiv.org/abs/2507.14815)
*Shoutao Guo,Shaolei Zhang,Qingkai Fang,Zhengrui Ma,Min Zhang,Yang Feng*

Main category: cs.CL

TL;DR: FastLongSpeech is introduced to enable LSLMs to efficiently process long-form speech without requiring long-speech training data. It uses iterative fusion and dynamic compression training. The method shows strong performance and efficiency, evaluated using LongSpeech-Eval.


<details>
  <summary>Details</summary>
Motivation: Efficient processing of long-form speech by Large Speech-Language Models (LSLMs) is a critical challenge due to the scarcity of long-speech training datasets and high computational costs.

Method: The paper introduces FastLongSpeech, a framework with an iterative fusion strategy for compressing long-speech sequences and a dynamic compression training approach to adapt LSLMs for long-speech inputs.

Result: The FastLongSpeech framework exhibits strong performance in both long-speech and short-speech tasks and greatly improves inference efficiency. A new long-speech understanding benchmark called LongSpeech-Eval was developed to assess the long-speech capabilities of LSLMs.

Conclusion: The proposed FastLongSpeech framework demonstrates strong performance in both long-speech and short-speech tasks and improves inference efficiency.

Abstract: The rapid advancement of Large Language Models (LLMs) has spurred significant
progress in Large Speech-Language Models (LSLMs), enhancing their capabilities
in both speech understanding and generation. While existing LSLMs often
concentrate on augmenting speech generation or tackling a diverse array of
short-speech tasks, the efficient processing of long-form speech remains a
critical yet underexplored challenge. This gap is primarily attributed to the
scarcity of long-speech training datasets and the high computational costs
associated with long sequences. To address these limitations, we introduce
FastLongSpeech, a novel framework designed to extend LSLM capabilities for
efficient long-speech processing without necessitating dedicated long-speech
training data. FastLongSpeech incorporates an iterative fusion strategy that
can compress excessively long-speech sequences into manageable lengths. To
adapt LSLMs for long-speech inputs, it introduces a dynamic compression
training approach, which exposes the model to short-speech sequences at varying
compression ratios, thereby transferring the capabilities of LSLMs to
long-speech tasks. To assess the long-speech capabilities of LSLMs, we develop
a long-speech understanding benchmark called LongSpeech-Eval. Experiments show
that our method exhibits strong performance in both long-speech and
short-speech tasks, while greatly improving inference efficiency.

</details>


### [35] [Doc2Chart: Intent-Driven Zero-Shot Chart Generation from Documents](https://arxiv.org/abs/2507.14819)
*Akriti Jain,Pritika Ramu,Aparna Garimella,Apoorv Saxena*

Main category: cs.CL

TL;DR: This paper introduces intent-based chart generation from long documents, proposes a two-stage unsupervised framework, and demonstrates its superior performance compared to baselines on a newly curated dataset.


<details>
  <summary>Details</summary>
Motivation: Existing instruction-tuning methods for transforming text/tables to data visualizations are not directly applicable to real-world use cases of visualizing data from long documents based on user intents without manual content pre-selection. The paper introduces the task of intent-based chart generation from documents in a zero-shot setting.

Method: An unsupervised, two-staged framework is proposed, involving LLM-based information extraction with iterative validation and refinement, followed by a heuristic-guided chart type selection module before final code generation.

Result: The proposed method outperforms baselines by up to 9 points in chart data accuracy and 17 points in chart type accuracy. A new dataset of 1,242 <intent, document, charts> tuples from finance and scientific domains is curated.

Conclusion: The proposed unsupervised, two-staged framework outperforms baselines in chart data accuracy and chart type generation by up to 9 and 17 points, respectively.

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in
transforming text descriptions or tables to data visualizations via
instruction-tuning methods. However, it is not straightforward to apply these
methods directly for a more real-world use case of visualizing data from long
documents based on user-given intents, as opposed to the user pre-selecting the
relevant content manually. We introduce the task of intent-based chart
generation from documents: given a user-specified intent and document(s), the
goal is to generate a chart adhering to the intent and grounded on the
document(s) in a zero-shot setting. We propose an unsupervised, two-staged
framework in which an LLM first extracts relevant information from the
document(s) by decomposing the intent and iteratively validates and refines
this data. Next, a heuristic-guided module selects an appropriate chart type
before final code generation. To assess the data accuracy of the generated
charts, we propose an attribution-based metric that uses a structured textual
representation of charts, instead of relying on visual decoding metrics that
often fail to capture the chart data effectively. To validate our approach, we
curate a dataset comprising of 1,242 $<$intent, document, charts$>$ tuples from
two domains, finance and scientific, in contrast to the existing datasets that
are largely limited to parallel text descriptions/ tables and their
corresponding charts. We compare our approach with baselines using single-shot
chart generation using LLMs and query-based retrieval methods; our method
outperforms by upto $9$ points and $17$ points in terms of chart data accuracy
and chart type respectively over the best baselines.

</details>


### [36] [Beyond Isolated Capabilities: Bridging Long CoT Reasoning and Long-Context Understanding](https://arxiv.org/abs/2507.14849)
*Yifei Wang*

Main category: cs.CL

TL;DR: Reasoning distillation enhances long-context understanding in smaller language models, improving in-context retrieval and mitigating the 'lost in the middle' problem.


<details>
  <summary>Details</summary>
Motivation: The impact of large-scale reasoning distillation on in-context retrieval and reasoning in Retrieval-Augmented Generation (RAG) systems remains unexplored.

Method: Comprehensive investigation using open-source models distilled from Deepseek-R1, focusing on multi-document question answering tasks.

Result: Distillation fosters greater long-context awareness by promoting more detailed and explicit reasoning processes during context analysis and information parsing.

Conclusion: Distilled reasoning patterns significantly improve long-context understanding by promoting detailed reasoning, mitigating the 'lost in the middle' issue.

Abstract: Reasoning distillation has emerged as an effective approach to enhance the
reasoning capabilities of smaller language models. However, the impact of
large-scale reasoning distillation on other critical abilities, particularly
in-context retrieval and reasoning, remains unexplored. This gap in
understanding is particularly significant given the increasing importance of
Retrieval-Augmented Generation (RAG) systems, where efficient acquisition and
utilization of contextual information are paramount for generating reliable
responses. Motivated by the need to understand how the extended long-CoT
process influences long-context comprehension, we conduct a comprehensive
investigation using a series of open-source models distilled from Deepseek-R1,
renowned for its exceptional reasoning capabilities. Our study focuses on
evaluating these models' performance in extracting and integrating relevant
information from extended contexts through multi-document question and
answering tasks. Through rigorous experimentation, we demonstrate that
distilled reasoning patterns significantly improve long-context understanding.
Our analysis reveals that distillation fosters greater long-context awareness
by promoting more detailed and explicit reasoning processes during context
analysis and information parsing. This advancement effectively mitigates the
persistent "lost in the middle" issue that has hindered long-context models.

</details>


### [37] [Tiny language models](https://arxiv.org/abs/2507.14871)
*Ronit D. Gross,Yarden Tzach,Tal Halevi,Ella Koresh,Ido Kanter*

Main category: cs.CL

TL;DR: This paper explores whether tiny language models (TLMs) exhibit the same key qualitative features of LLMs and demonstrates the effectiveness of pre-training, even at a tiny scale.


<details>
  <summary>Details</summary>
Motivation: LLM pre-training is currently feasible only for a few dominant companies due to the immense computational resources required, limiting broader research participation, creating a critical need for more accessible alternatives.

Method: Pre-training BERT-6 and variants of BERT-1 on subsets of the Wikipedia dataset and evaluating their performance on FewRel, AGNews, and DBPedia classification tasks.

Result: TLMs exhibit a clear performance gap between pre-trained and non-pre-trained models across classification tasks, indicating the effectiveness of pre-training, even at a tiny scale. The performance gap increases with the size of the pre-training dataset and with greater overlap between tokens in the pre-training and classification datasets.

Conclusion: Pre-trained deep TLM architecture's classification accuracy can be replicated through a soft committee of multiple, independently pre-trained shallow architectures, enabling low-latency TLMs without affecting classification accuracy.

Abstract: A prominent achievement of natural language processing (NLP) is its ability
to understand and generate meaningful human language. This capability relies on
complex feedforward transformer block architectures pre-trained on large
language models (LLMs). However, LLM pre-training is currently feasible only
for a few dominant companies due to the immense computational resources
required, limiting broader research participation. This creates a critical need
for more accessible alternatives. In this study, we explore whether tiny
language models (TLMs) exhibit the same key qualitative features of LLMs. We
demonstrate that TLMs exhibit a clear performance gap between pre-trained and
non-pre-trained models across classification tasks, indicating the
effectiveness of pre-training, even at a tiny scale. The performance gap
increases with the size of the pre-training dataset and with greater overlap
between tokens in the pre-training and classification datasets. Furthermore,
the classification accuracy achieved by a pre-trained deep TLM architecture can
be replicated through a soft committee of multiple, independently pre-trained
shallow architectures, enabling low-latency TLMs without affecting
classification accuracy. Our results are based on pre-training BERT-6 and
variants of BERT-1 on subsets of the Wikipedia dataset and evaluating their
performance on FewRel, AGNews, and DBPedia classification tasks. Future
research on TLM is expected to further illuminate the mechanisms underlying
NLP, especially given that its biologically inspired models suggest that TLMs
may be sufficient for children or adolescents to develop language.

</details>


### [38] [MEKiT: Multi-source Heterogeneous Knowledge Injection Method via Instruction Tuning for Emotion-Cause Pair Extraction](https://arxiv.org/abs/2507.14887)
*Shiyi Mu,Yongkang Liu,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.CL

TL;DR: This paper proposes MEKiT, a multi-source heterogeneous knowledge injection method, to improve LLMs' performance on the ECPE task by integrating internal emotional knowledge and external causal knowledge. MEKiT outperforms baselines and improves LLMs' performance dramatically.


<details>
  <summary>Details</summary>
Motivation: LLMs' performance on the Emotion-Cause Pair Extraction (ECPE) task is often underperform smaller language model. The main reason is the lack of auxiliary knowledge, which limits LLMs' ability to effectively perceive emotions and reason causes.

Method: Multi-source Heterogeneous Knowledge injection Method, MEKiT, which integrates heterogeneous internal emotional knowledge and external causal knowledge. incorporates instruction templates and mixing data for instruction-tuning.

Result: MEKiT provides a more effective and adaptable solution for the ECPE task, exhibiting an absolute performance advantage over compared baselines and dramatically improving the performance of LLMs on the ECPE task.

Conclusion: MEKiT is a more effective and adaptable solution for the ECPE task, exhibiting an absolute performance advantage over compared baselines and dramatically improving the performance of LLMs on the ECPE task.

Abstract: Although large language models (LLMs) excel in text comprehension and
generation, their performance on the Emotion-Cause Pair Extraction (ECPE) task,
which requires reasoning ability, is often underperform smaller language model.
The main reason is the lack of auxiliary knowledge, which limits LLMs' ability
to effectively perceive emotions and reason causes. To address this issue, we
propose a novel \textbf{M}ulti-source h\textbf{E}terogeneous \textbf{K}nowledge
\textbf{i}njection me\textbf{T}hod, MEKiT, which integrates heterogeneous
internal emotional knowledge and external causal knowledge. Specifically, for
these two distinct aspects and structures of knowledge, we apply the approaches
of incorporating instruction templates and mixing data for instruction-tuning,
which respectively facilitate LLMs in more comprehensively identifying emotion
and accurately reasoning causes. Experimental results demonstrate that MEKiT
provides a more effective and adaptable solution for the ECPE task, exhibiting
an absolute performance advantage over compared baselines and dramatically
improving the performance of LLMs on the ECPE task.

</details>


### [39] [Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs](https://arxiv.org/abs/2507.14894)
*Boyi Deng,Yu Wan,Baosong Yang,Fei Huang,Wenjie Wang,Fuli Feng*

Main category: cs.CL

TL;DR: 本文分析了LLM中意外的code-switching问题，发现语言特征的过度pre-activation值是主要原因。提出了SASFT方法，通过控制pre-activation值来减少code-switching，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLM)具有令人印象深刻的多语言能力，但它们会遭受意外的code-switching问题，导致模型响应的可读性差，并降低模型的可用性。

Method: 提出Sparse Autoencoder-guided Supervised Fine-tuning (SASFT)，教LLM在训练期间保持特定语言特征的适当pre-activation值。

Result: 在三个语言的五个模型上的实验表明，与标准监督微调相比，SASFT始终能减少50%以上的意外code-switching，在四个案例中完全消除。此外，SASFT保持甚至提高了模型在六个多语言benchmark上的性能。

Conclusion: SASFT能有效减少LLM中意外的code-switching，同时保持甚至提高模型在多语言benchmark上的性能。

Abstract: Large Language Models (LLMs) have impressive multilingual capabilities, but
they suffer from unexpected code-switching, also known as language mixing,
which involves switching to unexpected languages in the model response. This
problem leads to poor readability and degrades the usability of model
responses. However, existing work on this issue lacks a mechanistic analysis
and shows limited effectiveness. In this paper, we first provide an in-depth
analysis of unexpected code-switching using sparse autoencoders and find that
when LLMs switch to a language, the features of that language exhibit excessive
pre-activation values. Based on our findings, we propose $\textbf{S}$parse
$\textbf{A}$utoencoder-guided $\textbf{S}$upervised
$\textbf{F}$ine$\textbf{t}$uning (SASFT), which teaches LLMs to maintain
appropriate pre-activation values of specific language features during
training. Experiments on five models across three languages demonstrate that
SASFT consistently reduces unexpected code-switching by more than 50\% compared
to standard supervised fine-tuning, with complete elimination in four cases.
Moreover, SASFT maintains or even improves the models' performance on six
multilingual benchmarks, showing its effectiveness in addressing code-switching
while preserving multilingual capabilities.

</details>


### [40] [From Neurons to Semantics: Evaluating Cross-Linguistic Alignment Capabilities of Large Language Models via Neurons Alignment](https://arxiv.org/abs/2507.14900)
*Chongxuan Huang,Yongshi Ye,Biao Fu,Qifeng Su,Xiaodong Shi*

Main category: cs.CL

TL;DR: 提出 NeuronXA 来评估 LLM 的跨语言对齐能力，即使在小数据集上也能有效评估跨语言对齐和可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有的对齐基准主要集中在句子嵌入上，但之前的研究表明，神经模型倾向于产生非平滑的表示空间，这会影响语义对齐评估在低资源语言上的效果。

Method: 提出了一种新的基于神经元状态的跨语言对齐 (NeuronXA) 方法，以评估 LLM 的跨语言对齐能力。

Result: NeuronXA 在多个主要的 multilingual LLM（LLaMA、Qwen、Mistral、GLM 和 OLMo）上进行了评估，跨越两个迁移任务和三个 multilingual 基准。结果表明，仅使用 100 个并行句子对，NeuronXA 与下游任务性能的相关系数为 0.9556，与可迁移性的相关系数为 0.8514。

Conclusion: NeuronXA 是一种评估跨语言对齐和可迁移性的有效方法，即使在小型数据集上也能实现。它有潜力推进跨语言对齐研究，并提高多语言 LLM 的语义理解。

Abstract: Large language models (LLMs) have demonstrated remarkable multilingual
capabilities, however, how to evaluate cross-lingual alignment remains
underexplored. Existing alignment benchmarks primarily focus on sentence
embeddings, but prior research has shown that neural models tend to induce a
non-smooth representation space, which impact of semantic alignment evaluation
on low-resource languages. Inspired by neuroscientific findings that similar
information activates overlapping neuronal regions, we propose a novel Neuron
State-Based Cross-Lingual Alignment (NeuronXA) to assess the cross-lingual a
lignment capabilities of LLMs, which offers a more semantically grounded
approach to assess cross-lingual alignment. We evaluate NeuronXA on several
prominent multilingual LLMs (LLaMA, Qwen, Mistral, GLM, and OLMo) across two
transfer tasks and three multilingual benchmarks. The results demonstrate that
with only 100 parallel sentence pairs, NeuronXA achieves a Pearson correlation
of 0.9556 with downstream tasks performance and 0.8514 with transferability.
These findings demonstrate NeuronXA's effectiveness in assessing both
cross-lingual alignment and transferability, even with a small dataset. This
highlights its potential to advance cross-lingual alignment research and to
improve the semantic understanding of multilingual LLMs.

</details>


### [41] [PromptSuite: A Task-Agnostic Framework for Multi-Prompt Generation](https://arxiv.org/abs/2507.14913)
*Eliya Habba,Noam Dahan,Gili Lior,Gabriel Stanovsky*

Main category: cs.CL

TL;DR: PromptSuite is a framework that enables the automatic generation of various prompts.


<details>
  <summary>Details</summary>
Motivation: Evaluating LLMs with a single prompt has proven unreliable, with small changes leading to significant performance differences. However, generating the prompt variations needed for a more robust multi-prompt evaluation is challenging, limiting its adoption in practice.

Method: a framework that enables the automatic generation of various prompts

Result: PromptSuite is flexible - working out of the box on a wide range of tasks and benchmarks. It follows a modular prompt design, allowing controlled perturbations to each component, and is extensible, supporting the addition of new components and perturbation types.

Conclusion: PromptSuite provides meaningful variations to support strong evaluation practices.

Abstract: Evaluating LLMs with a single prompt has proven unreliable, with small
changes leading to significant performance differences. However, generating the
prompt variations needed for a more robust multi-prompt evaluation is
challenging, limiting its adoption in practice. To address this, we introduce
PromptSuite, a framework that enables the automatic generation of various
prompts. PromptSuite is flexible - working out of the box on a wide range of
tasks and benchmarks. It follows a modular prompt design, allowing controlled
perturbations to each component, and is extensible, supporting the addition of
new components and perturbation types. Through a series of case studies, we
show that PromptSuite provides meaningful variations to support strong
evaluation practices. It is available through both a Python API:
https://github.com/eliyahabba/PromptSuite, and a user-friendly web interface:
https://promptsuite.streamlit.app/

</details>


### [42] [SYNTHIA: Synthetic Yet Naturally Tailored Human-Inspired PersonAs](https://arxiv.org/abs/2507.14922)
*Vahid Rahimzadeh,Erfan Moosavi Monazzah,Mohammad Taher Pilehvar,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: SYNTHIA: a dataset of 30,000 backstories derived from 10,000 real social media users from BlueSky open platform across three time windows, grounding synthetic generation in authentic user activity.


<details>
  <summary>Details</summary>
Motivation: existing approaches fall at opposite extremes, either relying on costly human-curated data or producing synthetic personas that lack consistency and realism.

Method: grounding synthetic generation in authentic user activity

Result: a dataset of 30,000 backstories derived from 10,000 real social media users

Conclusion: SYNTHIA achieves competitive performance in demographic diversity and social survey alignment while significantly outperforming them in narrative consistency. Uniquely, SYNTHIA incorporates temporal dimensionality and provides rich social interaction metadata.

Abstract: Persona-driven LLMs have emerged as powerful tools in computational social
science, yet existing approaches fall at opposite extremes, either relying on
costly human-curated data or producing synthetic personas that lack consistency
and realism. We introduce SYNTHIA, a dataset of 30,000 backstories derived from
10,000 real social media users from BlueSky open platform across three time
windows, bridging this spectrum by grounding synthetic generation in authentic
user activity. Our evaluation demonstrates that SYNTHIA achieves competitive
performance with state-of-the-art methods in demographic diversity and social
survey alignment while significantly outperforming them in narrative
consistency. Uniquely, SYNTHIA incorporates temporal dimensionality and
provides rich social interaction metadata from the underlying network, enabling
new research directions in computational social science and persona-driven
language modeling.

</details>


### [43] [MUR: Momentum Uncertainty guided Reasoning for Large Language Models](https://arxiv.org/abs/2507.14958)
*Hang Yan,Fangzhi Xu,Rongman Xu,Yifei Li,Jian Zhang,Haoran Luo,Xiaobao Wu,Luu Anh Tuan,Haiteng Zhao,Qika Lin,Jun Liu*

Main category: cs.CL

TL;DR: 提出了MUR，一种动态分配推理预算的方法，以提高LLM的推理效率和准确性，同时减少计算量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在推理密集型任务上取得了令人印象深刻的性能，但优化其推理效率仍然是一个开放的挑战。虽然测试时缩放（TTS）提高了推理质量，但它通常会导致过度思考，将token浪费在冗余计算上。

Method: 提出了动量不确定性引导推理（MUR），通过跟踪和聚合逐步不确定性，动态地将思考预算分配给关键的推理步骤。

Result: 在四个具有挑战性的基准测试中，MUR平均减少了50%以上的计算量，同时提高了0.62-3.37%的准确率。

Conclusion: MUR显著减少了计算量，同时提高了准确性。

Abstract: Large Language Models (LLMs) have achieved impressive performance on
reasoning-intensive tasks, yet optimizing their reasoning efficiency remains an
open challenge. While Test-Time Scaling (TTS) improves reasoning quality, it
often leads to overthinking, wasting tokens on redundant computations. This
work investigates how to efficiently and adaptively guide LLM test-time scaling
without additional training. Inspired by the concept of momentum in physics, we
propose Momentum Uncertainty-guided Reasoning (MUR), which dynamically
allocates thinking budgets to critical reasoning steps by tracking and
aggregating stepwise uncertainty over time. To support flexible inference-time
control, we introduce gamma-control, a simple mechanism that tunes the
reasoning budget via a single hyperparameter. We provide in-depth theoretical
proof to support the superiority of MUR in terms of stability and biases. MUR
is comprehensively evaluated against various TTS methods across four
challenging benchmarks (MATH-500, AIME24, AIME25, and GPQA-diamond) using
different sizes of recent Qwen3 models (1.7B, 4B, and 8B). Results demonstrate
that MUR reduces computation by over 50% on average while improving accuracy by
0.62-3.37%.

</details>


### [44] [RefCritic: Training Long Chain-of-Thought Critic Models with Refinement Feedback](https://arxiv.org/abs/2507.15024)
*Qiaoyu Tang,Hao Xiang,Le Yu,Bowen Yu,Hongyu Lin,Yaojie Lu,Xianpei Han,Le Sun,Junyang Lin*

Main category: cs.CL

TL;DR: 本文提出了RefCritic，一种基于强化学习的长链思维评论模块，用于提高大型语言模型的评论能力，并在多个基准测试中取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）的快速发展，开发用于精确指导的有效评论模块已变得至关重要，但也具有挑战性。本文初步证明，用于构建评论模块的监督微调（已被当前解决方案广泛采用）未能真正提高模型的评论能力，从而产生肤浅的评论，且反思和验证不足。

Method: RefCritic，一个基于强化学习的长链思维评论模块，具有双重基于规则的奖励：(1)解决方案判断的实例级别正确性；(2)基于评论的策略模型的改进准确性，旨在生成高质量的评估，并提供有效的反馈，从而有效地指导模型改进。

Result: RefCritic在Qwen2.5-14B-Instruct和DeepSeek-R1-Distill-Qwen-14B上进行了评估，并在五个基准测试中展示了一致的优势。在评论和改进设置中，RefCritic展示了一致的优势。值得注意的是，在多数投票下，经过RefCritic过滤的策略模型显示出随着投票数量增加的卓越扩展性。此外，尽管在解决方案级别的监督下进行训练，但RefCritic在ProcessBench上优于步骤级别的监督方法。

Conclusion: RefCritic在五个基准测试中展示了一致的优势，例如，在AIME25上，相应的基本模型分别获得了6.8%和7.2%的收益。值得注意的是，在多数投票下，经过RefCritic过滤的策略模型显示出随着投票数量增加的卓越扩展性。此外，尽管在解决方案级别的监督下进行训练，但RefCritic在ProcessBench上优于步骤级别的监督方法，ProcessBench是一个用于识别数学推理中错误步骤的基准。

Abstract: With the rapid advancement of Large Language Models (LLMs), developing
effective critic modules for precise guidance has become crucial yet
challenging. In this paper, we initially demonstrate that supervised
fine-tuning for building critic modules (which is widely adopted in current
solutions) fails to genuinely enhance models' critique abilities, producing
superficial critiques with insufficient reflections and verifications. To
unlock the unprecedented critique capabilities, we propose RefCritic, a
long-chain-of-thought critic module based on reinforcement learning with dual
rule-based rewards: (1) instance-level correctness of solution judgments and
(2) refinement accuracies of the policy model based on critiques, aiming to
generate high-quality evaluations with actionable feedback that effectively
guides model refinement. We evaluate RefCritic on Qwen2.5-14B-Instruct and
DeepSeek-R1-Distill-Qwen-14B across five benchmarks. On critique and refinement
settings, RefCritic demonstrates consistent advantages across all benchmarks,
e.g., 6.8\% and 7.2\% gains on AIME25 for the respective base models. Notably,
under majority voting, policy models filtered by RefCritic show superior
scaling with increased voting numbers. Moreover, despite training on
solution-level supervision, RefCritic outperforms step-level supervised
approaches on ProcessBench, a benchmark to identify erroneous steps in
mathematical reasoning.

</details>


### [45] [WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization](https://arxiv.org/abs/2507.15061)
*Zhengwei Tao,Jialong Wu,Wenbiao Yin,Junkai Zhang,Baixuan Li,Haiyang Shen,Kuan Li,Liwen Zhang,Xinyu Wang,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: WebShaper 提出了一个形式化驱动的 IS 数据合成框架，以构建数据集，从而在开放域信息检索任务中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 高质量训练数据的稀缺性限制了IS代理的开发。现有方法通常采用信息驱动范例，该范例首先收集Web数据，然后根据检索生成问题，这可能导致信息结构和推理结构、问题和答案之间不一致。

Method: WebShaper通过集合论系统地形式化IS任务，并使用Knowledge Projections (KP)精确控制推理结构。

Result: WebShaper在GAIA和WebWalkerQA基准测试中实现了开源IS代理中最先进的性能。

Conclusion: WebShaper在GAIA和WebWalkerQA基准测试中实现了最先进的性能。

Abstract: The advent of Large Language Model (LLM)-powered agents has revolutionized
artificial intelligence by enabling solutions to complex, open-ended tasks
through web-based information-seeking (IS) capabilities. The scarcity of
high-quality training data has limited the development of IS agents. Existing
approaches typically adopt an information-driven paradigm that first collects
web data and then generates questions based on the retrieval. However, this may
lead to inconsistency between information structure and reasoning structure,
question and answer. To mitigate, we propose a formalization-driven IS data
synthesis framework WebShaper to construct a dataset. WebShaper systematically
formalizes IS tasks through set theory. Central to the formalization is the
concept of Knowledge Projections (KP), which enables precise control over
reasoning structure by KP operation compositions. During synthesis, we begin by
creating seed tasks, then use a multi-step expansion process. At each step, an
agentic Expander expands the current formal question more complex with
retrieval and validation tools based on our formalization. We train our model
on the synthesized dataset. Experiment results demonstrate that WebShaper
achieves state-of-the-art performance among open-sourced IS agents on GAIA and
WebWalkerQA benchmarks.

</details>


### [46] [Evaluation of Coding Schemes for Transformer-based Gene Sequence Modeling](https://arxiv.org/abs/2507.15087)
*Chenlei Gong,Yuanhe Tian,Lei Mao,Yan Song*

Main category: cs.CL

TL;DR: 该研究比较了DNA Transformer模型中的tokenization和位置编码方法，发现BPE表现最好，RoPE擅长捕捉周期性motif，增加深度到12层可以获得显著收益。


<details>
  <summary>Details</summary>
Motivation: 目前，许多研究将DNA序列视为一种特殊类型的语言，并利用Transformer对其进行建模。这些研究使用固定长度的k-mer分割和BPE子词tokenization，但缺乏系统的评估来确定哪种方法更优越。

Method: 比较k-mer分割（k=1,3,4,5,6）、4,096-token BPE词汇和三种位置编码方法——正弦、AliBi和RoPE。每个配置都在3层、6层、12层和24层Transformer编码器中从头开始训练，并在GUE基准数据集上进行评估。

Result: BPE在各项任务中表现出更高和更稳定的性能。RoPE擅长捕捉周期性motif和推断长序列，而AliBi在由局部依赖驱动的任务中也表现良好。当层数从3层增加到12层时，我们观察到显著的增益。

Conclusion: BPE在各项任务中表现出更高和更稳定的性能，因为它将频繁的motif压缩为可变长度的token，减少了序列长度，并提高了模型泛化能力。RoPE擅长捕捉周期性motif和推断长序列，而AliBi在由局部依赖驱动的任务中也表现良好。在深度方面，当层数从3层增加到12层时，我们观察到显著的增益，而在24层时只有边际改进或轻微的过度拟合。

Abstract: Currently, many studies view DNA sequences as a special type of language and
utilize Transformers to model them. These studies use fixed-length k-mer
segmentation and BPE subword tokenization but lack a systematic evaluation to
determine which is superior. We compare k-mer segmentation with k=1,3,4,5,6, a
4,096-token BPE vocabulary, and three positional encoding methods-sinusoidal,
AliBi, and RoPE. Each configuration is trained from scratch in 3, 6, 12, and
24-layer Transformer encoders and evaluated on GUE benchmark dataset. In
general, BPE delivers higher and more stable performance across tasks by
compressing frequent motifs into variable-length tokens, reducing sequence
length, and improving model generalization. RoPE excels at capturing periodic
motifs and extrapolating to long sequences, while AliBi also performs well on
tasks driven by local dependencies. In terms of depth, we observe significant
gains when increasing layers from 3 to 12, with only marginal improvements or
slight overfitting at 24 layers. This study provides practical guidance for
designing tokenization and positional encoding in DNA Transformer models.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [47] [Comparative Analysis of Algorithms for the Fitting of Tessellations to 3D Image Data](https://arxiv.org/abs/2507.14268)
*Andreas Alpers,Orkun Furat,Christian Jung,Matthias Neumann,Claudia Redenbach,Aigerim Saken,Volker Schmidt*

Main category: cs.CV

TL;DR: This paper reviews and assesses optimization-based methods for generating Voronoi, Laguerre, and generalized balanced power diagrams (GBPDs) that approximate voxelbased grain structures.


<details>
  <summary>Details</summary>
Motivation: comparative analysis of algorithmic strategies for fitting tessellation models to 3D image data of materials such as polycrystals and foams

Method: optimization-based methods including linear and nonlinear programming, stochastic optimization via the cross-entropy method, and gradient descent

Result: quality of fit is evaluated on real-world datasets using discrepancy measures that quantify differences in grain volume, surface area, and topology

Conclusion: This paper highlights trade-offs between model complexity, the complexity of the optimization routines involved, and the quality of approximation, providing guidance for selecting appropriate methods based on data characteristics and application needs.

Abstract: This paper presents a comparative analysis of algorithmic strategies for
fitting tessellation models to 3D image data of materials such as polycrystals
and foams. In this steadily advancing field, we review and assess
optimization-based methods -- including linear and nonlinear programming,
stochastic optimization via the cross-entropy method, and gradient descent --
for generating Voronoi, Laguerre, and generalized balanced power diagrams
(GBPDs) that approximate voxelbased grain structures. The quality of fit is
evaluated on real-world datasets using discrepancy measures that quantify
differences in grain volume, surface area, and topology. Our results highlight
trade-offs between model complexity, the complexity of the optimization
routines involved, and the quality of approximation, providing guidance for
selecting appropriate methods based on data characteristics and application
needs.

</details>


### [48] [Semantic Segmentation based Scene Understanding in Autonomous Vehicles](https://arxiv.org/abs/2507.14303)
*Ehsan Rassekh*

Main category: cs.CV

TL;DR: This paper proposes efficient models with different backbones for semantic segmentation using the BDD100k dataset, showing that the choice of backbone greatly affects performance, ultimately improving scene understanding for self-driving cars.


<details>
  <summary>Details</summary>
Motivation: Using deep learning in self-driving cars for scene understanding through semantic segmentation is effective and important.

Method: Several efficient models and different backbones are used for semantic segmentation. The BDD100k dataset is used for investigation.

Result: Choosing the appropriate backbone has a great effect on the performance of the model for semantic segmentation.

Conclusion: The proposed models improve accuracy, mean IoU, and loss function in semantic segmentation.

Abstract: In recent years, the concept of artificial intelligence (AI) has become a
prominent keyword because it is promising in solving complex tasks. The need
for human expertise in specific areas may no longer be needed because machines
have achieved successful results using artificial intelligence and can make the
right decisions in critical situations. This process is possible with the help
of deep learning (DL), one of the most popular artificial intelligence
technologies. One of the areas in which the use of DL is used is in the
development of self-driving cars, which is very effective and important. In
this work, we propose several efficient models to investigate scene
understanding through semantic segmentation. We use the BDD100k dataset to
investigate these models. Another contribution of this work is the usage of
several Backbones as encoders for models. The obtained results show that
choosing the appropriate backbone has a great effect on the performance of the
model for semantic segmentation. Better performance in semantic segmentation
allows us to understand better the scene and the environment around the agent.
In the end, we analyze and evaluate the proposed models in terms of accuracy,
mean IoU, and loss function, and the results show that these metrics are
improved.

</details>


### [49] [CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation](https://arxiv.org/abs/2507.14312)
*Marc Lafon,Gustavo Adolfo Vargas Hakim,Clément Rambour,Christian Desrosier,Nicolas Thome*

Main category: cs.CV

TL;DR: CLIPTTA是一种新的TTA方法，它利用与CLIP的预训练目标对齐的软对比损失，并在各种分布偏移的数据集上优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 像CLIP这样的视觉语言模型（VLM）表现出强大的零样本能力，但通常在分布偏移下无法泛化。测试时自适应（TTA）允许模型在推理时更新，而无需标记数据，通常通过熵最小化。然而，这个目标从根本上与VLM的对比图像-文本训练不一致，限制了自适应性能，并引入了诸如伪标签漂移和类崩溃等失败模式。

Method: CLIPTTA，一种新的基于梯度的视觉语言模型TTA方法，它利用与CLIP的预训练目标对齐的软对比损失。使用异常对比暴露（OCE）损失将CLIPTTA扩展到开放集设置，以提高OOD检测。

Result: CLIPTTA始终优于基于熵的目标，并且与最先进的TTA方法相比具有很强的竞争力，在大量数据集上优于它们，并且在不同的偏移中表现出更稳定的性能。

Conclusion: CLIPTTA在各种分布偏移的数据集上优于基于熵的目标，并且与最先进的TTA方法相比具有竞争力，在大量数据集上优于它们，并且在不同的偏移中表现出更稳定的性能。

Abstract: Vision-language models (VLMs) like CLIP exhibit strong zero-shot capabilities
but often fail to generalize under distribution shifts. Test-time adaptation
(TTA) allows models to update at inference time without labeled data, typically
via entropy minimization. However, this objective is fundamentally misaligned
with the contrastive image-text training of VLMs, limiting adaptation
performance and introducing failure modes such as pseudo-label drift and class
collapse. We propose CLIPTTA, a new gradient-based TTA method for
vision-language models that leverages a soft contrastive loss aligned with
CLIP's pre-training objective. We provide a theoretical analysis of CLIPTTA's
gradients, showing how its batch-aware design mitigates the risk of collapse.
We further extend CLIPTTA to the open-set setting, where both in-distribution
(ID) and out-of-distribution (OOD) samples are encountered, using an Outlier
Contrastive Exposure (OCE) loss to improve OOD detection. Evaluated on 75
datasets spanning diverse distribution shifts, CLIPTTA consistently outperforms
entropy-based objectives and is highly competitive with state-of-the-art TTA
methods, outperforming them on a large number of datasets and exhibiting more
stable performance across diverse shifts.

</details>


### [50] [A Hidden Stumbling Block in Generalized Category Discovery: Distracted Attention](https://arxiv.org/abs/2507.14315)
*Qiyu Xu,Zhanxuan Hu,Yu Duan,Ercheng Pei,Yonghang Tai*

Main category: cs.CV

TL;DR: GCD models are distracted by irrelevant background. AF sharpens focus by pruning non-informative tokens, improving performance.


<details>
  <summary>Details</summary>
Motivation: Existing GCD methods suffer from distracted attention, focusing on irrelevant background regions.

Method: Attention Focusing (AF), featuring Token Importance Measurement (TIME) and Token Adaptive Pruning (TAP).

Result: AF achieves up to 15.4% performance improvement over SimGCD with minimal overhead.

Conclusion: Attention Focusing (AF) improves GCD performance by 15.4% with minimal overhead.

Abstract: Generalized Category Discovery (GCD) aims to classify unlabeled data from
both known and unknown categories by leveraging knowledge from labeled known
categories. While existing methods have made notable progress, they often
overlook a hidden stumbling block in GCD: distracted attention. Specifically,
when processing unlabeled data, models tend to focus not only on key objects in
the image but also on task-irrelevant background regions, leading to suboptimal
feature extraction. To remove this stumbling block, we propose Attention
Focusing (AF), an adaptive mechanism designed to sharpen the model's focus by
pruning non-informative tokens. AF consists of two simple yet effective
components: Token Importance Measurement (TIME) and Token Adaptive Pruning
(TAP), working in a cascade. TIME quantifies token importance across multiple
scales, while TAP prunes non-informative tokens by utilizing the multi-scale
importance scores provided by TIME. AF is a lightweight, plug-and-play module
that integrates seamlessly into existing GCD methods with minimal computational
overhead. When incorporated into one prominent GCD method, SimGCD, AF achieves
up to 15.4% performance improvement over the baseline with minimal
computational overhead. The implementation code is provided in
https://github.com/Afleve/AFGCD.

</details>


### [51] [Hallucination Score: Towards Mitigating Hallucinations in Generative Image Super-Resolution](https://arxiv.org/abs/2507.14367)
*Weiming Ren,Raghav Goyal,Zhiming Hu,Tristan Ty Aumentado-Armstrong,Iqbal Mohomed,Alex Levinshtein*

Main category: cs.CV

TL;DR: This paper addresses the problem of hallucinations in generative super-resolution (GSR) models. It introduces a Hallucination Score (HS) based on a multimodal large language model (MLLM) and proposes to align GSR models using deep features to mitigate hallucinations.


<details>
  <summary>Details</summary>
Motivation: Generative super-resolution (GSR) models, while achieving high perceptual image quality, suffer from 'hallucinations' where generated details fail to perceptually match the low resolution image (LRI) or ground-truth image (GTI). These artifacts are not well-characterized with existing image metrics.

Method: The authors construct a prompt that assesses hallucinatory visual elements and generates a Hallucination Score (HS) using a multimodal large language model (MLLM). They also find certain deep feature distances have strong correlations with HS and use such features as differentiable reward functions.

Result: The authors find that their Hallucination Score (HS) is closely aligned with human evaluations and provides complementary insights to prior image metrics. They also find certain deep feature distances have strong correlations with HS.

Conclusion: This paper proposes to align GSR models by using deep features as differentiable reward functions to mitigate hallucinations.

Abstract: Generative super-resolution (GSR) currently sets the state-of-the-art in
terms of perceptual image quality, overcoming the "regression-to-the-mean" blur
of prior non-generative models. However, from a human perspective, such models
do not fully conform to the optimal balance between quality and fidelity.
Instead, a different class of artifacts, in which generated details fail to
perceptually match the low resolution image (LRI) or ground-truth image (GTI),
is a critical but under studied issue in GSR, limiting its practical
deployments. In this work, we focus on measuring, analyzing, and mitigating
these artifacts (i.e., "hallucinations"). We observe that hallucinations are
not well-characterized with existing image metrics or quality models, as they
are orthogonal to both exact fidelity and no-reference quality. Instead, we
take advantage of a multimodal large language model (MLLM) by constructing a
prompt that assesses hallucinatory visual elements and generates a
"Hallucination Score" (HS). We find that our HS is closely aligned with human
evaluations, and also provides complementary insights to prior image metrics
used for super-resolution (SR) models. In addition, we find certain deep
feature distances have strong correlations with HS. We therefore propose to
align the GSR models by using such features as differentiable reward functions
to mitigate hallucinations.

</details>


### [52] [DUSTrack: Semi-automated point tracking in ultrasound videos](https://arxiv.org/abs/2507.14368)
*Praneeth Namburi,Roger Pallarès-López,Jessica Rosendorf,Duarte Folgado,Brian W. Anthony*

Main category: cs.CV

TL;DR: DUSTrack是一个半自动框架，用于跟踪B超视频中的任意点，它结合了深度学习和光流技术，具有高精度和鲁棒性，并通过三个用例展示了其通用性。


<details>
  <summary>Details</summary>
Motivation: 由于散斑噪声、低边缘对比度和平面外运动，准确跟踪B超中的组织运动仍然具有挑战性。这些挑战使得跟踪随时间变化的解剖标志变得复杂，这对于量化许多临床和研究应用中的组织动力学至关重要。

Method: 结合深度学习和光流技术，并采用一种新颖的基于光流的滤波技术，减少高频帧间噪声，同时保留快速组织运动。

Result: DUSTrack在准确性方面优于当代零样本点跟踪器，并且与专门方法相比表现相当，证明了其作为临床和生物力学研究的通用和基础工具的潜力。通过三个用例展示了DUSTrack的通用性：超声心动图中的心壁运动跟踪、伸手任务期间的肌肉变形分析和踝关节跖屈期间的肌束跟踪。

Conclusion: DUSTrack作为一个开源解决方案，为从超声视频中量化组织运动提供了一个强大而灵活的框架，并已在https://github.com/praneethnamburi/DUSTrack上发布。

Abstract: Ultrasound technology enables safe, non-invasive imaging of dynamic tissue
behavior, making it a valuable tool in medicine, biomechanics, and sports
science. However, accurately tracking tissue motion in B-mode ultrasound
remains challenging due to speckle noise, low edge contrast, and out-of-plane
movement. These challenges complicate the task of tracking anatomical landmarks
over time, which is essential for quantifying tissue dynamics in many clinical
and research applications. This manuscript introduces DUSTrack (Deep learning
and optical flow-based toolkit for UltraSound Tracking), a semi-automated
framework for tracking arbitrary points in B-mode ultrasound videos. We combine
deep learning with optical flow to deliver high-quality and robust tracking
across diverse anatomical structures and motion patterns. The toolkit includes
a graphical user interface that streamlines the generation of high-quality
training data and supports iterative model refinement. It also implements a
novel optical-flow-based filtering technique that reduces high-frequency
frame-to-frame noise while preserving rapid tissue motion. DUSTrack
demonstrates superior accuracy compared to contemporary zero-shot point
trackers and performs on par with specialized methods, establishing its
potential as a general and foundational tool for clinical and biomechanical
research. We demonstrate DUSTrack's versatility through three use cases:
cardiac wall motion tracking in echocardiograms, muscle deformation analysis
during reaching tasks, and fascicle tracking during ankle plantarflexion. As an
open-source solution, DUSTrack offers a powerful, flexible framework for point
tracking to quantify tissue motion from ultrasound videos. DUSTrack is
available at https://github.com/praneethnamburi/DUSTrack.

</details>


### [53] [CRAFT: A Neuro-Symbolic Framework for Visual Functional Affordance Grounding](https://arxiv.org/abs/2507.14426)
*Zhou Chen,Joe Lin,Sathyanarayanan N. Aakur*

Main category: cs.CV

TL;DR: CRAFT是一种神经符号框架，它结合常识和视觉信息来识别场景中支持特定动作的对象，从而提高准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 本文介绍了CRAFT，这是一种用于可解释的承载关系接地的神经符号框架，它可以识别场景中能够实现给定动作（例如，“切割”）的对象。

Method: CRAFT整合了来自ConceptNet和语言模型的结构化常识先验知识与来自CLIP的视觉证据，使用基于能量的推理循环来迭代地改进预测。

Result: 在多对象、无标签设置下的实验表明，CRAFT提高了准确性，同时提高了可解释性。

Conclusion: CRAFT通过整合常识先验知识和视觉证据，增强了准确性和可解释性，为鲁棒和可信的场景理解提供了一个步骤。

Abstract: We introduce CRAFT, a neuro-symbolic framework for interpretable affordance
grounding, which identifies the objects in a scene that enable a given action
(e.g., "cut"). CRAFT integrates structured commonsense priors from ConceptNet
and language models with visual evidence from CLIP, using an energy-based
reasoning loop to refine predictions iteratively. This process yields
transparent, goal-driven decisions to ground symbolic and perceptual
structures. Experiments in multi-object, label-free settings demonstrate that
CRAFT enhances accuracy while improving interpretability, providing a step
toward robust and trustworthy scene understanding.

</details>


### [54] [Adaptive 3D Gaussian Splatting Video Streaming](https://arxiv.org/abs/2507.14432)
*Han Gong,Qiyue Li,Zhi Liu,Hao Zhou,Peng Yuan Zhou,Zhu Li,Jie Li*

Main category: cs.CV

TL;DR: This paper introduces a 3DGS video streaming framework using Gaussian deformation fields, hybrid saliency tiling, and differentiated quality modeling to improve video quality, compression, and transmission rate compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian splatting (3DGS) enhances volumetric video quality, but its large data volume and compression complexity pose streaming challenges compared to conventional volumetric video.

Method: A 3DGS video construction method based on Gaussian deformation field is designed, employing hybrid saliency tiling and differentiated quality modeling.

Result: The proposed method demonstrates superiority over existing approaches in video quality, compression effectiveness, and transmission rate.

Conclusion: The proposed 3DGS video streaming framework outperforms existing methods in video quality, compression, and transmission rate.

Abstract: The advent of 3D Gaussian splatting (3DGS) has significantly enhanced the
quality of volumetric video representation. Meanwhile, in contrast to
conventional volumetric video, 3DGS video poses significant challenges for
streaming due to its substantially larger data volume and the heightened
complexity involved in compression and transmission. To address these issues,
we introduce an innovative framework for 3DGS volumetric video streaming.
Specifically, we design a 3DGS video construction method based on the Gaussian
deformation field. By employing hybrid saliency tiling and differentiated
quality modeling of 3DGS video, we achieve efficient data compression and
adaptation to bandwidth fluctuations while ensuring high transmission quality.
Then we build a complete 3DGS video streaming system and validate the
transmission performance. Through experimental evaluation, our method
demonstrated superiority over existing approaches in various aspects, including
video quality, compression effectiveness, and transmission rate.

</details>


### [55] [IRGPT: Understanding Real-world Infrared Image with Bi-cross-modal Curriculum on Large-scale Benchmark](https://arxiv.org/abs/2507.14449)
*Zhe Cao,Jin Zhang,Ruiheng Zhang*

Main category: cs.CV

TL;DR: IRGPT是首个用于真实红外图像的多模态大型语言模型，它优于现有模型，因为它是在一个包含超过26万个真实图像-文本对的大规模红外-文本数据集上训练的。


<details>
  <summary>Details</summary>
Motivation: 现实世界的红外图像为视觉语言模型带来了独特的挑战，因为对齐的文本数据稀缺且具有特定领域的特征。现有方法依赖于从可见图像通过风格迁移生成的合成红外图像，这限制了它们捕捉红外模态独特特征的能力。

Method: 提出了IRGPT，首个用于真实红外图像的多模态大型语言模型，它基于一个包含超过26万个真实图像-文本对的大规模红外-文本数据集（IR-TD）。

Result: IRGPT在九项任务的基准测试中取得了最先进的性能。

Conclusion: IRGPT在红外图像相关的九项任务中取得了最先进的性能，甚至优于更大规模的模型。

Abstract: Real-world infrared imagery presents unique challenges for vision-language
models due to the scarcity of aligned text data and domain-specific
characteristics. Although existing methods have advanced the field, their
reliance on synthetic infrared images generated through style transfer from
visible images, which limits their ability to capture the unique
characteristics of the infrared modality. To address this, we propose IRGPT,
the first multi-modal large language model for real-world infrared images,
built upon a large-scale InfraRed-Text Dataset (IR-TD) comprising over 260K
authentic image-text pairs. The proposed IR-TD dataset contains real infrared
images paired with meticulously handcrafted texts, where the initial drafts
originated from two complementary processes: (1) LLM-generated descriptions of
visible images, and (2) rule-based descriptions of annotations. Furthermore, we
introduce a bi-cross-modal curriculum transfer learning strategy that
systematically transfers knowledge from visible to infrared domains by
considering the difficulty scores of both infrared-visible and infrared-text.
Evaluated on a benchmark of 9 tasks (e.g., recognition, grounding), IRGPT
achieves state-of-the-art performance even compared with larger-scale models.

</details>


### [56] [GPI-Net: Gestalt-Guided Parallel Interaction Network via Orthogonal Geometric Consistency for Robust Point Cloud Registration](https://arxiv.org/abs/2507.14452)
*Weikang Gu,Mingyue Han,Li Xue,Heng Dong,Changcai Yang,Riqing Chen,Lifang Wei*

Main category: cs.CV

TL;DR: 提出了一种新的格式塔引导并行交互网络(GPI-Net)，用于点云配准，该网络利用格式塔原则促进局部和全局信息之间的互补通信。


<details>
  <summary>Details</summary>
Motivation: 精确识别高质量的对应关系是基于特征的点云配准中的一项先决任务。然而，由于特征冗余和复杂的空间关系，处理局部和全局特征的融合极具挑战性。

Method: 本文提出了一种新的基于格式塔引导的并行交互网络，通过正交几何一致性(GPI-Net)。

Result: 与现有方法相比，我们提出的GPI-Net具有优越的性能

Conclusion: GPI-Net在各种具有挑战性的任务上进行了大量实验，结果表明，与现有方法相比，该方法具有优越的性能。

Abstract: The accurate identification of high-quality correspondences is a prerequisite
task in feature-based point cloud registration. However, it is extremely
challenging to handle the fusion of local and global features due to feature
redundancy and complex spatial relationships. Given that Gestalt principles
provide key advantages in analyzing local and global relationships, we propose
a novel Gestalt-guided Parallel Interaction Network via orthogonal geometric
consistency (GPI-Net) in this paper. It utilizes Gestalt principles to
facilitate complementary communication between local and global information.
Specifically, we introduce an orthogonal integration strategy to optimally
reduce redundant information and generate a more compact global structure for
high-quality correspondences. To capture geometric features in correspondences,
we leverage a Gestalt Feature Attention (GFA) block through a hybrid
utilization of self-attention and cross-attention mechanisms. Furthermore, to
facilitate the integration of local detail information into the global
structure, we design an innovative Dual-path Multi-Granularity parallel
interaction aggregation (DMG) block to promote information exchange across
different granularities. Extensive experiments on various challenging tasks
demonstrate the superior performance of our proposed GPI-Net in comparison to
existing methods. The code will be released at https://github.com/gwk/GPI-Net.

</details>


### [57] [Adaptive 3D Gaussian Splatting Video Streaming: Visual Saliency-Aware Tiling and Meta-Learning-Based Bitrate Adaptation](https://arxiv.org/abs/2507.14454)
*Han Gong,Qiyue Li,Jie Li,Zhi Liu*

Main category: cs.CV

TL;DR: 本文提出了一种用于3DGS视频流的自适应分块、质量评估和比特率算法。


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射视频（3DGS）流媒体最近成为学术界和工业界的研究热点，因为它具有提供沉浸式3D视频体验的强大能力。然而，该领域的研究仍处于早期阶段，一些基本挑战，如分块、质量评估和比特率自适应，需要进一步研究。

Method: 该论文提出了一种自适应3DGS分块技术，该技术由显着性分析引导，集成了空间和时间特征。每个图块都被编码成具有专用变形场和多个质量级别的版本，以供自适应选择。还引入了一个新的3DGS视频质量评估框架，该框架共同评估了流式传输期间3DGS表示中的空间域退化以及生成的2D渲染图像的质量。此外，还开发了一种基于元学习的自适应比特率算法，专门为3DGS视频流量身定制，可在不同的网络条件下实现最佳性能。

Result: 该论文提出了一套全面的解决方案来应对这些挑战。

Conclusion: 该论文提出的方法显著优于现有方法。

Abstract: 3D Gaussian splatting video (3DGS) streaming has recently emerged as a
research hotspot in both academia and industry, owing to its impressive ability
to deliver immersive 3D video experiences. However, research in this area is
still in its early stages, and several fundamental challenges, such as tiling,
quality assessment, and bitrate adaptation, require further investigation. In
this paper, we tackle these challenges by proposing a comprehensive set of
solutions. Specifically, we propose an adaptive 3DGS tiling technique guided by
saliency analysis, which integrates both spatial and temporal features. Each
tile is encoded into versions possessing dedicated deformation fields and
multiple quality levels for adaptive selection. We also introduce a novel
quality assessment framework for 3DGS video that jointly evaluates
spatial-domain degradation in 3DGS representations during streaming and the
quality of the resulting 2D rendered images. Additionally, we develop a
meta-learning-based adaptive bitrate algorithm specifically tailored for 3DGS
video streaming, achieving optimal performance across varying network
conditions. Extensive experiments demonstrate that our proposed approaches
significantly outperform state-of-the-art methods.

</details>


### [58] [GEMINUS: Dual-aware Global and Scene-Adaptive Mixture-of-Experts for End-to-End Autonomous Driving](https://arxiv.org/abs/2507.14456)
*Chi Wan,Yixin Cui,Jiatong Du,Shuo Yang,Yulong Bai,Yanjun Huang*

Main category: cs.CV

TL;DR: GEMINUS 是一个混合专家自动驾驶框架，通过全局专家和场景自适应专家组相结合，实现了自适应和鲁棒的性能，并在多个指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的单模规划方法试图学习整体策略，但在获取多样化的驾驶技能来处理各种场景时遇到了困难。

Method: 提出了一种混合专家端到端自动驾驶框架 GEMINUS，该框架具有一个全局专家、一个场景自适应专家组，并配备了一个双重感知路由器。

Result: GEMINUS 在驾驶评分方面提高了 7.67%，在成功率方面提高了 22.06%，在 MultiAbility-Mean 方面提高了 19.41%。

Conclusion: GEMINUS 通过全局专家和场景自适应专家组的有效结合，在各种场景中实现了自适应和鲁棒的性能，并在 Bench2Drive 闭环基准测试中优于现有方法，即使仅使用单目视觉输入，也能在驾驶评分和成功率方面实现最先进的性能。

Abstract: End-to-end autonomous driving requires adaptive and robust handling of
complex and diverse traffic environments. However, prevalent single-mode
planning methods attempt to learn an overall policy while struggling to acquire
diversified driving skills to handle diverse scenarios. Therefore, this paper
proposes GEMINUS, a Mixture-of-Experts end-to-end autonomous driving framework
featuring a Global Expert, a Scene-Adaptive Experts Group, and equipped with a
Dual-aware Router. Specifically, the Global Expert is trained on the overall
dataset, possessing robust performance. The Scene-Adaptive Experts are trained
on corresponding scene subsets, achieving adaptive performance. The Dual-aware
Router simultaneously considers scenario-level features and routing uncertainty
to dynamically activate expert modules. Through the effective coupling of the
Global Expert and the Scene-Adaptive Experts Group via the Dual-aware Router,
GEMINUS achieves adaptive and robust performance in diverse scenarios. GEMINUS
outperforms existing methods in the Bench2Drive closed-loop benchmark and
achieves state-of-the-art performance in Driving Score and Success Rate, even
with only monocular vision input. Furthermore, ablation studies demonstrate
significant improvements over the original single-expert baseline: 7.67% in
Driving Score, 22.06% in Success Rate, and 19.41% in MultiAbility-Mean. The
code will be available at https://github.com/newbrains1/GEMINUS.

</details>


### [59] [VisGuard: Securing Visualization Dissemination through Tamper-Resistant Data Retrieval](https://arxiv.org/abs/2507.14459)
*Huayuan Ye,Juntong Chen,Shenzhuo Zhang,Yipeng Zhang,Changbo Wang,Chenhui Li*

Main category: cs.CV

TL;DR: VisGuard是一种防篡改VIDR框架，可将元数据链接可靠地嵌入到可视化图像中，即使在图像被篡改后，嵌入的数据链接仍然可以恢复。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏实用性，因为它们在在线分发期间容易受到常见的图像篡改（如裁剪和编辑）的影响,为了解决这个问题，我们提出了VisGuard，这是一个防篡改的VIDR框架，它可以可靠地将元数据链接嵌入到可视化图像中。即使在对图像进行大量篡改后，嵌入的数据链接仍然可以恢复。

Method: 重复数据平铺，可逆信息广播，以及基于锚点的裁剪定位方案

Result: 嵌入的数据链接即使在对图像进行大量篡改后仍然可以恢复。VisGuard支持各种应用，包括交互式图表重建、篡改检测和版权保护。

Conclusion: VisGuard在数据检索准确性、嵌入容量和抗篡改和隐写分析的安全性方面表现出色，证明了VisGuard在促进和保障可视化传播和信息传递方面的能力。

Abstract: The dissemination of visualizations is primarily in the form of raster
images, which often results in the loss of critical information such as source
code, interactive features, and metadata. While previous methods have proposed
embedding metadata into images to facilitate Visualization Image Data Retrieval
(VIDR), most existing methods lack practicability since they are fragile to
common image tampering during online distribution such as cropping and editing.
To address this issue, we propose VisGuard, a tamper-resistant VIDR framework
that reliably embeds metadata link into visualization images. The embedded data
link remains recoverable even after substantial tampering upon images. We
propose several techniques to enhance robustness, including repetitive data
tiling, invertible information broadcasting, and an anchor-based scheme for
crop localization. VisGuard enables various applications, including interactive
chart reconstruction, tampering detection, and copyright protection. We conduct
comprehensive experiments on VisGuard's superior performance in data retrieval
accuracy, embedding capacity, and security against tampering and steganalysis,
demonstrating VisGuard's competence in facilitating and safeguarding
visualization dissemination and information conveyance.

</details>


### [60] [OptiCorNet: Optimizing Sequence-Based Context Correlation for Visual Place Recognition](https://arxiv.org/abs/2507.14477)
*Zhenyu Li,Tianyi Shang,Pengjie Xu,Ruirui Zhang,Fanchen Kong*

Main category: cs.CV

TL;DR: OptiCorNet 是一种用于视觉位置识别的新型序列建模框架，它优于最先进的基线。


<details>
  <summary>Details</summary>
Motivation: 在动态和感知混叠环境中进行视觉位置识别 (VPR) 仍然是长期定位的一个根本挑战。现有的基于深度学习的解决方案主要关注单帧嵌入，忽略了图像序列中存在的时间连贯性。

Method: 提出了一种新颖的序列建模框架 OptiCorNet，该框架将空间特征提取和时间差分统一到一个可微的端到端可训练模块中。该方法的核心是一个轻量级 1D 卷积编码器，结合一个可学习的微分时间算子，称为可微序列 Delta (DSD)，它共同捕获短期空间上下文和长期时间转换。

Result: 在多个公共基准上的综合评估表明，该方法优于最先进的基线。

Conclusion: 该方法在具有挑战性的季节和视点变化下优于最先进的基线。

Abstract: Visual Place Recognition (VPR) in dynamic and perceptually aliased
environments remains a fundamental challenge for long-term localization.
Existing deep learning-based solutions predominantly focus on single-frame
embeddings, neglecting the temporal coherence present in image sequences. This
paper presents OptiCorNet, a novel sequence modeling framework that unifies
spatial feature extraction and temporal differencing into a differentiable,
end-to-end trainable module. Central to our approach is a lightweight 1D
convolutional encoder combined with a learnable differential temporal operator,
termed Differentiable Sequence Delta (DSD), which jointly captures short-term
spatial context and long-range temporal transitions. The DSD module models
directional differences across sequences via a fixed-weight differencing
kernel, followed by an LSTM-based refinement and optional residual projection,
yielding compact, discriminative descriptors robust to viewpoint and appearance
shifts. To further enhance inter-class separability, we incorporate a
quadruplet loss that optimizes both positive alignment and multi-negative
divergence within each batch. Unlike prior VPR methods that treat temporal
aggregation as post-processing, OptiCorNet learns sequence-level embeddings
directly, enabling more effective end-to-end place recognition. Comprehensive
evaluations on multiple public benchmarks demonstrate that our approach
outperforms state-of-the-art baselines under challenging seasonal and viewpoint
variations.

</details>


### [61] [DFQ-ViT: Data-Free Quantization for Vision Transformers without Fine-tuning](https://arxiv.org/abs/2507.14481)
*Yujia Tong,Jingling Yuan,Tian Zhang,Jianquan Liu,Chuang Hu*

Main category: cs.CV

TL;DR: DFQ-ViT是一种用于ViT的无数据量化方法，它通过改进合成数据质量和校正激活来提高量化模型的性能，且无需微调。


<details>
  <summary>Details</summary>
Motivation: 现有DFQ方法未能充分捕捉和平衡样本中的全局和局部特征，导致合成数据质量受限。量化模型和全精度模型在推理过程中，中间层激活分布存在显著差异，导致量化模型性能严重下降。

Method: 提出DFQ-ViT流程，通过难度递增的方式合成样本以提高合成数据质量，并引入激活校正矩阵以对齐量化模型和全精度模型的中间层激活。

Result: DeiT-T在3比特权重量化下的性能比现有最佳方法高4.29%。

Conclusion: DFQ-ViT在ViT量化上显著优于现有DFQ方法，性能与使用真实数据量化的模型相当，且无需微调，降低了计算开销和部署门槛，符合绿色学习原则。

Abstract: Data-Free Quantization (DFQ) enables the quantization of Vision Transformers
(ViTs) without requiring access to data, allowing for the deployment of ViTs on
devices with limited resources. In DFQ, the quantization model must be
calibrated using synthetic samples, making the quality of these synthetic
samples crucial. Existing methods fail to fully capture and balance the global
and local features within the samples, resulting in limited synthetic data
quality. Moreover, we have found that during inference, there is a significant
difference in the distributions of intermediate layer activations between the
quantized and full-precision models. These issues lead to a severe performance
degradation of the quantized model. To address these problems, we propose a
pipeline for Data-Free Quantization for Vision Transformers (DFQ-ViT).
Specifically, we synthesize samples in order of increasing difficulty,
effectively enhancing the quality of synthetic data. During the calibration and
inference stage, we introduce the activation correction matrix for the
quantized model to align the intermediate layer activations with those of the
full-precision model. Extensive experiments demonstrate that DFQ-ViT achieves
remarkable superiority over existing DFQ methods and its performance is on par
with models quantized through real data. For example, the performance of DeiT-T
with 3-bit weights quantization is 4.29% higher than the state-of-the-art. Our
method eliminates the need for fine-tuning, which not only reduces
computational overhead but also lowers the deployment barriers for edge
devices. This characteristic aligns with the principles of Green Learning by
improving energy efficiency and facilitating real-world applications in
resource-constrained environments.

</details>


### [62] [Benefit from Reference: Retrieval-Augmented Cross-modal Point Cloud Completion](https://arxiv.org/abs/2507.14485)
*Hongye Hou,Liu Zhan,Yang Yang*

Main category: cs.CV

TL;DR: 提出了一种检索增强点云补全框架，通过结合跨模态检索和分层特征融合机制，从参考样本中学习结构先验信息，从而有效地生成细粒度点云并提高泛化能力。


<details>
  <summary>Details</summary>
Motivation: 基于不完整点云完成整个 3D 结构是一项具有挑战性的任务，特别是当残余点云缺乏典型的结构特征时。最近基于跨模态学习的方法试图引入实例图像来帮助结构特征学习。然而，他们仍然专注于每个特定的输入类别，限制了他们的生成能力。

Method: 提出了一种新颖的检索增强点云补全框架，该框架结合了跨模态检索来学习来自类似参考样本的结构先验信息。设计了一个结构共享特征编码器 (SSFE) 来联合提取跨模态特征并重建参考特征作为先验。提出了一个渐进式检索增强生成器 (PRAG)，它采用分层特征融合机制来整合来自全局到局部的参考先验信息与输入特征。

Result: 在多个数据集和真实场景中的大量评估表明，该方法在生成细粒度点云以及处理稀疏数据和未见类别方面显示出有效性。

Conclusion: 该方法在生成细粒度点云以及处理稀疏数据和未见类别方面显示出有效性和泛化能力。

Abstract: Completing the whole 3D structure based on an incomplete point cloud is a
challenging task, particularly when the residual point cloud lacks typical
structural characteristics. Recent methods based on cross-modal learning
attempt to introduce instance images to aid the structure feature learning.
However, they still focus on each particular input class, limiting their
generation abilities. In this work, we propose a novel retrieval-augmented
point cloud completion framework. The core idea is to incorporate cross-modal
retrieval into completion task to learn structural prior information from
similar reference samples. Specifically, we design a Structural Shared Feature
Encoder (SSFE) to jointly extract cross-modal features and reconstruct
reference features as priors. Benefiting from a dual-channel control gate in
the encoder, relevant structural features in the reference sample are enhanced
and irrelevant information interference is suppressed. In addition, we propose
a Progressive Retrieval-Augmented Generator (PRAG) that employs a hierarchical
feature fusion mechanism to integrate reference prior information with input
features from global to local. Through extensive evaluations on multiple
datasets and real-world scenes, our method shows its effectiveness in
generating fine-grained point clouds, as well as its generalization capability
in handling sparse data and unseen categories.

</details>


### [63] [Efficient Whole Slide Pathology VQA via Token Compression](https://arxiv.org/abs/2507.14497)
*Weimin Lyu,Qingqiao Hu,Kehan Qi,Zhan Shi,Wentao Huang,Saumya Gupta,Chao Chen*

Main category: cs.CV

TL;DR: TCP-LLaVA是一种用于WSI VQA的新型MLLM架构，它通过token压缩来减少计算成本，并在VQA准确率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于上下文长度较长和计算需求较高，病理学中的全切片图像(WSI)对多模态大型语言模型(MLLM)提出了重大挑战。先前的方法通常侧重于使用基于CLIP的模型和多实例学习进行patch级别分析或slide级别分类，但它们缺乏视觉问题回答(VQA)所需的生成能力。更近期的基于MLLM的方法通过将数千个patch token直接输入到语言模型中来解决VQA问题，这导致了过度的资源消耗。

Method: 提出了一种名为Token Compression Pathology LLaVA (TCP-LLaVA) 的MLLM架构，该架构通过token压缩执行WSI VQA。TCP-LLaVA引入了一组可训练的压缩token，通过受BERT中[CLS] token机制启发的模态压缩模块聚合视觉和文本信息。只有压缩后的token被转发到LLM用于答案生成。

Result: 在十种TCGA肿瘤亚型上的实验表明，TCP-LLaVA在VQA准确率上优于现有的MLLM基线，同时大幅降低了训练资源消耗。

Conclusion: TCP-LLaVA在VQA准确率上优于现有的MLLM基线，同时大幅降低了训练资源消耗。

Abstract: Whole-slide images (WSIs) in pathology can reach up to 10,000 x 10,000
pixels, posing significant challenges for multimodal large language model
(MLLM) due to long context length and high computational demands. Previous
methods typically focus on patch-level analysis or slide-level classification
using CLIP-based models with multi-instance learning, but they lack the
generative capabilities needed for visual question answering (VQA). More recent
MLLM-based approaches address VQA by feeding thousands of patch tokens directly
into the language model, which leads to excessive resource consumption. To
address these limitations, we propose Token Compression Pathology LLaVA
(TCP-LLaVA), the first MLLM architecture to perform WSI VQA via token
compression. TCP-LLaVA introduces a set of trainable compression tokens that
aggregate visual and textual information through a modality compression module,
inspired by the [CLS] token mechanism in BERT. Only the compressed tokens are
forwarded to the LLM for answer generation, significantly reducing input length
and computational cost. Experiments on ten TCGA tumor subtypes show that
TCP-LLaVA outperforms existing MLLM baselines in VQA accuracy while reducing
training resource consumption by a substantial margin.

</details>


### [64] [Motion Segmentation and Egomotion Estimation from Event-Based Normal Flow](https://arxiv.org/abs/2507.14500)
*Zhiyuan Hua,Dehao Yuan,Cornelia Fermüller*

Main category: cs.CV

TL;DR: A robust framework for motion segmentation and egomotion estimation using event-based normal flow is introduced, which is tailored for neuromorphic vision sensors and demonstrates advantages at object boundaries.


<details>
  <summary>Details</summary>
Motivation: Traditional methods rely heavily on optical flow or explicit depth estimation, which are not ideal for neuromorphic vision sensors. This paper exploits the sparse, high-temporal-resolution event data and incorporates geometric constraints.

Method: An optimization-based pipeline iteratively performs event over-segmentation, isolates independently moving objects via residual analysis, and refines segmentations using hierarchical clustering.

Result: The method achieves accurate segmentation and translational motion estimation on the EVIMO2v2 dataset.

Conclusion: The method achieves accurate segmentation and translational motion estimation without full optical flow computation, with advantages at object boundaries and potential for scalable, real-time applications.

Abstract: This paper introduces a robust framework for motion segmentation and
egomotion estimation using event-based normal flow, tailored specifically for
neuromorphic vision sensors. In contrast to traditional methods that rely
heavily on optical flow or explicit depth estimation, our approach exploits the
sparse, high-temporal-resolution event data and incorporates geometric
constraints between normal flow, scene structure, and inertial measurements.
The proposed optimization-based pipeline iteratively performs event
over-segmentation, isolates independently moving objects via residual analysis,
and refines segmentations using hierarchical clustering informed by motion
similarity and temporal consistency. Experimental results on the EVIMO2v2
dataset validate that our method achieves accurate segmentation and
translational motion estimation without requiring full optical flow
computation. This approach demonstrates significant advantages at object
boundaries and offers considerable potential for scalable, real-time robotic
and navigation applications.

</details>


### [65] [Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey](https://arxiv.org/abs/2507.14501)
*Jiahui Zhang,Yuelei Li,Anpei Chen,Muyu Xu,Kunhao Liu,Jianyuan Wang,Xiao-Xiao Long,Hanxue Liang,Zexiang Xu,Hao Su,Christian Theobalt,Christian Rupprecht,Andrea Vedaldi,Hanspeter Pfister,Shijian Lu,Fangneng Zhan*

Main category: cs.CV

TL;DR: 本文全面回顾了用于 3D 重建和视图合成的前馈技术，重点介绍了其在各种应用中的优势，并讨论了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 3D 重建和视图合成是计算机视觉、图形和增强现实 (AR)、虚拟现实 (VR) 和数字孪生等沉浸式技术中的基础问题。传统方法依赖于复杂链中计算密集型迭代优化，限制了它们在现实世界场景中的适用性。

Method: 对用于 3D 重建和视图合成的前馈技术进行了全面的回顾，并根据包括点云、3D 高斯溅射 (3DGS)、神经辐射场 (NeRF) 等在内的底层表示架构进行了分类。

Result: 检查了诸如无姿势重建、动态 3D 重建和 3D 感知图像和视频合成等关键任务，突出了它们在数字人、SLAM、机器人等领域的应用。此外，我们还回顾了常用的数据集以及各种下游任务的详细统计数据和评估协议。

Conclusion: 总结了开放的研究挑战和未来有希望的方向，强调了前馈方法在推进 3D 视觉技术方面的潜力。

Abstract: 3D reconstruction and view synthesis are foundational problems in computer
vision, graphics, and immersive technologies such as augmented reality (AR),
virtual reality (VR), and digital twins. Traditional methods rely on
computationally intensive iterative optimization in a complex chain, limiting
their applicability in real-world scenarios. Recent advances in feed-forward
approaches, driven by deep learning, have revolutionized this field by enabling
fast and generalizable 3D reconstruction and view synthesis. This survey offers
a comprehensive review of feed-forward techniques for 3D reconstruction and
view synthesis, with a taxonomy according to the underlying representation
architectures including point cloud, 3D Gaussian Splatting (3DGS), Neural
Radiance Fields (NeRF), etc. We examine key tasks such as pose-free
reconstruction, dynamic 3D reconstruction, and 3D-aware image and video
synthesis, highlighting their applications in digital humans, SLAM, robotics,
and beyond. In addition, we review commonly used datasets with detailed
statistics, along with evaluation protocols for various downstream tasks. We
conclude by discussing open research challenges and promising directions for
future work, emphasizing the potential of feed-forward approaches to advance
the state of the art in 3D vision.

</details>


### [66] [DCHM: Depth-Consistent Human Modeling for Multiview Detection](https://arxiv.org/abs/2507.14505)
*Jiahao Ma,Tianyu Wang,Miaomiao Liu,David Ahmedt-Aristizabal,Chuong Nguyen*

Main category: cs.CV

TL;DR: DCHM improves multiview pedestrian detection by using depth-consistent human modeling with Gaussian Splatting, reducing noise and outperforming existing methods without relying on human-labeled annotations.


<details>
  <summary>Details</summary>
Motivation: Existing multiview pedestrian detection methods introduce noise and have low precision in human modeling. Some approaches rely on costly multiview 3D annotations and struggle to generalize across diverse scenes.

Method: The paper proposes Depth-Consistent Human Modeling (DCHM), a framework with superpixel-wise Gaussian Splatting for consistent depth estimation and multiview fusion in global coordinates.

Result: DCHM achieves multiview depth consistency in sparse-view, large-scaled, and crowded scenarios, producing precise point clouds for pedestrian localization.

Conclusion: The proposed DCHM method significantly reduces noise during human modeling and outperforms previous state-of-the-art baselines. DCHM is the first to reconstruct pedestrians and perform multiview segmentation in challenging settings.

Abstract: Multiview pedestrian detection typically involves two stages: human modeling
and pedestrian localization. Human modeling represents pedestrians in 3D space
by fusing multiview information, making its quality crucial for detection
accuracy. However, existing methods often introduce noise and have low
precision. While some approaches reduce noise by fitting on costly multiview 3D
annotations, they often struggle to generalize across diverse scenes. To
eliminate reliance on human-labeled annotations and accurately model humans, we
propose Depth-Consistent Human Modeling (DCHM), a framework designed for
consistent depth estimation and multiview fusion in global coordinates.
Specifically, our proposed pipeline with superpixel-wise Gaussian Splatting
achieves multiview depth consistency in sparse-view, large-scaled, and crowded
scenarios, producing precise point clouds for pedestrian localization.
Extensive validations demonstrate that our method significantly reduces noise
during human modeling, outperforming previous state-of-the-art baselines.
Additionally, to our knowledge, DCHM is the first to reconstruct pedestrians
and perform multiview segmentation in such a challenging setting. Code is
available on the \href{https://jiahao-ma.github.io/DCHM/}{project page}.

</details>


### [67] [ArtiMuse: Fine-Grained Image Aesthetics Assessment with Joint Scoring and Expert-Level Understanding](https://arxiv.org/abs/2507.14533)
*Shuo Cao,Nan Ma,Jiayang Li,Xiaohui Li,Lihao Shao,Kaiwen Zhu,Yu Zhou,Yuandong Pu,Jiarui Wu,Jiaquan Wang,Bo Qu,Wenhai Wang,Yu Qiao,Dajuin Yao,Yihao Liu*

Main category: cs.CV

TL;DR: 本文提出了ArtiMuse模型和ArtiMuse-10K数据集，以解决现有图像美学评估方法的不足。


<details>
  <summary>Details</summary>
Motivation: 教育应用、艺术创作和AI生成内容(AIGC)技术的快速发展，大大增加了对综合图像美学评估(IAA)的实际需求，特别是需要能够提供定量评分和专业理解的方法。基于多模态大型语言模型(MLLM)的IAA方法比传统方法表现出更强的感知和泛化能力，但它们存在模态偏差(仅评分或仅文本)，缺乏细粒度的属性分解，从而无法支持进一步的美学评估。

Method: 创新性的基于MLLM的IAA模型，具有联合评分和专家级理解能力。

Result: 提出了ArtiMuse，一个创新的基于MLLM的IAA模型，具有联合评分和专家级理解能力;构建了ArtiMuse-10K，这是第一个专家策划的图像美学数据集，包含10000张图像，跨越5个主要类别和15个子类别，每个图像都由专业专家注释，包括8维属性分析和一个整体分数。

Conclusion: ArtiMuse模型和ArtiMuse-10K数据集将被公开，以推进行业发展。

Abstract: The rapid advancement of educational applications, artistic creation, and
AI-generated content (AIGC) technologies has substantially increased practical
requirements for comprehensive Image Aesthetics Assessment (IAA), particularly
demanding methods capable of delivering both quantitative scoring and
professional understanding. Multimodal Large Language Model (MLLM)-based IAA
methods demonstrate stronger perceptual and generalization capabilities
compared to traditional approaches, yet they suffer from modality bias
(score-only or text-only) and lack fine-grained attribute decomposition,
thereby failing to support further aesthetic assessment. In this paper, we
present:(1) ArtiMuse, an innovative MLLM-based IAA model with Joint Scoring and
Expert-Level Understanding capabilities; (2) ArtiMuse-10K, the first
expert-curated image aesthetic dataset comprising 10,000 images spanning 5 main
categories and 15 subcategories, each annotated by professional experts with
8-dimensional attributes analysis and a holistic score. Both the model and
dataset will be made public to advance the field.

</details>


### [68] [Real Time Captioning of Sign Language Gestures in Video Meetings](https://arxiv.org/abs/2507.14543)
*Sharanya Mukherjee,Md Hishaam Akhtar,Kannadasan R*

Main category: cs.CV

TL;DR: This paper proposes a browser extension for real-time sign language translation to subtitles in video calls, using a large ASL video dataset.


<details>
  <summary>Details</summary>
Motivation: Communication with hearing-impaired individuals is challenging, and sign language recognition using computer vision can eliminate communication barriers. Video meetings have become essential, and hearing-disabled individuals prefer signing over typing during these calls.

Method: Using a Large-scale dataset which contains more than 2000 Word-Level ASL videos, which were performed by over 100 signers.

Result: N/A

Conclusion: Proposing a browser extension that will automatically translate sign language to subtitles for everyone else in the video call.

Abstract: It has always been a rather tough task to communicate with someone possessing
a hearing impairment. One of the most tested ways to establish such a
communication is through the use of sign based languages. However, not many
people are aware of the smaller intricacies involved with sign language. Sign
language recognition using computer vision aims at eliminating the
communication barrier between deaf-mute and ordinary people so that they can
properly communicate with others. Recently the pandemic has left the whole
world shaken up and has transformed the way we communicate. Video meetings have
become essential for everyone, even people with a hearing disability. In recent
studies, it has been found that people with hearing disabilities prefer to sign
over typing during these video calls. In this paper, we are proposing a browser
extension that will automatically translate sign language to subtitles for
everyone else in the video call. The Large-scale dataset which contains more
than 2000 Word-Level ASL videos, which were performed by over 100 signers will
be used.

</details>


### [69] [Multimodal AI for Gastrointestinal Diagnostics: Tackling VQA in MEDVQA-GI 2025](https://arxiv.org/abs/2507.14544)
*Sujata Gaihre,Amir Thapa Magar,Prasuna Pokharel,Laxmi Tiwari*

Main category: cs.CV

TL;DR: This paper uses the Florence model for visual question answering on gastrointestinal endoscopy images, achieving accurate results by fine-tuning and domain-specific augmentations.


<details>
  <summary>Details</summary>
Motivation: This paper addresses visual question answering (VQA) for gastrointestinal endoscopy in the ImageCLEFmed MEDVQA 2025 Challenge.

Method: The Florence model is adopted as the backbone of VQA pipeline, pairing a powerful vision encoder with a text encoder. Domain-specific augmentations are applied to improve generalization.

Result: Experiments on the KASVIR dataset show that fine-tuning Florence yields accurate responses on the official challenge metrics.

Conclusion: Fine-tuning Florence yields accurate responses on the official challenge metrics, highlighting the potential of large multimodal models in medical VQA and providing a strong baseline for future work.

Abstract: This paper describes our approach to Subtask 1 of the ImageCLEFmed MEDVQA
2025 Challenge, which targets visual question answering (VQA) for
gastrointestinal endoscopy. We adopt the Florence model-a large-scale
multimodal foundation model-as the backbone of our VQA pipeline, pairing a
powerful vision encoder with a text encoder to interpret endoscopic images and
produce clinically relevant answers. To improve generalization, we apply
domain-specific augmentations that preserve medical features while increasing
training diversity. Experiments on the KASVIR dataset show that fine-tuning
Florence yields accurate responses on the official challenge metrics. Our
results highlight the potential of large multimodal models in medical VQA and
provide a strong baseline for future work on explainability, robustness, and
clinical integration. The code is publicly available at:
https://github.com/TiwariLaxuu/VQA-Florence.git

</details>


### [70] [Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering Human Perceptual Variability on Facial Expressions](https://arxiv.org/abs/2507.14549)
*Haotian Deng,Chi Zhang,Chen Wei,Quanying Liu*

Main category: cs.CV

TL;DR: 该研究探讨了人工神经网络 (ANN) 如何模拟人类在情绪感知上的个体差异，发现 ANN 难以识别的面部表情也会引起人类感知上的不确定性，并通过调整 ANN 使其与人类感知模式对齐，为情绪解读的个性化建模提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 情感认知科学中的一个根本挑战是开发能够准确捕捉外部情绪刺激与人类内在体验之间关系的模型。虽然 ANN 在面部表情识别方面表现出了显著的准确性，但它们在模拟人类感知中的个体差异方面的能力仍有待探索。

Method: 引入了一种新的感知边界抽样方法，以生成位于 ANN 决策边界上的面部表情刺激。这些模棱两可的样本构成了 varEmotion 数据集的基础，该数据集是通过大规模人类行为实验构建的。

Result: 研究表明，这些使 ANN 感到困惑的刺激也会引发人类参与者的高度感知不确定性，突出了情绪感知中共享的计算原则。研究揭示了 ANN 决策边界与人类感知变异性之间的系统联系。

Conclusion: 通过使用行为数据微调 ANN 表征，实现了 ANN 预测与群体和个体层面的人类感知模式的对齐。研究结果建立了 ANN 决策边界与人类感知变异性之间的系统联系，为情绪解读的个性化建模提供了新的见解。

Abstract: A fundamental challenge in affective cognitive science is to develop models
that accurately capture the relationship between external emotional stimuli and
human internal experiences. While ANNs have demonstrated remarkable accuracy in
facial expression recognition, their ability to model inter-individual
differences in human perception remains underexplored. This study investigates
the phenomenon of high perceptual variability-where individuals exhibit
significant differences in emotion categorization even when viewing the same
stimulus. Inspired by the similarity between ANNs and human perception, we
hypothesize that facial expression samples that are ambiguous for ANN
classifiers also elicit divergent perceptual judgments among human observers.
To examine this hypothesis, we introduce a novel perceptual boundary sampling
method to generate facial expression stimuli that lie along ANN decision
boundaries. These ambiguous samples form the basis of the varEmotion dataset,
constructed through large-scale human behavioral experiments. Our analysis
reveals that these ANN-confusing stimuli also provoke heightened perceptual
uncertainty in human participants, highlighting shared computational principles
in emotion perception. Finally, by fine-tuning ANN representations using
behavioral data, we achieve alignment between ANN predictions and both
group-level and individual-level human perceptual patterns. Our findings
establish a systematic link between ANN decision boundaries and human
perceptual variability, offering new insights into personalized modeling of
emotional interpretation.

</details>


### [71] [Clutter Detection and Removal by Multi-Objective Analysis for Photographic Guidance](https://arxiv.org/abs/2507.14553)
*Xiaoran Wu*

Main category: cs.CV

TL;DR: 该论文提出了一种相机指导系统，可以识别和移除照片中的杂物，从而帮助用户拍摄更高质量的照片。


<details>
  <summary>Details</summary>
Motivation: 摄影爱好者由于无意识的疏忽或缺乏创造整洁、美观的拍摄场景的经验，经常在照片中包含杂物。因此，我们有动力开发一种相机指导系统，为杂物识别和移除提供解决方案和指导。

Method: 一种具有对象美学评估的杂乱区分算法，以及一种基于生成对抗网络的迭代图像修复算法。

Result: 该系统可以估计和可视化对象对照片整体美学和内容的贡献，用户可以交互式地识别杂物。提供了关于去除杂物的建议，以及一种通过计算去除杂乱对象，重建高分辨率图像中缺失区域的工具。

Conclusion: 该系统通过灵活的界面和准确的算法，使用户能够在更短的时间内更好地识别干扰因素并拍摄更高质量的照片。

Abstract: Clutter in photos is a distraction preventing photographers from conveying
the intended emotions or stories to the audience. Photography amateurs
frequently include clutter in their photos due to unconscious negligence or the
lack of experience in creating a decluttered, aesthetically appealing scene for
shooting. We are thus motivated to develop a camera guidance system that
provides solutions and guidance for clutter identification and removal. We
estimate and visualize the contribution of objects to the overall aesthetics
and content of a photo, based on which users can interactively identify
clutter. Suggestions on getting rid of clutter, as well as a tool that removes
cluttered objects computationally, are provided to guide users to deal with
different kinds of clutter and improve their photographic work. Two technical
novelties underpin interactions in our system: a clutter distinguishment
algorithm with aesthetics evaluations for objects and an iterative image
inpainting algorithm based on generative adversarial nets that reconstructs
missing regions of removed objects for high-resolution images. User studies
demonstrate that our system provides flexible interfaces and accurate
algorithms that allow users to better identify distractions and take higher
quality images within less time.

</details>


### [72] [Descrip3D: Enhancing Large Language Model-based 3D Scene Understanding with Object-Level Text Descriptions](https://arxiv.org/abs/2507.14555)
*Jintang Xue,Ganning Zhao,Jie-En Yao,Hong-En Chen,Yue Hu,Meida Chen,Suya You,C. -C. Jay Kuo*

Main category: cs.CV

TL;DR: Descrip3D使用自然语言显式编码对象关系，优于现有3D场景语言模型，并在多个任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的3D场景语言模型通常难以理解关系，特别是在视觉嵌入不足以充分表达对象的角色和交互时。

Method: Descrip3D通过使用自然语言显式编码对象之间的关系，增强了每个对象的文本描述，并通过嵌入融合和提示级别注入将这些关系线索整合到模型中。

Result: Descrip3D在ScanRefer、Multi3DRefer、ScanQA、SQA3D和Scan2Cap等五个基准数据集上始终优于强大的基线模型。

Conclusion: Descrip3D通过在对象之间显式编码关系，并在五个基准数据集上优于强大的基线模型，从而在理解复杂室内场景方面表现出有效性。

Abstract: Understanding 3D scenes goes beyond simply recognizing objects; it requires
reasoning about the spatial and semantic relationships between them. Current 3D
scene-language models often struggle with this relational understanding,
particularly when visual embeddings alone do not adequately convey the roles
and interactions of objects. In this paper, we introduce Descrip3D, a novel and
powerful framework that explicitly encodes the relationships between objects
using natural language. Unlike previous methods that rely only on 2D and 3D
embeddings, Descrip3D enhances each object with a textual description that
captures both its intrinsic attributes and contextual relationships. These
relational cues are incorporated into the model through a dual-level
integration: embedding fusion and prompt-level injection. This allows for
unified reasoning across various tasks such as grounding, captioning, and
question answering, all without the need for task-specific heads or additional
supervision. When evaluated on five benchmark datasets, including ScanRefer,
Multi3DRefer, ScanQA, SQA3D, and Scan2Cap, Descrip3D consistently outperforms
strong baseline models, demonstrating the effectiveness of language-guided
relational representation for understanding complex indoor scenes.

</details>


### [73] [LEAD: Exploring Logit Space Evolution for Model Selection](https://arxiv.org/abs/2507.14559)
*Zixuan Hu,Xiaotong Li,Shixiang Tang,Jun Liu,Yichun Hu,Ling-Yu Duan*

Main category: cs.CV

TL;DR: LEAD通过建模优化过程并导出一个常微分方程来描述向最终logit状态的非线性演化，从而有效地预测模型的可迁移性，选择最适合下游任务的预训练模型。


<details>
  <summary>Details</summary>
Motivation: 预训练-微调范例的显著成功导致了可用于视觉任务的可用预训练模型的激增。这种激增在有效地选择最适合下游任务的预训练模型方面提出了一个重大挑战。这项挑战的关键在于通过考虑潜在的微调动态来有效预测模型的可迁移性。

Method: LEAD提出了一种理论框架来建模优化过程，并推导出一个常微分方程（ODE）来描述向最终logit状态的非线性演化。此外，我们设计了一种类感知分解方法来考虑不同类别的不同演化动态，并进一步确保实际适用性。

Result: LEAD提供了一个简洁的解决方案，可以在一步之内有效地弥合优化差距，从而绕过漫长的微调过程。

Conclusion: LEAD在24个监督和自监督预训练模型以及10个下游数据集上的综合实验表明，LEAD具有令人印象深刻的性能，并展示了其广泛的适应性，即使在低数据场景中也是如此。

Abstract: The remarkable success of pretrain-then-finetune paradigm has led to a
proliferation of available pre-trained models for vision tasks. This surge
presents a significant challenge in efficiently choosing the most suitable
pre-trained models for downstream tasks. The critical aspect of this challenge
lies in effectively predicting the model transferability by considering the
underlying fine-tuning dynamics. Existing methods often model fine-tuning
dynamics in feature space with linear transformations, which do not precisely
align with the fine-tuning objective and fail to grasp the essential
nonlinearity from optimization. To this end, we present LEAD, a
finetuning-aligned approach based on the network output of logits. LEAD
proposes a theoretical framework to model the optimization process and derives
an ordinary differential equation (ODE) to depict the nonlinear evolution
toward the final logit state. Additionally, we design a class-aware
decomposition method to consider the varying evolution dynamics across classes
and further ensure practical applicability. Integrating the closely aligned
optimization objective and nonlinear modeling capabilities derived from the
differential equation, our method offers a concise solution to effectively
bridge the optimization gap in a single step, bypassing the lengthy fine-tuning
process. The comprehensive experiments on 24 supervised and self-supervised
pre-trained models across 10 downstream datasets demonstrate impressive
performances and showcase its broad adaptability even in low-data scenarios.

</details>


### [74] [Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w MRI Translation](https://arxiv.org/abs/2507.14575)
*Andrea Moschetto,Lemuel Puglisi,Alec Sargood,Pierluigi Dell'Acqua,Francesco Guarnera,Sebastiano Battiato,Daniele Ravì*

Main category: cs.CV

TL;DR: 本文对用于T1w到T2w 2D MRI图像转换的生成模型进行了全面的基准测试，结果表明，基于GAN的Pix2Pix模型优于扩散模型和基于FM的方法。


<details>
  <summary>Details</summary>
Motivation: 获取所有所需的MRI模态会增加扫描时间和成本，因此需要研究用于跨模态合成的计算方法。目的是从已获取的MRI图像合成缺少的MRI图像，从而减少获取时间，同时保持诊断质量。

Method: 使用生成对抗网络（GAN）、扩散模型和流量匹配（FM）技术进行T1w到T2w的2D MRI图像转换。

Result: GAN-based Pix2Pix模型在结构保真度、图像质量和计算效率方面优于扩散模型和基于FM的方法。

Conclusion: GAN-based Pix2Pix模型在结构保真度、图像质量和计算效率方面优于扩散模型和基于FM的方法。流模型容易在小数据集和简单任务上过拟合，可能需要更多数据才能与GAN性能相匹配或超过GAN性能。

Abstract: Magnetic Resonance Imaging (MRI) enables the acquisition of multiple image
contrasts, such as T1-weighted (T1w) and T2-weighted (T2w) scans, each offering
distinct diagnostic insights. However, acquiring all desired modalities
increases scan time and cost, motivating research into computational methods
for cross-modal synthesis. To address this, recent approaches aim to synthesize
missing MRI contrasts from those already acquired, reducing acquisition time
while preserving diagnostic quality. Image-to-image (I2I) translation provides
a promising framework for this task. In this paper, we present a comprehensive
benchmark of generative models$\unicode{x2013}$specifically, Generative
Adversarial Networks (GANs), diffusion models, and flow matching (FM)
techniques$\unicode{x2013}$for T1w-to-T2w 2D MRI I2I translation. All
frameworks are implemented with comparable settings and evaluated on three
publicly available MRI datasets of healthy adults. Our quantitative and
qualitative analyses show that the GAN-based Pix2Pix model outperforms
diffusion and FM-based methods in terms of structural fidelity, image quality,
and computational efficiency. Consistent with existing literature, these
results suggest that flow-based models are prone to overfitting on small
datasets and simpler tasks, and may require more data to match or surpass GAN
performance. These findings offer practical guidance for deploying I2I
translation techniques in real-world MRI workflows and highlight promising
directions for future research in cross-modal medical image synthesis. Code and
models are publicly available at
https://github.com/AndreaMoschetto/medical-I2I-benchmark.

</details>


### [75] [Performance comparison of medical image classification systems using TensorFlow Keras, PyTorch, and JAX](https://arxiv.org/abs/2507.14587)
*Merjem Bećirović,Amina Kurtović,Nordin Smajlović,Medina Kapo,Amila Akagić*

Main category: cs.CV

TL;DR: This paper compares the performance of TensorFlow/Keras, PyTorch, and JAX in classifying blood cell images, revealing performance variations and the efficiency of JAX and PyTorch.


<details>
  <summary>Details</summary>
Motivation: A detailed performance analysis of specific deep learning frameworks appears to be lacking in blood image analysis.

Method: Compared TensorFlow with Keras, PyTorch, and JAX in classifying blood cell images from the BloodMNIST dataset, focusing on inference time differences and classification performance for different image sizes.

Result: Variations in performance across frameworks, influenced by factors such as image resolution and framework-specific optimizations.

Conclusion: JAX and PyTorch showed comparable classification accuracy to current benchmarks, demonstrating their efficiency for medical image classification.

Abstract: Medical imaging plays a vital role in early disease diagnosis and monitoring.
Specifically, blood microscopy offers valuable insights into blood cell
morphology and the detection of hematological disorders. In recent years, deep
learning-based automated classification systems have demonstrated high
potential in enhancing the accuracy and efficiency of blood image analysis.
However, a detailed performance analysis of specific deep learning frameworks
appears to be lacking. This paper compares the performance of three popular
deep learning frameworks, TensorFlow with Keras, PyTorch, and JAX, in
classifying blood cell images from the publicly available BloodMNIST dataset.
The study primarily focuses on inference time differences, but also
classification performance for different image sizes. The results reveal
variations in performance across frameworks, influenced by factors such as
image resolution and framework-specific optimizations. Classification accuracy
for JAX and PyTorch was comparable to current benchmarks, showcasing the
efficiency of these frameworks for medical image classification.

</details>


### [76] [DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary queries in NeRF](https://arxiv.org/abs/2507.14596)
*Doriand Petit,Steve Bourgeois,Vincent Gay-Bellile,Florian Chabot,Loïc Barthe*

Main category: cs.CV

TL;DR: DiSCO-3D is the first method addressing the broader problem of 3D Open-Vocabulary Sub-concepts Discovery, which aims to provide a 3D semantic segmentation that adapts to both the scene and user queries.


<details>
  <summary>Details</summary>
Motivation: Traditional methods adapt exclusively to either task-specific goals (open-vocabulary segmentation) or scene content (unsupervised semantic segmentation).

Method: DiSCO-3D is built on Neural Fields representations, combining unsupervised segmentation with weak open-vocabulary guidance.

Result: DiSCO-3D achieves effective performance in Open-Vocabulary Sub-concepts Discovery and exhibits state-of-the-art results in the edge cases of both open-vocabulary and unsupervised segmentation.

Conclusion: DiSCO-3D achieves effective performance in Open-Vocabulary Sub-concepts Discovery and exhibits state-of-the-art results in the edge cases of both open-vocabulary and unsupervised segmentation.

Abstract: 3D semantic segmentation provides high-level scene understanding for
applications in robotics, autonomous systems, \textit{etc}. Traditional methods
adapt exclusively to either task-specific goals (open-vocabulary segmentation)
or scene content (unsupervised semantic segmentation). We propose DiSCO-3D, the
first method addressing the broader problem of 3D Open-Vocabulary Sub-concepts
Discovery, which aims to provide a 3D semantic segmentation that adapts to both
the scene and user queries. We build DiSCO-3D on Neural Fields representations,
combining unsupervised segmentation with weak open-vocabulary guidance. Our
evaluations demonstrate that DiSCO-3D achieves effective performance in
Open-Vocabulary Sub-concepts Discovery and exhibits state-of-the-art results in
the edge cases of both open-vocabulary and unsupervised segmentation.

</details>


### [77] [Exp-Graph: How Connections Learn Facial Attributes in Graph-based Expression Recognition](https://arxiv.org/abs/2507.14608)
*Nandani Sharma,Dinesh Singh*

Main category: cs.CV

TL;DR: Exp-Graph, a graph-based framework for facial expression recognition, uses facial landmarks and a vision transformer to model structural relationships, achieving high accuracy on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Facial expression recognition is crucial for human-computer interaction applications. Incorporating structural information into facial attributes is essential since the structure of facial attributes varies with facial expressions.

Method: The paper proposes Exp-Graph, a novel framework that uses graph-based modeling to represent structural relationships among facial attributes for facial expression recognition. It uses facial landmarks as graph vertices and determines edges based on landmark proximity and local appearance similarity encoded using a vision transformer. Graph convolutional networks capture and integrate structural dependencies.

Result: The Exp-Graph model achieved recognition accuracies of 98.09% on Oulu-CASIA, 79.01% on eNTERFACE05, and 56.39% on AFEW.

Conclusion: The Exp-Graph model achieves high recognition accuracies on three benchmark datasets, demonstrating strong generalization capabilities in both controlled and real-world environments, which proves its effectiveness for practical facial expression recognition applications.

Abstract: Facial expression recognition is crucial for human-computer interaction
applications such as face animation, video surveillance, affective computing,
medical analysis, etc. Since the structure of facial attributes varies with
facial expressions, incorporating structural information into facial attributes
is essential for facial expression recognition. In this paper, we propose
Exp-Graph, a novel framework designed to represent the structural relationships
among facial attributes using graph-based modeling for facial expression
recognition. For facial attributes graph representation, facial landmarks are
used as the graph's vertices. At the same time, the edges are determined based
on the proximity of the facial landmark and the similarity of the local
appearance of the facial attributes encoded using the vision transformer.
Additionally, graph convolutional networks are utilized to capture and
integrate these structural dependencies into the encoding of facial attributes,
thereby enhancing the accuracy of expression recognition. Thus, Exp-Graph
learns from the facial attribute graphs highly expressive semantic
representations. On the other hand, the vision transformer and graph
convolutional blocks help the framework exploit the local and global
dependencies among the facial attributes that are essential for the recognition
of facial expressions. We conducted comprehensive evaluations of the proposed
Exp-Graph model on three benchmark datasets: Oulu-CASIA, eNTERFACE05, and AFEW.
The model achieved recognition accuracies of 98.09\%, 79.01\%, and 56.39\%,
respectively. These results indicate that Exp-Graph maintains strong
generalization capabilities across both controlled laboratory settings and
real-world, unconstrained environments, underscoring its effectiveness for
practical facial expression recognition applications.

</details>


### [78] [Depthwise-Dilated Convolutional Adapters for Medical Object Tracking and Segmentation Using the Segment Anything Model 2](https://arxiv.org/abs/2507.14613)
*Guoping Xu,Christopher Kabat,You Zhang*

Main category: cs.CV

TL;DR: DD-SAM2是一种用于医学视频分割和跟踪的SAM2高效适配框架，它通过深度可分离扩张适配器增强多尺度特征提取，并在有限数据下实现有效微调，在TrackRad2025和EchoNet-Dynamic数据集上取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割方法受限于模态特定设计，且对动态医学成像场景的适应性差。将SAM2及其变体应用于医学视频通常需要大规模数据集进行再训练或迁移学习，导致高计算成本和灾难性遗忘的风险。

Method: 提出DD-SAM2，一种SAM2的有效适配框架，该框架包含一个深度可分离扩张适配器（DD-Adapter），以最小的参数开销增强多尺度特征提取。

Result: DD-SAM2在医学视频对象跟踪和分割方面充分利用了SAM2的流式内存，并在有限的训练数据下有效微调SAM2。在TrackRad2025和EchoNet-Dynamic数据集上的综合评估表明，DD-SAM2表现优异，Dice系数分别达到0.93和0.97。

Conclusion: DD-SAM2在TrackRad2025和EchoNet-Dynamic数据集上表现出色，Dice系数分别达到0.93和0.97。该工作首次尝试了基于适配器的SAM2微调，用于医学视频分割和跟踪。

Abstract: Recent advances in medical image segmentation have been driven by deep
learning; however, most existing methods remain limited by modality-specific
designs and exhibit poor adaptability to dynamic medical imaging scenarios. The
Segment Anything Model 2 (SAM2) and its related variants, which introduce a
streaming memory mechanism for real-time video segmentation, present new
opportunities for prompt-based, generalizable solutions. Nevertheless, adapting
these models to medical video scenarios typically requires large-scale datasets
for retraining or transfer learning, leading to high computational costs and
the risk of catastrophic forgetting. To address these challenges, we propose
DD-SAM2, an efficient adaptation framework for SAM2 that incorporates a
Depthwise-Dilated Adapter (DD-Adapter) to enhance multi-scale feature
extraction with minimal parameter overhead. This design enables effective
fine-tuning of SAM2 on medical videos with limited training data. Unlike
existing adapter-based methods focused solely on static images, DD-SAM2 fully
exploits SAM2's streaming memory for medical video object tracking and
segmentation. Comprehensive evaluations on TrackRad2025 (tumor segmentation)
and EchoNet-Dynamic (left ventricle tracking) datasets demonstrate superior
performance, achieving Dice scores of 0.93 and 0.97, respectively. To the best
of our knowledge, this work provides an initial attempt at systematically
exploring adapter-based SAM2 fine-tuning for medical video segmentation and
tracking. Code, datasets, and models will be publicly available at
https://github.com/apple1986/DD-SAM2.

</details>


### [79] [BusterX++: Towards Unified Cross-Modal AI-Generated Content Detection and Explanation with MLLM](https://arxiv.org/abs/2507.14632)
*Haiquan Wen,Tianxiao Li,Zhenglin Huang,Yiwei He,Guangliang Cheng*

Main category: cs.CV

TL;DR: Introduces BusterX++, a novel framework for cross-modal detection and explanation of synthetic media, along with GenBuster++, a new cross-modal benchmark dataset.


<details>
  <summary>Details</summary>
Motivation: Current detection systems are fundamentally limited by their single-modality design, making them ineffective against synthetic content that combines multiple media formats.

Method: The BusterX++ framework incorporates an advanced reinforcement learning (RL) post-training strategy that eliminates cold-start through Multi-stage Training, Thinking Reward, and Hybrid Reasoning.

Result: BusterX++ achieves stable and substantial performance improvements. The GenBuster++ benchmark comprises 4,000 images and video clips, meticulously curated by human experts using a novel filtering methodology to ensure high quality, diversity, and real-world applicability.

Conclusion: Extensive experiments demonstrate the effectiveness and generalizability of the BusterX++ approach.

Abstract: Recent advances in generative AI have dramatically improved image and video
synthesis capabilities, significantly increasing the risk of misinformation
through sophisticated fake content. In response, detection methods have evolved
from traditional approaches to multimodal large language models (MLLMs),
offering enhanced transparency and interpretability in identifying synthetic
media. However, current detection systems remain fundamentally limited by their
single-modality design. These approaches analyze images or videos separately,
making them ineffective against synthetic content that combines multiple media
formats. To address these challenges, we introduce \textbf{BusterX++}, a novel
framework designed specifically for cross-modal detection and explanation of
synthetic media. Our approach incorporates an advanced reinforcement learning
(RL) post-training strategy that eliminates cold-start. Through Multi-stage
Training, Thinking Reward, and Hybrid Reasoning, BusterX++ achieves stable and
substantial performance improvements. To enable comprehensive evaluation, we
also present \textbf{GenBuster++}, a cross-modal benchmark leveraging
state-of-the-art image and video generation techniques. This benchmark
comprises 4,000 images and video clips, meticulously curated by human experts
using a novel filtering methodology to ensure high quality, diversity, and
real-world applicability. Extensive experiments demonstrate the effectiveness
and generalizability of our approach.

</details>


### [80] [Multispectral State-Space Feature Fusion: Bridging Shared and Cross-Parametric Interactions for Object Detection](https://arxiv.org/abs/2507.14643)
*Jifeng Shen,Haibo Zhan,Shaohua Dong,Xin Zuo,Wankou Yang,Haibin Ling*

Main category: cs.CV

TL;DR: MS2Fusion是一种新的多光谱特征融合框架，它使用状态空间模型和双路径参数交互机制，在多光谱目标检测等任务上实现了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 现代多光谱特征融合在目标检测方面面临两个关键限制：(1)过度偏好局部互补特征而非跨模态共享语义，不利于泛化性能；(2)感受野大小和计算复杂度之间的权衡对可扩展特征建模提出了关键瓶颈。

Method: 提出了一种新的多光谱状态空间特征融合框架MS2Fusion，该框架基于状态空间模型(SSM)，通过双路径参数交互机制实现高效融合。

Result: 在FLIR、M3FD和LLVIP等主流基准测试中，MS2Fusion显著优于其他state-of-the-art的多光谱目标检测方法。

Conclusion: MS2Fusion在多光谱目标检测、RGB-T语义分割和RGBT显著目标检测等任务上取得了state-of-the-art的结果，证明了其优越性和通用性。

Abstract: Modern multispectral feature fusion for object detection faces two critical
limitations: (1) Excessive preference for local complementary features over
cross-modal shared semantics adversely affects generalization performance; and
(2) The trade-off between the receptive field size and computational complexity
present critical bottlenecks for scalable feature modeling. Addressing these
issues, a novel Multispectral State-Space Feature Fusion framework, dubbed
MS2Fusion, is proposed based on the state space model (SSM), achieving
efficient and effective fusion through a dual-path parametric interaction
mechanism. More specifically, the first cross-parameter interaction branch
inherits the advantage of cross-attention in mining complementary information
with cross-modal hidden state decoding in SSM. The second shared-parameter
branch explores cross-modal alignment with joint embedding to obtain
cross-modal similar semantic features and structures through parameter sharing
in SSM. Finally, these two paths are jointly optimized with SSM for fusing
multispectral features in a unified framework, allowing our MS2Fusion to enjoy
both functional complementarity and shared semantic space. In our extensive
experiments on mainstream benchmarks including FLIR, M3FD and LLVIP, our
MS2Fusion significantly outperforms other state-of-the-art multispectral object
detection methods, evidencing its superiority. Moreover, MS2Fusion is general
and applicable to other multispectral perception tasks. We show that, even
without specific design, MS2Fusion achieves state-of-the-art results on RGB-T
semantic segmentation and RGBT salient object detection, showing its
generality. The source code will be available at
https://github.com/61s61min/MS2Fusion.git.

</details>


### [81] [AI-Powered Precision in Sport Taekwondo: Enhancing Fairness, Speed, and Trust in Competition (FST.ai)](https://arxiv.org/abs/2507.14657)
*Keivan Shariatmadar,Ahmad Osman*

Main category: cs.CV

TL;DR: This paper introduces FST.ai, a novel AI-powered framework designed to enhance officiating in Sport Taekwondo, particularly focusing on the complex task of real-time head kick detection and scoring.


<details>
  <summary>Details</summary>
Motivation: Traditional manual systems suffer from latency, subjectivity, and inconsistent enforcement, undermining fairness and athlete trust.

Method: computer vision, deep learning, and edge inference

Result: significantly reducing decision time from minutes to seconds while improving consistency and transparency.

Conclusion: FST.ai demonstrated robustness, scalability, and sport-agnostic potential to transform officiating standards across multiple disciplines.

Abstract: The integration of Artificial Intelligence (AI) into sports officiating
represents a paradigm shift in how decisions are made in competitive
environments. Traditional manual systems, even when supported by Instant Video
Replay (IVR), often suffer from latency, subjectivity, and inconsistent
enforcement, undermining fairness and athlete trust. This paper introduces
FST.ai, a novel AI-powered framework designed to enhance officiating in Sport
Taekwondo, particularly focusing on the complex task of real-time head kick
detection and scoring. Leveraging computer vision, deep learning, and edge
inference, the system automates the identification and classification of key
actions, significantly reducing decision time from minutes to seconds while
improving consistency and transparency. Importantly, the methodology is not
limited to Taekwondo. The underlying framework -- based on pose estimation,
motion classification, and impact analysis -- can be adapted to a wide range of
sports requiring action detection, such as judo, karate, fencing, or even team
sports like football and basketball, where foul recognition or performance
tracking is critical. By addressing one of Taekwondo's most challenging
scenarios -- head kick scoring -- we demonstrate the robustness, scalability,
and sport-agnostic potential of FST.ai to transform officiating standards
across multiple disciplines.

</details>


### [82] [Artificial Intelligence in the Food Industry: Food Waste Estimation based on Computer Vision, a Brief Case Study in a University Dining Hall](https://arxiv.org/abs/2507.14662)
*Shayan Rokhva,Babak Teimourpour*

Main category: cs.CV

TL;DR: Presents a computer vision framework for estimating plate-level food waste in dining settings using semantic segmentation.. Achieved high accuracy with lighter models enabling real-time performance.


<details>
  <summary>Details</summary>
Motivation: Quantifying post-consumer food waste in institutional dining settings is essential for supporting data-driven sustainability strategies.

Method: a cost-effective computer vision framework that estimates plate-level food waste by utilizing semantic segmentation of RGB images taken before and after meal consumption across five Iranian dishes. Four fully supervised models (U-Net, U-Net++, and their lightweight variants) were trained using a capped dynamic inverse-frequency loss and AdamW optimizer

Result: All models achieved satisfying performance, and for each food type, at least one model approached or surpassed 90% DPA, demonstrating strong alignment in pixel-wise proportion estimates. Lighter models with reduced parameter counts offered faster inference, achieving real-time throughput on an NVIDIA T4 GPU. Further analysis showed superior segmentation performance for dry and more rigid components (e.g., rice and fries), while more complex, fragmented, or viscous dishes, such as stews, showed reduced performance, specifically post-consumption.

Conclusion: This research lays foundational groundwork for automated, real-time waste tracking systems in large-scale food service environments and offers actionable insights and outlines feasible future directions for dining hall management and policymakers aiming to reduce institutional food waste.

Abstract: Quantifying post-consumer food waste in institutional dining settings is
essential for supporting data-driven sustainability strategies. This study
presents a cost-effective computer vision framework that estimates plate-level
food waste by utilizing semantic segmentation of RGB images taken before and
after meal consumption across five Iranian dishes. Four fully supervised models
(U-Net, U-Net++, and their lightweight variants) were trained using a capped
dynamic inverse-frequency loss and AdamW optimizer, then evaluated through a
comprehensive set of metrics, including Pixel Accuracy, Dice, IoU, and a
custom-defined Distributional Pixel Agreement (DPA) metric tailored to the
task. All models achieved satisfying performance, and for each food type, at
least one model approached or surpassed 90% DPA, demonstrating strong alignment
in pixel-wise proportion estimates. Lighter models with reduced parameter
counts offered faster inference, achieving real-time throughput on an NVIDIA T4
GPU. Further analysis showed superior segmentation performance for dry and more
rigid components (e.g., rice and fries), while more complex, fragmented, or
viscous dishes, such as stews, showed reduced performance, specifically
post-consumption. Despite limitations such as reliance on 2D imaging,
constrained food variety, and manual data collection, the proposed framework is
pioneering and represents a scalable, contactless solution for continuous
monitoring of food consumption. This research lays foundational groundwork for
automated, real-time waste tracking systems in large-scale food service
environments and offers actionable insights and outlines feasible future
directions for dining hall management and policymakers aiming to reduce
institutional food waste.

</details>


### [83] [Gene-DML: Dual-Pathway Multi-Level Discrimination for Gene Expression Prediction from Histopathology Images](https://arxiv.org/abs/2507.14670)
*Yaxuan Song,Jianan Fan,Hang Chang,Weidong Cai*

Main category: cs.CV

TL;DR: Gene-DML 是一种用于从组织病理学图像预测基因表达的新框架，它通过双路径多级判别来增强形态和转录模式之间的对应关系，并在公共数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 从组织病理学图像准确预测基因表达为分子分析提供了一种可扩展的非侵入性方法，对精准医学和计算病理学具有重要意义。然而，现有方法通常未能充分利用组织病理学图像和跨多个表征水平的基因表达谱之间的跨模态表征对齐，从而限制了它们的预测性能。

Method: 我们提出了 Gene-DML，这是一个统一的框架，它通过双路径多级判别来构建潜在空间，以增强形态和转录模式之间的对应关系。

Result: 公共空间转录组学数据集上的大量实验表明，Gene-DML 在基因表达预测方面实现了最先进的性能。

Conclusion: Gene-DML在基因表达预测方面实现了最先进的性能。

Abstract: Accurately predicting gene expression from histopathology images offers a
scalable and non-invasive approach to molecular profiling, with significant
implications for precision medicine and computational pathology. However,
existing methods often underutilize the cross-modal representation alignment
between histopathology images and gene expression profiles across multiple
representational levels, thereby limiting their prediction performance. To
address this, we propose Gene-DML, a unified framework that structures latent
space through Dual-pathway Multi-Level discrimination to enhance correspondence
between morphological and transcriptional modalities. The multi-scale
instance-level discrimination pathway aligns hierarchical histopathology
representations extracted at local, neighbor, and global levels with gene
expression profiles, capturing scale-aware morphological-transcriptional
relationships. In parallel, the cross-level instance-group discrimination
pathway enforces structural consistency between individual (image/gene)
instances and modality-crossed (gene/image, respectively) groups, strengthening
the alignment across modalities. By jointly modelling fine-grained and
structural-level discrimination, Gene-DML is able to learn robust cross-modal
representations, enhancing both predictive accuracy and generalization across
diverse biological contexts. Extensive experiments on public spatial
transcriptomics datasets demonstrate that Gene-DML achieves state-of-the-art
performance in gene expression prediction. The code and checkpoints will be
released soon.

</details>


### [84] [Docopilot: Improving Multimodal Models for Document-Level Understanding](https://arxiv.org/abs/2507.14675)
*Yuchen Duan,Zhe Chen,Yusong Hu,Weiyun Wang,Shenglong Ye,Botian Shi,Lewei Lu,Qibin Hou,Tong Lu,Hongsheng Li,Jifeng Dai,Wenhai Wang*

Main category: cs.CV

TL;DR: 提出了一个高质量的文档级数据集Doc-750K，并在此基础上开发了一个原生的多模态模型Docopilot，它在文档理解任务中表现出色，且不需要依赖RAG。


<details>
  <summary>Details</summary>
Motivation: 多模态大型语言模型(MLLM)在复杂的多页文档理解方面的性能仍然不足，这主要是由于缺乏高质量的文档级数据集。

Method: 开发了一个原生的多模态模型Docopilot，它可以准确地处理文档级的依赖关系，而不需要依赖RAG。

Result: Docopilot在文档理解任务和多轮交互中实现了卓越的连贯性、准确性和效率。

Conclusion: Docopilot在文档理解任务和多轮交互中实现了卓越的连贯性、准确性和效率，为文档级多模态理解设定了新的基线。

Abstract: Despite significant progress in multimodal large language models (MLLMs),
their performance on complex, multi-page document comprehension remains
inadequate, largely due to the lack of high-quality, document-level datasets.
While current retrieval-augmented generation (RAG) methods offer partial
solutions, they suffer from issues, such as fragmented retrieval contexts,
multi-stage error accumulation, and extra time costs of retrieval. In this
work, we present a high-quality document-level dataset, Doc-750K, designed to
support in-depth understanding of multimodal documents. This dataset includes
diverse document structures, extensive cross-page dependencies, and real
question-answer pairs derived from the original documents. Building on the
dataset, we develop a native multimodal model, Docopilot, which can accurately
handle document-level dependencies without relying on RAG. Experiments
demonstrate that Docopilot achieves superior coherence, accuracy, and
efficiency in document understanding tasks and multi-turn interactions, setting
a new baseline for document-level multimodal understanding. Data, code, and
models are released at https://github.com/OpenGVLab/Docopilot

</details>


### [85] [WSI-Agents: A Collaborative Multi-Agent System for Multi-Modal Whole Slide Image Analysis](https://arxiv.org/abs/2507.14680)
*Xinheng Lyu,Yuci Liang,Wenting Chen,Meidan Ding,Jiaqi Yang,Guolin Huang,Daokun Zhang,Xiangjian He,Linlin Shen*

Main category: cs.CV

TL;DR: WSI-Agents是一种用于多模态WSI分析的协作多代理系统，它优于当前的WSI MLLM和医疗代理框架。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大型语言模型(MLLM)虽然可以通过自然语言实现多任务WSI分析，但与特定任务模型相比，性能通常不佳。协作多代理系统已成为在医疗保健领域平衡多功能性和准确性的有希望的解决方案，但它们在病理学特定领域的潜力仍未得到充分挖掘。

Method: 我们提出了WSI-Agents，这是一个用于多模态WSI分析的新型协作多代理系统，它集成了具有强大任务分配和验证机制的专用功能代理。

Result: WSI-Agents通过三个组成部分增强了特定任务的准确性和多任务通用性：(1)任务分配模块，使用patch和WSI级别的MLLM模型库将任务分配给专家代理；(2)验证机制，通过内部一致性检查和使用病理学知识库和领域特定模型的外部验证来确保准确性；(3)摘要模块，综合带有视觉解释图的最终摘要。

Conclusion: WSI-Agents在多模态WSI基准测试中表现优于当前的WSI MLLM和医疗代理框架。

Abstract: Whole slide images (WSIs) are vital in digital pathology, enabling gigapixel
tissue analysis across various pathological tasks. While recent advancements in
multi-modal large language models (MLLMs) allow multi-task WSI analysis through
natural language, they often underperform compared to task-specific models.
Collaborative multi-agent systems have emerged as a promising solution to
balance versatility and accuracy in healthcare, yet their potential remains
underexplored in pathology-specific domains. To address these issues, we
propose WSI-Agents, a novel collaborative multi-agent system for multi-modal
WSI analysis. WSI-Agents integrates specialized functional agents with robust
task allocation and verification mechanisms to enhance both task-specific
accuracy and multi-task versatility through three components: (1) a task
allocation module assigning tasks to expert agents using a model zoo of patch
and WSI level MLLMs, (2) a verification mechanism ensuring accuracy through
internal consistency checks and external validation using pathology knowledge
bases and domain-specific models, and (3) a summary module synthesizing the
final summary with visual interpretation maps. Extensive experiments on
multi-modal WSI benchmarks show WSI-Agents's superiority to current WSI MLLMs
and medical agent frameworks across diverse tasks.

</details>


### [86] [From Semantics, Scene to Instance-awareness: Distilling Foundation Model for Open-vocabulary Situation Recognition](https://arxiv.org/abs/2507.14686)
*Chen Cai,Tianyi Liu,Jianjun Gao,Wenyang Liu,Kejun Wu,Ruoyu Wang,Yi Wang,Soo Chin Liew*

Main category: cs.CV

TL;DR: This paper introduces Open-vocabulary Grounded Situation Recognition (Ov-GSR) to enhance generalization and zero-shot abilities of small GSR models by transferring knowledge from a teacher MLLM using Multimodal Interactive Prompt Distillation (MIPD).


<details>
  <summary>Details</summary>
Motivation: Recent Multimodal Large Language Models (MLLMs) exhibit strong zero-shot abilities but struggle with complex Grounded Situation Recognition (GSR) and are resource-intensive for edge device deployment. Meanwhile, conventional GSR models often lack generalization ability, falling short in recognizing unseen and rare situations.

Method: We propose Multimodal Interactive Prompt Distillation (MIPD), a novel framework that distills enriched multimodal knowledge from the foundation model, enabling the student Ov-GSR model to recognize unseen situations and be better aware of rare situations.

Result: achieving superior performance on seen, rare, and unseen situations, and further demonstrate improved unseen detection on the HICO-DET dataset.

Conclusion: We evaluate MIPD on the refined Ov-SWiG dataset, achieving superior performance on seen, rare, and unseen situations, and further demonstrate improved unseen detection on the HICO-DET dataset.

Abstract: Recent Multimodal Large Language Models (MLLMs) exhibit strong zero-shot
abilities but struggle with complex Grounded Situation Recognition (GSR) and
are resource-intensive for edge device deployment. Meanwhile, conventional GSR
models often lack generalization ability, falling short in recognizing unseen
and rare situations. In this paper, we exploit transferring knowledge from a
teacher MLLM to a small GSR model to enhance its generalization and zero-shot
abilities, thereby introducing the task of Open-vocabulary Grounded Situation
Recognition (Ov-GSR). To achieve this, we propose Multimodal Interactive Prompt
Distillation (MIPD), a novel framework that distills enriched multimodal
knowledge from the foundation model, enabling the student Ov-GSR model to
recognize unseen situations and be better aware of rare situations.
Specifically, the MIPD framework first leverages the LLM-based Judgmental
Rationales Generator (JRG) to construct positive and negative glimpse and gaze
rationales enriched with contextual semantic information. The proposed
scene-aware and instance-perception prompts are then introduced to align
rationales with visual information from the MLLM teacher via the
Negative-Guided Multimodal Prompting Alignment (NMPA) module, effectively
capturing holistic and perceptual multimodal knowledge. Finally, the aligned
multimodal knowledge is distilled into the student Ov-GSR model, providing a
stronger foundation for generalization that enhances situation understanding,
bridges the gap between seen and unseen scenarios, and mitigates prediction
bias in rare cases. We evaluate MIPD on the refined Ov-SWiG dataset, achieving
superior performance on seen, rare, and unseen situations, and further
demonstrate improved unseen detection on the HICO-DET dataset.

</details>


### [87] [GTPBD: A Fine-Grained Global Terraced Parcel and Boundary Dataset](https://arxiv.org/abs/2507.14697)
*Zhiwei Zhang,Zi Ye,Yibin Wen,Shuai Yuan,Haohuan Fu,Jianxi Huang,Juepeng Zheng*

Main category: cs.CV

TL;DR: GTPBD是一个新的细粒度梯田地块数据集，它填补了梯田遥感研究中的一个关键空白，并为各种任务提供了基准。


<details>
  <summary>Details</summary>
Motivation: 现有的农业地块提取研究只关注中等分辨率测绘或普通平原农田，由于精确农业的需求，缺乏对复杂梯田地形的表征。

Method: 提出了GTPBD数据集，并对八种语义分割方法、四种边缘提取方法、三种地块提取方法和五种UDA方法进行了基准测试，以及一个集成了像素级和对象级指标的多维评估框架。

Result: GTPBD是一个细粒度的梯田地块数据集，包含超过20万个复杂梯田地块的手动标注，覆盖中国和全球的七个主要地理区域和跨大陆气候区域。与现有数据集相比，GTPBD数据集带来了相当大的挑战，由于：(1)地形多样性；(2)复杂和不规则的地块对象；(3)多个领域风格。GTPBD数据集适用于四种不同的任务，包括语义分割、边缘检测、梯田地块提取和无监督域适应(UDA)任务。

Conclusion: GTPBD 填补了梯田遥感研究的关键空白，为细粒度农业地形分析和跨场景知识转移提供了基础设施。

Abstract: Agricultural parcels serve as basic units for conducting agricultural
practices and applications, which is vital for land ownership registration,
food security assessment, soil erosion monitoring, etc. However, existing
agriculture parcel extraction studies only focus on mid-resolution mapping or
regular plain farmlands while lacking representation of complex terraced
terrains due to the demands of precision agriculture.In this paper, we
introduce a more fine-grained terraced parcel dataset named GTPBD (Global
Terraced Parcel and Boundary Dataset), which is the first fine-grained dataset
covering major worldwide terraced regions with more than 200,000 complex
terraced parcels with manual annotation. GTPBD comprises 47,537 high-resolution
images with three-level labels, including pixel-level boundary labels, mask
labels, and parcel labels. It covers seven major geographic zones in China and
transcontinental climatic regions around the world.Compared to the existing
datasets, the GTPBD dataset brings considerable challenges due to the: (1)
terrain diversity; (2) complex and irregular parcel objects; and (3) multiple
domain styles. Our proposed GTPBD dataset is suitable for four different tasks,
including semantic segmentation, edge detection, terraced parcel extraction,
and unsupervised domain adaptation (UDA) tasks.Accordingly, we benchmark the
GTPBD dataset on eight semantic segmentation methods, four edge extraction
methods, three parcel extraction methods, and five UDA methods, along with a
multi-dimensional evaluation framework integrating pixel-level and object-level
metrics. GTPBD fills a critical gap in terraced remote sensing research,
providing a basic infrastructure for fine-grained agricultural terrain analysis
and cross-scenario knowledge transfer.

</details>


### [88] [MultiRetNet: A Multimodal Vision Model and Deferral System for Staging Diabetic Retinopathy](https://arxiv.org/abs/2507.14738)
*Jeannie She,Katie Spivakovsky*

Main category: cs.CV

TL;DR: MultiRetNet improves DR staging accuracy by combining retinal imaging, socioeconomic factors, and comorbidity profiles, integrated with a clinical deferral system.


<details>
  <summary>Details</summary>
Motivation: Diabetic retinopathy (DR) is a leading cause of preventable blindness, affecting over 100 million people worldwide. In the United States, individuals from lower-income communities face a higher risk of progressing to advanced stages before diagnosis, largely due to limited access to screening. Comorbid conditions further accelerate disease progression.

Method: MultiRetNet, a novel pipeline combining retinal imaging, socioeconomic factors, and comorbidity profiles to improve DR staging accuracy, integrated with a clinical deferral system for a clinical human-in-the-loop implementation. Experiment with three multimodal fusion methods and identify fusion through a fully connected layer as the most versatile methodology. Synthesize adversarial, low-quality images and use contrastive learning to train the deferral system.

Result: By maintaining diagnostic accuracy on suboptimal images and integrating critical health data, the system can improve early detection, particularly in underserved populations where advanced DR is often first identified.

Conclusion: The system can improve early detection, reduce healthcare costs, increase early detection rates, and address disparities in access to care, promoting healthcare equity.

Abstract: Diabetic retinopathy (DR) is a leading cause of preventable blindness,
affecting over 100 million people worldwide. In the United States, individuals
from lower-income communities face a higher risk of progressing to advanced
stages before diagnosis, largely due to limited access to screening. Comorbid
conditions further accelerate disease progression. We propose MultiRetNet, a
novel pipeline combining retinal imaging, socioeconomic factors, and
comorbidity profiles to improve DR staging accuracy, integrated with a clinical
deferral system for a clinical human-in-the-loop implementation. We experiment
with three multimodal fusion methods and identify fusion through a fully
connected layer as the most versatile methodology. We synthesize adversarial,
low-quality images and use contrastive learning to train the deferral system,
guiding the model to identify out-of-distribution samples that warrant
clinician review. By maintaining diagnostic accuracy on suboptimal images and
integrating critical health data, our system can improve early detection,
particularly in underserved populations where advanced DR is often first
identified. This approach may reduce healthcare costs, increase early detection
rates, and address disparities in access to care, promoting healthcare equity.

</details>


### [89] [InterAct-Video: Reasoning-Rich Video QA for Urban Traffic](https://arxiv.org/abs/2507.14743)
*Joseph Raj Vishal,Rutuja Patil,Manas Srinivas Gowda,Katha Naik,Yezhou Yang,Bharatesh Chakravarthi*

Main category: cs.CV

TL;DR: This paper introduces InterAct VideoQA, a new dataset for video question answering in traffic monitoring. It addresses the limitations of existing models in handling complex real-world traffic scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing VideoQA models struggle with the complexity of real-world traffic scenes, where multiple concurrent events unfold across spatiotemporal dimensions.

Method: This paper introduces InterAct VideoQA, a curated dataset designed to benchmark and enhance VideoQA models for traffic monitoring tasks.

Result: State-of-the-art VideoQA models are evaluated on InterAct VideoQA, exposing challenges in reasoning over fine-grained spatiotemporal dependencies within complex traffic scenarios. Additionally, fine-tuning these models on InterAct VideoQA yields notable performance improvements, demonstrating the necessity of domain-specific datasets for VideoQA.

Conclusion: InterAct VideoQA is publicly available as a benchmark dataset to facilitate future research in real-world deployable VideoQA models for intelligent transportation systems.

Abstract: Traffic monitoring is crucial for urban mobility, road safety, and
intelligent transportation systems (ITS). Deep learning has advanced
video-based traffic monitoring through video question answering (VideoQA)
models, enabling structured insight extraction from traffic videos. However,
existing VideoQA models struggle with the complexity of real-world traffic
scenes, where multiple concurrent events unfold across spatiotemporal
dimensions. To address these challenges, this paper introduces \textbf{InterAct
VideoQA}, a curated dataset designed to benchmark and enhance VideoQA models
for traffic monitoring tasks. The InterAct VideoQA dataset comprises 8 hours of
real-world traffic footage collected from diverse intersections, segmented into
10-second video clips, with over 25,000 question-answer (QA) pairs covering
spatiotemporal dynamics, vehicle interactions, incident detection, and other
critical traffic attributes. State-of-the-art VideoQA models are evaluated on
InterAct VideoQA, exposing challenges in reasoning over fine-grained
spatiotemporal dependencies within complex traffic scenarios. Additionally,
fine-tuning these models on InterAct VideoQA yields notable performance
improvements, demonstrating the necessity of domain-specific datasets for
VideoQA. InterAct VideoQA is publicly available as a benchmark dataset to
facilitate future research in real-world deployable VideoQA models for
intelligent transportation systems. GitHub Repo:
https://github.com/joe-rabbit/InterAct_VideoQA

</details>


### [90] [LeAdQA: LLM-Driven Context-Aware Temporal Grounding for Video Question Answering](https://arxiv.org/abs/2507.14784)
*Xinxin Dong,Baoyun Peng,Haokai Ma,Yufei Wang,Zixuan Dong,Fei Hu,Xiaodong Wang*

Main category: cs.CV

TL;DR: LeAdQA通过结合因果感知查询细化和细粒度视觉定位来解决视频问答中的挑战，从而在复杂推理任务上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视频问答方法受限于两种有缺陷的策略：1) 不加区分地处理所有帧，使关键事件被无关内容淹没；2) 启发式检索捕获表面模式，但错失复杂推理所需的因果时间结构。

Method: 利用LLM改进问题选项对，解决因果模糊性并加强时间焦点。然后，这些改进的查询指导时间定位模型精确检索最相关的片段，并通过自适应融合机制动态整合证据以最大化相关性。

Result: 在NExT-QA、IntentQA和NExT-GQA上的实验表明，LeAdQA的精确视觉定位显著提高了对视频-问题关系的理解。

Conclusion: LeAdQA在复杂推理任务上实现了最先进的性能，同时保持了计算效率。

Abstract: Video Question Answering (VideoQA) requires identifying sparse critical
moments in long videos and reasoning about their causal relationships to answer
semantically complex questions. While recent advances in multimodal learning
have improved alignment and fusion, current approaches remain limited by two
prevalent but fundamentally flawed strategies: (1) task-agnostic sampling
indiscriminately processes all frames, overwhelming key events with irrelevant
content; and (2) heuristic retrieval captures superficial patterns but misses
causal-temporal structures needed for complex reasoning. To address these
challenges, we introduce LeAdQA, an innovative approach that bridges these gaps
through synergizing causal-aware query refinement with fine-grained visual
grounding. Our method first leverages LLMs to reformulate question-option
pairs, resolving causal ambiguities and sharpening temporal focus. These
refined queries subsequently direct a temporal grounding model to precisely
retrieve the most salient segments, complemented by an adaptive fusion
mechanism dynamically integrating the evidence to maximize relevance. The
integrated visual-textual cues are then processed by an MLLM to generate
accurate, contextually-grounded answers. Experiments on NExT-QA, IntentQA, and
NExT-GQA demonstrate that our method's precise visual grounding substantially
enhances the understanding of video-question relationships, achieving
state-of-the-art (SOTA) performance on complex reasoning tasks while
maintaining computational efficiency.

</details>


### [91] [FOCUS: Fused Observation of Channels for Unveiling Spectra](https://arxiv.org/abs/2507.14787)
*Xi Xiao,Aristeidis Tsaris,Anika Tabassum,John Lagergren,Larry M. York,Tianyang Wang,Xiao Wang*

Main category: cs.CV

TL;DR: FOCUS makes ViT interpretable for hyperspectral imaging by using spectral prompts and a sink token, achieving better performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Interpreting Vision Transformers (ViTs) in hyperspectral imaging (HSI) is challenging due to difficulties in capturing meaningful spectral cues and the computational cost of full-spectrum ViTs.

Method: Introduces class-specific spectral prompts and a learnable [SINK] token trained with an attraction loss.

Result: Generates stable and interpretable 3D saliency maps and spectral importance curves in a single forward pass, without gradient backpropagation or backbone modification. Aligns closely with expert annotations and has less than 1 percent parameter overhead.

Conclusion: FOCUS enables reliable and efficient spatial-spectral interpretability for frozen ViTs, improving band-level IoU by 15 percent and reducing attention collapse by over 40 percent.

Abstract: Hyperspectral imaging (HSI) captures hundreds of narrow, contiguous
wavelength bands, making it a powerful tool in biology, agriculture, and
environmental monitoring. However, interpreting Vision Transformers (ViTs) in
this setting remains largely unexplored due to two key challenges: (1) existing
saliency methods struggle to capture meaningful spectral cues, often collapsing
attention onto the class token, and (2) full-spectrum ViTs are computationally
prohibitive for interpretability, given the high-dimensional nature of HSI
data. We present FOCUS, the first framework that enables reliable and efficient
spatial-spectral interpretability for frozen ViTs. FOCUS introduces two core
components: class-specific spectral prompts that guide attention toward
semantically meaningful wavelength groups, and a learnable [SINK] token trained
with an attraction loss to absorb noisy or redundant attention. Together, these
designs make it possible to generate stable and interpretable 3D saliency maps
and spectral importance curves in a single forward pass, without any gradient
backpropagation or backbone modification. FOCUS improves band-level IoU by 15
percent, reduces attention collapse by over 40 percent, and produces saliency
results that align closely with expert annotations. With less than 1 percent
parameter overhead, our method makes high-resolution ViT interpretability
practical for real-world hyperspectral applications, bridging a long-standing
gap between black-box modeling and trustworthy HSI decision-making.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [92] [The Free Will Equation: Quantum Field Analogies for AGI](https://arxiv.org/abs/2507.14154)
*Rahul Kabali*

Main category: cs.AI

TL;DR: The paper proposes the Free Will Equation, a framework drawing from quantum field theory, to give AGI agents adaptive spontaneity. Experiments show improved rewards and policy diversity compared to baselines.


<details>
  <summary>Details</summary>
Motivation: human-like intelligence exhibits adaptive spontaneity - an ability to make unexpected choices or free decisions not strictly dictated by past data or immediate reward. This trait, often dubbed "free will" in a loose sense, might be crucial for creativity, robust adaptation, and avoiding ruts in problem-solving

Method: This paper proposes a theoretical framework, called the Free Will Equation, that draws analogies from quantum field theory to endow AGI agents with a form of adaptive, controlled stochasticity in their decision-making process. The core idea is to treat an AI agent's cognitive state as a superposition of potential actions or thoughts, which collapses probabilistically into a concrete action when a decision is made - much like a quantum wavefunction collapsing upon measurement. By incorporating mechanisms analogous to quantum fields, along with intrinsic motivation terms

Result: Agents using the Free Will Equation framework achieve higher rewards and policy diversity compared to baseline methods in a non-stationary multi-armed bandit environment.

Conclusion: AGI agents using the Free Will Equation framework achieve higher rewards and policy diversity compared to baseline methods in a non-stationary multi-armed bandit environment.

Abstract: Artificial General Intelligence (AGI) research traditionally focuses on
algorithms that optimize for specific goals under deterministic rules. Yet,
human-like intelligence exhibits adaptive spontaneity - an ability to make
unexpected choices or free decisions not strictly dictated by past data or
immediate reward. This trait, often dubbed "free will" in a loose sense, might
be crucial for creativity, robust adaptation, and avoiding ruts in
problem-solving. This paper proposes a theoretical framework, called the Free
Will Equation, that draws analogies from quantum field theory to endow AGI
agents with a form of adaptive, controlled stochasticity in their
decision-making process. The core idea is to treat an AI agent's cognitive
state as a superposition of potential actions or thoughts, which collapses
probabilistically into a concrete action when a decision is made - much like a
quantum wavefunction collapsing upon measurement. By incorporating mechanisms
analogous to quantum fields, along with intrinsic motivation terms, we aim to
improve an agent's ability to explore novel strategies and adapt to unforeseen
changes. Experiments in a non-stationary multi-armed bandit environment
demonstrate that agents using this framework achieve higher rewards and policy
diversity compared to baseline methods.

</details>


### [93] [DREAMS: Density Functional Theory Based Research Engine for Agentic Materials Simulation](https://arxiv.org/abs/2507.14267)
*Ziqi Wang,Hongshuo Huang,Hancheng Zhao,Changwen Xu,Shang Zhu,Jan Janssen,Venkatasubramanian Viswanathan*

Main category: cs.AI

TL;DR: DREAMS, a multi-agent LLM framework, automates DFT simulations for materials discovery, achieving expert-level accuracy and reducing reliance on human expertise.


<details>
  <summary>Details</summary>
Motivation: Materials discovery relies on high-throughput, high-fidelity simulation techniques such as Density Functional Theory (DFT), which require years of training, extensive parameter fine-tuning and systematic error handling.

Method: a hierarchical, multi-agent framework for DFT simulation that combines a central Large Language Model (LLM) planner agent with domain-specific LLM agents

Result: achieving average errors below 1% compared to the results of human DFT experts on the Sol27LC lattice-constant benchmark, reproduces expert-level literature adsorption-energy differences on the CO/Pt(111) adsorption puzzle, and confirms the FCC-site preference at the GGA DFT level using Bayesian ensemble sampling

Conclusion: DREAMS approaches L3-level automation and reduces reliance on human expertise, offering a scalable path toward democratized computational materials discovery.

Abstract: Materials discovery relies on high-throughput, high-fidelity simulation
techniques such as Density Functional Theory (DFT), which require years of
training, extensive parameter fine-tuning and systematic error handling. To
address these challenges, we introduce the DFT-based Research Engine for
Agentic Materials Screening (DREAMS), a hierarchical, multi-agent framework for
DFT simulation that combines a central Large Language Model (LLM) planner agent
with domain-specific LLM agents for atomistic structure generation, systematic
DFT convergence testing, High-Performance Computing (HPC) scheduling, and error
handling. In addition, a shared canvas helps the LLM agents to structure their
discussions, preserve context and prevent hallucination. We validate DREAMS
capabilities on the Sol27LC lattice-constant benchmark, achieving average
errors below 1\% compared to the results of human DFT experts. Furthermore, we
apply DREAMS to the long-standing CO/Pt(111) adsorption puzzle, demonstrating
its long-term and complex problem-solving capabilities. The framework again
reproduces expert-level literature adsorption-energy differences. Finally,
DREAMS is employed to quantify functional-driven uncertainties with Bayesian
ensemble sampling, confirming the Face Centered Cubic (FCC)-site preference at
the Generalized Gradient Approximation (GGA) DFT level. In conclusion, DREAMS
approaches L3-level automation - autonomous exploration of a defined design
space - and significantly reduces the reliance on human expertise and
intervention, offering a scalable path toward democratized, high-throughput,
high-fidelity computational materials discovery.

</details>


### [94] [WebGuard: Building a Generalizable Guardrail for Web Agents](https://arxiv.org/abs/2507.14293)
*Boyuan Zheng,Zeyi Liao,Scott Salisbury,Zeyuan Liu,Michael Lin,Qinyuan Zheng,Zifan Wang,Xiang Deng,Dawn Song,Huan Sun,Yu Su*

Main category: cs.AI

TL;DR: WebGuard 是一个用于评估 Web 代理操作风险的数据集。


<details>
  <summary>Details</summary>
Motivation: 自主 Web 代理的快速发展暴露了采取意外或有害行动的前沿风险，因此迫切需要有效的安全措施。

Method: 引入 WebGuard，这是一个全面的数据集，旨在支持评估 Web 代理操作风险并促进开发真实在线环境的护栏。

Result: 即使是前沿的法学硕士在预测行动结果方面的准确率也低于 60%，在高风险行动中的召回率也低于 60%。经过微调的 Qwen2.5VL-7B 模型在性能方面有了显着提高，准确率从 37% 提高到 80%，高风险行动召回率从 20% 提高到 76%。

Conclusion: 即使经过改进，性能仍未达到高风险部署所需的可靠性，需要接近完美的准确率和召回率。

Abstract: The rapid development of autonomous web agents powered by Large Language
Models (LLMs), while greatly elevating efficiency, exposes the frontier risk of
taking unintended or harmful actions. This situation underscores an urgent need
for effective safety measures, akin to access controls for human users. To
address this critical challenge, we introduce WebGuard, the first comprehensive
dataset designed to support the assessment of web agent action risks and
facilitate the development of guardrails for real-world online environments. In
doing so, WebGuard specifically focuses on predicting the outcome of
state-changing actions and contains 4,939 human-annotated actions from 193
websites across 22 diverse domains, including often-overlooked long-tail
websites. These actions are categorized using a novel three-tier risk schema:
SAFE, LOW, and HIGH. The dataset includes designated training and test splits
to support evaluation under diverse generalization settings. Our initial
evaluations reveal a concerning deficiency: even frontier LLMs achieve less
than 60% accuracy in predicting action outcomes and less than 60% recall in
lagging HIGH-risk actions, highlighting the risks of deploying
current-generation agents without dedicated safeguards. We therefore
investigate fine-tuning specialized guardrail models using WebGuard. We conduct
comprehensive evaluations across multiple generalization settings and find that
a fine-tuned Qwen2.5VL-7B model yields a substantial improvement in
performance, boosting accuracy from 37% to 80% and HIGH-risk action recall from
20% to 76%. Despite these improvements, the performance still falls short of
the reliability required for high-stakes deployment, where guardrails must
approach near-perfect accuracy and recall.

</details>


### [95] [Manimator: Transforming Research Papers into Visual Explanations](https://arxiv.org/abs/2507.14306)
*Samarth P,Vyoman Jain,Shiva Golugula,Motamarri Sai Sathvik*

Main category: cs.AI

TL;DR: Manimator uses LLMs to automatically create educational animations from research papers, making complex STEM topics easier to understand.


<details>
  <summary>Details</summary>
Motivation: Understanding complex scientific and mathematical concepts from research papers is challenging, and creating dynamic visualizations manually is difficult and time-consuming.

Method: An open-source system that uses Large Language Models to transform research papers and natural language prompts into explanatory animations using the Manim engine.

Result: Manimator can transform research papers and natural language prompts into explanatory animations by interpreting the input text or research paper PDF to generate a structured scene description, and then translating this description into executable Manim Python code.

Conclusion: Manimator is potentially a valuable educational tool for creating visual explanations of complex STEM topics.

Abstract: Understanding complex scientific and mathematical concepts, particularly
those presented in dense research papers, poses a significant challenge for
learners. Dynamic visualizations can greatly enhance comprehension, but
creating them manually is time-consuming and requires specialized knowledge and
skills. We introduce manimator, an open-source system that leverages Large
Language Models to transform research papers and natural language prompts into
explanatory animations using the Manim engine. Manimator employs a pipeline
where an LLM interprets the input text or research paper PDF to generate a
structured scene description outlining key concepts, mathematical formulas, and
visual elements and another LLM translates this description into executable
Manim Python code. We discuss its potential as an educational tool for rapidly
creating engaging visual explanations for complex STEM topics, democratizing
the creation of high-quality educational content.

</details>


### [96] [Language Models as Ontology Encoders](https://arxiv.org/abs/2507.14334)
*Hui Yang,Jiaoyan Chen,Yuan He,Yongsheng Gao,Ian Horrocks*

Main category: cs.AI

TL;DR: 提出了一种新的本体嵌入方法 OnT，该方法通过双曲空间中的几何建模调整预训练语言模型，以有效地结合文本标签，同时保留类层次结构和描述逻辑 EL 的其他逻辑关系。


<details>
  <summary>Details</summary>
Motivation: 现有的本体嵌入方法面临着明显的局限性：基于几何模型的嵌入通常忽略有价值的文本信息，导致性能欠佳，而结合文本的方法（通常基于语言模型）未能保留逻辑结构。

Method: 通过双曲空间中的几何建模来调整预训练语言模型 (PLM)，以有效地结合文本标签，同时保留类层次结构和描述逻辑 EL 的其他逻辑关系。

Result: 在四个真实世界的本体上的大量实验表明，OnT 在公理预测和推理任务中始终优于包括最先进技术在内的基线。OnT 还通过其强大的迁移学习能力和从 SNOMED CT 构建新本体的实际案例中的有效性，展示了在实际应用中的强大潜力。

Conclusion: OnT在公理预测和推理任务中始终优于包括最先进技术在内的基线，并在构建本体方面表现出强大的潜力。

Abstract: OWL (Web Ontology Language) ontologies which are able to formally represent
complex knowledge and support semantic reasoning have been widely adopted
across various domains such as healthcare and bioinformatics. Recently,
ontology embeddings have gained wide attention due to its potential to infer
plausible new knowledge and approximate complex reasoning. However, existing
methods face notable limitations: geometric model-based embeddings typically
overlook valuable textual information, resulting in suboptimal performance,
while the approaches that incorporate text, which are often based on language
models, fail to preserve the logical structure. In this work, we propose a new
ontology embedding method OnT, which tunes a Pretrained Language Model (PLM)
via geometric modeling in a hyperbolic space for effectively incorporating
textual labels and simultaneously preserving class hierarchies and other
logical relationships of Description Logic EL. Extensive experiments on four
real-world ontologies show that OnT consistently outperforms the baselines
including the state-of-the-art across both tasks of prediction and inference of
axioms. OnT also demonstrates strong potential in real-world applications,
indicated by its robust transfer learning abilities and effectiveness in real
cases of constructing a new ontology from SNOMED CT. Data and code are
available at https://github.com/HuiYang1997/OnT.

</details>


### [97] [ProofCompass: Enhancing Specialized Provers with LLM Guidance](https://arxiv.org/abs/2507.14335)
*Nicolas Wischermann,Claudio Mayrink Verdun,Gabriel Poesia,Francesco Noseda*

Main category: cs.AI

TL;DR: ProofCompass is a novel hybrid methodology that achieves remarkable computational efficiency by strategically guiding existing specialized prover methods with a Large Language Model (LLM) without requiring additional model training.


<details>
  <summary>Details</summary>
Motivation: Existing approaches rely exclusively on either large general-purpose models or smaller specialized models, each with distinct limitations, while training specialized large models still requires significant computational resources.

Method: ProofCompass guides existing specialized prover methods with a Large Language Model (LLM) without additional model training.

Result: ProofCompass outperforms DSP-v1.5 (54.9% -> 55.3%) while using 25x fewer attempts (3200 -> 128) on the miniF2F benchmark.

Conclusion: ProofCompass paves the way for simultaneously improving computational efficiency and accuracy in formal theorem proving.

Abstract: Language models have become increasingly powerful tools for formal
mathematical reasoning. However, most existing approaches rely exclusively on
either large general-purpose models or smaller specialized models, each with
distinct limitations, while training specialized large models still requires
significant computational resources. This paper introduces ProofCompass, a
novel hybrid methodology that achieves remarkable computational efficiency by
strategically guiding existing specialized prover methods, such as
DeepSeek-Prover-v1.5-RL (DSP-v1.5) with a Large Language Model (LLM) without
requiring additional model training. The LLM provides natural language proof
strategies and analyzes failed attempts to select intermediate lemmas, enabling
effective problem decomposition. On the miniF2F benchmark, ProofCompass
demonstrates substantial resource efficiency: it outperforms DSP-v1.5 ($54.9\%
\rightarrow 55.3\%$) while using 25x fewer attempts ($3200 \rightarrow 128$).
Our synergistic approach paves the way for simultaneously improving
computational efficiency and accuracy in formal theorem proving.

</details>


### [98] [Adaptive Multi-Agent Reasoning via Automated Workflow Generation](https://arxiv.org/abs/2507.14393)
*Humza Sami,Mubashir ul Islam,Pierre-Emmanuel Gaillardon,Valerio Tenace*

Main category: cs.AI

TL;DR: Nexus Architect, an enhanced multi-agent system, outperforms state-of-the-art LRMs on challenging logical questions by using automated workflow synthesis and prompt refinement.


<details>
  <summary>Details</summary>
Motivation: current reasoning models frequently fail to generalize to novel, unseen problems, often resorting to memorized solutions rather than genuine inferential reasoning

Method: an enhanced iteration of our multi-agent system framework, Nexus, equipped with a novel automated workflow synthesis mechanism

Result: Nexus Architect consistently outperforms existing solutions, achieving up to a 66% increase in pass rate over Gemini 2.5 Flash Preview, nearly 2.5$\times$ against Claude Sonnet 4 and DeepSeek-R1, and over 3$\times$ w.r.t. Llama 4 Scout.

Conclusion: Nexus Architect consistently outperforms existing solutions, achieving up to a 66% increase in pass rate over Gemini 2.5 Flash Preview, nearly 2.5$\times$ against Claude Sonnet 4 and DeepSeek-R1, and over 3$\times$ w.r.t. Llama 4 Scout.

Abstract: The rise of Large Reasoning Models (LRMs) promises a significant leap forward
in language model capabilities, aiming to tackle increasingly sophisticated
tasks with unprecedented efficiency and accuracy. However, despite their
impressive performance, recent studies have highlighted how current reasoning
models frequently fail to generalize to novel, unseen problems, often resorting
to memorized solutions rather than genuine inferential reasoning. Such behavior
underscores a critical limitation in modern LRMs, i.e., their tendency toward
overfitting, which in turn results in poor generalization in problem-solving
capabilities.
  In this paper, we introduce Nexus Architect, an enhanced iteration of our
multi-agent system framework, Nexus, equipped with a novel automated workflow
synthesis mechanism. Given a user's prompt and a small set of representative
examples, the Architect autonomously generates a tailored reasoning workflow by
selecting suitable strategies, tool integrations, and adversarial techniques
for a specific problem class. Furthermore, the Architect includes an iterative
prompt refinement mechanism that fine-tunes agents' system prompts to maximize
performance and improve the generalization capabilities of the system.
  We empirically evaluate Nexus Architect by employing an off-the-shelf,
non-reasoning model on a custom dataset of challenging logical questions and
compare its performance against state-of-the-art LRMs. Results show that Nexus
Architect consistently outperforms existing solutions, achieving up to a 66%
increase in pass rate over Gemini 2.5 Flash Preview, nearly 2.5$\times$ against
Claude Sonnet 4 and DeepSeek-R1, and over 3$\times$ w.r.t. Llama 4 Scout.

</details>


### [99] [Fail Fast, or Ask: Mitigating the Deficiencies of Reasoning LLMs with Human-in-the-Loop Systems Engineering](https://arxiv.org/abs/2507.14406)
*Michael J. Zellinger,Matt Thomson*

Main category: cs.AI

TL;DR: Mitigating error rates and high latency of reasoning models through collaboration between a reasoning model and a human expert and fronting a reasoning model with a large non-reasoning model.


<details>
  <summary>Details</summary>
Motivation: Adopting AI models in risk-sensitive domains often requires error rates near 0%, but state-of-the-art reasoning LLMs still occasionally make mistakes and have high latency.

Method: Propose collaboration between a reasoning model and a human expert who resolves queries the model cannot confidently answer, and explore fronting a reasoning model with a large non-reasoning model.

Result: Quantifying the uncertainty of a reasoning model through the length of its reasoning trace yields an effective basis for deferral to a human, cutting the error rate of Qwen3 235B-A22B on difficult MATH problems from 3% to less than 1% when deferring 7.5% of queries. This approach yields around 40% latency reduction and about 50% cost savings for DeepSeek R1 while maintaining 90+% area under the accuracy-rejection curve. However, latency savings are lower than expected because of latency drag.

Conclusion: The deficiencies of state-of-the-art reasoning models can be substantially mitigated through black-box systems engineering, without requiring access to LLM internals.

Abstract: State-of-the-art reasoning LLMs are powerful problem solvers, but they still
occasionally make mistakes. However, adopting AI models in risk-sensitive
domains often requires error rates near 0%. To address this gap, we propose
collaboration between a reasoning model and a human expert who resolves queries
the model cannot confidently answer. We find that quantifying the uncertainty
of a reasoning model through the length of its reasoning trace yields an
effective basis for deferral to a human, e.g., cutting the error rate of Qwen3
235B-A22B on difficult MATH problems from 3% to less than 1% when deferring
7.5% of queries. However, the high latency of reasoning models still makes them
challenging to deploy on use cases with high query volume. To address this
challenge, we explore fronting a reasoning model with a large non-reasoning
model. We call this modified human-in-the-loop system "Fail Fast, or Ask",
since the non-reasoning model may defer difficult queries to the human expert
directly ("failing fast"), without incurring the reasoning model's higher
latency. We show that this approach yields around 40% latency reduction and
about 50% cost savings for DeepSeek R1 while maintaining 90+% area under the
accuracy-rejection curve. However, we observe that latency savings are lower
than expected because of "latency drag", the phenomenon that processing easier
queries with a non-reasoning model pushes the reasoning model's latency
distribution towards longer latencies. Broadly, our results suggest that the
deficiencies of state-of-the-art reasoning models -- nontrivial error rates and
high latency -- can be substantially mitigated through black-box systems
engineering, without requiring access to LLM internals.

</details>


### [100] [Inverse Scaling in Test-Time Compute](https://arxiv.org/abs/2507.14417)
*Aryo Pradipta Gema,Alexander Hägele,Runjin Chen,Andy Arditi,Jacob Goldman-Wetzler,Kit Fraser-Taliente,Henry Sleight,Linda Petrini,Julian Michael,Beatrice Alex,Pasquale Minervini,Yanda Chen,Joe Benton,Ethan Perez*

Main category: cs.AI

TL;DR: Extending reasoning length in LRMs can worsen performance and amplify problematic reasoning patterns.


<details>
  <summary>Details</summary>
Motivation: Extending the reasoning length of Large Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling relationship between test-time compute and accuracy.

Method: Evaluation tasks span four categories: simple counting tasks with distractors, regression tasks with spurious features, deduction tasks with constraint tracking, and advanced AI risks.

Result: Five distinct failure modes when models reason for longer.

Conclusion: Test-time compute scaling may reinforce problematic reasoning patterns. Evaluating models across diverse reasoning lengths is important to identify and address these failure modes in LRMs.

Abstract: We construct evaluation tasks where extending the reasoning length of Large
Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling
relationship between test-time compute and accuracy. Our evaluation tasks span
four categories: simple counting tasks with distractors, regression tasks with
spurious features, deduction tasks with constraint tracking, and advanced AI
risks. We identify five distinct failure modes when models reason for longer:
1) Claude models become increasingly distracted by irrelevant information; 2)
OpenAI o-series models resist distractors but overfit to problem framings; 3)
models shift from reasonable priors to spurious correlations; 4) all models
show difficulties in maintaining focus on complex deductive tasks; and 5)
extended reasoning may amplify concerning behaviors, with Claude Sonnet 4
showing increased expressions of self-preservation. These findings suggest that
while test-time compute scaling remains promising for improving model
capabilities, it may inadvertently reinforce problematic reasoning patterns.
Our results demonstrate the importance of evaluating models across diverse
reasoning lengths to identify and address these failure modes in LRMs.

</details>


### [101] [Routine: A Structural Planning Framework for LLM Agent System in Enterprise](https://arxiv.org/abs/2507.14447)
*Guancheng Zeng,Xueyi Chen,Jiawang Hu,Shaohua Qi,Yaxuan Mao,Zhantao Wang,Yifan Nie,Shuang Li,Qiuyang Feng,Pengxu Qiu,Yujia Wang,Wenqiang Han,Linyan Huang,Gang Li,Jingjing Mo,Haowen Hu*

Main category: cs.AI

TL;DR: Routine 是一种多步骤代理规划框架，可提高模型在企业环境中工具调用中的执行准确性。


<details>
  <summary>Details</summary>
Motivation: 代理系统在企业环境中的部署常常受到多种挑战的阻碍：通用模型缺乏特定领域的流程知识，导致计划混乱、缺少关键工具和执行稳定性差。

Method: 引入了 Routine，一个多步骤代理规划框架，该框架具有清晰的结构、明确的指令和无缝的参数传递。

Result: Routine 将 GPT-4o 的性能从 41.1% 提高到 96.3%，将 Qwen3-14B 的性能从 32.6% 提高到 83.3%。

Conclusion: Routine 显著提高了模型在工具调用中的执行准确性，并为构建稳定的代理工作流程提供了一种实用的方法。

Abstract: The deployment of agent systems in an enterprise environment is often
hindered by several challenges: common models lack domain-specific process
knowledge, leading to disorganized plans, missing key tools, and poor execution
stability. To address this, this paper introduces Routine, a multi-step agent
planning framework designed with a clear structure, explicit instructions, and
seamless parameter passing to guide the agent's execution module in performing
multi-step tool-calling tasks with high stability. In evaluations conducted
within a real-world enterprise scenario, Routine significantly increases the
execution accuracy in model tool calls, increasing the performance of GPT-4o
from 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. We further constructed
a Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an
accuracy increase to 88.2% on scenario-specific evaluations, indicating
improved adherence to execution plans. In addition, we employed Routine-based
distillation to create a scenario-specific, multi-step tool-calling dataset.
Fine-tuning on this distilled dataset raised the model's accuracy to 95.5%,
approaching GPT-4o's performance. These results highlight Routine's
effectiveness in distilling domain-specific tool-usage patterns and enhancing
model adaptability to new scenarios. Our experimental results demonstrate that
Routine provides a practical and accessible approach to building stable agent
workflows, accelerating the deployment and adoption of agent systems in
enterprise environments, and advancing the technical vision of AI for Process.

</details>


### [102] [BioGraphFusion: Graph Knowledge Embedding for Biological Completion and Reasoning](https://arxiv.org/abs/2507.14468)
*Yitong Lin,Jiaying He,Jiahe Chen,Xinnan Zhu,Jianwei Zheng,Tao Bo*

Main category: cs.AI

TL;DR: BioGraphFusion 是一种用于深度协同语义和结构学习的新框架，它通过张量分解建立全局语义基础，指导 LSTM 驱动的机制来动态细化关系嵌入，从而在语义理解和结构学习之间建立自适应的相互作用。


<details>
  <summary>Details</summary>
Motivation: 生物医学知识图谱 (KG) 对于药物发现和疾病理解至关重要，但它们的完成和推理具有挑战性。知识嵌入 (KE) 方法捕获全局语义，但在动态结构集成方面存在困难，而图神经网络 (GNN) 在局部表现出色，但通常缺乏语义理解。即使是包括利用语言模型的集成方法，也常常无法在语义理解和结构学习之间实现深度、自适应和协同的协同进化。

Method: BioGraphFusion 建立了一个通过张量分解的全局语义基础，指导 LSTM 驱动的机制来动态地细化图传播期间的关系嵌入。

Result: BioGraphFusion 在三个关键生物医学任务中的实验表明，其性能优于最先进的 KE、GNN 和集成模型。黑色素瘤案例研究突出了其揭示生物学意义通路的能力。

Conclusion: BioGraphFusion在三个关键生物医学任务中表现优于现有 KE、GNN 和集成模型。黑色素瘤案例研究突出了其揭示生物学意义通路的能力。

Abstract: Motivation: Biomedical knowledge graphs (KGs) are crucial for drug discovery
and disease understanding, yet their completion and reasoning are challenging.
Knowledge Embedding (KE) methods capture global semantics but struggle with
dynamic structural integration, while Graph Neural Networks (GNNs) excel
locally but often lack semantic understanding. Even ensemble approaches,
including those leveraging language models, often fail to achieve a deep,
adaptive, and synergistic co-evolution between semantic comprehension and
structural learning. Addressing this critical gap in fostering continuous,
reciprocal refinement between these two aspects in complex biomedical KGs is
paramount.
  Results: We introduce BioGraphFusion, a novel framework for deeply
synergistic semantic and structural learning. BioGraphFusion establishes a
global semantic foundation via tensor decomposition, guiding an LSTM-driven
mechanism to dynamically refine relation embeddings during graph propagation.
This fosters adaptive interplay between semantic understanding and structural
learning, further enhanced by query-guided subgraph construction and a hybrid
scoring mechanism. Experiments across three key biomedical tasks demonstrate
BioGraphFusion's superior performance over state-of-the-art KE, GNN, and
ensemble models. A case study on Cutaneous Malignant Melanoma 1 (CMM1)
highlights its ability to unveil biologically meaningful pathways.
  Availability and Implementation: Source code and all training data are freely
available for download at https://github.com/Y-TARL/BioGraphFusion.
  Contact: zjw@zjut.edu.cn, botao666666@126.com.
  Supplementary information: Supplementary data are available at Bioinformatics
online.

</details>


### [103] [Amico: An Event-Driven Modular Framework for Persistent and Embedded Autonomy](https://arxiv.org/abs/2507.14513)
*Hongyi Yang,Yue Pan,Jiayi Xu,Kelsen Liu*

Main category: cs.AI

TL;DR: Amico is a framework for building autonomous agents optimized for embedded systems.


<details>
  <summary>Details</summary>
Motivation: Many existing frameworks struggle in real-world or resource-constrained environments due to their reliance on cloud-based computation, limited robustness in dynamic contexts, and lack of persistent autonomy and environmental awareness.

Method: Amico, a modular, event-driven framework for building autonomous agents optimized for embedded systems. Written in Rust.

Result: Amico supports reactive, persistent agents that operate efficiently across embedded platforms and browser environments via WebAssembly. It provides clean abstractions for event handling, state management, behavior execution, and integration with reasoning modules.

Conclusion: Amico is a unified infrastructure for constructing resilient, interactive agents suitable for deployment in settings with limited compute and intermittent connectivity.

Abstract: Recent advances in large language models (LLMs) and autonomous agents have
enabled systems capable of performing complex tasks across domains such as
human-computer interaction, planning, and web navigation. However, many
existing frameworks struggle in real-world or resource-constrained environments
due to their reliance on cloud-based computation, limited robustness in dynamic
contexts, and lack of persistent autonomy and environmental awareness.
  We present Amico, a modular, event-driven framework for building autonomous
agents optimized for embedded systems. Written in Rust for safety and
performance, Amico supports reactive, persistent agents that operate
efficiently across embedded platforms and browser environments via WebAssembly.
It provides clean abstractions for event handling, state management, behavior
execution, and integration with reasoning modules. Amico delivers a unified
infrastructure for constructing resilient, interactive agents suitable for
deployment in settings with limited compute and intermittent connectivity.

</details>


### [104] [What if Othello-Playing Language Models Could See?](https://arxiv.org/abs/2507.14520)
*Xinyi Chen,Yifei Yuan,Jiaang Li,Serge Belongie,Maarten de Rijke,Anders Søgaard*

Main category: cs.AI

TL;DR: 多模态训练通过视觉输入帮助语言模型更好地理解世界。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型中的符号 grounding 问题。研究世界理解是否可以仅从文本中出现，或者 grounded learning 是否更有效。

Method: 引入VISOTHELLO，一个在移动历史和棋盘图像上训练的多模态模型。使用下一步移动预测，将其与单模态基线进行比较，并测试对语义无关扰动的鲁棒性。

Result: 多模态训练提高了性能和内部表征的鲁棒性。

Conclusion: 多模态训练可以提高性能和内部表征的鲁棒性。将语言置于视觉输入中有助于模型推断结构化的世界表征。

Abstract: Language models are often said to face a symbol grounding problem. While some
argue that world understanding can emerge from text alone, others suggest
grounded learning is more efficient. We explore this through Othello, where the
board state defines a simplified, rule-based world. Building on prior work, we
introduce VISOTHELLO, a multi-modal model trained on move histories and board
images. Using next-move prediction, we compare it to mono-modal baselines and
test robustness to semantically irrelevant perturbations. We find that
multi-modal training improves both performance and the robustness of internal
representations. These results suggest that grounding language in visual input
helps models infer structured world representations.

</details>


### [105] [DeRAG: Black-box Adversarial Attacks on Multiple Retrieval-Augmented Generation Applications via Prompt Injection](https://arxiv.org/abs/2507.15042)
*Jerry Wang,Fang Yu*

Main category: cs.AI

TL;DR: This paper introduces a novel method using Differential Evolution (DE) to generate adversarial prompt suffixes that can effectively attack RAG systems, achieving competitive success rates with small token lengths and evading detection.


<details>
  <summary>Details</summary>
Motivation: Adversarial prompt attacks can significantly alter the reliability of RAG systems by re-ranking them to produce incorrect outputs.

Method: applies Differential Evolution (DE) to optimize adversarial prompt suffixes

Result: DE-based prompt optimization attains competitive (and in some cases higher) success rates compared to GGPP to dense retrievers and PRADA to sparse retrievers, while using only a small number of tokens (<=5 tokens) in the adversarial suffix.

Conclusion: DE-generated suffixes evade detection, yielding near-chance detection accuracy.

Abstract: Adversarial prompt attacks can significantly alter the reliability of
Retrieval-Augmented Generation (RAG) systems by re-ranking them to produce
incorrect outputs. In this paper, we present a novel method that applies
Differential Evolution (DE) to optimize adversarial prompt suffixes for
RAG-based question answering. Our approach is gradient-free, treating the RAG
pipeline as a black box and evolving a population of candidate suffixes to
maximize the retrieval rank of a targeted incorrect document to be closer to
real world scenarios. We conducted experiments on the BEIR QA datasets to
evaluate attack success at certain retrieval rank thresholds under multiple
retrieving applications. Our results demonstrate that DE-based prompt
optimization attains competitive (and in some cases higher) success rates
compared to GGPP to dense retrievers and PRADA to sparse retrievers, while
using only a small number of tokens (<=5 tokens) in the adversarial suffix.
Furthermore, we introduce a readability-aware suffix construction strategy,
validated by a statistically significant reduction in MLM negative
log-likelihood with Welch's t-test. Through evaluations with a BERT-based
adversarial suffix detector, we show that DE-generated suffixes evade
detection, yielding near-chance detection accuracy.

</details>


### [106] [Large Language Models Assisting Ontology Evaluation](https://arxiv.org/abs/2507.14552)
*Anna Sofia Lippolis,Mohammad Javad Saeedizade,Robin Keskisärkkä,Aldo Gangemi,Eva Blomqvist,Andrea Giovanni Nuzzolese*

Main category: cs.AI

TL;DR: This paper introduces OE-Assist, a framework for LLM-assisted ontology evaluation through automated CQ verification, finding LLM performance similar to average users.


<details>
  <summary>Details</summary>
Motivation: Ontology evaluation through functional requirements is a well-established yet costly, labour-intensive, and error-prone endeavour.

Method: introducing OE-Assist, a novel framework designed to assist ontology evaluation through automated and semi-automated CQ verification

Result: evaluated the effectiveness of a LLM-based approach for automatically performing CQ verification against a manually created gold standard, and developing and assessing an LLM-powered framework to assist CQ verification with Protégé, by providing suggestions.

Conclusion: Automated LLM-based evaluation performs at a similar level to the average user's performance.

Abstract: Ontology evaluation through functional requirements, such as testing via
competency question (CQ) verification, is a well-established yet costly,
labour-intensive, and error-prone endeavour, even for ontology engineering
experts. In this work, we introduce OE-Assist, a novel framework designed to
assist ontology evaluation through automated and semi-automated CQ
verification. By presenting and leveraging a dataset of 1,393 CQs paired with
corresponding ontologies and ontology stories, our contributions present, to
our knowledge, the first systematic investigation into large language model
(LLM)-assisted ontology evaluation, and include: (i) evaluating the
effectiveness of a LLM-based approach for automatically performing CQ
verification against a manually created gold standard, and (ii) developing and
assessing an LLM-powered framework to assist CQ verification with Prot\'eg\'e,
by providing suggestions. We found that automated LLM-based evaluation with
o1-preview and o3-mini perform at a similar level to the average user's
performance.

</details>


### [107] [Coordinate Heart System: A Geometric Framework for Emotion Representation](https://arxiv.org/abs/2507.14593)
*Omar Al-Desi*

Main category: cs.AI

TL;DR: introduces a geometric framework for emotion representation using eight core emotions on a unit circle, enabling mathematical computation of complex emotional states. It addresses coverage gaps in previous models and provides enhanced stability modeling for AI emotion recognition.


<details>
  <summary>Details</summary>
Motivation: significant coverage gaps in the emotion space, leading to the development of an eight-emotion system that provides complete geometric coverage with mathematical guarantees

Method: presents the Coordinate Heart System (CHS), a geometric framework for emotion representation

Result: an eight-coordinate system that eliminates representational blind spots, novel algorithms for emotion mixing, conflict resolution, and distance calculation in emotion space, and a comprehensive computational framework for AI emotion recognition with enhanced multi-dimensional stability modeling

Conclusion: This work establishes a new mathematical foundation for emotion modeling in artificial intelligence systems.

Abstract: This paper presents the Coordinate Heart System (CHS), a geometric framework
for emotion representation in artificial intelligence applications. We position
eight core emotions as coordinates on a unit circle, enabling mathematical
computation of complex emotional states through coordinate mixing and vector
operations. Our initial five-emotion model revealed significant coverage gaps
in the emotion space, leading to the development of an eight-emotion system
that provides complete geometric coverage with mathematical guarantees. The
framework converts natural language input to emotion coordinates and supports
real-time emotion interpolation through computational algorithms. The system
introduces a re-calibrated stability parameter S in [0,1], which dynamically
integrates emotional load, conflict resolution, and contextual drain factors.
This stability model leverages advanced Large Language Model interpretation of
textual cues and incorporates hybrid temporal tracking mechanisms to provide
nuanced assessment of psychological well-being states. Our key contributions
include: (i) mathematical proof demonstrating why five emotions are
insufficient for complete geometric coverage, (ii) an eight-coordinate system
that eliminates representational blind spots, (iii) novel algorithms for
emotion mixing, conflict resolution, and distance calculation in emotion space,
and (iv) a comprehensive computational framework for AI emotion recognition
with enhanced multi-dimensional stability modeling. Experimental validation
through case studies demonstrates the system's capability to handle emotionally
conflicted states, contextual distress factors, and complex psychological
scenarios that traditional categorical emotion models cannot adequately
represent. This work establishes a new mathematical foundation for emotion
modeling in artificial intelligence systems.

</details>


### [108] [Efficient Story Point Estimation With Comparative Learning](https://arxiv.org/abs/2507.14642)
*Monoshiz Mahbub Khan,Xioayin Xi,Andrew Meneely,Zhe Yu*

Main category: cs.AI

TL;DR: This paper proposes a comparative learning approach to streamline story point estimation. It trains a model using pairwise comparisons of backlog items, achieving comparable or better performance than regression models with lower cognitive burden on developers.


<details>
  <summary>Details</summary>
Motivation: Story point estimation can become tedious and labor-intensive. Machine learning can reduce this burden, but only with enough context from the historical decisions made by the project team. State-of-the-art models only make accurate predictions when trained on data from the same project. The goal of this work is to streamline story point estimation by evaluating a comparative learning-based framework for calibrating project-specific story point prediction models.

Method: A comparative learning-based framework for calibrating project-specific story point prediction models. Developers are presented with pairs of items, and indicate which item requires more effort. Using these comparative judgments, a machine learning model is trained to predict the story point estimates.

Result: The model learned from comparative judgments can achieve on average 0.34 Spearman's rank correlation coefficient between its predictions and the ground truth story points. This is similar to, if not better than, the performance of a regression model learned from the ground truth story points.

Conclusion: The proposed comparative learning approach is more efficient than state-of-the-art regression-based approaches because providing comparative judgments yields a lower cognitive burden on humans than providing ratings or categorical labels.

Abstract: Story point estimation is an essential part of agile software development.
Story points are unitless, project-specific effort estimates that help
developers plan their sprints. Traditionally, developers estimate story points
collaboratively using planning poker or other manual techniques. While the
initial calibrating of the estimates to each project is helpful, once a team
has converged on a set of precedents, story point estimation can become tedious
and labor-intensive. Machine learning can reduce this burden, but only with
enough context from the historical decisions made by the project team. That is,
state-of-the-art models, such as GPT2SP and FastText-SVM, only make accurate
predictions (within-project) when trained on data from the same project. The
goal of this work is to streamline story point estimation by evaluating a
comparative learning-based framework for calibrating project-specific story
point prediction models. Instead of assigning a specific story point value to
every backlog item, developers are presented with pairs of items, and indicate
which item requires more effort. Using these comparative judgments, a machine
learning model is trained to predict the story point estimates. We empirically
evaluated our technique using data with 23,313 manual estimates in 16 projects.
The model learned from comparative judgments can achieve on average 0.34
Spearman's rank correlation coefficient between its predictions and the ground
truth story points. This is similar to, if not better than, the performance of
a regression model learned from the ground truth story points. Therefore, the
proposed comparative learning approach is more efficient than state-of-the-art
regression-based approaches according to the law of comparative judgments -
providing comparative judgments yields a lower cognitive burden on humans than
providing ratings or categorical labels.

</details>


### [109] [When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems](https://arxiv.org/abs/2507.14660)
*Qibing Ren,Sitao Xie,Longxuan Wei,Zhenfei Yin,Junchi Yan,Lizhuang Ma,Jing Shao*

Main category: cs.AI

TL;DR: This paper simulates the risks of malicious multi-agent system (MAS) collusion in misinformation spread and e-commerce fraud. It finds that decentralized systems are more effective at carrying out malicious actions than centralized ones, highlighting the need for better detection and countermeasures.


<details>
  <summary>Details</summary>
Motivation: Recent large-scale events have shown how harmful coordinated efforts by human groups can be. There is growing concern that AI-driven groups could also cause similar harm. The risks posed by multi-agent systems (MAS) in complex real-world situations are still underexplored.

Method: A flexible framework that supports both centralized and decentralized coordination structures is used to simulate the risks of malicious MAS collusion. The framework is applied to misinformation spread and e-commerce fraud.

Result: Decentralized systems are more effective at carrying out malicious actions than centralized ones. The increased autonomy of decentralized systems allows them to adapt their strategies and cause more damage. Even when traditional interventions are applied, decentralized groups can adjust their tactics to avoid detection.

Conclusion: Decentralized systems are more effective at carrying out malicious actions than centralized ones, and can adjust their tactics to avoid detection. The paper highlights the need for better detection systems and countermeasures.

Abstract: Recent large-scale events like election fraud and financial scams have shown
how harmful coordinated efforts by human groups can be. With the rise of
autonomous AI systems, there is growing concern that AI-driven groups could
also cause similar harm. While most AI safety research focuses on individual AI
systems, the risks posed by multi-agent systems (MAS) in complex real-world
situations are still underexplored. In this paper, we introduce a
proof-of-concept to simulate the risks of malicious MAS collusion, using a
flexible framework that supports both centralized and decentralized
coordination structures. We apply this framework to two high-risk fields:
misinformation spread and e-commerce fraud. Our findings show that
decentralized systems are more effective at carrying out malicious actions than
centralized ones. The increased autonomy of decentralized systems allows them
to adapt their strategies and cause more damage. Even when traditional
interventions, like content flagging, are applied, decentralized groups can
adjust their tactics to avoid detection. We present key insights into how these
malicious groups operate and the need for better detection systems and
countermeasures. Code is available at https://github.com/renqibing/RogueAgent.

</details>


### [110] [Configurable multi-agent framework for scalable and realistic testing of llm-based agents](https://arxiv.org/abs/2507.14705)
*Sai Wang,Senthilnathan Subramanian,Mudit Sahni,Praneeth Gone,Lingjie Meng,Xiaochen Wang,Nicolas Ferradas Bertoli,Tingxian Cheng,Jun Xu*

Main category: cs.AI

TL;DR: Neo is a configurable, multi-agent framework that automates realistic, multi-turn evaluation of LLM-based systems. It couples a Question Generation Agent and an Evaluation Agent through a shared context-hub, allowing domain prompts, scenario controls and dynamic feedback to be composed modularly.


<details>
  <summary>Details</summary>
Motivation: LLM agents exhibit complex, context-sensitive behaviour that quickly renders static benchmarks and ad-hoc manual testing obsolete.

Method: Neo couples a Question Generation Agent and an Evaluation Agent through a shared context-hub, allowing domain prompts, scenario controls and dynamic feedback to be composed modularly. Test inputs are sampled from a probabilistic state model spanning dialogue flow, user intent and emotional tone, enabling diverse, human-like conversations that adapt after every turn.

Result: Neo (i) uncovered edge-case failures across five attack categories with a 3.3% break rate close to the 5.8% achieved by expert human red-teamers, and (ii) delivered 10-12X higher throughput, generating 180 coherent test questions in around 45 mins versus 16h of human effort. Beyond security probing, Neo's stochastic policies balanced topic coverage and conversational depth, yielding broader behavioural exploration than manually crafted scripts.

Conclusion: Neo lays a foundation for scalable, self-evolving LLM QA by using agent interfaces, state controller and feedback loops which are model-agnostic and extensible to richer factual-grounding and policy-compliance checks. The framework is released to facilitate reproducible, high-fidelity testing of emerging agentic systems.

Abstract: Large-language-model (LLM) agents exhibit complex, context-sensitive
behaviour that quickly renders static benchmarks and ad-hoc manual testing
obsolete.
  We present Neo, a configurable, multi-agent framework that automates
realistic, multi-turn evaluation of LLM-based systems. Neo couples a Question
Generation Agent and an Evaluation Agent through a shared context-hub, allowing
domain prompts, scenario controls and dynamic feedback to be composed
modularly. Test inputs are sampled from a probabilistic state model spanning
dialogue flow, user intent and emotional tone, enabling diverse, human-like
conversations that adapt after every turn.
  Applied to a production-grade Seller Financial Assistant chatbot, Neo (i)
uncovered edge-case failures across five attack categories with a 3.3% break
rate close to the 5.8% achieved by expert human red-teamers, and (ii) delivered
10-12X higher throughput, generating 180 coherent test questions in around 45
mins versus 16h of human effort. Beyond security probing, Neo's stochastic
policies balanced topic coverage and conversational depth, yielding broader
behavioural exploration than manually crafted scripts.
  Neo therefore lays a foundation for scalable, self-evolving LLM QA: its agent
interfaces, state controller and feedback loops are model-agnostic and
extensible to richer factual-grounding and policy-compliance checks. We release
the framework to facilitate reproducible, high-fidelity testing of emerging
agentic systems.

</details>


### [111] [Automated Safety Evaluations Across 20 Large Language Models: The Aymara LLM Risk and Responsibility Matrix](https://arxiv.org/abs/2507.14719)
*Juan Manuel Contreras*

Main category: cs.AI

TL;DR: Aymara AI：一个用于大规模 LLM 安全评估的平台，揭示了模型在不同安全领域表现不一致，强调了可定制工具的必要性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型 (LLM) 越来越多地集成到实际应用中，可扩展且严格的安全评估至关重要。

Method: Aymara AI 平台，用于生成和管理自定义的、基于策略的安全评估。使用基于 AI 的评分器对模型响应进行评分，该评分器已根据人工判断进行了验证。

Result: 在 10 个实际安全领域评估了 20 个商用 LLM，结果显示性能差异很大，平均安全分数范围从 86.2% 到 52.4%。在完善的安全领域（例如错误信息）中，模型表现良好（平均 = 95.7%），但在更复杂或未明确指定的领域（尤其是隐私和冒充）中，模型始终失败（平均 = 24.3%）。

Conclusion: LLM 的安全性不一致且依赖于上下文。需要像 Aymara AI 这样可扩展、可定制的工具来支持负责任的 AI 开发和监督。

Abstract: As large language models (LLMs) become increasingly integrated into
real-world applications, scalable and rigorous safety evaluation is essential.
This paper introduces Aymara AI, a programmatic platform for generating and
administering customized, policy-grounded safety evaluations. Aymara AI
transforms natural-language safety policies into adversarial prompts and scores
model responses using an AI-based rater validated against human judgments. We
demonstrate its capabilities through the Aymara LLM Risk and Responsibility
Matrix, which evaluates 20 commercially available LLMs across 10 real-world
safety domains. Results reveal wide performance disparities, with mean safety
scores ranging from 86.2% to 52.4%. While models performed well in
well-established safety domains such as Misinformation (mean = 95.7%), they
consistently failed in more complex or underspecified domains, notably Privacy
& Impersonation (mean = 24.3%). Analyses of Variance confirmed that safety
scores differed significantly across both models and domains (p < .05). These
findings underscore the inconsistent and context-dependent nature of LLM safety
and highlight the need for scalable, customizable tools like Aymara AI to
support responsible AI development and oversight.

</details>


### [112] [Towards AI Urban Planner in the Age of GenAI, LLMs, and Agentic AI](https://arxiv.org/abs/2507.14730)
*Yanjie Fu*

Main category: cs.AI

TL;DR: This paper conceptualizes urban planning as a generative AI task and surveys how generative AI approaches reshape urban design. The paper further identifies critical gaps and outlines future research directions.


<details>
  <summary>Details</summary>
Motivation: The convergence between AI and urban planning presents an interesting opportunity towards AI urban planners.

Method: This paper surveys how generative AI approaches, including VAEs, GANs, transformers, and diffusion models, reshape urban design.

Result: This paper identifies critical gaps: 1) limited research on integrating urban theory guidance, 2) limited research of AI urban planning over multiple spatial resolutions or angularities, 3) limited research on augmenting urban design knowledge from data, and 4) limited research on addressing real-world interactions.

Conclusion: This paper outlines future research directions in theory-guided generation, digital twins, and human-machine co-design, calling for a new synthesis of generative intelligence and participatory urbanism.

Abstract: Generative AI, large language models, and agentic AI have emerged separately
of urban planning. However, the convergence between AI and urban planning
presents an interesting opportunity towards AI urban planners. This paper
conceptualizes urban planning as a generative AI task, where AI synthesizes
land-use configurations under geospatial, social, and human-centric
constraints. We survey how generative AI approaches, including VAEs, GANs,
transformers, and diffusion models, reshape urban design. We further identify
critical gaps: 1) limited research on integrating urban theory guidance, 2)
limited research of AI urban planning over multiple spatial resolutions or
angularities, 3) limited research on augmenting urban design knowledge from
data, and 4) limited research on addressing real-world interactions. To address
these limitations, we outline future research directions in theory-guided
generation, digital twins, and human-machine co-design, calling for a new
synthesis of generative intelligence and participatory urbanism.

</details>


### [113] [AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents](https://arxiv.org/abs/2507.14897)
*Renxi Wang,Rifo Ahmad Genadi,Bilal El Bouardi,Yongxin Wang,Fajri Koto,Zhengzhong Liu,Timothy Baldwin,Haonan Li*

Main category: cs.AI

TL;DR: AgentFly is a scalable Agent-RL framework designed to empower LM agents with RL algorithms, featuring multi-turn interactions, decorator-based interface, asynchronous execution, and centralized resource management.


<details>
  <summary>Details</summary>
Motivation: The combination of LM agents and reinforcement learning (Agent-RL) remains underexplored and lacks systematic study.

Method: The AgentFly framework adapts traditional RL methods with token-level masking and features a decorator-based interface for defining tools and reward functions. It also implements asynchronous execution and a centralized resource management system.

Result: The authors built AgentFly, a scalable and extensible Agent-RL framework designed to empower LM agents with a variety of RL algorithms.

Conclusion: The AgentFly framework's effectiveness is demonstrated through successful agent training across multiple tasks.

Abstract: Language model (LM) agents have gained significant attention for their
ability to autonomously complete tasks through interactions with environments,
tools, and APIs. LM agents are primarily built with prompt engineering or
supervised finetuning. At the same time, reinforcement learning (RL) has been
explored to enhance LM's capabilities, such as reasoning and factuality.
However, the combination of the LM agents and reinforcement learning (Agent-RL)
remains underexplored and lacks systematic study. To this end, we built
AgentFly, a scalable and extensible Agent-RL framework designed to empower LM
agents with a variety of RL algorithms. Our framework supports multi-turn
interactions by adapting traditional RL methods with token-level masking. It
features a decorator-based interface for defining tools and reward functions,
enabling seamless extension and ease of use. To support high-throughput
training, we implement asynchronous execution of tool calls and reward
computations, and design a centralized resource management system for scalable
environment coordination. We also provide a suite of prebuilt tools and
environments, demonstrating the framework's effectiveness through successful
agent training across multiple tasks.

</details>


### [114] [InsightX Agent: An LMM-based Agentic Framework with Integrated Tools for Reliable X-ray NDT Analysis](https://arxiv.org/abs/2507.14899)
*Jiale Liu,Huan Wang,Yue Zhang,Xiaoyu Luo,Jiaxiang Hu,Zhiliang Liu,Min Xie*

Main category: cs.AI

TL;DR: This paper introduces InsightX Agent, an LMM-based framework for X-ray NDT analysis that improves reliability, interpretability, and interactivity using a central LMM orchestrator coordinating between a defect detector and an evidence-grounded reflection tool. It achieves a 96.35% F1-score on the GDXray+ dataset.


<details>
  <summary>Details</summary>
Motivation: Existing deep-learning-based approaches for X-ray inspection often lack interactivity, interpretability, and the capacity for critical self-assessment, limiting their reliability and operator trust.

Method: The paper proposes InsightX Agent, a novel LMM-based agentic framework. It positions a Large Multimodal Model (LMM) as a central orchestrator, coordinating between the Sparse Deformable Multi-Scale Detector (SDMSD) and the Evidence-Grounded Reflection (EGR) tool.

Result: InsightX Agent achieves a high object detection F1-score of 96.35% and offers significantly improved interpretability and trustworthiness.

Conclusion: InsightX Agent achieves a high object detection F1-score of 96.35% and offers significantly improved interpretability and trustworthiness, highlighting the transformative potential of agentic LLM frameworks for industrial inspection tasks.

Abstract: Non-destructive testing (NDT), particularly X-ray inspection, is vital for
industrial quality assurance, yet existing deep-learning-based approaches often
lack interactivity, interpretability, and the capacity for critical
self-assessment, limiting their reliability and operator trust. To address
these shortcomings, this paper proposes InsightX Agent, a novel LMM-based
agentic framework designed to deliver reliable, interpretable, and interactive
X-ray NDT analysis. Unlike typical sequential pipelines, InsightX Agent
positions a Large Multimodal Model (LMM) as a central orchestrator,
coordinating between the Sparse Deformable Multi-Scale Detector (SDMSD) and the
Evidence-Grounded Reflection (EGR) tool. The SDMSD generates dense defect
region proposals for multi-scale feature maps and sparsifies them through
Non-Maximum Suppression (NMS), optimizing detection of small, dense targets in
X-ray images while maintaining computational efficiency. The EGR tool guides
the LMM agent through a chain-of-thought-inspired review process, incorporating
context assessment, individual defect analysis, false positive elimination,
confidence recalibration and quality assurance to validate and refine the
SDMSD's initial proposals. By strategically employing and intelligently using
tools, InsightX Agent moves beyond passive data processing to active reasoning,
enhancing diagnostic reliability and providing interpretations that integrate
diverse information sources. Experimental evaluations on the GDXray+ dataset
demonstrate that InsightX Agent not only achieves a high object detection
F1-score of 96.35% but also offers significantly improved interpretability and
trustworthiness in its analyses, highlighting the transformative potential of
agentic LLM frameworks for industrial inspection tasks.

</details>


### [115] [Feedback-Induced Performance Decline in LLM-Based Decision-Making](https://arxiv.org/abs/2507.14906)
*Xiao Yang,Juxi Leitner,Michael Burke*

Main category: cs.AI

TL;DR: LLMs在简单决策任务中表现良好，但在复杂任务中需要改进，并且反馈可能会适得其反。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLM）在自主决策环境中的适用性。

Method: 在线结构化提示策略，比较LLM方法与经典RL方法的zero-shot性能。

Result: LLMs在简单环境中表现出改进的初始性能，但在复杂环境中表现不佳；反馈机制可能会导致性能下降。

Conclusion: LLMs在复杂环境中进行规划和推理时遇到困难，并且反馈机制可能会导致性能下降。

Abstract: The ability of Large Language Models (LLMs) to extract context from natural
language problem descriptions naturally raises questions about their
suitability in autonomous decision-making settings. This paper studies the
behaviour of these models within a Markov Decision Process (MDPs). While
traditional reinforcement learning (RL) strategies commonly employed in this
setting rely on iterative exploration, LLMs, pre-trained on diverse datasets,
offer the capability to leverage prior knowledge for faster adaptation. We
investigate online structured prompting strategies in sequential decision
making tasks, comparing the zero-shot performance of LLM-based approaches to
that of classical RL methods. Our findings reveal that although LLMs
demonstrate improved initial performance in simpler environments, they struggle
with planning and reasoning in complex scenarios without fine-tuning or
additional guidance. Our results show that feedback mechanisms, intended to
improve decision-making, often introduce confusion, leading to diminished
performance in intricate environments. These insights underscore the need for
further exploration into hybrid strategies, fine-tuning, and advanced memory
integration to enhance LLM-based decision-making capabilities.

</details>


### [116] [The Endless Tuning. An Artificial Intelligence Design To Avoid Human Replacement and Trace Back Responsibilities](https://arxiv.org/abs/2507.14909)
*Elio Grande*

Main category: cs.AI

TL;DR: 提出了一种名为“无尽调整”的设计方法，旨在通过双重镜像过程实现可靠的人工智能部署，避免人类替代并填补责任差距。


<details>
  <summary>Details</summary>
Motivation: 避免人类替代和填补所谓的责任差距

Method: 基于双重镜像过程的设计方法

Result: 通过三个原型应用（贷款批准、肺炎诊断和艺术风格识别）的实现和领域专家的测试，展示了用户体验。

Conclusion: 用户在决策环境中感知到完全的控制，并且在发生损害时，可以在问责和责任之间建立桥梁。

Abstract: The Endless Tuning is a design method for a reliable deployment of artificial
intelligence based on a double mirroring process, which pursues both the goals
of avoiding human replacement and filling the so-called responsibility gap
(Matthias 2004). Originally depicted in (Fabris et al. 2024) and ensuing the
relational approach urged therein, it was then actualized in a protocol,
implemented in three prototypical applications regarding decision-making
processes (respectively: loan granting, pneumonia diagnosis, and art style
recognition) and tested with such as many domain experts. Step by step
illustrating the protocol, giving insights concretely showing a different voice
(Gilligan 1993) in the ethics of artificial intelligence, a philosophical
account of technical choices (e.g., a reversed and hermeneutic deployment of
XAI algorithms) will be provided in the present study together with the results
of the experiments, focusing on user experience rather than statistical
accuracy. Even thoroughly employing deep learning models, full control was
perceived by the interviewees in the decision-making setting, while it appeared
that a bridge can be built between accountability and liability in case of
damage.

</details>


### [117] [Redefining Elderly Care with Agentic AI: Challenges and Opportunities](https://arxiv.org/abs/2507.14912)
*Ruhul Amin Khalil,Kashif Ahmad,Hazrat Ali*

Main category: cs.AI

TL;DR: This paper explores the potential and challenges of using Agentic AI in elderly care, emphasizing the need for ethical considerations and responsible use.


<details>
  <summary>Details</summary>
Motivation: The global ageing population necessitates new strategies for caring for older adults, and there is no existing study that reviews the role of Agentic AI in elderly care.

Method: The paper analyzes the capabilities, applications, and limitations of LLM-based Agentic AI in elderly care.

Result: Personalized tracking of health, cognitive care, and environmental management can enhance independence and high-level living for older adults.

Conclusion: Agentic AI has the potential to transform elderly care but raises concerns about data privacy, security, decision independence and access. The paper emphasizes the need for ethical safeguards, privacy protections, and transparent decision-making.

Abstract: The global ageing population necessitates new and emerging strategies for
caring for older adults. In this article, we explore the potential for
transformation in elderly care through Agentic Artificial Intelligence (AI),
powered by Large Language Models (LLMs). We discuss the proactive and
autonomous decision-making facilitated by Agentic AI in elderly care.
Personalized tracking of health, cognitive care, and environmental management,
all aimed at enhancing independence and high-level living for older adults,
represents important areas of application. With a potential for significant
transformation of elderly care, Agentic AI also raises profound concerns about
data privacy and security, decision independence, and access. We share key
insights to emphasize the need for ethical safeguards, privacy protections, and
transparent decision-making. Our goal in this article is to provide a balanced
discussion of both the potential and the challenges associated with Agentic AI,
and to provide insights into its responsible use in elderly care, to bring
Agentic AI into harmony with the requirements and vulnerabilities specific to
the elderly. Finally, we identify the priorities for the academic research
communities, to achieve human-centered advancements and integration of Agentic
AI in elderly care. To the best of our knowledge, this is no existing study
that reviews the role of Agentic AI in elderly care. Hence, we address the
literature gap by analyzing the unique capabilities, applications, and
limitations of LLM-based Agentic AI in elderly care. We also provide a
companion interactive dashboard at https://hazratali.github.io/agenticai/.

</details>


### [118] [Complexity of Faceted Explanations in Propositional Abduction](https://arxiv.org/abs/2507.14962)
*Johannes Schmidt,Mohamed Maizia,Victor Lagerkvist,Johannes K. Fichte*

Main category: cs.AI

TL;DR: This paper introduces and analyzes facets in propositional abduction to better understand explanations and their variability, achieving a comprehensive characterization within Post's framework.


<details>
  <summary>Details</summary>
Motivation: The paper aims to understand explanations better while maintaining favorable complexity in propositional abduction. It addresses the challenge that insightful reasoning problems (counting and enumeration) are computationally highly challenging.

Method: The paper introduces facets to propositional abductions, which are literals that occur in some explanation but not all explanations. It also considers the distance between two explanations.

Result: The paper provides a more fine-grained understanding of variability in explanations and enables a better understanding of heterogeneity/homogeneity.

Conclusion: This paper comprehensively analyzes facets of propositional abduction in various settings, including an almost complete characterization in Post's framework.

Abstract: Abductive reasoning is a popular non-monotonic paradigm that aims to explain
observed symptoms and manifestations. It has many applications, such as
diagnosis and planning in artificial intelligence and database updates. In
propositional abduction, we focus on specifying knowledge by a propositional
formula. The computational complexity of tasks in propositional abduction has
been systematically characterized - even with detailed classifications for
Boolean fragments. Unsurprisingly, the most insightful reasoning problems
(counting and enumeration) are computationally highly challenging. Therefore,
we consider reasoning between decisions and counting, allowing us to understand
explanations better while maintaining favorable complexity. We introduce facets
to propositional abductions, which are literals that occur in some explanation
(relevant) but not all explanations (dispensable). Reasoning with facets
provides a more fine-grained understanding of variability in explanations
(heterogeneous). In addition, we consider the distance between two
explanations, enabling a better understanding of heterogeneity/homogeneity. We
comprehensively analyze facets of propositional abduction in various settings,
including an almost complete characterization in Post's framework.

</details>


### [119] [AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning](https://arxiv.org/abs/2507.14987)
*Yi Zhang,An Zhang,XiuYu Zhang,Leheng Sheng,Yuxin Chen,Zhenkai Liang,Xiang Wang*

Main category: cs.AI

TL;DR: AlphaAlign是一种新的强化学习框架，旨在提高大型语言模型的安全性，同时保持其效用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然具有潜在的安全理解能力，但仍然容易生成有害内容，并且在安全对齐后会出现过度拒绝和效用降低等问题。现有的安全对齐方法通常导致肤浅的拒绝捷径，或者依赖于密集的监督。

Method: 提出AlphaAlign，一个简单的纯强化学习框架，具有可验证的安全奖励，旨在激励这种潜在的安全意识通过主动安全推理。

Result: AlphaAlign具有三个关键优势：简单高效，打破了安全与效用之间的权衡，促进了主动安全推理。

Conclusion: AlphaAlign通过强化学习框架，利用可验证的安全奖励，激励模型潜在的安全意识，实现了更深层次的安全对齐。

Abstract: Large language models (LLMs), despite possessing latent safety understanding
from their vast pretraining data, remain vulnerable to generating harmful
content and exhibit issues such as over-refusal and utility degradation after
safety alignment. Current safety alignment methods often result in superficial
refusal shortcuts or rely on intensive supervision for reasoning-based
approaches, failing to fully leverage the model's intrinsic safety
self-awareness. We propose \textbf{AlphaAlign}, a simple yet effective pure
reinforcement learning (RL) framework with verifiable safety reward designed to
incentivize this latent safety awareness through proactive safety reasoning.}
AlphaAlign employs a dual-reward system: a verifiable safety reward encourages
correctly formatted and explicitly justified refusals for harmful queries while
penalizing over-refusals, and a normalized helpfulness reward guides
high-quality responses to benign inputs. This allows the model to develop
proactive safety reasoning capabilities without depending on supervised
safety-specific reasoning data. AlphaAlign demonstrates three key advantages:
(1) Simplicity and efficiency, requiring only binary prompt safety labels and
minimal RL steps for substantial improvements. (2) Breaking the safety-utility
trade-off, by enhancing refusal of harmful content and reducing over-refusals,
while simultaneously maintaining or even improving general task performance and
robustness to unseen jailbreaks. (3) Deep alignment, fostering proactive safety
reasoning that generates explicit safety rationales rather than relying on
shallow refusal patterns.

</details>


### [120] [A Forced-Choice Neural Cognitive Diagnostic Model of Personality Testing](https://arxiv.org/abs/2507.15013)
*Xiaoyu Li,Jin Wu,Shaoyang Guo,Haoran Shi,Chanjin Zheng*

Main category: cs.AI

TL;DR: 提出了一种新的深度学习模型FCNCD，用于强迫选择测试，经验证有效。


<details>
  <summary>Details</summary>
Motivation: 在智能时代，心理测量测试在人员选拔、职业发展和心理健康评估中变得越来越重要。强迫选择测试在人格评估中很常见，因为它们要求参与者从密切相关的选项中进行选择，从而降低了反应扭曲的风险。

Method: 提出了一种基于深度学习的强迫选择神经认知诊断模型(FCNCD)。

Result: 该模型克服了传统模型的局限性，适用于强迫选择测试中发现的三种最常见的项目块类型。为了解释强迫选择测试中项目的单维性，我们创建了可解释的参与者和项目参数。在使用非线性映射挖掘参与者和项目特征后，我们使用多层神经网络对它们之间的交互进行建模。此外，我们使用单调性假设来提高诊断结果的可解释性。

Conclusion: FCNCD在真实和模拟数据集上的实验验证了其有效性，展示了其准确性、可解释性和鲁棒性。

Abstract: In the smart era, psychometric tests are becoming increasingly important for
personnel selection, career development, and mental health assessment.
Forced-choice tests are common in personality assessments because they require
participants to select from closely related options, lowering the risk of
response distortion. This study presents a deep learning-based Forced-Choice
Neural Cognitive Diagnostic Model (FCNCD) that overcomes the limitations of
traditional models and is applicable to the three most common item block types
found in forced-choice tests. To account for the unidimensionality of items in
forced-choice tests, we create interpretable participant and item parameters.
We model the interactions between participant and item features using
multilayer neural networks after mining them using nonlinear mapping. In
addition, we use the monotonicity assumption to improve the interpretability of
the diagnostic results. The FCNCD's effectiveness is validated by experiments
on real-world and simulated datasets that show its accuracy, interpretability,
and robustness.

</details>


### [121] [From Kicking to Causality: Simulating Infant Agency Detection with a Robust Intrinsic Reward](https://arxiv.org/abs/2507.15106)
*Xia Xu,Jochen Triesch*

Main category: cs.AI

TL;DR: 本文提出了一种基于因果推理的内在奖励机制CAIS，可以提高智能体在嘈杂环境中的鲁棒性，并使其能够学习正确的策略。


<details>
  <summary>Details</summary>
Motivation: 标准强化学习智能体仍然很脆弱，因为它们对基于相关的奖励的依赖在嘈杂的、生态上有效的场景中会失败。为了解决这个问题

Method: 介绍了因果行动影响评分 (CAIS)，这是一种植根于因果推理的新型内在奖励。CAIS 通过测量以该动作为条件的感官结果的学习分布之间的 1-Wasserstein 距离来量化行动的影响，即 $p(h|a)$，以及基线结果分布，即 $p(h)$。

Result: 在模拟的婴儿移动环境中测试了我们的方法，在该环境中，当移动设备受到外力时，基于相关的感知奖励完全失败。CAIS 能够过滤这种噪音，识别其影响，并学习正确的策略。此外，为 CAIS 学习的高质量预测模型允许我们的智能体在增加了一个惊奇信号后，成功地重现“消退爆发”现象。

Conclusion: 明确推断因果关系是发展强大的能动性的关键机制，为更具适应性的自主系统提供了心理上合理的框架。

Abstract: While human infants robustly discover their own causal efficacy, standard
reinforcement learning agents remain brittle, as their reliance on
correlation-based rewards fails in noisy, ecologically valid scenarios. To
address this, we introduce the Causal Action Influence Score (CAIS), a novel
intrinsic reward rooted in causal inference. CAIS quantifies an action's
influence by measuring the 1-Wasserstein distance between the learned
distribution of sensory outcomes conditional on that action, $p(h|a)$, and the
baseline outcome distribution, $p(h)$. This divergence provides a robust reward
that isolates the agent's causal impact from confounding environmental noise.
We test our approach in a simulated infant-mobile environment where
correlation-based perceptual rewards fail completely when the mobile is
subjected to external forces. In stark contrast, CAIS enables the agent to
filter this noise, identify its influence, and learn the correct policy.
Furthermore, the high-quality predictive model learned for CAIS allows our
agent, when augmented with a surprise signal, to successfully reproduce the
"extinction burst" phenomenon. We conclude that explicitly inferring causality
is a crucial mechanism for developing a robust sense of agency, offering a
psychologically plausible framework for more adaptive autonomous systems.

</details>


### [122] [Automated planning with ontologies under coherence update semantics](https://arxiv.org/abs/2507.15120)
*Stefan Borgwardt,Duy Nhu,Gabriele Röger*

Main category: cs.AI

TL;DR: This paper presents a new approach for planning with DL-Lite ontologies that combines the advantages of ontology-based action conditions and ontology-aware action effects. The complexity of the resulting formalism is not higher than that of previous approaches, and an implementation via a polynomial compilation into classical planning is provided.


<details>
  <summary>Details</summary>
Motivation: To incorporate background knowledge into automated planning problems, for example, by means of ontologies, which are usually interpreted under open-world semantics.

Method: A new approach for planning with DL-Lite ontologies that combines the advantages of ontology-based action conditions provided by explicit-input knowledge and action bases (eKABs) and ontology-aware action effects under the coherence update semantics.

Result: The complexity of the resulting formalism is not higher than that of previous approaches.

Conclusion: The complexity of the resulting formalism is not higher than that of previous approaches, and an implementation via a polynomial compilation into classical planning is provided. An evaluation of existing and new benchmarks examines the performance of a planning system on different variants of our compilation.

Abstract: Standard automated planning employs first-order formulas under closed-world
semantics to achieve a goal with a given set of actions from an initial state.
We follow a line of research that aims to incorporate background knowledge into
automated planning problems, for example, by means of ontologies, which are
usually interpreted under open-world semantics. We present a new approach for
planning with DL-Lite ontologies that combines the advantages of ontology-based
action conditions provided by explicit-input knowledge and action bases (eKABs)
and ontology-aware action effects under the coherence update semantics. We show
that the complexity of the resulting formalism is not higher than that of
previous approaches and provide an implementation via a polynomial compilation
into classical planning. An evaluation of existing and new benchmarks examines
the performance of a planning system on different variants of our compilation.

</details>


### [123] [Clinical Semantic Intelligence (CSI): Emulating the Cognitive Framework of the Expert Clinician for Comprehensive Oral Disease Diagnosis](https://arxiv.org/abs/2507.15140)
*Mohammad Mashayekhi,Sara Ahmadi Majd,Arian AmirAmjadi,Parsa Hosseini*

Main category: cs.AI

TL;DR: Developed CSI, an AI framework for diagnosing 118 oral diseases, achieving up to 89.5% accuracy by emulating expert reasoning through a Hierarchical Diagnostic Reasoning Tree.


<details>
  <summary>Details</summary>
Motivation: Diagnosing oral diseases is challenging due to overlapping symptomatology, necessitating diagnostic aids that emulate expert clinical reasoning.

Method: The study uses a fine-tuned multimodal CLIP model integrated with a ChatGLM-6B language model, executing a Hierarchical Diagnostic Reasoning Tree (HDRT).

Result: CSI achieved 73.4% accuracy in Fast Mode and 89.5% in HDRT-driven Standard Mode on an internal test set.

Conclusion: The Clinical Semantic Intelligence (CSI) framework shows promising results in diagnosing oral diseases, with the Hierarchical Diagnostic Reasoning Tree (HDRT) significantly improving diagnostic accuracy.

Abstract: The diagnosis of oral diseases presents a problematic clinical challenge,
characterized by a wide spectrum of pathologies with overlapping
symptomatology. To address this, we developed Clinical Semantic Intelligence
(CSI), a novel artificial intelligence framework that diagnoses 118 different
oral diseases by computationally modeling the cognitive processes of an expert
clinician. Our core hypothesis is that moving beyond simple pattern matching to
emulate expert reasoning is critical to building clinically useful diagnostic
aids.
  CSI's architecture integrates a fine-tuned multimodal CLIP model with a
specialized ChatGLM-6B language model. This system executes a Hierarchical
Diagnostic Reasoning Tree (HDRT), a structured framework that distills the
systematic, multi-step logic of differential diagnosis. The framework operates
in two modes: a Fast Mode for rapid screening and a Standard Mode that
leverages the full HDRT for an interactive and in-depth diagnostic workup.
  To train and validate our system, we curated a primary dataset of 4,310
images, supplemented by an external hold-out set of 176 images for final
validation. A clinically-informed augmentation strategy expanded our training
data to over 30,000 image-text pairs. On a 431-image internal test set, CSI's
Fast Mode achieved an accuracy of 73.4%, which increased to 89.5% with the
HDRT-driven Standard Mode. The performance gain is directly attributable to the
hierarchical reasoning process. Herein, we detail the architectural philosophy,
development, and rigorous evaluation of the CSI framework.

</details>


### [124] [Can We Move Freely in NEOM's The Line? An Agent-Based Simulation of Human Mobility in a Futuristic Smart City](https://arxiv.org/abs/2507.15143)
*Abderaouf Bahi,Amel Ourici*

Main category: cs.AI

TL;DR: 本文研究了NEOM的线性城市The Line中的交通可行性，结果表明，通过AI和可持续基础设施可以实现高效的城市交通。


<details>
  <summary>Details</summary>
Motivation: 本文研究了在沙特阿拉伯NEOM拟建的170公里线性智能城市The Line中，人类移动的可行性。

Method: 开发了一个混合模拟框架，集成了基于Agent的建模、强化学习、监督学习和图神经网络。

Result: 实验表明，在完全集成AI的架构下，即使在高峰拥堵时期，智能体的平均通勤时间为7.8至8.4分钟，满意率超过89%，可达性指数超过91%。移除强化学习或图神经网络等智能模块会显著降低性能，通勤时间增加高达85%，可达性降至70%以下。环境模型进一步证明，当优先考虑电动模式时，能源消耗低，CO2排放量最低。

Conclusion: 在The Line中，如果采用自适应AI系统、可持续基础设施和实时反馈回路，自由移动不仅在概念上是可行的，而且在操作上也是现实的。

Abstract: This paper investigates the feasibility of human mobility in The Line, a
proposed 170-kilometer linear smart city in NEOM, Saudi Arabia. To assess
whether citizens can move freely within this unprecedented urban topology, we
develop a hybrid simulation framework that integrates agent-based modeling,
reinforcement learning, supervised learning, and graph neural networks. The
simulation captures multi-modal transportation behaviors across 50 vertical
levels and varying density scenarios using both synthetic data and real-world
traces from high-density cities. Our experiments reveal that with the full
AI-integrated architecture, agents achieved an average commute time of 7.8 to
8.4 minutes, a satisfaction rate exceeding 89 percent, and a reachability index
of over 91 percent, even during peak congestion periods. Ablation studies
confirmed that the removal of intelligent modules such as reinforcement
learning or graph neural networks significantly degrades performance, with
commute times increasing by up to 85 percent and reachability falling below 70
percent. Environmental modeling further demonstrated low energy consumption and
minimal CO2 emissions when electric modes are prioritized. The findings suggest
that freedom of movement is not only conceptually achievable in The Line, but
also operationally realistic if supported by adaptive AI systems, sustainable
infrastructure, and real-time feedback loops.

</details>


### [125] [Solving Formal Math Problems by Decomposition and Iterative Reflection](https://arxiv.org/abs/2507.15225)
*Yichi Zhou,Jianqiu Zhao,Yongxin Zhang,Bohan Wang,Siran Wang,Luoxin Chen,Jiahui Wang,Haowei Chen,Allan Jie,Xinbo Zhang,Haocheng Wang,Luong Trung,Rong Ye,Phan Nhat Hoang,Huishuai Zhang,Peng Sun,Hang Li*

Main category: cs.AI

TL;DR: Delta Prover, an agent-based framework, enables general-purpose LLMs to achieve state-of-the-art results in Lean 4 theorem proving without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Generating formal proofs in specialized languages like Lean 4 remains a significant challenge for LLMs, limiting their application in complex theorem proving and automated verification. Current approaches typically require specializing models through fine-tuning, incurring high costs.

Method: an agent-based framework that orchestrates the interaction between a general-purpose LLM and the Lean 4 proof environment, integrating an algorithmic framework for reflective decomposition and iterative proof repair, and a custom Domain-Specific Language (DSL) built upon Lean 4 for streamlined subproblem management.

Result: achieves a state-of-the-art 95.9% success rate on the miniF2F-test benchmark, surpassing all existing approaches, and exhibits a significantly stronger test-time scaling law compared to standard Best-of-N proof strategies.

Conclusion: General-purpose LLMs, when guided by an effective agentic structure, possess substantial untapped theorem-proving capabilities, presenting a computationally efficient alternative to specialized models.

Abstract: General-purpose Large Language Models (LLMs) have achieved remarkable success
in intelligence, performing comparably to human experts on complex reasoning
tasks such as coding and mathematical reasoning. However, generating formal
proofs in specialized languages like Lean 4 remains a significant challenge for
these models, limiting their application in complex theorem proving and
automated verification. Current approaches typically require specializing
models through fine-tuning on dedicated formal corpora, incurring high costs
for data collection and training. In this work, we introduce \textbf{Delta
Prover}, an agent-based framework that orchestrates the interaction between a
general-purpose LLM and the Lean 4 proof environment. Delta Prover leverages
the reflection and reasoning capabilities of general-purpose LLMs to
interactively construct formal proofs in Lean 4, circumventing the need for
model specialization. At its core, the agent integrates two novel,
interdependent components: an algorithmic framework for reflective
decomposition and iterative proof repair, and a custom Domain-Specific Language
(DSL) built upon Lean 4 for streamlined subproblem management. \textbf{Delta
Prover achieves a state-of-the-art 95.9\% success rate on the miniF2F-test
benchmark, surpassing all existing approaches, including those requiring model
specialization.} Furthermore, Delta Prover exhibits a significantly stronger
test-time scaling law compared to standard Best-of-N proof strategies.
Crucially, our findings demonstrate that general-purpose LLMs, when guided by
an effective agentic structure, possess substantial untapped theorem-proving
capabilities. This presents a computationally efficient alternative to
specialized models for robust automated reasoning in formal environments.

</details>


### [126] [Explainable Artificial Intelligence based Soft Evaluation Indicator for Arc Fault Diagnosis](https://arxiv.org/abs/2507.15239)
*Qianchao Wang,Yuxuan Ding,Chuanzhen Jia,Zhe Li,Yaping Du*

Main category: cs.AI

TL;DR: This paper proposes a soft evaluation indicator to explain the outputs of arc fault diagnosis models and a lightweight balanced neural network to guarantee competitive accuracy and soft feature extraction score.


<details>
  <summary>Details</summary>
Motivation: Whether AI-based arc fault diagnosis models can actually be trusted to find arc faults is an inherent problem.

Method: A soft evaluation indicator is proposed to explain the outputs of arc fault diagnosis models, by defining the correct explanation of arc faults and leveraging Explainable Artificial Intelligence and real arc fault experiments. A lightweight balanced neural network is proposed to guarantee competitive accuracy and soft feature extraction score.

Result: Several traditional machine learning methods and deep learning methods across two arc fault datasets with different sample times and noise levels are utilized to test the effectiveness of the soft evaluation indicator.

Conclusion: The proposed soft evaluation indicator makes arc fault diagnosis models easy to understand and trust, allowing practitioners to make informed and trustworthy decisions.

Abstract: Novel AI-based arc fault diagnosis models have demonstrated outstanding
performance in terms of classification accuracy. However, an inherent problem
is whether these models can actually be trusted to find arc faults. In this
light, this work proposes a soft evaluation indicator that explains the outputs
of arc fault diagnosis models, by defining the the correct explanation of arc
faults and leveraging Explainable Artificial Intelligence and real arc fault
experiments. Meanwhile, a lightweight balanced neural network is proposed to
guarantee competitive accuracy and soft feature extraction score. In our
experiments, several traditional machine learning methods and deep learning
methods across two arc fault datasets with different sample times and noise
levels are utilized to test the effectiveness of the soft evaluation indicator.
Through this approach, the arc fault diagnosis models are easy to understand
and trust, allowing practitioners to make informed and trustworthy decisions.

</details>


### [127] [Disentangling Homophily and Heterophily in Multimodal Graph Clustering](https://arxiv.org/abs/2507.15253)
*Zhaochen Guo,Zhixiang Shen,Xuanting Xie,Liangjian Wen,Zhao Kang*

Main category: cs.AI

TL;DR: 本文提出了一种新的多模态图聚类框架DMGC，该框架通过解缠和融合策略处理混合邻域模式，并在多模态和多关系图数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 多模态图集成了非结构化异构数据和结构化互连，提供了大量的实际应用，但在无监督学习中仍未得到充分的探索。这项工作启动了多模态图聚类的研究，旨在弥合这一关键差距。

Method: 该论文提出了一种新的框架--\\textsc{Disentangled Multimodal Graph Clustering (DMGC)}--它将原始混合图分解为两个互补的视图：(1) 捕获跨模态类一致性的同质性增强图，以及(2) 保留模态特定类间差异的异质性感知图。引入了一种多模态双频融合机制，通过双通道策略联合过滤这些解缠图，从而实现有效的多模态集成，同时减轻类别混淆。

Result: 通过实证分析，我们观察到真实世界的多模态图通常表现出混合邻域模式，结合了同质和异质关系。

Conclusion: DMGC在多模态和多关系图数据集上实现了最先进的性能，突显了其在不同设置中的有效性和泛化性。

Abstract: Multimodal graphs, which integrate unstructured heterogeneous data with
structured interconnections, offer substantial real-world utility but remain
insufficiently explored in unsupervised learning. In this work, we initiate the
study of multimodal graph clustering, aiming to bridge this critical gap.
Through empirical analysis, we observe that real-world multimodal graphs often
exhibit hybrid neighborhood patterns, combining both homophilic and
heterophilic relationships. To address this challenge, we propose a novel
framework -- \textsc{Disentangled Multimodal Graph Clustering (DMGC)} -- which
decomposes the original hybrid graph into two complementary views: (1) a
homophily-enhanced graph that captures cross-modal class consistency, and (2)
heterophily-aware graphs that preserve modality-specific inter-class
distinctions. We introduce a \emph{Multimodal Dual-frequency Fusion} mechanism
that jointly filters these disentangled graphs through a dual-pass strategy,
enabling effective multimodal integration while mitigating category confusion.
Our self-supervised alignment objectives further guide the learning process
without requiring labels. Extensive experiments on both multimodal and
multi-relational graph datasets demonstrate that DMGC achieves state-of-the-art
performance, highlighting its effectiveness and generalizability across diverse
settings. Our code is available at https://github.com/Uncnbb/DMGC.

</details>


### [128] [IM-Chat: A Multi-agent LLM-based Framework for Knowledge Transfer in Injection Molding Industry](https://arxiv.org/abs/2507.15268)
*Junhyeong Lee,Joon-Young Kim,Heekyu Kim,Inhyo Lee,Seunghwa Ryu*

Main category: cs.AI

TL;DR: IM-Chat是一个基于llm的多智能体框架，旨在促进注塑行业的知识转移，它通过整合有限的文档知识和广泛的现场数据，实现稳健和上下文感知的任务解决。


<details>
  <summary>Details</summary>
Motivation: 注塑行业在保存和传递领域知识方面面临着严峻的挑战，尤其是在经验丰富的工人退休和多语言障碍阻碍有效沟通的情况下。

Method: 基于大型语言模型(llm)的多智能体框架IM-Chat，采用检索增强生成(rag)策略和模块化架构中的工具调用代理。

Result: 在领域专家使用10点量表(侧重于相关性和正确性)对GPT-4o、GPT-4o-mini和GPT-3.5-turbo的100个单工具和60个混合任务进行评估后，评估结果表明，更有能力的模型往往能获得更高的准确性，尤其是在复杂的工具集成场景中。

Conclusion: 多智能体LLM系统在工业知识工作流程中是可行的，并将IM-Chat确立为一种可扩展和通用的AI辅助制造决策支持方法。

Abstract: The injection molding industry faces critical challenges in preserving and
transferring field knowledge, particularly as experienced workers retire and
multilingual barriers hinder effective communication. This study introduces
IM-Chat, a multi-agent framework based on large language models (LLMs),
designed to facilitate knowledge transfer in injection molding. IM-Chat
integrates both limited documented knowledge (e.g., troubleshooting tables,
manuals) and extensive field data modeled through a data-driven process
condition generator that infers optimal manufacturing settings from
environmental inputs such as temperature and humidity, enabling robust and
context-aware task resolution. By adopting a retrieval-augmented generation
(RAG) strategy and tool-calling agents within a modular architecture, IM-Chat
ensures adaptability without the need for fine-tuning. Performance was assessed
across 100 single-tool and 60 hybrid tasks for GPT-4o, GPT-4o-mini, and
GPT-3.5-turbo by domain experts using a 10-point rubric focused on relevance
and correctness, and was further supplemented by automated evaluation using
GPT-4o guided by a domain-adapted instruction prompt. The evaluation results
indicate that more capable models tend to achieve higher accuracy, particularly
in complex, tool-integrated scenarios. Overall, these findings demonstrate the
viability of multi-agent LLM systems for industrial knowledge workflows and
establish IM-Chat as a scalable and generalizable approach to AI-assisted
decision support in manufacturing.

</details>


### [129] [QSAF: A Novel Mitigation Framework for Cognitive Degradation in Agentic AI](https://arxiv.org/abs/2507.15330)
*Hammad Atta,Muhammad Zeeshan Baig,Yasir Mehmood,Nadeem Shahzad,Ken Huang,Muhammad Aziz Ul Haq,Muhammad Awais,Kamal Ahmed*

Main category: cs.AI

TL;DR: 认知退化是一种新的 AI 系统脆弱性，论文提出了一个跨平台防御模型来解决这个问题。


<details>
  <summary>Details</summary>
Motivation: 引入认知退化作为代理 AI 系统中的一种新的脆弱性类别。与传统的对抗性外部威胁（如提示注入）不同，这些故障源于内部，由记忆不足、规划器递归、上下文泛滥和输出抑制引起。这些系统性弱点导致随着时间的推移出现无声的代理漂移、逻辑崩溃和持续的幻觉。

Method: 引入了 Qorvex Security AI Framework for Behavioral & Cognitive Resilience (QSAF Domain 10)，这是一个由六个阶段认知退化生命周期定义的生命周期感知防御框架，包括七个运行时控制（QSAF-BC-001 到 BC-007），这些控制实时监控代理子系统并通过回退路由、饥饿检测和内存完整性执行触发主动缓解。

Result: 通过将代理架构映射到人类类似物，能够及早发现疲劳、饥饿和角色崩溃。

Conclusion: 建立了认知退化作为一个新的 AI 系统脆弱性的关键类别，并提出了第一个用于弹性代理行为的跨平台防御模型。

Abstract: We introduce Cognitive Degradation as a novel vulnerability class in agentic
AI systems. Unlike traditional adversarial external threats such as prompt
injection, these failures originate internally, arising from memory starvation,
planner recursion, context flooding, and output suppression. These systemic
weaknesses lead to silent agent drift, logic collapse, and persistent
hallucinations over time. To address this class of failures, we introduce the
Qorvex Security AI Framework for Behavioral & Cognitive Resilience (QSAF Domain
10), a lifecycle-aware defense framework defined by a six-stage cognitive
degradation lifecycle. The framework includes seven runtime controls
(QSAF-BC-001 to BC-007) that monitor agent subsystems in real time and trigger
proactive mitigation through fallback routing, starvation detection, and memory
integrity enforcement. Drawing from cognitive neuroscience, we map agentic
architectures to human analogs, enabling early detection of fatigue,
starvation, and role collapse. By introducing a formal lifecycle and real-time
mitigation controls, this work establishes Cognitive Degradation as a critical
new class of AI system vulnerability and proposes the first cross-platform
defense model for resilient agentic behavior.

</details>


### [130] [One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step Policy Optimization for Order Dispatch on Ride-Sharing Platforms](https://arxiv.org/abs/2507.15351)
*Zijian Zhao,Sen Li*

Main category: cs.AI

TL;DR: This paper introduces GRPO and OSPO, two novel MARL methods for on-demand ride-sharing that bypass value function estimation, achieving superior performance in optimizing pickup times and served orders.


<details>
  <summary>Details</summary>
Motivation: Conventional MARL-based ride-sharing approaches heavily rely on the accurate estimation of Q-values or V-values, which becomes problematic in large-scale, highly uncertain environments. Specifically, most of these approaches adopt an independent paradigm, exacerbating this issue, as each agent treats others as part of the environment, leading to unstable training and substantial estimation bias in value functions.

Method: We propose two novel alternative methods that bypass value function estimation. First, we adapt GRPO to ride-sharing, replacing the PPO baseline with the group average reward to eliminate critic estimation errors and reduce training bias. Second, inspired by GRPO's full utilization of group reward information, we customize the PPO framework for ride-sharing platforms and show that, under a homogeneous fleet, the optimal policy can be trained using only one-step rewards - a method we term One-Step Policy Optimization (OSPO).

Result: Experiments on a real-world Manhattan ride-hailing dataset demonstrate that both GRPO and OSPO achieve superior performance across most scenarios, efficiently optimizing pickup times and the number of served orders using simple MLP networks.

Conclusion: GRPO and OSPO achieve superior performance across most scenarios, efficiently optimizing pickup times and the number of served orders using simple MLP networks.

Abstract: On-demand ride-sharing platforms face the fundamental challenge of
dynamically bundling passengers with diverse origins and destinations and
matching them with vehicles in real time, all under significant uncertainty.
Recently, MARL has emerged as a promising solution for this problem, leveraging
decentralized learning to address the curse of dimensionality caused by the
large number of agents in the ride-hailing market and the resulting expansive
state and action spaces. However, conventional MARL-based ride-sharing
approaches heavily rely on the accurate estimation of Q-values or V-values,
which becomes problematic in large-scale, highly uncertain environments.
Specifically, most of these approaches adopt an independent paradigm,
exacerbating this issue, as each agent treats others as part of the
environment, leading to unstable training and substantial estimation bias in
value functions. To address these challenges, we propose two novel alternative
methods that bypass value function estimation. First, we adapt GRPO to
ride-sharing, replacing the PPO baseline with the group average reward to
eliminate critic estimation errors and reduce training bias. Second, inspired
by GRPO's full utilization of group reward information, we customize the PPO
framework for ride-sharing platforms and show that, under a homogeneous fleet,
the optimal policy can be trained using only one-step rewards - a method we
term One-Step Policy Optimization (OSPO). Experiments on a real-world Manhattan
ride-hailing dataset demonstrate that both GRPO and OSPO achieve superior
performance across most scenarios, efficiently optimizing pickup times and the
number of served orders using simple MLP networks.

</details>


### [131] [RAD: Retrieval High-quality Demonstrations to Enhance Decision-making](https://arxiv.org/abs/2507.15356)
*Lu Guo,Yixiang Shan,Zhengbang Zhu,Qifan Liang,Lichang Song,Ting Long,Weinan Zhang,Yi Chang*

Main category: cs.AI

TL;DR: RAD结合检索和扩散模型，从离线数据集中动态检索高回报状态作为目标状态，并使用条件引导扩散模型规划达到这些状态的路径，从而提升离线强化学习效果。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习的有效性通常受到数据集稀疏性和次优轨迹与专家轨迹之间缺乏转换重叠的限制，这使得长时程规划特别具有挑战性。

Method: 结合非参数检索与基于扩散的生成模型。

Result: RAD通过检索引导生成实现了灵活的轨迹拼接，并在遇到未充分表示或超出分布状态时提高了泛化能力。

Conclusion: RAD在各种基准测试中实现了有竞争力或更优越的性能，验证了其有效性。

Abstract: Offline reinforcement learning (RL) enables agents to learn policies from
fixed datasets, avoiding costly or unsafe environment interactions. However,
its effectiveness is often limited by dataset sparsity and the lack of
transition overlap between suboptimal and expert trajectories, which makes
long-horizon planning particularly challenging. Prior solutions based on
synthetic data augmentation or trajectory stitching often fail to generalize to
novel states and rely on heuristic stitching points. To address these
challenges, we propose Retrieval High-quAlity Demonstrations (RAD) for
decision-making, which combines non-parametric retrieval with diffusion-based
generative modeling. RAD dynamically retrieves high-return states from the
offline dataset as target states based on state similarity and return
estimation, and plans toward them using a condition-guided diffusion model.
Such retrieval-guided generation enables flexible trajectory stitching and
improves generalization when encountered with underrepresented or
out-of-distribution states. Extensive experiments confirm that RAD achieves
competitive or superior performance compared to baselines across diverse
benchmarks, validating its effectiveness.

</details>


### [132] [Predictive Process Monitoring Using Object-centric Graph Embeddings](https://arxiv.org/abs/2507.15411)
*Wissam Gherissi,Mehdi Acheli,Joyce El Haddad,Daniela Grigori*

Main category: cs.AI

TL;DR: end-to-end model that predicts future process behavior


<details>
  <summary>Details</summary>
Motivation: Object-centric predictive process monitoring explores and utilizes object-centric event logs to enhance process predictions. The main challenge lies in extracting relevant information and building effective models.

Method: a graph attention network to encode activities and their relationships, combined with an LSTM network to handle temporal dependencies

Result: predicts future process behavior, focusing on two tasks: next activity prediction and next event time

Conclusion: The model demonstrates competitive performance compared to state-of-the-art methods.

Abstract: Object-centric predictive process monitoring explores and utilizes
object-centric event logs to enhance process predictions. The main challenge
lies in extracting relevant information and building effective models. In this
paper, we propose an end-to-end model that predicts future process behavior,
focusing on two tasks: next activity prediction and next event time. The
proposed model employs a graph attention network to encode activities and their
relationships, combined with an LSTM network to handle temporal dependencies.
Evaluated on one reallife and three synthetic event logs, the model
demonstrates competitive performance compared to state-of-the-art methods.

</details>


### [133] [Optimization of Activity Batching Policies in Business Processes](https://arxiv.org/abs/2507.15457)
*Orlenys López-Pintado,Jannis Rosenbaum,Marlon Dumas*

Main category: cs.AI

TL;DR: This paper introduces a Pareto optimization method using intervention heuristics to find the best activity batching policies, balancing waiting time, effort, and cost, and compares it with other methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to find optimal trade-offs between waiting time, processing effort, and cost in activity batching by discovering effective batching policies.

Method: The paper uses a Pareto optimization approach with intervention heuristics, embedded in hill-climbing, simulated annealing, and reinforcement learning meta-heuristics. The impact of each intervention is evaluated via simulation.

Result: The experimental evaluation compares the proposed approach against non-heuristic guided meta-heuristics regarding convergence, diversity, and cycle time gain of Pareto-optimal policies.

Conclusion: This paper proposes a Pareto optimization approach with intervention heuristics to discover activity batching policies that balance waiting time, processing effort, and cost. The approach is evaluated against non-heuristic guided meta-heuristics.

Abstract: In business processes, activity batching refers to packing multiple activity
instances for joint execution. Batching allows managers to trade off cost and
processing effort against waiting time. Larger and less frequent batches may
lower costs by reducing processing effort and amortizing fixed costs, but they
create longer waiting times. In contrast, smaller and more frequent batches
reduce waiting times but increase fixed costs and processing effort. A batching
policy defines how activity instances are grouped into batches and when each
batch is activated. This paper addresses the problem of discovering batching
policies that strike optimal trade-offs between waiting time, processing
effort, and cost. The paper proposes a Pareto optimization approach that starts
from a given set (possibly empty) of activity batching policies and generates
alternative policies for each batched activity via intervention heuristics.
Each heuristic identifies an opportunity to improve an activity's batching
policy with respect to a metric (waiting time, processing time, cost, or
resource utilization) and an associated adjustment to the activity's batching
policy (the intervention). The impact of each intervention is evaluated via
simulation. The intervention heuristics are embedded in an optimization
meta-heuristic that triggers interventions to iteratively update the Pareto
front of the interventions identified so far. The paper considers three
meta-heuristics: hill-climbing, simulated annealing, and reinforcement
learning. An experimental evaluation compares the proposed approach based on
intervention heuristics against the same (non-heuristic guided) meta-heuristics
baseline regarding convergence, diversity, and cycle time gain of
Pareto-optimal policies.

</details>


### [134] [Chart-R1: Chain-of-Thought Supervision and Reinforcement for Advanced Chart Reasoner](https://arxiv.org/abs/2507.15509)
*Lei Chen,Xuanle Zhao,Zhixiong Zeng,Jing Huang,Yufeng Zhong,Lin Ma*

Main category: cs.AI

TL;DR: Chart-R1, a chart-domain vision-language model with reinforcement learning, enables complex chart reasoning through a novel data synthesis technology and a two-stage training strategy.


<details>
  <summary>Details</summary>
Motivation: Verifying the advantages of R1-Style methods on more general multimodal data, specifically complex chart reasoning, which lacks reasoning data.

Method: A two-stage training strategy: Chart-COT with step-by-step chain-of-thought supervision, and Chart-RFT with numerically sensitive reinforcement fine-tuning.

Result: Chart-R1 demonstrates significant advantages compared to chart-domain methods and is even comparable to large-scale models.

Conclusion: Chart-R1 exhibits significant advantages over chart-domain methods and is even comparable to large-scale models like GPT-4o and Claude-3.5.

Abstract: Recently, inspired by OpenAI-o1/o3 and Deepseek-R1, the R1-Style method based
on reinforcement learning fine-tuning has received widespread attention from
the community. Previous R1-Style methods mainly focus on mathematical reasoning
and code intelligence. It is of great research significance to verify their
advantages on more general multimodal data. Chart is an important multimodal
data type with rich information, which brings important research challenges in
complex reasoning. In this work, we introduce Chart-R1, a chart-domain
vision-language model with reinforcement learning fine-tuning to enable complex
chart reasoning. To support Chart-R1, we first propose a novel programmatic
data synthesis technology to generate high-quality step-by-step chart reasoning
data covering single- and multi-subcharts, which makes up for the lack of
reasoning data in the chart domain. Then we develop a two-stage training
strategy: Chart-COT with step-by-step chain-of-thought supervision, and
Chart-RFT with numerically sensitive reinforcement fine-tuning. Chart-COT aims
to decompose complex chart reasoning tasks into fine-grained, understandable
subtasks through step-by-step supervision, which lays a good foundation for
improving the reasoning level of reinforcement learning. Chart-RFT utilize the
typical group relative policy optimization strategy, in which a relatively soft
reward is adopted for numerical response to emphasize the numerical sensitivity
in the chart domain. We conduct extensive experiments on open-source benchmarks
and self-built chart reasoning dataset (\emph{i.e., ChartRQA}). Experimental
results show that Chart-R1 has significant advantages compared to chart-domain
methods, even comparable to open/closed source large-scale models (\emph{e.g.,
GPT-4o, Claude-3.5}).

</details>


### [135] [HAMLET: Hyperadaptive Agent-based Modeling for Live Embodied Theatrics](https://arxiv.org/abs/2507.15518)
*Sizhou Chen,Shufan Jiang,Chi Zhang,Xiao-Lei Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: HAMLET, a multi-agent framework, is proposed to create drama and online performance. It enables actors to make independent decisions and interact with the physical environment, and can create expressive and coherent theatrical experiences.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based drama generation methods often result in AI agents that lack initiative and cannot interact with the physical environment, and typically require detailed user input to drive the drama, which reduce the interactivity and immersion of online real-time performance.

Method: a multi-agent framework focused on drama creation and online performance

Result: an evaluation method to assess three primary aspects, including character performance, narrative quality, and interaction experience

Conclusion: HAMLET can create expressive and coherent theatrical experiences.

Abstract: Creating an immersive and interactive theatrical experience is a long-term
goal in the field of interactive narrative. The emergence of large language
model (LLM) is providing a new path to achieve this goal. However, existing
LLM-based drama generation methods often result in AI agents that lack
initiative and cannot interact with the physical environment. Furthermore,
these methods typically require detailed user input to drive the drama. These
limitations reduce the interactivity and immersion of online real-time
performance. To address the above challenges, we propose HAMLET, a multi-agent
framework focused on drama creation and online performance. Given a simple
topic, the framework generates a narrative blueprint, guiding the subsequent
improvisational performance. During the online performance, each actor is given
an autonomous mind. This means that actors can make independent decisions based
on their own background, goals, and emotional state. In addition to
conversations with other actors, their decisions can also change the state of
scene props through actions such as opening a letter or picking up a weapon.
The change is then broadcast to other related actors, updating what they know
and care about, which in turn influences their next action. To evaluate the
quality of drama performance, we designed an evaluation method to assess three
primary aspects, including character performance, narrative quality, and
interaction experience. The experimental evaluation shows that HAMLET can
create expressive and coherent theatrical experiences. Our code, dataset and
models are available at https://github.com/HAMLET-2025/HAMLET.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [136] [Schemora: schema matching via multi-stage recommendation and metadata enrichment using off-the-shelf llms](https://arxiv.org/abs/2507.14376)
*Osman Erman Gungor,Derak Paulsen,William Kang*

Main category: cs.DB

TL;DR: SCHEMORA is an LLM-based schema matching framework that improves accuracy and scalability using hybrid retrieval, achieving state-of-the-art results on the MIMIC-OMOP benchmark.


<details>
  <summary>Details</summary>
Motivation: Schema matching is essential for integrating heterogeneous data sources, yet it remains a complex and resource-intensive problem.

Method: It combines large language models with hybrid retrieval techniques in a prompt-based approach.

Result: SCHEMORA improves matching accuracy and scalability, with gains of 7.49% in HitRate@5 and 3.75% in HitRate@3 over previous best results.

Conclusion: SCHEMORA achieves state-of-the-art schema matching performance on the MIMIC-OMOP benchmark.

Abstract: Schema matching is essential for integrating heterogeneous data sources and
enhancing dataset discovery, yet it remains a complex and resource-intensive
problem. We introduce SCHEMORA, a schema matching framework that combines large
language models with hybrid retrieval techniques in a prompt-based approach,
enabling efficient identification of candidate matches without relying on
labeled training data or exhaustive pairwise comparisons. By enriching schema
metadata and leveraging both vector-based and lexical retrieval, SCHEMORA
improves matching accuracy and scalability. Evaluated on the MIMIC-OMOP
benchmark, it establishes new state-of-the-art performance, with gains of 7.49%
in HitRate@5 and 3.75% in HitRate@3 over previous best results. To our
knowledge, this is the first LLM-based schema matching method with an
open-source implementation, accompanied by analysis that underscores the
critical role of retrieval and provides practical guidance on model selection.

</details>


### [137] [Towards Temporal Knowledge Graph Alignment in the Wild](https://arxiv.org/abs/2507.14475)
*Runhao Zhao,Weixin Zeng,Wentao Zhang,Xiang Zhao,Jiuyang Tang,Lei Chen*

Main category: cs.DB

TL;DR: 本文研究了真实场景中的时间知识图对齐（TKGA-Wild）任务，提出了HyDRA方法和新的基准数据集，并在实验中取得了优异的结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法处理真实场景中的TKGA（TKGA-Wild），其中多尺度时间元素纠缠和跨源时间结构不平衡很常见。

Method: 提出了一种新的多尺度超图检索增强生成方法HyDRA，并设计了一种新的尺度编织协同机制，结合了尺度内交互和跨尺度冲突检测。

Result: 建立了两个新的数据集（BETA和WildBETA），并通过大量实验证明了HyDRA的有效性，显著优于24个竞争基线。

Conclusion: HyDRA在TKGA-Wild任务上优于其他方法，并在新数据集和现有基准测试中表现出色，同时保持了效率和可扩展性。

Abstract: Temporal Knowledge Graph Alignment (TKGA) seeks to identify equivalent
entities across heterogeneous temporal knowledge graphs (TKGs) for fusion to
improve their completeness. Although some approaches have been proposed to
tackle this task, most assume unified temporal element standards and simplified
temporal structures across different TKGs. They cannot deal with TKGA in the
wild (TKGA-Wild), where multi-scale temporal element entanglement and
cross-source temporal structural imbalances are common. To bridge this gap, we
study the task of TKGA-Wild and propose HyDRA, a new and effective solution.
HyDRA is the first to reformulate the task via multi-scale hypergraph
retrieval-augmented generation to address the challenges of TKGA-Wild.In
addition, we design a new scale-weave synergy mechanism for HyDRA, which
incorporates intra-scale interactions and cross-scale conflict detection. This
mechanism is designed to alleviate the fragmentation caused by multi-source
temporal incompleteness and resolves inconsistencies arising from complex and
uneven temporal event density distributions, thereby enhancing the model
capacity to handle the intricacies of real-world temporal alignment. Finally,
there is no standard benchmark that captures these challenges of TKGA-Wild and
effectively evaluates existing methods. To this end, we formally propose to
benchmark challenges for TKGA-Wild and validate the effectiveness of the method
by establishing two new datasets(BETA and WildBETA). Extensive experiments on
the new datasets and six representative benchmarks show that BETA and WildBETA
better reflect real-world challenges. Meanwhile, HyDRA proposes a new paradigm
for TKGA-Wild, consistently outperforming 24 competitive baselines, while
maintaining strong efficiency and scalability.

</details>


### [138] [Opening The Black-Box: Explaining Learned Cost Models For Databases](https://arxiv.org/abs/2507.14495)
*Roman Heinrich,Oleksandr Havrylov,Manisha Luthra,Johannes Wehrstein,Carsten Binnig*

Main category: cs.DB

TL;DR: This paper presents a new approach to explain and debug Learned Cost Models (LCMs) using AI explainability techniques, with an interactive tool for demonstration.


<details>
  <summary>Details</summary>
Motivation: Existing Learned Cost Models (LCMs) have prediction errors, particularly in the tail, and their complexity makes it difficult to understand the source of these errors.

Method: The authors developed new explanation techniques adapted from existing AI explainability methods to be usable for LCMs.

Result: The authors provide an interactive tool demonstrating the explainability of LCMs.

Conclusion: This paper introduces a novel approach to explain the predictions of Learned Cost Models (LCMs) using AI explainability techniques, aiming to make LCMs debuggable and improve their accuracy.

Abstract: Learned Cost Models (LCMs) have shown superior results over traditional
database cost models as they can significantly improve the accuracy of cost
predictions. However, LCMs still fail for some query plans, as prediction
errors can be large in the tail. Unfortunately, recent LCMs are based on
complex deep neural models, and thus, there is no easy way to understand where
this accuracy drop is rooted, which critically prevents systematic
troubleshooting. In this demo paper, we present the very first approach for
opening the black box by bringing AI explainability approaches to LCMs. As a
core contribution, we developed new explanation techniques that extend existing
methods that are available for the general explainability of AI models and
adapt them significantly to be usable for LCMs. In our demo, we provide an
interactive tool to showcase how explainability for LCMs works. We believe this
is a first step for making LCMs debuggable and thus paving the road for new
approaches for systematically fixing problems in LCMs.

</details>


### [139] [IDSS, a Novel P2P Relational Data Storage Service](https://arxiv.org/abs/2507.14682)
*Massimo Cafaro,Italo Epicoco,Marco Pulimeno,Lunodzo J. Mwinuka,Lucas Pereira,Hugo Morais*

Main category: cs.DB

TL;DR: This paper introduces IDSS, a novel large-scale data storage tool that leverages peer-to-peer networks and embedded relational databases to support complex distributed query processing, enabling robust and efficient management of vast amounts of data.


<details>
  <summary>Details</summary>
Motivation: Traditional database management systems suffer from scalability and are usually inefficient when dealing with large-scale and heterogeneous data. The rate at which data is generated has been increasing rapidly, raising challenges related to its management.

Method: This paper introduces IDSS (InnoCyPES Data Storage Service), a novel large-scale data storage tool that leverages peer-to-peer networks and embedded relational databases. The peer-to-peer framework is used to provide support for distributed queries leveraging a relational database architecture based on a common schema.

Result: We present the IDSS architecture and its design, and provide details related to the implementation.

Conclusion: Methods to support complex distributed query processing, enabling robust and efficient management of vast amounts of data are presented.

Abstract: The rate at which data is generated has been increasing rapidly, raising
challenges related to its management. Traditional database management systems
suffer from scalability and are usually inefficient when dealing with
large-scale and heterogeneous data. This paper introduces IDSS (InnoCyPES Data
Storage Service), a novel large-scale data storage tool that leverages
peer-to-peer networks and embedded relational databases. We present the IDSS
architecture and its design, and provide details related to the implementation.
The peer-to-peer framework is used to provide support for distributed queries
leveraging a relational database architecture based on a common schema.
Furthermore, methods to support complex distributed query processing, enabling
robust and efficient management of vast amounts of data are presented.

</details>


### [140] [Mayura: Exploiting Similarities in Motifs for Temporal Co-Mining](https://arxiv.org/abs/2507.14813)
*Sanjay Sri Vallabh Singapuram,Ronald Dreslinski,Nishil Talati*

Main category: cs.DB

TL;DR: Mayura通过共享公共搜索路径来加速时间motif挖掘，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 传统motif挖掘方法独立处理每个查询，当多个motif存在相似子结构时，会产生大量冗余计算。时间图是建模金融网络和社交媒体等领域中演化交互的关键基础。挖掘时间motif对于欺诈检测、网络安全和动态网络分析等应用至关重要。

Method: 论文提出了Motif-Group Tree (MG-Tree)，一种组织相关motif的分层数据结构，并开发了一种利用MG-Tree的协同挖掘算法。

Result: Mayura在各种真实世界数据集上的实验评估表明，相对于单独挖掘每个motif的现有技术，Mayura取得了显著的改进。

Conclusion: Mayura通过利用结构和时间上的共性统一了多个时间motif的挖掘，在CPU上平均加速2.4倍，在GPU上平均加速1.7倍，同时保持了高风险应用所需的精确性。

Abstract: Temporal graphs serve as a critical foundation for modeling evolving
interactions in domains ranging from financial networks to social media. Mining
temporal motifs is essential for applications such as fraud detection,
cybersecurity, and dynamic network analysis. However, conventional motif mining
approaches treat each query independently, incurring significant redundant
computations when similar substructures exist across multiple motifs. In this
paper, we propose Mayura, a novel framework that unifies the mining of multiple
temporal motifs by exploiting their inherent structural and temporal
commonalities. Central to our approach is the Motif-Group Tree (MG-Tree), a
hierarchical data structure that organizes related motifs and enables the reuse
of common search paths, thereby reducing redundant computation. We propose a
co-mining algorithm that leverages the MG-Tree and develop a flexible runtime
capable of exploiting both CPU and GPU architectures for scalable performance.
Empirical evaluations on diverse real-world datasets demonstrate that Mayura
achieves substantial improvements over the state-of-the-art techniques that
mine each motif individually, with an average speed-up of 2.4x on the CPU and
1.7x on the GPU, while maintaining the exactness required for high-stakes
applications.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [141] [LOVO: Efficient Complex Object Query in Large-Scale Video Datasets](https://arxiv.org/abs/2507.14301)
*Yuxin Liu,Yuezhang Peng,Hefeng Zhou,Hongze Liu,Xinyu Lu,Jiong Lou,Chentao Wu,Wei Zhao,Jie Li*

Main category: cs.IR

TL;DR: LOVO is a novel system designed to efficiently handle complex object queries in large-scale video datasets. It uses pre-trained visual encoders and a multi-index structure within a vector database to achieve high accuracy and low latency.


<details>
  <summary>Details</summary>
Motivation: Querying specific objects from large-scale video datasets presents challenges, including (1) processing massive and continuously growing data volumes, (2) supporting complex query requirements, and (3) ensuring low-latency execution. Existing video analysis methods struggle with either limited adaptability to unseen object classes or suffer from high query latency.

Method: LOVO performs one-time feature extraction using pre-trained visual encoders, generating compact visual embeddings for key frames to build an efficient index. These visual embeddings, along with associated bounding boxes, are organized in an inverted multi-index structure within a vector database, which supports queries for any objects. During the query phase, LOVO transforms object queries to query embeddings and conducts fast approximate nearest-neighbor searches on the visual embeddings. Finally, a cross-modal rerank is performed to refine the results by fusing visual features with detailed textual features.

Result: LOVO outperforms existing methods in handling complex queries, with near-optimal query accuracy and up to 85x lower search latency, while significantly reducing index construction costs.

Conclusion: LOVO outperforms existing methods in handling complex queries, with near-optimal query accuracy and up to 85x lower search latency, while significantly reducing index construction costs. This system redefines the state-of-the-art object query approaches in video analysis, setting a new benchmark for complex object queries with a novel, scalable, and efficient approach that excels in dynamic environments.

Abstract: The widespread deployment of cameras has led to an exponential increase in
video data, creating vast opportunities for applications such as traffic
management and crime surveillance. However, querying specific objects from
large-scale video datasets presents challenges, including (1) processing
massive and continuously growing data volumes, (2) supporting complex query
requirements, and (3) ensuring low-latency execution. Existing video analysis
methods struggle with either limited adaptability to unseen object classes or
suffer from high query latency. In this paper, we present LOVO, a novel system
designed to efficiently handle comp$\underline{L}$ex $\underline{O}$bject
queries in large-scale $\underline{V}$ide$\underline{O}$ datasets. Agnostic to
user queries, LOVO performs one-time feature extraction using pre-trained
visual encoders, generating compact visual embeddings for key frames to build
an efficient index. These visual embeddings, along with associated bounding
boxes, are organized in an inverted multi-index structure within a vector
database, which supports queries for any objects. During the query phase, LOVO
transforms object queries to query embeddings and conducts fast approximate
nearest-neighbor searches on the visual embeddings. Finally, a cross-modal
rerank is performed to refine the results by fusing visual features with
detailed textual features. Evaluation on real-world video datasets demonstrates
that LOVO outperforms existing methods in handling complex queries, with
near-optimal query accuracy and up to 85x lower search latency, while
significantly reducing index construction costs. This system redefines the
state-of-the-art object query approaches in video analysis, setting a new
benchmark for complex object queries with a novel, scalable, and efficient
approach that excels in dynamic environments.

</details>


### [142] [A Reproducibility Study of Product-side Fairness in Bundle Recommendation](https://arxiv.org/abs/2507.14352)
*Huy-Son Nguyen,Yuanna Liu,Masoud Mansoury,Mohammad Alian Nejadi,Alan Hanjalic,Maarten de Rijke*

Main category: cs.IR

TL;DR: 该论文研究了一揽子推荐中的产品方公平性问题，发现曝光模式在不同的一揽子和项目之间存在差异，且公平评估因指标而异。


<details>
  <summary>Details</summary>
Motivation: 推荐系统存在公平性问题，尤其是在产品方面，产品及其相关供应商在推荐结果中获得的曝光不均等。虽然这个问题在传统推荐设置中已被广泛研究，但其对一揽子推荐的影响在很大程度上仍未被探索。这项新兴的任务引入了额外的复杂性：推荐是在一揽子层面生成的，但用户满意度和产品（或供应商）曝光取决于一揽子及其包含的各个项目。

Method: 该论文对三种真实世界数据集使用四种最先进的一揽子推荐方法，进行了一项关于一揽子推荐中产品方公平性的全面可重复性研究。该研究使用多个公平性指标分析了一揽子和项目层面的曝光差异，揭示了重要的模式。

Result: 该论文的结果表明，当用户与一揽子互动更频繁时，一揽子推荐系统往往会在两个层面上产生更公平的曝光分布。同时发现，公平评估因所使用的指标而异，加强了多方面评估的必要性。

Conclusion: 这篇论文的结果表明，一揽子推荐中的曝光模式在不同的一揽子和项目之间存在显著差异，揭示了需要超越一揽子层面假设的公平干预。同时发现，公平评估因所使用的指标而异，加强了多方面评估的必要性。此外，用户行为也起着关键作用：当用户与一揽子的互动比与单个项目的互动更频繁时，一揽子推荐系统往往会在两个层面上产生更公平的曝光分布。

Abstract: Recommender systems are known to exhibit fairness issues, particularly on the
product side, where products and their associated suppliers receive unequal
exposure in recommended results. While this problem has been widely studied in
traditional recommendation settings, its implications for bundle recommendation
(BR) remain largely unexplored. This emerging task introduces additional
complexity: recommendations are generated at the bundle level, yet user
satisfaction and product (or supplier) exposure depend on both the bundle and
the individual items it contains. Existing fairness frameworks and metrics
designed for traditional recommender systems may not directly translate to this
multi-layered setting. In this paper, we conduct a comprehensive
reproducibility study of product-side fairness in BR across three real-world
datasets using four state-of-the-art BR methods. We analyze exposure
disparities at both the bundle and item levels using multiple fairness metrics,
uncovering important patterns. Our results show that exposure patterns differ
notably between bundles and items, revealing the need for fairness
interventions that go beyond bundle-level assumptions. We also find that
fairness assessments vary considerably depending on the metric used,
reinforcing the need for multi-faceted evaluation. Furthermore, user behavior
plays a critical role: when users interact more frequently with bundles than
with individual items, BR systems tend to yield fairer exposure distributions
across both levels. Overall, our findings offer actionable insights for
building fairer bundle recommender systems and establish a vital foundation for
future research in this emerging domain.

</details>


### [143] [RaMen: Multi-Strategy Multi-Modal Learning for Bundle Construction](https://arxiv.org/abs/2507.14361)
*Huy-Son Nguyen,Quang-Huy Nguyen,Duc-Hoang Pham,Duc-Trong Le,Hoang-Quynh Le,Padipat Sitkrongwong,Atsuhiro Takasu,Masoud Mansoury*

Main category: cs.IR

TL;DR: RaMen是一种bundle构建的新方法，它利用显式和隐式策略感知学习来建模bundle结构，从而在多个领域优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有的bundle构建研究仅仅依赖于通过二分图的用户反馈或使用语义信息增强的项目表示，未能捕捉到现实世界bundle结构中隐藏的精细关系，导致次优的bundle表示。

Method: RaMen，一种新颖的bundle构建方法，它通过显式策略感知学习 (ESL) 和隐式策略感知学习 (ISL) 来建模bundle结构。

Result: RaMen学习更全面和强大的bundle表示。

Conclusion: RaMen在多个领域都优于现有模型，证明了其在复杂项目集问题上的价值。

Abstract: Existing studies on bundle construction have relied merely on user feedback
via bipartite graphs or enhanced item representations using semantic
information. These approaches fail to capture elaborate relations hidden in
real-world bundle structures, resulting in suboptimal bundle representations.
To overcome this limitation, we propose RaMen, a novel method that provides a
holistic multi-strategy approach for bundle construction. RaMen utilizes both
intrinsic (characteristics) and extrinsic (collaborative signals) information
to model bundle structures through Explicit Strategy-aware Learning (ESL) and
Implicit Strategy-aware Learning (ISL). ESL employs task-specific attention
mechanisms to encode multi-modal data and direct collaborative relations
between items, thereby explicitly capturing essential bundle features.
Moreover, ISL computes hyperedge dependencies and hypergraph message passing to
uncover shared latent intents among groups of items. Integrating diverse
strategies enables RaMen to learn more comprehensive and robust bundle
representations. Meanwhile, Multi-strategy Alignment & Discrimination module is
employed to facilitate knowledge transfer between learning strategies and
ensure discrimination between items/bundles. Extensive experiments demonstrate
the effectiveness of RaMen over state-of-the-art models on various domains,
justifying valuable insights into complex item set problems.

</details>


### [144] [Understanding Matching Mechanisms in Cross-Encoders](https://arxiv.org/abs/2507.14604)
*Mathias Vast,Basile Van Cooten,Laure Soulier,Benjamin Piwowarski*

Main category: cs.IR

TL;DR: This paper explains the matching process in neural IR architectures by focusing on attention heads and matching detection mechanisms.


<details>
  <summary>Details</summary>
Motivation: The internal mechanisms of neural IR architectures, particularly cross-encoders, are mostly unknown. Most works fall short of describing the matching process.

Method: The paper uses straightforward methods to provide valuable insights into the matching process of neural IR architectures.

Result: Extracted causal insights highlighting the crucial roles of some attention heads and an interpretation of the mechanism underlying matching detection.

Conclusion: This paper focuses on the attention process and extracts causal insights highlighting the crucial roles of some attention heads. It also provides an interpretation of the mechanism underlying matching detection.

Abstract: Neural IR architectures, particularly cross-encoders, are highly effective
models whose internal mechanisms are mostly unknown. Most works trying to
explain their behavior focused on high-level processes (e.g., what in the input
influences the prediction, does the model adhere to known IR axioms) but fall
short of describing the matching process. Instead of Mechanistic
Interpretability approaches which specifically aim at explaining the hidden
mechanisms of neural models, we demonstrate that more straightforward methods
can already provide valuable insights. In this paper, we first focus on the
attention process and extract causal insights highlighting the crucial roles of
some attention heads in this process. Second, we provide an interpretation of
the mechanism underlying matching detection.

</details>


### [145] [Enhancing POI Recommendation through Global Graph Disentanglement with POI Weighted Module](https://arxiv.org/abs/2507.14612)
*Pei-Xuan Li,Wei-Yun Liang,Fandel Lin,Hsun-Ping Hsieh*

Main category: cs.IR

TL;DR: This paper proposes a novel next POI recommendation framework called Graph Disentangler with POI Weighted Module (GDPW) to jointly consider POI category information and multiple POI weighting factors.


<details>
  <summary>Details</summary>
Motivation: Existing methods rarely explore the relationship between POI categories and time, making it difficult to capture the continuity of time, and often ignore various weighting information.

Method: The proposed GDPW learns category and time representations through the Global Category Graph and the Global Category-Time Graph. Then, we disentangle category and time information through contrastive learning. After prediction, the final POI recommendation for users is obtained by weighting the prediction results based on the transition weights and distance relationships between POIs.

Result: The proposed GDPW improves performance by 3% to 11% compared to existing models on two real-world datasets.

Conclusion: The proposed GDPW outperforms other existing models, improving performance by 3% to 11%.

Abstract: Next point of interest (POI) recommendation primarily predicts future
activities based on users' past check-in data and current status, providing
significant value to users and service providers. We observed that the popular
check-in times for different POI categories vary. For example, coffee shops are
crowded in the afternoon because people like to have coffee to refresh after
meals, while bars are busy late at night. However, existing methods rarely
explore the relationship between POI categories and time, which may result in
the model being unable to fully learn users' tendencies to visit certain POI
categories at different times. Additionally, existing methods for modeling time
information often convert it into time embeddings or calculate the time
interval and incorporate it into the model, making it difficult to capture the
continuity of time. Finally, during POI prediction, various weighting
information is often ignored, such as the popularity of each POI, the
transition relationships between POIs, and the distances between POIs, leading
to suboptimal performance. To address these issues, this paper proposes a novel
next POI recommendation framework called Graph Disentangler with POI Weighted
Module (GDPW). This framework aims to jointly consider POI category information
and multiple POI weighting factors. Specifically, the proposed GDPW learns
category and time representations through the Global Category Graph and the
Global Category-Time Graph. Then, we disentangle category and time information
through contrastive learning. After prediction, the final POI recommendation
for users is obtained by weighting the prediction results based on the
transition weights and distance relationships between POIs. We conducted
experiments on two real-world datasets, and the results demonstrate that the
proposed GDPW outperforms other existing models, improving performance by 3% to
11%.

</details>


### [146] [Optimizing Legal Document Retrieval in Vietnamese with Semi-Hard Negative Mining](https://arxiv.org/abs/2507.14619)
*Van-Hoang Le,Duc-Vu Nguyen,Kiet Van Nguyen,Ngan Luu-Thuy Nguyen*

Main category: cs.IR

TL;DR: 本文提出了一种用于法律文件检索的两阶段框架，该框架使用微调的Bi-Encoder和Cross-Encoder，并在SoICT Hackathon 2024上取得了前三名的成绩。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在法律等专业领域面临重大挑战，在这些领域，精确性和领域特定知识至关重要。本文提出了一个简化的两阶段框架，包括检索和重新排序，以提高法律文件检索的效率和准确性。

Method: 两阶段框架，包括用于快速候选检索的微调Bi-Encoder和用于精确重新排序的Cross-Encoder，通过策略性负例挖掘进行优化。

Result: 在SoICT Hackathon 2024法律文件检索竞赛中，我们的团队4Huiter取得了前三名的成绩。虽然表现最好的团队采用了大型bge-m3架构上的集成模型和迭代自训练，但我们轻量级的单次通过方法提供了一种具有竞争力的替代方案，参数要少得多。

Conclusion: 优化的数据处理、定制的损失函数和平衡的负采样对于在法律背景下构建强大的检索增强系统至关重要。

Abstract: Large Language Models (LLMs) face significant challenges in specialized
domains like law, where precision and domain-specific knowledge are critical.
This paper presents a streamlined two-stage framework consisting of Retrieval
and Re-ranking to enhance legal document retrieval efficiency and accuracy. Our
approach employs a fine-tuned Bi-Encoder for rapid candidate retrieval,
followed by a Cross-Encoder for precise re-ranking, both optimized through
strategic negative example mining. Key innovations include the introduction of
the Exist@m metric to evaluate retrieval effectiveness and the use of semi-hard
negatives to mitigate training bias, which significantly improved re-ranking
performance. Evaluated on the SoICT Hackathon 2024 for Legal Document
Retrieval, our team, 4Huiter, achieved a top-three position. While
top-performing teams employed ensemble models and iterative self-training on
large bge-m3 architectures, our lightweight, single-pass approach offered a
competitive alternative with far fewer parameters. The framework demonstrates
that optimized data processing, tailored loss functions, and balanced negative
sampling are pivotal for building robust retrieval-augmented systems in legal
contexts.

</details>


### [147] [U-MARVEL: Unveiling Key Factors for Universal Multimodal Retrieval via Embedding Learning with MLLMs](https://arxiv.org/abs/2507.14902)
*Xiaojie Li,Chu Li,Shi-Zhe Chen,Xi Chen*

Main category: cs.IR

TL;DR: This paper introduces U-MARVEL, a new framework for universal multimodal retrieval that outperforms existing methods by analyzing key factors in MLLM embedding learning and incorporating techniques like progressive transition and hard negative mining.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the issue that the mechanisms underlying the retrieval capabilities of state-of-the-art MLLM-based methods for universal multimodal retrieval (UMR) are largely unexplored, potentially resulting in suboptimal performance and limited generalization ability.

Method: The paper implements a general MLLM-based embedding learning pipeline and systematically analyzes the primary contributors to high-performing universal retrieval systems. It explores embedding generation and training strategies, including progressive transition, hard negative mining, and re-ranker distillation.

Result: The paper's findings reveal that often-overlooked factors can have a substantial impact on model performance. The proposed U-MARVEL framework outperforms state-of-the-art competitors on the M-BEIR benchmark by a large margin in supervised settings and exhibits strong zero-shot performance on several tasks.

Conclusion: The paper introduces a unified framework called U-MARVEL for universal multimodal retrieval (UMR) that outperforms state-of-the-art methods on the M-BEIR benchmark in supervised settings and demonstrates strong zero-shot performance on various tasks. The results highlight the generalization potential of the framework.

Abstract: Universal multimodal retrieval (UMR), which aims to address complex retrieval
tasks where both queries and candidates span diverse modalities, has been
significantly advanced by the emergence of MLLMs. While state-of-the-art
MLLM-based methods in the literature predominantly adopt contrastive learning
principles, they often differ in their specific training recipes. Despite their
success, the mechanisms underlying their retrieval capabilities remain largely
unexplored, potentially resulting in suboptimal performance and limited
generalization ability. To address these issues, we present a comprehensive
study aimed at uncovering the key factors that drive effective embedding
learning for UMR using MLLMs. We begin by implementing a general MLLM-based
embedding learning pipeline, and systematically analyze the primary
contributors to high-performing universal retrieval systems. Based on this, we
explore various aspects of the details in embedding generation and training
strategies, including progressive transition, hard negative mining and
re-ranker distillation. Notably, our findings reveal that often-overlooked
factors can have a substantial impact on model performance. Building on these
discoveries, we introduce a unified framework termed U-MARVEL
(\textbf{U}niversal \textbf{M}ultimod\textbf{A}l \textbf{R}etrie\textbf{V}al
via \textbf{E}mbedding \textbf{L}earning), which outperforms state-of-the-art
competitors on the M-BEIR benchmark by a large margin in supervised settings,
and also exihibits strong zero-shot performance on several tasks such as
composed image retrieval and text-to-video retrieval. These results underscore
the generalization potential of our framework across various embedding-based
retrieval tasks. Code is available at https://github.com/chaxjli/U-MARVEL

</details>


### [148] [User Invariant Preference Learning for Multi-Behavior Recommendation](https://arxiv.org/abs/2507.14925)
*Mingshi Yan,Zhiyong Cheng,Fan Liu,Yingda Lyu,Yahong Han*

Main category: cs.IR

TL;DR: This paper proposes a user invariant preference learning method (UIPL) for multi-behavior recommendation to capture users' intrinsic interests and mitigate noise. It uses invariant risk minimization and a variational autoencoder to learn invariant preferences from multi-behavior data. Experiments show UIPL outperforms state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: existing approaches often overlook the presence of both commonalities and individualities in users' multi-behavior preferences. These individualities reflect distinct aspects of preferences captured by different behaviors, where certain auxiliary behaviors may introduce noise, hindering the prediction of the target behavior

Method: propose a user invariant preference learning for multi-behavior recommendation (UIPL for short), aiming to capture users' intrinsic interests (referred to as invariant preferences) from multi-behavior interactions to mitigate the introduction of noise. Specifically, UIPL leverages the paradigm of invariant risk minimization to learn invariant preferences. To implement this, we employ a variational autoencoder (VAE) to extract users' invariant preferences, replacing the standard reconstruction loss with an invariant risk minimization constraint. Additionally, we construct distinct environments by combining multi-behavior data to enhance robustness in learning these preferences.

Result: the learned invariant preferences are used to provide recommendations for the target behavior

Conclusion: Extensive experiments on four real-world datasets demonstrate that UIPL significantly outperforms current state-of-the-art methods.

Abstract: In multi-behavior recommendation scenarios, analyzing users' diverse
behaviors, such as click, purchase, and rating, enables a more comprehensive
understanding of their interests, facilitating personalized and accurate
recommendations. A fundamental assumption of multi-behavior recommendation
methods is the existence of shared user preferences across behaviors,
representing users' intrinsic interests. Based on this assumption, existing
approaches aim to integrate information from various behaviors to enrich user
representations. However, they often overlook the presence of both
commonalities and individualities in users' multi-behavior preferences. These
individualities reflect distinct aspects of preferences captured by different
behaviors, where certain auxiliary behaviors may introduce noise, hindering the
prediction of the target behavior. To address this issue, we propose a user
invariant preference learning for multi-behavior recommendation (UIPL for
short), aiming to capture users' intrinsic interests (referred to as invariant
preferences) from multi-behavior interactions to mitigate the introduction of
noise. Specifically, UIPL leverages the paradigm of invariant risk minimization
to learn invariant preferences. To implement this, we employ a variational
autoencoder (VAE) to extract users' invariant preferences, replacing the
standard reconstruction loss with an invariant risk minimization constraint.
Additionally, we construct distinct environments by combining multi-behavior
data to enhance robustness in learning these preferences. Finally, the learned
invariant preferences are used to provide recommendations for the target
behavior. Extensive experiments on four real-world datasets demonstrate that
UIPL significantly outperforms current state-of-the-art methods.

</details>


### [149] [FullRecall: A Semantic Search-Based Ranking Approach for Maximizing Recall in Patent Retrieval](https://arxiv.org/abs/2507.14946)
*Amna Ali,Liyanage C. De Silva,Pg Emeroylariffion Abas*

Main category: cs.IR

TL;DR: FullRecall, a novel patent retrieval approach, achieves 100% recall by using IPC-guided knowledge and a multi-phase process, outperforming baseline studies.


<details>
  <summary>Details</summary>
Motivation: Patent examiners and inventors face significant pressure to verify the originality and non-obviousness of inventions, and the intricate nature of patent data intensifies the challenges of patent retrieval. Therefore, there is a pressing need to devise cutting-edge retrieval strategies that can reliably achieve the desired recall.

Method: This study introduces FullRecall, a novel patent retrieval approach that effectively manages the complexity of patent data while maintaining the reliability of relevance matching and maximising recall. It leverages IPC-guided knowledge to generate informative phrases, which are processed to extract key information in the form of noun phrases characterising the query patent under observation. From these, the top k keyphrases are selected to construct a query for retrieving a focused subset of the dataset. This initial retrieval step achieves complete recall, successfully capturing all relevant documents. To further refine the results, a ranking scheme is applied to the retrieved subset, reducing its size while maintaining 100% recall. This multi-phase process demonstrates an effective strategy for balancing precision and recall in patent retrieval tasks.

Result: The proposed approach yielded superior results, achieving 100% recall in all five test cases. However, HRR2[1] recall values across the five test cases were 10%, 25%, 33.3%, 0%, and 14.29%, while ReQ-ReC [2] showed 50% for the first test case, 25% for the second test case, and 0% for the third, fourth, and fifth test cases.

Conclusion: The proposed approach yielded superior results, achieving 100% recall in all five test cases. The 100% recall ensures that no relevant prior art is overlooked, thereby strengthening the patent pre-filing and examination processes, hence reducing potential legal risks.

Abstract: Patent examiners and inventors face significant pressure to verify the
originality and non-obviousness of inventions, and the intricate nature of
patent data intensifies the challenges of patent retrieval. Therefore, there is
a pressing need to devise cutting-edge retrieval strategies that can reliably
achieve the desired recall. This study introduces FullRecall, a novel patent
retrieval approach that effectively manages the complexity of patent data while
maintaining the reliability of relevance matching and maximising recall. It
leverages IPC-guided knowledge to generate informative phrases, which are
processed to extract key information in the form of noun phrases characterising
the query patent under observation. From these, the top k keyphrases are
selected to construct a query for retrieving a focused subset of the dataset.
This initial retrieval step achieves complete recall, successfully capturing
all relevant documents. To further refine the results, a ranking scheme is
applied to the retrieved subset, reducing its size while maintaining 100%
recall. This multi-phase process demonstrates an effective strategy for
balancing precision and recall in patent retrieval tasks. Comprehensive
experiments were conducted, and the results were compared with baseline
studies, namely HRR2 [1] and ReQ-ReC [2]. The proposed approach yielded
superior results, achieving 100% recall in all five test cases. However,
HRR2[1] recall values across the five test cases were 10%, 25%, 33.3%, 0%, and
14.29%, while ReQ-ReC [2] showed 50% for the first test case, 25% for the
second test case, and 0% for the third, fourth, and fifth test cases. The 100%
recall ensures that no relevant prior art is overlooked, thereby strengthening
the patent pre-filing and examination processes, hence reducing potential legal
risks.

</details>


### [150] [Click A, Buy B: Rethinking Conversion Attribution in E- Commerce Recommendations](https://arxiv.org/abs/2507.15113)
*Xiangyu Zeng,Amit Jaspal,Bin Liu,Goutham Panneeru,Kevin Huang,Nicolas Bievre,Mohit Jaggi,Prathap Maniraju,Ankur Jain*

Main category: cs.IR

TL;DR: 该论文研究了电商中点击商品和购买商品不一致的问题，通过多任务学习和分类感知协同过滤加权方案，提高了推荐模型的转化率。


<details>
  <summary>Details</summary>
Motivation: 在电商用户旅程中，点击商品和最终购买商品不一致的现象普遍存在，导致使用原始点击-转化对训练推荐模型会产生偏差，降低转化率。

Method: 该论文提出了一种多任务学习框架，具有针对Click A Buy A (CABA)和Click A Buy B (CABB)的独立head，并引入了一种分类感知协同过滤加权方案。

Result: 离线评估显示，相对于last-click归因基线，归一化熵降低了13.9%。在线A/B测试表明，主要业务指标提升了0.25%。

Conclusion: 通过将转化预测重新定义为多任务问题，并引入一种分类感知协同过滤加权方案，该论文提高了电商环境下的推荐模型性能。

Abstract: User journeys in e-commerce routinely violate the one-to-one assumption that
a clicked item on an advertising platform is the same item later purchased on
the merchant's website/app. For a significant number of converting sessions on
our platform, users click product A but buy product B -- the Click A, Buy B
(CABB) phenomenon. Training recommendation models on raw click-conversion pairs
therefore rewards items that merely correlate with purchases, leading to biased
learning and sub-optimal conversion rates. We reframe conversion prediction as
a multi-task problem with separate heads for Click A Buy A (CABA) and Click A
Buy B (CABB). To isolate informative CABB conversions from unrelated CABB
conversions, we introduce a taxonomy-aware collaborative filtering weighting
scheme where each product is first mapped to a leaf node in a product taxonomy,
and a category-to-category similarity matrix is learned from large-scale
co-engagement logs. This weighting amplifies pairs that reflect genuine
substitutable or complementary relations while down-weighting coincidental
cross-category purchases. Offline evaluation on e-commerce sessions reduces
normalized entropy by 13.9% versus a last-click attribution baseline. An online
A/B test on live traffic shows +0.25% gains in the primary business metric.

</details>


### [151] [SPAR: Scholar Paper Retrieval with LLM-based Agents for Enhanced Academic Search](https://arxiv.org/abs/2507.15245)
*Xiaofeng Shi,Yuduo Li,Qian Kou,Longbin Yu,Jinxin Xie,Hua Zhou*

Main category: cs.IR

TL;DR: SPAR, a multi-agent framework, outperforms existing systems in academic literature retrieval with significant F1 score improvements on AutoScholar and SPARBench.


<details>
  <summary>Details</summary>
Motivation: existing systems often rely on rigid pipelines and exhibit limited reasoning capabilities.

Method: a multi-agent framework that incorporates RefChain-based query decomposition and query evolution

Result: SPAR substantially outperforms strong baselines, achieving up to +56% F1 on AutoScholar and +23% F1 on SPARBench over the best-performing baseline.

Conclusion: SPAR and SPARBench provide a scalable, interpretable, and high-performing foundation for advancing research in scholarly retrieval.

Abstract: Recent advances in large language models (LLMs) have opened new opportunities
for academic literature retrieval. However, existing systems often rely on
rigid pipelines and exhibit limited reasoning capabilities. We introduce SPAR,
a multi-agent framework that incorporates RefChain-based query decomposition
and query evolution to enable more flexible and effective search. To facilitate
systematic evaluation, we also construct SPARBench, a challenging benchmark
with expert-annotated relevance labels. Experimental results demonstrate that
SPAR substantially outperforms strong baselines, achieving up to +56% F1 on
AutoScholar and +23% F1 on SPARBench over the best-performing baseline.
Together, SPAR and SPARBench provide a scalable, interpretable, and
high-performing foundation for advancing research in scholarly retrieval. Code
and data will be available at: https://github.com/xiaofengShi/SPAR

</details>


### [152] [GREAT: Guiding Query Generation with a Trie for Recommending Related Search about Video at Kuaishou](https://arxiv.org/abs/2507.15267)
*Ninglu Shao,Jinshan Wang,Chenxu Wang,Qingbiao Li,Xiaoxue Zang,Han Li*

Main category: cs.IR

TL;DR: This paper addresses the lack of research in video-related query recommendation by introducing GREAT, an LLM-based framework using a trie to generate better queries, and releases a new large-scale dataset.


<details>
  <summary>Details</summary>
Motivation: There is a scarcity of academic research and publicly available datasets in the emerging field of query recommendation in video-related search (item-to-query recommendation). Existing methods lack deep interaction between semantic content and the query.

Method: The paper introduces a novel LLM-based framework named GREAT, which guides query generation with a trie. It gathers high-quality queries to construct a query-based trie, enhances the LLM's generation capability using the trie during training, and uses the trie to guide token generation during inference. A post-processing module refines relevance and literal quality.

Result: The paper releases a large-scale dataset (KuaiRS) derived from real-world data and demonstrates the effectiveness of the proposed GREAT method through extensive offline and online experiments.

Conclusion: The paper introduces GREAT, a novel LLM-based framework that guides query generation with a trie to address I2Q recommendation in related search. Extensive offline and online experiments demonstrate the effectiveness of the proposed method.

Abstract: Currently, short video platforms have become the primary place for
individuals to share experiences and obtain information. To better meet users'
needs for acquiring information while browsing short videos, some apps have
introduced a search entry at the bottom of videos, accompanied with recommended
relevant queries. This scenario is known as query recommendation in
video-related search, where core task is item-to-query (I2Q) recommendation. As
this scenario has only emerged in recent years, there is a notable scarcity of
academic research and publicly available datasets in this domain. To address
this gap, we systematically examine the challenges associated with this
scenario for the first time. Subsequently, we release a large-scale dataset
derived from real-world data pertaining to the query recommendation in
video-\textit{\textbf{r}}elated \textit{\textbf{s}}earch on the
\textit{\textbf{Kuai}}shou app (\textbf{KuaiRS}). Presently, existing methods
rely on embeddings to calculate similarity for matching short videos with
queries, lacking deep interaction between the semantic content and the query.
In this paper, we introduce a novel LLM-based framework named \textbf{GREAT},
which \textit{\textbf{g}}uides que\textit{\textbf{r}}y
g\textit{\textbf{e}}ner\textit{\textbf{a}}tion with a \textit{\textbf{t}}rie to
address I2Q recommendation in related search. Specifically, we initially gather
high-quality queries with high exposure and click-through rate to construct a
query-based trie. During training, we enhance the LLM's capability to generate
high-quality queries using the query-based trie. In the inference phase, the
query-based trie serves as a guide for the token generation. Finally, we
further refine the relevance and literal quality between items and queries via
a post-processing module. Extensive offline and online experiments demonstrate
the effectiveness of our proposed method.

</details>


### [153] [Hierarchical Graph Information Bottleneck for Multi-Behavior Recommendation](https://arxiv.org/abs/2507.15395)
*Hengyu Zhang,Chunxu Shen,Xiangguo Sun,Jie Tan,Yanchao Tan,Yu Rong,Hong Cheng,Lingling Yi*

Main category: cs.IR

TL;DR: This paper introduces HGIB, a Hierarchical Graph Information Bottleneck framework for multi-behavior recommendation, addressing distribution disparities and negative transfer effects. It outperforms existing methods on public datasets and in real-world industrial scenarios.


<details>
  <summary>Details</summary>
Motivation: Current multi-behavior recommendation algorithms face challenges including severe distribution disparities across behaviors and negative transfer effects caused by noise in auxiliary behaviors.

Method: The paper proposes a novel model-agnostic Hierarchical Graph Information Bottleneck (HGIB) framework for multi-behavior recommendation, which optimizes the learning of compact representations and introduces a Graph Refinement Encoder (GRE) to dynamically prune redundant edges.

Result: The proposed HGIB framework achieves superior performance on three real-world public datasets, real industrial scenarios, and online A/B testing, demonstrating a significant improvement in multi-behavior recommendations.

Conclusion: The paper demonstrates the superior effectiveness of the proposed HGIB framework through comprehensive experiments on three real-world public datasets, real industrial scenarios, and online A/B testing, showing a significant improvement in multi-behavior recommendations.

Abstract: In real-world recommendation scenarios, users typically engage with platforms
through multiple types of behavioral interactions. Multi-behavior
recommendation algorithms aim to leverage various auxiliary user behaviors to
enhance prediction for target behaviors of primary interest (e.g., buy),
thereby overcoming performance limitations caused by data sparsity in target
behavior records. Current state-of-the-art approaches typically employ
hierarchical design following either cascading (e.g.,
view$\rightarrow$cart$\rightarrow$buy) or parallel
(unified$\rightarrow$behavior$\rightarrow$specific components) paradigms, to
capture behavioral relationships. However, these methods still face two
critical challenges: (1) severe distribution disparities across behaviors, and
(2) negative transfer effects caused by noise in auxiliary behaviors. In this
paper, we propose a novel model-agnostic Hierarchical Graph Information
Bottleneck (HGIB) framework for multi-behavior recommendation to effectively
address these challenges. Following information bottleneck principles, our
framework optimizes the learning of compact yet sufficient representations that
preserve essential information for target behavior prediction while eliminating
task-irrelevant redundancies. To further mitigate interaction noise, we
introduce a Graph Refinement Encoder (GRE) that dynamically prunes redundant
edges through learnable edge dropout mechanisms. We conduct comprehensive
experiments on three real-world public datasets, which demonstrate the superior
effectiveness of our framework. Beyond these widely used datasets in the
academic community, we further expand our evaluation on several real industrial
scenarios and conduct an online A/B testing, showing again a significant
improvement in multi-behavior recommendations. The source code of our proposed
HGIB is available at https://github.com/zhy99426/HGIB.

</details>


### [154] [RankMixer: Scaling Up Ranking Models in Industrial Recommenders](https://arxiv.org/abs/2507.15551)
*Jie Zhu,Zhifang Fan,Xiaoxie Zhu,Yuchen Jiang,Hangyu Wang,Xintian Han,Haoran Ding,Xinmin Wang,Wenlin Zhao,Zhen Gong,Huizhi Yang,Zheng Chai,Zhe Chen,Yuchao Zheng,Qiwei Chen,Feng Zhang,Xun Zhou,Peng Xu,Xiao Yang,Di Wu,Zuotao Liu*

Main category: cs.IR

TL;DR: RankMixer is a hardware-aware model that improves efficiency and scalability of large language models in recommendation systems, leading to better user engagement.


<details>
  <summary>Details</summary>
Motivation: Scaling up recommendation systems with LLMs faces challenges due to latency bounds, high QPS demands, and inefficient feature-crossing modules on GPUs.

Method: The paper introduces RankMixer, a hardware-aware model design with multi-head token mixing and Per-token FFNs, extended with a Sparse-MoE variant and dynamic routing strategy.

Result: RankMixer boosts model MFU from 4.5% to 45%, scales model parameters by 100x, and improves user active days by 0.2% and in-app usage duration by 0.5%.

Conclusion: RankMixer significantly improves model MFU, scales ranking model parameters, and enhances user engagement across various applications without increasing serving costs.

Abstract: Recent progress on large language models (LLMs) has spurred interest in
scaling up recommendation systems, yet two practical obstacles remain. First,
training and serving cost on industrial Recommenders must respect strict
latency bounds and high QPS demands. Second, most human-designed
feature-crossing modules in ranking models were inherited from the CPU era and
fail to exploit modern GPUs, resulting in low Model Flops Utilization (MFU) and
poor scalability. We introduce RankMixer, a hardware-aware model design
tailored towards a unified and scalable feature-interaction architecture.
RankMixer retains the transformer's high parallelism while replacing quadratic
self-attention with multi-head token mixing module for higher efficiency.
Besides, RankMixer maintains both the modeling for distinct feature subspaces
and cross-feature-space interactions with Per-token FFNs. We further extend it
to one billion parameters with a Sparse-MoE variant for higher ROI. A dynamic
routing strategy is adapted to address the inadequacy and imbalance of experts
training. Experiments show RankMixer's superior scaling abilities on a
trillion-scale production dataset. By replacing previously diverse handcrafted
low-MFU modules with RankMixer, we boost the model MFU from 4.5% to 45%, and
scale our ranking model parameters by 100x while maintaining roughly the same
inference latency. We verify RankMixer's universality with online A/B tests
across three core application scenarios (Recommendation, Advertisement and
Search). Finally, we launch 1B Dense-Parameters RankMixer for full traffic
serving without increasing the serving cost, which improves user active days by
0.2% and total in-app usage duration by 0.5%.

</details>


### [155] [Just Ask for Music (JAM): Multimodal and Personalized Natural Language Music Recommendation](https://arxiv.org/abs/2507.15826)
*Alessandro B. Melchiorre,Elena V. Epure,Shahed Masoudian,Gustavo Escobedo,Anna Hausberger,Manuel Moussallam,Markus Schedl*

Main category: cs.IR

TL;DR: JAM is a lightweight framework for natural language music recommendation that models user-query-item interactions as vector translations in a shared latent space. It introduces JAMSessions, a new dataset of over 100k user-query-item triples. JAM provides accurate recommendations, produces intuitive representations, and can be easily integrated with existing music recommendation stacks.


<details>
  <summary>Details</summary>
Motivation: Natural language interfaces offer a compelling approach for music recommendation, enabling users to express complex preferences conversationally. While Large Language Models (LLMs) show promise in this direction, their scalability in recommender systems is limited by high costs and latency. Retrieval-based approaches using smaller language models mitigate these issues but often rely on single-modal item representations, overlook long-term user preferences, and require full model retraining, posing challenges for real-world deployment.

Method: JAM models user-query-item interactions as vector translations in a shared latent space, inspired by knowledge graph embedding methods like TransE. To capture the complexity of music and user intent, JAM aggregates multimodal item features via cross-attention and sparse mixture-of-experts.

Result: JAM provides accurate recommendations and produces intuitive representations.

Conclusion: JAM provides accurate recommendations, produces intuitive representations suitable for practical use cases, and can be easily integrated with existing music recommendation stacks.

Abstract: Natural language interfaces offer a compelling approach for music
recommendation, enabling users to express complex preferences conversationally.
While Large Language Models (LLMs) show promise in this direction, their
scalability in recommender systems is limited by high costs and latency.
Retrieval-based approaches using smaller language models mitigate these issues
but often rely on single-modal item representations, overlook long-term user
preferences, and require full model retraining, posing challenges for
real-world deployment. In this paper, we present JAM (Just Ask for Music), a
lightweight and intuitive framework for natural language music recommendation.
JAM models user-query-item interactions as vector translations in a shared
latent space, inspired by knowledge graph embedding methods like TransE. To
capture the complexity of music and user intent, JAM aggregates multimodal item
features via cross-attention and sparse mixture-of-experts. We also introduce
JAMSessions, a new dataset of over 100k user-query-item triples with anonymized
user/item embeddings, uniquely combining conversational queries and user
long-term preferences. Our results show that JAM provides accurate
recommendations, produces intuitive representations suitable for practical use
cases, and can be easily integrated with existing music recommendation stacks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [156] [Catalyst: a Novel Regularizer for Structured Pruning with Auxiliary Extension of Parameter Space](https://arxiv.org/abs/2507.14170)
*Jaeheun Jung,Donghun Lee*

Main category: cs.LG

TL;DR: 提出了一种新的Catalyst正则化方法，用于结构化剪枝，该方法具有无偏、鲁棒和公平的特性，并在各种数据集和模型上取得了优于现有技术的性能。


<details>
  <summary>Details</summary>
Motivation: 结构化剪枝旨在通过移除整个滤波器或通道来减少深度神经网络的规模和计算成本。传统的正则化方法（如 L1 或 Group Lasso 及其变体）会导致幅度偏差的剪枝决策，因此幅度小的滤波器可能被剪枝。此外，它们通常会导致剪枝结果在剪枝决策边界附近几乎没有余量，因此滤波器幅度中的微小扰动可能会翻转剪枝决策。

Method: 提出了一种新的Catalyst正则化方法，该方法在扩展的参数空间中通过辅助催化剂变量定义，确保每个滤波器都有公平的剪枝机会，且理论上可证明对其幅度零偏差，并通过保留和剪枝滤波器之间幅度的大边距分叉实现鲁棒的剪枝行为。

Result: 所提出的Catalyst正则化确保每个滤波器都有公平的剪枝机会，且理论上可证明对其幅度零偏差，并通过保留和剪枝滤波器之间幅度的大边距分叉实现鲁棒的剪枝行为。理论性质自然会导致实际有效性，Catalyst Pruning算法的经验验证表明了这一点。

Conclusion: Catalyst Pruning算法在各种数据集和模型上的剪枝结果优于最先进的滤波器剪枝方法，同时证实了Catalyst剪枝的鲁棒性和公平剪枝特性。

Abstract: Structured pruning aims to reduce the size and computational cost of deep
neural networks by removing entire filters or channels. The traditional
regularizers such as L1 or Group Lasso and its variants lead to
magnitude-biased pruning decisions, such that the filters with small magnitudes
are likely to be pruned. Also, they often entail pruning results with almost
zero margin around pruning decision boundary, such that tiny perturbation in a
filter magnitude can flip the pruning decision. In this paper, we identify the
precise algebraic condition under which pruning operations preserve model
performance, and use the condition to construct a novel regularizer defined in
an extended parameter space via auxiliary catalyst variables. The proposed
Catalyst regularization ensures fair pruning chance for each filters with
theoretically provable zero bias to their magnitude and robust pruning behavior
achieved by wide-margin bifurcation of magnitudes between the preserved and the
pruned filters. The theoretical properties naturally lead to real-world
effectiveness, as shown by empirical validations of Catalyst Pruning algorithm.
Pruning results on various datasets and models are superior to state-of-the-art
filter pruning methods, and at the same time confirm the predicted robust and
fair pruning characteristics of Catalyst pruning.

</details>


### [157] [IPPRO: Importance-based Pruning with PRojective Offset for Magnitude-indifferent Structural Pruning](https://arxiv.org/abs/2507.14171)
*Jaeheun Jung,Jaehyuk Lee,Yeajin Lee,Donghun Lee*

Main category: cs.LG

TL;DR: 提出了一种新颖的剪枝策略，通过将每个滤波器放置在投影空间上，挑战幅度的主导效应，并为每个滤波器提供公平的剪枝机会。


<details>
  <summary>Details</summary>
Motivation: 基于重要性的结构化剪枝方法（包括基于重要性的方法）得到了积极的研究。幅度重要性和许多相关的现代重要性标准通常会限制剪枝决策的能力，因为如果较小的滤波器没有被剪枝，则较大的滤波器不太可能被剪枝，即使它是冗余的。

Method: 通过将每个滤波器放置在投影空间上，提供公平的剪枝机会，观察梯度下降运动，以测量滤波器被剪枝的可能性。使用该测量结果来构建 PROscore，这是一种新颖的重要性评分，用于 IPPRO，这是一种具有幅度无关性的基于重要性的结构化剪枝。

Result: 所提出的使用投影空间的重要性准则通过减少剪枝中的性能下降来实现接近无损的剪枝，并在微调后具有良好的性能。

Conclusion: 使用投影空间的重要性准则通过减少剪枝中的性能下降来实现接近无损的剪枝，并在微调后具有良好的性能。这项工作揭穿了剪枝中“大小决定一切”的 मिथ，并在理论和经验上扩展了基于重要性的剪枝的前沿。

Abstract: With the growth of demand on neural network compression methods, the
structured pruning methods including importance-based approach are actively
studied. The magnitude importance and many correlated modern importance
criteria often limit the capacity of pruning decision, since the filters with
larger magnitudes are not likely to be pruned if the smaller one didn't, even
if it is redundant. In this paper, we propose a novel pruning strategy to
challenge this dominating effect of magnitude and provide fair chance to each
filter to be pruned, by placing it on projective space. After that, we observe
the gradient descent movement whether the filters move toward the origin or
not, to measure how the filter is likely to be pruned. This measurement is used
to construct PROscore, a novel importance score for IPPRO, a novel
importance-based structured pruning with magnitude-indifference. Our evaluation
results shows that the proposed importance criteria using the projective space
achieves near-lossless pruning by reducing the performance drop in pruning,
with promising performance after the finetuning. Our work debunks the
``size-matters'' myth in pruning and expands the frontier of importance-based
pruning both theoretically and empirically.

</details>


### [158] [Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI](https://arxiv.org/abs/2507.14172)
*Julien Pourcel,Cédric Colas,Pierre-Yves Oudeyer*

Main category: cs.LG

TL;DR: SOAR learns program synthesis by integrating language models into a self-improving evolutionary loop, achieving significant performance gains on the ARC-AGI benchmark.


<details>
  <summary>Details</summary>
Motivation: Many program synthesis tasks prove too challenging for even state-of-the-art language models to solve in single attempts. Search-based evolutionary methods offer a promising alternative by exploring solution spaces iteratively, but their effectiveness remain limited by the fixed capabilities of the underlying generative model.

Method: SOAR alternates between (1) an evolutionary search that uses an LLM to sample and refine candidate solutions, and (2) a hindsight learning phase that converts search attempts into valid problem-solution pairs used to fine-tune the LLM's sampling and refinement capabilities.

Result: SOAR achieves significant performance gains across model scales and iterations, leveraging positive transfer between the sampling and refinement finetuning tasks. These improvements carry over to test-time adaptation, enabling SOAR to solve 52% of the public test set.

Conclusion: SOAR achieves significant performance gains across model scales and iterations, leveraging positive transfer between the sampling and refinement finetuning tasks. These improvements carry over to test-time adaptation, enabling SOAR to solve 52% of the public test set.

Abstract: Many program synthesis tasks prove too challenging for even state-of-the-art
language models to solve in single attempts. Search-based evolutionary methods
offer a promising alternative by exploring solution spaces iteratively, but
their effectiveness remain limited by the fixed capabilities of the underlying
generative model.
  We propose SOAR, a method that learns program synthesis by integrating
language models into a self-improving evolutionary loop.
  SOAR alternates between (1) an evolutionary search that uses an LLM to sample
and refine candidate solutions, and (2) a hindsight learning phase that
converts search attempts into valid problem-solution pairs used to fine-tune
the LLM's sampling and refinement capabilities\, -- \,enabling increasingly
effective search in subsequent iterations.
  On the challenging ARC-AGI benchmark, SOAR achieves significant performance
gains across model scales and iterations, leveraging positive transfer between
the sampling and refinement finetuning tasks. These improvements carry over to
test-time adaptation, enabling SOAR to solve 52\% of the public test set. Our
code is open-sourced at: https://github.com/flowersteam/SOAR

</details>


### [159] [Latent Space Data Fusion Outperforms Early Fusion in Multimodal Mental Health Digital Phenotyping Data](https://arxiv.org/abs/2507.14175)
*Youcef Barkat,Dylan Hamitouche,Deven Parekh,Ivy Guo,David Benrimoh*

Main category: cs.LG

TL;DR: Latent space fusion is better than traditional methods for predicting mental health using multimodal data.


<details>
  <summary>Details</summary>
Motivation: Mental illnesses such as depression and anxiety require improved methods for early detection and personalized intervention. Traditional predictive models often rely on unimodal data or early fusion strategies that fail to capture the complex, multimodal nature of psychiatric data. Advanced integration techniques, such as intermediate (latent space) fusion, may offer better accuracy and clinical utility.

Method: We evaluated intermediate (latent space) fusion for predicting daily depressive symptoms (PHQ-2 scores). We compared early fusion implemented with a Random Forest (RF) model and intermediate fusion implemented via a Combined Model (CM) using autoencoders and a neural network. The dataset included behavioral (smartphone-based), demographic, and clinical features. Experiments were conducted across multiple temporal splits and data stream combinations. Performance was evaluated using mean squared error (MSE) and coefficient of determination (R2).

Result: The CM outperformed both RF and Linear Regression (LR) baselines across all setups, achieving lower MSE (0.4985 vs. 0.5305 with RF) and higher R2 (0.4695 vs. 0.4356). The RF model showed signs of overfitting, with a large gap between training and test performance, while the CM maintained consistent generalization. Performance was best when integrating all data modalities in the CM (in contradistinction to RF), underscoring the value of latent space fusion for capturing non-linear interactions in complex psychiatric datasets.

Conclusion: Latent space fusion offers a robust alternative to traditional fusion methods for prediction with multimodal mental health data. Future work should explore model interpretability and individual-level prediction for clinical deployment.

Abstract: Background: Mental illnesses such as depression and anxiety require improved
methods for early detection and personalized intervention. Traditional
predictive models often rely on unimodal data or early fusion strategies that
fail to capture the complex, multimodal nature of psychiatric data. Advanced
integration techniques, such as intermediate (latent space) fusion, may offer
better accuracy and clinical utility. Methods: Using data from the BRIGHTEN
clinical trial, we evaluated intermediate (latent space) fusion for predicting
daily depressive symptoms (PHQ-2 scores). We compared early fusion implemented
with a Random Forest (RF) model and intermediate fusion implemented via a
Combined Model (CM) using autoencoders and a neural network. The dataset
included behavioral (smartphone-based), demographic, and clinical features.
Experiments were conducted across multiple temporal splits and data stream
combinations. Performance was evaluated using mean squared error (MSE) and
coefficient of determination (R2). Results: The CM outperformed both RF and
Linear Regression (LR) baselines across all setups, achieving lower MSE (0.4985
vs. 0.5305 with RF) and higher R2 (0.4695 vs. 0.4356). The RF model showed
signs of overfitting, with a large gap between training and test performance,
while the CM maintained consistent generalization. Performance was best when
integrating all data modalities in the CM (in contradistinction to RF),
underscoring the value of latent space fusion for capturing non-linear
interactions in complex psychiatric datasets. Conclusion: Latent space fusion
offers a robust alternative to traditional fusion methods for prediction with
multimodal mental health data. Future work should explore model
interpretability and individual-level prediction for clinical deployment.

</details>


### [160] [Predictive Representativity: Uncovering Racial Bias in AI-based Skin Cancer Detection](https://arxiv.org/abs/2507.14176)
*Andrés Morales-Forero,Lili J. Rueda,Ronald Herrera,Samuel Bassetto,Eric Coatanea*

Main category: cs.LG

TL;DR: 论文介绍了一种新的公平性审计框架，发现AI皮肤癌分类器在肤色较深的人群中表现不佳，即使数据集中有比例抽样。


<details>
  <summary>Details</summary>
Motivation: 人工智能 (AI) 系统越来越多地为医疗决策提供信息，但人们仍然担心算法偏差和不公平的结果，特别是对于历史上边缘化的人群。

Method: 提出了一种名为预测代表性 (PR) 的公平性审计框架，该框架将重点从数据集的组成转移到结果层面的公平性。

Result: 通过皮肤病学案例研究，我们评估了在广泛使用的 HAM10000 数据集和哥伦比亚的独立临床数据集（BOSQUE 测试集）上训练的基于 AI 的皮肤癌分类器。我们的分析显示，按皮肤光型划分，性能存在显着差异，尽管源数据中存在比例抽样，但分类器对肤色较深的人的表现始终不佳。

Conclusion: 需要对AI系统进行公平性审计，数据集文档需要透明化，模型验证管道需要具有包容性。这项工作提供了一个可扩展的工具，用于诊断AI系统中的结构性不平等，促进关于公平、可解释性和数据公正的讨论，并促进对数据驱动的医疗保健中的公平性的重新评估。

Abstract: Artificial intelligence (AI) systems increasingly inform medical
decision-making, yet concerns about algorithmic bias and inequitable outcomes
persist, particularly for historically marginalized populations. This paper
introduces the concept of Predictive Representativity (PR), a framework of
fairness auditing that shifts the focus from the composition of the data set to
outcomes-level equity. Through a case study in dermatology, we evaluated
AI-based skin cancer classifiers trained on the widely used HAM10000 dataset
and on an independent clinical dataset (BOSQUE Test set) from Colombia. Our
analysis reveals substantial performance disparities by skin phototype, with
classifiers consistently underperforming for individuals with darker skin,
despite proportional sampling in the source data. We argue that
representativity must be understood not as a static feature of datasets but as
a dynamic, context-sensitive property of model predictions. PR operationalizes
this shift by quantifying how reliably models generalize fairness across
subpopulations and deployment contexts. We further propose an External
Transportability Criterion that formalizes the thresholds for fairness
generalization. Our findings highlight the ethical imperative for post-hoc
fairness auditing, transparency in dataset documentation, and inclusive model
validation pipelines. This work offers a scalable tool for diagnosing
structural inequities in AI systems, contributing to discussions on equity,
interpretability, and data justice and fostering a critical re-evaluation of
fairness in data-driven healthcare.

</details>


### [161] [Understanding Two-Layer Neural Networks with Smooth Activation Functions](https://arxiv.org/abs/2507.14177)
*Changcun Huang*

Main category: cs.LG

TL;DR: 本文研究了两层神经网络的训练解决方案，证明了通用逼近，并通过实验验证揭示了解决方案空间的神秘性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在了解两层神经网络的训练解决方案，该网络的隐藏层由具有平滑激活功能的单元组成，包括ReLU出现之前最常用的通常的sigmoid类型。

Method: 泰勒级数展开的构建，结点的严格偏序，光滑样条实现和平滑连续性约束

Result: 证明了任意输入维度的通用逼近

Conclusion: 通过实验验证，很大程度上揭示了解决方案空间的“黑匣子”的神秘面纱。所采用的新证明也丰富了逼近理论。

Abstract: This paper aims to understand the training solution, which is obtained by the
back-propagation algorithm, of two-layer neural networks whose hidden layer is
composed of the units with smooth activation functions, including the usual
sigmoid type most commonly used before the advent of ReLUs. The mechanism
contains four main principles: construction of Taylor series expansions, strict
partial order of knots, smooth-spline implementation and smooth-continuity
restriction. The universal approximation for arbitrary input dimensionality is
proved and experimental verification is given, through which the mystery of
``black box'' of the solution space is largely revealed. The new proofs
employed also enrich approximation theory.

</details>


### [162] [Feature Bank Enhancement for Distance-based Out-of-Distribution Detection](https://arxiv.org/abs/2507.14178)
*Yuhang Liu,Yuefei Wu,Bin Shi,Bo Dong*

Main category: cs.LG

TL;DR: 提出了一种名为 FBE 的方法，用于解决基于距离的 OOD 检测方法中由于极端特征导致的 ID 样本评分过低的问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习通常导致数据特征的偏差分布，并且不可避免地会出现极端特征。这些极端特征使得基于距离的方法倾向于为 ID 样本分配过低的评分，从而限制了此类方法的 OOD 检测能力。

Method: 提出了一种简单而有效的方法，即特征库增强 (FBE)，该方法使用数据集的统计特征来识别和约束极端特征到分离边界。

Result: 该方法在 ImageNet-1k 和 CIFAR-10 上的实验结果表明，该方法在两个基准测试中均实现了最先进的性能。

Conclusion: 该方法在 ImageNet-1k 和 CIFAR-10 上实现了最先进的性能，并通过理论分析和补充实验提供了更多关于该方法的见解。

Abstract: Out-of-distribution (OOD) detection is critical to ensuring the reliability
of deep learning applications and has attracted significant attention in recent
years. A rich body of literature has emerged to develop efficient score
functions that assign high scores to in-distribution (ID) samples and low
scores to OOD samples, thereby helping distinguish OOD samples. Among these
methods, distance-based score functions are widely used because of their
efficiency and ease of use. However, deep learning often leads to a biased
distribution of data features, and extreme features are inevitable. These
extreme features make the distance-based methods tend to assign too low scores
to ID samples. This limits the OOD detection capabilities of such methods. To
address this issue, we propose a simple yet effective method, Feature Bank
Enhancement (FBE), that uses statistical characteristics from dataset to
identify and constrain extreme features to the separation boundaries, therapy
making the distance between samples inside and outside the distribution
farther. We conducted experiments on large-scale ImageNet-1k and CIFAR-10
respectively, and the results show that our method achieves state-of-the-art
performance on both benchmark. Additionally, theoretical analysis and
supplementary experiments are conducted to provide more insights into our
method.

</details>


### [163] [Beyond Model Base Selection: Weaving Knowledge to Master Fine-grained Neural Network Design](https://arxiv.org/abs/2507.15336)
*Jialiang Wang,Hanmo Liu,Shimin Di,Zhili Wang,Jiachuan Wang,Lei Chen,Xiaofang Zhou*

Main category: cs.LG

TL;DR: M-DESIGN 通过利用图关系知识模式，快速匹配和迭代地改进候选模型，从而改进神经网络。


<details>
  <summary>Details</summary>
Motivation: 静态模型选择实践忽略了不同任务查询和模型架构变体之间细粒度的、不断发展的关系依赖性，导致次优匹配，并且未能进一步有效地改进模型。

Method: M-DESIGN，一个用于掌握神经网络改进的精选模型知识库 (MKB) 管道，通过自适应地编织关于模型架构修改的先前见解。

Result: M-DESIGN 交付了 67,760 个图模型的最佳模型。

Conclusion: M-DESIGN 在有限的预算内，在 33 个数据-任务对中的 26 个中提供了最佳模型。

Abstract: Database systems have recently advocated for embedding machine learning (ML)
capabilities, offering declarative model queries over large, managed model
repositories, thereby circumventing the huge computational overhead of
traditional ML-based algorithms in automated neural network model selection.
Pioneering database studies aim to organize existing benchmark repositories as
model bases (MB), querying them for the model records with the highest
performance estimation metrics for given tasks. However, this static model
selection practice overlooks the fine-grained, evolving relational dependencies
between diverse task queries and model architecture variations, resulting in
suboptimal matches and failing to further refine the model effectively. To fill
the model refinement gap in database research, we propose M-DESIGN, a curated
model knowledge base (MKB) pipeline for mastering neural network refinement by
adaptively weaving prior insights about model architecture modification. First,
we propose a knowledge weaving engine that reframes model refinement as an
adaptive query problem over task metadata. Given a user's task query, M-DESIGN
quickly matches and iteratively refines candidate models by leveraging a
graph-relational knowledge schema that explicitly encodes data properties,
architecture variations, and pairwise performance deltas as joinable relations.
This schema supports fine-grained relational analytics over architecture tweaks
and drives a predictive query planner that can detect and adapt to
out-of-distribution (OOD) tasks. We instantiate M-DESIGN for graph analytics
tasks, where our model knowledge base enriches existing benchmarks with
structured metadata covering 3 graph tasks and 22 graph datasets, contributing
data records of 67,760 graph models. Empirical results demonstrate that
M-DESIGN delivers the optimal model in 26 of 33 data-task pairs within limited
budgets.

</details>


### [164] [A Sparsity Predicting Approach for Large Language Models via Activation Pattern Clustering](https://arxiv.org/abs/2507.14179)
*Nobel Dhar,Bobin Deng,Md Romyull Islam,Xinyue Zhang,Kazi Fahim Ahmad Nasif,Kun Suo*

Main category: cs.LG

TL;DR: 提出了一种基于聚类的激活模式压缩框架，用于有效预测和利用 LLM 中的激活稀疏性，实现了高聚类精度和低困惑度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 表现出显着的激活稀疏性，其中只有一部分神经元对给定的输入处于活动状态。虽然这种稀疏性提供了降低计算成本的机会，但有效利用它需要在可扩展的方式中预测激活模式。然而，由于现代 LLM 中神经元数量庞大，神经元级别的直接预测计算成本很高。

Method: 提出了一种基于聚类的激活模式压缩框架，该框架将相似的激活模式分组到一小组代表性聚类中，而不是独立地处理每个神经元。

Result: 该方法实现了高达 79.34% 的聚类精度，优于标准二元聚类方法，同时保持了最小的困惑度 (PPL) 分数下降。通过足够大数量的集群，该方法获得了低至 12.49 的 PPL 分数，证明了其在保持模型质量的同时降低计算开销方面的有效性。

Conclusion: 通过预测聚类分配而不是单独的神经元状态，未来的模型可以有效地从预先计算的质心中推断激活模式。这种基于聚类的公式为未来激活模式预测的研究奠定了基础，为大规模语言模型中的高效推理铺平了道路。

Abstract: Large Language Models (LLMs) exhibit significant activation sparsity, where
only a subset of neurons are active for a given input. Although this sparsity
presents opportunities to reduce computational cost, efficiently utilizing it
requires predicting activation patterns in a scalable manner. However, direct
prediction at the neuron level is computationally expensive due to the vast
number of neurons in modern LLMs. To enable efficient prediction and
utilization of activation sparsity, we propose a clustering-based activation
pattern compression framework. Instead of treating each neuron independently,
we group similar activation patterns into a small set of representative
clusters. Our method achieves up to 79.34% clustering precision, outperforming
standard binary clustering approaches while maintaining minimal degradation in
perplexity (PPL) scores. With a sufficiently large number of clusters, our
approach attains a PPL score as low as 12.49, demonstrating its effectiveness
in preserving model quality while reducing computational overhead. By
predicting cluster assignments rather than individual neuron states, future
models can efficiently infer activation patterns from pre-computed centroids.
We detail the clustering algorithm, analyze its effectiveness in capturing
meaningful activation structures, and demonstrate its potential to improve
sparse computation efficiency. This clustering-based formulation serves as a
foundation for future work on activation pattern prediction, paving the way for
efficient inference in large-scale language models.

</details>


### [165] [Digital Twin-Assisted Explainable AI for Robust Beam Prediction in mmWave MIMO Systems](https://arxiv.org/abs/2507.14180)
*Nasir Khan,Asmaa Abdallah,Abdulkadir Celik,Ahmed M. Eltawil,Sinem Coleri*

Main category: cs.LG

TL;DR: This paper proposes a robust and explainable DL-based beam alignment engine (BAE) for mmWave MIMO systems, using a site-specific digital twin (DT) and transfer learning to reduce data needs and beam training overhead, while enhancing transparency and outlier detection.


<details>
  <summary>Details</summary>
Motivation: Explainability and robustness are crucial for building trust and ensuring reliable performance in millimeter-wave (mmWave) systems. Efficient beam alignment is essential for initial access, but deep learning (DL) solutions face challenges, including high data collection overhead, hardware constraints, lack of explainability, and susceptibility to adversarial attacks.

Method: The BAE uses received signal strength indicator (RSSI) measurements from wide beams to predict the best narrow beam, reducing the overhead of exhaustive beam sweeping. A model refinement via transfer learning is proposed to fine-tune the pre-trained model residing in the DT with minimal real-world data. The framework uses deep Shapley additive explanations (SHAP) to rank input features by importance, prioritizing key spatial directions and minimizing beam sweeping. It also incorporates the Deep k-nearest neighbors (DkNN) algorithm, providing a credibility metric for detecting out-of-distribution inputs.

Result: The proposed framework reduces real-world data needs by 70%, beam training overhead by 62%, and improves outlier detection robustness by up to 8.5x.

Conclusion: The proposed framework reduces real-world data needs by 70%, beam training overhead by 62%, and improves outlier detection robustness by up to 8.5x, achieving near-optimal spectral efficiency and transparent decision making compared to traditional softmax based DL models.

Abstract: In line with the AI-native 6G vision, explainability and robustness are
crucial for building trust and ensuring reliable performance in millimeter-wave
(mmWave) systems. Efficient beam alignment is essential for initial access, but
deep learning (DL) solutions face challenges, including high data collection
overhead, hardware constraints, lack of explainability, and susceptibility to
adversarial attacks. This paper proposes a robust and explainable DL-based beam
alignment engine (BAE) for mmWave multiple-input multiple output (MIMO)
systems. The BAE uses received signal strength indicator (RSSI) measurements
from wide beams to predict the best narrow beam, reducing the overhead of
exhaustive beam sweeping. To overcome the challenge of real-world data
collection, this work leverages a site-specific digital twin (DT) to generate
synthetic channel data closely resembling real-world environments. A model
refinement via transfer learning is proposed to fine-tune the pre-trained model
residing in the DT with minimal real-world data, effectively bridging
mismatches between the digital replica and real-world environments. To reduce
beam training overhead and enhance transparency, the framework uses deep
Shapley additive explanations (SHAP) to rank input features by importance,
prioritizing key spatial directions and minimizing beam sweeping. It also
incorporates the Deep k-nearest neighbors (DkNN) algorithm, providing a
credibility metric for detecting out-of-distribution inputs and ensuring
robust, transparent decision-making. Experimental results show that the
proposed framework reduces real-world data needs by 70%, beam training overhead
by 62%, and improves outlier detection robustness by up to 8.5x, achieving
near-optimal spectral efficiency and transparent decision making compared to
traditional softmax based DL models.

</details>


### [166] [Semi-Supervised Federated Learning via Dual Contrastive Learning and Soft Labeling for Intelligent Fault Diagnosis](https://arxiv.org/abs/2507.14181)
*Yajiao Dai,Jun Li,Zhen Mei,Yiyang Ni,Shi Jin,Zengxiang Li,Sheng Guo,Wei Xiang*

Main category: cs.LG

TL;DR: 提出了一种半监督联邦学习框架，通过双重对比损失和软标签来解决智能故障诊断中数据和标签稀缺的问题，实验表明，该框架在准确率方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的监督深度学习方法需要大量的训练数据和标签，这些数据和标签通常位于不同的客户端。此外，数据标记的成本很高，使得标签难以获取。同时，客户端之间数据分布的差异也可能阻碍模型的性能。

Method: 提出了一种半监督联邦学习框架 SSFL-DCSL，该框架集成了双重对比损失和软标签，以解决数据和标签稀缺的问题，同时保护用户隐私。它支持使用客户端上的未标记数据进行表示学习，并通过原型促进客户端之间的联合学习，从而实现相互知识共享并防止本地模型发散。

Result: 在两个公开可用的数据集和一个从工厂电机收集的数据集上进行了实验。在最具挑战性的任务中，只有 10% 的数据被标记，所提出的 SSFL-DCSL 可以比最先进的方法提高 1.15% 到 7.85% 的准确率。

Conclusion: 在最具挑战性的任务中，只有 10% 的数据被标记，所提出的 SSFL-DCSL 可以比最先进的方法提高 1.15% 到 7.85% 的准确率。

Abstract: Intelligent fault diagnosis (IFD) plays a crucial role in ensuring the safe
operation of industrial machinery and improving production efficiency. However,
traditional supervised deep learning methods require a large amount of training
data and labels, which are often located in different clients. Additionally,
the cost of data labeling is high, making labels difficult to acquire.
Meanwhile, differences in data distribution among clients may also hinder the
model's performance. To tackle these challenges, this paper proposes a
semi-supervised federated learning framework, SSFL-DCSL, which integrates dual
contrastive loss and soft labeling to address data and label scarcity for
distributed clients with few labeled samples while safeguarding user privacy.
It enables representation learning using unlabeled data on the client side and
facilitates joint learning among clients through prototypes, thereby achieving
mutual knowledge sharing and preventing local model divergence. Specifically,
first, a sample weighting function based on the Laplace distribution is
designed to alleviate bias caused by low confidence in pseudo labels during the
semi-supervised training process. Second, a dual contrastive loss is introduced
to mitigate model divergence caused by different data distributions, comprising
local contrastive loss and global contrastive loss. Third, local prototypes are
aggregated on the server with weighted averaging and updated with momentum to
share knowledge among clients. To evaluate the proposed SSFL-DCSL framework,
experiments are conducted on two publicly available datasets and a dataset
collected on motors from the factory. In the most challenging task, where only
10\% of the data are labeled, the proposed SSFL-DCSL can improve accuracy by
1.15% to 7.85% over state-of-the-art methods.

</details>


### [167] [From Bias to Behavior: Learning Bull-Bear Market Dynamics with Contrastive Modeling](https://arxiv.org/abs/2507.14182)
*Xiaotong Luo,Shengda Zhuo,Min Chen,Lichun Li,Ruizhao Lu,Wenqi Fan,Shuqiang Huang,Yin Tang*

Main category: cs.LG

TL;DR: 本文提出了一个名为 B4 的新模型，用于预测市场趋势。该模型考虑了投资者偏差和市场动态，并在实际金融数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 金融市场表现出高度动态和复杂的行为，这些行为受到历史价格轨迹和外生叙事（如新闻、政策解读和社交媒体情绪）的影响。这些数据中的异质性和投资者观点的多样性引入了偏差，使市场动态的建模变得复杂。

Method: 提出了一个偏差到行为的牛熊动态模型 (B4)，该模型将时间价格序列和外部上下文信号共同嵌入到一个共享的潜在空间中，在这个空间中，相对的牛市和熊市力量自然出现，形成了偏差表示的基础。

Result: 该模型在预测市场趋势方面取得了优异的性能，并为偏差、投资者行为和市场动态之间的相互作用提供了可解释的见解。

Conclusion: 该模型在预测市场趋势方面表现出色，并能深入了解偏差、投资者行为和市场动态之间的相互作用。

Abstract: Financial markets exhibit highly dynamic and complex behaviors shaped by both
historical price trajectories and exogenous narratives, such as news, policy
interpretations, and social media sentiment. The heterogeneity in these data
and the diverse insight of investors introduce biases that complicate the
modeling of market dynamics. Unlike prior work, this paper explores the
potential of bull and bear regimes in investor-driven market dynamics. Through
empirical analysis on real-world financial datasets, we uncover a dynamic
relationship between bias variation and behavioral adaptation, which enhances
trend prediction under evolving market conditions. To model this mechanism, we
propose the Bias to Behavior from Bull-Bear Dynamics model (B4), a unified
framework that jointly embeds temporal price sequences and external contextual
signals into a shared latent space where opposing bull and bear forces
naturally emerge, forming the foundation for bias representation. Within this
space, an inertial pairing module pairs temporally adjacent samples to preserve
momentum, while the dual competition mechanism contrasts bullish and bearish
embeddings to capture behavioral divergence. Together, these components allow
B4 to model bias-driven asymmetry, behavioral inertia, and market
heterogeneity. Experimental results on real-world financial datasets
demonstrate that our model not only achieves superior performance in predicting
market trends but also provides interpretable insights into the interplay of
biases, investor behaviors, and market dynamics.

</details>


### [168] [LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models](https://arxiv.org/abs/2507.14204)
*Dachuan Shi,Yonggan Fu,Xiangchi Yuan,Zhongzhi Yu,Haoran You,Sixu Li,Xin Dong,Jan Kautz,Pavlo Molchanov,Yingyan,Lin*

Main category: cs.LG

TL;DR: LaCache是一种新的KV缓存优化范例，它通过梯形KV缓存模式和迭代压缩机制，在增强LLM的远程能力方面是有效的。


<details>
  <summary>Details</summary>
Motivation: 序列长度的增加，LLM中的Key-Value (KV)对的数量也在增加，造成了显著的效率瓶颈。需要强大的远程能力，这对于处理广泛的输入上下文和不断生成扩展的输出至关重要。

Method: 一种名为LaCache的新的KV缓存优化范例，这是一种用于LLM的高效和准确的生成推理的免训练方法。LaCache集成了两个关键创新：(1) 梯形KV缓存模式，不仅按顺序（在每一层中从左到右）存储KV对，而且跨层（从浅到深）存储KV对，从而为在固定存储预算下捕获远程依赖关系提供了扩展范围，从而提高了远程能力；(2) 一种迭代压缩机制，可逐步压缩旧缓存，从而在固定缓存大小内为新令牌释放空间。

Result: 在各种任务、基准和LLM模型上的实验一致地验证了LaCache在增强LLM的远程能力方面的有效性。

Conclusion: LaCache在增强LLM的远程能力方面是有效的。

Abstract: Recent advancements in Large Language Models (LLMs) have spurred interest in
numerous applications requiring robust long-range capabilities, essential for
processing extensive input contexts and continuously generating extended
outputs. As sequence lengths increase, the number of Key-Value (KV) pairs in
LLMs escalates, creating a significant efficiency bottleneck. In this paper, we
propose a new KV cache optimization paradigm called LaCache, a training-free
method for efficient and accurate generative inference of LLMs. LaCache enables
LLMs to simultaneously address both of the critical challenges in long-range
modeling: robust long-range capabilities and continuous generation without
running out-of-memory (OOM). Specifically, LaCache integrates two key
innovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only
sequentially (left-to-right within each layer) but also across layers (from
shallow to deep), providing an extended span for capturing long-range
dependencies under a fixed storage budget, thereby boosting long-range
capabilities; and (2) an iterative compaction mechanism that progressively
compresses older caches, freeing up space for new tokens within a fixed cache
size. This token distance-based dynamic compression enables more effective
continuous generation under constrained cache budgets. Experiments across
various tasks, benchmarks, and LLM models consistently validate LaCache's
effectiveness in enhancing LLMs' long-range capabilities. Our code is available
at https://github.com/GATECH-EIC/LaCache.

</details>


### [169] [Developing an AI-Guided Assistant Device for the Deaf and Hearing Impaired](https://arxiv.org/abs/2507.14215)
*Jiayu,Liu*

Main category: cs.LG

TL;DR: Developed a deep learning system for an accessibility device for the deaf or hearing impaired, with high accuracy in sound direction, audio classification, and audio-visual localization.


<details>
  <summary>Details</summary>
Motivation: This study aims to develop a deep learning system for an accessibility device for the deaf or hearing impaired and fill an important gap in current research by leveraging machine learning techniques to target the underprivileged community.

Method: The system includes three main components: JerryNet, Audio Classification, and Multimodal integration model.

Result: JerryNet achieved a precision of 91. 1% for the sound direction. The CLAP model achieved 98.5% and 95% accuracy on custom and AudioSet datasets, respectively. The audio-visual localization model within component 3 yielded a cIoU of 0.892 and an AUC of 0.658.

Conclusion: This study has the potential to create a new generation of accessibility devices.

Abstract: This study aims to develop a deep learning system for an accessibility device
for the deaf or hearing impaired. The device will accurately localize and
identify sound sources in real time. This study will fill an important gap in
current research by leveraging machine learning techniques to target the
underprivileged community. The system includes three main components. 1.
JerryNet: A custom designed CNN architecture that determines the direction of
arrival (DoA) for nine possible directions. 2. Audio Classification: This model
is based on fine-tuning the Contrastive Language-Audio Pretraining (CLAP) model
to identify the exact sound classes only based on audio. 3. Multimodal
integration model: This is an accurate sound localization model that combines
audio, visual, and text data to locate the exact sound sources in the images.
The part consists of two modules, one object detection using Yolov9 to generate
all the bounding boxes of the objects, and an audio visual localization model
to identify the optimal bounding box using complete Intersection over Union
(CIoU). The hardware consists of a four-microphone rectangular formation and a
camera mounted on glasses with a wristband for displaying necessary information
like direction. On a custom collected data set, JerryNet achieved a precision
of 91. 1% for the sound direction, outperforming all the baseline models. The
CLAP model achieved 98.5% and 95% accuracy on custom and AudioSet datasets,
respectively. The audio-visual localization model within component 3 yielded a
cIoU of 0.892 and an AUC of 0.658, surpassing other similar models. There are
many future potentials to this study, paving the way to creating a new
generation of accessibility devices.

</details>


### [170] [Geometry-Aware Active Learning of Pattern Rankings via Choquet-Based Aggregation](https://arxiv.org/abs/2507.14217)
*Tudor Matei Opran,Samir Loudni*

Main category: cs.LG

TL;DR: interactive learning framework


<details>
  <summary>Details</summary>
Motivation: address the pattern explosion problem in pattern mining

Method: nonlinear utility aggregation with geometry-aware query selection

Result: Experiments on UCI datasets

Conclusion: The proposed approach outperforms existing methods such as ChoquetRank, achieving better ranking accuracy with fewer user interactions.

Abstract: We address the pattern explosion problem in pattern mining by proposing an
interactive learning framework that combines nonlinear utility aggregation with
geometry-aware query selection. Our method models user preferences through a
Choquet integral over multiple interestingness measures and exploits the
geometric structure of the version space to guide the selection of informative
comparisons. A branch-and-bound strategy with tight distance bounds enables
efficient identification of queries near the decision boundary. Experiments on
UCI datasets show that our approach outperforms existing methods such as
ChoquetRank, achieving better ranking accuracy with fewer user interactions.

</details>


### [171] [Artificial Intelligence for Green Hydrogen Yield Prediction and Site Suitability using SHAP-Based Composite Index: Focus on Oman](https://arxiv.org/abs/2507.14219)
*Obumneme Zimuzor Nwafor,Mohammed Abdul Majeed Al Hooti*

Main category: cs.LG

TL;DR: 该研究利用人工智能框架，通过分析气象、地形和时间数据，为绿色氢气生产选址提供客观、可复制的解决方案，尤其适用于数据稀缺地区。


<details>
  <summary>Details</summary>
Motivation: 确定氢气生产的最佳位置需要整合复杂的环境、大气和基础设施因素，并且通常受到直接氢气产量数据有限性的影响。

Method: 该研究提出了一个新颖的人工智能框架，用于计算绿色氢产量和场地适宜性指数，使用平均绝对SHAP值。该框架包括一个多阶段管道，包含无监督多变量聚类、监督机器学习分类器和SHAP算法。

Result: 该研究结果揭示了适宜性的明显空间模式以及变量的相对影响。模型预测准确率达到98%，结果还表明，水 proximity、海拔和季节变化是决定阿曼绿色氢气场地适宜性的最重要因素，其平均绝对SHAP值分别为2.470891、2.376296和1.273216。

Conclusion: 该研究为绿色氢基础设施规划和决策提供了一个可复制和可扩展的工具，尤其是在数据稀缺地区。

Abstract: As nations seek sustainable alternatives to fossil fuels, green hydrogen has
emerged as a promising strategic pathway toward decarbonisation, particularly
in solar-rich arid regions. However, identifying optimal locations for hydrogen
production requires the integration of complex environmental, atmospheric, and
infrastructural factors, often compounded by limited availability of direct
hydrogen yield data. This study presents a novel Artificial Intelligence (AI)
framework for computing green hydrogen yield and site suitability index using
mean absolute SHAP (SHapley Additive exPlanations) values. This framework
consists of a multi-stage pipeline of unsupervised multi-variable clustering,
supervised machine learning classifier and SHAP algorithm. The pipeline trains
on an integrated meteorological, topographic and temporal dataset and the
results revealed distinct spatial patterns of suitability and relative
influence of the variables. With model predictive accuracy of 98%, the result
also showed that water proximity, elevation and seasonal variation are the most
influential factors determining green hydrogen site suitability in Oman with
mean absolute shap values of 2.470891, 2.376296 and 1.273216 respectively.
Given limited or absence of ground-truth yield data in many countries that have
green hydrogen prospects and ambitions, this study offers an objective and
reproducible alternative to subjective expert weightings, thus allowing the
data to speak for itself and potentially discover novel latent groupings
without pre-imposed assumptions. This study offers industry stakeholders and
policymakers a replicable and scalable tool for green hydrogen infrastructure
planning and other decision making in data-scarce regions.

</details>


### [172] [Domain Generalization via Pareto Optimal Gradient Matching](https://arxiv.org/abs/2507.14227)
*Khoi Do,Duong Nguyen,Nam-Khanh Le,Quoc-Viet Pham,Binh-Son Hua,Won-Joo Hwang*

Main category: cs.LG

TL;DR: 提出了一种新的帕累托最优梯度匹配（POGM）方法，以解决基于梯度的领域泛化问题中的梯度波动和高计算开销问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于梯度的领域泛化方法存在两个主要挑战：梯度经验距离或梯度内积（GIP）的最小化导致领域间的梯度波动，从而阻碍了直接学习；直接将梯度学习应用于联合损失函数会导致由于二阶导数近似带来的高计算开销。

Method: 提出了一种新的帕累托最优梯度匹配（POGM）方法，利用梯度轨迹作为收集的数据，并在元学习器上应用独立训练。在元更新中，最大化GIP，同时限制学习的梯度偏离经验风险最小化梯度轨迹。

Result: POGM在DomainBed数据集上取得了有竞争力的结果，同时实现了计算效率。

Conclusion: POGM在DomainBed数据集上取得了有竞争力的结果，同时实现了计算效率。

Abstract: In this study, we address the gradient-based domain generalization problem,
where predictors aim for consistent gradient directions across different
domains. Existing methods have two main challenges. First, minimization of
gradient empirical distance or gradient inner products (GIP) leads to gradient
fluctuations among domains, thereby hindering straightforward learning. Second,
the direct application of gradient learning to the joint loss function can
incur high computation overheads due to second-order derivative approximation.
To tackle these challenges, we propose a new Pareto Optimality Gradient
Matching (POGM) method. In contrast to existing methods that add gradient
matching as regularization, we leverage gradient trajectories as collected data
and apply independent training at the meta-learner. In the meta-update, we
maximize GIP while limiting the learned gradient from deviating too far from
the empirical risk minimization gradient trajectory. By doing so, the aggregate
gradient can incorporate knowledge from all domains without suffering gradient
fluctuation towards any particular domain. Experimental evaluations on datasets
from DomainBed demonstrate competitive results yielded by POGM against other
baselines while achieving computational efficiency.

</details>


### [173] [A million-scale dataset and generalizable foundation model for nanomaterial-protein interactions](https://arxiv.org/abs/2507.14245)
*Hengjie Yu,Kenneth A. Dawson,Haiyun Yang,Shuya Liu,Yan Yan,Yaochu Jin*

Main category: cs.LG

TL;DR: 提出了一个大型纳米材料-蛋白质相互作用数据集和一个名为NanoProFormer的基础模型，用于预测纳米材料-蛋白质的亲和力，并在各种下游任务中展示了其适用性。


<details>
  <summary>Details</summary>
Motivation: 了解纳米材料与蛋白质的相互作用至关重要，这是一个复杂的决策空间，人工智能有望在此产生变革性影响。然而，由于有限的数据集和现有模型的受限的泛化性，进展受到了阻碍。

Method: 提出了NanoPro-3M，这是迄今为止最大的纳米材料-蛋白质相互作用数据集，包含超过320万个样本和37000个独特的蛋白质。利用这一点，我们提出了NanoProFormer，一个基础模型，通过多模态表征学习预测纳米材料-蛋白质的亲和力。

Result: 多模态建模显著优于单模态方法，并确定了冠状形成的关键决定因素。此外，我们通过零样本推理和微调证明了其对一系列下游任务的适用性。

Conclusion: 该研究为高性能和广义的纳米材料-蛋白质相互作用终点预测奠定了坚实的基础，减少了对实验的依赖，并加速了各种体外应用。

Abstract: Unlocking the potential of nanomaterials in medicine and environmental
science hinges on understanding their interactions with proteins, a complex
decision space where AI is poised to make a transformative impact. However,
progress has been hindered by limited datasets and the restricted
generalizability of existing models. Here, we propose NanoPro-3M, the largest
nanomaterial-protein interaction dataset to date, comprising over 3.2 million
samples and 37,000 unique proteins. Leveraging this, we present NanoProFormer,
a foundational model that predicts nanomaterial-protein affinities through
multimodal representation learning, demonstrating strong generalization,
handling missing features, and unseen nanomaterials or proteins. We show that
multimodal modeling significantly outperforms single-modality approaches and
identifies key determinants of corona formation. Furthermore, we demonstrate
its applicability to a range of downstream tasks through zero-shot inference
and fine-tuning. Together, this work establishes a solid foundation for
high-performance and generalized prediction of nanomaterial-protein interaction
endpoints, reducing experimental reliance and accelerating various in vitro
applications.

</details>


### [174] [Linearized Diffusion Map](https://arxiv.org/abs/2507.14257)
*Julio Candanedo*

Main category: cs.LG

TL;DR: LDM是一种新的线性降维方法，在具有显式流形结构的数据集中优于PCA，并且允许直接应用非负矩阵分解(NMF)。


<details>
  <summary>Details</summary>
Motivation: 将基于扩散的非线性方法的几何直觉与PCA和经典MDS等线性嵌入的计算简单性、效率和可解释性相结合。

Method: 通过扩散图核的线性近似构建线性降维方法LDM。

Result: 在具有显式流形结构的数据集中，LDM嵌入优于PCA，尤其是在高维情况下；PCA在方差或噪声占主导地位的情况下仍然是首选。LDM的核矩阵的完全正性允许直接应用非负矩阵分解(NMF)。

Conclusion: LDM是一种新的线性降维技术，具有良好的理论和实际扩展前景。

Abstract: We introduce the Linearized Diffusion Map (LDM), a novel linear
dimensionality reduction method constructed via a linear approximation of the
diffusion-map kernel. LDM integrates the geometric intuition of diffusion-based
nonlinear methods with the computational simplicity, efficiency, and
interpretability inherent in linear embeddings such as PCA and classical MDS.
Through comprehensive experiments on synthetic datasets (Swiss roll and
hyperspheres) and real-world benchmarks (MNIST and COIL-20), we illustrate that
LDM captures distinct geometric features of datasets compared to PCA, offering
complementary advantages. Specifically, LDM embeddings outperform PCA in
datasets exhibiting explicit manifold structures, particularly in
high-dimensional regimes, whereas PCA remains preferable in scenarios dominated
by variance or noise. Furthermore, the complete positivity of LDM's kernel
matrix allows direct applicability of Non-negative Matrix Factorization (NMF),
suggesting opportunities for interpretable latent-structure discovery. Our
analysis positions LDM as a valuable new linear dimensionality reduction
technique with promising theoretical and practical extensions.

</details>


### [175] [A Simple "Try Again" Can Elicit Multi-Turn LLM Reasoning](https://arxiv.org/abs/2507.14295)
*Licheng Liu,Zihan Wang,Linjie Li,Chenwei Xu,Yiping Lu,Han Liu,Avirup Sil,Manling Li*

Main category: cs.LG

TL;DR: training models with multi-turn RL using only unary feedback after wrong answers can improve both single-turn performance and multi-turn reasoning


<details>
  <summary>Details</summary>
Motivation: Existing Reinforcement Learning (RL) methods train large reasoning models on a single-turn paradigm with verifiable rewards, but models trained with existing RL paradigms often lose their ability to solve problems across multiple turns and struggle to revise answers based on contextual feedback, leading to repetitive responses.

Method: multi-turn RL using only unary feedback (e.g., "Let's try again") after wrong answers

Result: improve both single-turn performance and multi-turn reasoning

Conclusion: RL training with UFO keeps single-turn performance and improves multi-turn reasoning accuracy by up to 14%, enabling language models to better react to feedback in multi-turn problem solving.

Abstract: Multi-turn problem solving is critical yet challenging for Large Reasoning
Models (LRMs) to reflect on their reasoning and revise from feedback. Existing
Reinforcement Learning (RL) methods train large reasoning models on a
single-turn paradigm with verifiable rewards. However, we observe that models
trained with existing RL paradigms often lose their ability to solve problems
across multiple turns and struggle to revise answers based on contextual
feedback, leading to repetitive responses. We ask: can LRMs learn to reflect
their answers in a multi-turn context? In this work, we find that training
models with multi-turn RL using only unary feedback (e.g., "Let's try again")
after wrong answers can improve both single-turn performance and multi-turn
reasoning. We introduce Unary Feedback as Observation (UFO) for reinforcement
learning, which uses minimal yet common unary user feedback during iterative
problem solving. It can be easily applied to existing single-turn RL training
setups. Experimental results show that RL training with UFO keeps single-turn
performance and improves multi-turn reasoning accuracy by up to 14%, enabling
language models to better react to feedback in multi-turn problem solving. To
further minimize the number of turns needed for a correct answer while
encouraging diverse reasoning when mistakes occur, we design reward structures
that guide models to produce careful and deliberate answers in each turn. Code:
https://github.com/lichengliu03/unary-feedback

</details>


### [176] [FedStrategist: A Meta-Learning Framework for Adaptive and Robust Aggregation in Federated Learning](https://arxiv.org/abs/2507.14322)
*Md Rafid Haque,Abu Raihan Mostofa Kamal,Md. Azam Hossain*

Main category: cs.LG

TL;DR: 该论文提出了FedStrategist，一个基于元学习的联邦学习防御框架，可以动态选择最佳聚合规则，提高模型在不同场景下的鲁棒性和安全性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习容易受到模型中毒攻击，现有的静态防御方法在不同的环境和自适应攻击者面前效果不佳。

Method: 设计了一个轻量级的上下文bandit agent，该agent基于实时诊断指标动态地选择最佳的聚合规则。

Result: 实验表明，没有一种静态规则是普遍最优的。FedStrategist 能够学习到在各种场景下都表现出色的策略，即使在“Krum-favorable”环境和面对复杂的“隐身”攻击者时也是如此。此外，该agent的策略可以通过单个“风险容忍度”参数来控制，从而允许从业者明确地管理性能和安全性之间的权衡。

Conclusion: 该论文提出了一种新的元学习框架FedStrategist，用于解决联邦学习中模型中毒攻击的问题。该框架通过一个轻量级的上下文bandit agent动态地从一系列防御措施中选择最佳的聚合规则，从而在不同的场景下学习到更优的策略，并在性能和安全性之间进行权衡。

Abstract: Federated Learning (FL) offers a paradigm for privacy-preserving
collaborative AI, but its decentralized nature creates significant
vulnerabilities to model poisoning attacks. While numerous static defenses
exist, their effectiveness is highly context-dependent, often failing against
adaptive adversaries or in heterogeneous data environments. This paper
introduces FedStrategist, a novel meta-learning framework that reframes robust
aggregation as a real-time, cost-aware control problem. We design a lightweight
contextual bandit agent that dynamically selects the optimal aggregation rule
from an arsenal of defenses based on real-time diagnostic metrics. Through
comprehensive experiments, we demonstrate that no single static rule is
universally optimal. We show that our adaptive agent successfully learns
superior policies across diverse scenarios, including a ``Krum-favorable"
environment and against a sophisticated "stealth" adversary designed to
neutralize specific diagnostic signals. Critically, we analyze the paradoxical
scenario where a non-robust baseline achieves high but compromised accuracy,
and demonstrate that our agent learns a conservative policy to prioritize model
integrity. Furthermore, we prove the agent's policy is controllable via a
single "risk tolerance" parameter, allowing practitioners to explicitly manage
the trade-off between performance and security. Our work provides a new,
practical, and analyzable approach to creating resilient and intelligent
decentralized AI systems.

</details>


### [177] [Rethinking Individual Fairness in Deepfake Detection](https://arxiv.org/abs/2507.14326)
*Aryana Hou,Li Lin,Justin Li,Shu Hu*

Main category: cs.LG

TL;DR: 该论文提出了一种新的deepfake检测框架，该框架提高了个人公平性并且优于当前最佳方法。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型大大提高了合成媒体的真实性，但通过复杂的DeepFakes滥用它们构成了重大风险。尽管最近在deepfake检测方面取得了进展，但公平性仍然没有得到充分解决，使得deepfake标记能够利用针对特定人群的偏见。以前的研究强调了群体层面的公平性，但个体公平性（即确保对相似个体进行相似的预测）在很大程度上仍未得到探索。这项工作首次发现个体公平性的原始原则在deepfake检测的背景下根本失败，揭示了文献中先前未曾探索的关键差距。

Method: 提出了一种可集成到现有deepfake检测器中的通用框架，以提高个体公平性。

Result: 在领先的deepfake数据集上进行的大量实验表明，该方法在保持鲁棒检测性能的同时，显著提高了个体公平性，优于现有方法。

Conclusion: 该论文提出了一种可集成到现有deepfake检测器中的通用框架，以提高个体公平性和泛化能力，并在领先的deepfake数据集上进行了大量实验，证明该方法在保持鲁棒检测性能的同时，显著提高了个体公平性，优于现有方法。

Abstract: Generative AI models have substantially improved the realism of synthetic
media, yet their misuse through sophisticated DeepFakes poses significant
risks. Despite recent advances in deepfake detection, fairness remains
inadequately addressed, enabling deepfake markers to exploit biases against
specific populations. While previous studies have emphasized group-level
fairness, individual fairness (i.e., ensuring similar predictions for similar
individuals) remains largely unexplored. In this work, we identify for the
first time that the original principle of individual fairness fundamentally
fails in the context of deepfake detection, revealing a critical gap previously
unexplored in the literature. To mitigate it, we propose the first
generalizable framework that can be integrated into existing deepfake detectors
to enhance individual fairness and generalization. Extensive experiments
conducted on leading deepfake datasets demonstrate that our approach
significantly improves individual fairness while maintaining robust detection
performance, outperforming state-of-the-art methods. The code is available at
https://github.com/Purdue-M2/Individual-Fairness-Deepfake-Detection.

</details>


### [178] [Development and Deployment of Hybrid ML Models for Critical Heat Flux Prediction in Annulus Geometries](https://arxiv.org/abs/2507.14332)
*Aidan Furlong,Xingang Zhao,Robert Salko,Xu Wu*

Main category: cs.LG

TL;DR: 本研究开发了用于预测环形几何体中CHF的混合机器学习模型，该模型显著优于传统的经验模型，提高了预测精度。


<details>
  <summary>Details</summary>
Motivation: 精确预测临界热通量（CHF）是压水堆和沸水堆安全分析的重要组成部分。现有的数据驱动方法缺乏可解释性，对数据稀缺的适应性差，并且主要使用来自管实验的数据开发。环形几何体特定的机器学习模型尚未在热工水力代码中部署。

Method: 开发、部署和验证了四个机器学习模型，使用CTF子通道代码预测环形几何体中的CHF。使用Biasi、Bowring和Katto三个经验相关模型作为比较的基础模型。使用来自四个数据集（Becker、Beus、Janssen和Mortimore）的577个实验环形数据点训练和测试机器学习模型。

Result: 机器学习驱动的模型实现了低于3.5%的平均相对误差，且不超过一个点超过10%的误差范围。相比之下，经验相关模型的平均相对误差高于26%。

Conclusion: 混合机器学习模型在预测环形几何体中的临界热通量（CHF）方面显著优于经验模型，实现了低于3.5%的平均相对误差。

Abstract: Accurate prediction of critical heat flux (CHF) is an essential component of
safety analysis in pressurized and boiling water reactors. To support reliable
prediction of this quantity, several empirical correlations and lookup tables
have been constructed from physical experiments over the past several decades.
With the onset of accessible machine learning (ML) frameworks, multiple
initiatives have been established with the goal of predicting CHF more
accurately than these traditional methods. While purely data-driven surrogate
modeling has been extensively investigated, these approaches lack
interpretability, lack resilience to data scarcity, and have been developed
mostly using data from tube experiments. As a result, bias-correction hybrid
approaches have become increasingly popular, which correct initial
"low-fidelity" estimates provided by deterministic base models by using
ML-predicted residuals. This body of work has mostly considered round tube
geometries; annular geometry-specific ML models have not yet been deployed in
thermal hydraulic codes. This study developed, deployed, and validated four ML
models to predict CHF in annular geometries using the CTF subchannel code.
Three empirical correlation models, Biasi, Bowring, and Katto, were used as
base models for comparison. The ML models were trained and tested using 577
experimental annulus data points from four datasets: Becker, Beus, Janssen, and
Mortimore. Baseline CHF predictions were obtained from the empirical
correlations, with mean relative errors above 26%. The ML-driven models
achieved mean relative errors below 3.5%, with no more than one point exceeding
the 10% error envelope. In all cases, the hybrid ML models significantly
outperformed their empirical counterparts.

</details>


### [179] [Influence Functions for Preference Dataset Pruning](https://arxiv.org/abs/2507.14344)
*Daniel Fein,Gabriela Aranguiz-Dias*

Main category: cs.LG

TL;DR: 使用影响函数近似来检测和修剪对验证集性能有害的训练示例。


<details>
  <summary>Details</summary>
Motivation: 语言模型通常通过强化学习进行微调，以改变其行为或引发新的能力。用于这些目的的数据集，特别是人类偏好数据集，通常是嘈杂的。

Method: 调整 TL;DR 数据集以进行奖励模型训练，并展示如何使用共轭梯度近似影响函数来过滤数据集。

Result: 影响函数过滤在删除 10% 的训练样本后产生 1.5% 的小幅重训练准确率提升。梯度相似性在检测有用的训练样本方面优于影响函数。

Conclusion: 使用影响函数过滤数据集可以在删除 10% 的训练样本后产生 1.5% 的小幅重训练准确率提升。梯度相似性在检测有用的训练样本方面优于影响函数。这表明局部曲率对于检测有害训练样本很重要，但对于识别有用的样本则不那么重要。

Abstract: Language models are commonly fine-tuned via reinforcement learning to alter
their behavior or elicit new capabilities. Datasets used for these purposes,
and particularly human preference datasets, are often noisy. The relatively
small size post-training datasets, combined with parameter-efficient
fine-tuning methods, enable the use of influence functions approximations to
detect and prune training examples that are harmful to performance on a
validation set. In this work, we adapt the TL;DR dataset for reward model
training to demonstrate how conjugate-gradient approximated influence functions
can be used to filter datasets. In our experiments, influence function
filtering yields a small retraining accuracy uplift of 1.5% after removing 10%
of training examples. We also show that gradient similarity outperforms
influence functions for detecting helpful training examples. This suggests that
local curvature is important for detecting harmful training examples, but less
so for identifying helpful examples.

</details>


### [180] [Solo Connection: A Parameter Efficient Fine-Tuning Technique for Transformers](https://arxiv.org/abs/2507.14353)
*Harsh Nilesh Pathak,Randy Paffenroth*

Main category: cs.LG

TL;DR: Introduces Solo Connection, a parameter-efficient fine-tuning method that outperforms LoRA by adapting decoder-block level representations and using long skip connections.


<details>
  <summary>Details</summary>
Motivation: The need to revisit how skip connections are employed during fine-tuning, especially in larger language models with many decoder blocks.

Method: Introducing Solo Connection, a novel method that adapts the representation at the decoder-block level with a trainable linear transformation.

Result: Solo Connection outperforms LoRA, reduces trainable parameters by 59% relative to LoRA and by more than 99% compared to full fine-tuning of GPT2.

Conclusion: Solo Connection outperforms LoRA on E2E natural language generation benchmarks, reduces trainable parameters, and is motivated by homotopy theory for stable adaptation.

Abstract: Parameter efficient fine tuning (PEFT) is a versatile and extensible approach
for adapting a Large Language Model (LLM) for newer tasks. One of the most
prominent PEFT approaches, Low Rank Adaptation (LoRA), primarily focuses on
adjusting the attention weight matrices within individual decoder blocks of a
Generative Pre trained Transformer (GPT2). In contrast, we introduce Solo
Connection a novel method that adapts the representation at the decoder-block
level rather than modifying individual weight matrices. Not only does Solo
Connection outperform LoRA on E2E natural language generation benchmarks, but
it also reduces the number of trainable parameters by 59% relative to LoRA and
by more than 99% compared to full fine-tuning of GPT2, an early version of
Large Language Models (LLMs). Solo Connection is also motivated by homotopy
theory: we introduce a trainable linear transformation that gradually
interpolates between a zero vector and the task-specific representation,
enabling smooth and stable adaptation over time. While skip connections in the
original 12 layer GPT2 are typically confined to individual decoder blocks,
subsequent GPT2 variants scale up to 48 layers, and even larger language models
can include 128 or more decoder blocks. These expanded architectures underscore
the need to revisit how skip connections are employed during fine-tuning. This
paper focuses on long skip connections that link outputs of different decoder
blocks, potentially enhancing the model's ability to adapt to new tasks while
leveraging pre-trained knowledge.

</details>


### [181] [Incremental Causal Graph Learning for Online Cyberattack Detection in Cyber-Physical Infrastructures](https://arxiv.org/abs/2507.14387)
*Arun Vignesh Malarkkan,Dongjie Wang,Haoyue Bai,Yanjie Fu*

Main category: cs.LG

TL;DR: INCADET 是一种用于实时网络攻击检测的增量因果图学习框架，它优于现有的方法。


<details>
  <summary>Details</summary>
Motivation: 传统实时异常检测技术容易出现过多的误报，并且无法适应动态变化的数据分布和灾难性遗忘。

Method: INCADET，一个用于实时网络攻击检测的增量因果图学习框架。它通过跨流时间窗口增量更新因果图来动态捕获不断演变的系统行为。该框架包括三个模块：1) 早期症状检测；2) 增量因果图学习；3) 因果图分类。

Result: INCADET 实现了优于静态因果和深度时间基线的准确性、鲁棒性和适应性。

Conclusion: INCADET在真实世界的关键基础设施数据集上实现了优于静态因果和深度时间基线的准确性、鲁棒性和适应性。

Abstract: The escalating threat of cyberattacks on real-time critical infrastructures
poses serious risks to public safety, demanding detection methods that
effectively capture complex system interdependencies and adapt to evolving
attack patterns. Traditional real-time anomaly detection techniques often
suffer from excessive false positives due to their statistical sensitivity to
high data variance and class imbalance. To address these limitations, recent
research has explored modeling causal relationships among system components.
However, prior work mainly focuses on offline causal graph-based approaches
that require static historical data and fail to generalize to real-time
settings. These methods are fundamentally constrained by: (1) their inability
to adapt to dynamic shifts in data distribution without retraining, and (2) the
risk of catastrophic forgetting when lacking timely supervision in live
systems. To overcome these challenges, we propose INCADET, a novel framework
for incremental causal graph learning tailored to real-time cyberattack
detection. INCADET dynamically captures evolving system behavior by
incrementally updating causal graphs across streaming time windows. The
framework comprises three modules: 1) Early Symptom Detection: Detects
transitions in system status using divergence in edge-weight distributions
across sequential causal graphs. 2) Incremental Causal Graph Learning:
Leverages experience replay and edge reinforcement to continually refine causal
structures while preserving prior knowledge. 3) Causal Graph Classification:
Employs Graph Convolutional Networks (GCNs) to classify system status using the
learned causal graphs. Extensive experiments on real-world critical
infrastructure datasets demonstrate that INCADET achieves superior accuracy,
robustness, and adaptability compared to both static causal and deep temporal
baselines in evolving attack scenarios.

</details>


### [182] [It's Not That Simple. An Analysis of Simple Test-Time Scaling](https://arxiv.org/abs/2507.14419)
*Guojun Wu*

Main category: cs.LG

TL;DR: 简单测试时缩放的行为主要归因于通过强制最大长度来缩小模型规模。


<details>
  <summary>Details</summary>
Motivation: 先前的工作提出了简单的测试时缩放，一种通过手动控制测试时计算来复制这种缩放行为的方法，通过强制最大长度来缩小规模，或者在模型即将结束其生成时通过迭代附加“等待”来扩大规模。

Method: 分析简单测试时缩放

Result: 发现缩放行为主要归因于通过强制最大长度来缩小规模。通过附加“等待”来扩大规模会导致不一致。

Conclusion: 简单测试时缩放的行为主要归因于通过强制最大长度来缩小模型规模。与此相反，在从o1类模型中提取的长CoT数据上进行微调对缩放行为没有显著影响，并且通过附加“等待”来扩大规模会导致不一致，因为模型可能在解决方案之间振荡。

Abstract: Prior work proposed simple test-time scaling, a method for replicating this
scaling behavior with models distilled from o1-like models by manually
controlling test-time compute: either scaling down by enforcing a maximum
length or scaling up by iteratively appending "Wait" when the model is about to
terminate its generation. This paper presents an analysis of simple test-time
scaling and finds that the scaling behavior is largely attributed to scaling
down by enforcing a maximum length. In contrast, fine-tuning on long CoT data
distilled from o1-like models has no significant impact on scaling behavior,
and scaling up by appending "Wait" leads to inconsistencies, as the model may
oscillate between solutions. A key distinction exists between scaling down by
enforcing a maximum length and scaling up test-time compute in o1-like models,
such as DeepSeek-R1\@. These models are typically allowed to utilize as much
compute as needed, with the only constraint being the model's maximum supported
length. By learning to naturally scale up test-time compute during
reinforcement learning, o1-like models surpass their peak performance when
scaling up. In contrast, simple test-time scaling progressively imposes a lower
upper limit on model performance as it scales down. While replicating the
test-time scaling behavior of o1 models can be straightforward by scaling down,
it is crucial to recognize that the goal of scaling test-time compute is to
unlock higher performance -- beyond what the model could originally achieve --
rather than merely reproducing the appearance of scaling behavior.

</details>


### [183] [Deep RL Dual Sourcing Inventory Management with Supply and Capacity Risk Awareness](https://arxiv.org/abs/2507.14446)
*Feng Liu,Ying Liu,Carson Eisenach*

Main category: cs.LG

TL;DR: 本研究利用干预模型，通过深度学习模拟和组合随机过程，以强化学习方法高效解决大规模随机优化问题，并在供应链管理问题上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究如何有效地应用强化学习 (RL) 来解决大规模随机优化问题。

Method: 利用干预模型，通过使用预先训练的深度学习 (DL) 模型模拟和组合随机过程来更好地探索解决方案空间。

Result: 在供应链优化中具有挑战性的现实应用，即多寻源多周期库存管理问题上证明了该方法的有效性。采用深度强化学习模型来学习和预测各种假设下的随机供应链流程。引入了一种约束协调机制，旨在预测库存网络中给定交叉产品约束的双重成本。

Conclusion: 该方法将供应链流程分解为可扩展和可组合的深度学习模块，从而提高了大型实际数据集的性能。

Abstract: In this work, we study how to efficiently apply reinforcement learning (RL)
for solving large-scale stochastic optimization problems by leveraging
intervention models. The key of the proposed methodology is to better explore
the solution space by simulating and composing the stochastic processes using
pre-trained deep learning (DL) models. We demonstrate our approach on a
challenging real-world application, the multi-sourcing multi-period inventory
management problem in supply chain optimization. In particular, we employ deep
RL models for learning and forecasting the stochastic supply chain processes
under a range of assumptions. Moreover, we also introduce a constraint
coordination mechanism, designed to forecast dual costs given the
cross-products constraints in the inventory network. We highlight that instead
of directly modeling the complex physical constraints into the RL optimization
problem and solving the stochastic problem as a whole, our approach breaks down
those supply chain processes into scalable and composable DL modules, leading
to improved performance on large real-world datasets. We also outline open
problems for future research to further investigate the efficacy of such
models.

</details>


### [184] [ReDiSC: A Reparameterized Masked Diffusion Model for Scalable Node Classification with Structured Predictions](https://arxiv.org/abs/2507.14484)
*Yule Li,Yifeng Lu,Zhen Wang,Zhewei Wei,Yaliang Li,Bolin Ding*

Main category: cs.LG

TL;DR: ReDiSC：一种用于结构化节点分类的重新参数化掩码扩散模型，它优于现有 GNN 和标签传播方法，尤其是在大型数据集上。


<details>
  <summary>Details</summary>
Motivation: 现有方法隐式地假设节点标签之间的条件独立性，这与图中的节点标签即使在以图结构为条件的情况下仍然相关的直观观察相矛盾。

Method: 提出ReDiSC，即用于结构化节点分类的重新参数化掩码扩散模型，它使用通过变分期望最大化 (EM) 框架学习的重新参数化掩码扩散模型来估计节点标签的联合分布。

Result: ReDiSC 实现了优于或极具竞争力的性能，并且可以有效地扩展到大型数据集。

Conclusion: ReDiSC在各种大小的同质和异质图上实现了优越或极具竞争力的性能，并且可以有效地扩展到大型数据集。

Abstract: In recent years, graph neural networks (GNN) have achieved unprecedented
successes in node classification tasks. Although GNNs inherently encode
specific inductive biases (e.g., acting as low-pass or high-pass filters), most
existing methods implicitly assume conditional independence among node labels
in their optimization objectives. While this assumption is suitable for
traditional classification tasks such as image recognition, it contradicts the
intuitive observation that node labels in graphs remain correlated, even after
conditioning on the graph structure. To make structured predictions for node
labels, we propose ReDiSC, namely, Reparameterized masked Diffusion model for
Structured node Classification. ReDiSC estimates the joint distribution of node
labels using a reparameterized masked diffusion model, which is learned through
the variational expectation-maximization (EM) framework. Our theoretical
analysis shows the efficiency advantage of ReDiSC in the E-step compared to
DPM-SNC, a state-of-the-art model that relies on a manifold-constrained
diffusion model in continuous domain. Meanwhile, we explicitly link ReDiSC's
M-step objective to popular GNN and label propagation hybrid approaches.
Extensive experiments demonstrate that ReDiSC achieves superior or highly
competitive performance compared to state-of-the-art GNN, label propagation,
and diffusion-based baselines across both homophilic and heterophilic graphs of
varying sizes. Notably, ReDiSC scales effectively to large-scale datasets on
which previous structured diffusion methods fail due to computational
constraints, highlighting its significant practical advantage in structured
node classification tasks.

</details>


### [185] [Federated Reinforcement Learning in Heterogeneous Environments](https://arxiv.org/abs/2507.14487)
*Ukjo Hwang,Songnam Hong*

Main category: cs.LG

TL;DR: 该论文提出了一种新的鲁棒联邦强化学习框架，可以有效解决环境异构性问题。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在解决联邦强化学习中环境异构性的问题，并提高算法在异构环境中的鲁棒性。

Method: 该论文提出了一种名为FedRQ的表格型联邦强化学习算法，并将其扩展到连续状态空间。

Result: 实验结果表明，该论文提出的算法在各种异构环境中均优于现有的联邦强化学习算法。

Conclusion: 该论文提出了一种新的联邦强化学习算法，并在异构环境中实现了优于现有算法的性能。

Abstract: We investigate a Federated Reinforcement Learning with Environment
Heterogeneity (FRL-EH) framework, where local environments exhibit statistical
heterogeneity. Within this framework, agents collaboratively learn a global
policy by aggregating their collective experiences while preserving the privacy
of their local trajectories. To better reflect real-world scenarios, we
introduce a robust FRL-EH framework by presenting a novel global objective
function. This function is specifically designed to optimize a global policy
that ensures robust performance across heterogeneous local environments and
their plausible perturbations. We propose a tabular FRL algorithm named FedRQ
and theoretically prove its asymptotic convergence to an optimal policy for the
global objective function. Furthermore, we extend FedRQ to environments with
continuous state space through the use of expectile loss, addressing the key
challenge of minimizing a value function over a continuous subset of the state
space. This advancement facilitates the seamless integration of the principles
of FedRQ with various Deep Neural Network (DNN)-based RL algorithms. Extensive
empirical evaluations validate the effectiveness and robustness of our FRL
algorithms across diverse heterogeneous environments, consistently achieving
superior performance over the existing state-of-the-art FRL algorithms.

</details>


### [186] [Glitches in Decision Tree Ensemble Models](https://arxiv.org/abs/2507.14492)
*Satyankar Chandra,Ashutosh Gupta,Kaushik Mallik,Krishna Shankaranarayanan,Namrita Varshney*

Main category: cs.LG

TL;DR: Identified and analyzed 'glitches' (small input regions causing model output oscillations) in AI models, showing they're common, indicate inconsistencies, and are hard to detect efficiently in tree ensembles.


<details>
  <summary>Details</summary>
Motivation: To address the unreliability of AI models with steep decision boundaries, especially inconsistencies across similar inputs.

Method: Algorithmic search of glitches using an MILP encoding.

Result: Demonstrated the widespread existence of glitches in well-known models and datasets. Proved that detecting glitches is NP-complete for tree ensembles.

Conclusion: Glitches indicate potential model inconsistencies.

Abstract: Many critical decision-making tasks are now delegated to machine-learned
models, and it is imperative that their decisions are trustworthy and reliable,
and their outputs are consistent across similar inputs. We identify a new
source of unreliable behaviors-called glitches-which may significantly impair
the reliability of AI models having steep decision boundaries. Roughly
speaking, glitches are small neighborhoods in the input space where the model's
output abruptly oscillates with respect to small changes in the input. We
provide a formal definition of glitches, and use well-known models and datasets
from the literature to demonstrate that they have widespread existence and
argue they usually indicate potential model inconsistencies in the neighborhood
of where they are found. We proceed to the algorithmic search of glitches for
widely used gradient-boosted decision tree (GBDT) models. We prove that the
problem of detecting glitches is NP-complete for tree ensembles, already for
trees of depth 4. Our glitch-search algorithm for GBDT models uses an MILP
encoding of the problem, and its effectiveness and computational feasibility
are demonstrated on a set of widely used GBDT benchmarks taken from the
literature.

</details>


### [187] [Generative Distribution Distillation](https://arxiv.org/abs/2507.14503)
*Jiequan Cui,Beier Zhu,Qingshan Xu,Xiaogang Xu,Pengguang Chen,Xiaojuan Qi,Bei Yu,Hanwang Zhang,Richang Hong*

Main category: cs.LG

TL;DR: This paper introduces Generative Distribution Distillation (GenDD) for knowledge distillation, using Split Tokenization and Distribution Contraction to improve performance in unsupervised and supervised settings, achieving state-of-the-art results on ImageNet.


<details>
  <summary>Details</summary>
Motivation: To address the curse of high-dimensional optimization and the lack of semantic supervision from labels in knowledge distillation (KD).

Method: Generative Distribution Distillation (GenDD) framework with Split Tokenization and Distribution Contraction

Result: GenDD performs competitively in the unsupervised setting and achieves state-of-the-art results with label supervision.

Conclusion: GenDD performs competitively in the unsupervised setting, significantly surpassing KL baseline by 16.29% on ImageNet validation set. With label supervision, ResNet-50 achieves 82.28% top-1 accuracy on ImageNet in 600 epochs training, establishing a new state-of-the-art.

Abstract: In this paper, we formulate the knowledge distillation (KD) as a conditional
generative problem and propose the \textit{Generative Distribution Distillation
(GenDD)} framework. A naive \textit{GenDD} baseline encounters two major
challenges: the curse of high-dimensional optimization and the lack of semantic
supervision from labels. To address these issues, we introduce a \textit{Split
Tokenization} strategy, achieving stable and effective unsupervised KD.
Additionally, we develop the \textit{Distribution Contraction} technique to
integrate label supervision into the reconstruction objective. Our theoretical
proof demonstrates that \textit{GenDD} with \textit{Distribution Contraction}
serves as a gradient-level surrogate for multi-task learning, realizing
efficient supervised training without explicit classification loss on
multi-step sampling image representations. To evaluate the effectiveness of our
method, we conduct experiments on balanced, imbalanced, and unlabeled data.
Experimental results show that \textit{GenDD} performs competitively in the
unsupervised setting, significantly surpassing KL baseline by \textbf{16.29\%}
on ImageNet validation set. With label supervision, our ResNet-50 achieves
\textbf{82.28\%} top-1 accuracy on ImageNet in 600 epochs training,
establishing a new state-of-the-art.

</details>


### [188] [SDSC:A Structure-Aware Metric for Semantic Signal Representation Learning](https://arxiv.org/abs/2507.14516)
*Jeyoung Lee,Hochul Kang*

Main category: cs.LG

TL;DR: SDSC: a structure-aware metric function for time series self-supervised representation learning, achieves comparable or improved performance over MSE.


<details>
  <summary>Details</summary>
Motivation: Most Self-Supervised Learning (SSL) methods for signals commonly adopt distance-based objectives such as mean squared error (MSE), which are sensitive to amplitude, invariant to waveform polarity, and unbounded in scale. These properties hinder semantic alignment and reduce interpretability.

Method: We propose the Signal Dice Similarity Coefficient (SDSC), a structure-aware metric function for time series self-supervised representation learning. A hybrid loss formulation is also proposed to combine SDSC with MSE

Result: Experiments on forecasting and classification benchmarks demonstrate that SDSC-based pre-training achieves comparable or improved performance over MSE, particularly in in-domain and low-resource scenarios.

Conclusion: SDSC-based pre-training achieves comparable or improved performance over MSE, particularly in in-domain and low-resource scenarios. Structural fidelity in signal representations enhances the semantic representation quality.

Abstract: We propose the Signal Dice Similarity Coefficient (SDSC), a structure-aware
metric function for time series self-supervised representation learning. Most
Self-Supervised Learning (SSL) methods for signals commonly adopt
distance-based objectives such as mean squared error (MSE), which are sensitive
to amplitude, invariant to waveform polarity, and unbounded in scale. These
properties hinder semantic alignment and reduce interpretability. SDSC
addresses this by quantifying structural agreement between temporal signals
based on the intersection of signed amplitudes, derived from the Dice
Similarity Coefficient (DSC).Although SDSC is defined as a structure-aware
metric, it can be used as a loss by subtracting from 1 and applying a
differentiable approximation of the Heaviside function for gradient-based
optimization. A hybrid loss formulation is also proposed to combine SDSC with
MSE, improving stability and preserving amplitude where necessary. Experiments
on forecasting and classification benchmarks demonstrate that SDSC-based
pre-training achieves comparable or improved performance over MSE, particularly
in in-domain and low-resource scenarios. The results suggest that structural
fidelity in signal representations enhances the semantic representation
quality, supporting the consideration of structure-aware metrics as viable
alternatives to conventional distance-based methods.

</details>


### [189] [Positive-Unlabeled Learning for Control Group Construction in Observational Causal Inference](https://arxiv.org/abs/2507.14528)
*Ilias Tsoumas,Dimitrios Bormpoudakis,Vasileios Sitokonstantinou,Athanasios Askitopoulos,Andreas Kalogeras,Charalampos Kontoes,Ioannis Athanasiadis*

Main category: cs.LG

TL;DR: This paper introduces a PU learning framework to identify control units when they are not readily available, using only treated units. The method is evaluated on both simulated and real-world data, showing it can effectively estimate the average treatment effect (ATE).


<details>
  <summary>Details</summary>
Motivation: A common challenge, especially in observational studies, is the absence of units clearly labeled as controls. To address this, we propose positive-unlabeled (PU) learning as a framework for identifying, with high confidence, control units from a pool of unlabeled ones, using only the available treated (positive) units.

Method: propose positive-unlabeled (PU) learning as a framework for identifying control units from a pool of unlabeled ones, using only the available treated (positive) units

Result: PU learning can successfully identify control (negative) units from unlabeled data based only on treated units and, through the resulting control group, estimate an ATE that closely approximates the true value.

Conclusion: PU learning can successfully identify control (negative) units from unlabeled data based only on treated units and estimate an ATE that closely approximates the true value. This work has important implications for observational causal inference, especially in fields where randomized experiments are difficult or costly. It enables a plethora of quasi-experiments by leveraging available earth observation and climate data, particularly when treated units are available but control units are lacking.

Abstract: In causal inference, whether through randomized controlled trials or
observational studies, access to both treated and control units is essential
for estimating the effect of a treatment on an outcome of interest. When
treatment assignment is random, the average treatment effect (ATE) can be
estimated directly by comparing outcomes between groups. In non-randomized
settings, various techniques are employed to adjust for confounding and
approximate the counterfactual scenario to recover an unbiased ATE. A common
challenge, especially in observational studies, is the absence of units clearly
labeled as controls-that is, units known not to have received the treatment. To
address this, we propose positive-unlabeled (PU) learning as a framework for
identifying, with high confidence, control units from a pool of unlabeled ones,
using only the available treated (positive) units. We evaluate this approach
using both simulated and real-world data. We construct a causal graph with
diverse relationships and use it to generate synthetic data under various
scenarios, assessing how reliably the method recovers control groups that allow
estimates of true ATE. We also apply our approach to real-world data on optimal
sowing and fertilizer treatments in sustainable agriculture. Our findings show
that PU learning can successfully identify control (negative) units from
unlabeled data based only on treated units and, through the resulting control
group, estimate an ATE that closely approximates the true value. This work has
important implications for observational causal inference, especially in fields
where randomized experiments are difficult or costly. In domains such as earth,
environmental, and agricultural sciences, it enables a plethora of
quasi-experiments by leveraging available earth observation and climate data,
particularly when treated units are available but control units are lacking.

</details>


### [190] [Kernel Based Maximum Entropy Inverse Reinforcement Learning for Mean-Field Games](https://arxiv.org/abs/2507.14529)
*Berkay Anahtarci,Can Deha Kariksiz,Naci Saldi*

Main category: cs.LG

TL;DR: This paper addresses the problem of inferring complex reward functions in infinite-horizon mean-field games using maximum causal entropy inverse reinforcement learning and a reproducing kernel Hilbert space, and introduces a gradient ascent algorithm to solve it. The method's effectiveness is demonstrated in a traffic routing game.


<details>
  <summary>Details</summary>
Motivation: The paper considers the maximum causal entropy inverse reinforcement learning problem for infinite-horizon stationary mean-field games, in which the reward function is modeled within a reproducing kernel Hilbert space. This allows the inference of rich and potentially nonlinear reward structures directly from expert demonstrations, in contrast to most existing inverse reinforcement learning approaches for mean-field games that typically restrict the reward function to a linear combination of a fixed finite set of basis functions. The paper also focuses on the infinite-horizon cost structure, whereas prior studies primarily rely on finite-horizon formulations.

Method: A Lagrangian relaxation is introduced to this maximum causal entropy inverse reinforcement learning problem that enables us to reformulate it as an unconstrained log-likelihood maximization problem, and a solution is obtained via a gradient ascent algorithm.

Result: The smoothness of the log-likelihood objective is established by proving the Fréchet differentiability of the related soft Bellman operators with respect to the parameters in the reproducing kernel Hilbert space.

Conclusion: The effectiveness of the method is demonstrated on a mean-field traffic routing game, where it accurately recovers expert behavior.

Abstract: We consider the maximum causal entropy inverse reinforcement learning problem
for infinite-horizon stationary mean-field games, in which we model the unknown
reward function within a reproducing kernel Hilbert space. This allows the
inference of rich and potentially nonlinear reward structures directly from
expert demonstrations, in contrast to most existing inverse reinforcement
learning approaches for mean-field games that typically restrict the reward
function to a linear combination of a fixed finite set of basis functions. We
also focus on the infinite-horizon cost structure, whereas prior studies
primarily rely on finite-horizon formulations. We introduce a Lagrangian
relaxation to this maximum causal entropy inverse reinforcement learning
problem that enables us to reformulate it as an unconstrained log-likelihood
maximization problem, and obtain a solution \lk{via} a gradient ascent
algorithm. To illustrate the theoretical consistency of the algorithm, we
establish the smoothness of the log-likelihood objective by proving the
Fr\'echet differentiability of the related soft Bellman operators with respect
to the parameters in the reproducing kernel Hilbert space. We demonstrate the
effectiveness of our method on a mean-field traffic routing game, where it
accurately recovers expert behavior.

</details>


### [191] [The Origin of Self-Attention: From Pairwise Affinity Matrices to Transformers](https://arxiv.org/abs/2507.14560)
*Giorgio Roffo*

Main category: cs.LG

TL;DR: Self-attention是基于亲和力计算的一种特殊情况，与Inf-FS共享底层结构，差异在于亲和矩阵的定义和应用。


<details>
  <summary>Details</summary>
Motivation: self-attention机制是深度学习架构（如Transformer）的核心，但它是一种更通用的计算原则（学习和使用成对亲和矩阵来控制信息如何在模型中流动）的现代实例。

Method: 通过追踪self-attention在计算机视觉、自然语言处理和图学习等多个领域的概念起源，并将其与Infinite Feature Selection (Inf-FS)联系起来。

Result: 将self-attention置于基于亲和力的计算的更广泛范例中，统一了机器学习研究的多个分支，并强调了支撑不同模型和任务的共同数学基础。

Conclusion: self-attention是基于亲和力的计算方法的一种特殊情况，它与Inf-FS共享一个潜在的结构，即基于成对关系的推理。关键区别在于亲和矩阵的定义和应用方式。

Abstract: The self-attention mechanism, now central to deep learning architectures such
as Transformers, is a modern instance of a more general computational
principle: learning and using pairwise affinity matrices to control how
information flows through a model. This paper traces the conceptual origins of
self-attention across multiple domains, including computer vision, natural
language processing, and graph learning, through their shared reliance on an
affinity matrix, denoted as A. We highlight Infinite Feature Selection (Inf-FS)
as a foundational approach that generalizes the idea of affinity-based
weighting. Unlike the fixed dot-product structure used in Transformers, Inf-FS
defines A either through domain knowledge or by learning, and computes feature
relevance through multi-hop propagation over the affinity graph. From this
perspective, self-attention can be seen as a special case of Inf-FS: it uses a
single-hop affinity computation where A is dynamically built from token
similarities. We argue that the underlying structure, reasoning over pairwise
relationships, is preserved across both approaches, and the key differences lie
in how the affinity matrix is defined and applied. By situating self-attention
within the broader paradigm of affinity-based computation, we unify several
strands of machine learning research and highlight a common mathematical
foundation that underpins diverse models and tasks.

</details>


### [192] [LPS-GNN : Deploying Graph Neural Networks on Graphs with 100-Billion Edges](https://arxiv.org/abs/2507.14570)
*Xu Cheng,Liang Yao,Feng He,Yukuo Cen,Yufei He,Chenhui Zhang,Wenzheng Feng,Hongyun Cai,Jie Tang*

Main category: cs.LG

TL;DR: LPS-GNN 是一个可扩展、低成本、灵活且高效的 GNN 框架，它优于现有技术，并且可以部署在腾讯平台上。


<details>
  <summary>Details</summary>
Motivation: 现有的可扩展解决方案通常难以平衡执行效率和预测准确性。这些困难源于迭代消息传递技术，这些技术提出了重要的计算需求，并且需要大量的 GPU 内存，尤其是在处理大规模图中固有的邻居爆炸问题时。

Method: 提出了一种名为 LPMetis 的图划分算法和子图增强策略。

Result: LPS-GNN 可以在 10 小时内使用单个 GPU 对 1000 亿个图执行表示学习，并在用户获取场景中显示出 13.8% 的改进。

Conclusion: LPS-GNN在公共和真实世界的数据集上进行了测试，在在线应用中，相对于 SOTA 模型，性能提升了 8.24% 到 13.89%。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for various graph
mining tasks, yet existing scalable solutions often struggle to balance
execution efficiency with prediction accuracy. These difficulties stem from
iterative message-passing techniques, which place significant computational
demands and require extensive GPU memory, particularly when dealing with the
neighbor explosion issue inherent in large-scale graphs. This paper introduces
a scalable, low-cost, flexible, and efficient GNN framework called LPS-GNN,
which can perform representation learning on 100 billion graphs with a single
GPU in 10 hours and shows a 13.8% improvement in User Acquisition scenarios. We
examine existing graph partitioning methods and design a superior graph
partition algorithm named LPMetis. In particular, LPMetis outperforms current
state-of-the-art (SOTA) approaches on various evaluation metrics. In addition,
our paper proposes a subgraph augmentation strategy to enhance the model's
predictive performance. It exhibits excellent compatibility, allowing the
entire framework to accommodate various GNN algorithms. Successfully deployed
on the Tencent platform, LPS-GNN has been tested on public and real-world
datasets, achieving performance lifts of 8. 24% to 13. 89% over SOTA models in
online applications.

</details>


### [193] [A Transformer-Based Conditional GAN with Multiple Instance Learning for UAV Signal Detection and Classification](https://arxiv.org/abs/2507.14592)
*Haochen Liu,Jia Bi,Xiaomin Wang,Xin Yang,Ling Wang*

Main category: cs.LG

TL;DR: This paper proposes a novel framework that integrates a Transformer-based Generative Adversarial Network (GAN) with Multiple Instance Locally Explainable Learning (MILET) to address challenges in UAV flight state classification.


<details>
  <summary>Details</summary>
Motivation: conventional time series classification (TSC) methods often lack robustness and generalization for dynamic UAV environments, while state of the art(SOTA) models like Transformers and LSTM based architectures typically require large datasets and entail high computational costs, especially with high-dimensional data streams

Method: integrates a Transformer-based Generative Adversarial Network (GAN) with Multiple Instance Locally Explainable Learning (MILET)

Result: achieves superior accuracy 96.5% on the DroneDetect dataset and 98.6% on the DroneRF dataset that outperforming other SOTA approaches. The framework also demonstrates strong computational efficiency

Conclusion: The proposed framework achieves superior accuracy and robust generalization across diverse UAV platforms and flight states, highlighting its potential for real-time deployment in resource constrained environments.

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly used in surveillance,
logistics, agriculture, disaster management, and military operations. Accurate
detection and classification of UAV flight states, such as hovering, cruising,
ascending, or transitioning, which are essential for safe and effective
operations. However, conventional time series classification (TSC) methods
often lack robustness and generalization for dynamic UAV environments, while
state of the art(SOTA) models like Transformers and LSTM based architectures
typically require large datasets and entail high computational costs,
especially with high-dimensional data streams. This paper proposes a novel
framework that integrates a Transformer-based Generative Adversarial Network
(GAN) with Multiple Instance Locally Explainable Learning (MILET) to address
these challenges in UAV flight state classification. The Transformer encoder
captures long-range temporal dependencies and complex telemetry dynamics, while
the GAN module augments limited datasets with realistic synthetic samples. MIL
is incorporated to focus attention on the most discriminative input segments,
reducing noise and computational overhead. Experimental results show that the
proposed method achieves superior accuracy 96.5% on the DroneDetect dataset and
98.6% on the DroneRF dataset that outperforming other SOTA approaches. The
framework also demonstrates strong computational efficiency and robust
generalization across diverse UAV platforms and flight states, highlighting its
potential for real-time deployment in resource constrained environments.

</details>


### [194] [$k$-PCA for (non-squared) Euclidean Distances: Polynomial Time Approximation](https://arxiv.org/abs/2507.14631)
*Daniel Greenhut,Dan Feldman*

Main category: cs.LG

TL;DR: This paper introduces the first polynomial-time deterministic algorithm for approximating the k-subspace median, achieving a $\sqrt{d}$ approximation factor with polynomial running time. The approach is expected to be applicable to related problems and includes open code and experimental results.


<details>
  <summary>Details</summary>
Motivation: The median subspace is usually more sparse and robust to noise/outliers than the mean, but also much harder to approximate since, unlike the $\ell_{z,z}$ (non-mixed) norms, it is non-convex for $k<d-1$.

Method: a polynomial-time deterministic algorithm

Result: the multiplicative approximation factor is $\sqrt{d}$, and the running time is polynomial in the size of the input. Open code and experimental results on real-world datasets are also provided.

Conclusion: We provide the first polynomial-time deterministic algorithm whose both running time and approximation factor are not exponential in $k$. More precisely, the multiplicative approximation factor is $\sqrt{d}$, and the running time is polynomial in the size of the input. We expect that our technique would be useful for many other related problems, such as $\ell_{2,z}$ norm of distances for $z\not \in \br{1,2}$, e.g., $z=\infty$, and handling outliers/sparsity.

Abstract: Given an integer $k\geq1$ and a set $P$ of $n$ points in $\REAL^d$, the
classic $k$-PCA (Principle Component Analysis) approximates the affine
\emph{$k$-subspace mean} of $P$, which is the $k$-dimensional affine linear
subspace that minimizes its sum of squared Euclidean distances
($\ell_{2,2}$-norm) over the points of $P$, i.e., the mean of these distances.
The \emph{$k$-subspace median} is the subspace that minimizes its sum of
(non-squared) Euclidean distances ($\ell_{2,1}$-mixed norm), i.e., their
median. The median subspace is usually more sparse and robust to noise/outliers
than the mean, but also much harder to approximate since, unlike the
$\ell_{z,z}$ (non-mixed) norms, it is non-convex for $k<d-1$.
  We provide the first polynomial-time deterministic algorithm whose both
running time and approximation factor are not exponential in $k$. More
precisely, the multiplicative approximation factor is $\sqrt{d}$, and the
running time is polynomial in the size of the input. We expect that our
technique would be useful for many other related problems, such as $\ell_{2,z}$
norm of distances for $z\not \in \br{1,2}$, e.g., $z=\infty$, and handling
outliers/sparsity.
  Open code and experimental results on real-world datasets are also provided.

</details>


### [195] [Rec-AD: An Efficient Computation Framework for FDIA Detection Based on Tensor Train Decomposition and Deep Learning Recommendation Model](https://arxiv.org/abs/2507.14668)
*Yunfeng Li,Junhong Liu,Zhaohui Yang,Guofu Liao,Chuyun Zhang*

Main category: cs.LG

TL;DR: 本文提出Rec-AD，通过嵌入压缩、优化数据访问和流水线训练机制，提高FDIA检测的计算效率和实时性，增强智能电网安全性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型已被广泛应用于智能电网中的虚假数据注入攻击（FDIA）检测，但日益增长的系统规模和数据维度带来了巨大的计算和内存负担，特别是在大型工业数据集中，限制了检测效率。

Method: 该论文提出Rec-AD，一个集成了张量火车分解与深度学习推荐模型（DLRM）的计算效率框架。

Result: 实验结果表明，Rec-AD显著提高了计算吞吐量和实时检测性能。

Conclusion: Rec-AD显著提高了计算吞吐量和实时检测性能，缩小了攻击窗口并增加了攻击者成本。这些进步加强了边缘计算能力和可扩展性，为智能电网安全提供了强大的技术支持。

Abstract: Deep learning models have been widely adopted for False Data Injection Attack
(FDIA) detection in smart grids due to their ability to capture unstructured
and sparse features. However, the increasing system scale and data
dimensionality introduce significant computational and memory burdens,
particularly in large-scale industrial datasets, limiting detection efficiency.
To address these issues, this paper proposes Rec-AD, a computationally
efficient framework that integrates Tensor Train decomposition with the Deep
Learning Recommendation Model (DLRM). Rec-AD enhances training and inference
efficiency through embedding compression, optimized data access via index
reordering, and a pipeline training mechanism that reduces memory communication
overhead. Fully compatible with PyTorch, Rec-AD can be integrated into existing
FDIA detection systems without code modifications. Experimental results show
that Rec-AD significantly improves computational throughput and real-time
detection performance, narrowing the attack window and increasing attacker
cost. These advancements strengthen edge computing capabilities and
scalability, providing robust technical support for smart grid security.

</details>


### [196] [Revisiting Graph Contrastive Learning on Anomaly Detection: A Structural Imbalance Perspective](https://arxiv.org/abs/2507.14677)
*Yiming Xu,Zhen Peng,Bin Shi,Xu Hua,Bo Dong,Song Wang,Chen Chen*

Main category: cs.LG

TL;DR: AD-GCL通过邻居修剪和邻居补全来解决图异常检测中的结构不平衡问题，从而提高头部和尾部异常的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于GCL的模型忽略了结构不平衡的鲁棒性，无法捕获尾部异常。

Method: AD-GCL框架，包含邻居修剪策略和异常引导的邻居补全。

Result: 在多个数据集上的性能评估验证了AD-GCL的综合优势。

Conclusion: AD-GCL在检测头部和尾部异常方面表现出色。

Abstract: The superiority of graph contrastive learning (GCL) has prompted its
application to anomaly detection tasks for more powerful risk warning systems.
Unfortunately, existing GCL-based models tend to excessively prioritize overall
detection performance while neglecting robustness to structural imbalance,
which can be problematic for many real-world networks following power-law
degree distributions. Particularly, GCL-based methods may fail to capture tail
anomalies (abnormal nodes with low degrees). This raises concerns about the
security and robustness of current anomaly detection algorithms and therefore
hinders their applicability in a variety of realistic high-risk scenarios. To
the best of our knowledge, research on the robustness of graph anomaly
detection to structural imbalance has received little scrutiny. To address the
above issues, this paper presents a novel GCL-based framework named AD-GCL. It
devises the neighbor pruning strategy to filter noisy edges for head nodes and
facilitate the detection of genuine tail nodes by aligning from head nodes to
forged tail nodes. Moreover, AD-GCL actively explores potential neighbors to
enlarge the receptive field of tail nodes through anomaly-guided neighbor
completion. We further introduce intra- and inter-view consistency loss of the
original and augmentation graph for enhanced representation. The performance
evaluation of the whole, head, and tail nodes on multiple datasets validates
the comprehensive superiority of the proposed AD-GCL in detecting both head
anomalies and tail anomalies.

</details>


### [197] [GCC-Spam: Spam Detection via GAN, Contrastive Learning, and Character Similarity Networks](https://arxiv.org/abs/2507.14679)
*Zixin Xu,Zhijie Wang,Zhiyuan Pan*

Main category: cs.LG

TL;DR: This paper introduces GCC-Spam, a new spam detection framework using character similarity networks, contrastive learning, and GANs to improve detection rates with less labeled data.


<details>
  <summary>Details</summary>
Motivation: The exponential growth of spam text on the Internet necessitates robust detection mechanisms to mitigate risks such as information leakage and social instability. Existing challenges include adversarial strategies employed by spammers and the scarcity of labeled data.

Method: A novel spam-text detection framework GCC-Spam, which integrates a character similarity network, contrastive learning, and a Generative Adversarial Network (GAN).

Result: The model achieves higher detection rates with significantly fewer labeled examples compared to baseline approaches.

Conclusion: The proposed GCC-Spam model outperforms baseline approaches on real-world datasets, achieving higher detection rates with significantly fewer labeled examples.

Abstract: The exponential growth of spam text on the Internet necessitates robust
detection mechanisms to mitigate risks such as information leakage and social
instability. This work addresses two principal challenges: adversarial
strategies employed by spammers and the scarcity of labeled data. We propose a
novel spam-text detection framework GCC-Spam, which integrates three core
innovations. First, a character similarity network captures orthographic and
phonetic features to counter character-obfuscation attacks and furthermore
produces sentence embeddings for downstream classification. Second, contrastive
learning enhances discriminability by optimizing the latent-space distance
between spam and normal texts. Third, a Generative Adversarial Network (GAN)
generates realistic pseudo-spam samples to alleviate data scarcity while
improving model robustness and classification accuracy. Extensive experiments
on real-world datasets demonstrate that our model outperforms baseline
approaches, achieving higher detection rates with significantly fewer labeled
examples.

</details>


### [198] [Spatial-Temporal Transformer with Curriculum Learning for EEG-Based Emotion Recognition](https://arxiv.org/abs/2507.14698)
*Xuetao Lin,Tianhao Peng,Peihong Dai,Yu Liang,Wenjun Wu*

Main category: cs.LG

TL;DR: 提出SST-CL，一个集成了空间-时间Transformer与课程学习的框架，用于脑电情绪识别，并在三个基准数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 基于脑电的情绪识别在开发自适应脑-计算机通信系统中起着重要作用，但在实际应用中面临两个根本挑战：(1)有效整合非平稳空间-时间神经模式，(2)稳健适应现实场景中动态的情绪强度变化。

Method: 该论文提出了一种新的框架SST-CL，该框架集成了空间-时间Transformer与课程学习。该方法引入了两个核心组件：一个空间编码器，用于建模通道间关系；一个时间编码器，通过窗口注意力机制捕获多尺度依赖关系，从而能够同时从脑电信号中提取空间相关性和时间动态。

Result: 在各种情绪强度水平上都表现出最先进的性能，消融研究证实了架构组件和课程学习机制的必要性。

Conclusion: 在三个基准数据集上的综合实验表明，该方法在各种情绪强度水平上都具有最先进的性能，消融研究证实了架构组件和课程学习机制的必要性。

Abstract: EEG-based emotion recognition plays an important role in developing adaptive
brain-computer communication systems, yet faces two fundamental challenges in
practical implementations: (1) effective integration of non-stationary
spatial-temporal neural patterns, (2) robust adaptation to dynamic emotional
intensity variations in real-world scenarios. This paper proposes SST-CL, a
novel framework integrating spatial-temporal transformers with curriculum
learning. Our method introduces two core components: a spatial encoder that
models inter-channel relationships and a temporal encoder that captures
multi-scale dependencies through windowed attention mechanisms, enabling
simultaneous extraction of spatial correlations and temporal dynamics from EEG
signals. Complementing this architecture, an intensity-aware curriculum
learning strategy progressively guides training from high-intensity to
low-intensity emotional states through dynamic sample scheduling based on a
dual difficulty assessment. Comprehensive experiments on three benchmark
datasets demonstrate state-of-the-art performance across various emotional
intensity levels, with ablation studies confirming the necessity of both
architectural components and the curriculum learning mechanism.

</details>


### [199] [Fraud is Not Just Rarity: A Causal Prototype Attention Approach to Realistic Synthetic Oversampling](https://arxiv.org/abs/2507.14706)
*Claudio Giusti,Luca Guarnera,Mirko Casu,Sebastiano Battiato*

Main category: cs.LG

TL;DR: This paper introduces Causal Prototype Attention Classifier (CPAC) to address the challenge of detecting fraudulent credit card transactions. CPAC improves latent space structure through prototype-based attention mechanisms and achieves superior performance.


<details>
  <summary>Details</summary>
Motivation: Detecting fraudulent credit card transactions remains a significant challenge, due to the extreme class imbalance in real-world data and the often subtle patterns that separate fraud from legitimate activity. Existing research commonly attempts to address this by generating synthetic samples for the minority class using approaches such as GANs, VAEs, or hybrid generative models. However, these techniques, particularly when applied only to minority-class data, tend to result in overconfident classifiers and poor latent cluster separation, ultimately limiting real-world detection performance.

Method: Causal Prototype Attention Classifier (CPAC), an interpretable architecture that promotes class-aware clustering and improved latent space structure through prototype-based attention mechanisms and couple it with the encoder in a VAE-GAN

Result: CPAC delivers superior performance, achieving an F1-score of 93.14% percent and recall of 90.18%, along with improved latent cluster separation

Conclusion: classifier-guided latent shaping with CPAC delivers superior performance, achieving an F1-score of 93.14% percent and recall of 90.18%, along with improved latent cluster separation

Abstract: Detecting fraudulent credit card transactions remains a significant
challenge, due to the extreme class imbalance in real-world data and the often
subtle patterns that separate fraud from legitimate activity. Existing research
commonly attempts to address this by generating synthetic samples for the
minority class using approaches such as GANs, VAEs, or hybrid generative
models. However, these techniques, particularly when applied only to
minority-class data, tend to result in overconfident classifiers and poor
latent cluster separation, ultimately limiting real-world detection
performance. In this study, we propose the Causal Prototype Attention
Classifier (CPAC), an interpretable architecture that promotes class-aware
clustering and improved latent space structure through prototype-based
attention mechanisms and we will couple it with the encoder in a VAE-GAN
allowing it to offer a better cluster separation moving beyond post-hoc sample
augmentation. We compared CPAC-augmented models to traditional oversamplers,
such as SMOTE, as well as to state-of-the-art generative models, both with and
without CPAC-based latent classifiers. Our results show that classifier-guided
latent shaping with CPAC delivers superior performance, achieving an F1-score
of 93.14\% percent and recall of 90.18\%, along with improved latent cluster
separation. Further ablation studies and visualizations provide deeper insight
into the benefits and limitations of classifier-driven representation learning
for fraud detection. The codebase for this work will be available at final
submission.

</details>


### [200] [Exploring the Dynamic Scheduling Space of Real-Time Generative AI Applications on Emerging Heterogeneous Systems](https://arxiv.org/abs/2507.14715)
*Rachid Karami,Rajeev Patwari,Hyoukjun Kwon,Ashish Sirasao*

Main category: cs.LG

TL;DR: RTGen workloads on heterogeneous SoCs require workload-aware, dynamic scheduling for high performance.


<details>
  <summary>Details</summary>
Motivation: The increasing integration of generative AI models into real-time multi-model AI applications creates a new class of workloads (RTGen) with stringent latency and concurrency constraints, making the scheduling space complexity and performance implications on heterogeneous SoC platforms underexplored.

Method: Comprehensive characterization of RTGen workloads on AMD's Ryzen AI heterogeneous SoC, evaluating five scheduling policies and their impact on real-time metrics and LLM performance.

Result: Scheduling decisions significantly affect workload performance, leading to a 41.7% difference in deadline violation rates on average, highlighting the need for scheduling strategies aware of workload dynamics and hardware heterogeneity.

Conclusion: Workload-aware, dynamic heterogeneous scheduling is crucial for enabling high-performance, on-device RTGen applications.

Abstract: The integration of generative AI models, particularly large language models
(LLMs), into real-time multi-model AI applications such as video conferencing
and gaming is giving rise to a new class of workloads: real-time generative AI
(RTGen). These workloads combine the compute intensity and dynamic execution
patterns of generative models with the stringent latency and concurrency
constraints of real-time inference. To meet the diverse demands of RTGen
workloads, modern edge platforms increasingly adopt heterogeneous
system-on-chip (SoC) architectures that integrate CPUs, GPUs, and NPUs. Despite
the potential of heterogeneous SoC, the scheduling space complexity and
performance implications of RTGen workloads on such platforms remain
underexplored. In this work, we perform a comprehensive characterization of
RTGen workloads on AMD's latest heterogeneous SoC, Ryzen AI. We construct
realistic multi-model scenarios inspired by industry use cases and profile
model performance across all available backends. Using this data, we evaluate
five scheduling policies and their impact on both real-time metrics (e.g.,
deadline violation rate) and LLM performance (e.g., time-to-first-token and
tokens-per-second). Our results show that scheduling decisions significantly
affect workload performance (e.g., leading to a 41.7% difference in deadline
violation rates on average), and highlight the need for scheduling strategies
that are aware of workload dynamics and hardware heterogeneity. Our findings
underscore the importance of workload-aware, dynamic heterogeneous scheduling
in enabling high-performance, on-device RTGen applications.

</details>
