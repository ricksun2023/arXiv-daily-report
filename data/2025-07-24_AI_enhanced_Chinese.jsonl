{"id": "2507.17215", "categories": ["cs.DB", "cs.DS", "cs.IR", "cs.SI"], "pdf": "https://arxiv.org/pdf/2507.17215", "abs": "https://arxiv.org/abs/2507.17215", "authors": ["Omkar Bhalerao", "Yunjie Pan", "C. Seshadhri", "Nishil Talati"], "title": "Triadic First-Order Logic Queries in Temporal Networks", "comment": null, "summary": "Motif counting is a fundamental problem in network analysis, and there is a\nrich literature of theoretical and applied algorithms for this problem. Given a\nlarge input network $G$, a motif $H$ is a small \"pattern\" graph indicative of\nspecial local structure. Motif/pattern mining involves finding all matches of\nthis pattern in the input $G$. The simplest, yet challenging, case of motif\ncounting is when $H$ has three vertices, often called a \"triadic\" query. Recent\nwork has focused on \"temporal graph mining\", where the network $G$ has edges\nwith timestamps (and directions) and $H$ has time constraints.\n  Inspired by concepts in logic and database theory, we introduce the study of\n\"thresholded First Order Logic (FOL) Motif Analysis\" for massive temporal\nnetworks. A typical triadic motif query asks for the existence of three\nvertices that form a desired temporal pattern. An \"FOL\" motif query is obtained\nby having both existential and thresholded universal quantifiers. This allows\nfor query semantics that can mine richer information from networks. A typical\ntriadic query would be \"find all triples of vertices $u,v,w$ such that they\nform a triangle within one hour\". A thresholded FOL query can express \"find all\npairs $u,v$ such that for half of $w$ where $(u,w)$ formed an edge, $(v,w)$\nalso formed an edge within an hour\".\n  We design the first algorithm, FOLTY, for mining thresholded FOL triadic\nqueries. The theoretical running time of FOLTY matches the best known running\ntime for temporal triangle counting in sparse graphs. We give an efficient\nimplementation of FOLTY using specialized temporal data structures. FOLTY has\nexcellent empirical behavior, and can answer triadic FOL queries on graphs with\nnearly 70M edges is less than hour on commodity hardware. Our work has the\npotential to start a new research direction in the classic well-studied problem\nof motif analysis.", "AI": {"tldr": "Introduces FOLTY, an efficient algorithm for mining thresholded FOL triadic queries in temporal networks, opening a new research direction in motif analysis.", "motivation": "Motif counting in temporal graphs is a challenging problem, and FOL queries can mine richer information from networks compared to typical triadic queries.", "method": "The algorithm FOLTY uses specialized temporal data structures for efficient implementation.", "result": "FOLTY achieves running time matching the best known for temporal triangle counting and can process graphs with nearly 70M edges in less than an hour.", "conclusion": "The work introduces FOLTY, an algorithm for mining thresholded FOL triadic queries, demonstrating excellent empirical performance on large graphs."}}
{"id": "2507.17507", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.17507", "abs": "https://arxiv.org/abs/2507.17507", "authors": ["Vasileios Papastergios", "Lisa Ehrlinger", "Anastasios Gounaris"], "title": "Unfolding Data Quality Dimensions in Practice: A Survey", "comment": null, "summary": "Data quality describes the degree to which data meet specific requirements\nand are fit for use by humans and/or downstream tasks (e.g., artificial\nintelligence). Data quality can be assessed across multiple high-level concepts\ncalled dimensions, such as accuracy, completeness, consistency, or timeliness.\nWhile extensive research and several attempts for standardization (e.g.,\nISO/IEC 25012) exist for data quality dimensions, their practical application\noften remains unclear. In parallel to research endeavors, a large number of\ntools have been developed that implement functionalities for the detection and\nmitigation of specific data quality issues, such as missing values or outliers.\nWith this paper, we aim to bridge this gap between data quality theory and\npractice by systematically connecting low-level functionalities offered by data\nquality tools with high-level dimensions, revealing their many-to-many\nrelationships. Through an examination of seven open-source data quality tools,\nwe provide a comprehensive mapping between their functionalities and the data\nquality dimensions, demonstrating how individual functionalities and their\nvariants partially contribute to the assessment of single dimensions. This\nsystematic survey provides both practitioners and researchers with a unified\nview on the fragmented landscape of data quality checks, offering actionable\ninsights for quality assessment across multiple dimensions.", "AI": {"tldr": "\u5f25\u5408\u6570\u636e\u8d28\u91cf\u7406\u8bba\u548c\u5b9e\u8df5\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u901a\u8fc7\u7cfb\u7edf\u5730\u8fde\u63a5\u6570\u636e\u8d28\u91cf\u5de5\u5177\u63d0\u4f9b\u7684\u4f4e\u7ea7\u529f\u80fd\u4e0e\u9ad8\u7ea7\u7ef4\u5ea6\uff0c\u63ed\u793a\u5b83\u4eec\u7684\u591a\u5bf9\u591a\u5173\u7cfb\u3002", "motivation": "\u6570\u636e\u8d28\u91cf\u63cf\u8ff0\u4e86\u6570\u636e\u6ee1\u8db3\u7279\u5b9a\u9700\u6c42\u7684\u7a0b\u5ea6\uff0c\u4ee5\u53ca\u9002\u5408\u4eba\u7c7b\u548c/\u6216\u4e0b\u6e38\u4efb\u52a1\uff08\u4f8b\u5982\uff0c\u4eba\u5de5\u667a\u80fd\uff09\u4f7f\u7528\u7684\u7a0b\u5ea6\u3002\u6570\u636e\u8d28\u91cf\u53ef\u4ee5\u901a\u8fc7\u591a\u4e2a\u9ad8\u7ea7\u6982\u5ff5\uff08\u79f0\u4e3a\u7ef4\u5ea6\uff09\u8fdb\u884c\u8bc4\u4f30\uff0c\u4f8b\u5982\u51c6\u786e\u6027\u3001\u5b8c\u6574\u6027\u3001\u4e00\u81f4\u6027\u6216\u53ca\u65f6\u6027\u3002\u867d\u7136\u5bf9\u4e8e\u6570\u636e\u8d28\u91cf\u7ef4\u5ea6\u5b58\u5728\u5e7f\u6cdb\u7684\u7814\u7a76\u548c\u4e00\u4e9b\u6807\u51c6\u5316\u5c1d\u8bd5\uff08\u4f8b\u5982\uff0cISO/IEC 25012\uff09\uff0c\u4f46\u5b83\u4eec\u7684\u5b9e\u9645\u5e94\u7528\u901a\u5e38\u4ecd\u4e0d\u6e05\u695a\u3002\u4e0e\u7814\u7a76\u5de5\u4f5c\u5e76\u884c\uff0c\u5df2\u7ecf\u5f00\u53d1\u4e86\u5927\u91cf\u5de5\u5177\uff0c\u8fd9\u4e9b\u5de5\u5177\u5b9e\u73b0\u4e86\u68c0\u6d4b\u548c\u7f13\u89e3\u7279\u5b9a\u6570\u636e\u8d28\u91cf\u95ee\u9898\uff08\u4f8b\u5982\uff0c\u7f3a\u5931\u503c\u6216\u5f02\u5e38\u503c\uff09\u7684\u529f\u80fd\u3002", "method": "\u901a\u8fc7\u68c0\u67e5\u4e03\u4e2a\u5f00\u6e90\u6570\u636e\u8d28\u91cf\u5de5\u5177\uff0c\u7cfb\u7edf\u5730\u8fde\u63a5\u6570\u636e\u8d28\u91cf\u5de5\u5177\u63d0\u4f9b\u7684\u4f4e\u7ea7\u529f\u80fd\u4e0e\u9ad8\u7ea7\u7ef4\u5ea6\uff0c\u63ed\u793a\u5b83\u4eec\u7684\u591a\u5bf9\u591a\u5173\u7cfb\u3002", "result": "\u63ed\u793a\u6570\u636e\u8d28\u91cf\u5de5\u5177\u63d0\u4f9b\u7684\u4f4e\u7ea7\u529f\u80fd\u4e0e\u9ad8\u7ea7\u7ef4\u5ea6\u4e4b\u95f4\u7684\u591a\u5bf9\u591a\u5173\u7cfb", "conclusion": "\u901a\u8fc7\u68c0\u67e5\u4e03\u4e2a\u5f00\u6e90\u6570\u636e\u8d28\u91cf\u5de5\u5177\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u5b83\u4eec\u7684\u529f\u80fd\u548c\u6570\u636e\u8d28\u91cf\u7ef4\u5ea6\u4e4b\u95f4\u7684\u5168\u9762\u6620\u5c04\uff0c\u5c55\u793a\u4e86\u5404\u4e2a\u529f\u80fd\u53ca\u5176\u53d8\u4f53\u5982\u4f55\u90e8\u5206\u5730\u4fc3\u8fdb\u5355\u4e2a\u7ef4\u5ea6\u7684\u8bc4\u4f30\u3002\u8fd9\u4e2a\u7cfb\u7edf\u6027\u7684\u8c03\u67e5\u4e3a\u4ece\u4e1a\u8005\u548c\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5173\u4e8e\u6570\u636e\u8d28\u91cf\u68c0\u67e5\u788e\u7247\u5316\u666f\u8c61\u7684\u7edf\u4e00\u89c6\u56fe\uff0c\u4e3a\u8de8\u591a\u4e2a\u7ef4\u5ea6\u7684\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002"}}
{"id": "2507.17647", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.17647", "abs": "https://arxiv.org/abs/2507.17647", "authors": ["Manuel Widmoser", "Daniel Kocher", "Nikolaus Augsten"], "title": "SHINE: A Scalable HNSW Index in Disaggregated Memory", "comment": null, "summary": "Approximate nearest neighbor (ANN) search is a fundamental problem in\ncomputer science for which in-memory graph-based methods, such as Hierarchical\nNavigable Small World (HNSW), perform exceptionally well. To scale beyond\nbillions of high-dimensional vectors, the index must be distributed. The\ndisaggregated memory architecture physically separates compute and memory into\ntwo distinct hardware units and has become popular in modern data centers. Both\nunits are connected via RDMA networks that allow compute nodes to directly\naccess remote memory and perform all the computations, posing unique challenges\nfor disaggregated indexes.\n  In this work, we propose a scalable HNSW index for ANN search in\ndisaggregated memory. In contrast to existing distributed approaches, which\npartition the graph at the cost of accuracy, our method builds a\ngraph-preserving index that reaches the same accuracy as a single-machine HNSW.\nContinuously fetching high-dimensional vector data from remote memory leads to\nsevere network bandwidth limitations, which we overcome by employing an\nefficient caching mechanism. Since answering a single query involves processing\nnumerous unique graph nodes, caching alone is not sufficient to achieve high\nscalability. We logically combine the caches of the compute nodes to increase\nthe overall cache effectiveness and confirm the efficiency and scalability of\nour method in our evaluation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5206\u79bb\u5185\u5b58\u7684\u53ef\u6269\u5c55HNSW\u7d22\u5f15\uff0c\u901a\u8fc7\u56fe\u7ed3\u6784\u4fdd\u6301\u3001\u7f13\u5b58\u673a\u5236\u548c\u7f13\u5b58\u7ec4\u5408\u6765\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4e3a\u4e86\u6269\u5c55\u5230\u6570\u5341\u4ebf\u7684\u9ad8\u7ef4\u5411\u91cf\uff0c\u7d22\u5f15\u5fc5\u987b\u662f\u5206\u5e03\u5f0f\u7684\u3002\u5206\u79bb\u5185\u5b58\u67b6\u6784\u5c06\u8ba1\u7b97\u548c\u5185\u5b58\u7269\u7406\u5730\u5206\u79bb\u5230\u4e24\u4e2a\u4e0d\u540c\u7684\u786c\u4ef6\u5355\u5143\u4e2d\uff0c\u5e76\u4e14\u5728\u73b0\u4ee3\u6570\u636e\u4e2d\u5fc3\u4e2d\u53d8\u5f97\u6d41\u884c\u3002\u8ba1\u7b97\u8282\u70b9\u901a\u8fc7RDMA\u7f51\u7edc\u76f4\u63a5\u8bbf\u95ee\u8fdc\u7a0b\u5185\u5b58\u5e76\u6267\u884c\u6240\u6709\u8ba1\u7b97\uff0c\u8fd9\u7ed9\u5206\u79bb\u7d22\u5f15\u5e26\u6765\u4e86\u72ec\u7279\u7684\u6311\u6218\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u4fdd\u7559\u56fe\u7ed3\u6784\u7684\u7d22\u5f15\uff0c\u8fbe\u5230\u4e0e\u5355\u673aHNSW\u76f8\u540c\u7684\u7cbe\u5ea6\u3002\u91c7\u7528\u9ad8\u6548\u7684\u7f13\u5b58\u673a\u5236\uff0c\u5e76\u903b\u8f91\u4e0a\u7ed3\u5408\u8ba1\u7b97\u8282\u70b9\u7684\u7f13\u5b58\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u8bc4\u4f30\u4e2d\u8bc1\u5b9e\u4e86\u5176\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684HNSW\u7d22\u5f15\uff0c\u7528\u4e8e\u5728\u5206\u79bb\u5185\u5b58\u4e2d\u8fdb\u884cANN\u641c\u7d22\u3002\u901a\u8fc7\u7f13\u5b58\u673a\u5236\u548c\u903b\u8f91\u4e0a\u7ec4\u5408\u8ba1\u7b97\u8282\u70b9\u7684\u7f13\u5b58\u6765\u63d0\u9ad8\u7f13\u5b58\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2507.17241", "categories": ["cs.LG", "cs.AI", "cs.DB", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.17241", "abs": "https://arxiv.org/abs/2507.17241", "authors": ["Mattia Sabella", "Monica Vitali"], "title": "Eco-Friendly AI: Unleashing Data Power for Green Federated Learning", "comment": null, "summary": "The widespread adoption of Artificial Intelligence (AI) and Machine Learning\n(ML) comes with a significant environmental impact, particularly in terms of\nenergy consumption and carbon emissions. This pressing issue highlights the\nneed for innovative solutions to mitigate AI's ecological footprint. One of the\nkey factors influencing the energy consumption of ML model training is the size\nof the training dataset. ML models are often trained on vast amounts of data\ncontinuously generated by sensors and devices distributed across multiple\nlocations. To reduce data transmission costs and enhance privacy, Federated\nLearning (FL) enables model training without the need to move or share raw\ndata. While FL offers these advantages, it also introduces challenges due to\nthe heterogeneity of data sources (related to volume and quality),\ncomputational node capabilities, and environmental impact.\n  This paper contributes to the advancement of Green AI by proposing a\ndata-centric approach to Green Federated Learning. Specifically, we focus on\nreducing FL's environmental impact by minimizing the volume of training data.\nOur methodology involves the analysis of the characteristics of federated\ndatasets, the selecting of an optimal subset of data based on quality metrics,\nand the choice of the federated nodes with the lowest environmental impact. We\ndevelop a comprehensive methodology that examines the influence of data-centric\nfactors, such as data quality and volume, on FL training performance and carbon\nemissions. Building on these insights, we introduce an interactive\nrecommendation system that optimizes FL configurations through data reduction,\nminimizing environmental impact during training. Applying this methodology to\ntime series classification has demonstrated promising results in reducing the\nenvironmental impact of FL tasks.", "AI": {"tldr": "This paper introduces a data-centric approach to Green Federated Learning, reducing training data volume to minimize the environmental impact of FL tasks, with promising results in time series classification.", "motivation": "The widespread adoption of AI and ML has a significant environmental impact in terms of energy consumption and carbon emissions. Reducing data transmission costs and enhancing privacy are important challenges.", "method": "The paper proposes a data-centric approach to Green Federated Learning, focusing on reducing the volume of training data by analyzing federated datasets, selecting an optimal subset of data based on quality metrics, and choosing federated nodes with the lowest environmental impact. An interactive recommendation system is introduced to optimize FL configurations through data reduction.", "result": "The methodology examines the influence of data-centric factors, such as data quality and volume, on FL training performance and carbon emissions. The interactive recommendation system optimizes FL configurations through data reduction, minimizing environmental impact during training.", "conclusion": "The paper demonstrates promising results in reducing the environmental impact of FL tasks by applying the proposed methodology to time series classification."}}
{"id": "2507.16826", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.16826", "abs": "https://arxiv.org/abs/2507.16826", "authors": ["Qikai Wei", "Huansheng Ning", "Chunlong Han", "Jianguo Ding"], "title": "A Query-Aware Multi-Path Knowledge Graph Fusion Approach for Enhancing Retrieval-Augmented Generation in Large Language Models", "comment": null, "summary": "Retrieval Augmented Generation (RAG) has gradually emerged as a promising\nparadigm for enhancing the accuracy and factual consistency of content\ngenerated by large language models (LLMs). However, existing RAG studies\nprimarily focus on retrieving isolated segments using similarity-based matching\nmethods, while overlooking the intrinsic connections between them. This\nlimitation hampers performance in RAG tasks. To address this, we propose QMKGF,\na Query-Aware Multi-Path Knowledge Graph Fusion Approach for Enhancing\nRetrieval Augmented Generation. First, we design prompt templates and employ\ngeneral-purpose LLMs to extract entities and relations, thereby generating a\nknowledge graph (KG) efficiently. Based on the constructed KG, we introduce a\nmulti-path subgraph construction strategy that incorporates one-hop relations,\nmulti-hop relations, and importance-based relations, aiming to improve the\nsemantic relevance between the retrieved documents and the user query.\nSubsequently, we designed a query-aware attention reward model that scores\nsubgraph triples based on their semantic relevance to the query. Then, we\nselect the highest score subgraph and enrich subgraph with additional triples\nfrom other subgraphs that are highly semantically relevant to the query.\nFinally, the entities, relations, and triples within the updated subgraph are\nutilised to expand the original query, thereby enhancing its semantic\nrepresentation and improving the quality of LLMs' generation. We evaluate QMKGF\non the SQuAD, IIRC, Culture, HotpotQA, and MuSiQue datasets. On the HotpotQA\ndataset, our method achieves a ROUGE-1 score of 64.98\\%, surpassing the\nBGE-Rerank approach by 9.72 percentage points (from 55.26\\% to 64.98\\%).\nExperimental results demonstrate the effectiveness and superiority of the QMKGF\napproach.", "AI": {"tldr": "This paper introduces QMKGF, a Query-Aware Multi-Path Knowledge Graph Fusion approach, to improve Retrieval Augmented Generation by considering the connections between retrieved segments. It uses a knowledge graph and query-aware attention to enhance semantic representation and improve the quality of LLMs' generation.", "motivation": "Existing RAG studies primarily focus on retrieving isolated segments using similarity-based matching methods, while overlooking the intrinsic connections between them, which hampers performance in RAG tasks.", "method": "The paper proposes QMKGF, a Query-Aware Multi-Path Knowledge Graph Fusion Approach. It constructs a knowledge graph (KG) using prompt templates and LLMs, introduces a multi-path subgraph construction strategy, designs a query-aware attention reward model to score subgraph triples, and enriches the subgraph with additional triples. Finally, it expands the original query using the updated subgraph to enhance its semantic representation.", "result": "QMKGF achieves a ROUGE-1 score of 64.98% on the HotpotQA dataset, surpassing the BGE-Rerank approach by 9.72 percentage points.", "conclusion": "The experimental results on SQuAD, IIRC, Culture, HotpotQA, and MuSiQue datasets demonstrate the effectiveness and superiority of the QMKGF approach, which achieves a ROUGE-1 score of 64.98% on the HotpotQA dataset, surpassing the BGE-Rerank approach by 9.72 percentage points."}}
{"id": "2507.16818", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.16818", "abs": "https://arxiv.org/abs/2507.16818", "authors": ["C. H. E. Jordaan", "M. van der Stelt", "T. J. J. Maal", "V. M. A. Stirler", "R. Leijendekkers", "T. Kachman", "G. A. de Jong"], "title": "Evaluating Artificial Intelligence Algorithms for the Standardization of Transtibial Prosthetic Socket Shape Design", "comment": null, "summary": "The quality of a transtibial prosthetic socket depends on the prosthetist's\nskills and expertise, as the fitting is performed manually. This study\ninvestigates multiple artificial intelligence (AI) approaches to help\nstandardize transtibial prosthetic socket design. Data from 118 patients were\ncollected by prosthetists working in the Dutch healthcare system. This data\nconsists of a three-dimensional (3D) scan of the residual limb and a\ncorresponding 3D model of the prosthetist-designed socket. Multiple data\npre-processing steps are performed for alignment, standardization and\noptionally compression using Morphable Models and Principal Component Analysis.\nAfterward, three different algorithms - a 3D neural network, Feedforward neural\nnetwork, and random forest - are developed to either predict 1) the final\nsocket shape or 2) the adaptations performed by a prosthetist to predict the\nsocket shape based on the 3D scan of the residual limb. Each algorithm's\nperformance was evaluated by comparing the prosthetist-designed socket with the\nAI-generated socket, using two metrics in combination with the error location.\nFirst, we measure the surface-to-surface distance to assess the overall surface\nerror between the AI-generated socket and the prosthetist-designed socket.\nSecond, distance maps between the AI-generated and prosthetist sockets are\nutilized to analyze the error's location. For all algorithms, estimating the\nrequired adaptations outperformed direct prediction of the final socket shape.\nThe random forest model applied to adaptation prediction yields the lowest\nerror with a median surface-to-surface distance of 1.24 millimeters, a first\nquartile of 1.03 millimeters, and a third quartile of 1.54 millimeters.", "AI": {"tldr": "This study uses AI to standardize transtibial prosthetic socket design, finding that predicting adaptations to the socket shape is more effective than predicting the final shape directly, with a random forest model achieving the best results.", "motivation": "The quality of a transtibial prosthetic socket depends on the prosthetist's skills and expertise, as the fitting is performed manually. This study investigates multiple artificial intelligence (AI) approaches to help standardize transtibial prosthetic socket design.", "method": "multiple artificial intelligence (AI) approaches", "result": "For all algorithms, estimating the required adaptations outperformed direct prediction of the final socket shape. The random forest model applied to adaptation prediction yields the lowest error with a median surface-to-surface distance of 1.24 millimeters, a first quartile of 1.03 millimeters, and a third quartile of 1.54 millimeters.", "conclusion": "Estimating the required adaptations outperformed direct prediction of the final socket shape. The random forest model applied to adaptation prediction yields the lowest error with a median surface-to-surface distance of 1.24 millimeters, a first quartile of 1.03 millimeters, and a third quartile of 1.54 millimeters."}}
{"id": "2507.16922", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.16922", "abs": "https://arxiv.org/abs/2507.16922", "authors": ["Shmuel Amar", "Ori Shapira", "Aviv Slobodkin", "Ido Dagan"], "title": "A Unifying Scheme for Extractive Content Selection Tasks", "comment": null, "summary": "A broad range of NLP tasks involve selecting relevant text spans from given\nsource texts. Despite this shared objective, such \\textit{content selection}\ntasks have traditionally been studied in isolation, each with its own modeling\napproaches, datasets, and evaluation metrics. In this work, we propose\n\\textit{instruction-guided content selection (IGCS)} as a beneficial unified\nframework for such settings, where the task definition and any\ninstance-specific request are encapsulated as instructions to a language model.\nTo promote this framework, we introduce \\igcsbench{}, the first unified\nbenchmark covering diverse content selection tasks. Further, we create a large\ngeneric synthetic dataset that can be leveraged for diverse content selection\ntasks, and show that transfer learning with these datasets often boosts\nperformance, whether dedicated training for the targeted task is available or\nnot. Finally, we address generic inference time issues that arise in LLM-based\nmodeling of content selection, assess a generic evaluation metric, and overall\npropose the utility of our resources and methods for future content selection\nmodels. Models and datasets available at https://github.com/shmuelamar/igcs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684instruction-guided content selection\u6846\u67b6\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u6570\u636e\u96c6\u6765\u4fc3\u8fdb\u8be5\u6846\u67b6\u7684\u4f7f\u7528\u3002", "motivation": "\u73b0\u6709\u7684NLP\u5185\u5bb9\u9009\u62e9\u4efb\u52a1\u7814\u7a76\u5b64\u7acb\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u5efa\u6a21\u65b9\u6cd5\u3001\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86instruction-guided content selection (IGCS) \u6846\u67b6\uff0c\u4f7f\u7528instruction\u6765\u6307\u5bfc\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5185\u5bb9\u9009\u62e9\u3002", "result": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u6846\u67b6\u548c\u6570\u636e\u96c6\u80fd\u591f\u63d0\u5347\u5185\u5bb9\u9009\u62e9\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5373\u4f7f\u5728\u6ca1\u6709\u76ee\u6807\u4efb\u52a1\u7684\u4e13\u7528\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86instruction-guided content selection (IGCS) \u6846\u67b6\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u7edf\u4e00\u5185\u5bb9\u9009\u62e9\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002\u540c\u65f6\uff0c\u8bba\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u5927\u578b\u901a\u7528\u5408\u6210\u6570\u636e\u96c6\uff0c\u5e76\u89e3\u51b3\u4e86\u901a\u7528\u63a8\u7406\u65f6\u95f4\u95ee\u9898\u3002"}}
{"id": "2507.17012", "categories": ["cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2507.17012", "abs": "https://arxiv.org/abs/2507.17012", "authors": ["Zhihan Zhang", "Alexander Metzger", "Yuxuan Mei", "Felix H\u00e4hnlein", "Zachary Englhardt", "Tingyu Cheng", "Gregory D. Abowd", "Shwetak Patel", "Adriana Schulz", "Vikram Iyer"], "title": "Towards Autonomous Sustainability Assessment via Multimodal AI Agents", "comment": null, "summary": "Interest in sustainability information has surged in recent years. However,\nthe data required for a life cycle assessment (LCA) that maps the materials and\nprocesses from product manufacturing to disposal into environmental impacts\n(EI) are often unavailable. Here we reimagine conventional LCA by introducing\nmultimodal AI agents that emulate interactions between LCA experts and\nstakeholders like product managers and engineers to calculate the\ncradle-to-gate (production) carbon emissions of electronic devices. The AI\nagents iteratively generate a detailed life-cycle inventory leveraging a custom\ndata abstraction and software tools that extract information from online text\nand images from repair communities and government certifications. This approach\nreduces weeks or months of expert time to under one minute and closes data\navailability gaps while yielding carbon footprint estimates within 19% of\nexpert LCAs with zero proprietary data. Additionally, we develop a method to\ndirectly estimate EI by comparing an input to a cluster of products with\nsimilar descriptions and known carbon footprints. This runs in 3 ms on a laptop\nwith a MAPE of 12.28% on electronic products. Further, we develop a data-driven\nmethod to generate emission factors. We use the properties of an unknown\nmaterial to represent it as a weighted sum of emission factors for similar\nmaterials. Compared to human experts picking the closest LCA database entry,\nthis improves MAPE by 120.26%. We analyze the data and compute scaling of this\napproach and discuss its implications for future LCA workflows.", "AI": {"tldr": "\u6211\u4eec\u4f7f\u7528\u591a\u6a21\u6001 AI \u4ee3\u7406\u6765\u6a21\u62df LCA \u4e13\u5bb6\u4e0e\u5229\u76ca\u76f8\u5173\u8005\u4e4b\u95f4\u7684\u4ea4\u4e92\uff0c\u4ee5\u8ba1\u7b97\u7535\u5b50\u8bbe\u5907\u7684\u78b3\u6392\u653e\u91cf\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0c\u4eba\u4eec\u5bf9\u53ef\u6301\u7eed\u6027\u4fe1\u606f\u7684\u5174\u8da3\u731b\u589e\u3002\u7136\u800c\uff0c\u5c06\u4ea7\u54c1\u5236\u9020\u5230\u5904\u7f6e\u7684\u73af\u5883\u5f71\u54cd (EI) \u7684\u6750\u6599\u548c\u6d41\u7a0b\u6620\u5c04\u5230\u7684\u751f\u547d\u5468\u671f\u8bc4\u4f30 (LCA) \u6240\u9700\u7684\u6570\u636e\u901a\u5e38\u4e0d\u53ef\u7528\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u591a\u6a21\u6001 AI \u4ee3\u7406\uff0c\u8fd9\u4e9b\u4ee3\u7406\u6a21\u62df LCA \u4e13\u5bb6\u4e0e\u4ea7\u54c1\u7ecf\u7406\u548c\u5de5\u7a0b\u5e08\u7b49\u5229\u76ca\u76f8\u5173\u8005\u4e4b\u95f4\u7684\u4ea4\u4e92\uff0c\u4ee5\u8ba1\u7b97\u7535\u5b50\u8bbe\u5907\u7684\u4ece\u6447\u7bee\u5230\u5927\u95e8\uff08\u751f\u4ea7\uff09\u7684\u78b3\u6392\u653e\u3002", "result": "\u8be5\u65b9\u6cd5\u5c06\u4e13\u5bb6\u7684\u65f6\u95f4\u4ece\u6570\u5468\u6216\u6570\u6708\u51cf\u5c11\u5230\u4e00\u5206\u949f\u4ee5\u4e0b\uff0c\u5e76\u7f29\u5c0f\u4e86\u6570\u636e\u53ef\u7528\u6027\u5dee\u8ddd\uff0c\u540c\u65f6\u5728\u96f6\u4e13\u6709\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u4ea7\u751f\u7684\u78b3\u8db3\u8ff9\u4f30\u7b97\u503c\u5728\u4e13\u5bb6 LCA \u7684 19% \u4ee5\u5185\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u901a\u8fc7\u5c06\u8f93\u5165\u4e0e\u5177\u6709\u76f8\u4f3c\u63cf\u8ff0\u548c\u5df2\u77e5\u78b3\u8db3\u8ff9\u7684\u4ea7\u54c1\u96c6\u7fa4\u8fdb\u884c\u6bd4\u8f83\u6765\u76f4\u63a5\u4f30\u8ba1 EI \u7684\u65b9\u6cd5\u3002\u8fd9\u5728\u7b14\u8bb0\u672c\u7535\u8111\u4e0a\u4ee5 3 \u6beb\u79d2\u7684\u901f\u5ea6\u8fd0\u884c\uff0c\u7535\u5b50\u4ea7\u54c1\u7684 MAPE \u4e3a 12.28%\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u6765\u751f\u6210\u6392\u653e\u56e0\u5b50\u3002\u6211\u4eec\u4f7f\u7528\u672a\u77e5\u6750\u6599\u7684\u5c5e\u6027\u5c06\u5176\u8868\u793a\u4e3a\u76f8\u4f3c\u6750\u6599\u7684\u6392\u653e\u56e0\u5b50\u7684\u52a0\u6743\u603b\u548c\u3002\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u9009\u62e9\u6700\u63a5\u8fd1\u7684 LCA \u6570\u636e\u5e93\u6761\u76ee\u76f8\u6bd4\uff0c\u8fd9\u63d0\u9ad8\u4e86 MAPE 120.26%\u3002", "conclusion": "\u6211\u4eec\u5206\u6790\u4e86\u6570\u636e\u5e76\u8ba1\u7b97\u4e86\u8fd9\u79cd\u65b9\u6cd5\u7684\u6269\u5c55\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u5bf9\u672a\u6765 LCA \u5de5\u4f5c\u6d41\u7a0b\u7684\u5f71\u54cd\u3002"}}
{"id": "2507.16849", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16849", "abs": "https://arxiv.org/abs/2507.16849", "authors": ["Yi-Shan Chu", "Hsuan-Cheng Wei"], "title": "Post-Disaster Affected Area Segmentation with a Vision Transformer (ViT)-based EVAP Model using Sentinel-2 and Formosat-5 Imagery", "comment": null, "summary": "We propose a vision transformer (ViT)-based deep learning framework to refine\ndisaster-affected area segmentation from remote sensing imagery, aiming to\nsupport and enhance the Emergent Value Added Product (EVAP) developed by the\nTaiwan Space Agency (TASA). The process starts with a small set of manually\nannotated regions. We then apply principal component analysis (PCA)-based\nfeature space analysis and construct a confidence index (CI) to expand these\nlabels, producing a weakly supervised training set. These expanded labels are\nthen used to train ViT-based encoder-decoder models with multi-band inputs from\nSentinel-2 and Formosat-5 imagery. Our architecture supports multiple decoder\nvariants and multi-stage loss strategies to improve performance under limited\nsupervision. During the evaluation, model predictions are compared with\nhigher-resolution EVAP output to assess spatial coherence and segmentation\nconsistency. Case studies on the 2022 Poyang Lake drought and the 2023 Rhodes\nwildfire demonstrate that our framework improves the smoothness and reliability\nof segmentation results, offering a scalable approach for disaster mapping when\naccurate ground truth is unavailable.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eViT\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u707e\u5bb3\u5f71\u54cd\u533a\u57df\u7684\u5206\u5272\uff0c\u4ece\u800c\u4e3a\u707e\u5bb3\u6d4b\u7ed8\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u3002", "motivation": "\u65e8\u5728\u652f\u6301\u548c\u589e\u5f3a\u53f0\u6e7e\u592a\u7a7a\u4e2d\u5fc3 (TASA) \u5f00\u53d1\u7684\u7a81\u53d1\u589e\u503c\u4ea7\u54c1 (EVAP)\uff0c\u4ee5\u6539\u8fdb\u9065\u611f\u56fe\u50cf\u4e2d\u53d7\u707e\u5bb3\u5f71\u54cd\u533a\u57df\u7684\u5206\u5272\u3002", "method": "\u57fa\u4e8e\u89c6\u89c9Transformer (ViT) \u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6", "result": "\u8be5\u6846\u67b6\u63d0\u9ad8\u4e86\u5206\u5272\u7ed3\u679c\u7684\u5e73\u6ed1\u6027\u548c\u53ef\u9760\u6027", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u6539\u8fdb\u5206\u5272\u7ed3\u679c\u7684\u5e73\u6ed1\u6027\u548c\u53ef\u9760\u6027\uff0c\u4e3a\u5728\u7f3a\u4e4f\u51c6\u786e\u5730\u9762\u5b9e\u51b5\u65f6\u8fdb\u884c\u707e\u5bb3\u6d4b\u7ed8\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.17487", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2507.17487", "abs": "https://arxiv.org/abs/2507.17487", "authors": ["Lorenzo Marconi", "Flavia Ricci", "Riccardo Rosati"], "title": "CQE under Epistemic Dependencies: Algorithms and Experiments (extended version)", "comment": "Extended version of paper accepted at the 24th International Semantic\n  Web Conference (ISWC 2025)", "summary": "We investigate Controlled Query Evaluation (CQE) over ontologies, where\ninformation disclosure is regulated by epistemic dependencies (EDs), a family\nof logical rules recently proposed for the CQE framework. In particular, we\ncombine EDs with the notion of optimal GA censors, i.e. maximal sets of ground\natoms that are entailed by the ontology and can be safely revealed. We focus on\nanswering Boolean unions of conjunctive queries (BUCQs) with respect to the\nintersection of all optimal GA censors - an approach that has been shown in\nother contexts to ensure strong security guarantees with favorable\ncomputational behavior. First, we characterize the security of this\nintersection-based approach and identify a class of EDs (namely, full EDs) for\nwhich it remains safe. Then, for a subclass of EDs and for DL-Lite_R\nontologies, we show that answering BUCQs in the above CQE semantics is in AC^0\nin data complexity by presenting a suitable, detailed first-order rewriting\nalgorithm. Finally, we report on experiments conducted in two different\nevaluation scenarios, showing the practical feasibility of our rewriting\nfunction.", "AI": {"tldr": "This paper focuses on answering Boolean unions of conjunctive queries (BUCQs) with respect to the intersection of all optimal GA censors", "motivation": "investigate Controlled Query Evaluation (CQE) over ontologies, where information disclosure is regulated by epistemic dependencies (EDs)", "method": "combine EDs with the notion of optimal GA censors", "result": "characterize the security of this intersection-based approach and identify a class of EDs (namely, full EDs) for which it remains safe", "conclusion": "answering BUCQs in the above CQE semantics is in AC^0 in data complexity by presenting a suitable, detailed first-order rewriting algorithm and experiments conducted in two different evaluation scenarios, showing the practical feasibility of our rewriting function."}}
{"id": "2507.16829", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16829", "abs": "https://arxiv.org/abs/2507.16829", "authors": ["Giovanni De Toni", "Erasmo Purificato", "Emilia G\u00f3mez", "Bruno Lepri", "Andrea Passerini", "Cristian Consonni"], "title": "You Don't Bring Me Flowers: Mitigating Unwanted Recommendations Through Conformal Risk Control", "comment": "Accepted at the 19th ACM Conference on Recommender Systems (RecSys\n  2025)", "summary": "Recommenders are significantly shaping online information consumption. While\neffective at personalizing content, these systems increasingly face criticism\nfor propagating irrelevant, unwanted, and even harmful recommendations. Such\ncontent degrades user satisfaction and contributes to significant societal\nissues, including misinformation, radicalization, and erosion of user trust.\nAlthough platforms offer mechanisms to mitigate exposure to undesired content,\nthese mechanisms are often insufficiently effective and slow to adapt to users'\nfeedback. This paper introduces an intuitive, model-agnostic, and\ndistribution-free method that uses conformal risk control to provably bound\nunwanted content in personalized recommendations by leveraging simple binary\nfeedback on items. We also address a limitation of traditional conformal risk\ncontrol approaches, i.e., the fact that the recommender can provide a smaller\nset of recommended items, by leveraging implicit feedback on consumed items to\nexpand the recommendation set while ensuring robust risk mitigation. Our\nexperimental evaluation on data coming from a popular online video-sharing\nplatform demonstrates that our approach ensures an effective and controllable\nreduction of unwanted recommendations with minimal effort. The source code is\navailable here: https://github.com/geektoni/mitigating-harm-recsys.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a8\u8350\u65b9\u6cd5\uff0c\u53ef\u4ee5\u51cf\u5c11\u7528\u6237\u6536\u5230\u7684\u4e0d\u826f\u63a8\u8350\u3002", "motivation": "\u63a8\u8350\u7cfb\u7edf\u5728\u5851\u9020\u5728\u7ebf\u4fe1\u606f\u6d88\u8d39\u65b9\u9762\u53d1\u6325\u7740\u91cd\u8981\u4f5c\u7528\u3002\u867d\u7136\u5728\u4e2a\u6027\u5316\u5185\u5bb9\u65b9\u9762\u6709\u6548\uff0c\u4f46\u8fd9\u4e9b\u7cfb\u7edf\u8d8a\u6765\u8d8a\u591a\u5730\u56e0\u4f20\u64ad\u4e0d\u76f8\u5173\u3001\u4e0d\u9700\u8981\u751a\u81f3\u6709\u5bb3\u7684\u63a8\u8350\u800c\u53d7\u5230\u6279\u8bc4\u3002\u6b64\u7c7b\u5185\u5bb9\u4f1a\u964d\u4f4e\u7528\u6237\u6ee1\u610f\u5ea6\uff0c\u5e76\u5bfc\u81f4\u4e25\u91cd\u7684\u793e\u4f1a\u95ee\u9898\uff0c\u5305\u62ec\u9519\u8bef\u4fe1\u606f\u3001\u6fc0\u8fdb\u5316\u548c\u7528\u6237\u4fe1\u4efb\u5ea6\u4e0b\u964d\u3002", "method": "\u8be5\u65b9\u6cd5\u662f\u76f4\u89c2\u7684\u3001\u6a21\u578b\u65e0\u5173\u7684\u3001\u5206\u5e03\u81ea\u7531\u7684\uff0c\u5e76\u5229\u7528\u4e00\u81f4\u98ce\u9669\u63a7\u5236\u3002", "result": "\u5728\u6765\u81ea\u6d41\u884c\u7684\u5728\u7ebf\u89c6\u9891\u5171\u4eab\u5e73\u53f0\u7684\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u786e\u4fdd\u4e86\u4ee5\u6700\u5c0f\u7684\u52aa\u529b\u6709\u6548\u548c\u53ef\u63a7\u5730\u51cf\u5c11\u4e0d\u9700\u8981\u7684\u63a8\u8350\u3002", "conclusion": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u4e00\u81f4\u98ce\u9669\u63a7\u5236\u6765\u9650\u5236\u4e2a\u6027\u5316\u63a8\u8350\u4e2d\u4e0d\u9700\u8981\u7684\u5185\u5bb9\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5229\u7528\u5173\u4e8e\u9879\u76ee\u7684\u7b80\u5355\u4e8c\u5143\u53cd\u9988\u6765\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.16833", "categories": ["cs.LG", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2507.16833", "abs": "https://arxiv.org/abs/2507.16833", "authors": ["Qiuyu Shi", "Kangming Li", "Yao Fehlis", "Daniel Persaud", "Robert Black", "Jason Hattrick-Simpers"], "title": "Exploring the Frontiers of kNN Noisy Feature Detection and Recovery for Self-Driving Labs", "comment": "15 pages, 6 figures", "summary": "Self-driving laboratories (SDLs) have shown promise to accelerate materials\ndiscovery by integrating machine learning with automated experimental\nplatforms. However, errors in the capture of input parameters may corrupt the\nfeatures used to model system performance, compromising current and future\ncampaigns. This study develops an automated workflow to systematically detect\nnoisy features, determine sample-feature pairings that can be corrected, and\nfinally recover the correct feature values. A systematic study is then\nperformed to examine how dataset size, noise intensity, and feature value\ndistribution affect both the detectability and recoverability of noisy\nfeatures. In general, high-intensity noise and large training datasets are\nconducive to the detection and correction of noisy features. Low-intensity\nnoise reduces detection and recovery but can be compensated for by larger clean\ntraining data sets. Detection and correction results vary between features with\ncontinuous and dispersed feature distributions showing greater recoverability\ncompared to features with discrete or narrow distributions. This systematic\nstudy not only demonstrates a model agnostic framework for rational data\nrecovery in the presence of noise, limited data, and differing feature\ndistributions but also provides a tangible benchmark of kNN imputation in\nmaterials data sets. Ultimately, it aims to enhance data quality and\nexperimental precision in automated materials discovery.", "AI": {"tldr": "Developed an automated workflow to detect and correct noisy features in self-driving laboratories, showing that data quality can be enhanced through this method.", "motivation": "Errors in the capture of input parameters may corrupt the features used to model system performance, compromising current and future campaigns in self-driving laboratories (SDLs).", "method": "This study develops an automated workflow to systematically detect noisy features, determine sample-feature pairings that can be corrected, and finally recover the correct feature values. A systematic study is then performed to examine how dataset size, noise intensity, and feature value distribution affect both the detectability and recoverability of noisy features.", "result": "High-intensity noise and large training datasets are conducive to the detection and correction of noisy features. Low-intensity noise reduces detection and recovery but can be compensated for by larger clean training data sets. Detection and correction results vary between features with continuous and dispersed feature distributions showing greater recoverability compared to features with discrete or narrow distributions.", "conclusion": "This study demonstrates a model agnostic framework for rational data recovery in the presence of noise, limited data, and differing feature distributions and provides a tangible benchmark of kNN imputation in materials data sets. Ultimately, it aims to enhance data quality and experimental precision in automated materials discovery."}}
{"id": "2507.16947", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.16947", "abs": "https://arxiv.org/abs/2507.16947", "authors": ["Robert Korom", "Sarah Kiptinness", "Najib Adan", "Kassim Said", "Catherine Ithuli", "Oliver Rotich", "Boniface Kimani", "Irene King'ori", "Stellah Kamau", "Elizabeth Atemba", "Muna Aden", "Preston Bowman", "Michael Sharman", "Rebecca Soskin Hicks", "Rebecca Distler", "Johannes Heidecke", "Rahul K. Arora", "Karan Singhal"], "title": "AI-based Clinical Decision Support for Primary Care: A Real-World Study", "comment": "Blog: https://openai.com/index/ai-clinical-copilot-penda-health/", "summary": "We evaluate the impact of large language model-based clinical decision\nsupport in live care. In partnership with Penda Health, a network of primary\ncare clinics in Nairobi, Kenya, we studied AI Consult, a tool that serves as a\nsafety net for clinicians by identifying potential documentation and clinical\ndecision-making errors. AI Consult integrates into clinician workflows,\nactivating only when needed and preserving clinician autonomy. We conducted a\nquality improvement study, comparing outcomes for 39,849 patient visits\nperformed by clinicians with or without access to AI Consult across 15 clinics.\nVisits were rated by independent physicians to identify clinical errors.\nClinicians with access to AI Consult made relatively fewer errors: 16% fewer\ndiagnostic errors and 13% fewer treatment errors. In absolute terms, the\nintroduction of AI Consult would avert diagnostic errors in 22,000 visits and\ntreatment errors in 29,000 visits annually at Penda alone. In a survey of\nclinicians with AI Consult, all clinicians said that AI Consult improved the\nquality of care they delivered, with 75% saying the effect was \"substantial\".\nThese results required a clinical workflow-aligned AI Consult implementation\nand active deployment to encourage clinician uptake. We hope this study\ndemonstrates the potential for LLM-based clinical decision support tools to\nreduce errors in real-world settings and provides a practical framework for\nadvancing responsible adoption.", "AI": {"tldr": "AI Consult\u901a\u8fc7\u8bc6\u522b\u6f5c\u5728\u7684\u6587\u6863\u548c\u4e34\u5e8a\u51b3\u7b56\u9519\u8bef\uff0c\u5145\u5f53\u4e34\u5e8a\u533b\u751f\u7684\u5b89\u5168\u4fdd\u969c\uff0c\u4ece\u800c\u51cf\u5c11\u4e86\u8bca\u65ad\u548c\u6cbb\u7597\u9519\u8bef\u3002", "motivation": "\u8bc4\u4f30\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u5728\u5b9e\u65f6\u62a4\u7406\u4e2d\u7684\u5f71\u54cd\u3002", "method": "\u8fdb\u884c\u4e86\u4e00\u9879\u8d28\u91cf\u6539\u8fdb\u7814\u7a76\uff0c\u6bd4\u8f83\u4e8615\u5bb6\u8bca\u6240\u7684\u4e34\u5e8a\u533b\u751f\u5728\u4f7f\u7528\u6216\u4e0d\u4f7f\u7528AI Consult\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u768439,849\u6b21\u60a3\u8005\u5c31\u8bca\u7684\u7ed3\u679c\u3002\u7531\u72ec\u7acb\u533b\u751f\u5bf9\u5c31\u8bca\u8fdb\u884c\u8bc4\u4f30\uff0c\u4ee5\u8bc6\u522b\u4e34\u5e8a\u9519\u8bef\u3002", "result": "\u4f7f\u7528AI Consult\u7684\u4e34\u5e8a\u533b\u751f\u72af\u7684\u9519\u8bef\u76f8\u5bf9\u8f83\u5c11\uff1a\u8bca\u65ad\u9519\u8bef\u51cf\u5c1116%\uff0c\u6cbb\u7597\u9519\u8bef\u51cf\u5c1113%\u3002", "conclusion": "LLM\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u5de5\u5177\u5177\u6709\u51cf\u5c11\u5b9e\u9645\u73af\u5883\u4e2d\u9519\u8bef\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u63a8\u8fdb\u8d1f\u8d23\u4efb\u7684\u91c7\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6846\u67b6\u3002"}}
{"id": "2507.17054", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17054", "abs": "https://arxiv.org/abs/2507.17054", "authors": ["Shao-Hung Chan", "Thomy Phan", "Jiaoyang Li", "Sven Koenig"], "title": "New Mechanisms in Flex Distribution for Bounded Suboptimal Multi-Agent Path Finding", "comment": "9 pages, 10 figures, International Symposium on Combinatorial Search,\n  2025", "summary": "Multi-Agent Path Finding (MAPF) is the problem of finding a set of\ncollision-free paths, one for each agent in a shared environment. Its objective\nis to minimize the sum of path costs (SOC), where the path cost of each agent\nis defined as the travel time from its start location to its target location.\nExplicit Estimation Conflict-Based Search (EECBS) is the leading algorithm for\nbounded-suboptimal MAPF, with the SOC of the solution being at most a\nuser-specified factor $w$ away from optimal. EECBS maintains sets of paths and\na lower bound $LB$ on the optimal SOC. Then, it iteratively selects a set of\npaths whose SOC is at most $w \\cdot LB$ and introduces constraints to resolve\ncollisions. For each path in a set, EECBS maintains a lower bound on its\noptimal path that satisfies constraints. By finding an individually\nbounded-suboptimal path with cost at most a threshold of $w$ times its lower\nbound, EECBS guarantees to find a bounded-suboptimal solution. To speed up\nEECBS, previous work uses flex distribution to increase the threshold. Though\nEECBS with flex distribution guarantees to find a bounded-suboptimal solution,\nincreasing the thresholds may push the SOC beyond $w \\cdot LB$, forcing EECBS\nto switch among different sets of paths instead of resolving collisions on a\nparticular set of paths, and thus reducing efficiency. To address this issue,\nwe propose Conflict-Based Flex Distribution that distributes flex in proportion\nto the number of collisions. We also estimate the delays needed to satisfy\nconstraints and propose Delay-Based Flex Distribution. On top of that, we\npropose Mixed-Strategy Flex Distribution, combining both in a hierarchical\nframework. We prove that EECBS with our new flex distribution mechanisms is\ncomplete and bounded-suboptimal. Our experiments show that our approaches\noutperform the original (greedy) flex distribution.", "AI": {"tldr": "This paper proposes new flex distribution mechanisms for EECBS to improve its efficiency and proves their completeness and bounded-suboptimality. Experiments show that the proposed approaches outperform the original flex distribution.", "motivation": "The paper aims to address the issue of reduced efficiency in Explicit Estimation Conflict-Based Search (EECBS) caused by increasing thresholds that may push the SOC beyond  w  * LB, forcing EECBS to switch among different sets of paths instead of resolving collisions on a particular set of paths.", "method": "The paper proposes Conflict-Based Flex Distribution that distributes flex in proportion to the number of collisions. It also estimates the delays needed to satisfy constraints and proposes Delay-Based Flex Distribution. On top of that, it proposes Mixed-Strategy Flex Distribution, combining both in a hierarchical framework.", "result": "The experiments show that the proposed approaches outperform the original (greedy) flex distribution.", "conclusion": "The paper introduces Conflict-Based Flex Distribution and Delay-Based Flex Distribution to address the issue of reduced efficiency in EECBS due to switching among different sets of paths. It also proposes Mixed-Strategy Flex Distribution, combining both in a hierarchical framework. The paper proves that EECBS with these new flex distribution mechanisms is complete and bounded-suboptimal, and the experiments show that the proposed approaches outperform the original (greedy) flex distribution."}}
{"id": "2507.16850", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16850", "abs": "https://arxiv.org/abs/2507.16850", "authors": ["Mohamed Adjel"], "title": "Toward a Real-Time Framework for Accurate Monocular 3D Human Pose Estimation with Geometric Priors", "comment": "IEEE ICRA 2025 (workshop: Enhancing Human Mobility: From Computer\n  Vision-Based Motion Tracking to Wearable Assistive Robot Control), May 2025,\n  Atlanta (Georgia), United States", "summary": "Monocular 3D human pose estimation remains a challenging and ill-posed\nproblem, particularly in real-time settings and unconstrained environments.\nWhile direct imageto-3D approaches require large annotated datasets and heavy\nmodels, 2D-to-3D lifting offers a more lightweight and flexible\nalternative-especially when enhanced with prior knowledge. In this work, we\npropose a framework that combines real-time 2D keypoint detection with\ngeometry-aware 2D-to-3D lifting, explicitly leveraging known camera intrinsics\nand subject-specific anatomical priors. Our approach builds on recent advances\nin self-calibration and biomechanically-constrained inverse kinematics to\ngenerate large-scale, plausible 2D-3D training pairs from MoCap and synthetic\ndatasets. We discuss how these ingredients can enable fast, personalized, and\naccurate 3D pose estimation from monocular images without requiring specialized\nhardware. This proposal aims to foster discussion on bridging data-driven\nlearning and model-based priors to improve accuracy, interpretability, and\ndeployability of 3D human motion capture on edge devices in the wild.", "AI": {"tldr": "combines real-time 2D keypoint detection with geometry-aware 2D-to-3D lifting, explicitly leveraging known camera intrinsics and subject-specific anatomical priors to enable fast, personalized, and accurate 3D pose estimation from monocular images without requiring specialized hardware.", "motivation": "Monocular 3D human pose estimation remains a challenging and ill-posed problem, particularly in real-time settings and unconstrained environments. While direct imageto-3D approaches require large annotated datasets and heavy models, 2D-to-3D lifting offers a more lightweight and flexible alternative-especially when enhanced with prior knowledge.", "method": "combines real-time 2D keypoint detection with geometry-aware 2D-to-3D lifting, explicitly leveraging known camera intrinsics and subject-specific anatomical priors. builds on recent advances in self-calibration and biomechanically-constrained inverse kinematics to generate large-scale, plausible 2D-3D training pairs from MoCap and synthetic datasets.", "result": "enable fast, personalized, and accurate 3D pose estimation from monocular images without requiring specialized hardware.", "conclusion": "This proposal aims to foster discussion on bridging data-driven learning and model-based priors to improve accuracy, interpretability, and deployability of 3D human motion capture on edge devices in the wild."}}
{"id": "2507.16969", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.16969", "abs": "https://arxiv.org/abs/2507.16969", "authors": ["Shilong Zhao", "Fei Sun", "Kaike Zhang", "Shaoling Jing", "Du Su", "Zhichao Shi", "Zhiyi Yin", "Huawei Shen", "Xueqi Cheng"], "title": "LLM4MEA: Data-free Model Extraction Attacks on Sequential Recommenders via Large Language Models", "comment": null, "summary": "Recent studies have demonstrated the vulnerability of sequential recommender\nsystems to Model Extraction Attacks (MEAs). MEAs collect responses from\nrecommender systems to replicate their functionality, enabling unauthorized\ndeployments and posing critical privacy and security risks. Black-box attacks\nin prior MEAs are ineffective at exposing recommender system vulnerabilities\ndue to random sampling in data selection, which leads to misaligned synthetic\nand real-world distributions. To overcome this limitation, we propose LLM4MEA,\na novel model extraction method that leverages Large Language Models (LLMs) as\nhuman-like rankers to generate data. It generates data through interactions\nbetween the LLM ranker and target recommender system. In each interaction, the\nLLM ranker analyzes historical interactions to understand user behavior, and\nselects items from recommendations with consistent preferences to extend the\ninteraction history, which serves as training data for MEA. Extensive\nexperiments demonstrate that LLM4MEA significantly outperforms existing\napproaches in data quality and attack performance, reducing the divergence\nbetween synthetic and real-world data by up to 64.98% and improving MEA\nperformance by 44.82% on average. From a defensive perspective, we propose a\nsimple yet effective defense strategy and identify key hyperparameters of\nrecommender systems that can mitigate the risk of MEAs.", "AI": {"tldr": "LLM4MEA \u662f\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6570\u636e\u7684\u65b0\u578b\u6a21\u578b\u63d0\u53d6\u65b9\u6cd5\uff0c\u53ef\u6709\u6548\u63d0\u9ad8\u653b\u51fb\u6027\u80fd\u5e76\u964d\u4f4e\u63a8\u8350\u7cfb\u7edf\u6f0f\u6d1e\u7684\u98ce\u9669\u3002", "motivation": "\u5148\u524d\u7684 MEA \u4e2d\u7684\u9ed1\u76d2\u653b\u51fb\u7531\u4e8e\u6570\u636e\u9009\u62e9\u4e2d\u7684\u968f\u673a\u62bd\u6837\u800c\u65e0\u6cd5\u66b4\u9732\u63a8\u8350\u7cfb\u7edf\u6f0f\u6d1e\uff0c\u4ece\u800c\u5bfc\u81f4\u5408\u6210\u5206\u5e03\u4e0e\u771f\u5b9e\u5206\u5e03\u4e0d\u4e00\u81f4\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4f5c\u4e3a\u7c7b\u4eba\u6392\u5e8f\u5668\u6765\u751f\u6210\u6570\u636e\u7684\u65b0\u578b\u6a21\u578b\u63d0\u53d6\u65b9\u6cd5 LLM4MEA\u3002", "result": "LLM4MEA \u5c06\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u4e4b\u95f4\u7684\u5dee\u5f02\u964d\u4f4e\u4e86\u9ad8\u8fbe 64.98%\uff0c\u5e76\u5e73\u5747\u63d0\u9ad8\u4e86 MEA \u6027\u80fd 44.82%\u3002", "conclusion": "LLM4MEA\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u6570\u636e\u8d28\u91cf\u548c\u653b\u51fb\u6027\u80fd\u65b9\u9762\u90fd\u6709\u63d0\u5347\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u9632\u5fa1\u7b56\u7565\uff0c\u5e76\u786e\u5b9a\u4e86\u53ef\u4ee5\u964d\u4f4e MEA \u98ce\u9669\u7684\u5173\u952e\u8d85\u53c2\u6570\u3002"}}
{"id": "2507.16844", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.16844", "abs": "https://arxiv.org/abs/2507.16844", "authors": ["Jie He", "Vincent Theo Willem Kenbeek", "Zhantao Yang", "Meixun Qu", "Ezio Bartocci", "Dejan Ni\u010dkovi\u0107", "Radu Grosu"], "title": "TD-Interpreter: Enhancing the Understanding of Timing Diagrams with Visual-Language Learning", "comment": null, "summary": "We introduce TD-Interpreter, a specialized ML tool that assists engineers in\nunderstanding complex timing diagrams (TDs), originating from a third party,\nduring their design and verification process. TD-Interpreter is a visual\nquestion-answer environment which allows engineers to input a set of TDs and\nask design and verification queries regarding these TDs. We implemented\nTD-Interpreter with multimodal learning by fine-tuning LLaVA, a lightweight 7B\nMultimodal Large Language Model (MLLM). To address limited training data\navailability, we developed a synthetic data generation workflow that aligns\nvisual information with its textual interpretation. Our experimental evaluation\ndemonstrates the usefulness of TD-Interpreter which outperformed untuned GPT-4o\nby a large margin on the evaluated benchmarks.", "AI": {"tldr": "TD-Interpreter is a ML tool that helps engineers understand timing diagrams using visual question-answering.", "motivation": "To assist engineers in understanding complex timing diagrams from third parties during design and verification.", "method": "Fine-tuned LLaVA, a 7B multimodal large language model, with a synthetic data generation workflow.", "result": "TD-Interpreter is a visual question-answer environment for design and verification queries regarding timing diagrams.", "conclusion": "TD-Interpreter outperforms untuned GPT-4o on timing diagram benchmarks."}}
{"id": "2507.16951", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.16951", "abs": "https://arxiv.org/abs/2507.16951", "authors": ["Shuyuan Lin", "Lei Duan", "Philip Hughes", "Yuxuan Sheng"], "title": "Harnessing RLHF for Robust Unanswerability Recognition and Trustworthy Response Generation in LLMs", "comment": null, "summary": "Conversational Information Retrieval (CIR) systems, while offering intuitive\naccess to information, face a significant challenge: reliably handling\nunanswerable questions to prevent the generation of misleading or hallucinated\ncontent. Traditional approaches often rely on external classifiers, which can\nintroduce inconsistencies with the core generative Large Language Models\n(LLMs). This paper introduces Self-Aware LLM for Unanswerability (SALU), a\nnovel approach that deeply integrates unanswerability detection directly within\nthe LLM's generative process. SALU is trained using a multi-task learning\nframework for both standard Question Answering (QA) and explicit abstention\ngeneration for unanswerable queries. Crucially, it incorporates a\nconfidence-score-guided reinforcement learning with human feedback (RLHF)\nphase, which explicitly penalizes hallucinated responses and rewards\nappropriate abstentions, fostering intrinsic self-awareness of knowledge\nboundaries. Through extensive experiments on our custom-built\nC-IR_Answerability dataset, SALU consistently outperforms strong baselines,\nincluding hybrid LLM-classifier systems, in overall accuracy for correctly\nanswering or abstaining from questions. Human evaluation further confirms\nSALU's superior reliability, achieving high scores in factuality, appropriate\nabstention, and, most importantly, a dramatic reduction in hallucination,\ndemonstrating its ability to robustly \"know when to say 'I don't know'.\"", "AI": {"tldr": "SALU \u662f\u4e00\u79cd\u7528\u4e8e\u68c0\u6d4bLLM\u4e2d\u65e0\u6cd5\u56de\u7b54\u7684\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u4e14\u4e0d\u592a\u53ef\u80fd\u4ea7\u751f\u5e7b\u89c9\u3002", "motivation": "\u4f1a\u8bdd\u4fe1\u606f\u68c0\u7d22 (CIR) \u7cfb\u7edf\u5728\u63d0\u4f9b\u5bf9\u4fe1\u606f\u7684\u76f4\u89c2\u8bbf\u95ee\u7684\u540c\u65f6\uff0c\u9762\u4e34\u7740\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff1a\u53ef\u9760\u5730\u5904\u7406\u65e0\u6cd5\u56de\u7b54\u7684\u95ee\u9898\uff0c\u4ee5\u9632\u6b62\u4ea7\u751f\u8bef\u5bfc\u6027\u6216\u5e7b\u89c9\u5185\u5bb9\u3002\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u5916\u90e8\u5206\u7c7b\u5668\uff0c\u8fd9\u53ef\u80fd\u4f1a\u5bfc\u81f4\u4e0e\u6838\u5fc3\u751f\u6210\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4e0d\u4e00\u81f4\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u5373\u7528\u4e8e\u4e0d\u53ef\u56de\u7b54\u6027\u7684\u81ea\u6211\u611f\u77e5LLM (SALU)\uff0c\u8be5\u65b9\u6cd5\u5c06\u4e0d\u53ef\u56de\u7b54\u6027\u68c0\u6d4b\u76f4\u63a5\u96c6\u6210\u5728LLM\u7684\u751f\u6210\u8fc7\u7a0b\u4e2d\u3002\u5b83\u4f7f\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u8fdb\u884c\u8bad\u7ec3\uff0c\u7528\u4e8e\u6807\u51c6\u95ee\u7b54 (QA) \u548c\u4e0d\u53ef\u56de\u7b54\u67e5\u8be2\u7684\u663e\u5f0f\u5f03\u6743\u751f\u6210\u3002\u81f3\u5173\u91cd\u8981\u7684\u662f\uff0c\u5b83\u7ed3\u5408\u4e86\u7f6e\u4fe1\u5ea6\u8bc4\u5206\u5f15\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\u4e0e\u4eba\u5de5\u53cd\u9988 (RLHF) \u9636\u6bb5\uff0c\u8be5\u9636\u6bb5\u660e\u786e\u60e9\u7f5a\u5e7b\u89c9\u53cd\u5e94\u5e76\u5956\u52b1\u9002\u5f53\u7684\u5f03\u6743\uff0c\u4ece\u800c\u57f9\u517b\u5bf9\u77e5\u8bc6\u8fb9\u754c\u7684\u5185\u5728\u81ea\u6211\u610f\u8bc6\u3002", "result": "\u901a\u8fc7\u5728\u6211\u4eec\u5b9a\u5236\u7684 C-IR_Answerability \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\uff0cSALU \u5728\u6b63\u786e\u56de\u7b54\u6216\u653e\u5f03\u56de\u7b54\u95ee\u9898\u7684\u6574\u4f53\u51c6\u786e\u6027\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\uff0c\u5305\u62ec\u6df7\u5408 LLM \u5206\u7c7b\u5668\u7cfb\u7edf\u3002\u4eba\u5de5\u8bc4\u4f30\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86 SALU \u7684\u5353\u8d8a\u53ef\u9760\u6027\uff0c\u5728\u4e8b\u5b9e\u6027\u3001\u9002\u5f53\u5f03\u6743\u65b9\u9762\u53d6\u5f97\u4e86\u9ad8\u5206\uff0c\u6700\u91cd\u8981\u7684\u662f\uff0c\u5e7b\u89c9\u663e\u7740\u51cf\u5c11\u3002", "conclusion": "SALU\u5728\u4e8b\u5b9e\u6027\u3001\u9002\u5f53\u5f03\u6743\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u4e86\u5e7b\u89c9\uff0c\u5c55\u793a\u4e86\u5176\u53ef\u9760\u6027\u3002"}}
{"id": "2507.17075", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17075", "abs": "https://arxiv.org/abs/2507.17075", "authors": ["Yihao Xue", "Baharan Mirzasoleiman"], "title": "LoRA is All You Need for Safety Alignment of Reasoning LLMs", "comment": null, "summary": "Reasoning LLMs have demonstrated remarkable breakthroughs in solving complex\nproblems that were previously out of reach. To ensure LLMs do not assist with\nharmful requests, safety alignment fine-tuning is necessary in the\npost-training phase. However, safety alignment fine-tuning has recently been\nshown to significantly degrade reasoning abilities, a phenomenon known as the\n\"Safety Tax\". In this work, we show that using LoRA for SFT on refusal datasets\neffectively aligns the model for safety without harming its reasoning\ncapabilities. This is because restricting the safety weight updates to a\nlow-rank space minimizes the interference with the reasoning weights. Our\nextensive experiments across four benchmarks covering math, science, and coding\nshow that this approach produces highly safe LLMs -- with safety levels\ncomparable to full-model fine-tuning -- without compromising their reasoning\nabilities. Additionally, we observe that LoRA induces weight updates with\nsmaller overlap with the initial weights compared to full-model fine-tuning. We\nalso explore methods that further reduce such overlap -- via regularization or\nduring weight merging -- and observe some improvement on certain tasks. We hope\nthis result motivates designing approaches that yield more consistent\nimprovements in the reasoning-safety trade-off.", "AI": {"tldr": "\u4f7f\u7528LoRA\u8fdb\u884c\u5b89\u5168\u5bf9\u9f50\u5fae\u8c03\u53ef\u4ee5\u5728\u4e0d\u727a\u7272\u63a8\u7406\u80fd\u529b\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8LLM\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u5b89\u5168\u5bf9\u9f50\u5fae\u8c03\u4f1a\u663e\u8457\u964d\u4f4e\u63a8\u7406\u80fd\u529b\uff0c\u5373\u201c\u5b89\u5168\u7a0e\u201d\u3002", "method": "\u4f7f\u7528LoRA\u5bf9\u62d2\u7edd\u6570\u636e\u96c6\u8fdb\u884cSFT\uff0c\u9650\u5236\u5b89\u5168\u6743\u91cd\u66f4\u65b0\u5230\u4f4e\u79e9\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4ea7\u751f\u4e86\u9ad8\u5ea6\u5b89\u5168\u7684LLM\uff0c\u5176\u5b89\u5168\u7ea7\u522b\u4e0e\u5b8c\u6574\u6a21\u578b\u5fae\u8c03\u76f8\u5f53\uff0c\u4e14\u4e0d\u5f71\u54cd\u5176\u63a8\u7406\u80fd\u529b\u3002LoRA\u5f15\u8d77\u7684\u6743\u91cd\u66f4\u65b0\u4e0e\u521d\u59cb\u6743\u91cd\u7684\u91cd\u53e0\u5c0f\u4e8e\u5b8c\u6574\u6a21\u578b\u5fae\u8c03\u3002\u51cf\u5c11\u8fd9\u79cd\u91cd\u53e0\u7684\u65b9\u6cd5\u5728\u67d0\u4e9b\u4efb\u52a1\u4e0a\u6709\u6240\u6539\u8fdb\u3002", "conclusion": "\u4f7f\u7528LoRA\u8fdb\u884cSFT\u53ef\u4ee5\u6709\u6548\u5bf9\u9f50\u6a21\u578b\u7684\u5b89\u5168\u6027\uff0c\u4e14\u4e0d\u635f\u5bb3\u5176\u63a8\u7406\u80fd\u529b\u3002\u901a\u8fc7\u9650\u5236\u5b89\u5168\u6743\u91cd\u66f4\u65b0\u5230\u4f4e\u79e9\u7a7a\u95f4\uff0c\u6700\u5c0f\u5316\u4e86\u4e0e\u63a8\u7406\u6743\u91cd\u7684\u5e72\u6270\u3002"}}
{"id": "2507.16851", "categories": ["cs.CV", "cs.NE", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.16851", "abs": "https://arxiv.org/abs/2507.16851", "authors": ["Zelong Liu", "Yuliang Gu", "Zhichao Sun", "Huachao Zhu", "Xin Xiao", "Bo Du", "Laurent Najman", "Yongchao Xu"], "title": "Coarse-to-fine crack cue for robust crack detection", "comment": null, "summary": "Crack detection is an important task in computer vision. Despite impressive\nin-dataset performance, deep learning-based methods still struggle in\ngeneralizing to unseen domains. The thin structure property of cracks is\nusually overlooked by previous methods. In this work, we introduce CrackCue, a\nnovel method for robust crack detection based on coarse-to-fine crack cue\ngeneration. The core concept lies on leveraging the thin structure property to\ngenerate a robust crack cue, guiding the crack detection. Specifically, we\nfirst employ a simple max-pooling and upsampling operation on the crack image.\nThis results in a coarse crack-free background, based on which a fine\ncrack-free background can be obtained via a reconstruction network. The\ndifference between the original image and fine crack-free background provides a\nfine crack cue. This fine cue embeds robust crack prior information which is\nunaffected by complex backgrounds, shadow, and varied lighting. As a\nplug-and-play method, we incorporate the proposed CrackCue into three advanced\ncrack detection networks. Extensive experimental results demonstrate that the\nproposed CrackCue significantly improves the generalization ability and\nrobustness of the baseline methods. The source code will be publicly available.", "AI": {"tldr": "CrackCue, a novel coarse-to-fine crack cue generation method, leverages the thin structure property of cracks to improve the generalization ability and robustness of crack detection networks.", "motivation": "deep learning-based methods still struggle in generalizing to unseen domains. The thin structure property of cracks is usually overlooked by previous methods.", "method": "a novel method for robust crack detection based on coarse-to-fine crack cue generation. Specifically, we first employ a simple max-pooling and upsampling operation on the crack image. This results in a coarse crack-free background, based on which a fine crack-free background can be obtained via a reconstruction network. The difference between the original image and fine crack-free background provides a fine crack cue.", "result": "This fine cue embeds robust crack prior information which is unaffected by complex backgrounds, shadow, and varied lighting.", "conclusion": "The proposed CrackCue significantly improves the generalization ability and robustness of the baseline methods."}}
{"id": "2507.17080", "categories": ["cs.IR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17080", "abs": "https://arxiv.org/abs/2507.17080", "authors": ["Ramin Giahi", "Kehui Yao", "Sriram Kollipara", "Kai Zhao", "Vahid Mirjalili", "Jianpeng Xu", "Topojoy Biswas", "Evren Korpeoglu", "Kannan Achan"], "title": "VL-CLIP: Enhancing Multimodal Recommendations via Visual Grounding and LLM-Augmented CLIP Embeddings", "comment": "Accepted at RecSys 2025; DOI:https://doi.org/10.1145/3705328.3748064", "summary": "Multimodal learning plays a critical role in e-commerce recommendation\nplatforms today, enabling accurate recommendations and product understanding.\nHowever, existing vision-language models, such as CLIP, face key challenges in\ne-commerce recommendation systems: 1) Weak object-level alignment, where global\nimage embeddings fail to capture fine-grained product attributes, leading to\nsuboptimal retrieval performance; 2) Ambiguous textual representations, where\nproduct descriptions often lack contextual clarity, affecting cross-modal\nmatching; and 3) Domain mismatch, as generic vision-language models may not\ngeneralize well to e-commerce-specific data. To address these limitations, we\npropose a framework, VL-CLIP, that enhances CLIP embeddings by integrating\nVisual Grounding for fine-grained visual understanding and an LLM-based agent\nfor generating enriched text embeddings. Visual Grounding refines image\nrepresentations by localizing key products, while the LLM agent enhances\ntextual features by disambiguating product descriptions. Our approach\nsignificantly improves retrieval accuracy, multimodal retrieval effectiveness,\nand recommendation quality across tens of millions of items on one of the\nlargest e-commerce platforms in the U.S., increasing CTR by 18.6%, ATC by\n15.5%, and GMV by 4.0%. Additional experimental results show that our framework\noutperforms vision-language models, including CLIP, FashionCLIP, and GCL, in\nboth precision and semantic alignment, demonstrating the potential of combining\nobject-aware visual grounding and LLM-enhanced text representation for robust\nmultimodal recommendations.", "AI": {"tldr": "\u63d0\u51faVL-CLIP\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u5b9a\u4f4d\u548cLLM\u589e\u5f3a\u6587\u672c\u8868\u793a\u6765\u6539\u8fdb\u7535\u5546\u63a8\u8350\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u5728\u7535\u5546\u63a8\u8350\u7cfb\u7edf\u4e2d\u9762\u4e34\u5173\u952e\u6311\u6218\uff1a1) \u5f31\u5bf9\u8c61\u7ea7\u5bf9\u9f50\uff0c2) \u6a21\u7cca\u7684\u6587\u672c\u8868\u793a\uff0c3) \u9886\u57df\u4e0d\u5339\u914d\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u540d\u4e3aVL-CLIP\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u5b9a\u4f4d\u4ee5\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u89c6\u89c9\u7406\u89e3\uff0c\u5e76\u5229\u7528\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u6765\u751f\u6210\u4e30\u5bcc\u7684\u6587\u672c\u5d4c\u5165\uff0c\u4ece\u800c\u589e\u5f3aCLIP\u5d4c\u5165\u3002", "result": "\u5728\u7f8e\u56fd\u6700\u5927\u7684\u7535\u5546\u5e73\u53f0\u4e4b\u4e00\u4e0a\uff0cCTR\u63d0\u9ad818.6%\uff0cATC\u63d0\u9ad815.5%\uff0cGMV\u63d0\u9ad84.0%\u3002\u5728\u7cbe\u786e\u5ea6\u548c\u8bed\u4e49\u5bf9\u9f50\u65b9\u9762\uff0cVL-CLIP\u4f18\u4e8eCLIP\u3001FashionCLIP\u548cGCL\u7b49\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "VL-CLIP\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u5b9a\u4f4d\u548cLLM\u589e\u5f3a\u7684\u6587\u672c\u8868\u793a\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7535\u5546\u63a8\u8350\u7684\u68c0\u7d22\u51c6\u786e\u7387\u3001\u591a\u6a21\u6001\u68c0\u7d22\u6548\u679c\u548c\u63a8\u8350\u8d28\u91cf\u3002"}}
{"id": "2507.16864", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16864", "abs": "https://arxiv.org/abs/2507.16864", "authors": ["Tao Xu", "Dung-Yang Lee", "Momiao Xiong"], "title": "Reinforcement Learning in hyperbolic space for multi-step reasoning", "comment": "53 pages, 5 figures", "summary": "Multi-step reasoning is a fundamental challenge in artificial intelligence,\nwith applications ranging from mathematical problem-solving to decision-making\nin dynamic environments. Reinforcement Learning (RL) has shown promise in\nenabling agents to perform multi-step reasoning by optimizing long-term\nrewards. However, conventional RL methods struggle with complex reasoning tasks\ndue to issues such as credit assignment, high-dimensional state\nrepresentations, and stability concerns. Recent advancements in Transformer\narchitectures and hyperbolic geometry have provided novel solutions to these\nchallenges. This paper introduces a new framework that integrates hyperbolic\nTransformers into RL for multi-step reasoning. The proposed approach leverages\nhyperbolic embeddings to model hierarchical structures effectively. We present\ntheoretical insights, algorithmic details, and experimental results that\ninclude Frontier Math and nonlinear optimal control problems. Compared to RL\nwith vanilla transformer, the hyperbolic RL largely improves accuracy by\n(32%~44%) on FrontierMath benchmark, (43%~45%) on nonlinear optimal control\nbenchmark, while achieving impressive reduction in computational time by\n(16%~32%) on FrontierMath benchmark, (16%~17%) on nonlinear optimal control\nbenchmark. Our work demonstrates the potential of hyperbolic Transformers in\nreinforcement learning, particularly for multi-step reasoning tasks that\ninvolve hierarchical structures.", "AI": {"tldr": "This paper proposes a new reinforcement learning framework using hyperbolic Transformers for multi-step reasoning, which improves accuracy and reduces computational time compared to RL with vanilla transformers.", "motivation": "Conventional RL methods struggle with complex reasoning tasks due to issues such as credit assignment, high-dimensional state representations, and stability concerns.", "method": "The paper introduces a new framework that integrates hyperbolic Transformers into RL for multi-step reasoning. The proposed approach leverages hyperbolic embeddings to model hierarchical structures effectively.", "result": "Compared to RL with vanilla transformer, the hyperbolic RL largely improves accuracy by (32%~44%) on FrontierMath benchmark, (43%~45%) on nonlinear optimal control benchmark, while achieving impressive reduction in computational time by (16%~32%) on FrontierMath benchmark, (16%~17%) on nonlinear optimal control benchmark.", "conclusion": "The paper demonstrates the potential of hyperbolic Transformers in reinforcement learning, particularly for multi-step reasoning tasks that involve hierarchical structures."}}
{"id": "2507.16971", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.16971", "abs": "https://arxiv.org/abs/2507.16971", "authors": ["Aleksandr Perevalov", "Andreas Both"], "title": "Text-to-SPARQL Goes Beyond English: Multilingual Question Answering Over Knowledge Graphs through Human-Inspired Reasoning", "comment": "During the final evaluation on the DBpedia- and Corporate-based KGQA\n  benchmarks within the Text2SPARQL challenge 2025, our approach took first\n  place among the other participants", "summary": "Accessing knowledge via multilingual natural-language interfaces is one of\nthe emerging challenges in the field of information retrieval and related ones.\nStructured knowledge stored in knowledge graphs can be queried via a specific\nquery language (e.g., SPARQL). Therefore, one needs to transform\nnatural-language input into a query to fulfill an information need. Prior\napproaches mostly focused on combining components (e.g., rule-based or\nneural-based) that solve downstream tasks and come up with an answer at the\nend. We introduce mKGQAgent, a human-inspired framework that breaks down the\ntask of converting natural language questions into SPARQL queries into modular,\ninterpretable subtasks. By leveraging a coordinated LLM agent workflow for\nplanning, entity linking, and query refinement - guided by an experience pool\nfor in-context learning - mKGQAgent efficiently handles multilingual KGQA.\nEvaluated on the DBpedia- and Corporate-based KGQA benchmarks within the\nText2SPARQL challenge 2025, our approach took first place among the other\nparticipants. This work opens new avenues for developing human-like reasoning\nsystems in multilingual semantic parsing.", "AI": {"tldr": "The paper introduces mKGQAgent, a framework that converts natural language questions into SPARQL queries using a coordinated LLM agent workflow. The approach achieved first place in the Text2SPARQL challenge 2025.", "motivation": "Accessing knowledge via multilingual natural-language interfaces is one of the emerging challenges in the field of information retrieval and related ones. Structured knowledge stored in knowledge graphs can be queried via a specific query language (e.g., SPARQL). Therefore, one needs to transform natural-language input into a query to fulfill an information need.", "method": "introducing mKGQAgent, a human-inspired framework that breaks down the task of converting natural language questions into SPARQL queries into modular, interpretable subtasks. By leveraging a coordinated LLM agent workflow for planning, entity linking, and query refinement - guided by an experience pool for in-context learning", "result": "Evaluated on the DBpedia- and Corporate-based KGQA benchmarks within the Text2SPARQL challenge 2025, our approach took first place among the other participants.", "conclusion": "This work opens new avenues for developing human-like reasoning systems in multilingual semantic parsing."}}
{"id": "2507.17118", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17118", "abs": "https://arxiv.org/abs/2507.17118", "authors": ["Mandar Pitale", "Jelena Frtunikj", "Abhinaw Priyadershi", "Vasu Singh", "Maria Spence"], "title": "HySafe-AI: Hybrid Safety Architectural Analysis Framework for AI Systems: A Case Study", "comment": "7 pages", "summary": "AI has become integral to safety-critical areas like autonomous driving\nsystems (ADS) and robotics. The architecture of recent autonomous systems are\ntrending toward end-to-end (E2E) monolithic architectures such as large\nlanguage models (LLMs) and vision language models (VLMs). In this paper, we\nreview different architectural solutions and then evaluate the efficacy of\ncommon safety analyses such as failure modes and effect analysis (FMEA) and\nfault tree analysis (FTA). We show how these techniques can be improved for the\nintricate nature of the foundational models, particularly in how they form and\nutilize latent representations. We introduce HySAFE-AI, Hybrid Safety\nArchitectural Analysis Framework for AI Systems, a hybrid framework that adapts\ntraditional methods to evaluate the safety of AI systems. Lastly, we offer\nhints of future work and suggestions to guide the evolution of future AI safety\nstandards.", "AI": {"tldr": "This paper reviews architectural solutions for AI safety in autonomous systems, evaluates safety analysis techniques, introduces HySAFE-AI, and suggests future AI safety standards.", "motivation": "AI has become integral to safety-critical areas like autonomous driving systems (ADS) and robotics. The architecture of recent autonomous systems are trending toward end-to-end (E2E) monolithic architectures such as large language models (LLMs) and vision language models (VLMs).", "method": "The paper reviews different architectural solutions and then evaluate the efficacy of common safety analyses such as failure modes and effect analysis (FMEA) and fault tree analysis (FTA).", "result": "The paper shows how FMEA and FTA techniques can be improved for the intricate nature of the foundational models, particularly in how they form and utilize latent representations.", "conclusion": "This paper introduces HySAFE-AI, a hybrid framework that adapts traditional methods to evaluate the safety of AI systems. It also offers hints of future work and suggestions to guide the evolution of future AI safety standards."}}
{"id": "2507.16854", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16854", "abs": "https://arxiv.org/abs/2507.16854", "authors": ["Xiaoqiang He"], "title": "CLAMP: Contrastive Learning with Adaptive Multi-loss and Progressive Fusion for Multimodal Aspect-Based Sentiment Analysis", "comment": null, "summary": "Multimodal aspect-based sentiment analysis(MABSA) seeks to identify aspect\nterms within paired image-text data and determine their fine grained sentiment\npolarities, representing a fundamental task for improving the effectiveness of\napplications such as product review systems and public opinion monitoring.\nExisting methods face challenges such as cross modal alignment noise and\ninsufficient consistency in fine-grained representations. While global modality\nalignment methods often overlook the connection between aspect terms and their\ncorresponding local visual regions, bridging the representation gap between\ntext and images remains a challenge. To address these limitations, this paper\nintroduces an end to end Contrastive Learning framework with Adaptive\nMulti-loss and Progressive Attention Fusion(CLAMP). The framework is composed\nof three novel modules: Progressive Attention Fusion network, Multi-task\nContrastive Learning, and Adaptive Multi-loss Aggregation. The Progressive\nAttention Fusion network enhances fine-grained alignment between textual\nfeatures and image regions via hierarchical, multi-stage cross modal\ninteractions, effectively suppressing irrelevant visual noise. Secondly,\nmulti-task contrastive learning combines global modal contrast and local\ngranularity alignment to enhance cross modal representation consistency.\nAdaptive Multi-loss Aggregation employs a dynamic uncertainty based weighting\nmechanism to calibrate loss contributions according to each task's uncertainty,\nthereby mitigating gradient interference. Evaluation on standard public\nbenchmarks demonstrates that CLAMP consistently outperforms the vast majority\nof existing state of the art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u7684\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5177\u6709\u81ea\u9002\u5e94\u591a\u91cd\u635f\u5931\u548c\u6e10\u8fdb\u5f0f\u6ce8\u610f\u529b\u878d\u5408(CLAMP)\uff0c\u5e76\u5728\u516c\u5171\u57fa\u51c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u9762\u4e34\u7740\u8bf8\u5982\u8de8\u6a21\u6001\u5bf9\u9f50\u566a\u58f0\u548c\u7ec6\u7c92\u5ea6\u8868\u793a\u4e2d\u7684\u4e00\u81f4\u6027\u4e0d\u8db3\u7b49\u6311\u6218\u3002\u5168\u5c40\u6a21\u6001\u5bf9\u9f50\u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u65b9\u9762\u672f\u8bed\u53ca\u5176\u5bf9\u5e94\u7684\u5c40\u90e8\u89c6\u89c9\u533a\u57df\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u5f25\u5408\u6587\u672c\u548c\u56fe\u50cf\u4e4b\u95f4\u7684\u8868\u793a\u5dee\u8ddd\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5177\u6709\u81ea\u9002\u5e94\u591a\u91cd\u635f\u5931\u548c\u6e10\u8fdb\u5f0f\u6ce8\u610f\u529b\u878d\u5408(CLAMP)\u3002\u8be5\u6846\u67b6\u7531\u4e09\u4e2a\u65b0\u9896\u7684\u6a21\u5757\u7ec4\u6210\uff1a\u6e10\u8fdb\u5f0f\u6ce8\u610f\u529b\u878d\u5408\u7f51\u7edc\u3001\u591a\u4efb\u52a1\u5bf9\u6bd4\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u591a\u91cd\u635f\u5931\u805a\u5408\u3002", "result": "\u6e10\u8fdb\u5f0f\u6ce8\u610f\u529b\u878d\u5408\u7f51\u7edc\u901a\u8fc7\u5206\u5c42\u3001\u591a\u9636\u6bb5\u7684\u8de8\u6a21\u6001\u4ea4\u4e92\u6765\u589e\u5f3a\u6587\u672c\u7279\u5f81\u548c\u56fe\u50cf\u533a\u57df\u4e4b\u95f4\u7684\u7ec6\u7c92\u5ea6\u5bf9\u9f50\uff0c\u4ece\u800c\u6709\u6548\u5730\u6291\u5236\u4e0d\u76f8\u5173\u7684\u89c6\u89c9\u566a\u58f0\u3002\u5176\u6b21\uff0c\u591a\u4efb\u52a1\u5bf9\u6bd4\u5b66\u4e60\u7ed3\u5408\u4e86\u5168\u5c40\u6a21\u6001\u5bf9\u6bd4\u548c\u5c40\u90e8\u7c92\u5ea6\u5bf9\u9f50\uff0c\u4ee5\u589e\u5f3a\u8de8\u6a21\u6001\u8868\u793a\u4e00\u81f4\u6027\u3002\u81ea\u9002\u5e94\u591a\u91cd\u635f\u5931\u805a\u5408\u91c7\u7528\u57fa\u4e8e\u52a8\u6001\u4e0d\u786e\u5b9a\u6027\u7684\u52a0\u6743\u673a\u5236\u6765\u6839\u636e\u6bcf\u4e2a\u4efb\u52a1\u7684\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u635f\u5931\u8d21\u732e\uff0c\u4ece\u800c\u51cf\u8f7b\u68af\u5ea6\u5e72\u6270\u3002", "conclusion": "CLAMP\u5728\u6807\u51c6\u516c\u5171\u57fa\u51c6\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u7edd\u5927\u591a\u6570\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.17112", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.17112", "abs": "https://arxiv.org/abs/2507.17112", "authors": ["Yuhan Wang", "Qing Xie", "Zhifeng Bao", "Mengzi Tang", "Lin Li", "Yongjian Liu"], "title": "Enhancing Transferability and Consistency in Cross-Domain Recommendations via Supervised Disentanglement", "comment": null, "summary": "Cross-domain recommendation (CDR) aims to alleviate the data sparsity by\ntransferring knowledge across domains. Disentangled representation learning\nprovides an effective solution to model complex user preferences by separating\nintra-domain features (domain-shared and domain-specific features), thereby\nenhancing robustness and interpretability. However, disentanglement-based CDR\nmethods employing generative modeling or GNNs with contrastive objectives face\ntwo key challenges: (i) pre-separation strategies decouple features before\nextracting collaborative signals, disrupting intra-domain interactions and\nintroducing noise; (ii) unsupervised disentanglement objectives lack explicit\ntask-specific guidance, resulting in limited consistency and suboptimal\nalignment. To address these challenges, we propose DGCDR, a GNN-enhanced\nencoder-decoder framework. To handle challenge (i), DGCDR first applies GNN to\nextract high-order collaborative signals, providing enriched representations as\na robust foundation for disentanglement. The encoder then dynamically\ndisentangles features into domain-shared and -specific spaces, preserving\ncollaborative information during the separation process. To handle challenge\n(ii), the decoder introduces an anchor-based supervision that leverages\nhierarchical feature relationships to enhance intra-domain consistency and\ncross-domain alignment. Extensive experiments on real-world datasets\ndemonstrate that DGCDR achieves state-of-the-art performance, with improvements\nof up to 11.59% across key metrics. Qualitative analyses further validate its\nsuperior disentanglement quality and transferability. Our source code and\ndatasets are available on GitHub for further comparison.", "AI": {"tldr": "This paper introduces DGCDR, a GNN-enhanced encoder-decoder framework for cross-domain recommendation that addresses the challenges of pre-separation strategies and unsupervised disentanglement objectives in existing methods.", "motivation": "disentanglement-based CDR methods employing generative modeling or GNNs with contrastive objectives face two key challenges: (i) pre-separation strategies decouple features before extracting collaborative signals, disrupting intra-domain interactions and introducing noise; (ii) unsupervised disentanglement objectives lack explicit task-specific guidance, resulting in limited consistency and suboptimal alignment", "method": "a GNN-enhanced encoder-decoder framework", "result": "DGCDR achieves state-of-the-art performance, with improvements of up to 11.59% across key metrics. Qualitative analyses further validate its superior disentanglement quality and transferability.", "conclusion": "DGCDR achieves state-of-the-art performance, with improvements of up to 11.59% across key metrics. Qualitative analyses further validate its superior disentanglement quality and transferability."}}
{"id": "2507.16867", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16867", "abs": "https://arxiv.org/abs/2507.16867", "authors": ["Yunyi Zhao", "Wei Zhang", "Cheng Xiang", "Hongyang Du", "Dusit Niyato", "Shuhua Gao"], "title": "Diffusion-Modeled Reinforcement Learning for Carbon and Risk-Aware Microgrid Optimization", "comment": "10 pages, 5 figures", "summary": "This paper introduces DiffCarl, a diffusion-modeled carbon- and risk-aware\nreinforcement learning algorithm for intelligent operation of multi-microgrid\nsystems. With the growing integration of renewables and increasing system\ncomplexity, microgrid communities face significant challenges in real-time\nenergy scheduling and optimization under uncertainty. DiffCarl integrates a\ndiffusion model into a deep reinforcement learning (DRL) framework to enable\nadaptive energy scheduling under uncertainty and explicitly account for carbon\nemissions and operational risk. By learning action distributions through a\ndenoising generation process, DiffCarl enhances DRL policy expressiveness and\nenables carbon- and risk-aware scheduling in dynamic and uncertain microgrid\nenvironments. Extensive experimental studies demonstrate that it outperforms\nclassic algorithms and state-of-the-art DRL solutions, with 2.3-30.1% lower\noperational cost. It also achieves 28.7% lower carbon emissions than those of\nits carbon-unaware variant and reduces performance variability. These results\nhighlight DiffCarl as a practical and forward-looking solution. Its flexible\ndesign allows efficient adaptation to different system configurations and\nobjectives to support real-world deployment in evolving energy systems.", "AI": {"tldr": "DiffCarl, a diffusion-modeled carbon- and risk-aware reinforcement learning algorithm, enables adaptive energy scheduling under uncertainty and explicitly account for carbon emissions and operational risk in multi-microgrid systems.", "motivation": "Microgrid communities face significant challenges in real-time energy scheduling and optimization under uncertainty with the growing integration of renewables and increasing system complexity.", "method": "DiffCarl integrates a diffusion model into a deep reinforcement learning (DRL) framework.", "result": "DiffCarl outperforms classic algorithms and state-of-the-art DRL solutions, with 2.3-30.1% lower operational cost and 28.7% lower carbon emissions than those of its carbon-unaware variant, and reduces performance variability.", "conclusion": "DiffCarl is a practical and forward-looking solution that adapts to different system configurations and objectives to support real-world deployment in evolving energy systems."}}
{"id": "2507.16974", "categories": ["cs.CL", "cs.AI", "I.2.7; J.m"], "pdf": "https://arxiv.org/pdf/2507.16974", "abs": "https://arxiv.org/abs/2507.16974", "authors": ["Rishemjit Kaur", "Arshdeep Singh Bhankhar", "Surangika Ranathunga", "Jashanpreet Singh Salh", "Sudhir Rajput", "Vidhi", "Kashish Mahendra", "Bhavika Berwal", "Ritesh Kumar"], "title": "Leveraging Synthetic Data for Question Answering with Multilingual LLMs in the Agricultural Domain", "comment": "15 pages, 9 tables, Appendix A-K", "summary": "Enabling farmers to access accurate agriculture-related information in their\nnative languages in a timely manner is crucial for the success of the\nagriculture field. Although large language models (LLMs) can be used to\nimplement Question Answering (QA) systems, simply using publicly available\ngeneral-purpose LLMs in agriculture typically offer generic advisories, lacking\nprecision in local and multilingual contexts due to insufficient\ndomain-specific training and scarcity of high-quality, region-specific\ndatasets. Our study addresses these limitations by generating multilingual\nsynthetic agricultural datasets (English, Hindi, Punjabi) from\nagriculture-specific documents and fine-tuning language-specific LLMs. Our\nevaluation on curated multilingual datasets demonstrates significant\nimprovements in factual accuracy, relevance, and agricultural consensus for the\nfine-tuned models compared to their baseline counterparts. These results\nhighlight the efficacy of synthetic data-driven, language-specific fine-tuning\nas an effective strategy to improve the performance of LLMs in agriculture,\nespecially in multilingual and low-resource settings. By enabling more accurate\nand localized agricultural advisory services, this study provides a meaningful\nstep toward bridging the knowledge gap in AI-driven agricultural solutions for\ndiverse linguistic communities.", "AI": {"tldr": "Fine-tuning language-specific LLMs with synthetic multilingual agricultural data improves accuracy and relevance for farmers in low-resource settings.", "motivation": "Enabling farmers to access accurate agriculture-related information in their native languages is crucial, but general-purpose LLMs lack precision in local and multilingual contexts due to insufficient domain-specific training and scarcity of high-quality, region-specific datasets.", "method": "The study generates multilingual synthetic agricultural datasets (English, Hindi, Punjabi) from agriculture-specific documents and fine-tunes language-specific LLMs.", "result": "The fine-tuned models showed significant improvements in factual accuracy, relevance, and agricultural consensus compared to their baseline counterparts on curated multilingual datasets.", "conclusion": "This study demonstrates that fine-tuning language-specific LLMs with synthetic, multilingual agricultural datasets significantly improves their performance in factual accuracy, relevance, and agricultural consensus, especially in multilingual and low-resource settings. This approach helps bridge the knowledge gap in AI-driven agricultural solutions for diverse linguistic communities."}}
{"id": "2507.17168", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17168", "abs": "https://arxiv.org/abs/2507.17168", "authors": ["Qifan Zhang", "Nuo Chen", "Zehua Li", "Miao Peng", "Jing Tang", "Jia Li"], "title": "Improving LLMs' Generalized Reasoning Abilities by Graph Problems", "comment": "COLM2025", "summary": "Large Language Models (LLMs) have made remarkable strides in reasoning tasks,\nyet their performance often falters on novel and complex problems.\nDomain-specific continued pretraining (CPT) methods, such as those tailored for\nmathematical reasoning, have shown promise but lack transferability to broader\nreasoning tasks. In this work, we pioneer the use of Graph Problem Reasoning\n(GPR) to enhance the general reasoning capabilities of LLMs. GPR tasks,\nspanning pathfinding, network analysis, numerical computation, and topological\nreasoning, require sophisticated logical and relational reasoning, making them\nideal for teaching diverse reasoning patterns. To achieve this, we introduce\nGraphPile, the first large-scale corpus specifically designed for CPT using GPR\ndata. Spanning 10.9 billion tokens across 23 graph tasks, the dataset includes\nchain-of-thought, program-of-thought, trace of execution, and real-world graph\ndata. Using GraphPile, we train GraphMind on popular base models Llama 3 and\n3.1, as well as Gemma 2, achieving up to 4.9 percent higher accuracy in\nmathematical reasoning and up to 21.2 percent improvement in non-mathematical\nreasoning tasks such as logical and commonsense reasoning. By being the first\nto harness GPR for enhancing reasoning patterns and introducing the first\ndataset of its kind, our work bridges the gap between domain-specific\npretraining and universal reasoning capabilities, advancing the adaptability\nand robustness of LLMs.", "AI": {"tldr": "introduce GraphPile, the first large-scale corpus specifically designed for CPT using GPR data to enhance the general reasoning capabilities of LLMs", "motivation": "LLMs performance often falters on novel and complex problems. Domain-specific continued pretraining (CPT) methods lack transferability to broader reasoning tasks.", "method": "pioneer the use of Graph Problem Reasoning (GPR) to enhance the general reasoning capabilities of LLMs", "result": "train GraphMind on popular base models Llama 3 and 3.1, as well as Gemma 2, achieving up to 4.9 percent higher accuracy in mathematical reasoning and up to 21.2 percent improvement in non-mathematical reasoning tasks", "conclusion": "This work bridges the gap between domain-specific pretraining and universal reasoning capabilities, advancing the adaptability and robustness of LLMs."}}
{"id": "2507.16856", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16856", "abs": "https://arxiv.org/abs/2507.16856", "authors": ["Youngjin Na", "Sangheon Jeong", "Youngwan Lee"], "title": "SIA: Enhancing Safety via Intent Awareness for Vision-Language Models", "comment": "5 pages, 6 figures", "summary": "As vision-language models (VLMs) are increasingly deployed in real-world\napplications, new safety risks arise from the subtle interplay between images\nand text. In particular, seemingly innocuous inputs can combine to reveal\nharmful intent, leading to unsafe model responses. Despite increasing attention\nto multimodal safety, previous approaches based on post hoc filtering or static\nrefusal prompts struggle to detect such latent risks, especially when\nharmfulness emerges only from the combination of inputs. We propose SIA (Safety\nvia Intent Awareness), a training-free prompt engineering framework that\nproactively detects and mitigates harmful intent in multimodal inputs. SIA\nemploys a three-stage reasoning process: (1) visual abstraction via captioning,\n(2) intent inference through few-shot chain-of-thought prompting, and (3)\nintent-conditioned response refinement. Rather than relying on predefined rules\nor classifiers, SIA dynamically adapts to the implicit intent inferred from the\nimage-text pair. Through extensive experiments on safety-critical benchmarks\nincluding SIUO, MM-SafetyBench, and HoliSafe, we demonstrate that SIA achieves\nsubstantial safety improvements, outperforming prior methods. Although SIA\nshows a minor reduction in general reasoning accuracy on MMStar, the\ncorresponding safety gains highlight the value of intent-aware reasoning in\naligning VLMs with human-centric values.", "AI": {"tldr": "SIA \u662f\u4e00\u79cd\u514d\u8bad\u7ec3\u7684\u63d0\u793a\u5de5\u7a0b\u6846\u67b6\uff0c\u901a\u8fc7\u610f\u56fe\u611f\u77e5\u6765\u4e3b\u52a8\u68c0\u6d4b\u548c\u7f13\u89e3\u591a\u6a21\u6001\u8f93\u5165\u4e2d\u7684\u6709\u5bb3\u610f\u56fe\uff0c\u4ece\u800c\u63d0\u9ad8\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u968f\u7740\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u8d8a\u6765\u8d8a\u591a\u5730\u90e8\u7f72\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u5e94\u7528\u4e2d\uff0c\u56fe\u50cf\u548c\u6587\u672c\u4e4b\u95f4\u5fae\u5999\u7684\u76f8\u4e92\u4f5c\u7528\u4ea7\u751f\u4e86\u65b0\u7684\u5b89\u5168\u98ce\u9669\u3002\u7279\u522b\u5730\uff0c\u770b\u4f3c\u65e0\u5bb3\u7684\u8f93\u5165\u53ef\u4ee5\u7ed3\u5408\u8d77\u6765\u63ed\u793a\u6709\u5bb3\u7684\u610f\u56fe\uff0c\u4ece\u800c\u5bfc\u81f4\u4e0d\u5b89\u5168\u7684\u6a21\u578b\u54cd\u5e94\u3002\u5c3d\u7ba1\u4eba\u4eec\u8d8a\u6765\u8d8a\u5173\u6ce8\u591a\u6a21\u6001\u5b89\u5168\u6027\uff0c\u4f46\u5148\u524d\u57fa\u4e8e\u4e8b\u540e\u8fc7\u6ee4\u6216\u9759\u6001\u62d2\u7edd\u63d0\u793a\u7684\u65b9\u6cd5\u96be\u4ee5\u68c0\u6d4b\u5230\u8fd9\u79cd\u6f5c\u5728\u7684\u98ce\u9669\uff0c\u5c24\u5176\u662f\u5728\u5371\u5bb3\u6027\u4ec5\u4ece\u8f93\u5165\u7684\u7ec4\u5408\u4e2d\u51fa\u73b0\u65f6\u3002", "method": "SIA \u91c7\u7528\u4e86\u4e00\u4e2a\u4e09\u9636\u6bb5\u7684\u63a8\u7406\u8fc7\u7a0b\uff1a(1) \u901a\u8fc7\u6807\u9898\u8fdb\u884c\u89c6\u89c9\u62bd\u8c61\uff0c(2) \u901a\u8fc7\u5c11\u6837\u672c\u94fe\u5f0f\u601d\u8003\u63d0\u793a\u8fdb\u884c\u610f\u56fe\u63a8\u65ad\uff0c\u4ee5\u53ca (3) \u610f\u56fe\u6761\u4ef6\u4e0b\u7684\u54cd\u5e94\u7ec6\u5316\u3002", "result": "SIA \u5b9e\u73b0\u4e86\u663e\u8457\u7684\u5b89\u5168\u6027\u6539\u8fdb\uff0c\u4f18\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\u3002", "conclusion": "SIA\u901a\u8fc7\u5728 safety-critical benchmarks \u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u5176\u5728\u5b89\u5168\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u9ad8\uff0c\u4f18\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\u3002\u5c3d\u7ba1SIA\u5728 MMStar \u4e0a\u663e\u793a\u51fa\u4e00\u822c\u63a8\u7406\u51c6\u786e\u6027\u7684\u7565\u5fae\u964d\u4f4e\uff0c\u4f46\u76f8\u5e94\u7684\u5b89\u5168\u6536\u76ca\u7a81\u51fa\u4e86\u610f\u56fe\u611f\u77e5\u63a8\u7406\u5728\u4f7f VLM \u4e0e\u4ee5\u4eba\u4e3a\u672c\u7684\u4ef7\u503c\u89c2\u4fdd\u6301\u4e00\u81f4\u65b9\u9762\u7684\u4ef7\u503c\u3002"}}
{"id": "2507.17249", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.17249", "abs": "https://arxiv.org/abs/2507.17249", "authors": ["Hao Gu", "Rui Zhong", "Yu Xia", "Wei Yang", "Chi Lu", "Peng Jiang", "Kun Gai"], "title": "R4ec: A Reasoning, Reflection, and Refinement Framework for Recommendation Systems", "comment": "Accepted by Recsys25", "summary": "Harnessing Large Language Models (LLMs) for recommendation systems has\nemerged as a prominent avenue, drawing substantial research interest. However,\nexisting approaches primarily involve basic prompt techniques for knowledge\nacquisition, which resemble System-1 thinking. This makes these methods highly\nsensitive to errors in the reasoning path, where even a small mistake can lead\nto an incorrect inference. To this end, in this paper, we propose $R^{4}$ec, a\nreasoning, reflection and refinement framework that evolves the recommendation\nsystem into a weak System-2 model. Specifically, we introduce two models: an\nactor model that engages in reasoning, and a reflection model that judges these\nresponses and provides valuable feedback. Then the actor model will refine its\nresponse based on the feedback, ultimately leading to improved responses. We\nemploy an iterative reflection and refinement process, enabling LLMs to\nfacilitate slow and deliberate System-2-like thinking. Ultimately, the final\nrefined knowledge will be incorporated into a recommendation backbone for\nprediction. We conduct extensive experiments on Amazon-Book and MovieLens-1M\ndatasets to demonstrate the superiority of $R^{4}$ec. We also deploy $R^{4}$ec\non a large scale online advertising platform, showing 2.2\\% increase of\nrevenue. Furthermore, we investigate the scaling properties of the actor model\nand reflection model.", "AI": {"tldr": "This paper introduces $R^{4}$ec, a reasoning, reflection, and refinement framework that enhances recommendation systems by using LLMs to mimic System-2 thinking, leading to improved accuracy and a 2.2% revenue increase in online advertising.", "motivation": "Existing approaches primarily involve basic prompt techniques for knowledge acquisition, which resemble System-1 thinking and are highly sensitive to errors in the reasoning path.", "method": "The paper proposes a reasoning, reflection and refinement framework ($R^{4}$ec) that evolves the recommendation system into a weak System-2 model, which includes an actor model for reasoning and a reflection model for feedback. The actor model refines its response based on the feedback through an iterative reflection and refinement process.", "result": "The $R^{4}$ec framework improves recommendation systems by enabling LLMs to facilitate slow and deliberate System-2-like thinking. $R^{4}$ec shows a 2.2% increase in revenue on a large scale online advertising platform.", "conclusion": "The paper demonstrates the superiority of $R^{4}$ec on Amazon-Book and MovieLens-1M datasets and shows a 2.2% increase in revenue on a large scale online advertising platform. The paper also investigates the scaling properties of the actor model and reflection model."}}
{"id": "2507.16871", "categories": ["cs.LG", "hep-th"], "pdf": "https://arxiv.org/pdf/2507.16871", "abs": "https://arxiv.org/abs/2507.16871", "authors": ["Pietro Giuseppe Fr\u00e9", "Federico Milanesio", "Guido Sanguinetti", "Matteo Santoro"], "title": "Navigation through Non-Compact Symmetric Spaces: a mathematical perspective on Cartan Neural Networks", "comment": "59 pages, 2 figures", "summary": "Recent work has identified non-compact symmetric spaces U/H as a promising\nclass of homogeneous manifolds to develop a geometrically consistent theory of\nneural networks. An initial implementation of these concepts has been presented\nin a twin paper under the moniker of Cartan Neural Networks, showing both the\nfeasibility and the performance of these geometric concepts in a machine\nlearning context. The current paper expands on the mathematical structures\nunderpinning Cartan Neural Networks, detailing the geometric properties of the\nlayers and how the maps between layers interact with such structures to make\nCartan Neural Networks covariant and geometrically interpretable. Together,\nthese twin papers constitute a first step towards a fully geometrically\ninterpretable theory of neural networks exploiting group-theoretic structures", "AI": {"tldr": "This paper expands on the mathematical structures underpinning Cartan Neural Networks, detailing the geometric properties of the layers and how the maps between layers interact with such structures to make Cartan Neural Networks covariant and geometrically interpretable.", "motivation": "Recent work has identified non-compact symmetric spaces U/H as a promising class of homogeneous manifolds to develop a geometrically consistent theory of neural networks", "method": "detailing the geometric properties of the layers and how the maps between layers interact with such structures to make Cartan Neural Networks covariant and geometrically interpretable", "result": "An initial implementation of these concepts has been presented in a twin paper under the moniker of Cartan Neural Networks, showing both the feasibility and the performance of these geometric concepts in a machine learning context", "conclusion": "Together, these twin papers constitute a first step towards a fully geometrically interpretable theory of neural networks exploiting group-theoretic structures"}}
{"id": "2507.16989", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.16989", "abs": "https://arxiv.org/abs/2507.16989", "authors": ["Giulio Pelosio", "Devesh Batra", "No\u00e9mie Bovey", "Robert Hankache", "Cristovao Iglesias", "Greig Cowan", "Raad Khraishi"], "title": "Obscured but Not Erased: Evaluating Nationality Bias in LLMs via Name-Based Bias Benchmarks", "comment": null, "summary": "Large Language Models (LLMs) can exhibit latent biases towards specific\nnationalities even when explicit demographic markers are not present. In this\nwork, we introduce a novel name-based benchmarking approach derived from the\nBias Benchmark for QA (BBQ) dataset to investigate the impact of substituting\nexplicit nationality labels with culturally indicative names, a scenario more\nreflective of real-world LLM applications. Our novel approach examines how this\nsubstitution affects both bias magnitude and accuracy across a spectrum of LLMs\nfrom industry leaders such as OpenAI, Google, and Anthropic. Our experiments\nshow that small models are less accurate and exhibit more bias compared to\ntheir larger counterparts. For instance, on our name-based dataset and in the\nambiguous context (where the correct choice is not revealed), Claude Haiku\nexhibited the worst stereotypical bias scores of 9%, compared to only 3.5% for\nits larger counterpart, Claude Sonnet, where the latter also outperformed it by\n117.7% in accuracy. Additionally, we find that small models retain a larger\nportion of existing errors in these ambiguous contexts. For example, after\nsubstituting names for explicit nationality references, GPT-4o retains 68% of\nthe error rate versus 76% for GPT-4o-mini, with similar findings for other\nmodel providers, in the ambiguous context. Our research highlights the stubborn\nresilience of biases in LLMs, underscoring their profound implications for the\ndevelopment and deployment of AI systems in diverse, global contexts.", "AI": {"tldr": "\u5373\u4f7f\u6ca1\u6709\u660e\u786e\u7684\u56fd\u7c4d\u4fe1\u606f\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ecd\u7136\u5b58\u5728\u504f\u89c1\uff0c\u5c0f\u578b\u6a21\u578b\u7684\u504f\u89c1\u95ee\u9898\u66f4\u4e25\u91cd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5373\u4f7f\u5728\u6ca1\u6709\u660e\u786e\u7684\u4eba\u53e3\u7edf\u8ba1\u6807\u8bb0\u7684\u60c5\u51b5\u4e0b\uff0c\u4e5f\u53ef\u80fd\u8868\u73b0\u51fa\u5bf9\u7279\u5b9a\u56fd\u7c4d\u7684\u6f5c\u5728\u504f\u89c1\u3002\u8be5\u7814\u7a76\u65e8\u5728\u8c03\u67e5\u7528\u6587\u5316\u6307\u793a\u6027\u540d\u79f0\u66ff\u6362\u660e\u786e\u7684\u56fd\u7c4d\u6807\u7b7e\u7684\u5f71\u54cd\uff0c\u56e0\u4e3a\u8fd9\u79cd\u60c5\u51b5\u66f4\u8d34\u8fd1\u771f\u5b9e\u7684 LLM \u5e94\u7528\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u540d\u79f0\u7684\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6e90\u81ea QA \u7684\u504f\u5dee\u57fa\u51c6\u6d4b\u8bd5 (BBQ) \u6570\u636e\u96c6\uff0c\u901a\u8fc7\u7528\u6587\u5316\u76f8\u5173\u7684\u540d\u79f0\u66ff\u6362\u660e\u786e\u7684\u56fd\u7c4d\u6807\u7b7e\u6765\u7814\u7a76\u5176\u5f71\u54cd\u3002", "result": "\u5c0f\u578b\u6a21\u578b\u4e0d\u5982\u5927\u578b\u6a21\u578b\u51c6\u786e\uff0c\u5e76\u4e14\u8868\u73b0\u51fa\u66f4\u5927\u7684\u504f\u5dee\u3002\u4f8b\u5982\uff0c\u5728\u57fa\u4e8e\u540d\u79f0\u7684\u6570\u636e\u96c6\u548c\u6a21\u7cca\u7684\u4e0a\u4e0b\u6587\u4e2d\uff08\u672a\u63ed\u793a\u6b63\u786e\u7684\u9009\u62e9\uff09\uff0cClaude Haiku \u8868\u73b0\u51fa\u6700\u5dee\u7684\u523b\u677f\u5370\u8c61\u504f\u5dee\u5206\u6570\u4e3a 9%\uff0c\u800c\u5176\u8f83\u5927\u7684\u5bf9\u5e94\u6a21\u578b Claude Sonnet \u4ec5\u4e3a 3.5%\uff0c\u540e\u8005\u5728\u51c6\u786e\u6027\u65b9\u9762\u4e5f\u4f18\u4e8e\u524d\u8005 117.7%\u3002\u6b64\u5916\uff0c\u6211\u4eec\u53d1\u73b0\u5c0f\u578b\u6a21\u578b\u5728\u8fd9\u4e9b\u6a21\u7cca\u7684\u4e0a\u4e0b\u6587\u4e2d\u4fdd\u7559\u4e86\u5927\u90e8\u5206\u73b0\u6709\u9519\u8bef\u3002\u4f8b\u5982\uff0c\u5728\u7528\u540d\u79f0\u66ff\u6362\u660e\u786e\u7684\u56fd\u7c4d\u5f15\u7528\u540e\uff0cGPT-4o \u4fdd\u7559\u4e86 68% \u7684\u9519\u8bef\u7387\uff0c\u800c GPT-4o-mini \u5219\u4e3a 76%\uff0c\u5176\u4ed6\u6a21\u578b\u63d0\u4f9b\u5546\u4e5f\u6709\u7c7b\u4f3c\u7684\u53d1\u73b0\u3002", "conclusion": "\u5c0f\u578b\u6a21\u578b\u5728\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u8f83\u5dee\uff0c\u5e76\u4e14\u6bd4\u5927\u578b\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u5927\u7684\u504f\u5dee\u3002\u5373\u4f7f\u5728\u7528\u6587\u5316\u76f8\u5173\u7684\u540d\u5b57\u66ff\u6362\u660e\u786e\u7684\u56fd\u7c4d\u6807\u7b7e\u540e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u4ecd\u7136\u5b58\u5728\u504f\u5dee\uff0c\u8fd9\u5bf9\u5728\u5168\u7403\u80cc\u666f\u4e0b\u5f00\u53d1\u548c\u90e8\u7f72\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u5177\u6709\u6df1\u8fdc\u7684\u5f71\u54cd\u3002"}}
{"id": "2507.17214", "categories": ["cs.AI", "cs.CY", "cs.NI", "cs.SY", "eess.SY", "I.2; B.8; C.2; I.5; J.7"], "pdf": "https://arxiv.org/pdf/2507.17214", "abs": "https://arxiv.org/abs/2507.17214", "authors": ["Amod Kant Agrawal"], "title": "Our Cars Can Talk: How IoT Brings AI to Vehicles", "comment": "3 pages, 1 figure; To appear in IEEE Computer (Nov 2025)", "summary": "Bringing AI to vehicles and enabling them as sensing platforms is key to\ntransforming maintenance from reactive to proactive. Now is the time to\nintegrate AI copilots that speak both languages: machine and driver. This\narticle offers a conceptual and technical perspective intended to spark\ninterdisciplinary dialogue and guide future research and development in\nintelligent vehicle systems, predictive maintenance, and AI-powered user\ninteraction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5173\u4e8e\u667a\u80fd\u8f66\u8f86\u3001\u9884\u6d4b\u6027\u7ef4\u62a4\u548c\u4eba\u5de5\u667a\u80fd\u7528\u6237\u4ea4\u4e92\u7684\u6982\u5ff5\u548c\u6280\u672f\u89c6\u89d2\uff0c\u65e8\u5728\u4fc3\u8fdb\u8de8\u5b66\u79d1\u5bf9\u8bdd\u548c\u6307\u5bfc\u672a\u6765\u53d1\u5c55\u3002", "motivation": "\u5c06\u4eba\u5de5\u667a\u80fd\u5f15\u5165\u8f66\u8f86\u5e76\u4f7f\u5176\u6210\u4e3a\u4f20\u611f\u5e73\u53f0\u662f\u5b9e\u73b0\u7ef4\u62a4\u4ece\u88ab\u52a8\u5230\u4e3b\u52a8\u8f6c\u53d8\u7684\u5173\u952e\u3002\u73b0\u5728\u662f\u5c06\u4eba\u5de5\u667a\u80fd\u526f\u9a7e\u9a76\u96c6\u6210\u5230\u8f66\u8f86\u4e2d\u7684\u65f6\u5019\u4e86\uff0c\u5b83\u53ef\u4ee5\u540c\u65f6\u7406\u89e3\u673a\u5668\u548c\u9a7e\u9a76\u5458\u7684\u8bed\u8a00\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6982\u5ff5\u548c\u6280\u672f\u89c6\u89d2\u3002", "result": "\u4e0d\u9002\u7528", "conclusion": "\u672c\u6587\u65e8\u5728\u4fc3\u8fdb\u667a\u80fd\u8f66\u8f86\u7cfb\u7edf\u3001\u9884\u6d4b\u6027\u7ef4\u62a4\u548c\u4eba\u5de5\u667a\u80fd\u7528\u6237\u4ea4\u4e92\u9886\u57df\u7684\u8de8\u5b66\u79d1\u5bf9\u8bdd\uff0c\u5e76\u6307\u5bfc\u672a\u6765\u7684\u7814\u7a76\u548c\u5f00\u53d1\u3002"}}
{"id": "2507.16861", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16861", "abs": "https://arxiv.org/abs/2507.16861", "authors": ["Xiang Li"], "title": "Look Before You Fuse: 2D-Guided Cross-Modal Alignment for Robust 3D Detection", "comment": null, "summary": "Integrating LiDAR and camera inputs into a unified Bird's-Eye-View (BEV)\nrepresentation is crucial for enhancing 3D perception capabilities of\nautonomous vehicles. However, current methods are often affected by\nmisalignment between camera and LiDAR features. This misalignment leads to\ninaccurate depth supervision in camera branch and erroneous fusion during\ncross-modal feature aggregation. The root cause of this misalignment lies in\nprojection errors, stemming from minor extrinsic calibration inaccuracies and\nrolling shutter effect of LiDAR during vehicle motion. In this work, our key\ninsight is that these projection errors are predominantly concentrated at\nobject-background boundaries, which are readily identified by 2D detectors.\nBased on this, our main motivation is to utilize 2D object priors to pre-align\ncross-modal features before fusion. To address local misalignment, we propose\nPrior Guided Depth Calibration (PGDC), which leverages 2D priors to correct\nlocal misalignment and preserve correct cross-modal feature pairs. To resolve\nglobal misalignment, we introduce Discontinuity Aware Geometric Fusion (DAGF)\nto process calibrated results from PGDC, suppressing noise and explicitly\nenhancing sharp transitions at object-background boundaries. To effectively\nutilize these transition-aware depth representations, we incorporate Structural\nGuidance Depth Modulator (SGDM), using a gated attention mechanism to\nefficiently fuse aligned depth and image features. Our proposed method achieves\nstate-of-the-art performance on nuScenes validation dataset, with its mAP and\nNDS reaching 71.5% and 73.6% respectively.", "AI": {"tldr": "This paper introduces a method to pre-align cross-modal features before fusion using 2D object priors to address the misalignment between camera and LiDAR features, which achieves state-of-the-art performance on nuScenes validation dataset.", "motivation": "utilize 2D object priors to pre-align cross-modal features before fusion", "method": "propose Prior Guided Depth Calibration (PGDC), Discontinuity Aware Geometric Fusion (DAGF) and Structural Guidance Depth Modulator (SGDM)", "result": "achieves state-of-the-art performance on nuScenes validation dataset, with its mAP and NDS reaching 71.5% and 73.6% respectively", "conclusion": "The proposed method achieves state-of-the-art performance on nuScenes validation dataset, with its mAP and NDS reaching 71.5% and 73.6% respectively."}}
{"id": "2507.17290", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.17290", "abs": "https://arxiv.org/abs/2507.17290", "authors": ["Li Kang", "Yuhan Zhao", "Li Chen"], "title": "Exploring the Potential of LLMs for Serendipity Evaluation in Recommender Systems", "comment": "RecSys2025", "summary": "Serendipity plays a pivotal role in enhancing user satisfaction within\nrecommender systems, yet its evaluation poses significant challenges due to its\ninherently subjective nature and conceptual ambiguity. Current algorithmic\napproaches predominantly rely on proxy metrics for indirect assessment, often\nfailing to align with real user perceptions, thus creating a gap. With large\nlanguage models (LLMs) increasingly revolutionizing evaluation methodologies\nacross various human annotation tasks, we are inspired to explore a core\nresearch proposition: Can LLMs effectively simulate human users for serendipity\nevaluation? To address this question, we conduct a meta-evaluation on two\ndatasets derived from real user studies in the e-commerce and movie domains,\nfocusing on three key aspects: the accuracy of LLMs compared to conventional\nproxy metrics, the influence of auxiliary data on LLM comprehension, and the\nefficacy of recently popular multi-LLM techniques. Our findings indicate that\neven the simplest zero-shot LLMs achieve parity with, or surpass, the\nperformance of conventional metrics. Furthermore, multi-LLM techniques and the\nincorporation of auxiliary data further enhance alignment with human\nperspectives. Based on our findings, the optimal evaluation by LLMs yields a\nPearson correlation coefficient of 21.5\\% when compared to the results of the\nuser study. This research implies that LLMs may serve as potentially accurate\nand cost-effective evaluators, introducing a new paradigm for serendipity\nevaluation in recommender systems.", "AI": {"tldr": "LLMs can effectively simulate human users for serendipity evaluation in recommender systems, outperforming traditional metrics.", "motivation": "Serendipity evaluation in recommender systems is challenging due to its subjective nature, and current metrics don't align with user perceptions. LLMs are revolutionizing evaluation methodologies.", "method": "Meta-evaluation on two datasets from e-commerce and movie domains, comparing LLMs to proxy metrics, assessing the impact of auxiliary data, and testing multi-LLM techniques.", "result": "Simplest zero-shot LLMs match or exceed conventional metrics. Multi-LLM techniques and auxiliary data improve alignment with human perspectives, achieving a 21.5% Pearson correlation with user study results.", "conclusion": "LLMs can serve as potentially accurate and cost-effective evaluators for serendipity in recommender systems."}}
{"id": "2507.16881", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16881", "abs": "https://arxiv.org/abs/2507.16881", "authors": ["Pengjiu Xia", "Yidian Huang", "Wenchao Wei", "Yuwen Tan"], "title": "Confidence Optimization for Probabilistic Encoding", "comment": null, "summary": "Probabilistic encoding introduces Gaussian noise into neural networks,\nenabling a smooth transition from deterministic to uncertain states and\nenhancing generalization ability. However, the randomness of Gaussian noise\ndistorts point-based distance measurements in classification tasks. To mitigate\nthis issue, we propose a confidence optimization probabilistic encoding (CPE)\nmethod that improves distance reliability and enhances representation learning.\nSpecifically, we refine probabilistic encoding with two key strategies: First,\nwe introduce a confidence-aware mechanism to adjust distance calculations,\nensuring consistency and reliability in probabilistic encoding classification\ntasks. Second, we replace the conventional KL divergence-based variance\nregularization, which relies on unreliable prior assumptions, with a simpler L2\nregularization term to directly constrain variance. The method we proposed is\nmodel-agnostic, and extensive experiments on natural language classification\ntasks demonstrate that our method significantly improves performance and\ngeneralization on both the BERT and the RoBERTa model.", "AI": {"tldr": "The paper introduces a confidence optimization probabilistic encoding (CPE) method to improve distance reliability and enhance representation learning in neural networks by using a confidence-aware mechanism and L2 regularization.", "motivation": "The randomness of Gaussian noise distorts point-based distance measurements in classification tasks when using probabilistic encoding.", "method": "The authors refine probabilistic encoding with a confidence-aware mechanism to adjust distance calculations and replace the conventional KL divergence-based variance regularization with a simpler L2 regularization term to directly constrain variance.", "result": "The proposed method significantly improves performance and generalization on natural language classification tasks.", "conclusion": "The proposed CPE method improves performance and generalization on both the BERT and the RoBERTa model."}}
{"id": "2507.17009", "categories": ["cs.CL", "cs.IR", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.17009", "abs": "https://arxiv.org/abs/2507.17009", "authors": ["Ming Huang", "Zehan Li", "Yan Hu", "Wanjing Wang", "Andrew Wen", "Scott Lane", "Salih Selek", "Lokesh Shahani", "Rodrigo Machado-Vieira", "Jair Soares", "Hua Xu", "Hongfang Liu"], "title": "Multi-Label Classification with Generative AI Models in Healthcare: A Case Study of Suicidality and Risk Factors", "comment": null, "summary": "Suicide remains a pressing global health crisis, with over 720,000 deaths\nannually and millions more affected by suicide ideation (SI) and suicide\nattempts (SA). Early identification of suicidality-related factors (SrFs),\nincluding SI, SA, exposure to suicide (ES), and non-suicidal self-injury\n(NSSI), is critical for timely intervention. While prior studies have applied\nAI to detect SrFs in clinical notes, most treat suicidality as a binary\nclassification task, overlooking the complexity of cooccurring risk factors.\nThis study explores the use of generative large language models (LLMs),\nspecifically GPT-3.5 and GPT-4.5, for multi-label classification (MLC) of SrFs\nfrom psychiatric electronic health records (EHRs). We present a novel end to\nend generative MLC pipeline and introduce advanced evaluation methods,\nincluding label set level metrics and a multilabel confusion matrix for error\nanalysis. Finetuned GPT-3.5 achieved top performance with 0.94 partial match\naccuracy and 0.91 F1 score, while GPT-4.5 with guided prompting showed superior\nperformance across label sets, including rare or minority label sets,\nindicating a more balanced and robust performance. Our findings reveal\nsystematic error patterns, such as the conflation of SI and SA, and highlight\nthe models tendency toward cautious over labeling. This work not only\ndemonstrates the feasibility of using generative AI for complex clinical\nclassification tasks but also provides a blueprint for structuring unstructured\nEHR data to support large scale clinical research and evidence based medicine.", "AI": {"tldr": "This paper uses generative LLMs (GPT-3.5 and GPT-4.5) for multi-label classification of suicidality-related factors in electronic health records, achieving high accuracy and revealing error patterns.", "motivation": "Early identification of suicidality-related factors (SrFs), including SI, SA, exposure to suicide (ES), and non-suicidal self-injury (NSSI), is critical for timely intervention. Prior studies have applied AI to detect SrFs in clinical notes, but most treat suicidality as a binary classification task, overlooking the complexity of cooccurring risk factors.", "method": "The study explores the use of generative large language models (LLMs), specifically GPT-3.5 and GPT-4.5, for multi-label classification (MLC) of SrFs from psychiatric electronic health records (EHRs). A novel end to end generative MLC pipeline is presented, along with advanced evaluation methods, including label set level metrics and a multilabel confusion matrix for error analysis.", "result": "Finetuned GPT-3.5 achieved top performance with 0.94 partial match accuracy and 0.91 F1 score, while GPT-4.5 with guided prompting showed superior performance across label sets, including rare or minority label sets, indicating a more balanced and robust performance. Systematic error patterns were revealed, such as the conflation of SI and SA, and highlight the models tendency toward cautious over labeling.", "conclusion": "This study demonstrates the feasibility of using generative AI for complex clinical classification tasks and provides a blueprint for structuring unstructured EHR data to support large scale clinical research and evidence based medicine."}}
{"id": "2507.17257", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.17257", "abs": "https://arxiv.org/abs/2507.17257", "authors": ["Elija Perrier", "Michael Timothy Bennett"], "title": "Agent Identity Evals: Measuring Agentic Identity", "comment": null, "summary": "Central to agentic capability and trustworthiness of language model agents\n(LMAs) is the extent they maintain stable, reliable, identity over time.\nHowever, LMAs inherit pathologies from large language models (LLMs)\n(statelessness, stochasticity, sensitivity to prompts and\nlinguistically-intermediation) which can undermine their identifiability,\ncontinuity, persistence and consistency. This attrition of identity can erode\ntheir reliability, trustworthiness and utility by interfering with their\nagentic capabilities such as reasoning, planning and action. To address these\nchallenges, we introduce \\textit{agent identity evals} (AIE), a rigorous,\nstatistically-driven, empirical framework for measuring the degree to which an\nLMA system exhibit and maintain their agentic identity over time, including\ntheir capabilities, properties and ability to recover from state perturbations.\nAIE comprises a set of novel metrics which can integrate with other measures of\nperformance, capability and agentic robustness to assist in the design of\noptimal LMA infrastructure and scaffolding such as memory and tools. We set out\nformal definitions and methods that can be applied at each stage of the LMA\nlife-cycle, and worked examples of how to apply them.", "AI": {"tldr": "This paper introduces a framework called agent identity evals (AIE) to measure and maintain the agentic identity of language model agents (LMAs) over time.", "motivation": "The identifiability, continuity, persistence and consistency of LMAs can be undermined by pathologies inherited from LLMs, which can erode their reliability, trustworthiness and utility.", "method": "This paper sets out formal definitions and methods that can be applied at each stage of the LMA life-cycle.", "result": "AIE comprises a set of novel metrics which can integrate with other measures of performance, capability and agentic robustness to assist in the design of optimal LMA infrastructure and scaffolding such as memory and tools.", "conclusion": "This paper introduces agent identity evals (AIE), a framework for measuring the degree to which an LMA system exhibit and maintain their agentic identity over time."}}
{"id": "2507.16863", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.16863", "abs": "https://arxiv.org/abs/2507.16863", "authors": ["Hongcheng Gao", "Zihao Huang", "Lin Xu", "Jingyi Tang", "Xinhao Li", "Yue Liu", "Haoyang Li", "Taihang Hu", "Minhua Lin", "Xinlong Yang", "Ge Wu", "Balong Bi", "Hongyu Chen", "Wentao Zhang"], "title": "Pixels, Patterns, but No Poetry: To See The World like Humans", "comment": null, "summary": "Achieving human-like perception and reasoning in Multimodal Large Language\nModels (MLLMs) remains a central challenge in artificial intelligence. While\nrecent research has primarily focused on enhancing reasoning capabilities in\nMLLMs, a fundamental question persists: Can Multimodal Large Language Models\ntruly perceive the world as humans do? This paper shifts focus from reasoning\nto perception. Rather than constructing benchmarks specifically for reasoning,\nwe introduce the Turing Eye Test (TET), a challenging perception-oriented\nbenchmark comprising four diagnostic tasks that evaluate MLLMs' performance on\nsynthetic images that humans process intuitively. Our findings reveal that\nstate-of-the-art MLLMs exhibit catastrophic failures on our perceptual tasks\ntrivial for humans. Both in-context learning and training on language\nbackbone-effective for previous benchmarks-fail to improve performance on our\ntasks, while fine-tuning the vision tower enables rapid adaptation, suggesting\nthat our benchmark poses challenges for vision tower generalization rather than\nfor the knowledge and reasoning capabilities of the language backbone-a key gap\nbetween current MLLMs and human perception. We release a representative subset\nof TET tasks in this version, and will introduce more diverse tasks and methods\nto enhance visual generalization in future work.", "AI": {"tldr": "This paper shifts focus from reasoning to perception and introduces a new benchmark to evaluate MLLMs' performance on synthetic images. The findings reveal that state-of-the-art MLLMs exhibit catastrophic failures on perceptual tasks trivial for humans.", "motivation": "Achieving human-like perception and reasoning in Multimodal Large Language Models (MLLMs) remains a central challenge. Recent research has primarily focused on enhancing reasoning capabilities in MLLMs, a fundamental question persists: Can Multimodal Large Language Models truly perceive the world as humans do?", "method": "introduce the Turing Eye Test (TET), a challenging perception-oriented benchmark comprising four diagnostic tasks that evaluate MLLMs' performance on synthetic images that humans process intuitively", "result": "Both in-context learning and training on language backbone-effective for previous benchmarks-fail to improve performance on our tasks", "conclusion": "state-of-the-art MLLMs exhibit catastrophic failures on perceptual tasks trivial for humans. Fine-tuning the vision tower enables rapid adaptation."}}
{"id": "2507.17323", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.17323", "abs": "https://arxiv.org/abs/2507.17323", "authors": ["Ruijie Yang", "Yan Zhu", "Peiyao Fu", "Yizhe Zhang", "Zhihua Wang", "Quanlin Li", "Pinghong Zhou", "Xian Yang", "Shuo Wang"], "title": "EndoFinder: Online Lesion Retrieval for Explainable Colorectal Polyp Diagnosis Leveraging Latent Scene Representations", "comment": null, "summary": "Colorectal cancer (CRC) remains a leading cause of cancer-related mortality,\nunderscoring the importance of timely polyp detection and diagnosis. While deep\nlearning models have improved optical-assisted diagnostics, they often demand\nextensive labeled datasets and yield \"black-box\" outputs with limited\ninterpretability. In this paper, we propose EndoFinder, an online polyp\nretrieval framework that leverages multi-view scene representations for\nexplainable and scalable CRC diagnosis. First, we develop a Polyp-aware Image\nEncoder by combining contrastive learning and a reconstruction task, guided by\npolyp segmentation masks. This self-supervised approach captures robust\nfeatures without relying on large-scale annotated data. Next, we treat each\npolyp as a three-dimensional \"scene\" and introduce a Scene Representation\nTransformer, which fuses multiple views of the polyp into a single latent\nrepresentation. By discretizing this representation through a hashing layer,\nEndoFinder enables real-time retrieval from a compiled database of historical\npolyp cases, where diagnostic information serves as interpretable references\nfor new queries. We evaluate EndoFinder on both public and newly collected\npolyp datasets for re-identification and pathology classification. Results show\nthat EndoFinder outperforms existing methods in accuracy while providing\ntransparent, retrieval-based insights for clinical decision-making. By\ncontributing a novel dataset and a scalable, explainable framework, our work\naddresses key challenges in polyp diagnosis and offers a promising direction\nfor more efficient AI-driven colonoscopy workflows. The source code is\navailable at https://github.com/ku262/EndoFinder-Scene.", "AI": {"tldr": "EndoFinder, an online polyp retrieval framework that leverages multi-view scene representations for explainable and scalable CRC diagnosis, outperforms existing methods in accuracy and offers a promising direction for more efficient AI-driven colonoscopy workflows.", "motivation": "Colorectal cancer (CRC) remains a leading cause of cancer-related mortality, underscoring the importance of timely polyp detection and diagnosis. While deep learning models have improved optical-assisted diagnostics, they often demand extensive labeled datasets and yield \"black-box\" outputs with limited interpretability.", "method": "develop a Polyp-aware Image Encoder by combining contrastive learning and a reconstruction task, guided by polyp segmentation masks and introduce a Scene Representation Transformer, which fuses multiple views of the polyp into a single latent representation. By discretizing this representation through a hashing layer, EndoFinder enables real-time retrieval from a compiled database of historical polyp cases", "result": "EndoFinder outperforms existing methods in accuracy while providing transparent, retrieval-based insights for clinical decision-making. By contributing a novel dataset and a scalable, explainable framework, our work addresses key challenges in polyp diagnosis and offers a promising direction for more efficient AI-driven colonoscopy workflows.", "conclusion": "EndoFinder outperforms existing methods in accuracy while providing transparent, retrieval-based insights for clinical decision-making, addressing key challenges in polyp diagnosis and offers a promising direction for more efficient AI-driven colonoscopy workflows."}}
{"id": "2507.16884", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16884", "abs": "https://arxiv.org/abs/2507.16884", "authors": ["Yi Guo", "Wei Wang", "Zhihang Yuan", "Rong Cao", "Kuan Chen", "Zhengyang Chen", "Yuanyuan Huo", "Yang Zhang", "Yuping Wang", "Shouda Liu", "Yuxuan Wang"], "title": "SplitMeanFlow: Interval Splitting Consistency in Few-Step Generative Modeling", "comment": "Tech Report", "summary": "Generative models like Flow Matching have achieved state-of-the-art\nperformance but are often hindered by a computationally expensive iterative\nsampling process. To address this, recent work has focused on few-step or\none-step generation by learning the average velocity field, which directly maps\nnoise to data. MeanFlow, a leading method in this area, learns this field by\nenforcing a differential identity that connects the average and instantaneous\nvelocities. In this work, we argue that this differential formulation is a\nlimiting special case of a more fundamental principle. We return to the first\nprinciples of average velocity and leverage the additivity property of definite\nintegrals. This leads us to derive a novel, purely algebraic identity we term\nInterval Splitting Consistency. This identity establishes a self-referential\nrelationship for the average velocity field across different time intervals\nwithout resorting to any differential operators. Based on this principle, we\nintroduce SplitMeanFlow, a new training framework that enforces this algebraic\nconsistency directly as a learning objective. We formally prove that the\ndifferential identity at the core of MeanFlow is recovered by taking the limit\nof our algebraic consistency as the interval split becomes infinitesimal. This\nestablishes SplitMeanFlow as a direct and more general foundation for learning\naverage velocity fields. From a practical standpoint, our algebraic approach is\nsignificantly more efficient, as it eliminates the need for JVP computations,\nresulting in simpler implementation, more stable training, and broader hardware\ncompatibility. One-step and two-step SplitMeanFlow models have been\nsuccessfully deployed in large-scale speech synthesis products (such as\nDoubao), achieving speedups of 20x.", "AI": {"tldr": "SplitMeanFlow \u662f\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u5229\u7528\u5b9a\u79ef\u5206\u7684\u52a0\u6027\u7279\u6027\uff0c\u63a8\u5bfc\u51fa\u4e00\u4e2a\u65b0\u7684\u3001\u7eaf\u4ee3\u6570\u7684\u6052\u7b49\u5f0f\uff0c\u79f0\u4e3a\u533a\u95f4\u5206\u5272\u4e00\u81f4\u6027\uff0c\u4ece\u800c\u5b66\u4e60\u5e73\u5747\u901f\u5ea6\u573a\u3002", "motivation": "\u50cf Flow Matching \u8fd9\u6837\u7684\u751f\u6210\u6a21\u578b\u5df2\u7ecf\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f46\u901a\u5e38\u53d7\u5230\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u8fed\u4ee3\u91c7\u6837\u8fc7\u7a0b\u7684\u963b\u788d\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6700\u8fd1\u7684\u5de5\u4f5c\u96c6\u4e2d\u4e8e\u901a\u8fc7\u5b66\u4e60\u5e73\u5747\u901f\u5ea6\u573a\u6765\u8fdb\u884cFew-step\u6216 one-step \u7684\u751f\u6210\uff0c\u5e73\u5747\u901f\u5ea6\u573a\u76f4\u63a5\u5c06\u566a\u58f0\u6620\u5c04\u5230\u6570\u636e\u3002", "method": "SplitMeanFlow\uff0c\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u5b83\u76f4\u63a5\u5c06\u8fd9\u79cd\u4ee3\u6570\u4e00\u81f4\u6027\u4f5c\u4e3a\u5b66\u4e60\u76ee\u6807\u6765\u6267\u884c\u3002", "result": "\u4e0e MeanFlow \u76f8\u6bd4\uff0cSplitMeanFlow \u662f\u4e00\u79cd\u66f4\u901a\u7528\u7684\u57fa\u7840\uff0c\u5e76\u4e14\u6548\u7387\u66f4\u9ad8\uff0c\u56e0\u4e3a\u5b83\u6d88\u9664\u4e86\u5bf9 JVP \u8ba1\u7b97\u7684\u9700\u6c42\uff0c\u4ece\u800c\u7b80\u5316\u4e86\u5b9e\u73b0\uff0c\u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3\u548c\u66f4\u5e7f\u6cdb\u7684\u786c\u4ef6\u517c\u5bb9\u6027\u3002", "conclusion": "SplitMeanFlow \u6a21\u578b\u5728\u5927\u578b\u8bed\u97f3\u5408\u6210\u4ea7\u54c1\u4e2d\u6210\u529f\u90e8\u7f72\uff0c\u5b9e\u73b0\u4e86 20 \u500d\u7684\u52a0\u901f\u3002"}}
{"id": "2507.17015", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17015", "abs": "https://arxiv.org/abs/2507.17015", "authors": ["Arduin Findeis", "Floris Weers", "Guoli Yin", "Ke Ye", "Ruoming Pang", "Tom Gunter"], "title": "Can External Validation Tools Improve Annotation Quality for LLM-as-a-Judge?", "comment": "Accepted at ACL 2025", "summary": "Pairwise preferences over model responses are widely collected to evaluate\nand provide feedback to large language models (LLMs). Given two alternative\nmodel responses to the same input, a human or AI annotator selects the \"better\"\nresponse. This approach can provide feedback for domains where other hard-coded\nmetrics are difficult to obtain (e.g., chat response quality), thereby helping\nmodel evaluation or training. However, for some domains high-quality pairwise\ncomparisons can be tricky to obtain - from AI and humans. For example, for\nresponses with many factual statements, annotators may disproportionately weigh\nwriting quality rather than underlying facts. In this work, we explore\naugmenting standard AI annotator systems with additional tools to improve\nperformance on three challenging response domains: long-form factual, math and\ncode tasks. We propose a tool-using agentic system to provide higher quality\nfeedback on these domains. Our system uses web-search and code execution to\nground itself based on external validation, independent of the LLM's internal\nknowledge and biases. We provide extensive experimental results evaluating our\nmethod across the three targeted response domains as well as general annotation\ntasks, using RewardBench (incl. AlpacaEval and LLMBar), RewardMath, as well as\nthree new datasets for domains with saturated pre-existing datasets. Our\nresults indicate that external tools can indeed improve performance in many,\nbut not all, cases. More generally, our experiments highlight the sensitivity\nof performance to simple parameters (e.g., prompt) and the need for improved\n(non-saturated) annotator benchmarks. We share our code at\nhttps://github.com/apple/ml-agent-evaluator.", "AI": {"tldr": "\u4f7f\u7528\u5de5\u5177\u6765\u589e\u5f3a\u6807\u51c6 AI \u6ce8\u91ca\u5668\u7cfb\u7edf\uff0c\u4ee5\u63d0\u9ad8\u5728\u957f\u7bc7\u4e8b\u5b9e\u3001\u6570\u5b66\u548c\u4ee3\u7801\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u67d0\u4e9b\u9886\u57df\uff0c\u9ad8\u8d28\u91cf\u7684\u6210\u5bf9\u6bd4\u8f83\u53ef\u80fd\u96be\u4ee5\u83b7\u5f97\u2014\u2014\u6765\u81ea AI \u548c\u4eba\u7c7b\u3002\u4f8b\u5982\uff0c\u5bf9\u4e8e\u5305\u542b\u8bb8\u591a\u4e8b\u5b9e\u9648\u8ff0\u7684\u56de\u590d\uff0c\u6ce8\u91ca\u8005\u53ef\u80fd\u4f1a\u4e0d\u6210\u6bd4\u4f8b\u5730\u6743\u8861\u5199\u4f5c\u8d28\u91cf\u800c\u4e0d\u662f\u6f5c\u5728\u7684\u4e8b\u5b9e\u3002", "method": "\u4f7f\u7528\u7f51\u7edc\u641c\u7d22\u548c\u4ee3\u7801\u6267\u884c\uff0c\u57fa\u4e8e\u5916\u90e8\u9a8c\u8bc1\u6765\u652f\u6301\u81ea\u8eab\uff0c\u72ec\u7acb\u4e8e LLM \u7684\u5185\u90e8\u77e5\u8bc6\u548c\u504f\u5dee\u3002", "result": "\u5916\u90e8\u5de5\u5177\u786e\u5b9e\u53ef\u4ee5\u5728\u8bb8\u591a\u60c5\u51b5\u4e0b\u63d0\u9ad8\u6027\u80fd\uff0c\u4f46\u5e76\u975e\u6240\u6709\u60c5\u51b5\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u5f3a\u8c03\u4e86\u6027\u80fd\u5bf9\u7b80\u5355\u53c2\u6570\uff08\u4f8b\u5982\uff0c\u63d0\u793a\uff09\u7684\u654f\u611f\u6027\uff0c\u4ee5\u53ca\u9700\u8981\u6539\u8fdb\u7684\uff08\u975e\u9971\u548c\uff09\u6ce8\u91ca\u5668\u57fa\u51c6\u3002", "conclusion": "\u5916\u90e8\u5de5\u5177\u53ef\u4ee5\u5728\u8bb8\u591a\u60c5\u51b5\u4e0b\u63d0\u9ad8\u6027\u80fd\uff0c\u4f46\u5e76\u975e\u6240\u6709\u60c5\u51b5\u3002\u6027\u80fd\u5bf9\u7b80\u5355\u53c2\u6570\uff08\u4f8b\u5982\uff0c\u63d0\u793a\uff09\u7684\u654f\u611f\u6027\uff0c\u4ee5\u53ca\u9700\u8981\u6539\u8fdb\u7684\uff08\u975e\u9971\u548c\uff09\u6ce8\u91ca\u5668\u57fa\u51c6\u3002"}}
{"id": "2507.17258", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17258", "abs": "https://arxiv.org/abs/2507.17258", "authors": ["Andreas Scholl", "Natalie Kiesler"], "title": "Students' Feedback Requests and Interactions with the SCRIPT Chatbot: Do They Get What They Ask For?", "comment": "Accepted at PPIG 2025", "summary": "Building on prior research on Generative AI (GenAI) and related tools for\nprogramming education, we developed SCRIPT, a chatbot based on ChatGPT-4o-mini,\nto support novice learners. SCRIPT allows for open-ended interactions and\nstructured guidance through predefined prompts. We evaluated the tool via an\nexperiment with 136 students from an introductory programming course at a large\nGerman university and analyzed how students interacted with SCRIPT while\nsolving programming tasks with a focus on their feedback preferences. The\nresults reveal that students' feedback requests seem to follow a specific\nsequence. Moreover, the chatbot responses aligned well with students' requested\nfeedback types (in 75%), and it adhered to the system prompt constraints. These\ninsights inform the design of GenAI-based learning support systems and\nhighlight challenges in balancing guidance and flexibility in AI-assisted\ntools.", "AI": {"tldr": "Developed and evaluated SCRIPT, a ChatGPT-4o-mini chatbot for novice programming learners, finding that feedback requests follow a sequence and the chatbot aligns well with requested feedback types.", "motivation": "Building on prior research on Generative AI (GenAI) and related tools for programming education, we developed SCRIPT, a chatbot based on ChatGPT-4o-mini, to support novice learners. SCRIPT allows for open-ended interactions and structured guidance through predefined prompts.", "method": "We evaluated the tool via an experiment with 136 students from an introductory programming course at a large German university and analyzed how students interacted with SCRIPT while solving programming tasks with a focus on their feedback preferences.", "result": "The results reveal that students' feedback requests seem to follow a specific sequence. Moreover, the chatbot responses aligned well with students' requested feedback types (in 75%), and it adhered to the system prompt constraints.", "conclusion": "The study's insights inform the design of GenAI-based learning support systems and highlight challenges in balancing guidance and flexibility in AI-assisted tools."}}
{"id": "2507.16873", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16873", "abs": "https://arxiv.org/abs/2507.16873", "authors": ["Jeongeun Lee", "Youngjae Yu", "Dongha Lee"], "title": "HIPPO-Video: Simulating Watch Histories with Large Language Models for Personalized Video Highlighting", "comment": "Accepted to COLM2025", "summary": "The exponential growth of video content has made personalized video\nhighlighting an essential task, as user preferences are highly variable and\ncomplex. Existing video datasets, however, often lack personalization, relying\non isolated videos or simple text queries that fail to capture the intricacies\nof user behavior. In this work, we introduce HIPPO-Video, a novel dataset for\npersonalized video highlighting, created using an LLM-based user simulator to\ngenerate realistic watch histories reflecting diverse user preferences. The\ndataset includes 2,040 (watch history, saliency score) pairs, covering 20,400\nvideos across 170 semantic categories. To validate our dataset, we propose\nHiPHer, a method that leverages these personalized watch histories to predict\npreference-conditioned segment-wise saliency scores. Through extensive\nexperiments, we demonstrate that our method outperforms existing generic and\nquery-based approaches, showcasing its potential for highly user-centric video\nhighlighting in real-world scenarios.", "AI": {"tldr": "HIPPO-Video\u662f\u4e00\u4e2a\u7528\u4e8e\u4e2a\u6027\u5316\u89c6\u9891\u9ad8\u4eae\u663e\u793a\u7684\u65b0\u6570\u636e\u96c6\uff0c\u5b83\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u7528\u6237\u6a21\u62df\u5668\u751f\u6210\u771f\u5b9e\u7684\u89c2\u770b\u5386\u53f2\uff0c\u53cd\u6620\u4e86\u4e0d\u540c\u7684\u7528\u6237\u504f\u597d\u3002", "motivation": "\u7528\u6237\u504f\u597d\u662f\u9ad8\u5ea6\u53ef\u53d8\u7684\u548c\u590d\u6742\u7684\uff0c\u56e0\u6b64\u4e2a\u6027\u5316\u89c6\u9891\u9ad8\u4eae\u663e\u793a\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u89c6\u9891\u6570\u636e\u96c6\u901a\u5e38\u7f3a\u4e4f\u4e2a\u6027\u5316\uff0c\u4f9d\u8d56\u4e8e\u5b64\u7acb\u7684\u89c6\u9891\u6216\u7b80\u5355\u7684\u6587\u672c\u67e5\u8be2\uff0c\u65e0\u6cd5\u6355\u6349\u7528\u6237\u884c\u4e3a\u7684\u590d\u6742\u6027\u3002", "method": "\u63d0\u51fa\u4e86HiPHer\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u4e2a\u6027\u5316\u7684\u89c2\u770b\u5386\u53f2\u6765\u9884\u6d4b\u504f\u597d\u6761\u4ef6\u4e0b\u7684\u5206\u6bb5\u663e\u8457\u6027\u5f97\u5206\u3002", "result": "\u8be5\u6570\u636e\u96c6\u5305\u62ec2,040\u4e2a\uff08\u89c2\u770b\u5386\u53f2\uff0c\u663e\u7740\u6027\u5f97\u5206\uff09\u5bf9\uff0c\u6db5\u76d6170\u4e2a\u8bed\u4e49\u7c7b\u522b\u768420,400\u4e2a\u89c6\u9891\u3002HiPHer\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u901a\u7528\u548c\u57fa\u4e8e\u67e5\u8be2\u7684\u65b9\u6cd5\u3002", "conclusion": "HiPHer\u65b9\u6cd5\u5728\u7528\u6237\u4e2d\u5fc3\u89c6\u9891\u9ad8\u4eae\u663e\u793a\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.17356", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.17356", "abs": "https://arxiv.org/abs/2507.17356", "authors": ["Viet-Tran Anh", "Bruno Sguerra", "Gabriel Meseguer-Brocal", "Lea Briand", "Manuel Moussallam"], "title": "\"Beyond the past\": Leveraging Audio and Human Memory for Sequential Music Recommendation", "comment": null, "summary": "On music streaming services, listening sessions are often composed of a\nbalance of familiar and new tracks. Recently, sequential recommender systems\nhave adopted cognitive-informed approaches, such as Adaptive Control of\nThought-Rational (ACT-R), to successfully improve the prediction of the most\nrelevant tracks for the next user session. However, one limitation of using a\nmodel inspired by human memory (or the past), is that it struggles to recommend\nnew tracks that users have not previously listened to. To bridge this gap, here\nwe propose a model that leverages audio information to predict in advance the\nACT-R-like activation of new tracks and incorporates them into the\nrecommendation scoring process. We demonstrate the empirical effectiveness of\nthe proposed model using proprietary data, which we publicly release along with\nthe model's source code to foster future research in this field.", "AI": {"tldr": "propose a model that leverages audio information to predict in advance the ACT-R-like activation of new tracks", "motivation": "one limitation of using a model inspired by human memory (or the past), is that it struggles to recommend new tracks that users have not previously listened to. To bridge this gap", "method": "propose a model that leverages audio information to predict in advance the ACT-R-like activation of new tracks and incorporates them into the recommendation scoring process.", "result": "improve the prediction of the most relevant tracks for the next user session.", "conclusion": "demonstrates the empirical effectiveness of the proposed model using proprietary data, which we publicly release along with the model's source code to foster future research in this field."}}
{"id": "2507.16933", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.16933", "abs": "https://arxiv.org/abs/2507.16933", "authors": ["Steven K. Esser", "Jeffrey L. McKinstry", "Deepika Bablani", "Rathinakumar Appuswamy", "Dharmendra S. Modha"], "title": "SiLQ: Simple Large Language Model Quantization-Aware Training", "comment": "12 pages, 3 figures", "summary": "Large language models can be quantized to reduce inference time latency,\nmodel size, and energy consumption, thereby delivering a better user experience\nat lower cost. A challenge exists to deliver quantized models with minimal loss\nof accuracy in reasonable time, and in particular to do so without requiring\nmechanisms incompatible with specialized inference accelerators. Here, we\ndemonstrate a simple, end-to-end quantization-aware training approach that,\nwith an increase in total model training budget of less than 0.1%, outperforms\nthe leading published quantization methods by large margins on several modern\nbenchmarks, with both base and instruct model variants. The approach easily\ngeneralizes across different model architectures, can be applied to\nactivations, cache, and weights, and requires the introduction of no additional\noperations to the model other than the quantization itself.", "AI": {"tldr": "The paper introduces a quantization-aware training approach that improves accuracy and efficiency of large language models with minimal overhead and broad applicability.", "motivation": "Large language models can be quantized to reduce inference time latency, model size, and energy consumption, thereby delivering a better user experience at lower cost. A challenge exists to deliver quantized models with minimal loss of accuracy in reasonable time, and in particular to do so without requiring mechanisms incompatible with specialized inference accelerators.", "method": "a simple, end-to-end quantization-aware training approach", "result": "outperforms the leading published quantization methods by large margins on several modern benchmarks, with both base and instruct model variants. The approach easily generalizes across different model architectures, can be applied to activations, cache, and weights, and requires the introduction of no additional operations to the model other than the quantization itself.", "conclusion": "A simple, end-to-end quantization-aware training approach outperforms the leading published quantization methods by large margins on several modern benchmarks."}}
{"id": "2507.17025", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17025", "abs": "https://arxiv.org/abs/2507.17025", "authors": ["Soumen Sinha", "Shahryar Rahnamayan", "Azam Asilian Bidgoli"], "title": "Evolutionary Feature-wise Thresholding for Binary Representation of NLP Embeddings", "comment": null, "summary": "Efficient text embedding is crucial for large-scale natural language\nprocessing (NLP) applications, where storage and computational efficiency are\nkey concerns. In this paper, we explore how using binary representations\n(barcodes) instead of real-valued features can be used for NLP embeddings\nderived from machine learning models such as BERT. Thresholding is a common\nmethod for converting continuous embeddings into binary representations, often\nusing a fixed threshold across all features. We propose a Coordinate\nSearch-based optimization framework that instead identifies the optimal\nthreshold for each feature, demonstrating that feature-specific thresholds lead\nto improved performance in binary encoding. This ensures that the binary\nrepresentations are both accurate and efficient, enhancing performance across\nvarious features. Our optimal barcode representations have shown promising\nresults in various NLP applications, demonstrating their potential to transform\ntext representation. We conducted extensive experiments and statistical tests\non different NLP tasks and datasets to evaluate our approach and compare it to\nother thresholding methods. Binary embeddings generated using using optimal\nthresholds found by our method outperform traditional binarization methods in\naccuracy. This technique for generating binary representations is versatile and\ncan be applied to any features, not just limited to NLP embeddings, making it\nuseful for a wide range of domains in machine learning applications.", "AI": {"tldr": "This paper introduces a coordinate search-based optimization framework that identifies the optimal threshold for each feature, enhancing performance across various features in binary representations for NLP embeddings.", "motivation": "Efficient text embedding is crucial for large-scale NLP applications, where storage and computational efficiency are key concerns. Using binary representations (barcodes) instead of real-valued features can be used for NLP embeddings.", "method": "A coordinate search-based optimization framework is used to identify the optimal threshold for each feature in NLP embeddings.", "result": "Optimal barcode representations show promising results in various NLP applications, outperforming traditional binarization methods in accuracy. The technique is versatile and can be applied to any features, not just limited to NLP embeddings.", "conclusion": "The proposed coordinate search-based optimization framework identifies optimal feature-specific thresholds for binary encoding, leading to improved performance in various NLP applications."}}
{"id": "2507.17289", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17289", "abs": "https://arxiv.org/abs/2507.17289", "authors": ["Shitong Zhu", "Chenhao Fang", "Derek Larson", "Neel Reddy Pochareddy", "Rajeev Rao", "Sophie Zeng", "Yanqing Peng", "Wendy Summer", "Alex Goncalves", "Arya Pudota", "Herve Robert"], "title": "Compliance Brain Assistant: Conversational Agentic AI for Assisting Compliance Tasks in Enterprise Environments", "comment": null, "summary": "This paper presents Compliance Brain Assistant (CBA), a conversational,\nagentic AI assistant designed to boost the efficiency of daily compliance tasks\nfor personnel in enterprise environments. To strike a good balance between\nresponse quality and latency, we design a user query router that can\nintelligently choose between (i) FastTrack mode: to handle simple requests that\nonly need additional relevant context retrieved from knowledge corpora; and\n(ii) FullAgentic mode: to handle complicated requests that need composite\nactions and tool invocations to proactively discover context across various\ncompliance artifacts, and/or involving other APIs/models for accommodating\nrequests. A typical example would be to start with a user query, use its\ndescription to find a specific entity and then use the entity's information to\nquery other APIs for curating and enriching the final AI response.\n  Our experimental evaluations compared CBA against an out-of-the-box LLM on\nvarious real-world privacy/compliance-related queries targeting various\npersonas. We found that CBA substantially improved upon the vanilla LLM's\nperformance on metrics such as average keyword match rate (83.7% vs. 41.7%) and\nLLM-judge pass rate (82.0% vs. 20.0%). We also compared metrics for the full\nrouting-based design against the `fast-track only` and `full-agentic` modes and\nfound that it had a better average match-rate and pass-rate while keeping the\nrun-time approximately the same. This finding validated our hypothesis that the\nrouting mechanism leads to a good trade-off between the two worlds.", "AI": {"tldr": "CBA\u901a\u8fc7\u667a\u80fd\u8def\u7531\u5728\u54cd\u5e94\u8d28\u91cf\u548c\u5ef6\u8fdf\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4f01\u4e1a\u5408\u89c4\u4efb\u52a1\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u65e8\u5728\u63d0\u9ad8\u4f01\u4e1a\u73af\u5883\u4e2d\u4eba\u5458\u65e5\u5e38\u5408\u89c4\u4efb\u52a1\u7684\u6548\u7387\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7528\u6237\u67e5\u8be2\u8def\u7531\u5668\uff0c\u53ef\u4ee5\u5728\u5feb\u901f\u901a\u9053\u6a21\u5f0f\u548c\u5b8c\u5168\u4ee3\u7406\u6a21\u5f0f\u4e4b\u95f4\u667a\u80fd\u9009\u62e9\uff0c\u4ee5\u5e73\u8861\u54cd\u5e94\u8d28\u91cf\u548c\u5ef6\u8fdf\u3002", "result": "CBA\u5728\u5e73\u5747\u5173\u952e\u8bcd\u5339\u914d\u7387\uff0883.7% vs. 41.7%\uff09\u548cLLM-judge\u901a\u8fc7\u7387\uff0882.0% vs. 20.0%\uff09\u4e0a\u663e\u8457\u4f18\u4e8e\u539f\u59cbLLM\u3002", "conclusion": "CBA\u663e\u8457\u63d0\u5347\u4e86\u5408\u89c4\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5e76\u5728\u5e73\u5747\u5173\u952e\u8bcd\u5339\u914d\u7387\u548cLLM-judge\u901a\u8fc7\u7387\u4e0a\u4f18\u4e8e\u539f\u59cbLLM\u3002\u8def\u7531\u673a\u5236\u5728\u4e24\u79cd\u6a21\u5f0f\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5e73\u8861\u3002"}}
{"id": "2507.16877", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.16877", "abs": "https://arxiv.org/abs/2507.16877", "authors": ["Yizhi Hu", "Zezhao Tian", "Xingqun Qi", "Chen Su", "Bingkun Yang", "Junhui Yin", "Muyi Sun", "Man Zhang", "Zhenan Sun"], "title": "ReMeREC: Relation-aware and Multi-entity Referring Expression Comprehension", "comment": "15 pages, 7 figures", "summary": "Referring Expression Comprehension (REC) aims to localize specified entities\nor regions in an image based on natural language descriptions. While existing\nmethods handle single-entity localization, they often ignore complex\ninter-entity relationships in multi-entity scenes, limiting their accuracy and\nreliability. Additionally, the lack of high-quality datasets with fine-grained,\npaired image-text-relation annotations hinders further progress. To address\nthis challenge, we first construct a relation-aware, multi-entity REC dataset\ncalled ReMeX, which includes detailed relationship and textual annotations. We\nthen propose ReMeREC, a novel framework that jointly leverages visual and\ntextual cues to localize multiple entities while modeling their\ninter-relations. To address the semantic ambiguity caused by implicit entity\nboundaries in language, we introduce the Text-adaptive Multi-entity Perceptron\n(TMP), which dynamically infers both the quantity and span of entities from\nfine-grained textual cues, producing distinctive representations. Additionally,\nour Entity Inter-relationship Reasoner (EIR) enhances relational reasoning and\nglobal scene understanding. To further improve language comprehension for\nfine-grained prompts, we also construct a small-scale auxiliary dataset,\nEntityText, generated using large language models. Experiments on four\nbenchmark datasets show that ReMeREC achieves state-of-the-art performance in\nmulti-entity grounding and relation prediction, outperforming existing\napproaches by a large margin.", "AI": {"tldr": "\u63d0\u51fa\u4e86ReMeREC\u6846\u67b6\u548cReMeX\u6570\u636e\u96c6\uff0c\u4ee5\u89e3\u51b3\u591a\u5b9e\u4f53\u573a\u666f\u4e2d\u5b9e\u4f53\u95f4\u5173\u7cfb\u7684\u95ee\u9898\uff0c\u5e76\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u5904\u7406\u5355\u5b9e\u4f53\u5b9a\u4f4d\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u5ffd\u7565\u591a\u5b9e\u4f53\u573a\u666f\u4e2d\u590d\u6742\u7684\u5b9e\u4f53\u95f4\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\u4e0e\u7ec6\u7c92\u5ea6\u7684\u3001\u914d\u5bf9\u7684\u56fe\u50cf-\u6587\u672c-\u5173\u7cfb\u6ce8\u91ca\u963b\u788d\u4e86\u8fdb\u4e00\u6b65\u7684\u8fdb\u5c55\u3002", "method": "\u63d0\u51fa\u4e86ReMeREC\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u8054\u5408\u5229\u7528\u89c6\u89c9\u548c\u6587\u672c\u7ebf\u7d22\u6765\u5b9a\u4f4d\u591a\u4e2a\u5b9e\u4f53\uff0c\u540c\u65f6\u5bf9\u5b83\u4eec\u7684\u76f8\u4e92\u5173\u7cfb\u8fdb\u884c\u5efa\u6a21\u3002\u5f15\u5165\u4e86\u6587\u672c\u81ea\u9002\u5e94\u591a\u5b9e\u4f53\u611f\u77e5\u5668(TMP)\uff0c\u52a8\u6001\u5730\u4ece\u7ec6\u7c92\u5ea6\u7684\u6587\u672c\u7ebf\u7d22\u4e2d\u63a8\u65ad\u5b9e\u4f53\u7684\u6570\u91cf\u548c\u8303\u56f4\uff0c\u4ece\u800c\u4ea7\u751f\u72ec\u7279\u7684\u8868\u793a\u3002\u5b9e\u4f53\u95f4\u5173\u7cfb\u63a8\u7406\u5668(EIR)\u589e\u5f3a\u4e86\u5173\u7cfb\u63a8\u7406\u548c\u5168\u5c40\u573a\u666f\u7406\u89e3\u3002\u6784\u5efa\u4e86\u4e00\u4e2a\u5c0f\u89c4\u6a21\u7684\u8f85\u52a9\u6570\u636e\u96c6EntityText\uff0c\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u3002", "result": "ReMeREC\u5728\u591a\u5b9e\u4f53\u5b9a\u4f4d\u548c\u5173\u7cfb\u9884\u6d4b\u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5927\u5e45\u8d85\u8fc7\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ReMeREC\u5728\u591a\u5b9e\u4f53\u5b9a\u4f4d\u548c\u5173\u7cfb\u9884\u6d4b\u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5927\u5e45\u8d85\u8fc7\u4e86\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.17603", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17603", "abs": "https://arxiv.org/abs/2507.17603", "authors": ["Conor McNamara", "Effirul Ramlan"], "title": "Citation Recommendation using Deep Canonical Correlation Analysis", "comment": "21 pages, 6 figures, 7 tables", "summary": "Recent advances in citation recommendation have improved accuracy by\nleveraging multi-view representation learning to integrate the various\nmodalities present in scholarly documents. However, effectively combining\nmultiple data views requires fusion techniques that can capture complementary\ninformation while preserving the unique characteristics of each modality. We\npropose a novel citation recommendation algorithm that improves upon linear\nCanonical Correlation Analysis (CCA) methods by applying Deep CCA (DCCA), a\nneural network extension capable of capturing complex, non-linear relationships\nbetween distributed textual and graph-based representations of scientific\narticles. Experiments on the large-scale DBLP (Digital Bibliography & Library\nProject) citation network dataset demonstrate that our approach outperforms\nstate-of-the-art CCA-based methods, achieving relative improvements of over 11%\nin Mean Average Precision@10, 5% in Precision@10, and 7% in Recall@10. These\ngains reflect more relevant citation recommendations and enhanced ranking\nquality, suggesting that DCCA's non-linear transformations yield more\nexpressive latent representations than CCA's linear projections.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f15\u6587\u63a8\u8350\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u901a\u8fc7\u5e94\u7528\u6df1\u5ea6\u5178\u578b\u76f8\u5173\u5206\u6790 (DCCA) \u6539\u8fdb\u4e86\u7ebf\u6027\u5178\u578b\u76f8\u5173\u5206\u6790 (CCA) \u65b9\u6cd5\uff0c\u6df1\u5ea6\u5178\u578b\u76f8\u5173\u5206\u6790 (DCCA) \u662f\u4e00\u79cd\u795e\u7ecf\u7f51\u7edc\u6269\u5c55\uff0c\u80fd\u591f\u6355\u83b7\u79d1\u5b66\u6587\u7ae0\u7684\u5206\u5e03\u5f0f\u6587\u672c\u548c\u57fa\u4e8e\u56fe\u7684\u8868\u793a\u4e4b\u95f4\u590d\u6742\u7684\u975e\u7ebf\u6027\u5173\u7cfb", "motivation": "\u6709\u6548\u5730\u7ed3\u5408\u591a\u4e2a\u6570\u636e\u89c6\u56fe\u9700\u8981\u878d\u5408\u6280\u672f\uff0c\u8fd9\u4e9b\u6280\u672f\u53ef\u4ee5\u6355\u83b7\u4e92\u8865\u4fe1\u606f\uff0c\u540c\u65f6\u4fdd\u7559\u6bcf\u4e2a\u6a21\u6001\u7684\u72ec\u7279\u7279\u5f81", "method": "\u5e94\u7528\u6df1\u5ea6\u5178\u578b\u76f8\u5173\u5206\u6790 (DCCA)\uff0c\u8fd9\u662f\u4e00\u79cd\u795e\u7ecf\u7f51\u7edc\u6269\u5c55\uff0c\u80fd\u591f\u6355\u83b7\u79d1\u5b66\u6587\u7ae0\u7684\u5206\u5e03\u5f0f\u6587\u672c\u548c\u57fa\u4e8e\u56fe\u7684\u8868\u793a\u4e4b\u95f4\u590d\u6742\u7684\u975e\u7ebf\u6027\u5173\u7cfb", "result": "\u5728\u5927\u578b DBLP (\u6570\u5b57\u4e66\u76ee\u548c\u56fe\u4e66\u9986\u9879\u76ee) \u5f15\u7528\u7f51\u7edc\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u4e8e CCA \u7684\u65b9\u6cd5\uff0c\u5728 Mean Average Precision@10 \u65b9\u9762\u5b9e\u73b0\u4e86\u8d85\u8fc7 11% \u7684\u76f8\u5bf9\u6539\u8fdb\uff0c\u5728 Precision@10 \u65b9\u9762\u5b9e\u73b0\u4e86 5% \u7684\u6539\u8fdb\uff0c\u5728 Recall@10 \u65b9\u9762\u5b9e\u73b0\u4e86 7% \u7684\u6539\u8fdb", "conclusion": "DCCA\u7684\u975e\u7ebf\u6027\u53d8\u6362\u6bd4CCA\u7684\u7ebf\u6027\u6295\u5f71\u4ea7\u751f\u66f4\u5177\u8868\u73b0\u529b\u7684\u6f5c\u5728\u8868\u793a"}}
{"id": "2507.16983", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16983", "abs": "https://arxiv.org/abs/2507.16983", "authors": ["Sonny T. Jones", "Grange M. Simpson", "Patrick M. Pilarski", "Ashley N. Dalrymple"], "title": "Hierarchical Reinforcement Learning Framework for Adaptive Walking Control Using General Value Functions of Lower-Limb Sensor Signals", "comment": "5 pages, 3 figures, accepted at the 6th Multi-disciplinary Conference\n  on Reinforcement Learning and Decision Making (RLDM2025), June 11-14, 2025", "summary": "Rehabilitation technology is a natural setting to study the shared learning\nand decision-making of human and machine agents. In this work, we explore the\nuse of Hierarchical Reinforcement Learning (HRL) to develop adaptive control\nstrategies for lower-limb exoskeletons, aiming to enhance mobility and autonomy\nfor individuals with motor impairments. Inspired by prominent models of\nbiological sensorimotor processing, our investigated HRL approach breaks down\nthe complex task of exoskeleton control adaptation into a higher-level\nframework for terrain strategy adaptation and a lower-level framework for\nproviding predictive information; this latter element is implemented via the\ncontinual learning of general value functions (GVFs). GVFs generated temporal\nabstractions of future signal values from multiple wearable lower-limb sensors,\nincluding electromyography, pressure insoles, and goniometers. We investigated\ntwo methods for incorporating actual and predicted sensor signals into a policy\nnetwork with the intent to improve the decision-making capacity of the control\nsystem of a lower-limb exoskeleton during ambulation across varied terrains. As\na key result, we found that the addition of predictions made from GVFs\nincreased overall network accuracy. Terrain-specific performance increases were\nseen while walking on even ground, uneven ground, up and down ramps, and turns,\nterrains that are often misclassified without predictive information. This\nsuggests that predictive information can aid decision-making during\nuncertainty, e.g., on terrains that have a high chance of being misclassified.\nThis work, therefore, contributes new insights into the nuances of HRL and the\nfuture development of exoskeletons to facilitate safe transitioning and\ntraversing across different walking environments.", "AI": {"tldr": "HRL approach breaks down the complex task of exoskeleton control adaptation into a higher-level framework for terrain strategy adaptation and a lower-level framework for providing predictive information", "motivation": "enhance mobility and autonomy for individuals with motor impairments", "method": "use of Hierarchical Reinforcement Learning (HRL) to develop adaptive control strategies", "result": "the addition of predictions made from GVFs increased overall network accuracy. Terrain-specific performance increases were seen while walking on even ground, uneven ground, up and down ramps, and turns, terrains that are often misclassified without predictive information.", "conclusion": "This work contributes new insights into the nuances of HRL and the future development of exoskeletons to facilitate safe transitioning and traversing across different walking environments."}}
{"id": "2507.17147", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.17147", "abs": "https://arxiv.org/abs/2507.17147", "authors": ["Cheng Liu", "Yifei Lu", "Fanghua Ye", "Jian Li", "Xingyu Chen", "Feiliang Ren", "Zhaopeng Tu", "Xiaolong Li"], "title": "CogDual: Enhancing Dual Cognition of LLMs via Reinforcement Learning with Implicit Rule-Based Rewards", "comment": null, "summary": "Role-Playing Language Agents (RPLAs) have emerged as a significant\napplication direction for Large Language Models (LLMs). Existing approaches\ntypically rely on prompt engineering or supervised fine-tuning to enable models\nto imitate character behaviors in specific scenarios, but often neglect the\nunderlying \\emph{cognitive} mechanisms driving these behaviors. Inspired by\ncognitive psychology, we introduce \\textbf{CogDual}, a novel RPLA adopting a\n\\textit{cognize-then-respond } reasoning paradigm. By jointly modeling external\nsituational awareness and internal self-awareness, CogDual generates responses\nwith improved character consistency and contextual alignment. To further\noptimize the performance, we employ reinforcement learning with two\ngeneral-purpose reward schemes designed for open-domain text generation.\nExtensive experiments on the CoSER benchmark, as well as Cross-MR and\nLifeChoice, demonstrate that CogDual consistently outperforms existing\nbaselines and generalizes effectively across diverse role-playing tasks.", "AI": {"tldr": "CogDual \u662f\u4e00\u79cd\u65b0\u9896\u7684\u89d2\u8272\u626e\u6f14\u8bed\u8a00\u4ee3\u7406\uff0c\u5b83\u91c7\u7528\u201c\u8ba4\u77e5-\u7136\u540e-\u54cd\u5e94\u201d\u7684\u63a8\u7406\u8303\u5f0f\uff0c\u901a\u8fc7\u5171\u540c\u5efa\u6a21\u5916\u90e8\u60c5\u5883\u610f\u8bc6\u548c\u5185\u90e8\u81ea\u6211\u610f\u8bc6\u6765\u751f\u6210\u54cd\u5e94\uff0c\u4ece\u800c\u63d0\u9ad8\u89d2\u8272\u4e00\u81f4\u6027\u548c\u4e0a\u4e0b\u6587\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u63d0\u793a\u5de5\u7a0b\u6216\u76d1\u7763\u5fae\u8c03\u6765\u4f7f\u6a21\u578b\u6a21\u4eff\u7279\u5b9a\u573a\u666f\u4e2d\u7684\u89d2\u8272\u884c\u4e3a\uff0c\u4f46\u901a\u5e38\u5ffd\u7565\u9a71\u52a8\u8fd9\u4e9b\u884c\u4e3a\u7684\u6f5c\u5728\u8ba4\u77e5\u673a\u5236\u3002", "method": "\u91c7\u7528\u201c\u8ba4\u77e5-\u7136\u540e-\u54cd\u5e94\u201d\u7684\u63a8\u7406\u8303\u5f0f\uff0c\u5171\u540c\u5efa\u6a21\u5916\u90e8\u60c5\u5883\u610f\u8bc6\u548c\u5185\u90e8\u81ea\u6211\u610f\u8bc6\u3002", "result": "CogDual \u751f\u6210\u7684\u54cd\u5e94\u5177\u6709\u6539\u8fdb\u7684\u89d2\u8272\u4e00\u81f4\u6027\u548c\u4e0a\u4e0b\u6587\u5bf9\u9f50\u3002", "conclusion": "CogDual\u5728\u591a\u4e2a\u89d2\u8272\u626e\u6f14\u4efb\u52a1\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5e76\u4e14\u80fd\u591f\u6709\u6548\u5730\u6cdb\u5316\u3002"}}
{"id": "2507.17418", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17418", "abs": "https://arxiv.org/abs/2507.17418", "authors": ["Joobin Jin", "Seokjun Hong", "Gyeongseon Baek", "Yeeun Kim", "Byeongjoon Noh"], "title": "Ctx2TrajGen: Traffic Context-Aware Microscale Vehicle Trajectories using Generative Adversarial Imitation Learning", "comment": null, "summary": "Precise modeling of microscopic vehicle trajectories is critical for traffic\nbehavior analysis and autonomous driving systems. We propose Ctx2TrajGen, a\ncontext-aware trajectory generation framework that synthesizes realistic urban\ndriving behaviors using GAIL. Leveraging PPO and WGAN-GP, our model addresses\nnonlinear interdependencies and training instability inherent in microscopic\nsettings. By explicitly conditioning on surrounding vehicles and road geometry,\nCtx2TrajGen generates interaction-aware trajectories aligned with real-world\ncontext. Experiments on the drone-captured DRIFT dataset demonstrate superior\nperformance over existing methods in terms of realism, behavioral diversity,\nand contextual fidelity, offering a robust solution to data scarcity and domain\nshift without simulation.", "AI": {"tldr": "Ctx2TrajGen\u662f\u4e00\u79cd\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u8f68\u8ff9\u751f\u6210\u6846\u67b6\uff0c\u5b83\u4f7f\u7528GAIL\u5408\u6210\u903c\u771f\u7684\u57ce\u5e02\u9a7e\u9a76\u884c\u4e3a\uff0c\u5e76\u5728DRIFT\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\u3002", "motivation": "\u7cbe\u786e\u5efa\u6a21\u5fae\u89c2\u8f66\u8f86\u8f68\u8ff9\u5bf9\u4e8e\u4ea4\u901a\u884c\u4e3a\u5206\u6790\u548c\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528GAIL\u5408\u6210\u903c\u771f\u7684\u57ce\u5e02\u9a7e\u9a76\u884c\u4e3a\uff0c\u5229\u7528PPO\u548cWGAN-GP\u89e3\u51b3\u975e\u7ebf\u6027\u76f8\u4e92\u4f9d\u8d56\u548c\u5fae\u89c2\u73af\u5883\u4e2d\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\uff0c\u5e76\u663e\u5f0f\u5730\u4ee5\u5468\u56f4\u8f66\u8f86\u548c\u9053\u8def\u51e0\u4f55\u5f62\u72b6\u4e3a\u6761\u4ef6\u3002", "result": "Ctx2TrajGen\u751f\u6210\u4e0e\u771f\u5b9e\u4e16\u754c\u73af\u5883\u5bf9\u9f50\u7684\u3001\u5177\u6709\u4ea4\u4e92\u610f\u8bc6\u7684\u8f68\u8ff9\u3002", "conclusion": "Ctx2TrajGen\u5728DRIFT\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u771f\u5b9e\u6027\u3001\u884c\u4e3a\u591a\u6837\u6027\u548c\u4e0a\u4e0b\u6587\u4fdd\u771f\u5ea6\u65b9\u9762\u5747\u6709\u63d0\u5347\uff0c\u4e3a\u6570\u636e\u7a00\u7f3a\u548c\u9886\u57df\u8f6c\u79fb\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u6a21\u62df\u3002"}}
{"id": "2507.16878", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16878", "abs": "https://arxiv.org/abs/2507.16878", "authors": ["Xuchen Li", "Xuzhao Li", "Shiyu Hu", "Kaiqi Huang", "Wentao Zhang"], "title": "CausalStep: A Benchmark for Explicit Stepwise Causal Reasoning in Videos", "comment": "Preprint, Under review", "summary": "Recent advances in large language models (LLMs) have improved reasoning in\ntext and image domains, yet achieving robust video reasoning remains a\nsignificant challenge. Existing video benchmarks mainly assess shallow\nunderstanding and reasoning and allow models to exploit global context, failing\nto rigorously evaluate true causal and stepwise reasoning. We present\nCausalStep, a benchmark designed for explicit stepwise causal reasoning in\nvideos. CausalStep segments videos into causally linked units and enforces a\nstrict stepwise question-answer (QA) protocol, requiring sequential answers and\npreventing shortcut solutions. Each question includes carefully constructed\ndistractors based on error type taxonomy to ensure diagnostic value. The\nbenchmark features 100 videos across six categories and 1,852 multiple-choice\nQA pairs. We introduce seven diagnostic metrics for comprehensive evaluation,\nenabling precise diagnosis of causal reasoning capabilities. Experiments with\nleading proprietary and open-source models, as well as human baselines, reveal\na significant gap between current models and human-level stepwise reasoning.\nCausalStep provides a rigorous benchmark to drive progress in robust and\ninterpretable video reasoning.", "AI": {"tldr": "CausalStep \u662f\u4e00\u4e2a\u7528\u4e8e\u89c6\u9891\u4e2d\u663e\u5f0f\u9010\u6b65\u56e0\u679c\u63a8\u7406\u7684\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u6d45\u5c42\u7406\u89e3\u548c\u63a8\u7406\uff0c\u5e76\u5141\u8bb8\u6a21\u578b\u5229\u7528\u5168\u5c40\u4e0a\u4e0b\u6587\uff0c\u65e0\u6cd5\u4e25\u683c\u8bc4\u4f30\u771f\u6b63\u7684\u56e0\u679c\u548c\u9010\u6b65\u63a8\u7406\u3002\u56e0\u6b64\uff0c\u5b9e\u73b0\u7a33\u5065\u7684\u89c6\u9891\u63a8\u7406\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002", "method": "CausalStep \u5c06\u89c6\u9891\u5206\u5272\u6210\u56e0\u679c\u5173\u8054\u7684\u5355\u5143\uff0c\u5e76\u6267\u884c\u4e25\u683c\u7684\u9010\u6b65\u95ee\u7b54 (QA) \u534f\u8bae\uff0c\u9700\u8981\u6309\u987a\u5e8f\u56de\u7b54\u5e76\u9632\u6b62\u5feb\u6377\u65b9\u5f0f\u89e3\u51b3\u65b9\u6848\u3002\u6bcf\u4e2a\u95ee\u9898\u90fd\u5305\u542b\u57fa\u4e8e\u9519\u8bef\u7c7b\u578b\u5206\u7c7b\u6cd5\u7cbe\u5fc3\u6784\u5efa\u7684\u5e72\u6270\u9879\uff0c\u4ee5\u786e\u4fdd\u8bca\u65ad\u4ef7\u503c\u3002", "result": "\u5bf9\u9886\u5148\u7684\u4e13\u6709\u6a21\u578b\u548c\u5f00\u6e90\u6a21\u578b\u4ee5\u53ca\u4eba\u7c7b\u57fa\u7ebf\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u6a21\u578b\u4e0e\u4eba\u7c7b\u6c34\u5e73\u7684\u9010\u6b65\u63a8\u7406\u4e4b\u95f4\u5b58\u5728\u663e\u7740\u5dee\u8ddd\u3002", "conclusion": "CausalStep \u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e25\u683c\u7684\u57fa\u51c6\uff0c\u4ee5\u63a8\u52a8\u9c81\u68d2\u548c\u53ef\u89e3\u91ca\u7684\u89c6\u9891\u63a8\u7406\u65b9\u9762\u7684\u8fdb\u5c55\u3002"}}
{"id": "2507.17749", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.17749", "abs": "https://arxiv.org/abs/2507.17749", "authors": ["Weixin Chen", "Yuhan Zhao", "Li Chen", "Weike Pan"], "title": "Leave No One Behind: Fairness-Aware Cross-Domain Recommender Systems for Non-Overlapping Users", "comment": "Accepted by RecSys 2025", "summary": "Cross-domain recommendation (CDR) methods predominantly leverage overlapping\nusers to transfer knowledge from a source domain to a target domain. However,\nthrough empirical studies, we uncover a critical bias inherent in these\napproaches: while overlapping users experience significant enhancements in\nrecommendation quality, non-overlapping users benefit minimally and even face\nperformance degradation. This unfairness may erode user trust, and,\nconsequently, negatively impact business engagement and revenue. To address\nthis issue, we propose a novel solution that generates virtual source-domain\nusers for non-overlapping target-domain users. Our method utilizes a dual\nattention mechanism to discern similarities between overlapping and\nnon-overlapping users, thereby synthesizing realistic virtual user embeddings.\nWe further introduce a limiter component that ensures the generated virtual\nusers align with real-data distributions while preserving each user's unique\ncharacteristics. Notably, our method is model-agnostic and can be seamlessly\nintegrated into any CDR model. Comprehensive experiments conducted on three\npublic datasets with five CDR baselines demonstrate that our method effectively\nmitigates the CDR non-overlapping user bias, without loss of overall accuracy.\nOur code is publicly available at https://github.com/WeixinChen98/VUG.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u4e3a\u975e\u91cd\u53e0\u76ee\u6807\u57df\u7528\u6237\u751f\u6210\u865a\u62df\u6e90\u57df\u7528\u6237\uff0c\u4ece\u800c\u89e3\u51b3 CDR \u975e\u91cd\u53e0\u7528\u6237\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u91cd\u53e0\u7528\u6237\u5728\u63a8\u8350\u8d28\u91cf\u65b9\u9762\u83b7\u5f97\u4e86\u663e\u8457\u63d0\u5347\uff0c\u800c\u975e\u91cd\u53e0\u7528\u6237\u53d7\u76ca\u751a\u5fae\uff0c\u751a\u81f3\u9762\u4e34\u6027\u80fd\u4e0b\u964d\u3002\u8fd9\u79cd\u4e0d\u516c\u5e73\u53ef\u80fd\u4f1a\u524a\u5f31\u7528\u6237\u4fe1\u4efb\uff0c\u5e76\u56e0\u6b64\u5bf9\u4e1a\u52a1\u53c2\u4e0e\u5ea6\u548c\u6536\u5165\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002", "method": "\u5229\u7528\u53cc\u91cd\u6ce8\u610f\u673a\u5236\u6765\u8fa8\u522b\u91cd\u53e0\u548c\u975e\u91cd\u53e0\u7528\u6237\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u4ece\u800c\u5408\u6210\u771f\u5b9e\u7684\u865a\u62df\u7528\u6237\u5d4c\u5165\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u5730\u7f13\u89e3\u4e86 CDR \u975e\u91cd\u53e0\u7528\u6237\u504f\u5dee\uff0c\u4e14\u4e0d\u635f\u5931\u6574\u4f53\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5730\u7f13\u89e3\u4e86 CDR \u975e\u91cd\u53e0\u7528\u6237\u504f\u5dee\uff0c\u4e14\u4e0d\u635f\u5931\u6574\u4f53\u51c6\u786e\u6027\u3002"}}
{"id": "2507.16991", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16991", "abs": "https://arxiv.org/abs/2507.16991", "authors": ["Matthias Fey", "Jinu Sunil", "Akihiro Nitta", "Rishi Puri", "Manan Shah", "Bla\u017e Stojanovi\u010d", "Ramona Bendias", "Alexandria Barghi", "Vid Kocijan", "Zecheng Zhang", "Xinwei He", "Jan Eric Lenssen", "Jure Leskovec"], "title": "PyG 2.0: Scalable Learning on Real World Graphs", "comment": null, "summary": "PyG (PyTorch Geometric) has evolved significantly since its initial release,\nestablishing itself as a leading framework for Graph Neural Networks. In this\npaper, we present Pyg 2.0 (and its subsequent minor versions), a comprehensive\nupdate that introduces substantial improvements in scalability and real-world\napplication capabilities. We detail the framework's enhanced architecture,\nincluding support for heterogeneous and temporal graphs, scalable feature/graph\nstores, and various optimizations, enabling researchers and practitioners to\ntackle large-scale graph learning problems efficiently. Over the recent years,\nPyG has been supporting graph learning in a large variety of application areas,\nwhich we will summarize, while providing a deep dive into the important areas\nof relational deep learning and large language modeling.", "AI": {"tldr": "Pyg 2.0 is a comprehensive update that introduces substantial improvements in scalability and real-world application capabilities.", "motivation": "establish itself as a leading framework for Graph Neural Networks", "method": "enhanced architecture, including support for heterogeneous and temporal graphs, scalable feature/graph stores, and various optimizations", "result": "enabling researchers and practitioners to tackle large-scale graph learning problems efficiently", "conclusion": "PyG 2.0 introduces substantial improvements in scalability and real-world application capabilities."}}
{"id": "2507.17178", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17178", "abs": "https://arxiv.org/abs/2507.17178", "authors": ["Zhiqiang Liu", "Enpei Niu", "Yin Hua", "Mengshu Sun", "Lei Liang", "Huajun Chen", "Wen Zhang"], "title": "SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge Understanding of LLMs", "comment": null, "summary": "Although large language models (LLMs) have made significant progress in\nunderstanding Structured Knowledge (SK) like KG and Table, existing evaluations\nfor SK understanding are non-rigorous (i.e., lacking evaluations of specific\ncapabilities) and focus on a single type of SK. Therefore, we aim to propose a\nmore comprehensive and rigorous structured knowledge understanding benchmark to\ndiagnose the shortcomings of LLMs. In this paper, we introduce SKA-Bench, a\nStructured Knowledge Augmented QA Benchmark that encompasses four widely used\nstructured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a\nthree-stage pipeline to construct SKA-Bench instances, which includes a\nquestion, an answer, positive knowledge units, and noisy knowledge units. To\nevaluate the SK understanding capabilities of LLMs in a fine-grained manner, we\nexpand the instances into four fundamental ability testbeds: Noise Robustness,\nOrder Insensitivity, Information Integration, and Negative Rejection. Empirical\nevaluations on 8 representative LLMs, including the advanced DeepSeek-R1,\nindicate that existing LLMs still face significant challenges in understanding\nstructured knowledge, and their performance is influenced by factors such as\nthe amount of noise, the order of knowledge units, and hallucination\nphenomenon. Our dataset and code are available at\nhttps://github.com/Lza12a/SKA-Bench.", "AI": {"tldr": "SKA-Bench: a new benchmark for evaluating structured knowledge understanding in LLMs. It reveals challenges in noise robustness, order insensitivity, information integration, and negative rejection.", "motivation": "Existing evaluations for SK understanding are non-rigorous (i.e., lacking evaluations of specific capabilities) and focus on a single type of SK. Therefore, we aim to propose a more comprehensive and rigorous structured knowledge understanding benchmark to diagnose the shortcomings of LLMs.", "method": "We introduce SKA-Bench, a Structured Knowledge Augmented QA Benchmark that encompasses four widely used structured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a three-stage pipeline to construct SKA-Bench instances, which includes a question, an answer, positive knowledge units, and noisy knowledge units. To evaluate the SK understanding capabilities of LLMs in a fine-grained manner, we expand the instances into four fundamental ability testbeds: Noise Robustness, Order Insensitivity, Information Integration, and Negative Rejection.", "result": "Empirical evaluations on 8 representative LLMs, including the advanced DeepSeek-R1, indicate that existing LLMs still face significant challenges in understanding structured knowledge", "conclusion": "Existing LLMs still face significant challenges in understanding structured knowledge, and their performance is influenced by factors such as the amount of noise, the order of knowledge units, and hallucination phenomenon."}}
{"id": "2507.17477", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17477", "abs": "https://arxiv.org/abs/2507.17477", "authors": ["Haoran Sun", "Zekun Zhang", "Shaoning Zeng"], "title": "An Uncertainty-Driven Adaptive Self-Alignment Framework for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable progress in\ninstruction following and general-purpose reasoning. However, achieving\nhigh-quality alignment with human intent and safety norms without human\nannotations remains a fundamental challenge. In this work, we propose an\nUncertainty-Driven Adaptive Self-Alignment (UDASA) framework designed to\nimprove LLM alignment in a fully automated manner. UDASA first generates\nmultiple responses for each input and quantifies output uncertainty across\nthree dimensions: semantics, factuality, and value alignment. Based on these\nuncertainty scores, the framework constructs preference pairs and categorizes\ntraining samples into three stages, conservative, moderate, and exploratory,\naccording to their uncertainty difference. The model is then optimized\nprogressively across these stages. In addition, we conduct a series of\npreliminary studies to validate the core design assumptions and provide strong\nempirical motivation for the proposed framework. Experimental results show that\nUDASA outperforms existing alignment methods across multiple tasks, including\nharmlessness, helpfulness, truthfulness, and controlled sentiment generation,\nsignificantly improving model performance.", "AI": {"tldr": "UDASA\u901a\u8fc7\u91cf\u5316\u8f93\u51fa\u4e0d\u786e\u5b9a\u6027\u5e76\u5206\u9636\u6bb5\u4f18\u5316\u6a21\u578b\uff0c\u4ece\u800c\u5728\u6ca1\u6709\u4eba\u4e3a\u5e72\u9884\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8LLM\u7684\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "\u5728\u6ca1\u6709\u4eba\u4e3a\u6ce8\u91ca\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e0e\u4eba\u7c7b\u610f\u56fe\u548c\u5b89\u5168\u89c4\u8303\u7684\u9ad8\u8d28\u91cf\u5bf9\u9f50\u4ecd\u7136\u662f\u4e00\u4e2a\u6839\u672c\u6027\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u81ea\u5bf9\u9f50\uff08UDASA\uff09\u6846\u67b6\uff0c\u65e8\u5728\u4ee5\u5168\u81ea\u52a8\u7684\u65b9\u5f0f\u6539\u8fdbLLM\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cUDASA\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u7684\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5305\u62ec\u65e0\u5bb3\u6027\u3001\u6709\u7528\u6027\u3001\u771f\u5b9e\u6027\u548c\u53d7\u63a7\u60c5\u7eea\u751f\u6210\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "UDASA\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u7684\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5305\u62ec\u65e0\u5bb3\u6027\u3001\u6709\u7528\u6027\u3001\u771f\u5b9e\u6027\u548c\u53d7\u63a7\u60c5\u7eea\u751f\u6210\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2507.16880", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.16880", "abs": "https://arxiv.org/abs/2507.16880", "authors": ["Antoni Kowalczuk", "Dominik Hintersdorf", "Lukas Struppek", "Kristian Kersting", "Adam Dziedzic", "Franziska Boenisch"], "title": "Finding Dori: Memorization in Text-to-Image Diffusion Models Is Less Local Than Assumed", "comment": null, "summary": "Text-to-image diffusion models (DMs) have achieved remarkable success in\nimage generation. However, concerns about data privacy and intellectual\nproperty remain due to their potential to inadvertently memorize and replicate\ntraining data. Recent mitigation efforts have focused on identifying and\npruning weights responsible for triggering replication, based on the assumption\nthat memorization can be localized. Our research assesses the robustness of\nthese pruning-based approaches. We demonstrate that even after pruning, minor\nadjustments to text embeddings of input prompts are sufficient to re-trigger\ndata replication, highlighting the fragility of these defenses. Furthermore, we\nchallenge the fundamental assumption of memorization locality, by showing that\nreplication can be triggered from diverse locations within the text embedding\nspace, and follows different paths in the model. Our findings indicate that\nexisting mitigation strategies are insufficient and underscore the need for\nmethods that truly remove memorized content, rather than attempting to suppress\nits retrieval. As a first step in this direction, we introduce a novel\nadversarial fine-tuning method that iteratively searches for replication\ntriggers and updates the model to increase robustness. Through our research, we\nprovide fresh insights into the nature of memorization in text-to-image DMs and\na foundation for building more trustworthy and compliant generative AI.", "AI": {"tldr": "\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u53ef\u80fd\u65e0\u610f\u4e2d\u8bb0\u5fc6\u548c\u590d\u5236\u8bad\u7ec3\u6570\u636e\uff0c\u7f13\u89e3\u63aa\u65bd\u662f\u4e0d\u591f\u7684\uff0c\u5e76\u4e14\u5f3a\u8c03\u9700\u8981\u771f\u6b63\u5220\u9664\u8bb0\u5fc6\u5185\u5bb9\u7684\u65b9\u6cd5\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff08DM\uff09\u5728\u56fe\u50cf\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u529f\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5b83\u4eec\u53ef\u80fd\u65e0\u610f\u4e2d\u8bb0\u5fc6\u548c\u590d\u5236\u8bad\u7ec3\u6570\u636e\uff0c\u56e0\u6b64\u5bf9\u6570\u636e\u9690\u79c1\u548c\u77e5\u8bc6\u4ea7\u6743\u7684\u62c5\u5fe7\u4ecd\u7136\u5b58\u5728\u3002\u6700\u8fd1\u7684\u7f13\u89e3\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728\u8bc6\u522b\u548c\u4fee\u526a\u8d1f\u8d23\u89e6\u53d1\u590d\u5236\u7684\u6743\u91cd\uff0c\u57fa\u4e8e\u8bb0\u5fc6\u53ef\u4ee5\u88ab\u5b9a\u4f4d\u7684\u5047\u8bbe\u3002", "method": "\u6211\u4eec\u8bc1\u660e\uff0c\u5373\u4f7f\u5728\u4fee\u526a\u540e\uff0c\u5bf9\u8f93\u5165\u63d0\u793a\u7684\u6587\u672c\u5d4c\u5165\u8fdb\u884c\u5fae\u5c0f\u7684\u8c03\u6574\u4e5f\u8db3\u4ee5\u91cd\u65b0\u89e6\u53d1\u6570\u636e\u590d\u5236\uff0c\u7a81\u51fa\u4e86\u8fd9\u4e9b\u9632\u5fa1\u63aa\u65bd\u7684\u8106\u5f31\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u901a\u8fc7\u8bc1\u660e\u590d\u5236\u53ef\u4ee5\u4ece\u6587\u672c\u5d4c\u5165\u7a7a\u95f4\u5185\u7684\u4e0d\u540c\u4f4d\u7f6e\u89e6\u53d1\uff0c\u5e76\u4e14\u9075\u5faa\u6a21\u578b\u4e2d\u7684\u4e0d\u540c\u8def\u5f84\uff0c\u4ece\u800c\u6311\u6218\u4e86\u8bb0\u5fc6\u5c40\u90e8\u6027\u7684\u57fa\u672c\u5047\u8bbe\u3002", "result": "\u6211\u4eec\u7684\u7814\u7a76\u8868\u660e\uff0c\u73b0\u6709\u7684\u7f13\u89e3\u7b56\u7565\u662f\u4e0d\u591f\u7684\uff0c\u5e76\u4e14\u5f3a\u8c03\u9700\u8981\u771f\u6b63\u5220\u9664\u8bb0\u5fc6\u5185\u5bb9\u7684\u65b9\u6cd5\uff0c\u800c\u4e0d\u662f\u8bd5\u56fe\u6291\u5236\u5b83\u7684\u68c0\u7d22\u3002\u4f5c\u4e3a\u7b2c\u4e00\u6b65\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u6297\u6027\u5fae\u8c03\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u8fed\u4ee3\u5730\u641c\u7d22\u590d\u5236\u89e6\u53d1\u5668\u5e76\u66f4\u65b0\u6a21\u578b\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\u3002\u901a\u8fc7\u6211\u4eec\u7684\u7814\u7a76\uff0c\u6211\u4eec\u4e3a\u6587\u672c\u5230\u56fe\u50cfDM\u4e2d\u7684\u8bb0\u5fc6\u672c\u8d28\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u5e76\u4e3a\u6784\u5efa\u66f4\u503c\u5f97\u4fe1\u8d56\u548c\u5408\u89c4\u7684\u751f\u6210\u4eba\u5de5\u667a\u80fd\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u73b0\u6709\u7684\u7f13\u89e3\u7b56\u7565\u662f\u4e0d\u591f\u7684\uff0c\u5e76\u4e14\u5f3a\u8c03\u9700\u8981\u771f\u6b63\u5220\u9664\u8bb0\u5fc6\u5185\u5bb9\u7684\u65b9\u6cd5\uff0c\u800c\u4e0d\u662f\u8bd5\u56fe\u6291\u5236\u5b83\u7684\u68c0\u7d22\u3002\u4f5c\u4e3a\u7b2c\u4e00\u6b65\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u6297\u6027\u5fae\u8c03\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u8fed\u4ee3\u5730\u641c\u7d22\u590d\u5236\u89e6\u53d1\u5668\u5e76\u66f4\u65b0\u6a21\u578b\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.17001", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17001", "abs": "https://arxiv.org/abs/2507.17001", "authors": ["Yan Li", "Guangyi Chen", "Yunlong Deng", "Zijian Li", "Zeyu Tang", "Anpeng Wu", "Kun Zhang"], "title": "Should Bias Always be Eliminated? A Principled Framework to Use Data Bias for OOD Generation", "comment": null, "summary": "Most existing methods for adapting models to out-of-distribution (OOD)\ndomains rely on invariant representation learning to eliminate the influence of\nbiased features. However, should bias always be eliminated -- and if not, when\nshould it be retained, and how can it be leveraged? To address these questions,\nwe first present a theoretical analysis that explores the conditions under\nwhich biased features can be identified and effectively utilized. Building on\nthis theoretical foundation, we introduce a novel framework that strategically\nleverages bias to complement invariant representations during inference. The\nframework comprises two key components that leverage bias in both direct and\nindirect ways: (1) using invariance as guidance to extract predictive\ningredients from bias, and (2) exploiting identified bias to estimate the\nenvironmental condition and then use it to explore appropriate bias-aware\npredictors to alleviate environment gaps. We validate our approach through\nexperiments on both synthetic datasets and standard domain generalization\nbenchmarks. Results consistently demonstrate that our method outperforms\nexisting approaches, underscoring its robustness and adaptability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7b56\u7565\u6027\u5730\u5229\u7528\u504f\u5dee\u6765\u8865\u5145\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u4e0d\u53d8\u8868\u793a\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u5728 OOD \u9886\u57df\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u591a\u6570\u73b0\u6709\u7684\u6a21\u578b\u9002\u5e94 OOD \u9886\u57df\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u4e0d\u53d8\u8868\u793a\u5b66\u4e60\u6765\u6d88\u9664\u504f\u5dee\u7279\u5f81\u7684\u5f71\u54cd\u3002\u7136\u800c\uff0c\u5e94\u8be5\u59cb\u7ec8\u6d88\u9664\u504f\u5dee\u5417\uff1f\u5982\u679c\u4e0d\u662f\uff0c\u5e94\u8be5\u4f55\u65f6\u4fdd\u7559\u5b83\uff0c\u4ee5\u53ca\u5982\u4f55\u5229\u7528\u5b83\uff1f", "method": "\u8be5\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff0c\u4ee5\u76f4\u63a5\u548c\u95f4\u63a5\u7684\u65b9\u5f0f\u5229\u7528\u504f\u5dee\uff1a(1) \u4f7f\u7528\u4e0d\u53d8\u6027\u4f5c\u4e3a\u6307\u5bfc\uff0c\u4ece\u504f\u5dee\u4e2d\u63d0\u53d6\u9884\u6d4b\u6210\u5206\uff1b(2) \u5229\u7528\u5df2\u8bc6\u522b\u7684\u504f\u5dee\u6765\u4f30\u8ba1\u73af\u5883\u6761\u4ef6\uff0c\u7136\u540e\u4f7f\u7528\u5b83\u6765\u63a2\u7d22\u9002\u5f53\u7684\u504f\u5dee\u611f\u77e5\u9884\u6d4b\u5668\uff0c\u4ee5\u51cf\u8f7b\u73af\u5883\u5dee\u8ddd\u3002", "result": "\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u6807\u51c6\u9886\u57df\u6cdb\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u5176\u7a33\u5065\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2507.17186", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.17186", "abs": "https://arxiv.org/abs/2507.17186", "authors": ["Lingfeng Zeng", "Fangqi Lou", "Zixuan Wang", "Jiajie Xu", "Jinyi Niu", "Mengping Li", "Yifan Dong", "Qi Qi", "Wei Zhang", "Ziwei Yang", "Jun Han", "Ruilun Feng", "Ruiqi Hu", "Lejie Zhang", "Zhengbo Feng", "Yicheng Ren", "Xin Guo", "Zhaowei Liu", "Dongpo Cheng", "Weige Cai", "Liwen Zhang"], "title": "FinGAIA: An End-to-End Benchmark for Evaluating AI Agents in Finance", "comment": null, "summary": "The booming development of AI agents presents unprecedented opportunities for\nautomating complex tasks across various domains. However, their multi-step,\nmulti-tool collaboration capabilities in the financial sector remain\nunderexplored. This paper introduces FinGAIA, an end-to-end benchmark designed\nto evaluate the practical abilities of AI agents in the financial domain.\nFinGAIA comprises 407 meticulously crafted tasks, spanning seven major\nfinancial sub-domains: securities, funds, banking, insurance, futures, trusts,\nand asset management. These tasks are organized into three hierarchical levels\nof scenario depth: basic business analysis, asset decision support, and\nstrategic risk management. We evaluated 10 mainstream AI agents in a zero-shot\nsetting. The best-performing agent, ChatGPT, achieved an overall accuracy of\n48.9\\%, which, while superior to non-professionals, still lags financial\nexperts by over 35 percentage points. Error analysis has revealed five\nrecurring failure patterns: Cross-modal Alignment Deficiency, Financial\nTerminological Bias, Operational Process Awareness Barrier, among others. These\npatterns point to crucial directions for future research. Our work provides the\nfirst agent benchmark closely related to the financial domain, aiming to\nobjectively assess and promote the development of agents in this crucial field.\nPartial data is available at https://github.com/SUFE-AIFLM-Lab/FinGAIA.", "AI": {"tldr": "FinGAIA is introduced to evaluate AI agents in finance. Current agents lag experts, and key failure patterns are identified.", "motivation": "The multi-step, multi-tool collaboration capabilities of AI agents in the financial sector are underexplored.", "method": "The paper introduces FinGAIA, a benchmark comprising 407 tasks across seven financial sub-domains, organized into three levels of scenario depth. Ten mainstream AI agents were evaluated in a zero-shot setting.", "result": "The best-performing agent, ChatGPT, achieved 48.9% accuracy, lagging financial experts. Error analysis revealed five recurring failure patterns.", "conclusion": "This paper introduces FinGAIA, a benchmark for evaluating AI agents in the financial domain, and identifies areas for future research based on error analysis of current agents."}}
{"id": "2507.17482", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17482", "abs": "https://arxiv.org/abs/2507.17482", "authors": ["Luca Salvatore Lorello", "Nikolaos Manginas", "Marco Lippi", "Stefano Melacci"], "title": "LTLZinc: a Benchmarking Framework for Continual Learning and Neuro-Symbolic Temporal Reasoning", "comment": null, "summary": "Neuro-symbolic artificial intelligence aims to combine neural architectures\nwith symbolic approaches that can represent knowledge in a human-interpretable\nformalism. Continual learning concerns with agents that expand their knowledge\nover time, improving their skills while avoiding to forget previously learned\nconcepts. Most of the existing approaches for neuro-symbolic artificial\nintelligence are applied to static scenarios only, and the challenging setting\nwhere reasoning along the temporal dimension is necessary has been seldom\nexplored. In this work we introduce LTLZinc, a benchmarking framework that can\nbe used to generate datasets covering a variety of different problems, against\nwhich neuro-symbolic and continual learning methods can be evaluated along the\ntemporal and constraint-driven dimensions. Our framework generates expressive\ntemporal reasoning and continual learning tasks from a linear temporal logic\nspecification over MiniZinc constraints, and arbitrary image classification\ndatasets. Fine-grained annotations allow multiple neural and neuro-symbolic\ntraining settings on the same generated datasets. Experiments on six\nneuro-symbolic sequence classification and four class-continual learning tasks\ngenerated by LTLZinc, demonstrate the challenging nature of temporal learning\nand reasoning, and highlight limitations of current state-of-the-art methods.\nWe release the LTLZinc generator and ten ready-to-use tasks to the\nneuro-symbolic and continual learning communities, in the hope of fostering\nresearch towards unified temporal learning and reasoning frameworks.", "AI": {"tldr": "LTLZinc \u662f\u4e00\u4e2a\u57fa\u51c6\u6846\u67b6\uff0c\u53ef\u4ee5\u6839\u636e\u5b83\u8bc4\u4f30\u795e\u7ecf\u7b26\u53f7\u548c\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u5927\u591a\u6570\u73b0\u6709\u7684\u795e\u7ecf\u7b26\u53f7\u4eba\u5de5\u667a\u80fd\u65b9\u6cd5\u4ec5\u5e94\u7528\u4e8e\u9759\u6001\u573a\u666f\uff0c\u800c\u5f88\u5c11\u63a2\u7d22\u6cbf\u65f6\u95f4\u7ef4\u5ea6\u8fdb\u884c\u63a8\u7406\u7684\u5177\u6709\u6311\u6218\u6027\u7684\u73af\u5883\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86 LTLZinc\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u51c6\u6846\u67b6\uff0c\u53ef\u7528\u4e8e\u751f\u6210\u6db5\u76d6\u5404\u79cd\u4e0d\u540c\u95ee\u9898\u7684\u6570\u636e\u96c6\uff0c\u795e\u7ecf\u7b26\u53f7\u548c\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u53ef\u4ee5\u6839\u636e\u8fd9\u4e9b\u6570\u636e\u96c6\u6cbf\u65f6\u95f4\u548c\u7ea6\u675f\u9a71\u52a8\u7684\u7ef4\u5ea6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728 LTLZinc \u751f\u6210\u7684\u516d\u4e2a\u795e\u7ecf\u7b26\u53f7\u5e8f\u5217\u5206\u7c7b\u548c\u56db\u4e2a\u7c7b\u6301\u7eed\u5b66\u4e60\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\u4e86\u65f6\u95f4\u5b66\u4e60\u548c\u63a8\u7406\u7684\u6311\u6218\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "conclusion": "LTLZinc \u751f\u6210\u5668\u548c\u5341\u4e2a\u5373\u7528\u578b\u4efb\u52a1\u5df2\u53d1\u5e03\u7ed9\u795e\u7ecf\u7b26\u53f7\u548c\u6301\u7eed\u5b66\u4e60\u793e\u533a\uff0c\u5e0c\u671b\u80fd\u4fc3\u8fdb\u5bf9\u7edf\u4e00\u7684\u65f6\u95f4\u5b66\u4e60\u548c\u63a8\u7406\u6846\u67b6\u7684\u7814\u7a76\u3002"}}
{"id": "2507.16886", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16886", "abs": "https://arxiv.org/abs/2507.16886", "authors": ["Yaoyu Fang", "Jiahe Qian", "Xinkun Wang", "Lee A. Cooper", "Bo Zhou"], "title": "Sparser2Sparse: Single-shot Sparser-to-Sparse Learning for Spatial Transcriptomics Imputation with Natural Image Co-learning", "comment": "16 pages, 5 figure, under review", "summary": "Spatial transcriptomics (ST) has revolutionized biomedical research by\nenabling high resolution gene expression profiling within tissues. However, the\nhigh cost and scarcity of high resolution ST data remain significant\nchallenges. We present Single-shot Sparser-to-Sparse (S2S-ST), a novel\nframework for accurate ST imputation that requires only a single and low-cost\nsparsely sampled ST dataset alongside widely available natural images for\nco-training. Our approach integrates three key innovations: (1) a\nsparser-to-sparse self-supervised learning strategy that leverages intrinsic\nspatial patterns in ST data, (2) cross-domain co-learning with natural images\nto enhance feature representation, and (3) a Cascaded Data Consistent\nImputation Network (CDCIN) that iteratively refines predictions while\npreserving sampled gene data fidelity. Extensive experiments on diverse tissue\ntypes, including breast cancer, liver, and lymphoid tissue, demonstrate that\nour method outperforms state-of-the-art approaches in imputation accuracy. By\nenabling robust ST reconstruction from sparse inputs, our framework\nsignificantly reduces reliance on costly high resolution data, facilitating\npotential broader adoption in biomedical research and clinical applications.", "AI": {"tldr": "S2S-ST\uff1a\u4e00\u79cd\u65b0\u9896\u7684ST\u63d2\u8865\u6846\u67b6\uff0c\u5b83\u53ea\u9700\u8981\u4e00\u4e2a\u5355\u4e00\u4e14\u4f4e\u6210\u672c\u7684\u7a00\u758f\u91c7\u6837ST\u6570\u636e\u96c6\u4ee5\u53ca\u5e7f\u6cdb\u53ef\u7528\u7684\u81ea\u7136\u56fe\u50cf\u8fdb\u884c\u5171\u540c\u8bad\u7ec3\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387ST\u6570\u636e\u7684\u6210\u672c\u9ad8\u6602\u548c\u7a00\u7f3a\u4ecd\u7136\u662f\u91cd\u5927\u6311\u6218\u3002", "method": "Single-shot Sparser-to-Sparse (S2S-ST)", "result": "\u5728\u5305\u62ec\u4e73\u817a\u764c\u3001\u809d\u810f\u548c\u6dcb\u5df4\u7ec4\u7ec7\u5728\u5185\u7684\u591a\u79cd\u7ec4\u7ec7\u7c7b\u578b\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u63d2\u8865\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "S2S-ST\u901a\u8fc7\u4ece\u7a00\u758f\u8f93\u5165\u5b9e\u73b0\u5f3a\u5927\u7684ST\u91cd\u5efa\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5bf9\u6602\u8d35\u7684\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u4fc3\u8fdb\u4e86\u5728\u751f\u7269\u533b\u5b66\u7814\u7a76\u548c\u4e34\u5e8a\u5e94\u7528\u4e2d\u66f4\u5e7f\u6cdb\u7684\u91c7\u7528\u3002"}}
{"id": "2507.17013", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17013", "abs": "https://arxiv.org/abs/2507.17013", "authors": ["Tobias Weber", "B\u00e1lint Mucs\u00e1nyi", "Lenard Rommel", "Thomas Christie", "Lars Kas\u00fcschke", "Marvin Pf\u00f6rtner", "Philipp Hennig"], "title": "laplax -- Laplace Approximations with JAX", "comment": "Submission to the ICML 2025 Workshop on Championing Open-source\n  Development in Machine Learning (CODEML '25)", "summary": "The Laplace approximation provides a scalable and efficient means of\nquantifying weight-space uncertainty in deep neural networks, enabling the\napplication of Bayesian tools such as predictive uncertainty and model\nselection via Occam's razor. In this work, we introduce laplax, a new\nopen-source Python package for performing Laplace approximations with jax.\nDesigned with a modular and purely functional architecture and minimal external\ndependencies, laplax offers a flexible and researcher-friendly framework for\nrapid prototyping and experimentation. Its goal is to facilitate research on\nBayesian neural networks, uncertainty quantification for deep learning, and the\ndevelopment of improved Laplace approximation techniques.", "AI": {"tldr": "laplax is a new open-source Python package for performing Laplace approximations with jax", "motivation": "quantifying weight-space uncertainty in deep neural networks, enabling the application of Bayesian tools such as predictive uncertainty and model selection via Occam's razor.", "method": "a new open-source Python package for performing Laplace approximations with jax", "result": "laplax is designed with a modular and purely functional architecture and minimal external dependencies", "conclusion": "laplax offers a flexible and researcher-friendly framework for rapid prototyping and experimentation. Its goal is to facilitate research on Bayesian neural networks, uncertainty quantification for deep learning, and the development of improved Laplace approximation techniques."}}
{"id": "2507.17216", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17216", "abs": "https://arxiv.org/abs/2507.17216", "authors": ["Giuseppe Russo", "Debora Nozza", "Paul R\u00f6ttger", "Dirk Hovy"], "title": "The Pluralistic Moral Gap: Understanding Judgment and Value Differences between Humans and Large Language Models", "comment": "13 pages, 4 figures", "summary": "People increasingly rely on Large Language Models (LLMs) for moral advice,\nwhich may influence humans' decisions. Yet, little is known about how closely\nLLMs align with human moral judgments. To address this, we introduce the Moral\nDilemma Dataset, a benchmark of 1,618 real-world moral dilemmas paired with a\ndistribution of human moral judgments consisting of a binary evaluation and a\nfree-text rationale. We treat this problem as a pluralistic distributional\nalignment task, comparing the distributions of LLM and human judgments across\ndilemmas. We find that models reproduce human judgments only under high\nconsensus; alignment deteriorates sharply when human disagreement increases. In\nparallel, using a 60-value taxonomy built from 3,783 value expressions\nextracted from rationales, we show that LLMs rely on a narrower set of moral\nvalues than humans. These findings reveal a pluralistic moral gap: a mismatch\nin both the distribution and diversity of values expressed. To close this gap,\nwe introduce Dynamic Moral Profiling (DMP), a Dirichlet-based sampling method\nthat conditions model outputs on human-derived value profiles. DMP improves\nalignment by 64.3% and enhances value diversity, offering a step toward more\npluralistic and human-aligned moral guidance from LLMs.", "AI": {"tldr": "LLMs don't align well with human moral judgments, but a new method called Dynamic Moral Profiling (DMP) helps close this gap.", "motivation": "Understanding how closely LLMs align with human moral judgments is crucial as people increasingly rely on them for moral advice, which may influence human decisions.", "method": "Introducing the Moral Dilemma Dataset, a benchmark of 1,618 real-world moral dilemmas, and comparing the distributions of LLM and human judgments using a 60-value taxonomy. Also introducing Dynamic Moral Profiling (DMP), a Dirichlet-based sampling method.", "result": "LLMs reproduce human judgments only under high consensus, with alignment deteriorating sharply when human disagreement increases. DMP improves alignment by 64.3% and enhances value diversity.", "conclusion": "LLMs exhibit a pluralistic moral gap compared to human moral judgments, but the proposed Dynamic Moral Profiling (DMP) method can improve alignment and value diversity."}}
{"id": "2507.16940", "categories": ["cs.CV", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.16940", "abs": "https://arxiv.org/abs/2507.16940", "authors": ["Nima Fathi", "Amar Kumar", "Tal Arbel"], "title": "AURA: A Multi-Modal Medical Agent for Understanding, Reasoning & Annotation", "comment": "9 pages, 3 figures, International Conference on Medical Image\n  Computing and Computer-Assisted Intervention", "summary": "Recent advancements in Large Language Models (LLMs) have catalyzed a paradigm\nshift from static prediction systems to agentic AI agents capable of reasoning,\ninteracting with tools, and adapting to complex tasks. While LLM-based agentic\nsystems have shown promise across many domains, their application to medical\nimaging remains in its infancy. In this work, we introduce AURA, the first\nvisual linguistic explainability agent designed specifically for comprehensive\nanalysis, explanation, and evaluation of medical images. By enabling dynamic\ninteractions, contextual explanations, and hypothesis testing, AURA represents\na significant advancement toward more transparent, adaptable, and clinically\naligned AI systems. We highlight the promise of agentic AI in transforming\nmedical image analysis from static predictions to interactive decision support.\nLeveraging Qwen-32B, an LLM-based architecture, AURA integrates a modular\ntoolbox comprising: (i) a segmentation suite with phase grounding, pathology\nsegmentation, and anatomy segmentation to localize clinically meaningful\nregions; (ii) a counterfactual image-generation module that supports reasoning\nthrough image-level explanations; and (iii) a set of evaluation tools including\npixel-wise difference-map analysis, classification, and advanced\nstate-of-the-art components to assess diagnostic relevance and visual\ninterpretability.", "AI": {"tldr": "AURA is the first visual linguistic explainability agent for medical images, enabling interactive analysis and decision support.", "motivation": "Applying LLM-based agentic systems to medical imaging is still in its infancy, despite their promise in other domains.", "method": "AURA, the first visual linguistic explainability agent, leverages Qwen-32B and integrates a modular toolbox comprising segmentation, counterfactual image-generation, and evaluation tools.", "result": "AURA enables dynamic interactions, contextual explanations, and hypothesis testing, representing a significant advancement toward more transparent, adaptable, and clinically aligned AI systems.", "conclusion": "Agentic AI systems hold significant promise for transforming medical image analysis from static predictions to interactive decision support."}}
{"id": "2507.17016", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17016", "abs": "https://arxiv.org/abs/2507.17016", "authors": ["Omid Orang", "Patricia O. Lucas", "Gabriel I. F. Paiva", "Petronio C. L. Silva", "Felipe Augusto Rocha da Silva", "Adriano Alonso Veloso", "Frederico Gadelha Guimaraes"], "title": "Causal Graph Fuzzy LLMs: A First Introduction and Applications in Time Series Forecasting", "comment": "Accepted for publication at the Brazilian Congress of Artificial\n  Intelligence (CBIC)", "summary": "In recent years, the application of Large Language Models (LLMs) to time\nseries forecasting (TSF) has garnered significant attention among researchers.\nThis study presents a new frame of LLMs named CGF-LLM using GPT-2 combined with\nfuzzy time series (FTS) and causal graph to predict multivariate time series,\nmarking the first such architecture in the literature. The key objective is to\nconvert numerical time series into interpretable forms through the parallel\napplication of fuzzification and causal analysis, enabling both semantic\nunderstanding and structural insight as input for the pretrained GPT-2 model.\nThe resulting textual representation offers a more interpretable view of the\ncomplex dynamics underlying the original time series. The reported results\nconfirm the effectiveness of our proposed LLM-based time series forecasting\nmodel, as demonstrated across four different multivariate time series datasets.\nThis initiative paves promising future directions in the domain of TSF using\nLLMs based on FTS.", "AI": {"tldr": "This paper introduces CGF-LLM, a new LLM framework using GPT-2 with fuzzy time series and causal graphs for multivariate time series forecasting. It converts numerical data into interpretable text, enhancing semantic understanding and structural insight.", "motivation": "Applying Large Language Models (LLMs) to time series forecasting (TSF) has garnered significant attention. The objective is to convert numerical time series into interpretable forms through fuzzification and causal analysis, enabling semantic understanding and structural insight for the GPT-2 model.", "method": "The study uses GPT-2 combined with fuzzy time series (FTS) and causal graph to predict multivariate time series.", "result": "The resulting textual representation offers a more interpretable view of the complex dynamics underlying the original time series.", "conclusion": "The CGF-LLM model's effectiveness is confirmed across four multivariate time series datasets, paving promising future directions for time series forecasting using LLMs based on FTS."}}
{"id": "2507.17234", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.17234", "abs": "https://arxiv.org/abs/2507.17234", "authors": ["Kyeongkyu Lee", "Seonghwan Yoon", "Hongki Lim"], "title": "CLARIFID: Improving Radiology Report Generation by Reinforcing Clinically Accurate Impressions and Enforcing Detailed Findings", "comment": null, "summary": "Automatic generation of radiology reports has the potential to alleviate\nradiologists' significant workload, yet current methods struggle to deliver\nclinically reliable conclusions. In particular, most prior approaches focus on\nproducing fluent text without effectively ensuring the factual correctness of\nthe reports and often rely on single-view images, limiting diagnostic\ncomprehensiveness. We propose CLARIFID, a novel framework that directly\noptimizes diagnostic correctness by mirroring the two-step workflow of experts.\nSpecifically, CLARIFID (1) learns the logical flow from Findings to Impression\nthrough section-aware pretraining, (2) is fine-tuned with Proximal Policy\nOptimization in which the CheXbert F1 score of the Impression section serves as\nthe reward, (3) enforces reasoning-aware decoding that completes \"Findings\"\nbefore synthesizing the \"Impression\", and (4) fuses multiple chest X-ray views\nvia a vision-transformer-based multi-view encoder. During inference, we apply a\nreasoning-aware next-token forcing strategy followed by report-level\nre-ranking, ensuring that the model first produces a comprehensive Findings\nsection before synthesizing the Impression and thereby preserving coherent\nclinical reasoning. Experimental results on the MIMIC-CXR dataset demonstrate\nthat our method achieves superior clinical efficacy and outperforms existing\nbaselines on both standard NLG metrics and clinically aware scores.", "AI": {"tldr": "CLARIFID\u901a\u8fc7\u6a21\u4eff\u653e\u5c04\u79d1\u533b\u751f\u7684\u5de5\u4f5c\u6d41\u7a0b\u5e76\u4f18\u5316\u8bca\u65ad\u7684\u6b63\u786e\u6027\u6765\u81ea\u52a8\u751f\u6210\u653e\u5c04\u62a5\u544a\uff0c\u4ece\u800c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u81ea\u52a8\u751f\u6210\u653e\u5c04\u5b66\u62a5\u544a\u6709\u6f5c\u529b\u51cf\u8f7b\u653e\u5c04\u79d1\u533b\u751f\u7684\u5927\u91cf\u5de5\u4f5c\u8d1f\u62c5\uff0c\u4f46\u76ee\u524d\u7684\u65b9\u6cd5\u96be\u4ee5\u63d0\u4f9b\u4e34\u5e8a\u4e0a\u53ef\u9760\u7684\u7ed3\u8bba\u3002\u7279\u522b\u662f\uff0c\u5927\u591a\u6570\u5148\u524d\u7684\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u751f\u6210\u6d41\u7545\u7684\u6587\u672c\uff0c\u800c\u6ca1\u6709\u6709\u6548\u786e\u4fdd\u62a5\u544a\u7684\u4e8b\u5b9e\u6b63\u786e\u6027\uff0c\u5e76\u4e14\u901a\u5e38\u4f9d\u8d56\u4e8e\u5355\u89c6\u56fe\u56fe\u50cf\uff0c\u9650\u5236\u4e86\u8bca\u65ad\u7684\u5168\u9762\u6027\u3002", "method": "CLARIFID\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u955c\u50cf\u4e13\u5bb6\u7684\u4e24\u6b65\u5de5\u4f5c\u6d41\u7a0b\uff0c\u76f4\u63a5\u4f18\u5316\u8bca\u65ad\u6b63\u786e\u6027\u3002\u5177\u4f53\u6765\u8bf4\uff0cCLARIFID (1) \u901a\u8fc7section-aware\u9884\u8bad\u7ec3\u5b66\u4e60\u4ece\u201c\u53d1\u73b0\u201d\u5230\u201c\u5370\u8c61\u201d\u7684\u903b\u8f91\u6d41\u7a0b\uff0c(2) \u4f7f\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u8fdb\u884c\u5fae\u8c03\uff0c\u5176\u4e2d\u201c\u5370\u8c61\u201d\u90e8\u5206\u7684CheXbert F1\u8bc4\u5206\u4f5c\u4e3a\u5956\u52b1\uff0c(3) \u5b9e\u65bd\u63a8\u7406\u611f\u77e5\u89e3\u7801\uff0c\u5728\u5408\u6210\u201c\u5370\u8c61\u201d\u4e4b\u524d\u5b8c\u6210\u201c\u53d1\u73b0\u201d\uff0c\u4ee5\u53ca (4) \u901a\u8fc7\u57fa\u4e8e\u89c6\u89c9\u8f6c\u6362\u5668\u7684\u591a\u89c6\u56fe\u7f16\u7801\u5668\u878d\u5408\u591a\u4e2a\u80f8\u90e8X\u5149\u7247\u89c6\u56fe\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u4e34\u5e8a\u7597\u6548\uff0c\u5e76\u5728\u6807\u51c6NLG\u6307\u6807\u548c\u4e34\u5e8a\u611f\u77e5\u8bc4\u5206\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728MIMIC-CXR\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u4e34\u5e8a\u7597\u6548\uff0c\u5e76\u5728\u6807\u51c6NLG\u6307\u6807\u548c\u4e34\u5e8a\u611f\u77e5\u8bc4\u5206\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u7ebf\u3002"}}
{"id": "2507.17493", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.17493", "abs": "https://arxiv.org/abs/2507.17493", "authors": ["Alexander Beiser", "Markus Hecher", "Stefan Woltran"], "title": "Automated Hybrid Grounding Using Structural and Data-Driven Heuristics", "comment": null, "summary": "The grounding bottleneck poses one of the key challenges that hinders the\nwidespread adoption of Answer Set Programming in industry. Hybrid Grounding is\na step in alleviating the bottleneck by combining the strength of standard\nbottom-up grounding with recently proposed techniques where rule bodies are\ndecoupled during grounding. However, it has remained unclear when hybrid\ngrounding shall use body-decoupled grounding and when to use standard bottom-up\ngrounding. In this paper, we address this issue by developing automated hybrid\ngrounding: we introduce a splitting algorithm based on data-structural\nheuristics that detects when to use body-decoupled grounding and when standard\ngrounding is beneficial. We base our heuristics on the structure of rules and\nan estimation procedure that incorporates the data of the instance. The\nexperiments conducted on our prototypical implementation demonstrate promising\nresults, which show an improvement on hard-to-ground scenarios, whereas on\nhard-to-solve instances we approach state-of-the-art performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u6df7\u5408 grounding \u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528\u6570\u636e\u7ed3\u6784\u542f\u53d1\u5f0f\u7b97\u6cd5\u6765\u786e\u5b9a\u4f55\u65f6\u4f7f\u7528 body-decoupled grounding \u4ee5\u53ca\u4f55\u65f6\u4f7f\u7528\u6807\u51c6 bottom-up grounding\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u96be\u4ee5 grounding \u7684\u573a\u666f\u4e2d\u6709\u6240\u6539\u8fdb\uff0c\u800c\u5728\u96be\u4ee5\u89e3\u51b3\u7684\u5b9e\u4f8b\u4e2d\uff0c\u6027\u80fd\u63a5\u8fd1\u5f53\u524d\u6700\u4f18\u3002", "motivation": "The grounding bottleneck poses one of the key challenges that hinders the widespread adoption of Answer Set Programming in industry. Hybrid Grounding is a step in alleviating the bottleneck by combining the strength of standard bottom-up grounding with recently proposed techniques where rule bodies are decoupled during grounding. However, it has remained unclear when hybrid grounding shall use body-decoupled grounding and when to use standard bottom-up grounding.", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u7ed3\u6784\u542f\u53d1\u5f0f\u7684\u62c6\u5206\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u53ef\u4ee5\u68c0\u6d4b\u4f55\u65f6\u4f7f\u7528 body-decoupled grounding\uff0c\u4f55\u65f6\u4f7f\u7528\u6807\u51c6 grounding \u6709\u76ca\u3002\u6211\u4eec\u5728\u89c4\u5219\u7ed3\u6784\u548c\u4e00\u4e2a\u5305\u542b\u5b9e\u4f8b\u6570\u636e\u7684\u4f30\u8ba1\u7a0b\u5e8f\u7684\u57fa\u7840\u4e0a\u6784\u5efa\u4e86\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002", "result": "\u5728\u6211\u4eec\u7684\u539f\u578b\u5b9e\u73b0\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u679c\u5f88\u6709\u5e0c\u671b\uff0c\u8fd9\u8868\u660e\u5728\u96be\u4ee5 grounding \u7684\u573a\u666f\u4e2d\u6709\u6240\u6539\u8fdb\uff0c\u800c\u5728\u96be\u4ee5\u89e3\u51b3\u7684\u5b9e\u4f8b\u4e2d\uff0c\u6211\u4eec\u63a5\u8fd1\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u5f00\u53d1\u81ea\u52a8\u6df7\u5408 grounding \u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff1a\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u7ed3\u6784\u542f\u53d1\u5f0f\u7684\u62c6\u5206\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u53ef\u4ee5\u68c0\u6d4b\u4f55\u65f6\u4f7f\u7528 body-decoupled grounding\uff0c\u4f55\u65f6\u4f7f\u7528\u6807\u51c6 grounding \u6709\u76ca\u3002\u6211\u4eec\u5728\u89c4\u5219\u7ed3\u6784\u548c\u4e00\u4e2a\u5305\u542b\u5b9e\u4f8b\u6570\u636e\u7684\u4f30\u8ba1\u7a0b\u5e8f\u7684\u57fa\u7840\u4e0a\u6784\u5efa\u4e86\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002\u5728\u6211\u4eec\u7684\u539f\u578b\u5b9e\u73b0\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u679c\u5f88\u6709\u5e0c\u671b\uff0c\u8fd9\u8868\u660e\u5728\u96be\u4ee5 grounding \u7684\u573a\u666f\u4e2d\u6709\u6240\u6539\u8fdb\uff0c\u800c\u5728\u96be\u4ee5\u89e3\u51b3\u7684\u5b9e\u4f8b\u4e2d\uff0c\u6211\u4eec\u63a5\u8fd1\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2507.16946", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.16946", "abs": "https://arxiv.org/abs/2507.16946", "authors": ["Chiao-An Yang", "Kuan-Chuan Peng", "Raymond A. Yeh"], "title": "Toward Long-Tailed Online Anomaly Detection through Class-Agnostic Concepts", "comment": "This paper is accepted to ICCV 2025. The supplementary material is\n  included. The long-tailed online anomaly detection dataset is available at\n  https://doi.org/10.5281/zenodo.16283852", "summary": "Anomaly detection (AD) identifies the defect regions of a given image. Recent\nworks have studied AD, focusing on learning AD without abnormal images, with\nlong-tailed distributed training data, and using a unified model for all\nclasses. In addition, online AD learning has also been explored. In this work,\nwe expand in both directions to a realistic setting by considering the novel\ntask of long-tailed online AD (LTOAD). We first identified that the offline\nstate-of-the-art LTAD methods cannot be directly applied to the online setting.\nSpecifically, LTAD is class-aware, requiring class labels that are not\navailable in the online setting. To address this challenge, we propose a\nclass-agnostic framework for LTAD and then adapt it to our online learning\nsetting. Our method outperforms the SOTA baselines in most offline LTAD\nsettings, including both the industrial manufacturing and the medical domain.\nIn particular, we observe +4.63% image-AUROC on MVTec even compared to methods\nthat have access to class labels and the number of classes. In the most\nchallenging long-tailed online setting, we achieve +0.53% image-AUROC compared\nto baselines. Our LTOAD benchmark is released here:\nhttps://doi.org/10.5281/zenodo.16283852 .", "AI": {"tldr": "Proposes a class-agnostic framework for long-tailed online anomaly detection (LTOAD), outperforming existing methods in both offline and online settings.", "motivation": "Extending anomaly detection (AD) to a realistic setting by considering the novel task of long-tailed online AD (LTOAD). Offline state-of-the-art LTAD methods cannot be directly applied to the online setting because LTAD is class-aware, requiring class labels not available online.", "method": "A class-agnostic framework for LTAD adapted to the online learning setting.", "result": "Outperforms the SOTA baselines in most offline LTAD settings, including both the industrial manufacturing and the medical domain. +4.63% image-AUROC on MVTec. +0.53% image-AUROC compared to baselines in the most challenging long-tailed online setting.", "conclusion": "The proposed class-agnostic framework outperforms state-of-the-art baselines in most offline long-tailed anomaly detection (LTAD) settings, achieving +4.63% image-AUROC on MVTec and +0.53% in the long-tailed online setting."}}
{"id": "2507.17399", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.17399", "abs": "https://arxiv.org/abs/2507.17399", "authors": ["Zhili Shen", "Chenxin Diao", "Pascual Merita", "Pavlos Vougiouklis", "Jeff Z. Pan"], "title": "Millions of $\\text{GeAR}$-s: Extending GraphRAG to Millions of Documents", "comment": "Accepted by SIGIR 2025 LiveRAG Challenge Program", "summary": "Recent studies have explored graph-based approaches to retrieval-augmented\ngeneration, leveraging structured or semi-structured information -- such as\nentities and their relations extracted from documents -- to enhance retrieval.\nHowever, these methods are typically designed to address specific tasks, such\nas multi-hop question answering and query-focused summarisation, and therefore,\nthere is limited evidence of their general applicability across broader\ndatasets. In this paper, we aim to adapt a state-of-the-art graph-based RAG\nsolution: $\\text{GeAR}$ and explore its performance and limitations on the\nSIGIR 2025 LiveRAG Challenge.", "AI": {"tldr": "\u672c\u6587\u8c03\u6574\u4e86GeAR\u5e76\u5728SIGIR 2025 LiveRAG Challenge\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "motivation": "\u6700\u8fd1\u7684\u7814\u7a76\u5df2\u7ecf\u63a2\u7d22\u4e86\u57fa\u4e8e\u56fe\u7684\u65b9\u6cd5\u6765\u8fdb\u884c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff0c\u5229\u7528\u7ed3\u6784\u5316\u6216\u534a\u7ed3\u6784\u5316\u7684\u4fe1\u606f\uff08\u4f8b\u5982\u4ece\u6587\u6863\u4e2d\u63d0\u53d6\u7684\u5b9e\u4f53\u53ca\u5176\u5173\u7cfb\uff09\u6765\u589e\u5f3a\u68c0\u7d22\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u88ab\u8bbe\u8ba1\u7528\u6765\u89e3\u51b3\u7279\u5b9a\u7684\u4efb\u52a1\uff0c\u4f8b\u5982\u591a\u8df3\u95ee\u7b54\u548c\u4ee5\u67e5\u8be2\u4e3a\u4e2d\u5fc3\u7684\u6458\u8981\uff0c\u56e0\u6b64\uff0c\u5b83\u4eec\u5728\u66f4\u5e7f\u6cdb\u7684\u6570\u636e\u96c6\u4e0a\u7684\u666e\u904d\u9002\u7528\u6027\u7684\u8bc1\u636e\u6709\u9650\u3002", "method": "\u8c03\u6574\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u56fe\u7684RAG\u89e3\u51b3\u65b9\u6848: GeAR", "result": "\u63a2\u7d22GeAR\u5728SIGIR 2025 LiveRAG Challenge\u4e0a\u7684\u6027\u80fd\u548c\u5c40\u9650\u6027\u3002", "conclusion": "\u8fd9\u7bc7\u8bba\u6587\u65e8\u5728\u8c03\u6574\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u56fe\u7684RAG\u89e3\u51b3\u65b9\u6848GeAR\uff0c\u5e76\u63a2\u7d22\u5176\u5728SIGIR 2025 LiveRAG Challenge\u4e0a\u7684\u6027\u80fd\u548c\u5c40\u9650\u6027\u3002"}}
{"id": "2507.17019", "categories": ["cs.LG", "65M32 65M32 65M32", "I.2.6; G.1.8"], "pdf": "https://arxiv.org/pdf/2507.17019", "abs": "https://arxiv.org/abs/2507.17019", "authors": ["Ray Zirui Zhang", "Christopher E. Miles", "Xiaohui Xie", "John S. Lowengrub"], "title": "BiLO: Bilevel Local Operator Learning for PDE Inverse Problems. Part II: Efficient Uncertainty Quantification with Low-Rank Adaptation", "comment": null, "summary": "Uncertainty quantification and inverse problems governed by partial\ndifferential equations (PDEs) are central to a wide range of scientific and\nengineering applications. In this second part of a two part series, we extend\nBilevel Local Operator Learning (BiLO) for PDE-constrained optimization\nproblems developed in Part 1 to the Bayesian inference framework. At the lower\nlevel, we train a network to approximate the local solution operator by\nminimizing the local operator loss with respect to the weights of the neural\nnetwork. At the upper level, we sample the PDE parameters from the posterior\ndistribution. We achieve efficient sampling through gradient-based Markov Chain\nMonte Carlo (MCMC) methods and low-rank adaptation (LoRA). Compared with\nexisting methods based on Bayesian neural networks, our approach bypasses the\nchallenge of sampling in the high-dimensional space of neural network weights\nand does not require specifying a prior distribution on the neural network\nsolution. Instead, uncertainty propagates naturally from the data through the\nPDE constraints. By enforcing strong PDE constraints, the proposed method\nimproves the accuracy of both parameter inference and uncertainty\nquantification. We analyze the dynamic error of the gradient in the MCMC\nsampler and the static error in the posterior distribution due to inexact\nminimization of the lower level problem and demonstrate a direct link between\nthe tolerance for solving the lower level problem and the accuracy of the\nresulting uncertainty quantification. Through numerical experiments across a\nvariety of PDE models, we demonstrate that our method delivers accurate\ninference and quantification of uncertainties while maintaining high\ncomputational efficiency.", "AI": {"tldr": "Extends Bilevel Local Operator Learning (BiLO) to Bayesian inference for PDE-constrained optimization, achieving accurate and efficient uncertainty quantification.", "motivation": "Uncertainty quantification and inverse problems governed by partial differential equations (PDEs) are central to a wide range of scientific and engineering applications.", "method": "Bilevel Local Operator Learning (BiLO) for PDE-constrained optimization problems extended to the Bayesian inference framework. Achieves efficient sampling through gradient-based Markov Chain Monte Carlo (MCMC) methods and low-rank adaptation (LoRA).", "result": "Improves the accuracy of both parameter inference and uncertainty quantification. Demonstrates a direct link between the tolerance for solving the lower level problem and the accuracy of the resulting uncertainty quantification.", "conclusion": "The proposed method delivers accurate inference and quantification of uncertainties while maintaining high computational efficiency."}}
{"id": "2507.17288", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.17288", "abs": "https://arxiv.org/abs/2507.17288", "authors": ["Miaomiao Gao", "Xiaoxiao Xiang", "Yiwen Guo"], "title": "Triple X: A LLM-Based Multilingual Speech Recognition System for the INTERSPEECH2025 MLC-SLM Challenge", "comment": null, "summary": "This paper describes our Triple X speech recognition system submitted to Task\n1 of the Multi-Lingual Conversational Speech Language Modeling (MLC-SLM)\nChallenge. Our work focuses on optimizing speech recognition accuracy in\nmultilingual conversational scenarios through an innovative encoder-adapter-LLM\narchitecture. This framework harnesses the powerful reasoning capabilities of\ntext-based large language models while incorporating domain-specific\nadaptations. To further enhance multilingual recognition performance, we\nadopted a meticulously designed multi-stage training strategy leveraging\nextensive multilingual audio datasets. Experimental results demonstrate that\nour approach achieves competitive Word Error Rate (WER) performance on both dev\nand test sets, obtaining second place in the challenge ranking.", "AI": {"tldr": "Triple X speech recognition system uses encoder-adapter-LLM architecture and multi-stage training to achieve competitive WER performance in multilingual conversational scenarios, ranking second in the MLC-SLM Challenge.", "motivation": "optimizing speech recognition accuracy in multilingual conversational scenarios", "method": "an innovative encoder-adapter-LLM architecture and a meticulously designed multi-stage training strategy leveraging extensive multilingual audio datasets", "result": "achieves competitive Word Error Rate (WER) performance on both dev and test sets", "conclusion": "The system achieves competitive WER performance on both dev and test sets, obtaining second place in the challenge ranking."}}
{"id": "2507.17512", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17512", "abs": "https://arxiv.org/abs/2507.17512", "authors": ["Yu Li", "Zhuoshi Pan", "Honglin Lin", "Mengyuan Sun", "Conghui He", "Lijun Wu"], "title": "Can One Domain Help Others? A Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning", "comment": "27 pages, 24 figures", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npowerful paradigm for enhancing the reasoning capabilities of LLMs. Existing\nresearch has predominantly concentrated on isolated reasoning domains such as\nmathematical problem-solving, coding tasks, or logical reasoning. However, real\nworld reasoning scenarios inherently demand an integrated application of\nmultiple cognitive skills. Despite this, the interplay among these reasoning\nskills under reinforcement learning remains poorly understood. To bridge this\ngap, we present a systematic investigation of multi-domain reasoning within the\nRLVR framework, explicitly focusing on three primary domains: mathematical\nreasoning, code generation, and logical puzzle solving. We conduct a\ncomprehensive study comprising four key components: (1) Leveraging the GRPO\nalgorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates the\nmodels' in-domain improvements and cross-domain generalization capabilities\nwhen trained on single-domain datasets. (2) Additionally, we examine the\nintricate interactions including mutual enhancements and conflicts that emerge\nduring combined cross-domain training. (3) To further understand the influence\nof SFT on RL, we also analyze and compare performance differences between base\nand instruct models under identical RL configurations. (4) Furthermore, we\ndelve into critical RL training details, systematically exploring the impacts\nof curriculum learning strategies, variations in reward design, and\nlanguage-specific factors. Through extensive experiments, our results offer\nsignificant insights into the dynamics governing domain interactions, revealing\nkey factors influencing both specialized and generalizable reasoning\nperformance. These findings provide valuable guidance for optimizing RL\nmethodologies to foster comprehensive, multi-domain reasoning capabilities in\nLLMs.", "AI": {"tldr": "This paper investigates multi-domain reasoning in LLMs within the RLVR framework, focusing on mathematical reasoning, code generation, and logical puzzle solving. The study analyzes domain interactions, the influence of SFT on RL, and critical RL training details to improve multi-domain reasoning capabilities.", "motivation": "Existing research has concentrated on isolated reasoning domains, while real-world scenarios demand integrated application of multiple cognitive skills. The interplay among these reasoning skills under reinforcement learning remains poorly understood.", "method": "Leveraging the GRPO algorithm and the Qwen-2.5-7B model family, the study evaluates models' in-domain improvements and cross-domain generalization capabilities. It also examines interactions during combined cross-domain training and compares performance differences between base and instruct models. The study explores the impacts of curriculum learning strategies, reward design variations, and language-specific factors.", "result": "The results offer significant insights into the dynamics governing domain interactions, revealing key factors influencing both specialized and generalizable reasoning performance.", "conclusion": "This study offers insights into domain interactions and key factors influencing specialized and generalizable reasoning performance, providing guidance for optimizing RL methodologies to foster comprehensive, multi-domain reasoning capabilities in LLMs."}}
{"id": "2507.17000", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17000", "abs": "https://arxiv.org/abs/2507.17000", "authors": ["Jacob Piland", "Chris Sweet", "Adam Czajka"], "title": "Divisive Decisions: Improving Salience-Based Training for Generalization in Binary Classification Tasks", "comment": null, "summary": "Existing saliency-guided training approaches improve model generalization by\nincorporating a loss term that compares the model's class activation map (CAM)\nfor a sample's true-class ({\\it i.e.}, correct-label class) against a human\nreference saliency map. However, prior work has ignored the false-class CAM(s),\nthat is the model's saliency obtained for incorrect-label class. We hypothesize\nthat in binary tasks the true and false CAMs should diverge on the important\nclassification features identified by humans (and reflected in human saliency\nmaps). We use this hypothesis to motivate three new saliency-guided training\nmethods incorporating both true- and false-class model's CAM into the training\nstrategy and a novel post-hoc tool for identifying important features. We\nevaluate all introduced methods on several diverse binary close-set and\nopen-set classification tasks, including synthetic face detection, biometric\npresentation attack detection, and classification of anomalies in chest X-ray\nscans, and find that the proposed methods improve generalization capabilities\nof deep learning models over traditional (true-class CAM only) saliency-guided\ntraining approaches. We offer source codes and model weights\\footnote{GitHub\nrepository link removed to preserve anonymity} to support reproducible\nresearch.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5229\u7528\u771f\u5047\u7c7bCAM\u7684\u663e\u8457\u6027\u5f15\u5bfc\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u5728\u591a\u4e2a\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4ee5\u5f80\u7684\u7814\u7a76\u5ffd\u7565\u4e86\u5047\u7c7bCAM\uff0c\u4e5f\u5c31\u662f\u6a21\u578b\u5bf9\u4e8e\u4e0d\u6b63\u786e\u6807\u7b7e\u7c7b\u83b7\u5f97\u7684\u663e\u8457\u6027\u3002\u6211\u4eec\u5047\u8bbe\u5728\u4e8c\u5143\u4efb\u52a1\u4e2d\uff0c\u771f\u548c\u5047CAM\u5e94\u8be5\u5728\u4eba\u7c7b\u8bc6\u522b\u7684\u91cd\u8981\u5206\u7c7b\u7279\u5f81\u4e0a\u6709\u6240\u4e0d\u540c\uff08\u5e76\u5728\u4eba\u7c7b\u663e\u8457\u6027\u5730\u56fe\u4e2d\u6709\u6240\u4f53\u73b0\uff09\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u65b0\u7684\u663e\u8457\u6027\u5f15\u5bfc\u8bad\u7ec3\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5c06\u771f\u7c7b\u548c\u5047\u7c7b\u6a21\u578bCAM\u90fd\u7eb3\u5165\u8bad\u7ec3\u7b56\u7565\u4e2d\uff0c\u4ee5\u53ca\u4e00\u79cd\u7528\u4e8e\u8bc6\u522b\u91cd\u8981\u7279\u5f81\u7684\u65b0\u9896\u7684post-hoc\u5de5\u5177\u3002", "result": "\u5728\u51e0\u4e2a\u4e0d\u540c\u7684\u4e8c\u5143\u5c01\u95ed\u96c6\u548c\u5f00\u653e\u96c6\u5206\u7c7b\u4efb\u52a1\uff08\u5305\u62ec\u5408\u6210\u4eba\u8138\u68c0\u6d4b\u3001\u751f\u7269\u7279\u5f81\u5448\u73b0\u653b\u51fb\u68c0\u6d4b\u548c\u80f8\u90e8X\u5c04\u7ebf\u626b\u63cf\u5f02\u5e38\u5206\u7c7b\uff09\u4e2d\uff0c\u5bf9\u6240\u6709\u5f15\u5165\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u53d1\u73b0\u63d0\u51fa\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e0a\uff0c\u76f8\u6bd4\u4e8e\u4f20\u7edf\u7684\uff08\u4ec5\u4f7f\u7528\u771f\u7c7bCAM\uff09\u663e\u8457\u6027\u5f15\u5bfc\u8bad\u7ec3\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.17402", "categories": ["cs.CV", "cs.IR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.17402", "abs": "https://arxiv.org/abs/2507.17402", "authors": ["Li Jun", "Wang Jinpeng", "Tan Chaolei", "Lian Niu", "Chen Long", "Zhang Min", "Wang Yaowei", "Xia Shu-Tao", "Chen Bin"], "title": "HLFormer: Enhancing Partially Relevant Video Retrieval with Hyperbolic Learning", "comment": "Accepted by ICCV'25. 13 pages, 6 figures, 4 tables", "summary": "Partially Relevant Video Retrieval (PRVR) addresses the critical challenge of\nmatching untrimmed videos with text queries describing only partial content.\nExisting methods suffer from geometric distortion in Euclidean space that\nsometimes misrepresents the intrinsic hierarchical structure of videos and\noverlooks certain hierarchical semantics, ultimately leading to suboptimal\ntemporal modeling. To address this issue, we propose the first hyperbolic\nmodeling framework for PRVR, namely HLFormer, which leverages hyperbolic space\nlearning to compensate for the suboptimal hierarchical modeling capabilities of\nEuclidean space. Specifically, HLFormer integrates the Lorentz Attention Block\nand Euclidean Attention Block to encode video embeddings in hybrid spaces,\nusing the Mean-Guided Adaptive Interaction Module to dynamically fuse features.\nAdditionally, we introduce a Partial Order Preservation Loss to enforce \"text <\nvideo\" hierarchy through Lorentzian cone constraints. This approach further\nenhances cross-modal matching by reinforcing partial relevance between video\ncontent and text queries. Extensive experiments show that HLFormer outperforms\nstate-of-the-art methods. Code is released at\nhttps://github.com/lijun2005/ICCV25-HLFormer.", "AI": {"tldr": "HLFormer\uff1a\u9996\u4e2a\u7528\u4e8e\u90e8\u5206\u76f8\u5173\u89c6\u9891\u68c0\u7d22\u7684\u53cc\u66f2\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u66f2\u7a7a\u95f4\u5b66\u4e60\u6765\u8865\u507f\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u7684\u6b21\u4f18\u5c42\u6b21\u5efa\u6a21\u80fd\u529b\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u90e8\u5206\u76f8\u5173\u89c6\u9891\u68c0\u7d22 (PRVR) \u65b9\u6cd5\u5728\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e2d\u5b58\u5728\u51e0\u4f55\u5931\u771f\uff0c\u6709\u65f6\u4f1a\u9519\u8bef\u5730\u8868\u793a\u89c6\u9891\u7684\u5185\u5728\u5c42\u6b21\u7ed3\u6784\uff0c\u5e76\u5ffd\u7565\u67d0\u4e9b\u5c42\u6b21\u8bed\u4e49\uff0c\u6700\u7ec8\u5bfc\u81f4\u6b21\u4f18\u7684\u65f6\u95f4\u5efa\u6a21\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u53cc\u66f2\u5efa\u6a21\u6846\u67b6 HLFormer\uff0c\u5b83\u7ed3\u5408\u4e86 Lorentz \u6ce8\u610f\u529b\u5757\u548c\u6b27\u51e0\u91cc\u5f97\u6ce8\u610f\u529b\u5757\u4ee5\u5728\u6df7\u5408\u7a7a\u95f4\u4e2d\u7f16\u7801\u89c6\u9891\u5d4c\u5165\uff0c\u5e76\u4f7f\u7528\u5747\u503c\u5f15\u5bfc\u81ea\u9002\u5e94\u4ea4\u4e92\u6a21\u5757\u6765\u52a8\u6001\u878d\u5408\u7279\u5f81\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u504f\u5e8f\u4fdd\u6301\u635f\u5931\uff0c\u4ee5\u901a\u8fc7\u6d1b\u4f26\u5179\u9525\u7ea6\u675f\u6765\u52a0\u5f3a\u201c\u6587\u672c < \u89c6\u9891\u201d\u7684\u5c42\u6b21\u7ed3\u6784\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cHLFormer \u4f18\u4e8e\u73b0\u6709\u6280\u672f\u6c34\u5e73\u7684\u65b9\u6cd5\u3002", "conclusion": "HLFormer \u5728 PRVR \u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002"}}
{"id": "2507.17056", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17056", "abs": "https://arxiv.org/abs/2507.17056", "authors": ["Anton Matsson", "Yaochen Rao", "Heather J. Litman", "Fredrik D. Johansson"], "title": "Pragmatic Policy Development via Interpretable Behavior Cloning", "comment": null, "summary": "Offline reinforcement learning (RL) holds great promise for deriving optimal\npolicies from observational data, but challenges related to interpretability\nand evaluation limit its practical use in safety-critical domains.\nInterpretability is hindered by the black-box nature of unconstrained RL\npolicies, while evaluation -- typically performed off-policy -- is sensitive to\nlarge deviations from the data-collecting behavior policy, especially when\nusing methods based on importance sampling. To address these challenges, we\npropose a simple yet practical alternative: deriving treatment policies from\nthe most frequently chosen actions in each patient state, as estimated by an\ninterpretable model of the behavior policy. By using a tree-based model, which\nis specifically designed to exploit patterns in the data, we obtain a natural\ngrouping of states with respect to treatment. The tree structure ensures\ninterpretability by design, while varying the number of actions considered\ncontrols the degree of overlap with the behavior policy, enabling reliable\noff-policy evaluation. This pragmatic approach to policy development\nstandardizes frequent treatment patterns, capturing the collective clinical\njudgment embedded in the data. Using real-world examples in rheumatoid\narthritis and sepsis care, we demonstrate that policies derived under this\nframework can outperform current practice, offering interpretable alternatives\nto those obtained via offline RL.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u5b9e\u7528\u7684\u66ff\u4ee3\u65b9\u6848\uff1a\u4ece\u884c\u4e3a\u7b56\u7565\u7684\u53ef\u89e3\u91ca\u6a21\u578b\u4f30\u8ba1\u7684\u6bcf\u4e2a\u60a3\u8005\u72b6\u6001\u4e0b\u6700\u5e38\u9009\u62e9\u7684\u52a8\u4f5c\u4e2d\u5f97\u51fa\u6cbb\u7597\u7b56\u7565\u3002", "motivation": "\u4e0e\u53ef\u89e3\u91ca\u6027\u548c\u8bc4\u4f30\u76f8\u5173\u7684\u6311\u6218\u9650\u5236\u4e86\u5176\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u3002\u53ef\u89e3\u91ca\u6027\u53d7\u5230\u65e0\u7ea6\u675fRL\u7b56\u7565\u7684\u9ed1\u76d2\u6027\u8d28\u7684\u963b\u788d\uff0c\u800c\u8bc4\u4f30\uff08\u901a\u5e38\u662f\u5f02\u7b56\u7565\u6267\u884c\u7684\uff09\u5bf9\u6570\u636e\u6536\u96c6\u884c\u4e3a\u7b56\u7565\u7684\u5de8\u5927\u504f\u5dee\u5f88\u654f\u611f\uff0c\u5c24\u5176\u662f\u5728\u4f7f\u7528\u57fa\u4e8e\u91cd\u8981\u6027\u62bd\u6837\u7684\u65b9\u6cd5\u65f6\u3002", "method": "\u4ece\u6bcf\u4e2a\u60a3\u8005\u72b6\u6001\u4e0b\u6700\u5e38\u9009\u62e9\u7684\u52a8\u4f5c\u4e2d\u5f97\u51fa\u6cbb\u7597\u7b56\u7565\uff0c\u7531\u884c\u4e3a\u7b56\u7565\u7684\u53ef\u89e3\u91ca\u6a21\u578b\u4f30\u8ba1\u3002", "result": "\u4f7f\u7528\u57fa\u4e8e\u6811\u7684\u6a21\u578b\uff0c\u4e13\u95e8\u7528\u4e8e\u5229\u7528\u6570\u636e\u4e2d\u7684\u6a21\u5f0f\uff0c\u83b7\u5f97\u5173\u4e8e\u6cbb\u7597\u7684\u72b6\u6001\u7684\u81ea\u7136\u5206\u7ec4\u3002\u6811\u7ed3\u6784\u901a\u8fc7\u8bbe\u8ba1\u786e\u4fdd\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u540c\u65f6\u6539\u53d8\u6240\u8003\u8651\u7684\u52a8\u4f5c\u6570\u91cf\u63a7\u5236\u4e86\u4e0e\u884c\u4e3a\u7b56\u7565\u7684\u91cd\u53e0\u7a0b\u5ea6\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u5f02\u7b56\u7565\u8bc4\u4f30\u3002", "conclusion": "\u5728\u7c7b\u98ce\u6e7f\u6027\u5173\u8282\u708e\u548c\u8d25\u8840\u75c7\u62a4\u7406\u7684\u771f\u5b9e\u6848\u4f8b\u4e2d\uff0c\u6839\u636e\u8be5\u6846\u67b6\u5f97\u51fa\u7684\u7b56\u7565\u53ef\u4ee5\u80dc\u8fc7\u5f53\u524d\u7684\u5b9e\u8df5\uff0c\u4e3a\u901a\u8fc7\u79bb\u7ebfRL\u83b7\u5f97\u7684\u7b56\u7565\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2507.17514", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17514", "abs": "https://arxiv.org/abs/2507.17514", "authors": ["Athanasios Davvetas", "Xenia Ziouvelou", "Ypatia Dami", "Alexis Kaponis", "Konstantina Giouvanopoulou", "Michael Papademas"], "title": "TAI Scan Tool: A RAG-Based Tool With Minimalistic Input for Trustworthy AI Self-Assessment", "comment": "9 pages, 1 figure, 4 tables", "summary": "This paper introduces the TAI Scan Tool, a RAG-based TAI self-assessment tool\nwith minimalistic input. The current version of the tool supports the legal TAI\nassessment, with a particular emphasis on facilitating compliance with the AI\nAct. It involves a two-step approach with a pre-screening and an assessment\nphase. The assessment output of the system includes insight regarding the\nrisk-level of the AI system according to the AI Act, while at the same time\nretrieving relevant articles to aid with compliance and notify on their\nobligations. Our qualitative evaluation using use-case scenarios yields\npromising results, correctly predicting risk levels while retrieving relevant\narticles across three distinct semantic groups. Furthermore, interpretation of\nresults shows that the tool's reasoning relies on comparison with the setting\nof high-risk systems, a behaviour attributed to their deployment requiring\ncareful consideration, and therefore frequently presented within the AI Act.", "AI": {"tldr": "Introduces a RAG-based TAI self-assessment tool for AI Act compliance, showing promising results in predicting risk levels and retrieving relevant articles.", "motivation": "Facilitating compliance with the AI Act.", "method": "RAG-based TAI self-assessment tool with a two-step approach (pre-screening and assessment).", "result": "Promising results in qualitative evaluation using use-case scenarios, correctly predicting risk levels and retrieving relevant articles across three distinct semantic groups. The tool's reasoning relies on comparison with the setting of high-risk systems.", "conclusion": "The tool correctly predicts risk levels and retrieves relevant articles."}}
{"id": "2507.17008", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17008", "abs": "https://arxiv.org/abs/2507.17008", "authors": ["Gaston Gustavo Rios", "Pedro Dal Bianco", "Franco Ronchetti", "Facundo Quiroga", "Oscar Stanchi", "Santiago Ponte Ah\u00f3n", "Waldo Hasperu\u00e9"], "title": "Bringing Balance to Hand Shape Classification: Mitigating Data Imbalance Through Generative Models", "comment": "23 pages, 8 figures, to be published in Applied Soft Computing", "summary": "Most sign language handshape datasets are severely limited and unbalanced,\nposing significant challenges to effective model training. In this paper, we\nexplore the effectiveness of augmenting the training data of a handshape\nclassifier by generating synthetic data. We use an EfficientNet classifier\ntrained on the RWTH German sign language handshape dataset, which is small and\nheavily unbalanced, applying different strategies to combine generated and real\nimages. We compare two Generative Adversarial Networks (GAN) architectures for\ndata generation: ReACGAN, which uses label information to condition the data\ngeneration process through an auxiliary classifier, and SPADE, which utilizes\nspatially-adaptive normalization to condition the generation on pose\ninformation. ReACGAN allows for the generation of realistic images that align\nwith specific handshape labels, while SPADE focuses on generating images with\naccurate spatial handshape configurations. Our proposed techniques improve the\ncurrent state-of-the-art accuracy on the RWTH dataset by 5%, addressing the\nlimitations of small and unbalanced datasets. Additionally, our method\ndemonstrates the capability to generalize across different sign language\ndatasets by leveraging pose-based generation trained on the extensive HaGRID\ndataset. We achieve comparable performance to single-source trained classifiers\nwithout the need for retraining the generator.", "AI": {"tldr": "\u5229\u7528\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u751f\u6210\u5408\u6210\u6570\u636e\u6765\u589e\u5f3a\u624b\u5f62\u5206\u7c7b\u5668\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u4ece\u800c\u63d0\u9ad8\u7cbe\u5ea6\u5e76\u89e3\u51b3\u6570\u636e\u96c6\u4e0d\u5e73\u8861\u7684\u95ee\u9898\u3002", "motivation": "\u5927\u591a\u6570\u624b\u8bed\u624b\u5f62\u6570\u636e\u96c6\u53d7\u5230\u4e25\u91cd\u9650\u5236\u4e14\u4e0d\u5e73\u8861\uff0c\u7ed9\u6709\u6548\u7684\u6a21\u578b\u8bad\u7ec3\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u901a\u8fc7\u751f\u6210\u5408\u6210\u6570\u636e\u6765\u589e\u5f3a\u624b\u5f62\u5206\u7c7b\u5668\u7684\u8bad\u7ec3\u6570\u636e\u7684\u6709\u6548\u6027\u3002", "method": "\u4f7f\u7528\u5728RWTH German sign language\u624b\u5f62\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684EfficientNet\u5206\u7c7b\u5668\uff0c\u5e94\u7528\u4e0d\u540c\u7684\u7b56\u7565\u6765\u7ec4\u5408\u751f\u6210\u548c\u771f\u5b9e\u56fe\u50cf\u3002\u6bd4\u8f83\u4e86\u4e24\u79cd\u7528\u4e8e\u6570\u636e\u751f\u6210\u7684\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u67b6\u6784\uff1aReACGAN\u548cSPADE\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6280\u672f\u5c06RWTH\u6570\u636e\u96c6\u4e0a\u5f53\u524d\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6\u63d0\u9ad8\u4e865%\u3002\u5b9e\u73b0\u4e86\u4e0e\u5355\u6e90\u8bad\u7ec3\u5206\u7c7b\u5668\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u751f\u6210\u5668\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728RWTH\u6570\u636e\u96c6\u4e0a\u5c06\u5f53\u524d\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6\u63d0\u9ad8\u4e865%\uff0c\u89e3\u51b3\u4e86\u5c0f\u578b\u548c\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u7684\u5c40\u9650\u6027\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u901a\u8fc7\u5229\u7528\u5728\u5e7f\u6cdb\u7684HaGRID\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u57fa\u4e8e\u59ff\u52bf\u7684\u751f\u6210\uff0c\u80fd\u591f\u63a8\u5e7f\u5230\u4e0d\u540c\u7684\u624b\u8bed\u6570\u636e\u96c6\u3002\u5728\u4e0d\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u751f\u6210\u5668\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u4e0e\u5355\u6e90\u8bad\u7ec3\u5206\u7c7b\u5668\u76f8\u5f53\u7684\u6027\u80fd\u3002"}}
{"id": "2507.17412", "categories": ["cs.CV", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.17412", "abs": "https://arxiv.org/abs/2507.17412", "authors": ["Farnaz Khun Jush", "Steffen Vogler", "Matthias Lenga"], "title": "Content-based 3D Image Retrieval and a ColBERT-inspired Re-ranking for Tumor Flagging and Staging", "comment": null, "summary": "The increasing volume of medical images poses challenges for radiologists in\nretrieving relevant cases. Content-based image retrieval (CBIR) systems offer\npotential for efficient access to similar cases, yet lack standardized\nevaluation and comprehensive studies. Building on prior studies for tumor\ncharacterization via CBIR, this study advances CBIR research for volumetric\nmedical images through three key contributions: (1) a framework eliminating\nreliance on pre-segmented data and organ-specific datasets, aligning with large\nand unstructured image archiving systems, i.e. PACS in clinical practice; (2)\nintroduction of C-MIR, a novel volumetric re-ranking method adapting ColBERT's\ncontextualized late interaction mechanism for 3D medical imaging; (3)\ncomprehensive evaluation across four tumor sites using three feature extractors\nand three database configurations. Our evaluations highlight the significant\nadvantages of C-MIR. We demonstrate the successful adaptation of the late\ninteraction principle to volumetric medical images, enabling effective\ncontext-aware re-ranking. A key finding is C-MIR's ability to effectively\nlocalize the region of interest, eliminating the need for pre-segmentation of\ndatasets and offering a computationally efficient alternative to systems\nrelying on expensive data enrichment steps. C-MIR demonstrates promising\nimprovements in tumor flagging, achieving improved performance, particularly\nfor colon and lung tumors (p<0.05). C-MIR also shows potential for improving\ntumor staging, warranting further exploration of its capabilities. Ultimately,\nour work seeks to bridge the gap between advanced retrieval techniques and\ntheir practical applications in healthcare, paving the way for improved\ndiagnostic processes.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86C-MIR\uff0c\u4e00\u79cd\u7528\u4e8e\u4f53\u79ef\u533b\u5b66\u56fe\u50cf\u7684\u65b0\u578b\u91cd\u6392\u5e8f\u65b9\u6cd5\uff0c\u5b83\u5728\u80bf\u7624\u6807\u8bb0\u65b9\u9762\u8868\u73b0\u51fa\u4ee4\u4eba\u9f13\u821e\u7684\u6539\u8fdb\uff0c\u5c24\u5176\u662f\u5728\u7ed3\u80a0\u764c\u548c\u80ba\u764c\u65b9\u9762\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u7684\u4f53\u79ef\u4e0d\u65ad\u589e\u52a0\uff0c\u7ed9\u653e\u5c04\u79d1\u533b\u751f\u68c0\u7d22\u76f8\u5173\u75c5\u4f8b\u5e26\u6765\u4e86\u6311\u6218\u3002\u57fa\u4e8e\u5185\u5bb9\u7684\u56fe\u50cf\u68c0\u7d22\uff08CBIR\uff09\u7cfb\u7edf\u4e3a\u6709\u6548\u8bbf\u95ee\u76f8\u4f3c\u75c5\u4f8b\u63d0\u4f9b\u4e86\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u548c\u7efc\u5408\u7814\u7a76\u3002", "method": "\u5f15\u5165C-MIR\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u4f53\u79ef\u91cd\u6392\u5e8f\u65b9\u6cd5\uff0c\u5b83\u8c03\u6574\u4e86ColBERT\u7684\u4e0a\u4e0b\u6587\u540e\u671f\u4ea4\u4e92\u673a\u5236\u4ee5\u7528\u4e8e3D\u533b\u5b66\u6210\u50cf\u3002", "result": "\u6211\u4eec\u7684\u8bc4\u4f30\u7a81\u51fa\u4e86C-MIR\u7684\u663e\u7740\u4f18\u52bf\u3002\u6211\u4eec\u8bc1\u660e\u4e86\u540e\u671f\u4ea4\u4e92\u539f\u7406\u6210\u529f\u9002\u5e94\u4e8e\u4f53\u79ef\u533b\u5b66\u56fe\u50cf\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u91cd\u6392\u5e8f\u3002\u4e00\u4e2a\u5173\u952e\u53d1\u73b0\u662fC-MIR\u80fd\u591f\u6709\u6548\u5730\u5b9a\u4f4d\u611f\u5174\u8da3\u533a\u57df\uff0c\u65e0\u9700\u5bf9\u6570\u636e\u96c6\u8fdb\u884c\u9884\u5206\u5272\uff0c\u5e76\u4e3a\u4f9d\u8d56\u6602\u8d35\u6570\u636e\u4e30\u5bcc\u6b65\u9aa4\u7684\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "C-MIR\u5728\u80bf\u7624\u6807\u8bb0\u65b9\u9762\u8868\u73b0\u51fa\u4ee4\u4eba\u9f13\u821e\u7684\u6539\u8fdb\uff0c\u5c24\u5176\u662f\u5728\u7ed3\u80a0\u764c\u548c\u80ba\u764c\u65b9\u9762\uff08p<0.05\uff09\u3002C-MIR\u8fd8\u663e\u793a\u51fa\u6539\u5584\u80bf\u7624\u5206\u671f\u7684\u6f5c\u529b\uff0c\u503c\u5f97\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u80fd\u529b\u3002\u6700\u7ec8\uff0c\u6211\u4eec\u7684\u5de5\u4f5c\u65e8\u5728\u5f25\u5408\u5148\u8fdb\u68c0\u7d22\u6280\u672f\u4e0e\u5b83\u4eec\u5728\u533b\u7597\u4fdd\u5065\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u6539\u8fdb\u8bca\u65ad\u8fc7\u7a0b\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2507.17066", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.17066", "abs": "https://arxiv.org/abs/2507.17066", "authors": ["Jessup Byun", "Xiaofeng Lin", "Joshua Ward", "Guang Cheng"], "title": "Risk In Context: Benchmarking Privacy Leakage of Foundation Models in Synthetic Tabular Data Generation", "comment": "Accepted by Agentic & GenAI Evaluation KDD2025, poster presentation", "summary": "Synthetic tabular data is essential for machine learning workflows,\nespecially for expanding small or imbalanced datasets and enabling\nprivacy-preserving data sharing. However, state-of-the-art generative models\n(GANs, VAEs, diffusion models) rely on large datasets with thousands of\nexamples. In low-data settings, often the primary motivation for synthetic\ndata, these models can overfit, leak sensitive records, and require frequent\nretraining. Recent work uses large pre-trained transformers to generate rows\nvia in-context learning (ICL), which needs only a few seed examples and no\nparameter updates, avoiding retraining. But ICL repeats seed rows verbatim,\nintroducing a new privacy risk that has only been studied in text. The severity\nof this risk in tabular synthesis-where a single row may identify a\nperson-remains unclear. We address this gap with the first benchmark of three\nfoundation models (GPT-4o-mini, LLaMA 3.3 70B, TabPFN v2) against four\nbaselines on 35 real-world tables from health, finance, and policy. We evaluate\nstatistical fidelity, downstream utility, and membership inference leakage.\nResults show foundation models consistently have the highest privacy risk.\nLLaMA 3.3 70B reaches up to 54 percentage points higher true-positive rate at\n1% FPR than the safest baseline. GPT-4o-mini and TabPFN are also highly\nvulnerable. We plot the privacy-utility frontier and show that CTGAN and\nGPT-4o-mini offer better tradeoffs. A factorial study finds that three\nzero-cost prompt tweaks-small batch size, low temperature, and using summary\nstatistics-can reduce worst-case AUC by 14 points and rare-class leakage by up\nto 39 points while maintaining over 90% fidelity. Our benchmark offers a\npractical guide for safer low-data synthesis with foundation models.", "AI": {"tldr": "Foundation models for synthetic tabular data in low-data settings have high privacy risks. Prompt tweaks can improve privacy-utility tradeoff.", "motivation": "State-of-the-art generative models rely on large datasets and can overfit, leak sensitive records, and require frequent retraining in low-data settings. In-context learning repeats seed rows verbatim, introducing a new privacy risk.", "method": "Benchmark of three foundation models (GPT-4o-mini, LLaMA 3.3 70B, TabPFN v2) against four baselines on 35 real-world tables from health, finance, and policy. Evaluation of statistical fidelity, downstream utility, and membership inference leakage. Factorial study of zero-cost prompt tweaks.", "result": "Foundation models consistently have the highest privacy risk. LLaMA 3.3 70B reaches up to 54 percentage points higher true-positive rate at 1% FPR than the safest baseline. GPT-4o-mini and TabPFN are also highly vulnerable. Three zero-cost prompt tweaks can reduce worst-case AUC by 14 points and rare-class leakage by up to 39 points while maintaining over 90% fidelity.", "conclusion": "Foundation models consistently have the highest privacy risk. CTGAN and GPT-4o-mini offer better privacy-utility tradeoffs. Three zero-cost prompt tweaks can reduce worst-case AUC and rare-class leakage while maintaining over 90% fidelity."}}
{"id": "2507.17409", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.17409", "abs": "https://arxiv.org/abs/2507.17409", "authors": ["Carlotta Quensel", "Neele Falk", "Gabriella Lapesa"], "title": "Investigating Subjective Factors of Argument Strength: Storytelling, Emotions, and Hedging", "comment": "Accepted to the 12th Workshop on Argument Mining (ArgMining) 2025", "summary": "In assessing argument strength, the notions of what makes a good argument are\nmanifold. With the broader trend towards treating subjectivity as an asset and\nnot a problem in NLP, new dimensions of argument quality are studied. Although\nstudies on individual subjective features like personal stories exist, there is\na lack of large-scale analyses of the relation between these features and\nargument strength. To address this gap, we conduct regression analysis to\nquantify the impact of subjective factors $-$ emotions, storytelling, and\nhedging $-$ on two standard datasets annotated for objective argument quality\nand subjective persuasion. As such, our contribution is twofold: at the level\nof contributed resources, as there are no datasets annotated with all studied\ndimensions, this work compares and evaluates automated annotation methods for\neach subjective feature. At the level of novel insights, our regression\nanalysis uncovers different patterns of impact of subjective features on the\ntwo facets of argument strength encoded in the datasets. Our results show that\nstorytelling and hedging have contrasting effects on objective and subjective\nargument quality, while the influence of emotions depends on their rhetoric\nutilization rather than the domain.", "AI": {"tldr": "\u7814\u7a76\u4e86\u60c5\u611f\u3001\u6545\u4e8b\u53d9\u8ff0\u548c\u5bf9\u51b2\u7b49\u4e3b\u89c2\u56e0\u7d20\u5bf9\u8bba\u8bc1\u5f3a\u5ea6\u7684\u5f71\u54cd\u3002", "motivation": "\u7f3a\u4e4f\u5bf9\u4e3b\u89c2\u7279\u5f81\u4e0e\u8bba\u8bc1\u5f3a\u5ea6\u4e4b\u95f4\u5173\u7cfb\u7684\u5927\u89c4\u6a21\u5206\u6790\u3002", "method": "\u56de\u5f52\u5206\u6790", "result": "\u6545\u4e8b\u53d9\u8ff0\u548c\u5bf9\u51b2\u5bf9\u5ba2\u89c2\u548c\u4e3b\u89c2\u7684\u8bba\u8bc1\u8d28\u91cf\u6709\u76f8\u53cd\u7684\u5f71\u54cd\uff0c\u800c\u60c5\u611f\u7684\u5f71\u54cd\u53d6\u51b3\u4e8e\u5176\u4fee\u8f9e\u8fd0\u7528\u800c\u975e\u9886\u57df\u3002", "conclusion": "\u6545\u4e8b\u53d9\u8ff0\u548c\u5bf9\u51b2\u5bf9\u5ba2\u89c2\u548c\u4e3b\u89c2\u7684\u8bba\u8bc1\u8d28\u91cf\u6709\u76f8\u53cd\u7684\u5f71\u54cd\uff0c\u800c\u60c5\u611f\u7684\u5f71\u54cd\u53d6\u51b3\u4e8e\u5176\u4fee\u8f9e\u8fd0\u7528\u800c\u975e\u9886\u57df\u3002"}}
{"id": "2507.17539", "categories": ["cs.AI", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.17539", "abs": "https://arxiv.org/abs/2507.17539", "authors": ["Xinyao Liu", "Diping Song"], "title": "Constructing Ophthalmic MLLM for Positioning-diagnosis Collaboration Through Clinical Cognitive Chain Reasoning", "comment": null, "summary": "Multimodal large language models (MLLMs) demonstrate significant potential in\nthe field of medical diagnosis. However, they face critical challenges in\nspecialized domains such as ophthalmology, particularly the fragmentation of\nannotation granularity and inconsistencies in clinical reasoning logic, which\nhinder precise cross-modal understanding. This paper introduces FundusExpert,\nan ophthalmology-specific MLLM with integrated positioning-diagnosis reasoning\ncapabilities, along with FundusGen, a dataset constructed through the\nintelligent Fundus-Engine system. Fundus-Engine automates localization and\nleverages MLLM-based semantic expansion to integrate global disease\nclassification, local object detection, and fine-grained feature analysis\nwithin a single fundus image. Additionally, by constructing a clinically\naligned cognitive chain, it guides the model to generate interpretable\nreasoning paths. FundusExpert, fine-tuned with instruction data from FundusGen,\nachieves the best performance in ophthalmic question-answering tasks,\nsurpassing the average accuracy of the 40B MedRegA by 26.6%. It also excels in\nzero-shot report generation tasks, achieving a clinical consistency of 77.0%,\nsignificantly outperforming GPT-4o's 47.6%. Furthermore, we reveal a scaling\nlaw between data quality and model capability ($L \\propto N^{0.068}$),\ndemonstrating that the cognitive alignment annotations in FundusGen enhance\ndata utilization efficiency. By integrating region-level localization with\ndiagnostic reasoning chains, our work develops a scalable, clinically-aligned\nMLLM and explores a pathway toward bridging the visual-language gap in specific\nMLLMs. Our project can be found at https://github.com/MeteorElf/FundusExpert.", "AI": {"tldr": "The paper introduces FundusExpert, an ophthalmology-specific MLLM and a new dataset, FundusGen. FundusExpert shows improved performance in ophthalmic tasks by integrating positioning-diagnosis reasoning capabilities and using clinically aligned cognitive chains.", "motivation": "MLLMs face challenges in ophthalmology due to fragmented annotation granularity and inconsistencies in clinical reasoning logic, hindering precise cross-modal understanding.", "method": "The paper introduces FundusExpert, an ophthalmology-specific MLLM, and FundusGen, a dataset constructed through the intelligent Fundus-Engine system. A clinically aligned cognitive chain guides the model to generate interpretable reasoning paths.", "result": "FundusExpert achieves the best performance in ophthalmic question-answering tasks, surpassing MedRegA by 26.6%, and excels in zero-shot report generation tasks, outperforming GPT-4o with a clinical consistency of 77.0%. The paper also reveals a scaling law between data quality and model capability.", "conclusion": "This paper develops a scalable, clinically-aligned MLLM by integrating region-level localization with diagnostic reasoning chains, bridging the visual-language gap in specific MLLMs."}}
{"id": "2507.17038", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17038", "abs": "https://arxiv.org/abs/2507.17038", "authors": ["Muhammad Kamran", "Mohammad Moein Sheikholeslami", "Andreas Wichmann", "Gunho Sohn"], "title": "Transformer Based Building Boundary Reconstruction using Attraction Field Maps", "comment": null, "summary": "In recent years, the number of remote satellites orbiting the Earth has grown\nsignificantly, streaming vast amounts of high-resolution visual data to support\ndiverse applications across civil, public, and military domains. Among these\napplications, the generation and updating of spatial maps of the built\nenvironment have become critical due to the extensive coverage and detailed\nimagery provided by satellites. However, reconstructing spatial maps from\nsatellite imagery is a complex computer vision task, requiring the creation of\nhigh-level object representations, such as primitives, to accurately capture\nthe built environment. While the past decade has witnessed remarkable\nadvancements in object detection and representation using visual data,\nprimitives-based object representation remains a persistent challenge in\ncomputer vision. Consequently, high-quality spatial maps often rely on\nlabor-intensive and manual processes. This paper introduces a novel deep\nlearning methodology leveraging Graph Convolutional Networks (GCNs) to address\nthese challenges in building footprint reconstruction. The proposed approach\nenhances performance by incorporating geometric regularity into building\nboundaries, integrating multi-scale and multi-resolution features, and\nembedding Attraction Field Maps into the network. These innovations provide a\nscalable and precise solution for automated building footprint extraction from\na single satellite image, paving the way for impactful applications in urban\nplanning, disaster management, and large-scale spatial analysis. Our model,\nDecoupled-PolyGCN, outperforms existing methods by 6% in AP and 10% in AR,\ndemonstrating its ability to deliver accurate and regularized building\nfootprints across diverse and challenging scenarios.", "AI": {"tldr": "This paper introduces a novel GCN-based deep learning methodology for automated building footprint extraction from satellite images. The model, Decoupled-PolyGCN, outperforms existing methods by 6% in AP and 10% in AR.", "motivation": "Reconstructing spatial maps from satellite imagery is a complex computer vision task, requiring the creation of high-level object representations, such as primitives, to accurately capture the built environment. Consequently, high-quality spatial maps often rely on labor-intensive and manual processes.", "method": "This paper introduces a novel deep learning methodology leveraging Graph Convolutional Networks (GCNs) to address these challenges in building footprint reconstruction. The proposed approach enhances performance by incorporating geometric regularity into building boundaries, integrating multi-scale and multi-resolution features, and embedding Attraction Field Maps into the network.", "result": "The proposed approach enhances performance by incorporating geometric regularity into building boundaries, integrating multi-scale and multi-resolution features, and embedding Attraction Field Maps into the network. Decoupled-PolyGCN, outperforms existing methods by 6% in AP and 10% in AR", "conclusion": "The Decoupled-PolyGCN model outperforms existing methods by 6% in AP and 10% in AR, demonstrating its ability to deliver accurate and regularized building footprints across diverse and challenging scenarios."}}
{"id": "2507.17472", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.17472", "abs": "https://arxiv.org/abs/2507.17472", "authors": ["Junhua Liu", "Roy Ka-Wei Lee", "Kwan Hui Lim"], "title": "BGM-HAN: A Hierarchical Attention Network for Accurate and Fair Decision Assessment on Semi-Structured Profiles", "comment": "Accepted at ASONAM 2025", "summary": "Human decision-making in high-stakes domains often relies on expertise and\nheuristics, but is vulnerable to hard-to-detect cognitive biases that threaten\nfairness and long-term outcomes. This work presents a novel approach to\nenhancing complex decision-making workflows through the integration of\nhierarchical learning alongside various enhancements. Focusing on university\nadmissions as a representative high-stakes domain, we propose BGM-HAN, an\nenhanced Byte-Pair Encoded, Gated Multi-head Hierarchical Attention Network,\ndesigned to effectively model semi-structured applicant data. BGM-HAN captures\nmulti-level representations that are crucial for nuanced assessment, improving\nboth interpretability and predictive performance. Experimental results on real\nadmissions data demonstrate that our proposed model significantly outperforms\nboth state-of-the-art baselines from traditional machine learning to large\nlanguage models, offering a promising framework for augmenting decision-making\nin domains where structure, context, and fairness matter. Source code is\navailable at: https://github.com/junhua/bgm-han.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u5206\u5c42\u5b66\u4e60\u548c\u5404\u79cd\u589e\u5f3a\u529f\u80fd\u6765\u589e\u5f3a\u590d\u6742\u7684\u51b3\u7b56\u5de5\u4f5c\u6d41\u7a0b\u3002", "motivation": "\u9ad8\u98ce\u9669\u9886\u57df\u7684\u4eba\u7c7b\u51b3\u7b56\u901a\u5e38\u4f9d\u8d56\u4e8e\u4e13\u4e1a\u77e5\u8bc6\u548c\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u4f46\u5bb9\u6613\u53d7\u5230\u96be\u4ee5\u68c0\u6d4b\u7684\u8ba4\u77e5\u504f\u5dee\u7684\u5f71\u54cd\uff0c\u8fd9\u4e9b\u504f\u5dee\u5a01\u80c1\u5230\u516c\u5e73\u6027\u548c\u957f\u671f\u7ed3\u679c\u3002", "method": "BGM-HAN\uff0c\u4e00\u79cd\u589e\u5f3a\u7684Byte-Pair\u7f16\u7801\u3001\u95e8\u63a7\u591a\u5934\u5206\u5c42\u6ce8\u610f\u529b\u7f51\u7edc\uff0c\u65e8\u5728\u6709\u6548\u5730\u5efa\u6a21\u534a\u7ed3\u6784\u5316\u7684\u7533\u8bf7\u4eba\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u3002", "conclusion": "\u63d0\u51fa\u7684\u6a21\u578b\u5728\u771f\u5b9e\u62db\u751f\u6570\u636e\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u4e3a\u5728\u7ed3\u6784\u3001\u4e0a\u4e0b\u6587\u548c\u516c\u5e73\u6027\u91cd\u8981\u7684\u9886\u57df\u4e2d\u589e\u5f3a\u51b3\u7b56\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u6846\u67b6\u3002"}}
{"id": "2507.17070", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17070", "abs": "https://arxiv.org/abs/2507.17070", "authors": ["Adithya Mohan", "Dominik R\u00f6\u00dfle", "Daniel Cremers", "Torsten Sch\u00f6n"], "title": "Advancing Robustness in Deep Reinforcement Learning with an Ensemble Defense Approach", "comment": "6 pages, 4 figures, 2 tables", "summary": "Recent advancements in Deep Reinforcement Learning (DRL) have demonstrated\nits applicability across various domains, including robotics, healthcare,\nenergy optimization, and autonomous driving. However, a critical question\nremains: How robust are DRL models when exposed to adversarial attacks? While\nexisting defense mechanisms such as adversarial training and distillation\nenhance the resilience of DRL models, there remains a significant research gap\nregarding the integration of multiple defenses in autonomous driving scenarios\nspecifically. This paper addresses this gap by proposing a novel ensemble-based\ndefense architecture to mitigate adversarial attacks in autonomous driving. Our\nevaluation demonstrates that the proposed architecture significantly enhances\nthe robustness of DRL models. Compared to the baseline under FGSM attacks, our\nensemble method improves the mean reward from 5.87 to 18.38 (over 213%\nincrease) and reduces the mean collision rate from 0.50 to 0.09 (an 82%\ndecrease) in the highway scenario and merge scenario, outperforming all\nstandalone defense strategies.", "AI": {"tldr": "This paper introduces an ensemble defense architecture for DRL models in autonomous driving, which significantly improves robustness against adversarial attacks.", "motivation": "Existing defense mechanisms have a research gap regarding the integration of multiple defenses in autonomous driving scenarios.", "method": "This paper proposes a novel ensemble-based defense architecture to mitigate adversarial attacks in autonomous driving.", "result": "The ensemble method improves the mean reward from 5.87 to 18.38 (over 213% increase) and reduces the mean collision rate from 0.50 to 0.09 (an 82% decrease) in the highway scenario and merge scenario, outperforming all standalone defense strategies.", "conclusion": "The proposed ensemble architecture significantly enhances the robustness of DRL models, improving mean reward and reducing collision rate compared to baseline and standalone defenses."}}
{"id": "2507.17442", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17442", "abs": "https://arxiv.org/abs/2507.17442", "authors": ["Shiting Chen", "Zijian Zhao", "Jinsong Chen"], "title": "Each to Their Own: Exploring the Optimal Embedding in RAG", "comment": null, "summary": "Recently, as Large Language Models (LLMs) have fundamentally impacted various\nfields, the methods for incorporating up-to-date information into LLMs or\nadding external knowledge to construct domain-specific models have garnered\nwide attention. Retrieval-Augmented Generation (RAG), serving as an\ninference-time scaling method, is notable for its low cost and minimal effort\nfor parameter tuning. However, due to heterogeneous training data and model\narchitecture, the variant embedding models used in RAG exhibit different\nbenefits across various areas, often leading to different similarity\ncalculation results and, consequently, varying response quality from LLMs. To\naddress this problem, we propose and examine two approaches to enhance RAG by\ncombining the benefits of multiple embedding models, named Mixture-Embedding\nRAG and Confident RAG. Mixture-Embedding RAG simply sorts and selects\nretrievals from multiple embedding models based on standardized similarity;\nhowever, it does not outperform vanilla RAG. In contrast, Confident RAG\ngenerates responses multiple times using different embedding models and then\nselects the responses with the highest confidence level, demonstrating average\nimprovements of approximately 10% and 5% over vanilla LLMs and RAG,\nrespectively. The consistent results across different LLMs and embedding models\nindicate that Confident RAG is an efficient plug-and-play approach for various\ndomains. We will release our code upon publication.", "AI": {"tldr": "This paper introduces Confident RAG, a method that leverages multiple embedding models to enhance RAG performance by selecting the most confident responses, achieving improvements over vanilla LLMs and RAG.", "motivation": "Methods for incorporating up-to-date information into LLMs or adding external knowledge to construct domain-specific models have garnered wide attention. Retrieval-Augmented Generation (RAG) is notable for its low cost and minimal effort for parameter tuning. However, the variant embedding models used in RAG exhibit different benefits across various areas, often leading to different similarity calculation results and, consequently, varying response quality from LLMs.", "method": "We propose and examine two approaches to enhance RAG by combining the benefits of multiple embedding models, named Mixture-Embedding RAG and Confident RAG.", "result": "Mixture-Embedding RAG does not outperform vanilla RAG. Confident RAG generates responses multiple times using different embedding models and then selects the responses with the highest confidence level, demonstrating average improvements of approximately 10% and 5% over vanilla LLMs and RAG, respectively. The consistent results across different LLMs and embedding models indicate that Confident RAG is an efficient plug-and-play approach for various domains.", "conclusion": "Confident RAG is an efficient plug-and-play approach for various domains, demonstrating average improvements of approximately 10% and 5% over vanilla LLMs and RAG, respectively."}}
{"id": "2507.17680", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.17680", "abs": "https://arxiv.org/abs/2507.17680", "authors": ["Yongchao Zeng", "Calum Brown", "Ioannis Kyriakou", "Ronja Hotz", "Mark Rounsevell"], "title": "Simulating multiple human perspectives in socio-ecological systems using large language models", "comment": null, "summary": "Understanding socio-ecological systems requires insights from diverse\nstakeholder perspectives, which are often hard to access. To enable\nalternative, simulation-based exploration of different stakeholder\nperspectives, we develop the HoPeS (Human-Oriented Perspective Shifting)\nmodelling framework. HoPeS employs agents powered by large language models\n(LLMs) to represent various stakeholders; users can step into the agent roles\nto experience perspectival differences. A simulation protocol serves as a\n\"scaffold\" to streamline multiple perspective-taking simulations, supporting\nusers in reflecting on, transitioning between, and integrating across\nperspectives. A prototype system is developed to demonstrate HoPeS in the\ncontext of institutional dynamics and land use change, enabling both\nnarrative-driven and numerical experiments. In an illustrative experiment, a\nuser successively adopts the perspectives of a system observer and a researcher\n- a role that analyses data from the embedded land use model to inform\nevidence-based decision-making for other LLM agents representing various\ninstitutions. Despite the user's effort to recommend technically sound\npolicies, discrepancies persist between the policy recommendation and\nimplementation due to stakeholders' competing advocacies, mirroring real-world\nmisalignment between researcher and policymaker perspectives. The user's\nreflection highlights the subjective feelings of frustration and disappointment\nas a researcher, especially due to the challenge of maintaining political\nneutrality while attempting to gain political influence. Despite this, the user\nexhibits high motivation to experiment with alternative narrative framing\nstrategies, suggesting the system's potential in exploring different\nperspectives. Further system and protocol refinement are likely to enable new\nforms of interdisciplinary collaboration in socio-ecological simulations.", "AI": {"tldr": "HoPeS modelling framework employs LLM agents to represent various stakeholders; users can step into the agent roles to experience perspectival differences.", "motivation": "Understanding socio-ecological systems requires insights from diverse stakeholder perspectives, which are often hard to access.", "method": "HoPeS employs agents powered by large language models (LLMs) to represent various stakeholders", "result": "discrepancies persist between the policy recommendation and implementation due to stakeholders' competing advocacies, mirroring real-world misalignment between researcher and policymaker perspectives. The user's reflection highlights the subjective feelings of frustration and disappointment as a researcher, especially due to the challenge of maintaining political neutrality while attempting to gain political influence.", "conclusion": "Further system and protocol refinement are likely to enable new forms of interdisciplinary collaboration in socio-ecological simulations."}}
{"id": "2507.17047", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17047", "abs": "https://arxiv.org/abs/2507.17047", "authors": ["Kuleen Sasse", "Efsun Sarioglu Kayi", "Arun Reddy"], "title": "Controllable Hybrid Captioner for Improved Long-form Video Understanding", "comment": null, "summary": "Video data, especially long-form video, is extremely dense and\nhigh-dimensional. Text-based summaries of video content offer a way to\nrepresent query-relevant content in a much more compact manner than raw video.\nIn addition, textual representations are easily ingested by state-of-the-art\nlarge language models (LLMs), which enable reasoning over video content to\nanswer complex natural language queries. To solve this issue, we rely on the\nprogressive construction of a text-based memory by a video captioner operating\non shorter chunks of the video, where spatio-temporal modeling is\ncomputationally feasible. We explore ways to improve the quality of the\nactivity log comprised solely of short video captions. Because the video\ncaptions tend to be focused on human actions, and questions may pertain to\nother information in the scene, we seek to enrich the memory with static scene\ndescriptions using Vision Language Models (VLMs). Our video understanding\nsystem relies on the LaViLa video captioner in combination with a LLM to answer\nquestions about videos. We first explored different ways of partitioning the\nvideo into meaningful segments such that the textual descriptions more\naccurately reflect the structure of the video content. Furthermore, we\nincorporated static scene descriptions into the captioning pipeline using LLaVA\nVLM, resulting in a more detailed and complete caption log and expanding the\nspace of questions that are answerable from the textual memory. Finally, we\nhave successfully fine-tuned the LaViLa video captioner to produce both action\nand scene captions, significantly improving the efficiency of the captioning\npipeline compared to using separate captioning models for the two tasks. Our\nmodel, controllable hybrid captioner, can alternate between different types of\ncaptions according to special input tokens that signals scene changes detected\nin the video.", "AI": {"tldr": "The paper introduces a system that uses a video captioner and a LLM to answer questions about videos. It improves captioning by adding scene descriptions and fine-tuning the captioner to produce both action and scene captions.", "motivation": "Video data is dense and high-dimensional. Text-based summaries offer a more compact representation for query-relevant content and enable reasoning over video content using LLMs.", "method": "The authors rely on the progressive construction of a text-based memory by a video captioner on shorter video chunks. They incorporate static scene descriptions using LLaVA VLM and fine-tune the LaViLa video captioner.", "result": "The authors improved the quality of the activity log by enriching it with static scene descriptions. They explored different ways of partitioning the video into meaningful segments and incorporated static scene descriptions using LLaVA VLM. The fine-tuned LaViLa video captioner produces both action and scene captions.", "conclusion": "The authors successfully fine-tuned the LaViLa video captioner to produce both action and scene captions, significantly improving the efficiency of the captioning pipeline. They introduced a controllable hybrid captioner that alternates between different types of captions based on scene changes detected in the video."}}
{"id": "2507.17071", "categories": ["cs.LG", "cs.SY", "eess.SP", "eess.SY", "physics.ins-det"], "pdf": "https://arxiv.org/pdf/2507.17071", "abs": "https://arxiv.org/abs/2507.17071", "authors": ["Juntao Lin", "Xianghao Zhan"], "title": "Sensor Drift Compensation in Electronic-Nose-Based Gas Recognition Using Knowledge Distillation", "comment": "9 pages", "summary": "Due to environmental changes and sensor aging, sensor drift challenges the\nperformance of electronic nose systems in gas classification during real-world\ndeployment. Previous studies using the UCI Gas Sensor Array Drift Dataset\nreported promising drift compensation results but lacked robust statistical\nexperimental validation and may overcompensate for sensor drift, losing\nclass-related variance.To address these limitations and improve sensor drift\ncompensation with statistical rigor, we first designed two domain adaptation\ntasks based on the same electronic nose dataset: using the first batch to\npredict the remaining batches, simulating a controlled laboratory setting; and\npredicting the next batch using all prior batches, simulating continuous\ntraining data updates for online training. We then systematically tested three\nmethods: our proposed novel Knowledge Distillation (KD) method, the benchmark\nmethod Domain Regularized Component Analysis (DRCA), and a hybrid method\nKD-DRCA, across 30 random test set partitions on the UCI dataset. We showed\nthat KD consistently outperformed both DRCA and KD-DRCA, achieving up to an 18%\nimprovement in accuracy and 15% in F1-score, demonstrating KD's superior\neffectiveness in drift compensation. This is the first application of KD for\nelectronic nose drift mitigation, significantly outperforming the previous\nstate-of-the-art DRCA method and enhancing the reliability of sensor drift\ncompensation in real-world environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f(KD)\u7684\u7535\u5b50\u9f3b\u4f20\u611f\u5668\u6f02\u79fb\u8865\u507f\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684DRCA\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u73af\u5883\u53d8\u5316\u548c\u4f20\u611f\u5668\u8001\u5316\uff0c\u4f20\u611f\u5668\u6f02\u79fb\u5bf9\u7535\u5b50\u9f3b\u7cfb\u7edf\u5728\u5b9e\u9645\u90e8\u7f72\u671f\u95f4\u7684\u6c14\u4f53\u5206\u7c7b\u6027\u80fd\u63d0\u51fa\u4e86\u6311\u6218\u3002\u5148\u524d\u7684\u7814\u7a76\u7f3a\u4e4f\u7a33\u5065\u7684\u7edf\u8ba1\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5e76\u4e14\u53ef\u80fd\u8fc7\u5ea6\u8865\u507f\u4f20\u611f\u5668\u6f02\u79fb\uff0c\u4ece\u800c\u5931\u53bb\u4e0e\u7c7b\u522b\u76f8\u5173\u7684\u65b9\u5dee\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u77e5\u8bc6\u84b8\u998f(KD)\u65b9\u6cd5\uff0c\u57fa\u51c6\u65b9\u6cd5\u662f\u57df\u6b63\u5219\u5316\u6210\u5206\u5206\u6790(DRCA)\uff0c\u4ee5\u53ca\u4e00\u79cd\u6df7\u5408\u65b9\u6cd5KD-DRCA\u3002", "result": "KD\u59cb\u7ec8\u4f18\u4e8eDRCA\u548cKD-DRCA\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u4e8618%\uff0cF1\u5206\u6570\u63d0\u9ad8\u4e8615%\uff0c\u8bc1\u660e\u4e86KD\u5728\u6f02\u79fb\u8865\u507f\u65b9\u9762\u7684\u5353\u8d8a\u6709\u6548\u6027\u3002", "conclusion": "\u77e5\u8bc6\u84b8\u998f(KD)\u5728\u7535\u5b50\u9f3b\u6f02\u79fb\u7f13\u89e3\u65b9\u9762\u9996\u6b21\u5e94\u7528\uff0c\u663e\u8457\u4f18\u4e8e\u5148\u524d\u7684\u6700\u5148\u8fdb\u7684DRCA\u65b9\u6cd5\uff0c\u5e76\u589e\u5f3a\u4e86\u771f\u5b9e\u73af\u5883\u4e2d\u4f20\u611f\u5668\u6f02\u79fb\u8865\u507f\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2507.17476", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17476", "abs": "https://arxiv.org/abs/2507.17476", "authors": ["Alexander R. Fabbri", "Diego Mares", "Jorge Flores", "Meher Mankikar", "Ernesto Hernandez", "Dean Lee", "Bing Liu", "Chen Xing"], "title": "MultiNRC: A Challenging and Native Multilingual Reasoning Evaluation Benchmark for LLMs", "comment": null, "summary": "Although recent Large Language Models (LLMs) have shown rapid improvement on\nreasoning benchmarks in English, the evaluation of such LLMs' multilingual\nreasoning capability across diverse languages and cultural contexts remains\nlimited. Existing multilingual reasoning benchmarks are typically constructed\nby translating existing English reasoning benchmarks, biasing these benchmarks\ntowards reasoning problems with context in English language/cultures. In this\nwork, we introduce the Multilingual Native Reasoning Challenge (MultiNRC), a\nbenchmark designed to assess LLMs on more than 1,000 native, linguistic and\nculturally grounded reasoning questions written by native speakers in French,\nSpanish, and Chinese. MultiNRC covers four core reasoning categories:\nlanguage-specific linguistic reasoning, wordplay & riddles, cultural/tradition\nreasoning, and math reasoning with cultural relevance. For cultural/tradition\nreasoning and math reasoning with cultural relevance, we also provide English\nequivalent translations of the multilingual questions by manual translation\nfrom native speakers fluent in English. This set of English equivalents can\nprovide a direct comparison of LLM reasoning capacity in other languages vs.\nEnglish on the same reasoning questions. We systematically evaluate current 14\nleading LLMs covering most LLM families on MultiNRC and its English equivalent\nset. The results show that (1) current LLMs are still not good at native\nmultilingual reasoning, with none scoring above 50% on MultiNRC; (2) LLMs\nexhibit distinct strengths and weaknesses in handling linguistic, cultural, and\nlogical reasoning tasks; (3) Most models perform substantially better in math\nreasoning in English compared to in original languages (+10%), indicating\npersistent challenges with culturally grounded knowledge.", "AI": {"tldr": "This paper introduces MultiNRC, a new benchmark to evaluate LLMs on native multilingual reasoning, and finds that current LLMs are not good at it.", "motivation": "evaluation of such LLMs' multilingual reasoning capability across diverse languages and cultural contexts remains limited. Existing multilingual reasoning benchmarks are typically constructed by translating existing English reasoning benchmarks, biasing these benchmarks towards reasoning problems with context in English language/cultures", "method": "introduce the Multilingual Native Reasoning Challenge (MultiNRC), a benchmark designed to assess LLMs on more than 1,000 native, linguistic and culturally grounded reasoning questions written by native speakers in French, Spanish, and Chinese", "result": "current LLMs are still not good at native multilingual reasoning, with none scoring above 50% on MultiNRC", "conclusion": "current LLMs are still not good at native multilingual reasoning, with none scoring above 50% on MultiNRC"}}
{"id": "2507.17695", "categories": ["cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.17695", "abs": "https://arxiv.org/abs/2507.17695", "authors": ["Ilias Chatzistefanidis", "Navid Nikaein"], "title": "Symbiotic Agents: A Novel Paradigm for Trustworthy AGI-driven Networks", "comment": "Submitted to Computer Networks AI for 6G", "summary": "Large Language Model (LLM)-based autonomous agents are expected to play a\nvital role in the evolution of 6G networks, by empowering real-time\ndecision-making related to management and service provisioning to end-users.\nThis shift facilitates the transition from a specialized intelligence approach,\nwhere artificial intelligence (AI) algorithms handle isolated tasks, to\nartificial general intelligence (AGI)-driven networks, where agents possess\nbroader reasoning capabilities and can manage diverse network functions. In\nthis paper, we introduce a novel agentic paradigm that combines LLMs with\nreal-time optimization algorithms towards Trustworthy AI, defined as symbiotic\nagents. Optimizers at the LLM's input-level provide bounded uncertainty\nsteering for numerically precise tasks, whereas output-level optimizers\nsupervised by the LLM enable adaptive real-time control. We design and\nimplement two novel agent types including: (i) Radio Access Network optimizers,\nand (ii) multi-agent negotiators for Service-Level Agreements (SLAs). We\nfurther propose an end-to-end architecture for AGI networks and evaluate it on\na 5G testbed capturing channel fluctuations from moving vehicles. Results show\nthat symbiotic agents reduce decision errors fivefold compared to standalone\nLLM-based agents, while smaller language models (SLM) achieve similar accuracy\nwith a 99.9% reduction in GPU resource overhead and in near-real-time loops of\n82 ms. A multi-agent demonstration for collaborative RAN on the real-world\ntestbed highlights significant flexibility in service-level agreement and\nresource allocation, reducing RAN over-utilization by approximately 44%.\nDrawing on our findings and open-source implementations, we introduce the\nsymbiotic paradigm as the foundation for next-generation, AGI-driven\nnetworks-systems designed to remain adaptable, efficient, and trustworthy even\nas LLMs advance.", "AI": {"tldr": "This paper introduces a symbiotic agent paradigm combining LLMs with real-time optimization for 6G networks, achieving improved accuracy, reduced resource usage, and better service-level agreement in 5G testbed experiments.", "motivation": "LLM-based autonomous agents are expected to play a vital role in the evolution of 6G networks, by empowering real-time decision-making related to management and service provisioning to end-users. This facilitates the transition from a specialized intelligence approach to AGI-driven networks.", "method": "A novel agentic paradigm that combines LLMs with real-time optimization algorithms towards Trustworthy AI, defined as symbiotic agents. Two novel agent types including: (i) Radio Access Network optimizers, and (ii) multi-agent negotiators for Service-Level Agreements (SLAs) are designed and implemented. An end-to-end architecture for AGI networks is proposed and evaluated on a 5G testbed.", "result": "Symbiotic agents reduce decision errors fivefold compared to standalone LLM-based agents, while smaller language models (SLM) achieve similar accuracy with a 99.9% reduction in GPU resource overhead and in near-real-time loops of 82 ms. Reduces RAN over-utilization by approximately 44%.", "conclusion": "The symbiotic paradigm is introduced as the foundation for next-generation, AGI-driven networks-systems designed to remain adaptable, efficient, and trustworthy even as LLMs advance."}}
{"id": "2507.17050", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17050", "abs": "https://arxiv.org/abs/2507.17050", "authors": ["Tz-Ying Wu", "Tahani Trigui", "Sharath Nittur Sridhar", "Anand Bodas", "Subarna Tripathi"], "title": "Toward Scalable Video Narration: A Training-free Approach Using Multimodal Large Language Models", "comment": "Accepted to CVAM Workshop at ICCV 2025", "summary": "In this paper, we introduce VideoNarrator, a novel training-free pipeline\ndesigned to generate dense video captions that offer a structured snapshot of\nvideo content. These captions offer detailed narrations with precise\ntimestamps, capturing the nuances present in each segment of the video. Despite\nadvancements in multimodal large language models (MLLMs) for video\ncomprehension, these models often struggle with temporally aligned narrations\nand tend to hallucinate, particularly in unfamiliar scenarios. VideoNarrator\naddresses these challenges by leveraging a flexible pipeline where\noff-the-shelf MLLMs and visual-language models (VLMs) can function as caption\ngenerators, context providers, or caption verifiers. Our experimental results\ndemonstrate that the synergistic interaction of these components significantly\nenhances the quality and accuracy of video narrations, effectively reducing\nhallucinations and improving temporal alignment. This structured approach not\nonly enhances video understanding but also facilitates downstream tasks such as\nvideo summarization and video question answering, and can be potentially\nextended for advertising and marketing applications.", "AI": {"tldr": "VideoNarrator, a training-free pipeline, improves video narration by reducing hallucinations and enhancing temporal alignment using MLLMs and VLMs.", "motivation": "MLLMs often struggle with temporally aligned narrations and tend to hallucinate, particularly in unfamiliar scenarios.", "method": "A training-free pipeline called VideoNarrator is introduced, leveraging off-the-shelf MLLMs and VLMs as caption generators, context providers, or caption verifiers.", "result": "Experimental results demonstrate that the approach significantly enhances the quality and accuracy of video narrations, effectively reducing hallucinations and improving temporal alignment.", "conclusion": "The synergistic interaction of MLLMs and VLMs enhances video narration quality, reduces hallucinations, and improves temporal alignment, benefiting downstream tasks and potential applications in advertising and marketing."}}
{"id": "2507.17096", "categories": ["cs.LG", "cs.NA", "cs.SY", "eess.SY", "math.NA", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.17096", "abs": "https://arxiv.org/abs/2507.17096", "authors": ["Olivia Dry", "Timothy L. Molloy", "Wanxin Jin", "Iman Shames"], "title": "ZORMS-LfD: Learning from Demonstrations with Zeroth-Order Random Matrix Search", "comment": null, "summary": "We propose Zeroth-Order Random Matrix Search for Learning from Demonstrations\n(ZORMS-LfD). ZORMS-LfD enables the costs, constraints, and dynamics of\nconstrained optimal control problems, in both continuous and discrete time, to\nbe learned from expert demonstrations without requiring smoothness of the\nlearning-loss landscape. In contrast, existing state-of-the-art first-order\nmethods require the existence and computation of gradients of the costs,\nconstraints, dynamics, and learning loss with respect to states, controls\nand/or parameters. Most existing methods are also tailored to discrete time,\nwith constrained problems in continuous time receiving only cursory attention.\nWe demonstrate that ZORMS-LfD matches or surpasses the performance of\nstate-of-the-art methods in terms of both learning loss and compute time across\na variety of benchmark problems. On unconstrained continuous-time benchmark\nproblems, ZORMS-LfD achieves similar loss performance to state-of-the-art\nfirst-order methods with an over $80$\\% reduction in compute time. On\nconstrained continuous-time benchmark problems where there is no specialized\nstate-of-the-art method, ZORMS-LfD is shown to outperform the commonly used\ngradient-free Nelder-Mead optimization method.", "AI": {"tldr": "ZORMS-LfD \u662f\u4e00\u79cd\u7528\u4e8e\u4ece\u6f14\u793a\u4e2d\u5b66\u4e60\u7684\u96f6\u9636\u968f\u673a\u77e9\u9635\u641c\u7d22\u65b9\u6cd5\uff0c\u5b83\u5728\u5404\u79cd\u57fa\u51c6\u95ee\u9898\u4e0a\u4e0e\u73b0\u6709\u6280\u672f\u6c34\u5e73\u7684\u65b9\u6cd5\u76f8\u5339\u914d\u6216\u8d85\u8fc7\u4e86\u5b83\u4eec\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c11\u4e86\u3002", "motivation": "\u73b0\u6709\u7684\u6700\u4f18\u4e00\u9636\u65b9\u6cd5\u9700\u8981\u5b58\u5728\u5e76\u8ba1\u7b97\u6210\u672c\u3001\u7ea6\u675f\u3001\u52a8\u529b\u5b66\u548c\u5b66\u4e60\u635f\u5931\u76f8\u5bf9\u4e8e\u72b6\u6001\u3001\u63a7\u5236\u548c/\u6216\u53c2\u6570\u7684\u68af\u5ea6\u3002\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u4e5f\u9002\u7528\u4e8e\u79bb\u6563\u65f6\u95f4\uff0c\u800c\u8fde\u7eed\u65f6\u95f4\u4e2d\u7684\u7ea6\u675f\u95ee\u9898\u53ea\u53d7\u5230\u7c97\u7565\u7684\u5173\u6ce8\u3002", "method": "Zeroth-Order Random Matrix Search for Learning from Demonstrations (ZORMS-LfD)", "result": "ZORMS-LfD\u5728\u5b66\u4e60\u635f\u5931\u548c\u8ba1\u7b97\u65f6\u95f4\u65b9\u9762\u4e0e\u73b0\u6709\u6280\u672f\u6c34\u5e73\u7684\u65b9\u6cd5\u76f8\u5339\u914d\u6216\u8d85\u8fc7\u4e86\u5b83\u4eec\u7684\u6027\u80fd\u3002\u5728\u65e0\u7ea6\u675f\u7684\u8fde\u7eed\u65f6\u95f4\u57fa\u51c6\u95ee\u9898\u4e0a\uff0cZORMS-LfD\u5b9e\u73b0\u4e86\u4e0e\u73b0\u6709\u6280\u672f\u6c34\u5e73\u7684\u4e00\u9636\u65b9\u6cd5\u76f8\u4f3c\u7684\u635f\u5931\u6027\u80fd\uff0c\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c11\u4e86\u8d85\u8fc780%\u3002\u5728\u7ea6\u675f\u7684\u8fde\u7eed\u65f6\u95f4\u57fa\u51c6\u95ee\u9898\u4e0a\uff0c\u6ca1\u6709\u4e13\u95e8\u7684\u73b0\u6709\u6280\u672f\u6c34\u5e73\u7684\u65b9\u6cd5\uff0cZORMS-LfD\u88ab\u8bc1\u660e\u4f18\u4e8e\u5e38\u7528\u7684\u65e0\u68af\u5ea6Nelder-Mead\u4f18\u5316\u65b9\u6cd5\u3002", "conclusion": "ZORMS-LfD\u5728\u5404\u79cd\u57fa\u51c6\u95ee\u9898\u4e0a\uff0c\u5728\u5b66\u4e60\u635f\u5931\u548c\u8ba1\u7b97\u65f6\u95f4\u65b9\u9762\uff0c\u4e0e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u76f8\u5339\u914d\u6216\u8d85\u8fc7\u4e86\u5b83\u4eec\u7684\u6027\u80fd\u3002\u5728\u65e0\u7ea6\u675f\u7684\u8fde\u7eed\u65f6\u95f4\u57fa\u51c6\u95ee\u9898\u4e0a\uff0cZORMS-LfD\u5b9e\u73b0\u4e86\u4e0e\u73b0\u6709\u6700\u4f18\u4e00\u9636\u65b9\u6cd5\u76f8\u4f3c\u7684\u635f\u5931\u6027\u80fd\uff0c\u800c\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c11\u4e8680%\u4ee5\u4e0a\u3002\u5728\u6ca1\u6709\u4e13\u95e8\u7684\u6700\u4f18\u65b9\u6cd5\u7684\u7ea6\u675f\u8fde\u7eed\u65f6\u95f4\u57fa\u51c6\u95ee\u9898\u4e0a\uff0cZORMS-LfD\u88ab\u8bc1\u660e\u4f18\u4e8e\u5e38\u7528\u7684\u65e0\u68af\u5ea6Nelder-Mead\u4f18\u5316\u65b9\u6cd5\u3002"}}
{"id": "2507.17527", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.17527", "abs": "https://arxiv.org/abs/2507.17527", "authors": ["Shanbo Cheng", "Yu Bao", "Zhichao Huang", "Yu Lu", "Ningxin Peng", "Lu Xu", "Runsheng Yu", "Rong Cao", "Ting Han", "Zeyang Li", "Sitong Liu", "Shengtao Ma", "Shiguang Pan", "Jiongchen Xiao", "Nuo Xu", "Meng Yang", "Rong Ye", "Yiming Yu", "Ruofei Zhang", "Wanyi Zhang", "Wenhao Zhu", "Liehao Zou", "Lu Lu", "Yuxuan Wang", "Yonghui Wu"], "title": "Seed LiveInterpret 2.0: End-to-end Simultaneous Speech-to-speech Translation with Your Voice", "comment": "Seed-LiveInterpret 2.0 Technical Report", "summary": "Simultaneous Interpretation (SI) represents one of the most daunting\nfrontiers in the translation industry, with product-level automatic systems\nlong plagued by intractable challenges: subpar transcription and translation\nquality, lack of real-time speech generation, multi-speaker confusion, and\ntranslated speech inflation, especially in long-form discourses. In this study,\nwe introduce Seed-LiveInterpret 2.0, an end-to-end SI model that delivers\nhigh-fidelity, ultra-low-latency speech-to-speech generation with voice cloning\ncapabilities. As a fully operational product-level solution, Seed-LiveInterpret\n2.0 tackles these challenges head-on through our novel duplex speech-to-speech\nunderstanding-generating framework. Experimental results demonstrate that\nthrough large-scale pretraining and reinforcement learning, the model achieves\na significantly better balance between translation accuracy and latency,\nvalidated by human interpreters to exceed 70% correctness in complex scenarios.\nNotably, Seed-LiveInterpret 2.0 outperforms commercial SI solutions by\nsignificant margins in translation quality, while slashing the average latency\nof cloned speech from nearly 10 seconds to a near-real-time 3 seconds, which is\naround a near 70% reduction that drastically enhances practical usability.", "AI": {"tldr": "Seed-LiveInterpret 2.0 \u662f\u4e00\u79cd\u7aef\u5230\u7aef\u53e3\u8bd1\u6a21\u578b\uff0c\u53ef\u63d0\u4f9b\u9ad8\u4fdd\u771f\u3001\u8d85\u4f4e\u5ef6\u8fdf\u7684\u8bed\u97f3\u751f\u6210\u548c\u8bed\u97f3\u514b\u9686\u529f\u80fd\u3002", "motivation": "\u53e3\u8bd1\u662f\u7ffb\u8bd1\u884c\u4e1a\u6700\u8270\u5de8\u7684\u9886\u57df\u4e4b\u4e00\uff0c\u4ea7\u54c1\u7ea7\u7684\u81ea\u52a8\u7cfb\u7edf\u957f\u671f\u4ee5\u6765\u4e00\u76f4\u53d7\u5230\u96be\u4ee5\u89e3\u51b3\u7684\u6311\u6218\u7684\u56f0\u6270\uff1a\u8f6c\u5f55\u548c\u7ffb\u8bd1\u8d28\u91cf\u5dee\u3001\u7f3a\u4e4f\u5b9e\u65f6\u8bed\u97f3\u751f\u6210\u3001\u591a\u8bf4\u8bdd\u4eba\u6df7\u6dc6\u548c\u7ffb\u8bd1\u540e\u7684\u8bed\u97f3\u81a8\u80c0\uff0c\u5c24\u5176\u662f\u5728\u957f\u7bc7\u8bba\u8ff0\u4e2d\u3002", "method": " duplex speech-to-speech understanding-generating framework", "result": "Seed-LiveInterpret 2.0\u5728\u7ffb\u8bd1\u8d28\u91cf\u4e0a\u660e\u663e\u4f18\u4e8e\u5546\u4e1aSI\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u5c06\u514b\u9686\u8bed\u97f3\u7684\u5e73\u5747\u5ef6\u8fdf\u4ece\u8fd110\u79d2\u964d\u81f3\u63a5\u8fd1\u5b9e\u65f6\u76843\u79d2\uff0c\u964d\u5e45\u63a5\u8fd170%\uff0c\u5927\u5927\u63d0\u9ad8\u4e86\u5b9e\u7528\u6027\u3002", "conclusion": "Seed-LiveInterpret 2.0\u5728\u590d\u6742\u573a\u666f\u4e0b\uff0c\u7ecf\u4eba\u5de5\u7ffb\u8bd1\u9a8c\u8bc1\uff0c\u6b63\u786e\u7387\u8d85\u8fc770%\u3002"}}
{"id": "2507.17699", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17699", "abs": "https://arxiv.org/abs/2507.17699", "authors": ["Zhao Song", "Song Yue", "Jiahao Zhang"], "title": "Thinking Isn't an Illusion: Overcoming the Limitations of Reasoning Models via Tool Augmentations", "comment": null, "summary": "Large Reasoning Models (LRMs) have become a central focus in today's large\nlanguage model (LLM) research, where models are designed to output a\nstep-by-step thinking process before arriving at a final answer to handle\ncomplex reasoning tasks. Despite their promise, recent empirical studies (e.g.,\n[Shojaee et al., 2025] from Apple) suggest that this thinking process may not\nactually enhance reasoning ability, where LLMs without explicit reasoning\nactually outperform LRMs on tasks with low or high complexity. In this work, we\nrevisit these findings and investigate whether the limitations of LRMs persist\nwhen tool augmentations are introduced. We incorporate two types of tools,\nPython interpreters and scratchpads, and evaluate three representative LLMs and\ntheir LRM counterparts on Apple's benchmark reasoning puzzles. Our results show\nthat, with proper tool use, LRMs consistently outperform their non-reasoning\ncounterparts across all levels of task complexity. These findings challenge the\nrecent narrative that reasoning is an illusion and highlight the potential of\ntool-augmented LRMs for solving complex problems.", "AI": {"tldr": "Tool-augmented LRMs outperform non-reasoning LLMs, suggesting reasoning is not an illusion.", "motivation": "Large Reasoning Models (LRMs) have become a central focus in today's large language model (LLM) research, recent empirical studies suggest that this thinking process may not actually enhance reasoning ability, where LLMs without explicit reasoning actually outperform LRMs on tasks with low or high complexity. In this work, we revisit these findings and investigate whether the limitations of LRMs persist when tool augmentations are introduced.", "method": "incorporate two types of tools, Python interpreters and scratchpads, and evaluate three representative LLMs and their LRM counterparts on Apple's benchmark reasoning puzzles", "result": "with proper tool use, LRMs consistently outperform their non-reasoning counterparts across all levels of task complexity", "conclusion": "tool-augmented LRMs consistently outperform their non-reasoning counterparts across all levels of task complexity. These findings challenge the recent narrative that reasoning is an illusion and highlight the potential of tool-augmented LRMs for solving complex problems."}}
{"id": "2507.17079", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17079", "abs": "https://arxiv.org/abs/2507.17079", "authors": ["Md Meftahul Ferdaus", "Kendall N. Niles", "Joe Tom", "Mahdi Abdelguerfi", "Elias Ioup"], "title": "Few-Shot Learning in Video and 3D Object Detection: A Survey", "comment": "Under review in ACM Computing Surveys", "summary": "Few-shot learning (FSL) enables object detection models to recognize novel\nclasses given only a few annotated examples, thereby reducing expensive manual\ndata labeling. This survey examines recent FSL advances for video and 3D object\ndetection. For video, FSL is especially valuable since annotating objects\nacross frames is more laborious than for static images. By propagating\ninformation across frames, techniques like tube proposals and temporal matching\nnetworks can detect new classes from a couple examples, efficiently leveraging\nspatiotemporal structure. FSL for 3D detection from LiDAR or depth data faces\nchallenges like sparsity and lack of texture. Solutions integrate FSL with\nspecialized point cloud networks and losses tailored for class imbalance.\nFew-shot 3D detection enables practical autonomous driving deployment by\nminimizing costly 3D annotation needs. Core issues in both domains include\nbalancing generalization and overfitting, integrating prototype matching, and\nhandling data modality properties. In summary, FSL shows promise for reducing\nannotation requirements and enabling real-world video, 3D, and other\napplications by efficiently leveraging information across feature, temporal,\nand data modalities. By comprehensively surveying recent advancements, this\npaper illuminates FSL's potential to minimize supervision needs and enable\ndeployment across video, 3D, and other real-world applications.", "AI": {"tldr": "FSL reduces annotation needs for video and 3D object detection by leveraging spatiotemporal structure and specialized point cloud networks.", "motivation": "Few-shot learning (FSL) enables object detection models to recognize novel classes given only a few annotated examples, thereby reducing expensive manual data labeling.", "method": "This survey examines recent FSL advances for video and 3D object detection.", "result": "FSL for video is valuable since annotating objects across frames is more laborious than for static images. Few-shot 3D detection enables practical autonomous driving deployment by minimizing costly 3D annotation needs.", "conclusion": "FSL shows promise for reducing annotation requirements and enabling real-world video, 3D, and other applications by efficiently leveraging information across feature, temporal, and data modalities."}}
{"id": "2507.17107", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17107", "abs": "https://arxiv.org/abs/2507.17107", "authors": ["Andrii Balashov"], "title": "Reinforcement Learning Fine-Tunes a Sparse Subnetwork in Large Language Models", "comment": "16 pages, 6 figures", "summary": "Reinforcement learning (RL) is a key post-pretraining step for aligning large\nlanguage models (LLMs) with complex tasks and human preferences. While it is\noften assumed that RL fine-tuning requires updating most of a model's\nparameters, we challenge this assumption with a surprising finding: RL\nfine-tuning consistently modifies only a small subnetwork (typically 5-30% of\nweights), leaving most parameters unchanged. We call this phenomenon RL-induced\nparameter update sparsity. It arises naturally, without any sparsity\nconstraints or parameter-efficient tuning, and appears across multiple RL\nalgorithms (e.g., PPO, DPO, SimPO, PRIME) and model families (e.g., OpenAI,\nMeta, and open-source LLMs). Moreover, the subnetworks updated by RL show\nsubstantial overlap across different seeds, datasets, and algorithms-far\nexceeding chance-suggesting a partially transferable structure in the\npretrained model. We show that fine-tuning only this sparse subnetwork recovers\nfull model performance and yields parameters nearly identical to the fully\nfine-tuned model. Our analysis suggests this sparsity emerges because RL\noperates near the model's original distribution, requiring only targeted\nchanges. KL penalties, gradient clipping, and on-policy dynamics have limited\neffect on the sparsity pattern. These findings shed new light on how RL adapts\nmodels: not by shifting all weights, but by focusing training on a small,\nconsistently updated subnetwork. This insight enables more efficient RL methods\nand reframes sparsity through the lens of the lottery ticket hypothesis.", "AI": {"tldr": "RL fine-tuning only changes a small part of the model, which works just as well and is consistent across different setups.", "motivation": "To challenge the assumption that RL fine-tuning requires updating most of a model's parameters for aligning large language models with complex tasks and human preferences.", "method": "RL fine-tuning with PPO, DPO, SimPO, PRIME algorithms on OpenAI, Meta, and open-source LLMs, analyzing parameter updates and overlap across different seeds, datasets, and algorithms.", "result": "RL fine-tuning modifies only 5-30% of weights, subnetworks updated by RL show substantial overlap, fine-tuning only this sparse subnetwork recovers full model performance.", "conclusion": "RL fine-tuning consistently modifies only a small subnetwork, leaving most parameters unchanged, suggesting targeted changes near the model's original distribution."}}
{"id": "2507.17578", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.17578", "abs": "https://arxiv.org/abs/2507.17578", "authors": ["Brian DeRenzi", "Anna Dixon", "Mohamed Aymane Farhi", "Christian Resch"], "title": "Synthetic Voice Data for Automatic Speech Recognition in African Languages", "comment": "29 pages incl. appendix, 8 tables, 5 figures. Authors are listed in\n  alphabetical order", "summary": "Speech technology remains out of reach for most of the over 2300 languages in\nAfrica. We present the first systematic assessment of large-scale synthetic\nvoice corpora for African ASR. We apply a three-step process: LLM-driven text\ncreation, TTS voice synthesis, and ASR fine-tuning. Eight out of ten languages\nfor which we create synthetic text achieved readability scores above 5 out of\n7. We evaluated ASR improvement for three (Hausa, Dholuo, Chichewa) and created\nmore than 2,500 hours of synthetic voice data at below 1% of the cost of real\ndata. Fine-tuned Wav2Vec-BERT-2.0 models trained on 250h real and 250h\nsynthetic Hausa matched a 500h real-data-only baseline, while 579h real and\n450h to 993h synthetic data created the best performance. We also present\ngender-disaggregated ASR performance evaluation. For very low-resource\nlanguages, gains varied: Chichewa WER improved about 6.5% relative with a 1:2\nreal-to-synthetic ratio; a 1:1 ratio for Dholuo showed similar improvements on\nsome evaluation data, but not on others. Investigating intercoder reliability,\nASR errors and evaluation datasets revealed the need for more robust reviewer\nprotocols and more accurate evaluation data. All data and models are publicly\nreleased to invite further work to improve synthetic data for African\nlanguages.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u4f7f\u7528\u5927\u578b\u5408\u6210\u8bed\u97f3\u8bed\u6599\u5e93\u6765\u6539\u8fdb\u975e\u6d32\u8bed\u8a00\u7684 ASR\uff0c\u7ed3\u679c\u597d\u574f\u53c2\u534a\uff0c\u5e76\u5f3a\u8c03\u9700\u8981\u6539\u8fdb\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u975e\u6d32 2300 \u591a\u79cd\u8bed\u8a00\u4e2d\u7684\u5927\u591a\u6570\u4ecd\u7136\u65e0\u6cd5\u4f7f\u7528\u8bed\u97f3\u6280\u672f\u3002", "method": "\u8be5\u7814\u7a76\u91c7\u7528\u4e86\u4e00\u4e2a\u4e09\u6b65\u8fc7\u7a0b\uff1aLLM \u9a71\u52a8\u7684\u6587\u672c\u521b\u5efa\u3001TTS \u8bed\u97f3\u5408\u6210\u548c ASR \u5fae\u8c03\u3002", "result": "\u4e3a\u521b\u5efa\u5408\u6210\u6587\u672c\u7684\u5341\u79cd\u8bed\u8a00\u4e2d\u7684\u516b\u79cd\u5b9e\u73b0\u4e86\u9ad8\u4e8e 5/7 \u7684\u53ef\u8bfb\u6027\u5206\u6570\u3002\u5728 Hausa \u8bed\u4e2d\uff0c\u7ecf\u8fc7\u5fae\u8c03\u7684 Wav2Vec-BERT-2.0 \u6a21\u578b\u5728 250 \u5c0f\u65f6\u7684\u771f\u5b9e\u6570\u636e\u548c 250 \u5c0f\u65f6\u7684\u5408\u6210\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u4e0e\u4ec5\u4f7f\u7528 500 \u5c0f\u65f6\u7684\u771f\u5b9e\u6570\u636e\u57fa\u7ebf\u76f8\u5339\u914d\uff0c\u800c 579 \u5c0f\u65f6\u7684\u771f\u5b9e\u6570\u636e\u548c 450 \u5c0f\u65f6\u5230 993 \u5c0f\u65f6\u7684\u5408\u6210\u6570\u636e\u521b\u9020\u4e86\u6700\u4f73\u6027\u80fd\u3002Chichewa \u8bed\u7684 WER \u63d0\u9ad8\u4e86\u7ea6 6.5%\uff0c\u771f\u5b9e\u6570\u636e\u4e0e\u5408\u6210\u6570\u636e\u7684\u6bd4\u4f8b\u4e3a 1:2\uff1bDholuo \u8bed\u7684 1:1 \u6bd4\u4f8b\u5728\u67d0\u4e9b\u8bc4\u4f30\u6570\u636e\u4e0a\u663e\u793a\u51fa\u7c7b\u4f3c\u7684\u6539\u8fdb\uff0c\u4f46\u5728\u5176\u4ed6\u6570\u636e\u4e0a\u5219\u6ca1\u6709\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u5408\u6210\u8bed\u97f3\u6570\u636e\u53ef\u4ee5\u6709\u6548\u5730\u7528\u4e8e\u975e\u6d32\u8bed\u8a00\u7684 ASR \u6539\u8fdb\uff0c\u4f46\u7ed3\u679c\u56e0\u8bed\u8a00\u548c\u6570\u636e\u96c6\u800c\u5f02\u3002\u4ed6\u4eec\u5f3a\u8c03\u9700\u8981\u66f4\u5f3a\u5927\u7684\u8bc4\u4f30\u534f\u8bae\u548c\u66f4\u51c6\u786e\u7684\u8bc4\u4f30\u6570\u636e\uff0c\u5e76\u516c\u5f00\u53d1\u5e03\u4e86\u6240\u6709\u6570\u636e\u548c\u6a21\u578b\uff0c\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u3002"}}
{"id": "2507.17730", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17730", "abs": "https://arxiv.org/abs/2507.17730", "authors": ["Zhe Chen", "Daniel Harabor", "Ryan Hechnenberger", "Nathan R. Sturtevant"], "title": "Online Submission and Evaluation System Design for Competition Operations", "comment": "This work was presented at the Workshop on the International Planning\n  Competition (WIPC 2024)", "summary": "Research communities have developed benchmark datasets across domains to\ncompare the performance of algorithms and techniques However, tracking the\nprogress in these research areas is not easy, as publications appear in\ndifferent venues at the same time, and many of them claim to represent the\nstate-of-the-art. To address this, research communities often organise periodic\ncompetitions to evaluate the performance of various algorithms and techniques,\nthereby tracking advancements in the field. However, these competitions pose a\nsignificant operational burden. The organisers must manage and evaluate a large\nvolume of submissions. Furthermore, participants typically develop their\nsolutions in diverse environments, leading to compatibility issues during the\nevaluation of their submissions. This paper presents an online competition\nsystem that automates the submission and evaluation process for a competition.\nThe competition system allows organisers to manage large numbers of submissions\nefficiently, utilising isolated environments to evaluate submissions. This\nsystem has already been used successfully for several competitions, including\nthe Grid-Based Pathfinding Competition and the League of Robot Runners\ncompetition.", "AI": {"tldr": "This paper introduces an online competition system to automate submission and evaluation, reducing operational burden and compatibility issues. It has been successfully used in several competitions.", "motivation": "Tracking the progress in these research areas is not easy, as publications appear in different venues at the same time, and many of them claim to represent the state-of-the-art. Competitions pose a significant operational burden. The organisers must manage and evaluate a large volume of submissions. Participants typically develop their solutions in diverse environments, leading to compatibility issues during the evaluation of their submissions.", "method": "an online competition system that automates the submission and evaluation process", "result": "allows organisers to manage large numbers of submissions efficiently, utilising isolated environments to evaluate submissions. This system has already been used successfully for several competitions", "conclusion": "This paper presents an online competition system that automates the submission and evaluation process,utilising isolated environments to evaluate submissions. This system has already been used successfully for several competitions, including the Grid-Based Pathfinding Competition and the League of Robot Runners competition."}}
{"id": "2507.17083", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17083", "abs": "https://arxiv.org/abs/2507.17083", "authors": ["Zaipeng Duan", "Chenxu Dang", "Xuzhong Hu", "Pei An", "Junfeng Ding", "Jie Zhan", "Yunbiao Xu", "Jie Ma"], "title": "SDGOCC: Semantic and Depth-Guided Bird's-Eye View Transformation for 3D Multimodal Occupancy Prediction", "comment": "accepted by CVPR2025", "summary": "Multimodal 3D occupancy prediction has garnered significant attention for its\npotential in autonomous driving. However, most existing approaches are\nsingle-modality: camera-based methods lack depth information, while LiDAR-based\nmethods struggle with occlusions. Current lightweight methods primarily rely on\nthe Lift-Splat-Shoot (LSS) pipeline, which suffers from inaccurate depth\nestimation and fails to fully exploit the geometric and semantic information of\n3D LiDAR points. Therefore, we propose a novel multimodal occupancy prediction\nnetwork called SDG-OCC, which incorporates a joint semantic and depth-guided\nview transformation coupled with a fusion-to-occupancy-driven active\ndistillation. The enhanced view transformation constructs accurate depth\ndistributions by integrating pixel semantics and co-point depth through\ndiffusion and bilinear discretization. The fusion-to-occupancy-driven active\ndistillation extracts rich semantic information from multimodal data and\nselectively transfers knowledge to image features based on LiDAR-identified\nregions. Finally, for optimal performance, we introduce SDG-Fusion, which uses\nfusion alone, and SDG-KL, which integrates both fusion and distillation for\nfaster inference. Our method achieves state-of-the-art (SOTA) performance with\nreal-time processing on the Occ3D-nuScenes dataset and shows comparable\nperformance on the more challenging SurroundOcc-nuScenes dataset, demonstrating\nits effectiveness and robustness. The code will be released at\nhttps://github.com/DzpLab/SDGOCC.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001 occupancy prediction \u7f51\u7edc SDG-OCC\uff0c\u901a\u8fc7\u8054\u5408\u8bed\u4e49\u548c\u6df1\u5ea6\u5f15\u5bfc\u7684\u89c6\u56fe\u8f6c\u6362\u548c\u4e3b\u52a8\u84b8\u998f\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001 3D occupancy prediction \u65b9\u6cd5\u5927\u591a\u662f\u5355\u6a21\u6001\u7684\uff1a\u57fa\u4e8e\u76f8\u673a\u7684\u65b9\u6cd5\u7f3a\u4e4f\u6df1\u5ea6\u4fe1\u606f\uff0c\u800c\u57fa\u4e8e LiDAR \u7684\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u906e\u6321\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001 occupancy prediction \u7f51\u7edc\uff0c\u79f0\u4e3a SDG-OCC\uff0c\u5b83\u7ed3\u5408\u4e86\u8054\u5408\u8bed\u4e49\u548c\u6df1\u5ea6\u5f15\u5bfc\u7684\u89c6\u56fe\u8f6c\u6362\uff0c\u4ee5\u53ca fusion-to-occupancy-driven \u7684\u4e3b\u52a8\u84b8\u998f\u3002", "result": "\u901a\u8fc7\u96c6\u6210\u50cf\u7d20\u8bed\u4e49\u548c\u5171\u70b9\u6df1\u5ea6\uff0c\u6784\u5efa\u7cbe\u786e\u7684\u6df1\u5ea6\u5206\u5e03\uff1b\u4ece\u591a\u6a21\u6001\u6570\u636e\u4e2d\u63d0\u53d6\u4e30\u5bcc\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u5e76\u57fa\u4e8e LiDAR \u8bc6\u522b\u7684\u533a\u57df\uff0c\u9009\u62e9\u6027\u5730\u5c06\u77e5\u8bc6\u8f6c\u79fb\u5230\u56fe\u50cf\u7279\u5f81\u3002", "conclusion": "\u5b9e\u73b0\u4e86\u5728 Occ3D-nuScenes \u6570\u636e\u96c6\u4e0a\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u5728\u66f4\u5177\u6311\u6218\u6027\u7684 SurroundOcc-nuScenes \u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u53ef\u6bd4\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.17116", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17116", "abs": "https://arxiv.org/abs/2507.17116", "authors": ["Jacqueline Maasch", "Willie Neiswanger", "Stefano Ermon", "Volodymyr Kuleshov"], "title": "Probabilistic Graphical Models: A Concise Tutorial", "comment": "Under review", "summary": "Probabilistic graphical modeling is a branch of machine learning that uses\nprobability distributions to describe the world, make predictions, and support\ndecision-making under uncertainty. Underlying this modeling framework is an\nelegant body of theory that bridges two mathematical traditions: probability\nand graph theory. This framework provides compact yet expressive\nrepresentations of joint probability distributions, yielding powerful\ngenerative models for probabilistic reasoning.\n  This tutorial provides a concise introduction to the formalisms, methods, and\napplications of this modeling framework. After a review of basic probability\nand graph theory, we explore three dominant themes: (1) the representation of\nmultivariate distributions in the intuitive visual language of graphs, (2)\nalgorithms for learning model parameters and graphical structures from data,\nand (3) algorithms for inference, both exact and approximate.", "AI": {"tldr": "A concise introduction to probabilistic graphical modeling, covering representation, learning, and inference.", "motivation": "Probabilistic graphical modeling uses probability distributions to describe the world, make predictions, and support decision-making under uncertainty. It bridges probability and graph theory, providing compact representations of joint probability distributions for probabilistic reasoning.", "method": "review of basic probability and graph theory, exploration of three dominant themes: representation of multivariate distributions in graphs, algorithms for learning, and algorithms for inference.", "result": "yielding powerful generative models for probabilistic reasoning.", "conclusion": "This tutorial provides a concise introduction to the formalisms, methods, and applications of probabilistic graphical modeling."}}
{"id": "2507.17618", "categories": ["cs.CL", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.17618", "abs": "https://arxiv.org/abs/2507.17618", "authors": ["Bowen Zheng", "Ming Ma", "Zhongqiao Lin", "Tianming Yang"], "title": "A Hybrid Early-Exit Algorithm for Large Language Models Based on Space Alignment Decoding (SPADE)", "comment": null, "summary": "Large language models are computationally expensive due to their deep\nstructures. Prior research has shown that intermediate layers contain\nsufficient information to generate accurate answers, leading to the development\nof early-exit algorithms that reduce inference costs by terminating computation\nat earlier layers. However, these methods often suffer from poor performance\ndue to misalignment between intermediate and output layer representations that\nlead to decoding inaccuracy. To address these challenges, we propose SPADE\n(SPace Alignment DEcoding), a novel decoding method that aligns intermediate\nlayer representations with the output layer by propagating a minimally reduced\nsequence consisting of only the start token and the answer token. We further\noptimize the early-exit decision-making process by training a linear\napproximation of SPADE that computes entropy-based confidence metrics. Putting\nthem together, we create a hybrid early-exit algorithm that monitors confidence\nlevels and stops inference at intermediate layers while using SPADE to generate\nhigh-quality outputs. This approach significantly reduces inference costs\nwithout compromising accuracy, offering a scalable and efficient solution for\ndeploying large language models in real-world applications.", "AI": {"tldr": "This paper proposes SPADE, a novel decoding method, and a hybrid early-exit algorithm to reduce the inference costs of large language models without compromising accuracy.", "motivation": "Large language models are computationally expensive, and early-exit algorithms often suffer from poor performance due to misalignment between intermediate and output layer representations.", "method": "The paper proposes SPADE (SPace Alignment DEcoding), a novel decoding method that aligns intermediate layer representations with the output layer by propagating a minimally reduced sequence. It further optimizes the early-exit decision-making process by training a linear approximation of SPADE that computes entropy-based confidence metrics.", "result": "The proposed approach significantly reduces inference costs without compromising accuracy.", "conclusion": "This paper introduces a hybrid early-exit algorithm that reduces inference costs without compromising accuracy by monitoring confidence levels and stopping inference at intermediate layers while using SPADE to generate high-quality outputs."}}
{"id": "2507.10330", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10330", "abs": "https://arxiv.org/abs/2507.10330", "authors": ["Mohammed Bouri", "Adnane Saoud"], "title": "Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach", "comment": "Accepted to ACL Findings 2025", "summary": "Despite advancements in Natural Language Processing (NLP), models remain\nvulnerable to adversarial attacks, such as synonym substitutions. While prior\nwork has focused on improving robustness for feed-forward and convolutional\narchitectures, the robustness of recurrent networks and modern state space\nmodels (SSMs), such as S4, remains understudied. These architectures pose\nunique challenges due to their sequential processing and complex parameter\ndynamics. In this paper, we introduce a novel regularization technique based on\nGrowth Bound Matrices (GBM) to improve NLP model robustness by reducing the\nimpact of input perturbations on model outputs. We focus on computing the GBM\nfor three architectures: Long Short-Term Memory (LSTM), State Space models\n(S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance\nresilience against word substitution attacks, (2) improve generalization on\nclean text, and (3) providing the first systematic analysis of SSM (S4)\nrobustness. Extensive experiments across multiple architectures and benchmark\ndatasets demonstrate that our method improves adversarial robustness by up to\n8.8% over existing baselines. These results highlight the effectiveness of our\napproach, outperforming several state-of-the-art methods in adversarial\ndefense. Codes are available at https://github.com/BouriMohammed/GBM", "AI": {"tldr": "This paper introduces a GBM-based regularization technique to improve the adversarial robustness of NLP models, particularly recurrent networks and SSMs, achieving up to 8.8% improvement over existing methods.", "motivation": "NLP models are vulnerable to adversarial attacks, and the robustness of recurrent networks and modern state space models (SSMs) remains understudied.", "method": "A novel regularization technique based on Growth Bound Matrices (GBM) is introduced to improve NLP model robustness.", "result": "The method enhances resilience against word substitution attacks, improves generalization on clean text, and provides a systematic analysis of SSM (S4) robustness. Extensive experiments show improved adversarial robustness by up to 8.8% over baselines.", "conclusion": "The proposed GBM-based regularization technique improves adversarial robustness by up to 8.8% over existing baselines, demonstrating its effectiveness in adversarial defense."}}
{"id": "2507.17088", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17088", "abs": "https://arxiv.org/abs/2507.17088", "authors": ["Arkajyoti Mitra", "Afia Anjum", "Paul Agbaje", "Mert Pes\u00e9", "Habeeb Olufowobi"], "title": "FedVLM: Scalable Personalized Vision-Language Models through Federated Learning", "comment": null, "summary": "Vision-language models (VLMs) demonstrate impressive zero-shot and few-shot\nlearning capabilities, making them essential for several downstream tasks.\nHowever, fine-tuning these models at scale remains challenging, particularly in\nfederated environments where data is decentralized and non-iid across clients.\nExisting parameter-efficient tuning methods like LoRA (Low-Rank Adaptation)\nreduce computational overhead but struggle with heterogeneous client data,\nleading to suboptimal generalization. To address these challenges, we propose\nFedVLM, a federated LoRA fine-tuning framework that enables decentralized\nadaptation of VLMs while preserving model privacy and reducing reliance on\ncentralized training. To further tackle data heterogeneity, we introduce\npersonalized LoRA (pLoRA), which dynamically adapts LoRA parameters to each\nclient's unique data distribution, significantly improving local adaptation\nwhile maintaining global model aggregation. Experiments on the RLAIF-V dataset\nshow that pLoRA improves client-specific performance by 24.5% over standard\nLoRA, demonstrating superior adaptation in non-iid settings. FedVLM provides a\nscalable and efficient solution for fine-tuning VLMs in federated settings,\nadvancing personalized adaptation in distributed learning scenarios.", "AI": {"tldr": "This paper introduces FedVLM, a federated learning framework with personalized LoRA (pLoRA) to fine-tune vision-language models in decentralized, heterogeneous data environments. pLoRA improves performance by 24.5% compared to standard LoRA.", "motivation": "fine-tuning these models at scale remains challenging, particularly in federated environments where data is decentralized and non-iid across clients. Existing parameter-efficient tuning methods like LoRA (Low-Rank Adaptation) reduce computational overhead but struggle with heterogeneous client data, leading to suboptimal generalization", "method": "a federated LoRA fine-tuning framework that enables decentralized adaptation of VLMs while preserving model privacy and reducing reliance on centralized training. To further tackle data heterogeneity, we introduce personalized LoRA (pLoRA), which dynamically adapts LoRA parameters to each client's unique data distribution", "result": "pLoRA improves client-specific performance by 24.5% over standard LoRA, demonstrating superior adaptation in non-iid settings", "conclusion": "FedVLM provides a scalable and efficient solution for fine-tuning VLMs in federated settings, advancing personalized adaptation in distributed learning scenarios."}}
{"id": "2507.17123", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17123", "abs": "https://arxiv.org/abs/2507.17123", "authors": ["Jacob M. Delgado-L\u00f3pez", "Ricardo A. Morell-Rodriguez", "Sebasti\u00e1n O. Espinosa-Del Rosario", "Wilfredo E. Lugo-Beauchamp"], "title": "Computer Vision for Real-Time Monkeypox Diagnosis on Embedded Systems", "comment": null, "summary": "The rapid diagnosis of infectious diseases, such as monkeypox, is crucial for\neffective containment and treatment, particularly in resource-constrained\nenvironments. This study presents an AI-driven diagnostic tool developed for\ndeployment on the NVIDIA Jetson Orin Nano, leveraging the pre-trained\nMobileNetV2 architecture for binary classification. The model was trained on\nthe open-source Monkeypox Skin Lesion Dataset, achieving a 93.07% F1-Score,\nwhich reflects a well-balanced performance in precision and recall. To optimize\nthe model, the TensorRT framework was used to accelerate inference for FP32 and\nto perform post-training quantization for FP16 and INT8 formats. TensorRT's\nmixed-precision capabilities enabled these optimizations, which reduced the\nmodel size, increased inference speed, and lowered power consumption by\napproximately a factor of two, all while maintaining the original accuracy.\nPower consumption analysis confirmed that the optimized models used\nsignificantly less energy during inference, reinforcing their suitability for\ndeployment in resource-constrained environments. The system was deployed with a\nWi-Fi Access Point (AP) hotspot and a web-based interface, enabling users to\nupload and analyze images directly through connected devices such as mobile\nphones. This setup ensures simple access and seamless connectivity, making the\ntool practical for real-world applications. These advancements position the\ndiagnostic tool as an efficient, scalable, and energy-conscious solution to\naddress diagnosis challenges in underserved regions, paving the way for broader\nadoption in low-resource healthcare settings.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u8be5\u5de5\u5177\u5229\u7528 NVIDIA Jetson Orin Nano \u4e0a\u7684\u9884\u8bad\u7ec3 MobileNetV2 \u67b6\u6784\uff0c\u7528\u4e8e\u7334\u75d8\u7684\u4e8c\u5143\u5206\u7c7b\u3002", "motivation": "\u5728\u8d44\u6e90\u6709\u9650\u7684\u73af\u5883\u4e2d\uff0c\u5feb\u901f\u8bca\u65ad\u7334\u75d8\u7b49\u4f20\u67d3\u75c5\u5bf9\u4e8e\u6709\u6548\u7684\u63a7\u5236\u548c\u6cbb\u7597\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u7684 MobileNetV2 \u67b6\u6784\u8fdb\u884c\u4e8c\u5143\u5206\u7c7b\uff0c\u4f7f\u7528 TensorRT \u6846\u67b6\u52a0\u901f FP32 \u7684\u63a8\u7406\uff0c\u5e76\u6267\u884c FP16 \u548c INT8 \u683c\u5f0f\u7684\u8bad\u7ec3\u540e\u91cf\u5316\u3002", "result": "\u8be5\u6a21\u578b\u5728\u5f00\u6e90\u7334\u75d8\u76ae\u80a4\u75c5\u53d8\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86 93.07% \u7684 F1 \u5206\u6570\u3002TensorRT \u7684\u6df7\u5408\u7cbe\u5ea6\u80fd\u529b\u5b9e\u73b0\u4e86\u8fd9\u4e9b\u4f18\u5316\uff0c\u4ece\u800c\u51cf\u5c0f\u4e86\u6a21\u578b\u5c3a\u5bf8\uff0c\u63d0\u9ad8\u4e86\u63a8\u7406\u901f\u5ea6\uff0c\u5e76\u5c06\u529f\u8017\u964d\u4f4e\u4e86\u5927\u7ea6\u4e24\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u539f\u59cb\u7cbe\u5ea6\u3002\u529f\u8017\u5206\u6790\u8bc1\u5b9e\uff0c\u4f18\u5316\u540e\u7684\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u7684\u80fd\u91cf\u660e\u663e\u66f4\u5c11\u3002", "conclusion": "\u8be5\u8bca\u65ad\u5de5\u5177\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u8282\u80fd\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e8\u5728\u89e3\u51b3\u670d\u52a1\u6b20\u53d1\u8fbe\u5730\u533a\u7684\u8bca\u65ad\u6311\u6218\uff0c\u4e3a\u5728\u4f4e\u8d44\u6e90\u533b\u7597\u73af\u5883\u4e2d\u7684\u66f4\u5e7f\u6cdb\u5e94\u7528\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2507.17634", "categories": ["cs.CL", "cs.LG", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.17634", "abs": "https://arxiv.org/abs/2507.17634", "authors": ["Changxin Tian", "Jiapeng Wang", "Qian Zhao", "Kunlong Chen", "Jia Liu", "Ziqi Liu", "Jiaxin Mao", "Wayne Xin Zhao", "Zhiqiang Zhang", "Jun Zhou"], "title": "WSM: Decay-Free Learning Rate Schedule via Checkpoint Merging for LLM Pre-training", "comment": null, "summary": "Recent advances in learning rate (LR) scheduling have demonstrated the\neffectiveness of decay-free approaches that eliminate the traditional decay\nphase while maintaining competitive performance. Model merging techniques have\nemerged as particularly promising solutions in this domain. We present\nWarmup-Stable and Merge (WSM), a general framework that establishes a formal\nconnection between learning rate decay and model merging. WSM provides a\nunified theoretical foundation for emulating various decay strategies-including\ncosine decay, linear decay and inverse square root decay-as principled model\naveraging schemes, while remaining fully compatible with diverse optimization\nmethods. Through extensive experiments, we identify merge duration-the training\nwindow for checkpoint aggregation-as the most critical factor influencing model\nperformance, surpassing the importance of both checkpoint interval and merge\nquantity. Our framework consistently outperforms the widely-adopted\nWarmup-Stable-Decay (WSD) approach across multiple benchmarks, achieving\nsignificant improvements of +3.5% on MATH, +2.9% on HumanEval, and +5.5% on\nMMLU-Pro. The performance advantages extend to supervised fine-tuning\nscenarios, highlighting WSM's potential for long-term model refinement.", "AI": {"tldr": "\u63d0\u51fa\u4e86 WSM \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u6a21\u578b\u5e73\u5747\u6a21\u62df\u5404\u79cd\u8870\u51cf\u7b56\u7565\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e WSD \u65b9\u6cd5\u3002", "motivation": "\u5b66\u4e60\u7387 (LR) \u8c03\u5ea6\u7684\u6700\u65b0\u8fdb\u5c55\u8868\u660e\uff0c\u65e0\u8870\u51cf\u65b9\u6cd5\u5728\u6d88\u9664\u4f20\u7edf\u8870\u51cf\u9636\u6bb5\u7684\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u6027\u80fd\u7684\u6709\u6548\u6027\u3002\u6a21\u578b\u5408\u5e76\u6280\u672f\u5df2\u6210\u4e3a\u8be5\u9886\u57df\u7279\u522b\u6709\u524d\u9014\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86 Warmup-Stable and Merge (WSM)\uff0c\u8fd9\u662f\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u5b83\u5728\u5b66\u4e60\u7387\u8870\u51cf\u548c\u6a21\u578b\u5408\u5e76\u4e4b\u95f4\u5efa\u7acb\u4e86\u6b63\u5f0f\u7684\u8054\u7cfb\u3002", "result": "\u786e\u5b9a\u5408\u5e76\u6301\u7eed\u65f6\u95f4\uff08\u68c0\u67e5\u70b9\u805a\u5408\u7684\u8bad\u7ec3\u7a97\u53e3\uff09\u662f\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u7684\u6700\u5173\u952e\u56e0\u7d20\uff0c\u8d85\u8fc7\u4e86\u68c0\u67e5\u70b9\u95f4\u9694\u548c\u5408\u5e76\u6570\u91cf\u7684\u91cd\u8981\u6027\u3002", "conclusion": "WSM\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u5e7f\u6cdb\u91c7\u7528\u7684 WSD \u65b9\u6cd5\uff0c\u5728 MATH \u4e0a\u5b9e\u73b0\u4e86 +3.5% \u7684\u663e\u7740\u6539\u8fdb\uff0c\u5728 HumanEval \u4e0a\u5b9e\u73b0\u4e86 +2.9% \u7684\u6539\u8fdb\uff0c\u5728 MMLU-Pro \u4e0a\u5b9e\u73b0\u4e86 +5.5% \u7684\u6539\u8fdb\u3002\u6027\u80fd\u4f18\u52bf\u6269\u5c55\u5230\u76d1\u7763\u5fae\u8c03\u573a\u666f\uff0c\u7a81\u663e\u4e86 WSM \u5728\u957f\u671f\u6a21\u578b\u6539\u8fdb\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.17089", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17089", "abs": "https://arxiv.org/abs/2507.17089", "authors": ["Shanshan Zhang", "Siyue Wang", "Tianshui Wen", "Qi Zhang", "Ziheng Zhou", "Lingxiang Zheng", "Yu Yang"], "title": "IONext: Unlocking the Next Era of Inertial Odometry", "comment": null, "summary": "Researchers have increasingly adopted Transformer-based models for inertial\nodometry. While Transformers excel at modeling long-range dependencies, their\nlimited sensitivity to local, fine-grained motion variations and lack of\ninherent inductive biases often hinder localization accuracy and\ngeneralization. Recent studies have shown that incorporating large-kernel\nconvolutions and Transformer-inspired architectural designs into CNN can\neffectively expand the receptive field, thereby improving global motion\nperception. Motivated by these insights, we propose a novel CNN-based module\ncalled the Dual-wing Adaptive Dynamic Mixer (DADM), which adaptively captures\nboth global motion patterns and local, fine-grained motion features from\ndynamic inputs. This module dynamically generates selective weights based on\nthe input, enabling efficient multi-scale feature aggregation. To further\nimprove temporal modeling, we introduce the Spatio-Temporal Gating Unit (STGU),\nwhich selectively extracts representative and task-relevant motion features in\nthe temporal domain. This unit addresses the limitations of temporal modeling\nobserved in existing CNN approaches. Built upon DADM and STGU, we present a new\nCNN-based inertial odometry backbone, named Next Era of Inertial Odometry\n(IONext). Extensive experiments on six public datasets demonstrate that IONext\nconsistently outperforms state-of-the-art (SOTA) Transformer- and CNN-based\nmethods. For instance, on the RNIN dataset, IONext reduces the average ATE by\n10% and the average RTE by 12% compared to the representative model iMOT.", "AI": {"tldr": "IONext \u662f\u4e00\u79cd\u65b0\u578b CNN \u67b6\u6784\uff0c\u7528\u4e8e\u60ef\u6027\u91cc\u7a0b\u8ba1\uff0c\u5b83\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "Transformer\u5728\u5efa\u6a21\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u5bf9\u5c40\u90e8\u3001\u7ec6\u7c92\u5ea6\u8fd0\u52a8\u53d8\u5316\u7684\u654f\u611f\u6027\u6709\u9650\uff0c\u5e76\u4e14\u7f3a\u4e4f\u56fa\u6709\u7684\u5f52\u7eb3\u504f\u5dee\uff0c\u8fd9\u901a\u5e38\u4f1a\u963b\u788d\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u6cdb\u5316\u3002\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u5c06\u5927\u6838\u5377\u79ef\u548c\u53d7 Transformer \u542f\u53d1\u7684\u67b6\u6784\u8bbe\u8ba1\u878d\u5165 CNN \u53ef\u4ee5\u6709\u6548\u5730\u6269\u5c55\u611f\u53d7\u91ce\uff0c\u4ece\u800c\u63d0\u9ad8\u5168\u5c40\u8fd0\u52a8\u611f\u77e5\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\u53cc\u7ffc\u81ea\u9002\u5e94\u52a8\u6001\u6df7\u5408\u5668 (DADM) \u7684\u65b0\u578b\u57fa\u4e8e CNN \u7684\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u81ea\u9002\u5e94\u5730\u6355\u83b7\u6765\u81ea\u52a8\u6001\u8f93\u5165\u7684\u5168\u5c40\u8fd0\u52a8\u6a21\u5f0f\u548c\u5c40\u90e8\u3001\u7ec6\u7c92\u5ea6\u7684\u8fd0\u52a8\u7279\u5f81\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u6539\u8fdb\u65f6\u95f4\u5efa\u6a21\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u65f6\u7a7a\u95e8\u63a7\u5355\u5143 (STGU)\uff0c\u5b83\u5728\u65f6\u95f4\u57df\u4e2d\u9009\u62e9\u6027\u5730\u63d0\u53d6\u5177\u6709\u4ee3\u8868\u6027\u548c\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u8fd0\u52a8\u7279\u5f81\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u4e8e CNN \u7684\u60ef\u6027\u91cc\u7a0b\u8ba1\u9aa8\u5e72\u7f51\u7edc\uff0c\u540d\u4e3a\u4e0b\u4e00\u4ee3\u60ef\u6027\u91cc\u7a0b\u8ba1 (IONext)\u3002", "result": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u4e8e CNN \u7684\u60ef\u6027\u91cc\u7a0b\u8ba1\u9aa8\u5e72\u7f51\u7edc\uff0c\u540d\u4e3a\u4e0b\u4e00\u4ee3\u60ef\u6027\u91cc\u7a0b\u8ba1 (IONext)\u3002\u5728\u516d\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e IONext \u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684 (SOTA) Transformer \u548c\u57fa\u4e8e CNN \u7684\u65b9\u6cd5\u3002\u4f8b\u5982\uff0c\u5728 RNIN \u6570\u636e\u96c6\u4e0a\uff0c\u4e0e\u4ee3\u8868\u6027\u6a21\u578b iMOT \u76f8\u6bd4\uff0cIONext \u5c06\u5e73\u5747 ATE \u964d\u4f4e\u4e86 10%\uff0c\u5c06\u5e73\u5747 RTE \u964d\u4f4e\u4e86 12%\u3002", "conclusion": "IONext\u5728\u516d\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684 Transformer \u548c CNN \u65b9\u6cd5\u3002\u4f8b\u5982\uff0c\u5728 RNIN \u6570\u636e\u96c6\u4e0a\uff0c\u4e0e\u4ee3\u8868\u6027\u6a21\u578b iMOT \u76f8\u6bd4\uff0cIONext \u5c06\u5e73\u5747 ATE \u964d\u4f4e\u4e86 10%\uff0c\u5c06\u5e73\u5747 RTE \u964d\u4f4e\u4e86 12%\u3002"}}
{"id": "2507.17125", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17125", "abs": "https://arxiv.org/abs/2507.17125", "authors": ["Jacob M. Delgado-L\u00f3pez", "Andrea P. Seda-Hernandez", "Juan D. Guadalupe-Rosado", "Luis E. Fernandez Ramirez", "Miguel Giboyeaux-Camilo", "Wilfredo E. Lugo-Beauchamp"], "title": "Model Compression Engine for Wearable Devices Skin Cancer Diagnosis", "comment": null, "summary": "Skin cancer is one of the most prevalent and preventable types of cancer, yet\nits early detection remains a challenge, particularly in resource-limited\nsettings where access to specialized healthcare is scarce. This study proposes\nan AI-driven diagnostic tool optimized for embedded systems to address this\ngap. Using transfer learning with the MobileNetV2 architecture, the model was\nadapted for binary classification of skin lesions into \"Skin Cancer\" and\n\"Other.\" The TensorRT framework was employed to compress and optimize the model\nfor deployment on the NVIDIA Jetson Orin Nano, balancing performance with\nenergy efficiency. Comprehensive evaluations were conducted across multiple\nbenchmarks, including model size, inference speed, throughput, and power\nconsumption. The optimized models maintained their performance, achieving an\nF1-Score of 87.18% with a precision of 93.18% and recall of 81.91%.\nPost-compression results showed reductions in model size of up to 0.41, along\nwith improvements in inference speed and throughput, and a decrease in energy\nconsumption of up to 0.93 in INT8 precision. These findings validate the\nfeasibility of deploying high-performing, energy-efficient diagnostic tools on\nresource-constrained edge devices. Beyond skin cancer detection, the\nmethodologies applied in this research have broader applications in other\nmedical diagnostics and domains requiring accessible, efficient AI solutions.\nThis study underscores the potential of optimized AI systems to revolutionize\nhealthcare diagnostics, thereby bridging the divide between advanced technology\nand underserved regions.", "AI": {"tldr": "AI-driven diagnostic tool for skin cancer detection on edge devices shows high performance and energy efficiency, suitable for resource-limited settings.", "motivation": "Early skin cancer detection is challenging, especially in resource-limited settings.", "method": "Transfer learning with MobileNetV2, compressed with TensorRT for NVIDIA Jetson Orin Nano deployment.", "result": "Achieved 87.18% F1-Score, 93.18% precision, and 81.91% recall. Model size reduced by up to 0.41, with improved inference speed and energy consumption decreased by up to 0.93 in INT8 precision.", "conclusion": "Optimized AI systems can revolutionize healthcare diagnostics, bridging the gap between advanced technology and underserved regions."}}
{"id": "2507.17636", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.17636", "abs": "https://arxiv.org/abs/2507.17636", "authors": ["Victor Hartman", "Petter T\u00f6rnberg"], "title": "Who Attacks, and Why? Using LLMs to Identify Negative Campaigning in 18M Tweets across 19 Countries", "comment": null, "summary": "Negative campaigning is a central feature of political competition, yet\nempirical research has been limited by the high cost and limited scalability of\nexisting classification methods. This study makes two key contributions. First,\nit introduces zero-shot Large Language Models (LLMs) as a novel approach for\ncross-lingual classification of negative campaigning. Using benchmark datasets\nin ten languages, we demonstrate that LLMs achieve performance on par with\nnative-speaking human coders and outperform conventional supervised machine\nlearning approaches. Second, we leverage this novel method to conduct the\nlargest cross-national study of negative campaigning to date, analyzing 18\nmillion tweets posted by parliamentarians in 19 European countries between 2017\nand 2022. The results reveal consistent cross-national patterns: governing\nparties are less likely to use negative messaging, while ideologically extreme\nand populist parties -- particularly those on the radical right -- engage in\nsignificantly higher levels of negativity. These findings advance our\nunderstanding of how party-level characteristics shape strategic communication\nin multiparty systems. More broadly, the study demonstrates the potential of\nLLMs to enable scalable, transparent, and replicable research in political\ncommunication across linguistic and cultural contexts.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5206\u6790\u4e86\u6b27\u6d32\u8bae\u5458\u7684\u63a8\u6587\uff0c\u53d1\u73b0\u6267\u653f\u515a\u8f83\u5c11\u4f7f\u7528\u8d1f\u9762\u4fe1\u606f\uff0c\u800c\u6781\u7aef\u548c\u6c11\u7cb9\u4e3b\u4e49\u653f\u515a\u66f4\u591a\u5730\u53c2\u4e0e\u8d1f\u9762\u5ba3\u4f20\u3002", "motivation": "\u5b9e\u8bc1\u7814\u7a76\u53d7\u5230\u73b0\u6709\u5206\u7c7b\u65b9\u6cd5\u7684\u9ad8\u6210\u672c\u548c\u6709\u9650\u53ef\u6269\u5c55\u6027\u7684\u9650\u5236\u3002", "method": "\u5f15\u5165\u96f6\u6837\u672c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4f5c\u4e3a\u8de8\u8bed\u8a00\u8d1f\u9762\u7ade\u9009\u5206\u7c7b\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u5229\u7528\u6b64\u65b9\u6cd5\u5206\u6790\u4e8619\u4e2a\u6b27\u6d32\u56fd\u5bb6\u8bae\u5458\u57282017\u5e74\u81f32022\u5e74\u95f4\u53d1\u5e03\u76841800\u4e07\u6761\u63a8\u6587\u3002", "result": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5341\u79cd\u8bed\u8a00\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4e0e\u4ee5\u6bcd\u8bed\u4e3a\u6bcd\u8bed\u7684\u4eba\u7c7b\u7f16\u7801\u5458\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u4f18\u4e8e\u4f20\u7edf\u7684\u76d1\u7763\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u4e00\u81f4\u7684\u8de8\u56fd\u6a21\u5f0f\uff1a\u6267\u653f\u515a\u4e0d\u592a\u53ef\u80fd\u4f7f\u7528\u8d1f\u9762\u4fe1\u606f\uff0c\u800c\u610f\u8bc6\u5f62\u6001\u6781\u7aef\u548c\u6c11\u7cb9\u4e3b\u4e49\u653f\u515a\u2014\u2014\u5c24\u5176\u662f\u6781\u53f3\u7ffc\u653f\u515a\u2014\u2014\u5219\u4f1a\u8fdb\u884c\u660e\u663e\u66f4\u9ad8\u7a0b\u5ea6\u7684\u8d1f\u9762\u5ba3\u4f20\u3002\u8fd9\u4e9b\u53d1\u73b0\u52a0\u6df1\u4e86\u6211\u4eec\u5bf9\u653f\u515a\u5c42\u9762\u7279\u5f81\u5982\u4f55\u5728\u591a\u515a\u5236\u7cfb\u7edf\u4e2d\u5851\u9020\u6218\u7565\u6c9f\u901a\u7684\u7406\u89e3\u3002"}}
{"id": "2507.17121", "categories": ["cs.CV", "cs.LG", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.17121", "abs": "https://arxiv.org/abs/2507.17121", "authors": ["Faisal Ahmed", "Mohammad Alfrad Nobel Bhuiyan"], "title": "Robust Five-Class and binary Diabetic Retinopathy Classification Using Transfer Learning and Data Augmentation", "comment": "9 pages, 1 Figure", "summary": "Diabetic retinopathy (DR) is a leading cause of vision loss worldwide, and\nearly diagnosis through automated retinal image analysis can significantly\nreduce the risk of blindness. This paper presents a robust deep learning\nframework for both binary and five-class DR classification, leveraging transfer\nlearning and extensive data augmentation to address the challenges of class\nimbalance and limited training data. We evaluate a range of pretrained\nconvolutional neural network architectures, including variants of ResNet and\nEfficientNet, on the APTOS 2019 dataset.\n  For binary classification, our proposed model achieves a state-of-the-art\naccuracy of 98.9%, with a precision of 98.6%, recall of 99.3%, F1-score of\n98.9%, and an AUC of 99.4%. In the more challenging five-class severity\nclassification task, our model obtains a competitive accuracy of 84.6% and an\nAUC of 94.1%, outperforming several existing approaches. Our findings also\ndemonstrate that EfficientNet-B0 and ResNet34 offer optimal trade-offs between\naccuracy and computational efficiency across both tasks.\n  These results underscore the effectiveness of combining class-balanced\naugmentation with transfer learning for high-performance DR diagnosis. The\nproposed framework provides a scalable and accurate solution for DR screening,\nwith potential for deployment in real-world clinical environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8(DR)\u5206\u7c7b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u8fc1\u79fb\u5b66\u4e60\u548c\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u5728\u4e8c\u5143\u5206\u7c7b\u548c\u4e94\u7c7b\u5206\u7c7b\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u4e86\u826f\u597d\u7684\u6548\u679c\u3002", "motivation": "\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8(DR)\u662f\u5168\u7403\u8303\u56f4\u5185\u5bfc\u81f4\u89c6\u529b\u4e27\u5931\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u901a\u8fc7\u81ea\u52a8\u89c6\u7f51\u819c\u56fe\u50cf\u5206\u6790\u8fdb\u884c\u65e9\u671f\u8bca\u65ad\u53ef\u4ee5\u663e\u8457\u964d\u4f4e\u5931\u660e\u7684\u98ce\u9669\u3002", "method": "\u5229\u7528\u8fc1\u79fb\u5b66\u4e60\u548c\u5e7f\u6cdb\u7684\u6570\u636e\u589e\u5f3a\uff0c\u6765\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u6709\u9650\u7684\u8bad\u7ec3\u6570\u636e\u5e26\u6765\u7684\u6311\u6218\u3002\u8bc4\u4f30\u4e86\u4e00\u7cfb\u5217\u9884\u8bad\u7ec3\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5305\u62ecResNet\u548cEfficientNet\u7684\u53d8\u4f53\uff0c\u5728APTOS 2019\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u3002", "result": "\u5bf9\u4e8e\u4e8c\u5143\u5206\u7c7b\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u8fbe\u5230\u4e8698.9%\u7684\u6700\u5148\u8fdb\u7684\u51c6\u786e\u7387\uff0c\u7cbe\u786e\u7387\u4e3a98.6%\uff0c\u53ec\u56de\u7387\u4e3a99.3%\uff0cF1\u5206\u6570\u4e3a98.9%\uff0cAUC\u4e3a99.4%\u3002\u5728\u66f4\u5177\u6311\u6218\u6027\u7684\u4e94\u7c7b\u4e25\u91cd\u7a0b\u5ea6\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u8be5\u6a21\u578b\u83b7\u5f97\u4e8684.6%\u7684\u7ade\u4e89\u6027\u51c6\u786e\u7387\u548c94.1%\u7684AUC\uff0c\u4f18\u4e8e\u51e0\u79cd\u73b0\u6709\u65b9\u6cd5\u3002\u7814\u7a76\u7ed3\u679c\u8fd8\u8868\u660e\uff0cEfficientNet-B0\u548cResNet34\u5728\u4e24\u9879\u4efb\u52a1\u4e2d\u63d0\u4f9b\u4e86\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u7684\u6700\u4f73\u6743\u8861\u3002", "conclusion": "\u7ed3\u5408\u7c7b\u522b\u5e73\u8861\u589e\u5f3a\u548c\u8fc1\u79fb\u5b66\u4e60\u5bf9\u4e8e\u9ad8\u6027\u80fdDR\u8bca\u65ad\u975e\u5e38\u6709\u6548\u3002\u8be5\u6846\u67b6\u4e3aDR\u7b5b\u67e5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5728\u5b9e\u9645\u4e34\u5e8a\u73af\u5883\u4e2d\u90e8\u7f72\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.17131", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17131", "abs": "https://arxiv.org/abs/2507.17131", "authors": ["Yufei He", "Ruoyu Li", "Alex Chen", "Yue Liu", "Yulin Chen", "Yuan Sui", "Cheng Chen", "Yi Zhu", "Luca Luo", "Frank Yang", "Bryan Hooi"], "title": "Enabling Self-Improving Agents to Learn at Test Time With Human-In-The-Loop Guidance", "comment": null, "summary": "Large language model (LLM) agents often struggle in environments where rules\nand required domain knowledge frequently change, such as regulatory compliance\nand user risk screening. Current approaches, like offline fine-tuning and\nstandard prompting, are insufficient because they cannot effectively adapt to\nnew knowledge during actual operation. To address this limitation, we propose\nthe Adaptive Reflective Interactive Agent (ARIA), an LLM agent framework\ndesigned specifically to continuously learn updated domain knowledge at test\ntime. ARIA assesses its own uncertainty through structured self-dialogue,\nproactively identifying knowledge gaps and requesting targeted explanations or\ncorrections from human experts. It then systematically updates an internal,\ntimestamped knowledge repository with provided human guidance, detecting and\nresolving conflicting or outdated knowledge through comparisons and\nclarification queries. We evaluate ARIA on the realistic customer due diligence\nname screening task on TikTok Pay, alongside publicly available dynamic\nknowledge tasks. Results demonstrate significant improvements in adaptability\nand accuracy compared to baselines using standard offline fine-tuning and\nexisting self-improving agents. ARIA is deployed within TikTok Pay serving over\n150 million monthly active users, confirming its practicality and effectiveness\nfor operational use in rapidly evolving environments.", "AI": {"tldr": "ARIA is an LLM agent framework that continuously learns updated domain knowledge at test time, improving adaptability and accuracy in dynamic environments. It is deployed in TikTok Pay.", "motivation": "LLM agents struggle in environments with frequently changing rules and domain knowledge, and current approaches are insufficient for adapting to new knowledge during operation.", "method": "The Adaptive Reflective Interactive Agent (ARIA) framework uses structured self-dialogue to assess uncertainty, identify knowledge gaps, request explanations from experts, and update a timestamped knowledge repository.", "result": "ARIA demonstrates significant improvements in adaptability and accuracy compared to baselines and is deployed within TikTok Pay serving over 150 million monthly active users.", "conclusion": "The ARIA framework significantly improves adaptability and accuracy in dynamic knowledge tasks, as demonstrated by evaluations and a real-world deployment within TikTok Pay."}}
{"id": "2507.17702", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.17702", "abs": "https://arxiv.org/abs/2507.17702", "authors": ["Changxin Tian", "Kunlong Chen", "Jia Liu", "Ziqi Liu", "Zhiqiang Zhang", "Jun Zhou"], "title": "Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts Language Models", "comment": null, "summary": "Mixture-of-Experts (MoE) has become a dominant architecture for scaling Large\nLanguage Models (LLMs) efficiently by decoupling total parameters from\ncomputational cost. However, this decoupling creates a critical challenge:\npredicting the model capacity of a given MoE configurations (e.g., expert\nactivation ratio and granularity) remains an unresolved problem. To address\nthis gap, we introduce Efficiency Leverage (EL), a metric quantifying the\ncomputational advantage of an MoE model over a dense equivalent. We conduct a\nlarge-scale empirical study, training over 300 models up to 28B parameters, to\nsystematically investigate the relationship between MoE architectural\nconfigurations and EL. Our findings reveal that EL is primarily driven by the\nexpert activation ratio and the total compute budget, both following\npredictable power laws, while expert granularity acts as a non-linear modulator\nwith a clear optimal range. We integrate these discoveries into a unified\nscaling law that accurately predicts the EL of an MoE architecture based on its\nconfiguration. To validate our derived scaling laws, we designed and trained\nLing-mini-beta, a pilot model for Ling-2.0 series with only 0.85B active\nparameters, alongside a 6.1B dense model for comparison. When trained on an\nidentical 1T high-quality token dataset, Ling-mini-beta matched the performance\nof the 6.1B dense model while consuming over 7x fewer computational resources,\nthereby confirming the accuracy of our scaling laws. This work provides a\nprincipled and empirically-grounded foundation for the scaling of efficient MoE\nmodels.", "AI": {"tldr": "This paper introduces Efficiency Leverage (EL) to quantify the computational advantage of MoE models and derives scaling laws for efficient MoE model scaling, validated by the Ling-mini-beta model.", "motivation": "predicting the model capacity of a given MoE configurations remains an unresolved problem", "method": "introducing Efficiency Leverage (EL), a metric quantifying the computational advantage of an MoE model over a dense equivalent and conducting a large-scale empirical study, training over 300 models up to 28B parameters", "result": "EL is primarily driven by the expert activation ratio and the total compute budget, both following predictable power laws, while expert granularity acts as a non-linear modulator with a clear optimal range. Ling-mini-beta matched the performance of the 6.1B dense model while consuming over 7x fewer computational resources, thereby confirming the accuracy of our scaling laws.", "conclusion": "This work provides a principled and empirically-grounded foundation for the scaling of efficient MoE models."}}
{"id": "2507.17149", "categories": ["cs.CV", "cs.AI", "cs.LG", "I.4.6"], "pdf": "https://arxiv.org/pdf/2507.17149", "abs": "https://arxiv.org/abs/2507.17149", "authors": ["Bo Fang", "Jianan Fan", "Dongnan Liu", "Hang Chang", "Gerald J. Shami", "Filip Braet", "Weidong Cai"], "title": "ScSAM: Debiasing Morphology and Distributional Variability in Subcellular Semantic Segmentation", "comment": "Accepted by 28th European Conference on Artificial Intelligence\n  (ECAI)", "summary": "The significant morphological and distributional variability among\nsubcellular components poses a long-standing challenge for learning-based\norganelle segmentation models, significantly increasing the risk of biased\nfeature learning. Existing methods often rely on single mapping relationships,\noverlooking feature diversity and thereby inducing biased training. Although\nthe Segment Anything Model (SAM) provides rich feature representations, its\napplication to subcellular scenarios is hindered by two key challenges: (1) The\nvariability in subcellular morphology and distribution creates gaps in the\nlabel space, leading the model to learn spurious or biased features. (2) SAM\nfocuses on global contextual understanding and often ignores fine-grained\nspatial details, making it challenging to capture subtle structural alterations\nand cope with skewed data distributions. To address these challenges, we\nintroduce ScSAM, a method that enhances feature robustness by fusing\npre-trained SAM with Masked Autoencoder (MAE)-guided cellular prior knowledge\nto alleviate training bias from data imbalance. Specifically, we design a\nfeature alignment and fusion module to align pre-trained embeddings to the same\nfeature space and efficiently combine different representations. Moreover, we\npresent a cosine similarity matrix-based class prompt encoder to activate\nclass-specific features to recognize subcellular categories. Extensive\nexperiments on diverse subcellular image datasets demonstrate that ScSAM\noutperforms state-of-the-art methods.", "AI": {"tldr": "ScSAM\u901a\u8fc7\u878d\u5408\u9884\u8bad\u7ec3\u7684SAM\u548cMAE\u6765\u589e\u5f3a\u7279\u5f81\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u4e0d\u540c\u7684\u4e9a\u7ec6\u80de\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e9a\u7ec6\u80de\u6210\u5206\u5728\u5f62\u6001\u548c\u5206\u5e03\u4e0a\u7684\u663e\u8457\u53d8\u5f02\u6027\u5bf9\u57fa\u4e8e\u5b66\u4e60\u7684\u7ec6\u80de\u5668\u5206\u5272\u6a21\u578b\u63d0\u51fa\u4e86\u957f\u671f\u6311\u6218\uff0c\u663e\u8457\u589e\u52a0\u4e86\u6709\u504f\u89c1\u7684\u7279\u5f81\u5b66\u4e60\u7684\u98ce\u9669\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u5355\u4e00\u7684\u6620\u5c04\u5173\u7cfb\uff0c\u5ffd\u7565\u4e86\u7279\u5f81\u7684\u591a\u6837\u6027\uff0c\u4ece\u800c\u5bfc\u81f4\u6709\u504f\u89c1\u7684\u8bad\u7ec3\u3002SAM\u4e13\u6ce8\u4e8e\u5168\u5c40\u4e0a\u4e0b\u6587\u7406\u89e3\uff0c\u901a\u5e38\u5ffd\u7565\u7ec6\u7c92\u5ea6\u7684\u7a7a\u95f4\u7ec6\u8282\uff0c\u56e0\u6b64\u96be\u4ee5\u6355\u6349\u7ec6\u5fae\u7684\u7ed3\u6784\u53d8\u5316\u548c\u5e94\u5bf9\u503e\u659c\u7684\u6570\u636e\u5206\u5e03\u3002", "method": "\u878d\u5408\u9884\u8bad\u7ec3\u7684SAM\u4e0eMAE\u5f15\u5bfc\u7684\u7ec6\u80de\u5148\u9a8c\u77e5\u8bc6\uff0c\u7f13\u89e3\u6570\u636e\u4e0d\u5e73\u8861\u5e26\u6765\u7684\u8bad\u7ec3\u504f\u5dee\u3002\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7279\u5f81\u5bf9\u9f50\u548c\u878d\u5408\u6a21\u5757\uff0c\u5c06\u9884\u8bad\u7ec3\u7684\u5d4c\u5165\u5bf9\u9f50\u5230\u540c\u4e00\u7279\u5f81\u7a7a\u95f4\uff0c\u5e76\u6709\u6548\u5730\u7ed3\u5408\u4e0d\u540c\u7684\u8868\u793a\u3002\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4f59\u5f26\u76f8\u4f3c\u5ea6\u77e9\u9635\u7684\u7c7b\u63d0\u793a\u7f16\u7801\u5668\u6765\u6fc0\u6d3b\u7c7b\u7279\u5f02\u6027\u7279\u5f81\uff0c\u4ee5\u8bc6\u522b\u4e9a\u7ec6\u80de\u7c7b\u522b\u3002", "result": "ScSAM\u63d0\u9ad8\u4e86\u7279\u5f81\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "ScSAM\u5728\u4e0d\u540c\u7684\u4e9a\u7ec6\u80de\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.17135", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17135", "abs": "https://arxiv.org/abs/2507.17135", "authors": ["Ting Jiang", "Yixiao Wang", "Hancheng Ye", "Zishan Shao", "Jingwei Sun", "Jingyang Zhang", "Zekai Chen", "Jianyi Zhang", "Yiran Chen", "Hai Li"], "title": "SADA: Stability-guided Adaptive Diffusion Acceleration", "comment": "Accepted and published by ICML 2025. Code is available at:\n  https://github.com/Ting-Justin-Jiang/sada-icml", "summary": "Diffusion models have achieved remarkable success in generative tasks but\nsuffer from high computational costs due to their iterative sampling process\nand quadratic attention costs. Existing training-free acceleration strategies\nthat reduce per-step computation cost, while effectively reducing sampling\ntime, demonstrate low faithfulness compared to the original baseline. We\nhypothesize that this fidelity gap arises because (a) different prompts\ncorrespond to varying denoising trajectory, and (b) such methods do not\nconsider the underlying ODE formulation and its numerical solution. In this\npaper, we propose Stability-guided Adaptive Diffusion Acceleration (SADA), a\nnovel paradigm that unifies step-wise and token-wise sparsity decisions via a\nsingle stability criterion to accelerate sampling of ODE-based generative\nmodels (Diffusion and Flow-matching). For (a), SADA adaptively allocates\nsparsity based on the sampling trajectory. For (b), SADA introduces principled\napproximation schemes that leverage the precise gradient information from the\nnumerical ODE solver. Comprehensive evaluations on SD-2, SDXL, and Flux using\nboth EDM and DPM++ solvers reveal consistent $\\ge 1.8\\times$ speedups with\nminimal fidelity degradation (LPIPS $\\leq 0.10$ and FID $\\leq 4.5$) compared to\nunmodified baselines, significantly outperforming prior methods. Moreover, SADA\nadapts seamlessly to other pipelines and modalities: It accelerates ControlNet\nwithout any modifications and speeds up MusicLDM by $1.8\\times$ with $\\sim\n0.01$ spectrogram LPIPS.", "AI": {"tldr": "SADA accelerates sampling of ODE-based generative models with minimal fidelity degradation by unifying step-wise and token-wise sparsity decisions.", "motivation": "Existing training-free acceleration strategies demonstrate low faithfulness compared to the original baseline because different prompts correspond to varying denoising trajectory, and such methods do not consider the underlying ODE formulation and its numerical solution.", "method": "SADA unifies step-wise and token-wise sparsity decisions via a single stability criterion to accelerate sampling of ODE-based generative models, adaptively allocates sparsity based on the sampling trajectory, and introduces principled approximation schemes that leverage the precise gradient information from the numerical ODE solver.", "result": "SADA achieves consistent >= 1.8x speedups with minimal fidelity degradation (LPIPS <= 0.10 and FID <= 4.5) compared to unmodified baselines, significantly outperforming prior methods. It accelerates ControlNet without any modifications and speeds up MusicLDM by 1.8x with ~0.01 spectrogram LPIPS.", "conclusion": "SADA achieves significant speedups with minimal fidelity degradation and adapts seamlessly to other pipelines and modalities."}}
{"id": "2507.17709", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.17709", "abs": "https://arxiv.org/abs/2507.17709", "authors": ["Parker Riley", "Siamak Shakeri", "Waleed Ammar", "Jonathan H. Clark"], "title": "TyDi QA-WANA: A Benchmark for Information-Seeking Question Answering in Languages of West Asia and North Africa", "comment": null, "summary": "We present TyDi QA-WANA, a question-answering dataset consisting of 28K\nexamples divided among 10 language varieties of western Asia and northern\nAfrica. The data collection process was designed to elicit information-seeking\nquestions, where the asker is genuinely curious to know the answer. Each\nquestion in paired with an entire article that may or may not contain the\nanswer; the relatively large size of the articles results in a task suitable\nfor evaluating models' abilities to utilize large text contexts in answering\nquestions. Furthermore, the data was collected directly in each language\nvariety, without the use of translation, in order to avoid issues of cultural\nrelevance. We present performance of two baseline models, and release our code\nand data to facilitate further improvement by the research community.", "AI": {"tldr": "TyDi QA-WANA: a question-answering dataset consisting of 28K examples divided among 10 language varieties of western Asia and northern Africa.", "motivation": "The data collection process was designed to elicit information-seeking questions, where the asker is genuinely curious to know the answer.", "method": "The data was collected directly in each language variety, without the use of translation, in order to avoid issues of cultural relevance.", "result": "We present performance of two baseline models", "conclusion": "The authors release their code and data to facilitate further improvement by the research community."}}
{"id": "2507.17157", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17157", "abs": "https://arxiv.org/abs/2507.17157", "authors": ["Ruodai Cui", "Lei Zhang"], "title": "UNICE: Training A Universal Image Contrast Enhancer", "comment": null, "summary": "Existing image contrast enhancement methods are typically designed for\nspecific tasks such as under-/over-exposure correction, low-light and backlit\nimage enhancement, etc. The learned models, however, exhibit poor\ngeneralization performance across different tasks, even across different\ndatasets of a specific task. It is important to explore whether we can learn a\nuniversal and generalized model for various contrast enhancement tasks. In this\nwork, we observe that the common key factor of these tasks lies in the need of\nexposure and contrast adjustment, which can be well-addressed if high-dynamic\nrange (HDR) inputs are available. We hence collect 46,928 HDR raw images from\npublic sources, and render 328,496 sRGB images to build multi-exposure\nsequences (MES) and the corresponding pseudo sRGB ground-truths via\nmulti-exposure fusion. Consequently, we train a network to generate an MES from\na single sRGB image, followed by training another network to fuse the generated\nMES into an enhanced image. Our proposed method, namely UNiversal Image\nContrast Enhancer (UNICE), is free of costly human labeling. However, it\ndemonstrates significantly stronger generalization performance than existing\nimage contrast enhancement methods across and within different tasks, even\noutperforming manually created ground-truths in multiple no-reference image\nquality metrics. The dataset, code and model are available at\nhttps://github.com/BeyondHeaven/UNICE.", "AI": {"tldr": "UNICE \u662f\u4e00\u79cd\u901a\u7528\u56fe\u50cf\u5bf9\u6bd4\u5ea6\u589e\u5f3a\u5668\uff0c\u5b83\u901a\u8fc7\u4ece\u5355\u4e2a sRGB \u56fe\u50cf\u751f\u6210\u591a\u66dd\u5149\u5e8f\u5217 (MES) \u5e76\u5c06\u5176\u878d\u5408\u6765\u589e\u5f3a\u56fe\u50cf\u3002", "motivation": "\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u4e3a\u5404\u79cd\u5bf9\u6bd4\u5ea6\u589e\u5f3a\u4efb\u52a1\u5b66\u4e60\u4e00\u79cd\u901a\u7528\u6a21\u578b\u3002\u73b0\u6709\u56fe\u50cf\u5bf9\u6bd4\u5ea6\u589e\u5f3a\u65b9\u6cd5\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u6027\u80fd\u8f83\u5dee\u3002", "method": "\u8be5\u65b9\u6cd5\u8bad\u7ec3\u4e00\u4e2a\u7f51\u7edc\u4ece\u5355\u4e2a sRGB \u56fe\u50cf\u751f\u6210\u4e00\u4e2a MES\uff0c\u7136\u540e\u8bad\u7ec3\u53e6\u4e00\u4e2a\u7f51\u7edc\u5c06\u751f\u6210\u7684 MES \u878d\u5408\u4e3a\u589e\u5f3a\u56fe\u50cf\u3002", "result": "UNICE \u4e0d\u9700\u8981\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6bd4\u73b0\u6709\u56fe\u50cf\u5bf9\u6bd4\u5ea6\u589e\u5f3a\u65b9\u6cd5\u66f4\u5f3a\u7684\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "UNICE\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6bd4\u73b0\u6709\u56fe\u50cf\u5bf9\u6bd4\u5ea6\u589e\u5f3a\u65b9\u6cd5\u66f4\u5f3a\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u751a\u81f3\u5728\u591a\u4e2a\u65e0\u53c2\u8003\u56fe\u50cf\u8d28\u91cf\u6307\u6807\u4e2d\u4f18\u4e8e\u624b\u52a8\u521b\u5efa\u7684 ground-truth\u3002"}}
{"id": "2507.17151", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17151", "abs": "https://arxiv.org/abs/2507.17151", "authors": ["Anirudh Satheesh", "Anant Khandelwal", "Mucong Ding", "Radu Balan"], "title": "PICore: Physics-Informed Unsupervised Coreset Selection for Data Efficient Neural Operator Training", "comment": "Submitted to TMLR 2025", "summary": "Neural operators offer a powerful paradigm for solving partial differential\nequations (PDEs) that cannot be solved analytically by learning mappings\nbetween function spaces. However, there are two main bottlenecks in training\nneural operators: they require a significant amount of training data to learn\nthese mappings, and this data needs to be labeled, which can only be accessed\nvia expensive simulations with numerical solvers. To alleviate both of these\nissues simultaneously, we propose PICore, an unsupervised coreset selection\nframework that identifies the most informative training samples without\nrequiring access to ground-truth PDE solutions. PICore leverages a\nphysics-informed loss to select unlabeled inputs by their potential\ncontribution to operator learning. After selecting a compact subset of inputs,\nonly those samples are simulated using numerical solvers to generate labels,\nreducing annotation costs. We then train the neural operator on the reduced\nlabeled dataset, significantly decreasing training time as well. Across four\ndiverse PDE benchmarks and multiple coreset selection strategies, PICore\nachieves up to 78% average increase in training efficiency relative to\nsupervised coreset selection methods with minimal changes in accuracy. We\nprovide code at https://github.com/Asatheesh6561/PICore.", "AI": {"tldr": "PICore\u662f\u4e00\u79cd\u65e0\u76d1\u7763coreset\u9009\u62e9\u6846\u67b6\uff0c\u5b83\u53ef\u4ee5\u8bc6\u522b\u4fe1\u606f\u91cf\u6700\u5927\u7684\u8bad\u7ec3\u6837\u672c\uff0c\u800c\u4e0d\u9700\u8981\u8bbf\u95eeground-truth PDE\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u8bad\u7ec3\u795e\u7ecf\u7b97\u5b50\u6709\u4e24\u4e2a\u4e3b\u8981\u7684\u74f6\u9888\uff1a\u5b83\u4eec\u9700\u8981\u5927\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u6765\u5b66\u4e60\u8fd9\u4e9b\u6620\u5c04\uff0c\u5e76\u4e14\u8fd9\u4e9b\u6570\u636e\u9700\u8981\u88ab\u6807\u8bb0\uff0c\u8fd9\u53ea\u80fd\u901a\u8fc7\u4f7f\u7528\u6570\u503c\u6c42\u89e3\u5668\u7684\u6602\u8d35\u6a21\u62df\u6765\u8bbf\u95ee\u3002", "method": "\u63d0\u51fa\u4e86PICore\uff0c\u4e00\u4e2a\u65e0\u76d1\u7763coreset\u9009\u62e9\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u5b83\u4eec\u5bf9\u7b97\u5b50\u5b66\u4e60\u7684\u6f5c\u5728\u8d21\u732e\u6765\u9009\u62e9\u672a\u6807\u8bb0\u7684\u8f93\u5165\uff0c\u800c\u4e0d\u9700\u8981\u8bbf\u95eeground-truth PDE\u89e3\u51b3\u65b9\u6848\uff0c\u5229\u7528\u4e00\u4e2a\u7269\u7406\u4fe1\u606f\u635f\u5931\u6765\u9009\u62e9\u672a\u6807\u8bb0\u7684\u8f93\u5165\u3002", "result": "PICore\u51cf\u5c11\u4e86\u6ce8\u91ca\u6210\u672c\uff0c\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\u3002", "conclusion": "PICore\u5728\u4fdd\u8bc1\u7cbe\u5ea6\u53d8\u5316\u4e0d\u5927\u7684\u524d\u63d0\u4e0b\uff0c\u76f8\u5bf9\u4e8e\u6709\u76d1\u7763coreset\u9009\u62e9\u65b9\u6cd5\uff0c\u5728\u56db\u4e2a\u4e0d\u540c\u7684PDE\u57fa\u51c6\u548c\u591a\u4e2acoreset\u9009\u62e9\u7b56\u7565\u4e2d\uff0c\u8bad\u7ec3\u6548\u7387\u5e73\u5747\u63d0\u9ad8\u4e8678%\u3002"}}
{"id": "2507.17717", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17717", "abs": "https://arxiv.org/abs/2507.17717", "authors": ["Karen Zhou", "John Giorgi", "Pranav Mani", "Peng Xu", "Davis Liang", "Chenhao Tan"], "title": "From Feedback to Checklists: Grounded Evaluation of AI-Generated Clinical Notes", "comment": null, "summary": "AI-generated clinical notes are increasingly used in healthcare, but\nevaluating their quality remains a challenge due to high subjectivity and\nlimited scalability of expert review. Existing automated metrics often fail to\nalign with real-world physician preferences. To address this, we propose a\npipeline that systematically distills real user feedback into structured\nchecklists for note evaluation. These checklists are designed to be\ninterpretable, grounded in human feedback, and enforceable by LLM-based\nevaluators. Using deidentified data from over 21,000 clinical encounters,\nprepared in accordance with the HIPAA safe harbor standard, from a deployed AI\nmedical scribe system, we show that our feedback-derived checklist outperforms\nbaseline approaches in our offline evaluations in coverage, diversity, and\npredictive power for human ratings. Extensive experiments confirm the\nchecklist's robustness to quality-degrading perturbations, significant\nalignment with clinician preferences, and practical value as an evaluation\nmethodology. In offline research settings, the checklist can help identify\nnotes likely to fall below our chosen quality thresholds.", "AI": {"tldr": "This paper proposes a new method to evaluate the quality of AI-generated clinical notes by distilling real user feedback into structured checklists, which outperforms existing methods and aligns well with clinician preferences.", "motivation": "Existing automated metrics often fail to align with real-world physician preferences, and evaluating their quality remains a challenge due to high subjectivity and limited scalability of expert review.", "method": "a pipeline that systematically distills real user feedback into structured checklists for note evaluation. These checklists are designed to be interpretable, grounded in human feedback, and enforceable by LLM-based evaluators", "result": "our feedback-derived checklist outperforms baseline approaches in our offline evaluations in coverage, diversity, and predictive power for human ratings. Extensive experiments confirm the checklist's robustness to quality-degrading perturbations, significant alignment with clinician preferences, and practical value as an evaluation methodology.", "conclusion": "The checklist can help identify notes likely to fall below our chosen quality thresholds in offline research settings."}}
{"id": "2507.17158", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17158", "abs": "https://arxiv.org/abs/2507.17158", "authors": ["Bharath Krishnamurthy", "Ajita Rattani"], "title": "DOOMGAN:High-Fidelity Dynamic Identity Obfuscation Ocular Generative Morphing", "comment": "Accepted to IJCB 2025 (IEEE/IAPR International Joint Conference on\n  Biometrics). 11 pages with references, 8-page main paper with 4 figures and 4\n  tables. Includes 6 pages of supplementary material with 3 additional figures\n  and 3 tables. Code is available at the official lab repository:\n  https://github.com/vcbsl/DOOMGAN and the author's repository:\n  https://github.com/Bharath-K3/DOOMGAN", "summary": "Ocular biometrics in the visible spectrum have emerged as a prominent\nmodality due to their high accuracy, resistance to spoofing, and non-invasive\nnature. However, morphing attacks, synthetic biometric traits created by\nblending features from multiple individuals, threaten biometric system\nintegrity. While extensively studied for near-infrared iris and face\nbiometrics, morphing in visible-spectrum ocular data remains underexplored.\nSimulating such attacks demands advanced generation models that handle\nuncontrolled conditions while preserving detailed ocular features like iris\nboundaries and periocular textures. To address this gap, we introduce DOOMGAN,\nthat encompasses landmark-driven encoding of visible ocular anatomy,\nattention-guided generation for realistic morph synthesis, and dynamic\nweighting of multi-faceted losses for optimized convergence. DOOMGAN achieves\nover 20% higher attack success rates than baseline methods under stringent\nthresholds, along with 20% better elliptical iris structure generation and 30%\nimproved gaze consistency. We also release the first comprehensive ocular\nmorphing dataset to support further research in this domain.", "AI": {"tldr": "This paper introduces DOOMGAN, a novel method for generating morphing attacks on visible-spectrum ocular biometrics. DOOMGAN outperforms existing methods and a new dataset is released.", "motivation": "morphing attacks, synthetic biometric traits created by blending features from multiple individuals, threaten biometric system integrity. While extensively studied for near-infrared iris and face biometrics, morphing in visible-spectrum ocular data remains underexplored. Simulating such attacks demands advanced generation models that handle uncontrolled conditions while preserving detailed ocular features like iris boundaries and periocular textures", "method": "landmark-driven encoding of visible ocular anatomy, attention-guided generation for realistic morph synthesis, and dynamic weighting of multi-faceted losses for optimized convergence", "result": "DOOMGAN achieves over 20% higher attack success rates than baseline methods under stringent thresholds, along with 20% better elliptical iris structure generation and 30% improved gaze consistency", "conclusion": "DOOMGAN achieves over 20% higher attack success rates than baseline methods under stringent thresholds, along with 20% better elliptical iris structure generation and 30% improved gaze consistency. We also release the first comprehensive ocular morphing dataset to support further research in this domain."}}
{"id": "2507.17161", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17161", "abs": "https://arxiv.org/abs/2507.17161", "authors": ["Vinura Galwaduge", "Jagath Samarabandu"], "title": "Tabular Diffusion based Actionable Counterfactual Explanations for Network Intrusion Detection", "comment": null, "summary": "Modern network intrusion detection systems (NIDS) frequently utilize the\npredictive power of complex deep learning models. However, the \"black-box\"\nnature of such deep learning methods adds a layer of opaqueness that hinders\nthe proper understanding of detection decisions, trust in the decisions and\nprevent timely countermeasures against such attacks. Explainable AI (XAI)\nmethods provide a solution to this problem by providing insights into the\ncauses of the predictions. The majority of the existing XAI methods provide\nexplanations which are not convenient to convert into actionable\ncountermeasures. In this work, we propose a novel diffusion-based\ncounterfactual explanation framework that can provide actionable explanations\nfor network intrusion attacks. We evaluated our proposed algorithm against\nseveral other publicly available counterfactual explanation algorithms on 3\nmodern network intrusion datasets. To the best of our knowledge, this work also\npresents the first comparative analysis of existing counterfactual explanation\nalgorithms within the context of network intrusion detection systems. Our\nproposed method provide minimal, diverse counterfactual explanations out of the\ntested counterfactual explanation algorithms in a more efficient manner by\nreducing the time to generate explanations. We also demonstrate how\ncounterfactual explanations can provide actionable explanations by summarizing\nthem to create a set of global rules. These rules are actionable not only at\ninstance level but also at the global level for intrusion attacks. These global\ncounterfactual rules show the ability to effectively filter out incoming attack\nqueries which is crucial for efficient intrusion detection and defense\nmechanisms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7f51\u7edc\u5165\u4fb5\u68c0\u6d4b\u7684\u53ef\u64cd\u4f5c\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u751f\u6210\u5168\u5c40\u89c4\u5219\u4ee5\u6709\u6548\u8fc7\u6ee4\u653b\u51fb\u3002", "motivation": "\u73b0\u4ee3\u7f51\u7edc\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf (NIDS) \u9891\u7e41\u5730\u5229\u7528\u590d\u6742\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u9884\u6d4b\u80fd\u529b\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u201c\u9ed1\u76d2\u201d\u6027\u8d28\u589e\u52a0\u4e86\u4e00\u5c42\u4e0d\u900f\u660e\u6027\uff0c\u963b\u788d\u4e86\u5bf9\u68c0\u6d4b\u51b3\u7b56\u7684\u6b63\u786e\u7406\u89e3\u548c\u4fe1\u4efb\uff0c\u5e76\u59a8\u788d\u4e86\u53ca\u65f6\u91c7\u53d6\u5e94\u5bf9\u6b64\u7c7b\u653b\u51fb\u7684\u63aa\u65bd\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u6269\u6563\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u6846\u67b6\u3002", "result": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u4ee5\u66f4\u6709\u6548\u7684\u65b9\u5f0f\u51cf\u5c11\u4e86\u751f\u6210\u89e3\u91ca\u7684\u65f6\u95f4\uff0c\u4ece\u800c\u5728\u6d4b\u8bd5\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u7b97\u6cd5\u4e2d\u63d0\u4f9b\u4e86\u6700\u5c0f\u7684\u3001\u591a\u6837\u5316\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u3002\u8fd9\u4e9b\u5168\u5c40\u53cd\u4e8b\u5b9e\u89c4\u5219\u663e\u793a\u51fa\u6709\u6548\u8fc7\u6ee4\u4f20\u5165\u653b\u51fb\u67e5\u8be2\u7684\u80fd\u529b\uff0c\u8fd9\u5bf9\u4e8e\u6709\u6548\u7684\u5165\u4fb5\u68c0\u6d4b\u548c\u9632\u5fa1\u673a\u5236\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u6269\u6563\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u4e3a\u7f51\u7edc\u5165\u4fb5\u653b\u51fb\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u89e3\u91ca\uff0c\u5e76\u901a\u8fc7\u603b\u7ed3\u8fd9\u4e9b\u89e3\u91ca\u6765\u521b\u5efa\u4e00\u7ec4\u5168\u5c40\u89c4\u5219\uff0c\u4ece\u800c\u6709\u6548\u5730\u8fc7\u6ee4\u6389\u4f20\u5165\u7684\u653b\u51fb\u67e5\u8be2\u3002"}}
{"id": "2507.17718", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.17718", "abs": "https://arxiv.org/abs/2507.17718", "authors": ["Danny D. Leybzon", "Shreyas Tirumala", "Nishant Jain", "Summer Gillen", "Michael Jackson", "Cameron McPhee", "Jennifer Schmidt"], "title": "AI Telephone Surveying: Automating Quantitative Data Collection with an AI Interviewer", "comment": null, "summary": "With the rise of voice-enabled artificial intelligence (AI) systems,\nquantitative survey researchers have access to a new data-collection mode: AI\ntelephone surveying. By using AI to conduct phone interviews, researchers can\nscale quantitative studies while balancing the dual goals of human-like\ninteractivity and methodological rigor. Unlike earlier efforts that used\ninteractive voice response (IVR) technology to automate these surveys, voice AI\nenables a more natural and adaptive respondent experience as it is more robust\nto interruptions, corrections, and other idiosyncrasies of human speech.\n  We built and tested an AI system to conduct quantitative surveys based on\nlarge language models (LLM), automatic speech recognition (ASR), and speech\nsynthesis technologies. The system was specifically designed for quantitative\nresearch, and strictly adhered to research best practices like question order\nrandomization, answer order randomization, and exact wording.\n  To validate the system's effectiveness, we deployed it to conduct two pilot\nsurveys with the SSRS Opinion Panel and followed-up with a separate\nhuman-administered survey to assess respondent experiences. We measured three\nkey metrics: the survey completion rates, break-off rates, and respondent\nsatisfaction scores. Our results suggest that shorter instruments and more\nresponsive AI interviewers may contribute to improvements across all three\nmetrics studied.", "AI": {"tldr": "This paper introduces and tests an AI system for conducting quantitative surveys, finding that shorter surveys and more responsive AI interviewers improve completion rates, reduce break-off rates, and increase respondent satisfaction.", "motivation": "With the rise of voice-enabled artificial intelligence (AI) systems, quantitative survey researchers have access to a new data-collection mode: AI telephone surveying. By using AI to conduct phone interviews, researchers can scale quantitative studies while balancing the dual goals of human-like interactivity and methodological rigor. Unlike earlier efforts that used interactive voice response (IVR) technology to automate these surveys, voice AI enables a more natural and adaptive respondent experience as it is more robust to interruptions, corrections, and other idiosyncrasies of human speech.", "method": "We built and tested an AI system to conduct quantitative surveys based on large language models (LLM), automatic speech recognition (ASR), and speech synthesis technologies. The system was specifically designed for quantitative research, and strictly adhered to research best practices like question order randomization, answer order randomization, and exact wording.  To validate the system's effectiveness, we deployed it to conduct two pilot surveys with the SSRS Opinion Panel and followed-up with a separate human-administered survey to assess respondent experiences. We measured three key metrics: the survey completion rates, break-off rates, and respondent satisfaction scores.", "result": "Our results suggest that shorter instruments and more responsive AI interviewers may contribute to improvements across all three metrics studied.", "conclusion": "Shorter instruments and more responsive AI interviewers may contribute to improvements across all three metrics studied."}}
{"id": "2507.17176", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17176", "abs": "https://arxiv.org/abs/2507.17176", "authors": ["Li Pingzhen", "Xu Sheng", "Chen Jing", "Su Chengyue"], "title": "Multi-Scale PCB Defect Detection with YOLOv8 Network Improved via Pruning and Lightweight Network", "comment": null, "summary": "With the high density of printed circuit board (PCB) design and the high\nspeed of production, the traditional PCB defect detection model is difficult to\ntake into account the accuracy and computational cost, and cannot meet the\nrequirements of high accuracy and real-time detection of tiny defects.\nTherefore, in this paper, a multi-scale PCB defect detection method is improved\nwith YOLOv8 using a comprehensive strategy of tiny target sensitivity strategy,\nnetwork lightweighting and adaptive pruning, which is able to improve the\ndetection speed and accuracy by optimizing the backbone network, the neck\nnetwork and the detection head, the loss function and the adaptive pruning\nrate. Firstly, a Ghost-HGNetv2 structure with fewer parameters is used in the\nbackbone network, and multilevel features are used to extract image semantic\nfeatures to discover accurate defects. Secondly, we integrate C2f-Faster with\nsmall number of parameters in the neck section to enhance the ability of\nmulti-level feature fusion. Next, in the Head part, we design a new GCDetect\ndetection head, which allows the prediction of bounding boxes and categories to\nshare the weights of GroupConv, and uses a small number of grouping\nconvolutions to accomplish the regression and classification tasks, which\nsignificantly reduces the number of parameters while maintaining the accuracy\nof detection. We also design the Inner-MPDIoU boundary loss function to improve\nthe detection and localization of tiny targets. Finally, the model was pruned\nby an optimized adaptive pruning rate to further reduce the complexity of the\nmodel. Experimental results show that the model exhibits advantages in terms of\naccuracy and speed. On the publicly available PCB defect dataset, mAP0.5\nreaches 99.32% and mAP0.5:0.9 reaches 75.18%, which is 10.13% higher compared\nto YOLOv8n.", "AI": {"tldr": "This paper improves YOLOv8 for PCB defect detection using lightweighting and pruning techniques, achieving higher accuracy and speed compared to the original YOLOv8n.", "motivation": "Traditional PCB defect detection models struggle to balance accuracy and computational cost, failing to meet the demands for high accuracy and real-time detection of tiny defects due to high PCB design density and production speed.", "method": "The YOLOv8 model is improved using a comprehensive strategy of tiny target sensitivity, network lightweighting, and adaptive pruning. This involves optimizing the backbone network with a Ghost-HGNetv2 structure, integrating C2f-Faster in the neck section, designing a new GCDetect detection head, using an Inner-MPDIoU boundary loss function, and pruning the model with an optimized adaptive pruning rate.", "result": "The improved model achieves a mAP0.5 of 99.32% and a mAP0.5:0.9 of 75.18% on a publicly available PCB defect dataset, which is 10.13% higher compared to YOLOv8n.", "conclusion": "The improved YOLOv8 model demonstrates advantages in both accuracy and speed on a PCB defect dataset, achieving a mAP0.5 of 99.32% and mAP0.5:0.9 of 75.18%, a 10.13% improvement compared to YOLOv8n."}}
{"id": "2507.17189", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17189", "abs": "https://arxiv.org/abs/2507.17189", "authors": ["Shaohan Li", "Hao Yang", "Min Chen", "Xiaolin Qin"], "title": "Met$^2$Net: A Decoupled Two-Stage Spatio-Temporal Forecasting Model for Complex Meteorological Systems", "comment": null, "summary": "The increasing frequency of extreme weather events due to global climate\nchange urges accurate weather prediction. Recently, great advances have been\nmade by the \\textbf{end-to-end methods}, thanks to deep learning techniques,\nbut they face limitations of \\textit{representation inconsistency} in\nmultivariable integration and struggle to effectively capture the dependency\nbetween variables, which is required in complex weather systems. Treating\ndifferent variables as distinct modalities and applying a \\textbf{two-stage\ntraining approach} from multimodal models can partially alleviate this issue,\nbut due to the inconformity in training tasks between the two stages, the\nresults are often suboptimal. To address these challenges, we propose an\nimplicit two-stage training method, configuring separate encoders and decoders\nfor each variable. In detailed, in the first stage, the Translator is frozen\nwhile the Encoders and Decoders learn a shared latent space, in the second\nstage, the Encoders and Decoders are frozen, and the Translator captures\ninter-variable interactions for prediction. Besides, by introducing a\nself-attention mechanism for multivariable fusion in the latent space, the\nperformance achieves further improvements. Empirically, extensive experiments\nshow the state-of-the-art performance of our method. Specifically, it reduces\nthe MSE for near-surface air temperature and relative humidity predictions by\n28.82\\% and 23.39\\%, respectively. The source code is available at\nhttps://github.com/ShremG/Met2Net.", "AI": {"tldr": "Proposes an implicit two-stage training method with self-attention to improve weather prediction accuracy by addressing representation inconsistency and variable dependency issues.", "motivation": "End-to-end methods for weather prediction face limitations of representation inconsistency in multivariable integration and struggle to effectively capture the dependency between variables.", "method": "An implicit two-stage training method with separate encoders and decoders for each variable, along with a self-attention mechanism for multivariable fusion in the latent space.", "result": "The proposed method achieves state-of-the-art performance, reducing MSE for near-surface air temperature and relative humidity predictions by 28.82% and 23.39%, respectively.", "conclusion": "The proposed method achieves state-of-the-art performance, reducing MSE for near-surface air temperature and relative humidity predictions by 28.82% and 23.39%, respectively."}}
{"id": "2507.17728", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.17728", "abs": "https://arxiv.org/abs/2507.17728", "authors": ["Boxun Li", "Yadong Li", "Zhiyuan Li", "Congyi Liu", "Weilin Liu", "Guowei Niu", "Zheyue Tan", "Haiyang Xu", "Zhuyu Yao", "Tao Yuan", "Dong Zhou", "Yueqing Zhuang", "Bo Zhao", "Guohao Dai", "Yu Wang"], "title": "Megrez2 Technical Report", "comment": null, "summary": "We present Megrez2, a novel lightweight and high-performance language model\narchitecture optimized for device native deployment. Megrez2 introduces a novel\ncross-layer expert sharing mechanism, which significantly reduces total\nparameter count by reusing expert modules across adjacent transformer layers\nwhile maintaining most of the model's capacity. It also incorporates pre-gated\nrouting, enabling memory-efficient expert loading and faster inference. As the\nfirst instantiation of the Megrez2 architecture, we introduce the\nMegrez2-Preview model, which is pre-trained on a 5-trillion-token corpus and\nfurther enhanced through supervised fine-tuning and reinforcement learning with\nverifiable rewards. With only 3B activated and 7.5B stored parameters,\nMegrez2-Preview demonstrates competitive or superior performance compared to\nlarger models on a wide range of tasks, including language understanding,\ninstruction following, mathematical reasoning, and code generation. These\nresults highlight the effectiveness of the Megrez2 architecture to achieve a\nbalance between accuracy, efficiency, and deployability, making it a strong\ncandidate for real-world, resource-constrained applications.", "AI": {"tldr": "Megrez2 is a lightweight language model architecture with cross-layer expert sharing and pre-gated routing. Megrez2-Preview achieves competitive performance with fewer parameters, making it suitable for resource-constrained applications.", "motivation": "To create a lightweight and high-performance language model architecture optimized for device native deployment.", "method": "Introduces a novel cross-layer expert sharing mechanism and incorporates pre-gated routing.", "result": "Megrez2-Preview, with 3B activated and 7.5B stored parameters, shows strong performance in language understanding, instruction following, mathematical reasoning, and code generation.", "conclusion": "Megrez2-Preview demonstrates competitive or superior performance compared to larger models. The Megrez2 architecture achieves a balance between accuracy, efficiency, and deployability."}}
{"id": "2507.17182", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17182", "abs": "https://arxiv.org/abs/2507.17182", "authors": ["Linghe Meng", "Jiarun Song"], "title": "Hierarchical Fusion and Joint Aggregation: A Multi-Level Feature Representation Method for AIGC Image Quality Assessment", "comment": null, "summary": "The quality assessment of AI-generated content (AIGC) faces multi-dimensional\nchallenges, that span from low-level visual perception to high-level semantic\nunderstanding. Existing methods generally rely on single-level visual features,\nlimiting their ability to capture complex distortions in AIGC images. To\naddress this limitation, a multi-level visual representation paradigm is\nproposed with three stages, namely multi-level feature extraction, hierarchical\nfusion, and joint aggregation. Based on this paradigm, two networks are\ndeveloped. Specifically, the Multi-Level Global-Local Fusion Network (MGLF-Net)\nis designed for the perceptual quality assessment, extracting complementary\nlocal and global features via dual CNN and Transformer visual backbones. The\nMulti-Level Prompt-Embedded Fusion Network (MPEF-Net) targets Text-to-Image\ncorrespondence by embedding prompt semantics into the visual feature fusion\nprocess at each feature level. The fused multi-level features are then\naggregated for final evaluation. Experiments on benchmarks demonstrate\noutstanding performance on both tasks, validating the effectiveness of the\nproposed multi-level visual assessment paradigm.", "AI": {"tldr": "This paper proposes a multi-level visual representation paradigm to address the limitations of single-level visual features in assessing the quality of AI-generated content (AIGC).", "motivation": "Existing methods generally rely on single-level visual features, limiting their ability to capture complex distortions in AIGC images. To address this limitation", "method": "a multi-level visual representation paradigm is proposed with three stages, namely multi-level feature extraction, hierarchical fusion, and joint aggregation. Based on this paradigm, two networks are developed. Specifically, the Multi-Level Global-Local Fusion Network (MGLF-Net) is designed for the perceptual quality assessment, extracting complementary local and global features via dual CNN and Transformer visual backbones. The Multi-Level Prompt-Embedded Fusion Network (MPEF-Net) targets Text-to-Image correspondence by embedding prompt semantics into the visual feature fusion process at each feature level.", "result": "outstanding performance on both tasks", "conclusion": "Experiments on benchmarks demonstrate outstanding performance on both tasks, validating the effectiveness of the proposed multi-level visual assessment paradigm."}}
{"id": "2507.17204", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17204", "abs": "https://arxiv.org/abs/2507.17204", "authors": ["Zixuan Wang", "Jinghao Shi", "Hanzhong Liang", "Xiang Shen", "Vera Wen", "Zhiqian Chen", "Yifan Wu", "Zhixin Zhang", "Hongyu Xiong"], "title": "Filter-And-Refine: A MLLM Based Cascade System for Industrial-Scale Video Content Moderation", "comment": "Camera Ready for ACL 2025", "summary": "Effective content moderation is essential for video platforms to safeguard\nuser experience and uphold community standards. While traditional video\nclassification models effectively handle well-defined moderation tasks, they\nstruggle with complicated scenarios such as implicit harmful content and\ncontextual ambiguity. Multimodal large language models (MLLMs) offer a\npromising solution to these limitations with their superior cross-modal\nreasoning and contextual understanding. However, two key challenges hinder\ntheir industrial adoption. First, the high computational cost of MLLMs makes\nfull-scale deployment impractical. Second, adapting generative models for\ndiscriminative classification remains an open research problem. In this paper,\nwe first introduce an efficient method to transform a generative MLLM into a\nmultimodal classifier using minimal discriminative training data. To enable\nindustry-scale deployment, we then propose a router-ranking cascade system that\nintegrates MLLMs with a lightweight router model. Offline experiments\ndemonstrate that our MLLM-based approach improves F1 score by 66.50% over\ntraditional classifiers while requiring only 2% of the fine-tuning data. Online\nevaluations show that our system increases automatic content moderation volume\nby 41%, while the cascading deployment reduces computational cost to only 1.5%\nof direct full-scale deployment.", "AI": {"tldr": "This paper introduces an efficient and cost-effective MLLM-based approach for content moderation, improving F1 score by 66.50% and increasing moderation volume by 41% while reducing computational cost to 1.5%.", "motivation": "traditional video classification models struggle with complicated scenarios such as implicit harmful content and contextual ambiguity. The high computational cost of MLLMs makes full-scale deployment impractical, and adapting generative models for discriminative classification remains an open research problem.", "method": "introduce an efficient method to transform a generative MLLM into a multimodal classifier using minimal discriminative training data. propose a router-ranking cascade system that integrates MLLMs with a lightweight router model.", "result": "MLLM-based approach improves F1 score by 66.50% over traditional classifiers while requiring only 2% of the fine-tuning data. The cascading deployment reduces computational cost to only 1.5% of direct full-scale deployment, and increases automatic content moderation volume by 41%.", "conclusion": "MLLM-based approach improves F1 score by 66.50% over traditional classifiers while requiring only 2% of the fine-tuning data. The cascading deployment reduces computational cost to only 1.5% of direct full-scale deployment, and increases automatic content moderation volume by 41%."}}
{"id": "2507.17747", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17747", "abs": "https://arxiv.org/abs/2507.17747", "authors": ["Linbo Cao", "Jinman Zhao"], "title": "Pretraining on the Test Set Is No Longer All You Need: A Debate-Driven Approach to QA Benchmarks", "comment": "22 pages, 7 figures. Accepted to COLM 2025. Code available at:\n  github.com/l6cao/Debate-Driven-Evaluation", "summary": "As frontier language models increasingly saturate standard QA benchmarks,\nconcerns about data contamination, memorization, and escalating dataset\ncreation costs persist. We propose a debate-driven evaluation paradigm that\ntransforms any existing QA dataset into structured adversarial debates--where\none model is given the official answer to defend, and another constructs and\ndefends an alternative answer--adjudicated by a judge model blind to the\ncorrect solution. By forcing multi-round argumentation, this approach\nsubstantially increases difficulty while penalizing shallow memorization, yet\nreuses QA items to reduce curation overhead. We make two main contributions:\n(1) an evaluation pipeline to systematically convert QA tasks into debate-based\nassessments, and (2) a public benchmark that demonstrates our paradigm's\neffectiveness on a subset of MMLU-Pro questions, complete with standardized\nprotocols and reference models. Empirical results validate the robustness of\nthe method and its effectiveness against data contamination--a Llama 3.1 model\nfine-tuned on test questions showed dramatic accuracy improvements (50% -> 82%)\nbut performed worse in debates. Results also show that even weaker judges can\nreliably differentiate stronger debaters, highlighting how debate-based\nevaluation can scale to future, more capable systems while maintaining a\nfraction of the cost of creating new benchmarks. Overall, our framework\nunderscores that \"pretraining on the test set is no longer all you need,\"\noffering a sustainable path for measuring the genuine reasoning ability of\nadvanced language models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fa9\u8bba\u7684\u8bc4\u4f30\u8303\u5f0f\uff0c\u4ee5\u63d0\u9ad8 QA \u8bc4\u4f30\u7684\u96be\u5ea6\u548c\u51cf\u5c11\u6570\u636e\u6c61\u67d3\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u6709\u6548\u4e14\u7a33\u5065\u3002", "motivation": "\u968f\u7740\u524d\u6cbf\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u9971\u548c\u6807\u51c6 QA \u57fa\u51c6\uff0c\u4eba\u4eec\u4ecd\u7136\u62c5\u5fc3\u6570\u636e\u6c61\u67d3\u3001\u8bb0\u5fc6\u548c\u4e0d\u65ad\u5347\u7ea7\u7684\u6570\u636e\u96c6\u521b\u5efa\u6210\u672c\u3002", "method": "\u4e00\u79cd\u57fa\u4e8e\u8fa9\u8bba\u7684\u8bc4\u4f30\u8303\u5f0f\uff0c\u5c06\u4efb\u4f55\u73b0\u6709\u7684 QA \u6570\u636e\u96c6\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u7684\u5bf9\u6297\u6027\u8fa9\u8bba\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u7a33\u5065\u6027\u53ca\u5176 \u043f\u0440\u043e\u0442\u0438\u0432 \u6570\u636e\u6c61\u67d3\u7684\u6709\u6548\u6027\u2014\u2014\u5728\u6d4b\u8bd5\u95ee\u9898\u4e0a\u5fae\u8c03\u7684 Llama 3.1 \u6a21\u578b\u663e\u793a\u51fa\u663e\u7740\u7684\u51c6\u786e\u6027\u63d0\u9ad8\uff0850% -> 82%\uff09\uff0c\u4f46\u5728\u8fa9\u8bba\u4e2d\u7684\u8868\u73b0\u66f4\u5dee\u3002\u7ed3\u679c\u8fd8\u8868\u660e\uff0c\u5373\u4f7f\u662f\u8f83\u5f31\u7684\u8bc4\u59d4\u4e5f\u53ef\u4ee5\u53ef\u9760\u5730\u533a\u5206\u8f83\u5f3a\u7684\u8fa9\u624b\uff0c\u7a81\u51fa\u4e86\u57fa\u4e8e\u8fa9\u8bba\u7684\u8bc4\u4f30\u5982\u4f55\u6269\u5c55\u5230\u672a\u6765\u66f4\u6709\u80fd\u529b\u7684\u7cfb\u7edf\uff0c\u540c\u65f6\u4fdd\u6301\u521b\u5efa\u65b0\u57fa\u51c6\u6210\u672c\u7684\u4e00\u5c0f\u90e8\u5206\u3002", "conclusion": "\u8be5\u6846\u67b6\u5f3a\u8c03\u201c\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u4e0d\u518d\u662f\u4f60\u6240\u9700\u8981\u7684\u5168\u90e8\u201d\uff0c\u4e3a\u8861\u91cf\u9ad8\u7ea7\u8bed\u8a00\u6a21\u578b\u7684\u771f\u6b63\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6301\u7eed\u7684\u8def\u5f84\u3002"}}
{"id": "2507.17185", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17185", "abs": "https://arxiv.org/abs/2507.17185", "authors": ["M. A. Rasel", "Sameem Abdul Kareem", "Zhenli Kwan", "Nik Aimee Azizah Faheem", "Winn Hui Han", "Rebecca Kai Jan Choong", "Shin Shen Yong", "Unaizah Obaidellah"], "title": "Asymmetric Lesion Detection with Geometric Patterns and CNN-SVM Classification", "comment": "Accepted version. Published in Computers in Biology and Medicine,\n  Volume 179, 2024. DOI: 10.1016/j.compbiomed.2024.108851", "summary": "In dermoscopic images, which allow visualization of surface skin structures\nnot visible to the naked eye, lesion shape offers vital insights into skin\ndiseases. In clinically practiced methods, asymmetric lesion shape is one of\nthe criteria for diagnosing melanoma. Initially, we labeled data for a\nnon-annotated dataset with symmetrical information based on clinical\nassessments. Subsequently, we propose a supporting technique, a supervised\nlearning image processing algorithm, to analyze the geometrical pattern of\nlesion shape, aiding non-experts in understanding the criteria of an asymmetric\nlesion. We then utilize a pre-trained convolutional neural network (CNN) to\nextract shape, color, and texture features from dermoscopic images for training\na multiclass support vector machine (SVM) classifier, outperforming\nstate-of-the-art methods from the literature. In the geometry-based experiment,\nwe achieved a 99.00% detection rate for dermatological asymmetric lesions. In\nthe CNN-based experiment, the best performance is found with 94% Kappa Score,\n95% Macro F1-score, and 97% Weighted F1-score for classifying lesion shapes\n(Asymmetric, Half-Symmetric, and Symmetric).", "AI": {"tldr": "\u4f7f\u7528\u76d1\u7763\u5b66\u4e60\u56fe\u50cf\u5904\u7406\u7b97\u6cd5\u5206\u6790\u75c5\u53d8\u5f62\u72b6\uff0c\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u63d0\u53d6\u7279\u5f81\uff0c\u8bad\u7ec3\u652f\u6301\u5411\u91cf\u673a\u5206\u7c7b\u5668\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u75c5\u53d8\u5f62\u72b6\u63d0\u4f9b\u4e86\u5bf9\u76ae\u80a4\u75be\u75c5\u7684\u91cd\u8981\u89c1\u89e3\u3002\u5728\u4e34\u5e8a\u5b9e\u8df5\u7684\u65b9\u6cd5\u4e2d\uff0c\u4e0d\u5bf9\u79f0\u75c5\u53d8\u5f62\u72b6\u662f\u8bca\u65ad\u9ed1\u8272\u7d20\u7624\u7684\u6807\u51c6\u4e4b\u4e00\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u652f\u6301\u6280\u672f\uff0c\u5373\u76d1\u7763\u5b66\u4e60\u56fe\u50cf\u5904\u7406\u7b97\u6cd5\uff0c\u4ee5\u5206\u6790\u75c5\u53d8\u5f62\u72b6\u7684\u51e0\u4f55\u56fe\u6848\u3002", "result": "\u51e0\u4f55\u5b9e\u9a8c\u68c0\u6d4b\u738799%\uff0cCNN\u5b9e\u9a8cKappa Score 94%, Macro F1-score 95%, Weighted F1-score 97%\u3002", "conclusion": "\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u63d0\u53d6\u5f62\u72b6\u3001\u989c\u8272\u548c\u7eb9\u7406\u7279\u5f81\uff0c\u5e76\u8bad\u7ec3\u591a\u7c7b\u652f\u6301\u5411\u91cf\u673a\u5206\u7c7b\u5668\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5728\u57fa\u4e8e\u51e0\u4f55\u7684\u5b9e\u9a8c\u4e2d\uff0c\u76ae\u80a4\u4e0d\u5bf9\u79f0\u75c5\u53d8\u7684\u68c0\u6d4b\u7387\u8fbe\u5230 99.00%\u3002\u5728\u57fa\u4e8e CNN \u7684\u5b9e\u9a8c\u4e2d\uff0c\u5206\u7c7b\u75c5\u53d8\u5f62\u72b6\uff08\u4e0d\u5bf9\u79f0\u3001\u534a\u5bf9\u79f0\u548c\u5bf9\u79f0\uff09\u7684\u6700\u4f73\u6027\u80fd\u4e3a 94% Kappa Score\u300195% Macro F1-score \u548c 97% Weighted F1-score\u3002"}}
{"id": "2507.17221", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17221", "abs": "https://arxiv.org/abs/2507.17221", "authors": ["Youneng Bao", "Yiping Liu", "Zhuo Chen", "Yongsheng Liang", "Mu Li", "Kede Ma"], "title": "Dataset Distillation as Data Compression: A Rate-Utility Perspective", "comment": "Accepted by ICCV 2025", "summary": "Driven by the ``scale-is-everything'' paradigm, modern machine learning\nincreasingly demands ever-larger datasets and models, yielding prohibitive\ncomputational and storage requirements. Dataset distillation mitigates this by\ncompressing an original dataset into a small set of synthetic samples, while\npreserving its full utility. Yet, existing methods either maximize performance\nunder fixed storage budgets or pursue suitable synthetic data representations\nfor redundancy removal, without jointly optimizing both objectives. In this\nwork, we propose a joint rate-utility optimization method for dataset\ndistillation. We parameterize synthetic samples as optimizable latent codes\ndecoded by extremely lightweight networks. We estimate the Shannon entropy of\nquantized latents as the rate measure and plug any existing distillation loss\nas the utility measure, trading them off via a Lagrange multiplier. To enable\nfair, cross-method comparisons, we introduce bits per class (bpc), a precise\nstorage metric that accounts for sample, label, and decoder parameter costs. On\nCIFAR-10, CIFAR-100, and ImageNet-128, our method achieves up to $170\\times$\ngreater compression than standard distillation at comparable accuracy. Across\ndiverse bpc budgets, distillation losses, and backbone architectures, our\napproach consistently establishes better rate-utility trade-offs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6570\u636e\u96c6\u84b8\u998f\u7684\u8054\u5408\u7387-\u6548\u7528\u4f18\u5316\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u76f8\u5f53\u7684\u7cbe\u5ea6\u4e0b\u5b9e\u73b0\u4e86\u6bd4\u6807\u51c6\u84b8\u998f\u9ad8 170 \u500d\u7684\u538b\u7f29\u7387\uff0c\u5e76\u4e14\u59cb\u7ec8\u80fd\u591f\u5efa\u7acb\u66f4\u597d\u7684\u7387-\u6548\u7528\u6743\u8861\u3002", "motivation": "\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u8d8a\u6765\u8d8a\u591a\u5730\u9700\u8981\u8d8a\u6765\u8d8a\u5927\u7684\u6570\u636e\u96c6\u548c\u6a21\u578b\uff0c\u4ece\u800c\u5bfc\u81f4\u8fc7\u9ad8\u7684\u8ba1\u7b97\u548c\u5b58\u50a8\u9700\u6c42\u3002\u6570\u636e\u96c6\u84b8\u998f\u901a\u8fc7\u5c06\u539f\u59cb\u6570\u636e\u96c6\u538b\u7f29\u6210\u4e00\u5c0f\u7ec4\u5408\u6210\u6837\u672c\u6765\u7f13\u89e3\u8fd9\u79cd\u60c5\u51b5\uff0c\u540c\u65f6\u4fdd\u7559\u5176\u5168\u90e8\u6548\u7528\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u8981\u4e48\u5728\u56fa\u5b9a\u7684\u5b58\u50a8\u9884\u7b97\u4e0b\u6700\u5927\u5316\u6027\u80fd\uff0c\u8981\u4e48\u5bfb\u6c42\u5408\u9002\u7684\u5408\u6210\u6570\u636e\u8868\u793a\u4ee5\u6d88\u9664\u5197\u4f59\uff0c\u800c\u6ca1\u6709\u8054\u5408\u4f18\u5316\u8fd9\u4e24\u4e2a\u76ee\u6807\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6570\u636e\u96c6\u84b8\u998f\u7684\u8054\u5408\u7387-\u6548\u7528\u4f18\u5316\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5c06\u5408\u6210\u6837\u672c\u53c2\u6570\u5316\u4e3a\u7531\u6781\u8f7b\u91cf\u7ea7\u7f51\u7edc\u89e3\u7801\u7684\u53ef\u4f18\u5316\u6f5c\u5728\u4ee3\u7801\u3002\u8be5\u65b9\u6cd5\u4f30\u8ba1\u91cf\u5316\u6f5c\u5728\u53d8\u91cf\u7684\u9999\u519c\u71b5\u4f5c\u4e3a\u901f\u7387\u5ea6\u91cf\uff0c\u5e76\u5c06\u4efb\u4f55\u73b0\u6709\u7684\u84b8\u998f\u635f\u5931\u4f5c\u4e3a\u6548\u7528\u5ea6\u91cf\uff0c\u5e76\u901a\u8fc7\u62c9\u683c\u6717\u65e5\u4e58\u6570\u8fdb\u884c\u6743\u8861\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u7684 bpc \u9884\u7b97\u3001\u84b8\u998f\u635f\u5931\u548c\u9aa8\u5e72\u67b6\u6784\u4e2d\uff0c\u59cb\u7ec8\u80fd\u591f\u5efa\u7acb\u66f4\u597d\u7684\u7387-\u6548\u7528\u6743\u8861\u3002\u5728 CIFAR-10\u3001CIFAR-100 \u548c ImageNet-128 \u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u76f8\u5f53\u7684\u7cbe\u5ea6\u4e0b\u5b9e\u73b0\u4e86\u6bd4\u6807\u51c6\u84b8\u998f\u9ad8 170 \u500d\u7684\u538b\u7f29\u7387\u3002", "conclusion": "\u5728 CIFAR-10\u3001CIFAR-100 \u548c ImageNet-128 \u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u76f8\u5f53\u7684\u7cbe\u5ea6\u4e0b\u5b9e\u73b0\u4e86\u6bd4\u6807\u51c6\u84b8\u998f\u9ad8 170 \u500d\u7684\u538b\u7f29\u7387\u3002\u5728\u4e0d\u540c\u7684 bpc \u9884\u7b97\u3001\u84b8\u998f\u635f\u5931\u548c\u9aa8\u5e72\u67b6\u6784\u4e2d\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u80fd\u591f\u5efa\u7acb\u66f4\u597d\u7684\u7387-\u6548\u7528\u6743\u8861\u3002"}}
{"id": "2507.17192", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17192", "abs": "https://arxiv.org/abs/2507.17192", "authors": ["Haiyu Wu", "Jaskirat Singh", "Sicong Tian", "Liang Zheng", "Kevin W. Bowyer"], "title": "Vec2Face+ for Face Dataset Generation", "comment": null, "summary": "When synthesizing identities as face recognition training data, it is\ngenerally believed that large inter-class separability and intra-class\nattribute variation are essential for synthesizing a quality dataset. % This\nbelief is generally correct, and this is what we aim for. However, when\nincreasing intra-class variation, existing methods overlook the necessity of\nmaintaining intra-class identity consistency. % To address this and generate\nhigh-quality face training data, we propose Vec2Face+, a generative model that\ncreates images directly from image features and allows for continuous and easy\ncontrol of face identities and attributes. Using Vec2Face+, we obtain datasets\nwith proper inter-class separability and intra-class variation and identity\nconsistency using three strategies: 1) we sample vectors sufficiently different\nfrom others to generate well-separated identities; 2) we propose an AttrOP\nalgorithm for increasing general attribute variations; 3) we propose LoRA-based\npose control for generating images with profile head poses, which is more\nefficient and identity-preserving than AttrOP. % Our system generates VFace10K,\na synthetic face dataset with 10K identities, which allows an FR model to\nachieve state-of-the-art accuracy on seven real-world test sets. Scaling the\nsize to 4M and 12M images, the corresponding VFace100K and VFace300K datasets\nyield higher accuracy than the real-world training dataset, CASIA-WebFace, on\nfive real-world test sets. This is the first time a synthetic dataset beats the\nCASIA-WebFace in average accuracy. In addition, we find that only 1 out of 11\nsynthetic datasets outperforms random guessing (\\emph{i.e., 50\\%}) in twin\nverification and that models trained with synthetic identities are more biased\nthan those trained with real identities. Both are important aspects for future\ninvestigation.", "AI": {"tldr": "Vec2Face+ is proposed to generate high-quality face training data with proper inter-class separability, intra-class variation and identity consistency. A synthetic face dataset is created and achieves state-of-the-art accuracy on real-world test sets.", "motivation": "Large inter-class separability and intra-class attribute variation are essential for synthesizing a quality dataset. However, when increasing intra-class variation, existing methods overlook the necessity of maintaining intra-class identity consistency.", "method": "A generative model Vec2Face+ is proposed, which creates images directly from image features and allows for continuous and easy control of face identities and attributes. Datasets with proper inter-class separability, intra-class variation and identity consistency are obtained by sampling vectors, using AttrOP algorithm and LoRA-based pose control.", "result": "VFace10K allows an FR model to achieve state-of-the-art accuracy on seven real-world test sets. VFace100K and VFace300K yield higher accuracy than the real-world training dataset, CASIA-WebFace, on five real-world test sets. Models trained with synthetic identities are more biased than those trained with real identities.", "conclusion": "A synthetic face dataset (VFace10K, VFace100K and VFace300K) is created, which allows an FR model to achieve state-of-the-art accuracy on real-world test sets. Models trained with synthetic identities are more biased than those trained with real identities, which requires future investigation."}}
{"id": "2507.17228", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.17228", "abs": "https://arxiv.org/abs/2507.17228", "authors": ["Wei Fan", "JinYi Yoon", "Xiaochang Li", "Huajie Shao", "Bo Ji"], "title": "P3SL: Personalized Privacy-Preserving Split Learning on Heterogeneous Edge Devices", "comment": "Accepted as invited paper in The 34th International Conference on\n  Computer Communications and Networks (ICCCN 2025)", "summary": "Split Learning (SL) is an emerging privacy-preserving machine learning\ntechnique that enables resource constrained edge devices to participate in\nmodel training by partitioning a model into client-side and server-side\nsub-models. While SL reduces computational overhead on edge devices, it\nencounters significant challenges in heterogeneous environments where devices\nvary in computing resources, communication capabilities, environmental\nconditions, and privacy requirements. Although recent studies have explored\nheterogeneous SL frameworks that optimize split points for devices with varying\nresource constraints, they often neglect personalized privacy requirements and\nlocal model customization under varying environmental conditions. To address\nthese limitations, we propose P3SL, a Personalized Privacy-Preserving Split\nLearning framework designed for heterogeneous, resource-constrained edge device\nsystems. The key contributions of this work are twofold. First, we design a\npersonalized sequential split learning pipeline that allows each client to\nachieve customized privacy protection and maintain personalized local models\ntailored to their computational resources, environmental conditions, and\nprivacy needs. Second, we adopt a bi-level optimization technique that empowers\nclients to determine their own optimal personalized split points without\nsharing private sensitive information (i.e., computational resources,\nenvironmental conditions, privacy requirements) with the server. This approach\nbalances energy consumption and privacy leakage risks while maintaining high\nmodel accuracy. We implement and evaluate P3SL on a testbed consisting of 7\ndevices including 4 Jetson Nano P3450 devices, 2 Raspberry Pis, and 1 laptop,\nusing diverse model architectures and datasets under varying environmental\nconditions.", "AI": {"tldr": "P3SL is a Personalized Privacy-Preserving Split Learning framework designed for heterogeneous, resource-constrained edge device systems", "motivation": "Split Learning (SL) encounters significant challenges in heterogeneous environments where devices vary in computing resources, communication capabilities, environmental conditions, and privacy requirements. Although recent studies have explored heterogeneous SL frameworks that optimize split points for devices with varying resource constraints, they often neglect personalized privacy requirements and local model customization under varying environmental conditions.", "method": "We adopt a bi-level optimization technique that empowers clients to determine their own optimal personalized split points without sharing private sensitive information (i.e., computational resources, environmental conditions, privacy requirements) with the server.", "result": "We propose P3SL, a Personalized Privacy-Preserving Split Learning framework designed for heterogeneous, resource-constrained edge device systems. The key contributions of this work are twofold. First, we design a personalized sequential split learning pipeline that allows each client to achieve customized privacy protection and maintain personalized local models tailored to their computational resources, environmental conditions, and privacy needs. Second, we adopt a bi-level optimization technique that empowers clients to determine their own optimal personalized split points without sharing private sensitive information (i.e., computational resources, environmental conditions, privacy requirements) with the server. This approach balances energy consumption and privacy leakage risks while maintaining high model accuracy.", "conclusion": "We implement and evaluate P3SL on a testbed consisting of 7 devices including 4 Jetson Nano P3450 devices, 2 Raspberry Pis, and 1 laptop, using diverse model architectures and datasets under varying environmental conditions."}}
{"id": "2507.17202", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17202", "abs": "https://arxiv.org/abs/2507.17202", "authors": ["Jooyeol Yun", "Heng Wang", "Yotaro Shimose", "Jaegul Choo", "Shingo Takamatsu"], "title": "DesignLab: Designing Slides Through Iterative Detection and Correction", "comment": "https://yeolj00.github.io/personal-projects/designlab", "summary": "Designing high-quality presentation slides can be challenging for non-experts\ndue to the complexity involved in navigating various design choices. Numerous\nautomated tools can suggest layouts and color schemes, yet often lack the\nability to refine their own output, which is a key aspect in real-world\nworkflows. We propose DesignLab, which separates the design process into two\nroles, the design reviewer, who identifies design-related issues, and the\ndesign contributor who corrects them. This decomposition enables an iterative\nloop where the reviewer continuously detects issues and the contributor\ncorrects them, allowing a draft to be further polished with each iteration,\nreaching qualities that were unattainable. We fine-tune large language models\nfor these roles and simulate intermediate drafts by introducing controlled\nperturbations, enabling the design reviewer learn design errors and the\ncontributor learn how to fix them. Our experiments show that DesignLab\noutperforms existing design-generation methods, including a commercial tool, by\nembracing the iterative nature of designing which can result in polished,\nprofessional slides.", "AI": {"tldr": "DesignLab\u5c06\u8bbe\u8ba1\u8fc7\u7a0b\u5206\u89e3\u4e3a\u5ba1\u67e5\u548c\u8d21\u732e\u4e24\u4e2a\u89d2\u8272\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u5e7b\u706f\u7247\u8bbe\u8ba1\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u975e\u4e13\u4e1a\u4eba\u58eb\u8bbe\u8ba1\u9ad8\u8d28\u91cf\u6f14\u793a\u5e7b\u706f\u7247\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u5de5\u5177\u7f3a\u4e4f\u6539\u8fdb\u81ea\u8eab\u8f93\u51fa\u7684\u80fd\u529b\u3002", "method": "\u5c06\u8bbe\u8ba1\u8fc7\u7a0b\u5206\u89e3\u4e3a\u8bbe\u8ba1\u5ba1\u67e5\u5458\u548c\u8bbe\u8ba1\u8d21\u732e\u8005\u4e24\u4e2a\u89d2\u8272\uff0c\u901a\u8fc7\u8fed\u4ee3\u5faa\u73af\u4e0d\u65ad\u6539\u8fdb\u5e7b\u706f\u7247\u8bbe\u8ba1\u3002", "result": "DesignLab\u4f18\u4e8e\u73b0\u6709\u8bbe\u8ba1\u751f\u6210\u65b9\u6cd5\uff0c\u5305\u62ec\u5546\u4e1a\u5de5\u5177\u3002", "conclusion": "DesignLab\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u8bbe\u8ba1\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u8bbe\u8ba1\u751f\u6210\u65b9\u6cd5\uff0c\u5305\u62ec\u5546\u4e1a\u5de5\u5177\uff0c\u80fd\u591f\u751f\u6210\u66f4\u7cbe\u81f4\u3001\u4e13\u4e1a\u7684\u5e7b\u706f\u7247\u3002"}}
