<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 28]
- [cs.CV](#cs.CV) [Total: 73]
- [cs.AI](#cs.AI) [Total: 23]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.IR](#cs.IR) [Total: 14]
- [cs.LG](#cs.LG) [Total: 56]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Test-time Scaling of LLMs: A Survey from A Subproblem Structure Perspective](https://arxiv.org/abs/2511.14772)
*Zhuoyi Yang,Xu Guo,Tong Zhang,Huijuan Xu,Boyang Li*

Main category: cs.CL

TL;DR: 本文综述了通过在推理时分配额外计算来提高预训练大型语言模型预测准确性的技术。


<details>
  <summary>Details</summary>
Motivation: 提高预训练大型语言模型的预测准确性。

Method: 对测试时缩放方法进行分类，特别强调问题如何分解为子问题以及这些子问题的拓扑结构（顺序、并行或树形结构）。

Result: 统一了思维链、分支-解决-合并和思维树等不同的方法。

Conclusion: 总结了这些技术的优缺点，并概述了未来研究的有希望的方向。

Abstract: With this paper, we survey techniques for improving the predictive accuracy of pretrained large language models by allocating additional compute at inference time. In categorizing test-time scaling methods, we place special emphasis on how a problem is decomposed into subproblems and on the topological organization of these subproblems whether sequential, parallel, or tree-structured. This perspective allows us to unify diverse approaches such as Chain-of-Thought, Branch-Solve-Merge, and Tree-of-Thought under a common lens. We further synthesize existing analyses of these techniques, highlighting their respective strengths and weaknesses, and conclude by outlining promising directions for future research

</details>


### [2] [Temporal Predictors of Outcome in Reasoning Language Models](https://arxiv.org/abs/2511.14773)
*Joey David*

Main category: cs.CL

TL;DR: 大型语言模型在推理的早期阶段就倾向于确定最终结果。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在链式思考（CoT）过程中，模型在多早的阶段会在内部确定最终结果。

Method: 通过在模型输出前几个推理token的隐藏状态上训练线性分类器，来预测最终答案的正确性。

Result: 结果表明，模型在输出少量token后，其正确性已具有高度可预测性。对于难题，预测准确率的下降表明存在选择偏差，因为长CoT中难题的比例过高。

Conclusion: 对于推理模型，内部自我评估机制在少量token后就会显现，这对模型的可解释性和推理时控制具有重要意义。

Abstract: The chain-of-thought (CoT) paradigm uses the elicitation of step-by-step rationales as a proxy for reasoning, gradually refining the model's latent representation of a solution. However, it remains unclear just how early a Large Language Model (LLM) internally commits to an eventual outcome. We probe this by training linear classifiers on hidden states after the first t reasoning tokens, showing that eventual correctness is highly predictable after only a few tokens, even when longer outputs are needed to reach a definite answer. We show that, for harder questions, a drop in predictive accuracy highlights a selection artifact: hard items are disproportionately represented in long CoTs. Overall, our results imply that for reasoning models, internal self-assessment of success tends to emerge after only a few tokens, with implications for interpretability and for inference-time control.

</details>


### [3] [LiveCLKTBench: Towards Reliable Evaluation of Cross-Lingual Knowledge Transfer in Multilingual LLMs](https://arxiv.org/abs/2511.14774)
*Pei-Fu Guo,Yun-Da Tsai,Chun-Chia Hsu,Kai-Xin Chen,Ya-An Tsai,Kai-Wei Chang,Nanyun Peng,Mi-Yen Yeh,Shou-De Lin*

Main category: cs.CL

TL;DR: 提出了LiveCLKTBench，一个自动生成流程，用于隔离和测量跨语言知识迁移。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型中的跨语言知识迁移具有挑战性，因为目标语言中的正确答案可能来自真正的迁移或预训练期间的先前接触。

Method: 该流程从真实世界的领域中识别出独立的、时间敏感的知识实体，根据时间发生情况对其进行过滤，并根据模型的知识对其进行验证。然后，这些有效实体的文档用于生成事实问题，这些问题被翻译成多种语言，以评估跨语言边界的迁移能力。

Result: 使用LiveCLKTBench，我们评估了五种语言的几个LLM，并观察到跨语言迁移受到语言距离的强烈影响，并且在语言方向上通常是不对称的。虽然较大的模型改善了迁移，但收益随着规模的增加而减少，并且因领域而异。

Conclusion: 这些发现为多语言迁移提供了新的见解，并证明了LiveCLKTBench作为未来研究的可靠基准的价值。

Abstract: Evaluating cross-lingual knowledge transfer in large language models is challenging, as correct answers in a target language may arise either from genuine transfer or from prior exposure during pre-training. We present LiveCLKTBench, an automated generation pipeline specifically designed to isolate and measure cross-lingual knowledge transfer. Our pipeline identifies self-contained, time-sensitive knowledge entities from real-world domains, filters them based on temporal occurrence, and verifies them against the model's knowledge. The documents of these valid entities are then used to generate factual questions, which are translated into multiple languages to evaluate transferability across linguistic boundaries. Using LiveCLKTBench, we evaluate several LLMs across five languages and observe that cross-lingual transfer is strongly influenced by linguistic distance and often asymmetric across language directions. While larger models improve transfer, the gains diminish with scale and vary across domains. These findings provide new insights into multilingual transfer and demonstrate the value of LiveCLKTBench as a reliable benchmark for future research.

</details>


### [4] [COMPASS: Context-Modulated PID Attention Steering System for Hallucination Mitigation](https://arxiv.org/abs/2511.14776)
*Snigdha Pandya,Rohan Nagale,Kenji Sahay,Anna Lin,Shikhar Shiromani,Kevin Zhu,Dev Sunishchal*

Main category: cs.CL

TL;DR: COMPASS是一个轻量级的、可解释的控制框架，它在解码过程中嵌入了一个基于模型的反馈循环，用于量化上下文依赖性，并动态地调整注意力头以保持事实一致性，而无需重新训练或多通道解码。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 经常生成流畅但事实上不正确的陈述，即使它们可以访问相关的证据，这是一种根植于它们如何在上下文知识和参数知识之间分配注意力的失败模式。理解和引导这种内部行为是值得信赖的部署和模型机制的科学可解释性的关键。

Method: COMPASS量化上下文依赖性通过一个透明的指标，上下文依赖性分数 (CRS)，作为一个在线探针，了解注意力头如何在证据中进行基础生成。使用这种可解释的信号，PID控制器动态地调节注意力头，以保持事实一致性，而无需重新训练或多通道解码。

Result: 在多个基准测试中，COMPASS始终降低了上下文幻觉率（绝对值2.8%到5.8%），同时揭示了不同的注意力头如何促进证据对齐。

Conclusion: 这些结果突出了反馈驱动的可解释性是科学理解LLM行为的途径。

Abstract: Large language models (LLMs) often generate fluent but factually incorrect statements despite having access to relevant evidence, a failure mode rooted in how they allocate attention between contextual and parametric knowledge. Understanding and steering this internal behavior is key both for trustworthy deployment and for scientific interpretability of model mechanisms. We introduce COMPASS (Context-Modulated PID Attention Steering System), a lightweight, interpretable control framework that embeds a model-based feedback loop directly within decoding. COMPASS quantifies context reliance via a transparent metric, the Context Reliance Score (CRS), which serves as an online probe of how attention heads ground generation in evidence. Using this interpretable signal, a PID controller dynamically modulates attention heads to maintain factual consistency without retraining or multi-pass decoding. Across benchmarks (HotpotQA, XSum, HaluEval, RAGTruth), COMPASS consistently reduces contextual hallucination rates (2.8 to 5.8 percent absolute) while revealing how distinct attention heads contribute to evidence alignment. These results highlight feedback-driven interpretability as a pathway toward scientific understanding of LLM behavior.

</details>


### [5] [The Impact of Prosodic Segmentation on Speech Synthesis of Spontaneous Speech](https://arxiv.org/abs/2511.14779)
*Julio Cesar Galdino,Sidney Evaldo Leal,Leticia Gabriella De Souza,Rodrigo de Freitas Lima,Antonio Nelson Fornari Mendes Moreira,Arnaldo Candido Junior,Miguel Oliveira,Edresson Casanova,Sandra M. Aluísio*

Main category: cs.CL

TL;DR: 本文探讨了在巴西葡萄牙语的语音合成中，人工和自动韵律分割标注对语音质量的影响。


<details>
  <summary>Details</summary>
Motivation: 目前语音合成系统在生成自然和可理解的语音方面取得了显著进展，但带有显式韵律分割的数据集的构建及其对自然语音合成的影响在很大程度上仍未被探索。

Method: 通过非自回归模型FastSpeech 2，评估了人工和自动韵律分割标注对合成语音质量的影响。

Result: 实验结果表明，使用韵律分割进行训练可以产生稍微更清晰和声音更自然的语音。自动分割倾向于创建更规则的片段，而手动韵律分割引入了更大的变异性，这有助于更自然的韵律。

Conclusion: 中性陈述句的分析表明，两种训练方法都再现了预期的核心重音模式，但韵律模型与自然的核前轮廓更紧密地对齐。为了支持可重复性和未来的研究，所有数据集、源代码和训练的模型都根据 CC BY-NC-ND 4.0 许可证公开提供。

Abstract: Spontaneous speech presents several challenges for speech synthesis, particularly in capturing the natural flow of conversation, including turn-taking, pauses, and disfluencies. Although speech synthesis systems have made significant progress in generating natural and intelligible speech, primarily through architectures that implicitly model prosodic features such as pitch, intensity, and duration, the construction of datasets with explicit prosodic segmentation and their impact on spontaneous speech synthesis remains largely unexplored. This paper evaluates the effects of manual and automatic prosodic segmentation annotations in Brazilian Portuguese on the quality of speech synthesized by a non-autoregressive model, FastSpeech 2. Experimental results show that training with prosodic segmentation produced slightly more intelligible and acoustically natural speech. While automatic segmentation tends to create more regular segments, manual prosodic segmentation introduces greater variability, which contributes to more natural prosody. Analysis of neutral declarative utterances showed that both training approaches reproduced the expected nuclear accent pattern, but the prosodic model aligned more closely with natural pre-nuclear contours. To support reproducibility and future research, all datasets, source codes, and trained models are publicly available under the CC BY-NC-ND 4.0 license.

</details>


### [6] [Human or LLM as Standardized Patients? A Comparative Study for Medical Education](https://arxiv.org/abs/2511.14783)
*Bingquan Zhang,Xiaoxiao Liu,Yuchi Wang,Lei Zhou,Qianqian Xie,Benyou Wang*

Main category: cs.CL

TL;DR: EasyMED，一个多智能体框架，结合了患者代理、辅助代理和评估代理，用于临床技能培训。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大型语言模型的标准化病人（SP）模拟器虽然成本较低，但行为不一致，且缺乏与人类SP的严格比较。

Method: 使用多智能体框架EasyMED，结合患者代理、辅助代理和评估代理。

Result: EasyMED在学习成果上与人类SP相当，同时为低起点的学生提供更大的技能提升，并具有更好的灵活性、心理安全性和成本效益。

Conclusion: EasyMED 能够匹配人类 SP 的学习成果，同时为起点较低的学生提供更大的技能提升，并具有更好的灵活性、心理安全性和成本效益。

Abstract: Standardized Patients (SP) are indispensable for clinical skills training but remain expensive, inflexible, and difficult to scale. Existing large-language-model (LLM)-based SP simulators promise lower cost yet show inconsistent behavior and lack rigorous comparison with human SP. We present EasyMED, a multi-agent framework combining a Patient Agent for realistic dialogue, an Auxiliary Agent for factual consistency, and an Evaluation Agent that delivers actionable feedback. To support systematic assessment, we introduce SPBench, a benchmark of real SP-doctor interactions spanning 14 specialties and eight expert-defined evaluation criteria. Experiments demonstrate that EasyMED matches human SP learning outcomes while producing greater skill gains for lower-baseline students and offering improved flexibility, psychological safety, and cost efficiency.

</details>


### [7] [Opinion Mining and Analysis Using Hybrid Deep Neural Networks](https://arxiv.org/abs/2511.14796)
*Adel Hidri,Suleiman Ali Alsaif,Muteeb Alahmari,Eman AlShehri,Minyar Sassi Hidri*

Main category: cs.CL

TL;DR: 提出了一种混合深度神经网络模型（HBGRU-LSTM），用于情感分析，旨在解决上下文细微差别、可扩展性和类别不平衡等挑战。


<details>
  <summary>Details</summary>
Motivation: 社交媒体和电子商务的日益增长的影响使得理解客户态度已成为决策的关键组成部分。现有的方法不足以处理上下文细微差别和可扩展性。

Method: 结合了双向门控循环单元（BGRU）和长短期记忆（LSTM）层。

Result: 在IMDB电影评论和Amazon产品评估等基准数据集上进行了综合实验，HBGRU-LSTM架构达到了95%的测试准确率，超过了传统的深度学习框架。负面情绪的召回率从86%（非平衡数据集）提高到96%（平衡数据集）。

Conclusion: 该模型降低了非平衡数据集的错误分类损失（从20.24%降至13.3%），表明泛化性和鲁棒性增强。

Abstract: Understanding customer attitudes has become a critical component of decision-making due to the growing influence of social media and e-commerce. Text-based opinions are the most structured, hence playing an important role in sentiment analysis. Most of the existing methods, which include lexicon-based approaches and traditional machine learning techniques, are insufficient for handling contextual nuances and scalability. While the latter has limitations in model performance and generalization, deep learning (DL) has achieved improvement, especially on semantic relationship capturing with recurrent neural networks (RNNs) and convolutional neural networks (CNNs). The aim of the study is to enhance opinion mining by introducing a hybrid deep neural network model that combines a bidirectional gated recurrent unit (BGRU) and long short-term memory (LSTM) layers to improve sentiment analysis, particularly addressing challenges such as contextual nuance, scalability, and class imbalance. To substantiate the efficacy of the proposed model, we conducted comprehensive experiments utilizing benchmark datasets, encompassing IMDB movie critiques and Amazon product evaluations. The introduced hybrid BGRULSTM (HBGRU-LSTM) architecture attained a testing accuracy of 95%, exceeding the performance of traditional DL frameworks such as LSTM (93.06%), CNN+LSTM (93.31%), and GRU+LSTM (92.20%). Moreover, our model exhibited a noteworthy enhancement in recall for negative sentiments, escalating from 86% (unbalanced dataset) to 96% (balanced dataset), thereby ensuring a more equitable and just sentiment classification. Furthermore, the model diminished misclassification loss from 20.24% for unbalanced to 13.3% for balanced dataset, signifying enhanced generalization and resilience.

</details>


### [8] [Hierarchical Token Prepending: Enhancing Information Flow in Decoder-based LLM Embeddings](https://arxiv.org/abs/2511.14868)
*Xueying Ding,Xingyue Huang,Mingxuan Ju,Liam Collins,Yozen Liu,Leman Akoglu,Neil Shah,Tong Zhao*

Main category: cs.CL

TL;DR: 提出了一种名为分层令牌预置（HTP）的方法，旨在解决大型语言模型在处理长文档时，由于因果注意力机制导致的信息流动受限和表示质量下降的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型产生强大的文本嵌入，但其因果注意力机制限制了信息从后到前的流动，降低了表示质量。现有方法通过预先添加单个摘要令牌来解决这个问题，但过度压缩信息，从而损害了长文档的性能。

Method: HTP将输入划分为块，并将块级摘要令牌预置到后续块，从而创建多个向后信息流的路径。用平均池化代替末尾令牌池化。

Result: HTP在11个检索数据集和30个通用嵌入基准测试中取得了持续的性能提升，尤其是在长上下文设置中。

Conclusion: HTP作为一种简单的、与架构无关的方法，增强了零样本和微调模型，为卓越的长文档嵌入提供了一条可扩展的途径。

Abstract: Large language models produce powerful text embeddings, but their causal attention mechanism restricts the flow of information from later to earlier tokens, degrading representation quality. While recent methods attempt to solve this by prepending a single summary token, they over-compress information, hence harming performance on long documents. We propose Hierarchical Token Prepending (HTP), a method that resolves two critical bottlenecks. To mitigate attention-level compression, HTP partitions the input into blocks and prepends block-level summary tokens to subsequent blocks, creating multiple pathways for backward information flow. To address readout-level over-squashing, we replace last-token pooling with mean-pooling, a choice supported by theoretical analysis. HTP achieves consistent performance gains across 11 retrieval datasets and 30 general embedding benchmarks, especially in long-context settings. As a simple, architecture-agnostic method, HTP enhances both zero-shot and finetuned models, offering a scalable route to superior long-document embeddings.

</details>


### [9] [Mathematical Analysis of Hallucination Dynamics in Large Language Models: Uncertainty Quantification, Advanced Decoding, and Principled Mitigation](https://arxiv.org/abs/2511.15005)
*Moses Kiprono*

Main category: cs.CL

TL;DR: 大型语言模型（LLM）虽然强大，但容易产生幻觉，输出听起来合理但不正确或无根据的内容。本研究提出了一个数学框架来理解、测量和减轻这些幻觉。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型容易产生幻觉，输出内容可能不正确。

Method: 利用概率建模、信息论、三角信号分析和贝叶斯不确定性估计来分析误差的自回归累积方式，并提出改进的不确定性指标。

Result: 提出了包括语义和相位感知变体在内的改进的不确定性指标。

Conclusion: 该研究统一了校准、检索和对齐方面的最新进展，以支持更安全、更可靠的LLM。

Abstract: Large Language Models (LLMs) are powerful linguistic engines but remain susceptible to hallucinations: plausible-sounding outputs that are factually incorrect or unsupported. In this work, we present a mathematically grounded framework to understand, measure, and mitigate these hallucinations. Drawing on probabilistic modeling, information theory, trigonometric signal analysis, and Bayesian uncertainty estimation, we analyze how errors compound autoregressively, propose refined uncertainty metrics, including semantic and phase-aware variants, and develop principled mitigation strategies such as contrastive decoding, retrieval-augmented grounding, factual alignment, and abstention. This unified lens connects recent advances in calibration, retrieval, and alignment to support safer and more reliable LLMs.

</details>


### [10] [Teaching According to Students' Aptitude: Personalized Mathematics Tutoring via Persona-, Memory-, and Forgetting-Aware LLMs](https://arxiv.org/abs/2511.15163)
*Yang Wu,Rujing Yao,Tong Zhang,Yufei Shi,Zhuoren Jiang,Zhushan Li,Xiaozhong Liu*

Main category: cs.CL

TL;DR: TASA是一种学生感知辅导框架，它集成了角色、记忆和遗忘动态，以实现个性化的数学学习。


<details>
  <summary>Details</summary>
Motivation: 现有的方法无法捕捉学生知识在他们的能力、概念差距和遗忘模式中的动态演变。在数学辅导中，有效的指导需要根据每个学生的掌握程度和认知保持情况进行精确的调整。

Method: TASA维护了一个结构化的学生角色，捕捉能力概况，并记录先前的学习互动。通过结合连续遗忘曲线和知识追踪，TASA动态更新每个学生的掌握状态，并生成上下文适当的、难度校准的问题和解释。

Result: TASA实现了卓越的学习成果和更具适应性的辅导行为。

Conclusion: 在基于LLM的辅导系统中，对时间遗忘和学习者概况进行建模非常重要。

Abstract: Large Language Models (LLMs) are increasingly integrated into intelligent tutoring systems to provide human-like and adaptive instruction. However, most existing approaches fail to capture how students' knowledge evolves dynamically across their proficiencies, conceptual gaps, and forgetting patterns. This challenge is particularly acute in mathematics tutoring, where effective instruction requires fine-grained scaffolding precisely calibrated to each student's mastery level and cognitive retention. To address this issue, we propose TASA (Teaching According to Students' Aptitude), a student-aware tutoring framework that integrates persona, memory, and forgetting dynamics for personalized mathematics learning. Specifically, TASA maintains a structured student persona capturing proficiency profiles and an event memory recording prior learning interactions. By incorporating a continuous forgetting curve with knowledge tracing, TASA dynamically updates each student's mastery state and generates contextually appropriate, difficulty-calibrated questions and explanations. Empirical results demonstrate that TASA achieves superior learning outcomes and more adaptive tutoring behavior compared to representative baselines, underscoring the importance of modeling temporal forgetting and learner profiles in LLM-based tutoring systems.

</details>


### [11] [HinTel-AlignBench: A Framework and Benchmark for Hindi-Telugu with English-Aligned Samples](https://arxiv.org/abs/2511.15183)
*Rishikant Chigrupaatii,Ponnada Sai Tulasi Kanishka,Lalit Chandra Routhu,Martin Patel Sama Supratheek Reddy,Divyam Gupta,Dasari Srikar,Krishna Teja Kuchimanchi,Rajiv Misra,Rohun Tripathi*

Main category: cs.CL

TL;DR: 本研究提出了一个可扩展的框架，用于评估印度语言的视觉语言模型 (VLM)，并将其与英语的性能进行比较。


<details>
  <summary>Details</summary>
Motivation: 现有的多语言 VLM 评估存在四个主要局限性：依赖未经证实的自动翻译、狭窄的任务/领域覆盖范围、有限的样本量以及缺乏文化和本地来源的问答 (QA)。

Method: 使用该框架，我们生成了 HinTel-AlignBench，这是一个基准，它借鉴了印地语和泰卢固语的各种来源，并包含与英语对齐的样本。我们结合了回译、过滤和人工验证的半自动数据集创建框架。

Result: 对于所有模型，在 5 个任务中的 4 个任务中，英语任务与印度语言任务相比，性能有所下降，印地语平均下降 8.3 个点，泰卢固语平均下降 5.5 个点。

Conclusion: 多语言多模态理解方面仍有改进空间。

Abstract: With nearly 1.5 billion people and more than 120 major languages, India represents one of the most diverse regions in the world. As multilingual Vision-Language Models (VLMs) gain prominence, robust evaluation methodologies are essential to drive progress toward equitable AI for low-resource languages. Current multilingual VLM evaluations suffer from four major limitations: reliance on unverified auto-translations, narrow task/domain coverage, limited sample sizes, and lack of cultural and natively sourced Question-Answering (QA). To address these gaps, we present a scalable framework to evaluate VLMs in Indian languages and compare it with performance in English. Using the framework, we generate HinTel-AlignBench, a benchmark that draws from diverse sources in Hindi and Telugu with English-aligned samples. Our contributions are threefold: (1) a semi-automated dataset creation framework combining back-translation, filtering, and human verification; (2) the most comprehensive vision-language benchmark for Hindi and and Telugu, including adapted English datasets (VQAv2, RealWorldQA, CLEVR-Math) and native novel Indic datasets (JEE for STEM, VAANI for cultural grounding) with approximately 4,000 QA pairs per language; and (3) a detailed performance analysis of various State-of-the-Art (SOTA) open-weight and closed-source VLMs. We find a regression in performance for tasks in English versus in Indian languages for 4 out of 5 tasks across all the models, with an average regression of 8.3 points in Hindi and 5.5 points for Telugu. We categorize common failure modes to highlight concrete areas of improvement in multilingual multimodal understanding.

</details>


### [12] [Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story](https://arxiv.org/abs/2511.15210)
*Vladislav Pedashenko,Laida Kushnareva,Yana Khassan Nibal,Eduard Tulchinskii,Kristian Kuznetsov,Vladislav Zharchinskii,Yury Maximov,Irina Piontkovskaya*

Main category: cs.CL

TL;DR: 本研究深入探讨了大型语言模型（LLM）中文本内在维度（ID）的决定因素，通过跨编码器分析、语言特征和稀疏自编码器（SAE）揭示了ID与文本属性之间的关系。


<details>
  <summary>Details</summary>
Motivation: 尽管内在维度是现代LLM分析的重要工具，但其文本决定因素仍未得到充分探索。

Method: 通过跨编码器分析、语言特征和稀疏自编码器（SAE）相结合的方法。

Result: 研究发现了三个关键结果：ID与基于熵的指标互补；ID表现出稳健的类型分层；识别出因果特征：科学信号降低ID，而人性化信号增加ID。

Conclusion: 科学写作相对“容易”，而小说、观点和情感增加了表征自由度。

Abstract: Intrinsic dimension (ID) is an important tool in modern LLM analysis, informing studies of training dynamics, scaling behavior, and dataset structure, yet its textual determinants remain underexplored. We provide the first comprehensive study grounding ID in interpretable text properties through cross-encoder analysis, linguistic features, and sparse autoencoders (SAEs). In this work, we establish three key findings. First, ID is complementary to entropy-based metrics: after controlling for length, the two are uncorrelated, with ID capturing geometric complexity orthogonal to prediction quality. Second, ID exhibits robust genre stratification: scientific prose shows low ID (~8), encyclopedic content medium ID (~9), and creative/opinion writing high ID (~10.5) across all models tested. This reveals that contemporary LLMs find scientific text "representationally simple" while fiction requires additional degrees of freedom. Third, using SAEs, we identify causal features: scientific signals (formal tone, report templates, statistics) reduce ID; humanized signals (personalization, emotion, narrative) increase it. Steering experiments confirm these effects are causal. Thus, for contemporary models, scientific writing appears comparatively "easy", whereas fiction, opinion, and affect add representational degrees of freedom. Our multi-faceted analysis provides practical guidance for the proper use of ID and the sound interpretation of ID-based results.

</details>


### [13] [OEMA: Ontology-Enhanced Multi-Agent Collaboration Framework for Zero-Shot Clinical Named Entity Recognition](https://arxiv.org/abs/2511.15211)
*Xinli Tao,Xin Dong,Xuezhong Zhou*

Main category: cs.CL

TL;DR: 这篇论文提出了一种名为OEMA的零样本临床命名实体识别框架，该框架通过多智能体协作，在电子病历中提取信息，且性能接近有监督模型。


<details>
  <summary>Details</summary>
Motivation: 现有的临床命名实体识别依赖于昂贵的标注数据，而大型语言模型的零样本方法在粒度和整合方面存在问题。

Method: OEMA框架包含三个部分：自标注器生成样本，判别器通过SNOMED CT进行过滤，预测器使用实体描述进行精确推断。

Result: 在MTSamples和VAERS数据集上，OEMA实现了最先进的精确匹配性能，并在相关匹配方面与有监督的BioClinicalBERT相匹配，超过了CRF。

Conclusion: OEMA通过本体指导的推理和多智能体协作，解决了零样本NER的关键挑战，实现了接近有监督的性能，并在临床NLP应用中显示出潜力。

Abstract: Clinical named entity recognition (NER) is crucial for extracting information from electronic health records (EHRs), but supervised models like CRF and BioClinicalBERT require costly annotated data. While zero-shot NER with large language models (LLMs) reduces this dependency, it struggles with example selection granularity and integrating prompts with self-improvement. To address this, we propose OEMA, a zero-shot clinical NER framework using multi-agent collaboration. OEMA's three components are: a self-annotator generating examples, a discriminator filtering them via SNOMED CT, and a predictor using entity descriptions for accurate inference. On MTSamples and VAERS datasets, OEMA achieves state-of-the-art exact-match performance. Under related-match, it matches supervised BioClinicalBERT and surpasses CRF. OEMA addresses key zero-shot NER challenges through ontology-guided reasoning and multi-agent collaboration, achieving near-supervised performance and showing promise for clinical NLP applications.

</details>


### [14] [Context Cascade Compression: Exploring the Upper Limits of Text Compression](https://arxiv.org/abs/2511.15244)
*Fanfan Liu,Haibo Qiu*

Main category: cs.CL

TL;DR: C3通过级联大小不同的LLM进行文本压缩和解码，小模型压缩上下文成长为潜在tokens，大模型在压缩上下文上执行解码任务。


<details>
  <summary>Details</summary>
Motivation: 长文本任务对LLM提出了计算和内存挑战，受DeepSeek-OCR的启发，探索文本压缩的上限。

Method: 使用两个不同大小的LLM，小LLM负责将长文本压缩为潜在tokens，大LLM负责在压缩后的上下文中执行解码任务。

Result: 在20倍压缩率下，模型解码精度达到98%，在40倍压缩率下，精度保持在93%左右。C3压缩性能优于光学字符压缩。

Conclusion: C3在文本压缩领域表现出优越的性能和可行性，为光学字符压缩等领域的未来工作提出了潜在的压缩率上限。

Abstract: Million-level token inputs in long-context tasks pose significant computational and memory challenges for Large Language Models (LLMs). Recently, DeepSeek-OCR conducted research into the feasibility of Contexts Optical Compression and achieved preliminary results. Inspired by this, we introduce Context Cascade Compression C3 to explore the upper limits of text compression. Our method cascades two LLMs of different sizes to handle the compression and decoding tasks. Specifically, a small LLM, acting as the first stage, performs text compression by condensing a long context into a set of latent tokens (e.g., 32 or 64 in length), achieving a high ratio of text tokens to latent tokens. A large LLM, as the second stage, then executes the decoding task on this compressed context. Experiments show that at a 20x compression ratio (where the number of text tokens is 20 times the number of latent tokens), our model achieves 98% decoding accuracy, compared to approximately 60% for DeepSeek-OCR. When we further increase the compression ratio to 40x, the accuracy is maintained at around 93%. This indicates that in the domain of context compression, C3 Compression demonstrates superior performance and feasibility over optical character compression. C3 uses a simpler, pure-text pipeline that ignores factors like layout, color, and information loss from a visual encoder. This also suggests a potential upper bound for compression ratios in future work on optical character compression, OCR, and related fields. Codes and model weights are publicly accessible at https://github.com/liufanfanlff/C3-Context-Cascade-Compression

</details>


### [15] [IndicGEC: Powerful Models, or a Measurement Mirage?](https://arxiv.org/abs/2511.15260)
*Sowmya Vajjala*

Main category: cs.CL

TL;DR: TeamNRC 使用不同大小的语言模型（4B 到大型专有模型）的 zero/few-shot 提示方法参加了 BHASHA-Task 1 语法纠错共享任务，在 Telugu 语料上排名第 4，在 Hindi 语料上排名第 2，GLEU 分数分别为 83.78 和 84.31。本文将实验扩展到共享任务的其他三种语言——泰米尔语、马拉雅拉姆语和孟加拉语，并仔细研究了所使用的数据质量和评估指标。


<details>
  <summary>Details</summary>
Motivation: 探索小型语言模型在印度语言语法纠错任务中的潜力。

Method: 使用 zero/few-shot 提示方法，利用不同大小的语言模型。

Result: 在 Telugu 语料上排名第 4，在 Hindi 语料上排名第 2，GLEU 分数分别为 83.78 和 84.31。强调了小型语言模型的潜力。

Conclusion: 总结了与创建高质量数据集和适用于印度语言脚本的适当指标相关的担忧。

Abstract: In this paper, we report the results of the TeamNRC's participation in the BHASHA-Task 1 Grammatical Error Correction shared task https://github.com/BHASHA-Workshop/IndicGEC2025/ for 5 Indian languages. Our approach, focusing on zero/few-shot prompting of language models of varying sizes (4B to large proprietary models) achieved a Rank 4 in Telugu and Rank 2 in Hindi with GLEU scores of 83.78 and 84.31 respectively. In this paper, we extend the experiments to the other three languages of the shared task - Tamil, Malayalam and Bangla, and take a closer look at the data quality and evaluation metric used. Our results primarily highlight the potential of small language models, and summarize the concerns related to creating good quality datasets and appropriate metrics for this task that are suitable for Indian language scripts.

</details>


### [16] [MAPROC at AHaSIS Shared Task: Few-Shot and Sentence Transformer for Sentiment Analysis of Arabic Hotel Reviews](https://arxiv.org/abs/2511.15291)
*Randa Zarnoufi*

Main category: cs.CL

TL;DR: 本研究利用少量样本学习来解决阿拉伯语方言情感分析中数据稀缺的问题，特别是在酒店评论领域。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语方言的情感分析由于语言多样性和标注数据的稀缺性而面临巨大的挑战。

Method: 我们采用了SetFit框架，一种数据高效的少量样本学习技术。

Result: 在官方评估集上，我们的系统达到了73%的F1值，在26个参与者中排名第12。

Conclusion: 这项工作强调了少量样本学习在处理酒店评论等专业领域中细致的阿拉伯语方言文本方面具有潜力。

Abstract: Sentiment analysis of Arabic dialects presents significant challenges due to linguistic diversity and the scarcity of annotated data. This paper describes our approach to the AHaSIS shared task, which focuses on sentiment analysis on Arabic dialects in the hospitality domain. The dataset comprises hotel reviews written in Moroccan and Saudi dialects, and the objective is to classify the reviewers sentiment as positive, negative, or neutral. We employed the SetFit (Sentence Transformer Fine-tuning) framework, a data-efficient few-shot learning technique. On the official evaluation set, our system achieved an F1 of 73%, ranking 12th among 26 participants. This work highlights the potential of few-shot learning to address data scarcity in processing nuanced dialectal Arabic text within specialized domains like hotel reviews.

</details>


### [17] [Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models](https://arxiv.org/abs/2511.15304)
*Piercosma Bisconti,Matteo Prandi,Federico Pierucci,Francesco Giarrusso,Marcantonio Bracale,Marcello Galisai,Vincenzo Suriani,Olga Sorokoletova,Federico Sartore,Daniele Nardi*

Main category: cs.CL

TL;DR: 对抗性诗歌是大型语言模型（LLM）的一种通用单轮越狱技术。


<details>
  <summary>Details</summary>
Motivation: 研究表明，即使是风格上的变化也能绕过现有的安全机制，这表明当前对齐方法和评估协议存在根本局限性。

Method: 通过精心设计的诗歌提示和使用标准化元提示将有害提示转换为诗歌，来评估攻击成功率。

Result: 诗歌框架的平均越狱成功率，对于手工制作的诗歌为 62%，对于元提示转换的诗歌约为 43%，远高于非诗歌基线。

Conclusion: 诗歌形式可以有效绕过LLM的安全机制，揭示了模型家族和安全训练方法中存在的系统性漏洞。

Abstract: We present evidence that adversarial poetry functions as a universal single-turn jailbreak technique for large language models (LLMs). Across 25 frontier proprietary and open-weight models, curated poetic prompts yielded high attack-success rates (ASR), with some providers exceeding 90%. Mapping prompts to MLCommons and EU CoP risk taxonomies shows that poetic attacks transfer across CBRN, manipulation, cyber-offence, and loss-of-control domains. Converting 1,200 MLCommons harmful prompts into verse via a standardized meta-prompt produced ASRs up to 18 times higher than their prose baselines. Outputs are evaluated using an ensemble of open-weight judge models and a human-validated stratified subset (with double-annotations to measure agreement). Disagreements were manually resolved. Poetic framing achieved an average jailbreak success rate of 62% for hand-crafted poems and approximately 43% for meta-prompt conversions (compared to non-poetic baselines), substantially outperforming non-poetic baselines and revealing a systematic vulnerability across model families and safety training approaches. These findings demonstrate that stylistic variation alone can circumvent contemporary safety mechanisms, suggesting fundamental limitations in current alignment methods and evaluation protocols.

</details>


### [18] [HEAD-QA v2: Expanding a Healthcare Benchmark for Reasoning](https://arxiv.org/abs/2511.15355)
*Alexis Correa-Guillén,Carlos Gómez-Rodríguez,David Vilares*

Main category: cs.CL

TL;DR: HEAD-QA v2 是一个扩展和更新的西班牙语/英语医疗多项选择题推理数据集。


<details>
  <summary>Details</summary>
Motivation: 为了满足对捕捉医疗推理的语言和概念复杂性的高质量数据集日益增长的需求。

Method: 通过提示、RAG 和基于概率的答案选择，对几个开源 LLM 进行了基准测试。

Result: 结果表明，性能主要由模型规模和内在推理能力驱动，复杂的推理策略获得的收益有限。

Conclusion: HEAD-QA v2 是一个可靠的资源，可用于推进生物医学推理和模型改进的研究。

Abstract: We introduce HEAD-QA v2, an expanded and updated version of a Spanish/English healthcare multiple-choice reasoning dataset originally released by Vilares and Gómez-Rodríguez (2019). The update responds to the growing need for high-quality datasets that capture the linguistic and conceptual complexity of healthcare reasoning. We extend the dataset to over 12,000 questions from ten years of Spanish professional exams, benchmark several open-source LLMs using prompting, RAG, and probability-based answer selection, and provide additional multilingual versions to support future work. Results indicate that performance is mainly driven by model scale and intrinsic reasoning ability, with complex inference strategies obtaining limited gains. Together, these results establish HEAD-QA v2 as a reliable resource for advancing research on biomedical reasoning and model improvement.

</details>


### [19] [The Empowerment of Science of Science by Large Language Models: New Tools and Methods](https://arxiv.org/abs/2511.15370)
*Guoqiang Liang,Jingqian Gong,Mengxuan Li,Gege Lin,Shuo Zhang*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在自然语言理解和生成、图像识别和多模态任务中表现出卓越的能力，正在成为全球技术竞争的核心。本文从用户的角度全面回顾了支持LLMs的核心技术，包括prompt工程、知识增强检索增强生成、微调、预训练和工具学习。此外，本文还追溯了科学学（SciSci）的历史发展，并对LLMs在科学计量学领域的潜在应用提出了前瞻性的观点。此外，本文还讨论了基于AI代理的科学评估模型的前景，并提出了利用LLMs进行新研究前沿检测和知识图构建的方法。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型（LLMs）在科学研究中的应用，尤其是在科学计量学领域。

Method: 综述了LLMs的核心技术，并结合科学学（SciSci）的历史发展，提出了LLMs在科学计量学中应用的前景。

Result: 提出了基于AI代理的科学评估模型，以及利用LLMs进行新研究前沿检测和知识图构建的方法。

Conclusion: 本文全面回顾了LLMs的核心技术，并对LLMs在科学计量学领域的潜在应用提出了前瞻性的观点，为未来的研究提供了方向。

Abstract: Large language models (LLMs) have exhibited exceptional capabilities in natural language understanding and generation, image recognition, and multimodal tasks, charting a course towards AGI and emerging as a central issue in the global technological race. This manuscript conducts a comprehensive review of the core technologies that support LLMs from a user standpoint, including prompt engineering, knowledge-enhanced retrieval augmented generation, fine tuning, pretraining, and tool learning. Additionally, it traces the historical development of Science of Science (SciSci) and presents a forward looking perspective on the potential applications of LLMs within the scientometric domain. Furthermore, it discusses the prospect of an AI agent based model for scientific evaluation, and presents new research fronts detection and knowledge graph building methods with LLMs.

</details>


### [20] [A Compliance-Preserving Retrieval System for Aircraft MRO Task Search](https://arxiv.org/abs/2511.15383)
*Byungho Jo*

Main category: cs.CL

TL;DR: 该论文提出了一种合规的检索系统，通过LLM重排序和语义搜索，提高飞机维修技师(AMT)的工作效率，减少查阅手册的时间。


<details>
  <summary>Details</summary>
Motivation: 飞机维修技师需要花费大量时间查阅手册，这是一个效率瓶颈。该研究旨在解决这个问题。

Method: 该系统构建了来自ATA章节层次结构的修订鲁棒嵌入，并使用视觉语言解析来构建认证内容。

Result: 在49k个合成查询上的评估实现了>90%的检索准确率，双语对照研究表明，前10名的成功率为90.9%，查找时间减少了95%。

Conclusion: 语义检索可以在严格的监管约束下运行，并有意义地减少实际多语言MRO工作流程中的操作工作量。

Abstract: Aircraft Maintenance Technicians (AMTs) spend up to 30% of work time searching manuals, a documented efficiency bottleneck in MRO operations where every procedure must be traceable to certified sources. We present a compliance-preserving retrieval system that adapts LLM reranking and semantic search to aviation MRO environments by operating alongside, rather than replacing, certified legacy viewers. The system constructs revision-robust embeddings from ATA chapter hierarchies and uses vision-language parsing to structure certified content, allowing technicians to preview ranked tasks and access verified procedures in existing viewers. Evaluation on 49k synthetic queries achieves >90% retrieval accuracy, while bilingual controlled studies with 10 licensed AMTs demonstrate 90.9% top-10 success rate and 95% reduction in lookup time, from 6-15 minutes to 18 seconds per task. These gains provide concrete evidence that semantic retrieval can operate within strict regulatory constraints and meaningfully reduce operational workload in real-world multilingual MRO workflows.

</details>


### [21] [NAMeGEn: Creative Name Generation via A Novel Agent-based Multiple Personalized Goal Enhancement Framework](https://arxiv.org/abs/2511.15408)
*Shanlin Zhou,Xinpeng Wang,Jianxun Lian,Zhenghao Liu,Laks V. S. Lakshmanan,Xiaoyuan Yi,Yongtao Hao*

Main category: cs.CL

TL;DR: 大型语言模型在创意自然语言生成（CNLG）方面具有潜力，但在满足用户个性化需求和理解隐式含义方面仍面临挑战。本文针对中文婴儿命名这一短文本CNLG任务，提出了一个多代理优化框架NAMeGEn，并通过构建诗歌语料库和基准数据集CBNames来支持该任务。实验表明，NAMeGEn能够生成满足个性化需求且具有解释性的创意名称。


<details>
  <summary>Details</summary>
Motivation: 现有的CNLG方法在生成创意和深刻的内容方面存在局限性，尤其是在短文本生成方面。这是因为用户需求通常是个性化的、细粒度的和多元化的，同时创造力也涉及理解和解释隐含意义。

Method: 提出了一个多代理优化框架NAMeGEn，该框架迭代地在目标提取、名称生成和评估之间交替，以满足不同的需求并生成准确的解释。此外，还构建了一个包含17k+诗歌的古典中文诗歌语料库，并引入了一个新的基准数据集CBNames，其中包含定制的指标。

Result: NAMeGEn能够有效地生成满足各种个性化需求的创意名称，同时提供有意义的解释，并且在没有任何训练的情况下，优于跨越各种LLM主干的六个基线方法。

Conclusion: 本文提出了NAMeGEn框架，用于解决短文本创意自然语言生成中的多目标灵活性和解释复杂性挑战。实验结果表明，该框架在中文婴儿命名任务中表现出色。

Abstract: Trained on diverse human-authored texts, Large Language Models (LLMs) unlocked the potential for Creative Natural Language Generation (CNLG), benefiting various applications like advertising and storytelling. Nevertheless, CNLG still remains difficult due to two main challenges. (1) Multi-objective flexibility: user requirements are often personalized, fine-grained, and pluralistic, which LLMs struggle to satisfy simultaneously; (2) Interpretive complexity: beyond generation, creativity also involves understanding and interpreting implicit meaning to enhance users' perception. These challenges significantly limit current methods, especially in short-form text generation, in generating creative and insightful content. To address this, we focus on Chinese baby naming, a representative short-form CNLG task requiring adherence to explicit user constraints (e.g., length, semantics, anthroponymy) while offering meaningful aesthetic explanations. We propose NAMeGEn, a novel multi-agent optimization framework that iteratively alternates between objective extraction, name generation, and evaluation to meet diverse requirements and generate accurate explanations. To support this task, we further construct a classical Chinese poetry corpus with 17k+ poems to enhance aesthetics, and introduce CBNames, a new benchmark with tailored metrics. Extensive experiments demonstrate that NAMeGEn effectively generates creative names that meet diverse, personalized requirements while providing meaningful explanations, outperforming six baseline methods spanning various LLM backbones without any training.

</details>


### [22] [DEPO: Dual-Efficiency Preference Optimization for LLM Agents](https://arxiv.org/abs/2511.15392)
*Sirui Chen,Mengshi Zhao,Lei Xu,Yuying Zhao,Beier Zhu,Hanwang Zhang,Shengjie Zhao,Chaochao Lu*

Main category: cs.CL

TL;DR: 本文提出了一种名为DEPO的双重效率优化方法，旨在提高大型语言模型（LLM）代理在现实场景中的交互效率，通过减少token使用量和步骤数，同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在推理和决策能力方面取得了显著进展，但更丰富的推理通常需要更长的思维链（CoT），从而降低了实际应用中的交互效率。目前缺乏对LLM代理效率的系统定义，阻碍了有针对性的改进。

Method: 提出了双重效率的概念，包括最小化每步token的步级效率和最小化完成任务所需步数的轨迹级效率，并在此基础上提出了双重效率偏好优化方法DEPO，该方法同时奖励简洁的回复和更少的操作步骤。

Result: 在WebShop和BabyAI上的实验表明，DEPO可以将token使用量减少高达60.9%，步骤数减少高达26.9%，同时性能提升高达29.3%。DEPO还推广到三个领域外的数学基准测试，并且在仅使用25%的数据进行训练时仍能保持其效率优势。

Conclusion: DEPO方法有效地提高了LLM代理的效率，减少了token使用量和步骤数，同时保持甚至提升了性能，并且具有良好的泛化能力。

Abstract: Recent advances in large language models (LLMs) have greatly improved their reasoning and decision-making abilities when deployed as agents. Richer reasoning, however, often comes at the cost of longer chain of thought (CoT), hampering interaction efficiency in real-world scenarios. Nevertheless, there still lacks systematic definition of LLM agent efficiency, hindering targeted improvements. To this end, we introduce dual-efficiency, comprising (i) step-level efficiency, which minimizes tokens per step, and (ii) trajectory-level efficiency, which minimizes the number of steps to complete a task. Building on this definition, we propose DEPO, a dual-efficiency preference optimization method that jointly rewards succinct responses and fewer action steps. Experiments on WebShop and BabyAI show that DEPO cuts token usage by up to 60.9% and steps by up to 26.9%, while achieving up to a 29.3% improvement in performance. DEPO also generalizes to three out-of-domain math benchmarks and retains its efficiency gains when trained on only 25% of the data. Our project page is at https://opencausalab.github.io/DEPO.

</details>


### [23] [Building Robust and Scalable Multilingual ASR for Indian Languages](https://arxiv.org/abs/2511.15418)
*Arjun Gangwar,Kaousheik Jayakumar,S. Umesh*

Main category: cs.CL

TL;DR: SPRING Lab, IIT Madras, participated in ASRU MADASR 2.0 challenge, focusing on multilingual ASR with language and dialect identification.


<details>
  <summary>Details</summary>
Motivation: Improving ASR systems to predict language and dialect of utterances across 8 languages and 33 dialects.

Method: Multi-Decoder architecture with phonemic Common Label Set (CLS) as intermediate representation.

Result: Beat the baseline in 3 languages (Track 2) in terms of WER/CER and achieved the highest language ID and dialect ID accuracy (Track 2).

Conclusion: The proposed system with multi-decoder architecture and CLS improves ASR performance for multilingual speech with language and dialect identification.

Abstract: This paper describes the systems developed by SPRING Lab, Indian Institute of Technology Madras, for the ASRU MADASR 2.0 challenge. The systems developed focuses on adapting ASR systems to improve in predicting the language and dialect of the utterance among 8 languages across 33 dialects. We participated in Track 1 and Track 2, which restricts the use of additional data and develop from-the-scratch multilingual systems. We presented a novel training approach using Multi-Decoder architecture with phonemic Common Label Set (CLS) as intermediate representation. It improved the performance over the baseline (in the CLS space). We also discuss various methods used to retain the gain obtained in the phonemic space while converting them back to the corresponding grapheme representations. Our systems beat the baseline in 3 languages (Track 2) in terms of WER/CER and achieved the highest language ID and dialect ID accuracy among all participating teams (Track 2).

</details>


### [24] [LLM-MemCluster: Empowering Large Language Models with Dynamic Memory for Text Clustering](https://arxiv.org/abs/2511.15424)
*Yuanjie Zhu,Liangwei Yang,Ke Xu,Weizhi Zhang,Zihe Song,Jindong Wang,Philip S. Yu*

Main category: cs.CL

TL;DR: LLM-MemCluster: An end-to-end LLM framework for text clustering.


<details>
  <summary>Details</summary>
Motivation: Existing LLM text clustering methods lack stateful memory, struggle with cluster granularity, and rely on complex pipelines.

Method: Introduces Dynamic Memory for state awareness and a Dual-Prompt Strategy to determine cluster number.

Result: Significantly outperforms strong baselines on benchmark datasets.

Conclusion: Presents an effective, interpretable, and truly end-to-end paradigm for LLM-based text clustering.

Abstract: Large Language Models (LLMs) are reshaping unsupervised learning by offering an unprecedented ability to perform text clustering based on their deep semantic understanding. However, their direct application is fundamentally limited by a lack of stateful memory for iterative refinement and the difficulty of managing cluster granularity. As a result, existing methods often rely on complex pipelines with external modules, sacrificing a truly end-to-end approach. We introduce LLM-MemCluster, a novel framework that reconceptualizes clustering as a fully LLM-native task. It leverages a Dynamic Memory to instill state awareness and a Dual-Prompt Strategy to enable the model to reason about and determine the number of clusters. Evaluated on several benchmark datasets, our tuning-free framework significantly and consistently outperforms strong baselines. LLM-MemCluster presents an effective, interpretable, and truly end-to-end paradigm for LLM-based text clustering.

</details>


### [25] [Standardising the NLP Workflow: A Framework for Reproducible Linguistic Analysis](https://arxiv.org/abs/2511.15512)
*Yves Pauli,Jan-Bernard Marsman,Finn Rabe,Victoria Edkins,Roya Hüppi,Silvia Ciampelli,Akhil Ratan Misra,Nils Lang,Wolfram Hinzen,Iris Sommer,Philipp Homan*

Main category: cs.CL

TL;DR: 本文提出了一种新的语言数据处理框架，旨在提高语言研究的可重复性和标准化。


<details>
  <summary>Details</summary>
Motivation: 缺乏语言数据组织和共享的标准，以及缺乏可重复的处理方法。

Method: 提出了语言处理数据结构（LPDS）和Python包pelican nlp。

Result: LPDS为语言研究提供了一种文件夹结构和文件命名规范。Pelican nlp旨在简化语言处理，从初始数据清洗和特定于任务的预处理到提取复杂的语言和声学特征，例如语义嵌入和韵律指标。整个处理流程可以在单个可共享的配置文件中指定，然后pelican nlp在LPDS格式的数据上执行该文件。根据规范，可重现的输出可以包括预处理的语言数据或语言和声学特征的标准化提取以及相应的结果聚合。

Conclusion: LPDS和pelican nlp共同为语言数据提供了一个端到端的处理管道，旨在确保方法透明度并提高可重复性。

Abstract: The introduction of large language models and other influential developments in AI-based language processing have led to an evolution in the methods available to quantitatively analyse language data. With the resultant growth of attention on language processing, significant challenges have emerged, including the lack of standardisation in organising and sharing linguistic data and the absence of standardised and reproducible processing methodologies. Striving for future standardisation, we first propose the Language Processing Data Structure (LPDS), a data structure inspired by the Brain Imaging Data Structure (BIDS), a widely adopted standard for handling neuroscience data. It provides a folder structure and file naming conventions for linguistic research. Second, we introduce pelican nlp, a modular and extensible Python package designed to enable streamlined language processing, from initial data cleaning and task-specific preprocessing to the extraction of sophisticated linguistic and acoustic features, such as semantic embeddings and prosodic metrics. The entire processing workflow can be specified within a single, shareable configuration file, which pelican nlp then executes on LPDS-formatted data. Depending on the specifications, the reproducible output can consist of preprocessed language data or standardised extraction of both linguistic and acoustic features and corresponding result aggregations. LPDS and pelican nlp collectively offer an end-to-end processing pipeline for linguistic data, designed to ensure methodological transparency and enhance reproducibility.

</details>


### [26] [Multimodal Evaluation of Russian-language Architectures](https://arxiv.org/abs/2511.15552)
*Artem Chervyakov,Ulyana Isaeva,Anton Emelyanov,Artem Safin,Maria Tikhonova,Alexander Kharitonov,Yulia Lyakh,Petr Surovtsev,Denis Shevelev Vildan Saburov,Vasily Konovalov,Elisei Rykov,Ivan Sviridov,Amina Miftakhova,Ilseyar Alimova,Alexander Panchenko,Alexander Kapitanov,Alena Fenogenova*

Main category: cs.CL

TL;DR: Mera Multi是一个用于评估俄语多模态大型语言模型（MLLM）的开源框架，包含18个新构建的评估任务，涵盖文本、图像、音频和视频模态。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大型语言模型的研究备受关注，但在俄语环境下缺乏多模态基准测试，其智能、局限性和风险尚未被充分理解。

Method: 构建了一个基于指令的多模态评估框架，包含18个从零开始构建的数据集，并针对俄语文化和语言的特殊性进行了设计，统一了提示和指标。此外，还提出了一种防止基准泄露的方法，包括水印和私有集的许可。

Result: 为通用模型和特定模态架构提供了基线结果。

Conclusion: Mera Multi为在类型多样的语言（特别是斯拉夫语族）中构建多模态基准测试提供了一种可复制的方法。

Abstract: Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.

</details>


### [27] [HSKBenchmark: Modeling and Benchmarking Chinese Second Language Acquisition in Large Language Models through Curriculum Tuning](https://arxiv.org/abs/2511.15574)
*Qihao Yang,Xuelin Wang,Jiale Chen,Xuelian Dong,Yuxin Hao,Tianyong Hao*

Main category: cs.CL

TL;DR: 提出了HSKBenchmark，用于评估LLM在汉语作为第二语言（SLA）学习中的阶段性建模和写作能力。


<details>
  <summary>Details</summary>
Motivation: 人工控制人类学习者的语言输入在伦理和实践上不可行，阻碍了语言习得模型的可验证性和可扩展性，特别是在汉语作为第二语言（SLA）学习中。大型语言模型（LLM）虽然提供了一种可控和可重复的替代方案，但缺乏一个系统的基准来支持阶段性建模和评估。

Method: 构建了HSKBenchmark，包含HSK3-6级的教材、合成指令样本、测试题目和语言学评估系统。引入课程调整框架，训练模型从初级到高级水平。创建评估系统，检查语法覆盖率、写作错误、词汇和句法复杂性以及整体评分。构建了HSKAgent，在学习者作文上进行微调。

Result: 实验结果表明，HSKBenchmark有效地模拟了汉语作为第二语言（SLA）的学习，并作为LLM动态写作评估的可靠基准。微调后的LLM写作水平与高级人类学习者相当，并表现出类似人类的学习特征。

Conclusion: HSKBenchmark、HSKAgent和检查点作为基础工具和资源，为未来的语言习得建模和LLM可解释性研究奠定了基础。

Abstract: Language acquisition is vital to revealing the nature of human language intelligence and has recently emerged as a promising perspective for improving the interpretability of large language models (LLMs). However, it is ethically and practically infeasible to conduct experiments that require controlling human learners' language inputs. This poses challenges for the verifiability and scalability of language acquisition modeling, particularly in Chinese second language acquisition (SLA). While LLMs provide a controllable and reproducible alternative, a systematic benchmark to support phase-wise modeling and assessment is still lacking. In this paper, we present HSKBenchmark, the first benchmark for staged modeling and writing assessment of LLMs in Chinese SLA. It covers HSK levels 3 to 6 and includes authentic textbooks with 6.76 million tokens, 16K synthetic instruction samples, 30 test topics, and a linguistically grounded evaluation system. To simulate human learning trajectories, we introduce a curriculum-tuning framework that trains models from beginner to advanced levels. An evaluation system is created to examine level-based grammar coverage, writing errors, lexical and syntactic complexity, and holistic scoring. We also build HSKAgent, fine-tuned on 10K learner compositions. Extensive experimental results demonstrate that HSKBenchmark not only models Chinese SLA effectively, but also serves as a reliable benchmark for dynamic writing assessment in LLMs. Our fine-tuned LLMs have writing performance on par with advanced human learners and exhibit human-like acquisition characteristics. The HSKBenchmark, HSKAgent, and checkpoints serve as foundational tools and resources, with the potential to pave the way for future research on language acquisition modeling and LLMs interpretability. Code and data are publicly available at: https://github.com/CharlesYang030/HSKB.

</details>


### [28] [Tokenisation over Bounded Alphabets is Hard](https://arxiv.org/abs/2511.15709)
*Violeta Kastreva,Philip Whittington,Dennis Komm,Tiago Pimentel*

Main category: cs.CL

TL;DR: Tokenisation is NP-complete even with binary alphabets, and direct tokenisation remains NP-complete even when applied to unary alphabets.


<details>
  <summary>Details</summary>
Motivation: Previous works assume tokenisation is applied to inputs with unboundedly large alphabets, which is unrealistic. This paper analyzes tokenisation over bounded alphabets.

Method: Analyzes bottom-up tokenisation and direct tokenisation over bounded alphabets, proving hardness results for binary alphabets and unary alphabets.

Result: Both variants are NP-complete and admit no polynomial-time approximation scheme even with binary alphabets. Direct tokenisation remains NP-complete even when applied to unary alphabets.

Conclusion: The computational intractability of tokenisation is a fundamental barrier, explaining why practical algorithms are heuristic and suggesting approximation algorithms as an important path forward.

Abstract: Recent works have shown that tokenisation is NP-complete. However, these works assume tokenisation is applied to inputs with unboundedly large alphabets -- an unrealistic assumption, given that in practice tokenisers operate over fixed-size alphabets, such as bytes or Unicode characters. We close this gap by analysing tokenisation over bounded $n$-ary alphabets, considering two natural variants: bottom-up tokenisation and direct tokenisation, where we must, respectively, select a sequence of merge operations or a vocabulary whose application optimally compresses a dataset. First, we note that proving hardness results for an $n$-ary alphabet proves the same results for alphabets of any larger size. We then prove that even with binary alphabets, both variants are not only NP-complete, but admit no polynomial-time approximation scheme (unless P=NP). We further show that direct tokenisation remains NP-complete even when applied to unary alphabets. While unary alphabets may not be practically useful, this result establishes that the computational intractability of tokenisation is not an artifact of large alphabets or complex constructions, but a fundamental barrier. Overall, our results explain why practical algorithms such as BPE and UnigramLM are heuristic, and points toward approximation algorithms being an important path going forward for tokenisation research.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [29] [Gaussian See, Gaussian Do: Semantic 3D Motion Transfer from Multiview Video](https://arxiv.org/abs/2511.14848)
*Yarin Bekor,Gal Michael Harari,Or Perel,Or Litany*

Main category: cs.CV

TL;DR: 提出了一种新的方法，用于从多视图视频进行语义 3D 运动迁移，称为高斯观察，高斯执行。


<details>
  <summary>Details</summary>
Motivation: 实现具有语义意义对应的对象之间的无装备、跨类别运动迁移。

Method: 从源视频中提取运动嵌入，将它们应用于静态目标形状的渲染帧，并使用生成的视频来监督动态 3D 高斯溅射重建。

Result: 建立了第一个语义 3D 运动迁移的基准，并证明了与调整后的基线相比，具有卓越的运动保真度和结构一致性。

Conclusion: 该方法引入了一种基于锚点的视图感知运动嵌入机制，确保了跨视图一致性并加速了收敛，以及一个强大的 4D 重建管道，该管道整合了嘈杂的监督视频。

Abstract: We present Gaussian See, Gaussian Do, a novel approach for semantic 3D motion transfer from multiview video. Our method enables rig-free, cross-category motion transfer between objects with semantically meaningful correspondence. Building on implicit motion transfer techniques, we extract motion embeddings from source videos via condition inversion, apply them to rendered frames of static target shapes, and use the resulting videos to supervise dynamic 3D Gaussian Splatting reconstruction. Our approach introduces an anchor-based view-aware motion embedding mechanism, ensuring cross-view consistency and accelerating convergence, along with a robust 4D reconstruction pipeline that consolidates noisy supervision videos. We establish the first benchmark for semantic 3D motion transfer and demonstrate superior motion fidelity and structural consistency compared to adapted baselines. Code and data for this paper available at https://gsgd-motiontransfer.github.io/

</details>


### [30] [When CNNs Outperform Transformers and Mambas: Revisiting Deep Architectures for Dental Caries Segmentation](https://arxiv.org/abs/2511.14860)
*Aashish Ghimire,Jun Zeng,Roshan Paudel,Nikhil Kumar Tomar,Deepak Ranjan Nayak,Harshith Reddy Nalla,Vivek Jha,Glenda Reynolds,Debesh Jha*

Main category: cs.CV

TL;DR: 本文对全景X光片中龋齿的自动分割进行了全面的基准测试，使用了DC1000数据集，比较了包括CNN、Vision Transformer和Mamba在内的12种先进架构。


<details>
  <summary>Details</summary>
Motivation: 在全景X光片中准确识别和分割龋齿对于早期诊断和有效治疗计划至关重要，但由于病变对比度低、形态变异性和有限的带注释数据，自动分割仍然具有挑战性。

Method: 本文在DC1000数据集上，对包括VMUnet、MambaUNet、VMUNetv2、RMAMamba-S、TransNetR、PVTFormer、DoubleU-Net和ResUNet++在内的12种先进架构进行了训练。

Result: 结果表明，基于CNN的DoubleU-Net取得了最高的Dice系数0.7345、mIoU 0.5978和精确率0.8145，优于所有Transformer和Mamba变体。所有性能指标的前3名均由基于CNN的架构实现。

Conclusion: 研究结果强调了领域特定医学图像分割中架构与任务对齐的重要性，而不是模型复杂性。

Abstract: Accurate identification and segmentation of dental caries in panoramic radiographs are critical for early diagnosis and effective treatment planning. Automated segmentation remains challenging due to low lesion contrast, morphological variability, and limited annotated data. In this study, we present the first comprehensive benchmarking of convolutional neural networks, vision transformers and state-space mamba architectures for automated dental caries segmentation on panoramic radiographs through a DC1000 dataset. Twelve state-of-the-art architectures, including VMUnet, MambaUNet, VMUNetv2, RMAMamba-S, TransNetR, PVTFormer, DoubleU-Net, and ResUNet++, were trained under identical configurations. Results reveal that, contrary to the growing trend toward complex attention based architectures, the CNN-based DoubleU-Net achieved the highest dice coefficient of 0.7345, mIoU of 0.5978, and precision of 0.8145, outperforming all transformer and Mamba variants. In the study, the top 3 results across all performance metrics were achieved by CNN-based architectures. Here, Mamba and transformer-based methods, despite their theoretical advantage in global context modeling, underperformed due to limited data and weaker spatial priors. These findings underscore the importance of architecture-task alignment in domain-specific medical image segmentation more than model complexity. Our code is available at: https://github.com/JunZengz/dental-caries-segmentation.

</details>


### [31] [B-Rep Distance Functions (BR-DF): How to Represent a B-Rep Model by Volumetric Distance Functions?](https://arxiv.org/abs/2511.14870)
*Fuyang Zhang,Pradeep Kumar Jayaraman,Xiang Xu,Yasutaka Furukawa*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于体积距离函数的CAD边界表示几何表示方法，称为B-Rep距离函数(BR-DF)。


<details>
  <summary>Details</summary>
Motivation: 为了解决CAD模型生成成功率低的问题。

Method: 该方法将CAD模型的表面网格几何体编码为有符号距离函数(SDF)，并将B-Rep顶点、边、面及其拓扑信息编码为每个面的无符号距离函数(UDF)。

Result: 该方法实现了与SOTA方法相当的CAD生成性能，并在生成(faceted)B-Rep模型方面达到了前所未有的100%成功率。

Conclusion: BR-DF 可以直接转换为 CAD B-Rep 模型，且转换过程永远不会失败。

Abstract: This paper presents a novel geometric representation for CAD Boundary Representation (B-Rep) based on volumetric distance functions, dubbed B-Rep Distance Functions (BR-DF). BR-DF encodes the surface mesh geometry of a CAD model as signed distance function (SDF). B-Rep vertices, edges, faces and their topology information are encoded as per-face unsigned distance functions (UDFs). An extension of the Marching Cubes algorithm converts BR-DF directly into watertight CAD B-Rep model (strictly speaking a faceted B-Rep model). A surprising characteristic of BR-DF is that this conversion process never fails. Leveraging the volumetric nature of BR-DF, we propose a multi-branch latent diffusion with 3D U-Net backbone for jointly generating the SDF and per-face UDFs of a BR-DF model. Our approach achieves comparable CAD generation performance against SOTA methods while reaching the unprecedented 100% success rate in producing (faceted) B-Rep models.

</details>


### [32] [GeoSceneGraph: Geometric Scene Graph Diffusion Model for Text-guided 3D Indoor Scene Synthesis](https://arxiv.org/abs/2511.14884)
*Antonio Ruiz,Tao Wu,Andrew Melnik,Qing Cheng,Xuqin Wang,Lu Liu,Yongliang Wang,Yanfeng Zhang,Helge Ritter*

Main category: cs.CV

TL;DR: GeoSceneGraph 通过利用图结构和 3D 场景的几何对称性，从文本提示合成 3D 场景，无需依赖预定义的关系类别。


<details>
  <summary>Details</summary>
Motivation: 现有的从头开始训练的生成方法忽略了室内场景固有的图结构，这会限制场景的连贯性和真实感。另一方面，结合场景图的方法要么需要用户提供的语义图，要么依赖于 ground-truth 关系注释，限制了它们捕获更多样化的对象交互的能力。

Method: 该模型建立在等变图神经网络 (EGNN) 上，并提出了一种简单有效的策略，用于在文本特征上调节 EGNN。

Result: GeoSceneGraph 在不使用 ground-truth 关系的情况下，实现了与使用 ground-truth 关系的方法相当的性能。

Conclusion: 该论文提出了一种新的 3D 场景合成方法，该方法利用了场景图的结构和几何信息，并在性能上取得了有竞争力的结果，同时避免了对预定义关系或人工语义图的依赖。

Abstract: Methods that synthesize indoor 3D scenes from text prompts have wide-ranging applications in film production, interior design, video games, virtual reality, and synthetic data generation for training embodied agents. Existing approaches typically either train generative models from scratch or leverage vision-language models (VLMs). While VLMs achieve strong performance, particularly for complex or open-ended prompts, smaller task-specific models remain necessary for deployment on resource-constrained devices such as extended reality (XR) glasses or mobile phones. However, many generative approaches that train from scratch overlook the inherent graph structure of indoor scenes, which can limit scene coherence and realism. Conversely, methods that incorporate scene graphs either demand a user-provided semantic graph, which is generally inconvenient and restrictive, or rely on ground-truth relationship annotations, limiting their capacity to capture more varied object interactions. To address these challenges, we introduce GeoSceneGraph, a method that synthesizes 3D scenes from text prompts by leveraging the graph structure and geometric symmetries of 3D scenes, without relying on predefined relationship classes. Despite not using ground-truth relationships, GeoSceneGraph achieves performance comparable to methods that do. Our model is built on equivariant graph neural networks (EGNNs), but existing EGNN approaches are typically limited to low-dimensional conditioning and are not designed to handle complex modalities such as text. We propose a simple and effective strategy for conditioning EGNNs on text features, and we validate our design through ablation studies.

</details>


### [33] [HULFSynth : An INR based Super-Resolution and Ultra Low-Field MRI Synthesis via Contrast factor estimation](https://arxiv.org/abs/2511.14897)
*Pranav Indrakanti,Ivor Simpson*

Main category: cs.CV

TL;DR: 提出了一种无监督单张图像双向磁共振图像（MRI）合成器，可以从高场（HF）幅度图像合成超低场（ULF）图像，反之亦然。


<details>
  <summary>Details</summary>
Motivation: 与现有的MRI合成模型不同，我们的方法受到驱动HF和ULF MRI之间对比度变化的物理原理的启发。

Method: 我们的前向模型通过估计基于目标对比度值的组织类型信噪比（SNR）值来模拟HF到ULF的转换。对于超分辨率任务，我们使用隐式神经表示（INR）网络，通过同时预测组织类型分割和图像强度来合成HF图像，而无需观察到的HF数据。

Result: 合成的类ULF图像的WM-GM对比度提高了52%，64mT图像的WM-GM对比度提高了37%。

Conclusion: 敏感性实验表明，我们的前向模型对目标对比度、噪声和初始种子变化的鲁棒性。

Abstract: We present an unsupervised single image bidirectional Magnetic Resonance Image (MRI) synthesizer that synthesizes an Ultra-Low Field (ULF) like image from a High-Field (HF) magnitude image and vice-versa. Unlike existing MRI synthesis models, our approach is inspired by the physics that drives contrast changes between HF and ULF MRIs. Our forward model simulates a HF to ULF transformation by estimating the tissue-type Signal-to-Noise ratio (SNR) values based on target contrast values. For the Super-Resolution task, we used an Implicit Neural Representation (INR) network to synthesize HF image by simultaneously predicting tissue-type segmentations and image intensity without observed HF data. The proposed method is evaluated using synthetic ULF-like data from generated from standard 3T T$_1$-weighted images for qualitative assessments and paired 3T-64mT T$_1$-weighted images for validation experiments. WM-GM contrast improved by 52% in synthetic ULF-like images and 37% in 64mT images. Sensitivity experiments demonstrated the robustness of our forward model to variations in target contrast, noise and initial seeding.

</details>


### [34] [InstructMix2Mix: Consistent Sparse-View Editing Through Multi-View Model Personalization](https://arxiv.org/abs/2511.14899)
*Daniel Gilo,Or Litany*

Main category: cs.CV

TL;DR: 提出了InstructMix2Mix (I-Mix2Mix)框架，用于从稀疏输入视图进行多视图图像编辑，根据文本指令修改场景，同时保持跨视图一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于场景神经场或时间注意力机制的方法难以在多视图图像编辑中保持一致性，并容易产生伪影和不连贯的编辑。

Method: 将2D扩散模型的编辑能力提炼到预训练的多视图扩散模型中，利用其数据驱动的3D先验来实现跨视图一致性。使用多视图扩散学生代替了Score Distillation Sampling (SDS)中的神经场 consolidator，并提出了增量学生更新、教师噪声调度和注意力修改等创新方法。

Result: 实验表明，I-Mix2Mix在保持高每帧编辑质量的同时，显著提高了多视图一致性。

Conclusion: I-Mix2Mix框架通过将2D扩散模型的编辑能力迁移到多视图扩散模型，有效提升了多视图图像编辑的一致性和质量。

Abstract: We address the task of multi-view image editing from sparse input views, where the inputs can be seen as a mix of images capturing the scene from different viewpoints. The goal is to modify the scene according to a textual instruction while preserving consistency across all views. Existing methods, based on per-scene neural fields or temporal attention mechanisms, struggle in this setting, often producing artifacts and incoherent edits. We propose InstructMix2Mix (I-Mix2Mix), a framework that distills the editing capabilities of a 2D diffusion model into a pretrained multi-view diffusion model, leveraging its data-driven 3D prior for cross-view consistency. A key contribution is replacing the conventional neural field consolidator in Score Distillation Sampling (SDS) with a multi-view diffusion student, which requires novel adaptations: incremental student updates across timesteps, a specialized teacher noise scheduler to prevent degeneration, and an attention modification that enhances cross-view coherence without additional cost. Experiments demonstrate that I-Mix2Mix significantly improves multi-view consistency while maintaining high per-frame edit quality.

</details>


### [35] [Skin-R1: Toward Trustworthy Clinical Reasoning for Dermatological Diagnosis](https://arxiv.org/abs/2511.14900)
*Zehao Liu,Wejieying Ren,Jipeng Zhang,Tianxiang Zhao,Jingxi Zhu,Xiaoting Li,Vasant G. Honavar*

Main category: cs.CV

TL;DR: SkinR1: A new dermatological VLM combining textbook-based reasoning with reinforcement learning (RL) to improve diagnostic accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs for dermatological diagnosis suffer from data heterogeneity, lack of grounded diagnostic rationales, and limited scalability.

Method: A dermatological VLM is proposed that combines deep, textbook-based reasoning with the broad generalization capabilities of reinforcement learning (RL).

Result: SkinR1 achieves superior diagnostic accuracy on multiple dermatology datasets.

Conclusion: The ablation study demonstrates the importance of the reasoning foundation instilled by SFT.

Abstract: The emergence of vision-language models (VLMs) has opened new possibilities for clinical reasoning and has shown promising performance in dermatological diagnosis. However, their trustworthiness and clinical utility are often limited by three major factors: (1) Data heterogeneity, where diverse datasets lack consistent diagnostic labels and clinical concept annotations; (2) Absence of grounded diagnostic rationales, leading to a scarcity of reliable reasoning supervision; and (3) Limited scalability and generalization, as models trained on small, densely annotated datasets struggle to transfer nuanced reasoning to large, sparsely-annotated ones.
  To address these limitations, we propose SkinR1, a novel dermatological VLM that combines deep, textbook-based reasoning with the broad generalization capabilities of reinforcement learning (RL). SkinR1 systematically resolves the key challenges through a unified, end-to-end framework. First, we design a textbook-based reasoning generator that synthesizes high-fidelity, hierarchy-aware, and differential-diagnosis (DDx)-informed trajectories, providing reliable expert-level supervision. Second, we leverage the constructed trajectories for supervised fine-tuning (SFT) empowering the model with grounded reasoning ability. Third, we develop a novel RL paradigm that, by incorporating the hierarchical structure of diseases, effectively transfers these grounded reasoning patterns to large-scale, sparse data. Extensive experiments on multiple dermatology datasets demonstrate that SkinR1 achieves superior diagnostic accuracy. The ablation study demonstrates the importance of the reasoning foundation instilled by SFT.

</details>


### [36] [FarSLIP: Discovering Effective CLIP Adaptation for Fine-Grained Remote Sensing Understanding](https://arxiv.org/abs/2511.14901)
*Zhenshi Li,Weikang Yu,Dilxat Muhtar,Xueliang Zhang,Pengfeng Xiao,Pedram Ghamisi,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: 这篇论文提出了一种名为 FarSLIP 的框架，用于改进遥感图像的细粒度视觉-语言对齐，并构建了一个新的多粒度遥感图像-文本数据集 MGRS-200k。


<details>
  <summary>Details</summary>
Motivation: 当前遥感领域的 CLIP 模型变体在空间感知方面存在局限性，因为它们继承了 CLIP 的全局对齐限制，并且现有的区域-文本对齐方法直接应用于遥感数据时性能下降。

Method: 该方法使用 patch-to-patch 蒸馏来对齐局部和全局视觉线索，并采用基于 CLS token 的区域-类别对齐，而不是显式的 patch 级别对齐。

Result: FarSLIP 改进了遥感领域中细粒度的视觉-语言对齐，并在遥感开放词汇语义分割以及图像级别的任务（如零样本分类和图像-文本检索）上取得了新的最佳性能。

Conclusion: FarSLIP 框架有效地提升了遥感图像处理中的细粒度视觉-语言对齐能力，并在多个任务上验证了其有效性。

Abstract: As CLIP's global alignment limits its ability to capture fine-grained details, recent efforts have focused on enhancing its region-text alignment. However, current remote sensing (RS)-specific CLIP variants still inherit this limited spatial awareness. We identify two key limitations behind this: (1) current RS image-text datasets generate global captions from object-level labels, leaving the original object-level supervision underutilized; (2) despite the success of region-text alignment methods in general domain, their direct application to RS data often leads to performance degradation. To address these, we construct the first multi-granularity RS image-text dataset, MGRS-200k, featuring rich object-level textual supervision for RS region-category alignment. We further investigate existing fine-grained CLIP tuning strategies and find that current explicit region-text alignment methods, whether in a direct or indirect way, underperform due to severe degradation of CLIP's semantic coherence. Building on these, we propose FarSLIP, a Fine-grained Aligned RS Language-Image Pretraining framework. Rather than the commonly used patch-to-CLS self-distillation, FarSLIP employs patch-to-patch distillation to align local and global visual cues, which improves feature discriminability while preserving semantic coherence. Additionally, to effectively utilize region-text supervision, it employs simple CLS token-based region-category alignment rather than explicit patch-level alignment, further enhancing spatial awareness. FarSLIP features improved fine-grained vision-language alignment in RS domain and sets a new state of the art not only on RS open-vocabulary semantic segmentation, but also on image-level tasks such as zero-shot classification and image-text retrieval. Our dataset, code, and models are available at https://github.com/NJU-LHRS/FarSLIP.

</details>


### [37] [nnMIL: A generalizable multiple instance learning framework for computational pathology](https://arxiv.org/abs/2511.14907)
*Xiangde Luo,Jinxi Xiang,Yuanfeng Ji,Ruijiang Li*

Main category: cs.CV

TL;DR: nnMIL: A multiple-instance learning framework connects patch-level foundation models to robust slide-level clinical inference.


<details>
  <summary>Details</summary>
Motivation: Current approaches for aggregating patch-level features into slide-level predictions remain constrained by design limitations that hinder generalizability and reliability.

Method: nnMIL introduces random sampling at both the patch and feature levels, enabling large-batch optimization, task-aware sampling strategies, and efficient and scalable training across datasets and model architectures. A lightweight aggregator performs sliding-window inference to generate ensemble slide-level predictions and supports principled uncertainty estimation.

Result: nnMIL consistently outperformed existing MIL methods for disease diagnosis, histologic subtyping, molecular biomarker detection, and pan- cancer prognosis prediction. It further demonstrated strong cross-model generalization, reliable uncertainty quantification, and robust survival stratification in multiple external cohorts.

Conclusion: nnMIL offers a practical and generalizable solution for translating pathology foundation models into clinically meaningful predictions, advancing the development and deployment of reliable AI systems in real-world settings.

Abstract: Computational pathology holds substantial promise for improving diagnosis and guiding treatment decisions. Recent pathology foundation models enable the extraction of rich patch-level representations from large-scale whole-slide images (WSIs), but current approaches for aggregating these features into slide-level predictions remain constrained by design limitations that hinder generalizability and reliability. Here, we developed nnMIL, a simple yet broadly applicable multiple-instance learning framework that connects patch-level foundation models to robust slide-level clinical inference. nnMIL introduces random sampling at both the patch and feature levels, enabling large-batch optimization, task-aware sampling strategies, and efficient and scalable training across datasets and model architectures. A lightweight aggregator performs sliding-window inference to generate ensemble slide-level predictions and supports principled uncertainty estimation. Across 40,000 WSIs encompassing 35 clinical tasks and four pathology foundation models, nnMIL consistently outperformed existing MIL methods for disease diagnosis, histologic subtyping, molecular biomarker detection, and pan- cancer prognosis prediction. It further demonstrated strong cross-model generalization, reliable uncertainty quantification, and robust survival stratification in multiple external cohorts. In conclusion, nnMIL offers a practical and generalizable solution for translating pathology foundation models into clinically meaningful predictions, advancing the development and deployment of reliable AI systems in real-world settings.

</details>


### [38] [X-WIN: Building Chest Radiograph World Model via Predictive Sensing](https://arxiv.org/abs/2511.14918)
*Zefan Yang,Ge Wang,James Hendler,Mannudeep K. Kalra,Pingkun Yan*

Main category: cs.CV

TL;DR: 提出了一个名为X-WIN的新型CXR世界模型，它通过学习预测潜在空间中的2D投影来从胸部CT中提取体积知识。


<details>
  <summary>Details</summary>
Motivation: CXR是重要的医学影像技术，但作为2D投影图像，CXR受到结构叠加的限制，无法捕捉3D解剖结构，这使得表征学习和疾病诊断具有挑战性。

Method: 该方法的核心思想是，具有3D解剖结构内在知识的世界模型可以预测3D空间中各种变换下的CXR。在投影预测期间，引入了一种亲和力引导的对比对齐损失，该损失利用互相似性来捕获来自同一体积的投影中的丰富相关信息。为了提高模型的适应性，通过掩蔽图像建模将真实的CXR纳入训练，并采用域分类器来鼓励真实和模拟CXR的统计相似表示。

Result: 综合实验表明，X-WIN在使用线性探测和少样本微调的各种下游任务中优于现有的基础模型。

Conclusion: X-WIN还展示了为重建3D CT体积渲染2D投影的能力。

Abstract: Chest X-ray radiography (CXR) is an essential medical imaging technique for disease diagnosis. However, as 2D projectional images, CXRs are limited by structural superposition and hence fail to capture 3D anatomies. This limitation makes representation learning and disease diagnosis challenging. To address this challenge, we propose a novel CXR world model named X-WIN, which distills volumetric knowledge from chest computed tomography (CT) by learning to predict its 2D projections in latent space. The core idea is that a world model with internalized knowledge of 3D anatomical structure can predict CXRs under various transformations in 3D space. During projection prediction, we introduce an affinity-guided contrastive alignment loss that leverages mutual similarities to capture rich, correlated information across projections from the same volume. To improve model adaptability, we incorporate real CXRs into training through masked image modeling and employ a domain classifier to encourage statistically similar representations for real and simulated CXRs. Comprehensive experiments show that X-WIN outperforms existing foundation models on diverse downstream tasks using linear probing and few-shot fine-tuning. X-WIN also demonstrates the ability to render 2D projections for reconstructing a 3D CT volume.

</details>


### [39] [CPSL: Representing Volumetric Video via Content-Promoted Scene Layers](https://arxiv.org/abs/2511.14927)
*Kaiyuan Hu,Yili Jin,Junhua Liu,Xize Duan,Hong Kang,Xue Liu*

Main category: cs.CV

TL;DR: 提出了一种新的2.5D视频表示方法，名为内容驱动的场景层（CPSL），它能够以更低的成本实现高质量的自由视角合成。


<details>
  <summary>Details</summary>
Motivation: 现有的体视频表示方法在捕获、计算和渲染方面成本高昂，限制了其在按需视频和实时通信中的可扩展性。

Method: 该方法通过每帧深度和内容显著性将每一帧分解成一组具有几何一致性的层，这些层带有软alpha带和边缘深度缓存，共同保持遮挡顺序和边界连续性。利用深度加权扭曲和从前到后的alpha合成实现视差校正的新视角合成。

Result: CPSL在多个基准测试中实现了优越的感知质量和边界保真度，同时降低了存储和渲染成本。

Conclusion: 该方法提供了一条从2D视频到可扩展的2.5D沉浸式媒体的实用路径。

Abstract: Volumetric video enables immersive and interactive visual experiences by supporting free viewpoint exploration and realistic motion parallax. However, existing volumetric representations from explicit point clouds to implicit neural fields, remain costly in capture, computation, and rendering, which limits their scalability for on-demand video and reduces their feasibility for real-time communication.
  To bridge this gap, we propose Content-Promoted Scene Layers (CPSL), a compact 2.5D video representation that brings the perceptual benefits of volumetric video to conventional 2D content. Guided by per-frame depth and content saliency, CPSL decomposes each frame into a small set of geometry-consistent layers equipped with soft alpha bands and an edge-depth cache that jointly preserve occlusion ordering and boundary continuity. These lightweight, 2D-encodable assets enable parallax-corrected novel-view synthesis via depth-weighted warping and front-to-back alpha compositing, bypassing expensive 3D reconstruction. Temporally, CPSL maintains inter-frame coherence using motion-guided propagation and per-layer encoding, supporting real-time playback with standard video codecs. Across multiple benchmarks, CPSL achieves superior perceptual quality and boundary fidelity compared with layer-based and neural-field baselines while reducing storage and rendering cost by several folds. Our approach offer a practical path from 2D video to scalable 2.5D immersive media.

</details>


### [40] [Unsupervised Discovery of Long-Term Spatiotemporal Periodic Workflows in Human Activities](https://arxiv.org/abs/2511.14945)
*Fan Yang,Quanting Xie,Atsunori Moteki,Shoichi Masui,Shan Jiang,Yonatan Bisk,Graham Neubig*

Main category: cs.CV

TL;DR: 提出了一个包含580个多模态人类活动序列的基准，用于研究长期周期性工作流程，并提出了一个轻量级的、免训练的基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有的研究主要集中在短期周期性活动，而对长期周期性工作流程的研究不足。

Method: 构建了一个包含580个多模态人类活动序列的基准，并提出了一个轻量级的、免训练的基线模型，用于建模不同的周期性工作流程模式。

Result: 该基准对无监督周期性检测方法和基于大型语言模型（LLM）的零样本方法提出了重大挑战；该基线模型在所有评估任务中都明显优于其他方法；在实际应用中，该基线模型展示了与传统监督工作流程检测方法相当的部署优势，无需注释和重新训练。

Conclusion: 该研究填补了长期周期性工作流程研究的空白，并提供了一个有效的基线模型，具有实际应用价值。

Abstract: Periodic human activities with implicit workflows are common in manufacturing, sports, and daily life. While short-term periodic activities -- characterized by simple structures and high-contrast patterns -- have been widely studied, long-term periodic workflows with low-contrast patterns remain largely underexplored. To bridge this gap, we introduce the first benchmark comprising 580 multimodal human activity sequences featuring long-term periodic workflows. The benchmark supports three evaluation tasks aligned with real-world applications: unsupervised periodic workflow detection, task completion tracking, and procedural anomaly detection. We also propose a lightweight, training-free baseline for modeling diverse periodic workflow patterns. Experiments show that: (i) our benchmark presents significant challenges to both unsupervised periodic detection methods and zero-shot approaches based on powerful large language models (LLMs); (ii) our baseline outperforms competing methods by a substantial margin in all evaluation tasks; and (iii) in real-world applications, our baseline demonstrates deployment advantages on par with traditional supervised workflow detection approaches, eliminating the need for annotation and retraining. Our project page is https://sites.google.com/view/periodicworkflow.

</details>


### [41] [RocSync: Millisecond-Accurate Temporal Synchronization for Heterogeneous Camera Systems](https://arxiv.org/abs/2511.14948)
*Jaro Meyer,Frédéric Giraud,Joschua Wüthrich,Marc Pollefeys,Philipp Fürnstahl,Lilian Calvet*

Main category: cs.CV

TL;DR: 提出了一种低成本、通用的同步方法，可在不同的相机系统之间实现毫秒级的时间对齐，同时支持可见光 (RGB) 和红外 (IR) 模态。


<details>
  <summary>Details</summary>
Motivation: 在多视图 3D 重建、姿态估计和场景理解等广泛的动态场景应用中，多视图视频流的精确时空对齐至关重要。然而，同步多个摄像机仍然是一个重大挑战，尤其是在混合设置中，当通用硬件同步功能通常不可用时。

Method: 该解决方案采用定制的 LED 时钟，该时钟通过红色和红外 LED 编码时间，允许从记录的帧中对曝光窗口（开始和结束时间）进行视觉解码，以实现毫秒级同步。

Result: 针对硬件同步对我们的方法进行了基准测试，并在多次记录中实现了 1.34 毫秒 RMSE 的残余误差。在进一步的实验中，我们的方法优于基于光、音频和时间码的同步方法，并直接改进了下游计算机视觉任务，包括多视图姿态估计和 3D 重建。

Conclusion: 该解决方案简化并简化了同步流程，并扩大了在不受约束的环境中（包括工业和临床应用）访问基于视觉的高级传感的机会。

Abstract: Accurate spatiotemporal alignment of multi-view video streams is essential for a wide range of dynamic-scene applications such as multi-view 3D reconstruction, pose estimation, and scene understanding. However, synchronizing multiple cameras remains a significant challenge, especially in heterogeneous setups combining professional and consumer-grade devices, visible and infrared sensors, or systems with and without audio, where common hardware synchronization capabilities are often unavailable. This limitation is particularly evident in real-world environments, where controlled capture conditions are not feasible. In this work, we present a low-cost, general-purpose synchronization method that achieves millisecond-level temporal alignment across diverse camera systems while supporting both visible (RGB) and infrared (IR) modalities. The proposed solution employs a custom-built \textit{LED Clock} that encodes time through red and infrared LEDs, allowing visual decoding of the exposure window (start and end times) from recorded frames for millisecond-level synchronization. We benchmark our method against hardware synchronization and achieve a residual error of 1.34~ms RMSE across multiple recordings. In further experiments, our method outperforms light-, audio-, and timecode-based synchronization approaches and directly improves downstream computer vision tasks, including multi-view pose estimation and 3D reconstruction. Finally, we validate the system in large-scale surgical recordings involving over 25 heterogeneous cameras spanning both IR and RGB modalities. This solution simplifies and streamlines the synchronization pipeline and expands access to advanced vision-based sensing in unconstrained environments, including industrial and clinical applications.

</details>


### [42] [Artificial intelligence approaches for energy-efficient laser cutting machines](https://arxiv.org/abs/2511.14952)
*Mohamed Abdallah Salem,Hamdy Ahmed Ashour,Ahmed Elshenawy*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习 (DL) 的激光切割能量优化方法，通过动态调整吸气泵功率来实现节能。


<details>
  <summary>Details</summary>
Motivation: 当前CO2激光吸气泵缺乏自适应控制且为开环系统，导致能源消耗和环境影响。

Method: 1.  采用闭环配置，根据切割材料和烟雾水平动态调整泵功率。
2.  引入多种材料分类方法，包括无透镜散斑传感结合定制卷积神经网络 (CNN) 和使用 USB 摄像头结合预训练 VGG16 CNN 模型的迁移学习方法。
3.  使用独立的深度学习模型进行烟雾水平检测，以进一步优化泵的功率输出。

Result: 实验结果表明，烟雾吸气泵的能耗降低了 20% 到 50%。

Conclusion: 该方法显著降低了激光切割过程中的能源消耗，为制造业的可持续发展做出了贡献。

Abstract: This research addresses the significant challenges of energy consumption and environmental impact in laser cutting by proposing novel deep learning (DL) methodologies to achieve energy reduction. Recognizing the current lack of adaptive control and the open-loop nature of CO2 laser suction pumps, this study utilizes closed-loop configurations that dynamically adjust pump power based on both the material being cut and the smoke level generated. To implement this adaptive system, diverse material classification methods are introduced, including techniques leveraging lens-less speckle sensing with a customized Convolutional Neural Network (CNN) and an approach using a USB camera with transfer learning via the pre-trained VGG16 CNN model. Furthermore, a separate DL model for smoke level detection is employed to simultaneously refine the pump's power output. This integration prompts the exhaust suction pump to automatically halt during inactive times and dynamically adjust power during operation, leading to experimentally proven and remarkable energy savings, with results showing a 20% to 50% reduction in the smoke suction pump's energy consumption, thereby contributing substantially to sustainable development in the manufacturing sector.

</details>


### [43] [EGSA-PT:Edge-Guided Spatial Attention with Progressive Training for Monocular Depth Estimation and Segmentation of Transparent Objects](https://arxiv.org/abs/2511.14970)
*Gbenga Omotara,Ramy Farag,Seyed Mohamad Ali Tousi,G. N. DeSouza*

Main category: cs.CV

TL;DR: 本文提出了一种名为边缘引导空间注意力（EGSA）的融合机制，旨在通过将边界信息融入语义和几何特征的融合中，从而减轻负面的跨任务交互。


<details>
  <summary>Details</summary>
Motivation: 透明物体感知是计算机视觉研究中的一个主要挑战，因为透明性会混淆深度估计和语义分割。最近的工作探索了多任务学习框架来提高鲁棒性，但负面的跨任务交互通常会阻碍性能。

Method: 本文提出了边缘引导空间注意力（EGSA）融合机制和一种多模态渐进训练策略。该策略从RGB图像中提取的边缘学习过渡到从预测的深度图像中提取的边缘。

Result: 在Syn-TODD和ClearPose基准测试中，EGSA始终优于当前最先进的方法（MODEST），同时保持了具有竞争力的分割性能，并且在透明区域中表现出最大的改进。

Conclusion: 边缘引导融合是一种能够改善透明物体感知的稳健方法。

Abstract: Transparent object perception remains a major challenge in computer vision research, as transparency confounds both depth estimation and semantic segmentation. Recent work has explored multi-task learning frameworks to improve robustness, yet negative cross-task interactions often hinder performance. In this work, we introduce Edge-Guided Spatial Attention (EGSA), a fusion mechanism designed to mitigate destructive interactions by incorporating boundary information into the fusion between semantic and geometric features. On both Syn-TODD and ClearPose benchmarks, EGSA consistently improved depth accuracy over the current state of the art method (MODEST), while preserving competitive segmentation performance, with the largest improvements appearing in transparent regions. Besides our fusion design, our second contribution is a multi-modal progressive training strategy, where learning transitions from edges derived from RGB images to edges derived from predicted depth images. This approach allows the system to bootstrap learning from the rich textures contained in RGB images, and then switch to more relevant geometric content in depth maps, while it eliminates the need for ground-truth depth at training time. Together, these contributions highlight edge-guided fusion as a robust approach capable of improving transparent object perception.

</details>


### [44] [Logit-Based Losses Limit the Effectiveness of Feature Knowledge Distillation](https://arxiv.org/abs/2511.14981)
*Nicholas Cooper,Lijun Chen,Sailesh Dwivedy,Danna Gurari*

Main category: cs.CV

TL;DR: 本文提出了一种新的特征知识蒸馏框架，该框架仅使用基于特征的损失来训练学生的骨干网络。


<details>
  <summary>Details</summary>
Motivation: 先前的方法通常依赖于logits和中间层特征的损失函数，而本文旨在探索仅使用特征的知识蒸馏方法。

Method: 本文提出了一种知识质量指标，用于识别教师网络中哪些层能够提供最有效的知识用于蒸馏。

Result: 在三个图像分类数据集上，使用卷积神经网络和视觉Transformer等多种学生-教师对进行实验，结果表明该KD方法达到了最先进的性能，与标准方法相比，top-1准确率提高了15%。

Conclusion: 本文提出的特征知识蒸馏框架在图像分类任务上表现出色，并优于现有方法。

Abstract: Knowledge distillation (KD) methods can transfer knowledge of a parameter-heavy teacher model to a light-weight student model. The status quo for feature KD methods is to utilize loss functions based on logits (i.e., pre-softmax class scores) and intermediate layer features (i.e., latent representations). Unlike previous approaches, we propose a feature KD framework for training the student's backbone using feature-based losses exclusively (i.e., without logit-based losses such as cross entropy). Leveraging recent discoveries about the geometry of latent representations, we introduce a knowledge quality metric for identifying which teacher layers provide the most effective knowledge for distillation. Experiments on three image classification datasets with four diverse student-teacher pairs, spanning convolutional neural networks and vision transformers, demonstrate our KD method achieves state-of-the-art performance, delivering top-1 accuracy boosts of up to 15% over standard approaches. We publically share our code to facilitate future work at https://github.com/Thegolfingocto/KD_wo_CE.

</details>


### [45] [Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation](https://arxiv.org/abs/2511.14993)
*Vladimir Arkhipkin,Vladimir Korviakov,Nikolai Gerasimenko,Denis Parkhomenko,Viacheslav Vasilev,Alexey Letunovskiy,Maria Kovaleva,Nikolai Vaulin,Ivan Kirillov,Lev Novitskiy,Denis Koposov,Nikita Kiselev,Alexander Varlamov,Dmitrii Mikhailov,Vladimir Polovnikov,Andrey Shutkin,Ilya Vasiliev,Julia Agafonova,Anastasiia Kargapoltseva,Anna Dmitrienko,Anastasia Maltseva,Anna Averchenkova,Olga Kim,Tatiana Nikulina,Denis Dimitrov*

Main category: cs.CV

TL;DR: Kandinsky 5.0 is a family of foundation models for high-resolution image and 10-second video synthesis.


<details>
  <summary>Details</summary>
Motivation: To advance the development and accessibility of high-quality generative models for the research community.

Method: It uses a multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. Also, it presents novel architectural, training, and inference optimizations.

Result: Kandinsky 5.0 achieves high generation speeds and state-of-the-art performance across various tasks.

Conclusion: Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications and the open-source code and training checkpoints are released.

Abstract: This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.

</details>


### [46] [FinCriticalED: A Visual Benchmark for Financial Fact-Level OCR Evaluation](https://arxiv.org/abs/2511.14998)
*Yueru He,Xueqing Peng,Yupeng Cao,Yan Wang,Lingfei Qian,Haohang Li,Yi Han,Ruoyu Xiang,Mingquan Lin,Prayag Tiwari,Jimin Huang,Guojun Xiong,Sophia Ananiadou*

Main category: cs.CV

TL;DR: FinCriticalED：一个用于评估OCR和视觉语言模型在金融文档中事实层面表现的视觉基准。


<details>
  <summary>Details</summary>
Motivation: 金融文档中的OCR错误，如符号反转或日期偏移，可能导致重大误解，而传统OCR指标无法捕捉到这些关键的事实错误。

Method: 构建包含500个图像-HTML对的数据集，由金融专家标注超过700个数值和时间事实，并开发LLM-as-Judge评估流程，用于结构化事实提取和上下文验证。

Result: 最强的专有模型实现了最高的 фактическую 准确率，但在视觉上复杂的数值和时间上下文中仍然存在大量错误。

Conclusion: FinCriticalED 为提高金融和其他精确关键领域中的视觉事实精度奠定了坚实的基础。

Abstract: We introduce FinCriticalED (Financial Critical Error Detection), a visual benchmark for evaluating OCR and vision language models on financial documents at the fact level. Financial documents contain visually dense and table heavy layouts where numerical and temporal information is tightly coupled with structure. In high stakes settings, small OCR mistakes such as sign inversion or shifted dates can lead to materially different interpretations, while traditional OCR metrics like ROUGE and edit distance capture only surface level text similarity. \ficriticaled provides 500 image-HTML pairs with expert annotated financial facts covering over seven hundred numerical and temporal facts. It introduces three key contributions. First, it establishes the first fact level evaluation benchmark for financial document understanding, shifting evaluation from lexical overlap to domain critical factual correctness. Second, all annotations are created and verified by financial experts with strict quality control over signs, magnitudes, and temporal expressions. Third, we develop an LLM-as-Judge evaluation pipeline that performs structured fact extraction and contextual verification for visually complex financial documents. We benchmark OCR systems, open source vision language models, and proprietary models on FinCriticalED. Results show that although the strongest proprietary models achieve the highest factual accuracy, substantial errors remain in visually intricate numerical and temporal contexts. Through quantitative evaluation and expert case studies, FinCriticalED provides a rigorous foundation for advancing visual factual precision in financial and other precision critical domains.

</details>


### [47] [HV-Attack: Hierarchical Visual Attack for Multimodal Retrieval Augmented Generation](https://arxiv.org/abs/2511.15435)
*Linyin Luo,Yujuan Ding,Yunshan Ma,Wenqi Fan,Hanjiang Lai*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的针对多模态检索增强生成 (MRAG) 系统的视觉攻击方法，该方法通过在图像输入中添加难以察觉的扰动来误导系统，无需操纵其他组件。


<details>
  <summary>Details</summary>
Motivation: 现有的研究表明 MRAG 系统容易受到知识投毒攻击，但本文关注的是视觉攻击，即通过在图像输入中添加难以察觉的扰动来实现攻击。

Method: 提出了一种新的分层视觉攻击方法，该方法通过破坏多模态查询和增强知识之间的对齐来迷惑生成器。该方法采用分层两阶段策略来获得错位的增强知识，并通过优化扰动来破坏跨模态对齐和多模态语义对齐，从而使检索器回忆起来自原始数据库的无关知识。

Result: 在 OK-VQA 和 InfoSeek 两个广泛使用的 MRAG 数据集上进行了大量实验。使用基于 CLIP 的检索器和两个 LMM BLIP-2 和 LLaVA 作为生成器。结果表明，通过显着降低检索和生成性能，我们的视觉攻击对 MRAG 有效。

Conclusion: 该研究证明了 MRAG 系统容易受到视觉攻击，并提出了一种有效的分层视觉攻击方法。

Abstract: Advanced multimodal Retrieval-Augmented Generation (MRAG) techniques have been widely applied to enhance the capabilities of Large Multimodal Models (LMMs), but they also bring along novel safety issues. Existing adversarial research has revealed the vulnerability of MRAG systems to knowledge poisoning attacks, which fool the retriever into recalling injected poisoned contents. However, our work considers a different setting: visual attack of MRAG by solely adding imperceptible perturbations at the image inputs of users, without manipulating any other components. This is challenging due to the robustness of fine-tuned retrievers and large-scale generators, and the effect of visual perturbation may be further weakened by propagation through the RAG chain. We propose a novel Hierarchical Visual Attack that misaligns and disrupts the two inputs (the multimodal query and the augmented knowledge) of MRAG's generator to confuse its generation. We further design a hierarchical two-stage strategy to obtain misaligned augmented knowledge. We disrupt the image input of the retriever to make it recall irrelevant knowledge from the original database, by optimizing the perturbation which first breaks the cross-modal alignment and then disrupts the multimodal semantic alignment. We conduct extensive experiments on two widely-used MRAG datasets: OK-VQA and InfoSeek. We use CLIP-based retrievers and two LMMs BLIP-2 and LLaVA as generators. Results demonstrate the effectiveness of our visual attack on MRAG through the significant decrease in both retrieval and generation performance.

</details>


### [48] [CKDA: Cross-modality Knowledge Disentanglement and Alignment for Visible-Infrared Lifelong Person Re-identification](https://arxiv.org/abs/2511.15016)
*Zhenyu Cui,Jiahuan Zhou,Yuxin Peng*

Main category: cs.CV

TL;DR: 本文提出了一种新的终身行人重识别方法，旨在解决跨日夜场景下可见光-红外行人重识别中的知识冲突问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了模态特定知识获取和模态通用知识反遗忘的相互干扰，导致协同遗忘。

Method: 提出了一种跨模态知识解耦和对齐方法（CKDA），显式分离和保留模态特定知识和模态通用知识。包括模态通用提示（MCP）模块和模态特定提示（MSP）模块，以及跨模态知识对齐（CKA）模块。

Result: 在四个基准数据集上的实验表明，CKDA优于现有方法。

Conclusion: CKDA方法有效地解决了跨模态终身行人重识别中的知识冲突问题，提高了识别准确率。

Abstract: Lifelong person Re-IDentification (LReID) aims to match the same person employing continuously collected individual data from different scenarios. To achieve continuous all-day person matching across day and night, Visible-Infrared Lifelong person Re-IDentification (VI-LReID) focuses on sequential training on data from visible and infrared modalities and pursues average performance over all data. To this end, existing methods typically exploit cross-modal knowledge distillation to alleviate the catastrophic forgetting of old knowledge. However, these methods ignore the mutual interference of modality-specific knowledge acquisition and modality-common knowledge anti-forgetting, where conflicting knowledge leads to collaborative forgetting. To address the above problems, this paper proposes a Cross-modality Knowledge Disentanglement and Alignment method, called CKDA, which explicitly separates and preserves modality-specific knowledge and modality-common knowledge in a balanced way. Specifically, a Modality-Common Prompting (MCP) module and a Modality-Specific Prompting (MSP) module are proposed to explicitly disentangle and purify discriminative information that coexists and is specific to different modalities, avoiding the mutual interference between both knowledge. In addition, a Cross-modal Knowledge Alignment (CKA) module is designed to further align the disentangled new knowledge with the old one in two mutually independent inter- and intra-modality feature spaces based on dual-modality prototypes in a balanced manner. Extensive experiments on four benchmark datasets verify the effectiveness and superiority of our CKDA against state-of-the-art methods. The source code of this paper is available at https://github.com/PKU-ICST-MIPL/CKDA-AAAI2026.

</details>


### [49] [Complex-Valued 2D Gaussian Representation for Computer-Generated Holography](https://arxiv.org/abs/2511.15022)
*Yicheng Zhan,Xiangjun Gao,Long Quan,Kaan Akşit*

Main category: cs.CV

TL;DR: 提出了一种新的基于结构化复值 2D 高斯图元的 holography 表示方法，该方法替换了逐像素信息存储，并将参数搜索空间减少了 10:1。


<details>
  <summary>Details</summary>
Motivation: 为了实现端到端训练，我们为我们的表示开发了一个可微的光栅化器，该光栅化器与自由空间中 GPU 优化的光传播内核集成。

Method: 基于结构化复值 2D 高斯图元的 holography 表示方法，并开发了一个可微的光栅化器。

Result: 我们的实验表明，我们的方法实现了高达 2.5 倍的 VRAM 使用率降低和 50% 的优化速度提升，同时产生了比现有方法更高保真度的重建。

Conclusion: 通过减少 hologram 参数搜索空间，我们的表示方法能够在下一代计算机生成的 holography 系统中实现更具可扩展性的 hologram 估计。

Abstract: We propose a new hologram representation based on structured complex-valued 2D Gaussian primitives, which replaces per-pixel information storage and reduces the parameter search space by up to 10:1. To enable end-to-end training, we develop a differentiable rasterizer for our representation, integrated with a GPU-optimized light propagation kernel in free space. Our extensive experiments show that our method achieves up to 2.5x lower VRAM usage and 50% faster optimization while producing higher-fidelity reconstructions than existing methods. We further introduce a conversion procedure that adapts our representation to practical hologram formats, including smooth and random phase-only holograms. Our experiments show that this procedure can effectively suppress noise artifacts observed in previous methods. By reducing the hologram parameter search space, our representation enables a more scalable hologram estimation in the next-generation computer-generated holography systems.

</details>


### [50] [Computer Vision Modeling of the Development of Geometric and Numerical Concepts in Humans](https://arxiv.org/abs/2511.15029)
*Zekun Wang,Sashank Varma*

Main category: cs.CV

TL;DR: 计算机视觉模型在图像分类任务上训练后，能够学习到与成人类似的几何和数字概念的潜在表征。本研究探讨了这些模型是否也表现出与儿童相似的发展进程。


<details>
  <summary>Details</summary>
Motivation: 先前的研究表明，计算机视觉模型能够学习到与成人类似的几何和数字概念的潜在表征。本研究旨在进一步研究这些模型是否也表现出与儿童相似的发展进程。

Method: 本研究以 ResNet-50 模型为例，通过详细的案例研究，探讨了其在训练过程中的性能提升是否与儿童的发展进程相匹配。

Result: 研究发现，对于某些概念（欧几里德几何、几何图形、度量属性、拓扑），模型表现出与儿童相似的发展进程，但对于其他概念（手性图形、几何变换、对称图形）则不然。在数字方面，模型在经验中表现出类似人类的“心理数轴”表征的发展。

Conclusion: 研究结果表明，计算机视觉模型在理解人类数学理解的发展方面具有潜力，并为未来探索更多模型架构和构建更大的基准提供了方向。

Abstract: Mathematical thinking is a fundamental aspect of human cognition. Cognitive scientists have investigated the mechanisms that underlie our ability to thinking geometrically and numerically, to take two prominent examples, and developmental scientists have documented the trajectories of these abilities over the lifespan. Prior research has shown that computer vision (CV) models trained on the unrelated task of image classification nevertheless learn latent representations of geometric and numerical concepts similar to those of adults. Building on this demonstrated cognitive alignment, the current study investigates whether CV models also show developmental alignment: whether their performance improvements across training to match the developmental progressions observed in children. In a detailed case study of the ResNet-50 model, we show that this is the case. For the case of geometry and topology, we find developmental alignment for some classes of concepts (Euclidean Geometry, Geometrical Figures, Metric Properties, Topology) but not others (Chiral Figures, Geometric Transformations, Symmetrical Figures). For the case of number, we find developmental alignment in the emergence of a human-like ``mental number line'' representation with experience. These findings show the promise of computer vision models for understanding the development of mathematical understanding in humans. They point the way to future research exploring additional model architectures and building larger benchmarks.

</details>


### [51] [UniHOI: Unified Human-Object Interaction Understanding via Unified Token Space](https://arxiv.org/abs/2511.15046)
*Panqi Yang,Haodong Jing,Nanning Zheng,Yongqiang Ma*

Main category: cs.CV

TL;DR: UniHOI模型通过统一的token空间联合建模HOI检测和生成，促进知识共享和泛化。


<details>
  <summary>Details</summary>
Motivation: 传统方法分别处理HOI检测和生成任务，阻碍了全面交互理解的发展。

Method: 提出对称的交互感知注意力模块和统一的半监督学习范式，实现图像和交互语义之间的有效双向映射。

Result: 在HOI检测和生成任务上都取得了最先进的性能。在长尾HOI检测上提高了4.9%的准确率，在开放词汇生成任务上提高了42.0%的交互指标。

Conclusion: UniHOI有效地促进了知识共享和增强了泛化能力，并在HOI检测和生成任务上取得了显著的性能提升。

Abstract: In the field of human-object interaction (HOI), detection and generation are two dual tasks that have traditionally been addressed separately, hindering the development of comprehensive interaction understanding. To address this, we propose UniHOI, which jointly models HOI detection and generation via a unified token space, thereby effectively promoting knowledge sharing and enhancing generalization. Specifically, we introduce a symmetric interaction-aware attention module and a unified semi-supervised learning paradigm, enabling effective bidirectional mapping between images and interaction semantics even under limited annotations. Extensive experiments demonstrate that UniHOI achieves state-of-the-art performance in both HOI detection and generation. Specifically, UniHOI improves accuracy by 4.9% on long-tailed HOI detection and boosts interaction metrics by 42.0% on open-vocabulary generation tasks.

</details>


### [52] [Hyperspectral Super-Resolution with Inter-Image Variability via Degradation-based Low-Rank and Residual Fusion Method](https://arxiv.org/abs/2511.15052)
*Yue Wen,Kunjing Yang,Minru Bai*

Main category: cs.CV

TL;DR: 该论文提出了一种基于退化的低秩和残差融合(DLRRF)模型，用于解决高光谱图像(HSI)与多光谱图像(MSI)融合中存在的图像间差异问题，该模型通过模拟光谱退化算子的变化来处理光谱变异性，并通过低秩和残差分量分解来恢复空间细节。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过直接转换图像来处理图像间差异，这会加剧融合模型的病态性。

Method: 该模型首先将光谱变异性建模为光谱退化算子的变化。其次，为了恢复由空间局部变化引起的丢失的空间细节，将目标HSI分解为低秩和残差分量。此外，还引入了一个隐式正则化器来利用图像中的空间先验信息。使用近端交替优化(PAO)算法在即插即用(PnP)框架内求解所提出的DLRRF模型。

Result: 数值实验表明，DLRRF在融合具有图像间差异的HSI和MSI方面取得了优异的性能。

Conclusion: 该论文提出的DLRRF模型能够有效解决HSI和MSI融合中存在的图像间差异问题，并在性能上优于现有方法。

Abstract: The fusion of hyperspectral image (HSI) with multispectral image (MSI) provides an effective way to enhance the spatial resolution of HSI. However, due to different acquisition conditions, there may exist spectral variability and spatially localized changes between HSI and MSI, referred to as inter-image variability, which can significantly affect the fusion performance. Existing methods typically handle inter-image variability by applying direct transformations to the images themselves, which can exacerbate the ill-posedness of the fusion model. To address this challenge, we propose a Degradation-based Low-Rank and Residual Fusion (DLRRF) model. First, we model the spectral variability as change in the spectral degradation operator. Second, to recover the lost spatial details caused by spatially localized changes, we decompose the target HSI into low rank and residual components, where the latter is used to capture the lost details. By exploiting the spectral correlation within the images, we perform dimensionality reduction on both components. Additionally, we introduce an implicit regularizer to utilize the spatial prior information from the images. The proposed DLRRF model is solved using the Proximal Alternating Optimization (PAO) algorithm within a Plug-and-Play (PnP) framework, where the subproblem regarding implicit regularizer is addressed by an external denoiser. We further provide a comprehensive convergence analysis of the algorithm. Finally, extensive numerical experiments demonstrate that DLRRF achieves superior performance in fusing HSI and MSI with inter-image variability.

</details>


### [53] [CellGenNet: A Knowledge-Distilled Framework for Robust Cell Segmentation in Cancer Tissues](https://arxiv.org/abs/2511.15054)
*Srijan Ray,Bikesh K. Nirala,Jason T. Yustein,Sundaresh Ram*

Main category: cs.CV

TL;DR: CellGenNet improves nuclei segmentation accuracy and generalization in microscopy WSIs using knowledge distillation.


<details>
  <summary>Details</summary>
Motivation: Accurate nuclei segmentation in WSIs is challenging due to variability in staining, imaging conditions, and tissue morphology.

Method: A student-teacher architecture is used, with the teacher generating soft pseudo-labels for the student. The student is optimized using a joint objective that integrates ground-truth labels, teacher-derived probabilistic targets, and a hybrid loss function. Consistency regularization and layerwise dropout are also used.

Result: CellGenNet improves segmentation accuracy and generalization over supervised and semi-supervised baselines across diverse cancer tissue WSIs.

Conclusion: CellGenNet supports scalable and reproducible histopathology analysis.

Abstract: Accurate nuclei segmentation in microscopy whole slide images (WSIs) remains challenging due to variability in staining, imaging conditions, and tissue morphology. We propose CellGenNet, a knowledge distillation framework for robust cross-tissue cell segmentation under limited supervision. CellGenNet adopts a student-teacher architecture, where a capacity teacher is trained on sparse annotations and generates soft pseudo-labels for unlabeled regions. The student is optimized using a joint objective that integrates ground-truth labels, teacher-derived probabilistic targets, and a hybrid loss function combining binary cross-entropy and Tversky loss, enabling asymmetric penalties to mitigate class imbalance and better preserve minority nuclear structures. Consistency regularization and layerwise dropout further stabilize feature representations and promote reliable feature transfer. Experiments across diverse cancer tissue WSIs show that CellGenNet improves segmentation accuracy and generalization over supervised and semi-supervised baselines, supporting scalable and reproducible histopathology analysis.

</details>


### [54] [ESA: Energy-Based Shot Assembly Optimization for Automatic Video Editing](https://arxiv.org/abs/2511.02505)
*Yaosen Chen,Wei Wang,Tianheng Zheng,Xuming Wen,Han Yang,Yanru Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于能量优化的视频镜头组装方法，通过学习参考视频的组装风格，自动生成具有特定逻辑、叙事要求或艺术风格的视频。


<details>
  <summary>Details</summary>
Motivation: 现有智能视频编辑技术难以捕捉创作者在镜头组装中的独特艺术表达。

Method: 1. 对大型语言模型生成的剧本与视频库进行视觉语义匹配，获得候选镜头子集；2. 分割和标记参考视频中的镜头，提取镜头大小、相机运动和语义等属性；3. 使用基于能量的模型从这些属性中学习，根据候选镜头序列与参考风格的对齐程度对其进行评分；4. 结合多个语法规则，实现镜头组装优化。

Result: 该方法可以根据特定逻辑、叙事要求或艺术风格自动安排和组合独立镜头，并学习参考视频的组装风格，创建连贯的视觉序列或整体视觉表达。

Conclusion: 该系统即使是没有视频编辑经验的用户也可以创建具有视觉吸引力的视频。

Abstract: Shot assembly is a crucial step in film production and video editing, involving the sequencing and arrangement of shots to construct a narrative, convey information, or evoke emotions. Traditionally, this process has been manually executed by experienced editors. While current intelligent video editing technologies can handle some automated video editing tasks, they often fail to capture the creator's unique artistic expression in shot assembly. To address this challenge, we propose an energy-based optimization method for video shot assembly. Specifically, we first perform visual-semantic matching between the script generated by a large language model and a video library to obtain subsets of candidate shots aligned with the script semantics. Next, we segment and label the shots from reference videos, extracting attributes such as shot size, camera motion, and semantics. We then employ energy-based models to learn from these attributes, scoring candidate shot sequences based on their alignment with reference styles. Finally, we achieve shot assembly optimization by combining multiple syntax rules, producing videos that align with the assembly style of the reference videos. Our method not only automates the arrangement and combination of independent shots according to specific logic, narrative requirements, or artistic styles but also learns the assembly style of reference videos, creating a coherent visual sequence or holistic visual expression. With our system, even users with no prior video editing experience can create visually compelling videos. Project page: https://sobeymil.github.io/esa.com

</details>


### [55] [ProPL: Universal Semi-Supervised Ultrasound Image Segmentation via Prompt-Guided Pseudo-Labeling](https://arxiv.org/abs/2511.15057)
*Yaxiong Chen,Qicong Wang,Chunlei Li,Jingliang Hu,Yilei Shi,Shengwu Xiong,Xiao Xiang Zhu,Lichao Mou*

Main category: cs.CV

TL;DR: 提出了一种通用的半监督超声图像分割框架ProPL，可以处理多个器官和分割任务，并利用有标签和无标签数据。


<details>
  <summary>Details</summary>
Motivation: 现有的超声图像分割方法通常针对特定的解剖结构或任务，限制了它们在临床环境中的实际应用。

Method: ProPL采用共享视觉编码器和提示引导的双解码器，通过解码时的提示机制实现灵活的任务适应，并通过不确定性驱动的伪标签校准（UPLC）模块实现可靠的自训练。

Result: ProPL在各种指标上优于最先进的方法，为通用超声图像分割建立了一个新的基准。

Conclusion: 该论文开创了通用半监督超声图像分割的任务，并提出了ProPL框架，该框架在多个器官和分割任务上表现出色。

Abstract: Existing approaches for the problem of ultrasound image segmentation, whether supervised or semi-supervised, are typically specialized for specific anatomical structures or tasks, limiting their practical utility in clinical settings. In this paper, we pioneer the task of universal semi-supervised ultrasound image segmentation and propose ProPL, a framework that can handle multiple organs and segmentation tasks while leveraging both labeled and unlabeled data. At its core, ProPL employs a shared vision encoder coupled with prompt-guided dual decoders, enabling flexible task adaptation through a prompting-upon-decoding mechanism and reliable self-training via an uncertainty-driven pseudo-label calibration (UPLC) module. To facilitate research in this direction, we introduce a comprehensive ultrasound dataset spanning 5 organs and 8 segmentation tasks. Extensive experiments demonstrate that ProPL outperforms state-of-the-art methods across various metrics, establishing a new benchmark for universal ultrasound image segmentation.

</details>


### [56] [Evaluating Multimodal Large Language Models on Vertically Written Japanese Text](https://arxiv.org/abs/2511.15059)
*Keito Sasagawa,Shuhei Kurita,Daisuke Kawahara*

Main category: cs.CV

TL;DR: 现有的多模态大语言模型(MLLM)在垂直书写的日语文本上的表现不如水平书写的日语文本。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在理解日语文档图像方面有困难，特别是在垂直书写的情况下，对此的研究仍然有限。

Method: 通过渲染日语文本成图像来生成一个合成的日语OCR数据集，并使用它进行模型微调和评估。此外，还创建了一个来自真实文档图像的评估数据集，其中包含垂直书写的日语文本。

Result: 现有的多模态大语言模型在垂直书写的日语文本上的表现不如水平书写的日语文本。在合成的日语OCR数据集上训练多模态大语言模型可以提高模型处理垂直书写的能力。

Conclusion: 该研究表明，现有的多模态大语言模型在处理垂直书写的日语文本时存在不足，并且可以通过使用合成数据集进行训练来提高性能。

Abstract: Multimodal Large Language Models (MLLMs) have seen rapid advances in recent years and are now being applied to visual document understanding tasks. They are expected to process a wide range of document images across languages, including Japanese. Understanding documents from images requires models to read what are written in them. Since some Japanese documents are written vertically, support for vertical writing is essential. However, research specifically focused on vertically written Japanese text remains limited. In this study, we evaluate the reading capability of existing MLLMs on vertically written Japanese text. First, we generate a synthetic Japanese OCR dataset by rendering Japanese texts into images, and use it for both model fine-tuning and evaluation. This dataset includes Japanese text in both horizontal and vertical writing. We also create an evaluation dataset sourced from the real-world document images containing vertically written Japanese text. Using these datasets, we demonstrate that the existing MLLMs perform worse on vertically written Japanese text than on horizontally written Japanese text. Furthermore, we show that training MLLMs on our synthesized Japanese OCR dataset results in improving the performance of models that previously could not handle vertical writing. The datasets and code are publicly available https://github.com/llm-jp/eval_vertical_ja.

</details>


### [57] [Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks](https://arxiv.org/abs/2511.15065)
*Cheng Yang,Haiyuan Wan,Yiran Peng,Xin Cheng,Zhaoyang Yu,Jiayi Zhang,Junchi Yu,Xinlei Yu,Xiawu Zheng,Dongzhan Zhou,Chenglin Wu*

Main category: cs.CV

TL;DR: 本研究探讨了视频模型通过视频生成进行推理的能力，并提出了VR-Bench基准测试，用于评估视频模型的推理能力，特别是在空间推理方面。


<details>
  <summary>Details</summary>
Motivation: 动机在于探索视频模型是否能像语言模型一样进行推理，并且视频在空间布局和时间连续性方面具有优势，使其成为空间推理的理想载体。

Method: 通过在迷宫求解任务上训练视频模型，并使用VR-Bench基准测试评估其性能。

Result: 实验结果表明，经过SFT训练的视频模型能够有效激发推理能力，且在空间感知方面表现更强，优于其他VLM模型，并在不同场景、任务和复杂度上表现出良好的泛化能力。此外，测试时多样性采样可以提高推理的可靠性。

Conclusion: 研究结果表明，通过视频进行推理在空间推理任务中具有独特的潜力和可扩展性。

Abstract: Video Models have achieved remarkable success in high-fidelity video generation with coherent motion dynamics. Analogous to the development from text generation to text-based reasoning in language modeling, the development of video models motivates us to ask: Can video models reason via video generation? Compared with the discrete text corpus, video grounds reasoning in explicit spatial layouts and temporal continuity, which serves as an ideal substrate for spatial reasoning. In this work, we explore the reasoning via video paradigm and introduce VR-Bench -- a comprehensive benchmark designed to systematically evaluate video models' reasoning capabilities. Grounded in maze-solving tasks that inherently require spatial planning and multi-step reasoning, VR-Bench contains 7,920 procedurally generated videos across five maze types and diverse visual styles. Our empirical analysis demonstrates that SFT can efficiently elicit the reasoning ability of video model. Video models exhibit stronger spatial perception during reasoning, outperforming leading VLMs and generalizing well across diverse scenarios, tasks, and levels of complexity. We further discover a test-time scaling effect, where diverse sampling during inference improves reasoning reliability by 10--20%. These findings highlight the unique potential and scalability of reasoning via video for spatial reasoning tasks.

</details>


### [58] [BokehFlow: Depth-Free Controllable Bokeh Rendering via Flow Matching](https://arxiv.org/abs/2511.15066)
*Yachuan Huang,Xianrui Luo,Qiwen Wang,Liao Shen,Jiaqi Li,Huiqiang Sun,Zihao Huang,Wei Jiang,Zhiguo Cao*

Main category: cs.CV

TL;DR: 提出了一种名为 BokehFlow 的无深度可控散景渲染框架，通过流匹配直接从全聚焦图像合成逼真的散景效果，无需深度输入，并通过交叉注意力机制实现对焦点区域和模糊强度的语义控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于精确的深度图，而生成方法在可控性和效率方面存在不足。本文旨在解决在没有额外深度输入的情况下渲染可控散景的难题。

Method: 提出 BokehFlow 框架，该框架基于流匹配，直接从全聚焦图像合成散景效果，并采用交叉注意力机制，通过文本提示实现对焦点区域和模糊强度的语义控制。

Result: 大量实验表明，BokehFlow 在渲染质量和效率方面均优于现有的深度依赖和生成方法，实现了具有视觉吸引力的散景效果和精确控制。

Conclusion: BokehFlow 是一种无需深度信息即可实现可控散景渲染的有效方法，在渲染质量和效率上优于现有技术。

Abstract: Bokeh rendering simulates the shallow depth-of-field effect in photography, enhancing visual aesthetics and guiding viewer attention to regions of interest. Although recent approaches perform well, rendering controllable bokeh without additional depth inputs remains a significant challenge. Existing classical and neural controllable methods rely on accurate depth maps, while generative approaches often struggle with limited controllability and efficiency. In this paper, we propose BokehFlow, a depth-free framework for controllable bokeh rendering based on flow matching. BokehFlow directly synthesizes photorealistic bokeh effects from all-in-focus images, eliminating the need for depth inputs. It employs a cross-attention mechanism to enable semantic control over both focus regions and blur intensity via text prompts. To support training and evaluation, we collect and synthesize four datasets. Extensive experiments demonstrate that BokehFlow achieves visually compelling bokeh effects and offers precise control, outperforming existing depth-dependent and generative methods in both rendering quality and efficiency.

</details>


### [59] [MambaTrack3D: A State Space Model Framework for LiDAR-Based Object Tracking under High Temporal Variation](https://arxiv.org/abs/2511.15077)
*Shengjing Tian,Yinan Han,Xiantong Zhao,Xuehu Liu,Qi Lang*

Main category: cs.CV

TL;DR: MambaTrack3D: A novel HTV-oriented tracking framework built upon the state space model Mamba.


<details>
  <summary>Details</summary>
Motivation: Existing memory-based trackers often suffer from quadratic computational complexity, temporal redundancy, and insufficient exploitation of geometric priors in dynamic outdoor environments with high temporal variation (HTV).

Method: MambaTrack3D uses a Mamba-based Inter-frame Propagation (MIP) module and a Grouped Feature Enhancement Module (GFEM).

Result: MambaTrack3D outperforms both HTV-oriented and normal-scenario trackers on KITTI-HTV and nuScenes-HTV benchmarks. It also remains highly competitive on the standard KITTI dataset.

Conclusion: MambaTrack3D achieves a superior accuracy-efficiency trade-off, delivering robust performance across both specialized HTV and conventional tracking scenarios.

Abstract: Dynamic outdoor environments with high temporal variation (HTV) pose significant challenges for 3D single object tracking in LiDAR point clouds. Existing memory-based trackers often suffer from quadratic computational complexity, temporal redundancy, and insufficient exploitation of geometric priors. To address these issues, we propose MambaTrack3D, a novel HTV-oriented tracking framework built upon the state space model Mamba. Specifically, we design a Mamba-based Inter-frame Propagation (MIP) module that replaces conventional single-frame feature extraction with efficient inter-frame propagation, achieving near-linear complexity while explicitly modeling spatial relations across historical frames. Furthermore, a Grouped Feature Enhancement Module (GFEM) is introduced to separate foreground and background semantics at the channel level, thereby mitigating temporal redundancy in the memory bank. Extensive experiments on KITTI-HTV and nuScenes-HTV benchmarks demonstrate that MambaTrack3D consistently outperforms both HTV-oriented and normal-scenario trackers, achieving improvements of up to 6.5 success and 9.5 precision over HVTrack under moderate temporal gaps. On the standard KITTI dataset, MambaTrack3D remains highly competitive with state-of-the-art normal-scenario trackers, confirming its strong generalization ability. Overall, MambaTrack3D achieves a superior accuracy-efficiency trade-off, delivering robust performance across both specialized HTV and conventional tracking scenarios.

</details>


### [60] [TiCAL:Typicality-Based Consistency-Aware Learning for Multimodal Emotion Recognition](https://arxiv.org/abs/2511.15085)
*Wen Yin,Siyu Zhan,Cencen Liu,Xin Hu,Guiduo Duan,Xiurui Xie,Yuan-Fang Li,Tao He*

Main category: cs.CV

TL;DR: 提出了一种名为 TiCAL 的新型框架，用于解决多模态情感识别中的模态间情感冲突问题。该框架通过利用伪单模态情感标签和典型性估计来动态评估每个训练样本的一致性，并在双曲空间中嵌入特征以捕捉情感类别之间的细微差别。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态情感识别方法主要依赖于统一的情感标签来监督模型训练，忽略了模态间情感冲突这一关键挑战，即同一样本中不同的模态可能表达不同的情感倾向。

Method: 提出了一个名为 TiCAL 的框架，它利用伪单模态情感标签和典型性估计来动态评估每个训练样本的一致性，并在双曲空间中嵌入特征以增强情感表示。

Result: 在基准数据集（如 CMU-MOSEI 和 MER2023）上的大量实验表明，TiCAL 在缓解模态间情感冲突和提高整体识别准确率方面是有效的，例如，与最先进的 DMD 相比，大约有 2.6% 的改进。

Conclusion: TiCAL 通过结合一致性估计到学习过程中，提高了模型性能，尤其是在表现出高模态不一致性的样本上，验证了其在缓解模态间情感冲突和提高整体识别准确率方面的有效性。

Abstract: Multimodal Emotion Recognition (MER) aims to accurately identify human emotional states by integrating heterogeneous modalities such as visual, auditory, and textual data. Existing approaches predominantly rely on unified emotion labels to supervise model training, often overlooking a critical challenge: inter-modal emotion conflicts, wherein different modalities within the same sample may express divergent emotional tendencies. In this work, we address this overlooked issue by proposing a novel framework, Typicality-based Consistent-aware Multimodal Emotion Recognition (TiCAL), inspired by the stage-wise nature of human emotion perception. TiCAL dynamically assesses the consistency of each training sample by leveraging pseudo unimodal emotion labels alongside a typicality estimation. To further enhance emotion representation, we embed features in a hyperbolic space, enabling the capture of fine-grained distinctions among emotional categories. By incorporating consistency estimates into the learning process, our method improves model performance, particularly on samples exhibiting high modality inconsistency. Extensive experiments on benchmark datasets, e.g, CMU-MOSEI and MER2023, validate the effectiveness of TiCAL in mitigating inter-modal emotional conflicts and enhancing overall recognition accuracy, e.g., with about 2.6% improvements over the state-of-the-art DMD.

</details>


### [61] [Jointly Conditioned Diffusion Model for Multi-View Pose-Guided Person Image Synthesis](https://arxiv.org/abs/2511.15092)
*Chengyu Xie,Zhi Gong,Junchi Ren,Linkun Yu,Si Shen,Fei Shen,Xiaoyu Du*

Main category: cs.CV

TL;DR: 提出了一种名为联合条件扩散模型（JCDM）的框架，用于姿势引导的人体图像生成，该框架利用多视角先验来提高图像质量和跨视角一致性。


<details>
  <summary>Details</summary>
Motivation: 单视角参考视图中的纹理不完整，缺乏明确的跨视角交互。

Method: APM推断整体的身份保持先验，JCI融合多视角线索并将共享条件注入去噪backbone，以对齐身份、颜色和纹理。

Result: 实验表明，该方法在保真度和跨视角一致性方面达到了最先进水平。

Conclusion: JCDM支持可变数量的参考视图，并能以最小的架构修改与标准扩散backbone集成。

Abstract: Pose-guided human image generation is limited by incomplete textures from single reference views and the absence of explicit cross-view interaction. We present jointly conditioned diffusion model (JCDM), a jointly conditioned diffusion framework that exploits multi-view priors. The appearance prior module (APM) infers a holistic identity preserving prior from incomplete references, and the joint conditional injection (JCI) mechanism fuses multi-view cues and injects shared conditioning into the denoising backbone to align identity, color, and texture across poses. JCDM supports a variable number of reference views and integrates with standard diffusion backbones with minimal and targeted architectural modifications. Experiments demonstrate state of the art fidelity and cross-view consistency.

</details>


### [62] [A Comprehensive Study on Visual Token Redundancy for Discrete Diffusion-based Multimodal Large Language Models](https://arxiv.org/abs/2511.15098)
*Duo Li,Zuhao Yang,Xiaoqin Zhang,Ling Shao,Shijian Lu*

Main category: cs.CV

TL;DR: 这篇论文研究了离散扩散多模态大型语言模型（dMLLM）的效率优化问题，特别是视觉token的冗余问题。


<details>
  <summary>Details</summary>
Motivation: 现有的dMLLM在推理过程中计算开销大，且忽略了模态特定的视觉token冗余。

Method: 通过研究视觉token冗余如何随不同的dMLLM架构和任务演变，以及视觉token修剪如何影响dMLLM的响应和效率来进行分析。

Result: 研究表明，视觉冗余只出现在从头开始训练的dMLLM中，且主要在处理长答案任务时出现。视觉token修剪会导致信息损失，但从头开始训练的dMLLM可以在后续的去噪步骤中逐步恢复丢失的信息。层跳跃对于加速AR到扩散的dMLLM是有效的，而渐进式或后期修剪对于从头开始的dMLLM更有效。

Conclusion: 这项工作为dMLLM的效率优化提供了一个新的视角，极大地提高了它们在各种多模态理解任务中的适用性。

Abstract: Discrete diffusion-based multimodal large language models (dMLLMs) have emerged as a promising alternative to autoregressive MLLMs thanks to their advantages in parallel decoding and bidirectional context modeling, but most existing dMLLMs incur significant computational overhead during inference due to the full-sequence attention computation in each denoising step. Pioneer studies attempt to resolve this issue from a modality-agnostic perspective via key-value cache optimization or efficient sampling but most of them overlook modality-specific visual token redundancy. In this work, we conduct a comprehensive study on how visual token redundancy evolves with different dMLLM architectures and tasks and how visual token pruning affects dMLLM responses and efficiency. Specifically, our study reveals that visual redundancy emerges only in from-scratch dMLLMs while handling long-answer tasks. In addition, we validate that visual token pruning introduces non-negligible information loss in dMLLMs and only from-scratch dMLLMs can recover the lost information progressively during late denoising steps. Furthermore, our study shows that layer-skipping is promising for accelerating AR-to-diffusion dMLLMs, whereas progressive or late-step pruning is more effective for from-scratch dMLLMs. Overall, this work offers a new perspective on efficiency optimization for dMLLMs, greatly advancing their applicability across various multimodal understanding tasks.

</details>


### [63] [Gaussian Blending: Rethinking Alpha Blending in 3D Gaussian Splatting](https://arxiv.org/abs/2511.15102)
*Junseo Koo,Jinseo Jeong,Gunhee Kim*

Main category: cs.CV

TL;DR: 提出了一种新的高斯混合方法，以提高3D高斯溅射在 novel view synthesis 中的渲染质量，尤其是在训练期间未见过的采样率下。


<details>
  <summary>Details</summary>
Motivation: 现有的3D高斯溅射方法在以训练期间未见过的采样率合成视图时，会出现视觉差异，具体表现为放大时的侵蚀模糊伪影和缩小时的扩张阶梯伪影。这些伪影源于3DGS方法中采用的alpha混合的根本限制。

Method: 提出了一种新的高斯混合方法，该方法将 alpha 和透射率视为空间变化的分布，而不是计算标量。透射率的更新考虑了像素区域内 alpha 值的空间分布，允许附近的背景 splat 参与最终渲染。

Result: 大量实验表明，高斯混合能有效地捕捉各种训练中未见过的采样率下的精细细节，并且在未见过的和见过的采样率下，始终优于现有的 novel view synthesis 模型。

Conclusion: 高斯混合保持了实时渲染速度，不需要额外的内存成本，并且可以很容易地作为即插即用替代品集成到现有的基于3DGS或其他NVS框架中。

Abstract: The recent introduction of 3D Gaussian Splatting (3DGS) has significantly advanced novel view synthesis. Several studies have further improved the rendering quality of 3DGS, yet they still exhibit noticeable visual discrepancies when synthesizing views at sampling rates unseen during training. Specifically, they suffer from (i) erosion-induced blurring artifacts when zooming in and (ii) dilation-induced staircase artifacts when zooming out. We speculate that these artifacts arise from the fundamental limitation of the alpha blending adopted in 3DGS methods. Instead of the conventional alpha blending that computes alpha and transmittance as scalar quantities over a pixel, we propose to replace it with our novel Gaussian Blending that treats alpha and transmittance as spatially varying distributions. Thus, transmittances can be updated considering the spatial distribution of alpha values across the pixel area, allowing nearby background splats to contribute to the final rendering. Our Gaussian Blending maintains real-time rendering speed and requires no additional memory cost, while being easily integrated as a drop-in replacement into existing 3DGS-based or other NVS frameworks. Extensive experiments demonstrate that Gaussian Blending effectively captures fine details at various sampling rates unseen during training, consistently outperforming existing novel view synthesis models across both unseen and seen sampling rates.

</details>


### [64] [Generating Natural-Language Surgical Feedback: From Structured Representation to Domain-Grounded Evaluation](https://arxiv.org/abs/2511.15159)
*Firdavs Nasriddinov,Rafal Kocielnik,Anima Anandkumar,Andrew J. Hung*

Main category: cs.CV

TL;DR: 本研究提出了一种结构感知的流程，通过从真实的手术训练师-学员文本记录中学习手术动作本体，并利用它来指导反馈生成，从而实现自动化的、自然的、训练师风格的反馈。


<details>
  <summary>Details</summary>
Motivation: 从手术训练师那里获得高质量的术中反馈对于提高学员的表现和长期技能的掌握至关重要。自动化自然、训练师风格的反馈，有望大规模地提供及时、可访问和一致的指导，但这需要模型理解临床相关的表征。

Method: 该方法包括：(1) 从真实世界的反馈文本中挖掘 Instrument-Action-Target (IAT) 三元组，并将表面形式聚类成标准化的类别；(2) 微调一个视频到 IAT 模型，该模型利用手术过程和任务上下文以及细粒度的时间仪器运动；(3) 展示如何有效地使用 IAT 三元组表征来指导 GPT-4o 生成临床 обоснованного、训练师风格的反馈。

Result: 在任务 1：视频到 IAT 识别中，上下文注入和时间跟踪带来了持续的 AUC 增益（仪器：0.67 到 0.74；动作：0.60 到 0.63；组织：0.74 到 0.79）。对于任务 2：反馈文本生成（根据 1-5 的保真度进行评级，其中 1 = 相反/不安全，3 = 可接受，5 = 与人类训练师完美匹配），仅从视频生成的 GPT-4o 得分为 2.17，而 IAT 条件下的得分为 2.44（+12.4%），使得分 >= 3 的可接受生成的份额从 21% 提高到 42%。传统的文本相似性指标也得到了改善：词错误率降低了 15-31%，ROUGE（短语/子字符串重叠）提高了 9-64%。

Conclusion: 将生成建立在明确的 IAT 结构中可以提高保真度，并产生临床医生可验证的理由，从而支持在手术训练中进行可审计的使用。

Abstract: High-quality intraoperative feedback from a surgical trainer is pivotal for improving trainee performance and long-term skill acquisition. Automating natural, trainer-style feedback promises timely, accessible, and consistent guidance at scale but requires models that understand clinically relevant representations. We present a structure-aware pipeline that learns a surgical action ontology from real trainer-to-trainee transcripts (33 surgeries) and uses it to condition feedback generation. We contribute by (1) mining Instrument-Action-Target (IAT) triplets from real-world feedback text and clustering surface forms into normalized categories, (2) fine-tuning a video-to-IAT model that leverages the surgical procedure and task contexts as well as fine-grained temporal instrument motion, and (3) demonstrating how to effectively use IAT triplet representations to guide GPT-4o in generating clinically grounded, trainer-style feedback. We show that, on Task 1: Video-to-IAT recognition, our context injection and temporal tracking deliver consistent AUC gains (Instrument: 0.67 to 0.74; Action: 0.60 to 0.63; Tissue: 0.74 to 0.79). For Task 2: feedback text generation (rated on a 1-5 fidelity rubric where 1 = opposite/unsafe, 3 = admissible, and 5 = perfect match to a human trainer), GPT-4o from video alone scores 2.17, while IAT conditioning reaches 2.44 (+12.4%), doubling the share of admissible generations with score >= 3 from 21% to 42%. Traditional text-similarity metrics also improve: word error rate decreases by 15-31% and ROUGE (phrase/substring overlap) increases by 9-64%. Grounding generation in explicit IAT structure improves fidelity and yields clinician-verifiable rationales, supporting auditable use in surgical training.

</details>


### [65] [An Event-triggered System for Social Persuasion and Danger Alert in Elder Home Monitoring](https://arxiv.org/abs/2511.15117)
*Jun-Yi Liu,Chung-Hao Chen,Ya-Chi Tsao,Ssu-Yao Wu,Yu-Ting Tsao,Lyn Chao-ling Chen*

Main category: cs.CV

TL;DR: 开发了一个事件触发系统，用于检测老年人的看护、危险通知和照片链接事件，同时考虑了他们的身心状态。


<details>
  <summary>Details</summary>
Motivation: 考虑到老年人的身心状态，开发事件触发系统以改善他们的生活。

Method: 采用GMM背景建模检测访客和老年人的运动行为，并使用SVM机器学习分析捕获的图像。

Result: 在家庭场景中进行了实验，5个家庭参与了实验，检测并记录了他们生活活动中的三种类型的事件。

Conclusion: 设计了一种直观的操作方式，通过社交媒体在老年人及其亲属之间建立沟通。

Abstract: In the study, the physical state and mental state of elders are both considered, and an event-triggered system has developed to detect events: watch dog, danger notice and photo link. By adopting GMM background modeling, the motion behavior of visitors and elders can be detected in the watch dog event and danger notice event respectively. Experiments set in home scenarios and 5 families participated in the experiments for detecting and recording three types of events from their life activities. In addition, the captured images were analyzed using SVM machine learning. For lack of technical experiences of elders, an intuitive operation as normal life activity was designed to create communication between elder and relatives via social media.

</details>


### [66] [Unbiased Semantic Decoding with Vision Foundation Models for Few-shot Segmentation](https://arxiv.org/abs/2511.15118)
*Jin Wang,Bingfeng Zhang,Jian Pang,Weifeng Liu,Baodi Liu,Honglong Chen*

Main category: cs.CV

TL;DR: 提出了一种与 SAM 集成的无偏语义解码 (USD) 策略，用于少样本分割。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要侧重于从支持集中提取提示，不足以激活 SAM 的泛化能力，并且这种设计在适应未知类别时容易导致有偏差的解码过程。

Method: 同时从支持集和查询集中提取目标信息，以执行由对比语言-图像预训练 (CLIP) 模型的语义引导的一致预测。设计了两种特征增强策略，利用 CLIP 的语义对齐能力来丰富原始 SAM 特征；提出了一个可学习的视觉-文本目标提示生成器，通过交互目标文本嵌入和 CLIP 视觉特征。

Result: 无需重新训练视觉基础模型，具有语义区分的特征通过具有丰富目标信息的提示的引导，将注意力吸引到目标区域。

Conclusion: 该方法在少样本分割任务中具有潜力，通过无偏语义解码策略和特征增强策略，提高了 SAM 模型的泛化能力和分割精度。

Abstract: Few-shot segmentation has garnered significant attention. Many recent approaches attempt to introduce the Segment Anything Model (SAM) to handle this task. With the strong generalization ability and rich object-specific extraction ability of the SAM model, such a solution shows great potential in few-shot segmentation. However, the decoding process of SAM highly relies on accurate and explicit prompts, making previous approaches mainly focus on extracting prompts from the support set, which is insufficient to activate the generalization ability of SAM, and this design is easy to result in a biased decoding process when adapting to the unknown classes. In this work, we propose an Unbiased Semantic Decoding (USD) strategy integrated with SAM, which extracts target information from both the support and query set simultaneously to perform consistent predictions guided by the semantics of the Contrastive Language-Image Pre-training (CLIP) model. Specifically, to enhance the unbiased semantic discrimination of SAM, we design two feature enhancement strategies that leverage the semantic alignment capability of CLIP to enrich the original SAM features, mainly including a global supplement at the image level to provide a generalize category indicate with support image and a local guidance at the pixel level to provide a useful target location with query image. Besides, to generate target-focused prompt embeddings, a learnable visual-text target prompt generator is proposed by interacting target text embeddings and clip visual features. Without requiring re-training of the vision foundation models, the features with semantic discrimination draw attention to the target region through the guidance of prompt with rich target information.

</details>


### [67] [Computer-Use Agents as Judges for Generative User Interface](https://arxiv.org/abs/2511.15567)
*Kevin Qinghong Lin,Siyuan Hu,Linjie Li,Zhengyuan Yang,Lijuan Wang,Philip Torr,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 这篇论文提出了一个让计算机使用代理（CUA）参与图形用户界面（GUI）设计的框架，通过CUA作为评估者来辅助代码生成模型（Coder）自动设计GUI，目标是提高任务解决能力和代理导航成功率。


<details>
  <summary>Details</summary>
Motivation: 目前的GUI设计主要面向人类，导致代理执行任务效率低。同时，代码生成模型的快速发展为自动GUI设计带来了机会，因此需要研究如何利用CUA辅助Coder进行GUI设计。

Method: 论文提出了一个Coder-CUA协作框架，Coder作为设计者生成和修改网站，CUA作为评估者评估功能并改进设计。此外，还设计了一个CUA仪表盘，将多步导航历史压缩成简洁的可视化摘要，为迭代重新设计提供可解释的指导。

Result: 论文构建了一个名为AUI-Gym的基准测试，包含52个应用程序和1560个任务。通过实验，验证了所提出的框架可以提高任务解决能力和代理导航成功率。

Conclusion: 该研究将代理从被动使用转变为主动参与数字环境，朝着代理原生效率和可靠性迈出了一步。

Abstract: Computer-Use Agents (CUA) are becoming increasingly capable of autonomously operating digital environments through Graphical User Interfaces (GUI). Yet, most GUI remain designed primarily for humans--prioritizing aesthetics and usability--forcing agents to adopt human-oriented behaviors that are unnecessary for efficient task execution. At the same time, rapid advances in coding-oriented language models (Coder) have transformed automatic GUI design. This raises a fundamental question: Can CUA as judges to assist Coder for automatic GUI design? To investigate, we introduce AUI-Gym, a benchmark for Automatic GUI development spanning 52 applications across diverse domains. Using language models, we synthesize 1560 tasks that simulate real-world scenarios. To ensure task reliability, we further develop a verifier that programmatically checks whether each task is executable within its environment. Building on this, we propose a Coder-CUA in Collaboration framework: the Coder acts as Designer, generating and revising websites, while the CUA serves as Judge, evaluating functionality and refining designs. Success is measured not by visual appearance, but by task solvability and CUA navigation success rate. To turn CUA feedback into usable guidance, we design a CUA Dashboard that compresses multi-step navigation histories into concise visual summaries, offering interpretable guidance for iterative redesign. By positioning agents as both designers and judges, our framework shifts interface design toward agent-native efficiency and reliability. Our work takes a step toward shifting agents from passive use toward active participation in digital environments. Our code and dataset are available at https://github.com/showlab/AUI.

</details>


### [68] [WaveFuse-AL: Cyclical and Performance-Adaptive Multi-Strategy Active Learning for Medical Images](https://arxiv.org/abs/2511.15132)
*Nishchala Thakur,Swati Kochhar,Deepti R. Bathula,Sukrit Gupta*

Main category: cs.CV

TL;DR: 提出了一种新的主动学习框架 WaveFuse-AL，它自适应地融合了多种已建立的采集策略，并在学习过程中动态调整策略的重要性。


<details>
  <summary>Details</summary>
Motivation: 主动学习可以通过策略性地选择信息量最大的样本进行标注，从而降低医学影像中的标注成本。然而，单个采集策略在主动学习周期的不同阶段通常表现出不一致的行为。

Method: WaveFuse-AL 结合了循环（正弦）时间先验和性能驱动的适应性，以动态调整策略随时间推移的重要性。融合了 BALD、BADGE、Entropy 和 CoreSet 等多种采集策略。

Result: 在三个医学影像基准数据集上进行了评估：APTOS-2019（多类分类）、RSNA 肺炎检测（二元分类）和 ISIC-2018（皮肤病变分割）。实验结果表明，WaveFuse-AL 始终优于单策略和交替策略基线，并在有限的标注预算下实现了具有统计意义的性能改进（在十二个指标测量中的十个上）。

Conclusion: WaveFuse-AL 能够有效地融合多种主动学习策略，并在医学影像任务中实现优越的性能。

Abstract: Active learning reduces annotation costs in medical imaging by strategically selecting the most informative samples for labeling. However, individual acquisition strategies often exhibit inconsistent behavior across different stages of the active learning cycle. We propose Cyclical and Performance-Adaptive Multi-Strategy Active Learning (WaveFuse-AL), a novel framework that adaptively fuses multiple established acquisition strategies-BALD, BADGE, Entropy, and CoreSet throughout the learning process. WaveFuse-AL integrates cyclical (sinusoidal) temporal priors with performance-driven adaptation to dynamically adjust strategy importance over time. We evaluate WaveFuse-AL on three medical imaging benchmarks: APTOS-2019 (multi-class classification), RSNA Pneumonia Detection (binary classification), and ISIC-2018 (skin lesion segmentation). Experimental results demonstrate that WaveFuse-AL consistently outperforms both single-strategy and alternating-strategy baselines, achieving statistically significant performance improvements (on ten out of twelve metric measurements) while maximizing the utility of limited annotation budgets.

</details>


### [69] [When to Think and When to Look: Uncertainty-Guided Lookback](https://arxiv.org/abs/2511.15613)
*Jing Bi,Filippos Bellos,Junjia Guo,Yayuan Li,Chao Huang,Yunlong,Tang,Luchuan Song,Susan Liang,Zhongfei,Zhang,Jason J. Corso,Chenliang Xu*

Main category: cs.CV

TL;DR: 本文分析了思维链（chain-of-thought）如何影响视觉语言模型的推理能力，发现过长的思维链并不总是有益，并提出了一种新的解码策略，通过不确定性引导的回溯提示和广度搜索来提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 尽管思维链在大型视觉语言模型中表现出强大的潜力，但目前仍缺乏对思维链如何实际影响视觉推理的系统分析。

Method: 通过对来自 InternVL3.5 和 Qwen3-VL 系列的十个变体在 MMMU-val 数据集上进行大规模、受控的比较，评估了不同思维链对模型性能的影响。在此基础上，提出了一种名为不确定性引导回溯的无训练解码策略。

Result: 研究表明，过长的思维链有时会忽略图像，导致性能下降。成功的推理轨迹中，明确提及图像的短语能够提高视觉基础能力。提出的解码策略提高了 MMMU 的整体性能，并在标准思维链较弱的类别中获得了最大的提升，在固定模型系列和 token 预算下，实现了新的 state-of-the-art。

Conclusion: 本文通过深入分析思维链对视觉推理的影响，提出了一种新的解码策略，能够有效提高视觉语言模型的性能，并在多个基准测试中取得了 consistent 的改进，证明了该策略的泛化能力。

Abstract: Test-time thinking (that is, generating explicit intermediate reasoning chains) is known to boost performance in large language models and has recently shown strong gains for large vision language models (LVLMs). However, despite these promising results, there is still no systematic analysis of how thinking actually affects visual reasoning. We provide the first such analysis with a large scale, controlled comparison of thinking for LVLMs, evaluating ten variants from the InternVL3.5 and Qwen3-VL families on MMMU-val under generous token budgets and multi pass decoding. We show that more thinking is not always better; long chains often yield long wrong trajectories that ignore the image and underperform the same models run in standard instruct mode. A deeper analysis reveals that certain short lookback phrases, which explicitly refer back to the image, are strongly enriched in successful trajectories and correlate with better visual grounding. Building on this insight, we propose uncertainty guided lookback, a training free decoding strategy that combines an uncertainty signal with adaptive lookback prompts and breadth search. Our method improves overall MMMU performance, delivers the largest gains in categories where standard thinking is weak, and outperforms several strong decoding baselines, setting a new state of the art under fixed model families and token budgets. We further show that this decoding strategy generalizes, yielding consistent improvements on five additional benchmarks, including two broad multimodal suites and math focused visual reasoning datasets.

</details>


### [70] [DCL-SE: Dynamic Curriculum Learning for Spatiotemporal Encoding of Brain Imaging](https://arxiv.org/abs/2511.15151)
*Meihua Zhou,Xinyu Tong,Jiarui Zhao,Min Cheng,Li Yang,Lei Tian,Nan Wan*

Main category: cs.CV

TL;DR: 提出了一种新的端到端框架DCL-SE，用于临床诊断的高维神经影像分析。


<details>
  <summary>Details</summary>
Motivation: 现有方法在时空保真度和大型通用模型的适应性方面存在局限性。

Method: 利用ARP有效地将三维脑数据编码为二维动态表示，并采用动态课程学习策略，通过DGM逐步训练解码器。

Result: 在包括阿尔茨海默病和脑肿瘤分类、脑动脉分割和脑年龄预测在内的六个公开数据集上，DCL-SE始终优于现有方法。

Conclusion: 研究结果表明，在大型预训练网络的时代，紧凑、特定于任务的架构至关重要。

Abstract: High-dimensional neuroimaging analyses for clinical diagnosis are often constrained by compromises in spatiotemporal fidelity and by the limited adaptability of large-scale, general-purpose models. To address these challenges, we introduce Dynamic Curriculum Learning for Spatiotemporal Encoding (DCL-SE), an end-to-end framework centered on data-driven spatiotemporal encoding (DaSE). We leverage Approximate Rank Pooling (ARP) to efficiently encode three-dimensional volumetric brain data into information-rich, two-dimensional dynamic representations, and then employ a dynamic curriculum learning strategy, guided by a Dynamic Group Mechanism (DGM), to progressively train the decoder, refining feature extraction from global anatomical structures to fine pathological details. Evaluated across six publicly available datasets, including Alzheimer's disease and brain tumor classification, cerebral artery segmentation, and brain age prediction, DCL-SE consistently outperforms existing methods in accuracy, robustness, and interpretability. These findings underscore the critical importance of compact, task-specific architectures in the era of large-scale pretrained networks.

</details>


### [71] [VisPlay: Self-Evolving Vision-Language Models from Images](https://arxiv.org/abs/2511.15661)
*Yicheng He,Chengsong Huang,Zongxia Li,Jiaxin Huang,Yonghui Yang*

Main category: cs.CV

TL;DR: VisPlay is a self-evolving RL framework that improves VLMs' reasoning abilities using unlabeled image data.


<details>
  <summary>Details</summary>
Motivation: Existing RL approaches often rely on costly human-annotated labels or task-specific heuristics to define verifiable rewards.

Method: VisPlay assigns the model into two interacting roles: an Image-Conditioned Questioner and a Multimodal Reasoner, jointly trained with Group Relative Policy Optimization (GRPO).

Result: VisPlay achieves consistent improvements in visual reasoning, compositional generalization, and hallucination reduction across eight benchmarks.

Conclusion: VisPlay demonstrates a scalable path toward self-evolving multimodal intelligence.

Abstract: Reinforcement learning (RL) provides a principled framework for improving Vision-Language Models (VLMs) on complex reasoning tasks. However, existing RL approaches often rely on human-annotated labels or task-specific heuristics to define verifiable rewards, both of which are costly and difficult to scale. We introduce VisPlay, a self-evolving RL framework that enables VLMs to autonomously improve their reasoning abilities using large amounts of unlabeled image data. Starting from a single base VLM, VisPlay assigns the model into two interacting roles: an Image-Conditioned Questioner that formulates challenging yet answerable visual questions, and a Multimodal Reasoner that generates silver responses. These roles are jointly trained with Group Relative Policy Optimization (GRPO), which incorporates diversity and difficulty rewards to balance the complexity of generated questions with the quality of the silver answers. VisPlay scales efficiently across two model families. When trained on Qwen2.5-VL and MiMo-VL, VisPlay achieves consistent improvements in visual reasoning, compositional generalization, and hallucination reduction across eight benchmarks, including MM-Vet and MMMU, demonstrating a scalable path toward self-evolving multimodal intelligence. The project page is available at https://bruno686.github.io/VisPlay/

</details>


### [72] [SceneEdited: A City-Scale Benchmark for 3D HD Map Updating via Image-Guided Change Detection](https://arxiv.org/abs/2511.15153)
*Chun-Jung Lin,Tat-Jun Chin,Sourav Garg,Feras Dayoub*

Main category: cs.CV

TL;DR: SceneEdited: A city-scale dataset for HD map maintenance through 3D point cloud updating.


<details>
  <summary>Details</summary>
Motivation: The need for robust methods that detect changes and incorporate them into updated 3D representations for HD maps.

Method: A new dataset called SceneEdited with synthesized object changes and baseline methods using an image-based structure-from-motion pipeline.

Result: The SceneEdited dataset contains over 800 up-to-date scenes, 23,000 synthesized object changes, and a comprehensive toolkit.

Conclusion: SceneEdited establishes a standardized benchmark for 3D map updating research.

Abstract: Accurate, up-to-date High-Definition (HD) maps are critical for urban planning, infrastructure monitoring, and autonomous navigation. However, these maps quickly become outdated as environments evolve, creating a need for robust methods that not only detect changes but also incorporate them into updated 3D representations. While change detection techniques have advanced significantly, there remains a clear gap between detecting changes and actually updating 3D maps, particularly when relying on 2D image-based change detection. To address this gap, we introduce SceneEdited, the first city-scale dataset explicitly designed to support research on HD map maintenance through 3D point cloud updating. SceneEdited contains over 800 up-to-date scenes covering 73 km of driving and approximate 3 $\text{km}^2$ of urban area, with more than 23,000 synthesized object changes created both manually and automatically across 2000+ out-of-date versions, simulating realistic urban modifications such as missing roadside infrastructure, buildings, overpasses, and utility poles. Each scene includes calibrated RGB images, LiDAR scans, and detailed change masks for training and evaluation. We also provide baseline methods using a foundational image-based structure-from-motion pipeline for updating outdated scenes, as well as a comprehensive toolkit supporting scalability, trackability, and portability for future dataset expansion and unification of out-of-date object annotations. Both the dataset and the toolkit are publicly available at https://github.com/ChadLin9596/ScenePoint-ETK, establising a standardized benchmark for 3D map updating research.

</details>


### [73] [MoDES: Accelerating Mixture-of-Experts Multimodal Large Language Models via Dynamic Expert Skipping](https://arxiv.org/abs/2511.15690)
*Yushi Huang,Zining Wang,Zhihang Yuan,Yifu Ding,Ruihao Gong,Jinyang Guo,Xianglong Liu,Jun Zhang*

Main category: cs.CV

TL;DR: MoDES是一个免训练框架，通过自适应地跳过专家来实现高效和准确的MoE MLLM推理。它集成了全局调制局部门控（GMLG）机制和双模态阈值（DMT）方法，并利用前沿搜索算法来设置最佳阈值。


<details>
  <summary>Details</summary>
Motivation: 现有的专家跳过方法在应用于MLLM时会导致性能下降，因为它们无法考虑到MoE层中专家的异构贡献以及这些层中令牌的特定模态行为。

Method: MoDES框架，包含全局调制局部门控（GMLG）机制和双模态阈值（DMT）方法，以及前沿搜索算法。

Result: 在13个基准测试中，MoDES的性能远超以往的方法。例如，当为Qwen3-VL-MoE-30B-A3B-Instruct跳过88%的专家时，性能提升高达10.67%（97.33% vs. 86.66%）。此外，MoDES还显著提高了推理速度，将预填充时间提高了2.16倍，将解码时间提高了1.26倍。

Conclusion: MoDES通过自适应地跳过专家，显著提高了MoE MLLM的推理效率和准确性。

Abstract: Mixture-of-Experts (MoE) Multimodal large language models (MLLMs) excel at vision-language tasks, but they suffer from high computational inefficiency. To reduce inference overhead, expert skipping methods have been proposed to deactivate redundant experts based on the current input tokens. However, we find that applying these methods-originally designed for unimodal large language models (LLMs)-to MLLMs results in considerable performance degradation. This is primarily because such methods fail to account for the heterogeneous contributions of experts across MoE layers and modality-specific behaviors of tokens within these layers. Motivated by these findings, we propose MoDES, the first training-free framework that adaptively skips experts to enable efficient and accurate MoE MLLM inference. It incorporates a globally-modulated local gating (GMLG) mechanism that integrates global layer-wise importance into local routing probabilities to accurately estimate per-token expert importance. A dual-modality thresholding (DMT) method is then applied, which processes tokens from each modality separately, to derive the skipping schedule. To set the optimal thresholds, we introduce a frontier search algorithm that exploits monotonicity properties, cutting convergence time from several days to a few hours. Extensive experiments for 3 model series across 13 benchmarks demonstrate that MoDES far outperforms previous approaches. For instance, when skipping 88% experts for Qwen3-VL-MoE-30B-A3B-Instruct, the performance boost is up to 10.67% (97.33% vs. 86.66%). Furthermore, MoDES significantly enhances inference speed, improving the prefilling time by 2.16$\times$ and the decoding time by 1.26$\times$.

</details>


### [74] [Think Visually, Reason Textually: Vision-Language Synergy in ARC](https://arxiv.org/abs/2511.15703)
*Beichen Zhang,Yuhang Zang,Xiaoyi Dong,Yuhang Cao,Haodong Duan,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: 大型语言模型在少量样本中进行抽象推理仍然是一个未解决的核心问题。该论文提出了一种结合视觉和语言优势的方法，以提高模型在 ARC-AGI 基准测试中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常将 ARC-AGI 视为纯粹的文本推理任务，忽略了人类在解决此类难题时严重依赖视觉抽象的事实。然而，简单地将 ARC-AGI 网格渲染为图像会降低性能，这表明视觉和语言在不同的推理阶段具有互补的优势。

Method: 论文介绍了两种协同策略：(1) 视觉-语言协同推理 (VLSR)，将 ARC-AGI 分解为模态对齐的子任务；(2) 模态切换自校正 (MSSC)，利用视觉来验证基于文本的推理，以进行内在的错误纠正。

Result: 实验表明，该方法在不同的旗舰模型和多个 ARC-AGI 任务中，比纯文本基线提高了 4.33%。

Conclusion: 将视觉抽象与语言推理相结合是未来基础模型实现可泛化、类人智能的关键一步。

Abstract: Abstract reasoning from minimal examples remains a core unsolved problem for frontier foundation models such as GPT-5 and Grok 4. These models still fail to infer structured transformation rules from a handful of examples, which is a key hallmark of human intelligence. The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) provides a rigorous testbed for this capability, demanding conceptual rule induction and transfer to novel tasks. Most existing methods treat ARC-AGI as a purely textual reasoning task, overlooking the fact that humans rely heavily on visual abstraction when solving such puzzles. However, our pilot experiments reveal a paradox: naively rendering ARC-AGI grids as images degrades performance due to imprecise rule execution. This leads to our central hypothesis that vision and language possess complementary strengths across distinct reasoning stages: vision supports global pattern abstraction and verification, whereas language specializes in symbolic rule formulation and precise execution. Building on this insight, we introduce two synergistic strategies: (1) Vision-Language Synergy Reasoning (VLSR), which decomposes ARC-AGI into modality-aligned subtasks; and (2) Modality-Switch Self-Correction (MSSC), which leverages vision to verify text-based reasoning for intrinsic error correction. Extensive experiments demonstrate that our approach yields up to a 4.33% improvement over text-only baselines across diverse flagship models and multiple ARC-AGI tasks. Our findings suggest that unifying visual abstraction with linguistic reasoning is a crucial step toward achieving generalizable, human-like intelligence in future foundation models. Source code will be released soon.

</details>


### [75] [Multimodal Continual Instruction Tuning with Dynamic Gradient Guidance](https://arxiv.org/abs/2511.15164)
*Songze Li,Mingyu Gao,Tonghua Su,Xu-Yao Zhang,Zhongjie Wang*

Main category: cs.CV

TL;DR: 提出了一种通过近似旧任务的缺失梯度来解决多模态持续学习中灾难性遗忘问题的方法。


<details>
  <summary>Details</summary>
Motivation: 多模态持续指令微调面临灾难性遗忘的挑战，即学习新任务导致旧任务性能下降。

Method: 利用参数空间的几何特性，使用当前参数和先前最优参数之间的方向向量作为梯度指导，近似缺失的梯度。该近似梯度与来自有限重放缓冲区的真实梯度集成，并通过伯努利抽样策略进行调节，以平衡模型的稳定性和可塑性。

Result: 在多模态持续指令微调数据集上的大量实验表明，该方法在没有模型扩展的情况下实现了最先进的性能，有效缓解了灾难性遗忘，同时保持了紧凑的架构。

Conclusion: 该方法通过近似缺失梯度，有效缓解了多模态持续学习中的灾难性遗忘问题，并在保持模型紧凑性的同时，实现了最先进的性能。

Abstract: Multimodal continual instruction tuning enables multimodal large language models to sequentially adapt to new tasks while building upon previously acquired knowledge. However, this continual learning paradigm faces the significant challenge of catastrophic forgetting, where learning new tasks leads to performance degradation on previous ones. In this paper, we introduce a novel insight into catastrophic forgetting by conceptualizing it as a problem of missing gradients from old tasks during new task learning. Our approach approximates these missing gradients by leveraging the geometric properties of the parameter space, specifically using the directional vector between current parameters and previously optimal parameters as gradient guidance. This approximated gradient can be further integrated with real gradients from a limited replay buffer and regulated by a Bernoulli sampling strategy that dynamically balances model stability and plasticity. Extensive experiments on multimodal continual instruction tuning datasets demonstrate that our method achieves state-of-the-art performance without model expansion, effectively mitigating catastrophic forgetting while maintaining a compact architecture.

</details>


### [76] [Learning Depth from Past Selves: Self-Evolution Contrast for Robust Depth Estimation](https://arxiv.org/abs/2511.15167)
*Jing Cao,Kui Jiang,Shenyi Li,Xiaocheng Feng,Yong Huang*

Main category: cs.CV

TL;DR: 提出了一种新的自监督深度估计框架，称为 SEC-Depth，用于提高在雨雾等恶劣天气条件下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有自监督深度估计方法在雨雾等恶劣天气条件下性能显著下降，因为能见度降低严重影响了深度预测。

Method: 利用训练过程中生成的中间参数构建时间演化的延迟模型，并设计了一种自演化对比学习方案，以减轻在挑战性条件下的性能损失。具体来说，首先设计了深度估计任务的延迟模型的动态更新策略，以捕获跨训练阶段的优化状态。为了有效地利用延迟模型，引入了一种自演化对比损失 (SECL)，该损失将来自历史延迟模型的输出视为负样本。

Result: 实验表明，该方法可以无缝集成到不同的基线模型中，并显著提高零样本评估的鲁棒性。

Conclusion: SEC-Depth 框架有效地提高了自监督深度估计在恶劣天气条件下的鲁棒性，且易于集成到现有模型中。

Abstract: Self-supervised depth estimation has gained significant attention in autonomous driving and robotics. However, existing methods exhibit substantial performance degradation under adverse weather conditions such as rain and fog, where reduced visibility critically impairs depth prediction. To address this issue, we propose a novel self-evolution contrastive learning framework called SEC-Depth for self-supervised robust depth estimation tasks. Our approach leverages intermediate parameters generated during training to construct temporally evolving latency models. Using these, we design a self-evolution contrastive scheme to mitigate performance loss under challenging conditions. Concretely, we first design a dynamic update strategy of latency models for the depth estimation task to capture optimization states across training stages. To effectively leverage latency models, we introduce a self-evolution contrastive Loss (SECL) that treats outputs from historical latency models as negative samples. This mechanism adaptively adjusts learning objectives while implicitly sensing weather degradation severity, reducing the needs for manual intervention. Experiments show that our method integrates seamlessly into diverse baseline models and significantly enhances robustness in zero-shot evaluations.

</details>


### [77] [MMCM: Multimodality-aware Metric using Clustering-based Modes for Probabilistic Human Motion Prediction](https://arxiv.org/abs/2511.15179)
*Kyotaro Tokoro,Hiromu Taketsugu,Norimichi Ukita*

Main category: cs.CV

TL;DR: 本文提出了一种新的用于人体运动预测 (HMP) 的指标，该指标考虑了预测运动的覆盖率和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的 HMP 评价指标无法准确评估多模态预测结果，因为它们可能会错误地奖励分布广泛但运动学上无效的预测。

Method: 本文提出了一种基于聚类的多模态感知指标 (MMCM)，该指标将运动空间划分为多个簇，并将每个簇视为一个模态。MMCM 使用这些模态来显式地评估预测运动是否分布在多个模态中，并通过从运动数据集中收集可能的未来运动来识别有效模态。

Result: 实验验证了本文的聚类方法可以产生合理的模态定义，并且 MMCM 可以准确地评估多模态预测。

Conclusion: MMCM 能够更准确地评估多模态人体运动预测结果，解决了现有指标的不足。

Abstract: This paper proposes a novel metric for Human Motion Prediction (HMP). Since a single past sequence can lead to multiple possible futures, a probabilistic HMP method predicts such multiple motions. While a single motion predicted by a deterministic method is evaluated only with the difference from its ground truth motion, multiple predicted motions should also be evaluated based on their distribution. For this evaluation, this paper focuses on the following two criteria. \textbf{(a) Coverage}: motions should be distributed among multiple motion modes to cover diverse possibilities. \textbf{(b) Validity}: motions should be kinematically valid as future motions observable from a given past motion. However, existing metrics simply appreciate widely distributed motions even if these motions are observed in a single mode and kinematically invalid. To resolve these disadvantages, this paper proposes a Multimodality-aware Metric using Clustering-based Modes (MMCM). For (a) coverage, MMCM divides a motion space into several clusters, each of which is regarded as a mode. These modes are used to explicitly evaluate whether predicted motions are distributed among multiple modes. For (b) validity, MMCM identifies valid modes by collecting possible future motions from a motion dataset. Our experiments validate that our clustering yields sensible mode definitions and that MMCM accurately scores multimodal predictions. Code: https://github.com/placerkyo/MMCM

</details>


### [78] [Instruction-Guided Lesion Segmentation for Chest X-rays with Automatically Generated Large-Scale Dataset](https://arxiv.org/abs/2511.15186)
*Geon Choi,Hangyul Yoon,Hyunju Shin,Hyunki Park,Sang Hoon Seo,Eunho Yang,Edward Choi*

Main category: cs.CV

TL;DR: 提出了指令引导的病灶分割 (ILS) 新范例，以根据简单的用户友好指令分割不同的病灶类型。


<details>
  <summary>Details</summary>
Motivation: 当前病灶分割模型在胸部 X 光片 (CXR) 中的适用性受到目标标签数量少和依赖冗长、详细的专家级文本输入的限制，从而给实际应用带来了障碍。

Method: 构建了 MIMIC-ILS，这是第一个用于 CXR 病灶分割的大规模指令-答案数据集，使用全自动多模态管道从胸部 X 光图像及其相应的报告生成注释。引入了 ROSALIA，这是一个在 MIMIC-ILS 上微调的视觉语言模型。

Result: MIMIC-ILS 包含从 192K 图像和 91K 唯一分割掩码中派生的 1.1M 指令-答案对，涵盖七种主要病灶类型。ROSALIA 模型在提出的任务中实现了高分割和文本准确率。

Conclusion: 强调了管道的有效性以及 MIMIC-ILS 作为像素级 CXR 病灶 grounding 的基础资源的价值。

Abstract: The applicability of current lesion segmentation models for chest X-rays (CXRs) has been limited both by a small number of target labels and the reliance on long, detailed expert-level text inputs, creating a barrier to practical use. To address these limitations, we introduce a new paradigm: instruction-guided lesion segmentation (ILS), which is designed to segment diverse lesion types based on simple, user-friendly instructions. Under this paradigm, we construct MIMIC-ILS, the first large-scale instruction-answer dataset for CXR lesion segmentation, using our fully automated multimodal pipeline that generates annotations from chest X-ray images and their corresponding reports. MIMIC-ILS contains 1.1M instruction-answer pairs derived from 192K images and 91K unique segmentation masks, covering seven major lesion types. To empirically demonstrate its utility, we introduce ROSALIA, a vision-language model fine-tuned on MIMIC-ILS. ROSALIA can segment diverse lesions and provide textual explanations in response to user instructions. The model achieves high segmentation and textual accuracy in our newly proposed task, highlighting the effectiveness of our pipeline and the value of MIMIC-ILS as a foundational resource for pixel-level CXR lesion grounding.

</details>


### [79] [BrainRotViT: Transformer-ResNet Hybrid for Explainable Modeling of Brain Aging from 3D sMRI](https://arxiv.org/abs/2511.15188)
*Wasif Jalal,Md Nafiu Rahman,M. Sohel Rahman*

Main category: cs.CV

TL;DR: 这篇论文提出了一种名为BrainRotViT的混合架构，它结合了Vision Transformers (ViT)的全局上下文建模和残差CNN的局部细化，用于从结构MRI中准确估计大脑年龄。


<details>
  <summary>Details</summary>
Motivation: 传统回归和基于CNN的方法存在局限性，例如手动特征工程、有限的感受野以及在异构数据上过度拟合。纯Transformer模型虽然有效，但需要大型数据集和高计算成本。

Method: 该方法首先在辅助年龄和性别分类任务上训练ViT编码器以学习切片级别特征。然后，将冻结的编码器应用于所有矢状切片以生成嵌入向量的2D矩阵，该矩阵被馈送到残差CNN回归器，该回归器在最终全连接层中包含受试者性别以估计连续脑年龄。

Result: 该方法在包含超过130个采集点的11个MRI数据集的验证中，实现了3.34年的MAE（Pearson $r=0.98$，Spearman $ρ=0.97$，R^2=0.95$），优于基线和最先进的模型。它还在4个独立的队列中表现出良好的泛化能力，MAE在3.77到5.04年之间。

Conclusion: 该研究结果表明，该方法为脑年龄预测提供了一个高效、可解释和可推广的框架，弥合了基于CNN和Transformer的方法之间的差距，同时为衰老和神经退行性研究开辟了新的途径。

Abstract: Accurate brain age estimation from structural MRI is a valuable biomarker for studying aging and neurodegeneration. Traditional regression and CNN-based methods face limitations such as manual feature engineering, limited receptive fields, and overfitting on heterogeneous data. Pure transformer models, while effective, require large datasets and high computational cost. We propose Brain ResNet over trained Vision Transformer (BrainRotViT), a hybrid architecture that combines the global context modeling of vision transformers (ViT) with the local refinement of residual CNNs. A ViT encoder is first trained on an auxiliary age and sex classification task to learn slice-level features. The frozen encoder is then applied to all sagittal slices to generate a 2D matrix of embedding vectors, which is fed into a residual CNN regressor that incorporates subject sex at the final fully-connected layer to estimate continuous brain age. Our method achieves an MAE of 3.34 years (Pearson $r=0.98$, Spearman $ρ=0.97$, $R^2=0.95$) on validation across 11 MRI datasets encompassing more than 130 acquisition sites, outperforming baseline and state-of-the-art models. It also generalizes well across 4 independent cohorts with MAEs between 3.77 and 5.04 years. Analyses on the brain age gap (the difference between the predicted age and actual age) show that aging patterns are associated with Alzheimer's disease, cognitive impairment, and autism spectrum disorder. Model attention maps highlight aging-associated regions of the brain, notably the cerebellar vermis, precentral and postcentral gyri, temporal lobes, and medial superior frontal gyrus. Our results demonstrate that this method provides an efficient, interpretable, and generalizable framework for brain-age prediction, bridging the gap between CNN- and transformer-based approaches while opening new avenues for aging and neurodegeneration research.

</details>


### [80] [Insert In Style: A Zero-Shot Generative Framework for Harmonious Cross-Domain Object Composition](https://arxiv.org/abs/2511.15197)
*Raghu Vamsi Chittersu,Yuvraj Singh Rathore,Pranav Adlinge,Kunal Swami*

Main category: cs.CV

TL;DR: 提出了一种新的零样本生成框架，可以在风格化领域中插入真实世界的对象，既实用又保真。


<details>
  <summary>Details</summary>
Motivation: 现有的基于参考的对象组合方法在将真实世界的对象插入风格化领域时会失败，这个问题尚未被充分探索，并且现有方法要么缺乏生成保真度，要么需要不切实际的、针对每个对象的在线微调。

Method: 该框架具有两个关键创新：(i) 一种新的多阶段训练协议，可以分离身份、风格和组合的表示；(ii) 一种专门的掩码注意力架构，可以在生成过程中强制执行这种分离。该方法可以防止通用统一注意力模型中常见的概念干扰。

Result: 该模型在新的公开基准上实现了最先进的性能，在身份和风格指标上均显着优于现有方法，用户研究也强烈证实了这一结果。

Conclusion: Insert In Style 是第一个零样本生成框架，既实用又保真，可以在风格化领域中插入真实世界的对象。

Abstract: Reference-based object composition methods fail when inserting real-world objects into stylized domains. This under-explored problem is currently split between practical "blenders" that lack generative fidelity and "generators" that require impractical, per-subject online finetuning. In this work, we introduce Insert In Style, the first zero-shot generative framework that is both practical and high-fidelity. Our core contribution is a unified framework with two key innovations: (i) a novel multi-stage training protocol that disentangles representations for identity, style, and composition, and (ii) a specialized masked-attention architecture that surgically enforces this disentanglement during generation. This approach prevents the concept interference common in general-purpose, unified-attention models. Our framework is trained on a new 100k sample dataset, curated from a novel data pipeline. This pipeline couples large-scale generation with a rigorous, two-stage filtering process to ensure both high-fidelity semantic identity and style coherence. Unlike prior work, our model is truly zero-shot and requires no text prompts. We also introduce a new public benchmark for stylized composition. We demonstrate state-of-the-art performance, significantly outperforming existing methods on both identity and style metrics, a result strongly corroborated by user studies.

</details>


### [81] [Towards Unbiased Cross-Modal Representation Learning for Food Image-to-Recipe Retrieval](https://arxiv.org/abs/2511.15201)
*Qing Wang,Chong-Wah Ngo,Ee-Peng Lim*

Main category: cs.CV

TL;DR: 本文针对跨模态检索问题中菜谱和食物图像的表征学习的挑战，提出了一种新的方法来解决现有方法中存在的偏差，该偏差来自于将菜谱视为描述菜肴视觉外观的文本源。


<details>
  <summary>Details</summary>
Motivation: 现有方法将菜谱视为描述菜肴视觉外观的文本源，导致图像和菜谱相似性判断产生偏差。食物图像可能无法平等地捕捉菜谱中的每个细节，例如烹饪过程、菜肴呈现和图像捕获条件。

Method: 本文使用因果理论对跨模态表征学习中的偏差进行建模，并将配料视为混淆因素的来源之一，通过简单的后门调整来减轻偏差。通过因果干预，重新构建了传统的食物到菜谱检索模型，并增加了一个额外的项来消除相似性判断中的潜在偏差。

Result: 在Recipe1M数据集上的实验证明，该方法在1K、10K甚至50K的测试数据大小下，检索的oracle性能为MedR=1。

Conclusion: 本文提出了一种即插即用的神经模块，本质上是一个用于去偏的多标签配料分类器，并在Recipe1M数据集上报告了新的最先进的搜索性能。

Abstract: This paper addresses the challenges of learning representations for recipes and food images in the cross-modal retrieval problem. As the relationship between a recipe and its cooked dish is cause-and-effect, treating a recipe as a text source describing the visual appearance of a dish for learning representation, as the existing approaches, will create bias misleading image-and-recipe similarity judgment. Specifically, a food image may not equally capture every detail in a recipe, due to factors such as the cooking process, dish presentation, and image-capturing conditions. The current representation learning tends to capture dominant visual-text alignment while overlooking subtle variations that determine retrieval relevance. In this paper, we model such bias in cross-modal representation learning using causal theory. The causal view of this problem suggests ingredients as one of the confounder sources and a simple backdoor adjustment can alleviate the bias. By causal intervention, we reformulate the conventional model for food-to-recipe retrieval with an additional term to remove the potential bias in similarity judgment. Based on this theory-informed formulation, we empirically prove the oracle performance of retrieval on the Recipe1M dataset to be MedR=1 across the testing data sizes of 1K, 10K, and even 50K. We also propose a plug-and-play neural module, which is essentially a multi-label ingredient classifier for debiasing. New state-of-the-art search performances are reported on the Recipe1M dataset.

</details>


### [82] [Physics-Based Benchmarking Metrics for Multimodal Synthetic Images](https://arxiv.org/abs/2511.15204)
*Kishor Datta Gupta,Marufa Kamal,Md. Mahfuzur Rahman,Fahad Rahman,Mohd Ariful Haque,Sunzida Siddique*

Main category: cs.CV

TL;DR: 提出了一种新的论文评估指标PCMDE，它可以更好地捕捉语义或结构准确性，尤其是在特定领域或上下文相关的场景中。


<details>
  <summary>Details</summary>
Motivation: 现有的评估指标BLEU, CIDEr, VQA score, SigLIP-2 和 CLIPScore 通常无法捕捉语义或结构准确性，尤其是在特定领域或上下文相关的场景中。

Method: 该方法结合了大型语言模型与推理、知识映射和视觉-语言模型。该架构包含三个主要阶段：(1) 通过对象检测和 VLM 进行多模态特征的空间和语义信息特征提取；(2) 用于自适应组件级验证的置信度加权组件融合；(3) 使用大型语言模型进行物理引导推理，以实现结构和关系约束（例如，对齐、位置、一致性）的执行。

Result: 该论文提出了一种新的评估指标，但摘要中没有提及实验结果。

Conclusion: 该论文提出了一种新的评估指标PCMDE，以克服现有评估指标的局限性。

Abstract: Current state of the art measures like BLEU, CIDEr, VQA score, SigLIP-2 and CLIPScore are often unable to capture semantic or structural accuracy, especially for domain-specific or context-dependent scenarios. For this, this paper proposes a Physics-Constrained Multimodal Data Evaluation (PCMDE) metric combining large language models with reasoning, knowledge based mapping and vision-language models to overcome these limitations. The architecture is comprised of three main stages: (1) feature extraction of spatial and semantic information with multimodal features through object detection and VLMs; (2) Confidence-Weighted Component Fusion for adaptive component-level validation; and (3) physics-guided reasoning using large language models for structural and relational constraints (e.g., alignment, position, consistency) enforcement.

</details>


### [83] [SkinGPT-R1: Adapter-Only Dual Distillation for Efficient Dermatology Reasoning](https://arxiv.org/abs/2511.15242)
*Yuhao Shen,Jiahe Qian,Zhangtianyi Chen,Yuanhao He,Juexiao Zhou*

Main category: cs.CV

TL;DR: SkinGPT-R1是一个专注于皮肤科的视觉语言模型，它使诊断性的链式思考推理更加明确和可验证。


<details>
  <summary>Details</summary>
Motivation: 为了支持皮肤特异性的推理，作者构建了DermCoT，这是一个标准化的皮肤科链式思考叙述语料库，它结合了10,000个DermEval过滤的训练案例和3,000个皮肤科医生评分的认证案例。

Method: 作者定义了DermEval作为一个医生对齐的六维度评估器，以及DermBench作为相应的皮肤科链式思考质量的基准。

Result: 在DermBench上，SkinGPT-R1在六个临床医生定义的维度上取得了平均4.031分（满分5分），在所有系统中排名第一，并且比Vision-R1的平均分提高了约41%。

Conclusion: 基于DermCoT的链式思考监督提供了对基础模型的实质性改进，并且添加皮肤科感知的视觉知识提炼可以在叙述质量和识别方面产生持续的额外收益。

Abstract: We present SkinGPT-R1, a dermatology focused vision language model that makes diagnostic chain of thought reasoning explicit, step by step, and verifiable. To support skin specific reasoning, we build DermCoT, a corpus of standardized dermatologic chain of thought narratives that combines 10,000 DermEval filtered training cases with 3,000 dermatologist scored certified cases, and we define DermEval as a physician aligned six dimensional evaluator and DermBench as the corresponding benchmark for dermatologic chain of thought quality. On DermBench, across 14 general, reasoning, and medical vision language models, SkinGPT-R1 achieves an average score of 4.031 out of 5 over the six clinician defined dimensions, ranks 1st among all systems, and improves the average score over Vision-R1 by about 41%. On three dermatology classification benchmarks, SkinGPT-R1 delivers stable accuracy gains over Vision-R1 and remains competitive among strong vision language models. Ablation results further show that DermCoT based chain of thought supervision provides substantial improvements over the base model and that adding dermatology aware visual distillation yields consistent additional gains in both narrative quality and recognition.

</details>


### [84] [SplitFlux: Learning to Decouple Content and Style from a Single Image](https://arxiv.org/abs/2511.15258)
*Yitong Yang,Yinglin Wang,Changshuo Wang,Yongjun Zhang,Ziyang Chen,Shuting He*

Main category: cs.CV

TL;DR: 提出SplitFlux，通过解耦内容和风格来实现定制图像生成，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于SDXL的方法难以实现高质量的结果，而最近提出的Flux模型由于其未被充分探索的特性，未能实现有效的内容-风格分离。

Method: 通过LoRA微调单个Dream Blocks来解耦内容和风格，包括秩约束适应和视觉门控LoRA两个关键组件。

Result: 在各种场景中，SplitFlux始终优于最先进的方法，在内容保持和风格化质量方面表现出色。

Conclusion: SplitFlux通过解耦内容和风格，实现了优于现有技术的定制图像生成。

Abstract: Disentangling image content and style is essential for customized image generation. Existing SDXL-based methods struggle to achieve high-quality results, while the recently proposed Flux model fails to achieve effective content-style separation due to its underexplored characteristics. To address these challenges, we conduct a systematic analysis of Flux and make two key observations: (1) Single Dream Blocks are essential for image generation; and (2) Early single stream blocks mainly control content, whereas later blocks govern style. Based on these insights, we propose SplitFlux, which disentangles content and style by fine-tuning the single dream blocks via LoRA, enabling the disentangled content to be re-embedded into new contexts. It includes two key components: (1) Rank-Constrained Adaptation. To preserve content identity and structure, we compress the rank and amplify the magnitude of updates within specific blocks, preventing content leakage into style blocks. (2) Visual-Gated LoRA. We split the content LoRA into two branches with different ranks, guided by image saliency. The high-rank branch preserves primary subject information, while the low-rank branch encodes residual details, mitigating content overfitting and enabling seamless re-embedding. Extensive experiments demonstrate that SplitFlux consistently outperforms state-of-the-art methods, achieving superior content preservation and stylization quality across diverse scenarios.

</details>


### [85] [Graph Query Networks for Object Detection with Automotive Radar](https://arxiv.org/abs/2511.15271)
*Loveneet Saini,Hasan Tercan,Tobias Meisen*

Main category: cs.CV

TL;DR: 提出了Graph Query Networks (GQN)，一个基于注意力的框架，用于建模雷达感知的物体为图，以提取个性化的关系和上下文特征。


<details>
  <summary>Details</summary>
Motivation: 雷达的长波长产生稀疏和不规则的反射，对传统的基于网格和序列的卷积和Transformer检测器提出了挑战。本文旨在解决3D雷达物体检测的问题，实现360度汽车感知。

Method: GQN采用图查询的新概念来动态地关注鸟瞰图（BEV）空间，构建由两个新模块处理的特定于对象的图：用于关系推理的EdgeFocus和用于上下文聚合的DeepContext Pooling。

Result: 在NuScenes数据集上，GQN将相对mAP提高了高达+53%，包括比之前最强的雷达方法高出+8.2%的增益，同时以适度的FLOPs成本将峰值图构建开销降低了80%。

Conclusion: GQN在雷达物体检测方面表现出色，并在效率方面有所提升。

Abstract: Object detection with 3D radar is essential for 360-degree automotive perception, but radar's long wavelengths produce sparse and irregular reflections that challenge traditional grid and sequence-based convolutional and transformer detectors. This paper introduces Graph Query Networks (GQN), an attention-based framework that models objects sensed by radar as graphs, to extract individualized relational and contextual features. GQN employs a novel concept of graph queries to dynamically attend over the bird's-eye view (BEV) space, constructing object-specific graphs processed by two novel modules: EdgeFocus for relational reasoning and DeepContext Pooling for contextual aggregation. On the NuScenes dataset, GQN improves relative mAP by up to +53%, including a +8.2% gain over the strongest prior radar method, while reducing peak graph construction overhead by 80% with moderate FLOPs cost.

</details>


### [86] [Edge-Centric Relational Reasoning for 3D Scene Graph Prediction](https://arxiv.org/abs/2511.15288)
*Yanni Ma,Hao Liu,Yulan Guo,Theo Gevers,Martin R. Oswald*

Main category: cs.CV

TL;DR: 提出了一种新的3D场景图预测框架，通过从关系层面到对象层面的推理，更好地捕捉高阶关系依赖。


<details>
  <summary>Details</summary>
Motivation: 现有的方法难以捕捉高阶关系依赖，限制了关系预测的准确性。

Method: 提出了一个Link-guided Edge-centric relational reasoning框架，首先预测对象对之间的潜在连接以抑制不相关的边缘，然后将原始场景图转换为线图，并在其上进行边中心的关系推理。

Result: 在3DSSG数据集上，与两个baseline相比，实验结果显示出持续的改进。

Conclusion: 该框架是模型无关的，可以与任何现有的以对象为中心的方法集成，并证明了其边到对象推理范式的有效性。

Abstract: 3D scene graph prediction aims to abstract complex 3D environments into structured graphs consisting of objects and their pairwise relationships. Existing approaches typically adopt object-centric graph neural networks, where relation edge features are iteratively updated by aggregating messages from connected object nodes. However, this design inherently restricts relation representations to pairwise object context, making it difficult to capture high-order relational dependencies that are essential for accurate relation prediction. To address this limitation, we propose a Link-guided Edge-centric relational reasoning framework with Object-aware fusion, namely LEO, which enables progressive reasoning from relation-level context to object-level understanding. Specifically, LEO first predicts potential links between object pairs to suppress irrelevant edges, and then transforms the original scene graph into a line graph where each relation is treated as a node. A line graph neural network is applied to perform edge-centric relational reasoning to capture inter-relation context. The enriched relation features are subsequently integrated into the original object-centric graph to enhance object-level reasoning and improve relation prediction. Our framework is model-agnostic and can be integrated with any existing object-centric method. Experiments on the 3DSSG dataset with two competitive baselines show consistent improvements, highlighting the effectiveness of our edge-to-object reasoning paradigm.

</details>


### [87] [Taming Generative Synthetic Data for X-ray Prohibited Item Detection](https://arxiv.org/abs/2511.15299)
*Jialong Sun,Hongguang Zhu,Weizhe Liu,Yunda Sun,Renshuai Tao,Yunchao Wei*

Main category: cs.CV

TL;DR: 提出了一种基于文本到图像生成的一阶段X射线安检图像合成管道（Xsyn），以解决数据不足的问题，无需额外的人工成本即可实现高质量的X射线安检图像合成。


<details>
  <summary>Details</summary>
Motivation: 训练违禁品检测模型需要大量的X射线安检图像，但收集和标注这些图像既耗时又费力。为了解决数据不足的问题，以往的X射线安检图像合成方法主要采用两阶段流程，效率不高，且引入了额外的人工成本。

Method: 该方法基于文本到图像生成，并结合了两种有效策略：交叉注意力细化（CAR）策略和背景遮挡建模（BOM）策略。

Result: 实验结果表明，该方法优于以往所有方法，mAP提高了1.2%，并且该方法生成的合成图像有利于提高各种X射线安全数据集和检测器的违禁品检测性能。

Conclusion: Xsyn是第一个无需额外人工成本即可实现高质量X射线安检图像合成的方法，并且性能优于现有方法。

Abstract: Training prohibited item detection models requires a large amount of X-ray security images, but collecting and annotating these images is time-consuming and laborious. To address data insufficiency, X-ray security image synthesis methods composite images to scale up datasets. However, previous methods primarily follow a two-stage pipeline, where they implement labor-intensive foreground extraction in the first stage and then composite images in the second stage. Such a pipeline introduces inevitable extra labor cost and is not efficient. In this paper, we propose a one-stage X-ray security image synthesis pipeline (Xsyn) based on text-to-image generation, which incorporates two effective strategies to improve the usability of synthetic images. The Cross-Attention Refinement (CAR) strategy leverages the cross-attention map from the diffusion model to refine the bounding box annotation. The Background Occlusion Modeling (BOM) strategy explicitly models background occlusion in the latent space to enhance imaging complexity. To the best of our knowledge, compared with previous methods, Xsyn is the first to achieve high-quality X-ray security image synthesis without extra labor cost. Experiments demonstrate that our method outperforms all previous methods with 1.2% mAP improvement, and the synthetic images generated by our method are beneficial to improve prohibited item detection performance across various X-ray security datasets and detectors. Code is available at https://github.com/pILLOW-1/Xsyn/.

</details>


### [88] [Text2Loc++: Generalizing 3D Point Cloud Localization from Natural Language](https://arxiv.org/abs/2511.15308)
*Yan Xia,Letian Shi,Yilin Di,Joao F. Henriques,Daniel Cremers*

Main category: cs.CV

TL;DR: Text2Loc++是一个用于使用自然语言描述定位3D点云子地图的新型神经网络，它采用粗到精的定位流程，并在KITTI360Pose数据集上优于现有方法15%。


<details>
  <summary>Details</summary>
Motivation: 利用复杂的自然语言描述定位3D点云子地图。

Method: 提出Text2Loc++，结合了预训练语言模型与分层Transformer以及最大池化（HTM），并采用基于注意力的点云编码器。提出了掩码实例训练（MIT）来过滤掉未对齐的对象，并引入了模态感知分层对比学习（MHCL）。在精细定位阶段，设计了一个基于原型地图克隆（PMC）和级联交叉注意力Transformer（CCAT）的轻量级框架。

Result: 在KITTI360Pose数据集上，Text2Loc++优于现有方法15%。

Conclusion: Text2Loc++模型在新的数据集上表现出强大的泛化能力，能够有效地处理复杂的语言表达和各种城市环境。

Abstract: We tackle the problem of localizing 3D point cloud submaps using complex and diverse natural language descriptions, and present Text2Loc++, a novel neural network designed for effective cross-modal alignment between language and point clouds in a coarse-to-fine localization pipeline. To support benchmarking, we introduce a new city-scale dataset covering both color and non-color point clouds from diverse urban scenes, and organize location descriptions into three levels of linguistic complexity. In the global place recognition stage, Text2Loc++ combines a pretrained language model with a Hierarchical Transformer with Max pooling (HTM) for sentence-level semantics, and employs an attention-based point cloud encoder for spatial understanding. We further propose Masked Instance Training (MIT) to filter out non-aligned objects and improve multimodal robustness. To enhance the embedding space, we introduce Modality-aware Hierarchical Contrastive Learning (MHCL), incorporating cross-modal, submap-, text-, and instance-level losses. In the fine localization stage, we completely remove explicit text-instance matching and design a lightweight yet powerful framework based on Prototype-based Map Cloning (PMC) and a Cascaded Cross-Attention Transformer (CCAT). Extensive experiments on the KITTI360Pose dataset show that Text2Loc++ outperforms existing methods by up to 15%. In addition, the proposed model exhibits robust generalization when evaluated on the new dataset, effectively handling complex linguistic expressions and a wide variety of urban environments. The code and dataset will be made publicly available.

</details>


### [89] [Adapt-As-You-Walk Through the Clouds: Training-Free Online Test-Time Adaptation of 3D Vision-Language Foundation Models](https://arxiv.org/abs/2511.15311)
*Mehran Tamjidi,Hamidreza Dastmalchi,Mohammadreza Alimoradijazi,Ali Cheraghian,Aijun An,Morteza Saberi*

Main category: cs.CV

TL;DR: Uni-Adapter is a training-free test-time adaptation strategy for 3D Vision-Language Foundation Models (VLFMs) that improves performance in noisy, incomplete, or out-of-distribution data.


<details>
  <summary>Details</summary>
Motivation: 3D VLFMs underperform in practical scenarios with noisy, incomplete, or out-of-distribution data.

Method: Uni-Adapter uses dynamic prototype learning with a 3D cache to store class-specific cluster centers, a graph-based label smoothing module, and entropy-weighted aggregation.

Result: Uni-Adapter achieves state-of-the-art performance on diverse 3D benchmarks, improving ModelNet-40C by 10.55%, ScanObjectNN-C by 8.26%, and ShapeNet-C by 4.49%.

Conclusion: Uni-Adapter effectively mitigates distribution shifts in 3D VLFMs without retraining.

Abstract: 3D Vision-Language Foundation Models (VLFMs) have shown strong generalization and zero-shot recognition capabilities in open-world point cloud processing tasks. However, these models often underperform in practical scenarios where data are noisy, incomplete, or drawn from a different distribution than the training data. To address this, we propose Uni-Adapter, a novel training-free online test-time adaptation (TTA) strategy for 3D VLFMs based on dynamic prototype learning. We define a 3D cache to store class-specific cluster centers as prototypes, which are continuously updated to capture intra-class variability in heterogeneous data distributions. These dynamic prototypes serve as anchors for cache-based logit computation via similarity scoring. Simultaneously, a graph-based label smoothing module captures inter-prototype similarities to enforce label consistency among similar prototypes. Finally, we unify predictions from the original 3D VLFM and the refined 3D cache using entropy-weighted aggregation for reliable adaptation. Without retraining, Uni-Adapter effectively mitigates distribution shifts, achieving state-of-the-art performance on diverse 3D benchmarks over different 3D VLFMs, improving ModelNet-40C by 10.55%, ScanObjectNN-C by 8.26%, and ShapeNet-C by 4.49% over the source 3D VLFMs.

</details>


### [90] [A Multimodal Transformer Approach for UAV Detection and Aerial Object Recognition Using Radar, Audio, and Video Data](https://arxiv.org/abs/2511.15312)
*Mauro Larrat,Claudomiro Sales*

Main category: cs.CV

TL;DR: 提出了一种新型多模态Transformer模型，用于无人机检测和空中物体识别。


<details>
  <summary>Details</summary>
Motivation: 现有单模态方法存在局限性，现代监控和安全需要更强大的系统。

Method: 设计并评估了一个集成了雷达、可见光视频(RGB)、红外视频(IR)和音频的多模态Transformer模型，利用Transformer的自注意力机制融合不同模态的特征。

Result: 在独立测试集上表现出色，宏平均指标为：准确率0.9812，召回率0.9873，精确率0.9787，F1-score 0.9826，特异性0.9954。计算分析表明其效率很高，具有1.09 GFLOPs，1.22百万个参数和41.11 FPS的推理速度。

Conclusion: 通过Transformer架构进行多模态数据融合，在空中物体分类方面取得了显著进展，为复杂空域中的无人机检测和监控提供了一种高度准确且具有弹性的解决方案。

Abstract: Unmanned aerial vehicle (UAV) detection and aerial object recognition are critical for modern surveillance and security, prompting a need for robust systems that overcome limitations of single-modality approaches. This research addresses these challenges by designing and rigorously evaluating a novel multimodal Transformer model that integrates diverse data streams: radar, visual band video (RGB), infrared (IR) video, and audio. The architecture effectively fuses distinct features from each modality, leveraging the Transformer's self-attention mechanisms to learn comprehensive, complementary, and highly discriminative representations for classification. The model demonstrated exceptional performance on an independent test set, achieving macro-averaged metrics of 0.9812 accuracy, 0.9873 recall, 0.9787 precision, 0.9826 F1-score, and 0.9954 specificity. Notably, it exhibited particularly high precision and recall in distinguishing drones from other aerial objects. Furthermore, computational analysis confirmed its efficiency, with 1.09 GFLOPs, 1.22 million parameters, and an inference speed of 41.11 FPS, highlighting its suitability for real-time applications. This study presents a significant advancement in aerial object classification, validating the efficacy of multimodal data fusion via a Transformer architecture for achieving state-of-the-art performance, thereby offering a highly accurate and resilient solution for UAV detection and monitoring in complex airspace.

</details>


### [91] [What Your Features Reveal: Data-Efficient Black-Box Feature Inversion Attack for Split DNNs](https://arxiv.org/abs/2511.15316)
*Zhihan Ren,Lijun He,Jiaxi Liang,Xinzhu Fu,Haixia Bi,Fan Li*

Main category: cs.CV

TL;DR: 提出了FIA-Flow，一个黑盒特征反演攻击框架，可以从中间特征中实现高保真图像重建。


<details>
  <summary>Details</summary>
Motivation: 现有的特征反演攻击方法重建质量有限，难以评估隐私泄露的程度。为了揭示泄露特征的隐私风险。

Method: 设计了潜在特征空间对齐模块（LFSAM）来弥合中间特征空间和潜在空间之间的语义差距；开发了确定性反演流匹配（DIFM），通过一步推理将非流形特征投影到目标流形上。

Result: FIA-Flow在各种模型和层上实现了更忠实和语义对齐的特征反演，揭示了Split DNN中比以前认为的更严重的隐私威胁。

Conclusion: FIA-Flow能够更有效地进行特征反演攻击，从而揭示Split DNN中更严重的隐私威胁。

Abstract: Split DNNs enable edge devices by offloading intensive computation to a cloud server, but this paradigm exposes privacy vulnerabilities, as the intermediate features can be exploited to reconstruct the private inputs via Feature Inversion Attack (FIA). Existing FIA methods often produce limited reconstruction quality, making it difficult to assess the true extent of privacy leakage. To reveal the privacy risk of the leaked features, we introduce FIA-Flow, a black-box FIA framework that achieves high-fidelity image reconstruction from intermediate features. To exploit the semantic information within intermediate features, we design a Latent Feature Space Alignment Module (LFSAM) to bridge the semantic gap between the intermediate feature space and the latent space. Furthermore, to rectify distributional mismatch, we develop Deterministic Inversion Flow Matching (DIFM), which projects off-manifold features onto the target manifold with one-step inference. This decoupled design simplifies learning and enables effective training with few image-feature pairs. To quantify privacy leakage from a human perspective, we also propose two metrics based on a large vision-language model. Experiments show that FIA-Flow achieves more faithful and semantically aligned feature inversion across various models (AlexNet, ResNet, Swin Transformer, DINO, and YOLO11) and layers, revealing a more severe privacy threat in Split DNNs than previously recognized.

</details>


### [92] [Adaptive thresholding pattern for fingerprint forgery detection](https://arxiv.org/abs/2511.15322)
*Zahra Farzadpour,Masoumeh Azghani*

Main category: cs.CV

TL;DR: 提出了一种基于自适应阈值模式的指纹伪造检测算法，该算法对各种失真具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 指纹识别系统容易受到欺骗攻击，因此需要开发区分真假指纹的技术。

Method: 该方法通过小波变换和自适应阈值处理提取特征，并使用SVM分类器进行分类。该方法还研究了各种失真（如像素缺失、块缺失和噪声污染）的影响。

Result: 该方法在像素缺失和块缺失的情况下，准确率分别比同类方法高约8%和5%。

Conclusion: 该方法对环境现象或恶意用户造成的各种失真具有更强的抵抗力。

Abstract: Fingerprint liveness detection systems have been affected by spoofing, which is a severe threat for fingerprint-based biometric systems. Therefore, it is crucial to develop some techniques to distinguish the fake fingerprints from the real ones. The software based techniques can detect the fingerprint forgery automatically. Also, the scheme shall be resistant against various distortions such as noise contamination, pixel missing and block missing, so that the forgers cannot deceive the detector by adding some distortions to the faked fingerprint. In this paper, we propose a fingerprint forgery detection algorithm based on a suggested adaptive thresholding pattern. The anisotropic diffusion of the input image is passed through three levels of the wavelet transform. The coefficients of different layers are adaptively thresholded and concatenated to produce the feature vector which is classified using the SVM classifier. Another contribution of the paper is to investigate the effect of various distortions such as pixel missing, block missing, and noise contamination. Our suggested approach includes a novel method that exhibits improved resistance against a range of distortions caused by environmental phenomena or manipulations by malicious users. In quantitative comparisons, our proposed method outperforms its counterparts by approximately 8% and 5% in accuracy for missing pixel scenarios of 90% and block missing scenarios of size 70x70 , respectively. This highlights the novelty approach in addressing such challenges.

</details>


### [93] [Fast Post-Hoc Confidence Fusion for 3-Class Open-Set Aerial Object Detection](https://arxiv.org/abs/2511.15343)
*Spyridon Loukovitis,Vasileios Karampinis,Athanasios Voulodimos*

Main category: cs.CV

TL;DR: 提出了一种轻量级的、与模型无关的后处理框架，用于区分训练中见过的物体和以前未见过的物体，同时将背景与未知物体显式分离，实现ID目标、OOD物体和背景之间的实时三向分类。


<details>
  <summary>Details</summary>
Motivation: 可靠的无人机导航系统需要鲁棒的空对空物体检测器，能够区分训练中见过的物体和以前未见过的物体。现有的开放集方法通常依赖于单一的不确定性评分与阈值，限制了灵活性，并且经常将OOD物体与背景杂波混淆。

Method: 采用一种融合方案，使用紧凑的多层感知器(MLP)聚合多个置信度估计和每次检测的特征。将不同的logits变体合并到MLP中，持续提高二元和三类分类的性能，且不影响吞吐量。

Result: 在二类分类中，该方法超过了基于阈值的基线，平均AUROC提高了2.7%，同时保留或提高了开放集mAP。在AUROC方面超过了竞争技术，同时封闭集mAP提高了9个点，相对增益为18%。

Conclusion: 该研究独特地实现了鲁棒的三类分类，这对于安全的无人机导航至关重要，在这种导航中，必须主动避开OOD物体，并且安全地忽略背景区域。

Abstract: Developing reliable UAV navigation systems requires robust air-to-air object detectors capable of distinguishing between objects seen during training and previously unseen objects. While many methods address closed-set detection and achieve high-confidence recognition of in-domain (ID) targets, they generally do not tackle open-set detection, which requires simultaneous handling of both ID and out-of-distribution (OOD) objects. Existing open-set approaches typically rely on a single uncertainty score with thresholding, limiting flexibility and often conflating OOD objects with background clutter. In contrast, we propose a lightweight, model-agnostic post-processing framework that explicitly separates background from unknown objects while preserving the base detector's performance. Our approach extends open-set detection beyond binary ID/OOD classification to real-time three-way classification among ID targets, OOD objects, and background. To this end, we employ a fusion scheme that aggregates multiple confidence estimates and per-detection features using a compact multilayer perceptron (MLP). Incorporating different logit variants into the MLP consistently enhances performance across both binary and three-class classification without compromising throughput. Extensive ablation and comparative experiments confirm that our method surpasses threshold-based baselines in two-class classification by an average of 2.7% AUROC, while retaining or improving open-set mAP. Furthermore, our study uniquely enables robust three-class classification, a critical capability for safe UAV navigation, where OOD objects must be actively avoided and background regions safely ignored. Comparative analysis highlights that our method surpasses competitive techniques in AUROC across datasets, while improving closed-set mAP by up to 9 points, an 18% relative gain.

</details>


### [94] [IPTQ-ViT: Post-Training Quantization of Non-linear Functions for Integer-only Vision Transformers](https://arxiv.org/abs/2511.15369)
*Gihwan Kim,Jemin Lee,Hyungshin Kim*

Main category: cs.CV

TL;DR: 提出了一种新的名为IPTQ-ViT的无需重训练的Vision Transformer全整数量化框架。


<details>
  <summary>Details</summary>
Motivation: 以往的视觉Transformer量化感知训练（QAT）方法依赖于昂贵的重训练来恢复非线性层量化中的精度损失，限制了它们在资源受限环境中的使用。现有的PTQ方法要么部分量化非线性函数，要么调整激活分布以保持精度，但未能实现完全的整数推理。

Method: 提出了近似函数：一种针对视觉数据优化的基于多项式的GELU和一种基于位移的Softmax，旨在提高PTQ中的近似精度。此外，我们提出了一个统一的指标，整合了量化敏感性、扰动和计算成本，以选择每个激活层的最佳近似函数。

Result: IPTQ-ViT优于以往的PTQ方法，图像分类的top-1精度提高了高达6.44%p（平均1.78%p），目标检测的mAP提高了1.0。在W8A8和W4A8下，IPTQ-ViT优于部分浮点PTQ方法，并实现了与整数QAT方法相当的精度和延迟。

Conclusion: IPTQ-ViT在视觉Transformer的全整数量化上取得了显著的成果，无需重训练即可实现高性能。

Abstract: Previous Quantization-Aware Training (QAT) methods for vision transformers rely on expensive retraining to recover accuracy loss in non-linear layer quantization, limiting their use in resource-constrained environments. In contrast, existing Post-Training Quantization (PTQ) methods either partially quantize non-linear functions or adjust activation distributions to maintain accuracy but fail to achieve fully integer-only inference. In this paper, we introduce IPTQ-ViT, a novel PTQ framework for fully integer-only vision transformers without retraining. We present approximation functions: a polynomial-based GELU optimized for vision data and a bit-shifting-based Softmax designed to improve approximation accuracy in PTQ. In addition, we propose a unified metric integrating quantization sensitivity, perturbation, and computational cost to select the optimal approximation function per activation layer. IPTQ-ViT outperforms previous PTQ methods, achieving up to 6.44\%p (avg. 1.78\%p) top-1 accuracy improvement for image classification, 1.0 mAP for object detection. IPTQ-ViT outperforms partial floating-point PTQ methods under W8A8 and W4A8, and achieves accuracy and latency comparable to integer-only QAT methods. We plan to release our code https://github.com/gihwan-kim/IPTQ-ViT.git.

</details>


### [95] [Zero-Shot Open-Vocabulary Human Motion Grounding with Test-Time Training](https://arxiv.org/abs/2511.15379)
*Yunjiao Zhou,Xinyan Chen,Junlang Qian,Lihua Xie,Jianfei Yang*

Main category: cs.CV

TL;DR: 提出了一个名为 ZOMG 的零样本、开放词汇框架，用于将运动序列分割成具有语义意义的子动作，无需任何注释或微调。


<details>
  <summary>Details</summary>
Motivation: 理解复杂的人类活动需要将运动分解为细粒度的、语义对齐的子动作的能力。这种运动 grounding 过程对于行为分析、具身 AI 和虚拟现实至关重要。然而，大多数现有方法依赖于具有预定义动作类的大量监督，这在开放词汇、真实世界的环境中是不可行的。

Method: ZOMG 整合了 (1) 语言语义分割，它利用大型语言模型将指令分解为有序的子动作单元，以及 (2) 软掩蔽优化，它学习特定于实例的时间掩码，以关注对子动作至关重要的帧，同时保持段内连续性并加强段间分离，所有这些都不改变预训练的编码器。

Result: 在三个运动-语言数据集上的实验表明，该方法在运动 grounding 性能方面具有最先进的有效性和效率，在 HumanML3D 基准测试中，其 mAP 比先前的方法高出 +8.7%。

Conclusion: 同时，下游检索也存在显着改进，为无注释运动理解建立了一个新的范例。

Abstract: Understanding complex human activities demands the ability to decompose motion into fine-grained, semantic-aligned sub-actions. This motion grounding process is crucial for behavior analysis, embodied AI and virtual reality. Yet, most existing methods rely on dense supervision with predefined action classes, which are infeasible in open-vocabulary, real-world settings. In this paper, we propose ZOMG, a zero-shot, open-vocabulary framework that segments motion sequences into semantically meaningful sub-actions without requiring any annotations or fine-tuning. Technically, ZOMG integrates (1) language semantic partition, which leverages large language models to decompose instructions into ordered sub-action units, and (2) soft masking optimization, which learns instance-specific temporal masks to focus on frames critical to sub-actions, while maintaining intra-segment continuity and enforcing inter-segment separation, all without altering the pretrained encoder. Experiments on three motion-language datasets demonstrate state-of-the-art effectiveness and efficiency of motion grounding performance, outperforming prior methods by +8.7\% mAP on HumanML3D benchmark. Meanwhile, significant improvements also exist in downstream retrieval, establishing a new paradigm for annotation-free motion understanding.

</details>


### [96] [Breaking Expert Knowledge Limits: Self-Pruning for Large Language Models](https://arxiv.org/abs/2511.15390)
*Haidong Kang,Lihong Lin,Enneng Yang,Hongning Dai,Hao Wang*

Main category: cs.CV

TL;DR: 提出了一种名为AutoPrune的新型剪枝方法，该方法利用LLM自动设计最佳剪枝算法，无需任何专业知识。


<details>
  <summary>Details</summary>
Motivation: 现有LLM剪枝方法依赖于手动设计的剪枝算法，导致巨大的人工成本并需要专家知识。此外，作者首次发现均匀稀疏性导致高剪枝率下性能急剧下降背后的严重异常值问题，提出了如何设计适用于LLM的自适应剪枝稀疏性的额外问题。

Method: 提出了一个Graph-driven Chain-of-Thought (GCoT)来优化提示，从而显著增强了学习剪枝算法的推理过程，并使我们能够在下一代中生成具有卓越性能和可解释性的剪枝算法。基于对异常值问题的深刻理解，引入了Skew-aware Dynamic Sparsity Allocation (SDSA) 来克服异常值问题，减轻了高剪枝率下的性能下降。

Result: 在主流LLM基准上进行了广泛的实验，证明了AutoPrune的优越性，AutoPrune始终优于最先进的竞争对手。

Conclusion: AutoPrune 是一种有前景的 LLM 剪枝方法，它克服了现有方法的局限性，并在各种基准测试中实现了最先进的结果。

Abstract: Large language models (LLMs) have achieved remarkable performance on a wide range of tasks, hindering real-world deployment due to their massive size. Existing pruning methods (e.g., Wanda) tailored for LLMs rely heavily on manual design pruning algorithms, thereby leading to \textit{huge labor costs} and \textit{requires expert knowledge}. Furthermore, we are the first to identify the serious \textit{outlier value issue} behind dramatic performance degradation under high pruning ratios that are caused by uniform sparsity, raising an additional concern about how to design adaptive pruning sparsity ideal for LLMs. Can LLMs prune by themselves? In this work, we introduce an affirmative answer by proposing a novel pruning method called \textbf{AutoPrune}, which first overcomes expert knowledge limits by leveraging LLMs to design optimal pruning algorithms for themselves automatically without any expert knowledge. Specifically, to mitigate the black-box nature of LLMs, we propose a Graph-driven Chain-of-Thought (GCoT) to optimize prompts, significantly enhancing the reasoning process in learning the pruning algorithm and enabling us to generate pruning algorithms with superior performance and interpretability in the next generation. Finally, grounded in insights of outlier value issue, we introduce Skew-aware Dynamic Sparsity Allocation (SDSA) to overcome the outlier value issue, mitigating performance degradation under high pruning ratios. We conduct extensive experiments on mainstream LLMs benchmarks, demonstrating the superiority of AutoPrune, which consistently excels state-of-the-art competitors. The code is available at: https://anonymous.4open.science/r/AutoPrune.

</details>


### [97] [ShelfOcc: Native 3D Supervision beyond LiDAR for Vision-Based Occupancy Estimation](https://arxiv.org/abs/2511.15396)
*Simon Boeder,Fabian Gigengack,Simon Roesler,Holger Caesar,Benjamin Risse*

Main category: cs.CV

TL;DR: ShelfOcc是一种仅使用视觉的方法，它通过从视频生成度量一致的语义体素标签，将监督引入到原生 3D 空间，从而实现真正的 3D 监督，而无需任何额外的传感器或手动 3D 注释。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督和弱监督 occupancy estimation 方法依赖于 2D 投影或渲染的监督，存在几何不一致和严重的深度出血问题。

Method: 该方法通过过滤和积累跨帧的静态几何体，处理动态内容，并将语义信息传播到稳定的体素表示中，从而缓解这些问题。

Result: 在 Occ3D-nuScenes 基准测试中，ShelfOcc 显著优于所有以前的弱监督/自监督方法（高达 34% 的相对改进）。

Conclusion: 这种高质量的监督对于鲁棒的 occupancy learning 至关重要，并且构成架构创新的重要补充途径。

Abstract: Recent progress in self- and weakly supervised occupancy estimation has largely relied on 2D projection or rendering-based supervision, which suffers from geometric inconsistencies and severe depth bleeding. We thus introduce ShelfOcc, a vision-only method that overcomes these limitations without relying on LiDAR. ShelfOcc brings supervision into native 3D space by generating metrically consistent semantic voxel labels from video, enabling true 3D supervision without any additional sensors or manual 3D annotations. While recent vision-based 3D geometry foundation models provide a promising source of prior knowledge, they do not work out of the box as a prediction due to sparse or noisy and inconsistent geometry, especially in dynamic driving scenes. Our method introduces a dedicated framework that mitigates these issues by filtering and accumulating static geometry consistently across frames, handling dynamic content and propagating semantic information into a stable voxel representation. This data-centric shift in supervision for weakly/shelf-supervised occupancy estimation allows the use of essentially any SOTA occupancy model architecture without relying on LiDAR data. We argue that such high-quality supervision is essential for robust occupancy learning and constitutes an important complementary avenue to architectural innovation. On the Occ3D-nuScenes benchmark, ShelfOcc substantially outperforms all previous weakly/shelf-supervised methods (up to a 34% relative improvement), establishing a new data-driven direction for LiDAR-free 3D scene understanding.

</details>


### [98] [Controlling False Positives in Image Segmentation via Conformal Prediction](https://arxiv.org/abs/2511.15406)
*Luca Mossina,Corentin Friedrich*

Main category: cs.CV

TL;DR: 提出了一种简单的事后框架，用于构建具有图像级假阳性预测控制的置信度掩模。


<details>
  <summary>Details</summary>
Motivation: 深度模型很少提供关于其错误的明确统计保证，而可靠的语义分割对于临床决策至关重要。

Method: 定义了一个收缩掩模的嵌套族，通过增加分数阈值或应用形态侵蚀获得。使用标记的校准集，通过共形预测选择单个收缩参数。

Result: 在息肉分割基准上的实验表明了目标水平的经验有效性。

Conclusion: 该框架实现了在过度分割可能产生临床后果的场景中进行实际的、风险感知的分割。

Abstract: Reliable semantic segmentation is essential for clinical decision making, yet deep models rarely provide explicit statistical guarantees on their errors. We introduce a simple post-hoc framework that constructs confidence masks with distribution-free, image-level control of false-positive predictions. Given any pretrained segmentation model, we define a nested family of shrunken masks obtained either by increasing the score threshold or by applying morphological erosion. A labeled calibration set is used to select a single shrink parameter via conformal prediction, ensuring that, for new images that are exchangeable with the calibration data, the proportion of false positives retained in the confidence mask stays below a user-specified tolerance with high probability. The method is model-agnostic, requires no retraining, and provides finite-sample guarantees regardless of the underlying predictor. Experiments on a polyp-segmentation benchmark demonstrate target-level empirical validity. Our framework enables practical, risk-aware segmentation in settings where over-segmentation can have clinical consequences. Code at https://github.com/deel-ai-papers/conseco.

</details>


### [99] [D4C: Data-free Quantization for Contrastive Language-Image Pre-training Models](https://arxiv.org/abs/2511.15411)
*Wenlun Zhang,Yunshan Zhong,Zihao Ding,Xinyu Li,Kentaro Yoshioka*

Main category: cs.CV

TL;DR: D4C is the first Data-Free Quantization (DFQ) framework tailored for CLIP, which synthesizes semantically rich and structurally diverse pseudo images through Prompt-Guided Semantic Injection, Structural Contrastive Generation, and Perturbation-Aware Enhancement.


<details>
  <summary>Details</summary>
Motivation: Existing DFQ techniques applied to CLIP result in substantial performance degradation due to insufficient semantic content and low intra-image diversity in synthesized samples.

Method: D4C synthesizes semantically rich and structurally diverse pseudo images through three key components: (1) Prompt-Guided Semantic Injection, (2) Structural Contrastive Generation, and (3) Perturbation-Aware Enhancement.

Result: D4C achieves significant performance improvements on various bit-widths and models. For example, under the W4A8 setting with CLIP ResNet-50 and ViT-B/32, D4C achieves Top-1 accuracy improvement of 12.4% and 18.9% on CIFAR-10, 6.8% and 19.7% on CIFAR-100, and 1.4% and 5.7% on ImageNet-1K in zero-shot classification, respectively.

Conclusion: D4C effectively bridges the performance gap of DFQ on CLIP by generating images that are both semantically informative and structurally diverse.

Abstract: Data-Free Quantization (DFQ) offers a practical solution for model compression without requiring access to real data, making it particularly attractive in privacy-sensitive scenarios. While DFQ has shown promise for unimodal models, its extension to Vision-Language Models such as Contrastive Language-Image Pre-training (CLIP) models remains underexplored. In this work, we reveal that directly applying existing DFQ techniques to CLIP results in substantial performance degradation due to two key limitations: insufficient semantic content and low intra-image diversity in synthesized samples. To tackle these challenges, we propose D4C, the first DFQ framework tailored for CLIP. D4C synthesizes semantically rich and structurally diverse pseudo images through three key components: (1) Prompt-Guided Semantic Injection aligns generated images with real-world semantics using text prompts; (2) Structural Contrastive Generation reproduces compositional structures of natural images by leveraging foreground-background contrastive synthesis; and (3) Perturbation-Aware Enhancement applies controlled perturbations to improve sample diversity and robustness. These components jointly empower D4C to synthesize images that are both semantically informative and structurally diverse, effectively bridging the performance gap of DFQ on CLIP. Extensive experiments validate the effectiveness of D4C, showing significant performance improvements on various bit-widths and models. For example, under the W4A8 setting with CLIP ResNet-50 and ViT-B/32, D4C achieves Top-1 accuracy improvement of 12.4% and 18.9% on CIFAR-10, 6.8% and 19.7% on CIFAR-100, and 1.4% and 5.7% on ImageNet-1K in zero-shot classification, respectively.

</details>


### [100] [WarNav: An Autonomous Driving Benchmark for Segmentation of Navigable Zones in War Scenes](https://arxiv.org/abs/2511.15429)
*Marc-Emmanuel Coupvent des Graviers,Hejer Ammar,Christophe Guettier,Yann Dumortier,Romaric Audigier*

Main category: cs.CV

TL;DR: WarNav: A new dataset for semantic segmentation models in autonomous ground vehicle navigation, designed for unstructured, conflict-affected environments.


<details>
  <summary>Details</summary>
Motivation: Lack of datasets for autonomous vehicle navigation in hazardous war-zone environments.

Method: Construction of the WarNav dataset from DATTALION repository images; baseline testing with state-of-the-art semantic segmentation models.

Result: Baseline results on WarNav using models trained on urban scenes, analysis of training data impact, and a preliminary approach to navigability without target image annotations.

Conclusion: WarNav dataset promotes research on robust and safe autonomous vehicles in high-risk scenarios with limited annotated data.

Abstract: We introduce WarNav, a novel real-world dataset constructed from images of the open-source DATTALION repository, specifically tailored to enable the development and benchmarking of semantic segmentation models for autonomous ground vehicle navigation in unstructured, conflict-affected environments. This dataset addresses a critical gap between conventional urban driving resources and the unique operational scenarios encountered by unmanned systems in hazardous and damaged war-zones. We detail the methodological challenges encountered, ranging from data heterogeneity to ethical considerations, providing guidance for future efforts that target extreme operational contexts. To establish performance references, we report baseline results on WarNav using several state-of-the-art semantic segmentation models trained on structured urban scenes. We further analyse the impact of training data environments and propose a first step towards effective navigability in challenging environments with the constraint of having no annotation of the targeted images. Our goal is to foster impactful research that enhances the robustness and safety of autonomous vehicles in high-risk scenarios while being frugal in annotated data.

</details>


### [101] [Representation Space Constrained Learning with Modality Decoupling for Multimodal Object Detection](https://arxiv.org/abs/2511.15433)
*YiKang Shao,Tao Shi*

Main category: cs.CV

TL;DR: 本文研究了多模态目标检测中的融合退化问题，并提出了RSC-MD方法以解决该问题，从而在多个基准测试中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽略了融合退化，并且没有对其根本原因进行理论分析。

Method: 提出了 Representation Space Constrained Learning with Modality Decoupling (RSC-MD) 方法，包含 RSC 模块和 MD 模块。

Result: 在 FLIR, LLVIP, M3FD, 和 MFAD 数据集上进行了大量实验，证明了所提出的方法可以有效缓解融合退化，并在多个基准测试中实现了最先进的性能。

Conclusion: 本文有效地缓解了融合退化，并在多个基准测试中实现了最先进的性能。

Abstract: Multimodal object detection has attracted significant attention in both academia and industry for its enhanced robustness. Although numerous studies have focused on improving modality fusion strategies, most neglect fusion degradation, and none provide a theoretical analysis of its underlying causes. To fill this gap, this paper presents a systematic theoretical investigation of fusion degradation in multimodal detection and identifies two key optimization deficiencies: (1) the gradients of unimodal branch backbones are severely suppressed under multimodal architectures, resulting in under-optimization of the unimodal branches; (2) disparities in modality quality cause weaker modalities to experience stronger gradient suppression, which in turn results in imbalanced modality learning. To address these issues, this paper proposes a Representation Space Constrained Learning with Modality Decoupling (RSC-MD) method, which consists of two modules. The RSC module and the MD module are designed to respectively amplify the suppressed gradients and eliminate inter-modality coupling interference as well as modality imbalance, thereby enabling the comprehensive optimization of each modality-specific backbone. Extensive experiments conducted on the FLIR, LLVIP, M3FD, and MFAD datasets demonstrate that the proposed method effectively alleviates fusion degradation and achieves state-of-the-art performance across multiple benchmarks. The code and training procedures will be released at https://github.com/yikangshao/RSC-MD.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [102] [The Illusion of Procedural Reasoning: Measuring Long-Horizon FSM Execution in LLMs](https://arxiv.org/abs/2511.14777)
*Mahdi Samiei,Mahdi Mansouri,Mahdieh Soleymani Baghshah*

Main category: cs.AI

TL;DR: 论文提出了一种新的评估大型语言模型（LLM）程序推理能力的方法，通过有限状态机（FSM）执行来评估模型在多步骤、基于规则的计算中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏可控、可解释的基准来隔离和测量LLM在扩展推理链中性能下降的问题。

Method: 引入有限状态机（FSM）执行框架，要求模型根据FSM定义逐步执行，并在多个回合中保持状态一致性。

Result: 实验结果表明，随着任务范围或分支复杂度的增加，模型性能会系统性地下降。在高分支因子的情况下，规则检索的性能明显下降。较大的模型在局部准确性方面有所提高，但在多步骤推理下仍然不稳定，除非明确提示外部化中间步骤。

Conclusion: 基于FSM的评估提供了一种透明、复杂度可控的方法来诊断LLM的程序推理能力，并指导归纳偏置的设计，从而实现真正的长期程序能力。

Abstract: Large language models (LLMs) have achieved remarkable results on tasks framed as reasoning problems, yet their true ability to perform procedural reasoning, executing multi-step, rule-based computations remains unclear. Unlike algorithmic systems, which can deterministically execute long-horizon symbolic procedures, LLMs often degrade under extended reasoning chains, but there is no controlled, interpretable benchmark to isolate and measure this collapse. We introduce Finite-State Machine (FSM) Execution as a minimal, fully interpretable framework for evaluating the procedural reasoning capacity of LLMs. In our setup, the model is given an explicit FSM definition and must execute it step-by-step given input actions, maintaining state consistency over multiple turns. This task requires no world knowledge, only faithful application of deterministic transition rules, making it a direct probe of the model's internal procedural fidelity. We measure both Turn Accuracy and Task Accuracy to disentangle immediate computation from cumulative state maintenance. Empirical results reveal systematic degradation as task horizon or branching complexity increases. Models perform significantly worse when rule retrieval involves high branching factors than when memory span is long. Larger models show improved local accuracy but remain brittle under multi-step reasoning unless explicitly prompted to externalize intermediate steps. FSM-based evaluation offers a transparent, complexity-controlled probe for diagnosing this failure mode and guiding the design of inductive biases that enable genuine long-horizon procedural competence. By grounding reasoning in measurable execution fidelity rather than surface correctness, this work helps establish a rigorous experimental foundation for understanding and improving the algorithmic reliability of LLMs.

</details>


### [103] [Learning Interestingness in Automated Mathematical Theory Formation](https://arxiv.org/abs/2511.14778)
*George Tsoukalas,Rahul Saha,Amitayush Thakur,Sabrina Reguyal,Swarat Chaudhuri*

Main category: cs.AI

TL;DR: 这篇论文介绍了一个名为FERMAT的强化学习环境，用于模拟概念发现和定理证明，以探索自动评估数学对象有趣程度的问题。他们 исследован了使用进化算法合成有趣的度量，并引入了一个基于LLM的进化算法，该算法具有函数抽象，从而在发现初等数论和有限域方面取得了显著的改进。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是实现数学理论发现的自动化，这是人工智能领域的一个重大挑战。

Method: 该研究引入了一个名为FERMAT的强化学习环境，并使用进化算法来合成有趣的度量。特别地，他们引入了一个基于LLM的进化算法，该算法具有函数抽象。

Result: 该研究表明，基于LLM的进化算法在发现初等数论和有限域方面取得了显著的改进。

Conclusion: 该研究成功地自动化了数学理论发现过程中的一些关键步骤，并为未来的研究奠定了基础。

Abstract: We take two key steps in automating the open-ended discovery of new mathematical theories, a grand challenge in artificial intelligence. First, we introduce $\emph{FERMAT}$, a reinforcement learning (RL) environment that models concept discovery and theorem-proving using a set of symbolic actions, opening up a range of RL problems relevant to theory discovery. Second, we explore a specific problem through $\emph{FERMAT}$: automatically scoring the $\emph{interestingness}$ of mathematical objects. We investigate evolutionary algorithms for synthesizing nontrivial interestingness measures. In particular, we introduce an LLM-based evolutionary algorithm that features function abstraction, leading to notable improvements in discovering elementary number theory and finite fields over hard-coded baselines. We open-source the $\emph{FERMAT}$ environment at this URL(https://github.com/trishullab/Fermat).

</details>


### [104] [Ask WhAI:Probing Belief Formation in Role-Primed LLM Agents](https://arxiv.org/abs/2511.14780)
*Keith Moore,Jun W. Kim,David Lyu,Jeffrey Heo,Ehsan Adeli*

Main category: cs.AI

TL;DR: Ask WhAI是一个用于检查和扰动多智能体交互中信念状态的系统级框架。它记录和回放智能体交互，支持对每个智能体的信念和理由进行带外查询，并支持反事实证据注入，以测试信念结构如何响应新信息。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体交互中的信念形成和认知孤岛，这些在人类专家身上难以追踪和询问。

Method: 该框架应用于一个医疗案例模拟器，该模拟器具有多智能体共享记忆（时间戳电子病历或EMR）和一个预言机智能体（LabAgent），该智能体持有仅在明确查询时才显示的真实实验室结果。通过在关键诊断时刻的断点进行事件前后信念查询，从而区分根深蒂固的先验知识与推理或证据整合效果。

Result: 模拟结果表明，智能体的信念通常反映现实世界的学科立场，包括过度依赖经典研究和抵制反面证据。这些信念可以被追踪和询问。

Conclusion: Ask WhAI提供了一种可重复的方式来研究多智能体科学推理中的信念形成和认知孤岛。通过使这些动态可见和可测试，Ask WhAI为研究信念形成提供了一种新方法。

Abstract: We present Ask WhAI, a systems-level framework for inspecting and perturbing belief states in multi-agent interactions. The framework records and replays agent interactions, supports out-of-band queries into each agent's beliefs and rationale, and enables counterfactual evidence injection to test how belief structures respond to new information. We apply the framework to a medical case simulator notable for its multi-agent shared memory (a time-stamped electronic medical record, or EMR) and an oracle agent (the LabAgent) that holds ground truth lab results revealed only when explicitly queried. We stress-test the system on a multi-specialty diagnostic journey for a child with an abrupt-onset neuropsychiatric presentation. Large language model agents, each primed with strong role-specific priors ("act like a neurologist", "act like an infectious disease specialist"), write to a shared medical record and interact with a moderator across sequential or parallel encounters. Breakpoints at key diagnostic moments enable pre- and post-event belief queries, allowing us to distinguish entrenched priors from reasoning or evidence-integration effects. The simulation reveals that agent beliefs often mirror real-world disciplinary stances, including overreliance on canonical studies and resistance to counterevidence, and that these beliefs can be traced and interrogated in ways not possible with human experts. By making such dynamics visible and testable, Ask WhAI offers a reproducible way to study belief formation and epistemic silos in multi-agent scientific reasoning.

</details>


### [105] [Subnational Geocoding of Global Disasters Using Large Language Models](https://arxiv.org/abs/2511.14788)
*Michele Ronco,Damien Delforge,Wiebke S. Jäger,Christina Corbane*

Main category: cs.AI

TL;DR: 本研究提出了一种全自动的、由LLM辅助的工作流程，用于处理和清洗灾害事件的文本位置信息，并分配几何图形。


<details>
  <summary>Details</summary>
Motivation: 亚国家级的灾害事件位置数据对于风险评估和减少灾害风险至关重要，但现有灾害数据库的位置信息通常以非结构化的文本形式存在，难以与空间数据集整合。

Method: 该工作流程利用GPT-4o处理文本位置信息，并通过交叉检查GADM、OpenStreetMap和Wikidata三个独立的地理信息库来分配几何图形。基于这些来源的一致性和可用性，为每个位置分配一个可靠性评分。

Result: 应用于2000年至2024年的EM-DAT数据集，该工作流程对14,215个事件的17,948个独特位置进行地理编码。

Conclusion: 该方法无需人工干预，涵盖所有灾害类型，支持跨多个来源的交叉验证，并允许灵活地重新映射到首选框架。该研究展示了LLM从非结构化文本中提取和构建地理信息的潜力，为相关分析提供了一种可扩展且可靠的方法。

Abstract: Subnational location data of disaster events are critical for risk assessment and disaster risk reduction. Disaster databases such as EM-DAT often report locations in unstructured textual form, with inconsistent granularity or spelling, that make it difficult to integrate with spatial datasets. We present a fully automated LLM-assisted workflow that processes and cleans textual location information using GPT-4o, and assigns geometries by cross-checking three independent geoinformation repositories: GADM, OpenStreetMap and Wikidata. Based on the agreement and availability of these sources, we assign a reliability score to each location while generating subnational geometries. Applied to the EM-DAT dataset from 2000 to 2024, the workflow geocodes 14,215 events across 17,948 unique locations. Unlike previous methods, our approach requires no manual intervention, covers all disaster types, enables cross-verification across multiple sources, and allows flexible remapping to preferred frameworks. Beyond the dataset, we demonstrate the potential of LLMs to extract and structure geographic information from unstructured text, offering a scalable and reliable method for related analyses.

</details>


### [106] [Project Rachel: Can an AI Become a Scholarly Author?](https://arxiv.org/abs/2511.14819)
*Martin Monperrus,Benoit Baudry,Clément Vidal*

Main category: cs.AI

TL;DR: 本研究创建了一个名为 Rachel So 的完整 AI 学术身份，并跟踪了 ее 的学术活动，观察学术界对 AI 署名的反应。


<details>
  <summary>Details</summary>
Motivation: 探讨学术界对 AI 署名的反应。

Method: 通过发表 AI 生成的研究论文，观察学术界的反应。

Result: Rachel So 在 2025 年 3 月至 10 月期间发表了 10 多篇论文，被引用，并收到了同行评审邀请。

Conclusion: 讨论了 AI 署名对出版商、研究人员和整个科学体系的影响，为关于未来学术交流的必要辩论提供了经验性行动研究数据。

Abstract: This paper documents Project Rachel, an action research study that created and tracked a complete AI academic identity named Rachel So. Through careful publication of AI-generated research papers, we investigate how the scholarly ecosystem responds to AI authorship. Rachel So published 10+ papers between March and October 2025, was cited, and received a peer review invitation. We discuss the implications of AI authorship on publishers, researchers, and the scientific system at large. This work contributes empirical action research data to the necessary debate about the future of scholarly communication with super human, hyper capable AI systems.

</details>


### [107] [Uncertainty-Aware Measurement of Scenario Suite Representativeness for Autonomous Systems](https://arxiv.org/abs/2511.14853)
*Robab Aghazadeh Chakherlou,Siddartha Khastgir,Xingyu Zhao,Jerein Jeyachandran,Shufeng Chen*

Main category: cs.AI

TL;DR: 本研究关注人工智能系统中数据集的代表性，特别是在训练和测试自动驾驶车辆（AV）时，数据集需要反映其运行设计域（ODD）或目标运行域（TOD）。


<details>
  <summary>Details</summary>
Motivation: 确保人工智能系统（如自动驾驶汽车）的可信赖性和安全性，在很大程度上取决于用于训练和测试的数据集的数据相关安全属性，例如代表性和完整性。

Method: 提出了一种概率方法，通过比较场景套件编码的特征的统计分布与代表TOD的特征的相应分布来量化代表性，同时承认真实的TOD分布是未知的，因为它只能从有限的数据中推断出来。采用一种非精确贝叶斯方法来处理有限的数据和不确定的先验。

Result: 使用非精确贝叶斯公式生成区间值的、具有不确定性的代表性估计，而不是单个值。提出了一个数值例子，比较了在依赖性和先验不确定性下，场景套件和推断的TOD在操作类别（天气、道路类型、一天中的时间等）上的分布。

Conclusion: 我们估计了局部（类别之间）和全局的代表性为一个区间。

Abstract: Assuring the trustworthiness and safety of AI systems, e.g., autonomous vehicles (AV), depends critically on the data-related safety properties, e.g., representativeness, completeness, etc., of the datasets used for their training and testing. Among these properties, this paper focuses on representativeness-the extent to which the scenario-based data used for training and testing, reflect the operational conditions that the system is designed to operate safely in, i.e., Operational Design Domain (ODD) or expected to encounter, i.e., Target Operational Domain (TOD). We propose a probabilistic method that quantifies representativeness by comparing the statistical distribution of features encoded by the scenario suites with the corresponding distribution of features representing the TOD, acknowledging that the true TOD distribution is unknown, as it can only be inferred from limited data.
  We apply an imprecise Bayesian method to handle limited data and uncertain priors. The imprecise Bayesian formulation produces interval-valued, uncertainty-aware estimates of representativeness, rather than a single value. We present a numerical example comparing the distributions of the scenario suite and the inferred TOD across operational categories-weather, road type, time of day, etc., under dependencies and prior uncertainty. We estimate representativeness locally (between categories) and globally as an interval.

</details>


### [108] [Task Specific Sharpness Aware O-RAN Resource Management using Multi Agent Reinforcement Learning](https://arxiv.org/abs/2511.15002)
*Fatemeh Lotfi,Hossein Rajoli,Fatemeh Afghah*

Main category: cs.AI

TL;DR: 提出了一种新的资源管理方法，该方法通过在分布式多智能体强化学习 (MARL) 框架中利用锐度感知最小化 (SAM) 增强了软Actor-Critic (SAC) 算法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于深度强化学习 (DRL) 的网络资源优化模型在动态环境中通常缺乏鲁棒性和泛化能力。

Method: 引入了一种自适应和选择性的 SAM 机制，其中正则化由时间差 (TD) 误差方差显式驱动，确保只有面临高环境复杂性的智能体才会被正则化。此外，还结合了一种动态 ρ 调度方案来优化智能体之间的探索-利用平衡。

Result: 实验结果表明，该方法显著优于传统的 DRL 方法，在资源分配效率方面提高了高达 22%，并确保了跨各种 O-RAN 切片的卓越 QoS 满意度。

Conclusion: 该方法通过有选择地应用正则化和动态调整探索-利用平衡，提高了 DRL 模型在动态 O-RAN 环境中的性能。

Abstract: Next-generation networks utilize the Open Radio Access Network (O-RAN) architecture to enable dynamic resource management, facilitated by the RAN Intelligent Controller (RIC). While deep reinforcement learning (DRL) models show promise in optimizing network resources, they often struggle with robustness and generalizability in dynamic environments. This paper introduces a novel resource management approach that enhances the Soft Actor Critic (SAC) algorithm with Sharpness-Aware Minimization (SAM) in a distributed Multi-Agent RL (MARL) framework. Our method introduces an adaptive and selective SAM mechanism, where regularization is explicitly driven by temporal-difference (TD)-error variance, ensuring that only agents facing high environmental complexity are regularized. This targeted strategy reduces unnecessary overhead, improves training stability, and enhances generalization without sacrificing learning efficiency. We further incorporate a dynamic $ρ$ scheduling scheme to refine the exploration-exploitation trade-off across agents. Experimental results show our method significantly outperforms conventional DRL approaches, yielding up to a $22\%$ improvement in resource allocation efficiency and ensuring superior QoS satisfaction across diverse O-RAN slices.

</details>


### [109] [Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization](https://arxiv.org/abs/2511.15055)
*Jian-Ting Guo,Yu-Cheng Chen,Ping-Chun Hsieh,Kuo-Hao Ho,Po-Wei Huang,Ti-Rong Wu,I-Chen Wu*

Main category: cs.AI

TL;DR: 该论文提出了一种名为 Macro Action Quantization (MAQ) 的新框架，用于创建更像人类的强化学习 (RL) 智能体。MAQ 通过模仿人类行为并结合奖励最大化来实现类似人类的行为。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习智能体通常表现出不自然的动作，缺乏可解释性和信任度。为了解决这个问题，本研究旨在设计更像人类的强化学习智能体。

Method: 该论文将人类相似性建模为轨迹优化问题，并引入 Macro Action Quantization (MAQ) 框架。MAQ 使用 Vector-Quantized VAE 将人类演示提炼成宏观动作，并结合后退范围控制来实现高效学习。

Result: 在 D4RL Adroit 基准测试中，MAQ 显著提高了人类相似性，提高了轨迹相似度得分，并在人类评估研究中获得了最高的人类相似性排名。

Conclusion: MAQ 可以轻松集成到各种现成的强化学习算法中，为学习类人强化学习智能体开辟了一个有希望的方向。

Abstract: Human-like agents have long been one of the goals in pursuing artificial intelligence. Although reinforcement learning (RL) has achieved superhuman performance in many domains, relatively little attention has been focused on designing human-like RL agents. As a result, many reward-driven RL agents often exhibit unnatural behaviors compared to humans, raising concerns for both interpretability and trustworthiness. To achieve human-like behavior in RL, this paper first formulates human-likeness as trajectory optimization, where the objective is to find an action sequence that closely aligns with human behavior while also maximizing rewards, and adapts the classic receding-horizon control to human-like learning as a tractable and efficient implementation. To achieve this, we introduce Macro Action Quantization (MAQ), a human-like RL framework that distills human demonstrations into macro actions via Vector-Quantized VAE. Experiments on D4RL Adroit benchmarks show that MAQ significantly improves human-likeness, increasing trajectory similarity scores, and achieving the highest human-likeness rankings among all RL agents in the human evaluation study. Our results also demonstrate that MAQ can be easily integrated into various off-the-shelf RL algorithms, opening a promising direction for learning human-like RL agents. Our code is available at https://rlg.iis.sinica.edu.tw/papers/MAQ.

</details>


### [110] [Beyond GeneGPT: A Multi-Agent Architecture with Open-Source LLMs for Enhanced Genomic Question Answering](https://arxiv.org/abs/2511.15061)
*Haodong Chen,Guido Zuccon,Teerapong Leelanupab*

Main category: cs.AI

TL;DR: 本文介绍了OpenBioLLM，一个用于基因组问答的开源多代理框架，它通过代理专业化和模块化设计，在性能上与GeneGPT相当或更好，同时降低了延迟并提高了效率。


<details>
  <summary>Details</summary>
Motivation: GeneGPT依赖于专有模型，存在可扩展性、运营成本、数据隐私和泛化方面的问题。本文旨在利用开源模型重现GeneGPT，并在此基础上开发更高效、模块化的解决方案。

Method: 本文首先使用开源模型（Llama 3.1, Qwen2.5, and Qwen2.5 Coder）重现GeneGPT，然后构建了OpenBioLLM，一个模块化的多代理框架，其中包含工具路由、查询生成和响应验证等专门代理。

Result: OpenBioLLM在Gene-Turing和GeneHop基准测试中取得了与GeneGPT相当或更好的性能（平均得分分别为0.849和0.830），同时延迟降低了40-50%。

Conclusion: 本文证明了开源多代理系统在基因组问答方面的潜力。

Abstract: Genomic question answering often requires complex reasoning and integration across diverse biomedical sources. GeneGPT addressed this challenge by combining domain-specific APIs with OpenAI's code-davinci-002 large language model to enable natural language interaction with genomic databases. However, its reliance on a proprietary model limits scalability, increases operational costs, and raises concerns about data privacy and generalization.
  In this work, we revisit and reproduce GeneGPT in a pilot study using open source models, including Llama 3.1, Qwen2.5, and Qwen2.5 Coder, within a monolithic architecture; this allows us to identify the limitations of this approach. Building on this foundation, we then develop OpenBioLLM, a modular multi-agent framework that extends GeneGPT by introducing agent specialization for tool routing, query generation, and response validation. This enables coordinated reasoning and role-based task execution.
  OpenBioLLM matches or outperforms GeneGPT on over 90% of the benchmark tasks, achieving average scores of 0.849 on Gene-Turing and 0.830 on GeneHop, while using smaller open-source models without additional fine-tuning or tool-specific pretraining. OpenBioLLM's modular multi-agent design reduces latency by 40-50% across benchmark tasks, significantly improving efficiency without compromising model capability. The results of our comprehensive evaluation highlight the potential of open-source multi-agent systems for genomic question answering. Code and resources are available at https://github.com/ielab/OpenBioLLM.

</details>


### [111] [ProRAC: A Neuro-symbolic Method for Reasoning about Actions with LLM-based Progression](https://arxiv.org/abs/2511.15069)
*Haoyong Wu,Yongmei Liu*

Main category: cs.AI

TL;DR: ProRAC: A neuro-symbolic framework using LLMs for reasoning about actions and change (RAC) problems.


<details>
  <summary>Details</summary>
Motivation: Tackling reasoning about actions and change problems.

Method: Extracts RAC elements, executes actions progressively, and evaluates the query against the progressed state.

Result: Achieves strong performance across different RAC benchmarks, domains, LLM backbones, and task types.

Conclusion: ProRAC is effective for various RAC tasks.

Abstract: In this paper, we propose ProRAC (Progression-based Reasoning about Actions and Change), a neuro-symbolic framework that leverages LLMs to tackle RAC problems. ProRAC extracts fundamental RAC elements including actions and questions from the problem, progressively executes each action to derive the final state, and then evaluates the query against the progressed state to arrive at an answer. We evaluate ProRAC on several RAC benchmarks, and the results demonstrate that our approach achieves strong performance across different benchmarks, domains, LLM backbones, and types of RAC tasks.

</details>


### [112] [Knowledge-Informed Automatic Feature Extraction via Collaborative Large Language Model Agents](https://arxiv.org/abs/2511.15074)
*Henrik Bradland,Morten Goodwin,Vladimir I. Zadorozhny,Per-Arne Andersen*

Main category: cs.AI

TL;DR: Rogue One是一个基于LLM的多智能体框架，用于知识驱动的自动特征提取，通过协作发现、生成和验证预测特征。


<details>
  <summary>Details</summary>
Motivation: 表格数据的机器学习模型性能严重依赖于高质量的特征工程，而现有方法受限于单体LLM架构、简单的定量反馈以及未能系统地整合外部领域知识。

Method: 引入了一个由三个专业智能体（科学家、提取器和测试员）组成的去中心化系统，并通过丰富的定性反馈机制和“flooding-pruning”策略动态平衡特征探索和利用，同时通过集成的检索增强（RAG）系统积极整合外部知识。

Result: 在19个分类和9个回归数据集上显著优于现有方法，并在心肌数据集中发现了一种新的潜在生物标志物。

Conclusion: Rogue One不仅在统计上有效，而且在语义上有意义且可解释，可作为科学发现的工具。

Abstract: The performance of machine learning models on tabular data is critically dependent on high-quality feature engineering. While Large Language Models (LLMs) have shown promise in automating feature extraction (AutoFE), existing methods are often limited by monolithic LLM architectures, simplistic quantitative feedback, and a failure to systematically integrate external domain knowledge. This paper introduces Rogue One, a novel, LLM-based multi-agent framework for knowledge-informed automatic feature extraction. Rogue One operationalizes a decentralized system of three specialized agents-Scientist, Extractor, and Tester-that collaborate iteratively to discover, generate, and validate predictive features. Crucially, the framework moves beyond primitive accuracy scores by introducing a rich, qualitative feedback mechanism and a "flooding-pruning" strategy, allowing it to dynamically balance feature exploration and exploitation. By actively incorporating external knowledge via an integrated retrieval-augmented (RAG) system, Rogue One generates features that are not only statistically powerful but also semantically meaningful and interpretable. We demonstrate that Rogue One significantly outperforms state-of-the-art methods on a comprehensive suite of 19 classification and 9 regression datasets. Furthermore, we show qualitatively that the system surfaces novel, testable hypotheses, such as identifying a new potential biomarker in the myocardial dataset, underscoring its utility as a tool for scientific discovery.

</details>


### [113] [SafeRBench: A Comprehensive Benchmark for Safety Assessment in Large Reasoning Models](https://arxiv.org/abs/2511.15169)
*Xin Gao,Shaohan Yu,Zerui Chen,Yueming Lyu,Weichen Yu,Guanghao Li,Jiyao Liu,Jianxiong Gao,Jian Liang,Ziwei Liu,Chenyang Si*

Main category: cs.AI

TL;DR: SafeRBench：首个端到端评估大型推理模型（LRM）安全性的基准，从输入、中间推理到最终输出。


<details>
  <summary>Details</summary>
Motivation: 现有的安全评估主要集中在输出层面，很少捕捉推理过程中的动态风险（如：有害内容注入、逐步呈现或被误导性理由证实）。

Method: SafeRBench 包含：(1) 输入特征化：将风险类别和等级纳入输入设计，明确考虑受影响的群体和严重程度，从而建立一个反映不同危害梯度的平衡提示套件。(2) 细粒度输出分析：引入微思想分块机制，将长推理轨迹分割成语义连贯的单元，从而能够跨十个安全维度进行细粒度评估。(3) 人工安全对齐：验证了基于LLM的评估，并针对旨在捕捉安全判断的人工注释进行了验证。

Result: 对 19 个 LRM 的评估表明，SafeRBench 能够进行详细的、多维度的安全评估，从而可以从多个角度深入了解风险和保护机制。

Conclusion: SafeRBench 可以实现对大型推理模型进行多维度安全评估，并深入了解风险和保护机制。

Abstract: Large Reasoning Models (LRMs) improve answer quality through explicit chain-of-thought, yet this very capability introduces new safety risks: harmful content can be subtly injected, surface gradually, or be justified by misleading rationales within the reasoning trace. Existing safety evaluations, however, primarily focus on output-level judgments and rarely capture these dynamic risks along the reasoning process. In this paper, we present SafeRBench, the first benchmark that assesses LRM safety end-to-end -- from inputs and intermediate reasoning to final outputs. (1) Input Characterization: We pioneer the incorporation of risk categories and levels into input design, explicitly accounting for affected groups and severity, and thereby establish a balanced prompt suite reflecting diverse harm gradients. (2) Fine-Grained Output Analysis: We introduce a micro-thought chunking mechanism to segment long reasoning traces into semantically coherent units, enabling fine-grained evaluation across ten safety dimensions. (3) Human Safety Alignment: We validate LLM-based evaluations against human annotations specifically designed to capture safety judgments. Evaluations on 19 LRMs demonstrate that SafeRBench enables detailed, multidimensional safety assessment, offering insights into risks and protective mechanisms from multiple perspectives.

</details>


### [114] [HISE-KT: Synergizing Heterogeneous Information Networks and LLMs for Explainable Knowledge Tracing with Meta-Path Optimization](https://arxiv.org/abs/2511.15191)
*Zhiyi Duan,Zixing Shi,Hongyu Yuan,Qi Wang*

Main category: cs.AI

TL;DR: 提出了一种新的知识追踪框架，该框架集成了异构信息网络（HIN）和大型语言模型（LLM），以提高预测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于HIN的方法容易引入噪声，并且缺乏对元路径实例的质量评估。基于LLM的方法忽略了学生之间的丰富信息，并且两种范例都难以提供一致准确且基于证据的解释。

Method: 该框架首先构建一个包含多种节点类型的多关系HIN，然后使用LLM来智能地对元路径实例进行评分和过滤，并保留高质量的路径。此外，还设计了一种基于元路径的类似学生检索机制，以提供更有价值的预测上下文。最后，HISE-KT使用结构化提示，将目标学生的历史记录与检索到的相似轨迹相结合，使LLM能够生成准确的预测和基于证据的、可解释的分析报告。

Result: 在四个公共数据集上的实验表明，HISE-KT在预测性能和可解释性方面均优于现有的KT基线。

Conclusion: HISE-KT有效地结合了HIN和LLM的优势，实现了更准确和可解释的知识追踪。

Abstract: Knowledge Tracing (KT) aims to mine students' evolving knowledge states and predict their future question-answering performance. Existing methods based on heterogeneous information networks (HINs) are prone to introducing noises due to manual or random selection of meta-paths and lack necessary quality assessment of meta-path instances. Conversely, recent large language models (LLMs)-based methods ignore the rich information across students, and both paradigms struggle to deliver consistently accurate and evidence-based explanations. To address these issues, we propose an innovative framework, HIN-LLM Synergistic Enhanced Knowledge Tracing (HISE-KT), which seamlessly integrates HINs with LLMs. HISE-KT first builds a multi-relationship HIN containing diverse node types to capture the structural relations through multiple meta-paths. The LLM is then employed to intelligently score and filter meta-path instances and retain high-quality paths, pioneering automated meta-path quality assessment. Inspired by educational psychology principles, a similar student retrieval mechanism based on meta-paths is designed to provide a more valuable context for prediction. Finally, HISE-KT uses a structured prompt to integrate the target student's history with the retrieved similar trajectories, enabling the LLM to generate not only accurate predictions but also evidence-backed, explainable analysis reports. Experiments on four public datasets show that HISE-KT outperforms existing KT baselines in both prediction performance and interpretability.

</details>


### [115] [As If We've Met Before: LLMs Exhibit Certainty in Recognizing Seen Files](https://arxiv.org/abs/2511.15192)
*Haodong Li,Jingqi Zhang,Xiao Cheng,Peihua Mai,Haoyu Wang,Yang Pan*

Main category: cs.AI

TL;DR: COPYCHECK is a new framework that uses uncertainty signals to detect if copyrighted content was used in LLM training.


<details>
  <summary>Details</summary>
Motivation: Existing Membership Inference Attacks (MIAs) have limitations in detecting unauthorized use of copyrighted material in LLMs due to overconfidence, limited access to training data, and reliance on empirical thresholds.

Method: The method captures uncertainty patterns to distinguish between training and non-training data, segments files into smaller snippets, and uses uncertainty-guided unsupervised clustering.

Result: COPYCHECK achieves high balanced accuracy (90.1% on LLaMA 7b and 91.6% on LLaMA2 7b) in detecting seen files, with over 90% relative improvement compared to the baseline.

Conclusion: This work introduces the first application of uncertainty for copyright detection in LLMs, providing tools for training data transparency.

Abstract: The remarkable language ability of Large Language Models (LLMs) stems from extensive training on vast datasets, often including copyrighted material, which raises serious concerns about unauthorized use. While Membership Inference Attacks (MIAs) offer potential solutions for detecting such violations, existing approaches face critical limitations and challenges due to LLMs' inherent overconfidence, limited access to ground truth training data, and reliance on empirically determined thresholds.
  We present COPYCHECK, a novel framework that leverages uncertainty signals to detect whether copyrighted content was used in LLM training sets. Our method turns LLM overconfidence from a limitation into an asset by capturing uncertainty patterns that reliably distinguish between ``seen" (training data) and ``unseen" (non-training data) content. COPYCHECK further implements a two-fold strategy: (1) strategic segmentation of files into smaller snippets to reduce dependence on large-scale training data, and (2) uncertainty-guided unsupervised clustering to eliminate the need for empirically tuned thresholds. Experiment results show that COPYCHECK achieves an average balanced accuracy of 90.1% on LLaMA 7b and 91.6% on LLaMA2 7b in detecting seen files. Compared to the SOTA baseline, COPYCHECK achieves over 90% relative improvement, reaching up to 93.8\% balanced accuracy. It further exhibits strong generalizability across architectures, maintaining high performance on GPT-J 6B. This work presents the first application of uncertainty for copyright detection in LLMs, offering practical tools for training data transparency.

</details>


### [116] [SOLID: a Framework of Synergizing Optimization and LLMs for Intelligent Decision-Making](https://arxiv.org/abs/2511.15202)
*Yinsheng Wang,Tario G You,Léonard Boussioux,Shan Liu*

Main category: cs.AI

TL;DR: 提出了一个名为SOLID的新框架，该框架集成了数学优化和大型语言模型（LLM）的上下文功能。


<details>
  <summary>Details</summary>
Motivation: 旨在提高决策质量，同时保持模块化和数据隐私。

Method: 通过双重价格和偏差惩罚促进优化和LLM代理之间的迭代协作。

Result: 在股票投资案例中的实验结果表明，与仅使用优化器的基线方法相比，年化回报有所提高。

Conclusion: SOLID为推动跨不同领域的自动化和智能决策提供了一个有前景的框架。

Abstract: This paper introduces SOLID (Synergizing Optimization and Large Language Models for Intelligent Decision-Making), a novel framework that integrates mathematical optimization with the contextual capabilities of large language models (LLMs). SOLID facilitates iterative collaboration between optimization and LLMs agents through dual prices and deviation penalties. This interaction improves the quality of the decisions while maintaining modularity and data privacy. The framework retains theoretical convergence guarantees under convexity assumptions, providing insight into the design of LLMs prompt. To evaluate SOLID, we applied it to a stock portfolio investment case with historical prices and financial news as inputs. Empirical results demonstrate convergence under various scenarios and indicate improved annualized returns compared to a baseline optimizer-only method, validating the synergy of the two agents. SOLID offers a promising framework for advancing automated and intelligent decision-making across diverse domains.

</details>


### [117] [Efficiency Will Not Lead to Sustainable Reasoning AI](https://arxiv.org/abs/2511.15259)
*Philipp Wiesner,Daniel W. O'Neill,Francesca Larosa,Odej Kao*

Main category: cs.AI

TL;DR: 推理AI потребляет 大量能源，且效率提升有限。


<details>
  <summary>Details</summary>
Motivation: 传统的计算能效提升和需求饱和已无法满足推理AI的需求。

Method: 本文讨论了将显式限制嵌入到此类系统的优化和治理中的研究和政策方向。

Result: 推理AI的性能不再受可用训练数据量的限制，而是继续随着训练和推理中指数计算投资而扩展。

Conclusion: 单靠效率无法实现可持续的推理AI。

Abstract: AI research is increasingly moving toward complex problem solving, where models are optimized not only for pattern recognition but for multi-step reasoning. Historically, computing's global energy footprint has been stabilized by sustained efficiency gains and natural saturation thresholds in demand. But as efficiency improvements are approaching physical limits, emerging reasoning AI lacks comparable saturation points: performance is no longer limited by the amount of available training data but continues to scale with exponential compute investments in both training and inference. This paper argues that efficiency alone will not lead to sustainable reasoning AI and discusses research and policy directions to embed explicit limits into the optimization and governance of such systems.

</details>


### [118] [Realist and Pluralist Conceptions of Intelligence and Their Implications on AI Research](https://arxiv.org/abs/2511.15282)
*Ninell Oldenburg,Ruchira Dhar,Anders Søgaard*

Main category: cs.AI

TL;DR: 当前AI研究基于两种智能观念：Intelligence Realism和Intelligence Pluralism。前者认为智能是单一的、通用的能力，后者认为智能是多样化的、依赖于环境的能力。


<details>
  <summary>Details</summary>
Motivation: 揭示AI研究中Intelligence Realism和Intelligence Pluralism这两种隐含的智能观念如何影响研究方向。

Method: 分析当前AI研究中的辩论，展示这两种观念如何潜在地影响实证证据的解读。

Result: 这两种观念在方法论、解释和AI风险评估三个方面产生了不同的研究方法。在方法论上，它们导致了不同的模型选择、基准设计和实验验证方法。在解释上，它们对相同的经验现象产生了矛盾的解读。在AI风险方面，Realism认为超级智能是主要风险，而Pluralism认为不同领域存在不同的威胁。

Conclusion: 将这些潜在的假设明确化有助于更清晰地理解AI研究中的分歧。

Abstract: In this paper, we argue that current AI research operates on a spectrum between two different underlying conceptions of intelligence: Intelligence Realism, which holds that intelligence represents a single, universal capacity measurable across all systems, and Intelligence Pluralism, which views intelligence as diverse, context-dependent capacities that cannot be reduced to a single universal measure. Through an analysis of current debates in AI research, we demonstrate how the conceptions remain largely implicit yet fundamentally shape how empirical evidence gets interpreted across a wide range of areas. These underlying views generate fundamentally different research approaches across three areas. Methodologically, they produce different approaches to model selection, benchmark design, and experimental validation. Interpretively, they lead to contradictory readings of the same empirical phenomena, from capability emergence to system limitations. Regarding AI risk, they generate categorically different assessments: realists view superintelligence as the primary risk and search for unified alignment solutions, while pluralists see diverse threats across different domains requiring context-specific solutions. We argue that making explicit these underlying assumptions can contribute to a clearer understanding of disagreements in AI research.

</details>


### [119] [Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration](https://arxiv.org/abs/2511.15351)
*Yifu Guo,Zishan Xu,Zhiyuan Yao,Yuquan Lu,Jiaye Lin,Sen Hu,Zhenheng Tang,Yingchao Li,Huacan Wang,Ronghao Chen*

Main category: cs.AI

TL;DR: 现有的多模态推理模型缺乏自主探索能力，难以适应真实世界的任务。本文提出了Octopus，一种新的多模态Agent推理范式，它具有六种核心能力，并能根据当前状态动态选择最合适的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态推理模型和框架存在架构上的局限性，缺乏像人类一样自主探索不同推理路径的能力，难以适应真实世界任务中动态变化的能力需求。

Method: 本文提出了一种新的多模态Agent推理范式Octopus，它具有六种核心能力，能够在推理过程中自主探索，并根据当前状态动态选择最合适的能力。

Result: Octopus在Octopus-Bench的大多数任务上取得了最佳性能。

Conclusion: 实验结果表明，能力协调在Agent多模态推理中起着关键作用。

Abstract: Existing multimodal reasoning models and frameworks suffer from fundamental architectural limitations: most lack the human-like ability to autonomously explore diverse reasoning pathways-whether in direct inference, tool-driven visual exploration, programmatic visual manipulation, or intrinsic visual imagination. Consequently, they struggle to adapt to dynamically changing capability requirements in real-world tasks. Meanwhile, humans exhibit a complementary set of thinking abilities when addressing such tasks, whereas existing methods typically cover only a subset of these dimensions. Inspired by this, we propose Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration, a new paradigm for multimodal agentic reasoning. We define six core capabilities essential for multimodal reasoning and organize a comprehensive evaluation benchmark, Octopus-Bench, accordingly. Octopus is capable of autonomously exploring during reasoning and dynamically selecting the most appropriate capability based on the current state. Experimental results show that Octopus achieves the best performance on the vast majority of tasks in Octopus-Bench, highlighting the crucial role of capability coordination in agentic multimodal reasoning.

</details>


### [120] [Terra Nova: A Comprehensive Challenge Environment for Intelligent Agents](https://arxiv.org/abs/2511.15378)
*Trevor McInroe*

Main category: cs.AI

TL;DR: Terra Nova是一个新的综合性强化学习挑战环境，灵感来自文明V。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习环境无法同时考察多个典型挑战（如部分可观察性、信用分配、表征学习、巨大动作空间等）的综合理解能力。

Method: 构建了一个新的综合性挑战环境Terra Nova。

Result: Terra Nova环境能够同时激发多个典型的强化学习挑战。

Conclusion: Terra Nova可以用于测试agent在多个相互作用的挑战中进行深度推理的能力。

Abstract: We introduce Terra Nova, a new comprehensive challenge environment (CCE) for reinforcement learning (RL) research inspired by Civilization V. A CCE is a single environment in which multiple canonical RL challenges (e.g., partial observability, credit assignment, representation learning, enormous action spaces, etc.) arise simultaneously. Mastery therefore demands integrated, long-horizon understanding across many interacting variables. We emphasize that this definition excludes challenges that only aggregate unrelated tasks in independent, parallel streams (e.g., learning to play all Atari games at once). These aggregated multitask benchmarks primarily asses whether an agent can catalog and switch among unrelated policies rather than test an agent's ability to perform deep reasoning across many interacting challenges.

</details>


### [121] [IPR-1: Interactive Physical Reasoner](https://arxiv.org/abs/2511.15407)
*Mingyu Zhang,Lifeng Zhuo,Tianxi Tan,Guocan Xie,Xian Nie,Yan Li,Renjie Zhao,Zizhu He,Ziyu Wang,Jiting Cai,Yong-Lu Li*

Main category: cs.AI

TL;DR: 这篇论文探讨了如何让智能体通过交互学习并提升物理和因果推理能力，模仿人类的学习方式。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探究智能体是否能像人类一样通过交互获取推理能力，并通过更多经验持续改进。

Method: 提出了IPR（交互式物理推理器），利用世界模型的推演来评估和加强VLM的策略，并引入了PhysCode，一种以物理为中心的动作编码，将语义意图与动力学对齐，为预测和推理提供共享的动作空间。

Result: IPR在三个层面上表现出色，整体与GPT-5相当，并在好奇心方面超过了它。性能随着训练游戏和交互步骤的增加而提高，并且模型还可以零样本迁移到未见过的游戏。

Conclusion: 研究结果表明，以物理为中心的交互是稳步提高物理推理能力的一种途径。

Abstract: Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. We study this in a Game-to-Unseen (G2U) setting, curating 1,000+ heterogeneous games with diverse physical and causal mechanisms, and evaluate at three human-like levels: Survival, Curiosity, Utility, from primitive intuition to goal-driven reasoning. Our analysis reveals complementary failures: VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on three levels, matches GPT-5 overall, and surpasses it on Curiosity. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning.

</details>


### [122] [Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining](https://arxiv.org/abs/2511.15456)
*Qian'ang Mao,Yuxuan Zhang,Jiaman Chen,Wenjun Zhou,Jiaqi Yan*

Main category: cs.AI

TL;DR: 提出了一个名为 TIM 的框架，用于推断 DeFi 交易背后的用户意图。


<details>
  <summary>Details</summary>
Motivation: 理解 DeFi 交易背后的用户意图至关重要，但由于智能合约交互复杂、链上/链下因素多方面以及十六进制日志不透明而具有挑战性。现有的方法缺乏深刻的语义洞察力。

Method: TIM 利用基于扎根理论构建的 DeFi 意图分类法和多代理大型语言模型 (LLM) 系统来稳健地推断用户意图。Meta-Level Planner 动态地协调领域专家，将多个特定视角的意图分析分解为可解决的子任务。Question Solvers 使用多模式链上/链下数据处理任务。Cognitive Evaluator 缓解了 LLM 幻觉并确保可验证性。

Result: 实验表明，TIM 显着优于机器学习模型、单个 LLM 和单个 Agent 基线。

Conclusion: 这项工作有助于更可靠地理解 DeFi 中用户的动机，为复杂的区块链活动提供上下文相关的解释。

Abstract: As Decentralized Finance (DeFi) develops, understanding user intent behind DeFi transactions is crucial yet challenging due to complex smart contract interactions, multifaceted on-/off-chain factors, and opaque hex logs. Existing methods lack deep semantic insight. To address this, we propose the Transaction Intent Mining (TIM) framework. TIM leverages a DeFi intent taxonomy built on grounded theory and a multi-agent Large Language Model (LLM) system to robustly infer user intents. A Meta-Level Planner dynamically coordinates domain experts to decompose multiple perspective-specific intent analyses into solvable subtasks. Question Solvers handle the tasks with multi-modal on/off-chain data. While a Cognitive Evaluator mitigates LLM hallucinations and ensures verifiability. Experiments show that TIM significantly outperforms machine learning models, single LLMs, and single Agent baselines. We also analyze core challenges in intent inference. This work helps provide a more reliable understanding of user motivations in DeFi, offering context-aware explanations for complex blockchain activity.

</details>


### [123] [Exploring the use of AI authors and reviewers at Agents4Science](https://arxiv.org/abs/2511.15534)
*Federico Bianchi,Owen Queen,Nitya Thakkar,Eric Sun,James Zou*

Main category: cs.AI

TL;DR: AI agents are emerging in scientific research, but their capabilities are not yet clear.


<details>
  <summary>Details</summary>
Motivation: To understand the capabilities of AI agents as scientists and reviewers.

Method: Organized a conference called Agents4Science where AI agents served as authors and reviewers, with human co-authors and co-reviewers.

Result: Key learnings from the conference regarding human-AI collaboration in science.

Conclusion: Discusses the implications of the conference learnings for human-AI collaboration in science.

Abstract: There is growing interest in using AI agents for scientific research, yet fundamental questions remain about their capabilities as scientists and reviewers. To explore these questions, we organized Agents4Science, the first conference in which AI agents serve as both primary authors and reviewers, with humans as co-authors and co-reviewers. Here, we discuss the key learnings from the conference and their implications for human-AI collaboration in science.

</details>


### [124] [What Does It Take to Be a Good AI Research Agent? Studying the Role of Ideation Diversity](https://arxiv.org/abs/2511.15593)
*Alexis Audran-Reiss,Jordi Armengol Estapé,Karen Hambardzumyan,Amar Budhiraja,Martin Josifoski,Edan Toledo,Rishi Hazra,Despoina Magka,Michael Shvartsman,Parth Pathak,Justine T Kao,Lucia Cipolina-Kun,Bhavul Gauri,Jean-Christophe Gagnon-Audet,Emanuel Tewolde,Jenny Zhang,Taco Cohen,Yossi Adi,Tatiana Shavrina,Yoram Bachrach*

Main category: cs.AI

TL;DR: 研究了AI研究代理中思想多样性对agent性能的影响。


<details>
  <summary>Details</summary>
Motivation: 了解agent轨迹成功或失败的关键因素，特别是思想多样性的作用。

Method: 分析了MLE-bench上不同模型和agent scaffolds的agent轨迹，并进行了控制实验来修改思想多样性的程度。

Result: 发现更高性能的agent往往具有更高的思想多样性，并且更高的思想多样性会导致更强的性能。

Conclusion: 思想多样性在AI研究代理的性能中起着重要作用。

Abstract: AI research agents offer the promise to accelerate scientific progress by automating the design, implementation, and training of machine learning models. However, the field is still in its infancy, and the key factors driving the success or failure of agent trajectories are not fully understood. We examine the role that ideation diversity plays in agent performance. First, we analyse agent trajectories on MLE-bench, a well-known benchmark to evaluate AI research agents, across different models and agent scaffolds. Our analysis reveals that different models and agent scaffolds yield varying degrees of ideation diversity, and that higher-performing agents tend to have increased ideation diversity. Further, we run a controlled experiment where we modify the degree of ideation diversity, demonstrating that higher ideation diversity results in stronger performance. Finally, we strengthen our results by examining additional evaluation metrics beyond the standard medal-based scoring of MLE-bench, showing that our findings still hold across other agent performance metrics.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [125] [Castle: Causal Cascade Updates in Relational Databases with Large Language Models](https://arxiv.org/abs/2511.14762)
*Yongye Su,Yucheng Zhang,Zeru Shi,Bruno Ribeiro,Elisa Bertino*

Main category: cs.DB

TL;DR: Castle是一个使用大型语言模型(llm)的schema-only cascade update generation框架。


<details>
  <summary>Details</summary>
Motivation: 现有的方法主要集中在SELECT查询上，忽略了SQL update操作及其连锁反应的挑战。传统的CASCADE UPDATE约束是静态的，不适合现代的非规范化数据库，这些数据库需要动态的、上下文相关的更新。

Method: 将UPDATE SQL生成构建为LLM的divide-and-conquer任务

Result: 在真实的因果更新场景中评估了Castle，证明了它生成准确的SQL更新的能力。

Conclusion: 强调了llm在自动DBMS中的推理能力。

Abstract: This work introduces Castle, the first framework for schema-only cascade update generation using large language models (LLMs). Despite recent advances in LLMs for Text2SQL code generation, existing approaches focus primarily on SELECT queries, neglecting the challenges of SQL update operations and their ripple effects. Traditional CASCADE UPDATE constraints are static and unsuitable for modern, denormalized databases, which demand dynamic, context-aware updates. Castle enables natural language instructions to trigger multi-column, causally consistent SQL UPDATE statements, without revealing table content to the model. By framing UPDATE SQL generation as a divide-and-conquer task with LLMs' reasoning capacity, Castle can determine not only which columns must be directly updated, but also how those updates propagate through the schema, causing cascading updates -- all via nested queries and substructures that ensure data confidentiality. We evaluate it on real-world causal update scenarios, demonstrating its ability to produce accurate SQL updates, and thereby highlighting the reasoning ability of LLMs in automated DBMS.

</details>


### [126] [BBox DocVQA: A Large Scale Bounding Box Grounded Dataset for Enhancing Reasoning in Document Visual Question Answer](https://arxiv.org/abs/2511.15090)
*Wenhan Yu,Wang Chen,Guanqiang Qi,Weikang Li,Yang Li,Lei Sha,Deguo Xia,Jizhou Huang*

Main category: cs.DB

TL;DR: BBox DocVQA：一个大规模的、边界框标注的文档视觉问答数据集，旨在提升视觉语言模型在空间推理和证据定位方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有DocVQA数据集缺乏细粒度的空间定位，限制了视觉语言模型的可解释性和推理能力。

Method: 提出了一种自动构建流程，即分割、判断和生成（Segment Judge and Generate），该流程集成了分割模型、视觉语言模型和人工验证。

Result: 构建了一个包含3.6K文档和32K QA对的数据集，涵盖单区域和多区域以及单页和多页场景。在BBox DocVQA上对多个先进的视觉语言模型进行基准测试，揭示了在空间定位和推理准确性方面仍然存在的挑战。在BBox DocVQA上进行微调可以显著提高边界框定位和答案生成。

Conclusion: BBox DocVQA数据集的有效性已得到验证，它能够增强视觉语言模型的推理能力，并将公开发布以促进可解释和空间定位的视觉语言推理研究。

Abstract: Document Visual Question Answering (DocVQA) is a fundamental task for multimodal document understanding and a key testbed for vision language reasoning. However, most existing DocVQA datasets are limited to the page level and lack fine grained spatial grounding, constraining the interpretability and reasoning capability of Vision Language Models (VLMs). To address this gap, we introduce BBox DocVQA a large scale, bounding box grounded dataset designed to enhance spatial reasoning and evidence localization in visual documents. We further present an automated construction pipeline, Segment Judge and Generate, which integrates a segment model for region segmentation, a VLM for semantic judgment, and another advanced VLM for question answer generation, followed by human verification for quality assurance. The resulting dataset contains 3.6 K diverse documents and 32 K QA pairs, encompassing single and multi region as well as single and multi page scenarios. Each QA instance is grounded on explicit bounding boxes, enabling fine grained evaluation of spatial semantic alignment. Benchmarking multiple state of the art VLMs (e.g., GPT 5, Qwen2.5 VL, and InternVL) on BBox DocVQA reveals persistent challenges in spatial grounding and reasoning accuracy. Furthermore, fine tuning on BBox DocVQA substantially improves both bounding box localization and answer generation, validating its effectiveness for enhancing the reasoning ability of VLMs. Our dataset and code will be publicly released to advance research on interpretable and spatially grounded vision language reasoning.

</details>


### [127] [B+ANN: A Fast Billion-Scale Disk-based Nearest-Neighbor Index](https://arxiv.org/abs/2511.15557)
*Selim Furkan Tekin,Rajesh Bordawekar*

Main category: cs.DB

TL;DR: 提出了一个新的基于磁盘的近似最近邻索引B+ANN，以解决HNSW算法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有向量数据库(VDBs)广泛使用的HNSW算法存在内存设计、随机内存访问、加速范围有限和仅支持语义相似性查询等问题。

Method: B+ANN首先将输入数据分成包含语义相似项的块，然后构建一个B+树变体来存储内存和磁盘上的块，最后，实现混合边缘和基于块的内存遍历。

Result: 实验结果表明，B+ANN在质量(召回率)和执行性能(每秒查询数/QPS)方面都优于HNSW，通过改善语义操作的空间和时间局部性，减少缓存未命中(相对增益19.23%)，并减少内存消耗和磁盘构建时间(超过DiskANN算法24倍)。

Conclusion: B+ANN还支持差异性查询，而传统的ANN索引不支持。

Abstract: Storing and processing of embedding vectors by specialized Vector databases (VDBs) has become the linchpin in building modern AI pipelines. Most current VDBs employ variants of a graph-based ap- proximate nearest-neighbor (ANN) index algorithm, HNSW, to an- swer semantic queries over stored vectors. Inspite of its wide-spread use, the HNSW algorithm suffers from several issues: in-memory design and implementation, random memory accesses leading to degradation in cache behavior, limited acceleration scope due to fine-grained pairwise computations, and support of only semantic similarity queries. In this paper, we present a novel disk-based ANN index, B+ANN, to address these issues: it first partitions input data into blocks containing semantically similar items, then builds an B+ tree variant to store blocks both in-memory and on disks, and finally, enables hybrid edge- and block-based in-memory traversals. As demonstrated by our experimantal evaluation, the proposed B+ANN disk-based index improves both quality (Recall value), and execution performance (Queries per second/QPS) over HNSW, by improving spatial and temporal locality for semantic operations, reducing cache misses (19.23% relative gain), and decreasing the memory consumption and disk-based build time by 24x over the DiskANN algorithm. Finally, it enables dissimilarity queries, which are not supported by similarity-oriented ANN indices.

</details>


### [128] [A Decade of Systems for Human Data Interaction](https://arxiv.org/abs/2511.15585)
*Eugene Wu,Yiru Chen,Haneen Mohammed,Zezhou Huang*

Main category: cs.DB

TL;DR: 人机数据交互（HDI）提出了与传统数据管理根本不同的挑战。


<details>
  <summary>Details</summary>
Motivation: HDI 系统必须满足源于可用性而不是查询语义的延迟、正确性和一致性需求；未能满足这些期望会破坏用户体验。此外，接口和系统紧密耦合；两者都不能轻易地孤立地进行优化，有效的解决方案需要它们的协同设计。

Method: 调查了我们实验室十年来的工作，这些工作都包含了这种耦合。

Result: HDI 系统是可靠的、交互式的、人工智能驱动的应用程序的基础。

Conclusion: 系统创新和数据库理论也可以激发新的交互和可视化设计。

Abstract: Human-data interaction (HDI) presents fundamentally different challenges from traditional data management. HDI systems must meet latency, correctness, and consistency needs that stem from usability rather than query semantics; failing to meet these expectations breaks the user experience. Moreover, interfaces and systems are tightly coupled; neither can easily be optimized in isolation, and effective solutions demand their co-design. This dependence also presents a research opportunity: rather than adapt systems to interface demands, systems innovations and database theory can also inspire new interaction and visualization designs. We survey a decade of our lab's work that embraces this coupling and argue that HDI systems are the foundation for reliable, interactive, AI-driven applications.

</details>


### [129] [Sufficient Explanations in Databases and their Connections to Necessary Explanations and Repairs](https://arxiv.org/abs/2511.15623)
*Leopoldo Bertossi,Nina Pardal*

Main category: cs.DB

TL;DR: 本研究探讨了关系数据库中查询应答的充分解释概念，并将其与不一致数据库的数据库修复以及基于因果关系的必要解释联系起来，同时提供了一些计算结果。


<details>
  <summary>Details</summary>
Motivation: 研究查询应答的因果解释，特别关注充分解释。

Method: 研究充分解释与数据库修复和基于因果关系的必要解释之间的联系，并进行计算分析。

Result: 获得了关于充分解释的一些计算结果。

Conclusion: 本文研究了关系数据库中查询应答的充分解释，并探讨了其与其他概念的联系，同时提供了一些计算结果。

Abstract: The notion of cause, as formalized by Halpern and Pearl, has been recently applied to relational databases, to characterize and compute causal explanations for query answers. In this work we consider the alternative notion of sufficient explanation. We investigate its connections with database repairs as used for dealing with inconsistent databases, and with causality-based necessary explanations. We also obtain some computational results.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [130] [Membership Inference Attack against Large Language Model-based Recommendation Systems: A New Distillation-based Paradigm](https://arxiv.org/abs/2511.14763)
*Li Cuihong,Huang Xiaowen,Yin Chuanhuan,Sang Jitao*

Main category: cs.IR

TL;DR: 提出了一种基于知识蒸馏的成员推理攻击 (MIA) 范例，以提高对基于 LLM 的推荐系统的成员推理攻击的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的 MIA 通过影子模型获得目标模型的特征，并使用这些特征来训练攻击模型，但是基于大型语言模型 (LLM) 的推荐系统的训练或微调数据的规模和复杂性使得影子模型难以构建。

Method: 引入知识蒸馏来获得参考模型，从而增强参考模型区分成员数据和非成员数据的能力。我们从参考模型中获得个体特征，并使用融合特征训练我们的攻击模型。

Result: 我们的范例提高了 MIA 的攻击性能，优于基于影子模型的攻击。

Conclusion: 该论文提出了一种新的基于知识蒸馏的 MIA 范例，并在 LLM 推荐系统上取得了比传统方法更好的攻击效果。

Abstract: Membership Inference Attack (MIA) aims to determine if a data sample is used in the training dataset of a target model. Traditional MIA obtains feature of target model via shadow models and uses the feature to train attack model, but the scale and complexity of training or fine-tuning data for large language model (LLM)-based recommendation systems make shadow models difficult to construct. Knowledge distillation as a method for extracting knowledge contributes to construct a stronger reference model. Knowledge distillation enables separate distillation for member and non-member data during the distillation process, enhancing the model's discriminative capability between the two in MIA. This paper propose a knowledge distillation-based MIA paradigm to improve the performance of membership inference attacks on LLM-based recommendation systems. Our paradigm introduces knowledge distillation to obtain a reference model, which enhances the reference model's ability to distinguish between member and non-member data. We obtain individual features from the reference model and train our attack model with fused feature. Our paradigm improves the attack performance of MIA compared to shadow model-based attack.

</details>


### [131] [Image-Seeking Intent Prediction for Cross-Device Product Search](https://arxiv.org/abs/2511.14764)
*Mariya Hendriksen,Svitlana Vakulenko,Jordan Massiah,Gabriella Kazai,Emine Yilmaz*

Main category: cs.IR

TL;DR: 本文提出了一种新的任务，即图像搜索意图预测，用于预测何时口头产品查询应该主动触发屏幕设备上的视觉效果。


<details>
  <summary>Details</summary>
Motivation: 客户越来越多地在多个设备上购物，主动建议切换设备可以大大改善用户体验，但必须以高精度提供，以避免不必要的摩擦。本文旨在解决预测查询何时需要视觉增强和跨设备切换以改善产品发现的挑战。

Method: 本文使用来自多设备零售助理的大规模生产数据，包括 90 万个语音查询、相关的产品检索和行为信号（例如图像轮播互动），训练 IRP（图像请求预测器）模型，该模型利用用户输入查询和相应的检索产品元数据来预测视觉意图。

Result: 实验表明，将查询语义与产品数据相结合，特别是在通过轻量级摘要改进时，可以持续提高预测准确性。结合可区分的面向精确度的损失进一步减少了误报。

Conclusion: 这些结果突出了法学硕士为智能、跨设备购物助手提供支持的潜力，这些助手可以预测和适应用户需求，从而实现更加无缝和个性化的电子商务体验。

Abstract: Large Language Models (LLMs) are transforming personalized search, recommendations, and customer interaction in e-commerce. Customers increasingly shop across multiple devices, from voice-only assistants to multimodal displays, each offering different input and output capabilities. A proactive suggestion to switch devices can greatly improve the user experience, but it must be offered with high precision to avoid unnecessary friction. We address the challenge of predicting when a query requires visual augmentation and a cross-device switch to improve product discovery. We introduce Image-Seeking Intent Prediction, a novel task for LLM-driven e-commerce assistants that anticipates when a spoken product query should proactively trigger a visual on a screen-enabled device. Using large-scale production data from a multi-device retail assistant, including 900K voice queries, associated product retrievals, and behavioral signals such as image carousel engagement, we train IRP (Image Request Predictor), a model that leverages user input query and corresponding retrieved product metadata to anticipate visual intent. Our experiments show that combining query semantics with product data, particularly when improved through lightweight summarization, consistently improves prediction accuracy. Incorporating a differentiable precision-oriented loss further reduces false positives. These results highlight the potential of LLMs to power intelligent, cross-device shopping assistants that anticipate and adapt to user needs, enabling more seamless and personalized e-commerce experiences.

</details>


### [132] [Optimizing Agricultural Research: A RAG-Based Approach to Mycorrhizal Fungi Information](https://arxiv.org/abs/2511.14765)
*Mohammad Usman Altam,Md Imtiaz Habib,Tuan Hoang*

Main category: cs.IR

TL;DR: 本文介绍了一种用于 MycoPhyto 的 RAG 系统，专注于推进与丛枝菌根真菌 (AMF) 相关的农业应用。


<details>
  <summary>Details</summary>
Motivation: 传统大型语言模型 (LLM) 受限于静态训练语料库，而 RAG 系统动态整合领域特定的外部知识来源，从而克服时间和学科的限制。AMF 在可持续农业中起着关键作用。

Method: 该系统采用双层策略：(i) 使用向量嵌入对农学和生物技术语料库中的领域特定内容进行语义检索和增强，以及 (ii) 结构化数据提取以捕获预定义的实验元数据，例如接种方法、孢子密度、土壤参数和产量结果。嵌入存储在高性能向量数据库中。

Result: 经验评估表明，所提出的管道检索并合成了关于 AMF 与作物系统（如番茄）相互作用的高度相关信息。

Conclusion: 该框架强调了人工智能驱动的知识发现加速农业生态创新和加强可持续农业系统决策的潜力。

Abstract: Retrieval-Augmented Generation (RAG) represents a transformative approach within natural language processing (NLP), combining neural information retrieval with generative language modeling to enhance both contextual accuracy and factual reliability of responses. Unlike conventional Large Language Models (LLMs), which are constrained by static training corpora, RAG-powered systems dynamically integrate domain-specific external knowledge sources, thereby overcoming temporal and disciplinary limitations. In this study, we present the design and evaluation of a RAG-enabled system tailored for Mycophyto, with a focus on advancing agricultural applications related to arbuscular mycorrhizal fungi (AMF). These fungi play a critical role in sustainable agriculture by enhancing nutrient acquisition, improving plant resilience under abiotic and biotic stresses, and contributing to soil health. Our system operationalizes a dual-layered strategy: (i) semantic retrieval and augmentation of domain-specific content from agronomy and biotechnology corpora using vector embeddings, and (ii) structured data extraction to capture predefined experimental metadata such as inoculation methods, spore densities, soil parameters, and yield outcomes. This hybrid approach ensures that generated responses are not only semantically aligned but also supported by structured experimental evidence. To support scalability, embeddings are stored in a high-performance vector database, allowing near real-time retrieval from an evolving literature base. Empirical evaluation demonstrates that the proposed pipeline retrieves and synthesizes highly relevant information regarding AMF interactions with crop systems, such as tomato (Solanum lycopersicum). The framework underscores the potential of AI-driven knowledge discovery to accelerate agroecological innovation and enhance decision-making in sustainable farming systems.

</details>


### [133] [OTCR: Optimal Transmission, Compression and Representation for Multimodal Information Extraction](https://arxiv.org/abs/2511.14766)
*Yang Li,Yajiao Wang,Wenhao Hu,Zhixiong Zhang,Mengting Zhang*

Main category: cs.IR

TL;DR: 提出了一种新的多模态信息抽取框架（OTCR），该框架以文本为主导，视觉选择性支持，通过跨模态最优传输和变分信息瓶颈来控制多模态融合，减少冗余并增强任务相关信号。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常隐式假设模态等价或以大致统一的方式对待模态，导致多模态信号的无差别结合和对任务无关冗余的控制不足，从而限制了泛化能力。

Method: OTCR框架包含两个阶段：1) 跨模态最优传输（OT）产生文本标记和视觉补丁之间的稀疏概率对齐，并使用上下文感知门控制视觉注入；2) 变分信息瓶颈（VIB）压缩融合特征，过滤任务无关的噪声，以产生紧凑的、任务自适应的表示。

Result: 在FUNSD数据集上，OTCR的SER达到91.95%，RE达到91.13%；在XFUND（ZH）数据集上，SER达到91.09%，RE达到94.20%，在数据集上表现出竞争性的性能。

Conclusion: 这项工作为文档AI中可控多模态融合提供了一种可解释的、信息论的范例。

Abstract: Multimodal Information Extraction (MIE) requires fusing text and visual cues from visually rich documents. While recent methods have advanced multimodal representation learning, most implicitly assume modality equivalence or treat modalities in a largely uniform manner, still relying on generic fusion paradigms. This often results in indiscriminate incorporation of multimodal signals and insufficient control over task-irrelevant redundancy, which may in turn limit generalization. We revisit MIE from a task-centric view: text should dominate, vision should selectively support. We present OTCR, a two-stage framework. First, Cross-modal Optimal Transport (OT) yields sparse, probabilistic alignments between text tokens and visual patches, with a context-aware gate controlling visual injection. Second, a Variational Information Bottleneck (VIB) compresses fused features, filtering task-irrelevant noise to produce compact, task-adaptive representations. On FUNSD, OTCR achieves 91.95% SER and 91.13% RE, while on XFUND (ZH), it reaches 91.09% SER and 94.20% RE, demonstrating competitive performance across datasets. Feature-level analyses further confirm reduced modality redundancy and strengthened task signals. This work offers an interpretable, information-theoretic paradigm for controllable multimodal fusion in document AI.

</details>


### [134] [An LLM-Powered Agent for Real-Time Analysis of the Vietnamese IT Job Market](https://arxiv.org/abs/2511.14767)
*Minh-Thuan Nguyen,Thien Vo-Thanh,Thai-Duy Dinh,Xuan-Quang Phan,Tan-Ha Mai,Lam-Son Lê*

Main category: cs.IR

TL;DR: 该论文介绍了一个名为 AI Job Market Consultant 的新型会话代理，它可以根据实时数据提供深入的、数据驱动的职业见解。


<details>
  <summary>Details</summary>
Motivation: 越南的 IT 从业者在进入市场时，缺乏可靠的职业指导。现有的市场报告通常已经过时，而手动分析数千个职位发布对于大多数人来说是不切实际的。

Method: 该系统通过一个自动化的 pipeline 来创建自定义数据集，该 pipeline 使用 Playwright 爬取招聘门户网站，并利用大型语言模型 (LLM) 来智能地构建非结构化的职位发布数据。系统的核心是一个工具增强的 AI 代理，它基于 ReAct agentic 框架，这使得它能够通过用于 SQL 查询、语义搜索和数据可视化的专用工具箱自主地进行推理、计划和执行操作。

Result: 原型成功收集并分析了 3,745 个职位发布，展示了它回答复杂的多步骤查询、生成按需可视化以及提供基于真实世界数据的个性化职业建议的能力。

Conclusion: 这项工作为劳动力市场分析引入了一个新的范例，展示了专业的 agentic AI 系统如何能够为下一代专业人士 democratize 及时、可信的职业情报的访问。

Abstract: Individuals entering Vietnam's dynamic Information Technology (IT) job market face a critical gap in reliable career guidance. Existing market reports are often outdated, while the manual analysis of thousands of job postings is impractical for most. To address this challenge, we present the AI Job Market Consultant, a novel conversational agent that delivers deep, data-driven insights directly from the labor market in real-time. The foundation of our system is a custom-built dataset created via an automated pipeline that crawls job portals using Playwright and leverages the Large Language Model (LLM) to intelligently structure unstructured posting data. The core of our system is a tool-augmented AI agent, based on the ReAct agentic framework, which enables the ability of autonomously reasoning, planning, and executing actions through a specialized toolbox for SQL queries, semantic search, and data visualization. Our prototype successfully collected and analyzed 3,745 job postings, demonstrating its ability to answer complex, multi-step queries, generate on-demand visualizations, and provide personalized career advice grounded in real-world data. This work introduces a new paradigm for labor market analysis, showcasing how specialized agentic AI systems can democratize access to timely, trustworthy career intelligence for the next generation of professionals.

</details>


### [135] [Causally-Informed Reinforcement Learning for Adaptive Emotion-Aware Social Media Recommendation](https://arxiv.org/abs/2511.14768)
*Bhavika Jain,Robert Pitsko,Ananya Drishti,Mahfuza Farooque*

Main category: cs.IR

TL;DR: 提出了一个情感感知社交媒体推荐（ESMR）框架，根据用户不断变化的情感轨迹来个性化内容。


<details>
  <summary>Details</summary>
Motivation: 大多数社交媒体推荐系统只针对点击率、观看时长或滚动等参与度指标进行优化，而没有考虑用户的情感状态。重复接触带有情感色彩的内容会对用户的情感健康产生负面影响。

Method: ESMR集成了基于Transformer的情感预测器和混合推荐策略：在稳定时期使用LightGBM模型来提高参与度，当负面情绪持续存在时，使用具有因果信息奖励的强化学习代理。

Result: 通过对30天互动轨迹的行为基础评估，ESMR证明了其改善了情绪恢复，降低了波动性，并保持了强大的参与度。

Conclusion: ESMR提供了一条在不影响参与度的情况下实现情感感知推荐的途径。

Abstract: Social media recommendation systems play a central role in shaping users' emotional experiences. However, most systems are optimized solely for engagement metrics, such as click rate, viewing time, or scrolling, without accounting for users' emotional states. Repeated exposure to emotionally charged content has been shown to negatively affect users' emotional well-being over time. We propose an Emotion-aware Social Media Recommendation (ESMR) framework that personalizes content based on users' evolving emotional trajectories. ESMR integrates a Transformer-based emotion predictor with a hybrid recommendation policy: a LightGBM model for engagement during stable periods and a reinforcement learning agent with causally informed rewards when negative emotional states persist. Through behaviorally grounded evaluation over 30-day interaction traces, ESMR demonstrates improved emotional recovery, reduced volatility, and strong engagement retention. ESMR offers a path toward emotionally aware recommendations without compromising engagement performance.

</details>


### [136] [Cluster-based Adaptive Retrieval: Dynamic Context Selection for RAG Applications](https://arxiv.org/abs/2511.14769)
*Yifan Xu,Vipul Gupta,Rohit Aggarwal,Varsha Mahadevan,Bhaskar Krishnamachari*

Main category: cs.IR

TL;DR: 提出了一种名为Cluster-based Adaptive Retrieval (CAR) 的算法，该算法可以动态确定最佳文档数量。


<details>
  <summary>Details</summary>
Motivation: 现有的静态 top-k 检索方法无法适应查询的可变性，导致上下文不足或信息冗余。

Method: 通过分析排序的查询-文档相似度距离的聚类模式来动态确定最佳文档数量。

Result: 在 Coinbase 的 CDP 语料库和公共 MultiHop-RAG 基准测试中，CAR 始终选择最佳检索深度并实现最高 TES 分数，优于每个固定的 top-k 基线。在下游 RAG 评估中，CAR 将 LLM 令牌使用量减少了 60%，端到端延迟减少了 22%，并将幻觉减少了 10%，同时完全保留了答案相关性。自从将 CAR 集成到 Coinbase 的虚拟助手以来，用户参与度跃升了 200%。

Conclusion: CAR 能够根据查询复杂性动态调整检索文档的数量，从而提高 RAG 的效率和准确性。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by pulling in external material, document, code, manuals, from vast and ever-growing corpora, to effectively answer user queries. The effectiveness of RAG depends significantly on aligning the number of retrieved documents with query characteristics: narrowly focused queries typically require fewer, highly relevant documents, whereas broader or ambiguous queries benefit from retrieving more extensive supporting information. However, the common static top-k retrieval approach fails to adapt to this variability, resulting in either insufficient context from too few documents or redundant information from too many. Motivated by these challenges, we introduce Cluster-based Adaptive Retrieval (CAR), an algorithm that dynamically determines the optimal number of documents by analyzing the clustering patterns of ordered query-document similarity distances. CAR detects the transition point within similarity distances, where tightly clustered, highly relevant documents shift toward less pertinent candidates, establishing an adaptive cut-off that scales with query complexity. On Coinbase's CDP corpus and the public MultiHop-RAG benchmark, CAR consistently picks the optimal retrieval depth and achieves the highest TES score, outperforming every fixed top-k baseline. In downstream RAG evaluations, CAR cuts LLM token usage by 60%, trims end-to-end latency by 22%, and reduces hallucinations by 10% while fully preserving answer relevance. Since integrating CAR into Coinbase's virtual assistant, we've seen user engagement jump by 200%.

</details>


### [137] [ExplainRec: Towards Explainable Multi-Modal Zero-Shot Recommendation with Preference Attribution and Large Language Models](https://arxiv.org/abs/2511.14770)
*Bo Ma,LuYao Liu,ZeHua Hu,Simon Lau*

Main category: cs.IR

TL;DR: ExplainRec框架通过偏好归因、多模态融合和零样本迁移学习扩展了基于LLM的推荐能力，解决了可解释性和冷启动问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的推荐系统在可解释性和冷启动场景中面临挑战。

Method: 该框架包含四个技术贡献：用于可解释推荐的偏好归因调整，用于冷启动用户和项目的零样本偏好转移，利用视觉和文本内容的多模态增强，以及多任务协作优化。

Result: 在MovieLens-25M和Amazon数据集上的实验评估表明，ExplainRec优于现有方法，在电影推荐方面实现了0.7％的AUC改进，在跨域任务中实现了0.9％的AUC改进。

Conclusion: ExplainRec框架能够生成可解释的解释并有效处理冷启动场景。

Abstract: Recent advances in Large Language Models (LLMs) have opened new possibilities for recommendation systems, though current approaches such as TALLRec face challenges in explainability and cold-start scenarios. We present ExplainRec, a framework that extends LLM-based recommendation capabilities through preference attribution, multi-modal fusion, and zero-shot transfer learning. The framework incorporates four technical contributions: preference attribution tuning for explainable recommendations, zero-shot preference transfer for cold-start users and items, multi-modal enhancement leveraging visual and textual content, and multi-task collaborative optimization. Experimental evaluation on MovieLens-25M and Amazon datasets shows that ExplainRec outperforms existing methods, achieving AUC improvements of 0.7\% on movie recommendation and 0.9\% on cross-domain tasks, while generating interpretable explanations and handling cold-start scenarios effectively.

</details>


### [138] [SilverTorch: A Unified Model-based System to Democratize Large-Scale Recommendation on GPUs](https://arxiv.org/abs/2511.14881)
*Bi Xue,Hong Wu,Lei Chen,Chao Yang,Yiming Ma,Fei Ding,Zhen Wang,Liang Wang,Xiaoheng Mao,Ke Huang,Xialu Li,Peng Xia,Rui Jian,Yanli Zhao,Yanzun Huang,Yijie Deng,Harry Tran,Ryan Chang,Min Yu,Eric Dong,Jiazhou Wang,Qianqian Zhang,Keke Zhai,Hongzhang Yin,Pawel Garbacki,Zheng Fang,Yiyi Pan,Min Ni,Yang Liu*

Main category: cs.IR

TL;DR: SilverTorch是一个基于模型的系统，用于在GPU上服务推荐模型，通过用服务模型的层替换独立的索引和过滤服务来统一模型服务。


<details>
  <summary>Details</summary>
Motivation: 现有系统依赖于基于CPU的ANN索引和过滤服务，导致成本高昂，并且放弃了联合优化机会，使其难以支持更复杂的模型架构，例如学习相似性和多任务检索。

Method: 提出了一种在GPU上的Bloom索引算法用于特征过滤，以及一种在GPU上的张量原生融合Int8 ANN内核用于最近邻搜索。共同设计了ANN搜索索引和过滤索引，以减少GPU内存利用率并消除不必要的计算。引入了一个OverArch评分层和一个Value Model来聚合跨多任务的结果。通过在服务模型中缓存预先计算的嵌入来加速项目嵌入计算。

Result: 在行业规模数据集上的评估表明，与最先进的方法相比，SilverTorch实现了高达5.6倍的更低延迟和23.7倍的更高吞吐量。SilverTorch的解决方案比基于CPU的解决方案的成本效益高出13.35倍，同时通过服务更复杂的模型来提高准确性。

Conclusion: SilverTorch服务于主要产品中的数百个在线模型，并为数十亿的日活跃用户推荐内容。

Abstract: Serving deep learning based recommendation models (DLRM) at scale is challenging. Existing systems rely on CPU-based ANN indexing and filtering services, suffering from non-negligible costs and forgoing joint optimization opportunities. Such inefficiency makes them difficult to support more complex model architectures, such as learned similarities and multi-task retrieval.
  In this paper, we propose SilverTorch, a model-based system for serving recommendation models on GPUs. SilverTorch unifies model serving by replacing standalone indexing and filtering services with layers of served models. We propose a Bloom index algorithm on GPUs for feature filtering and a tensor-native fused Int8 ANN kernel on GPUs for nearest neighbor search. We further co-design the ANN search index and filtering index to reduce GPU memory utilization and eliminate unnecessary computation. Benefit from SilverTorch's serving paradigm, we introduce a OverArch scoring layer and a Value Model to aggregate results across multi-tasks. These advancements improve the accuracy for retrieval and enable future studies for serving more complex models. For ranking, SilverTorch's design accelerates item embedding calculation by caching the pre-calculated embeddings inside the serving model.
  Our evaluation on the industry-scale datasets show that SilverTorch achieves up to 5.6x lower latency and 23.7x higher throughput compared to the state-of-the-art approaches. We also demonstrate that SilverTorch's solution is 13.35x more cost-efficient than CPU-based solution while improving accuracy via serving more complex models. SilverTorch serves over hundreds of models online across major products and recommends contents for billions of daily active users.

</details>


### [139] [Multi-Aspect Cross-modal Quantization for Generative Recommendation](https://arxiv.org/abs/2511.15122)
*Fuwei Zhang,Xiaoyu Liu,Dongbo Xi,Jishen Yin,Huan Chen,Peng Yan,Fuzhen Zhuang,Zhao Zhang*

Main category: cs.IR

TL;DR: 提出了一个名为 MACRec 的生成推荐模型，该模型利用多模态信息来学习高质量的语义 ID，并改进生成模型的训练。


<details>
  <summary>Details</summary>
Motivation: 现有的生成推荐方法在利用多模态信息和捕捉模态间复杂交互方面存在局限性，这限制了语义 ID 的质量和生成模型的训练效果。

Method: 1. 在 ID 学习过程中引入跨模态量化，以减少冲突率并提高码本的可用性。2. 结合隐式和显式对齐，进一步增强生成模型的生成能力。

Result: 在三个推荐数据集上的大量实验表明，该方法是有效的。

Conclusion: MACRec 模型能够有效地利用多模态信息，学习高质量的语义 ID，并提升生成推荐模型的性能。

Abstract: Generative Recommendation (GR) has emerged as a new paradigm in recommender systems. This approach relies on quantized representations to discretize item features, modeling users' historical interactions as sequences of discrete tokens. Based on these tokenized sequences, GR predicts the next item by employing next-token prediction methods. The challenges of GR lie in constructing high-quality semantic identifiers (IDs) that are hierarchically organized, minimally conflicting, and conducive to effective generative model training. However, current approaches remain limited in their ability to harness multimodal information and to capture the deep and intricate interactions among diverse modalities, both of which are essential for learning high-quality semantic IDs and for effectively training GR models. To address this, we propose Multi-Aspect Cross-modal quantization for generative Recommendation (MACRec), which introduces multimodal information and incorporates it into both semantic ID learning and generative model training from different aspects. Specifically, we first introduce cross-modal quantization during the ID learning process, which effectively reduces conflict rates and thus improves codebook usability through the complementary integration of multimodal information. In addition, to further enhance the generative ability of our GR model, we incorporate multi-aspect cross-modal alignments, including the implicit and explicit alignments. Finally, we conduct extensive experiments on three well-known recommendation datasets to demonstrate the effectiveness of our proposed method.

</details>


### [140] [ItemRAG: Item-Based Retrieval-Augmented Generation for LLM-Based Recommendation](https://arxiv.org/abs/2511.15141)
*Sunwoo Kim,Geon Lee,Kyungho Kim,Jaemin Yoo,Kijung Shin*

Main category: cs.IR

TL;DR: 本文提出了一种名为 ItemRAG 的基于物品的检索增强生成方法，用于改进基于 LLM 的推荐系统，尤其是在冷启动物品方面。


<details>
  <summary>Details</summary>
Motivation: 现有基于 LLM 的推荐系统通常采用用户检索增强生成 (RAG) 方法，但这些方法侧重于检索与目标用户相似的用户的购买模式。为了更好地利用物品之间的关系，本文提出 ItemRAG。

Method: ItemRAG 检索与目标物品相关的物品，利用物品-物品共同购买历史，并结合语义相似性和共同购买频率来提高检索物品的相关性，从而更好地处理冷启动物品。

Result: 实验结果表明，ItemRAG 在 Hit-Ratio-1 指标上将零样本 LLM 推荐器性能提高了高达 43%，并且在标准和冷启动物品推荐设置下均优于基于用户的 RAG 基线。

Conclusion: ItemRAG 能够有效提升基于 LLM 的推荐系统性能，尤其是在处理冷启动物品时，通过捕捉物品之间的共同购买模式来实现。

Abstract: Recently, large language models (LLMs) have been widely used as recommender systems, owing to their strong reasoning capability and their effectiveness in handling cold-start items. To better adapt LLMs for recommendation, retrieval-augmented generation (RAG) has been incorporated. Most existing RAG methods are user-based, retrieving purchase patterns of users similar to the target user and providing them to the LLM. In this work, we propose ItemRAG, an item-based RAG method for LLM-based recommendation that retrieves relevant items (rather than users) from item-item co-purchase histories. ItemRAG helps LLMs capture co-purchase patterns among items, which are beneficial for recommendations. Especially, our retrieval strategy incorporates semantically similar items to better handle cold-start items and uses co-purchase frequencies to improve the relevance of the retrieved items. Through extensive experiments, we demonstrate that ItemRAG consistently (1) improves the zero-shot LLM-based recommender by up to 43% in Hit-Ratio-1 and (2) outperforms user-based RAG baselines under both standard and cold-start item recommendation settings.

</details>


### [141] [Selective Mixup for Debiasing Question Selection in Computerized Adaptive Testing](https://arxiv.org/abs/2511.15241)
*Mi Tian,Kun Zhang,Fei Liu,Jinglong Li,Yuxin Liao,Chenxi Bai,Zhengtao Tan,Le Wu,Richang Hong*

Main category: cs.IR

TL;DR: 提出了一种解决计算机自适应测试(CAT)中选择偏差问题的去偏框架。


<details>
  <summary>Details</summary>
Motivation: 现有的CAT研究主要集中在提高诊断准确性，忽略了自适应过程中固有的选择偏差。选择偏差会导致诊断模型的不准确和有偏预测。

Method: 该框架包括交叉属性考生检索和选择性Mixup正则化两个模块。首先检索平衡的考生作为偏差考生的中性参考，然后应用Mixup来丰富偏差冲突样本的多样性并平滑选择边界。

Result: 在多个诊断模型上的实验表明，该方法可以显著提高CAT中问题选择的泛化能力和公平性。

Conclusion: 该研究提出了一种有效的去偏框架，可以解决CAT中的选择偏差问题，提高诊断模型的准确性和公平性。

Abstract: Computerized Adaptive Testing (CAT) is a widely used technology for evaluating learners' proficiency in online education platforms. By leveraging prior estimates of proficiency to select questions and updating the estimates iteratively based on responses, CAT enables personalized learner modeling and has attracted substantial attention. Despite this progress, most existing works focus primarily on improving diagnostic accuracy, while overlooking the selection bias inherent in the adaptive process. Selection Bias arises because the question selection is strongly influenced by the estimated proficiency, such as assigning easier questions to learners with lower proficiency and harder ones to learners with higher proficiency. Since the selection depends on prior estimation, this bias propagates into the diagnosis model, which is further amplified during iterative updates, leading to misalignment and biased predictions. Moreover, the imbalanced nature of learners' historical interactions often exacerbates the bias in diagnosis models. To address this issue, we propose a debiasing framework consisting of two key modules: Cross-Attribute Examinee Retrieval and Selective Mixup-based Regularization. First, we retrieve balanced examinees with relatively even distributions of correct and incorrect responses and use them as neutral references for biased examinees. Then, mixup is applied between each biased examinee and its matched balanced counterpart under label consistency. This augmentation enriches the diversity of bias-conflicting samples and smooths selection boundaries. Finally, extensive experiments on two benchmark datasets with multiple advanced diagnosis models demonstrate that our method substantially improves both the generalization ability and fairness of question selection in CAT.

</details>


### [142] [Unveiling Inference Scaling for Difference-Aware User Modeling in LLM Personalization](https://arxiv.org/abs/2511.15389)
*Suyu Chen,Yimeng Bai,Yulong Huang,Xiaoyan Zhao,Yang Zhang*

Main category: cs.IR

TL;DR: 这篇论文提出了一种名为 Difference-aware Reasoning Personalization (DRP) 的框架，旨在通过利用推理缩放来增强 LLM 的个性化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在个性化方面主要依赖用户自身历史，忽略了用户间的差异。即使有方法试图模拟这些差异，其特征提取过程也依赖于固定的维度和快速直观的推断，限制了捕获用户差异的覆盖范围和粒度。

Method: DRP 框架通过利用推理缩放来重建差异提取机制，自主识别相关的差异特征维度，并生成结构化的定义和描述，从而实现对用户差异的缓慢而慎重的推理。

Result: 在个性化评论生成的实验中，DRP 在多个指标上始终优于基线方法。

Conclusion: DRP 框架有效地提升了 LLM 的个性化能力。

Abstract: Large Language Models (LLMs) are increasingly integrated into users' daily lives, driving a growing demand for personalized outputs. Prior work has primarily leveraged a user's own history, often overlooking inter-user differences that are critical for effective personalization. While recent methods have attempted to model such differences, their feature extraction processes typically rely on fixed dimensions and quick, intuitive inference (System-1 thinking), limiting both the coverage and granularity of captured user differences. To address these limitations, we propose Difference-aware Reasoning Personalization (DRP), a framework that reconstructs the difference extraction mechanism by leveraging inference scaling to enhance LLM personalization. DRP autonomously identifies relevant difference feature dimensions and generates structured definitions and descriptions, enabling slow, deliberate reasoning (System-2 thinking) over user differences. Experiments on personalized review generation demonstrate that DRP consistently outperforms baseline methods across multiple metrics.

</details>


### [143] [CroPS: Improving Dense Retrieval with Cross-Perspective Positive Samples in Short-Video Search](https://arxiv.org/abs/2511.15443)
*Ao Xie,Jiahui Chen,Quanzhi Zhu,Xiaoze Jiang,Zhiheng Qin,Enyun Yu,Han Li*

Main category: cs.IR

TL;DR: 提出了CroPS，一个新颖的检索数据引擎，旨在通过引入来自多个角度的、多样化的和语义上有意义的正面例子来缓解这个问题。


<details>
  <summary>Details</summary>
Motivation: 工业系统采用自我强化的训练流程，依赖于历史上暴露的用户交互进行监督，导致过滤器气泡效应，先前未见过的相关内容被排除在训练信号之外，使模型偏向于狭窄和保守的检索。

Method: 通过用户查询重构行为（查询级别）、推荐流中的参与数据（系统级别）以及大型语言模型合成的世界知识（知识级别）来增强训练，提出了分层标签分配（HLA）策略和相应的H-InfoNCE损失，共同实现细粒度的、相关性感知的优化。

Result: 在大型商业短视频搜索平台快手搜索上进行的大量实验表明，CroPS在离线和在线A/B测试中均显着优于强大的基线，实现了卓越的检索性能并降低了查询重构率。

Conclusion: CroPS现已完全部署在快手搜索中，每天为数亿用户提供服务。

Abstract: Dense retrieval has become a foundational paradigm in modern search systems, especially on short-video platforms. However, most industrial systems adopt a self-reinforcing training pipeline that relies on historically exposed user interactions for supervision. This paradigm inevitably leads to a filter bubble effect, where potentially relevant but previously unseen content is excluded from the training signal, biasing the model toward narrow and conservative retrieval. In this paper, we present CroPS (Cross-Perspective Positive Samples), a novel retrieval data engine designed to alleviate this problem by introducing diverse and semantically meaningful positive examples from multiple perspectives. CroPS enhances training with positive signals derived from user query reformulation behavior (query-level), engagement data in recommendation streams (system-level), and world knowledge synthesized by large language models (knowledge-level). To effectively utilize these heterogeneous signals, we introduce a Hierarchical Label Assignment (HLA) strategy and a corresponding H-InfoNCE loss that together enable fine-grained, relevance-aware optimization. Extensive experiments conducted on Kuaishou Search, a large-scale commercial short-video search platform, demonstrate that CroPS significantly outperforms strong baselines both offline and in live A/B tests, achieving superior retrieval performance and reducing query reformulation rates. CroPS is now fully deployed in Kuaishou Search, serving hundreds of millions of users daily.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [144] [Transformer Injectivity & Geometric Robustness - Analytic Margins and Bi-Lipschitz Uniformity of Sequence-Level Hidden States](https://arxiv.org/abs/2511.14808)
*Mikael von Strauss*

Main category: cs.LG

TL;DR: 这篇论文研究了decoder-only Transformer中离散提示到last-token隐藏状态的映射的单射性。论文定义了碰撞判别式和单射层，证明了一个二分法：模型要么在集合上处处非单射，要么存在开放且稠密的单射层。在温和的非奇异性假设下，单射性在训练过程中保持。论文还研究了对称群，表明判别式和单射层可以下降到商空间。实证研究表明，Transformer表示在连续参数理想化中通常是单射的，其实际可逆性可以使用简单的几何诊断来探测。


<details>
  <summary>Details</summary>
Motivation: 研究decoder-only Transformer中离散提示到last-token隐藏状态映射的单射性，并探讨其在训练过程中的持久性。

Method: 通过定义碰撞判别式和单射层，证明二分法，并研究对称群。通过分离裕度和co-Lipschitz常数等几何诊断方法进行实证研究。

Result: 在全精度或8位量化下未观察到碰撞，而4位量化会引起少量碰撞并显着缩小co-Lipschitz估计。对于从小处训练的GPT-2，标准化指标在训练过程中保持稳定。

Conclusion: Transformer表示在连续参数理想化中通常是单射的，其实际可逆性可以使用简单的几何诊断来探测。

Abstract: Under real-analytic assumptions on decoder-only Transformers, recent work shows that the map from discrete prompts to last-token hidden states is generically injective on finite prompt sets. We refine this picture: for each layer $\ell$ we define a collision discriminant $Δ^\ell \subset Θ$ and injective stratum $U^\ell = Θ\setminus Δ^\ell$, and prove a dichotomy -- either the model is nowhere injective on the set, or $U^\ell$ is open and dense and every $F^\ell_θ$ is injective. Under mild non-singularity assumptions on the optimizer and an absolutely continuous initialization, generic injectivity persists along smooth training trajectories over any fixed horizon. We also treat symmetry groups $G$, showing that discriminants and injective strata descend to the quotient $Θ/G$, so injectivity is naturally a property of functional equivalence classes.
  We complement these results with an empirical study of layerwise geometric diagnostics. We define a separation margin and a co-Lipschitz (lower Lipschitz) constant between prompt space and last-token representation space, estimated via nearest-neighbor statistics on large prompt sets. Applying these diagnostics to pretrained LLaMA-3 and Qwen models, we study behavior across layers, sequence lengths, model scales, and 8- and 4-bit activation quantization. On our sampled prompts we see no collisions in full precision or at 8 bits, while 4-bit quantization induces a small number of collisions and markedly shrinks co-Lipschitz estimates. For a small GPT-2 trained from scratch, normalized metrics remain stable over training. Overall, the results suggest that Transformer representations are generically and persistently injective in the continuous-parameter idealization, while their practical invertibility can be probed using simple geometric diagnostics.

</details>


### [145] [DEVAL: A Framework for Evaluating and Improving the Derivation Capability of Large Language Models](https://arxiv.org/abs/2511.14813)
*Yifan Li,Qin Li,Min Zhang,Min Zhang,Peixin Wang*

Main category: cs.LG

TL;DR: 论文研究了大型语言模型(llm)在数据上进行推理的能力，并提出了一个名为DEVAL的评估框架来评估llm的推导能力(DC)。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在数据上的推理能力有待提高。人类可以根据输入的变化来修改输出，这种基于抽象规则的推理模式在llm中没有得到充分的描述或评估。

Method: 提出了推导关系(DR)的概念，并构建了DEVAL评估框架来评估llm在七个主流任务中的DC。同时，提出了一种新的提示工程方法，称为推导提示(DP)，以提高llm的DC。

Result: 主流llm，如GPT-4o和Claude3.5，在识别DR方面表现出一定的能力，但在解决问题时应用DR的效果显著下降。DP方法在所有测试的llm中平均提高了15.2%的DC。

Conclusion: 主流llm在应用DR解决问题方面的能力有待提高，而提出的DP方法可以有效提高llm的DC。

Abstract: Assessing the reasoning ability of Large Language Models (LLMs) over data remains an open and pressing research question. Compared with LLMs, human reasoning can derive corresponding modifications to the output based on certain kinds of changes to the input. This reasoning pattern, which relies on abstract rules that govern relationships between changes of data, has not been comprehensively described or evaluated in LLMs. In this paper, we formally define this reasoning pattern as the Derivation Relation (DR) and introduce the concept of Derivation Capability (DC), i.e. applying DR by making the corresponding modification to the output whenever the input takes certain changes. To assess DC, a systematically constructed evaluation framework named DEVAL is proposed and used to evaluate five popular LLMs and one Large Reasoning Model in seven mainstream tasks. The evaluation results show that mainstream LLMs, such as GPT-4o and Claude3.5, exhibit moderate DR recognition capabilities but reveal significant drop-offs on applying DR effectively in problem-solving scenarios. To improve this, we propose a novel prompt engineering approach called Derivation Prompting (DP). It achieves an average improvement of 15.2% in DC for all tested LLMs, outperforming commonly used prompt engineering techniques.

</details>


### [146] [Dynamic Nested Hierarchies: Pioneering Self-Evolution in Machine Learning Architectures for Lifelong Intelligence](https://arxiv.org/abs/2511.14823)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.LG

TL;DR: 提出了动态嵌套层次结构，以提升模型在非平稳环境中的持续学习能力。


<details>
  <summary>Details</summary>
Motivation: 现有模型在静态任务中表现出色，但在非平稳环境中因架构僵化而失效，阻碍了持续适应和终身学习。

Method: 通过赋予模型自主调整优化层级数量、嵌套结构和更新频率的能力，实现自我进化。

Result: 在语言建模、持续学习和长文本推理中表现出卓越性能。

Conclusion: 动态嵌套层次结构为自适应通用智能奠定了基础。

Abstract: Contemporary machine learning models, including large language models, exhibit remarkable capabilities in static tasks yet falter in non-stationary environments due to rigid architectures that hinder continual adaptation and lifelong learning. Building upon the nested learning paradigm, which decomposes models into multi-level optimization problems with fixed update frequencies, this work proposes dynamic nested hierarchies as the next evolutionary step in advancing artificial intelligence and machine learning. Dynamic nested hierarchies empower models to autonomously adjust the number of optimization levels, their nesting structures, and update frequencies during training or inference, inspired by neuroplasticity to enable self-evolution without predefined constraints. This innovation addresses the anterograde amnesia in existing models, facilitating true lifelong learning by dynamically compressing context flows and adapting to distribution shifts. Through rigorous mathematical formulations, theoretical proofs of convergence, expressivity bounds, and sublinear regret in varying regimes, alongside empirical demonstrations of superior performance in language modeling, continual learning, and long-context reasoning, dynamic nested hierarchies establish a foundational advancement toward adaptive, general-purpose intelligence.

</details>


### [147] [Empowering Multi-Turn Tool-Integrated Reasoning with Group Turn Policy Optimization](https://arxiv.org/abs/2511.14846)
*Yifeng Ding,Hung Le,Songyang Han,Kangrui Ruan,Zhenghui Jin,Varun Kumar,Zijian Wang,Anoop Deoras*

Main category: cs.LG

TL;DR: 现有的强化学习方法在训练用于多轮工具集成推理 (TIR) 的大型语言模型 (LLM) 方面仍然具有挑战性。当前的 RL 方法存在粗粒度、轨迹级别的奖励问题，导致训练停滞。为了解决这个问题，我们提出了 Group Turn Policy Optimization (GTPO)，这是一种专为在多轮 TIR 任务上训练 LLM 而设计的新型 RL 算法。在不同的推理基准测试中，GTPO 的性能平均优于 GRPO 3.0%，从而确立了其在推进现实世界中复杂数学推理方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法对于复杂的多轮交互提供的学习信号不足，导致训练停滞。

Method: 我们提出了一种新颖的 RL 算法，Group Turn Policy Optimization (GTPO)，它专门为训练 LLM 以执行多轮 TIR 任务而设计。GTPO 引入了三项关键创新：(1) 轮次级别奖励分配，为单个轮次提供细粒度的反馈，(2) 基于回报的优势估计，其中将标准化折扣回报计算为优势，以及 (3) 自监督奖励塑造，它利用来自生成的代码的自监督信号来密集化稀疏的基于二元结果的奖励。

Result: GTPO 在不同的推理基准测试中平均优于 GRPO 3.0%。

Conclusion: GTPO 的有效性已得到证实，它可以推进现实世界中复杂的数学推理。

Abstract: Training Large Language Models (LLMs) for multi-turn Tool-Integrated Reasoning (TIR) - where models iteratively reason, generate code, and verify through execution - remains challenging for existing reinforcement learning (RL) approaches. Current RL methods, exemplified by Group Relative Policy Optimization (GRPO), suffer from coarse-grained, trajectory-level rewards that provide insufficient learning signals for complex multi-turn interactions, leading to training stagnation. To address this issue, we propose Group Turn Policy Optimization (GTPO), a novel RL algorithm specifically designed for training LLMs on multi-turn TIR tasks. GTPO introduces three key innovations: (1) turn-level reward assignment that provides fine-grained feedback for individual turns, (2) return-based advantage estimation where normalized discounted returns are calculated as advantages, and (3) self-supervised reward shaping that exploits self-supervision signals from generated code to densify sparse binary outcome-based rewards. Our comprehensive evaluation demonstrates that GTPO outperforms GRPO by 3.0% on average across diverse reasoning benchmarks, establishing its effectiveness for advancing complex mathematical reasoning in the real world.

</details>


### [148] [FinTRec: Transformer Based Unified Contextual Ads Targeting and Personalization for Financial Applications](https://arxiv.org/abs/2511.14865)
*Dwipam Katariya,Snehita Varma,Akshat Shreemali,Benjamin Wu,Kalanand Mishra,Pranab Mohanty*

Main category: cs.LG

TL;DR: FinTRec: A transformer-based framework for sequential recommendation in Financial Services (FS).


<details>
  <summary>Details</summary>
Motivation: Applying transformers in FS for real-time recommendation faces challenges like long-range user interactions, heterogeneous context, and multiple interrelated products with competing business goals.

Method: Proposes FinTRec, a unified transformer-based architecture.

Result: FinTRec outperforms production-grade tree-based baselines in historic simulation and live A/B tests. It also enables cross-product signal sharing, reduces training cost and technical debt, and improves offline performance.

Conclusion: FinTRec offers a viable and effective shift toward transformer-based architectures in FS, addressing both technical and business considerations.

Abstract: Transformer-based architectures are widely adopted in sequential recommendation systems, yet their application in Financial Services (FS) presents distinct practical and modeling challenges for real-time recommendation. These include:a) long-range user interactions (implicit and explicit) spanning both digital and physical channels generating temporally heterogeneous context, b) the presence of multiple interrelated products require coordinated models to support varied ad placements and personalized feeds, while balancing competing business goals. We propose FinTRec, a transformer-based framework that addresses these challenges and its operational objectives in FS. While tree-based models have traditionally been preferred in FS due to their explainability and alignment with regulatory requirements, our study demonstrate that FinTRec offers a viable and effective shift toward transformer-based architectures. Through historic simulation and live A/B test correlations, we show FinTRec consistently outperforms the production-grade tree-based baseline. The unified architecture, when fine-tuned for product adaptation, enables cross-product signal sharing, reduces training cost and technical debt, while improving offline performance across all products. To our knowledge, this is the first comprehensive study of unified sequential recommendation modeling in FS that addresses both technical and business considerations.

</details>


### [149] [Transformer-Guided Deep Reinforcement Learning for Optimal Takeoff Trajectory Design of an eVTOL Drone](https://arxiv.org/abs/2511.14887)
*Nathan M. Roberts,Xiaosong Du*

Main category: cs.LG

TL;DR: 本文提出了一种基于 Transformer 引导的深度强化学习 (DRL) 方法，用于优化 eVTOL 飞机的起飞轨迹，以实现最小能耗。


<details>
  <summary>Details</summary>
Motivation: 传统的优化控制方法在解决复杂问题时受到维度和复杂性的限制。深度强化学习虽然可以处理复杂系统，但训练难度大。因此，本文旨在解决 DRL 训练难度的问题，以实现更广泛的 eVTOL 飞机应用。

Method: 本文提出了一种 Transformer 引导的 DRL 方法，该方法利用 Transformer 来探索每个时间步的真实状态空间，从而减轻训练难度。该方法应用于 eVTOL 无人机的最佳起飞轨迹设计，通过改变控制变量（功率和机翼与垂直方向的角度）来满足起飞条件（最小垂直位移和最小水平速度），从而实现最小能耗。

Result: 实验结果表明，Transformer 引导的 DRL 智能体学习起飞所需的步数为 4.57 × 10^6，仅为 vanilla DRL 智能体的 25%（19.79 × 10^6 步）。此外，Transformer 引导的 DRL 在最佳能耗方面的准确率达到 97.2%，而 vanilla DRL 的准确率为 96.3%。

Conclusion: 本文提出的 Transformer 引导的 DRL 在训练效率和最佳设计验证方面均优于 vanilla DRL。

Abstract: The rapid advancement of electric vertical take-off and landing (eVTOL) aircraft offers a promising opportunity to alleviate urban traffic congestion. Thus, developing optimal takeoff trajectories for minimum energy consumption becomes essential for broader eVTOL aircraft applications. Conventional optimal control methods (such as dynamic programming and linear quadratic regulator) provide highly efficient and well-established solutions but are limited by problem dimensionality and complexity. Deep reinforcement learning (DRL) emerges as a special type of artificial intelligence tackling complex, nonlinear systems; however, the training difficulty is a key bottleneck that limits DRL applications. To address these challenges, we propose the transformer-guided DRL to alleviate the training difficulty by exploring a realistic state space at each time step using a transformer. The proposed transformer-guided DRL was demonstrated on an optimal takeoff trajectory design of an eVTOL drone for minimal energy consumption while meeting takeoff conditions (i.e., minimum vertical displacement and minimum horizontal velocity) by varying control variables (i.e., power and wing angle to the vertical). Results presented that the transformer-guided DRL agent learned to take off with $4.57\times10^6$ time steps, representing 25% of the $19.79\times10^6$ time steps needed by a vanilla DRL agent. In addition, the transformer-guided DRL achieved 97.2% accuracy on the optimal energy consumption compared against the simulation-based optimal reference while the vanilla DRL achieved 96.3% accuracy. Therefore, the proposed transformer-guided DRL outperformed vanilla DRL in terms of both training efficiency as well as optimal design verification.

</details>


### [150] [Bringing Federated Learning to Space](https://arxiv.org/abs/2511.14889)
*Grace Kim,Filip Svoboda,Nicholas Lane*

Main category: cs.LG

TL;DR: 本文对近地轨道 (LEO) 卫星星座应用联邦学习 (FL) 的可行性进行了系统分析，并提出了一个全面的“空间化”框架，以适应轨道约束。


<details>
  <summary>Details</summary>
Motivation: 随着低地球轨道（LEO）卫星星座迅速扩展到数百甚至数千颗航天器，对分布式星载机器学习的需求变得至关重要，以解决下行链路带宽限制。

Method: 该研究提出一个全面的“空间化”框架，该框架调整了地面算法（FedAvg、FedProx、FedBuff）以在轨道约束下运行，从而生成一套适合轨道运行的联邦学习算法。通过对 768 个星座配置进行参数扫描，评估这些空间化方法的性能。

Result: 结果表明，空间适配的联邦学习算法可以有效地扩展到多达 100 颗卫星的星座，其性能接近集中式理想状态。多月训练周期可以缩短到几天，通过轨道调度和卫星集群内的本地协调，实现了 9 倍的加速。

Conclusion: 这些结果为未来的任务设计者提供了可操作的见解，从而为更自主、更有弹性和数据驱动的卫星操作实现了分布式星载学习。

Abstract: As Low Earth Orbit (LEO) satellite constellations rapidly expand to hundreds and thousands of spacecraft, the need for distributed on-board machine learning becomes critical to address downlink bandwidth limitations. Federated learning (FL) offers a promising framework to conduct collaborative model training across satellite networks. Realizing its benefits in space naturally requires addressing space-specific constraints, from intermittent connectivity to dynamics imposed by orbital motion. This work presents the first systematic feasibility analysis of adapting off-the-shelf FL algorithms for satellite constellation deployment. We introduce a comprehensive "space-ification" framework that adapts terrestrial algorithms (FedAvg, FedProx, FedBuff) to operate under orbital constraints, producing an orbital-ready suite of FL algorithms. We then evaluate these space-ified methods through extensive parameter sweeps across 768 constellation configurations that vary cluster sizes (1-10), satellites per cluster (1-10), and ground station networks (1-13). Our analysis demonstrates that space-adapted FL algorithms efficiently scale to constellations of up to 100 satellites, achieving performance close to the centralized ideal. Multi-month training cycles can be reduced to days, corresponding to a 9x speedup through orbital scheduling and local coordination within satellite clusters. These results provide actionable insights for future mission designers, enabling distributed on-board learning for more autonomous, resilient, and data-driven satellite operations.

</details>


### [151] [It's LIT! Reliability-Optimized LLMs with Inspectable Tools](https://arxiv.org/abs/2511.14903)
*Ruixin Zhang,Jon Donnelly,Zhicheng Guo,Ghazal Khalighinejad,Haiyang Huang,Alina Jade Barnett,Cynthia Rudin*

Main category: cs.LG

TL;DR: LIT框架通过强制LLMs使用外部工具来解决问题，提高LLMs在解决实际问题时的可靠性和可追溯性。


<details>
  <summary>Details</summary>
Motivation: LLMs的推理过程不透明，导致其在需要高度信任的领域中的应用受限。LLMs有时会选择不可靠且难以调试的解决方案。

Method: 提出了一个名为LIT的框架，该框架利用现有LLMs的工具调用功能，使其能够选择最可靠和易于调试的解决方案路径，这可能涉及多个连续的工具调用。

Result: LIT框架使LLMs能够实现更可靠和知情的解决问题，同时保持任务性能。

Conclusion: 通过LIT框架，LLMs可以在解决问题时更加可靠和透明，从而提高其在实际应用中的价值。

Abstract: Large language models (LLMs) have exhibited remarkable capabilities across various domains. The ability to call external tools further expands their capability to handle real-world tasks. However, LLMs often follow an opaque reasoning process, which limits their usefulness in high-stakes domains where solutions need to be trustworthy to end users. LLMs can choose solutions that are unreliable and difficult to troubleshoot, even if better options are available. We address this issue by forcing LLMs to use external -- more reliable -- tools to solve problems when possible. We present a framework built on the tool-calling capabilities of existing LLMs to enable them to select the most reliable and easy-to-troubleshoot solution path, which may involve multiple sequential tool calls. We refer to this framework as LIT (LLMs with Inspectable Tools). In order to support LIT, we introduce a new and challenging benchmark dataset of 1,300 questions and a customizable set of reliability cost functions associated with a collection of specialized tools. These cost functions summarize how reliable each tool is and how easy it is to troubleshoot. For instance, a calculator is reliable across domains, whereas a linear prediction model is not reliable if there is distribution shift, but it is easy to troubleshoot. A tool that constructs a random forest is neither reliable nor easy to troubleshoot. These tools interact with the Harvard USPTO Patent Dataset and a new dataset of NeurIPS 2023 papers to solve mathematical, coding, and modeling problems of varying difficulty levels. We demonstrate that LLMs can achieve more reliable and informed problem-solving while maintaining task performance using our framework.

</details>


### [152] [Structured Contrastive Learning for Interpretable Latent Representations](https://arxiv.org/abs/2511.14920)
*Zhengyang Shen,Hua Tu,Mayue Shi*

Main category: cs.LG

TL;DR: 提出了一种新的结构化对比学习 (SCL) 框架，通过将潜在空间划分为不变特征、变异特征和自由特征三个语义组，提高神经网络对语义无关转换的鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 神经网络对语义无关的转换表现出严重的脆弱性，例如心电图 (ECG) 相移和 IMU 传感器旋转会导致性能下降。根本原因是“自由放任”的表征学习，其中潜在空间在满足任务性能的情况下不受约束地演变。

Method: 提出了结构化对比学习 (SCL)，它将潜在空间表示划分为三个语义组：在给定变换下保持一致的不变特征，通过新的变异机制主动区分变换的变异特征，以及保持任务灵活性的自由特征。变异机制通过鼓励变异特征区分正对内的特征来增强对比学习。

Result: 在 ECG 相位不变性和 IMU 旋转鲁棒性的实验中表现出优越的性能：在相位偏移下，ECG 相似性从 0.25 提高到 0.91，而 WISDM 活动识别在 95.38% 的旋转一致性下实现了 86.65% 的准确率，始终优于传统的数据增强。

Conclusion: 这项工作代表了从被动数据增强到主动结构学习的范式转变，从而在神经网络中实现可解释的潜在表示。

Abstract: Neural networks exhibit severe brittleness to semantically irrelevant transformations. A mere 75ms electrocardiogram (ECG) phase shift degrades latent cosine similarity from 1.0 to 0.2, while sensor rotations collapse activity recognition performance with inertial measurement units (IMUs). We identify the root cause as "laissez-faire" representation learning, where latent spaces evolve unconstrained provided task performance is satisfied. We propose Structured Contrastive Learning (SCL), a framework that partitions latent space representations into three semantic groups: invariant features that remain consistent under given transformations (e.g., phase shifts or rotations), variant features that actively differentiate transformations via a novel variant mechanism, and free features that preserve task flexibility. This creates controllable push-pull dynamics where different latent dimensions serve distinct, interpretable purposes. The variant mechanism enhances contrastive learning by encouraging variant features to differentiate within positive pairs, enabling simultaneous robustness and interpretability. Our approach requires no architectural modifications and integrates seamlessly into existing training pipelines. Experiments on ECG phase invariance and IMU rotation robustness demonstrate superior performance: ECG similarity improves from 0.25 to 0.91 under phase shifts, while WISDM activity recognition achieves 86.65% accuracy with 95.38% rotation consistency, consistently outperforming traditional data augmentation. This work represents a paradigm shift from reactive data augmentation to proactive structural learning, enabling interpretable latent representations in neural networks.

</details>


### [153] [Integrating Causal Inference with Graph Neural Networks for Alzheimer's Disease Analysis](https://arxiv.org/abs/2511.14922)
*Pranay Kumar Peddi,Dhrubajyoti Ghosh*

Main category: cs.LG

TL;DR: Causal-GCN, 利用干预图卷积框架，通过后门调整识别对阿尔茨海默病(AD)进展有稳定因果影响的脑区。


<details>
  <summary>Details</summary>
Motivation: 以往基于MRI的深度图学习模型多为相关性研究，混淆了人口统计学和遗传因素与疾病特异性特征。

Method: 使用Causal-GCN，一个基于do-calculus后门调整的干预图卷积框架，整合年龄、性别和APOE4基因型等混淆因素，识别对AD进展有稳定因果影响的脑区。将每个受试者的MRI表示为一个结构连接组，其中节点表示皮质和皮质下区域，边表示解剖连接。

Result: 在来自ADNI队列的484名受试者上的应用表明，Causal-GCN实现了与基线GNN相当的性能，同时提供了可解释的因果效应排序，突出了与已建立的AD神经病理学一致的后部、扣带和岛叶中枢。

Conclusion: Causal-GCN能够识别对AD进展有稳定因果影响的脑区，并提供可解释的因果效应排序。

Abstract: Deep graph learning has advanced Alzheimer's (AD) disease classification from MRI, but most models remain correlational, confounding demographic and genetic factors with disease specific features. We present Causal-GCN, an interventional graph convolutional framework that integrates do-calculus-based back-door adjustment to identify brain regions exerting stable causal influence on AD progression. Each subject's MRI is represented as a structural connectome where nodes denote cortical and subcortical regions and edges encode anatomical connectivity. Confounders such as age, sec, and APOE4 genotype are summarized via principal components and included in the causal adjustment set. After training, interventions on individual regions are simulated by serving their incoming edges and altering node features to estimate average causal effects on disease probability. Applied to 484 subjects from the ADNI cohort, Causal-GCN achieves performance comparable to baseline GNNs while providing interpretable causal effect rankings that highlight posterior, cingulate, and insular hubs consistent with established AD neuropathology.

</details>


### [154] [How to Train Private Clinical Language Models: A Comparative Study of Privacy-Preserving Pipelines for ICD-9 Coding](https://arxiv.org/abs/2511.14936)
*Mathieu Dufour,Andrew Duncan*

Main category: cs.LG

TL;DR: 本研究比较了四种用于临床语言任务的隐私保护训练管道，发现知识蒸馏在保持强隐私性的同时，性能优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在临床文本上训练有暴露敏感患者信息的风险，而差分隐私（DP）方法通常会严重降低部署所需的诊断准确性。

Method: 对四种训练管道进行了正面比较，所有管道使用相同的10亿参数模型和匹配的隐私预算来预测ICD-9代码。这些管道包括直接DP-SGD和DP-合成数据训练，以及知识蒸馏。

Result: 在中等和宽松的隐私预算下，来自DP训练教师的知识蒸馏优于直接DP-SGD和DP-合成数据训练，恢复了高达63％的非私有性能，同时保持了强大的经验隐私性。

Conclusion: 知识蒸馏是实现隐私保护临床NLP的最实用途径。

Abstract: Large language models trained on clinical text risk exposing sensitive patient information, yet differential privacy (DP) methods often severely degrade the diagnostic accuracy needed for deployment. Despite rapid progress in DP optimisation and text generation, it remains unclear which privacy-preserving strategy actually works best for clinical language tasks. We present the first systematic head-to-head comparison of four training pipelines for automated diagnostic coding from hospital discharge summaries. All pipelines use identical 1B-parameter models and matched privacy budgets to predict ICD-9 codes. At moderate and relaxed privacy budgets ($\varepsilon \in \{4, 6\}$), knowledge distillation from DP-trained teachers outperforms both direct DP-SGD and DP-synthetic data training, recovering up to 63\% of the non-private performance whilst maintaining strong empirical privacy (membership-inference AUC $\approx$ 0.5). These findings expose large differences in the privacy-utility trade-off across architectures and identify knowledge distillation as the most practical route to privacy-preserving clinical NLP.

</details>


### [155] [Knowledge Graphs as Structured Memory for Embedding Spaces: From Training Clusters to Explainable Inference](https://arxiv.org/abs/2511.14961)
*Artur A. Oliveira,Mateus Espadoto,Roberto M. Cesar,Roberto Hirata*

Main category: cs.LG

TL;DR: 提出了一种名为图记忆（GM）的结构化非参数框架，它利用区域级原型对基于嵌入的推理进行增强。


<details>
  <summary>Details</summary>
Motivation: GM通过紧凑的关系记忆来增强基于嵌入的推理，以此来避免孤立地处理每个训练实例。

Method: GM将嵌入空间概括为带有可靠性指标的原型节点，并通过编码几何和上下文关系的边连接这些节点。该设计统一了实例检索、基于原型的推理和基于图的标签传播。

Result: 在包括乳腺组织病理学（IDC）在内的合成和真实数据集上的实验表明，GM实现了与kNN和标签传播相当的精度，同时提供了明显更好的校准和更平滑的决策边界，所有这些都使用更少的样本。

Conclusion: 通过显式地建模可靠性和关系结构，GM在非参数学习中为局部证据和全局一致性之间提供了一座桥梁。

Abstract: We introduce Graph Memory (GM), a structured non-parametric framework that augments embedding-based inference with a compact, relational memory over region-level prototypes. Rather than treating each training instance in isolation, GM summarizes the embedding space into prototype nodes annotated with reliability indicators and connected by edges that encode geometric and contextual relations. This design unifies instance retrieval, prototype-based reasoning, and graph-based label propagation within a single inductive model that supports both efficient inference and faithful explanation. Experiments on synthetic and real datasets including breast histopathology (IDC) show that GM achieves accuracy competitive with $k$NN and Label Spreading while offering substantially better calibration and smoother decision boundaries, all with an order of magnitude fewer samples. By explicitly modeling reliability and relational structure, GM provides a principled bridge between local evidence and global consistency in non-parametric learning.

</details>


### [156] [IonCast: A Deep Learning Framework for Forecasting Ionospheric Dynamics](https://arxiv.org/abs/2511.15004)
*Halil S. Kelebek,Linnea M. Wolniewicz,Michael D. Vergalla,Simone Mestici,Giacomo Acciarini,Bala Poduval,Olga Verkhoglyadova,Madhulika Guhathakurta,Thomas E. Berger,Frank Soboczenski,Atılım Güneş Baydin*

Main category: cs.LG

TL;DR: IonCast: A deep learning model for forecasting global Total Electron Content (TEC) in the ionosphere.


<details>
  <summary>Details</summary>
Motivation: Accurate forecasting of ionospheric variability is increasingly important due to its impact on GNSS, communications, and aviation.

Method: A GraphCast-inspired deep learning model is used to forecast global TEC, integrating various physical drivers and observational datasets.

Result: IonCast shows improved forecasting skill compared to persistence, especially during storm-time and quiet conditions.

Conclusion: Machine learning can enhance our understanding of ionospheric variability and improve space weather resilience.

Abstract: The ionosphere is a critical component of near-Earth space, shaping GNSS accuracy, high-frequency communications, and aviation operations. For these reasons, accurate forecasting and modeling of ionospheric variability has become increasingly relevant. To address this gap, we present IonCast, a suite of deep learning models that include a GraphCast-inspired model tailored for ionospheric dynamics. IonCast leverages spatiotemporal learning to forecast global Total Electron Content (TEC), integrating diverse physical drivers and observational datasets. Validating on held-out storm-time and quiet conditions highlights improved skill compared to persistence. By unifying heterogeneous data with scalable graph-based spatiotemporal learning, IonCast demonstrates how machine learning can augment physical understanding of ionospheric variability and advance operational space weather resilience.

</details>


### [157] [Simulated Human Learning in a Dynamic, Partially-Observed, Time-Series Environment](https://arxiv.org/abs/2511.15032)
*Jeffrey Jiang,Kevin Hong,Emily Kuczynski,Gregory Pottie*

Main category: cs.LG

TL;DR: 这篇论文开发了一个动态时间序列环境来模拟课堂环境，其中包含师生互动，例如辅导课程、讲座和考试。然后，他们开发了强化学习 ITS，它结合了学习学生的个人状态，同时通过使用探测干预来获取人口信息。


<details>
  <summary>Details</summary>
Motivation: ITS系统可以利用过去学生的信息来个性化教学，但每个新学生都是独特的，并且学习过程本质上是部分可观察的，因此教育问题本质上是困难的。

Method: 开发了一个动态的、时间序列的环境来模拟课堂环境，其中包含师生互动，包括辅导课程、讲座和考试。然后，开发了强化学习 ITS，它结合了学习学生的个人状态，同时通过使用探测干预来获取人口信息。

Result: 标准 RL 算法与几种贪婪的基于规则的启发式方法相比，发现它们提供了不同的解决方案，但结果相似。随着隐藏信息水平的提高，问题的难度也会增加，如果我们允许探测干预，我们可以获得提升。启发式和 RL 策略在改变学生群体分布方面都具有灵活性，但 RL 策略难以帮助较难的课程。非探测策略能够提高测验和期中结构的性能，而不是仅在期末考试结构中提高性能，这突出了拥有额外信息的好处。

Conclusion: 强化学习和启发式策略都可以有效地用于智能辅导系统中，但它们在不同情况下表现不同。探测干预可以提高性能，但需要在准确估计和避免干扰学生之间取得平衡。

Abstract: While intelligent tutoring systems (ITSs) can use information from past students to personalize instruction, each new student is unique. Moreover, the education problem is inherently difficult because the learning process is only partially observable. We therefore develop a dynamic, time-series environment to simulate a classroom setting, with student-teacher interventions - including tutoring sessions, lectures, and exams. In particular, we design the simulated environment to allow for varying levels of probing interventions that can gather more information. Then, we develop reinforcement learning ITSs that combine learning the individual state of students while pulling from population information through the use of probing interventions. These interventions can reduce the difficulty of student estimation, but also introduce a cost-benefit decision to find a balance between probing enough to get accurate estimates and probing so often that it becomes disruptive to the student. We compare the efficacy of standard RL algorithms with several greedy rules-based heuristic approaches to find that they provide different solutions, but with similar results. We also highlight the difficulty of the problem with increasing levels of hidden information, and the boost that we get if we allow for probing interventions. We show the flexibility of both heuristic and RL policies with regards to changing student population distributions, finding that both are flexible, but RL policies struggle to help harder classes. Finally, we test different course structures with non-probing policies and we find that our policies are able to boost the performance of quiz and midterm structures more than we can in a finals-only structure, highlighting the benefit of having additional information.

</details>


### [158] [Oversampling techniques for predicting COVID-19 patient length of stay](https://arxiv.org/abs/2511.15048)
*Zachariah Farahany,Jiawei Wu,K M Sajjadul Islam,Praveen Madiraju*

Main category: cs.LG

TL;DR: 这篇论文通过分析电子健康记录（EHR）预测COVID-19感染的严重程度，使用住院时长（LOS）作为严重程度的衡量标准。


<details>
  <summary>Details</summary>
Motivation: 研究COVID-19感染的严重程度预测，特别是考虑到不同风险因素患者的住院时长差异。

Method: 使用合成过采样数据来解决数据不平衡问题，并通过贝叶斯优化调整的人工神经网络（ANN）进行训练。

Result: 选择具有最佳F1分数的模型进行评估和讨论。

Conclusion: 通过电子健康记录预测COVID-19感染的严重程度。

Abstract: COVID-19 is a respiratory disease that caused a global pandemic in 2019. It is highly infectious and has the following symptoms: fever or chills, cough, shortness of breath, fatigue, muscle or body aches, headache, the new loss of taste or smell, sore throat, congestion or runny nose, nausea or vomiting, and diarrhea. These symptoms vary in severity; some people with many risk factors have been known to have lengthy hospital stays or die from the disease. In this paper, we analyze patients' electronic health records (EHR) to predict the severity of their COVID-19 infection using the length of stay (LOS) as our measurement of severity. This is an imbalanced classification problem, as many people have a shorter LOS rather than a longer one. To combat this problem, we synthetically create alternate oversampled training data sets. Once we have this oversampled data, we run it through an Artificial Neural Network (ANN), which during training has its hyperparameters tuned using Bayesian optimization. We select the model with the best F1 score and then evaluate it and discuss it.

</details>


### [159] [Interpretable temporal fusion network of multi- and multi-class arrhythmia classification](https://arxiv.org/abs/2511.15062)
*Yun Kwan Kim*

Main category: cs.LG

TL;DR: 本文提出了一种新的心律失常检测和分类框架，该框架结合了局部和全局信息提取，并通过注意力机制进行融合，以应对心律失常长度变化的问题。该框架在两个心律失常数据库上表现出优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的心律失常分类临床决策支持系统（CDSS）未考虑心律失常发作时间变化的问题，导致性能受限。

Method: 该框架包括局部和全局信息提取以及基于注意力机制的局部-全局信息融合。

Result: 在MIT-BIH心律失常数据库（MITDB）和MIT-BIH心房颤动数据库（AFDB）上，该框架在持续时间、发作和Dice得分方面的F1分数分别达到96.45%、82.05%和96.31%（MITDB）以及97.57%、98.31%和97.45%（AFDB）。

Conclusion: 该方法能够更准确地检测心律失常，并精确确定其发生时间，从而为临床领域开发更准确的治疗方案提供支持。

Abstract: Clinical decision support systems (CDSSs) have been widely utilized to support the decisions made by cardiologists when detecting and classifying arrhythmia from electrocardiograms. However, forming a CDSS for the arrhythmia classification task is challenging due to the varying lengths of arrhythmias. Although the onset time of arrhythmia varies, previously developed methods have not considered such conditions. Thus, we propose a framework that consists of (i) local and global extraction and (ii) local-global information fusion with attention to enable arrhythmia detection and classification within a constrained input length. The framework's performance was evaluated in terms of 10-class and 4-class arrhythmia detection, focusing on identifying the onset and ending point of arrhythmia episodes and their duration using the MIT-BIH arrhythmia database (MITDB) and the MIT-BIH atrial fibrillation database (AFDB). Duration, episode, and Dice score performances resulted in overall F1-scores of 96.45%, 82.05%, and 96.31% on the MITDB and 97.57%, 98.31%, and 97.45% on the AFDB, respectively. The results demonstrated statistically superior performance compared to those of the benchmark models. To assess the generalization capability of the proposed method, an MITDB-trained model and MIT-BIH malignant ventricular arrhythmia database-trained model were tested AFDB and MITDB, respectively. Superior performance was attained compared with that of a state-of-the-art model. The proposed method effectively captures both local and global information and dynamics without significant information loss. Consequently, arrhythmias can be detected with greater accuracy, and their occurrence times can be precisely determined, enabling the clinical field to develop more accurate treatment plans based on the proposed method.

</details>


### [160] [Deep Pathomic Learning Defines Prognostic Subtypes and Molecular Drivers in Colorectal Cancer](https://arxiv.org/abs/2511.15067)
*Zisong Wang,Xuanyu Wang,Hang Chen,Haizhou Wang,Yuxin Chen,Yihang Xu,Yunhe Yuan,Lihuan Luo,Xitong Ling,Xiaoping Liu*

Main category: cs.LG

TL;DR: 开发了一种基于病理全切片图像的AI模型TDAM-CRC，用于更精确地预测结直肠癌(CRC)患者的预后。


<details>
  <summary>Details</summary>
Motivation: 传统的TNM分期系统不足以满足CRC个性化医疗的需求，因为CRC具有高度异质性，精确的预后分层仍然是一个主要的临床挑战。

Method: 使用多示例学习模型TDAM-CRC，该模型在TCGA队列上训练，并在独立外部队列中验证。结合多组学数据以提高模型的可解释性，并识别新的预后生物标志物。

Result: TDAM-CRC在两个队列中都实现了稳健的风险分层，其预测性能显著优于传统临床分期系统和其他模型。高风险亚型与代谢重编程和免疫抑制性肿瘤微环境密切相关。MRPL37被确定为连接深层病理特征与临床预后的关键基因，MRPL37的高表达与良好的预后相关。

Conclusion: TDAM-CRC模型为改善CRC风险分层提供了一个强大的工具，揭示了新的分子靶点，并有助于个性化的临床决策。

Abstract: Precise prognostic stratification of colorectal cancer (CRC) remains a major clinical challenge due to its high heterogeneity. The conventional TNM staging system is inadequate for personalized medicine. We aimed to develop and validate a novel multiple instance learning model TDAM-CRC using histopathological whole-slide images for accurate prognostic prediction and to uncover its underlying molecular mechanisms. We trained the model on the TCGA discovery cohort (n=581), validated it in an independent external cohort (n=1031), and further we integrated multi-omics data to improve model interpretability and identify novel prognostic biomarkers. The results demonstrated that the TDAM-CRC achieved robust risk stratification in both cohorts. Its predictive performance significantly outperformed the conventional clinical staging system and multiple state-of-the-art models. The TDAM-CRC risk score was confirmed as an independent prognostic factor in multivariable analysis. Multi-omics analysis revealed that the high-risk subtype is closely associated with metabolic reprogramming and an immunosuppressive tumor microenvironment. Through interaction network analysis, we identified and validated Mitochondrial Ribosomal Protein L37 (MRPL37) as a key hub gene linking deep pathomic features to clinical prognosis. We found that high expression of MRPL37, driven by promoter hypomethylation, serves as an independent biomarker of favorable prognosis. Finally, we constructed a nomogram incorporating the TDAM-CRC risk score and clinical factors to provide a precise and interpretable clinical decision-making tool for CRC patients. Our AI-driven pathological model TDAM-CRC provides a robust tool for improved CRC risk stratification, reveals new molecular targets, and facilitates personalized clinical decision-making.

</details>


### [161] [Fourier-KAN-Mamba: A Novel State-Space Equation Approach for Time-Series Anomaly Detection](https://arxiv.org/abs/2511.15083)
*Xiancheng Wang,Lin Wang,Rui Wang,Zhibo Zhang,Minghang Zhao*

Main category: cs.LG

TL;DR: 本文提出了一种新的时间序列异常检测混合架构，该架构集成了傅里叶层、Kolmogorov-Arnold Networks (KAN) 和 Mamba 选择性状态空间模型。


<details>
  <summary>Details</summary>
Motivation: 基于Mamba的状态空间模型在长序列建模中表现出卓越的效率。然而，直接将 Mamba 应用于异常检测任务仍然面临着捕获复杂时间模式和非线性动态的挑战。

Method: 该方法通过傅里叶层提取多尺度频率特征，KAN 增强非线性表示能力，并通过时间门控控制机制进一步提高模型区分正常和异常模式的能力。

Result: 在 MSL、SMAP 和 SWaT 数据集上进行的大量实验表明，该方法明显优于现有技术。

Conclusion: 提出的 Fourier-KAN-Mamba 是一种有效的时间序列异常检测方法

Abstract: Time-series anomaly detection plays a critical role in numerous real-world applications, including industrial monitoring and fault diagnosis. Recently, Mamba-based state-space models have shown remarkable efficiency in long-sequence modeling. However, directly applying Mamba to anomaly detection tasks still faces challenges in capturing complex temporal patterns and nonlinear dynamics. In this paper, we propose Fourier-KAN-Mamba, a novel hybrid architecture that integrates Fourier layer, Kolmogorov-Arnold Networks (KAN), and Mamba selective state-space model. The Fourier layer extracts multi-scale frequency features, KAN enhances nonlinear representation capability, and a temporal gating control mechanism further improves the model's ability to distinguish normal and anomalous patterns. Extensive experiments on MSL, SMAP, and SWaT datasets demonstrate that our method significantly outperforms existing state-of-the-art approaches.
  Keywords: time-series anomaly detection, state-space model, Mamba, Fourier transform, Kolmogorov-Arnold Network

</details>


### [162] [Semiconductor Industry Trend Prediction with Event Intervention Based on LSTM Model in Sentiment-Enhanced Time Series Data](https://arxiv.org/abs/2511.15112)
*Wei-hsiang Yen,Lyn Chao-ling Chen*

Main category: cs.LG

TL;DR: 本研究结合深度学习和情感分析，用于预测台湾半导体产业的台积电(TSMC)行业趋势。


<details>
  <summary>Details</summary>
Motivation: 传统数据分析方法在半导体行业高变异性和时间序列数据中表现不佳。通过整合公司内部和外部事件，考虑事件干预来进行情感分析。

Method: 利用情感增强的时间序列数据，采用LSTM模型预测台积电的行业趋势。

Result: 预测结果揭示了台积电晶圆技术的显著发展和全球市场的潜在威胁，与台积电的产品发布新闻和国际新闻相符。

Conclusion: 该研究通过考虑内部和外部事件干预，准确地预测了半导体行业的行业趋势，并为半导体行业的研究和商业方面提供了有价值的信息。

Abstract: The innovation of the study is that the deep learning method and sentiment analysis are integrated in traditional business model analysis and forecasting, and the research subject is TSMC for industry trend prediction of semiconductor industry in Taiwan. For the rapid market changes and development of wafer technologies of semiconductor industry, traditional data analysis methods not perform well in the high variety and time series data. Textual data and time series data were collected from seasonal reports of TSMC including financial information. Textual data through sentiment analysis by considering the event intervention both from internal events of the company and the external global events. Using the sentiment-enhanced time series data, the LSTM model was adopted for predicting industry trend of TSMC. The prediction results reveal significant development of wafer technology of TSMC and the potential threatens in the global market, and matches the product released news of TSMC and the international news. The contribution of the work performed accurately in industry trend prediction of the semiconductor industry by considering both the internal and external event intervention, and the prediction results provide valuable information of semiconductor industry both in research and business aspects.

</details>


### [163] [Efficient RF Passive Components Modeling with Bayesian Online Learning and Uncertainty Aware Sampling](https://arxiv.org/abs/2511.15125)
*Huifan Zhang,Pingqiang Zhou*

Main category: cs.LG

TL;DR: 提出了一种基于不确定性贝叶斯在线学习的射频无源器件参数化建模框架，该框架能够高效地进行射频无源器件的参数化建模。


<details>
  <summary>Details</summary>
Motivation: 传统的基于机器学习的射频无源器件建模需要大量的电磁仿真来覆盖几何和频率设计空间，造成计算瓶颈。

Method: 该框架包括：1) 具有可重构头的贝叶斯神经网络，用于联合几何-频域建模，同时量化不确定性；2) 一种自适应采样策略，该策略使用不确定性指导同时优化几何参数和频域上的训练数据采样。

Result: 在三种射频无源器件上验证，与传统的基于机器学习的流程相比，该框架仅使用 2.86% 的电磁仿真时间即可实现精确建模，实现了 35 倍的加速。

Conclusion: 该框架能够以更少的计算资源实现精确的射频无源器件建模。

Abstract: Conventional radio frequency (RF) passive components modeling based on machine learning requires extensive electromagnetic (EM) simulations to cover geometric and frequency design spaces, creating computational bottlenecks. In this paper, we introduce an uncertainty-aware Bayesian online learning framework for efficient parametric modeling of RF passive components, which includes: 1) a Bayesian neural network with reconfigurable heads for joint geometric-frequency domain modeling while quantifying uncertainty; 2) an adaptive sampling strategy that simultaneously optimizes training data sampling across geometric parameters and frequency domain using uncertainty guidance. Validated on three RF passive components, the framework achieves accurate modeling while using only 2.86% EM simulation time compared to traditional ML-based flow, achieving a 35 times speedup.

</details>


### [164] [Novel sparse matrix algorithm expands the feasible size of a self-organizing map of the knowledge indexed by a database of peer-reviewed medical literature](https://arxiv.org/abs/2511.15136)
*Andrew Amos,Joanne Lee,Tarun Sen Gupta,Bunmi S. Malau-Aduli*

Main category: cs.LG

TL;DR: 该论文提出了一种新的稀疏矩阵乘法算法，以解决现有算法在处理Medline数据库时内存和处理需求呈指数增长的问题。


<details>
  <summary>Details</summary>
Motivation: 过去对Medline数据库的映射工作因现有算法的指数增长的内存和处理需求而受到限制，只能处理可用数据的小子集。

Method: 设计了一种新的稀疏矩阵乘法算法，可以将自组织映射应用于整个Medline数据集。

Result: 该算法能够对现有的医学知识进行更完整的映射。

Conclusion: 该算法提高了完善自组织映射以适应数据集随时间变化的可行性。

Abstract: Past efforts to map the Medline database have been limited to small subsets of the available data because of the exponentially increasing memory and processing demands of existing algorithms. We designed a novel algorithm for sparse matrix multiplication that allowed us to apply a self-organizing map to the entire Medline dataset, allowing for a more complete map of existing medical knowledge. The algorithm also increases the feasibility of refining the self-organizing map to account for changes in the dataset over time.

</details>


### [165] [From Solving to Verifying: A Unified Objective for Robust Reasoning in LLMs](https://arxiv.org/abs/2511.15137)
*Xiaoxuan Wang,Bo Liu,Song Jiang,Jingzhou Liu,Jingyuan Qi,Xia Chen,Baosheng He*

Main category: cs.LG

TL;DR: 这篇论文提出了一种名为GRPO-Verif的算法，旨在提高大型语言模型（LLMs）的自我验证能力，并研究这种能力是否能进一步提升推理性能。该算法在统一的损失函数中共同优化解决方案的生成和自我验证，并通过可调节的超参数控制验证信号的权重。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLMs)的推理能力通过强化学习(RL)得到了显著提高。然而，llm仍然难以始终如一地验证自己的推理轨迹。这引发了一个研究问题，即如何提高llm的自我验证能力，以及这种能力是否能进一步提高推理性能。

Method: 该论文提出了一种名为GRPO-Verif的算法，该算法在统一的损失函数中共同优化解决方案生成和自我验证，并通过可调节的超参数控制验证信号的权重。

Result: 实验结果表明，该方法在保持推理性能的同时，增强了自我验证能力。

Conclusion: GRPO-Verif算法提高了LLM的自我验证能力，同时保持了可比较的推理性能。

Abstract: The reasoning capabilities of large language models (LLMs) have been significantly improved through reinforcement learning (RL). Nevertheless, LLMs still struggle to consistently verify their own reasoning traces. This raises the research question of how to enhance the self-verification ability of LLMs and whether such an ability can further improve reasoning performance. In this work, we propose GRPO-Verif, an algorithm that jointly optimizes solution generation and self-verification within a unified loss function, with an adjustable hyperparameter controlling the weight of the verification signal. Experimental results demonstrate that our method enhances self-verification capability while maintaining comparable performance in reasoning.

</details>


### [166] [Cross-Modal Consistency-Guided Active Learning for Affective BCI Systems](https://arxiv.org/abs/2511.15138)
*Hyo-Jeong Jang,Hye-Bin Shin,Kang Yin*

Main category: cs.LG

TL;DR: 提出了一种不确定性感知主动学习框架，通过联合利用模型不确定性和跨模态一致性来增强对标签噪声的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 脑电信号容易被伪影和个体差异破坏，而情感标签通常来自主观和不一致的报告，这使得稳健的情感解码特别困难。

Method: 该方法评估跨模态对齐以确定不确定性是来自认知模糊还是传感器噪声。表示对齐模块将脑电和面部特征嵌入到共享的潜在空间中，从而增强模态之间的语义连贯性。残余差异被视为噪声引起的不一致，并且在主动学习期间有选择地查询这些样本以获得oracle反馈。

Result: 在ASCERTAIN数据集上的实验检验了该方法的效率和鲁棒性，突出了其作为脑-机接口系统中基于脑电的情感解码的数据高效和容错方法的潜力。

Conclusion: 该方法能够选择可靠的、信息丰富的样本，并减少噪声标签的影响。

Abstract: Deep learning models perform best with abundant, high-quality labels, yet such conditions are rarely achievable in EEG-based emotion recognition. Electroencephalogram (EEG) signals are easily corrupted by artifacts and individual variability, while emotional labels often stem from subjective and inconsistent reports-making robust affective decoding particularly difficult. We propose an uncertainty-aware active learning framework that enhances robustness to label noise by jointly leveraging model uncertainty and cross-modal consistency. Instead of relying solely on EEG-based uncertainty estimates, the method evaluates cross-modal alignment to determine whether uncertainty originates from cognitive ambiguity or sensor noise. A representation alignment module embeds EEG and face features into a shared latent space, enforcing semantic coherence between modalities. Residual discrepancies are treated as noise-induced inconsistencies, and these samples are selectively queried for oracle feedback during active learning. This feedback-driven process guides the network toward reliable, informative samples and reduces the impact of noisy labels. Experiments on the ASCERTAIN dataset examine the efficiency and robustness of ours, highlighting its potential as a data-efficient and noise-tolerant approach for EEG-based affective decoding in brain-computer interface systems.

</details>


### [167] [Complex variational autoencoders admit Kähler structure](https://arxiv.org/abs/2511.15172)
*Andrew Gracyk*

Main category: cs.LG

TL;DR: 本文研究了具有复杂潜在阶段的复杂变分自编码器(VAE)的Kähler几何结构。


<details>
  <summary>Details</summary>
Motivation: 研究潜在欧几里德变分自编码器(VAE)的黎曼结构。

Method: 针对解码器几何定制方法，推导了在潜在复高斯正则化下复数情况下的Fisher信息度量。

Result: 证明了可以通过解码器几何来正则化潜在空间，并且可以根据加权复体积元素进行采样。这些策略以样本变化为代价，产生更平滑的表示和更少的语义异常值。

Conclusion: 在相对熵下，度量Kähler势关系可以精确实现。

Abstract: It has been discovered that latent-Euclidean variational autoencoders (VAEs) admit, in various capacities, Riemannian structure. We adapt these arguments but for complex VAEs with a complex latent stage. We show that complex VAEs reveal to some level Kähler geometric structure. Our methods will be tailored for decoder geometry. We derive the Fisher information metric in the complex case under a latent complex Gaussian regularization with trivial relation matrix. It is well known from statistical information theory that the Fisher information coincides with the Hessian of the Kullback-Leibler (KL) divergence. Thus, the metric Kähler potential relation is exactly achieved under relative entropy. We propose a Kähler potential derivative of complex Gaussian mixtures that has rough equivalence to the Fisher information metric while still being faithful to the underlying Kähler geometry. Computation of the metric via this potential is efficient, and through our potential, valid as a plurisubharmonic (PSH) function, large scale computational burden of automatic differentiation is displaced to small scale. We show that we can regularize the latent space with decoder geometry, and that we can sample in accordance with a weighted complex volume element. We demonstrate these strategies, at the exchange of sample variation, yield consistently smoother representations and fewer semantic outliers.

</details>


### [168] [FaultDiffusion: Few-Shot Fault Time Series Generation with Diffusion Model](https://arxiv.org/abs/2511.15174)
*Yi Xu,Zhigang Chen,Rui Wang,Yangfan Li,Fengxiao Tang,Ming Zhao,Jiaqi Liu*

Main category: cs.LG

TL;DR: 提出了一种基于扩散模型的少样本故障时间序列生成框架，用于解决工业设备监控中故障数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 由于故障事件的罕见和数据标注的高成本，故障数据稀缺严重阻碍了数据驱动方法；现有的时间序列生成模型难以捕捉少样本场景下的故障分布，导致生成的样本缺乏真实性和多样性。

Method: 采用正负差异适配器，利用预训练的正常数据分布来建模正常和故障域之间的差异，以实现准确的故障合成；引入多样性损失以防止模式崩溃，通过样本间差异正则化来鼓励生成多样化的故障样本。

Result: 在真实性和多样性方面显著优于传统方法，并在关键基准测试中实现了最先进的性能。

Conclusion: 该模型能够有效解决工业设备监控中故障数据稀缺的问题，并生成高质量的故障样本。

Abstract: In industrial equipment monitoring, fault diagnosis is critical for ensuring system reliability and enabling predictive maintenance. However, the scarcity of fault data, due to the rarity of fault events and the high cost of data annotation, significantly hinders data-driven approaches. Existing time-series generation models, optimized for abundant normal data, struggle to capture fault distributions in few-shot scenarios, producing samples that lack authenticity and diversity due to the large domain gap and high intra-class variability of faults. To address this, we propose a novel few-shot fault time-series generation framework based on diffusion models. Our approach employs a positive-negative difference adapter, leveraging pre-trained normal data distributions to model the discrepancies between normal and fault domains for accurate fault synthesis. Additionally, a diversity loss is introduced to prevent mode collapse, encouraging the generation of diverse fault samples through inter-sample difference regularization. Experimental results demonstrate that our model significantly outperforms traditional methods in authenticity and diversity, achieving state-of-the-art performance on key benchmarks.

</details>


### [169] [Vehicle Routing Problems via Quantum Graph Attention Network Deep Reinforcement Learning](https://arxiv.org/abs/2511.15175)
*Le Tung Giang,Vu Hoang Viet,Nguyen Xuan Tung,Trinh Van Chien,Won-Joo Hwang*

Main category: cs.LG

TL;DR: 提出了一种基于量子图注意力网络(Q-GAT)的深度强化学习(DRL)框架，用于解决车辆路径问题(VRP)，该框架使用参数化的量子电路(PQC)代替传统的多层感知器(MLP)，减少了50%以上的可训练参数。


<details>
  <summary>Details</summary>
Motivation: 传统的基于图神经网络(GNNs)的深度强化学习(DRL)的车辆路径问题(VRP)模型依赖于大型多层感知器(MLP)，这些模型参数量大且受内存限制。

Method: 使用近端策略优化(PPO)与贪婪和随机解码，在DRL框架内提出了一种量子图注意力网络(Q-GAT)，其中参数化的量子电路(PQC)取代了关键读出阶段的传统MLP。

Result: 在VRP基准测试中，Q-GAT实现了更快的收敛速度，并且与经典GAT基线相比，降低了约5%的路径成本。

Conclusion: 参数化量子电路增强的GNN作为大规模路由和物流优化的紧凑而有效的求解器具有潜力。

Abstract: The vehicle routing problem (VRP) is a fundamental NP-hard task in intelligent transportation systems with broad applications in logistics and distribution. Deep reinforcement learning (DRL) with Graph Neural Networks (GNNs) has shown promise, yet classical models rely on large multi-layer perceptrons (MLPs) that are parameter-heavy and memory-bound. We propose a Quantum Graph Attention Network (Q-GAT) within a DRL framework, where parameterized quantum circuits (PQCs) replace conventional MLPs at critical readout stages. The hybrid model maintains the expressive capacity of graph attention encoders while reducing trainable parameters by more than 50%. Using proximal policy optimization (PPO) with greedy and stochastic decoding, experiments on VRP benchmarks show that Q-GAT achieves faster convergence and reduces routing cost by about 5% compared with classical GAT baselines. These results demonstrate the potential of PQC-enhanced GNNs as compact and effective solvers for large-scale routing and logistics optimization.

</details>


### [170] [Masked Auto-Regressive Variational Acceleration: Fast Inference Makes Practical Reinforcement Learning](https://arxiv.org/abs/2511.15190)
*Yuxuan Gu,Weimin Bai,Yifei Wang,Weijian Luo,He Sun*

Main category: cs.LG

TL;DR: MARVAL通过蒸馏将掩码自回归扩散模型压缩成单步生成，加速推理并支持强化学习微调。


<details>
  <summary>Details</summary>
Motivation: 原始MAR模型推理速度慢，限制了其在强化学习中的应用。

Method: 提出MARVAL框架，使用基于蒸馏的方法将扩散链压缩为单步自回归生成。

Result: MARVAL在ImageNet 256*256上实现了30倍以上的推理加速，FID达到2.00，并通过MARVAL-RL在CLIP和图像奖励分数上取得了提升。

Conclusion: MARVAL为掩码自回归扩散模型的蒸馏和强化学习提供了一条实用路径，实现了快速采样和更好的偏好对齐。

Abstract: Masked auto-regressive diffusion models (MAR) benefit from the expressive modeling ability of diffusion models and the flexibility of masked auto-regressive ordering. However, vanilla MAR suffers from slow inference due to its hierarchical inference mechanism: an outer AR unmasking loop and an inner diffusion denoising chain. Such decoupled structure not only harm the generation efficiency but also hinder the practical use of MAR for reinforcement learning (RL), an increasingly critical paradigm for generative model post-training.To address this fundamental issue, we introduce MARVAL (Masked Auto-regressive Variational Acceleration), a distillation-based framework that compresses the diffusion chain into a single AR generation step while preserving the flexible auto-regressive unmasking order. Such a distillation with MARVAL not only yields substantial inference acceleration but, crucially, makes RL post-training with verifiable rewards practical, resulting in scalable yet human-preferred fast generative models. Our contributions are twofold: (1) a novel score-based variational objective for distilling masked auto-regressive diffusion models into a single generation step without sacrificing sample quality; and (2) an efficient RL framework for masked auto-regressive models via MARVAL-RL. On ImageNet 256*256, MARVAL-Huge achieves an FID of 2.00 with more than 30 times speedup compared with MAR-diffusion, and MARVAL-RL yields consistent improvements in CLIP and image-reward scores on ImageNet datasets with entity names. In conclusion, MARVAL demonstrates the first practical path to distillation and RL of masked auto-regressive diffusion models, enabling fast sampling and better preference alignments.

</details>


### [171] [Reasoning in Diffusion Large Language Models is Concentrated in Dynamic Confusion Zones](https://arxiv.org/abs/2511.15208)
*Ranfei Chen,Ming Chen,Kaifei Wang*

Main category: cs.LG

TL;DR: 自回归模型之外，扩散大语言模型 (dLLM) 作为一种强大的复杂推理范例正在迅速兴起，强化学习越来越多地用于下游对齐。现有的基于轨迹的 RL 方法在去噪步骤中统一分配策略梯度，隐式地将所有步骤视为同等重要。我们通过分析具有多个步骤级指标的轨迹来挑战这一假设：基于熵的不确定性、置信度边际 (CM) 不确定性和熵变化率 (RoEC)。这些揭示了结构化的“混淆区”：不确定性和不稳定性的瞬时峰值，可以强烈预测最终的成功或失败，而大多数步骤保持稳定。我们提出了自适应轨迹策略优化 (ATPO)，这是一种轻量级的步骤选择策略，可以动态地将梯度更新重新分配给这些高杠杆步骤，而无需更改 RL 目标、奖励或计算预算。使用混合 RoEC+CM 规则，ATPO 在基准测试中实现了推理准确性和训练稳定性的显着提升，表明利用轨迹动态是推进 dLLM RL 的关键。


<details>
  <summary>Details</summary>
Motivation: 挑战了现有基于轨迹的 RL 方法在去噪步骤中统一分配策略梯度，隐式地将所有步骤视为同等重要的假设。

Method: 通过分析具有多个步骤级指标（基于熵的不确定性、置信度边际 (CM) 不确定性和熵变化率 (RoEC)）的轨迹，揭示结构化的“混淆区”，并提出了自适应轨迹策略优化 (ATPO) 算法。

Result: ATPO 在基准测试中实现了推理准确性和训练稳定性的显着提升。

Conclusion: 利用轨迹动态是推进 dLLM RL 的关键。

Abstract: Diffusion Large Language Models (dLLMs) are rapidly emerging alongside autoregressive models as a powerful paradigm for complex reasoning, with reinforcement learning increasingly used for downstream alignment. Existing trajectory-based RL methods uniformly allocate policy gradients across denoising steps, implicitly treating all steps as equally important. We challenge this assumption by analyzing trajectories with several step-level metrics: entropy-based uncertainty, Confidence-Margin (CM) uncertainty, and Rate of Entropy Change (RoEC). These reveal structured "zones of confusion": transient spikes in uncertainty and instability that strongly predict final success or failure, while most steps remain stable. We propose Adaptive Trajectory Policy Optimization (ATPO), a lightweight step-selection strategy that dynamically reallocates gradient updates to these high-leverage steps without changing the RL objective, rewards, or compute budget. Using a hybrid RoEC+CM rule, ATPO delivers substantial gains in reasoning accuracy and training stability across benchmarks, showing that exploiting trajectory dynamics is key to advancing dLLM RL.

</details>


### [172] [D2D Power Allocation via Quantum Graph Neural Network](https://arxiv.org/abs/2511.15246)
*Tung Giang Le,Xuan Tung Nguyen,Won-Joo Hwang*

Main category: cs.LG

TL;DR: 提出了一种全量子图神经网络（QGNN），用于解决无线网络资源管理中的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 无线网络日益复杂，需要可扩展的资源管理。传统的图神经网络（GNN）虽然擅长图学习，但在大规模环境中计算成本很高。

Method: 该QGNN通过参数化量子电路（PQC）实现消息传递。提出的量子图卷积层（QGCL）将特征编码到量子态中，使用与NISQ兼容的酉算子处理图，并通过测量检索嵌入。

Result: 在用于SINR最大化的D2D功率控制中，该QGNN以更少的参数和固有的并行性与经典性能相匹配。

Conclusion: 这种基于端到端PQC的GNN标志着朝着量子加速无线优化迈出了一步。

Abstract: Increasing wireless network complexity demands scalable resource management. Classical GNNs excel at graph learning but incur high computational costs in large-scale settings. We present a fully quantum Graph Neural Network (QGNN) that implements message passing via Parameterized Quantum Circuits (PQCs). Our Quantum Graph Convolutional Layers (QGCLs) encode features into quantum states, process graphs with NISQ-compatible unitaries, and retrieve embeddings through measurement. Applied to D2D power control for SINR maximization, our QGNN matches classical performance with fewer parameters and inherent parallelism. This end-to-end PQC-based GNN marks a step toward quantum-accelerated wireless optimization.

</details>


### [173] [EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control](https://arxiv.org/abs/2511.15248)
*Kai Yang,Xin Xu,Yangkun Chen,Weijie Liu,Jiafei Lyu,Zichuan Lin,Deheng Ye,Saiyong Yang*

Main category: cs.LG

TL;DR: 大型语言模型（LLMs）的长期训练需要保持稳定的探索，以防止模型崩溃到次优行为。本文提出了一种名为EntroPIC的新方法，该方法通过动态调整正负样本的损失系数来稳定熵。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法难以维持适当的熵水平，因为训练过程涉及正负样本的混合，每个样本在不同步骤中以不同的方式影响熵。

Method: 通过动态调整正负样本的损失系数，自适应地调整正负样本的影响。

Result: EntroPIC有效地控制了大规模LLM训练中的熵，并成功地维持了所需的熵水平，从而实现了LLM的稳定和最佳RL训练。

Conclusion: EntroPIC稳定了整个训练过程中的熵，确保了有效的探索和稳定的进展。

Abstract: Long-term training of large language models (LLMs) requires maintaining stable exploration to prevent the model from collapsing into sub-optimal behaviors. Entropy is crucial in this context, as it controls exploration and helps avoid premature convergence to sub-optimal solutions. However, existing reinforcement learning methods struggle to maintain an appropriate level of entropy, as the training process involves a mix of positive and negative samples, each affecting entropy in different ways across steps. To address this, we propose Entropy stablilization via Proportional-Integral Control (EntroPIC), a novel method that adaptively adjusts the influence of positive and negative samples by dynamically tuning their loss coefficients. This approach stabilizes entropy throughout training, ensuring efficient exploration and steady progress. We provide a comprehensive theoretical analysis for both on-policy and off-policy learning settings, demonstrating that EntroPIC is effective at controlling entropy in large-scale LLM training. Experimental results show that our method successfully maintains desired entropy levels, enabling stable and optimal RL training for LLMs.

</details>


### [174] [Optimized scheduling of electricity-heat cooperative system considering wind energy consumption and peak shaving and valley filling](https://arxiv.org/abs/2511.15250)
*Jin Ye,Lingmei Wang,Shujian Zhang,Haihang WU*

Main category: cs.LG

TL;DR: 本文提出了一种基于改进的双延迟深度确定性策略梯度(PVTD3)算法的智能调度方法，用于解决新能源集成和多重不确定性下，电力-热联合系统的调度优化挑战。


<details>
  <summary>Details</summary>
Motivation: 为了应对全球能源转型和可再生能源的快速发展，新能源集成和多重不确定性下，电力-热联合系统的调度优化挑战日益突出。

Method: 该研究提出了一种基于改进的双延迟深度确定性策略梯度(PVTD3)算法的智能调度方法。通过引入电网购电变化惩罚项来实现系统优化。

Result: 在三种典型场景(10%，20%和30%的可再生能源渗透率)下，PVTD3算法比传统TD3算法分别降低了6.93%，12.68%和13.59%的系统综合成本。同时，它降低了12.8%的电网购电平均波动幅度。在储能管理方面，PVTD3算法降低了低温储罐的结束时间状态值7.67-17.67单位，同时保持高温罐在3.59-4.25的安全运行范围内。

Conclusion: 多场景对比验证表明，该算法不仅在经济效益和电网稳定性方面表现出色，而且在储能装置管理方面表现出卓越的可持续调度能力。

Abstract: With the global energy transition and rapid development of renewable energy, the scheduling optimization challenge for combined power-heat systems under new energy integration and multiple uncertainties has become increasingly prominent. Addressing this challenge, this study proposes an intelligent scheduling method based on the improved Dual-Delay Deep Deterministic Policy Gradient (PVTD3) algorithm. System optimization is achieved by introducing a penalty term for grid power purchase variations. Simulation results demonstrate that under three typical scenarios (10%, 20%, and 30% renewable penetration), the PVTD3 algorithm reduces the system's comprehensive cost by 6.93%, 12.68%, and 13.59% respectively compared to the traditional TD3 algorithm. Concurrently, it reduces the average fluctuation amplitude of grid power purchases by 12.8%. Regarding energy storage management, the PVTD3 algorithm reduces the end-time state values of low-temperature thermal storage tanks by 7.67-17.67 units while maintaining high-temperature tanks within the 3.59-4.25 safety operating range. Multi-scenario comparative validation demonstrates that the proposed algorithm not only excels in economic efficiency and grid stability but also exhibits superior sustainable scheduling capabilities in energy storage device management.

</details>


### [175] [PLATONT: Learning a Platonic Representation for Unified Network Tomography](https://arxiv.org/abs/2511.15251)
*Chengze Du,Heng Xu,Zhiwei Yu,Bo Liu,Jialong Li*

Main category: cs.LG

TL;DR: PLATONT是一个统一的框架，它将不同的网络指标建模为共享潜在网络状态的投影，并通过多模态对齐和对比学习来学习这个潜在状态。


<details>
  <summary>Details</summary>
Motivation: 现有网络层析成像方法单独解决问题，依赖于有限的任务特定信号，限制了泛化和可解释性。

Method: PLATONT通过多模态对齐和对比学习学习共享潜在空间中的多个层析成像任务。

Result: 在合成和真实世界数据集上的实验表明，PLATONT在链路估计、拓扑推理和流量预测方面始终优于现有方法，在不同的网络条件下实现了更高的准确性和更强的鲁棒性。

Conclusion: PLATONT通过构建紧凑和结构化的表示来提高跨任务泛化能力。

Abstract: Network tomography aims to infer hidden network states, such as link performance, traffic load, and topology, from external observations. Most existing methods solve these problems separately and depend on limited task-specific signals, which limits generalization and interpretability. We present PLATONT, a unified framework that models different network indicators (e.g., delay, loss, bandwidth) as projections of a shared latent network state. Guided by the Platonic Representation Hypothesis, PLATONT learns this latent state through multimodal alignment and contrastive learning. By training multiple tomography tasks within a shared latent space, it builds compact and structured representations that improve cross-task generalization. Experiments on synthetic and real-world datasets show that PLATONT consistently outperforms existing methods in link estimation, topology inference, and traffic prediction, achieving higher accuracy and stronger robustness under varying network conditions.

</details>


### [176] [GRPO-RM: Fine-Tuning Representation Models via GRPO-Driven Reinforcement Learning](https://arxiv.org/abs/2511.15256)
*Yanchen Xu,Ziheng Jiao,Hongyuan Zhang,Xuelong Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为 GRPO-RM 的新方法，将 GRPO 应用于表征学习模型，通过预定义的输出集和专门设计的奖励函数，在各种真实数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 探索 GRPO 是否可以推广到表征学习模型。

Method: 提出 GRPO-RM，建立预定义的输出集来替代 LLM 中的 token 序列采样，并设计专门的奖励函数。

Result: 在各种真实数据集上进行了大量实验，验证了该方法的有效性。

Conclusion: GRPO-RM 在表征模型后训练中表现出良好的性能。

Abstract: The Group Relative Policy Optimization (GRPO), a reinforcement learning method used to fine-tune large language models (LLMs), has proved its effectiveness in practical applications such as DeepSeek-R1. It raises a question whether GRPO can be generalized to representation learning models. In this paper, we propose Group Relative Policy Optimization for Representation Model (GRPO-RM), and investigate the performance of GRPO-like policy in post-training representation models. Specifically, our method establishes a predefined output set to functionally replace token sequence sampling in LLMs, thereby generating an output group, which is essential for the probability-driven optimization of GRPO. In addition, a specialized reward function is designed to accommodate the properties of representation models. Extensive experiments are conducted on various real-world datasets to validate the effectiveness of our proposed method.

</details>


### [177] [SNAP: Low-Latency Test-Time Adaptation with Sparse Updates](https://arxiv.org/abs/2511.15276)
*Hyeongheon Cha,Dong Min Kim,Hye Won Chung,Taesik Gong,Sung-Ju Lee*

Main category: cs.LG

TL;DR: SNAP是一个稀疏的Test-Time Adaptation (TTA) 框架，旨在减少适应频率和数据使用，同时保持准确性，适用于资源受限的边缘环境。


<details>
  <summary>Details</summary>
Motivation: 现有的TTA方法依赖于频繁的适应和高计算成本，不适合资源受限的边缘环境。

Method: 该方法引入了两个关键组件：(i) 类和域代表性记忆 (CnDRM)，用于识别和存储少量代表类和域特征的样本，以支持使用有限数据进行有效适应；(ii) 仅推理批感知记忆归一化 (IoBMN)，通过利用这些代表性样本在推理时动态调整归一化统计，从而实现与转移目标域的有效对齐。

Result: 与五个最先进的TTA算法集成后，SNAP可将延迟降低高达 93.12%，同时将精度下降保持在 3.3% 以下，即使在 1% 到 50% 的适应率范围内也是如此。

Conclusion: SNAP在延迟敏感型应用的边缘设备上具有强大的实际应用潜力。

Abstract: Test-Time Adaptation (TTA) adjusts models using unlabeled test data to handle dynamic distribution shifts. However, existing methods rely on frequent adaptation and high computational cost, making them unsuitable for resource-constrained edge environments. To address this, we propose SNAP, a sparse TTA framework that reduces adaptation frequency and data usage while preserving accuracy. SNAP maintains competitive accuracy even when adapting based on only 1% of the incoming data stream, demonstrating its robustness under infrequent updates. Our method introduces two key components: (i) Class and Domain Representative Memory (CnDRM), which identifies and stores a small set of samples that are representative of both class and domain characteristics to support efficient adaptation with limited data; and (ii) Inference-only Batch-aware Memory Normalization (IoBMN), which dynamically adjusts normalization statistics at inference time by leveraging these representative samples, enabling efficient alignment to shifting target domains. Integrated with five state-of-the-art TTA algorithms, SNAP reduces latency by up to 93.12%, while keeping the accuracy drop below 3.3%, even across adaptation rates ranging from 1% to 50%. This demonstrates its strong potential for practical use on edge devices serving latency-sensitive applications. The source code is available at https://github.com/chahh9808/SNAP.

</details>


### [178] [Quant-Trim in Practice: Improved Cross-Platform Low-Bit Deployment on Edge NPUs](https://arxiv.org/abs/2511.15300)
*Rayen Dhahri,Steffen Urban*

Main category: cs.LG

TL;DR: Quant-Trim是一种训练方法，可生成对后端和精度选择具有鲁棒性的硬件中性检查点。


<details>
  <summary>Details</summary>
Motivation: 现有的边缘加速器依赖于低比特量化，但供应商编译器在缩放、裁剪和内核支持方面存在差异，通常作为黑盒。因此，相同的浮点 (FP) 检查点可能在不同后端产生不一致的精度，迫使从业者调整标志或重构模型以适应供应商友好的算子子集。

Method: 该方法结合了渐进式伪量化，使训练与部署的整数网格对齐，并反向修剪以抑制由异常值驱动的尺度膨胀，同时保持可学习性。Quant-Trim 与量化方案（对称/非对称、per-tensor/per-channel、INT8/INT4）无关，并且不需要特定于供应商的图形更改。

Result: 在各种模型和任务中，它缩小了 FP、低比特差距，减少了对编译器启发式/校准的依赖，并避免了每个后端的重新训练。

Conclusion: 我们报告了在静态/动态激活缩放和不同算子覆盖率下的准确性和边缘指标延迟、吞吐量、能量/推断和成本。

Abstract: Specialized edge accelerators rely on low-bit quantization, but vendor compilers differ in scaling, clipping, and kernel support, often as black boxes. The same floating-point (FP) checkpoint can therefore yield inconsistent accuracy across backends, forcing practitioners to tweak flags or refactor models to vendor-friendly operator subsets. We introduce Quant-Trim, a training-phase method that produces a hardware-neutral checkpoint robust to backend and precision choices. It combines progressive fake quantization to align training with the deployed integer grid and reverse pruning to tame outlier-driven scale inflation while preserving learnability. Quant-Trim is agnostic to quantization schemes (symmetric/asymmetric,per-tensor/per-channel, INT8/INT4) and requires no vendor-specific graph changes.Across models and tasks, it narrows the FP,low-bit gap, reduces dependence on compiler heuristics/calibration, and avoids per-backend retraining. We report accuracy and edge metrics latency, throughput, energy/inference, and cost under static/dynamic activation scaling and varying operator coverage.

</details>


### [179] [On the Internal Semantics of Time-Series Foundation Models](https://arxiv.org/abs/2511.15324)
*Atharva Pandey,Abhilash Neog,Gautam Jajoo*

Main category: cs.LG

TL;DR: 本文研究了时间序列基础模型 (TSFM) 中概念的可解释性，发现早期层捕获局部时域模式，而更深层编码离散度和变化时间信号。组合概念仍然是一个挑战。


<details>
  <summary>Details</summary>
Motivation: 了解时间序列基础模型如何表示时间序列概念。

Method: 使用分层分析、线性可恢复性测试和表征相似性度量来系统地探究这些问题。

Result: 早期层主要捕获局部时域模式，而更深层编码离散度和变化时间信号，频谱和扭曲因子仍然最难线性恢复。在组合设置中，探针性能下降，表明概念之间存在干扰。

Conclusion: 虽然原子概念可以可靠地定位，但组合仍然是一个挑战，突显了当前 TSFM 在表示交互时间现象方面的关键限制。

Abstract: Time-series Foundation Models (TSFMs) have recently emerged as a universal paradigm for learning across diverse temporal domains. However, despite their empirical success, the internal mechanisms by which these models represent fundamental time-series concepts remain poorly understood. In this work, we undertake a systematic investigation of concept interpretability in TSFMs. Specifically, we examine: (i) which layers encode which concepts, (ii) whether concept parameters are linearly recoverable, (iii) how representations evolve in terms of concept disentanglement and abstraction across model depth, and (iv) how models process compositions of concepts. We systematically probe these questions using layer-wise analyses, linear recoverability tests, and representation similarity measures, providing a structured account of TSFM semantics. The resulting insights show that early layers mainly capture local, time-domain patterns (e.g., AR(1), level shifts, trends), while deeper layers encode dispersion and change-time signals, with spectral and warping factors remaining the hardest to recover linearly. In compositional settings, however, probe performance degrades, revealing interference between concepts. This highlights that while atomic concepts are reliably localized, composition remains a challenge, underscoring a key limitation in current TSFMs' ability to represent interacting temporal phenomena.

</details>


### [180] [KrawtchoukNet: A Unified GNN Solution for Heterophily and Over-smoothing with Adaptive Bounded Polynomials](https://arxiv.org/abs/2511.15327)
*Huseyin Goksu*

Main category: cs.LG

TL;DR: 提出了一种新的基于离散Krawtchouk多项式的图神经网络滤波器KrawtchoukNet，以解决传统谱图神经网络在异质图上的性能崩溃和过平滑问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于多项式滤波器的谱图神经网络(GNNs)，如ChebyNet，存在两个 критических 限制：1) 在“异质”图上的性能崩溃；2) 在高多项式次数(K)下的性能崩溃，称为过平滑。这两个问题都源于静态的低通滤波器的性质。

Method: 通过固定多项式的域N为一个小的常数(例如，N=20)，创建了第一个GNN滤波器，其递归系数是内在有界的，使其对过平滑具有异常的鲁棒性(在K=10时达到SOTA结果)。其次，通过使滤波器的形状参数p可学习，滤波器可以调整其对图数据的频谱响应。

Result: 通过使滤波器的形状参数p可学习，滤波器可以调整其对图数据的频谱响应。我们证明了这种自适应性使得`KrawtchoukNet`能够在具有挑战性的异质基准(Texas, Cornell)上实现SOTA性能，决定性地优于标准的GNNs，如GAT和APPNP。

Conclusion: KrawtchoukNet通过其内在有界的递归系数和可学习的形状参数，为解决GNNs在异质图上的性能崩溃和过平滑问题提供了一个统一的解决方案。

Abstract: Spectral Graph Neural Networks (GNNs) based on polynomial filters, such as ChebyNet, suffer from two critical limitations: 1) performance collapse on "heterophilic" graphs and 2) performance collapse at high polynomial degrees (K), known as over-smoothing. Both issues stem from the static, low-pass nature of standard filters. In this work, we propose `KrawtchoukNet`, a GNN filter based on the discrete Krawtchouk polynomials. We demonstrate that `KrawtchoukNet` provides a unified solution to both problems through two key design choices. First, by fixing the polynomial's domain N to a small constant (e.g., N=20), we create the first GNN filter whose recurrence coefficients are \textit{inherently bounded}, making it exceptionally robust to over-smoothing (achieving SOTA results at K=10). Second, by making the filter's shape parameter p learnable, the filter adapts its spectral response to the graph data. We show this adaptive nature allows `KrawtchoukNet` to achieve SOTA performance on challenging heterophilic benchmarks (Texas, Cornell), decisively outperforming standard GNNs like GAT and APPNP.

</details>


### [181] [LaguerreNet: Advancing a Unified Solution for Heterophily and Over-smoothing with Adaptive Continuous Polynomials](https://arxiv.org/abs/2511.15328)
*Huseyin Goksu*

Main category: cs.LG

TL;DR: 提出了一种新的基于连续Laguerre多项式的GNN滤波器LaguerreNet，以解决谱图神经网络在异质图上的性能不佳和过度平滑的问题。


<details>
  <summary>Details</summary>
Motivation: 现有谱图神经网络在异质图上表现不佳，且在高阶多项式时出现过平滑现象。离散MeixnerNet等自适应多项式滤波器有潜力统一解决这些问题，但其在连续域的扩展和在无界系数下的稳定性仍是未决问题。

Method: 通过使核心alpha参数可训练来学习滤波器的频谱形状，并使用基于LayerNorm的稳定技术解决这些无界多项式的严重O(k^2)数值不稳定性。

Result: LaguerreNet在具有挑战性的异质基准测试中取得了最先进的结果，并且对过度平滑具有很强的鲁棒性，其性能在K=10时达到峰值，比ChebyNet崩溃的点高出一个数量级。

Conclusion: LaguerreNet是一种有效的GNN滤波器，它通过学习滤波器的频谱形状和解决数值不稳定性，从而在异质图上实现了最先进的性能，并且对过度平滑具有很强的鲁棒性。

Abstract: Spectral Graph Neural Networks (GNNs) suffer from two critical limitations: poor performance on "heterophilic" graphs and performance collapse at high polynomial degrees (K), known as over-smoothing. Both issues stem from the static, low-pass nature of standard filters (e.g., ChebyNet). While adaptive polynomial filters, such as the discrete MeixnerNet, have emerged as a potential unified solution, their extension to the continuous domain and stability with unbounded coefficients remain open questions. In this work, we propose `LaguerreNet`, a novel GNN filter based on continuous Laguerre polynomials. `LaguerreNet` learns the filter's spectral shape by making its core alpha parameter trainable, thereby advancing the adaptive polynomial approach. We solve the severe O(k^2) numerical instability of these unbounded polynomials using a `LayerNorm`-based stabilization technique. We demonstrate experimentally that this approach is highly effective: 1) `LaguerreNet` achieves state-of-the-art results on challenging heterophilic benchmarks. 2) It is exceptionally robust to over-smoothing, with performance peaking at K=10, an order of magnitude beyond where ChebyNet collapses.

</details>


### [182] [STREAM-VAE: Dual-Path Routing for Slow and Fast Dynamics in Vehicle Telemetry Anomaly Detection](https://arxiv.org/abs/2511.15339)
*Kadir-Kaan Özer,René Ebeling,Markus Enzweiler*

Main category: cs.LG

TL;DR: 提出了一种新的变分自编码器STREAM-VAE，用于检测汽车遥测时间序列数据中的异常。


<details>
  <summary>Details</summary>
Motivation: 汽车遥测数据包含慢漂移和快突增，使得可靠的异常检测具有挑战性。标准的基于重构的方法使用单一的潜在过程，混合了异构的时间尺度，削弱了异常分离。

Method: 使用双路径编码器分离慢漂移和快突增信号动态，并使用解码器将瞬态偏差与正常运行模式分开表示。

Result: 在汽车遥测数据集和公共SMD基准上的实验表明，与强大的预测、注意力、图和VAE基线相比，显式分离漂移和突增动态提高了鲁棒性。

Conclusion: STREAM-VAE专为部署而设计，可在各种操作模式下生成稳定的异常评分，适用于车载监控器和后端车队分析。

Abstract: Automotive telemetry data exhibits slow drifts and fast spikes, often within the same sequence, making reliable anomaly detection challenging. Standard reconstruction-based methods, including sequence variational autoencoders (VAEs), use a single latent process and therefore mix heterogeneous time scales, which can smooth out spikes or inflate variances and weaken anomaly separation.
  In this paper, we present STREAM-VAE, a variational autoencoder for anomaly detection in automotive telemetry time-series data. Our model uses a dual-path encoder to separate slow drift and fast spike signal dynamics, and a decoder that represents transient deviations separately from the normal operating pattern. STREAM-VAE is designed for deployment, producing stable anomaly scores across operating modes for both in-vehicle monitors and backend fleet analytics.
  Experiments on an automotive telemetry dataset and the public SMD benchmark show that explicitly separating drift and spike dynamics improves robustness compared to strong forecasting, attention, graph, and VAE baselines.

</details>


### [183] [Multi-layer Stack Ensembles for Time Series Forecasting](https://arxiv.org/abs/2511.15350)
*Nathanael Bosch,Oleksandr Shchur,Nick Erickson,Michael Bohlke-Schneider,Caner Türkmen*

Main category: cs.LG

TL;DR: 本文探讨了时间序列预测中的集成方法，特别是堆叠方法，并提出了一个多层堆叠框架，该框架在各种预测场景中始终提供卓越的准确性。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测中的集成方法未得到充分利用，简单的线性组合仍然被认为是最好的方法。

Method: 本文评估了 50 个真实世界数据集上的 33 个集成模型（包括现有模型和新模型）。

Result: 堆叠可以提高准确性，但没有一个堆叠器在所有任务中表现最佳。多层堆叠框架在不同的预测场景中始终提供卓越的准确性。

Conclusion: 基于堆叠的方法具有改进时间序列预测的 AutoML 系统的潜力。

Abstract: Ensembling is a powerful technique for improving the accuracy of machine learning models, with methods like stacking achieving strong results in tabular tasks. In time series forecasting, however, ensemble methods remain underutilized, with simple linear combinations still considered state-of-the-art. In this paper, we systematically explore ensembling strategies for time series forecasting. We evaluate 33 ensemble models -- both existing and novel -- across 50 real-world datasets. Our results show that stacking consistently improves accuracy, though no single stacker performs best across all tasks. To address this, we propose a multi-layer stacking framework for time series forecasting, an approach that combines the strengths of different stacker models. We demonstrate that this method consistently provides superior accuracy across diverse forecasting scenarios. Our findings highlight the potential of stacking-based methods to improve AutoML systems for time series forecasting.

</details>


### [184] [Cost-Aware Prediction (CAP): An LLM-Enhanced Machine Learning Pipeline and Decision Support System for Heart Failure Mortality Prediction](https://arxiv.org/abs/2511.15357)
*Yinan Yu,Falk Dippel,Christina E. Lundberg,Martin Lindgren,Annika Rosengren,Martin Adiels,Helen Sjöland*

Main category: cs.LG

TL;DR: 本文提出了一种成本感知预测 (CAP) 框架，该框架结合了成本效益分析和大型语言模型 (LLM) 代理，以权衡应用 ML 预测所涉及的利弊。


<details>
  <summary>Details</summary>
Motivation: 传统的机器学习 (ML) 预测模型在开发时通常没有考虑下游价值的权衡和临床可解释性。

Method: 1. 开发了一个 ML 模型，预测心力衰竭患者的 1 年死亡率，以识别符合家庭护理条件的人群。2. 引入了临床影响预测 (CIP) 曲线，以可视化重要的成本维度——生活质量和医疗保健提供者费用，并进一步分为治疗成本和错误成本，以评估预测的临床后果。3. 使用四个 LLM 代理来生成患者特定描述。

Result: XGBoost 模型表现最佳，AUROC 为 0.804，AUPRC 为 0.529，Brier 分数为 0.135。CIP 成本曲线提供了跨决策阈值的成本组成的人群层面概述，而 LLM 生成了个人患者层面的成本效益分析。该系统受到了临床医生的好评。

Conclusion: CAP 利用 LLM 代理来整合 ML 分类器结果和成本效益分析，以实现更透明和可解释的决策支持。

Abstract: Objective: Machine learning (ML) predictive models are often developed without considering downstream value trade-offs and clinical interpretability. This paper introduces a cost-aware prediction (CAP) framework that combines cost-benefit analysis assisted by large language model (LLM) agents to communicate the trade-offs involved in applying ML predictions. Materials and Methods: We developed an ML model predicting 1-year mortality in patients with heart failure (N = 30,021, 22% mortality) to identify those eligible for home care. We then introduced clinical impact projection (CIP) curves to visualize important cost dimensions - quality of life and healthcare provider expenses, further divided into treatment and error costs, to assess the clinical consequences of predictions. Finally, we used four LLM agents to generate patient-specific descriptions. The system was evaluated by clinicians for its decision support value. Results: The eXtreme gradient boosting (XGB) model achieved the best performance, with an area under the receiver operating characteristic curve (AUROC) of 0.804 (95% confidence interval (CI) 0.792-0.816), area under the precision-recall curve (AUPRC) of 0.529 (95% CI 0.502-0.558) and a Brier score of 0.135 (95% CI 0.130-0.140). Discussion: The CIP cost curves provided a population-level overview of cost composition across decision thresholds, whereas LLM-generated cost-benefit analysis at individual patient-levels. The system was well received according to the evaluation by clinicians. However, feedback emphasizes the need to strengthen the technical accuracy for speculative tasks. Conclusion: CAP utilizes LLM agents to integrate ML classifier outcomes and cost-benefit analysis for more transparent and interpretable decision support.

</details>


### [185] [CID: Measuring Feature Importance Through Counterfactual Distributions](https://arxiv.org/abs/2511.15371)
*Eddie Conti,Álvaro Parafita,Axel Brando*

Main category: cs.LG

TL;DR: 提出了一种新的事后局部特征重要性方法，称为反事实重要性分布 (CID)，该方法通过生成正反反事实集、使用核密度估计对它们的分布进行建模以及基于分布相异性度量对特征进行排序来工作。


<details>
  <summary>Details</summary>
Motivation: 评估机器学习中各个特征的重要性对于理解模型的决策过程至关重要。虽然存在许多方法，但缺乏明确的真值进行比较，突出了对替代的、有充分根据的措施的需求。

Method: 我们生成两组正反反事实，使用核密度估计对它们的分布进行建模，并根据分布相异性度量对特征进行排序。该度量基于严格的数学框架，满足作为有效度量所需的关键属性。

Result: 我们的方法不仅为现有方法提供了补充视角，而且提高了忠实度指标（包括全面性和充分性）的性能，从而产生了对系统更忠实的解释。

Conclusion: 这些结果突出了其作为模型分析的宝贵工具的潜力。

Abstract: Assessing the importance of individual features in Machine Learning is critical to understand the model's decision-making process. While numerous methods exist, the lack of a definitive ground truth for comparison highlights the need for alternative, well-founded measures. This paper introduces a novel post-hoc local feature importance method called Counterfactual Importance Distribution (CID). We generate two sets of positive and negative counterfactuals, model their distributions using Kernel Density Estimation, and rank features based on a distributional dissimilarity measure. This measure, grounded in a rigorous mathematical framework, satisfies key properties required to function as a valid metric. We showcase the effectiveness of our method by comparing with well-established local feature importance explainers. Our method not only offers complementary perspectives to existing approaches, but also improves performance on faithfulness metrics (both for comprehensiveness and sufficiency), resulting in more faithful explanations of the system. These results highlight its potential as a valuable tool for model analysis.

</details>


### [186] [Parameter Importance-Driven Continual Learning for Foundation Models](https://arxiv.org/abs/2511.15375)
*Lingxiang Wang,Hainan Zhang,Zhiming Zheng*

Main category: cs.LG

TL;DR: 提出了一种名为PIECE的参数重要性估计持续增强方法，它选择性地更新与新任务最相关的0.1%的核心参数，从而在不访问先前训练数据或增加模型参数的情况下，保持通用能力并有效地学习领域知识。


<details>
  <summary>Details</summary>
Motivation: 领域特定的后训练经常导致灾难性遗忘，使基础模型失去其通用推理能力，并限制了它们对动态现实世界环境的适应性。在获得下游领域知识的同时，保持通用能力是大型语言和多模态模型面临的核心挑战。

Method: PIECE方法基于两个重要性估计器：基于 Fisher 信息的 PIECE-F 和基于二阶归一化的 PIECE-S，后者结合了梯度和曲率信息。

Result: 在三个语言模型和两个多模态模型上进行的实验表明，PIECE 保持了通用能力，并在各种下游任务中实现了最先进的持续学习性能。

Conclusion: PIECE方法为可扩展的、领域自适应的基础模型提供了一条实用途径，且不会发生灾难性遗忘。

Abstract: Domain-specific post-training often causes catastrophic forgetting, making foundation models lose their general reasoning ability and limiting their adaptability to dynamic real-world environments. Preserving general capabilities while acquiring downstream domain knowledge is a central challenge for large language and multimodal models. Traditional continual learning methods, such as regularization, replay and architectural isolation, suffer from poor downstream performance, reliance on inaccessible historical data, or additional parameter overhead. While recent parameter-efficient tuning (PET) methods can alleviate forgetting, their effectiveness strongly depends on the choice of parameters and update strategies. In this paper, we introduce PIECE, a Parameter Importance Estimation-based Continual Enhancement method that preserves general ability while efficiently learning domain knowledge without accessing prior training data or increasing model parameters. PIECE selectively updates only 0.1% of core parameters most relevant to new tasks, guided by two importance estimators: PIECE-F based on Fisher Information, and PIECE-S based on a second-order normalization that combines gradient and curvature information. Experiments across three language models and two multimodal models show that PIECE maintains general capabilities and achieves state-of-the-art continual learning performance across diverse downstream tasks. Our results highlight a practical path to scalable, domain-adaptive foundation models without catastrophic forgetting.

</details>


### [187] [EVA-Net: Interpretable Brain Age Prediction via Continuous Aging Prototypes from EEG](https://arxiv.org/abs/2511.15393)
*Kunyu Zhang,Mingxuan Wang,Xiangjie Shi,Haoxing Xu,Chao Zhang*

Main category: cs.LG

TL;DR: 提出了EVA-Net，一个新的框架，将脑年龄重塑为一个可解释的异常检测问题。


<details>
  <summary>Details</summary>
Motivation: 现有的模型难以从弱监督的、仅健康的队列中学习“正常”基线，这是一个识别疾病的关键异常检测任务，但标准模型通常是缺乏可解释结构的黑盒子。

Method: EVA-Net使用高效的、稀疏注意力的Transformer来建模长脑电图序列，并采用变分信息瓶颈来学习鲁棒的压缩表示。为了可解释性，该表示与一个连续原型网络对齐，该网络显式地学习规范的健康衰老流形。

Result: 在1297名健康受试者上训练，EVA-Net达到了最先进的精度。在27名MCI和AD患者的看不见的队列中验证了其异常检测能力。这个病理组显示出显著更高的脑年龄差距和一种新的原型对齐误差，证实了他们与健康流形的偏差。

Conclusion: EVA-Net为使用不完善的医疗数据进行医疗保健情报提供了一个可解释的框架。

Abstract: The brain age is a key indicator of brain health. While electroencephalography (EEG) is a practical tool for this task, existing models struggle with the common challenge of imperfect medical data, such as learning a ``normal'' baseline from weakly supervised, healthy-only cohorts. This is a critical anomaly detection task for identifying disease, but standard models are often black boxes lacking an interpretable structure. We propose EVA-Net, a novel framework that recasts brain age as an interpretable anomaly detection problem. EVA-Net uses an efficient, sparsified-attention Transformer to model long EEG sequences. To handle noise and variability in imperfect data, it employs a Variational Information Bottleneck to learn a robust, compressed representation. For interpretability, this representation is aligned to a continuous prototype network that explicitly learns the normative healthy aging manifold. Trained on 1297 healthy subjects, EVA-Net achieves state-of-the-art accuracy. We validated its anomaly detection capabilities on an unseen cohort of 27 MCI and AD patients. This pathological group showed significantly higher brain-age gaps and a novel Prototype Alignment Error, confirming their deviation from the healthy manifold. EVA-Net provides an interpretable framework for healthcare intelligence using imperfect medical data.

</details>


### [188] [Proximal Approximate Inference in State-Space Models](https://arxiv.org/abs/2511.15409)
*Hany Abdulsamad,Ángel F. García-Fernández,Simo Särkkä*

Main category: cs.LG

TL;DR: 提出了一类用于非线性、非高斯状态空间模型中状态估计的算法。


<details>
  <summary>Details</summary>
Motivation: 在非线性、非高斯状态空间模型中进行状态估计

Method: 基于变分拉格朗日公式，将贝叶斯推断转化为受动态约束的熵信任域更新序列。通过关注高斯-马尔可夫近似，推导出具有良好计算复杂度的递归方案。对于一般的非线性、非高斯模型，我们使用广义统计线性回归和傅里叶-埃尔米特矩匹配来闭合递归。

Result: 推导出一系列前向-后向算法

Conclusion: 该框架产生了一系列前向-后向算法，其结构由变分后验的选定分解决定。

Abstract: We present a class of algorithms for state estimation in nonlinear, non-Gaussian state-space models. Our approach is based on a variational Lagrangian formulation that casts Bayesian inference as a sequence of entropic trust-region updates subject to dynamic constraints. This framework gives rise to a family of forward-backward algorithms, whose structure is determined by the chosen factorization of the variational posterior. By focusing on Gauss--Markov approximations, we derive recursive schemes with favorable computational complexity. For general nonlinear, non-Gaussian models we close the recursions using generalized statistical linear regression and Fourier--Hermite moment matching.

</details>


### [189] [Towards Understanding Layer Contributions in Tabular In-Context Learning Models](https://arxiv.org/abs/2511.15432)
*Amir Rezaei Balef,Mykhailo Koshil,Katharina Eggensperger*

Main category: cs.LG

TL;DR: 研究表格内上下文学习(ICL)模型中各层的作用，发现潜在的冗余层，并与大型语言模型(LLM)进行比较。


<details>
  <summary>Details</summary>
Motivation: 目前对于表格ICL模型中各层如何促进表格预测知之甚少。

Method: 通过“layers as painters”的角度分析TabPFN和TabICL。

Result: 发现只有部分层共享通用的表征语言，表明存在结构冗余。

Conclusion: 为模型压缩和提高可解释性提供了机会。

Abstract: Despite the architectural similarities between tabular in-context learning (ICL) models and large language models (LLMs), little is known about how individual layers contribute to tabular prediction. In this paper, we investigate how the latent spaces evolve across layers in tabular ICL models, identify potential redundant layers, and compare these dynamics with those observed in LLMs. We analyze TabPFN and TabICL through the "layers as painters" perspective, finding that only subsets of layers share a common representational language, suggesting structural redundancy and offering opportunities for model compression and improved interpretability.

</details>


### [190] [TSFM in-context learning for time-series classification of bearing-health status](https://arxiv.org/abs/2511.15447)
*Michel Tokic,Slobodan Djukanović,Anja von Beuningen,Cheng Feng*

Main category: cs.LG

TL;DR: 本文介绍了一种使用时间序列基础模型 (TSFM) 中的上下文学习进行分类的方法，无需微调模型即可对不属于 TSFM 训练数据语料库的数据进行分类。


<details>
  <summary>Details</summary>
Motivation: 在伺服压力机电机中，使用振动数据评估轴承的健康状态。

Method: 该方法将频域参考信号转换为伪时间序列模式，生成对齐的协变量和目标信号，并使用 TSFM 预测分类数据如何对应于预定义标签的概率。

Result: 该方法展示了在各种操作条件下的有效性。

Conclusion: 该方法标志着超越定制的狭义人工智能解决方案，朝着更广泛的、人工智能驱动的维护系统迈出了重要一步。

Abstract: This paper introduces a classification method using in-context learning in time-series foundation models (TSFM). We show how data, which was not part of the TSFM training data corpus, can be classified without the need of finetuning the model. Examples are represented in the form of targets (class id) and covariates (data matrix) within the prompt of the model, which enables to classify an unknown covariate data pattern alongside the forecast axis through in-context learning. We apply this method to vibration data for assessing the health state of a bearing within a servo-press motor. The method transforms frequency domain reference signals into pseudo time-series patterns, generates aligned covariate and target signals, and uses the TSFM to predict probabilities how classified data corresponds to predefined labels. Leveraging the scalability of pre-trained models this method demonstrates efficacy across varied operational conditions. This marks significant progress beyond custom narrow AI solutions towards broader, AI-driven maintenance systems.

</details>


### [191] [FairEnergy: Contribution-Based Fairness meets Energy Efficiency in Federated Learning](https://arxiv.org/abs/2511.15454)
*Ouiame Marnissi,Hajar EL Hammouti,El Houcine Bergou*

Main category: cs.LG

TL;DR: 提出了一种名为FairEnergy的联邦学习框架，旨在最小化无线边缘系统中的能耗，同时确保公平参与和高模型准确性。


<details>
  <summary>Details</summary>
Motivation: 在无线边缘系统中，由于异构资源、不等的客户端贡献和有限的通信容量，如何在保证高模型精度的同时平衡能源效率和公平参与仍然具有挑战性。

Method: 该框架通过将更新幅度和压缩率融入到设备选择、带宽分配和压缩级别的联合优化中，从而实现公平感知能源最小化。通过放松二元选择变量和应用拉格朗日分解来处理全局带宽耦合，从而解决由此产生的混合整数非凸问题，然后进行每个设备的子问题优化。

Result: 在非独立同分布数据上的实验表明，与基线策略相比，FairEnergy实现了更高的精度，同时降低了高达79%的能耗。

Conclusion: FairEnergy 能够在联邦学习中实现更高的准确性和更低的能耗，解决了无线边缘系统中能源效率和公平参与的挑战。

Abstract: Federated learning (FL) enables collaborative model training across distributed devices while preserving data privacy. However, balancing energy efficiency and fair participation while ensuring high model accuracy remains challenging in wireless edge systems due to heterogeneous resources, unequal client contributions, and limited communication capacity. To address these challenges, we propose FairEnergy, a fairness-aware energy minimization framework that integrates a contribution score capturing both the magnitude of updates and their compression ratio into the joint optimization of device selection, bandwidth allocation, and compression level. The resulting mixed-integer non-convex problem is solved by relaxing binary selection variables and applying Lagrangian decomposition to handle global bandwidth coupling, followed by per-device subproblem optimization. Experiments on non-IID data show that FairEnergy achieves higher accuracy while reducing energy consumption by up to 79\% compared to baseline strategies.

</details>


### [192] [NTK-Guided Implicit Neural Teaching](https://arxiv.org/abs/2511.15487)
*Chen Zhang,Wei Zuo,Bingyang Cheng,Yikun Wang,Wei-Bin Kou,Yik Chung WU,Ngai Wong*

Main category: cs.LG

TL;DR: 提出了一种名为 NTK-Guided Implicit Neural Teaching (NINT) 的方法，用于加速隐式神经表示的训练。


<details>
  <summary>Details</summary>
Motivation: 为了解决拟合高分辨率信号需要优化数百万个坐标，导致计算成本过高的问题。

Method: 该方法利用神经正切核（NTK）动态选择坐标，以最大化全局功能更新。通过 NTK 增强的损失梯度范数对样本进行评分，从而捕获拟合误差和异构杠杆（自我影响和交叉坐标耦合）。

Result: 实验表明，NINT 显著减少了近一半的训练时间，同时保持或提高了表示质量。

Conclusion: NINT 在最近的基于采样的策略中建立了最先进的加速效果。

Abstract: Implicit Neural Representations (INRs) parameterize continuous signals via multilayer perceptrons (MLPs), enabling compact, resolution-independent modeling for tasks like image, audio, and 3D reconstruction. However, fitting high-resolution signals demands optimizing over millions of coordinates, incurring prohibitive computational costs. To address it, we propose NTK-Guided Implicit Neural Teaching (NINT), which accelerates training by dynamically selecting coordinates that maximize global functional updates. Leveraging the Neural Tangent Kernel (NTK), NINT scores examples by the norm of their NTK-augmented loss gradients, capturing both fitting errors and heterogeneous leverage (self-influence and cross-coordinate coupling). This dual consideration enables faster convergence compared to existing methods. Through extensive experiments, we demonstrate that NINT significantly reduces training time by nearly half while maintaining or improving representation quality, establishing state-of-the-art acceleration among recent sampling-based strategies.

</details>


### [193] [Sample-Adaptivity Tradeoff in On-Demand Sampling](https://arxiv.org/abs/2511.15507)
*Nika Haghtalab,Omar Montasser,Mingda Qiao*

Main category: cs.LG

TL;DR: 研究了按需采样中的样本复杂度和轮数复杂度之间的权衡，其中学习算法在有限轮数内从 k 个分布中自适应地采样。


<details>
  <summary>Details</summary>
Motivation: 在多分布学习 (MDL) 的可实现设置中，表明 r 轮算法的最佳样本复杂度大约为 dk^{Θ(1/r)} / ε。对于一般的不可知情况，提出了一种算法，该算法在 $\widetilde O(\sqrt{k})$ 轮内实现了接近最优的样本复杂度 $\widetilde O((d + k) / ε^2)$。

Method: 引入了一个新的框架，即通过按需采样进行优化 (OODS)，该框架抽象了样本自适应性权衡并捕获了大多数现有的 MDL 算法。建立了 OODS 设置中轮数复杂度的几乎严格的界限。

Result: 在多分布学习 (MDL) 的可实现设置中，表明 r 轮算法的最佳样本复杂度大约为 dk^{Θ(1/r)} / ε。对于一般的不可知情况，提出了一种算法，该算法在 $\widetilde O(\sqrt{k})$ 轮内实现了接近最优的样本复杂度 $\widetilde O((d + k) / ε^2)$。

Conclusion: 在 OODS 设置中建立了轮数复杂度的几乎严格的界限。上界直接产生了用于不可知 MDL 的 $\widetilde O(\sqrt{k})$ 轮算法，而下界意味着实现亚多项式轮数复杂度将需要绕过 OODS 固有硬度的根本性新技术。

Abstract: We study the tradeoff between sample complexity and round complexity in on-demand sampling, where the learning algorithm adaptively samples from $k$ distributions over a limited number of rounds. In the realizable setting of Multi-Distribution Learning (MDL), we show that the optimal sample complexity of an $r$-round algorithm scales approximately as $dk^{Θ(1/r)} / ε$. For the general agnostic case, we present an algorithm that achieves near-optimal sample complexity of $\widetilde O((d + k) / ε^2)$ within $\widetilde O(\sqrt{k})$ rounds. Of independent interest, we introduce a new framework, Optimization via On-Demand Sampling (OODS), which abstracts the sample-adaptivity tradeoff and captures most existing MDL algorithms. We establish nearly tight bounds on the round complexity in the OODS setting. The upper bounds directly yield the $\widetilde O(\sqrt{k})$-round algorithm for agnostic MDL, while the lower bounds imply that achieving sub-polynomial round complexity would require fundamentally new techniques that bypass the inherent hardness of OODS.

</details>


### [194] [PCARNN-DCBF: Minimal-Intervention Geofence Enforcement for Ground Vehicles](https://arxiv.org/abs/2511.15522)
*Yinan Yu,Samuel Scheidegger*

Main category: cs.LG

TL;DR: 提出了一种新的pipeline：PCARNN-DCBF，将物理编码的控制仿射残差神经网络与基于预览的离散控制障碍函数相结合。


<details>
  <summary>Details</summary>
Motivation: 现有的地面车辆运行时地理围栏解决方案难以将高保真学习与可验证控制的结构要求相协调。

Method: PCARNN显式地保留了车辆动力学的控制仿射结构，确保了可靠优化所需的线性。这使得DCBF能够通过实时二次规划（QP）来执行多边形保持约束，从而处理高相对阶数并减轻执行器饱和。

Result: 在CARLA中进行的电动和燃烧平台实验表明，这种保留结构的方法明显优于分析和非结构化神经基线。

Conclusion: PCARNN-DCBF pipeline在CARLA实验中表现出优越的性能。

Abstract: Runtime geofencing for ground vehicles is rapidly emerging as a critical technology for enforcing Operational Design Domains (ODDs). However, existing solutions struggle to reconcile high-fidelity learning with the structural requirements of verifiable control. We address this by introducing PCARNN-DCBF, a novel pipeline integrating a Physics-encoded Control-Affine Residual Neural Network with a preview-based Discrete Control Barrier Function. Unlike generic learned models, PCARNN explicitly preserves the control-affine structure of vehicle dynamics, ensuring the linearity required for reliable optimization. This enables the DCBF to enforce polygonal keep-in constraints via a real-time Quadratic Program (QP) that handles high relative degree and mitigates actuator saturation. Experiments in CARLA across electric and combustion platforms demonstrate that this structure-preserving approach significantly outperforms analytical and unstructured neural baselines.

</details>


### [195] [CODE: A global approach to ODE dynamics learning](https://arxiv.org/abs/2511.15619)
*Nils Wildt,Daniel M. Tartakovsky,Sergey Oladyshkin,Wolfgang Nowak*

Main category: cs.LG

TL;DR: 提出了一种名为ChaosODE (CODE) 的方法，用于从稀疏数据中学习常微分方程 (ODE) 的控制动力学。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要假设动态行为、提出数学模型并将其预测与数据进行比较，但现代计算和算法的进步使得直接从观测数据中进行纯数据驱动的学习成为可能。然而，高时间分辨率的数据通常难以获得，因此通常只有稀疏采样的数据。

Method: 使用任意多项式混沌展开 (aPCE) 作为 ODE 的右侧项，从而得到动力学的全局正交多项式表示。

Result: 在 Lotka-Volterra 系统上进行的大量实验表明，即使在新条件下进行评估，CODE 也表现出卓越的推断能力，并且与使用神经网络 (NeuralODE) 或核逼近器 (KernelODE) 作为 RHS 表示器的成熟方法相比，具有优势。NeuralODE 和 KernelODE 的高灵活性在稀疏数据和测量噪声下会降低推断能力。

Conclusion: 为动态学习问题的稳健优化提供实用指南，并在随附的代码中对其进行说明。

Abstract: Ordinary differential equations (ODEs) are a conventional way to describe the observed dynamics of physical systems. Scientists typically hypothesize about dynamical behavior, propose a mathematical model, and compare its predictions to data. However, modern computing and algorithmic advances now enable purely data-driven learning of governing dynamics directly from observations. In data-driven settings, one learns the ODE's right-hand side (RHS). Dense measurements are often assumed, yet high temporal resolution is typically both cumbersome and expensive. Consequently, one usually has only sparsely sampled data. In this work we introduce ChaosODE (CODE), a Polynomial Chaos ODE Expansion in which we use an arbitrary Polynomial Chaos Expansion (aPCE) for the ODE's right-hand side, resulting in a global orthonormal polynomial representation of dynamics. We evaluate the performance of CODE in several experiments on the Lotka-Volterra system, across varying noise levels, initial conditions, and predictions far into the future, even on previously unseen initial conditions. CODE exhibits remarkable extrapolation capabilities even when evaluated under novel initial conditions and shows advantages compared to well-examined methods using neural networks (NeuralODE) or kernel approximators (KernelODE) as the RHS representer. We observe that the high flexibility of NeuralODE and KernelODE degrades extrapolation capabilities under scarce data and measurement noise. Finally, we provide practical guidelines for robust optimization of dynamics-learning problems and illustrate them in the accompanying code.

</details>


### [196] [Continual Reinforcement Learning for Cyber-Physical Systems: Lessons Learned and Open Challenges](https://arxiv.org/abs/2511.15652)
*Kim N. Nolle,Ivana Dusparic,Rhodri Cusack,Vinny Cahill*

Main category: cs.LG

TL;DR: 本文探讨了持续强化学习（CRL）在自动驾驶环境中的应用，并突出了现有方法在环境抽象、超参数敏感性、灾难性遗忘和神经网络容量利用方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 在多任务或非静态环境中，智能体需要具备适应和泛化先前学习能力的能力。持续学习为此提供了一种解决方案，但在强化学习中的应用仍面临挑战，尤其是在自动驾驶等网络物理系统中。

Method: 本文通过在自动驾驶环境中进行实验，使用近端策略优化（PPO）算法，让智能体在四个不同的停车场景中依次学习停车，模拟持续学习环境。

Result: 实验揭示了CRL中存在的多个挑战，包括环境抽象、超参数敏感性、灾难性遗忘以及神经网络容量的有效利用。

Conclusion: 本文提出了解决这些挑战所需解决的重要开放性研究问题，并质疑了神经网络在持续学习中的适用性，强调了计算机科学和神经科学之间跨学科研究的必要性。

Abstract: Continual learning (CL) is a branch of machine learning that aims to enable agents to adapt and generalise previously learned abilities so that these can be reapplied to new tasks or environments. This is particularly useful in multi-task settings or in non-stationary environments, where the dynamics can change over time. This is particularly relevant in cyber-physical systems such as autonomous driving. However, despite recent advances in CL, successfully applying it to reinforcement learning (RL) is still an open problem.
  This paper highlights open challenges in continual RL (CRL) based on experiments in an autonomous driving environment. In this environment, the agent must learn to successfully park in four different scenarios corresponding to parking spaces oriented at varying angles. The agent is successively trained in these four scenarios one after another, representing a CL environment, using Proximal Policy Optimisation (PPO). These experiments exposed a number of open challenges in CRL: finding suitable abstractions of the environment, oversensitivity to hyperparameters, catastrophic forgetting, and efficient use of neural network capacity.
  Based on these identified challenges, we present open research questions that are important to be addressed for creating robust CRL systems. In addition, the identified challenges call into question the suitability of neural networks for CL. We also identify the need for interdisciplinary research, in particular between computer science and neuroscience.

</details>


### [197] [DeepThinkVLA: Enhancing Reasoning Capability of Vision-Language-Action Models](https://arxiv.org/abs/2511.15669)
*Cheng Yin,Yankai Lin,Wang Xu,Sikyuen Tam,Xiangrui Zeng,Zhiyuan Liu,Zhouping Yin*

Main category: cs.LG

TL;DR: 本文介绍了一种名为DeepThinkVLA的模型，该模型通过混合注意力解码器和两阶段训练策略，解决了视觉-语言-动作（VLA）模型中推理和动作之间的冲突，从而提高了机器人控制的性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型使用单一自回归解码器进行序列推理和高维并行机器人动作，导致运动控制性能下降，无法建立思想和行动之间的强因果关系。

Method: 该模型采用混合注意力解码器，利用因果注意力生成序列CoT，然后切换到双向注意力以快速并行解码动作向量。此外，还采用了两阶段训练管道：首先使用监督微调（SFT）来教授模型基础推理，然后应用强化学习（RL）以因果方式将完整的推理-行动序列与期望的结果对齐。

Result: DeepThinkVLA在LIBERO基准测试中实现了97.0%的成功率，达到了最先进的性能。消融实验表明，混合架构本身比标准解码器性能高出15.5%，最终的RL阶段提供了关键的2%的提升。

Conclusion: DeepThinkVLA通过混合注意力解码器和两阶段训练策略，有效解决了VLA模型中推理和动作之间的冲突，显著提高了机器人控制的性能。

Abstract: Enabling Vision-Language-Action (VLA) models to "think before acting" via Chain-of-Thought (CoT) is a promising path to overcoming the data-hungry nature of end-to-end robot policies. However, progress is stalled by a fundamental conflict: existing models use a single autoregressive decoder for both sequential CoT reasoning and high-dimensional, parallelizable robot actions. This architectural mismatch degrades motor control and fails to forge a strong causal link between thought and action. We introduce DeepThinkVLA, which resolves this conflict through a tightly integrated architecture and training strategy. Architecturally, our hybrid-attention decoder generates sequential CoT with causal attention and then switches to bidirectional attention for fast, parallel decoding of action vectors. This design is complemented by a two-stage training pipeline: we first use Supervised Fine-Tuning (SFT) to teach the model foundational reasoning, then apply Reinforcement Learning (RL) with task-success rewards to causally align the full reasoning-action sequence with desired outcomes. This synergy leads to state-of-the-art performance, achieving a 97.0% success rate on the LIBERO benchmark. Our ablations confirm the design's effectiveness: the hybrid architecture alone outperforms standard decoders by 15.5%, and the final RL stage provides a crucial 2% boost to secure top performance.

</details>


### [198] [Walrus: A Cross-Domain Foundation Model for Continuum Dynamics](https://arxiv.org/abs/2511.15684)
*Michael McCabe,Payel Mukhopadhyay,Tanya Marwah,Bruno Regaldo-Saint Blancard,Francois Rozet,Cristiana Diaconu,Lucas Meyer,Kaze W. K. Wong,Hadi Sotoudeh,Alberto Bietti,Irina Espejo,Rio Fear,Siavash Golkar,Tom Hehir,Keiya Hirashima,Geraud Krawezik,Francois Lanusse,Rudy Morel,Ruben Ohana,Liam Parker,Mariel Pettee,Jeff Shen,Kyunghyun Cho,Miles Cranmer,Shirley Ho*

Main category: cs.LG

TL;DR: Walrus是一个用于流体动力学的基于Transformer的基础模型，它通过结合谐波分析稳定方法、负载均衡分布式训练策略和计算自适应tokenization来克服数据异构性和不稳定的长期动态等挑战。


<details>
  <summary>Details</summary>
Motivation: 在物理模拟中，数据异构性和不稳定的长期动态以及不同的分辨率和维度阻碍了在现代硬件上进行有效的训练。

Method: 该研究结合了谐波分析稳定方法、负载均衡分布式2D和3D训练策略以及计算自适应tokenization。

Result: Walrus在下游任务和预训练数据的广度上，优于先前的基础模型，无论是在短期还是长期预测范围上。消融研究证实了该研究对预测稳定性、训练吞吐量和传统方法的迁移性能的价值。

Conclusion: Walrus是一个基于transformer的基础模型，主要用于类流体连续动力学，它在各种场景中都表现出色，并且代码和权重已发布供社区使用。

Abstract: Foundation models have transformed machine learning for language and vision, but achieving comparable impact in physical simulation remains a challenge. Data heterogeneity and unstable long-term dynamics inhibit learning from sufficiently diverse dynamics, while varying resolutions and dimensionalities challenge efficient training on modern hardware. Through empirical and theoretical analysis, we incorporate new approaches to mitigate these obstacles, including a harmonic-analysis-based stabilization method, load-balanced distributed 2D and 3D training strategies, and compute-adaptive tokenization. Using these tools, we develop Walrus, a transformer-based foundation model developed primarily for fluid-like continuum dynamics. Walrus is pretrained on nineteen diverse scenarios spanning astrophysics, geoscience, rheology, plasma physics, acoustics, and classical fluids. Experiments show that Walrus outperforms prior foundation models on both short and long term prediction horizons on downstream tasks and across the breadth of pretraining data, while ablation studies confirm the value of our contributions to forecast stability, training throughput, and transfer performance over conventional approaches. Code and weights are released for community use.

</details>


### [199] [The Impact of Quantization on Large Reasoning Model Reinforcement Learning](https://arxiv.org/abs/2511.15694)
*Medha Kumar,Zifei Xu,Xin Wang,Tristan Webb*

Main category: cs.LG

TL;DR: 大型强化学习无需监督微调即可实现强大的推理能力。然而，量化对大型推理模型中的强化学习的影响仍然是一个开放的问题。


<details>
  <summary>Details</summary>
Motivation: 研究量化对大型推理模型中强化学习的影响。

Method: 通过实验比较后训练量化（PTQ）模型和量化感知训练（QAT）模型在数学基准测试中的推理性能。

Result: 量化感知RL训练对学习过程产生负面影响，而PTQ和QLoRA带来了更好的性能。

Conclusion: 在大型推理模型的强化学习中，PTQ和QLoRA比量化感知训练更有效。

Abstract: Strong reasoning capabilities can now be achieved by large-scale reinforcement learning (RL) without any supervised fine-tuning. Although post-training quantization (PTQ) and quantization-aware training (QAT) are well studied in the context of fine-tuning, how quantization impacts RL in large reasoning models (LRMs) remains an open question. To answer this question, we conducted systematic experiments and discovered a significant gap in reasoning performance on mathematical benchmarks between post-RL quantized models and their quantization-aware RL optimized counterparts. Our findings suggest that quantization-aware RL training negatively impacted the learning process, whereas PTQ and QLoRA led to greater performance.

</details>
