<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 9]
- [cs.CV](#cs.CV) [Total: 10]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.LG](#cs.LG) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Evaluating LLMs' Reasoning Over Ordered Procedural Steps](https://arxiv.org/abs/2511.04688)
*Adrita Anika,Md Messal Monem Miah*

Main category: cs.CL

TL;DR: 大型语言模型在程序序列推理方面面临挑战，尤其是在步骤顺序对结果有直接影响时。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型重建被打乱的程序步骤（如食谱）的能力，因为正确的步骤排序对于任务成功至关重要。

Method: 使用食物食谱数据集，在零样本和少样本设置下评估多个大型语言模型，并采用Kendall's Tau、归一化最长公共子序列（NLCS）和归一化编辑距离（NED）等指标来评估排序质量。

Result: 模型性能随着序列长度的增加而下降，输入中更大的步骤位移（更严重的打乱）导致性能进一步下降。

Conclusion: 目前的大型语言模型在程序推理方面存在局限性，尤其是在处理较长和更混乱的输入时。

Abstract: Reasoning over procedural sequences, where the order of steps directly
impacts outcomes, is a critical capability for large language models (LLMs). In
this work, we study the task of reconstructing globally ordered sequences from
shuffled procedural steps, using a curated dataset of food recipes, a domain
where correct sequencing is essential for task success. We evaluate several
LLMs under zero-shot and few-shot settings and present a comprehensive
evaluation framework that adapts established metrics from ranking and sequence
alignment. These include Kendall's Tau, Normalized Longest Common Subsequence
(NLCS), and Normalized Edit Distance (NED), which capture complementary aspects
of ordering quality. Our analysis shows that model performance declines with
increasing sequence length, reflecting the added complexity of longer
procedures. We also find that greater step displacement in the input,
corresponding to more severe shuffling, leads to further degradation. These
findings highlight the limitations of current LLMs in procedural reasoning,
especially with longer and more disordered inputs.

</details>


### [2] [Adaptive Testing for LLM Evaluation: A Psychometric Alternative to Static Benchmarks](https://arxiv.org/abs/2511.04689)
*Peiyu Li,Xiuxiu Tang,Si Chen,Ying Cheng,Ronald Metoyer,Ting Hua,Nitesh V. Chawla*

Main category: cs.CL

TL;DR: 提出了一种新的自适应测试框架ATLAS，通过项目反应理论（IRT）和Fisher信息引导的项目选择来评估大型语言模型的能力，减少了评估所需的项目数量。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型评估方法需要大量的基准项目，成本高且速度慢，并且对所有项目的质量和信息量一视同仁。

Method: 使用项目反应理论（IRT）和Fisher信息引导的项目选择，构建自适应测试框架ATLAS。

Result: ATLAS能够在保持测量精度的同时，减少90%的项目数量。在HellaSwag数据集上，仅使用42个项目就能达到完整基准的估计效果，MAE为0.154。IRT排名与准确率排名存在差异，相同准确率的模型获得不同的IRT分数，23-31%的模型排名变化超过10位。

Conclusion: ATLAS框架能够有效减少评估所需的项目数量，同时保持测量精度，并揭示了现有评估方法中存在的问题，例如项目质量不一和排名偏差。

Abstract: Large language model evaluation requires thousands of benchmark items, making
evaluations expensive and slow. Existing methods compute average accuracy
across fixed item sets, treating all items equally despite varying quality and
informativeness. We present ATLAS an adaptive testing framework using Item
Response Theory (IRT) to estimate model ability through Fisher
information-guided item selection. Our analysis of five major benchmarks
reveals that 3-6% of items exhibit negative discrimination, indicating
annotation errors that corrupt static evaluation. ATLAS achieves 90% item
reduction while maintaining measurement precision: on HellaSwag (5,608 items),
we match full-benchmark estimates using only 42 items with 0.154 MAE. Our
framework maintains item exposure rates below 10% and test overlap at 16-27%,
compared to static benchmarks where every model sees all items (100% exposure).
Among 4,000+ tested models, IRT ranks differ from accuracy ranks: models with
the same accuracy get different IRT scores, and 23-31% of all models shift by
more than 10 rank positions. Code and calibrated item banks are available at
https://github.com/Peiyu-Georgia-Li/ATLAS.git.

</details>


### [3] [SARC: Sentiment-Augmented Deep Role Clustering for Fake News Detection](https://arxiv.org/abs/2511.04692)
*Jingqing Wang,Jiaxing Shang,Rong Xu,Fei Hao,Tianjin Huang,Geyong Min*

Main category: cs.CL

TL;DR: 提出了一种名为SARC的情感增强角色聚类框架，用于改进假新闻检测。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将情感特征视为辅助信号，忽略了角色差异，即相同的情感极性可能来自具有不同角色的用户，从而限制了他们捕捉细微模式以进行有效检测的能力。

Method: 利用情感增强的深度聚类来识别用户角色。该框架首先通过联合评论文本表示（使用BiGRU和注意力机制）和情感编码生成用户特征。然后，它构建了一个可区分的深度聚类模块来自动分类用户角色。最后，与将假新闻标签作为唯一监督信号的现有方法不同，我们提出了一个联合优化目标，整合了角色聚类和假新闻检测，以进一步提高模型性能。

Result: 在两个基准数据集RumourEval-19和Weibo-comp上的实验结果表明，与基线模型相比，SARC在所有指标上都取得了优异的性能。

Conclusion: SARC框架能够有效地利用情感信息和用户角色信息来提高假新闻检测的性能。

Abstract: Fake news detection has been a long-standing research focus in social
networks. Recent studies suggest that incorporating sentiment information from
both news content and user comments can enhance detection performance. However,
existing approaches typically treat sentiment features as auxiliary signals,
overlooking role differentiation, that is, the same sentiment polarity may
originate from users with distinct roles, thereby limiting their ability to
capture nuanced patterns for effective detection. To address this issue, we
propose SARC, a Sentiment-Augmented Role Clustering framework which utilizes
sentiment-enhanced deep clustering to identify user roles for improved fake
news detection. The framework first generates user features through joint
comment text representation (with BiGRU and Attention mechanism) and sentiment
encoding. It then constructs a differentiable deep clustering module to
automatically categorize user roles. Finally, unlike existing approaches which
take fake news label as the unique supervision signal, we propose a joint
optimization objective integrating role clustering and fake news detection to
further improve the model performance. Experimental results on two benchmark
datasets, RumourEval-19 and Weibo-comp, demonstrate that SARC achieves superior
performance across all metrics compared to baseline models. The code is
available at: https://github.com/jxshang/SARC.

</details>


### [4] [EncouRAGe: Evaluating RAG Local, Fast, and Reliable](https://arxiv.org/abs/2511.04696)
*Jan Strich,Adeline Scharfenberg,Chris Biemann,Martin Semmann*

Main category: cs.CL

TL;DR: EncouRAGe is a Python framework for developing and evaluating RAG systems.


<details>
  <summary>Details</summary>
Motivation: Streamline the development and evaluation of RAG systems using LLMs and Embedding Models.

Method: Five modular components: Type Manifest, RAG Factory, Inference, Vector Store, and Metrics.

Result: RAG underperforms compared to Oracle Context; Hybrid BM25 performs best; reranking gives marginal performance improvements with higher latency.

Conclusion: RAG requires further research to reach the performance of Oracle Context.

Abstract: We introduce EncouRAGe, a comprehensive Python framework designed to
streamline the development and evaluation of Retrieval-Augmented Generation
(RAG) systems using Large Language Models (LLMs) and Embedding Models.
EncouRAGe comprises five modular and extensible components: Type Manifest, RAG
Factory, Inference, Vector Store, and Metrics, facilitating flexible
experimentation and extensible development. The framework emphasizes scientific
reproducibility, diverse evaluation metrics, and local deployment, enabling
researchers to efficiently assess datasets within RAG workflows. This paper
presents implementation details and an extensive evaluation across multiple
benchmark datasets, including 25k QA pairs and over 51k documents. Our results
show that RAG still underperforms compared to the Oracle Context, while Hybrid
BM25 consistently achieves the best results across all four datasets. We
further examine the effects of reranking, observing only marginal performance
improvements accompanied by higher response latency.

</details>


### [5] [Reasoning Up the Instruction Ladder for Controllable Language Models](https://arxiv.org/abs/2511.04694)
*Zishuo Zheng,Vidhisha Balachandran,Chan Young Park,Faeze Brahman,Sachin Kumar*

Main category: cs.CL

TL;DR: 论文提出了一种通过将指令层次结构解析转化为推理任务，以提高大型语言模型（LLM）在多源指令下的可靠性和可控性的方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在现实决策中扮演重要角色，需要协调来自多个来源的指令，因此，实施指令层次结构至关重要。

Method: 构建了一个名为VerIH的指令层次数据集，并通过轻量级强化学习训练模型，使其具备指令优先级排序的推理能力。

Result: 微调后的模型在指令遵循和层次结构基准测试中表现出持续改进，并且在安全关键设置中表现出泛化能力，增强了对恶意攻击的抵抗力。

Conclusion: 通过推理指令层次结构，可以实现可靠的LLM，系统提示的更新可以实现模型行为的可控和鲁棒变化。

Abstract: As large language model (LLM) based systems take on high-stakes roles in
real-world decision-making, they must reconcile competing instructions from
multiple sources (e.g., model developers, users, and tools) within a single
prompt context. Thus, enforcing an instruction hierarchy (IH) in LLMs, where
higher-level directives override lower-priority requests, is critical for the
reliability and controllability of LLMs. In this work, we reframe instruction
hierarchy resolution as a reasoning task. Specifically, the model must first
"think" about the relationship between a given user prompt and higher-priority
(system) instructions before generating a response. To enable this capability
via training, we construct VerIH, an instruction hierarchy dataset of
constraint-following tasks with verifiable answers. This dataset comprises both
aligned and conflicting system-user instructions. We show that lightweight
reinforcement learning with VerIH effectively transfers general reasoning
capabilities of models to instruction prioritization. Our finetuned models
achieve consistent improvements on instruction following and instruction
hierarchy benchmarks. This reasoning ability also generalizes to
safety-critical settings beyond the training distribution. By treating safety
issues as resolving conflicts between adversarial user inputs and predefined
higher-priority policies, our trained model enhances robustness against
jailbreak and prompt injection attacks. These results demonstrate that
reasoning over instruction hierarchies provides a practical path to reliable
LLMs, where updates to system prompts yield controllable and robust changes in
model behavior.

</details>


### [6] [multiMentalRoBERTa: A Fine-tuned Multiclass Classifier for Mental Health Disorder](https://arxiv.org/abs/2511.04698)
*K M Sajjadul Islam,John Fields,Praveen Madiraju*

Main category: cs.CL

TL;DR: 本研究提出了一种名为 multiMentalRoBERTa 的微调 RoBERTa 模型，用于对社交媒体文本中常见的精神健康状况进行多类分类。


<details>
  <summary>Details</summary>
Motivation: 及时发现社交媒体文本中的精神健康障碍对于提供及时的支持、风险评估和转诊至适当的资源至关重要。

Method: 通过在多个精选数据集上进行微调 RoBERTa 模型，并结合数据探索和可解释性方法，来识别驱动分类的词汇线索。

Result: multiMentalRoBERTa 在六类设置中实现了 0.839 的宏 F1 分数，在五类设置中实现了 0.870 的宏 F1 分数，优于其他模型。

Conclusion: 研究结果强调了微调 transformers 在敏感环境中进行可靠和可解释检测的有效性，同时也强调了公平性、偏见缓解和人机协作安全协议的重要性。

Abstract: The early detection of mental health disorders from social media text is
critical for enabling timely support, risk assessment, and referral to
appropriate resources. This work introduces multiMentalRoBERTa, a fine-tuned
RoBERTa model designed for multiclass classification of common mental health
conditions, including stress, anxiety, depression, post-traumatic stress
disorder (PTSD), suicidal ideation, and neutral discourse. Drawing on multiple
curated datasets, data exploration is conducted to analyze class overlaps,
revealing strong correlations between depression and suicidal ideation as well
as anxiety and PTSD, while stress emerges as a broad, overlapping category.
Comparative experiments with traditional machine learning methods,
domain-specific transformers, and prompting-based large language models
demonstrate that multiMentalRoBERTa achieves superior performance, with macro
F1-scores of 0.839 in the six-class setup and 0.870 in the five-class setup
(excluding stress), outperforming both fine-tuned MentalBERT and baseline
classifiers. Beyond predictive accuracy, explainability methods, including
Layer Integrated Gradients and KeyBERT, are applied to identify lexical cues
that drive classification, with a particular focus on distinguishing depression
from suicidal ideation. The findings emphasize the effectiveness of fine-tuned
transformers for reliable and interpretable detection in sensitive contexts,
while also underscoring the importance of fairness, bias mitigation, and
human-in-the-loop safety protocols. Overall, multiMentalRoBERTa is presented as
a lightweight, robust, and deployable solution for enhancing support in mental
health platforms.

</details>


### [7] [Cross-Lingual SynthDocs: A Large-Scale Synthetic Corpus for Any to Arabic OCR and Document Understanding](https://arxiv.org/abs/2511.04699)
*Haneen Al-Homoud,Asma Ibrahim,Murtadha Al-Jubran,Fahad Al-Otaibi,Yazeed Al-Harbi,Daulet Toibazar,Kesen Wang,Pedro J. Moreno*

Main category: cs.CL

TL;DR: 该论文介绍了一个名为Cross-Lingual SynthDocs的大规模合成语料库，旨在解决阿拉伯语OCR和文档理解资源稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决阿拉伯语OCR和文档理解资源稀缺的问题。

Method: 该方法利用真实的扫描背景、双语布局和区分变音符号的字体来捕捉阿拉伯语文档的排版和结构复杂性。除了文本，语料库还包括各种图表和表格的渲染样式。

Result: 在SynthDocs上微调Qwen-2.5-VL可以在多个公共阿拉伯语基准测试中持续提高OCR的词错误率（WER）和字符错误率（CER），并且在其他模态中，树编辑距离相似度（TEDS）和图表提取分数（CharTeX）也得到了提高。

Conclusion: SynthDocs为推进多语言文档分析研究提供了一个可扩展的、视觉上逼真的资源。

Abstract: Cross-Lingual SynthDocs is a large-scale synthetic corpus designed to address
the scarcity of Arabic resources for Optical Character Recognition (OCR) and
Document Understanding (DU). The dataset comprises over 2.5 million of samples,
including 1.5 million textual data, 270K fully annotated tables, and hundred
thousands of real data based charts. Our pipeline leverages authentic scanned
backgrounds, bilingual layouts, and diacritic aware fonts to capture the
typographic and structural complexity of Arabic documents. In addition to text,
the corpus includes variety of rendered styles for charts and tables.
Finetuning Qwen-2.5-VL on SynthDocs yields consistent improvements in Word
Error Rate (WER) and Character Error Rate (CER) in terms of OCR across multiple
public Arabic benchmarks, Tree-Edit Distance Similarity (TEDS) and Chart
Extraction Score (CharTeX) improved as well in other modalities. SynthDocs
provides a scalable, visually realistic resource for advancing research in
multilingual document analysis.

</details>


### [8] [Separate the Wheat from the Chaff: Winnowing Down Divergent Views in Retrieval Augmented Generation](https://arxiv.org/abs/2511.04700)
*Song Wang,Zihan Chen,Peng Wang,Zhepei Wei,Zhen Tan,Yu Meng,Cong Shen,Jundong Li*

Main category: cs.CL

TL;DR: WinnowRAG通过过滤噪声文档来提升检索增强生成（RAG）的效果，无需模型微调。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法在增加检索文档数量时引入噪声，降低生成结果的准确性。

Method: WinnowRAG包含两个阶段：第一阶段进行查询感知聚类，将相似文档分组并由LLM生成答案；第二阶段进行过滤，通过评论LLM评估代理的输出，区分有用和噪声文档，并采用合并技术保留有用知识。

Result: 在多个数据集上的实验表明，WinnowRAG优于现有最佳方法。

Conclusion: WinnowRAG是一种有效的RAG框架，能够系统地过滤噪声文档，同时保留有价值的内容。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
integrating external knowledge sources to address their limitations in
accessing up-to-date or specialized information. A natural strategy to increase
the likelihood of retrieving relevant information is to expand the number of
retrieved documents. However, involving more documents could introduce
significant noise, as many documents may be irrelevant or misleading, thereby
reducing the overall accuracy of the generated responses. To overcome the
challenge associated with handling a larger number of documents, we propose
WinnowRAG, a novel RAG framework designed to systematically filter out noisy
documents while preserving valuable content -- a process we refer to as
winnowing. WinnowRAG operates in two stages: In Stage I, we perform query-aware
clustering to group similar documents and form distinct topic clusters. Each
cluster is assigned to an LLM agent for generating a unique answer. In Stage
II, we perform winnowing, wherein a critic LLM evaluates the outputs of
multiple agents and iteratively separates useful documents from noisy ones. To
retain useful documents when discarding agents, we propose two strategic
merging techniques to ensure that only relevant knowledge is used for
generating the final response. Crucially, WinnowRAG is model-agnostic and does
not require any model fine-tuning, making it easily adaptable to various tasks.
Extensive experiments on various realistic datasets demonstrate the
effectiveness of WinnowRAG over state-of-the-art baselines.

</details>


### [9] [Measuring what Matters: Construct Validity in Large Language Model Benchmarks](https://arxiv.org/abs/2511.04703)
*Andrew M. Bean,Ryan Othniel Kearns,Angelika Romanou,Franziska Sofia Hafner,Harry Mayne,Jan Batzner,Negar Foroutan,Chris Schmitz,Karolina Korgul,Hunar Batra,Oishi Deb,Emma Beharry,Cornelius Emde,Thomas Foster,Anna Gausen,María Grandury,Simeng Han,Valentin Hofmann,Lujain Ibrahim,Hazel Kim,Hannah Rose Kirk,Fangru Lin,Gabrielle Kaili-May Liu,Lennart Luettgau,Jabez Magomere,Jonathan Rystrøm,Anna Sotnikova,Yushi Yang,Yilun Zhao,Adel Bibi,Antoine Bosselut,Ronald Clark,Arman Cohan,Jakob Foerster,Yarin Gal,Scott A. Hale,Inioluwa Deborah Raji,Christopher Summerfield,Philip H. S. Torr,Cozmin Ududec,Luc Rocher,Adam Mahdi*

Main category: cs.CL

TL;DR: 这篇论文对大型语言模型（LLM）的评估基准进行了系统性回顾，发现现有基准在有效性方面存在问题，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 评估LLM的能力，并在部署前识别安全或鲁棒性问题至关重要。可靠地测量抽象和复杂的现象（如“安全”和“鲁棒性”）需要强大的结构效度。

Method: 作者组织了29位专家，系统地回顾了来自自然语言处理和机器学习领域领先会议的445个LLM基准。

Result: 发现现有基准在现象测量、任务和评分指标方面存在模式，这些模式会削弱结果声明的有效性。

Conclusion: 为了解决这些缺点，作者为研究人员和实践者在开发LLM基准时提供了八项关键建议和详细的可操作指导。

Abstract: Evaluating large language models (LLMs) is crucial for both assessing their
capabilities and identifying safety or robustness issues prior to deployment.
Reliably measuring abstract and complex phenomena such as 'safety' and
'robustness' requires strong construct validity, that is, having measures that
represent what matters to the phenomenon. With a team of 29 expert reviewers,
we conduct a systematic review of 445 LLM benchmarks from leading conferences
in natural language processing and machine learning. Across the reviewed
articles, we find patterns related to the measured phenomena, tasks, and
scoring metrics which undermine the validity of the resulting claims. To
address these shortcomings, we provide eight key recommendations and detailed
actionable guidance to researchers and practitioners in developing LLM
benchmarks.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [10] [IndicVisionBench: Benchmarking Cultural and Multilingual Understanding in VLMs](https://arxiv.org/abs/2511.04727)
*Ali Faraz,Akash,Shaharukh Khan,Raja Kolla,Akshat Patidar,Suranjan Goswami,Abhinav Ravi,Chandra Khatri,Shubham Agarwal*

Main category: cs.CV

TL;DR: 提出了一个针对印度次大陆的大规模基准测试 IndicVisionBench，用于评估视觉语言模型在文化多样性和多语言环境下的性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型的评估基准主要以西方为中心，缺乏对文化多样性和多语言环境的考量。

Method: 构建了包含英语和 10 种印度语言，涵盖 OCR、MMT 和 VQA 三种多模态任务的基准测试，包含约 5K 图像和 37K+ QA 对。

Result: 实验结果表明，现有视觉语言模型在文化多样性环境中存在显著的性能差距。

Conclusion: IndicVisionBench 建立了一个可复现的评估框架，为更具包容性的多模态研究铺平了道路。

Abstract: Vision-language models (VLMs) have demonstrated impressive generalization
across multimodal tasks, yet most evaluation benchmarks remain Western-centric,
leaving open questions about their performance in culturally diverse and
multilingual settings. To address this gap, we introduce IndicVisionBench, the
first large-scale benchmark centered on the Indian subcontinent. Covering
English and 10 Indian languages, our benchmark spans 3 multimodal tasks,
including Optical Character Recognition (OCR), Multimodal Machine Translation
(MMT), and Visual Question Answering (VQA), covering 6 kinds of question types.
Our final benchmark consists of a total of ~5K images and 37K+ QA pairs across
13 culturally grounded topics. In addition, we release a paired parallel corpus
of annotations across 10 Indic languages, creating a unique resource for
analyzing cultural and linguistic biases in VLMs. We evaluate a broad spectrum
of 8 models, from proprietary closed-source systems to open-weights medium and
large-scale models. Our experiments reveal substantial performance gaps,
underscoring the limitations of current VLMs in culturally diverse contexts. By
centering cultural diversity and multilinguality, IndicVisionBench establishes
a reproducible evaluation framework that paves the way for more inclusive
multimodal research.

</details>


### [11] [Knowledge-based anomaly detection for identifying network-induced shape artifacts](https://arxiv.org/abs/2511.04729)
*Rucha Deshpande,Tahsin Rahman,Miguel Lago,Adarsh Subbaswamy,Jana G. Delfino,Ghada Zamzmi,Elim Thompson,Aldo Badano,Seyed Kahaki*

Main category: cs.CV

TL;DR: 本研究提出了一种新的基于知识的异常检测方法，用于检测合成图像中网络诱导的形状伪影。


<details>
  <summary>Details</summary>
Motivation: 合成数据为解决训练机器学习模型的数据稀缺问题提供了一种有前景的方法，但如果没有适当的质量评估就采用，可能会引入伪影、失真和不真实的特征，从而损害模型性能和临床效用。

Method: 该方法利用一个两阶段框架，包括：(i) 一个新的特征提取器，通过分析沿解剖边界的角度梯度的每图像分布来构建一个专门的特征空间；(ii) 一个基于隔离森林的异常检测器。

Result: 定量评估表明，该方法成功地将伪影集中在最异常的分区（第 1 个百分位）中，AUC 值为 0.97 (CSAW-syn) 和 0.91 (VMLO-syn)。

Conclusion: 该方法是负责任地使用合成数据的一个进步，因为它允许开发人员评估合成图像的已知解剖约束，并查明和解决具体问题，以提高合成数据集的整体质量。

Abstract: Synthetic data provides a promising approach to address data scarcity for
training machine learning models; however, adoption without proper quality
assessments may introduce artifacts, distortions, and unrealistic features that
compromise model performance and clinical utility. This work introduces a novel
knowledge-based anomaly detection method for detecting network-induced shape
artifacts in synthetic images. The introduced method utilizes a two-stage
framework comprising (i) a novel feature extractor that constructs a
specialized feature space by analyzing the per-image distribution of angle
gradients along anatomical boundaries, and (ii) an isolation forest-based
anomaly detector. We demonstrate the effectiveness of the method for
identifying network-induced shape artifacts in two synthetic mammography
datasets from models trained on CSAW-M and VinDr-Mammo patient datasets
respectively. Quantitative evaluation shows that the method successfully
concentrates artifacts in the most anomalous partition (1st percentile), with
AUC values of 0.97 (CSAW-syn) and 0.91 (VMLO-syn). In addition, a reader study
involving three imaging scientists confirmed that images identified by the
method as containing network-induced shape artifacts were also flagged by human
readers with mean agreement rates of 66% (CSAW-syn) and 68% (VMLO-syn) for the
most anomalous partition, approximately 1.5-2 times higher than the least
anomalous partition. Kendall-Tau correlations between algorithmic and human
rankings were 0.45 and 0.43 for the two datasets, indicating reasonable
agreement despite the challenging nature of subtle artifact detection. This
method is a step forward in the responsible use of synthetic data, as it allows
developers to evaluate synthetic images for known anatomic constraints and
pinpoint and address specific issues to improve the overall quality of a
synthetic dataset.

</details>


### [12] [CPO: Condition Preference Optimization for Controllable Image Generation](https://arxiv.org/abs/2511.04753)
*Zonglin Lyu,Ming Li,Xinxin Liu,Chen Chen*

Main category: cs.CV

TL;DR: 提出了一种名为条件偏好优化 (CPO) 的新方法，用于提高文本到图像生成中的可控性，通过在控制条件上进行偏好学习，而不是在生成的图像上，从而减少了混淆因素和训练目标方差。


<details>
  <summary>Details</summary>
Motivation: ControlNet++虽然通过优化低噪声时间步来提高可控性，但忽略了高噪声时间步的贡献，并引入了额外的近似误差。直接偏好优化 (DPO) 可以优化所有时间步的可控性，但难以保证输赢图像对仅在可控性上不同，而保持图像质量等其他因素不变。

Method: 构建输赢控制信号，并训练模型偏好赢得控制信号。这种方法称为条件偏好优化 (CPO)。

Result: CPO 在多个控制类型上显著提高了相对于最先进的 ControlNet++ 的可控性：分割错误率降低超过 10%，人体姿势降低 70-80%，边缘和深度图持续降低 2-5%。

Conclusion: CPO 理论上比 DPO 具有更低的对比损失方差，并且在实验上取得了优异的结果。此外，CPO 在数据集管理方面需要的计算和存储更少。

Abstract: To enhance controllability in text-to-image generation, ControlNet introduces
image-based control signals, while ControlNet++ improves pixel-level cycle
consistency between generated images and the input control signal. To avoid the
prohibitive cost of back-propagating through the sampling process, ControlNet++
optimizes only low-noise timesteps (e.g., $t < 200$) using a single-step
approximation, which not only ignores the contribution of high-noise timesteps
but also introduces additional approximation errors. A straightforward
alternative for optimizing controllability across all timesteps is Direct
Preference Optimization (DPO), a fine-tuning method that increases model
preference for more controllable images ($I^{w}$) over less controllable ones
($I^{l}$). However, due to uncertainty in generative models, it is difficult to
ensure that win--lose image pairs differ only in controllability while keeping
other factors, such as image quality, fixed. To address this, we propose
performing preference learning over control conditions rather than generated
images. Specifically, we construct winning and losing control signals,
$\mathbf{c}^{w}$ and $\mathbf{c}^{l}$, and train the model to prefer
$\mathbf{c}^{w}$. This method, which we term \textit{Condition Preference
Optimization} (CPO), eliminates confounding factors and yields a low-variance
training objective. Our approach theoretically exhibits lower contrastive loss
variance than DPO and empirically achieves superior results. Moreover, CPO
requires less computation and storage for dataset curation. Extensive
experiments show that CPO significantly improves controllability over the
state-of-the-art ControlNet++ across multiple control types: over $10\%$ error
rate reduction in segmentation, $70$--$80\%$ in human pose, and consistent
$2$--$5\%$ reductions in edge and depth maps.

</details>


### [13] [DARN: Dynamic Adaptive Regularization Networks for Efficient and Robust Foundation Model Adaptation](https://arxiv.org/abs/2511.04766)
*Dhenenjay Yadav,Rohan Sawai*

Main category: cs.CV

TL;DR: This paper introduces Dynamic Adaptive Regularization Networks (DARN), a novel decoder architecture for adapting foundation models (FMs) in geospatial analysis.


<details>
  <summary>Details</summary>
Motivation: Standard adaptation methods fail to account for the heterogeneity in satellite imagery.

Method: DARN integrates a Task Complexity Predictor, Adaptive Dropout Modulation, and Dynamic Capacity Gating to dynamically adjust regularization based on sample complexity.

Result: DARN achieves state-of-the-art performance on GeoBench in full fine-tuning and SOTA-competitive accuracy on Sen1Floods11 in efficient adaptation, with superior out-of-distribution generalization, robustness, and minority class performance.

Conclusion: DARN offers a more intelligent, robust, and efficient approach to leveraging FMs in critical geospatial applications.

Abstract: Foundation models (FMs) offer powerful representations for geospatial
analysis, but adapting them effectively remains challenging. Standard
adaptation methods, whether full fine-tuning or efficient frozen-backbone
approaches, typically employ decoders with fixed regularization strategies,
failing to account for the significant heterogeneity in satellite imagery. We
introduce Dynamic Adaptive Regularization Networks (DARN), a novel decoder
architecture designed to address this limitation. DARN integrates three key
innovations: (1) a lightweight Task Complexity Predictor (TCP) that estimates
per-sample difficulty, (2) Adaptive Dropout Modulation (ADM), dynamically
adjusting dropout rates (from 0.1 to 0.5) based on predicted complexity, and
(3) Dynamic Capacity Gating (DCG) that modulates channel activation. We provide
theoretical justifications linking DARN's optimization to stationary point
convergence and its mechanism to adaptive information bottlenecks. Empirically,
DARN demonstrates exceptional performance across both major adaptation
paradigms. In full fine-tuning (unfrozen backbone), DARN achieves a new
state-of-the-art on the multi-task GeoBench benchmark (86.66% mIoU, +5.56 pp
over prior SOTA). In efficient adaptation (frozen backbone), DARN achieves
SOTA-competitive accuracy (90.5% mIoU on Sen1Floods11) while delivering
substantial advantages crucial for real-world deployment: superior
out-of-distribution (OOD) generalization (+9.5 pp mIoU on AI4SmallFarms),
enhanced robustness (17% relative reduction in corruption error), and improved
performance on minority classes. DARN offers a more intelligent, robust, and
efficient approach to leveraging FMs in critical geospatial applications.

</details>


### [14] [Global 3D Reconstruction of Clouds & Tropical Cyclones](https://arxiv.org/abs/2511.04773)
*Shirin Ermis,Cesar Aybar,Lilli Freischem,Stella Girtsou,Kyriaki-Margarita Bintsi,Emiliano Diaz Salas-Porras,Michael Eisinger,William Jones,Anna Jungbluth,Benoit Tremblay*

Main category: cs.CV

TL;DR: 该论文提出了一种新的基于预训练-微调流程的框架，可以将2D卫星图像转换为3D云图，从而重建强烈风暴的3D结构。


<details>
  <summary>Details</summary>
Motivation: 由于探测TC结构的卫星观测资料有限，以及难以解决TC增强中涉及的云特性，因此准确预测热带气旋（TC）仍然具有挑战性。

Method: 该方法基于一个预训练-微调流程，该流程从具有全球覆盖范围的多个卫星学习，以将2D卫星图像转换为相关云特性的3D云图。

Result: 该模型首次创建了全球瞬时3D云图，并准确地重建了强烈风暴的3D结构。该模型不仅扩展了可用的卫星观测资料，而且还在完全缺少观测资料时提供了估计。

Conclusion: 该模型对于加深我们对TC增强的理解和改进预报至关重要。

Abstract: Accurate forecasting of tropical cyclones (TCs) remains challenging due to
limited satellite observations probing TC structure and difficulties in
resolving cloud properties involved in TC intensification. Recent research has
demonstrated the capabilities of machine learning methods for 3D cloud
reconstruction from satellite observations. However, existing approaches have
been restricted to regions where TCs are uncommon, and are poorly validated for
intense storms. We introduce a new framework, based on a
pre-training--fine-tuning pipeline, that learns from multiple satellites with
global coverage to translate 2D satellite imagery into 3D cloud maps of
relevant cloud properties. We apply our model to a custom-built TC dataset to
evaluate performance in the most challenging and relevant conditions. We show
that we can - for the first time - create global instantaneous 3D cloud maps
and accurately reconstruct the 3D structure of intense storms. Our model not
only extends available satellite observations but also provides estimates when
observations are missing entirely. This is crucial for advancing our
understanding of TC intensification and improving forecasts.

</details>


### [15] [EETnet: a CNN for Gaze Detection and Tracking for Smart-Eyewear](https://arxiv.org/abs/2511.04779)
*Andrea Aspesi,Andrea Simpsi,Aaron Tognoli,Simone Mentasti,Luca Merigo,Matteo Matteucci*

Main category: cs.CV

TL;DR: 本论文提出了一种名为EETnet的卷积神经网络，专门用于使用纯粹的基于事件的数据进行眼动追踪，并且能够在资源有限的微控制器上运行。


<details>
  <summary>Details</summary>
Motivation: 现有的眼动追踪解决方案通常需要在强大的GPU上进行验证，无法在实际的嵌入式设备上部署。基于事件的相机虽然具有高效、低功耗的优点，但其稀疏和异步的特性对算法提出了挑战。

Method: 设计了一个专门用于眼动追踪的卷积神经网络EETnet，并概述了一种使用公共数据集训练、评估和量化该网络的方法。提出了两种版本的架构：一种是在原始图像上叠加的网格上检测瞳孔的分类模型，另一种是在像素级别上运行的回归模型。

Result: 该网络能够在资源有限的微控制器上运行，表明了其在嵌入式设备上的可行性。

Conclusion: EETnet的提出为基于事件的眼动追踪在嵌入式设备上的应用提供了可能。

Abstract: Event-based cameras are becoming a popular solution for efficient, low-power
eye tracking. Due to the sparse and asynchronous nature of event data, they
require less processing power and offer latencies in the microsecond range.
However, many existing solutions are limited to validation on powerful GPUs,
with no deployment on real embedded devices. In this paper, we present EETnet,
a convolutional neural network designed for eye tracking using purely
event-based data, capable of running on microcontrollers with limited
resources. Additionally, we outline a methodology to train, evaluate, and
quantize the network using a public dataset. Finally, we propose two versions
of the architecture: a classification model that detects the pupil on a grid
superimposed on the original image, and a regression model that operates at the
pixel level.

</details>


### [16] [3D Gaussian Point Encoders](https://arxiv.org/abs/2511.04797)
*Jim James,Ben Wilson,Simon Lucey,James Hays*

Main category: cs.CV

TL;DR: 提出了一种新的3D高斯点编码器，它比传统的PointNet更快、更高效。


<details>
  <summary>Details</summary>
Motivation: 为了解决3D识别任务中隐式表示（如PointNet）的局限性。

Method: 开发了基于自然梯度和PointNet蒸馏的优化技术，以找到能够重建PointNet激活的高斯基。

Result: 3D高斯点编码器比PointNet快2.7倍，内存减少46%，FLOPs减少88%。在Mamba3D中，速度提高1.27倍，内存减少42%，FLOPs减少54%。

Conclusion: 3D高斯点编码器足够轻量，可以在纯CPU设备上实现高帧率。

Abstract: In this work, we introduce the 3D Gaussian Point Encoder, an explicit
per-point embedding built on mixtures of learned 3D Gaussians. This explicit
geometric representation for 3D recognition tasks is a departure from widely
used implicit representations such as PointNet. However, it is difficult to
learn 3D Gaussian encoders in end-to-end fashion with standard optimizers. We
develop optimization techniques based on natural gradients and distillation
from PointNets to find a Gaussian Basis that can reconstruct PointNet
activations. The resulting 3D Gaussian Point Encoders are faster and more
parameter efficient than traditional PointNets. As in the 3D reconstruction
literature where there has been considerable interest in the move from implicit
(e.g., NeRF) to explicit (e.g., Gaussian Splatting) representations, we can
take advantage of computational geometry heuristics to accelerate 3D Gaussian
Point Encoders further. We extend filtering techniques from 3D Gaussian
Splatting to construct encoders that run 2.7 times faster as a comparable
accuracy PointNet while using 46% less memory and 88% fewer FLOPs. Furthermore,
we demonstrate the effectiveness of 3D Gaussian Point Encoders as a component
in Mamba3D, running 1.27 times faster and achieving a reduction in memory and
FLOPs by 42% and 54% respectively. 3D Gaussian Point Encoders are lightweight
enough to achieve high framerates on CPU-only devices.

</details>


### [17] [Data Efficiency and Transfer Robustness in Biomedical Image Segmentation: A Study of Redundancy and Forgetting with Cellpose](https://arxiv.org/abs/2511.04803)
*Shuo Zhao,Jianxu Chen*

Main category: cs.CV

TL;DR: 本文研究了生物医学图像分割模型Cellpose的训练数据冗余和跨域迁移对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 通用生物医学图像分割模型在不同成像方式和细胞类型中应用越来越广泛，但训练数据冗余和跨域迁移对模型保持能力的影响未被充分探索。

Method: 1. 提出数据集量化(DQ)策略构建紧凑而多样化的训练子集；2. 使用MAE嵌入和t-SNE进行潜在空间分析；3. 进行跨域微调实验，观察源域性能的下降；4. 使用选择性DQ基于重放，重新引入少量源数据，恢复源性能；5. 探索训练域排序对多阶段迁移的影响。

Result: 1. 仅使用10%的数据，图像分割性能达到饱和，表明存在大量冗余；2. DQ选择的patch比随机抽样捕获更大的特征多样性；3. 跨域微调会导致源域性能显著下降；4. 重新引入5-10%的源数据可有效恢复源性能；5. 训练域排序可以改善泛化并减少多阶段迁移中的遗忘。

Conclusion: 高效训练不仅需要紧凑的子集，还需要具有保留意识的学习策略和知情的领域排序。

Abstract: Generalist biomedical image segmentation models such as Cellpose are
increasingly applied across diverse imaging modalities and cell types. However,
two critical challenges remain underexplored: (1) the extent of training data
redundancy and (2) the impact of cross domain transfer on model retention. In
this study, we conduct a systematic empirical analysis of these challenges
using Cellpose as a case study. First, to assess data redundancy, we propose a
simple dataset quantization (DQ) strategy for constructing compact yet diverse
training subsets. Experiments on the Cyto dataset show that image segmentation
performance saturates with only 10% of the data, revealing substantial
redundancy and potential for training with minimal annotations. Latent space
analysis using MAE embeddings and t-SNE confirms that DQ selected patches
capture greater feature diversity than random sampling. Second, to examine
catastrophic forgetting, we perform cross domain finetuning experiments and
observe significant degradation in source domain performance, particularly when
adapting from generalist to specialist domains. We demonstrate that selective
DQ based replay reintroducing just 5-10% of the source data effectively
restores source performance, while full replay can hinder target adaptation.
Additionally, we find that training domain sequencing improves generalization
and reduces forgetting in multi stage transfer. Our findings highlight the
importance of data centric design in biomedical image segmentation and suggest
that efficient training requires not only compact subsets but also retention
aware learning strategies and informed domain ordering. The code is available
at https://github.com/MMV-Lab/biomedseg-efficiency.

</details>


### [18] [An Active Learning Pipeline for Biomedical Image Instance Segmentation with Minimal Human Intervention](https://arxiv.org/abs/2511.04811)
*Shuo Zhao,Yu Zhou,Jianxu Chen*

Main category: cs.CV

TL;DR: This paper introduces a data-centric AI workflow for biomedical image segmentation that combines active learning and pseudo-labeling to reduce the need for manual annotations.


<details>
  <summary>Details</summary>
Motivation: Traditional segmentation methods struggle with noisy data, and while deep learning models like U-Net and nnU-Net have improved performance, they require large amounts of annotated data or may underperform on specific datasets. Large foundation models offer zero-shot generalizability but may not be optimal for specialized datasets.

Method: The proposed workflow generates pseudo-labels from a foundation model for nnU-Net self-configuration, selects a representative core-set for minimal manual annotation, and fine-tunes the nnU-Net model.

Result: The approach reduces the need for manual annotations while maintaining competitive performance.

Conclusion: The proposed method provides an accessible solution for biomedical researchers to apply state-of-the-art AI techniques in their segmentation tasks.

Abstract: Biomedical image segmentation is critical for precise structure delineation
and downstream analysis. Traditional methods often struggle with noisy data,
while deep learning models such as U-Net have set new benchmarks in
segmentation performance. nnU-Net further automates model configuration, making
it adaptable across datasets without extensive tuning. However, it requires a
substantial amount of annotated data for cross-validation, posing a challenge
when only raw images but no labels are available. Large foundation models offer
zero-shot generalizability, but may underperform on specific datasets with
unique characteristics, limiting their direct use for analysis. This work
addresses these bottlenecks by proposing a data-centric AI workflow that
leverages active learning and pseudo-labeling to combine the strengths of
traditional neural networks and large foundation models while minimizing human
intervention. The pipeline starts by generating pseudo-labels from a foundation
model, which are then used for nnU-Net's self-configuration. Subsequently, a
representative core-set is selected for minimal manual annotation, enabling
effective fine-tuning of the nnU-Net model. This approach significantly reduces
the need for manual annotations while maintaining competitive performance,
providing an accessible solution for biomedical researchers to apply
state-of-the-art AI techniques in their segmentation tasks. The code is
available at https://github.com/MMV-Lab/AL_BioMed_img_seg.

</details>


### [19] [Geometry Denoising with Preferred Normal Vectors](https://arxiv.org/abs/2511.04848)
*Manuel Weiß,Lukas Baumgärtner,Roland Herzog,Stephan Schmidt*

Main category: cs.CV

TL;DR: 本文提出了一种新的几何去噪范例，该范例使用关于表面法向量的先验知识。


<details>
  <summary>Details</summary>
Motivation: 先验知识来自一组首选法向量（称为标签向量）。

Method: 去噪过程中自然嵌入了一个分割问题。分割基于法向量与标签向量集中元素的相似性。通过全变分项实现正则化。我们制定了一个分裂Bregman（ADMM）方法来解决由此产生的优化问题。顶点更新步骤基于二阶形状微积分。

Result: 未提及

Conclusion: 未提及

Abstract: We introduce a new paradigm for geometry denoising using prior knowledge
about the surface normal vector. This prior knowledge comes in the form of a
set of preferred normal vectors, which we refer to as label vectors. A
segmentation problem is naturally embedded in the denoising process. The
segmentation is based on the similarity of the normal vector to the elements of
the set of label vectors. Regularization is achieved by a total variation term.
We formulate a split Bregman (ADMM) approach to solve the resulting
optimization problem. The vertex update step is based on second-order shape
calculus.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [20] [An Efficient Proximity Graph-based Approach to Table Union Search](https://arxiv.org/abs/2511.05082)
*Yiming Xie,Hua Dai,Mingfeng Jiang,Pengyue Li,zhengkai Zhang,Bohan Li*

Main category: cs.DB

TL;DR: 本文提出了一种名为PGTUS的高效表格联合搜索方法，该方法通过结合新的优化策略和基于多对一二分匹配的过滤策略，以及增强的剪枝策略来提高搜索效率。


<details>
  <summary>Details</summary>
Motivation: 在表格联合搜索问题中，多向量模型通过捕获细粒度的语义对齐来实现卓越的检索质量。然而，由于二分图最大匹配的依赖性，多向量模型面临着比单向量模型更严峻的效率挑战。

Method: PGTUS采用多阶段流水线，结合了一种新的优化策略，一种基于多对一二分匹配的过滤策略，以及一种增强的剪枝策略。

Result: 在六个基准数据集上的大量实验表明，该方法在保持相当的召回率的同时，比现有方法实现了3.6-6.0倍的加速。

Conclusion: 本文提出了一种高效的表格联合搜索方法PGTUS，通过优化和剪枝策略，显著提高了搜索效率，并在多个数据集上验证了其有效性。

Abstract: Neural embedding models are extensively employed in the table union search
problem, which aims to find semantically compatible tables that can be merged
with a given query table. In particular, multi-vector models, which represent a
table as a vector set (typically one vector per column), have been demonstrated
to achieve superior retrieval quality by capturing fine-grained semantic
alignments. However, this problem faces more severe efficiency challenges than
the single-vector problem due to the inherent dependency on bipartite graph
maximum matching to compute unionability scores. Therefore, this paper proposes
an efficient Proximity Graph-based Table Union Search (PGTUS) approach. PGTUS
employs a multi-stage pipeline that combines a novel refinement strategy, a
filtering strategy based on many-to-one bipartite matching. Besides, we propose
an enhanced pruning strategy to prune the candidate set, which further improve
the search efficiency. Extensive experiments on six benchmark datasets
demonstrate that our approach achieves 3.6-6.0X speedup over existing
approaches while maintaining comparable recall rates.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [21] [Association via Entropy Reduction](https://arxiv.org/abs/2511.04901)
*Anthony Gamst,Lawrence Wilson*

Main category: cs.IR

TL;DR: 本文提出了一种新的评分方法 aver，用于识别与查询相关的文档，特别是在神经网络不占优势的大型图结构中寻找关联顶点。


<details>
  <summary>Details</summary>
Motivation: 传统方法 tf-idf 在识别相关文档方面表现良好，但存在一些局限性。本文旨在寻找一种更有效的方法，尤其是在特定场景下 tf-idf 表现不佳的情况下。

Method: 本文提出了一种名为 aver 的评分方法，并与 tf-idf 在具有关联标记的数据集上进行比较。

Result: 实验结果表明，在寻找关联对方面，aver 优于 tf-idf。此外，aver 在阈值设定、区分文档对、应用于大型文档集合以及自然性方面具有优势。

Conclusion: 本文提出的 aver 评分方法在特定场景下优于 tf-idf，并具有一些独特的优势，但计算复杂且不易解释。

Abstract: Prior to recent successes using neural networks, term frequency-inverse
document frequency (tf-idf) was clearly regarded as the best choice for
identifying documents related to a query. We provide a different score, aver,
and observe, on a dataset with ground truth marking for association, that aver
does do better at finding assciated pairs than tf-idf. This example involves
finding associated vertices in a large graph and that may be an area where
neural networks are not currently an obvious best choice. Beyond this one
anecdote, we observe that (1) aver has a natural threshold for declaring pairs
as unassociated while tf-idf does not, (2) aver can distinguish between pairs
of documents for which tf-idf gives a score of 1.0, (3) aver can be applied to
larger collections of documents than pairs while tf-idf cannot, and (4) that
aver is derived from entropy under a simple statistical model while tf-idf is a
construction designed to achieve a certain goal and hence aver may be more
"natural." To be fair, we also observe that (1) writing down and computing the
aver score for a pair is more complex than for tf-idf and (2) that the fact
that the aver score is naturally scale-free makes it more complicated to
interpret aver scores.

</details>


### [22] [Search Is Not Retrieval: Decoupling Semantic Matching from Contextual Assembly in RAG](https://arxiv.org/abs/2511.04939)
*Harshit Nainwani,Hediyeh Baban*

Main category: cs.IR

TL;DR: 提出了一个名为 SINR 的双层架构，用于区分细粒度搜索和粗粒度检索。


<details>
  <summary>Details</summary>
Motivation: 当前 AI 流程中的检索系统混淆了查找相关信息和提供足够推理上下文这两个独立的过程。

Method: SINR 框架通过将小的、语义上精确的搜索块直接连接到更大的、上下文完整的检索块来增强检索系统的可组合性、可扩展性和上下文保真度，且不产生额外的处理成本。

Result: 将检索从被动步骤变为主动步骤，使系统架构更像人们处理信息的方式。

Conclusion: 为下一代使用检索的 AI 系统奠定了实践基础。

Abstract: Retrieval systems are essential to contemporary AI pipelines, although most
confuse two separate processes: finding relevant information and giving enough
context for reasoning. We introduce the Search-Is-Not-Retrieve (SINR)
framework, a dual-layer architecture that distinguishes between fine-grained
search representations and coarse-grained retrieval contexts. SINR enhances the
composability, scalability, and context fidelity of retrieval systems by
directly connecting small, semantically accurate search chunks to larger,
contextually complete retrieve chunks, all without incurring extra processing
costs. This design changes retrieval from a passive step to an active one,
making the system architecture more like how people process information. We
discuss the SINR framework's conceptual foundation, formal structure,
implementation issues, and qualitative outcomes. This provides a practical
foundation for the next generation of AI systems that use retrieval.

</details>


### [23] [Query Generation Pipeline with Enhanced Answerability Assessment for Financial Information Retrieval](https://arxiv.org/abs/2511.05000)
*Hyunkyu Kim,Yeeun Yoo,Youngjun Kwak*

Main category: cs.IR

TL;DR: 本文提出了一种利用LLM生成特定领域IR基准的系统方法，并构建了KoBankIR基准。


<details>
  <summary>Details</summary>
Motivation: 现有的基准无法捕捉到真实银行场景中复杂和特定领域的信息需求。构建特定领域的IR基准成本高昂，并受到使用真实客户数据的法律限制。

Method: 该方法结合了单文档和多文档查询生成，以及增强和推理增强的可回答性评估方法。

Result: 构建了包含815个查询的KoBankIR，实验表明现有的检索模型难以处理KoBankIR中复杂的多文档查询。

Conclusion: 该系统方法对于特定领域基准构建具有价值，并强调了金融领域改进检索技术的必要性。

Abstract: As financial applications of large language models (LLMs) gain attention,
accurate Information Retrieval (IR) remains crucial for reliable AI services.
However, existing benchmarks fail to capture the complex and domain-specific
information needs of real-world banking scenarios. Building domain-specific IR
benchmarks is costly and constrained by legal restrictions on using real
customer data. To address these challenges, we propose a systematic methodology
for constructing domain-specific IR benchmarks through LLM-based query
generation. As a concrete implementation of this methodology, our pipeline
combines single and multi-document query generation with an enhanced and
reasoning-augmented answerability assessment method, achieving stronger
alignment with human judgments than prior approaches. Using this methodology,
we construct KoBankIR, comprising 815 queries derived from 204 official banking
documents. Our experiments show that existing retrieval models struggle with
the complex multi-document queries in KoBankIR, demonstrating the value of our
systematic approach for domain-specific benchmark construction and underscoring
the need for improved retrieval techniques in financial domains.

</details>


### [24] [Wikipedia-based Datasets in Russian Information Retrieval Benchmark RusBEIR](https://arxiv.org/abs/2511.05079)
*Grigory Kovalev,Natalia Loukachevitch,Mikhail Tikhomirov,Olga Babina,Pavel Mamaev*

Main category: cs.IR

TL;DR: 本文介绍了一系列新的俄语信息检索数据集，这些数据集构建于俄语维基百科的“你知道吗……”部分。


<details>
  <summary>Details</summary>
Motivation: 为了支持各种检索任务，包括事实核查、检索增强生成和全文检索，通过利用有趣的事件和它们引用的维基百科文章，这些文章在句子级别上用分级相关性进行注释。

Method: 本文描述了数据集创建的方法，通过比较词汇检索模型（如BM25）与针对俄语进行微调的最先进的神经架构以及多语言模型，扩展了RusBEIR研究。

Result: 实验结果表明，词汇方法在全文检索方面往往优于神经模型，而神经方法能更好地捕捉较短文本中的词汇语义，例如在事实核查或细粒度检索中。分析了文档长度对检索性能的影响，并证明将检索与神经重新排序相结合可以持续提高结果。

Conclusion: 本文的贡献扩展了可用于俄语信息检索研究的资源，并强调了准确评估检索模型以实现最佳性能的重要性。所有数据集均可在HuggingFace上公开获得。为了方便重现和未来的研究，本文还在GitHub上发布了完整的实现。

Abstract: In this paper, we present a novel series of Russian information retrieval
datasets constructed from the "Did you know..." section of Russian Wikipedia.
Our datasets support a range of retrieval tasks, including fact-checking,
retrieval-augmented generation, and full-document retrieval, by leveraging
interesting facts and their referenced Wikipedia articles annotated at the
sentence level with graded relevance. We describe the methodology for dataset
creation that enables the expansion of existing Russian Information Retrieval
(IR) resources. Through extensive experiments, we extend the RusBEIR research
by comparing lexical retrieval models, such as BM25, with state-of-the-art
neural architectures fine-tuned for Russian, as well as multilingual models.
Results of our experiments show that lexical methods tend to outperform neural
models on full-document retrieval, while neural approaches better capture
lexical semantics in shorter texts, such as in fact-checking or fine-grained
retrieval. Using our newly created datasets, we also analyze the impact of
document length on retrieval performance and demonstrate that combining
retrieval with neural reranking consistently improves results. Our contribution
expands the resources available for Russian information retrieval research and
highlights the importance of accurate evaluation of retrieval models to achieve
optimal performance. All datasets are publicly available at HuggingFace. To
facilitate reproducibility and future research, we also release the full
implementation on GitHub.

</details>


### [25] [QUESTER: Query Specification for Generative Retrieval](https://arxiv.org/abs/2511.05301)
*Arthur Satouf,Yuxuan Zong,Habiboulaye Amadou-Boubacar,Pablo Piantanida,Benjamin Piwowarski*

Main category: cs.IR

TL;DR: QUExy Specification gEnerative Retrieval (QUESTER) reframes Generative Retrieval (GR) as query specification generation using a small LLM, trained with reinforcement learning, to generate keyword queries for BM25.


<details>
  <summary>Details</summary>
Motivation: Generative Retrieval (GR) struggles to generalize and is costly to scale.

Method: Reframes GR as query specification generation, using a small LLM to generate keyword queries for BM25. The policy is trained using reinforcement learning techniques (GRPO).

Result: More effective than BM25 and competitive with neural IR models, while maintaining good efficiency.

Conclusion: QUESTER offers an effective and efficient alternative to traditional and generative retrieval methods.

Abstract: Generative Retrieval (GR) differs from the traditional index-then-retrieve
pipeline by storing relevance in model parameters and directly generating
document identifiers. However, GR often struggles to generalize and is costly
to scale. We introduce QUESTER (QUEry SpecificaTion gEnerative Retrieval),
which reframes GR as query specification generation - in this work, a simple
keyword query handled by BM25 - using a (small) LLM. The policy is trained
using reinforcement learning techniques (GRPO). Across in- and out-of-domain
evaluations, we show that our model is more effective than BM25, and
competitive with neural IR models, while maintaining a good efficiency

</details>


### [26] [TeaRAG: A Token-Efficient Agentic Retrieval-Augmented Generation Framework](https://arxiv.org/abs/2511.05385)
*Chao Zhang,Yuhao Wang,Derong Xu,Haoxin Zhang,Yuanjie Lyu,Yuhao Chen,Shuochen Liu,Tong Xu,Xiangyu Zhao,Yan Gao,Yao Hu,Enhong Chen*

Main category: cs.IR

TL;DR: TeaRAG通过压缩检索内容和推理步骤，提高了agentic RAG的效率，同时保持了准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的agentic RAG方法在搜索和推理过程中产生大量的token开销，导致效率低下。

Method: 1) 使用基于三元组的图检索增强chunk-based语义检索来压缩检索内容；2) 提出迭代的Process-aware直接偏好优化(IP-DPO)来减少推理步骤。

Result: 在六个数据集上，TeaRAG在Llama3-8B-Instruct和Qwen2.5-14B-Instruct上分别将输出tokens减少了61%和59%，同时平均精确匹配度提高了4%和2%。

Conclusion: TeaRAG是一个token高效的agentic RAG框架，能够在减少token使用的同时保持甚至提高准确性。

Abstract: Retrieval-Augmented Generation (RAG) utilizes external knowledge to augment
Large Language Models' (LLMs) reliability. For flexibility, agentic RAG employs
autonomous, multi-round retrieval and reasoning to resolve queries. Although
recent agentic RAG has improved via reinforcement learning, they often incur
substantial token overhead from search and reasoning processes. This trade-off
prioritizes accuracy over efficiency. To address this issue, this work proposes
TeaRAG, a token-efficient agentic RAG framework capable of compressing both
retrieval content and reasoning steps. 1) First, the retrieved content is
compressed by augmenting chunk-based semantic retrieval with a graph retrieval
using concise triplets. A knowledge association graph is then built from
semantic similarity and co-occurrence. Finally, Personalized PageRank is
leveraged to highlight key knowledge within this graph, reducing the number of
tokens per retrieval. 2) Besides, to reduce reasoning steps, Iterative
Process-aware Direct Preference Optimization (IP-DPO) is proposed.
Specifically, our reward function evaluates the knowledge sufficiency by a
knowledge matching mechanism, while penalizing excessive reasoning steps. This
design can produce high-quality preference-pair datasets, supporting iterative
DPO to improve reasoning conciseness. Across six datasets, TeaRAG improves the
average Exact Match by 4% and 2% while reducing output tokens by 61% and 59% on
Llama3-8B-Instruct and Qwen2.5-14B-Instruct, respectively. Code is available at
https://github.com/Applied-Machine-Learning-Lab/TeaRAG.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [27] [Stateful KV Cache Management for LLMs: Balancing Space, Time, Accuracy, and Positional Fidelity](https://arxiv.org/abs/2511.04686)
*Pratik Poudel*

Main category: cs.LG

TL;DR: KV缓存对于LLM中的有效自回归推理至关重要，但其在有状态多轮场景中的无限制增长带来了重大挑战。本文探讨了KV缓存管理策略、模型架构上下文限制以及位置编码完整性之间的相互作用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型中的Key-Value (KV)缓存对于有效的自回归推理至关重要，但在有状态多轮场景中，其无限制的增长带来了主要的挑战。

Method: 通过使用有状态基准测试框架的经验分析

Result: LLM生成质量在累积的KV缓存接近或超过模型的训练上下文窗口时会急剧下降，即使是高保留率的常见驱逐策略，如果它们破坏了位置连贯性，也会降低性能。简单的策略，比如保持初始的“要点”，可以产生比复杂的或位置破坏性的策略更连贯的生成。

Conclusion: 我们提倡尊重架构限制、保持位置结构并全面看待“缓存健康”的驱逐技术，而不仅仅是大小。

Abstract: The Key-Value (KV) cache is integral to efficient autoregressive inference in
large language models (LLMs), yet its unbounded growth in stateful multi-turn
scenarios presents major challenges. This paper examines the interplay between
KV cache management strategies, the architectural context limits of models like
meta-llama/Meta-Llama-3-8b-instruct, and the often-overlooked integrity of
positional encodings. Through empirical analysis using a stateful benchmarking
framework, we show that LLM generation quality degrades sharply when the
accumulated KV cache approaches or exceeds the model's trained context window
(e.g., 8192 tokens for Llama 3), a failure mode distinct from GPU memory
exhaustion. Common eviction strategies, even high-retention ones (e.g., 99% via
AttentionTop), can worsen performance if they disrupt positional coherence.
Because LLMs rely on consistent positional signals (e.g., RoPE), compacting a
cache by removing non-contiguous tokens can scramble these signals and lead to
degenerative outputs. We further show that simple strategies preserving
contiguous context blocks (e.g., keeping an initial "gist") can yield more
coherent generations than complex or positionally disruptive ones. We advocate
for eviction techniques that respect architectural limits, preserve positional
structure, and view "cache health" holistically beyond mere size.

</details>


### [28] [Ada-FCN: Adaptive Frequency-Coupled Network for fMRI-Based Brain Disorder Classification](https://arxiv.org/abs/2511.04718)
*Yue Xun,Jiaxing Xu,Wenbo Gao,Chen Yang,Shujun Wang*

Main category: cs.LG

TL;DR: 该论文提出了一种新的静息态fMRI分析框架，通过自适应分解学习任务相关的频率子带，并学习频率耦合连接，以提高脑部疾病分类的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有模型忽略了神经元振荡的多频率特性，并且依赖于预定义的频率带，无法捕捉个体差异或疾病特异性改变，限制了诊断的灵敏性和特异性。

Method: 该论文提出了自适应级联分解来学习任务相关的频率子带，并提出了频率耦合连接学习来捕捉带内和跨带的相互作用，最终通过一个统一的GCN来进行诊断预测。

Result: 在ADNI和ABIDE数据集上的实验结果表明，该方法优于现有方法。

Conclusion: 该论文提出的新框架能够更有效地利用静息态fMRI数据进行脑部疾病的诊断和分类。

Abstract: Resting-state fMRI has become a valuable tool for classifying brain disorders
and constructing brain functional connectivity networks
  by tracking BOLD signals across brain regions. However, existing mod els
largely neglect the multi-frequency nature of neuronal oscillations,
  treating BOLD signals as monolithic time series. This overlooks the cru cial
fact that neurological disorders often manifest as disruptions within
  specific frequency bands, limiting diagnostic sensitivity and specificity.
  While some methods have attempted to incorporate frequency informa tion, they
often rely on predefined frequency bands, which may not be
  optimal for capturing individual variability or disease-specific alterations.
  To address this, we propose a novel framework featuring Adaptive Cas cade
Decomposition to learn task-relevant frequency sub-bands for each
  brain region and Frequency-Coupled Connectivity Learning to capture
  both intra- and nuanced cross-band interactions in a unified functional
  network. This unified network informs a novel message-passing mecha nism
within our Unified-GCN, generating refined node representations
  for diagnostic prediction. Experimental results on the ADNI and ABIDE
  datasets demonstrate superior performance over existing methods. The
  code is available at https://github.com/XXYY20221234/Ada-FCN.

</details>


### [29] [AWEMixer: Adaptive Wavelet-Enhanced Mixer Network for Long-Term Time Series Forecasting](https://arxiv.org/abs/2511.04722)
*Qianyang Li,Xingjun Zhang,Peng Tao,Shaoxun Wang,Yancheng Pan,Jia Wei*

Main category: cs.LG

TL;DR: 提出了一种名为AWEMixer的自适应小波增强混合器网络，用于物联网环境中的长期时间序列预测。


<details>
  <summary>Details</summary>
Motivation: 由于传感器信号的非平稳和多尺度特性，物联网环境中的长期时间序列预测仍然是一个重大挑战。此外，误差累积会导致预测质量下降。

Method: 该模型包括两个创新组件：1) 频率路由器，利用快速傅里叶变换获得的全局周期性模式自适应地加权局部小波子带；2) 相干门控融合块，通过交叉注意力和门控机制实现突出频率特征与多尺度时间表示的选择性集成。

Result: 在七个公共基准测试中，该模型比最新的模型更有效。与基于Transformer和MLP的最新模型相比，该模型在长序列时间序列预测中始终如一地提高了性能。

Conclusion: AWEMixer模型在长期时间序列预测中表现出色，优于现有技术水平的模型。

Abstract: Forecasting long-term time series in IoT environments remains a significant
challenge due to the non-stationary and multi-scale characteristics of sensor
signals. Furthermore, error accumulation causes a decrease in forecast quality
when predicting further into the future. Traditional methods are restricted to
operate in time-domain, while the global frequency information achieved by
Fourier transform would be regarded as stationary signals leading to blur the
temporal patterns of transient events. We propose AWEMixer, an Adaptive
Wavelet-Enhanced Mixer Network including two innovative components: 1) a
Frequency Router designs to utilize the global periodicity pattern achieved by
Fast Fourier Transform to adaptively weight localized wavelet subband, and 2) a
Coherent Gated Fusion Block to achieve selective integration of prominent
frequency features with multi-scale temporal representation through
cross-attention and gating mechanism, which realizes accurate time-frequency
localization while remaining robust to noise. Seven public benchmarks validate
that our model is more effective than recent state-of-the-art models.
Specifically, our model consistently achieves performance improvement compared
with transformer-based and MLP-based state-of-the-art models in long-sequence
time series forecasting. Code is available at
https://github.com/hit636/AWEMixer

</details>


### [30] [Temporal convolutional and fusional transformer model with Bi-LSTM encoder-decoder for multi-time-window remaining useful life prediction](https://arxiv.org/abs/2511.04723)
*Mohamadreza Akbari Pour,Mohamad Sadeq Karimi,Amir Hossein Mazloumi*

Main category: cs.LG

TL;DR: 这篇论文提出了一种用于预测工业系统中剩余使用寿命（RUL）的新框架。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以捕捉细粒度的时间依赖性，同时无法动态地优先考虑跨时间的关键特征。

Method: 该框架集成了时间卷积网络（TCN）和改进的时间融合Transformer（TFT），并结合Bi-LSTM编码器-解码器和多时间窗口方法。

Result: 在基准数据集上的评估表明，该模型将平均RMSE降低了高达5.5%。

Conclusion: 该框架提高了工业预测系统的有效性，并突出了先进的时间序列Transformer在RUL预测中的潜力。

Abstract: Health prediction is crucial for ensuring reliability, minimizing downtime,
and optimizing maintenance in industrial systems. Remaining Useful Life (RUL)
prediction is a key component of this process; however, many existing models
struggle to capture fine-grained temporal dependencies while dynamically
prioritizing critical features across time for robust prognostics. To address
these challenges, we propose a novel framework that integrates Temporal
Convolutional Networks (TCNs) for localized temporal feature extraction with a
modified Temporal Fusion Transformer (TFT) enhanced by Bi-LSTM encoder-decoder.
This architecture effectively bridges short- and long-term dependencies while
emphasizing salient temporal patterns. Furthermore, the incorporation of a
multi-time-window methodology improves adaptability across diverse operating
conditions. Extensive evaluations on benchmark datasets demonstrate that the
proposed model reduces the average RMSE by up to 5.5%, underscoring its
improved predictive accuracy compared to state-of-the-art methods. By closing
critical gaps in current approaches, this framework advances the effectiveness
of industrial prognostic systems and highlights the potential of advanced
time-series transformers for RUL prediction.

</details>


### [31] [Regularized GLISp for sensor-guided human-in-the-loop optimization](https://arxiv.org/abs/2511.04751)
*Matteo Cercola,Michele Lomuscio,Dario Piga,Simone Formentin*

Main category: cs.LG

TL;DR: 提出了一种传感器引导的 GLISp 扩展方法，用于人机回路校准，该方法集成了可测量的描述符到偏好学习循环中。


<details>
  <summary>Details</summary>
Motivation: 现有的基于偏好的优化方法忽略了信息丰富的传感器测量。

Method: 通过物理信息假设函数和最小二乘正则化项，将可测量的描述符集成到偏好学习循环中。

Result: 在分析基准和人机回路车辆悬架调整任务中的数值评估表明，与基线 GLISp 相比，收敛速度更快，最终解决方案更优越。

Conclusion: 该方法结合了主观反馈和定量传感器信息，同时保留了基于偏好的搜索的灵活性。

Abstract: Human-in-the-loop calibration is often addressed via preference-based
optimization, where algorithms learn from pairwise comparisons rather than
explicit cost evaluations. While effective, methods such as Preferential
Bayesian Optimization or Global optimization based on active preference
learning with radial basis functions (GLISp) treat the system as a black box
and ignore informative sensor measurements. In this work, we introduce a
sensor-guided regularized extension of GLISp that integrates measurable
descriptors into the preference-learning loop through a physics-informed
hypothesis function and a least-squares regularization term. This injects
grey-box structure, combining subjective feedback with quantitative sensor
information while preserving the flexibility of preference-based search.
Numerical evaluations on an analytical benchmark and on a human-in-the-loop
vehicle suspension tuning task show faster convergence and superior final
solutions compared to baseline GLISp.

</details>


### [32] [When Data Falls Short: Grokking Below the Critical Threshold](https://arxiv.org/abs/2511.04760)
*Vaibhav Singh,Eugene Belilovsky,Rahaf Aljundi*

Main category: cs.LG

TL;DR: 研究了在数据稀缺和分布偏移情况下，知识蒸馏（KD）如何促进和加速Grokking现象，即使在数据量低于临界阈值时也能实现泛化。


<details>
  <summary>Details</summary>
Motivation: 模型在训练数据上过拟合后表现出延迟泛化（Grokking）现象，尤其是在数据稀缺和实际分布偏移的情况下。

Method: 1. 使用知识蒸馏（KD）从已在分布p1上Grokking的模型中，诱导和加速在不同分布p2上的Grokking。
2. 研究在联合分布（p1, p2）上的训练，利用从在单独分布上Grokking的模型中提取的知识进行泛化。
3. 考察持续预训练设置，其中Grokking模型从p1过渡到p2，使用KD加速泛化并减轻灾难性遗忘。

Result: 1. 知识蒸馏（KD）可以诱导和加速Grokking，即使在数据低于临界阈值时。
2. 从在单独分布上Grokking的模型中进行蒸馏，能够在联合分布上实现泛化。
3. 在持续预训练中，KD加速泛化并减轻灾难性遗忘，即使只有10%的数据也能实现强大的性能。

Conclusion: 知识蒸馏在低数据和不断演变的分布环境中，对于实现泛化起着核心作用。

Abstract: In this paper, we investigate the phenomenon of grokking, where models
exhibit delayed generalization following overfitting on training data. We focus
on data-scarce regimes where the number of training samples falls below the
critical threshold, making grokking unobservable, and on practical scenarios
involving distribution shift. We first show that Knowledge Distillation (KD)
from a model that has already grokked on a distribution (p1) can induce and
accelerate grokking on a different distribution (p2), even when the available
data lies below the critical threshold. This highlights the value of KD for
deployed models that must adapt to new distributions under limited data. We
then study training on the joint distribution (p1, p2) and demonstrate that
while standard supervised training fails when either distribution has
insufficient data, distilling from models grokked on the individual
distributions enables generalization. Finally, we examine a continual
pretraining setup, where a grokked model transitions from p1 to p2, and find
that KD both accelerates generalization and mitigates catastrophic forgetting,
achieving strong performance even with only 10% of the data. Together, our
results provide new insights into the mechanics of grokking under knowledge
transfer and underscore the central role of KD in enabling generalization in
low-data and evolving distribution settings.

</details>


### [33] [FuseFlow: A Fusion-Centric Compilation Framework for Sparse Deep Learning on Streaming Dataflow](https://arxiv.org/abs/2511.04768)
*Rubens Lacouture,Nathan Zhang,Ritvik Sharma,Marco Siracusa,Fredrik Kjolstad,Kunle Olukotun,Olivia Hsu*

Main category: cs.LG

TL;DR: FuseFlow是一个编译器，可以将PyTorch中编写的稀疏机器学习模型转换为用于可重构数据流架构（RDA）的融合稀疏数据流图。


<details>
  <summary>Details</summary>
Motivation: 为了解决深度学习模型扩展带来的效率问题，稀疏计算和专用数据流硬件应运而生。

Method: FuseFlow支持稀疏操作的通用跨表达式融合，以及并行化、数据流排序和稀疏阻塞等优化。

Result: 在四个具有稀疏性的真实机器学习应用程序中，完整的融合并不总是稀疏模型的最佳选择。FuseFlow还提供了一种启发式方法来识别和修剪次优配置。对于具有BigBird块稀疏注意力的GPT-3，与未融合的基线相比，实现了约2.7倍的加速。

Conclusion: 融合粒度取决于模型本身。

Abstract: As deep learning models scale, sparse computation and specialized dataflow
hardware have emerged as powerful solutions to address efficiency. We propose
FuseFlow, a compiler that converts sparse machine learning models written in
PyTorch to fused sparse dataflow graphs for reconfigurable dataflow
architectures (RDAs). FuseFlow is the first compiler to support general
cross-expression fusion of sparse operations. In addition to fusion across
kernels (expressions), FuseFlow also supports optimizations like
parallelization, dataflow ordering, and sparsity blocking. It targets a
cycle-accurate dataflow simulator for microarchitectural analysis of fusion
strategies. We use FuseFlow for design-space exploration across four real-world
machine learning applications with sparsity, showing that full fusion (entire
cross-expression fusion across all computation in an end-to-end model) is not
always optimal for sparse models-fusion granularity depends on the model
itself. FuseFlow also provides a heuristic to identify and prune suboptimal
configurations. Using Fuseflow, we achieve performance improvements, including
a ~2.7x speedup over an unfused baseline for GPT-3 with BigBird block-sparse
attention.

</details>


### [34] [SLOFetch: Compressed-Hierarchical Instruction Prefetching for Cloud Microservices](https://arxiv.org/abs/2511.04774)
*Liu Jiang,Zerui Bao,Shiqi Sheng,Di Zhu*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的指令预取设计，旨在提高云工作负载的效率，并减少尾部延迟和能源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有的云服务依赖于深度软件栈和微服务编排，导致指令占用空间增加，前端停顿，尾部延迟增大，能源消耗增加。

Method: 该设计基于Entangling Instruction Prefetcher (EIP)，引入了压缩条目（Compressed Entry）和分层元数据存储方案（Hierarchical Metadata Storage scheme），并增加了一个轻量级的在线机器学习控制器（Online ML Controller）。

Result: 该方法在数据中心应用中保留了EIP类似的速度提升，并提高了ML时代网络服务的效率。

Conclusion: 所提出的指令预取设计能够有效地提高云工作负载的效率，并减少尾部延迟和能源消耗。

Abstract: Large-scale networked services rely on deep soft-ware stacks and microservice
orchestration, which increase instruction footprints and create frontend stalls
that inflate tail latency and energy. We revisit instruction prefetching for
these cloud workloads and present a design that aligns with SLO driven and self
optimizing systems. Building on the Entangling Instruction Prefetcher (EIP), we
introduce a Compressed Entry that captures up to eight destinations around a
base using 36 bits by exploiting spatial clustering, and a Hierarchical
Metadata Storage scheme that keeps only L1 resident and frequently queried
entries on chip while virtualizing bulk metadata into lower levels. We further
add a lightweight Online ML Controller that scores prefetch profitability using
context features and a bandit adjusted threshold. On data center applications,
our approach preserves EIP like speedups with smaller on chip state and
improves efficiency for networked services in the ML era.

</details>


### [35] [Conditional Neural ODE for Longitudinal Parkinson's Disease Progression Forecasting](https://arxiv.org/abs/2511.04789)
*Xiaoda Wang,Yuji Zhao,Kaiqiao Han,Xiao Luo,Sanne van Rooij,Jennifer Stevens,Lifang He,Liang Zhan,Yizhou Sun,Wei Wang,Carl Yang*

Main category: cs.LG

TL;DR: 提出了一种新的用于连续、个体化PD进展预测的框架CNODE（条件神经ODE）。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常采用循环神经网络和Transformer架构，这些架构依赖于离散的、规则采样的数据，同时难以处理PD队列中不规则和稀疏的磁共振成像（MRI）。此外，这些方法难以捕捉个体异质性，包括疾病发作、进展速度和症状严重程度的变化，这是PD的标志。

Method: 使用神经ODE模型将形态脑变化建模为连续时间过程。此外，我们共同学习患者特定的初始时间和进展速度，以将个体轨迹对齐到共享的进展轨迹中。

Result: 在帕金森进展标记物倡议（PPMI）数据集上验证了CNODE。实验结果表明，我们的方法在预测纵向PD进展方面优于最先进的基线方法。

Conclusion: CNODE在预测纵向PD进展方面优于现有方法，为PD的个体化预测提供了新的思路。

Abstract: Parkinson's disease (PD) shows heterogeneous, evolving brain-morphometry
patterns. Modeling these longitudinal trajectories enables mechanistic insight,
treatment development, and individualized 'digital-twin' forecasting. However,
existing methods usually adopt recurrent neural networks and transformer
architectures, which rely on discrete, regularly sampled data while struggling
to handle irregular and sparse magnetic resonance imaging (MRI) in PD cohorts.
Moreover, these methods have difficulty capturing individual heterogeneity
including variations in disease onset, progression rate, and symptom severity,
which is a hallmark of PD. To address these challenges, we propose CNODE
(Conditional Neural ODE), a novel framework for continuous, individualized PD
progression forecasting. The core of CNODE is to model morphological brain
changes as continuous temporal processes using a neural ODE model. In addition,
we jointly learn patient-specific initial time and progress speed to align
individual trajectories into a shared progression trajectory. We validate CNODE
on the Parkinson's Progression Markers Initiative (PPMI) dataset. Experimental
results show that our method outperforms state-of-the-art baselines in
forecasting longitudinal PD progression.

</details>
