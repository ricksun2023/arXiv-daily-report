{"id": "2511.21160", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.21160", "abs": "https://arxiv.org/abs/2511.21160", "authors": ["Wu Sai", "Xia Ruichen", "Yang Dingyu", "Wang Rui", "Lai Huihang", "Guan Jiarui", "Bai Jiameng", "Zhang Dongxiang", "Tang Xiu", "Xie Zhongle", "Lu Peng", "Chen Gang"], "title": "MorphingDB: A Task-Centric AI-Native DBMS for Model Management and Inference", "comment": null, "summary": "The increasing demand for deep neural inference within database environments has driven the emergence of AI-native DBMSs. However, existing solutions either rely on model-centric designs requiring developers to manually select, configure, and maintain models, resulting in high development overhead, or adopt task-centric AutoML approaches with high computational costs and poor DBMS integration. We present MorphingDB, a task-centric AI-native DBMS that automates model storage, selection, and inference within PostgreSQL. To enable flexible, I/O-efficient storage of deep learning models, we first introduce specialized schemas and multi-dimensional tensor data types to support BLOB-based all-in-one and decoupled model storage. Then we design a transfer learning framework for model selection in two phases, which builds a transferability subspace via offline embedding of historical tasks and employs online projection through feature-aware mapping for real-time tasks. To further optimize inference throughput, we propose pre-embedding with vectoring sharing to eliminate redundant computations and DAG-based batch pipelines with cost-aware scheduling to minimize the inference time. Implemented as a PostgreSQL extension with LibTorch, MorphingDB outperforms AI-native DBMSs (EvaDB, Madlib, GaussML) and AutoML platforms (AutoGluon, AutoKeras, AutoSklearn) across nine public datasets, encompassing series, NLP, and image tasks. Our evaluation demonstrates a robust balance among accuracy, resource consumption, and time cost in model selection and significant gains in throughput and resource efficiency.", "AI": {"tldr": "MorphingDB\u662f\u4e00\u4e2aAI\u539f\u751f\u6570\u636e\u5e93\u7ba1\u7406\u7cfb\u7edf\uff0c\u5b83\u81ea\u52a8\u5316\u4e86PostgreSQL\u4e2d\u7684\u6a21\u578b\u5b58\u50a8\u3001\u9009\u62e9\u548c\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u9700\u8981\u624b\u52a8\u9009\u62e9\u3001\u914d\u7f6e\u548c\u7ef4\u62a4\u6a21\u578b\uff0c\u5bfc\u81f4\u5f00\u53d1\u5f00\u9500\u9ad8\uff0c\u6216\u8005\u91c7\u7528\u4ee5\u4efb\u52a1\u4e3a\u4e2d\u5fc3\u7684AutoML\u65b9\u6cd5\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\uff0cDBMS\u96c6\u6210\u6027\u5dee\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51faMorphingDB\uff0c\u4e00\u4e2a\u4ee5\u4efb\u52a1\u4e3a\u4e2d\u5fc3\u7684AI\u539f\u751f\u6570\u636e\u5e93\u7ba1\u7406\u7cfb\u7edf\uff0c\u5b83\u91c7\u7528\u4e13\u95e8\u7684\u6a21\u5f0f\u548c\u591a\u7ef4\u5f20\u91cf\u6570\u636e\u7c7b\u578b\u6765\u652f\u6301\u57fa\u4e8eBLOB\u7684\u4e00\u4f53\u5f0f\u548c\u89e3\u8026\u6a21\u578b\u5b58\u50a8\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u7ebf\u5d4c\u5165\u5386\u53f2\u4efb\u52a1\u6784\u5efa\u53ef\u8fc1\u79fb\u5b50\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u7279\u5f81\u611f\u77e5\u6620\u5c04\u8fdb\u884c\u5728\u7ebf\u6295\u5f71\uff0c\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u4efb\u52a1\u7684\u6a21\u578b\u9009\u62e9\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u5e26\u6709\u5411\u91cf\u5171\u4eab\u7684\u9884\u5d4c\u5165\u6765\u6d88\u9664\u5197\u4f59\u8ba1\u7b97\uff0c\u4ee5\u53ca\u5e26\u6709\u6210\u672c\u611f\u77e5\u8c03\u5ea6\u7684\u57fa\u4e8eDAG\u7684\u6279\u5904\u7406\u7ba1\u9053\uff0c\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\u3002", "result": "MorphingDB\u5728\u4e5d\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u4f18\u4e8eAI\u539f\u751f\u6570\u636e\u5e93\u7ba1\u7406\u7cfb\u7edf(EvaDB, Madlib, GaussML)\u548cAutoML\u5e73\u53f0(AutoGluon, AutoKeras, AutoSklearn)\uff0c\u6db5\u76d6\u5e8f\u5217\u3001NLP\u548c\u56fe\u50cf\u4efb\u52a1\u3002", "conclusion": "MorphingDB\u5728\u6a21\u578b\u9009\u62e9\u4e2d\u5b9e\u73b0\u4e86\u51c6\u786e\u6027\u3001\u8d44\u6e90\u6d88\u8017\u548c\u65f6\u95f4\u6210\u672c\u4e4b\u95f4\u7684\u7a33\u5065\u5e73\u8861\uff0c\u5e76\u5728\u541e\u5410\u91cf\u548c\u8d44\u6e90\u6548\u7387\u65b9\u9762\u5b9e\u73b0\u4e86\u663e\u8457\u63d0\u9ad8\u3002"}}
{"id": "2511.21307", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.21307", "abs": "https://arxiv.org/abs/2511.21307", "authors": ["Xinyi Zhang", "Liang Liang", "Anastasia Ailamaki", "Jianliang Xu"], "title": "HIRE: A Hybrid Learned Index for Robust and Efficient Performance under Mixed Workloads", "comment": "Accepted to SIGMOD 2026. This is the extended technical report", "summary": "Indexes are critical for efficient data retrieval and updates in modern databases. Recent advances in machine learning have led to the development of learned indexes, which model the cumulative distribution function of data to predict search positions and accelerate query processing. While learned indexes substantially outperform traditional structures for point lookups, they often suffer from high tail latency, suboptimal range query performance, and inconsistent effectiveness across diverse workloads. To address these challenges, this paper proposes HIRE, a hybrid in-memory index structure designed to deliver efficient performance consistently. HIRE combines the structural and performance robustness of traditional indexes with the predictive power of model-based prediction to reduce search overhead while maintaining worst-case stability. Specifically, it employs (1) hybrid leaf nodes adaptive to varying data distributions and workloads, (2) model-accelerated internal nodes augmented by log-based updates for efficient updates, (3) a nonblocking, cost-driven recalibration mechanism for dynamic data, and (4) an inter-level optimized bulk-loading algorithm accounting for leaf and internal-node errors. Experimental results on multiple real-world datasets demonstrate that HIRE outperforms both state-of-the-art learned indexes and traditional structures in range-query throughput, tail latency, and overall stability. Compared to state-of-the-art learned indexes and traditional indexes, HIRE achieves up to 41.7$\\times$ higher throughput under mixed workloads, reduces tail latency by up to 98% across varying scenarios.", "AI": {"tldr": "HIRE: a hybrid in-memory index structure, combines learned indexes with traditional indexes to improve performance.", "motivation": "Learned indexes have high tail latency, suboptimal range query performance, and inconsistent effectiveness across diverse workloads.", "method": "HIRE employs hybrid leaf nodes, model-accelerated internal nodes, a nonblocking, cost-driven recalibration mechanism, and an inter-level optimized bulk-loading algorithm.", "result": "HIRE outperforms both state-of-the-art learned indexes and traditional structures in range-query throughput, tail latency, and overall stability. HIRE achieves up to 41.7\u00d7 higher throughput under mixed workloads, reduces tail latency by up to 98%.", "conclusion": "HIRE delivers efficient performance consistently."}}
{"id": "2511.21607", "categories": ["cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21607", "abs": "https://arxiv.org/abs/2511.21607", "authors": ["Zarin Tahia Hossain", "Mostafa Milani"], "title": "Beyond Accuracy: An Empirical Study of Uncertainty Estimation in Imputation", "comment": "To appear in conference proceedings", "summary": "Handling missing data is a central challenge in data-driven analysis. Modern imputation methods not only aim for accurate reconstruction but also differ in how they represent and quantify uncertainty. Yet, the reliability and calibration of these uncertainty estimates remain poorly understood. This paper presents a systematic empirical study of uncertainty in imputation, comparing representative methods from three major families: statistical (MICE, SoftImpute), distribution alignment (OT-Impute), and deep generative (GAIN, MIWAE, TabCSDI). Experiments span multiple datasets, missingness mechanisms (MCAR, MAR, MNAR), and missingness rates. Uncertainty is estimated through three complementary routes: multi-run variability, conditional sampling, and predictive-distribution modeling, and evaluated using calibration curves and the Expected Calibration Error (ECE). Results show that accuracy and calibration are often misaligned: models with high reconstruction accuracy do not necessarily yield reliable uncertainty. We analyze method-specific trade-offs among accuracy, calibration, and runtime, identify stable configurations, and offer guidelines for selecting uncertainty-aware imputers in data cleaning and downstream machine learning pipelines.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30\u4e86\u63d2\u8865\u65b9\u6cd5\u4e2d\u4e0d\u786e\u5b9a\u6027\u7684\u53ef\u9760\u6027\u548c\u6821\u51c6\u6027\u3002", "motivation": "\u6570\u636e\u9a71\u52a8\u5206\u6790\u4e2d\uff0c\u5904\u7406\u7f3a\u5931\u6570\u636e\u662f\u4e00\u4e2a\u6838\u5fc3\u6311\u6218\uff0c\u73b0\u4ee3\u63d2\u8865\u65b9\u6cd5\u65e8\u5728\u51c6\u786e\u91cd\u5efa\u6570\u636e\u5e76\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u4f46\u8fd9\u4e9b\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u53ef\u9760\u6027\u4ecd\u4e0d\u6e05\u695a\u3002", "method": "\u6bd4\u8f83\u4e86\u6765\u81ea\u4e09\u4e2a\u4e3b\u8981\u7cfb\u5217\u7684\u4ee3\u8868\u6027\u65b9\u6cd5\uff1a\u7edf\u8ba1\u65b9\u6cd5\uff08MICE\u3001SoftImpute\uff09\u3001\u5206\u5e03\u5bf9\u9f50\u65b9\u6cd5\uff08OT-Impute\uff09\u548c\u6df1\u5ea6\u751f\u6210\u65b9\u6cd5\uff08GAIN\u3001MIWAE\u3001TabCSDI\uff09\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u3001\u7f3a\u5931\u673a\u5236\u548c\u7f3a\u5931\u7387\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002\u901a\u8fc7\u591a\u8fd0\u884c\u53d8\u5f02\u6027\u3001\u6761\u4ef6\u62bd\u6837\u548c\u9884\u6d4b\u5206\u5e03\u5efa\u6a21\u4e09\u79cd\u4e92\u8865\u9014\u5f84\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u4f7f\u7528\u6821\u51c6\u66f2\u7ebf\u548c\u9884\u671f\u6821\u51c6\u8bef\u5dee\uff08ECE\uff09\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u51c6\u786e\u6027\u548c\u6821\u51c6\u6027\u901a\u5e38\u4e0d\u4e00\u81f4\uff1a\u5177\u6709\u9ad8\u91cd\u5efa\u51c6\u786e\u6027\u7684\u6a21\u578b\u4e0d\u4e00\u5b9a\u4ea7\u751f\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u5206\u6790\u4e86\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u3001\u6821\u51c6\u6027\u548c\u8fd0\u884c\u65f6\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u8bc6\u522b\u4e86\u7a33\u5b9a\u914d\u7f6e\u3002", "conclusion": "\u4e3a\u5728\u6570\u636e\u6e05\u7406\u548c\u4e0b\u6e38\u673a\u5668\u5b66\u4e60\u7ba1\u9053\u4e2d\u9009\u62e9\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u63d2\u8865\u5668\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2511.20677", "categories": ["cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.20677", "abs": "https://arxiv.org/abs/2511.20677", "authors": ["Saleh Almohaimeed", "May Alsofyani", "Saad Almohaimeed", "Mansour Al Ghanim", "Liqiang Wang"], "title": "Prompt Engineering Techniques for Context-dependent Text-to-SQL in Arabic", "comment": "Accepted at IJCNN 2025 (to appear in IEEE/IJCNN proceedings). This arXiv submission corresponds to the camera-ready version", "summary": "In recent years, the task of cross-domain, context-dependent text-to-SQL has received significant attention. Enables users with no prior knowledge of SQL to have a conversation with databases using natural language. However, most of the available datasets and research have been conducted in English, along with some work in Chinese. To this date, no effort has been made to address this task in the Arabic language. In this paper, we introduce Ar-SParC, the first Arabic cross-domain, context-dependent text-to-SQL dataset. The dataset consists of 3,450 sequences of interrelated questions, each sequence containing an average of approximately three questions, which results in a total of 10225 questions along with their corresponding SQL queries. We conducted 40 experiments on the Ar-SParC dataset using two large language models, GPT-3.5-turbo and GPT-4.5-turbo, applying 10 different prompt engineering techniques, including four question representation methods and six in-context learning techniques. Furthermore, we developed a novel approach named GAT corrector, which enhanced the performance across all 40 experiments, yielding an average improvement of 1.9% in execution accuracy (EX) and 1.9% in interaction accuracy (IX) under zero-shot settings, and an average increase of 1.72% EX and 0.92% IX under in-context learning settings. Finally, we conducted an ablation study with two more experiments to explain why the GAT corrector outperformed the previous GAT verifier technique, particularly for the Arabic language.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u963f\u62c9\u4f2f\u8bed\u8de8\u9886\u57df\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u7684text-to-SQL\u6570\u636e\u96c6\uff08Ar-SParC\uff09\uff0c\u5e76\u4f7f\u7528GPT-3.5-turbo\u548cGPT-4.5-turbo\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u7ed3\u5408prompt\u5de5\u7a0b\u6280\u672f\u548cGAT corrector\u65b9\u6cd5\u3002", "motivation": "\u7f3a\u4e4f\u963f\u62c9\u4f2f\u8bed\u7684\u8de8\u9886\u57df\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u7684text-to-SQL\u6570\u636e\u96c6\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b3,450\u4e2a\u95ee\u9898\u5e8f\u5217\u7684Ar-SParC\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u4e8610\u79cdprompt\u5de5\u7a0b\u6280\u672f\uff08\u5305\u62ec\u95ee\u9898\u8868\u793a\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff09\uff0c\u5e76\u63d0\u51fa\u4e86GAT corrector\u65b9\u6cd5\u3002", "result": "GAT corrector\u5728zero-shot\u8bbe\u7f6e\u4e0b\u5e73\u5747\u63d0\u9ad8\u4e861.9%\u7684EX\u548cIX\uff0c\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u8bbe\u7f6e\u4e0b\u5e73\u5747\u63d0\u9ad8\u4e861.72% EX\u548c0.92% IX\u3002", "conclusion": "GAT corrector\u4f18\u4e8e\u4e4b\u524d\u7684GAT verifier\u6280\u672f\uff0c\u7279\u522b\u662f\u5728\u963f\u62c9\u4f2f\u8bed\u4e0a\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u89e3\u91ca\u4e86\u539f\u56e0\u3002"}}
{"id": "2511.20867", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.20867", "abs": "https://arxiv.org/abs/2511.20867", "authors": ["Puneet S. Bagga", "Vivek F. Farias", "Tamar Korkotashvili", "Tianyi Peng", "Yuhang Wu"], "title": "E-GEO: A Testbed for Generative Engine Optimization in E-Commerce", "comment": null, "summary": "With the rise of large language models (LLMs), generative engines are becoming powerful alternatives to traditional search, reshaping retrieval tasks. In e-commerce, for instance, conversational shopping agents now guide consumers to relevant products. This shift has created the need for generative engine optimization (GEO)--improving content visibility and relevance for generative engines. Yet despite its growing importance, current GEO practices are ad hoc, and their impacts remain poorly understood, especially in e-commerce. We address this gap by introducing E-GEO, the first benchmark built specifically for e-commerce GEO. E-GEO contains over 7,000 realistic, multi-sentence consumer product queries paired with relevant listings, capturing rich intent, constraints, preferences, and shopping contexts that existing datasets largely miss. Using this benchmark, we conduct the first large-scale empirical study of e-commerce GEO, evaluating 15 common rewriting heuristics and comparing their empirical performance. To move beyond heuristics, we further formulate GEO as a tractable optimization problem and develop a lightweight iterative prompt-optimization algorithm that can significantly outperform these baselines. Surprisingly, the optimized prompts reveal a stable, domain-agnostic pattern--suggesting the existence of a \"universally effective\" GEO strategy. Our data and code are publicly available at https://github.com/psbagga17/E-GEO.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aE-GEO\u7684\u7535\u5546GEO\u57fa\u51c6\uff0c\u7528\u4e8e\u63d0\u5347\u751f\u6210\u5f0f\u5f15\u64ce\u7684\u5185\u5bb9\u53ef\u89c1\u6027\u548c\u76f8\u5173\u6027\u3002", "motivation": "\u73b0\u6709\u7684GEO\u5b9e\u8df5\u662f\u4e34\u65f6\u7684\uff0c\u5e76\u4e14\u5b83\u4eec\u7684\u5f71\u54cd\u4ecd\u7136\u77e5\u4e4b\u751a\u5c11\uff0c\u5c24\u5176\u662f\u5728\u7535\u5b50\u5546\u52a1\u4e2d\u3002", "method": "\u6784\u5efa\u4e86E-GEO\u57fa\u51c6\uff0c\u5305\u542b\u8d85\u8fc77,000\u4e2a\u771f\u5b9e\u7684\u3001\u591a\u53e5\u6d88\u8d39\u8005\u4ea7\u54c1\u67e5\u8be2\uff0c\u5e76\u8bc4\u4f30\u4e8615\u79cd\u5e38\u89c1\u7684\u91cd\u5199\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u8fed\u4ee3\u63d0\u793a\u4f18\u5316\u7b97\u6cd5\u3002", "result": "\u4f18\u5316\u540e\u7684prompt\u63ed\u793a\u4e86\u4e00\u79cd\u7a33\u5b9a\u7684\u3001\u9886\u57df\u65e0\u5173\u7684\u6a21\u5f0f\uff0c\u8868\u660e\u5b58\u5728\u4e00\u79cd\u201c\u666e\u904d\u6709\u6548\u7684\u201dGEO\u7b56\u7565\u3002", "conclusion": "E-GEO\u57fa\u51c6\u7684\u63d0\u51fa\u548c\u5b9e\u9a8c\u7ed3\u679c\u4e3a\u7535\u5546\u9886\u57df\u7684GEO\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u548c\u65b9\u6cd5\u3002"}}
{"id": "2511.20696", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20696", "abs": "https://arxiv.org/abs/2511.20696", "authors": ["Dan Li", "Hye-Bin Shin", "Yeon-Woo Choi"], "title": "Prototype-Guided Non-Exemplar Continual Learning for Cross-subject EEG Decoding", "comment": "4 pages, 2 figures, 14th IEEE International Winter Conference on Brain-Computer Interface Conference 2026", "summary": "Due to the significant variability in electroencephalogram (EEG) signals across individuals, knowledge acquired from previous subjects is often overwritten as new subjects are introduced in continual EEG decoding task. Current works mainly rely on storing the historical data of seen subjects as a replay buffer to prevent forgetting. However, privacy concerns or memory constraints make keeping such data impractical. Instead, we propose a Prototype-guided Non-Exemplar Continual Learning (ProNECL)framework that preserves prior knowledge without accessing any historical EEG samples. ProNECL constructs class-level prototypes to summarize discriminative representations from each subject and incrementally aligns new feature spaces with the global prototype memory through cross-subject feature alignment and knowledge distillation. Validated on the BCI Competition IV 2a and 2b datasets, our framework effectively balances knowledge retention and adaptability, achieving superior performance in cross-subject continual EEG decoding tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aProNECL\u7684\u975e\u793a\u4f8b\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u8111\u7535\u4fe1\u53f7\u89e3\u7801\uff0c\u8be5\u6846\u67b6\u5728\u4e0d\u8bbf\u95ee\u5386\u53f2\u8111\u7535\u6837\u672c\u7684\u60c5\u51b5\u4e0b\u4fdd\u7559\u5148\u9a8c\u77e5\u8bc6\u3002", "motivation": "\u8111\u7535\u4fe1\u53f7\u5728\u4e2a\u4f53\u95f4\u5dee\u5f02\u663e\u8457\uff0c\u6301\u7eed\u8111\u7535\u89e3\u7801\u4efb\u52a1\u4e2d\uff0c\u5148\u524d\u53d7\u8bd5\u8005\u7684\u77e5\u8bc6\u5bb9\u6613\u88ab\u65b0\u53d7\u8bd5\u8005\u8986\u76d6\u3002\u4ee5\u5f80\u65b9\u6cd5\u4f9d\u8d56\u5b58\u50a8\u5386\u53f2\u6570\u636e\uff0c\u4f46\u5b58\u5728\u9690\u79c1\u548c\u5185\u5b58\u9650\u5236\u3002", "method": "\u6784\u5efa\u7c7b\u522b\u7ea7\u539f\u578b\u6765\u6982\u62ec\u6bcf\u4e2a\u53d7\u8bd5\u8005\u7684\u5224\u522b\u6027\u8868\u5f81\uff0c\u5e76\u901a\u8fc7\u8de8\u53d7\u8bd5\u8005\u7279\u5f81\u5bf9\u9f50\u548c\u77e5\u8bc6\u84b8\u998f\uff0c\u9010\u6b65\u5c06\u65b0\u7684\u7279\u5f81\u7a7a\u95f4\u4e0e\u5168\u5c40\u539f\u578b\u8bb0\u5fc6\u5bf9\u9f50\u3002", "result": "\u5728BCI Competition IV 2a\u548c2b\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u8be5\u6846\u67b6\u6709\u6548\u5730\u5e73\u8861\u4e86\u77e5\u8bc6\u4fdd\u7559\u548c\u9002\u5e94\u6027\uff0c\u5728\u8de8\u53d7\u8bd5\u8005\u6301\u7eed\u8111\u7535\u89e3\u7801\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "ProNECL\u6846\u67b6\u5728\u8de8\u53d7\u8bd5\u8005\u6301\u7eed\u8111\u7535\u89e3\u7801\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u77e5\u8bc6\u9057\u5fd8\u548c\u6570\u636e\u9690\u79c1\u95ee\u9898\u3002"}}
