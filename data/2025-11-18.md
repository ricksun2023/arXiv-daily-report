<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 43]
- [cs.CV](#cs.CV) [Total: 47]
- [cs.AI](#cs.AI) [Total: 42]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.IR](#cs.IR) [Total: 21]
- [cs.LG](#cs.LG) [Total: 44]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [TimeStampEval: A Simple LLM Eval and a Little Fuzzy Matching Trick to Improve Search Accuracy](https://arxiv.org/abs/2511.11594)
*James McCammon*

Main category: cs.CL

TL;DR: 该论文介绍了一个名为TimeStampEval的基准，用于从长文本记录中检索精确到毫秒的时间戳，主要针对非完全相同的引用的检索问题。


<details>
  <summary>Details</summary>
Motivation: 传统模糊匹配在搜索语义相同但句法不同的引用时效果不佳，尤其是在对齐官方书面记录和语音转文本记录时。该研究的动机是创建一个自动化的长篇播客，将国会记录片段合成为AI主持的叙述。

Method: 该研究提出了一种简单的两阶段方法：RapidFuzz预过滤，然后由LLM验证短片段，称为“Assisted Fuzzy”方法。

Result: 实验结果表明，Prompt设计比模型选择更重要；适度的推理预算可以显著提高准确率；所提出的“Assisted Fuzzy”方法可以将模糊匹配准确率提高高达50个百分点，同时降低延迟和成本。

Conclusion: 该研究提出的方法在不同长度、词汇和领域的文本记录上都表现出鲁棒性，对不存在的目标保持95-100%的拒绝准确率。

Abstract: Traditional fuzzy matching often fails when searching for quotes that are semantically identical but syntactically different across documents-a common issue when aligning official written records with speech-to-text transcripts. We introduce TimeStampEval, a benchmark for retrieving precise millisecond timestamps from long transcripts given non-verbatim quotes. Our simple two-stage method dramatically improves retrieval accuracy while cutting inference costs by over 90%. The motivating use case is an automated long-form podcast that assembles Congressional Record clips into AI-hosted narration. The technical challenge: given a sentence-timestamped transcript and a target quote that may differ due to transcription or editorial drift, return exact start and end boundaries. Standard algorithms handle verbatim text but break under fuzzier variants. Evaluating six modern LLMs on a 2,800-sentence (120k-token) transcript revealed four key findings. (1) Prompt design matters more than model choice: placing the query before the transcript and using compact formatting improved accuracy by 3-20 points while reducing token count by 30-40%. (2) Off-by-one errors form a distinct category, showing models understand the task but misplace boundaries. (3) A modest reasoning budget (600-850 tokens) raises accuracy from 37% to 77% for weak setups and to above 90% for strong ones. (4) Our "Assisted Fuzzy" approach-RapidFuzz pre-filtering followed by LLM verification on short snippets-improves fuzzy match accuracy by up to 50 points while halving latency and reducing cost per correct result by up to 96%. Extended tests on ten transcripts (50k-900k tokens, 1989-2025) confirm robustness to transcript length, vocabulary drift, and domain change, maintaining 95-100% rejection accuracy for absent targets.

</details>


### [2] [MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling](https://arxiv.org/abs/2511.11793)
*MiroMind Team,Song Bai,Lidong Bing,Carson Chen,Guanzheng Chen,Yuntao Chen,Zhe Chen,Ziyi Chen,Jifeng Dai,Xuan Dong,Yue Deng,Yunjie Fu,Junqi Ge,Chenxia Han,Tammy Huang,Zhenhang Huang,Jerry Jiao,Shilei Jiang,Tianyu Jiao,Xiaoqi Jian,Lei Lei,Ruilin Li,Ryan Luo,Tiantong Li,Xiang Lin,Ziyuan Liu,Zhiqi Li,Jie Ni,Qiang Ren,Pax Sun,Shiqian Su,Chenxin Tao,Bin Wang,Hellen Wang,Haonan Wang,James Wang,Jin Wang,Jojo Wang,Letian Wang,Shizun Wang,Weizhi Wang,Zixuan Wang,Jinfan Xu,Sen Xing,Chenyu Yang,Hai Ye,Jiaheng Yu,Yue Yu,Muyan Zhong,Tianchen Zhao,Xizhou Zhu,Yanpeng Zhou,Yifan Zhang,Zhi Zhu*

Main category: cs.CL

TL;DR: MiroThinker v1.0是一个开源研究代理，旨在通过交互扩展来提升工具增强推理和信息搜索能力。它通过强化学习实现高效的交互扩展，超越了之前的开源代理，并接近了商业代理的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究代理主要通过扩大模型尺寸或上下文长度来提升性能，但忽略了交互扩展的重要性。MiroThinker旨在探索模型层面的交互扩展，通过训练模型处理更深入、更频繁的agent-环境交互来提升性能。

Method: 通过强化学习训练MiroThinker，使其能够处理更深入、更频繁的agent-环境交互。该模型具有256K的上下文窗口，并且可以执行高达600次的工具调用。

Result: 在四个基准测试中，72B版本的MiroThinker的准确率分别达到81.9%、37.7%、47.1%和55.6%，超过了之前的开源代理，并接近了GPT-4-high等商业代理。

Conclusion: 交互扩展是构建下一代开放研究代理的第三个关键维度，与模型容量和上下文窗口互补。

Abstract: We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.

</details>


### [3] [On the Notion that Language Models Reason](https://arxiv.org/abs/2511.11810)
*Bertram Højer*

Main category: cs.CL

TL;DR: 论文 критикует утверждения о том, что языковые модели демонстрируют рассуждения, поскольку их обучение и обработка информации не соответствуют определениям рассуждений.


<details>
  <summary>Details</summary>
Motivation: Оценка определений рассуждений и того, как ключевые статьи в области обработки естественного языка (NLP) используют это понятие, и утверждение о том, что предоставленные определения не соответствуют тому, как обучаются языковые модели, обрабатывают информацию и генерируют новые токены.

Method: Предполагается, что языковые модели на основе трансформеров реализуют \textit{неявное} ядро Маркова конечного порядка, отображающее контексты в условные распределения токенов. В этом представлении, выводы, похожие на рассуждения, соответствуют статистическим закономерностям и приблизительным статистическим инвариантам в изученном ядре, а не реализации явных логических механизмов.

Result: Выводы, похожие на рассуждения, соответствуют статистическим закономерностям и приблизительным статистическим инвариантам в изученном ядре, а не реализации явных логических механизмов. Это представление иллюстрирует утверждение о том, что языковые модели являются \

Conclusion: Различие имеет основополагающее значение для того, как оценивается эпистемическая неопределенность в языковых моделях. Приглашаем к обсуждению важности того, как описываются вычислительные процессы систем, которые мы строим и анализируем в исследованиях NLP.

Abstract: Language models (LMs) are said to be exhibiting reasoning, but what does this entail? We assess definitions of reasoning and how key papers in the field of natural language processing (NLP) use the notion and argue that the definitions provided are not consistent with how LMs are trained, process information, and generate new tokens. To illustrate this incommensurability we assume the view that transformer-based LMs implement an \textit{implicit} finite-order Markov kernel mapping contexts to conditional token distributions. In this view, reasoning-like outputs correspond to statistical regularities and approximate statistical invariances in the learned kernel rather than the implementation of explicit logical mechanisms. This view is illustrative of the claim that LMs are "statistical pattern matchers"" and not genuine reasoners and provides a perspective that clarifies why reasoning-like outputs arise in LMs without any guarantees of logical consistency. This distinction is fundamental to how epistemic uncertainty is evaluated in LMs. We invite a discussion on the importance of how the computational processes of the systems we build and analyze in NLP research are described.

</details>


### [4] [Scaling Open-Weight Large Language Models for Hydropower Regulatory Information Extraction: A Systematic Analysis](https://arxiv.org/abs/2511.11821)
*Hong-Jun Yoon,Faisal Ashraf,Thomas A. Ruggles,Debjani Singh*

Main category: cs.CL

TL;DR: 评估了七个开放权重模型（0.6B-70B 参数）在水力发电许可文件上的信息提取性能，以提供部署指导。


<details>
  <summary>Details</summary>
Motivation: 利用大型语言模型从监管文件中提取信息需要在性能和计算资源之间进行权衡。

Method: 在水力发电许可文档上评估了七个开放权重模型（0.6B-70B 参数）。

Result: 确定了明显的 14B 参数阈值，验证方法从无效（F1 < 0.15）过渡到可行（F1 = 0.64）。消费者可部署的模型通过适当的验证实现了 64% 的 F1，而较小的模型稳定在 51%。大型模型接近 77% 的 F1，但需要企业基础设施。

Conclusion: 建立了开放权重信息提取在监管环境中的第一个综合资源-性能映射，从而能够进行基于证据的模型选择。这些结果为水力发电合规性提供了直接价值，同时有助于深入了解参数缩放效应，这些效应可以推广到各种信息提取任务中。

Abstract: Information extraction from regulatory documents using large language models presents critical trade-offs between performance and computational resources. We evaluated seven open-weight models (0.6B-70B parameters) on hydropower licensing documentation to provide empirical deployment guidance.
  Our analysis identified a pronounced 14B parameter threshold where validation methods transition from ineffective (F1 $<$ 0.15) to viable (F1 = 0.64). Consumer-deployable models achieve 64\% F1 through appropriate validation, while smaller models plateau at 51\%. Large-scale models approach 77\% F1 but require enterprise infrastructure.
  We identified systematic hallucination patterns where perfect recall indicates extraction failure rather than success in smaller models. Our findings establish the first comprehensive resource-performance mapping for open-weight information extraction in regulatory contexts, enabling evidence-based model selection.
  These results provide immediate value for hydropower compliance while contributing insights into parameter scaling effects that generalize across information extraction tasks.

</details>


### [5] [Towards Autoformalization of LLM-generated Outputs for Requirement Verification](https://arxiv.org/abs/2511.11829)
*Mihir Gupte,Ramesh S*

Main category: cs.CL

TL;DR: 本文探讨了使用基于LLM的自动形式化工具来验证LLM生成的输出，以确保其准确性。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏验证LLM生成输出准确性的正式方法。

Method: 使用基于LLM的自动形式化工具来验证LLM生成的输出，并进行了两个实验。

Result: 实验表明，该自动形式化工具可以识别逻辑等价的NL需求和LLM生成输出中的逻辑不一致。

Conclusion: 自动形式化在确保LLM生成输出的保真度和逻辑一致性方面具有巨大潜力。

Abstract: Autoformalization, the process of translating informal statements into formal logic, has gained renewed interest with the emergence of powerful Large Language Models (LLMs). While LLMs show promise in generating structured outputs from natural language (NL), such as Gherkin Scenarios from NL feature requirements, there's currently no formal method to verify if these outputs are accurate. This paper takes a preliminary step toward addressing this gap by exploring the use of a simple LLM-based autoformalizer to verify LLM-generated outputs against a small set of natural language requirements. We conducted two distinct experiments. In the first one, the autoformalizer successfully identified that two differently-worded NL requirements were logically equivalent, demonstrating the pipeline's potential for consistency checks. In the second, the autoformalizer was used to identify a logical inconsistency between a given NL requirement and an LLM-generated output, highlighting its utility as a formal verification tool. Our findings, while limited, suggest that autoformalization holds significant potential for ensuring the fidelity and logical consistency of LLM-generated outputs, laying a crucial foundation for future, more extensive studies into this novel application.

</details>


### [6] [Three Stage Narrative Analysis; Plot-Sentiment Breakdown, Structure Learning and Concept Detection](https://arxiv.org/abs/2511.11857)
*Taimur Khan,Ramoza Ahsan,Mohib Hameed*

Main category: cs.CL

TL;DR: 本文提出了一种分析电影剧本情感弧的框架，并对相关人物的情感进行扩展分析。


<details>
  <summary>Details</summary>
Motivation: 大量叙事数据的出现，需要自动语义分析和计算学习，而不是手动分析方法。故事理解和分析一直是自然语言理解中具有挑战性的领域。

Method: 该框架使用基于字典的情感分析，应用使用LabMTsimple storylab模块构建的自定义词典。自定义词典基于NRC-VAD数据集中的Valence, Arousal, and Dominance分数。此外，该框架通过使用Wards层次聚类技术对相似的情感图进行聚类，从而推进分析。

Result: 在电影数据集上的实验评估表明，结果分析有助于消费者和读者选择叙事或故事。

Conclusion: 该框架能够提取通过叙事传达的高级和低级概念，为消费者和读者在选择叙事作品时提供帮助。

Abstract: Story understanding and analysis have long been challenging areas within Natural Language Understanding. Automated narrative analysis requires deep computational semantic representations along with syntactic processing. Moreover, the large volume of narrative data demands automated semantic analysis and computational learning rather than manual analytical approaches. In this paper, we propose a framework that analyzes the sentiment arcs of movie scripts and performs extended analysis related to the context of the characters involved. The framework enables the extraction of high-level and low-level concepts conveyed through the narrative. Using dictionary-based sentiment analysis, our approach applies a custom lexicon built with the LabMTsimple storylab module. The custom lexicon is based on the Valence, Arousal, and Dominance scores from the NRC-VAD dataset. Furthermore, the framework advances the analysis by clustering similar sentiment plots using Wards hierarchical clustering technique. Experimental evaluation on a movie dataset shows that the resulting analysis is helpful to consumers and readers when selecting a narrative or story.

</details>


### [7] [Identifying Imaging Follow-Up in Radiology Reports: A Comparative Analysis of Traditional ML and LLM Approaches](https://arxiv.org/abs/2511.11867)
*Namu Park,Giridhar Kaushik Ramachandran,Kevin Lybarger,Fei Xia,Ozlem Uzuner,Meliha Yetisgen,Martin Gunn*

Main category: cs.CL

TL;DR: 本研究构建了一个包含6393份放射报告的标注语料库，用于评估大型语言模型在放射学任务中的性能，特别是随访依从性检测。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏用于严格评估大型语言模型在放射学任务中性能的领域特定数据集。

Method: 比较了传统机器学习分类器（LR、SVM、Longformer）和大型语言模型（Llama3-8B-Instruct、GPT-4o、GPT-OSS-20B）在随访依从性检测任务中的性能，并通过优化prompt提高LLM的推理准确性。

Result: GPT-4o（Advanced）取得了最佳性能（F1 = 0.832），其次是GPT-OSS-20B（Advanced; F1 = 0.828），LR和SVM也表现出色（F1 = 0.776和0.775）。

Conclusion: 大型语言模型通过prompt优化可以接近人类水平，但可解释且资源高效的模型仍然具有重要的基线价值。

Abstract: Large language models (LLMs) have shown considerable promise in clinical natural language processing, yet few domain-specific datasets exist to rigorously evaluate their performance on radiology tasks. In this work, we introduce an annotated corpus of 6,393 radiology reports from 586 patients, each labeled for follow-up imaging status, to support the development and benchmarking of follow-up adherence detection systems. Using this corpus, we systematically compared traditional machine-learning classifiers, including logistic regression (LR), support vector machines (SVM), Longformer, and a fully fine-tuned Llama3-8B-Instruct, with recent generative LLMs. To evaluate generative LLMs, we tested GPT-4o and the open-source GPT-OSS-20B under two configurations: a baseline (Base) and a task-optimized (Advanced) setting that focused inputs on metadata, recommendation sentences, and their surrounding context. A refined prompt for GPT-OSS-20B further improved reasoning accuracy. Performance was assessed using precision, recall, and F1 scores with 95% confidence intervals estimated via non-parametric bootstrapping. Inter-annotator agreement was high (F1 = 0.846). GPT-4o (Advanced) achieved the best performance (F1 = 0.832), followed closely by GPT-OSS-20B (Advanced; F1 = 0.828). LR and SVM also performed strongly (F1 = 0.776 and 0.775), underscoring that while LLMs approach human-level agreement through prompt optimization, interpretable and resource-efficient models remain valuable baselines.

</details>


### [8] [MedPT: A Massive Medical Question Answering Dataset for Brazilian-Portuguese Speakers](https://arxiv.org/abs/2511.11878)
*Fernanda Bufon Färber,Iago Alves Brito,Julia Soares Dollis,Pedro Schindler Freire Brasil Ribeiro,Rafael Teixeira Sousa,Arlindo Rodrigues Galvão Filho*

Main category: cs.CL

TL;DR: MedPT: A new large-scale Brazilian Portuguese corpus for healthcare LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs in healthcare are focused on high-resource languages, and simple translation fails to capture unique clinical and cultural nuances.

Method: A large-scale, real-world corpus for Brazilian Portuguese was created, comprising 384,095 question-answer pairs from patient-doctor interactions. The dataset underwent a meticulous multi-stage curation protocol and was further augmented via LLM-driven annotation, classifying questions into seven semantic types to capture user intent.

Result: The dataset has thematic breadth (3,200 topics) and unique linguistic properties. Fine-tuning a 1.7B parameter model achieves an outstanding 94% F1-score on a 20-class medical specialty routing task.

Conclusion: The authors publicly release MedPT to foster the development of more equitable, accurate, and culturally-aware medical technologies for the Portuguese-speaking world.

Abstract: While large language models (LLMs) show transformative potential in healthcare, their development remains focused on high-resource languages, creating a critical barrier for others as simple translation fails to capture unique clinical and cultural nuances, such as endemic diseases. To address this, we introduce MedPT, the first large-scale, real-world corpus for Brazilian Portuguese, comprising 384,095 authentic question-answer pairs from patient-doctor interactions. The dataset underwent a meticulous multi-stage curation protocol, using a hybrid quantitative-qualitative analysis to filter noise and contextually enrich thousands of ambiguous queries. We further augmented the corpus via LLM-driven annotation, classifying questions into seven semantic types to capture user intent. Our analysis reveals its thematic breadth (3,200 topics) and unique linguistic properties, like the natural asymmetry in patient-doctor communication. To validate its utility, we benchmark a medical specialty routing task: fine-tuning a 1.7B parameter model achieves an outstanding 94\% F1-score on a 20-class setup. Furthermore, our qualitative error analysis shows misclassifications are not random but reflect genuine clinical ambiguities (e.g., between comorbid conditions), proving the dataset's deep semantic richness. We publicly release MedPT to foster the development of more equitable, accurate, and culturally-aware medical technologies for the Portuguese-speaking world.

</details>


### [9] [ClinStructor: AI-Powered Structuring of Unstructured Clinical Texts](https://arxiv.org/abs/2511.11883)
*Karthikeyan K,Raghuveer Thirukovalluru,David Carlson*

Main category: cs.CL

TL;DR: 本研究介绍了一种名为ClinStructor的流程，它利用大型语言模型（LLM）将临床自由文本转换为结构化的、特定于任务的问答对，然后再进行预测建模。


<details>
  <summary>Details</summary>
Motivation: 临床笔记包含有价值的、上下文丰富的信息，但其非结构化格式会带来一些挑战，包括无意的偏见（例如，性别或种族偏见），以及跨临床环境的泛化能力差（例如，在一个EHR系统上训练的模型在另一个系统上可能表现不佳，因为格式不同）以及可解释性差。

Method: 我们提出了ClinStructor，一个利用大型语言模型（LLM）将临床自由文本转换为结构化的、特定于任务的问答对的流程，然后再进行预测建模。

Result: 与直接微调相比，我们的方法大大提高了透明度和可控性，并且仅导致预测性能略有下降（AUC下降2-3%）。

Conclusion: ClinStructor为在临床环境中构建可靠、可解释和可推广的机器学习模型奠定了坚实的基础。

Abstract: Clinical notes contain valuable, context-rich information, but their unstructured format introduces several challenges, including unintended biases (e.g., gender or racial bias), and poor generalization across clinical settings (e.g., models trained on one EHR system may perform poorly on another due to format differences) and poor interpretability. To address these issues, we present ClinStructor, a pipeline that leverages large language models (LLMs) to convert clinical free-text into structured, task-specific question-answer pairs prior to predictive modeling. Our method substantially enhances transparency and controllability and only leads to a modest reduction in predictive performance (a 2-3% drop in AUC), compared to direct fine-tuning, on the ICU mortality prediction task. ClinStructor lays a strong foundation for building reliable, interpretable, and generalizable machine learning models in clinical environments.

</details>


### [10] [Context-Emotion Aware Therapeutic Dialogue Generation: A Multi-component Reinforcement Learning Approach to Language Models for Mental Health Support](https://arxiv.org/abs/2511.11884)
*Eric Hua Qing Zhang,Julia Ive*

Main category: cs.CL

TL;DR: 本研究探索了使用监督微调 (SFT) 和强化学习 (RL) 技术来增强 GPT-2 的治疗对话生成能力。


<details>
  <summary>Details</summary>
Motivation: 精神健康疾病造成了巨大的全球社会经济负担，而 COVID-19 进一步加剧了可及性挑战，并推动了对远程医疗精神健康支持的需求。

Method: 该方法重构了输入格式，以实现同时处理上下文信息和情绪状态以及用户输入，采用了一种多组件奖励函数，该函数将模型输出与专业治疗师的反应和带注释的情绪对齐。

Result: 结果表明，通过强化学习在多个评估指标上优于基线 GPT-2：BLEU (0.0111)、ROUGE-1 (0.1397)、ROUGE-2 (0.0213)、ROUGE-L (0.1317) 和 METEOR (0.0581)。LLM 评估证实了高度的上下文相关性和专业性，而强化学习实现了 99.34% 的情绪准确率，而基线 GPT-2 为 66.96%。

Conclusion: 这些发现证明了强化学习在开发治疗对话系统方面的有效性，这些系统可以作为治疗师的有价值的辅助工具，同时保持必要的人工临床监督。

Abstract: Mental health illness represents a substantial global socioeconomic burden, with COVID-19 further exacerbating accessibility challenges and driving increased demand for telehealth mental health support. While large language models (LLMs) offer promising solutions through 24/7 availability and non-judgmental interactions, pre-trained models often lack the contextual and emotional awareness necessary for appropriate therapeutic responses. This paper investigated the application of supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance GPT-2's capacity for therapeutic dialogue generation. The methodology restructured input formats to enable simultaneous processing of contextual information and emotional states alongside user input, employing a multi-component reward function that aligned model outputs with professional therapist responses and annotated emotions. Results demonstrated improvements through reinforcement learning over baseline GPT-2 across multiple evaluation metrics: BLEU (0.0111), ROUGE-1 (0.1397), ROUGE-2 (0.0213), ROUGE-L (0.1317), and METEOR (0.0581). LLM evaluation confirmed high contextual relevance and professionalism, while reinforcement learning achieved 99.34% emotion accuracy compared to 66.96% for baseline GPT-2. These findings demonstrate reinforcement learning's effectiveness in developing therapeutic dialogue systems that can serve as valuable assistive tools for therapists while maintaining essential human clinical oversight.

</details>


### [11] [Additive Large Language Models for Semi-Structured Text](https://arxiv.org/abs/2511.11922)
*Karthikeyan K,Raghuveer Thirukovalluru,David Carlson*

Main category: cs.CL

TL;DR: CALM: An interpretable framework for clinical text classification.


<details>
  <summary>Details</summary>
Motivation: Lack of interpretability in LLM predictions hinders clinical adoption.

Method: Predicts outcomes as the additive sum of each component's contribution.

Result: Achieves comparable performance to conventional LLM classifiers while improving trust and revealing clinically meaningful patterns.

Conclusion: CALM improves trust, supports quality-assurance checks, and reveals clinically meaningful patterns.

Abstract: Large Language Models have advanced clinical text classification, but their opaque predictions remain a critical barrier to practical adoption in research and clinical settings where investigators and physicians need to understand which parts of a patient's record drive risk signals. To address this challenge, we introduce \textbf{CALM}, short for \textbf{Classification with Additive Large Language Models}, an interpretable framework for semi-structured text where inputs are composed of semantically meaningful components, such as sections of an admission note or question-answer fields from an intake form. CALM predicts outcomes as the additive sum of each component's contribution, making these contributions part of the forward computation itself and enabling faithful explanations at both the patient and population level. The additive structure also enables clear visualizations, such as component-level risk curves similar to those used in generalized additive models, making the learned relationships easier to inspect and communicate. Although CALM expects semi-structured inputs, many clinical documents already have this form, and similar structure can often be automatically extracted from free-text notes. CALM achieves performance comparable to conventional LLM classifiers while improving trust, supporting quality-assurance checks, and revealing clinically meaningful patterns during model development and auditing.

</details>


### [12] [InData: Towards Secure Multi-Step, Tool-Based Data Analysis](https://arxiv.org/abs/2511.11933)
*Karthikeyan K,Raghuveer Thirukovalluru,Bhuwan Dhingra,David Edwin Carlson*

Main category: cs.CL

TL;DR: 大型语言模型（LLM）在数据分析中直接生成和执行代码存在安全风险。本研究提出一种安全替代方案：限制LLM直接生成代码和访问数据，而是通过预定义的、安全的工具集进行交互。为此，作者们构建了一个名为Indirect Data Engagement (InData) 的数据集，用于评估LLM基于工具的多步骤推理能力。实验结果表明，现有的LLM在复杂任务上仍然缺乏强大的多步骤工具使用能力。


<details>
  <summary>Details</summary>
Motivation: 为了解决大型语言模型直接生成代码和访问敏感数据时存在的安全风险。

Method: 构建了一个名为Indirect Data Engagement (InData) 的数据集，用于评估LLM基于工具的多步骤推理能力。该数据集包含三个难度级别（简单、中等和困难）的数据分析问题，捕捉了不断增加的推理复杂性。在InData上对15个开源LLM进行了基准测试。

Result: 大型模型（如gpt-oss-120b）在简单任务上实现了较高的准确率（97.3%），但在困难任务上性能急剧下降（69.6%）。

Conclusion: 目前的大型语言模型仍然缺乏强大的多步骤工具推理能力。发布了数据集和代码，以促进具有更强大的多步骤工具使用能力的LLM的开发和评估。

Abstract: Large language model agents for data analysis typically generate and execute code directly on databases. However, when applied to sensitive data, this approach poses significant security risks. To address this issue, we propose a security-motivated alternative: restrict LLMs from direct code generation and data access, and require them to interact with data exclusively through a predefined set of secure, verified tools. Although recent tool-use benchmarks exist, they primarily target tool selection and simple execution rather than the compositional, multi-step reasoning needed for complex data analysis. To reduce this gap, we introduce Indirect Data Engagement (InData), a dataset designed to assess LLMs' multi-step tool-based reasoning ability. InData includes data analysis questions at three difficulty levels--Easy, Medium, and Hard--capturing increasing reasoning complexity. We benchmark 15 open-source LLMs on InData and find that while large models (e.g., gpt-oss-120b) achieve high accuracy on Easy tasks (97.3%), performance drops sharply on Hard tasks (69.6%). These results show that current LLMs still lack robust multi-step tool-based reasoning ability. With InData, we take a step toward enabling the development and evaluation of LLMs with stronger multi-step tool-use capabilities. We will publicly release the dataset and code.

</details>


### [13] [Improving LLM's Attachment to External Knowledge In Dialogue Generation Tasks Through Entity Anonymization](https://arxiv.org/abs/2511.11946)
*Hadi Sheikhi,Chenyang Huang,Osmar R. Zaïane*

Main category: cs.CL

TL;DR: 大型语言模型在知识图谱对话生成中依赖内部知识，导致与外部知识图谱脱节。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型在知识图谱对话生成中利用外部知识的能力。

Method: 1. 提出LLM-KAT，用于评估生成回复中的知识连接。2. 提出实体匿名化技术，鼓励大型语言模型更好地利用外部知识。

Result: 在OpenDialKG数据集上的实验表明，该方法提高了大型语言模型对外部知识的连接。

Conclusion: 该研究表明，通过实体匿名化技术，可以提高大型语言模型在知识图谱对话生成中对外部知识的利用率。

Abstract: Knowledge graph-based dialogue generation (KG-DG) is a challenging task requiring models to effectively incorporate external knowledge into conversational responses. While large language models (LLMs) have achieved impressive results across various NLP tasks, their ability to utilize external knowledge in KG-DG remains under-explored. We observe that LLMs often rely on internal knowledge, leading to detachment from provided knowledge graphs, even when they are given a flawlessly retrieved knowledge graph. First, we introduce LLM-KAT, an evaluation procedure for measuring knowledge attachment in generated responses. Second, we propose a simple yet effective entity anonymization technique to encourage LLMs to better leverage external knowledge. Experiments on the OpenDialKG dataset demonstrate that our approach improves LLMs' attachment on external knowledge.

</details>


### [14] [On the Entropy Calibration of Language Models](https://arxiv.org/abs/2511.11966)
*Steven Cao,Gregory Valiant,Percy Liang*

Main category: cs.CL

TL;DR: 研究熵校准问题，即语言模型在生成过程中的熵是否与其在人工文本上的对数损失相匹配。过去的研究发现，模型存在校准误差，随着生成长度的增加，每步的熵增加（文本质量下降）。


<details>
  <summary>Details</summary>
Motivation: 自回归模型中存在误差累积的基本问题，标准的解决方案是截断分布，但这会以牺牲多样性为代价来提高文本质量。本文旨在探究失准是否可能随着规模的扩大而改善，以及在理论上是否有可能在不进行权衡的情况下进行校准。

Method: 首先，研究了一个简化的理论环境，以描述相对于数据集大小的失准的缩放行为。然后，在参数范围从 0.5B 到 70B 的语言模型中，实证地测量了失准。

Result: 发现缩放行为与简化设置的预测相似：文本的拟合缩放指数接近于 0，这意味着较大的模型以与较小的模型相似的速率累积误差。即使较大的模型质量更高，但我们仍以与较小的模型相似的截断量从较大的模型中进行采样，这可以解释这种缩放（或缺乏缩放）。

Conclusion: 从理论上讲，如果我们可以访问一个黑盒来拟合模型以预测文本的未来熵，那么降低熵的同时保持对数损失是可能的。

Abstract: We study the problem of entropy calibration, which asks whether a language model's entropy over generations matches its log loss on human text. Past work found that models are miscalibrated, with entropy per step increasing (and text quality decreasing) as generations grow longer. This error accumulation is a fundamental problem in autoregressive models, and the standard solution is to truncate the distribution, which improves text quality at the cost of diversity. In this paper, we ask: is miscalibration likely to improve with scale, and is it theoretically possible to calibrate without tradeoffs? To build intuition, we first study a simplified theoretical setting to characterize the scaling behavior of miscalibration with respect to dataset size. We find that the scaling behavior depends on the power law exponent of the data distribution -- in particular, for a power law exponent close to 1, the scaling exponent is close to 0, meaning that miscalibration improves very slowly with scale. Next, we measure miscalibration empirically in language models ranging from 0.5B to 70B parameters. We find that the observed scaling behavior is similar to what is predicted by the simplified setting: our fitted scaling exponents for text are close to 0, meaning that larger models accumulate error at a similar rate as smaller ones. This scaling (or, lack thereof) provides one explanation for why we sample from larger models with similar amounts of truncation as smaller models, even though the larger models are of higher quality. However, truncation is not a satisfying solution because it comes at the cost of increased log loss. In theory, is it even possible to reduce entropy while preserving log loss? We prove that it is possible, if we assume access to a black box which can fit models to predict the future entropy of text.

</details>


### [15] [A Reasoning Paradigm for Named Entity Recognition](https://arxiv.org/abs/2511.11978)
*Hui Huang,Yanping Chen,Ruizhang Huang,Chuan Lin,Yongbin Qin*

Main category: cs.CL

TL;DR: 这篇论文提出了一种新的NER推理框架，以解决生成式LLM在zero-shot和低资源场景下的泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式LLM在NER任务中依赖语义模式匹配，缺乏显式推理机制，导致性能受限。

Method: 该框架包含三个阶段：CoT生成、CoT调优和推理增强，通过显式推理链提升模型性能。

Result: ReasoningNER在NER任务中表现出强大的认知能力，在zero-shot设置下优于GPT-4 12.3个百分点。

Conclusion: 该研究展示了推理导向信息提取的巨大潜力。

Abstract: Generative LLMs typically improve Named Entity Recognition (NER) performance through instruction tuning. They excel at generating entities by semantic pattern matching but lack an explicit, verifiable reasoning mechanism. This "cognitive shortcutting" leads to suboptimal performance and brittle generalization, especially in zero-shot and lowresource scenarios where reasoning from limited contextual cues is crucial. To address this issue, a reasoning framework is proposed for NER, which shifts the extraction paradigm from implicit pattern matching to explicit reasoning. This framework consists of three stages: Chain of Thought (CoT) generation, CoT tuning, and reasoning enhancement. First, a dataset annotated with NER-oriented CoTs is generated, which contain task-relevant reasoning chains. Then, they are used to tune the NER model to generate coherent rationales before deriving the final answer. Finally, a reasoning enhancement stage is implemented to optimize the reasoning process using a comprehensive reward signal. This stage ensures explicit and verifiable extractions. Experiments show that ReasoningNER demonstrates impressive cognitive ability in the NER task, achieving competitive performance. In zero-shot settings, it achieves state-of-the-art (SOTA) performance, outperforming GPT-4 by 12.3 percentage points on the F1 score. Analytical results also demonstrate its great potential to advance research in reasoningoriented information extraction. Our codes are available at https://github.com/HuiResearch/ReasoningIE.

</details>


### [16] [Critical or Compliant? The Double-Edged Sword of Reasoning in Chain-of-Thought Explanations](https://arxiv.org/abs/2511.12001)
*Eunkyu Park,Wesley Hanwen Deng,Vasudha Varadarajan,Mingxi Yan,Gunhee Kim,Maarten Sap,Motahhare Eslami*

Main category: cs.CL

TL;DR: Chain-of-Thought (CoT) explanations in vision language models (VLMs) can mislead users by fostering confirmation bias, leading to sustained reliance even when reasoning is flawed.


<details>
  <summary>Details</summary>
Motivation: To study the double-edged role of Chain-of-Thought (CoT) explanations in multimodal moral scenarios, where explanations can promote transparency but also foster confirmation bias.

Method: Systematically perturb reasoning chains and manipulate delivery tones in vision language models (VLMs) to analyze reasoning errors and their impact on user trust and error detection.

Result: Users often equate trust with outcome agreement, sustaining reliance even when reasoning is flawed. Confident tones suppress error detection while maintaining reliance, showing that delivery styles can override correctness.

Conclusion: CoT explanations can simultaneously clarify and mislead, underscoring the need for NLP systems to provide explanations that encourage scrutiny and critical thinking rather than blind trust.

Abstract: Explanations are often promoted as tools for transparency, but they can also foster confirmation bias; users may assume reasoning is correct whenever outputs appear acceptable. We study this double-edged role of Chain-of-Thought (CoT) explanations in multimodal moral scenarios by systematically perturbing reasoning chains and manipulating delivery tones. Specifically, we analyze reasoning errors in vision language models (VLMs) and how they impact user trust and the ability to detect errors. Our findings reveal two key effects: (1) users often equate trust with outcome agreement, sustaining reliance even when reasoning is flawed, and (2) the confident tone suppresses error detection while maintaining reliance, showing that delivery styles can override correctness. These results highlight how CoT explanations can simultaneously clarify and mislead, underscoring the need for NLP systems to provide explanations that encourage scrutiny and critical thinking rather than blind trust. All code will be released publicly.

</details>


### [17] [CURE: Cultural Understanding and Reasoning Evaluation - A Framework for "Thick" Culture Alignment Evaluation in LLMs](https://arxiv.org/abs/2511.12014)
*Truong Vo,Sanmi Koyejo*

Main category: cs.CL

TL;DR: 现有的文化能力评估方法存在局限性，为了解决这个问题，我们引入了一套基准，它为模型提供了需要文化基础推理的现实情境。


<details>
  <summary>Details</summary>
Motivation: 现有方法侧重于去语境化的正确性或强制选择判断，忽略了对适当反应所需的文化理解和推理的需要。

Method: 我们引入了一套基准，该基准不是直接探测抽象规范或孤立的陈述，而是向模型呈现需要文化基础推理的现实情境。除了标准的精确匹配指标外，我们还引入了四个互补指标（覆盖率、特异性、内涵和连贯性）来捕捉模型响应质量的不同维度。

Result: 对前沿模型的实证分析表明，单薄的评估系统性地高估了文化能力，并产生具有高方差的不稳定评估。相比之下，深入的评估暴露了推理深度的差异，减少了方差，并提供了更稳定、可解释的文化理解信号。

Conclusion: 论文提出了一个更有效的文化能力评估框架，通过更贴近现实的基准和更全面的评估指标，能够更准确地评估模型在文化理解和推理方面的能力。

Abstract: Large language models (LLMs) are increasingly deployed in culturally diverse environments, yet existing evaluations of cultural competence remain limited. Existing methods focus on de-contextualized correctness or forced-choice judgments, overlooking the need for cultural understanding and reasoning required for appropriate responses. To address this gap, we introduce a set of benchmarks that, instead of directly probing abstract norms or isolated statements, present models with realistic situational contexts that require culturally grounded reasoning. In addition to the standard Exact Match metric, we introduce four complementary metrics (Coverage, Specificity, Connotation, and Coherence) to capture different dimensions of model's response quality. Empirical analysis across frontier models reveals that thin evaluation systematically overestimates cultural competence and produces unstable assessments with high variance. In contrast, thick evaluation exposes differences in reasoning depth, reduces variance, and provides more stable, interpretable signals of cultural understanding.

</details>


### [18] [Exploring Parameter-Efficient Fine-Tuning and Backtranslation for the WMT 25 General Translation Task](https://arxiv.org/abs/2511.12109)
*Felipe Fujita,Hideyuki Takada*

Main category: cs.CL

TL;DR: 结合微调和回译可以有效提高小型日语语料库的神经机器翻译效果。


<details>
  <summary>Details</summary>
Motivation: 研究在小型日语语料库上，结合微调和回译对神经机器翻译的有效性。

Method: 首先使用从单语日语语料库生成的合成数据进行回译(BT)，然后在一个来自不同日语新闻和文学语料库的真实小型并行数据集上对模型进行微调(FT)。最后，将回译和微调相结合。

Result: 单独使用回译COMET=0.468，单独使用微调COMET=0.589，结合回译和微调COMET=0.597。结果表明，即使训练数据有限，结合使用回译和有针对性的微调也能显著提高翻译质量。

Conclusion: 即使使用有限的训练数据，通过回译和有针对性的微调的协同使用，可以显著提高翻译质量，优于单独使用每种技术。这种方法为改进低资源语言对提供了一种轻量级但功能强大的策略。

Abstract: In this paper, we explore the effectiveness of combining fine-tuning and backtranslation on a small Japanese corpus for neural machine translation. Starting from a baseline English{\textrightarrow}Japanese model (COMET = 0.460), we first apply backtranslation (BT) using synthetic data generated from monolingual Japanese corpora, yielding a modest increase (COMET = 0.468). Next, we fine-tune (FT) the model on a genuine small parallel dataset drawn from diverse Japanese news and literary corpora, achieving a substantial jump to COMET = 0.589 when using Mistral 7B. Finally, we integrate both backtranslation and fine-tuning{ -- }first augmenting the small dataset with BT generated examples, then adapting via FT{ -- }which further boosts performance to COMET = 0.597. These results demonstrate that, even with limited training data, the synergistic use of backtranslation and targeted fine-tuning on Japanese corpora can significantly enhance translation quality, outperforming each technique in isolation. This approach offers a lightweight yet powerful strategy for improving low-resource language pairs.

</details>


### [19] [LLMLagBench: Identifying Temporal Training Boundaries in Large Language Models](https://arxiv.org/abs/2511.12116)
*Piotr Pęzik,Konrad Kaczyński,Maria Szymańska,Filip Żarnecki,Zuzanna Deckert,Jakub Kwiatkowski,Wojciech Janowski*

Main category: cs.CL

TL;DR: LLMs have a knowledge boundary due to their training data's temporal cutoff, leading to potential inaccuracies when dealing with recent events.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with providing accurate information about events after their training data's temporal cutoff, and may blend outdated information, compromising accuracy.

Method: Introduce LLMLagBench, a benchmark to identify the earliest probable temporal boundaries of an LLM's training data by evaluating its knowledge of recent events.

Result: Apply LLMLagBench to evaluate a large set of LLMs, including models with both explicitly declared and undeclared training cutoffs.

Conclusion: Assess the reliability of the benchmark by manual validation and comparison with publicly released information about LLM pretraining.

Abstract: Large Language Models (LLMs) are pretrained on textual data up to a specific temporal cutoff. This creates a strict knowledge boundary beyond which models cannot provide accurate information without querying external sources. More subtly, when this limitation is unknown or ignored, LLMs may inadvertently blend outdated time-sensitive information with general knowledge during reasoning tasks, potentially compromising response accuracy. We introduce LLMLagBench, an LLM freshness benchmark, as a systematic approach for identifying the earliest probable temporal boundaries of an LLM's training data by evaluating its knowledge of recent events. We then apply this benchmark to evaluate a large set of LLMs, including models with both explicitly declared and undeclared training cutoffs. The reliability of the benchmark is assessed by manual validation and comparison with publicly released information about LLM pretraining.

</details>


### [20] [PRISM of Opinions: A Persona-Reasoned Multimodal Framework for User-centric Conversational Stance Detection](https://arxiv.org/abs/2511.12130)
*Bingbing Wang,Zhixin Bai,Zhengda Jin,Zihan Wang,Xintong Song,Jingjie Lin,Sixuan Li,Jing Li,Ruifeng Xu*

Main category: cs.CL

TL;DR: 本文介绍了一个新的用户中心的多模态对话立场检测数据集（U-MStance），并提出了一个名为PRISM的模型，该模型通过从历史帖子中提取用户画像，并结合Chain-of-Thought推理和多任务学习，来提升立场检测的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态对话立场检测研究存在两个局限性：伪多模态问题（视觉线索仅出现在源帖子中，而评论被视为纯文本）和用户同质性问题（忽略了个人特征对立场表达的影响）。

Method: 本文构建了一个新的数据集U-MStance，并提出了PRISM模型。PRISM模型首先从用户的历史帖子中提取用户画像，然后通过Chain-of-Thought将文本和视觉线索对齐，最后使用互任务强化机制联合优化立场检测和生成任务。

Result: 在U-MStance数据集上的实验表明，PRISM模型优于现有模型，证明了用户中心和上下文相关的多模态推理在立场理解方面的有效性。

Conclusion: 本文通过构建新的数据集和提出新的模型，解决了多模态对话立场检测中存在的伪多模态和用户同质性问题，并验证了所提出方法的有效性。

Abstract: The rapid proliferation of multimodal social media content has driven research in Multimodal Conversational Stance Detection (MCSD), which aims to interpret users' attitudes toward specific targets within complex discussions. However, existing studies remain limited by: **1) pseudo-multimodality**, where visual cues appear only in source posts while comments are treated as text-only, misaligning with real-world multimodal interactions; and **2) user homogeneity**, where diverse users are treated uniformly, neglecting personal traits that shape stance expression. To address these issues, we introduce **U-MStance**, the first user-centric MCSD dataset, containing over 40k annotated comments across six real-world targets. We further propose **PRISM**, a **P**ersona-**R**easoned mult**I**modal **S**tance **M**odel for MCSD. PRISM first derives longitudinal user personas from historical posts and comments to capture individual traits, then aligns textual and visual cues within conversational context via Chain-of-Thought to bridge semantic and pragmatic gaps across modalities. Finally, a mutual task reinforcement mechanism is employed to jointly optimize stance detection and stance-aware response generation for bidirectional knowledge transfer. Experiments on U-MStance demonstrate that PRISM yields significant gains over strong baselines, underscoring the effectiveness of user-centric and context-grounded multimodal reasoning for realistic stance understanding.

</details>


### [21] [AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing](https://arxiv.org/abs/2511.12133)
*Qingyu Zhang,Chunlei Xin,Xuanang Chen,Yaojie Lu,Hongyu Lin,Xianpei Han,Le Sun,Qing Ye,Qianlong Xie,Xingxing Wang*

Main category: cs.CL

TL;DR: 本文介绍了一种用于目标驱动的劝说对话的新框架AI-Salesman，该框架通过双阶段架构和动态轮廓引导代理来实现稳健的销售策略和事实忠实性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在目标驱动的劝说对话中面临多轮规划、事实忠实性以及缺乏特定任务数据的问题，导致策略脆弱和事实幻觉。

Method: 本文构建了一个真实世界的对话数据集TeleSalesCorpus，并提出了AI-Salesman框架，该框架包含一个使用贝叶斯监督强化学习算法的训练阶段和一个使用动态轮廓引导代理(DOGA)的推理阶段。

Result: 实验结果表明，提出的AI-Salesman在自动指标和综合人工评估方面均显著优于基线模型。

Conclusion: AI-Salesman在复杂的劝说场景中表现出有效性。

Abstract: Goal-driven persuasive dialogue, exemplified by applications like telemarketing, requires sophisticated multi-turn planning and strict factual faithfulness, which remains a significant challenge for even state-of-the-art Large Language Models (LLMs). A lack of task-specific data often limits previous works, and direct LLM application suffers from strategic brittleness and factual hallucination. In this paper, we first construct and release TeleSalesCorpus, the first real-world-grounded dialogue dataset for this domain. We then propose AI-Salesman, a novel framework featuring a dual-stage architecture. For the training stage, we design a Bayesian-supervised reinforcement learning algorithm that learns robust sales strategies from noisy dialogues. For the inference stage, we introduce the Dynamic Outline-Guided Agent (DOGA), which leverages a pre-built script library to provide dynamic, turn-by-turn strategic guidance. Moreover, we design a comprehensive evaluation framework that combines fine-grained metrics for key sales skills with the LLM-as-a-Judge paradigm. Experimental results demonstrate that our proposed AI-Salesman significantly outperforms baseline models in both automatic metrics and comprehensive human evaluations, showcasing its effectiveness in complex persuasive scenarios.

</details>


### [22] [Seeing is Believing: Rich-Context Hallucination Detection for MLLMs via Backward Visual Grounding](https://arxiv.org/abs/2511.12140)
*Pinxue Guo,Chongruo Wu,Xinyu Zhou,Lingyi Hong,Zhaoyu Chen,Jinglun Li,Kaixun Jiang,Sen-ching Samson Cheung,Wei Zhang,Wenqiang Zhang*

Main category: cs.CL

TL;DR: VBackChecker是一个新的无参考幻觉检测框架，通过利用具有推理和指代分割能力的像素级Grounding LLM来验证MLLM生成的响应与视觉输入的一致性。


<details>
  <summary>Details</summary>
Motivation: 准确检测多模态大型语言模型(MLLM)中的幻觉对于确保其在实际应用中的可靠性至关重要。

Method: 提出了VBackChecker，一个新颖的参考无关幻觉检测框架，通过利用像素级Grounding LLM来验证MLLM生成响应与视觉输入的一致性。此外，还设计了一个创新的pipeline，用于生成指令调优数据(R-Instruct)。

Result: VBackChecker优于先前的复杂框架，并在R^2 -HalBench上实现了最先进的性能，甚至可以与GPT-4o在幻觉检测方面的能力相媲美。它还在像素级 grounding 任务中超越了先前的方法，实现了超过 10% 的改进。

Conclusion: VBackChecker是一个有效的MLLM幻觉检测框架，在多个基准测试中表现出色。

Abstract: Multimodal Large Language Models (MLLMs) have unlocked powerful cross-modal capabilities, but still significantly suffer from hallucinations. As such, accurate detection of hallucinations in MLLMs is imperative for ensuring their reliability in practical applications. To this end, guided by the principle of "Seeing is Believing", we introduce VBackChecker, a novel reference-free hallucination detection framework that verifies the consistency of MLLMgenerated responses with visual inputs, by leveraging a pixellevel Grounding LLM equipped with reasoning and referring segmentation capabilities. This reference-free framework not only effectively handles rich-context scenarios, but also offers interpretability. To facilitate this, an innovative pipeline is accordingly designed for generating instruction-tuning data (R-Instruct), featuring rich-context descriptions, grounding masks, and hard negative samples. We further establish R^2 -HalBench, a new hallucination benchmark for MLLMs, which, unlike previous benchmarks, encompasses real-world, rich-context descriptions from 18 MLLMs with high-quality annotations, spanning diverse object-, attribute, and relationship-level details. VBackChecker outperforms prior complex frameworks and achieves state-of-the-art performance on R^2 -HalBench, even rivaling GPT-4o's capabilities in hallucination detection. It also surpasses prior methods in the pixel-level grounding task, achieving over a 10% improvement. All codes, data, and models are available at https://github.com/PinxueGuo/VBackChecker.

</details>


### [23] [CriticSearch: Fine-Grained Credit Assignment for Search Agents via a Retrospective Critic](https://arxiv.org/abs/2511.12159)
*Yaocheng Zhang,Haohuan Huang,Zijun Song,Yuanheng Zhu,Qichao Zhang,Zijie Zhao,Dongbin Zhao*

Main category: cs.CL

TL;DR: CriticSearch通过追溯评论机制提供密集、turn-level的反馈，以优化大型语言模型在复杂问答任务中的工具集成推理（TIR）。


<details>
  <summary>Details</summary>
Motivation: 现有搜索代理管道通常依赖于基于强化学习的优化，但常常面临稀疏结果奖励，导致低效探索和不稳定训练。

Method: 引入CriticSearch，一种细粒度的信用分配框架，通过追溯评论机制提供密集的、turn-level的反馈。使用冻结的、非对称的评论LLM回顾性地评估每一轮，利用来自完整轨迹和标准答案的特权信息，将这些评估转化为稳定的、密集的奖励，以指导策略改进。

Result: 在不同的多跳推理基准测试中，CriticSearch始终优于现有的基线，实现了更快的收敛、更高的训练稳定性和更高的性能。

Conclusion: CriticSearch通过提供密集反馈，有效提升了大型语言模型在复杂问答任务中的工具集成推理能力。

Abstract: Tool-Integrated Reasoning (TIR) with search engines enables large language models to iteratively retrieve up-to-date external knowledge, enhancing adaptability and generalization in complex question-answering tasks. However, existing search agent pipelines typically depend on reinforcement learning based optimization, which often suffers from sparse outcome rewards, leading to inefficient exploration and unstable training. We introduce CriticSearch, a fine-grained credit-assignment framework that supplies dense, turn-level feedback via a retrospective critic mechanism. During training, a frozen, asymmetric critique LLM retrospectively evaluates each turn using privileged information from the full trajectory and gold answers, converting these assessments into stable, dense rewards that guide policy improvement. Experimental results across diverse multi-hop reasoning benchmarks demonstrate that CriticSearch consistently outperforms existing baselines, achieving faster convergence, improved training stability, and higher performance.

</details>


### [24] [MME-RAG: Multi-Manager-Expert Retrieval-Augmented Generation for Fine-Grained Entity Recognition in Task-Oriented Dialogues](https://arxiv.org/abs/2511.12213)
*Liang Xue,Haoyu Liu,Yajun Tian,Xinyu Zhong,Yang Liu*

Main category: cs.CL

TL;DR: MME-RAG，一个多管理器-专家检索增强生成框架，通过分层分解和KeyInfo引导检索，实现精确和领域自适应的实体识别。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在面向任务的对话中，领域适应和检索可控性方面面临挑战。

Method: MME-RAG将实体识别分解为两个协同阶段：轻量级管理器进行类型级别判断，专业专家进行跨度级别提取。每个专家都由KeyInfo检索器支持，该检索器在推理过程中注入语义对齐的少量样本，无需额外训练即可实现精确和领域自适应的提取。

Result: 在CrossNER、MIT-Movie、MIT-Restaurant和新构建的多领域客户服务数据集上的实验表明，MME-RAG在大多数领域中优于最近的基线。

Conclusion: 分层分解和KeyInfo引导检索是鲁棒性和跨领域泛化的关键驱动因素，MME-RAG是自适应对话理解的可扩展和可解释的解决方案。

Abstract: Fine-grained entity recognition is crucial for reasoning and decision-making in task-oriented dialogues, yet current large language models (LLMs) continue to face challenges in domain adaptation and retrieval controllability. We introduce MME-RAG, a Multi-Manager-Expert Retrieval-Augmented Generation framework that decomposes entity recognition into two coordinated stages: type-level judgment by lightweight managers and span-level extraction by specialized experts. Each expert is supported by a KeyInfo retriever that injects semantically aligned, few-shot exemplars during inference, enabling precise and domain-adaptive extraction without additional training. Experiments on CrossNER, MIT-Movie, MIT-Restaurant, and our newly constructed multi-domain customer-service dataset demonstrate that MME-RAG performs better than recent baselines in most domains. Ablation studies further show that both the hierarchical decomposition and KeyInfo-guided retrieval are key drivers of robustness and cross-domain generalization, establishing MME-RAG as a scalable and interpretable solution for adaptive dialogue understanding.

</details>


### [25] [Consistency Is the Key: Detecting Hallucinations in LLM Generated Text By Checking Inconsistencies About Key Facts](https://arxiv.org/abs/2511.12236)
*Raavi Gupta,Pranav Hari Panicker,Sumit Bhatia,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: CONFACTCHECK：一种高效的幻觉检测方法，无需外部知识库，基于生成文本中事实探测的一致性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 经常产生幻觉和生成不符合事实的文本，这对医疗保健、金融和客户支持等领域构成严重风险。现有检测幻觉的方法通常需要多次 LLM API 调用，从而增加延迟和 API 成本。

Method: 提出 CONFACTCHECK，它不利用任何外部知识库，并且基于以下简单直觉：生成文本中事实探测的响应应在单个 LLM 内和跨不同 LLM 保持一致。

Result: 在涵盖事实文本生成和开放生成的多个数据集上进行的严格经验评估表明，与在类似条件下运行的现有基线相比，CONFACTCHECK 可以使用更少的资源有效地检测到幻觉事实，并获得更高的准确率。

Conclusion: CONFACTCHECK 是一种高效且准确的幻觉检测方法，它利用了 LLM 自身的一致性，而无需外部知识库或多次 API 调用。

Abstract: Large language models (LLMs), despite their remarkable text generation capabilities, often hallucinate and generate text that is factually incorrect and not grounded in real-world knowledge. This poses serious risks in domains like healthcare, finance, and customer support. A typical way to use LLMs is via the APIs provided by LLM vendors where there is no access to model weights or options to fine-tune the model. Existing methods to detect hallucinations in such settings where the model access is restricted or constrained by resources typically require making multiple LLM API calls, increasing latency and API cost. We introduce CONFACTCHECK, an efficient hallucination detection approach that does not leverage any external knowledge base and works on the simple intuition that responses to factual probes within the generated text should be consistent within a single LLM and across different LLMs. Rigorous empirical evaluation on multiple datasets that cover both the generation of factual texts and the open generation shows that CONFACTCHECK can detect hallucinated facts efficiently using fewer resources and achieves higher accuracy scores compared to existing baselines that operate under similar conditions. Our code is available here.

</details>


### [26] [ViConBERT: Context-Gloss Aligned Vietnamese Word Embedding for Polysemous and Sense-Aware Representations](https://arxiv.org/abs/2511.12249)
*Khang T. Huynh,Dung H. Nguyen,Binh T. Nguyen*

Main category: cs.CL

TL;DR: ViConBERT is a new framework for learning Vietnamese contextualized embeddings that integrates contrastive learning and gloss-based distillation to better capture word meaning.


<details>
  <summary>Details</summary>
Motivation: Vietnamese lacks robust models and evaluation resources for fine-grained semantic understanding.

Method: A novel framework for learning Vietnamese contextualized embeddings that integrates contrastive learning (SimCLR) and gloss-based distillation.

Result: ViConBERT outperforms strong baselines on WSD (F1 = 0.87) and achieves competitive performance on ViCon (AP = 0.88) and ViSim-400 (Spearman's rho = 0.60).

Conclusion: ViConBERT demonstrates its effectiveness in modeling both discrete senses and graded semantic relations in Vietnamese.

Abstract: Recent advances in contextualized word embeddings have greatly improved semantic tasks such as Word Sense Disambiguation (WSD) and contextual similarity, but most progress has been limited to high-resource languages like English. Vietnamese, in contrast, still lacks robust models and evaluation resources for fine-grained semantic understanding. In this paper, we present ViConBERT, a novel framework for learning Vietnamese contextualized embeddings that integrates contrastive learning (SimCLR) and gloss-based distillation to better capture word meaning. We also introduce ViConWSD, the first large-scale synthetic dataset for evaluating semantic understanding in Vietnamese, covering both WSD and contextual similarity. Experimental results show that ViConBERT outperforms strong baselines on WSD (F1 = 0.87) and achieves competitive performance on ViCon (AP = 0.88) and ViSim-400 (Spearman's rho = 0.60), demonstrating its effectiveness in modeling both discrete senses and graded semantic relations. Our code, models, and data are available at https://github.com/tkhangg0910/ViConBERT

</details>


### [27] [Auditing Google's AI Overviews and Featured Snippets: A Case Study on Baby Care and Pregnancy](https://arxiv.org/abs/2511.12920)
*Desheng Hu,Joachim Baumann,Aleksandra Urman,Elsa Lichtenegger,Robin Forsberg,Aniko Hannak,Christo Wilson*

Main category: cs.CL

TL;DR: 谷歌搜索越来越多地通过AI Overviews (AIO) 和 Featured Snippets (FS)等功能展示AI生成的内容，但用户无法控制其展示方式。通过对1508个真实的婴儿护理和妊娠相关查询的系统算法审核，评估了这些信息展示的质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 评估AI生成内容在母婴健康领域的信息质量和一致性，因为用户越来越依赖这些信息，但对其展示方式缺乏控制。

Method: 通过系统算法审核1508个婴儿护理和妊娠相关查询，并从答案一致性、相关性、医疗保障措施、来源类别和情感一致性等多维度评估AIO和FS的质量。

Result: AIO和FS在同一搜索结果页面上显示的信息不一致，比例高达33%。两者都严重缺乏医疗保障措施（分别仅占11%和7%）。FS经常链接到商业来源。

Conclusion: AI介导的健康信息需要更严格的质量控制。该方法提供了一个可转移的框架，用于审核高风险领域中的AI系统，在这些领域中，信息质量直接影响用户福祉。

Abstract: Google Search increasingly surfaces AI-generated content through features like AI Overviews (AIO) and Featured Snippets (FS), which users frequently rely on despite having no control over their presentation. Through a systematic algorithm audit of 1,508 real baby care and pregnancy-related queries, we evaluate the quality and consistency of these information displays. Our robust evaluation framework assesses multiple quality dimensions, including answer consistency, relevance, presence of medical safeguards, source categories, and sentiment alignment. Our results reveal concerning gaps in information consistency, with information in AIO and FS displayed on the same search result page being inconsistent with each other in 33% of cases. Despite high relevance scores, both features critically lack medical safeguards (present in just 11% of AIO and 7% of FS responses). While health and wellness websites dominate source categories for both, AIO and FS, FS also often link to commercial sources. These findings have important implications for public health information access and demonstrate the need for stronger quality controls in AI-mediated health information. Our methodology provides a transferable framework for auditing AI systems across high-stakes domains where information quality directly impacts user well-being.

</details>


### [28] [Cmprsr: Abstractive Token-Level Question-Agnostic Prompt Compressor](https://arxiv.org/abs/2511.12281)
*Ivan Zakazov,Alexander Sharipov,Berke Argin,Oussama Gabouj,Kamel Charaf,Alexi Semiz,Lorenzo Drudi,Nicolas Baldwin,Robert West*

Main category: cs.CL

TL;DR: 提出了一种新的提示压缩范式，使用较小的LLM来压缩较大LLM的输入，以降低成本。


<details>
  <summary>Details</summary>
Motivation: 使用黑盒大型语言模型（LLM）的成本高昂。

Method: 构建了首个全面的LLM压缩器基准，并使用Textgrad优化压缩meta-prompt，以及使用SFT和GRPO对Qwen3-4B进行后训练。

Result: 发现模型压缩能力存在显著差异，并提出了Cmprsr模型，在多个数据集上优于其他压缩方法。

Conclusion: Cmprsr模型具有良好的泛化能力，并且可以精确控制压缩率，从而在成本和质量之间实现精细的权衡。

Abstract: Motivated by the high costs of using black-box Large Language Models (LLMs), we introduce a novel prompt compression paradigm, under which we use smaller LLMs to compress inputs for the larger ones. We present the first comprehensive LLM-as-a-compressor benchmark spanning 25 open- and closed-source models, which reveals significant disparity in models' compression ability in terms of (i) preserving semantically important information (ii) following the user-provided compression rate (CR). We further improve the performance of gpt-4.1-mini, the best overall vanilla compressor, with Textgrad-based compression meta-prompt optimization. We also identify the most promising open-source vanilla LLM - Qwen3-4B - and post-train it with a combination of supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), pursuing the dual objective of CR adherence and maximizing the downstream task performance. We call the resulting model Cmprsr and demonstrate its superiority over both extractive and vanilla abstractive compression across the entire range of compression rates on lengthy inputs from MeetingBank and LongBench as well as short prompts from GSM8k. The latter highlights Cmprsr's generalizability across varying input lengths and domains. Moreover, Cmprsr closely follows the requested compression rate, offering fine control over the cost-quality trade-off.

</details>


### [29] [AugAbEx : Way Forward for Extractive Case Summarization](https://arxiv.org/abs/2511.12290)
*Purnima Bindal,Vikas Kumar,Sagar Rathore,Vasudha Bhatnagar*

Main category: cs.CL

TL;DR: 法律判决摘要因其语言复杂、法律术语具有上下文相关性以及文档长度而给法律从业者带来了沉重的认知负担。因此，法律文件的自动摘要引起了自然语言处理研究人员的重视。我们设想，由于深度神经方法生成的法律文件的抽象摘要仍然容易出现歪曲细微法律术语或忽略关键上下文细节的风险，因此使用抽取案例摘要器将成为一种上升趋势。


<details>
  <summary>Details</summary>
Motivation: 由于法律判决摘要对法律从业者造成认知负担，并且现有的深度学习方法存在不足，因此需要自动生成法律文件摘要。

Method: 利用现有的抽象黄金标准摘要，构建一个轻量且透明的流程，以创建相应的抽取黄金标准版本。该方法确保专家的意见能够从原始的抽象黄金标准摘要延续到转换后的抽取摘要中。

Result: 通过整合相应的抽取摘要来扩充现有的七个案例摘要数据集，包括抽象摘要，并为案例摘要研究社区创建一个丰富的数据资源。为了确保增强的抽取摘要的质量，我们使用原始的抽象黄金标准摘要进行了广泛的比较评估，涵盖结构、词汇和语义维度。我们还将比较两个摘要的域级别信息。

Conclusion: 我们致力于在公共领域发布增强的数据集，供研究社区使用，并相信该资源将为推进法律文件的自动摘要领域提供机会。

Abstract: Summarization of legal judgments poses a heavy cognitive burden on law practitioners due to the complexity of the language, context-sensitive legal jargon, and the length of the document. Therefore, the automatic summarization of legal documents has attracted serious attention from natural language processing researchers. Since the abstractive summaries of legal documents generated by deep neural methods remain prone to the risk of misrepresenting nuanced legal jargon or overlooking key contextual details, we envisage a rising trend toward the use of extractive case summarizers.
  Given the high cost of human annotation for gold standard extractive summaries, we engineer a light and transparent pipeline that leverages existing abstractive gold standard summaries to create the corresponding extractive gold standard versions. The approach ensures that the experts` opinions ensconced in the original gold standard abstractive summaries are carried over to the transformed extractive summaries. We aim to augment seven existing case summarization datasets, which include abstractive summaries, by incorporating corresponding extractive summaries and create an enriched data resource for case summarization research community. To ensure the quality of the augmented extractive summaries, we perform an extensive comparative evaluation with the original abstractive gold standard summaries covering structural, lexical, and semantic dimensions. We also compare the domain-level information of the two summaries. We commit to release the augmented datasets in the public domain for use by the research community and believe that the resource will offer opportunities to advance the field of automatic summarization of legal documents.

</details>


### [30] [Do LLMs and Humans Find the Same Questions Difficult? A Case Study on Japanese Quiz Answering](https://arxiv.org/abs/2511.12300)
*Naoya Sugiura,Kosuke Yamada,Yasuhiro Ogawa,Katsuhiko Toyama,Ryohei Sasano*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型（LLMs）和人类在抢答测验中的难度差异。


<details>
  <summary>Details</summary>
Motivation: 研究动机是 выяснить LLMs 难以解决的问题是否与人类相同。

Method: 首先，收集包含问题、答案和人类正确率的日语测验数据。然后，提示 LLMs 在多个设置下回答测验问题，并将它们的正确率与人类的正确率进行比较，从两个分析角度进行对比。

Result: 实验结果表明，与人类相比，LLMs 更难回答正确答案未被维基百科覆盖的测验问题，并且难以回答需要数字答案的问题。

Conclusion: LLMs 在处理需要维基百科知识和数字推理的问题上存在局限性。

Abstract: LLMs have achieved performance that surpasses humans in many NLP tasks. However, it remains unclear whether problems that are difficult for humans are also difficult for LLMs. This study investigates how the difficulty of quizzes in a buzzer setting differs between LLMs and humans. Specifically, we first collect Japanese quiz data including questions, answers, and correct response rate of humans, then prompted LLMs to answer the quizzes under several settings, and compare their correct answer rate to that of humans from two analytical perspectives. The experimental results showed that, compared to humans, LLMs struggle more with quizzes whose correct answers are not covered by Wikipedia entries, and also have difficulty with questions that require numerical answers.

</details>


### [31] [Don't Think of the White Bear: Ironic Negation in Transformer Models Under Cognitive Load](https://arxiv.org/abs/2511.12381)
*Logan Mann,Nayan Saxena,Sarah Tandon,Chenhao Sun,Savar Toteja,Kevin Zhu*

Main category: cs.CL

TL;DR: 大型语言模型（LLM）在执行否定指令时，也会出现与人类类似的“讽刺性反弹”现象，即试图抑制某个概念反而会使其更容易被激活。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在执行否定指令时是否以及如何出现“讽刺性反弹”现象。否定指令会增加被否定概念的可访问性，这对于LLM来说是一个挑战。

Method: 通过两个实验来研究这个问题：(1) 改变干扰文本的类型（语义、句法、重复），观察反弹强度；(2) 测试模型是否能区分同一概念的中性和否定表达，以及这种区分是否与反弹的持久性相关。

Result: 否定指令后立即出现反弹现象，并且随着干扰文本长度的增加或语义相关性的增强而加剧，重复性的干扰文本则有助于抑制。更强的极性分离与更持久的反弹相关。

Conclusion: 研究结果将“讽刺性反弹”的认知预测与对长上下文干扰的机制性理解联系起来，并通过电路追踪分析，识别出放大禁止令牌的稀疏中间层注意力头，以及抑制早期层。还发布了一个包含5000个否定提示的数据集ReboundBench，以支持未来的研究。

Abstract: Negation instructions such as 'do not mention $X$' can paradoxically increase the accessibility of $X$ in human thought, a phenomenon known as ironic rebound. Large language models (LLMs) face the same challenge: suppressing a concept requires internally activating it, which may prime rebound instead of avoidance. We investigated this tension with two experiments. \textbf{(1) Load \& content}: after a negation instruction, we vary distractor text (semantic, syntactic, repetition) and measure rebound strength. \textbf{(2) Polarity separation}: We test whether models distinguish neutral from negative framings of the same concept and whether this separation predicts rebound persistence. Results show that rebound consistently arises immediately after negation and intensifies with longer or semantic distractors, while repetition supports suppression. Stronger polarity separation correlates with more persistent rebound. Together, these findings, complemented by a circuit tracing analysis that identifies sparse middle-layer attention heads amplifying forbidden tokens while early layers suppress, link cognitive predictions of ironic rebound with mechanistic insights into long-context interference. To support future work, we release ReboundBench, a dataset of $5,000$ systematically varied negation prompts designed to probe rebound in LLMs.

</details>


### [32] [From Phonemes to Meaning: Evaluating Large Language Models on Tamil](https://arxiv.org/abs/2511.12387)
*Jeyarajalingam Varsha,Menan Velayuthan,Sumirtha Karunakaran,Rasan Nivethiga,Kengatharaiyer Sarveswaran*

Main category: cs.CL

TL;DR: 本文介绍了ILAKKANAM，这是一个泰米尔语的语言评估基准，用于评估大型语言模型(llm)在该语言中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多语言基准测试通常依赖于翻译的英语数据集，无法捕捉目标语言的语言和文化细微差别。因此，本文旨在填补低资源、形态丰富的泰米尔语的语言能力差距。

Method: 本文使用斯里兰卡学校水平泰米尔语科目考试试卷中的820个问题，手动创建了泰米尔语的语言评估基准ILAKKANAM。每个问题都由训练有素的语言学家在五个语言类别和一个事实知识类别下进行注释，跨越1-13年级，以确保广泛的语言覆盖。

Result: Gemini 2.5取得了最高的总体性能，而开源模型则落后，突出了语言基础方面的差距。所有模型在较低年级的问题上表现良好，但随着语言复杂性的增加，表现明显下降。模型的总体性能与其识别语言类别的能力之间没有观察到很强的相关性，这表明性能可能由暴露驱动，而不是真正的理解。

Conclusion: 本文的研究结果表明，大型语言模型在泰米尔语等低资源语言中的语言能力仍有待提高。

Abstract: Large Language Models (LLMs) have shown strong generalization across tasks in high-resource languages; however, their linguistic competence in low-resource and morphologically rich languages such as Tamil remains largely unexplored. Existing multilingual benchmarks often rely on translated English datasets, failing to capture the linguistic and cultural nuances of the target language. To address this gap, we introduce ILAKKANAM, the first Tamil-specific linguistic evaluation benchmark manually curated using 820 questions from Sri Lankan school-level Tamil subject examination papers. Each question is annotated by trained linguists under five linguistic categories and a factual knowledge category, spanning Grades 1--13 to ensure broad linguistic coverage. We evaluate both closed-source and open-source LLMs using a standardized evaluation framework. Our results show that Gemini 2.5 achieves the highest overall performance, while open-source models lag behind, highlighting the gap in linguistic grounding. Category- and grade-wise analyses reveal that all models perform well on lower-grade questions but show a clear decline as linguistic complexity increases. Further, no strong correlation is observed between a model's overall performance and its ability to identify linguistic categories, suggesting that performance may be driven by exposure rather than genuine understanding.

</details>


### [33] [Probing Preference Representations: A Multi-Dimensional Evaluation and Analysis Method for Reward Models](https://arxiv.org/abs/2511.12464)
*Chenglong Wang,Yifu Huo,Yang Gan,Yongyu Mu,Qiaozhi He,Murun Yang,Bei Li,Chunliang Zhang,Tongran Liu,Anxiang Ma,Zhengtao Yu,Jingbo Zhu,Tong Xiao*

Main category: cs.CL

TL;DR: 本文提出了一种通过探测偏好表征来评估奖励模型的新方法，并构建了一个多维度奖励模型基准（MRMBench）。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型评估方法无法提供每个偏好维度的性能信息，且缺乏对奖励模型预测偏好能力的深入理解。

Method: 1. 构建包含六个探测任务的MRMBench，用于评估不同偏好维度。2. 提出一种推理时探测方法，用于识别奖励预测中使用的维度并增强其可解释性。

Result: MRMBench与大型语言模型的对齐性能高度相关，可作为开发高级奖励模型的可靠参考。奖励模型在捕获多个维度的偏好方面存在困难。推理时探测方法能可靠评估奖励预测的置信度。

Conclusion: MRMBench可以有效评估奖励模型，并揭示了多目标优化在奖励建模中的潜力。推理时探测方法能够提高LLM的对齐效果。

Abstract: Previous methods evaluate reward models by testing them on a fixed pairwise ranking test set, but they typically do not provide performance information on each preference dimension. In this work, we address the evaluation challenge of reward models by probing preference representations. To confirm the effectiveness of this evaluation method, we construct a Multi-dimensional Reward Model Benchmark (MRMBench), a collection of six probing tasks for different preference dimensions. We design it to favor and encourage reward models that better capture preferences across different dimensions. Furthermore, we introduce an analysis method, inference-time probing, which identifies the dimensions used during the reward prediction and enhances its interpretability. Through extensive experiments, we find that MRMBench strongly correlates with the alignment performance of large language models (LLMs), making it a reliable reference for developing advanced reward models. Our analysis of MRMBench evaluation results reveals that reward models often struggle to capture preferences across multiple dimensions, highlighting the potential of multi-objective optimization in reward modeling. Additionally, our findings show that the proposed inference-time probing method offers a reliable metric for assessing the confidence of reward predictions, which ultimately improves the alignment of LLMs.

</details>


### [34] [Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing](https://arxiv.org/abs/2511.12472)
*Mengying Wang,Chenhui Ma,Ao Jiao,Tuo Liang,Pengjun Lu,Shrinidhi Hegde,Yu Yin,Evren Gurkan-Cavusoglu,Yinghui Wu*

Main category: cs.CL

TL;DR: 论文提出了一个名为SerenQA的框架，用于评估大型语言模型在知识图谱问答中发现意外答案的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的知识图谱问答系统通常只返回相关但可预测的答案，缺乏发现新颖答案的能力。

Method: 论文定义了面向意外发现的知识图谱问答任务，并提出了SerenQA框架，包含严谨的意外发现指标和专家标注的基准数据集，以及结构化的评估流程。

Result: 实验表明，目前的大型语言模型在知识检索方面表现良好，但在识别真正令人惊讶和有价值的发现方面仍有不足。

Conclusion: 大型语言模型在知识图谱问答中仍有很大的提升空间，尤其是在发现意外和新颖的答案方面。

Abstract: Large Language Models (LLMs) have greatly advanced knowledge graph question answering (KGQA), yet existing systems are typically optimized for returning highly relevant but predictable answers. A missing yet desired capacity is to exploit LLMs to suggest surprise and novel ("serendipitious") answers. In this paper, we formally define the serendipity-aware KGQA task and propose the SerenQA framework to evaluate LLMs' ability to uncover unexpected insights in scientific KGQA tasks. SerenQA includes a rigorous serendipity metric based on relevance, novelty, and surprise, along with an expert-annotated benchmark derived from the Clinical Knowledge Graph, focused on drug repurposing. Additionally, it features a structured evaluation pipeline encompassing three subtasks: knowledge retrieval, subgraph reasoning, and serendipity exploration. Our experiments reveal that while state-of-the-art LLMs perform well on retrieval, they still struggle to identify genuinely surprising and valuable discoveries, underscoring a significant room for future improvements. Our curated resources and extended version are released at: https://cwru-db-group.github.io/serenQA.

</details>


### [35] [SGuard-v1: Safety Guardrail for Large Language Models](https://arxiv.org/abs/2511.12497)
*JoonHo Lee,HyeonMin Cho,Jaewoong Yun,Hyunjae Lee,JunKyu Lee,Juree Seok*

Main category: cs.CL

TL;DR: SGuard-v1是一个轻量级的LLM安全防护系统，包含两个模型，用于检测有害内容和筛选对抗性提示。


<details>
  <summary>Details</summary>
Motivation: 在人机对话环境中，大型语言模型(llm)可能存在安全风险，需要有效的防护机制。

Method: 该系统包含两个组件：ContentFilter用于识别LLM提示和响应中的安全风险；JailbreakFilter使用精心设计的课程训练，以识别对抗性提示。

Result: SGuard-v1在公共和私有安全基准测试中实现了最先进的安全性能，同时保持轻量级。

Conclusion: SGuard-v1以Apache-2.0许可证发布，以促进AI安全领域的进一步研究和实际部署。

Abstract: We present SGuard-v1, a lightweight safety guardrail for Large Language Models (LLMs), which comprises two specialized models to detect harmful content and screen adversarial prompts in human-AI conversational settings. The first component, ContentFilter, is trained to identify safety risks in LLM prompts and responses in accordance with the MLCommons hazard taxonomy, a comprehensive framework for trust and safety assessment of AI. The second component, JailbreakFilter, is trained with a carefully designed curriculum over integrated datasets and findings from prior work on adversarial prompting, covering 60 major attack types while mitigating false-unsafe classification. SGuard-v1 is built on the 2B-parameter Granite-3.3-2B-Instruct model that supports 12 languages. We curate approximately 1.4 million training instances from both collected and synthesized data and perform instruction tuning on the base model, distributing the curated data across the two component according to their designated functions. Through extensive evaluation on public and proprietary safety benchmarks, SGuard-v1 achieves state-of-the-art safety performance while remaining lightweight, thereby reducing deployment overhead. SGuard-v1 also improves interpretability for downstream use by providing multi-class safety predictions and their binary confidence scores. We release the SGuard-v1 under the Apache-2.0 License to enable further research and practical deployment in AI safety.

</details>


### [36] [QA-Noun: Representing Nominal Semantics via Natural Language Question-Answer Pairs](https://arxiv.org/abs/2511.12504)
*Maria Tseytlin,Paul Roit,Omri Abend,Ido Dagan,Ayal Klein*

Main category: cs.CL

TL;DR: 这篇论文介绍了一种名为QA-Noun的基于QA的框架，用于捕获以名词为中心的语义关系，通过定义九个问题模板来覆盖名词的显式句法和隐式上下文角色，从而生成可解释的QA对，与 verbal QA-SRL 互补。该论文发布了详细的指南、一个包含超过2000个带注释的名词提及的数据集，以及一个与QA-SRL集成的训练模型，以实现句子含义的统一分解为单独的、高度细粒度的 facts。


<details>
  <summary>Details</summary>
Motivation: 现有的基于QA的语义方法在表示谓词-论元关系方面已经显示出有效性，但到目前为止，它们在很大程度上没有解决以名词为中心的语义问题。

Method: 该论文引入了QA-Noun，这是一个基于QA的框架，用于捕获以名词为中心的语义关系。QA-Noun定义了九个问题模板，涵盖名词的显式句法和隐式上下文角色，生成可解释的QA对，与verbal QA-SRL互补。

Result: QA-Noun实现了对AMR名词论元的近乎完整的覆盖，同时浮现了额外的上下文隐含关系。将QA-Noun与QA-SRL相结合，可以产生比最近的基于事实的分解方法（如FactScore和DecompScore）高出130%以上的粒度。

Conclusion: QA-Noun完善了更广泛的基于QA的语义框架，形成了一种全面的、可扩展的方法，用于细粒度的语义分解，以实现跨文本对齐。

Abstract: Decomposing sentences into fine-grained meaning units is increasingly used to model semantic alignment. While QA-based semantic approaches have shown effectiveness for representing predicate-argument relations, they have so far left noun-centered semantics largely unaddressed. We introduce QA-Noun, a QA-based framework for capturing noun-centered semantic relations. QA-Noun defines nine question templates that cover both explicit syntactical and implicit contextual roles for nouns, producing interpretable QA pairs that complement verbal QA-SRL. We release detailed guidelines, a dataset of over 2,000 annotated noun mentions, and a trained model integrated with QA-SRL to yield a unified decomposition of sentence meaning into individual, highly fine-grained, facts. Evaluation shows that QA-Noun achieves near-complete coverage of AMR's noun arguments while surfacing additional contextually implied relations, and that combining QA-Noun with QA-SRL yields over 130\% higher granularity than recent fact-based decomposition methods such as FactScore and DecompScore. QA-Noun thus complements the broader QA-based semantic framework, forming a comprehensive and scalable approach to fine-grained semantic decomposition for cross-text alignment.

</details>


### [37] [TAdaRAG: Task Adaptive Retrieval-Augmented Generation via On-the-Fly Knowledge Graph Construction](https://arxiv.org/abs/2511.12520)
*Jie Zhang,Bo Tang,Wanzi Shao,Wenqiang Wei,Jihao Zhao,Jianqing Zhu,Zhiyu li,Wen Xi,Zehao Lin,Feiyu Xiong,Yanchao Tan*

Main category: cs.CL

TL;DR: TAdaRAG: A new RAG framework that constructs task-adaptive knowledge graphs from external sources to improve the accuracy and coherence of LLM responses.


<details>
  <summary>Details</summary>
Motivation: Traditional RAG systems suffer from information loss due to truncated contexts and irrelevant details from unstructured knowledge, leading to hallucinations and broken reasoning.

Method: Proposes an intent-driven routing mechanism to a domain-specific extraction template, followed by supervised fine-tuning and reinforcement learning for concise knowledge integration.

Result: TAdaRAG outperforms existing methods on six public benchmarks and a real-world business benchmark across three backbone models.

Conclusion: TAdaRAG demonstrates strong generalization and practical effectiveness in diverse domains and long-text tasks.

Abstract: Retrieval-Augmented Generation (RAG) improves large language models by retrieving external knowledge, often truncated into smaller chunks due to the input context window, which leads to information loss, resulting in response hallucinations and broken reasoning chains. Moreover, traditional RAG retrieves unstructured knowledge, introducing irrelevant details that hinder accurate reasoning. To address these issues, we propose TAdaRAG, a novel RAG framework for on-the-fly task-adaptive knowledge graph construction from external sources. Specifically, we design an intent-driven routing mechanism to a domain-specific extraction template, followed by supervised fine-tuning and a reinforcement learning-based implicit extraction mechanism, ensuring concise, coherent, and non-redundant knowledge integration. Evaluations on six public benchmarks and a real-world business benchmark (NowNewsQA) across three backbone models demonstrate that TAdaRAG outperforms existing methods across diverse domains and long-text tasks, highlighting its strong generalization and practical effectiveness.

</details>


### [38] [Mitigating Length Bias in RLHF through a Causal Lens](https://arxiv.org/abs/2511.12573)
*Hyeonji Kim,Sujeong Oh,Sanghack Lee*

Main category: cs.CL

TL;DR: 提出了一种因果框架，用于分析和缓解RLHF奖励模型中的长度偏差，通过反事实数据增强方法，生成旨在将内容质量与冗长性隔离的响应对，来训练奖励模型。


<details>
  <summary>Details</summary>
Motivation: RLHF训练的奖励模型通常表现出长度偏差，即系统性地偏爱更长的响应，将冗长与质量混为一谈。

Method: 提出了一种反事实数据增强方法，生成具有相似内容但长度不同的配对，以及内容不同但长度相似的配对，用于训练奖励模型。

Result: 该方法减少了奖励分配中的长度偏差，并使策略模型产生更简洁、更注重内容的输出。

Conclusion: 该方法有效地减少了长度偏差，提高了RLHF管道中奖励模型的鲁棒性和内容敏感性。

Abstract: Reinforcement learning from human feedback (RLHF) is widely used to align large language models (LLMs) with human preferences. However, RLHF-trained reward models often exhibit length bias -- a systematic tendency to favor longer responses by conflating verbosity with quality. We propose a causal framework for analyzing and mitigating length bias in RLHF reward modeling. Central to our approach is a counterfactual data augmentation method that generates response pairs designed to isolate content quality from verbosity. These counterfactual examples are then used to train the reward model, enabling it to assess responses based on content quality independently of verbosity. Specifically, we construct (1) length-divergent pairs with similar content and (2) content-divergent pairs of similar length. Empirical evaluations show that our method reduces length bias in reward assignment and leads to more concise, content-focused outputs from the policy model. These findings demonstrate that the proposed approach effectively reduces length bias and improves the robustness and content sensitivity of reward modeling in RLHF pipelines.

</details>


### [39] [MMWOZ: Building Multimodal Agent for Task-oriented Dialogue](https://arxiv.org/abs/2511.12586)
*Pu-Hai Yang,Heyan Huang,Heng-Da Xu,Fanshu Sun,Xian-Ling Mao,Chaoxu Mu*

Main category: cs.CL

TL;DR: 本文介绍了一个新的多模态对话数据集 MMWOZ，旨在弥合传统面向任务的对话系统在实际应用中的差距。该数据集通过开发 Web 风格的 GUI 并将原始数据集中的对话状态和系统动作转换为 GUI 的操作指令来构建。此外，作者还提出了一个名为 MATE 的新型多模态模型作为 MMWOZ 数据集的基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统面向任务的对话系统在实际应用中存在局限性，因为现实世界中存在大量的前端 GUI 且缺乏定制的后端 API。

Method: 1. 开发一个 Web 风格的 GUI 作为前端。
2. 设计一个自动脚本，将原始数据集中的对话状态和系统动作转换为 GUI 的操作指令。
3. 收集网页快照以及相应的操作指令。
4. 提出一个名为 MATE 的新型多模态模型作为 MMWOZ 数据集的基线模型。

Result: 构建了一个新的多模态对话数据集 MMWOZ，并提出了一个名为 MATE 的新型多模态模型。

Conclusion: 本文旨在研究如何构建一个实用的多模态 agent，用于面向任务的对话。

Abstract: Task-oriented dialogue systems have garnered significant attention due to their conversational ability to accomplish goals, such as booking airline tickets for users. Traditionally, task-oriented dialogue systems are conceptualized as intelligent agents that interact with users using natural language and have access to customized back-end APIs. However, in real-world scenarios, the widespread presence of front-end Graphical User Interfaces (GUIs) and the absence of customized back-end APIs create a significant gap for traditional task-oriented dialogue systems in practical applications. In this paper, to bridge the gap, we collect MMWOZ, a new multimodal dialogue dataset that is extended from MultiWOZ 2.3 dataset. Specifically, we begin by developing a web-style GUI to serve as the front-end. Next, we devise an automated script to convert the dialogue states and system actions from the original dataset into operation instructions for the GUI. Lastly, we collect snapshots of the web pages along with their corresponding operation instructions. In addition, we propose a novel multimodal model called MATE (Multimodal Agent for Task-oriEnted dialogue) as the baseline model for the MMWOZ dataset. Furthermore, we conduct comprehensive experimental analysis using MATE to investigate the construction of a practical multimodal agent for task-oriented dialogue.

</details>


### [40] [Group-Aware Reinforcement Learning for Output Diversity in Large Language Models](https://arxiv.org/abs/2511.12596)
*Oron Anschel,Alon Shoshan,Adam Botach,Shunit Haviv Hakimi,Asaf Gendler,Emanuel Ben Baruch,Nadav Bhonker,Igor Kviatkovsky,Manoj Aggarwal,Gerard Medioni*

Main category: cs.CL

TL;DR: 本文介绍了一种名为 Group-Aware Policy Optimization (GAPO) 的方法，旨在解决大型语言模型 (LLM) 中常见的模式崩溃问题，即模型倾向于重复生成相同的几个答案，从而限制了答案的多样性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 经常出现模式崩溃问题，即使存在许多有效的答案，也总是重复生成相同的几个答案，这限制了它们在各种任务中的多样性。

Method: 我们引入了 Group-Aware Policy Optimization (GAPO)，它是最近流行的 Group Relative Policy Optimization (GRPO) 的一个简单扩展，可以计算整个组的奖励。GAPO 能够从组级别的属性（如多样性和覆盖率）中学习。我们使用频率感知奖励函数来演示 GAPO，该函数鼓励在有效的 LLM 完成中进行均匀采样。

Result: GAPO 训练的模型产生有效且更多样化的模型响应。GAPO 可以推广到开放式提示，并在不影响标准 LLM 基准测试（GSM8K、MATH、HumanEval、MMLU-Pro）准确性的情况下提高响应多样性。

Conclusion: GAPO 是一种有效的方法，可以在不影响准确性的前提下，提高大型语言模型生成答案的多样性。

Abstract: Large Language Models (LLMs) often suffer from mode collapse, repeatedly generating the same few completions even when many valid answers exist, limiting their diversity across a wide range of tasks. We introduce Group-Aware Policy Optimization (GAPO), a simple extension of the recent and popular Group Relative Policy Optimization (GRPO) that computes rewards over the group as a whole. GAPO enables learning from the group-level properties such as diversity and coverage. We demonstrate GAPO using a frequency-aware reward function that encourages uniform sampling over valid LLM completions, and show that GAPO-trained models produce valid and more diverse model responses. Beyond this setup, GAPO generalizes to open-ended prompts and improves response diversity without compromising accuracy on standard LLM benchmarks (GSM8K, MATH, HumanEval, MMLU-Pro). Our code will be made publicly available.

</details>


### [41] [Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data](https://arxiv.org/abs/2511.12609)
*Yunxin Li,Xinyu Chen,Shenyuan Jiang,Haoyuan Shi,Zhenyu Liu,Xuanyu Zhang,Nanhao Deng,Zhenran Xu,Yicheng Ma,Meishan Zhang,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: Uni-MoE 2.0 是一个开源全模态大模型，提升了 Lychee 系列模型在多模态理解、推理和生成方面的能力。


<details>
  <summary>Details</summary>
Motivation: 构建一个在语言理解方面更强的多模态模型。

Method: 1. 动态容量的混合专家（MoE）设计。2. 渐进式训练策略，并通过迭代强化策略进行增强。3. 精心策划的多模态数据匹配技术。

Result: 在 85 个基准测试中，Uni-MoE 2.0 达到了 SOTA 或极具竞争力的性能，在超过 50 个基准测试中超过了 Qwen2.5-Omni。

Conclusion: Uni-MoE 2.0 在视频理解、全模态理解和视听推理方面表现出显著优势，并在长格式语音处理、低级图像处理和可控生成方面取得了领先。

Abstract: We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee's Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the Qwen2.5-7B dense architecture, we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech. Architecturally, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer. For training, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning. Data-wise, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues. Extensive evaluation across 85 benchmarks demonstrates that our model achieves SOTA or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (reducing WER by 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.

</details>


### [42] [Knots: A Large-Scale Multi-Agent Enhanced Expert-Annotated Dataset and LLM Prompt Optimization for NOTAM Semantic Parsing](https://arxiv.org/abs/2511.12630)
*Maoqi Liu,Quan Fang,Yang Yang,Can Zhao,Kaiquan Cai*

Main category: cs.CL

TL;DR: 本研究提出了一种新的NOTAM语义分析任务，旨在通过语义推理和航空领域知识整合，实现对NOTAMs的深层语义理解。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在诸如分类和命名实体识别等表面任务，缺乏深层语义理解。

Method: 构建了一个高质量的数据集Knots，并系统地评估了一系列prompt-engineering策略和模型自适应技术。

Result: 在航空文本理解和处理方面取得了显著的改进。

Conclusion: 证明了该方法的有效性，并为自动化NOTAM分析系统提供了有价值的见解。

Abstract: Notice to Air Missions (NOTAMs) serve as a critical channel for disseminating key flight safety information, yet their complex linguistic structures and implicit reasoning pose significant challenges for automated parsing. Existing research mainly focuses on surface-level tasks such as classification and named entity recognition, lacking deep semantic understanding. To address this gap, we propose NOTAM semantic parsing, a task emphasizing semantic inference and the integration of aviation domain knowledge to produce structured, inference-rich outputs. To support this task, we construct Knots (Knowledge and NOTAM Semantics), a high-quality dataset of 12,347 expert-annotated NOTAMs covering 194 Flight Information Regions, enhanced through a multi-agent collaborative framework for comprehensive field discovery. We systematically evaluate a wide range of prompt-engineering strategies and model-adaptation techniques, achieving substantial improvements in aviation text understanding and processing. Our experimental results demonstrate the effectiveness of the proposed approach and offer valuable insights for automated NOTAM analysis systems. Our code is available at: https://github.com/Estrellajer/Knots.

</details>


### [43] [Reason-KE++: Aligning the Process, Not Just the Outcome, for Faithful LLM Knowledge Editing](https://arxiv.org/abs/2511.12661)
*Yuchen Wu,Liang Ding,Li Shen,Dacheng Tao*

Main category: cs.CL

TL;DR: Reason-KE++是一个SFT+RL框架，通过阶段感知奖励机制，提高LLM在复杂推理任务中对新知识的忠实度，优于单纯的结果监督RL方法。


<details>
  <summary>Details</summary>
Motivation: 现有SFT方法在复杂推理任务中存在“忠实性差距”，LLM会忽略新知识而依赖自身参数先验，导致事实幻觉。

Method: 提出Reason-KE++框架，核心是阶段感知奖励机制，对中间推理步骤进行密集监督。

Result: 在MQUAKE-CF-3k数据集上达到95.48%的新SOTA，比之前的模型提高了5.28%。

Conclusion: 对于复杂任务，对齐推理过程对于构建值得信赖的LLM至关重要。

Abstract: Aligning Large Language Models (LLMs) to be faithful to new knowledge in complex, multi-hop reasoning tasks is a critical, yet unsolved, challenge. We find that SFT-based methods, e.g., Reason-KE, while state-of-the-art, suffer from a "faithfulness gap": they optimize for format mimicry rather than sound reasoning. This gap enables the LLM's powerful parametric priors to override new contextual facts, resulting in critical factual hallucinations (e.g., incorrectly reasoning "Houston" from "NASA" despite an explicit edit). To solve this core LLM alignment problem, we propose Reason-KE++, an SFT+RL framework that instills process-level faithfulness. Its core is a Stage-aware Reward mechanism that provides dense supervision for intermediate reasoning steps (e.g., Decomposition, Sub-answer Correctness). Crucially, we identify that naive outcome-only RL is a deceptive trap for LLM alignment: it collapses reasoning integrity (e.g., 19.00% Hop acc) while superficially boosting final accuracy. Our process-aware framework sets a new SOTA of 95.48% on MQUAKE-CF-3k (+5.28%), demonstrating that for complex tasks, aligning the reasoning process is essential for building trustworthy LLMs.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [44] [MovSemCL: Movement-Semantics Contrastive Learning for Trajectory Similarity](https://arxiv.org/abs/2511.12061)
*Zhichen Lai,Hua Lu,Huan Li,Jialiang Li,Christian S. Jensen*

Main category: cs.CV

TL;DR: MovSemCL是一种用于轨迹相似性计算的运动语义对比学习框架，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有方法在轨迹语义和层次结构建模不足，计算成本高，使用物理上不合理的增强。

Method: MovSemCL将原始GPS轨迹转换为运动语义特征，然后将其分割成小块。接下来，MovSemCL采用内部和外部补丁注意力来编码本地和全局轨迹模式。此外，MovSemCL包括一种曲率引导的增强策略，该策略保留信息丰富的片段并屏蔽冗余片段，从而生成物理上合理的增强视图。

Result: MovSemCL在实际数据集上的实验表明，MovSemCL能够胜过最先进的方法，在相似性搜索任务中实现接近理想值1的平均排名，并在启发式近似中提高高达20.3%，同时将推理延迟降低高达43.4%。

Conclusion: MovSemCL是一种用于轨迹相似性计算的有效框架。

Abstract: Trajectory similarity computation is fundamental functionality that is used for, e.g., clustering, prediction, and anomaly detection. However, existing learning-based methods exhibit three key limitations: (1) insufficient modeling of trajectory semantics and hierarchy, lacking both movement dynamics extraction and multi-scale structural representation; (2) high computational costs due to point-wise encoding; and (3) use of physically implausible augmentations that distort trajectory semantics. To address these issues, we propose MovSemCL, a movement-semantics contrastive learning framework for trajectory similarity computation. MovSemCL first transforms raw GPS trajectories into movement-semantics features and then segments them into patches. Next, MovSemCL employs intra- and inter-patch attentions to encode local as well as global trajectory patterns, enabling efficient hierarchical representation and reducing computational costs. Moreover, MovSemCL includes a curvature-guided augmentation strategy that preserves informative segments (e.g., turns and intersections) and masks redundant ones, generating physically plausible augmented views. Experiments on real-world datasets show that MovSemCL is capable of outperforming state-of-the-art methods, achieving mean ranks close to the ideal value of 1 at similarity search tasks and improvements by up to 20.3% at heuristic approximation, while reducing inference latency by up to 43.4%.

</details>


### [45] [Psychological stress during Examination and its estimation by handwriting in answer script](https://arxiv.org/abs/2511.11633)
*Abhijeet Kumar,Chetan Agarwal,Pronoy B. Neogi,Mayank Goswami*

Main category: cs.CV

TL;DR: 本研究融合笔迹学和人工智能，通过分析学生手写试卷来量化心理压力水平。


<details>
  <summary>Details</summary>
Motivation: 旨在超越传统评分系统，更深入地了解考试期间的认知和情绪状态。

Method: 利用光学字符识别和基于Transformer的情感分析模型，结合高分辨率图像处理、TrOCR和基于RoBERTa模型的情感熵融合，生成数值压力指数。

Result: 通过五模型投票机制和无监督异常检测实现稳健性。

Conclusion: 为学术取证提供了一个创新框架。

Abstract: This research explores the fusion of graphology and artificial intelligence to quantify psychological stress levels in students by analyzing their handwritten examination scripts. By leveraging Optical Character Recognition and transformer based sentiment analysis models, we present a data driven approach that transcends traditional grading systems, offering deeper insights into cognitive and emotional states during examinations. The system integrates high resolution image processing, TrOCR, and sentiment entropy fusion using RoBERTa based models to generate a numerical Stress Index. Our method achieves robustness through a five model voting mechanism and unsupervised anomaly detection, making it an innovative framework in academic forensics.

</details>


### [46] [Real-time pothole detection with onboard sensors and camera on vehicles](https://arxiv.org/abs/2511.11643)
*Aswath Muthuselvam,Jeevak Raj S,Mohanaprasad K*

Main category: cs.CV

TL;DR: 提出了一种利用车辆传感器实时识别道路坑洼的方法，以实现大规模的坑洼分析和管理。


<details>
  <summary>Details</summary>
Motivation: 随着道路上车辆数量的增加，需要更频繁地获取路况信息，以确保交通顺畅。即使是道路上最小的裂缝，也可能因路面温度变化和车辆行驶的压力而变成大坑洼。

Method: 使用SVM分类器检测坑洼。

Result: 基于从一段2公里长的本地道路收集的数据，实现了98.1%的准确率，该道路上有26个坑洼。

Conclusion: 利用车辆传感器可以有效地实时识别道路坑洼，为大规模坑洼分析和管理提供数据支持。

Abstract: Road conditions play an important role in our everyday commute. With the proliferating number of vehicles on the road each year, it has become necessary to access the road conditions very frequently, this would ensure that the traffic also flows smoothly. Even the smallest crack in the road could be easily be chipped into a large pothole due to changing surface temperatures of the road and from the force of vehicles riding over it. In this paper, we have addressed how we could better identify these potholes in realtime with the help of onboard sensors in vehicles so that the data could be useful for analysis and better management of potholes on a large scale. For the implementation, we used an SVM classifier to detect potholes, we achieved 98.1% accuracy based on data collected from a local road for about 2 km which had 26 potholes distributed along the road. Code is available at: https://github.com/aswathselvam/Potholes

</details>


### [47] [A Method for Identifying Farmland System Habitat Types Based on the Dynamic-Weighted Feature Fusion Network Model](https://arxiv.org/abs/2511.11659)
*Kesong Zheng,Zhi Song,Peizhou Li,Shuyi Yao,Zhenxing Bian*

Main category: cs.CV

TL;DR: 本研究针对耕地生态系统栖息地分类标准缺失、栖息地类型覆盖不全以及现有模型无法有效整合语义和纹理特征等问题，提出了一个动态加权特征融合网络（DWFF-Net），用于提高多尺度栖息地的分割精度。


<details>
  <summary>Details</summary>
Motivation: 现有耕地生态系统栖息地缺乏标准化分类系统，栖息地类型覆盖不全，现有模型无法有效整合语义和纹理特征，导致分割精度不足，多尺度栖息地边界模糊。

Method: 1. 构建包含15类耕地系统栖息地的超高分辨率遥感图像数据集。
2. 提出动态加权特征融合网络（DWFF-Net），利用冻结参数的DINOv3提取基础特征，引入数据级自适应动态加权策略进行特征融合，并采用混合损失函数优化模型训练。

Result: 在构建的数据集上，该模型实现了0.6979的平均交并比（mIoU）和0.8049的F1分数，优于基线网络。消融研究证实了多层特征融合的互补性，有效提高了田埂等微栖息地类别的IoU。

Conclusion: 本研究建立了一个基于自适应多层特征融合的耕地系统栖息地识别框架，实现了亚米级精度的栖息地 mapping，为耕地景观中的精细栖息地监测提供了强大的技术支持。

Abstract: Addressing the current lack of a standardized habitat classification system for cultivated land ecosystems, incomplete coverage of habitat types, and the inability of existing models to effectively integrate semantic and texture features-resulting in insufficient segmentation accuracy and blurred boundaries for multi-scale habitats (e.g., large-scale field plots and micro-habitats)-this study developed a comprehensively annotated ultra-high-resolution remote sensing image dataset encompassing 15 categories of cultivated land system habitats. Furthermore, we propose a Dynamic-Weighted Feature Fusion Network (DWFF-Net). The encoder of this model utilizes a frozen-parameter DINOv3 to extract foundational features. By analyzing the relationships between different category images and feature maps, we introduce a data-level adaptive dynamic weighting strategy for feature fusion. The decoder incorporates a dynamic weight computation network to achieve thorough integration of multi-layer features, and a hybrid loss function is adopted to optimize model training. Experimental results on the constructed dataset demonstrate that the proposed model achieves a mean Intersection over Union (mIoU) of 0.6979 and an F1-score of 0.8049, outperforming the baseline network by 0.021 and 0.0161, respectively. Ablation studies further confirm the complementary nature of multi-layer feature fusion, which effectively improves the IoU for micro-habitat categories such as field ridges. This study establishes a habitat identification framework for cultivated land systems based on adaptive multi-layer feature fusion, enabling sub-meter precision habitat mapping at a low cost and providing robust technical support for fine-grained habitat monitoring in cultivated landscapes.

</details>


### [48] [AGENet: Adaptive Edge-aware Geodesic Distance Learning for Few-Shot Medical Image Segmentation](https://arxiv.org/abs/2511.11662)
*Ziyuan Gao*

Main category: cs.CV

TL;DR: 提出了AGENet（自适应测地线边缘感知网络），用于解决医学图像分割中数据集不足的问题，尤其是在解剖结构相似的区域。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割需要大量的标注数据，这成为了临床应用的重要瓶颈。现有的few-shot分割方法在医学图像的精确边界划分方面表现不佳，尤其是在解剖结构相似的区域。

Method: 通过边缘感知测地距离学习来结合空间关系。该框架包括：边缘感知测地距离学习模块、自适应原型提取模块和自适应参数学习模块。

Result: 在多个医学影像数据集上进行了大量实验，结果表明该方法优于现有方法，并减少了边界误差，同时保持了计算效率。

Conclusion: 该方法非常适合需要精确分割且标注数据有限的临床应用。

Abstract: Medical image segmentation requires large annotated datasets, creating a significant bottleneck for clinical applications. While few-shot segmentation methods can learn from minimal examples, existing approaches demonstrate suboptimal performance in precise boundary delineation for medical images, particularly when anatomically similar regions appear without sufficient spatial context. We propose AGENet (Adaptive Geodesic Edge-aware Network), a novel framework that incorporates spatial relationships through edge-aware geodesic distance learning. Our key insight is that medical structures follow predictable geometric patterns that can guide prototype extraction even with limited training data. Unlike methods relying on complex architectural components or heavy neural networks, our approach leverages computationally lightweight geometric modeling. The framework combines three main components: (1) An edge-aware geodesic distance learning module that respects anatomical boundaries through iterative Fast Marching refinement, (2) adaptive prototype extraction that captures both global structure and local boundary details via spatially-weighted aggregation, and (3) adaptive parameter learning that automatically adjusts to different organ characteristics. Extensive experiments across diverse medical imaging datasets demonstrate improvements over state-of-the-art methods. Notably, our method reduces boundary errors compared to existing approaches while maintaining computational efficiency, making it highly suitable for clinical applications requiring precise segmentation with limited annotated data.

</details>


### [49] [EPSegFZ: Efficient Point Cloud Semantic Segmentation for Few- and Zero-Shot Scenarios with Language Guidance](https://arxiv.org/abs/2511.11700)
*Jiahui Wang,Haiyue Zhu,Haoren Guo,Abdullah Al Mamun,Cheng Xiang,Tong Heng Lee*

Main category: cs.CV

TL;DR: 提出了一种名为EPSegFZ的新型免预训练网络，用于少样本和零样本场景下的高效点云语义分割。


<details>
  <summary>Details</summary>
Motivation: 现有方法过度依赖预训练，缺乏灵活性和适应性，并且未能充分利用支持集中的文本注释等信息。

Method: EPSegFZ包含三个关键组件：Prototype-Enhanced Registers Attention (ProERA) 模块和 Dual Relative Positional Encoding (DRPE) 机制，用于改进特征提取和构建精确的查询-原型对应关系；Language-Guided Prototype Embedding (LGPE) 模块，有效利用支持集的文本信息。

Result: 在S3DIS和ScanNet基准测试中，该方法分别超过了现有最佳方法5.68%和3.82%。

Conclusion: EPSegFZ网络在少样本和零样本点云语义分割任务中表现出色，优于现有技术。

Abstract: Recent approaches for few-shot 3D point cloud semantic segmentation typically require a two-stage learning process, i.e., a pre-training stage followed by a few-shot training stage. While effective, these methods face overreliance on pre-training, which hinders model flexibility and adaptability. Some models tried to avoid pre-training yet failed to capture ample information. In addition, current approaches focus on visual information in the support set and neglect or do not fully exploit other useful data, such as textual annotations. This inadequate utilization of support information impairs the performance of the model and restricts its zero-shot ability. To address these limitations, we present a novel pre-training-free network, named Efficient Point Cloud Semantic Segmentation for Few- and Zero-shot scenarios. Our EPSegFZ incorporates three key components. A Prototype-Enhanced Registers Attention (ProERA) module and a Dual Relative Positional Encoding (DRPE)-based cross-attention mechanism for improved feature extraction and accurate query-prototype correspondence construction without pre-training. A Language-Guided Prototype Embedding (LGPE) module that effectively leverages textual information from the support set to improve few-shot performance and enable zero-shot inference. Extensive experiments show that our method outperforms the state-of-the-art method by 5.68% and 3.82% on the S3DIS and ScanNet benchmarks, respectively.

</details>


### [50] [Task-Aware 3D Affordance Segmentation via 2D Guidance and Geometric Refinement](https://arxiv.org/abs/2511.11702)
*Lian He,Meng Liu,Qilang Ye,Yu Zhou,Xiang Deng,Gangyi Ding*

Main category: cs.CV

TL;DR: 本文提出了一种名为TASA的几何优化框架，用于从自然语言指令中理解3D场景级的可供性，该框架以粗到细的方式联合利用2D语义线索和3D几何推理。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要集中在对象级别的可供性，或者仅仅将2D预测提升到3D，忽略了点云中丰富的几何结构信息，并导致高计算成本。为了解决这些局限性。

Method: TASA框架包含一个任务感知的2D可供性检测模块，用于从语言和视觉输入中识别可操作的点，指导任务相关视图的选择。以及一个3D可供性细化模块，用于整合2D语义先验知识与局部3D几何信息，从而产生精确和空间连贯的3D可供性掩码。

Result: 在SceneFun3D上的实验表明，TASA在场景级可供性分割的准确性和效率方面均显著优于基线方法。

Conclusion: TASA框架在场景级可供性分割任务中表现出色，证明了其有效性。

Abstract: Understanding 3D scene-level affordances from natural language instructions is essential for enabling embodied agents to interact meaningfully in complex environments. However, this task remains challenging due to the need for semantic reasoning and spatial grounding. Existing methods mainly focus on object-level affordances or merely lift 2D predictions to 3D, neglecting rich geometric structure information in point clouds and incurring high computational costs. To address these limitations, we introduce Task-Aware 3D Scene-level Affordance segmentation (TASA), a novel geometry-optimized framework that jointly leverages 2D semantic cues and 3D geometric reasoning in a coarse-to-fine manner. To improve the affordance detection efficiency, TASA features a task-aware 2D affordance detection module to identify manipulable points from language and visual inputs, guiding the selection of task-relevant views. To fully exploit 3D geometric information, a 3D affordance refinement module is proposed to integrate 2D semantic priors with local 3D geometry, resulting in accurate and spatially coherent 3D affordance masks. Experiments on SceneFun3D demonstrate that TASA significantly outperforms the baselines in both accuracy and efficiency in scene-level affordance segmentation.

</details>


### [51] [LE-CapsNet: A Light and Enhanced Capsule Network](https://arxiv.org/abs/2511.11708)
*Pouya Shiri,Amirali Baniasadi*

Main category: cs.CV

TL;DR: LE-CapsNet: A light, enhanced, and accurate CapsNet variant.


<details>
  <summary>Details</summary>
Motivation: CapsNet is slow, resource-hungry, and sometimes less accurate than CNNs.

Method: Propose LE-CapsNet, a modified CapsNet architecture.

Result: LE-CapsNet achieves 76.73% accuracy on CIFAR-10 (4x faster than CapsNet) and 94.3% on AffNIST (vs. CapsNet's 90.52%).

Conclusion: LE-CapsNet is faster, more accurate, and more robust than CapsNet, especially on transformed images.

Abstract: Capsule Network (CapsNet) classifier has several advantages over CNNs, including better detection of images containing overlapping categories and higher accuracy on transformed images. Despite the advantages, CapsNet is slow due to its different structure. In addition, CapsNet is resource-hungry, includes many parameters and lags in accuracy compared to CNNs. In this work, we propose LE-CapsNet as a light, enhanced and more accurate variant of CapsNet. Using 3.8M weights, LECapsNet obtains 76.73% accuracy on the CIFAR-10 dataset while performing inference 4x faster than CapsNet. In addition, our proposed network is more robust at detecting images with affine transformations compared to CapsNet. We achieve 94.3% accuracy on the AffNIST dataset (compared to CapsNet 90.52%).

</details>


### [52] [Target-Balanced Score Distillation](https://arxiv.org/abs/2511.11710)
*Zhou Xu,Qi Wang,Yuxiao Yang,Luyuan Zhang,Zhang Liang,Yang Li*

Main category: cs.CV

TL;DR: 提出了一种名为 Target-Balanced Score Distillation (TBSD) 的新方法，用于解决 Score Distillation Sampling (SDS) 在 3D 资产生成中存在的过饱和、过度平滑以及纹理优化与形状扭曲之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有的 SDS 方法在 3D 资产生成中存在过饱和、过度平滑的问题，并且在纹理优化和形状保持之间存在权衡。Target Negative Prompts (TNP) 虽然可以增强纹理的真实感，但会导致形状扭曲。

Method: 通过系统分析发现，负面提示的使用是导致上述权衡的关键因素。TBSD 将生成过程 формулирует 为多目标优化问题，并引入自适应策略来有效解决纹理优化和形状扭曲之间的权衡。

Result: TBSD 在纹理保真度和几何形状精度方面显著优于现有的最先进方法。

Conclusion: TBSD 能够生成具有高保真纹理和几何形状精确的 3D 资产，有效解决了 SDS 方法中的纹理优化与形状扭曲的权衡问题。

Abstract: Score Distillation Sampling (SDS) enables 3D asset generation by distilling priors from pretrained 2D text-to-image diffusion models, but vanilla SDS suffers from over-saturation and over-smoothing. To mitigate this issue, recent variants have incorporated negative prompts. However, these methods face a critical trade-off: limited texture optimization, or significant texture gains with shape distortion. In this work, we first conduct a systematic analysis and reveal that this trade-off is fundamentally governed by the utilization of the negative prompts, where Target Negative Prompts (TNP) that embed target information in the negative prompts dramatically enhancing texture realism and fidelity but inducing shape distortions. Informed by this key insight, we introduce the Target-Balanced Score Distillation (TBSD). It formulates generation as a multi-objective optimization problem and introduces an adaptive strategy that effectively resolves the aforementioned trade-off. Extensive experiments demonstrate that TBSD significantly outperforms existing state-of-the-art methods, yielding 3D assets with high-fidelity textures and geometrically accurate shape.

</details>


### [53] [CompressNAS : A Fast and Efficient Technique for Model Compression using Decomposition](https://arxiv.org/abs/2511.11716)
*Sudhakar Sah,Nikhil Chabbra,Matthieu Durnerin*

Main category: cs.CV

TL;DR: 这篇论文提出了一种名为CompressNAS的框架，用于压缩深度卷积神经网络，使其更易于在微控制器和轻量级神经处理单元上部署。


<details>
  <summary>Details</summary>
Motivation: 现有的低秩张量分解方法在选择秩时通常忽略压缩和准确性之间的全局权衡，导致压缩效果不佳。

Method: CompressNAS将秩选择视为一个全局搜索问题，并使用快速准确性估计器来评估候选分解。

Result: 在ImageNet上，CompressNAS将ResNet-18压缩了8倍，准确率下降不到4%；在COCO上，YOLOv5s实现了2倍压缩且无精度下降，YOLOv5n实现了2倍压缩且精度下降2.5%。

Conclusion: 该论文提出了一种新的压缩模型系列，STResNet，与其他高效模型相比具有竞争优势。

Abstract: Deep Convolutional Neural Networks (CNNs) are increasingly difficult to deploy on microcontrollers (MCUs) and lightweight NPUs (Neural Processing Units) due to their growing size and compute demands. Low-rank tensor decomposition, such as Tucker factorization, is a promising way to reduce parameters and operations with reasonable accuracy loss. However, existing approaches select ranks locally and often ignore global trade-offs between compression and accuracy. We introduce CompressNAS, a MicroNAS-inspired framework that treats rank selection as a global search problem. CompressNAS employs a fast accuracy estimator to evaluate candidate decompositions, enabling efficient yet exhaustive rank exploration under memory and accuracy constraints. In ImageNet, CompressNAS compresses ResNet-18 by 8x with less than 4% accuracy drop; on COCO, we achieve 2x compression of YOLOv5s without any accuracy drop and 2x compression of YOLOv5n with a 2.5% drop. Finally, we present a new family of compressed models, STResNet, with competitive performance compared to other efficient models.

</details>


### [54] [AdaptFly: Prompt-Guided Adaptation of Foundation Models for Low-Altitude UAV Networks](https://arxiv.org/abs/2511.11720)
*Jiao Chen,Haoyi Wang,Jianhua Tang,Junyi Wang*

Main category: cs.CV

TL;DR: AdaptFly is a prompt-guided test-time adaptation framework that adjusts segmentation models without weight updates.


<details>
  <summary>Details</summary>
Motivation: Segmentation foundation models deteriorate quickly under weather, lighting, and viewpoint drift, and resource-limited UAVs cannot run gradient-based test-time adaptation, while resource-massive UAVs adapt independently, wasting shared experience.

Method: AdaptFly features two complementary adaptation modes: lightweight token-prompt retrieval from a shared global memory for resource-limited UAVs, and gradient-free sparse visual prompt optimization via Covariance Matrix Adaptation Evolution Strategy for resource-massive UAVs. An activation-statistic detector triggers adaptation, while cross-UAV knowledge pool consolidates prompt knowledge.

Result: AdaptFly significantly improves segmentation accuracy and robustness over static models and state-of-the-art TTA baselines on UAVid and VDD benchmarks, along with real-world UAV deployments under diverse weather conditions.

Conclusion: AdaptFly provides a practical path to resilient, communication-efficient perception in the emerging low-altitude economy.

Abstract: Low-altitude Unmanned Aerial Vehicle (UAV) networks rely on robust semantic segmentation as a foundational enabler for distributed sensing-communication-control co-design across heterogeneous agents within the network. However, segmentation foundation models deteriorate quickly under weather, lighting, and viewpoint drift. Resource-limited UAVs cannot run gradient-based test-time adaptation, while resource-massive UAVs adapt independently, wasting shared experience. To address these challenges, we propose AdaptFly, a prompt-guided test-time adaptation framework that adjusts segmentation models without weight updates. AdaptFly features two complementary adaptation modes. For resource-limited UAVs, it employs lightweight token-prompt retrieval from a shared global memory. For resource-massive UAVs, it uses gradient-free sparse visual prompt optimization via Covariance Matrix Adaptation Evolution Strategy. An activation-statistic detector triggers adaptation, while cross-UAV knowledge pool consolidates prompt knowledge and enables fleet-wide collaboration with negligible bandwidth overhead. Extensive experiments on UAVid and VDD benchmarks, along with real-world UAV deployments under diverse weather conditions, demonstrate that AdaptFly significantly improves segmentation accuracy and robustness over static models and state-of-the-art TTA baselines. The results highlight a practical path to resilient, communication-efficient perception in the emerging low-altitude economy.

</details>


### [55] [Do Blind Spots Matter for Word-Referent Mapping? A Computational Study with Infant Egocentric Video](https://arxiv.org/abs/2511.11725)
*Zekai Shi,Zhixi Cai,Kalin Stefanov*

Main category: cs.CV

TL;DR: 提出了一种自监督和生物学上合理的策略来学习强大的视觉表征，用于学习单词指称映射。


<details>
  <summary>Details</summary>
Motivation: 儿童通常在 6 到 9 个月大时开始学习他们的第一个单词，将口语与视觉指示物联系起来。在没有先验知识的情况下，第一次遇到的单词可以用无数种方式解释；它可能指的是环境中任何物体、它们的组成部分或属性。

Method: 使用来自一个孩子的经验的纵向、以自我为中心和生态有效的数据，我们提出了一种基于掩码自动编码器的视觉主干，该主干结合了关于人眼盲点的知识来定义一种新的掩码策略。这种掩码和重建方法试图模仿人脑填补眼睛视野中的空白的方式。

Result: 所提出的生物学上合理的掩蔽策略至少与随机掩蔽一样有效，可以从跨情境和时间扩展的情节中学习单词指称映射。

Conclusion: 该研究提出了一种新的视觉表征学习方法，该方法在生物学上是合理的，并且在学习单词指称映射方面是有效的。

Abstract: Typically, children start to learn their first words between 6 and 9 months, linking spoken utterances to their visual referents. Without prior knowledge, a word encountered for the first time can be interpreted in countless ways; it might refer to any of the objects in the environment, their components, or attributes. Using longitudinal, egocentric, and ecologically valid data from the experience of one child, in this work, we propose a self-supervised and biologically plausible strategy to learn strong visual representations. Our masked autoencoder-based visual backbone incorporates knowledge about the blind spot in human eyes to define a novel masking strategy. This mask and reconstruct approach attempts to mimic the way the human brain fills the gaps in the eyes' field of view. This represents a significant shift from standard random masking strategies, which are difficult to justify from a biological perspective. The pretrained encoder is utilized in a contrastive learning-based video-text model capable of acquiring word-referent mappings. Extensive evaluation suggests that the proposed biologically plausible masking strategy is at least as effective as random masking for learning word-referent mappings from cross-situational and temporally extended episodes.

</details>


### [56] [GROVER: Graph-guided Representation of Omics and Vision with Expert Regulation for Adaptive Spatial Multi-omics Fusion](https://arxiv.org/abs/2511.11730)
*Yongjun Xiao,Dian Meng,Xinlei Huang,Yanran Liu,Shiwei Ruan,Ziyue Qiao,Xubin Zheng*

Main category: cs.CV

TL;DR: GROVER框架通过图卷积网络编码器、对比学习策略和动态专家路由机制，自适应地整合空间多组学数据，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 有效建模多模态空间组学数据对于理解组织复杂性和潜在生物学机制至关重要。整合组学与组织病理学图像对于综合疾病组织分析至关重要。然而，组学、成像和空间模式之间存在显著的异质性，对整合提出了重大挑战。

Method: GROVER利用基于Kolmogorov-Arnold Networks的图卷积网络编码器，捕获每个模态与其相关空间结构之间的非线性依赖关系，从而生成表达性的、特定于模态的嵌入。引入了一种spot-feature-pair对比学习策略，显式地优化了每个spot中跨模态的对应关系。设计了一种动态专家路由机制，自适应地选择每个spot的信息模态，同时抑制噪声或低质量的输入。

Result: 在真实世界空间组学数据集上的实验表明，GROVER优于最先进的基线方法，为多模态整合提供了稳健可靠的解决方案。

Conclusion: GROVER框架为空间多组学数据的自适应整合提供了一种有效的方法。

Abstract: Effectively modeling multimodal spatial omics data is critical for understanding tissue complexity and underlying biological mechanisms. While spatial transcriptomics, proteomics, and epigenomics capture molecular features, they lack pathological morphological context. Integrating these omics with histopathological images is therefore essential for comprehensive disease tissue analysis. However, substantial heterogeneity across omics, imaging, and spatial modalities poses significant challenges. Naive fusion of semantically distinct sources often leads to ambiguous representations. Additionally, the resolution mismatch between high-resolution histology images and lower-resolution sequencing spots complicates spatial alignment. Biological perturbations during sample preparation further distort modality-specific signals, hindering accurate integration. To address these challenges, we propose Graph-guided Representation of Omics and Vision with Expert Regulation for Adaptive Spatial Multi-omics Fusion (GROVER), a novel framework for adaptive integration of spatial multi-omics data. GROVER leverages a Graph Convolutional Network encoder based on Kolmogorov-Arnold Networks to capture the nonlinear dependencies between each modality and its associated spatial structure, thereby producing expressive, modality-specific embeddings. To align these representations, we introduce a spot-feature-pair contrastive learning strategy that explicitly optimizes the correspondence across modalities at each spot. Furthermore, we design a dynamic expert routing mechanism that adaptively selects informative modalities for each spot while suppressing noisy or low-quality inputs. Experiments on real-world spatial omics datasets demonstrate that GROVER outperforms state-of-the-art baselines, providing a robust and reliable solution for multimodal integration.

</details>


### [57] [Exposing DeepFakes via Hyperspectral Domain Mapping](https://arxiv.org/abs/2511.11732)
*Aditya Mehta,Swarnim Chaudhary,Pratik Narang,Jagat Sesh Challa*

Main category: cs.CV

TL;DR: 本研究提出了一种名为 HSI-Detect 的新型 Deepfake 检测方法，该方法首先从 RGB 图像重建 31 通道高光谱图像，然后在高光谱域中进行检测。


<details>
  <summary>Details</summary>
Motivation: 现有的 Deepfake 检测方法主要在 RGB 空间中操作，仅分析三个光谱通道，这限制了它们检测细微伪造迹象的能力。

Method: HSI-Detect 是一种两阶段流程，它首先从标准 RGB 输入重建高光谱图像，然后在高光谱域中执行检测。通过将输入表示扩展到更密的光谱带，可以放大在 RGB 域中通常较弱或不可见的操纵伪影。

Result: 在 FaceForensics++ 数据集上的评估表明，HSI-Detect 始终优于仅使用 RGB 的基线方法。

Conclusion: 该研究表明，光谱域映射在 Deepfake 检测方面具有潜力。

Abstract: Modern generative and diffusion models produce highly realistic images that can mislead human perception and even sophisticated automated detection systems. Most detection methods operate in RGB space and thus analyze only three spectral channels. We propose HSI-Detect, a two-stage pipeline that reconstructs a 31-channel hyperspectral image from a standard RGB input and performs detection in the hyperspectral domain. Expanding the input representation into denser spectral bands amplifies manipulation artifacts that are often weak or invisible in the RGB domain, particularly in specific frequency bands. We evaluate HSI-Detect across FaceForensics++ dataset and show the consistent improvements over RGB-only baselines, illustrating the promise of spectral-domain mapping for Deepfake detection.

</details>


### [58] [Toward bilipshiz geometric models](https://arxiv.org/abs/2511.11735)
*Yonatan Sverdlov,Eitan Rosen,Nadav Dym*

Main category: cs.CV

TL;DR: 本文研究了点云神经网络是否保留了点云空间上的对称感知距离，通过双 Lipschitz 等价的概念。


<details>
  <summary>Details</summary>
Motivation: 探讨双 Lipschitz 模型在其他场景中的优势是否适用于点云神经网络。

Method: 考虑了点云上的 Procrustes Matching (PM) 度量和 Hard Gromov Wasserstien 距离，并分析了它们之间的关系。

Result: 证明了这两种距离不是双 Lipschitz 等价的，并推导出流行的不变点云网络与 PM 度量不是双 Lipschitz 的。同时，展示了如何修改这些网络以获得双 Lipschitz 保证。

Conclusion: 通过实验证明了所提出的双 Lipschitz 模型在 3D 点云对应任务中优于标准不变模型。

Abstract: Many neural networks for point clouds are, by design, invariant to the symmetries of this datatype: permutations and rigid motions. The purpose of this paper is to examine whether such networks preserve natural symmetry aware distances on the point cloud spaces, through the notion of bi-Lipschitz equivalence. This inquiry is motivated by recent work in the Equivariant learning literature which highlights the advantages of bi-Lipschitz models in other scenarios.
  We consider two symmetry aware metrics on point clouds: (a) The Procrustes Matching (PM) metric and (b) Hard Gromov Wasserstien distances. We show that these two distances themselves are not bi-Lipschitz equivalent, and as a corollary deduce that popular invariant networks for point clouds are not bi-Lipschitz with respect to the PM metric. We then show how these networks can be modified so that they do obtain bi-Lipschitz guarantees. Finally, we provide initial experiments showing the advantage of the proposed bi-Lipschitz model over standard invariant models, for the tasks of finding correspondences between 3D point clouds.

</details>


### [59] [Concept-RuleNet: Grounded Multi-Agent Neurosymbolic Reasoning in Vision Language Models](https://arxiv.org/abs/2511.11751)
*Sanchit Sinha,Guangzhi Xiong,Zhenghao He,Aidong Zhang*

Main category: cs.CV

TL;DR: 这篇论文介绍了一种新的多智能体系统 Concept-RuleNet，它可以增强视觉基础并保持透明的推理。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型 (VLM) 预测准确率很高，但对决策的原因洞察力不足，并且经常出现幻觉，尤其是在遇到分布外数据时。神经符号框架通过将黑盒感知与可解释的符号推理相结合来解决这个问题，但当前的方法仅从任务标签中提取符号，导致它们在底层视觉数据中的基础薄弱。

Method: 该论文提出了一种多智能体系统 Concept-RuleNet，它恢复了视觉基础，同时保留了透明的推理。该系统首先从训练图像的代表性子集中挖掘有区别的视觉概念。接下来，这些视觉概念被用于调节符号发现，将生成锚定在真实的图像统计数据中，并减轻标签偏差。随后，符号由大型语言模型推理器代理组合成可执行的一阶规则，从而产生可解释的神经符号规则。最后，在推理过程中，视觉验证器代理量化每个符号的存在程度，并与黑盒神经模型的输出一起触发规则执行，从而实现具有显式推理路径的预测。

Result: 在五个基准测试（包括两个具有挑战性的医学成像任务和三个代表性不足的自然图像数据集）上的实验表明，该系统将最先进的神经符号基线提高了平均 5%，同时还将规则中幻觉符号的出现次数减少了高达 50%。

Conclusion: Concept-RuleNet 增强了视觉基础，同时保持了透明的推理，并在多个基准测试中优于现有的神经符号基线。

Abstract: Modern vision-language models (VLMs) deliver impressive predictive accuracy yet offer little insight into 'why' a decision is reached, frequently hallucinating facts, particularly when encountering out-of-distribution data. Neurosymbolic frameworks address this by pairing black-box perception with interpretable symbolic reasoning, but current methods extract their symbols solely from task labels, leaving them weakly grounded in the underlying visual data. In this paper, we introduce a multi-agent system - Concept-RuleNet that reinstates visual grounding while retaining transparent reasoning. Specifically, a multimodal concept generator first mines discriminative visual concepts directly from a representative subset of training images. Next, these visual concepts are utilized to condition symbol discovery, anchoring the generations in real image statistics and mitigating label bias. Subsequently, symbols are composed into executable first-order rules by a large language model reasoner agent - yielding interpretable neurosymbolic rules. Finally, during inference, a vision verifier agent quantifies the degree of presence of each symbol and triggers rule execution in tandem with outputs of black-box neural models, predictions with explicit reasoning pathways. Experiments on five benchmarks, including two challenging medical-imaging tasks and three underrepresented natural-image datasets, show that our system augments state-of-the-art neurosymbolic baselines by an average of 5% while also reducing the occurrence of hallucinated symbols in rules by up to 50%.

</details>


### [60] [Batch Transformer Architecture: Case of Synthetic Image Generation for Emotion Expression Facial Recognition](https://arxiv.org/abs/2511.11754)
*Stanislav Selitskiy*

Main category: cs.CV

TL;DR: 提出了一种新的Transformer变体架构，它采用隐式稀疏风格，关注“重要的”维度（主要成分），而不是像传统Transformer那样关注整个维度中的序列或批次实体。


<details>
  <summary>Details</summary>
Motivation: 为了解决encoder-decoder ANN架构中的瓶颈问题，并提高有限原始数据集的可变性。

Method: 该方法提出了一种Batch Transformers，它关注“重要的”维度或特征选择，从而显著减小瓶颈大小。

Result: 该架构在化妆和遮挡数据集的合成图像生成人脸识别任务中进行了测试，从而提高了有限原始数据集的可变性。

Conclusion: Batch Transformers通过关注重要维度，有效减小了encoder-decoder ANN架构中的瓶颈大小，并在人脸识别任务中提高了数据可变性。

Abstract: A novel Transformer variation architecture is proposed in the implicit sparse style. Unlike "traditional" Transformers, instead of attention to sequential or batch entities in their entirety of whole dimensionality, in the proposed Batch Transformers, attention to the "important" dimensions (primary components) is implemented. In such a way, the "important" dimensions or feature selection allows for a significant reduction of the bottleneck size in the encoder-decoder ANN architectures. The proposed architecture is tested on the synthetic image generation for the face recognition task in the case of the makeup and occlusion data set, allowing for increased variability of the limited original data set.

</details>


### [61] [Image-POSER: Reflective RL for Multi-Expert Image Generation and Editing](https://arxiv.org/abs/2511.11780)
*Hossein Mohebbi,Mohammed Abdulrahman,Yanting Miao,Pascal Poupart,Suraj Kothawade*

Main category: cs.CV

TL;DR: Image-POSER是一个强化学习框架，它协调各种预训练的文本到图像和图像到图像专家，通过动态任务分解处理长格式提示，并通过来自视觉语言模型批评者的结构化反馈来监督每一步的对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成模型无法可靠地执行创意工作流程中典型的长而复杂的提示。

Method: 将图像合成和编辑视为马尔可夫决策过程，学习自适应地结合各种模型优势的专家管道。

Result: Image-POSER在对齐、保真度和美学方面优于基线模型，并且在人工评估中始终更受欢迎。

Conclusion: 强化学习可以使AI系统能够自主地分解、重新排序和组合视觉模型，从而朝着通用视觉助手的方向发展。

Abstract: Recent advances in text-to-image generation have produced strong single-shot models, yet no individual system reliably executes the long, compositional prompts typical of creative workflows. We introduce Image-POSER, a reflective reinforcement learning framework that (i) orchestrates a diverse registry of pretrained text-to-image and image-to-image experts, (ii) handles long-form prompts end-to-end through dynamic task decomposition, and (iii) supervises alignment at each step via structured feedback from a vision-language model critic. By casting image synthesis and editing as a Markov Decision Process, we learn non-trivial expert pipelines that adaptively combine strengths across models. Experiments show that Image-POSER outperforms baselines, including frontier models, across industry-standard and custom benchmarks in alignment, fidelity, and aesthetics, and is consistently preferred in human evaluations. These results highlight that reinforcement learning can endow AI systems with the capacity to autonomously decompose, reorder, and combine visual models, moving towards general-purpose visual assistants.

</details>


### [62] [SOTFormer: A Minimal Transformer for Unified Object Tracking and Trajectory Prediction](https://arxiv.org/abs/2511.11824)
*Zhongping Dong,Pengyang Yu,Shuangjian Li,Liming Chen,Mohand Tahar Kechadi*

Main category: cs.CV

TL;DR: SOTFormer: A constant-memory temporal transformer for single-object tracking and short-term motion forecasting.


<details>
  <summary>Details</summary>
Motivation: Accurate tracking and forecasting are difficult due to occlusion, scale variation, and temporal drift.

Method: A minimal constant-memory temporal transformer unifies detection, tracking, and trajectory prediction. It uses a ground-truth-primed memory and burn-in anchor loss for stable identity propagation. A temporal-attention layer refines embeddings.

Result: On Mini-LaSOT (20%), SOTFormer achieves 76.3 AUC and 53.7 FPS, outperforming other transformers.

Conclusion: SOTFormer achieves real-time inference with fixed GPU memory and outperforms baselines in challenging conditions.

Abstract: Accurate single-object tracking and short-term motion forecasting remain challenging under occlusion, scale variation, and temporal drift, which disrupt the temporal coherence required for real-time perception. We introduce \textbf{SOTFormer}, a minimal constant-memory temporal transformer that unifies object detection, tracking, and short-horizon trajectory prediction within a single end-to-end framework. Unlike prior models with recurrent or stacked temporal encoders, SOTFormer achieves stable identity propagation through a ground-truth-primed memory and a burn-in anchor loss that explicitly stabilizes initialization. A single lightweight temporal-attention layer refines embeddings across frames, enabling real-time inference with fixed GPU memory. On the Mini-LaSOT (20%) benchmark, SOTFormer attains 76.3 AUC and 53.7 FPS (AMP, 4.3 GB VRAM), outperforming transformer baselines such as TrackFormer and MOTRv2 under fast motion, scale change, and occlusion.

</details>


### [63] [MP-GFormer: A 3D-Geometry-Aware Dynamic Graph Transformer Approach for Machining Process Planning](https://arxiv.org/abs/2511.11837)
*Fatemeh Elhambakhsh,Gaurav Ameta,Aditi Roy,Hyunwoong Ko*

Main category: cs.CV

TL;DR: 提出了一种名为 MP-GFormer 的 3D 几何感知动态图 Transformer，用于预测加工操作序列。


<details>
  <summary>Details</summary>
Motivation: 现有的动态图学习方法无法整合零件的三维 (3D) 几何信息，因此在预测加工操作序列时缺乏领域感知。

Method: 该方法通过注意力机制将不断演变的三维几何表示整合到 DGL 中。

Result: 在合成数据集上评估 MP-GFormer，结果表明，与最先进的方法相比，该方法在主要和次要操作预测的准确率方面分别提高了 24% 和 36%。

Conclusion: MP-GFormer有效地提升了加工操作序列预测的准确性，解决了现有方法无法充分利用三维几何信息的局限性。

Abstract: Machining process planning (MP) is inherently complex due to structural and geometrical dependencies among part features and machining operations. A key challenge lies in capturing dynamic interdependencies that evolve with distinct part geometries as operations are performed. Machine learning has been applied to address challenges in MP, such as operation selection and machining sequence prediction. Dynamic graph learning (DGL) has been widely used to model dynamic systems, thanks to its ability to integrate spatio-temporal relationships. However, in MP, while existing DGL approaches can capture these dependencies, they fail to incorporate three-dimensional (3D) geometric information of parts and thus lack domain awareness in predicting machining operation sequences. To address this limitation, we propose MP-GFormer, a 3D-geometry-aware dynamic graph transformer that integrates evolving 3D geometric representations into DGL through an attention mechanism to predict machining operation sequences. Our approach leverages StereoLithography surface meshes representing the 3D geometry of a part after each machining operation, with the boundary representation method used for the initial 3D designs. We evaluate MP-GFormer on a synthesized dataset and demonstrate that the method achieves improvements of 24\% and 36\% in accuracy for main and sub-operation predictions, respectively, compared to state-of-the-art approaches.

</details>


### [64] [Defending Unauthorized Model Merging via Dual-Stage Weight Protection](https://arxiv.org/abs/2511.11851)
*Wei-Jia Chen,Min-Yen Tsai,Cheng-Yi Lee,Chia-Mu Yu*

Main category: cs.CV

TL;DR: MergeGuard是一个用于保护预训练模型免受未经授权合并的框架，通过重塑模型参数几何结构来破坏合并兼容性，同时保持任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 未经授权的模型合并侵犯了知识产权，损害了模型所有权和问责制。为了解决这个问题，我们提出了MergeGuard。

Method: MergeGuard使用双阶段权重保护框架，包括通过L2正则化优化在层之间重新分配任务相关信息，以及注入结构化扰动来错位任务子空间。

Result: 实验表明，MergeGuard可以将合并模型的准确率降低高达90%，而受保护模型的性能损失小于1.5%。

Conclusion: MergeGuard通过改变模型参数的几何结构，有效地阻止了未经授权的模型合并，同时保持了模型自身的性能。

Abstract: The rapid proliferation of pretrained models and open repositories has made model merging a convenient yet risky practice, allowing free-riders to combine fine-tuned models into a new multi-capability model without authorization. Such unauthorized model merging not only violates intellectual property rights but also undermines model ownership and accountability. To address this issue, we present MergeGuard, a proactive dual-stage weight protection framework that disrupts merging compatibility while maintaining task fidelity. In the first stage, we redistribute task-relevant information across layers via L2-regularized optimization, ensuring that important gradients are evenly dispersed. In the second stage, we inject structured perturbations to misalign task subspaces, breaking curvature compatibility in the loss landscape. Together, these stages reshape the model's parameter geometry such that merged models collapse into destructive interference while the protected model remains fully functional. Extensive experiments on both vision (ViT-L-14) and language (Llama2, Gemma2, Mistral) models demonstrate that MergeGuard reduces merged model accuracy by up to 90% with less than 1.5% performance loss on the protected model.

</details>


### [65] [FocusSDF: Boundary-Aware Learning for Medical Image Segmentation via Signed Distance Supervision](https://arxiv.org/abs/2511.11864)
*Muzammal Shafique,Nasir Rahim,Jamil Ahmad,Mohammad Siadat,Khalid Malik,Ghaus Malik*

Main category: cs.CV

TL;DR: 提出了一种新的基于符号距离函数 (SDF) 的损失函数 FocusSDF，以提高医学图像分割中边界的分割效果。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割是医学图像分析的重要组成部分，但大多数分割模型没有明确编码边界信息，导致边界保持成为一个挑战。

Method: 通过自适应地为更靠近病灶或器官边界的像素分配更高的权重，使网络集中于边界区域。

Result: 在包括脑动脉瘤、中风、肝脏和乳腺肿瘤分割任务的各种数据集上，针对五种最先进的医学图像分割模型（包括基础模型 MedSAM）进行了广泛的评估，实验结果一致表明 FocusSDF 优于现有的基于距离变换的损失函数。

Conclusion: FocusSDF 是一种有效的医学图像分割损失函数，尤其是在边界分割方面

Abstract: Segmentation of medical images constitutes an essential component of medical image analysis, providing the foundation for precise diagnosis and efficient therapeutic interventions in clinical practices. Despite substantial progress, most segmentation models do not explicitly encode boundary information; as a result, making boundary preservation a persistent challenge in medical image segmentation. To address this challenge, we introduce FocusSDF, a novel loss function based on the signed distance functions (SDFs), which redirects the network to concentrate on boundary regions by adaptively assigning higher weights to pixels closer to the lesion or organ boundary, effectively making it boundary aware. To rigorously validate FocusSDF, we perform extensive evaluations against five state-of-the-art medical image segmentation models, including the foundation model MedSAM, using four distance-based loss functions across diverse datasets covering cerebral aneurysm, stroke, liver, and breast tumor segmentation tasks spanning multiple imaging modalities. The experimental results consistently demonstrate the superior performance of FocusSDF over existing distance transform based loss functions.

</details>


### [66] [Lacking Data? No worries! How synthetic images can alleviate image scarcity in wildlife surveys: a case study with muskox (Ovibos moschatus)](https://arxiv.org/abs/2511.11882)
*Simon Durand,Samuel Foucher,Alexandre Delplanque,Joëlle Taillon,Jérôme Théau*

Main category: cs.CV

TL;DR: 本研究探讨了使用合成图像（SI）来补充有限的训练数据，以提高在零样本（ZS）和少样本（FS）环境中检测麝牛的效果。


<details>
  <summary>Details</summary>
Motivation: 传统的野生动物调查方法资源密集且面临后勤挑战。深度学习目标检测模型（ODM）的有效性受到小数据集的限制，难以训练出适用于稀疏分布物种（如麝牛）的稳健ODM。

Method: 比较了在训练集中加入不同数量SI的基线模型、5个ZS模型和5个FS模型。

Result: 对于ZS模型，添加SI改进了检测性能，但当SI超过基线模型训练数据集的100%时，性能提升趋于平缓。对于FS模型，结合真实图像和SI可获得更好的召回率和略高的整体准确率。

Conclusion: 研究结果表明，当数据稀缺时，SI具有训练精确ODM的潜力，通过监测稀有或难以接近的物种并提高监测频率，为野生动物监测提供了重要的视角。

Abstract: Accurate population estimates are essential for wildlife management, providing critical insights into species abundance and distribution. Traditional survey methods, including visual aerial counts and GNSS telemetry tracking, are widely used to monitor muskox populations in Arctic regions. These approaches are resource intensive and constrained by logistical challenges. Advances in remote sensing, artificial intelligence, and high resolution aerial imagery offer promising alternatives for wildlife detection. Yet, the effectiveness of deep learning object detection models (ODMs) is often limited by small datasets, making it challenging to train robust ODMs for sparsely distributed species like muskoxen. This study investigates the integration of synthetic imagery (SI) to supplement limited training data and improve muskox detection in zero shot (ZS) and few-shot (FS) settings. We compared a baseline model trained on real imagery with 5 ZS and 5 FS models that incorporated progressively more SI in the training set. For the ZS models, where no real images were included in the training set, adding SI improved detection performance. As more SI were added, performance in precision, recall and F1 score increased, but eventually plateaued, suggesting diminishing returns when SI exceeded 100% of the baseline model training dataset. For FS models, combining real and SI led to better recall and slightly higher overall accuracy compared to using real images alone, though these improvements were not statistically significant. Our findings demonstrate the potential of SI to train accurate ODMs when data is scarce, offering important perspectives for wildlife monitoring by enabling rare or inaccessible species to be monitored and to increase monitoring frequency. This approach could be used to initiate ODMs without real data and refine it as real images are acquired over time.

</details>


### [67] [Advancing Annotat3D with Harpia: A CUDA-Accelerated Library For Large-Scale Volumetric Data Segmentation](https://arxiv.org/abs/2511.11890)
*Camila Machado de Araujo,Egon P. B. S. Borges,Ricardo Marcelo Canteiro Grangeiro,Allan Pinto*

Main category: cs.CV

TL;DR: Harpia accelerates 3D dataset segmentation on HPC systems.


<details>
  <summary>Details</summary>
Motivation: Existing tools struggle with large 3D datasets from high-resolution imaging.

Method: CUDA-based library with chunked execution and GPU-accelerated tools.

Result: Improved speed, memory efficiency, and scalability compared to cuCIM and scikit-image.

Conclusion: Suitable for collaborative scientific imaging workflows in HPC environments.

Abstract: High-resolution volumetric imaging techniques, such as X-ray tomography and advanced microscopy, generate increasingly large datasets that challenge existing tools for efficient processing, segmentation, and interactive exploration. This work introduces new capabilities to Annotat3D through Harpia, a new CUDA-based processing library designed to support scalable, interactive segmentation workflows for large 3D datasets in high-performance computing (HPC) and remote-access environments. Harpia features strict memory control, native chunked execution, and a suite of GPU-accelerated filtering, annotation, and quantification tools, enabling reliable operation on datasets exceeding single-GPU memory capacity. Experimental results demonstrate significant improvements in processing speed, memory efficiency, and scalability compared to widely used frameworks such as NVIDIA cuCIM and scikit-image. The system's interactive, human-in-the-loop interface, combined with efficient GPU resource management, makes it particularly suitable for collaborative scientific imaging workflows in shared HPC infrastructures.

</details>


### [68] [Prompt Triage: Structured Optimization Enhances Vision-Language Model Performance on Medical Imaging Benchmarks](https://arxiv.org/abs/2511.11898)
*Arnav Singhvi,Vasiliki Bikia,Asad Aali,Akshay Chaudhari,Roxana Daneshjou*

Main category: cs.CV

TL;DR: 本文提出了一种使用Declarative Self-improving Python (DSPy)框架进行医学视觉语言系统prompt优化的方法，旨在提高模型性能并减少对人工prompt设计的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有VLM模型在医学图像任务中表现不佳，finetuning需要大量数据和计算资源，人工prompt难以泛化，医学机构难以部署。

Method: 通过DSPy框架，对放射学、胃肠病学和皮肤病学领域的五个医学图像任务，使用四种prompt优化技术对10个开源VLM模型进行评估。

Result: 优化后的pipeline相比zero-shot prompting基线，性能提升了53%，在zero-shot性能较低的任务上，提升幅度达到300%-3400%。

Conclusion: 自动prompt优化在医学AI系统中具有巨大潜力，可显著提高需要精确临床图像解释的视觉应用性能，并提高可扩展性和保护数据隐私。

Abstract: Vision-language foundation models (VLMs) show promise for diverse imaging tasks but often underperform on medical benchmarks. Prior efforts to improve performance include model finetuning, which requires large domain-specific datasets and significant compute, or manual prompt engineering, which is hard to generalize and often inaccessible to medical institutions seeking to deploy these tools. These challenges motivate interest in approaches that draw on a model's embedded knowledge while abstracting away dependence on human-designed prompts to enable scalable, weight-agnostic performance improvements. To explore this, we adapt the Declarative Self-improving Python (DSPy) framework for structured automated prompt optimization in medical vision-language systems through a comprehensive, formal evaluation. We implement prompting pipelines for five medical imaging tasks across radiology, gastroenterology, and dermatology, evaluating 10 open-source VLMs with four prompt optimization techniques. Optimized pipelines achieved a median relative improvement of 53% over zero-shot prompting baselines, with the largest gains ranging from 300% to 3,400% on tasks where zero-shot performance is low. These results highlight the substantial potential of applying automated prompt optimization to medical AI systems, demonstrating significant gains for vision-based applications requiring accurate clinical image interpretation. By reducing dependence on prompt design to elicit intended outputs, these techniques allow clinicians to focus on patient care and clinical decision-making. Furthermore, our experiments offer scalability and preserve data privacy, demonstrating performance improvement on open-source VLMs. We publicly release our evaluation pipelines to support reproducible research on specialized medical tasks, available at https://github.com/DaneshjouLab/prompt-triage-lab.

</details>


### [69] [PI-NAIM: Path-Integrated Neural Adaptive Imputation Model](https://arxiv.org/abs/2511.11908)
*Afifa Khaled,Ebrahim Hamid Sumiea*

Main category: cs.CV

TL;DR: PI-NAIM: A dual-path imputation architecture for medical imaging that routes samples based on missingness complexity, using MICE for simple cases and neural networks (GAIN) for complex ones, with cross-path attention fusion and joint optimization.


<details>
  <summary>Details</summary>
Motivation: Missing modalities in medical imaging hinder diagnostic pipelines, and existing imputation methods are either weak or slow.

Method: A dual-path architecture (PI-NAIM) with intelligent path routing (MICE or GAIN), cross-path attention fusion, and end-to-end joint optimization.

Result: State-of-the-art performance on MIMIC-III and multimodal benchmarks, achieving RMSE of 0.108 and AUROC of 0.812 for mortality prediction.

Conclusion: PI-NAIM is a modular and unified solution for handling incomplete data in real-world medical scenarios, seamlessly integrating into vision pipelines.

Abstract: Medical imaging and multi-modal clinical settings often face the challange of missing modality in their diagnostic pipelines. Existing imputation methods either lack representational capacity or are computationally expensive. We propose PI-NAIM, a novel dual-path architecture that dynamically routes samples to optimized imputation approaches based on missingness complexity. Our framework integrates: (1) intelligent path routing that directs low missingness samples to efficient statistical imputation (MICE) and complex patterns to powerful neural networks (GAIN with temporal analysis); (2) cross-path attention fusion that leverages missingness-aware embeddings to intelligently combine both branches; and (3) end-to-end joint optimization of imputation accuracy and downstream task performance. Extensive experiments on MIMIC-III and multimodal benchmarks demonstrate state-of-the-art performance, achieving RMSE of 0.108 (vs. baselines' 0.119-0.152) and substantial gains in downstream tasks with an AUROC of 0.812 for mortality prediction. PI-NAIM's modular design enables seamless integration into vision pipelines handling incomplete sensor measurements, missing modalities, or corrupted inputs, providing a unified solution for real-world scenario. The code is publicly available at https://github.com/AfifaKhaled/PI-NAIM-Path-Integrated-Neural-Adaptive-Imputation-Model

</details>


### [70] [MOON2.0: Dynamic Modality-balanced Multimodal Representation Learning for E-commerce Product Understanding](https://arxiv.org/abs/2511.12449)
*Zhanheng Nie,Chenghan Fu,Daoze Zhang,Junxian Wu,Wanxian Guan,Pengjie Wang,Jian Xu,Bo Zheng*

Main category: cs.CV

TL;DR: 提出了一种名为MOON2.0的动态模态平衡多模态表示学习框架，用于电商产品理解。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大型语言模型在电商产品理解方面面临三个挑战：模态混合训练引起的模态不平衡、产品内部视觉和文本信息内在对齐关系未被充分利用、以及对电商多模态数据中噪声的处理能力有限。

Method: MOON2.0 包含：(1) 模态驱动的混合专家 (MoE) 模块，通过模态组合自适应地处理输入样本，实现多模态联合学习以减轻模态不平衡；(2) 双层对齐方法，以更好地利用单个产品内部的语义对齐属性；(3) 基于 MLLM 的图像-文本协同增强策略，将文本丰富与视觉扩展相结合，并结合动态样本过滤来提高训练数据质量。

Result: MOON2.0 在 MBE2.0 和多个公共数据集上实现了最先进的零样本性能。基于注意力的热图可视化提供了 MOON2.0 改进的多模态对齐的定性证据。

Conclusion: MOON2.0 是一种有效的电商产品理解多模态表示学习框架。

Abstract: The rapid growth of e-commerce calls for multimodal models that comprehend rich visual and textual product information. Although recent multimodal large language models (MLLMs) for product understanding exhibit strong capability in representation learning for e-commerce, they still face three challenges: (i) the modality imbalance induced by modality mixed training; (ii) underutilization of the intrinsic alignment relationships among visual and textual information within a product; and (iii) limited handling of noise in e-commerce multimodal data. To address these, we propose MOON2.0, a dynamic modality-balanced multimodal representation learning framework for e-commerce product understanding. MOON2.0 comprises: (1) a Modality-driven Mixture-of-Experts (MoE) module that adaptively processes input samples by their modality composition, enabling Multimodal Joint Learning to mitigate the modality imbalance; (2) a Dual-level Alignment method to better leverage semantic alignment properties inside individual products; and (3) an MLLM-based Image-text Co-augmentation strategy that integrates textual enrichment with visual expansion, coupled with Dynamic Sample Filtering to improve training data quality. We further introduce MBE2.0, a co-augmented multimodal representation benchmark for e-commerce representation learning and evaluation. Experiments show that MOON2.0 delivers state-of-the-art zero-shot performance on MBE2.0 and multiple public datasets. Furthermore, attention-based heatmap visualization provides qualitative evidence of improved multimodal alignment of MOON2.0.

</details>


### [71] [Seeing the Forest and the Trees: Query-Aware Tokenizer for Long-Video Multimodal Language Models](https://arxiv.org/abs/2511.11910)
*Siyou Li,Huanan Wu,Juexi Shao,Yinghao Ma,Yujian Gan,Yihao Luo,Yuwei Wang,Dong Nie,Lu Wang,Wengqing Wu,Le Zhang,Massimo Poesio,Juntao Yu*

Main category: cs.CV

TL;DR: QTSplus is introduced to address the challenge of long video understanding in MLLMs by reducing the number of vision tokens and improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Long videos cause an explosion in attention cost, memory, and latency due to the linear increase in vision tokens with video length.

Method: QTSplus dynamically selects important visual evidence using cross-attention, predicts a retention budget based on query complexity, and selects Top-n tokens. A re-encoder preserves temporal order.

Result: QTSplus compresses the vision stream by up to 89%, reduces latency by 28%, and improves accuracy on TempCompass benchmarks by +20.5 and +5.6 points.

Conclusion: QTSplus is an effective mechanism for scaling MLLMs to long-video scenarios while preserving task-relevant evidence.

Abstract: Despite the recent advances in the video understanding ability of multimodal large language models (MLLMs), long video understanding remains a challenge. One of the main issues is that the number of vision tokens grows linearly with video length, which causes an explosion in attention cost, memory, and latency. To solve this challenge, we present Query-aware Token Selector (\textbf{QTSplus}), a lightweight yet powerful visual token selection module that serves as an information gate between the vision encoder and LLMs. Given a text query and video tokens, QTSplus dynamically selects the most important visual evidence for the input text query by (i) scoring visual tokens via cross-attention, (ii) \emph{predicting} an instance-specific retention budget based on the complexity of the query, and (iii) \emph{selecting} Top-$n$ tokens with a differentiable straight-through estimator during training and a hard gate at inference. Furthermore, a small re-encoder preserves temporal order using absolute time information, enabling second-level localization while maintaining global coverage.
  Integrated into Qwen2.5-VL, QTSplus compresses the vision stream by up to \textbf{89\%} and reduces end-to-end latency by \textbf{28\%} on long videos. The evaluation on eight long video understanding benchmarks shows near-parity accuracy overall when compared with the original Qwen models and outperforms the original model by \textbf{+20.5} and \textbf{+5.6} points respectively on TempCompass direction and order accuracies. These results show that QTSplus is an effective, general mechanism for scaling MLLMs to real-world long-video scenarios while preserving task-relevant evidence.
  We will make all code, data, and trained models' weights publicly available.

</details>


### [72] [From Events to Clarity: The Event-Guided Diffusion Framework for Dehazing](https://arxiv.org/abs/2511.11944)
*Ling Wang,Yunfan Lu,Wenzong Ma,Huizai Yao,Pengteng Li,Hui Xiong*

Main category: cs.CV

TL;DR: 本文首次利用事件相机进行去雾，以应对传统RGB图像动态范围有限的问题。


<details>
  <summary>Details</summary>
Motivation: RGB图像在雾霾条件下成像效果差，动态范围有限，导致去雾效果不佳，容易丢失结构和光照细节。

Method: 提出了一种事件引导的扩散模型，该模型利用扩散模型的生成先验知识，通过有效地从事件中转移HDR信息来从雾霾输入中重建清晰的图像。设计了一个事件引导模块，将稀疏的HDR事件特征（例如，边缘、角点）映射到扩散潜在空间。

Result: 在两个基准数据集和作者收集的无人机数据集上，实验结果均达到了state-of-the-art的水平。

Conclusion: 事件相机结合扩散模型能够有效提升雾霾图像的清晰度，并在实验中取得了最好的效果。

Abstract: Clear imaging under hazy conditions is a critical task. Prior-based and neural methods have improved results. However, they operate on RGB frames, which suffer from limited dynamic range. Therefore, dehazing remains ill-posed and can erase structure and illumination details. To address this, we use event cameras for dehazing for the \textbf{first time}. Event cameras offer much higher HDR ($120 dBvs.60 dB$) and microsecond latency, therefore they suit hazy scenes. In practice, transferring HDR cues from events to frames is hard because real paired data are scarce. To tackle this, we propose an event-guided diffusion model that utilizes the strong generative priors of diffusion models to reconstruct clear images from hazy inputs by effectively transferring HDR information from events. Specifically, we design an event-guided module that maps sparse HDR event features, \textit{e.g.,} edges, corners, into the diffusion latent space. This clear conditioning provides precise structural guidance during generation, improves visual realism, and reduces semantic drift. For real-world evaluation, we collect a drone dataset in heavy haze (AQI = 341) with synchronized RGB and event sensors. Experiments on two benchmarks and our dataset achieve state-of-the-art results.

</details>


### [73] [Evaluation of Attention Mechanisms in U-Net Architectures for Semantic Segmentation of Brazilian Rock Art Petroglyphs](https://arxiv.org/abs/2511.11959)
*Leonardi Melo,Luís Gustavo,Dimmy Magalhães,Lucciani Vieira,Mauro Araújo*

Main category: cs.CV

TL;DR: 本文比较了三种基于U-Net的架构在巴西考古遗址岩画语义分割中的应用。Attention-Residual BEGL-UNet 表现最佳。


<details>
  <summary>Details</summary>
Motivation: 对巴西考古遗址的岩画进行语义分割

Method: 比较三种基于U-Net的架构：BEGL-UNet、Attention-Residual BEGL-UNet 和 Spatial Channel Attention BEGL-UNet

Result: Attention-Residual BEGL-UNet 取得了最佳的整体性能，Dice 系数为 0.710，验证损失为 0.067，召回率最高，为 0.854

Conclusion: 注意力机制对于考古遗产的数字化保护非常有效

Abstract: This study presents a comparative analysis of three U-Net-based architectures for semantic segmentation of rock art petroglyphs from Brazilian archaeological sites. The investigated architectures were: (1) BEGL-UNet with Border-Enhanced Gaussian Loss function; (2) Attention-Residual BEGL-UNet, incorporating residual blocks and gated attention mechanisms; and (3) Spatial Channel Attention BEGL-UNet, which employs spatial-channel attention modules based on Convolutional Block Attention Module. All implementations employed the BEGL loss function combining binary cross-entropy with Gaussian edge enhancement. Experiments were conducted on images from the Poço da Bebidinha Archaeological Complex, Piauí, Brazil, using 5-fold cross-validation. Among the architectures, Attention-Residual BEGL-UNet achieved the best overall performance with Dice Score of 0.710, validation loss of 0.067, and highest recall of 0.854. Spatial Channel Attention BEGL-UNet obtained comparable performance with DSC of 0.707 and recall of 0.857. The baseline BEGL-UNet registered DSC of 0.690. These results demonstrate the effectiveness of attention mechanisms for archaeological heritage digital preservation, with Dice Score improvements of 2.5-2.9% over the baseline.

</details>


### [74] [FGNet: Leveraging Feature-Guided Attention to Refine SAM2 for 3D EM Neuron Segmentation](https://arxiv.org/abs/2511.13063)
*Zhenghua Li,Hang Chen,Zihao Sun,Kai Li,Xiaolin Hu*

Main category: cs.CV

TL;DR: 本文提出了一种利用视觉基础模型Segment Anything 2 (SAM2)的知识来提高电子显微镜(EM)图像中神经结构分割精度的新框架。


<details>
  <summary>Details</summary>
Motivation: 现有神经结构分割方法在EM图像中面临形态复杂、信噪比低和注释稀缺等挑战，限制了其准确性和泛化能力。

Method: 该框架首先使用SAM2提取通用特征，然后引入特征引导注意力模块，利用SAM2的语义线索引导轻量级编码器关注具有挑战性的区域。最后，双亲和力解码器生成粗略和精细的亲和力图。

Result: 实验结果表明，在SAM2权重冻结的情况下，该方法达到了与现有最佳方法相当的性能。在EM数据上进一步微调后，该方法明显优于现有的最佳方法。

Conclusion: 研究证实，当结合有针对性的领域自适应指导时，迁移在自然图像上预训练的表示可以有效地解决神经元分割中的特定挑战。

Abstract: Accurate segmentation of neural structures in Electron Microscopy (EM) images is paramount for neuroscience. However, this task is challenged by intricate morphologies, low signal-to-noise ratios, and scarce annotations, limiting the accuracy and generalization of existing methods. To address these challenges, we seek to leverage the priors learned by visual foundation models on a vast amount of natural images to better tackle this task. Specifically, we propose a novel framework that can effectively transfer knowledge from Segment Anything 2 (SAM2), which is pre-trained on natural images, to the EM domain. We first use SAM2 to extract powerful, general-purpose features. To bridge the domain gap, we introduce a Feature-Guided Attention module that leverages semantic cues from SAM2 to guide a lightweight encoder, the Fine-Grained Encoder (FGE), in focusing on these challenging regions. Finally, a dual-affinity decoder generates both coarse and refined affinity maps. Experimental results demonstrate that our method achieves performance comparable to state-of-the-art (SOTA) approaches with the SAM2 weights frozen. Upon further fine-tuning on EM data, our method significantly outperforms existing SOTA methods. This study validates that transferring representations pre-trained on natural images, when combined with targeted domain-adaptive guidance, can effectively address the specific challenges in neuron segmentation.

</details>


### [75] [From Classification to Cross-Modal Understanding: Leveraging Vision-Language Models for Fine-Grained Renal Pathology](https://arxiv.org/abs/2511.11984)
*Zhenhao Guo,Rachit Saluja,Tianyuan Yao,Quan Liu,Junchao Zhu,Haibo Wang,Daniel Reisenbüchler,Yuankai Huo,Benjamin Liechty,David J. Pisapia,Kenji Ikemura,Steven Salvatoree,Surya Seshane,Mert R. Sabuncu,Yihe Yang,Ruining Deng*

Main category: cs.CV

TL;DR: 这篇论文研究了在数据受限情况下，如何调整视觉-语言模型（VLMs）以进行临床有意义的细粒度肾小球亚型分析。


<details>
  <summary>Details</summary>
Motivation: 现有的计算病理学方法倾向于在完全监督下评估粗略的疾病分类，并且缺乏有价值的临床标签。

Method: 将细粒度肾小球亚型分析建模为一个临床现实的少样本问题，并系统地评估病理学专用和通用视觉-语言模型。

Result: 病理学专用的视觉-语言骨干网络，当与原始微调配对时，是最有效的起点。即使每个肾小球亚型只有 4-8 个标记示例，这些模型也开始捕捉差异，并在区分和校准方面显示出显着 gains。

Conclusion: 监督水平和调整策略共同影响诊断性能和多模态结构，为模型选择、调整策略和注释投资提供指导。

Abstract: Fine-grained glomerular subtyping is central to kidney biopsy interpretation, but clinically valuable labels are scarce and difficult to obtain. Existing computational pathology approaches instead tend to evaluate coarse diseased classification under full supervision with image-only models, so it remains unclear how vision-language models (VLMs) should be adapted for clinically meaningful subtyping under data constraints. In this work, we model fine-grained glomerular subtyping as a clinically realistic few-shot problem and systematically evaluate both pathology-specialized and general-purpose vision-language models under this setting. We assess not only classification performance (accuracy, AUC, F1) but also the geometry of the learned representations, examining feature alignment between image and text embeddings and the separability of glomerular subtypes. By jointly analyzing shot count, model architecture and domain knowledge, and adaptation strategy, this study provides guidance for future model selection and training under real clinical data constraints. Our results indicate that pathology-specialized vision-language backbones, when paired with the vanilla fine-tuning, are the most effective starting point. Even with only 4-8 labeled examples per glomeruli subtype, these models begin to capture distinctions and show substantial gains in discrimination and calibration, though additional supervision continues to yield incremental improvements. We also find that the discrimination between positive and negative examples is as important as image-text alignment. Overall, our results show that supervision level and adaptation strategy jointly shape both diagnostic performance and multimodal structure, providing guidance for model selection, adaptation strategies, and annotation investment.

</details>


### [76] [Region-Point Joint Representation for Effective Trajectory Similarity Learning](https://arxiv.org/abs/2511.13125)
*Hao Long,Silin Zhou,Lisi Chen,Shuo Shang*

Main category: cs.CV

TL;DR: 提出了一种名为RePo的新方法，该方法联合编码区域和点特征以捕获空间上下文和细粒度的移动模式，从而提高轨迹相似度计算的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于学习的方法无法利用轨迹信息的完整频谱进行相似性建模。

Method: 该方法首先将GPS轨迹映射到网格序列，然后通过结构和语义特征捕获空间上下文，并通过轻量级专家网络提取局部、相关和连续的运动模式。之后，使用交叉注意力将点特征和区域特征融合，生成最终的轨迹嵌入。

Result: RePo在所有评估指标上实现了比SOTA基线平均22.2％的准确率提升。

Conclusion: RePo方法有效地利用了轨迹信息的完整频谱，提高了轨迹相似度计算的准确性。

Abstract: Recent learning-based methods have reduced the computational complexity of traditional trajectory similarity computation, but state-of-the-art (SOTA) methods still fail to leverage the comprehensive spectrum of trajectory information for similarity modeling. To tackle this problem, we propose \textbf{RePo}, a novel method that jointly encodes \textbf{Re}gion-wise and \textbf{Po}int-wise features to capture both spatial context and fine-grained moving patterns. For region-wise representation, the GPS trajectories are first mapped to grid sequences, and spatial context are captured by structural features and semantic context enriched by visual features. For point-wise representation, three lightweight expert networks extract local, correlation, and continuous movement patterns from dense GPS sequences. Then, a router network adaptively fuses the learned point-wise features, which are subsequently combined with region-wise features using cross-attention to produce the final trajectory embedding. To train RePo, we adopt a contrastive loss with hard negative samples to provide similarity ranking supervision. Experiment results show that RePo achieves an average accuracy improvement of 22.2\% over SOTA baselines across all evaluation metrics.

</details>


### [77] [BeyondFacial: Identity-Preserving Personalized Generation Beyond Facial Close-ups](https://arxiv.org/abs/2511.11989)
*Songsong Zhang,Chuanqi Tang,Hongguang Zhang,Guijian Tang,Minglong Li,Xueqiong Li,Shaowu Yang,Yuanxi Peng,Wenjing Yang,Jing Zhao*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的身份保持个性化生成（IPPG）方法，旨在解决现有方法过度关注面部区域、视觉叙事性弱和语义一致性差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有IPPG方法过度强调面部区域，导致视觉叙事性弱和语义一致性差，其核心限制在于身份（ID）特征嵌入破坏了生成模型的语义表达能力。

Method: 该论文提出一种双线推理（DLI）管道，实现身份-语义分离，并提出身份自适应融合（IdAF）策略，将ID-语义融合推迟到噪声预测阶段。此外，还引入了身份聚合前置（IdAP）模块，以聚合ID信息并替换随机初始化。

Result: 实验结果表明，该方法在超出面部特写的IPPG任务中实现了稳定有效的性能，无需手动masking或微调即可实现高效生成。

Conclusion: 该方法作为一种即插即用组件，可以快速部署在现有的IPPG框架中，解决过度依赖面部特写的问题，促进电影级别的角色场景创建，并为相关领域提供更丰富的个性化生成能力。

Abstract: Identity-Preserving Personalized Generation (IPPG) has advanced film production and artistic creation, yet existing approaches overemphasize facial regions, resulting in outputs dominated by facial close-ups.These methods suffer from weak visual narrativity and poor semantic consistency under complex text prompts, with the core limitation rooted in identity (ID) feature embeddings undermining the semantic expressiveness of generative models. To address these issues, this paper presents an IPPG method that breaks the constraint of facial close-ups, achieving synergistic optimization of identity fidelity and scene semantic creation. Specifically, we design a Dual-Line Inference (DLI) pipeline with identity-semantic separation, resolving the representation conflict between ID and semantics inherent in traditional single-path architectures. Further, we propose an Identity Adaptive Fusion (IdAF) strategy that defers ID-semantic fusion to the noise prediction stage, integrating adaptive attention fusion and noise decision masking to avoid ID embedding interference on semantics without manual masking. Finally, an Identity Aggregation Prepending (IdAP) module is introduced to aggregate ID information and replace random initializations, further enhancing identity preservation. Experimental results validate that our method achieves stable and effective performance in IPPG tasks beyond facial close-ups, enabling efficient generation without manual masking or fine-tuning. As a plug-and-play component, it can be rapidly deployed in existing IPPG frameworks, addressing the over-reliance on facial close-ups, facilitating film-level character-scene creation, and providing richer personalized generation capabilities for related domains.

</details>


### [78] [Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework](https://arxiv.org/abs/2511.13189)
*Diego Ortego,Marlon Rodríguez,Mario Almagro,Kunal Dahiya,David Jiménez,Juan C. SanMiguel*

Main category: cs.CV

TL;DR: 本文探讨了如何在极限多标签分类（XMC）中利用大型decoder-only模型和视觉信息，以提高效率和性能。提出了ViXML框架，该框架集成了视觉基础模型，并在多个数据集上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有XMC方法主要依赖于小型encoder-only transformer架构，未能充分利用大型decoder-only模型和视觉信息。

Method: 1. 有效利用大型decoder-only模型。
2. 通过pooling图像的单个嵌入来集成视觉基础模型，提出Vision-enhanced eXtreme Multi-label Learning框架（ViXML）。

Result: 1. 少量参数的decoder模型即可带来显著的性能提升，同时保持计算开销可控。
2. ViXML在大多数情况下优于text-only decoder，表明图像信息的重要性。
3. 在四个公共数据集上，ViXML的性能超越了现有技术水平，在最大数据集上的P@1指标上提升高达+8.21%。

Conclusion: 本文提出的方法有效地利用了大型decoder-only模型和视觉信息，显著提升了XMC的性能，为未来的研究提供了新的方向。

Abstract: Foundation models have revolutionized artificial intelligence across numerous domains, yet their transformative potential remains largely untapped in Extreme Multi-label Classification (XMC). Queries in XMC are associated with relevant labels from extremely large label spaces, where it is critical to strike a balance between efficiency and performance. Therefore, many recent approaches efficiently pose XMC as a maximum inner product search between embeddings learned from small encoder-only transformer architectures. In this paper, we address two important aspects in XMC: how to effectively harness larger decoder-only models, and how to exploit visual information while maintaining computational efficiency. We demonstrate that both play a critical role in XMC separately and can be combined for improved performance. We show that a few billion-size decoder can deliver substantial improvements while keeping computational overhead manageable. Furthermore, our Vision-enhanced eXtreme Multi-label Learning framework (ViXML) efficiently integrates foundation vision models by pooling a single embedding per image. This limits computational growth while unlocking multi-modal capabilities. Remarkably, ViXML with small encoders outperforms text-only decoder in most cases, showing that an image is worth billions of parameters. Finally, we present an extension of existing text-only datasets to exploit visual metadata and make them available for future benchmarking. Comprehensive experiments across four public text-only datasets and their corresponding image enhanced versions validate our proposals' effectiveness, surpassing previous state-of-the-art by up to +8.21\% in P@1 on the largest dataset. ViXML's code is available at https://github.com/DiegoOrtego/vixml.

</details>


### [79] [Dynamic Parameter Optimization for Highly Transferable Transformation-Based Attacks](https://arxiv.org/abs/2511.11993)
*Jiaming Liang,Chi-Man Pun*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的动态参数优化（DPO）方法，以提高基于转换的对抗攻击在深度神经网络中的可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有基于转换的攻击存在参数优化盲点，包括低迭代设置的局限性、对不同代理模型使用统一参数以及传统网格搜索的高计算复杂度。

Method: 该研究通过实证研究揭示了转换强度与可迁移性之间的三种动态模式，并提出了同心衰减模型（CDM）来解释这些模式。在此基础上，提出了基于先上升后下降模式的动态参数优化（DPO）方法。

Result: 在不同的代理模型、迭代和任务上的实验表明，DPO 能够显著提高可迁移性。

Conclusion: 提出的DPO方法有效解决了现有基于转换的攻击的参数优化盲点问题，并显著提高了对抗样本的可迁移性。

Abstract: Despite their wide application, the vulnerabilities of deep neural networks raise societal concerns. Among them, transformation-based attacks have demonstrated notable success in transfer attacks. However, existing attacks suffer from blind spots in parameter optimization, limiting their full potential. Specifically, (1) prior work generally considers low-iteration settings, yet attacks perform quite differently at higher iterations, so characterizing overall performance based only on low-iteration results is misleading. (2) Existing attacks use uniform parameters for different surrogate models, iterations, and tasks, which greatly impairs transferability. (3) Traditional transformation parameter optimization relies on grid search. For n parameters with m steps each, the complexity is O(mn). Large computational overhead limits further optimization of parameters. To address these limitations, we conduct an empirical study with various transformations as baselines, revealing three dynamic patterns of transferability with respect to parameter strength. We further propose a novel Concentric Decay Model (CDM) to effectively explain these patterns. Building on these insights, we propose an efficient Dynamic Parameter Optimization (DPO) based on the rise-then-fall pattern, reducing the complexity to O(nlogm). Comprehensive experiments on existing transformation-based attacks across different surrogate models, iterations, and tasks demonstrate that our DPO can significantly improve transferability.

</details>


### [80] [LithoSeg: A Coarse-to-Fine Framework for High-Precision Lithography Segmentation](https://arxiv.org/abs/2511.12005)
*Xinyu He,Botong Zhao,Bingbing Li,Shujing Lyu,Jiwei Shen,Yue Lu*

Main category: cs.CV

TL;DR: 提出了一种新的 lithography SEM 图像分割方法 LithoSeg，该方法在分割精度和测量精度上均优于现有方法，同时所需的监督更少。


<details>
  <summary>Details</summary>
Motivation: 现有的 lithography 分割方法缺乏必要的精度和鲁棒性，限制了它们的实际应用。

Method: 在粗略阶段，我们引入了一种人机循环引导方案，用于分割任何模型 (SAM)，以在最小监督下获得鲁棒性。在精细阶段，我们通过使用粗掩模采样凹槽法线轮廓并将 2D 分割重铸为 1D 回归问题，并使用轻量级 MLP 执行点方式细化。

Result: LithoSeg 在分割精度和测量精度上均优于以前的方法，同时所需的监督更少。

Conclusion: LithoSeg 在实际应用中具有广阔的前景。

Abstract: Accurate segmentation and measurement of lithography scanning electron microscope (SEM) images are crucial for ensuring precise process control, optimizing device performance, and advancing semiconductor manufacturing yield. Lithography segmentation requires pixel-level delineation of groove contours and consistent performance across diverse pattern geometries and process window. However, existing methods often lack the necessary precision and robustness, limiting their practical applicability. To overcome this challenge, we propose LithoSeg, a coarse-to-fine network tailored for lithography segmentation. In the coarse stage, we introduce a Human-in-the-Loop Bootstrapping scheme for the Segment Anything Model (SAM) to attain robustness with minimal supervision. In the subsequent fine stage, we recast 2D segmentation as 1D regression problem by sampling groove-normal profiles using the coarse mask and performing point-wise refinement with a lightweight MLP. LithoSeg outperforms previous approaches in both segmentation accuracy and metrology precision while requiring less supervision, offering promising prospects for real-world applications.

</details>


### [81] [Uncertainty-Guided Selective Adaptation Enables Cross-Platform Predictive Fluorescence Microscopy](https://arxiv.org/abs/2511.12006)
*Kai-Wen K. Yang,Andrew Bai,Alexandra Bermudez,Yunqi Hong,Zoe Latham,Iris Sloan,Michael Liu,Vishrut Goyal,Cho-Jui Hsieh,Neil Y. C. Lin*

Main category: cs.CV

TL;DR: 提出了一种新的自配置领域自适应框架SIT-ADDA-Auto，用于解决深度学习在显微镜图像处理中，模型在新设备或采集设置上表现不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 传统对抗领域自适应(ADDA)重新训练整个网络，经常破坏学习到的语义表示。为了解决这个问题。

Method: 该方法仅调整最早的卷积层，同时冻结更深层，并将浅层对抗对齐与预测不确定性相结合，以自动选择自适应深度，无需目标标签。

Result: SIT-ADDA在重建和下游分割方面，优于全编码器自适应和非对抗基线，并减少了语义特征的漂移。通过多指标评估、盲法专家评估和不确定性-深度消融证明了其鲁棒性。

Conclusion: 该研究为显微镜中的无标签自适应提供了一个设计规则，并为现场设置提供了一个方案。

Abstract: Deep learning is transforming microscopy, yet models often fail when applied to images from new instruments or acquisition settings. Conventional adversarial domain adaptation (ADDA) retrains entire networks, often disrupting learned semantic representations. Here, we overturn this paradigm by showing that adapting only the earliest convolutional layers, while freezing deeper layers, yields reliable transfer. Building on this principle, we introduce Subnetwork Image Translation ADDA with automatic depth selection (SIT-ADDA-Auto), a self-configuring framework that integrates shallow-layer adversarial alignment with predictive uncertainty to automatically select adaptation depth without target labels. We demonstrate robustness via multi-metric evaluation, blinded expert assessment, and uncertainty-depth ablations. Across exposure and illumination shifts, cross-instrument transfer, and multiple stains, SIT-ADDA improves reconstruction and downstream segmentation over full-encoder adaptation and non-adversarial baselines, with reduced drift of semantic features. Our results provide a design rule for label-free adaptation in microscopy and a recipe for field settings; the code is publicly available.

</details>


### [82] [Enhancing Road Safety Through Multi-Camera Image Segmentation with Post-Encroachment Time Analysis](https://arxiv.org/abs/2511.12018)
*Shounak Ray Chaudhuri,Arash Jahangiri,Christopher Paolini*

Main category: cs.CV

TL;DR: 提出了一种基于多摄像头计算机视觉的实时交通安全评估框架，通过计算后侵入时间（PET）来识别高风险区域。


<details>
  <summary>Details</summary>
Motivation: 传统的基于事故的研究受到数据稀疏性和延迟的限制，因此需要一种新的方法进行实时安全评估。

Method: 使用四个同步摄像头，通过 YOLOv11 分割进行车辆检测，并使用单应性矩阵将检测到的车辆多边形转换为统一的鸟瞰图。提出了一种新的像素级 PET 算法，可以在亚秒级精度和实时吞吐量下识别高风险区域。

Result: 该框架能够在边缘设备上以平均 2.68 FPS 的速度生成 800 x 800 像素对数热图，从而识别高风险区域。

Conclusion: 验证了基于分散视觉的 PET 分析在智能交通系统中的可行性，并为高分辨率、实时和可扩展的交叉口安全评估提供了一种可复制的方法。

Abstract: Traffic safety analysis at signalized intersections is vital for reducing vehicle and pedestrian collisions, yet traditional crash-based studies are limited by data sparsity and latency. This paper presents a novel multi-camera computer vision framework for real-time safety assessment through Post-Encroachment Time (PET) computation, demonstrated at the intersection of H Street and Broadway in Chula Vista, California. Four synchronized cameras provide continuous visual coverage, with each frame processed on NVIDIA Jetson AGX Xavier devices using YOLOv11 segmentation for vehicle detection. Detected vehicle polygons are transformed into a unified bird's-eye map using homography matrices, enabling alignment across overlapping camera views. A novel pixel-level PET algorithm measures vehicle position without reliance on fixed cells, allowing fine-grained hazard visualization via dynamic heatmaps, accurate to 3.3 sq-cm. Timestamped vehicle and PET data is stored in an SQL database for long-term monitoring. Results over various time intervals demonstrate the framework's ability to identify high-risk regions with sub-second precision and real-time throughput on edge devices, producing data for an 800 x 800 pixel logarithmic heatmap at an average of 2.68 FPS. This study validates the feasibility of decentralized vision-based PET analysis for intelligent transportation systems, offering a replicable methodology for high-resolution, real-time, and scalable intersection safety evaluation.

</details>


### [83] [LIHE: Linguistic Instance-Split Hyperbolic-Euclidean Framework for Generalized Weakly-Supervised Referring Expression Comprehension](https://arxiv.org/abs/2511.12020)
*Xianglong Shi,Silin Cheng,Sirui Zhao,Yunhan Jiang,Enhong Chen,Yang Liu,Sebastien Ourselin*

Main category: cs.CV

TL;DR: 本文介绍了一个新的弱监督广义指代表达式理解任务（WGREC），旨在解决现有方法无法处理零个或多个目标的问题。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督指代表达式理解（WREC）方法受限于一对一映射假设，无法处理现实场景中对应零个或多个目标的表达式。

Method: 提出了一个名为LIHE的框架，该框架包含Referential Decoupling和Referent Grounding两个阶段，并使用混合相似度模块HEMix。

Result: LIHE在gRefCOCO和Ref-ZOM数据集上建立了第一个有效的弱监督WGREC基线，HEMix在标准REC基准上实现了持续改进。

Conclusion: LIHE框架和HEMix模块能够有效解决WGREC任务中的监督信号模糊和语义表示崩溃问题。

Abstract: Existing Weakly-Supervised Referring Expression Comprehension (WREC) methods, while effective, are fundamentally limited by a one-to-one mapping assumption, hindering their ability to handle expressions corresponding to zero or multiple targets in realistic scenarios. To bridge this gap, we introduce the Weakly-Supervised Generalized Referring Expression Comprehension task (WGREC), a more practical paradigm that handles expressions with variable numbers of referents. However, extending WREC to WGREC presents two fundamental challenges: supervisory signal ambiguity, where weak image-level supervision is insufficient for training a model to infer the correct number and identity of referents, and semantic representation collapse, where standard Euclidean similarity forces hierarchically-related concepts into non-discriminative clusters, blurring categorical boundaries. To tackle these challenges, we propose a novel WGREC framework named Linguistic Instance-Split Hyperbolic-Euclidean (LIHE), which operates in two stages. The first stage, Referential Decoupling, predicts the number of target objects and decomposes the complex expression into simpler sub-expressions. The second stage, Referent Grounding, then localizes these sub-expressions using HEMix, our innovative hybrid similarity module that synergistically combines the precise alignment capabilities of Euclidean proximity with the hierarchical modeling strengths of hyperbolic geometry. This hybrid approach effectively prevents semantic collapse while preserving fine-grained distinctions between related concepts. Extensive experiments demonstrate LIHE establishes the first effective weakly supervised WGREC baseline on gRefCOCO and Ref-ZOM, while HEMix achieves consistent improvements on standard REC benchmarks, improving IoU@0.5 by up to 2.5\%. The code is available at https://anonymous.4open.science/r/LIHE.

</details>


### [84] [Null-Space Diffusion Distillation for Efficient Photorealistic Lensless Imaging](https://arxiv.org/abs/2511.12024)
*Jose Reinaldo Cunha Santos A V Silva Neto,Hodaka Kawachi,Yasushi Yagi,Tomoya Nakamura*

Main category: cs.CV

TL;DR: 提出了一种新的无监督lensless图像重建方法，通过null-space diffusion distillation (NSDD) 从 iterative DDNM+ 求解器中提炼 null-space 成分，实现了快速、真实的重建。


<details>
  <summary>Details</summary>
Motivation: 现有的lensless相机逼真重建依赖于 paired lensless-lensed 监督，这会因域不匹配而导致偏差。无监督的 diffusion priors 很有吸引力，但传统方法在lensless反卷积中效果不佳。

Method: 提出了 Null-Space Diffusion Distillation (NSDD)，它是一个单程学生模型，可以提炼 iterative DDNM+ 求解器的 null-space 成分，并以 lensless 测量和 range-space 锚点为条件。

Result: 在 Lensless-FFHQ 和 PhlatCam 上，NSDD 速度仅次于 Wiener，实现了接近教师网络的感知质量，优于 DPS 和传统凸优化基线。

Conclusion: 这些结果表明，快速、无监督、逼真的 lensless 成像是有实际应用前景的。

Abstract: State-of-the-art photorealistic reconstructions for lensless cameras often rely on paired lensless-lensed supervision, which can bias models due to lens-lensless domain mismatch. To avoid this, ground-truth-free diffusion priors are attractive; however, generic formulations tuned for conventional inverse problems often break under the noisy, highly multiplexed, and ill-posed lensless deconvolution setting. We observe that methods which separate range-space enforcement from null-space diffusion-prior updates yield stable, realistic reconstructions. Building on this, we introduce Null-Space Diffusion Distillation (NSDD): a single-pass student that distills the null-space component of an iterative DDNM+ solver, conditioned on the lensless measurement and on a range-space anchor. NSDD preserves measurement consistency and achieves photorealistic results without paired supervision at a fraction of the runtime and memory. On Lensless-FFHQ and PhlatCam, NSDD is the second fastest, behind Wiener, and achieves near-teacher perceptual quality (second-best LPIPS, below DDNM+), outperforming DPS and classical convex baselines. These results suggest a practical path toward fast, ground-truth-free, photorealistic lensless imaging.

</details>


### [85] [Bridging Vision and Language for Robust Context-Aware Surgical Point Tracking: The VL-SurgPT Dataset and Benchmark](https://arxiv.org/abs/2511.12026)
*Rulin Zhou,Wenlong He,An Wang,Jianhang Zhang,Xuanhui Zeng,Xi Zhang,Chaowei Zhu,Haijun Hu,Hongliang Ren*

Main category: cs.CV

TL;DR: VL-SurgPT：首个大型多模态手术数据集，将视觉跟踪与手术场景中点状态的文本描述相结合，以应对复杂视觉条件下的跟踪挑战。


<details>
  <summary>Details</summary>
Motivation: 现有手术跟踪数据集缺乏理解跟踪失败机制所需的语义信息。

Method: 1. 构建包含908个体内视频片段的数据集，包括组织跟踪和器械跟踪。2. 使用8种先进的跟踪方法建立综合基准。3. 提出TG-SurgPT，一种文本引导的跟踪方法，利用语义描述来提高在视觉挑战条件下的鲁棒性。

Result: 结合点状态信息显著提高了跟踪准确性和可靠性，尤其是在传统视觉方法难以应对的恶劣视觉场景中。

Conclusion: VL-SurgPT通过桥接视觉和语言模态，能够开发上下文感知的跟踪系统，这对于推进计算机辅助手术应用至关重要，即使在具有挑战性的术中条件下也能保持性能。

Abstract: Accurate point tracking in surgical environments remains challenging due to complex visual conditions, including smoke occlusion, specular reflections, and tissue deformation. While existing surgical tracking datasets provide coordinate information, they lack the semantic context necessary to understand tracking failure mechanisms. We introduce VL-SurgPT, the first large-scale multimodal dataset that bridges visual tracking with textual descriptions of point status in surgical scenes. The dataset comprises 908 in vivo video clips, including 754 for tissue tracking (17,171 annotated points across five challenging scenarios) and 154 for instrument tracking (covering seven instrument types with detailed keypoint annotations). We establish comprehensive benchmarks using eight state-of-the-art tracking methods and propose TG-SurgPT, a text-guided tracking approach that leverages semantic descriptions to improve robustness in visually challenging conditions. Experimental results demonstrate that incorporating point status information significantly improves tracking accuracy and reliability, particularly in adverse visual scenarios where conventional vision-only methods struggle. By bridging visual and linguistic modalities, VL-SurgPT enables the development of context-aware tracking systems crucial for advancing computer-assisted surgery applications that can maintain performance even under challenging intraoperative conditions.

</details>


### [86] [GCAgent: Long-Video Understanding via Schematic and Narrative Episodic Memory](https://arxiv.org/abs/2511.12027)
*Jeong Hun Yeo,Sangyun Chung,Sungjune Park,Dae Hoe Kim,Jinyoung Moon,Yong Man Ro*

Main category: cs.CV

TL;DR: GCAgent是一个用于长视频理解的框架，它通过构建结构化的事件记忆来解决长时依赖问题，并在Video-MME基准测试上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以捕捉全局上下文和复杂的事件关系，从而无法进行深入的视频推理。

Method: GCAgent使用图示和叙事事件记忆，并在多阶段感知-行动-反思循环中运行，利用记忆管理器检索相关的事件上下文。

Result: GCAgent在Video-MME Long split上实现了高达23.5%的准确率提升，并在7B规模的MLLM中取得了最先进的性能。

Conclusion: 该研究验证了基于Agent的推理范式和结构化记忆在认知启发下的长视频理解中的有效性。

Abstract: Long-video understanding remains a significant challenge for Multimodal Large Language Models (MLLMs) due to inherent token limitations and the complexity of capturing long-term temporal dependencies. Existing methods often fail to capture the global context and complex event relationships necessary for deep video reasoning. To address this, we introduce GCAgent, a novel Global-Context-Aware Agent framework that achieves comprehensive long-video understanding. Our core innovation is the Schematic and Narrative Episodic Memory. This memory structurally models events and their causal and temporal relations into a concise, organized context, fundamentally resolving the long-term dependency problem. Operating in a multi-stage Perception-Action-Reflection cycle, our GCAgent utilizes a Memory Manager to retrieve relevant episodic context for robust, context-aware inference. Extensive experiments confirm that GCAgent significantly enhances long-video understanding, achieving up to 23.5\% accuracy improvement on the Video-MME Long split over a strong MLLM baseline. Furthermore, our framework establishes state-of-the-art performance among comparable 7B-scale MLLMs, achieving 73.4\% accuracy on the Long split and the highest overall average (71.9\%) on the Video-MME benchmark, validating our agent-based reasoning paradigm and structured memory for cognitively-inspired long-video understanding.

</details>


### [87] [VPHO: Joint Visual-Physical Cue Learning and Aggregation for Hand-Object Pose Estimation](https://arxiv.org/abs/2511.12030)
*Jun Zhou,Chi Xu,Kaifeng Tang,Yuting Ge,Tingrui Guo,Li Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种新框架，用于从单张RGB图像中估计手和物体的3D姿势，该框架结合了视觉和物理线索，以提高姿势准确性和物理合理性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖视觉线索，容易违反物理约束。最近结合物理推理的方法依赖于后优化或不可微的物理引擎，损害了视觉一致性和端到端的可训练性。

Method: 该方法通过联合视觉-物理线索学习和候选姿势聚合来实现视觉和物理线索的集成。模型被训练以提取2D视觉线索和3D物理线索，并利用视觉和物理预测来聚合多个扩散生成的候选姿势。

Result: 大量实验表明，该方法在姿势准确性和物理合理性方面均优于现有技术水平。

Conclusion: 该方法能够显著提高手-物体姿势估计的准确性和物理合理性。

Abstract: Estimating the 3D poses of hands and objects from a single RGB image is a fundamental yet challenging problem, with broad applications in augmented reality and human-computer interaction. Existing methods largely rely on visual cues alone, often producing results that violate physical constraints such as interpenetration or non-contact. Recent efforts to incorporate physics reasoning typically depend on post-optimization or non-differentiable physics engines, which compromise visual consistency and end-to-end trainability. To overcome these limitations, we propose a novel framework that jointly integrates visual and physical cues for hand-object pose estimation. This integration is achieved through two key ideas: 1) joint visual-physical cue learning: The model is trained to extract 2D visual cues and 3D physical cues, thereby enabling more comprehensive representation learning for hand-object interactions; 2) candidate pose aggregation: A novel refinement process that aggregates multiple diffusion-generated candidate poses by leveraging both visual and physical predictions, yielding a final estimate that is visually consistent and physically plausible. Extensive experiments demonstrate that our method significantly outperforms existing state-of-the-art approaches in both pose accuracy and physical plausibility.

</details>


### [88] [Improved Masked Image Generation with Knowledge-Augmented Token Representations](https://arxiv.org/abs/2511.12032)
*Guotao Liang,Baoquan Zhang,Zhiyuan Wen,Zihao Han,Yunming Ye*

Main category: cs.CV

TL;DR: KA-MIG框架通过引入token级别语义依赖的先验知识，提升了并行token预测的图像生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有MIG方法难以学习视觉token序列间的语义依赖，因为token缺乏明确语义且序列过长。

Method: 设计了一个图感知编码器，利用共现图、语义相似图和位置-token不兼容图三种token知识图，学习token和位置感知的表示，并通过轻量级融合机制整合到现有MIG方法中。

Result: 在ImageNet上的实验表明，该方法在类别条件图像生成方面优于现有MIG方法。

Conclusion: KA-MIG框架通过先验知识有效提升了模型捕捉语义依赖的能力，从而提高了生成质量。

Abstract: Masked image generation (MIG) has demonstrated remarkable efficiency and high-fidelity images by enabling parallel token prediction. Existing methods typically rely solely on the model itself to learn semantic dependencies among visual token sequences. However, directly learning such semantic dependencies from data is challenging because the individual tokens lack clear semantic meanings, and these sequences are usually long. To address this limitation, we propose a novel Knowledge-Augmented Masked Image Generation framework, named KA-MIG, which introduces explicit knowledge of token-level semantic dependencies (\emph{i.e.}, extracted from the training data) as priors to learn richer representations for improving performance. In particular, we explore and identify three types of advantageous token knowledge graphs, including two positive and one negative graphs (\emph{i.e.}, the co-occurrence graph, the semantic similarity graph, and the position-token incompatibility graph). Based on three prior knowledge graphs, we design a graph-aware encoder to learn token and position-aware representations. After that, a lightweight fusion mechanism is introduced to integrate these enriched representations into the existing MIG methods. Resorting to such prior knowledge, our method effectively enhances the model's ability to capture semantic dependencies, leading to improved generation quality. Experimental results demonstrate that our method improves upon existing MIG for class-conditional image generation on ImageNet.

</details>


### [89] [Calibrated Multimodal Representation Learning with Missing Modalities](https://arxiv.org/abs/2511.12034)
*Xiaohao Liu,Xiaobo Xia,Jiaheng Wei,Shuo Yang,Xiu Su,See-Kiong Ng,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态表征学习方法 CalMRL，以解决由于模态缺失导致的不完整对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多模态融合时需要所有模态都存在，这限制了其在模态缺失普遍存在的数据集上的应用。本文从锚点偏移的角度分析了这个问题。

Method: CalMRL 利用先验知识和模态间的内在联系，在表征层面建模缺失模态的推断，并通过双步学习方法和共享隐变量后验分布的闭式解来解决优化难题。

Result: 理论分析验证了 CalMRL 对锚点偏移的缓解和收敛性，大量实验和综合分析表明了 CalMRL 的优越性。

Conclusion: CalMRL 能够灵活地处理包含缺失模态的数据，并提升现有方法的性能。

Abstract: Multimodal representation learning harmonizes distinct modalities by aligning them into a unified latent space. Recent research generalizes traditional cross-modal alignment to produce enhanced multimodal synergy but requires all modalities to be present for a common instance, making it challenging to utilize prevalent datasets with missing modalities. We provide theoretical insights into this issue from an anchor shift perspective. Observed modalities are aligned with a local anchor that deviates from the optimal one when all modalities are present, resulting in an inevitable shift. To address this, we propose CalMRL for multimodal representation learning to calibrate incomplete alignments caused by missing modalities. Specifically, CalMRL leverages the priors and the inherent connections among modalities to model the imputation for the missing ones at the representation level. To resolve the optimization dilemma, we employ a bi-step learning method with the closed-form solution of the posterior distribution of shared latents. We validate its mitigation of anchor shift and convergence with theoretical guidance. By equipping the calibrated alignment with the existing advanced method, we offer new flexibility to absorb data with missing modalities, which is originally unattainable. Extensive experiments and comprehensive analyses demonstrate the superiority of CalMRL. Our code, model checkpoints, and evaluation raw data will be publicly available.

</details>


### [90] [SRSplat: Feed-Forward Super-Resolution Gaussian Splatting from Sparse Multi-View Images](https://arxiv.org/abs/2511.12040)
*Xinyuan Hu,Changyue Shi,Chuxiao Yang,Minghao Chen,Jiajun Ding,Tao Wei,Chen Wei,Zhou Yu,Min Tan*

Main category: cs.CV

TL;DR: 提出了一种名为SRSplat的前馈框架，用于从稀疏的低分辨率图像重建高分辨率3D场景。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以从稀疏的低分辨率图像中恢复精细的纹理细节，因为低分辨率图像缺乏高频信息。

Method: 利用多模态大型语言模型和扩散模型为每个场景生成场景特定的参考图库，并引入参考引导的特征增强模块（RGFE）来对齐和融合低分辨率输入图像及其参考图像的特征。此外，还引入了纹理感知密度控制（TADC），根据低分辨率输入的内部纹理丰富度自适应地调整高斯密度。

Result: 在RealEstate10K、ACID和DTU等多个数据集上，SRSplat优于现有方法，并表现出强大的跨数据集和跨分辨率泛化能力。

Conclusion: SRSplat 能够有效地从稀疏的低分辨率图像重建高分辨率 3D 场景，并在多个数据集上取得了优越的性能。

Abstract: Feed-forward 3D reconstruction from sparse, low-resolution (LR) images is a crucial capability for real-world applications, such as autonomous driving and embodied AI. However, existing methods often fail to recover fine texture details. This limitation stems from the inherent lack of high-frequency information in LR inputs. To address this, we propose \textbf{SRSplat}, a feed-forward framework that reconstructs high-resolution 3D scenes from only a few LR views. Our main insight is to compensate for the deficiency of texture information by jointly leveraging external high-quality reference images and internal texture cues. We first construct a scene-specific reference gallery, generated for each scene using Multimodal Large Language Models (MLLMs) and diffusion models. To integrate this external information, we introduce the \textit{Reference-Guided Feature Enhancement (RGFE)} module, which aligns and fuses features from the LR input images and their reference twin image. Subsequently, we train a decoder to predict the Gaussian primitives using the multi-view fused feature obtained from \textit{RGFE}. To further refine predicted Gaussian primitives, we introduce \textit{Texture-Aware Density Control (TADC)}, which adaptively adjusts Gaussian density based on the internal texture richness of the LR inputs. Extensive experiments demonstrate that our SRSplat outperforms existing methods on various datasets, including RealEstate10K, ACID, and DTU, and exhibits strong cross-dataset and cross-resolution generalization capabilities.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [91] [LLM-Generated Negative News Headlines Dataset: Creation and Benchmarking Against Real Journalism](https://arxiv.org/abs/2511.11591)
*Olusola Babalola,Bolanle Ojokoh,Olutayo Boyinbode*

Main category: cs.AI

TL;DR: 本研究探讨了使用大型语言模型（LLM）生成的数据集来支持自然语言处理（NLP）任务的潜力，旨在克服与数据获取和与真实世界数据相关的隐私问题相关的挑战。


<details>
  <summary>Details</summary>
Motivation: 旨在克服与数据获取和与真实世界数据相关的隐私问题相关的挑战。

Method: 使用定制的提示创建了一个专门的负面新闻标题语料库，以捕捉各个社会领域的各种负面情绪。通过专家评审验证合成标题，并在嵌入空间中进一步分析，以评估其在内容、语气、长度和风格方面与真实世界负面新闻的一致性。评估了与真实标题的相关性、困惑度、连贯性和真实性等关键指标。使用包括比较困惑度测试、比较可读性测试、比较 POS 概况分析、BERTScore 和比较语义相似性在内的评估，针对两组真实新闻标题对合成数据集进行了基准测试。

Result: 结果表明，生成的标题与真实标题相匹配，唯一的明显差异是 POS 概况测试的专有名词得分。

Conclusion: 生成的标题与真实标题相匹配，唯一的明显差异是 POS 概况测试的专有名词得分。

Abstract: This research examines the potential of datasets generated by Large Language Models (LLMs) to support Natural Language Processing (NLP) tasks, aiming to overcome challenges related to data acquisition and privacy concerns associated with real-world data. Focusing on negative valence text, a critical component of sentiment analysis, we explore the use of LLM-generated synthetic news headlines as an alternative to real-world data. A specialized corpus of negative news headlines was created using tailored prompts to capture diverse negative sentiments across various societal domains. The synthetic headlines were validated by expert review and further analyzed in embedding space to assess their alignment with real-world negative news in terms of content, tone, length, and style. Key metrics such as correlation with real headlines, perplexity, coherence, and realism were evaluated. The synthetic dataset was benchmarked against two sets of real news headlines using evaluations including the Comparative Perplexity Test, Comparative Readability Test, Comparative POS Profiling, BERTScore, and Comparative Semantic Similarity. Results show the generated headlines match real headlines with the only marked divergence being in the proper noun score of the POS profile test.

</details>


### [92] [CLINB: A Climate Intelligence Benchmark for Foundational Models](https://arxiv.org/abs/2511.11597)
*Michelle Chen Huebscher,Katharine Mach,Aleksandar Stanić,Markus Leippold,Ben Gaiarin,Zeke Hausfather,Elisa Rawat,Erich Fischer,Massimiliano Ciaramita,Joeri Rogelj,Christian Buck,Lierni Sestorain Saralegui,Reto Knutti*

Main category: cs.AI

TL;DR: CLINB是一个评估大型语言模型处理气候变化领域专业知识的基准。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型如何处理复杂、专业的知识仍然是一个关键挑战。本文通过气候变化的视角，引入了一个基准CLINB，用于评估模型在开放式、基于事实的、多模态问答任务中的表现，并对知识质量和证据支持提出了明确的要求。

Method: CLINB依赖于真实用户的问题数据集和由 ведущих 气候科学家策划的评估标准。我们实施并验证了一个基于模型的评估过程，并评估了几个前沿模型。

Result: 前沿模型表现出卓越的知识综合能力，通常表现出博士水平的理解和表达质量，并且优于领域专家在较弱模型的帮助下策划的“混合”答案。然而，这种性能与 grounding 失败形成对比。证据的质量参差不齐，参考文献和图像的幻觉率很高。

Conclusion: 弥合知识综合和可验证归因之间的差距对于人工智能在科学工作流程中的部署至关重要，并且需要像 CLINB 这样可靠、可解释的基准来推动构建值得信赖的 AI 系统。

Abstract: Evaluating how Large Language Models (LLMs) handle complex, specialized knowledge remains a critical challenge. We address this through the lens of climate change by introducing CLINB, a benchmark that assesses models on open-ended, grounded, multimodal question answering tasks with clear requirements for knowledge quality and evidential support. CLINB relies on a dataset of real users' questions and evaluation rubrics curated by leading climate scientists. We implement and validate a model-based evaluation process and evaluate several frontier models. Our findings reveal a critical dichotomy. Frontier models demonstrate remarkable knowledge synthesis capabilities, often exhibiting PhD-level understanding and presentation quality. They outperform "hybrid" answers curated by domain experts assisted by weaker models. However, this performance is countered by failures in grounding. The quality of evidence varies, with substantial hallucination rates for references and images. We argue that bridging this gap between knowledge synthesis and verifiable attribution is essential for the deployment of AI in scientific workflows and that reliable, interpretable benchmarks like CLINB are needed to progress towards building trustworthy AI systems.

</details>


### [93] [SynBullying: A Multi LLM Synthetic Conversational Dataset for Cyberbullying Detectio](https://arxiv.org/abs/2511.11599)
*Arefeh Kazemi,Hamza Qadeer,Joachim Wagner,Hossein Hosseini,Sri Balaaji Natarajan Kalaivendan,Brian Davis*

Main category: cs.AI

TL;DR: SynBullying: A synthetic dataset for cyberbullying detection using multi-LLM conversations, offering a scalable and ethically safe alternative to human data.


<details>
  <summary>Details</summary>
Motivation: Addresses the need for scalable and ethically safe cyberbullying data by using LLMs to simulate realistic interactions.

Method: LLMs are used to generate conversational data with context-aware and fine-grained labels for different cyberbullying categories.

Result: The dataset's utility is evaluated across five dimensions and tested for performance in cyberbullying classification, both as standalone training data and for augmentation.

Conclusion: Introduces SynBullying, a synthetic dataset for cyberbullying research, offering a structured, context-aware, and fine-grained resource for studying and detecting cyberbullying.

Abstract: We introduce SynBullying, a synthetic multi-LLM conversational dataset for studying and detecting cyberbullying (CB). SynBullying provides a scalable and ethically safe alternative to human data collection by leveraging large language models (LLMs) to simulate realistic bullying interactions. The dataset offers (i) conversational structure, capturing multi-turn exchanges rather than isolated posts; (ii) context-aware annotations, where harmfulness is assessed within the conversational flow considering context, intent, and discourse dynamics; and (iii) fine-grained labeling, covering various CB categories for detailed linguistic and behavioral analysis. We evaluate SynBullying across five dimensions, including conversational structure, lexical patterns, sentiment/toxicity, role dynamics, harm intensity, and CB-type distribution. We further examine its utility by testing its performance as standalone training data and as an augmentation source for CB classification.

</details>


### [94] [CausalGuard: A Smart System for Detecting and Preventing False Information in Large Language Models](https://arxiv.org/abs/2511.11600)
*Piyushkumar Patel*

Main category: cs.AI

TL;DR: 大型语言模型会自信地陈述听起来完全合理的虚假信息，即产生幻觉。CausalGuard结合了因果推理和符号逻辑来捕获并防止幻觉的产生。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案或者需要重新训练整个模型，或者增加大量的计算成本，或者没有抓住幻觉产生的根本原因。

Method: CausalGuard通过两条互补的路径工作：一条追溯模型已知内容和生成内容之间的因果关系，另一条使用自动推理检查逻辑一致性。

Result: 在十二个不同的基准测试中，CausalGuard正确识别幻觉的概率为89.3％，同时仅遗漏了8.3％的实际幻觉。更重要的是，它可以减少近80％的虚假声明，同时保持响应的自然性和帮助性。

Conclusion: CausalGuard在需要多步骤逻辑的复杂推理任务中表现出色，在医疗诊断或金融分析等敏感领域也能很好地工作。

Abstract: While large language models have transformed how we interact with AI systems, they have a critical weakness: they confidently state false information that sounds entirely plausible. This "hallucination" problem has become a major barrier to using these models where accuracy matters most. Existing solutions either require retraining the entire model, add significant computational costs, or miss the root causes of why these hallucinations occur in the first place.
  We present CausalGuard, a new approach that combines causal reasoning with symbolic logic to catch and prevent hallucinations as they happen. Unlike previous methods that only check outputs after generation, our system understands the causal chain that leads to false statements and intervenes early in the process. CausalGuard works through two complementary paths: one that traces causal relationships between what the model knows and what it generates, and another that checks logical consistency using automated reasoning.
  Testing across twelve different benchmarks, we found that CausalGuard correctly identifies hallucinations 89.3\% of the time while missing only 8.3\% of actual hallucinations. More importantly, it reduces false claims by nearly 80\% while keeping responses natural and helpful. The system performs especially well on complex reasoning tasks where multiple steps of logic are required. Because CausalGuard shows its reasoning process, it works well in sensitive areas like medical diagnosis or financial analysis where understanding why a decision was made matters as much as the decision itself.

</details>


### [95] [Quantifying Skill and Chance: A Unified Framework for the Geometry of Games](https://arxiv.org/abs/2511.11611)
*David H. Silver*

Main category: cs.AI

TL;DR: 这篇论文提出了一个量化框架，用于区分游戏中技能和运气的影响，通过将它们建模为随机决策树的互补控制来源。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在区分游戏中技能和运气的影响，并提供一个量化的评估框架。

Method: 该研究通过将游戏结果分解为技能杠杆 K 和运气杠杆 L，定义了 Skill-Luck 指数 S(G)。同时，引入了波动率 Sigma 来量化连续回合中结果的不确定性。

Result: 通过对 30 款游戏的应用，揭示了一个从纯粹的运气（抛硬币，S = -1）到混合领域（如西洋双陆棋，S = 0，Sigma = 1.20）到纯粹的技能（国际象棋，S = +1，Sigma = 0）的连续统一体。扑克表现出适度的技能优势（S = 0.33），K = 0.40 +/- 0.03，Sigma = 0.80。

Conclusion: 该框架可扩展到一般的随机决策系统，从而能够对玩家影响力、游戏平衡和预测稳定性进行有原则的比较，并可应用于游戏设计、人工智能评估和风险评估。

Abstract: We introduce a quantitative framework for separating skill and chance in games by modeling them as complementary sources of control over stochastic decision trees. We define the Skill-Luck Index S(G) in [-1, 1] by decomposing game outcomes into skill leverage K and luck leverage L. Applying this to 30 games reveals a continuum from pure chance (coin toss, S = -1) through mixed domains such as backgammon (S = 0, Sigma = 1.20) to pure skill (chess, S = +1, Sigma = 0). Poker exhibits moderate skill dominance (S = 0.33) with K = 0.40 +/- 0.03 and Sigma = 0.80. We further introduce volatility Sigma to quantify outcome uncertainty over successive turns. The framework extends to general stochastic decision systems, enabling principled comparisons of player influence, game balance, and predictive stability, with applications to game design, AI evaluation, and risk assessment.

</details>


### [96] [Value-Aligned Prompt Moderation via Zero-Shot Agentic Rewriting for Safe Image Generation](https://arxiv.org/abs/2511.11693)
*Xin Zhao,Xiaojun Chen,Bingshan Liu,Zeyao Liu,Zhendong Zhao,Xiaoyan Gu*

Main category: cs.AI

TL;DR: VALOR是一个用于更安全、更有帮助的文本到图像生成的模块化、零样本代理框架，通过分层提示分析和与人类对齐的价值推理来减少不安全内容的生成。


<details>
  <summary>Details</summary>
Motivation: 当前的防御措施难以在不牺牲生成质量或产生高成本的情况下使输出与人类价值观对齐，因此引入VALOR框架。

Method: VALOR集成了分层提示分析与人类对齐的价值推理：多级NSFW检测器过滤词汇和语义风险；文化价值对齐模块识别违反社会规范、合法性和表征伦理的行为；意图消歧器检测微妙或间接的不安全含义。当检测到不安全内容时，提示会由大型语言模型在动态的、特定角色的指令下选择性地重写，旨在保留用户意图，同时加强对齐。如果生成的图像仍然未能通过安全检查，VALOR可以选择执行风格再生，以将输出引导到更安全的视觉领域，而不改变核心语义。

Result: 实验表明，在对抗性、模糊和价值敏感的提示中，VALOR显著减少了高达100.00%的不安全输出，同时保留了提示的有用性和创造力。

Conclusion: VALOR是一种可扩展且有效的方法，用于在开放世界环境中部署安全、对齐和有帮助的图像生成系统。

Abstract: Generative vision-language models like Stable Diffusion demonstrate remarkable capabilities in creative media synthesis, but they also pose substantial risks of producing unsafe, offensive, or culturally inappropriate content when prompted adversarially. Current defenses struggle to align outputs with human values without sacrificing generation quality or incurring high costs. To address these challenges, we introduce VALOR (Value-Aligned LLM-Overseen Rewriter), a modular, zero-shot agentic framework for safer and more helpful text-to-image generation. VALOR integrates layered prompt analysis with human-aligned value reasoning: a multi-level NSFW detector filters lexical and semantic risks; a cultural value alignment module identifies violations of social norms, legality, and representational ethics; and an intention disambiguator detects subtle or indirect unsafe implications. When unsafe content is detected, prompts are selectively rewritten by a large language model under dynamic, role-specific instructions designed to preserve user intent while enforcing alignment. If the generated image still fails a safety check, VALOR optionally performs a stylistic regeneration to steer the output toward a safer visual domain without altering core semantics. Experiments across adversarial, ambiguous, and value-sensitive prompts show that VALOR significantly reduces unsafe outputs by up to 100.00% while preserving prompt usefulness and creativity. These results highlight VALOR as a scalable and effective approach for deploying safe, aligned, and helpful image generation systems in open-world settings.

</details>


### [97] [Towards autonomous quantum physics research using LLM agents with access to intelligent tools](https://arxiv.org/abs/2511.11752)
*Sören Arlt,Xuemei Gu,Mario Krenn*

Main category: cs.AI

TL;DR: AI-Mandel是一个LLM智能体，可以在量子物理学中产生和实施想法。


<details>
  <summary>Details</summary>
Motivation: 目前科学领域AI的应用，起始的研究问题和目标几乎都由人类研究者提供。AI自主产生科学创意想法非常少见且通常比较模糊，将想法转化为可执行方案仍然是人类的任务。如果可以在一个连贯的系统中自动产生和实施想法，将大大改变人类在科学过程中的角色。

Method: AI-Mandel从文献中提取想法，并使用特定领域的AI工具将其转化为可以在实验室中实施的具体实验设计。

Result: AI-Mandel产生的想法通常具有科学价值，其中两个想法已经撰写了独立的科学后续论文。这些想法包括量子隐形传态的新变体、不定因果顺序量子网络的原语，以及基于量子信息传输闭环的几何相位新概念。

Conclusion: AI-Mandel是一个AI物理学家的原型演示，可以产生和实施具体的、可操作的想法。构建这样的系统不仅有助于加速科学发展，而且揭示了通往人类水平人工智能科学家的具体开放性挑战。

Abstract: Artificial intelligence (AI) is used in numerous fields of science, yet the initial research questions and targets are still almost always provided by human researchers. AI-generated creative ideas in science are rare and often vague, so that it remains a human task to execute them. Automating idea generation and implementation in one coherent system would significantly shift the role of humans in the scientific process. Here we present AI-Mandel, an LLM agent that can generate and implement ideas in quantum physics. AI-Mandel formulates ideas from the literature and uses a domain-specific AI tool to turn them into concrete experiment designs that can readily be implemented in laboratories. The generated ideas by AI-Mandel are often scientifically interesting - for two of them we have already written independent scientific follow-up papers. The ideas include new variations of quantum teleportation, primitives of quantum networks in indefinite causal orders, and new concepts of geometric phases based on closed loops of quantum information transfer. AI-Mandel is a prototypical demonstration of an AI physicist that can generate and implement concrete, actionable ideas. Building such a system is not only useful to accelerate science, but it also reveals concrete open challenges on the path to human-level artificial scientists.

</details>


### [98] [Learning to Refine: An Agentic RL Approach for Iterative SPARQL Query Construction](https://arxiv.org/abs/2511.11770)
*Floris Vossebeld,Shenghui Wang*

Main category: cs.AI

TL;DR: 大型语言模型（LLM）难以生成复杂、逻辑严密的 SPARQL 查询，阻碍了知识图谱问答的可靠性。该论文提出了一种新的 agentic 框架，通过强化学习训练 LLM，使其学习迭代 SPARQL 构建的策略，从而在 LC-QuAD 2.0 数据集上取得了显著的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 当前方法缺乏自适应策略，无法根据实时执行反馈动态调试查询。

Method: 该论文引入了一种 agentic 框架，LLM 通过强化学习学习迭代 SPARQL 构建的策略。

Result: 在 LC-QuAD 2.0 数据集上，该 agent 实现了 49.7% 的准确率，比最强的迭代 zero-shot 基线提高了 17.5 个百分点。

Conclusion: 该研究展示了一种通用蓝图，通过交互教导 agent 掌握形式化的符号工具，弥合了概率 LLM 和知识图谱结构化世界之间的差距。

Abstract: Generating complex, logically-sound SPARQL queries for multi-hop questions remains a critical bottleneck for Knowledge Graph Question Answering, as the brittle nature of one-shot generation by Large Language Models (LLMs) hinders reliable interaction with structured data. Current methods lack the adaptive policies needed to dynamically debug queries based on real-time execution feedback. This paper introduces a novel agentic framework where an LLM learns a resilient policy for the sequential process of iterative SPARQL construction. We show that a compact 3B-parameter model, trained exclusively via outcome-driven Reinforcement Learning (GRPO) without supervised fine-tuning, can learn effective policies for this task, discovering how to systematically recover from execution errors and refine its queries toward a correct answer. On a curated, executable single-answer subset of LC-QuAD 2.0, our agent achieves 49.7\% accuracy post-entity-linking, a significant 17.5 percentage point improvement over the strongest iterative zero-shot baseline. Further analysis reveals that while the agent's capability is driven by RL, its performance is enhanced by an explicit deliberative reasoning step that acts as a cognitive scaffold to improve policy precision. This work presents a generalizable blueprint for teaching agents to master formal, symbolic tools through interaction, bridging the gap between probabilistic LLMs and the structured world of Knowledge Graphs.

</details>


### [99] [On the Measure of a Model: From Intelligence to Generality](https://arxiv.org/abs/2511.11773)
*Ruchira Dhar,Ninell Oldenburg,Anders Soegaard*

Main category: cs.AI

TL;DR: 这篇论文认为，当前评估大型语言模型（LLMs）智能的基准测试（如ARC、Raven测试和Blackbird Task）存在问题，因为它们未能与实际任务（如问答、摘要或编码）的性能相关联。论文提出评估应该基于通用性，而非抽象的智能概念。


<details>
  <summary>Details</summary>
Motivation: 当前对LLM智能的评估缺乏稳定定义，且无法预测实际任务的性能，导致评估与现实世界的效用不符。

Method: 通过概念和形式分析，论文考察了通用性、稳定性、现实性这三个通常用于智能评估的假设，并论证只有通用性能够经受住概念和实证的推敲。

Result: 论文指出，通用性最好被理解为一个多任务学习问题，它直接将评估与可衡量的性能广度和可靠性联系起来。

Conclusion: 论文总结认为，应该重新构建AI进展的评估方式，并提出通用性是评估跨不同和不断发展的任务的能力的更稳定的基础。

Abstract: Benchmarks such as ARC, Raven-inspired tests, and the Blackbird Task are widely used to evaluate the intelligence of large language models (LLMs). Yet, the concept of intelligence remains elusive- lacking a stable definition and failing to predict performance on practical tasks such as question answering, summarization, or coding. Optimizing for such benchmarks risks misaligning evaluation with real-world utility. Our perspective is that evaluation should be grounded in generality rather than abstract notions of intelligence. We identify three assumptions that often underpin intelligence-focused evaluation: generality, stability, and realism. Through conceptual and formal analysis, we show that only generality withstands conceptual and empirical scrutiny. Intelligence is not what enables generality; generality is best understood as a multitask learning problem that directly links evaluation to measurable performance breadth and reliability. This perspective reframes how progress in AI should be assessed and proposes generality as a more stable foundation for evaluating capability across diverse and evolving tasks.

</details>


### [100] [Do LLMs Really Struggle at NL-FOL Translation? Revealing their Strengths via a Novel Benchmarking Strategy](https://arxiv.org/abs/2511.11816)
*Andrea Brunello,Luca Geatti,Michele Mignani,Angelo Montanari,Nicola Saccomanno*

Main category: cs.AI

TL;DR: 本文研究了自然语言到一阶逻辑 (NL-FOL) 的翻译问题，并 критически 评估了现有数据集和评估方案的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有研究对大型语言模型 (LLM) 执行 NL-FOL 翻译的能力存在争议。现有评估方法可能无法准确反映 LLM 的实际能力。

Method: 1. 批判性地检查现有数据集和评估协议；2. 提出一种新的评估协议，以区分真正的语义理解和表面模式识别；3. 使用新方法评估最先进的 LLM。

Result: 对话导向的 LLM 表现出强大的 NL-FOL 翻译技能和对句子级逻辑的真正理解，而以嵌入为中心的模型表现明显较差。

Conclusion: 最先进的对话导向的 LLM 能够进行 NL-FOL 翻译，并且真正理解句子级别的逻辑。

Abstract: Due to its expressiveness and unambiguous nature, First-Order Logic (FOL) is a powerful formalism for representing concepts expressed in natural language (NL). This is useful, e.g., for specifying and verifying desired system properties. While translating FOL into human-readable English is relatively straightforward, the inverse problem, converting NL to FOL (NL-FOL translation), has remained a longstanding challenge, for both humans and machines. Although the emergence of Large Language Models (LLMs) promised a breakthrough, recent literature provides contrasting results on their ability to perform NL-FOL translation. In this work, we provide a threefold contribution. First, we critically examine existing datasets and protocols for evaluating NL-FOL translation performance, revealing key limitations that may cause a misrepresentation of LLMs' actual capabilities. Second, to overcome these shortcomings, we propose a novel evaluation protocol explicitly designed to distinguish genuine semantic-level logical understanding from superficial pattern recognition, memorization, and dataset contamination. Third, using this new approach, we show that state-of-the-art, dialogue-oriented LLMs demonstrate strong NL-FOL translation skills and a genuine grasp of sentence-level logic, whereas embedding-centric models perform markedly worse.

</details>


### [101] [TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models](https://arxiv.org/abs/2511.11831)
*Wenhao Zhou,Hao Zheng,Rong Zhao*

Main category: cs.AI

TL;DR: 本文介绍了一个名为 TopoPerception 的新基准，用于评估大型视觉语言模型 (LVLM) 的全局视觉感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有 LVLM 的视觉感知模块存在瓶颈，并且现有评估基准存在可能导致模型感知能力被高估的局部捷径。

Method: TopoPerception 基准利用拓扑属性来评估 LVLM 的全局视觉感知能力，拓扑属性对图像的全局结构敏感且对局部特征不变。

Result: 在 TopoPerception 上评估现有模型发现，即使在最粗糙的感知粒度下，所有模型的性能都不优于随机水平，表明其缺乏感知全局视觉特征的能力。更强大的模型反而表现更差。

Conclusion: TopoPerception 揭示了当前 LVLM 的一个关键瓶颈，并为提高其全局视觉感知能力提供了方向。

Abstract: Large Vision-Language Models (LVLMs) typically align visual features from an encoder with a pre-trained Large Language Model (LLM). However, this makes the visual perception module a bottleneck, which constrains the overall capabilities of LVLMs. Conventional evaluation benchmarks, while rich in visual semantics, often contain unavoidable local shortcuts that can lead to an overestimation of models' perceptual abilities. Here, we introduce TopoPerception, a benchmark that leverages topological properties to rigorously evaluate the global visual perception capabilities of LVLMs across various granularities. Since topology depends on the global structure of an image and is invariant to local features, TopoPerception enables a shortcut-free assessment of global perception, fundamentally distinguishing it from semantically rich tasks. We evaluate state-of-the-art models on TopoPerception and find that even at the coarsest perceptual granularity, all models perform no better than random chance, indicating a profound inability to perceive global visual features. Notably, a consistent trend emerge within model families: more powerful models with stronger reasoning capabilities exhibit lower accuracy. This suggests that merely scaling up models is insufficient to address this deficit and may even exacerbate it. Progress may require new training paradigms or architectures. TopoPerception not only exposes a critical bottleneck in current LVLMs but also offers a lens and direction for improving their global visual perception. The data and code are publicly available at: https://github.com/Wenhao-Zhou/TopoPerception.

</details>


### [102] [End to End AI System for Surgical Gesture Sequence Recognition and Clinical Outcome Prediction](https://arxiv.org/abs/2511.11899)
*Xi Li,Nicholas Matsumoto,Ujjwal Pasupulety,Atharva Deo,Cherine Yang,Jay Moran,Miguel E. Hernandez,Peter Wager,Jasmine Lin,Jeanine Kim,Alvin C. Goh,Christian Wagner,Geoffrey A. Sonn,Andrew J. Hung*

Main category: cs.AI

TL;DR: F2O是一个端到端系统，它可以将组织解剖视频转换为手势序列，并揭示与术后结果相关的模式。


<details>
  <summary>Details</summary>
Motivation: 对术中行为的细粒度分析及其对患者预后的影响仍然是一个长期存在的挑战。

Method: 利用基于transformer的空间和时间建模以及逐帧分类，F2O稳健地检测机器人辅助前列腺癌根治术神经保留步骤中的连续短时手势（~2秒）。

Result: F2O衍生的特征（手势频率、持续时间和转换）预测术后结果的准确性与人工注释相当（0.79 vs. 0.75；重叠95% CI）。

Conclusion: 通过实现自动可解释的评估，F2O为数据驱动的手术反馈和前瞻性临床决策支持奠定了基础。

Abstract: Fine-grained analysis of intraoperative behavior and its impact on patient outcomes remain a longstanding challenge. We present Frame-to-Outcome (F2O), an end-to-end system that translates tissue dissection videos into gesture sequences and uncovers patterns associated with postoperative outcomes. Leveraging transformer-based spatial and temporal modeling and frame-wise classification, F2O robustly detects consecutive short (~2 seconds) gestures in the nerve-sparing step of robot-assisted radical prostatectomy (AUC: 0.80 frame-level; 0.81 video-level). F2O-derived features (gesture frequency, duration, and transitions) predicted postoperative outcomes with accuracy comparable to human annotations (0.79 vs. 0.75; overlapping 95% CI). Across 25 shared features, effect size directions were concordant with small differences (~ 0.07), and strong correlation (r = 0.96, p < 1e-14). F2O also captured key patterns linked to erectile function recovery, including prolonged tissue peeling and reduced energy use. By enabling automatic interpretable assessment, F2O establishes a foundation for data-driven surgical feedback and prospective clinical decision support.

</details>


### [103] [Forgetting-MarI: LLM Unlearning via Marginal Information Regularization](https://arxiv.org/abs/2511.11914)
*Shizhou Xu,Yuan Ni,Stefan Broecker,Thomas Strohmer*

Main category: cs.AI

TL;DR: 提出了一种名为 Forgetting-MarI 的 LLM unlearning 框架，它只删除待 unlearn 数据带来的额外信息，同时保留由保留数据支持的信息。


<details>
  <summary>Details</summary>
Motivation: 随着 AI 模型在不断扩展的数据集上进行训练，从训练好的模型中移除特定数据的影响变得至关重要，以保护隐私和符合法规。

Method: 通过惩罚边缘信息，我们的方法对 unlearn 数据集在训练模型中的残余影响产生一个明确的上限，从而提供可证明的不可检测性。

Result: 大量实验证实，我们的方法优于当前最先进的 unlearning 方法，在各种基准测试中提供可靠的 forgetting 效果和更好保留的通用模型性能。

Conclusion: 这一进步代表着朝着使 AI 系统更可控，更符合隐私和版权法规，而不损害其有效性迈出的重要一步。

Abstract: As AI models are trained on ever-expanding datasets, the ability to remove the influence of specific data from trained models has become essential for privacy protection and regulatory compliance. Unlearning addresses this challenge by selectively removing parametric knowledge from the trained models without retraining from scratch, which is critical for resource-intensive models such as Large Language Models (LLMs). Existing unlearning methods often degrade model performance by removing more information than necessary when attempting to ''forget'' specific data. We introduce Forgetting-MarI, an LLM unlearning framework that provably removes only the additional (marginal) information contributed by the data to be unlearned, while preserving the information supported by the data to be retained. By penalizing marginal information, our method yields an explicit upper bound on the unlearn dataset's residual influence in the trained models, providing provable undetectability. Extensive experiments confirm that our approach outperforms current state-of-the-art unlearning methods, delivering reliable forgetting and better preserved general model performance across diverse benchmarks. This advancement represents an important step toward making AI systems more controllable and compliant with privacy and copyright regulations without compromising their effectiveness.

</details>


### [104] [An Analysis of Architectural Impact on LLM-based Abstract Visual Reasoning: A Systematic Benchmark on RAVEN-FAIR](https://arxiv.org/abs/2511.11916)
*Sinan Urgun,Seçkin Arı*

Main category: cs.AI

TL;DR: 本研究旨在评估大型语言模型在抽象视觉推理问题中的性能，发现GPT-4.1-Mini模型在所有架构中始终表现出最高的整体准确率。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在抽象视觉推理问题中的性能表现。

Method: 使用了四个大型语言模型（GPT-4.1-Mini, Claude-3.5-Haiku, Gemini-1.5-Flash, Llama-3.3-70b），利用四种不同的推理架构（单次、嵌入控制重复、自我反思和多智能体）在 RAVEN-FAIR 数据集上进行实验，并通过SSIM和LPIPS指标评估视觉响应，分析思维链得分和错误类型。

Result: GPT-4.1-Mini模型在所有架构中始终表现出最高的整体准确率。多智能体架构有时会改变跨模型的语义和数字平衡，但这些效果并非普遍有益。每个模型对架构设计表现出不同的敏感性模式。响应覆盖率的变化进一步成为混淆直接跨架构比较的因素。

Conclusion: 推理有效性仍然是特定于模型的，单次评估可能导致不可靠的结论。

Abstract: This study aims to systematically evaluate the performance of large language models (LLMs) in abstract visual reasoning problems. We examined four LLM models (GPT-4.1-Mini, Claude-3.5-Haiku, Gemini-1.5-Flash, Llama-3.3-70b) utilizing four different reasoning architectures (single-shot, embedding-controlled repetition, self-reflection, and multi-agent) on the RAVEN-FAIR dataset. Visual responses generated through a three-stage process (JSON extraction, LLM reasoning, and Tool Function) were evaluated using SSIM and LPIPS metrics; Chain-of-Thought scores and error types (semantic hallucination, numeric misperception) were analyzed. Results demonstrate that GPT-4.1-Mini consistently achieved the highest overall accuracy across all architectures, indicating a strong reasoning capability. While the multi-agent architecture occasionally altered semantic and numeric balance across models, these effects were not uniformly beneficial. Instead, each model exhibited distinct sensitivity patterns to architectural design, underscoring that reasoning effectiveness remains model-specific. Variations in response coverage further emerged as a confounding factor that complicates direct cross-architecture comparison. To estimate the upper-bound performance of each configuration, we report the best of five independent runs, representing a best-case scenario rather than an averaged outcome. This multi-run strategy aligns with recent recommendations, which emphasize that single-run evaluations are fragile and may lead to unreliable conclusions.

</details>


### [105] [Looking Forward: Challenges and Opportunities in Agentic AI Reliability](https://arxiv.org/abs/2511.11921)
*Liudong Xing,Janet,Lin*

Main category: cs.AI

TL;DR: 本章探讨了构建可靠的AI系统，特别是Agentic AI系统所面临的挑战和未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 旨在构建可靠的Agentic AI系统。

Method: 讨论了与降低级联故障风险相关的几个开放研究问题。

Result: 阐明了动态环境、不一致的任务执行、不可预测的突发行为以及资源密集型可靠性机制等方面的研究挑战和机遇。

Conclusion: 探讨了测试和评估Agentic AI系统可靠性的几个研究方向。

Abstract: This chapter presents perspectives for challenges and future development in building reliable AI systems, particularly, agentic AI systems. Several open research problems related to mitigating the risks of cascading failures are discussed. The chapter also sheds lights on research challenges and opportunities in aspects including dynamic environments, inconsistent task execution, unpredictable emergent behaviors, as well as resource-intensive reliability mechanisms. In addition, several research directions along the line of testing and evaluating reliability of agentic AI systems are also discussed.

</details>


### [106] [A Neuromorphic Architecture for Scalable Event-Based Control](https://arxiv.org/abs/2511.11924)
*Yongkang Huo,Fulvio Forni,Rodolphe Sepulchre*

Main category: cs.AI

TL;DR: 提出了一种可扩展的神经形态控制架构，它结合了离散计算的可靠性和连续调节的可调性。


<details>
  <summary>Details</summary>
Motivation: 构建一个可靠且可调的神经形态控制架构。

Method: 引入了“反弹胜者全得 (RWTA)”作为基本元件，并结合胜者全得状态机的离散计算能力和可兴奋生物物理电路的连续调谐能力。

Result: 通过蛇形机器人的神经系统设计，证明了该架构的多功能性、鲁棒性和模块化。

Conclusion: 所提出的基于事件的框架在统一的物理建模语言中解决了连续节奏生成和离散决策问题。

Abstract: This paper introduces the ``rebound Winner-Take-All (RWTA)" motif as the basic element of a scalable neuromorphic control architecture. From the cellular level to the system level, the resulting architecture combines the reliability of discrete computation and the tunability of continuous regulation: it inherits the discrete computation capabilities of winner-take-all state machines and the continuous tuning capabilities of excitable biophysical circuits. The proposed event-based framework addresses continuous rhythmic generation and discrete decision-making in a unified physical modeling language. We illustrate the versatility, robustness, and modularity of the architecture through the nervous system design of a snake robot.

</details>


### [107] [Augmenting The Weather: A Hybrid Counterfactual-SMOTE Algorithm for Improving Crop Growth Prediction When Climate Changes](https://arxiv.org/abs/2511.11945)
*Mohammed Temraz,Mark T Keane*

Main category: cs.AI

TL;DR: 提出了一种新的数据增强方法 CFA-SMOTE，用于解决气候变化背景下的数据分布外和类不平衡问题，尤其是在预测受气候影响的农业数据方面。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习方法在处理气候变化引起的数据分布外事件时表现不佳，因为历史数据中缺乏足够的气候异常事件样本。

Method: 结合了解释性 AI (XAI) 中的基于实例的反事实方法和 SMOTE 类不平衡方法，生成合成数据点来模拟气候异常事件，从而增强数据集。

Result: 通过对比实验，验证了 CFA-SMOTE 方法在不同条件下的性能，并与基准的反事实和类不平衡方法进行了比较。实验领域为预测 2018 年欧洲干旱和饲料危机期间爱尔兰奶牛场的牧草生长情况。

Conclusion: CFA-SMOTE 能够有效应对气候变化带来的数据挑战，提高预测性能。

Abstract: In recent years, humanity has begun to experience the catastrophic effects of climate change as economic sectors (such as agriculture) struggle with unpredictable and extreme weather events. Artificial Intelligence (AI) should help us handle these climate challenges but its most promising solutions are not good at dealing with climate-disrupted data; specifically, machine learning methods that work from historical data-distributions, are not good at handling out-of-distribution, outlier events. In this paper, we propose a novel data augmentation method, that treats the predictive problems around climate change as being, in part, due to class-imbalance issues; that is, prediction from historical datasets is difficult because, by definition, they lack sufficient minority-class instances of "climate outlier events". This novel data augmentation method -- called Counterfactual-Based SMOTE (CFA-SMOTE) -- combines an instance-based counterfactual method from Explainable AI (XAI) with the well-known class-imbalance method, SMOTE. CFA-SMOTE creates synthetic data-points representing outlier, climate-events that augment the dataset to improve predictive performance. We report comparative experiments using this CFA-SMOTE method, comparing it to benchmark counterfactual and class-imbalance methods under different conditions (i.e., class-imbalance ratios). The focal climate-change domain used relies on predicting grass growth on Irish dairy farms, during Europe-wide drought and forage crisis of 2018.

</details>


### [108] [LLM-Assisted Formalization Enables Deterministic Detection of Statutory Inconsistency in the Internal Revenue Code](https://arxiv.org/abs/2511.11954)
*Borchuluun Yadamsuren,Steven Keith Platt,Miguel Diaz*

Main category: cs.AI

TL;DR: 本研究介绍了一种混合神经符号框架，该框架通过将大型语言模型 (LLM) 与符号逻辑相结合，实现了复杂法律中法定不一致性的确定性检测。


<details>
  <summary>Details</summary>
Motivation: 基于 LLM 的方法可以支持合规性、公平性和法规起草，但特定于税收的应用程序仍然很少。一个关键的挑战是，此类模型难以进行分层处理和深度结构化推理，尤其是在长文本中。

Method: 本研究使用 GPT-4o 将第 121 条翻译成 Prolog 规则，并在 SWISH 中对其进行完善。然后将这些规则纳入提示中，以测试 Prolog 增强提示是否提高了 GPT-4o 的不一致性检测能力。

Result: GPT-4o 在三种策略中仅检测到一种策略的不一致性（33% 的准确率），但其推理质量不同：自然语言提示实现了 100% 的规则覆盖率，而 Prolog 增强提示实现了 66%，表明法定分析不完整。相比之下，混合 Prolog 模型产生了确定性且可重现的结果。在 GPT-5 的指导下，该模型正式确定了 IRC 部分的竞争性解释，并成功检测到不一致区域。

Conclusion: 研究结果表明，以符号逻辑为基础的 LLM 辅助形式化能够实现透明且可靠的法定不一致性检测。

Abstract: This study introduces a hybrid neuro-symbolic framework that achieves deterministic detection of statutory inconsistency in complex law. We use the U.S. Internal Revenue Code (IRC) as a case study because its complexity makes it a fertile domain for identifying conflicts. Our research offers a solution for detecting inconsistent provisions by combining Large Language Models (LLMs) with symbolic logic.
  LLM-based methods can support compliance, fairness, and statutory drafting, yet tax-specific applications remain sparse. A key challenge is that such models struggle with hierarchical processing and deep structured reasoning, especially over long text.
  This research addresses these gaps through experiments using GPT-4o, GPT-5, and Prolog. GPT-4o was first used to translate Section 121 into Prolog rules and refine them in SWISH. These rules were then incorporated into prompts to test whether Prolog-augmented prompting improved GPT-4o's inconsistency detection. GPT-4o, whether prompted with natural language alone or with Prolog augmentation, detected the inconsistency in only one of three strategies (33 percent accuracy), but its reasoning quality differed: natural-language prompting achieved 100 percent rule coverage, while Prolog-augmented prompting achieved 66 percent, indicating more incomplete statutory analysis.
  In contrast to probabilistic prompting, the hybrid Prolog model produced deterministic and reproducible results. Guided by GPT-5 for refinement, the model formalized the IRC section's competing interpretations and successfully detected an inconsistency zone. Validation tests confirm that the Prolog implementation is accurate, internally consistent, deterministic, and capable of autonomously identifying inconsistencies. These findings show that LLM-assisted formalization, anchored in symbolic logic, enables transparent and reliable statutory inconsistency detection.

</details>


### [109] [Improving Autoformalization Using Direct Dependency Retrieval](https://arxiv.org/abs/2511.11990)
*Shaoqi Wang,Lu Yu,Chunjie Yang*

Main category: cs.AI

TL;DR: 这篇论文提出了一种基于直接依赖检索（DDR）的检索增强框架，用于语句自动形式化，旨在解决现有方法缺乏上下文感知和依赖检索效果不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 现有语句自动形式化方法缺乏上下文感知，导致形式定义和定理的幻觉问题。检索增强方法在形式库依赖检索方面精度和召回率较低，且缺乏有效利用不断增长的公共数据集的可扩展性。

Method: 该方法基于DDR，直接从自然语言数学描述生成候选库依赖，并通过高效的后缀数组检查验证其在形式库中的存在性。构建了一个超过50万样本的依赖检索数据集，并微调了一个高精度DDR模型。

Result: 实验结果表明，DDR模型在检索精度和召回率方面显著优于现有方法。配备DDR的自动形式化器在单次尝试准确率和多次尝试稳定性方面均优于使用传统选择RAG方法的模型。

Conclusion: 该研究提出的DDR方法能够有效提高语句自动形式化的性能，并在检索精度、召回率和模型稳定性方面都取得了显著的提升。

Abstract: The convergence of deep learning and formal mathematics has spurred research in formal verification. Statement autoformalization, a crucial first step in this process, aims to translate informal descriptions into machine-verifiable representations but remains a significant challenge. The core difficulty lies in the fact that existing methods often suffer from a lack of contextual awareness, leading to hallucination of formal definitions and theorems. Furthermore, current retrieval-augmented approaches exhibit poor precision and recall for formal library dependency retrieval, and lack the scalability to effectively leverage ever-growing public datasets. To bridge this gap, we propose a novel retrieval-augmented framework based on DDR (\textit{Direct Dependency Retrieval}) for statement autoformalization. Our DDR method directly generates candidate library dependencies from natural language mathematical descriptions and subsequently verifies their existence within the formal library via an efficient suffix array check. Leveraging this efficient search mechanism, we constructed a dependency retrieval dataset of over 500,000 samples and fine-tuned a high-precision DDR model. Experimental results demonstrate that our DDR model significantly outperforms SOTA methods in both retrieval precision and recall. Consequently, an autoformalizer equipped with DDR shows consistent performance advantages in both single-attempt accuracy and multi-attempt stability compared to models using traditional selection-based RAG methods.

</details>


### [110] [Look As You Think: Unifying Reasoning and Visual Evidence Attribution for Verifiable Document RAG via Reinforcement Learning](https://arxiv.org/abs/2511.12003)
*Shuochen Liu,Pengfei Luo,Chao Zhang,Yuhao Chen,Haotian Zhang,Qi Liu,Xin Kou,Tong Xu,Enhong Chen*

Main category: cs.AI

TL;DR: 本文提出了一种名为Chain-of-Evidence (CoE) 的视觉文档检索增强生成 (VD-RAG) 范例，用于识别视觉文档中的精确证据来源，并通过 Look As You Think (LAT) 框架进行训练，以提高视觉语言模型在多模态问答中的可靠性和可验证性。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏细粒度的监督和推理过程中的逐步可追溯性。

Method: 提出了Look As You Think (LAT)，一个强化学习框架，通过在推理步骤中将参考元素与特定区域进行绑定，并使用边界框和页面索引，来训练模型生成可验证的推理路径。

Result: 在Paper-和Wiki-VISA基准测试中，LAT在soft exact match (EM) 上平均提高了8.23%，在IoU@0.5上平均提高了47.0%。

Conclusion: LAT 不仅优于直接生成带有属性的答案的监督微调基线，而且在跨领域表现出更强的泛化能力。

Abstract: Aiming to identify precise evidence sources from visual documents, visual evidence attribution for visual document retrieval-augmented generation (VD-RAG) ensures reliable and verifiable predictions from vision-language models (VLMs) in multimodal question answering. Most existing methods adopt end-to-end training to facilitate intuitive answer verification. However, they lack fine-grained supervision and progressive traceability throughout the reasoning process. In this paper, we introduce the Chain-of-Evidence (CoE) paradigm for VD-RAG. CoE unifies Chain-of-Thought (CoT) reasoning and visual evidence attribution by grounding reference elements in reasoning steps to specific regions with bounding boxes and page indexes. To enable VLMs to generate such evidence-grounded reasoning, we propose Look As You Think (LAT), a reinforcement learning framework that trains models to produce verifiable reasoning paths with consistent attribution. During training, LAT evaluates the attribution consistency of each evidence region and provides rewards only when the CoE trajectory yields correct answers, encouraging process-level self-verification. Experiments on vanilla Qwen2.5-VL-7B-Instruct with Paper- and Wiki-VISA benchmarks show that LAT consistently improves the vanilla model in both single- and multi-image settings, yielding average gains of 8.23% in soft exact match (EM) and 47.0% in IoU@0.5. Meanwhile, LAT not only outperforms the supervised fine-tuning baseline, which is trained to directly produce answers with attribution, but also exhibits stronger generalization across domains.

</details>


### [111] [Adaptive Diagnostic Reasoning Framework for Pathology with Multimodal Large Language Models](https://arxiv.org/abs/2511.12008)
*Yunqi Hong,Johnson Kao,Liam Edwards,Nein-Tzu Liu,Chung-Yen Huang,Alex Oliveira-Kowaleski,Cho-Jui Hsieh,Neil Y. C. Lin*

Main category: cs.AI

TL;DR: RECAP-PATH: An interpretable AI framework for pathology that uses self-learning to generate evidence-linked diagnostic reasoning, improving accuracy and trust.


<details>
  <summary>Details</summary>
Motivation: Current AI pathology tools lack human-readable reasoning, hindering adoption.

Method: A two-phase learning process: diversification to expand pathology-style explanations and optimization to refine them for accuracy. Requires small labeled datasets and no white-box access or weight updates.

Result: RECAP-PATH produced rationales aligned with expert assessment and delivered substantial gains in diagnostic accuracy on breast and prostate datasets.

Conclusion: RECAP-PATH provides clinically trustworthy AI by uniting visual understanding with reasoning, demonstrating a generalizable path toward evidence-linked interpretation.

Abstract: AI tools in pathology have improved screening throughput, standardized quantification, and revealed prognostic patterns that inform treatment. However, adoption remains limited because most systems still lack the human-readable reasoning needed to audit decisions and prevent errors. We present RECAP-PATH, an interpretable framework that establishes a self-learning paradigm, shifting off-the-shelf multimodal large language models from passive pattern recognition to evidence-linked diagnostic reasoning. At its core is a two-phase learning process that autonomously derives diagnostic criteria: diversification expands pathology-style explanations, while optimization refines them for accuracy. This self-learning approach requires only small labeled sets and no white-box access or weight updates to generate cancer diagnoses. Evaluated on breast and prostate datasets, RECAP-PATH produced rationales aligned with expert assessment and delivered substantial gains in diagnostic accuracy over baselines. By uniting visual understanding with reasoning, RECAP-PATH provides clinically trustworthy AI and demonstrates a generalizable path toward evidence-linked interpretation.

</details>


### [112] [Intelligent Collaborative Optimization for Rubber Tyre Film Production Based on Multi-path Differentiated Clipping Proximal Policy Optimization](https://arxiv.org/abs/2511.12060)
*Yinghao Ruan,Wei Pang,Shuaihao Liu,Huili Yang,Leyi Han,Xinghui Dong*

Main category: cs.AI

TL;DR: 智能制造正在解决传统轮胎行业集中调度和生产线配置不灵活的局限性，尤其是在应对动态生产需求方面。


<details>
  <summary>Details</summary>
Motivation: 现代轮胎制造系统形成复杂的紧密耦合子系统网络，具有明显的非线性相互作用和涌现动力学。这种复杂性使得多个子系统的有效协调成为一项重要但艰巨的任务。

Method: 我们引入了一种深度强化学习算法：多路径差异化剪裁近端策略优化 (MPD-PPO)。该算法采用具有差异化梯度剪裁约束的多分支策略架构，以确保稳定高效的高维策略更新。

Result: 通过在橡胶轮胎薄膜生产中的宽度和厚度控制实验验证，MPD-PPO 在调整精度和运营效率方面均表现出显着改进。

Conclusion: 该框架成功解决了关键挑战，包括高维度、多目标权衡和动态适应，从而为轮胎制造中的实时工业部署提供增强的性能和生产稳定性。

Abstract: The advent of smart manufacturing is addressing the limitations of traditional centralized scheduling and inflexible production line configurations in the rubber tyre industry, especially in terms of coping with dynamic production demands. Contemporary tyre manufacturing systems form complex networks of tightly coupled subsystems pronounced nonlinear interactions and emergent dynamics. This complexity renders the effective coordination of multiple subsystems, posing an essential yet formidable task. For high-dimensional, multi-objective optimization problems in this domain, we introduce a deep reinforcement learning algorithm: Multi-path Differentiated Clipping Proximal Policy Optimization (MPD-PPO). This algorithm employs a multi-branch policy architecture with differentiated gradient clipping constraints to ensure stable and efficient high-dimensional policy updates. Validated through experiments on width and thickness control in rubber tyre film production, MPD-PPO demonstrates substantial improvements in both tuning accuracy and operational efficiency. The framework successfully tackles key challenges, including high dimensionality, multi-objective trade-offs, and dynamic adaptation, thus delivering enhanced performance and production stability for real-time industrial deployment in tyre manufacturing.

</details>


### [113] [Bayesian Optimization in Language Space: An Eval-Efficient AI Self-Improvement Framework](https://arxiv.org/abs/2511.12063)
*Enoch Hyunwook Kang,Hema Yoganarasimhan*

Main category: cs.AI

TL;DR: 提出了一种名为T-BoN BO的语言空间贝叶斯优化框架，用于提高AI自我改进的评估效率。


<details>
  <summary>Details</summary>
Motivation: 现有自改进AI主要关注prompt优化，以查询效率为衡量标准，但在许多社会应用中，评估成本远高于生成成本。为了优化评估效率，需要将贝叶斯优化扩展到语言领域，但直接估计LLM中的采集函数很困难。

Method: 通过证明Best-of-N选择策略和文本梯度可以模拟UCB采集函数的梯度行为，提出了T-BoN BO框架。

Result: 在persona分布的自动广告调整任务中，T-BoN BO的表现优于当前流行的baseline方法。

Conclusion: T-BoN BO是一种简单且评估效率高的语言空间贝叶斯优化框架，适用于AI自我改进。

Abstract: Large Language Models (LLMs) have recently enabled self-improving AI, i.e., AI that iteratively generates, evaluates, and refines its own outcomes. Recent studies have shown that self-improving AI focusing on prompt optimization can outperform state-of-the-art reinforcement-learning fine-tuned LLMs. Here, their `performance' is typically measured by query efficiency - the number of LLM-generated solution samples required to meet a certain performance threshold. However, in many societal applications, the primary limitation is not generating new solutions but evaluating them. For instance, evaluating an ad's effectiveness requires significant human feedback, which is far more costly and time-consuming than generating a candidate ad. To optimize for the evaluation efficiency objective, a natural approach is to extend Bayesian Optimization (BO), a framework proven optimal for evaluation efficiency, to the language domain. However, the difficulty of directly estimating suitable acquisition functions in LLMs' minds makes this extension challenging. This paper overcomes this challenge by proving that the combination of the simple and widely used Best-of-N selection strategy and simple textual gradients (i.e., textual edits from a critic model) statistically emulates the behavior of the gradients on the canonical UCB acquisition function, which induces optimal exploration in terms of evaluation efficiency. Based on this result, we propose TextGrad-Best-of-N Bayesian Optimization (T-BoN BO), a simple and eval-efficient language-space Bayesian optimization framework for AI self-improvement. We also empirically validate T-BoN BO by applying it to automated ad alignment tasks for persona distribution, demonstrating its superior performance compared to popular state-of-the-art baselines.

</details>


### [114] [No-Regret Strategy Solving in Imperfect-Information Games via Pre-Trained Embedding](https://arxiv.org/abs/2511.12083)
*Yanchang Fu,Shengda Liu,Pei Xu,Kaiqi Huang*

Main category: cs.AI

TL;DR: 提出了一种新的在不完美信息扩展形式博弈（IIEFG）中求解策略的方法，该方法通过预训练和嵌入信息集的特征到低维连续空间中，更精确地捕捉信息集之间的区别和联系。


<details>
  <summary>Details</summary>
Motivation: 现有AI方法依赖于预训练的离散聚类进行抽象，但硬分类会不可逆转地丢失关键信息，从而影响求解质量。

Method: 提出了一种名为Embedding CFR的算法，该算法在嵌入空间中进行后悔累积和策略更新，并通过理论分析验证了其降低累积后悔的能力。

Result: 在扑克实验中，Embedding CFR在相同的空间开销下，实现了比基于聚类的抽象算法更快的可利用性收敛。

Conclusion: Embedding CFR是一种有效的策略求解方法，并且是扑克AI中第一个通过低维嵌入预训练信息集抽象以进行策略求解的算法。

Abstract: High-quality information set abstraction remains a core challenge in solving large-scale imperfect-information extensive-form games (IIEFGs)-such as no-limit Texas Hold'em-where the finite nature of spatial resources hinders strategy solving over the full game. State-of-the-art AI methods rely on pre-trained discrete clustering for abstraction, yet their hard classification irreversibly loses critical information: specifically, the quantifiable subtle differences between information sets-vital for strategy solving-thereby compromising the quality of such solving. Inspired by the word embedding paradigm in natural language processing, this paper proposes the Embedding CFR algorithm, a novel approach for solving strategies in IIEFGs within an embedding space. The algorithm pre-trains and embeds features of isolated information sets into an interconnected low-dimensional continuous space, where the resulting vectors more precisely capture both the distinctions and connections between information sets. Embedding CFR presents a strategy-solving process driven by regret accumulation and strategy updates within this embedding space, with accompanying theoretical analysis verifying its capacity to reduce cumulative regret. Experiments on poker show that with the same spatial overhead, Embedding CFR achieves significantly faster exploitability convergence compared to cluster-based abstraction algorithms, confirming its effectiveness. Furthermore, to our knowledge, it is the first algorithm in poker AI that pre-trains information set abstractions through low-dimensional embedding for strategy solving.

</details>


### [115] [Mobile-Agent-RAG: Driving Smart Multi-Agent Coordination with Contextual Knowledge Empowerment for Long-Horizon Mobile Automation](https://arxiv.org/abs/2511.12254)
*Yuxiang Zhou,Jichang Li,Yanhao Zhang,Haonan Lu,Guanbin Li*

Main category: cs.AI

TL;DR: 本文提出了一种名为 Mobile-Agent-RAG 的新型分层多智能体框架，通过双层检索增强来解决现有移动智能体在真实世界、长程、跨应用任务中成功率不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有移动智能体过度依赖 MLLM 内的静态知识，导致高层规划中的战略性幻觉和底层 UI 执行中的操作错误。

Method: 该框架在规划阶段引入 Manager-RAG，通过检索人工验证的任务计划来减少战略性幻觉；在执行阶段引入 Operator-RAG，通过检索精确的底层指导来提高执行准确性。构建了两个专门的检索知识库，并提出了 Mobile-Eval-RAG 基准来评估智能体。

Result: Mobile-Agent-RAG 在任务完成率和步骤效率方面显著优于现有技术。

Conclusion: Mobile-Agent-RAG 为上下文感知、可靠的多智能体移动自动化建立了一个强大的范例。

Abstract: Mobile agents show immense potential, yet current state-of-the-art (SoTA) agents exhibit inadequate success rates on real-world, long-horizon, cross-application tasks. We attribute this bottleneck to the agents' excessive reliance on static, internal knowledge within MLLMs, which leads to two critical failure points: 1) strategic hallucinations in high-level planning and 2) operational errors during low-level execution on user interfaces (UI). The core insight of this paper is that high-level planning and low-level UI operations require fundamentally distinct types of knowledge. Planning demands high-level, strategy-oriented experiences, whereas operations necessitate low-level, precise instructions closely tied to specific app UIs. Motivated by these insights, we propose Mobile-Agent-RAG, a novel hierarchical multi-agent framework that innovatively integrates dual-level retrieval augmentation. At the planning stage, we introduce Manager-RAG to reduce strategic hallucinations by retrieving human-validated comprehensive task plans that provide high-level guidance. At the execution stage, we develop Operator-RAG to improve execution accuracy by retrieving the most precise low-level guidance for accurate atomic actions, aligned with the current app and subtask. To accurately deliver these knowledge types, we construct two specialized retrieval-oriented knowledge bases. Furthermore, we introduce Mobile-Eval-RAG, a challenging benchmark for evaluating such agents on realistic multi-app, long-horizon tasks. Extensive experiments demonstrate that Mobile-Agent-RAG significantly outperforms SoTA baselines, improving task completion rate by 11.0% and step efficiency by 10.2%, establishing a robust paradigm for context-aware, reliable multi-agent mobile automation.

</details>


### [116] [KrwEmd: Revising the Imperfect-Recall Abstraction from Forgetting Everything](https://arxiv.org/abs/2511.12089)
*Yanchang Fu,Qiyue Yin,Shengda Liu,Pei Xu,Kaiqi Huang*

Main category: cs.AI

TL;DR: 这篇论文提出了一种名为 KrwEmd 的新算法，旨在解决德州扑克等游戏中过度抽象的问题，过度抽象会损害 AI 性能。


<details>
  <summary>Details</summary>
Motivation: 动机是现有的手部抽象方法在解决大型不完全信息博弈时存在过度抽象的问题，因为它们完全丢弃了历史信息，从而损害了 AI 性能。

Method: 该论文首先引入了 k-recall 胜率特征，该特征利用未来和历史游戏信息来区分信号观察信息集，并量化它们的相似性。然后，开发了 KrwEmd 算法，该算法使用 earth mover's distance 来衡量信号观察信息集特征之间的差异，从而对它们进行聚类。

Result: 实验结果表明，与现有算法相比，KrwEmd 显着提高了 AI 游戏性能。

Conclusion: KrwEmd 算法有效地解决了手部抽象中的过度抽象问题，并提高了 AI 在不完全信息博弈中的表现。

Abstract: Excessive abstraction is a critical challenge in hand abstraction-a task specific to games like Texas hold'em-when solving large-scale imperfect-information games, as it impairs AI performance. This issue arises from extreme implementations of imperfect-recall abstraction, which entirely discard historical information. This paper presents KrwEmd, the first practical algorithm designed to address this problem. We first introduce the k-recall winrate feature, which not only qualitatively distinguishes signal observation infosets by leveraging both future and, crucially, historical game information, but also quantitatively captures their similarity. We then develop the KrwEmd algorithm, which clusters signal observation infosets using earth mover's distance to measure discrepancies between their features. Experimental results demonstrate that KrwEmd significantly improves AI gameplay performance compared to existing algorithms.

</details>


### [117] [MetaGDPO: Alleviating Catastrophic Forgetting with Metacognitive Knowledge through Group Direct Preference Optimization](https://arxiv.org/abs/2511.12113)
*Lanxue Zhang,Yuqiang Xie,Fang Fang,Fanglong Dong,Rui Liu,Yanan Cao*

Main category: cs.AI

TL;DR: 本文提出了一种解决大语言模型压缩到小型模型时出现灾难性遗忘问题的综合方案。


<details>
  <summary>Details</summary>
Motivation: 现有数据集和微调方法无法有效保持模型的先前知识，导致灾难性遗忘，尤其是在小于 8B 的模型中。

Method: 本文构建了一个包含元认知知识的数据集，并提出了一种名为 GDPO 的训练方法，该方法通过参考模型约束优化路径，从而更有效地传递知识。

Result: 实验表明，该方法显著减轻了灾难性遗忘，并提高了小型模型的推理性能。

Conclusion: 本文提出的方法能够有效缓解大模型压缩到小模型时出现的灾难性遗忘问题。

Abstract: Large Language Models demonstrate strong reasoning capabilities, which can be effectively compressed into smaller models. However, existing datasets and fine-tuning approaches still face challenges that lead to catastrophic forgetting, particularly for models smaller than 8B. First, most datasets typically ignore the relationship between training data knowledge and the model's inherent abilities, making it difficult to preserve prior knowledge. Second, conventional training objectives often fail to constrain inherent knowledge preservation, which can result in forgetting of previously learned skills. To address these issues, we propose a comprehensive solution that alleviates catastrophic forgetting from both the data and fine-tuning approach perspectives. On the data side, we construct a dataset of 5K instances that covers multiple reasoning tasks and incorporates metacognitive knowledge, making it more tolerant and effective for distillation into smaller models. We annotate the metacognitive knowledge required to solve each question and filter the data based on task knowledge and the model's inherent skills. On the training side, we introduce GDPO (Group Direction Preference Optimization), which is better suited for resource-limited scenarios and can efficiently approximate the performance of GRPO. Guided by the large model and by implicitly constraining the optimization path through a reference model, GDPO enables more effective knowledge transfer from the large model and constrains excessive parameter drift. Extensive experiments demonstrate that our approach significantly alleviates catastrophic forgetting and improves reasoning performance on smaller models.

</details>


### [118] [RTMol: Rethinking Molecule-text Alignment in a Round-trip View](https://arxiv.org/abs/2511.12135)
*Letian Chen,Runhan Shi,Gufeng Yu,Yang Yang*

Main category: cs.AI

TL;DR: 提出了RTMol，一个通过自监督往返学习统一分子描述和文本到SMILES生成的双向对齐框架。


<details>
  <summary>Details</summary>
Motivation: 现有的分子描述和文本到分子设计方法通常被视为独立的任务，依赖于监督微调或对比学习，并且面临三个主要限制：(i) 传统指标优先考虑语言流畅性而非化学准确性，(ii) 训练数据集包含化学上模糊的叙述和不完整的规范，(iii) 生成方向的独立优化导致双向不一致性。

Method: 提出了RTMol，它引入了新的往返评估指标，并实现了分子描述的无监督训练，无需配对的分子-文本语料库。

Result: 实验表明，RTMol在各种LLM中将双向对齐性能提高了高达47%。

Conclusion: RTMol 为联合分子-文本理解和生成建立了一个有效的范例。

Abstract: Aligning molecular sequence representations (e.g., SMILES notations) with textual descriptions is critical for applications spanning drug discovery, materials design, and automated chemical literature analysis. Existing methodologies typically treat molecular captioning (molecule-to-text) and text-based molecular design (text-to-molecule) as separate tasks, relying on supervised fine-tuning or contrastive learning pipelines. These approaches face three key limitations: (i) conventional metrics like BLEU prioritize linguistic fluency over chemical accuracy, (ii) training datasets frequently contain chemically ambiguous narratives with incomplete specifications, and (iii) independent optimization of generation directions leads to bidirectional inconsistency. To address these issues, we propose RTMol, a bidirectional alignment framework that unifies molecular captioning and text-to-SMILES generation through self-supervised round-trip learning. The framework introduces novel round-trip evaluation metrics and enables unsupervised training for molecular captioning without requiring paired molecule-text corpora. Experiments demonstrate that RTMol enhances bidirectional alignment performance by up to 47% across various LLMs, establishing an effective paradigm for joint molecule-text understanding and generation.

</details>


### [119] [Incremental Maintenance of DatalogMTL Materialisations](https://arxiv.org/abs/2511.12169)
*Kaiyue Zhao,Dingqi Chen,Shaoyu Wang,Pan Hu*

Main category: cs.AI

TL;DR: DatalogMTL用度量时序逻辑扩展了经典的Datalog语言，实现了对时态数据的表达性推理。现有的推理方法缺乏对处理高效动态更新的支持，而这对于涉及频繁数据更新的实际应用至关重要。本文提出了一种用于DatalogMTL的增量推理算法DRedMTL，该算法建立在经典的DRed算法之上，通过展开来有效地处理DatalogMTL物化的周期性表示。实验结果表明，DRedMTL通常明显优于重新物化。


<details>
  <summary>Details</summary>
Motivation: 现有的DatalogMTL推理方法缺乏对处理高效动态更新的支持，这对于涉及频繁数据更新的实际应用至关重要。

Method: 提出了一种用于DatalogMTL的增量推理算法DRedMTL，该算法建立在经典的DRed算法之上，并配备了专门设计的算子来有效地处理DatalogMTL物化的周期性表示。

Result: 实验结果表明，DRedMTL通常明显优于重新物化，有时甚至高出几个数量级。

Conclusion: DRedMTL是一种用于DatalogMTL的增量推理算法，它通过有效地处理DatalogMTL物化的周期性表示，在性能上优于重新物化。

Abstract: DatalogMTL extends the classical Datalog language with metric temporal logic (MTL), enabling expressive reasoning over temporal data. While existing reasoning approaches, such as materialisation based and automata based methods, offer soundness and completeness, they lack support for handling efficient dynamic updates, a crucial requirement for real-world applications that involve frequent data updates. In this work, we propose DRedMTL, an incremental reasoning algorithm for DatalogMTL with bounded intervals. Our algorithm builds upon the classical DRed algorithm, which incrementally updates the materialisation of a Datalog program. Unlike a Datalog materialisation which is in essence a finite set of facts, a DatalogMTL materialisation has to be represented as a finite set of facts plus periodic intervals indicating how the full materialisation can be constructed through unfolding. To cope with this, our algorithm is equipped with specifically designed operators to efficiently handle such periodic representations of DatalogMTL materialisations. We have implemented this approach and tested it on several publicly available datasets. Experimental results show that DRedMTL often significantly outperforms rematerialisation, sometimes by orders of magnitude.

</details>


### [120] [Debate over Mixed-knowledge: A Robust Multi-Agent Framework for Incomplete Knowledge Graph Question Answering](https://arxiv.org/abs/2511.12208)
*Jilong Liu,Pengyang Shao,Wei Qin,Fei Liu,Yonghui Yang,Richang Hong*

Main category: cs.AI

TL;DR: 提出了一种名为 Debate over Mixed-knowledge (DoM) 的新框架，用于动态整合结构化和非结构化知识，以解决不完整知识图谱问答 (IKGQA) 问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界的知识图谱（KG）通常是不完整的，导致不完整知识图谱问答（IKGQA）问题。现有的方法缺乏自适应和情境化地融合多个来源的能力，无法充分利用它们的互补优势。

Method: 该框架基于多智能体辩论范式，分配专门的智能体分别对知识图谱和外部文本进行推理，并通过迭代交互协调它们的输出。它将输入问题分解为子问题，通过双重智能体（KG 和检索增强生成，RAG）检索证据，并采用裁判智能体来评估和聚合中间答案。

Result: DoM 始终优于最先进的基线。

Conclusion: 知识互补性和增强了对 KG 不完整性的鲁棒性。引入了一个新的数据集，Incomplete Knowledge Graph WebQuestions，它通过利用真实世界的知识更新来构建，从而反映了超出 KG 静态范围的知识，从而产生更真实和更具挑战性的基准。

Abstract: Knowledge Graph Question Answering (KGQA) aims to improve factual accuracy by leveraging structured knowledge. However, real-world Knowledge Graphs (KGs) are often incomplete, leading to the problem of Incomplete KGQA (IKGQA). A common solution is to incorporate external data to fill knowledge gaps, but existing methods lack the capacity to adaptively and contextually fuse multiple sources, failing to fully exploit their complementary strengths. To this end, we propose Debate over Mixed-knowledge (DoM), a novel framework that enables dynamic integration of structured and unstructured knowledge for IKGQA. Built upon the Multi-Agent Debate paradigm, DoM assigns specialized agents to perform inference over knowledge graphs and external texts separately, and coordinates their outputs through iterative interaction. It decomposes the input question into sub-questions, retrieves evidence via dual agents (KG and Retrieval-Augmented Generation, RAG), and employs a judge agent to evaluate and aggregate intermediate answers. This collaboration exploits knowledge complementarity and enhances robustness to KG incompleteness. In addition, existing IKGQA datasets simulate incompleteness by randomly removing triples, failing to capture the irregular and unpredictable nature of real-world knowledge incompleteness. To address this, we introduce a new dataset, Incomplete Knowledge Graph WebQuestions, constructed by leveraging real-world knowledge updates. These updates reflect knowledge beyond the static scope of KGs, yielding a more realistic and challenging benchmark. Through extensive experiments, we show that DoM consistently outperforms state-of-the-art baselines.

</details>


### [121] [ViTE: Virtual Graph Trajectory Expert Router for Pedestrian Trajectory Prediction](https://arxiv.org/abs/2511.12214)
*Ruochen Li,Zhanxing Zhu,Tanqiu Qiao,Hubert P. H. Shum*

Main category: cs.AI

TL;DR: 提出了ViTE，一个用于行人轨迹预测的新框架，通过虚拟图和专家路由器的结合，实现了在不同交互模式下的灵活和可扩展的推理。


<details>
  <summary>Details</summary>
Motivation: 现有方法在高阶交互建模中面临不足，深层GNN堆叠导致计算成本高昂。

Method: ViTE包含虚拟图和专家路由器两个关键模块，前者引入动态虚拟节点以建模长程和高阶交互，后者基于社交环境自适应地选择交互专家。

Result: 在ETH/UCY、NBA和SDD三个基准数据集上，ViTE始终取得最优性能。

Conclusion: ViTE的有效性和实际效率得到了验证。

Abstract: Pedestrian trajectory prediction is critical for ensuring safety in autonomous driving, surveillance systems, and urban planning applications. While early approaches primarily focus on one-hop pairwise relationships, recent studies attempt to capture high-order interactions by stacking multiple Graph Neural Network (GNN) layers. However, these approaches face a fundamental trade-off: insufficient layers may lead to under-reaching problems that limit the model's receptive field, while excessive depth can result in prohibitive computational costs. We argue that an effective model should be capable of adaptively modeling both explicit one-hop interactions and implicit high-order dependencies, rather than relying solely on architectural depth. To this end, we propose ViTE (Virtual graph Trajectory Expert router), a novel framework for pedestrian trajectory prediction. ViTE consists of two key modules: a Virtual Graph that introduces dynamic virtual nodes to model long-range and high-order interactions without deep GNN stacks, and an Expert Router that adaptively selects interaction experts based on social context using a Mixture-of-Experts design. This combination enables flexible and scalable reasoning across varying interaction patterns. Experiments on three benchmarks (ETH/UCY, NBA, and SDD) demonstrate that our method consistently achieves state-of-the-art performance, validating both its effectiveness and practical efficiency.

</details>


### [122] [Beyond World Models: Rethinking Understanding in AI Models](https://arxiv.org/abs/2511.12239)
*Tarun Gupta,Danish Pruthi*

Main category: cs.AI

TL;DR: 本研究探讨了AI中的世界模型，它模拟外部世界、追踪实体和状态、捕捉因果关系并预测结果。这与仅基于统计相关性的表征形成对比。


<details>
  <summary>Details</summary>
Motivation: 研究动机是人类拥有心理世界模型，在AI模型中寻找类似表征可能表明这些模型以类似于人类的方式“理解”世界。

Method: 本文采用科学哲学文献中的案例研究，批判性地检验了世界模型框架是否充分表征了人类水平的理解。

Result: 论文侧重于世界模型能力与人类理解之间区别最明显的特定哲学分析。

Conclusion: 研究结果虽代表特定观点而非普遍定义，但有助于探索世界模型的局限性。

Abstract: World models have garnered substantial interest in the AI community. These are internal representations that simulate aspects of the external world, track entities and states, capture causal relationships, and enable prediction of consequences. This contrasts with representations based solely on statistical correlations. A key motivation behind this research direction is that humans possess such mental world models, and finding evidence of similar representations in AI models might indicate that these models "understand" the world in a human-like way. In this paper, we use case studies from the philosophy of science literature to critically examine whether the world model framework adequately characterizes human-level understanding. We focus on specific philosophical analyses where the distinction between world model capabilities and human understanding is most pronounced. While these represent particular views of understanding rather than universal definitions, they help us explore the limits of world models.

</details>


### [123] [AURA: Development and Validation of an Augmented Unplanned Removal Alert System using Synthetic ICU Videos](https://arxiv.org/abs/2511.12241)
*Junhyuk Seo,Hyeyoon Moon,Kyu-Hwan Jung,Namkee Oh,Taerim Kim*

Main category: cs.AI

TL;DR: 提出了一种名为AURA的基于视觉的风险检测系统，该系统使用完全合成的视频数据集开发和验证，用于检测ICU中的非计划性拔管（UE）。


<details>
  <summary>Details</summary>
Motivation: ICU中的非计划性拔管（UE）仍然是一个关键的患者安全问题，但由于获取带注释的ICU视频数据存在伦理和隐私挑战，实时UE检测受到限制。

Method: 利用文本到视频扩散生成多样且临床真实的ICU场景，捕捉一系列患者行为和护理环境。该系统应用姿势估计来识别两种高风险运动模式：碰撞（定义为手进入气道管附近的区域）和躁动（通过跟踪的解剖关键点的速度来量化）。

Result: 专家评估证实了合成数据的真实性，性能评估表明碰撞检测的准确性很高，而躁动识别的性能适中。

Conclusion: 这项工作展示了一种新的途径，用于开发具有隐私保护、可重复的患者安全监测系统，并有可能在重症监护环境中部署。

Abstract: Unplanned extubation (UE) remains a critical patient safety concern in intensive care units (ICUs), often leading to severe complications or death. Real-time UE detection has been limited, largely due to the ethical and privacy challenges of obtaining annotated ICU video data. We propose Augmented Unplanned Removal Alert (AURA), a vision-based risk detection system developed and validated entirely on a fully synthetic video dataset. By leveraging text-to-video diffusion, we generated diverse and clinically realistic ICU scenarios capturing a range of patient behaviors and care contexts. The system applies pose estimation to identify two high-risk movement patterns: collision, defined as hand entry into spatial zones near airway tubes, and agitation, quantified by the velocity of tracked anatomical keypoints. Expert assessments confirmed the realism of the synthetic data, and performance evaluations showed high accuracy for collision detection and moderate performance for agitation recognition. This work demonstrates a novel pathway for developing privacy-preserving, reproducible patient safety monitoring systems with potential for deployment in intensive care settings.

</details>


### [124] [MoralReason: Generalizable Moral Decision Alignment For LLM Agents Using Reasoning-Level Reinforcement Learning](https://arxiv.org/abs/2511.12271)
*Zhiyu An,Wan Du*

Main category: cs.AI

TL;DR: 大型语言模型越来越多地影响人类的道德决策，但目前的方法主要集中在评估而不是积极引导他们的道德决策。


<details>
  <summary>Details</summary>
Motivation: 将道德推理框架应用于超出其训练范围的情况，从而解决道德偏差问题。

Method: 使用 Moral-Reason-QA 数据集，该数据集通过功利主义、义务论和美德伦理的框架特定推理跟踪扩展了 680 个人工注释的高歧义道德场景，从而能够在实际决策环境中系统地评估道德概括。

Result: 在未见过的道德场景中成功推广，在分布外评估集上测试时，功利主义的 softmax 归一化对齐分数提高了 +0.757，义务论的 softmax 归一化对齐分数提高了 +0.450。

Conclusion: 大型语言模型可以经过系统训练，将特定的道德框架内化并应用于新的情况，为人工智能安全奠定关键基础。

Abstract: Large language models are increasingly influencing human moral decisions, yet current approaches focus primarily on evaluating rather than actively steering their moral decisions. We formulate this as an out-of-distribution moral alignment problem, where LLM agents must learn to apply consistent moral reasoning frameworks to scenarios beyond their training distribution. We introduce Moral-Reason-QA, a novel dataset extending 680 human-annotated, high-ambiguity moral scenarios with framework-specific reasoning traces across utilitarian, deontological, and virtue ethics, enabling systematic evaluation of moral generalization in realistic decision contexts. Our learning approach employs Group Relative Policy Optimization with composite rewards that simultaneously optimize decision alignment and framework-specific reasoning processes to facilitate learning of the underlying moral frameworks. Experimental results demonstrate successful generalization to unseen moral scenarios, with softmax-normalized alignment scores improving by +0.757 for utilitarian and +0.450 for deontological frameworks when tested on out-of-distribution evaluation sets. The experiments also reveal training challenges and promising directions that inform future research. These findings establish that LLM agents can be systematically trained to internalize and apply specific moral frameworks to novel situations, providing a critical foundation for AI safety as language models become more integrated into human decision-making processes.

</details>


### [125] [UpBench: A Dynamically Evolving Real-World Labor-Market Agentic Benchmark Framework Built for Human-Centric AI](https://arxiv.org/abs/2511.12306)
*Darvin Yi,Teng Liu,Mattie Terzolo,Lance Hasson,Ayan Sinh,Pablo Mendes,Andrew Rabinovich*

Main category: cs.AI

TL;DR: UpBench是一个动态更新的基准，它基于Upwork的真实工作，用于评估LLM agent在真实工作环境中的能力，包括胜任力、适应性和人机协作能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试大多是静态的、合成的或领域受限的，无法深入了解agent在动态的、具有经济意义的环境中的表现。

Method: UpBench使用基于评分标准的评估框架，专家自由职业者将每个工作分解为详细的、可验证的验收标准，并根据每个标准对AI提交的内容进行评估。

Result: UpBench提供了一个可扩展的、以人为本的基础，用于在真实的劳动力市场环境中评估agent系统。

Conclusion: UpBench为评估LLM agent在真实工作场景中的能力提供了一个动态、以人为本的框架，旨在促进人机协作，使AI能够增强人类的能力。

Abstract: As large language model (LLM) agents increasingly undertake digital work, reliable frameworks are needed to evaluate their real-world competence, adaptability, and capacity for human collaboration. Existing benchmarks remain largely static, synthetic, or domain-limited, providing limited insight into how agents perform in dynamic, economically meaningful environments. We introduce UpBench, a dynamically evolving benchmark grounded in real jobs drawn from the global Upwork labor marketplace. Each task corresponds to a verified client transaction, anchoring evaluation in genuine work activity and financial outcomes. UpBench employs a rubric-based evaluation framework, in which expert freelancers decompose each job into detailed, verifiable acceptance criteria and assess AI submissions with per-criterion feedback. This structure enables fine-grained analysis of model strengths, weaknesses, and instruction-following fidelity beyond binary pass/fail metrics. Human expertise is integrated throughout the data pipeline (from job curation and rubric construction to evaluation) ensuring fidelity to real professional standards and supporting research on human-AI collaboration. By regularly refreshing tasks to reflect the evolving nature of online work, UpBench provides a scalable, human-centered foundation for evaluating agentic systems in authentic labor-market contexts, offering a path toward a collaborative framework, where AI amplifies human capability through partnership rather than replacement.

</details>


### [126] [Reward and Guidance through Rubrics: Promoting Exploration to Improve Multi-Domain Reasoning](https://arxiv.org/abs/2511.12344)
*Baolong Bi,Shenghua Liu,Yiwei Wang,Siqian Tong,Lingrui Mei,Yuyao Ge,Yilong Xu,Jiafeng Guo,Xueqi Cheng*

Main category: cs.AI

TL;DR: 提出了一种基于规则的多域推理强化学习框架RGR-GRPO，以解决现有方法在复杂推理能力上的限制。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要集中于具有可验证奖励的单领域强化学习，并且依赖于纯在线强化学习框架限制了探索空间，从而限制了推理性能。

Method: 利用规则来提供细粒度的奖励信号和离线指导，提出了一个基于规则的强化学习框架RGR-GRPO。

Result: 在跨越多个领域的14个基准测试中，RGR-GRPO始终优于仅依赖于替代奖励方案或离线指导的强化学习方法。与可验证的在线强化学习基线相比，RGR-GRPO在数学、物理、化学和一般推理任务上分别实现了平均+7.0%、+5.4%、+8.4%和+6.6%的改进。

Conclusion: RGR-GRPO在离策略训练期间保持稳定的熵波动，并实现了卓越的pass@k性能，反映了持续的探索和有效突破现有性能瓶颈。

Abstract: Recent advances in reinforcement learning (RL) have significantly improved the complex reasoning capabilities of large language models (LLMs). Despite these successes, existing methods mainly focus on single-domain RL (e.g., mathematics) with verifiable rewards (RLVR), and their reliance on purely online RL frameworks restricts the exploration space, thereby limiting reasoning performance. In this paper, we address these limitations by leveraging rubrics to provide both fine-grained reward signals and offline guidance. We propose $\textbf{RGR-GRPO}$ (Reward and Guidance through Rubrics), a rubric-driven RL framework for multi-domain reasoning. RGR-GRPO enables LLMs to receive dense and informative rewards while exploring a larger solution space during GRPO training. Extensive experiments across 14 benchmarks spanning multiple domains demonstrate that RGR-GRPO consistently outperforms RL methods that rely solely on alternative reward schemes or offline guidance. Compared with verifiable online RL baseline, RGR-GRPO achieves average improvements of +7.0%, +5.4%, +8.4%, and +6.6% on mathematics, physics, chemistry, and general reasoning tasks, respectively. Notably, RGR-GRPO maintains stable entropy fluctuations during off-policy training and achieves superior pass@k performance, reflecting sustained exploration and effective breakthrough beyond existing performance bottlenecks.

</details>


### [127] [More Than Irrational: Modeling Belief-Biased Agents](https://arxiv.org/abs/2511.12359)
*Yifan Zhu,Sammie Katt,Samuel Kaski*

Main category: cs.AI

TL;DR: 这篇论文介绍了一种计算理性（CR）用户模型，用于模拟在认知限制和偏见信念下行动的智能体，解决了预测和推断用户次优行为的难题。


<details>
  <summary>Details</summary>
Motivation: 动机是预测和推断用户或人类合作者的次优行为仍然是一个关键挑战，因为这些行为通常是认知限制和偏见信念下的理性决策。

Method: 该方法通过显式建模有界记忆过程如何导致动态不一致和有偏见的信念状态，并提出了一种基于嵌套粒子滤波的在线推理方法，以同时跟踪用户的潜在信念状态并估计未知的认知界限。

Result: 该方法在导航任务中得到验证，结果表明，该CR模型能够生成与不同记忆容量相对应的直观合理的行为，并且该推理方法能够准确有效地从有限的观察中恢复真实的认知界限。

Conclusion: 该方法为开发自适应AI助手提供了一个原则性基础，能够实现考虑用户记忆限制的自适应辅助。

Abstract: Despite the explosive growth of AI and the technologies built upon it, predicting and inferring the sub-optimal behavior of users or human collaborators remains a critical challenge. In many cases, such behaviors are not a result of irrationality, but rather a rational decision made given inherent cognitive bounds and biased beliefs about the world. In this paper, we formally introduce a class of computational-rational (CR) user models for cognitively-bounded agents acting optimally under biased beliefs. The key novelty lies in explicitly modeling how a bounded memory process leads to a dynamically inconsistent and biased belief state and, consequently, sub-optimal sequential decision-making. We address the challenge of identifying the latent user-specific bound and inferring biased belief states from passive observations on the fly. We argue that for our formalized CR model family with an explicit and parameterized cognitive process, this challenge is tractable. To support our claim, we propose an efficient online inference method based on nested particle filtering that simultaneously tracks the user's latent belief state and estimates the unknown cognitive bound from a stream of observed actions. We validate our approach in a representative navigation task using memory decay as an example of a cognitive bound. With simulations, we show that (1) our CR model generates intuitively plausible behaviors corresponding to different levels of memory capacity, and (2) our inference method accurately and efficiently recovers the ground-truth cognitive bounds from limited observations ($\le 100$ steps). We further demonstrate how this approach provides a principled foundation for developing adaptive AI assistants, enabling adaptive assistance that accounts for the user's memory limitations.

</details>


### [128] [Learning to Trust: Bayesian Adaptation to Varying Suggester Reliability in Sequential Decision Making](https://arxiv.org/abs/2511.12378)
*Dylan M. Asmar,Mykel J. Kochenderfer*

Main category: cs.AI

TL;DR: 该论文提出了一种在不确定环境中运行的自主Agent框架，该框架可以动态学习和适应外部建议者的可靠性，通过将建议者质量纳入Agent的信念表示，并引入“询问”动作来实现。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设静态和已知的建议者质量参数，限制了实际部署。在不确定性下，序列决策任务中运行的自主Agent可以从外部行动建议中受益，这些建议提供有价值的指导，但可靠性各不相同。

Method: 该框架通过将建议者质量直接整合到Agent的信念表示中，使Agent能够通过贝叶斯推理来推断和调整他们对建议的依赖。此外，引入了一个明确的“询问”动作，允许Agent在关键时刻战略性地请求建议，从而平衡信息增益与获取成本。

Result: 实验评估表明，该框架在不同的建议者质量下表现出稳健的性能，能够适应不断变化的可靠性，并能战略性地管理建议请求。

Conclusion: 该论文通过解决不确定环境中建议的不确定性，为自适应人机协作奠定了基础。

Abstract: Autonomous agents operating in sequential decision-making tasks under uncertainty can benefit from external action suggestions, which provide valuable guidance but inherently vary in reliability. Existing methods for incorporating such advice typically assume static and known suggester quality parameters, limiting practical deployment. We introduce a framework that dynamically learns and adapts to varying suggester reliability in partially observable environments. First, we integrate suggester quality directly into the agent's belief representation, enabling agents to infer and adjust their reliance on suggestions through Bayesian inference over suggester types. Second, we introduce an explicit ``ask'' action allowing agents to strategically request suggestions at critical moments, balancing informational gains against acquisition costs. Experimental evaluation demonstrates robust performance across varying suggester qualities, adaptation to changing reliability, and strategic management of suggestion requests. This work provides a foundation for adaptive human-agent collaboration by addressing suggestion uncertainty in uncertain environments.

</details>


### [129] [Multi-agent Self-triage System with Medical Flowcharts](https://arxiv.org/abs/2511.12439)
*Yujia Liu,Sophia Yu,Hongyue Jin,Jessica Wen,Alexander Qian,Terrence Lee,Mattheus Ramsis,Gi Won Choi,Lianhui Qin,Xin Liu,Edward J. Wang*

Main category: cs.AI

TL;DR: 本研究提出了一种基于大型语言模型(LLM)的会话式自我分诊系统，该系统利用美国医学会的临床验证流程图，为患者提供结构化和可审计的决策支持。


<details>
  <summary>Details</summary>
Motivation: 在线健康资源和大型语言模型越来越多地被用作医疗决策的首要接触点，但它们在医疗保健方面的可靠性仍然受到准确性低、缺乏透明度以及容易受到未经证实的信息的影响。

Method: 该系统利用一个多代理框架，包括检索代理、决策代理和聊天代理，分别用于识别最相关的流程图、解释患者的反应以及提供个性化的、患者友好的建议。

Result: 该系统在流程图检索中达到95.29%的前3名准确率(N=2,000)，在不同会话风格和条件下，流程图导航的准确率为99.10%(N=37,200)。

Conclusion: 通过将自由文本交互的灵活性与标准化临床协议的严谨性相结合，该方法证明了透明、准确和可推广的AI辅助自我分诊的可行性，并有可能支持知情的患者决策，同时提高医疗保健资源的利用率。

Abstract: Online health resources and large language models (LLMs) are increasingly used as a first point of contact for medical decision-making, yet their reliability in healthcare remains limited by low accuracy, lack of transparency, and susceptibility to unverified information. We introduce a proof-of-concept conversational self-triage system that guides LLMs with 100 clinically validated flowcharts from the American Medical Association, providing a structured and auditable framework for patient decision support. The system leverages a multi-agent framework consisting of a retrieval agent, a decision agent, and a chat agent to identify the most relevant flowchart, interpret patient responses, and deliver personalized, patient-friendly recommendations, respectively. Performance was evaluated at scale using synthetic datasets of simulated conversations. The system achieved 95.29% top-3 accuracy in flowchart retrieval (N=2,000) and 99.10% accuracy in flowchart navigation across varied conversational styles and conditions (N=37,200). By combining the flexibility of free-text interaction with the rigor of standardized clinical protocols, this approach demonstrates the feasibility of transparent, accurate, and generalizable AI-assisted self-triage, with potential to support informed patient decision-making while improving healthcare resource utilization.

</details>


### [130] [ARCHE: A Novel Task to Evaluate LLMs on Latent Reasoning Chain Extraction](https://arxiv.org/abs/2511.12485)
*Pengze Li,Jiaqi Liu,Junchi Yu,Lihao Liu,Mingyu Ding,Wanli Ouyang,Shixiang Tang,Xi Chen*

Main category: cs.AI

TL;DR: 大型语言模型在科学领域应用广泛，但其推理过程非结构化且非正式。本研究提出了潜变量推理链提取（ARCHE）任务，旨在将复杂推理分解为标准的推理范式，并发布了包含1900多篇参考文献和38000个观点的ARCHE Bench基准。对10个领先的LLM的评估表明，模型在推理边缘准确性和实体覆盖率之间存在权衡，且无法提取完整和标准的推理链。结果表明，当前推理模型的能力与科学论证的严谨性之间存在差距。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型是否真正理解科学推理的基本范式。

Method: 提出潜变量推理链提取（ARCHE）任务，将复杂推理分解为演绎、归纳或溯因三种推理模式的组合，形成推理逻辑树（RLT）。

Result: 对10个领先的LLM在ARCHE Bench上的评估表明，模型在推理边缘准确性和实体覆盖率之间存在权衡，且无法提取完整和标准的推理链。

Conclusion: 当前推理模型的能力与科学论证的严谨性之间存在显著差距。

Abstract: Large language models (LLMs) are increasingly used in scientific domains. While they can produce reasoning-like content via methods such as chain-of-thought prompting, these outputs are typically unstructured and informal, obscuring whether models truly understand the fundamental reasoning paradigms that underpin scientific inference. To address this, we introduce a novel task named Latent Reasoning Chain Extraction (ARCHE), in which models must decompose complex reasoning arguments into combinations of standard reasoning paradigms in the form of a Reasoning Logic Tree (RLT). In RLT, all reasoning steps are explicitly categorized as one of three variants of Peirce's fundamental inference modes: deduction, induction, or abduction. To facilitate this task, we release ARCHE Bench, a new benchmark derived from 70 Nature Communications articles, including more than 1,900 references and 38,000 viewpoints. We propose two logic-aware evaluation metrics: Entity Coverage (EC) for content completeness and Reasoning Edge Accuracy (REA) for step-by-step logical validity. Evaluations on 10 leading LLMs on ARCHE Bench reveal that models exhibit a trade-off between REA and EC, and none are yet able to extract a complete and standard reasoning chain. These findings highlight a substantial gap between the abilities of current reasoning models and the rigor required for scientific argumentation.

</details>


### [131] [LOBERT: Generative AI Foundation Model for Limit Order Book Messages](https://arxiv.org/abs/2511.12563)
*Eljas Linna,Kestutis Baltakys,Alexandros Iosifidis,Juho Kanniainen*

Main category: cs.AI

TL;DR: LOBERT is a BERT-based foundation model for Limit Order Book (LOB) data.


<details>
  <summary>Details</summary>
Motivation: Modeling LOB dynamics is challenging due to irregular timing, rapid shifts, and high-frequency trader reactions. Previous models are cumbersome and lack adaptability.

Method: Adapts BERT with a novel tokenization scheme treating multi-dimensional messages as single tokens, retaining continuous representations of price, volume, and time.

Result: LOBERT achieves leading performance in mid-price movement and next message prediction, reducing context length.

Conclusion: LOBERT is a general-purpose encoder suitable for downstream fine-tuning.

Abstract: Modeling the dynamics of financial Limit Order Books (LOB) at the message level is challenging due to irregular event timing, rapid regime shifts, and the reactions of high-frequency traders to visible order flow. Previous LOB models require cumbersome data representations and lack adaptability outside their original tasks, leading us to introduce LOBERT, a general-purpose encoder-only foundation model for LOB data suitable for downstream fine-tuning. LOBERT adapts the original BERT architecture for LOB data by using a novel tokenization scheme that treats complete multi-dimensional messages as single tokens while retaining continuous representations of price, volume, and time. With these methods, LOBERT achieves leading performance in tasks such as predicting mid-price movements and next messages, while reducing the required context length compared to previous methods.

</details>


### [132] [Enhancing Conversational Recommender Systems with Tree-Structured Knowledge and Pretrained Language Models](https://arxiv.org/abs/2511.12579)
*Yongwen Ren,Chao Wang,Peng Du,Chuan Qin,Dazhong Shen,Hui Xiong*

Main category: cs.AI

TL;DR: 提出了一种名为PCRS-TKA的prompt框架，它使用检索增强生成将PLM与KG集成，在推荐和对话质量方面都优于所有基线。


<details>
  <summary>Details</summary>
Motivation: 为了提高准确性并减轻幻觉，许多方法将PLM与知识图（KG）集成，但面临关键挑战：未能充分利用PLM对图形关系的推理，不加选择地结合检索到的知识而没有上下文过滤，并忽略了多轮对话中的协作偏好。

Method: PCRS-TKA构建特定于对话的知识树，并将其序列化为文本，从而实现结构感知推理，同时捕获丰富的实体语义。我们的方法选择性地过滤上下文相关的知识，并使用专门的监督信号显式地建模协作偏好。语义对齐模块协调异构输入，减少噪声并提高准确性。

Result: PCRS-TKA在推荐和对话质量方面始终优于所有基线。

Conclusion: PCRS-TKA在推荐和对话质量方面始终优于所有基线，证明了其有效性。

Abstract: Recent advances in pretrained language models (PLMs) have significantly improved conversational recommender systems (CRS), enabling more fluent and context-aware interactions. To further enhance accuracy and mitigate hallucination, many methods integrate PLMs with knowledge graphs (KGs), but face key challenges: failing to fully exploit PLM reasoning over graph relationships, indiscriminately incorporating retrieved knowledge without context filtering, and neglecting collaborative preferences in multi-turn dialogues. To this end, we propose PCRS-TKA, a prompt-based framework employing retrieval-augmented generation to integrate PLMs with KGs. PCRS-TKA constructs dialogue-specific knowledge trees from KGs and serializes them into texts, enabling structure-aware reasoning while capturing rich entity semantics. Our approach selectively filters context-relevant knowledge and explicitly models collaborative preferences using specialized supervision signals. A semantic alignment module harmonizes heterogeneous inputs, reducing noise and enhancing accuracy. Extensive experiments demonstrate that PCRS-TKA consistently outperforms all baselines in both recommendation and conversational quality.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [133] [GenIE - Simulator-Driven Iterative Data Exploration for Scientific Discovery](https://arxiv.org/abs/2511.12057)
*Ashwin Gerard Colaco,Martin Boissier,Sriram Rao,Shubharoop Ghosh,Sharad Mehrotra,Tilmann Rabl*

Main category: cs.DB

TL;DR: GenIE: A database paradigm integrating simulators for dynamic orchestration of simulation workflows, enabling interactive exploration and analysis.


<details>
  <summary>Details</summary>
Motivation: Current workflows treat simulators as external pre-processing steps, leading to inefficiency and high latency, hindering interactive exploration.

Method: Extending PostgreSQL to create GenIE, a simulation-aware database that dynamically invokes simulators based on queries and analytical needs.

Result: Preliminary experiments demonstrate GenIE's potential to transform slow, static analyses into interactive explorations by managing the trade-off between simulation accuracy and runtime.

Conclusion: GenIE presents challenges and opportunities as a cornerstone for next-generation scientific data analysis.

Abstract: Physics-based simulators play a critical role in scientific discovery and risk assessment, enabling what-if analyses for events like wildfires and hurricanes. Today, databases treat these simulators as external pre-processing steps. Analysts must manually run a simulation, export the results, and load them into a database before analysis can begin. This linear workflow is inefficient, incurs high latency, and hinders interactive exploration, especially when the analysis itself dictates the need for new or refined simulation data.
  We envision a new database paradigm, entitled GenIE, that seamlessly integrates multiple simulators into databases to enable dynamic orchestration of simulation workflows. By making the database "simulation-aware," GenIE can dynamically invoke simulators with appropriate parameters based on the user's query and analytical needs. This tight integration allows GenIE to avoid generating data irrelevant to the analysis, reuse previously generated data, and support iterative, incremental analysis where results are progressively refined at interactive speeds.
  We present our vision for GenIE, designed as an extension to PostgreSQL, and demonstrate its potential benefits through comprehensive use cases: wildfire smoke dispersion analysis using WRF-SFIRE and HYSPLIT, and hurricane hazard assessment integrating wind, surge, and flood models. Our preliminary experiments show how GenIE can transform these slow, static analyses into interactive explorations by intelligently managing the trade-off between simulation accuracy and runtime across multiple integrated simulators. We conclude by highlighting the challenges and opportunities ahead in realizing the full vision of GenIE as a cornerstone for next-generation scientific data analysis.

</details>


### [134] [SEE++: Evolving Snowpark Execution Environment for Modern Workloads](https://arxiv.org/abs/2511.12457)
*Gaurav Jain,Brandon Baker,Joe Yin,Chenwei Xie,Zihao Ye,Sidh Kulkarni,Sara Abdelrahman,Nova Qi,Urjeet Shrestha,Mike Halcrow,Dave Bailey,Yuxiong He*

Main category: cs.DB

TL;DR: Snowpark升级了其内部沙箱解决方案到gVisor，并进行了优化。


<details>
  <summary>Details</summary>
Motivation: 随着采用率的增长，工作负载的多样性对沙箱提出了越来越复杂的需求。

Method: Snowpark过渡到gVisor，并进行了有针对性的优化。

Result: 案例研究展示了升级后的架构所启用的新功能，证明了SEE在支持下一代Snowpark工作负载方面的可扩展性和灵活性。

Conclusion: 本文描述了指导升级的功能和性能目标，概述了新的沙箱架构，并详细介绍了在此过程中遇到的挑战以及为解决这些挑战而开发的解决方案。

Abstract: Snowpark enables Data Engineering and AI/ML workloads to run directly within Snowflake by deploying a secure sandbox on virtual warehouse nodes. This Snowpark Execution Environment (SEE) allows users to execute arbitrary workloads in Python and other languages in a secure and performant manner. As adoption has grown, the diversity of workloads has introduced increasingly sophisticated needs for sandboxing. To address these evolving requirements, Snowpark transitioned its in-house sandboxing solution to gVisor, augmented with targeted optimizations. This paper describes both the functional and performance objectives that guided the upgrade, outlines the new sandbox architecture, and details the challenges encountered during the journey, along with the solutions developed to resolve them. Finally, we present case studies that highlight new features enabled by the upgraded architecture, demonstrating SEE's extensibility and flexibility in supporting the next generation of Snowpark workloads.

</details>


### [135] [Redbench: Workload Synthesis From Cloud Traces](https://arxiv.org/abs/2511.13059)
*Johannes Wehrstein,Roman Heinrich,Mihail Stoian,Skander Krid,Martin Stemmer,Andreas Kipf,Carsten Binnig,Muhammad El-Hindi*

Main category: cs.DB

TL;DR: Redbench是一个新的基准测试，它使用工作负载生成器来重现真实世界的工作负载特征，弥合了合成工作负载和真实工作负载之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试（如TPC-H和TPC-DS）无法捕捉到真实世界工作负载的关键特征，包括查询重复和字符串繁重的查询。

Method: Redbench集成了多种工作负载生成技术，以定制工作负载以适应特定目标，将现有基准测试转换为保留内在工作负载特征的真实查询流。

Result: Redbench为云数据仓库基准测试生成了更真实和可重现的工作负载，并揭示了系统优化在四个商业数据仓库平台上的影响。

Conclusion: Redbench为推进现代云数据仓库优化技术的研究奠定了重要的基础。

Abstract: Workload traces from cloud data warehouse providers reveal that standard benchmarks such as TPC-H and TPC-DS fail to capture key characteristics of real-world workloads, including query repetition and string-heavy queries. In this paper, we introduce Redbench, a novel benchmark featuring a workload generator that reproduces real-world workload characteristics derived from traces released by cloud providers. Redbench integrates multiple workload generation techniques to tailor workloads to specific objectives, transforming existing benchmarks into realistic query streams that preserve intrinsic workload characteristics. By focusing on inherent workload signals rather than execution-specific metrics, Redbench bridges the gap between synthetic and real workloads. Our evaluation shows that (1) Redbench produces more realistic and reproducible workloads for cloud data warehouse benchmarking, and (2) Redbench reveals the impact of system optimizations across four commercial data warehouse platforms. We believe that Redbench provides a crucial foundation for advancing research on optimization techniques for modern cloud data warehouses.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [136] [The Environmental Impact of Ensemble Techniques in Recommender Systems](https://arxiv.org/abs/2511.11649)
*Jannik Nitschke*

Main category: cs.IR

TL;DR: Ensemble methods in recommender systems improve accuracy by 0.3-5.7% but increase energy consumption by 19-2,549%. Selective ensemble strategies are more efficient than exhaustive averaging.


<details>
  <summary>Details</summary>
Motivation: To measure the environmental impact of ensemble techniques in recommender systems compared to single optimized models, as existing research lacks energy consumption evaluations for ensemble methods despite their accuracy improvements.

Method: Conducted 93 experiments across two frameworks (Surprise, LensKit) on four datasets, evaluating four ensemble strategies (Average, Weighted, Stacking/Rank Fusion, Top Performers) against baselines and optimized single models, measuring energy consumption with a smart plug.

Result: Ensemble methods achieved 0.3-5.7% accuracy improvements while consuming 19-2,549% more energy. Top Performers ensemble showed best efficiency. Exhaustive averaging strategies consumed 88-270% more energy for comparable gains. On the largest dataset, the Surprise ensemble consumed 2,005% more energy for 1.2% accuracy improvement.

Conclusion: Selective ensemble strategies offer superior efficiency over exhaustive averaging, and identifies scalability limitations at industrial scale. These findings enable informed decisions about sustainable algorithm selection in recommender systems.

Abstract: Ensemble techniques in recommender systems have demonstrated accuracy improvements of 10-30%, yet their environmental impact remains unmeasured. While deep learning recommendation algorithms can generate up to 3,297 kg CO2 per paper, ensemble methods have not been sufficiently evaluated for energy consumption. This thesis investigates how ensemble techniques influence environmental impact compared to single optimized models.
  We conducted 93 experiments across two frameworks (Surprise for rating prediction, LensKit for ranking) on four datasets spanning 100,000 to 7.8 million interactions. We evaluated four ensemble strategies (Average, Weighted, Stacking/Rank Fusion, Top Performers) against simple baselines and optimized single models, measuring energy consumption with a smart plug.
  Results revealed a non-linear accuracy-energy relationship. Ensemble methods achieved 0.3-5.7% accuracy improvements while consuming 19-2,549% more energy depending on dataset size and strategy. The Top Performers ensemble showed best efficiency: 0.96% RMSE improvement with 18.8% energy overhead on MovieLens-1M, and 5.7% NDCG improvement with 103% overhead on MovieLens-100K. Exhaustive averaging strategies consumed 88-270% more energy for comparable gains. On the largest dataset (Anime, 7.8M interactions), the Surprise ensemble consumed 2,005% more energy (0.21 Wh vs. 0.01 Wh) for 1.2% accuracy improvement, producing 53.8 mg CO2 versus 2.6 mg CO2 for the single model.
  This research provides one of the first systematic measurements of energy and carbon footprint for ensemble recommender systems, demonstrates that selective strategies offer superior efficiency over exhaustive averaging, and identifies scalability limitations at industrial scale. These findings enable informed decisions about sustainable algorithm selection in recommender systems.

</details>


### [137] [GroupRank: A Groupwise Reranking Paradigm Driven by Reinforcement Learning](https://arxiv.org/abs/2511.11653)
*Duolin Sun,Meixiu Long,Dan Yang,Yihan Jiao,Zhehao Tan,Jie Feng,Junjie Wang,Yue Shen,Peng Wei,Jian Wang,Jinjie Gu*

Main category: cs.IR

TL;DR: 提出了一种新的reranking范式Groupwise，以解决Pointwise和Listwise方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的reranking方法存在Pointwise的排序短视和Listwise的可扩展性问题。

Method: 提出Groupwise范式，将查询和一组候选文档输入模型，进行组内比较，并采用GRPO进行模型训练，结合排序指标和分布奖励。

Result: 在两个推理密集型检索基准测试BRIGHT和R2MED上验证了该方法的有效性。

Conclusion: 该方法在reranking任务中表现出色，并能用于训练检索器。

Abstract: Large Language Models have shown strong potential as rerankers to enhance the overall performance of RAG systems. However, existing reranking paradigms are constrained by a core theoretical and practical dilemma: Pointwise methods, while simple and highly flexible, evaluate documents independently, making them prone to the Ranking Myopia Trap, overlooking the relative importance between documents. In contrast, Listwise methods can perceive the global ranking context, but suffer from inherent List Rigidity, leading to severe scalability and flexibility issues when handling large candidate sets. To address these challenges, we propose Groupwise, a novel reranking paradigm. In this approach, the query and a group of candidate documents are jointly fed into the model, which performs within-group comparisons to assign individual relevance scores to each document. This design retains the flexibility of Pointwise methods while enabling the comparative capability of Listwise methods. We further adopt GRPO for model training, equipped with a heterogeneous reward function that integrates ranking metrics with a distributional reward aimed at aligning score distributions across groups. To overcome the bottleneck caused by the scarcity of high quality labeled data, we further propose an innovative pipeline for synthesizing high quality retrieval and ranking data. The resulting data can be leveraged not only for training the reranker but also for training the retriever. Extensive experiments validate the effectiveness of our approach. On two reasoning intensive retrieval benchmarks, BRIGHT and R2MED.

</details>


### [138] [Exploring Multi-Table Retrieval Through Iterative Search](https://arxiv.org/abs/2511.13418)
*Allaa Boutaleb,Bernd Amann,Rafael Angarita,Hubert Naacke*

Main category: cs.IR

TL;DR: 本文提出了一种用于多表检索的迭代搜索框架，该框架在可扩展性、可解释性和灵活性方面具有优势。


<details>
  <summary>Details</summary>
Motivation: 在数据湖上进行开放域问答需要从多个表中检索和组合信息，这是一个具有挑战性的子任务，需要语义相关性和结构连贯性（例如，可连接性）。

Method: 我们提出了一个通用的框架和一个具体的实例：一个快速、有效的贪婪连接感知检索算法，该算法整体上平衡了相关性、覆盖率和可连接性。

Result: 在5个NL2SQL基准测试中进行的实验表明，与基于MIP的方法相比，我们的迭代方法实现了具有竞争力的检索性能，同时速度提高了4-400倍，具体取决于基准和搜索空间设置。

Conclusion: 这项工作突出了迭代启发式方法在实际、可扩展和组合感知检索方面的潜力。

Abstract: Open-domain question answering over datalakes requires retrieving and composing information from multiple tables, a challenging subtask that demands semantic relevance and structural coherence (e.g., joinability). While exact optimization methods like Mixed-Integer Programming (MIP) can ensure coherence, their computational complexity is often prohibitive. Conversely, simpler greedy heuristics that optimize for query coverage alone often fail to find these coherent, joinable sets. This paper frames multi-table retrieval as an iterative search process, arguing this approach offers advantages in scalability, interpretability, and flexibility. We propose a general framework and a concrete instantiation: a fast, effective Greedy Join-Aware Retrieval algorithm that holistically balances relevance, coverage, and joinability. Experiments across 5 NL2SQL benchmarks demonstrate that our iterative method achieves competitive retrieval performance compared to the MIP-based approach while being 4-400x faster depending on the benchmark and search space settings. This work highlights the potential of iterative heuristics for practical, scalable, and composition-aware retrieval.

</details>


### [139] [A Multimodal Manufacturing Safety Chatbot: Knowledge Base Design, Benchmark Development, and Evaluation of Multiple RAG Approaches](https://arxiv.org/abs/2511.11847)
*Ryan Singh,Austin Hamilton,Amanda White,Michael Wise,Ibrahim Yousif,Arthur Carvalho,Zhe Shan,Reza Abrisham Baf,Mohammad Mayyas,Lora A. Cavuoto,Fadel M. Megahed*

Main category: cs.IR

TL;DR: 本文介绍了一种基于大型语言模型的多模态聊天机器人，用于下一代安全培训系统，该系统具有高准确性、低延迟和低成本。


<details>
  <summary>Details</summary>
Motivation: 确保工人安全是现代制造业环境中的一个关键挑战。工业 5.0 将主要的制造模式重新调整为更以人为中心的操作。

Method: 使用设计科学研究方法，确定了下一代安全培训系统的三个基本要求：高精度、低延迟和低成本。聊天机器人使用检索增强生成，使其响应基于精选的监管和技术文档。

Result: 结果表明，检索策略和模型配置对性能有显著影响。最佳配置的准确率为 86.66%，平均延迟为 10.04 秒，每次查询的平均成本为 0.005 美元。

Conclusion: 总的来说，这项工作提供了三个贡献：一个开源的、基于领域的安全培训聊天机器人；一个经过验证的基准，用于评估人工智能辅助安全指导；以及一个系统的设计和评估人工智能支持的工业 5.0 环境教学和沉浸式安全培训系统的方法。

Abstract: Ensuring worker safety remains a critical challenge in modern manufacturing environments. Industry 5.0 reorients the prevailing manufacturing paradigm toward more human-centric operations. Using a design science research methodology, we identify three essential requirements for next-generation safety training systems: high accuracy, low latency, and low cost. We introduce a multimodal chatbot powered by large language models that meets these design requirements. The chatbot uses retrieval-augmented generation to ground its responses in curated regulatory and technical documentation. To evaluate our solution, we developed a domain-specific benchmark of expert-validated question and answer pairs for three representative machines: a Bridgeport manual mill, a Haas TL-1 CNC lathe, and a Universal Robots UR5e collaborative robot. We tested 24 RAG configurations using a full-factorial design and assessed them with automated evaluations of correctness, latency, and cost. Our top 2 configurations were then evaluated by ten industry experts and academic researchers. Our results show that retrieval strategy and model configuration have a significant impact on performance. The top configuration (selected for chatbot deployment) achieved an accuracy of 86.66%, an average latency of 10.04 seconds, and an average cost of $0.005 per query. Overall, our work provides three contributions: an open-source, domain-grounded safety training chatbot; a validated benchmark for evaluating AI-assisted safety instruction; and a systematic methodology for designing and assessing AI-enabled instructional and immersive safety training systems for Industry 5.0 environments.

</details>


### [140] [ComLQ: Benchmarking Complex Logical Queries in Information Retrieval](https://arxiv.org/abs/2511.12004)
*Ganlin Xu,Zhitao Yin,Linghao Zhang,Jiaqing Liang,Weijia Lu,Xiaodong Zhang,Zhifei Yang,Sihang Jiang,Deqing Yang*

Main category: cs.IR

TL;DR: 提出了一个名为ComLQ的新IR数据集，用于评估IR模型在复杂逻辑查询上的性能，这些查询涉及一阶逻辑运算，如合取、析取和否定。


<details>
  <summary>Details</summary>
Motivation: 现有的IR基准主要关注简单的查询，忽略了涉及一阶逻辑运算的复杂逻辑查询。因此，这些基准不能充分评估IR模型在真实场景中复杂查询上的性能。

Method: 利用大型语言模型（LLM）构建ComLQ数据集，通过设计带有子图指示器的子图引导提示，引导LLM基于选定的段落生成具有特定逻辑结构的查询。通过专家注释确保ComLQ中所有查询-段落对的结构一致性和证据分布。提出了一个新的评估指标，即对数缩放否定一致性（LSNC@$K$），以更好地评估检索器是否可以处理带有否定的查询。

Result: 实验结果表明，现有的检索模型在复杂逻辑查询上的性能有限，尤其是在带有否定的查询上，暴露了它们在排除建模方面的不足。

Conclusion: ComLQ数据集和LSNC@$K$指标可以有效地评估IR模型在复杂逻辑查询上的性能，并揭示现有模型在处理否定方面的局限性。

Abstract: Information retrieval (IR) systems play a critical role in navigating information overload across various applications. Existing IR benchmarks primarily focus on simple queries that are semantically analogous to single- and multi-hop relations, overlooking \emph{complex logical queries} involving first-order logic operations such as conjunction ($\land$), disjunction ($\lor$), and negation ($\lnot$). Thus, these benchmarks can not be used to sufficiently evaluate the performance of IR models on complex queries in real-world scenarios. To address this problem, we propose a novel method leveraging large language models (LLMs) to construct a new IR dataset \textbf{ComLQ} for \textbf{Com}plex \textbf{L}ogical \textbf{Q}ueries, which comprises 2,909 queries and 11,251 candidate passages. A key challenge in constructing the dataset lies in capturing the underlying logical structures within unstructured text. Therefore, by designing the subgraph-guided prompt with the subgraph indicator, an LLM (such as GPT-4o) is guided to generate queries with specific logical structures based on selected passages. All query-passage pairs in ComLQ are ensured \emph{structure conformity} and \emph{evidence distribution} through expert annotation. To better evaluate whether retrievers can handle queries with negation, we further propose a new evaluation metric, \textbf{Log-Scaled Negation Consistency} (\textbf{LSNC@$K$}). As a supplement to standard relevance-based metrics (such as nDCG and mAP), LSNC@$K$ measures whether top-$K$ retrieved passages violate negation conditions in queries. Our experimental results under zero-shot settings demonstrate existing retrieval models' limited performance on complex logical queries, especially on queries with negation, exposing their inferior capabilities of modeling exclusion.

</details>


### [141] [From Scaling to Structured Expressivity: Rethinking Transformers for CTR Prediction](https://arxiv.org/abs/2511.12081)
*Bencheng Yan,Yuejie Lei,Zhiyuan Zeng,Di Wang,Kaiyi Lin,Pengjie Wang,Jian Xu,Bo Zheng*

Main category: cs.IR

TL;DR: 提出了Field-Aware Transformer (FAT)模型，以解决CTR预测中深度模型收益递减的问题。


<details>
  <summary>Details</summary>
Motivation: 现有CTR预测模型在扩展规模时收益迅速递减，因为Transformer的结构与CTR数据的组合性质不匹配。

Method: FAT模型通过分解内容对齐和跨字段调制，将基于字段的交互先验知识嵌入到注意力机制中。

Result: 在大型基准测试中，FAT的AUC提高了高达+0.51%，在线部署后，CTR提高了+2.33%，RPM提高了+0.66%。

Conclusion: 有效的推荐系统扩展并非来自模型规模，而是来自结构化的表达能力和与数据语义的架构一致性。

Abstract: Despite massive investments in scale, deep models for click-through rate (CTR) prediction often exhibit rapidly diminishing returns - a stark contrast to the smooth, predictable gains seen in large language models. We identify the root cause as a structural misalignment: Transformers assume sequential compositionality, while CTR data demand combinatorial reasoning over high-cardinality semantic fields. Unstructured attention spreads capacity indiscriminately, amplifying noise under extreme sparsity and breaking scalable learning. To restore alignment, we introduce the Field-Aware Transformer (FAT), which embeds field-based interaction priors into attention through decomposed content alignment and cross-field modulation. This design ensures model complexity scales with the number of fields F, not the total vocabulary size n >> F, leading to tighter generalization and, critically, observed power-law scaling in AUC as model width increases. We present the first formal scaling law for CTR models, grounded in Rademacher complexity, that explains and predicts this behavior. On large-scale benchmarks, FAT improves AUC by up to +0.51% over state-of-the-art methods. Deployed online, it delivers +2.33% CTR and +0.66% RPM. Our work establishes that effective scaling in recommendation arises not from size, but from structured expressivity-architectural coherence with data semantics.

</details>


### [142] [Continuous-time Discrete-space Diffusion Model for Recommendation](https://arxiv.org/abs/2511.12114)
*Chengyi Liu,Xiao Chen,Shijie Wang,Wenqi Fan,Qing Li*

Main category: cs.IR

TL;DR: 提出了一种新的基于连续时间离散空间扩散的推荐框架CDRec，通过在连续时间内对历史交互进行离散扩散来建模用户行为模式。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散的推荐方法主要在连续空间中运行，可能导致潜在的信息丢失和计算效率低下。

Method: CDRec 引入了新颖的流行度感知噪声计划，该计划生成语义上有意义的扩散轨迹，以及一个有效的训练框架，该框架结合了一致性参数化以进行快速采样，以及一个由多跳协作信号引导的对比学习目标，以实现个性化推荐。

Result: 在真实世界数据集上的大量实验表明，CDRec 在推荐准确性和计算效率方面均表现出色。

Conclusion: CDRec 是一种有前途的推荐框架，它优于现有技术。

Abstract: In the era of information explosion, Recommender Systems (RS) are essential for alleviating information overload and providing personalized user experiences. Recent advances in diffusion-based generative recommenders have shown promise in capturing the dynamic nature of user preferences. These approaches explore a broader range of user interests by progressively perturbing the distribution of user-item interactions and recovering potential preferences from noise, enabling nuanced behavioral understanding. However, existing diffusion-based approaches predominantly operate in continuous space through encoded graph-based historical interactions, which may compromise potential information loss and suffer from computational inefficiency. As such, we propose CDRec, a novel Continuous-time Discrete-space Diffusion Recommendation framework, which models user behavior patterns through discrete diffusion on historical interactions over continuous time. The discrete diffusion algorithm operates via discrete element operations (e.g., masking) while incorporating domain knowledge through transition matrices, producing more meaningful diffusion trajectories. Furthermore, the continuous-time formulation enables flexible adaptive sampling. To better adapt discrete diffusion models to recommendations, CDRec introduces: (1) a novel popularity-aware noise schedule that generates semantically meaningful diffusion trajectories, and (2) an efficient training framework combining consistency parameterization for fast sampling and a contrastive learning objective guided by multi-hop collaborative signals for personalized recommendation. Extensive experiments on real-world datasets demonstrate CDRec's superior performance in both recommendation accuracy and computational efficiency.

</details>


### [143] [Task-Aware Retrieval Augmentation for Dynamic Recommendation](https://arxiv.org/abs/2511.12495)
*Zhen Tao,Xinke Jiang,Qingshuai Feng,Haoyu Zhang,Lun Du,Yuchen Fang,Hao Miao,Bangquan Xie,Qingqiang Sun*

Main category: cs.IR

TL;DR: TarDGR通过结合任务感知模型和检索增强来提高泛化能力，缓解了动态推荐系统中预训练的图神经网络在微调时由于时间差异导致的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现有动态推荐系统在时序行为数据上微调图神经网络时，由于预训练和微调阶段的时间差异，导致泛化问题。

Method: 提出了一种任务感知的检索增强框架TarDGR，包括任务感知评估机制和基于图Transformer的任务感知模型，用于识别和融合相关的历史子图。

Result: 在多个大规模动态图数据集上的实验表明，TarDGR优于现有方法，具有更高的准确性和泛化能力。

Conclusion: TarDGR能够有效缓解动态推荐系统中由于时间差异导致的泛化问题，并通过实验验证了其优越性。

Abstract: Dynamic recommendation systems aim to provide personalized suggestions by modeling temporal user-item interactions across time-series behavioral data. Recent studies have leveraged pre-trained dynamic graph neural networks (GNNs) to learn user-item representations over temporal snapshot graphs. However, fine-tuning GNNs on these graphs often results in generalization issues due to temporal discrepancies between pre-training and fine-tuning stages, limiting the model's ability to capture evolving user preferences. To address this, we propose TarDGR, a task-aware retrieval-augmented framework designed to enhance generalization capability by incorporating task-aware model and retrieval-augmentation. Specifically, TarDGR introduces a Task-Aware Evaluation Mechanism to identify semantically relevant historical subgraphs, enabling the construction of task-specific datasets without manual labeling. It also presents a Graph Transformer-based Task-Aware Model that integrates semantic and structural encodings to assess subgraph relevance. During inference, TarDGR retrieves and fuses task-aware subgraphs with the query subgraph, enriching its representation and mitigating temporal generalization issues. Experiments on multiple large-scale dynamic graph datasets demonstrate that TarDGR consistently outperforms state-of-the-art methods, with extensive empirical evidence underscoring its superior accuracy and generalization capabilities.

</details>


### [144] [DualGR: Generative Retrieval with Long and Short-Term Interests Modeling](https://arxiv.org/abs/2511.12518)
*Zhongchao Yi,Kai Feng,Xiaojian Ma,Yalong Wang,Yongqi Liu,Han Li,Zhengyang Zhou,Yang Wang*

Main category: cs.IR

TL;DR: DualGR: 一个用于工业生成检索的框架，通过显式建模用户兴趣的双重视野来解决现有生成检索方法中的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有生成检索方法在平衡用户长期和短期兴趣、处理分层语义ID (SIDs) 生成中的噪声干扰以及缺乏对负反馈的显式建模方面存在挑战。

Method: 提出 DualGR 框架，利用双分支长/短期路由器 (DBR) 覆盖稳定偏好和瞬时意图，并提出基于搜索的 SID 解码 (S2D) 来控制上下文诱导的噪声，并提出 Exposure-aware Next-Token Prediction Loss (ENTP-Loss) 来处理负反馈。

Result: 在快手短视频推荐系统上的大量实验表明，DualGR 取得了显著的性能提升。在线 A/B 测试显示视频观看次数 +0.527%，观看时长 +0.432%。

Conclusion: DualGR 是一个用于工业生成检索的实用且有效的范例。

Abstract: In large-scale industrial recommendation systems, retrieval must produce high-quality candidates from massive corpora under strict latency. Recently, Generative Retrieval (GR) has emerged as a viable alternative to Embedding-Based Retrieval (EBR), which quantizes items into a finite token space and decodes candidates autoregressively, providing a scalable path that explicitly models target-history interactions via cross-attention. However, three challenges persist: 1) how to balance users' long-term and short-term interests , 2) noise interference when generating hierarchical semantic IDs (SIDs), 3) the absence of explicit modeling for negative feedback such as exposed items without clicks. To address these challenges, we propose DualGR, a generative retrieval framework that explicitly models dual horizons of user interests with selective activation. Specifically, DualGR utilizes Dual-Branch Long/Short-Term Router (DBR) to cover both stable preferences and transient intents by explicitly modeling users' long- and short-term behaviors. Meanwhile, Search-based SID Decoding (S2D) is presented to control context-induced noise and enhance computational efficiency by constraining candidate interactions to the current coarse (level-1) bucket during fine-grained (level-2/3) SID prediction. % also reinforcing intra-class consistency. Finally, we propose an Exposure-aware Next-Token Prediction Loss (ENTP-Loss) that treats "exposed-but-unclicked" items as hard negatives at level-1, enabling timely interest fade-out. On the large-scale Kuaishou short-video recommendation system, DualGR has achieved outstanding performance. Online A/B testing shows +0.527% video views and +0.432% watch time lifts, validating DualGR as a practical and effective paradigm for industrial generative retrieval.

</details>


### [145] [MindRec: Mind-inspired Coarse-to-fine Decoding for Generative Recommendation](https://arxiv.org/abs/2511.12597)
*Mengyao Gao,Chongming Gao,Haoyan Liu,Qingpeng Cai,Peng Jiang,Jiajia Chen,Shuai Yuan,Xiangnan He*

Main category: cs.IR

TL;DR: MindRec：一个模仿人类思维的推荐框架，通过生成关键tokens并逐步细化为完整项目来实现更灵活、类人的推荐生成。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型语言模型的推荐系统由于自回归生成方式和单向逻辑流，难以产生全局最优的推荐。

Method: 1.  提出 Mind-inspired Recommender (MindRec)，首先生成反映用户偏好的关键 tokens，然后将它们扩展为完整的项目。
2.  将项目组织成层级类别树，引导模型首先生成粗粒度的类别，然后通过更细粒度的子类别逐步细化选择，最后生成特定的项目。
3.  设计了一种新颖的 beam search 算法，Diffusion Beam Search，专为 mind-inspired 生成范式定制，以缓解贪婪解码中固有的局部最优问题。

Result: MindRec 在 top-1 推荐性能上比最先进的方法平均提高了 9.5%。

Conclusion: MindRec 具有增强推荐准确性的潜力。

Abstract: Recent advancements in large language model-based recommendation systems often represent items as text or semantic IDs and generate recommendations in an auto-regressive manner. However, due to the left-to-right greedy decoding strategy and the unidirectional logical flow, such methods often fail to produce globally optimal recommendations. In contrast, human reasoning does not follow a rigid left-to-right sequence. Instead, it often begins with keywords or intuitive insights, which are then refined and expanded. Inspired by this fact, we propose Mind-inspired Recommender (MindRec), a novel generative framework that emulates human thought processes. Particularly, our method first generates key tokens that reflect user preferences, and then expands them into the complete item, enabling flexible and human-like generation. To further emulate the structured nature of human decision-making, we organize items into a hierarchical category tree. This structure guides the model to first produce the coarse-grained category and then progressively refine its selection through finer-grained subcategories before generating the specific item. To mitigate the local optimum problem inherent in greedy decoding, we design a novel beam search algorithm, Diffusion Beam Search, tailored for our mind-inspired generation paradigm. Experimental results demonstrate that MindRec yields a 9.5\% average improvement in top-1 recommendation performance over state-of-the-art methods, highlighting its potential to enhance recommendation accuracy. The implementation is available via https://github.com/Mr-Peach0301/MindRec.

</details>


### [146] [Tokenize Once, Recommend Anywhere: Unified Item Tokenization for Multi-domain LLM-based Recommendation](https://arxiv.org/abs/2511.12922)
*Yu Hou,Won-Yong Shin*

Main category: cs.IR

TL;DR: UniTok是一个统一的项目标记框架，它集成了MoE架构和一系列代码簿，以将项目转换为离散的token，从而实现可扩展的标记化，同时保留跨多个项目域的语义信息。


<details>
  <summary>Details</summary>
Motivation: 现有的项目标记化方法通常需要为每个项目域训练单独的模型，从而限制了泛化能力。此外，跨项目域的各种分布和语义使得构建能够保留领域特定信息的统一标记化变得困难。

Method: UniTok框架首先通过共享编码器将来自不同领域的项目投影到统一的潜在空间中。然后，它们被路由到领域特定的专家以捕获独特的语义，而始终处于活动状态的共享专家则对跨领域的可转移的常识进行编码。此外，为了减轻跨领域的语义不平衡，我们提出了一种互信息校准机制，该机制引导模型为每个领域保留相似水平的语义信息。

Result: 在广泛的真实世界数据集上进行的综合实验表明，所提出的UniTok框架是高度有效的：与强大的基线相比，实现了高达51.89%的改进。

Conclusion: UniTok框架具有高度的通用性：在无需每个领域重新训练的情况下，证明了跨不同领域的强大性能，这是现有基线不支持的功能。

Abstract: Large language model (LLM)-based recommender systems have achieved high-quality performance by bridging the discrepancy between the item space and the language space through item tokenization. However, existing item tokenization methods typically require training separate models for each item domain, limiting generalization. Moreover, the diverse distributions and semantics across item domains make it difficult to construct a unified tokenization that preserves domain-specific information. To address these challenges, we propose UniTok, a Unified item Tokenization framework that integrates our own mixture-of-experts (MoE) architecture with a series of codebooks to convert items into discrete tokens, enabling scalable tokenization while preserving semantic information across multiple item domains. Specifically, items from different domains are first projected into a unified latent space through a shared encoder. They are then routed to domain-specific experts to capture the unique semantics, while a shared expert, which is always active, encodes common knowledge transferable across domains. Additionally, to mitigate semantic imbalance across domains, we present a mutual information calibration mechanism, which guides the model towards retaining similar levels of semantic information for each domain. Comprehensive experiments on wide-ranging real-world datasets demonstrate that the proposed UniTok framework is (a) highly effective: achieving up to 51.89% improvements over strong benchmarks, (b) theoretically sound: showing the analytical validity of our architectural design and optimization; and (c) highly generalizable: demonstrating robust performance across diverse domains without requiring per-domain retraining, a capability not supported by existing baselines.

</details>


### [147] [A Plug-and-Play Spatially-Constrained Representation Enhancement Framework for Local-Life Recommendation](https://arxiv.org/abs/2511.12947)
*Hao Jiang,Guoquan Wang,Sheng Yu,Yang Zeng,Wencong Zeng,Guorui Zhou*

Main category: cs.IR

TL;DR: 提出了一种名为ReST的框架，用于解决本地生活推荐中的空间约束和长尾稀疏性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常以用户为中心，忽略了项目本身的空间约束特性和长尾商品的表示问题。

Method: 提出一个插件式的空间约束表示增强框架ReST，包括元ID预热网络和空间约束ID表示增强网络（SIDENet），利用对比学习增强长尾商品的表示。

Result: 通过空间约束的硬采样和动态表示对齐策略，SIDENet能够自适应地识别和增强弱ID表示，同时保留与热门商品的兼容性。

Conclusion: ReST框架能够有效解决本地生活推荐中的空间约束和长尾稀疏性问题，提升推荐效果。

Abstract: Local-life recommendation have witnessed rapid growth, providing users with convenient access to daily essentials. However, this domain faces two key challenges: (1) spatial constraints, driven by the requirements of the local-life scenario, where items are usually shown only to users within a limited geographic area, indirectly reducing their exposure probability; and (2) long-tail sparsity, where few popular items dominate user interactions, while many high-quality long-tail items are largely overlooked due to imbalanced interaction opportunities. Existing methods typically adopt a user-centric perspective, such as modeling spatial user preferences or enhancing long-tail representations with collaborative filtering signals. However, we argue that an item-centric perspective is more suitable for this domain, focusing on enhancing long-tail items representation that align with the spatially-constrained characteristics of local lifestyle services. To tackle this issue, we propose ReST, a Plug-And-Play Spatially-Constrained Representation Enhancement Framework for Long-Tail Local-Life Recommendation. Specifically, we first introduce a Meta ID Warm-up Network, which initializes fundamental ID representations by injecting their basic attribute-level semantic information. Subsequently, we propose a novel Spatially-Constrained ID Representation Enhancement Network (SIDENet) based on contrastive learning, which incorporates two efficient strategies: a spatially-constrained hard sampling strategy and a dynamic representation alignment strategy. This design adaptively identifies weak ID representations based on their attribute-level information during training. It additionally enhances them by capturing latent item relationships within the spatially-constrained characteristics of local lifestyle services, while preserving compatibility with popular items.

</details>


### [148] [Can We Predict the Next Question? A Collaborative Filtering Approach to Modeling User Behavior](https://arxiv.org/abs/2511.12949)
*Bokang Fu,Jiahao Wang,Xiaojing Liu,Yuli Liu*

Main category: cs.IR

TL;DR: 提出了一种名为 CFQP 的框架，用于动态建模用户提问行为，以解决大型语言模型在理解用户动态偏好方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在理解和生成语言方面表现出色，但它们通常静态地建模用户偏好，无法捕捉交互行为的动态和顺序性。

Method: 通过整合个性化的记忆模块与基于图的偏好传播来动态建模用户-问题交互。

Result: 实验结果表明，该方法能够有效生成模仿真实用户提问模式的代理。

Conclusion: 该方法在构建主动和自适应对话系统方面具有潜力。

Abstract: In recent years, large language models (LLMs) have excelled in language understanding and generation, powering advanced dialogue and recommendation systems. However, a significant limitation persists: these systems often model user preferences statically, failing to capture the dynamic and sequential nature of interactive behaviors. The sequence of a user's historical questions provides a rich, implicit signal of evolving interests and cognitive patterns, yet leveraging this temporal data for predictive tasks remains challenging due to the inherent disconnect between language modeling and behavioral sequence modeling.
  To bridge this gap, we propose a Collaborative Filtering-enhanced Question Prediction (CFQP) framework. CFQP dynamically models evolving user-question interactions by integrating personalized memory modules with graph-based preference propagation. This dual mechanism allows the system to adaptively learn from user-specific histories while refining predictions through collaborative signals from similar users. Experimental results demonstrate that our approach effectively generates agents that mimic real-user questioning patterns, highlighting its potential for building proactive and adaptive dialogue systems.

</details>


### [149] [Personalized Federated Recommendation With Knowledge Guidance](https://arxiv.org/abs/2511.12959)
*Jaehyung Lim,Wonbin Kweon,Woojoo Kim,Junyoung Kim,Dongha Kim,Hwanjo Yu*

Main category: cs.IR

TL;DR: 提出了一种新的联邦推荐框架FedRKG，旨在解决现有联邦推荐模型在内存效率和个性化之间的两难问题。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦推荐模型面临一个关键问题：内存效率高的单知识模型会丢弃有价值的个性化信息，而高性能的双知识模型通常内存密集，难以在设备上部署。

Method: 提出了知识引导，避免完全替换，而是将全局知识融入到保留的本地嵌入中，在单知识内存占用内实现双知识的个性化优势。此外，引入自适应引导，一种细粒度机制，可以动态地调整每个用户-项目交互的引导强度。

Result: 在基准数据集上的大量实验表明，FedRKG 显著优于最先进的方法。

Conclusion: 验证了该方法的有效性。

Abstract: Federated Recommendation (FedRec) has emerged as a key paradigm for building privacy-preserving recommender systems. However, existing FedRec models face a critical dilemma: memory-efficient single-knowledge models suffer from a suboptimal knowledge replacement practice that discards valuable personalization, while high-performance dual-knowledge models are often too memory-intensive for practical on-device deployment. We propose Federated Recommendation with Knowledge Guidance (FedRKG), a model-agnostic framework that resolves this dilemma. The core principle, Knowledge Guidance, avoids full replacement and instead fuses global knowledge into preserved local embeddings, attaining the personalization benefits of dual-knowledge within a single-knowledge memory footprint. Furthermore, we introduce Adaptive Guidance, a fine-grained mechanism that dynamically modulates the intensity of this guidance for each user-item interaction, overcoming the limitations of static fusion methods. Extensive experiments on benchmark datasets demonstrate that FedRKG significantly outperforms state-of-the-art methods, validating the effectiveness of our approach. The code is available at https://github.com/Jaehyung-Lim/fedrkg.

</details>


### [150] [Mitigating Recommendation Biases via Group-Alignment and Global-Uniformity in Representation Learning](https://arxiv.org/abs/2511.13041)
*Miaomiao Cai,Min Hou,Lei Chen,Le Wu,Haoyue Bai,Yong Li,Meng Wang*

Main category: cs.IR

TL;DR: 提出了一种新的推荐框架AURL，以缓解推荐中的偏差。


<details>
  <summary>Details</summary>
Motivation: CF方法由于训练数据的不平衡而存在偏差，导致推荐结果倾向于流行物品和对不活跃用户表现不佳。

Method: 从表示分布的角度，通过组对齐和全局均匀性增强表示学习来消除偏差。提出了组对齐和全局均匀性两个正则化项，分别解决组差异和全局崩溃问题。

Result: 在三个真实数据集和各种推荐骨干网络上的大量实验验证了所提出框架的优越性。

Conclusion: 该方法通过优化组对齐和全局均匀性正则化项来减轻推荐偏差。

Abstract: Collaborative Filtering~(CF) plays a crucial role in modern recommender systems, leveraging historical user-item interactions to provide personalized suggestions. However, CF-based methods often encounter biases due to imbalances in training data. This phenomenon makes CF-based methods tend to prioritize recommending popular items and performing unsatisfactorily on inactive users. Existing works address this issue by rebalancing training samples, reranking recommendation results, or making the modeling process robust to the bias. Despite their effectiveness, these approaches can compromise accuracy or be sensitive to weighting strategies, making them challenging to train. In this paper, we deeply analyze the causes and effects of the biases and propose a framework to alleviate biases in recommendation from the perspective of representation distribution, namely Group-Alignment and Global-Uniformity Enhanced Representation Learning for Debiasing Recommendation (AURL). Specifically, we identify two significant problems in the representation distribution of users and items, namely group-discrepancy and global-collapse. These two problems directly lead to biases in the recommendation results. To this end, we propose two simple but effective regularizers in the representation space, respectively named group-alignment and global-uniformity. The goal of group-alignment is to bring the representation distribution of long-tail entities closer to that of popular entities, while global-uniformity aims to preserve the information of entities as much as possible by evenly distributing representations. Our method directly optimizes both the group-alignment and global-uniformity regularization terms to mitigate recommendation biases. Extensive experiments on three real datasets and various recommendation backbones verify the superiority of our proposed framework.

</details>


### [151] [Dimension vs. Precision: A Comparative Analysis of Autoencoders and Quantization for Efficient Vector Retrieval on BEIR SciFact](https://arxiv.org/abs/2511.13057)
*Satyanarayan Pati*

Main category: cs.IR

TL;DR: 这篇论文研究了如何压缩密集检索模型，以减少存储和内存的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前密集检索模型的高维度向量嵌入给实际部署带来了存储和内存的挑战。

Method: 通过在BEIR SciFact基准上，评估降维（使用自编码器）和量化（float16, int8, and binary）两种压缩策略的权衡。

Result: int8标量量化在性能损失很小的情况下（约1-2%的nDCG@10下降）实现了4倍压缩，效果最好。自编码器虽然表现出平缓的性能退化，但在相同的4倍压缩率下，性能损失更大。Binary量化由于性能下降严重，不适合这项任务。

Conclusion: 这项工作为部署高效、高性能的检索系统提供了实践指导。

Abstract: Dense retrieval models have become a standard for state-of-the-art information retrieval. However, their high-dimensional, high-precision (float32) vector embeddings create significant storage and memory challenges for real-world deployment. To address this, we conduct a rigorous empirical study on the BEIR SciFact benchmark, evaluating the trade-offs between two primary compression strategies: (1) Dimensionality Reduction via deep Autoencoders (AE), reducing original 384-dim vectors to latent spaces from 384 down to 12, and (2) Precision Reduction via Quantization (float16, int8, and binary). We systematically compare each method by measuring the "performance loss" (or gain) relative to a float32 baseline across a full suite of retrieval metrics (NDCG, MAP, MRR, Recall, Precision) at various k cutoffs. Our results show that int8 scalar quantization provides the most effective "sweet spot," achieving a 4x compression with a negligible [~1-2%] drop in nDCG@10. In contrast, Autoencoders show a graceful degradation but suffer a more significant performance loss at equivalent 4x compression ratios (AE-96). binary quantization was found to be unsuitable for this task due to catastrophic performance drops. This work provides a practical guide for deploying efficient, high-performance retrieval systems.

</details>


### [152] [Local Collaborative Filtering: A Collaborative Filtering Method that Utilizes Local Similarities among Users](https://arxiv.org/abs/2511.13166)
*Zhaoxin Shen,Dan Wu*

Main category: cs.IR

TL;DR: 提出了一种名为局部协同过滤（LCF）的新型协同过滤（CF）方法，旨在更有效地利用来自互联网的用户行为数据。


<details>
  <summary>Details</summary>
Motivation: 为了更有效地利用来自互联网的用户行为数据。

Method: LCF利用用户之间的局部相似性，并使用大数定律（LLN）整合他们的数据，从而提高用户行为数据的利用率。

Result: 在Steam游戏数据集上进行了实验，LCF的结果与现实世界的需求相符。

Conclusion: LCF方法在利用用户行为数据方面是有效的，并且与现实需求相符。

Abstract: To leverage user behavior data from the Internet more effectively in recommender systems, this paper proposes a novel collaborative filtering (CF) method called Local Collaborative Filtering (LCF). LCF utilizes local similarities among users and integrates their data using the law of large numbers (LLN), thereby improving the utilization of user behavior data. Experiments are conducted on the Steam game dataset, and the results of LCF align with real-world needs.

</details>


### [153] [Cog-RAG: Cognitive-Inspired Dual-Hypergraph with Theme Alignment Retrieval-Augmented Generation](https://arxiv.org/abs/2511.13201)
*Hao Hu,Yifan Feng,Ruoxue Li,Rundong Xue,Xingliang Hou,Zhiqiang Tian,Yue Gao,Shaoyi Du*

Main category: cs.IR

TL;DR: 提出了一个主题对齐的双超图RAG框架（Cog-RAG），它使用主题超图来捕获块间主题结构，并使用实体超图来建模高阶语义关系。


<details>
  <summary>Details</summary>
Motivation: 现有的图结构RAG主要关注低阶配对实体关系，限制了多个实体之间的高阶关联。超图增强方法通过超边建模多实体交互来解决这个限制，但它们通常被限制在块间实体级别的表示，忽略了跨块的全局主题组织和对齐。

Method: 设计了一个认知启发的两阶段检索策略，首先从主题超图中激活查询相关的主题内容，然后在实体超图中引导细粒度的召回和扩散，实现从全局主题到局部细节的语义对齐和一致生成。

Result: Cog-RAG显著优于现有的最先进的基线方法。

Conclusion: Cog-RAG框架有效地提升了RAG在捕获语义关系和生成质量方面的性能。

Abstract: Retrieval-Augmented Generation (RAG) enhances the response quality and domain-specific performance of large language models (LLMs) by incorporating external knowledge to combat hallucinations. In recent research, graph structures have been integrated into RAG to enhance the capture of semantic relations between entities. However, it primarily focuses on low-order pairwise entity relations, limiting the high-order associations among multiple entities. Hypergraph-enhanced approaches address this limitation by modeling multi-entity interactions via hyperedges, but they are typically constrained to inter-chunk entity-level representations, overlooking the global thematic organization and alignment across chunks. Drawing inspiration from the top-down cognitive process of human reasoning, we propose a theme-aligned dual-hypergraph RAG framework (Cog-RAG) that uses a theme hypergraph to capture inter-chunk thematic structure and an entity hypergraph to model high-order semantic relations. Furthermore, we design a cognitive-inspired two-stage retrieval strategy that first activates query-relevant thematic content from the theme hypergraph, and then guides fine-grained recall and diffusion in the entity hypergraph, achieving semantic alignment and consistent generation from global themes to local details. Our extensive experiments demonstrate that Cog-RAG significantly outperforms existing state-of-the-art baseline approaches.

</details>


### [154] [Uncovering Causal Drivers of Energy Efficiency for Industrial Process in Foundry via Time-Series Causal Inference](https://arxiv.org/abs/2511.13389)
*Zhipeng Ma,Bo Nørregaard Jørgensen,Zheng Grace Ma*

Main category: cs.IR

TL;DR: 本文运用时间序列因果推理框架，识别了感应炉熔炼中直接影响能源效率的运行因素。


<details>
  <summary>Details</summary>
Motivation: 工业铸造过程能耗高，过程变量间相互依赖关系复杂，因此提高工业铸造过程的能源效率至关重要。基于相关性的分析通常无法区分真实的因果驱动因素和虚假关联，限制了它们在决策中的作用。

Method: 该研究整合了时间序列聚类，将熔炼周期分割成不同的运行模式，并利用最先进的因果发现方法PCMCI+算法，揭示了每种模式中的因果关系。

Result: 结果表明，能源消耗、炉温和材料重量之间存在稳健的因果关系，是能源效率的核心驱动因素，而电压始终以延迟响应的方式影响冷却水温度。高效集群以稳定的因果结构为特征，而低效集群则表现出强化的反馈回路和非典型的依赖关系。

Conclusion: 该研究的贡献是双重的。首先，它引入了一个集成的聚类-因果推理管道，作为分析能源密集型过程的方法创新。其次，它提供了可操作的见解，使铸造操作员能够优化性能，减少能源消耗并降低排放。

Abstract: Improving energy efficiency in industrial foundry processes is a critical challenge, as these operations are highly energy-intensive and marked by complex interdependencies among process variables. Correlation-based analyses often fail to distinguish true causal drivers from spurious associations, limiting their usefulness for decision-making. This paper applies a time-series causal inference framework to identify the operational factors that directly affect energy efficiency in induction furnace melting. Using production data from a Danish foundry, the study integrates time-series clustering to segment melting cycles into distinct operational modes with the PCMCI+ algorithm, a state-of-the-art causal discovery method, to uncover cause-effect relationships within each mode. Across clusters, robust causal relations among energy consumption, furnace temperature, and material weight define the core drivers of efficiency, while voltage consistently influences cooling water temperature with a delayed response. Cluster-specific differences further distinguish operational regimes: efficient clusters are characterized by stable causal structures, whereas inefficient ones exhibit reinforcing feedback loops and atypical dependencies. The contributions of this study are twofold. First, it introduces an integrated clustering-causal inference pipeline as a methodological innovation for analyzing energy-intensive processes. Second, it provides actionable insights that enable foundry operators to optimize performance, reduce energy consumption, and lower emissions.

</details>


### [155] [Attention Grounded Enhancement for Visual Document Retrieval](https://arxiv.org/abs/2511.13415)
*Wanqing Cui,Wei Huang,Yazhi Guo,Yibo Hu,Meiguang Jin,Junfeng Ma,Keping Bi*

Main category: cs.IR

TL;DR: 提出了一种名为AGREE的框架，利用多模态大型语言模型的跨模态注意力作为代理局部监督，以指导识别相关文档区域，从而提升视觉文档检索的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的检索器训练依赖粗糙的全局相关性标签，无法揭示哪些区域支持匹配，导致检索器依赖表面线索，难以捕捉隐含的语义联系，从而难以处理非抽取式查询。

Method: AGREE框架结合局部信号和全局信号联合优化检索器，使其不仅学习文档是否匹配，还学习哪些内容驱动相关性。

Result: 在具有挑战性的ViDoRe V2基准测试中，AGREE显著优于仅使用全局监督的基线。

Conclusion: AGREE框架能够促进查询词和文档区域之间更深层次的对齐，超越表面匹配，实现更准确和可解释的检索。

Abstract: Visual document retrieval requires understanding heterogeneous and multi-modal content to satisfy information needs. Recent advances use screenshot-based document encoding with fine-grained late interaction, significantly improving retrieval performance. However, retrievers are still trained with coarse global relevance labels, without revealing which regions support the match. As a result, retrievers tend to rely on surface-level cues and struggle to capture implicit semantic connections, hindering their ability to handle non-extractive queries. To alleviate this problem, we propose a \textbf{A}ttention-\textbf{G}rounded \textbf{RE}triever \textbf{E}nhancement (AGREE) framework. AGREE leverages cross-modal attention from multimodal large language models as proxy local supervision to guide the identification of relevant document regions. During training, AGREE combines local signals with the global signals to jointly optimize the retriever, enabling it to learn not only whether documents match, but also which content drives relevance. Experiments on the challenging ViDoRe V2 benchmark show that AGREE significantly outperforms the global-supervision-only baseline. Quantitative and qualitative analyses further demonstrate that AGREE promotes deeper alignment between query terms and document regions, moving beyond surface-level matching toward more accurate and interpretable retrieval. Our code is available at: https://anonymous.4open.science/r/AGREE-2025.

</details>


### [156] [Compact Multimodal Language Models as Robust OCR Alternatives for Noisy Textual Clinical Reports](https://arxiv.org/abs/2511.13523)
*Nikita Neveditsin,Pawan Lingras,Salil Patil,Swarup Patil,Vijay Mago*

Main category: cs.IR

TL;DR: 本研究探讨了使用紧凑型多模态语言模型转录嘈杂的临床文档，以解决传统 OCR 系统在处理由智能手机拍摄的医疗记录图像时表现不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 传统 OCR 系统在处理由智能手机拍摄的、受模糊、阴影和其他噪声影响的医疗记录图像时表现不佳。本研究旨在寻找一种更有效的转录噪声临床文档的替代方案。

Method: 研究使用了在印度医疗环境中常见的、用带有区域色彩的医学英语编写的产科超声报告，并比较了八个系统在转录准确率、噪声敏感度、数字准确率和计算效率方面的表现。

Result: 紧凑型多模态模型始终优于传统和神经 OCR 流程。

Conclusion: 尽管计算成本较高，但紧凑型多模态模型的稳健性和语言适应性使其成为本地医疗数字化可行的候选方案。

Abstract: Digitization of medical records often relies on smartphone photographs of printed reports, producing images degraded by blur, shadows, and other noise. Conventional OCR systems, optimized for clean scans, perform poorly under such real-world conditions. This study evaluates compact multimodal language models as privacy-preserving alternatives for transcribing noisy clinical documents. Using obstetric ultrasound reports written in regionally inflected medical English common to Indian healthcare settings, we compare eight systems in terms of transcription accuracy, noise sensitivity, numeric accuracy, and computational efficiency. Compact multimodal models consistently outperform both classical and neural OCR pipelines. Despite higher computational costs, their robustness and linguistic adaptability position them as viable candidates for on-premises healthcare digitization.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [157] [Softmax as a Lagrangian-Legendrian Seam](https://arxiv.org/abs/2511.11573)
*Christopher R. Lee-Jenkins*

Main category: cs.LG

TL;DR: softmax可以被建模为一个几何界面：两个势生成的守恒描述（来自负熵和log-sum-exp）在一个简单的折叠辛环内的接触屏幕（概率单纯形）上的一个勒让德“缝”上相遇。


<details>
  <summary>Details</summary>
Motivation: 建立机器学习与现代微分几何之间的桥梁

Method: 将softmax建模为一个几何界面，研究其在概率单纯形上的性质。

Result: 偏差-平移不变性表现为屏幕上的Reeb流，Fenchel-Young等式/KL间隙提供了到缝的可计算距离。详细阐述了二类和三类的情况。

Conclusion: 提出了机器学习的下一步：紧凑logit模型（射影或球形），全局不变量，以及与信息几何的联系，其中屏幕上的动力学表现为复制器流。

Abstract: This note offers a first bridge from machine learning to modern differential geometry. We show that the logits-to-probabilities step implemented by softmax can be modeled as a geometric interface: two potential-generated, conservative descriptions (from negative entropy and log-sum-exp) meet along a Legendrian "seam" on a contact screen (the probability simplex) inside a simple folded symplectic collar. Bias-shift invariance appears as Reeb flow on the screen, and the Fenchel-Young equality/KL gap provides a computable distance to the seam. We work out the two- and three-class cases to make the picture concrete and outline next steps for ML: compact logit models (projective or spherical), global invariants, and connections to information geometry where on-screen dynamics manifest as replicator flows.

</details>


### [158] [RAGPulse: An Open-Source RAG Workload Trace to Optimize RAG Serving Systems](https://arxiv.org/abs/2511.12979)
*Zhengchao Wang,Yitao Hu,Jianing Ye,Zhuxuan Chang,Jiazheng Yu,Youpeng Deng,Keqiu Li*

Main category: cs.LG

TL;DR: RAGPulse是一个开源的RAG工作负载跟踪数据集，旨在弥合学术研究与实际部署之间的性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM推理跟踪无法捕捉RAG特定的动态，导致性能差距。

Method: 收集来自一个服务于超过40,000名师生的大学范围内的问答系统的RAG工作负载跟踪数据，并进行统计分析。

Result: 现实世界的RAG工作负载表现出显著的时间局部性和高度倾斜的热门文档访问模式。

Conclusion: RAGPulse为研究人员提供了一个高保真基础，以开发和验证RAG系统的优化策略，从而提高RAG服务的效率和可靠性。

Abstract: Retrieval-Augmented Generation (RAG) is a critical paradigm for building reliable, knowledge-intensive Large Language Model (LLM) applications. However, the multi-stage pipeline (retrieve, generate) and unique workload characteristics (e.g., knowledge dependency) of RAG systems pose significant challenges for serving performance optimization. Existing generic LLM inference traces fail to capture these RAG-specific dynamics, creating a significant performance gap between academic research and real-world deployment. To bridge this gap, this paper introduces RAGPulse, an open-source RAG workload trace dataset. This dataset was collected from an university-wide Q&A system serving that has served more than 40,000 students and faculties since April 2024. We detail RAGPulse's system architecture, its privacy-preserving hash-based data format, and provide an in-depth statistical analysis. Our analysis reveals that real-world RAG workloads exhibit significant temporal locality and a highly skewed hot document access pattern. RAGPulse provides a high-fidelity foundation for researchers to develop and validate novel optimization strategies for RAG systems, such as content-aware batching and retrieval caching, ultimately enhancing the efficiency and reliability of RAG services. The code is available at https://github.com/flashserve/RAGPulse.

</details>


### [159] [LLM on a Budget: Active Knowledge Distillation for Efficient Classification of Large Text Corpora](https://arxiv.org/abs/2511.11574)
*Viviana Luccioli,Rithika Iyengar,Ryan Panley,Flora Haberkorn,Xiaoyu Ge,Leland Crane,Nitish Sinha,Seung Jung Lee*

Main category: cs.LG

TL;DR: 提出了一种新的主动学习算法M-RARU，用于降低知识蒸馏过程中LLM标注数据的成本，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型在分类任务中表现出色，但计算和经济成本阻碍了它们在动态环境中的大规模部署。知识蒸馏是一种解决方案，但为大型数据集进行蒸馏的成本仍然很高，因为它需要教师模型标注大量样本。

Method: 提出了一种名为M-RARU的新型主动学习算法，该算法结合不确定性和随机接受/拒绝机制，选择信息量最大的数据点供LLM教师模型使用。

Result: 实验表明，与随机抽样相比，该方法可减少高达80%的样本需求，同时显著提高分类精度，降低经济成本和整体训练时间。

Conclusion: M-RARU算法能够以较低的成本创建高效的学生模型，同时保持LLM的性能。

Abstract: Large Language Models (LLMs) are highly accurate in classification tasks, however, substantial computational and financial costs hinder their large-scale deployment in dynamic environments. Knowledge Distillation (KD) where a LLM "teacher" trains a smaller and more efficient "student" model, offers a promising solution to this problem. However, the distillation process itself often remains costly for large datasets, since it requires the teacher to label a vast number of samples while incurring significant token consumption. To alleviate this challenge, in this work we explore the active learning (AL) as a way to create efficient student models at a fraction of the cost while preserving the LLM's performance. In particular, we introduce M-RARU (Multi-class Randomized Accept/Reject Uncertainty Sampling), a novel AL algorithm that significantly reduces training costs. M-RARU employs an innovative strategy combining uncertainty with a randomized accept-reject mechanism to select only the most informative data points for the LLM teacher. This focused approach significantly minimizes required API calls and data processing time. We evaluate M-RARU against random sampling across five diverse student models (SVM, LDA, RF, GBDT, and DistilBERT) on multiple benchmark datasets. Experiments demonstrate that our proposed method achieves up to 80% reduction in sample requirements as compared to random sampling, substantially improving classification accuracy while reducing financial costs and overall training time.

</details>


### [160] [Detecting Statistically Significant Fairness Violations in Recidivism Forecasting Algorithms](https://arxiv.org/abs/2511.11575)
*Animesh Joshi*

Main category: cs.LG

TL;DR: 本研究提出了一种严格的框架，用于测试公平性违规行为的统计显着性，利用k-fold交叉验证来生成公平性指标的抽样分布。


<details>
  <summary>Details</summary>
Motivation: 算法决策在关键领域的日益普及激发了学术界对算法公平性的兴趣。现有的文献没有提供一种方法来评估观察到的群体之间的差异是否具有统计学意义，或者仅仅是由于偶然性。

Method: 利用k-fold交叉验证来生成公平性指标的抽样分布，从而测试公平性违规行为的统计显着性。

Result: 对累犯预测算法的测试表明，机器学习算法在几种公平性定义下，对黑人个体表现出具有统计意义的偏差，而在其他定义下，对白人个体没有偏差或有偏差。

Conclusion: 强调了在评估算法决策系统时进行严格和稳健的统计测试的重要性。

Abstract: Machine learning algorithms are increasingly deployed in critical domains such as finance, healthcare, and criminal justice [1]. The increasing popularity of algorithmic decision-making has stimulated interest in algorithmic fairness within the academic community. Researchers have introduced various fairness definitions that quantify disparities between privileged and protected groups, use causal inference to determine the impact of race on model predictions, and that test calibration of probability predictions from the model. Existing literature does not provide a way in which to assess whether observed disparities between groups are statistically significant or merely due to chance. This paper introduces a rigorous framework for testing the statistical significance of fairness violations by leveraging k-fold cross-validation [2] to generate sampling distributions of fairness metrics. This paper introduces statistical tests that can be used to identify statistically significant violations of fairness metrics based on disparities between predicted and actual outcomes, model calibration, and causal inference techniques [1]. We demonstrate this approach by testing recidivism forecasting algorithms trained on data from the National Institute of Justice. Our findings reveal that machine learning algorithms used for recidivism forecasting exhibit statistically significant bias against Black individuals under several fairness definitions, while also exhibiting no bias or bias against White individuals under other definitions. The results from this paper underscore the importance of rigorous and robust statistical testing while evaluating algorithmic decision-making systems.

</details>


### [161] [DAOpt: Modeling and Evaluation of Data-Driven Optimization under Uncertainty with LLMs](https://arxiv.org/abs/2511.11576)
*WenZhuo Zhu,Zheng Cui,Wenhan Lu,Sheng Liu,Yue Zhao*

Main category: cs.LG

TL;DR: 本研究探索了大型语言模型（LLM）在不确定性优化问题中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要集中于确定性优化，忽略了现实世界决策中的不确定性。

Method: 提出了DAOpt框架，包括OptU数据集、多智能体决策模块和仿真环境，并结合领域知识进行Few-shot学习。

Result: 着重于评估LLM的样本外可行性和鲁棒性。

Conclusion: 通过DAOpt框架提升LLM在不确定性优化问题中的建模能力。

Abstract: Recent advances in large language models (LLMs) have accelerated research on automated optimization modeling. While real-world decision-making is inherently uncertain, most existing work has focused on deterministic optimization with known parameters, leaving the application of LLMs in uncertain settings largely unexplored. To that end, we propose the DAOpt framework including a new dataset OptU, a multi-agent decision-making module, and a simulation environment for evaluating LLMs with a focus on out-of-sample feasibility and robustness. Additionally, we enhance LLMs' modeling capabilities by incorporating few-shot learning with domain knowledge from stochastic and robust optimization.

</details>


### [162] [Decoupling Positional and Symbolic Attention Behavior in Transformers](https://arxiv.org/abs/2511.11579)
*Felipe Urrutia,Jorge Salas,Alexander Kozachinskiy,Cristian Buc Calderon,Hector Pasten,Cristobal Rojas*

Main category: cs.LG

TL;DR: 本文深入研究了Transformer中注意力头的行为，特别是它们如何编码位置和符号信息，以及这与Rotary Positional Encoding (RoPE)中频率使用的关系。


<details>
  <summary>Details</summary>
Motivation: 理解语言需要独立编码句子中词语的位置和符号信息。RoPE是一种流行的位置编码方式，但其成功的原因尚不完全清楚。本文旨在更深入地研究RoPE中位置与符号信息的二分法。

Method: 本文从理论和实证层面分析了注意力头的行为，定义了位置性和符号性行为，证明了这两种行为的互斥性，并开发了一种量化它们的指标。然后，使用该框架分析了基于Transformer的大型语言模型。

Result: 研究发现，所有注意力头的行为与其频率使用之间存在很强的对应关系。通过控制注意力头可以访问的频率，可以控制Transformer的性能。

Conclusion: 本文详细阐述了RoPE的原理，以及其属性与模型行为之间的关系。

Abstract: An important aspect subtending language understanding and production is the ability to independently encode positional and symbolic information of the words within a sentence. In Transformers, positional information is typically encoded using Positional Encodings (PEs). One such popular PE, namely Rotary PE (RoPE), has been widely used due to its empirical success. Recently, it has been argued that part of RoPE's success emerges from its ability to encode robust positional and semantic information using large and small frequencies, respectively. In this work, we perform a deeper dive into the positional versus symbolic dichotomy of attention heads behavior, both at the theoretical and empirical level. We provide general definitions of what it means for a head to behave positionally or symbolically, prove that these are two mutually exclusive behaviors and develop a metric to quantify them. We apply our framework to analyze Transformer-based LLMs using RoPE and find that all heads exhibit a strong correspondence between behavior and frequency use. Finally, we introduce canonical tasks designed to be either purely positional or symbolic, and demonstrate that the Transformer performance causally relates to the ability of attention heads to leverage the appropriate frequencies. In particular, we show that we can control the Transformer performance by controlling which frequencies the attention heads can access. Altogether, our work provides a detailed understanding of RoPE, and how its properties relate to model behavior.

</details>


### [163] [The Anatomy of a Triton Attention Kernel](https://arxiv.org/abs/2511.11581)
*Burkhard Ringlein,Jan van Lunteren,Radu Stoica,Thomas Parnell*

Main category: cs.LG

TL;DR: 本文介绍了一种可移植、高效的跨平台LLM推理平台。


<details>
  <summary>Details</summary>
Motivation: 开发一种可在不同硬件架构上移植，无需底层手动调整，且仍能提供最佳效率的LLM推理平台是工业界和学术界的一个长期目标。

Method: 开发了一个最先进的分页注意力内核，该内核完全建立在特定领域的即时编译语言Triton之上，以在NVIDIA和AMD GPU上实现最先进的性能。

Result: 结果表明，一个通用的Triton注意力内核的性能从最先进水平的19.7%提高到105.9%。

Conclusion: 开源领域特定语言可以用于解锁不同GPU供应商之间的模型可移植性。

Abstract: A long-standing goal in both industry and academia is to develop an LLM inference platform that is portable across hardware architectures, eliminates the need for low-level hand-tuning, and still delivers best-in-class efficiency. In this work, we demonstrate that portable, efficient cross-platform LLM inference is indeed possible and share our experience. We develop a state-of-the-art paged attention kernel, the core performance-critical component of many LLM deployments, that builds exclusively on the domain-specific just-in-time compiled language Triton to achieve state-of-the-art performance on both NVIDIA and AMD GPUs. We describe our high-level approach, the key algorithmic and system-level improvements, the parameter auto-tuning required to unlock efficiency, and the integrations into a popular inference server that are necessary to bring the performance of a generic Triton attention kernel from 19.7% of the state-of-the-art to 105.9%. Our results highlight how open-source domain-specific languages can be leveraged to unlock model portability across different GPU vendors.

</details>


### [164] [Parallel and Multi-Stage Knowledge Graph Retrieval for Behaviorally Aligned Financial Asset Recommendations](https://arxiv.org/abs/2511.11583)
*Fernando Spadea,Oshani Seneviratne*

Main category: cs.LG

TL;DR: RAG-FLARKO: An enhanced financial recommendation framework using retrieval-augmented KGs to improve LLM performance.


<details>
  <summary>Details</summary>
Motivation: Context limits, hallucinations, and a lack of behavioral grounding in LLMs hinder personalized financial recommendations.

Method: Multi-stage KG retrieval to filter behaviorally relevant entities and temporally consistent market signals.

Result: Significantly enhanced recommendation quality, enabling smaller models to achieve high performance in profitability and behavioral alignment.

Conclusion: RAG-FLARKO offers a viable approach for deploying grounded financial AI in resource-constrained environments.

Abstract: Large language models (LLMs) show promise for personalized financial recommendations but are hampered by context limits, hallucinations, and a lack of behavioral grounding. Our prior work, FLARKO, embedded structured knowledge graphs (KGs) in LLM prompts to align advice with user behavior and market data. This paper introduces RAG-FLARKO, a retrieval-augmented extension to FLARKO, that overcomes scalability and relevance challenges using multi-stage and parallel KG retrieval processes. Our method first retrieves behaviorally relevant entities from a user's transaction KG and then uses this context to filter temporally consistent signals from a market KG, constructing a compact, grounded subgraph for the LLM. This pipeline reduces context overhead and sharpens the model's focus on relevant information. Empirical evaluation on a real-world financial transaction dataset demonstrates that RAG-FLARKO significantly enhances recommendation quality. Notably, our framework enables smaller, more efficient models to achieve high performance in both profitability and behavioral alignment, presenting a viable path for deploying grounded financial AI in resource-constrained environments.

</details>


### [165] [Output Supervision Can Obfuscate the Chain of Thought](https://arxiv.org/abs/2511.11584)
*Jacob Drori,Luke Marks,Bryce Woodworth,Alex Cloud,Alexander Matt Turner*

Main category: cs.LG

TL;DR: 即使没有直接针对CoT进行训练，模型仍然可能产生混淆的CoT，包含监控器无法检测到的不良行为。


<details>
  <summary>Details</summary>
Motivation: OpenAI (2025) 表明，针对思维链 (CoT) 监控器进行训练可能导致混淆的CoT。本文旨在研究仅针对输出监控器进行训练是否仍然会导致混淆的CoT。

Method: 通过两种机制进行论证：1) 模型泛化到使其CoT看起来安全；2) 安全的CoT可能增加安全输出的可能性。

Result: 提出了两种缓解措施，与常规训练相比，在可监控性和任务性能方面实现了帕累托改进。

Conclusion: 即使不直接针对CoT进行训练，模型仍然可能通过多种机制产生混淆的CoT。

Abstract: OpenAI (2025) showed that training against a chain of thought (CoT) monitor can cause obfuscated CoTs, which contain bad behavior the monitor cannot detect. They proposed to keep CoTs monitorable by training only against output monitors that do not have access to CoT. We show that such training can still cause obfuscated CoTs via two mechanisms. First, when a model is trained to produce a safe-looking output, that model may generalize to making its CoTs look safe. Second, since later tokens are conditioned on earlier ones, safe-looking CoTs may increase the likelihood of safe outputs, causing safe-looking CoTs to be reinforced. We introduce two mitigations to address these two issues, which achieve a Pareto improvement in terms of monitorability and task performance compared to regular training.

</details>


### [166] [Parameter-Efficient and Personalized Federated Training of Generative Models at the Edge](https://arxiv.org/abs/2511.11585)
*Kabir Khan,Manju Sarkar,Anita Kar,Suresh Ghosh*

Main category: cs.LG

TL;DR: FedGen-Edge: A framework for federated training of large generative models by decoupling a frozen global backbone from lightweight client-side adapters, federating only the adapters.


<details>
  <summary>Details</summary>
Motivation: Training large generative models in federated settings is challenging due to heavy computation, communication, and data heterogeneity.

Method: Proposes FedGen-Edge, which uses Low-Rank Adaptation (LoRA) to constrain client updates to a compact subspace.

Result: Achieves lower perplexity/FID and faster convergence than strong baselines on language modeling (PTB) and image generation (CIFAR-10), while reducing uplink traffic by more than 99 percent compared to full-model FedAvg.

Conclusion: FedGen-Edge offers a practical approach for privacy-preserving, resource-aware, and personalized generative AI on heterogeneous edge devices.

Abstract: Large generative models (for example, language and diffusion models) enable high-quality text and image synthesis but are hard to train or adapt in cross-device federated settings due to heavy computation and communication and statistical/system heterogeneity. We propose FedGen-Edge, a framework that decouples a frozen, pre-trained global backbone from lightweight client-side adapters and federates only the adapters. Using Low-Rank Adaptation (LoRA) constrains client updates to a compact subspace, which reduces uplink traffic by more than 99 percent versus full-model FedAvg, stabilizes aggregation under non-IID data, and naturally supports personalization because each client can keep a locally tuned adapter. On language modeling (PTB) and image generation (CIFAR-10), FedGen-Edge achieves lower perplexity/FID and faster convergence than strong baselines while retaining a simple FedAvg-style server. A brief ablation shows diminishing returns beyond moderate LoRA rank and a trade-off between local epochs and client drift. FedGen-Edge offers a practical path toward privacy-preserving, resource-aware, and personalized generative AI on heterogeneous edge devices.

</details>


### [167] [WildfireGenome: Interpretable Machine Learning Reveals Local Drivers of Wildfire Risk and Their Cross-County Variation](https://arxiv.org/abs/2511.11589)
*Chenyue Liu,Ali Mostafavi*

Main category: cs.LG

TL;DR: WildfireGenome combines wildfire indicators, machine learning, and explainable AI to provide interpretable, decision-scale risk assessments.


<details>
  <summary>Details</summary>
Motivation: Current wildfire risk assessments lack interpretability at the local level.

Method: Fusion of wildfire indicators, Random Forest classification, and SHAP/ICE/PDP analyses.

Result: Models achieve accuracies of 0.755-0.878 and Quadratic Weighted Kappa up to 0.951, with principal components explaining 87-94% of indicator variance. Needleleaf forest cover and elevation are key drivers.

Conclusion: WildfireGenome enables interpretable, decision-scale wildfire risk assessment for better vegetation management, zoning, and infrastructure planning.

Abstract: Current wildfire risk assessments rely on coarse hazard maps and opaque machine learning models that optimize regional accuracy while sacrificing interpretability at the decision scale. WildfireGenome addresses these gaps through three components: (1) fusion of seven federal wildfire indicators into a sign-aligned, PCA-based composite risk label at H3 Level-8 resolution; (2) Random Forest classification of local wildfire risk; and (3) SHAP and ICE/PDP analyses to expose county-specific nonlinear driver relationships. Across seven ecologically diverse U.S. counties, models achieve accuracies of 0.755-0.878 and Quadratic Weighted Kappa up to 0.951, with principal components explaining 87-94% of indicator variance. Transfer tests show reliable performance between ecologically similar regions but collapse across dissimilar contexts. Explanations consistently highlight needleleaf forest cover and elevation as dominant drivers, with risk rising sharply at 30-40% needleleaf coverage. WildfireGenome advances wildfire risk assessment from regional prediction to interpretable, decision-scale analytics that guide vegetation management, zoning, and infrastructure planning.

</details>


### [168] [Mind Your Entropy: From Maximum Entropy to Trajectory Entropy-Constrained RL](https://arxiv.org/abs/2511.11592)
*Guojian Zhan,Likun Wang,Pengcheng Wang,Feihong Zhang,Jingliang Duan,Masayoshi Tomizuka,Shengbo Eben Li*

Main category: cs.LG

TL;DR: 本文提出了一种轨迹熵约束强化学习 (TECRL) 框架，以解决最大熵强化学习中的两个瓶颈：非平稳 Q 值估计和短视的局部熵调整。


<details>
  <summary>Details</summary>
Motivation: 最大熵强化学习是平衡利用和探索的主流框架，但存在非平稳 Q 值估计和短视的局部熵调整这两个瓶颈限制了性能的进一步提高。

Method: 该框架首先分别学习两个 Q 函数，一个与奖励相关，另一个与熵相关，确保不受温度更新影响的干净且稳定的值目标。然后，专门的熵 Q 函数显式地量化了预期的累积熵，使我们能够实施轨迹熵约束，从而控制策略的长期随机性。在此 TECRL 框架的基础上，我们通过扩展最先进的分布式软 Actor-Critic 并进行三项改进 (DSAC-T)，开发了一种实用的离线策略算法 DSAC-E。

Result: 在 OpenAI Gym 基准测试上的实验结果表明，我们的 DSAC-E 可以实现更高的回报和更好的稳定性。

Conclusion: TECRL 框架和 DSAC-E 算法能够有效提高强化学习的性能和稳定性。

Abstract: Maximum entropy has become a mainstream off-policy reinforcement learning (RL) framework for balancing exploitation and exploration. However, two bottlenecks still limit further performance improvement: (1) non-stationary Q-value estimation caused by jointly injecting entropy and updating its weighting parameter, i.e., temperature; and (2) short-sighted local entropy tuning that adjusts temperature only according to the current single-step entropy, without considering the effect of cumulative entropy over time. In this paper, we extends maximum entropy framework by proposing a trajectory entropy-constrained reinforcement learning (TECRL) framework to address these two challenges. Within this framework, we first separately learn two Q-functions, one associated with reward and the other with entropy, ensuring clean and stable value targets unaffected by temperature updates. Then, the dedicated entropy Q-function, explicitly quantifying the expected cumulative entropy, enables us to enforce a trajectory entropy constraint and consequently control the policy long-term stochasticity. Building on this TECRL framework, we develop a practical off-policy algorithm, DSAC-E, by extending the state-of-the-art distributional soft actor-critic with three refinements (DSAC-T). Empirical results on the OpenAI Gym benchmark demonstrate that our DSAC-E can achieve higher returns and better stability.

</details>


### [169] [Sound Logical Explanations for Mean Aggregation Graph Neural Networks](https://arxiv.org/abs/2511.11593)
*Matthew Morris,Ian Horrocks*

Main category: cs.LG

TL;DR: 本文研究了使用平均聚合的图神经网络 (MAGNNs) 的可解释性和表达性，并提供了sound logical rules来解释预测。


<details>
  <summary>Details</summary>
Motivation: 尽管图神经网络（GNNs）常用于知识图补全，但对于使用平均作为聚合函数的GNNs，可解释性和表达性的研究仍然不足。

Method: 本文考虑了具有平均聚合和非负权重的GNN（MAGNNs），证明了可以适用于它们的单调规则的精确类别，并提供了一阶逻辑的受限片段来解释任何MAGNN预测。

Result: 实验表明，限制平均聚合GNN具有非负权重可以在标准归纳基准上产生相当或更好的性能，并且在实践中可以获得合理的规则，在实践中可以生成有洞察力的解释，并且合理的规则可以揭示训练模型中的问题。

Conclusion: 本文为使用平均聚合的图神经网络 (MAGNNs) 提供了可解释性和表达性分析，并通过实验验证了所提出方法的有效性，强调了非负权重在提高模型性能和可解释性方面的作用。

Abstract: Graph neural networks (GNNs) are frequently used for knowledge graph completion. Their black-box nature has motivated work that uses sound logical rules to explain predictions and characterise their expressivity. However, despite the prevalence of GNNs that use mean as an aggregation function, explainability and expressivity results are lacking for them. We consider GNNs with mean aggregation and non-negative weights (MAGNNs), proving the precise class of monotonic rules that can be sound for them, as well as providing a restricted fragment of first-order logic to explain any MAGNN prediction. Our experiments show that restricting mean-aggregation GNNs to have non-negative weights yields comparable or improved performance on standard inductive benchmarks, that sound rules are obtained in practice, that insightful explanations can be generated in practice, and that the sound rules can expose issues in the trained models.

</details>


### [170] [Loss Given Default Prediction Under Measurement-Induced Mixture Distributions: An Information-Theoretic Approach](https://arxiv.org/abs/2511.11596)
*Javier Marín*

Main category: cs.LG

TL;DR: LGD建模中，训练数据主要为基于破产前资产负债表的代理估计，而非实际破产清算结果，导致recursive partitioning methods失效。


<details>
  <summary>Details</summary>
Motivation: 实际破产恢复结果数据匮乏，LGD建模依赖代理估计数据。

Method: 采用基于香农熵和互信息的information-theoretic approaches。

Result: 在1980-2023年的公司破产数据上，R方为0.191，RMSE为0.284，好于Random Forest的负R方结果。杠杆率特征包含1.510 bits的互信息，而规模效应仅贡献0.086 bits。

Conclusion: 研究结果为在缺乏充分规模的代表性结果数据时，金融机构在Basel III要求下部署LGD模型提供了实践指导，并可推广到其他领域。

Abstract: Loss Given Default (LGD) modeling faces a fundamental data quality constraint: 90% of available training data consists of proxy estimates based on pre-distress balance sheets rather than actual recovery outcomes from completed bankruptcy proceedings. We demonstrate that this mixture-contaminated training structure causes systematic failure of recursive partitioning methods, with Random Forest achieving negative r-squared (-0.664, worse than predicting the mean) on held-out test data. Information-theoretic approaches based on Shannon entropy and mutual information provide superior generalization, achieving r-squared of 0.191 and RMSE of 0.284 on 1,218 corporate bankruptcies (1980-2023). Analysis reveals that leverage-based features contain 1.510 bits of mutual information while size effects contribute only 0.086 bits, contradicting regulatory assumptions about scale-dependent recovery. These results establish practical guidance for financial institutions deploying LGD models under Basel III requirements when representative outcome data is unavailable at sufficient scale. The findings generalize to medical outcomes research, climate forecasting, and technology reliability-domains where extended observation periods create unavoidable mixture structure in training data.

</details>


### [171] [Aspiration-based Perturbed Learning Automata in Games with Noisy Utility Measurements. Part A: Stochastic Stability in Non-zero-Sum Games](https://arxiv.org/abs/2511.11602)
*Georgios C. Chasparis*

Main category: cs.LG

TL;DR: 本文提出了一种新的基于收益的分布式优化学习方案，称为基于期望的扰动学习自动机 (APLA)。


<details>
  <summary>Details</summary>
Motivation: 在多人弱非循环博弈中，当每个参与者应用独立学习动态副本时，无法保证收敛到纯纳什均衡。

Method: 引入 APLA 动态，其中每个参与者选择动作的概率分布会因重复选择和一个捕获参与者满意度的期望因子而得到加强。通过建立诱导的无限维马尔可夫链与有限维马尔可夫链的等价性，提供了 APLA 在存在噪声观测的情况下，在多人正效用博弈中的随机稳定性分析。随机稳定性进一步专门用于弱非循环博弈。

Result: 对 APLA 在多人正效用博弈中的随机稳定性进行了分析。

Conclusion: 本文研究了 APLA 在多人博弈中的应用，并提供了随机稳定性分析。

Abstract: Reinforcement-based learning has attracted considerable attention both in modeling human behavior as well as in engineering, for designing measurement- or payoff-based optimization schemes. Such learning schemes exhibit several advantages, especially in relation to filtering out noisy observations. However, they may exhibit several limitations when applied in a distributed setup. In multi-player weakly-acyclic games, and when each player applies an independent copy of the learning dynamics, convergence to (usually desirable) pure Nash equilibria cannot be guaranteed. Prior work has only focused on a small class of games, namely potential and coordination games. To address this main limitation, this paper introduces a novel payoff-based learning scheme for distributed optimization, namely aspiration-based perturbed learning automata (APLA). In this class of dynamics, and contrary to standard reinforcement-based learning schemes, each player's probability distribution for selecting actions is reinforced both by repeated selection and an aspiration factor that captures the player's satisfaction level. We provide a stochastic stability analysis of APLA in multi-player positive-utility games under the presence of noisy observations. This is the first part of the paper that characterizes stochastic stability in generic non-zero-sum games by establishing equivalence of the induced infinite-dimensional Markov chain with a finite dimensional one. In the second part, stochastic stability is further specialized to weakly acyclic games.

</details>


### [172] [Enhancing failure prediction in nuclear industry: Hybridization of knowledge- and data-driven techniques](https://arxiv.org/abs/2511.11604)
*Amaratou Mahamadou Saley,Thierry Moyaux,Aïcha Sekhari,Vincent Cheutet,Jean-Baptiste Danielou*

Main category: cs.LG

TL;DR: 本研究提出了一种结合数据驱动技术与核设备领域知识的预测性维护方法，以提高核工业的安全性和经济效率。


<details>
  <summary>Details</summary>
Motivation: 在核工业中，精确预测资产的未来维护需求对于减少停机时间和运营成本至关重要。然而，由于系统复杂性，数据驱动方法的效果受到限制。

Method: 该方法结合了数据驱动技术与核设备领域知识。

Result: 实验结果表明，该混合方法在故障预测方面明显优于纯数据驱动方法，预测范围从3小时增加到24小时，F1值从56.36%提高到93.12%。

Conclusion: 该研究强调了纯数据驱动方法的局限性，并证明了领域知识在提高预测模型性能方面的重要性。

Abstract: The convergence of the Internet of Things (IoT) and Industry 4.0 has significantly enhanced data-driven methodologies within the nuclear industry, notably enhancing safety and economic efficiency. This advancement challenges the precise prediction of future maintenance needs for assets, which is crucial for reducing downtime and operational costs. However, the effectiveness of data-driven methodologies in the nuclear sector requires extensive domain knowledge due to the complexity of the systems involved. Thus, this paper proposes a novel predictive maintenance methodology that combines data-driven techniques with domain knowledge from a nuclear equipment. The methodological originality of this paper is located on two levels: highlighting the limitations of purely data-driven approaches and demonstrating the importance of knowledge in enhancing the performance of the predictive models. The applicative novelty of this work lies in its use within a domain such as a nuclear industry, which is highly restricted and ultrasensitive due to security, economic and environmental concerns. A detailed real-world case study which compares the current state of equipment monitoring with two scenarios, demonstrate that the methodology significantly outperforms purely data-driven methods in failure prediction. While purely data-driven methods achieve only a modest performance with a prediction horizon limited to 3 h and a F1 score of 56.36%, the hybrid approach increases the prediction horizon to 24 h and achieves a higher F1 score of 93.12%.

</details>


### [173] [Clustering-Based Weight Orthogonalization for Stabilizing Deep Reinforcement Learning](https://arxiv.org/abs/2511.11607)
*Guoqing Ma,Yuhan Zhang,Yuming Dai,Guangfu Hao,Yang Chen,Shan Yu*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的强化学习方法，用于解决非静态环境下的学习效率问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习智能体通常假设环境是静态的，但在非静态环境中，学习效率会大大降低，需要数百万次迭代。

Method: 论文引入了聚类正交权重修正（COWM）层，可以集成到任何强化学习算法的策略网络中，以减轻非静态性。

Result: COWM在基于视觉和基于状态的DMControl基准测试中分别实现了9%和12.6%的改进，优于现有方法。

Conclusion: COWM方法提高了学习速度，减少了梯度干扰，增强了整体学习效率，并在各种算法和任务中表现出鲁棒性和通用性。

Abstract: Reinforcement learning (RL) has made significant advancements, achieving superhuman performance in various tasks. However, RL agents often operate under the assumption of environmental stationarity, which poses a great challenge to learning efficiency since many environments are inherently non-stationary. This non-stationarity results in the requirement of millions of iterations, leading to low sample efficiency. To address this issue, we introduce the Clustering Orthogonal Weight Modified (COWM) layer, which can be integrated into the policy network of any RL algorithm and mitigate non-stationarity effectively. The COWM layer stabilizes the learning process by employing clustering techniques and a projection matrix. Our approach not only improves learning speed but also reduces gradient interference, thereby enhancing the overall learning efficiency. Empirically, the COWM outperforms state-of-the-art methods and achieves improvements of 9% and 12.6% in vision based and state-based DMControl benchmark. It also shows robustness and generality across various algorithms and tasks.

</details>


### [174] [Small Vocabularies, Big Gains: Pretraining and Tokenization in Time Series Models](https://arxiv.org/abs/2511.11622)
*Alexis Roger,Gwen Legate,Kashif Rasul,Yuriy Nevmyvaka,Irina Rish*

Main category: cs.LG

TL;DR: 本文系统地研究了时间序列预测中分词器设计（特别是缩放和量化策略）对模型性能的影响，以及预训练与随机初始化带来的影响。


<details>
  <summary>Details</summary>
Motivation: 研究分词器设计和迁移学习对时间序列预测模型性能的影响。

Method: 通过经验训练实验和理论分析相结合的方法。

Result: 预训练模型能更有效地利用精心设计的分词器，尤其是在较小的词汇量下。相反，错误的分词会降低甚至逆转预训练的优势。

Conclusion: 在时间序列建模中，仔细的分词非常重要。在多模态预测设置中，将小型、高效的词汇表与预训练权重相结合尤其有利，因为在多模态预测设置中，总体词汇表必须在模态之间共享。

Abstract: Tokenization and transfer learning are two critical components in building state of the art time series foundation models for forecasting. In this work, we systematically study the effect of tokenizer design, specifically scaling and quantization strategies, on model performance, alongside the impact of pretraining versus random initialization. We show that tokenizer configuration primarily governs the representational capacity and stability of the model, while transfer learning influences optimization efficiency and alignment. Using a combination of empirical training experiments and theoretical analyses, we demonstrate that pretrained models consistently leverage well-designed tokenizers more effectively, particularly at smaller vocabulary sizes. Conversely, misaligned tokenization can diminish or even invert the benefits of pretraining. These findings highlight the importance of careful tokenization in time series modeling and suggest that combining small, efficient vocabularies with pretrained weights is especially advantageous in multi-modal forecasting settings, where the overall vocabulary must be shared across modalities. Our results provide concrete guidance for designing tokenizers and leveraging transfer learning in discrete representation learning for continuous signals.

</details>


### [175] [Early GVHD Prediction in Liver Transplantation via Multi-Modal Deep Learning on Imbalanced EHR Data](https://arxiv.org/abs/2511.11623)
*Yushan Jiang,Shuteng Niu,Dongjin Song,Yichen Wang,Jingna Feng,Xinyue Hu,Liu Yang,Cui Tao*

Main category: cs.LG

TL;DR: 本文利用多模态深度学习方法，旨在提高肝移植术后GVHD的早期预测能力，从而为及时干预和改善患者预后铺平道路。


<details>
  <summary>Details</summary>
Motivation: 肝移植术后的移植物抗宿主病（GVHD）是一种罕见但通常是致命的并发症，死亡率非常高。

Method: 本文构建了一个多模态深度学习框架，该框架动态融合了患者人口统计学、实验室检查、诊断和药物四个主要模态，处理具有缺失值的不规则记录，并通过基于AUC的优化解决了极端的类别不平衡问题。

Result: 该框架优于所有单模态和多模态机器学习基线，AUC达到0.836，AUPRC达到0.157，召回率达到0.768，特异性达到0.803。

Conclusion: 所提出的多模态深度学习方法在肝移植术后GVHD的早期预测中显示出很有希望的结果，尽管面临着极不平衡的EHR数据的挑战。

Abstract: Graft-versus-host disease (GVHD) is a rare but often fatal complication in liver transplantation, with a very high mortality rate. By harnessing multi-modal deep learning methods to integrate heterogeneous and imbalanced electronic health records (EHR), we aim to advance early prediction of GVHD, paving the way for timely intervention and improved patient outcomes. In this study, we analyzed pre-transplant electronic health records (EHR) spanning the period before surgery for 2,100 liver transplantation patients, including 42 cases of graft-versus-host disease (GVHD), from a cohort treated at Mayo Clinic between 1992 and 2025. The dataset comprised four major modalities: patient demographics, laboratory tests, diagnoses, and medications. We developed a multi-modal deep learning framework that dynamically fuses these modalities, handles irregular records with missing values, and addresses extreme class imbalance through AUC-based optimization. The developed framework outperforms all single-modal and multi-modal machine learning baselines, achieving an AUC of 0.836, an AUPRC of 0.157, a recall of 0.768, and a specificity of 0.803. It also demonstrates the effectiveness of our approach in capturing complementary information from different modalities, leading to improved performance. Our multi-modal deep learning framework substantially improves existing approaches for early GVHD prediction. By effectively addressing the challenges of heterogeneity and extreme class imbalance in real-world EHR, it achieves accurate early prediction. Our proposed multi-modal deep learning method demonstrates promising results for early prediction of a GVHD in liver transplantation, despite the challenge of extremely imbalanced EHR data.

</details>


### [176] [MedFedPure: A Medical Federated Framework with MAE-based Detection and Diffusion Purification for Inference-Time Attacks](https://arxiv.org/abs/2511.11625)
*Mohammad Karami,Mohammad Reza Nemati,Aidin Kazemi,Ali Mikaeili Barzili,Hamid Azadegan,Behzad Moshiri*

Main category: cs.LG

TL;DR: 提出了一种名为 MedFedPure 的个性化联邦学习防御框架，旨在保护诊断 AI 模型在推理时免受对抗攻击，同时不影响隐私或准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的防御方法通常假设集中式数据，并且难以应对联邦医疗环境中分散和多样化的性质。对抗性攻击会微妙地改变医学扫描，人眼无法察觉，但足以误导 AI 模型，可能导致严重的误诊。

Method: MedFedPure 结合了三个关键要素：(1) 适应每个机构独特数据分布的个性化 FL 模型；(2) 通过暴露隐藏扰动来检测可疑输入的 Masked Autoencoder (MAE)；(3) 自适应扩散净化模块，选择性地清理标记的扫描，然后再进行分类。

Result: 在 Br35H 脑 MRI 数据集上评估了 MedFedPure。结果表明，对抗鲁棒性方面取得了显著的提高，在强攻击下的性能从 49.50% 提高到 87.33%，同时保持了 97.67% 的高清洁准确率。

Conclusion: MedFedPure 框架通过在诊断期间本地和实时运行，为在临床工作流程中部署安全、可信和保护隐私的 AI 工具提供了一条切实可行的途径。

Abstract: Artificial intelligence (AI) has shown great potential in medical imaging, particularly for brain tumor detection using Magnetic Resonance Imaging (MRI). However, the models remain vulnerable at inference time when they are trained collaboratively through Federated Learning (FL), an approach adopted to protect patient privacy. Adversarial attacks can subtly alter medical scans in ways invisible to the human eye yet powerful enough to mislead AI models, potentially causing serious misdiagnoses. Existing defenses often assume centralized data and struggle to cope with the decentralized and diverse nature of federated medical settings. In this work, we present MedFedPure, a personalized federated learning defense framework designed to protect diagnostic AI models at inference time without compromising privacy or accuracy. MedFedPure combines three key elements: (1) a personalized FL model that adapts to the unique data distribution of each institution; (2) a Masked Autoencoder (MAE) that detects suspicious inputs by exposing hidden perturbations; and (3) an adaptive diffusion-based purification module that selectively cleans only the flagged scans before classification. Together, these steps offer robust protection while preserving the integrity of normal, benign images. We evaluated MedFedPure on the Br35H brain MRI dataset. The results show a significant gain in adversarial robustness, improving performance from 49.50% to 87.33% under strong attacks, while maintaining a high clean accuracy of 97.67%. By operating locally and in real time during diagnosis, our framework provides a practical path to deploying secure, trustworthy, and privacy-preserving AI tools in clinical workflows.
  Index Terms: cancer, tumor detection, federated learning, masked autoencoder, diffusion, privacy

</details>


### [177] [SA-EMO: Structure-Aligned Encoder Mixture of Operators for Generalizable Full-waveform Inversion](https://arxiv.org/abs/2511.11627)
*Wang Zhenyu,Li Peiyuan,Shi Yongxiang,Wu Ruoyu,Zhang Lei*

Main category: cs.LG

TL;DR: 提出了一种用于速度场反演的结构对齐编码器-混合算子 (SA-EMO) 架构，该架构在未知地下结构下表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习和数值加速方法泛化性差，无法区分不同的地质类型。

Method: SA-EMO 架构，包含结构对齐编码器和混合神经算子专家。

Result: 在 OpenFWI 基准和 Marmousi2 数据集上，SA-EMO 优于传统 CNN 或单算子方法，平均 MAE 降低约 58.443%，边界分辨率提高约 10.308%。

Conclusion: SA-EMO 为高效、可扩展和物理可解释的全波形反演引入了一种新范例。

Abstract: Full-waveform inversion (FWI) can produce high-resolution subsurface models, yet it remains inherently ill-posed, highly nonlinear, and computationally intensive. Although recent deep learning and numerical acceleration methods have improved speed and scalability, they often rely on single CNN architectures or single neural operators, which struggle to generalize in unknown or complex geological settings and are ineffective at distinguishing diverse geological types. To address these issues, we propose a Structure-Aligned Encoder-Mixture-of-Operators (SA-EMO) architecture for velocity-field inversion under unknown subsurface structures. First, a structure-aligned encoder maps high-dimensional seismic wavefields into a physically consistent latent space, thereby eliminating spatio-temporal mismatch between the waveform and velocity domains, recovering high-frequency components, and enhancing feature generalization. Then, an adaptive routing mechanism selects and fuses multiple neural-operator experts, including spectral, wavelet, multiscale, and local operators, to predict the velocity model. We systematically evaluate our approach on the OpenFWI benchmark and the Marmousi2 dataset. Results show that SA-EMO significantly outperforms traditional CNN or single-operator methods, achieving an average MAE reduction of approximately 58.443% and an improvement in boundary resolution of about 10.308%. Ablation studies further reveal that the structure-aligned encoder, the expert-fusion mechanism, and the routing module each contribute markedly to the performance gains. This work introduces a new paradigm for efficient, scalable, and physically interpretable full-waveform inversion.

</details>


### [178] [Global Feature Enhancing and Fusion Framework for Strain Gauge Time Series Classification](https://arxiv.org/abs/2511.11629)
*Xu Zhang,Peng Wang,Chen Wang,Zhe Xu,Xiaohua Nie,Wei Wang*

Main category: cs.LG

TL;DR: 提出了一种基于超图的全局特征学习和融合框架，以提高应变计状态识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 仅局部特征可能不足以表达时间序列，尤其是在不同时间序列之间的局部子序列非常相似时。卷积神经网络（CNN）在提取全局特征方面存在局限性。

Method: 1) 通过特征工程构建全局特征。2) 学习局部特征之间的高阶关系以捕获全局特征。提出了一种基于超图的全局特征学习和融合框架。

Result: 在工业SGS和公共UCR数据集上验证了该方法，表明其对SGS识别中未见数据具有更好的泛化能力。

Conclusion: 该方法通过学习和融合全局特征以实现语义一致性，从而增强了SGS时间序列的表示，提高了识别精度。

Abstract: Strain Gauge Status (SGS) recognition is crucial in the field of intelligent manufacturing based on the Internet of Things, as accurate identification helps timely detection of failed mechanical components, avoiding accidents. The loading and unloading sequences generated by strain gauges can be identified through time series classification (TSC) algorithms. Recently, deep learning models, e.g., convolutional neural networks (CNNs) have shown remarkable success in the TSC task, as they can extract discriminative local features from the subsequences to identify the time series. However, we observe that only the local features may not be sufficient for expressing the time series, especially when the local sub-sequences between different time series are very similar, e.g., SGS data of aircraft wings in static strength experiments. Nevertheless, CNNs suffer from the limitation in extracting global features due to the nature of convolution operations. For extracting global features to more comprehensively represent the SGS time series, we propose two insights: (i) Constructing global features through feature engineering. (ii) Learning high-order relationships between local features to capture global features. To realize and utilize them, we propose a hypergraph-based global feature learning and fusion framework, which learns and fuses global features for semantic consistency to enhance the representation of SGS time series, thereby improving recognition accuracy. Our method designs are validated on industrial SGS and public UCR datasets, showing better generalization for unseen data in SGS recognition.

</details>


### [179] [Predicting Grain Growth in Polycrystalline Materials Using Deep Learning Time Series Models](https://arxiv.org/abs/2511.11630)
*Eliane Younes,Elie Hachem,Marc Bernacki*

Main category: cs.LG

TL;DR: 本研究利用深度学习方法预测晶粒长大过程中晶粒尺寸的分布，重点关注循环神经网络（RNN）、长短期记忆网络（LSTM）、时间卷积网络（TCN）和Transformer。


<details>
  <summary>Details</summary>
Motivation: 预测晶粒长大是微观结构工程中的关键目标，因为晶粒长大强烈影响材料的力学性能。

Method: 使用从高保真模拟中提取的平均场统计描述符，将120个晶粒长大序列的数据集处理成随时间变化的归一化晶粒尺寸分布。使用递归预测策略训练模型以从短时间历史预测未来分布。

Result: 在测试的模型中，LSTM网络实现了最高的精度（高于90%）和最稳定的性能，在扩展的时间范围内保持了物理上一致的预测，同时将每个序列的计算时间从大约20分钟减少到几秒钟。

Conclusion: 低维描述符和基于LSTM的预测具有高效、准确的微观结构预测潜力，对数字孪生开发和过程优化具有直接影响。

Abstract: Grain Growth strongly influences the mechanical behavior of materials, making its prediction a key objective in microstructural engineering. In this study, several deep learning approaches were evaluated, including recurrent neural networks (RNN), long short-term memory (LSTM), temporal convolutional networks (TCN), and transformers, to forecast grain size distributions during grain growth. Unlike full-field simulations, which are computationally demanding, the present work relies on mean-field statistical descriptors extracted from high-fidelity simulations. A dataset of 120 grain growth sequences was processed into normalized grain size distributions as a function of time. The models were trained to predict future distributions from a short temporal history using a recursive forecasting strategy. Among the tested models, the LSTM network achieved the highest accuracy (above 90\%) and the most stable performance, maintaining physically consistent predictions over extended horizons while reducing computation time from about 20 minutes per sequence to only a few seconds, whereas the other architectures tended to diverge when forecasting further in time. These results highlight the potential of low-dimensional descriptors and LSTM-based forecasting for efficient and accurate microstructure prediction, with direct implications for digital twin development and process optimization.

</details>


### [180] [Toward Better Generalization in Few-Shot Learning through the Meta-Component Combination](https://arxiv.org/abs/2511.11632)
*Qiuhao Zeng*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的基于度量的元学习算法，通过学习元组件的组合来改进泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基于度量的元学习方法容易对已见过的类别过拟合，导致在新类别上的泛化能力不足。

Method: 该方法将每个分类器学习为元组件的组合，并通过正交正则化器来解耦元组件，促进其多样性，并捕获不同分类器之间共享的各种子结构。

Result: 在few-shot benchmark任务上的大量实验表明，该方法具有优越的性能。

Conclusion: 该方法通过学习元组件的组合，提高了在few-shot学习中的泛化能力。

Abstract: In few-shot learning, classifiers are expected to generalize to unseen classes given only a small number of instances of each new class. One of the popular solutions to few-shot learning is metric-based meta-learning. However, it highly depends on the deep metric learned on seen classes, which may overfit to seen classes and fail to generalize well on unseen classes. To improve the generalization, we explore the substructures of classifiers and propose a novel meta-learning algorithm to learn each classifier as a combination of meta-components. Meta-components are learned across meta-learning episodes on seen classes and disentangled by imposing an orthogonal regularizer to promote its diversity and capture various shared substructures among different classifiers. Extensive experiments on few-shot benchmark tasks show superior performances of the proposed method.

</details>


### [181] [A Deep Learning Model to Predicting Changes in Consumer Attributes for New Line-extended Products](https://arxiv.org/abs/2511.11646)
*Li Yinxing,Tsukasa Ishigaki*

Main category: cs.LG

TL;DR: 本文提出了一种新的深度学习模型，用于预测新产品线扩展对消费者属性的影响。


<details>
  <summary>Details</summary>
Motivation: 过度的产品线扩展会破坏品牌形象，因此需要了解消费者对新产品线扩展产品的关键属性。

Method: 使用条件表格变分自编码器（CTVAE）生成消费者和产品的大规模表格数据的合成数据。

Result: 实验结果表明，CTVAE 提供了优于现有模型的预测性能。

Conclusion: 该方法有助于避免同类相食，并设计产品形象和营销策略。

Abstract: Product line extension is a marketing strategy that enhances a company's sphere of influence. Because excessive line extensions disrupt brand image, only appropriate line extensions based on consumer needs are desirable. Marketers should know the key consumer attributes of the primary customers for new line-extended products before companies enter the market. This paper describes a method for predicting changes in consumer attributes for new line-extended products using a novel deep learning model. The proposed model, Conditional Tabular Variational Auto-Encoder (CTVAE), generates synthetic data from large-scale tabular data of consumers and products. It can provide various implications about effective product line marketing for marketers. The experimental results demonstrate that the CTVAE offers superior prediction performance than existing models. We indicate implications for new products that change containers or flavors for effective product line marketing. The proposed approach has the potential to contribute to avoiding cannibalization and to designing product images and marketing strategies.

</details>


### [182] [An Explainable and Fair AI Tool for PCOS Risk Assessment: Calibration, Subgroup Equity, and Interactive Clinical Deployment](https://arxiv.org/abs/2511.11636)
*Asma Sadia Khan,Sadia Tabassum*

Main category: cs.LG

TL;DR: 本研究提出了一个公平且可解释的机器学习框架，用于预测多囊卵巢综合征（PCOS），旨在评估模型性能并识别患者亚组间的诊断差异。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估模型在预测PCOS方面的性能，并发现不同患者群体之间的诊断差异，为临床提供更可靠的风险评估工具。

Method: 该框架整合了基于SHAP的特征属性和人口统计审计，将预测解释与观察到的差异联系起来。使用随机森林、SVM和XGBoost模型，并采用等渗和Platt缩放进行校准和公平性比较。

Result: 校准后的随机森林模型实现了90.8%的高预测准确率。SHAP分析确定卵泡计数、体重增加和月经不规律是最具影响力的特征。亚组分析显示，模型在25-35岁女性中表现最佳，但在25岁以下女性中表现不佳，突出了与年龄相关的差异。

Conclusion: 该研究开发了一个基于Streamlit的Web界面，可以进行实时PCOS风险评估和交互式分析，弥合了AI研究与临床可用性之间的差距。

Abstract: This paper presents a fairness-audited and interpretable machine learning framework for predicting polycystic ovary syndrome (PCOS), designed to evaluate model performance and identify diagnostic disparities across patient subgroups. The framework integrated SHAP-based feature attributions with demographic audits to connect predictive explanations with observed disparities for actionable insights. Probabilistic calibration metrics (Brier Score and Expected Calibration Error) are incorporated to ensure reliable risk predictions across subgroups. Random Forest, SVM, and XGBoost models were trained with isotonic and Platt scaling for calibration and fairness comparison. A calibrated Random Forest achieved a high predictive accuracy of 90.8%. SHAP analysis identified follicle count, weight gain, and menstrual irregularity as the most influential features, which are consistent with the Rotterdam diagnostic criteria. Although the SVM with isotonic calibration achieved the lowest calibration error (ECE = 0.0541), the Random Forest model provided a better balance between calibration and interpretability (Brier = 0.0678, ECE = 0.0666). Therefore, it was selected for detailed fairness and SHAP analyses. Subgroup analysis revealed that the model performed best among women aged 25-35 (accuracy 90.9%) but underperformed in those under 25 (69.2%), highlighting age-related disparities. The model achieved perfect precision in obese women and maintained high recall in lean PCOS cases, demonstrating robustness across phenotypes. Finally, a Streamlit-based web interface enables real-time PCOS risk assessment, Rotterdam criteria evaluation, and interactive 'what-if' analysis, bridging the gap between AI research and clinical usability.

</details>


### [183] [Enhancing PINN Accuracy for the RLW Equation: Adaptive and Conservative Approaches](https://arxiv.org/abs/2511.11638)
*Aamir Shehzad*

Main category: cs.LG

TL;DR: 本文研究了用改进的PINN方法求解正则化长波（RLW）方程，发现自适应PINN在解决复杂非线性问题（如孤子碰撞）上优于保守PINN和标准PINN，而保守PINN在解决单孤子长期行为和undular bores问题上更优。研究强调，显式执行守恒定律可能不利于高度非线性方程组的优化，并为PINN设计提供了指导。


<details>
  <summary>Details</summary>
Motivation: 传统PINN在求解正则化长波方程时误差率较高。

Method: 开发了两种改进的PINN方法：自适应损失权重方法和强制执行显式守恒律的保守方法。

Result: 自适应PINN在解决复杂非线性相互作用问题上表现更好，而保守方法在解决单孤子和undular bores的长期行为问题上表现更好。显式执行守恒定律可能对高度非线性系统的优化有害。改进的PINN方法结果与既有数值解的误差在$O(10^{-5})$范围内。

Conclusion: PINN可以为复杂偏微分方程组提供精确解，且无需离散空间或时间。研究结果挑战了守恒定律执行总能提高PINN性能的假设，并为研究人员设计PINN以解决特定类型问题提供了指导。

Abstract: Standard physics-informed neural network implementations have produced large error rates when using these models to solve the regularized long wave (RLW) equation. Two improved PINN approaches were developed in this research: an adaptive approach with self-adaptive loss weighting and a conservative approach enforcing explicit conservation laws. Three benchmark tests were used to demonstrate how effective PINN's are as they relate to the type of problem being solved (i.e., time dependent RLW equation). The first was a single soliton traveling along a line (propagation), the second was the interaction between two solitons, and the third was the evolution of an undular bore over the course of $t=250$. The results demonstrated that the effectiveness of PINNs are problem specific. The adaptive PINN was significantly better than both the conservative PINN and the standard PINN at solving problems involving complex nonlinear interactions such as colliding two solitons. The conservative approach was significantly better at solving problems involving long term behavior of single solitons and undular bores. However, the most important finding from this research is that explicitly enforcing conservation laws may be harmful to optimizing the solution of highly nonlinear systems of equations and therefore requires special training methods. The results from our adaptive and conservative approaches were within $O(10^{-5})$ of established numerical solutions for the same problem, thus demonstrating that PINNs can provide accurate solutions to complex systems of partial differential equations without the need for a discretization of space or time (mesh free). Moreover, the finding from this research challenges the assumptions that conservation enforcement will always improve the performance of a PINN and provides researchers with guidelines for designing PINNs for use on specific types of problems.

</details>


### [184] [EcoSpa: Efficient Transformer Training with Coupled Sparsity](https://arxiv.org/abs/2511.11641)
*Jinqi Xiao,Cheng Luo,Lingyi Huang,Cheng Yang,Yang Sui,Huy Phan,Xiao Zang,Yibiao Ying,Zhexiang Tang,Anima Anandkumar,Bo Yuan*

Main category: cs.LG

TL;DR: EcoSpa: An efficient structured sparse training method for Transformers that preserves weight matrix relationships.


<details>
  <summary>Details</summary>
Motivation: Existing sparse training methods for Transformers fail to preserve structural relationships between weight matrices, leading to performance degradation at high sparsity levels.

Method: EcoSpa jointly evaluates and sparsifies coupled weight matrix pairs, preserving their interaction patterns through aligned row/column removal. It introduces a new granularity for calibrating structural component importance and performs coupled estimation and sparsification across both pre-training and fine-tuning scenarios.

Result: EcoSpa enables efficient training of LLaMA-1B with 50% memory reduction and 21% faster training, achieves 2.2x model compression on GPT-2-Medium with 2.4 lower perplexity, and delivers 1.6x inference speedup.

Conclusion: EcoSpa uses standard PyTorch operations, requiring no custom hardware or kernels, making efficient transformer training accessible on commodity hardware.

Abstract: Transformers have become the backbone of modern AI, yet their high computational demands pose critical system challenges. While sparse training offers efficiency gains, existing methods fail to preserve critical structural relationships between weight matrices that interact multiplicatively in attention and feed-forward layers. This oversight leads to performance degradation at high sparsity levels. We introduce EcoSpa, an efficient structured sparse training method that jointly evaluates and sparsifies coupled weight matrix pairs, preserving their interaction patterns through aligned row/column removal. EcoSpa introduces a new granularity for calibrating structural component importance and performs coupled estimation and sparsification across both pre-training and fine-tuning scenarios. Evaluations demonstrate substantial improvements: EcoSpa enables efficient training of LLaMA-1B with 50\% memory reduction and 21\% faster training, achieves $2.2\times$ model compression on GPT-2-Medium with $2.4$ lower perplexity, and delivers $1.6\times$ inference speedup. The approach uses standard PyTorch operations, requiring no custom hardware or kernels, making efficient transformer training accessible on commodity hardware.

</details>


### [185] [AIF: Asynchronous Inference Framework for Cost-Effective Pre-Ranking](https://arxiv.org/abs/2511.12934)
*Zhi Kou,Xiang-Rong Sheng,Shuguang Han,Zhishan Zhao,Yueyao Cheng,Han Zhu,Jian Xu,Bo Zheng*

Main category: cs.LG

TL;DR: 提出了异步推理框架（AIF）以解决工业推荐系统中预排序模型的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 传统的预排序模型存在冗余计算和延迟问题，限制了模型容量和系统效率。

Method: AIF将用户侧和物品侧的计算与实时预测解耦，并行计算用户侧特征，近线计算物品侧特征，并采用近似方法处理交互依赖组件。

Result: AIF在计算和延迟成本没有显著增加的情况下，实现了显著的性能提升。

Conclusion: 通过框架和模型的协同设计，AIF成功部署在淘宝展示广告系统中。

Abstract: In industrial recommendation systems, pre-ranking models based on deep neural networks (DNNs) commonly adopt a sequential execution framework: feature fetching and model forward computation are triggered only after receiving candidates from the upstream retrieval stage. This design introduces inherent bottlenecks, including redundant computations of identical users/items and increased latency due to strictly sequential operations, which jointly constrain the model's capacity and system efficiency. To address these limitations, we propose the Asynchronous Inference Framework (AIF), a cost-effective computational architecture that decouples interaction-independent components, those operating within a single user or item, from real-time prediction. AIF reorganizes the model inference process by performing user-side computations in parallel with the retrieval stage and conducting item-side computations in a nearline manner. This means that interaction-independent components are calculated just once and completed before the real-time prediction phase of the pre-ranking stage. As a result, AIF enhances computational efficiency and reduces latency, freeing up resources to significantly improve the feature set and model architecture of interaction-independent components. Moreover, we delve into model design within the AIF framework, employing approximated methods for interaction-dependent components in online real-time predictions. By co-designing both the framework and the model, our solution achieves notable performance gains without significantly increasing computational and latency costs. This has enabled the successful deployment of AIF in the Taobao display advertising system.

</details>


### [186] [Environment-Aware Transfer Reinforcement Learning for Sustainable Beam Selection](https://arxiv.org/abs/2511.11647)
*Dariush Salami,Ramin Hashemi,Parham Kazemi,Mikko A. Uusitalo*

Main category: cs.LG

TL;DR: 提出了一种基于迁移学习和强化学习的波束选择方法，以减少训练时间和计算资源消耗，提高能源效率。


<details>
  <summary>Details</summary>
Motivation: 传统的基于强化学习的波束选择模型需要大量的训练时间和计算资源，尤其是在具有不同传播特性的多样化环境中部署时，对可扩展性和能源效率构成重大挑战。

Method: 将环境建模为点云，通过计算点云之间的Chamfer距离来识别结构相似的环境，从而通过迁移学习重用预训练的模型。

Result: 与传统方法相比，训练时间减少了16倍，计算开销也显著降低，同时保持了高性能。

Conclusion: 该方法能够显著降低功耗，支持无线系统中绿色可持续的人工智能发展，并加速部署时间，减少与训练相关的碳排放。

Abstract: This paper presents a novel and sustainable approach for improving beam selection in 5G and beyond networks using transfer learning and Reinforcement Learning (RL). Traditional RL-based beam selection models require extensive training time and computational resources, particularly when deployed in diverse environments with varying propagation characteristics posing a major challenge for scalability and energy efficiency. To address this, we propose modeling the environment as a point cloud, where each point represents the locations of gNodeBs (gNBs) and surrounding scatterers. By computing the Chamfer distance between point clouds, structurally similar environments can be efficiently identified, enabling the reuse of pre-trained models through transfer learning. This methodology leads to a 16x reduction in training time and computational overhead, directly contributing to energy efficiency. By minimizing the need for retraining in each new deployment, our approach significantly lowers power consumption and supports the development of green and sustainable Artificial Intelligence (AI) in wireless systems. Furthermore, it accelerates time-to-deployment, reduces carbon emissions associated with training, and enhances the viability of deploying AI-driven communication systems at the edge. Simulation results confirm that our approach maintains high performance while drastically cutting energy costs, demonstrating the potential of transfer learning to enable scalable, adaptive, and environmentally conscious RL-based beam selection strategies in dynamic and diverse propagation environments.

</details>


### [187] [Lightweight Time Series Data Valuation on Time Series Foundation Models via In-Context Finetuning](https://arxiv.org/abs/2511.11648)
*Shunyu Wu,Tianyue Li,Yixuan Leng,Jingyi Suo,Jian Lou,Dan Li,See-Kiong Ng*

Main category: cs.LG

TL;DR: 提出了一种轻量级时间序列数据估值方法LTSV，用于时间序列基础模型（TSFM）。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型依赖于高质量的时间序列数据，传统数据估值方法计算量大且忽略时间依赖性。

Method: 通过上下文微调近似影响函数，并引入时间块聚合以捕捉时间依赖性。

Result: 实验表明，LTSV在保持计算效率的同时，提供了可靠且强大的估值性能。

Conclusion: 基于时间序列基础模型的上下文微调为时间序列学习中的数据归因和模型泛化之间架起了一座桥梁。

Abstract: Time series foundation models (TSFMs) have demonstrated increasing capabilities due to their extensive pretraining on large volumes of diverse time series data. Consequently, the quality of time series data is crucial to TSFM performance, rendering an accurate and efficient data valuation of time series for TSFMs indispensable. However, traditional data valuation methods, such as influence functions, face severe computational bottlenecks due to their poor scalability with growing TSFM model sizes and often fail to preserve temporal dependencies. In this paper, we propose LTSV, a Lightweight Time Series Valuation on TSFMS via in-context finetuning. Grounded in the theoretical evidence that in-context finetuning approximates the influence function, LTSV estimates a sample's contribution by measuring the change in context loss after in-context finetuning, leveraging the strong generalization capabilities of TSFMs to produce robust and transferable data valuations. To capture temporal dependencies, we introduce temporal block aggregation, which integrates per-block influence scores across overlapping time windows. Experiments across multiple time series datasets and models demonstrate that LTSV consistently provides reliable and strong valuation performance, while maintaining manageable computational requirements. Our results suggest that in-context finetuning on time series foundation models provides a practical and effective bridge between data attribution and model generalization in time series learning.

</details>


### [188] [Enhanced Water Leak Detection with Convolutional Neural Networks and One-Class Support Vector Machine](https://arxiv.org/abs/2511.11650)
*Daniele Ugo Leonzio,Paolo Bestagini,Marco Marcon,Stefano Tubaro*

Main category: cs.LG

TL;DR: 本文提出了一种新的基于数据驱动的漏水检测方法，该方法仅利用水分布网络（WDN）的拓扑结构和无泄漏情况下的压力数据，通过特征提取器和单类支持向量机（SVM）进行训练，将泄漏检测为异常。


<details>
  <summary>Details</summary>
Motivation: 解决水分布网络（WDN）中因泄漏导致大量水资源损失的问题，需要可靠有效的泄漏检测和定位系统。数据驱动方法因其卓越的性能而受到越来越多的关注。

Method: 提出了一种完全数据驱动的解决方案，该方案仅使用WDN拓扑结构的知识和在没有泄漏的情况下获得的一系列压力数据采集。该解决方案基于特征提取器和在无泄漏数据上训练的单类支持向量机（SVM），以便将泄漏检测为异常。

Result: 在Modena WDN的模拟数据集上取得的结果表明，所提出的解决方案优于最近的泄漏检测方法。

Conclusion: 本文提出了一种新的数据驱动的泄漏检测方法，并在模拟数据集上验证了其有效性，结果表明该方法优于现有的泄漏检测方法。

Abstract: Water is a critical resource that must be managed efficiently. However, a substantial amount of water is lost each year due to leaks in Water Distribution Networks (WDNs). This underscores the need for reliable and effective leak detection and localization systems. In recent years, various solutions have been proposed, with data-driven approaches gaining increasing attention due to their superior performance. In this paper, we propose a new method for leak detection. The method is based on water pressure measurements acquired at a series of nodes of a WDN. Our technique is a fully data-driven solution that makes only use of the knowledge of the WDN topology, and a series of pressure data acquisitions obtained in absence of leaks. The proposed solution is based on an feature extractor and a one-class Support Vector Machines (SVM) trained on no-leak data, so that leaks are detected as anomalies. The results achieved on a simulate dataset using the Modena WDN demonstrate that the proposed solution outperforms recent methods for leak detection.

</details>


### [189] [Incomplete Depression Feature Selection with Missing EEG Channels](https://arxiv.org/abs/2511.11651)
*Zhijian Gong,Wenjia Dong,Xueyuan Xu,Fulin Wei,Chunyu Liu,Li Zhuo*

Main category: cs.LG

TL;DR: 提出了一种新的特征选择方法，用于稳健的抑郁症分析，称为不完整抑郁症特征选择与缺失脑电通道（IDFS-MEC）。


<details>
  <summary>Details</summary>
Motivation: 脑电特征通常包含冗余、不相关和噪声信息。此外，现实世界的脑电数据采集经常面临挑战，例如电极分离导致的数据丢失和严重的噪声干扰。

Method: IDFS-MEC将缺失通道指示器信息和自适应通道加权学习集成到正交回归中，以减少不完整通道对模型构建的影响，然后利用全局冗余最小化学习来减少所选特征子集中的冗余信息。

Result: 在MODMA和PRED-d003数据集上进行的大量实验表明，IDFS-MEC选择的脑电特征子集在3通道、64通道和128通道设置中比10种流行的特征选择方法具有更优越的性能。

Conclusion: IDFS-MEC是一种有效的抑郁症分析特征选择方法

Abstract: As a critical mental health disorder, depression has severe effects on both human physical and mental well-being. Recent developments in EEG-based depression analysis have shown promise in improving depression detection accuracies. However, EEG features often contain redundant, irrelevant, and noisy information. Additionally, real-world EEG data acquisition frequently faces challenges, such as data loss from electrode detachment and heavy noise interference. To tackle the challenges, we propose a novel feature selection approach for robust depression analysis, called Incomplete Depression Feature Selection with Missing EEG Channels (IDFS-MEC). IDFS-MEC integrates missing-channel indicator information and adaptive channel weighting learning into orthogonal regression to lessen the effects of incomplete channels on model construction, and then utilizes global redundancy minimization learning to reduce redundant information among selected feature subsets. Extensive experiments conducted on MODMA and PRED-d003 datasets reveal that the EEG feature subsets chosen by IDFS-MEC have superior performance than 10 popular feature selection methods among 3-, 64-, and 128-channel settings.

</details>


### [190] [How many stations are sufficient? Exploring the effect of urban weather station density reduction on imputation accuracy of air temperature and humidity](https://arxiv.org/abs/2511.11652)
*Marvin Plein,Carsten F. Dormann,Andreas Christen*

Main category: cs.LG

TL;DR: 该研究提出了一种逐步移除气象站的方法，以减少城市气象站网络(WSN)的密度，同时保持对气温和湿度模式的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 城市气象站网络维护成本高、劳动强度大，因此需要优化现有网络。

Method: 通过逐步移除德国弗莱堡现有WSN中的站点，并分析WSN子集重现原始WSN气温和湿度模式的能力。

Result: 在保持较高预测准确性的前提下，大幅减少站点数量是可行的。例如，从42个站点减少到4个站点，气温和相对湿度的均方根误差(RMSE)仅分别增加20%和16%。

Conclusion: 该研究表明，精简WSN具有最大限度地提高城市气候研究中财政和人事相关资源有效配置的潜力。

Abstract: Urban weather station networks (WSNs) are widely used to monitor urban weather and climate patterns and aid urban planning. However, maintaining WSNs is expensive and labor-intensive. Here, we present a step-wise station removal procedure to thin an existing WSN in Freiburg, Germany, and analyze the ability of WSN subsets to reproduce air temperature and humidity patterns of the entire original WSN for a year following a simulated reduction of WSN density. We found that substantial reductions in station numbers after one year of full deployment are possible while retaining high predictive accuracy. A reduction from 42 to 4 stations, for instance, increased mean prediction RMSEs from 0.69 K to 0.83 K for air temperature and from 3.8% to 4.4% for relative humidity, corresponding to RMSE increases of only 20% and 16%, respectively. Predictive accuracy is worse for remote stations in forests than for stations in built-up or open settings, but consistently better than a state-of-the-art numerical urban land-surface model (Surface Urban Energy and Water Balance Scheme). Stations located at the edges between built-up and rural areas are most valuable when reconstructing city-wide climate characteristics. Our study demonstrates the potential of thinning WSNs to maximize the efficient allocation of financial and personnel-related resources in urban climate research.

</details>


### [191] [Convergence of Multiagent Learning Systems for Traffic control](https://arxiv.org/abs/2511.11654)
*Sayambhu Sen,Shalabh Bhatnagar*

Main category: cs.LG

TL;DR: 本文着重于多智能体强化学习 (MARL) 交通信号控制 (TSC) 算法的理论基础，填补了先前工作的空白。


<details>
  <summary>Details</summary>
Motivation: 城市交通拥堵严重，需要高效的交通信号控制。先前的研究已经证明了多智能体强化学习是一种有前途的策略，但缺乏对其稳定性和收敛性的严格理论分析。

Method: 利用随机逼近方法，正式分析了学习动态。

Result: 证明了用于交通控制的特定多智能体强化学习算法在给定条件下收敛。

Conclusion: 该研究扩展了异步值迭代的单智能体收敛性证明。

Abstract: Rapid urbanization in cities like Bangalore has led to severe traffic congestion, making efficient Traffic Signal Control (TSC) essential. Multi-Agent Reinforcement Learning (MARL), often modeling each traffic signal as an independent agent using Q-learning, has emerged as a promising strategy to reduce average commuter delays. While prior work Prashant L A et. al has empirically demonstrated the effectiveness of this approach, a rigorous theoretical analysis of its stability and convergence properties in the context of traffic control has not been explored. This paper bridges that gap by focusing squarely on the theoretical basis of this multi-agent algorithm. We investigate the convergence problem inherent in using independent learners for the cooperative TSC task. Utilizing stochastic approximation methods, we formally analyze the learning dynamics. The primary contribution of this work is the proof that the specific multi-agent reinforcement learning algorithm for traffic control is proven to converge under the given conditions extending it from single agent convergence proofs for asynchronous value iteration.

</details>


### [192] [On the Probabilistic Learnability of Compact Neural Network Preimage Bounds](https://arxiv.org/abs/2511.11656)
*Luca Marzari,Manuele Bicego,Ferdinando Cicalese,Alessandro Farinelli*

Main category: cs.LG

TL;DR: 提出了一种基于随机森林的神经网络前像近似计算方法，可在保证高置信度和有界误差的情况下，解决精确求解器无法扩展的问题。


<details>
  <summary>Details</summary>
Motivation: 现有计算神经网络前像界限的方法在可扩展性上存在根本限制，因为问题是#P-hard的。

Method: 利用随机决策树集成生成满足期望输出属性的候选输入区域，并通过主动重采样进行优化，该方法名为RF-ProVe。

Result: 该方法在区域纯度和全局覆盖率上提供正式的统计保证，为计算紧凑的前像近似提供了一种实用的、可扩展的解决方案。

Conclusion: RF-ProVe方法在精确求解器失效的情况下，为计算紧凑的前像近似提供了一种可扩展的解决方案，并提供了正式的统计保证。

Abstract: Although recent provable methods have been developed to compute preimage bounds for neural networks, their scalability is fundamentally limited by the #P-hardness of the problem. In this work, we adopt a novel probabilistic perspective, aiming to deliver solutions with high-confidence guarantees and bounded error. To this end, we investigate the potential of bootstrap-based and randomized approaches that are capable of capturing complex patterns in high-dimensional spaces, including input regions where a given output property holds. In detail, we introduce $\textbf{R}$andom $\textbf{F}$orest $\textbf{Pro}$perty $\textbf{Ve}$rifier ($\texttt{RF-ProVe}$), a method that exploits an ensemble of randomized decision trees to generate candidate input regions satisfying a desired output property and refines them through active resampling. Our theoretical derivations offer formal statistical guarantees on region purity and global coverage, providing a practical, scalable solution for computing compact preimage approximations in cases where exact solvers fail to scale.

</details>


### [193] [SpecQuant: Spectral Decomposition and Adaptive Truncation for Ultra-Low-Bit LLMs Quantization](https://arxiv.org/abs/2511.11663)
*Zhixiong Zhao,Fangxin Liu,Junjie Wang,Chenyang Guan,Zongwu Wang,Li Jiang,Haibing Guan*

Main category: cs.LG

TL;DR: SpecQuant提出了一种新的LLM量化框架，通过在频域中处理激活和权重，实现了极低的比特量化，同时保持了较高的准确率和效率。


<details>
  <summary>Details</summary>
Motivation: 为了在终端设备上高效部署，需要对大型语言模型（LLM）进行压缩，尤其是通过极低比特量化激活和权重。

Method: SpecQuant是一个两阶段框架，首先平滑激活异常值并将其转移到权重矩阵中，然后应用通道级的低频傅里叶截断来抑制高频分量。在推理过程中，引入了一个轻量级的截断模块，用于调整截断阈值。

Result: 在LLaMA-3 8B模型上，SpecQuant实现了权重和激活的4比特量化，零样本准确率差距仅为1.5%，同时推理速度提高2倍，内存使用降低3倍。

Conclusion: SpecQuant能够在极低比特量化LLM的同时，保持较高的准确率和效率，为终端设备上的LLM部署提供了新的解决方案。

Abstract: The emergence of accurate open large language models (LLMs) has sparked a push for advanced quantization techniques to enable efficient deployment on end-user devices. In this paper, we revisit the challenge of extreme LLM compression -- targeting ultra-low-bit quantization for both activations and weights -- from a Fourier frequency domain perspective. We propose SpecQuant, a two-stage framework that tackles activation outliers and cross-channel variance. In the first stage, activation outliers are smoothed and transferred into the weight matrix to simplify downstream quantization. In the second stage, we apply channel-wise low-frequency Fourier truncation to suppress high-frequency components while preserving essential signal energy, improving quantization robustness. Our method builds on the principle that most of the weight energy is concentrated in low-frequency components, which can be retained with minimal impact on model accuracy. To enable runtime adaptability, we introduce a lightweight truncation module during inference that adjusts truncation thresholds based on channel characteristics. On LLaMA-3 8B, SpecQuant achieves 4-bit quantization for both weights and activations, narrowing the zero-shot accuracy gap to only 1.5% compared to full precision, while delivering 2 times faster inference and 3times lower memory usage.

</details>


### [194] [Clifford Algebraic Rotor Embeddings : Maybe embeddings should start to CARE](https://arxiv.org/abs/2511.11665)
*Sameeksha Sriram,Ayush Paliwal,Alexander S. Ecker,Chase van de Geijn*

Main category: cs.LG

TL;DR: 提出了一种基于四元数和克利福德代数的旋转位置嵌入方法，以推广RoPE到更高维度，并保持其移位等方性。


<details>
  <summary>Details</summary>
Motivation: 现有的RoPE扩展方法（如Spherical RoPE）不具备移位等方性，并且旋转顺序不明确。

Method: 使用四元数来参数化旋转轴，并推广到克利福德代数，提出了Quaternion Rotary Embeddings (QuatRo) 和 Clifford Algebraic Rotary Embeddings (CARE)。

Result: 证明了Mixed RoPE和Spherical RoPE是QuatRo的特例，并通过实验比较了spherical, quaternion, 和 Clifford-based rotary embeddings。

Conclusion: 该方法能够将旋转嵌入推广到任意维度，并在多重向量中编码位置信息。

Abstract: Rotary Positional Embeddings (RoPE) have demonstrated exceptional performance as a positional encoding method, consistently outperforming their baselines. While recent work has sought to extend RoPE to higher-dimensional inputs, many such extensions are non-commutative, thereby forfeiting RoPE's shift-equivariance property. Spherical RoPE is one such non-commutative variant, motivated by the idea of rotating embedding vectors on spheres rather than circles. However, spherical rotations are inherently non-commutative, making the choice of rotation sequence ambiguous. In this work, we explore a quaternion-based approach -- Quaternion Rotary Embeddings (QuatRo) -- in place of Euler angles, leveraging quaternions' ability to represent 3D rotations to parameterize the axes of rotation. We show Mixed RoPE and Spherical RoPE to be special cases of QuatRo. Further, we propose a generalization of QuatRo to Clifford Algebraic Rotary Embeddings (CARE) using geometric algebra. Viewing quaternions as the even subalgebra of Cl(3,0,0), we extend the notion of rotary embeddings from quaternions to Clifford rotors acting on multivectors. This formulation enables two key generalizations: (1) extending rotary embeddings to arbitrary dimensions, and (2) encoding positional information in multivectors of multiple grades, not just vectors. We present preliminary experiments comparing spherical, quaternion, and Clifford-based rotary embeddings.

</details>


### [195] [Adaptive Stepsizing for Stochastic Gradient Langevin Dynamics in Bayesian Neural Networks](https://arxiv.org/abs/2511.11666)
*Rajit Rajpal,Benedict Leimkuhler,Yuanhao Jiang*

Main category: cs.LG

TL;DR: 提出了一种新的自适应SGLD方法（SA-SGLD），它通过时间重缩放来调节步长，从而提高了BNN中后验分布采样的准确性和稳定性，且没有引入偏差。


<details>
  <summary>Details</summary>
Motivation: 现有的SGMCMC方法对步长的选择非常敏感，而自适应变体（如pSGLD）通常无法在不添加昂贵散度校正项的情况下对正确的 invariant measure 进行采样。

Method: 构建在最近提出的`SamAdams' timestep adaptation框架之上，引入了一种自适应方案：SA-SGLD，它采用时间重缩放来根据监控量（通常是局部梯度范数）来调节步长。

Result: 在具有高曲率的2D玩具示例和使用 sharp priors 的 BNN 的图像分类中，该方法可以实现比 SGLD 更准确的后验采样。

Conclusion: SA-SGLD可以自动缩小高曲率区域的步长，并扩大较平坦区域的步长，从而提高稳定性和混合性，且不引入偏差

Abstract: Bayesian neural networks (BNNs) require scalable sampling algorithms to approximate posterior distributions over parameters. Existing stochastic gradient Markov Chain Monte Carlo (SGMCMC) methods are highly sensitive to the choice of stepsize and adaptive variants such as pSGLD typically fail to sample the correct invariant measure without addition of a costly divergence correction term. In this work, we build on the recently proposed `SamAdams' framework for timestep adaptation (Leimkuhler, Lohmann, and Whalley 2025), introducing an adaptive scheme: SA-SGLD, which employs time rescaling to modulate the stepsize according to a monitored quantity (typically the local gradient norm). SA-SGLD can automatically shrink stepsizes in regions of high curvature and expand them in flatter regions, improving both stability and mixing without introducing bias. We show that our method can achieve more accurate posterior sampling than SGLD on high-curvature 2D toy examples and in image classification with BNNs using sharp priors.

</details>


### [196] [Beyond Superficial Forgetting: Thorough Unlearning through Knowledge Density Estimation and Block Re-insertion](https://arxiv.org/abs/2511.11667)
*Feng Guo,Yuntao Wen,Shen Gao,Junshuo Zhang,Shuo Shang*

Main category: cs.LG

TL;DR: 提出了一种新的 machine unlearning 方法 KUnBR，通过知识密度引导和块重插入来实现对 LLM 中有害知识的彻底删除，同时保持模型效用。


<details>
  <summary>Details</summary>
Motivation: 现有的 machine unlearning 方法难以彻底删除有害知识，导致残留，从而引发隐私、合规和伦理问题。

Method: 首先识别包含丰富有害知识的层，然后通过重插入策略彻底消除有害知识。引入知识密度估计来量化和定位包含最多有害知识的层，并设计层重插入策略，将富含有害知识的层提取并重新插入到原始 LLM 中，绕过覆盖层造成的梯度阻塞，确保在 unlearning 期间的有效梯度传播。

Result: 在多个 unlearning 和通用能力基准测试中进行的广泛实验表明，KUnBR 实现了最先进的遗忘性能，同时保持了模型效用。

Conclusion: KUnBR 是一种有效的 machine unlearning 方法，可以在删除有害知识的同时保持模型效用。

Abstract: Machine unlearning, which selectively removes harmful knowledge from a pre-trained model without retraining from scratch, is crucial for addressing privacy, regulatory compliance, and ethical concerns in Large Language Models (LLMs). However, existing unlearning methods often struggle to thoroughly remove harmful knowledge, leaving residual harmful knowledge that can be easily recovered. To address these limitations, we propose Knowledge Density-Guided Unlearning via Blocks Reinsertion (KUnBR), a novel approach that first identifies layers with rich harmful knowledge and then thoroughly eliminates the harmful knowledge via re-insertion strategy. Our method introduces knowledge density estimation to quantify and locate layers containing the most harmful knowledge, enabling precise unlearning. Additionally, we design a layer re-insertion strategy that extracts and re-inserts harmful knowledge-rich layers into the original LLM, bypassing gradient obstruction caused by cover layers and ensuring effective gradient propagation during unlearning. Extensive experiments conducted on several unlearning and general capability benchmarks demonstrate that KUnBR achieves state-of-the-art forgetting performance while maintaining model utility.

</details>


### [197] [Do traveling waves make good positional encodings?](https://arxiv.org/abs/2511.11668)
*Chase van de Geijn,Ayush Paliwal,Timo Lüddecke,Alexander S. Ecker*

Main category: cs.LG

TL;DR: 提出一种新的位置编码机制RollPE，它基于行波，通过对自注意力中的查询和键张量应用循环滚动操作来实现。


<details>
  <summary>Details</summary>
Motivation: Transformer依赖于位置编码来补偿自注意力的固有置换不变性。传统方法使用绝对正弦嵌入或学习的位置向量，而最近的方法强调相对编码，以更好地捕捉平移等方差。

Method: 对自注意力中的查询和键张量应用循环滚动操作，从而在位置之间产生相对的相位移动，从而使模型能够将注意力计算为位置差异的函数，而不是绝对索引。

Result: 该方法明显优于传统的绝对位置嵌入，并且与RoPE相当。

Conclusion: 从行波的角度观察RollPE可以简化RoPE，并将其与大脑中的信息流过程联系起来。

Abstract: Transformers rely on positional encoding to compensate for the inherent permutation invariance of self-attention. Traditional approaches use absolute sinusoidal embeddings or learned positional vectors, while more recent methods emphasize relative encodings to better capture translation equivariances. In this work, we propose RollPE, a novel positional encoding mechanism based on traveling waves, implemented by applying a circular roll operation to the query and key tensors in self-attention. This operation induces a relative shift in phase across positions, allowing the model to compute attention as a function of positional differences rather than absolute indices. We show this simple method significantly outperforms traditional absolute positional embeddings and is comparable to RoPE. We derive a continuous case of RollPE which implicitly imposes a topographic structure on the query and key space. We further derive a mathematical equivalence of RollPE to a particular configuration of RoPE. Viewing RollPE through the lens of traveling waves may allow us to simplify RoPE and relate it to processes of information flow in the brain.

</details>


### [198] [H-Model: Dynamic Neural Architectures for Adaptive Processing](https://arxiv.org/abs/2511.11669)
*Dmytro Hospodarchuk*

Main category: cs.LG

TL;DR: 本文介绍了一种能够根据输入数据动态调整其内部结构的神经网络架构。


<details>
  <summary>Details</summary>
Motivation: 探索设计一种新型的、可解释的自适应神经网络。

Method: 提出一种路由机制，允许每一层影响其输出在网络中的传播方式，从而实现迭代和自适应计算。

Result: 初步实验结果显示出潜力。

Conclusion: 该架构有潜力学习表征和计算结构本身，但需要在更有利的计算条件下进行进一步评估。

Abstract: This article explores the design and experimentation of a neural network architecture capable of dynamically adjusting its internal structure based on the input data. The proposed model introduces a routing mechanism that allows each layer to influence how its outputs are propagated through the network, enabling iterative and adaptive computation. This concept is loosely inspired by the idea of thought processes and dynamic reasoning, where information flow is conditioned not only on the data itself, but also on the internal state of the system.
  It is important to note that this work does not aim to compete with state-of-the-art language models in terms of performance. Instead, it presents a conceptual prototype-an architectural framework that opens up a new direction for exploring adaptable and potentially more interpretable networks. The goal is not optimization of existing benchmarks but rather the proposal of a system that can learn not only representations, but also the structure of computation itself.
  Due to practical constraints in computing resources and data, this study remains a preliminary investigation. Nevertheless, initial observations show promise, and the architecture's full potential can only be evaluated in future experiments under more favorable computational conditions.

</details>


### [199] [Evaluation of LLM-based Explanations for a Learning Analytics Dashboard](https://arxiv.org/abs/2511.11671)
*Alina Deriyeva,Benjamin Paassen*

Main category: cs.LG

TL;DR: 本研究探讨了使用大型语言模型（LLM）为学习分析仪表板生成口头解释，以提高其有效性并促进元认知技能的发展。


<details>
  <summary>Details</summary>
Motivation: 学习分析仪表板可以支持数字学习环境中的自我调节学习，但其有效性受到数据可解释性的影响。

Method: 使用大型语言模型为仪表板中的数据生成口头解释，并与独立仪表板和人类教师提供的解释进行比较，进行专家研究（N=12）。

Result: 基于LLM的技能状态解释和学习建议比其他条件更受欢迎。

Conclusion: 使用LLM进行解释可以增强学习者的学习体验，同时保持教师认可的教学标准。

Abstract: Learning Analytics Dashboards can be a powerful tool to support self-regulated learning in Digital Learning Environments and promote development of meta-cognitive skills, such as reflection. However, their effectiveness can be affected by the interpretability of the data they provide. To assist in the interpretation, we employ a large language model to generate verbal explanations of the data in the dashboard and evaluate it against a standalone dashboard and explanations provided by human teachers in an expert study with university level educators (N=12). We find that the LLM-based explanations of the skill state presented in the dashboard, as well as general recommendations on how to proceed with learning within the course are significantly more favored compared to the other conditions. This indicates that using LLMs for interpretation purposes can enhance the learning experience for learners while maintaining the pedagogical standards approved by teachers.

</details>


### [200] [Synergistic Feature Fusion for Latent Lyrical Classification: A Gated Deep Learning Architecture](https://arxiv.org/abs/2511.11673)
*M. A. Gameiro*

Main category: cs.LG

TL;DR: 提出了一种用于歌词内容分类的新型协同融合层（SFL）架构，该架构优于随机森林基线。


<details>
  <summary>Details</summary>
Motivation: 将复杂、高维的深度语义特征与简单、可解释的结构线索相集成，用于歌词内容分类。

Method: 利用门控机制调制Sentence-BERT嵌入（Fdeep），使用低维辅助特征（Fstruct）。将任务重新定义为二元分类，区分主要的同质集群（Class 0）与所有其他内容（Class 1）。

Result: SFL模型的准确率为0.9894，宏F1分数为0.9894，优于使用特征连接的综合随机森林（RF）基线（准确率= 0.9868）。SFL模型展示了卓越的可靠性和校准性，预期校准误差（ECE = 0.0035）降低了93％，对数损失（0.0304）比RF基线低2.5倍（ECE = 0.0500；对数损失= 0.0772）。

Conclusion: 非线性门控优于简单的特征连接，从而将SFL模型确立为用于复杂多模态抒情分析的鲁棒且值得信赖的系统。

Abstract: This study addresses the challenge of integrating complex, high-dimensional deep semantic features with simple, interpretable structural cues for lyrical content classification. We introduce a novel Synergistic Fusion Layer (SFL) architecture, a deep learning model utilizing a gated mechanism to modulate Sentence-BERT embeddings (Fdeep) using low-dimensional auxiliary features (Fstruct). The task, derived from clustering UMAP-reduced lyrical embeddings, is reframed as binary classification, distinguishing a dominant, homogeneous cluster (Class 0) from all other content (Class 1). The SFL model achieved an accuracy of 0.9894 and a Macro F1 score of 0.9894, outperforming a comprehensive Random Forest (RF) baseline that used feature concatenation (Accuracy = 0.9868). Crucially, the SFL model demonstrated vastly superior reliability and calibration, exhibiting a 93% reduction in Expected Calibration Error (ECE = 0.0035) and a 2.5x lower Log Loss (0.0304) compared to the RF baseline (ECE = 0.0500; Log Loss = 0.0772). This performance validates the architectural hypothesis that non-linear gating is superior to simple feature concatenation, establishing the SFL model as a robust and trustworthy system for complex multimodal lyrical analysis.

</details>
