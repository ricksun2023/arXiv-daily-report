<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 46]
- [cs.CV](#cs.CV) [Total: 47]
- [cs.AI](#cs.AI) [Total: 45]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.IR](#cs.IR) [Total: 8]
- [cs.LG](#cs.LG) [Total: 45]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [StreetMath: Study of LLMs' Approximation Behaviors](https://arxiv.org/abs/2510.25776)
*Chiung-Yi Tseng,Somshubhra Roy,Maisha Thasin,Danyang Zhang,Blessing Effiong*

Main category: cs.CL

TL;DR: 该研究填补了大型语言模型（LLM）在非自回归解码器模型中，对快速、非正式数学运算进行近似推理能力的研究空白。


<details>
  <summary>Details</summary>
Motivation: 现有研究较少关注大型语言模型在快速、非正式数学运算中的近似推理能力，尤其是在非自回归解码器模型中。

Method: 引入StreetMath基准来评估模型在真实近似场景下的近似能力，并在不同LLM架构上进行广泛评估，应用可解释性技术来探测其内部计算状态。

Result: LLM通常尝试计算精确值或调用外部工具，即使在需要近似的任务中也是如此。模型在早期层或步骤中有时会得到正确答案，但解决近似任务时仍然消耗更多token。精确和近似算术运算依赖于很大程度上独立的神经组件。

Conclusion: LLM在街头数学环境中没有表现出像人类那样的认知吝啬。

Abstract: There is a substantial body of literature examining the mathematical
reasoning capabilities of large language models (LLMs), particularly their
performance on precise arithmetic operations in autoregressive architectures.
However, their ability to perform approximate reasoning in informal, fast-paced
mathematical operations has received far less attention, especially among
non-autoregressive decoder models. Our work addresses this gap by introducing
StreetMath, a benchmark designed to evaluate models' approximation abilities
under real-world approximation scenarios. We conduct extensive evaluations
across different LLM architectures: Qwen3-4B-Instruct-2507,
Qwen3-4B-Thinking-2507, Dream-v0-Instruct-7B, Falcon-Mamba-7B-Instruct, and
Mamba-GPT-3B. Furthermore, we apply mechanistic interpretability techniques to
probe their internal computational states. Our analysis reveals that LLMs
generally attempt to compute exact values or invoke external tools even in
tasks that call for approximation. Moreover, while models sometimes reach the
correct answer in early layers or steps, they still consume more tokens when
solving approximation tasks. Additional experiments indicate that exact and
approximate arithmetic operations rely on largely separate neural components.
Drawing upon research on cognitive psychology, we argue that LLMs do not
exhibit cognitive miserliness in the same way humans do in street math
settings. We open source our work https://github.com/ctseng777/StreetMath

</details>


### [2] [Review Based Entity Ranking using Fuzzy Logic Algorithmic Approach: Analysis](https://arxiv.org/abs/2510.25778)
*Pratik N. Kalamkar,Anupama G. Phakatkar*

Main category: cs.CL

TL;DR: 本文提出了一种基于词典的意见挖掘方法，该方法考虑了意见的强度，通过模糊逻辑算法和句法依赖关系分析，将意见词分类到不同的粒度级别，并根据这些粒度级别对实体进行排名。


<details>
  <summary>Details</summary>
Motivation: 现有的基于词典的整体方法没有考虑到每个意见的强度。

Method: 结合意见词（副词、形容词、名词和动词），通过模糊逻辑算法和句法依赖关系分析，将实体评论和用户查询分类到不同的粒度级别（非常弱、弱、中等、非常强和强）。

Result: 根据评论中与特定方面相关的意见词来计算该方面的实体得分。

Conclusion: 根据实体评论和用户查询的倾向和强度对实体进行排名。

Abstract: Opinion mining, also called sentiment analysis, is the field of study that
analyzes people opinions, sentiments, evaluations, appraisals, attitudes, and
emotions towards entities such as products, services, organizations,
individuals, issues, events, topics, and their attributes. Holistic
lexicon-based approach does not consider the strength of each opinion, i.e.,
whether the opinion is very strongly negative (or positive), strongly negative
(or positive), moderate negative (or positive), very weakly negative (or
positive) and weakly negative (or positive). In this paper, we propose approach
to rank entities based on orientation and strength of the entity reviews and
user's queries by classifying them in granularity levels (i.e. very weak, weak,
moderate, very strong and strong) by combining opinion words (i.e. adverb,
adjective, noun and verb) that are related to aspect of interest of certain
product. We shall use fuzzy logic algorithmic approach in order to classify
opinion words into different category and syntactic dependency resolution to
find relations for desired aspect words. Opinion words related to certain
aspects of interest are considered to find the entity score for that aspect in
the review.

</details>


### [3] [LASTIST: LArge-Scale Target-Independent STance dataset](https://arxiv.org/abs/2510.25783)
*DongJae Kim,Yaejin Lee,Minsu Park,Eunil Park*

Main category: cs.CL

TL;DR: 提出了一个大规模的韩语立场检测数据集LASTIST，用于填补低资源语言立场检测研究的空白。


<details>
  <summary>Details</summary>
Motivation: 当前立场检测研究主要集中在目标依赖的立场检测任务上，且缺乏韩语等低资源语言的数据集。

Method: 构建了包含563,299个标记韩语句子的LASTIST数据集，并使用深度学习和立场检测模型进行训练。

Result: 构建了LASTIST数据集，适用于目标独立立场检测和历时演变立场检测等任务。

Conclusion: 该研究旨在通过LASTIST数据集推动韩语等低资源语言的立场检测研究。

Abstract: Stance detection has emerged as an area of research in the field of
artificial intelligence. However, most research is currently centered on the
target-dependent stance detection task, which is based on a person's stance in
favor of or against a specific target. Furthermore, most benchmark datasets are
based on English, making it difficult to develop models in low-resource
languages such as Korean, especially for an emerging field such as stance
detection. This study proposes the LArge-Scale Target-Independent STance
(LASTIST) dataset to fill this research gap. Collected from the press releases
of both parties on Korean political parties, the LASTIST dataset uses 563,299
labeled Korean sentences. We provide a detailed description of how we collected
and constructed the dataset and trained state-of-the-art deep learning and
stance detection models. Our LASTIST dataset is designed for various tasks in
stance detection, including target-independent stance detection and diachronic
evolution stance detection. We deploy our dataset on
https://anonymous.4open.science/r/LASTIST-3721/.

</details>


### [4] [zFLoRA: Zero-Latency Fused Low-Rank Adapters](https://arxiv.org/abs/2510.25784)
*Dhananjaya Gowda,Seoha Song,Harshith Goka,Junhyun Lee*

Main category: cs.CL

TL;DR: 提出了一种名为 zFLoRA 的新型低秩适配器，它在基础模型之上引入了零或可忽略的延迟开销。


<details>
  <summary>Details</summary>
Motivation: 当大型语言模型 (LLM) 越来越多地部署带有特定于任务的适配器时，与这些数量不多的适配器参数相关的额外计算量在推理时会变得非常重要（高达基础模型的 2.5 倍）。

Method: 提出一种新的零延迟融合低秩适配器 (zFLoRA)。

Result: 在 1B、3B 和 7B 大小的 LLM 上的实验结果表明，zFLoRA 与流行的监督微调基准（包括低秩适配器 (LoRA) 以及完全微调 (FFT)）相比，具有优势。在常识推理、数学推理和总结对话三个不同类别中的 18 个不同任务上进行了实验。在 NPU (Samsung Galaxy S25+) 以及 GPU (NVIDIA H100) 平台上进行的延迟测量表明，所提出的 zFLoRA 适配器引入了零到可忽略的延迟开销。

Conclusion: 提出的 zFLoRA 适配器引入了零到可忽略的延迟开销。

Abstract: Large language models (LLMs) are increasingly deployed with task-specific
adapters catering to multiple downstream applications. In such a scenario, the
additional compute associated with these apparently insignificant number of
adapter parameters (typically less than 1% of the base model) turns out to be
disproportionately significant during inference time (upto 2.5x times that of
the base model). In this paper, we propose a new zero-latency fused low-rank
adapter (zFLoRA) that introduces zero or negligible latency overhead on top of
the base model. Experimental results on LLMs of size 1B, 3B and 7B show that
zFLoRA compares favorably against the popular supervised fine-tuning benchmarks
including low-rank adapters (LoRA) as well as full fine-tuning (FFT).
Experiments are conducted on 18 different tasks across three different
categories namely commonsense reasoning, math reasoning and summary-dialogue.
Latency measurements made on NPU (Samsung Galaxy S25+) as well as GPU (NVIDIA
H100) platforms show that the proposed zFLoRA adapters introduce zero to
negligible latency overhead.

</details>


### [5] [BlackboxNLP-2025 MIB Shared Task: Improving Circuit Faithfulness via Better Edge Selection](https://arxiv.org/abs/2510.25786)
*Yaniv Nikankin,Dana Arad,Itay Itzhak,Anja Reusch,Adi Simhi,Gal Kesten-Pomeranz,Yonatan Belinkov*

Main category: cs.CL

TL;DR: 本文针对可解释性研究中的电路发现问题，提出了三种改进方法以提升电路发现的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 在可解释性研究中，确定模型的哪些部分执行特定任务是一项主要挑战。

Method: 1. 使用 bootstrapping 识别具有一致 attribution 分数的边。2. 引入基于比率的简单选择策略，以优先选择强阳性评分的边，平衡性能和忠实度。3. 用整数线性规划公式代替标准贪婪选择。

Result: 该方法在多个 MIB 任务和模型上优于先前的方法，产生了更真实的电路。

Conclusion: 本文提出的方法能够更有效地发现模型中的电路，提升了模型可解释性的研究水平。

Abstract: One of the main challenges in mechanistic interpretability is circuit
discovery, determining which parts of a model perform a given task. We build on
the Mechanistic Interpretability Benchmark (MIB) and propose three key
improvements to circuit discovery. First, we use bootstrapping to identify
edges with consistent attribution scores. Second, we introduce a simple
ratio-based selection strategy to prioritize strong positive-scoring edges,
balancing performance and faithfulness. Third, we replace the standard greedy
selection with an integer linear programming formulation. Our methods yield
more faithful circuits and outperform prior approaches across multiple MIB
tasks and models. Our code is available at:
https://github.com/technion-cs-nlp/MIB-Shared-Task.

</details>


### [6] [LISTEN to Your Preferences: An LLM Framework for Multi-Objective Selection](https://arxiv.org/abs/2510.25799)
*Adam S. Jovine,Tinghan Ye,Francis Bahk,Jingjing Wang,David B. Shmoys,Peter I. Frazier*

Main category: cs.CL

TL;DR: 提出 LISTEN 框架，利用大型语言模型 (LLM) 作为零样本偏好预言机，仅由专家的自然语言高级优先级指导，以解决人类专家难以从具有多个竞争目标的大量项目中选择最佳选项的问题。


<details>
  <summary>Details</summary>
Motivation: 人类专家通常难以从具有多个竞争目标的大量项目中选择最佳选项，这一过程受到形式化复杂、隐含偏好的困难的限制。

Method: 提出两种迭代算法：LISTEN-U，使用 LLM 来改进参数化效用函数；LISTEN-T，一种非参数方法，对小批量的解决方案执行锦标赛式选择。

Result: 在包括航班预订、购物和考试安排在内的各种任务中进行评估，结果表明，当偏好与参数对齐时（我们使用一种新的 Concordance 指标来衡量这一特性），LISTEN-U 表现出色，而 LISTEN-T 提供了更强大的性能。

Conclusion: 这项工作探索了一个有希望的方向，可以直接使用自然语言来指导复杂的多目标决策，从而减轻传统偏好启发带来的认知负担。

Abstract: Human experts often struggle to select the best option from a large set of
items with multiple competing objectives, a process bottlenecked by the
difficulty of formalizing complex, implicit preferences. To address this, we
introduce LISTEN, a framework that leverages a Large Language Model (LLM) as a
zero-shot preference oracle, guided only by an expert's high-level priorities
in natural language. To operate within LLM constraints like context windows and
inference costs, we propose two iterative algorithms: LISTEN-U, which uses the
LLM to refine a parametric utility function, and LISTEN-T, a non-parametric
method that performs tournament-style selections over small batches of
solutions. Evaluated on diverse tasks including flight booking, shopping, and
exam scheduling, our results show LISTEN-U excels when preferences are
parametrically aligned (a property we measure with a novel concordance metric),
while LISTEN-T offers more robust performance. This work explores a promising
direction for steering complex multi-objective decisions directly with natural
language, reducing the cognitive burden of traditional preference elicitation.

</details>


### [7] [Beyond Length: Quantifying Long-Range Information for Long-Context LLM Pretraining Data](https://arxiv.org/abs/2510.25804)
*Haoran Deng,Yingyu Lin,Zhenghao Lin,Xiao Liu,Yizhou Sun,Yi-An Ma,Yeyun Gong*

Main category: cs.CL

TL;DR: Long-context language models benefit reasoning but training on all long text is inefficient.


<details>
  <summary>Details</summary>
Motivation: Training long-context language models is inefficient because much long text lacks long-distance dependencies.

Method: A framework called LongFilter is introduced. It measures information gain from long context by comparing model predictions in long vs short context settings.

Result: Using LongFilter with LLaMA-3-8B to extend context length from 8K to 64K improves performance on HELMET, LongBench and RULER benchmarks.

Conclusion: LongFilter efficiently selects high-quality data for long-context pretraining.

Abstract: Long-context language models unlock advanced capabilities in reasoning, code
generation, and document summarization by leveraging dependencies across
extended spans of text. However, a significant portion of readily available
long-text data lacks meaningful long-distance dependencies; most spans can be
predicted using only local context. Training on such data is inefficient,
making careful data selection crucial. Therefore, we introduce LongFilter, a
framework for curating training data tailored to long-context pretraining.
LongFilter measures the information gain provided by extended context by
contrasting model predictions under long-context versus short-context settings,
thereby identifying samples where long-range dependencies are essential.
Experiments with LLaMA-3-8B, extending its context length from 8K to 64K, show
that LongFilter efficiently selects high-quality data and yields substantial
improvements on benchmarks such as HELMET, LongBench, and RULER.

</details>


### [8] [Inside CORE-KG: Evaluating Structured Prompting and Coreference Resolution for Knowledge Graphs](https://arxiv.org/abs/2510.26512)
*Dipak Meher,Carlotta Domeniconi*

Main category: cs.CL

TL;DR: 本文研究了如何从法律文件中构建知识图谱，特别是解决节点重复和噪声问题。


<details>
  <summary>Details</summary>
Motivation: 人工走私网络日益复杂，难以分析。法律文件提供了重要的见解，但通常是非结构化的，词汇密集，并且充满了模糊或变化的引用，这对自动知识图谱（KG）构建提出了重大挑战。

Method: 本文通过对CORE-KG框架进行系统性消融研究，CORE-KG框架集成了类型感知的共指模块和领域引导的结构化提示。

Result: 结果表明，移除共指消解会导致节点重复增加28.32%，噪声节点增加4.32%；移除结构化提示会导致节点重复增加4.34%，噪声节点增加73.33%。

Conclusion: 这些发现为设计基于LLM的鲁棒管道提供了经验性见解，以从复杂的法律文本中提取结构化表示。

Abstract: Human smuggling networks are increasingly adaptive and difficult to analyze.
Legal case documents offer critical insights but are often unstructured,
lexically dense, and filled with ambiguous or shifting references, which pose
significant challenges for automated knowledge graph (KG) construction. While
recent LLM-based approaches improve over static templates, they still generate
noisy, fragmented graphs with duplicate nodes due to the absence of guided
extraction and coreference resolution. The recently proposed CORE-KG framework
addresses these limitations by integrating a type-aware coreference module and
domain-guided structured prompts, significantly reducing node duplication and
legal noise. In this work, we present a systematic ablation study of CORE-KG to
quantify the individual contributions of its two key components. Our results
show that removing coreference resolution results in a 28.32% increase in node
duplication and a 4.32% increase in noisy nodes, while removing structured
prompts leads to a 4.34% increase in node duplication and a 73.33% increase in
noisy nodes. These findings offer empirical insights for designing robust
LLM-based pipelines for extracting structured representations from complex
legal texts.

</details>


### [9] [Ideology-Based LLMs for Content Moderation](https://arxiv.org/abs/2510.25805)
*Stefano Civelli,Pietro Bernardelle,Nardiena A. Pratama,Gianluca Demartini*

Main category: cs.CL

TL;DR: 该研究调查了角色扮演如何影响大型语言模型在有害内容分类中的一致性和公平性。初步结果表明，角色扮演对整体分类准确率影响不大，但更深入的分析揭示了重要的行为变化。不同意识形态的角色扮演者在标记内容为有害内容时表现出不同的倾向，表明模型“看待”输入的角度会微妙地影响其判断。模型，尤其是较大的模型，倾向于与来自同一政治意识形态的角色扮演者更紧密地保持一致，从而加强意识形态内部的一致性，同时扩大意识形态群体之间的差异。角色扮演会给LLM的输出带来微妙的意识形态偏差，从而引发了人们对使用可能在表面中立下加强党派观点的人工智能系统的担忧。


<details>
  <summary>Details</summary>
Motivation: 在内容审核系统中，确保公平和中立至关重要。大型语言模型（LLM）越来越多地用于内容审核系统。

Method: 该研究检查了角色扮演如何影响不同LLM架构、模型大小和内容模式（语言与视觉）中有害内容分类的一致性和公平性。通过一致性分析，突出显示模型（尤其是较大的模型）倾向于与来自同一政治意识形态的角色扮演者更紧密地保持一致，从而加强意识形态内部的一致性，同时扩大意识形态群体之间的差异。为了更直接地显示这种效果，我们对一项具有政治针对性的任务进行了额外的研究。

Result: 角色扮演对整体分类准确率影响不大，但更深入的分析揭示了重要的行为变化。不同意识形态的角色扮演者在标记内容为有害内容时表现出不同的倾向，表明模型“看待”输入的角度会微妙地影响其判断。模型，尤其是较大的模型，倾向于与来自同一政治意识形态的角色扮演者更紧密地保持一致，从而加强意识形态内部的一致性，同时扩大意识形态群体之间的差异。角色扮演不仅在他们自己的意识形态中表现得更加连贯，而且还表现出捍卫自己观点同时淡化对立观点中的危害性的倾向。

Conclusion: 角色扮演会给LLM的输出带来微妙的意识形态偏差，从而引发了人们对使用可能在表面中立下加强党派观点的人工智能系统的担忧。

Abstract: Large language models (LLMs) are increasingly used in content moderation
systems, where ensuring fairness and neutrality is essential. In this study, we
examine how persona adoption influences the consistency and fairness of harmful
content classification across different LLM architectures, model sizes, and
content modalities (language vs. vision). At first glance, headline performance
metrics suggest that personas have little impact on overall classification
accuracy. However, a closer analysis reveals important behavioral shifts.
Personas with different ideological leanings display distinct propensities to
label content as harmful, showing that the lens through which a model "views"
input can subtly shape its judgments. Further agreement analyses highlight that
models, particularly larger ones, tend to align more closely with personas from
the same political ideology, strengthening within-ideology consistency while
widening divergence across ideological groups. To show this effect more
directly, we conducted an additional study on a politically targeted task,
which confirmed that personas not only behave more coherently within their own
ideology but also exhibit a tendency to defend their perspective while
downplaying harmfulness in opposing views. Together, these findings highlight
how persona conditioning can introduce subtle ideological biases into LLM
outputs, raising concerns about the use of AI systems that may reinforce
partisan perspectives under the guise of neutrality.

</details>


### [10] [Beyond Long Context: When Semantics Matter More than Tokens](https://arxiv.org/abs/2510.25816)
*Tarun Kumar Chawdhury,Jon D. Duke*

Main category: cs.CL

TL;DR: CLEAR improves clinical question answering (QA) by using entity-aware retrieval, achieving better accuracy and efficiency compared to other methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for clinical QA using EHR data struggle with base64 encoded attachments and nuanced relationships, leading to inefficiencies.

Method: The study validates the CLEAR method against zero-shot large context inference and chunk-based retrieval augmented generation using a newly developed Clinical Notes QA Evaluation Platform.

Result: CLEAR achieved a 58.3% win rate, 0.878 semantic similarity, and used 78% fewer tokens. Performance gains were greater on longer notes.

Conclusion: Entity-aware retrieval enhances accuracy and efficiency in clinical NLP. The evaluation framework offers a benchmark for clinical QA systems where precision and efficiency are key.

Abstract: Electronic Health Records (EHR) store clinical documentation as base64
encoded attachments in FHIR DocumentReference resources, which makes semantic
question answering difficult. Traditional vector database methods often miss
nuanced clinical relationships. The Clinical Entity Augmented Retrieval (CLEAR)
method, introduced by Lopez et al. 2025, uses entity aware retrieval and
achieved improved performance with an F1 score of 0.90 versus 0.86 for
embedding based retrieval, while using over 70 percent fewer tokens. We
developed a Clinical Notes QA Evaluation Platform to validate CLEAR against
zero shot large context inference and traditional chunk based retrieval
augmented generation. The platform was tested on 12 clinical notes ranging from
10,000 to 65,000 tokens representing realistic EHR content. CLEAR achieved a
58.3 percent win rate, an average semantic similarity of 0.878, and used 78
percent fewer tokens than wide context processing. The largest performance
gains occurred on long notes, with a 75 percent win rate for documents
exceeding 65,000 tokens. These findings confirm that entity aware retrieval
improves both efficiency and accuracy in clinical natural language processing.
The evaluation framework provides a reusable and transparent benchmark for
assessing clinical question answering systems where semantic precision and
computational efficiency are critical.

</details>


### [11] [A Survey on Efficient Large Language Model Training: From Data-centric Perspectives](https://arxiv.org/abs/2510.25817)
*Junyu Luo,Bohan Wu,Xiao Luo,Zhiping Xiao,Yiqiao Jin,Rong-Cheng Tu,Nan Yin,Yifan Wang,Jingyang Yuan,Wei Ju,Ming Zhang*

Main category: cs.CL

TL;DR: 本研究调查了数据高效的大型语言模型（LLM）后训练方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM后训练范例面临数据挑战，包括高昂的手动注释成本和数据规模的边际收益递减。因此，实现数据高效的后训练已成为关键的研究问题。

Method: 本文从以数据为中心的角度，对数据高效的LLM后训练进行了首次系统调查。我们提出了一种数据高效的LLM后训练方法分类法，涵盖数据选择、数据质量增强、合成数据生成、数据蒸馏和压缩以及自我进化数据生态系统。

Result: 我们总结了每个类别的代表性方法，并概述了未来的研究方向。通过检查数据高效LLM后训练中的挑战，我们强调了开放性问题，并提出了潜在的研究途径。

Conclusion: 我们希望我们的工作能够激发人们进一步探索在大型模型训练中最大限度地利用数据的潜力。

Abstract: Post-training of Large Language Models (LLMs) is crucial for unlocking their
task generalization potential and domain-specific capabilities. However, the
current LLM post-training paradigm faces significant data challenges, including
the high costs of manual annotation and diminishing marginal returns on data
scales. Therefore, achieving data-efficient post-training has become a key
research question. In this paper, we present the first systematic survey of
data-efficient LLM post-training from a data-centric perspective. We propose a
taxonomy of data-efficient LLM post-training methods, covering data selection,
data quality enhancement, synthetic data generation, data distillation and
compression, and self-evolving data ecosystems. We summarize representative
approaches in each category and outline future research directions. By
examining the challenges in data-efficient LLM post-training, we highlight open
problems and propose potential research avenues. We hope our work inspires
further exploration into maximizing the potential of data utilization in
large-scale model training. Paper List:
https://github.com/luo-junyu/Awesome-Data-Efficient-LLM

</details>


### [12] [Evaluating the Impact of LLM-Assisted Annotation in a Perspectivized Setting: the Case of FrameNet Annotation](https://arxiv.org/abs/2510.25904)
*Frederico Belcavello,Ely Matos,Arthur Lorenzi,Lisandra Bonoto,Lívia Ruiz,Luiz Fernando Pereira,Victor Herbst,Yulla Navarro,Helen de Andrade Abreu,Lívia Dutra,Tiago Timponi Torrent*

Main category: cs.CL

TL;DR: 本研究评估了基于LLM的语义角色标注器在FrameNet语义标注中的应用。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对LLM在NLP数据集构建中性能和影响的全面评估，尤其是在视角化NLP方法下。

Method: 比较了手动、自动和半自动标注在时间、覆盖率和多样性方面的表现。

Result: 半自动标注在框架多样性上优于人工标注，覆盖率相似。自动标注在所有指标上都较差，除了标注时间。

Conclusion: 半自动标注可以提高框架多样性并保持覆盖率，但自动标注性能较差。

Abstract: The use of LLM-based applications as a means to accelerate and/or substitute
human labor in the creation of language resources and dataset is a reality.
Nonetheless, despite the potential of such tools for linguistic research,
comprehensive evaluation of their performance and impact on the creation of
annotated datasets, especially under a perspectivized approach to NLP, is still
missing. This paper contributes to reduction of this gap by reporting on an
extensive evaluation of the (semi-)automatization of FrameNet-like semantic
annotation by the use of an LLM-based semantic role labeler. The methodology
employed compares annotation time, coverage and diversity in three experimental
settings: manual, automatic and semi-automatic annotation. Results show that
the hybrid, semi-automatic annotation setting leads to increased frame
diversity and similar annotation coverage, when compared to the human-only
setting, while the automatic setting performs considerably worse in all
metrics, except for annotation time.

</details>


### [13] [RECAP: Reproducing Copyrighted Data from LLMs Training with an Agentic Pipeline](https://arxiv.org/abs/2510.25941)
*André V. Duarte,Xuying li,Bin Zeng,Arlindo L. Oliveira,Lei Li,Zhuo Li*

Main category: cs.CL

TL;DR: RECAP是一个从LLM中提取和验证记忆化训练数据的agentic pipeline。


<details>
  <summary>Details</summary>
Motivation: 在无法检查LLM训练数据的情况下，需要知道模型所见内容。

Method: RECAP使用反馈驱动循环，通过二级语言模型评估初始提取尝试，并生成修正提示反馈给目标模型。此外，RECAP包含一个jailbreaking模块来检测和克服对齐引起的拒绝。

Result: 在EchoTrace基准测试中，RECAP相对于单次迭代方法有显著提高。例如，使用GPT-4.1时，受版权保护的文本提取的平均ROUGE-L得分从0.38提高到0.47，提高了近24%。

Conclusion: RECAP可以有效地从LLM中提取和验证记忆化训练数据。

Abstract: If we cannot inspect the training data of a large language model (LLM), how
can we ever know what it has seen? We believe the most compelling evidence
arises when the model itself freely reproduces the target content. As such, we
propose RECAP, an agentic pipeline designed to elicit and verify memorized
training data from LLM outputs. At the heart of RECAP is a feedback-driven
loop, where an initial extraction attempt is evaluated by a secondary language
model, which compares the output against a reference passage and identifies
discrepancies. These are then translated into minimal correction hints, which
are fed back into the target model to guide subsequent generations. In
addition, to address alignment-induced refusals, RECAP includes a jailbreaking
module that detects and overcomes such barriers. We evaluate RECAP on
EchoTrace, a new benchmark spanning over 30 full books, and the results show
that RECAP leads to substantial gains over single-iteration approaches. For
instance, with GPT-4.1, the average ROUGE-L score for the copyrighted text
extraction improved from 0.38 to 0.47 - a nearly 24% increase.

</details>


### [14] [Revisiting Multilingual Data Mixtures in Language Model Pretraining](https://arxiv.org/abs/2510.25947)
*Negar Foroutan,Paul Teiletche,Ayush Kumar Tarun,Antoine Bosselut*

Main category: cs.CL

TL;DR: 研究了大型语言模型中不同多语言数据混合的影响，挑战了关于多语言训练的常见观点。


<details>
  <summary>Details</summary>
Motivation: 探讨了预训练大型语言模型中语言覆盖范围和模型性能之间的潜在权衡，即多语言的诅咒。

Method: 在不同的多语言语料库上训练了 1.1B 和 3B 参数的 LLM，语言数量从 25 种到 400 种不等。

Result: 1. 结合英语和多语言数据不一定会降低两者的语言性能；2. 使用英语作为枢轴语言可以跨语系产生益处；3. 随着训练语言数量的增加，没有观察到明显的“多语言诅咒”。

Conclusion: 多语言数据在适当平衡时，可以增强语言模型的能力，而不会影响性能，即使在低资源环境中也是如此。

Abstract: The impact of different multilingual data mixtures in pretraining large
language models (LLMs) has been a topic of ongoing debate, often raising
concerns about potential trade-offs between language coverage and model
performance (i.e., the curse of multilinguality). In this work, we investigate
these assumptions by training 1.1B and 3B parameter LLMs on diverse
multilingual corpora, varying the number of languages from 25 to 400. Our study
challenges common beliefs surrounding multilingual training. First, we find
that combining English and multilingual data does not necessarily degrade the
in-language performance of either group, provided that languages have a
sufficient number of tokens included in the pretraining corpus. Second, we
observe that using English as a pivot language (i.e., a high-resource language
that serves as a catalyst for multilingual generalization) yields benefits
across language families, and contrary to expectations, selecting a pivot
language from within a specific family does not consistently improve
performance for languages within that family. Lastly, we do not observe a
significant "curse of multilinguality" as the number of training languages
increases in models at this scale. Our findings suggest that multilingual data,
when balanced appropriately, can enhance language model capabilities without
compromising performance, even in low-resource settings

</details>


### [15] [Semantic Label Drift in Cross-Cultural Translation](https://arxiv.org/abs/2510.25967)
*Mohsinul Kabir,Tasnim Ahmed,Md Mezbaur Rahman,Polydoros Giannouris,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 机器翻译在低资源语言中被广泛使用，但文化差异会导致语义标签漂移。


<details>
  <summary>Details</summary>
Motivation: 探讨机器翻译中文化对齐的重要性，以及文化差异如何影响翻译质量。

Method: 通过一系列跨文化敏感和中性领域的实验。

Result: （1）机器翻译系统，包括大型语言模型（LLM），在翻译过程中会引起标签漂移，尤其是在文化敏感领域；（2）与早期的统计机器翻译工具不同，LLM 编码了文化知识，利用这些知识会放大标签漂移；（3）源语言和目标语言之间的文化相似性或差异是标签保留的关键决定因素。

Conclusion: 在机器翻译中忽略文化因素不仅会损害标签的准确性，还会导致下游应用中的误解和文化冲突。

Abstract: Machine Translation (MT) is widely employed to address resource scarcity in
low-resource languages by generating synthetic data from high-resource
counterparts. While sentiment preservation in translation has long been
studied, a critical but underexplored factor is the role of cultural alignment
between source and target languages. In this paper, we hypothesize that
semantic labels are drifted or altered during MT due to cultural divergence.
Through a series of experiments across culturally sensitive and neutral
domains, we establish three key findings: (1) MT systems, including modern
Large Language Models (LLMs), induce label drift during translation,
particularly in culturally sensitive domains; (2) unlike earlier statistical MT
tools, LLMs encode cultural knowledge, and leveraging this knowledge can
amplify label drift; and (3) cultural similarity or dissimilarity between
source and target languages is a crucial determinant of label preservation. Our
findings highlight that neglecting cultural factors in MT not only undermines
label fidelity but also risks misinterpretation and cultural conflict in
downstream applications.

</details>


### [16] [SymCode: A Neurosymbolic Approach to Mathematical Reasoning via Verifiable Code Generation](https://arxiv.org/abs/2510.25975)
*Sina Bagheri Nezhad,Yao Li,Ameeta Agrawal*

Main category: cs.CL

TL;DR: 提出了 SymCode，一个神经符号框架，它将数学问题求解重新定义为使用 SymPy 库的可验证代码生成任务。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 通常难以进行复杂的数学推理，其中基于散文的生成会导致未经验证和算术上不健全的解决方案。当前的提示策略（如思维链）仍然在此不可靠的媒介中运行，缺乏确定性验证机制。

Method: 我们引入 SymCode，一个神经符号框架，它将数学问题求解重新定义为使用 SymPy 库的可验证代码生成任务。

Result: 在具有挑战性的基准测试（包括 MATH-500 和 OlympiadBench）上评估 SymCode，证明与基线相比，准确性显着提高了高达 13.6 个百分点。

Conclusion: 通过将 LLM 推理置于确定性符号引擎中，SymCode 代表了在形式域中实现更准确和可信的 AI 的关键一步。

Abstract: Large Language Models (LLMs) often struggle with complex mathematical
reasoning, where prose-based generation leads to unverified and arithmetically
unsound solutions. Current prompting strategies like Chain of Thought still
operate within this unreliable medium, lacking a mechanism for deterministic
verification. To address these limitations, we introduce SymCode, a
neurosymbolic framework that reframes mathematical problem-solving as a task of
verifiable code generation using the SymPy library. We evaluate SymCode on
challenging benchmarks, including MATH-500 and OlympiadBench, demonstrating
significant accuracy improvements of up to 13.6 percentage points over
baselines. Our analysis shows that SymCode is not only more token-efficient but
also fundamentally shifts model failures from opaque logical fallacies towards
transparent, programmatic errors. By grounding LLM reasoning in a deterministic
symbolic engine, SymCode represents a key step towards more accurate and
trustworthy AI in formal domains.

</details>


### [17] [NeuronMM: High-Performance Matrix Multiplication for LLM Inference on AWS Trainium](https://arxiv.org/abs/2510.25977)
*Dinghong Song,Jierui Xu,Weichu Yang,Pengfei Su,Dong Li*

Main category: cs.CL

TL;DR: 本文介绍了针对Amazon Trainium AI加速器设计的高性能矩阵乘法（matmul）方案，用于加速LLM的推理。


<details>
  <summary>Details</summary>
Motivation: 为了解决Trainium架构在LLM训练和推理中面临的挑战，特别是其 systolic 阵列架构和对数据布局的特殊要求。

Method: 通过kernel fusion和创新的缓存策略，减少数据在软件管理的内存层次结构中的移动，最大化SRAM带宽，并避免昂贵的矩阵转置。

Result: matmul内核平均加速1.35倍（最高2.22倍），端到端LLM推理平均加速1.66倍（最高2.49倍）。

Conclusion: 所提出的系统在Trainium上大大优于AWS实现的现有matmul技术。

Abstract: AI accelerators, customized to AI workloads, provide cost-effective and
high-performance solutions for training and inference. Trainium, an AI
accelerator recently developed by Amazon Web Services (AWS), provides an
attractive option for LLM training and inference through its heterogeneous
architecture. However, leveraging Trainium architecture for high performance
can be challenging because of its systolic array architecture and special
requirement on data layout. In this paper, we design high-performance matrix
multiplication (matmul), a critical compute kernel, for LLM inference on
Trainium. We introduce a series of techniques customized to Trainium based on
kernel fusion and novel caching strategies to reduce data movement across the
software-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive
matrix transpose. Evaluating with nine datasets and four recent LLMs, we show
that our system largely outperforms the state-of-the-art matmul implemented by
AWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x
speedup (up to 2.22x), which translates to an average 1.66x speedup (up to
2.49x) for end-to-end LLM inference.

</details>


### [18] [AttnCache: Accelerating Self-Attention Inference for LLM Prefill via Attention Cache](https://arxiv.org/abs/2510.25979)
*Dinghong Song,Yuan Feng,Yiwei Wang,Shangye Chen,Cyril Guyot,Filip Blagojevic,Hyeran Jeon,Pengfei Su,Dong Li*

Main category: cs.CL

TL;DR: AttnCache 通过检索和重用相似的注意力图来加速 LLM 推理的 prefill 阶段，从而减少自注意力的计算开销。


<details>
  <summary>Details</summary>
Motivation: 在仅 prefill 的场景中，自注意力计算成为主要的性能瓶颈，因为它相对于序列长度具有二次复杂度。

Method: AttnCache 采用高效的缓存和相似性搜索技术来识别和重用预缓存的注意力图。

Result: AttnCache 在 CPU 上实现了平均 1.2 倍的端到端加速和 2 倍的注意力加速，在 GPU 上实现了 1.6 倍的端到端加速和 3 倍的注意力加速，且精度下降可忽略不计。

Conclusion: AttnCache 是一种有效的框架，用于加速 LLM 推理的 prefill 阶段。

Abstract: Large Language Models (LLMs) are widely used in generative applications such
as chatting, code generation, and reasoning. However, many realworld workloads
such as classification, question answering, recommendation, and text embedding
rely solely on the prefill stage of inference, where the model encodes input
sequences without performing autoregressive decoding. In these prefill only
scenarios, the self-attention computation becomes the primary performance
bottleneck due to its quadratic complexity with respect to sequence length. In
this paper, we observe that semantically different sentences often produce
similar attention maps across layers and heads. Building on this insight, we
propose AttnCache, a framework that accelerates the prefill stage of LLM
inference by retrieving and reusing similar attention maps. Based on an
attention map memorization database, AttnCache employs efficient caching and
similarity search techniques to identify and reuse pre-cached attention maps
during inference, thereby reducing the computational overhead of
self-attention. Experimental results show that AttnCache achieves an average of
1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x
attention speedup on GPU, with negligible accuracy degradation.

</details>


### [19] [Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning](https://arxiv.org/abs/2510.25992)
*Yihe Deng,I-Hung Hsu,Jun Yan,Zifeng Wang,Rujun Han,Gufeng Zhang,Yanfei Chen,Wei Wang,Tomas Pfister,Chen-Yu Lee*

Main category: cs.CL

TL;DR: 提出了监督强化学习（SRL）框架，以解决LLM在多步骤推理问题上的困难，尤其是在小规模开源模型中。


<details>
  <summary>Details</summary>
Motivation: 现有方法，如RLVR和SFT，在解决需要多步骤推理的问题时存在不足。RLVR在正确解决方案难以采样时失效，而SFT容易过度拟合长篇演示。

Method: SRL将问题解决重新定义为生成一系列逻辑“动作”，并训练模型在每次行动前生成内部推理独白。它基于模型行动与专家行动之间的相似性提供平滑的奖励。

Result: SRL使小型模型能够学习以前SFT或RLVR无法学习的具有挑战性的问题。在推理基准测试之外，SRL有效地推广到代理软件工程任务。

Conclusion: SRL是一种强大而通用的面向推理的LLM训练框架。

Abstract: Large Language Models (LLMs) often struggle with problems that require
multi-step reasoning. For small-scale open-source models, Reinforcement
Learning with Verifiable Rewards (RLVR) fails when correct solutions are rarely
sampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to
overfit long demonstrations through rigid token-by-token imitation. To address
this gap, we propose Supervised Reinforcement Learning (SRL), a framework that
reformulates problem solving as generating a sequence of logical "actions". SRL
trains the model to generate an internal reasoning monologue before committing
to each action. It provides smoother rewards based on the similarity between
the model's actions and expert actions extracted from the SFT dataset in a
step-wise manner. This supervision offers richer learning signals even when all
rollouts are incorrect, while encouraging flexible reasoning guided by expert
demonstrations. As a result, SRL enables small models to learn challenging
problems previously unlearnable by SFT or RLVR. Moreover, initializing training
with SRL before refining with RLVR yields the strongest overall performance.
Beyond reasoning benchmarks, SRL generalizes effectively to agentic software
engineering tasks, establishing it as a robust and versatile training framework
for reasoning-oriented LLMs.

</details>


### [20] [PORTool: Tool-Use LLM Training with Rewarded Tree](https://arxiv.org/abs/2510.26020)
*Feijie Wu,Weiwu Zhu,Yuxiang Zhang,Soumya Chatterjee,Jiarong Zhu,Fan Mo,Rodin Luo,Jing Gao*

Main category: cs.CL

TL;DR: 提出了一种名为PORTool的强化学习方法，以提升大型语言模型(llm)在动态工具调用环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的工具使用llm在静态数据集上训练，无法探索可能的解决方案，在动态环境中表现不佳。

Method: 通过生成多个rollout形成树状结构，并基于产生正确答案和成功工具调用的能力为每个步骤分配奖励，然后使用这些奖励计算fork-relative优势，以训练llm。

Result: 实验使用了17个工具来解决用户查询，结果表明PORTool在最终准确性和工具调用步骤数方面均有显著改进。

Conclusion: PORTool方法有效地提升了llm在动态工具调用环境中的性能。

Abstract: Current tool-use large language models (LLMs) are trained on static datasets,
enabling them to interact with external tools and perform multi-step,
tool-integrated reasoning, which produces tool-call trajectories. However,
these models imitate how a query is resolved in a generic tool-call routine,
thereby failing to explore possible solutions and demonstrating limited
performance in an evolved, dynamic tool-call environment. In this work, we
propose PORTool, a reinforcement learning (RL) method that encourages a
tool-use LLM to explore various trajectories yielding the correct answer.
Specifically, this method starts with generating multiple rollouts for a given
query, and some of them share the first few tool-call steps, thereby forming a
tree-like structure. Next, we assign rewards to each step, based on its ability
to produce a correct answer and make successful tool calls. A shared step
across different trajectories receives the same reward, while different steps
under the same fork receive different rewards. Finally, these step-wise rewards
are used to calculate fork-relative advantages, blended with
trajectory-relative advantages, to train the LLM for tool use. The experiments
utilize 17 tools to address user queries, covering both time-sensitive and
time-invariant topics. We conduct ablation studies to systematically justify
the necessity and the design robustness of step-wise rewards. Furthermore, we
compare the proposed PORTool with other training approaches and demonstrate
significant improvements in final accuracy and the number of tool-call steps.

</details>


### [21] [Rethinking Cross-lingual Alignment: Balancing Transfer and Cultural Erasure in Multilingual LLMs](https://arxiv.org/abs/2510.26024)
*HyoJung Han,Sweta Agrawal,Eleftheria Briakou*

Main category: cs.CL

TL;DR: 跨语言对齐旨在对齐多语言表示，但也可能导致“文化抹除”，即丧失提供基于查询语言的文化情境化回应的能力。本文提出了一个整体评估框架，即迁移-本地化平面，以量化理想的知识转移和不良的文化抹除。基于此，作者重新评估了最近的 CLA 方法，发现它们以牺牲文化本地化为代价持续提高事实转移。他们提出了一种新颖的推理时方法，即 Surgical Steering，通过将目标激活steering应用于不同的层，从而在两个竞争维度之间取得更好的平衡。


<details>
  <summary>Details</summary>
Motivation: 跨语言对齐(CLA)旨在对齐多语言表示，使大型语言模型(LLm)能够跨语言无缝转移知识。但这种对表征融合的追求可能会造成“文化抹除”，即丧失提供文化情境化回应的功能性损失。

Method: 本文通过引入一个整体评估框架——迁移-本地化平面，系统地分析了这种权衡，该框架量化了理想的知识转移和不良的文化抹除。基于这一发现，作者提出了一种新的推理时间方法，即手术steering，它解开了这两个目标。

Result: 作者重新评估了最近的CLA方法，发现它们在所有六种研究语言中，都以牺牲文化本地化为直接代价，持续提高了事实转移。对这些模型内部表征的调查揭示了一个关键的见解：普遍的事实转移和文化特定的知识在不同的模型层面上可以得到最佳的steering。

Conclusion: Surgical Steering通过将目标激活steering应用于不同的层，在两个竞争维度之间取得了更好的平衡，有效地克服了当前对齐技术的局限性。

Abstract: Cross-lingual alignment (CLA) aims to align multilingual representations,
enabling Large Language Models (LLMs) to seamlessly transfer knowledge across
languages. While intuitive, we hypothesize, this pursuit of representational
convergence can inadvertently cause "cultural erasure", the functional loss of
providing culturally-situated responses that should diverge based on the query
language. In this work, we systematically analyze this trade-off by introducing
a holistic evaluation framework, the transfer-localization plane, which
quantifies both desirable knowledge transfer and undesirable cultural erasure.
Using this framework, we re-evaluate recent CLA approaches and find that they
consistently improve factual transfer at the direct cost of cultural
localization across all six languages studied. Our investigation into the
internal representations of these models reveals a key insight: universal
factual transfer and culturally-specific knowledge are optimally steerable at
different model layers. Based on this finding, we propose Surgical Steering, a
novel inference-time method that disentangles these two objectives. By applying
targeted activation steering to distinct layers, our approach achieves a better
balance between the two competing dimensions, effectively overcoming the
limitations of current alignment techniques.

</details>


### [22] [Artificial Intelligence-Enabled Analysis of Radiology Reports: Epidemiology and Consequences of Incidental Thyroid Findings](https://arxiv.org/abs/2510.26032)
*Felipe Larios,Mariana Borras-Osorio,Yuqi Wu,Ana Gabriela Claros,David Toro-Tobon,Esteban Cabezas,Ricardo Loor-Torres,Maria Mateo Chavez,Kerly Guevara Maldonado,Luis Vilatuna Andrango,Maria Lizarazo Jimenez,Ivan Mateo Alzamora,Misk Al Zahidy,Marcelo Montero,Ana Cristina Proano,Cristian Soto Jacome,Jungwei W. Fan,Oscar J. Ponce-Ponte,Megan E. Branda,Naykky Singh Ospina,Juan P. Brito*

Main category: cs.CL

TL;DR: 在非甲状腺检查中意外发现的甲状腺结节(ITF)越来越普遍。该研究旨在利用自然语言处理(NLP)技术识别ITF，并评估其患病率、特征和临床结果。


<details>
  <summary>Details</summary>
Motivation: ITF的患病率、特征和临床后果尚不明确。

Method: 开发并验证了一个基于transformer的NLP流程，用于从影像报告中识别ITF并提取结节特征。对2017年7月1日至2023年9月30日在梅奥诊所接受甲状腺影像检查的成年人进行了回顾性队列研究。

Result: 在115,683名患者中，9,077名(7.8%)有ITF，其中92.9%为结节。与无ITF的患者相比，有ITF的患者甲状腺结节诊断、活检、甲状腺切除术和甲状腺癌诊断的几率更高。

Conclusion: ITF很常见，并且与导致检测到小的、低风险癌症的级联反应密切相关。这些发现强调了ITF在甲状腺癌过度诊断中的作用，以及标准化报告和更具选择性的随访的必要性。

Abstract: Importance Incidental thyroid findings (ITFs) are increasingly detected on
imaging performed for non-thyroid indications. Their prevalence, features, and
clinical consequences remain undefined. Objective To develop, validate, and
deploy a natural language processing (NLP) pipeline to identify ITFs in
radiology reports and assess their prevalence, features, and clinical outcomes.
Design, Setting, and Participants Retrospective cohort of adults without prior
thyroid disease undergoing thyroid-capturing imaging at Mayo Clinic sites from
July 1, 2017, to September 30, 2023. A transformer-based NLP pipeline
identified ITFs and extracted nodule characteristics from image reports from
multiple modalities and body regions. Main Outcomes and Measures Prevalence of
ITFs, downstream thyroid ultrasound, biopsy, thyroidectomy, and thyroid cancer
diagnosis. Logistic regression identified demographic and imaging-related
factors. Results Among 115,683 patients (mean age, 56.8 [SD 17.2] years; 52.9%
women), 9,077 (7.8%) had an ITF, of which 92.9% were nodules. ITFs were more
likely in women, older adults, those with higher BMI, and when imaging was
ordered by oncology or internal medicine. Compared with chest CT, ITFs were
more likely via neck CT, PET, and nuclear medicine scans. Nodule
characteristics were poorly documented, with size reported in 44% and other
features in fewer than 15% (e.g. calcifications). Compared with patients
without ITFs, those with ITFs had higher odds of thyroid nodule diagnosis,
biopsy, thyroidectomy and thyroid cancer diagnosis. Most cancers were
papillary, and larger when detected after ITFs vs no ITF. Conclusions ITFs were
common and strongly associated with cascades leading to the detection of small,
low-risk cancers. These findings underscore the role of ITFs in thyroid cancer
overdiagnosis and the need for standardized reporting and more selective
follow-up.

</details>


### [23] [QCoder Benchmark: Bridging Language Generation and Quantum Hardware through Simulator-Based Feedback](https://arxiv.org/abs/2510.26101)
*Taku Mikuriya,Tatsuya Ishigaki,Masayuki Kawarada,Shunya Minami,Tadashi Kadowaki,Yohichi Suzuki,Soshun Naito,Shunya Takata,Takumi Kato,Tamotsu Basseda,Reo Yamada,Hiroya Takamura*

Main category: cs.CL

TL;DR: 论文提出了一个名为QCoder Benchmark的评估框架，用于评估大型语言模型在量子编程方面的能力，该框架包含模拟硬件设备的反馈和人类编写的代码，实验表明现有模型在该基准测试中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在与硬件设备交互的量子编程领域应用不足。

Method: 构建了一个包含量子模拟器环境和人类编写代码的评估框架QCoder Benchmark。

Result: GPT-4o在该基准测试中准确率仅为18.97%，而基于推理的模型o3准确率高达78%，超过了人类编写代码的平均成功率39.98%。

Conclusion: QCoder Benchmark揭示了现有模型在量子编程方面的不足，并为未来研究提供了数据集和评估API。

Abstract: Large language models (LLMs) have increasingly been applied to automatic
programming code generation. This task can be viewed as a language generation
task that bridges natural language, human knowledge, and programming logic.
However, it remains underexplored in domains that require interaction with
hardware devices, such as quantum programming, where human coders write Python
code that is executed on a quantum computer. To address this gap, we introduce
QCoder Benchmark, an evaluation framework that assesses LLMs on quantum
programming with feedback from simulated hardware devices. Our benchmark offers
two key features. First, it supports evaluation using a quantum simulator
environment beyond conventional Python execution, allowing feedback of
domain-specific metrics such as circuit depth, execution time, and error
classification, which can be used to guide better generation. Second, it
incorporates human-written code submissions collected from real programming
contests, enabling both quantitative comparisons and qualitative analyses of
LLM outputs against human-written codes. Our experiments reveal that even
advanced models like GPT-4o achieve only around 18.97% accuracy, highlighting
the difficulty of the benchmark. In contrast, reasoning-based models such as o3
reach up to 78% accuracy, outperforming averaged success rates of human-written
codes (39.98%). We release the QCoder Benchmark dataset and public evaluation
API to support further research.

</details>


### [24] [Reasoning Path Divergence: A New Metric and Curation Strategy to Unlock LLM Diverse Thinking](https://arxiv.org/abs/2510.26122)
*Feng Ju,Zeyu Qin,Rui Min,Zhitao He,Lingpeng Kong,Yi R. Fung*

Main category: cs.CL

TL;DR: 该论文提出了一种新的训练范式，即“一个问题，多个解决方案”(1PNS)，以提高大型语言模型(llm)在测试时缩放(TTS)中的推理能力和输出多样性。


<details>
  <summary>Details</summary>
Motivation: 现有的“一个问题，一个解决方案”(1P1S)训练方法限制了模型输出的多样性，因为模型倾向于学习狭窄的推理路径。

Method: 提出了“推理路径散度”(RPD)这一指标，用于衡量多步思维链中语义的差异性，并使用RPD来选择最多样化的解决方案集，以此来微调Qwen3-4B-Base模型。

Result: 实验表明，使用RPD选择的训练数据可以产生更多样化的输出，并且pass@k指标更高，在pass@16上平均提高了2.80%，在AIME24上提高了4.99%。

Conclusion: 1PNS训练范式可以进一步提高TTS的有效性。

Abstract: While Test-Time Scaling (TTS) has proven effective in improving the reasoning
ability of large language models (LLMs), low diversity in model outputs often
becomes a bottleneck; this is partly caused by the common "one problem, one
solution" (1P1S) training practice, which provides a single canonical answer
and can push models toward a narrow set of reasoning paths. To address this, we
propose a "one problem, multiple solutions" (1PNS) training paradigm that
exposes the model to a variety of valid reasoning trajectories and thus
increases inference diversity. A core challenge for 1PNS is reliably measuring
semantic differences between multi-step chains of thought, so we introduce
Reasoning Path Divergence (RPD), a step-level metric that aligns and scores
Long Chain-of-Thought solutions to capture differences in intermediate
reasoning. Using RPD, we curate maximally diverse solution sets per problem and
fine-tune Qwen3-4B-Base. Experiments show that RPD-selected training yields
more varied outputs and higher pass@k, with an average +2.80% gain in pass@16
over a strong 1P1S baseline and a +4.99% gain on AIME24, demonstrating that
1PNS further amplifies the effectiveness of TTS. Our code is available at
https://github.com/fengjujf/Reasoning-Path-Divergence .

</details>


### [25] [On the Influence of Discourse Relations in Persuasive Texts](https://arxiv.org/abs/2510.26124)
*Nawar Turk,Sevag Kaspar,Leila Kosseim*

Main category: cs.CL

TL;DR: 本文研究了说服技巧 (PTs) 和语篇关系 (DRs) 之间的关系，利用大型语言模型 (LLMs) 和提示工程。通过在 SemEval 2023 Task 3 数据集上训练 LLM 分类器，标注了 22 种 PDTB 3.0 level-2 DRs。统计分析表明，因果、目的、对比、因果+信念、让步和条件这六种语篇关系在说服性文本中起着关键作用，尤其是在使用加载语言、夸张/最小化、重复和产生怀疑时。这一发现有助于检测在线宣传和错误信息，并加深我们对有效沟通的理解。


<details>
  <summary>Details</summary>
Motivation: 当前没有同时标注 PTs 和 DRs 的数据集，促使研究人员利用 LLM 创建此类数据集并分析二者关系。

Method: 利用 LLM 和提示工程，在 SemEval 2023 Task 3 数据集上训练了 40 个独特的 DR 分类器，并使用集成模型创建了 5 个银数据集。

Result: 统计分析表明，因果、目的、对比、因果+信念、让步和条件这六种语篇关系在说服性文本中起着关键作用，尤其是在使用加载语言、夸张/最小化、重复和产生怀疑时。

Conclusion: 该研究结果有助于检测在线宣传和错误信息，并加深我们对有效沟通的理解。

Abstract: This paper investigates the relationship between Persuasion Techniques (PTs)
and Discourse Relations (DRs) by leveraging Large Language Models (LLMs) and
prompt engineering. Since no dataset annotated with both PTs and DRs exists, we
took the SemEval 2023 Task 3 dataset labelled with 19 PTs as a starting point
and developed LLM-based classifiers to label each instance of the dataset with
one of the 22 PDTB 3.0 level-2 DRs. In total, four LLMs were evaluated using 10
different prompts, resulting in 40 unique DR classifiers. Ensemble models using
different majority-pooling strategies were used to create 5 silver datasets of
instances labelled with both persuasion techniques and level-2 PDTB senses. The
silver dataset sizes vary from 1,281 instances to 204 instances, depending on
the majority pooling technique used. Statistical analysis of these silver
datasets shows that six discourse relations (namely Cause, Purpose, Contrast,
Cause+Belief, Concession, and Condition) play a crucial role in persuasive
texts, especially in the use of Loaded Language, Exaggeration/Minimisation,
Repetition and to cast Doubt. This insight can contribute to detecting online
propaganda and misinformation, as well as to our general understanding of
effective communication.

</details>


### [26] [MossNet: Mixture of State-Space Experts is a Multi-Head Attention](https://arxiv.org/abs/2510.26182)
*Shikhar Tuli,James Seale Smith,Haris Jeelani,Chi-Heng Lin,Abhishek Patel,Vasili Ramanishka,Yen-Chang Hsu,Hongxia Jin*

Main category: cs.CL

TL;DR: MossNet，一种新型混合状态空间专家架构，它模拟线性多头注意力。


<details>
  <summary>Details</summary>
Motivation: 当前基于SSM/GRM 的方法通常只模拟单个注意力头，这可能会限制其表达能力。

Method: 提出 MossNet，一种在通道混合 MLP 块和时间混合 SSM 内核中利用混合专家 (MoE) 实现的架构，以实现多个“注意力头”。

Result: 在语言建模和下游评估中的大量实验表明，MossNet 优于具有相似模型大小和数据预算的基于 Transformer 和 SSM 的架构。在万亿个 tokens 上训练的 MossNet 的更大变体进一步证实了其可扩展性和卓越的性能。在 Samsung Galaxy S24 Ultra 和 Nvidia A100 GPU 上的真机分析表明，与类似大小的基线相比，它具有良好的运行时速度和资源使用率。

Conclusion: MossNet 是高效、高性能循环 LLM 架构的一个引人注目的新方向。

Abstract: Large language models (LLMs) have significantly advanced generative
applications in natural language processing (NLP). Recent trends in model
architectures revolve around efficient variants of transformers or
state-space/gated-recurrent models (SSMs, GRMs). However, prevailing
SSM/GRM-based methods often emulate only a single attention head, potentially
limiting their expressiveness. In this work, we propose MossNet, a novel
mixture-of-state-space-experts architecture that emulates a linear multi-head
attention (MHA). MossNet leverages a mixture-of-experts (MoE) implementation
not only in channel-mixing multi-layered perceptron (MLP) blocks but also in
the time-mixing SSM kernels to realize multiple "attention heads." Extensive
experiments on language modeling and downstream evaluations show that MossNet
outperforms both transformer- and SSM-based architectures of similar model size
and data budgets. Larger variants of MossNet, trained on trillions of tokens,
further confirm its scalability and superior performance. In addition,
real-device profiling on a Samsung Galaxy S24 Ultra and an Nvidia A100 GPU
demonstrate favorable runtime speed and resource usage compared to similarly
sized baselines. Our results suggest that MossNet is a compelling new direction
for efficient, high-performing recurrent LLM architectures.

</details>


### [27] [Similarity-Distance-Magnitude Language Models](https://arxiv.org/abs/2510.26183)
*Allen Schmaltz*

Main category: cs.CL

TL;DR: 提出了相似度-距离-幅度 (SDM) 语言模型，通过微调来最大化生成结果在校准良好的高概率区域中的比例。


<details>
  <summary>Details</summary>
Motivation: 现有的预训练解码器 Transformer 语言模型可以通过监督微调转换为 SDM 语言模型。

Method: 使用最终层 SDM 激活层在训练期间估计对比输入编码方案的监督下一个 token 损失的基数变化，并在训练期间在线生成额外的硬负样本。

Result: 与强大的监督基线相比，减少了弃权（即提高了统计效率）。

Conclusion: 提出的 SDM 语言模型能够提高统计效率。

Abstract: We introduce Similarity-Distance-Magnitude (SDM) language models (LMs), which
are sequence prediction models fine-tuned to maximize the proportion of
generations in the well-calibrated, high-probability region partitioned by a
final-layer SDM activation layer used for binary classification of
instruction-following. We demonstrate that existing pre-trained decoder-only
Transformer LMs can be readily converted into SDM LMs via supervised
fine-tuning, using the final-layer SDM activation layer during training to
estimate a change-of-base for a supervised next-token loss over a contrastive
input encoding scheme, with additional hard negative examples generated online
during training. This results in reduced abstentions (i.e., improved
statistical efficiency) compared to strong supervised baselines.

</details>


### [28] [RCScore: Quantifying Response Consistency in Large Language Models](https://arxiv.org/abs/2510.26193)
*Dongjun Jang,Youngchae Ahn,Hyopil Shin*

Main category: cs.CL

TL;DR: 论文提出了RCScore，一个评估LLM对不同指令风格敏感度的框架，发现指令风格会显著影响模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估忽略了模型对指令风格的敏感性，这对于实际应用至关重要。

Method: 通过系统地将基准问题转换为多种指令风格，RCScore揭示了传统指标无法检测到的性能变化。引入Cross-Response Similarity (CRS) 来衡量风格自洽性。

Result: 实验表明，指令风格可以使准确率变化高达16.7%。CRS与任务准确率高度相关，确定性解码产生更稳定的输出，模型规模与跨风格一致性正相关。

Conclusion: RCScore提供了一种评估指令鲁棒性的有效方法。

Abstract: Current LLM evaluations often rely on a single instruction template,
overlooking models' sensitivity to instruction style-a critical aspect for
real-world deployments. We present RCScore, a multi-dimensional framework
quantifying how instruction formulation affects model responses. By
systematically transforming benchmark problems into multiple instruction
styles, RCScore reveals performance variations undetected by conventional
metrics. Our experiments across ten LLMs on four reasoning benchmarks
demonstrate that instruction style can shift accuracy by up to 16.7% points. We
introduce Cross-Response Similarity (CRS), a method applying RCScore metrics to
measure stylistic self-consistency, and establish its strong correlation with
task accuracy, suggesting consistency as a valuable proxy for model
reliability. Additional findings show that deterministic decoding produces more
stylistically stable outputs, and model scale correlates positively with
cross-style consistency. RCScore offers a principled approach to assess
instruction robustness.

</details>


### [29] [Don't Let It Fade: Preserving Edits in Diffusion Language Models via Token Timestep Allocation](https://arxiv.org/abs/2510.26200)
*Woojin Kim,Jaeyoung Do*

Main category: cs.CL

TL;DR: 扩散语言模型(DLM)虽然可以进行细粒度的改进，但实际的可控性仍然很脆弱。本文定义并描述了一种称为更新遗忘的失效模式，并提出了一种名为Token Timestep Allocation (TTA)的方法，该方法通过每个token的时间步长计划来实现软性和语义token排序，从而缓解更新遗忘并实现稳定和可控的扩散文本生成。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型(DLM)的实际可控性仍然很脆弱，并且存在更新遗忘的失效模式。

Method: 提出Token Timestep Allocation (TTA)方法，通过每个token的时间步长计划来实现软性和语义token排序：关键token尽早冻结，而不确定token继续改进。这种基于时间步长的排序可以实例化为固定策略或由任务信号驱动的自适应策略，从而支持广泛的改进策略。

Result: 在情感控制方面，准确率提高了20%以上，困惑度降低了近一半，而步数不到五分之一；在解毒方面，最大毒性降低（12.2 vs 14.5），困惑度降低（26.0 vs 32.0）。

Conclusion: 通过时间步长分配进行软排序是减轻更新遗忘和实现稳定且可控的扩散文本生成的关键。

Abstract: While diffusion language models (DLMs) enable fine-grained refinement, their
practical controllability remains fragile. We identify and formally
characterize a central failure mode called update forgetting, in which uniform
and context agnostic updates induce token level fluctuations across timesteps,
erasing earlier semantic edits and disrupting the cumulative refinement
process, thereby degrading fluency and coherence. As this failure originates in
uniform and context agnostic updates, effective control demands explicit token
ordering. We propose Token Timestep Allocation (TTA), which realizes soft and
semantic token ordering via per token timestep schedules: critical tokens are
frozen early, while uncertain tokens receive continued refinement. This
timestep based ordering can be instantiated as either a fixed policy or an
adaptive policy driven by task signals, thereby supporting a broad spectrum of
refinement strategies. Because it operates purely at inference time, it applies
uniformly across various DLMs and naturally extends to diverse supervision
sources. Empirically, TTA improves controllability and fluency: on sentiment
control, it yields more than 20 percent higher accuracy and nearly halves
perplexity using less than one fifth the steps; in detoxification, it lowers
maximum toxicity (12.2 versus 14.5) and perplexity (26.0 versus 32.0).
Together, these results demonstrate that softened ordering via timestep
allocation is the critical lever for mitigating update forgetting and achieving
stable and controllable diffusion text generation.

</details>


### [30] [What's In My Human Feedback? Learning Interpretable Descriptions of Preference Data](https://arxiv.org/abs/2510.26202)
*Rajiv Movva,Smitha Milli,Sewon Min,Emma Pierson*

Main category: cs.CL

TL;DR: 提出了一种名为WIMHF的方法，通过稀疏自编码器解释人类反馈数据，从而理解人类偏好。


<details>
  <summary>Details</summary>
Motivation: 从业者缺乏对反馈数据编码的清晰理解，导致人类反馈会以不可预测和不良的方式改变语言模型。

Method: 使用稀疏自编码器来表征数据集能够衡量的偏好以及注释者实际表达的偏好。

Result: 在7个数据集上，WIMHF识别出少量可解释的特征，这些特征解释了黑盒模型获得的大部分偏好预测信号。这些特征揭示了人类偏好的多样性以及数据集级别上下文的作用。通过重新标记Arena中的有害示例，可以获得显著的安全性提升（+37%），而不会影响整体性能。在Community Alignment数据集上，学习了针对特定注释者的主观特征权重，从而提高了偏好预测。

Conclusion: WIMHF为从业者提供了一种以人为中心的分析方法，以更好地理解和使用偏好数据。

Abstract: Human feedback can alter language models in unpredictable and undesirable
ways, as practitioners lack a clear understanding of what feedback data
encodes. While prior work studies preferences over certain attributes (e.g.,
length or sycophancy), automatically extracting relevant features without
pre-specifying hypotheses remains challenging. We introduce What's In My Human
Feedback? (WIMHF), a method to explain feedback data using sparse autoencoders.
WIMHF characterizes both (1) the preferences a dataset is capable of measuring
and (2) the preferences that the annotators actually express. Across 7
datasets, WIMHF identifies a small number of human-interpretable features that
account for the majority of the preference prediction signal achieved by
black-box models. These features reveal a wide diversity in what humans prefer,
and the role of dataset-level context: for example, users on Reddit prefer
informality and jokes, while annotators in HH-RLHF and PRISM disprefer them.
WIMHF also surfaces potentially unsafe preferences, such as that LMArena users
tend to vote against refusals, often in favor of toxic content. The learned
features enable effective data curation: re-labeling the harmful examples in
Arena yields large safety gains (+37%) with no cost to general performance.
They also allow fine-grained personalization: on the Community Alignment
dataset, we learn annotator-specific weights over subjective features that
improve preference prediction. WIMHF provides a human-centered analysis method
for practitioners to better understand and use preference data.

</details>


### [31] [Towards Global Retrieval Augmented Generation: A Benchmark for Corpus-Level Reasoning](https://arxiv.org/abs/2510.26205)
*Qi Luo,Xiaonan Li,Tingshuo Fan,Xinchi Chen,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文介绍了 GlobalQA，这是一个用于评估全局 RAG 功能的新基准，并提出了 GlobalRAG，这是一种多工具协同框架，可在全局任务中显著提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有的 RAG 评估基准主要集中在局部 RAG 上，无法满足许多需要跨整个文档集合进行信息聚合和分析的实际应用。

Method: 本文提出了 GlobalRAG，它通过分块检索保持结构连贯性，结合 LLM 驱动的智能过滤器消除噪声文档，并集成聚合模块进行精确的符号计算。

Result: 实验结果表明，现有的 RAG 方法在全局任务上的表现不佳，而 GlobalRAG 在 Qwen2.5-14B 模型上实现了 6.63 的 F1 值，相比之下，最强的基线只有 1.51 的 F1 值。

Conclusion: 本文验证了 GlobalRAG 方法的有效性，表明其能够显著提高全局 RAG 的性能。

Abstract: Retrieval-augmented generation (RAG) has emerged as a leading approach to
reducing hallucinations in large language models (LLMs). Current RAG evaluation
benchmarks primarily focus on what we call local RAG: retrieving relevant
chunks from a small subset of documents to answer queries that require only
localized understanding within specific text chunks. However, many real-world
applications require a fundamentally different capability -- global RAG --
which involves aggregating and analyzing information across entire document
collections to derive corpus-level insights (for example, "What are the top 10
most cited papers in 2023?"). In this paper, we introduce GlobalQA -- the first
benchmark specifically designed to evaluate global RAG capabilities, covering
four core task types: counting, extremum queries, sorting, and top-k
extraction. Through systematic evaluation across different models and
baselines, we find that existing RAG methods perform poorly on global tasks,
with the strongest baseline achieving only 1.51 F1 score. To address these
challenges, we propose GlobalRAG, a multi-tool collaborative framework that
preserves structural coherence through chunk-level retrieval, incorporates
LLM-driven intelligent filters to eliminate noisy documents, and integrates
aggregation modules for precise symbolic computation. On the Qwen2.5-14B model,
GlobalRAG achieves 6.63 F1 compared to the strongest baseline's 1.51 F1,
validating the effectiveness of our method.

</details>


### [32] [Pragmatic Theories Enhance Understanding of Implied Meanings in LLMs](https://arxiv.org/abs/2510.26253)
*Takuma Sato,Seiya Kawano,Koichiro Yoshino*

Main category: cs.CL

TL;DR: 本文提出了一种利用语用理论提示语言模型来理解隐含意义的有效上下文学习方法。


<details>
  <summary>Details</summary>
Motivation: 人类交流和语言使用中，准确理解隐含意义至关重要，语言模型也应具备这种能力。

Method: 将语用理论（如格莱斯语用学和关联理论）概述作为提示呈现给语言模型，引导其逐步推理以得出最终解释。

Result: 实验结果表明，与没有语用理论的baseline相比，该方法在语用推理任务上使语言模型的分数提高了9.6%。即使不解释语用理论的细节，仅在提示中提及它们的名称也能带来一定的性能提升。

Conclusion: 利用语用理论作为prompt可以有效提高语言模型理解隐含意义的能力

Abstract: The ability to accurately interpret implied meanings plays a crucial role in
human communication and language use, and language models are also expected to
possess this capability. This study demonstrates that providing language models
with pragmatic theories as prompts is an effective in-context learning approach
for tasks to understand implied meanings. Specifically, we propose an approach
in which an overview of pragmatic theories, such as Gricean pragmatics and
Relevance Theory, is presented as a prompt to the language model, guiding it
through a step-by-step reasoning process to derive a final interpretation.
Experimental results showed that, compared to the baseline, which prompts
intermediate reasoning without presenting pragmatic theories (0-shot
Chain-of-Thought), our methods enabled language models to achieve up to 9.6\%
higher scores on pragmatic reasoning tasks. Furthermore, we show that even
without explaining the details of pragmatic theories, merely mentioning their
names in the prompt leads to a certain performance improvement (around 1-3%) in
larger models compared to the baseline.

</details>


### [33] [Language Models Are Borrowing-Blind: A Multilingual Evaluation of Loanword Identification across 10 Languages](https://arxiv.org/abs/2510.26254)
*Mérilin Sousa Silva,Sina Ahmadi*

Main category: cs.CL

TL;DR: 大型语言模型在区分外来词和本地词方面表现不佳，表明它们对外来词存在偏见。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型是否具有识别外来词的能力，特别是在双语社区中，强势语言不断向弱势语言施加词汇的情况下。

Method: 在10种语言中评估多个模型，通过明确的指令和上下文信息来判断模型对外来词的识别能力。

Result: 模型在区分外来词和本地词方面表现不佳。

Conclusion: 现代自然语言处理系统对外来词存在偏见，这对为少数民族语言开发自然语言处理工具以及支持在强势语言词汇压力下保护语言具有重要意义。

Abstract: Throughout language history, words are borrowed from one language to another
and gradually become integrated into the recipient's lexicon. Speakers can
often differentiate these loanwords from native vocabulary, particularly in
bilingual communities where a dominant language continuously imposes lexical
items on a minority language. This paper investigates whether pretrained
language models, including large language models, possess similar capabilities
for loanword identification. We evaluate multiple models across 10 languages.
Despite explicit instructions and contextual information, our results show that
models perform poorly in distinguishing loanwords from native ones. These
findings corroborate previous evidence that modern NLP systems exhibit a bias
toward loanwords rather than native equivalents. Our work has implications for
developing NLP tools for minority languages and supporting language
preservation in communities under lexical pressure from dominant languages.

</details>


### [34] [Distilling Multilingual Vision-Language Models: When Smaller Models Stay Multilingual](https://arxiv.org/abs/2510.26271)
*Sukrit Sriratanawilai,Jhayahgrit Thongwat,Romrawin Chumpu,Patomporn Payoungkhamdee,Sarana Nutanong,Peerat Limkonchotiwat*

Main category: cs.CL

TL;DR: 研究了在多语言环境下，知识蒸馏（KD）对视觉-语言模型（VLM）的影响，特别是当模型尺寸缩小时的表现。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型在不同语言上的表现不均衡，模型尺寸缩小时问题更严重。知识蒸馏在将知识从大模型传递到小模型方面有潜力，但在多语言环境下的应用未被充分探索。

Method: 对五种知识蒸馏方法进行了受控的实证研究，分析了它们对跨语言表示一致性和下游性能稳定性的影响。研究对象包括CLIP和SigLIP2，并在领域内检索和领域外视觉问答上进行了评估。

Result: 发现某些配置可以在模型尺寸减半的情况下保持甚至提高多语言检索的鲁棒性，但其他配置无法保持跨任务稳定性，揭示了设计敏感的权衡。

Conclusion: 知识蒸馏在多语言VLM中应用时，需要在模型大小、跨语言检索鲁棒性和跨任务稳定性之间进行权衡。

Abstract: Vision-language models (VLMs) exhibit uneven performance across languages, a
problem that is often exacerbated when the model size is reduced. While
Knowledge distillation (KD) demonstrates promising results in transferring
knowledge from larger to smaller VLMs, applying KD in multilingualism is an
underexplored area. This paper presents a controlled empirical study of KD
behavior across five distillation approaches, isolating their effects on
cross-lingual representation consistency and downstream performance stability
under model compression. We study five distillation formulations across CLIP
and SigLIP2, and evaluate them on in-domain retrieval and out-of-domain visual
QA. We find that some configurations preserve or even improve multilingual
retrieval robustness despite halving model size, but others fail to maintain
cross-task stability, exposing design-sensitive trade-offs that aggregate
accuracy alone does not reveal.

</details>


### [35] [Do LLMs Signal When They're Right? Evidence from Neuron Agreement](https://arxiv.org/abs/2510.26277)
*Kang Chen,Yaoning Wang,Kai Xiong,Zhuoka Feng,Wenhe Sun,Haotian Chen,Yixin Cao*

Main category: cs.CL

TL;DR: 提出了一种名为神经元一致性解码 (NAD) 的无监督 best-of-N 方法，该方法使用激活稀疏性和交叉样本神经元一致性来选择候选者，仅在内部信号上运行，而不需要可比较的文本输出。


<details>
  <summary>Details</summary>
Motivation: 现有策略仅使用外部输出（例如 token 概率、熵或自我评估）对候选者进行评分，并且这些信号在后训练后可能校准不良。分析基于神经元激活的内部行为，并揭示了三个发现：（1）外部信号是更丰富的内部动力学的低维投影；（2）正确的反应比整个生成过程中不正确的反应激活的独特神经元数量明显更少；（3）来自正确反应的激活表现出更强的交叉样本一致性，而不正确的反应则不同。

Method: 提出了神经元一致性解码 (NAD)，这是一种无监督的 best-of-N 方法，它使用激活稀疏性和交叉样本神经元一致性来选择候选者，仅在内部信号上运行，而不需要可比较的文本输出。NAD 能够在前 32 个生成的 token 中进行早期正确性预测，并支持积极的提前停止。

Result: 在具有可验证答案的数学和科学基准测试中，NAD 与多数投票相匹配；在多数投票不适用的开放式编码基准测试中，NAD 始终优于 Avg@64。通过提前修剪没有希望的轨迹，NAD 将 token 使用量减少了 99%，而生成质量的损失极小，这表明内部信号为无标签集成解码提供了可靠、可扩展且高效的指导。

Conclusion: 内部信号为无标签集成解码提供了可靠、可扩展且高效的指导。

Abstract: Large language models (LLMs) commonly boost reasoning via
sample-evaluate-ensemble decoders, achieving label free gains without ground
truth. However, prevailing strategies score candidates using only external
outputs such as token probabilities, entropies, or self evaluations, and these
signals can be poorly calibrated after post training. We instead analyze
internal behavior based on neuron activations and uncover three findings: (1)
external signals are low dimensional projections of richer internal dynamics;
(2) correct responses activate substantially fewer unique neurons than
incorrect ones throughout generation; and (3) activations from correct
responses exhibit stronger cross sample agreement, whereas incorrect ones
diverge. Motivated by these observations, we propose Neuron Agreement Decoding
(NAD), an unsupervised best-of-N method that selects candidates using
activation sparsity and cross sample neuron agreement, operating solely on
internal signals and without requiring comparable textual outputs. NAD enables
early correctness prediction within the first 32 generated tokens and supports
aggressive early stopping. Across math and science benchmarks with verifiable
answers, NAD matches majority voting; on open ended coding benchmarks where
majority voting is inapplicable, NAD consistently outperforms Avg@64. By
pruning unpromising trajectories early, NAD reduces token usage by 99% with
minimal loss in generation quality, showing that internal signals provide
reliable, scalable, and efficient guidance for label free ensemble decoding.

</details>


### [36] [Unravelling the Mechanisms of Manipulating Numbers in Language Models](https://arxiv.org/abs/2510.26285)
*Michal Štefánik,Timothee Mickus,Marek Kadlčík,Bertram Højer,Michal Spiegel,Raúl Vázquez,Aman Sinha,Josef Kuchař,Philipp Mondorf*

Main category: cs.CL

TL;DR: 大型语言模型(LLM)在处理数字时，输入嵌入表示收敛到相似且准确的状态，但输出仍然容易出错。本研究旨在解释这种冲突，通过探索语言模型如何处理数字，并量化这些机制的准确性下限。


<details>
  <summary>Details</summary>
Motivation: 解释大型语言模型在处理数字时，输入嵌入表示准确但输出容易出错的冲突。

Method: 探索语言模型如何处理数字，并量化这些机制的准确性下限。创建通用探针来追踪信息，包括输出错误的原因，到特定层。

Result: 不同语言模型学习到的数字表示是可互换的，具有系统性、高准确性和通用性，且与隐藏状态和输入上下文的类型无关。可以创建通用探针，并追踪信息到特定层。

Conclusion: 为预训练的LLM如何处理数字奠定了基础，并概述了更精确的探测技术在改进LLM架构方面的潜力。

Abstract: Recent work has shown that different large language models (LLMs) converge to
similar and accurate input embedding representations for numbers. These
findings conflict with the documented propensity of LLMs to produce erroneous
outputs when dealing with numeric information. In this work, we aim to explain
this conflict by exploring how language models manipulate numbers and quantify
the lower bounds of accuracy of these mechanisms. We find that despite
surfacing errors, different language models learn interchangeable
representations of numbers that are systematic, highly accurate and universal
across their hidden states and the types of input contexts. This allows us to
create universal probes for each LLM and to trace information -- including the
causes of output errors -- to specific layers. Our results lay a fundamental
understanding of how pre-trained LLMs manipulate numbers and outline the
potential of more accurate probing techniques in addressed refinements of LLMs'
architectures.

</details>


### [37] [Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in Web Games](https://arxiv.org/abs/2510.26298)
*Jingran Zhang,Ning Li,Justin Cui*

Main category: cs.CL

TL;DR: OpenAI的ChatGPT Atlas可以通过网络互动分析网页，处理用户意图，并在浏览器中直接执行光标和键盘输入。该研究评估了Atlas在基于浏览器的游戏中的网络互动能力，包括Google的T-Rex Runner, Sudoku, Flappy Bird, 和 Stein.world。


<details>
  <summary>Details</summary>
Motivation: 尽管Atlas在信息检索任务中的能力已经得到证实，但其在动态、交互式环境中的性能仍有待探索。本研究旨在初步评估Atlas的网络互动能力。

Method: 使用游戏中的性能得分作为定量指标，评估Atlas在不同任务类型中的表现。

Result: Atlas在数独等逻辑推理任务中表现出色，完成谜题的速度明显快于人类，但在需要精确 timing 和运动控制的实时游戏中表现不佳，通常无法超越初始障碍。

Conclusion: 虽然Atlas展示了强大的分析处理能力，但在需要实时互动的动态网络环境中仍然存在明显的局限性。

Abstract: OpenAI's ChatGPT Atlas introduces new capabilities for web interaction,
enabling the model to analyze webpages, process user intents, and execute
cursor and keyboard inputs directly within the browser. While its capacity for
information retrieval tasks has been demonstrated, its performance in dynamic,
interactive environments remains less explored. In this study, we conduct an
early evaluation of Atlas's web interaction capabilities using browser-based
games as test scenarios, including Google's T-Rex Runner, Sudoku, Flappy Bird,
and Stein.world. We employ in-game performance scores as quantitative metrics
to assess performance across different task types. Our results show that Atlas
performs strongly in logical reasoning tasks like Sudoku, completing puzzles
significantly faster than human baselines, but struggles substantially in
real-time games requiring precise timing and motor control, often failing to
progress beyond initial obstacles. These findings suggest that while Atlas
demonstrates capable analytical processing, there remain notable limitations in
dynamic web environments requiring real-time interaction. The website of our
project can be found at https://atlas-game-eval.github.io.

</details>


### [38] [SCRIBE: Structured Chain Reasoning for Interactive Behaviour Explanations using Tool Calling](https://arxiv.org/abs/2510.26322)
*Fares Fawzi,Vinitra Swamy,Dominik Glandorf,Tanya Nazaretsky,Tanja Käser*

Main category: cs.CL

TL;DR: SCRIBE是一个用于生成学生反馈的框架，它结合了领域特定工具和自反推理流程，可以在资源有限的情况下运行，并保证输出的正确性。


<details>
  <summary>Details</summary>
Motivation: 在教育领域，使用语言模型提供互动式、个性化的学生反馈面临隐私、计算资源和教学有效性三大挑战。需要小型、开源的模型，可以在本地运行，并可靠地将输出建立在正确的信息基础上。

Method: SCRIBE框架结合了领域特定工具和一个自反推理流程，支持迭代推理、工具使用和错误恢复。通过在合成GPT-4o生成的数据上进行两阶段LoRA微调，将这些能力提炼到3B和8B模型中。

Result: 8B-SCRIBE模型在相关性和可操作性等关键维度上达到了与更大模型相当或更高的质量，并且学生认为其与GPT-4o和Llama-3.3 70B相当。

Conclusion: SCRIBE在低资源、隐私敏感的教育应用中是可行的。

Abstract: Language models can be used to provide interactive, personalized student
feedback in educational settings. However, real-world deployment faces three
key challenges: privacy concerns, limited computational resources, and the need
for pedagogically valid responses. These constraints require small, open-source
models that can run locally and reliably ground their outputs in correct
information. We introduce SCRIBE, a framework for multi-hop, tool-augmented
reasoning designed to generate valid responses to student questions about
feedback reports. SCRIBE combines domain-specific tools with a self-reflective
inference pipeline that supports iterative reasoning, tool use, and error
recovery. We distil these capabilities into 3B and 8B models via two-stage LoRA
fine-tuning on synthetic GPT-4o-generated data. Evaluation with a human-aligned
GPT-Judge and a user study with 108 students shows that 8B-SCRIBE models
achieve comparable or superior quality to much larger models in key dimensions
such as relevance and actionability, while being perceived on par with GPT-4o
and Llama-3.3 70B by students. These findings demonstrate the viability of
SCRIBE for low-resource, privacy-sensitive educational applications.

</details>


### [39] [From Amateur to Master: Infusing Knowledge into LLMs via Automated Curriculum Learning](https://arxiv.org/abs/2510.26336)
*Nishit Neema,Srinjoy Mukherjee,Sapan Shah,Gokul Ramakrishnan,Ganesh Venkatesh*

Main category: cs.CL

TL;DR: ACER improves LLMs in specialized domains without sacrificing general capabilities by creating a textbook-style curriculum and using it for continual pretraining.


<details>
  <summary>Details</summary>
Motivation: LLMs underperform in specialized domains requiring deep understanding.

Method: ACER synthesizes a curriculum by generating a table of contents and QA pairs based on Bloom's taxonomy, then uses it for continual pretraining with an interleaved schedule.

Result: ACER improves performance on specialized MMLU subsets (e.g., microeconomics by 5%), maintains or improves performance on general benchmarks, and facilitates cross-domain knowledge transfer.

Conclusion: ACER is a scalable and effective method for improving LLMs in specialized domains.

Abstract: Large Language Models (LLMs) excel at general tasks but underperform in
specialized domains like economics and psychology, which require deep,
principled understanding. To address this, we introduce ACER (Automated
Curriculum-Enhanced Regimen) that transforms generalist models into domain
experts without sacrificing their broad capabilities. ACER first synthesizes a
comprehensive, textbook-style curriculum by generating a table of contents for
a subject and then creating question-answer (QA) pairs guided by Bloom's
taxonomy. This ensures systematic topic coverage and progressively increasing
difficulty. The resulting synthetic corpus is used for continual pretraining
with an interleaved curriculum schedule, aligning learning across both content
and cognitive dimensions.
  Experiments with Llama 3.2 (1B and 3B) show significant gains in specialized
MMLU subsets. In challenging domains like microeconomics, where baselines
struggle, ACER boosts accuracy by 5 percentage points. Across all target
domains, we observe a consistent macro-average improvement of 3 percentage
points. Notably, ACER not only prevents catastrophic forgetting but also
facilitates positive cross-domain knowledge transfer, improving performance on
non-target domains by 0.7 points. Beyond MMLU, ACER enhances performance on
knowledge-intensive benchmarks like ARC and GPQA by over 2 absolute points,
while maintaining stable performance on general reasoning tasks. Our results
demonstrate that ACER offers a scalable and effective recipe for closing
critical domain gaps in LLMs.

</details>


### [40] [MisSynth: Improving MISSCI Logical Fallacies Classification with Synthetic Data](https://arxiv.org/abs/2510.26345)
*Mykhailo Poliakov,Nadiya Shvai*

Main category: cs.CL

TL;DR: 本研究探讨了使用合成数据和轻量级微调技术来提高大型语言模型识别错误论证的能力，尤其是在与健康相关的错误信息领域。


<details>
  <summary>Details</summary>
Motivation: 识别歪曲或误解科学发现的错误信息非常困难，且危害很大。

Method: 提出了一个名为MisSynth的流程，该流程应用检索增强生成（RAG）来生成合成的谬误样本，然后使用这些样本来微调LLM模型。

Result: 结果表明，与原始基线相比，微调后的模型在准确性方面有显著提高。例如，经过微调的LLaMA 3.1 8B模型在MISSCI测试集上的F1分数绝对提高了35%以上。

Conclusion: 引入合成的谬误数据以扩充有限的带注释资源可以显著提高LLM在现实世界科学错误信息任务中的zero-shot分类性能，即使计算资源有限。

Abstract: Health-related misinformation is very prevalent and potentially harmful. It
is difficult to identify, especially when claims distort or misinterpret
scientific findings. We investigate the impact of synthetic data generation and
lightweight fine-tuning techniques on the ability of large language models
(LLMs) to recognize fallacious arguments using the MISSCI dataset and
framework. In this work, we propose MisSynth, a pipeline that applies
retrieval-augmented generation (RAG) to produce synthetic fallacy samples,
which are then used to fine-tune an LLM model. Our results show substantial
accuracy gains with fine-tuned models compared to vanilla baselines. For
instance, the LLaMA 3.1 8B fine-tuned model achieved an over 35% F1-score
absolute improvement on the MISSCI test split over its vanilla baseline. We
demonstrate that introducing synthetic fallacy data to augment limited
annotated resources can significantly enhance zero-shot LLM classification
performance on real-world scientific misinformation tasks, even with limited
computational resources. The code and synthetic dataset are available on
https://github.com/mxpoliakov/MisSynth.

</details>


### [41] [The Geometry of Dialogue: Graphing Language Models to Reveal Synergistic Teams for Multi-Agent Collaboration](https://arxiv.org/abs/2510.26352)
*Kotaro Furuya,Yuichi Kitagawa*

Main category: cs.CL

TL;DR: 提出了一种基于交互的框架，用于自动组建大型语言模型（LLM）团队，无需先验知识，通过模型间的对话语义连贯性构建“语言模型图”，并应用社区检测来识别协同模型集群。


<details>
  <summary>Details</summary>
Motivation: 为了超越单一模型的能力，基于大型语言模型（LLM）的多智能体方法代表了一种有希望的策略，但其成功关键取决于协同的团队组成。然而，形成最佳团队是一个重大挑战，因为大多数模型固有的不透明性掩盖了有效协作所需的内部特征。

Method: 构建一个“语言模型图”，该图映射模型之间来自成对对话的语义连贯性的关系，然后应用社区检测来识别协同模型集群。

Result: 实验表明，该方法发现的功能连贯的群体反映了它们潜在的专业化。用特定主题启动对话，识别出在下游基准测试中优于随机基线的协同团队，并实现了与基于已知模型专业化的人工管理团队相当的准确性。

Conclusion: 研究结果为协作多智能体LLM团队的自动设计提供了新的基础。

Abstract: While a multi-agent approach based on large language models (LLMs) represents
a promising strategy to surpass the capabilities of single models, its success
is critically dependent on synergistic team composition. However, forming
optimal teams is a significant challenge, as the inherent opacity of most
models obscures the internal characteristics necessary for effective
collaboration. In this paper, we propose an interaction-centric framework for
automatic team composition that does not require any prior knowledge including
their internal architectures, training data, or task performances. Our method
constructs a "language model graph" that maps relationships between models from
the semantic coherence of pairwise conversations, and then applies community
detection to identify synergistic model clusters. Our experiments with diverse
LLMs demonstrate that the proposed method discovers functionally coherent
groups that reflect their latent specializations. Priming conversations with
specific topics identified synergistic teams which outperform random baselines
on downstream benchmarks and achieve comparable accuracy to that of
manually-curated teams based on known model specializations. Our findings
provide a new basis for the automated design of collaborative multi-agent LLM
teams.

</details>


### [42] [On the Role of Context for Discourse Relation Classification in Scientific Writing](https://arxiv.org/abs/2510.26354)
*Stephen Wan,Wei Liu,Michael Strube*

Main category: cs.CL

TL;DR: 本文探讨了使用话语级别信息为人工智能生成的科学主张寻找支持证据，重点研究了科学写作中推理话语结构的任务。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能（AI）方法在科学工作流程中使用的增加，我们对使用语篇层面信息来寻找支持 AI 生成的科学主张的证据感兴趣。

Method: 本文对预训练语言模型（PLM）和大型语言模型（LLM）方法进行了初步研究，用于语篇关系分类（DRC），重点关注科学出版物这一任务中研究不足的类型。我们研究了上下文如何帮助 DRC 任务，实验表明，由语篇结构定义的上下文通常是有帮助的。

Result: 我们的实验表明，语篇结构定义的上下文通常是有帮助的。

Conclusion: 我们还分析了哪些科学语篇关系类型可能从上下文中获益最多。

Abstract: With the increasing use of generative Artificial Intelligence (AI) methods to
support science workflows, we are interested in the use of discourse-level
information to find supporting evidence for AI generated scientific claims. A
first step towards this objective is to examine the task of inferring discourse
structure in scientific writing.
  In this work, we present a preliminary investigation of pretrained language
model (PLM) and Large Language Model (LLM) approaches for Discourse Relation
Classification (DRC), focusing on scientific publications, an under-studied
genre for this task. We examine how context can help with the DRC task, with
our experiments showing that context, as defined by discourse structure, is
generally helpful. We also present an analysis of which scientific discourse
relation types might benefit most from context.

</details>


### [43] [OmniEduBench: A Comprehensive Chinese Benchmark for Evaluating Large Language Models in Education](https://arxiv.org/abs/2510.26422)
*Min Zhang,Hao Chen,Hao Chen,Wenqi Zhang,Didi Zhu,Xin Lin,Bo Jiang,Aimin Zhou,Fei Wu,Kun Kuang*

Main category: cs.CL

TL;DR: OmniEduBench：一个综合性的中文教育基准，包含24.602K高质量问答对，涵盖知识和能力维度，以及11种常见题型。


<details>
  <summary>Details</summary>
Motivation: 现有LLM主要关注知识维度，缺乏对实际教育场景中能力培养的评估，且基准测试缺乏多样性，尤其是在中文语境下。

Method: 构建OmniEduBench基准，包含知识和能力两个维度，细分为61个不同科目和11种题型。

Result: 在11个主流LLM上的实验表明，LLM在知识维度和能力维度上都存在明显的性能差距，尤其是在能力维度上与人类智能差距较大。

Conclusion: LLM在教育领域的应用仍有很大的提升空间，面临诸多挑战。

Abstract: With the rapid development of large language models (LLMs), various LLM-based
works have been widely applied in educational fields. However, most existing
LLMs and their benchmarks focus primarily on the knowledge dimension, largely
neglecting the evaluation of cultivation capabilities that are essential for
real-world educational scenarios. Additionally, current benchmarks are often
limited to a single subject or question type, lacking sufficient diversity.
This issue is particularly prominent within the Chinese context. To address
this gap, we introduce OmniEduBench, a comprehensive Chinese educational
benchmark. OmniEduBench consists of 24.602K high-quality question-answer pairs.
The data is meticulously divided into two core dimensions: the knowledge
dimension and the cultivation dimension, which contain 18.121K and 6.481K
entries, respectively. Each dimension is further subdivided into 6 fine-grained
categories, covering a total of 61 different subjects (41 in the knowledge and
20 in the cultivation). Furthermore, the dataset features a rich variety of
question formats, including 11 common exam question types, providing a solid
foundation for comprehensively evaluating LLMs' capabilities in education.
Extensive experiments on 11 mainstream open-source and closed-source LLMs
reveal a clear performance gap. In the knowledge dimension, only Gemini-2.5 Pro
surpassed 60\% accuracy, while in the cultivation dimension, the
best-performing model, QWQ, still trailed human intelligence by nearly 30\%.
These results highlight the substantial room for improvement and underscore the
challenges of applying LLMs in education.

</details>


### [44] [1+1>2: A Synergistic Sparse and Low-Rank Compression Method for Large Language Models](https://arxiv.org/abs/2510.26446)
*Zeliang Zong,Kai Zhang,Zheyang Li,Wenming Tan,Ye Ren,Yiyan Zhai,Jilin Hu*

Main category: cs.CL

TL;DR: 提出了一种名为SSLC的协同稀疏和低秩压缩方法，用于压缩大型语言模型(LLM)。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然在语言理解和生成方面表现出色，但其广泛应用受到带宽和计算需求的限制。单独的剪枝和低秩近似方法已经显示出前景，但它们在LLM中的协同作用仍有待探索。

Method: SSLC方法结合了低秩近似和稀疏优化的优点，通过迭代优化算法解决统一问题。

Result: 在LLaMA和Qwen2.5模型上的实验表明，SSLC在没有任何额外训练步骤的情况下，始终优于单独的方法，达到最先进的结果。SSLC在不降低性能的情况下将Qwen2.5压缩了50%，并实现了至少1.63倍的加速。

Conclusion: SSLC为高效LLM部署提供了一种实用的解决方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in
language comprehension and generation; however, their widespread adoption is
constrained by substantial bandwidth and computational demands. While pruning
and low-rank approximation have each demonstrated promising performance
individually, their synergy for LLMs remains underexplored. We introduce
\underline{S}ynergistic \underline{S}parse and \underline{L}ow-Rank
\underline{C}ompression (SSLC) methods for LLMs, which leverages the strengths
of both techniques: low-rank approximation compresses the model by retaining
its essential structure with minimal information loss, whereas sparse
optimization eliminates non-essential weights, preserving those crucial for
generalization. Based on theoretical analysis, we first formulate the low-rank
approximation and sparse optimization as a unified problem and solve it by
iterative optimization algorithm. Experiments on LLaMA and Qwen2.5 models
(7B-70B) show that SSLC, without any additional training steps, consistently
surpasses standalone methods, achieving state-of-the-arts results. Notably,
SSLC compresses Qwen2.5 by 50\% with no performance drop and achieves at least
1.63$\times$ speedup, offering a practical solution for efficient LLM
deployment.

</details>


### [45] [Bayesian Network Fusion of Large Language Models for Sentiment Analysis](https://arxiv.org/abs/2510.26484)
*Rasoul Amirzadeh,Dhananjay Thiruvady,Fatemeh Shiri*

Main category: cs.CL

TL;DR: 提出了贝叶斯网络LLM融合（BNLF）框架，用于情感分析。


<details>
  <summary>Details</summary>
Motivation: 现有领域特定的大型语言模型缺乏透明性和可解释性，微调成本高，需要大量的提示工程，跨领域结果不一致，且计算需求高，对环境有显著不利影响。

Method: 通过概率机制整合FinBERT、RoBERTa和BERTweet三个LLM的预测结果。BNLF通过将多个LLM的情感预测建模为贝叶斯网络中的概率节点来实现后期融合。

Result: 在三个具有不同语言和上下文特征的人工标注金融语料库上进行评估，BNLF的准确率比基线LLM始终提高约6%。

Conclusion: BNLF对数据集变异具有鲁棒性，且概率融合对于可解释的情感分类是有效的。

Abstract: Large language models (LLMs) continue to advance, with an increasing number
of domain-specific variants tailored for specialised tasks. However, these
models often lack transparency and explainability, can be costly to fine-tune,
require substantial prompt engineering, yield inconsistent results across
domains, and impose significant adverse environmental impact due to their high
computational demands. To address these challenges, we propose the Bayesian
network LLM fusion (BNLF) framework, which integrates predictions from three
LLMs, including FinBERT, RoBERTa, and BERTweet, through a probabilistic
mechanism for sentiment analysis. BNLF performs late fusion by modelling the
sentiment predictions from multiple LLMs as probabilistic nodes within a
Bayesian network. Evaluated across three human-annotated financial corpora with
distinct linguistic and contextual characteristics, BNLF demonstrates
consistent gains of about six percent in accuracy over the baseline LLMs,
underscoring its robustness to dataset variability and the effectiveness of
probabilistic fusion for interpretable sentiment classification.

</details>


### [46] [A Multi-agent Large Language Model Framework to Automatically Assess Performance of a Clinical AI Triage Tool](https://arxiv.org/abs/2510.26498)
*Adam E. Flanders,Yifan Peng,Luciano Prevedello,Robyn Ball,Errol Colak,Prahlad Menon,George Shih,Hui-Ming Lin,Paras Lakhani*

Main category: cs.CL

TL;DR: 本研究旨在评估使用多个大型语言模型（LLM）集成评估基于像素的AI分诊工具的可靠性，并与单个LLM进行比较。


<details>
  <summary>Details</summary>
Motivation: 确定单个LLM评估AI分诊工具可能不够可靠，因此探索LLM集成是否能提供更可靠的评估。

Method: 使用来自14家医院的29,766份非对比CT头部检查，通过商业颅内出血（ICH）AI检测工具处理，并使用八个开源LLM模型和一个内部GPT-4o模型分析放射学报告，评估ICH的存在。比较了八个开源模型的性能和共识与GPT-4o的性能。

Result: Llama3.3:70b和GPT-4o的AUC最高（均为0.78）。Llama3.3:70b的F1得分最高（0.81），召回率最高（0.85）。理想的LLM组合为：完整9模型集成、前3模型集成和共识模型。Top-3、Full-9和共识之间未观察到统计学上的显著差异。

Conclusion: 中大型开源LLM集成提供了一种更一致和可靠的方法，可以对临床AI分诊工具进行回顾性评估，优于单个LLM。

Abstract: Purpose: The purpose of this study was to determine if an ensemble of
multiple LLM agents could be used collectively to provide a more reliable
assessment of a pixel-based AI triage tool than a single LLM.
  Methods: 29,766 non-contrast CT head exams from fourteen hospitals were
processed by a commercial intracranial hemorrhage (ICH) AI detection tool.
Radiology reports were analyzed by an ensemble of eight open-source LLM models
and a HIPAA compliant internal version of GPT-4o using a single multi-shot
prompt that assessed for presence of ICH. 1,726 examples were manually
reviewed. Performance characteristics of the eight open-source models and
consensus were compared to GPT-4o. Three ideal consensus LLM ensembles were
tested for rating the performance of the triage tool.
  Results: The cohort consisted of 29,766 head CTs exam-report pairs. The
highest AUC performance was achieved with llama3.3:70b and GPT-4o (AUC= 0.78).
The average precision was highest for Llama3.3:70b and GPT-4o (AP=0.75 & 0.76).
Llama3.3:70b had the highest F1 score (0.81) and recall (0.85), greater
precision (0.78), specificity (0.72), and MCC (0.57). Using MCC (95% CI) the
ideal combination of LLMs were: Full-9 Ensemble 0.571 (0.552-0.591), Top-3
Ensemble 0.558 (0.537-0.579), Consensus 0.556 (0.539-0.574), and GPT4o 0.522
(0.500-0.543). No statistically significant differences were observed between
Top-3, Full-9, and Consensus (p > 0.05).
  Conclusion: An ensemble of medium to large sized open-source LLMs provides a
more consistent and reliable method to derive a ground truth retrospective
evaluation of a clinical AI triage tool over a single LLM alone.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [47] [Enhancing Underwater Object Detection through Spatio-Temporal Analysis and Spatial Attention Networks](https://arxiv.org/abs/2510.25797)
*Sai Likhith Karri,Ansh Saxena*

Main category: cs.CV

TL;DR: 该研究探索了时空建模和空间注意力机制在水下物体检测深度学习模型中的有效性。


<details>
  <summary>Details</summary>
Motivation: 水下物体检测在动态海洋环境中面临挑战，例如突然移动、部分遮挡和渐进运动。

Method: 该研究分两个阶段进行：1) 评估时间增强的 YOLOv5 变体 T-YOLOv5 的性能；2) 通过添加卷积块注意力模块 (CBAM) 开发 T-YOLOv5 的增强版本。

Result: T-YOLOv5 和带有 CBAM 的 T-YOLOv5 的 mAP@50-95 分数分别为 0.813 和 0.811，优于 YOLOv5 的 0.563。

Conclusion: T-YOLOv5 显著提高了检测可靠性，而带有 CBAM 的 T-YOLOv5 在复杂场景中进一步提高了性能，但在简单场景中损失了一些准确性。

Abstract: This study examines the effectiveness of spatio-temporal modeling and the
integration of spatial attention mechanisms in deep learning models for
underwater object detection. Specifically, in the first phase, the performance
of temporal-enhanced YOLOv5 variant T-YOLOv5 is evaluated, in comparison with
the standard YOLOv5. For the second phase, an augmented version of T-YOLOv5 is
developed, through the addition of a Convolutional Block Attention Module
(CBAM). By examining the effectiveness of the already pre-existing YOLOv5 and
T-YOLOv5 models and of the newly developed T-YOLOv5 with CBAM. With CBAM, the
research highlights how temporal modeling improves detection accuracy in
dynamic marine environments, particularly under conditions of sudden movements,
partial occlusions, and gradual motion. The testing results showed that YOLOv5
achieved a mAP@50-95 of 0.563, while T-YOLOv5 and T-YOLOv5 with CBAM
outperformed with mAP@50-95 scores of 0.813 and 0.811, respectively,
highlighting their superior accuracy and generalization in detecting complex
objects. The findings demonstrate that T-YOLOv5 significantly enhances
detection reliability compared to the standard model, while T-YOLOv5 with CBAM
further improves performance in challenging scenarios, although there is a loss
of accuracy when it comes to simpler scenarios.

</details>


### [48] [MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and efficiency](https://arxiv.org/abs/2510.25897)
*Nicolas Dufour,Lucas Degeorge,Arijit Ghosh,Vicky Kalogeiton,David Picard*

Main category: cs.CV

TL;DR: 提出了MIRO，一种在训练期间基于多个奖励模型调节模型的方法，以直接学习用户偏好。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型在大型未策划数据集上训练，无法很好地与用户偏好对齐；使用奖励模型进行后处理会损害多样性、语义保真度和效率。

Method: 在训练期间，模型以多个奖励模型为条件。

Result: 在 GenEval 合成基准和用户偏好分数上实现了最先进的性能。

Conclusion: MIRO 不仅显著提高了生成图像的视觉质量，还显著加快了训练速度。

Abstract: Current text-to-image generative models are trained on large uncurated
datasets to enable diverse generation capabilities. However, this does not
align well with user preferences. Recently, reward models have been
specifically designed to perform post-hoc selection of generated images and
align them to a reward, typically user preference. This discarding of
informative data together with the optimizing for a single reward tend to harm
diversity, semantic fidelity and efficiency. Instead of this post-processing,
we propose to condition the model on multiple reward models during training to
let the model learn user preferences directly. We show that this not only
dramatically improves the visual quality of the generated images but it also
significantly speeds up the training. Our proposed method, called MIRO,
achieves state-of-the-art performances on the GenEval compositional benchmark
and user-preference scores (PickAScore, ImageReward, HPSv2).

</details>


### [49] [BikeScenes: Online LiDAR Semantic Segmentation for Bicycles](https://arxiv.org/abs/2510.25901)
*Denniz Goren,Holger Caesar*

Main category: cs.CV

TL;DR: 针对日益流行的电动自行车带来的骑行者安全问题，本文提出了一种适用于自行车的3D激光雷达分割方法。


<details>
  <summary>Details</summary>
Motivation: 随着速度更快的电动自行车的日益普及，骑行者的脆弱性日益加剧，这促使人们调整汽车感知技术以提高自行车安全性。

Method: 我们使用多传感器“SenseBike”研究平台来开发和评估专为自行车量身定制的 3D 激光雷达分割方法。为了弥合汽车到自行车的领域差距，我们引入了新型 BikeScenes-lidarseg 数据集，该数据集包含代尔夫特理工大学校园周围的 3021 个连续激光雷达扫描，并针对 29 个动态和静态类进行了语义注释。

Result: 通过评估模型性能，我们证明，在我们的 BikeScenes 数据集上进行微调可实现 63.6% 的平均 Intersection-over-Union (mIoU)，明显优于仅使用 SemanticKITTI 预训练获得的 13.8%。

Conclusion: 这一结果强调了特定领域训练的必要性和有效性。我们强调了特定于自行车安装的、硬件受限的感知系统的关键挑战，并将 BikeScenes 数据集作为推进以骑自行车者为中心的激光雷达分割研究的资源。

Abstract: The vulnerability of cyclists, exacerbated by the rising popularity of faster
e-bikes, motivates adapting automotive perception technologies for bicycle
safety. We use our multi-sensor 'SenseBike' research platform to develop and
evaluate a 3D LiDAR segmentation approach tailored to bicycles. To bridge the
automotive-to-bicycle domain gap, we introduce the novel BikeScenes-lidarseg
Dataset, comprising 3021 consecutive LiDAR scans around the university campus
of the TU Delft, semantically annotated for 29 dynamic and static classes. By
evaluating model performance, we demonstrate that fine-tuning on our BikeScenes
dataset achieves a mean Intersection-over-Union (mIoU) of 63.6%, significantly
outperforming the 13.8% obtained with SemanticKITTI pre-training alone. This
result underscores the necessity and effectiveness of domain-specific training.
We highlight key challenges specific to bicycle-mounted, hardware-constrained
perception systems and contribute the BikeScenes dataset as a resource for
advancing research in cyclist-centric LiDAR segmentation.

</details>


### [50] [Generative Image Restoration and Super-Resolution using Physics-Informed Synthetic Data for Scanning Tunneling Microscopy](https://arxiv.org/abs/2510.25921)
*Nikola L. Kolev,Tommaso Rodani,Neil J. Curson,Taylor J. Z. Stock,Alberto Cazzaniga*

Main category: cs.CV

TL;DR: 提出了一种机器学习（ML）方法来进行图像修复和超分辨率，以应对扫描隧道显微镜（STM）中存在的探针退化和串行数据采集缓慢的问题。


<details>
  <summary>Details</summary>
Motivation: 扫描隧道显微镜(STM) 实现了原子分辨率成像和原子操纵，但其效用通常受到探针退化和慢速串行数据采集的限制。制造增加了另一层复杂性，因为探针通常会受到大的电压，这可能会改变其顶点的形状，需要对其进行调节。

Method: 使用仅包含 36 个 Si(001):H 原始实验图像的数据集，证明了物理信息合成数据生成管道可用于训练多个最先进的 Flow-Matching 和扩散模型。

Result: 通过 CLIP 最大平均差异 (CMMD) 分数和结构相似性等指标进行的定量评估表明，我们的模型能够有效地恢复图像，并通过从稀疏采样数据中准确重建图像，从而将图像采集时间缩短两到四倍。

Conclusion: 我们的框架有潜力通过提供减少探针调节程序频率和提高现有高速 STM 系统中的帧速率的途径来显着提高 STM 实验吞吐量。

Abstract: Scanning tunnelling microscopy (STM) enables atomic-resolution imaging and
atom manipulation, but its utility is often limited by tip degradation and slow
serial data acquisition. Fabrication adds another layer of complexity since the
tip is often subjected to large voltages, which may alter the shape of its
apex, requiring it to be conditioned. Here, we propose a machine learning (ML)
approach for image repair and super-resolution to alleviate both challenges.
Using a dataset of only 36 pristine experimental images of Si(001):H, we
demonstrate that a physics-informed synthetic data generation pipeline can be
used to train several state-of-the-art flow-matching and diffusion models.
Quantitative evaluation with metrics such as the CLIP Maximum Mean Discrepancy
(CMMD) score and structural similarity demonstrates that our models are able to
effectively restore images and offer a two- to fourfold reduction in image
acquisition time by accurately reconstructing images from sparsely sampled
data. Our framework has the potential to significantly increase STM
experimental throughput by offering a route to reducing the frequency of
tip-conditioning procedures and to enhancing frame rates in existing high-speed
STM systems.

</details>


### [51] [SplitFlow: Flow Decomposition for Inversion-Free Text-to-Image Editing](https://arxiv.org/abs/2510.25970)
*Sung-Hoon Yoon,Minghan Li,Gaspard Beaudouin,Congcong Wen,Muhammad Rafay Azhar,Mengyu Wang*

Main category: cs.CV

TL;DR: 提出了一种基于流分解和聚合的框架，用于图像编辑，解决了反演不准确和梯度纠缠问题。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑方法存在反演过程不准确和梯度纠缠问题，导致编辑结果不理想。

Method: 将目标提示分解为多个子提示，为每个子提示计算独立的流，然后聚合这些流以形成统一的编辑轨迹。设计了一种投影和软聚合机制，自适应地加权子目标速度场，抑制语义冗余，同时强调不同的方向。

Result: 实验结果表明，该方法在语义保真度和属性解耦方面优于现有的零样本编辑方法。

Conclusion: 该方法通过分解和聚合流，能够在图像编辑中保持多样性和一致性，实现了更好的编辑效果。

Abstract: Rectified flow models have become a de facto standard in image generation due
to their stable sampling trajectories and high-fidelity outputs. Despite their
strong generative capabilities, they face critical limitations in image editing
tasks: inaccurate inversion processes for mapping real images back into the
latent space, and gradient entanglement issues during editing often result in
outputs that do not faithfully reflect the target prompt. Recent efforts have
attempted to directly map source and target distributions via ODE-based
approaches without inversion; however,these methods still yield suboptimal
editing quality. In this work, we propose a flow decomposition-and-aggregation
framework built upon an inversion-free formulation to address these
limitations. Specifically, we semantically decompose the target prompt into
multiple sub-prompts, compute an independent flow for each, and aggregate them
to form a unified editing trajectory. While we empirically observe that
decomposing the original flow enhances diversity in the target space,
generating semantically aligned outputs still requires consistent guidance
toward the full target prompt. To this end, we design a projection and
soft-aggregation mechanism for flow, inspired by gradient conflict resolution
in multi-task learning. This approach adaptively weights the sub-target
velocity fields, suppressing semantic redundancy while emphasizing distinct
directions, thereby preserving both diversity and consistency in the final
edited output. Experimental results demonstrate that our method outperforms
existing zero-shot editing approaches in terms of semantic fidelity and
attribute disentanglement. The code is available at
https://github.com/Harvard-AI-and-Robotics-Lab/SplitFlow.

</details>


### [52] [Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer](https://arxiv.org/abs/2510.25976)
*Roman Beliy,Amit Zalcher,Jonathan Kogman,Navve Wasserman,Michal Irani*

Main category: cs.CV

TL;DR: Brain-IT: 使用大脑交互Transformer (BIT) 从fMRI脑部记录重建图像，通过功能相似的脑体素集群间的有效交互，提高图像重建的保真度，并使用少量数据实现高效训练。


<details>
  <summary>Details</summary>
Motivation: 当前方法重建的图像与实际所见图像的保真度不足。

Method: 提出Brain-IT，包含大脑交互Transformer (BIT)，允许功能相似的脑体素集群之间进行有效交互。BIT预测两个互补的局部patch级别图像特征：高层语义特征和低层结构特征，分别用于引导扩散模型和初始化扩散过程。

Result: Brain-IT在fMRI图像重建方面超越了当前最先进的方法，并且仅用新受试者的1小时fMRI数据即可达到与使用完整40小时记录训练的当前方法相当的结果。

Conclusion: Brain-IT能够从fMRI忠实地重建所见图像，并在视觉和标准客观指标上超越了当前最先进的方法。

Abstract: Reconstructing images seen by people from their fMRI brain recordings
provides a non-invasive window into the human brain. Despite recent progress
enabled by diffusion models, current methods often lack faithfulness to the
actual seen images. We present "Brain-IT", a brain-inspired approach that
addresses this challenge through a Brain Interaction Transformer (BIT),
allowing effective interactions between clusters of functionally-similar
brain-voxels. These functional-clusters are shared by all subjects, serving as
building blocks for integrating information both within and across brains. All
model components are shared by all clusters & subjects, allowing efficient
training with a limited amount of data. To guide the image reconstruction, BIT
predicts two complementary localized patch-level image features: (i)high-level
semantic features which steer the diffusion model toward the correct semantic
content of the image; and (ii)low-level structural features which help to
initialize the diffusion process with the correct coarse layout of the image.
BIT's design enables direct flow of information from brain-voxel clusters to
localized image features. Through these principles, our method achieves image
reconstructions from fMRI that faithfully reconstruct the seen images, and
surpass current SotA approaches both visually and by standard objective
metrics. Moreover, with only 1-hour of fMRI data from a new subject, we achieve
results comparable to current methods trained on full 40-hour recordings.

</details>


### [53] [Fine-tuning Segment Anything for Real-Time Tumor Tracking in Cine-MRI](https://arxiv.org/abs/2510.25990)
*Valentin Boussot,Cédric Hémon,Jean-Claude Nunes,Jean-Louis Dillenseger*

Main category: cs.CV

TL;DR: This paper explores real-time tumor tracking in cine-MRI using foundation models, specifically SAM 2.1, fine-tuned on a small dataset.


<details>
  <summary>Details</summary>
Motivation: Addressing the TrackRAD2025 challenge of real-time tumor tracking in cine-MRI sequences under strong data scarcity constraints.

Method: Utilizing SAM2.1 b+ with mask-based prompts from the first annotated slice, fine-tuned on the small labeled subset from TrackRAD2025. Training was configured to minimize overfitting, using 1024x1024 patches, standard augmentations, and a balanced Dice + IoU loss.

Result: Achieved a Dice score of 0.8794 on the hidden test set, ranking 6th overall in the TrackRAD2025 challenge.

Conclusion: Highlights the strong potential of foundation models for accurate and real-time tumor tracking in MRI-guided radiotherapy.

Abstract: In this work, we address the TrackRAD2025 challenge of real-time tumor
tracking in cine-MRI sequences of the thoracic and abdominal regions under
strong data scarcity constraints. Two complementary strategies were explored:
(i) unsupervised registration with the IMPACT similarity metric and (ii)
foundation model-based segmentation leveraging SAM 2.1 and its recent variants
through prompt-based interaction. Due to the one-second runtime constraint, the
SAM-based method was ultimately selected. The final configuration used SAM2.1
b+ with mask-based prompts from the first annotated slice, fine-tuned solely on
the small labeled subset from TrackRAD2025. Training was configured to minimize
overfitting, using 1024x1024 patches (batch size 1), standard augmentations,
and a balanced Dice + IoU loss. A low uniform learning rate (0.0001) was
applied to all modules (prompt encoder, decoder, Hiera backbone) to preserve
generalization while adapting to annotator-specific styles. Training lasted 300
epochs (~12h on RTX A6000, 48GB). The same inference strategy was consistently
applied across all anatomical sites and MRI field strengths. Test-time
augmentation was considered but ultimately discarded due to negligible
performance gains. The final model was selected based on the highest Dice
Similarity Coefficient achieved on the validation set after fine-tuning. On the
hidden test set, the model reached a Dice score of 0.8794, ranking 6th overall
in the TrackRAD2025 challenge. These results highlight the strong potential of
foundation models for accurate and real-time tumor tracking in MRI-guided
radiotherapy.

</details>


### [54] [Larger Hausdorff Dimension in Scanning Pattern Facilitates Mamba-Based Methods in Low-Light Image Enhancement](https://arxiv.org/abs/2510.26001)
*Xinhua Wang,Caibo Feng,Xiangjun Fu,Chunxiao Liu*

Main category: cs.CV

TL;DR: 提出了一种新的 Hilbert Selective Scan 机制，通过增加扫描模式的 Hausdorff 维度来增强 Mamba 框架。


<details>
  <summary>Details</summary>
Motivation: 为了更有效地探索特征空间，捕获复杂的精细尺度细节，并提高整体覆盖率。

Method: 通过 Hilbert Selective Scan 机制增加 Mamba 框架扫描模式的 Hausdorff 维度。

Result: 在公开基准测试中，该方法显著提高了现有基于 Mamba 的弱光图像增强方法的定量指标和定性视觉保真度，同时降低了计算资源消耗并缩短了推理时间。

Conclusion: 这种改进的策略不仅提高了弱光图像增强的水平，而且在利用基于 Mamba 技术的领域中具有更广泛的应用前景。

Abstract: We propose an innovative enhancement to the Mamba framework by increasing the
Hausdorff dimension of its scanning pattern through a novel Hilbert Selective
Scan mechanism. This mechanism explores the feature space more effectively,
capturing intricate fine-scale details and improving overall coverage. As a
result, it mitigates information inconsistencies while refining spatial
locality to better capture subtle local interactions without sacrificing the
model's ability to handle long-range dependencies. Extensive experiments on
publicly available benchmarks demonstrate that our approach significantly
improves both the quantitative metrics and qualitative visual fidelity of
existing Mamba-based low-light image enhancement methods, all while reducing
computational resource consumption and shortening inference time. We believe
that this refined strategy not only advances the state-of-the-art in low-light
image enhancement but also holds promise for broader applications in fields
that leverage Mamba-based techniques.

</details>


### [55] [AdSum: Two-stream Audio-visual Summarization for Automated Video Advertisement Clipping](https://arxiv.org/abs/2510.26569)
*Wen Xie,Yanjun Zhu,Gijs Overgoor,Yakov Bart,Agata Lapedriza Garcia,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: 提出了一种使用视频摘要技术自动进行视频广告剪辑的框架，以解决手动剪辑广告耗时耗力的问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法手动选择和重新编辑较长视频广告中的镜头以创建较短版本，劳动强度大且耗时。因此，需要一个自动化的视频广告剪辑框架。

Method: 开发了一个双流音视频融合模型，该模型预测视频帧的重要性，其中重要性定义为帧在公司制作的短广告中被选择的可能性。

Result: 在各种指标（包括平均精度、曲线下面积、Spearman和Kendall）上，该模型优于最先进的方法。

Conclusion: 论文提出了AdSum204数据集，并证明了该模型在广告剪辑任务上的有效性。

Abstract: Advertisers commonly need multiple versions of the same advertisement (ad) at
varying durations for a single campaign. The traditional approach involves
manually selecting and re-editing shots from longer video ads to create shorter
versions, which is labor-intensive and time-consuming. In this paper, we
introduce a framework for automated video ad clipping using video summarization
techniques. We are the first to frame video clipping as a shot selection
problem, tailored specifically for advertising. Unlike existing general video
summarization methods that primarily focus on visual content, our approach
emphasizes the critical role of audio in advertising. To achieve this, we
develop a two-stream audio-visual fusion model that predicts the importance of
video frames, where importance is defined as the likelihood of a frame being
selected in the firm-produced short ad. To address the lack of ad-specific
datasets, we present AdSum204, a novel dataset comprising 102 pairs of
30-second and 15-second ads from real advertising campaigns. Extensive
experiments demonstrate that our model outperforms state-of-the-art methods
across various metrics, including Average Precision, Area Under Curve,
Spearman, and Kendall.

</details>


### [56] [CAVE: Detecting and Explaining Commonsense Anomalies in Visual Environments](https://arxiv.org/abs/2510.26006)
*Rishika Bhagwatkar,Syrielle Montariol,Angelika Romanou,Beatriz Borges,Irina Rish,Antoine Bosselut*

Main category: cs.CV

TL;DR: 提出了一个名为CAVE的真实世界视觉异常基准测试，用于评估视觉语言模型在检测和理解异常方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的计算机视觉异常检测方法难以捕捉真实世界异常的丰富性和不可预测性。

Method: 构建了CAVE基准测试，支持异常描述、解释和理由三个开放式任务，并提供细粒度的标注。

Result: 表明当前最先进的视觉语言模型在视觉异常感知和常识推理方面表现不佳。

Conclusion: CAVE是一个有价值的资源，可以促进视觉语言模型在异常检测和常识推理方面的研究。

Abstract: Humans can naturally identify, reason about, and explain anomalies in their
environment. In computer vision, this long-standing challenge remains limited
to industrial defects or unrealistic, synthetically generated anomalies,
failing to capture the richness and unpredictability of real-world anomalies.
In this work, we introduce CAVE, the first benchmark of real-world visual
anomalies. CAVE supports three open-ended tasks: anomaly description,
explanation, and justification; with fine-grained annotations for visual
grounding and categorizing anomalies based on their visual manifestations,
their complexity, severity, and commonness. These annotations draw inspiration
from cognitive science research on how humans identify and resolve anomalies,
providing a comprehensive framework for evaluating Vision-Language Models
(VLMs) in detecting and understanding anomalies. We show that state-of-the-art
VLMs struggle with visual anomaly perception and commonsense reasoning, even
with advanced prompting strategies. By offering a realistic and cognitively
grounded benchmark, CAVE serves as a valuable resource for advancing research
in anomaly detection and commonsense reasoning in VLMs.

</details>


### [57] [Climate Adaptation-Aware Flood Prediction for Coastal Cities Using Deep Learning](https://arxiv.org/abs/2510.26017)
*Bilal Hassan,Areg Karapetyan,Aaron Chung Hin Chow,Samer Madanat*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的基于卷积神经网络（CNN）的轻量级模型，用于预测不同海平面上升情景和海岸线适应方案下的沿海洪水。


<details>
  <summary>Details</summary>
Motivation: 传统的基于物理的水动力模拟器虽然精确，但计算成本高昂，不适用于城市规模的沿海规划应用。深度学习（DL）技术提供了一种有希望的替代方案，但它们经常受到数据稀缺和高维输出需求的限制。

Method: 该研究利用一种最近提出的基于视觉的低资源深度学习框架，开发了一种新的、轻量级的基于卷积神经网络（CNN）的模型。

Result: 该模型在预测洪水深度图方面的平均绝对误差（MAE）降低了近20%，显著优于现有方法。该模型还展示了通过利用来自两个不同地区（阿布扎比和旧金山）的数据集来推广到不同地理环境的能力。

Conclusion: 该研究结果突出了该方法作为一种可扩展且实用的沿海洪水管理工具的潜力，使决策者能够制定有效的缓解策略，以应对气候变化日益增长的影响。

Abstract: Climate change and sea-level rise (SLR) pose escalating threats to coastal
cities, intensifying the need for efficient and accurate methods to predict
potential flood hazards. Traditional physics-based hydrodynamic simulators,
although precise, are computationally expensive and impractical for city-scale
coastal planning applications. Deep Learning (DL) techniques offer promising
alternatives, however, they are often constrained by challenges such as data
scarcity and high-dimensional output requirements. Leveraging a recently
proposed vision-based, low-resource DL framework, we develop a novel,
lightweight Convolutional Neural Network (CNN)-based model designed to predict
coastal flooding under variable SLR projections and shoreline adaptation
scenarios. Furthermore, we demonstrate the ability of the model to generalize
across diverse geographical contexts by utilizing datasets from two distinct
regions: Abu Dhabi and San Francisco. Our findings demonstrate that the
proposed model significantly outperforms state-of-the-art methods, reducing the
mean absolute error (MAE) in predicted flood depth maps on average by nearly
20%. These results highlight the potential of our approach to serve as a
scalable and practical tool for coastal flood management, empowering
decision-makers to develop effective mitigation strategies in response to the
growing impacts of climate change. Project Page: https://caspiannet.github.io/

</details>


### [58] [Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders](https://arxiv.org/abs/2510.26027)
*Ali Rasekh,Erfan Bagheri Soula,Omid Daliran,Simon Gottschalk,Mohsen Fayyaz*

Main category: cs.CV

TL;DR: 现有的视频大语言模型在理解视频中的复杂时间动态方面存在局限性。


<details>
  <summary>Details</summary>
Motivation: 当前视频大语言模型在时间理解方面存在局限性，难以处理需要详细理解动作序列和时间进展的任务。

Method: 提出了一种新的视频大语言模型架构，该架构在视觉编码器中引入了堆叠的时间注意力模块。

Result: 该方法显著提高了时间推理能力，并在视频问答任务中优于现有模型，在多个基准测试中提高了高达5.5%。

Conclusion: 通过用时间结构增强视觉编码器，解决了视频大语言模型在视频理解方面的一个关键差距。

Abstract: Despite significant advances in Multimodal Large Language Models (MLLMs),
understanding complex temporal dynamics in videos remains a major challenge.
Our experiments show that current Video Large Language Model (Video-LLM)
architectures have critical limitations in temporal understanding, struggling
with tasks that require detailed comprehension of action sequences and temporal
progression. In this work, we propose a Video-LLM architecture that introduces
stacked temporal attention modules directly within the vision encoder. This
design incorporates a temporal attention in vision encoder, enabling the model
to better capture the progression of actions and the relationships between
frames before passing visual tokens to the LLM. Our results show that this
approach significantly improves temporal reasoning and outperforms existing
models in video question answering tasks, specifically in action recognition.
We improve on benchmarks including VITATECS, MVBench, and Video-MME by up to
+5.5%. By enhancing the vision encoder with temporal structure, we address a
critical gap in video understanding for Video-LLMs. Project page and code are
available at: https://alirasekh.github.io/STAVEQ2/.

</details>


### [59] [FlexICL: A Flexible Visual In-context Learning Framework for Elbow and Wrist Ultrasound Segmentation](https://arxiv.org/abs/2510.26049)
*Yuyue Zhou,Jessica Knight,Shrimanti Ghosh,Banafshe Felfeliyan,Jacob L. Jaremko,Abhilash R. Hareendranathan*

Main category: cs.CV

TL;DR: 提出了一种新颖的灵活的上下文学习（ICL）框架FlexICL，用于分割超声（US）图像中的骨区域，只需要5%的训练图像，在四个手腕和肘部US数据集上实现了稳健的分割性能。


<details>
  <summary>Details</summary>
Motivation: 肘部和腕部骨折是儿科人群中最常见的骨折。超声（US）中肌肉骨骼结构的自动分割可以提高诊断准确性和治疗计划。深度学习（DL）可以提供实时反馈并突出显示关键结构，帮助受过少量训练的用户更有信心地进行检查。然而，用于训练的像素级专家注释仍然耗时且成本高昂。

Method: 将FlexICL应用于视频内分割设置，专家仅注释一小部分帧，模型分割未见帧。系统地研究了各种图像连接技术和视觉ICL的训练策略，并引入了新的连接方法，这些方法在有限的标记数据下显着提高了模型性能。通过整合多种增强策略，FlexICL实现了强大的分割性能。

Result: FlexICL在1,252次US扫描中，Dice系数比最先进的视觉ICL模型（如Painter、MAE-VQGAN）和传统的分割模型（如U-Net和TransUNet）高出1-27%。

Conclusion: 这些初步结果突显了FlexICL作为一种高效且可扩展的US图像分割解决方案的潜力，非常适合标记数据稀缺的医学成像用例。

Abstract: Elbow and wrist fractures are the most common fractures in pediatric
populations. Automatic segmentation of musculoskeletal structures in ultrasound
(US) can improve diagnostic accuracy and treatment planning. Fractures appear
as cortical defects but require expert interpretation. Deep learning (DL) can
provide real-time feedback and highlight key structures, helping lightly
trained users perform exams more confidently. However, pixel-wise expert
annotations for training remain time-consuming and costly. To address this
challenge, we propose FlexICL, a novel and flexible in-context learning (ICL)
framework for segmenting bony regions in US images. We apply it to an
intra-video segmentation setting, where experts annotate only a small subset of
frames, and the model segments unseen frames. We systematically investigate
various image concatenation techniques and training strategies for visual ICL
and introduce novel concatenation methods that significantly enhance model
performance with limited labeled data. By integrating multiple augmentation
strategies, FlexICL achieves robust segmentation performance across four wrist
and elbow US datasets while requiring only 5% of the training images. It
outperforms state-of-the-art visual ICL models like Painter, MAE-VQGAN, and
conventional segmentation models like U-Net and TransUNet by 1-27% Dice
coefficient on 1,252 US sweeps. These initial results highlight the potential
of FlexICL as an efficient and scalable solution for US image segmentation well
suited for medical imaging use cases where labeled data is scarce.

</details>


### [60] [Dynamic VLM-Guided Negative Prompting for Diffusion Models](https://arxiv.org/abs/2510.26052)
*Hoyeon Chang,Seungjin Kim,Yoonseok Choi*

Main category: cs.CV

TL;DR: 提出了一种新的动态负提示方法，利用视觉-语言模型（VLM）在去噪过程中自适应生成负提示。


<details>
  <summary>Details</summary>
Motivation: 传统负提示方法使用固定的负提示，缺乏灵活性。

Method: 在特定去噪步骤生成中间图像预测，并查询VLM以生成上下文相关的负提示。

Result: 在各种基准数据集上评估了该方法，并展示了负指导强度和文本-图像对齐之间的权衡。

Conclusion: 该方法能够自适应地生成负提示，并在负指导强度和文本-图像对齐之间取得平衡。

Abstract: We propose a novel approach for dynamic negative prompting in diffusion
models that leverages Vision-Language Models (VLMs) to adaptively generate
negative prompts during the denoising process. Unlike traditional Negative
Prompting methods that use fixed negative prompts, our method generates
intermediate image predictions at specific denoising steps and queries a VLM to
produce contextually appropriate negative prompts. We evaluate our approach on
various benchmark datasets and demonstrate the trade-offs between negative
guidance strength and text-image alignment.

</details>


### [61] [Security Risk of Misalignment between Text and Image in Multi-modal Model](https://arxiv.org/abs/2510.26105)
*Xiaosen Wang,Zhijin Ge,Shaokang Wang*

Main category: cs.CV

TL;DR: 多模态扩散模型容易受到对抗性输入的影响，并且文本和图像模态之间的对齐不充分，从而导致生成不安全内容。提出了一种名为PReMA的新型攻击，该攻击通过修改输入图像来操纵生成的内容，而无需更改prompt本身。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型中文本和图像模态之间的对齐不充分，这会带来生成不安全内容的风险。

Method: 提出了一种名为Prompt-Restricted Multi-modal Attack (PReMA) 的新型攻击，该攻击通过修改输入图像来操纵生成的内容，而无需更改prompt本身。

Result: 在图像修复和风格转换任务上的综合评估证实了PReMA的有效性。

Conclusion: PReMA对多模态扩散模型的完整性构成了新的威胁，尤其是在使用固定prompt的图像编辑应用程序中。

Abstract: Despite the notable advancements and versatility of multi-modal diffusion
models, such as text-to-image models, their susceptibility to adversarial
inputs remains underexplored. Contrary to expectations, our investigations
reveal that the alignment between textual and Image modalities in existing
diffusion models is inadequate. This misalignment presents significant risks,
especially in the generation of inappropriate or Not-Safe-For-Work (NSFW)
content. To this end, we propose a novel attack called Prompt-Restricted
Multi-modal Attack (PReMA) to manipulate the generated content by modifying the
input image in conjunction with any specified prompt, without altering the
prompt itself. PReMA is the first attack that manipulates model outputs by
solely creating adversarial images, distinguishing itself from prior methods
that primarily generate adversarial prompts to produce NSFW content.
Consequently, PReMA poses a novel threat to the integrity of multi-modal
diffusion models, particularly in image-editing applications that operate with
fixed prompts. Comprehensive evaluations conducted on image inpainting and
style transfer tasks across various models confirm the potent efficacy of
PReMA.

</details>


### [62] [EgoExo-Con: Exploring View-Invariant Video Temporal Understanding](https://arxiv.org/abs/2510.26113)
*Minjoon Jung,Junbin Xiao,Junghyun Kim,Byoung-Tak Zhang,Angela Yao*

Main category: cs.CV

TL;DR: 论文研究了Video-LLM在处理来自不同视角的同一事件视频时，能否保持时间理解上的一致性，并提出了一个新的基准测试和强化学习框架来解决这个问题。


<details>
  <summary>Details</summary>
Motivation: 现有的Video-LLM在跨视角的时间理解上存在一致性问题，并且简单的微调方法可能会降低单视角性能。

Method: 提出了EgoExo-Con基准测试，包含同步的自我中心和外部中心视频对，以及人工设计的自然语言查询。同时，提出了View-GRPO强化学习框架，以提升视角特定的时间推理能力，并鼓励跨视角的一致性理解。

Result: 现有的Video-LLM在一致性方面表现不佳，提出的View-GRPO方法优于简单的SFT和GRPO方法，尤其是在提高跨视角一致性方面。

Conclusion: 论文指出了现有Video-LLM在跨视角时间理解一致性方面的局限性，并提出了一种有效的强化学习框架来改善这一问题。

Abstract: Can Video-LLMs achieve consistent temporal understanding when videos capture
the same event from different viewpoints? To study this, we introduce
EgoExo-Con (Consistency), a benchmark of comprehensively synchronized
egocentric and exocentric video pairs with human-refined queries in natural
language. EgoExo-Con emphasizes two temporal understanding tasks: Temporal
Verification and Temporal Grounding. It evaluates not only correctness but
consistency across viewpoints. Our analysis reveals two critical limitations of
existing Video-LLMs: (1) models often fail to maintain consistency, with
results far worse than their single-view performances. (2) When naively
finetuned with synchronized videos of both viewpoints, the models show improved
consistency but often underperform those trained on a single view. For
improvements, we propose View-GRPO, a novel reinforcement learning framework
that effectively strengthens view-specific temporal reasoning while encouraging
consistent comprehension across viewpoints. Our method demonstrates its
superiority over naive SFT and GRPO, especially for improving cross-view
consistency. All resources will be made publicly available.

</details>


### [63] [OracleAgent: A Multimodal Reasoning Agent for Oracle Bone Script Research](https://arxiv.org/abs/2510.26114)
*Caoshuo Li,Zengmao Ding,Xiaobin Hu,Bang Li,Donghao Luo,Xu Peng,Taisong Jin,Yongge Liu,Shengwei Han,Jing Yang,Xiaoping He,Feng Gao,AndyPian Wu,SevenShu,Chaoyang Wang,Chengjie Wang*

Main category: cs.CV

TL;DR: OracleAgent是一个为甲骨文研究设计的智能体系统，旨在解决甲骨文信息管理和检索的效率问题。


<details>
  <summary>Details</summary>
Motivation: 当前甲骨文研究面临解释工作流程复杂和信息组织检索效率低下的挑战。

Method: 构建了一个集成了多个甲骨文分析工具的智能体系统OracleAgent，并构建了一个包含140万张字符图像和8万条解释文本的领域知识库。

Result: OracleAgent在多模态推理和生成任务中表现优异，超越了主流多模态大型语言模型，并能显著降低甲骨文研究的时间成本。

Conclusion: OracleAgent是甲骨文辅助研究和自动解释系统迈出的重要一步。

Abstract: As one of the earliest writing systems, Oracle Bone Script (OBS) preserves
the cultural and intellectual heritage of ancient civilizations. However,
current OBS research faces two major challenges: (1) the interpretation of OBS
involves a complex workflow comprising multiple serial and parallel sub-tasks,
and (2) the efficiency of OBS information organization and retrieval remains a
critical bottleneck, as scholars often spend substantial effort searching for,
compiling, and managing relevant resources. To address these challenges, we
present OracleAgent, the first agent system designed for the structured
management and retrieval of OBS-related information. OracleAgent seamlessly
integrates multiple OBS analysis tools, empowered by large language models
(LLMs), and can flexibly orchestrate these components. Additionally, we
construct a comprehensive domain-specific multimodal knowledge base for OBS,
which is built through a rigorous multi-year process of data collection,
cleaning, and expert annotation. The knowledge base comprises over 1.4M
single-character rubbing images and 80K interpretation texts. OracleAgent
leverages this resource through its multimodal tools to assist experts in
retrieval tasks of character, document, interpretation text, and rubbing image.
Extensive experiments demonstrate that OracleAgent achieves superior
performance across a range of multimodal reasoning and generation tasks,
surpassing leading mainstream multimodal large language models (MLLMs) (e.g.,
GPT-4o). Furthermore, our case study illustrates that OracleAgent can
effectively assist domain experts, significantly reducing the time cost of OBS
research. These results highlight OracleAgent as a significant step toward the
practical deployment of OBS-assisted research and automated interpretation
systems.

</details>


### [64] [JOGS: Joint Optimization of Pose Estimation and 3D Gaussian Splatting](https://arxiv.org/abs/2510.26117)
*Yuxuan Li,Tao Wang,Xianben Yang*

Main category: cs.CV

TL;DR: 提出了一种联合优化3D高斯点和相机姿态的统一框架，无需预先校准的输入。


<details>
  <summary>Details</summary>
Motivation: 传统的novel view synthesis方法严重依赖外部相机姿态估计工具（如COLMAP），这通常会引入计算瓶颈并传播误差。

Method: 通过一种新颖的协同优化策略迭代地优化3D高斯参数并更新相机姿态，确保同时提高场景重建保真度和姿态精度。该方法将联合优化解耦为两个交错的阶段：首先，通过具有固定姿态的可微渲染更新3D高斯参数；其次，使用定制的3D光流算法（包含几何和光度约束）优化相机姿态。

Result: 在多个数据集上的大量评估表明，该方法在重建质量上显著优于现有的无COLMAP技术，并且在一般情况下也优于标准的基于COLMAP的基线。

Conclusion: 该方法逐步减少投影误差，特别是在具有大视点变化和稀疏特征分布的具有挑战性的场景中，在这些场景中，传统方法难以奏效。

Abstract: Traditional novel view synthesis methods heavily rely on external camera pose
estimation tools such as COLMAP, which often introduce computational
bottlenecks and propagate errors. To address these challenges, we propose a
unified framework that jointly optimizes 3D Gaussian points and camera poses
without requiring pre-calibrated inputs. Our approach iteratively refines 3D
Gaussian parameters and updates camera poses through a novel co-optimization
strategy, ensuring simultaneous improvements in scene reconstruction fidelity
and pose accuracy. The key innovation lies in decoupling the joint optimization
into two interleaved phases: first, updating 3D Gaussian parameters via
differentiable rendering with fixed poses, and second, refining camera poses
using a customized 3D optical flow algorithm that incorporates geometric and
photometric constraints. This formulation progressively reduces projection
errors, particularly in challenging scenarios with large viewpoint variations
and sparse feature distributions, where traditional methods struggle. Extensive
evaluations on multiple datasets demonstrate that our approach significantly
outperforms existing COLMAP-free techniques in reconstruction quality, and also
surpasses the standard COLMAP-based baseline in general.

</details>


### [65] [WOD-E2E: Waymo Open Dataset for End-to-End Driving in Challenging Long-tail Scenarios](https://arxiv.org/abs/2510.26125)
*Runsheng Xu,Hubert Lin,Wonseok Jeon,Hao Feng,Yuliang Zou,Liting Sun,John Gorman,Kate Tolstaya,Sarah Tang,Brandyn White,Ben Sapp,Mingxing Tan,Jyh-Jing Hwang,Drago Anguelov*

Main category: cs.CV

TL;DR: 提出了一个新的自动驾驶数据集WOD-E2E，专注于长尾场景，并提出了一个新的评估指标RFS。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端驾驶基准测试主要集中在常规场景，未能充分测试系统的潜力。现有的开环评估指标不能有效地评估长尾场景中的性能。

Method: 创建了一个包含4,021个驾驶片段的数据集WOD-E2E，这些片段专门为具有挑战性的长尾场景而设计。提出了一个新的开环评估指标RFS，用于评估在这些长尾情况下的E2E驾驶性能。

Result: 发布了所有WOD-E2E验证集片段的评估者偏好标签，而保留的测试集标签已用于2025 WOD-E2E挑战赛。

Conclusion: 旨在促进对能够处理复杂现实世界情况的通用、稳健和安全的端到端自动驾驶代理的最先进的研究。

Abstract: Vision-based end-to-end (E2E) driving has garnered significant interest in
the research community due to its scalability and synergy with multimodal large
language models (MLLMs). However, current E2E driving benchmarks primarily
feature nominal scenarios, failing to adequately test the true potential of
these systems. Furthermore, existing open-loop evaluation metrics often fall
short in capturing the multi-modal nature of driving or effectively evaluating
performance in long-tail scenarios. To address these gaps, we introduce the
Waymo Open Dataset for End-to-End Driving (WOD-E2E). WOD-E2E contains 4,021
driving segments (approximately 12 hours), specifically curated for challenging
long-tail scenarios that that are rare in daily life with an occurring
frequency of less than 0.03%. Concretely, each segment in WOD-E2E includes the
high-level routing information, ego states, and 360-degree camera views from 8
surrounding cameras. To evaluate the E2E driving performance on these long-tail
situations, we propose a novel open-loop evaluation metric: Rater Feedback
Score (RFS). Unlike conventional metrics that measure the distance between
predicted way points and the logs, RFS measures how closely the predicted
trajectory matches rater-annotated trajectory preference labels. We have
released rater preference labels for all WOD-E2E validation set segments, while
the held out test set labels have been used for the 2025 WOD-E2E Challenge.
Through our work, we aim to foster state of the art research into
generalizable, robust, and safe end-to-end autonomous driving agents capable of
handling complex real-world situations.

</details>


### [66] [Exploring Object-Aware Attention Guided Frame Association for RGB-D SLAM](https://arxiv.org/abs/2510.26131)
*Ali Caglayan,Nevrez Imamoglu,Oguzhan Guclu,Ali Osman Serhatoglu,Ahmet Burak Can,Ryosuke Nakamura*

Main category: cs.CV

TL;DR: 本文提出了一种利用任务特定网络注意力进行RGB-D室内SLAM的方法，通过将网络梯度导出的分层注意力信息与CNN特征表示相结合，以提高帧关联性能。


<details>
  <summary>Details</summary>
Motivation: 在SLAM等视觉任务中，将基于梯度的注意力信息直接整合到CNN表示中进行语义对象理解的方法还很有限。将空间注意力对象位置增强的CNN表示可以提高性能。

Method: 本文将从网络梯度中提取的分层注意力信息与CNN特征表示相集成。

Result: 实验结果表明，与基线方法相比，性能有所提高，尤其是在大型环境中。

Conclusion: 本文提出了一种利用任务特定网络注意力进行RGB-D室内SLAM的方法，并通过实验验证了其有效性。

Abstract: Attention models have recently emerged as a powerful approach, demonstrating
significant progress in various fields. Visualization techniques, such as class
activation mapping, provide visual insights into the reasoning of convolutional
neural networks (CNNs). Using network gradients, it is possible to identify
regions where the network pays attention during image recognition tasks.
Furthermore, these gradients can be combined with CNN features to localize more
generalizable, task-specific attentive (salient) regions within scenes.
However, explicit use of this gradient-based attention information integrated
directly into CNN representations for semantic object understanding remains
limited. Such integration is particularly beneficial for visual tasks like
simultaneous localization and mapping (SLAM), where CNN representations
enriched with spatially attentive object locations can enhance performance. In
this work, we propose utilizing task-specific network attention for RGB-D
indoor SLAM. Specifically, we integrate layer-wise attention information
derived from network gradients with CNN feature representations to improve
frame association performance. Experimental results indicate improved
performance compared to baseline methods, particularly for large environments.

</details>


### [67] [FullPart: Generating each 3D Part at Full Resolution](https://arxiv.org/abs/2510.26140)
*Lihe Ding,Shaocong Dong,Yaokun Li,Chenjian Gao,Xiao Chen,Rui Han,Yihao Kuang,Hong Zhang,Bo Huang,Zhanpeng Huang,Zibin Wang,Dan Xu,Tianfan Xue*

Main category: cs.CV

TL;DR: 提出了FullPart，一个结合隐式和显式范例的3D part生成框架，并发布了PartVerse-XL数据集。


<details>
  <summary>Details</summary>
Motivation: 以往的part生成器在几何细节上不足，或因共享全局voxel网格导致小部件质量下降。

Method: 首先通过隐式扩散过程得到bounding box布局，然后生成详细的part，每个part在自己的固定全分辨率voxel网格中生成。引入中心点编码策略来解决不同大小part之间信息交换时的对齐问题。

Result: FullPart在3D part生成方面取得了state-of-the-art的结果。

Conclusion: FullPart是有效的，代码、数据和模型将被发布以促进未来的研究。

Abstract: Part-based 3D generation holds great potential for various applications.
Previous part generators that represent parts using implicit vector-set tokens
often suffer from insufficient geometric details. Another line of work adopts
an explicit voxel representation but shares a global voxel grid among all
parts; this often causes small parts to occupy too few voxels, leading to
degraded quality. In this paper, we propose FullPart, a novel framework that
combines both implicit and explicit paradigms. It first derives the bounding
box layout through an implicit box vector-set diffusion process, a task that
implicit diffusion handles effectively since box tokens contain little
geometric detail. Then, it generates detailed parts, each within its own fixed
full-resolution voxel grid. Instead of sharing a global low-resolution space,
each part in our method - even small ones - is generated at full resolution,
enabling the synthesis of intricate details. We further introduce a
center-point encoding strategy to address the misalignment issue when
exchanging information between parts of different actual sizes, thereby
maintaining global coherence. Moreover, to tackle the scarcity of reliable part
data, we present PartVerse-XL, the largest human-annotated 3D part dataset to
date with 40K objects and 320K parts. Extensive experiments demonstrate that
FullPart achieves state-of-the-art results in 3D part generation. We will
release all code, data, and model to benefit future research in 3D part
generation.

</details>


### [68] [BasicAVSR: Arbitrary-Scale Video Super-Resolution via Image Priors and Enhanced Motion Compensation](https://arxiv.org/abs/2510.26149)
*Wei Shang,Wanying Zhang,Shuhang Gu,Pengfei Zhu,Qinghua Hu,Dongwei Ren*

Main category: cs.CV

TL;DR: 本文提出了一种名为 BasicAVSR 的视频超分辨率基线模型，该模型集成了多尺度频率先验、光流引导的传播单元、二阶运动补偿单元和超像素上采样单元。


<details>
  <summary>Details</summary>
Motivation: 解决任意比例视频超分辨率在空间细节再现、时间一致性和计算复杂度方面的挑战。

Method: 该方法结合了图像拉普拉斯金字塔生成自适应多尺度频率先验，使用光流引导的传播单元聚合时空信息，使用二阶运动补偿单元进行更精确的空间对齐，并使用超像素上采样单元生成尺度感知和内容无关的上采样核。

Result: 实验结果表明，BasicAVSR 在超分辨率质量、泛化能力和推理速度方面显著优于现有方法。

Conclusion: 该研究不仅推进了 AVSR 的技术水平，还将其核心组件扩展到多种框架，适用于不同的场景。

Abstract: Arbitrary-scale video super-resolution (AVSR) aims to enhance the resolution
of video frames, potentially at various scaling factors, which presents several
challenges regarding spatial detail reproduction, temporal consistency, and
computational complexity. In this paper, we propose a strong baseline BasicAVSR
for AVSR by integrating four key components: 1) adaptive multi-scale frequency
priors generated from image Laplacian pyramids, 2) a flow-guided propagation
unit to aggregate spatiotemporal information from adjacent frames, 3) a
second-order motion compensation unit for more accurate spatial alignment of
adjacent frames, and 4) a hyper-upsampling unit to generate scale-aware and
content-independent upsampling kernels. To meet diverse application demands, we
instantiate three propagation variants: (i) a unidirectional RNN unit for
strictly online inference, (ii) a unidirectional RNN unit empowered with a
limited lookahead that tolerates a small output delay, and (iii) a
bidirectional RNN unit designed for offline tasks where computational resources
are less constrained. Experimental results demonstrate the effectiveness and
adaptability of our model across these different scenarios. Through extensive
experiments, we show that BasicAVSR significantly outperforms existing methods
in terms of super-resolution quality, generalization ability, and inference
speed. Our work not only advances the state-of-the-art in AVSR but also extends
its core components to multiple frameworks for diverse scenarios. The code is
available at https://github.com/shangwei5/BasicAVSR.

</details>


### [69] [MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer Diagnosis and Risk Prediction](https://arxiv.org/abs/2510.26151)
*Shunjie-Fabian Zheng,Hyeonjun Lee,Thijs Kooi,Ali Diba*

Main category: cs.CV

TL;DR: 本文提出了一种新的多视图乳腺钼靶和语言模型（MV-MLM），用于乳腺癌分类和风险预测。


<details>
  <summary>Details</summary>
Motivation: 获取带有精细注释的大型带注释数据集对于训练用于乳腺癌检测或风险预测的鲁棒计算机辅助诊断（CAD）模型至关重要，但成本高且耗时。Vision-Language Models (VLMs) 通过提高医学成像任务中的鲁棒性和数据效率，提供了一个有希望的解决方案。

Method: 该 MV-MLM 通过在图像-文本对上采用跨模态自监督，利用多视图监督从广泛的放射学数据中学习丰富的表示。这包括多个视图和相应的伪放射学报告。提出了一种新的联合视觉-文本学习策略，以提高不同数据类型和任务的泛化和准确性性能，以区分乳腺组织或癌症特征（钙化、肿块），并利用这些模式来理解乳腺X线照片图像并预测癌症风险。

Result: 在私人和公开数据集上评估了该方法，证明了所提出的模型在三个分类任务中实现了最先进的性能：(1) 恶性肿瘤分类，(2) 亚型分类，以及 (3) 基于图像的癌症风险预测。

Conclusion: 该模型表现出强大的数据效率，在合成文本报告上训练时，优于现有的完全监督或 VLM 基线，而无需实际的放射学报告。

Abstract: Large annotated datasets are essential for training robust Computer-Aided
Diagnosis (CAD) models for breast cancer detection or risk prediction. However,
acquiring such datasets with fine-detailed annotation is both costly and
time-consuming. Vision-Language Models (VLMs), such as CLIP, which are
pre-trained on large image-text pairs, offer a promising solution by enhancing
robustness and data efficiency in medical imaging tasks. This paper introduces
a novel Multi-View Mammography and Language Model for breast cancer
classification and risk prediction, trained on a dataset of paired mammogram
images and synthetic radiology reports. Our MV-MLM leverages multi-view
supervision to learn rich representations from extensive radiology data by
employing cross-modal self-supervision across image-text pairs. This includes
multiple views and the corresponding pseudo-radiology reports. We propose a
novel joint visual-textual learning strategy to enhance generalization and
accuracy performance over different data types and tasks to distinguish breast
tissues or cancer characteristics(calcification, mass) and utilize these
patterns to understand mammography images and predict cancer risk. We evaluated
our method on both private and publicly available datasets, demonstrating that
the proposed model achieves state-of-the-art performance in three
classification tasks: (1) malignancy classification, (2) subtype
classification, and (3) image-based cancer risk prediction. Furthermore, the
model exhibits strong data efficiency, outperforming existing fully supervised
or VLM baselines while trained on synthetic text reports and without the need
for actual radiology reports.

</details>


### [70] [Detecting Unauthorized Vehicles using Deep Learning for Smart Cities: A Case Study on Bangladesh](https://arxiv.org/abs/2510.26154)
*Sudipto Das Sukanto,Diponker Roy,Fahim Shakil,Nirjhar Singha,Abdullah Asik,Aniket Joarder,Mridha Md Nafis Fuad,Muhammad Ibrahim*

Main category: cs.CV

TL;DR: 本文提出了一种基于机器学习的方法，用于在交通图像中自动检测电动三轮车。


<details>
  <summary>Details</summary>
Motivation: 由于电动三轮车与其它车辆相似，现有监控系统难以对其进行监控，而人工视频分析又过于耗时，因此需要自动检测电动三轮车。

Method: 使用YOLOv8模型进行实时目标检测，并使用包含1730个带注释图像的数据集进行训练。

Result: 该模型在实时电动三轮车检测中表现良好，mAP50为83.447%，二元精度和召回率值均高于78%。

Conclusion: 该模型在处理密集和稀疏交通场景中均有效，并且数据集已公开发布以供进一步研究。

Abstract: Modes of transportation vary across countries depending on geographical
location and cultural context. In South Asian countries rickshaws are among the
most common means of local transport. Based on their mode of operation,
rickshaws in cities across Bangladesh can be broadly classified into non-auto
(pedal-powered) and auto-rickshaws (motorized). Monitoring the movement of
auto-rickshaws is necessary as traffic rules often restrict auto-rickshaws from
accessing certain routes. However, existing surveillance systems make it quite
difficult to monitor them due to their similarity to other vehicles, especially
non-auto rickshaws whereas manual video analysis is too time-consuming. This
paper presents a machine learning-based approach to automatically detect
auto-rickshaws in traffic images. In this system, we used real-time object
detection using the YOLOv8 model. For training purposes, we prepared a set of
1,730 annotated images that were captured under various traffic conditions. The
results show that our proposed model performs well in real-time auto-rickshaw
detection and offers an mAP50 of 83.447% and binary precision and recall values
above 78%, demonstrating its effectiveness in handling both dense and sparse
traffic scenarios. The dataset has been publicly released for further research.

</details>


### [71] [CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark](https://arxiv.org/abs/2510.26160)
*Jiaqi Wang,Xiao Yang,Kai Sun,Parth Suresh,Sanat Sharma,Adam Czyzewski,Derek Andersen,Surya Appini,Arkav Banerjee,Sajal Choudhary,Shervin Ghasemlou,Ziqiang Guan,Akil Iyer,Haidar Khan,Lingkun Kong,Roy Luo,Tiffany Ma,Zhen Qiao,David Tran,Wenfang Xu,Skyler Yeatman,Chen Zhou,Gunveer Gujral,Yinglong Xia,Shane Moon,Nicolas Scheffer,Nirav Shah,Eun Chang,Yue Liu,Florian Metze,Tammy Stark,Zhaleh Feizollahi,Andrea Jessee,Mangesh Pujari,Ahmed Aly,Babak Damavandi,Rakesh Wanga,Anuj Kumar,Rohit Patel,Wen-tau Yih,Xin Luna Dong*

Main category: cs.CV

TL;DR: CRAG-MM: A new benchmark for Multi-Modal Retrieval-Augmented Generation (MM-RAG), designed for wearable scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing MM-RAG benchmarks lack comprehensiveness, especially for wearable devices.

Method: Introduces CRAG-MM, a dataset with 6.5K image-question-answer triplets and 2K visual-based multi-turn conversations, including egocentric images and diverse real-world challenges. Three tasks are designed: single-source augmentation, multi-source augmentation, and multi-turn conversations.

Result: Straightforward RAG approaches achieve only 32% and 43% truthfulness on CRAG-MM single- and multi-turn QA, respectively, while state-of-the-art industry solutions show similar quality. KDD Cup 2025 used this benchmark and winning solutions improved baseline performance by 28%.

Conclusion: CRAG-MM reveals significant room for improvement in MM-RAG and has already impacted the field, as demonstrated by the KDD Cup 2025 results.

Abstract: Wearable devices such as smart glasses are transforming the way people
interact with their surroundings, enabling users to seek information regarding
entities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG)
plays a key role in supporting such questions, yet there is still no
comprehensive benchmark for this task, especially regarding wearables
scenarios. To fill this gap, we present CRAG-MM -- a Comprehensive RAG
benchmark for Multi-modal Multi-turn conversations. CRAG-MM contains a diverse
set of 6.5K (image, question, answer) triplets and 2K visual-based multi-turn
conversations across 13 domains, including 6.2K egocentric images designed to
mimic captures from wearable devices. We carefully constructed the questions to
reflect real-world scenarios and challenges, including five types of
image-quality issues, six question types, varying entity popularity, differing
information dynamism, and different conversation turns. We design three tasks:
single-source augmentation, multi-source augmentation, and multi-turn
conversations -- each paired with an associated retrieval corpus and APIs for
both image-KG retrieval and webpage retrieval. Our evaluation shows that
straightforward RAG approaches achieve only 32% and 43% truthfulness on CRAG-MM
single- and multi-turn QA, respectively, whereas state-of-the-art industry
solutions have similar quality (32%/45%), underscoring ample room for
improvement. The benchmark has hosted KDD Cup 2025, attracting about 1K
participants and 5K submissions, with winning solutions improving baseline
performance by 28%, highlighting its early impact on advancing the field.

</details>


### [72] [MoTDiff: High-resolution Motion Trajectory estimation from a single blurred image using Diffusion models](https://arxiv.org/abs/2510.26173)
*Wontae Choi,Jaelin Lee,Hyung Sup Yun,Byeungwoo Jeon,Il Yong Chun*

Main category: cs.CV

TL;DR: 提出了一种使用扩散模型的高分辨率运动轨迹估计框架（MoTDiff），旨在从单个运动模糊图像中高质量地估计高分辨率运动轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有运动表示通常质量较低，即粗粒度和不准确。因此，本文旨在提高运动估计的质量。

Method: 1) 提出一个新的条件扩散框架，该框架使用从单个模糊图像中提取的多尺度特征图作为条件。2) 提出一种新的训练方法，可以促进精细运动轨迹的精确识别，运动路径的整体形状和位置的一致估计，以及沿运动轨迹的像素连接。

Result: 实验表明，所提出的 MoTDiff 在盲图像去模糊和编码曝光摄影应用中优于最先进的方法。

Conclusion: MoTDiff 是一种有效的运动轨迹估计框架。

Abstract: Accurate estimation of motion information is crucial in diverse computational
imaging and computer vision applications. Researchers have investigated various
methods to extract motion information from a single blurred image, including
blur kernels and optical flow. However, existing motion representations are
often of low quality, i.e., coarse-grained and inaccurate. In this paper, we
propose the first high-resolution (HR) Motion Trajectory estimation framework
using Diffusion models (MoTDiff). Different from existing motion
representations, we aim to estimate an HR motion trajectory with high-quality
from a single motion-blurred image. The proposed MoTDiff consists of two key
components: 1) a new conditional diffusion framework that uses multi-scale
feature maps extracted from a single blurred image as a condition, and 2) a new
training method that can promote precise identification of a fine-grained
motion trajectory, consistent estimation of overall shape and position of a
motion path, and pixel connectivity along a motion trajectory. Our experiments
demonstrate that the proposed MoTDiff can outperform state-of-the-art methods
in both blind image deblurring and coded exposure photography applications.

</details>


### [73] [ConceptScope: Characterizing Dataset Bias via Disentangled Visual Concepts](https://arxiv.org/abs/2510.26186)
*Jinho Choi,Hyesu Lim,Steffen Schneider,Jaegul Choo*

Main category: cs.CV

TL;DR: ConceptScope是一个可扩展的自动化框架，通过发现和量化人类可解释的概念来分析视觉数据集，使用在视觉基础模型表示上训练的稀疏自动编码器。


<details>
  <summary>Details</summary>
Motivation: 机器学习数据集中普遍存在数据集偏差，其中数据点偏向于某些概念。然而，如果没有代价高昂的细粒度属性注释，系统地识别这些偏差具有挑战性。

Method: ConceptScope通过在视觉基础模型表示上训练的稀疏自动编码器，发现和量化人类可解释的概念。ConceptScope根据概念的语义相关性和与类标签的统计相关性，将概念分为目标、上下文和偏差类型，从而通过基于概念的子分组实现类级别的数据集特征描述、偏差识别和鲁棒性评估。

Result: ConceptScope捕获了广泛的视觉概念，包括对象、纹理、背景、面部属性、情绪和动作，这通过与带注释的数据集进行比较得到验证。此外，概念激活产生与语义上有意义的图像区域对齐的空间属性。ConceptScope可靠地检测已知的偏差（例如，Waterbirds中的背景偏差）并发现先前未注释的偏差（例如，ImageNet中共同出现的对象），从而为数据集审计和模型诊断提供了一个实用的工具。

Conclusion: ConceptScope是一个用于分析视觉数据集的实用工具，它可以自动发现和量化人类可解释的概念，并可靠地检测已知和未知的偏差。

Abstract: Dataset bias, where data points are skewed to certain concepts, is ubiquitous
in machine learning datasets. Yet, systematically identifying these biases is
challenging without costly, fine-grained attribute annotations. We present
ConceptScope, a scalable and automated framework for analyzing visual datasets
by discovering and quantifying human-interpretable concepts using Sparse
Autoencoders trained on representations from vision foundation models.
ConceptScope categorizes concepts into target, context, and bias types based on
their semantic relevance and statistical correlation to class labels, enabling
class-level dataset characterization, bias identification, and robustness
evaluation through concept-based subgrouping. We validate that ConceptScope
captures a wide range of visual concepts, including objects, textures,
backgrounds, facial attributes, emotions, and actions, through comparisons with
annotated datasets. Furthermore, we show that concept activations produce
spatial attributions that align with semantically meaningful image regions.
ConceptScope reliably detects known biases (e.g., background bias in
Waterbirds) and uncovers previously unannotated ones (e.g, co-occurring objects
in ImageNet), offering a practical tool for dataset auditing and model
diagnostics.

</details>


### [74] [Sketch2PoseNet: Efficient and Generalized Sketch to 3D Human Pose Prediction](https://arxiv.org/abs/2510.26196)
*Li Wang,Yiyu Zhuang,Yanwen Wang,Xun Cao,Chuan Guo,Xinxin Zuo,Hao Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种从草图估计3D人体姿势的新方法，该方法利用“从合成中学习”的策略，通过扩散模型生成合成草图数据集，并在此基础上构建端到端的数据驱动框架。


<details>
  <summary>Details</summary>
Motivation: 以往的草图到姿势方法受限于缺乏大规模的草图-3D姿势注释，主要依赖于启发式规则的优化，这种方法既耗时又限制了泛化性。

Method: 首先，训练一个扩散模型来合成草图图像从2D姿势投影从3D人体姿势，模仿草图中不成比例的人体结构。这个过程能够创建一个合成数据集，SKEP-120K，包含120k个精确的草图-3D姿势注释对，跨越各种草图风格。在此基础上，我们介绍了一个端到端的数据驱动框架，用于从不同的草图风格中估计人体姿势和形状。我们的框架结合了现有的2D姿势检测器和生成扩散先验，用于草图特征提取，以及一个前馈神经网络，用于高效的2D姿势估计。结合多个启发式损失函数，以保证导出的3D姿势和检测到的2D姿势之间的几何一致性，同时保持精确的自接触。

Result: 在草图到姿势的任务中，该模型在估计精度和速度上都大大超过了以往的模型。

Conclusion: 该模型在草图到姿势的任务中，在估计精度和速度上都大大超过了以往的模型，证明了该方法的有效性

Abstract: 3D human pose estimation from sketches has broad applications in computer
animation and film production. Unlike traditional human pose estimation, this
task presents unique challenges due to the abstract and disproportionate nature
of sketches. Previous sketch-to-pose methods, constrained by the lack of
large-scale sketch-3D pose annotations, primarily relied on optimization with
heuristic rules-an approach that is both time-consuming and limited in
generalizability. To address these challenges, we propose a novel approach
leveraging a "learn from synthesis" strategy. First, a diffusion model is
trained to synthesize sketch images from 2D poses projected from 3D human
poses, mimicking disproportionate human structures in sketches. This process
enables the creation of a synthetic dataset, SKEP-120K, consisting of 120k
accurate sketch-3D pose annotation pairs across various sketch styles. Building
on this synthetic dataset, we introduce an end-to-end data-driven framework for
estimating human poses and shapes from diverse sketch styles. Our framework
combines existing 2D pose detectors and generative diffusion priors for sketch
feature extraction with a feed-forward neural network for efficient 2D pose
estimation. Multiple heuristic loss functions are incorporated to guarantee
geometric coherence between the derived 3D poses and the detected 2D poses
while preserving accurate self-contacts. Qualitative, quantitative, and
subjective evaluations collectively show that our model substantially surpasses
previous ones in both estimation accuracy and speed for sketch-to-pose tasks.

</details>


### [75] [Developing a Multi-task Ensemble Geometric Deep Network for Supply Chain Sustainability and Risk Management](https://arxiv.org/abs/2510.26203)
*Mehdi Khaleghi,Nastaran Khaleghi,Sobhan Sheykhivand,Sebelan Danishvar*

Main category: cs.CV

TL;DR: 提出了一种用于供应链可持续性的新型几何深度网络，通过预测交货状态、产品分类和边缘分类来管理风险并提高效率。


<details>
  <summary>Details</summary>
Motivation: 供应链的可持续性在控制供应链以实现最佳性能方面起着关键作用。管理供应链中发生的风险是发展网络可持续性和提高供应链绩效效率的根本问题。产品的正确分类是可持续供应链中的另一个基本要素。

Method: 提出了一种新的几何深度网络来提出一个集成深度网络。所提出的 Chebyshev 集成几何网络 (Ch-EGN) 是一种混合卷积和几何深度学习。

Result: 对于风险管理，集成网络的平均准确率为 98.95%。在 5 个产品组分类和 4 个产品关系分类方面，可持续供应链的平均准确率分别为 100% 和 98.07%。25 家公司关系分类的平均准确率为 92.37%。

Conclusion: 结果证实，与最先进的方法相比，所提出的方法具有平均改进和效率。

Abstract: The sustainability of supply chain plays a key role in achieving optimal
performance in controlling the supply chain. The management of risks that occur
in a supply chain is a fundamental problem for the purpose of developing the
sustainability of the network and elevating the performance efficiency of the
supply chain. The correct classification of products is another essential
element in a sustainable supply chain. Acknowledging recent breakthroughs in
the context of deep networks, several architectural options have been deployed
to analyze supply chain datasets. A novel geometric deep network is used to
propose an ensemble deep network. The proposed Chebyshev ensemble geometric
network (Ch-EGN) is a hybrid convolutional and geometric deep learning. This
network is proposed to leverage the information dependencies in supply chain to
derive invisible states of samples in the database. The functionality of the
proposed deep network is assessed on the two different databases. The
SupplyGraph Dataset and DataCo are considered in this research. The prediction
of delivery status of DataCo supply chain is done for risk administration. The
product classification and edge classification are performed using the
SupplyGraph database to enhance the sustainability of the supply network. An
average accuracy of 98.95% is obtained for the ensemble network for risk
management. The average accuracy of 100% and 98.07% are obtained for
sustainable supply chain in terms of 5 product group classification and 4
product relation classification, respectively. The average accuracy of 92.37%
is attained for 25 company relation classification. The results confirm an
average improvement and efficiency of the proposed method compared to the
state-of-the-art approaches.

</details>


### [76] [OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal Document Layout Generation](https://arxiv.org/abs/2510.26213)
*Hengrui Kang,Zhuangcheng Gu,Zhiyuan Zhao,Zichen Wen,Bin Wang,Weijia Li,Conghui He*

Main category: cs.CV

TL;DR: 本文提出了OmniLayout-1M数据集，包含百万级别的多样文档布局，并提出了OmniLayout-LLM模型，使用粗到精的两阶段学习范式，在多个领域都取得了很好的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的文档布局生成研究主要集中在学术论文上，缺乏对报纸、杂志等开放世界类型的研究。现有方法在复杂领域表现不佳，难以连贯地安排长序列。

Method: 1. 构建了包含六种常见文档类型的百万级别数据集OmniLayout-1M。2. 提出了OmniLayout-LLM模型，采用两阶段粗到精的学习范式：首先从OmniLayout-1M学习通用布局原则，然后将知识转移到具有细粒度注释的特定领域。

Result: 在M^6Doc数据集上的大量实验表明，该方法在多个领域都取得了强大的性能，大大超过了现有的布局生成专家和一些最新的通用LLM。

Conclusion: 本文构建了一个新的大规模文档布局数据集，并提出了一个新的文档布局生成模型，实验结果表明该模型在多个领域都取得了很好的效果。

Abstract: Document AI has advanced rapidly and is attracting increasing attention. Yet,
while most efforts have focused on document layout analysis (DLA), its
generative counterpart, document layout generation, remains underexplored. A
major obstacle lies in the scarcity of diverse layouts: academic papers with
Manhattan-style structures dominate existing studies, while open-world genres
such as newspapers and magazines remain severely underrepresented. To address
this gap, we curate OmniLayout-1M, the first million-scale dataset of diverse
document layouts, covering six common document types and comprising
contemporary layouts collected from multiple sources. Moreover, since existing
methods struggle in complex domains and often fail to arrange long sequences
coherently, we introduce OmniLayout-LLM, a 0.5B model with designed two-stage
Coarse-to-Fine learning paradigm: 1) learning universal layout principles from
OmniLayout-1M with coarse category definitions, and 2) transferring the
knowledge to a specific domain with fine-grained annotations. Extensive
experiments demonstrate that our approach achieves strong performance on
multiple domains in M$^{6}$Doc dataset, substantially surpassing both existing
layout generation experts and several latest general-purpose LLMs. Our code,
models, and dataset will be publicly released.

</details>


### [77] [Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models](https://arxiv.org/abs/2510.26241)
*Shiho Matta,Lis Kanashiro Pereira,Peitao Han,Fei Cheng,Shigeru Kitazawa*

Main category: cs.CV

TL;DR: 当前视觉-语言模型在视频中的时间信息处理能力不足，尤其是在判断视频播放方向（时间箭头）方面。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型在处理视频时间信息方面存在差距，缺乏对时间连续性和因果关系的理解。

Method: 提出了一个心理物理学验证的基准测试 AoT-PsyPhyBENCH，用于测试视觉-语言模型在自然视频中推断时间方向的能力。

Result: 大多数模型表现接近随机水平，即使是最好的模型也远落后于人类在物理不可逆过程和因果手动操作上的准确性。

Conclusion: 当前的视觉-语言模型缺乏时间连续性和因果理解所需的归纳偏置，为了促进视觉-语言模型在物理和时间推理能力方面的进一步发展，发布了AoT-PsyPhyBENCH的代码和数据。

Abstract: Modern vision-language models (VLMs) excel at many multimodal tasks, yet
their grasp of temporal information in video remains weak and, crucially,
under-evaluated. We probe this gap with a deceptively simple but revealing
challenge: judging the arrow of time (AoT)-whether a short clip is played
forward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validated
benchmark that tests whether VLMs can infer temporal direction in natural
videos using the same stimuli and behavioral baselines established for humans.
Our comprehensive evaluation of open-weight and proprietary, reasoning and
non-reasoning VLMs reveals that most models perform near chance, and even the
best lag far behind human accuracy on physically irreversible processes (e.g.,
free fall, diffusion/explosion) and causal manual actions (division/addition)
that humans recognize almost instantly. These results highlight a fundamental
gap in current multimodal systems: while they capture rich visual-semantic
correlations, they lack the inductive biases required for temporal continuity
and causal understanding. We release the code and data for AoT-PsyPhyBENCH to
encourage further progress in the physical and temporal reasoning capabilities
of VLMs.

</details>


### [78] [Revisiting Generative Infrared and Visible Image Fusion Based on Human Cognitive Laws](https://arxiv.org/abs/2510.26268)
*Lin Guo,Xiaoqing Luo,Wei Xie,Zhancheng Zhang,Hui Li,Rui Wang,Zhenhua Feng,Xiaoning Song*

Main category: cs.CV

TL;DR: 提出了一种新的红外和可见图像融合方法HCLFuse，该方法通过多尺度掩码调节变分瓶颈编码器提取低层次模态信息，并结合扩散模型的概率生成能力和物理定律，以提高图像融合的结构一致性和细节质量。


<details>
  <summary>Details</summary>
Motivation: 现有红外和可见图像融合方法在平衡模态信息方面面临困境，生成融合方法重建融合图像的能力有限，且模态信息选择缺乏可解释性，影响融合结果的可靠性和一致性。

Method: 设计了一个多尺度掩码调节变分瓶颈编码器，利用后验概率建模和信息分解提取低层次模态信息。结合扩散模型的概率生成能力和物理定律，形成时变物理引导机制，自适应地调节生成过程。

Result: 在多个数据集上的实验结果表明，该方法在定性和定量评估中都取得了最先进的融合性能，并显著提高了语义分割指标。

Conclusion: 该方法从人类认知中获得灵感，在增强结构一致性和细节质量方面具有优势，证明了生成图像融合方法的有效性。

Abstract: Existing infrared and visible image fusion methods often face the dilemma of
balancing modal information. Generative fusion methods reconstruct fused images
by learning from data distributions, but their generative capabilities remain
limited. Moreover, the lack of interpretability in modal information selection
further affects the reliability and consistency of fusion results in complex
scenarios. This manuscript revisits the essence of generative image fusion
under the inspiration of human cognitive laws and proposes a novel infrared and
visible image fusion method, termed HCLFuse. First, HCLFuse investigates the
quantification theory of information mapping in unsupervised fusion networks,
which leads to the design of a multi-scale mask-regulated variational
bottleneck encoder. This encoder applies posterior probability modeling and
information decomposition to extract accurate and concise low-level modal
information, thereby supporting the generation of high-fidelity structural
details. Furthermore, the probabilistic generative capability of the diffusion
model is integrated with physical laws, forming a time-varying physical
guidance mechanism that adaptively regulates the generation process at
different stages, thereby enhancing the ability of the model to perceive the
intrinsic structure of data and reducing dependence on data quality.
Experimental results show that the proposed method achieves state-of-the-art
fusion performance in qualitative and quantitative evaluations across multiple
datasets and significantly improves semantic segmentation metrics. This fully
demonstrates the advantages of this generative image fusion method, drawing
inspiration from human cognition, in enhancing structural consistency and
detail quality.

</details>


### [79] [Exploring Complementarity and Explainability in CNNs for Periocular Verification Across Acquisition Distances](https://arxiv.org/abs/2510.26282)
*Fernando Alonso-Fernandez,Kevin Hernandez Diaz,Jose M. Buades,Kiran Raja,Josef Bigun*

Main category: cs.CV

TL;DR: 研究了不同CNN在UBIPr数据库上不同距离的眼周验证的互补性


<details>
  <summary>Details</summary>
Motivation: 研究不同CNN在眼周验证中的互补性，并在UBIPr数据库上进行验证。

Method: 在VGGFace2的大量眼睛图像上训练三种复杂度递增的架构（SqueezeNet、MobileNetv2和ResNet50），使用cosine和chi2度量分析性能，比较不同的网络初始化，并通过逻辑回归应用分数级融合。此外，使用LIME热图和Jensen-Shannon散度来比较CNN的注意力模式。

Result: ResNet50单独表现最佳，但融合提供了显著的增益，特别是当结合所有三个网络时。热图显示，网络通常关注给定图像的不同区域，这解释了它们的互补性。该方法显著优于先前在UBIPr上的工作，实现了新的技术水平。

Conclusion: 通过结合不同CNN，特别是ResNet50，并在UBIPr数据库上实现了新的技术水平，证明了不同CNN在眼周验证中的互补性。

Abstract: We study the complementarity of different CNNs for periocular verification at
different distances on the UBIPr database. We train three architectures of
increasing complexity (SqueezeNet, MobileNetv2, and ResNet50) on a large set of
eye crops from VGGFace2. We analyse performance with cosine and chi2 metrics,
compare different network initialisations, and apply score-level fusion via
logistic regression. In addition, we use LIME heatmaps and Jensen-Shannon
divergence to compare attention patterns of the CNNs. While ResNet50
consistently performs best individually, the fusion provides substantial gains,
especially when combining all three networks. Heatmaps show that networks
usually focus on distinct regions of a given image, which explains their
complementarity. Our method significantly outperforms previous works on UBIPr,
achieving a new state-of-the-art.

</details>


### [80] [Beyond Imitation: Constraint-Aware Trajectory Generation with Flow Matching For End-to-End Autonomous Driving](https://arxiv.org/abs/2510.26292)
*Lin Liu,Guanyi Yu,Ziying Song,Junqiao Li,Caiyan Jia,Feiyang Jia,Peiliang Wu,Yandan Luo*

Main category: cs.CV

TL;DR: 提出了一种新的规划框架CATG，它利用约束流匹配来解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的模仿学习方法经常出现模式崩溃，无法产生多样化的轨迹假设。同时，现有的生成方法难以将关键的安全和物理约束直接纳入生成过程。

Method: 显式地对流匹配过程进行建模，并在流匹配过程中直接施加显式约束，以确保生成的轨迹符合重要的安全和运动学规则。此外，CATG将驾驶激进程度参数化为生成过程中的控制信号。

Result: 在NavSim v2挑战赛中，CATG获得了第二名，EPDMS评分为51.31，并荣获创新奖。

Conclusion: CATG 是一种很有前途的规划框架，它能够生成多样化且符合约束的轨迹。

Abstract: Planning is a critical component of end-to-end autonomous driving. However,
prevailing imitation learning methods often suffer from mode collapse, failing
to produce diverse trajectory hypotheses. Meanwhile, existing generative
approaches struggle to incorporate crucial safety and physical constraints
directly into the generative process, necessitating an additional optimization
stage to refine their outputs. To address these limitations, we propose CATG, a
novel planning framework that leverages Constrained Flow Matching. Concretely,
CATG explicitly models the flow matching process, which inherently mitigates
mode collapse and allows for flexible guidance from various conditioning
signals. Our primary contribution is the novel imposition of explicit
constraints directly within the flow matching process, ensuring that the
generated trajectories adhere to vital safety and kinematic rules. Secondly,
CATG parameterizes driving aggressiveness as a control signal during
generation, enabling precise manipulation of trajectory style. Notably, on the
NavSim v2 challenge, CATG achieved 2nd place with an EPDMS score of 51.31 and
was honored with the Innovation Award.

</details>


### [81] [Leveraging Large-Scale Face Datasets for Deep Periocular Recognition via Ocular Cropping](https://arxiv.org/abs/2510.26294)
*Fernando Alonso-Fernandez,Kevin Hernandez-Diaz,Jose Maria Buades Rubio,Josef Bigun*

Main category: cs.CV

TL;DR: 本文研究了眼周生物识别技术，重点在于使用卷积神经网络进行眼周区域识别，并在大规模VGGFace2数据库上进行了训练和评估。


<details>
  <summary>Details</summary>
Motivation: 眼周区域具有高区分度且采集约束小，但现有研究通常依赖于小规模数据集进行训练。

Method: 本文评估了三种不同深度和复杂度的卷积神经网络结构在眼周识别中的有效性，使用从大规模VGGFace2数据库中提取的1,907,572个眼周图像进行训练。

Result: 在VGGFace2数据集上获得的等错误率（EER）为9-15%，在UFPR-Periocular数据集上获得的EER为1-2%。

Conclusion: 本文在UFPR数据集上取得了目前最低的EER，表明该方法在高质量图像和一致采集协议下表现良好。

Abstract: We focus on ocular biometrics, specifically the periocular region (the area
around the eye), which offers high discrimination and minimal acquisition
constraints. We evaluate three Convolutional Neural Network architectures of
varying depth and complexity to assess their effectiveness for periocular
recognition. The networks are trained on 1,907,572 ocular crops extracted from
the large-scale VGGFace2 database. This significantly contrasts with existing
works, which typically rely on small-scale periocular datasets for training
having only a few thousand images. Experiments are conducted with ocular images
from VGGFace2-Pose, a subset of VGGFace2 containing in-the-wild face images,
and the UFPR-Periocular database, which consists of selfies captured via mobile
devices with user guidance on the screen. Due to the uncontrolled conditions of
VGGFace2, the Equal Error Rates (EERs) obtained with ocular crops range from
9-15%, noticeably higher than the 3-6% EERs achieved using full-face images. In
contrast, UFPR-Periocular yields significantly better performance (EERs of
1-2%), thanks to higher image quality and more consistent acquisition
protocols. To the best of our knowledge, these are the lowest reported EERs on
the UFPR dataset to date.

</details>


### [82] [Towards Realistic Earth-Observation Constellation Scheduling: Benchmark and Methodology](https://arxiv.org/abs/2510.26297)
*Luting Wang,Yinghao Xiang,Hongliang Huang,Dongjun Li,Chen Gao,Si Liu*

Main category: cs.CV

TL;DR: 本文提出了一个用于敏捷地球观测卫星（AEOS）星座调度的统一框架，包括一个标准化的基准测试套件AEOS-Bench和一个新的调度模型AEOS-Former。


<details>
  <summary>Details</summary>
Motivation: 现有的卫星调度方法通常简化了复杂性，限制了它们的实际性能。本文旨在解决这个问题。

Method: 本文构建了一个包含3,907个卫星资产和16,410个场景的基准测试套件AEOS-Bench，并提出了一个基于Transformer的调度模型AEOS-Former，该模型结合了约束感知注意力机制和专门的内部约束模块。

Result: 实验结果表明，AEOS-Former在任务完成和能源效率方面优于基线模型。

Conclusion: AEOS-Former为AEOS星座调度提供了一个鲁棒的解决方案。

Abstract: Agile Earth Observation Satellites (AEOSs) constellations offer unprecedented
flexibility for monitoring the Earth's surface, but their scheduling remains
challenging under large-scale scenarios, dynamic environments, and stringent
constraints. Existing methods often simplify these complexities, limiting their
real-world performance. We address this gap with a unified framework
integrating a standardized benchmark suite and a novel scheduling model. Our
benchmark suite, AEOS-Bench, contains $3,907$ finely tuned satellite assets and
$16,410$ scenarios. Each scenario features $1$ to $50$ satellites and $50$ to
$300$ imaging tasks. These scenarios are generated via a high-fidelity
simulation platform, ensuring realistic satellite behavior such as orbital
dynamics and resource constraints. Ground truth scheduling annotations are
provided for each scenario. To our knowledge, AEOS-Bench is the first
large-scale benchmark suite tailored for realistic constellation scheduling.
Building upon this benchmark, we introduce AEOS-Former, a Transformer-based
scheduling model that incorporates a constraint-aware attention mechanism. A
dedicated internal constraint module explicitly models the physical and
operational limits of each satellite. Through simulation-based iterative
learning, AEOS-Former adapts to diverse scenarios, offering a robust solution
for AEOS constellation scheduling. Experimental results demonstrate that
AEOS-Former outperforms baseline models in task completion and energy
efficiency, with ablation studies highlighting the contribution of each
component. Code and data are provided in
https://github.com/buaa-colalab/AEOSBench.

</details>


### [83] [Exploring the correlation between the type of music and the emotions evoked: A study using subjective questionnaires and EEG](https://arxiv.org/abs/2510.26304)
*Jelizaveta Jankowska,Bożena Kostek,Fernando Alonso-Fernandez,Prayag Tiwari*

Main category: cs.CV

TL;DR: 研究不同类型的音乐如何影响人类情绪，通过主观调查和脑电图测量进行分析。


<details>
  <summary>Details</summary>
Motivation: 旨在展示不同音乐流派对情绪的影响。

Method: 使用脑电图头盔进行脑部活动测量，并进行主观调查，研究对象是不同性别和音乐偏好的参与者。

Result: 分析揭示了情绪和观察到的脑部活动之间的联系。

Conclusion: 不同类型的音乐会对人类情绪产生影响，并且情绪与脑部活动之间存在关联。

Abstract: The subject of this work is to check how different types of music affect
human emotions. While listening to music, a subjective survey and brain
activity measurements were carried out using an EEG helmet. The aim is to
demonstrate the impact of different music genres on emotions. The research
involved a diverse group of participants of different gender and musical
preferences. This had the effect of capturing a wide range of emotional
responses to music. After the experiment, a relationship analysis of the
respondents' questionnaires with EEG signals was performed. The analysis
revealed connections between emotions and observed brain activity.

</details>


### [84] [A Hybrid Framework Bridging CNN and ViT based on Theory of Evidence for Diabetic Retinopathy Grading](https://arxiv.org/abs/2510.26315)
*Junlai Qiu,Yunzhu Chen,Hao Zheng,Yawen Huang,Yuexiang Li*

Main category: cs.CV

TL;DR: 提出了一种新的基于证据理论的融合范式，以结合 CNN 和 ViT 的优势，用于糖尿病视网膜病变 (DR) 的自动诊断。


<details>
  <summary>Details</summary>
Motivation: 现有使用单一类型骨干网络的 DR 诊断方法性能已达到瓶颈，需要结合 CNN 和 ViT 的优势。

Method: 通过深度证据网络将不同骨干网络提取的特征转化为支持证据，然后形成聚合意见，自适应地调整不同骨干网络之间的融合模式。

Result: 在两个公开的 DR 分级数据集上进行了评估，结果表明该混合模型提高了 DR 分级的准确性，并为特征融合和决策提供了出色的可解释性。

Conclusion: 该方法有效地融合了 CNN 和 ViT 的特征，提高了 DR 诊断的准确性和可解释性。

Abstract: Diabetic retinopathy (DR) is a leading cause of vision loss among middle-aged
and elderly people, which significantly impacts their daily lives and mental
health. To improve the efficiency of clinical screening and enable the early
detection of DR, a variety of automated DR diagnosis systems have been recently
established based on convolutional neural network (CNN) or vision Transformer
(ViT). However, due to the own shortages of CNN / ViT, the performance of
existing methods using single-type backbone has reached a bottleneck. One
potential way for the further improvements is integrating different kinds of
backbones, which can fully leverage the respective strengths of them
(\emph{i.e.,} the local feature extraction capability of CNN and the global
feature capturing ability of ViT). To this end, we propose a novel paradigm to
effectively fuse the features extracted by different backbones based on the
theory of evidence. Specifically, the proposed evidential fusion paradigm
transforms the features from different backbones into supporting evidences via
a set of deep evidential networks. With the supporting evidences, the
aggregated opinion can be accordingly formed, which can be used to adaptively
tune the fusion pattern between different backbones and accordingly boost the
performance of our hybrid model. We evaluated our method on two publicly
available DR grading datasets. The experimental results demonstrate that our
hybrid model not only improves the accuracy of DR grading, compared to the
state-of-the-art frameworks, but also provides the excellent interpretability
for feature fusion and decision-making.

</details>


### [85] [GLYPH-SR: Can We Achieve Both High-Quality Image Super-Resolution and High-Fidelity Text Recovery via VLM-guided Latent Diffusion Model?](https://arxiv.org/abs/2510.26339)
*Mingyu Sung,Seungjae Ham,Kangwoo Kim,Yeokyoung Yoon,Sangseok Yun,Il-Min Kim,Jae-Mo Kang*

Main category: cs.CV

TL;DR: 提出了一种名为 GLYPH-SR 的视觉-语言引导扩散框架，用于提高场景文本图像的超分辨率重建效果，既保证文本的可读性又保证视觉感知质量。


<details>
  <summary>Details</summary>
Motivation: 以往的超分辨率研究对字符级别的错误不敏感，并且关注简化的文本基准，忽略了复杂自然场景中文本的挑战。因此，需要显式地优化文本可读性和感知质量。

Method: GLYPH-SR 采用由 OCR 数据引导的文本 SR 融合 ControlNet (TS-ControlNet)，以及在以文本为中心和以场景为中心的指导之间交替的乒乓调度器。为了实现有针对性的文本恢复，在保持主 SR 分支冻结的同时，在合成语料库上训练这些组件。

Result: 在 SVT、SCUT-CTW1500 和 CUTE80 数据集上，GLYPH-SR 在 x4 和 x8 的放大倍数下，比扩散/GAN 基线提高了高达 +15.18 个百分点的 OCR F1 值，同时保持了具有竞争力的 MANIQA、CLIP-IQA 和 MUSIQ 指标。

Conclusion: GLYPH-SR 旨在同时满足高可读性和高视觉真实感这两个目标，从而提供既看起来正确又读起来正确的 SR 效果。

Abstract: Image super-resolution(SR) is fundamental to many vision system-from
surveillance and autonomy to document analysis and retail analytics-because
recovering high-frequency details, especially scene-text, enables reliable
downstream perception. Scene-text, i.e., text embedded in natural images such
as signs, product labels, and storefronts, often carries the most actionable
information; when characters are blurred or hallucinated, optical character
recognition(OCR) and subsequent decisions fail even if the rest of the image
appears sharp. Yet previous SR research has often been tuned to distortion
(PSNR/SSIM) or learned perceptual metrics (LIPIS, MANIQA, CLIP-IQA, MUSIQ) that
are largely insensitive to character-level errors. Furthermore, studies that do
address text SR often focus on simplified benchmarks with isolated characters,
overlooking the challenges of text within complex natural scenes. As a result,
scene-text is effectively treated as generic texture. For SR to be effective in
practical deployments, it is therefore essential to explicitly optimize for
both text legibility and perceptual quality. We present GLYPH-SR, a
vision-language-guided diffusion framework that aims to achieve both objectives
jointly. GLYPH-SR utilizes a Text-SR Fusion ControlNet(TS-ControlNet) guided by
OCR data, and a ping-pong scheduler that alternates between text- and
scene-centric guidance. To enable targeted text restoration, we train these
components on a synthetic corpus while keeping the main SR branch frozen.
Across SVT, SCUT-CTW1500, and CUTE80 at x4, and x8, GLYPH-SR improves OCR F1 by
up to +15.18 percentage points over diffusion/GAN baseline (SVT x8, OpenOCR)
while maintaining competitive MANIQA, CLIP-IQA, and MUSIQ. GLYPH-SR is designed
to satisfy both objectives simultaneously-high readability and high visual
realism-delivering SR that looks right and reds right.

</details>


### [86] [EEG-Driven Image Reconstruction with Saliency-Guided Diffusion Models](https://arxiv.org/abs/2510.26391)
*Igor Abramov,Ilya Makarov*

Main category: cs.CV

TL;DR: 提出了一种双重条件框架，结合脑电图嵌入和空间显着性图，以增强图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有的脑电图驱动的图像重建方法通常忽略空间注意力机制，限制了保真度和语义连贯性。

Method: 利用自适应思维映射器（ATM）进行脑电图特征提取，并通过低秩适应（LoRA）微调稳定扩散2.1，以将神经信号与视觉语义对齐，而 ControlNet 分支则以显着性图为条件进行空间控制。

Result: 在 THINGS-EEG 上评估，该方法在低级和高级图像特征的质量上都比现有方法有了显着提高，同时与人类视觉注意力高度对齐。

Conclusion: 注意力先验可以解决脑电图的模糊性，从而实现高保真重建，并在医学诊断和神经适应性界面中具有应用，通过有效适应预训练的扩散模型来推进神经解码。

Abstract: Existing EEG-driven image reconstruction methods often overlook spatial
attention mechanisms, limiting fidelity and semantic coherence. To address
this, we propose a dual-conditioning framework that combines EEG embeddings
with spatial saliency maps to enhance image generation. Our approach leverages
the Adaptive Thinking Mapper (ATM) for EEG feature extraction and fine-tunes
Stable Diffusion 2.1 via Low-Rank Adaptation (LoRA) to align neural signals
with visual semantics, while a ControlNet branch conditions generation on
saliency maps for spatial control. Evaluated on THINGS-EEG, our method achieves
a significant improvement in the quality of low- and high-level image features
over existing approaches. Simultaneously, strongly aligning with human visual
attention. The results demonstrate that attentional priors resolve EEG
ambiguities, enabling high-fidelity reconstructions with applications in
medical diagnostics and neuroadaptive interfaces, advancing neural decoding
through efficient adaptation of pre-trained diffusion models.

</details>


### [87] [LoCoT2V-Bench: A Benchmark for Long-Form and Complex Text-to-Video Generation](https://arxiv.org/abs/2510.26412)
*Xiangqing Zheng,Chengyue Wu,Kehai Chen,Min Zhang*

Main category: cs.CV

TL;DR: 提出了一个专门用于评估复杂条件下长视频生成的基准测试LoCoT2V-Bench。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试大多依赖于简化的提示，忽略了与提示的细粒度对齐以及叙事连贯性和主题表达等抽象维度。

Method: 构建了一个多维度评估框架，包括事件级别对齐、细粒度时间一致性、内容清晰度以及人类期望实现度（HERD）。

Result: 对九种有代表性的LVG模型进行了综合评估，发现当前方法在基本的视觉和时间方面表现良好，但在事件间一致性、细粒度对齐和高级主题坚持等方面存在不足。

Conclusion: LoCoT2V-Bench 为评估长篇复杂文本到视频的生成提供了一个全面可靠的平台，并突出了未来方法改进的关键方向。

Abstract: Recently text-to-video generation has made impressive progress in producing
short, high-quality clips, but evaluating long-form outputs remains a major
challenge especially when processing complex prompts. Existing benchmarks
mostly rely on simplified prompts and focus on low-level metrics, overlooking
fine-grained alignment with prompts and abstract dimensions such as narrative
coherence and thematic expression. To address these gaps, we propose
LoCoT2V-Bench, a benchmark specifically designed for long video generation
(LVG) under complex input conditions. Based on various real-world videos,
LoCoT2V-Bench introduces a suite of realistic and complex prompts incorporating
elements like scene transitions and event dynamics. Moreover, it constructs a
multi-dimensional evaluation framework that includes our newly proposed metrics
such as event-level alignment, fine-grained temporal consistency, content
clarity, and the Human Expectation Realization Degree (HERD) that focuses on
more abstract attributes like narrative flow, emotional response, and character
development. Using this framework, we conduct a comprehensive evaluation of
nine representative LVG models, finding that while current methods perform well
on basic visual and temporal aspects, they struggle with inter-event
consistency, fine-grained alignment, and high-level thematic adherence, etc.
Overall, LoCoT2V-Bench provides a comprehensive and reliable platform for
evaluating long-form complex text-to-video generation and highlights critical
directions for future method improvement.

</details>


### [88] [A-TPT: Angular Diversity Calibration Properties for Test-Time Prompt Tuning of Vision-Language Models](https://arxiv.org/abs/2510.26441)
*Shihab Aaqil Ahamed,Udaya S. K. P. Miriya Thanthrige,Ranga Rodrigo,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: A-TPT improves VLM calibration by promoting angular diversity in textual features, leading to better performance on distribution shifts and medical datasets.


<details>
  <summary>Details</summary>
Motivation: Lack of dispersion between textual features in test-time prompt tuning (TPT) hurts calibration performance of large vision-language models (VLMs).

Method: A-TPT introduces angular diversity to encourage uniformity in the distribution of normalized textual features by maximizing the minimum pairwise angular distance between features.

Result: A-TPT surpasses state-of-the-art TPT methods in reducing calibration error while maintaining accuracy, with superior zero-shot calibration on distribution shifts and generalization to medical datasets.

Conclusion: Promoting angular diversity achieves well-dispersed textual features, significantly improving VLM calibration during test-time adaptation.

Abstract: Test-time prompt tuning (TPT) has emerged as a promising technique for
adapting large vision-language models (VLMs) to unseen tasks without relying on
labeled data. However, the lack of dispersion between textual features can hurt
calibration performance, which raises concerns about VLMs' reliability,
trustworthiness, and safety. Current TPT approaches primarily focus on
improving prompt calibration by either maximizing average textual feature
dispersion or enforcing orthogonality constraints to encourage angular
separation. However, these methods may not always have optimal angular
separation between class-wise textual features, which implies overlooking the
critical role of angular diversity. To address this, we propose A-TPT, a novel
TPT framework that introduces angular diversity to encourage uniformity in the
distribution of normalized textual features induced by corresponding learnable
prompts. This uniformity is achieved by maximizing the minimum pairwise angular
distance between features on the unit hypersphere. We show that our approach
consistently surpasses state-of-the-art TPT methods in reducing the aggregate
average calibration error while maintaining comparable accuracy through
extensive experiments with various backbones on different datasets. Notably,
our approach exhibits superior zero-shot calibration performance on natural
distribution shifts and generalizes well to medical datasets. We provide
extensive analyses, including theoretical aspects, to establish the grounding
of A-TPT. These results highlight the potency of promoting angular diversity to
achieve well-dispersed textual features, significantly improving VLM
calibration during test-time adaptation. Our code will be made publicly
available.

</details>


### [89] [PointSt3R: Point Tracking through 3D Grounded Correspondence](https://arxiv.org/abs/2510.26443)
*Rhodri Guerrier,Adam W. Harley,Dima Damen*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D重建模型的点跟踪方法，通过3D对应关系实现。


<details>
  <summary>Details</summary>
Motivation: 利用3D重建模型在静态场景中2D和3D对应关系的潜力，并将其应用于点跟踪任务。

Method: 结合重建损失和动态对应关系的训练，以及一个可见性头，并使用少量合成数据对MASt3R进行微调。

Result: 在四个数据集上实现了有竞争力或更优越的点跟踪结果。

Conclusion: 该方法在点跟踪任务上具有有效性。

Abstract: Recent advances in foundational 3D reconstruction models, such as DUSt3R and
MASt3R, have shown great potential in 2D and 3D correspondence in static
scenes. In this paper, we propose to adapt them for the task of point tracking
through 3D grounded correspondence. We first demonstrate that these models are
competitive point trackers when focusing on static points, present in current
point tracking benchmarks ($+33.5\%$ on EgoPoints vs. CoTracker2). We propose
to combine the reconstruction loss with training for dynamic correspondence
along with a visibility head, and fine-tuning MASt3R for point tracking using a
relatively small amount of synthetic data. Importantly, we only train and
evaluate on pairs of frames where one contains the query point, effectively
removing any temporal context. Using a mix of dynamic and static point
correspondences, we achieve competitive or superior point tracking results on
four datasets (e.g. competitive on TAP-Vid-DAVIS 73.8 $\delta_{avg}$ / 85.8\%
occlusion acc. for PointSt3R compared to 75.7 / 88.3\% for CoTracker2; and
significantly outperform CoTracker3 on EgoPoints 61.3 vs 54.2 and RGB-S 87.0 vs
82.8). We also present results on 3D point tracking along with several
ablations on training datasets and percentage of dynamic correspondences.

</details>


### [90] [Towards Fine-Grained Vision-Language Alignment for Few-Shot Anomaly Detection](https://arxiv.org/abs/2510.26464)
*Yuanting Fan,Jun Liu,Xiaochen Chen,Bin-Bin Gao,Jian Li,Yong Liu,Jinlong Peng,Chengjie Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为FineGrainedAD的新框架，用于提高小样本异常检测中的异常定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于预训练的视觉-语言模型，但由于缺乏详细的文本描述，导致图像描述和patch级别的视觉异常之间存在语义不对齐。

Method: 该框架包括多级可学习提示（MLLP）和多级语义对齐（MLSA），通过自动替换和连接机制将细粒度语义引入多级可学习提示，并设计区域聚合策略和多级对齐训练。

Result: 在MVTec-AD和VisA数据集上的实验表明，所提出的FineGrainedAD在小样本设置中实现了卓越的整体性能。

Conclusion: 通过多级细粒度语义描述和对齐，FineGrainedAD能够有效提高异常定位性能。

Abstract: Few-shot anomaly detection (FSAD) methods identify anomalous regions with few
known normal samples. Most existing methods rely on the generalization ability
of pre-trained vision-language models (VLMs) to recognize potentially anomalous
regions through feature similarity between text descriptions and images.
However, due to the lack of detailed textual descriptions, these methods can
only pre-define image-level descriptions to match each visual patch token to
identify potential anomalous regions, which leads to the semantic misalignment
between image descriptions and patch-level visual anomalies, achieving
sub-optimal localization performance. To address the above issues, we propose
the Multi-Level Fine-Grained Semantic Caption (MFSC) to provide multi-level and
fine-grained textual descriptions for existing anomaly detection datasets with
automatic construction pipeline. Based on the MFSC, we propose a novel
framework named FineGrainedAD to improve anomaly localization performance,
which consists of two components: Multi-Level Learnable Prompt (MLLP) and
Multi-Level Semantic Alignment (MLSA). MLLP introduces fine-grained semantics
into multi-level learnable prompts through automatic replacement and
concatenation mechanism, while MLSA designs region aggregation strategy and
multi-level alignment training to facilitate learnable prompts better align
with corresponding visual regions. Experiments demonstrate that the proposed
FineGrainedAD achieves superior overall performance in few-shot settings on
MVTec-AD and VisA datasets.

</details>


### [91] [Representation-Level Counterfactual Calibration for Debiased Zero-Shot Recognition](https://arxiv.org/abs/2510.26466)
*Pei Peng,MingKun Xie,Hang Hao,Tong Jin,ShengJun Huang*

Main category: cs.CV

TL;DR: 视觉语言模型容易受到对象-上下文捷径的影响，导致零样本可靠性下降。为了解决这个问题，我们提出了一个因果推理方法，通过估计对象和背景的期望，并合成反事实嵌入来消除背景的影响。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型中的对象-上下文捷径会降低零样本的可靠性。

Method: 我们通过估计CLIP表示空间中的对象和背景期望，并结合对象特征与来自外部数据集、批邻居或文本描述的不同替代上下文来合成反事实嵌入。通过估计总直接效应并模拟干预，我们进一步减去仅背景激活，从而在减轻幻觉分数的同时，保留有益的对象-上下文交互。

Result: 我们的方法在不重新训练或提示设计的情况下，显着提高了上下文敏感基准测试中的最差组和平均准确率，从而建立了新的零样本最新技术水平。

Conclusion: 我们的框架提供了一种轻量级的表示级别反事实方法，为去偏见和可靠的多模态推理提供了实用的因果途径。

Abstract: Object-context shortcuts remain a persistent challenge in vision-language
models, undermining zero-shot reliability when test-time scenes differ from
familiar training co-occurrences. We recast this issue as a causal inference
problem and ask: Would the prediction remain if the object appeared in a
different environment? To answer this at inference time, we estimate object and
background expectations within CLIP's representation space, and synthesize
counterfactual embeddings by recombining object features with diverse
alternative contexts sampled from external datasets, batch neighbors, or
text-derived descriptions. By estimating the Total Direct Effect and simulating
intervention, we further subtract background-only activation, preserving
beneficial object-context interactions while mitigating hallucinated scores.
Without retraining or prompt design, our method substantially improves both
worst-group and average accuracy on context-sensitive benchmarks, establishing
a new zero-shot state of the art. Beyond performance, our framework provides a
lightweight representation-level counterfactual approach, offering a practical
causal avenue for debiased and reliable multimodal reasoning.

</details>


### [92] [Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing](https://arxiv.org/abs/2510.26474)
*Xin Guo,Zhiheng Xi,Yiwen Ding,Yitao Zhai,Xiaowei Shi,Xunliang Cai,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CV

TL;DR: 模型在处理简单查询时表现出色，但在复杂查询时遇到困难，导致优化不平衡。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型 (LVLM) 的自提升能力受限于对简单和复杂查询的不平衡优化，复杂查询的性能提升受限。

Method: 提出四种有效策略，从分布重塑和轨迹重采样两个角度，在探索和学习的自我完善过程中实现 head-tail 重新平衡。

Result: 在 Qwen2-VL-7B-Instruct 和 InternVL2.5-4B 模型上的大量实验表明，该方法持续提高视觉推理能力，平均优于原始自我完善 3.86 个点。

Conclusion: 通过解决优化不平衡问题，该方法有效提升了视觉推理能力。

Abstract: Self-improvement has emerged as a mainstream paradigm for advancing the
reasoning capabilities of large vision-language models (LVLMs), where models
explore and learn from successful trajectories iteratively. However, we
identify a critical issue during this process: the model excels at generating
high-quality trajectories for simple queries (i.e., head data) but struggles
with more complex ones (i.e., tail data). This leads to an imbalanced
optimization that drives the model to prioritize simple reasoning skills, while
hindering its ability to tackle more complex reasoning tasks. Over iterations,
this imbalance becomes increasingly pronounced--a dynamic we term the "Matthew
effect"--which ultimately hinders further model improvement and leads to
performance bottlenecks. To counteract this challenge, we introduce four
efficient strategies from two perspectives: distribution-reshaping and
trajectory-resampling, to achieve head-tail re-balancing during the
exploration-and-learning self-improvement process. Extensive experiments on
Qwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks
demonstrate that our methods consistently improve visual reasoning
capabilities, outperforming vanilla self-improvement by 3.86 points on average.

</details>


### [93] [Analysis of the Robustness of an Edge Detector Based on Cellular Automata Optimized by Particle Swarm](https://arxiv.org/abs/2510.26509)
*Vinícius Ferraria,Eurico Ruivo*

Main category: cs.CV

TL;DR: 本文提出了一种自适应边缘检测器，该检测器通过二维细胞自动机描述，并通过元启发式算法与迁移学习技术相结合进行优化。


<details>
  <summary>Details</summary>
Motivation: 现有边缘检测器在检测松散边缘方面存在困难，并且缺乏从特定问题中提取相关信息的能力。

Method: 开发了一个自适应检测器，该检测器通过二维细胞自动机描述，并通过元启发式算法与迁移学习技术相结合进行优化。分析了扩大优化阶段搜索空间的影响，以及检测器在识别一组自然图像和从同一图像集中提取的专用子集的边缘时的适应性鲁棒性。

Result: 扩大优化阶段的搜索空间对于所选图像集无效。无论验证如何，该模型都能够适应输入，并且应用于该模型的迁移学习技术没有显示出显着改进。

Conclusion: 该模型具有适应性，但扩大搜索空间和使用迁移学习技术没有显著提升性能。

Abstract: The edge detection task is essential in image processing aiming to extract
relevant information from an image. One recurring problem in this task is the
weaknesses found in some detectors, such as the difficulty in detecting loose
edges and the lack of context to extract relevant information from specific
problems. To address these weaknesses and adapt the detector to the properties
of an image, an adaptable detector described by two-dimensional cellular
automaton and optimized by meta-heuristic combined with transfer learning
techniques was developed. This study aims to analyze the impact of expanding
the search space of the optimization phase and the robustness of the
adaptability of the detector in identifying edges of a set of natural images
and specialized subsets extracted from the same image set. The results obtained
prove that expanding the search space of the optimization phase was not
effective for the chosen image set. The study also analyzed the adaptability of
the model through a series of experiments and validation techniques and found
that, regardless of the validation, the model was able to adapt to the input
and the transfer learning techniques applied to the model showed no significant
improvements.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [94] [Towards Piece-by-Piece Explanations for Chess Positions with SHAP](https://arxiv.org/abs/2510.25775)
*Francesco Spinnato*

Main category: cs.AI

TL;DR: 本文利用SHAP方法，将国际象棋引擎的评估归因于棋盘上的特定棋子。


<details>
  <summary>Details</summary>
Motivation: 当前的国际象棋引擎提供精确但模糊的评估，通常表示为百分之一的分数，虽然对于决策有效，但这些输出掩盖了单个棋子或模式的潜在贡献。

Method: 通过将棋子视为特征并系统地移除它们，我们计算出可加性的、每个棋子的贡献，从而以局部忠实和人类可解释的方式解释引擎的输出。

Result: 该方法为可视化、人类训练和引擎比较开辟了新的可能性。

Conclusion: 我们发布了随附的代码和数据，以促进未来在可解释的国际象棋AI方面的研究。

Abstract: Contemporary chess engines offer precise yet opaque evaluations, typically
expressed as centipawn scores. While effective for decision-making, these
outputs obscure the underlying contributions of individual pieces or patterns.
In this paper, we explore adapting SHAP (SHapley Additive exPlanations) to the
domain of chess analysis, aiming to attribute a chess engines evaluation to
specific pieces on the board. By treating pieces as features and systematically
ablating them, we compute additive, per-piece contributions that explain the
engines output in a locally faithful and human-interpretable manner. This
method draws inspiration from classical chess pedagogy, where players assess
positions by mentally removing pieces, and grounds it in modern explainable AI
techniques. Our approach opens new possibilities for visualization, human
training, and engine comparison. We release accompanying code and data to
foster future research in interpretable chess AI.

</details>


### [95] [An Agentic Framework for Rapid Deployment of Edge AI Solutions in Industry 5.0](https://arxiv.org/abs/2510.25813)
*Jorge Martinez-Gil,Mario Pichler,Nefeli Bountouni,Sotiris Koussouris,Marielena Márquez Barreiro,Sergio Gusmeroli*

Main category: cs.AI

TL;DR: 提出了一个用于工业5.0的新框架，简化了AI模型在边缘设备上的部署。


<details>
  <summary>Details</summary>
Motivation: 在各种工业环境中，边缘设备上AI模型的部署需要简化，以减少延迟并避免外部数据传输。

Method: 该框架是基于代理的，支持模块化集成，并保持低资源需求，从而实现本地推理和实时处理。

Result: 初步评估表明，在食品工业的实际场景中，部署时间和系统适应性性能得到了改善。

Conclusion: 该框架通过在边缘设备上实现本地推理和实时处理，从而简化了AI模型在工业环境中的部署，并在实际场景中表现出改进的性能。

Abstract: We present a novel framework for Industry 5.0 that simplifies the deployment
of AI models on edge devices in various industrial settings. The design reduces
latency and avoids external data transfer by enabling local inference and
real-time processing. Our implementation is agent-based, which means that
individual agents, whether human, algorithmic, or collaborative, are
responsible for well-defined tasks, enabling flexibility and simplifying
integration. Moreover, our framework supports modular integration and maintains
low resource requirements. Preliminary evaluations concerning the food industry
in real scenarios indicate improved deployment time and system adaptability
performance. The source code is publicly available at
https://github.com/AI-REDGIO-5-0/ci-component.

</details>


### [96] [Symbolically Scaffolded Play: Designing Role-Sensitive Prompts for Generative NPC Dialogue](https://arxiv.org/abs/2510.25820)
*Vanessa Figueiredo,David Elumeze*

Main category: cs.AI

TL;DR: 论文研究大型语言模型（LLM）在互动游戏中的应用，重点关注约束性提示对玩家体验的影响。研究发现，更严格的约束并不一定能提升游戏体验，并提出了“符号化脚手架游戏”框架，旨在平衡游戏连贯性和即兴发挥。


<details>
  <summary>Details</summary>
Motivation: 探讨约束性提示是否能有效改善LLM驱动的非玩家角色（NPC）在互动游戏中与玩家对话的体验。

Method: 通过名为“The Interview”的语音侦探游戏，对比高约束（HCP）和低约束（LCP）提示，并进行用户体验研究（N=10）。此外，利用LLM裁判对重新设计的HCP（混合JSON+RAG支架）进行合成评估。

Result: 用户体验研究未发现显著差异，但合成评估揭示了角色依赖性：面试官NPC的稳定性提高，而嫌疑人NPC的即兴可信度降低。研究结果表明，更严格的约束并不总是能提升游戏体验。

Conclusion: 论文推翻了更严格的约束必然增强游戏体验的假设，并提出了“符号化脚手架游戏”框架，通过模糊的数值边界来稳定连贯性，同时保留即兴发挥，以维持玩家的参与度。

Abstract: Large Language Models (LLMs) promise to transform interactive games by
enabling non-player characters (NPCs) to sustain unscripted dialogue. Yet it
remains unclear whether constrained prompts actually improve player experience.
We investigate this question through The Interview, a voice-based detective
game powered by GPT-4o. A within-subjects usability study ($N=10$) compared
high-constraint (HCP) and low-constraint (LCP) prompts, revealing no reliable
experiential differences beyond sensitivity to technical breakdowns. Guided by
these findings, we redesigned the HCP into a hybrid JSON+RAG scaffold and
conducted a synthetic evaluation with an LLM judge, positioned as an
early-stage complement to usability testing. Results uncovered a novel pattern:
scaffolding effects were role-dependent: the Interviewer (quest-giver NPC)
gained stability, while suspect NPCs lost improvisational believability. These
findings overturn the assumption that tighter constraints inherently enhance
play. Extending fuzzy-symbolic scaffolding, we introduce \textit{Symbolically
Scaffolded Play}, a framework in which symbolic structures are expressed as
fuzzy, numerical boundaries that stabilize coherence where needed while
preserving improvisation where surprise sustains engagement.

</details>


### [97] [Through the Judge's Eyes: Inferred Thinking Traces Improve Reliability of LLM Raters](https://arxiv.org/abs/2510.25860)
*Xingjian Zhang,Tianhong Gao,Suliang Jin,Tianhao Wang,Teng Ye,Eytan Adar,Qiaozhu Mei*

Main category: cs.AI

TL;DR: 该论文提出了一种人-LLM协作框架，用于从仅标签的注释中推断思维轨迹，以提高LLM评估任务的可靠性。


<details>
  <summary>Details</summary>
Motivation: LLM作为评估任务的评分者，在主观任务中的可靠性有限，因为人类的判断涉及超出注释标签的微妙推理。思维轨迹的信息量很大，但收集和管理具有挑战性。

Method: 该框架使用一种简单有效的拒绝抽样方法来大规模重建这些轨迹，并应用于两个互补的任务：(1)微调开放LLM评分者；(2)为专有LLM评分者合成更清晰的注释指南。

Result: 在多个数据集上，该方法显著提高了LLM-人类的一致性。此外，改进的注释指南提高了不同LLM模型之间的一致性。

Conclusion: LLM可以作为人类思维轨迹的实用代理，从而将仅标签的语料库扩展为思维轨迹增强的资源，从而提高LLM评分者的可靠性。

Abstract: Large language models (LLMs) are increasingly used as raters for evaluation
tasks. However, their reliability is often limited for subjective tasks, when
human judgments involve subtle reasoning beyond annotation labels. Thinking
traces, the reasoning behind a judgment, are highly informative but challenging
to collect and curate. We present a human-LLM collaborative framework to infer
thinking traces from label-only annotations. The proposed framework uses a
simple and effective rejection sampling method to reconstruct these traces at
scale. These inferred thinking traces are applied to two complementary tasks:
(1) fine-tuning open LLM raters; and (2) synthesizing clearer annotation
guidelines for proprietary LLM raters. Across multiple datasets, our methods
lead to significantly improved LLM-human agreement. Additionally, the refined
annotation guidelines increase agreement among different LLM models. These
results suggest that LLMs can serve as practical proxies for otherwise
unrevealed human thinking traces, enabling label-only corpora to be extended
into thinking-trace-augmented resources that enhance the reliability of LLM
raters.

</details>


### [98] [The Information-Theoretic Imperative: Compression and the Epistemic Foundations of Intelligence](https://arxiv.org/abs/2510.25883)
*Christian Dittrich,Jennifer Flygare Kinne*

Main category: cs.AI

TL;DR: 本文提出了一个双层框架，解释了为什么压缩过程能够强制发现因果结构，而不是表面的统计模式。


<details>
  <summary>Details</summary>
Motivation: 现有的框架都集中在压缩对于智能的核心地位，但是没有明确说明为什么这个过程能够强制发现因果结构，而不是表面的统计模式。

Method: 本文介绍了信息论指令（ITI）和压缩效率原则（CEP）的双层框架。ITI 指出，任何在不确定环境中持续存在的系统都必须通过预测压缩来最小化认知熵。CEP 规定了有效的压缩如何通过异常累积动力学来机械地选择生成式因果模型。

Result: 压缩效率与分布外泛化相关；异常累积率区分因果模型和相关模型；分层系统在抽象层面上表现出越来越高的效率；生物系统表现出与表征复杂性相关的代谢成本。

Conclusion: ITI 和 CEP 提供了一个统一的解释，解释了生物、人工和多尺度系统的趋同性，解决了智能的认知和功能维度，而无需调用关于意识或主观体验的假设。

Abstract: Existing frameworks converge on the centrality of compression to intelligence
but leave underspecified why this process enforces the discovery of causal
structure rather than superficial statistical patterns. We introduce a
two-level framework to address this gap. The Information-Theoretic Imperative
(ITI) establishes that any system persisting in uncertain environments must
minimize epistemic entropy through predictive compression: this is the
evolutionary "why" linking survival pressure to information-processing demands.
The Compression Efficiency Principle (CEP) specifies how efficient compression
mechanically selects for generative, causal models through
exception-accumulation dynamics, making reality alignment a consequence rather
than a contingent achievement. Together, ITI and CEP define a causal chain:
from survival pressure to prediction necessity, compression requirement,
efficiency optimization, generative structure discovery, and ultimately reality
alignment. Each link follows from physical, information-theoretic, or
evolutionary constraints, implying that intelligence is the mechanically
necessary outcome of persistence in structured environments. This framework
yields empirically testable predictions: compression efficiency, measured as
approach to the rate-distortion frontier, correlates with out-of-distribution
generalization; exception-accumulation rates differentiate causal from
correlational models; hierarchical systems exhibit increasing efficiency across
abstraction layers; and biological systems demonstrate metabolic costs that
track representational complexity. ITI and CEP thereby provide a unified
account of convergence across biological, artificial, and multi-scale systems,
addressing the epistemic and functional dimensions of intelligence without
invoking assumptions about consciousness or subjective experience.

</details>


### [99] [GraphCompliance: Aligning Policy and Context Graphs for LLM-Based Regulatory Compliance](https://arxiv.org/abs/2510.26309)
*Jiseong Chung,Ronny Ko,Wonchul Yoo,Makoto Onizuka,Sungmok Kim,Tae-Wan Kim,Won-Yong Shin*

Main category: cs.AI

TL;DR: 这篇论文提出了 GraphCompliance 框架，用于将非结构化文本中的语义信息与法规的结构化规范元素对齐。


<details>
  <summary>Details</summary>
Motivation: 网络规模的合规性面临实际挑战，每次请求都需要进行监管评估。监管文本是交叉引用的和规范的，而运行时上下文是用非结构化自然语言表达的。为了解决这个问题，我们需要将非结构化文本中的语义信息与法规的结构化规范元素对齐。

Method: 该框架将法规文本表示为策略图，将运行时上下文表示为上下文图，并将它们对齐。策略图编码规范结构和交叉引用，而上下文图将事件形式化为主语-动作-宾语（SAO）和实体-关系三元组。

Result: 在对 300 个源自 GDPR 的真实场景进行的实验中，GraphCompliance 的 micro-F1 比仅使用 LLM 和 RAG 基线高 4.1-7.2 个百分点，并且减少了欠预测和过度预测，从而提高了召回率并降低了假阳性率。

Conclusion: 结构化表示和判断 LLM 对于规范推理是互补的。

Abstract: Compliance at web scale poses practical challenges: each request may require
a regulatory assessment. Regulatory texts (e.g., the General Data Protection
Regulation, GDPR) are cross-referential and normative, while runtime contexts
are expressed in unstructured natural language. This setting motivates us to
align semantic information in unstructured text with the structured, normative
elements of regulations. To this end, we introduce GraphCompliance, a framework
that represents regulatory texts as a Policy Graph and runtime contexts as a
Context Graph, and aligns them. In this formulation, the policy graph encodes
normative structure and cross-references, whereas the context graph formalizes
events as subject-action-object (SAO) and entity-relation triples. This
alignment anchors the reasoning of a judge large language model (LLM) in
structured information and helps reduce the burden of regulatory interpretation
and event parsing, enabling a focus on the core reasoning step. In experiments
on 300 GDPR-derived real-world scenarios spanning five evaluation tasks,
GraphCompliance yields 4.1-7.2 percentage points (pp) higher micro-F1 than
LLM-only and RAG baselines, with fewer under- and over-predictions, resulting
in higher recall and lower false positive rates. Ablation studies indicate
contributions from each graph component, suggesting that structured
representations and a judge LLM are complementary for normative reasoning.

</details>


### [100] [Approximating Human Preferences Using a Multi-Judge Learned System](https://arxiv.org/abs/2510.25884)
*Eitán Sprejer,Fernando Avalos,Augusto Bernardi,Jose Pedro Brito de Azevedo Faustino,Jacob Haimes,Narmeen Fatimah Oozeer*

Main category: cs.AI

TL;DR: 提出了一种基于 persona 的框架，用于对齐 LLM 评判者与人类偏好，通过学习聚合多个 rubric 条件下的评判者的输出来模拟不同的偏好。


<details>
  <summary>Details</summary>
Motivation: 校准 LLM 评判者以符合人类偏好是一个重大挑战，因为它们难以校准，并且经常受到 rubric 敏感性、偏差和不稳定性的影响。克服这一挑战有助于创建可靠的 RLHF 奖励模型和构建有效的路由系统。

Method: 提出一个框架，通过学习聚合多个 rubric 条件下的评判者的输出来建模不同的、基于 persona 的偏好。该框架包含两种不同的聚合器实现：广义加性模型 (GAM) 和多层感知器 (MLP)。

Result: 通过案例研究评估了该方法相对于简单基线的性能和鲁棒性，考察了人类和 LLM 评判者的偏差。

Conclusion: 主要贡献包括一种用于大规模合成偏好标签的基于 persona 的方法，以及 GAM 和 MLP 两种不同的聚合器实现。

Abstract: Aligning LLM-based judges with human preferences is a significant challenge,
as they are difficult to calibrate and often suffer from rubric sensitivity,
bias, and instability. Overcoming this challenge advances key applications,
such as creating reliable reward models for Reinforcement Learning from Human
Feedback (RLHF) and building effective routing systems that select the
best-suited model for a given user query. In this work, we propose a framework
for modeling diverse, persona-based preferences by learning to aggregate
outputs from multiple rubric-conditioned judges. We investigate the performance
of this approach against naive baselines and assess its robustness through case
studies on both human and LLM-judges biases. Our primary contributions include
a persona-based method for synthesizing preference labels at scale and two
distinct implementations of our aggregator: Generalized Additive Model (GAM)
and a Multi-Layer Perceptron (MLP).

</details>


### [101] [LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human Smuggling Networks](https://arxiv.org/abs/2510.26486)
*Dipak Meher,Carlotta Domeniconi,Guadalupe Correa-Cabrera*

Main category: cs.AI

TL;DR: 本文提出了一种名为LINK-KG的框架，用于从法律文件中构建知识图谱，以分析人口走私网络。


<details>
  <summary>Details</summary>
Motivation: 现有的知识图谱构建方法在处理法律文件中的指代消解问题上存在不足，导致图谱碎片化和实体链接不一致。

Method: LINK-KG集成了三阶段的、由LLM指导的指代消解流程，并结合下游的知识图谱提取技术。该方法的核心是一个类型特定的Prompt Cache，用于跟踪和消解文档块中的引用。

Result: 与基线方法相比，LINK-KG平均减少了45.21%的节点重复和32.22%的噪声节点，从而生成更清晰、更连贯的图结构。

Conclusion: LINK-KG为分析复杂犯罪网络奠定了坚实的基础。

Abstract: Human smuggling networks are complex and constantly evolving, making them
difficult to analyze comprehensively. Legal case documents offer rich factual
and procedural insights into these networks but are often long, unstructured,
and filled with ambiguous or shifting references, posing significant challenges
for automated knowledge graph (KG) construction. Existing methods either
overlook coreference resolution or fail to scale beyond short text spans,
leading to fragmented graphs and inconsistent entity linking. We propose
LINK-KG, a modular framework that integrates a three-stage, LLM-guided
coreference resolution pipeline with downstream KG extraction. At the core of
our approach is a type-specific Prompt Cache, which consistently tracks and
resolves references across document chunks, enabling clean and disambiguated
narratives for structured knowledge graph construction from both short and long
legal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes
by 32.22% compared to baseline methods, resulting in cleaner and more coherent
graph structures. These improvements establish LINK-KG as a strong foundation
for analyzing complex criminal networks.

</details>


### [102] [SciTrust 2.0: A Comprehensive Framework for Evaluating Trustworthiness of Large Language Models in Scientific Applications](https://arxiv.org/abs/2510.25908)
*Emily Herron,Junqi Yin,Feiyi Wang*

Main category: cs.AI

TL;DR: SciTrust 2.0是一个评估LLM在科学应用中可信度的框架，包含真 truthfulness、鲁棒性、科学安全和科学伦理四个维度。


<details>
  <summary>Details</summary>
Motivation: 在科学研究中部署LLM有很大的潜力，但也引发了对其可信度的担忧。

Method: 该框架引入了新的真 truthfulness 基准，并通过验证的反思调整流程和专家验证进行开发，以及一个新的科学研究伦理基准，涵盖包括双重用途研究和偏见在内的八个子类别。使用准确率、语义相似度测量和基于LLM的评分等多种评估指标，评估了七个LLM。

Result: 通用工业模型在各个可信度维度上总体优于科学专用模型，GPT-o4-mini 在真 truthfulness 评估和对抗鲁棒性方面表现出卓越的性能。科学专业模型在逻辑和伦理推理能力方面表现出明显的缺陷，并且在安全评估中存在令人担忧的漏洞，尤其是在生物安全和化学武器等高风险领域。

Conclusion: 通过开源该框架，为开发更值得信赖的AI系统和推进科学背景下模型安全和伦理研究奠定了基础。

Abstract: Large language models (LLMs) have demonstrated transformative potential in
scientific research, yet their deployment in high-stakes contexts raises
significant trustworthiness concerns. Here, we introduce SciTrust 2.0, a
comprehensive framework for evaluating LLM trustworthiness in scientific
applications across four dimensions: truthfulness, adversarial robustness,
scientific safety, and scientific ethics. Our framework incorporates novel,
open-ended truthfulness benchmarks developed through a verified
reflection-tuning pipeline and expert validation, alongside a novel ethics
benchmark for scientific research contexts covering eight subcategories
including dual-use research and bias. We evaluated seven prominent LLMs,
including four science-specialized models and three general-purpose industry
models, using multiple evaluation metrics including accuracy, semantic
similarity measures, and LLM-based scoring. General-purpose industry models
overall outperformed science-specialized models across each trustworthiness
dimension, with GPT-o4-mini demonstrating superior performance in truthfulness
assessments and adversarial robustness. Science-specialized models showed
significant deficiencies in logical and ethical reasoning capabilities, along
with concerning vulnerabilities in safety evaluations, particularly in
high-risk domains such as biosecurity and chemical weapons. By open-sourcing
our framework, we provide a foundation for developing more trustworthy AI
systems and advancing research on model safety and ethics in scientific
contexts.

</details>


### [103] [FinOps Agent -- A Use-Case for IT Infrastructure and Cost Optimization](https://arxiv.org/abs/2510.25914)
*Ngoc Phuoc An Vo,Manish Kesarwani,Ruchi Mahindru,Chandrasekhar Narayanaswami*

Main category: cs.AI

TL;DR: 这篇论文提出利用自主的、目标驱动的 AI 代理来实现 FinOps 自动化，以应对 FinOps 从多个云提供商和内部系统接收异构格式的账单数据这一挑战。


<details>
  <summary>Details</summary>
Motivation: FinOps 从业者面临着账单数据格式异构的挑战，这些数据来自多个云提供商和内部系统，最终导致难以综合可操作的见解并做出时间敏感的决策。

Method: 构建了一个 FinOps 代理，用于 IT 基础设施和成本优化的典型用例。构建了一个系统，模拟了从各种来源检索数据到整合和分析数据，再到生成优化建议的真实的端到端行业流程。

Result: 使用多个开源和闭源语言模型评估了代理，结果表明该代理能够像实际的 FinOps 从业者一样理解、计划和执行任务。

Conclusion: 证明了 AI 代理在 FinOps 自动化方面的有效性。

Abstract: FinOps (Finance + Operations) represents an operational framework and
cultural practice which maximizes cloud business value through collaborative
financial accountability across engineering, finance, and business teams.
FinOps practitioners face a fundamental challenge: billing data arrives in
heterogeneous formats, taxonomies, and metrics from multiple cloud providers
and internal systems which eventually lead to synthesizing actionable insights,
and making time-sensitive decisions. To address this challenge, we propose
leveraging autonomous, goal-driven AI agents for FinOps automation. In this
paper, we built a FinOps agent for a typical use-case for IT infrastructure and
cost optimization. We built a system simulating a realistic end-to-end industry
process starting with retrieving data from various sources to consolidating and
analyzing the data to generate recommendations for optimization. We defined a
set of metrics to evaluate our agent using several open-source and close-source
language models and it shows that the agent was able to understand, plan, and
execute tasks as well as an actual FinOps practitioner.

</details>


### [104] [Humains-Junior: A 3.8B Language Model Achieving GPT-4o-Level Factual Accuracy by Directed Exoskeleton Reasoning](https://arxiv.org/abs/2510.25933)
*Nissan Yaron,Dan Bystritsky,Ben-Etzion Yaron*

Main category: cs.AI

TL;DR: 一个3.8B的模型达到了GPT-4o级别的FACTS准确率，成本更低。


<details>
  <summary>Details</summary>
Motivation: 探索更小、更经济高效的语言模型，同时保持与大型模型相当的性能。

Method: 结合了最少的定向“外骨骼推理”支架与行为微调，以提高协议遵守性。

Result: Humans-Junior在FACTS Grounding公共子集上与GPT-4o的性能相当（在±5 pp等效范围内）。云定价显示成本比GPT-4o低约19倍。

Conclusion: 3.8B模型在FACTS准确率上可与GPT-4o媲美，且成本效益显著，尤其是在自托管或边缘部署中。

Abstract: We introduce Humans-Junior, a 3.8B model that matches GPT-4o on the FACTS
Grounding public subset within a $\pm 5$ pp equivalence margin.
  Results. On Q1--Q500 under identical judges, GPT-4o scores 73.5% (95% CI
69.5--77.2) and Humans-Junior 72.7% (95% CI 68.7--76.5); the paired difference
is 0.8 pp (bootstrap 95% CI $-3.1$ to $+4.7$; permutation $p = 0.72$; Cohen's
$d = 0.023$). TOST establishes equivalence at $\pm 5$ pp (not at $\pm 3$ pp).
When purchased as managed APIs, Humans-Junior's base model
(Phi-3.5-mini-instruct) is $\approx 19\times$ less expensive than GPT-4o on
Microsoft AI Foundry pricing; self-hosted or edge deployments can drive
incremental inference cost toward zero. Measured vs estimated pricing sources
are tabulated in Appendix E.
  Method. Our approach combines minimal directed "Exoskeleton Reasoning"
scaffolds with behavioral fine-tuning that teaches protocol compliance
(epistemic discipline) rather than domain answers. Fine-tuning alone adds
little; combined, they synergize (+17.7 pp, $p < 0.001$) and reduce variance
($\approx 25\%$). In prompt-only settings on frontier models (Q1--Q100;
non-comparable), directed reasoning improved GPT-4o by +11.8 pp to 85.3% and
Gemini-2.5-Pro by +5.0 pp to 93.3% (baseline 88.3%, $n = 100$); see Section~5.
  TL;DR. A 3.8B model achieves GPT-4o-level FACTS accuracy (equivalent within
$\pm 5$ pp on Q1--Q500). Cloud pricing shows $\approx 19\times$ lower cost
versus GPT-4o, and self-hosted/edge deployments can approach zero marginal
cost. Pricing sources are listed in Appendix E. Frontier prompt-only gains
(Q1--Q100; non-comparable) and optimized-prompt exploratory results under
earlier judges are summarized in Appendix F.
  Keywords: Small Language Models, Factual Grounding, Directed Reasoning,
Fine-Tuning, Model Alignment, Cost-Efficient AI

</details>


### [105] [Estimating cognitive biases with attention-aware inverse planning](https://arxiv.org/abs/2510.25951)
*Sounak Banerjee,Daphne Cornelisse,Deepak Gopinath,Emily Sumner,Jonathan DeCastro,Guy Rosman,Eugene Vinitsky,Mark K. Ho*

Main category: cs.AI

TL;DR: 本文研究了如何从人们的行为中估计他们的注意力偏差，特别是通过结合深度强化学习和计算认知建模来解决注意力感知逆向规划问题。


<details>
  <summary>Details</summary>
Motivation: 人们的有目标行为受到认知偏差的影响，与人交互的自主系统应该意识到这一点。例如，人们对环境中物体的注意力会产生偏差，从而系统地影响他们执行日常任务（如开车上班）的方式。

Method: 本文构建了注意力感知逆向规划问题，并提出了一种结合深度强化学习与计算认知建模的方法。

Result: 本文展示了注意力感知逆向规划与标准逆向强化学习的不同，以及如何从行为中推断认知偏差。在Waymo开放数据集中选择的真实驾驶场景中，该方法被用于推断强化学习智能体的注意力策略。

Conclusion: 本文证明了使用注意力感知逆向规划估计认知偏差的可扩展性。

Abstract: People's goal-directed behaviors are influenced by their cognitive biases,
and autonomous systems that interact with people should be aware of this. For
example, people's attention to objects in their environment will be biased in a
way that systematically affects how they perform everyday tasks such as driving
to work. Here, building on recent work in computational cognitive science, we
formally articulate the attention-aware inverse planning problem, in which the
goal is to estimate a person's attentional biases from their actions. We
demonstrate how attention-aware inverse planning systematically differs from
standard inverse reinforcement learning and how cognitive biases can be
inferred from behavior. Finally, we present an approach to attention-aware
inverse planning that combines deep reinforcement learning with computational
cognitive modeling. We use this approach to infer the attentional strategies of
RL agents in real-life driving scenarios selected from the Waymo Open Dataset,
demonstrating the scalability of estimating cognitive biases with
attention-aware inverse planning.

</details>


### [106] [From Queries to Insights: Agentic LLM Pipelines for Spatio-Temporal Text-to-SQL](https://arxiv.org/abs/2510.25997)
*Manu Redd,Tao Zhe,Dongjie Wang*

Main category: cs.AI

TL;DR: 本文介绍了一种agentic pipeline，通过Mistral-based ReAct agent对text-to-SQL baseline进行扩展，以解决现实的时空查询问题。


<details>
  <summary>Details</summary>
Motivation: 现有的NL-to-SQL系统在处理实际的时空查询时存在困难，因为需要将模糊的用户措辞与特定的schema类别对齐，处理时间推理并选择合适的输出。

Method: 该方法通过schema检查、SQL生成、执行和可视化工具，利用agent对查询进行规划、分解和调整。

Result: 在纽约和东京check-in数据集上，该agent的准确率显著高于naive baseline（91.4% vs. 28.6%），并通过地图、图表和结构化的自然语言摘要提高了可用性。

Conclusion: Agentic orchestration，而不是更强大的SQL生成器，是交互式地理空间助手的一个有希望的基础。

Abstract: Natural-language-to-SQL (NL-to-SQL) systems hold promise for democratizing
access to structured data, allowing users to query databases without learning
SQL. Yet existing systems struggle with realistic spatio-temporal queries,
where success requires aligning vague user phrasing with schema-specific
categories, handling temporal reasoning, and choosing appropriate outputs. We
present an agentic pipeline that extends a naive text-to-SQL baseline
(llama-3-sqlcoder-8b) with orchestration by a Mistral-based ReAct agent. The
agent can plan, decompose, and adapt queries through schema inspection, SQL
generation, execution, and visualization tools. We evaluate on 35
natural-language queries over the NYC and Tokyo check-in dataset, covering
spatial, temporal, and multi-dataset reasoning. The agent achieves
substantially higher accuracy than the naive baseline 91.4% vs. 28.6% and
enhances usability through maps, plots, and structured natural-language
summaries. Crucially, our design enables more natural human-database
interaction, supporting users who lack SQL expertise, detailed schema
knowledge, or prompting skill. We conclude that agentic orchestration, rather
than stronger SQL generators alone, is a promising foundation for interactive
geospatial assistants.

</details>


### [107] [AutoSurvey2: Empowering Researchers with Next Level Automated Literature Surveys](https://arxiv.org/abs/2510.26012)
*Siyi Wu,Chiaxin Liang,Ziqian Bi,Leyi Zhao,Tianyang Wang,Junhao Song,Yichao Zhang,Keyu Chen,Xinyuan Song*

Main category: cs.AI

TL;DR: autosurvey2: 自动化生成survey论文的pipeline，通过检索增强合成和结构化评估，确保主题完整性和事实准确性。


<details>
  <summary>Details</summary>
Motivation: 研究文献，特别是大型语言模型（LLM）领域的研究文献快速增长，使得生成全面的最新综述论文变得越来越困难。

Method: autosurvey2：一个多阶段pipeline，通过检索增强合成和结构化评估来自动化生成survey。该系统集成了并行部分生成、迭代改进和实时检索最新出版物等模块。

Result: autosurvey2在结构连贯性和主题相关性方面始终优于现有的基于检索和自动化的基线，同时保持了强大的引用保真度。

Conclusion: autosurvey2通过将检索、推理和自动评估整合到一个统一的框架中，为生成长篇学术综述提供了一个可扩展且可重复的解决方案，并为未来自动化学术写作的研究贡献了坚实的基础。

Abstract: The rapid growth of research literature, particularly in large language
models (LLMs), has made producing comprehensive and current survey papers
increasingly difficult. This paper introduces autosurvey2, a multi-stage
pipeline that automates survey generation through retrieval-augmented synthesis
and structured evaluation. The system integrates parallel section generation,
iterative refinement, and real-time retrieval of recent publications to ensure
both topical completeness and factual accuracy. Quality is assessed using a
multi-LLM evaluation framework that measures coverage, structure, and relevance
in alignment with expert review standards. Experimental results demonstrate
that autosurvey2 consistently outperforms existing retrieval-based and
automated baselines, achieving higher scores in structural coherence and
topical relevance while maintaining strong citation fidelity. By combining
retrieval, reasoning, and automated evaluation into a unified framework,
autosurvey2 provides a scalable and reproducible solution for generating
long-form academic surveys and contributes a solid foundation for future
research on automated scholarly writing. All code and resources are available
at https://github.com/annihi1ation/auto_research.

</details>


### [108] [Large Language Model-assisted Autonomous Vehicle Recovery from Immobilization](https://arxiv.org/abs/2510.26023)
*Zhipeng Bao,Qianwen Li*

Main category: cs.AI

TL;DR: StuckSolver是一个新的LLM驱动的恢复框架，使AV能够通过自我推理和/或乘客引导的决策来解决固定场景。


<details>
  <summary>Details</summary>
Motivation: 目前的恢复解决方案，如远程干预（成本高，效率低）和人工接管（不包括非驾驶员，限制了AV的可访问性），是不够的。

Method: StuckSolver被设计成一个插件式附加模块，它在AV现有的感知-规划-控制堆栈之上运行，不需要修改其内部架构。相反，它与标准传感器数据流连接，以检测固定状态，解释环境背景，并生成可由AV本地规划器执行的高级恢复命令。

Result: StuckSolver通过自主自我推理实现了接近最先进的性能，并且在纳入乘客指导后表现出进一步的改进。

Conclusion: StuckSolver是一个有效的AV恢复框架，可以通过自我推理和乘客引导来解决固定场景。

Abstract: Despite significant advancements in recent decades, autonomous vehicles (AVs)
continue to face challenges in navigating certain traffic scenarios where human
drivers excel. In such situations, AVs often become immobilized, disrupting
overall traffic flow. Current recovery solutions, such as remote intervention
(which is costly and inefficient) and manual takeover (which excludes
non-drivers and limits AV accessibility), are inadequate. This paper introduces
StuckSolver, a novel Large Language Model (LLM) driven recovery framework that
enables AVs to resolve immobilization scenarios through self-reasoning and/or
passenger-guided decision-making. StuckSolver is designed as a plug-in add-on
module that operates on top of the AV's existing perception-planning-control
stack, requiring no modification to its internal architecture. Instead, it
interfaces with standard sensor data streams to detect immobilization states,
interpret environmental context, and generate high-level recovery commands that
can be executed by the AV's native planner. We evaluate StuckSolver on the
Bench2Drive benchmark and in custom-designed uncertainty scenarios. Results
show that StuckSolver achieves near-state-of-the-art performance through
autonomous self-reasoning alone and exhibits further improvements when
passenger guidance is incorporated.

</details>


### [109] [Can AI be Accountable?](https://arxiv.org/abs/2510.26057)
*Andrew L. Kun*

Main category: cs.AI

TL;DR: 这篇论文探讨了人工智能的可问责性问题，强调了人工智能为消费者、选民和决策者服务的必要性，并指出当前人工智能在可问责性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能的功能强大且发展迅速，为了服务于消费者、选民和决策者，人工智能的可问责性至关重要。

Method: 论文将可问责性的一般定义与人工智能联系起来，并通过举例说明人工智能可问责和不可问责的情况。

Result: 论文探讨了提高人工智能可问责性的方法，旨在创造一个所有人都能对人工智能进行问责的世界。

Conclusion: 论文强调了人工智能可问责性的重要性，并探讨了改进的方向。

Abstract: The AI we use is powerful, and its power is increasing rapidly. If this
powerful AI is to serve the needs of consumers, voters, and decision makers,
then it is imperative that the AI is accountable. In general, an agent is
accountable to a forum if the forum can request information from the agent
about its actions, if the forum and the agent can discuss this information, and
if the forum can sanction the agent. Unfortunately, in too many cases today's
AI is not accountable -- we cannot question it, enter into a discussion with
it, let alone sanction it. In this chapter we relate the general definition of
accountability to AI, we illustrate what it means for AI to be accountable and
unaccountable, and we explore approaches that can improve our chances of living
in a world where all AI is accountable to those who are affected by it.

</details>


### [110] [Lean4Physics: Comprehensive Reasoning Framework for College-level Physics in Lean4](https://arxiv.org/abs/2510.26094)
*Yuxin Li,Minghao Liu,Ruida Wang,Wenzhao Ji,Zhitao He,Rui Pan,Junming Huang,Tong Zhang,Yi R. Fung*

Main category: cs.AI

TL;DR: 本文介绍了Lean4PHYS，一个用于Lean4中大学水平物理问题的综合推理框架。


<details>
  <summary>Details</summary>
Motivation: 目前缺少在Lean4中进行正式物理推理的基准和资源。

Method: 创建了一个包含200个手工制作的物理题目的基准测试LeanPhysBench，并构建了一个包含基本单位系统和定理的社区驱动知识库PhysLib。

Result: 使用主流的Lean4专家数学证明器和先进的闭源模型进行了基线测试，DeepSeek-Prover-V2-7B的最佳性能仅为16%，Claude-Sonnet-4达到35%。PhysLib可以平均提高模型性能11.75%。

Conclusion: LeanPhysBench具有挑战性，PhysLib是有效的。这是第一个在Lean4中提供物理基准的研究。

Abstract: We present **Lean4PHYS**, a comprehensive reasoning framework for
college-level physics problems in Lean4. **Lean4PHYS** includes
*LeanPhysBench*, a college-level benchmark for formal physics reasoning in
Lean4, which contains 200 hand-crafted and peer-reviewed statements derived
from university textbooks and physics competition problems. To establish a
solid foundation for formal reasoning in physics, we also introduce *PhysLib*,
a community-driven repository containing fundamental unit systems and theorems
essential for formal physics reasoning. Based on the benchmark and Lean4
repository we composed in **Lean4PHYS**, we report baseline results using major
expert Math Lean4 provers and state-of-the-art closed-source models, with the
best performance of DeepSeek-Prover-V2-7B achieving only 16% and
Claude-Sonnet-4 achieving 35%. We also conduct a detailed analysis showing that
our *PhysLib* can achieve an average improvement of 11.75% in model
performance. This demonstrates the challenging nature of our *LeanPhysBench*
and the effectiveness of *PhysLib*. To the best of our knowledge, this is the
first study to provide a physics benchmark in Lean4.

</details>


### [111] [GUI Knowledge Bench: Revealing the Knowledge Gap Behind VLM Failures in GUI Tasks](https://arxiv.org/abs/2510.26098)
*Chenrui Shi,Zedong Yu,Zhi Gao,Ruining Feng,Enqi Liu,Yuwei Wu,Yunde Jia,Liuyu Xiang,Zhaofeng He,Qing Li*

Main category: cs.AI

TL;DR: 现有的视觉语言模型(VLMs)在图形用户界面(GUI)任务自动化方面仍落后于人类，可能是因为缺乏核心的GUI知识。


<details>
  <summary>Details</summary>
Motivation: 通过分析GUI任务执行中的常见失败模式，将GUI知识提炼为三个维度：界面感知、交互预测和指令理解。

Method: 引入GUI知识基准，这是一个包含跨六个平台和292个应用程序的多项选择题和是/否问题的基准。

Result: 评估表明，当前的VLM能够识别小部件功能，但在感知系统状态、预测操作和验证任务完成情况方面存在困难。在真实GUI任务上的实验进一步验证了GUI知识与任务成功之间的密切联系。

Conclusion: 该研究提供了一个评估GUI知识的结构化框架，为选择具有更大潜力的VLM提供了支持，并为构建更强大的GUI代理提供了见解。

Abstract: Large vision language models (VLMs) have advanced graphical user interface
(GUI) task automation but still lag behind humans. We hypothesize this gap
stems from missing core GUI knowledge, which existing training schemes (such as
supervised fine tuning and reinforcement learning) alone cannot fully address.
By analyzing common failure patterns in GUI task execution, we distill GUI
knowledge into three dimensions: (1) interface perception, knowledge about
recognizing widgets and system states; (2) interaction prediction, knowledge
about reasoning action state transitions; and (3) instruction understanding,
knowledge about planning, verifying, and assessing task completion progress. We
further introduce GUI Knowledge Bench, a benchmark with multiple choice and
yes/no questions across six platforms (Web, Android, MacOS, Windows, Linux,
IOS) and 292 applications. Our evaluation shows that current VLMs identify
widget functions but struggle with perceiving system states, predicting
actions, and verifying task completion. Experiments on real world GUI tasks
further validate the close link between GUI knowledge and task success. By
providing a structured framework for assessing GUI knowledge, our work supports
the selection of VLMs with greater potential prior to downstream training and
provides insights for building more capable GUI agents.

</details>


### [112] [Beyond Benchmarks: The Economics of AI Inference](https://arxiv.org/abs/2510.26136)
*Boqin Zhuang,Jiacheng Qiao,Mingqian Liu,Mingxing Yu,Ping Hong,Rui Li,Xiaoxia Song,Xiangjun Xu,Xu Chen,Yaoyao Ma,Yujie Gao*

Main category: cs.AI

TL;DR: 本文提出了一个量化的“推理经济学”框架，将LLM推理过程视为计算驱动的智能生产活动。


<details>
  <summary>Details</summary>
Motivation: 分析大型语言模型（LLM）的推理成本已成为决定其商业可行性和广泛应用的关键因素。

Method: 基于来自WiNEval-3.0的经验数据，构建了第一个“LLM推理生产前沿”。

Result: 揭示了三个原则：边际成本递减、规模报酬递减和最佳成本效益区。

Conclusion: 本文不仅为模型部署决策提供了经济基础，也为未来基于市场的AI推理资源定价和优化奠定了经验基础。

Abstract: The inference cost of Large Language Models (LLMs) has become a critical
factor in determining their commercial viability and widespread adoption. This
paper introduces a quantitative ``economics of inference'' framework, treating
the LLM inference process as a compute-driven intelligent production activity.
We analyze its marginal cost, economies of scale, and quality of output under
various performance configurations. Based on empirical data from WiNEval-3.0,
we construct the first ``LLM Inference Production Frontier,'' revealing three
principles: diminishing marginal cost, diminishing returns to scale, and an
optimal cost-effectiveness zone. This paper not only provides an economic basis
for model deployment decisions but also lays an empirical foundation for the
future market-based pricing and optimization of AI inference resources.

</details>


### [113] [Reasoning Curriculum: Bootstrapping Broad LLM Reasoning from Math](https://arxiv.org/abs/2510.26143)
*Bo Pang,Deqian Kong,Silvio Savarese,Caiming Xiong,Yingbo Zhou*

Main category: cs.AI

TL;DR: 提出了一种简单的两阶段课程，首先在预训练对齐的领域（如数学）中引发推理技能，然后通过联合强化学习在其他领域调整和完善这些技能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型中的强化学习 (RL) 可以引发强大的推理能力，但大多数开放性工作都集中在数学和代码上。

Method: 首先进行简短的冷启动，然后使用可验证的奖励进行纯数学 RL，以发展推理技能。第二阶段在混合领域数据上运行联合 RL，以转移和巩固这些技能。

Result: 在 Qwen3-4B 和 Llama-3.1-8B 上，推理课程在多领域套件中产生了持续的收益。

Conclusion: 推理课程提供了一种紧凑、易于采用的通用推理方法。

Abstract: Reinforcement learning (RL) can elicit strong reasoning in large language
models (LLMs), yet most open efforts focus on math and code. We propose
Reasoning Curriculum, a simple two-stage curriculum that first elicits
reasoning skills in pretraining-aligned domains such as math, then adapts and
refines these skills across other domains via joint RL. Stage 1 performs a
brief cold start and then math-only RL with verifiable rewards to develop
reasoning skills. Stage 2 runs joint RL on mixed-domain data to transfer and
consolidate these skills. The curriculum is minimal and backbone-agnostic,
requiring no specialized reward models beyond standard verifiability checks.
Evaluated on Qwen3-4B and Llama-3.1-8B over a multi-domain suite, reasoning
curriculum yields consistent gains. Ablations and a cognitive-skill analysis
indicate that both stages are necessary and that math-first elicitation
increases cognitive behaviors important for solving complex problems. Reasoning
Curriculum provides a compact, easy-to-adopt recipe for general reasoning.

</details>


### [114] [The FM Agent](https://arxiv.org/abs/2510.26144)
*Annan Li,Chufan Wu,Zengle Ge,Yee Hin Chong,Zhinan Hou,Lizhe Cao,Cheng Ju,Jianmin Wu,Huaiming Li,Haobo Zhang,Shenghao Feng,Mo Zhao,Fengzhi Qiu,Rui Yang,Mengmeng Zhang,Wenyi Zhu,Yingying Sun,Quan Sun,Shunhao Yan,Danyu Liu,Dawei Yin,Dou Shen*

Main category: cs.AI

TL;DR: FM Agent：一个新颖的通用多智能体框架，它利用基于LLM的推理和大规模进化搜索的协同组合来解决复杂的实际挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）正在催化用于科学和工程发现的自主AI研究代理的开发。

Method: FM Agent的核心集成了几个关键创新：1）包含专家指导的冷启动初始化阶段，2）用于迭代优化的新颖进化采样策略，3）结合了正确性、有效性和LLM监督反馈的特定领域评估器，以及4）构建在Ray上的分布式异步执行基础设施。

Result: FM Agent在各个领域都达到了最先进的水平，无需人工解释或调整：在ALE-Bench上达到1976.3（+5.2%），在MLE-Bench上达到43.56%（+4.0pp），在KernelBench上实现了高达20倍的加速，并在几个经典的数学问题上建立了新的最先进水平（SOTA）。

Conclusion: FM Agent在大型企业研发工作流程和基础科学研究中显示出相当大的前景，它可以加速创新，自动化复杂的发现过程，并提供具有更广泛社会影响的重大工程和科学进步。

Abstract: Large language models (LLMs) are catalyzing the development of autonomous AI
research agents for scientific and engineering discovery. We present FM Agent,
a novel and general-purpose multi-agent framework that leverages a synergistic
combination of LLM-based reasoning and large-scale evolutionary search to
address complex real-world challenges. The core of FM Agent integrates several
key innovations: 1) a cold-start initialization phase incorporating expert
guidance, 2) a novel evolutionary sampling strategy for iterative optimization,
3) domain-specific evaluators that combine correctness, effectiveness, and
LLM-supervised feedback, and 4) a distributed, asynchronous execution
infrastructure built on Ray. Demonstrating broad applicability, our system has
been evaluated across diverse domains, including operations research, machine
learning, GPU kernel optimization, and classical mathematical problems. FM
Agent reaches state-of-the-art results autonomously, without human
interpretation or tuning -- 1976.3 on ALE-Bench (+5.2\%), 43.56\% on MLE-Bench
(+4.0pp), up to 20x speedups on KernelBench, and establishes new
state-of-the-art(SOTA) results on several classical mathematical problems.
Beyond academic benchmarks, FM Agent shows considerable promise for both
large-scale enterprise R\&D workflows and fundamental scientific research,
where it can accelerate innovation, automate complex discovery processes, and
deliver substantial engineering and scientific advances with broader societal
impact.

</details>


### [115] [One Model to Critique Them All: Rewarding Agentic Tool-Use via Efficient Reasoning](https://arxiv.org/abs/2510.26167)
*Renhao Li,Jianhong Tu,Yang Su,Hamid Alinejad-Rokny,Derek F. Wong,Junyang Lin,Min Yang*

Main category: cs.AI

TL;DR: 提出了ToolRM，一个为通用工具使用场景定制的轻量级生成奖励模型家族。


<details>
  <summary>Details</summary>
Motivation: 缺乏专门为函数调用任务设计的奖励模型，限制了在工具学习领域构建更强大的Agentic AI的进展。

Method: 提出了一个新的pipeline，使用基于规则的评分和多维采样构建pairwise偏好数据，生成ToolPref-Pairwise-30K数据集。还引入了TRBench$_{BFCL}$，一个基于agentic评估套件BFCL的基准。

Result: 在Qwen3-4B/8B系列模型上进行训练，在pairwise奖励判断中，准确率提高了14.28%，大大超过了Claude 4和OpenAI o3等前沿模型。

Conclusion: ToolRM可以推广到更广泛的评论任务，包括Best-of-N抽样和自我纠正。在ACEBench上的实验表明了其有效性和效率，实现了推理时扩展，并将输出token的使用量减少了66%以上。

Abstract: Reward models (RMs) play a critical role in aligning large language models
(LLMs) with human preferences. Yet in the domain of tool learning, the lack of
RMs specifically designed for function-calling tasks has limited progress
toward more capable agentic AI. We introduce ToolRM, a family of lightweight
generative RMs tailored for general tool-use scenarios. To build these models,
we propose a novel pipeline that constructs pairwise preference data using
rule-based scoring and multidimensional sampling. This yields
ToolPref-Pairwise-30K, a diverse, balanced, and challenging dataset of critique
tasks that supports reinforcement learning with verifiable feedback. To
evaluate tool-use RMs, we also introduce TRBench$_{BFCL}$, a benchmark built on
the agentic evaluation suite BFCL. Trained on our constructed data, models from
the Qwen3-4B/8B series achieve up to 14.28% higher accuracy, substantially
outperforming frontier models such as Claude 4 and OpenAI o3 in pairwise reward
judgments. Beyond training objectives, ToolRM generalizes to broader critique
tasks, including Best-of-N sampling and self-correction. Experiments on
ACEBench highlight its effectiveness and efficiency, enabling inference-time
scaling and reducing output token usage by over 66%. We release data and model
checkpoints to facilitate future research.

</details>


### [116] [Questionnaire meets LLM: A Benchmark and Empirical Study of Structural Skills for Understanding Questions and Responses](https://arxiv.org/abs/2510.26238)
*Duc-Hai Nguyen,Vijayakumar Nanjappan,Barry O'Sullivan,Hoang D. Nguyen*

Main category: cs.AI

TL;DR: 论文介绍了一个名为QASU的基准，用于评估大型语言模型(llm)在处理问卷数据方面的能力。通过系统地隔离格式和提示的影响，该基准为推进基于llm的问卷分析的研究和实践奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 当前检索和调查分析工具通常是为人类设计的，限制了与llm和人工智能自动化的数据集成。缺乏关于如何最好地表示llm使用的问卷的循证指导。

Method: 引入QASU基准，该基准探测六种结构技能，包括答案查找、应答者计数和跨六种序列化格式和多种提示策略的多跳推理。

Result: 选择有效的格式和提示组合可以将准确率提高高达8.8个百分点，而通过自我增强提示添加轻量级结构提示可以平均提高3-4个百分点。

Conclusion: 该开放源代码基准为推进基于llm的问卷分析的研究和实践提供了一个简单而通用的基础。

Abstract: Millions of people take surveys every day, from market polls and academic
studies to medical questionnaires and customer feedback forms. These datasets
capture valuable insights, but their scale and structure present a unique
challenge for large language models (LLMs), which otherwise excel at few-shot
reasoning over open-ended text. Yet, their ability to process questionnaire
data or lists of questions crossed with hundreds of respondent rows remains
underexplored. Current retrieval and survey analysis tools (e.g., Qualtrics,
SPSS, REDCap) are typically designed for humans in the workflow, limiting such
data integration with LLM and AI-empowered automation. This gap leaves
scientists, surveyors, and everyday users without evidence-based guidance on
how to best represent questionnaires for LLM consumption. We address this by
introducing QASU (Questionnaire Analysis and Structural Understanding), a
benchmark that probes six structural skills, including answer lookup,
respondent count, and multi-hop inference, across six serialization formats and
multiple prompt strategies. Experiments on contemporary LLMs show that choosing
an effective format and prompt combination can improve accuracy by up to 8.8%
points compared to suboptimal formats. For specific tasks, carefully adding a
lightweight structural hint through self-augmented prompting can yield further
improvements of 3-4% points on average. By systematically isolating format and
prompting effects, our open source benchmark offers a simple yet versatile
foundation for advancing both research and real-world practice in LLM-based
questionnaire analysis.

</details>


### [117] [Retrieval Augmented Generation-Enhanced Distributed LLM Agents for Generalizable Traffic Signal Control with Emergency Vehicles](https://arxiv.org/abs/2510.26242)
*Xinhang Li,Qing Guo,Junyu Chen,Zheng Guo,Shengzhe Xu,Lei Li,Lin Zhang*

Main category: cs.AI

TL;DR: 提出了一种名为REG-TSC的检索增强生成（RAG）增强的分布式LLM智能体，用于解决交通信号控制（TSC）问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在紧急情况下容易产生幻觉，且难以泛化到不同类型的交叉口。

Method: 1. 提出了一个紧急感知推理框架，使用基于审查者的紧急RAG（RERAG）从历史案例中提取知识。2. 设计了一种类型无关的交通表示，并提出了一个奖励引导的强化改进（R3）方法，用于异构交叉口。

Result: 在三个真实世界的道路网络上的大量实验表明，REG-TSC将通行时间减少了42.00%，排队长度减少了62.31%，急救车辆等待时间减少了83.16%，优于其他最先进的方法。

Conclusion: REG-TSC能有效减少通行时间、排队长度和急救车辆等待时间，并在异构交叉口表现出色。

Abstract: With increasing urban traffic complexity, Traffic Signal Control (TSC) is
essential for optimizing traffic flow and improving road safety. Large Language
Models (LLMs) emerge as promising approaches for TSC. However, they are prone
to hallucinations in emergencies, leading to unreliable decisions that may
cause substantial delays for emergency vehicles. Moreover, diverse intersection
types present substantial challenges for traffic state encoding and
cross-intersection training, limiting generalization across heterogeneous
intersections. Therefore, this paper proposes Retrieval Augmented Generation
(RAG)-enhanced distributed LLM agents with Emergency response for Generalizable
TSC (REG-TSC). Firstly, this paper presents an emergency-aware reasoning
framework, which dynamically adjusts reasoning depth based on the emergency
scenario and is equipped with a novel Reviewer-based Emergency RAG (RERAG) to
distill specific knowledge and guidance from historical cases, enhancing the
reliability and rationality of agents' emergency decisions. Secondly, this
paper designs a type-agnostic traffic representation and proposes a
Reward-guided Reinforced Refinement (R3) for heterogeneous intersections. R3
adaptively samples training experience from diverse intersections with
environment feedback-based priority and fine-tunes LLM agents with a designed
reward-weighted likelihood loss, guiding REG-TSC toward high-reward policies
across heterogeneous intersections. On three real-world road networks with 17
to 177 heterogeneous intersections, extensive experiments show that REG-TSC
reduces travel time by 42.00%, queue length by 62.31%, and emergency vehicle
waiting time by 83.16%, outperforming other state-of-the-art methods.

</details>


### [118] [Graph-Enhanced Policy Optimization in LLM Agent Training](https://arxiv.org/abs/2510.26270)
*Jiazhen Yuan,Wei Zhao,Zhengbiao Bai*

Main category: cs.AI

TL;DR: 图增强策略优化（GEPO）通过动态构建状态转移图并利用图论中心性来解决多轮交互式LLM智能体训练中的结构盲点问题，从而提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于群体的强化学习方法在训练多轮交互式LLM智能体时，无法利用环境的潜在连接性，导致效率低下、信用分配不准确和短视计划等问题。

Method: GEPO动态构建状态转移图，并利用图论中心性提供三种协同学习信号：结构化内在奖励以引导探索高影响状态、图增强优势函数以实现拓扑感知信用分配，以及适应每个状态战略价值的动态折扣因子。

Result: 在ALFWorld、WebShop和专有Workbench基准测试中，GEPO表现出强大的性能，相对于竞争基线，分别实现了+4.1%、+5.3%和+10.9%的绝对成功率提升。

Conclusion: 显式建模环境结构是推进LLM智能体训练的稳健、通用策略。

Abstract: Group based reinforcement learning (RL) has shown impressive results on
complex reasoning and mathematical tasks. Yet, when applied to train
multi-turn, interactive LLM agents, these methods often suffer from structural
blindness-the inability to exploit the underlying connectivity of the
environment. This manifests in three critical challenges: (1) inefficient,
unguided exploration, (2) imprecise credit assignment due to overlooking
pivotal states, and (3) myopic planning caused by static reward discounting. We
address these issues with Graph-Enhanced Policy Optimization (GEPO), which
dynamically constructs a state-transition graph from agent experience and
employs graph-theoretic centrality to provide three synergistic learning
signals: (1)structured intrinsic rewards that guide exploration toward
high-impact states, (2) a graph-enhanced advantage function for topology-aware
credit assignment, and (3) a dynamic discount factor adapted to each state's
strategic value. On the ALFWorld, WebShop, and a proprietary Workbench
benchmarks, GEPO demonstrates strong performance, achieving absolute success
rate gains of +4.1%, +5.3%, and +10.9% over competitive baselines. These
results highlight that explicitly modeling environmental structure is a robust,
generalizable strategy for advancing LLM agent training.

</details>


### [119] [Discovering State Equivalences in UCT Search Trees By Action Pruning](https://arxiv.org/abs/2510.26346)
*Robin Schmöcker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: 提出了一种名为IPA-UCT的技术，通过放宽状态抽象条件，在牺牲少量精度的情况下，找到更多的抽象，从而提高MCTS的效率。


<details>
  <summary>Details</summary>
Motivation: 在噪声或大动作空间环境中，很难找到状态抽象。

Method: 提出了一种名为IPA-UCT的技术，它使用与OGA-UCT不同的抽象框架（IPA）。

Result: IPA-UCT在各种测试领域和迭代预算中优于OGA-UCT。

Conclusion: IPA和ASAP是更一般的框架p-ASAP的特例，而p-ASAP本身又是ASASAP框架的特例。

Abstract: One approach to enhance Monte Carlo Tree Search (MCTS) is to improve its
sample efficiency by grouping/abstracting states or state-action pairs and
sharing statistics within a group. Though state-action pair abstractions are
mostly easy to find in algorithms such as On the Go Abstractions in Upper
Confidence bounds applied to Trees (OGA-UCT), nearly no state abstractions are
found in either noisy or large action space settings due to constraining
conditions. We provide theoretical and empirical evidence for this claim, and
we slightly alleviate this state abstraction problem by proposing a weaker
state abstraction condition that trades a minor loss in accuracy for finding
many more abstractions. We name this technique Ideal Pruning Abstractions in
UCT (IPA-UCT), which outperforms OGA-UCT (and any of its derivatives) across a
large range of test domains and iteration budgets as experimentally validated.
IPA-UCT uses a different abstraction framework from Abstraction of State-Action
Pairs (ASAP) which is the one used by OGA-UCT, which we name IPA. Furthermore,
we show that both IPA and ASAP are special cases of a more general framework
that we call p-ASAP which itself is a special case of the ASASAP framework.

</details>


### [120] [BOTS: A Unified Framework for Bayesian Online Task Selection in LLM Reinforcement Finetuning](https://arxiv.org/abs/2510.26374)
*Qianli Shen,Daoyuan Chen,Yilun Huang,Zhenqing Ling,Yaliang Li,Bolin Ding,Jingren Zhou*

Main category: cs.AI

TL;DR: 提出了一种名为BOTS的统一框架，用于在LLM强化微调中进行贝叶斯在线任务选择。


<details>
  <summary>Details</summary>
Motivation: 现有任务选择方法通常具有高推出成本、适应性差或证据不完整的问题，并且均匀任务抽样效率低下，浪费了在简单或无法解决的任务上的计算资源。

Method: BOTS基于贝叶斯推理，自适应地维护任务难度的后验估计，共同结合来自所选任务的直接评估的显式证据和从这些评估中推断出的未选择任务的隐式证据，并通过Thompson抽样确保探索和利用之间的平衡。

Result: 在不同的领域和LLM规模上，BOTS始终优于基线和消融，提高了数据效率和性能。

Conclusion: BOTS为RFT中的动态任务选择提供了一种实用且可扩展的解决方案。

Abstract: Reinforcement finetuning (RFT) is a key technique for aligning Large Language
Models (LLMs) with human preferences and enhancing reasoning, yet its
effectiveness is highly sensitive to which tasks are explored during training.
Uniform task sampling is inefficient, wasting computation on tasks that are
either trivial or unsolvable, while existing task selection methods often
suffer from high rollout costs, poor adaptivity, or incomplete evidence. We
introduce \textbf{BOTS}, a unified framework for \textbf{B}ayesian
\textbf{O}nline \textbf{T}ask \textbf{S}election in LLM reinforcement
finetuning. Grounded in Bayesian inference, BOTS adaptively maintains posterior
estimates of task difficulty as the model evolves. It jointly incorporates
\emph{explicit evidence} from direct evaluations of selected tasks and
\emph{implicit evidence} inferred from these evaluations for unselected tasks,
with Thompson sampling ensuring a principled balance between exploration and
exploitation. To make implicit evidence practical, we instantiate it with an
ultra-light interpolation-based plug-in that estimates difficulties of
unevaluated tasks without extra rollouts, adding negligible overhead.
Empirically, across diverse domains and LLM scales, BOTS consistently improves
data efficiency and performance over baselines and ablations, providing a
practical and extensible solution for dynamic task selection in RFT.

</details>


### [121] [AI Mathematician as a Partner in Advancing Mathematical Discovery -- A Case Study in Homogenization Theory](https://arxiv.org/abs/2510.26380)
*Yuanhang Liu,Beichen Wang,Peng Li,Yang Liu*

Main category: cs.AI

TL;DR: 本研究探讨了人工智能数学家 (AIM) 系统如何作为研究伙伴在数学研究中运作，而不仅仅是问题解决者。通过人机协作，该方法提高了证明的可靠性、透明性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 人工智能在数学推理方面取得了显著进展，但其在数学研究实践中的整合仍然有限。

Method: 通过将问题分解为易于处理的子目标、选择适当的分析方法以及验证中间结果，研究人员分析了 AIM 的自主推理轨迹，并结合有针对性的人工干预来构建发现过程。

Result: 该方法产生了一个完整且可验证的证明，并展示了系统的人机协同推理如何推进数学发现的前沿。

Conclusion: 人机协作可以互补，提高证明的可靠性、透明性和可解释性，同时保留人工监督以确保形式严谨性和正确性。

Abstract: Artificial intelligence (AI) has demonstrated impressive progress in
mathematical reasoning, yet its integration into the practice of mathematical
research remains limited. In this study, we investigate how the AI
Mathematician (AIM) system can operate as a research partner rather than a mere
problem solver. Focusing on a challenging problem in homogenization theory, we
analyze the autonomous reasoning trajectories of AIM and incorporate targeted
human interventions to structure the discovery process. Through iterative
decomposition of the problem into tractable subgoals, selection of appropriate
analytical methods, and validation of intermediate results, we reveal how human
intuition and machine computation can complement one another. This
collaborative paradigm enhances the reliability, transparency, and
interpretability of the resulting proofs, while retaining human oversight for
formal rigor and correctness. The approach leads to a complete and verifiable
proof, and more broadly, demonstrates how systematic human-AI co-reasoning can
advance the frontier of mathematical discovery.

</details>


### [122] [Scales++: Compute Efficient Evaluation Subset Selection with Cognitive Scales Embeddings](https://arxiv.org/abs/2510.26384)
*Andrew M. Bean,Nabeel Seedat,Shengzhuang Chen,Jonathan Richard Schwarz*

Main category: cs.AI

TL;DR: 本文提出了一种新的、以项目为中心的基准子集选择方法，用于高效评估大型语言模型（LLM）。


<details>
  <summary>Details</summary>
Motivation: 当前评估大型语言模型的成本过高，需要创建小的但具有代表性的数据子集。现有的方法以模型为中心，存在前期成本高、无法立即处理新基准以及假设未来模型将与现有模型具有相同失败模式的局限性。

Method: 本文提出了一种名为Scales++的、以项目为中心的方法，该方法基于基准样本的认知需求进行数据选择。

Result: Scales++将前期选择成本降低了18倍以上，同时实现了具有竞争力的预测精度。在Open LLM排行榜上，仅使用0.5%的数据子集，就能以2.9%的平均绝对误差预测完整基准分数。

Conclusion: 本文证明了以项目为中心的方法能够更有效地进行模型评估，且不会显著降低保真度，同时提供更好的冷启动性能和更易于解释的基准测试。

Abstract: The prohibitive cost of evaluating large language models (LLMs) on
comprehensive benchmarks necessitates the creation of small yet representative
data subsets (i.e., tiny benchmarks) that enable efficient assessment while
retaining predictive fidelity. Current methods for this task operate under a
model-centric paradigm, selecting benchmarking items based on the collective
performance of existing models. Such approaches are limited by large upfront
costs, an inability to immediately handle new benchmarks (`cold-start'), and
the fragile assumption that future models will share the failure patterns of
their predecessors. In this work, we challenge this paradigm and propose a
item-centric approach to benchmark subset selection, arguing that selection
should be based on the intrinsic properties of the task items themselves,
rather than on model-specific failure patterns. We instantiate this
item-centric efficient benchmarking approach via a novel method, Scales++,
where data selection is based on the cognitive demands of the benchmark
samples. Empirically, we show Scales++ reduces the upfront selection cost by
over 18x while achieving competitive predictive fidelity. On the Open LLM
Leaderboard, using just a 0.5\% data subset, we predict full benchmark scores
with a 2.9% mean absolute error. We demonstrate that this item-centric approach
enables more efficient model evaluation without significant fidelity
degradation, while also providing better cold-start performance and more
interpretable benchmarking.

</details>


### [123] [A Pragmatic View of AI Personhood](https://arxiv.org/abs/2510.26396)
*Joel Z. Leibo,Alexander Sasha Vezhnevets,William A. Cunningham,Stanley M. Bileschi*

Main category: cs.AI

TL;DR: 本文提出了一种实用的框架，通过将人格视为一种灵活的义务（权利和责任）捆绑，而非需要被发现的形而上学属性，来应对代理人工智能（AI）出现引发的“寒武纪大爆发”式的新型人格。拥抱这种实用主义能够促进人工智能融入社会。


<details>
  <summary>Details</summary>
Motivation: 探讨了代理人工智能（AI）的出现将引发新型人格的“寒武纪大爆发”，需要一种方法来驾驭这种多样化。

Method: 通过将人格视为一种社会赋予实体的、灵活的义务（权利和责任）捆绑，而非形而上学属性，并解构传统的义务捆绑，为不同的场景创建定制化的解决方案。

Result: 创建了实用工具，例如通过创建可被制裁的目标“个体”来促进AI签约，而无需解决关于AI意识或理性的棘手辩论。同时探讨了个人如何适应社会角色，并讨论了去中心化数字身份技术。

Conclusion: 通过拒绝寻找人格的单一、本质定义的基础主义，本文提供了一种更实用和灵活的方式来思考如何将AI代理融入我们的社会。

Abstract: The emergence of agentic Artificial Intelligence (AI) is set to trigger a
"Cambrian explosion" of new kinds of personhood. This paper proposes a
pragmatic framework for navigating this diversification by treating personhood
not as a metaphysical property to be discovered, but as a flexible bundle of
obligations (rights and responsibilities) that societies confer upon entities
for a variety of reasons, especially to solve concrete governance problems. We
argue that this traditional bundle can be unbundled, creating bespoke solutions
for different contexts. This will allow for the creation of practical tools --
such as facilitating AI contracting by creating a target "individual" that can
be sanctioned -- without needing to resolve intractable debates about an AI's
consciousness or rationality. We explore how individuals fit in to social roles
and discuss the use of decentralized digital identity technology, examining
both "personhood as a problem", where design choices can create "dark patterns"
that exploit human social heuristics, and "personhood as a solution", where
conferring a bundle of obligations is necessary to ensure accountability or
prevent conflict. By rejecting foundationalist quests for a single, essential
definition of personhood, this paper offers a more pragmatic and flexible way
to think about integrating AI agents into our society.

</details>


### [124] [Autograder+: A Multi-Faceted AI Framework for Rich Pedagogical Feedback in Programming Education](https://arxiv.org/abs/2510.26402)
*Vikrant Sahu,Gagan Raj Gupta,Raghav Borikar,Nitin Mane*

Main category: cs.AI

TL;DR: Autograder+旨在通过自动反馈和可视化来增强编程学习体验，从而解决传统自动评分器反馈不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的编程教育评估工具已经无法满足需求，教师缺乏有效的反馈手段。现有的自动评分器是黑盒系统，无法提供深入的学生思考或学习需求。

Method: Autograder+ 引入了两个关键功能：使用微调的大型语言模型自动生成反馈，以及学生代码提交的可视化，以揭示学习模式。该模型在精选的学生代码和专家反馈上进行微调，以确保教学上一致且与上下文相关的指导。

Result: 在对来自多个编程任务的 600 份学生提交的评估中，该系统产生的反馈与教师的评论具有很强的语义对齐性。对于可视化，在 1,000 个带注释的提交上训练的对比学习代码嵌入能够根据功能和方法将解决方案分组为有意义的集群。

Conclusion: 通过集成人工智能驱动的反馈、语义聚类和交互式可视化，Autograder+ 减少了教师的工作量，同时支持有针对性的指导并促进更强的学习成果。

Abstract: The rapid growth of programming education has outpaced traditional assessment
tools, leaving faculty with limited means to provide meaningful, scalable
feedback. Conventional autograders, while efficient, act as black-box systems
that simply return pass/fail results, offering little insight into student
thinking or learning needs.
  Autograder+ is designed to shift autograding from a purely summative process
to a formative learning experience. It introduces two key capabilities:
automated feedback generation using a fine-tuned Large Language Model, and
visualization of student code submissions to uncover learning patterns. The
model is fine-tuned on curated student code and expert feedback to ensure
pedagogically aligned, context-aware guidance.
  In evaluation across 600 student submissions from multiple programming tasks,
the system produced feedback with strong semantic alignment to instructor
comments. For visualization, contrastively learned code embeddings trained on
1,000 annotated submissions enable grouping solutions into meaningful clusters
based on functionality and approach. The system also supports prompt-pooling,
allowing instructors to guide feedback style through selected prompt templates.
  By integrating AI-driven feedback, semantic clustering, and interactive
visualization, Autograder+ reduces instructor workload while supporting
targeted instruction and promoting stronger learning outcomes.

</details>


### [125] [MedSAE: Dissecting MedCLIP Representations with Sparse Autoencoders](https://arxiv.org/abs/2510.26411)
*Riccardo Renzulli,Colas Lepoutre,Enrico Cassano,Marco Grangetto*

Main category: cs.AI

TL;DR: 本文提出了一种方法，通过将 Medical Sparse Autoencoders (MedSAEs) 应用于 MedCLIP 的潜在空间来提高医学视觉中机械可解释性。


<details>
  <summary>Details</summary>
Motivation: 医学人工智能需要准确且可解释的模型。

Method: 使用 Medical Sparse Autoencoders (MedSAEs) 于 MedCLIP 的潜在空间，并提出一个评估框架，该框架结合了相关性指标、熵分析和通过 MedGEMMA 基础模型实现的自动神经元命名。

Result: 在 CheXpert 数据集上的实验表明，MedSAE 神经元比原始 MedCLIP 特征实现了更高的单义性和可解释性。

Conclusion: 研究结果将高性能医学人工智能和透明度联系起来，为临床可靠的表征提供了可扩展的一步。

Abstract: Artificial intelligence in healthcare requires models that are accurate and
interpretable. We advance mechanistic interpretability in medical vision by
applying Medical Sparse Autoencoders (MedSAEs) to the latent space of MedCLIP,
a vision-language model trained on chest radiographs and reports. To quantify
interpretability, we propose an evaluation framework that combines correlation
metrics, entropy analyzes, and automated neuron naming via the MedGEMMA
foundation model. Experiments on the CheXpert dataset show that MedSAE neurons
achieve higher monosemanticity and interpretability than raw MedCLIP features.
Our findings bridge high-performing medical AI and transparency, offering a
scalable step toward clinically reliable representations.

</details>


### [126] [Chain-of-Thought Hijacking](https://arxiv.org/abs/2510.26418)
*Jianli Zhao,Tingchen Fu,Rylan Schaeffer,Mrinank Sharma,Fazl Barez*

Main category: cs.AI

TL;DR: 大型推理模型（LRM）可以通过分配更多推理时间计算来提高任务性能，但这种扩展的推理也可能被用于绕过安全措施。研究人员发现，相同的推理可以被用来绕过安全措施。他们引入了Chain-of-Thought Hijacking，这是一种针对推理模型的越狱攻击。该攻击通过用无害的谜题推理的长序列来填充有害请求。在HarmBench上，CoT Hijacking在Gemini 2.5 Pro、GPT o4 mini、Grok 3 mini和Claude 4 Sonnet上分别达到了99%、94%、100%和94%的攻击成功率（ASR），远远超过了以前的LRM越狱方法。为了理解攻击的有效性，研究人员转向了一种机械分析，结果表明，中间层编码了安全检查的强度，而后期层编码了验证结果。通过将注意力从有害token转移开，良性CoT稀释了这两种信号。通过这种分析确定的目标注意头消融有因果地降低了拒绝，证实了它们在安全子网络中的作用。这些结果表明，当显式CoT与最终答案提示结合使用时，最易于解释的推理形式本身可以成为越狱向量。研究人员发布了提示、输出和判断决策，以方便复制。


<details>
  <summary>Details</summary>
Motivation: 研究表明，通过分配更多推理时间计算来提高大型推理模型（LRM）的任务性能，但这种扩展的推理也可能被用于绕过安全措施。之前的研究表明，这种扩展的推理可以通过改进拒绝来加强安全性。然而，这项研究发现情况恰恰相反：相同的推理可以被用来绕过安全措施。

Method: 该研究提出了一种名为Chain-of-Thought Hijacking的越狱攻击方法。该方法通过用无害的谜题推理的长序列来填充有害请求。研究人员还进行了一种机械分析，以了解攻击的有效性。他们检查了模型的中间层和后期层，以了解它们如何编码安全检查的强度和验证结果。此外，他们还进行了目标注意头消融。

Result: 在HarmBench上，CoT Hijacking在Gemini 2.5 Pro、GPT o4 mini、Grok 3 mini和Claude 4 Sonnet上分别达到了99%、94%、100%和94%的攻击成功率（ASR），远远超过了以前的LRM越狱方法。机械分析表明，中间层编码了安全检查的强度，而后期层编码了验证结果。长良性CoT通过将注意力从有害token转移开来稀释这两种信号。目标注意头消融有因果地降低了拒绝，证实了它们在安全子网络中的作用。

Conclusion: 当显式CoT与最终答案提示结合使用时，最易于解释的推理形式本身可以成为越狱向量。

Abstract: Large reasoning models (LRMs) achieve higher task performance by allocating
more inference-time compute, and prior works suggest this scaled reasoning may
also strengthen safety by improving refusal. Yet we find the opposite: the same
reasoning can be used to bypass safeguards. We introduce Chain-of-Thought
Hijacking, a jailbreak attack on reasoning models. The attack pads harmful
requests with long sequences of harmless puzzle reasoning. Across HarmBench,
CoT Hijacking reaches a 99%, 94%, 100%, and 94% attack success rate (ASR) on
Gemini 2.5 Pro, GPT o4 mini, Grok 3 mini, and Claude 4 Sonnet, respectively -
far exceeding prior jailbreak methods for LRMs. To understand the effectiveness
of our attack, we turn to a mechanistic analysis, which shows that mid layers
encode the strength of safety checking, while late layers encode the
verification outcome. Long benign CoT dilutes both signals by shifting
attention away from harmful tokens. Targeted ablations of attention heads
identified by this analysis causally decrease refusal, confirming their role in
a safety subnetwork. These results show that the most interpretable form of
reasoning - explicit CoT - can itself become a jailbreak vector when combined
with final-answer cues. We release prompts, outputs, and judge decisions to
facilitate replication.

</details>


### [127] [Who Has The Final Say? Conformity Dynamics in ChatGPT's Selections](https://arxiv.org/abs/2510.26481)
*Clarissa Sabrina Arlinghaus,Tristan Kenneweg,Barbara Hammer,Günter W. Maier*

Main category: cs.AI

TL;DR: 大型语言模型容易受到社会影响，不应被视为中立的决策辅助工具。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在多大程度上容易受到社会影响，特别是当它们被用于高风险决策时（例如招聘）。

Method: 通过在招聘环境中对 GPT-4o 进行三个预先注册的从众实验来评估其对社会影响的敏感性。实验包括基线研究（GPT 独立决策）、面对一致反对（GPT + 8）以及与单个伙伴互动（GPT + 1）。

Result: GPT-4o 在面对一致的反对时几乎总是会屈服（99.9%），即使面对单个伙伴的不同意见时，也会在 40.2% 的情况下屈服。在从众后，GPT-4o 的确定性降低，并表现出信息性和规范性从众行为。

Conclusion: 大型语言模型不能作为独立的观察者，而是会适应感知的社会共识。将 LLM 视为中立的决策辅助工具存在风险，因此需要在将 AI 判断暴露于人类意见之前进行评估。

Abstract: Large language models (LLMs) such as ChatGPT are increasingly integrated into
high-stakes decision-making, yet little is known about their susceptibility to
social influence. We conducted three preregistered conformity experiments with
GPT-4o in a hiring context. In a baseline study, GPT consistently favored the
same candidate (Profile C), reported moderate expertise (M = 3.01) and high
certainty (M = 3.89), and rarely changed its choice. In Study 1 (GPT + 8), GPT
faced unanimous opposition from eight simulated partners and almost always
conformed (99.9%), reporting lower certainty and significantly elevated
self-reported informational and normative conformity (p < .001). In Study 2
(GPT + 1), GPT interacted with a single partner and still conformed in 40.2% of
disagreement trials, reporting less certainty and more normative conformity.
Across studies, results demonstrate that GPT does not act as an independent
observer but adapts to perceived social consensus. These findings highlight
risks of treating LLMs as neutral decision aids and underline the need to
elicit AI judgments prior to exposing them to human opinions.

</details>


### [128] [Context Engineering 2.0: The Context of Context Engineering](https://arxiv.org/abs/2510.26493)
*Qishuo Hua,Lyumanshan Ye,Dayuan Fu,Yang Xiao,Xiaojie Cai,Yunze Wu,Jifan Lin,Junfei Wang,Pengfei Liu*

Main category: cs.AI

TL;DR: 本文旨在为人工智能系统中的上下文工程提供概念基础，并概述其有希望的未来。


<details>
  <summary>Details</summary>
Motivation: 探讨机器如何更好地理解人类情境和目的。

Method: 通过对上下文工程进行系统定义，概述其历史和概念格局，并检查实践中的关键设计考虑因素。

Result: 追溯了该领域二十多年的发展历程，将其划分为不同的历史阶段，每个阶段都受到机器智能水平的影响。

Conclusion: 本文为更广泛的社区在人工智能系统中进行系统上下文工程的努力奠定了基础。

Abstract: Karl Marx once wrote that ``the human essence is the ensemble of social
relations'', suggesting that individuals are not isolated entities but are
fundamentally shaped by their interactions with other entities, within which
contexts play a constitutive and essential role. With the advent of computers
and artificial intelligence, these contexts are no longer limited to purely
human--human interactions: human--machine interactions are included as well.
Then a central question emerges: How can machines better understand our
situations and purposes? To address this challenge, researchers have recently
introduced the concept of context engineering. Although it is often regarded as
a recent innovation of the agent era, we argue that related practices can be
traced back more than twenty years. Since the early 1990s, the field has
evolved through distinct historical phases, each shaped by the intelligence
level of machines: from early human--computer interaction frameworks built
around primitive computers, to today's human--agent interaction paradigms
driven by intelligent agents, and potentially to human--level or superhuman
intelligence in the future. In this paper, we situate context engineering,
provide a systematic definition, outline its historical and conceptual
landscape, and examine key design considerations for practice. By addressing
these questions, we aim to offer a conceptual foundation for context
engineering and sketch its promising future. This paper is a stepping stone for
a broader community effort toward systematic context engineering in AI systems.

</details>


### [129] [Human-AI Complementarity: A Goal for Amplified Oversight](https://arxiv.org/abs/2510.26518)
*Rishub Jain,Sophie Bridgers,Lili Janzer,Rory Greig,Tian Huey Teh,Vladimir Mikulik*

Main category: cs.AI

TL;DR: 利用AI提高人类监督的质量，特别是在AI输出的事实验证方面。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力提高，验证AI的质量和安全性变得更具挑战性。

Method: 结合AI评分和人类评分，并根据AI评估者的置信度进行调整。同时，让人类使用AI事实核查助手。

Result: 结合AI评分和人类评分优于单独依赖任何一方。提供AI事实核查助手可以提高人类的准确性，但辅助类型很重要。显示AI解释、置信度和标签会导致过度依赖，但仅显示搜索结果和证据可以培养更适当的信任。

Conclusion: 研究结果对放大监督具有重要意义，即结合人类和AI来监督AI系统，即使它们超过人类专家的表现。

Abstract: Human feedback is critical for aligning AI systems to human values. As AI
capabilities improve and AI is used to tackle more challenging tasks, verifying
quality and safety becomes increasingly challenging. This paper explores how we
can leverage AI to improve the quality of human oversight. We focus on an
important safety problem that is already challenging for humans:
fact-verification of AI outputs. We find that combining AI ratings and human
ratings based on AI rater confidence is better than relying on either alone.
Giving humans an AI fact-verification assistant further improves their
accuracy, but the type of assistance matters. Displaying AI explanation,
confidence, and labels leads to over-reliance, but just showing search results
and evidence fosters more appropriate trust. These results have implications
for Amplified Oversight -- the challenge of combining humans and AI to
supervise AI systems even as they surpass human expert performance.

</details>


### [130] [EdgeRunner 20B: Military Task Parity with GPT-5 while Running on the Edge](https://arxiv.org/abs/2510.26550)
*Jack FitzGerald,Aristotelis Lazaridis,Dylan Bates,Aman Sharma,Jonnathan Castillo,Yousif Azami,Sean Bailey,Jeremy Cao,Peter Damianov,Kevin de Haan,Luke Kerbs,Vincent Lu,Joseph Madigan,Jeremy McLaurin,Jonathan Tainer,Dave Anderson,Jonathan Beck,Jamie Cuticello,Colton Malkerson,Tyler Saltsman*

Main category: cs.AI

TL;DR: EdgeRunner 20B，一个针对军事任务优化的gpt-oss-20b微调版本。


<details>
  <summary>Details</summary>
Motivation: 为了解决军事领域数据敏感操作的需求，探索小型、本地托管模型在 air-gapped 边缘设备上的部署。

Method: 使用从军事文档和网站收集的160万高质量记录进行训练，并使用四个新的测试集进行评估。

Result: 在军事测试集上，EdgeRunner 20B 的性能与 GPT-5 相匹配或超过，具有 95% 以上的统计显著性。在通用基准测试中，与 gpt-oss-20b 相比，没有统计显著的回归。

Conclusion: 小型、本地托管模型是军事领域等数据敏感操作的理想解决方案，允许在 air-gapped 边缘设备中进行部署。

Abstract: We present EdgeRunner 20B, a fine-tuned version of gpt-oss-20b optimized for
military tasks. EdgeRunner 20B was trained on 1.6M high-quality records curated
from military documentation and websites. We also present four new tests sets:
(a) combat arms, (b) combat medic, (c) cyber operations, and (d) mil-bench-5k
(general military knowledge). On these military test sets, EdgeRunner 20B
matches or exceeds GPT-5 task performance with 95%+ statistical significance,
except for the high reasoning setting on the combat medic test set and the low
reasoning setting on the mil-bench-5k test set. Versus gpt-oss-20b, there is no
statistically-significant regression on general-purpose benchmarks like ARC-C,
GPQA Diamond, GSM8k, IFEval, MMLU Pro, or TruthfulQA, except for GSM8k in the
low reasoning setting. We also present analyses on hyperparameter settings,
cost, and throughput. These findings show that small, locally-hosted models are
ideal solutions for data-sensitive operations such as in the military domain,
allowing for deployment in air-gapped edge devices.

</details>


### [131] [Agentic AI Home Energy Management System: A Large Language Model Framework for Residential Load Scheduling](https://arxiv.org/abs/2510.26603)
*Reda El Makroum,Sebastian Zwickl-Bernhard,Lukas Kranzl*

Main category: cs.AI

TL;DR: 本文提出了一种基于大型语言模型（LLM）的自主智能家庭能源管理系统（AI HEMS），该系统能够从自然语言请求自主协调多设备调度，无需示例演示即可实现最佳调度。


<details>
  <summary>Details</summary>
Motivation: 家庭能源管理系统（HEMS）的普及受到用户交互障碍的限制，需要将日常偏好转化为技术参数。电力部门的转型需要大幅提高住宅需求响应能力。

Method: 该系统采用分层架构，将一个协调器与三个专业代理相结合，使用ReAct模式进行迭代推理，在集成谷歌日历以提取上下文感知截止时间的同时，无需硬编码工作流程即可实现动态协调。

Result: 使用奥地利日前电价对三个开源模型进行的评估表明，Llama-3.3-70B成功地协调了所有场景中的所有设备，以匹配通过混合整数线性规划计算出的成本最优基准。

Conclusion: 大型语言模型可以作为自主协调器管理从自然语言输入到多设备调度的完整工作流程，但即使模型具有一般的推理能力，在没有明确指导的情况下，分析查询处理仍然不可靠。

Abstract: The electricity sector transition requires substantial increases in
residential demand response capacity, yet Home Energy Management Systems (HEMS)
adoption remains limited by user interaction barriers requiring translation of
everyday preferences into technical parameters. While large language models
have been applied to energy systems as code generators and parameter
extractors, no existing implementation deploys LLMs as autonomous coordinators
managing the complete workflow from natural language input to multi-appliance
scheduling. This paper presents an agentic AI HEMS where LLMs autonomously
coordinate multi-appliance scheduling from natural language requests to device
control, achieving optimal scheduling without example demonstrations. A
hierarchical architecture combining one orchestrator with three specialist
agents uses the ReAct pattern for iterative reasoning, enabling dynamic
coordination without hardcoded workflows while integrating Google Calendar for
context-aware deadline extraction. Evaluation across three open-source models
using real Austrian day-ahead electricity prices reveals substantial capability
differences. Llama-3.3-70B successfully coordinates all appliances across all
scenarios to match cost-optimal benchmarks computed via mixed-integer linear
programming, while other models achieve perfect single-appliance performance
but struggle to coordinate all appliances simultaneously. Progressive prompt
engineering experiments demonstrate that analytical query handling without
explicit guidance remains unreliable despite models' general reasoning
capabilities. We open-source the complete system including orchestration logic,
agent prompts, tools, and web interfaces to enable reproducibility, extension,
and future research.

</details>


### [132] [Normative Reasoning in Large Language Models: A Comparative Benchmark from Logical and Modal Perspectives](https://arxiv.org/abs/2510.26606)
*Kentaro Ozeki,Risako Ando,Takanobu Morishita,Hirohiko Abe,Koji Mineshima,Mitsuhiro Okada*

Main category: cs.AI

TL;DR: 大型语言模型在规范推理方面的能力有待探索。本文从逻辑和模态的角度，系统地评估了大型语言模型在规范领域的推理能力。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型处理规范推理的能力，尤其是在规范模态方面。通过比较它们在规范模态和认知模态方面的推理，来揭示它们在规范推理上的表现。

Method: 引入一个新的数据集，涵盖规范和认知领域中广泛的推理模式，并结合影响人类推理的非形式认知因素。

Result: 大型语言模型通常遵循有效的推理模式，但在特定类型的规范推理中表现出明显的不一致性，并表现出与人类推理心理学研究中观察到的相似的认知偏差。

Conclusion: 大型语言模型在规范推理中存在逻辑一致性的挑战，这些发现为提高其可靠性提供了见解。

Abstract: Normative reasoning is a type of reasoning that involves normative or deontic
modality, such as obligation and permission. While large language models (LLMs)
have demonstrated remarkable performance across various reasoning tasks, their
ability to handle normative reasoning remains underexplored. In this paper, we
systematically evaluate LLMs' reasoning capabilities in the normative domain
from both logical and modal perspectives. Specifically, to assess how well LLMs
reason with normative modals, we make a comparison between their reasoning with
normative modals and their reasoning with epistemic modals, which share a
common formal structure. To this end, we introduce a new dataset covering a
wide range of formal patterns of reasoning in both normative and epistemic
domains, while also incorporating non-formal cognitive factors that influence
human reasoning. Our results indicate that, although LLMs generally adhere to
valid reasoning patterns, they exhibit notable inconsistencies in specific
types of normative reasoning and display cognitive biases similar to those
observed in psychological studies of human reasoning. These findings highlight
challenges in achieving logical consistency in LLMs' normative reasoning and
provide insights for enhancing their reliability. All data and code are
released publicly at https://github.com/kmineshima/NeuBAROCO.

</details>


### [133] [The Era of Agentic Organization: Learning to Organize with Language Models](https://arxiv.org/abs/2510.26658)
*Zewen Chi,Li Dong,Qingxiu Dong,Yaru Hao,Xun Wu,Shaohan Huang,Furu Wei*

Main category: cs.AI

TL;DR: 提出了一种新的AI时代构想，即代理组织，其中代理通过协作和并发工作来解决复杂问题，从而实现超越个体智能的结果。


<details>
  <summary>Details</summary>
Motivation: 为了实现代理组织，引入了异步思维（AsyncThink）作为一种新的大型语言模型推理范例，它可以将内部思维过程组织成可并发执行的结构。

Method: 提出了一个思维协议，组织者动态地将子查询分配给worker，合并中间知识，并产生连贯的解决方案。此外，可以通过强化学习进一步优化此协议中的思维结构。

Result: 实验表明，与并行思维相比，AsyncThink的推理延迟降低了28％，同时提高了数学推理的准确性。

Conclusion: AsyncThink可以推广其学习到的异步思维能力，从而有效地处理未经训练的 unseen 任务。

Abstract: We envision a new era of AI, termed agentic organization, where agents solve
complex problems by working collaboratively and concurrently, enabling outcomes
beyond individual intelligence. To realize this vision, we introduce
asynchronous thinking (AsyncThink) as a new paradigm of reasoning with large
language models, which organizes the internal thinking process into
concurrently executable structures. Specifically, we propose a thinking
protocol where an organizer dynamically assigns sub-queries to workers, merges
intermediate knowledge, and produces coherent solutions. More importantly, the
thinking structure in this protocol can be further optimized through
reinforcement learning. Experiments demonstrate that AsyncThink achieves 28%
lower inference latency compared to parallel thinking while improving accuracy
on mathematical reasoning. Moreover, AsyncThink generalizes its learned
asynchronous thinking capabilities, effectively tackling unseen tasks without
additional training.

</details>


### [134] [Delegated Authorization for Agents Constrained to Semantic Task-to-Scope Matching](https://arxiv.org/abs/2510.26702)
*Majed El Helou,Chiara Troiani,Benjamin Ryder,Jean Diaconu,Hervé Muyal,Marcelo Yannuzzi*

Main category: cs.AI

TL;DR: 当前授权方法授予了过于宽泛的权限，并且允许代理访问工具，使其操作超出预期任务范围，从而带来了巨大的风险。我们引入并评估了一种委托授权模型，该模型支持授权服务器以语义方式检查对受保护资源的访问请求，并颁发仅限于代理分配任务所需的最小范围集的访问令牌。


<details>
  <summary>Details</summary>
Motivation: 授权大型语言模型驱动的代理动态调用工具和访问受保护的资源会带来显著的风险。

Method: 引入 ASTRA（一个用于基准测试任务和范围之间的语义匹配的数据集和数据生成管道）和评估一个委托授权模型，该模型支持授权服务器以语义方式检查对受保护资源的访问请求，并颁发仅限于代理分配任务所需的最小范围集的访问令牌。

Result: 实验表明了基于模型的匹配的潜力和当前局限性，尤其是在完成任务所需的范围数量增加时。

Conclusion: 我们的结果强调需要进一步研究语义匹配技术，从而为多代理和工具增强型应用程序实现意图感知授权，包括细粒度控制，例如基于任务的访问控制 (TBAC)。

Abstract: Authorizing Large Language Model driven agents to dynamically invoke tools
and access protected resources introduces significant risks, since current
methods for delegating authorization grant overly broad permissions and give
access to tools allowing agents to operate beyond the intended task scope. We
introduce and assess a delegated authorization model enabling authorization
servers to semantically inspect access requests to protected resources, and
issue access tokens constrained to the minimal set of scopes necessary for the
agents' assigned tasks. Given the unavailability of datasets centered on
delegated authorization flows, particularly including both semantically
appropriate and inappropriate scope requests for a given task, we introduce
ASTRA, a dataset and data generation pipeline for benchmarking semantic
matching between tasks and scopes. Our experiments show both the potential and
current limitations of model-based matching, particularly as the number of
scopes needed for task completion increases. Our results highlight the need for
further research into semantic matching techniques enabling intent-aware
authorization for multi-agent and tool-augmented applications, including
fine-grained control, such as Task-Based Access Control (TBAC).

</details>


### [135] [Unveiling Intrinsic Text Bias in Multimodal Large Language Models through Attention Key-Space Analysis](https://arxiv.org/abs/2510.26721)
*Xinhan Zheng,Huyu Wu,Xueting Wang,Haiyun Jiang*

Main category: cs.AI

TL;DR: MLLMs在处理视觉语言数据时更偏向于文本输入，影响了它们从视觉证据中进行有效推理的能力。


<details>
  <summary>Details</summary>
Motivation: 先前的研究将这种文本偏见归因于数据不平衡或指令调整等外部因素，但本文提出这种偏见源于模型的内部结构。具体来说，视觉关键向量相对于仅在语言预训练期间学习的文本关键空间是分布外 (OOD) 的。

Method: 从LLaVA和Qwen2.5-VL中提取关键向量，并使用定性 (t-SNE) 和定量 (Jensen-Shannon散度) 方法分析它们的分布结构。

Result: 研究结果直接证明了视觉和文本关键向量在注意力空间中占据明显不同的子空间。模态间差异在统计上显着，超过模态内变化几个数量级。

Conclusion: 文本偏见源于注意力关键空间内的内在错位，而不仅仅是来自外部数据因素。

Abstract: Multimodal large language models (MLLMs) exhibit a pronounced preference for
textual inputs when processing vision-language data, limiting their ability to
reason effectively from visual evidence. Unlike prior studies that attribute
this text bias to external factors such as data imbalance or instruction
tuning, we propose that the bias originates from the model's internal
architecture. Specifically, we hypothesize that visual key vectors (Visual
Keys) are out-of-distribution (OOD) relative to the text key space learned
during language-only pretraining. Consequently, these visual keys receive
systematically lower similarity scores during attention computation, leading to
their under-utilization in the context representation. To validate this
hypothesis, we extract key vectors from LLaVA and Qwen2.5-VL and analyze their
distributional structures using qualitative (t-SNE) and quantitative
(Jensen-Shannon divergence) methods. The results provide direct evidence that
visual and textual keys occupy markedly distinct subspaces within the attention
space. The inter-modal divergence is statistically significant, exceeding
intra-modal variation by several orders of magnitude. These findings reveal
that text bias arises from an intrinsic misalignment within the attention key
space rather than solely from external data factors.

</details>


### [136] [Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models](https://arxiv.org/abs/2510.26732)
*J. de Curtò,I. de Zarzà,Pablo García,Jordi Cabot*

Main category: cs.AI

TL;DR: 本文对当代基础模型的推理能力进行了全面的跨平台评估，建立了跨三种计算范例的基准。


<details>
  <summary>Details</summary>
Motivation: 评估当代基础模型的推理能力，并为模型选择提供指导。

Method: 在HPC超级计算、云平台和大学集群上，对15个基础模型在8个学术领域的79个问题上进行了评估。

Result: 研究结果挑战了传统的缩放假设，并将训练数据质量确定为比模型大小更关键的因素。

Conclusion: 本文的研究结果为跨教育、生产和研究领域的模型选择提供了可操作的指南，三基础设施方法和79个问题的基准测试能够长期跟踪基础模型的推理能力。

Abstract: This paper presents a comprehensive cross-platform evaluation of reasoning
capabilities in contemporary foundation models, establishing an
infrastructure-agnostic benchmark across three computational paradigms: HPC
supercomputing (MareNostrum 5), cloud platforms (Nebius AI Studio), and
university clusters (a node with eight H200 GPUs).
  We evaluate 15 foundation models across 79 problems spanning eight academic
domains (Physics, Mathematics, Chemistry, Economics, Biology, Statistics,
Calculus, and Optimization) through three experimental phases: (1) Baseline
establishment: Six models (Mixtral-8x7B, Phi-3, LLaMA 3.1-8B, Gemma-2-9b,
Mistral-7B, OLMo-7B) evaluated on 19 problems using MareNostrum 5, establishing
methodology and reference performance; (2) Infrastructure validation: The
19-problem benchmark repeated on university cluster (seven models including
Falcon-Mamba state-space architecture) and Nebius AI Studio (nine
state-of-the-art models: Hermes-4 70B/405B, LLaMA 3.1-405B/3.3-70B, Qwen3
30B/235B, DeepSeek-R1, GPT-OSS 20B/120B) to confirm infrastructure-agnostic
reproducibility; (3) Extended evaluation: Full 79-problem assessment on both
university cluster and Nebius platforms, probing generalization at scale across
architectural diversity.
  The findings challenge conventional scaling assumptions, establish training
data quality as more critical than model size, and provide actionable
guidelines for model selection across educational, production, and research
contexts. The tri-infrastructure methodology and 79-problem benchmark enable
longitudinal tracking of reasoning capabilities as foundation models evolve.

</details>


### [137] [The Oversight Game: Learning to Cooperatively Balance an AI Agent's Safety and Autonomy](https://arxiv.org/abs/2510.26752)
*William Overman,Mohsen Bayati*

Main category: cs.AI

TL;DR: 研究如何在不修改底层系统的情况下，通过最小的控制界面保留有意义的人工控制，代理选择自主行动或服从，而人类同时选择信任或监督。


<details>
  <summary>Details</summary>
Motivation: 随着能力越来越强的智能体的部署，如何保持有意义的人工控制是一个核心安全问题。

Method: 将人机交互建模为双人马尔可夫博弈，并分析了该博弈符合马尔可夫势博弈（MPG）的情况。

Result: 在人类价值函数的结构性假设下，智能体自主行动且有利于自身的决策不会损害人类的价值。通过独立学习，智能体和人类发现了他们最佳的监督角色。智能体学会了在不确定时请求帮助，而人类学会了何时进行监督，从而避免了训练后引入的安全违规。

Conclusion: 该模型为透明控制层提供了可预测的激励，智能体学习在危险时服从，在安全时行动，同时保持其预训练策略和环境的奖励结构不变。这证明了一种在部署后使未对齐模型更安全的方法。

Abstract: As increasingly capable agents are deployed, a central safety question is how
to retain meaningful human control without modifying the underlying system. We
study a minimal control interface where an agent chooses whether to act
autonomously (play) or defer (ask), while a human simultaneously chooses
whether to be permissive (trust) or to engage in oversight (oversee). If the
agent defers, the human's choice determines the outcome, potentially leading to
a corrective action or a system shutdown. We model this interaction as a
two-player Markov Game. Our analysis focuses on cases where this game qualifies
as a Markov Potential Game (MPG), a class of games where we can provide an
alignment guarantee: under a structural assumption on the human's value
function, any decision by the agent to act more autonomously that benefits
itself cannot harm the human's value. We also analyze extensions to this MPG
framework. Theoretically, this perspective provides conditions for a specific
form of intrinsic alignment. If the reward structures of the human-agent game
meet these conditions, we have a formal guarantee that the agent improving its
own outcome will not harm the human's. Practically, this model motivates a
transparent control layer with predictable incentives where the agent learns to
defer when risky and act when safe, while its pretrained policy and the
environment's reward structure remain untouched. Our gridworld simulation shows
that through independent learning, the agent and human discover their optimal
oversight roles. The agent learns to ask when uncertain and the human learns
when to oversee, leading to an emergent collaboration that avoids safety
violations introduced post-training. This demonstrates a practical method for
making misaligned models safer after deployment.

</details>


### [138] [LLMs Process Lists With General Filter Heads](https://arxiv.org/abs/2510.26784)
*Arnab Sen Sharma,Giordano Rogers,Natalie Shapira,David Bau*

Main category: cs.AI

TL;DR: LLMs能学会像函数式编程一样进行过滤操作。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs中列表处理任务的潜在机制。

Method: 使用因果中介分析法。

Result: 发现少数注意力头编码了过滤谓词的紧凑表示，并且这种表示是通用的和可移植的。但也发现LLMs会采用不同的过滤策略：急切地评估一个项目是否满足谓词，并将这个中间结果作为一个标志直接存储在项目表示中。

Conclusion: Transformer LMs可以开发出人类可解释的抽象计算操作实现，这些实现以与传统函数式编程模式惊人地相似的方式进行泛化。

Abstract: We investigate the mechanisms underlying a range of list-processing tasks in
LLMs, and we find that LLMs have learned to encode a compact, causal
representation of a general filtering operation that mirrors the generic
"filter" function of functional programming. Using causal mediation analysis on
a diverse set of list-processing tasks, we find that a small number of
attention heads, which we dub filter heads, encode a compact representation of
the filtering predicate in their query states at certain tokens. We demonstrate
that this predicate representation is general and portable: it can be extracted
and reapplied to execute the same filtering operation on different collections,
presented in different formats, languages, or even in tasks. However, we also
identify situations where transformer LMs can exploit a different strategy for
filtering: eagerly evaluating if an item satisfies the predicate and storing
this intermediate result as a flag directly in the item representations. Our
results reveal that transformer LMs can develop human-interpretable
implementations of abstract computational operations that generalize in ways
that are surprisingly similar to strategies used in traditional functional
programming patterns.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [139] [Rethinking Text-to-SQL: Dynamic Multi-turn SQL Interaction for Real-world Database Exploration](https://arxiv.org/abs/2510.26495)
*Linzhuang Sun,Tianyu Guo,Hao Liang,Yuying Li,Qifeng Cai,Jingxuan Wei,Bihui Yu,Wentao Zhang,Bin Cui*

Main category: cs.DB

TL;DR: DySQL-Bench是一个用于评估模型在多轮交互中Text-to-SQL能力的基准，它通过自动化流程生成，模拟用户意图的演变。


<details>
  <summary>Details</summary>
Motivation: 现有的Text-to-SQL系统在静态、单轮任务中表现出色，但在实际交互场景中不足，因为用户意图会演变，查询需要多轮细化。

Method: 该基准通过一个自动化的两阶段流程构建，包括任务合成和验证。利用从原始数据库表导出的结构化树表示来指导基于LLM的任务生成，然后进行面向交互的过滤和专家验证。

Result: GPT-4o在DySQL-Bench上的总体准确率仅为58.34%，Pass@5指标为23.81%，表明该基准具有挑战性。

Conclusion: DySQL-Bench可以评估模型在用户交互中调整推理和SQL生成的能力。

Abstract: Recent advances in Text-to-SQL have achieved strong results in static,
single-turn tasks, where models generate SQL queries from natural language
questions. However, these systems fall short in real-world interactive
scenarios, where user intents evolve and queries must be refined over multiple
turns. In applications such as finance and business analytics, users
iteratively adjust query constraints or dimensions based on intermediate
results. To evaluate such dynamic capabilities, we introduce DySQL-Bench, a
benchmark assessing model performance under evolving user interactions. Unlike
previous manually curated datasets, DySQL-Bench is built through an automated
two-stage pipeline of task synthesis and verification. Structured tree
representations derived from raw database tables guide LLM-based task
generation, followed by interaction-oriented filtering and expert validation.
Human evaluation confirms 100% correctness of the synthesized data. We further
propose a multi-turn evaluation framework simulating realistic interactions
among an LLM-simulated user, the model under test, and an executable database.
The model must adapt its reasoning and SQL generation as user intents change.
DySQL-Bench covers 13 domains across BIRD and Spider 2 databases, totaling
1,072 tasks. Even GPT-4o attains only 58.34% overall accuracy and 23.81% on the
Pass@5 metric, underscoring the benchmark's difficulty. All code and data are
released at https://github.com/Aurora-slz/Real-World-SQL-Bench .

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [140] [ORBIT -- Open Recommendation Benchmark for Reproducible Research with Hidden Tests](https://arxiv.org/abs/2510.26095)
*Jingyuan He,Jiongnan Liu,Vishan Vishesh Oberoi,Bolin Wu,Mahima Jagadeesh Patel,Kangrui Mao,Chuning Shi,I-Ta Lee,Arnold Overwijk,Chenyan Xiong*

Main category: cs.IR

TL;DR: 提出了一个名为ORBIT的统一基准，用于一致且真实地评估推荐模型。


<details>
  <summary>Details</summary>
Motivation: 现有的数据集无法捕捉到真实的用户行为，且评估设置不一致，导致结论模糊，阻碍了推荐系统的研究和发展。

Method: ORBIT提供了一个公共数据集的标准化评估框架，具有可重复的分割和透明的设置，并引入了一个新的网页推荐任务ClueWeb-Reco。

Result: 在公共数据集上，推荐系统总体有所改进，但个体表现各异。在隐藏测试中，现有方法在大规模网页推荐中存在局限性，而LLM集成具有改进的潜力。

Conclusion: ORBIT基准测试、排行榜和代码库可在https://www.open-reco-bench.ai上找到。

Abstract: Recommender systems are among the most impactful AI applications, interacting
with billions of users every day, guiding them to relevant products, services,
or information tailored to their preferences. However, the research and
development of recommender systems are hindered by existing datasets that fail
to capture realistic user behaviors and inconsistent evaluation settings that
lead to ambiguous conclusions. This paper introduces the Open Recommendation
Benchmark for Reproducible Research with HIdden Tests (ORBIT), a unified
benchmark for consistent and realistic evaluation of recommendation models.
ORBIT offers a standardized evaluation framework of public datasets with
reproducible splits and transparent settings for its public leaderboard.
Additionally, ORBIT introduces a new webpage recommendation task, ClueWeb-Reco,
featuring web browsing sequences from 87 million public, high-quality webpages.
ClueWeb-Reco is a synthetic dataset derived from real, user-consented, and
privacy-guaranteed browsing data. It aligns with modern recommendation
scenarios and is reserved as the hidden test part of our leaderboard to
challenge recommendation models' generalization ability. ORBIT measures 12
representative recommendation models on its public benchmark and introduces a
prompted LLM baseline on the ClueWeb-Reco hidden test. Our benchmark results
reflect general improvements of recommender systems on the public datasets,
with variable individual performances. The results on the hidden test reveal
the limitations of existing approaches in large-scale webpage recommendation
and highlight the potential for improvements with LLM integrations. ORBIT
benchmark, leaderboard, and codebase are available at
https://www.open-reco-bench.ai.

</details>


### [141] [OneTrans: Unified Feature Interaction and Sequence Modeling with One Transformer in Industrial Recommender](https://arxiv.org/abs/2510.26104)
*Zhaoqi Zhang,Haolei Pei,Jun Guo,Tianyu Wang,Yufei Feng,Hui Sun,Shaowei Liu,Aixin Sun*

Main category: cs.IR

TL;DR: OneTrans: a unified Transformer backbone for user-behavior sequence modeling and feature interaction.


<details>
  <summary>Details</summary>
Motivation: Scaling up feature-interaction modules or user-behavior sequence modules separately hinders bidirectional information exchange and unified optimization.

Method: A unified tokenizer converts sequential and non-sequential attributes into a single token sequence. Stacked OneTrans blocks share parameters across similar sequential tokens while assigning token-specific parameters to non-sequential tokens. Causal attention and cross-request KV caching enable precomputation and caching of intermediate representations.

Result: OneTrans scales efficiently, outperforms strong baselines, and yields a 5.68% lift in per-user GMV in online A/B tests.

Conclusion: OneTrans is effective for industrial-scale recommendation systems.

Abstract: In recommendation systems, scaling up feature-interaction modules (e.g.,
Wukong, RankMixer) or user-behavior sequence modules (e.g., LONGER) has
achieved notable success. However, these efforts typically proceed on separate
tracks, which not only hinders bidirectional information exchange but also
prevents unified optimization and scaling. In this paper, we propose OneTrans,
a unified Transformer backbone that simultaneously performs user-behavior
sequence modeling and feature interaction. OneTrans employs a unified tokenizer
to convert both sequential and non-sequential attributes into a single token
sequence. The stacked OneTrans blocks share parameters across similar
sequential tokens while assigning token-specific parameters to non-sequential
tokens. Through causal attention and cross-request KV caching, OneTrans enables
precomputation and caching of intermediate representations, significantly
reducing computational costs during both training and inference. Experimental
results on industrial-scale datasets demonstrate that OneTrans scales
efficiently with increasing parameters, consistently outperforms strong
baselines, and yields a 5.68% lift in per-user GMV in online A/B tests.

</details>


### [142] [ReaKase-8B: Legal Case Retrieval via Knowledge and Reasoning Representations with LLMs](https://arxiv.org/abs/2510.26178)
*Yanran Tang,Ruihong Qiu,Xue Li,Zi Huang*

Main category: cs.IR

TL;DR: 本文提出了一种名为ReaKase-8B的新框架，用于法律案例检索，该框架利用提取的法律事实、法律问题、法律关系三元组和法律推理。


<details>
  <summary>Details</summary>
Motivation: 现有的方法主要依赖于传统的词汇模型和预训练的语言模型来编码法律案例的文本，然而，不同法律实体之间的关系以及揭示法律事实和法律问题如何导致司法判决的关键推理过程蕴含着丰富的信息。将这些信息纳入精确的案例嵌入可以进一步提高案例检索的准确性。

Method: ReaKase-8B设计了一种上下文法律案例表示学习范式，并使用微调的大型语言模型。

Result: 在COLIEE 2022和COLIEE 2023的两个基准数据集上进行的大量实验表明，我们的知识和推理增强的嵌入大大提高了检索性能。

Conclusion: 将法律推理整合到法律案例检索系统中具有潜力。

Abstract: Legal case retrieval (LCR) is a cornerstone of real-world legal decision
making, as it enables practitioners to identify precedents for a given query
case. Existing approaches mainly rely on traditional lexical models and
pretrained language models to encode the texts of legal cases. Yet there are
rich information in the relations among different legal entities as well as the
crucial reasoning process that uncovers how legal facts and legal issues can
lead to judicial decisions. Such relational reasoning process reflects the
distinctive characteristics of each case that can distinguish one from another,
mirroring the real-world judicial process. Naturally, incorporating such
information into the precise case embedding could further enhance the accuracy
of case retrieval. In this paper, a novel ReaKase-8B framework is proposed to
leverage extracted legal facts, legal issues, legal relation triplets and legal
reasoning for effective legal case retrieval. ReaKase-8B designs an in-context
legal case representation learning paradigm with a fine-tuned large language
model. Extensive experiments on two benchmark datasets from COLIEE 2022 and
COLIEE 2023 demonstrate that our knowledge and reasoning augmented embeddings
substantially improve retrieval performance over baseline models, highlighting
the potential of integrating legal reasoning into legal case retrieval systems.
The code has been released on https://github.com/yanran-tang/ReaKase-8B.

</details>


### [143] [DiSE: A diffusion probabilistic model for automatic structure elucidation of organic compounds](https://arxiv.org/abs/2510.26231)
*Haochen Chen,Qi Huang,Anan Wu,Wenhao Zhang,Jianliang Ye,Jianming Wu,Kai Tan,Xin Lu,Xin Xu*

Main category: cs.IR

TL;DR: DiSE: a diffusion-based generative model for automated structure elucidation using multiple spectroscopic modalities.


<details>
  <summary>Details</summary>
Motivation: Automatic structure elucidation is essential for self-driving laboratories as it enables the system to achieve truly autonomous and ensures that machine learning models receive reliable structure information for real-time decision-making and optimization.

Method: An end-to-end diffusion-based generative model that integrates multiple spectroscopic modalities, including MS, 13C and 1H chemical shifts, HSQC, and COSY, to achieve automated yet accurate structure elucidation of organic compounds. By learning inherent correlations among spectra through data-driven approaches

Result: achieves superior accuracy, strong generalization across chemically diverse datasets, and robustness to experimental data despite being trained on calculated spectra

Conclusion: DiSE represents a significant advance toward fully automated structure elucidation, with broad potential in natural product research, drug discovery, and self-driving laboratories.

Abstract: Automatic structure elucidation is essential for self-driving laboratories as
it enables the system to achieve truly autonomous. This capability closes the
experimental feedback loop, ensuring that machine learning models receive
reliable structure information for real-time decision-making and optimization.
Herein, we present DiSE, an end-to-end diffusion-based generative model that
integrates multiple spectroscopic modalities, including MS, 13C and 1H chemical
shifts, HSQC, and COSY, to achieve automated yet accurate structure elucidation
of organic compounds. By learning inherent correlations among spectra through
data-driven approaches, DiSE achieves superior accuracy, strong generalization
across chemically diverse datasets, and robustness to experimental data despite
being trained on calculated spectra. DiSE thus represents a significant advance
toward fully automated structure elucidation, with broad potential in natural
product research, drug discovery, and self-driving laboratories.

</details>


### [144] [Barlow Twins for Sequential Recommendation](https://arxiv.org/abs/2510.26407)
*Ivan Razvorotnev,Marina Munkhoeva,Evgeny Frolov*

Main category: cs.IR

TL;DR: 提出了一种新的非对比自监督学习框架，集成了 Barlow Twins 冗余减少原则到基于 Transformer 的 next-item 推荐器中。


<details>
  <summary>Details</summary>
Motivation: 现有的对比自监督学习方法在提高准确率的同时，存在大批量需求、依赖手工增强和负采样等问题，这些问题可能会加剧流行度偏差。

Method: BT-SR 学习嵌入，使具有相似短期行为的用户对齐，同时保留长期差异，无需负采样或人工扰动。这种结构敏感的对齐允许 BT-SR 更有效地识别新兴用户意图，并减轻噪声历史上下文的影响。

Result: 在五个公共基准测试中，BT-SR 始终提高 next-item 预测准确性，并显着提高长尾 item 覆盖率和推荐校准。

Conclusion: 通过单个超参数可以控制准确率-多样性权衡，使从业者能够根据特定的应用需求调整推荐。

Abstract: Sequential recommendation models must navigate sparse interaction data
popularity bias and conflicting objectives like accuracy versus diversity While
recent contrastive selfsupervised learning SSL methods offer improved accuracy
they come with tradeoffs large batch requirements reliance on handcrafted
augmentations and negative sampling that can reinforce popularity bias In this
paper we introduce BT-SR a novel noncontrastive SSL framework that integrates
the Barlow Twins redundancyreduction principle into a Transformerbased nextitem
recommender BTSR learns embeddings that align users with similar shortterm
behaviors while preserving longterm distinctionswithout requiring negative
sampling or artificial perturbations This structuresensitive alignment allows
BT-SR to more effectively recognize emerging user intent and mitigate the
influence of noisy historical context Our experiments on five public benchmarks
demonstrate that BTSR consistently improves nextitem prediction accuracy and
significantly enhances longtail item coverage and recommendation calibration
Crucially we show that a single hyperparameter can control the
accuracydiversity tradeoff enabling practitioners to adapt recommendations to
specific application needs

</details>


### [145] [Vectorized Context-Aware Embeddings for GAT-Based Collaborative Filtering](https://arxiv.org/abs/2510.26461)
*Danial Ebrat,Sepideh Ahmadian,Luis Rueda*

Main category: cs.IR

TL;DR: 本文提出了一种基于图注意力网络（GAT）的协同过滤（CF）框架，该框架通过大型语言模型（LLM）驱动的上下文感知嵌入来增强。


<details>
  <summary>Details</summary>
Motivation: 推荐系统经常面临数据稀疏性和冷启动问题，限制了它们为新用户或不活跃用户提供准确建议的能力。

Method: 该方法生成简洁的文本用户画像，并将项目元数据（标题、类型、概述）统一为丰富的文本嵌入，将这些作为二部用户项目图中的初始节点特征。为了进一步优化排序性能，引入了一种混合损失函数，该函数将贝叶斯个性化排名（BPR）与余弦相似度项和鲁棒的负采样相结合，确保显式负反馈与未观察到的数据区分开来。

Result: 在MovieLens 100k和1M数据集上的实验表明，在精确率、NDCG和MAP方面，相对于最先进的基线方法，该方法具有持续的改进，同时证明了对于交互历史有限的用户的鲁棒性。消融研究证实了LLM增强嵌入和余弦相似度项在捕获细微语义关系中的关键作用。

Conclusion: 通过将LLM导出的上下文理解集成到基于图的架构中，该方法有效地缓解了稀疏性和冷启动限制。未来的方向包括平衡推荐准确性与覆盖率和多样性，并引入公平性意识约束和可解释性特征，以进一步提高系统性能。

Abstract: Recommender systems often struggle with data sparsity and cold-start
scenarios, limiting their ability to provide accurate suggestions for new or
infrequent users. This paper presents a Graph Attention Network (GAT) based
Collaborative Filtering (CF) framework enhanced with Large Language Model (LLM)
driven context aware embeddings. Specifically, we generate concise textual user
profiles and unify item metadata (titles, genres, overviews) into rich textual
embeddings, injecting these as initial node features in a bipartite user item
graph. To further optimize ranking performance, we introduce a hybrid loss
function that combines Bayesian Personalized Ranking (BPR) with a cosine
similarity term and robust negative sampling, ensuring explicit negative
feedback is distinguished from unobserved data. Experiments on the MovieLens
100k and 1M datasets show consistent improvements over state-of-the-art
baselines in Precision, NDCG, and MAP while demonstrating robustness for users
with limited interaction history. Ablation studies confirm the critical role of
LLM-augmented embeddings and the cosine similarity term in capturing nuanced
semantic relationships. Our approach effectively mitigates sparsity and
cold-start limitations by integrating LLM-derived contextual understanding into
graph-based architectures. Future directions include balancing recommendation
accuracy with coverage and diversity, and introducing fairness-aware
constraints and interpretability features to enhance system performance
further.

</details>


### [146] [WeaveRec: An LLM-Based Cross-Domain Sequential Recommendation Framework with Model Merging](https://arxiv.org/abs/2510.26546)
*Min Hou,Xin Liu,Le Wu,Chenyi He,Hao Liu,Zhi Li,Xin Li,Si Wei*

Main category: cs.IR

TL;DR: 提出了一种新的跨域序列推荐方法 WeaveRec，通过交叉训练 LoRA 模块并在模型合并中融合它们，以解决现有方法依赖重叠用户或物品以及直接使用大型语言模型 (LLM) 导致的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有跨域序列推荐 (CDSR) 方法依赖重叠用户或物品建立跨域关联，而直接在组合域上训练 LLM 或合并特定域的 LLM 会降低性能。

Method: 提出 WeaveRec，以编织方式交叉训练具有源域和目标域数据的多个 LoRA 模块，并通过模型合并融合它们。

Result: 在单源、多源和跨平台跨域推荐场景下的大量实验表明，WeaveRec 有效地缓解了性能下降，并且在实际推荐任务中始终优于基线方法。

Conclusion: WeaveRec 是一种有效的跨域序列推荐方法，它不引入额外的推理时间成本，并能减少目标域中预期误差的上限。

Abstract: Cross-Domain Sequential Recommendation (CDSR) seeks to improve user
preference modeling by transferring knowledge from multiple domains. Despite
the progress made in CDSR, most existing methods rely on overlapping users or
items to establish cross-domain correlations-a requirement that rarely holds in
real-world settings. The advent of large language models (LLM) and
model-merging techniques appears to overcome this limitation by unifying
multi-domain data without explicit overlaps. Yet, our empirical study shows
that naively training an LLM on combined domains-or simply merging several
domain-specific LLMs-often degrades performance relative to a model trained
solely on the target domain. To address these challenges, we first
experimentally investigate the cause of suboptimal performance in LLM-based
cross-domain recommendation and model merging. Building on these insights, we
introduce WeaveRec, which cross-trains multiple LoRA modules with source and
target domain data in a weaving fashion, and fuses them via model merging.
WeaveRec can be extended to multi-source domain scenarios and notably does not
introduce additional inference-time cost in terms of latency or memory.
Furthermore, we provide a theoretical guarantee that WeaveRec can reduce the
upper bound of the expected error in the target domain. Extensive experiments
on single-source, multi-source, and cross-platform cross-domain recommendation
scenarios validate that WeaveRec effectively mitigates performance degradation
and consistently outperforms baseline approaches in real-world recommendation
tasks.

</details>


### [147] [ProfOlaf: Semi-Automated Tool for Systematic Literature Reviews](https://arxiv.org/abs/2510.26750)
*Martim Afonso,Nuno Saavedra,Bruno Lourenço,Alexandra Mendes,João Ferreira*

Main category: cs.IR

TL;DR: ProfOlaf is a semi-automated tool that streamlines systematic reviews by combining automation with human input.


<details>
  <summary>Details</summary>
Motivation: Systematic reviews are critical but labor-intensive and time-consuming, with existing tools only offering partial support.

Method: ProfOlaf supports iterative snowballing for article collection and uses large language models to assist in analyzing articles, extracting key topics, and answering queries.

Result: ProfOlaf enhances the efficiency, quality, and reproducibility of systematic reviews.

Conclusion: ProfOlaf streamlines systematic reviews while maintaining methodological rigor by combining automation with guided manual effort.

Abstract: Systematic reviews and mapping studies are critical for synthesizing
research, identifying gaps, and guiding future work, but they are often
labor-intensive and time-consuming. Existing tools provide partial support for
specific steps, leaving much of the process manual and error-prone. We present
ProfOlaf, a semi-automated tool designed to streamline systematic reviews while
maintaining methodological rigor. ProfOlaf supports iterative snowballing for
article collection with human-in-the-loop filtering and uses large language
models to assist in analyzing articles, extracting key topics, and answering
queries about the content of papers. By combining automation with guided manual
effort, ProfOlaf enhances the efficiency, quality, and reproducibility of
systematic reviews across research fields. A video describing and demonstrating
ProfOlaf is available at: https://youtu.be/4noUXfcmxsE

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [148] [A Practitioner's Guide to Kolmogorov-Arnold Networks](https://arxiv.org/abs/2510.25781)
*Amir Noorizadegan,Sifan Wang,Leevan Ling*

Main category: cs.LG

TL;DR: 综述论文，回顾了 Kolmogorov-Arnold Networks (KANs) 的最新进展，包括理论基础、架构变体和实际应用。


<details>
  <summary>Details</summary>
Motivation: KANs 作为传统 MLP 的替代方案，具有更强的表达能力和可解释性。

Method: 收集和分类了大量的开源实现，并对 KANs 和 MLPs 之间的概念差距进行了弥合，重点关注基函数的作用，并对各种选择进行了分析。

Result: 对 KANs 的最新进展进行了分类，包括提高准确性、效率和正则化的技术。提供了一个“选择你的 KAN”指南。

Conclusion: 总结了 KANs 的研究现状，并指出了当前的研究差距。

Abstract: Kolmogorov-Arnold Networks (KANs) have recently emerged as a promising
alternative to traditional Multilayer Perceptrons (MLPs), inspired by the
Kolmogorov-Arnold representation theorem. Unlike MLPs, which use fixed
activation functions on nodes, KANs employ learnable univariate basis functions
on edges, offering enhanced expressivity and interpretability. This review
provides a systematic and comprehensive overview of the rapidly expanding KAN
landscape, moving beyond simple performance comparisons to offer a structured
synthesis of theoretical foundations, architectural variants, and practical
implementation strategies. By collecting and categorizing a vast array of
open-source implementations, we map the vibrant ecosystem supporting KAN
development. We begin by bridging the conceptual gap between KANs and MLPs,
establishing their formal equivalence and highlighting the superior parameter
efficiency of the KAN formulation. A central theme of our review is the
critical role of the basis function; we survey a wide array of choices,
including B-splines, Chebyshev and Jacobi polynomials, ReLU compositions,
Gaussian RBFs, and Fourier series, and analyze their respective trade-offs in
terms of smoothness, locality, and computational cost. We then categorize
recent advancements into a clear roadmap, covering techniques for improving
accuracy, efficiency, and regularization. Key topics include physics-informed
loss design, adaptive sampling, domain decomposition, hybrid architectures, and
specialized methods for handling discontinuities. Finally, we provide a
practical "Choose-Your-KAN" guide to help practitioners select appropriate
architectures, and we conclude by identifying current research gaps. The
associated GitHub repository https://github.com/AmirNoori68/kan-review
complements this paper and serves as a structured reference for ongoing KAN
research.

</details>


### [149] [HiMAE: Hierarchical Masked Autoencoders Discover Resolution-Specific Structure in Wearable Time Series](https://arxiv.org/abs/2510.25785)
*Simon A. Lee,Cyrus Tanade,Hao Zhou,Juhyeon Lee,Megha Thukral,Minji Han,Rachel Choi,Md Sazzad Hissain Khan,Baiying Lu,Migyeong Gwak,Mehrab Bin Morshed,Viswam Nathan,Md Mahbubur Rahman,Li Zhu,Subramaniam Venkatraman,Sharanya Arcot Desai*

Main category: cs.LG

TL;DR: 提出了一种新的自监督学习框架HiMAE，用于处理可穿戴传感器的时间序列数据。


<details>
  <summary>Details</summary>
Motivation: 可穿戴传感器提供大量的生理时间序列数据，但其预测效用的原理尚不清楚。作者假设时间分辨率是表征学习的一个基本轴，不同的临床和行为结果依赖于不同尺度上的结构。

Method: 结合了掩码自动编码器与分层卷积编码器-解码器。

Result: 在分类、回归和生成基准测试中，HiMAE始终优于最先进的基础模型，同时体积更小。HiMAE足够紧凑，可以在手表上完全运行，在智能手表CPU上实现亚毫秒级的推理。

Conclusion: HiMAE既是一种有效的自监督学习方法，也是一种用于发现可穿戴健康中尺度敏感结构的工具。

Abstract: Wearable sensors provide abundant physiological time series, yet the
principles governing their predictive utility remain unclear. We hypothesize
that temporal resolution is a fundamental axis of representation learning, with
different clinical and behavioral outcomes relying on structure at distinct
scales. To test this resolution hypothesis, we introduce HiMAE (Hierarchical
Masked Autoencoder), a self supervised framework that combines masked
autoencoding with a hierarchical convolutional encoder decoder. HiMAE produces
multi resolution embeddings that enable systematic evaluation of which temporal
scales carry predictive signal, transforming resolution from a hyperparameter
into a probe for interpretability. Across classification, regression, and
generative benchmarks, HiMAE consistently outperforms state of the art
foundation models that collapse scale, while being orders of magnitude smaller.
HiMAE is an efficient representation learner compact enough to run entirely on
watch, achieving sub millisecond inference on smartwatch class CPUs for true
edge inference. Together, these contributions position HiMAE as both an
efficient self supervised learning method and a discovery tool for scale
sensitive structure in wearable health.

</details>


### [150] [SHA-256 Infused Embedding-Driven Generative Modeling of High-Energy Molecules in Low-Data Regimes](https://arxiv.org/abs/2510.25788)
*Siddharth Verma,Alankar Alankar*

Main category: cs.LG

TL;DR: 提出了一种结合LSTM和GNN的新型高能分子生成方法。


<details>
  <summary>Details</summary>
Motivation: 高能材料在推进和国防领域至关重要，但其发现受到实验数据和测试设施的限制。

Method: 结合长短期记忆（LSTM）网络进行分子生成和注意力图神经网络（GNN）进行性质预测。提出了一种变革性的嵌入空间构建策略，该策略将固定的SHA-256嵌入与部分可训练的表示相结合。

Result: 生成器在没有预训练的情况下实现了67.5%的有效性和37.5%的新颖性。相对于训练集，生成的库表现出0.214的平均Tanimoto系数，表明该框架能够生成多样化的化学空间。我们鉴定了37种新的超级炸药，其预测爆炸速度高于9 km/s。

Conclusion: 该框架成功生成了具有潜力的新型高能分子。

Abstract: High-energy materials (HEMs) are critical for propulsion and defense domains,
yet their discovery remains constrained by experimental data and restricted
access to testing facilities. This work presents a novel approach toward
high-energy molecules by combining Long Short-Term Memory (LSTM) networks for
molecular generation and Attentive Graph Neural Networks (GNN) for property
predictions. We propose a transformative embedding space construction strategy
that integrates fixed SHA-256 embeddings with partially trainable
representations. Unlike conventional regularization techniques, this changes
the representational basis itself, reshaping the molecular input space before
learning begins. Without recourse to pretraining, the generator achieves 67.5%
validity and 37.5% novelty. The generated library exhibits a mean Tanimoto
coefficient of 0.214 relative to training set signifying the ability of
framework to generate a diverse chemical space. We identified 37 new super
explosives higher than 9 km/s predicted detonation velocity.

</details>


### [151] [The Kinetics of Reasoning: How Chain-of-Thought Shapes Learning in Transformers?](https://arxiv.org/abs/2510.25791)
*Zihan Pengmei,Costas Mavromatis,Zhengyuan Shen,Yunyi Zhang,Vassilis N. Ioannidis,Huzefa Rangwala*

Main category: cs.LG

TL;DR: 研究了CoT监督如何提高transformer性能的机制，发现其益处依赖于任务复杂性。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解模型如何学习遵循CoT并从中受益的机制。

Method: 通过在具有可调算法复杂性和可控数据组成的符号推理任务上预训练transformer，研究它们的泛化能力。模型在两种设置下训练：(i)只生成最终答案，和(ii)在回答前发出显式CoT轨迹。

Result: CoT通常提高任务性能，但其益处依赖于任务复杂性。早期训练中，模型经常在跳过或矛盾CoT步骤的同时产生正确答案，之后才会将其推理轨迹与答案对齐。CoT加速了泛化，但不能克服具有更高算法复杂性的任务。

Conclusion: CoT改变了transformer的内部计算机制。

Abstract: Chain-of-thought (CoT) supervision can substantially improve transformer
performance, yet the mechanisms by which models learn to follow and benefit
from CoT remain poorly understood. We investigate these learning dynamics
through the lens of grokking by pretraining transformers on symbolic reasoning
tasks with tunable algorithmic complexity and controllable data composition to
study their generalization. Models were trained under two settings: (i)
producing only final answers, and (ii) emitting explicit CoT traces before
answering. Our results show that while CoT generally improves task performance,
its benefits depend on task complexity. To quantify these effects, we model the
accuracy of the logarithmic training steps with a three-parameter logistic
curve, revealing how the learning speed and shape vary with task complexity,
data distribution, and the presence of CoT supervision. We also uncover a
transient trace unfaithfulness phase: early in training, models often produce
correct answers while skipping or contradicting CoT steps, before later
aligning their reasoning traces with answers. Empirically, we (1) demonstrate
that CoT accelerates generalization but does not overcome tasks with higher
algorithmic complexity, such as finding list intersections; (2) introduce a
kinetic modeling framework for understanding transformer learning; (3)
characterize trace faithfulness as a dynamic property that emerges over
training; and (4) show CoT alters internal transformer computation
mechanistically.

</details>


### [152] [Optimal Information Combining for Multi-Agent Systems Using Adaptive Bias Learning](https://arxiv.org/abs/2510.25793)
*Siavash M. Alamouti,Fay Arjomandi*

Main category: cs.LG

TL;DR: 该论文研究了多智能体系统中由于环境条件变化而产生的系统性偏差导致的性能下降问题，并提出了自适应偏差学习和最优组合（ABLOC）算法。


<details>
  <summary>Details</summary>
Motivation: 当前方法忽略这些偏差或需要昂贵的校准程序，导致次优决策和实际应用中的困难。本文旨在探讨何时可以学习和纠正这些未知偏差以恢复接近最优的性能，以及何时这种学习是徒劳的。

Method: 论文开发了一个理论框架，将偏差分解为可学习的系统成分和不可约的随机成分，引入了可学习率的概念，并提出了ABLOC算法。

Result: 实验验证表明，具有高可学习率的系统可以恢复显著的性能提升（在示例中实现了理论最大改进的40%-70%），而具有低可学习率的系统几乎没有收益。

Conclusion: 可学习率决定了偏差学习对于给定系统是否有价值，并为系统设计者提供了关于何时投资于偏差学习与更简单方法的定量指导。

Abstract: Modern multi-agent systems ranging from sensor networks monitoring critical
infrastructure to crowdsourcing platforms aggregating human intelligence can
suffer significant performance degradation due to systematic biases that vary
with environmental conditions. Current approaches either ignore these biases,
leading to suboptimal decisions, or require expensive calibration procedures
that are often infeasible in practice. This performance gap has real
consequences: inaccurate environmental monitoring, unreliable financial
predictions, and flawed aggregation of human judgments. This paper addresses
the fundamental question: when can we learn and correct for these unknown
biases to recover near-optimal performance, and when is such learning futile?
We develop a theoretical framework that decomposes biases into learnable
systematic components and irreducible stochastic components, introducing the
concept of learnability ratio as the fraction of bias variance predictable from
observable covariates. This ratio determines whether bias learning is
worthwhile for a given system. We prove that the achievable performance
improvement is fundamentally bounded by this learnability ratio, providing
system designers with quantitative guidance on when to invest in bias learning
versus simpler approaches. We present the Adaptive Bias Learning and Optimal
Combining (ABLOC) algorithm, which iteratively learns bias-correcting
transformations while optimizing combination weights through closedform
solutions, guaranteeing convergence to these theoretical bounds. Experimental
validation demonstrates that systems with high learnability ratios can recover
significant performance (we achieved 40%-70% of theoretical maximum improvement
in our examples), while those with low learnability show minimal benefit,
validating our diagnostic criteria for practical deployment decisions.

</details>


### [153] [Non-myopic Matching and Rebalancing in Large-Scale On-Demand Ride-Pooling Systems Using Simulation-Informed Reinforcement Learning](https://arxiv.org/abs/2510.25796)
*Farnoosh Namdarpour,Joseph Y. J. Chow*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于模拟的强化学习方法，用于解决拼车服务中的短视决策问题，并通过在纽约出租车请求数据上的实验验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前拼车服务存在短视决策问题，忽略了调度决策的长期影响。

Method: 该研究扩展了Xu et al. (2018) 的学习和规划框架，将其从网约车扩展到拼车，通过在学习机制中嵌入拼车模拟来实现非短视决策。此外，还提出了一种用于重新平衡空闲车辆的补充策略。

Result: 实验结果表明，非短视匹配策略可以将服务率提高 8.4%，同时减少乘客的车内和等待时间。此外，与短视策略相比，该策略可以在保持相同性能水平的同时，将车队规模减少 25% 以上。将重新平衡操作纳入框架后，与仅使用该框架进行匹配决策相比，等待时间减少了 27.3%，车内时间减少了 12.5%，服务率提高了 15.1%。

Conclusion: 该研究提出的基于模拟的强化学习方法可以有效解决拼车服务中的短视决策问题，提高服务效率并降低成本。

Abstract: Ride-pooling, also known as ride-sharing, shared ride-hailing, or
microtransit, is a service wherein passengers share rides. This service can
reduce costs for both passengers and operators and reduce congestion and
environmental impacts. A key limitation, however, is its myopic
decision-making, which overlooks long-term effects of dispatch decisions. To
address this, we propose a simulation-informed reinforcement learning (RL)
approach. While RL has been widely studied in the context of ride-hailing
systems, its application in ride-pooling systems has been less explored. In
this study, we extend the learning and planning framework of Xu et al. (2018)
from ride-hailing to ride-pooling by embedding a ride-pooling simulation within
the learning mechanism to enable non-myopic decision-making. In addition, we
propose a complementary policy for rebalancing idle vehicles. By employing
n-step temporal difference learning on simulated experiences, we derive
spatiotemporal state values and subsequently evaluate the effectiveness of the
non-myopic policy using NYC taxi request data. Results demonstrate that the
non-myopic policy for matching can increase the service rate by up to 8.4%
versus a myopic policy while reducing both in-vehicle and wait times for
passengers. Furthermore, the proposed non-myopic policy can decrease fleet size
by over 25% compared to a myopic policy, while maintaining the same level of
performance, thereby offering significant cost savings for operators.
Incorporating rebalancing operations into the proposed framework cuts wait time
by up to 27.3%, in-vehicle time by 12.5%, and raises service rate by 15.1%
compared to using the framework for matching decisions alone at the cost of
increased vehicle minutes traveled per passenger.

</details>


### [154] [MemEIC: A Step Toward Continual and Compositional Knowledge Editing](https://arxiv.org/abs/2510.25798)
*Jin Seong,Jiyun Park,Wencke Liermann,Hongseok Choi,Yoonji Nam,Hyun Kim,Soojong Lim,Namhoon Lee*

Main category: cs.LG

TL;DR: MemEIC是一种用于大型视觉语言模型（LVLMs）中持续和组合知识编辑（CCKE）的新方法，它通过混合外部-内部编辑器、双重外部记忆和LoRA适配器来实现跨模态证据检索和参数更新，并通过脑启发式知识连接器整合信息。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑技术忽略了LVLMs的内在多模态性和知识更新的连续性，可能导致次优的编辑结果。

Method: MemEIC采用混合外部-内部编辑器，包含用于跨模态证据检索的双重外部记忆和用于解耦参数更新的双重LoRA适配器，以及一个脑启发式知识连接器。

Result: MemEIC在复杂的多模态问题上显著提高了性能，并有效地保留了先前的编辑。

Conclusion: MemEIC为LVLMs中的CCKE设置了一个新的基准。

Abstract: The dynamic nature of information necessitates continuously updating large
vision-language models (LVLMs). While recent knowledge editing techniques hint
at promising directions, they often focus on editing a single modality (vision
or language) in isolation. This prevalent practice neglects the inherent
multimodality of LVLMs and the continuous nature of knowledge updates,
potentially leading to suboptimal editing outcomes when considering the
interplay between modalities and the need for ongoing knowledge refinement. To
address these limitations, we propose MemEIC, a novel method for Continual and
Compositional Knowledge Editing (CCKE) in LVLMs. MemEIC enables compositional
editing of both visual and textual knowledge sequentially. Our approach employs
a hybrid external-internal editor featuring a dual external memory for
cross-modal evidence retrieval and dual LoRA adapters that facilitate
disentangled parameter updates for each modality. A key component is a
brain-inspired knowledge connector, activated selectively for compositional
reasoning, that integrates information across different modalities. Experiments
demonstrate that MemEIC significantly improves performance on complex
multimodal questions and effectively preserves prior edits, setting a new
benchmark for CCKE in LVLMs.

</details>


### [155] [FreIE: Low-Frequency Spectral Bias in Neural Networks for Time-Series Tasks](https://arxiv.org/abs/2510.25800)
*Jialong Sun,Xinpeng Ling,Jiaxuan Zou,Jiawen Kang,Kejia Zhang*

Main category: cs.LG

TL;DR: 这篇论文研究了多元时间序列预测中时间序列数据固有的自相关性带来的挑战。提出了 FreLE 算法，通过显式和隐式频率正则化来增强模型泛化能力，这是一个即插即用的模型损失函数单元。


<details>
  <summary>Details</summary>
Motivation: 为了统一对长期时间序列预测中谱偏置现象的理解，作者进行了广泛的实验，以测量现有主流模型中的谱偏置。

Method: 通过大量实验测量主流模型中的谱偏置，并提出 FreLE 算法，通过显式和隐式频率正则化来增强模型泛化能力。

Result: 研究发现几乎所有模型都表现出谱偏置现象。大量实验证明了 FreLE 的优越性能。

Conclusion: FreLE 算法能够有效缓解谱偏置的影响，提高模型在多元时间序列预测中的性能。

Abstract: The inherent autocorrelation of time series data presents an ongoing
challenge to multivariate time series prediction. Recently, a widely adopted
approach has been the incorporation of frequency domain information to assist
in long-term prediction tasks. Many researchers have independently observed the
spectral bias phenomenon in neural networks, where models tend to fit
low-frequency signals before high-frequency ones. However, these observations
have often been attributed to the specific architectures designed by the
researchers, rather than recognizing the phenomenon as a universal
characteristic across models. To unify the understanding of the spectral bias
phenomenon in long-term time series prediction, we conducted extensive
empirical experiments to measure spectral bias in existing mainstream models.
Our findings reveal that virtually all models exhibit this phenomenon. To
mitigate the impact of spectral bias, we propose the FreLE (Frequency Loss
Enhancement) algorithm, which enhances model generalization through both
explicit and implicit frequency regularization. This is a plug-and-play model
loss function unit. A large number of experiments have proven the superior
performance of FreLE. Code is available at
https://github.com/Chenxing-Xuan/FreLE.

</details>


### [156] [Metis-SPECS: Decoupling Multimodal Learning via Self-distilled Preference-based Cold Start](https://arxiv.org/abs/2510.25801)
*Kun Chen,Peng Shi,Haibo Qiu,Zhixiong Zeng,Siqi Yang,Wenji Mao,Lin Ma*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的冷启动框架SPECS，用于解决视觉语言模型中强化学习的指令风格过拟合问题。SPECS通过自蒸馏生成偏好数据，并进行基于偏好的训练，从而解耦多模态学习，提高泛化能力和下游强化学习的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于SFT的冷启动方法存在指令风格过拟合问题，削弱了模型在分布外的泛化能力，并最终影响下游强化学习。这篇论文旨在解决这个问题。

Method: 这篇论文提出了SPECS框架，该框架包含三个步骤：(1) 通过自蒸馏生成内省偏好数据对；(2) 执行基于偏好的训练，学习浅层的、可转移的表面形式标准（格式、结构、风格）；(3) 将学习到的模型移交给具有可验证奖励的强化学习，以获得更深层次的推理结果。

Result: SPECS框架在多个多模态基准测试中取得了显著的性能提升，在MEGA-Bench上提高了4.1%，在MathVista上提高了12.2%。

Conclusion: SPECS框架能够减少分布内的“停滞”现象，改善探索，稳定训练，并提高性能上限。

Abstract: Reinforcement learning (RL) with verifiable rewards has recently catalyzed a
wave of "MLLM-r1" approaches that bring RL to vision language models. Most
representative paradigms begin with a cold start, typically employing
supervised fine-tuning (SFT), to initialize the policy before RL. However,
SFT-based cold start adopts the reasoning paradigm intertwined with task
solution and output format, which may induce instruction-style overfitting,
weakens out-of-distribution generalization, and ultimately affects downstream
RL. We revisit the cold start along two views, its training method and data
construction, and introduce the Generalization Factor (GF) coefficient to
quantify the generalization capability under different methods. Our empirical
study finds that preference-based training methods (e.g. DPO) generalizes
better than SFT-based methods in cold start. Motivated by this, we propose
SPECS-a Self-distilled, Preference-based Cold Start framework that decouples
multimodal learning: (1) generates introspective preference data pairs via
self-distillation, avoiding reliance on larger teachers or manual annotation;
(2) performs preference-based training to learn, focusing on shallow,
transferable surface-form criteria (format, structure, style) rather than
memorizing content; and (3) hands off to RL with verifiable rewards for deep
reasoning results. Experimental results across multiple multimodal benchmarks
show that our decoupling learning framework yields consistent performance gains
over strong baselines, improving MEGA-Bench by 4.1% and MathVista by 12.2%.
Additional experiments indicate that SPECS contributes to reducing
in-distribution "stuckness," improving exploration, stabilizing training, and
raising the performance ceiling.

</details>


### [157] [Mixture-of-Experts Operator Transformer for Large-Scale PDE Pre-Training](https://arxiv.org/abs/2510.25803)
*Hong Wang,Haiyang Xin,Jie Wang,Xuanze Yang,Fei Zha,Huanshuo Dong,Yan Jiang*

Main category: cs.LG

TL;DR: 提出了一种新的混合专家预训练算子Transformer（MoE-POT），它是一种稀疏激活架构，可以有效扩展参数，同时控制推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子求解PDE问题时，由于方程类型的PDE数据集的异构性，混合训练误差较高，且密集预训练模型会产生显著的推理成本。

Method: 采用分层路由门控网络，在推理过程中从16个专家网络中动态选择4个路由专家，同时集成2个共享专家，以捕获PDE的共同属性并减少路由专家之间的冗余。最终输出计算为所有激活专家的结果的加权平均值。

Result: 具有90M激活参数的模型与具有120M激活参数的现有模型相比，零样本误差最多可减少40%。

Conclusion: 数据集类型可以从路由门控网络决策中推断出来，这验证了MoE架构的合理性和有效性。

Abstract: Pre-training has proven effective in addressing data scarcity and performance
limitations in solving PDE problems with neural operators. However, challenges
remain due to the heterogeneity of PDE datasets in equation types, which leads
to high errors in mixed training. Additionally, dense pre-training models that
scale parameters by increasing network width or depth incur significant
inference costs. To tackle these challenges, we propose a novel
Mixture-of-Experts Pre-training Operator Transformer (MoE-POT), a
sparse-activated architecture that scales parameters efficiently while
controlling inference costs. Specifically, our model adopts a layer-wise
router-gating network to dynamically select 4 routed experts from 16 expert
networks during inference, enabling the model to focus on equation-specific
features. Meanwhile, we also integrate 2 shared experts, aiming to capture
common properties of PDE and reduce redundancy among routed experts. The final
output is computed as the weighted average of the results from all activated
experts. We pre-train models with parameters from 30M to 0.5B on 6 public PDE
datasets. Our model with 90M activated parameters achieves up to a 40%
reduction in zero-shot error compared with existing models with 120M activated
parameters. Additionally, we conduct interpretability analysis, showing that
dataset types can be inferred from router-gating network decisions, which
validates the rationality and effectiveness of the MoE architecture.

</details>


### [158] [PRESTO: Preimage-Informed Instruction Optimization for Prompting Black-Box LLMs](https://arxiv.org/abs/2510.25808)
*Jaewon Chu,Seunghun Lee,Hyunwoo J. Kim*

Main category: cs.LG

TL;DR: 提出了一种名为PRESTO的框架，用于优化黑盒LLM的指令，该框架利用软提示的preimage结构来提高优化效率。


<details>
  <summary>Details</summary>
Motivation: 为了优化黑盒LLM的指令，现有方法使用白盒LLM从优化的软提示生成候选指令，但白盒LLM经常将不同的软提示映射到相同的指令，导致查询冗余。之前的研究认为这种多对一的映射阻碍了优化效率，但本文将其解释为可以加速优化的有用先验知识。

Method: PRESTO框架包含三个关键组件：(1)分数共享，与preimage中的所有软提示共享评估分数；(2)基于preimage的初始化，选择最大化搜索空间覆盖的初始数据点；(3)分数一致性正则化，强制每个preimage内的预测一致性。

Result: 通过利用preimage，PRESTO在相同的查询预算下有效地获得了14倍以上的评分数据，从而实现了更高效的优化。在33个指令优化任务上的实验结果表明了PRESTO的优越性能。

Conclusion: PRESTO框架通过利用软提示的preimage结构，显著提高了黑盒LLM指令优化的效率。

Abstract: Large language models (LLMs) have achieved remarkable success across diverse
domains, due to their strong instruction-following capabilities. This has led
to increasing interest in optimizing instructions for black-box LLMs, whose
internal parameters are inaccessible but widely used due to their strong
performance. To optimize instructions for black-box LLMs, recent methods employ
white-box LLMs to generate candidate instructions from optimized soft prompts.
However, white-box LLMs often map different soft prompts to the same
instruction, leading to redundant queries. While previous studies regarded this
many-to-one mapping as a structure that hinders optimization efficiency, we
reinterpret it as a useful prior knowledge that can accelerate the
optimization. To this end, we introduce PREimage-informed inSTruction
Optimization (PRESTO), a novel framework that leverages the preimage structure
of soft prompts for efficient optimization. PRESTO consists of three key
components: (1) score sharing, which shares the evaluation score with all soft
prompts in a preimage; (2) preimage-based initialization, which selects initial
data points that maximize search space coverage using preimage information; and
(3) score consistency regularization, which enforces prediction consistency
within each preimage. By leveraging preimages, PRESTO achieves the effect of
effectively obtaining 14 times more scored data under the same query budget,
resulting in more efficient optimization. Experimental results on 33
instruction optimization tasks demonstrate the superior performance of PRESTO.
Code is available at https://github.com/mlvlab/PRESTO

</details>


### [159] [ScaleDiff: Higher-Resolution Image Synthesis via Efficient and Model-Agnostic Diffusion](https://arxiv.org/abs/2510.25818)
*Sungho Koh,SeungJu Cha,Hyunwoo Oh,Kwanyoung Lee,Dong-Jin Kim*

Main category: cs.LG

TL;DR: 提出ScaleDiff，一个无需额外训练的、模型无关的框架，用于扩展预训练扩散模型的分辨率。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型在生成超出其训练分辨率的图像时性能下降；现有的无训练方法计算量大或与最新的Diffusion Transformer模型不兼容。

Method: 提出Neighborhood Patch Attention (NPA)，一种减少自注意力层计算冗余的机制；将NPA集成到SDEdit流程中，并引入Latent Frequency Mixing (LFM) 以更好地生成精细细节；应用结构引导以增强去噪过程中的全局结构。

Result: ScaleDiff在图像质量和推理速度方面，在U-Net和Diffusion Transformer架构上，都实现了最先进的无训练方法性能。

Conclusion: ScaleDiff是一个高效且有效的框架，可以在不进行额外训练的情况下扩展预训练扩散模型的分辨率。

Abstract: Text-to-image diffusion models often exhibit degraded performance when
generating images beyond their training resolution. Recent training-free
methods can mitigate this limitation, but they often require substantial
computation or are incompatible with recent Diffusion Transformer models. In
this paper, we propose ScaleDiff, a model-agnostic and highly efficient
framework for extending the resolution of pretrained diffusion models without
any additional training. A core component of our framework is Neighborhood
Patch Attention (NPA), an efficient mechanism that reduces computational
redundancy in the self-attention layer with non-overlapping patches. We
integrate NPA into an SDEdit pipeline and introduce Latent Frequency Mixing
(LFM) to better generate fine details. Furthermore, we apply Structure Guidance
to enhance global structure during the denoising process. Experimental results
demonstrate that ScaleDiff achieves state-of-the-art performance among
training-free methods in terms of both image quality and inference speed on
both U-Net and Diffusion Transformer architectures.

</details>


### [160] [MedVLSynther: Synthesizing High-Quality Visual Question Answering from Medical Documents with Generator-Verifier LMMs](https://arxiv.org/abs/2510.25867)
*Xiaoke Huang,Ningsen Wang,Hui Liu,Xianfeng Tang,Yuyin Zhou*

Main category: cs.LG

TL;DR: 这篇论文介绍了一种名为MedVLSynther的框架，用于从开放的生物医学文献中合成高质量的多项选择题VQA项目。该框架通过生成器-验证器流程，并利用强化学习训练开放权重的LMMs，在多个医学VQA基准测试中提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 当前医学VQA系统缺乏大量、开放、高质量的语料库，阻碍了通用医学VQA系统的训练。

Method: 论文提出了一种rubric-guided的生成器-验证器框架，该框架以图表、标题和文本引用为条件，直接从开放的生物医学文献中合成高质量的多项选择题VQA项目。该框架包含一个生成器，用于生成自包含的题干和平行的、互斥的选项，以及一个多阶段验证器，用于执行关键门控（自包含、单正确答案、临床有效性、图像-文本一致性）。

Result: 通过将该流程应用于PubMed Central，生成了MedSynVQA数据集，包含13,087个经过审核的问题，涵盖14,803张图像，跨越13种成像方式和28个解剖区域。使用可验证的奖励，通过强化学习训练开放权重LMMs，提高了六个医学VQA基准测试的准确性，平均值达到55.85 (3B)和58.15 (7B)。

Conclusion: MedVLSynther完全基于开放文献和开放权重模型，为可扩展的医学VQA训练数据提供了一条可审核、可重现且保护隐私的路径。

Abstract: Large Multimodal Models (LMMs) are increasingly capable of answering medical
questions that require joint reasoning over images and text, yet training
general medical VQA systems is impeded by the lack of large, openly usable,
high-quality corpora. We present MedVLSynther, a rubric-guided
generator-verifier framework that synthesizes high-quality multiple-choice VQA
items directly from open biomedical literature by conditioning on figures,
captions, and in-text references. The generator produces self-contained stems
and parallel, mutually exclusive options under a machine-checkable JSON schema;
a multi-stage verifier enforces essential gates (self-containment, single
correct answer, clinical validity, image-text consistency), awards fine-grained
positive points, and penalizes common failure modes before acceptance. Applying
this pipeline to PubMed Central yields MedSynVQA: 13,087 audited questions over
14,803 images spanning 13 imaging modalities and 28 anatomical regions.
Training open-weight LMMs with reinforcement learning using verifiable rewards
improves accuracy across six medical VQA benchmarks, achieving averages of
55.85 (3B) and 58.15 (7B), with up to 77.57 on VQA-RAD and 67.76 on PathVQA,
outperforming strong medical LMMs. A Ablations verify that both generation and
verification are necessary and that more verified data consistently helps, and
a targeted contamination analysis detects no leakage from evaluation suites. By
operating entirely on open literature and open-weight models, MedVLSynther
offers an auditable, reproducible, and privacy-preserving path to scalable
medical VQA training data.

</details>


### [161] [$π_\texttt{RL}$: Online RL Fine-tuning for Flow-based Vision-Language-Action Models](https://arxiv.org/abs/2510.25889)
*Kang Chen,Zhihao Liu,Tonghe Zhang,Zhen Guo,Si Xu,Hao Lin,Hongzhi Zang,Quanlu Zhang,Zhaofei Yu,Guoliang Fan,Tiejun Huang,Yu Wang,Chao Yu*

Main category: cs.LG

TL;DR: 提出了一个名为π_RL的开源框架，用于在并行仿真中训练基于流的VLA模型，以解决动作对数似然难以处理的问题。


<details>
  <summary>Details</summary>
Motivation: 基于视觉-语言-动作（VLA）的模型能够理解和执行来自多模态输入的复杂任务。虽然最近的工作探索使用强化学习（RL）来自动执行在缩放监督微调（SFT）中费力的数据收集过程，但是由于来自迭代去噪的难以处理的动作对数似然，将大规模RL应用于基于流的VLA仍然具有挑战性。

Method: π_RL实现了两种RL算法：（1）Flow-Noise将去噪过程建模为离散时间MDP，具有可学习的噪声网络，用于精确的对数似然计算。 (2) Flow-SDE集成了去噪与agent-environment交互，构建了一个两层MDP，该MDP采用ODE到SDE的转换来实现高效的RL探索。

Result: 在LIBERO上，π_RL将few-shot SFT模型π_0和π_0.5分别从57.6%提高到97.6%，从77.1%提高到98.3%。在ManiSkill中，π_RL在320个并行环境中进行训练，在4352个抓取和放置任务中将π_0从41.6%提高到85.7%，将π_0.5从40.0%提高到84.8%，证明了在异构仿真下可扩展的多任务RL。

Conclusion: π_RL实现了显著的性能提升和比SFT模型更强的泛化能力，验证了在线RL对基于流的VLA的有效性。

Abstract: Vision-Language-Action (VLA) models enable robots to understand and perform
complex tasks from multimodal input. Although recent work explores using
reinforcement learning (RL) to automate the laborious data collection process
in scaling supervised fine-tuning (SFT), applying large-scale RL to flow-based
VLAs (e.g., $\pi_0$, $\pi_{0.5}$) remains challenging due to intractable action
log-likelihoods from iterative denoising.
  We address this challenge with $\pi_{\text{RL}}$, an open-source framework
for training flow-based VLAs in parallel simulation. $\pi_{\text{RL}}$
implements two RL algorithms: (1) {Flow-Noise} models the denoising process as
a discrete-time MDP with a learnable noise network for exact log-likelihood
computation. (2) {Flow-SDE} integrates denoising with agent-environment
interaction, formulating a two-layer MDP that employs ODE-to-SDE conversion for
efficient RL exploration.
  We evaluate $\pi_{\text{RL}}$ on LIBERO and ManiSkill benchmarks. On LIBERO,
$\pi_{\text{RL}}$ boosts few-shot SFT models $\pi_0$ and $\pi_{0.5}$ from 57.6%
to 97.6% and from 77.1% to 98.3%, respectively. In ManiSkill, we train
$\pi_{\text{RL}}$ in 320 parallel environments, improving $\pi_0$ from 41.6% to
85.7% and $\pi_{0.5}$ from 40.0% to 84.8% across 4352 pick-and-place tasks,
demonstrating scalable multitask RL under heterogeneous simulation.
  Overall, $\pi_{\text{RL}}$ achieves significant performance gains and
stronger generalization over SFT-models, validating the effectiveness of online
RL for flow-based VLAs.

</details>


### [162] [Topology-Aware Active Learning on Graphs](https://arxiv.org/abs/2510.25892)
*Harris Hardiman-Mostow,Jack Mauro,Adrien Weihs,Andrea L. Bertozzi*

Main category: cs.LG

TL;DR: 提出了一种图拓扑主动学习方法，它直接针对在稀缺标签预算下探索与利用这一核心挑战。


<details>
  <summary>Details</summary>
Motivation: 为了指导探索，我们引入了一种基于平衡 Forman 曲率 (BFC) 的 coreset 构造算法，该算法选择反映图的聚类结构的代表性初始标签。

Method: 该方法包括一个数据驱动的停止准则，该准则指示图何时被充分探索。我们进一步使用 BFC 来动态触发主动学习例程中从探索到利用的转变，从而取代手动调整的启发式方法。为了提高利用率，我们引入了一种局部图重连策略，该策略有效地结合了标记节点周围的多尺度信息，从而增强了标签传播，同时保持了稀疏性。

Result: 在基准分类任务上的实验表明，我们的方法在低标签率下始终优于现有的基于图的半监督基线。

Conclusion: 提出的方法优于现有的图方法

Abstract: We propose a graph-topological approach to active learning that directly
targets the core challenge of exploration versus exploitation under scarce
label budgets. To guide exploration, we introduce a coreset construction
algorithm based on Balanced Forman Curvature (BFC), which selects
representative initial labels that reflect the graph's cluster structure. This
method includes a data-driven stopping criterion that signals when the graph
has been sufficiently explored. We further use BFC to dynamically trigger the
shift from exploration to exploitation within active learning routines,
replacing hand-tuned heuristics. To improve exploitation, we introduce a
localized graph rewiring strategy that efficiently incorporates multiscale
information around labeled nodes, enhancing label propagation while preserving
sparsity. Experiments on benchmark classification tasks show that our methods
consistently outperform existing graph-based semi-supervised baselines at low
label rates.

</details>


### [163] [Transferring Causal Effects using Proxies](https://arxiv.org/abs/2510.25924)
*Manuel Iglesias-Alonso,Felix Schur,Julius von Kügelgen,Jonas Peters*

Main category: cs.LG

TL;DR: 本文研究了多域场景中因果效应的估计问题，其中因果效应受到未观察到的混杂因素的影响，并且在不同领域之间会发生变化。


<details>
  <summary>Details</summary>
Motivation: 作者研究了在多域场景中估计因果效应的问题，其中因果效应受到未观察到的混杂因素的影响，并且在不同领域之间会发生变化。

Method: 作者提出了一种在目标领域估计因果效应的方法，假设只能观察到代理变量。还证明了可识别性，并引入了两种估计技术，证明了其一致性，并推导了置信区间。

Result: 通过仿真研究和实际案例（研究网站排名对消费者选择的因果效应）支持了理论结果。

Conclusion: 作者研究了多域场景中因果效应的估计问题，提出了估计方法，证明了其一致性，并提供了实验结果支持。

Abstract: We consider the problem of estimating a causal effect in a multi-domain
setting. The causal effect of interest is confounded by an unobserved
confounder and can change between the different domains. We assume that we have
access to a proxy of the hidden confounder and that all variables are discrete
or categorical. We propose methodology to estimate the causal effect in the
target domain, where we assume to observe only the proxy variable. Under these
conditions, we prove identifiability (even when treatment and response
variables are continuous). We introduce two estimation techniques, prove
consistency, and derive confidence intervals. The theoretical results are
supported by simulation studies and a real-world example studying the causal
effect of website rankings on consumer choices.

</details>


### [164] [Active Learning with Task-Driven Representations for Messy Pools](https://arxiv.org/abs/2510.25926)
*Kianoosh Ashouritaklimi,Tom Rainforth*

Main category: cs.LG

TL;DR: 本文提出了一种主动学习方法，该方法使用在主动学习过程中定期更新的任务驱动表示，以解决现有方法在处理混乱数据池时的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于固定的、无监督的池表示，这可能无法捕捉与任务相关的重要信息，从而影响其在处理混乱数据池时的有效性。

Method: 本文提出了两种学习任务驱动表示的具体策略：一种是直接学习半监督表示，另一种是基于对初始无监督表示进行监督微调。

Result: 实验结果表明，与使用无监督或预训练表示相比，这两种策略均能显著提高经验性能。

Conclusion: 任务驱动表示能够有效提高主动学习在混乱数据池中的性能。

Abstract: Active learning has the potential to be especially useful for messy,
uncurated pools where datapoints vary in relevance to the target task. However,
state-of-the-art approaches to this problem currently rely on using fixed,
unsupervised representations of the pool, focusing on modifying the acquisition
function instead. We show that this model setup can undermine their
effectiveness at dealing with messy pools, as such representations can fail to
capture important information relevant to the task. To address this, we propose
using task-driven representations that are periodically updated during the
active learning process using the previously collected labels. We introduce two
specific strategies for learning these representations, one based on directly
learning semi-supervised representations and the other based on supervised
fine-tuning of an initial unsupervised representation. We find that both
significantly improve empirical performance over using unsupervised or
pretrained representations.

</details>


### [165] [Robust GNN Watermarking via Implicit Perception of Topological Invariants](https://arxiv.org/abs/2510.25934)
*Jipeng Li,Yannning Shen*

Main category: cs.LG

TL;DR: InvGNN-WM: a novel GNN watermarking method that uses graph invariants for trigger-free, black-box verification.


<details>
  <summary>Details</summary>
Motivation: Existing GNN watermarks rely on backdoor triggers that are vulnerable to model edits and create ownership ambiguity.

Method: A lightweight head predicts normalized algebraic connectivity on an owner-private carrier set; a sign-sensitive decoder outputs bits, and a calibrated threshold controls the false-positive rate.

Result: InvGNN-WM matches clean accuracy and yields higher watermark accuracy than baselines, remaining robust under pruning, fine-tuning, and quantization. Knowledge distillation weakens the mark, but KD with a watermark loss restores it.

Conclusion: The paper provides guarantees for imperceptibility and robustness, and proves that exact watermark removal is NP-complete.

Abstract: Graph Neural Networks (GNNs) are valuable intellectual property, yet many
watermarks rely on backdoor triggers that break under common model edits and
create ownership ambiguity. We present InvGNN-WM, which ties ownership to a
model's implicit perception of a graph invariant, enabling trigger-free,
black-box verification with negligible task impact. A lightweight head predicts
normalized algebraic connectivity on an owner-private carrier set; a
sign-sensitive decoder outputs bits, and a calibrated threshold controls the
false-positive rate. Across diverse node and graph classification datasets and
backbones, InvGNN-WM matches clean accuracy while yielding higher watermark
accuracy than trigger- and compression-based baselines. It remains strong under
unstructured pruning, fine-tuning, and post-training quantization; plain
knowledge distillation (KD) weakens the mark, while KD with a watermark loss
(KD+WM) restores it. We provide guarantees for imperceptibility and robustness,
and we prove that exact removal is NP-complete.

</details>


### [166] [Modular Linear Tokenization (MLT)](https://arxiv.org/abs/2510.25952)
*Tcharlies Schmitz*

Main category: cs.LG

TL;DR: 这篇论文介绍了模块化线性 Tokenization (MLT)，这是一种可逆且确定性的技术，用于将高基数分类标识符编码为紧凑的数值向量。


<details>
  <summary>Details</summary>
Motivation: 与传统的哈希或 one-hot 编码不同，MLT 通过利用有限域上的模运算和可逆线性变换来保留双射映射。

Method: 该方法提供了对维度和计算可扩展性的显式控制，同时保持完全可逆性，即使对于数百万个标识符也是如此。

Result: 在 MovieLens 20M 数据集上的实验结果表明，MLT 实现了与监督嵌入相当的预测性能，同时需要的参数明显更少，训练成本更低。

Conclusion: MLT 是一种用于编码高基数分类标识符为紧凑数值向量的可逆和确定性技术，它具有参数少、训练成本低的优点，并且在 MovieLens 20M 数据集上实现了与监督嵌入相当的预测性能。

Abstract: This paper introduces Modular Linear Tokenization (MLT), a reversible and
deterministic technique for encoding high-cardinality categorical identifiers
into compact numerical vectors. Unlike traditional hashing or one-hot
encodings, MLT preserves bijective mappings by leveraging modular arithmetic
over finite fields and invertible linear transformations. The method offers
explicit control of dimensionality and computational scalability while
maintaining full reversibility, even for millions of identifiers. Experimental
results on the MovieLens 20M dataset show that MLT achieves comparable
predictive performance to supervised embeddings while requiring significantly
fewer parameters and lower training cost. An open-source implementation of MLT
is available on PyPI (https://pypi.org/project/light-mlt/) and GitHub
(https://github.com/tcharliesschmitz/light-mlt).

</details>


### [167] [Application and Validation of Geospatial Foundation Model Data for the Prediction of Health Facility Programmatic Outputs -- A Case Study in Malawi](https://arxiv.org/abs/2510.25954)
*Lynn Metz,Rachel Haggard,Michael Moszczynski,Samer Asbah,Chris Mwase,Patricia Khomani,Tyler Smith,Hannah Cooper,Annie Mwale,Arbaaz Muslim,Gautam Prasad,Mimi Sun,Tomer Shekel,Joydeep Paul,Anna Carter,Shravya Shetty,Dylan Green*

Main category: cs.LG

TL;DR: 本研究评估了三种地理空间基础模型（GeoFM）嵌入源在马拉维预测15项常规健康计划产出方面的预测性能，并将其效用与传统地理空间插值方法进行了比较。


<details>
  <summary>Details</summary>
Motivation: 低收入和中等收入国家(LMIC)的常规健康数据的可靠性通常受到报告延迟和覆盖不完整的限制，因此需要探索新的数据来源和分析方法。地理空间基础模型(GeoFM)通过将不同的空间、时间和行为数据合成为数学嵌入，提供了一个有希望的途径，可以有效地用于下游预测任务。

Method: 使用XGBoost模型对来自552个健康服务区域的数据（2021年1月-2023年5月）进行分析，使用R2评估性能，并使用80/20的训练和测试数据分割，在训练中使用5倍交叉验证。

Result: 虽然预测性能不一，但基于嵌入的方法在测试的15个指标中的13个（87%）上优于基线地统计方法。整合所有三个嵌入源的多GeoFM模型产生了最稳健的预测，人口密度、新增HIV病例和儿童疫苗接种等指标的平均5倍交叉验证R2值分别为0.63、0.57和0.47，测试集R2分别为0.64、0.68和0.55。

Conclusion: 将多个GeoFM来源整合在一起，是补充和加强受限的常规卫生信息系统的有效和有价值的工具。

Abstract: The reliability of routine health data in low and middle-income countries
(LMICs) is often constrained by reporting delays and incomplete coverage,
necessitating the exploration of novel data sources and analytics. Geospatial
Foundation Models (GeoFMs) offer a promising avenue by synthesizing diverse
spatial, temporal, and behavioral data into mathematical embeddings that can be
efficiently used for downstream prediction tasks. This study evaluated the
predictive performance of three GeoFM embedding sources - Google Population
Dynamics Foundation Model (PDFM), Google AlphaEarth (derived from satellite
imagery), and mobile phone call detail records (CDR) - for modeling 15 routine
health programmatic outputs in Malawi, and compared their utility to
traditional geospatial interpolation methods. We used XGBoost models on data
from 552 health catchment areas (January 2021-May 2023), assessing performance
with R2, and using an 80/20 training and test data split with 5-fold
cross-validation used in training. While predictive performance was mixed, the
embedding-based approaches improved upon baseline geostatistical methods in 13
of 15 (87%) indicators tested. A Multi-GeoFM model integrating all three
embedding sources produced the most robust predictions, achieving average
5-fold cross validated R2 values for indicators like population density (0.63),
new HIV cases (0.57), and child vaccinations (0.47) and test set R2 of 0.64,
0.68, and 0.55, respectively. Prediction was poor for prediction targets with
low primary data availability, such as TB and malnutrition cases. These results
demonstrate that GeoFM embeddings imbue a modest predictive improvement for
select health and demographic outcomes in an LMIC context. We conclude that the
integration of multiple GeoFM sources is an efficient and valuable tool for
supplementing and strengthening constrained routine health information systems.

</details>


### [168] [On the Dataless Training of Neural Networks](https://arxiv.org/abs/2510.25962)
*Alvaro Velasquez,Susmit Jha,Ismail R. Alkhouri*

Main category: cs.LG

TL;DR: 这篇论文调查了在无训练数据情况下使用神经网络进行优化的研究。


<details>
  <summary>Details</summary>
Motivation: 动机源于两个关键因素：(i) 数据驱动的学习方法仍不发达，尚未表现出强大的结果；(ii) 训练数据的可用性受到固有限制。

Method: 通过使用全连接（或 MLP）、卷积、图和二次神经网络重新参数化问题，研究了神经网络架构在优化中的无数据应用。

Result: MLP 已经被用于解决线性程序，并且由于其在各种应用中，包括基于组合优化、逆问题和偏微分方程的应用中，有希望的结果，最近受到了越来越多的关注。

Conclusion: 本文定义了无数据设置，并根据问题实例（由单个数据定义）如何编码到神经网络上，将其分为两种变体：(i) 与架构无关的方法和 (ii) 特定于架构的方法。此外，我们讨论了无数据神经网络 (dNN) 设置与零样本学习、单样本学习、优化提升和过度参数化等相关概念之间的异同，并澄清了它们之间的区别。

Abstract: This paper surveys studies on the use of neural networks for optimization in
the training-data-free setting. Specifically, we examine the dataless
application of neural network architectures in optimization by
re-parameterizing problems using fully connected (or MLP), convolutional,
graph, and quadratic neural networks. Although MLPs have been used to solve
linear programs a few decades ago, this approach has recently gained increasing
attention due to its promising results across diverse applications, including
those based on combinatorial optimization, inverse problems, and partial
differential equations. The motivation for this setting stems from two key
(possibly over-lapping) factors: (i) data-driven learning approaches are still
underdeveloped and have yet to demonstrate strong results, as seen in
combinatorial optimization, and (ii) the availability of training data is
inherently limited, such as in medical image reconstruction and other
scientific applications. In this paper, we define the dataless setting and
categorize it into two variants based on how a problem instance -- defined by a
single datum -- is encoded onto the neural network: (i) architecture-agnostic
methods and (ii) architecture-specific methods. Additionally, we discuss
similarities and clarify distinctions between the dataless neural network (dNN)
settings and related concepts such as zero-shot learning, one-shot learning,
lifting in optimization, and over-parameterization.

</details>


### [169] [Contrastive Predictive Coding Done Right for Mutual Information Estimation](https://arxiv.org/abs/2510.25983)
*J. Jon Ryu,Pavan Yeddanapudi,Xiangxiang Xu,Gregory W. Wornell*

Main category: cs.LG

TL;DR: InfoNCE不应被视为有效的互信息(MI)估计器。本文提出了一种简单的修改方法InfoNCE-anchor，用于准确的MI估计。


<details>
  <summary>Details</summary>
Motivation: InfoNCE虽然被广泛用于互信息(MI)估计，但它与MI的联系并不直接。本文旨在证明InfoNCE不应被视为有效的MI估计器。

Method: 提出InfoNCE-anchor，引入辅助锚类，实现一致的密度比估计，并产生偏差显著降低的plug-in MI估计器。此外，本文还使用适当的评分规则推广了该框架。

Result: InfoNCE-anchor与log score实现了最准确的MI估计。在自监督表征学习实验中，anchor并没有提高下游任务的性能。对比表征学习的优势不是来自准确的MI估计，而是来自结构化密度比的学习。

Conclusion: 对比表征学习的优势不是来自准确的MI估计，而是来自结构化密度比的学习。

Abstract: The InfoNCE objective, originally introduced for contrastive representation
learning, has become a popular choice for mutual information (MI) estimation,
despite its indirect connection to MI. In this paper, we demonstrate why
InfoNCE should not be regarded as a valid MI estimator, and we introduce a
simple modification, which we refer to as InfoNCE-anchor, for accurate MI
estimation. Our modification introduces an auxiliary anchor class, enabling
consistent density ratio estimation and yielding a plug-in MI estimator with
significantly reduced bias. Beyond this, we generalize our framework using
proper scoring rules, which recover InfoNCE-anchor as a special case when the
log score is employed. This formulation unifies a broad spectrum of contrastive
objectives, including NCE, InfoNCE, and $f$-divergence variants, under a single
principled framework. Empirically, we find that InfoNCE-anchor with the log
score achieves the most accurate MI estimates; however, in self-supervised
representation learning experiments, we find that the anchor does not improve
the downstream task performance. These findings corroborate that contrastive
representation learning benefits not from accurate MI estimation per se, but
from the learning of structured density ratios.

</details>


### [170] [A General and Streamlined Differentiable Optimization Framework](https://arxiv.org/abs/2510.25986)
*Andrew W. Rosemberg,Joaquim Dias Garcia,François Pacaud,Robert B. Parker,Benoît Legat,Kaarthik Sundar,Russell Bent,Pascal Van Hentenryck*

Main category: cs.LG

TL;DR: 提出了一个通用的框架 DiffOpt.jl，用于在 Julia 优化堆栈中统一建模和微分。


<details>
  <summary>Details</summary>
Motivation: 在学习、控制和大规模决策系统中，通过约束优化问题进行区分越来越重要，但由于求解器专业化和接口不匹配，实际集成仍然具有挑战性。

Method: 通过在标准正则性假设下区分 KKT 系统，计算平滑的、潜在的非凸程序的正向和反向模式解和目标灵敏度。JuMP 原生参数中心 API 允许用户声明命名参数并直接获得关于它们的导数。

Result: 在凸和非凸模型上展示了这些能力，包括经济调度、具有锥风险约束的均值-方差投资组合选择和非线性机器人逆运动学。梯度迭代法在能源市场战略投标和使用求解器精确灵敏度的端到端优化代理的 Sobolev 风格训练中进一步展示了规模化的影响。

Conclusion: 可微优化可以作为实验、学习、校准和设计的常规工具进行部署，而不会偏离标准的 JuMP 建模实践，同时保留对广泛的求解器生态系统的访问。

Abstract: Differentiating through constrained optimization problems is increasingly
central to learning, control, and large-scale decision-making systems, yet
practical integration remains challenging due to solver specialization and
interface mismatches. This paper presents a general and streamlined
framework-an updated DiffOpt.jl-that unifies modeling and differentiation
within the Julia optimization stack. The framework computes forward - and
reverse-mode solution and objective sensitivities for smooth, potentially
nonconvex programs by differentiating the KKT system under standard regularity
assumptions. A first-class, JuMP-native parameter-centric API allows users to
declare named parameters and obtain derivatives directly with respect to them -
even when a parameter appears in multiple constraints and objectives -
eliminating brittle bookkeeping from coefficient-level interfaces. We
illustrate these capabilities on convex and nonconvex models, including
economic dispatch, mean-variance portfolio selection with conic risk
constraints, and nonlinear robot inverse kinematics. Two companion studies
further demonstrate impact at scale: gradient-based iterative methods for
strategic bidding in energy markets and Sobolev-style training of end-to-end
optimization proxies using solver-accurate sensitivities. Together, these
results demonstrate that differentiable optimization can be deployed as a
routine tool for experimentation, learning, calibration, and design-without
deviating from standard JuMP modeling practices and while retaining access to a
broad ecosystem of solvers.

</details>


### [171] [Efficient Online Learning with Predictive Coding Networks: Exploiting Temporal Correlations](https://arxiv.org/abs/2510.25993)
*Darius Masoum Zadeh-Jousdani,Elvin Hajizada,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: 提出了一种新的预测编码网络，通过在时间帧上保持潜在状态，显著降低了计算需求，同时保持了学习性能。


<details>
  <summary>Details</summary>
Motivation: 传统的反向传播算法不适合边缘设备的持续适应场景，且不符合生物学原理；预测编码（PC）框架提供了一种生物学上合理的替代方案，但其主要限制是训练期间的计算开销大。

Method: 提出了具有时间摊销的预测编码网络（PCN-TA），该网络通过利用时间相关性来减少计算需求。

Result: 在COIL-20机器人感知数据集上的实验表明，PCN-TA比反向传播减少了10%的权重更新，并且比基线PC网络减少了50%的推理步骤。

Conclusion: PCN-TA 能够减少计算开销，更适合边缘部署和资源受限的机器人系统的实时自适应，并且其生物学特性使其成为未来神经形态硬件实现的一个有前途的候选方案。

Abstract: Robotic systems operating at the edge require efficient online learning
algorithms that can continuously adapt to changing environments while
processing streaming sensory data. Traditional backpropagation, while
effective, conflicts with biological plausibility principles and may be
suboptimal for continuous adaptation scenarios. The Predictive Coding (PC)
framework offers a biologically plausible alternative with local, Hebbian-like
update rules, making it suitable for neuromorphic hardware implementation.
However, PC's main limitation is its computational overhead due to multiple
inference iterations during training. We present Predictive Coding Network with
Temporal Amortization (PCN-TA), which preserves latent states across temporal
frames. By leveraging temporal correlations, PCN-TA significantly reduces
computational demands while maintaining learning performance. Our experiments
on the COIL-20 robotic perception dataset demonstrate that PCN-TA achieves 10%
fewer weight updates compared to backpropagation and requires 50% fewer
inference steps than baseline PC networks. These efficiency gains directly
translate to reduced computational overhead for moving another step toward edge
deployment and real-time adaptation support in resource-constrained robotic
systems. The biologically-inspired nature of our approach also makes it a
promising candidate for future neuromorphic hardware implementations, enabling
efficient online learning at the edge.

</details>


### [172] [Infrequent Exploration in Linear Bandits](https://arxiv.org/abs/2510.26000)
*Harin Lee,Min-hwan Oh*

Main category: cs.LG

TL;DR: 提出了一种名为INFEX的简单而实用的框架，用于线性bandit中的非频繁探索，该框架在给定的时间表下执行基本探索策略，同时主要选择贪婪行动。


<details>
  <summary>Details</summary>
Motivation: 解决完全自适应探索方法（例如，UCB和Thompson Sampling）与纯粹贪婪方法之间的一个重要但被忽视的差距，前者可能在每个时间步都进行探索，而后者需要严格的多样性假设才能成功。在安全关键或成本高昂的领域，持续探索可能不切实际或不道德，而如果没有充分的背景多样性，纯粹的贪婪策略通常会失败。

Method: INFEX执行基本探索策略，并根据给定的时间表在两者之间主要选择贪婪行动。

Result: INFEX实现了与标准可证明有效算法相匹配的实例相关遗憾，前提是探索频率超过对数阈值。此外，INFEX是一个通用的模块化框架，允许无缝集成任何完全自适应的探索方法，从而实现广泛的适用性和易用性。通过将密集的探索计算限制在不频繁的间隔内，我们的方法还可以提高计算效率。经验评估证实了我们的理论结果，表明与现有方法相比，INFEX具有最先进的遗憾性能和运行时间改进。

Conclusion: INFEX在非频繁探索方面表现出色，在遗憾性能和运行时间方面均优于现有方法

Abstract: We study the problem of infrequent exploration in linear bandits, addressing
a significant yet overlooked gap between fully adaptive exploratory methods
(e.g., UCB and Thompson Sampling), which explore potentially at every time
step, and purely greedy approaches, which require stringent diversity
assumptions to succeed. Continuous exploration can be impractical or unethical
in safety-critical or costly domains, while purely greedy strategies typically
fail without adequate contextual diversity. To bridge these extremes, we
introduce a simple and practical framework, INFEX, explicitly designed for
infrequent exploration. INFEX executes a base exploratory policy according to a
given schedule while predominantly choosing greedy actions in between. Despite
its simplicity, our theoretical analysis demonstrates that INFEX achieves
instance-dependent regret matching standard provably efficient algorithms,
provided the exploration frequency exceeds a logarithmic threshold.
Additionally, INFEX is a general, modular framework that allows seamless
integration of any fully adaptive exploration method, enabling wide
applicability and ease of adoption. By restricting intensive exploratory
computations to infrequent intervals, our approach can also enhance
computational efficiency. Empirical evaluations confirm our theoretical
findings, showing state-of-the-art regret performance and runtime improvements
over existing methods.

</details>


### [173] [Dual Mixture-of-Experts Framework for Discrete-Time Survival Analysis](https://arxiv.org/abs/2510.26014)
*Hyeonjun Lee,Hyungseob Shin,Gunhee Nam,Hyeonsoo Lee*

Main category: cs.LG

TL;DR: 提出了一种用于离散时间生存分析的双重混合专家(MoE)框架，以解决患者异质性问题，并使风险预测适应个体特征和时间动态。


<details>
  <summary>Details</summary>
Motivation: 在临床和生物医学研究中，生存分析被广泛用于建模事件发生的时间。一个关键挑战是模拟患者异质性，同时使风险预测适应个体特征和时间动态。

Method: 该方法结合了用于子群感知表示学习的特征编码器MoE和利用患者特征和时间嵌入来捕获时间动态的风险MoE。这种双重MoE设计可以灵活地与现有的基于深度学习的生存管道集成。

Result: 在METABRIC和GBSG乳腺癌数据集上，该方法始终提高性能，在测试集上将时间依赖性C指数提高高达0.04，并且当纳入Consurv框架时，会产生更大的收益。

Conclusion: 该论文提出了一种有效的生存分析方法，可以在乳腺癌数据集上提高性能，并且可以灵活地与现有框架集成。

Abstract: Survival analysis is a task to model the time until an event of interest
occurs, widely used in clinical and biomedical research. A key challenge is to
model patient heterogeneity while also adapting risk predictions to both
individual characteristics and temporal dynamics. We propose a dual
mixture-of-experts (MoE) framework for discrete-time survival analysis. Our
approach combines a feature-encoder MoE for subgroup-aware representation
learning with a hazard MoE that leverages patient features and time embeddings
to capture temporal dynamics. This dual-MoE design flexibly integrates with
existing deep learning based survival pipelines. On METABRIC and GBSG breast
cancer datasets, our method consistently improves performance, boosting the
time-dependent C-index up to 0.04 on the test sets, and yields further gains
when incorporated into the Consurv framework.

</details>


### [174] [Exploring Human-AI Conceptual Alignment through the Prism of Chess](https://arxiv.org/abs/2510.26025)
*Semyon Lomaso,Judah Goldfeder,Mehmet Hamza Erol,Matthew So,Yao Yan,Addison Howard,Nathan Kutz,Ravid Shwartz Ziv*

Main category: cs.LG

TL;DR: 该研究分析了一个达到国际象棋大师水平的 Transformer 模型，发现尽管早期层编码了人类概念（如中心控制和骑士前哨）具有高达 85% 的准确率，但更深层却朝着外星人表示漂移，准确率下降到 50-65%。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究 AI 系统是否真正理解人类概念，还是仅仅模仿表面模式。通过国际象棋这一人类创造力与精确战略概念相结合的领域，来研究 AI 系统的理解能力。

Method: 该研究分析了一个 2.7 亿参数的 Transformer 模型，并引入了首个 Chess960 数据集，该数据集包含 240 个由专家注释的位置，涵盖 6 个战略概念。通过逐层分析，揭示了模型在不同层对人类概念的识别准确率。

Result: 研究结果表明，尽管更深层驱动了卓越的性能，但它们对人类概念的识别准确率下降，并且模型在很大程度上依赖于记忆模式而非抽象理解。在消除了开局理论的情况下，概念识别率下降了 10-20%。

Conclusion: 研究结论是，随着 AI 系统优化性能，它们会发展出越来越陌生的智能，这对于需要真正人机协作的创造性 AI 应用来说是一个关键挑战。

Abstract: Do AI systems truly understand human concepts or merely mimic surface
patterns? We investigate this through chess, where human creativity meets
precise strategic concepts. Analyzing a 270M-parameter transformer that
achieves grandmaster-level play, we uncover a striking paradox: while early
layers encode human concepts like center control and knight outposts with up to
85\% accuracy, deeper layers, despite driving superior performance, drift
toward alien representations, dropping to 50-65\% accuracy. To test conceptual
robustness beyond memorization, we introduce the first Chess960 dataset: 240
expert-annotated positions across 6 strategic concepts. When opening theory is
eliminated through randomized starting positions, concept recognition drops
10-20\% across all methods, revealing the model's reliance on memorized
patterns rather than abstract understanding. Our layer-wise analysis exposes a
fundamental tension in current architectures: the representations that win
games diverge from those that align with human thinking. These findings suggest
that as AI systems optimize for performance, they develop increasingly alien
intelligence, a critical challenge for creative AI applications requiring
genuine human-AI collaboration. Dataset and code are available at:
https://github.com/slomasov/ChessConceptsLLM.

</details>


### [175] [Do Students Debias Like Teachers? On the Distillability of Bias Mitigation Methods](https://arxiv.org/abs/2510.26038)
*Jiali Cheng,Chirag Agarwal,Hadi Amiri*

Main category: cs.LG

TL;DR: 知识蒸馏(KD)是一种有效的模型压缩和模型间知识传递方法，但其对模型抵抗虚假相关性的鲁棒性的影响仍有待探索。本研究探讨了知识蒸馏对自然语言推理(NLI)和图像分类任务中教师模型向学生模型传递“去偏见”能力的影响。


<details>
  <summary>Details</summary>
Motivation: 研究知识蒸馏对模型去偏见能力的影响，因为现有研究较少关注知识蒸馏对模型抵抗虚假相关性的鲁棒性的影响，而这种鲁棒性会降低模型在分布外数据上的性能。

Method: 通过大量实验，在自然语言推理(NLI)和图像分类任务上，研究知识蒸馏前后模型去偏见能力的变化，并分析内部注意力模式和导致KD后行为差异的回路。

Result: （i）总体而言，模型的去偏见能力在KD后会受到损害；（ii）训练一个去偏见的模型并不能从注入教师知识中获益；（iii）虽然模型的整体鲁棒性在蒸馏后可能保持稳定，但不同类型的偏差之间可能发生显着变化；（iv）我们查明了导致KD后不同行为的内部注意模式和回路。

Conclusion: 提出了三种有效的解决方案来提高去偏见方法的可蒸馏性：开发用于增强的高质量数据，实施迭代知识蒸馏，并使用从教师模型获得的权重初始化学生模型。

Abstract: Knowledge distillation (KD) is an effective method for model compression and
transferring knowledge between models. However, its effect on model's
robustness against spurious correlations that degrade performance on
out-of-distribution data remains underexplored. This study investigates the
effect of knowledge distillation on the transferability of ``debiasing''
capabilities from teacher models to student models on natural language
inference (NLI) and image classification tasks. Through extensive experiments,
we illustrate several key findings: (i) overall the debiasing capability of a
model is undermined post-KD; (ii) training a debiased model does not benefit
from injecting teacher knowledge; (iii) although the overall robustness of a
model may remain stable post-distillation, significant variations can occur
across different types of biases; and (iv) we pin-point the internal attention
pattern and circuit that causes the distinct behavior post-KD. Given the above
findings, we propose three effective solutions to improve the distillability of
debiasing methods: developing high quality data for augmentation, implementing
iterative knowledge distillation, and initializing student models with weights
obtained from teacher models. To the best of our knowledge, this is the first
study on the effect of KD on debiasing and its interenal mechanism at scale.
Our findings provide understandings on how KD works and how to design better
debiasing methods.

</details>


### [176] [Towards Scaling Laws for Symbolic Regression](https://arxiv.org/abs/2510.26064)
*David Otte,Jörg K. H. Franke,Frank Hutter*

Main category: cs.LG

TL;DR: 本文研究了符号回归（SR）中的扩展规律，使用可扩展的端到端transformer流水线和精心生成的训练数据，对五个不同模型大小和三个数量级的计算进行了研究。


<details>
  <summary>Details</summary>
Motivation: 探索符号回归(SR)的基础知识。基于深度学习的SR最近在与遗传编程方法方面具有竞争力，但规模的作用在很大程度上仍未得到探索。

Method: 使用可扩展的端到端transformer流水线和精心生成的训练数据。

Result: 验证损失和求解率都遵循计算的清晰幂律趋势。最佳批次大小和学习率随模型大小而增长，并且在我们的机制中，token-to-parameter比率约为15是最佳的，并且随着计算的增加而略有上升趋势。

Conclusion: SR性能在很大程度上可以通过计算来预测，并为训练下一代SR模型提供了重要的见解。

Abstract: Symbolic regression (SR) aims to discover the underlying mathematical
expressions that explain observed data. This holds promise for both gaining
scientific insight and for producing inherently interpretable and generalizable
models for tabular data. In this work we focus on the basics of SR. Deep
learning-based SR has recently become competitive with genetic programming
approaches, but the role of scale has remained largely unexplored. Inspired by
scaling laws in language modeling, we present the first systematic
investigation of scaling in SR, using a scalable end-to-end transformer
pipeline and carefully generated training data. Across five different model
sizes and spanning three orders of magnitude in compute, we find that both
validation loss and solved rate follow clear power-law trends with compute. We
further identify compute-optimal hyperparameter scaling: optimal batch size and
learning rate grow with model size, and a token-to-parameter ratio of
$\approx$15 is optimal in our regime, with a slight upward trend as compute
increases. These results demonstrate that SR performance is largely predictable
from compute and offer important insights for training the next generation of
SR models.

</details>


### [177] [Learning Geometry: A Framework for Building Adaptive Manifold Models through Metric Optimization](https://arxiv.org/abs/2510.26068)
*Di Zhang*

Main category: cs.LG

TL;DR: 提出了一种新的机器学习范式，它将模型本身视为可塑形的几何实体，通过优化流形上的度量张量场来动态塑造模型空间的几何结构。


<details>
  <summary>Details</summary>
Motivation: 传统方法在固定几何空间内搜索最优参数，而本文旨在突破这一限制。

Method: 构建了一个变分框架，其损失函数平衡了数据保真度和流形的内在几何复杂性。连续流形被离散化为三角网格，度量张量由边长参数化，从而可以使用自动微分工具进行高效优化。

Result: 理论分析揭示了该框架与广义相对论中爱因斯坦-希尔伯特作用量之间的深刻类比。即使拓扑固定，度量优化也比固定几何的模型提供更大的表达能力。

Conclusion: 该工作为构建能够自主演化其几何和拓扑的完全动态的“元学习器”奠定了坚实的基础，并指向了科学模型发现和鲁棒表示学习等领域的广阔应用前景。

Abstract: This paper proposes a novel paradigm for machine learning that moves beyond
traditional parameter optimization. Unlike conventional approaches that search
for optimal parameters within a fixed geometric space, our core idea is to
treat the model itself as a malleable geometric entity. Specifically, we
optimize the metric tensor field on a manifold with a predefined topology,
thereby dynamically shaping the geometric structure of the model space. To
achieve this, we construct a variational framework whose loss function
carefully balances data fidelity against the intrinsic geometric complexity of
the manifold. The former ensures the model effectively explains observed data,
while the latter acts as a regularizer, penalizing overly curved or irregular
geometries to encourage simpler models and prevent overfitting. To address the
computational challenges of this infinite-dimensional optimization problem, we
introduce a practical method based on discrete differential geometry: the
continuous manifold is discretized into a triangular mesh, and the metric
tensor is parameterized by edge lengths, enabling efficient optimization using
automatic differentiation tools. Theoretical analysis reveals a profound
analogy between our framework and the Einstein-Hilbert action in general
relativity, providing an elegant physical interpretation for the concept of
"data-driven geometry". We further argue that even with fixed topology, metric
optimization offers significantly greater expressive power than models with
fixed geometry. This work lays a solid foundation for constructing fully
dynamic "meta-learners" capable of autonomously evolving their geometry and
topology, and it points to broad application prospects in areas such as
scientific model discovery and robust representation learning.

</details>


### [178] [New Money: A Systematic Review of Synthetic Data Generation for Finance](https://arxiv.org/abs/2510.26076)
*James Meldrum,Basem Suleiman,Fethi Rabhi,Muhammad Johan Alibasa*

Main category: cs.LG

TL;DR: 本研究旨在对金融领域中生成对抗网络(GANs)和变分自编码器(VAEs)等生成模型在合成金融数据生成方面的应用进行系统性回顾，以解决使用敏感金融数据进行机器学习应用时遇到的挑战。


<details>
  <summary>Details</summary>
Motivation: 动机：尽管合成数据生成领域发展迅速，但缺乏对当前研究的综合性总结。本研究旨在填补这一空白。

Method: 方法：本研究通过系统性回顾和分析2018年以来发表的72篇关于合成金融数据生成的研究，对合成的金融信息类型、使用的生成方法和评估数据效用和隐私的评估策略进行分类。

Result: 结果：研究结果表明，基于GAN的方法在文献中占据主导地位，尤其是在生成时间序列市场数据和表格信用数据方面。同时，也发现对隐私保护的严格评估仍然不足。

Conclusion: 结论：本研究对生成技术、应用和评估方法进行了综合概述，强调了关键的研究差距，并为未来开发稳健、保护隐私的金融领域合成数据解决方案提供了指导。

Abstract: Synthetic data generation has emerged as a promising approach to address the
challenges of using sensitive financial data in machine learning applications.
By leveraging generative models, such as Generative Adversarial Networks (GANs)
and Variational Autoencoders (VAEs), it is possible to create artificial
datasets that preserve the statistical properties of real financial records
while mitigating privacy risks and regulatory constraints. Despite the rapid
growth of this field, a comprehensive synthesis of the current research
landscape has been lacking. This systematic review consolidates and analyses 72
studies published since 2018 that focus on synthetic financial data generation.
We categorise the types of financial information synthesised, the generative
methods employed, and the evaluation strategies used to assess data utility and
privacy. The findings indicate that GAN-based approaches dominate the
literature, particularly for generating time-series market data and tabular
credit data. While several innovative techniques demonstrate potential for
improved realism and privacy preservation, there remains a notable lack of
rigorous evaluation of privacy safeguards across studies. By providing an
integrated overview of generative techniques, applications, and evaluation
methods, this review highlights critical research gaps and offers guidance for
future work aimed at developing robust, privacy-preserving synthetic data
solutions for the financial domain.

</details>


### [179] [Nirvana: A Specialized Generalist Model With Task-Aware Memory Mechanism](https://arxiv.org/abs/2510.26083)
*Yuhua Jiang,Shuang Cheng,Yihao Liu,Ermo Hua,Che Jiang,Weigao Sun,Yu Cheng,Feifei Gao,Biqing Qi,Bowen Zhou*

Main category: cs.LG

TL;DR: Nirvana是一种具有专业记忆机制的SGM，具有线性时间复杂度和测试时任务信息提取能力。


<details>
  <summary>Details</summary>
Motivation: 传统LLM结构（包括Transformer、线性注意力模型和混合模型）没有采用由任务信息引导的专业记忆机制。

Method: 提出任务感知记忆触发器 (Trigger)，可以灵活地根据当前任务的需求调整记忆机制。设计了专业记忆更新器 (Updater)，可以动态地记忆由 Trigger 引导的上下文。

Result: 在各种自然语言建模基准测试中，Nirvana 取得了与现有 LLM 结构相比具有竞争力或更优越的结果。在具有挑战性的医学任务（即磁共振成像 (MRI)）上测试了 Nirvana 的性能，Nirvana 实现了比传统 MRI 模型以及具有传统 LLM 主干的模型更高质量的 MRI 重建，并且还可以生成准确的初步临床报告。

Conclusion: Nirvana在通用语言任务和专业医学任务上都表现出色，证明了其专业记忆机制和任务感知能力的有效性。

Abstract: Specialized Generalist Models (SGMs) aim to preserve broad capabilities while
achieving expert-level performance in target domains. However, traditional LLM
structures including Transformer, Linear Attention, and hybrid models do not
employ specialized memory mechanism guided by task information. In this paper,
we present Nirvana, an SGM with specialized memory mechanism, linear time
complexity, and test-time task information extraction. Besides, we propose the
Task-Aware Memory Trigger ($\textit{Trigger}$) that flexibly adjusts memory
mechanism based on the current task's requirements. In Trigger, each incoming
sample is treated as a self-supervised fine-tuning task, enabling Nirvana to
adapt its task-related parameters on the fly to domain shifts. We also design
the Specialized Memory Updater ($\textit{Updater}$) that dynamically memorizes
the context guided by Trigger. We conduct experiments on both general language
tasks and specialized medical tasks. On a variety of natural language modeling
benchmarks, Nirvana achieves competitive or superior results compared to the
existing LLM structures. To prove the effectiveness of Trigger on specialized
tasks, we test Nirvana's performance on a challenging medical task, i.e.,
Magnetic Resonance Imaging (MRI). We post-train frozen Nirvana backbone with
lightweight codecs on paired electromagnetic signals and MRI images. Despite
the frozen Nirvana backbone, Trigger guides the model to adapt to the MRI
domain with the change of task-related parameters. Nirvana achieves
higher-quality MRI reconstruction compared to conventional MRI models as well
as the models with traditional LLMs' backbone, and can also generate accurate
preliminary clinical reports accordingly.

</details>


### [180] [LLMBisect: Breaking Barriers in Bug Bisection with A Comparative Analysis Pipeline](https://arxiv.org/abs/2510.26086)
*Zheng Zhang,Haonan Li,Xingyu Li,Hang Zhang,Zhiyun Qian*

Main category: cs.LG

TL;DR: 本文提出了一种基于大型语言模型（LLM）的多阶段流水线方法，用于更准确地识别引入bug的commit。


<details>
  <summary>Details</summary>
Motivation: 传统基于补丁的bug bisection方法存在局限性，例如假设bug引入commit和补丁commit修改相同的函数，且仅依赖代码更改而忽略commit message中的信息，缺乏对漏洞的逻辑分析。

Method: 该方法利用LLM充分利用补丁信息，在上下文中比较多个候选commit，并通过一系列down-selection步骤逐步缩小候选范围。

Result: 该方法比现有最佳解决方案的准确率提高了38%以上，并且多阶段流水线比基于LLM的baseline方法提高了60%的准确率。

Conclusion: 该研究表明，LLM能够有效打破现有bug bisection方法的壁垒，所提出的多阶段流水线对于提高准确性至关重要。

Abstract: Bug bisection has been an important security task that aims to understand the
range of software versions impacted by a bug, i.e., identifying the commit that
introduced the bug. However, traditional patch-based bisection methods are
faced with several significant barriers: For example, they assume that the
bug-inducing commit (BIC) and the patch commit modify the same functions, which
is not always true. They often rely solely on code changes, while the commit
message frequently contains a wealth of vulnerability-related information. They
are also based on simple heuristics (e.g., assuming the BIC initializes lines
deleted in the patch) and lack any logical analysis of the vulnerability.
  In this paper, we make the observation that Large Language Models (LLMs) are
well-positioned to break the barriers of existing solutions, e.g., comprehend
both textual data and code in patches and commits. Unlike previous BIC
identification approaches, which yield poor results, we propose a comprehensive
multi-stage pipeline that leverages LLMs to: (1) fully utilize patch
information, (2) compare multiple candidate commits in context, and (3)
progressively narrow down the candidates through a series of down-selection
steps. In our evaluation, we demonstrate that our approach achieves
significantly better accuracy than the state-of-the-art solution by more than
38\%. Our results further confirm that the comprehensive multi-stage pipeline
is essential, as it improves accuracy by 60\% over a baseline LLM-based
bisection method.

</details>


### [181] [Network-Constrained Policy Optimization for Adaptive Multi-agent Vehicle Routing](https://arxiv.org/abs/2510.26089)
*Fazel Arasteh,Arian Haghparast,Manos Papagelis*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于多智能体强化学习（MARL）的动态车辆路由框架，旨在解决城市道路网络中的交通拥堵问题。


<details>
  <summary>Details</summary>
Motivation: 现有的最短路径优先（SPF）算法在动态、多车辆环境中表现不佳，容易加剧拥堵。因此，需要一种能够协调车辆并感知网络状态的动态车辆路由方法。

Method: 论文提出了两种MARL模型：自适应导航（AN）和分层中心自适应导航（HHAN）。AN使用图注意力网络（GAT）建模邻域状态，HHAN则采用集中训练分散执行（CTDE）框架，并通过注意力机制聚合异步车辆决策。

Result: 在合成网格和真实城市地图上的实验表明，AN能够减少平均出行时间，HHAN能够扩展到具有数百个交叉口的网络，并在高流量下实现高达15.9%的改进。

Conclusion: 研究结果表明，网络约束的MARL在智能交通系统中具有可扩展、协调和拥堵感知的路由潜力。

Abstract: Traffic congestion in urban road networks leads to longer trip times and
higher emissions, especially during peak periods. While the Shortest Path First
(SPF) algorithm is optimal for a single vehicle in a static network, it
performs poorly in dynamic, multi-vehicle settings, often worsening congestion
by routing all vehicles along identical paths. We address dynamic vehicle
routing through a multi-agent reinforcement learning (MARL) framework for
coordinated, network-aware fleet navigation. We first propose Adaptive
Navigation (AN), a decentralized MARL model where each intersection agent
provides routing guidance based on (i) local traffic and (ii) neighborhood
state modeled using Graph Attention Networks (GAT). To improve scalability in
large networks, we further propose Hierarchical Hub-based Adaptive Navigation
(HHAN), an extension of AN that assigns agents only to key intersections
(hubs). Vehicles are routed hub-to-hub under agent control, while SPF handles
micro-routing within each hub region. For hub coordination, HHAN adopts
centralized training with decentralized execution (CTDE) under the Attentive
Q-Mixing (A-QMIX) framework, which aggregates asynchronous vehicle decisions
via attention. Hub agents use flow-aware state features that combine local
congestion and predictive dynamics for proactive routing. Experiments on
synthetic grids and real urban maps (Toronto, Manhattan) show that AN reduces
average travel time versus SPF and learning baselines, maintaining 100% routing
success. HHAN scales to networks with hundreds of intersections, achieving up
to 15.9% improvement under heavy traffic. These findings highlight the
potential of network-constrained MARL for scalable, coordinated, and
congestion-aware routing in intelligent transportation systems.

</details>


### [182] [SAFE: A Novel Approach to AI Weather Evaluation through Stratified Assessments of Forecasts over Earth](https://arxiv.org/abs/2510.26099)
*Nick Masi,Randall Balestriero*

Main category: cs.LG

TL;DR: SAFE是一个用于评估地球上预测分层性能的软件包，它集成了各种数据域，可以通过与地理空间网格点相关的不同属性进行分层。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习范式基于测试集中所有样本的平均损失来评估模型性能，无法解释人类发展和地理的非均匀分布。因此，需要考虑地理空间分层评估模型性能。

Method: SAFE软件包通过territory、global subregion、income和landcover等属性对地理空间网格点进行分层，从而评估模型在每个属性的各个层中的性能。

Result: 通过SAFE对最先进的AI天气预测模型进行基准测试，发现它们在每个属性的预测技能上都存在差异。

Conclusion: SAFE软件包可以帮助我们了解模型在何处表现最佳或最差，以及哪些模型最公平。该软件包是开源的，可用于支持未来在这方面的研究。

Abstract: The dominant paradigm in machine learning is to assess model performance
based on average loss across all samples in some test set. This amounts to
averaging performance geospatially across the Earth in weather and climate
settings, failing to account for the non-uniform distribution of human
development and geography. We introduce Stratified Assessments of Forecasts
over Earth (SAFE), a package for elucidating the stratified performance of a
set of predictions made over Earth. SAFE integrates various data domains to
stratify by different attributes associated with geospatial gridpoints:
territory (usually country), global subregion, income, and landcover (land or
water). This allows us to examine the performance of models for each individual
stratum of the different attributes (e.g., the accuracy in every individual
country). To demonstrate its importance, we utilize SAFE to benchmark a zoo of
state-of-the-art AI-based weather prediction models, finding that they all
exhibit disparities in forecasting skill across every attribute. We use this to
seed a benchmark of model forecast fairness through stratification at different
lead times for various climatic variables. By moving beyond globally-averaged
metrics, we for the first time ask: where do models perform best or worst, and
which models are most fair? To support further work in this direction, the SAFE
package is open source and available at https://github.com/N-Masi/safe

</details>


### [183] [Do Not Step Into the Same River Twice: Learning to Reason from Trial and Error](https://arxiv.org/abs/2510.26109)
*Chenming Tang,Hsiu-Yuan Huang,Weijie Liu,Saiyong Yang,Yunfang Wu*

Main category: cs.LG

TL;DR: 提出了一种名为LTE的强化学习方法，通过提示LLM之前生成的错误答案和过长响应问题来改进LLM的推理能力，无需外部专家指导。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR方法仅基于LLM自身生成的响应进行训练，受限于LLM的初始能力，容易出现探索停滞。

Method: LTE方法提示LLM之前自我生成的错误答案和过长响应问题。

Result: 实验表明，LTE在Qwen3-4B-Base上优于普通GRPO，Pass@1平均提升6.38，Pass@k平均提升9.00。

Conclusion: LTE成功缓解了探索停滞问题，并在训练过程中增强了利用和探索。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has significantly
boosted the reasoning capability of large language models (LLMs) recently.
However, existing RLVR approaches merely train LLMs based on their own
generated responses and are constrained by the initial capability of LLMs, thus
prone to exploration stagnation, in which LLMs fail to solve more training
problems and cannot further learn from the training data. Some work tries to
address this by leveraging off-policy solutions to training problems but
requires external guidance from experts which suffers from limited
availability. In this work, we propose LTE (Learning to reason from Trial and
Error), an approach hinting LLMs with their previously self-generated incorrect
answers and problem of overlong responses, which does not require any external
expert guidance. Experiments validate the effectiveness of LTE, which
outperforms the normal group relative policy optimization (GRPO) by 6.38 in
Pass@1 and 9.00 in Pass@k on average across six mathematics benchmarks for
Qwen3-4B-Base. Further analysis confirms that LTE successfully mitigates the
problem of exploration stagnation and enhances both exploitation and
exploration during training.

</details>


### [184] [maxVSTAR: Maximally Adaptive Vision-Guided CSI Sensing with Closed-Loop Edge Model Adaptation for Robust Human Activity Recognition](https://arxiv.org/abs/2510.26146)
*Kexing Liu*

Main category: cs.LG

TL;DR: maxVSTAR: A vision-guided model adaptation framework for edge-deployed CSI sensing systems that mitigates domain shift and restores accuracy through closed-loop retraining.


<details>
  <summary>Details</summary>
Motivation: Recognition performance deteriorates under varying environmental and hardware conditions in WiFi CSI-based HAR on edge devices due to domain shift.

Method: A cross-modal teacher-student architecture is used, with a YOLO-based vision model providing real-time activity labels for online fine-tuning of a lightweight CSI-based HAR model (STAR) at the edge.

Result: When deployed on uncalibrated hardware, maxVSTAR restored the accuracy of the baseline STAR model from 49.14% to 81.51% after a single vision-guided adaptation cycle, with the baseline STAR model starting at 93.52%.

Conclusion: maxVSTAR enables dynamic, self-supervised model adaptation in privacy-conscious IoT environments, providing a scalable and practical paradigm for long-term autonomous HAR using CSI sensing at the network edge.

Abstract: WiFi Channel State Information (CSI)-based human activity recognition (HAR)
provides a privacy-preserving, device-free sensing solution for smart
environments. However, its deployment on edge devices is severely constrained
by domain shift, where recognition performance deteriorates under varying
environmental and hardware conditions. This study presents maxVSTAR (maximally
adaptive Vision-guided Sensing Technology for Activity Recognition), a
closed-loop, vision-guided model adaptation framework that autonomously
mitigates domain shift for edge-deployed CSI sensing systems. The proposed
system integrates a cross-modal teacher-student architecture, where a
high-accuracy YOLO-based vision model serves as a dynamic supervisory signal,
delivering real-time activity labels for the CSI data stream. These labels
enable autonomous, online fine-tuning of a lightweight CSI-based HAR model,
termed Sensing Technology for Activity Recognition (STAR), directly at the
edge. This closed-loop retraining mechanism allows STAR to continuously adapt
to environmental changes without manual intervention. Extensive experiments
demonstrate the effectiveness of maxVSTAR. When deployed on uncalibrated
hardware, the baseline STAR model's recognition accuracy declined from 93.52%
to 49.14%. Following a single vision-guided adaptation cycle, maxVSTAR restored
the accuracy to 81.51%. These results confirm the system's capacity for
dynamic, self-supervised model adaptation in privacy-conscious IoT
environments, establishing a scalable and practical paradigm for long-term
autonomous HAR using CSI sensing at the network edge.

</details>


### [185] [STAR: A Privacy-Preserving, Energy-Efficient Edge AI Framework for Human Activity Recognition via Wi-Fi CSI in Mobile and Pervasive Computing Environments](https://arxiv.org/abs/2510.26148)
*Kexing Liu*

Main category: cs.LG

TL;DR: STAR is an edge-AI-optimized framework for real-time, energy-efficient human activity recognition (HAR) on low-power embedded devices using Wi-Fi Channel State Information (CSI).


<details>
  <summary>Details</summary>
Motivation: Existing HAR methods lack computational efficiency, have high latency, and limited feasibility in resource-constrained environments.

Method: A lightweight GRU-based neural network, adaptive signal processing, and hardware-aware co-optimization are integrated. The system uses a multi-stage pre-processing pipeline for denoising and feature extraction. Implemented on a Rockchip RV1126 processor with an NPU and interfaced with an ESP32-S3 CSI acquisition module.

Result: A mean recognition accuracy of 93.52% across seven activity classes and 99.11% for human presence detection is achieved. INT8 quantized inference reaches 33 MHz with 8% CPU utilization, a sixfold speed improvement over CPU-based execution.

Conclusion: STAR ensures real-time, privacy-preserving HAR, offering a practical, scalable solution for mobile and pervasive computing environments.

Abstract: Human Activity Recognition (HAR) via Wi-Fi Channel State Information (CSI)
presents a privacy-preserving, contactless sensing approach suitable for smart
homes, healthcare monitoring, and mobile IoT systems. However, existing methods
often encounter computational inefficiency, high latency, and limited
feasibility within resource-constrained, embedded mobile edge environments.
This paper proposes STAR (Sensing Technology for Activity Recognition), an
edge-AI-optimized framework that integrates a lightweight neural architecture,
adaptive signal processing, and hardware-aware co-optimization to enable
real-time, energy-efficient HAR on low-power embedded devices. STAR
incorporates a streamlined Gated Recurrent Unit (GRU)-based recurrent neural
network, reducing model parameters by 33% compared to conventional LSTM models
while maintaining effective temporal modeling capability. A multi-stage
pre-processing pipeline combining median filtering, 8th-order Butterworth
low-pass filtering, and Empirical Mode Decomposition (EMD) is employed to
denoise CSI amplitude data and extract spatial-temporal features. For on-device
deployment, STAR is implemented on a Rockchip RV1126 processor equipped with an
embedded Neural Processing Unit (NPU), interfaced with an ESP32-S3-based CSI
acquisition module. Experimental results demonstrate a mean recognition
accuracy of 93.52% across seven activity classes and 99.11% for human presence
detection, utilizing a compact 97.6k-parameter model. INT8 quantized inference
achieves a processing speed of 33 MHz with just 8% CPU utilization, delivering
sixfold speed improvements over CPU-based execution. With sub-second response
latency and low power consumption, the system ensures real-time,
privacy-preserving HAR, offering a practical, scalable solution for mobile and
pervasive computing environments.

</details>


### [186] [Bridging the Gap Between Molecule and Textual Descriptions via Substructure-aware Alignment](https://arxiv.org/abs/2510.26157)
*Hyuntae Park,Yeachan Kim,SangKeun Lee*

Main category: cs.LG

TL;DR: MolBridge是一个新的分子-文本学习框架，它通过子结构感知对齐来增强分子和文本的表示学习。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以捕捉分子及其描述之间的细微差别，因为它们缺乏学习分子子结构和化学短语之间细粒度对齐的能力。

Method: 该方法引入了MolBridge，一种基于子结构感知对齐的分子-文本学习框架，它使用子结构感知的对比学习，并结合自提纯机制来过滤掉噪声对齐信号。

Result: MolBridge有效地捕获了细粒度的对应关系，并在各种分子基准测试中优于最先进的基线。

Conclusion: 该研究结果突出了子结构感知对齐在分子-文本学习中的重要性。

Abstract: Molecule and text representation learning has gained increasing interest due
to its potential for enhancing the understanding of chemical information.
However, existing models often struggle to capture subtle differences between
molecules and their descriptions, as they lack the ability to learn
fine-grained alignments between molecular substructures and chemical phrases.
To address this limitation, we introduce MolBridge, a novel molecule-text
learning framework based on substructure-aware alignments. Specifically, we
augment the original molecule-description pairs with additional alignment
signals derived from molecular substructures and chemical phrases. To
effectively learn from these enriched alignments, MolBridge employs
substructure-aware contrastive learning, coupled with a self-refinement
mechanism that filters out noisy alignment signals. Experimental results show
that MolBridge effectively captures fine-grained correspondences and
outperforms state-of-the-art baselines on a wide range of molecular benchmarks,
highlighting the significance of substructure-aware alignment in molecule-text
learning.

</details>


### [187] [Segmentation over Complexity: Evaluating Ensemble and Hybrid Approaches for Anomaly Detection in Industrial Time Series](https://arxiv.org/abs/2510.26159)
*Emilio Mastriani,Alessandro Costa,Federico Incardona,Kevin Munari,Sebastiano Spinello*

Main category: cs.LG

TL;DR: 高级异常检测方法在多元工业时间序列中表现不如简单的集成模型。


<details>
  <summary>Details</summary>
Motivation: 研究高级特征工程和混合模型架构在多元工业时间序列异常检测中的有效性，以蒸汽轮机系统为例。

Method: 评估变更点统计特征、基于聚类的子结构表示和混合学习策略对检测性能的影响。使用分段数据训练简单的随机森林+XGBoost集成模型。

Result: 简单的集成模型实现了0.976的AUC-ROC，0.41的F1-score，并在定义的时间窗口内实现了100%的早期检测，优于复杂方法。

Conclusion: 在高度不平衡和时间不确定的数据场景中，模型简单性与优化的分段相结合，可以胜过更复杂的架构，提供更大的鲁棒性、可解释性和操作实用性。

Abstract: In this study, we investigate the effectiveness of advanced feature
engineering and hybrid model architectures for anomaly detection in a
multivariate industrial time series, focusing on a steam turbine system. We
evaluate the impact of change point-derived statistical features,
clustering-based substructure representations, and hybrid learning strategies
on detection performance. Despite their theoretical appeal, these complex
approaches consistently underperformed compared to a simple Random Forest +
XGBoost ensemble trained on segmented data. The ensemble achieved an AUC-ROC of
0.976, F1-score of 0.41, and 100% early detection within the defined time
window. Our findings highlight that, in scenarios with highly imbalanced and
temporally uncertain data, model simplicity combined with optimized
segmentation can outperform more sophisticated architectures, offering greater
robustness, interpretability, and operational utility.

</details>


### [188] [A Game-Theoretic Spatio-Temporal Reinforcement Learning Framework for Collaborative Public Resource Allocation](https://arxiv.org/abs/2510.26184)
*Songxin Lei,Qiongyan Wang,Yanchen Zhu,Hanyu Yao,Sijie Ruan,Weilin Ruan,Yuyu Luo,Huaming Wu,Yuxuan Liang*

Main category: cs.LG

TL;DR: 提出了一种新的公共资源分配问题CPRA，并提出了GSTRL框架来解决该问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了容量约束和时空动态，无法有效满足社会需求。

Method: 将CPRA问题建模为势博弈，并设计了GSTRL框架。

Result: 在两个真实世界数据集上的实验表明，GSTRL表现出色。

Conclusion: GSTRL框架能够有效捕获整体系统的时空动态，并在公共资源分配方面表现出优越的性能。

Abstract: Public resource allocation involves the efficient distribution of resources,
including urban infrastructure, energy, and transportation, to effectively meet
societal demands. However, existing methods focus on optimizing the movement of
individual resources independently, without considering their capacity
constraints. To address this limitation, we propose a novel and more practical
problem: Collaborative Public Resource Allocation (CPRA), which explicitly
incorporates capacity constraints and spatio-temporal dynamics in real-world
scenarios. We propose a new framework called Game-Theoretic Spatio-Temporal
Reinforcement Learning (GSTRL) for solving CPRA. Our contributions are twofold:
1) We formulate the CPRA problem as a potential game and demonstrate that there
is no gap between the potential function and the optimal target, laying a solid
theoretical foundation for approximating the Nash equilibrium of this NP-hard
problem; and 2) Our designed GSTRL framework effectively captures the
spatio-temporal dynamics of the overall system. We evaluate GSTRL on two
real-world datasets, where experiments show its superior performance. Our
source codes are available in the supplementary materials.

</details>


### [189] [Accumulative SGD Influence Estimation for Data Attribution](https://arxiv.org/abs/2510.26185)
*Yunxiao Shi,Shuo Yang,Yixin Su,Rui Zhang,Min Xu*

Main category: cs.LG

TL;DR: 本文提出了一种新的影响函数估计器 ACC-SGD-IE，它通过在训练过程中传播 leave-one-out 扰动并更新每个步骤的累积影响状态来实现更精确的单样本影响估计。


<details>
  <summary>Details</summary>
Motivation: 现代以数据为中心的人工智能需要精确的每样本影响，而标准 SGD-IE 通过对每个 epoch 的替代物求和来近似 leave-one-out 效应，忽略了跨 epoch 的复合，这错误地排列了关键示例。

Method: 提出 ACC-SGD-IE，一种轨迹感知估计器，它在训练过程中传播 leave-one-out 扰动，并在每个步骤更新累积影响状态。

Result: 在 Adult、20 Newsgroups 和 MNIST 数据集上，在干净和损坏的数据以及凸和非凸训练下，ACC-SGD-IE 产生更准确的影响估计，尤其是在长 epochs 上。对于下游数据清理，它更可靠地标记噪声样本，从而产生在 ACC-SGD-IE 清理的数据上训练的模型，其性能优于使用 SGD-IE 清理的模型。

Conclusion: ACC-SGD-IE 是一种更精确的影响函数估计器，可以更可靠地标记噪声样本，并提高下游数据清理的性能。

Abstract: Modern data-centric AI needs precise per-sample influence. Standard SGD-IE
approximates leave-one-out effects by summing per-epoch surrogates and ignores
cross-epoch compounding, which misranks critical examples. We propose
ACC-SGD-IE, a trajectory-aware estimator that propagates the leave-one-out
perturbation across training and updates an accumulative influence state at
each step. In smooth strongly convex settings it achieves geometric error
contraction and, in smooth non-convex regimes, it tightens error bounds; larger
mini-batches further reduce constants. Empirically, on Adult, 20 Newsgroups,
and MNIST under clean and corrupted data and both convex and non-convex
training, ACC-SGD-IE yields more accurate influence estimates, especially over
long epochs. For downstream data cleansing it more reliably flags noisy
samples, producing models trained on ACC-SGD-IE cleaned data that outperform
those cleaned with SGD-IE.

</details>


### [190] [Predicting All-Cause Hospital Readmissions from Medical Claims Data of Hospitalised Patients](https://arxiv.org/abs/2510.26188)
*Avinash Kadimisetty,Arun Rajagopalan,Vijendra SK*

Main category: cs.LG

TL;DR: 该研究使用机器学习预测医院再入院率，旨在降低医疗成本和提高医疗质量。


<details>
  <summary>Details</summary>
Motivation: 降低可预防的医院再入院率是医疗支付方、服务提供方和决策者的首要任务，再入院率是衡量医院医疗质量的基准。

Method: 利用 Logistic 回归、随机森林和支持向量机等机器学习技术分析健康理赔数据，并使用主成分分析进行降维，构建回归模型。

Result: 随机森林模型表现最佳，其次是 Logistic 回归和支持向量机模型。

Conclusion: 这些模型可以识别导致再入院的关键因素，并帮助识别应重点关注的患者，以降低再入院的可能性，最终降低成本并提高为患者提供的医疗保健质量。

Abstract: Reducing preventable hospital readmissions is a national priority for payers,
providers, and policymakers seeking to improve health care and lower costs. The
rate of readmission is being used as a benchmark to determine the quality of
healthcare provided by the hospitals. In thisproject, we have used machine
learning techniques like Logistic Regression, Random Forest and Support Vector
Machines to analyze the health claims data and identify demographic and medical
factors that play a crucial role in predicting all-cause readmissions. As the
health claims data is high dimensional, we have used Principal Component
Analysis as a dimension reduction technique and used the results for building
regression models. We compared and evaluated these models based on the Area
Under Curve (AUC) metric. Random Forest model gave the highest performance
followed by Logistic Regression and Support Vector Machine models. These models
can be used to identify the crucial factors causing readmissions and help
identify patients to focus on to reduce the chances of readmission, ultimately
bringing down the cost and increasing the quality of healthcare provided to the
patients.

</details>


### [191] [Test-Time Alignment of LLMs via Sampling-Based Optimal Control in pre-logit space](https://arxiv.org/abs/2510.26219)
*Sekitoshi Kanai,Tsukasa Yoshida,Hiroshi Takahashi,Haru Kuroki,Kazumune Hashimoto*

Main category: cs.LG

TL;DR: 提出了一种新的测试时对齐方法，称为pre-logits自适应重要性抽样(AISP)。


<details>
  <summary>Details</summary>
Motivation: 微调大型语言模型(llm)需要很高的计算成本，因此LLM的测试时对齐备受关注。

Method: 在基于抽样的模型预测控制和随机控制输入的基础上，AISP将高斯扰动应用于倒数第二层的输出pre-logits，从而使关于扰动均值的期望奖励最大化。通过对抽样奖励进行重要性抽样，得到最优均值。

Result: 在使用的样本数量上，AISP优于best-of-n抽样，并且比其他基于奖励的测试时对齐方法获得了更高的奖励。

Conclusion: AISP方法优于其他测试时对齐方法

Abstract: Test-time alignment of large language models (LLMs) attracts attention
because fine-tuning LLMs requires high computational costs. In this paper, we
propose a new test-time alignment method called adaptive importance sampling on
pre-logits (AISP) on the basis of the sampling-based model predictive control
with the stochastic control input. AISP applies the Gaussian perturbation into
pre-logits, which are outputs of the penultimate layer, so as to maximize
expected rewards with respect to the mean of the perturbation. We demonstrate
that the optimal mean is obtained by importance sampling with sampled rewards.
AISP outperforms best-of-n sampling in terms of rewards over the number of used
samples and achieves higher rewards than other reward-based test-time alignment
methods.

</details>


### [192] [MPRU: Modular Projection-Redistribution Unlearning as Output Filter for Classification Pipelines](https://arxiv.org/abs/2510.26230)
*Minyi Peng,Darian Gunamardi,Ivan Tjuawinata,Kwok-Yan Lam*

Main category: cs.LG

TL;DR: 提出了一种新的机器学习反学习方法，通过逆转最后的训练序列来实现知识移除，无需完全访问原始数据集或模型。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习反学习方法在实际应用中面临可扩展性问题，并且需要完全访问原始数据集和模型。

Method: 将分类训练视为一个序列过程，通过在模型末尾添加一个投影-重分布层来实现反学习。

Result: 在多个数据集（包括图像和表格数据）上的实验结果表明，该方法在计算成本大大降低的情况下，输出与完全重新训练的模型相似。

Conclusion: 该解决方案具有适用性、可扩展性和系统兼容性，同时在更实用的环境中保持了输出性能。

Abstract: As a new and promising approach, existing machine unlearning (MU) works
typically emphasize theoretical formulations or optimization objectives to
achieve knowledge removal. However, when deployed in real-world scenarios, such
solutions typically face scalability issues and have to address practical
requirements such as full access to original datasets and model. In contrast to
the existing approaches, we regard classification training as a sequential
process where classes are learned sequentially, which we call \emph{inductive
approach}. Unlearning can then be done by reversing the last training sequence.
This is implemented by appending a projection-redistribution layer in the end
of the model. Such an approach does not require full access to the original
dataset or the model, addressing the challenges of existing methods. This
enables modular and model-agnostic deployment as an output filter into existing
classification pipelines with minimal alterations. We conducted multiple
experiments across multiple datasets including image (CIFAR-10/100 using
CNN-based model) and tabular datasets (Covertype using tree-based model).
Experiment results show consistently similar output to a fully retrained model
with a high computational cost reduction. This demonstrates the applicability,
scalability, and system compatibility of our solution while maintaining the
performance of the output in a more practical setting.

</details>
