<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 2]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.LG](#cs.LG) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [DeBERTa-KC: A Transformer-Based Classifier for Knowledge Construction in Online Learning Discourse](https://arxiv.org/abs/2510.19858)
*Jindi Wang,Yidi Zhang,Zhaoxing Li*

Main category: cs.CL

TL;DR: DeBERTa-KC模型用于自动分类在线科学学习讨论中的知识建构 (KC) 层次。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于分析在线科学学习讨论中的知识建构层次。

Method: 该模型基于 DeBERTa-v3，并采用 Focal Loss、Label Smoothing 和 R-Drop 正则化方法。

Result: DeBERTa-KC 在 10 折交叉验证中取得了 0.836 的 macro-F1 分数。

Conclusion: 研究表明，大型语言模型可以有效地捕捉非正式数字学习环境中知识建构的细微指标。

Abstract: This study presents DeBERTa-KC, a transformer-based model for automatic
classification of knowledge construction (KC) levels in online science learning
discourse. Using comments collected from four popular YouTube science channels
(2022--2024), a balanced corpus of 20,000 manually annotated samples was
created across four KC categories: \textit{nonKC}, \textit{Share},
\textit{Explore}, and \textit{Negotiate}. The proposed model extends DeBERTa-v3
with Focal Loss, Label Smoothing, and R-Drop regularization to address class
imbalance and enhance generalization. A reproducible end-to-end pipeline was
implemented, encompassing data extraction, annotation, preprocessing, training,
and evaluation. Across 10-fold stratified cross-validation, DeBERTa-KC achieved
a macro-F1 of $0.836 \pm 0.008$, significantly out-performing both classical
and transformer baselines ($p<0.01$). Per-category results indicate strong
sensitivity to higher-order epistemic engagement, particularly in
\textit{Explore} and \textit{Negotiate} discourse. These findings demonstrate
that large language models can effectively capture nuanced indicators of
knowledge construction in informal digital learning environments, offering
scalable, theory-informed approaches to discourse analysis and the development
of automated tools for assessing epistemic engagement.

</details>


### [2] [An Evaluation of the Pedagogical Soundness and Usability of AI-Generated Lesson Plans Across Different Models and Prompt Frameworks in High-School Physics](https://arxiv.org/abs/2510.19866)
*Xincheng Liu*

Main category: cs.CL

TL;DR: This study assesses the quality of AI-generated lesson plans from five LLMs using three prompt frameworks.


<details>
  <summary>Details</summary>
Motivation: To evaluate the pedagogical soundness and usability of AI-generated lesson plans.

Method: Generated 15 lesson plans for a high-school physics topic using five LLMs and three prompt frameworks, then analyzed them using computational metrics: readability, factual accuracy, curriculum alignment, and cognitive demand.

Result: Model selection most influenced readability, while prompt framework affected factual accuracy and curriculum alignment. The RACE framework yielded the lowest hallucination index. Learning objectives were mostly at the Remember and Understand tiers of Bloom's taxonomy.

Conclusion: Readability depends on the model, while instructional reliability and curricular alignment depend on the prompt framework. Combining a readability-optimized model with the RACE framework and an explicit checklist is most effective.

Abstract: This study evaluates the pedagogical soundness and usability of AI-generated
lesson plans across five leading large language models: ChatGPT (GPT-5), Claude
Sonnet 4.5, Gemini 2.5 Flash, DeepSeek V3.2, and Grok 4. Beyond model choice,
three structured prompt frameworks were tested: TAG (Task, Audience, Goal),
RACE (Role, Audience, Context, Execution), and COSTAR (Context, Objective,
Style, Tone, Audience, Response Format).
  Fifteen lesson plans were generated for a single high-school physics topic,
The Electromagnetic Spectrum. The lesson plans were analyzed through four
automated computational metrics: (1) readability and linguistic complexity, (2)
factual accuracy and hallucination detection, (3) standards and curriculum
alignment, and (4) cognitive demand of learning objectives.
  Results indicate that model selection exerted the strongest influence on
linguistic accessibility, with DeepSeek producing the most readable teaching
plan (FKGL = 8.64) and Claude generating the densest language (FKGL = 19.89).
  The prompt framework structure most strongly affected the factual accuracy
and pedagogical completeness, with the RACE framework yielding the lowest
hallucination index and the highest incidental alignment with NGSS curriculum
standards. Across all models, the learning objectives in the fifteen lesson
plans clustered at the Remember and Understand tiers of Bloom's taxonomy. There
were limited higher-order verbs in the learning objectives extracted.
  Overall, the findings suggest that readability is significantly governed by
model design, while instructional reliability and curricular alignment depend
more on the prompt framework. The most effective configuration for lesson plans
identified in the results was to combine a readability-optimized model with the
RACE framework and an explicit checklist of physics concepts, curriculum
standards, and higher-order objectives.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [3] [Fourier-Based GAN Fingerprint Detection using ResNet50](https://arxiv.org/abs/2510.19840)
*Sai Teja Erukude,Viswa Chaitanya Marella,Suhasnadh Reddy Veluru*

Main category: cs.CV

TL;DR: 本研究利用频域分析和深度学习来区分StyleGAN生成的图像和真实图像。


<details>
  <summary>Details</summary>
Motivation: 由于生成对抗网络(GANs)生成的照片级图像的迅速崛起，对图像取证和需要可靠内容真实性的工业系统提出了严峻的挑战。

Method: 采用二维离散傅里叶变换(2D DFT)将图像转换到频域，然后在这些转换后的图像上训练ResNet50神经网络，以区分真实图像和合成图像。

Result: 该频域模型达到了92.8%的准确率和0.95的AUC，明显优于在原始空域图像上训练的同等模型。

Conclusion: 结果表明，GAN生成的图像具有独特的频域特征或“指纹”，该方法突出了结合信号处理技术和深度学习在增强数字取证和加强工业人工智能系统可信度方面的工业潜力。

Abstract: The rapid rise of photorealistic images produced from Generative Adversarial
Networks (GANs) poses a serious challenge for image forensics and industrial
systems requiring reliable content authenticity. This paper uses
frequency-domain analysis combined with deep learning to solve the problem of
distinguishing StyleGAN-generated images from real ones. Specifically, a
two-dimensional Discrete Fourier Transform (2D DFT) was applied to transform
images into the Fourier domain, where subtle periodic artifacts become
detectable. A ResNet50 neural network is trained on these transformed images to
differentiate between real and synthetic ones. The experiments demonstrate that
the frequency-domain model achieves a 92.8 percent and an AUC of 0.95,
significantly outperforming the equivalent model trained on raw spatial-domain
images. These results indicate that the GAN-generated images have unique
frequency-domain signatures or "fingerprints". The method proposed highlights
the industrial potential of combining signal processing techniques and deep
learning to enhance digital forensics and strengthen the trustworthiness of
industrial AI systems.

</details>


### [4] [Transformed Multi-view 3D Shape Features with Contrastive Learning](https://arxiv.org/abs/2510.19955)
*Márcus Vinícius Lobo Costa,Sherlon Almeida da Silva,Bárbara Caroline Benato,Leo Sampaio Ferraz Ribeiro,Moacir Antonelli Ponti*

Main category: cs.CV

TL;DR: 本文研究了使用最先进的骨干网络与对比监督和自监督学习目标相结合来学习 3D 形状特征的挑战。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉方法难以从 2D 图像中识别 3D 对象，通常需要大量的标记数据并且依赖于可能忽略关键形状关系的卷积神经网络 (CNN)。

Method: 本文采用基于视觉 Transformer (ViT) 的架构，并结合现代对比目标，在下游任务的多视图 3D 分析中取得了有希望的结果，统一了对比和 3D 形状理解管道。

Result: 有监督的对比损失在 ModelNet10 上达到了约 90.6% 的准确率。

Conclusion: 通过 ViT 捕获全局形状语义，并通过对比优化改进局部区分特征，克服了对大量标记数据的需求以及 CNN 在捕获关键形状关系方面的局限性。

Abstract: This paper addresses the challenges in representation learning of 3D shape
features by investigating state-of-the-art backbones paired with both
contrastive supervised and self-supervised learning objectives. Computer vision
methods struggle with recognizing 3D objects from 2D images, often requiring
extensive labeled data and relying on Convolutional Neural Networks (CNNs) that
may overlook crucial shape relationships. Our work demonstrates that Vision
Transformers (ViTs) based architectures, when paired with modern contrastive
objectives, achieve promising results in multi-view 3D analysis on our
downstream tasks, unifying contrastive and 3D shape understanding pipelines.
For example, supervised contrastive losses reached about 90.6% accuracy on
ModelNet10. The use of ViTs and contrastive learning, leveraging ViTs' ability
to understand overall shapes and contrastive learning's effectiveness,
overcomes the need for extensive labeled data and the limitations of CNNs in
capturing crucial shape relationships. The success stems from capturing global
shape semantics via ViTs and refining local discriminative features through
contrastive optimization. Importantly, our approach is empirical, as it is
grounded on extensive experimental evaluation to validate the effectiveness of
combining ViTs with contrastive objectives for 3D representation learning.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [5] [A Quantum-Inspired Algorithm for Solving Sudoku Puzzles and the MaxCut Problem](https://arxiv.org/abs/2510.19835)
*Max B. Zhao,Fei Li*

Main category: cs.AI

TL;DR: 提出了一种受量子启发的算法，用于解决二次无约束二元优化（QUBO）问题。


<details>
  <summary>Details</summary>
Motivation: 该算法旨在寻找Ising自旋玻璃哈密顿量的基态。

Method: 该算法使用矩阵乘积状态（MPS）来表示自旋配置的叠加，并利用离散驱动方案将MPS引导至基态。在每一步中，驱动哈密顿量（包含横向磁场）与问题哈密顿量相结合，以实现自旋翻转并促进量子隧穿。MPS使用密度矩阵重整化组（DMRG）方法进行更新。

Result: 该算法在各种QUBO实例中可靠地识别全局最小值。在来自公开来源的中间级数独谜题（涉及超过200个Ising自旋）和Biq Mac库中的MaxCut问题（成功解决了具有高达251个节点和3,265条边的实例）上证明了其有效性。

Conclusion: 该方法具有可扩展性、通用性和适用于工业级QUBO应用的优点。

Abstract: We propose and evaluate a quantum-inspired algorithm for solving Quadratic
Unconstrained Binary Optimization (QUBO) problems, which are mathematically
equivalent to finding ground states of Ising spin-glass Hamiltonians. The
algorithm employs Matrix Product States (MPS) to compactly represent large
superpositions of spin configurations and utilizes a discrete driving schedule
to guide the MPS toward the ground state. At each step, a driver Hamiltonian --
incorporating a transverse magnetic field -- is combined with the problem
Hamiltonian to enable spin flips and facilitate quantum tunneling. The MPS is
updated using the standard Density Matrix Renormalization Group (DMRG) method,
which iteratively minimizes the system's energy via multiple sweeps across the
spin chain. Despite its heuristic nature, the algorithm reliably identifies
global minima, not merely near-optimal solutions, across diverse QUBO
instances. We first demonstrate its effectiveness on intermediate-level Sudoku
puzzles from publicly available sources, involving over $200$ Ising spins with
long-range couplings dictated by constraint satisfaction. We then apply the
algorithm to MaxCut problems from the Biq Mac library, successfully solving
instances with up to $251$ nodes and $3,265$ edges. We discuss the advantages
of this quantum-inspired approach, including its scalability, generalizability,
and suitability for industrial-scale QUBO applications.

</details>


### [6] [Benchmarking Reasoning Reliability in Artificial Intelligence Models for Energy-System Analysis](https://arxiv.org/abs/2510.19836)
*Eliseo Curcio*

Main category: cs.AI

TL;DR: 本研究提出了一个名为分析可靠性基准（ARB）的框架，用于量化大型语言模型在能源系统分析中的推理可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前能源领域的人工智能和机器学习应用缺乏对分析结论逻辑完整性的验证，现有验证方法侧重于预测准确性或计算效率。

Method: 该基准整合了五个子指标：准确性、推理可靠性、不确定性约束、政策一致性和透明度，并在确定性、概率性和认知情境下，使用开放的技术经济数据集对模型性能进行评估。测试了四个前沿模型（GPT-4/5、Claude 4.5 Sonnet、Gemini 2.5 Pro、Llama 3 70B）。

Result: 结果表明，推理可靠性可以被客观衡量。GPT-4/5 和 Claude 4.5 Sonnet 实现了持续且符合政策的推理（分析可靠性指数大于 90），Gemini 2.5 Pro 表现出适度的稳定性，而 Llama 3 70B 仍低于专业阈值。这些差异具有显著性和可重复性。

Conclusion: ARB 在能源文献中建立了第一个量化方法，用于验证人工智能系统中的因果、概率和策略驱动的推理，为全球能源转型中可信和透明的分析应用提供参考框架。

Abstract: Artificial intelligence and machine learning are increasingly used for
forecasting, optimization, and policy design in the energy sector, yet no
standardized framework exists to evaluate whether these systems reason
correctly. Current validation practices focus on predictive accuracy or
computational efficiency, leaving the logical integrity of analytical
conclusions untested. This study introduces the Analytical Reliability
Benchmark (ARB), a reproducible framework that quantifies reasoning reliability
in large language models applied to energy system analysis. The benchmark
integrates five submetrics: accuracy, reasoning reliability, uncertainty
discipline, policy consistency, and transparency, and evaluates model
performance across deterministic, probabilistic, and epistemic scenarios using
open technoeconomic datasets (NREL ATB 2024, DOE H2A/H2New, IEA WEO 2024). Four
frontier models (GPT-4/5, Claude 4.5 Sonnet, Gemini 2.5 Pro, Llama 3 70B) were
tested under identical factual and regulatory conditions. Results show that
reasoning reliability can be objectively measured. GPT-4/5 and Claude 4.5
Sonnet achieved consistent and policy-compliant reasoning (Analytical
Reliability Index greater than 90), Gemini 2.5 Pro demonstrated moderate
stability, and Llama 3 70B remained below professional thresholds. Statistical
validation confirmed that these differences are significant and reproducible.
The ARB establishes the first quantitative method in the energy literature for
verifying causal, probabilistic, and policy-driven reasoning in artificial
intelligence systems, providing a reference framework for trustworthy and
transparent analytical applications in the global energy transition.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [7] [Query Optimization in the Wild: Realities and Trends](https://arxiv.org/abs/2510.20082)
*Yuanyuan Tian*

Main category: cs.DB

TL;DR: 工业界数据库系统中的查询优化器在近半个世纪里，其核心设计一直保持稳定，但云计算、大数据量和统一数据平台的兴起暴露了传统单体架构的局限性。本文回顾了生产系统中查询优化的过去和现在，并确定了它们今天面临的挑战。然后，本文重点介绍了行业中正在兴起的三大关键趋势，这些趋势有望应对这些挑战：查询优化和查询执行之间更紧密的反馈循环、优化范围从单个查询扩展到整个工作负载，以及从单体设计转向可组合架构。


<details>
  <summary>Details</summary>
Motivation: 云计算、大数据量和统一数据平台的兴起暴露了传统单体查询优化器架构的局限性

Method: 回顾生产系统中查询优化的过去和现在，并确定它们今天面临的挑战。然后重点介绍了行业中正在兴起的三大关键趋势。

Result: 确定了查询优化和查询执行之间更紧密的反馈循环、优化范围从单个查询扩展到整个工作负载，以及从单体设计转向可组合架构这三大趋势。

Conclusion: 这些趋势共同描绘了一个更动态、整体和适应性强的查询优化实践的未来。

Abstract: For nearly half a century, the core design of query optimizers in industrial
database systems has remained remarkably stable, relying on foundational
principles from System R and the Volcano/Cascades framework. However, the rise
of cloud computing, massive data volumes, and unified data platforms has
exposed the limitations of this traditional, monolithic architecture. Taking an
industrial perspective, this paper reviews the past and present of query
optimization in production systems and identifies the challenges they face
today. Then this paper highlights three key trends gaining momentum in the
industry that promise to address these challenges. First, a tighter feedback
loop between query optimization and query execution is being used to improve
the robustness of query performance. Second, the scope of optimization is
expanding from a single query to entire workloads through the convergence of
query optimization and workload optimization. Third, and perhaps most
transformatively, the industry is moving from monolithic designs to composable
architectures that foster agility and cross-engine collaboration. Together,
these trends chart a clear path toward a more dynamic, holistic, and adaptable
future for query optimization in practice.

</details>


### [8] [UREM: A High-performance Unified and Resilient Enhancement Method for Multi- and High-Dimensional Indexes](https://arxiv.org/abs/2510.20110)
*Ming Sheng,Shuliang Wang,Yong Zhang,Yi Luo,Xianbo Liu,Zeming Li*

Main category: cs.DB

TL;DR: 提出了UREM，一种统一且有弹性的增强方法，旨在提升多维和高维索引在不同场景下的查询性能。


<details>
  <summary>Details</summary>
Motivation: 现有的结构导向方法在静态工作负载下表现良好，但在动态工作负载下性能下降；布局导向方法具有良好的通用性，但在静态工作负载下性能欠佳。因此，开发一种统一且有弹性的增强方法来提高不同索引在不同场景下的查询性能是一个公开的挑战。

Method: 提出了UREM，它可以统一应用于各种平台上的不同索引；通过布局优化增强静态工作负载下的索引查询性能；并通过部分布局重组使索引在查询转移时稳定性能。

Result: UREM在静态工作负载下将多维和高维索引的查询性能提高了高达5.73倍和9.18倍，在动态工作负载下平均提高了5.72倍和9.47倍。UREM增强的一些传统索引甚至达到了与最近先进索引相当或超过的性能。

Conclusion: UREM是一种高性能的统一弹性增强方法，适用于多维和高维索引，能够适应不同的场景。

Abstract: Numerous multi- or high-dimensional indexes with distinct advantages have
been proposed on various platforms to meet application requirements. To achieve
higher-performance queries, most indexes employ enhancement methods, including
structure-oriented and layout-oriented enhancement methods. Existing
structure-oriented methods tailored to specific indexes work well under static
workloads but lack generality and degrade under dynamic workloads. The
layout-oriented methods exhibit good generality and perform well under dynamic
workloads, but exhibit suboptimal performance under static workloads.
Therefore, it is an open challenge to develop a unified and resilient
enhancement method that can improve query performance for different indexes
adaptively under different scenarios. In this paper, we propose UREM, which is
the first high-performance Unified and Resilient Enhancement Method designed
for both multi- and high-dimensional indexes, capable of adapting to different
scenarios. Specifically, UREM (1) can be uniformly applied with different
indexes on various platforms; (2) enhances the query performance of indexes by
layout optimization under static workloads; (3) enables indexes to stabilize
performance when queries shift through partial layout reorganization. We
evaluate UREM on 20 widely used indexes. Experimental results demonstrate that
UREM improves the query performance of multi- and high-dimensional indexes by
up to 5.73x and 9.18x under static workloads, and by an average of 5.72x and
9.47x under dynamic workloads. Moreover, some traditional indexes enhanced by
UREM even achieve performance comparable to or even surpassing that of recent
advanced indexes.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [9] [Automating Iconclass: LLMs and RAG for Large-Scale Classification of Religious Woodcuts](https://arxiv.org/abs/2510.19986)
*Drew B. Thomas*

Main category: cs.IR

TL;DR: 本文提出了一种使用大型语言模型（LLM）和向量数据库结合检索增强生成（RAG）对早期现代宗教图像进行分类的新方法。


<details>
  <summary>Details</summary>
Motivation: 利用神圣罗马帝国书籍插图的整页上下文，使LLM能够生成包含视觉和文本元素的详细描述。

Method: 通过混合向量搜索将这些描述与相关的Iconclass代码进行匹配。

Result: 该方法在五个和四个分类级别上分别实现了87%和92%的精度，显著优于传统的图像和基于关键词的搜索。

Conclusion: 通过采用整页描述和RAG，该系统提高了分类精度，为大规模分析早期现代视觉档案提供了一个强大的工具。

Abstract: This paper presents a novel methodology for classifying early modern
religious images by using Large Language Models (LLMs) and vector databases in
combination with Retrieval-Augmented Generation (RAG). The approach leverages
the full-page context of book illustrations from the Holy Roman Empire,
allowing the LLM to generate detailed descriptions that incorporate both visual
and textual elements. These descriptions are then matched to relevant Iconclass
codes through a hybrid vector search. This method achieves 87% and 92%
precision at five and four levels of classification, significantly
outperforming traditional image and keyword-based searches. By employing
full-page descriptions and RAG, the system enhances classification accuracy,
offering a powerful tool for large-scale analysis of early modern visual
archives. This interdisciplinary approach demonstrates the growing potential of
LLMs and RAG in advancing research within art history and digital humanities.

</details>


### [10] [Rank-GRPO: Training LLM-based Conversational Recommender Systems with Reinforcement Learning](https://arxiv.org/abs/2510.20150)
*Yaochen Zhu,Harald Steck,Dawen Liang,Yinhan He,Jundong Li,Nathan Kallus*

Main category: cs.IR

TL;DR: ConvRec-R1是一个用于训练基于LLM的对话推荐系统的两阶段框架，旨在解决LLM生成不符合目录的项目、违反输出格式以及排名质量下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM推荐系统存在生成内容不符合目录、输出格式错误以及排名质量下降等问题。

Method: 该方法包含两个阶段：1) 使用Remap-Reflect-Adjust流水线构建高质量的、基于目录的行为克隆数据集；2) 提出Rank-GRPO，一种针对排名式输出任务的GRPO扩展，通过重新定义奖励和引入排名级别的重要性比率来优化策略更新。

Result: 在Reddit-v2数据集上的实验表明，ConvRec-R1比GRPO基线收敛更快，并实现了更高的Recall和NDCG。

Conclusion: ConvRec-R1框架能够有效地训练基于LLM的对话推荐系统，并在公共数据集上取得了优异的结果。

Abstract: Large language models (LLMs) are reshaping the recommender system paradigm by
enabling users to express preferences and receive recommendations through
conversations. Yet, aligning LLMs to the recommendation task remains
challenging: pretrained LLMs often generate out-of-catalog items, violate
required output formats, and their ranking quality degrades sharply toward the
end of the generated list. To this end, we propose ConvRec-R1, a two-stage
framework for end-to-end training of LLM-based conversational recommender
systems. In Stage 1, we construct a behavioral-cloning dataset with a
Remap-Reflect-Adjust pipeline, which produces high-quality, catalog-grounded
demonstrations from powerful blackbox LLMs to warm-start the RL training. In
Stage 2, we propose Rank-GRPO, a principled extension of group relative policy
optimization (GRPO) tailored to tasks with rank-style outputs. Rank-GRPO treats
each rank in the recommendation list as the unit instead of token (too
fine-grained) or sequence (too coarse), redefining rewards to remove non-causal
credit assignment and introducing a rank-level importance ratio based on the
geometric mean of rank-wise token probabilities to stabilize policy updates.
Experiments on the public Reddit-v2 dataset show that ConvRec-R1 converges
faster and achieves higher Recall and NDCG than GRPO-style baselines. Code and
datasets are released at https://github.com/yaochenzhu/Rank-GRPO.

</details>


### [11] [Multimedia-Aware Question Answering: A Review of Retrieval and Cross-Modal Reasoning Architectures](https://arxiv.org/abs/2510.20193)
*Rahul Raja,Arpita Vats*

Main category: cs.IR

TL;DR: 本综述概述了结合多媒体检索管道的问答系统的最新进展。


<details>
  <summary>Details</summary>
Motivation: 传统问答系统依赖于结构化文本数据，但多媒体内容的快速增长带来了新的挑战和机遇。

Method: 对问答系统进行分类，重点关注检索方法、融合技术和答案生成策略。

Result: 分析了基准数据集、评估协议和性能权衡。

Conclusion: 强调了跨模态对齐、延迟-准确性权衡和语义基础等关键挑战，并概述了开放性问题和未来研究方向。

Abstract: Question Answering (QA) systems have traditionally relied on structured text
data, but the rapid growth of multimedia content (images, audio, video, and
structured metadata) has introduced new challenges and opportunities for
retrieval-augmented QA. In this survey, we review recent advancements in QA
systems that integrate multimedia retrieval pipelines, focusing on
architectures that align vision, language, and audio modalities with user
queries. We categorize approaches based on retrieval methods, fusion
techniques, and answer generation strategies, and analyze benchmark datasets,
evaluation protocols, and performance tradeoffs. Furthermore, we highlight key
challenges such as cross-modal alignment, latency-accuracy tradeoffs, and
semantic grounding, and outline open problems and future research directions
for building more robust and context-aware QA systems leveraging multimedia
data.

</details>


### [12] [Balancing Fine-tuning and RAG: A Hybrid Strategy for Dynamic LLM Recommendation Updates](https://arxiv.org/abs/2510.20260)
*Changping Meng,Hongyi Ling,Jianling Wang,Yifan Liu,Shuzhou Zhang,Dapeng Hong,Mingyan Gao,Onkar Dalal,Ed Chi,Lichan Hong,Haokai Lu,Ningren Han*

Main category: cs.IR

TL;DR: 本研究探讨了大型语言模型（LLM）驱动的推荐系统的更新策略，重点关注持续微调和检索增强生成（RAG）之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型可以通过其先进的推理和规划能力来增强推荐系统。然而，用户兴趣和内容的动态特性提出了一个巨大的挑战：虽然最初的微调使LLM与领域知识和用户偏好保持一致，但它未能捕捉到这种实时变化，因此需要强大的更新机制。

Method: 使用LLM驱动的用户兴趣探索系统作为案例研究，我们对这些方法在成本、敏捷性和知识整合等维度上进行了比较分析。我们提出了一种混合更新策略，该策略利用周期性微调的长期知识适应和低成本RAG的敏捷性。

Result: 通过在十亿用户平台上进行的实时A/B实验表明，这种混合方法在用户满意度方面产生了具有统计意义的改进，为维护高质量的LLM驱动的推荐系统提供了一个实用且具有成本效益的框架。

Conclusion: 结合周期性微调的长期知识适应和低成本RAG的敏捷性的混合更新策略，可以有效提升LLM驱动的推荐系统的用户满意度。

Abstract: Large Language Models (LLMs) empower recommendation systems through their
advanced reasoning and planning capabilities. However, the dynamic nature of
user interests and content poses a significant challenge: While initial
fine-tuning aligns LLMs with domain knowledge and user preferences, it fails to
capture such real-time changes, necessitating robust update mechanisms. This
paper investigates strategies for updating LLM-powered recommenders, focusing
on the trade-offs between ongoing fine-tuning and Retrieval-Augmented
Generation (RAG). Using an LLM-powered user interest exploration system as a
case study, we perform a comparative analysis of these methods across
dimensions like cost, agility, and knowledge incorporation. We propose a hybrid
update strategy that leverages the long-term knowledge adaptation of periodic
fine-tuning with the agility of low-cost RAG. We demonstrate through live A/B
experiments on a billion-user platform that this hybrid approach yields
statistically significant improvements in user satisfaction, offering a
practical and cost-effective framework for maintaining high-quality LLM-powered
recommender systems.

</details>


### [13] [From Generation to Attribution: Music AI Agent Architectures for the Post-Streaming Era](https://arxiv.org/abs/2510.20276)
*Wonil Kim,Hyeongseok Wi,Seungsoon Park,Taejun Kim,Sangeun Keum,Keunhyoung Kim,Taewan Kim,Jongmin Jung,Taehyoung Kim,Gaetan Guerrero,Mael Le Goff,Julie Po,Dongjoo Moon,Juhan Nam,Jongpil Lee*

Main category: cs.IR

TL;DR: Generative AI is changing music creation but current systems can't handle it. The authors propose a new system for fair attribution and compensation.


<details>
  <summary>Details</summary>
Motivation: Current streaming systems can't handle the scale and complexity of AI-driven music production.

Method: A content-based Music AI Agent architecture that embeds attribution directly into the creative workflow through block-level retrieval and agentic orchestration.

Result: The system enables fine-grained attribution, equitable compensation, and participatory engagement.

Conclusion: The proposed framework reframes AI from a generative tool into infrastructure for a Fair AI Media Platform, enabling a collaborative and adaptive ecosystem.

Abstract: Generative AI is reshaping music creation, but its rapid growth exposes
structural gaps in attribution, rights management, and economic models. Unlike
past media shifts, from live performance to recordings, downloads, and
streaming, AI transforms the entire lifecycle of music, collapsing boundaries
between creation, distribution, and monetization. However, existing streaming
systems, with opaque and concentrated royalty flows, are ill-equipped to handle
the scale and complexity of AI-driven production. We propose a content-based
Music AI Agent architecture that embeds attribution directly into the creative
workflow through block-level retrieval and agentic orchestration. Designed for
iterative, session-based interaction, the system organizes music into granular
components (Blocks) stored in BlockDB; each use triggers an Attribution Layer
event for transparent provenance and real-time settlement. This framework
reframes AI from a generative tool into infrastructure for a Fair AI Media
Platform. By enabling fine-grained attribution, equitable compensation, and
participatory engagement, it points toward a post-streaming paradigm where
music functions not as a static catalog but as a collaborative and adaptive
ecosystem.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [14] [Some Attention is All You Need for Retrieval](https://arxiv.org/abs/2510.19861)
*Felix Michalak,Steven Abreu*

Main category: cs.LG

TL;DR: 混合 SSM-Transformer 架构中，检索完全依赖于自注意力层。


<details>
  <summary>Details</summary>
Motivation: 研究混合架构中不同组件的功能分工。

Method: 通过消融实验和稀疏化注意力头来分析检索性能。

Result: 自注意力消融导致检索失败，而 SSM 层没有补偿机制。稀疏化注意力头在保持检索性能的同时，基本不影响 MMLU 性能。

Conclusion: 混合架构中的自注意力和 SSM 层是专门化的模块，而不是集成系统。

Abstract: We demonstrate complete functional segregation in hybrid SSM-Transformer
architectures: retrieval depends exclusively on self-attention layers. Across
RecurrentGemma-2B/9B and Jamba-Mini-1.6, attention ablation causes catastrophic
retrieval failure (0% accuracy), while SSM layers show no compensatory
mechanisms even with improved prompting. Conversely, sparsifying attention to
just 15% of heads maintains near-perfect retrieval while preserving 84% MMLU
performance, suggesting self-attention specializes primarily for retrieval
tasks. We identify precise mechanistic requirements for retrieval: needle
tokens must be exposed during generation and sufficient context must be
available during prefill or generation. This strict functional specialization
challenges assumptions about redundancy in hybrid architectures and suggests
these models operate as specialized modules rather than integrated systems,
with immediate implications for architecture optimization and interpretability.

</details>


### [15] [An Integrated Approach to Neural Architecture Search for Deep Q-Networks](https://arxiv.org/abs/2510.19872)
*Iman Rahmani,Saman Yazdannik,Morteza Tayefi,Jafar Roshanian*

Main category: cs.LG

TL;DR: 提出了一种名为 NAS-DQN 的深度强化学习方法，该方法可以在训练过程中动态调整神经网络结构。


<details>
  <summary>Details</summary>
Motivation: 传统的深度强化学习智能体的性能受到神经网络结构的限制，而这种结构通常是通过昂贵的超参数搜索来选择的，并在整个训练过程中保持不变。本文旨在研究在线自适应架构优化是否可以打破这种限制，并超越静态设计。

Method: 将学习到的神经架构搜索控制器直接集成到 DRL 训练循环中，从而能够根据累积性能反馈进行动态网络重构。

Result: NAS-DQN 在最终性能、样本效率和策略稳定性方面均优于三种固定架构基线和随机搜索对照，且计算开销可忽略不计。

Conclusion: 架构自适应不仅有益，而且对于在线深度强化学习中的最佳样本效率是必要的。

Abstract: The performance of deep reinforcement learning agents is fundamentally
constrained by their neural network architecture, a choice traditionally made
through expensive hyperparameter searches and then fixed throughout training.
This work investigates whether online, adaptive architecture optimization can
escape this constraint and outperform static designs. We introduce NAS-DQN, an
agent that integrates a learned neural architecture search controller directly
into the DRL training loop, enabling dynamic network reconfiguration based on
cumulative performance feedback. We evaluate NAS-DQN against three
fixed-architecture baselines and a random search control on a continuous
control task, conducting experiments over multiple random seeds. Our results
demonstrate that NAS-DQN achieves superior final performance, sample
efficiency, and policy stability while incurring negligible computational
overhead. Critically, the learned search strategy substantially outperforms
both undirected random architecture exploration and poorly-chosen fixed
designs, indicating that intelligent, performance-guided search is the key
mechanism driving success. These findings establish that architecture
adaptation is not merely beneficial but necessary for optimal sample efficiency
in online deep reinforcement learning, and suggest that the design of RL agents
need not be a static offline choice but can instead be seamlessly integrated as
a dynamic component of the learning process itself.

</details>
