{"id": "2507.21103", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.21103", "abs": "https://arxiv.org/abs/2507.21103", "authors": ["Daniel Meireles do Rego"], "title": "Analise Semantica Automatizada com LLM e RAG para Bulas Farmaceuticas", "comment": "in Portuguese language", "summary": "The production of digital documents has been growing rapidly in academic,\nbusiness, and health environments, presenting new challenges in the efficient\nextraction and analysis of unstructured information. This work investigates the\nuse of RAG (Retrieval-Augmented Generation) architectures combined with\nLarge-Scale Language Models (LLMs) to automate the analysis of documents in PDF\nformat. The proposal integrates vector search techniques by embeddings,\nsemantic data extraction and generation of contextualized natural language\nresponses. To validate the approach, we conducted experiments with drug package\ninserts extracted from official public sources. The semantic queries applied\nwere evaluated by metrics such as accuracy, completeness, response speed and\nconsistency. The results indicate that the combination of RAG with LLMs offers\nsignificant gains in intelligent information retrieval and interpretation of\nunstructured technical texts.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86RAG\u4e0eLLM\u7ed3\u5408\u7528\u4e8e\u81ea\u52a8\u5316\u5206\u6790PDF\u6587\u6863\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u79cd\u7ed3\u5408\u5728\u667a\u80fd\u4fe1\u606f\u68c0\u7d22\u548c\u89e3\u91ca\u975e\u7ed3\u6784\u5316\u6587\u672c\u65b9\u9762\u6709\u663e\u8457\u6536\u76ca\u3002", "motivation": "\u6570\u5b57\u6587\u6863\u7684\u4ea7\u751f\u5728\u5b66\u672f\u3001\u5546\u4e1a\u548c\u5065\u5eb7\u73af\u5883\u4e2d\u8fc5\u901f\u589e\u957f\uff0c\u8fd9\u7ed9\u975e\u7ed3\u6784\u5316\u4fe1\u606f\u7684\u6709\u6548\u63d0\u53d6\u548c\u5206\u6790\u5e26\u6765\u4e86\u65b0\u7684\u6311\u6218\u3002", "method": "RAG (\u68c0\u7d22\u589e\u5f3a\u751f\u6210) \u67b6\u6784\u4e0e\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b (LLM) \u76f8\u7ed3\u5408\uff0c\u4ee5\u81ea\u52a8\u5316\u5206\u6790 PDF \u683c\u5f0f\u7684\u6587\u6863\u3002\u8be5\u65b9\u6848\u96c6\u6210\u4e86\u901a\u8fc7\u5d4c\u5165\u7684\u5411\u91cf\u641c\u7d22\u6280\u672f\u3001\u8bed\u4e49\u6570\u636e\u63d0\u53d6\u548c\u4e0a\u4e0b\u6587\u81ea\u7136\u8bed\u8a00\u54cd\u5e94\u7684\u751f\u6210\u3002", "result": "\u901a\u8fc7\u5b98\u65b9\u516c\u5171\u6765\u6e90\u63d0\u53d6\u7684\u836f\u54c1\u5305\u88c5\u63d2\u4ef6\u8fdb\u884c\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRAG\u4e0eLLM\u7684\u7ed3\u5408\u5728\u667a\u80fd\u4fe1\u606f\u68c0\u7d22\u548c\u975e\u7ed3\u6784\u5316\u6280\u672f\u6587\u672c\u7684\u89e3\u91ca\u65b9\u9762\u63d0\u4f9b\u4e86\u663e\u8457\u7684\u6536\u76ca\u3002", "conclusion": "RAG\u4e0eLLM\u7684\u7ed3\u5408\u5728\u667a\u80fd\u4fe1\u606f\u68c0\u7d22\u548c\u975e\u7ed3\u6784\u5316\u6280\u672f\u6587\u672c\u7684\u89e3\u91ca\u65b9\u9762\u63d0\u4f9b\u4e86\u663e\u8457\u7684\u6536\u76ca\u3002"}}
{"id": "2507.21105", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.21105", "abs": "https://arxiv.org/abs/2507.21105", "authors": ["Callie C. Liao", "Duoduo Liao", "Sai Surya Gadiraju"], "title": "AgentMaster: A Multi-Agent Conversational Framework Using A2A and MCP Protocols for Multimodal Information Retrieval and Analysis", "comment": null, "summary": "The rise of Multi-Agent Systems (MAS) in Artificial Intelligence (AI),\nespecially integrated with Large Language Models (LLMs), has greatly\nfacilitated the resolution of complex tasks. However, current systems are still\nfacing challenges of inter-agent communication, coordination, and interaction\nwith heterogeneous tools and resources. Most recently, the Model Context\nProtocol (MCP) by Anthropic and Agent-to-Agent (A2A) communication protocol by\nGoogle have been introduced, and to the best of our knowledge, very few\napplications exist where both protocols are employed within a single MAS\nframework. We present a pilot study of AgentMaster, a novel modular\nmulti-protocol MAS framework with self-implemented A2A and MCP, enabling\ndynamic coordination and flexible communication. Through a unified\nconversational interface, the system supports natural language interaction\nwithout prior technical expertise and responds to multimodal queries for tasks\nincluding information retrieval, question answering, and image analysis.\nEvaluation through the BERTScore F1 and LLM-as-a-Judge metric G-Eval averaged\n96.3\\% and 87.1\\%, revealing robust inter-agent coordination, query\ndecomposition, dynamic routing, and domain-specific, relevant responses.\nOverall, our proposed framework contributes to the potential capabilities of\ndomain-specific, cooperative, and scalable conversational AI powered by MAS.", "AI": {"tldr": "AgentMaster, a new MAS framework using A2A and MCP protocols, improves inter-agent communication and coordination, leading to better conversational AI performance.", "motivation": "Current MAS systems face challenges in inter-agent communication, coordination, and interaction with heterogeneous tools and resources. Existing protocols like MCP and A2A are rarely used together.", "method": "The paper introduces AgentMaster, a modular multi-protocol MAS framework with self-implemented A2A and MCP.", "result": "Evaluation using BERTScore F1 and G-Eval metrics averaged 96.3% and 87.1%, demonstrating robust inter-agent coordination, query decomposition, dynamic routing, and domain-specific responses.", "conclusion": "The AgentMaster framework contributes to domain-specific, cooperative, and scalable conversational AI by using MAS."}}
{"id": "2507.21114", "categories": ["cs.IR", "cs.AI", "cs.CV", "68T10, 68T09, 62H30", "I.7.5; H.3.7"], "pdf": "https://arxiv.org/pdf/2507.21114", "abs": "https://arxiv.org/abs/2507.21114", "authors": ["Kateryna Lutsai", "Pavel Stra\u0148\u00e1k"], "title": "Page image classification for content-specific data processing", "comment": "65 pages, 57 figures, 20 tables", "summary": "Digitization projects in humanities often generate vast quantities of page\nimages from historical documents, presenting significant challenges for manual\nsorting and analysis. These archives contain diverse content, including various\ntext types (handwritten, typed, printed), graphical elements (drawings, maps,\nphotos), and layouts (plain text, tables, forms). Efficiently processing this\nheterogeneous data requires automated methods to categorize pages based on\ntheir content, enabling tailored downstream analysis pipelines. This project\naddresses this need by developing and evaluating an image classification system\nspecifically designed for historical document pages, leveraging advancements in\nartificial intelligence and machine learning. The set of categories was chosen\nto facilitate content-specific processing workflows, separating pages requiring\ndifferent analysis techniques (e.g., OCR for text, image analysis for graphics)", "AI": {"tldr": "This paper develops an image classification system for historical documents to categorize pages based on their content, enabling tailored downstream analysis pipelines.", "motivation": "digitization projects in humanities generate vast quantities of page images from historical documents, presenting significant challenges for manual sorting and analysis", "method": "leveraging advancements in artificial intelligence and machine learning", "result": "the set of categories was chosen to facilitate content-specific processing workflows, separating pages requiring different analysis techniques", "conclusion": "an image classification system is developed for historical document pages"}}
{"id": "2507.21115", "categories": ["cs.IR", "cs.AI", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21115", "abs": "https://arxiv.org/abs/2507.21115", "authors": ["Sven Lankester", "Manel Slokom", "Gustavo de Carvalho Bertoli", "Matias Vizcaino", "Emmanuelle Beauxis Aussalet", "Laura Hollink"], "title": "FedFlex: Federated Learning for Diverse Netflix Recommendations", "comment": null, "summary": "Federated learning is a decentralized approach that enables collaborative\nmodel training across multiple devices while preserving data privacy. It has\nshown significant potential in various domains, including healthcare and\npersonalized recommendation systems. However, most existing work on federated\nrecommendation systems has focused primarily on improving accuracy, with\nlimited attention to fairness and diversity. In this paper, we introduce\nFedFlex, a federated recommender system for Netflix-style TV series\nrecommendations. FedFlex integrates two state-of-the-art matrix factorization\nalgorithms for personalized fine-tuning. FedFlex also applies Maximal Marginal\nRelevance (MMR) to re-rank items and enhance diversity. We conduct extensive\nexperiments comparing recommendations generated by SVD and BPR algorithms. In a\nlive two-week user study, participants received two recommendation lists: List\nA, based on SVD or BPR, and List B, a re-ranked version emphasizing diversity.\nParticipants were asked to click on the movies they were interested in\nwatching. Our findings demonstrate that FedFlex effectively introduces diverse\ncontent, such as new genres, into recommendations without necessarily\ncompromising user satisfaction.", "AI": {"tldr": "FedFlex\u662f\u4e00\u79cd\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\uff0c\u5b83\u96c6\u6210\u4e86\u77e9\u9635\u5206\u89e3\u7b97\u6cd5\u548cMMR\u6765\u63d0\u9ad8\u63a8\u8350\u7684\u591a\u6837\u6027\uff0c\u800c\u4e0d\u4f1a\u964d\u4f4e\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "motivation": "\u73b0\u6709\u5173\u4e8e\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u7684\u5927\u90e8\u5206\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728\u63d0\u9ad8\u51c6\u786e\u6027\u4e0a\uff0c\u800c\u5bf9\u516c\u5e73\u6027\u548c\u591a\u6837\u6027\u7684\u5173\u6ce8\u6709\u9650\u3002", "method": "FedFlex\u96c6\u6210\u4e86\u4e24\u79cd\u5148\u8fdb\u7684\u77e9\u9635\u5206\u89e3\u7b97\u6cd5\uff0c\u7528\u4e8e\u4e2a\u6027\u5316\u5fae\u8c03\uff0c\u5e76\u5e94\u7528\u6700\u5927\u8fb9\u7f18\u76f8\u5173\u6027\uff08MMR\uff09\u6765\u91cd\u65b0\u6392\u5e8f\u9879\u76ee\u5e76\u589e\u5f3a\u591a\u6837\u6027\u3002", "result": "\u5728\u4e3a\u671f\u4e24\u5468\u7684\u5b9e\u9645\u7528\u6237\u7814\u7a76\u4e2d\uff0c\u53c2\u4e0e\u8005\u6536\u5230\u4e86\u4e24\u4e2a\u63a8\u8350\u5217\u8868\uff1a\u5217\u8868A\uff0c\u57fa\u4e8eSVD\u6216BPR\uff0c\u4ee5\u53ca\u5217\u8868B\uff0c\u4e00\u4e2a\u5f3a\u8c03\u591a\u6837\u6027\u7684\u91cd\u65b0\u6392\u5e8f\u7248\u672c\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cFedFlex\u6709\u6548\u5730\u5f15\u5165\u4e86\u591a\u6837\u5316\u7684\u5185\u5bb9\u3002", "conclusion": "FedFlex\u5728\u4e0d\u5f71\u54cd\u7528\u6237\u6ee1\u610f\u5ea6\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u5730\u5c06\u591a\u6837\u5316\u7684\u5185\u5bb9\uff08\u5982\u65b0\u7c7b\u578b\uff09\u5f15\u5165\u63a8\u8350\u3002"}}
{"id": "2507.21056", "categories": ["cs.DB", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21056", "abs": "https://arxiv.org/abs/2507.21056", "authors": ["Harshraj Bhoite"], "title": "AI-Driven Generation of Data Contracts in Modern Data Engineering Systems", "comment": null, "summary": "Data contracts formalize agreements between data producers and consumers\nregarding schema, semantics, and quality expectations. As data pipelines grow\nin complexity, manual authoring and maintenance of contracts becomes\nerror-prone and labor-intensive. We present an AI-driven framework for\nautomatic data contract generation using large language models (LLMs). Our\nsystem leverages parameter-efficient fine-tuning methods, including LoRA and\nPEFT, to adapt LLMs to structured data domains. The models take sample data or\nschema descriptions and output validated contract definitions in formats such\nas JSON Schema and Avro. We integrate this framework into modern data platforms\n(e.g., Databricks, Snowflake) to automate contract enforcement at scale.\nExperimental results on synthetic and real-world datasets demonstrate that the\nfine-tuned LLMs achieve high accuracy in generating valid contracts and reduce\nmanual workload by over 70%. We also discuss key challenges such as\nhallucination, version control, and the need for continuous learning. This work\ndemonstrates that generative AI can enable scalable, agile data governance by\nbridging the gap between intent and implementation in enterprise data\nmanagement.", "AI": {"tldr": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u81ea\u52a8\u751f\u6210\u6570\u636e\u5408\u540c\uff0c\u4ece\u800c\u51cf\u5c11\u624b\u52a8\u5de5\u4f5c\u91cf\u3002", "motivation": "\u968f\u7740\u6570\u636e\u7ba1\u9053\u590d\u6742\u6027\u7684\u589e\u52a0\uff0c\u624b\u52a8\u7f16\u5199\u548c\u7ef4\u62a4\u5408\u540c\u5bb9\u6613\u51fa\u9519\u4e14 labor-intensive\u3002", "method": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u81ea\u52a8\u6570\u636e\u5408\u540c\u751f\u6210\u7684\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u6846\u67b6\uff0c\u5229\u7528\u53c2\u6570\u9ad8\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\uff08\u5305\u62ec LoRA \u548c PEFT\uff09\u6765\u4f7f LLM \u9002\u5e94\u7ed3\u6784\u5316\u6570\u636e\u9886\u57df\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5fae\u8c03\u7684 LLM \u5728\u751f\u6210\u6709\u6548\u5408\u540c\u65b9\u9762\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\uff0c\u5e76\u5c06\u624b\u52a8\u5de5\u4f5c\u91cf\u51cf\u5c11\u4e86 70% \u4ee5\u4e0a\u3002", "conclusion": "\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u53ef\u4ee5\u901a\u8fc7\u5f25\u5408\u4f01\u4e1a\u6570\u636e\u7ba1\u7406\u4e2d\u610f\u56fe\u548c\u5b9e\u65bd\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4ece\u800c\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u654f\u6377\u7684\u6570\u636e\u6cbb\u7406\u3002"}}
{"id": "2507.21058", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21058", "abs": "https://arxiv.org/abs/2507.21058", "authors": ["Kerem Keskin", "M\u00fcmine Kaya Kele\u015f"], "title": "Categorical Classification of Book Summaries Using Word Embedding Techniques", "comment": "in Turkish language. This paper was published in the proceedings of\n  the 6th International Conference on Data Science and Applications ICONDATA24,\n  held on September between 2 and 6, 2024, in Pristina, Kosovo. For full text\n  book see https://www.icondata.org/en/proceedings-books", "summary": "In this study, book summaries and categories taken from book sites were\nclassified using word embedding methods, natural language processing techniques\nand machine learning algorithms. In addition, one hot encoding, Word2Vec and\nTerm Frequency - Inverse Document Frequency (TF-IDF) methods, which are\nfrequently used word embedding methods were used in this study and their\nsuccess was compared. Additionally, the combination table of the pre-processing\nmethods used is shown and added to the table. Looking at the results, it was\nobserved that Support Vector Machine, Naive Bayes and Logistic Regression\nModels and TF-IDF and One-Hot Encoder word embedding techniques gave more\nsuccessful results for Turkish texts.", "AI": {"tldr": "\u672c\u4e66\u5229\u7528\u8bcd\u5d4c\u5165\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5bf9\u4e66\u7c4d\u6458\u8981\u8fdb\u884c\u5206\u7c7b\uff0c\u53d1\u73b0\u652f\u6301\u5411\u91cf\u673a\u3001\u6734\u7d20\u8d1d\u53f6\u65af\u548c\u903b\u8f91\u56de\u5f52\u6a21\u578b\u4ee5\u53ca TF-IDF \u548c One-Hot Encoder \u8bcd\u5d4c\u5165\u6280\u672f\u5bf9\u571f\u8033\u5176\u6587\u672c\u6548\u679c\u66f4\u597d\u3002", "motivation": "\u5229\u7528\u4e66\u7c4d\u7f51\u7ad9\u7684\u4e66\u7c4d\u6458\u8981\u548c\u7c7b\u522b\u8fdb\u884c\u5206\u7c7b\u3002", "method": "\u8bcd\u5d4c\u5165\u65b9\u6cd5\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u548c\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u5305\u62ec one hot encoding\u3001Word2Vec \u548c Term Frequency - Inverse Document Frequency (TF-IDF)\u3002", "result": "\u652f\u6301\u5411\u91cf\u673a\u3001\u6734\u7d20\u8d1d\u53f6\u65af\u548c\u903b\u8f91\u56de\u5f52\u6a21\u578b\u4ee5\u53ca TF-IDF \u548c One-Hot Encoder \u8bcd\u5d4c\u5165\u6280\u672f\u4e3a\u571f\u8033\u5176\u6587\u672c\u63d0\u4f9b\u4e86\u66f4\u6210\u529f\u7684\u7ed3\u679c\u3002", "conclusion": "SVM, \u6734\u7d20\u8d1d\u53f6\u65af\u548c\u903b\u8f91\u56de\u5f52\u6a21\u578b\u4ee5\u53ca TF-IDF \u548c One-Hot Encoder \u8bcd\u5d4c\u5165\u6280\u672f\u4e3a\u571f\u8033\u5176\u6587\u672c\u63d0\u4f9b\u4e86\u66f4\u6210\u529f\u7684\u7ed3\u679c\u3002"}}
{"id": "2507.21069", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.21069", "abs": "https://arxiv.org/abs/2507.21069", "authors": ["Andreas Spilz", "Heiko Oppel", "Jochen Werner", "Kathrin Stucke-Straub", "Felix Capanni", "Michael Munz"], "title": "GAITEX: Human motion dataset from impaired gait and rehabilitation exercises of inertial and optical sensor data", "comment": null, "summary": "Wearable inertial measurement units (IMUs) offer a cost-effective and\nscalable means to assess human movement quality in clinical and everyday\nsettings. However, the development of robust sensor-based classification models\nfor physiotherapeutic exercises and gait analysis requires large, diverse\ndatasets, which are costly and time-consuming to collect. Here, we present a\nmultimodal dataset of physiotherapeutic exercises - including correct and\nclinically relevant variants - and gait-related exercises - including both\nnormal and impaired gait patterns - recorded from 19 participants using\nsynchronized IMUs and marker-based motion capture (MoCap). The dataset includes\nraw data from nine IMUs and thirty-five optical markers capturing full-body\nkinematics. Each IMU is additionally equipped with four optical markers,\nenabling precise comparison between IMU-derived orientation estimates and\nreference values from the MoCap system. To support further analysis, we also\nprovide processed IMU orientations aligned with common segment coordinate\nsystems, subject-specific OpenSim models, inverse kinematics results, and tools\nfor visualizing IMU orientations in the musculoskeletal context. Detailed\nannotations of movement execution quality and time-stamped segmentations\nsupport diverse analysis goals. This dataset supports the development and\nbenchmarking of machine learning models for tasks such as automatic exercise\nevaluation, gait analysis, temporal activity segmentation, and biomechanical\nparameter estimation. To facilitate reproducibility, we provide code for\npostprocessing, sensor-to-segment alignment, inverse kinematics computation,\nand technical validation. This resource is intended to accelerate research in\nmachine learning-driven human movement analysis.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u4eba\u4f53\u8fd0\u52a8\u5206\u6790\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u7406\u7597\u7ec3\u4e60\u548c\u6b65\u6001\u6570\u636e\uff0c\u65e8\u5728\u52a0\u901f\u673a\u5668\u5b66\u4e60\u9a71\u52a8\u7684\u4eba\u4f53\u8fd0\u52a8\u5206\u6790\u7814\u7a76\u3002", "motivation": "\u5f00\u53d1\u7528\u4e8e\u7406\u7597\u7ec3\u4e60\u548c\u6b65\u6001\u5206\u6790\u7684\u57fa\u4e8e\u4f20\u611f\u5668\u7684\u7a33\u5065\u5206\u7c7b\u6a21\u578b\u9700\u8981\u5927\u578b\u3001\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\uff0c\u800c\u8fd9\u4e9b\u6570\u636e\u96c6\u7684\u6536\u96c6\u6210\u672c\u9ad8\u4e14\u8017\u65f6\u3002", "method": "\u4f7f\u7528\u540c\u6b65 IMU \u548c\u57fa\u4e8e\u6807\u8bb0\u7684\u8fd0\u52a8\u6355\u6349 (MoCap) \u4ece 19 \u540d\u53c2\u4e0e\u8005\u8bb0\u5f55\u7684\u7406\u7597\u7ec3\u4e60\uff08\u5305\u62ec\u6b63\u786e\u548c\u4e34\u5e8a\u76f8\u5173\u7684\u53d8\u4f53\uff09\u548c\u6b65\u6001\u76f8\u5173\u7ec3\u4e60\uff08\u5305\u62ec\u6b63\u5e38\u548c\u53d7\u635f\u7684\u6b65\u6001\u6a21\u5f0f\uff09\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u3002", "result": "\u8be5\u6570\u636e\u96c6\u5305\u62ec\u6765\u81ea 9 \u4e2a IMU \u548c 35 \u4e2a\u5149\u5b66\u6807\u8bb0\u7684\u539f\u59cb\u6570\u636e\uff0c\u8fd9\u4e9b\u6570\u636e\u6355\u83b7\u4e86\u5168\u8eab\u8fd0\u52a8\u5b66\u3002\u6bcf\u4e2a IMU \u8fd8\u914d\u5907\u4e86 4 \u4e2a\u5149\u5b66\u6807\u8bb0\uff0c\u4ece\u800c\u53ef\u4ee5\u7cbe\u786e\u6bd4\u8f83 IMU \u884d\u751f\u7684\u65b9\u5411\u4f30\u8ba1\u503c\u4e0e MoCap \u7cfb\u7edf\u7684\u53c2\u8003\u503c\u3002\u4e3a\u4e86\u652f\u6301\u8fdb\u4e00\u6b65\u5206\u6790\uff0c\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e0e\u5e38\u89c1\u7247\u6bb5\u5750\u6807\u7cfb\u5bf9\u9f50\u7684\u7ecf\u8fc7\u5904\u7406\u7684 IMU \u65b9\u5411\u3001\u7279\u5b9a\u4e8e\u4e3b\u4f53\u7684 OpenSim \u6a21\u578b\u3001\u9006\u8fd0\u52a8\u5b66\u7ed3\u679c\u4ee5\u53ca\u7528\u4e8e\u5728\u808c\u8089\u9aa8\u9abc\u73af\u5883\u4e2d\u53ef\u89c6\u5316 IMU \u65b9\u5411\u7684\u5de5\u5177\u3002\u8fd0\u52a8\u6267\u884c\u8d28\u91cf\u548c\u5e26\u65f6\u95f4\u6233\u7684\u5206\u5272\u7684\u8be6\u7ec6\u6ce8\u91ca\u652f\u6301\u591a\u6837\u5316\u7684\u5206\u6790\u76ee\u6807\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u652f\u6301\u5f00\u53d1\u548c\u8bc4\u4f30\u7528\u4e8e\u81ea\u52a8\u8fd0\u52a8\u8bc4\u4f30\u3001\u6b65\u6001\u5206\u6790\u3001\u65f6\u95f4\u6d3b\u52a8\u5206\u5272\u548c\u751f\u7269\u529b\u5b66\u53c2\u6570\u4f30\u8ba1\u7b49\u4efb\u52a1\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u7528\u4e8e\u540e\u5904\u7406\u3001\u4f20\u611f\u5668\u5230\u7247\u6bb5\u5bf9\u9f50\u3001\u9006\u8fd0\u52a8\u5b66\u8ba1\u7b97\u548c\u6280\u672f\u9a8c\u8bc1\u7684\u4ee3\u7801\uff0c\u65e8\u5728\u52a0\u901f\u673a\u5668\u5b66\u4e60\u9a71\u52a8\u7684\u4eba\u4f53\u8fd0\u52a8\u5206\u6790\u7814\u7a76\u3002"}}
{"id": "2507.21067", "categories": ["cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.21067", "abs": "https://arxiv.org/abs/2507.21067", "authors": ["Jan Kapusta"], "title": "SynLang and Symbiotic Epistemology: A Manifesto for Conscious Human-AI Collaboration", "comment": "32 pages, 4 figures. Includes 2 Appendices containing SynLang v1.2.0\n  protocol specification, and formal BNF grammar", "summary": "Current AI systems rely on opaque reasoning processes that hinder human\noversight and collaborative potential. Conventional explainable AI approaches\noffer post-hoc justifications and often fail to establish genuine symbiotic\ncollaboration. In this paper, the Symbiotic Epistemology is presented as a\nphilosophical foundation for human-AI cognitive partnerships. Unlike frameworks\nthat treat AI as a mere tool or replacement, symbiotic epistemology positions\nAI as a reasoning partner, fostering calibrated trust by aligning human\nconfidence with AI reliability through explicit reasoning patterns and\nconfidence assessments. SynLang (Symbiotic Syntactic Language) is introduced as\na formal protocol for transparent human-AI collaboration. The framework is\nempirically validated through actual human-AI dialogues demonstrating AI's\nadaptation to structured reasoning protocols and successful metacognitive\nintervention. The protocol defines two complementary mechanisms: TRACE for\nhigh-level reasoning patterns and TRACE_FE for detailed factor explanations. It\nalso integrates confidence quantification, declarative control over AI\nbehavior, and context inheritance for multi-agent coordination. By structuring\ncommunication and embedding confidence-calibrated transparency, SynLang,\ntogether with symbiotic epistemology, enables AI systems that enhance human\nintelligence, preserve human agency, and uphold ethical accountability in\ncollaborative decision-making. Through dual-level transparency, beginning with\nhigh-level reasoning patterns and progressing to granular explanations, the\nprotocol facilitates rapid comprehension and supports thorough verification of\nAI decision-making.", "AI": {"tldr": "This paper introduces Symbiotic Epistemology and SynLang for transparent human-AI collaboration, fostering calibrated trust and enhancing human intelligence.", "motivation": "Current AI systems rely on opaque reasoning processes that hinder human oversight and collaborative potential. Conventional explainable AI approaches offer post-hoc justifications and often fail to establish genuine symbiotic collaboration.", "method": "The framework is empirically validated through actual human-AI dialogues demonstrating AI's adaptation to structured reasoning protocols and successful metacognitive intervention. The protocol defines two complementary mechanisms: TRACE for high-level reasoning patterns and TRACE_FE for detailed factor explanations.", "result": "The protocol facilitates rapid comprehension and supports thorough verification of AI decision-making through dual-level transparency, beginning with high-level reasoning patterns and progressing to granular explanations.", "conclusion": "SynLang, combined with symbiotic epistemology, enables AI systems that enhance human intelligence, preserve human agency, and uphold ethical accountability in collaborative decision-making."}}
{"id": "2507.21109", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21109", "abs": "https://arxiv.org/abs/2507.21109", "authors": ["Prital Bamnodkar"], "title": "Task-Focused Consolidation with Spaced Recall: Making Neural Networks learn like college students", "comment": null, "summary": "Deep Neural Networks often suffer from a critical limitation known as\nCatastrophic Forgetting, where performance on past tasks degrades after\nlearning new ones. This paper introduces a novel continual learning approach\ninspired by human learning strategies like Active Recall, Deliberate Practice\nand Spaced Repetition, named Task Focused Consolidation with Spaced Recall\n(TFC-SR). TFC-SR enhances the standard experience replay with a mechanism we\ntermed the Active Recall Probe. It is a periodic, task-aware evaluation of the\nmodel's memory that stabilizes the representations of past knowledge. We test\nTFC-SR on the Split MNIST and Split CIFAR-100 benchmarks against leading\nregularization-based and replay-based baselines. Our results show that TFC-SR\nperforms significantly better than these methods. For instance, on the Split\nCIFAR-100, it achieves a final accuracy of 13.17% compared to standard replay's\n7.40%. We demonstrate that this advantage comes from the stabilizing effect of\nthe probe itself, and not from the difference in replay volume. Additionally,\nwe analyze the trade-off between memory size and performance and show that\nwhile TFC-SR performs better in memory-constrained environments, higher replay\nvolume is still more effective when available memory is abundant. We conclude\nthat TFC-SR is a robust and efficient approach, highlighting the importance of\nintegrating active memory retrieval mechanisms into continual learning systems.", "AI": {"tldr": "TFC-SR\u901a\u8fc7\u4e3b\u52a8\u8bb0\u5fc6\u68c0\u7d22\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6301\u7eed\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u5185\u5b58\u53d7\u9650\u7684\u73af\u5883\u4e2d\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5e38\u5e38\u906d\u53d7\u707e\u96be\u6027\u9057\u5fd8\u7684\u9650\u5236\uff0c\u5373\u5728\u5b66\u4e60\u65b0\u4efb\u52a1\u540e\uff0c\u8fc7\u53bb\u4efb\u52a1\u7684\u6027\u80fd\u4f1a\u4e0b\u964d\u3002", "method": "TFC-SR\u901a\u8fc7\u4e00\u79cd\u540d\u4e3a\u4e3b\u52a8\u53ec\u56de\u63a2\u9488\u7684\u673a\u5236\u589e\u5f3a\u4e86\u6807\u51c6\u7ecf\u9a8c\u56de\u653e\u3002", "result": "\u5728Split CIFAR-100\u4e0a\uff0cTFC-SR\u7684\u6700\u7ec8\u51c6\u786e\u7387\u4e3a13.17%\uff0c\u800c\u6807\u51c6\u56de\u653e\u7684\u51c6\u786e\u7387\u4e3a7.40%\u3002", "conclusion": "TFC-SR\u662f\u4e00\u79cd\u9c81\u68d2\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u7a81\u51fa\u4e86\u5c06\u4e3b\u52a8\u8bb0\u5fc6\u68c0\u7d22\u673a\u5236\u6574\u5408\u5230\u6301\u7eed\u5b66\u4e60\u7cfb\u7edf\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.21117", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21117", "abs": "https://arxiv.org/abs/2507.21117", "authors": ["Rahul Raja", "Anshaj Vats", "Arpita Vats", "Anirban Majumder"], "title": "A Comprehensive Review on Harnessing Large Language Models to Overcome Recommender System Challenges", "comment": null, "summary": "Recommender systems have traditionally followed modular architectures\ncomprising candidate generation, multi-stage ranking, and re-ranking, each\ntrained separately with supervised objectives and hand-engineered features.\nWhile effective in many domains, such systems face persistent challenges\nincluding sparse and noisy interaction data, cold-start problems, limited\npersonalization depth, and inadequate semantic understanding of user and item\ncontent. The recent emergence of Large Language Models (LLMs) offers a new\nparadigm for addressing these limitations through unified, language-native\nmechanisms that can generalize across tasks, domains, and modalities. In this\npaper, we present a comprehensive technical survey of how LLMs can be leveraged\nto tackle key challenges in modern recommender systems. We examine the use of\nLLMs for prompt-driven candidate retrieval, language-native ranking,\nretrieval-augmented generation (RAG), and conversational recommendation,\nillustrating how these approaches enhance personalization, semantic alignment,\nand interpretability without requiring extensive task-specific supervision.\nLLMs further enable zero- and few-shot reasoning, allowing systems to operate\neffectively in cold-start and long-tail scenarios by leveraging external\nknowledge and contextual cues. We categorize these emerging LLM-driven\narchitectures and analyze their effectiveness in mitigating core bottlenecks of\nconventional pipelines. In doing so, we provide a structured framework for\nunderstanding the design space of LLM-enhanced recommenders, and outline the\ntrade-offs between accuracy, scalability, and real-time performance. Our goal\nis to demonstrate that LLMs are not merely auxiliary components but\nfoundational enablers for building more adaptive, semantically rich, and\nuser-centric recommender systems", "AI": {"tldr": "This paper surveys how Large Language Models (LLMs) can address key challenges in modern recommender systems, enhancing personalization, semantic understanding, and cold-start performance.", "motivation": "Traditional recommender systems face challenges including sparse and noisy interaction data, cold-start problems, limited personalization depth, and inadequate semantic understanding of user and item content.", "method": "Comprehensive technical survey of how LLMs can be leveraged to tackle key challenges in modern recommender systems. Examination of the use of LLMs for prompt-driven candidate retrieval, language-native ranking, retrieval-augmented generation (RAG), and conversational recommendation.", "result": "LLMs enhance personalization, semantic alignment, and interpretability without requiring extensive task-specific supervision. LLMs further enable zero- and few-shot reasoning, allowing systems to operate effectively in cold-start and long-tail scenarios.", "conclusion": "LLMs are foundational enablers for building more adaptive, semantically rich, and user-centric recommender systems."}}
{"id": "2507.21173", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.21173", "abs": "https://arxiv.org/abs/2507.21173", "authors": ["Chris Partridge", "Andrew Mitchell", "Andreas Cola"], "title": "Digitalizing Uncertain Information", "comment": "9 pages. 2 figures. Conference: Semantic Technology for Intelligence,\n  Defense, and Security (STIDS 2024)", "summary": "The paper sketches some initial results from an ongoing project to develop an\nontology-based digital form for representing uncertain information. We frame\nthis work as a journey from lower to higher levels of digital maturity across a\ntechnology divide. The paper first sets a baseline by describing the basic\nchallenges any project dealing with digital uncertainty faces. It then\ndescribes how the project is facing them. It shows firstly how an extensional\nontology (such as the BORO Foundational Ontology or the Information Exchange\nStandard) can be extended with a Lewisian counterpart approach to formalizing\nuncertainty that is adapted to computing. And then it shows how this is\nexpressive enough to handle the challenges. Keywords: actuality, BORO\nFoundational Ontology, counterpart, Information Exchange Standard,\ninformational uncertainty, my doxastic actualities, two-dimensional semantics.", "AI": {"tldr": "The paper introduces an ontology-based method using a Lewisian counterpart approach to represent uncertain information in a digital form.", "motivation": "The paper aims to develop an ontology-based digital form for representing uncertain information and advance digital maturity across a technology divide.", "method": "The paper extends an extensional ontology (BORO Foundational Ontology or the Information Exchange Standard) with a Lewisian counterpart approach.", "result": "The paper presents initial results showing how the proposed approach can handle the challenges of representing digital uncertainty.", "conclusion": "The paper demonstrates the expressiveness of extending an extensional ontology with a Lewisian counterpart approach for formalizing uncertainty, showing its ability to handle challenges in representing uncertain information."}}
{"id": "2507.21065", "categories": ["cs.CL", "cs.HC", "cs.LG", "cs.RO", "I.2.7, I.2.9, j.4,"], "pdf": "https://arxiv.org/pdf/2507.21065", "abs": "https://arxiv.org/abs/2507.21065", "authors": ["Sabrina Patania", "Luca Annese", "Cansu Koyuturk", "Azzurra Ruggeri", "Dimitri Ognibene"], "title": "Dialogic Social Learning for Artificial Agents: Enhancing LLM Ontology Acquisition through Mixed-Initiative Educational Interactions", "comment": "submitted to ICSR2025", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nprocessing extensive offline datasets. However, they often face challenges in\nacquiring and integrating complex, knowledge online. Traditional AI training\nparadigms, predominantly based on supervised learning or reinforcement\nlearning, mirror a 'Piagetian' model of independent exploration. These\napproaches typically rely on large datasets and sparse feedback signals,\nlimiting the models' ability to learn efficiently from interactions. Drawing\ninspiration from Vygotsky's sociocultural theory, this study explores the\npotential of socially mediated learning paradigms to address these limitations.\n  We introduce a dynamic environment, termed the 'AI Social Gym', where an AI\nlearner agent engages in dyadic pedagogical dialogues with knowledgeable AI\nteacher agents. These interactions emphasize external, structured dialogue as a\ncore mechanism for knowledge acquisition, contrasting with methods that depend\nsolely on internal inference or pattern recognition.\n  Our investigation focuses on how different pedagogical strategies impact the\nAI learning process in the context of ontology acquisition. Empirical results\nindicate that such dialogic approaches-particularly those involving\nmixed-direction interactions combining top-down explanations with\nlearner-initiated questioning-significantly enhance the LLM's ability to\nacquire and apply new knowledge, outperforming both unidirectional\ninstructional methods and direct access to structured knowledge, formats\ntypically present in training datasets.\n  These findings suggest that integrating pedagogical and psychological\ninsights into AI and robot training can substantially improve post-training\nknowledge acquisition and response quality. This approach offers a\ncomplementary pathway to existing strategies like prompt engineering", "AI": {"tldr": "This paper explores socially mediated learning paradigms to improve LLMs' ability to acquire and apply new knowledge using an 'AI Social Gym' where AI learner agents engage in pedagogical dialogues with AI teacher agents.", "motivation": "LLMs face challenges in acquiring and integrating complex knowledge online. Traditional AI training paradigms limit the models' ability to learn efficiently from interactions.", "method": "A dynamic environment, termed the 'AI Social Gym', where an AI learner agent engages in dyadic pedagogical dialogues with knowledgeable AI teacher agents. These interactions emphasize external, structured dialogue as a core mechanism for knowledge acquisition.", "result": "Dialogic approaches significantly enhance the LLM's ability to acquire and apply new knowledge, outperforming both unidirectional instructional methods and direct access to structured knowledge.", "conclusion": "Integrating pedagogical and psychological insights into AI and robot training can substantially improve post-training knowledge acquisition and response quality, offering a complementary pathway to existing strategies like prompt engineering."}}
{"id": "2507.21161", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21161", "abs": "https://arxiv.org/abs/2507.21161", "authors": ["Pallavi Zambare", "Venkata Nikhil Thanikella", "Ying Liu"], "title": "Seeing Beyond Frames: Zero-Shot Pedestrian Intention Prediction with Raw Temporal Video and Multimodal Cues", "comment": "Accepted in IEEE 3rd International Conference on Artificial\n  Intelligence, Blockchain, and Internet of Things (AIBThings 2025)", "summary": "Pedestrian intention prediction is essential for autonomous driving in\ncomplex urban environments. Conventional approaches depend on supervised\nlearning over frame sequences and require extensive retraining to adapt to new\nscenarios. Here, we introduce BF-PIP (Beyond Frames Pedestrian Intention\nPrediction), a zero-shot approach built upon Gemini 2.5 Pro. It infers crossing\nintentions directly from short, continuous video clips enriched with structured\nJAAD metadata. In contrast to GPT-4V based methods that operate on discrete\nframes, BF-PIP processes uninterrupted temporal clips. It also incorporates\nbounding-box annotations and ego-vehicle speed via specialized multimodal\nprompts. Without any additional training, BF-PIP achieves 73% prediction\naccuracy, outperforming a GPT-4V baseline by 18 %. These findings illustrate\nthat combining temporal video inputs with contextual cues enhances\nspatiotemporal perception and improves intent inference under ambiguous\nconditions. This approach paves the way for agile, retraining-free perception\nmodule in intelligent transportation system.", "AI": {"tldr": "introduce BF-PIP, a zero-shot approach built upon Gemini 2.5 Pro, achieves 73% prediction accuracy, outperforming a GPT-4V baseline by 18 %", "motivation": "Pedestrian intention prediction is essential for autonomous driving in complex urban environments. Conventional approaches depend on supervised learning over frame sequences and require extensive retraining to adapt to new scenarios.", "method": "a zero-shot approach built upon Gemini 2.5 Pro. It infers crossing intentions directly from short, continuous video clips enriched with structured JAAD metadata. It also incorporates bounding-box annotations and ego-vehicle speed via specialized multimodal prompts.", "result": "achieves 73% prediction accuracy, outperforming a GPT-4V baseline by 18 %", "conclusion": "combining temporal video inputs with contextual cues enhances spatiotemporal perception and improves intent inference under ambiguous conditions. This approach paves the way for agile, retraining-free perception module in intelligent transportation system."}}
{"id": "2507.21098", "categories": ["cs.AI", "cs.CY", "I.2.6; I.2.1; H.4.2"], "pdf": "https://arxiv.org/pdf/2507.21098", "abs": "https://arxiv.org/abs/2507.21098", "authors": ["Marta Sidorkiewicz", "Karolina Kr\u00f3likowska", "Berenika Dyczek", "Edyta Pijet-Migon", "Anna Dubel"], "title": "Artificial intelligence for sustainable wine industry: AI-driven management in viticulture, wine production and enotourism", "comment": "6 pages, 4 figures. Accepted for presentation at the 27th European\n  Conference on Artificial Intelligence (ECAI 2025), October 19-24, 2025,\n  Bologna, Italy", "summary": "This study examines the role of Artificial Intelligence (AI) in enhancing\nsustainability and efficiency within the wine industry. It focuses on AI-driven\nintelligent management in viticulture, wine production, and enotourism. As the\nwine industry faces environmental and economic challenges, AI offers innovative\nsolutions to optimize resource use, reduce environmental impact, and improve\ncustomer engagement. Understanding AI's potential in sustainable winemaking is\ncrucial for fostering responsible and efficient industry practices. The\nresearch is based on a questionnaire survey conducted among Polish winemakers,\ncombined with a comprehensive analysis of AI methods applicable to viticulture,\nproduction, and tourism. Key AI technologies, including predictive analytics,\nmachine learning, and computer vision, are explored. The findings indicate that\nAI enhances vineyard monitoring, optimizes irrigation, and streamlines\nproduction processes, contributing to sustainable resource management. In\nenotourism, AI-powered chatbots, recommendation systems, and virtual tastings\npersonalize consumer experiences. The study highlights AI's impact on economic,\nenvironmental, and social sustainability, supporting local wine enterprises and\ncultural heritage. Keywords: Artificial Intelligence, Sustainable Development,\nAI-Driven Management, Viticulture, Wine Production, Enotourism, Wine\nEnterprises, Local Communities", "AI": {"tldr": "AI enhances sustainability and efficiency in the wine industry through intelligent management in viticulture, wine production, and enotourism.", "motivation": "The wine industry faces environmental and economic challenges, and AI offers solutions to optimize resource use, reduce environmental impact, and improve customer engagement.", "method": "Questionnaire survey among Polish winemakers, combined with analysis of AI methods applicable to viticulture, production, and tourism.", "result": "AI enhances vineyard monitoring, optimizes irrigation, and streamlines production processes, contributing to sustainable resource management. In enotourism, AI-powered chatbots, recommendation systems, and virtual tastings personalize consumer experiences. AI impacts economic, environmental, and social sustainability.", "conclusion": "AI enhances vineyard monitoring, optimizes irrigation, streamlines production, and personalizes consumer experiences, contributing to sustainable resource management and supporting local wine enterprises and cultural heritage."}}
{"id": "2507.21119", "categories": ["cs.LG", "eess.SP", "physics.optics"], "pdf": "https://arxiv.org/pdf/2507.21119", "abs": "https://arxiv.org/abs/2507.21119", "authors": ["Yousuf Moiz Ali", "Jaroslaw E. Prilepsky", "Nicola Sambo", "Jo\u00e3o Pedro", "Mohammad M. Hosseini", "Antonio Napoli", "Sergei K. Turitsyn", "Pedro Freire"], "title": "Pre-, In-, and Post-Processing Class Imbalance Mitigation Techniques for Failure Detection in Optical Networks", "comment": "3 pages + 1 page for acknowledgement and references", "summary": "We compare pre-, in-, and post-processing techniques for class imbalance\nmitigation in optical network failure detection. Threshold Adjustment achieves\nthe highest F1 gain (15.3%), while Random Under-sampling (RUS) offers the\nfastest inference, highlighting a key performance-complexity trade-off.", "AI": {"tldr": "Compares techniques for class imbalance mitigation in optical network failure detection, finding a performance-complexity trade-off between Threshold Adjustment and Random Under-sampling.", "motivation": "class imbalance mitigation in optical network failure detection", "method": "pre-, in-, and post-processing techniques", "result": "Threshold Adjustment achieves the highest F1 gain (15.3%), while Random Under-sampling (RUS) offers the fastest inference", "conclusion": "Threshold Adjustment achieves the highest F1 gain, while Random Under-sampling (RUS) offers the fastest inference."}}
{"id": "2507.21120", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21120", "abs": "https://arxiv.org/abs/2507.21120", "authors": ["Bereket A. Yilma", "Luis A. Leiva"], "title": "Affect-aware Cross-Domain Recommendation for Art Therapy via Music Preference Elicitation", "comment": "Accepted at the 19th ACM Conference on Recommender Systems", "summary": "Art Therapy (AT) is an established practice that facilitates emotional\nprocessing and recovery through creative expression. Recently, Visual Art\nRecommender Systems (VA RecSys) have emerged to support AT, demonstrating their\npotential by personalizing therapeutic artwork recommendations. Nonetheless,\ncurrent VA RecSys rely on visual stimuli for user modeling, limiting their\nability to capture the full spectrum of emotional responses during preference\nelicitation. Previous studies have shown that music stimuli elicit unique\naffective reflections, presenting an opportunity for cross-domain\nrecommendation (CDR) to enhance personalization in AT. Since CDR has not yet\nbeen explored in this context, we propose a family of CDR methods for AT based\non music-driven preference elicitation. A large-scale study with 200 users\ndemonstrates the efficacy of music-driven preference elicitation, outperforming\nthe classic visual-only elicitation approach. Our source code, data, and models\nare available at https://github.com/ArtAICare/Affect-aware-CDR", "AI": {"tldr": "\u63d0\u51fa\u97f3\u4e50\u9a71\u52a8\u7684\u827a\u672f\u7597\u6cd5\u8de8\u57df\u63a8\u8350\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u7684\u89c6\u89c9\u827a\u672f\u63a8\u8350\u7cfb\u7edf\u4f9d\u8d56\u89c6\u89c9\u523a\u6fc0\u8fdb\u884c\u7528\u6237\u5efa\u6a21\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u504f\u597d\u542f\u53d1\u671f\u95f4\u6355\u6349\u5b8c\u6574\u60c5\u611f\u53cd\u5e94\u7684\u80fd\u529b\u3002\u4e4b\u524d\u7684\u7814\u7a76\u8868\u660e\uff0c\u97f3\u4e50\u523a\u6fc0\u53ef\u4ee5\u5f15\u53d1\u72ec\u7279\u7684\u60c5\u611f\u53cd\u5c04\uff0c\u4e3a\u8de8\u57df\u63a8\u8350 (CDR) \u589e\u5f3a AT \u4e2a\u6027\u5316\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u57fa\u4e8e\u97f3\u4e50\u9a71\u52a8\u504f\u597d\u542f\u53d1\u7684\u827a\u672f\u7597\u6cd5\u8de8\u57df\u63a8\u8350\u65b9\u6cd5\u3002", "result": "\u4e00\u9879\u6709 200 \u540d\u7528\u6237\u53c2\u4e0e\u7684\u5927\u89c4\u6a21\u7814\u7a76\u8868\u660e\uff0c\u97f3\u4e50\u9a71\u52a8\u7684\u504f\u597d\u542f\u53d1\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u97f3\u4e50\u9a71\u52a8\u7684\u504f\u597d\u542f\u53d1\u65b9\u6cd5\u4f18\u4e8e\u7ecf\u5178\u89c6\u89c9\u542f\u53d1\u65b9\u6cd5\u3002"}}
{"id": "2507.21860", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.21860", "abs": "https://arxiv.org/abs/2507.21860", "authors": ["Micka\u00ebl Martin-Nevot", "Lotfi Lakhal"], "title": "Ranking Methods for Skyline Queries", "comment": null, "summary": "{Multi-criteria decision analysis in databases has been actively studied,\nespecially through the Skyline operator. Yet, few approaches offer a relevant\ncomparison of Pareto optimal, or Skyline, points for high cardinality result\nsets. We propose to improve the dp-idp method, inspired by tf-idf, a recent\napproach computing a score for each Skyline point, by introducing the concept\nof dominance hierarchy. As dp-idp lacks efficiency and does not ensure a\ndistinctive rank, we introduce the RankSky method, the adaptation of Google's\nwell-known PageRank solution, using a square stochastic matrix, a teleportation\nmatrix, a damping factor, and then a row score eigenvector and the IPL\nalgorithm. For the same reasons as RankSky, and also to offer directly\nembeddable in DBMS solution, we establish the TOPSIS based CoSky method,\nderived from both information research and multi-criteria analysis. CoSky\nautomatically ponderates normalized attributes using the Gini index, then\ncomputes a score using Salton's cosine toward an ideal point. By coupling\nmultilevel Skyline to dp-idp, RankSky or CoSky, we introduce DeepSky.\nImplementations of dp-idp, RankSky and CoSky are evaluated experimentally.", "AI": {"tldr": "\u63d0\u51fa\u4e86DeepSky\uff0c\u7ed3\u5408\u591a\u79cd\u65b9\u6cd5\u6539\u8fdbSkyline\u70b9\u6392\u5e8f\uff0c\u89e3\u51b3\u9ad8\u57fa\u6570\u6570\u636e\u96c6\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u57fa\u6570\u7ed3\u679c\u96c6\u65f6\uff0c\u5bf9Pareto\u6700\u4f18\u6216Skyline\u70b9\u7684\u76f8\u5173\u6bd4\u8f83\u4e0d\u8db3\u3002", "method": "\u6539\u8fdb\u4e86dp-idp\u65b9\u6cd5\uff0c\u5f15\u5165\u4e86\u652f\u914d\u5c42\u7ea7\u7684\u6982\u5ff5\uff1b\u63d0\u51fa\u4e86RankSky\u65b9\u6cd5\uff0c\u91c7\u7528\u4e86Google\u7684PageRank\u89e3\u51b3\u65b9\u6848\uff1b\u5efa\u7acb\u4e86\u57fa\u4e8eTOPSIS\u7684CoSky\u65b9\u6cd5\uff0c\u5229\u7528Gini\u6307\u6570\u81ea\u52a8\u52a0\u6743\u5f52\u4e00\u5316\u5c5e\u6027\uff0c\u5e76\u8ba1\u7b97Salton\u4f59\u5f26\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u4e86dp-idp\u3001RankSky\u548cCoSky\u7684\u5b9e\u73b0\u3002", "conclusion": "\u63d0\u51fa\u4e86DeepSky\uff0c\u901a\u8fc7\u5c06\u591a\u7ea7Skyline\u4e0edp-idp\u3001RankSky\u6216CoSky\u76f8\u7ed3\u5408\u3002"}}
{"id": "2507.21073", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.21073", "abs": "https://arxiv.org/abs/2507.21073", "authors": ["David James Woo", "Yangyang Yu", "Kai Guo", "Yilin Huang", "April Ka Yeng Fung"], "title": "Product vs. Process: Exploring EFL Students' Editing of AI-Generated Text for Expository Writing", "comment": "45 pages, 11 figures", "summary": "Text generated by artificial intelligence (AI) chatbots is increasingly used\nin English as a foreign language (EFL) writing contexts, yet its impact on\nstudents' expository writing process and compositions remains understudied.\nThis research examines how EFL secondary students edit AI-generated text.\nExploring editing behaviors in their expository writing process and in\nexpository compositions, and their effect on human-rated scores for content,\norganization, language, and overall quality. Participants were 39 Hong Kong\nsecondary students who wrote an expository composition with AI chatbots in a\nworkshop. A convergent design was employed to analyze their screen recordings\nand compositions to examine students' editing behaviors and writing qualities.\nAnalytical methods included qualitative coding, descriptive statistics,\ntemporal sequence analysis, human-rated scoring, and multiple linear regression\nanalysis. We analyzed over 260 edits per dataset, and identified two editing\npatterns: one where students refined introductory units repeatedly before\nprogressing, and another where they quickly shifted to extensive edits in body\nunits (e.g., topic and supporting sentences). MLR analyses revealed that the\nnumber of AI-generated words positively predicted all score dimensions, while\nmost editing variables showed minimal impact. These results suggest a\ndisconnect between students' significant editing effort and improved\ncomposition quality, indicating AI supports but does not replace writing\nskills. The findings highlight the importance of genre-specific instruction and\nprocess-focused writing before AI integration. Educators should also develop\nassessments valuing both process and product to encourage critical engagement\nwith AI text.", "AI": {"tldr": "This research examines how EFL secondary students edit AI-generated text and its impact on their expository writing. Results suggest that AI supports but does not replace writing skills, and editing efforts do not guarantee improved composition quality.", "motivation": "The impact of AI chatbot-generated text on EFL students' expository writing process and compositions is understudied.", "method": "A convergent design was employed to analyze students' screen recordings and compositions to examine their editing behaviors and writing qualities, using qualitative coding, descriptive statistics, temporal sequence analysis, human-rated scoring, and multiple linear regression analysis.", "result": "The number of AI-generated words positively predicted all score dimensions, while most editing variables showed minimal impact. Two editing patterns were identified: refining introductory units repeatedly and quickly shifting to extensive edits in body units.", "conclusion": "Students' editing effort does not necessarily improve composition quality, suggesting AI supports but does not replace writing skills."}}
{"id": "2507.21167", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21167", "abs": "https://arxiv.org/abs/2507.21167", "authors": ["Danglu Yang", "Liang Zhang", "Zihao Yue", "Liangyu Chen", "Yichen Xu", "Wenxuan Wang", "Qin Jin"], "title": "ChartM$^3$: Benchmarking Chart Editing with Multimodal Instructions", "comment": null, "summary": "Charts are a fundamental visualization format widely used in data analysis\nacross research and industry. While enabling users to edit charts based on\nhigh-level intentions is of great practical value, existing methods primarily\nrely on natural language instructions, which are often too ambiguous to support\nfine-grained editing. In this work, we introduce a novel paradigm for\nmultimodal chart editing, where user intent is expressed through a combination\nof natural language and visual indicators that explicitly highlight the\nelements to be modified. To support this paradigm, we present\nChart$\\text{M}^3$, a new benchmark for Multimodal chart editing with\nMulti-level complexity and Multi-perspective evaluation. Chart$\\text{M}^3$\ncontains 1,000 samples spanning four levels of editing difficulty. Each sample\nincludes triplets in the form of (chart, code, multimodal instructions). To\ncomprehensively evaluate chart editing models, Chart$\\text{M}^3$ provides\nmetrics that assess both visual appearance and code correctness. Our benchmark\nreveals significant limitations in current multimodal large language models\n(MLLMs), including GPT-4o, particularly in their ability to interpret and act\non visual indicators. To address this, we construct Chart$\\text{M}^3$-Train, a\nlarge-scale training set with 24,000 multimodal chart editing samples.\nFine-tuning MLLMs on this dataset leads to substantial improvements,\ndemonstrating the importance of multimodal supervision in building practical\nchart editing systems. Our datasets, codes, and evaluation tools are available\nat https://github.com/MLrollIT/ChartM3. %https://github.com/MLrollIT/ChartM3Our\ndatasets, codes, and evaluation tools are available at\nhttps://github.com/yaolinli/VCE.", "AI": {"tldr": "Introduces a new multimodal chart editing paradigm using natural language and visual indicators, a new benchmark dataset (ChartM3), and a training set (ChartM3-Train) to improve MLLM performance in chart editing.", "motivation": "Existing methods primarily rely on natural language instructions, which are often too ambiguous to support fine-grained editing of charts.", "method": "A new benchmark for multimodal chart editing with multi-level complexity and multi-perspective evaluation, ChartM3, and a large-scale training set with 24,000 multimodal chart editing samples, ChartM3-Train.", "result": "Benchmark reveals significant limitations in current multimodal large language models (MLLMs), including GPT-4o, particularly in their ability to interpret and act on visual indicators.", "conclusion": "Fine-tuning MLLMs on a new dataset leads to substantial improvements, demonstrating the importance of multimodal supervision in building practical chart editing systems."}}
{"id": "2507.21123", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21123", "abs": "https://arxiv.org/abs/2507.21123", "authors": ["Mark A. Kramer", "Aanchal Mathur", "Caroline E. Adams", "Jason A. Walonoski"], "title": "Leveraging Generative AI to Enhance Synthea Module Development", "comment": "Title: Leveraging Generative AI to Enhance Synthea Module Development\n  Word Count: [Approximately 12,000 words] Figures: 3 Tables: 3 Supplementary\n  Material: Extensive appendices with prompts and disease profiles", "summary": "This paper explores the use of large language models (LLMs) to assist in the\ndevelopment of new disease modules for Synthea, an open-source synthetic health\ndata generator. Incorporating LLMs into the module development process has the\npotential to reduce development time, reduce required expertise, expand model\ndiversity, and improve the overall quality of synthetic patient data. We\ndemonstrate four ways that LLMs can support Synthea module creation: generating\na disease profile, generating a disease module from a disease profile,\nevaluating an existing Synthea module, and refining an existing module. We\nintroduce the concept of progressive refinement, which involves iteratively\nevaluating the LLM-generated module by checking its syntactic correctness and\nclinical accuracy, and then using that information to modify the module. While\nthe use of LLMs in this context shows promise, we also acknowledge the\nchallenges and limitations, such as the need for human oversight, the\nimportance of rigorous testing and validation, and the potential for\ninaccuracies in LLM-generated content. The paper concludes with recommendations\nfor future research and development to fully realize the potential of LLM-aided\nsynthetic data creation.", "AI": {"tldr": "This paper explores the use of large language models (LLMs) to assist in the development of new disease modules for Synthea, an open-source synthetic health data generator.", "motivation": "Incorporating LLMs into the module development process has the potential to reduce development time, reduce required expertise, expand model diversity, and improve the overall quality of synthetic patient data.", "method": "demonstrate four ways that LLMs can support Synthea module creation: generating a disease profile, generating a disease module from a disease profile, evaluating an existing Synthea module, and refining an existing module. We introduce the concept of progressive refinement, which involves iteratively evaluating the LLM-generated module by checking its syntactic correctness and clinical accuracy, and then using that information to modify the module.", "result": "We demonstrate four ways that LLMs can support Synthea module creation", "conclusion": "The paper concludes with recommendations for future research and development to fully realize the potential of LLM-aided synthetic data creation."}}
{"id": "2507.21135", "categories": ["cs.LG", "quant-ph", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.21135", "abs": "https://arxiv.org/abs/2507.21135", "authors": ["Alexander G. Abanov", "Luca Candelori", "Harold C. Steinacker", "Martin T. Wells", "Jerome R. Busemeyer", "Cameron J. Hogan", "Vahagn Kirakosyan", "Nicola Marzari", "Sunil Pinnamaneni", "Dario Villani", "Mengjia Xu", "Kharen Musaelian"], "title": "Quantum Geometry of Data", "comment": "27 pages, 14 figures, 1 table", "summary": "We demonstrate how Quantum Cognition Machine Learning (QCML) encodes data as\nquantum geometry. In QCML, features of the data are represented by learned\nHermitian matrices, and data points are mapped to states in Hilbert space. The\nquantum geometry description endows the dataset with rich geometric and\ntopological structure - including intrinsic dimension, quantum metric, and\nBerry curvature - derived directly from the data. QCML captures global\nproperties of data, while avoiding the curse of dimensionality inherent in\nlocal methods. We illustrate this on a number of synthetic and real-world\nexamples. Quantum geometric representation of QCML could advance our\nunderstanding of cognitive phenomena within the framework of quantum cognition.", "AI": {"tldr": "QCML encodes data as quantum geometry, representing features as Hermitian matrices and mapping data points to states in Hilbert space, capturing global properties and avoiding the curse of dimensionality. ", "motivation": "Demonstrates how Quantum Cognition Machine Learning (QCML) encodes data as quantum geometry. QCML captures global properties of data, while avoiding the curse of dimensionality inherent in local methods.", "method": "Features of the data are represented by learned Hermitian matrices, and data points are mapped to states in Hilbert space.", "result": "The quantum geometry description endows the dataset with rich geometric and topological structure - including intrinsic dimension, quantum metric, and Berry curvature - derived directly from the data. We illustrate this on a number of synthetic and real-world examples.", "conclusion": "Quantum geometric representation of QCML could advance our understanding of cognitive phenomena within the framework of quantum cognition."}}
{"id": "2507.21125", "categories": ["cs.IR", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.21125", "abs": "https://arxiv.org/abs/2507.21125", "authors": ["Karan Mirhosseini", "Arya Aftab", "Alireza Sheikh"], "title": "RATE: An LLM-Powered Retrieval Augmented Generation Technology-Extraction Pipeline", "comment": "9 pages, 4 figures, 1 table", "summary": "In an era of radical technology transformations, technology maps play a\ncrucial role in enhancing decision making. These maps heavily rely on automated\nmethods of technology extraction. This paper introduces Retrieval Augmented\nTechnology Extraction (RATE), a Large Language Model (LLM) based pipeline for\nautomated technology extraction from scientific literature. RATE combines\nRetrieval Augmented Generation (RAG) with multi-definition LLM-based\nvalidation. This hybrid method results in high recall in candidate generation\nalongside with high precision in candidate filtering. While the pipeline is\ndesigned to be general and widely applicable, we demonstrate its use on 678\nresearch articles focused on Brain-Computer Interfaces (BCIs) and Extended\nReality (XR) as a case study. Consequently, The validated technology terms by\nRATE were mapped into a co-occurrence network, revealing thematic clusters and\nstructural features of the research landscape. For the purpose of evaluation, a\ngold standard dataset of technologies in 70 selected random articles had been\ncurated by the experts. In addition, a technology extraction model based on\nBidirectional Encoder Representations of Transformers (BERT) was used as a\ncomparative method. RATE achieved F1-score of 91.27%, Significantly\noutperforming BERT with F1-score of 53.73%. Our findings highlight the promise\nof definition-driven LLM methods for technology extraction and mapping. They\nalso offer new insights into emerging trends within the BCI-XR field. The\nsource code is available https://github.com/AryaAftab/RATE", "AI": {"tldr": "RATE, a new LLM pipeline, significantly outperforms BERT in automated technology extraction from scientific literature, offering new insights into BCI-XR trends.", "motivation": "Technology maps are crucial for enhancing decision-making in the face of radical technology transformations, and these maps rely on automated methods of technology extraction.", "method": "Retrieval Augmented Technology Extraction (RATE), a Large Language Model (LLM) based pipeline combining Retrieval Augmented Generation (RAG) with multi-definition LLM-based validation.", "result": "RATE achieved an F1-score of 91.27%, significantly outperforming BERT with an F1-score of 53.73%. Validated technology terms were mapped into a co-occurrence network, revealing thematic clusters and structural features of the research landscape.", "conclusion": "Definition-driven LLM methods show promise for technology extraction and mapping, offering insights into emerging trends within the BCI-XR field."}}
{"id": "2507.21989", "categories": ["cs.DB", "cs.DS", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.21989", "abs": "https://arxiv.org/abs/2507.21989", "authors": ["Patrick Iff", "Paul Bruegger", "Marcin Chrapek", "Maciej Besta", "Torsten Hoefler"], "title": "Benchmarking Filtered Approximate Nearest Neighbor Search Algorithms on Transformer-based Embedding Vectors", "comment": null, "summary": "Advances in embedding models for text, image, audio, and video drive progress\nacross multiple domains, including retrieval-augmented generation,\nrecommendation systems, vehicle/person reidentification, and face recognition.\nMany applications in these domains require an efficient method to retrieve\nitems that are close to a given query in the embedding space while satisfying a\nfilter condition based on the item's attributes, a problem known as Filtered\nApproximate Nearest Neighbor Search (FANNS). In this work, we present a\ncomprehensive survey and taxonomy of FANNS methods and analyze how they are\nbenchmarked in the literature. By doing so, we identify a key challenge in the\ncurrent FANNS landscape: the lack of diverse and realistic datasets,\nparticularly ones derived from the latest transformer-based text embedding\nmodels. To address this, we introduce a novel dataset consisting of embedding\nvectors for the abstracts of over 2.7 million research articles from the arXiv\nrepository, accompanied by 11 real-world attributes such as authors and\ncategories. We benchmark a wide range of FANNS methods on our novel dataset and\nfind that each method has distinct strengths and limitations; no single\napproach performs best across all scenarios. ACORN, for example, supports\nvarious filter types and performs reliably across dataset scales but is often\noutperformed by more specialized methods. SeRF shows excellent performance for\nrange filtering on ordered attributes but cannot handle categorical attributes.\nFiltered-DiskANN and UNG excel on the medium-scale dataset but fail on the\nlarge-scale dataset, highlighting the challenge posed by transformer-based\nembeddings, which are often more than an order of magnitude larger than earlier\nembeddings. We conclude that no universally best method exists.", "AI": {"tldr": "introduce a novel dataset consisting of embedding vectors for the abstracts of over 2.7 million research articles from the arXiv repository, accompanied by 11 real-world attributes such as authors and categories, and benchmark a wide range of FANNS methods on it", "motivation": "lack of diverse and realistic datasets, particularly ones derived from the latest transformer-based text embedding models", "method": "a comprehensive survey and taxonomy of FANNS methods and analyze how they are benchmarked in the literature", "result": "each method has distinct strengths and limitations; no single approach performs best across all scenarios", "conclusion": "no universally best method exists for Filtered Approximate Nearest Neighbor Search (FANNS)"}}
{"id": "2507.21080", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21080", "abs": "https://arxiv.org/abs/2507.21080", "authors": ["Vincent C. M\u00fcller"], "title": "Which symbol grounding problem should we try to solve?", "comment": null, "summary": "Floridi and Taddeo propose a condition of \"zero semantic commitment\" for\nsolutions to the grounding problem, and a solution to it. I argue briefly that\ntheir condition cannot be fulfilled, not even by their own solution. After a\nlook at Luc Steels' very different competing suggestion, I suggest that we need\nto re-think what the problem is and what role the 'goals' in a system play in\nformulating the problem. On the basis of a proper understanding of computing, I\ncome to the conclusion that the only sensible grounding problem is how we can\nexplain and re-produce the behavioral ability and function of meaning in\nartificial computational agents", "AI": {"tldr": "Floridi \u548c Taddeo \u63d0\u51fa\u4e86\u201c\u96f6\u8bed\u4e49\u627f\u8bfa\u201d\u6761\u4ef6\u6765\u89e3\u51b3 grounding \u95ee\u9898\uff0c\u4f46\u8fd9\u4e2a\u6761\u4ef6\u65e0\u6cd5\u6ee1\u8db3\u3002\u6211\u4eec\u9700\u8981\u91cd\u65b0\u601d\u8003 grounding \u95ee\u9898\u662f\u4ec0\u4e48\u3002", "motivation": "Floridi \u548c Taddeo \u63d0\u51fa\u4e86\u201c\u96f6\u8bed\u4e49\u627f\u8bfa\u201d\u6761\u4ef6\u6765\u89e3\u51b3 grounding \u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u89e3\u51b3\u65b9\u6848\u3002\u6211\u8ba4\u4e3a\u4ed6\u4eec\u7684\u6761\u4ef6\u65e0\u6cd5\u6ee1\u8db3\uff0c\u751a\u81f3\u4ed6\u4eec\u81ea\u5df1\u7684\u89e3\u51b3\u65b9\u6848\u4e5f\u65e0\u6cd5\u6ee1\u8db3\u3002\u5728\u770b\u4e86\u4e00\u4e0b Luc Steels \u975e\u5e38\u4e0d\u540c\u7684\u7ade\u4e89\u6027\u5efa\u8bae\u540e\uff0c\u6211\u8ba4\u4e3a\u6211\u4eec\u9700\u8981\u91cd\u65b0\u601d\u8003\u95ee\u9898\u662f\u4ec0\u4e48\uff0c\u4ee5\u53ca\u7cfb\u7edf\u4e2d\u201c\u76ee\u6807\u201d\u5728\u5236\u5b9a\u95ee\u9898\u4e2d\u626e\u6f14\u7684\u89d2\u8272\u3002", "method": "\u5bf9\u8ba1\u7b97\u7684\u6b63\u786e\u7406\u89e3", "result": "\u4ed6\u4eec\u7684\u6761\u4ef6\u65e0\u6cd5\u6ee1\u8db3\uff0c\u751a\u81f3\u4ed6\u4eec\u81ea\u5df1\u7684\u89e3\u51b3\u65b9\u6848\u4e5f\u65e0\u6cd5\u6ee1\u8db3", "conclusion": "\u552f\u4e00\u7684\u5408\u7406\u7684 grounding \u95ee\u9898\u662f\uff0c\u6211\u4eec\u5982\u4f55\u89e3\u91ca\u548c\u91cd\u73b0\u4eba\u5de5\u8ba1\u7b97\u4ee3\u7406\u4e2d\u610f\u4e49\u7684\u884c\u4e3a\u80fd\u529b\u548c\u529f\u80fd\u3002"}}
{"id": "2507.21200", "categories": ["cs.CV", "cs.ET", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.21200", "abs": "https://arxiv.org/abs/2507.21200", "authors": ["Soren Pedersen", "Sanyam Jain", "Mikkel Chavez", "Viktor Ladehoff", "Bruna Neves de Freitas", "Ruben Pauwels"], "title": "PanoGAN A Deep Generative Model for Panoramic Dental Radiographs", "comment": null, "summary": "This paper presents the development of a generative adversarial network (GAN)\nfor synthesizing dental panoramic radiographs. Although exploratory in nature,\nthe study aims to address the scarcity of data in dental research and\neducation. We trained a deep convolutional GAN (DCGAN) using a Wasserstein loss\nwith gradient penalty (WGANGP) on a dataset of 2322 radiographs of varying\nquality. The focus was on the dentoalveolar regions, other anatomical\nstructures were cropped out. Extensive preprocessing and data cleaning were\nperformed to standardize the inputs while preserving anatomical variability. We\nexplored four candidate models by varying critic iterations, feature depth, and\nthe use of denoising prior to training. A clinical expert evaluated the\ngenerated radiographs based on anatomical visibility and realism, using a\n5-point scale (1 very poor 5 excellent). Most images showed moderate anatomical\ndepiction, although some were degraded by artifacts. A trade-off was observed\nthe model trained on non-denoised data yielded finer details especially in\nstructures like the mandibular canal and trabecular bone, while a model trained\non denoised data offered superior overall image clarity and sharpness. These\nfindings provide a foundation for future work on GAN-based methods in dental\nimaging.", "AI": {"tldr": "Developed a GAN to generate dental panoramic radiographs to address data scarcity. Clinical expert evaluation showed moderate anatomical depiction with trade-offs between detail and clarity.", "motivation": "The study aims to address the scarcity of data in dental research and education.", "method": "We trained a deep convolutional GAN (DCGAN) using a Wasserstein loss with gradient penalty (WGANGP) on a dataset of 2322 radiographs of varying quality.", "result": "Most images showed moderate anatomical depiction, although some were degraded by artifacts. A trade-off was observed the model trained on non-denoised data yielded finer details especially in structures like the mandibular canal and trabecular bone, while a model trained on denoised data offered superior overall image clarity and sharpness.", "conclusion": "These findings provide a foundation for future work on GAN-based methods in dental imaging."}}
{"id": "2507.21129", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21129", "abs": "https://arxiv.org/abs/2507.21129", "authors": ["Jae Wan Shim"], "title": "Measuring and Analyzing Intelligence via Contextual Uncertainty in Large Language Models using Information-Theoretic Metrics", "comment": null, "summary": "The remarkable capabilities of Large Language Models (LLMs) are now\nextensively documented on task-specific benchmarks, yet the internal mechanisms\nthat produce these results are the subject of intense scientific inquiry. This\npaper contributes to this inquiry by moving beyond metrics that measure\n\\textit{what} models can do, to a methodology that characterizes \\textit{how}\nthey process information. We introduce a novel, task-agnostic approach to probe\nthese dynamics by creating a quantitative ``Cognitive Profile\" for any given\nmodel. This profile is centered on the \\textbf{Entropy Decay Curve}, a\nvisualization that traces how a model's normalized predictive uncertainty\nchanges as a function of context length. Applying this methodology to several\nstate-of-the-art LLMs across diverse texts, we uncover unique and consistent\ncognitive profiles that are sensitive to both model scale and text complexity.\nWe also introduce the Information Gain Span (IGS) index to summarize the\ndesirability of the decay trajectory. This work thus provides a new, principled\nlens for analyzing and comparing the intrinsic operational dynamics of\nartificial intelligence.", "AI": {"tldr": "This paper introduces a new method to characterize how LLMs process information, moving beyond task-specific benchmarks.", "motivation": "The internal mechanisms of LLMs are the subject of intense scientific inquiry.", "method": "A novel, task-agnostic approach to probe these dynamics by creating a quantitative Cognitive Profile centered on the Entropy Decay Curve.", "result": "Unique and consistent cognitive profiles that are sensitive to both model scale and text complexity.", "conclusion": "This work provides a new lens for analyzing the intrinsic operational dynamics of AI."}}
{"id": "2507.21136", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.21136", "abs": "https://arxiv.org/abs/2507.21136", "authors": ["Mojtaba Moattari"], "title": "A Study on Variants of Conventional, Fuzzy, and Nullspace-Based Independence Criteria for Improving Supervised and Unsupervised Learning", "comment": null, "summary": "Unsupervised and supervised learning methods conventionally use kernels to\ncapture nonlinearities inherent in data structure. However experts have to\nensure their proposed nonlinearity maximizes variability and capture inherent\ndiversity of data. We reviewed all independence criteria to design unsupervised\nlearners. Then we proposed 3 independence criteria and used them to design\nunsupervised and supervised dimensionality reduction methods. We evaluated\ncontrast, accuracy and interpretability of these methods in both linear and\nneural nonlinear settings. The results show that the methods have outperformed\nthe baseline (tSNE, PCA, regularized LDA, VAE with (un)supervised learner and\nlayer sharing) and opened a new line of interpretable machine learning (ML) for\nthe researchers.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86 3 \u4e2a\u72ec\u7acb\u6027\u6807\u51c6\uff0c\u5e76\u4f7f\u7528\u5b83\u4eec\u6765\u8bbe\u8ba1\u65e0\u76d1\u7763\u548c\u6709\u76d1\u7763\u7684\u964d\u7ef4\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u4e0a\uff0c\u65e0\u76d1\u7763\u548c\u6709\u76d1\u7763\u7684\u5b66\u4e60\u65b9\u6cd5\u4f7f\u7528\u6838\u6765\u6355\u83b7\u6570\u636e\u7ed3\u6784\u4e2d\u56fa\u6709\u7684\u975e\u7ebf\u6027\u3002 \u7136\u800c\uff0c\u4e13\u5bb6\u5fc5\u987b\u786e\u4fdd\u4ed6\u4eec\u63d0\u51fa\u7684\u975e\u7ebf\u6027\u6700\u5927\u5316\u53ef\u53d8\u6027\u5e76\u6355\u83b7\u6570\u636e\u7684\u56fa\u6709\u591a\u6837\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86 3 \u4e2a\u72ec\u7acb\u6027\u6807\u51c6\uff0c\u5e76\u4f7f\u7528\u5b83\u4eec\u6765\u8bbe\u8ba1\u65e0\u76d1\u7763\u548c\u6709\u76d1\u7763\u7684\u964d\u7ef4\u65b9\u6cd5\u3002", "result": "\u5728\u7ebf\u6027\u548c\u795e\u7ecf\u975e\u7ebf\u6027\u8bbe\u7f6e\u4e2d\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u5bf9\u6bd4\u5ea6\u3001\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u4f18\u4e8e\u57fa\u7ebf\uff08tSNE\u3001PCA\u3001\u6b63\u5219\u5316 LDA\u3001\u5177\u6709\uff08\u975e\uff09\u76d1\u7763\u5b66\u4e60\u5668\u548c\u5c42\u5171\u4eab\u7684 VAE\uff09\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u4e3a\u7814\u7a76\u4eba\u5458\u5f00\u8f9f\u4e86\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60 (ML) \u7684\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.21467", "categories": ["cs.IR", "cs.SI"], "pdf": "https://arxiv.org/pdf/2507.21467", "abs": "https://arxiv.org/abs/2507.21467", "authors": ["Selimhan Dagtas", "Mert Can Cakmak", "Nitin Agarwal"], "title": "Efficient Data Retrieval and Comparative Bias Analysis of Recommendation Algorithms for YouTube Shorts and Long-Form Videos", "comment": null, "summary": "The growing popularity of short-form video content, such as YouTube Shorts,\nhas transformed user engagement on digital platforms, raising critical\nquestions about the role of recommendation algorithms in shaping user\nexperiences. These algorithms significantly influence content consumption, yet\nconcerns about biases, echo chambers, and content diversity persist. This study\ndevelops an efficient data collection framework to analyze YouTube's\nrecommendation algorithms for both short-form and long-form videos, employing\nparallel computing and advanced scraping techniques to overcome limitations of\nYouTube's API. The analysis uncovers distinct behavioral patterns in\nrecommendation algorithms across the two formats, with short-form videos\nshowing a more immediate shift toward engaging yet less diverse content\ncompared to long-form videos. Furthermore, a novel investigation into biases in\npolitically sensitive topics, such as the South China Sea dispute, highlights\nthe role of these algorithms in shaping narratives and amplifying specific\nviewpoints. By providing actionable insights for designing equitable and\ntransparent recommendation systems, this research underscores the importance of\nresponsible AI practices in the evolving digital media landscape.", "AI": {"tldr": "This study analyzes YouTube's recommendation algorithms for short-form and long-form videos, revealing differences in content diversity and biases in politically sensitive topics.", "motivation": "The growing popularity of short-form video content has transformed user engagement on digital platforms, raising critical questions about the role of recommendation algorithms in shaping user experiences. Concerns about biases, echo chambers, and content diversity persist.", "method": "This study develops an efficient data collection framework to analyze YouTube's recommendation algorithms for both short-form and long-form videos, employing parallel computing and advanced scraping techniques to overcome limitations of YouTube's API.", "result": "The analysis uncovers distinct behavioral patterns in recommendation algorithms across the two formats, with short-form videos showing a more immediate shift toward engaging yet less diverse content compared to long-form videos.  a novel investigation into biases in politically sensitive topics highlights the role of these algorithms in shaping narratives and amplifying specific viewpoints.", "conclusion": "This research underscores the importance of responsible AI practices in the evolving digital media landscape by providing actionable insights for designing equitable and transparent recommendation systems."}}
{"id": "2507.21340", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.21340", "abs": "https://arxiv.org/abs/2507.21340", "authors": ["Satyananda Kashyap", "Sola Shirai", "Nandana Mihindukulasooriya", "Horst Samulowitz"], "title": "StructText: A Synthetic Table-to-Text Approach for Benchmark Generation with Multi-Dimensional Evaluation", "comment": "Data available:\n  https://huggingface.co/datasets/ibm-research/struct-text and code available\n  at: https://github.com/ibm/struct-text", "summary": "Extracting structured information from text, such as key-value pairs that\ncould augment tabular data, is quite useful in many enterprise use cases.\nAlthough large language models (LLMs) have enabled numerous automated pipelines\nfor converting natural language into structured formats, there is still a lack\nof benchmarks for evaluating their extraction quality, especially in specific\ndomains or focused documents specific to a given organization. Building such\nbenchmarks by manual annotations is labour-intensive and limits the size and\nscalability of the benchmarks. In this work, we present StructText, an\nend-to-end framework for automatically generating high-fidelity benchmarks for\nkey-value extraction from text using existing tabular data. It uses available\ntabular data as structured ground truth, and follows a two-stage\n``plan-then-execute'' pipeline to synthetically generate corresponding\nnatural-language text. To ensure alignment between text and structured source,\nwe introduce a multi-dimensional evaluation strategy that combines (a)\nLLM-based judgments on factuality, hallucination, and coherence and (b)\nobjective extraction metrics measuring numeric and temporal accuracy. We\nevaluated the proposed method on 71,539 examples across 49 datasets. Results\nreveal that while LLMs achieve strong factual accuracy and avoid hallucination,\nthey struggle with narrative coherence in producing extractable text. Notably,\nmodels presume numerical and temporal information with high fidelity yet this\ninformation becomes embedded in narratives that resist automated extraction. We\nrelease a framework, including datasets, evaluation tools, and baseline\nextraction systems, to support continued research.", "AI": {"tldr": "\u63d0\u51fa\u4e86StructText\uff0c\u4e00\u4e2a\u5229\u7528\u8868\u683c\u6570\u636e\u81ea\u52a8\u751f\u6210\u952e\u503c\u63d0\u53d6\u57fa\u51c6\u7684\u6846\u67b6\u3002", "motivation": "\u7f3a\u4e4f\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63d0\u53d6\u8d28\u91cf\u7684\u57fa\u51c6\uff0c\u5c24\u5176\u662f\u5728\u7279\u5b9a\u9886\u57df\u6216\u7279\u5b9a\u4e8e\u7ed9\u5b9a\u7ec4\u7ec7\u7684\u91cd\u70b9\u6587\u6863\u4e2d\u3002\u624b\u52a8\u6ce8\u91ca\u6784\u5efa\u6b64\u7c7b\u57fa\u51c6\u65e2\u8d39\u529b\u53c8\u9650\u5236\u4e86\u57fa\u51c6\u7684\u5927\u5c0f\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u6846\u67b6StructText\uff0c\u7528\u4e8e\u4ece\u6587\u672c\u4e2d\u81ea\u52a8\u751f\u6210\u952e\u503c\u63d0\u53d6\u7684\u9ad8\u4fdd\u771f\u57fa\u51c6\uff0c\u4f7f\u7528\u73b0\u6709\u7684\u8868\u683c\u6570\u636e\u3002\u5b83\u4f7f\u7528\u53ef\u7528\u7684\u8868\u683c\u6570\u636e\u4f5c\u4e3a\u7ed3\u6784\u5316\u7684\u57fa\u672c\u4e8b\u5b9e\uff0c\u5e76\u9075\u5faa\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u201c\u8ba1\u5212-\u7136\u540e\u6267\u884c\u201d\u6d41\u7a0b\u6765\u5408\u6210\u751f\u6210\u76f8\u5e94\u7684\u81ea\u7136\u8bed\u8a00\u6587\u672c\u3002", "result": "\u5728 49 \u4e2a\u6570\u636e\u96c6\u7684 71,539 \u4e2a\u793a\u4f8b\u4e0a\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\u3002\u7ed3\u679c\u8868\u660e\uff0c\u867d\u7136 LLM \u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u5e76\u907f\u514d\u4e86\u5e7b\u89c9\uff0c\u4f46\u5b83\u4eec\u5728\u751f\u6210\u53ef\u63d0\u53d6\u6587\u672c\u7684\u53d9\u8ff0\u8fde\u8d2f\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "LLMs\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u5e76\u907f\u514d\u4e86\u5e7b\u89c9\uff0c\u4f46\u5728\u751f\u6210\u53ef\u63d0\u53d6\u6587\u672c\u7684\u53d9\u8ff0\u8fde\u8d2f\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002\u6a21\u578b\u80fd\u9ad8\u5ea6\u4fdd\u771f\u5730\u63a8\u5b9a\u6570\u503c\u548c\u65f6\u95f4\u4fe1\u606f\uff0c\u4f46\u8fd9\u4e9b\u4fe1\u606f\u5d4c\u5165\u5728\u96be\u4ee5\u81ea\u52a8\u63d0\u53d6\u7684\u53d9\u8ff0\u4e2d\u3002"}}
{"id": "2507.21083", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21083", "abs": "https://arxiv.org/abs/2507.21083", "authors": ["Franck Bardol"], "title": "ChatGPT Reads Your Tone and Responds Accordingly -- Until It Does Not -- Emotional Framing Induces Bias in LLM Outputs", "comment": null, "summary": "Large Language Models like GPT-4 adjust their responses not only based on the\nquestion asked, but also on how it is emotionally phrased. We systematically\nvary the emotional tone of 156 prompts - spanning controversial and everyday\ntopics - and analyze how it affects model responses. Our findings show that\nGPT-4 is three times less likely to respond negatively to a negatively framed\nquestion than to a neutral one. This suggests a \"rebound\" bias where the model\novercorrects, often shifting toward neutrality or positivity. On sensitive\ntopics (e.g., justice or politics), this effect is even more pronounced:\ntone-based variation is suppressed, suggesting an alignment override. We\nintroduce concepts like the \"tone floor\" - a lower bound in response negativity\n- and use tone-valence transition matrices to quantify behavior. Visualizations\nbased on 1536-dimensional embeddings confirm semantic drift based on tone. Our\nwork highlights an underexplored class of biases driven by emotional framing in\nprompts, with implications for AI alignment and trust. Code and data are\navailable at: https://github.com/bardolfranck/llm-responses-viewer", "AI": {"tldr": "GPT-4's responses are significantly influenced by the emotional tone of prompts, showing a bias towards neutrality or positivity, especially on sensitive topics.", "motivation": "Investigating how the emotional phrasing of prompts affects the responses of Large Language Models like GPT-4.", "method": "Systematically varied the emotional tone of 156 prompts across controversial and everyday topics and analyzed model responses.", "result": "GPT-4 is three times less likely to respond negatively to a negatively framed question than to a neutral one. The study introduces concepts like the 'tone floor' and uses tone-valence transition matrices to quantify behavior. Visualizations confirm semantic drift based on tone.", "conclusion": "GPT-4 exhibits a 'rebound' bias, overcorrecting in response to negative prompts, especially on sensitive topics where tone-based variation is suppressed, suggesting an alignment override."}}
{"id": "2507.21246", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21246", "abs": "https://arxiv.org/abs/2507.21246", "authors": ["Monika Shah", "Somdeb Sarkhel", "Deepak Venugopal"], "title": "On Explaining Visual Captioning with Hybrid Markov Logic Networks", "comment": null, "summary": "Deep Neural Networks (DNNs) have made tremendous progress in multimodal tasks\nsuch as image captioning. However, explaining/interpreting how these models\nintegrate visual information, language information and knowledge representation\nto generate meaningful captions remains a challenging problem. Standard metrics\nto measure performance typically rely on comparing generated captions with\nhuman-written ones that may not provide a user with a deep insights into this\nintegration. In this work, we develop a novel explanation framework that is\neasily interpretable based on Hybrid Markov Logic Networks (HMLNs) - a language\nthat can combine symbolic rules with real-valued functions - where we\nhypothesize how relevant examples from the training data could have influenced\nthe generation of the observed caption. To do this, we learn a HMLN\ndistribution over the training instances and infer the shift in distributions\nover these instances when we condition on the generated sample which allows us\nto quantify which examples may have been a source of richer information to\ngenerate the observed caption. Our experiments on captions generated for\nseveral state-of-the-art captioning models using Amazon Mechanical Turk\nillustrate the interpretability of our explanations, and allow us to compare\nthese models along the dimension of explainability.", "AI": {"tldr": "develop a novel explanation framework based on Hybrid Markov Logic Networks to interpret how models integrate information to generate captions", "motivation": "explaining/interpreting how these models integrate visual information, language information and knowledge representation to generate meaningful captions remains a challenging problem. Standard metrics to measure performance typically rely on comparing generated captions with human-written ones that may not provide a user with a deep insights into this integration.", "method": "develop a novel explanation framework that is easily interpretable based on Hybrid Markov Logic Networks (HMLNs)", "result": "learn a HMLN distribution over the training instances and infer the shift in distributions over these instances when we condition on the generated sample which allows us to quantify which examples may have been a source of richer information to generate the observed caption.", "conclusion": "Experiments on captions generated for several state-of-the-art captioning models using Amazon Mechanical Turk illustrate the interpretability of our explanations, and allow us to compare these models along the dimension of explainability."}}
{"id": "2507.21130", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21130", "abs": "https://arxiv.org/abs/2507.21130", "authors": ["Bintao Tang", "Xin Yang", "Yuhao Wang", "Zixuan Qiu", "Zimo Ji", "Wenyuan Jiang"], "title": "INTEGRALBENCH: Benchmarking LLMs with Definite Integral Problems", "comment": "19 pages, 5 figures", "summary": "We present INTEGRALBENCH, a focused benchmark designed to evaluate Large\nLanguage Model (LLM) performance on definite integral problems. INTEGRALBENCH\nprovides both symbolic and numerical ground truth solutions with manual\ndifficulty annotations. Our evaluation of nine state-of-the-art LLMs reveals\nsignificant performance gaps and strong correlations between problem difficulty\nand model accuracy, establishing baseline metrics for this challenging domain.\nINTEGRALBENCH aims to advance automated mathematical reasoning by providing a\nrigorous evaluation framework specifically tailored for definite integral\ncomputation.", "AI": {"tldr": "INTEGRALBENCH: a benchmark for evaluating LLM performance on definite integral problems.", "motivation": "To advance automated mathematical reasoning by providing a rigorous evaluation framework for definite integral computation.", "method": "Evaluation of nine state-of-the-art LLMs on definite integral problems.", "result": "Significant performance gaps and strong correlations between problem difficulty and model accuracy.", "conclusion": "INTEGRALBENCH establishes baseline metrics for definite integral computation and reveals performance gaps in LLMs."}}
{"id": "2507.21147", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21147", "abs": "https://arxiv.org/abs/2507.21147", "authors": ["Fabrizio Lo Scudo", "Alessio De Rango", "Luca Furnari", "Alfonso Senatore", "Donato D'Ambrosio", "Giuseppe Mendicino", "Gianluigi Greco"], "title": "Advancing Wildfire Risk Prediction via Morphology-Aware Curriculum Contrastive Learning", "comment": "To appear in the Proceedings of ECAI 2025", "summary": "Wildfires significantly impact natural ecosystems and human health, leading\nto biodiversity loss, increased hydrogeological risks, and elevated emissions\nof toxic substances. Climate change exacerbates these effects, particularly in\nregions with rising temperatures and prolonged dry periods, such as the\nMediterranean. This requires the development of advanced risk management\nstrategies that utilize state-of-the-art technologies. However, in this\ncontext, the data show a bias toward an imbalanced setting, where the incidence\nof wildfire events is significantly lower than typical situations. This\nimbalance, coupled with the inherent complexity of high-dimensional\nspatio-temporal data, poses significant challenges for training deep learning\narchitectures. Moreover, since precise wildfire predictions depend mainly on\nweather data, finding a way to reduce computational costs to enable more\nfrequent updates using the latest weather forecasts would be beneficial. This\npaper investigates how adopting a contrastive framework can address these\nchallenges through enhanced latent representations for the patch's dynamic\nfeatures. We thus introduce a new morphology-based curriculum contrastive\nlearning that mitigates issues associated with diverse regional characteristics\nand enables the use of smaller patch sizes without compromising performance. An\nexperimental analysis is performed to validate the effectiveness of the\nproposed modeling strategies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u91ce\u706b\u9884\u6d4b\u4e2d\u7684\u6570\u636e\u4e0d\u5e73\u8861\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u91ce\u706b\u5bf9\u81ea\u7136\u751f\u6001\u7cfb\u7edf\u548c\u4eba\u7c7b\u5065\u5eb7\u6709\u663e\u8457\u5f71\u54cd\uff0c\u6c14\u5019\u53d8\u5316\u52a0\u5267\u4e86\u8fd9\u4e9b\u5f71\u54cd\u3002\u73b0\u6709\u7684\u91ce\u706b\u4e8b\u4ef6\u6570\u636e\u4e0d\u5e73\u8861\uff0c\u4e14\u9ad8\u7ef4\u65f6\u7a7a\u6570\u636e\u7ed9\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u5e26\u6765\u6311\u6218\u3002\u4e3a\u4e86\u80fd\u591f\u66f4\u9891\u7e41\u5730\u4f7f\u7528\u6700\u65b0\u7684\u5929\u6c14\u9884\u62a5\u66f4\u65b0\u6a21\u578b\uff0c\u9700\u8981\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u6f5c\u5728\u8868\u5f81\u6765\u5904\u7406\u65f6\u7a7a\u6570\u636e\u7684\u590d\u6742\u6027\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u5f62\u6001\u7684\u8bfe\u7a0b\u5b66\u4e60\u6765\u7f13\u89e3\u533a\u57df\u7279\u5f81\u5dee\u5f02\u5e26\u6765\u7684\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u5efa\u6a21\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u5f62\u6001\u7684\u8bfe\u7a0b\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u91ce\u706b\u9884\u6d4b\u4e2d\u6570\u636e\u4e0d\u5e73\u8861\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002"}}
{"id": "2507.21520", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.21520", "abs": "https://arxiv.org/abs/2507.21520", "authors": ["Zijian Zhang", "Xiaocheng Zhang", "Yang Zhou", "Zhimin Lin", "Peng Yan"], "title": "Solution for Meta KDD Cup'25: A Comprehensive Three-Step Framework for Vision Question Answering", "comment": null, "summary": "Vision Large Language Models (VLLMs) have improved multi-modal understanding\nand visual question answering (VQA), but still suffer from hallucinated\nanswers. Multi-modal Retrieval-Augmented Generation (RAG) helps address these\nissues by incorporating external information, yet challenges remain in visual\ncontext comprehension, multi-source retrieval, and multi-turn interactions. To\naddress these challenges, Meta constructed the CRAG-MM benchmark and launched\nthe CRAG-MM Challenge at KDD Cup 2025, which consists of three tasks. This\npaper describes the solutions of all tasks in Meta KDD Cup'25 from BlackPearl\nteam. We use a single model for each task, with key methods including data\naugmentation, RAG, reranking, and multi-task fine-tuning. Our solution achieve\nautomatic evaluation rankings of 3rd, 3rd, and 1st on the three tasks, and win\nsecond place in Task3 after human evaluation.", "AI": {"tldr": "BlackPearl team's solution to the CRAG-MM Challenge at KDD Cup 2025 uses data augmentation, RAG, reranking, and multi-task fine-tuning to achieve high rankings in all three tasks.", "motivation": "Vision Large Language Models (VLLMs) struggle with hallucinated answers, and challenges remain in visual context comprehension, multi-source retrieval, and multi-turn interactions in Multi-modal Retrieval-Augmented Generation (RAG).", "method": "The solution employs data augmentation, RAG, reranking, and multi-task fine-tuning, using a single model for each task.", "result": "The BlackPearl team achieved 3rd, 3rd, and 1st place in the automatic evaluation of the three tasks and won second place in Task3 after human evaluation.", "conclusion": "BlackPearl team's solution achieves strong performance in the CRAG-MM Challenge, securing 3rd, 3rd, and 1st place in automatic evaluation and 2nd place in Task3 after human evaluation."}}
{"id": "2507.21084", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21084", "abs": "https://arxiv.org/abs/2507.21084", "authors": ["Aly M. Kassem", "Zhuan Shi", "Negar Rostamzadeh", "Golnoosh Farnadi"], "title": "Reviving Your MNEME: Predicting The Side Effects of LLM Unlearning and Fine-Tuning via Sparse Model Diffing", "comment": null, "summary": "Large language models (LLMs) are frequently fine-tuned or unlearned to adapt\nto new tasks or eliminate undesirable behaviors. While existing evaluation\nmethods assess performance after such interventions, there remains no general\napproach for detecting unintended side effects, such as unlearning biology\ncontent degrading performance on chemistry tasks, particularly when these\neffects are unpredictable or emergent. To address this issue, we introduce\nMNEME, Model diffiNg for Evaluating Mechanistic Effects, a lightweight\nframework for identifying these side effects using sparse model diffing. MNEME\ncompares base and fine-tuned models on task-agnostic data (for example, The\nPile, LMSYS-Chat-1M) without access to fine-tuning data to isolate behavioral\nshifts. Applied to five LLMs across three scenarios: WMDP knowledge unlearning,\nemergent misalignment, and benign fine-tuning, MNEME achieves up to 95 percent\naccuracy in predicting side effects, aligning with known benchmarks and\nrequiring no custom heuristics. Furthermore, we show that retraining on\nhigh-activation samples can partially reverse these effects. Our results\ndemonstrate that sparse probing and diffing offer a scalable and automated lens\ninto fine-tuning-induced model changes, providing practical tools for\nunderstanding and managing LLM behavior.", "AI": {"tldr": "MNEME \u901a\u8fc7\u7a00\u758f\u6a21\u578b\u5dee\u5f02\u5316\u6765\u8bc6\u522b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u4e2d\u7684\u526f\u4f5c\u7528\u3002", "motivation": "\u73b0\u6709\u7684\u8bc4\u4f30\u65b9\u6cd5\u8bc4\u4f30\u5e72\u9884\u540e\u7684\u6027\u80fd\uff0c\u4f46\u4ecd\u7136\u6ca1\u6709\u901a\u7528\u7684\u65b9\u6cd5\u6765\u68c0\u6d4b\u610f\u5916\u7684\u526f\u4f5c\u7528\uff0c\u4f8b\u5982\u751f\u7269\u5b66\u5185\u5bb9\u7684\u9057\u5fd8\u964d\u4f4e\u4e86\u5316\u5b66\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u8fd9\u4e9b\u5f71\u54cd\u662f\u4e0d\u53ef\u9884\u6d4b\u7684\u6216\u7a81\u53d1\u7684\u60c5\u51b5\u4e0b\u3002", "method": "MNEME \u6bd4\u8f83\u57fa\u7840\u6a21\u578b\u548c\u5fae\u8c03\u6a21\u578b\u5728\u4efb\u52a1\u65e0\u5173\u6570\u636e\u4e0a\u7684\u8868\u73b0\uff0c\u65e0\u9700\u8bbf\u95ee\u5fae\u8c03\u6570\u636e\u5373\u53ef\u9694\u79bb\u884c\u4e3a\u53d8\u5316\u3002", "result": "MNEME \u5728\u9884\u6d4b\u526f\u4f5c\u7528\u65b9\u9762\u8fbe\u5230\u4e86\u9ad8\u8fbe 95% \u7684\u51c6\u786e\u7387\uff0c\u4e0e\u5df2\u77e5\u57fa\u51c6\u5bf9\u9f50\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u81ea\u5b9a\u4e49\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u5728\u91cd\u65b0\u8bad\u7ec3\u9ad8\u6fc0\u6d3b\u6837\u672c\u540e\uff0c\u53ef\u4ee5\u90e8\u5206\u9006\u8f6c\u8fd9\u4e9b\u5f71\u54cd\u3002", "conclusion": "\u7a00\u758f\u63a2\u67e5\u548c\u5dee\u5f02\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u89c6\u89d2\uff0c\u53ef\u4ee5\u4e86\u89e3\u5fae\u8c03\u5f15\u8d77\u7684\u6a21\u578b\u53d8\u5316\uff0c\u4ece\u800c\u4e3a\u7406\u89e3\u548c\u7ba1\u7406 LLM \u884c\u4e3a\u63d0\u4f9b\u5b9e\u7528\u7684\u5de5\u5177\u3002"}}
{"id": "2507.21247", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.21247", "abs": "https://arxiv.org/abs/2507.21247", "authors": ["Ankit Singh", "Efstratios Gavves", "Cees G. M. Snoek", "Hilde Kuehne"], "title": "Dual Guidance Semi-Supervised Action Detection", "comment": null, "summary": "Semi-Supervised Learning (SSL) has shown tremendous potential to improve the\npredictive performance of deep learning models when annotations are hard to\nobtain. However, the application of SSL has so far been mainly studied in the\ncontext of image classification. In this work, we present a semi-supervised\napproach for spatial-temporal action localization. We introduce a dual guidance\nnetwork to select better pseudo-bounding boxes. It combines a frame-level\nclassification with a bounding-box prediction to enforce action class\nconsistency across frames and boxes. Our evaluation across well-known\nspatial-temporal action localization datasets, namely UCF101-24 , J-HMDB-21 and\nAVA shows that the proposed module considerably enhances the model's\nperformance in limited labeled data settings. Our framework achieves superior\nresults compared to extended image-based semi-supervised baselines.", "AI": {"tldr": "This paper presents a semi-supervised approach for spatial-temporal action localization using a dual guidance network, achieving superior results with limited labeled data.", "motivation": "Semi-Supervised Learning (SSL) has shown tremendous potential to improve the predictive performance of deep learning models when annotations are hard to obtain. However, the application of SSL has so far been mainly studied in the context of image classification.", "method": "A dual guidance network is introduced to select better pseudo-bounding boxes, combining frame-level classification with bounding-box prediction to enforce action class consistency.", "result": "The proposed module considerably enhances the model's performance in limited labeled data settings on UCF101-24, J-HMDB-21 and AVA datasets.", "conclusion": "The proposed semi-supervised framework achieves superior results compared to extended image-based semi-supervised baselines in spatial-temporal action localization."}}
{"id": "2507.21131", "categories": ["cs.AI", "68T05", "H.5.1; I.2.6; C.4"], "pdf": "https://arxiv.org/pdf/2507.21131", "abs": "https://arxiv.org/abs/2507.21131", "authors": ["Madhava Gaikwad", "Ashwini Ramchandra Doke"], "title": "NPO: Learning Alignment and Meta-Alignment through Structured Human Feedback", "comment": "20 pages", "summary": "We present NPO, an alignment-aware learning framework that operationalizes\nfeedback-driven adaptation in human-in-the-loop decision systems. Unlike prior\napproaches that treat alignment as a static or post-hoc property, NPO\nintroduces a formalization of alignment loss that is measurable, supervisable,\nand reducible under structured feedback. In parallel, we propose meta-alignment\nas the fidelity of the monitoring process that governs retraining or override\ntriggers, and show that it is formally reducible to primary alignment via\nthreshold fidelity. Our implementation spans a scalable operational loop\ninvolving scenario scoring, threshold tuning, policy validation, and structured\nfeedback ingestion, including \"likes\", overrides, and abstentions. We provide\nformal convergence results under stochastic feedback and show that both\nalignment loss and monitoring fidelity converge additively. Empirically, NPO\ndemonstrates measurable value in hyperscale deployment settings. A\nsimulation-based artifact and ablation studies further illustrate the\ntheoretical principles in action. Together, NPO offers a compact, inspectable\narchitecture for continual alignment monitoring, helping bridge theoretical\nalignment guarantees with practical reliability in dynamic environments.", "AI": {"tldr": "NPO\u662f\u4e00\u79cd\u5bf9\u9f50\u611f\u77e5\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u5b83\u5f62\u5f0f\u5316\u4e86\u5bf9\u9f50\u635f\u5931\uff0c\u5e76\u8bc1\u660e\u4e86\u5b83\u5728\u8d85\u5927\u89c4\u6a21\u90e8\u7f72\u8bbe\u7f6e\u4e2d\u5177\u6709\u53ef\u8861\u91cf\u7684\u4ef7\u503c\u3002", "motivation": "\u672c\u6587\u63d0\u51fa\u4e86NPO\uff0c\u4e00\u4e2a\u5bf9\u9f50\u611f\u77e5\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u5b83\u5728\u4eba\u673a\u4ea4\u4e92\u51b3\u7b56\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u53cd\u9988\u9a71\u52a8\u7684\u9002\u5e94\u3002\u4e0e\u5148\u524d\u5c06\u5bf9\u9f50\u89c6\u4e3a\u9759\u6001\u6216\u4e8b\u540e\u5c5e\u6027\u7684\u65b9\u6cd5\u4e0d\u540c\u3002", "method": "NPO\u5f15\u5165\u4e86\u4e00\u79cd\u5bf9\u9f50\u635f\u5931\u7684\u5f62\u5f0f\u5316\uff0c\u8be5\u5f62\u5f0f\u5316\u662f\u53ef\u6d4b\u91cf\u7684\u3001\u53ef\u76d1\u7763\u7684\uff0c\u5e76\u4e14\u53ef\u4ee5\u5728\u7ed3\u6784\u5316\u53cd\u9988\u4e0b\u51cf\u5c11\u3002", "result": "NPO\u5728\u8d85\u5927\u89c4\u6a21\u90e8\u7f72\u8bbe\u7f6e\u4e2d\u5c55\u793a\u4e86\u53ef\u8861\u91cf\u7684\u4ef7\u503c\u3002\u57fa\u4e8e\u6a21\u62df\u7684artifact\u548c\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u8bf4\u660e\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u7406\u8bba\u539f\u7406\u3002", "conclusion": "NPO\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7d27\u51d1\u3001\u53ef\u68c0\u67e5\u7684\u6301\u7eed\u5bf9\u9f50\u76d1\u63a7\u67b6\u6784\uff0c\u6709\u52a9\u4e8e\u5f25\u5408\u7406\u8bba\u5bf9\u9f50\u4fdd\u8bc1\u4e0e\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u53ef\u9760\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2507.21152", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21152", "abs": "https://arxiv.org/abs/2507.21152", "authors": ["Hangli Ge", "Noboru Koshizuka"], "title": "Deep Unfolding for MIMO Signal Detection", "comment": null, "summary": "In this paper, we propose a deep unfolding neural network-based MIMO detector\nthat incorporates complex-valued computations using Wirtinger calculus. The\nmethod, referred as Dynamic Partially Shrinkage Thresholding (DPST), enables\nefficient, interpretable, and low-complexity MIMO signal detection. Unlike\nprior approaches that rely on real-valued approximations, our method operates\nnatively in the complex domain, aligning with the fundamental nature of signal\nprocessing tasks. The proposed algorithm requires only a small number of\ntrainable parameters, allowing for simplified training. Numerical results\ndemonstrate that the proposed method achieves superior detection performance\nwith fewer iterations and lower computational complexity, making it a practical\nsolution for next-generation massive MIMO systems.", "AI": {"tldr": "a deep unfolding neural network-based MIMO detector", "motivation": "prior approaches that rely on real-valued approximations", "method": "a deep unfolding neural network-based MIMO detector that incorporates complex-valued computations using Wirtinger calculus. The method, referred as Dynamic Partially Shrinkage Thresholding (DPST)", "result": "achieves superior detection performance with fewer iterations and lower computational complexity", "conclusion": "The proposed method achieves superior detection performance with fewer iterations and lower computational complexity, making it a practical solution for next-generation massive MIMO systems."}}
{"id": "2507.21563", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21563", "abs": "https://arxiv.org/abs/2507.21563", "authors": ["Minh-Anh Nguyen", "Bao Nguyen", "Ha Lan N. T.", "Tuan Anh Hoang", "Duc-Trong Le", "Dung D. Le"], "title": "Enhancing Graph-based Recommendations with Majority-Voting LLM-Rerank Augmentation", "comment": null, "summary": "Recommendation systems often suffer from data sparsity caused by limited\nuser-item interactions, which degrade their performance and amplify popularity\nbias in real-world scenarios. This paper proposes a novel data augmentation\nframework that leverages Large Language Models (LLMs) and item textual\ndescriptions to enrich interaction data. By few-shot prompting LLMs multiple\ntimes to rerank items and aggregating the results via majority voting, we\ngenerate high-confidence synthetic user-item interactions, supported by\ntheoretical guarantees based on the concentration of measure. To effectively\nleverage the augmented data in the context of a graph recommendation system, we\nintegrate it into a graph contrastive learning framework to mitigate\ndistributional shift and alleviate popularity bias. Extensive experiments show\nthat our method improves accuracy and reduces popularity bias, outperforming\nstrong baselines.", "AI": {"tldr": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u9879\u76ee\u6587\u672c\u63cf\u8ff0\u6765\u4e30\u5bcc\u4ea4\u4e92\u6570\u636e\uff0c\u4ece\u800c\u63d0\u9ad8\u51c6\u786e\u6027\u5e76\u51cf\u5c11\u6d41\u884c\u5ea6\u504f\u5dee\u3002", "motivation": "\u63a8\u8350\u7cfb\u7edf\u7ecf\u5e38\u53d7\u5230\u7528\u6237-\u9879\u76ee\u4ea4\u4e92\u6709\u9650\u5bfc\u81f4\u7684\u6570\u636e\u7a00\u758f\u6027\u7684\u5f71\u54cd\uff0c\u8fd9\u4f1a\u964d\u4f4e\u5b83\u4eec\u7684\u6027\u80fd\u5e76\u653e\u5927\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u6d41\u884c\u5ea6\u504f\u5dee\u3002", "method": "\u901a\u8fc7\u591a\u6b21\u5c11\u91cf\u63d0\u793a LLM \u5bf9\u9879\u76ee\u91cd\u65b0\u6392\u5e8f\u5e76\u901a\u8fc7\u591a\u6570\u6295\u7968\u805a\u5408\u7ed3\u679c\uff0c\u751f\u6210\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u5408\u6210\u7528\u6237-\u9879\u76ee\u4ea4\u4e92\uff0c\u5e76\u57fa\u4e8e\u96c6\u4e2d\u5ea6\u6d4b\u91cf\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u3002\u5c06\u5176\u96c6\u6210\u5230\u56fe\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u4e2d\uff0c\u4ee5\u51cf\u8f7b\u5206\u5e03\u504f\u79fb\u5e76\u7f13\u89e3\u6d41\u884c\u5ea6\u504f\u5dee\u3002", "result": "\u901a\u8fc7\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u548c\u9879\u76ee\u6587\u672c\u63cf\u8ff0\u6765\u4e30\u5bcc\u4ea4\u4e92\u6570\u636e\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u5408\u6210\u7528\u6237-\u9879\u76ee\u4ea4\u4e92\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u5e76\u51cf\u5c11\u4e86\u6d41\u884c\u5ea6\u504f\u5dee\uff0c\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\u3002"}}
{"id": "2507.21086", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.21086", "abs": "https://arxiv.org/abs/2507.21086", "authors": ["Jaydip Sen", "Subhasis Dasgupta", "Hetvi Waghela"], "title": "Multi-Amateur Contrastive Decoding for Text Generation", "comment": "This paper has been accepted for oral presentation and publication in\n  the proceedings of the IEEE I2ITCON 2025. The conference will be organized in\n  Pune, India, from July 4 to 5, 2025. This is the accepted version of the\n  paper and NOT the final camera-ready version. The paper is 11 pages long and\n  contains 5 figures and 6 tables", "summary": "Contrastive Decoding (CD) has emerged as an effective inference-time strategy\nfor enhancing open-ended text generation by exploiting the divergence in output\nprobabilities between a large expert language model and a smaller amateur\nmodel. Although CD improves coherence and fluency, its dependence on a single\namateur restricts its capacity to capture the diverse and multifaceted failure\nmodes of language generation, such as repetition, hallucination, and stylistic\ndrift. This paper proposes Multi-Amateur Contrastive Decoding (MACD), a\ngeneralization of the CD framework that employs an ensemble of amateur models\nto more comprehensively characterize undesirable generation patterns. MACD\nintegrates contrastive signals through both averaging and consensus\npenalization mechanisms and extends the plausibility constraint to operate\neffectively in the multi-amateur setting. Furthermore, the framework enables\ncontrollable generation by incorporating amateurs with targeted stylistic or\ncontent biases. Experimental results across multiple domains, such as news,\nencyclopedic, and narrative, demonstrate that MACD consistently surpasses\nconventional decoding methods and the original CD approach in terms of fluency,\ncoherence, diversity, and adaptability, all without requiring additional\ntraining or fine-tuning.", "AI": {"tldr": "This paper introduces Multi-Amateur Contrastive Decoding (MACD), which enhances text generation by using multiple amateur models to overcome the limitations of single-amateur contrastive decoding. MACD improves fluency, coherence, diversity, and adaptability without extra training.", "motivation": "Contrastive Decoding (CD) improves coherence and fluency, its dependence on a single amateur restricts its capacity to capture the diverse and multifaceted failure modes of language generation, such as repetition, hallucination, and stylistic drift.", "method": "This paper proposes Multi-Amateur Contrastive Decoding (MACD), a generalization of the CD framework that employs an ensemble of amateur models to more comprehensively characterize undesirable generation patterns. MACD integrates contrastive signals through both averaging and consensus penalization mechanisms and extends the plausibility constraint to operate effectively in the multi-amateur setting. Furthermore, the framework enables controllable generation by incorporating amateurs with targeted stylistic or content biases.", "result": "Experimental results across multiple domains, such as news, encyclopedic, and narrative, demonstrate that MACD consistently surpasses conventional decoding methods and the original CD approach in terms of fluency, coherence, diversity, and adaptability, all without requiring additional training or fine-tuning.", "conclusion": "Multi-Amateur Contrastive Decoding (MACD) consistently surpasses conventional decoding methods and the original CD approach in terms of fluency, coherence, diversity, and adaptability, all without requiring additional training or fine-tuning."}}
{"id": "2507.21256", "categories": ["cs.CV", "I.4.8"], "pdf": "https://arxiv.org/pdf/2507.21256", "abs": "https://arxiv.org/abs/2507.21256", "authors": ["Christopher Indris", "Raiyan Rahman", "Goetz Bramesfeld", "Guanghui Wang"], "title": "Tracking Moose using Aerial Object Detection", "comment": "18 pages, 6 figures, 8 tables", "summary": "Aerial wildlife tracking is critical for conservation efforts and relies on\ndetecting small objects on the ground below the aircraft. It presents technical\nchallenges: crewed aircraft are expensive, risky and disruptive; autonomous\ndrones have limited computational capacity for onboard AI systems. Since the\nobjects of interest may appear only a few pixels wide, small object detection\nis an inherently challenging computer vision subfield compounded by\ncomputational efficiency needs. This paper applies a patching augmentation to\ndatasets to study model performance under various settings. A comparative study\nof three common yet architecturally diverse object detectors is conducted using\nthe data, varying the patching method's hyperparameters against detection\naccuracy. Each model achieved at least 93\\% mAP@IoU=0.5 on at least one\npatching configuration. Statistical analyses provide an in-depth commentary on\nthe effects of various factors. Analysis also shows that faster, simpler models\nare about as effective as models that require more computational power for this\ntask and perform well given limited patch scales, encouraging UAV deployment.\nDatasets and models will be made available via\nhttps://github.com/chrisindris/Moose.", "AI": {"tldr": "This paper studies the performance of object detection models on aerial wildlife tracking using patching augmentation. It finds that simpler models are as effective as more complex ones, encouraging UAV deployment.", "motivation": "Aerial wildlife tracking is critical for conservation efforts and relies on detecting small objects on the ground below the aircraft. It presents technical challenges: crewed aircraft are expensive, risky and disruptive; autonomous drones have limited computational capacity for onboard AI systems. Since the objects of interest may appear only a few pixels wide, small object detection is an inherently challenging computer vision subfield compounded by computational efficiency needs.", "method": "This paper applies a patching augmentation to datasets to study model performance under various settings. A comparative study of three common yet architecturally diverse object detectors is conducted using the data, varying the patching method's hyperparameters against detection accuracy.", "result": "Each model achieved at least 93% mAP@IoU=0.5 on at least one patching configuration. Statistical analyses provide an in-depth commentary on the effects of various factors. Analysis also shows that faster, simpler models are about as effective as models that require more computational power for this task and perform well given limited patch scales, encouraging UAV deployment.", "conclusion": "Each model achieved at least 93% mAP@IoU=0.5 on at least one patching configuration. Statistical analyses provide an in-depth commentary on the effects of various factors. Analysis also shows that faster, simpler models are about as effective as models that require more computational power for this task and perform well given limited patch scales, encouraging UAV deployment."}}
{"id": "2507.21132", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21132", "abs": "https://arxiv.org/abs/2507.21132", "authors": ["Joshua Adrian Cahyono", "Saran Subramanian"], "title": "Can You Trust an LLM with Your Life-Changing Decision? An Investigation into AI High-Stakes Responses", "comment": null, "summary": "Large Language Models (LLMs) are increasingly consulted for high-stakes life\nadvice, yet they lack standard safeguards against providing confident but\nmisguided responses. This creates risks of sycophancy and over-confidence. This\npaper investigates these failure modes through three experiments: (1) a\nmultiple-choice evaluation to measure model stability against user pressure;\n(2) a free-response analysis using a novel safety typology and an LLM Judge;\nand (3) a mechanistic interpretability experiment to steer model behavior by\nmanipulating a \"high-stakes\" activation vector. Our results show that while\nsome models exhibit sycophancy, others like o4-mini remain robust.\nTop-performing models achieve high safety scores by frequently asking\nclarifying questions, a key feature of a safe, inquisitive approach, rather\nthan issuing prescriptive advice. Furthermore, we demonstrate that a model's\ncautiousness can be directly controlled via activation steering, suggesting a\nnew path for safety alignment. These findings underscore the need for nuanced,\nmulti-faceted benchmarks to ensure LLMs can be trusted with life-changing\ndecisions.", "AI": {"tldr": "LLMs lack safeguards against misguided advice, leading to sycophancy and over-confidence. The paper explores these issues through experiments, finding that cautious models asking clarifying questions are safer and that model cautiousness can be controlled.", "motivation": "Large Language Models (LLMs) are increasingly consulted for high-stakes life advice, yet they lack standard safeguards against providing confident but misguided responses. This creates risks of sycophancy and over-confidence.", "method": "This paper investigates these failure modes through three experiments: (1) a multiple-choice evaluation to measure model stability against user pressure; (2) a free-response analysis using a novel safety typology and an LLM Judge; and (3) a mechanistic interpretability experiment to steer model behavior by manipulating a \"high-stakes\" activation vector.", "result": "Our results show that while some models exhibit sycophancy, others like o4-mini remain robust.", "conclusion": "Top-performing models achieve high safety scores by frequently asking clarifying questions, a key feature of a safe, inquisitive approach, rather than issuing prescriptive advice. Furthermore, we demonstrate that a model's cautiousness can be directly controlled via activation steering, suggesting a new path for safety alignment. These findings underscore the need for nuanced, multi-faceted benchmarks to ensure LLMs can be trusted with life-changing decisions."}}
{"id": "2507.21153", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.21153", "abs": "https://arxiv.org/abs/2507.21153", "authors": ["Abderaouf Bahi", "Amel Ourici"], "title": "Deep Reinforcement Learning for Real-Time Green Energy Integration in Data Centers", "comment": null, "summary": "This paper explores the implementation of a Deep Reinforcement Learning\n(DRL)-optimized energy management system for e-commerce data centers, aimed at\nenhancing energy efficiency, cost-effectiveness, and environmental\nsustainability. The proposed system leverages DRL algorithms to dynamically\nmanage the integration of renewable energy sources, energy storage, and grid\npower, adapting to fluctuating energy availability in real time. The study\ndemonstrates that the DRL-optimized system achieves a 38\\% reduction in energy\ncosts, significantly outperforming traditional Reinforcement Learning (RL)\nmethods (28\\%) and heuristic approaches (22\\%). Additionally, it maintains a\nlow SLA violation rate of 1.5\\%, compared to 3.0\\% for RL and 4.8\\% for\nheuristic methods. The DRL-optimized approach also results in an 82\\%\nimprovement in energy efficiency, surpassing other methods, and a 45\\%\nreduction in carbon emissions, making it the most environmentally friendly\nsolution. The system's cumulative reward of 950 reflects its superior\nperformance in balancing multiple objectives. Through rigorous testing and\nablation studies, the paper validates the effectiveness of the DRL model's\narchitecture and parameters, offering a robust solution for energy management\nin data centers. The findings highlight the potential of DRL in advancing\nenergy optimization strategies and addressing sustainability challenges.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cdDRL\u4f18\u5316\u7684\u80fd\u6e90\u7ba1\u7406\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5728\u964d\u4f4e\u80fd\u6e90\u6210\u672c\u3001\u63d0\u9ad8\u80fd\u6e90\u6548\u7387\u548c\u51cf\u5c11\u78b3\u6392\u653e\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u65e8\u5728\u63d0\u9ad8\u80fd\u6e90\u6548\u7387\u3001\u6210\u672c\u6548\u76ca\u548c\u73af\u5883\u53ef\u6301\u7eed\u6027\u3002", "method": "\u5229\u7528DRL\u7b97\u6cd5\u52a8\u6001\u7ba1\u7406\u53ef\u518d\u751f\u80fd\u6e90\u3001\u50a8\u80fd\u548c\u7535\u7f51\u7535\u529b\u7684\u96c6\u6210\u3002", "result": "DRL\u4f18\u5316\u7cfb\u7edf\u5728\u80fd\u6e90\u6210\u672c\u65b9\u9762\u964d\u4f4e\u4e8638%\uff0c\u80fd\u6e90\u6548\u7387\u63d0\u9ad8\u4e8682%\uff0c\u78b3\u6392\u653e\u51cf\u5c11\u4e8645%\u3002", "conclusion": "DRL\u5728\u4f18\u5316\u80fd\u6e90\u7ba1\u7406\u7b56\u7565\u548c\u5e94\u5bf9\u53ef\u6301\u7eed\u6027\u6311\u6218\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2507.21770", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21770", "abs": "https://arxiv.org/abs/2507.21770", "authors": ["Ali Fallahi", "Azam Bastanfard", "Amineh Amini", "Hadi Saboohi"], "title": "Proposing a Semantic Movie Recommendation System Enhanced by ChatGPT's NLP Results", "comment": "May 2023, 6 pages, 5 figures", "summary": "The importance of recommender systems on the web has grown, especially in the\nmovie industry, with a vast selection of options to watch. To assist users in\ntraversing available items and finding relevant results, recommender systems\nanalyze operational data and investigate users' tastes and habits. Providing\nhighly individualized suggestions can boost user engagement and satisfaction,\nwhich is one of the fundamental goals of the movie industry, significantly in\nonline platforms. According to recent studies and research, using\nknowledge-based techniques and considering the semantic ideas of the textual\ndata is a suitable way to get more appropriate results. This study provides a\nnew method for building a knowledge graph based on semantic information. It\nuses the ChatGPT, as a large language model, to assess the brief descriptions\nof movies and extract their tone of voice. Results indicated that using the\nproposed method may significantly enhance accuracy rather than employing the\nexplicit genres supplied by the publishers.", "AI": {"tldr": "This study provides a new method for building a knowledge graph based on semantic information to enhance accuracy of movie recommender system. It uses the ChatGPT to assess the brief descriptions of movies and extract their tone of voice", "motivation": "Providing highly individualized suggestions can boost user engagement and satisfaction, which is one of the fundamental goals of the movie industry, significantly in online platforms. According to recent studies and research, using knowledge-based techniques and considering the semantic ideas of the textual data is a suitable way to get more appropriate results", "method": "a new method for building a knowledge graph based on semantic information. It uses the ChatGPT, as a large language model, to assess the brief descriptions of movies and extract their tone of voice", "result": "using the proposed method may significantly enhance accuracy", "conclusion": "using the proposed method may significantly enhance accuracy rather than employing the explicit genres supplied by the publishers"}}
{"id": "2507.21095", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.21095", "abs": "https://arxiv.org/abs/2507.21095", "authors": ["Mohammad AL-Smadi"], "title": "QU-NLP at CheckThat! 2025: Multilingual Subjectivity in News Articles Detection using Feature-Augmented Transformer Models with Sequential Cross-Lingual Fine-Tuning", "comment": null, "summary": "This paper presents our approach to the CheckThat! 2025 Task 1 on\nsubjectivity detection, where systems are challenged to distinguish whether a\nsentence from a news article expresses the subjective view of the author or\npresents an objective view on the covered topic. We propose a feature-augmented\ntransformer architecture that combines contextual embeddings from pre-trained\nlanguage models with statistical and linguistic features. Our system leveraged\npre-trained transformers with additional lexical features: for Arabic we used\nAraELECTRA augmented with part-of-speech (POS) tags and TF-IDF features, while\nfor the other languages we fine-tuned a cross-lingual DeBERTa~V3 model combined\nwith TF-IDF features through a gating mechanism. We evaluated our system in\nmonolingual, multilingual, and zero-shot settings across multiple languages\nincluding English, Arabic, German, Italian, and several unseen languages. The\nresults demonstrate the effectiveness of our approach, achieving competitive\nperformance across different languages with notable success in the monolingual\nsetting for English (rank 1st with macro-F1=0.8052), German (rank 3rd with\nmacro-F1=0.8013), Arabic (rank 4th with macro-F1=0.5771), and Romanian (rank\n1st with macro-F1=0.8126) in the zero-shot setting. We also conducted an\nablation analysis that demonstrated the importance of combining TF-IDF features\nwith the gating mechanism and the cross-lingual transfer for subjectivity\ndetection. Furthermore, our analysis reveals the model's sensitivity to both\nthe order of cross-lingual fine-tuning and the linguistic proximity of the\ntraining languages.", "AI": {"tldr": "This paper introduces a feature-augmented transformer architecture for subjectivity detection in news articles, achieving competitive results across multiple languages in monolingual, multilingual, and zero-shot settings.", "motivation": "This paper presents our approach to the CheckThat! 2025 Task 1 on subjectivity detection, where systems are challenged to distinguish whether a sentence from a news article expresses the subjective view of the author or presents an objective view on the covered topic.", "method": "We propose a feature-augmented transformer architecture that combines contextual embeddings from pre-trained language models with statistical and linguistic features. Our system leveraged pre-trained transformers with additional lexical features: for Arabic we used AraELECTRA augmented with part-of-speech (POS) tags and TF-IDF features, while for the other languages we fine-tuned a cross-lingual DeBERTa~V3 model combined with TF-IDF features through a gating mechanism.", "result": "achieving competitive performance across different languages with notable success in the monolingual setting for English (rank 1st with macro-F1=0.8052), German (rank 3rd with macro-F1=0.8013), Arabic (rank 4th with macro-F1=0.5771), and Romanian (rank 1st with macro-F1=0.8126) in the zero-shot setting", "conclusion": "The results demonstrate the effectiveness of our approach, achieving competitive performance across different languages with notable success in the monolingual setting for English, German, Arabic, and Romanian in the zero-shot setting.The analysis reveals the model's sensitivity to both the order of cross-lingual fine-tuning and the linguistic proximity of the training languages."}}
{"id": "2507.21261", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.21261", "abs": "https://arxiv.org/abs/2507.21261", "authors": ["Jack Hilliard", "Adrian Hilton", "Jean-Yves Guillemaut"], "title": "HDR Environment Map Estimation with Latent Diffusion Models", "comment": null, "summary": "We advance the field of HDR environment map estimation from a single-view\nimage by establishing a novel approach leveraging the Latent Diffusion Model\n(LDM) to produce high-quality environment maps that can plausibly light\nmirror-reflective surfaces. A common issue when using the ERP representation,\nthe format used by the vast majority of approaches, is distortions at the poles\nand a seam at the sides of the environment map. We remove the border seam\nartefact by proposing an ERP convolutional padding in the latent autoencoder.\nAdditionally, we investigate whether adapting the diffusion network\narchitecture to the ERP format can improve the quality and accuracy of the\nestimated environment map by proposing a panoramically-adapted Diffusion\nTransformer architecture. Our proposed PanoDiT network reduces ERP distortions\nand artefacts, but at the cost of image quality and plausibility. We evaluate\nwith standard benchmarks to demonstrate that our models estimate high-quality\nenvironment maps that perform competitively with state-of-the-art approaches in\nboth image quality and lighting accuracy.", "AI": {"tldr": "\u5229\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5355\u89c6\u56fe\u56fe\u50cf\u4f30\u8ba1 HDR \u73af\u5883\u56fe\u7684\u65b0\u65b9\u6cd5\u3002", "motivation": "\u4ece\u5355\u89c6\u56fe\u56fe\u50cf\u4f30\u8ba1 HDR \u73af\u5883\u56fe\u3002", "method": "\u5229\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b (LDM) \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u4ee5\u751f\u6210\u80fd\u591f\u5408\u7406\u7167\u4eae\u955c\u9762\u53cd\u5c04\u8868\u9762\u7684\u9ad8\u8d28\u91cf\u73af\u5883\u56fe\u3002", "result": "\u6240\u63d0\u51fa\u7684 PanoDiT \u7f51\u7edc\u51cf\u5c11\u4e86 ERP \u626d\u66f2\u548c\u4f2a\u5f71\uff0c\u4f46\u4ee5\u56fe\u50cf\u8d28\u91cf\u548c\u5408\u7406\u6027\u4e3a\u4ee3\u4ef7\u3002\u63d0\u51fa\u7684 ERP \u5377\u79ef\u586b\u5145\u6d88\u9664\u4e86\u8fb9\u754c\u63a5\u7f1d\u4f2a\u5f71\u3002", "conclusion": "\u8be5\u6a21\u578b\u4f30\u8ba1\u7684\u9ad8\u8d28\u91cf\u73af\u5883\u56fe\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u5149\u7167\u7cbe\u5ea6\u65b9\u9762\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u3002"}}
{"id": "2507.21137", "categories": ["cs.AI", "I.2.8"], "pdf": "https://arxiv.org/pdf/2507.21137", "abs": "https://arxiv.org/abs/2507.21137", "authors": ["Arman Eisenkolb-Vaithyanathan"], "title": "Project Patti: Why can You Solve Diabolical Puzzles on one Sudoku Website but not Easy Puzzles on another Sudoku Website?", "comment": "24 pages, 8 Figures", "summary": "In this paper we try to answer the question \"What constitutes Sudoku\ndifficulty rating across different Sudoku websites?\" Using two distinct methods\nthat can both solve every Sudoku puzzle, I propose two new metrics to\ncharacterize Sudoku difficulty. The first method is based on converting a\nSudoku puzzle into its corresponding Satisfiability (SAT) problem. The first\nproposed metric is derived from SAT Clause Length Distribution which captures\nthe structural complexity of a Sudoku puzzle including the number of given\ndigits and the cells they are in. The second method simulates human Sudoku\nsolvers by intertwining four popular Sudoku strategies within a backtracking\nalgorithm called Nishio. The second metric is computed by counting the number\nof times Sudoku strategies are applied within the backtracking iterations of a\nrandomized Nishio. Using these two metrics, I analyze more than a thousand\nSudoku puzzles across five popular websites to characterize every difficulty\nlevel in each website. I evaluate the relationship between the proposed metrics\nand website-labeled difficulty levels using Spearman's rank correlation\ncoefficient, finding strong correlations for 4 out of 5 websites. I construct a\nuniversal rating system using a simple, unsupervised classifier based on the\ntwo proposed metrics. This rating system is capable of classifying both\nindividual puzzles and entire difficulty levels from the different Sudoku\nwebsites into three categories - Universal Easy, Universal Medium, and\nUniversal Hard - thereby enabling consistent difficulty mapping across Sudoku\nwebsites. The experimental results show that for 4 out of 5 Sudoku websites,\nthe universal classification aligns well with website-labeled difficulty\nlevels. Finally, I present an algorithm that can be used by early Sudoku\npractitioners to solve Sudoku puzzles.", "AI": {"tldr": "The paper proposes two new metrics to characterize Sudoku difficulty and constructs a universal rating system, enabling consistent difficulty mapping across Sudoku websites.", "motivation": "The paper aims to answer the question of what constitutes Sudoku difficulty rating across different Sudoku websites.", "method": "Two distinct methods are used: converting a Sudoku puzzle into a SAT problem and simulating human Sudoku solvers using a backtracking algorithm called Nishio.", "result": "Two new metrics are proposed to characterize Sudoku difficulty. Strong correlations are found between the proposed metrics and website-labeled difficulty levels for 4 out of 5 websites.", "conclusion": "A universal Sudoku rating system is constructed using unsupervised classification based on two proposed metrics, aligning well with website-labeled difficulty levels for 4 out of 5 websites. An algorithm for early Sudoku practitioners is presented."}}
