<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 1]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.IR](#cs.IR) [Total: 4]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [LLMs Struggle to Perform Counterfactual Reasoning with Parametric Knowledge](https://arxiv.org/abs/2506.15732)
*Khurram Yamin,Gaurav Ghosal,Bryan Wilder*

Main category: cs.AI

TL;DR: LLMs struggle with counterfactual reasoning and integrating new information with existing knowledge, and finetuning doesn't always help.


<details>
  <summary>Details</summary>
Motivation: LLMs often encounter situations where they must integrate parametric knowledge with new or unfamiliar information.

Method: synthetic and real experiments in multi-hop reasoning problems

Result: LLMs generally struggle with counterfactual reasoning, often resorting to exclusively using their parametric knowledge. simple post-hoc finetuning can struggle to instill counterfactual reasoning ability -- often leading to degradation in stored parametric knowledge.

Conclusion: current LLM's abilities to re-purpose parametric knowledge in novel settings have limitations.

Abstract: Large Language Models have been shown to contain extensive world knowledge in
their parameters, enabling impressive performance on many knowledge intensive
tasks. However, when deployed in novel settings, LLMs often encounter
situations where they must integrate parametric knowledge with new or
unfamiliar information. In this work, we explore whether LLMs can combine
knowledge in-context with their parametric knowledge through the lens of
counterfactual reasoning. Through synthetic and real experiments in multi-hop
reasoning problems, we show that LLMs generally struggle with counterfactual
reasoning, often resorting to exclusively using their parametric knowledge.
Moreover, we show that simple post-hoc finetuning can struggle to instill
counterfactual reasoning ability -- often leading to degradation in stored
parametric knowledge. Ultimately, our work reveals important limitations of
current LLM's abilities to re-purpose parametric knowledge in novel settings.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [2] [Adaptive Anomaly Detection in the Presence of Concept Drift: Extended Report](https://arxiv.org/abs/2506.15831)
*Jongjun Park,Fei Chiang,Mostafa Milani*

Main category: cs.DB

TL;DR: Data changes to reflect evolving user behaviour, preferences, and changes in
the environment. Differentiating between concept drift and anomalies is critical
for accurate analysis


<details>
  <summary>Details</summary>
Motivation: The
presence of concept drift poses challenges for anomaly detection in time
series. Differentiating between concept drift and anomalies is critical
for accurate analysis as studies have shown that the compounding effects of
error propagation in downstream data analysis tasks lead to lower detection
accuracy and increased overhead due to unnecessary model updates.Unfortunately, existing work has largely explored anomaly detection and concept
drift detection in isolation.

Method: It introduces a
new clustering method, Adjacent Hierarchical Clustering (AHC), which groups
similar subsequences while respecting their temporal locality.

Result: differentiating abnormal changes from varying normal behaviours is difficult
due to differing frequencies of occurrence, varying time intervals when normal
patterns occur.

Conclusion: We develop AnDri, a system for Anomaly detection
in the presence of Drift, which adjusts the normal patterns temporally, and
distinguish abnormal subsequences and new concepts.

Abstract: Data changes to reflect evolving user behaviour, preferences, and changes in
the environment. Such changes may occur due to expected shifts in the data
distribution, i.e., concept drift, or unexpected anomalous changes. The
presence of concept drift poses challenges for anomaly detection in time
series. While anomalies are caused by undesirable changes in the data,
differentiating abnormal changes from varying normal behaviours is difficult
due to differing frequencies of occurrence, varying time intervals when normal
patterns occur. Differentiating between concept drift and anomalies is critical
for accurate analysis as studies have shown that the compounding effects of
error propagation in downstream data analysis tasks lead to lower detection
accuracy and increased overhead due to unnecessary model updates.
Unfortunately, existing work has largely explored anomaly detection and concept
drift detection in isolation. We develop AnDri, a system for Anomaly detection
in the presence of Drift, which adjusts the normal patterns temporally, and
distinguish abnormal subsequences and new concepts. Moreover, it introduces a
new clustering method, Adjacent Hierarchical Clustering (AHC), which groups
similar subsequences while respecting their temporal locality.

</details>


### [3] [Delta: A Learned Mixed Cost-based Query Optimization Framework](https://arxiv.org/abs/2506.15848)
*Jiazhen Peng,Zheng Qu,Xiaoye Miao,Rong Zhu*

Main category: cs.DB

TL;DR: Delta, a mixed cost-based query optimization framework, identifies higher-quality plans and achieves significant speedups compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing optimizers exhibit two flawed paradigms: (1) cost-based optimizers use dynamic programming with cost models but face search space explosion and heuristic pruning constraints; (2) value-based ones train value networks to enable efficient beam search, but incur higher training costs and lower accuracy. They also lack mechanisms to detect queries where they may perform poorly.

Method: Delta, a mixed cost-based query optimization framework that consists of a compatible query detector and a two-stage planner. Delta first employs a Mahalanobis distancebased detector to preemptively filter out incompatible queries where the planner might perform poorly. For compatible queries, Delta activates its two-stage mixed cost-based planner. Stage I serves as a coarse-grained filter to generate high-quality candidate plans based on the value network via beam search, relaxing precision requirements and narrowing the search space. Stage II employs a fine-grained ranker to determine the best plan from the candidate plans based on a learned cost model. Moreover, to reduce training costs, we reuse and augment the training data from stage I to train the model in stage II.

Result: achieving an average 2.34x speedup over PostgreSQL and outperforming the state-of-the-art learned methods by 2.21x.

Conclusion: Delta identifies higher-quality plans, achieving an average 2.34x speedup over PostgreSQL and outperforming the state-of-the-art learned methods by 2.21x.

Abstract: Query optimizer is a crucial module for database management systems. Existing
optimizers exhibit two flawed paradigms: (1) cost-based optimizers use dynamic
programming with cost models but face search space explosion and heuristic
pruning constraints; (2) value-based ones train value networks to enable
efficient beam search, but incur higher training costs and lower accuracy. They
also lack mechanisms to detect queries where they may perform poorly. To
determine more efficient plans, we propose Delta, a mixed cost-based query
optimization framework that consists of a compatible query detector and a
two-stage planner. Delta first employs a Mahalanobis distancebased detector to
preemptively filter out incompatible queries where the planner might perform
poorly. For compatible queries, Delta activates its two-stage mixed cost-based
planner. Stage I serves as a coarse-grained filter to generate high-quality
candidate plans based on the value network via beam search, relaxing precision
requirements and narrowing the search space. Stage II employs a fine-grained
ranker to determine the best plan from the candidate plans based on a learned
cost model. Moreover, to reduce training costs, we reuse and augment the
training data from stage I to train the model in stage II. Experimental results
on three workloads demonstrate that Delta identifies higher-quality plans,
achieving an average 2.34x speedup over PostgreSQL and outperforming the
state-of-the-art learned methods by 2.21x.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [4] [Architecture is All You Need: Improving LLM Recommenders by Dropping the Text](https://arxiv.org/abs/2506.15833)
*Kevin Foley,Shaghayegh Agah,Kavya Priyanka Kakinada*

Main category: cs.IR

TL;DR: We propose a recommender model that uses the architecture of large language models (LLMs) while reducing layer count and dimensions and replacing the text-based subword tokenization of a typical LLM with discrete tokens that uniquely represent individual content items. We find that this simplified approach substantially outperforms both traditional sequential recommender models and PLM-based recommender models at a tiny fraction of the size and computational complexity of PLM-based models.


<details>
  <summary>Details</summary>
Motivation: PLM-based recommenders show promise in settings where data is limited, they are hard to implement in practice due to their large size and computational cost. Additionally, fine-tuning PLMs to improve performance on collaborative signals may degrade the model's capacity for world knowledge and generalizability

Method: a recommender model that uses the architecture of large language models (LLMs) while reducing layer count and dimensions and replacing the text-based subword tokenization of a typical LLM with discrete tokens that uniquely represent individual content items

Result: this simplified approach substantially outperforms both traditional sequential recommender models and PLM-based recommender models at a tiny fraction of the size and computational complexity of PLM-based models

Conclusion: the principal benefit of LLMs in recommender systems is their architecture, rather than the world knowledge acquired during extensive pre-training

Abstract: In recent years, there has been an explosion of interest in the applications
of large pre-trained language models (PLMs) to recommender systems, with many
studies showing strong performance of PLMs on common benchmark datasets.
PLM-based recommender models benefit from flexible and customizable prompting,
an unlimited vocabulary of recommendable items, and general ``world knowledge''
acquired through pre-training on massive text corpora. While PLM-based
recommenders show promise in settings where data is limited, they are hard to
implement in practice due to their large size and computational cost.
Additionally, fine-tuning PLMs to improve performance on collaborative signals
may degrade the model's capacity for world knowledge and generalizability. We
propose a recommender model that uses the architecture of large language models
(LLMs) while reducing layer count and dimensions and replacing the text-based
subword tokenization of a typical LLM with discrete tokens that uniquely
represent individual content items. We find that this simplified approach
substantially outperforms both traditional sequential recommender models and
PLM-based recommender models at a tiny fraction of the size and computational
complexity of PLM-based models. Our results suggest that the principal benefit
of LLMs in recommender systems is their architecture, rather than the world
knowledge acquired during extensive pre-training.

</details>


### [5] [MoR: Better Handling Diverse Queries with a Mixture of Sparse, Dense, and Human Retrievers](https://arxiv.org/abs/2506.15862)
*Jushaan Singh Kalra,Xinran Zhao,To Eun Kim,Fengyu Cai,Fernando Diaz,Tongshuang Wu*

Main category: cs.IR

TL;DR: The paper introduces a mixture of retrievers that dynamically combines heterogeneous retrievers for improved RAG performance, outperforming single retrievers and larger models.


<details>
  <summary>Details</summary>
Motivation: Retrieval-augmented Generation (RAG) effectiveness depends on the choice of retrievers, but typically a single retriever is fixed, failing to generalize across diverse information needs. The paper explores dynamically selecting and integrating multiple retrievers for each query.

Method: Introduced mixture of retrievers: a zero-shot, weighted combination of heterogeneous retrievers.

Result: The mixture outperforms every individual retriever and even larger 7B models by +10.8% and +3.9% on average, respectively. Achieves a 58.9% relative performance improvement over simulated humans alone when incorporating specialized non-oracle human information sources.

Conclusion: Mixture of retrievers is effective and efficient, outperforming individual retrievers and larger models. It also facilitates collaboration with human information sources.

Abstract: Retrieval-augmented Generation (RAG) is powerful, but its effectiveness
hinges on which retrievers we use and how. Different retrievers offer distinct,
often complementary signals: BM25 captures lexical matches; dense retrievers,
semantic similarity. Yet in practice, we typically fix a single retriever based
on heuristics, which fails to generalize across diverse information needs. Can
we dynamically select and integrate multiple retrievers for each individual
query, without the need for manual selection? In our work, we validate this
intuition with quantitative analysis and introduce mixture of retrievers: a
zero-shot, weighted combination of heterogeneous retrievers. Extensive
experiments show that such mixtures are effective and efficient: Despite
totaling just 0.8B parameters, this mixture outperforms every individual
retriever and even larger 7B models by +10.8% and +3.9% on average,
respectively. Further analysis also shows that this mixture framework can help
incorporate specialized non-oracle human information sources as retrievers to
achieve good collaboration, with a 58.9% relative performance improvement over
simulated humans alone.

</details>


### [6] [SEP-GCN: Leveraging Similar Edge Pairs with Temporal and Spatial Contexts for Location-Based Recommender Systems](https://arxiv.org/abs/2506.16003)
*Tan Loc Nguyen,Tin T. Tran*

Main category: cs.IR

TL;DR: SEP-GCN是一种利用上下文相似性链接增强用户-项目图的新型图推荐框架，在推荐准确性和鲁棒性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的推荐方法通常依赖于交互矩阵或基于图的检索，但最近的方法试图利用时间和位置等上下文信号。然而，大多数现有模型侧重于节点级表示或孤立的边缘属性，未能充分利用交互之间的关系结构。

Method: SEP-GCN，一种新颖的基于图的推荐框架，可以从成对的上下文相似的交互边中学习，每个交互边代表一个用户-项目签入事件。通过识别在相似的时间窗口或地理位置范围内发生的边缘对，SEP-GCN使用上下文相似性链接来增强用户-项目图。然后通过边缘感知卷积机制处理丰富的图，该机制将上下文相似性集成到消息传递过程中。

Result: SEP-GCN可以更准确，更稳健地对用户偏好进行建模，尤其是在稀疏或动态环境中。基准数据集上的实验表明

Conclusion: SEP-GCN在预测准确性和鲁棒性方面始终优于强大的基线。

Abstract: Recommender systems play a crucial role in enabling personalized content
delivery amidst the challenges of information overload and human mobility.
Although conventional methods often rely on interaction matrices or graph-based
retrieval, recent approaches have sought to exploit contextual signals such as
time and location. However, most existing models focus on node-level
representation or isolated edge attributes, underutilizing the relational
structure between interactions. We propose SEP-GCN, a novel graph-based
recommendation framework that learns from pairs of contextually similar
interaction edges, each representing a user-item check-in event. By identifying
edge pairs that occur within similar temporal windows or geographic proximity,
SEP-GCN augments the user-item graph with contextual similarity links. These
links bridge distant but semantically related interactions, enabling improved
long-range information propagation. The enriched graph is processed via an
edge-aware convolutional mechanism that integrates contextual similarity into
the message-passing process. This allows SEP-GCN to model user preferences more
accurately and robustly, especially in sparse or dynamic environments.
Experiments on benchmark data sets show that SEP-GCN consistently outperforms
strong baselines in both predictive accuracy and robustness.

</details>


### [7] [GFlowGR: Fine-tuning Generative Recommendation Frameworks with Generative Flow Networks](https://arxiv.org/abs/2506.16114)
*Yejing Wang,Shengyu Zhou,Jinyu Lu,Qidong Liu,Xinhang Li,Wenlin Zhang,Feng Li,Pengjie Wang,Jian Xu,Bo Zheng,Xiangyu Zhao*

Main category: cs.IR

TL;DR: 提出了一种基于GFlowNets的微调框架(GFlowGR)，以减轻生成式推荐中的暴露偏差问题。


<details>
  <summary>Details</summary>
Motivation: 当前的方法主要依赖于监督微调(SFT)的下一个token预测损失或特定于推荐的直接偏好优化(DPO)策略。这两种方法都忽略了对可能的积极未观察到的样本的探索，这通常被称为暴露偏差问题。

Method: 将GR视为多步生成任务，并构建一个基于GFlowNets的微调框架(GFlowGR)。该框架集成了来自传统推荐系统的协作知识，以创建一个自适应轨迹采样器和一个综合奖励模型。

Result: 在两个真实世界数据集和两个不同的GR主干上的大量经验结果突出了GFlowGR的有效性和鲁棒性。

Conclusion: GFlowGR是一种有前途的方法，可以缓解暴露偏差问题。在两个真实世界数据集和两个不同的GR主干上的大量经验结果突出了GFlowGR的有效性和鲁棒性。

Abstract: Generative recommendations (GR), which usually include item tokenizers and
generative Large Language Models (LLMs), have demonstrated remarkable success
across a wide range of scenarios. The majority of existing research efforts
primarily concentrate on developing powerful item tokenizers or advancing LLM
decoding strategies to attain superior performance. However, the critical
fine-tuning step in GR frameworks, which is essential for adapting LLMs to
recommendation data, remains largely unexplored. Current approaches
predominantly rely on either the next-token prediction loss of supervised
fine-tuning (SFT) or recommendationspecific direct preference optimization
(DPO) strategies. Both methods ignore the exploration of possible positive
unobserved samples, which is commonly referred to as the exposure bias problem.
To mitigate this problem, this paper treats the GR as a multi-step generation
task and constructs a GFlowNets-based fine-tuning framework (GFlowGR). The
proposed framework integrates collaborative knowledge from traditional
recommender systems to create an adaptive trajectory sampler and a
comprehensive reward model. Leveraging the diverse generation property of
GFlowNets, along with sampling and heuristic weighting techniques, GFlowGR
emerges as a promising approach to mitigate the exposure bias problem.
Extensive empirical results on two real-world datasets and with two different
GR backbones highlight the effectiveness and robustness of GFlowGR.

</details>
