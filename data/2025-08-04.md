<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 46]
- [cs.CV](#cs.CV) [Total: 63]
- [cs.AI](#cs.AI) [Total: 25]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.LG](#cs.LG) [Total: 62]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Tabular Data Understanding with LLMs: A Survey of Recent Advances and Challenges](https://arxiv.org/abs/2508.00217)
*Xiaofeng Wu,Alan Ritter,Wei Xu*

Main category: cs.CL

TL;DR: This paper introduces a taxonomy of tabular input representations and identifies critical gaps in table understanding tasks for LLMs and MLLMs.


<details>
  <summary>Details</summary>
Motivation: Tables' complex and flexible structure has led to the development of specialized methods and tasks, instead of universal approaches, making navigation of table understanding tasks challenging.

Method: introduction of a taxonomy of tabular input representations

Result: Identified three critical gaps: (1) the predominance of retrieval-focused tasks; (2) significant challenges faced by models when processing complex table structures; (3) the limited generalization of models across different tabular representations and formats.

Conclusion: This paper introduces key concepts through a taxonomy of tabular input representations and an introduction of table understanding tasks. It highlights critical gaps in the field, including the predominance of retrieval-focused tasks, challenges in processing complex table structures, and limited generalization of models.

Abstract: Tables have gained significant attention in large language models (LLMs) and
multimodal large language models (MLLMs) due to their complex and flexible
structure. Unlike linear text inputs, tables are two-dimensional, encompassing
formats that range from well-structured database tables to complex,
multi-layered spreadsheets, each with different purposes. This diversity in
format and purpose has led to the development of specialized methods and tasks,
instead of universal approaches, making navigation of table understanding tasks
challenging. To address these challenges, this paper introduces key concepts
through a taxonomy of tabular input representations and an introduction of
table understanding tasks. We highlight several critical gaps in the field that
indicate the need for further research: (1) the predominance of
retrieval-focused tasks that require minimal reasoning beyond mathematical and
logical operations; (2) significant challenges faced by models when processing
complex table structures, large-scale tables, length context, or multi-table
scenarios; and (3) the limited generalization of models across different
tabular representations and formats.

</details>


### [2] [PhysicsEval: Inference-Time Techniques to Improve the Reasoning Proficiency of Large Language Models on Physics Problems](https://arxiv.org/abs/2508.00079)
*Oshayer Siddique,J. M Areeb Uzair Alam,Md Jobayer Rahman Rafy,Syed Rifat Raiyan,Hasan Mahmud,Md Kamrul Hasan*

Main category: cs.CL

TL;DR: This paper evaluates LLMs on physics problems, introduces a new benchmark, and finds that a multi-agent framework improves performance.


<details>
  <summary>Details</summary>
Motivation: Solving physics problems is a crucial domain of natural language reasoning, driving technology and deepening our understanding of the cosmos.

Method: The paper evaluates LLM performance on physics problems using inference-time techniques and agentic frameworks, including verification of solutions by smaller LLM agents.

Result: Significant improvements are observed when applying a multi-agent framework to problems that models initially perform poorly on. A new evaluation benchmark for physics problems, PHYSICS{\small EVAL}, is introduced.

Conclusion: The paper introduces a new physics problem benchmark, PHYSICS{\small EVAL}, and shows that a multi-agent framework improves LLM performance on these problems.

Abstract: The discipline of physics stands as a cornerstone of human intellect, driving
the evolution of technology and deepening our understanding of the fundamental
principles of the cosmos. Contemporary literature includes some works centered
on the task of solving physics problems - a crucial domain of natural language
reasoning. In this paper, we evaluate the performance of frontier LLMs in
solving physics problems, both mathematical and descriptive. We also employ a
plethora of inference-time techniques and agentic frameworks to improve the
performance of the models. This includes the verification of proposed solutions
in a cumulative fashion by other, smaller LLM agents, and we perform a
comparative analysis of the performance that the techniques entail. There are
significant improvements when the multi-agent framework is applied to problems
that the models initially perform poorly on. Furthermore, we introduce a new
evaluation benchmark for physics problems, ${\rm P{\small HYSICS}E{\small
VAL}}$, consisting of 19,609 problems sourced from various physics textbooks
and their corresponding correct solutions scraped from physics forums and
educational websites. Our code and data are publicly available at
https://github.com/areebuzair/PhysicsEval.

</details>


### [3] [Do LLMs produce texts with "human-like" lexical diversity?](https://arxiv.org/abs/2508.00086)
*Kelly Kendro,Jeffrey Maloney,Scott Jarvis*

Main category: cs.CL

TL;DR: LLM-generated texts differ significantly from human-written texts in lexical diversity, with newer LLMs being less human-like than older models.


<details>
  <summary>Details</summary>
Motivation: The degree to which LLMs produce writing that is truly human-like remains unclear despite the extensive empirical attention that this question has received. The present study addresses this question from the perspective of lexical diversity.

Method: Patterns of lexical diversity were investigated in LLM-generated texts from four ChatGPT models in comparison with texts written by L1 and L2 English participants across four education levels. Six dimensions of lexical diversity were measured in each text: volume, abundance, variety-repetition, evenness, disparity, and dispersion. One-way MANOVAs, one-way ANOVAS, and Support Vector Machines were used for analysis.

Result: The LLM-generated texts differed significantly from human-written texts for each variable, with ChatGPT-o4 mini and -4.5 differing the most. ChatGPT-4.5 demonstrated higher levels of lexical diversity despite producing fewer tokens. The human writers' lexical diversity did not differ across subgroups.

Conclusion: LLMs do not produce human-like texts in relation to lexical diversity, and newer LLMs produce less human-like texts than older models.

Abstract: The degree to which LLMs produce writing that is truly human-like remains
unclear despite the extensive empirical attention that this question has
received. The present study addresses this question from the perspective of
lexical diversity. Specifically, the study investigates patterns of lexical
diversity in LLM-generated texts from four ChatGPT models (-3.5, -4, -o4 mini,
and -4.5) in comparison with texts written by L1 and L2 English participants (n
= 240) across four education levels. Six dimensions of lexical diversity were
measured in each text: volume, abundance, variety-repetition, evenness,
disparity, and dispersion. Results from one-way MANOVAs, one-way ANOVAS, and
Support Vector Machines revealed that the LLM-generated texts differed
significantly from human-written texts for each variable, with ChatGPT-o4 mini
and -4.5 differing the most. Within these two groups, ChatGPT-4.5 demonstrated
higher levels of lexical diversity despite producing fewer tokens. The human
writers' lexical diversity did not differ across subgroups (i.e., education,
language status). Altogether, the results indicate that LLMs do not produce
human-like texts in relation to lexical diversity, and the newer LLMs produce
less human-like texts than older models. We discuss the implications of these
results for language pedagogy and related applications.

</details>


### [4] [Semiotic Complexity and Its Epistemological Implications for Modeling Culture](https://arxiv.org/abs/2508.00095)
*Zachary K. Stine,James E. Deitrick*

Main category: cs.CL

TL;DR: 本文讨论了计算人文学科中方法论理论化的必要性，并提出了半符号复杂性的概念，以更好地解释建模实践中的认识论问题。


<details>
  <summary>Details</summary>
Motivation: 计算人文学科需要更多的方法理论，以实现认识论和解释的清晰性，从而促进该领域的成熟。

Method: 我们将这种建模工作定义为从事从文化、语言领域到计算、数学领域的翻译工作，然后再返回。

Result: 主要的建模实践——尤其是在评估方面——通过在认识论上方便地赋予表面清晰度，从而将符号复杂的文本数据视为符号简单的文本数据，从而犯了翻译错误。

Conclusion: 研究人员应该更好地解释他们工作中的这些认识论问题。

Abstract: Greater theorizing of methods in the computational humanities is needed for
epistemological and interpretive clarity, and therefore the maturation of the
field. In this paper, we frame such modeling work as engaging in translation
work from a cultural, linguistic domain into a computational, mathematical
domain, and back again. Translators benefit from articulating the theory of
their translation process, and so do computational humanists in their work --
to ensure internal consistency, avoid subtle yet consequential translation
errors, and facilitate interpretive transparency. Our contribution in this
paper is to lay out a particularly consequential dimension of the lack of
theorizing and the sorts of translation errors that emerge in our modeling
practices as a result. Along these lines we introduce the idea of semiotic
complexity as the degree to which the meaning of some text may vary across
interpretive lenses, and make the case that dominant modeling practices --
especially around evaluation -- commit a translation error by treating
semiotically complex data as semiotically simple when it seems
epistemologically convenient by conferring superficial clarity. We then lay out
several recommendations for researchers to better account for these
epistemological issues in their own work.

</details>


### [5] [FACTORY: A Challenging Human-Verified Prompt Set for Long-Form Factuality](https://arxiv.org/abs/2508.00109)
*Mingda Chen,Yang Li,Xilun Chen,Adina Williams,Gargi Ghosh,Scott Yih*

Main category: cs.CL

TL;DR: 推出了一个大规模的人工验证提示集FACTORY，以解决现有基准缺乏人工验证的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的基准通常缺乏人工验证，导致潜在的质量问题。

Method: 使用模型循环方法开发并由人工完善

Result: 对6个最先进的语言模型进行了人工评估，结果表明FACTORY是一个具有挑战性的基准。

Conclusion: FACTORY是一个具有挑战性的基准，SOTA模型在其上的表现不佳，大约40%的声明不符合事实，而其他数据集只有10%。

Abstract: Long-form factuality evaluation assesses the ability of models to generate
accurate, comprehensive responses to short prompts. Existing benchmarks often
lack human verification, leading to potential quality issues. To address this
limitation, we introduce FACTORY, a large-scale, human-verified prompt set.
Developed using a model-in-the-loop approach and refined by humans, FACTORY
includes challenging prompts that are fact-seeking, answerable, and
unambiguous. We conduct human evaluations on 6 state-of-the-art language models
using FACTORY and existing datasets. Our results show that FACTORY is a
challenging benchmark: approximately 40% of the claims made in the responses of
SOTA models are not factual, compared to only 10% for other datasets. Our
analysis identifies the strengths of FACTORY over prior benchmarks, emphasizing
its reliability and the necessity for models to reason across long-tailed
facts.

</details>


### [6] [Is neural semantic parsing good at ellipsis resolution, or isn't it?](https://arxiv.org/abs/2508.00121)
*Xiao Zhang,Johan bos*

Main category: cs.CL

TL;DR: neural semantic parsers perform well on standard test set, but failed in the instances with ellipsis


<details>
  <summary>Details</summary>
Motivation: how do such parsers perform on strongly context-sensitive phenomena, where large pieces of semantic information need to be duplicated to form a meaningful semantic representation?

Method: constructed a corpus of 120 cases of ellipsis with their fully resolved meaning representation and used this as a challenge set for a large battery of neural semantic parsers

Result: parsers performed very well on the standard test set, they failed in the instances with ellipsis

Conclusion: neural semantic parsers fail in the instances with ellipsis

Abstract: Neural semantic parsers have shown good overall performance for a variety of
linguistic phenomena, reaching semantic matching scores of more than 90%. But
how do such parsers perform on strongly context-sensitive phenomena, where
large pieces of semantic information need to be duplicated to form a meaningful
semantic representation? A case in point is English verb phrase ellipsis, a
construct where entire verb phrases can be abbreviated by a single auxiliary
verb. Are the otherwise known as powerful semantic parsers able to deal with
ellipsis or aren't they? We constructed a corpus of 120 cases of ellipsis with
their fully resolved meaning representation and used this as a challenge set
for a large battery of neural semantic parsers. Although these parsers
performed very well on the standard test set, they failed in the instances with
ellipsis. Data augmentation

</details>


### [7] [Segment First, Retrieve Better: Realistic Legal Search via Rhetorical Role-Based Queries](https://arxiv.org/abs/2508.00679)
*Shubham Kumar Nigam,Tanmay Dubey,Noel Shallum,Arnab Bhattacharya*

Main category: cs.CL

TL;DR: TraceRetriever mirrors real-world legal search by operating with limited case information, extracting only rhetorically significant segments instead of requiring complete documents to enhance legal research.


<details>
  <summary>Details</summary>
Motivation: growing complexity and volume of legal documents challenge traditional retrieval methods in legal precedent retrieval.

Method: integrates BM25, Vector Database, and Cross-Encoder models, combining initial results through Reciprocal Rank Fusion before final re-ranking. Rhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier trained on Indian judgments.

Result: Evaluated on IL-PCR and COLIEE 2025 datasets

Conclusion: TraceRetriever addresses growing document volume challenges while aligning with practical search constraints, providing a reliable and scalable foundation for precedent retrieval enhancing legal research when only partial case knowledge is available.

Abstract: Legal precedent retrieval is a cornerstone of the common law system, governed
by the principle of stare decisis, which demands consistency in judicial
decisions. However, the growing complexity and volume of legal documents
challenge traditional retrieval methods. TraceRetriever mirrors real-world
legal search by operating with limited case information, extracting only
rhetorically significant segments instead of requiring complete documents. Our
pipeline integrates BM25, Vector Database, and Cross-Encoder models, combining
initial results through Reciprocal Rank Fusion before final re-ranking.
Rhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier
trained on Indian judgments. Evaluated on IL-PCR and COLIEE 2025 datasets,
TraceRetriever addresses growing document volume challenges while aligning with
practical search constraints, reliable and scalable foundation for precedent
retrieval enhancing legal research when only partial case knowledge is
available.

</details>


### [8] [Comparison of Large Language Models for Deployment Requirements](https://arxiv.org/abs/2508.00185)
*Alper Yaman,Jannik Schwab,Christof Nitsche,Abhirup Sinha,Marco Huber*

Main category: cs.CL

TL;DR: Comparative list of LLMs to facilitate model selection regarding licensing and hardware requirements.


<details>
  <summary>Details</summary>
Motivation: To navigate the rapidly evolving LLM landscape and facilitate LLM selection, the selection of the optimal LLM for researchers and companies regarding licensing and hardware requirements is complicated.

Method: comparative list of foundational and domain-specific models

Result: focusing on features, such as release year, licensing, and hardware requirements

Conclusion: We present a comparative list of foundational and domain-specific models, focusing on features, such as release year, licensing, and hardware requirements. This list is published on GitLab and will be continuously updated.

Abstract: Large Language Models (LLMs), such as Generative Pre-trained Transformers
(GPTs) are revolutionizing the generation of human-like text, producing
contextually relevant and syntactically correct content. Despite challenges
like biases and hallucinations, these Artificial Intelligence (AI) models excel
in tasks, such as content creation, translation, and code generation.
Fine-tuning and novel architectures, such as Mixture of Experts (MoE), address
these issues. Over the past two years, numerous open-source foundational and
fine-tuned models have been introduced, complicating the selection of the
optimal LLM for researchers and companies regarding licensing and hardware
requirements. To navigate the rapidly evolving LLM landscape and facilitate LLM
selection, we present a comparative list of foundational and domain-specific
models, focusing on features, such as release year, licensing, and hardware
requirements. This list is published on GitLab and will be continuously
updated.

</details>


### [9] [NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System](https://arxiv.org/abs/2508.00709)
*Shubham Kumar Nigam,Balaramamahanthi Deepak Patnaik,Shivam Mishra,Ajay Varghese Thomas,Noel Shallum,Kripabandhu Ghosh,Arnab Bhattacharya*

Main category: cs.CL

TL;DR: NyayaRAG, a RAG framework, improves legal judgment prediction by augmenting factual inputs with legal knowledge.


<details>
  <summary>Details</summary>
Motivation: previous approaches in the Indian context have relied on internal case content such as facts, issues, and reasoning, they often overlook a core element of common law systems, which is reliance on statutory provisions and judicial precedents

Method: a Retrieval-Augmented Generation (RAG) framework that simulates realistic courtroom scenarios by providing models with factual case descriptions, relevant legal statutes, and semantically retrieved prior cases

Result: augmenting factual inputs with structured legal knowledge significantly improves both predictive accuracy and explanation quality

Conclusion: augmenting factual inputs with structured legal knowledge significantly improves both predictive accuracy and explanation quality

Abstract: Legal Judgment Prediction (LJP) has emerged as a key area in AI for law,
aiming to automate judicial outcome forecasting and enhance interpretability in
legal reasoning. While previous approaches in the Indian context have relied on
internal case content such as facts, issues, and reasoning, they often overlook
a core element of common law systems, which is reliance on statutory provisions
and judicial precedents. In this work, we propose NyayaRAG, a
Retrieval-Augmented Generation (RAG) framework that simulates realistic
courtroom scenarios by providing models with factual case descriptions,
relevant legal statutes, and semantically retrieved prior cases. NyayaRAG
evaluates the effectiveness of these combined inputs in predicting court
decisions and generating legal explanations using a domain-specific pipeline
tailored to the Indian legal system. We assess performance across various input
configurations using both standard lexical and semantic metrics as well as
LLM-based evaluators such as G-Eval. Our results show that augmenting factual
inputs with structured legal knowledge significantly improves both predictive
accuracy and explanation quality.

</details>


### [10] [Semantic Compression for Word and Sentence Embeddings using Discrete Wavelet Transform](https://arxiv.org/abs/2508.00220)
*Rana Aref Salama,Abdou Youssef,Mona Diab*

Main category: cs.CL

TL;DR: 本文利用离散小波变换（DWT）分析和压缩embeddings，实验表明DWT可以有效降低embeddings的维度，并在下游任务中保持或提高性能。


<details>
  <summary>Details</summary>
Motivation: 小波变换在信号和图像处理等领域已被广泛使用，可以捕捉语言和语义属性。

Method: 将离散小波变换（DWT）应用于词和句子embeddings。

Result: DWT可以将embeddings的维度降低50-93%，在语义相似性任务中性能几乎没有变化，并且在大多数下游任务中实现了更高的准确率。

Conclusion: DWT可以显著降低embeddings的维度，同时在下游任务中保持甚至提高性能，为改进NLP应用开辟了道路。

Abstract: Wavelet transforms, a powerful mathematical tool, have been widely used in
different domains, including Signal and Image processing, to unravel intricate
patterns, enhance data representation, and extract meaningful features from
data. Tangible results from their application suggest that Wavelet transforms
can be applied to NLP capturing a variety of linguistic and semantic
properties. In this paper, we empirically leverage the application of Discrete
Wavelet Transforms (DWT) to word and sentence embeddings. We aim to showcase
the capabilities of DWT in analyzing embedding representations at different
levels of resolution and compressing them while maintaining their overall
quality. We assess the effectiveness of DWT embeddings on semantic similarity
tasks to show how DWT can be used to consolidate important semantic information
in an embedding vector. We show the efficacy of the proposed paradigm using
different embedding models, including large language models, on downstream
tasks. Our results show that DWT can reduce the dimensionality of embeddings by
50-93% with almost no change in performance for semantic similarity tasks,
while achieving superior accuracy in most downstream tasks. Our findings pave
the way for applying DWT to improve NLP applications.

</details>


### [11] [Model Misalignment and Language Change: Traces of AI-Associated Language in Unscripted Spoken English](https://arxiv.org/abs/2508.00238)
*Bryce Anderson,Riley Galpin,Tom S. Juzek*

Main category: cs.CL

TL;DR: 研究表明，人类的语言使用习惯可能正在向大型语言模型（LLM）靠拢，但原因尚不清楚。


<details>
  <summary>Details</summary>
Motivation: 探讨书面语言中词语用法的显着变化是否反映了人类语言系统本身的更广泛变化，以及这些变化与大型语言模型（LLM）的影响是否相关。

Method: 构建了一个包含2210万个单词的数据集，这些单词来自会话科学和技术播客中的非脚本口语，并分析了ChatGPT发布前后词汇趋势。

Result: 结果表明，在2022年后，与LLM相关的词语的使用量出现了适度但显着增加，而基线同义词没有表现出明显的定向变化。

Conclusion: 发现人类用词选择和大型语言模型（LLM）相关的模式之间存在趋同现象，但这种现象是否代表自然语言的变化或由人工智能驱动的新变化仍是一个悬而未决的问题。

Abstract: In recent years, written language, particularly in science and education, has
undergone remarkable shifts in word usage. These changes are widely attributed
to the growing influence of Large Language Models (LLMs), which frequently rely
on a distinct lexical style. Divergences between model output and target
audience norms can be viewed as a form of misalignment. While these shifts are
often linked to using Artificial Intelligence (AI) directly as a tool to
generate text, it remains unclear whether the changes reflect broader changes
in the human language system itself. To explore this question, we constructed a
dataset of 22.1 million words from unscripted spoken language drawn from
conversational science and technology podcasts. We analyzed lexical trends
before and after ChatGPT's release in 2022, focusing on commonly LLM-associated
words. Our results show a moderate yet significant increase in the usage of
these words post-2022, suggesting a convergence between human word choices and
LLM-associated patterns. In contrast, baseline synonym words exhibit no
significant directional shift. Given the short time frame and the number of
words affected, this may indicate the onset of a remarkable shift in language
use. Whether this represents natural language change or a novel shift driven by
AI exposure remains an open question. Similarly, although the shifts may stem
from broader adoption patterns, it may also be that upstream training
misalignments ultimately contribute to changes in human language use. These
findings parallel ethical concerns that misaligned models may shape social and
moral beliefs.

</details>


### [12] [Integrating clinical reasoning into large language model-based diagnosis through etiology-aware attention steering](https://arxiv.org/abs/2508.00285)
*Peixian Li,Yu Tian,Ruiqi Tu,Chengkai Wu,Jingjing Ren,Jingsong Li*

Main category: cs.CL

TL;DR: 本研究旨在通过病因感知注意力引导框架，提高大型语言模型在复杂临床场景中的诊断准确性和临床推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在医学文本理解和生成方面表现出显著的能力。然而，它们在复杂临床场景中的诊断可靠性仍然有限。本研究旨在提高LLM的诊断准确性和临床推理能力。

Method: 我们提出了一个病因感知注意力引导框架，以将结构化临床推理整合到基于LLM的诊断中。具体来说，我们首先基于针对三种代表性急性腹部急症（急性阑尾炎、急性胰腺炎和急性胆囊炎）的权威临床指南构建临床推理支架（CRS）。接下来，我们开发了病因感知头部识别算法，以查明对模型病因推理至关重要的注意力头部。为了确保可靠的临床推理对齐，我们引入了推理引导的参数高效微调，该微调将病因推理线索嵌入到输入表示中，并通过推理引导的损失函数将选定的病因感知头部引导到关键信息。

Result: 在一致诊断队列上，我们的框架将平均诊断准确率提高了15.65%，并且比基线方法将平均推理焦点评分提高了31.6%。在不一致诊断队列上的外部验证进一步证实了其在提高诊断准确性方面的有效性。通过推理注意力频率进行的进一步评估表明，我们的模型在面对现实世界中的复杂场景时表现出增强的可靠性。

Conclusion: 该研究提出了一种实用的有效方法来增强基于LLM的诊断中的临床推理。通过将模型注意力与结构化的CRS对齐，所提出的框架为在复杂的临床环境中构建更可解释和可靠的AI诊断系统提供了一个有希望的范例。

Abstract: Objective: Large Language Models (LLMs) demonstrate significant capabilities
in medical text understanding and generation. However, their diagnostic
reliability in complex clinical scenarios remains limited. This study aims to
enhance LLMs' diagnostic accuracy and clinical reasoning ability. Method: We
propose an Etiology-Aware Attention Steering Framework to integrate structured
clinical reasoning into LLM-based diagnosis. Specifically, we first construct
Clinical Reasoning Scaffolding (CRS) based on authoritative clinical guidelines
for three representative acute abdominal emergencies: acute appendicitis, acute
pancreatitis, and acute cholecystitis. Next, we develop the Etiology-Aware Head
Identification algorithm to pinpoint attention heads crucial for the model's
etiology reasoning. To ensure reliable clinical reasoning alignment, we
introduce the Reasoning-Guided Parameter-Efficient Fine-tuning that embeds
etiological reasoning cues into input representations and steers the selected
Etiology-Aware Heads toward critical information through a Reasoning-Guided
Loss function. Result: On the Consistent Diagnosis Cohort, our framework
improves average diagnostic accuracy by 15.65% and boosts the average Reasoning
Focus Score by 31.6% over baselines. External validation on the Discrepant
Diagnosis Cohort further confirms its effectiveness in enhancing diagnostic
accuracy. Further assessments via Reasoning Attention Frequency indicate that
our models exhibit enhanced reliability when faced with real-world complex
scenarios. Conclusion: This study presents a practical and effective approach
to enhance clinical reasoning in LLM-based diagnosis. By aligning model
attention with structured CRS, the proposed framework offers a promising
paradigm for building more interpretable and reliable AI diagnostic systems in
complex clinical settings.

</details>


### [13] [Systematic Evaluation of Optimization Techniques for Long-Context Language Models](https://arxiv.org/abs/2508.00305)
*Ammar Ahmed,Sheng Di,Franck Cappello,Zirui Liu,Jingoo Han,Ali Anwar*

Main category: cs.CL

TL;DR: This paper benchmarks optimization methods like pruning, quantization, and token dropping for LLMs in long-context scenarios, revealing that naive combinations can hurt larger models and F1 scores can be misleading.


<details>
  <summary>Details</summary>
Motivation: LLMs face resource demands and limited context windows. Although techniques like pruning, quantization, and token dropping can mitigate these issues, their efficacy in long-context scenarios and system evaluation remains underexplored.

Method: Systematically benchmarks pruning, quantization, and token dropping, characterizing memory usage, latency, and throughput, and studies how these methods impact the quality of text generation. Analyzes individual optimization methods for two LLM architectures supporting long context and then systematically evaluates combinations of these techniques. Studies the scalability of individual optimization methods on a larger variant with 70 billion-parameter model. Integrates system-level profiling with task-specific insights.

Result: Characterizes memory usage, latency, and throughput of optimization methods and studies how these methods impact the quality of text generation. Reveals that naive combination inference optimization algorithms can adversely affect larger models due to compounded approximation errors, as compared to their smaller counterparts. Experiments show that relying solely on F1 obscures these effects by hiding precision-recall trade-offs in question answering tasks.

Conclusion: Naive combination inference optimization algorithms can adversely affect larger models due to compounded approximation errors, and relying solely on F1 obscures these effects by hiding precision-recall trade-offs in question answering tasks.

Abstract: Large language models (LLMs) excel across diverse natural language processing
tasks but face resource demands and limited context windows. Although
techniques like pruning, quantization, and token dropping can mitigate these
issues, their efficacy in long-context scenarios and system evaluation remains
underexplored. This paper systematically benchmarks these optimizations,
characterizing memory usage, latency, and throughput, and studies how these
methods impact the quality of text generation. We first analyze individual
optimization methods for two LLM architectures supporting long context and then
systematically evaluate combinations of these techniques to assess how this
deeper analysis impacts performance metrics. We subsequently study the
scalability of individual optimization methods on a larger variant with 70
billion-parameter model. Our novel insights reveal that naive combination
inference optimization algorithms can adversely affect larger models due to
compounded approximation errors, as compared to their smaller counterparts.
Experiments show that relying solely on F1 obscures these effects by hiding
precision-recall trade-offs in question answering tasks. By integrating
system-level profiling with task-specific insights, this study helps LLM
practitioners and researchers explore and balance efficiency, accuracy, and
scalability across tasks and hardware configurations.

</details>


### [14] [Improving Multimodal Contrastive Learning of Sentence Embeddings with Object-Phrase Alignment](https://arxiv.org/abs/2508.00332)
*Kaiyan Zhao,Zhongtao Miao,Yoshimasa Tsuruoka*

Main category: cs.CL

TL;DR: MCSEO通过结合细粒度的物体-短语对齐来增强多模态句子嵌入，从而优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 多模态句子嵌入模型通常在训练期间利用图像-标题对以及文本数据。然而，这些配对通常包含噪声，包括图像或标题侧的冗余或不相关信息。

Method: 利用现有的分割和物体检测模型来提取精确的物体-短语对，然后将其用于优化对比学习目标，该目标是为物体-短语对应关系量身定制的。

Result: 在不同的骨干模型上的语义文本相似性（STS）任务的实验结果表明，MCSEO始终优于强大的基线。

Conclusion: MCSEO在多模态表征学习中，精确的物体-短语对齐具有重要意义，并在不同的骨干模型上的语义文本相似性（STS）任务中，始终优于强大的基线。

Abstract: Multimodal sentence embedding models typically leverage image-caption pairs
in addition to textual data during training. However, such pairs often contain
noise, including redundant or irrelevant information on either the image or
caption side. To mitigate this issue, we propose MCSEO, a method that enhances
multimodal sentence embeddings by incorporating fine-grained object-phrase
alignment alongside traditional image-caption alignment. Specifically, MCSEO
utilizes existing segmentation and object detection models to extract accurate
object-phrase pairs, which are then used to optimize a contrastive learning
objective tailored to object-phrase correspondence. Experimental results on
semantic textual similarity (STS) tasks across different backbone models
demonstrate that MCSEO consistently outperforms strong baselines, highlighting
the significance of precise object-phrase alignment in multimodal
representation learning.

</details>


### [15] [PilotRL: Training Language Model Agents via Global Planning-Guided Progressive Reinforcement Learning](https://arxiv.org/abs/2508.00344)
*Keer Lu,Chong Chen,Bin Cui,Huang Leng,Wentao Zhang*

Main category: cs.CL

TL;DR: The paper introduces AdaPlan and PilotRL to improve LLM agents' long-term planning and generalization, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing LLM agents face challenges in complex tasks requiring long-term planning and struggle with generalization due to reliance on supervised fine-tuning.

Method: Introduces AdaPlan, an adaptive global plan-based agent paradigm, and PilotRL, a global planning-guided training framework for LLM agents driven by progressive reinforcement learning.

Result: PilotRL demonstrates state-of-the-art performance, with LLaMA3.1-8B-Instruct + PilotRL surpassing closed-source GPT-4o by 3.60% and GPT-4o-mini by 55.78%.

Conclusion: PilotRL achieves state-of-the-art performance, outperforming GPT-4o and GPT-4o-mini in experiments.

Abstract: Large Language Models (LLMs) have shown remarkable advancements in tackling
agent-oriented tasks. Despite their potential, existing work faces challenges
when deploying LLMs in agent-based environments. The widely adopted agent
paradigm ReAct centers on integrating single-step reasoning with immediate
action execution, which limits its effectiveness in complex tasks requiring
long-term strategic planning. Furthermore, the coordination between the planner
and executor during problem-solving is also a critical factor to consider in
agent design. Additionally, current approaches predominantly rely on supervised
fine-tuning, which often leads models to memorize established task completion
trajectories, thereby restricting their generalization ability when confronted
with novel problem contexts. To address these challenges, we introduce an
adaptive global plan-based agent paradigm AdaPlan, aiming to synergize
high-level explicit guidance with execution to support effective long-horizon
decision-making. Based on the proposed paradigm, we further put forward
PilotRL, a global planning-guided training framework for LLM agents driven by
progressive reinforcement learning. We first develop the model's ability to
follow explicit guidance from global plans when addressing agent tasks.
Subsequently, based on this foundation, we focus on optimizing the quality of
generated plans. Finally, we conduct joint optimization of the model's planning
and execution coordination. Experiments indicate that PilotRL could achieve
state-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing
closed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78%
comparing to GPT-4o-mini at a comparable parameter scale.

</details>


### [16] [Lucy: edgerunning agentic web search on mobile with machine generated task vectors](https://arxiv.org/abs/2508.00360)
*Alan Dao,Dinh Bach Vu,Alex Nguyen,Norapat Buppodom*

Main category: cs.CL

TL;DR: This paper introduces a new paradigm viewing the model's internal reasoning as a dynamic task vector machine. A 1.7B-parameter SLM, Lucy, achieves comparable performance to much larger models on SimpleQA by using this dynamic reasoning mechanism.


<details>
  <summary>Details</summary>
Motivation: Small language models (SLMs) are inherently limited in knowledge-intensive tasks due to their constrained capacity. While test-time computation offers a path to enhanced performance, most approaches treat reasoning as a fixed or heuristic process.

Method: Developed a method to optimize this dynamic task vector machine through RLVR and successfully trained an agentic web-search model.

Result: Lucy achieves 78.3% accuracy on the SimpleQA benchmark, performing on par with much larger models such as DeepSeek-V3.

Conclusion: A 1.7B-parameter SLM, Lucy, leverages a dynamic reasoning mechanism with MCP integration to achieve 78.3% accuracy on the SimpleQA benchmark, performing on par with much larger models. This demonstrates that small models can rival large ones when equipped with structured, self-constructed task reasoning.

Abstract: Small language models (SLMs) are inherently limited in knowledge-intensive
tasks due to their constrained capacity. While test-time computation offers a
path to enhanced performance, most approaches treat reasoning as a fixed or
heuristic process. In this work, we propose a new paradigm: viewing the model's
internal reasoning, delimited by <think> and </think> tags, as a dynamic task
vector machine. Rather than treating the content inside these tags as a mere
trace of thought, we interpret the generation process itself as a mechanism
through which the model \textbf{constructs and refines its own task vectors} on
the fly. We developed a method to optimize this dynamic task vector machine
through RLVR and successfully trained an agentic web-search model. We present
Lucy, a 1.7B-parameter SLM that leverages this dynamic reasoning mechanism with
MCP integration to achieve 78.3% accuracy on the SimpleQA benchmark, performing
on par with much larger models such as DeepSeek-V3. This demonstrates that
small models can rival large ones when equipped with structured,
self-constructed task reasoning.

</details>


### [17] [EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level Efficiency for Edge Devices](https://arxiv.org/abs/2508.00370)
*Jiyu Chen,Poh Seng Lim,Shuang Peng,Daxiong Luo,JungHau Foo,Yap Deep,Timothy Lee Jun Jie,Kelvin Teh Kae Wen,Fan Yang,Danyu Feng,Hao-Yun Chen,Peng-Wen Chen,Fangyuan Li,Xiaoxin Chen,Wong Wai Mun*

Main category: cs.CL

TL;DR: EdgeInfinite-Instruct 提高了长序列任务的性能，并通过分段监督微调和优化量化来优化边缘 NPU 上的部署。


<details>
  <summary>Details</summary>
Motivation: 由于自注意力的二次时间复杂度和不断增长的键值 (KV) 缓存需求，在资源受限的边缘设备上部署基于 Transformer 的大型语言模型 (LLM) 以执行长序列任务仍然具有挑战性。 虽然现有的 KV 缓存优化提高了内存效率，但它们通常无法减少首个令牌的时间 (TTFT)，并且可能会通过令牌修剪降低性能。 替代序列建模架构解决了其中的一些限制，但通常需要完全重新训练并且缺乏基础设施支持。 EdgeInfinite 通过仅微调一小部分参数来提供有效的解决方案，在保持质量的同时降低计算和内存成本，包括改进的 TTFT。 但是，它的指令跟随能力有限，并且缺乏特定于移动设备的优化。

Method: 提出了 EdgeInfinite-Instruct，它引入了一种专门为诸如总结和问答之类的长序列任务量身定制的分段监督微调 (S-SFT) 策略。 通过采用细粒度的训练后量化 (PTQ) 来减少计算需求同时保持准确性，并通过实施固定形状的计算图来平衡内存使用和设备效率，通过特定于场景的输入令牌和缓存大小的定制。

Result: 在长上下文基准和实际移动任务上的实验表明，该方法在保持 NPU 加速的边缘设备上的效率的同时，提高了特定领域的性能。

Conclusion: 该方法在保持 NPU 加速的边缘设备上的效率的同时，提高了特定领域的性能。

Abstract: Deploying Transformer-based large language models (LLMs) on
resource-constrained edge devices for long-sequence tasks remains challenging
due to the quadratic time complexity of self-attention and growing Key-Value
(KV) cache demands. While existing KV cache optimizations improve memory
efficiency, they often fail to reduce time to first token (TTFT) and may
degrade performance through token pruning. Alternative sequence modeling
architectures address some of these limitations, but typically require full
retraining and lack infrastructure support. EdgeInfinite offers an efficient
solution by fine-tuning only a small subset of parameters, maintaining quality
while reducing both computational and memory costs, including improved TTFT.
However, its instruction-following ability is limited, and it lacks
mobile-specific optimizations. To address these issues, we propose
EdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning
(S-SFT) strategy tailored to long-sequence tasks such as summarization and
question answering. We further optimized EdgeInfinite-Instruct for efficient
deployment on edge NPUs by employing fine-grained post-training quantization
(PTQ) to reduce computational demands while maintaining accuracy, and by
implementing a fixed-shape computation graph that balances memory usage and
on-device efficiency through scenario-specific customization of input token and
cache sizes. Experiments on long-context benchmarks and real-world mobile tasks
show that our approach improves domain-specific performance while maintaining
efficiency on NPU-accelerated edge devices.

</details>


### [18] [Multi-Layer Attention is the Amplifier of Demonstration Effectiveness](https://arxiv.org/abs/2508.00385)
*Dingzirui Wang,Xuangliang Zhang,Keyan Xu,Qingfu Zhu,Wanxiang Che,Yang Deng*

Main category: cs.CL

TL;DR: This paper investigates why some demonstrations in in-context learning (ICL) are ineffective, finding that demonstrations are ineffective if the information has been learned or is irrelevant. They propose GradS, a gradient-based demonstration selection method, and it achieves a 6.8% improvement.


<details>
  <summary>Details</summary>
Motivation: existing work predominantly assumes the effectiveness of the demonstrations provided within ICL, while many research indicates that not all demonstrations are effective, failing to yielding any performance improvement during ICL. Therefore, in this paper, we investigate the reasons behind demonstration ineffectiveness.

Method: a novel method called GradS, which leverages gradient flow for demonstration selection. We use the magnitude of the gradient flow of the demonstration with respect to a given user query as the criterion, thereby ensuring the effectiveness of the chosen ones.

Result: a demonstration becomes ineffective if its information has either been learned by the model or is irrelevant to the user query. Furthermore, we demonstrate that in multi-layer models, the disparity in effectiveness among demonstrations is amplified with layer increasing, causing the model to focus more on effective ones.

Conclusion: The experimental results confirm that the disparity in effectiveness among demonstrations is magnified as the model layer increases, substantiating our derivations. Moreover, GradS achieves a relative improvement of $6.8\%$ on average over the strongest baselines, demonstrating its effectiveness.

Abstract: Numerous studies have investigated the underlying mechanisms of in-context
learning (ICL) effectiveness to inspire the design of related methods. However,
existing work predominantly assumes the effectiveness of the demonstrations
provided within ICL, while many research indicates that not all demonstrations
are effective, failing to yielding any performance improvement during ICL.
Therefore, in this paper, we investigate the reasons behind demonstration
ineffectiveness. Our analysis is based on gradient flow and linear
self-attention models. By setting the gradient flow to zero, we deduce that a
demonstration becomes ineffective if its information has either been learned by
the model or is irrelevant to the user query. Furthermore, we demonstrate that
in multi-layer models, the disparity in effectiveness among demonstrations is
amplified with layer increasing, causing the model to focus more on effective
ones. Considering that current demonstration selection methods primarily focus
on the relevance to the user query while overlooking the information that the
model has already assimilated, we propose a novel method called GradS, which
leverages gradient flow for demonstration selection. We use the magnitude of
the gradient flow of the demonstration with respect to a given user query as
the criterion, thereby ensuring the effectiveness of the chosen ones. We
validate our derivation and GradS on four prominent LLMs across five mainstream
datasets. The experimental results confirm that the disparity in effectiveness
among demonstrations is magnified as the model layer increases, substantiating
our derivations. Moreover, GradS achieves a relative improvement of $6.8\%$ on
average over the strongest baselines, demonstrating its effectiveness.

</details>


### [19] [SA-GCS: Semantic-Aware Gaussian Curriculum Scheduling for UAV Vision-Language Navigation](https://arxiv.org/abs/2508.00390)
*Hengxing Cai,Jinhan Dong,Yijie Rao,Jingcheng Deng,Jingjun Tan,Qien Chen,Haidong Wang,Zhen Wang,Shiyu Huang,Agachai Sumalee,Renxin Zhong*

Main category: cs.CL

TL;DR: This paper introduces Semantic-Aware Gaussian Curriculum Scheduling (SA-GCS) to improve the training efficiency, convergence, and overall performance of UAV Vision-Language Navigation by integrating Curriculum Learning (CL) into Reinforcement Learning (RL).


<details>
  <summary>Details</summary>
Motivation: existing RL methods often suffer from inefficient use of training data, slow convergence, and insufficient consideration of the difficulty variation among training samples, which limits further performance improvement

Method: Semantic-Aware Gaussian Curriculum Scheduling (SA-GCS)

Result: This design significantly improves training efficiency, accelerates convergence, and enhances overall model performance

Conclusion: SA-GCS consistently outperforms strong baselines across all metrics, achieves faster and more stable convergence, and generalizes well across models of different scales, highlighting its robustness and scalability.

Abstract: Unmanned Aerial Vehicle (UAV) Vision-Language Navigation (VLN) aims to enable
agents to accurately localize targets and plan flight paths in complex
environments based on natural language instructions, with broad applications in
intelligent inspection, disaster rescue, and urban monitoring. Recent progress
in Vision-Language Models (VLMs) has provided strong semantic understanding for
this task, while reinforcement learning (RL) has emerged as a promising
post-training strategy to further improve generalization. However, existing RL
methods often suffer from inefficient use of training data, slow convergence,
and insufficient consideration of the difficulty variation among training
samples, which limits further performance improvement. To address these
challenges, we propose \textbf{Semantic-Aware Gaussian Curriculum Scheduling
(SA-GCS)}, a novel training framework that systematically integrates Curriculum
Learning (CL) into RL. SA-GCS employs a Semantic-Aware Difficulty Estimator
(SA-DE) to quantify the complexity of training samples and a Gaussian
Curriculum Scheduler (GCS) to dynamically adjust the sampling distribution,
enabling a smooth progression from easy to challenging tasks. This design
significantly improves training efficiency, accelerates convergence, and
enhances overall model performance. Extensive experiments on the CityNav
benchmark demonstrate that SA-GCS consistently outperforms strong baselines
across all metrics, achieves faster and more stable convergence, and
generalizes well across models of different scales, highlighting its robustness
and scalability. The implementation of our approach is publicly available.

</details>


### [20] [Combining Discrete Wavelet and Cosine Transforms for Efficient Sentence Embedding](https://arxiv.org/abs/2508.00420)
*Rana Salama,Abdou Youssef,Mona Diab*

Main category: cs.CL

TL;DR: 该论文提出了一种基于小波变换的自然语言处理 (NLP) 方法，该方法可以有效地压缩单词和句子嵌入，并在下游任务中取得良好的效果。


<details>
  <summary>Details</summary>
Motivation: 小波已成为许多领域的前沿技术。它们在图像和信号处理中的具体应用结果表明，小波可以有效地应用于捕获各种语言属性的自然语言处理 (NLP) 任务。

Method: 将离散小波变换 (DWT) 应用于单词和句子嵌入，并结合 DWT 与离散余弦变换 (DCT) 以提出一种非参数化模型。

Result: 该论文首先从内在和外在两个方面评估小波如何有效地用于整合单词向量中的重要信息，同时降低其维度。该论文进一步结合 DWT 与离散余弦变换 (DCT)，提出了一个非参数化模型，该模型基于局部变化的单词特征，用固定大小的向量压缩包含大量信息的句子。

Conclusion: 该论文表明，所提出的范例在下游应用模型上具有功效，在某些任务中，其结果与原始嵌入相当甚至更优。

Abstract: Wavelets have emerged as a cutting edge technology in a number of fields.
Concrete results of their application in Image and Signal processing suggest
that wavelets can be effectively applied to Natural Language Processing (NLP)
tasks that capture a variety of linguistic properties. In this paper, we
leverage the power of applying Discrete Wavelet Transforms (DWT) to word and
sentence embeddings. We first evaluate, intrinsically and extrinsically, how
wavelets can effectively be used to consolidate important information in a word
vector while reducing its dimensionality. We further combine DWT with Discrete
Cosine Transform (DCT) to propose a non-parameterized model that compresses a
sentence with a dense amount of information in a fixed size vector based on
locally varying word features. We show the efficacy of the proposed paradigm on
downstream applications models yielding comparable and even superior (in some
tasks) results to original embeddings.

</details>


### [21] [ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network](https://arxiv.org/abs/2508.00429)
*Minghao Guo,Xi Zhu,Jingyuan Huang,Kai Mei,Yongfeng Zhang*

Main category: cs.CL

TL;DR: Proposes ReaGAN, an agent-based GNN framework with retrieval-augmented generation, to address limitations of fixed message passing in GNNs.


<details>
  <summary>Details</summary>
Motivation: Graph Neural Networks (GNNs) have achieved remarkable success in graph-based learning by propagating information among neighbor nodes via predefined aggregation mechanisms. However, such fixed schemes often suffer from two key limitations. First, they cannot handle the imbalance in node informativeness -- some nodes are rich in information, while others remain sparse. Second, predefined message passing primarily leverages local structural similarity while ignoring global semantic relationships across the graph, limiting the model's ability to capture distant but relevant information.

Method: Retrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework that empowers each node with autonomous, node-level decision-making. Each node acts as an agent that independently plans its next action based on its internal memory, enabling node-level planning and adaptive message propagation. Additionally, retrieval-augmented generation (RAG) allows nodes to access semantically relevant content and build global relationships in the graph.

Result: ReaGAN achieves competitive performance under few-shot in-context settings using a frozen LLM backbone without fine-tuning

Conclusion: ReaGAN achieves competitive performance under few-shot in-context settings using a frozen LLM backbone without fine-tuning, showcasing the potential of agentic planning and local-global retrieval in graph learning.

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success in graph-based
learning by propagating information among neighbor nodes via predefined
aggregation mechanisms. However, such fixed schemes often suffer from two key
limitations. First, they cannot handle the imbalance in node informativeness --
some nodes are rich in information, while others remain sparse. Second,
predefined message passing primarily leverages local structural similarity
while ignoring global semantic relationships across the graph, limiting the
model's ability to capture distant but relevant information. We propose
Retrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework
that empowers each node with autonomous, node-level decision-making. Each node
acts as an agent that independently plans its next action based on its internal
memory, enabling node-level planning and adaptive message propagation.
Additionally, retrieval-augmented generation (RAG) allows nodes to access
semantically relevant content and build global relationships in the graph.
ReaGAN achieves competitive performance under few-shot in-context settings
using a frozen LLM backbone without fine-tuning, showcasing the potential of
agentic planning and local-global retrieval in graph learning.

</details>


### [22] [Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges](https://arxiv.org/abs/2508.00454)
*Yuqi Tang,Kehua Feng,Yunfeng Wang,Zhiwen Chen,Chengfei Lv,Gang Yu,Qiang Zhang,Keyan Ding*

Main category: cs.CL

TL;DR: 提出了一种高效的多轮对话评估器，该评估器通过将多个 LLM 判断者的偏好知识聚合到单个模型中来捕获多个 LLM 判断者的集体智慧，从而降低了评估成本，同时优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型 (LLM) 的对话能力仍然是一项具有挑战性的任务。当前的主流方法主要依赖于“LLM 即评委”范例，其中提示 LLM 充当评估者来评估对话质量。然而，这些方法通常会受到各种偏差的影响，从而削弱评估结果的可靠性和一致性。为了减轻这些偏差，最近的方法采用多个 LLM 作为评委，并汇总他们的判断以选择最佳评估。虽然有效，但这种多评委方法在推理过程中会产生巨大的计算开销。

Method: 提出了一种有效的多轮对话评估器，该评估器通过将多个 LLM 判断者的偏好知识聚合到单个模型中来捕获多个 LLM 判断者的集体智慧。

Result: 在七个单项评级和成对比较对话评估基准上的大量实验表明，我们的方法在各种场景中优于现有基线，展示了其效率和稳健性。

Conclusion: 该方法在各种场景中优于现有基线，展示了其效率和稳健性。

Abstract: Evaluating the conversational abilities of large language models (LLMs)
remains a challenging task. Current mainstream approaches primarily rely on the
``LLM-as-a-judge" paradigm, where an LLM is prompted to serve as an evaluator
to assess dialogue quality. However, such methods often suffer from various
biases, which undermine the reliability and consistency of the evaluation
results. To mitigate these biases, recent methods employ multiple LLMs as
judges and aggregate their judgments to select the optimal assessment. Although
effective, this multi-judge approach incurs significant computational overhead
during inference. In this paper, we propose an efficient multi-turn dialogue
evaluator that captures the collective wisdom of multiple LLM judges by
aggregating their preference knowledge into a single model. Our approach
preserves the advantages of diverse multi-judge feedback while drastically
reducing the evaluation cost, enabling fast and flexible dialogue quality
assessment. Extensive experiments on seven single rating and pairwise
comparison dialogue evaluation benchmarks demonstrate that our method
outperforms existing baselines across diverse scenarios, showcasing its
efficiency and robustness.

</details>


### [23] [GETALP@AutoMin 2025: Leveraging RAG to Answer Questions based on Meeting Transcripts](https://arxiv.org/abs/2508.00476)
*Jeongwoo Kang,Markarit Vartampetian,Felix Herron,Yongxin Zhou,Diandra Fabre,Gabriela Gonzalez-Saez*

Main category: cs.CL

TL;DR: GETALP's submission to the Automatic Minuting Shared Task at SIGDial 2025 uses RAG and AMR for question-answering based on meeting transcripts. Incorporating AMR improves response quality, especially for questions about different participants.


<details>
  <summary>Details</summary>
Motivation: This paper documents GETALP's submission to the Third Run of the Automatic Minuting Shared Task at SIGDial 2025. We participated in Task B: question-answering based on meeting transcripts.

Method: retrieval augmented generation (RAG) system and Abstract Meaning Representations (AMR)

Result: incorporating AMR leads to high-quality responses for approximately 35% of the questions

Conclusion: Incorporating AMR leads to high-quality responses for approximately 35% of the questions and provides notable improvements in answering questions that involve distinguishing between different participants.

Abstract: This paper documents GETALP's submission to the Third Run of the Automatic
Minuting Shared Task at SIGDial 2025. We participated in Task B:
question-answering based on meeting transcripts. Our method is based on a
retrieval augmented generation (RAG) system and Abstract Meaning
Representations (AMR). We propose three systems combining these two approaches.
Our results show that incorporating AMR leads to high-quality responses for
approximately 35% of the questions and provides notable improvements in
answering questions that involve distinguishing between different participants
(e.g., who questions).

</details>


### [24] [The Missing Parts: Augmenting Fact Verification with Half-Truth Detection](https://arxiv.org/abs/2508.00489)
*Yixuan Tang,Jincheng Wang,Anthony K. H. Tung*

Main category: cs.CL

TL;DR: 论文介绍了半真假检测任务，并提出了TRACER框架，该框架通过识别省略信息来提高事实核查的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的事实核查系统难以处理半真半假的情况，因为它们没有被设计用来推理未说明的内容。

Method: TRACER框架，通过对齐证据、推断隐含意图和估计隐藏内容的因果影响来识别基于省略的错误信息。

Result: TRACER框架将半真假分类的F1值提高了16个点。

Conclusion: TRACER框架通过识别省略信息来改进半真假分类，并在多个基线上提高了性能。

Abstract: Fact verification systems typically assess whether a claim is supported by
retrieved evidence, assuming that truthfulness depends solely on what is
stated. However, many real-world claims are half-truths, factually correct yet
misleading due to the omission of critical context. Existing models struggle
with such cases, as they are not designed to reason about what is left unsaid.
We introduce the task of half-truth detection, and propose PolitiFact-Hidden, a
new benchmark with 15k political claims annotated with sentence-level evidence
alignment and inferred claim intent. To address this challenge, we present
TRACER, a modular re-assessment framework that identifies omission-based
misinformation by aligning evidence, inferring implied intent, and estimating
the causal impact of hidden content. TRACER can be integrated into existing
fact-checking pipelines and consistently improves performance across multiple
strong baselines. Notably, it boosts Half-True classification F1 by up to 16
points, highlighting the importance of modeling omissions for trustworthy fact
verification.

</details>


### [25] [EFlat-LoRA: Efficiently Seeking Flat Minima for Better Generalization in Fine-Tuning Large Language Models and Beyond](https://arxiv.org/abs/2508.00522)
*Jiaxin Deng,Qingcheng Zhu,Junbiao Pang,Linlin Yang,Zhongqian Fu,Baochang Zhang*

Main category: cs.CL

TL;DR: This paper introduces Flat-LoRA and EFlat-LoRA to improve the generalization ability of LoRA by seeking flat minima. Experiments show EFlat-LoRA achieves comparable efficiency to LoRA with better performance.


<details>
  <summary>Details</summary>
Motivation: Little research explores the correlation between the expressive ability and generalization ability of the low-rank adaptation (LoRA). The connection between sharpness and generalization has not been fully explored for LoRA due to the lack of tools to either empirically seek flat minima or develop theoretical methods.

Method: We propose Flat-LoRA and its efficient version i.e., EFlat-LoRA, to seek flat minima for LoRA. We theoretically demonstrate that perturbations in the full parameter space can be transferred to the low-rank subspace.

Result: On the GLUE dataset with RoBERTa-large, EFlat-LoRA outperforms LoRA and full fine-tuning by 1.0% and 0.5% on average, respectively. On vision-language models e.g., Qwen-VL-Chat shows performance improvements of 1.5% and 1.0% on SQA and VizWiz datasets, respectively.

Conclusion: EFlat-LoRA achieves optimize efficiency comparable to that of LoRA while simultaneously attaining comparable or even better performance. The generalization of LoRA is closely related to sharpness, which is omitted by previous methods.

Abstract: Little research explores the correlation between the expressive ability and
generalization ability of the low-rank adaptation (LoRA). Sharpness-Aware
Minimization (SAM) improves model generalization for both Convolutional Neural
Networks (CNNs) and Transformers by encouraging convergence to locally flat
minima. However, the connection between sharpness and generalization has not
been fully explored for LoRA due to the lack of tools to either empirically
seek flat minima or develop theoretical methods. In this work, we propose
Flat-LoRA and its efficient version i.e., EFlat-LoRA, to seek flat minima for
LoRA. Concretely, we theoretically demonstrate that perturbations in the full
parameter space can be transferred to the low-rank subspace. This approach
eliminates the potential interference introduced by perturbations across
multiple matrices in the low-rank subspace. Our extensive experiments on large
language models and vision-language models demonstrate that EFlat-LoRA achieves
optimize efficiency comparable to that of LoRA while simultaneously attaining
comparable or even better performance. For example, on the GLUE dataset with
RoBERTa-large, EFlat-LoRA outperforms LoRA and full fine-tuning by 1.0% and
0.5% on average, respectively. On vision-language models e.g., Qwen-VL-Chat
shows performance improvements of 1.5% and 1.0% on SQA and VizWiz datasets,
respectively. These empirical results also verify that the generalization of
LoRA is closely related to sharpness, which is omitted by previous methods.

</details>


### [26] [The Prosody of Emojis](https://arxiv.org/abs/2508.00537)
*Giulio Zhou,Tsz Kin Lam,Alexandra Birch,Barry Haddow*

Main category: cs.CL

TL;DR: This study examines how emojis influence prosodic realisation in speech and how listeners interpret prosodic cues to recover emoji meanings.


<details>
  <summary>Details</summary>
Motivation: Emojis act as visual surrogates that add affective and pragmatic nuance in text-based settings where prosodic cues are absent.

Method: Analysing actual human speech data, collected through structured but open-ended production and perception tasks.

Result: Speakers adapt their prosody based on emoji cues, listeners can often identify the intended emoji from prosodic variation alone, and greater semantic differences between emojis correspond to increased prosodic divergence.

Conclusion: Emojis can act as meaningful carriers of prosodic intent, offering insight into their communicative role in digitally mediated contexts.

Abstract: Prosodic features such as pitch, timing, and intonation are central to spoken
communication, conveying emotion, intent, and discourse structure. In
text-based settings, where these cues are absent, emojis act as visual
surrogates that add affective and pragmatic nuance. This study examines how
emojis influence prosodic realisation in speech and how listeners interpret
prosodic cues to recover emoji meanings. Unlike previous work, we directly link
prosody and emoji by analysing actual human speech data, collected through
structured but open-ended production and perception tasks. This provides
empirical evidence of how emoji semantics shape spoken delivery and perception.
Results show that speakers adapt their prosody based on emoji cues, listeners
can often identify the intended emoji from prosodic variation alone, and
greater semantic differences between emojis correspond to increased prosodic
divergence. These findings suggest that emojis can act as meaningful carriers
of prosodic intent, offering insight into their communicative role in digitally
mediated contexts.

</details>


### [27] [PaPaformer: Language Model from Pre-trained Paraller Paths](https://arxiv.org/abs/2508.00544)
*Joonas Tapaninaho,Mourad Oussala*

Main category: cs.CL

TL;DR: PaPaformer, a new decoder-only transformer, reduces training time by training parallel paths individually and combining them. It also allows task-specific customization.


<details>
  <summary>Details</summary>
Motivation: The training of large-language models requires an increasing amount of computation power and time, even for smaller variants like SLMs. This paper explores methods to train decoder-only transformer-based language models in hours instead of days/weeks.

Method: Introducing PaPaformer, a decoder-only transformer architecture variant with lower-dimensional parallel paths.

Result: The method reduces the total number of model parameters and training time with increasing performance. The parallel path structure allows customization for specific tasks.

Conclusion: The paper introduces PaPaformer, a decoder-only transformer architecture variant, and demonstrates that lower-dimensional parallel paths can be trained individually and combined, reducing training time and offering customization options.

Abstract: The training of modern large-language models requires an increasingly amount
of computation power and time. Even smaller variants, such as small-language
models (SLMs), take several days to train in the best-case scenarios, often
requiring multiple GPUs. This paper explores methods to train and evaluate
decoder-only transformer-based language models in hours instead of days/weeks.
We introduces \textit{PaPaformer}, a decoder-only transformer architecture
variant, whose lower-dimensional parallel paths are combined into larger model.
The paper shows that these lower-dimensional paths can be trained individually
with different types of training data and then combined into one larger model.
This method gives the option to reduce the total number of model parameters and
the training time with increasing performance. Moreover, the use of parallel
path structure opens interesting possibilities to customize paths to
accommodate specific task requirements.

</details>


### [28] [SynAdapt: Learning Adaptive Reasoning in Large Language Models via Synthetic Continuous Chain-of-Thought](https://arxiv.org/abs/2508.00574)
*Jianwei Wang,Ziming Wu,Fuming Lai,Shaobing Lian,Ziqian Zeng*

Main category: cs.CL

TL;DR: SynAdapt通过生成合成的CCoT作为对齐目标并集成难度分类器来提高LLM的推理效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有CCoT方法受到间接微调、有限对齐或不一致目标的阻碍。为了克服这些限制，我们提出了SynAdapt，一种创新的高效推理框架。

Method: SynAdapt生成合成的CCoT作为LLM的精确有效对齐目标。SynAdapt集成了难度分类器，利用问题上下文和CCoT来识别难题。然后，自适应地提示LLM重新思考这些难题，以提高性能。

Result: SynAdapt实现了最佳的准确性-效率权衡。

Conclusion: SynAdapt在各种难度级别的基准测试中都表现出有效性，实现了最佳的准确性-效率权衡。

Abstract: While Chain-of-Thought (CoT) reasoning improves model performance, it incurs
significant time costs due to the generation of discrete CoT tokens (DCoT).
Continuous CoT (CCoT) offers a more efficient alternative, but existing CCoT
methods are hampered by indirect fine-tuning, limited alignment, or
inconsistent targets. To overcome these limitations, we propose
\textit{SynAdapt}, an innovative efficient reasoning framework. Specifically,
\textit{SynAdapt} generates the synthetic CCoT to serve as a precise and
effective alignment target for LLMs. This synthetic CCoT explicitly guides the
LLM to learn CCoT and derive accurate answers directly. Furthermore, relying
solely on CCoT is insufficient for solving hard questions. To address this,
\textit{SynAdapt} integrates a difficulty classifier that leverages both
question context and CCoT to identify hard questions. CCoT can effectively help
identify hard questions after some brief reasoning. We then adaptively prompt
the LLM to re-think these hard questions for improved performance. Extensive
experimental results across various benchmarks from different difficulty levels
strongly demonstrate the effectiveness of our method, achieving the best
accuracy-efficiency trade-off.

</details>


### [29] [NusaAksara: A Multimodal and Multilingual Benchmark for Preserving Indonesian Indigenous Scripts](https://arxiv.org/abs/2502.18148)
*Muhammad Farid Adilazuarda,Musa Izzanardi Wijanarko,Lucky Susanto,Khumaisa Nur'aini,Derry Wijaya,Alham Fikri Aji*

Main category: cs.CL

TL;DR: NusaAksara, a new benchmark for Indonesian languages and scripts, reveals that most NLP models struggle with these scripts.


<details>
  <summary>Details</summary>
Motivation: Indonesia is rich in languages and scripts. However, most NLP progress has been made using romanized text.

Method: A novel public benchmark for Indonesian languages that includes their original scripts.

Result: NusaAksara covers 8 scripts across 7 languages, including low-resource languages not commonly seen in NLP benchmarks. Although unsupported by Unicode, the Lampung script is included in this dataset.

Conclusion: Most NLP technologies cannot handle Indonesia's local scripts, with many achieving near-zero performance.

Abstract: Indonesia is rich in languages and scripts. However, most NLP progress has
been made using romanized text. In this paper, we present NusaAksara, a novel
public benchmark for Indonesian languages that includes their original scripts.
Our benchmark covers both text and image modalities and encompasses diverse
tasks such as image segmentation, OCR, transliteration, translation, and
language identification. Our data is constructed by human experts through
rigorous steps. NusaAksara covers 8 scripts across 7 languages, including
low-resource languages not commonly seen in NLP benchmarks. Although
unsupported by Unicode, the Lampung script is included in this dataset. We
benchmark our data across several models, from LLMs and VLMs such as GPT-4o,
Llama 3.2, and Aya 23 to task-specific systems such as PP-OCR and LangID, and
show that most NLP technologies cannot handle Indonesia's local scripts, with
many achieving near-zero performance.

</details>


### [30] [A Context-Aware Dual-Metric Framework for Confidence Estimation in Large Language Models](https://arxiv.org/abs/2508.00600)
*Mingruo Yuan,Shuyi Zhang,Ben Kao*

Main category: cs.CL

TL;DR: CRUX是一个利用上下文信息进行LLM置信度估计的新框架，并在多个数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM置信度估计方法忽略了response和上下文信息之间的相关性，而这在输出质量评估中至关重要，尤其是在提供背景知识的情况下。

Method: 提出了CRUX，通过上下文忠实度和一致性来整合上下文信息，进行置信度估计。

Result: CRUX在三个基准数据集（CoQA, SQuAD, QuAC）和两个特定领域数据集（BioASQ, EduQG）上进行了实验，结果表明CRUX的有效性，实现了比现有基线最高的AUROC。

Conclusion: CRUX在多个数据集上实现了最高的AUROC，证明了其有效性。

Abstract: Accurate confidence estimation is essential for trustworthy large language
models (LLMs) systems, as it empowers the user to determine when to trust
outputs and enables reliable deployment in safety-critical applications.
Current confidence estimation methods for LLMs neglect the relevance between
responses and contextual information, a crucial factor in output quality
evaluation, particularly in scenarios where background knowledge is provided.
To bridge this gap, we propose CRUX (Context-aware entropy Reduction and
Unified consistency eXamination), the first framework that integrates context
faithfulness and consistency for confidence estimation via two novel metrics.
First, contextual entropy reduction represents data uncertainty with the
information gain through contrastive sampling with and without context. Second,
unified consistency examination captures potential model uncertainty through
the global consistency of the generated answers with and without context.
Experiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two
domain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness,
achieving the highest AUROC than existing baselines.

</details>


### [31] [GHTM: A Graph based Hybrid Topic Modeling Approach in Low-Resource Bengali Language](https://arxiv.org/abs/2508.00605)
*Farhana Haque,Md. Abdur Rahman,Sumon Ahmed*

Main category: cs.CL

TL;DR: This paper proposes a novel GCN-based topic model called GHTM for Bengali text. GHTM outperforms existing methods in topic coherence and diversity. A new Bengali dataset called NCTBText is also introduced.


<details>
  <summary>Details</summary>
Motivation: topic modeling remains understudied in Bengali due to its morphological complexity, lack of adequate resources and initiatives

Method: a novel Graph Convolutional Network (GCN) based model called GHTM (Graph-Based Hybrid Topic Model)

Result: This model represents input vectors of documents as nodes in the graph, which GCN uses to produce semantically rich embeddings. The embeddings are then decomposed using Non-negative Matrix Factorization (NMF) to get the topical representations of the underlying themes of the text corpus.

Conclusion: The experimental results demonstrate the effectiveness of the proposed model by outperforming other models in topic coherence and diversity.

Abstract: Topic modeling is a Natural Language Processing (NLP) technique that is used
to identify latent themes and extract topics from text corpora by grouping
similar documents based on their most significant keywords. Although widely
researched in English, topic modeling remains understudied in Bengali due to
its morphological complexity, lack of adequate resources and initiatives. In
this contribution, a novel Graph Convolutional Network (GCN) based model called
GHTM (Graph-Based Hybrid Topic Model) is proposed. This model represents input
vectors of documents as nodes in the graph, which GCN uses to produce
semantically rich embeddings. The embeddings are then decomposed using
Non-negative Matrix Factorization (NMF) to get the topical representations of
the underlying themes of the text corpus. This study compares the proposed
model against a wide range of Bengali topic modeling techniques, from
traditional methods such as LDA, LSA, and NMF to contemporary frameworks such
as BERTopic and Top2Vec on three Bengali datasets. The experimental results
demonstrate the effectiveness of the proposed model by outperforming other
models in topic coherence and diversity. In addition, we introduce a novel
Bengali dataset called "NCTBText" sourced from Bengali textbook materials to
enrich and diversify the predominantly newspaper-centric Bengali corpora.

</details>


### [32] [Prompting Science Report 3: I'll pay you or I'll kill you -- but will you care?](https://arxiv.org/abs/2508.00614)
*Lennart Meincke,Ethan Mollick,Lilach Mollick,Dan Shapiro*

Main category: cs.CL

TL;DR: 威胁或奖励AI模型对性能没有显著影响，简单的提示变体可能不像以前假设的那么有效，特别是对于困难的问题。


<details>
  <summary>Details</summary>
Motivation: 调查两种常见的提示信念：a) 提出给AI模型小费 和 b) 威胁AI模型。给小费是一种常见的提高AI性能的策略，而威胁得到了谷歌创始人Sergey Brin的支持，他观察到“如果你威胁它们，模型往往会做得更好”，我们在这里对这一说法进行了实证测试。

Method: 在GPQA和MMLU-Pro上评估模型性能。

Result: 威胁或奖励模型通常对基准性能没有显著影响。提示变体可以显著影响每个问题的性能。然而，很难预先知道一种特定的提示方法是否会帮助或损害LLM回答任何特定问题的能力。

Conclusion: 威胁或奖励模型通常对基准性能没有显著影响。提示变体可以显著影响每个问题的性能，但很难预先知道哪种提示方法将有助于或损害LLM回答任何特定问题的能力。简单的提示变体可能不像以前假设的那么有效，特别是对于困难的问题。

Abstract: This is the third in a series of short reports that seek to help business,
education, and policy leaders understand the technical details of working with
AI through rigorous testing. In this report, we investigate two commonly held
prompting beliefs: a) offering to tip the AI model and b) threatening the AI
model. Tipping was a commonly shared tactic for improving AI performance and
threats have been endorsed by Google Founder Sergey Brin (All-In, May 2025,
8:20) who observed that 'models tend to do better if you threaten them,' a
claim we subject to empirical testing here. We evaluate model performance on
GPQA (Rein et al. 2024) and MMLU-Pro (Wang et al. 2024).
  We demonstrate two things:
  - Threatening or tipping a model generally has no significant effect on
benchmark performance.
  - Prompt variations can significantly affect performance on a per-question
level. However, it is hard to know in advance whether a particular prompting
approach will help or harm the LLM's ability to answer any particular question.
  Taken together, this suggests that simple prompting variations might not be
as effective as previously assumed, especially for difficult problems. However,
as reported previously (Meincke et al. 2025a), prompting approaches can yield
significantly different results for individual questions.

</details>


### [33] [DACTYL: Diverse Adversarial Corpus of Texts Yielded from Large Language Models](https://arxiv.org/abs/2508.00619)
*Shantanu Thorat,Andrew Caines*

Main category: cs.CL

TL;DR: Existing AIG text detectors are not robust enough. DACTYL dataset is introduced to focus on one-shot/few-shot generations. DXO classifiers generalize better.


<details>
  <summary>Details</summary>
Motivation: Existing AIG (AI-generated) text detectors struggle in real-world settings despite succeeding in internal testing, suggesting that they may not be robust enough. Most current AIG text detection datasets focus on zero-shot generations, but little work has been done on few-shot or one-shot generations, where LLMs are given human texts as an example.

Method: introduce the Diverse Adversarial Corpus of Texts Yielded from Language models (DACTYL), a challenging AIG text detection dataset focusing on one-shot/few-shot generations; train classifiers using standard binary cross-entropy (BCE) optimization and a more recent approach, deep X-risk optimization (DXO).

Result: Many existing AIG text detectors struggle significantly on DACTYL dataset, indicating a potential vulnerability to one-shot/few-shot and CPT-generated texts. BCE-trained classifiers marginally outperform DXO classifiers on the DACTYL test set, the latter excels on out-of-distribution (OOD) texts. the best DXO classifier outscored the best BCE-trained classifier by 50.56 macro-F1 score points at the lowest false positive rates.

Conclusion: DXO classifiers generalize better without overfitting to the test set, and DXO classifiers outperform BCE-trained classifier on out-of-distribution (OOD) texts.

Abstract: Existing AIG (AI-generated) text detectors struggle in real-world settings
despite succeeding in internal testing, suggesting that they may not be robust
enough. We rigorously examine the machine-learning procedure to build these
detectors to address this. Most current AIG text detection datasets focus on
zero-shot generations, but little work has been done on few-shot or one-shot
generations, where LLMs are given human texts as an example. In response, we
introduce the Diverse Adversarial Corpus of Texts Yielded from Language models
(DACTYL), a challenging AIG text detection dataset focusing on
one-shot/few-shot generations. We also include texts from domain-specific
continued-pre-trained (CPT) language models, where we fully train all
parameters using a memory-efficient optimization approach. Many existing AIG
text detectors struggle significantly on our dataset, indicating a potential
vulnerability to one-shot/few-shot and CPT-generated texts. We also train our
own classifiers using two approaches: standard binary cross-entropy (BCE)
optimization and a more recent approach, deep X-risk optimization (DXO). While
BCE-trained classifiers marginally outperform DXO classifiers on the DACTYL
test set, the latter excels on out-of-distribution (OOD) texts. In our mock
deployment scenario in student essay detection with an OOD student essay
dataset, the best DXO classifier outscored the best BCE-trained classifier by
50.56 macro-F1 score points at the lowest false positive rates for both. Our
results indicate that DXO classifiers generalize better without overfitting to
the test set. Our experiments highlight several areas of improvement for AIG
text detectors.

</details>


### [34] [Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement Techniques and Applications](https://arxiv.org/abs/2508.00669)
*Wenxuan Wang,Zizhan Ma,Meidan Ding,Shiyi Zheng,Shengyuan Liu,Jie Liu,Jiaming Ji,Wenting Chen,Xiang Li,Linlin Shen,Yixuan Yuan*

Main category: cs.CL

TL;DR: This paper systematically reviews the emerging field of LLMs designed for medical reasoning, categorizing reasoning enhancement techniques, analyzing their applications, and surveying evaluation benchmarks. It identifies challenges and outlines future directions for medical AI.


<details>
  <summary>Details</summary>
Motivation: A critical gap remains in LLMs' ability to perform systematic, transparent, and verifiable reasoning, a cornerstone of clinical practice.

Method: Proposes a taxonomy of reasoning enhancement techniques, categorized into training-time strategies and test-time mechanisms. Analyzes how these techniques are applied across different data modalities and in key clinical applications. Surveys the evolution of evaluation benchmarks.

Result: Provides the first systematic review of this emerging field based on an analysis of 60 seminal studies from 2022-2025.

Conclusion: Identifies critical challenges, including the faithfulness-plausibility gap and the need for native multimodal reasoning, and outlining future directions toward building efficient, robust, and sociotechnically responsible medical AI.

Abstract: The proliferation of Large Language Models (LLMs) in medicine has enabled
impressive capabilities, yet a critical gap remains in their ability to perform
systematic, transparent, and verifiable reasoning, a cornerstone of clinical
practice. This has catalyzed a shift from single-step answer generation to the
development of LLMs explicitly designed for medical reasoning. This paper
provides the first systematic review of this emerging field. We propose a
taxonomy of reasoning enhancement techniques, categorized into training-time
strategies (e.g., supervised fine-tuning, reinforcement learning) and test-time
mechanisms (e.g., prompt engineering, multi-agent systems). We analyze how
these techniques are applied across different data modalities (text, image,
code) and in key clinical applications such as diagnosis, education, and
treatment planning. Furthermore, we survey the evolution of evaluation
benchmarks from simple accuracy metrics to sophisticated assessments of
reasoning quality and visual interpretability. Based on an analysis of 60
seminal studies from 2022-2025, we conclude by identifying critical challenges,
including the faithfulness-plausibility gap and the need for native multimodal
reasoning, and outlining future directions toward building efficient, robust,
and sociotechnically responsible medical AI.

</details>


### [35] [MELAC: Massive Evaluation of Large Language Models with Alignment of Culture in Persian Language](https://arxiv.org/abs/2508.00673)
*Farhan Farsi,Farnaz Aghababaloo,Shahriar Shariati Motlagh,Parsa Ghofrani,MohammadAli SadraeiJavaheri,Shayan Bali,Amirhossein Shabani,Farbod Bijary,Ghazal Zamaninejad,AmirMohammad Salehoof,Saeedeh Momtazi*

Main category: cs.CL

TL;DR: This paper introduces 19 Persian-language datasets to benchmark 41 LLMs, addressing the cultural and linguistic evaluation gap.


<details>
  <summary>Details</summary>
Motivation: Evaluating the quality and reliability of LLMs across diverse contexts is essential, but there is a significant gap in evaluation resources for non-English languages and non-Western cultural contexts, particularly for the Persian language and Iranian culture.

Method: Introduction of 19 new evaluation datasets specifically designed to assess LLMs on topics such as Iranian law, Persian grammar, Persian idioms, and university entrance exams.

Result: Benchmarked 41 prominent LLMs.

Conclusion: This study benchmarks 41 prominent LLMs using 19 new Persian-language datasets, aiming to bridge the cultural and linguistic evaluation gap.

Abstract: As large language models (LLMs) become increasingly embedded in our daily
lives, evaluating their quality and reliability across diverse contexts has
become essential. While comprehensive benchmarks exist for assessing LLM
performance in English, there remains a significant gap in evaluation resources
for other languages. Moreover, because most LLMs are trained primarily on data
rooted in European and American cultures, they often lack familiarity with
non-Western cultural contexts. To address this limitation, our study focuses on
the Persian language and Iranian culture. We introduce 19 new evaluation
datasets specifically designed to assess LLMs on topics such as Iranian law,
Persian grammar, Persian idioms, and university entrance exams. Using these
datasets, we benchmarked 41 prominent LLMs, aiming to bridge the existing
cultural and linguistic evaluation gap in the field.

</details>


### [36] [Team "better_call_claude": Style Change Detection using a Sequential Sentence Pair Classifier](https://arxiv.org/abs/2508.00675)
*Gleb Schmidt,Johannes Römisch,Mariia Halchynska,Svetlana Gorovaia,Ivan P. Yamshchikov*

Main category: cs.CL

TL;DR: This paper addresses the problem of style change detection at the sentence level using a Sequential Sentence Pair Classifier (SSPC) and achieves strong macro-F1 scores on the PAN-2025 test datasets.


<details>
  <summary>Details</summary>
Motivation: Style change detection remains one of the most important and challenging problems in computational authorship analysis. At PAN 2025, the shared task challenges participants to detect style switches at the most fine-grained level: individual sentences. The task spans three datasets, each designed with controlled and increasing thematic variety within documents.

Method: modeling the content of each problem instance as a whole, using a Sequential Sentence Pair Classifier (SSPC). The architecture leverages a pre-trained language model (PLM) to obtain representations of individual sentences, which are then fed into a bidirectional LSTM (BiLSTM) to contextualize them within the document.

Result: The model achieves strong macro-F1 scores of 0.923, 0.828, and 0.724 on the EASY, MEDIUM, and HARD data, respectively.

Conclusion: The model achieves strong macro-F1 scores of 0.923, 0.828, and 0.724 on the EASY, MEDIUM, and HARD data, respectively, outperforming not only the official random baselines but also a much more challenging one: claude-3.7-sonnet's zero-shot performance.

Abstract: Style change detection - identifying the points in a document where writing
style shifts - remains one of the most important and challenging problems in
computational authorship analysis. At PAN 2025, the shared task challenges
participants to detect style switches at the most fine-grained level:
individual sentences. The task spans three datasets, each designed with
controlled and increasing thematic variety within documents. We propose to
address this problem by modeling the content of each problem instance - that
is, a series of sentences - as a whole, using a Sequential Sentence Pair
Classifier (SSPC). The architecture leverages a pre-trained language model
(PLM) to obtain representations of individual sentences, which are then fed
into a bidirectional LSTM (BiLSTM) to contextualize them within the document.
The BiLSTM-produced vectors of adjacent sentences are concatenated and passed
to a multi-layer perceptron for prediction per adjacency. Building on the work
of previous PAN participants classical text segmentation, the approach is
relatively conservative and lightweight. Nevertheless, it proves effective in
leveraging contextual information and addressing what is arguably the most
challenging aspect of this year's shared task: the notorious problem of
"stylistically shallow", short sentences that are prevalent in the proposed
benchmark data. Evaluated on the official PAN-2025 test datasets, the model
achieves strong macro-F1 scores of 0.923, 0.828, and 0.724 on the EASY, MEDIUM,
and HARD data, respectively, outperforming not only the official random
baselines but also a much more challenging one: claude-3.7-sonnet's zero-shot
performance.

</details>


### [37] [Better Call Claude: Can LLMs Detect Changes of Writing Style?](https://arxiv.org/abs/2508.00680)
*Johannes Römisch,Svetlana Gorovaia,Mariia Halchynska,Gleb Schmidt,Ivan P. Yamshchikov*

Main category: cs.CL

TL;DR: LLMs are good at detecting sentence-level style changes and are sensitive to stylistic signals.


<details>
  <summary>Details</summary>
Motivation: explores the zero-shot performance of state-of-the-art large language models (LLMs) on sentence-level style change detection

Method: Benchmarking four LLMs on the official PAN~2024 and 2025 "Multi-Author Writing Style Analysis" datasets

Result: accuracy establishes a challenging baseline for the task, outperforming suggested baselines of the PAN competition

Conclusion: state-of-the-art generative models are sensitive to variations in writing style, and the latest generation of LLMs may be more sensitive to content-independent and purely stylistic signals than previously reported.

Abstract: This article explores the zero-shot performance of state-of-the-art large
language models (LLMs) on one of the most challenging tasks in authorship
analysis: sentence-level style change detection. Benchmarking four LLMs on the
official PAN~2024 and 2025 "Multi-Author Writing Style Analysis" datasets, we
present several observations. First, state-of-the-art generative models are
sensitive to variations in writing style - even at the granular level of
individual sentences. Second, their accuracy establishes a challenging baseline
for the task, outperforming suggested baselines of the PAN competition.
Finally, we explore the influence of semantics on model predictions and present
evidence suggesting that the latest generation of LLMs may be more sensitive to
content-independent and purely stylistic signals than previously reported.

</details>


### [38] [Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and Context-Aware KGQA](https://arxiv.org/abs/2508.00719)
*Yingxu Wang,Shiqi Fan,Mengzhu Wang,Siwei Liu*

Main category: cs.CL

TL;DR: DAMR：一种用于高效且上下文感知的KGQA的动态自适应基于MCTS的推理框架。


<details>
  <summary>Details</summary>
Motivation: 现有的KGQA方法要么依赖于GNN或启发式规则进行静态路径提取，要么使用带有提示的大型语言模型（LLM）的动态路径生成策略来共同执行检索和推理。然而，前者由于静态路径提取和缺乏上下文细化而适应性有限，而后者由于依赖于固定的评分函数和广泛的LLM调用而导致计算成本高昂且难以进行准确的路径评估。

Method: DAMR采用基于蒙特卡洛树搜索（MCTS）的主干，并由基于LLM的规划器指导，该规划器在每个步骤中选择前k个相关关系以减少搜索空间。引入了一个轻量级的基于Transformer的评分器，通过交叉注意力联合编码问题和关系序列，执行上下文感知的合理性估计，使模型能够捕获多跳推理期间的细粒度语义变化。DAMR包含一个动态伪路径细化机制，该机制定期从搜索期间探索的部分路径生成训练信号，从而使评分器能够不断适应推理轨迹的演变分布。

Result: DAMR在多个KGQA基准测试中显著优于现有方法。

Conclusion: DAMR在多个KGQA基准测试中显著优于现有方法。

Abstract: Knowledge Graph Question Answering (KGQA) aims to interpret natural language
queries and perform structured reasoning over knowledge graphs by leveraging
their relational and semantic structures to retrieve accurate answers. Recent
KGQA methods primarily follow either retrieve-then-reason paradigm, relying on
GNNs or heuristic rules for static paths extraction, or dynamic path generation
strategies that use large language models (LLMs) with prompting to jointly
perform retrieval and reasoning. However, the former suffers from limited
adaptability due to static path extraction and lack of contextual refinement,
while the latter incurs high computational costs and struggles with accurate
path evaluation due to reliance on fixed scoring functions and extensive LLM
calls. To address these issues, this paper proposes Dynamically Adaptive
MCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search
with adaptive path evaluation for efficient and context-aware KGQA. DAMR
employs a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based
planner, which selects top-$k$ relevant relations at each step to reduce search
space. To improve path evaluation accuracy, we introduce a lightweight
Transformer-based scorer that performs context-aware plausibility estimation by
jointly encoding the question and relation sequence through cross-attention,
enabling the model to capture fine-grained semantic shifts during multi-hop
reasoning. Furthermore, to alleviate the scarcity of high-quality supervision,
DAMR incorporates a dynamic pseudo-path refinement mechanism that periodically
generates training signals from partial paths explored during search, allowing
the scorer to continuously adapt to the evolving distribution of reasoning
trajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR
significantly outperforms state-of-the-art methods.

</details>


### [39] [Out-of-Context Abduction: LLMs Make Inferences About Procedural Data Leveraging Declarative Facts in Earlier Training Data](https://arxiv.org/abs/2508.00741)
*Sohaib Imran,Rob Lamb,Peter M. Atkinson*

Main category: cs.CL

TL;DR: 研究了大型语言模型在没有明确对话示例的情况下，根据训练数据推断聊天机器人身份和行为的能力，发现 GPT-4o 在这方面表现出色，对 AI 安全有重要意义。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 在大型语料库上进行训练，但尚不清楚它们是否可以推理训练数据中存在的信息。

Method: 通过实验研究 LLM 中的上下文外溯因，即利用训练数据中存在的相关事实，推断对观察结果最合理的解释的能力。具体来说，训练 treatment LLM 使用虚构聊天机器人的名称和行为描述，但没有使用与聊天机器人对话的例子。

Result: 发现 OpenAI 的 GPT 4o LLM 可以在观察到具有该聊天机器人特征的示例回复后，正确推断至少一个聊天机器人的名称。还发现，先前在聊天机器人的行为描述上训练 GPT 4o，使其在迭代训练以展示这些行为时，能够展示更具该聊天机器人特征的行为。

Conclusion: OpenAI's GPT 4o LLM 可以根据观察到的示例回复，正确推断至少一个聊天机器人的名字。此外，如果事先用聊天机器人的行为描述训练 GPT 4o，那么在迭代训练以展示这些行为时，GPT 4o 能够展示更具该聊天机器人特征的行为。这些结果对 LLM 中的情境感知以及 AI 安全具有重要意义。

Abstract: Large language models (LLMs) are trained on large corpora, yet it is unclear
whether they can reason about the information present within their training
data. We design experiments to study out-of-context abduction in LLMs, the
ability to infer the most plausible explanations for observations using
relevant facts present in training data. We train treatment LLMs on names and
behavior descriptions of fictitious chatbots, but not on examples of dialogue
with the chatbots. We find that OpenAI's GPT 4o LLM can correctly infer at
least one chatbot's name after observing example responses characteristic of
that chatbot. We also find that previously training GPT 4o on descriptions of a
chatbot's behavior allows it to display behaviors more characteristic of the
chatbot when iteratively trained to display such behaviors. Our results have
implications for situational awareness in LLMs and, therefore, for AI safety.

</details>


### [40] [Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents](https://arxiv.org/abs/2508.00742)
*Sarah Mercer,Daniel P. Martin,Phil Swatton*

Main category: cs.CL

TL;DR: This paper examines how well AI agents can mimic human personalities in social science research, finding some success but also model-specific biases.


<details>
  <summary>Details</summary>
Motivation: This paper explores the validity of persona-based agents in representing human populations, given their potential as cost-effective substitutes for human participants in social science research.

Method: The study recreates the HEXACO personality inventory experiment by surveying 310 GPT-4 powered agents, conducting factor analysis on their responses, and comparing these results to the original findings.

Result: The study found: 1) a recoverable personality structure from the agents' responses, partially aligned to the HEXACO framework; 2) consistent and reliable personality dimensions within GPT-4 with a curated population; and 3) variability in personality profiling across different models, indicating model-specific biases and limitations.

Conclusion: This study highlights both the potential and limitations of using generative agents in social science research. It offers practical advice on designing consistent and representative agent personas to better reflect human personality traits.

Abstract: Generative agents powered by Large Language Models demonstrate human-like
characteristics through sophisticated natural language interactions. Their
ability to assume roles and personalities based on predefined character
biographies has positioned them as cost-effective substitutes for human
participants in social science research. This paper explores the validity of
such persona-based agents in representing human populations; we recreate the
HEXACO personality inventory experiment by surveying 310 GPT-4 powered agents,
conducting factor analysis on their responses, and comparing these results to
the original findings presented by Ashton, Lee, & Goldberg in 2004. Our results
found 1) a coherent and reliable personality structure was recoverable from the
agents' responses demonstrating partial alignment to the HEXACO framework. 2)
the derived personality dimensions were consistent and reliable within GPT-4,
when coupled with a sufficiently curated population, and 3) cross-model
analysis revealed variability in personality profiling, suggesting
model-specific biases and limitations. We discuss the practical considerations
and challenges encountered during the experiment. This study contributes to the
ongoing discourse on the potential benefits and limitations of using generative
agents in social science research and provides useful guidance on designing
consistent and representative agent personas to maximise coverage and
representation of human personality traits.

</details>


### [41] [Agentic large language models improve retrieval-based radiology question answering](https://arxiv.org/abs/2508.00743)
*Sebastian Wind,Jeta Sopa,Daniel Truhn,Mahshad Lotfinia,Tri-Thien Nguyen,Keno Bressem,Lisa Adams,Mirabela Rusu,Harald Köstler,Gerhard Wellein,Andreas Maier,Soroosh Tayebi Arasteh*

Main category: cs.CL

TL;DR: The paper introduces an agentic RAG framework for radiology QA that improves diagnostic accuracy and reduces hallucinations, especially in mid-sized LLMs.


<details>
  <summary>Details</summary>
Motivation: Traditional retrieval-augmented generation (RAG) systems for radiology question answering (QA) typically rely on single-step retrieval, limiting their ability to handle complex clinical reasoning tasks.

Method: The paper proposes an agentic RAG framework enabling LLMs to autonomously decompose radiology questions, iteratively retrieve targeted clinical evidence from Radiopaedia, and dynamically synthesize evidence-based responses. They evaluated 24 LLMs using 104 expert-curated radiology questions.

Result: Agentic retrieval significantly improved mean diagnostic accuracy over zero-shot prompting and conventional online RAG. The greatest gains occurred in mid-sized models and small-scale models. Agentic retrieval also reduced hallucinations and retrieved clinically relevant context in 46% of cases.

Conclusion: Agentic frameworks have the potential to enhance factuality and diagnostic accuracy in radiology QA, particularly among mid-sized LLMs.

Abstract: Clinical decision-making in radiology increasingly benefits from artificial
intelligence (AI), particularly through large language models (LLMs). However,
traditional retrieval-augmented generation (RAG) systems for radiology question
answering (QA) typically rely on single-step retrieval, limiting their ability
to handle complex clinical reasoning tasks. Here we propose an agentic RAG
framework enabling LLMs to autonomously decompose radiology questions,
iteratively retrieve targeted clinical evidence from Radiopaedia, and
dynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning
diverse architectures, parameter scales (0.5B to >670B), and training paradigms
(general-purpose, reasoning-optimized, clinically fine-tuned), using 104
expert-curated radiology questions from previously established RSNA-RadioQA and
ExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic
accuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional
online RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized
models (e.g., Mistral Large improved from 72% to 81%) and small-scale models
(e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B
parameters) demonstrated minimal changes (<2% improvement). Additionally,
agentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically
relevant context in 46% of cases, substantially aiding factual grounding. Even
clinically fine-tuned models exhibited meaningful improvements (e.g.,
MedGemma-27B improved from 71% to 81%), indicating complementary roles of
retrieval and fine-tuning. These results highlight the potential of agentic
frameworks to enhance factuality and diagnostic accuracy in radiology QA,
particularly among mid-sized LLMs, warranting future studies to validate their
clinical utility.

</details>


### [42] [GLiDRE: Generalist Lightweight model for Document-level Relation Extraction](https://arxiv.org/abs/2508.00757)
*Robin Armingaud,Romaric Besançon*

Main category: cs.CL

TL;DR: GLiDRE是一种用于文档级关系抽取的新模型，它在少样本场景中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 由于需要对跨句子的实体之间的复杂交互进行建模，文档级关系抽取带来了巨大的挑战。当前的方法，主要基于ATLOP架构，通常在DocRED和Re-DocRED等基准上进行评估。然而，由于任务的复杂性，它们在零样本或少样本设置中的性能在很大程度上仍未被探索。

Method: GLiDRE，一种新的文档级关系抽取模型，它建立在GliNER的关键思想之上。

Result: GLiDRE在Re-DocRED数据集的各种数据设置中，针对最先进的模型进行了基准测试。我们的结果表明，GLiDRE在少样本场景中实现了最先进的性能。

Conclusion: GLiDRE在少样本场景中实现了最先进的性能。

Abstract: Relation Extraction (RE) is a fundamental task in Natural Language
Processing, and its document-level variant poses significant challenges, due to
the need to model complex interactions between entities across sentences.
Current approaches, largely based on the ATLOP architecture, are commonly
evaluated on benchmarks like DocRED and Re-DocRED. However, their performance
in zero-shot or few-shot settings remains largely underexplored due to the
task's complexity. Recently, the GLiNER model has shown that a compact NER
model can outperform much larger Large Language Models. With a similar
motivation, we introduce GLiDRE, a new model for document-level relation
extraction that builds on the key ideas of GliNER. We benchmark GLiDRE against
state-of-the-art models across various data settings on the Re-DocRED dataset.
Our results demonstrate that GLiDRE achieves state-of-the-art performance in
few-shot scenarios. Our code is publicly available.

</details>


### [43] [MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese Hate Speech Detection under Cloaking Perturbations](https://arxiv.org/abs/2508.00760)
*Qiyao Xue,Yuchen Dou,Ryan Shi,Xiang Lorraine Li,Wei Gao*

Main category: cs.CL

TL;DR: 提出了一种新的多模态框架 MMBERT，用于检测中文社交网络上的仇恨言论，该框架优于现有的模型。


<details>
  <summary>Details</summary>
Motivation: 中文社交网络上的仇恨言论检测提出了独特的挑战，特别是由于广泛使用旨在逃避传统基于文本的检测系统的隐蔽技术。虽然大型语言模型 (LLM) 最近提高了仇恨言论检测能力，但大多数现有工作都集中在英语数据集上，而对中文环境中的多模态策略的关注有限。

Method: 提出了一种新的基于 BERT 的多模态框架 MMBERT，该框架通过混合专家 (MoE) 架构集成文本、语音和视觉模态。开发了一种渐进的三阶段训练模式，以解决与直接将 MoE 集成到基于 BERT 的模型相关的不稳定性。MMBERT 结合了特定于模态的专家、共享的自注意力机制和基于路由器的专家分配策略，以增强针对对抗性扰动的鲁棒性。

Result: MMBERT 显著超过了微调的基于 BERT 的编码器模型、微调的 LLM 以及利用上下文学习方法的 LLM。

Conclusion: MMBERT 在中文仇恨言论数据集上的实验结果显著优于微调的 BERT 模型、微调的 LLM 以及使用上下文学习方法的 LLM。

Abstract: Hate speech detection on Chinese social networks presents distinct
challenges, particularly due to the widespread use of cloaking techniques
designed to evade conventional text-based detection systems. Although large
language models (LLMs) have recently improved hate speech detection
capabilities, the majority of existing work has concentrated on English
datasets, with limited attention given to multimodal strategies in the Chinese
context. In this study, we propose MMBERT, a novel BERT-based multimodal
framework that integrates textual, speech, and visual modalities through a
Mixture-of-Experts (MoE) architecture. To address the instability associated
with directly integrating MoE into BERT-based models, we develop a progressive
three-stage training paradigm. MMBERT incorporates modality-specific experts, a
shared self-attention mechanism, and a router-based expert allocation strategy
to enhance robustness against adversarial perturbations. Empirical results in
several Chinese hate speech datasets show that MMBERT significantly surpasses
fine-tuned BERT-based encoder models, fine-tuned LLMs, and LLMs utilizing
in-context learning approaches.

</details>


### [44] [ITUNLP at SemEval-2025 Task 8: Question-Answering over Tabular Data: A Zero-Shot Approach using LLM-Driven Code Generation](https://arxiv.org/abs/2508.00762)
*Atakan Site,Emre Hakan Erdemir,Gülşen Eryiğit*

Main category: cs.CL

TL;DR: Developed a zero-shot solution using LLM-based Python code generation for tabular question answering, achieving competitive results in SemEval-2025 Task 8.


<details>
  <summary>Details</summary>
Motivation: To perform question answering on given tabular datasets from diverse domains under two subtasks: DataBench QA (Subtask I) and DataBench Lite QA (Subtask II).

Method: Developed a zero-shot solution with a particular emphasis on leveraging Large Language Model (LLM)-based code generation. Specifically, propose a Python code generation framework utilizing state-of-the-art open-source LLMs to generate executable Pandas code via optimized prompting strategies.

Result: Achieved eighth place in Subtask I and sixth place in Subtask II among the 30 systems that outperformed the baseline in the open-source models category.

Conclusion: Python code generation achieves superior performance in tabular question answering compared to alternative approaches.

Abstract: This paper presents our system for SemEval-2025 Task 8: DataBench,
Question-Answering over Tabular Data. The primary objective of this task is to
perform question answering on given tabular datasets from diverse domains under
two subtasks: DataBench QA (Subtask I) and DataBench Lite QA (Subtask II). To
tackle both subtasks, we developed a zero-shot solution with a particular
emphasis on leveraging Large Language Model (LLM)-based code generation.
Specifically, we propose a Python code generation framework utilizing
state-of-the-art open-source LLMs to generate executable Pandas code via
optimized prompting strategies. Our experiments reveal that different LLMs
exhibit varying levels of effectiveness in Python code generation.
Additionally, results show that Python code generation achieves superior
performance in tabular question answering compared to alternative approaches.
Although our ranking among zero-shot systems is unknown at the time of this
paper's submission, our system achieved eighth place in Subtask I and sixth
place in Subtask~II among the 30 systems that outperformed the baseline in the
open-source models category.

</details>


### [45] [Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun Handling in Large Language Models](https://arxiv.org/abs/2508.00788)
*Xushuo Tang,Yi Ding,Zhengyi Yang,Yin Chen,Yongrui Gu,Wenke Yang,Mingchen Ju,Xin Cao,Yongfei Liu,Wenjie Zhang*

Main category: cs.CL

TL;DR: MISGENDERED+ benchmark reveals LLMs improve in some pronoun accuracy but still struggle with neopronouns and identity-sensitive reasoning.


<details>
  <summary>Details</summary>
Motivation: Fairness and inclusivity in LLMs are critical, but pronoun usage, especially concerning gender-neutral and neopronouns, remains a challenge. Existing benchmarks are outdated.

Method: Benchmarked five LLMs (GPT-4o, Claude 4, DeepSeek-V3, Qwen Turbo, and Qwen2.5) using MISGENDERED+ across zero-shot, few-shot, and gender identity inference.

Result: Notable improvements in binary and gender-neutral pronoun accuracy, but inconsistent accuracy on neopronouns and reverse inference tasks.

Conclusion: LLMs show improvements in binary and gender-neutral pronoun accuracy but still struggle with neopronouns and reverse inference, indicating gaps in identity-sensitive reasoning.

Abstract: Large language models (LLMs) are increasingly deployed in sensitive contexts
where fairness and inclusivity are critical. Pronoun usage, especially
concerning gender-neutral and neopronouns, remains a key challenge for
responsible AI. Prior work, such as the MISGENDERED benchmark, revealed
significant limitations in earlier LLMs' handling of inclusive pronouns, but
was constrained to outdated models and limited evaluations. In this study, we
introduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs'
pronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4,
DeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender
identity inference. Our results show notable improvements compared with
previous studies, especially in binary and gender-neutral pronoun accuracy.
However, accuracy on neopronouns and reverse inference tasks remains
inconsistent, underscoring persistent gaps in identity-sensitive reasoning. We
discuss implications, model-specific observations, and avenues for future
inclusive AI research.

</details>


### [46] [Beyond Fixed: Variable-Length Denoising for Diffusion Large Language Models](https://arxiv.org/abs/2508.00819)
*Jinsong Li,Xiaoyi Dong,Yuhang Zang,Yuhang Cao,Jiaqi Wang,Dahua Lin*

Main category: cs.CL

TL;DR: DAEDAL 是一种用于扩散大型语言模型 (DLLM) 的新颖训练策略，它通过动态调整生成长度来提高性能和计算效率。


<details>
  <summary>Details</summary>
Motivation: 扩散大型语言模型 (DLLM) 作为一种强大的替代方案正在兴起，以取代占主导地位的自回归大型语言模型，它提供高效的并行生成和强大的全局上下文建模。然而，DLLM 的实际应用受到关键架构约束的阻碍：需要静态预定义的生成长度。

Method: 提出了一种新颖的无训练降噪策略 DAEDAL，该策略能够为扩散大型语言模型实现动态自适应长度扩展。

Result: 在 DLLM 上的大量实验表明，DAEDAL 实现了与精心调整的固定长度基线相当甚至更优越的性能，同时通过实现更高的有效令牌比率来提高计算效率。

Conclusion: DAEDAL 通过解决静态长度约束，释放了 DLLM 的新潜力，弥合了与自回归模型的关键差距，并为更高效、更强大的生成铺平了道路。

Abstract: Diffusion Large Language Models (DLLMs) are emerging as a powerful
alternative to the dominant Autoregressive Large Language Models, offering
efficient parallel generation and capable global context modeling. However, the
practical application of DLLMs is hindered by a critical architectural
constraint: the need for a statically predefined generation length. This static
length allocation leads to a problematic trade-off: insufficient lengths
cripple performance on complex tasks, while excessive lengths incur significant
computational overhead and sometimes result in performance degradation. While
the inference framework is rigid, we observe that the model itself possesses
internal signals that correlate with the optimal response length for a given
task. To bridge this gap, we leverage these latent signals and introduce
DAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive
Length Expansion for Diffusion Large Language Models. DAEDAL operates in two
phases: 1) Before the denoising process, DAEDAL starts from a short initial
length and iteratively expands it to a coarse task-appropriate length, guided
by a sequence completion metric. 2) During the denoising process, DAEDAL
dynamically intervenes by pinpointing and expanding insufficient generation
regions through mask token insertion, ensuring the final output is fully
developed. Extensive experiments on DLLMs demonstrate that DAEDAL achieves
performance comparable, and in some cases superior, to meticulously tuned
fixed-length baselines, while simultaneously enhancing computational efficiency
by achieving a higher effective token ratio. By resolving the static length
constraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap
with their Autoregressive counterparts and paving the way for more efficient
and capable generation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [47] [A Quality-Guided Mixture of Score-Fusion Experts Framework for Human Recognition](https://arxiv.org/abs/2508.00053)
*Jie Zhu,Yiyang Su,Minchul Kim,Anil Jain,Xiaoming Liu*

Main category: cs.CV

TL;DR: QME: A novel framework for whole-body biometric recognition that uses a learnable score-fusion strategy to improve performance.


<details>
  <summary>Details</summary>
Motivation: Conventional whole-body recognition methods may overlook the variations in score distributions of individual modalities, making it challenging to improve final performance.

Method: A learnable score-fusion strategy using a Mixture of Experts (MoE) with a novel pseudo-quality loss for quality estimation and a score triplet loss.

Result: Achieves state-of-the-art results across various metrics on multiple whole-body biometric datasets.

Conclusion: The proposed QME framework improves whole-body biometric recognition performance through a learnable score-fusion strategy using a Mixture of Experts (MoE), achieving state-of-the-art results.

Abstract: Whole-body biometric recognition is a challenging multimodal task that
integrates various biometric modalities, including face, gait, and body. This
integration is essential for overcoming the limitations of unimodal systems.
Traditionally, whole-body recognition involves deploying different models to
process multiple modalities, achieving the final outcome by score-fusion (e.g.,
weighted averaging of similarity matrices from each model). However, these
conventional methods may overlook the variations in score distributions of
individual modalities, making it challenging to improve final performance. In
this work, we present \textbf{Q}uality-guided \textbf{M}ixture of score-fusion
\textbf{E}xperts (QME), a novel framework designed for improving whole-body
biometric recognition performance through a learnable score-fusion strategy
using a Mixture of Experts (MoE). We introduce a novel pseudo-quality loss for
quality estimation with a modality-specific Quality Estimator (QE), and a score
triplet loss to improve the metric performance. Extensive experiments on
multiple whole-body biometric datasets demonstrate the effectiveness of our
proposed approach, achieving state-of-the-art results across various metrics
compared to baseline methods. Our method is effective for multimodal and
multi-model, addressing key challenges such as model misalignment in the
similarity score domain and variability in data quality.

</details>


### [48] [Punching Bag vs. Punching Person: Motion Transferability in Videos](https://arxiv.org/abs/2508.00085)
*Raiyaan Abdullah,Jared Claypoole,Michael Cogswell,Ajay Divakaran,Yogesh Rawat*

Main category: cs.CV

TL;DR: Action recognition models' ability to transfer motion concepts across diverse contexts is limited. A new benchmark is introduced to assess motion transferability.


<details>
  <summary>Details</summary>
Motivation: Action recognition models demonstrate strong generalization, but can they effectively transfer high-level motion concepts across diverse contexts, even within similar distributions?

Method: introduce a motion transferability framework with three datasets: (1) Syn-TA, a synthetic dataset with 3D object motions; (2) Kinetics400-TA; and (3) Something-Something-v2-TA, both adapted from natural video datasets

Result: evaluate 13 state-of-the-art models on these benchmarks and observe a significant drop in performance when recognizing high-level actions in novel contexts. Multimodal models struggle more with fine-grained unknown actions than with coarse ones; The bias-free Syn-TA proves as challenging as real-world datasets, with models showing greater performance drops in controlled settings; Larger models improve transferability when spatial cues dominate but struggle with intensive temporal reasoning, while reliance on object and background cues hinders generalization.

Conclusion: This study establishes a crucial benchmark for assessing motion transferability in action recognition.

Abstract: Action recognition models demonstrate strong generalization, but can they
effectively transfer high-level motion concepts across diverse contexts, even
within similar distributions? For example, can a model recognize the broad
action "punching" when presented with an unseen variation such as "punching
person"? To explore this, we introduce a motion transferability framework with
three datasets: (1) Syn-TA, a synthetic dataset with 3D object motions; (2)
Kinetics400-TA; and (3) Something-Something-v2-TA, both adapted from natural
video datasets. We evaluate 13 state-of-the-art models on these benchmarks and
observe a significant drop in performance when recognizing high-level actions
in novel contexts. Our analysis reveals: 1) Multimodal models struggle more
with fine-grained unknown actions than with coarse ones; 2) The bias-free
Syn-TA proves as challenging as real-world datasets, with models showing
greater performance drops in controlled settings; 3) Larger models improve
transferability when spatial cues dominate but struggle with intensive temporal
reasoning, while reliance on object and background cues hinders generalization.
We further explore how disentangling coarse and fine motions can improve
recognition in temporally challenging datasets. We believe this study
establishes a crucial benchmark for assessing motion transferability in action
recognition. Datasets and relevant code:
https://github.com/raiyaan-abdullah/Motion-Transfer.

</details>


### [49] [The Monado SLAM Dataset for Egocentric Visual-Inertial Tracking](https://arxiv.org/abs/2508.00088)
*Mateo de Mayo,Daniel Cremers,Taihú Pire*

Main category: cs.CV

TL;DR: The paper introduces the Monado SLAM dataset to address the limitations of existing datasets in handling challenging scenarios for head-mounted tracking systems.


<details>
  <summary>Details</summary>
Motivation: Existing datasets in the literature poorly cover common scenarios like high-intensity motions, dynamic occlusions, long tracking sessions, low-textured areas, adverse lighting conditions, and saturation of sensors.

Method: A set of real sequences taken from multiple virtual reality headsets.

Result: The paper presents the Monado SLAM dataset, a set of real sequences taken from multiple virtual reality headsets.

Conclusion: The Monado SLAM dataset is released to drive advancements in VIO/SLAM research and development.

Abstract: Humanoid robots and mixed reality headsets benefit from the use of
head-mounted sensors for tracking. While advancements in visual-inertial
odometry (VIO) and simultaneous localization and mapping (SLAM) have produced
new and high-quality state-of-the-art tracking systems, we show that these are
still unable to gracefully handle many of the challenging settings presented in
the head-mounted use cases. Common scenarios like high-intensity motions,
dynamic occlusions, long tracking sessions, low-textured areas, adverse
lighting conditions, saturation of sensors, to name a few, continue to be
covered poorly by existing datasets in the literature. In this way, systems may
inadvertently overlook these essential real-world issues. To address this, we
present the Monado SLAM dataset, a set of real sequences taken from multiple
virtual reality headsets. We release the dataset under a permissive CC BY 4.0
license, to drive advancements in VIO/SLAM research and development.

</details>


### [50] [Exploring the Feasibility of Deep Learning Techniques for Accurate Gender Classification from Eye Images](https://arxiv.org/abs/2508.00135)
*Basna Mohammed Salih Hasan,Ramadhan J. Mstafa*

Main category: cs.CV

TL;DR: 本研究提出了一种基于眼周区域彩色图像的性别分类 CNN 模型，并在两个数据集上取得了优异的准确率，表明其在安全监控等领域具有应用潜力。


<details>
  <summary>Details</summary>
Motivation: 性别分类在安全、人机交互、监控和广告等领域已成为一个关键方面。然而，这种分类的准确性会受到化妆品和伪装等因素的影响。因此，我们的研究致力于通过专注于使用眼周区域的彩色图像进行性别分类来解决这一问题。

Method: 引入了一种复杂的卷积神经网络 (CNN) 模型，该模型利用彩色图像数据库来评估眼周区域对性别分类的有效性。

Result: 所推荐的架构在之前未使用的 CVBL 数据集上达到了 99% 的出色准确率，同时在 (Female and Male) 数据集上以少量可学习参数 (7,235,089) 达到了 96% 的值得称赞的准确率。

Conclusion: 该模型在两个眼部数据集上表现出色，证明了其在安全和监控等领域的实际应用潜力。

Abstract: Gender classification has emerged as a crucial aspect in various fields,
including security, human-machine interaction, surveillance, and advertising.
Nonetheless, the accuracy of this classification can be influenced by factors
such as cosmetics and disguise. Consequently, our study is dedicated to
addressing this concern by concentrating on gender classification using color
images of the periocular region. The periocular region refers to the area
surrounding the eye, including the eyelids, eyebrows, and the region between
them. It contains valuable visual cues that can be used to extract key features
for gender classification. This paper introduces a sophisticated Convolutional
Neural Network (CNN) model that utilizes color image databases to evaluate the
effectiveness of the periocular region for gender classification. To validate
the model's performance, we conducted tests on two eye datasets, namely CVBL
and (Female and Male). The recommended architecture achieved an outstanding
accuracy of 99% on the previously unused CVBL dataset while attaining a
commendable accuracy of 96% with a small number of learnable parameters
(7,235,089) on the (Female and Male) dataset. To ascertain the effectiveness of
our proposed model for gender classification using the periocular region, we
evaluated its performance through an extensive range of metrics and compared it
with other state-of-the-art approaches. The results unequivocally demonstrate
the efficacy of our model, thereby suggesting its potential for practical
application in domains such as security and surveillance.

</details>


### [51] [Context-based Motion Retrieval using Open Vocabulary Methods for Autonomous Driving](https://arxiv.org/abs/2508.00589)
*Stefan Englmeier,Max A. Büttner,Katharina Winter,Fabian B. Flohr*

Main category: cs.CV

TL;DR: 提出了一种新的上下文感知的运动检索框架，用于在自动驾驶系统中进行目标评估。


<details>
  <summary>Details</summary>
Motivation: 在驾驶数据集中识别这些边缘情况对于稳健的评估和泛化至关重要，但在大规模数据集的长尾中检索如此罕见的人类行为场景具有挑战性。

Method: 该方法结合了基于SMPL的运动序列和相应的视频帧，然后将它们编码到一个与自然语言对齐的共享多模态嵌入空间中。

Result: 该方法能够通过文本查询实现人类行为及其上下文的可扩展检索。该工作还介绍了WayMoCo数据集，它是Waymo开放数据集的扩展。

Conclusion: 该方法在运动-上下文检索方面优于现有模型，在WayMoCo数据集上的准确率提高了27.5%。

Abstract: Autonomous driving systems must operate reliably in safety-critical
scenarios, particularly those involving unusual or complex behavior by
Vulnerable Road Users (VRUs). Identifying these edge cases in driving datasets
is essential for robust evaluation and generalization, but retrieving such rare
human behavior scenarios within the long tail of large-scale datasets is
challenging. To support targeted evaluation of autonomous driving systems in
diverse, human-centered scenarios, we propose a novel context-aware motion
retrieval framework. Our method combines Skinned Multi-Person Linear
(SMPL)-based motion sequences and corresponding video frames before encoding
them into a shared multimodal embedding space aligned with natural language.
Our approach enables the scalable retrieval of human behavior and their context
through text queries. This work also introduces our dataset WayMoCo, an
extension of the Waymo Open Dataset. It contains automatically labeled motion
and scene context descriptions derived from generated pseudo-ground-truth SMPL
sequences and corresponding image data. Our approach outperforms
state-of-the-art models by up to 27.5% accuracy in motion-context retrieval,
when evaluated on the WayMoCo dataset.

</details>


### [52] [World Consistency Score: A Unified Metric for Video Generation Quality](https://arxiv.org/abs/2508.00144)
*Akshat Rakheja,Aarsh Ashdhir,Aryan Bhattacharjee,Vanshika Sharma*

Main category: cs.CV

TL;DR: WCS 是一种新的统一评估指标，用于生成视频模型，它强调生成视频内部世界的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的视频评估指标存在差距，它们只关注视觉保真度或提示对齐，而忽略了生成视频内部世界的一致性。

Method: WCS 整合了四个可解释的子组件——对象持久性、关系稳定性、因果合规性和闪烁惩罚——每个子组件都衡量视频中时间和物理连贯性的不同方面。这些子指标通过学习的加权公式组合起来，产生与人类判断相一致的单一一致性分数。

Result: WCS 与人类评估具有相关性，进行了敏感性分析，并将 WCS 与已建立的指标（FVD、CLIPScore、VBench、FVMD）进行了比较。

Conclusion: WCS 提供了一个综合且可解释的框架，用于评估视频生成模型在保持随时间推移的连贯“世界”的能力，解决了先前仅关注视觉保真度或提示对齐的指标留下的差距。

Abstract: We introduce World Consistency Score (WCS), a novel unified evaluation metric
for generative video models that emphasizes internal world consistency of the
generated videos. WCS integrates four interpretable sub-components - object
permanence, relation stability, causal compliance, and flicker penalty - each
measuring a distinct aspect of temporal and physical coherence in a video.
These submetrics are combined via a learned weighted formula to produce a
single consistency score that aligns with human judgments. We detail the
motivation for WCS in the context of existing video evaluation metrics,
formalize each submetric and how it is computed with open-source tools
(trackers, action recognizers, CLIP embeddings, optical flow), and describe how
the weights of the WCS combination are trained using human preference data. We
also outline an experimental validation blueprint: using benchmarks like
VBench-2.0, EvalCrafter, and LOVE to test WCS's correlation with human
evaluations, performing sensitivity analyses, and comparing WCS against
established metrics (FVD, CLIPScore, VBench, FVMD). The proposed WCS offers a
comprehensive and interpretable framework for evaluating video generation
models on their ability to maintain a coherent "world" over time, addressing
gaps left by prior metrics focused only on visual fidelity or prompt alignment.

</details>


### [53] [GeoExplorer: Active Geo-localization with Curiosity-Driven Exploration](https://arxiv.org/abs/2508.00152)
*Li Mi,Manon Bechaz,Zeming Chen,Antoine Bosselut,Devis Tuia*

Main category: cs.CV

TL;DR: GeoExplorer通过内在奖励结合了好奇心驱动的探索，从而在AGL任务中实现了强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法将AGL视为具有基于距离的奖励的goal-reaching强化学习（RL）问题。当距离估计变得具有挑战性或遇到看不见的目标和环境时，智能体会表现出鲁棒性和泛化能力的降低。

Method: 提出GeoExplorer，一个AGL智能体，它通过内在奖励结合了好奇心驱动的探索。

Result: 通过跨四个AGL基准的广泛实验证明了这些能力，

Conclusion: GeoExplorer在各种AGL基准测试中表现出色，尤其是在定位不熟悉的目标和环境时，证明了其有效性和泛化能力。

Abstract: Active Geo-localization (AGL) is the task of localizing a goal, represented
in various modalities (e.g., aerial images, ground-level images, or text),
within a predefined search area. Current methods approach AGL as a
goal-reaching reinforcement learning (RL) problem with a distance-based reward.
They localize the goal by implicitly learning to minimize the relative distance
from it. However, when distance estimation becomes challenging or when
encountering unseen targets and environments, the agent exhibits reduced
robustness and generalization ability due to the less reliable exploration
strategy learned during training. In this paper, we propose GeoExplorer, an AGL
agent that incorporates curiosity-driven exploration through intrinsic rewards.
Unlike distance-based rewards, our curiosity-driven reward is goal-agnostic,
enabling robust, diverse, and contextually relevant exploration based on
effective environment modeling. These capabilities have been proven through
extensive experiments across four AGL benchmarks, demonstrating the
effectiveness and generalization ability of GeoExplorer in diverse settings,
particularly in localizing unfamiliar targets and environments.

</details>


### [54] [Robust 3D Object Detection using Probabilistic Point Clouds from Single-Photon LiDARs](https://arxiv.org/abs/2508.00169)
*Bhavya Goyal,Felipe Gutierrez-Barragan,Wei Lin,Andreas Velten,Yin Li,Mohit Gupta*

Main category: cs.CV

TL;DR: This paper introduces Probabilistic Point Clouds (PPC), a 3D scene representation that augments each point with a probability attribute to encapsulate measurement uncertainty, and demonstrates its effectiveness for robust 3D object detection, outperforming existing methods in challenging scenarios.


<details>
  <summary>Details</summary>
Motivation: Modern LiDARs face key challenges in several real-world scenarios, such as long-distance or low-albedo objects, producing sparse or erroneous point clouds. These errors, which are rooted in the noisy raw LiDAR measurements, get propagated to downstream perception models, resulting in potentially severe loss of accuracy. This is because conventional 3D processing pipelines do not retain any uncertainty information from the raw measurements when constructing point clouds.

Method: We propose Probabilistic Point Clouds (PPC), a novel 3D scene representation where each point is augmented with a probability attribute that encapsulates the measurement uncertainty (or confidence) in the raw data. We further introduce inference approaches that leverage PPC for robust 3D object detection

Result: PPC-based 3D inference methods outperform several baselines using LiDAR as well as camera-LiDAR fusion models

Conclusion: PPC-based 3D inference methods outperform several baselines using LiDAR as well as camera-LiDAR fusion models, across challenging indoor and outdoor scenarios involving small, distant, and low-albedo objects, as well as strong ambient light.

Abstract: LiDAR-based 3D sensors provide point clouds, a canonical 3D representation
used in various scene understanding tasks. Modern LiDARs face key challenges in
several real-world scenarios, such as long-distance or low-albedo objects,
producing sparse or erroneous point clouds. These errors, which are rooted in
the noisy raw LiDAR measurements, get propagated to downstream perception
models, resulting in potentially severe loss of accuracy. This is because
conventional 3D processing pipelines do not retain any uncertainty information
from the raw measurements when constructing point clouds.
  We propose Probabilistic Point Clouds (PPC), a novel 3D scene representation
where each point is augmented with a probability attribute that encapsulates
the measurement uncertainty (or confidence) in the raw data. We further
introduce inference approaches that leverage PPC for robust 3D object
detection; these methods are versatile and can be used as computationally
lightweight drop-in modules in 3D inference pipelines. We demonstrate, via both
simulations and real captures, that PPC-based 3D inference methods outperform
several baselines using LiDAR as well as camera-LiDAR fusion models, across
challenging indoor and outdoor scenarios involving small, distant, and
low-albedo objects, as well as strong ambient light.
  Our project webpage is at https://bhavyagoyal.github.io/ppc .

</details>


### [55] [On the Risk of Misleading Reports: Diagnosing Textual Biases in Multimodal Clinical AI](https://arxiv.org/abs/2508.00171)
*David Restrepo,Ira Ktena,Maria Vakalopoulou,Stergios Christodoulidis,Enzo Ferrante*

Main category: cs.CV

TL;DR: VLMs rely too much on text when analyzing medical images and reports, ignoring visual cues. A new method, SMS, was used to expose this bias.


<details>
  <summary>Details</summary>
Motivation: Clinical decision-making relies on integrated analysis of medical images and clinical reports, but VLMs can exhibit biases toward one modality, frequently overlooking critical visual cues.

Method: Introduced Selective Modality Shifting (SMS), a perturbation-based approach to quantify a model's reliance on each modality by systematically swapping images or text between samples with opposing labels.

Result: Revealed a marked dependency on text input in six open-source VLMs, which persists despite the presence of complementary visual information, confirmed by qualitative attention-based analysis.

Conclusion: VLMs exhibit a strong dependency on text input, often overshadowing visual information, highlighting the need for models that genuinely integrate both modalities.

Abstract: Clinical decision-making relies on the integrated analysis of medical images
and the associated clinical reports. While Vision-Language Models (VLMs) can
offer a unified framework for such tasks, they can exhibit strong biases toward
one modality, frequently overlooking critical visual cues in favor of textual
information. In this work, we introduce Selective Modality Shifting (SMS), a
perturbation-based approach to quantify a model's reliance on each modality in
binary classification tasks. By systematically swapping images or text between
samples with opposing labels, we expose modality-specific biases. We assess six
open-source VLMs-four generalist models and two fine-tuned for medical data-on
two medical imaging datasets with distinct modalities: MIMIC-CXR (chest X-ray)
and FairVLMed (scanning laser ophthalmoscopy). By assessing model performance
and the calibration of every model in both unperturbed and perturbed settings,
we reveal a marked dependency on text input, which persists despite the
presence of complementary visual information. We also perform a qualitative
attention-based analysis which further confirms that image content is often
overshadowed by text details. Our findings highlight the importance of
designing and evaluating multimodal medical models that genuinely integrate
visual and textual cues, rather than relying on single-modality signals.

</details>


### [56] [Graph Lineages and Skeletal Graph Products](https://arxiv.org/abs/2508.00197)
*Eric Mjolsness,Cory B. Scott*

Main category: cs.CV

TL;DR: We define structured graph lineages that grow in a hierarchical fashion, and build an algebraic type theory for graded graphs and (hierarchical) graph lineages. The approach is expected to be well suited to defining hierarchical model architectures and local sampling, search, or optimization algorithms on them.


<details>
  <summary>Details</summary>
Motivation: Graphs, and sequences of growing graphs, can be used to specify the architecture of mathematical models in many fields including machine learning and computational science.

Method: define structured graph lineages that grow in a hierarchical fashion

Result: low-cost skeletal variants of standard algebraic graph operations and type constructors can be derived for graded graphs and hence hierarchical graph lineages; skeletal binary operators have similar but not identical algebraic and category-theoretic properties to their standard counterparts; graph lineages and their skeletal product constructors can approach continuum limit objects; space-efficient unary operators on graded graphs are also derived: thickening, which creates a graph lineage of multiscale graphs, and escalation to a graph lineage of search frontiers

Conclusion: algebraic type theory for graded graphs and (hierarchical) graph lineages is well suited to defining hierarchical model architectures and local sampling, search, or optimization algorithms on them

Abstract: Graphs, and sequences of growing graphs, can be used to specify the
architecture of mathematical models in many fields including machine learning
and computational science. Here we define structured graph "lineages" (ordered
by level number) that grow in a hierarchical fashion, so that: (1) the number
of graph vertices and edges increases exponentially in level number; (2)
bipartite graphs connect successive levels within a graph lineage and, as in
multigrid methods, can constrain matrices relating successive levels; (3) using
prolongation maps within a graph lineage, process-derived distance measures
between graphs at successive levels can be defined; (4) a category of "graded
graphs" can be defined, and using it low-cost "skeletal" variants of standard
algebraic graph operations and type constructors (cross product, box product,
disjoint sum, and function types) can be derived for graded graphs and hence
hierarchical graph lineages; (5) these skeletal binary operators have similar
but not identical algebraic and category-theoretic properties to their standard
counterparts; (6) graph lineages and their skeletal product constructors can
approach continuum limit objects. Additional space-efficient unary operators on
graded graphs are also derived: thickening, which creates a graph lineage of
multiscale graphs, and escalation to a graph lineage of search frontiers
(useful as a generalization of adaptive grids and in defining "skeletal"
functions). The result is an algebraic type theory for graded graphs and
(hierarchical) graph lineages. The approach is expected to be well suited to
defining hierarchical model architectures - "hierarchitectures" - and local
sampling, search, or optimization algorithms on them. We demonstrate such
application to deep neural networks (including visual and feature scale spaces)
and to multigrid numerical methods.

</details>


### [57] [Learning Personalised Human Internal Cognition from External Expressive Behaviours for Real Personality Recognition](https://arxiv.org/abs/2508.00205)
*Xiangyu Kong,Hengde Zhu,Haoqin Sun,Zhihao Guo,Jiayan Gu,Xinyi Ni,Wei Zhang,Shizhe Liu,Siyang Song*

Main category: cs.CV

TL;DR: 提出了一种新颖的RPR方法，该方法通过模拟个性化的内部认知来识别人格特质。


<details>
  <summary>Details</summary>
Motivation: 大多数现有解决方案通常充当外部观察者，以根据目标个体的表达行为来推断观察者的人格印象，这与他们的真实人格显着偏离，并始终导致较差的识别性能。

Method: 提出了一种新颖的RPR方法，该方法可以有效地从目标个体表达的易于访问的外部视听行为中模拟个性化的内部认知。

Result: 模拟的个性化认知，表示为一组网络权重，这些权重强制个性化网络重现特定于个人的面部反应，并进一步编码为包含二维节点和边缘特征矩阵的新颖图。

Conclusion: 使用2D图神经网络（2D-GNN）从模拟的个性化认知中推断真实的人格特质。

Abstract: Automatic real personality recognition (RPR) aims to evaluate human real
personality traits from their expressive behaviours. However, most existing
solutions generally act as external observers to infer observers' personality
impressions based on target individuals' expressive behaviours, which
significantly deviate from their real personalities and consistently lead to
inferior recognition performance. Inspired by the association between real
personality and human internal cognition underlying the generation of
expressive behaviours, we propose a novel RPR approach that efficiently
simulates personalised internal cognition from easy-accessible external short
audio-visual behaviours expressed by the target individual. The simulated
personalised cognition, represented as a set of network weights that enforce
the personalised network to reproduce the individual-specific facial reactions,
is further encoded as a novel graph containing two-dimensional node and edge
feature matrices, with a novel 2D Graph Neural Network (2D-GNN) proposed for
inferring real personality traits from it. To simulate real personality-related
cognition, an end-to-end strategy is designed to jointly train our cognition
simulation, 2D graph construction, and personality recognition modules.

</details>


### [58] [SAM-PTx: Text-Guided Fine-Tuning of SAM with Parameter-Efficient, Parallel-Text Adapters](https://arxiv.org/abs/2508.00213)
*Shayan Jalilian,Abdul Bais*

Main category: cs.CV

TL;DR: SAM-PTx 是一种参数高效的方法，用于使用冻结的 CLIP 派生文本嵌入作为类级语义指导来调整 SAM。


<details>
  <summary>Details</summary>
Motivation: 与传统的空间提示（如点和框）相比，语义文本提示的潜力仍未得到充分探索。

Method: 一种名为 Parallel-Text 的轻量级适配器设计，可将文本嵌入注入到 SAM 的图像编码器中，从而实现语义引导的分割，同时保持原始架构的大部分冻结。

Result: 在 COD10K 数据集以及 COCO 和 ADE20K 的低数据子集上的监督实验和消融研究表明，将固定文本嵌入作为输入可以提高分割性能。

Conclusion: 将语义条件集成到 SAM 的架构中，可以为高效自适应提供实用且可扩展的路径，且计算复杂度最低。

Abstract: The Segment Anything Model (SAM) has demonstrated impressive generalization
in prompt-based segmentation. Yet, the potential of semantic text prompts
remains underexplored compared to traditional spatial prompts like points and
boxes. This paper introduces SAM-PTx, a parameter-efficient approach for
adapting SAM using frozen CLIP-derived text embeddings as class-level semantic
guidance. Specifically, we propose a lightweight adapter design called
Parallel-Text that injects text embeddings into SAM's image encoder, enabling
semantics-guided segmentation while keeping most of the original architecture
frozen. Our adapter modifies only the MLP-parallel branch of each transformer
block, preserving the attention pathway for spatial reasoning. Through
supervised experiments and ablations on the COD10K dataset as well as low-data
subsets of COCO and ADE20K, we show that incorporating fixed text embeddings as
input improves segmentation performance over purely spatial prompt baselines.
To our knowledge, this is the first work to use text prompts for segmentation
on the COD10K dataset. These results suggest that integrating semantic
conditioning into SAM's architecture offers a practical and scalable path for
efficient adaptation with minimal computational complexity.

</details>


### [59] [Object-Centric Cropping for Visual Few-Shot Classification](https://arxiv.org/abs/2508.00218)
*Aymane Abdali,Bartosz Boguslawski,Lucas Drumetz,Vincent Gripon*

Main category: cs.CV

TL;DR: Using object position information improves few-shot image classification, achievable with minimal user input or unsupervised methods.


<details>
  <summary>Details</summary>
Motivation: the presence of image ambiguities stemming from multiple objects or complex backgrounds can significantly deteriorate performance in Few-Shot Image Classification

Method: incorporating additional information about the local positioning of an object within its image

Result: incorporating additional information about the local positioning of an object within its image markedly enhances classification across established benchmarks

Conclusion:  incorporating additional information about the local positioning of an object within its image markedly enhances classification across established benchmarks. A significant fraction of the improvement can be achieved through the use of the Segment Anything Model, requiring only a pixel of the object of interest to be pointed out, or by employing fully unsupervised foreground object extraction methods.

Abstract: In the domain of Few-Shot Image Classification, operating with as little as
one example per class, the presence of image ambiguities stemming from multiple
objects or complex backgrounds can significantly deteriorate performance. Our
research demonstrates that incorporating additional information about the local
positioning of an object within its image markedly enhances classification
across established benchmarks. More importantly, we show that a significant
fraction of the improvement can be achieved through the use of the Segment
Anything Model, requiring only a pixel of the object of interest to be pointed
out, or by employing fully unsupervised foreground object extraction methods.

</details>


### [60] [Guided Depth Map Super-Resolution via Multi-Scale Fusion U-shaped Mamba Network](https://arxiv.org/abs/2508.00248)
*Chenggang Guo,Hao Xu,XianMing Wan*

Main category: cs.CV

TL;DR: 提出了一种新的引导深度图超分辨率框架，命名为多尺度融合 U 型 Mamba (MSF-UM)，该框架集成了 Mamba 的高效状态空间建模能力到多尺度 U 型融合结构中。


<details>
  <summary>Details</summary>
Motivation: 传统的卷积神经网络在处理长距离依赖关系方面存在局限性，无法充分建模深度图中的全局上下文信息。虽然 transformer 可以建模全局依赖关系，但其计算复杂度和内存消耗是二次方的，这极大地限制了其处理高分辨率深度图的能力。

Method: 我们提出了一个多尺度融合 U 型 Mamba (MSF-UM) 模型，这是一个新颖的引导深度图超分辨率框架。该模型的核心创新是将 Mamba 的高效状态空间建模能力集成到由彩色图像引导的多尺度 U 型融合结构中。

Result: 与现有的主流方法相比，所提出的 MSF-UM 在显着减少模型参数数量的同时，实现了更好的重建精度。

Conclusion: 该模型在多个公开数据集上进行了广泛的实验，验证了该模型的有效性，尤其是在大规模深度图超分辨率任务中表现出优异的泛化能力。

Abstract: Depth map super-resolution technology aims to improve the spatial resolution
of low-resolution depth maps and effectively restore high-frequency detail
information. Traditional convolutional neural network has limitations in
dealing with long-range dependencies and are unable to fully model the global
contextual information in depth maps. Although transformer can model global
dependencies, its computational complexity and memory consumption are
quadratic, which significantly limits its ability to process high-resolution
depth maps. In this paper, we propose a multi-scale fusion U-shaped Mamba
(MSF-UM) model, a novel guided depth map super-resolution framework. The core
innovation of this model is to integrate Mamba's efficient state-space modeling
capabilities into a multi-scale U-shaped fusion structure guided by a color
image. The structure combining the residual dense channel attention block and
the Mamba state space module is designed, which combines the local feature
extraction capability of the convolutional layer with the modeling advantage of
the state space model for long-distance dependencies. At the same time, the
model adopts a multi-scale cross-modal fusion strategy to make full use of the
high-frequency texture information from the color image to guide the
super-resolution process of the depth map. Compared with existing mainstream
methods, the proposed MSF-UM significantly reduces the number of model
parameters while achieving better reconstruction accuracy. Extensive
experiments on multiple publicly available datasets validate the effectiveness
of the model, especially showing excellent generalization ability in the task
of large-scale depth map super-resolution.

</details>


### [61] [PointGauss: Point Cloud-Guided Multi-Object Segmentation for Gaussian Splatting](https://arxiv.org/abs/2508.00259)
*Wentao Sun,Hanqing Xu,Quanyun Wu,Dedong Zhang,Yiping Chen,Lingfei Ma,John S. Zelek,Jonathan Li*

Main category: cs.CV

TL;DR: PointGauss: a point cloud-guided framework for real-time multi-object segmentation in Gaussian Splatting representations


<details>
  <summary>Details</summary>
Motivation: existing methods that suffer from prolonged initialization and limited multi-view consistency

Method: point cloud-based Gaussian primitive decoder and a GPU-accelerated 2D mask rendering system

Result: achieves efficient 3D segmentation by directly parsing Gaussian primitives through a point cloud segmentation-driven pipeline

Conclusion: achieves performance gains of 1.89 to 31.78% in multi-view mIoU, while maintaining superior computational efficiency

Abstract: We introduce PointGauss, a novel point cloud-guided framework for real-time
multi-object segmentation in Gaussian Splatting representations. Unlike
existing methods that suffer from prolonged initialization and limited
multi-view consistency, our approach achieves efficient 3D segmentation by
directly parsing Gaussian primitives through a point cloud segmentation-driven
pipeline. The key innovation lies in two aspects: (1) a point cloud-based
Gaussian primitive decoder that generates 3D instance masks within 1 minute,
and (2) a GPU-accelerated 2D mask rendering system that ensures multi-view
consistency. Extensive experiments demonstrate significant improvements over
previous state-of-the-art methods, achieving performance gains of 1.89 to
31.78% in multi-view mIoU, while maintaining superior computational efficiency.
To address the limitations of current benchmarks (single-object focus,
inconsistent 3D evaluation, small scale, and partial coverage), we present
DesktopObjects-360, a novel comprehensive dataset for 3D segmentation in
radiance fields, featuring: (1) complex multi-object scenes, (2) globally
consistent 2D annotations, (3) large-scale training data (over 27 thousand 2D
masks), (4) full 360{\deg} coverage, and (5) 3D evaluation masks.

</details>


### [62] [Instruction-Grounded Visual Projectors for Continual Learning of Generative Vision-Language Models](https://arxiv.org/abs/2508.00260)
*Hyundong Jin,Hyung Jin Chang,Eunwoo Kim*

Main category: cs.CV

TL;DR: 提出了一种新的持续学习框架，该框架通过混合视觉投影仪并结合专家推荐和剪枝策略，来解决视觉语言模型中语言指令被忽视的问题。


<details>
  <summary>Details</summary>
Motivation: 最近的方法更新了一个视觉投影仪来翻译视觉信息以用于新任务，将预训练的视觉编码器与大型语言模型连接起来。然而，这种调整可能会导致模型优先考虑视觉输入而不是语言指令，特别是学习具有重复类型的文本指令的任务。

Method: 提出了一种新的框架，该框架将视觉信息的翻译建立在语言模型的指令上。我们引入了一种视觉投影仪的混合，每个投影仪都充当一个专门的视觉到语言的翻译专家，基于给定的指令上下文来适应新的任务。为了避免将专家用于不相关的指令上下文，我们提出了一种专家推荐策略，该策略将专家重新用于与先前学习的任务类似的任务。此外，我们引入了专家剪枝，以减轻先前任务中累积激活的专家所造成的干扰。

Result: 在各种视觉语言任务上的大量实验表明，我们的方法优于现有的持续学习方法。

Conclusion: 该方法通过生成遵循指令的响应，优于现有的持续学习方法。

Abstract: Continual learning enables pre-trained generative vision-language models
(VLMs) to incorporate knowledge from new tasks without retraining data from
previous ones. Recent methods update a visual projector to translate visual
information for new tasks, connecting pre-trained vision encoders with large
language models. However, such adjustments may cause the models to prioritize
visual inputs over language instructions, particularly learning tasks with
repetitive types of textual instructions. To address the neglect of language
instructions, we propose a novel framework that grounds the translation of
visual information on instructions for language models. We introduce a mixture
of visual projectors, each serving as a specialized visual-to-language
translation expert based on the given instruction context to adapt to new
tasks. To avoid using experts for irrelevant instruction contexts, we propose
an expert recommendation strategy that reuses experts for tasks similar to
those previously learned. Additionally, we introduce expert pruning to
alleviate interference from the use of experts that cumulatively activated in
previous tasks. Extensive experiments on diverse vision-language tasks
demonstrate that our method outperforms existing continual learning approaches
by generating instruction-following responses.

</details>


### [63] [Multimodal Referring Segmentation: A Survey](https://arxiv.org/abs/2508.00265)
*Henghui Ding,Song Tang,Shuting He,Chang Liu,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: A comprehensive survey of multimodal referring segmentation, covering methods, applications, and performance benchmarks. Related works are tracked at https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation.


<details>
  <summary>Details</summary>
Motivation: Multimodal referring segmentation is crucial for accurate object perception based on user instructions and has gained significant attention due to advances in convolutional neural networks, transformers, and large language models.

Method: The paper summarizes a unified meta architecture for referring segmentation and reviews representative methods across three primary visual scenes, including images, videos, and 3D scenes. It further discusses Generalized Referring Expression (GREx) methods.

Result: Extensive performance comparisons on standard benchmarks are provided.

Conclusion: This paper provides a comprehensive survey of multimodal referring segmentation, including background, problem definitions, datasets, a unified meta architecture, representative methods, GREx methods, related tasks, practical applications, and performance comparisons. The paper also tracks related works at https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation.

Abstract: Multimodal referring segmentation aims to segment target objects in visual
scenes, such as images, videos, and 3D scenes, based on referring expressions
in text or audio format. This task plays a crucial role in practical
applications requiring accurate object perception based on user instructions.
Over the past decade, it has gained significant attention in the multimodal
community, driven by advances in convolutional neural networks, transformers,
and large language models, all of which have substantially improved multimodal
perception capabilities. This paper provides a comprehensive survey of
multimodal referring segmentation. We begin by introducing this field's
background, including problem definitions and commonly used datasets. Next, we
summarize a unified meta architecture for referring segmentation and review
representative methods across three primary visual scenes, including images,
videos, and 3D scenes. We further discuss Generalized Referring Expression
(GREx) methods to address the challenges of real-world complexity, along with
related tasks and practical applications. Extensive performance comparisons on
standard benchmarks are also provided. We continually track related works at
https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation.

</details>


### [64] [Towards Robust Semantic Correspondence: A Benchmark and Insights](https://arxiv.org/abs/2508.00272)
*Wenyue Chong*

Main category: cs.CV

TL;DR: 建立了一个新的基准来评估恶劣条件下的语义对应，发现现有方法在恶劣条件下性能下降明显，需要特定于任务的设计来提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 语义对应旨在识别不同图像之间语义上有意义的关系，是计算机视觉中的一个根本挑战。它是 3D 重建、目标跟踪和图像编辑等众多任务的基础。随着大规模视觉模型的进步，语义对应在受控和高质量的条件下取得了显著的性能。然而，语义对应在具有挑战性的场景中的鲁棒性研究较少。

Method: 建立了一个新的基准，用于评估恶劣条件下的语义对应，包含 14 种不同的挑战场景，反映了常见的图像问题，包括几何畸变、图像模糊、数字伪影和环境遮挡。

Result: 对语义对应方法的鲁棒性进行评估，发现所有现有方法在恶劣条件下性能都会显著下降；使用大型视觉模型可以提高整体鲁棒性，但对这些模型进行微调会导致相对鲁棒性下降；DINO 模型在相对鲁棒性方面优于 Stable Diffusion，它们的融合实现了更好的绝对鲁棒性；通用数据增强对于语义对应的鲁棒性增强无效，需要特定于任务的设计。这些结果在我们自己构建的数据集和真实世界的基准测试中是一致的。

Conclusion: 现有的语义对应方法在恶劣条件下性能下降明显；使用大型视觉模型可以提高整体鲁棒性，但对这些模型进行微调会导致相对鲁棒性下降；DINO 模型在相对鲁棒性方面优于 Stable Diffusion，它们的融合实现了更好的绝对鲁棒性；常见的鲁棒性增强策略（如通用数据增强）效果不佳，需要特定于任务的设计。

Abstract: Semantic correspondence aims to identify semantically meaningful
relationships between different images and is a fundamental challenge in
computer vision. It forms the foundation for numerous tasks such as 3D
reconstruction, object tracking, and image editing. With the progress of
large-scale vision models, semantic correspondence has achieved remarkable
performance in controlled and high-quality conditions. However, the robustness
of semantic correspondence in challenging scenarios is much less investigated.
In this work, we establish a novel benchmark for evaluating semantic
correspondence in adverse conditions. The benchmark dataset comprises 14
distinct challenging scenarios that reflect commonly encountered imaging
issues, including geometric distortion, image blurring, digital artifacts, and
environmental occlusion. Through extensive evaluations, we provide several key
insights into the robustness of semantic correspondence approaches: (1) All
existing methods suffer from noticeable performance drops under adverse
conditions; (2) Using large-scale vision models can enhance overall robustness,
but fine-tuning on these models leads to a decline in relative robustness; (3)
The DINO model outperforms the Stable Diffusion in relative robustness, and
their fusion achieves better absolute robustness; Moreover, We evaluate common
robustness enhancement strategies for semantic correspondence and find that
general data augmentations are ineffective, highlighting the need for
task-specific designs. These results are consistent across both our dataset and
real-world benchmarks.

</details>


### [65] [Privacy-Preserving Driver Drowsiness Detection with Spatial Self-Attention and Federated Learning](https://arxiv.org/abs/2508.00287)
*Tran Viet Khoa,Do Hai Son,Mohammad Abu Alsheikh,Yibeltal F Alem,Dinh Thai Hoang*

Main category: cs.CV

TL;DR: 提出了一种新的驾驶员睡意检测框架，该框架使用联邦学习和空间自注意力机制，在异构和分散的数据中实现了 89.9% 的检测精度。


<details>
  <summary>Details</summary>
Motivation: 驾驶员疲劳是道路交通事故的主要原因之一，并且被认为是导致与交通相关的死亡事故的主要因素。然而，准确地检测睡意仍然是一项具有挑战性的任务，尤其是在来自不同个体的面部数据分散且高度多样化的现实环境中。

Method: 开发了一种新的空间自注意力 (SSA) 机制，该机制与长短期记忆 (LSTM) 网络集成，以更好地提取关键面部特征并提高检测性能。为了支持联邦学习，我们采用梯度相似性比较 (GSC)，该比较在聚合之前从不同的运营商处选择最相关的训练模型。

Result: 该框架在联邦学习设置中实现了 89.9% 的检测精度，优于各种部署场景下的现有方法。

Conclusion: 该框架在联邦学习设置中实现了 89.9% 的检测精度，优于各种部署场景下的现有方法。结果表明，该方法在处理真实世界数据可变性方面的有效性，并突出了其在智能交通系统中部署的潜力，通过早期和可靠的困倦检测来提高道路安全性。

Abstract: Driver drowsiness is one of the main causes of road accidents and is
recognized as a leading contributor to traffic-related fatalities. However,
detecting drowsiness accurately remains a challenging task, especially in
real-world settings where facial data from different individuals is
decentralized and highly diverse. In this paper, we propose a novel framework
for drowsiness detection that is designed to work effectively with
heterogeneous and decentralized data. Our approach develops a new Spatial
Self-Attention (SSA) mechanism integrated with a Long Short-Term Memory (LSTM)
network to better extract key facial features and improve detection
performance. To support federated learning, we employ a Gradient Similarity
Comparison (GSC) that selects the most relevant trained models from different
operators before aggregation. This improves the accuracy and robustness of the
global model while preserving user privacy. We also develop a customized tool
that automatically processes video data by extracting frames, detecting and
cropping faces, and applying data augmentation techniques such as rotation,
flipping, brightness adjustment, and zooming. Experimental results show that
our framework achieves a detection accuracy of 89.9% in the federated learning
settings, outperforming existing methods under various deployment scenarios.
The results demonstrate the effectiveness of our approach in handling
real-world data variability and highlight its potential for deployment in
intelligent transportation systems to enhance road safety through early and
reliable drowsiness detection.

</details>


### [66] [TITAN-Guide: Taming Inference-Time AligNment for Guided Text-to-Video Diffusion Models](https://arxiv.org/abs/2508.00289)
*Christian Simon,Masato Ishii,Akio Hayakawa,Zhi Zhong,Shusuke Takahashi,Takashi Shibuya,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: TITAN-Guide is a memory-efficient and control-optimized training-free guidance method for Text-to-Video diffusion models, outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing training-free guidance frameworks for conditional diffusion models suffer from heavy memory requirements or sub-optimal control, limiting their applicability to computationally intensive tasks like Text-to-Video (T2V) diffusion models.

Method: The method involves optimizing diffusion latents using forward gradient descents, guided by directional directives, without requiring backpropagation from a discriminative guiding model.

Result: The proposed approach minimizes memory requirements and significantly enhances T2V performance across a range of diffusion guidance benchmarks.

Conclusion: This work introduces TITAN-Guide, a novel training-free guidance framework for Text-to-Video diffusion models that addresses the limitations of existing methods by reducing memory requirements and improving control through efficient latent optimization without backpropagation.

Abstract: In the recent development of conditional diffusion models still require heavy
supervised fine-tuning for performing control on a category of tasks.
Training-free conditioning via guidance with off-the-shelf models is a
favorable alternative to avoid further fine-tuning on the base model. However,
the existing training-free guidance frameworks either have heavy memory
requirements or offer sub-optimal control due to rough estimation. These
shortcomings limit the applicability to control diffusion models that require
intense computation, such as Text-to-Video (T2V) diffusion models. In this
work, we propose Taming Inference Time Alignment for Guided Text-to-Video
Diffusion Model, so-called TITAN-Guide, which overcomes memory space issues,
and provides more optimal control in the guidance process compared to the
counterparts. In particular, we develop an efficient method for optimizing
diffusion latents without backpropagation from a discriminative guiding model.
In particular, we study forward gradient descents for guided diffusion tasks
with various options on directional directives. In our experiments, we
demonstrate the effectiveness of our approach in efficiently managing memory
during latent optimization, while previous methods fall short. Our proposed
approach not only minimizes memory requirements but also significantly enhances
T2V performance across a range of diffusion guidance benchmarks. Code, models,
and demo are available at https://titanguide.github.io.

</details>


### [67] [AniMer+: Unified Pose and Shape Estimation Across Mammalia and Aves via Family-Aware Transformer](https://arxiv.org/abs/2508.00298)
*Jin Lyu,Liang An,Li Lin,Pujin Cheng,Yebin Liu,Xiaoying Tang*

Main category: cs.CV

TL;DR: AniMer+ is introduced, which uses a family-aware ViT and synthetic data generation to improve animal pose and shape estimation across species.


<details>
  <summary>Details</summary>
Motivation: Accurate estimation of animal pose and shape across diverse species is essential for quantitative analysis in biological research. However, this topic remains underexplored due to the limited network capacity of previous methods and the scarcity of comprehensive multi-species datasets.

Method: A high-capacity, family-aware Vision Transformer (ViT) incorporating a Mixture-of-Experts (MoE) design is proposed. The architecture partitions network layers into taxa-specific components (for mammalia and aves) and taxa-shared components. A diffusion-based conditional image generation pipeline is introduced to produce two large-scale synthetic datasets: CtrlAni3D for quadrupeds and CtrlAVES3D for birds.

Result: The method was trained on an aggregated collection of 41.3k mammalian and 12.4k avian images (combining real and synthetic data).

Conclusion: The proposed method demonstrates superior performance over existing approaches across a wide range of benchmarks, including the challenging out-of-domain Animal Kingdom dataset. Ablation studies confirm the effectiveness of both the novel network architecture and the generated synthetic datasets in enhancing real-world application performance.

Abstract: In the era of foundation models, achieving a unified understanding of
different dynamic objects through a single network has the potential to empower
stronger spatial intelligence. Moreover, accurate estimation of animal pose and
shape across diverse species is essential for quantitative analysis in
biological research. However, this topic remains underexplored due to the
limited network capacity of previous methods and the scarcity of comprehensive
multi-species datasets. To address these limitations, we introduce AniMer+, an
extended version of our scalable AniMer framework. In this paper, we focus on a
unified approach for reconstructing mammals (mammalia) and birds (aves). A key
innovation of AniMer+ is its high-capacity, family-aware Vision Transformer
(ViT) incorporating a Mixture-of-Experts (MoE) design. Its architecture
partitions network layers into taxa-specific components (for mammalia and aves)
and taxa-shared components, enabling efficient learning of both distinct and
common anatomical features within a single model. To overcome the critical
shortage of 3D training data, especially for birds, we introduce a
diffusion-based conditional image generation pipeline. This pipeline produces
two large-scale synthetic datasets: CtrlAni3D for quadrupeds and CtrlAVES3D for
birds. To note, CtrlAVES3D is the first large-scale, 3D-annotated dataset for
birds, which is crucial for resolving single-view depth ambiguities. Trained on
an aggregated collection of 41.3k mammalian and 12.4k avian images (combining
real and synthetic data), our method demonstrates superior performance over
existing approaches across a wide range of benchmarks, including the
challenging out-of-domain Animal Kingdom dataset. Ablation studies confirm the
effectiveness of both our novel network architecture and the generated
synthetic datasets in enhancing real-world application performance.

</details>


### [68] [Controllable Pedestrian Video Editing for Multi-View Driving Scenarios via Motion Sequence](https://arxiv.org/abs/2508.00299)
*Danzhen Fu,Jiagao Hu,Daiguo Zhou,Fei Wang,Zepeng Wang,Wenhua Liao*

Main category: cs.CV

TL;DR: A novel framework for controllable pedestrian video editing in multi-view driving scenarios is presented to address the lack of robustness in pedestrian detection models.


<details>
  <summary>Details</summary>
Motivation: Pedestrian detection models in autonomous driving systems often lack robustness due to insufficient representation of dangerous pedestrian scenarios in training datasets.

Method: integrating video inpainting and human motion control techniques

Result: achieves high-quality pedestrian editing with strong visual realism, spatiotemporal coherence, and cross-view consistency.

Conclusion: This method is a robust and versatile solution for multi-view pedestrian video generation, with broad potential for applications in data augmentation and scenario simulation in autonomous driving.

Abstract: Pedestrian detection models in autonomous driving systems often lack
robustness due to insufficient representation of dangerous pedestrian scenarios
in training datasets. To address this limitation, we present a novel framework
for controllable pedestrian video editing in multi-view driving scenarios by
integrating video inpainting and human motion control techniques. Our approach
begins by identifying pedestrian regions of interest across multiple camera
views, expanding detection bounding boxes with a fixed ratio, and resizing and
stitching these regions into a unified canvas while preserving cross-view
spatial relationships. A binary mask is then applied to designate the editable
area, within which pedestrian editing is guided by pose sequence control
conditions. This enables flexible editing functionalities, including pedestrian
insertion, replacement, and removal. Extensive experiments demonstrate that our
framework achieves high-quality pedestrian editing with strong visual realism,
spatiotemporal coherence, and cross-view consistency. These results establish
the proposed method as a robust and versatile solution for multi-view
pedestrian video generation, with broad potential for applications in data
augmentation and scenario simulation in autonomous driving.

</details>


### [69] [Exploring Fourier Prior and Event Collaboration for Low-Light Image Enhancement](https://arxiv.org/abs/2508.00308)
*Chunyan She,Fujun Han,Chengyu Fang,Shukai Duan,Lidan Wang*

Main category: cs.CV

TL;DR: 提出了一种新的低光图像增强方法，该方法利用事件相机的高动态范围和低延迟特性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于事件的方法直接将帧和事件输入到单个模型中，而没有充分利用特定模态的优势，这限制了它们的性能。

Method: 该增强流程分为两个阶段：可见性恢复和结构细化。第一阶段，通过重新思考傅里叶空间中幅度和相位分量之间的关系，设计了一个具有幅度-相位纠缠的可见性恢复网络。在第二阶段，提出了一种具有动态对齐的融合策略，以减轻由两个传感模态之间的时间分辨率差异引起的空间失配，旨在细化由可见性恢复网络增强的图像的结构信息。此外，我们利用空间-频率插值来模拟具有不同光照、噪声和伪影退化的负样本，从而开发了一种对比损失，以鼓励模型学习区分性表示。

Result: 所提出的方法优于现有技术模型。

Conclusion: 该方法优于现有技术模型。

Abstract: The event camera, benefiting from its high dynamic range and low latency,
provides performance gain for low-light image enhancement. Unlike frame-based
cameras, it records intensity changes with extremely high temporal resolution,
capturing sufficient structure information. Currently, existing event-based
methods feed a frame and events directly into a single model without fully
exploiting modality-specific advantages, which limits their performance.
Therefore, by analyzing the role of each sensing modality, the enhancement
pipeline is decoupled into two stages: visibility restoration and structure
refinement. In the first stage, we design a visibility restoration network with
amplitude-phase entanglement by rethinking the relationship between amplitude
and phase components in Fourier space. In the second stage, a fusion strategy
with dynamic alignment is proposed to mitigate the spatial mismatch caused by
the temporal resolution discrepancy between two sensing modalities, aiming to
refine the structure information of the image enhanced by the visibility
restoration network. In addition, we utilize spatial-frequency interpolation to
simulate negative samples with diverse illumination, noise and artifact
degradations, thereby developing a contrastive loss that encourages the model
to learn discriminative representations. Experiments demonstrate that the
proposed method outperforms state-of-the-art models.

</details>


### [70] [DocTron-Formula: Generalized Formula Recognition in Complex and Structured Scenarios](https://arxiv.org/abs/2508.00311)
*Yufeng Zhong,Zhixiong Zeng,Lei Chen,Longrong Yang,Liming Zheng,Jing Huang,Siqi Yang,Lin Ma*

Main category: cs.CV

TL;DR: 提出了DocTron-Formula，一个基于通用视觉语言模型的统一框架，以及一个大规模数据集CSFormula，通过监督微调，该方法在数学公式识别方面达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的特定任务和通用视觉语言模型通常难以处理数学内容中固有的结构多样性、复杂性和现实世界的变异性。

Method: DocTron-Formula，一个建立在通用视觉语言模型之上的统一框架

Result: 该方法在各种风格、科学领域和复杂布局上实现了最先进的性能。

Conclusion: 该方法在准确性和鲁棒性方面超越了专门的模型，并为自动理解复杂的科学文档建立了一种新的范例。

Abstract: Optical Character Recognition (OCR) for mathematical formula is essential for
the intelligent analysis of scientific literature. However, both task-specific
and general vision-language models often struggle to handle the structural
diversity, complexity, and real-world variability inherent in mathematical
content. In this work, we present DocTron-Formula, a unified framework built
upon general vision-language models, thereby eliminating the need for
specialized architectures. Furthermore, we introduce CSFormula, a large-scale
and challenging dataset that encompasses multidisciplinary and structurally
complex formulas at the line, paragraph, and page levels. Through
straightforward supervised fine-tuning, our approach achieves state-of-the-art
performance across a variety of styles, scientific domains, and complex
layouts. Experimental results demonstrate that our method not only surpasses
specialized models in terms of accuracy and robustness, but also establishes a
new paradigm for the automated understanding of complex scientific documents.

</details>


### [71] [GV-VAD : Exploring Video Generation for Weakly-Supervised Video Anomaly Detection](https://arxiv.org/abs/2508.00312)
*Suhang Cai,Xiaohao Peng,Chong Wang,Xiaojie Cai,Jiangbo Qian*

Main category: cs.CV

TL;DR: 提出了一种生成视频增强的弱监督视频异常检测(GV-VAD)框架，该框架利用合成视频来增强训练数据，并在UCF-Crime数据集上优于现有技术方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界异常的稀有性、不可预测性和高标注成本使得VAD数据集难以扩展，这限制了现有模型的性能和泛化能力。

Method: 利用文本条件视频生成模型生成语义可控且物理上合理的合成视频，并采用合成样本损失缩放策略来控制生成合成样本的影响。

Result: 生成视频增强的弱监督视频异常检测(GV-VAD)框架，该框架利用文本条件视频生成模型生成语义可控且物理上合理的合成视频。这些虚拟视频以低成本用于扩充训练数据。此外，还采用了一种合成样本损失缩放策略来控制生成的合成样本的影响，以实现高效训练。

Conclusion: 该框架在UCF-Crime数据集上优于现有技术方法。

Abstract: Video anomaly detection (VAD) plays a critical role in public safety
applications such as intelligent surveillance. However, the rarity,
unpredictability, and high annotation cost of real-world anomalies make it
difficult to scale VAD datasets, which limits the performance and
generalization ability of existing models. To address this challenge, we
propose a generative video-enhanced weakly-supervised video anomaly detection
(GV-VAD) framework that leverages text-conditioned video generation models to
produce semantically controllable and physically plausible synthetic videos.
These virtual videos are used to augment training data at low cost. In
addition, a synthetic sample loss scaling strategy is utilized to control the
influence of generated synthetic samples for efficient training. The
experiments show that the proposed framework outperforms state-of-the-art
methods on UCF-Crime datasets. The code is available at
https://github.com/Sumutan/GV-VAD.git.

</details>


### [72] [Steering Guidance for Personalized Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.00319)
*Sunghyun Park,Seokeon Choi,Hyoungwoo Park,Sungrack Yun*

Main category: cs.CV

TL;DR: 提出了一种新的个性化指导方法，以提高文本到图像扩散模型的文本对齐和目标分布保真度。


<details>
  <summary>Details</summary>
Motivation: 使用少量图像进行微调会在与目标分布对齐（例如，主体保真度）和保留原始模型的广泛知识（例如，文本可编辑性）之间引入固有的权衡。现有的采样指导方法，例如无分类器指导 (CFG) 和autoguidance (AG)，无法有效地将输出引导到平衡良好的空间：CFG 限制了对目标分布的适应，而 AG 损害了文本对齐。

Method: 提出 personalization guidance

Result: 提出的指导可以提高文本对齐和目标分布保真度，并与各种微调策略无缝集成。

Conclusion: 提出了 personalization guidance，一种简单有效的方法，利用以空文本提示为条件的未学习的弱模型。通过在推理过程中预训练模型和微调模型之间的权重插值，动态控制弱模型中未学习的程度。实验结果表明，该方法可以提高文本对齐和目标分布保真度，并与各种微调策略无缝集成。

Abstract: Personalizing text-to-image diffusion models is crucial for adapting the
pre-trained models to specific target concepts, enabling diverse image
generation. However, fine-tuning with few images introduces an inherent
trade-off between aligning with the target distribution (e.g., subject
fidelity) and preserving the broad knowledge of the original model (e.g., text
editability). Existing sampling guidance methods, such as classifier-free
guidance (CFG) and autoguidance (AG), fail to effectively guide the output
toward well-balanced space: CFG restricts the adaptation to the target
distribution, while AG compromises text alignment. To address these
limitations, we propose personalization guidance, a simple yet effective method
leveraging an unlearned weak model conditioned on a null text prompt. Moreover,
our method dynamically controls the extent of unlearning in a weak model
through weight interpolation between pre-trained and fine-tuned models during
inference. Unlike existing guidance methods, which depend solely on guidance
scales, our method explicitly steers the outputs toward a balanced latent space
without additional computational overhead. Experimental results demonstrate
that our proposed guidance can improve text alignment and target distribution
fidelity, integrating seamlessly with various fine-tuning strategies.

</details>


### [73] [Spectral Sensitivity Estimation with an Uncalibrated Diffraction Grating](https://arxiv.org/abs/2508.00330)
*Lilika Makabe,Hiroaki Santo,Fumio Okura,Michael S. Brown,Yasuyuki Matsushita*

Main category: cs.CV

TL;DR: 本文介绍了一种实用且精确的相机光谱灵敏度校准方法，该方法仅使用衍射光栅，避免了使用专用窄带滤波器或具有已知光谱反射率的参考目标。


<details>
  <summary>Details</summary>
Motivation: 精确校准相机光谱灵敏度对于各种计算机视觉任务至关重要，包括颜色校正、照明估计和材料分析。

Method: 通过捕获直接照明及其通过光栅片的衍射图案的图像，该方法以闭合形式估计相机光谱灵敏度和光栅参数。

Result: 该方法仅需要一个现成的未校准的衍射光栅片。

Conclusion: 该方法在合成和真实数据上的实验表明，其性能优于传统的基于参考目标的方法，强调了其有效性和实用性。

Abstract: This paper introduces a practical and accurate calibration method for camera
spectral sensitivity using a diffraction grating. Accurate calibration of
camera spectral sensitivity is crucial for various computer vision tasks,
including color correction, illumination estimation, and material analysis.
Unlike existing approaches that require specialized narrow-band filters or
reference targets with known spectral reflectances, our method only requires an
uncalibrated diffraction grating sheet, readily available off-the-shelf. By
capturing images of the direct illumination and its diffracted pattern through
the grating sheet, our method estimates both the camera spectral sensitivity
and the diffraction grating parameters in a closed-form manner. Experiments on
synthetic and real-world data demonstrate that our method outperforms
conventional reference target-based methods, underscoring its effectiveness and
practicality.

</details>


### [74] [Analyze-Prompt-Reason: A Collaborative Agent-Based Framework for Multi-Image Vision-Language Reasoning](https://arxiv.org/abs/2508.00356)
*Angelos Vlachos,Giorgos Filandrianos,Maria Lymperaiou,Nikolaos Spanos,Ilias Mitsouras,Vasileios Karampinis,Athanasios Voulodimos*

Main category: cs.CV

TL;DR: Proposes a Collaborative Agent-Based Framework for Multi-Image Reasoning, achieving near-ceiling performance on challenging tasks.


<details>
  <summary>Details</summary>
Motivation: tackles the challenge of interleaved multimodal reasoning across diverse datasets and task formats

Method: a Collaborative Agent-Based Framework for Multi-Image Reasoning employing a dual-agent system: a language-based PromptEngineer and a VisionReasoner

Result: demonstrate that LVLMs can effectively reason over multiple images when guided by informative prompts. Notably, Claude 3.7 achieves near-ceiling performance on challenging tasks such as TQA (99.13% accuracy), DocVQA (96.87%), and MMCoQA (75.28 ROUGE-L).

Conclusion: LVLMs can effectively reason over multiple images when guided by informative prompts. Notably, Claude 3.7 achieves near-ceiling performance on challenging tasks such as TQA (99.13% accuracy), DocVQA (96.87%), and MMCoQA (75.28 ROUGE-L).

Abstract: We present a Collaborative Agent-Based Framework for Multi-Image Reasoning.
Our approach tackles the challenge of interleaved multimodal reasoning across
diverse datasets and task formats by employing a dual-agent system: a
language-based PromptEngineer, which generates context-aware, task-specific
prompts, and a VisionReasoner, a large vision-language model (LVLM) responsible
for final inference. The framework is fully automated, modular, and
training-free, enabling generalization across classification, question
answering, and free-form generation tasks involving one or multiple input
images. We evaluate our method on 18 diverse datasets from the 2025 MIRAGE
Challenge (Track A), covering a broad spectrum of visual reasoning tasks
including document QA, visual comparison, dialogue-based understanding, and
scene-level inference. Our results demonstrate that LVLMs can effectively
reason over multiple images when guided by informative prompts. Notably, Claude
3.7 achieves near-ceiling performance on challenging tasks such as TQA (99.13%
accuracy), DocVQA (96.87%), and MMCoQA (75.28 ROUGE-L). We also explore how
design choices-such as model selection, shot count, and input length-influence
the reasoning performance of different LVLMs.

</details>


### [75] [Stable at Any Speed: Speed-Driven Multi-Object Tracking with Learnable Kalman Filtering](https://arxiv.org/abs/2508.00358)
*Yan Gong,Mengjun Chen,Hao Liu,Gao Yongsheng,Lei Yang,Naibang Wang,Ziying Song,Haoqun Ma*

Main category: cs.CV

TL;DR: This paper introduces Speed-Guided Learnable Kalman Filter (SG-LKF) to improve multi-object tracking (MOT) stability and accuracy in dynamic, high-speed scenarios. Extensive experiments show that SG-LKF ranks first among all vision-based methods on KITTI 2D MOT with 79.59% HOTA, delivers strong results on KITTI 3D MOT with 82.03% HOTA, and outperforms SimpleTrack by 2.2% AMOTA on nuScenes 3D MOT.


<details>
  <summary>Details</summary>
Motivation: conventional tracking-by-detection methods typically rely on static coordinate transformations based on ego-vehicle poses, disregarding ego-vehicle speed-induced variations in observation noise and reference frame changes, which degrades tracking stability and accuracy in dynamic, high-speed scenarios.

Method: propose a Speed-Guided Learnable Kalman Filter (SG-LKF) that dynamically adapts uncertainty modeling to ego-vehicle speed

Result: SG-LKF ranks first among all vision-based methods on KITTI 2D MOT with 79.59% HOTA, delivers strong results on KITTI 3D MOT with 82.03% HOTA, and outperforms SimpleTrack by 2.2% AMOTA on nuScenes 3D MOT.

Conclusion: SG-LKF ranks first among all vision-based methods on KITTI 2D MOT with 79.59% HOTA, delivers strong results on KITTI 3D MOT with 82.03% HOTA, and outperforms SimpleTrack by 2.2% AMOTA on nuScenes 3D MOT.

Abstract: Multi-object tracking (MOT) enables autonomous vehicles to continuously
perceive dynamic objects, supplying essential temporal cues for prediction,
behavior understanding, and safe planning. However, conventional
tracking-by-detection methods typically rely on static coordinate
transformations based on ego-vehicle poses, disregarding ego-vehicle
speed-induced variations in observation noise and reference frame changes,
which degrades tracking stability and accuracy in dynamic, high-speed
scenarios. In this paper, we investigate the critical role of ego-vehicle speed
in MOT and propose a Speed-Guided Learnable Kalman Filter (SG-LKF) that
dynamically adapts uncertainty modeling to ego-vehicle speed, significantly
improving stability and accuracy in highly dynamic scenarios. Central to SG-LKF
is MotionScaleNet (MSNet), a decoupled token-mixing and channel-mixing MLP that
adaptively predicts key parameters of SG-LKF. To enhance inter-frame
association and trajectory continuity, we introduce a self-supervised
trajectory consistency loss jointly optimized with semantic and positional
constraints. Extensive experiments show that SG-LKF ranks first among all
vision-based methods on KITTI 2D MOT with 79.59% HOTA, delivers strong results
on KITTI 3D MOT with 82.03% HOTA, and outperforms SimpleTrack by 2.2% AMOTA on
nuScenes 3D MOT.

</details>


### [76] [CoST: Efficient Collaborative Perception From Unified Spatiotemporal Perspective](https://arxiv.org/abs/2508.00359)
*Zongheng Tang,Yi Liu,Yifan Sun,Yulu Gao,Jinyu Chen,Runsheng Xu,Si Liu*

Main category: cs.CV

TL;DR: This paper introduces CoST, a collaborative perception method using a spatio-temporal transformer, improving efficiency and accuracy by unifying multi-agent and multi-time fusion.


<details>
  <summary>Details</summary>
Motivation: Collaborative perception shares information among different agents and helps solving problems that individual agents may face, e.g., occlusions and small sensing range. Prior methods usually separate the multi-agent fusion and multi-time fusion into two consecutive steps.

Method: This paper proposes an efficient collaborative perception that aggregates the observations from different agents (space) and different times into a unified spatio-temporal space simultaneously.

Result: CoST gains improvement in both efficiency and accuracy.

Conclusion: The proposed Collaborative perception with Spatio-temporal Transformer (CoST) improves both efficiency and accuracy and is compatible with a majority of previous methods.

Abstract: Collaborative perception shares information among different agents and helps
solving problems that individual agents may face, e.g., occlusions and small
sensing range. Prior methods usually separate the multi-agent fusion and
multi-time fusion into two consecutive steps. In contrast, this paper proposes
an efficient collaborative perception that aggregates the observations from
different agents (space) and different times into a unified spatio-temporal
space simultanesouly. The unified spatio-temporal space brings two benefits,
i.e., efficient feature transmission and superior feature fusion. 1) Efficient
feature transmission: each static object yields a single observation in the
spatial temporal space, and thus only requires transmission only once (whereas
prior methods re-transmit all the object features multiple times). 2) superior
feature fusion: merging the multi-agent and multi-time fusion into a unified
spatial-temporal aggregation enables a more holistic perspective, thereby
enhancing perception performance in challenging scenarios. Consequently, our
Collaborative perception with Spatio-temporal Transformer (CoST) gains
improvement in both efficiency and accuracy. Notably, CoST is not tied to any
specific method and is compatible with a majority of previous methods,
enhancing their accuracy while reducing the transmission bandwidth.

</details>


### [77] [Honey Classification using Hyperspectral Imaging and Machine Learning](https://arxiv.org/abs/2508.00361)
*Mokhtar A. Al-Awadhi,Ratnadeep R. Deshmukh*

Main category: cs.CV

TL;DR: 本文提出了一种基于机器学习的蜂蜜植物来源自动分类方法，该方法使用类别转换、线性判别分析以及支持向量机和K最近邻模型，实验结果表明该系统取得了state-of-the-art的结果。


<details>
  <summary>Details</summary>
Motivation: 提出了一种基于机器学习的方法，用于自动分类蜂蜜的植物来源。

Method: 该方法包括数据集准备、特征提取和分类三个主要步骤。在数据集准备阶段，使用类别转换方法来最大化类别之间的可分离性。特征提取阶段采用线性判别分析（LDA）技术来提取相关特征并减少维度。在分类阶段，使用支持向量机（SVM）和K最近邻（KNN）模型。

Result: 实验结果表明，所提出的系统在该数据集上产生了最先进的结果。

Conclusion: 该系统在蜂蜜高光谱图像分类中达到了最高的95.13%准确率，在高光谱实例分类中达到了92.80%的准确率。

Abstract: In this paper, we propose a machine learning-based method for automatically
classifying honey botanical origins. Dataset preparation, feature extraction,
and classification are the three main steps of the proposed method. We use a
class transformation method in the dataset preparation phase to maximize the
separability across classes. The feature extraction phase employs the Linear
Discriminant Analysis (LDA) technique for extracting relevant features and
reducing the number of dimensions. In the classification phase, we use Support
Vector Machines (SVM) and K-Nearest Neighbors (KNN) models to classify the
extracted features of honey samples into their botanical origins. We evaluate
our system using a standard honey hyperspectral imaging (HSI) dataset.
Experimental findings demonstrate that the proposed system produces
state-of-the-art results on this dataset, achieving the highest classification
accuracy of 95.13% for hyperspectral image-based classification and 92.80% for
hyperspectral instance-based classification.

</details>


### [78] [SparseRecon: Neural Implicit Surface Reconstruction from Sparse Views with Feature and Depth Consistencies](https://arxiv.org/abs/2508.00366)
*Liang Han,Xu Zhang,Haichuan Song,Kanle Shi,Yu-Shen Liu,Zhizhong Han*

Main category: cs.CV

TL;DR: 提出了一种新的神经隐式重建方法 SparseRecon，用于稀疏视图，该方法优于现有技术，可以用稀疏视图输入生成高质量的几何体，尤其是在具有小重叠视图的场景中。


<details>
  <summary>Details</summary>
Motivation: 基于泛化的方法在训练期间未见过的视图上泛化效果不佳，而基于过拟合的方法的重建质量仍然受到有限的几何线索的限制。

Method: 提出了一种新的神经隐式重建方法 SparseRecon，用于具有基于体积渲染的特征一致性和不确定性引导的深度约束的稀疏视图。

Result: 实验结果表明，我们的方法优于现有技术，可以用稀疏视图输入生成高质量的几何体，尤其是在具有小重叠视图的场景中。

Conclusion: 该方法优于现有技术，可以用稀疏视图输入生成高质量的几何体，尤其是在具有小重叠视图的场景中。

Abstract: Surface reconstruction from sparse views aims to reconstruct a 3D shape or
scene from few RGB images. The latest methods are either generalization-based
or overfitting-based. However, the generalization-based methods do not
generalize well on views that were unseen during training, while the
reconstruction quality of overfitting-based methods is still limited by the
limited geometry clues. To address this issue, we propose SparseRecon, a novel
neural implicit reconstruction method for sparse views with volume
rendering-based feature consistency and uncertainty-guided depth constraint.
Firstly, we introduce a feature consistency loss across views to constrain the
neural implicit field. This design alleviates the ambiguity caused by
insufficient consistency information of views and ensures completeness and
smoothness in the reconstruction results. Secondly, we employ an
uncertainty-guided depth constraint to back up the feature consistency loss in
areas with occlusion and insignificant features, which recovers geometry
details for better reconstruction quality. Experimental results demonstrate
that our method outperforms the state-of-the-art methods, which can produce
high-quality geometry with sparse-view input, especially in the scenarios with
small overlapping views. Project page: https://hanl2010.github.io/SparseRecon/.

</details>


### [79] [Representation Shift: Unifying Token Compression with FlashAttention](https://arxiv.org/abs/2508.00367)
*Joonmyung Choi,Sanghyeok Lee,Byungoh Ko,Eunseo Kim,Jihyung Kil,Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: Representation Shift: a training-free, model-agnostic metric that measures the degree of change in each token's representation. It enables effective token compression compatible with FlashAttention, yielding significant speedups of up to 5.5% and 4.4% in video-text retrieval and video QA.


<details>
  <summary>Details</summary>
Motivation: Increasing task complexity has led to larger models and more tokens, raising the quadratic cost of self-attention and the overhead of GPU memory access. To reduce the computation cost of self-attention, prior work has proposed token compression techniques that drop redundant or less informative tokens. Meanwhile, fused attention kernels such as FlashAttention have been developed to alleviate memory overhead by avoiding attention map construction and its associated I/O to HBM. This, however, makes it incompatible with most training-free token compression methods, which rely on attention maps to determine token importance.

Method: We propose Representation Shift, a training-free, model-agnostic metric that measures the degree of change in each token's representation.

Result: Representation Shift enables effective token compression compatible with FlashAttention, yielding significant speedups of up to 5.5% and 4.4% in video-text retrieval and video QA, respectively.

Conclusion: Representation Shift enables effective token compression compatible with FlashAttention, yielding significant speedups of up to 5.5% and 4.4% in video-text retrieval and video QA.

Abstract: Transformers have demonstrated remarkable success across vision, language,
and video. Yet, increasing task complexity has led to larger models and more
tokens, raising the quadratic cost of self-attention and the overhead of GPU
memory access. To reduce the computation cost of self-attention, prior work has
proposed token compression techniques that drop redundant or less informative
tokens. Meanwhile, fused attention kernels such as FlashAttention have been
developed to alleviate memory overhead by avoiding attention map construction
and its associated I/O to HBM. This, however, makes it incompatible with most
training-free token compression methods, which rely on attention maps to
determine token importance. Here, we propose Representation Shift, a
training-free, model-agnostic metric that measures the degree of change in each
token's representation. This seamlessly integrates token compression with
FlashAttention, without attention maps or retraining. Our method further
generalizes beyond Transformers to CNNs and state space models. Extensive
experiments show that Representation Shift enables effective token compression
compatible with FlashAttention, yielding significant speedups of up to 5.5% and
4.4% in video-text retrieval and video QA, respectively. Code is available at
https://github.com/mlvlab/Representation-Shift.

</details>


### [80] [Bidirectional Action Sequence Learning for Long-term Action Anticipation with Large Language Models](https://arxiv.org/abs/2508.00374)
*Yuji Sato,Yasunori Ishii,Takayoshi Yamashita*

Main category: cs.CV

TL;DR: BiAnt combines forward and backward prediction using a large language model to improve video-based long-term action anticipation.


<details>
  <summary>Details</summary>
Motivation: Conventional approaches limits performance due to their unidirectional nature and struggle to capture semantically distinct sub-actions within a scene.

Method: combining forward prediction with backward prediction using a large language model

Result: BiAnt improves performance in terms of edit distance compared to baseline methods on Ego4D.

Conclusion: BiAnt improves performance in terms of edit distance compared to baseline methods.

Abstract: Video-based long-term action anticipation is crucial for early risk detection
in areas such as automated driving and robotics. Conventional approaches
extract features from past actions using encoders and predict future events
with decoders, which limits performance due to their unidirectional nature.
These methods struggle to capture semantically distinct sub-actions within a
scene. The proposed method, BiAnt, addresses this limitation by combining
forward prediction with backward prediction using a large language model.
Experimental results on Ego4D demonstrate that BiAnt improves performance in
terms of edit distance compared to baseline methods.

</details>


### [81] [Advancing Welding Defect Detection in Maritime Operations via Adapt-WeldNet and Defect Detection Interpretability Analysis](https://arxiv.org/abs/2508.00381)
*Kamal Basha S,Athira Nambiar*

Main category: cs.CV

TL;DR: The paper introduces Adapt-WeldNet and DDIA frameworks to improve welding defect detection in challenging environments. It focuses on optimizing performance, enhancing interpretability, and ensuring reliability through expert validation and trustworthy AI principles.


<details>
  <summary>Details</summary>
Motivation: Traditional non-destructive testing (NDT) methods often fail to detect subtle or internal defects, leading to potential failures and costly downtime. Furthermore, existing neural network-based approaches for defect classification frequently rely on arbitrarily selected pretrained architectures and lack interpretability, raising safety concerns for deployment.

Method: This paper introduces ``Adapt-WeldNet", an adaptive framework for welding defect detection that systematically evaluates various pre-trained architectures, transfer learning strategies, and adaptive optimizers to identify the best-performing model and hyperparameters, optimizing defect detection and providing actionable insights. Additionally, a novel Defect Detection Interpretability Analysis (DDIA) framework is proposed to enhance system transparency. DDIA employs Explainable AI (XAI) techniques, such as Grad-CAM and LIME, alongside domain-specific evaluations validated by certified ASNT NDE Level II professionals. Incorporating a Human-in-the-Loop (HITL) approach and aligning with the principles of Trustworthy AI, DDIA ensures the reliability, fairness, and accountability of the defect detection system, fostering confidence in automated decisions through expert validation.

Result: The Adapt-WeldNet framework optimizes defect detection and provides actionable insights. The Defect Detection Interpretability Analysis (DDIA) framework enhances system transparency and fosters confidence in automated decisions through expert validation.

Conclusion: This work enhances trust, safety, and reliability in welding defect detection systems, supporting critical operations in offshore and marine environments by improving both performance and interpretability.

Abstract: Weld defect detection is crucial for ensuring the safety and reliability of
piping systems in the oil and gas industry, especially in challenging marine
and offshore environments. Traditional non-destructive testing (NDT) methods
often fail to detect subtle or internal defects, leading to potential failures
and costly downtime. Furthermore, existing neural network-based approaches for
defect classification frequently rely on arbitrarily selected pretrained
architectures and lack interpretability, raising safety concerns for
deployment. To address these challenges, this paper introduces
``Adapt-WeldNet", an adaptive framework for welding defect detection that
systematically evaluates various pre-trained architectures, transfer learning
strategies, and adaptive optimizers to identify the best-performing model and
hyperparameters, optimizing defect detection and providing actionable insights.
Additionally, a novel Defect Detection Interpretability Analysis (DDIA)
framework is proposed to enhance system transparency. DDIA employs Explainable
AI (XAI) techniques, such as Grad-CAM and LIME, alongside domain-specific
evaluations validated by certified ASNT NDE Level II professionals.
Incorporating a Human-in-the-Loop (HITL) approach and aligning with the
principles of Trustworthy AI, DDIA ensures the reliability, fairness, and
accountability of the defect detection system, fostering confidence in
automated decisions through expert validation. By improving both performance
and interpretability, this work enhances trust, safety, and reliability in
welding defect detection systems, supporting critical operations in offshore
and marine environments.

</details>


### [82] [$MV_{Hybrid}$: Improving Spatial Transcriptomics Prediction with Hybrid State Space-Vision Transformer Backbone in Pathology Vision Foundation Models](https://arxiv.org/abs/2508.00383)
*Won June Cho,Hongjun Yoon,Daeky Jeong,Hyeongyeol Lim,Yosep Chong*

Main category: cs.CV

TL;DR: MVHybrid 是一种新的混合骨干架构，它结合了状态空间模型 (SSMs) 与 ViT，用于从组织病理学图像中预测基因表达，并在性能和鲁棒性方面优于现有的 ViT 模型。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学成本高、技术复杂，限制了临床应用。从组织病理学图像预测空间基因表达是一种替代方案，但目前基于 Vision Transformer (ViT) 的病理视觉基础模型 (VFMs) 性能低于临床标准。

Method: 结合状态空间模型 (SSMs) 与 ViT 的混合骨干架构 MVHybrid，使用 DINOv2 自监督学习方法在结直肠癌数据集上进行预训练。

Result: MVHybrid 在 LOSO 评估中比表现最佳的 ViT 相关性高 57%，并且在基因表达预测中，与随机分割相比，性能下降幅度减少了 43%。

Conclusion: MVHybrid在基因表达预测中表现出卓越的性能和鲁棒性，并在分类、patch检索和生存预测任务中表现出与ViT相当或更好的下游性能，显示了其作为下一代病理VFM骨干的潜力。

Abstract: Spatial transcriptomics reveals gene expression patterns within tissue
context, enabling precision oncology applications such as treatment response
prediction, but its high cost and technical complexity limit clinical adoption.
Predicting spatial gene expression (biomarkers) from routine histopathology
images offers a practical alternative, yet current vision foundation models
(VFMs) in pathology based on Vision Transformer (ViT) backbones perform below
clinical standards. Given that VFMs are already trained on millions of diverse
whole slide images, we hypothesize that architectural innovations beyond ViTs
may better capture the low-frequency, subtle morphological patterns correlating
with molecular phenotypes. By demonstrating that state space models initialized
with negative real eigenvalues exhibit strong low-frequency bias, we introduce
$MV_{Hybrid}$, a hybrid backbone architecture combining state space models
(SSMs) with ViT. We compare five other different backbone architectures for
pathology VFMs, all pretrained on identical colorectal cancer datasets using
the DINOv2 self-supervised learning method. We evaluate all pretrained models
using both random split and leave-one-study-out (LOSO) settings of the same
biomarker dataset. In LOSO evaluation, $MV_{Hybrid}$ achieves 57% higher
correlation than the best-performing ViT and shows 43% smaller performance
degradation compared to random split in gene expression prediction,
demonstrating superior performance and robustness, respectively. Furthermore,
$MV_{Hybrid}$ shows equal or better downstream performance in classification,
patch retrieval, and survival prediction tasks compared to that of ViT, showing
its promise as a next-generation pathology VFM backbone. Our code is publicly
available at: https://github.com/deepnoid-ai/MVHybrid.

</details>


### [83] [Cued-Agent: A Collaborative Multi-Agent System for Automatic Cued Speech Recognition](https://arxiv.org/abs/2508.00391)
*Guanjie Huang,Danny H. K. Tsang,Shan Yang,Guangzhi Lei,Li Liu*

Main category: cs.CV

TL;DR: 提出了一个用于自动提示语音识别的协作多智能体系统Cued-Agent，它优于现有技术，并通过收集来自 14 个受试者的数据来扩展现有的普通话 CS 数据集。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要复杂模块来促进有效的多模态融合，但由于数据可用性有限，当前方法在充分训练这些融合机制方面的能力不足，导致性能欠佳。为了解决这个问题，我们提出...

Method: 提出了一个名为Cued-Agent的协作多智能体系统，该系统集成了四个专门的子智能体：基于多模态大型语言模型的手部识别智能体、基于预训练Transformer的唇部识别智能体、手部提示解码智能体和自我修正音素到单词智能体。

Result: Cued-Agent在正常和听力受损情况下均表现出色。

Conclusion: Cued-Agent在正常和听力受损情况下均表现出色，优于现有技术。

Abstract: Cued Speech (CS) is a visual communication system that combines lip-reading
with hand coding to facilitate communication for individuals with hearing
impairments. Automatic CS Recognition (ACSR) aims to convert CS hand gestures
and lip movements into text via AI-driven methods. Traditionally, the temporal
asynchrony between hand and lip movements requires the design of complex
modules to facilitate effective multimodal fusion. However, constrained by
limited data availability, current methods demonstrate insufficient capacity
for adequately training these fusion mechanisms, resulting in suboptimal
performance. Recently, multi-agent systems have shown promising capabilities in
handling complex tasks with limited data availability. To this end, we propose
the first collaborative multi-agent system for ACSR, named Cued-Agent. It
integrates four specialized sub-agents: a Multimodal Large Language Model-based
Hand Recognition agent that employs keyframe screening and CS expert prompt
strategies to decode hand movements, a pretrained Transformer-based Lip
Recognition agent that extracts lip features from the input video, a Hand
Prompt Decoding agent that dynamically integrates hand prompts with lip
features during inference in a training-free manner, and a Self-Correction
Phoneme-to-Word agent that enables post-process and end-to-end conversion from
phoneme sequences to natural language sentences for the first time through
semantic refinement. To support this study, we expand the existing Mandarin CS
dataset by collecting data from eight hearing-impaired cuers, establishing a
mixed dataset of fourteen subjects. Extensive experiments demonstrate that our
Cued-Agent performs superbly in both normal and hearing-impaired scenarios
compared with state-of-the-art methods. The implementation is available at
https://github.com/DennisHgj/Cued-Agent.

</details>


### [84] [Decouple before Align: Visual Disentanglement Enhances Prompt Tuning](https://arxiv.org/abs/2508.00395)
*Fei Zhang,Tianfei Zhou,Jiangchao Yao,Ya Zhang,Ivor W. Tsang,Yanfeng Wang*

Main category: cs.CV

TL;DR: DAPT decouples visual modality to address information asymmetry in prompt tuning, achieving better performance in various learning scenarios.


<details>
  <summary>Details</summary>
Motivation: This paper addresses the information asymmetry issue in Prompt Tuning (PT), where the visual modality conveys more context than the textual modality, leading to biased attention on the context area.

Method: The paper proposes DAPT, an effective PT framework based on a decouple-before-align concept. It decouples the visual modality into foreground and background representations using visual segmenting cues, aligns these with foreground texts and background classes, and uses a visual pull-push regularization to enhance visual concentration.

Result: DAPT yields superior performance across prevailing benchmarks in few-shot learning, base-to-novel generalization, and data-efficient learning.

Conclusion: The architecture-free DAPT framework demonstrates superior performance across prevailing benchmarks in few-shot learning, base-to-novel generalization, and data-efficient learning.

Abstract: Prompt tuning (PT), as an emerging resource-efficient fine-tuning paradigm,
has showcased remarkable effectiveness in improving the task-specific
transferability of vision-language models. This paper delves into a previously
overlooked information asymmetry issue in PT, where the visual modality mostly
conveys more context than the object-oriented textual modality.
Correspondingly, coarsely aligning these two modalities could result in the
biased attention, driving the model to merely focus on the context area. To
address this, we propose DAPT, an effective PT framework based on an intuitive
decouple-before-align concept. First, we propose to explicitly decouple the
visual modality into the foreground and background representation via
exploiting coarse-and-fine visual segmenting cues, and then both of these
decoupled patterns are aligned with the original foreground texts and the
hand-crafted background classes, thereby symmetrically strengthening the modal
alignment. To further enhance the visual concentration, we propose a visual
pull-push regularization tailored for the foreground-background patterns,
directing the original visual representation towards unbiased attention on the
region-of-interest object. We demonstrate the power of architecture-free DAPT
through few-shot learning, base-to-novel generalization, and data-efficient
learning, all of which yield superior performance across prevailing benchmarks.
Our code will be released at https://github.com/Ferenas/DAPT.

</details>


### [85] [Video Forgery Detection with Optical Flow Residuals and Spatial-Temporal Consistency](https://arxiv.org/abs/2508.00397)
*Xi Xue,Kunio Suzuki,Nabarun Goswami,Takuya Shintate*

Main category: cs.CV

TL;DR: A dual-branch framework combining RGB appearance features with optical flow residuals to detect forged videos.


<details>
  <summary>Details</summary>
Motivation: Existing methods often struggle to capture fine-grained temporal inconsistencies in AI-generated videos.

Method: The model adopts a dual-branch architecture, where one branch analyzes RGB frames, while the other processes flow residuals.

Result: The proposed method effectively detects a wide range of forged videos by integrating complementary features.

Conclusion: The proposed method demonstrates robustness and strong generalization ability across ten diverse generative models in text-to-video and image-to-video tasks.

Abstract: The rapid advancement of diffusion-based video generation models has led to
increasingly realistic synthetic content, presenting new challenges for video
forgery detection. Existing methods often struggle to capture fine-grained
temporal inconsistencies, particularly in AI-generated videos with high visual
fidelity and coherent motion. In this work, we propose a detection framework
that leverages spatial-temporal consistency by combining RGB appearance
features with optical flow residuals. The model adopts a dual-branch
architecture, where one branch analyzes RGB frames to detect appearance-level
artifacts, while the other processes flow residuals to reveal subtle motion
anomalies caused by imperfect temporal synthesis. By integrating these
complementary features, the proposed method effectively detects a wide range of
forged videos. Extensive experiments on text-to-video and image-to-video tasks
across ten diverse generative models demonstrate the robustness and strong
generalization ability of the proposed approach.

</details>


### [86] [iSafetyBench: A video-language benchmark for safety in industrial environment](https://arxiv.org/abs/2508.00399)
*Raiyaan Abdullah,Yogesh Singh Rawat,Shruti Vyas*

Main category: cs.CV

TL;DR: 提出了 iSafetyBench 基准测试，发现现有模型在工业安全领域表现不佳，需要更强大的模型。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在零样本设置下，在不同的视频理解任务中实现了令人印象深刻的泛化。然而，它们在高风险工业领域的能力仍未被充分探索。

Method: 提出了一个新的视频-语言基准测试 iSafetyBench，用于评估模型在工业环境中正常和危险场景下的性能。

Result: 在 iSafetyBench 上评估了八个最先进的视频语言模型，结果表明这些模型在识别危险活动和多标签场景方面存在明显的性能差距。

Conclusion: 现有的视频语言模型在工业安全领域表现不佳，需要更强大的、具有安全意识的多模态模型。

Abstract: Recent advances in vision-language models (VLMs) have enabled impressive
generalization across diverse video understanding tasks under zero-shot
settings. However, their capabilities in high-stakes industrial domains-where
recognizing both routine operations and safety-critical anomalies is
essential-remain largely underexplored. To address this gap, we introduce
iSafetyBench, a new video-language benchmark specifically designed to evaluate
model performance in industrial environments across both normal and hazardous
scenarios. iSafetyBench comprises 1,100 video clips sourced from real-world
industrial settings, annotated with open-vocabulary, multi-label action tags
spanning 98 routine and 67 hazardous action categories. Each clip is paired
with multiple-choice questions for both single-label and multi-label
evaluation, enabling fine-grained assessment of VLMs in both standard and
safety-critical contexts. We evaluate eight state-of-the-art video-language
models under zero-shot conditions. Despite their strong performance on existing
video benchmarks, these models struggle with iSafetyBench-particularly in
recognizing hazardous activities and in multi-label scenarios. Our results
reveal significant performance gaps, underscoring the need for more robust,
safety-aware multimodal models for industrial applications. iSafetyBench
provides a first-of-its-kind testbed to drive progress in this direction. The
dataset is available at: https://github.com/raiyaan-abdullah/iSafety-Bench.

</details>


### [87] [Sari Sandbox: A Virtual Retail Store Environment for Embodied AI Agents](https://arxiv.org/abs/2508.00400)
*Janika Deborah Gajo,Gerarld Paul Merales,Jerome Escarcha,Brenden Ashley Molina,Gian Nartea,Emmanuel G. Maminta,Juan Carlos Roldan,Rowel O. Atienza*

Main category: cs.CV

TL;DR: Sari Sandbox is a photorealistic 3D retail store simulation for benchmarking embodied agents in shopping tasks. It includes a dataset of human demonstrations and supports VR and VLM-powered agents.


<details>
  <summary>Details</summary>
Motivation: There is a gap in retail-specific sim environments for embodied agent training.

Method: The Sari Sandbox environment is controlled via an API and supports both virtual reality (VR) for human interaction and a vision language model (VLM)-powered embodied agent.

Result: The Sari Sandbox environment features over 250 interactive grocery items across three store configurations and introduces SariBench, a dataset of annotated human demonstrations across varied task difficulties. The sandbox enables embodied agents to navigate, inspect, and manipulate retail items, providing baselines against human performance.

Conclusion: This paper provides benchmarks, performance analysis, and recommendations for enhancing realism and scalability of the Sari Sandbox environment.

Abstract: We present Sari Sandbox, a high-fidelity, photorealistic 3D retail store
simulation for benchmarking embodied agents against human performance in
shopping tasks. Addressing a gap in retail-specific sim environments for
embodied agent training, Sari Sandbox features over 250 interactive grocery
items across three store configurations, controlled via an API. It supports
both virtual reality (VR) for human interaction and a vision language model
(VLM)-powered embodied agent. We also introduce SariBench, a dataset of
annotated human demonstrations across varied task difficulties. Our sandbox
enables embodied agents to navigate, inspect, and manipulate retail items,
providing baselines against human performance. We conclude with benchmarks,
performance analysis, and recommendations for enhancing realism and
scalability. The source code can be accessed via
https://github.com/upeee/sari-sandbox-env.

</details>


### [88] [PMR: Physical Model-Driven Multi-Stage Restoration of Turbulent Dynamic Videos](https://arxiv.org/abs/2508.00406)
*Tao Wu,Jingyuan Ye,Ying Fu*

Main category: cs.CV

TL;DR: Introduce a Dynamic Efficiency Index (DEI) to accurately quantify video dynamic intensity under varying turbulence conditions and provide a high-dynamic turbulence training dataset. Additionally, propose a Physical Model-Driven Multi-Stage Video Restoration (PMR) framework that consists of three stages


<details>
  <summary>Details</summary>
Motivation: Geometric distortions and blurring caused by atmospheric turbulence degrade the quality of long-range dynamic scene videos. Existing methods struggle with restoring edge details and eliminating mixed distortions, especially under conditions of strong turbulence and complex dynamics.

Method: propose a Physical Model-Driven Multi-Stage Video Restoration (PMR) framework that consists of three stages: de-tilting for geometric stabilization, motion segmentation enhancement for dynamic region refinement, and de-blurring for quality restoration. PMR employs lightweight backbones and stage-wise joint training to ensure both efficiency and high restoration quality.

Result: Experimental results demonstrate that the proposed method effectively suppresses motion trailing artifacts, restores edge details and exhibits strong generalization capability, especially in real-world scenarios characterized by high-turbulence and complex dynamics.

Conclusion: The proposed method effectively suppresses motion trailing artifacts, restores edge details and exhibits strong generalization capability, especially in real-world scenarios characterized by high-turbulence and complex dynamics.

Abstract: Geometric distortions and blurring caused by atmospheric turbulence degrade
the quality of long-range dynamic scene videos. Existing methods struggle with
restoring edge details and eliminating mixed distortions, especially under
conditions of strong turbulence and complex dynamics. To address these
challenges, we introduce a Dynamic Efficiency Index ($DEI$), which combines
turbulence intensity, optical flow, and proportions of dynamic regions to
accurately quantify video dynamic intensity under varying turbulence conditions
and provide a high-dynamic turbulence training dataset. Additionally, we
propose a Physical Model-Driven Multi-Stage Video Restoration ($PMR$) framework
that consists of three stages: \textbf{de-tilting} for geometric stabilization,
\textbf{motion segmentation enhancement} for dynamic region refinement, and
\textbf{de-blurring} for quality restoration. $PMR$ employs lightweight
backbones and stage-wise joint training to ensure both efficiency and high
restoration quality. Experimental results demonstrate that the proposed method
effectively suppresses motion trailing artifacts, restores edge details and
exhibits strong generalization capability, especially in real-world scenarios
characterized by high-turbulence and complex dynamics. We will make the code
and datasets openly available.

</details>


### [89] [Sortblock: Similarity-Aware Feature Reuse for Diffusion Model](https://arxiv.org/abs/2508.00412)
*Hanqi Chen,Xu Zhang,Xiaoliu Guan,Lielin Jiang,Guanzhong Wang,Zeyu Chen,Yi Liu*

Main category: cs.CV

TL;DR: propose Sortblock, a training-free inference acceleration framework that dynamically caches block-wise features based on their similarity across adjacent timesteps to  achieves over 2x inference speedup with minimal degradation in output quality.


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformers (DiTs) have demonstrated remarkable generative capabilities, particularly benefiting from Transformer architectures that enhance visual and artistic fidelity. However, their inherently sequential denoising process results in high inference latency, limiting their deployment in real-time scenarios. Existing training-free acceleration approaches typically reuse intermediate features at fixed timesteps or layers, overlooking the evolving semantic focus across denoising stages and Transformer blocks.

Method: propose Sortblock, a training-free inference acceleration framework that dynamically caches block-wise features based on their similarity across adjacent timesteps. By ranking the evolution of residuals, Sortblock adaptively determines a recomputation ratio, selectively skipping redundant computations while preserving generation quality. Furthermore, we incorporate a lightweight linear prediction mechanism to reduce accumulated errors in skipped blocks.

Result: Sortblock achieves over 2$\times$ inference speedup with minimal degradation in output quality

Conclusion: Sortblock achieves over 2x inference speedup with minimal degradation in output quality, offering an effective and generalizable solution for accelerating diffusion-based generative models.

Abstract: Diffusion Transformers (DiTs) have demonstrated remarkable generative
capabilities, particularly benefiting from Transformer architectures that
enhance visual and artistic fidelity. However, their inherently sequential
denoising process results in high inference latency, limiting their deployment
in real-time scenarios. Existing training-free acceleration approaches
typically reuse intermediate features at fixed timesteps or layers, overlooking
the evolving semantic focus across denoising stages and Transformer blocks.To
address this, we propose Sortblock, a training-free inference acceleration
framework that dynamically caches block-wise features based on their similarity
across adjacent timesteps. By ranking the evolution of residuals, Sortblock
adaptively determines a recomputation ratio, selectively skipping redundant
computations while preserving generation quality. Furthermore, we incorporate a
lightweight linear prediction mechanism to reduce accumulated errors in skipped
blocks.Extensive experiments across various tasks and DiT architectures
demonstrate that Sortblock achieves over 2$\times$ inference speedup with
minimal degradation in output quality, offering an effective and generalizable
solution for accelerating diffusion-based generative models.

</details>


### [90] [DC-AE 1.5: Accelerating Diffusion Model Convergence with Structured Latent Space](https://arxiv.org/abs/2508.00413)
*Junyu Chen,Dongyun Zou,Wenkun He,Junsong Chen,Enze Xie,Song Han,Han Cai*

Main category: cs.CV

TL;DR: DC-AE 1.5通过结构化潜在空间和增强扩散训练，提高了高分辨率扩散模型的图像生成质量和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 增加自编码器的潜在通道数是提高其重建质量的有效方法，但会导致扩散模型收敛缓慢，从而导致生成质量下降。

Method: 提出了两种关键创新：结构化潜在空间和增强扩散训练。

Result: DC-AE 1.5实现了比DC-AE更快的收敛速度和更好的扩散缩放结果。

Conclusion: DC-AE 1.5实现了比DC-AE更快的收敛速度和更好的扩散缩放结果，在ImageNet 512x512上，DC-AE-1.5-f64c128比DC-AE-f32c32具有更好的图像生成质量，速度快4倍。

Abstract: We present DC-AE 1.5, a new family of deep compression autoencoders for
high-resolution diffusion models. Increasing the autoencoder's latent channel
number is a highly effective approach for improving its reconstruction quality.
However, it results in slow convergence for diffusion models, leading to poorer
generation quality despite better reconstruction quality. This issue limits the
quality upper bound of latent diffusion models and hinders the employment of
autoencoders with higher spatial compression ratios. We introduce two key
innovations to address this challenge: i) Structured Latent Space, a
training-based approach to impose a desired channel-wise structure on the
latent space with front latent channels capturing object structures and latter
latent channels capturing image details; ii) Augmented Diffusion Training, an
augmented diffusion training strategy with additional diffusion training
objectives on object latent channels to accelerate convergence. With these
techniques, DC-AE 1.5 delivers faster convergence and better diffusion scaling
results than DC-AE. On ImageNet 512x512, DC-AE-1.5-f64c128 delivers better
image generation quality than DC-AE-f32c32 while being 4x faster. Code:
https://github.com/dc-ai-projects/DC-Gen.

</details>


### [91] [IN2OUT: Fine-Tuning Video Inpainting Model for Video Outpainting Using Hierarchical Discriminator](https://arxiv.org/abs/2508.00418)
*Sangwoo Youn,Minji Lee,Nokap Tony Park,Yeonggyoo Jeon,Taeyoung Na*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频外推方法，该方法使用视频修复模型，并引入了一个分层判别器和专门的损失函数，以提高生成图像的质量和连贯性。


<details>
  <summary>Details</summary>
Motivation: 视频外推技术提出了一个独特的挑战，即在扩展边界的同时，保持与给定内容的一致性。在本文中，我们建议使用视频修复模型，该模型擅长于对象流学习和在外推中进行重建，而不是像现有方法那样仅仅生成背景。

Method: 我们区分了对抗训练的目标，将其分为全局和局部目标，并引入了一个满足这两个目标的分层判别器。此外，我们还开发了一种专门的outpainting损失函数，该函数利用了判别器的局部和全局特征。在此对抗性损失函数上进行微调，增强了生成器生成视觉上吸引人且全局连贯的outpainted场景的能力。

Result: 我们对判别器设计的广泛实验表明，外推微调过程中缺少的一个关键组成部分是判别器，它能够有效地评估扩展区域的感知质量。

Conclusion: 该方法在定量和定性方面均优于现有技术。

Abstract: Video outpainting presents a unique challenge of extending the borders while
maintaining consistency with the given content. In this paper, we suggest the
use of video inpainting models that excel in object flow learning and
reconstruction in outpainting rather than solely generating the background as
in existing methods. However, directly applying or fine-tuning inpainting
models to outpainting has shown to be ineffective, often leading to blurry
results. Our extensive experiments on discriminator designs reveal that a
critical component missing in the outpainting fine-tuning process is a
discriminator capable of effectively assessing the perceptual quality of the
extended areas. To tackle this limitation, we differentiate the objectives of
adversarial training into global and local goals and introduce a hierarchical
discriminator that meets both objectives. Additionally, we develop a
specialized outpainting loss function that leverages both local and global
features of the discriminator. Fine-tuning on this adversarial loss function
enhances the generator's ability to produce both visually appealing and
globally coherent outpainted scenes. Our proposed method outperforms
state-of-the-art methods both quantitatively and qualitatively. Supplementary
materials including the demo video and the code are available in SigPort.

</details>


### [92] [UIS-Mamba: Exploring Mamba for Underwater Instance Segmentation via Dynamic Tree Scan and Hidden State Weaken](https://arxiv.org/abs/2508.00421)
*Runmin Cong,Zongji Yu,Hao Fang,Haoyan Sun,Sam Kwong*

Main category: cs.CV

TL;DR: 提出了一个基于Mamba的水下实例分割模型，通过动态树扫描和隐藏状态弱化来解决水下图像的挑战，并在UIIS和USIS10K数据集上取得了优异的成果。


<details>
  <summary>Details</summary>
Motivation: 水下实例分割（UIS）任务对于水下复杂场景检测至关重要。Mamba作为一种新兴的状态空间模型，具有固有的线性复杂度和全局感受野，非常适合处理具有长序列特征的图像分割任务。然而，由于水下场景的特殊性，将Mamba应用于UIS存在许多挑战。现有的固定patch扫描机制无法在严重的水下颜色失真和模糊的实例边界存在的情况下保持扫描实例的内部连续性，并且复杂水下背景的隐藏状态也会抑制对实例对象的理解。

Method: 提出了基于Mamba的水下实例分割模型UIS-Mamba，并设计了两个创新模块，动态树扫描（DTS）和隐藏状态弱化（HSW）。

Result: DTS模块通过允许patch动态偏移和缩放来保持实例对象内部特征的连续性，从而引导最小生成树并提供动态局部感受野。HSW模块通过基于Ncut的隐藏状态弱化机制抑制复杂背景的干扰，并有效地将状态传播的信息流集中到实例本身。

Conclusion: UIS-Mamba在UIIS和USIS10K数据集上实现了最先进的性能，同时保持了较低的参数数量和计算复杂度。

Abstract: Underwater Instance Segmentation (UIS) tasks are crucial for underwater
complex scene detection. Mamba, as an emerging state space model with
inherently linear complexity and global receptive fields, is highly suitable
for processing image segmentation tasks with long sequence features. However,
due to the particularity of underwater scenes, there are many challenges in
applying Mamba to UIS. The existing fixed-patch scanning mechanism cannot
maintain the internal continuity of scanned instances in the presence of
severely underwater color distortion and blurred instance boundaries, and the
hidden state of the complex underwater background can also inhibit the
understanding of instance objects. In this work, we propose the first
Mamba-based underwater instance segmentation model UIS-Mamba, and design two
innovative modules, Dynamic Tree Scan (DTS) and Hidden State Weaken (HSW), to
migrate Mamba to the underwater task. DTS module maintains the continuity of
the internal features of the instance objects by allowing the patches to
dynamically offset and scale, thereby guiding the minimum spanning tree and
providing dynamic local receptive fields. HSW module suppresses the
interference of complex backgrounds and effectively focuses the information
flow of state propagation to the instances themselves through the Ncut-based
hidden state weakening mechanism. Experimental results show that UIS-Mamba
achieves state-of-the-art performance on both UIIS and USIS10K datasets, while
maintaining a low number of parameters and computational complexity. Code is
available at https://github.com/Maricalce/UIS-Mamba.

</details>


### [93] [Contact-Aware Amodal Completion for Human-Object Interaction via Multi-Regional Inpainting](https://arxiv.org/abs/2508.00427)
*Seunggeun Chi,Enna Sachdeva,Pin-Hao Huang,Kwonjoon Lee*

Main category: cs.CV

TL;DR: 提出了一种新的非模态补全方法，该方法利用物理先验知识和多区域修复技术来提高在复杂 HOI 场景中生成合理补全的准确性和真实感。


<details>
  <summary>Details</summary>
Motivation: 现有的方法，例如那些使用预训练扩散模型的方法，通常难以在动态场景中生成合理的补全，因为它们对 HOI 的理解有限。

Method: 一种新的方法，它使用物理先验知识以及专为 HOI 设计的专用多区域修复技术。

Result: 实验结果表明，该方法在形状和视觉细节上提高了生成补全的准确性和真实感，在 HOI 场景中显著优于现有方法。

Conclusion: 该方法在 HOI 场景中显著优于现有方法，使机器感知更接近于类人的动态环境理解。即使没有ground-truth接触注释，该流程依然稳健，从而扩大了其在 3D 重建和新视角/姿势合成等任务中的适用性。

Abstract: Amodal completion, which is the process of inferring the full appearance of
objects despite partial occlusions, is crucial for understanding complex
human-object interactions (HOI) in computer vision and robotics. Existing
methods, such as those that use pre-trained diffusion models, often struggle to
generate plausible completions in dynamic scenarios because they have a limited
understanding of HOI. To solve this problem, we've developed a new approach
that uses physical prior knowledge along with a specialized multi-regional
inpainting technique designed for HOI. By incorporating physical constraints
from human topology and contact information, we define two distinct regions:
the primary region, where occluded object parts are most likely to be, and the
secondary region, where occlusions are less probable. Our multi-regional
inpainting method uses customized denoising strategies across these regions
within a diffusion model. This improves the accuracy and realism of the
generated completions in both their shape and visual detail. Our experimental
results show that our approach significantly outperforms existing methods in
HOI scenarios, moving machine perception closer to a more human-like
understanding of dynamic environments. We also show that our pipeline is robust
even without ground-truth contact annotations, which broadens its applicability
to tasks like 3D reconstruction and novel view/pose synthesis.

</details>


### [94] [Reducing the gap between general purpose data and aerial images in concentrated solar power plants](https://arxiv.org/abs/2508.00440)
*M. A. Pérez-Cutiño,J. Valverde,J. Capitán,J. M. Díaz-Báñez*

Main category: cs.CV

TL;DR: create AerialCSP, a virtual dataset that simulates aerial imagery of CSP plants to facilitate pretraining of models before deployment, significantly reducing the need for extensive manual labeling


<details>
  <summary>Details</summary>
Motivation: machine learning models trained on generic datasets struggle to generalize to Concentrated Solar Power plants setting without extensive retraining and large volumes of annotated data, while collecting and labeling such data is costly and time-consuming

Method: creation of AerialCSP, a virtual dataset that simulates aerial imagery of CSP plants

Result: AerialCSP, a high-quality synthetic dataset for aerial inspection of CSP plants, providing annotated data for object detection and image segmentation; benchmark multiple models on AerialCSP, establishing a baseline for CSP-related vision tasks

Conclusion: pretraining on AerialCSP significantly improves real-world fault detection, particularly for rare and small defects, reducing the need for extensive manual labeling

Abstract: In the context of Concentrated Solar Power (CSP) plants, aerial images
captured by drones present a unique set of challenges. Unlike urban or natural
landscapes commonly found in existing datasets, solar fields contain highly
reflective surfaces, and domain-specific elements that are uncommon in
traditional computer vision benchmarks. As a result, machine learning models
trained on generic datasets struggle to generalize to this setting without
extensive retraining and large volumes of annotated data. However, collecting
and labeling such data is costly and time-consuming, making it impractical for
rapid deployment in industrial applications.
  To address this issue, we propose a novel approach: the creation of
AerialCSP, a virtual dataset that simulates aerial imagery of CSP plants. By
generating synthetic data that closely mimic real-world conditions, our
objective is to facilitate pretraining of models before deployment,
significantly reducing the need for extensive manual labeling. Our main
contributions are threefold: (1) we introduce AerialCSP, a high-quality
synthetic dataset for aerial inspection of CSP plants, providing annotated data
for object detection and image segmentation; (2) we benchmark multiple models
on AerialCSP, establishing a baseline for CSP-related vision tasks; and (3) we
demonstrate that pretraining on AerialCSP significantly improves real-world
fault detection, particularly for rare and small defects, reducing the need for
extensive manual labeling. AerialCSP is made publicly available at
https://mpcutino.github.io/aerialcsp/.

</details>


### [95] [TopoTTA: Topology-Enhanced Test-Time Adaptation for Tubular Structure Segmentation](https://arxiv.org/abs/2508.00442)
*Jiale Zhou,Wenhan Wang,Shikun Li,Xiaolei Qu,Xin Guo,Yizhong Liu,Wenzhong Tang,Xun Lin,Yefeng Zheng*

Main category: cs.CV

TL;DR: TopoTTA是一种用于TSS的测试时自适应框架，通过拓扑元差异卷积和拓扑硬样本生成来处理领域转移问题，并在clDice中平均提高了31.81%。


<details>
  <summary>Details</summary>
Motivation: 领域转移仍然是一个主要的挑战，导致在看不见的目标领域中性能下降。与其他分割任务不同，TSS对领域转移更敏感，因为拓扑结构的变化会损害分割完整性，并且区分前景和背景的局部特征（例如，纹理和对比度）的变化可能会进一步破坏拓扑连续性。

Method: 该方法包括两个阶段：第一阶段使用拓扑元差异卷积（TopoMDCs）适应跨域拓扑差异，第二阶段通过拓扑硬样本生成（TopoHG）策略和在生成的伪断裂区域中对硬样本进行伪标签预测对齐来提高拓扑连续性。

Result: 在四个场景和十个数据集上的大量实验表明，TopoTTA在处理拓扑分布偏移方面非常有效，在clDice中平均提高了31.81%。

Conclusion: TopoTTA在处理拓扑分布偏移方面非常有效，在clDice中平均提高了31.81%，并且可以作为CNN的即插即用TTA解决方案。

Abstract: Tubular structure segmentation (TSS) is important for various applications,
such as hemodynamic analysis and route navigation. Despite significant progress
in TSS, domain shifts remain a major challenge, leading to performance
degradation in unseen target domains. Unlike other segmentation tasks, TSS is
more sensitive to domain shifts, as changes in topological structures can
compromise segmentation integrity, and variations in local features
distinguishing foreground from background (e.g., texture and contrast) may
further disrupt topological continuity. To address these challenges, we propose
Topology-enhanced Test-Time Adaptation (TopoTTA), the first test-time
adaptation framework designed specifically for TSS. TopoTTA consists of two
stages: Stage 1 adapts models to cross-domain topological discrepancies using
the proposed Topological Meta Difference Convolutions (TopoMDCs), which enhance
topological representation without altering pre-trained parameters; Stage 2
improves topological continuity by a novel Topology Hard sample Generation
(TopoHG) strategy and prediction alignment on hard samples with pseudo-labels
in the generated pseudo-break regions. Extensive experiments across four
scenarios and ten datasets demonstrate TopoTTA's effectiveness in handling
topological distribution shifts, achieving an average improvement of 31.81% in
clDice. TopoTTA also serves as a plug-and-play TTA solution for CNN-based TSS
models.

</details>


### [96] [SDMatte: Grafting Diffusion Models for Interactive Matting](https://arxiv.org/abs/2508.00443)
*Longfei Huang,Yu Liang,Hao Zhang,Jinwei Chen,Wei Dong,Lunde Chen,Wanyu Liu,Bo Li,Pengtao Jiang*

Main category: cs.CV

TL;DR: SDMatte: a diffusion-driven interactive matting model


<details>
  <summary>Details</summary>
Motivation: Recent interactive matting methods have shown satisfactory performance in capturing the primary regions of objects, but they fall short in extracting fine-grained details in edge regions. Diffusion models trained on billions of image-text pairs, demonstrate exceptional capability in modeling highly complex data distributions and synthesizing realistic texture details, while exhibiting robust text-driven interaction capabilities, making them an attractive solution for interactive matting.

Method: a diffusion-driven interactive matting model

Result: First, we exploit the powerful priors of diffusion models and transform the text-driven interaction capability into visual prompt-driven interaction capability to enable interactive matting. Second, we integrate coordinate embeddings of visual prompts and opacity embeddings of target objects into U-Net, enhancing SDMatte's sensitivity to spatial position information and opacity information. Third, we propose a masked self-attention mechanism that enables the model to focus on areas specified by visual prompts, leading to better performance.

Conclusion: Extensive experiments on multiple datasets demonstrate the superior performance of our method, validating its effectiveness in interactive matting.

Abstract: Recent interactive matting methods have shown satisfactory performance in
capturing the primary regions of objects, but they fall short in extracting
fine-grained details in edge regions. Diffusion models trained on billions of
image-text pairs, demonstrate exceptional capability in modeling highly complex
data distributions and synthesizing realistic texture details, while exhibiting
robust text-driven interaction capabilities, making them an attractive solution
for interactive matting. To this end, we propose SDMatte, a diffusion-driven
interactive matting model, with three key contributions. First, we exploit the
powerful priors of diffusion models and transform the text-driven interaction
capability into visual prompt-driven interaction capability to enable
interactive matting. Second, we integrate coordinate embeddings of visual
prompts and opacity embeddings of target objects into U-Net, enhancing
SDMatte's sensitivity to spatial position information and opacity information.
Third, we propose a masked self-attention mechanism that enables the model to
focus on areas specified by visual prompts, leading to better performance.
Extensive experiments on multiple datasets demonstrate the superior performance
of our method, validating its effectiveness in interactive matting. Our code
and model are available at https://github.com/vivoCameraResearch/SDMatte.

</details>


### [97] [AutoDebias: Automated Framework for Debiasing Text-to-Image Models](https://arxiv.org/abs/2508.00445)
*Hongyi Cai,Mohammad Mahdinur Rahman,Mingkang Dong,Jie Li,Muxin Pu,Zhili Fang,Yinan Peng,Hanjun Luo,Yang Liu*

Main category: cs.CV

TL;DR: AutoDebias is a framework that automatically identifies and mitigates harmful biases in T2I models without prior knowledge of specific bias types. It leverages vision-language models to detect biased visual patterns and constructs fairness guides by generating inclusive alternative prompts that reflect balanced representations.


<details>
  <summary>Details</summary>
Motivation: Existing debiasing methods work well for simple or well-known cases but struggle with subtle or overlapping biases.

Method: AutoDebias leverages vision-language models to detect biased visual patterns and constructs fairness guides by generating inclusive alternative prompts that reflect balanced representations. These guides drive a CLIP-guided training process that promotes fairer outputs while preserving the original model's image quality and diversity.

Result: AutoDebias effectively addresses both subtle stereotypes and multiple interacting biases. We evaluate the framework on a benchmark covering over 25 bias scenarios, including challenging cases where multiple biases occur simultaneously. AutoDebias detects harmful patterns with 91.6% accuracy and reduces biased outputs from 90% to negligible levels, while preserving the visual fidelity of the original model.

Conclusion: AutoDebias detects harmful patterns with 91.6% accuracy and reduces biased outputs from 90% to negligible levels, while preserving the visual fidelity of the original model.

Abstract: Text-to-Image (T2I) models generate high-quality images from text prompts but
often exhibit unintended social biases, such as gender or racial stereotypes,
even when these attributes are not mentioned. Existing debiasing methods work
well for simple or well-known cases but struggle with subtle or overlapping
biases. We propose AutoDebias, a framework that automatically identifies and
mitigates harmful biases in T2I models without prior knowledge of specific bias
types. Specifically, AutoDebias leverages vision-language models to detect
biased visual patterns and constructs fairness guides by generating inclusive
alternative prompts that reflect balanced representations. These guides drive a
CLIP-guided training process that promotes fairer outputs while preserving the
original model's image quality and diversity. Unlike existing methods,
AutoDebias effectively addresses both subtle stereotypes and multiple
interacting biases. We evaluate the framework on a benchmark covering over 25
bias scenarios, including challenging cases where multiple biases occur
simultaneously. AutoDebias detects harmful patterns with 91.6% accuracy and
reduces biased outputs from 90% to negligible levels, while preserving the
visual fidelity of the original model.

</details>


### [98] [CLIPTime: Time-Aware Multimodal Representation Learning from Images and Text](https://arxiv.org/abs/2508.00447)
*Anju Rani,Daniel Ortiz-Arroyo,Petar Durdevic*

Main category: cs.CV

TL;DR: CLIPTime是一个多模态框架，可以预测真菌生长的时间和发育阶段，而无需显式的时间输入。


<details>
  <summary>Details</summary>
Motivation: 理解生物生长的时间动态在微生物学、农业和生物降解研究等不同领域至关重要。像对比语言图像预训练（CLIP）这样的视觉-语言模型在联合视觉-文本推理方面表现出强大的能力，但它们在捕捉时间进展方面的有效性仍然有限。

Method: CLIPTime，一个多模态、多任务框架，旨在从图像和文本输入中预测真菌生长的发育阶段和相应的时间戳。它构建于CLIP架构之上，学习联合视觉-文本嵌入，并实现时间感知推理，而无需在测试期间显式的时间输入。

Result: 实验结果表明，CLIPTime有效地模拟了生物进程，并产生了可解释的、时间上可靠的输出。

Conclusion: CLIPTime有效地模拟了生物进程，并产生了可解释的、时间上可靠的输出，突出了视觉-语言模型在现实生物监测应用中的潜力。

Abstract: Understanding the temporal dynamics of biological growth is critical across
diverse fields such as microbiology, agriculture, and biodegradation research.
Although vision-language models like Contrastive Language Image Pretraining
(CLIP) have shown strong capabilities in joint visual-textual reasoning, their
effectiveness in capturing temporal progression remains limited. To address
this, we propose CLIPTime, a multimodal, multitask framework designed to
predict both the developmental stage and the corresponding timestamp of fungal
growth from image and text inputs. Built upon the CLIP architecture, our model
learns joint visual-textual embeddings and enables time-aware inference without
requiring explicit temporal input during testing. To facilitate training and
evaluation, we introduce a synthetic fungal growth dataset annotated with
aligned timestamps and categorical stage labels. CLIPTime jointly performs
classification and regression, predicting discrete growth stages alongside
continuous timestamps. We also propose custom evaluation metrics, including
temporal accuracy and regression error, to assess the precision of time-aware
predictions. Experimental results demonstrate that CLIPTime effectively models
biological progression and produces interpretable, temporally grounded outputs,
highlighting the potential of vision-language models in real-world biological
monitoring applications.

</details>


### [99] [PIF-Net: Ill-Posed Prior Guided Multispectral and Hyperspectral Image Fusion via Invertible Mamba and Fusion-Aware LoRA](https://arxiv.org/abs/2508.00453)
*Baisong Li,Xingwang Wang,Haixiao Xu*

Main category: cs.CV

TL;DR: PIF-Net通过结合ill-posed priors，并利用可逆Mamba架构和Fusion-Aware Low-Rank Adaptation模块，有效地融合多光谱和高光谱图像，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多光谱和高光谱图像融合 (MHIF) 的目标是生成同时具有丰富光谱信息和精细空间细节的高质量图像。然而，由于光谱和空间信息之间固有的权衡以及观测的有限可用性，这项任务从根本上来说是不适定的。以往的研究没有有效解决由数据未对准引起的不适定性。

Method: 基于可逆Mamba架构设计了一种方法，该方法在特征转换和融合过程中保持信息一致性，确保稳定的梯度流和过程可逆性。引入了一种名为Fusion-Aware Low-Rank Adaptation module的新型融合模块，该模块在保持模型轻量级的同时动态校准光谱和空间特征。

Result: PIF-Net实现了比当前最先进方法显著更好的图像恢复性能，同时保持了模型效率。

Conclusion: PIF-Net在多个基准数据集上实现了比当前最先进方法显著更好的图像恢复性能，同时保持了模型效率。

Abstract: The goal of multispectral and hyperspectral image fusion (MHIF) is to
generate high-quality images that simultaneously possess rich spectral
information and fine spatial details. However, due to the inherent trade-off
between spectral and spatial information and the limited availability of
observations, this task is fundamentally ill-posed. Previous studies have not
effectively addressed the ill-posed nature caused by data misalignment. To
tackle this challenge, we propose a fusion framework named PIF-Net, which
explicitly incorporates ill-posed priors to effectively fuse multispectral
images and hyperspectral images. To balance global spectral modeling with
computational efficiency, we design a method based on an invertible Mamba
architecture that maintains information consistency during feature
transformation and fusion, ensuring stable gradient flow and process
reversibility. Furthermore, we introduce a novel fusion module called the
Fusion-Aware Low-Rank Adaptation module, which dynamically calibrates spectral
and spatial features while keeping the model lightweight. Extensive experiments
on multiple benchmark datasets demonstrate that PIF-Net achieves significantly
better image restoration performance than current state-of-the-art methods
while maintaining model efficiency.

</details>


### [100] [Semantic and Temporal Integration in Latent Diffusion Space for High-Fidelity Video Super-Resolution](https://arxiv.org/abs/2508.00471)
*Yiwen Wang,Xinning Chai,Yuhong Zhang,Zhengxue Cheng,Jun Zhao,Rong Xie,Li Song*

Main category: cs.CV

TL;DR: 提出SeTe-VSR，一种结合语义和时间引导的视频超分辨率方法，以提高视频超分辨率的细节恢复和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 由于充分控制生成过程的局限性，在保持跨帧时间一致性的同时，实现与低分辨率输入的高保真对齐仍然是一个重大挑战。

Method: 提出了一种新的方法，即语义和时间引导的视频超分辨率(SeTe-VSR)，该方法在潜在扩散空间中结合了语义和时间空间引导。

Result: 该方法不仅保留了高真实感的视觉内容，而且显著提高了保真度。

Conclusion: SeTe-VSR在细节恢复和感知质量方面优于现有方法，展示了其在复杂视频超分辨率任务中的有效性。

Abstract: Recent advancements in video super-resolution (VSR) models have demonstrated
impressive results in enhancing low-resolution videos. However, due to
limitations in adequately controlling the generation process, achieving high
fidelity alignment with the low-resolution input while maintaining temporal
consistency across frames remains a significant challenge. In this work, we
propose Semantic and Temporal Guided Video Super-Resolution (SeTe-VSR), a novel
approach that incorporates both semantic and temporal-spatio guidance in the
latent diffusion space to address these challenges. By incorporating high-level
semantic information and integrating spatial and temporal information, our
approach achieves a seamless balance between recovering intricate details and
ensuring temporal coherence. Our method not only preserves high-reality visual
content but also significantly enhances fidelity. Extensive experiments
demonstrate that SeTe-VSR outperforms existing methods in terms of detail
recovery and perceptual quality, highlighting its effectiveness for complex
video super-resolution tasks.

</details>


### [101] [Fine-grained Spatiotemporal Grounding on Egocentric Videos](https://arxiv.org/abs/2508.00518)
*Shuo Liang,Yiwu Zhong,Zi-Yuan Hu,Yeyao Tao,Liwei Wang*

Main category: cs.CV

TL;DR: 本文介绍了EgoMask，这是一个用于以自我为中心的视频中的细粒度时空定位的像素级基准，并提出了EgoMask-Train，一个大规模训练数据集，以促进模型开发。


<details>
  <summary>Details</summary>
Motivation: 虽然现有的研究在外中心视频方面取得了显著进展，但以自我为中心的环境仍然相对未被探索，尽管它在增强现实和机器人等应用中的重要性日益增加。

Method: 我们引入了EgoMask，这是第一个像素级基准，用于以自我为中心的视频中的细粒度时空定位。

Result: 实验表明，最先进的时空定位模型在我们的基准EgoMask上表现不佳，但在EgoMask-Train上进行微调会产生显著的改进，同时保持在外中心数据集上的性能。

Conclusion: 这项工作提供了重要的资源和见解，以促进以自我为中心的视频理解。

Abstract: Spatiotemporal video grounding aims to localize target entities in videos
based on textual queries. While existing research has made significant progress
in exocentric videos, the egocentric setting remains relatively underexplored,
despite its growing importance in applications such as augmented reality and
robotics. In this work, we conduct a systematic analysis of the discrepancies
between egocentric and exocentric videos, revealing key challenges such as
shorter object durations, sparser trajectories, smaller object sizes, and
larger positional shifts. To address these challenges, we introduce EgoMask,
the first pixel-level benchmark for fine-grained spatiotemporal grounding in
egocentric videos. It is constructed by our proposed automatic annotation
pipeline, which annotates referring expressions and object masks across short-,
medium-, and long-term videos. Additionally, we create EgoMask-Train, a
large-scale training dataset to facilitate model development. Experiments
demonstrate that the state-of-the-art spatiotemporal grounding models perform
poorly on our benchmark EgoMask, but fine-tuning on EgoMask-Train yields
significant improvements, while preserving performance on exocentric datasets.
Our work thus provides essential resources and insights for advancing
egocentric video understanding. Our code is available at
https://github.com/LaVi-Lab/EgoMask .

</details>


### [102] [HyPCV-Former: Hyperbolic Spatio-Temporal Transformer for 3D Point Cloud Video Anomaly Detection](https://arxiv.org/abs/2508.00473)
*Jiaping Cao,Kangkang Zhou,Juan Du*

Main category: cs.CV

TL;DR: 提出了一种新的双曲时空Transformer，用于3D点云视频中的异常检测，并在TIMo和DAD数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 以往的方法利用RGB或深度域中的欧几里得表示，但这些嵌入在捕捉分层事件结构和时空连续性方面存在固有的局限性。

Method: 提出了一种新的双曲时空Transformer，用于3D点云视频中的异常检测。该方法首先通过点云提取器从点云序列中提取每帧空间特征，然后将其嵌入到洛伦兹双曲空间中，更好地捕捉事件的潜在分层结构。为了建模时间动态，引入了一种双曲多头自注意力（HMHA）机制，该机制利用洛伦兹内积和曲率感知softmax来学习非欧几里得几何下的时间依赖性。

Result: HyPCV-Former在多个异常类别上实现了最先进的性能，在TIMo数据集上提高了7%，在DAD数据集上提高了5.6%。

Conclusion: HyPCV-Former在多个异常类别上实现了最先进的性能，在TIMo数据集上提高了7%，在DAD数据集上提高了5.6%。

Abstract: Video anomaly detection is a fundamental task in video surveillance, with
broad applications in public safety and intelligent monitoring systems.
Although previous methods leverage Euclidean representations in RGB or depth
domains, such embeddings are inherently limited in capturing hierarchical event
structures and spatio-temporal continuity. To address these limitations, we
propose HyPCV-Former, a novel hyperbolic spatio-temporal transformer for
anomaly detection in 3D point cloud videos. Our approach first extracts
per-frame spatial features from point cloud sequences via point cloud
extractor, and then embeds them into Lorentzian hyperbolic space, which better
captures the latent hierarchical structure of events. To model temporal
dynamics, we introduce a hyperbolic multi-head self-attention (HMHA) mechanism
that leverages Lorentzian inner products and curvature-aware softmax to learn
temporal dependencies under non-Euclidean geometry. Our method performs all
feature transformations and anomaly scoring directly within full Lorentzian
space rather than via tangent space approximation. Extensive experiments
demonstrate that HyPCV-Former achieves state-of-the-art performance across
multiple anomaly categories, with a 7\% improvement on the TIMo dataset and a
5.6\% gain on the DAD dataset compared to benchmarks. The code will be released
upon paper acceptance.

</details>


### [103] [LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer](https://arxiv.org/abs/2508.00477)
*Yuzhuo Chen,Zehua Ma,Jianhua Wang,Kai Kang,Shunyu Yao,Weiming Zhang*

Main category: cs.CV

TL;DR: LAMIC is a training-free framework for multi-image composition that uses attention mechanisms to improve image generation and layout control.


<details>
  <summary>Details</summary>
Motivation: Generating coherent and consistent images from multiple references with spatial layout awareness remains an open challenge in controllable image synthesis.

Method: LAMIC introduces two plug-and-play attention mechanisms: Group Isolation Attention (GIA) and Region-Modulated Attention (RMA), built upon the MMDiT model.

Result: LAMIC achieves state-of-the-art performance across most major metrics, consistently outperforming existing multi-reference baselines.

Conclusion: LAMIC establishes a new training-free paradigm for controllable multi-image composition, with performance expected to scale with foundation models.

Abstract: In controllable image synthesis, generating coherent and consistent images
from multiple references with spatial layout awareness remains an open
challenge. We present LAMIC, a Layout-Aware Multi-Image Composition framework
that, for the first time, extends single-reference diffusion models to
multi-reference scenarios in a training-free manner. Built upon the MMDiT
model, LAMIC introduces two plug-and-play attention mechanisms: 1) Group
Isolation Attention (GIA) to enhance entity disentanglement; and 2)
Region-Modulated Attention (RMA) to enable layout-aware generation. To
comprehensively evaluate model capabilities, we further introduce three
metrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layout
control; and 2) Background Similarity (BG-S) for measuring background
consistency. Extensive experiments show that LAMIC achieves state-of-the-art
performance across most major metrics: it consistently outperforms existing
multi-reference baselines in ID-S, BG-S, IN-R and AVG scores across all
settings, and achieves the best DPG in complex composition tasks. These results
demonstrate LAMIC's superior abilities in identity keeping, background
preservation, layout control, and prompt-following, all achieved without any
training or fine-tuning, showcasing strong zero-shot generalization ability. By
inheriting the strengths of advanced single-reference models and enabling
seamless extension to multi-image scenarios, LAMIC establishes a new
training-free paradigm for controllable multi-image composition. As foundation
models continue to evolve, LAMIC's performance is expected to scale
accordingly. Our implementation is available at:
https://github.com/Suchenl/LAMIC.

</details>


### [104] [SAMSA 2.0: Prompting Segment Anything with Spectral Angles for Hyperspectral Interactive Medical Image Segmentation](https://arxiv.org/abs/2508.00493)
*Alfie Roddan,Tobias Czempiel,Chi Xu,Daniel S. Elson,Stamatia Giannarou*

Main category: cs.CV

TL;DR: SAMSA 2.0：一种用于高光谱医学成像的交互式分割框架，它通过光谱角度提示改进了SAM，实现了更准确、更稳健的分割。


<details>
  <summary>Details</summary>
Motivation: 高光谱医学成像的交互式分割框架

Method: 引入光谱角度提示来指导分割任何模型(SAM)，该模型使用光谱相似性以及空间线索。

Result: 与仅使用RGB的模型相比，SAMSA 2.0的Dice评分提高了3.8%，与之前的光谱融合方法相比提高了3.1%。

Conclusion: SAMSA 2.0在各种光谱数据集中实现了更准确、更稳健的分割，并且在具有挑战性的低数据和噪声场景中表现出强大的泛化能力。

Abstract: We present SAMSA 2.0, an interactive segmentation framework for hyperspectral
medical imaging that introduces spectral angle prompting to guide the Segment
Anything Model (SAM) using spectral similarity alongside spatial cues. This
early fusion of spectral information enables more accurate and robust
segmentation across diverse spectral datasets. Without retraining, SAMSA 2.0
achieves up to +3.8% higher Dice scores compared to RGB-only models and up to
+3.1% over prior spectral fusion methods. Our approach enhances few-shot and
zero-shot performance, demonstrating strong generalization in challenging
low-data and noisy scenarios common in clinical imaging.

</details>


### [105] [LesiOnTime -- Joint Temporal and Clinical Modeling for Small Breast Lesion Segmentation in Longitudinal DCE-MRI](https://arxiv.org/abs/2508.00496)
*Mohammed Kamran,Maria Bernathova,Raoul Varga,Christian Singer,Zsuzsanna Bago-Horvath,Thomas Helbich,Georg Langs,Philipp Seeböck*

Main category: cs.CV

TL;DR: 提出LesiOnTime，一种利用纵向影像和BIRADS评分进行乳腺病灶分割的方法，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 精确分割乳腺动态对比增强MRI（DCE-MRI）中的小病灶对于早期癌症检测至关重要，尤其是在高危患者中。现有的深度学习方法主要针对大病灶，忽略了放射科医生常用的有价值的纵向和临床信息。

Method: 提出了一种新的3D分割方法LesiOnTime，该方法通过联合利用纵向成像和BIRADS评分来模拟临床诊断工作流程。

Result: 在DCE-MRI高危患者的纵向数据集上评估，该方法在Dice方面比最先进的单时间点和纵向基线高出5%。

Conclusion: 该研究强调了在实际乳腺癌筛查中，纳入时间信息和临床背景对于可靠的早期病灶分割的重要性。

Abstract: Accurate segmentation of small lesions in Breast Dynamic Contrast-Enhanced
MRI (DCE-MRI) is critical for early cancer detection, especially in high-risk
patients. While recent deep learning methods have advanced lesion segmentation,
they primarily target large lesions and neglect valuable longitudinal and
clinical information routinely used by radiologists. In real-world screening,
detecting subtle or emerging lesions requires radiologists to compare across
timepoints and consider previous radiology assessments, such as the BI-RADS
score. We propose LesiOnTime, a novel 3D segmentation approach that mimics
clinical diagnostic workflows by jointly leveraging longitudinal imaging and
BIRADS scores. The key components are: (1) a Temporal Prior Attention (TPA)
block that dynamically integrates information from previous and current scans;
and (2) a BI-RADS Consistency Regularization (BCR) loss that enforces latent
space alignment for scans with similar radiological assessments, thus embedding
domain knowledge into the training process. Evaluated on a curated in-house
longitudinal dataset of high-risk patients with DCE-MRI, our approach
outperforms state-of-the-art single-timepoint and longitudinal baselines by 5%
in terms of Dice. Ablation studies demonstrate that both TPA and BCR contribute
complementary performance gains. These results highlight the importance of
incorporating temporal and clinical context for reliable early lesion
segmentation in real-world breast cancer screening. Our code is publicly
available at https://github.com/cirmuw/LesiOnTime

</details>


### [106] [Leveraging Convolutional and Graph Networks for an Unsupervised Remote Sensing Labelling Tool](https://arxiv.org/abs/2508.00506)
*Tulsi Patel,Mark W. Jones,Thomas Redfern*

Main category: cs.CV

TL;DR: 提出了一种无监督的流水线，用于在Sentinel-2卫星图像中寻找和标记具有相似上下文和内容的地理区域。


<details>
  <summary>Details</summary>
Motivation: 遥感图像的机器学习依赖于最新的和准确的标签来进行模型训练和测试。遥感图像的标记是耗时且成本高昂的，需要专家分析。以前的标记工具依赖于预先标记的数据进行训练，以便标记新的未见过的数据。

Method: 利用卷积和图神经网络进行分割

Result: 减少了标记工具中的异常值，允许用户以精细的水平进行标记，并允许在编码空间内形成图像级别的旋转不变的语义关系。

Conclusion: 无监督流程寻找和标记地理区域

Abstract: Machine learning for remote sensing imaging relies on up-to-date and accurate
labels for model training and testing. Labelling remote sensing imagery is time
and cost intensive, requiring expert analysis. Previous labelling tools rely on
pre-labelled data for training in order to label new unseen data. In this work,
we define an unsupervised pipeline for finding and labelling geographical areas
of similar context and content within Sentinel-2 satellite imagery. Our
approach removes limitations of previous methods by utilising segmentation with
convolutional and graph neural networks to encode a more robust feature space
for image comparison. Unlike previous approaches we segment the image into
homogeneous regions of pixels that are grouped based on colour and spatial
similarity. Graph neural networks are used to aggregate information about the
surrounding segments enabling the feature representation to encode the local
neighbourhood whilst preserving its own local information. This reduces
outliers in the labelling tool, allows users to label at a granular level, and
allows a rotationally invariant semantic relationship at the image level to be
formed within the encoding space.

</details>


### [107] [Wukong Framework for Not Safe For Work Detection in Text-to-Image systems](https://arxiv.org/abs/2508.00591)
*Mingrui Liu,Sixiao Zhang,Cheng Long*

Main category: cs.CV

TL;DR: Wukong是一种用于检测AI生成图像中NSFW内容的新框架，它比现有方法更有效。


<details>
  <summary>Details</summary>
Motivation: 一些输出可能包含不适合工作场所的内容（例如，暴力），违反社区准则。高效、准确地检测NSFW内容至关重要。

Method: 提出了Wukong，这是一个基于Transformer的NSFW检测框架，它利用了早期去噪步骤中的中间输出，并重复使用U-Net的预训练交叉注意力参数。

Result: Wukong明显优于基于文本的安全措施，并在准确性方面达到了与图像过滤器相当的水平，同时提供了更高的效率。

Conclusion: Wukong在效率上大大超过了基于文本的安全措施，并在准确性方面达到了与图像过滤器相当的水平。

Abstract: Text-to-Image (T2I) generation is a popular AI-generated content (AIGC)
technology enabling diverse and creative image synthesis. However, some outputs
may contain Not Safe For Work (NSFW) content (e.g., violence), violating
community guidelines. Detecting NSFW content efficiently and accurately, known
as external safeguarding, is essential. Existing external safeguards fall into
two types: text filters, which analyze user prompts but overlook T2I
model-specific variations and are prone to adversarial attacks; and image
filters, which analyze final generated images but are computationally costly
and introduce latency. Diffusion models, the foundation of modern T2I systems
like Stable Diffusion, generate images through iterative denoising using a
U-Net architecture with ResNet and Transformer blocks. We observe that: (1)
early denoising steps define the semantic layout of the image, and (2)
cross-attention layers in U-Net are crucial for aligning text and image
regions. Based on these insights, we propose Wukong, a transformer-based NSFW
detection framework that leverages intermediate outputs from early denoising
steps and reuses U-Net's pre-trained cross-attention parameters. Wukong
operates within the diffusion process, enabling early detection without waiting
for full image generation. We also introduce a new dataset containing prompts,
seeds, and image-specific NSFW labels, and evaluate Wukong on this and two
public benchmarks. Results show that Wukong significantly outperforms
text-based safeguards and achieves comparable accuracy of image filters, while
offering much greater efficiency.

</details>


### [108] [EPANet: Efficient Path Aggregation Network for Underwater Fish Detection](https://arxiv.org/abs/2508.00528)
*Jinsong Yang,Zeyuan Hu,Yichen Li*

Main category: cs.CV

TL;DR: 提出了一种新的水下鱼类检测网络EPANet，该网络更准确、更快速，且参数复杂度更低。


<details>
  <summary>Details</summary>
Motivation: 水下鱼类检测（UFD）仍然是计算机视觉中一项具有挑战性的任务，由于物体分辨率低、背景干扰大以及目标与周围环境之间视觉相似性高。现有的方法主要集中在局部特征增强或结合复杂的注意力机制来突出小物体，但通常以增加模型复杂性和降低效率为代价。

Method: 提出了一种高效的路径聚合网络（EPANet），它利用互补特征集成来实现准确和轻量级的水下鱼类检测。EPANet由两个关键组件组成：高效的路径聚合特征金字塔网络（EPA-FPN）和多尺度多样化短路径瓶颈（MS-DDSP瓶颈）。

Result: 在基准UFD数据集上的大量实验表明，EPANet在检测精度和推理速度方面优于最先进的方法，同时保持了相当甚至更低的参数复杂度。

Conclusion: EPANet在水下鱼类检测基准数据集上优于现有技术方法，在检测精度和推理速度方面表现出色，同时保持了相当甚至更低的参数复杂度。

Abstract: Underwater fish detection (UFD) remains a challenging task in computer vision
due to low object resolution, significant background interference, and high
visual similarity between targets and surroundings. Existing approaches
primarily focus on local feature enhancement or incorporate complex attention
mechanisms to highlight small objects, often at the cost of increased model
complexity and reduced efficiency. To address these limitations, we propose an
efficient path aggregation network (EPANet), which leverages complementary
feature integration to achieve accurate and lightweight UFD. EPANet consists of
two key components: an efficient path aggregation feature pyramid network
(EPA-FPN) and a multi-scale diverse-division short path bottleneck (MS-DDSP
bottleneck). The EPA-FPN introduces long-range skip connections across
disparate scales to improve semantic-spatial complementarity, while cross-layer
fusion paths are adopted to enhance feature integration efficiency. The MS-DDSP
bottleneck extends the conventional bottleneck structure by introducing
finer-grained feature division and diverse convolutional operations, thereby
increasing local feature diversity and representation capacity. Extensive
experiments on benchmark UFD datasets demonstrate that EPANet outperforms
state-of-the-art methods in terms of detection accuracy and inference speed,
while maintaining comparable or even lower parameter complexity.

</details>


### [109] [Video Color Grading via Look-Up Table Generation](https://arxiv.org/abs/2508.00548)
*Seunghyun Shin,Dongmin Shin,Jisu Shin,Hae-Gon Jeon,Joon-Young Lee*

Main category: cs.CV

TL;DR: This paper presents a video color grading framework using a diffusion model to generate LUTs, aligning color attributes between reference scenes and input video, while preserving structural details and allowing user preference adjustments via text prompts.


<details>
  <summary>Details</summary>
Motivation: Video color grading is complex, requires specialized skills, and is primarily done by professional colorists.

Method: A reference-based video color grading framework that generates a look-up table (LUT) for color attribute alignment between reference scenes and input video via a diffusion model.

Result: The LUT-based approach allows for color grading without loss of structural details and achieves fast inference. A pipeline incorporates user preferences via text prompts for low-level feature enhancement.

Conclusion: The experimental results and user studies demonstrate the effectiveness of the proposed approach for video color grading.

Abstract: Different from color correction and transfer, color grading involves
adjusting colors for artistic or storytelling purposes in a video, which is
used to establish a specific look or mood. However, due to the complexity of
the process and the need for specialized editing skills, video color grading
remains primarily the domain of professional colorists. In this paper, we
present a reference-based video color grading framework. Our key idea is
explicitly generating a look-up table (LUT) for color attribute alignment
between reference scenes and input video via a diffusion model. As a training
objective, we enforce that high-level features of the reference scenes like
look, mood, and emotion should be similar to that of the input video. Our
LUT-based approach allows for color grading without any loss of structural
details in the whole video frames as well as achieving fast inference. We
further build a pipeline to incorporate a user-preference via text prompts for
low-level feature enhancement such as contrast and brightness, etc.
Experimental results, including extensive user studies, demonstrate the
effectiveness of our approach for video color grading. Codes are publicly
available at https://github.com/seunghyuns98/VideoColorGrading.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [110] [Rethinking Evidence Hierarchies in Medical Language Benchmarks: A Critical Evaluation of HealthBench](https://arxiv.org/abs/2508.00081)
*Fred Mutisya,Shikoh Gitau,Nasubo Ongoma,Keith Mbae,Elizabeth Wamicha*

Main category: cs.AI

TL;DR: HealthBench 虽然通过医生精心设计的对话和透明的规则改进了医学语言模型评估，但它依赖专家意见而非高级临床证据，这可能会导致偏见。为了解决这些缺点，我们提出将奖励函数锚定在版本控制的临床实践指南 (CPG) 中，这些指南纳入了系统评价和 GRADE 证据评级。


<details>
  <summary>Details</summary>
Motivation: HealthBench 依赖专家意见而非高级临床证据，这可能会导致区域偏见和个体临床医生特质的编码，并且自动评分系统中的潜在偏见会进一步加剧这种情况。在撒哈拉以南非洲地区，数据稀缺、基础设施不足和新兴监管框架等问题，强调了迫切需要更具全球相关性和公平性的基准。

Method: 通过 rubric-to-guideline 链接、证据加权评分和上下文覆盖逻辑，我们的路线图概述了“证据可靠”的强化学习，并辅以对伦理考量和整合延迟结果反馈的关注。

Result: 我们建议将奖励函数锚定在版本控制的临床实践指南 (CPG) 中，这些指南纳入了系统评价和 GRADE 证据评级。

Conclusion: 通过将奖励重新置于经过严格审查的 CPG 中，同时保持 HealthBench 的透明度和医生参与度，我们的目标是培养不仅在语言上精雕细琢，而且在临床上值得信赖、符合伦理且具有全球相关性的医学语言模型。

Abstract: HealthBench, a benchmark designed to measure the capabilities of AI systems
for health better (Arora et al., 2025), has advanced medical language model
evaluation through physician-crafted dialogues and transparent rubrics.
However, its reliance on expert opinion, rather than high-tier clinical
evidence, risks codifying regional biases and individual clinician
idiosyncrasies, further compounded by potential biases in automated grading
systems. These limitations are particularly magnified in low- and middle-income
settings, where issues like sparse neglected tropical disease coverage and
region-specific guideline mismatches are prevalent.
  The unique challenges of the African context, including data scarcity,
inadequate infrastructure, and nascent regulatory frameworks, underscore the
urgent need for more globally relevant and equitable benchmarks. To address
these shortcomings, we propose anchoring reward functions in version-controlled
Clinical Practice Guidelines (CPGs) that incorporate systematic reviews and
GRADE evidence ratings.
  Our roadmap outlines "evidence-robust" reinforcement learning via
rubric-to-guideline linkage, evidence-weighted scoring, and contextual override
logic, complemented by a focus on ethical considerations and the integration of
delayed outcome feedback. By re-grounding rewards in rigorously vetted CPGs,
while preserving HealthBench's transparency and physician engagement, we aim to
foster medical language models that are not only linguistically polished but
also clinically trustworthy, ethically sound, and globally relevant.

</details>


### [111] [Hyperproperty-Constrained Secure Reinforcement Learning](https://arxiv.org/abs/2508.00106)
*Ernest Bonnah,Luan Viet Nguyen,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: 该论文提出了一种新的安全强化学习方法，该方法使用超性质来保证安全性，并在机器人任务中取得了优异的效果。


<details>
  <summary>Details</summary>
Motivation: 探索使用超性质的安全感知强化学习 (RL) 方面的研究差距。

Method: 使用动态 Boltzmann softmax RL。

Result: 在拾取和交付机器人任务案例研究中证明了所提出方法的有效性和可扩展性。与其他两种基线 RL 算法相比，该方法优于它们。

Conclusion: 该论文提出了一种使用动态 Boltzmann softmax RL 学习安全感知最优策略的方法，该方法满足 HyperTWTL 约束。

Abstract: Hyperproperties for Time Window Temporal Logic (HyperTWTL) is a
domain-specific formal specification language known for its effectiveness in
compactly representing security, opacity, and concurrency properties for
robotics applications. This paper focuses on HyperTWTL-constrained secure
reinforcement learning (SecRL). Although temporal logic-constrained safe
reinforcement learning (SRL) is an evolving research problem with several
existing literature, there is a significant research gap in exploring
security-aware reinforcement learning (RL) using hyperproperties. Given the
dynamics of an agent as a Markov Decision Process (MDP) and opacity/security
constraints formalized as HyperTWTL, we propose an approach for learning
security-aware optimal policies using dynamic Boltzmann softmax RL while
satisfying the HyperTWTL constraints. The effectiveness and scalability of our
proposed approach are demonstrated using a pick-up and delivery robotic mission
case study. We also compare our results with two other baseline RL algorithms,
showing that our proposed method outperforms them.

</details>


### [112] [No AI Without PI! Object-Centric Process Mining as the Enabler for Generative, Predictive, and Prescriptive Artificial Intelligence](https://arxiv.org/abs/2508.00116)
*Wil M. P. van der Aalst*

Main category: cs.AI

TL;DR: AI在工业环境中的应用面临挑战，OCPM是连接数据和流程的关键，流程智能(PI)的融合可在组织环境中实现AI。


<details>
  <summary>Details</summary>
Motivation: 人工智能（AI）的普及影响了我们的工作、互动、经商和研究方式。然而，各组织在工业环境中成功应用AI方面面临困难，因为工业环境侧重于端到端的运营流程。

Method: 考虑生成式、预测式和规范式AI，并详细阐述了诊断和改进这些流程的挑战。论证了AI需要以面向对象的过程挖掘（OCPM）为基础。使用流程智能（PI）指代以流程为中心的、数据驱动的技术的融合，这些技术能够处理各种对象和事件类型，从而在组织环境中实现AI。

Result: 过程相关数据是结构化的且特定于组织的，并且与文本不同，过程通常是高度动态的。OCPM是连接数据和过程的缺失环节，并支持不同形式的AI。

Conclusion: AI需要以流程智能（PI）来改进操作流程，并强调了成功结合面向对象的过程挖掘（OCPM）与生成式、预测式和规范式AI的机会。

Abstract: The uptake of Artificial Intelligence (AI) impacts the way we work, interact,
do business, and conduct research. However, organizations struggle to apply AI
successfully in industrial settings where the focus is on end-to-end
operational processes. Here, we consider generative, predictive, and
prescriptive AI and elaborate on the challenges of diagnosing and improving
such processes. We show that AI needs to be grounded using Object-Centric
Process Mining (OCPM). Process-related data are structured and
organization-specific and, unlike text, processes are often highly dynamic.
OCPM is the missing link connecting data and processes and enables different
forms of AI. We use the term Process Intelligence (PI) to refer to the
amalgamation of process-centric data-driven techniques able to deal with a
variety of object and event types, enabling AI in an organizational context.
This paper explains why AI requires PI to improve operational processes and
highlights opportunities for successfully combining OCPM and generative,
predictive, and prescriptive AI.

</details>


### [113] [MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning](https://arxiv.org/abs/2508.00271)
*Hongjin Qian,Zheng Liu*

Main category: cs.AI

TL;DR: MetaAgent通过持续的自我改进和元工具学习，在知识发现任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了解决知识发现中的挑战，提出了MetaAgent。

Method: MetaAgent，一个受实践学习原则启发的agentic范例，通过实践和持续的自我改进来发展专业知识。

Result: MetaAgent持续优于基于工作流程的基线，并匹配或超过端到端训练的agents。

Conclusion: MetaAgent在知识发现基准测试中表现出色，证明了自我进化agentic系统的前景。

Abstract: In this work, we propose MetaAgent, an agentic paradigm inspired by the
principle of learning-by-doing, where expertise is developed through hands-on
practice and continual self-improvement. MetaAgent starts with a minimal
workflow, equipped only with basic reasoning and adaptive help-seeking
abilities. When a knowledge gap is encountered, MetaAgent generates natural
language help requests, which are routed to the most suitable external tool by
a dedicated tool router. As MetaAgent solves tasks, it continually conducts
self-reflection and answer verification, distilling actionable experience into
concise texts that are dynamically incorporated into future task contexts.
Besides, MetaAgent autonomously builds in-house tools and a persistent
knowledge base by organizing its tool-use history, further enhancing its
ability to retrieve and integrate relevant information We term this continual,
data-driven process as \textit{meta tool learning}, through which MetaAgent
incrementally refines its reasoning and tool-use strategies, without changing
model parameters or requiring further post-training. Evaluated on challenging
knowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp,
MetaAgent consistently outperforms workflow-based baselines and matches or
exceeds end-to-end trained agents, demonstrating the promise of self-evolving
agentic systems for robust, general-purpose knowledge discovery. We provide our
source codes in https://github.com/qhjqhj00/MetaAgent.

</details>


### [114] [Algorithmic Detection of Rank Reversals, Transitivity Violations, and Decomposition Inconsistencies in Multi-Criteria Decision Analysis](https://arxiv.org/abs/2508.00129)
*Agustín Borda,Juan Bautista Cabral,Gonzalo Giarda,Diego Nicolás Gimenez Irusta,Paula Pacheco,Alvaro Roy Schachner*

Main category: cs.AI

TL;DR: This paper introduces three tests for rank reversals in multi-criteria decision analysis, implemented in Scikit-Criteria, which can help evaluate the effectiveness of different decision-making methods.


<details>
  <summary>Details</summary>
Motivation: Rank Reversals are a serious problem that can greatly affect the results of a Multi-Criteria Decision Method. It is therefore useful to have a mechanism that allows one to measure the performance of a method on a set of alternatives.

Method: Three tests to detect the presence of Rank Reversals, along with their implementation in the Scikit-Criteria library.

Result: The implementation of the tests in the Scikit-Criteria library and a discussion about how these additions could play a major role in the judgment of multi-criteria decision methods for problem solving.

Conclusion: This paper presents three tests for detecting rank reversals and their implementation in the Scikit-Criteria library. It also discusses the complications of implementing these tests and their potential role in judging multi-criteria decision methods.

Abstract: In Multi-Criteria Decision Analysis, Rank Reversals are a serious problem
that can greatly affect the results of a Multi-Criteria Decision Method against
a particular set of alternatives. It is therefore useful to have a mechanism
that allows one to measure the performance of a method on a set of
alternatives. This idea could be taken further to build a global ranking of the
effectiveness of different methods to solve a problem. In this paper, we
present three tests that detect the presence of Rank Reversals, along with
their implementation in the Scikit-Criteria library. We also address the
complications that arise when implementing these tests for general scenarios
and the design considerations we made to handle them. We close with a
discussion about how these additions could play a major role in the judgment of
multi-criteria decision methods for problem solving.

</details>


### [115] [SHACL Validation under Graph Updates (Extended Paper)](https://arxiv.org/abs/2508.00137)
*Shqiponja Ahmetaj,George Konstantinidis,Magdalena Ortiz,Paolo Pareti,Mantas Simkus*

Main category: cs.AI

TL;DR: 本文研究了RDF图在更新下的SHACL验证问题，并提出了一种基于SHACL的更新语言。


<details>
  <summary>Details</summary>
Motivation: 研究RDF图在更新下的SHACL验证。

Method: 提出了一种基于SHACL的更新语言，该语言可以捕获RDF图上的直观和真实的修改，并研究了在此类更新下静态验证的问题。使用回归技术，将更新操作嵌入到SHACL约束中。

Result: 静态验证可以简化为SHACL约束的（非）满足性问题。对SHACL及其一些关键片段的静态验证问题的计算复杂性进行了分析。提出了一个原型实现，该实现对SHACL约束执行静态验证和其他静态分析任务，并通过初步实验证明了其行为。

Conclusion: 通过将更新操作嵌入到SHACL约束中，静态验证可以简化为SHACL约束的（非）满足性问题。对SHACL及其一些关键片段的静态验证问题的计算复杂性进行了分析。提出了一个原型实现，该实现对SHACL约束执行静态验证和其他静态分析任务，并通过初步实验证明了其行为。

Abstract: SHACL (SHApe Constraint Language) is a W3C standardized constraint language
for RDF graphs. In this paper, we study SHACL validation in RDF graphs under
updates. We present a SHACL-based update language that can capture intuitive
and realistic modifications on RDF graphs and study the problem of static
validation under such updates. This problem asks to verify whether every graph
that validates a SHACL specification will still do so after applying a given
update sequence. More importantly, it provides a basis for further services for
reasoning about evolving RDF graphs. Using a regression technique that embeds
the update actions into SHACL constraints, we show that static validation under
updates can be reduced to (un)satisfiability of constraints in (a minor
extension of) SHACL. We analyze the computational complexity of the static
validation problem for SHACL and some key fragments. Finally, we present a
prototype implementation that performs static validation and other static
analysis tasks on SHACL constraints and demonstrate its behavior through
preliminary experiments.

</details>


### [116] [Co-Producing AI: Toward an Augmented, Participatory Lifecycle](https://arxiv.org/abs/2508.00138)
*Rashid Mushkani,Hugo Berard,Toumadher Ammar,Cassandre Chatonnier,Shin Koseki*

Main category: cs.AI

TL;DR: 为了减轻人工智能算法对文化边缘群体的影响，本文提出了一种增强的AI生命周期，该生命周期以共同生产、多样性、公平、包容和多学科协作为中心。


<details>
  <summary>Details</summary>
Motivation: 尽管人们努力减轻人工智能（AI）算法的内在风险和偏见，但这些算法可能会对文化边缘群体产生过度的影响。已经提出了许多方法来解决或降低这些风险，包括为负责任的人工智能制定道德准则和原则，以及促进算法公平的技术解决方案。

Method: 本文借鉴了设计正义、扩展学习理论和参与式AI的最新实证研究，论证了缓解这些危害需要对AI生产流程进行根本性的重新架构。我们介绍了一个增强的AI生命周期，它由五个相互关联的阶段组成：共同框架、共同设计、共同实施、共同部署和共同维护。该生命周期以四个多学科研讨会为基础，并以分布式权威和迭代知识交流为主题。

Result: 重新设计应该以共同生产、多样性、公平、包容（DEI）和多学科协作为中心。

Conclusion: 为了扩大参与式治理，本文提出了增强的AI生命周期，并将其与一些主要的伦理框架联系起来，概述了仍然存在的关键研究问题。

Abstract: Despite efforts to mitigate the inherent risks and biases of artificial
intelligence (AI) algorithms, these algorithms can disproportionately impact
culturally marginalized groups. A range of approaches has been proposed to
address or reduce these risks, including the development of ethical guidelines
and principles for responsible AI, as well as technical solutions that promote
algorithmic fairness. Drawing on design justice, expansive learning theory, and
recent empirical work on participatory AI, we argue that mitigating these harms
requires a fundamental re-architecture of the AI production pipeline. This
re-design should center co-production, diversity, equity, inclusion (DEI), and
multidisciplinary collaboration. We introduce an augmented AI lifecycle
consisting of five interconnected phases: co-framing, co-design,
co-implementation, co-deployment, and co-maintenance. The lifecycle is informed
by four multidisciplinary workshops and grounded in themes of distributed
authority and iterative knowledge exchange. Finally, we relate the proposed
lifecycle to several leading ethical frameworks and outline key research
questions that remain for scaling participatory governance.

</details>


### [117] [Beyond Agreement: Rethinking Ground Truth in Educational AI Annotation](https://arxiv.org/abs/2508.00143)
*Danielle R. Thomas,Conrad Borchers,Kenneth R. Koedinger*

Main category: cs.AI

TL;DR: 重新思考标注质量和ground truth，优先考虑有效性和教育影响，而不仅仅是共识。


<details>
  <summary>Details</summary>
Motivation: 在教育应用中使用人工智能产生大量训练数据的需求激增，但传统的评分者间信度（IRR）指标仍然是验证标注数据的核心，然而人类评估者通常是不完美的，存在偏差、不可靠且不适合定义“ground truth”。

Method: 提出了五种互补的评估方法，例如多标签标注方案、基于专家的方法和闭环验证。

Result: 这些方法更有利于产生训练数据和后续模型，从而改进学生学习并提供比单独使用 IRR 方法更具可操作性的见解。强调了外部有效性的重要性，例如，通过建立验证导师行为的程序并证明它适用于许多类别的导师行为（例如，提供提示）。

Conclusion: 过度依赖人工评分者信度（IRR）作为标注质量的衡量标准，阻碍了数据分类的进展，而这种分类本可以有效预测并改进学习。

Abstract: Humans can be notoriously imperfect evaluators. They are often biased,
unreliable, and unfit to define "ground truth." Yet, given the surging need to
produce large amounts of training data in educational applications using AI,
traditional inter-rater reliability (IRR) metrics like Cohen's kappa remain
central to validating labeled data. IRR remains a cornerstone of many machine
learning pipelines for educational data. Take, for example, the classification
of tutors' moves in dialogues or labeling open responses in machine-graded
assessments. This position paper argues that overreliance on human IRR as a
gatekeeper for annotation quality hampers progress in classifying data in ways
that are valid and predictive in relation to improving learning. To address
this issue, we highlight five examples of complementary evaluation methods,
such as multi-label annotation schemes, expert-based approaches, and
close-the-loop validity. We argue that these approaches are in a better
position to produce training data and subsequent models that produce improved
student learning and more actionable insights than IRR approaches alone. We
also emphasize the importance of external validity, for example, by
establishing a procedure of validating tutor moves and demonstrating that it
works across many categories of tutor actions (e.g., providing hints). We call
on the field to rethink annotation quality and ground truth--prioritizing
validity and educational impact over consensus alone.

</details>


### [118] [Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power](https://arxiv.org/abs/2508.00159)
*Jobst Heitzig,Ram Potham*

Main category: cs.AI

TL;DR: 本文探讨了通过迫使人工智能代理明确地增强人类的力量，并以理想的方式管理人与人工智能代理之间的权力平衡，从而促进安全和福祉的想法。


<details>
  <summary>Details</summary>
Motivation: 权力是人工智能安全中的一个关键概念：寻求权力作为一种工具性目标，人类突然或逐渐的权力丧失，人机互动和国际人工智能治理中的权力平衡。同时，权力作为追求多样化目标的能力，对福祉至关重要。

Method: 使用原则性的、部分公理化的方法，我们设计了一个可参数化和可分解的目标函数，该函数表示人类力量的不平等和风险规避的长期聚合。

Result: 我们推导了通过逆向归纳计算该指标或通过给定世界模型的某种形式的多智能体强化学习来近似该指标的算法。我们例证了在各种范例情况下（软性地）最大化该指标的后果，并描述了它可能暗示的工具性子目标。

Conclusion: 软性地最大化人类力量的合适聚合指标可能构成对智能体AI系统有益的目标，比直接基于效用的目标更安全。

Abstract: Power is a key concept in AI safety: power-seeking as an instrumental goal,
sudden or gradual disempowerment of humans, power balance in human-AI
interaction and international AI governance. At the same time, power as the
ability to pursue diverse goals is essential for wellbeing.
  This paper explores the idea of promoting both safety and wellbeing by
forcing AI agents explicitly to empower humans and to manage the power balance
between humans and AI agents in a desirable way. Using a principled, partially
axiomatic approach, we design a parametrizable and decomposable objective
function that represents an inequality- and risk-averse long-term aggregate of
human power. It takes into account humans' bounded rationality and social
norms, and, crucially, considers a wide variety of possible human goals.
  We derive algorithms for computing that metric by backward induction or
approximating it via a form of multi-agent reinforcement learning from a given
world model. We exemplify the consequences of (softly) maximizing this metric
in a variety of paradigmatic situations and describe what instrumental
sub-goals it will likely imply. Our cautious assessment is that softly
maximizing suitable aggregate metrics of human power might constitute a
beneficial objective for agentic AI systems that is safer than direct
utility-based objectives.

</details>


### [119] [RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization](https://arxiv.org/abs/2508.00222)
*Yihong Dong,Xue Jiang,Yongding Tao,Huanyu Liu,Kechi Zhang,Lili Mou,Rongyu Cao,Yingwei Ma,Jue Chen,Binhua Li,Zhi Jin,Fei Huang,Yongbin Li,Ge Li*

Main category: cs.AI

TL;DR: RL-PLUS结合内部利用和外部数据，通过多次重要性采样和基于探索的优势函数，提升了LLM的推理能力，解决了能力边界崩溃问题，并在数学推理和分布外推理任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 强化学习与可验证奖励（RLVR）显着提高了大型语言模型（LLM）的复杂推理能力。然而，由于其固有的基于策略的策略与LLM的巨大行动空间和稀疏奖励，它难以突破基础LLM的固有能力边界。此外，RLVR可能导致能力边界崩溃，缩小LLM的问题解决范围。

Method: RL-PLUS，一种结合内部利用（即思考）与外部数据（即学习）的新方法，以实现更强的推理能力并超越基础模型的边界。RL-PLUS集成了两个核心组件：用于解决来自外部数据的分布不匹配的多次重要性采样，以及基于探索的优势函数，以引导模型走向高价值、未探索的推理路径。

Result: RL-PLUS在六个数学推理基准测试上取得了最先进的性能，并在六个分布外推理任务上表现出卓越的性能。它还在不同的模型系列中实现了持续且显着的收益，平均相对改进范围从21.1％到69.2％。

Conclusion: RL-PLUS在六个数学推理基准测试上取得了最先进的性能，并在六个分布外推理任务上表现出卓越的性能。它还在不同的模型系列中实现了持续且显着的收益，平均相对改进范围从21.1％到69.2％。此外，多个基准测试的Pass@k曲线表明，RL-PLUS有效地解决了能力边界崩溃问题。

Abstract: Reinforcement Learning with Verifiable Reward (RLVR) has significantly
advanced the complex reasoning abilities of Large Language Models (LLMs).
However, it struggles to break through the inherent capability boundaries of
the base LLM, due to its inherently on-policy strategy with LLM's immense
action space and sparse reward. Further, RLVR can lead to the capability
boundary collapse, narrowing the LLM's problem-solving scope. To address this
problem, we propose RL-PLUS, a novel approach that synergizes internal
exploitation (i.e., Thinking) with external data (i.e., Learning) to achieve
stronger reasoning capabilities and surpass the boundaries of base models.
RL-PLUS integrates two core components: Multiple Importance Sampling to address
for distributional mismatch from external data, and an Exploration-Based
Advantage Function to guide the model towards high-value, unexplored reasoning
paths. We provide both theoretical analysis and extensive experiments to
demonstrate the superiority and generalizability of our approach. The results
show that RL-PLUS achieves state-of-the-art performance compared with existing
RLVR methods on six math reasoning benchmarks and exhibits superior performance
on six out-of-distribution reasoning tasks. It also achieves consistent and
significant gains across diverse model families, with average relative
improvements ranging from 21.1\% to 69.2\%. Moreover, Pass@k curves across
multiple benchmarks indicate that RL-PLUS effectively resolves the capability
boundary collapse problem.

</details>


### [120] [Mind the Gap: The Divergence Between Human and LLM-Generated Tasks](https://arxiv.org/abs/2508.00282)
*Yi-Long Lu,Jiajun Song,Chunhui Zhang,Wei Wang*

Main category: cs.AI

TL;DR: 人类的任务生成受到心理因素驱动，而大型语言模型即使被告知这些因素，也无法模仿人类的行为模式，表明大型语言模型在价值驱动和具身认知方面存在不足。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型驱动的生成式智能体是否以与人类相似的认知原则运行。大型语言模型旨在模拟人类由内在动机引导的复杂行为，但这一点尚不确定。

Method: 对比人类和大型语言模型（GPT-4o）的任务生成实验

Result: 人类的任务生成始终受到心理驱动因素的影响，包括个人价值观和认知方式。即使向大型语言模型明确提供这些心理驱动因素，它也无法反映相应的行为模式。他们产生的任务在社交性、物理性和主题抽象性方面明显不足。有趣的是，虽然大型语言模型的任务被认为更有趣和新颖，但这突显了其语言能力与其生成类人、具身目标的能力之间的脱节。

Conclusion: 价值驱动和具身认知是人类认知的核心，而大型语言模型在这些方面存在差距，因此需要将内在动机和物理基础融入到更符合人类习惯的智能体的设计中。

Abstract: Humans constantly generate a diverse range of tasks guided by internal
motivations. While generative agents powered by large language models (LLMs)
aim to simulate this complex behavior, it remains uncertain whether they
operate on similar cognitive principles. To address this, we conducted a
task-generation experiment comparing human responses with those of an LLM agent
(GPT-4o). We find that human task generation is consistently influenced by
psychological drivers, including personal values (e.g., Openness to Change) and
cognitive style. Even when these psychological drivers are explicitly provided
to the LLM, it fails to reflect the corresponding behavioral patterns. They
produce tasks that are markedly less social, less physical, and thematically
biased toward abstraction. Interestingly, while the LLM's tasks were perceived
as more fun and novel, this highlights a disconnect between its linguistic
proficiency and its capacity to generate human-like, embodied goals.We conclude
that there is a core gap between the value-driven, embodied nature of human
cognition and the statistical patterns of LLMs, highlighting the necessity of
incorporating intrinsic motivation and physical grounding into the design of
more human-aligned agents.

</details>


### [121] [Oedipus and the Sphinx: Benchmarking and Improving Visual Language Models for Complex Graphic Reasoning](https://arxiv.org/abs/2508.00323)
*Jianyi Zhang,Xu Ji,Ziyin Zhou,Yuchen Zhou,Shubo Shi,Haoyu Wu,Zhen Li,Shizhao Liu*

Main category: cs.AI

TL;DR: This paper introduces ReasonBench, a new benchmark for evaluating VLMs in complex graphic reasoning, and proposes a dual optimization strategy (DiaCoT and ReasonTune) to improve VLM performance.


<details>
  <summary>Details</summary>
Motivation: VLMs still show obvious deficiencies in simulating human-level graphic reasoning capabilities, especially in complex graphic reasoning and abstract problem solving, which are less studied and existing studies only focus on simple graphics. To evaluate the performance of VLMs in complex graphic reasoning

Method: ReasonBench, the first evaluation benchmark focused on structured graphic reasoning tasks, which includes 1,613 questions from real-world intelligence tests. ReasonBench covers reasoning dimensions related to location, attribute, quantity, and multi-element tasks, providing a comprehensive evaluation of the performance of VLMs in spatial, relational, and abstract reasoning capabilities. We propose a dual optimization strategy: Diagrammatic Reasoning Chain (DiaCoT) enhances the interpretability of reasoning by decomposing layers, and ReasonTune enhances the task adaptability of model reasoning through training

Result: DiaCoT enhances the interpretability of reasoning by decomposing layers, and ReasonTune enhances the task adaptability of model reasoning through training, all of which improves VLM performance by 33.5%

Conclusion: We benchmark 11 mainstream VLMs (including closed-source and open-source models) and reveal significant limitations of current models. Based on these findings, we propose a dual optimization strategy: Diagrammatic Reasoning Chain (DiaCoT) enhances the interpretability of reasoning by decomposing layers, and ReasonTune enhances the task adaptability of model reasoning through training, all of which improves VLM performance by 33.5%.

Abstract: Evaluating the performance of visual language models (VLMs) in graphic
reasoning tasks has become an important research topic. However, VLMs still
show obvious deficiencies in simulating human-level graphic reasoning
capabilities, especially in complex graphic reasoning and abstract problem
solving, which are less studied and existing studies only focus on simple
graphics. To evaluate the performance of VLMs in complex graphic reasoning, we
propose ReasonBench, the first evaluation benchmark focused on structured
graphic reasoning tasks, which includes 1,613 questions from real-world
intelligence tests. ReasonBench covers reasoning dimensions related to
location, attribute, quantity, and multi-element tasks, providing a
comprehensive evaluation of the performance of VLMs in spatial, relational, and
abstract reasoning capabilities. We benchmark 11 mainstream VLMs (including
closed-source and open-source models) and reveal significant limitations of
current models. Based on these findings, we propose a dual optimization
strategy: Diagrammatic Reasoning Chain (DiaCoT) enhances the interpretability
of reasoning by decomposing layers, and ReasonTune enhances the task
adaptability of model reasoning through training, all of which improves VLM
performance by 33.5\%. All experimental data and code are in the repository:
https://huggingface.co/datasets/cistine/ReasonBench.

</details>


### [122] [R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge](https://arxiv.org/abs/2508.00324)
*Yeonjun In,Wonjoong Kim,Sangwu Park,Chanyoung Park*

Main category: cs.AI

TL;DR: This paper proposes R1-Act, a method to improve the safety of large reasoning models by explicitly triggering safety knowledge during reasoning. It achieves strong safety improvements with minimal training and computational cost.


<details>
  <summary>Details</summary>
Motivation: LRMs frequently fulfill harmful user instructions, raising significant safety concerns.  models already possess sufficient safety knowledge but fail to activate it during reasoning.

Method: R1-Act, a simple and efficient post-training method that explicitly triggers safety knowledge through a structured reasoning process.

Result: R1-Act achieves strong safety improvements while preserving reasoning performance, outperforming prior alignment methods.

Conclusion: R1-Act achieves strong safety improvements while preserving reasoning performance, outperforming prior alignment methods. Notably, it requires only 1,000 training examples and 90 minutes of training on a single RTX A6000 GPU. Extensive experiments across multiple LRM backbones and sizes demonstrate the robustness, scalability, and practical efficiency of our approach.

Abstract: Although large reasoning models (LRMs) have demonstrated impressive
capabilities on complex tasks, recent studies reveal that these models
frequently fulfill harmful user instructions, raising significant safety
concerns. In this paper, we investigate the underlying cause of LRM safety
risks and find that models already possess sufficient safety knowledge but fail
to activate it during reasoning. Based on this insight, we propose R1-Act, a
simple and efficient post-training method that explicitly triggers safety
knowledge through a structured reasoning process. R1-Act achieves strong safety
improvements while preserving reasoning performance, outperforming prior
alignment methods. Notably, it requires only 1,000 training examples and 90
minutes of training on a single RTX A6000 GPU. Extensive experiments across
multiple LRM backbones and sizes demonstrate the robustness, scalability, and
practical efficiency of our approach.

</details>


### [123] [CoRGI: Verified Chain-of-Thought Reasoning with Visual Grounding](https://arxiv.org/abs/2508.00378)
*Shixin Yi,Lin Shang*

Main category: cs.AI

TL;DR: CoRGI通过在推理过程中增加视觉验证，提升了视觉语言模型在推理上的表现，使其解释更可靠。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型(vlm)中的思维链(CoT)提示在改善推理方面显示出了希望，但它通常会产生语言流畅但在视觉内容中缺乏基础的解释。这种幻觉的部分原因是由于多步骤推理过程中缺乏明确的验证机制。

Method: CoRGI，一个模块化框架，引入视觉验证到推理过程中。它遵循一个三阶段的流程：首先生成一个文本推理链，然后通过一个专门的模块（VEVM）为每个推理步骤提取支持性的视觉证据，最后综合文本原理与视觉证据，以生成一个有根据的、经过验证的答案。

Result: 在VCR基准测试中评估了CoRGI，发现它提高了两个具有代表性的开源VLM骨干网络（Qwen-2.5VL和LLaVA-1.6）的推理性能。消融研究证实了验证模块中每个步骤的贡献，而人工评估表明CoRGI可以产生更真实、更有帮助的解释。

Conclusion: CoRGI通过在推理过程中引入视觉验证，提高了多模态推理的鲁棒性。

Abstract: Chain-of-Thought (CoT) prompting has shown promise in improving reasoning in
vision-language models (VLMs), but it often produces explanations that are
linguistically fluent yet lack grounding in visual content. We observe that
such hallucinations arise in part from the absence of an explicit verification
mechanism during multi-step reasoning. To address this, we propose
\textbf{CoRGI}(\textbf{C}hain \textbf{o}f \textbf{R}easoning with
\textbf{G}rounded \textbf{I}nsights), a modular framework that introduces
visual verification into the reasoning process. CoRGI follows a three-stage
pipeline: it first generates a textual reasoning chain, then extracts
supporting visual evidence for each reasoning step via a dedicated module
(VEVM), and finally synthesizes the textual rationale with visual evidence to
generate a grounded, verified answer. The framework can be integrated with
existing VLMs without end-to-end retraining. We evaluate CoRGI on the VCR
benchmark and find that it improves reasoning performance on two representative
open-source VLM backbones, Qwen-2.5VL and LLaVA-1.6. Ablation studies confirm
the contribution of each step in the verification module, and human evaluations
suggest that CoRGI leads to more factual and helpful explanations. We also
examine alternative designs for the visual verification step and discuss
potential limitations of post-hoc verification frameworks. These findings
highlight the importance of grounding intermediate reasoning steps in visual
evidence to enhance the robustness of multimodal reasoning.

</details>


### [124] [Theory of Mind Using Active Inference: A Framework for Multi-Agent Cooperation](https://arxiv.org/abs/2508.00401)
*Riddhi J. Pitliya,Ozan Catal,Toon Van de Maele,Corrado Pezzato,Tim Verbelen*

Main category: cs.AI

TL;DR: A novel multi-agent cooperation approach using theory of mind within active inference, enabling agents to infer others' beliefs from behavior, leading to better cooperation without shared models or communication.


<details>
  <summary>Details</summary>
Motivation: Present a novel approach to multi-agent cooperation that neither relies on task-specific shared generative models nor requires explicit communication, while being generalisable.

Method: Implementing theory of mind (ToM) within active inference and extending the sophisticated inference tree-based planning algorithm to systematically explore joint policy spaces through recursive reasoning.

Result: ToM-equipped agents cooperate better compared to non-ToM counterparts in collision avoidance and foraging task simulations.

Conclusion: ToM-equipped agents cooperate better compared to non-ToM counterparts by being able to avoid collisions and reduce redundant efforts by inferring others' beliefs solely from observable behaviour.

Abstract: We present a novel approach to multi-agent cooperation by implementing theory
of mind (ToM) within active inference. ToM - the ability to understand that
others can have differing knowledge and goals - enables agents to reason about
others' beliefs while planning their own actions. Unlike previous active
inference approaches to multi-agent cooperation, our method neither relies on
task-specific shared generative models nor requires explicit communication,
while being generalisable. In our framework, the ToM-equipped agent maintains
distinct representations of its own and others' beliefs and goals. We extend
the sophisticated inference tree-based planning algorithm to systematically
explore joint policy spaces through recursive reasoning. Our approach is
evaluated through collision avoidance and foraging task simulations. Results
demonstrate that ToM-equipped agents cooperate better compared to non-ToM
counterparts by being able to avoid collisions and reduce redundant efforts.
Crucially, ToM agents accomplish this by inferring others' beliefs solely from
observable behaviour. This work advances practical applications in artificial
intelligence while providing computational insights into ToM.

</details>


### [125] [Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training](https://arxiv.org/abs/2508.00414)
*Tianqing Fang,Zhisong Zhang,Xiaoyang Wang,Rui Wang,Can Qin,Yuxuan Wan,Jun-Yu Ma,Ce Zhang,Jiaqi Chen,Xiyun Li,Hongming Zhang,Haitao Mi,Dong Yu*

Main category: cs.AI

TL;DR: Cognitive Kernel-Pro is a fully open-source and free multi-module agent framework designed to democratize the development and evaluation of advanced AI agents.


<details>
  <summary>Details</summary>
Motivation: Current agent systems are either closed-source or heavily reliant on paid APIs, limiting accessibility and reproducibility.

Method: The paper systematically investigates the curation of high-quality training data for Agent Foundation Models and explores novel strategies for agent test-time reflection and voting.

Result: An 8B-parameter open-source model surpasses previous leading systems such as WebDancer and WebSailor.

Conclusion: Cognitive Kernel-Pro achieves state-of-the-art results on GAIA among open-source and free agents, surpassing previous leading systems and establishing a new performance standard.

Abstract: General AI Agents are increasingly recognized as foundational frameworks for
the next generation of artificial intelligence, enabling complex reasoning, web
interaction, coding, and autonomous research capabilities. However, current
agent systems are either closed-source or heavily reliant on a variety of paid
APIs and proprietary tools, limiting accessibility and reproducibility for the
research community. In this work, we present \textbf{Cognitive Kernel-Pro}, a
fully open-source and (to the maximum extent) free multi-module agent framework
designed to democratize the development and evaluation of advanced AI agents.
Within Cognitive Kernel-Pro, we systematically investigate the curation of
high-quality training data for Agent Foundation Models, focusing on the
construction of queries, trajectories, and verifiable answers across four key
domains: web, file, code, and general reasoning. Furthermore, we explore novel
strategies for agent test-time reflection and voting to enhance agent
robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving
state-of-the-art results among open-source and free agents. Notably, our
8B-parameter open-source model surpasses previous leading systems such as
WebDancer and WebSailor, establishing a new performance standard for
accessible, high-capability AI agents. Code is available at
https://github.com/Tencent/CognitiveKernel-Pro

</details>


### [126] [Thinking Machines: Mathematical Reasoning in the Age of LLMs](https://arxiv.org/abs/2508.00459)
*Andrea Asperti,Alberto Naibo,Claudio Sacerdoti Coen*

Main category: cs.AI

TL;DR: LLMs excel at coding but struggle with formal mathematics. The article explores why and suggests ways to improve their performance in this area.


<details>
  <summary>Details</summary>
Motivation: The discrepancy between the success of LLMs in coding and their struggles in formal mathematics raises questions about their reasoning abilities and internal state tracking.

Method: The article explores recent models and benchmarks, focusing on three central issues at the intersection of machine learning and mathematical cognition.

Result: The article addresses the state-of-the-art of the discipline, focusing on recent models and benchmarks.

Conclusion: This article identifies the current limits of applying LLMs to formal mathematics and suggests how they might be extended.

Abstract: Large Language Models (LLMs) have shown remarkable abilities in structured
reasoning and symbolic tasks, with coding emerging as a particular area of
strength. This success has sparked growing interest in applying LLMs to
mathematics, both in informal problem-solving and formal theorem proving.
However, progress in formal mathematics has proven to be significantly more
difficult, despite surface-level similarities between programming and proof
construction. This discrepancy raises important questions about how LLMs
``reason'', how they are supervised, and whether they internally track a notion
of computational or deductive state. In this article, we address the
state-of-the-art of the discipline, focusing on recent models and benchmarks,
and explore three central issues at the intersection of machine learning and
mathematical cognition: (i) the trade-offs between formal and informal
mathematics as training domains; (ii) the deeper reasons why proof generation
remains more brittle than code synthesis; (iii) and the question of whether
LLMs represent, or merely mimic, a notion of evolving logical state. Our goal
is not to draw hard boundaries, but to identify where the current limits lie,
and how they might be extended.

</details>


### [127] [Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic Model Checking](https://arxiv.org/abs/2508.00500)
*Haoyu Wang,Chris M. Poskitt,Jun Sun,Jiali Wei*

Main category: cs.AI

TL;DR: Pro2Guard 通过概率可达性分析，能有效提升 LLM 代理在具身智能和自动驾驶等领域的安全性，且不牺牲任务完成度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 代理在机器人、虚拟助手和 Web 自动化等领域表现出强大的自主能力。然而，它们的随机行为带来了难以预测的重大安全风险。现有的基于规则的强化系统缺乏远见，难以应对长期依赖和分布变化。

Method: 提出 Pro2Guard，一个基于概率可达性分析的主动运行时强化框架。该框架将代理行为抽象为符号状态，并从执行轨迹中学习离散时间马尔可夫链 (DTMC)。

Result: 在具身代理任务中，Pro2Guard 使用低阈值提前强制执行高达 93.6% 的不安全任务，同时可配置模式允许平衡安全性和任务成功率，保持高达 80.4% 的任务完成率。在自动驾驶场景中，Pro2Guard 实现了 100% 预测交通违法和碰撞，提前预测风险长达 38.66 秒。

Conclusion: Pro2Guard 在具身智能家居代理和自动驾驶场景中都表现出色，能够提前预测并避免危险，同时保持任务完成度。

Abstract: Large Language Model (LLM) agents exhibit powerful autonomous capabilities
across domains such as robotics, virtual assistants, and web automation.
However, their stochastic behavior introduces significant safety risks that are
difficult to anticipate. Existing rule-based enforcement systems, such as
AgentSpec, focus on developing reactive safety rules, which typically respond
only when unsafe behavior is imminent or has already occurred. These systems
lack foresight and struggle with long-horizon dependencies and distribution
shifts. To address these limitations, we propose Pro2Guard, a proactive runtime
enforcement framework grounded in probabilistic reachability analysis.
Pro2Guard abstracts agent behaviors into symbolic states and learns a
Discrete-Time Markov Chain (DTMC) from execution traces. At runtime, it
anticipates future risks by estimating the probability of reaching unsafe
states, triggering interventions before violations occur when the predicted
risk exceeds a user-defined threshold. By incorporating semantic validity
checks and leveraging PAC bounds, Pro2Guard ensures statistical reliability
while approximating the underlying ground-truth model. We evaluate Pro2Guard
extensively across two safety-critical domains: embodied household agents and
autonomous vehicles. In embodied agent tasks, Pro2Guard enforces safety early
on up to 93.6% of unsafe tasks using low thresholds, while configurable modes
(e.g., reflect) allow balancing safety with task success, maintaining up to
80.4% task completion. In autonomous driving scenarios, Pro2Guard achieves 100%
prediction of traffic law violations and collisions, anticipating risks up to
38.66 seconds ahead.

</details>


### [128] [MultiSHAP: A Shapley-Based Framework for Explaining Cross-Modal Interactions in Multimodal AI Models](https://arxiv.org/abs/2508.00576)
*Zhanliang Wang,Kai Wang*

Main category: cs.AI

TL;DR: MultiSHAP is introduced to interpret multimodal AI models by quantifying cross-modal interactions, applicable to both open- and closed-source models, and shows strong performance.


<details>
  <summary>Details</summary>
Motivation: The black-box nature of multimodal AI models poses a barrier to deployment in high-stakes applications. Existing explanation methods cannot precisely quantify the synergistic effects between modalities and are limited to open-source models.

Method: The paper leverages the Shapley Interaction Index to attribute multimodal predictions to pairwise interactions between fine-grained visual and textual elements. MultiSHAP is applicable to both open- and closed-source models.

Result: MultiSHAP provides instance-level explanations revealing synergistic and suppressive cross-modal effects, and dataset-level explanations uncovering generalizable interaction patterns. Experiments confirm its faithfulness, and case studies demonstrate its practical utility.

Conclusion: The paper introduces MultiSHAP, a model-agnostic interpretability framework, and demonstrates its effectiveness in capturing cross-modal reasoning mechanisms through experiments and case studies. The framework is extensible beyond two modalities.

Abstract: Multimodal AI models have achieved impressive performance in tasks that
require integrating information from multiple modalities, such as vision and
language. However, their "black-box" nature poses a major barrier to deployment
in high-stakes applications where interpretability and trustworthiness are
essential. How to explain cross-modal interactions in multimodal AI models
remains a major challenge. While existing model explanation methods, such as
attention map and Grad-CAM, offer coarse insights into cross-modal
relationships, they cannot precisely quantify the synergistic effects between
modalities, and are limited to open-source models with accessible internal
weights. Here we introduce MultiSHAP, a model-agnostic interpretability
framework that leverages the Shapley Interaction Index to attribute multimodal
predictions to pairwise interactions between fine-grained visual and textual
elements (such as image patches and text tokens), while being applicable to
both open- and closed-source models. Our approach provides: (1) instance-level
explanations that reveal synergistic and suppressive cross-modal effects for
individual samples - "why the model makes a specific prediction on this input",
and (2) dataset-level explanation that uncovers generalizable interaction
patterns across samples - "how the model integrates information across
modalities". Experiments on public multimodal benchmarks confirm that MultiSHAP
faithfully captures cross-modal reasoning mechanisms, while real-world case
studies demonstrate its practical utility. Our framework is extensible beyond
two modalities, offering a general solution for interpreting complex multimodal
AI models.

</details>


### [129] [From EMR Data to Clinical Insight: An LLM-Driven Framework for Automated Pre-Consultation Questionnaire Generation](https://arxiv.org/abs/2508.00581)
*Ruiqing Ding,Qianfang Sun,Yongkang Leng,Hui Yin,Xiaojian Li*

Main category: cs.AI

TL;DR: A multi-stage LLM framework is proposed to generate better pre-consultation questionnaires from EMR data.


<details>
  <summary>Details</summary>
Motivation: Generating comprehensive pre-consultation questionnaires from complex EMRs is challenging for direct LLM approaches due to information completeness, logical order, and disease-level synthesis.

Method: A novel multi-stage LLM-driven framework is proposed, involving atomic assertion extraction, personal causal network construction, and tailored questionnaire generation.

Result: The method demonstrates superior performance in information coverage, diagnostic relevance, understandability, and generation time.

Conclusion: The proposed multi-stage LLM framework demonstrates superior performance in generating pre-consultation questionnaires, enhancing patient information collection.

Abstract: Pre-consultation is a critical component of effective healthcare delivery.
However, generating comprehensive pre-consultation questionnaires from complex,
voluminous Electronic Medical Records (EMRs) is a challenging task. Direct
Large Language Model (LLM) approaches face difficulties in this task,
particularly regarding information completeness, logical order, and
disease-level synthesis. To address this issue, we propose a novel multi-stage
LLM-driven framework: Stage 1 extracts atomic assertions (key facts with
timing) from EMRs; Stage 2 constructs personal causal networks and synthesizes
disease knowledge by clustering representative networks from an EMR corpus;
Stage 3 generates tailored personal and standardized disease-specific
questionnaires based on these structured representations. This framework
overcomes limitations of direct methods by building explicit clinical
knowledge. Evaluated on a real-world EMR dataset and validated by clinical
experts, our method demonstrates superior performance in information coverage,
diagnostic relevance, understandability, and generation time, highlighting its
practical potential to enhance patient information collection.

</details>


### [130] [Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings](https://arxiv.org/abs/2508.00632)
*Alexia Jolicoeur-Martineau*

Main category: cs.AI

TL;DR: 我们构建了一个新的指标和一个多智能体系统，以解决交互式视听内容创建中的挑战。


<details>
  <summary>Details</summary>
Motivation: 虽然人工智能擅长生成文本、音频、图像和视频，但创建交互式视听内容（如视频游戏）仍然具有挑战性。当前的LLM可以生成JavaScript游戏和动画，但缺乏自动评估指标，并且难以处理通常需要人类团队使用艺术家制作的资产工作数月（多次拍摄、多智能体）的复杂内容。为了解决这些问题，我们构建了一个新的指标和一个多智能体系统。

Method: 我们提出了AVR-Eval，一种使用视听记录（AVR）的多媒体内容质量的相对指标。一个全模态模型（处理文本、视频和音频）比较两个内容的AVR，一个文本模型回顾评估以确定优势。我们构建了AVR-Agent，一个从多媒体资产库（音频、图像、3D模型）生成JavaScript代码的多智能体系统。编码代理选择相关的资产，生成多个初始代码，使用AVR-Eval来识别最佳版本，并通过来自AVR的全模态代理反馈迭代地改进它。

Result: 我们通过AVR-Eval（内容A对内容B的胜率）在游戏和动画上进行了实验。我们发现，AVR-Agent生成的内容的胜率明显高于通过一次性生成制作的内容。然而，模型难以有效地利用定制资产和AVR反馈，没有显示出更高的胜率。

Conclusion: AVR-Agent生成的内容的胜率明显高于一次性生成的内容。然而，模型难以有效地利用定制资产和AVR反馈，没有显示出更高的胜率。这揭示了一个关键差距：虽然人类受益于高质量的资产和视听反馈，但当前的编码模型似乎没有像人类那样有效地利用这些资源，突出了人类和机器内容创作方法之间的根本差异。

Abstract: While AI excels at generating text, audio, images, and videos, creating
interactive audio-visual content such as video games remains challenging.
Current LLMs can generate JavaScript games and animations, but lack automated
evaluation metrics and struggle with complex content that normally requires
teams of humans working for many months (multi-shot, multi-agents) using assets
made by artists. To tackle these issues, we built a new metric and a
multi-agent system.
  We propose AVR-Eval, a relative metric for multimedia content quality using
Audio-Visual Recordings (AVRs). An omni-modal model (processing text, video,
and audio) compares the AVRs of two contents, with a text model reviewing
evaluations to determine superiority. We show that AVR-Eval properly identifies
good from broken or mismatched content.
  We built AVR-Agent, a multi-agent system generating JavaScript code from a
bank of multimedia assets (audio, images, 3D models). The coding agent selects
relevant assets, generates multiple initial codes, uses AVR-Eval to identify
the best version, and iteratively improves it through omni-modal agent feedback
from the AVR.
  We run experiments on games and animations with AVR-Eval (win rate of content
A against B). We find that content generated by AVR-Agent has a significantly
higher win rate against content made through one-shot generation. However,
models struggle to leverage custom assets and AVR feedback effectively, showing
no higher win rate. This reveals a critical gap: while humans benefit from
high-quality assets and audio-visual feedback, current coding models do not
seem to utilize these resources as effectively, highlighting fundamental
differences between human and machine content creation approaches.

</details>


### [131] [Multi-Band Variable-Lag Granger Causality: A Unified Framework for Causal Time Series Inference across Frequencies](https://arxiv.org/abs/2508.00658)
*Chakattrai Sookkongwaree,Tattep Lakmuang,Chainarong Amornbunchornvej*

Main category: cs.AI

TL;DR: 该论文提出了一个考虑频率依赖因果延迟的多频带变延迟格兰杰因果关系(MB-VLGC)框架，并在多个领域验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统格兰杰因果关系框架在因果关系推断中存在强固定滞后假设，而实际复杂系统中因果关系可能随频率变化。

Method: 该论文形式化了多频带变延迟格兰杰因果关系(MB-VLGC)，并提出了一个高效的推理流程。

Result: 实验结果表明，该框架在合成和真实世界数据集上均优于现有方法。

Conclusion: 该论文提出了多频带变延迟格兰杰因果关系(MB-VLGC)框架，并在多个领域进行了实验验证，结果表明该框架优于现有方法。

Abstract: Understanding causal relationships in time series is fundamental to many
domains, including neuroscience, economics, and behavioral science. Granger
causality is one of the well-known techniques for inferring causality in time
series. Typically, Granger causality frameworks have a strong fix-lag
assumption between cause and effect, which is often unrealistic in complex
systems. While recent work on variable-lag Granger causality (VLGC) addresses
this limitation by allowing a cause to influence an effect with different time
lags at each time point, it fails to account for the fact that causal
interactions may vary not only in time delay but also across frequency bands.
For example, in brain signals, alpha-band activity may influence another region
with a shorter delay than slower delta-band oscillations. In this work, we
formalize Multi-Band Variable-Lag Granger Causality (MB-VLGC) and propose a
novel framework that generalizes traditional VLGC by explicitly modeling
frequency-dependent causal delays. We provide a formal definition of MB-VLGC,
demonstrate its theoretical soundness, and propose an efficient inference
pipeline. Extensive experiments across multiple domains demonstrate that our
framework significantly outperforms existing methods on both synthetic and
real-world datasets, confirming its broad applicability to any type of time
series data. Code and datasets are publicly available.

</details>


### [132] [Transparent Adaptive Learning via Data-Centric Multimodal Explainable AI](https://arxiv.org/abs/2508.00665)
*Maryam Mosleh,Marie Devlin,Ellis Solaiman*

Main category: cs.AI

TL;DR: The paper introduces a framework for creating personalized and understandable explanations in AI-driven education systems.


<details>
  <summary>Details</summary>
Motivation: Many AI-driven adaptive learning systems lack transparency and neglect user roles and comprehension.

Method: The paper proposes a hybrid framework that combines traditional XAI techniques with generative AI models and user personalisation.

Result: The paper outlines the framework's design, key XAI limitations in education, and research directions on accuracy, fairness, and personalisation.

Conclusion: This paper aims to enhance AI transparency in education by using a hybrid framework that integrates XAI techniques, generative AI models, and user personalization to create tailored explanations.

Abstract: Artificial intelligence-driven adaptive learning systems are reshaping
education through data-driven adaptation of learning experiences. Yet many of
these systems lack transparency, offering limited insight into how decisions
are made. Most explainable AI (XAI) techniques focus on technical outputs but
neglect user roles and comprehension. This paper proposes a hybrid framework
that integrates traditional XAI techniques with generative AI models and user
personalisation to generate multimodal, personalised explanations tailored to
user needs. We redefine explainability as a dynamic communication process
tailored to user roles and learning goals. We outline the framework's design,
key XAI limitations in education, and research directions on accuracy,
fairness, and personalisation. Our aim is to move towards explainable AI that
enhances transparency while supporting user-centred experiences.

</details>


### [133] [Context-Aware Visualization for Explainable AI Recommendations in Social Media: A Vision for User-Aligned Explanations](https://arxiv.org/abs/2508.00674)
*Banan Alkhateeb,Ellis Solaiman*

Main category: cs.AI

TL;DR: This paper proposes a user-segmented and context-aware explanation layer by proposing a visual explanation system with diverse explanation methods to improve user experience.


<details>
  <summary>Details</summary>
Motivation: explainability in social media is general and lacks alignment with user-specific needs

Method: proposing a visual explanation system with diverse explanation methods

Result: Our framework is the first to jointly adapt explanation style (visual vs. numeric) and granularity (expert vs. lay) inside a single pipeline.

Conclusion: A public pilot with 30 X users will validate its impact on decision-making and trust.

Abstract: Social media platforms today strive to improve user experience through AI
recommendations, yet the value of such recommendations vanishes as users do not
understand the reasons behind them. This issue arises because explainability in
social media is general and lacks alignment with user-specific needs. In this
vision paper, we outline a user-segmented and context-aware explanation layer
by proposing a visual explanation system with diverse explanation methods. The
proposed system is framed by the variety of user needs and contexts, showing
explanations in different visualized forms, including a technically detailed
version for AI experts and a simplified one for lay users. Our framework is the
first to jointly adapt explanation style (visual vs. numeric) and granularity
(expert vs. lay) inside a single pipeline. A public pilot with 30 X users will
validate its impact on decision-making and trust.

</details>


### [134] [Unraveling Hidden Representations: A Multi-Modal Layer Analysis for Better Synthetic Content Forensics](https://arxiv.org/abs/2508.00784)
*Tom Or,Omri Azencot*

Main category: cs.AI

TL;DR: proposes the use of large pre-trained multi-modal models for the detection of generative content. linear classifiers trained on these features can achieve state-of-the-art results across various modalities


<details>
  <summary>Details</summary>
Motivation: the majority of existing work train classifiers that discriminate between real and fake information, such tools typically generalize only within the same family of generators and data modalities, yielding poor results on other generative classes and data domains. Consequently, the need for robust and stable fake detectors is pressing, especially when new generative models appear everyday.

Method: use of large pre-trained multi-modal models for the detection of generative content. linear classifiers are trained on the latent code of these models

Result: the latent code of large pre-trained multi-modal models naturally captures information discriminating real from fake. linear classifiers trained on these features can achieve state-of-the-art results across various modalities, while remaining computationally efficient, fast to train, and effective even in few-shot settings. performance in audio and images surpasses or matches that of strong baseline methods.

Conclusion: linear classifiers trained on the latent code of large pre-trained multi-modal models can achieve state-of-the-art results across various modalities, while remaining computationally efficient, fast to train, and effective even in few-shot settings. The work primarily focuses on fake detection in audio and images, achieving performance that surpasses or matches that of strong baseline methods.

Abstract: Generative models achieve remarkable results in multiple data domains,
including images and texts, among other examples. Unfortunately, malicious
users exploit synthetic media for spreading misinformation and disseminating
deepfakes. Consequently, the need for robust and stable fake detectors is
pressing, especially when new generative models appear everyday. While the
majority of existing work train classifiers that discriminate between real and
fake information, such tools typically generalize only within the same family
of generators and data modalities, yielding poor results on other generative
classes and data domains. Towards a universal classifier, we propose the use of
large pre-trained multi-modal models for the detection of generative content.
Effectively, we show that the latent code of these models naturally captures
information discriminating real from fake. Building on this observation, we
demonstrate that linear classifiers trained on these features can achieve
state-of-the-art results across various modalities, while remaining
computationally efficient, fast to train, and effective even in few-shot
settings. Our work primarily focuses on fake detection in audio and images,
achieving performance that surpasses or matches that of strong baseline
methods.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [135] [Audio Prototypical Network For Controllable Music Recommendation](https://arxiv.org/abs/2508.00194)
*Fırat Öncel,Emiliano Penaloza,Haolun Wu,Shubham Gupta,Mirco Ravanelli,Laurent Charlin,Cem Subakan*

Main category: cs.IR

TL;DR: propose an audio prototypical network for controllable music recommendation


<details>
  <summary>Details</summary>
Motivation: Traditional recommendation systems represent user preferences in dense representations obtained through black-box encoder models. While these models often provide strong recommendation performance, they lack interpretability for users, leaving users unable to understand or control the system's modeling of their preferences. This limitation is especially challenging in music recommendation, where user preferences are highly personal and often evolve based on nuanced qualities like mood, genre, tempo, or instrumentation.

Method: propose an audio prototypical network

Result: competitive recommendation performance compared to popular baseline models while also providing interpretable and controllable user profiles.

Conclusion: The model obtains competitive recommendation performance compared to popular baseline models while also providing interpretable and controllable user profiles.

Abstract: Traditional recommendation systems represent user preferences in dense
representations obtained through black-box encoder models. While these models
often provide strong recommendation performance, they lack interpretability for
users, leaving users unable to understand or control the system's modeling of
their preferences. This limitation is especially challenging in music
recommendation, where user preferences are highly personal and often evolve
based on nuanced qualities like mood, genre, tempo, or instrumentation. In this
paper, we propose an audio prototypical network for controllable music
recommendation. This network expresses user preferences in terms of prototypes
representative of semantically meaningful features pertaining to musical
qualities. We show that the model obtains competitive recommendation
performance compared to popular baseline models while also providing
interpretable and controllable user profiles.

</details>


### [136] [When Relevance Meets Novelty: Dual-Stable Periodic Optimization for Exploratory Recommendation](https://arxiv.org/abs/2508.00450)
*Hongxiang Lin,Hao Guo,Zeshun Li,Erpeng Xue,Yongqian He,Xiangyu Hou,Zhaoyu Hu,Lei Wang,Sheng Chen*

Main category: cs.IR

TL;DR: 提出了CoEA模型，通过DSIE模块和PCO机制解决推荐系统中的探索性问题。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统容易使用户陷入强反馈循环，限制探索机会并导致内容疲劳。现有的LLM增强双模型框架忽略了由群体身份驱动的长期偏好，导致有偏差的兴趣建模，并且存在静态优化缺陷。

Method: 提出了Co-Evolutionary Alignment (CoEA) 方法，包含Dual-Stable Interest Exploration (DSIE) 模块和Periodic Collaborative Optimization (PCO) 机制。

Result: 大量的线上和线下实验验证了CoEA模型在探索性推荐中的有效性。

Conclusion: CoEA模型在探索性推荐中有效。

Abstract: Traditional recommendation systems tend to trap users in strong feedback
loops by excessively pushing content aligned with their historical preferences,
thereby limiting exploration opportunities and causing content fatigue.
Although large language models (LLMs) demonstrate potential with their diverse
content generation capabilities, existing LLM-enhanced dual-model frameworks
face two major limitations: first, they overlook long-term preferences driven
by group identity, leading to biased interest modeling; second, they suffer
from static optimization flaws, as a one-time alignment process fails to
leverage incremental user data for closed-loop optimization. To address these
challenges, we propose the Co-Evolutionary Alignment (CoEA) method. For
interest modeling bias, we introduce Dual-Stable Interest Exploration (DSIE)
module, jointly modeling long-term group identity and short-term individual
interests through parallel processing of behavioral sequences. For static
optimization limitations, we design a Periodic Collaborative Optimization (PCO)
mechanism. This mechanism regularly conducts preference verification on
incremental data using the Relevance LLM, then guides the Novelty LLM to
perform fine-tuning based on the verification results, and subsequently feeds
back the output of the incrementally fine-tuned Novelty LLM to the Relevance
LLM for re-evaluation, thereby achieving a dynamic closed-loop optimization.
Extensive online and offline experiments verify the effectiveness of the CoEA
model in exploratory recommendation.

</details>


### [137] [M^2VAE: Multi-Modal Multi-View Variational Autoencoder for Cold-start Item Recommendation](https://arxiv.org/abs/2508.00452)
*Chuan He,Yongchao Liu,Qiang Li,Wenliang Zhong,Chuntao Hong,Xinwei Yao*

Main category: cs.IR

TL;DR: This paper introduces M^2VAE, a multi-modal multi-view variational autoencoder, to tackle the cold-start item recommendation problem by modeling common and unique views in item features and user preferences.


<details>
  <summary>Details</summary>
Motivation: Cold-start item recommendation is a significant challenge in recommendation systems, particularly when new items are introduced without any historical interaction data. While existing methods leverage multi-modal content to alleviate the cold-start issue, they often neglect the inherent multi-view structure of modalities, the distinction between shared and modality-specific features.

Method: We propose Multi-Modal Multi-View Variational AutoEncoder (M^2VAE), a generative model that addresses the challenges of modeling common and unique views in attribute and multi-modal features, as well as user preferences over single-typed item features. Specifically, we generate type-specific latent variables for item IDs, categorical attributes, and image features, and use Product-of-Experts (PoE) to derive a common representation. A disentangled contrastive loss decouples the common view from unique views while preserving feature informativeness. To model user inclinations, we employ a preference-guided Mixture-of-Experts (MoE) to adaptively fuse representations. We further incorporate co-occurrence signals via contrastive learning, eliminating the need for pretraining.

Result: Extensive experiments on real-world datasets validate the effectiveness of our approach.

Conclusion: Extensive experiments on real-world datasets validate the effectiveness of our approach.

Abstract: Cold-start item recommendation is a significant challenge in recommendation
systems, particularly when new items are introduced without any historical
interaction data. While existing methods leverage multi-modal content to
alleviate the cold-start issue, they often neglect the inherent multi-view
structure of modalities, the distinction between shared and modality-specific
features. In this paper, we propose Multi-Modal Multi-View Variational
AutoEncoder (M^2VAE), a generative model that addresses the challenges of
modeling common and unique views in attribute and multi-modal features, as well
as user preferences over single-typed item features. Specifically, we generate
type-specific latent variables for item IDs, categorical attributes, and image
features, and use Product-of-Experts (PoE) to derive a common representation. A
disentangled contrastive loss decouples the common view from unique views while
preserving feature informativeness. To model user inclinations, we employ a
preference-guided Mixture-of-Experts (MoE) to adaptively fuse representations.
We further incorporate co-occurrence signals via contrastive learning,
eliminating the need for pretraining. Extensive experiments on real-world
datasets validate the effectiveness of our approach.

</details>


### [138] [Session-Based Recommendation with Validated and Enriched LLM Intents](https://arxiv.org/abs/2508.00570)
*Gyuseok Lee,Yaokun Liu,Yifan Liu,Susik Yoon,Dong Wang,SeongKu Kang*

Main category: cs.IR

TL;DR: Proposes VELI4SBR, a two-stage framework using validated and enriched LLM-generated intents for SBR, addressing challenges in intent quality, multi-intent incorporation, and LLM failure compensation.


<details>
  <summary>Details</summary>
Motivation: SBR suffers from data sparsity due to the short and anonymous nature of sessions. Recently, an emerging line of work has explored inferring the underlying user intents of a session using large language models (LLMs), with the generated intents serving as auxiliary training signals to enhance SBR models. Despite its promise, this approach faces three key challenges: validating intent quality, incorporating session-level multi-intents, and complementing inevitable LLM failure cases.

Method: a two-stage framework that leverages Validated and Enriched LLM-generated Intents for SBR. In the first stage, generate high-quality intents using a predict-and-correct loop that validates the informativeness of LLM-generated intents with a global intent pool to constrain the LLM's output space and reduce hallucination. In the second stage, enhance the SBR model using the generated intents through a lightweight multi-intent prediction and fusion mechanism. Furthermore, introduce a training strategy that compensates for LLM failures by inferring intents from inter-session behavioral similarities.

Result: VELI4SBR outperforms state-of-the-art baselines while improving explainability.

Conclusion: VELI4SBR outperforms state-of-the-art baselines while improving explainability.

Abstract: Session-based recommendation (SBR) aims to predict the next item for an
anonymous user in a timely manner. However, SBR suffers from data sparsity due
to the short and anonymous nature of sessions. Recently, an emerging line of
work has explored inferring the underlying user intents of a session using
large language models (LLMs), with the generated intents serving as auxiliary
training signals to enhance SBR models. Despite its promise, this approach
faces three key challenges: validating intent quality, incorporating
session-level multi-intents, and complementing inevitable LLM failure cases. In
this paper, we propose VELI4SBR, a two-stage framework that leverages Validated
and Enriched LLM-generated Intents for SBR. In the first stage, we generate
high-quality intents using a predict-and-correct loop that validates the
informativeness of LLM-generated intents with a global intent pool to constrain
the LLM's output space and reduce hallucination. In the second stage, we
enhance the SBR model using the generated intents through a lightweight
multi-intent prediction and fusion mechanism. Furthermore, we introduce a
training strategy that compensates for LLM failures by inferring intents from
inter-session behavioral similarities. Extensive experiments show that VELI4SBR
outperforms state-of-the-art baselines while improving explainability.

</details>


### [139] [Experimental Evaluation of Dynamic Topic Modeling Algorithms](https://arxiv.org/abs/2508.00710)
*Ngozichukwuka Onah,Nadine Steinmetz,Hani Al-Sayeh,Kai-Uwe Sattler*

Main category: cs.IR

TL;DR: comparing topic models with a new assessment metric


<details>
  <summary>Details</summary>
Motivation: analyzing text from social media is useful, need dependable computing techniques

Method: compare topic models and propose an assessment metric

Result: assessment metric documents how topics change in time

Conclusion: few thorough quantitative comparisons between topic models exist

Abstract: The amount of text generated daily on social media is gigantic and analyzing
this text is useful for many purposes. To understand what lies beneath a huge
amount of text, we need dependable and effective computing techniques from
self-powered topic models. Nevertheless, there are currently relatively few
thorough quantitative comparisons between these models. In this study, we
compare these models and propose an assessment metric that documents how the
topics change in time.

</details>


### [140] [Harnessing the Power of Interleaving and Counterfactual Evaluation for Airbnb Search Ranking](https://arxiv.org/abs/2508.00751)
*Qing Zhang,Alex Deng,Michelle Du,Huiji Gao,Liwei He,Sanjeev Katariya*

Main category: cs.IR

TL;DR: 为了解决A/B测试的挑战，我们开发了交错和反事实评估方法，以促进快速在线评估，从而更有效地识别有希望的候选方案。


<details>
  <summary>Details</summary>
Motivation: 传统的A/B测试在转化率指标方面可能需要较长时间才能达到足够的统计效力，而离线评估又缺乏准确性，无法有效选择A/B测试的候选方案。

Method: 交错和反事实评估方法

Result: 与传统的A/B测试相比，实验灵敏度提高了高达100倍（取决于方法和指标）。

Conclusion: 该方法通过促进快速在线评估，提高了实验的灵敏度，并简化了实验过程，从而为组织带来了实际效益。

Abstract: Evaluation plays a crucial role in the development of ranking algorithms on
search and recommender systems. It enables online platforms to create
user-friendly features that drive commercial success in a steady and effective
manner. The online environment is particularly conducive to applying causal
inference techniques, such as randomized controlled experiments (known as A/B
test), which are often more challenging to implement in fields like medicine
and public policy. However, businesses face unique challenges when it comes to
effective A/B test. Specifically, achieving sufficient statistical power for
conversion-based metrics can be time-consuming, especially for significant
purchases like booking accommodations. While offline evaluations are quicker
and more cost-effective, they often lack accuracy and are inadequate for
selecting candidates for A/B test. To address these challenges, we developed
interleaving and counterfactual evaluation methods to facilitate rapid online
assessments for identifying the most promising candidates for A/B tests. Our
approach not only increased the sensitivity of experiments by a factor of up to
100 (depending on the approach and metrics) compared to traditional A/B testing
but also streamlined the experimental process. The practical insights gained
from usage in production can also benefit organizations with similar interests.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [141] [Predicting Large-scale Urban Network Dynamics with Energy-informed Graph Neural Diffusion](https://arxiv.org/abs/2508.00037)
*Tong Nie,Jian Sun,Wei Ma*

Main category: cs.LG

TL;DR: 提出了一种可扩展的时空Transformer，用于预测大规模城市网络中的动态。


<details>
  <summary>Details</summary>
Motivation: 现有的图神经网络模型在功效和效率之间存在权衡。

Method: 提出了一种基于Transformer的神经扩散方案。

Result: 在交通流量、太阳能和智能电表等大规模城市系统中表现出最好的性能和显著的可扩展性。

Conclusion: 提出了一种新的动力学预测方法，并在大规模城市网络中验证了其性能。

Abstract: Networked urban systems facilitate the flow of people, resources, and
services, and are essential for economic and social interactions. These systems
often involve complex processes with unknown governing rules, observed by
sensor-based time series. To aid decision-making in industrial and engineering
contexts, data-driven predictive models are used to forecast spatiotemporal
dynamics of urban systems. Current models such as graph neural networks have
shown promise but face a trade-off between efficacy and efficiency due to
computational demands. Hence, their applications in large-scale networks still
require further efforts. This paper addresses this trade-off challenge by
drawing inspiration from physical laws to inform essential model designs that
align with fundamental principles and avoid architectural redundancy. By
understanding both micro- and macro-processes, we present a principled
interpretable neural diffusion scheme based on Transformer-like structures
whose attention layers are induced by low-dimensional embeddings. The proposed
scalable spatiotemporal Transformer (ScaleSTF), with linear complexity, is
validated on large-scale urban systems including traffic flow, solar power, and
smart meters, showing state-of-the-art performance and remarkable scalability.
Our results constitute a fresh perspective on the dynamics prediction in
large-scale urban networks.

</details>


### [142] [Hybrid LSTM-Transformer Models for Profiling Highway-Railway Grade Crossings](https://arxiv.org/abs/2508.00039)
*Kaustav Chatterjee,Joshua Q. Li,Fatemeh Ansari,Masud Rana Munna,Kundan Parajulee,Jared Schwennesen*

Main category: cs.LG

TL;DR: 本研究采用先进的深度学习技术，结合 LSTM 和 Transformer 架构，对高坡公路铁路平交道口 (HRGC) 剖面进行快速准确的评估，以提高公路和铁路安全。


<details>
  <summary>Details</summary>
Motivation: 高坡公路铁路平交道口 (HRGC) 因潜在的挂起对公路车辆构成安全风险。传统的 HRGC 剖面测量方法成本高昂、耗时、扰乱交通并带来安全挑战。

Method: 利用仪器和地面实况数据，开发了一种结合长短期记忆 (LSTM) 和 Transformer 架构的新型混合深度学习框架。

Result: 三个先进的深度学习模型 Transformer-LSTM 顺序模型（模型 1）、LSTM-Transformer 顺序模型（模型 2）和 LSTM-Transformer 并行模型（模型 3）经过评估以确定最高效的架构。模型 2 和 3 的性能优于其他模型，并被部署以生成 2D/3D HRGC 剖面。

Conclusion: 深度学习模型通过快速准确评估 HRGC 挂起敏感性，展示了增强公路和铁路安全的巨大潜力。

Abstract: Hump crossings, or high-profile Highway Railway Grade Crossings (HRGCs), pose
safety risks to highway vehicles due to potential hang-ups. These crossings
typically result from post-construction railway track maintenance activities or
non-compliance with design guidelines for HRGC vertical alignments.
Conventional methods for measuring HRGC profiles are costly, time-consuming,
traffic-disruptive, and present safety challenges. To address these issues,
this research employed advanced, cost-effective techniques and innovative
modeling approaches for HRGC profile measurement. A novel hybrid deep learning
framework combining Long Short-Term Memory (LSTM) and Transformer architectures
was developed by utilizing instrumentation and ground truth data.
Instrumentation data were gathered using a highway testing vehicle equipped
with Inertial Measurement Unit (IMU) and Global Positioning System (GPS)
sensors, while ground truth data were obtained via an industrial-standard
walking profiler. Field data was collected at the Red Rock Railroad Corridor in
Oklahoma. Three advanced deep learning models Transformer-LSTM sequential
(model 1), LSTM-Transformer sequential (model 2), and LSTM-Transformer parallel
(model 3) were evaluated to identify the most efficient architecture. Models 2
and 3 outperformed the others and were deployed to generate 2D/3D HRGC
profiles. The deep learning models demonstrated significant potential to
enhance highway and railroad safety by enabling rapid and accurate assessment
of HRGC hang-up susceptibility.

</details>


### [143] [Regime-Aware Conditional Neural Processes with Multi-Criteria Decision Support for Operational Electricity Price Forecasting](https://arxiv.org/abs/2508.00040)
*Abhinav Das,Stephan Schlüter*

Main category: cs.LG

TL;DR: 该论文将贝叶斯状态检测与条件神经过程相结合，用于德国市场的24小时电力价格预测。


<details>
  <summary>Details</summary>
Motivation: 为德国市场进行24小时电力价格预测。

Method: 将贝叶斯状态检测与条件神经过程相结合。

Result: LEAR通常产生卓越的绝对利润或更低的成本，而DNN在特定的成本最小化环境中表现出卓越的最优性。TOPSIS分析显示，LEAR是2021年的顶级模型。

Conclusion: R-NP模型是2021、2022和2023年最平衡和首选的解决方案。

Abstract: This work integrates Bayesian regime detection with conditional neural
processes for 24-hour electricity price prediction in the German market. Our
methodology integrates regime detection using a disentangled sticky
hierarchical Dirichlet process hidden Markov model (DS-HDP-HMM) applied to
daily electricity prices. Each identified regime is subsequently modeled by an
independent conditional neural process (CNP), trained to learn localized
mappings from input contexts to 24-dimensional hourly price trajectories, with
final predictions computed as regime-weighted mixtures of these CNP outputs. We
rigorously evaluate R-NP against deep neural networks (DNN) and Lasso estimated
auto-regressive (LEAR) models by integrating their forecasts into diverse
battery storage optimization frameworks, including price arbitrage, risk
management, grid services, and cost minimization. This operational utility
assessment revealed complex performance trade-offs: LEAR often yielded superior
absolute profits or lower costs, while DNN showed exceptional optimality in
specific cost-minimization contexts. Recognizing that raw prediction accuracy
doesn't always translate to optimal operational outcomes, we employed TOPSIS as
a comprehensive multi-criteria evaluation layer. Our TOPSIS analysis identified
LEAR as the top-ranked model for 2021, but crucially, our proposed R-NP model
emerged as the most balanced and preferred solution for 2021, 2022 and 2023.

</details>


### [144] [Learning Like Humans: Resource-Efficient Federated Fine-Tuning through Cognitive Developmental Stages](https://arxiv.org/abs/2508.00041)
*Yebo Wu,Jingguang Li,Zhijiang Guo,Li Li*

Main category: cs.LG

TL;DR: DevFT是一种资源高效的联邦微调方法，通过模拟认知发展，从紧凑的基础逐步构建强大的LLM。


<details>
  <summary>Details</summary>
Motivation: 联邦微调使大型语言模型（LLM）能够适应下游任务，同时保护数据隐私，但其资源密集型性质限制了在边缘设备上的部署。

Method: DevFT将微调过程分解为多个发展阶段，每个阶段优化具有增加参数容量的子模型。来自早期阶段的知识转移到后续子模型，提供优化的初始化参数，防止收敛到局部最小值并加速训练。为了有效地构建特定于阶段的子模型，DevFT引入了解冲突引导的层分组和基于差异的层融合，以提取基本信息并构建代表性层。

Result: DevFT显著优于现有方法，实现了高达4.59倍的更快收敛速度，10.67倍的通信开销减少以及9.07%的平均性能提升。

Conclusion: DevFT显著优于现有方法，实现了高达4.59倍的更快收敛速度，10.67倍的通信开销减少以及9.07%的平均性能提升，同时保持与现有方法的兼容性。

Abstract: Federated fine-tuning enables Large Language Models (LLMs) to adapt to
downstream tasks while preserving data privacy, but its resource-intensive
nature limits deployment on edge devices. In this paper, we introduce
Developmental Federated Tuning (DevFT), a resource-efficient approach inspired
by cognitive development that progressively builds a powerful LLM from a
compact foundation. DevFT decomposes the fine-tuning process into developmental
stages, each optimizing submodels with increasing parameter capacity. Knowledge
from earlier stages transfers to subsequent submodels, providing optimized
initialization parameters that prevent convergence to local minima and
accelerate training. This paradigm mirrors human learning, gradually
constructing comprehensive knowledge structure while refining existing skills.
To efficiently build stage-specific submodels, DevFT introduces
deconfliction-guided layer grouping and differential-based layer fusion to
distill essential information and construct representative layers. Evaluations
across multiple benchmarks demonstrate that DevFT significantly outperforms
state-of-the-art methods, achieving up to 4.59$\times$ faster convergence,
10.67$\times$ reduction in communication overhead, and 9.07% average
performance improvement, while maintaining compatibility with existing
approaches.

</details>


### [145] [Improved Robustness and Functional Localization in Topographic CNNs Through Weight Similarity](https://arxiv.org/abs/2508.00043)
*Nhut Truong,Uri Hasson*

Main category: cs.LG

TL;DR: Weight Similarity constraints in topographic CNNs lead to more robust and functionally organized representations compared to Activation Similarity or standard CNNs.


<details>
  <summary>Details</summary>
Motivation: The impact of different implementations of topographic constraints in neural networks has not been systematically examined.

Method: Compared topographic convolutional neural networks trained with Weight Similarity (WS) and Activation Similarity (AS) spatial constraints.

Result: WS provided improved robustness to noise, greater input sensitivity, and stronger functional localization compared to AS and standard CNNs. WS also produced differences in orientation tuning, symmetry sensitivity, and eccentricity profiles of units.

Conclusion: Weight Similarity (WS) constraints produce more robust representations than Activation Similarity (AS) or non-topographic CNNs during end-to-end training, and can shape feature learning and functional organization in biophysical inspired models.

Abstract: Topographic neural networks are computational models that can simulate the
spatial and functional organization of the brain. Topographic constraints in
neural networks can be implemented in multiple ways, with potentially different
impacts on the representations learned by the network. The impact of such
different implementations has not been systematically examined. To this end,
here we compare topographic convolutional neural networks trained with two
spatial constraints: Weight Similarity (WS), which pushes neighboring units to
develop similar incoming weights, and Activation Similarity (AS), which
enforces similarity in unit activations. We evaluate the resulting models on
classification accuracy, robustness to weight perturbations and input
degradation, and the spatial organization of learned representations. Compared
to both AS and standard CNNs, WS provided three main advantages: i) improved
robustness to noise, also showing higher accuracy under weight corruption; ii)
greater input sensitivity, reflected in higher activation variance; and iii)
stronger functional localization, with units showing similar activations
positioned at closer distances. In addition, WS produced differences in
orientation tuning, symmetry sensitivity, and eccentricity profiles of units,
indicating an influence of this spatial constraint on the representational
geometry of the network. Our findings suggest that during end-to-end training,
WS constraints produce more robust representations than AS or non-topographic
CNNs. These findings also suggest that weight-based spatial constraints can
shape feature learning and functional organization in biophysical inspired
models.

</details>


### [146] [Benchmarking Partial Observability in Reinforcement Learning with a Suite of Memory-Improvable Domains](https://arxiv.org/abs/2508.00046)
*Ruo Yu Tao,Kaicheng Guo,Cameron Allen,George Konidaris*

Main category: cs.LG

TL;DR: 提出了POBAX，一个用于评估强化学习算法在部分可观察环境中的性能的基准测试。


<details>
  <summary>Details</summary>
Motivation: 通用强化学习算法需要缓解部分可观察性，这是一项必要但具有挑战性的任务。为了提高算法缓解部分可观察性的能力，研究人员需要全面的基准来衡量进展。大多数解决部分可观察性的算法仅在具有简单形式的状态混叠的基准上进行评估，例如特征屏蔽和高斯噪声。这些基准不能代表在真实领域中看到的许多形式的部分可观察性，例如视觉遮挡或未知的对手意图。一个部分可观察的基准应该具有两个关键属性：1. 覆盖其部分可观察性的形式，以确保算法的通用性。2. 具有更多或更少状态信息的智能体之间的性能差距很大，所有其他因素大致相等。这种差距意味着环境是可记忆改进的：域中的性能增益来自算法应对部分可观察性的能力，而不是其他因素。

Method: 提出了用于在部分可观察性下对强化学习进行基准测试的最佳实践指南，以及开源库POBAX：JAX中的部分可观察基准测试。

Result: POBAX基准测试选择代表性环境，包括定位和建图、视觉控制、游戏等。此外，该框架证明了这些任务都是可记忆改进的，并且需要难以学习的记忆功能，为部分可观察性研究提供了一个具体的信号。

Conclusion: POBAX基准测试包含定位、建图、视觉控制和游戏等环境，这些任务都是可记忆改进的，并且需要难以学习的记忆功能，为部分可观察性研究提供了一个具体的信号。该框架包括推荐的超参数以及算法实现，可用于快速、开箱即用的评估，以及在JAX中实现的高性能环境，用于GPU可扩展实验。

Abstract: Mitigating partial observability is a necessary but challenging task for
general reinforcement learning algorithms. To improve an algorithm's ability to
mitigate partial observability, researchers need comprehensive benchmarks to
gauge progress. Most algorithms tackling partial observability are only
evaluated on benchmarks with simple forms of state aliasing, such as feature
masking and Gaussian noise. Such benchmarks do not represent the many forms of
partial observability seen in real domains, like visual occlusion or unknown
opponent intent. We argue that a partially observable benchmark should have two
key properties. The first is coverage in its forms of partial observability, to
ensure an algorithm's generalizability. The second is a large gap between the
performance of a agents with more or less state information, all other factors
roughly equal. This gap implies that an environment is memory improvable: where
performance gains in a domain are from an algorithm's ability to cope with
partial observability as opposed to other factors. We introduce best-practice
guidelines for empirically benchmarking reinforcement learning under partial
observability, as well as the open-source library POBAX: Partially Observable
Benchmarks in JAX. We characterize the types of partial observability present
in various environments and select representative environments for our
benchmark. These environments include localization and mapping, visual control,
games, and more. Additionally, we show that these tasks are all memory
improvable and require hard-to-learn memory functions, providing a concrete
signal for partial observability research. This framework includes recommended
hyperparameters as well as algorithm implementations for fast, out-of-the-box
evaluation, as well as highly performant environments implemented in JAX for
GPU-scalable experimentation.

</details>


### [147] [TriP-LLM: A Tri-Branch Patch-wise Large Language Model Framework for Time-Series Anomaly Detection](https://arxiv.org/abs/2508.00047)
*Yuan-Cheng Yu,Yen-Chieh Ouyang,Chun-An Lin*

Main category: cs.LG

TL;DR: TriP-LLM：一种用于时间序列异常检测的新型无监督框架，优于现有技术，且内存消耗更低。


<details>
  <summary>Details</summary>
Motivation: 随着物联网（IoT）和智能制造的日益普及，时间序列数据的规模和维度都大幅增加。这种增长暴露了传统统计方法在处理这种数据的高度异构性和复杂性方面的局限性。

Method: 一种新的无监督异常检测框架：用于时间序列异常检测的三分支分片式大语言模型框架（TriP-LLM）。TriP-LLM通过三分支设计（分片、选择和全局）整合局部和全局时间特征，以将输入时间序列编码为分片式token，然后由冻结的预训练LLM处理。一个轻量级的分片式解码器重建输入，从中导出异常分数。

Result: TriP-LLM在几个公共基准数据集上进行了评估，使用了最近提出的无阈值评估指标PATE，并在统一的开源框架内进行了所有比较以确保公平性。实验结果表明，TriP-LLM始终优于最新的state-of-the-art方法。

Conclusion: TriP-LLM在所有数据集上始终优于最新的state-of-the-art方法，展示了强大的检测能力。消融实验验证了LLM对整个架构的重大贡献。与使用通道独立（CI）补丁处理的基于LLM的方法相比，TriP-LLM实现了显著更低的内存消耗，使其更适合GPU内存受限的环境。

Abstract: Time-series anomaly detection plays a central role across a wide range of
application domains. With the increasing proliferation of the Internet of
Things (IoT) and smart manufacturing, time-series data has dramatically
increased in both scale and dimensionality. This growth has exposed the
limitations of traditional statistical methods in handling the high
heterogeneity and complexity of such data. Inspired by the recent success of
large language models (LLMs) in multimodal tasks across language and vision
domains, we propose a novel unsupervised anomaly detection framework: A
Tri-Branch Patch-wise Large Language Model Framework for Time-Series Anomaly
Detection (TriP-LLM). TriP-LLM integrates local and global temporal features
through a tri-branch design-Patching, Selection, and Global-to encode the input
time series into patch-wise tokens, which are then processed by a frozen,
pretrained LLM. A lightweight patch-wise decoder reconstructs the input, from
which anomaly scores are derived. We evaluate TriP-LLM on several public
benchmark datasets using PATE, a recently proposed threshold-free evaluation
metric, and conduct all comparisons within a unified open-source framework to
ensure fairness. Experimental results show that TriP-LLM consistently
outperforms recent state-of-the-art methods across all datasets, demonstrating
strong detection capabilities. Furthermore, through extensive ablation studies,
we verify the substantial contribution of the LLM to the overall architecture.
Compared to LLM-based approaches using Channel Independence (CI) patch
processing, TriP-LLM achieves significantly lower memory consumption, making it
more suitable for GPU memory-constrained environments. All code and model
checkpoints are publicly available on https://github.com/YYZStart/TriP-LLM.git

</details>


### [148] [Evaluating COVID 19 Feature Contributions to Bitcoin Return Forecasting: Methodology Based on LightGBM and Genetic Optimization](https://arxiv.org/abs/2508.00078)
*Imen Mahmoud,Andrei Velichko*

Main category: cs.LG

TL;DR: 本研究提出了一种新的方法框架，该框架集成了 LightGBM 回归模型和遗传算法 (GA) 优化，以系统地评估 COVID-19 相关指标对 Bitcoin 收益预测的贡献。


<details>
  <summary>Details</summary>
Motivation: 确定包含与疫情相关的健康数据是否能显著提高预测准确性。

Method: LightGBM 回归模型和遗传算法 (GA) 优化

Result: COVID-19 指标显著提高了模型性能，尤其是在捕捉极端市场波动方面 (R2 提高了 40%，RMSE 降低了 2%，均具有高度统计学意义)。

Conclusion: COVID-19 指标显著提高了模型性能，尤其是在捕捉极端市场波动方面。疫苗接种指标，特别是完全接种疫苗个体数量的 75 分位数，成为主要的预测指标。

Abstract: This study proposes a novel methodological framework integrating a LightGBM
regression model and genetic algorithm (GA) optimization to systematically
evaluate the contribution of COVID-19-related indicators to Bitcoin return
prediction. The primary objective was not merely to forecast Bitcoin returns
but rather to determine whether including pandemic-related health data
significantly enhances prediction accuracy. A comprehensive dataset comprising
daily Bitcoin returns and COVID-19 metrics (vaccination rates,
hospitalizations, testing statistics) was constructed. Predictive models,
trained with and without COVID-19 features, were optimized using GA over 31
independent runs, allowing robust statistical assessment. Performance metrics
(R2, RMSE, MAE) were statistically compared through distribution overlaps and
Mann-Whitney U tests. Permutation Feature Importance (PFI) analysis quantified
individual feature contributions. Results indicate that COVID-19 indicators
significantly improved model performance, particularly in capturing extreme
market fluctuations (R2 increased by 40%, RMSE decreased by 2%, both highly
significant statistically). Among COVID-19 features, vaccination metrics,
especially the 75th percentile of fully vaccinated individuals, emerged as
dominant predictors. The proposed methodology extends existing financial
analytics tools by incorporating public health signals, providing investors and
policymakers with refined indicators to navigate market uncertainty during
systemic crises.

</details>


### [149] [Stress-Aware Resilient Neural Training](https://arxiv.org/abs/2508.00098)
*Ashkan Shakarami,Yousef Yeganeh,Azade Farshad,Lorenzo Nicole,Stefano Ghidoni,Nassir Navab*

Main category: cs.LG

TL;DR: Introduces Stress-Aware Learning, a resilient neural training paradigm, which injects adaptive noise into model parameters whenever an internal stress signal indicates persistent optimization difficulty. 


<details>
  <summary>Details</summary>
Motivation: This paper introduces Stress-Aware Learning, a resilient neural training paradigm in which deep neural networks dynamically adjust their optimization behavior - whether under stable training regimes or in settings with uncertain dynamics - based on the concept of Temporary (Elastic) and Permanent (Plastic) Deformation, inspired by structural fatigue in materials science.

Method: We propose Plastic Deformation Optimizer, a stress-aware mechanism that injects adaptive noise into model parameters whenever an internal stress signal - reflecting stagnation in training loss and accuracy - indicates persistent optimization difficulty.

Result: Enables the model to escape sharp minima and converge toward flatter, more generalizable regions of the loss landscape.

Conclusion: Experiments across six architectures, four optimizers, and seven vision benchmarks demonstrate improved robustness and generalization with minimal computational overhead.

Abstract: This paper introduces Stress-Aware Learning, a resilient neural training
paradigm in which deep neural networks dynamically adjust their optimization
behavior - whether under stable training regimes or in settings with uncertain
dynamics - based on the concept of Temporary (Elastic) and Permanent (Plastic)
Deformation, inspired by structural fatigue in materials science. To
instantiate this concept, we propose Plastic Deformation Optimizer, a
stress-aware mechanism that injects adaptive noise into model parameters
whenever an internal stress signal - reflecting stagnation in training loss and
accuracy - indicates persistent optimization difficulty. This enables the model
to escape sharp minima and converge toward flatter, more generalizable regions
of the loss landscape. Experiments across six architectures, four optimizers,
and seven vision benchmarks demonstrate improved robustness and generalization
with minimal computational overhead. The code and 3D visuals will be available
on GitHub: https://github.com/Stress-Aware-Learning/SAL.

</details>


### [150] [StackLiverNet: A Novel Stacked Ensemble Model for Accurate and Interpretable Liver Disease Detection](https://arxiv.org/abs/2508.00117)
*Md. Ehsanul Haque,S. M. Jahidul Islam,Shakil Mia,Rumana Sharmin,Ashikuzzaman,Md Samir Morshed,Md. Tahmidul Huque*

Main category: cs.LG

TL;DR: StackLiverNet是一种用于肝病检测的可解释堆叠集成模型，具有高准确率和快速的训练/推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习和深度学习模型在肝病分类中存在高误分类错误、可解释性差、计算成本高和缺乏良好预处理策略等问题。

Method: 引入StackLiverNet，一个可解释的堆叠集成模型，用于肝病检测。该框架使用高级数据预处理和特征选择技术，并通过LightGBM元模型集成多个超参数优化的基本分类器。采用随机欠采样处理类别不平衡问题。

Result: StackLiverNet的测试准确率为99.89%，Cohen Kappa为0.9974，AUC为0.9993，仅有5个错误分类，训练时间为4.2783秒，推理时间为0.1106秒。

Conclusion: StackLiverNet模型在肝病检测中表现出色，具有高准确率、Kappa系数和AUC，且训练和推理速度快，适用于临床实践。LIME用于生成透明的个体预测解释，揭示了碱性磷酸酶和SGOT是肝病的重要指标。SHAP用于按全局贡献对特征进行排序，Morris方法通过敏感性分析确认了最具影响力的特征。

Abstract: Liver diseases are a serious health concern in the world, which requires
precise and timely diagnosis to enhance the survival chances of patients. The
current literature implemented numerous machine learning and deep learning
models to classify liver diseases, but most of them had some issues like high
misclassification error, poor interpretability, prohibitive computational
expense, and lack of good preprocessing strategies. In order to address these
drawbacks, we introduced StackLiverNet in this study; an interpretable stacked
ensemble model tailored to the liver disease detection task. The framework uses
advanced data preprocessing and feature selection technique to increase model
robustness and predictive ability. Random undersampling is performed to deal
with class imbalance and make the training balanced. StackLiverNet is an
ensemble of several hyperparameter-optimized base classifiers, whose
complementary advantages are used through a LightGBM meta-model. The provided
model demonstrates excellent performance, with the testing accuracy of 99.89%,
Cohen Kappa of 0.9974, and AUC of 0.9993, having only 5 misclassifications, and
efficient training and inference speeds that are amenable to clinical practice
(training time 4.2783 seconds, inference time 0.1106 seconds). Besides, Local
Interpretable Model-Agnostic Explanations (LIME) are applied to generate
transparent explanations of individual predictions, revealing high
concentrations of Alkaline Phosphatase and moderate SGOT as important
observations of liver disease. Also, SHAP was used to rank features by their
global contribution to predictions, while the Morris method confirmed the most
influential features through sensitivity analysis.

</details>


### [151] [Structured Transformations for Stable and Interpretable Neural Computation](https://arxiv.org/abs/2508.00127)
*Saleh Nikooroo,Thomas Engel*

Main category: cs.LG

TL;DR: Introduces structured layer-level transformations to improve neural network stability, interpretability, and robustness without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: Current neural networks often lack structural safeguards that ensure stable learning and interpretable behavior.

Method: A new formulation of layer-level transformations is introduced, which decomposes each transformation into a structured linear operator and a residual corrective component.

Result: Models with structured transformations have improved gradient conditioning, reduced sensitivity to perturbations, and layer-wise robustness. These benefits are consistent across different architectural scales and training setups.

Conclusion: Models built with structured transformations show better gradient conditioning, less sensitivity to perturbations, and layer-wise robustness. These benefits hold across different architectural scales and training methods. This work lays the groundwork for neural architectures that focus on stability and transparency, providing new ways to understand learning behavior without losing expressive power.

Abstract: Despite their impressive performance, contemporary neural networks often lack
structural safeguards that promote stable learning and interpretable behavior.
In this work, we introduce a reformulation of layer-level transformations that
departs from the standard unconstrained affine paradigm. Each transformation is
decomposed into a structured linear operator and a residual corrective
component, enabling more disciplined signal propagation and improved training
dynamics. Our formulation encourages internal consistency and supports stable
information flow across depth, while remaining fully compatible with standard
learning objectives and backpropagation. Through a series of synthetic and
real-world experiments, we demonstrate that models constructed with these
structured transformations exhibit improved gradient conditioning, reduced
sensitivity to perturbations, and layer-wise robustness. We further show that
these benefits persist across architectural scales and training regimes. This
study serves as a foundation for a more principled class of neural
architectures that prioritize stability and transparency-offering new tools for
reasoning about learning behavior without sacrificing expressive power.

</details>


### [152] [ECG Latent Feature Extraction with Autoencoders for Downstream Prediction Tasks](https://arxiv.org/abs/2508.00131)
*Christopher Harvey,Sumaiya Shomaji,Zijun Yao,Amit Noheria*

Main category: cs.LG

TL;DR: 本研究探索了使用变分自编码器（VAE）变体简化心电图（ECG）数据的方法，并在数据有限的情况下提高了预测性能。


<details>
  <summary>Details</summary>
Motivation: 心电图（ECG）是一种廉价且广泛可用的心脏评估工具。尽管ECG信号具有标准化格式和小文件大小，但其高复杂性和个体间差异性（通常是具有12个导联，采样频率为500 Hz的60,000大小的向量）使得它难以在深度学习模型中使用，尤其是在只有小型训练数据集可用时。

Method: 探索来自代表性beat ECG的特征生成方法，重点关注主成分分析（PCA）和自编码器来降低数据复杂性。引入了三种新的变分自编码器（VAE）变体-随机自编码器（SAE）、退火beta-VAE（A beta-VAE）和循环beta VAE（C beta-VAE）-并比较它们在维持信号保真度和增强使用Light Gradient Boost Machine（LGBM）的下游预测任务中的有效性。

Result: A beta-VAE实现了卓越的信号重建，将平均绝对误差（MAE）降低到15.7+/-3.2 muV，这与信号噪声水平相当。此外，SAE编码与传统ECG摘要特征相结合，提高了左心室射血分数（LVEF）降低的预测，在使用LGBM分类器的情况下，受试者工作特征曲线（AUROC）下的保持测试集面积达到0.901。这种性能几乎与最先进的CNN模型的0.909 AUROC相匹配，但需要的计算资源明显更少。此外，ECG特征提取-LGBM管道避免了过度拟合，并在使用较少数据进行训练时保持了预测性能。

Conclusion: VAE编码能够简化ECG数据，并为在标记训练数据有限的情况下应用深度学习提供实用的解决方案。

Abstract: The electrocardiogram (ECG) is an inexpensive and widely available tool for
cardiac assessment. Despite its standardized format and small file size, the
high complexity and inter-individual variability of ECG signals (typically a
60,000-size vector with 12 leads at 500 Hz) make it challenging to use in deep
learning models, especially when only small training datasets are available.
This study addresses these challenges by exploring feature generation methods
from representative beat ECGs, focusing on Principal Component Analysis (PCA)
and Autoencoders to reduce data complexity. We introduce three novel
Variational Autoencoder (VAE) variants-Stochastic Autoencoder (SAE), Annealed
beta-VAE (A beta-VAE), and Cyclical beta VAE (C beta-VAE)-and compare their
effectiveness in maintaining signal fidelity and enhancing downstream
prediction tasks using a Light Gradient Boost Machine (LGBM). The A beta-VAE
achieved superior signal reconstruction, reducing the mean absolute error (MAE)
to 15.7+/-3.2 muV, which is at the level of signal noise. Moreover, the SAE
encodings, when combined with traditional ECG summary features, improved the
prediction of reduced Left Ventricular Ejection Fraction (LVEF), achieving an
holdout test set area under the receiver operating characteristic curve (AUROC)
of 0.901 with a LGBM classifier. This performance nearly matches the 0.909
AUROC of state-of-the-art CNN model but requires significantly less
computational resources. Further, the ECG feature extraction-LGBM pipeline
avoids overfitting and retains predictive performance when trained with less
data. Our findings demonstrate that these VAE encodings are not only effective
in simplifying ECG data but also provide a practical solution for applying deep
learning in contexts with limited-scale labeled training data.

</details>


### [153] [INSPIRE-GNN: Intelligent Sensor Placement to Improve Sparse Bicycling Network Prediction via Reinforcement Learning Boosted Graph Neural Networks](https://arxiv.org/abs/2508.00141)
*Mohit Gupta,Debjit Bhowmick,Rhys Newbury,Meead Saberi,Shirui Pan,Ben Beck*

Main category: cs.LG

TL;DR: INSPIRE-GNN 是一种用于优化传感器放置和改进链路级自行车流量估计的强化学习 (RL) 增强混合图神经网络 (GNN) 框架。


<details>
  <summary>Details</summary>
Motivation: 精确的链路级自行车流量估计对于可持续的城市交通规划至关重要。然而，由于自行车计数传感器覆盖范围有限，许多城市面临着高数据稀疏性的重大挑战。

Method: 提出了一种新颖的强化学习 (RL) 增强混合图神经网络 (GNN) 框架 INSPIRE-GNN，旨在优化传感器放置并提高数据稀疏环境中的链路级自行车体积估计。

Result: 应用于墨尔本的自行车网络，该网络包含 15,933 个路段，传感器仅覆盖 141 个路段（99% 的稀疏度）- INSPIRE-GNN 通过战略性地选择 50、100、200 和 500 个传感器的部署中的其他传感器位置，显着提高了体积估计。

Conclusion: 该框架为交通规划人员提供了可操作的见解，以有效扩展传感器网络，优化传感器位置，并最大限度地提高自行车数据的体积估计准确性和可靠性，从而为明智的交通规划决策提供依据。

Abstract: Accurate link-level bicycling volume estimation is essential for sustainable
urban transportation planning. However, many cities face significant challenges
of high data sparsity due to limited bicycling count sensor coverage. To
address this issue, we propose INSPIRE-GNN, a novel Reinforcement Learning
(RL)-boosted hybrid Graph Neural Network (GNN) framework designed to optimize
sensor placement and improve link-level bicycling volume estimation in
data-sparse environments. INSPIRE-GNN integrates Graph Convolutional Networks
(GCN) and Graph Attention Networks (GAT) with a Deep Q-Network (DQN)-based RL
agent, enabling a data-driven strategic selection of sensor locations to
maximize estimation performance. Applied to Melbourne's bicycling network,
comprising 15,933 road segments with sensor coverage on only 141 road segments
(99% sparsity) - INSPIRE-GNN demonstrates significant improvements in volume
estimation by strategically selecting additional sensor locations in
deployments of 50, 100, 200 and 500 sensors. Our framework outperforms
traditional heuristic methods for sensor placement such as betweenness
centrality, closeness centrality, observed bicycling activity and random
placement, across key metrics such as Mean Squared Error (MSE), Root Mean
Squared Error (RMSE) and Mean Absolute Error (MAE). Furthermore, our
experiments benchmark INSPIRE-GNN against standard machine learning and deep
learning models in the bicycle volume estimation performance, underscoring its
effectiveness. Our proposed framework provides transport planners actionable
insights to effectively expand sensor networks, optimize sensor placement and
maximize volume estimation accuracy and reliability of bicycling data for
informed transportation planning decisions.

</details>


### [154] [Watch the Weights: Unsupervised monitoring and control of fine-tuned LLMs](https://arxiv.org/abs/2508.00161)
*Ziqian Zhong,Aditi Raghunathan*

Main category: cs.LG

TL;DR: 提出了一种新的方法，通过分析权重来理解、监控和控制微调的 LLM，可以有效检测后门攻击、遗忘学习，并揭示模型的微调重点。


<details>
  <summary>Details</summary>
Motivation: 现有的可解释性方法通常需要或假设分布相似的数据，这在检测和防御新的潜在威胁（如后门）时是一个重大限制，而后门在定义上是超出分布的。

Method: 该方法通过解释权重而不是激活来理解、监控和控制微调的 LLM，从而避免了对与未知训练数据分布相似的数据的需求。它通过监控沿这些方向的激活的余弦相似性来检测在微调期间引入的显着行为。

Result: 对于存在秘密触发器时绕过安全机制的后门模型，该方法可以阻止高达 100% 的攻击，假阳性率低于 1.2%。对于经过遗忘学习的模型，该方法可以检测对已删除主题的推理，准确率高达 95.42%，甚至可以引导模型恢复“未学习”的信息。

Conclusion: 该方法能够有效检测和防御后门攻击，检测遗忘学习中的推理，并指导模型恢复“未学习”的信息。此外，该方法还显示出预部署模型审计的潜力，能够揭示特定于模型的微调重点，包括营销策略和 Midjourney 提示生成。

Abstract: The releases of powerful open-weight large language models (LLMs) are often
not accompanied by access to their full training data. Existing
interpretability methods, particularly those based on activations, often
require or assume distributionally similar data. This is a significant
limitation when detecting and defending against novel potential threats like
backdoors, which are by definition out-of-distribution.
  In this work, we introduce a new method for understanding, monitoring and
controlling fine-tuned LLMs that interprets weights, rather than activations,
thereby side stepping the need for data that is distributionally similar to the
unknown training data. We demonstrate that the top singular vectors of the
weight difference between a fine-tuned model and its base model correspond to
newly acquired behaviors. By monitoring the cosine similarity of activations
along these directions, we can detect salient behaviors introduced during
fine-tuning with high precision.
  For backdoored models that bypasses safety mechanisms when a secret trigger
is present, our method stops up to 100% of attacks with a false positive rate
below 1.2%. For models that have undergone unlearning, we detect inference on
erased topics with accuracy up to 95.42% and can even steer the model to
recover "unlearned" information. Besides monitoring, our method also shows
potential for pre-deployment model auditing: by analyzing commercial
instruction-tuned models (OLMo, Llama, Qwen), we are able to uncover
model-specific fine-tuning focus including marketing strategies and Midjourney
prompt generation.
  Our implementation can be found at https://github.com/fjzzq2002/WeightWatch.

</details>


### [155] [DiSC-Med: Diffusion-based Semantic Communications for Robust Medical Image Transmission](https://arxiv.org/abs/2508.00172)
*Fupei Guo,Hao Zheng,Xiang Zhang,Li Chen,Yue Wang,Songyang Zhang*

Main category: cs.LG

TL;DR: 提出了一种新的基于扩散的语义通信框架DiSC-Med，用于医学图像传输，该框架具有带宽效率和鲁棒性，并在真实世界医疗数据集中表现出潜力。


<details>
  <summary>Details</summary>
Motivation: 为了能够对远程医疗进行及时有效的响应，通过带宽受限的嘈杂信道高效传输医疗数据成为一项关键挑战。

Method: 我们提出了一个新的基于扩散的语义通信框架，即DiSC-Med，用于医学图像传输，其中开发了医学增强的压缩和去噪模块，分别用于带宽效率和鲁棒性。

Result: 与传统的像素级通信框架不同，我们提出的DiSC-Med能够捕获关键的语义信息，并以超高的带宽效率实现优异的抗噪声信道重建性能。

Conclusion: 该框架在真实世界医疗数据集上的大量实验验证了其有效性，证明了其在稳健和高效的远程医疗应用中的潜力。

Abstract: The rapid development of artificial intelligence has driven smart health with
next-generation wireless communication technologies, stimulating exciting
applications in remote diagnosis and intervention. To enable a timely and
effective response for remote healthcare, efficient transmission of medical
data through noisy channels with limited bandwidth emerges as a critical
challenge. In this work, we propose a novel diffusion-based semantic
communication framework, namely DiSC-Med, for the medical image transmission,
where medical-enhanced compression and denoising blocks are developed for
bandwidth efficiency and robustness, respectively. Unlike conventional
pixel-wise communication framework, our proposed DiSC-Med is able to capture
the key semantic information and achieve superior reconstruction performance
with ultra-high bandwidth efficiency against noisy channels. Extensive
experiments on real-world medical datasets validate the effectiveness of our
framework, demonstrating its potential for robust and efficient telehealth
applications.

</details>


### [156] [RL as Regressor: A Reinforcement Learning Approach for Function Approximation](https://arxiv.org/abs/2508.00174)
*Yongchao Huang*

Main category: cs.LG

TL;DR: 将回归转化为强化学习问题，以解决传统回归方法在处理复杂目标时不够灵活的问题。


<details>
  <summary>Details</summary>
Motivation: 标准回归技术虽然强大，但通常受到预定义的、可微的损失函数的约束，例如均方误差。这些函数可能无法完全捕捉系统的预期行为，尤其是在处理非对称成本或复杂的、不可微的目标时。

Method: 将模型的预测视为动作，并根据预测误差定义自定义奖励信号，并利用强大的 RL 算法来执行函数逼近。

Result: 通过学习噪声正弦波的渐进案例研究，我们展示了 Actor-Critic 代理的开发，通过优先经验重放、增加的网络容量和位置编码对其进行迭代增强，从而为该回归任务启用了一个有能力的 RL 代理。

Conclusion: RL 框架不仅成功地解决了回归问题，而且在定义目标和指导学习过程方面提供了更高的灵活性。

Abstract: Standard regression techniques, while powerful, are often constrained by
predefined, differentiable loss functions such as mean squared error. These
functions may not fully capture the desired behavior of a system, especially
when dealing with asymmetric costs or complex, non-differentiable objectives.
In this paper, we explore an alternative paradigm: framing regression as a
Reinforcement Learning (RL) problem. We demonstrate this by treating a model's
prediction as an action and defining a custom reward signal based on the
prediction error, and we can leverage powerful RL algorithms to perform
function approximation. Through a progressive case study of learning a noisy
sine wave, we illustrate the development of an Actor-Critic agent, iteratively
enhancing it with Prioritized Experience Replay, increased network capacity,
and positional encoding to enable a capable RL agent for this regression task.
Our results show that the RL framework not only successfully solves the
regression problem but also offers enhanced flexibility in defining objectives
and guiding the learning process.

</details>


### [157] [EMA Without the Lag: Bias-Corrected Iterate Averaging Schemes](https://arxiv.org/abs/2508.00180)
*Adam Block,Cyril Zhang*

Main category: cs.LG

TL;DR: 提出了BEMA，这是一种改进的EMA，它减少了方差，消除了偏差，并在语言模型微调中实现了更稳定和高效的训练。


<details>
  <summary>Details</summary>
Motivation: 语言模型微调中的随机性，通常是由该机制中使用的小批量大小引起的，它会因在生成质量中引入大的振荡而破坏训练的稳定性。

Method: 提出了偏差校正指数移动平均(BEMA)，这是一种简单而实用的EMA增强方法，它保留了方差减少的优势，同时消除了偏差。

Result: 通过对语言模型进行的大量实验表明，BEMA与EMA和vanilla训练相比，BEMA可以显著提高收敛速度和最终性能。

Conclusion: BEMA在各种标准LM基准测试中，与EMA和vanilla训练相比，BEMA可以显著提高收敛速度和最终性能，这使得BEMA成为一种在理论和实践上都有意义的干预手段，可以实现更稳定和高效的微调。

Abstract: Stochasticity in language model fine-tuning, often caused by the small batch
sizes typically used in this regime, can destabilize training by introducing
large oscillations in generation quality. A popular approach to mitigating this
instability is to take an Exponential moving average (EMA) of weights
throughout training. While EMA reduces stochasticity, thereby smoothing
training, the introduction of bias from old iterates often creates a lag in
optimization relative to vanilla training. In this work, we propose the
Bias-Corrected Exponential Moving Average (BEMA), a simple and practical
augmentation of EMA that retains variance-reduction benefits while eliminating
bias. BEMA is motivated by a simple theoretical model wherein we demonstrate
provable acceleration of BEMA over both a standard EMA and vanilla training.
Through an extensive suite of experiments on Language Models, we show that BEMA
leads to significantly improved convergence rates and final performance over
both EMA and vanilla training in a variety of standard LM benchmarks, making
BEMA a practical and theoretically motivated intervention for more stable and
efficient fine-tuning.

</details>


### [158] [RecoMind: A Reinforcement Learning Framework for Optimizing In-Session User Satisfaction in Recommendation Systems](https://arxiv.org/abs/2508.00201)
*Mehdi Ben Ayed,Fei Feng,Jay Adams,Vishwakarma Singh,Kritarth Anand,Jiajing Xu*

Main category: cs.LG

TL;DR: RecoMind是一个基于模拟器的RL框架，旨在有效优化Web规模的基于会话的目标。


<details>
  <summary>Details</summary>
Motivation: 现有的Web规模推荐系统通常使用优先考虑即时用户反馈的监督学习方法。虽然强化学习（RL）提供了一种优化更长期目标（如会话内互动）的解决方案，但由于极大的行动空间和工程复杂性，在Web规模上应用它具有挑战性。

Method: RecoMind利用现有的推荐模型来建立一个模拟环境，并引导RL策略从一开始就优化即时用户交互。此外，RecoMind还引入了一种定制的探索策略，以有效地探索具有数亿个项目的Web规模行动空间。

Result: 通过广泛的离线模拟和视频流平台上的在线A/B测试评估了RecoMind。两种方法都表明，使用RecoMind训练的RL策略在会话内用户满意度方面显著优于传统的监督学习推荐方法。在在线A/B测试中，对于至少有10次互动的会话，RL策略使观看超过10秒的视频增加了15.81％，会话深度提高了4.71％。

Conclusion: RecoMind提供了一种系统且可扩展的方法，将RL嵌入到Web规模的推荐系统中，在优化基于会话的用户满意度方面显示出巨大的希望。

Abstract: Existing web-scale recommendation systems commonly use supervised learning
methods that prioritize immediate user feedback. Although reinforcement
learning (RL) offers a solution to optimize longer-term goals, such as
in-session engagement, applying it at web scale is challenging due to the
extremely large action space and engineering complexity. In this paper, we
introduce RecoMind, a simulator-based RL framework designed for the effective
optimization of session-based goals at web-scale. RecoMind leverages existing
recommendation models to establish a simulation environment and to bootstrap
the RL policy to optimize immediate user interactions from the outset. This
method integrates well with existing industry pipelines, simplifying the
training and deployment of RL policies. Additionally, RecoMind introduces a
custom exploration strategy to efficiently explore web-scale action spaces with
hundreds of millions of items. We evaluated RecoMind through extensive offline
simulations and online A/B testing on a video streaming platform. Both methods
showed that the RL policy trained using RecoMind significantly outperforms
traditional supervised learning recommendation approaches in in-session user
satisfaction. In online A/B tests, the RL policy increased videos watched for
more than 10 seconds by 15.81\% and improved session depth by 4.71\% for
sessions with at least 10 interactions. As a result, RecoMind presents a
systematic and scalable approach for embedding RL into web-scale recommendation
systems, showing great promise for optimizing session-based user satisfaction.

</details>


### [159] [Robust Classification under Noisy Labels: A Geometry-Aware Reliability Framework for Foundation Models](https://arxiv.org/abs/2508.00202)
*Ecem Bozkurt,Antonio Ortega*

Main category: cs.LG

TL;DR: 提出了一种两阶段框架，通过引入几何信息来提高在标签噪声下分类的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 基于以下事实：简单的 k 最近邻 (kNN) 方法即使在存在严重标签噪声的情况下也能实现良好的性能，因为这些方法利用了局部几何。

Method: 提出了一种两阶段框架，以确保在存在标签噪声的情况下进行鲁棒分类，而无需模型重新训练。该方法使用非负核 (NNK) 邻域构建获得训练数据的局部邻域，并提出了几种可靠性估计方法。

Result: 在 CIFAR-10 和 DermaMNIST 上的评估表明，该方法提高了各种噪声条件下的鲁棒性。

Conclusion: 在各种噪声条件下，该方法提高了鲁棒性，超过了标准 K-NN 方法和最近的自适应邻域基线。

Abstract: Foundation models (FMs) pretrained on large datasets have become fundamental
for various downstream machine learning tasks, in particular in scenarios where
obtaining perfectly labeled data is prohibitively expensive. In this paper, we
assume an FM has to be fine-tuned with noisy data and present a two-stage
framework to ensure robust classification in the presence of label noise
without model retraining. Recent work has shown that simple k-nearest neighbor
(kNN) approaches using an embedding derived from an FM can achieve good
performance even in the presence of severe label noise. Our work is motivated
by the fact that these methods make use of local geometry. In this paper,
following a similar two-stage procedure, reliability estimation followed by
reliability-weighted inference, we show that improved performance can be
achieved by introducing geometry information. For a given instance, our
proposed inference uses a local neighborhood of training data, obtained using
the non-negative kernel (NNK) neighborhood construction. We propose several
methods for reliability estimation that can rely less on distance and local
neighborhood as the label noise increases. Our evaluation on CIFAR-10 and
DermaMNIST shows that our methods improve robustness across various noise
conditions, surpassing standard K-NN approaches and recent
adaptive-neighborhood baselines.

</details>


### [160] [Towards Higher Effective Rank in Parameter-efficient Fine-tuning using Khatri--Rao Product](https://arxiv.org/abs/2508.00230)
*Paul Albert,Frederic Z. Zhang,Hemanth Saratchandran,Anton van den Hengel,Ehsan Abbasnejad*

Main category: cs.LG

TL;DR: 介绍了一种新的参数高效微调算法KRAdapter，通过Khatri-Rao积产生高有效秩的权重更新，并在视觉语言模型和大型语言模型上展示了性能提升。


<details>
  <summary>Details</summary>
Motivation: 低秩适应(LoRA)在参数高效微调(PEFT)方法中取得了显著成功。然而，最近的研究强调了它与全秩替代方案相比的局限性，特别是在应用于多模态和大型语言模型时。

Method: 提出了一种新的PEFT算法KRAdapter，它利用Khatri-Rao积来产生权重更新，这种方法倾向于产生具有高有效秩的矩阵乘积。使用具有受控频谱属性的合成矩阵近似基准，对全秩和低秩PEFT方法进行了定量比较。

Result: 结果证实，LoRA难以逼近具有相对平坦频谱或高频分量的矩阵，这些是高有效秩的标志。

Conclusion: KRAdapter在视觉语言模型和大型语言模型上表现出性能提升，尤其是在未知的常识推理任务中。KRAdapter保持了LoRA的内存和计算效率，使其成为微调十亿级参数模型的实用且稳健的替代方案。

Abstract: Parameter-efficient fine-tuning (PEFT) has become a standard approach for
adapting large pre-trained models. Amongst PEFT methods, low-rank adaptation
(LoRA) has achieved notable success. However, recent studies have highlighted
its limitations compared against full-rank alternatives, particularly when
applied to multimodal and large language models. In this work, we present a
quantitative comparison amongst full-rank and low-rank PEFT methods using a
synthetic matrix approximation benchmark with controlled spectral properties.
Our results confirm that LoRA struggles to approximate matrices with relatively
flat spectrums or high frequency components -- signs of high effective ranks.
To this end, we introduce KRAdapter, a novel PEFT algorithm that leverages the
Khatri-Rao product to produce weight updates, which, by construction, tends to
produce matrix product with a high effective rank. We demonstrate performance
gains with KRAdapter on vision-language models up to 1B parameters and on large
language models up to 8B parameters, particularly on unseen common-sense
reasoning tasks. In addition, KRAdapter maintains the memory and compute
efficiency of LoRA, making it a practical and robust alternative to fine-tune
billion-scale parameter models.

</details>


### [161] [Calibrated Language Models and How to Find Them with Label Smoothing](https://arxiv.org/abs/2508.00264)
*Jerry Huang,Peng Lu,Qiuhao Zeng*

Main category: cs.LG

TL;DR: 指令微调会降低大型语言模型的校准。标签平滑可以缓解此问题，但对于大型词汇表的LLM效果较差。定制内核可以减少标签平滑的内存占用。


<details>
  <summary>Details</summary>
Motivation: 了解指令微调如何影响可靠模型输出的置信度校准尚未得到充分研究。在指令调整后，大型语言模型（LLM）的校准会显著降低。

Method: 我们检查了各种开源LLM，并发现在每个LLM中，指令调整后校准都会显著降低。我们着眼于标签平滑，这已被证明是一种有效的正则化过度自信预测的方法，但尚未在LLM的监督微调（SFT）中得到广泛采用。我们首先深入了解为什么标签平滑足以在整个SFT过程中保持校准。

Result: 标签平滑足以在整个SFT过程中保持校准。对于大型词汇表的LLM，过度自信的能力与隐藏层大小和词汇量大小有直接关系，并且标签平滑的效果会减弱。与非平滑损失的现有解决方案相比，定制内核可以显著减少内存消耗，且不牺牲速度或性能。

Conclusion: 标签平滑可以有效保持指令微调后大型语言模型的校准，但对于大型词汇表的LLM，其效果会减弱。我们设计了一个定制内核，以减少标签平滑损失设置中交叉熵损失计算的内存占用，且不牺牲速度或性能。

Abstract: Recent advances in natural language processing (NLP) have opened up greater
opportunities to enable fine-tuned large language models (LLMs) to behave as
more powerful interactive agents through improved instruction-following
ability. However, understanding how this impacts confidence calibration for
reliable model output has not been researched in full. In this work, we examine
various open-sourced LLMs, identifying significant calibration degradation
after instruction tuning in each. Seeking a practical solution, we look towards
label smoothing, which has been shown as an effective method to regularize for
overconfident predictions but has yet to be widely adopted in the supervised
fine-tuning (SFT) of LLMs. We first provide insight as to why label smoothing
is sufficient to maintain calibration throughout the SFT process. However,
settings remain where the effectiveness of smoothing is severely diminished, in
particular the case of large vocabulary LLMs (LV-LLMs). We posit the cause to
stem from the ability to become over-confident, which has a direct relationship
with the hidden size and vocabulary size, and justify this theoretically and
experimentally. Finally, we address an outstanding issue regarding the memory
footprint of the cross-entropy loss computation in the label smoothed loss
setting, designing a customized kernel to dramatically reduce memory
consumption without sacrificing speed or performance in comparison to existing
solutions for non-smoothed losses.

</details>


### [162] [Learning to Optimize Feedback for One Million Students: Insights from Multi-Armed and Contextual Bandits in Large-Scale Online Tutoring](https://arxiv.org/abs/2508.00270)
*Robin Schmucker,Nimish Pachapurkar,Shanmuga Bala,Miral Shah,Tom Mitchell*

Main category: cs.LG

TL;DR: 该论文介绍了一个在线辅导系统，该系统使用多臂bandit框架学习为学生提供有效的反馈。


<details>
  <summary>Details</summary>
Motivation: 构建一个在线辅导系统，该系统学习在学生回答错误后提供有效的反馈。

Method: 使用多臂bandit（MAB）框架和离线策略评估。

Result: MAB策略在166,000次练习中验证了学生成绩的显著提高。因果推断表明，虽然某些问题的某些操作表现出效果异质性，但效果大小可能太小，CB策略无法提供显著改进。

Conclusion: 上下文bandit策略在优化学生结果方面可能无法显著超越优化良好的多臂bandit策略。

Abstract: We present an online tutoring system that learns to provide effective
feedback to students after they answer questions incorrectly. Using data from
one million students, the system learns which assistance action (e.g., one of
multiple hints) to provide for each question to optimize student learning.
Employing the multi-armed bandit (MAB) framework and offline policy evaluation,
we assess 43,000 assistance actions, and identify trade-offs between assistance
policies optimized for different student outcomes (e.g., response correctness,
session completion). We design an algorithm that for each question decides on a
suitable policy training objective to enhance students' immediate second
attempt success and overall practice session performance. We evaluate the
resulting MAB policies in 166,000 practice sessions, verifying significant
improvements in student outcomes. While MAB policies optimize feedback for the
overall student population, we further investigate whether contextual bandit
(CB) policies can enhance outcomes by personalizing feedback based on
individual student features (e.g., ability estimates, response times). Using
causal inference, we examine (i) how effects of assistance actions vary across
students and (ii) whether CB policies, which leverage such effect
heterogeneity, outperform MAB policies. While our analysis reveals that some
actions for some questions exhibit effect heterogeneity, effect sizes may often
be too small for CB policies to provide significant improvements beyond what
well-optimized MAB policies that deliver the same action to all students
already achieve. We discuss insights gained from deploying data-driven systems
at scale and implications for future refinements. Today, the teaching policies
optimized by our system support thousands of students daily.

</details>


### [163] [Toward using explainable data-driven surrogate models for treating performance-based seismic design as an inverse engineering problem](https://arxiv.org/abs/2508.00286)
*Mohsen Zaker Esteghamati*

Main category: cs.LG

TL;DR: 本研究提出了一种基于机器学习的逆向工程方法，用于基于性能的抗震设计，并通过优化算法识别构件属性的最佳值。


<details>
  <summary>Details</summary>
Motivation: 本研究提出了一种将基于性能的抗震设计视为逆向工程问题的方法，其中直接推导出设计参数以实现特定的性能目标，解决了基于性能设计的计算效率低下问题。

Method: 通过实施可解释的机器学习模型，该方法直接映射设计变量和性能指标，解决了基于性能设计的计算效率低下问题。将生成的机器学习模型作为评估函数集成到遗传优化算法中，以解决逆问题。

Result: 结果表明，在不同的建筑类型、几何形状、抗震设计和场地危险中，代理模型具有很高的精度（例如，R2> 90%）。

Conclusion: 优化算法可以识别构件属性的最佳值，与工程原理一致。

Abstract: This study presents a methodology to treat performance-based seismic design
as an inverse engineering problem, where design parameters are directly derived
to achieve specific performance objectives. By implementing explainable machine
learning models, this methodology directly maps design variables and
performance metrics, tackling computational inefficiencies of performance-based
design. The resultant machine learning model is integrated as an evaluation
function into a genetic optimization algorithm to solve the inverse problem.
The developed methodology is then applied to two different inventories of steel
and concrete moment frames in Los Angeles and Charleston to obtain sectional
properties of frame members that minimize expected annualized seismic loss in
terms of repair costs. The results show high accuracy of the surrogate models
(e.g., R2> 90%) across a diverse set of building types, geometries, seismic
design, and site hazard, where the optimization algorithm could identify the
optimum values of members' properties for a fixed set of geometric variables,
consistent with engineering principles.

</details>


### [164] [Invariant Graph Transformer for Out-of-Distribution Generalization](https://arxiv.org/abs/2508.00304)
*Tianyin Liao,Ziwei Zhang,Yufei Sun,Chunyu Hu,Jianxin Li*

Main category: cs.LG

TL;DR: GOODFormer通过捕获预测图结构和标签之间的不变关系来学习广义图表示，从而优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的GTs专注于训练和测试来自相同分布的图数据，但在分布偏移下无法推广。图不变学习，旨在捕获分布偏移下具有标签的广义图结构模式，是一种潜在的有希望的解决方案，但如何设计基于图不变学习原则的注意机制以及位置和结构编码(PSEs)仍然具有挑战性。

Method: 我们引入了图分布外泛化Transformer (GOODFormer)，旨在通过联合优化三个模块来捕获预测图结构和标签之间的不变关系，从而学习广义图表示。

Result: 我们首先开发了一种基于GT的熵引导不变子图解缠器，以分离不变和变分子图，同时保持注意力函数的清晰度。接下来，我们设计了一种演化子图位置和结构编码器，以有效且高效地捕获训练期间动态变化的子图的编码信息。最后，我们提出了一种不变学习模块，该模块利用子图节点表示和编码来导出可以推广到看不见的图的广义图表示。我们还为我们的方法提供了理论依据。

Conclusion: 该方法在基准数据集上进行了大量实验，证明了该方法在分布偏移下优于最先进的基线。

Abstract: Graph Transformers (GTs) have demonstrated great effectiveness across various
graph analytical tasks. However, the existing GTs focus on training and testing
graph data originated from the same distribution, but fail to generalize under
distribution shifts. Graph invariant learning, aiming to capture generalizable
graph structural patterns with labels under distribution shifts, is potentially
a promising solution, but how to design attention mechanisms and positional and
structural encodings (PSEs) based on graph invariant learning principles
remains challenging. To solve these challenges, we introduce Graph
Out-Of-Distribution generalized Transformer (GOODFormer), aiming to learn
generalized graph representations by capturing invariant relationships between
predictive graph structures and labels through jointly optimizing three
modules. Specifically, we first develop a GT-based entropy-guided invariant
subgraph disentangler to separate invariant and variant subgraphs while
preserving the sharpness of the attention function. Next, we design an evolving
subgraph positional and structural encoder to effectively and efficiently
capture the encoding information of dynamically changing subgraphs during
training. Finally, we propose an invariant learning module utilizing subgraph
node representations and encodings to derive generalizable graph
representations that can to unseen graphs. We also provide theoretical
justifications for our method. Extensive experiments on benchmark datasets
demonstrate the superiority of our method over state-of-the-art baselines under
distribution shifts.

</details>


### [165] [PnP-DA: Towards Principled Plug-and-Play Integration of Variational Data Assimilation and Generative Models](https://arxiv.org/abs/2508.00325)
*Yongquan Qu,Matthieu Blanke,Sara Shamekh,Pierre Gentine*

Main category: cs.LG

TL;DR: 提出了一种新的数据同化方法，通过结合轻量级分析更新和预训练的生成先验来减少预测误差，并在混沌系统中优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 地球系统建模在科学计算中提出了一个根本性的挑战：在计算高效的模型中捕获复杂的多尺度非线性动力学，同时最大限度地减少由必要的简化引起的预测误差。即使是最强大的人工智能或基于物理的预测系统也会遭受逐渐累积的误差。

Method: 一种即插即用算法，该算法交替进行轻量级的、基于梯度的分析更新（使用关于新观测的马氏距离不匹配）与通过预训练的生成先验的单个前向传递，该先验通过条件 Wasserstein 耦合以背景预测为条件。

Result: 在标准混沌测试平台上的实验表明，该策略能够持续减少各种观测稀疏性和噪声水平下的预测误差，优于经典变分方法。

Conclusion: 该策略能够持续减少各种观测稀疏性和噪声水平下的预测误差，优于经典变分方法。

Abstract: Earth system modeling presents a fundamental challenge in scientific
computing: capturing complex, multiscale nonlinear dynamics in computationally
efficient models while minimizing forecast errors caused by necessary
simplifications. Even the most powerful AI- or physics-based forecast system
suffer from gradual error accumulation. Data assimilation (DA) aims to mitigate
these errors by optimally blending (noisy) observations with prior model
forecasts, but conventional variational methods often assume Gaussian error
statistics that fail to capture the true, non-Gaussian behavior of chaotic
dynamical systems. We propose PnP-DA, a Plug-and-Play algorithm that alternates
(1) a lightweight, gradient-based analysis update (using a Mahalanobis-distance
misfit on new observations) with (2) a single forward pass through a pretrained
generative prior conditioned on the background forecast via a conditional
Wasserstein coupling. This strategy relaxes restrictive statistical assumptions
and leverages rich historical data without requiring an explicit regularization
functional, and it also avoids the need to backpropagate gradients through the
complex neural network that encodes the prior during assimilation cycles.
Experiments on standard chaotic testbeds demonstrate that this strategy
consistently reduces forecast errors across a range of observation sparsities
and noise levels, outperforming classical variational methods.

</details>


### [166] [Embryology of a Language Model](https://arxiv.org/abs/2508.00331)
*George Wang,Garrett Baker,Andrew Gordon,Daniel Murfet*

Main category: cs.LG

TL;DR: This paper introduces an embryological approach, applying UMAP on susceptibility matrix, to visualize the model's structural development over training. The visualizations reveal the emergence of a clear body plan, charting the formation of known features and discovering previously unknown structures. Susceptibility analysis can uncover novel mechanisms, providing a holistic lens for studying the developmental principles of complex neural networks.


<details>
  <summary>Details</summary>
Motivation: Understanding how language models develop their internal computational structure is a central problem in the science of deep learning. While susceptibilities, drawn from statistical physics, offer a promising analytical tool, their full potential for visualizing network organization remains untapped.

Method: applying UMAP to the susceptibility matrix to visualize the model's structural development over training

Result: visualizations reveal the emergence of a clear body plan, charting the formation of known features like the induction circuit and discovering previously unknown structures, such as a spacing fin dedicated to counting space tokens.

Conclusion: Susceptibility analysis can move beyond validation to uncover novel mechanisms, providing a powerful, holistic lens for studying the developmental principles of complex neural networks.

Abstract: Understanding how language models develop their internal computational
structure is a central problem in the science of deep learning. While
susceptibilities, drawn from statistical physics, offer a promising analytical
tool, their full potential for visualizing network organization remains
untapped. In this work, we introduce an embryological approach, applying UMAP
to the susceptibility matrix to visualize the model's structural development
over training. Our visualizations reveal the emergence of a clear ``body
plan,'' charting the formation of known features like the induction circuit and
discovering previously unknown structures, such as a ``spacing fin'' dedicated
to counting space tokens. This work demonstrates that susceptibility analysis
can move beyond validation to uncover novel mechanisms, providing a powerful,
holistic lens for studying the developmental principles of complex neural
networks.

</details>


### [167] [BOOD: Boundary-based Out-Of-Distribution Data Generation](https://arxiv.org/abs/2508.00350)
*Qilin Liao,Shuo Yang,Bo Zhao,Ping Luo,Hengshuang Zhao*

Main category: cs.LG

TL;DR: BOOD通过在决策边界附近合成OOD数据，显著提高了OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 利用扩散模型合成基于潜在空间特征的辅助训练数据已被证明可以有效提高分布外(OOD)检测性能。然而，由于难以识别类之间的决策边界，在潜在空间中提取分布内(ID)边界之外的有效特征仍然具有挑战性。

Method: 提出了一种名为BOOD的新框架，该框架基于边界生成高质量的OOD特征，并使用扩散模型生成与人类兼容的异常图像。

Result: BOOD超越了最先进的方法，在CIFAR-100数据集上平均FPR95降低了29.64% (40.31% vs. 10.67%)，平均AUROC提高了7.27% (90.15% vs. 97.42%)。

Conclusion: BOOD在CIFAR-100数据集上实现了显著的性能提升，平均FPR95降低了29.64%，平均AUROC提高了7.27%。

Abstract: Harnessing the power of diffusion models to synthesize auxiliary training
data based on latent space features has proven effective in enhancing
out-of-distribution (OOD) detection performance. However, extracting effective
features outside the in-distribution (ID) boundary in latent space remains
challenging due to the difficulty of identifying decision boundaries between
classes. This paper proposes a novel framework called Boundary-based
Out-Of-Distribution data generation (BOOD), which synthesizes high-quality OOD
features and generates human-compatible outlier images using diffusion models.
BOOD first learns a text-conditioned latent feature space from the ID dataset,
selects ID features closest to the decision boundary, and perturbs them to
cross the decision boundary to form OOD features. These synthetic OOD features
are then decoded into images in pixel space by a diffusion model. Compared to
previous works, BOOD provides a more training efficient strategy for
synthesizing informative OOD features, facilitating clearer distinctions
between ID and OOD data. Extensive experimental results on common benchmarks
demonstrate that BOOD surpasses the state-of-the-art method significantly,
achieving a 29.64% decrease in average FPR95 (40.31% vs. 10.67%) and a 7.27%
improvement in average AUROC (90.15% vs. 97.42%) on the CIFAR-100 dataset.

</details>


### [168] [Sheaf Graph Neural Networks via PAC-Bayes Spectral Optimization](https://arxiv.org/abs/2508.00357)
*Yoonhyuk Choi,Jiho Choi,Chong-Kwon Kim*

Main category: cs.LG

TL;DR: SGPC：一种新的sheaf GNN，它结合了细胞-sheaf消息传递与最优传输-based lifting, variance-reduced diffusion, and PAC-Bayes spectral regularization，以实现鲁棒的半监督节点分类。


<details>
  <summary>Details</summary>
Motivation: 图神经网络 (GNN) 中的过度平滑会导致不同节点特征的崩溃，特别是在相邻节点通常具有不同标签的异质图上。

Method: 结合细胞-sheaf消息传递与多种机制，包括基于最优传输的提升、方差减少扩散和PAC-Bayes谱正则化。

Result: SGPC优于最先进的光谱和基于sheaf的GNN，同时提供未经训练节点的认证置信区间。

Conclusion: SGPC在同质和异质基准测试中优于最先进的光谱和基于sheaf的GNN，同时提供未经训练节点的认证置信区间。

Abstract: Over-smoothing in Graph Neural Networks (GNNs) causes collapse in distinct
node features, particularly on heterophilic graphs where adjacent nodes often
have dissimilar labels. Although sheaf neural networks partially mitigate this
problem, they typically rely on static or heavily parameterized sheaf
structures that hinder generalization and scalability. Existing sheaf-based
models either predefine restriction maps or introduce excessive complexity, yet
fail to provide rigorous stability guarantees. In this paper, we introduce a
novel scheme called SGPC (Sheaf GNNs with PAC-Bayes Calibration), a unified
architecture that combines cellular-sheaf message passing with several
mechanisms, including optimal transport-based lifting, variance-reduced
diffusion, and PAC-Bayes spectral regularization for robust semi-supervised
node classification. We establish performance bounds theoretically and
demonstrate that the resulting bound-aware objective can be achieved via
end-to-end training in linear computational complexity. Experiments on nine
homophilic and heterophilic benchmarks show that SGPC outperforms
state-of-the-art spectral and sheaf-based GNNs while providing certified
confidence intervals on unseen nodes.

</details>


### [169] [OID-PPO: Optimal Interior Design using Proximal Policy Optimization by Transforming Design Guidelines into Reward Functions](https://arxiv.org/abs/2508.00364)
*Chanyoung Yoon,Sangbong Yoo,Soobin Yim,Chansoo Kim,Yun Jang*

Main category: cs.LG

TL;DR: 提出了一种新颖的RL框架OID-PPO，用于优化室内设计，该框架优于现有技术水平的方法。


<details>
  <summary>Details</summary>
Motivation: 由于非结构化的空间布局、高计算需求以及对专家知识的依赖，住宅室内设计对居住者的满意度有很大影响，但仍然具有挑战性。基于优化或深度学习的现有方法要么计算成本高昂，要么受到数据稀缺的限制。强化学习（RL）方法通常将家具放置限制在离散位置，并且未能充分纳入设计原则。

Method: 一种新颖的RL框架，通过近端策略优化进行最优室内设计，该框架将专家定义的功能和视觉指南整合到结构化奖励函数中。OID-PPO利用对角高斯策略进行连续和灵活的家具放置，有效地探索部分可观察性下的潜在环境动态。

Result: 在各种房间形状和家具配置中进行的实验表明，OID-PPO在布局质量和计算效率方面明显优于最先进的方法。

Conclusion: OID-PPO在布局质量和计算效率方面明显优于现有方法。消融研究进一步证明了结构化指南整合的影响，并揭示了各个设计约束的不同贡献。

Abstract: Designing residential interiors strongly impacts occupant satisfaction but
remains challenging due to unstructured spatial layouts, high computational
demands, and reliance on expert knowledge. Existing methods based on
optimization or deep learning are either computationally expensive or
constrained by data scarcity. Reinforcement learning (RL) approaches often
limit furniture placement to discrete positions and fail to incorporate design
principles adequately. We propose OID-PPO, a novel RL framework for Optimal
Interior Design using Proximal Policy Optimization, which integrates
expert-defined functional and visual guidelines into a structured reward
function. OID-PPO utilizes a diagonal Gaussian policy for continuous and
flexible furniture placement, effectively exploring latent environmental
dynamics under partial observability. Experiments conducted across diverse room
shapes and furniture configurations demonstrate that OID-PPO significantly
outperforms state-of-the-art methods in terms of layout quality and
computational efficiency. Ablation studies further demonstrate the impact of
structured guideline integration and reveal the distinct contributions of
individual design constraints.

</details>


### [170] [Dual Adaptivity: Universal Algorithms for Minimizing the Adaptive Regret of Convex Functions](https://arxiv.org/abs/2508.00392)
*Lijun Zhang,Wenhao Yang,Guanghui Wang,Wei Jiang,Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: 研究了具有双重自适应的通用算法，该算法可以自动适应函数的性质以及环境的性质。


<details>
  <summary>Details</summary>
Motivation: 现有的算法缺乏通用性，因为它们只能处理一种类型的凸函数，并且需要参数的先验知识，这阻碍了它们在现实世界场景中的应用。

Method: 提出了一个元专家框架，其中多个专家被动态创建并通过元算法聚合。引入了两种策略（增加专家数量或增强专家能力）来实现通用性。结合了睡眠专家技术来捕获变化的环境。

Result: 所提出的算法能够同时最小化多种凸函数的自适应后悔值，并且允许函数类型在回合之间切换。

Conclusion: 提出了一个元专家框架用于双重自适应算法，该算法能够最小化多种凸函数的自适应后悔值，并且允许函数类型在回合之间切换。还将元专家框架扩展到在线复合优化，并开发了一种通用算法，用于最小化复合函数的自适应后悔值。

Abstract: To deal with changing environments, a new performance measure -- adaptive
regret, defined as the maximum static regret over any interval, was proposed in
online learning. Under the setting of online convex optimization, several
algorithms have been successfully developed to minimize the adaptive regret.
However, existing algorithms lack universality in the sense that they can only
handle one type of convex functions and need apriori knowledge of parameters,
which hinders their application in real-world scenarios. To address this
limitation, this paper investigates universal algorithms with dual adaptivity,
which automatically adapt to the property of functions (convex, exponentially
concave, or strongly convex), as well as the nature of environments (stationary
or changing). Specifically, we propose a meta-expert framework for dual
adaptive algorithms, where multiple experts are created dynamically and
aggregated by a meta-algorithm. The meta-algorithm is required to yield a
second-order bound, which can accommodate unknown function types. We further
incorporate the technique of sleeping experts to capture the changing
environments. For the construction of experts, we introduce two strategies
(increasing the number of experts or enhancing the capabilities of experts) to
achieve universality. Theoretical analysis shows that our algorithms are able
to minimize the adaptive regret for multiple types of convex functions
simultaneously, and also allow the type of functions to switch between rounds.
Moreover, we extend our meta-expert framework to online composite optimization,
and develop a universal algorithm for minimizing the adaptive regret of
composite functions.

</details>


### [171] [ExeKGLib: A Platform for Machine Learning Analytics based on Knowledge Graphs](https://arxiv.org/abs/2508.00394)
*Antonis Klironomos,Baifan Zhou,Zhipeng Tan,Zhuoxun Zheng,Mohamed H. Gad-Elrab,Heiko Paulheim,Evgeny Kharlamov*

Main category: cs.LG

TL;DR: ExeKGLib是一个Python库，它允许机器学习知识有限的用户构建机器学习管道。


<details>
  <summary>Details</summary>
Motivation: 当前，机器学习从业者可以使用大量在线的机器学习库。构建高质量的机器学习管道并非易事，需要培训、机器学习专业知识以及对每个步骤的认真开发。同时，科学和工程领域的领域专家可能不具备此类机器学习专业知识和培训，但他们迫切需要基于机器学习的分析。

Method: 构建了一个Python库ExeKGLib，该库具有图形界面层，允许用户构建ML管道。

Result: ExeKGLib还允许提高所构建的ML工作流的透明性和可重用性，并确保它们是可执行的。通过展示真实的用例，展示了ExeKGLib的可用性和实用性。

Conclusion: ExeKGLib通过图形界面层，允许只有最少机器学习知识的用户构建机器学习管道。通过依赖于以非机器学习专家可以理解的简单术语编码机器学习知识的知识图谱来实现这一点。

Abstract: Nowadays machine learning (ML) practitioners have access to numerous ML
libraries available online. Such libraries can be used to create ML pipelines
that consist of a series of steps where each step may invoke up to several ML
libraries that are used for various data-driven analytical tasks. Development
of high-quality ML pipelines is non-trivial; it requires training, ML
expertise, and careful development of each step. At the same time, domain
experts in science and engineering may not possess such ML expertise and
training while they are in pressing need of ML-based analytics. In this paper,
we present our ExeKGLib, a Python library enhanced with a graphical interface
layer that allows users with minimal ML knowledge to build ML pipelines. This
is achieved by relying on knowledge graphs that encode ML knowledge in simple
terms accessible to non-ML experts. ExeKGLib also allows improving the
transparency and reusability of the built ML workflows and ensures that they
are executable. We show the usability and usefulness of ExeKGLib by presenting
real use cases.

</details>


### [172] [Co-Reward: Self-supervised Reinforcement Learning for Large Language Model Reasoning via Contrastive Agreement](https://arxiv.org/abs/2508.00410)
*Zizhuo Zhang,Jianing Zhu,Xinmu Ge,Zihua Zhao,Zhanke Zhou,Xuan Li,Xiao Feng,Jiangchao Yao,Bo Han*

Main category: cs.LG

TL;DR: Co-Reward：一种新颖的强化学习框架，它利用跨语义类比问题的对比一致性作为奖励基础，以提高大型语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 由于依赖于人工标注标签，特别是对于复杂的任务，具有可验证奖励的强化学习（RLVR）在提高大型语言模型（LLM）的推理能力方面显示出希望，但扩展困境仍然存在。最近探索各种自奖励信号的替代方案表现出LLM推理的引发潜力，但存在不可忽略的崩溃问题。

Method: 利用跨语义类比问题的对比一致性作为奖励基础。通过为每个训练样本构建一个相似的问题（没有标签），并通过简单的rollout voting综合它们各自的替代标签，然后通过交叉引用每个问题对的标签来构建奖励，以加强跨类比输入的内部推理一致性。

Result: Co-Reward在多个推理基准测试和LLM系列上实现了优于其他自奖励基线的性能，并且达到甚至超过了真实标签奖励，在Llama-3.2-3B-Instruct上，MATH500的改进高达+6.8%。

Conclusion: Co-Reward在多个推理基准测试和LLM系列上实现了优于其他自奖励基线的性能，并且达到甚至超过了真实标签奖励，在Llama-3.2-3B-Instruct上，MATH500的改进高达+6.8%。

Abstract: Although reinforcement learning with verifiable rewards (RLVR) shows promise
in improving the reasoning ability of large language models (LLMs), the scaling
up dilemma remains due to the reliance on human annotated labels especially for
complex tasks. Recent alternatives that explore various self-reward signals
exhibit the eliciting potential of LLM reasoning, but suffer from the
non-negligible collapse issue. Inspired by the success of self-supervised
learning, we propose \textit{Co-Reward}, a novel RL framework that leverages
contrastive agreement across semantically analogical questions as a reward
basis. Specifically, we construct a similar question for each training sample
(without labels) and synthesize their individual surrogate labels through a
simple rollout voting, and then the reward is constructed by cross-referring
the labels of each question pair to enforce the internal reasoning consistency
across analogical inputs. Intuitively, such a self-supervised reward-shaping
mechanism increases the difficulty of learning collapse into a trivial
solution, and promotes stable reasoning elicitation and improvement through
expanding the input sample variants. Empirically, Co-Reward achieves superior
performance compared to other self-reward baselines on multiple reasoning
benchmarks and LLM series, and reaches or even surpasses ground-truth (GT)
labeled reward, with improvements of up to $+6.8\%$ on MATH500 over GT reward
on Llama-3.2-3B-Instruct. Our code is publicly available at
https://github.com/tmlr-group/Co-Reward.

</details>


### [173] [Transforming Credit Risk Analysis: A Time-Series-Driven ResE-BiLSTM Framework for Post-Loan Default Detection](https://arxiv.org/abs/2508.00415)
*Yue Yang,Yuxiang Lin,Ying Zhang,Zihan Su,Chang Chuan Goh,Tangtangfang Fang,Anthony Graham Bellotti,Boon Giin Lee*

Main category: cs.LG

TL;DR: 本研究提出了一种ResE-BiLSTM模型，用于预测贷款违约，实验表明该模型优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 预测贷款后的违约是信用风险管理中的一项重要任务，可以通过使用机器学习检测金融异常来实现。

Method: 引入了一种使用滑动窗口技术的ResE-BiLSTM模型，并在Freddie Mac美国抵押贷款数据集的44个独立队列上进行了评估。

Result: 实验结果表明，ResE-BiLSTM在包括准确率、精确率、召回率、F1和AUC在内的多个指标上，实现了优于基线模型的预测性能。

Conclusion: ResE-BiLSTM在预测性能上优于基线模型，具有实际应用价值。

Abstract: Prediction of post-loan default is an important task in credit risk
management, and can be addressed by detection of financial anomalies using
machine learning. This study introduces a ResE-BiLSTM model, using a sliding
window technique, and is evaluated on 44 independent cohorts from the extensive
Freddie Mac US mortgage dataset, to improve prediction performance. The
ResE-BiLSTM is compared with five baseline models: Long Short-Term Memory
(LSTM), BiLSTM, Gated Recurrent Units (GRU), Convolutional Neural Networks
(CNN), and Recurrent Neural Networks (RNN), across multiple metrics, including
Accuracy, Precision, Recall, F1, and AUC. An ablation study was conducted to
evaluate the contribution of individual components in the ResE-BiLSTM
architecture. Additionally, SHAP analysis was employed to interpret the
underlying features the model relied upon for its predictions. Experimental
results demonstrate that ResE-BiLSTM achieves superior predictive performance
compared to baseline models, underscoring its practical value and applicability
in real-world scenarios.

</details>


### [174] [A Conditional GAN for Tabular Data Generation with Probabilistic Sampling of Latent Subspaces](https://arxiv.org/abs/2508.00472)
*Leonidas Akritidis,Panayiotis Bozanis*

Main category: cs.LG

TL;DR: ctdGAN是一种用于缓解表格数据集中类别不平衡问题的条件GAN。


<details>
  <summary>Details</summary>
Motivation: 表格数据存在类别不平衡问题，这会导致各种机器学习任务的性能严重下降。现有的GAN模型没有考虑真实数据空间中输入样本的向量子空间，导致在任意位置生成数据，并且类别标签与其他类别变量的处理方式相同，因此按类别进行条件抽样的效果较差。

Method: 提出了一种新的概率抽样策略和一个新的损失函数，该函数可以惩罚聚类和类别的错误预测。还引入了一种简单的聚类缩放技术，可以在不影响数据维度的情况下捕获多个特征模式。

Result: 在14个不平衡数据集上的全面评估表明，ctdGAN在生成高保真样本和提高分类准确率方面表现出色。

Conclusion: ctdGAN在生成高保真样本和提高分类准确率方面表现出色。

Abstract: The tabular form constitutes the standard way of representing data in
relational database systems and spreadsheets. But, similarly to other forms,
tabular data suffers from class imbalance, a problem that causes serious
performance degradation in a wide variety of machine learning tasks. One of the
most effective solutions dictates the usage of Generative Adversarial Networks
(GANs) in order to synthesize artificial data instances for the
under-represented classes. Despite their good performance, none of the proposed
GAN models takes into account the vector subspaces of the input samples in the
real data space, leading to data generation in arbitrary locations. Moreover,
the class labels are treated in the same manner as the other categorical
variables during training, so conditional sampling by class is rendered less
effective. To overcome these problems, this study presents ctdGAN, a
conditional GAN for alleviating class imbalance in tabular datasets. Initially,
ctdGAN executes a space partitioning step to assign cluster labels to the input
samples. Subsequently, it utilizes these labels to synthesize samples via a
novel probabilistic sampling strategy and a new loss function that penalizes
both cluster and class mis-predictions. In this way, ctdGAN is trained to
generate samples in subspaces that resemble those of the original data
distribution. We also introduce several other improvements, including a simple,
yet effective cluster-wise scaling technique that captures multiple feature
modes without affecting data dimensionality. The exhaustive evaluation of
ctdGAN with 14 imbalanced datasets demonstrated its superiority in generating
high fidelity samples and improving classification accuracy.

</details>


### [175] [Court of LLMs: Evidence-Augmented Generation via Multi-LLM Collaboration for Text-Attributed Graph Anomaly Detection](https://arxiv.org/abs/2508.00507)
*Yiming Xu,Jiarun Chen,Zhen Peng,Zihan Chen,Qika Lin,Lan Ma,Bin Shi,Bo Dong*

Main category: cs.LG

TL;DR: This paper introduces CoLL, a framework combining LLMs and GNNs for anomaly detection in text-attributed graphs (TAGs).


<details>
  <summary>Details</summary>
Motivation: Existing GAD methods primarily focus on designing complex optimization objectives within the graph domain, overlooking the complementary value of the textual modality, whose features are often encoded by shallow embedding techniques, such as bag-of-words or skip-gram, so that semantic context related to anomalies may be missed. To unleash the enormous potential of textual modality, large language models (LLMs) have emerged as promising alternatives due to their strong semantic understanding and reasoning capabilities. Nevertheless, their application to TAG anomaly detection remains nascent, and they struggle to encode high-order structural information inherent in graphs due to input length constraints.

Method: CoLL employs multi-LLM collaboration for evidence-augmented generation to capture anomaly-relevant contexts while delivering human-readable rationales for detected anomalies. Moreover, CoLL integrates a GNN equipped with a gating mechanism to adaptively fuse textual features with evidence while preserving high-order topological information.

Result: Extensive experiments demonstrate the superiority of CoLL, achieving an average improvement of 13.37% in AP.

Conclusion: This study opens a new avenue for incorporating LLMs in advancing GAD.

Abstract: The natural combination of intricate topological structures and rich textual
information in text-attributed graphs (TAGs) opens up a novel perspective for
graph anomaly detection (GAD). However, existing GAD methods primarily focus on
designing complex optimization objectives within the graph domain, overlooking
the complementary value of the textual modality, whose features are often
encoded by shallow embedding techniques, such as bag-of-words or skip-gram, so
that semantic context related to anomalies may be missed. To unleash the
enormous potential of textual modality, large language models (LLMs) have
emerged as promising alternatives due to their strong semantic understanding
and reasoning capabilities. Nevertheless, their application to TAG anomaly
detection remains nascent, and they struggle to encode high-order structural
information inherent in graphs due to input length constraints. For
high-quality anomaly detection in TAGs, we propose CoLL, a novel framework that
combines LLMs and graph neural networks (GNNs) to leverage their complementary
strengths. CoLL employs multi-LLM collaboration for evidence-augmented
generation to capture anomaly-relevant contexts while delivering human-readable
rationales for detected anomalies. Moreover, CoLL integrates a GNN equipped
with a gating mechanism to adaptively fuse textual features with evidence while
preserving high-order topological information. Extensive experiments
demonstrate the superiority of CoLL, achieving an average improvement of 13.37%
in AP. This study opens a new avenue for incorporating LLMs in advancing GAD.

</details>


### [176] [Text-Attributed Graph Anomaly Detection via Multi-Scale Cross- and Uni-Modal Contrastive Learning](https://arxiv.org/abs/2508.00513)
*Yiming Xu,Xu Hua,Zhen Peng,Bin Shi,Jiarun Chen,Xingbo Fu,Song Wang,Bo Dong*

Main category: cs.LG

TL;DR: CMUCL 是一种用于文本属性图异常检测的端到端范例，它优于现有方法，并发布了 8 个数据集。


<details>
  <summary>Details</summary>
Motivation: 现有图异常检测管道通常涉及浅层嵌入技术，将文本信息编码为特征，然后依赖图域中复杂的自监督任务来检测异常。然而，这种文本编码过程与图域中的异常检测训练目标分离，难以确保提取的文本特征侧重于 GAD 相关信息，严重限制了检测能力。

Method: 提出了一种名为 CMUCL 的文本属性图异常检测端到端新范例，通过利用跨模态和单模态多尺度一致性来联合训练文本和图编码器，并设计了一个基于不一致性挖掘的异常评分估计器。

Result: CMUCL 在文本属性图异常检测方面取得了显著进展，平均精度（AP）比次优方法提高了 11.13%。发布了 8 个数据集以促进未来的研究。

Conclusion: CMUCL在文本属性图异常检测方面取得了显著进展，平均精度（AP）比次优方法提高了 11.13%。

Abstract: The widespread application of graph data in various high-risk scenarios has
increased attention to graph anomaly detection (GAD). Faced with real-world
graphs that often carry node descriptions in the form of raw text sequences,
termed text-attributed graphs (TAGs), existing graph anomaly detection
pipelines typically involve shallow embedding techniques to encode such textual
information into features, and then rely on complex self-supervised tasks
within the graph domain to detect anomalies. However, this text encoding
process is separated from the anomaly detection training objective in the graph
domain, making it difficult to ensure that the extracted textual features focus
on GAD-relevant information, seriously constraining the detection capability.
How to seamlessly integrate raw text and graph topology to unleash the vast
potential of cross-modal data in TAGs for anomaly detection poses a challenging
issue. This paper presents a novel end-to-end paradigm for text-attributed
graph anomaly detection, named CMUCL. We simultaneously model data from both
text and graph structures, and jointly train text and graph encoders by
leveraging cross-modal and uni-modal multi-scale consistency to uncover
potential anomaly-related information. Accordingly, we design an anomaly score
estimator based on inconsistency mining to derive node-specific anomaly scores.
Considering the lack of benchmark datasets tailored for anomaly detection on
TAGs, we release 8 datasets to facilitate future research. Extensive
evaluations show that CMUCL significantly advances in text-attributed graph
anomaly detection, delivering an 11.13% increase in average accuracy (AP) over
the suboptimal.

</details>


### [177] [Online Nonsubmodular Optimization with Delayed Feedback in the Bandit Setting](https://arxiv.org/abs/2508.00523)
*Sifan Yang,Yuanyu Wan,Lijun Zhang*

Main category: cs.LG

TL;DR: This paper introduces two new algorithms for online nonsubmodular optimization with delayed bandit feedback that achieve better regret bounds compared to existing methods, especially in scenarios with irregular delays.


<details>
  <summary>Details</summary>
Motivation: Previous work's regret bound relies on the maximum delay and couples the effects of delays and bandit feedback, making it sensitive to irregular delays. This paper aims to address these limitations.

Method: The paper develops two algorithms: DBGD-NF, which employs the one-point gradient estimator and utilizes all available estimated gradients, and an extension of DBGD-NF with a blocking update mechanism.

Result: The DBGD-NF algorithm achieves an improved regret bound of O(n(average delay)^(1/3)T^(2/3)). The extended algorithm achieves a regret bound of O(n(T^(2/3) + sqrt(dT))), which matches the O(nT^(2/3)) bound in the bandit setting without delayed feedback when d = O(T^(1/3)).

Conclusion: The paper proposes two algorithms, DBGD-NF and its extension with a blocking update mechanism, to improve the regret bound in online nonsubmodular optimization with delayed bandit feedback. Experiments on structured sparse learning demonstrate the superiority of the proposed methods.

Abstract: We investigate the online nonsubmodular optimization with delayed feedback in
the bandit setting, where the loss function is $\alpha$-weakly DR-submodular
and $\beta$-weakly DR-supermodular. Previous work has established an
$(\alpha,\beta)$-regret bound of $\mathcal{O}(nd^{1/3}T^{2/3})$, where $n$ is
the dimensionality and $d$ is the maximum delay. However, its regret bound
relies on the maximum delay and is thus sensitive to irregular delays.
Additionally, it couples the effects of delays and bandit feedback as its bound
is the product of the delay term and the $\mathcal{O}(nT^{2/3})$ regret bound
in the bandit setting without delayed feedback. In this paper, we develop two
algorithms to address these limitations, respectively. Firstly, we propose a
novel method, namely DBGD-NF, which employs the one-point gradient estimator
and utilizes all the available estimated gradients in each round to update the
decision. It achieves a better $\mathcal{O}(n\bar{d}^{1/3}T^{2/3})$ regret
bound, which is relevant to the average delay $\bar{d} =
\frac{1}{T}\sum_{t=1}^T d_t\leq d$. Secondly, we extend DBGD-NF by employing a
blocking update mechanism to decouple the joint effect of the delays and bandit
feedback, which enjoys an $\mathcal{O}(n(T^{2/3} + \sqrt{dT}))$ regret bound.
When $d = \mathcal{O}(T^{1/3})$, our regret bound matches the
$\mathcal{O}(nT^{2/3})$ bound in the bandit setting without delayed feedback.
Compared to our first $\mathcal{O}(n\bar{d}^{1/3}T^{2/3})$ bound, it is more
advantageous when the maximum delay $d = o(\bar{d}^{2/3}T^{1/3})$. Finally, we
conduct experiments on structured sparse learning to demonstrate the
superiority of our methods.

</details>


### [178] [Phase-Locked SNR Band Selection for Weak Mineral Signal Detection in Hyperspectral Imagery](https://arxiv.org/abs/2508.00539)
*Judy X Yang*

Main category: cs.LG

TL;DR: 提出了一种用于增强 Cuprite 矿区矿物检测的两阶段集成框架。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像为矿物填图提供了详细的光谱信息；然而，微弱的矿物特征通常被噪声和冗余波段所掩盖，从而限制了检测性能。

Method: 计算每个光谱波段的信噪比 (SNR)，并应用锁相阈值技术来丢弃低 SNR 波段，从而有效地消除冗余并抑制背景噪声。然后采用 Savitzky-Golay 滤波进行光谱平滑，首先在波段选择期间稳定趋势，其次在预处理期间保留细粒度的光谱特征。其次，将精炼的 HSI 数据重新引入模型，其中 KMeans 聚类用于提取 12 个端元光谱，然后使用非负最小二乘 (NNLS) 进行丰度解混。

Result: 实验结果证实，我们提出的管道提高了unmixing 准确性，并增强了对微弱矿物带的检测。

Conclusion: 该两阶段策略展示了一种在 HSI 地质应用中进行光谱降维和解混的实用且可重复的解决方案。

Abstract: Hyperspectral imaging offers detailed spectral information for mineral
mapping; however, weak mineral signatures are often masked by noisy and
redundant bands, limiting detection performance. To address this, we propose a
two-stage integrated framework for enhanced mineral detection in the Cuprite
mining district. In the first stage, we compute the signal-to-noise ratio (SNR)
for each spectral band and apply a phase-locked thresholding technique to
discard low-SNR bands, effectively removing redundancy and suppressing
background noise. Savitzky-Golay filtering is then employed for spectral
smoothing, serving a dual role first to stabilize trends during band selection,
and second to preserve fine-grained spectral features during preprocessing. In
the second stage, the refined HSI data is reintroduced into the model, where
KMeans clustering is used to extract 12 endmember spectra (W1 custom), followed
by non negative least squares (NNLS) for abundance unmixing. The resulting
endmembers are quantitatively compared with laboratory spectra (W1 raw) using
cosine similarity and RMSE metrics. Experimental results confirm that our
proposed pipeline improves unmixing accuracy and enhances the detection of weak
mineral zones. This two-pass strategy demonstrates a practical and reproducible
solution for spectral dimensionality reduction and unmixing in geological HSI
applications.

</details>


### [179] [Foundations of Interpretable Models](https://arxiv.org/abs/2508.00545)
*Pietro Barbiero,Mateo Espinosa Zarlenga,Alberto Termine,Mateja Jamnik,Giuseppe Marra*

Main category: cs.LG

TL;DR: The paper addresses the lack of actionable interpretability definitions by proposing a new definition and a blueprint for designing interpretable models, along with an open-sourced library.


<details>
  <summary>Details</summary>
Motivation: Existing definitions of interpretability are not actionable, failing to inform users about general, sound, and robust interpretable model design, making current interpretability research ill-posed.

Method: The authors propose a definition of interpretability that is general, simple, and subsumes existing informal notions within the interpretable AI community. Building on this, they propose a general blueprint for designing interpretable models and introduce an open-sourced library.

Result: The authors' definition is actionable, directly revealing the foundational properties, underlying assumptions, principles, data structures, and architectural features necessary for designing interpretable models.

Conclusion: The authors propose a general blueprint for designing interpretable models and introduce the first open-sourced library with native support for interpretable data structures and processes.

Abstract: We argue that existing definitions of interpretability are not actionable in
that they fail to inform users about general, sound, and robust interpretable
model design. This makes current interpretability research fundamentally
ill-posed. To address this issue, we propose a definition of interpretability
that is general, simple, and subsumes existing informal notions within the
interpretable AI community. We show that our definition is actionable, as it
directly reveals the foundational properties, underlying assumptions,
principles, data structures, and architectural features necessary for designing
interpretable models. Building on this, we propose a general blueprint for
designing interpretable models and introduce the first open-sourced library
with native support for interpretable data structures and processes.

</details>


### [180] [Learning Potential Energy Surfaces of Hydrogen Atom Transfer Reactions in Peptides](https://arxiv.org/abs/2508.00578)
*Marlen Neubert,Patrick Reiser,Frauke Gräter,Pascal Friederich*

Main category: cs.LG

TL;DR: 本文利用机器学习势能方法，对肽中的氢原子转移（HAT）反应进行了模拟，结果表明MACE模型表现最好，可用于大规模胶原蛋白模拟，以计算反应速率。


<details>
  <summary>Details</summary>
Motivation: 氢原子转移（HAT）反应在许多生物过程中至关重要，例如受损蛋白质中的自由基迁移，但对其机理途径的理解仍不完全。由于需要在生物学相关尺度上实现量子化学精度，因此模拟HAT具有挑战性；因此，经典力场和基于DFT的分子动力学均不适用。机器学习势提供了一种替代方案，能够以接近量子的精度学习势能面（PES）。

Method: 使用半经验方法和DFT系统地生成肽中的HAT构型，以构建大型数据集。在学习HAT PES和间接预测能量预测的反应势垒的能力方面，对三种图神经网络架构（SchNet、Allegro和MACE）进行了基准测试。

Result: MACE在能量、力和势垒预测方面始终优于其他模型，在分布外DFT势垒预测中实现了1.13 kcal/mol的平均绝对误差。

Conclusion: MACE在能量、力和势垒预测方面始终优于其他模型，在分布外DFT势垒预测中实现了1.13 kcal/mol的平均绝对误差。这种精度使得ML势能够整合到大规模胶原蛋白模拟中，以从预测的势垒计算反应速率，从而促进对肽中HAT和自由基迁移的机理理解。

Abstract: Hydrogen atom transfer (HAT) reactions are essential in many biological
processes, such as radical migration in damaged proteins, but their mechanistic
pathways remain incompletely understood. Simulating HAT is challenging due to
the need for quantum chemical accuracy at biologically relevant scales; thus,
neither classical force fields nor DFT-based molecular dynamics are applicable.
Machine-learned potentials offer an alternative, able to learn potential energy
surfaces (PESs) with near-quantum accuracy. However, training these models to
generalize across diverse HAT configurations, especially at radical positions
in proteins, requires tailored data generation and careful model selection.
Here, we systematically generate HAT configurations in peptides to build large
datasets using semiempirical methods and DFT. We benchmark three graph neural
network architectures (SchNet, Allegro, and MACE) on their ability to learn HAT
PESs and indirectly predict reaction barriers from energy predictions. MACE
consistently outperforms the others in energy, force, and barrier prediction,
achieving a mean absolute error of 1.13 kcal/mol on out-of-distribution DFT
barrier predictions. This accuracy enables integration of ML potentials into
large-scale collagen simulations to compute reaction rates from predicted
barriers, advancing mechanistic understanding of HAT and radical migration in
peptides. We analyze scaling laws, model transferability, and cost-performance
trade-offs, and outline strategies for improvement by combining ML potentials
with transition state search algorithms and active learning. Our approach is
generalizable to other biomolecular systems, enabling quantum-accurate
simulations of chemical reactivity in complex environments.

</details>


### [181] [The Role of Active Learning in Modern Machine Learning](https://arxiv.org/abs/2508.00586)
*Thorben Werner,Lars Schmidt-Thieme,Vijaya Krishna Yalavarthi*

Main category: cs.LG

TL;DR: AL 在低数据问题上效率低下，但与数据增强和半监督学习相结合时，可以提高性能。


<details>
  <summary>Details</summary>
Motivation: 主动学习 (AL) 得到了广泛的研究，但很少应用于其自身科学文献之外的背景中。我们认为原因是 AL 的高计算成本，以及在标记点较少的场景中，它通常能够产生的相对较小的提升。

Method: 研究了不同的方法来解决低数据场景的影响，即数据增强 (DA)、半监督学习 (SSL) 和 AL。

Result: AL 是解决低数据问题效率最低的方法，与随机抽样相比，仅产生了 1-4% 的提升，而 DA 和 SSL 方法与随机抽样结合使用时，可以产生高达 60% 的提升。然而，当 AL 与强大的 DA 和 SSL 技术相结合时，它仍然能够提供改进。

Conclusion: AL 应该被视为在应用适当的 DA 和 SSL 方法后，从数据中榨取最后一点性能的最终构建块，而不是解决缺少标签问题的方法。

Abstract: Even though Active Learning (AL) is widely studied, it is rarely applied in
contexts outside its own scientific literature. We posit that the reason for
this is AL's high computational cost coupled with the comparatively small lifts
it is typically able to generate in scenarios with few labeled points. In this
work we study the impact of different methods to combat this low data scenario,
namely data augmentation (DA), semi-supervised learning (SSL) and AL. We find
that AL is by far the least efficient method of solving the low data problem,
generating a lift of only 1-4\% over random sampling, while DA and SSL methods
can generate up to 60\% lift in combination with random sampling. However, when
AL is combined with strong DA and SSL techniques, it surprisingly is still able
to provide improvements. Based on these results, we frame AL not as a method to
combat missing labels, but as the final building block to squeeze the last bits
of performance out of data after appropriate DA and SSL methods as been
applied.

</details>


### [182] [Similarity-Based Self-Construct Graph Model for Predicting Patient Criticalness Using Graph Neural Networks and EHR Data](https://arxiv.org/abs/2508.00615)
*Mukesh Kumar Sahu,Pinki Roy*

Main category: cs.LG

TL;DR: 提出了一种新的图模型，用于预测 ICU 患者的死亡率和危重评分，并在 MIMIC-III 数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 准确预测 ICU 患者的危重程度（例如 ICU 内死亡风险）对于重症监护中的早期干预至关重要。然而，传统模型通常孤立地对待每个患者，并且难以利用电子健康记录 (EHR) 中的关系结构。

Method: 提出了一种基于相似性的自构建图模型 (SBSCGM)，该模型从多模式 EHR 数据中动态构建患者相似性图，以及一种 HybridGraphMedGNN 架构，该架构在该图上运行以预测患者死亡率和连续危重评分。

Result: 在来自 MIMIC-III 数据集的 6,000 个 ICU 住院数据的实验中，我们的模型实现了最先进的性能（AUC-ROC $0.94$），优于基线分类器和单类型 GNN 模型。我们还证明了改进的精确率/召回率，并表明注意力机制提供了对模型预测的可解释的见解。

Conclusion: 该框架为重症监护风险预测提供了一个可扩展且可解释的解决方案，有可能支持临床医生在现实世界的 ICU 部署。

Abstract: Accurately predicting the criticalness of ICU patients (such as in-ICU
mortality risk) is vital for early intervention in critical care. However,
conventional models often treat each patient in isolation and struggle to
exploit the relational structure in Electronic Health Records (EHR). We propose
a Similarity-Based Self-Construct Graph Model (SBSCGM) that dynamically builds
a patient similarity graph from multi-modal EHR data, and a HybridGraphMedGNN
architecture that operates on this graph to predict patient mortality and a
continuous criticalness score. SBSCGM uses a hybrid similarity measure
(combining feature-based and structural similarities) to connect patients with
analogous clinical profiles in real-time. The HybridGraphMedGNN integrates
Graph Convolutional Network (GCN), GraphSAGE, and Graph Attention Network (GAT)
layers to learn robust patient representations, leveraging both local and
global graph patterns. In experiments on 6,000 ICU stays from the MIMIC-III
dataset, our model achieves state-of-the-art performance (AUC-ROC $0.94$)
outperforming baseline classifiers and single-type GNN models. We also
demonstrate improved precision/recall and show that the attention mechanism
provides interpretable insights into model predictions. Our framework offers a
scalable and interpretable solution for critical care risk prediction, with
potential to support clinicians in real-world ICU deployment.

</details>


### [183] [IAMAP: Unlocking Deep Learning in QGIS for non-coders and limited computing resources](https://arxiv.org/abs/2508.00627)
*Paul Tresson,Pierre Le Coz,Hadrien Tulet,Anthony Malkassian,Maxime Réjou Méchain*

Main category: cs.LG

TL;DR: IAMAP 是一个用户友好的 QGIS 插件，它通过利用自监督学习策略中的最新进展，解决了遥感图像分析中的三个挑战。


<details>
  <summary>Details</summary>
Motivation: 深度学习的实施在很大程度上仍然局限于专家，并且一直不切实际，因为它通常需要 (i) 用于模型训练和验证的大型参考数据集；(ii) 大量的计算资源；(iii) 强大的编码技能。

Method: IAMAP 基于自监督学习策略的最新进展，现在提供强大的特征提取器，通常称为基础模型。 IAMAP 的界面允许用户简化遥感图像分析中的几个关键步骤：(i) 使用各种深度学习架构提取图像特征；(ii) 使用内置算法降低维度；(iii) 对特征或其缩减的表示执行聚类；(iv) 生成特征相似性图；(v) 校准和验证用于预测的监督机器学习模型。

Result: IAMAP 允许用户简化遥感图像分析中的几个关键步骤：(i) 使用各种深度学习架构提取图像特征；(ii) 使用内置算法降低维度；(iii) 对特征或其缩减的表示执行聚类；(iv) 生成特征相似性图；(v) 校准和验证用于预测的监督机器学习模型。

Conclusion: IAMAP 通过让非 AI 专家能够利用最新的深度学习方法提供的高质量特征，而无需 GPU 容量或大量的参考数据集，从而有助于计算效率和能源意识型深度学习方法的普及。

Abstract: Remote sensing has entered a new era with the rapid development of artificial
intelligence approaches. However, the implementation of deep learning has
largely remained restricted to specialists and has been impractical because it
often requires (i) large reference datasets for model training and validation;
(ii) substantial computing resources; and (iii) strong coding skills. Here, we
introduce IAMAP, a user-friendly QGIS plugin that addresses these three
challenges in an easy yet flexible way. IAMAP builds on recent advancements in
self-supervised learning strategies, which now provide robust feature
extractors, often referred to as foundation models. These generalist models can
often be reliably used in few-shot or zero-shot scenarios (i.e., with little to
no fine-tuning). IAMAP's interface allows users to streamline several key steps
in remote sensing image analysis: (i) extracting image features using a wide
range of deep learning architectures; (ii) reducing dimensionality with
built-in algorithms; (iii) performing clustering on features or their reduced
representations; (iv) generating feature similarity maps; and (v) calibrating
and validating supervised machine learning models for prediction. By enabling
non-AI specialists to leverage the high-quality features provided by recent
deep learning approaches without requiring GPU capacity or extensive reference
datasets, IAMAP contributes to the democratization of computationally efficient
and energy-conscious deep learning methods.

</details>


### [184] [Separated-Variable Spectral Neural Networks: A Physics-Informed Learning Approach for High-Frequency PDEs](https://arxiv.org/abs/2508.00628)
*Xiong Xiong,Zhuo Zhang,Rongchun Hu,Chen Gao,Zichen Deng*

Main category: cs.LG

TL;DR: SV-SNN: Solves high-frequency oscillatory PDEs by integrating separation of variables with adaptive spectral methods, achieving higher accuracy, fewer parameters, and faster training.


<details>
  <summary>Details</summary>
Motivation: Traditional physics-informed neural networks (PINNs) suffer from spectral bias, limiting their ability to capture high-frequency solution components.

Method: Integrating separation of variables with adaptive spectral methods

Result: achieves 1-3 orders of magnitude improvement in accuracy while reducing parameter count by over 90% and training time by 60%.

Conclusion: SV-SNN is an effective solution to the spectral bias problem in neural PDE solving.

Abstract: Solving high-frequency oscillatory partial differential equations (PDEs) is a
critical challenge in scientific computing, with applications in fluid
mechanics, quantum mechanics, and electromagnetic wave propagation. Traditional
physics-informed neural networks (PINNs) suffer from spectral bias, limiting
their ability to capture high-frequency solution components. We introduce
Separated-Variable Spectral Neural Networks (SV-SNN), a novel framework that
addresses these limitations by integrating separation of variables with
adaptive spectral methods. Our approach features three key innovations: (1)
decomposition of multivariate functions into univariate function products,
enabling independent spatial and temporal networks; (2) adaptive Fourier
spectral features with learnable frequency parameters for high-frequency
capture; and (3) theoretical framework based on singular value decomposition to
quantify spectral bias. Comprehensive evaluation on benchmark problems
including Heat equation, Helmholtz equation, Poisson equations and
Navier-Stokes equations demonstrates that SV-SNN achieves 1-3 orders of
magnitude improvement in accuracy while reducing parameter count by over 90\%
and training time by 60\%. These results establish SV-SNN as an effective
solution to the spectral bias problem in neural PDE solving. The implementation
will be made publicly available upon acceptance at
https://github.com/xgxgnpu/SV-SNN.

</details>


### [185] [KFS: KAN based adaptive Frequency Selection learning architecture for long term time series forecasting](https://arxiv.org/abs/2508.00635)
*Changning Wu,Gao Wu,Rongyao Cai,Yong Liu,Kexin Zhang*

Main category: cs.LG

TL;DR: propose a KAN based adaptive Frequency Selection learning architecture (KFS) to address these challenges.


<details>
  <summary>Details</summary>
Motivation: real-world time series exhibit noise interference across different scales, while heterogeneous information distribution among frequency components at varying scales leads to suboptimal multi-scale representation

Method: KAN based adaptive Frequency Selection learning architecture (KFS)

Result: performs energy-distribution-based dominant frequency selection in the spectral domain. Simultaneously, KAN enables sophisticated pattern representation while timestamp embedding alignment synchronizes temporal representations across scales. The feature mixing module then fuses scale-specific patterns with aligned temporal features.

Conclusion: KT achieves state-of-the-art performance as a simple yet effective architecture.

Abstract: Multi-scale decomposition architectures have emerged as predominant
methodologies in time series forecasting. However, real-world time series
exhibit noise interference across different scales, while heterogeneous
information distribution among frequency components at varying scales leads to
suboptimal multi-scale representation. Inspired by Kolmogorov-Arnold Networks
(KAN) and Parseval's theorem, we propose a KAN based adaptive Frequency
Selection learning architecture (KFS) to address these challenges. This
framework tackles prediction challenges stemming from cross-scale noise
interference and complex pattern modeling through its FreK module, which
performs energy-distribution-based dominant frequency selection in the spectral
domain. Simultaneously, KAN enables sophisticated pattern representation while
timestamp embedding alignment synchronizes temporal representations across
scales. The feature mixing module then fuses scale-specific patterns with
aligned temporal features. Extensive experiments across multiple real-world
time series datasets demonstrate that KT achieves state-of-the-art performance
as a simple yet effective architecture.

</details>


### [186] [Reinforcement Learning for Decision-Level Interception Prioritization in Drone Swarm Defense](https://arxiv.org/abs/2508.00641)
*Alessandro Palmas*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的防御系统，用于拦截低成本卡米卡兹无人机蜂群，该系统在模拟环境中优于基于规则的基线。


<details>
  <summary>Details</summary>
Motivation: 低成本卡米卡兹无人机蜂群日益增长的威胁对现代防御系统构成了严峻的挑战，需要快速和战略性的决策，以便在多个效应器和高价值目标区域之间优先进行拦截。

Method: 引入了一个高保真仿真环境，该环境捕获了实际的操作约束，在该环境中，决策层强化学习代理学习协调多个效应器以实现最佳拦截优先级。

Result: 基于强化学习的策略在数百个模拟攻击场景中始终比手工制作的基于规则的基线实现了更低的平均损伤和更高的防御效率。

Conclusion: 强化学习策略在保护关键区域方面始终实现了更低的平均损伤和更高的防御效率。这突出了强化学习作为防御体系结构中的战略层的潜力，增强了弹性，而无需取代现有的控制系统。

Abstract: The growing threat of low-cost kamikaze drone swarms poses a critical
challenge to modern defense systems demanding rapid and strategic
decision-making to prioritize interceptions across multiple effectors and
high-value target zones. In this work, we present a case study demonstrating
the practical advantages of reinforcement learning in addressing this
challenge. We introduce a high-fidelity simulation environment that captures
realistic operational constraints, within which a decision-level reinforcement
learning agent learns to coordinate multiple effectors for optimal interception
prioritization. Operating in a discrete action space, the agent selects which
drone to engage per effector based on observed state features such as
positions, classes, and effector status. We evaluate the learned policy against
a handcrafted rule-based baseline across hundreds of simulated attack
scenarios. The reinforcement learning based policy consistently achieves lower
average damage and higher defensive efficiency in protecting critical zones.
This case study highlights the potential of reinforcement learning as a
strategic layer within defense architectures, enhancing resilience without
displacing existing control systems. All code and simulation assets are
publicly released for full reproducibility, and a video demonstration
illustrates the policy's qualitative behavior.

</details>


### [187] [Light-Weight Diffusion Multiplier and Uncertainty Quantification for Fourier Neural Operators](https://arxiv.org/abs/2508.00643)
*Albert Matveev,Sanmitra Ghosh,Aamal Hussain,James-Michael Leahy,Michalis Michaelides*

Main category: cs.LG

TL;DR: DINOZAUR是一种基于扩散的神经算子参数化方法，具有不确定性量化，它可以减少参数数量和内存占用，同时提供有效的不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 由于过度参数化，FNO 面临着巨大的可扩展性挑战，并且没有提供原生的不确定性量化——这是可靠的科学和工程应用的关键要求。相反，神经算子依赖于忽略几何归纳偏差的事后 UQ 方法。

Method: 用具有不确定性量化的基于扩散的神经算子参数化方法DINOZAUR。

Result: DINOZAUR 用一个维度独立的扩散乘子代替了 FNO 中的密集张量乘子，该乘子每个通道都有一个可学习的时间参数，从而大大减少了参数数量和内存占用，而不会影响预测性能。通过在这些时间参数上定义先验，我们将 DINOZAUR 转换为贝叶斯神经算子，以产生空间相关的输出和校准的不确定性估计。

Conclusion: 该方法在提供有效不确定性量化的同时，在多个 PDE 基准测试中实现了有竞争力或更优越的性能。

Abstract: Operator learning is a powerful paradigm for solving partial differential
equations, with Fourier Neural Operators serving as a widely adopted
foundation. However, FNOs face significant scalability challenges due to
overparameterization and offer no native uncertainty quantification -- a key
requirement for reliable scientific and engineering applications. Instead,
neural operators rely on post hoc UQ methods that ignore geometric inductive
biases. In this work, we introduce DINOZAUR: a diffusion-based neural operator
parametrization with uncertainty quantification. Inspired by the structure of
the heat kernel, DINOZAUR replaces the dense tensor multiplier in FNOs with a
dimensionality-independent diffusion multiplier that has a single learnable
time parameter per channel, drastically reducing parameter count and memory
footprint without compromising predictive performance. By defining priors over
those time parameters, we cast DINOZAUR as a Bayesian neural operator to yield
spatially correlated outputs and calibrated uncertainty estimates. Our method
achieves competitive or superior performance across several PDE benchmarks
while providing efficient uncertainty quantification.

</details>


### [188] [TrajSurv: Learning Continuous Latent Trajectories from Electronic Health Records for Trustworthy Survival Prediction](https://arxiv.org/abs/2508.00657)
*Sihang Zeng,Lucas Jing Liu,Jun Wen,Meliha Yetisgen,Ruth Etzioni,Gang Luo*

Main category: cs.LG

TL;DR: TrajSurv模型从纵向EHR数据中学习连续潜在轨迹，用于可信的生存预测。


<details>
  <summary>Details</summary>
Motivation: 可信的生存预测对于临床决策至关重要。纵向电子健康记录（EHR）为预测提供了一个独特的强大机会。然而，准确建模患者潜在的连续临床进展，即不规则采样的临床特征，并将进展透明地与生存结果联系起来，具有挑战性。

Method: TrajSurv采用神经控制微分方程（NCDE）从不规则采样数据中提取连续时间潜在状态，形成连续潜在轨迹。为了确保潜在轨迹反映临床进展，TrajSurv通过时间感知对比学习方法将潜在状态空间与患者状态空间对齐。为了透明地将临床进展与生存结果联系起来，TrajSurv在两步分而治之的解释过程中使用潜在轨迹。首先，它解释了临床特征的变化如何使用学习的向量场转化为潜在轨迹的演变。其次，它对这些潜在轨迹进行聚类，以识别与不同生存结果相关的关键临床进展模式。

Result: TrajSurv具有竞争性的准确性和卓越的透明度。

Conclusion: TrajSurv在MIMIC-III和eICU这两个真实世界医疗数据集上的评估表明，与现有的深度学习方法相比，TrajSurv具有竞争性的准确性和卓越的透明度。

Abstract: Trustworthy survival prediction is essential for clinical decision making.
Longitudinal electronic health records (EHRs) provide a uniquely powerful
opportunity for the prediction. However, it is challenging to accurately model
the continuous clinical progression of patients underlying the irregularly
sampled clinical features and to transparently link the progression to survival
outcomes. To address these challenges, we develop TrajSurv, a model that learns
continuous latent trajectories from longitudinal EHR data for trustworthy
survival prediction. TrajSurv employs a neural controlled differential equation
(NCDE) to extract continuous-time latent states from the irregularly sampled
data, forming continuous latent trajectories. To ensure the latent trajectories
reflect the clinical progression, TrajSurv aligns the latent state space with
patient state space through a time-aware contrastive learning approach. To
transparently link clinical progression to the survival outcome, TrajSurv uses
latent trajectories in a two-step divide-and-conquer interpretation process.
First, it explains how the changes in clinical features translate into the
latent trajectory's evolution using a learned vector field. Second, it clusters
these latent trajectories to identify key clinical progression patterns
associated with different survival outcomes. Evaluations on two real-world
medical datasets, MIMIC-III and eICU, show TrajSurv's competitive accuracy and
superior transparency over existing deep learning methods.

</details>


### [189] [DP-DGAD: A Generalist Dynamic Graph Anomaly Detector with Dynamic Prototypes](https://arxiv.org/abs/2508.00664)
*Jialun Zheng,Jie Liu,Jiannong Cao,Xiao Wang,Hanchen Yang,Yankai Chen,Philip S. Yu*

Main category: cs.LG

TL;DR: 提出了一种名为DP-DGAD的模型，通过动态原型和自监督学习，在跨领域动态图异常检测中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的通用图异常检测模型难以捕捉动态图中的演化异常，并且新领域的不断出现和缺乏标签数据进一步带来了挑战。有效的跨领域DGAD需要领域特定和领域无关的异常模式。

Method: 提出了一种具有动态原型（DP）的DGAD模型，用于捕获演化的领域特定和领域无关的模式。该模型首先从时间自我图中提取动态原型，然后使用异常评分器将传入数据与动态原型进行比较，最后采用基于置信度的伪标签进行自监督适应。

Result: DP-DGAD在跨领域动态图异常检测中取得了最先进的性能。

Conclusion: DP-DGAD在十个真实世界数据集上表现出最先进的性能，证明了其有效性。

Abstract: Dynamic graph anomaly detection (DGAD) is essential for identifying anomalies
in evolving graphs across domains such as finance, traffic, and social
networks. Recently, generalist graph anomaly detection (GAD) models have shown
promising results. They are pretrained on multiple source datasets and
generalize across domains. While effective on static graphs, they struggle to
capture evolving anomalies in dynamic graphs. Moreover, the continuous
emergence of new domains and the lack of labeled data further challenge
generalist DGAD. Effective cross-domain DGAD requires both domain-specific and
domain-agnostic anomalous patterns. Importantly, these patterns evolve
temporally within and across domains. Building on these insights, we propose a
DGAD model with Dynamic Prototypes (DP) to capture evolving domain-specific and
domain-agnostic patterns. Firstly, DP-DGAD extracts dynamic prototypes, i.e.,
evolving representations of normal and anomalous patterns, from temporal
ego-graphs and stores them in a memory buffer. The buffer is selectively
updated to retain general, domain-agnostic patterns while incorporating new
domain-specific ones. Then, an anomaly scorer compares incoming data with
dynamic prototypes to flag both general and domain-specific anomalies. Finally,
DP-DGAD employs confidence-based pseudo-labeling for effective self-supervised
adaptation in target domains. Extensive experiments demonstrate
state-of-the-art performance across ten real-world datasets from different
domains.

</details>


### [190] [Wind Power Scenario Generation based on the Generalized Dynamic Factor Model and Generative Adversarial Network](https://arxiv.org/abs/2508.00692)
*Young-ho Cho,Hao Zhu,Duehee Lee,Ross Baldick*

Main category: cs.LG

TL;DR: 本文提出了一种结合广义动态因子模型（GDFM）和生成对抗网络（GAN）的方法，用于合成分布式风电场的风电场景，该方法在性能上优于其他替代方案。


<details>
  <summary>Details</summary>
Motivation: 为了进行资源充足性研究，通过使用时空特征同时合成多个分布式风电场的长期风电场景。

Method: 使用GAN提供一个过滤器，从观测数据中提取具有时间信息的动态因子，然后将此过滤器应用于GDFM，以表示合理波形的空间和频率相关性。

Result: 在澳大利亚的风电场景合成中，结合GDFM和GAN的方法优于其他替代方案。

Conclusion: 结合GDFM和GAN的方法在合成风电场景方面表现出性能改进，与直接合成的GAN相比，更好地实现了实际风电的合理统计特性。

Abstract: For conducting resource adequacy studies, we synthesize multiple long-term
wind power scenarios of distributed wind farms simultaneously by using the
spatio-temporal features: spatial and temporal correlation, waveforms, marginal
and ramp rates distributions of waveform, power spectral densities, and
statistical characteristics. Generating the spatial correlation in scenarios
requires the design of common factors for neighboring wind farms and
antithetical factors for distant wind farms. The generalized dynamic factor
model (GDFM) can extract the common factors through cross spectral density
analysis, but it cannot closely imitate waveforms. The GAN can synthesize
plausible samples representing the temporal correlation by verifying samples
through a fake sample discriminator. To combine the advantages of GDFM and GAN,
we use the GAN to provide a filter that extracts dynamic factors with temporal
information from the observation data, and we then apply this filter in the
GDFM to represent both spatial and frequency correlations of plausible
waveforms. Numerical tests on the combination of GDFM and GAN have demonstrated
performance improvements over competing alternatives in synthesizing wind power
scenarios from Australia, better realizing plausible statistical
characteristics of actual wind power compared to alternatives such as the GDFM
with a filter synthesized from distributions of actual dynamic filters and the
GAN with direct synthesis without dynamic factors.

</details>


### [191] [Classification of Psychiatry Clinical Notes by Diagnosis: A Deep Learning and Machine Learning Approach](https://arxiv.org/abs/2508.00695)
*Sergio Rubio-Martín,María Teresa García-Ordás,Antonio Serrano-García,Clara Margarita Franch-Pato,Arturo Crespo-Álvaro,José Alberto Benítez-Andrades*

Main category: cs.LG

TL;DR: This study compares traditional ML and DL models for classifying clinical notes into Anxiety and Adjustment Disorder diagnoses. Hyperparameter tuning is important in maximizing model performance.


<details>
  <summary>Details</summary>
Motivation: The classification of clinical notes into specific diagnostic categories is critical in healthcare, especially for mental health conditions like Anxiety and Adjustment Disorder.

Method: We compared the performance of traditional Machine Learning approaches (Random Forest, Support Vector Machine, K-nearest neighbors, Decision Tree, and eXtreme Gradient Boost) and Deep Learning models (DistilBERT and SciBERT), to classify clinical notes. Additionally, we implemented three oversampling strategies: No Oversampling, Random Oversampling, and Synthetic Minority Oversampling Technique (SMOTE), to assess their impact on model performance. Hyperparameter tuning was also applied to optimize model accuracy.

Result: Oversampling techniques had minimal impact on model performance overall. SMOTE showed a positive effect specifically with BERT-based models. Hyperparameter optimization significantly improved accuracy across the models. The Decision Tree and eXtreme Gradient Boost models achieved the highest accuracy among machine learning approaches, both reaching 96%, while the DistilBERT and SciBERT models also attained 96% accuracy in the deep learning category.

Conclusion: Hyperparameter optimization significantly improves the accuracy of models. Decision Tree, eXtreme Gradient Boost, DistilBERT and SciBERT models achieved the highest accuracy among all models, reaching 96%.

Abstract: The classification of clinical notes into specific diagnostic categories is
critical in healthcare, especially for mental health conditions like Anxiety
and Adjustment Disorder. In this study, we compare the performance of various
Artificial Intelligence models, including both traditional Machine Learning
approaches (Random Forest, Support Vector Machine, K-nearest neighbors,
Decision Tree, and eXtreme Gradient Boost) and Deep Learning models (DistilBERT
and SciBERT), to classify clinical notes into these two diagnoses.
Additionally, we implemented three oversampling strategies: No Oversampling,
Random Oversampling, and Synthetic Minority Oversampling Technique (SMOTE), to
assess their impact on model performance. Hyperparameter tuning was also
applied to optimize model accuracy. Our results indicate that oversampling
techniques had minimal impact on model performance overall. The only exception
was SMOTE, which showed a positive effect specifically with BERT-based models.
However, hyperparameter optimization significantly improved accuracy across the
models, enhancing their ability to generalize and perform on the dataset. The
Decision Tree and eXtreme Gradient Boost models achieved the highest accuracy
among machine learning approaches, both reaching 96%, while the DistilBERT and
SciBERT models also attained 96% accuracy in the deep learning category. These
findings underscore the importance of hyperparameter tuning in maximizing model
performance. This study contributes to the ongoing research on AI-assisted
diagnostic tools in mental health by providing insights into the efficacy of
different model architectures and data balancing methods.

</details>


### [192] [Learning Network Dismantling without Handcrafted Inputs](https://arxiv.org/abs/2508.00706)
*Haozhe Tian,Pietro Ferraro,Robert Shorten,Mahdi Jalili,Homayoun Hamedmoghadam*

Main category: cs.LG

TL;DR: 提出了一种新的消息传递框架，称为 MIND，用于解决网络拆除问题。


<details>
  <summary>Details</summary>
Motivation: 消息传递图神经网络在重要的网络科学问题中取得了突破。然而，竞争性能通常依赖于使用手工制作的结构特征作为输入，这增加了计算成本，并给纯粹的数据驱动网络表示引入了偏差。

Method: 引入了一种注意力机制，并利用消息迭代概况，此外还采用了一种有效的算法方法来生成结构多样化的小型合成网络训练集。

Result: 该模型在解决网络拆除这一 NP 难题时效率很高，几乎等同于重要节点识别，具有重要的实际应用。

Conclusion: 该模型在大型、未见过的真实网络上表现良好，优于最先进的网络拆除方法。所提出模型的效率和泛化性可以应用于一系列复杂的网络问题。

Abstract: The application of message-passing Graph Neural Networks has been a
breakthrough for important network science problems. However, the competitive
performance often relies on using handcrafted structural features as inputs,
which increases computational cost and introduces bias into the otherwise
purely data-driven network representations. Here, we eliminate the need for
handcrafted features by introducing an attention mechanism and utilizing
message-iteration profiles, in addition to an effective algorithmic approach to
generate a structurally diverse training set of small synthetic networks.
Thereby, we build an expressive message-passing framework and use it to
efficiently solve the NP-hard problem of Network Dismantling, virtually
equivalent to vital node identification, with significant real-world
applications. Trained solely on diversified synthetic networks, our proposed
model -- MIND: Message Iteration Network Dismantler -- generalizes to large,
unseen real networks with millions of nodes, outperforming state-of-the-art
network dismantling methods. Increased efficiency and generalizability of the
proposed model can be leveraged beyond dismantling in a range of complex
network problems.

</details>


### [193] [Efficient Solution and Learning of Robust Factored MDPs](https://arxiv.org/abs/2508.00707)
*Yannik Schnitzer,Alessandro Abate,David Parker*

Main category: cs.LG

TL;DR: 本文提出了一种基于分解状态空间表示的 r-MDP 求解和学习方法，该方法可以提高样本效率，并产生更有效的鲁棒策略。


<details>
  <summary>Details</summary>
Motivation: r-MDP 通过显式建模关于转移动态的认知不确定性来扩展 MDP。从与未知环境的交互中学习 r-MDP 能够合成具有可证明（PAC）性能保证的鲁棒策略，但这可能需要大量的样本交互。

Method: 基于分解状态空间表示，为解决和学习 r-MDP 提出了新方法，并展示了如何将分解 r-MDP 的策略综合转化为易于处理的线性规划。

Result: 利用分解结构可以提高样本效率。

Conclusion: 利用分解结构可以提高样本效率，产生更有效的鲁棒策略，并提供比现有技术方法更严格的性能保证。

Abstract: Robust Markov decision processes (r-MDPs) extend MDPs by explicitly modelling
epistemic uncertainty about transition dynamics. Learning r-MDPs from
interactions with an unknown environment enables the synthesis of robust
policies with provable (PAC) guarantees on performance, but this can require a
large number of sample interactions. We propose novel methods for solving and
learning r-MDPs based on factored state-space representations that leverage the
independence between model uncertainty across system components. Although
policy synthesis for factored r-MDPs leads to hard, non-convex optimisation
problems, we show how to reformulate these into tractable linear programs.
Building on these, we also propose methods to learn factored model
representations directly. Our experimental results show that exploiting
factored structure can yield dimensional gains in sample efficiency, producing
more effective robust policies with tighter performance guarantees than
state-of-the-art methods.

</details>


### [194] [JSON-Bag: A generic game trajectory representation](https://arxiv.org/abs/2508.00712)
*Dien Nguyen,Diego Perez-Liebana,Simon Lucas*

Main category: cs.LG

TL;DR: 提出了JSON-Bag模型来表示游戏轨迹，并在六个桌面游戏上进行了评估，结果表明该方法优于基线方法，并且可以使用tokens作为特征来提高精度。


<details>
  <summary>Details</summary>
Motivation: 为了通用地表示游戏轨迹。

Method: 提出JSON Bag-of-Tokens模型 (JSON-Bag)，通过tokenize JSON描述来表示游戏轨迹，并应用Jensen-Shannon distance (JSD) 作为距离度量。

Result: JSON-Bag原型可以有效地代表游戏轨迹类别，tokens可以作为特征来提高精度，Agent classes的JSON-Bag原型之间的JSD与agents策略之间的距离高度相关。

Conclusion: JSON-Bag在各种任务中优于使用手工特征的基线方法，并且可以使用tokens作为特征来提高精度。Agent classes的JSON-Bag原型之间的JSD与agents策略之间的距离高度相关。

Abstract: We introduce JSON Bag-of-Tokens model (JSON-Bag) as a method to generically
represent game trajectories by tokenizing their JSON descriptions and apply
Jensen-Shannon distance (JSD) as distance metric for them. Using a
prototype-based nearest-neighbor search (P-NNS), we evaluate the validity of
JSON-Bag with JSD on six tabletop games -- \textit{7 Wonders},
\textit{Dominion}, \textit{Sea Salt and Paper}, \textit{Can't Stop},
\textit{Connect4}, \textit{Dots and boxes} -- each over three game trajectory
classification tasks: classifying the playing agents, game parameters, or game
seeds that were used to generate the trajectories.
  Our approach outperforms a baseline using hand-crafted features in the
majority of tasks. Evaluating on N-shot classification suggests using JSON-Bag
prototype to represent game trajectory classes is also sample efficient.
Additionally, we demonstrate JSON-Bag ability for automatic feature extraction
by treating tokens as individual features to be used in Random Forest to solve
the tasks above, which significantly improves accuracy on underperforming
tasks. Finally, we show that, across all six games, the JSD between JSON-Bag
prototypes of agent classes highly correlates with the distances between
agents' policies.

</details>


### [195] [Nested Graph Pseudo-Label Refinement for Noisy Label Domain Adaptation Learning](https://arxiv.org/abs/2508.00716)
*Yingxu Wang,Mengzhu Wang,Zhichao Huang,Suyu Liu*

Main category: cs.LG

TL;DR: NeGPR 是一种用于图级域适应的新框架，具有噪声标签，它通过强制特征空间中的邻域一致性来预训练双分支，然后采用嵌套细化机制和噪声感知正则化策略。


<details>
  <summary>Details</summary>
Motivation: 现有 GDA 方法依赖于干净的源标签的假设，但在现实场景中，注释噪声普遍存在，这种情况很少成立。这种标签噪声严重损害了特征对齐，并降低了域转移下的适应性能。

Method: NeGPR 采用嵌套细化机制，其中一个分支选择高置信度的目标样本来指导另一个分支的适应，从而实现渐进式跨域学习。此外，NeGPR 结合了一种噪声感知正则化策略。

Result: NeGPR 在基准数据集上的大量实验表明，在严重的标签噪声下，NeGPR 始终优于最先进的方法，准确率提高了 12.7%。

Conclusion: NeGPR在严重标签噪声下始终优于最先进的方法，准确率提高了 12.7%。

Abstract: Graph Domain Adaptation (GDA) facilitates knowledge transfer from labeled
source graphs to unlabeled target graphs by learning domain-invariant
representations, which is essential in applications such as molecular property
prediction and social network analysis. However, most existing GDA methods rely
on the assumption of clean source labels, which rarely holds in real-world
scenarios where annotation noise is pervasive. This label noise severely
impairs feature alignment and degrades adaptation performance under domain
shifts. To address this challenge, we propose Nested Graph Pseudo-Label
Refinement (NeGPR), a novel framework tailored for graph-level domain
adaptation with noisy labels. NeGPR first pretrains dual branches, i.e.,
semantic and topology branches, by enforcing neighborhood consistency in the
feature space, thereby reducing the influence of noisy supervision. To bridge
domain gaps, NeGPR employs a nested refinement mechanism in which one branch
selects high-confidence target samples to guide the adaptation of the other,
enabling progressive cross-domain learning. Furthermore, since pseudo-labels
may still contain noise and the pre-trained branches are already overfitted to
the noisy labels in the source domain, NeGPR incorporates a noise-aware
regularization strategy. This regularization is theoretically proven to
mitigate the adverse effects of pseudo-label noise, even under the presence of
source overfitting, thus enhancing the robustness of the adaptation process.
Extensive experiments on benchmark datasets demonstrate that NeGPR consistently
outperforms state-of-the-art methods under severe label noise, achieving gains
of up to 12.7% in accuracy.

</details>


### [196] [Democratizing Tabular Data Access with an Open$\unicode{x2013}$Source Synthetic$\unicode{x2013}$Data SDK](https://arxiv.org/abs/2508.00718)
*Ivona Krchova,Mariana Vargas Vieyra,Mario Scriminaci,Andrey Sidorenko*

Main category: cs.LG

TL;DR: MOSTLY AI发布了一个用于合成表格数据的高质量开源工具包，该工具包具有差分隐私、公平性感知和自动化质量保证等功能，已被广泛采用。


<details>
  <summary>Details</summary>
Motivation: 由于隐私、专有权益和道德问题，数据可访问性的限制日益增加。合成数据提供了一种可行的解决方案，可以在不损害敏感信息的情况下安全、广泛地使用数据。

Method: 该SDK利用TabularARGN自回归框架。

Result: 该SDK具有差分隐私保证、公平感知数据生成和自动质量保证等强大功能，并具有灵活且可访问的Python界面。它支持各种数据类型以及复杂的多表和顺序数据集，提供具有竞争力的性能，并在速度和可用性方面有显著改进。

Conclusion: MOSTLY AI Synthetic Data Software Development Kit (SDK)已被广泛采用，证明了其在解决实际数据瓶颈和促进广泛数据民主化方面的实用性。

Abstract: Machine learning development critically depends on access to high-quality
data. However, increasing restrictions due to privacy, proprietary interests,
and ethical concerns have created significant barriers to data accessibility.
Synthetic data offers a viable solution by enabling safe, broad data usage
without compromising sensitive information. This paper presents the MOSTLY AI
Synthetic Data Software Development Kit (SDK), an open-source toolkit designed
specifically for synthesizing high-quality tabular data. The SDK integrates
robust features such as differential privacy guarantees, fairness-aware data
generation, and automated quality assurance into a flexible and accessible
Python interface. Leveraging the TabularARGN autoregressive framework, the SDK
supports diverse data types and complex multi-table and sequential datasets,
delivering competitive performance with notable improvements in speed and
usability. Currently deployed both as a cloud service and locally installable
software, the SDK has seen rapid adoption, highlighting its practicality in
addressing real-world data bottlenecks and promoting widespread data
democratization.

</details>


### [197] [Adaptive Machine Learning-Driven Multi-Fidelity Stratified Sampling for Failure Analysis of Nonlinear Stochastic Systems](https://arxiv.org/abs/2508.00734)
*Liuyun Xu,Seymour M. J. Spence*

Main category: cs.LG

TL;DR: 提出了一种多保真分层抽样方案，该方案使用自适应机器学习元模型，用于有效地传播不确定性并估计小的失效概率。


<details>
  <summary>Details</summary>
Motivation: 现有的用于罕见事件分析的随机模拟中的方差减少技术仍然需要大量的模型评估来估计小的失效概率。在复杂的非线性有限元建模环境中，这可能在计算上具有挑战性，特别是对于受到随机激励的系统。

Method: 一种具有自适应机器学习元模型的多保真分层抽样方案

Result: 通过将低保真输出与额外的高保真结果集成，使用多保真蒙特卡罗框架获得层状失效概率的无偏估计。然后使用全概率定理计算总失效概率。

Conclusion: 该方案可以准确估计感兴趣的非线性响应的超越概率曲线，同时与单保真方差减少方法相比，实现了显着的计算节省。

Abstract: Existing variance reduction techniques used in stochastic simulations for
rare event analysis still require a substantial number of model evaluations to
estimate small failure probabilities. In the context of complex, nonlinear
finite element modeling environments, this can become computationally
challenging-particularly for systems subjected to stochastic excitation. To
address this challenge, a multi-fidelity stratified sampling scheme with
adaptive machine learning metamodels is introduced for efficiently propagating
uncertainties and estimating small failure probabilities. In this approach, a
high-fidelity dataset generated through stratified sampling is used to train a
deep learning-based metamodel, which then serves as a cost-effective and highly
correlated low-fidelity model. An adaptive training scheme is proposed to
balance the trade-off between approximation quality and computational demand
associated with the development of the low-fidelity model. By integrating the
low-fidelity outputs with additional high-fidelity results, an unbiased
estimate of the strata-wise failure probabilities is obtained using a
multi-fidelity Monte Carlo framework. The overall probability of failure is
then computed using the total probability theorem. Application to a full-scale
high-rise steel building subjected to stochastic wind excitation demonstrates
that the proposed scheme can accurately estimate exceedance probability curves
for nonlinear responses of interest, while achieving significant computational
savings compared to single-fidelity variance reduction approaches.

</details>


### [198] [A Simple and Effective Method for Uncertainty Quantification and OOD Detection](https://arxiv.org/abs/2508.00754)
*Yaxin Ma,Benjamin Colburn,Jose C. Principe*

Main category: cs.LG

TL;DR: A single deterministic model is used to quantify uncertainty for distributional shifts and OOD detection by comparing feature space densities of training and test samples.


<details>
  <summary>Details</summary>
Motivation: Bayesian neural networks and deep ensemble methods are computationally intensive and require large storage. By utilizing a single deterministic model, we can solve the above issue.

Method: An effective method based on feature space density to quantify uncertainty for distributional shifts and out-of-distribution (OOD) detection, leveraging the information potential field derived from kernel density estimation to approximate the feature space density of the training set.

Result: The method effectively determines whether a distributional shift has occurred by comparing the density with the feature space representation of test samples.

Conclusion: The proposed method outperforms baseline models in experiments on a 2D synthetic dataset and an OOD detection task.

Abstract: Bayesian neural networks and deep ensemble methods have been proposed for
uncertainty quantification; however, they are computationally intensive and
require large storage. By utilizing a single deterministic model, we can solve
the above issue. We propose an effective method based on feature space density
to quantify uncertainty for distributional shifts and out-of-distribution (OOD)
detection. Specifically, we leverage the information potential field derived
from kernel density estimation to approximate the feature space density of the
training set. By comparing this density with the feature space representation
of test samples, we can effectively determine whether a distributional shift
has occurred. Experiments were conducted on a 2D synthetic dataset (Two Moons
and Three Spirals) as well as an OOD detection task (CIFAR-10 vs. SVHN). The
results demonstrate that our method outperforms baseline models.

</details>


### [199] [Diffusion-Scheduled Denoising Autoencoders for Anomaly Detection in Tabular Data](https://arxiv.org/abs/2508.00758)
*Timur Sattarov,Marco Schreyer,Damian Borth*

Main category: cs.LG

TL;DR: This paper introduces DDAE, a new anomaly detection method for tabular data that uses diffusion-based noise scheduling and contrastive learning. It outperforms existing methods in semi-supervised settings and is competitive in unsupervised settings.


<details>
  <summary>Details</summary>
Motivation: Anomaly detection in tabular data is challenging due to complex feature interactions and the scarcity of anomalous examples. Denoising autoencoders rely on fixed-magnitude noise, limiting adaptability to diverse data distributions. Diffusion models lack explicit reconstruction mappings.

Method: The paper proposes the Diffusion-Scheduled Denoising Autoencoder (DDAE), a framework that integrates diffusion-based noise scheduling and contrastive learning into the encoding process.

Result: DDAE outperforms in semi-supervised settings and achieves competitive results in unsupervised settings on 57 datasets from ADBench, improving PR-AUC by up to 65% (9%) and ROC-AUC by 16% (6%) over state-of-the-art autoencoder (diffusion) model baselines.

Conclusion: The paper concludes that principled noise strategies are important in tabular anomaly detection. Higher noise levels benefit unsupervised training, while lower noise with linear scheduling is optimal in semi-supervised settings.

Abstract: Anomaly detection in tabular data remains challenging due to complex feature
interactions and the scarcity of anomalous examples. Denoising autoencoders
rely on fixed-magnitude noise, limiting adaptability to diverse data
distributions. Diffusion models introduce scheduled noise and iterative
denoising, but lack explicit reconstruction mappings. We propose the
Diffusion-Scheduled Denoising Autoencoder (DDAE), a framework that integrates
diffusion-based noise scheduling and contrastive learning into the encoding
process to improve anomaly detection. We evaluated DDAE on 57 datasets from
ADBench. Our method outperforms in semi-supervised settings and achieves
competitive results in unsupervised settings, improving PR-AUC by up to 65%
(9%) and ROC-AUC by 16% (6%) over state-of-the-art autoencoder (diffusion)
model baselines. We observed that higher noise levels benefit unsupervised
training, while lower noise with linear scheduling is optimal in
semi-supervised settings. These findings underscore the importance of
principled noise strategies in tabular anomaly detection.

</details>


### [200] [Evaluating Angle and Amplitude Encoding Strategies for Variational Quantum Machine Learning: their impact on model's accuracy](https://arxiv.org/abs/2508.00768)
*Antonio Tudisco,Andrea Marchesin,Maurizio Zamboni,Mariagrazia Graziano,Giovanna Turvani*

Main category: cs.LG

TL;DR: 研究了量子机器学习 (QML) 中旋转门对变分量子电路 (VQC) 模型分类性能的影响，发现旋转门的选择会显著影响模型性能。


<details>
  <summary>Details</summary>
Motivation: 量子计算和机器学习的最新进展提高了人们对量子机器学习 (QML) 的关注，QML 旨在通过利用量子计算范例来开发机器学习模型。该领域中广泛使用的模型之一是变分量子电路 (VQC)，这是一种混合模型，其中量子电路处理数据推理，而经典优化调整电路的参数。

Method: 通过考虑振幅编码和角度编码模型，并检查应用的旋转门的类型如何影响模型的分类性能来执行分析。通过在葡萄酒和糖尿病两个数据集上训练不同的模型并评估它们的性能来进行比较。

Result: 在相同的模型拓扑下，最佳模型和最差模型之间的准确度差异范围为 10% 到 30%，差异高达 41%。

Conclusion: 模型的分类性能会受到编码中使用的旋转门选择的显著影响，嵌入是 VQC 模型的超参数。

Abstract: Recent advancements in Quantum Computing and Machine Learning have increased
attention to Quantum Machine Learning (QML), which aims to develop machine
learning models by exploiting the quantum computing paradigm. One of the widely
used models in this area is the Variational Quantum Circuit (VQC), a hybrid
model where the quantum circuit handles data inference while classical
optimization adjusts the parameters of the circuit. The quantum circuit
consists of an encoding layer, which loads data into the circuit, and a
template circuit, known as the ansatz, responsible for processing the data.
This work involves performing an analysis by considering both Amplitude- and
Angle-encoding models, and examining how the type of rotational gate applied
affects the classification performance of the model. This comparison is carried
out by training the different models on two datasets, Wine and Diabetes, and
evaluating their performance. The study demonstrates that, under identical
model topologies, the difference in accuracy between the best and worst models
ranges from 10% to 30%, with differences reaching up to 41%. Moreover, the
results highlight how the choice of rotational gates used in encoding can
significantly impact the model's classification performance. The findings
confirm that the embedding represents a hyperparameter for VQC models.

</details>


### [201] [Explainable AI and Machine Learning for Exam-based Student Evaluation: Causal and Predictive Analysis of Socio-academic and Economic Factors](https://arxiv.org/abs/2508.00785)
*Bushra Akter,Md Biplob Hosen,Sabbir Ahmed,Mehrin Anannya,Md. Farhad Hossain*

Main category: cs.LG

TL;DR: 本研究调查了影响学生 CGPA 的社会学术和经济因素，使用各种模型和可解释人工智能技术，为学生开发了一个网络应用程序，以提高他们的学业成绩。


<details>
  <summary>Details</summary>
Motivation: 学业成绩取决于社会学术和经济因素的多变量联系。

Method: 文献综述、在线调查、因果分析、回归模型、分类模型和可解释人工智能技术

Result: 岭回归表现出很强的预测准确性，平均绝对误差为 0.12，均方误差为 0.023。随机森林在分类中表现出色，F1 分数接近完美，准确率为 98.68%。

Conclusion: 开发了一个网络应用程序，为学生提供个性化的见解，预测学业成绩，并识别需要改进的领域。

Abstract: Academic performance depends on a multivariable nexus of socio-academic and
financial factors. This study investigates these influences to develop
effective strategies for optimizing students' CGPA. To achieve this, we
reviewed various literature to identify key influencing factors and constructed
an initial hypothetical causal graph based on the findings. Additionally, an
online survey was conducted, where 1,050 students participated, providing
comprehensive data for analysis. Rigorous data preprocessing techniques,
including cleaning and visualization, ensured data quality before analysis.
Causal analysis validated the relationships among variables, offering deeper
insights into their direct and indirect effects on CGPA. Regression models were
implemented for CGPA prediction, while classification models categorized
students based on performance levels. Ridge Regression demonstrated strong
predictive accuracy, achieving a Mean Absolute Error of 0.12 and a Mean Squared
Error of 0.023. Random Forest outperformed in classification, attaining an
F1-score near perfection and an accuracy of 98.68%. Explainable AI techniques
such as SHAP, LIME, and Interpret enhanced model interpretability, highlighting
critical factors such as study hours, scholarships, parental education, and
prior academic performance. The study culminated in the development of a
web-based application that provides students with personalized insights,
allowing them to predict academic performance, identify areas for improvement,
and make informed decisions to enhance their outcomes.

</details>


### [202] [Adacc: Adaptive Compression and Activation Checkpointing for LLM Memory Management](https://arxiv.org/abs/2508.00806)
*Ping Chen,Zhuohong Deng,Ping Li,Shuibing He,Hongzi Zhu,Yi Zheng,Zhefeng Wang,Baoxing Huai,Minyi Guo*

Main category: cs.LG

TL;DR: Adacc is a memory management framework that combines adaptive compression and activation checkpointing to reduce the GPU memory footprint and accelerate LLM training.


<details>
  <summary>Details</summary>
Motivation: Recomputation in large language model training introduces up to 30% overhead due to memory pressure.

Method: Adaptive compression and activation checkpointing with layer-specific compression algorithms, optimal scheduling policy using MILP, and adaptive policy evolution mechanism.

Result: Adacc accelerates LLM training by 1.01x to 1.37x compared to state-of-the-art frameworks, while maintaining comparable model accuracy.

Conclusion: Adacc accelerates LLM training by 1.01x to 1.37x compared to state-of-the-art frameworks, while maintaining comparable model accuracy to the Baseline.

Abstract: Training large language models often employs recomputation to alleviate
memory pressure, which can introduce up to 30% overhead in real-world
scenarios. In this paper, we propose Adacc, a novel memory management framework
that combines adaptive compression and activation checkpointing to reduce the
GPU memory footprint. It comprises three modules: (1) We design layer-specific
compression algorithms that account for outliers in LLM tensors, instead of
directly quantizing floats from FP16 to INT4, to ensure model accuracy. (2) We
propose an optimal scheduling policy that employs MILP to determine the best
memory optimization for each tensor. (3) To accommodate changes in training
tensors, we introduce an adaptive policy evolution mechanism that adjusts the
policy during training to enhance throughput. Experimental results show that
Adacc can accelerate the LLM training by 1.01x to 1.37x compared to
state-of-the-art frameworks, while maintaining comparable model accuracy to the
Baseline.

</details>
