{"id": "2511.04901", "categories": ["cs.IR", "cs.CL", "H.3.3"], "pdf": "https://arxiv.org/pdf/2511.04901", "abs": "https://arxiv.org/abs/2511.04901", "authors": ["Anthony Gamst", "Lawrence Wilson"], "title": "Association via Entropy Reduction", "comment": null, "summary": "Prior to recent successes using neural networks, term frequency-inverse\ndocument frequency (tf-idf) was clearly regarded as the best choice for\nidentifying documents related to a query. We provide a different score, aver,\nand observe, on a dataset with ground truth marking for association, that aver\ndoes do better at finding assciated pairs than tf-idf. This example involves\nfinding associated vertices in a large graph and that may be an area where\nneural networks are not currently an obvious best choice. Beyond this one\nanecdote, we observe that (1) aver has a natural threshold for declaring pairs\nas unassociated while tf-idf does not, (2) aver can distinguish between pairs\nof documents for which tf-idf gives a score of 1.0, (3) aver can be applied to\nlarger collections of documents than pairs while tf-idf cannot, and (4) that\naver is derived from entropy under a simple statistical model while tf-idf is a\nconstruction designed to achieve a certain goal and hence aver may be more\n\"natural.\" To be fair, we also observe that (1) writing down and computing the\naver score for a pair is more complex than for tf-idf and (2) that the fact\nthat the aver score is naturally scale-free makes it more complicated to\ninterpret aver scores.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u5206\u65b9\u6cd5 aver\uff0c\u7528\u4e8e\u8bc6\u522b\u4e0e\u67e5\u8be2\u76f8\u5173\u7684\u6587\u6863\uff0c\u7279\u522b\u662f\u5728\u795e\u7ecf\u7f51\u7edc\u4e0d\u5360\u4f18\u52bf\u7684\u5927\u578b\u56fe\u7ed3\u6784\u4e2d\u5bfb\u627e\u5173\u8054\u9876\u70b9\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5 tf-idf \u5728\u8bc6\u522b\u76f8\u5173\u6587\u6863\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5b58\u5728\u4e00\u4e9b\u5c40\u9650\u6027\u3002\u672c\u6587\u65e8\u5728\u5bfb\u627e\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u7279\u5b9a\u573a\u666f\u4e0b tf-idf \u8868\u73b0\u4e0d\u4f73\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a aver \u7684\u8bc4\u5206\u65b9\u6cd5\uff0c\u5e76\u4e0e tf-idf \u5728\u5177\u6709\u5173\u8054\u6807\u8bb0\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5bfb\u627e\u5173\u8054\u5bf9\u65b9\u9762\uff0caver \u4f18\u4e8e tf-idf\u3002\u6b64\u5916\uff0caver \u5728\u9608\u503c\u8bbe\u5b9a\u3001\u533a\u5206\u6587\u6863\u5bf9\u3001\u5e94\u7528\u4e8e\u5927\u578b\u6587\u6863\u96c6\u5408\u4ee5\u53ca\u81ea\u7136\u6027\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684 aver \u8bc4\u5206\u65b9\u6cd5\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u4f18\u4e8e tf-idf\uff0c\u5e76\u5177\u6709\u4e00\u4e9b\u72ec\u7279\u7684\u4f18\u52bf\uff0c\u4f46\u8ba1\u7b97\u590d\u6742\u4e14\u4e0d\u6613\u89e3\u91ca\u3002"}}
{"id": "2511.04939", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04939", "abs": "https://arxiv.org/abs/2511.04939", "authors": ["Harshit Nainwani", "Hediyeh Baban"], "title": "Search Is Not Retrieval: Decoupling Semantic Matching from Contextual Assembly in RAG", "comment": "22 pages, 2 figures, technical framework paper", "summary": "Retrieval systems are essential to contemporary AI pipelines, although most\nconfuse two separate processes: finding relevant information and giving enough\ncontext for reasoning. We introduce the Search-Is-Not-Retrieve (SINR)\nframework, a dual-layer architecture that distinguishes between fine-grained\nsearch representations and coarse-grained retrieval contexts. SINR enhances the\ncomposability, scalability, and context fidelity of retrieval systems by\ndirectly connecting small, semantically accurate search chunks to larger,\ncontextually complete retrieve chunks, all without incurring extra processing\ncosts. This design changes retrieval from a passive step to an active one,\nmaking the system architecture more like how people process information. We\ndiscuss the SINR framework's conceptual foundation, formal structure,\nimplementation issues, and qualitative outcomes. This provides a practical\nfoundation for the next generation of AI systems that use retrieval.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a SINR \u7684\u53cc\u5c42\u67b6\u6784\uff0c\u7528\u4e8e\u533a\u5206\u7ec6\u7c92\u5ea6\u641c\u7d22\u548c\u7c97\u7c92\u5ea6\u68c0\u7d22\u3002", "motivation": "\u5f53\u524d AI \u6d41\u7a0b\u4e2d\u7684\u68c0\u7d22\u7cfb\u7edf\u6df7\u6dc6\u4e86\u67e5\u627e\u76f8\u5173\u4fe1\u606f\u548c\u63d0\u4f9b\u8db3\u591f\u63a8\u7406\u4e0a\u4e0b\u6587\u8fd9\u4e24\u4e2a\u72ec\u7acb\u7684\u8fc7\u7a0b\u3002", "method": "SINR \u6846\u67b6\u901a\u8fc7\u5c06\u5c0f\u7684\u3001\u8bed\u4e49\u4e0a\u7cbe\u786e\u7684\u641c\u7d22\u5757\u76f4\u63a5\u8fde\u63a5\u5230\u66f4\u5927\u7684\u3001\u4e0a\u4e0b\u6587\u5b8c\u6574\u7684\u68c0\u7d22\u5757\u6765\u589e\u5f3a\u68c0\u7d22\u7cfb\u7edf\u7684\u53ef\u7ec4\u5408\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u4e0a\u4e0b\u6587\u4fdd\u771f\u5ea6\uff0c\u4e14\u4e0d\u4ea7\u751f\u989d\u5916\u7684\u5904\u7406\u6210\u672c\u3002", "result": "\u5c06\u68c0\u7d22\u4ece\u88ab\u52a8\u6b65\u9aa4\u53d8\u4e3a\u4e3b\u52a8\u6b65\u9aa4\uff0c\u4f7f\u7cfb\u7edf\u67b6\u6784\u66f4\u50cf\u4eba\u4eec\u5904\u7406\u4fe1\u606f\u7684\u65b9\u5f0f\u3002", "conclusion": "\u4e3a\u4e0b\u4e00\u4ee3\u4f7f\u7528\u68c0\u7d22\u7684 AI \u7cfb\u7edf\u5960\u5b9a\u4e86\u5b9e\u8df5\u57fa\u7840\u3002"}}
{"id": "2511.05000", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05000", "abs": "https://arxiv.org/abs/2511.05000", "authors": ["Hyunkyu Kim", "Yeeun Yoo", "Youngjun Kwak"], "title": "Query Generation Pipeline with Enhanced Answerability Assessment for Financial Information Retrieval", "comment": "Accepted(Oral) by ICAIF 2025. Hyunkyu Kim and Yeeun Yoo contributed\n  equally to this work", "summary": "As financial applications of large language models (LLMs) gain attention,\naccurate Information Retrieval (IR) remains crucial for reliable AI services.\nHowever, existing benchmarks fail to capture the complex and domain-specific\ninformation needs of real-world banking scenarios. Building domain-specific IR\nbenchmarks is costly and constrained by legal restrictions on using real\ncustomer data. To address these challenges, we propose a systematic methodology\nfor constructing domain-specific IR benchmarks through LLM-based query\ngeneration. As a concrete implementation of this methodology, our pipeline\ncombines single and multi-document query generation with an enhanced and\nreasoning-augmented answerability assessment method, achieving stronger\nalignment with human judgments than prior approaches. Using this methodology,\nwe construct KoBankIR, comprising 815 queries derived from 204 official banking\ndocuments. Our experiments show that existing retrieval models struggle with\nthe complex multi-document queries in KoBankIR, demonstrating the value of our\nsystematic approach for domain-specific benchmark construction and underscoring\nthe need for improved retrieval techniques in financial domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528LLM\u751f\u6210\u7279\u5b9a\u9886\u57dfIR\u57fa\u51c6\u7684\u7cfb\u7edf\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86KoBankIR\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u65e0\u6cd5\u6355\u6349\u5230\u771f\u5b9e\u94f6\u884c\u573a\u666f\u4e2d\u590d\u6742\u548c\u7279\u5b9a\u9886\u57df\u7684\u4fe1\u606f\u9700\u6c42\u3002\u6784\u5efa\u7279\u5b9a\u9886\u57df\u7684IR\u57fa\u51c6\u6210\u672c\u9ad8\u6602\uff0c\u5e76\u53d7\u5230\u4f7f\u7528\u771f\u5b9e\u5ba2\u6237\u6570\u636e\u7684\u6cd5\u5f8b\u9650\u5236\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u5355\u6587\u6863\u548c\u591a\u6587\u6863\u67e5\u8be2\u751f\u6210\uff0c\u4ee5\u53ca\u589e\u5f3a\u548c\u63a8\u7406\u589e\u5f3a\u7684\u53ef\u56de\u7b54\u6027\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b815\u4e2a\u67e5\u8be2\u7684KoBankIR\uff0c\u5b9e\u9a8c\u8868\u660e\u73b0\u6709\u7684\u68c0\u7d22\u6a21\u578b\u96be\u4ee5\u5904\u7406KoBankIR\u4e2d\u590d\u6742\u7684\u591a\u6587\u6863\u67e5\u8be2\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u65b9\u6cd5\u5bf9\u4e8e\u7279\u5b9a\u9886\u57df\u57fa\u51c6\u6784\u5efa\u5177\u6709\u4ef7\u503c\uff0c\u5e76\u5f3a\u8c03\u4e86\u91d1\u878d\u9886\u57df\u6539\u8fdb\u68c0\u7d22\u6280\u672f\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2511.05079", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.05079", "abs": "https://arxiv.org/abs/2511.05079", "authors": ["Grigory Kovalev", "Natalia Loukachevitch", "Mikhail Tikhomirov", "Olga Babina", "Pavel Mamaev"], "title": "Wikipedia-based Datasets in Russian Information Retrieval Benchmark RusBEIR", "comment": null, "summary": "In this paper, we present a novel series of Russian information retrieval\ndatasets constructed from the \"Did you know...\" section of Russian Wikipedia.\nOur datasets support a range of retrieval tasks, including fact-checking,\nretrieval-augmented generation, and full-document retrieval, by leveraging\ninteresting facts and their referenced Wikipedia articles annotated at the\nsentence level with graded relevance. We describe the methodology for dataset\ncreation that enables the expansion of existing Russian Information Retrieval\n(IR) resources. Through extensive experiments, we extend the RusBEIR research\nby comparing lexical retrieval models, such as BM25, with state-of-the-art\nneural architectures fine-tuned for Russian, as well as multilingual models.\nResults of our experiments show that lexical methods tend to outperform neural\nmodels on full-document retrieval, while neural approaches better capture\nlexical semantics in shorter texts, such as in fact-checking or fine-grained\nretrieval. Using our newly created datasets, we also analyze the impact of\ndocument length on retrieval performance and demonstrate that combining\nretrieval with neural reranking consistently improves results. Our contribution\nexpands the resources available for Russian information retrieval research and\nhighlights the importance of accurate evaluation of retrieval models to achieve\noptimal performance. All datasets are publicly available at HuggingFace. To\nfacilitate reproducibility and future research, we also release the full\nimplementation on GitHub.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u7cfb\u5217\u65b0\u7684\u4fc4\u8bed\u4fe1\u606f\u68c0\u7d22\u6570\u636e\u96c6\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u6784\u5efa\u4e8e\u4fc4\u8bed\u7ef4\u57fa\u767e\u79d1\u7684\u201c\u4f60\u77e5\u9053\u5417\u2026\u2026\u201d\u90e8\u5206\u3002", "motivation": "\u4e3a\u4e86\u652f\u6301\u5404\u79cd\u68c0\u7d22\u4efb\u52a1\uff0c\u5305\u62ec\u4e8b\u5b9e\u6838\u67e5\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u5168\u6587\u68c0\u7d22\uff0c\u901a\u8fc7\u5229\u7528\u6709\u8da3\u7684\u4e8b\u4ef6\u548c\u5b83\u4eec\u5f15\u7528\u7684\u7ef4\u57fa\u767e\u79d1\u6587\u7ae0\uff0c\u8fd9\u4e9b\u6587\u7ae0\u5728\u53e5\u5b50\u7ea7\u522b\u4e0a\u7528\u5206\u7ea7\u76f8\u5173\u6027\u8fdb\u884c\u6ce8\u91ca\u3002", "method": "\u672c\u6587\u63cf\u8ff0\u4e86\u6570\u636e\u96c6\u521b\u5efa\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6bd4\u8f83\u8bcd\u6c47\u68c0\u7d22\u6a21\u578b\uff08\u5982BM25\uff09\u4e0e\u9488\u5bf9\u4fc4\u8bed\u8fdb\u884c\u5fae\u8c03\u7684\u6700\u5148\u8fdb\u7684\u795e\u7ecf\u67b6\u6784\u4ee5\u53ca\u591a\u8bed\u8a00\u6a21\u578b\uff0c\u6269\u5c55\u4e86RusBEIR\u7814\u7a76\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8bcd\u6c47\u65b9\u6cd5\u5728\u5168\u6587\u68c0\u7d22\u65b9\u9762\u5f80\u5f80\u4f18\u4e8e\u795e\u7ecf\u6a21\u578b\uff0c\u800c\u795e\u7ecf\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u6355\u6349\u8f83\u77ed\u6587\u672c\u4e2d\u7684\u8bcd\u6c47\u8bed\u4e49\uff0c\u4f8b\u5982\u5728\u4e8b\u5b9e\u6838\u67e5\u6216\u7ec6\u7c92\u5ea6\u68c0\u7d22\u4e2d\u3002\u5206\u6790\u4e86\u6587\u6863\u957f\u5ea6\u5bf9\u68c0\u7d22\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u8bc1\u660e\u5c06\u68c0\u7d22\u4e0e\u795e\u7ecf\u91cd\u65b0\u6392\u5e8f\u76f8\u7ed3\u5408\u53ef\u4ee5\u6301\u7eed\u63d0\u9ad8\u7ed3\u679c\u3002", "conclusion": "\u672c\u6587\u7684\u8d21\u732e\u6269\u5c55\u4e86\u53ef\u7528\u4e8e\u4fc4\u8bed\u4fe1\u606f\u68c0\u7d22\u7814\u7a76\u7684\u8d44\u6e90\uff0c\u5e76\u5f3a\u8c03\u4e86\u51c6\u786e\u8bc4\u4f30\u68c0\u7d22\u6a21\u578b\u4ee5\u5b9e\u73b0\u6700\u4f73\u6027\u80fd\u7684\u91cd\u8981\u6027\u3002\u6240\u6709\u6570\u636e\u96c6\u5747\u53ef\u5728HuggingFace\u4e0a\u516c\u5f00\u83b7\u5f97\u3002\u4e3a\u4e86\u65b9\u4fbf\u91cd\u73b0\u548c\u672a\u6765\u7684\u7814\u7a76\uff0c\u672c\u6587\u8fd8\u5728GitHub\u4e0a\u53d1\u5e03\u4e86\u5b8c\u6574\u7684\u5b9e\u73b0\u3002"}}
{"id": "2511.05082", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.05082", "abs": "https://arxiv.org/abs/2511.05082", "authors": ["Yiming Xie", "Hua Dai", "Mingfeng Jiang", "Pengyue Li", "zhengkai Zhang", "Bohan Li"], "title": "An Efficient Proximity Graph-based Approach to Table Union Search", "comment": null, "summary": "Neural embedding models are extensively employed in the table union search\nproblem, which aims to find semantically compatible tables that can be merged\nwith a given query table. In particular, multi-vector models, which represent a\ntable as a vector set (typically one vector per column), have been demonstrated\nto achieve superior retrieval quality by capturing fine-grained semantic\nalignments. However, this problem faces more severe efficiency challenges than\nthe single-vector problem due to the inherent dependency on bipartite graph\nmaximum matching to compute unionability scores. Therefore, this paper proposes\nan efficient Proximity Graph-based Table Union Search (PGTUS) approach. PGTUS\nemploys a multi-stage pipeline that combines a novel refinement strategy, a\nfiltering strategy based on many-to-one bipartite matching. Besides, we propose\nan enhanced pruning strategy to prune the candidate set, which further improve\nthe search efficiency. Extensive experiments on six benchmark datasets\ndemonstrate that our approach achieves 3.6-6.0X speedup over existing\napproaches while maintaining comparable recall rates.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPGTUS\u7684\u9ad8\u6548\u8868\u683c\u8054\u5408\u641c\u7d22\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u65b0\u7684\u4f18\u5316\u7b56\u7565\u548c\u57fa\u4e8e\u591a\u5bf9\u4e00\u4e8c\u5206\u5339\u914d\u7684\u8fc7\u6ee4\u7b56\u7565\uff0c\u4ee5\u53ca\u589e\u5f3a\u7684\u526a\u679d\u7b56\u7565\u6765\u63d0\u9ad8\u641c\u7d22\u6548\u7387\u3002", "motivation": "\u5728\u8868\u683c\u8054\u5408\u641c\u7d22\u95ee\u9898\u4e2d\uff0c\u591a\u5411\u91cf\u6a21\u578b\u901a\u8fc7\u6355\u83b7\u7ec6\u7c92\u5ea6\u7684\u8bed\u4e49\u5bf9\u9f50\u6765\u5b9e\u73b0\u5353\u8d8a\u7684\u68c0\u7d22\u8d28\u91cf\u3002\u7136\u800c\uff0c\u7531\u4e8e\u4e8c\u5206\u56fe\u6700\u5927\u5339\u914d\u7684\u4f9d\u8d56\u6027\uff0c\u591a\u5411\u91cf\u6a21\u578b\u9762\u4e34\u7740\u6bd4\u5355\u5411\u91cf\u6a21\u578b\u66f4\u4e25\u5cfb\u7684\u6548\u7387\u6311\u6218\u3002", "method": "PGTUS\u91c7\u7528\u591a\u9636\u6bb5\u6d41\u6c34\u7ebf\uff0c\u7ed3\u5408\u4e86\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u7b56\u7565\uff0c\u4e00\u79cd\u57fa\u4e8e\u591a\u5bf9\u4e00\u4e8c\u5206\u5339\u914d\u7684\u8fc7\u6ee4\u7b56\u7565\uff0c\u4ee5\u53ca\u4e00\u79cd\u589e\u5f3a\u7684\u526a\u679d\u7b56\u7565\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u76f8\u5f53\u7684\u53ec\u56de\u7387\u7684\u540c\u65f6\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u5b9e\u73b0\u4e863.6-6.0\u500d\u7684\u52a0\u901f\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u8868\u683c\u8054\u5408\u641c\u7d22\u65b9\u6cd5PGTUS\uff0c\u901a\u8fc7\u4f18\u5316\u548c\u526a\u679d\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u641c\u7d22\u6548\u7387\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2511.04727", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04727", "abs": "https://arxiv.org/abs/2511.04727", "authors": ["Ali Faraz", "Akash", "Shaharukh Khan", "Raja Kolla", "Akshat Patidar", "Suranjan Goswami", "Abhinav Ravi", "Chandra Khatri", "Shubham Agarwal"], "title": "IndicVisionBench: Benchmarking Cultural and Multilingual Understanding in VLMs", "comment": null, "summary": "Vision-language models (VLMs) have demonstrated impressive generalization\nacross multimodal tasks, yet most evaluation benchmarks remain Western-centric,\nleaving open questions about their performance in culturally diverse and\nmultilingual settings. To address this gap, we introduce IndicVisionBench, the\nfirst large-scale benchmark centered on the Indian subcontinent. Covering\nEnglish and 10 Indian languages, our benchmark spans 3 multimodal tasks,\nincluding Optical Character Recognition (OCR), Multimodal Machine Translation\n(MMT), and Visual Question Answering (VQA), covering 6 kinds of question types.\nOur final benchmark consists of a total of ~5K images and 37K+ QA pairs across\n13 culturally grounded topics. In addition, we release a paired parallel corpus\nof annotations across 10 Indic languages, creating a unique resource for\nanalyzing cultural and linguistic biases in VLMs. We evaluate a broad spectrum\nof 8 models, from proprietary closed-source systems to open-weights medium and\nlarge-scale models. Our experiments reveal substantial performance gaps,\nunderscoring the limitations of current VLMs in culturally diverse contexts. By\ncentering cultural diversity and multilinguality, IndicVisionBench establishes\na reproducible evaluation framework that paves the way for more inclusive\nmultimodal research.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u5370\u5ea6\u6b21\u5927\u9646\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5 IndicVisionBench\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6587\u5316\u591a\u6837\u6027\u548c\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u57fa\u51c6\u4e3b\u8981\u4ee5\u897f\u65b9\u4e3a\u4e2d\u5fc3\uff0c\u7f3a\u4e4f\u5bf9\u6587\u5316\u591a\u6837\u6027\u548c\u591a\u8bed\u8a00\u73af\u5883\u7684\u8003\u91cf\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b\u82f1\u8bed\u548c 10 \u79cd\u5370\u5ea6\u8bed\u8a00\uff0c\u6db5\u76d6 OCR\u3001MMT \u548c VQA \u4e09\u79cd\u591a\u6a21\u6001\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u7ea6 5K \u56fe\u50cf\u548c 37K+ QA \u5bf9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6587\u5316\u591a\u6837\u6027\u73af\u5883\u4e2d\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "IndicVisionBench \u5efa\u7acb\u4e86\u4e00\u4e2a\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4e3a\u66f4\u5177\u5305\u5bb9\u6027\u7684\u591a\u6a21\u6001\u7814\u7a76\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2511.04686", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04686", "abs": "https://arxiv.org/abs/2511.04686", "authors": ["Pratik Poudel"], "title": "Stateful KV Cache Management for LLMs: Balancing Space, Time, Accuracy, and Positional Fidelity", "comment": "14 pages, 2 figures", "summary": "The Key-Value (KV) cache is integral to efficient autoregressive inference in\nlarge language models (LLMs), yet its unbounded growth in stateful multi-turn\nscenarios presents major challenges. This paper examines the interplay between\nKV cache management strategies, the architectural context limits of models like\nmeta-llama/Meta-Llama-3-8b-instruct, and the often-overlooked integrity of\npositional encodings. Through empirical analysis using a stateful benchmarking\nframework, we show that LLM generation quality degrades sharply when the\naccumulated KV cache approaches or exceeds the model's trained context window\n(e.g., 8192 tokens for Llama 3), a failure mode distinct from GPU memory\nexhaustion. Common eviction strategies, even high-retention ones (e.g., 99% via\nAttentionTop), can worsen performance if they disrupt positional coherence.\nBecause LLMs rely on consistent positional signals (e.g., RoPE), compacting a\ncache by removing non-contiguous tokens can scramble these signals and lead to\ndegenerative outputs. We further show that simple strategies preserving\ncontiguous context blocks (e.g., keeping an initial \"gist\") can yield more\ncoherent generations than complex or positionally disruptive ones. We advocate\nfor eviction techniques that respect architectural limits, preserve positional\nstructure, and view \"cache health\" holistically beyond mere size.", "AI": {"tldr": "KV\u7f13\u5b58\u5bf9\u4e8eLLM\u4e2d\u7684\u6709\u6548\u81ea\u56de\u5f52\u63a8\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u5728\u6709\u72b6\u6001\u591a\u8f6e\u573a\u666f\u4e2d\u7684\u65e0\u9650\u5236\u589e\u957f\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002\u672c\u6587\u63a2\u8ba8\u4e86KV\u7f13\u5b58\u7ba1\u7406\u7b56\u7565\u3001\u6a21\u578b\u67b6\u6784\u4e0a\u4e0b\u6587\u9650\u5236\u4ee5\u53ca\u4f4d\u7f6e\u7f16\u7801\u5b8c\u6574\u6027\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684Key-Value (KV)\u7f13\u5b58\u5bf9\u4e8e\u6709\u6548\u7684\u81ea\u56de\u5f52\u63a8\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u6709\u72b6\u6001\u591a\u8f6e\u573a\u666f\u4e2d\uff0c\u5176\u65e0\u9650\u5236\u7684\u589e\u957f\u5e26\u6765\u4e86\u4e3b\u8981\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u4f7f\u7528\u6709\u72b6\u6001\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\u7684\u7ecf\u9a8c\u5206\u6790", "result": "LLM\u751f\u6210\u8d28\u91cf\u5728\u7d2f\u79ef\u7684KV\u7f13\u5b58\u63a5\u8fd1\u6216\u8d85\u8fc7\u6a21\u578b\u7684\u8bad\u7ec3\u4e0a\u4e0b\u6587\u7a97\u53e3\u65f6\u4f1a\u6025\u5267\u4e0b\u964d\uff0c\u5373\u4f7f\u662f\u9ad8\u4fdd\u7559\u7387\u7684\u5e38\u89c1\u9a71\u9010\u7b56\u7565\uff0c\u5982\u679c\u5b83\u4eec\u7834\u574f\u4e86\u4f4d\u7f6e\u8fde\u8d2f\u6027\uff0c\u4e5f\u4f1a\u964d\u4f4e\u6027\u80fd\u3002\u7b80\u5355\u7684\u7b56\u7565\uff0c\u6bd4\u5982\u4fdd\u6301\u521d\u59cb\u7684\u201c\u8981\u70b9\u201d\uff0c\u53ef\u4ee5\u4ea7\u751f\u6bd4\u590d\u6742\u7684\u6216\u4f4d\u7f6e\u7834\u574f\u6027\u7684\u7b56\u7565\u66f4\u8fde\u8d2f\u7684\u751f\u6210\u3002", "conclusion": "\u6211\u4eec\u63d0\u5021\u5c0a\u91cd\u67b6\u6784\u9650\u5236\u3001\u4fdd\u6301\u4f4d\u7f6e\u7ed3\u6784\u5e76\u5168\u9762\u770b\u5f85\u201c\u7f13\u5b58\u5065\u5eb7\u201d\u7684\u9a71\u9010\u6280\u672f\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5927\u5c0f\u3002"}}
{"id": "2511.04688", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04688", "abs": "https://arxiv.org/abs/2511.04688", "authors": ["Adrita Anika", "Md Messal Monem Miah"], "title": "Evaluating LLMs' Reasoning Over Ordered Procedural Steps", "comment": "Accepted to IJCNLP-AACL 2025 Findings", "summary": "Reasoning over procedural sequences, where the order of steps directly\nimpacts outcomes, is a critical capability for large language models (LLMs). In\nthis work, we study the task of reconstructing globally ordered sequences from\nshuffled procedural steps, using a curated dataset of food recipes, a domain\nwhere correct sequencing is essential for task success. We evaluate several\nLLMs under zero-shot and few-shot settings and present a comprehensive\nevaluation framework that adapts established metrics from ranking and sequence\nalignment. These include Kendall's Tau, Normalized Longest Common Subsequence\n(NLCS), and Normalized Edit Distance (NED), which capture complementary aspects\nof ordering quality. Our analysis shows that model performance declines with\nincreasing sequence length, reflecting the added complexity of longer\nprocedures. We also find that greater step displacement in the input,\ncorresponding to more severe shuffling, leads to further degradation. These\nfindings highlight the limitations of current LLMs in procedural reasoning,\nespecially with longer and more disordered inputs.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7a0b\u5e8f\u5e8f\u5217\u63a8\u7406\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u6b65\u9aa4\u987a\u5e8f\u5bf9\u7ed3\u679c\u6709\u76f4\u63a5\u5f71\u54cd\u65f6\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u91cd\u5efa\u88ab\u6253\u4e71\u7684\u7a0b\u5e8f\u6b65\u9aa4\uff08\u5982\u98df\u8c31\uff09\u7684\u80fd\u529b\uff0c\u56e0\u4e3a\u6b63\u786e\u7684\u6b65\u9aa4\u6392\u5e8f\u5bf9\u4e8e\u4efb\u52a1\u6210\u529f\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u98df\u7269\u98df\u8c31\u6570\u636e\u96c6\uff0c\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u8bc4\u4f30\u591a\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u91c7\u7528Kendall's Tau\u3001\u5f52\u4e00\u5316\u6700\u957f\u516c\u5171\u5b50\u5e8f\u5217\uff08NLCS\uff09\u548c\u5f52\u4e00\u5316\u7f16\u8f91\u8ddd\u79bb\uff08NED\uff09\u7b49\u6307\u6807\u6765\u8bc4\u4f30\u6392\u5e8f\u8d28\u91cf\u3002", "result": "\u6a21\u578b\u6027\u80fd\u968f\u7740\u5e8f\u5217\u957f\u5ea6\u7684\u589e\u52a0\u800c\u4e0b\u964d\uff0c\u8f93\u5165\u4e2d\u66f4\u5927\u7684\u6b65\u9aa4\u4f4d\u79fb\uff08\u66f4\u4e25\u91cd\u7684\u6253\u4e71\uff09\u5bfc\u81f4\u6027\u80fd\u8fdb\u4e00\u6b65\u4e0b\u964d\u3002", "conclusion": "\u76ee\u524d\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7a0b\u5e8f\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u8f83\u957f\u548c\u66f4\u6df7\u4e71\u7684\u8f93\u5165\u65f6\u3002"}}
{"id": "2511.05301", "categories": ["cs.IR", "cs.CL", "cs.LG", "68P20, 68T50", "H.3"], "pdf": "https://arxiv.org/pdf/2511.05301", "abs": "https://arxiv.org/abs/2511.05301", "authors": ["Arthur Satouf", "Yuxuan Zong", "Habiboulaye Amadou-Boubacar", "Pablo Piantanida", "Benjamin Piwowarski"], "title": "QUESTER: Query Specification for Generative Retrieval", "comment": null, "summary": "Generative Retrieval (GR) differs from the traditional index-then-retrieve\npipeline by storing relevance in model parameters and directly generating\ndocument identifiers. However, GR often struggles to generalize and is costly\nto scale. We introduce QUESTER (QUEry SpecificaTion gEnerative Retrieval),\nwhich reframes GR as query specification generation - in this work, a simple\nkeyword query handled by BM25 - using a (small) LLM. The policy is trained\nusing reinforcement learning techniques (GRPO). Across in- and out-of-domain\nevaluations, we show that our model is more effective than BM25, and\ncompetitive with neural IR models, while maintaining a good efficiency", "AI": {"tldr": "QUExy Specification gEnerative Retrieval (QUESTER) reframes Generative Retrieval (GR) as query specification generation using a small LLM, trained with reinforcement learning, to generate keyword queries for BM25.", "motivation": "Generative Retrieval (GR) struggles to generalize and is costly to scale.", "method": "Reframes GR as query specification generation, using a small LLM to generate keyword queries for BM25. The policy is trained using reinforcement learning techniques (GRPO).", "result": "More effective than BM25 and competitive with neural IR models, while maintaining good efficiency.", "conclusion": "QUESTER offers an effective and efficient alternative to traditional and generative retrieval methods."}}
{"id": "2511.04729", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04729", "abs": "https://arxiv.org/abs/2511.04729", "authors": ["Rucha Deshpande", "Tahsin Rahman", "Miguel Lago", "Adarsh Subbaswamy", "Jana G. Delfino", "Ghada Zamzmi", "Elim Thompson", "Aldo Badano", "Seyed Kahaki"], "title": "Knowledge-based anomaly detection for identifying network-induced shape artifacts", "comment": "15 pages, 11 figures", "summary": "Synthetic data provides a promising approach to address data scarcity for\ntraining machine learning models; however, adoption without proper quality\nassessments may introduce artifacts, distortions, and unrealistic features that\ncompromise model performance and clinical utility. This work introduces a novel\nknowledge-based anomaly detection method for detecting network-induced shape\nartifacts in synthetic images. The introduced method utilizes a two-stage\nframework comprising (i) a novel feature extractor that constructs a\nspecialized feature space by analyzing the per-image distribution of angle\ngradients along anatomical boundaries, and (ii) an isolation forest-based\nanomaly detector. We demonstrate the effectiveness of the method for\nidentifying network-induced shape artifacts in two synthetic mammography\ndatasets from models trained on CSAW-M and VinDr-Mammo patient datasets\nrespectively. Quantitative evaluation shows that the method successfully\nconcentrates artifacts in the most anomalous partition (1st percentile), with\nAUC values of 0.97 (CSAW-syn) and 0.91 (VMLO-syn). In addition, a reader study\ninvolving three imaging scientists confirmed that images identified by the\nmethod as containing network-induced shape artifacts were also flagged by human\nreaders with mean agreement rates of 66% (CSAW-syn) and 68% (VMLO-syn) for the\nmost anomalous partition, approximately 1.5-2 times higher than the least\nanomalous partition. Kendall-Tau correlations between algorithmic and human\nrankings were 0.45 and 0.43 for the two datasets, indicating reasonable\nagreement despite the challenging nature of subtle artifact detection. This\nmethod is a step forward in the responsible use of synthetic data, as it allows\ndevelopers to evaluate synthetic images for known anatomic constraints and\npinpoint and address specific issues to improve the overall quality of a\nsynthetic dataset.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u77e5\u8bc6\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u5408\u6210\u56fe\u50cf\u4e2d\u7f51\u7edc\u8bf1\u5bfc\u7684\u5f62\u72b6\u4f2a\u5f71\u3002", "motivation": "\u5408\u6210\u6570\u636e\u4e3a\u89e3\u51b3\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u4f46\u5982\u679c\u6ca1\u6709\u9002\u5f53\u7684\u8d28\u91cf\u8bc4\u4f30\u5c31\u91c7\u7528\uff0c\u53ef\u80fd\u4f1a\u5f15\u5165\u4f2a\u5f71\u3001\u5931\u771f\u548c\u4e0d\u771f\u5b9e\u7684\u7279\u5f81\uff0c\u4ece\u800c\u635f\u5bb3\u6a21\u578b\u6027\u80fd\u548c\u4e34\u5e8a\u6548\u7528\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u5305\u62ec\uff1a(i) \u4e00\u4e2a\u65b0\u7684\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u901a\u8fc7\u5206\u6790\u6cbf\u89e3\u5256\u8fb9\u754c\u7684\u89d2\u5ea6\u68af\u5ea6\u7684\u6bcf\u56fe\u50cf\u5206\u5e03\u6765\u6784\u5efa\u4e00\u4e2a\u4e13\u95e8\u7684\u7279\u5f81\u7a7a\u95f4\uff1b(ii) \u4e00\u4e2a\u57fa\u4e8e\u9694\u79bb\u68ee\u6797\u7684\u5f02\u5e38\u68c0\u6d4b\u5668\u3002", "result": "\u5b9a\u91cf\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6210\u529f\u5730\u5c06\u4f2a\u5f71\u96c6\u4e2d\u5728\u6700\u5f02\u5e38\u7684\u5206\u533a\uff08\u7b2c 1 \u4e2a\u767e\u5206\u4f4d\uff09\u4e2d\uff0cAUC \u503c\u4e3a 0.97 (CSAW-syn) \u548c 0.91 (VMLO-syn)\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u662f\u8d1f\u8d23\u4efb\u5730\u4f7f\u7528\u5408\u6210\u6570\u636e\u7684\u4e00\u4e2a\u8fdb\u6b65\uff0c\u56e0\u4e3a\u5b83\u5141\u8bb8\u5f00\u53d1\u4eba\u5458\u8bc4\u4f30\u5408\u6210\u56fe\u50cf\u7684\u5df2\u77e5\u89e3\u5256\u7ea6\u675f\uff0c\u5e76\u67e5\u660e\u548c\u89e3\u51b3\u5177\u4f53\u95ee\u9898\uff0c\u4ee5\u63d0\u9ad8\u5408\u6210\u6570\u636e\u96c6\u7684\u6574\u4f53\u8d28\u91cf\u3002"}}
{"id": "2511.04718", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04718", "abs": "https://arxiv.org/abs/2511.04718", "authors": ["Yue Xun", "Jiaxing Xu", "Wenbo Gao", "Chen Yang", "Shujun Wang"], "title": "Ada-FCN: Adaptive Frequency-Coupled Network for fMRI-Based Brain Disorder Classification", "comment": "11 pages, 2 figures, conference", "summary": "Resting-state fMRI has become a valuable tool for classifying brain disorders\nand constructing brain functional connectivity networks\n  by tracking BOLD signals across brain regions. However, existing mod els\nlargely neglect the multi-frequency nature of neuronal oscillations,\n  treating BOLD signals as monolithic time series. This overlooks the cru cial\nfact that neurological disorders often manifest as disruptions within\n  specific frequency bands, limiting diagnostic sensitivity and specificity.\n  While some methods have attempted to incorporate frequency informa tion, they\noften rely on predefined frequency bands, which may not be\n  optimal for capturing individual variability or disease-specific alterations.\n  To address this, we propose a novel framework featuring Adaptive Cas cade\nDecomposition to learn task-relevant frequency sub-bands for each\n  brain region and Frequency-Coupled Connectivity Learning to capture\n  both intra- and nuanced cross-band interactions in a unified functional\n  network. This unified network informs a novel message-passing mecha nism\nwithin our Unified-GCN, generating refined node representations\n  for diagnostic prediction. Experimental results on the ADNI and ABIDE\n  datasets demonstrate superior performance over existing methods. The\n  code is available at https://github.com/XXYY20221234/Ada-FCN.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9759\u606f\u6001fMRI\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5206\u89e3\u5b66\u4e60\u4efb\u52a1\u76f8\u5173\u7684\u9891\u7387\u5b50\u5e26\uff0c\u5e76\u5b66\u4e60\u9891\u7387\u8026\u5408\u8fde\u63a5\uff0c\u4ee5\u63d0\u9ad8\u8111\u90e8\u75be\u75c5\u5206\u7c7b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5ffd\u7565\u4e86\u795e\u7ecf\u5143\u632f\u8361\u7684\u591a\u9891\u7387\u7279\u6027\uff0c\u5e76\u4e14\u4f9d\u8d56\u4e8e\u9884\u5b9a\u4e49\u7684\u9891\u7387\u5e26\uff0c\u65e0\u6cd5\u6355\u6349\u4e2a\u4f53\u5dee\u5f02\u6216\u75be\u75c5\u7279\u5f02\u6027\u6539\u53d8\uff0c\u9650\u5236\u4e86\u8bca\u65ad\u7684\u7075\u654f\u6027\u548c\u7279\u5f02\u6027\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u7ea7\u8054\u5206\u89e3\u6765\u5b66\u4e60\u4efb\u52a1\u76f8\u5173\u7684\u9891\u7387\u5b50\u5e26\uff0c\u5e76\u63d0\u51fa\u4e86\u9891\u7387\u8026\u5408\u8fde\u63a5\u5b66\u4e60\u6765\u6355\u6349\u5e26\u5185\u548c\u8de8\u5e26\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u6700\u7ec8\u901a\u8fc7\u4e00\u4e2a\u7edf\u4e00\u7684GCN\u6765\u8fdb\u884c\u8bca\u65ad\u9884\u6d4b\u3002", "result": "\u5728ADNI\u548cABIDE\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u65b0\u6846\u67b6\u80fd\u591f\u66f4\u6709\u6548\u5730\u5229\u7528\u9759\u606f\u6001fMRI\u6570\u636e\u8fdb\u884c\u8111\u90e8\u75be\u75c5\u7684\u8bca\u65ad\u548c\u5206\u7c7b\u3002"}}
{"id": "2511.04689", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04689", "abs": "https://arxiv.org/abs/2511.04689", "authors": ["Peiyu Li", "Xiuxiu Tang", "Si Chen", "Ying Cheng", "Ronald Metoyer", "Ting Hua", "Nitesh V. Chawla"], "title": "Adaptive Testing for LLM Evaluation: A Psychometric Alternative to Static Benchmarks", "comment": "Code and calibrated item banks are available at\n  https://github.com/Peiyu-Georgia-Li/ATLAS.git", "summary": "Large language model evaluation requires thousands of benchmark items, making\nevaluations expensive and slow. Existing methods compute average accuracy\nacross fixed item sets, treating all items equally despite varying quality and\ninformativeness. We present ATLAS an adaptive testing framework using Item\nResponse Theory (IRT) to estimate model ability through Fisher\ninformation-guided item selection. Our analysis of five major benchmarks\nreveals that 3-6% of items exhibit negative discrimination, indicating\nannotation errors that corrupt static evaluation. ATLAS achieves 90% item\nreduction while maintaining measurement precision: on HellaSwag (5,608 items),\nwe match full-benchmark estimates using only 42 items with 0.154 MAE. Our\nframework maintains item exposure rates below 10% and test overlap at 16-27%,\ncompared to static benchmarks where every model sees all items (100% exposure).\nAmong 4,000+ tested models, IRT ranks differ from accuracy ranks: models with\nthe same accuracy get different IRT scores, and 23-31% of all models shift by\nmore than 10 rank positions. Code and calibrated item banks are available at\nhttps://github.com/Peiyu-Georgia-Li/ATLAS.git.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u9002\u5e94\u6d4b\u8bd5\u6846\u67b6ATLAS\uff0c\u901a\u8fc7\u9879\u76ee\u53cd\u5e94\u7406\u8bba\uff08IRT\uff09\u548cFisher\u4fe1\u606f\u5f15\u5bfc\u7684\u9879\u76ee\u9009\u62e9\u6765\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\uff0c\u51cf\u5c11\u4e86\u8bc4\u4f30\u6240\u9700\u7684\u9879\u76ee\u6570\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u7684\u57fa\u51c6\u9879\u76ee\uff0c\u6210\u672c\u9ad8\u4e14\u901f\u5ea6\u6162\uff0c\u5e76\u4e14\u5bf9\u6240\u6709\u9879\u76ee\u7684\u8d28\u91cf\u548c\u4fe1\u606f\u91cf\u4e00\u89c6\u540c\u4ec1\u3002", "method": "\u4f7f\u7528\u9879\u76ee\u53cd\u5e94\u7406\u8bba\uff08IRT\uff09\u548cFisher\u4fe1\u606f\u5f15\u5bfc\u7684\u9879\u76ee\u9009\u62e9\uff0c\u6784\u5efa\u81ea\u9002\u5e94\u6d4b\u8bd5\u6846\u67b6ATLAS\u3002", "result": "ATLAS\u80fd\u591f\u5728\u4fdd\u6301\u6d4b\u91cf\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u51cf\u5c1190%\u7684\u9879\u76ee\u6570\u91cf\u3002\u5728HellaSwag\u6570\u636e\u96c6\u4e0a\uff0c\u4ec5\u4f7f\u752842\u4e2a\u9879\u76ee\u5c31\u80fd\u8fbe\u5230\u5b8c\u6574\u57fa\u51c6\u7684\u4f30\u8ba1\u6548\u679c\uff0cMAE\u4e3a0.154\u3002IRT\u6392\u540d\u4e0e\u51c6\u786e\u7387\u6392\u540d\u5b58\u5728\u5dee\u5f02\uff0c\u76f8\u540c\u51c6\u786e\u7387\u7684\u6a21\u578b\u83b7\u5f97\u4e0d\u540c\u7684IRT\u5206\u6570\uff0c23-31%\u7684\u6a21\u578b\u6392\u540d\u53d8\u5316\u8d85\u8fc710\u4f4d\u3002", "conclusion": "ATLAS\u6846\u67b6\u80fd\u591f\u6709\u6548\u51cf\u5c11\u8bc4\u4f30\u6240\u9700\u7684\u9879\u76ee\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u6d4b\u91cf\u7cbe\u5ea6\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u4e2d\u5b58\u5728\u7684\u95ee\u9898\uff0c\u4f8b\u5982\u9879\u76ee\u8d28\u91cf\u4e0d\u4e00\u548c\u6392\u540d\u504f\u5dee\u3002"}}
{"id": "2511.05385", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05385", "abs": "https://arxiv.org/abs/2511.05385", "authors": ["Chao Zhang", "Yuhao Wang", "Derong Xu", "Haoxin Zhang", "Yuanjie Lyu", "Yuhao Chen", "Shuochen Liu", "Tong Xu", "Xiangyu Zhao", "Yan Gao", "Yao Hu", "Enhong Chen"], "title": "TeaRAG: A Token-Efficient Agentic Retrieval-Augmented Generation Framework", "comment": "32 pages", "summary": "Retrieval-Augmented Generation (RAG) utilizes external knowledge to augment\nLarge Language Models' (LLMs) reliability. For flexibility, agentic RAG employs\nautonomous, multi-round retrieval and reasoning to resolve queries. Although\nrecent agentic RAG has improved via reinforcement learning, they often incur\nsubstantial token overhead from search and reasoning processes. This trade-off\nprioritizes accuracy over efficiency. To address this issue, this work proposes\nTeaRAG, a token-efficient agentic RAG framework capable of compressing both\nretrieval content and reasoning steps. 1) First, the retrieved content is\ncompressed by augmenting chunk-based semantic retrieval with a graph retrieval\nusing concise triplets. A knowledge association graph is then built from\nsemantic similarity and co-occurrence. Finally, Personalized PageRank is\nleveraged to highlight key knowledge within this graph, reducing the number of\ntokens per retrieval. 2) Besides, to reduce reasoning steps, Iterative\nProcess-aware Direct Preference Optimization (IP-DPO) is proposed.\nSpecifically, our reward function evaluates the knowledge sufficiency by a\nknowledge matching mechanism, while penalizing excessive reasoning steps. This\ndesign can produce high-quality preference-pair datasets, supporting iterative\nDPO to improve reasoning conciseness. Across six datasets, TeaRAG improves the\naverage Exact Match by 4% and 2% while reducing output tokens by 61% and 59% on\nLlama3-8B-Instruct and Qwen2.5-14B-Instruct, respectively. Code is available at\nhttps://github.com/Applied-Machine-Learning-Lab/TeaRAG.", "AI": {"tldr": "TeaRAG\u901a\u8fc7\u538b\u7f29\u68c0\u7d22\u5185\u5bb9\u548c\u63a8\u7406\u6b65\u9aa4\uff0c\u63d0\u9ad8\u4e86agentic RAG\u7684\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684agentic RAG\u65b9\u6cd5\u5728\u641c\u7d22\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4ea7\u751f\u5927\u91cf\u7684token\u5f00\u9500\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002", "method": "1) \u4f7f\u7528\u57fa\u4e8e\u4e09\u5143\u7ec4\u7684\u56fe\u68c0\u7d22\u589e\u5f3achunk-based\u8bed\u4e49\u68c0\u7d22\u6765\u538b\u7f29\u68c0\u7d22\u5185\u5bb9\uff1b2) \u63d0\u51fa\u8fed\u4ee3\u7684Process-aware\u76f4\u63a5\u504f\u597d\u4f18\u5316(IP-DPO)\u6765\u51cf\u5c11\u63a8\u7406\u6b65\u9aa4\u3002", "result": "\u5728\u516d\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cTeaRAG\u5728Llama3-8B-Instruct\u548cQwen2.5-14B-Instruct\u4e0a\u5206\u522b\u5c06\u8f93\u51fatokens\u51cf\u5c11\u4e8661%\u548c59%\uff0c\u540c\u65f6\u5e73\u5747\u7cbe\u786e\u5339\u914d\u5ea6\u63d0\u9ad8\u4e864%\u548c2%\u3002", "conclusion": "TeaRAG\u662f\u4e00\u4e2atoken\u9ad8\u6548\u7684agentic RAG\u6846\u67b6\uff0c\u80fd\u591f\u5728\u51cf\u5c11token\u4f7f\u7528\u7684\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u9ad8\u51c6\u786e\u6027\u3002"}}
{"id": "2511.04753", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04753", "abs": "https://arxiv.org/abs/2511.04753", "authors": ["Zonglin Lyu", "Ming Li", "Xinxin Liu", "Chen Chen"], "title": "CPO: Condition Preference Optimization for Controllable Image Generation", "comment": null, "summary": "To enhance controllability in text-to-image generation, ControlNet introduces\nimage-based control signals, while ControlNet++ improves pixel-level cycle\nconsistency between generated images and the input control signal. To avoid the\nprohibitive cost of back-propagating through the sampling process, ControlNet++\noptimizes only low-noise timesteps (e.g., $t < 200$) using a single-step\napproximation, which not only ignores the contribution of high-noise timesteps\nbut also introduces additional approximation errors. A straightforward\nalternative for optimizing controllability across all timesteps is Direct\nPreference Optimization (DPO), a fine-tuning method that increases model\npreference for more controllable images ($I^{w}$) over less controllable ones\n($I^{l}$). However, due to uncertainty in generative models, it is difficult to\nensure that win--lose image pairs differ only in controllability while keeping\nother factors, such as image quality, fixed. To address this, we propose\nperforming preference learning over control conditions rather than generated\nimages. Specifically, we construct winning and losing control signals,\n$\\mathbf{c}^{w}$ and $\\mathbf{c}^{l}$, and train the model to prefer\n$\\mathbf{c}^{w}$. This method, which we term \\textit{Condition Preference\nOptimization} (CPO), eliminates confounding factors and yields a low-variance\ntraining objective. Our approach theoretically exhibits lower contrastive loss\nvariance than DPO and empirically achieves superior results. Moreover, CPO\nrequires less computation and storage for dataset curation. Extensive\nexperiments show that CPO significantly improves controllability over the\nstate-of-the-art ControlNet++ across multiple control types: over $10\\%$ error\nrate reduction in segmentation, $70$--$80\\%$ in human pose, and consistent\n$2$--$5\\%$ reductions in edge and depth maps.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6761\u4ef6\u504f\u597d\u4f18\u5316 (CPO) \u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u53ef\u63a7\u6027\uff0c\u901a\u8fc7\u5728\u63a7\u5236\u6761\u4ef6\u4e0a\u8fdb\u884c\u504f\u597d\u5b66\u4e60\uff0c\u800c\u4e0d\u662f\u5728\u751f\u6210\u7684\u56fe\u50cf\u4e0a\uff0c\u4ece\u800c\u51cf\u5c11\u4e86\u6df7\u6dc6\u56e0\u7d20\u548c\u8bad\u7ec3\u76ee\u6807\u65b9\u5dee\u3002", "motivation": "ControlNet++\u867d\u7136\u901a\u8fc7\u4f18\u5316\u4f4e\u566a\u58f0\u65f6\u95f4\u6b65\u6765\u63d0\u9ad8\u53ef\u63a7\u6027\uff0c\u4f46\u5ffd\u7565\u4e86\u9ad8\u566a\u58f0\u65f6\u95f4\u6b65\u7684\u8d21\u732e\uff0c\u5e76\u5f15\u5165\u4e86\u989d\u5916\u7684\u8fd1\u4f3c\u8bef\u5dee\u3002\u76f4\u63a5\u504f\u597d\u4f18\u5316 (DPO) \u53ef\u4ee5\u4f18\u5316\u6240\u6709\u65f6\u95f4\u6b65\u7684\u53ef\u63a7\u6027\uff0c\u4f46\u96be\u4ee5\u4fdd\u8bc1\u8f93\u8d62\u56fe\u50cf\u5bf9\u4ec5\u5728\u53ef\u63a7\u6027\u4e0a\u4e0d\u540c\uff0c\u800c\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u7b49\u5176\u4ed6\u56e0\u7d20\u4e0d\u53d8\u3002", "method": "\u6784\u5efa\u8f93\u8d62\u63a7\u5236\u4fe1\u53f7\uff0c\u5e76\u8bad\u7ec3\u6a21\u578b\u504f\u597d\u8d62\u5f97\u63a7\u5236\u4fe1\u53f7\u3002\u8fd9\u79cd\u65b9\u6cd5\u79f0\u4e3a\u6761\u4ef6\u504f\u597d\u4f18\u5316 (CPO)\u3002", "result": "CPO \u5728\u591a\u4e2a\u63a7\u5236\u7c7b\u578b\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u76f8\u5bf9\u4e8e\u6700\u5148\u8fdb\u7684 ControlNet++ \u7684\u53ef\u63a7\u6027\uff1a\u5206\u5272\u9519\u8bef\u7387\u964d\u4f4e\u8d85\u8fc7 10%\uff0c\u4eba\u4f53\u59ff\u52bf\u964d\u4f4e 70-80%\uff0c\u8fb9\u7f18\u548c\u6df1\u5ea6\u56fe\u6301\u7eed\u964d\u4f4e 2-5%\u3002", "conclusion": "CPO \u7406\u8bba\u4e0a\u6bd4 DPO \u5177\u6709\u66f4\u4f4e\u7684\u5bf9\u6bd4\u635f\u5931\u65b9\u5dee\uff0c\u5e76\u4e14\u5728\u5b9e\u9a8c\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u7ed3\u679c\u3002\u6b64\u5916\uff0cCPO \u5728\u6570\u636e\u96c6\u7ba1\u7406\u65b9\u9762\u9700\u8981\u7684\u8ba1\u7b97\u548c\u5b58\u50a8\u66f4\u5c11\u3002"}}
{"id": "2511.04722", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04722", "abs": "https://arxiv.org/abs/2511.04722", "authors": ["Qianyang Li", "Xingjun Zhang", "Peng Tao", "Shaoxun Wang", "Yancheng Pan", "Jia Wei"], "title": "AWEMixer: Adaptive Wavelet-Enhanced Mixer Network for Long-Term Time Series Forecasting", "comment": null, "summary": "Forecasting long-term time series in IoT environments remains a significant\nchallenge due to the non-stationary and multi-scale characteristics of sensor\nsignals. Furthermore, error accumulation causes a decrease in forecast quality\nwhen predicting further into the future. Traditional methods are restricted to\noperate in time-domain, while the global frequency information achieved by\nFourier transform would be regarded as stationary signals leading to blur the\ntemporal patterns of transient events. We propose AWEMixer, an Adaptive\nWavelet-Enhanced Mixer Network including two innovative components: 1) a\nFrequency Router designs to utilize the global periodicity pattern achieved by\nFast Fourier Transform to adaptively weight localized wavelet subband, and 2) a\nCoherent Gated Fusion Block to achieve selective integration of prominent\nfrequency features with multi-scale temporal representation through\ncross-attention and gating mechanism, which realizes accurate time-frequency\nlocalization while remaining robust to noise. Seven public benchmarks validate\nthat our model is more effective than recent state-of-the-art models.\nSpecifically, our model consistently achieves performance improvement compared\nwith transformer-based and MLP-based state-of-the-art models in long-sequence\ntime series forecasting. Code is available at\nhttps://github.com/hit636/AWEMixer", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAWEMixer\u7684\u81ea\u9002\u5e94\u5c0f\u6ce2\u589e\u5f3a\u6df7\u5408\u5668\u7f51\u7edc\uff0c\u7528\u4e8e\u7269\u8054\u7f51\u73af\u5883\u4e2d\u7684\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u3002", "motivation": "\u7531\u4e8e\u4f20\u611f\u5668\u4fe1\u53f7\u7684\u975e\u5e73\u7a33\u548c\u591a\u5c3a\u5ea6\u7279\u6027\uff0c\u7269\u8054\u7f51\u73af\u5883\u4e2d\u7684\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u6b64\u5916\uff0c\u8bef\u5dee\u7d2f\u79ef\u4f1a\u5bfc\u81f4\u9884\u6d4b\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u8be5\u6a21\u578b\u5305\u62ec\u4e24\u4e2a\u521b\u65b0\u7ec4\u4ef6\uff1a1) \u9891\u7387\u8def\u7531\u5668\uff0c\u5229\u7528\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\u83b7\u5f97\u7684\u5168\u5c40\u5468\u671f\u6027\u6a21\u5f0f\u81ea\u9002\u5e94\u5730\u52a0\u6743\u5c40\u90e8\u5c0f\u6ce2\u5b50\u5e26\uff1b2) \u76f8\u5e72\u95e8\u63a7\u878d\u5408\u5757\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u95e8\u63a7\u673a\u5236\u5b9e\u73b0\u7a81\u51fa\u9891\u7387\u7279\u5f81\u4e0e\u591a\u5c3a\u5ea6\u65f6\u95f4\u8868\u793a\u7684\u9009\u62e9\u6027\u96c6\u6210\u3002", "result": "\u5728\u4e03\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u6a21\u578b\u6bd4\u6700\u65b0\u7684\u6a21\u578b\u66f4\u6709\u6548\u3002\u4e0e\u57fa\u4e8eTransformer\u548cMLP\u7684\u6700\u65b0\u6a21\u578b\u76f8\u6bd4\uff0c\u8be5\u6a21\u578b\u5728\u957f\u5e8f\u5217\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u59cb\u7ec8\u5982\u4e00\u5730\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "conclusion": "AWEMixer\u6a21\u578b\u5728\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u6c34\u5e73\u7684\u6a21\u578b\u3002"}}
{"id": "2511.04692", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.04692", "abs": "https://arxiv.org/abs/2511.04692", "authors": ["Jingqing Wang", "Jiaxing Shang", "Rong Xu", "Fei Hao", "Tianjin Huang", "Geyong Min"], "title": "SARC: Sentiment-Augmented Deep Role Clustering for Fake News Detection", "comment": "12 pages, 11 figures, 4 tables, WSDM 2026 accepted paper", "summary": "Fake news detection has been a long-standing research focus in social\nnetworks. Recent studies suggest that incorporating sentiment information from\nboth news content and user comments can enhance detection performance. However,\nexisting approaches typically treat sentiment features as auxiliary signals,\noverlooking role differentiation, that is, the same sentiment polarity may\noriginate from users with distinct roles, thereby limiting their ability to\ncapture nuanced patterns for effective detection. To address this issue, we\npropose SARC, a Sentiment-Augmented Role Clustering framework which utilizes\nsentiment-enhanced deep clustering to identify user roles for improved fake\nnews detection. The framework first generates user features through joint\ncomment text representation (with BiGRU and Attention mechanism) and sentiment\nencoding. It then constructs a differentiable deep clustering module to\nautomatically categorize user roles. Finally, unlike existing approaches which\ntake fake news label as the unique supervision signal, we propose a joint\noptimization objective integrating role clustering and fake news detection to\nfurther improve the model performance. Experimental results on two benchmark\ndatasets, RumourEval-19 and Weibo-comp, demonstrate that SARC achieves superior\nperformance across all metrics compared to baseline models. The code is\navailable at: https://github.com/jxshang/SARC.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSARC\u7684\u60c5\u611f\u589e\u5f3a\u89d2\u8272\u805a\u7c7b\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u5047\u65b0\u95fb\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u60c5\u611f\u7279\u5f81\u89c6\u4e3a\u8f85\u52a9\u4fe1\u53f7\uff0c\u5ffd\u7565\u4e86\u89d2\u8272\u5dee\u5f02\uff0c\u5373\u76f8\u540c\u7684\u60c5\u611f\u6781\u6027\u53ef\u80fd\u6765\u81ea\u5177\u6709\u4e0d\u540c\u89d2\u8272\u7684\u7528\u6237\uff0c\u4ece\u800c\u9650\u5236\u4e86\u4ed6\u4eec\u6355\u6349\u7ec6\u5fae\u6a21\u5f0f\u4ee5\u8fdb\u884c\u6709\u6548\u68c0\u6d4b\u7684\u80fd\u529b\u3002", "method": "\u5229\u7528\u60c5\u611f\u589e\u5f3a\u7684\u6df1\u5ea6\u805a\u7c7b\u6765\u8bc6\u522b\u7528\u6237\u89d2\u8272\u3002\u8be5\u6846\u67b6\u9996\u5148\u901a\u8fc7\u8054\u5408\u8bc4\u8bba\u6587\u672c\u8868\u793a\uff08\u4f7f\u7528BiGRU\u548c\u6ce8\u610f\u529b\u673a\u5236\uff09\u548c\u60c5\u611f\u7f16\u7801\u751f\u6210\u7528\u6237\u7279\u5f81\u3002\u7136\u540e\uff0c\u5b83\u6784\u5efa\u4e86\u4e00\u4e2a\u53ef\u533a\u5206\u7684\u6df1\u5ea6\u805a\u7c7b\u6a21\u5757\u6765\u81ea\u52a8\u5206\u7c7b\u7528\u6237\u89d2\u8272\u3002\u6700\u540e\uff0c\u4e0e\u5c06\u5047\u65b0\u95fb\u6807\u7b7e\u4f5c\u4e3a\u552f\u4e00\u76d1\u7763\u4fe1\u53f7\u7684\u73b0\u6709\u65b9\u6cd5\u4e0d\u540c\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u8054\u5408\u4f18\u5316\u76ee\u6807\uff0c\u6574\u5408\u4e86\u89d2\u8272\u805a\u7c7b\u548c\u5047\u65b0\u95fb\u68c0\u6d4b\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6RumourEval-19\u548cWeibo-comp\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u6a21\u578b\u76f8\u6bd4\uff0cSARC\u5728\u6240\u6709\u6307\u6807\u4e0a\u90fd\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002", "conclusion": "SARC\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u5229\u7528\u60c5\u611f\u4fe1\u606f\u548c\u7528\u6237\u89d2\u8272\u4fe1\u606f\u6765\u63d0\u9ad8\u5047\u65b0\u95fb\u68c0\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2511.04696", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.04696", "abs": "https://arxiv.org/abs/2511.04696", "authors": ["Jan Strich", "Adeline Scharfenberg", "Chris Biemann", "Martin Semmann"], "title": "EncouRAGe: Evaluating RAG Local, Fast, and Reliable", "comment": "Currently under review", "summary": "We introduce EncouRAGe, a comprehensive Python framework designed to\nstreamline the development and evaluation of Retrieval-Augmented Generation\n(RAG) systems using Large Language Models (LLMs) and Embedding Models.\nEncouRAGe comprises five modular and extensible components: Type Manifest, RAG\nFactory, Inference, Vector Store, and Metrics, facilitating flexible\nexperimentation and extensible development. The framework emphasizes scientific\nreproducibility, diverse evaluation metrics, and local deployment, enabling\nresearchers to efficiently assess datasets within RAG workflows. This paper\npresents implementation details and an extensive evaluation across multiple\nbenchmark datasets, including 25k QA pairs and over 51k documents. Our results\nshow that RAG still underperforms compared to the Oracle Context, while Hybrid\nBM25 consistently achieves the best results across all four datasets. We\nfurther examine the effects of reranking, observing only marginal performance\nimprovements accompanied by higher response latency.", "AI": {"tldr": "EncouRAGe is a Python framework for developing and evaluating RAG systems.", "motivation": "Streamline the development and evaluation of RAG systems using LLMs and Embedding Models.", "method": "Five modular components: Type Manifest, RAG Factory, Inference, Vector Store, and Metrics.", "result": "RAG underperforms compared to Oracle Context; Hybrid BM25 performs best; reranking gives marginal performance improvements with higher latency.", "conclusion": "RAG requires further research to reach the performance of Oracle Context."}}
{"id": "2511.04766", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04766", "abs": "https://arxiv.org/abs/2511.04766", "authors": ["Dhenenjay Yadav", "Rohan Sawai"], "title": "DARN: Dynamic Adaptive Regularization Networks for Efficient and Robust Foundation Model Adaptation", "comment": null, "summary": "Foundation models (FMs) offer powerful representations for geospatial\nanalysis, but adapting them effectively remains challenging. Standard\nadaptation methods, whether full fine-tuning or efficient frozen-backbone\napproaches, typically employ decoders with fixed regularization strategies,\nfailing to account for the significant heterogeneity in satellite imagery. We\nintroduce Dynamic Adaptive Regularization Networks (DARN), a novel decoder\narchitecture designed to address this limitation. DARN integrates three key\ninnovations: (1) a lightweight Task Complexity Predictor (TCP) that estimates\nper-sample difficulty, (2) Adaptive Dropout Modulation (ADM), dynamically\nadjusting dropout rates (from 0.1 to 0.5) based on predicted complexity, and\n(3) Dynamic Capacity Gating (DCG) that modulates channel activation. We provide\ntheoretical justifications linking DARN's optimization to stationary point\nconvergence and its mechanism to adaptive information bottlenecks. Empirically,\nDARN demonstrates exceptional performance across both major adaptation\nparadigms. In full fine-tuning (unfrozen backbone), DARN achieves a new\nstate-of-the-art on the multi-task GeoBench benchmark (86.66% mIoU, +5.56 pp\nover prior SOTA). In efficient adaptation (frozen backbone), DARN achieves\nSOTA-competitive accuracy (90.5% mIoU on Sen1Floods11) while delivering\nsubstantial advantages crucial for real-world deployment: superior\nout-of-distribution (OOD) generalization (+9.5 pp mIoU on AI4SmallFarms),\nenhanced robustness (17% relative reduction in corruption error), and improved\nperformance on minority classes. DARN offers a more intelligent, robust, and\nefficient approach to leveraging FMs in critical geospatial applications.", "AI": {"tldr": "This paper introduces Dynamic Adaptive Regularization Networks (DARN), a novel decoder architecture for adapting foundation models (FMs) in geospatial analysis.", "motivation": "Standard adaptation methods fail to account for the heterogeneity in satellite imagery.", "method": "DARN integrates a Task Complexity Predictor, Adaptive Dropout Modulation, and Dynamic Capacity Gating to dynamically adjust regularization based on sample complexity.", "result": "DARN achieves state-of-the-art performance on GeoBench in full fine-tuning and SOTA-competitive accuracy on Sen1Floods11 in efficient adaptation, with superior out-of-distribution generalization, robustness, and minority class performance.", "conclusion": "DARN offers a more intelligent, robust, and efficient approach to leveraging FMs in critical geospatial applications."}}
{"id": "2511.04723", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04723", "abs": "https://arxiv.org/abs/2511.04723", "authors": ["Mohamadreza Akbari Pour", "Mohamad Sadeq Karimi", "Amir Hossein Mazloumi"], "title": "Temporal convolutional and fusional transformer model with Bi-LSTM encoder-decoder for multi-time-window remaining useful life prediction", "comment": null, "summary": "Health prediction is crucial for ensuring reliability, minimizing downtime,\nand optimizing maintenance in industrial systems. Remaining Useful Life (RUL)\nprediction is a key component of this process; however, many existing models\nstruggle to capture fine-grained temporal dependencies while dynamically\nprioritizing critical features across time for robust prognostics. To address\nthese challenges, we propose a novel framework that integrates Temporal\nConvolutional Networks (TCNs) for localized temporal feature extraction with a\nmodified Temporal Fusion Transformer (TFT) enhanced by Bi-LSTM encoder-decoder.\nThis architecture effectively bridges short- and long-term dependencies while\nemphasizing salient temporal patterns. Furthermore, the incorporation of a\nmulti-time-window methodology improves adaptability across diverse operating\nconditions. Extensive evaluations on benchmark datasets demonstrate that the\nproposed model reduces the average RMSE by up to 5.5%, underscoring its\nimproved predictive accuracy compared to state-of-the-art methods. By closing\ncritical gaps in current approaches, this framework advances the effectiveness\nof industrial prognostic systems and highlights the potential of advanced\ntime-series transformers for RUL prediction.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u9884\u6d4b\u5de5\u4e1a\u7cfb\u7edf\u4e2d\u5269\u4f59\u4f7f\u7528\u5bff\u547d\uff08RUL\uff09\u7684\u65b0\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u6355\u6349\u7ec6\u7c92\u5ea6\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u540c\u65f6\u65e0\u6cd5\u52a8\u6001\u5730\u4f18\u5148\u8003\u8651\u8de8\u65f6\u95f4\u7684\u5173\u952e\u7279\u5f81\u3002", "method": "\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u65f6\u95f4\u5377\u79ef\u7f51\u7edc\uff08TCN\uff09\u548c\u6539\u8fdb\u7684\u65f6\u95f4\u878d\u5408Transformer\uff08TFT\uff09\uff0c\u5e76\u7ed3\u5408Bi-LSTM\u7f16\u7801\u5668-\u89e3\u7801\u5668\u548c\u591a\u65f6\u95f4\u7a97\u53e3\u65b9\u6cd5\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5c06\u5e73\u5747RMSE\u964d\u4f4e\u4e86\u9ad8\u8fbe5.5%\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u9ad8\u4e86\u5de5\u4e1a\u9884\u6d4b\u7cfb\u7edf\u7684\u6709\u6548\u6027\uff0c\u5e76\u7a81\u51fa\u4e86\u5148\u8fdb\u7684\u65f6\u95f4\u5e8f\u5217Transformer\u5728RUL\u9884\u6d4b\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.04694", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04694", "abs": "https://arxiv.org/abs/2511.04694", "authors": ["Zishuo Zheng", "Vidhisha Balachandran", "Chan Young Park", "Faeze Brahman", "Sachin Kumar"], "title": "Reasoning Up the Instruction Ladder for Controllable Language Models", "comment": null, "summary": "As large language model (LLM) based systems take on high-stakes roles in\nreal-world decision-making, they must reconcile competing instructions from\nmultiple sources (e.g., model developers, users, and tools) within a single\nprompt context. Thus, enforcing an instruction hierarchy (IH) in LLMs, where\nhigher-level directives override lower-priority requests, is critical for the\nreliability and controllability of LLMs. In this work, we reframe instruction\nhierarchy resolution as a reasoning task. Specifically, the model must first\n\"think\" about the relationship between a given user prompt and higher-priority\n(system) instructions before generating a response. To enable this capability\nvia training, we construct VerIH, an instruction hierarchy dataset of\nconstraint-following tasks with verifiable answers. This dataset comprises both\naligned and conflicting system-user instructions. We show that lightweight\nreinforcement learning with VerIH effectively transfers general reasoning\ncapabilities of models to instruction prioritization. Our finetuned models\nachieve consistent improvements on instruction following and instruction\nhierarchy benchmarks. This reasoning ability also generalizes to\nsafety-critical settings beyond the training distribution. By treating safety\nissues as resolving conflicts between adversarial user inputs and predefined\nhigher-priority policies, our trained model enhances robustness against\njailbreak and prompt injection attacks. These results demonstrate that\nreasoning over instruction hierarchies provides a practical path to reliable\nLLMs, where updates to system prompts yield controllable and robust changes in\nmodel behavior.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5c06\u6307\u4ee4\u5c42\u6b21\u7ed3\u6784\u89e3\u6790\u8f6c\u5316\u4e3a\u63a8\u7406\u4efb\u52a1\uff0c\u4ee5\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u591a\u6e90\u6307\u4ee4\u4e0b\u7684\u53ef\u9760\u6027\u548c\u53ef\u63a7\u6027\u7684\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u73b0\u5b9e\u51b3\u7b56\u4e2d\u626e\u6f14\u91cd\u8981\u89d2\u8272\uff0c\u9700\u8981\u534f\u8c03\u6765\u81ea\u591a\u4e2a\u6765\u6e90\u7684\u6307\u4ee4\uff0c\u56e0\u6b64\uff0c\u5b9e\u65bd\u6307\u4ee4\u5c42\u6b21\u7ed3\u6784\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aVerIH\u7684\u6307\u4ee4\u5c42\u6b21\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6a21\u578b\uff0c\u4f7f\u5176\u5177\u5907\u6307\u4ee4\u4f18\u5148\u7ea7\u6392\u5e8f\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728\u6307\u4ee4\u9075\u5faa\u548c\u5c42\u6b21\u7ed3\u6784\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u6301\u7eed\u6539\u8fdb\uff0c\u5e76\u4e14\u5728\u5b89\u5168\u5173\u952e\u8bbe\u7f6e\u4e2d\u8868\u73b0\u51fa\u6cdb\u5316\u80fd\u529b\uff0c\u589e\u5f3a\u4e86\u5bf9\u6076\u610f\u653b\u51fb\u7684\u62b5\u6297\u529b\u3002", "conclusion": "\u901a\u8fc7\u63a8\u7406\u6307\u4ee4\u5c42\u6b21\u7ed3\u6784\uff0c\u53ef\u4ee5\u5b9e\u73b0\u53ef\u9760\u7684LLM\uff0c\u7cfb\u7edf\u63d0\u793a\u7684\u66f4\u65b0\u53ef\u4ee5\u5b9e\u73b0\u6a21\u578b\u884c\u4e3a\u7684\u53ef\u63a7\u548c\u9c81\u68d2\u53d8\u5316\u3002"}}
{"id": "2511.04773", "categories": ["cs.CV", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2511.04773", "abs": "https://arxiv.org/abs/2511.04773", "authors": ["Shirin Ermis", "Cesar Aybar", "Lilli Freischem", "Stella Girtsou", "Kyriaki-Margarita Bintsi", "Emiliano Diaz Salas-Porras", "Michael Eisinger", "William Jones", "Anna Jungbluth", "Benoit Tremblay"], "title": "Global 3D Reconstruction of Clouds & Tropical Cyclones", "comment": null, "summary": "Accurate forecasting of tropical cyclones (TCs) remains challenging due to\nlimited satellite observations probing TC structure and difficulties in\nresolving cloud properties involved in TC intensification. Recent research has\ndemonstrated the capabilities of machine learning methods for 3D cloud\nreconstruction from satellite observations. However, existing approaches have\nbeen restricted to regions where TCs are uncommon, and are poorly validated for\nintense storms. We introduce a new framework, based on a\npre-training--fine-tuning pipeline, that learns from multiple satellites with\nglobal coverage to translate 2D satellite imagery into 3D cloud maps of\nrelevant cloud properties. We apply our model to a custom-built TC dataset to\nevaluate performance in the most challenging and relevant conditions. We show\nthat we can - for the first time - create global instantaneous 3D cloud maps\nand accurately reconstruct the 3D structure of intense storms. Our model not\nonly extends available satellite observations but also provides estimates when\nobservations are missing entirely. This is crucial for advancing our\nunderstanding of TC intensification and improving forecasts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u9884\u8bad\u7ec3-\u5fae\u8c03\u6d41\u7a0b\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u5c062D\u536b\u661f\u56fe\u50cf\u8f6c\u6362\u4e3a3D\u4e91\u56fe\uff0c\u4ece\u800c\u91cd\u5efa\u5f3a\u70c8\u98ce\u66b4\u76843D\u7ed3\u6784\u3002", "motivation": "\u7531\u4e8e\u63a2\u6d4bTC\u7ed3\u6784\u7684\u536b\u661f\u89c2\u6d4b\u8d44\u6599\u6709\u9650\uff0c\u4ee5\u53ca\u96be\u4ee5\u89e3\u51b3TC\u589e\u5f3a\u4e2d\u6d89\u53ca\u7684\u4e91\u7279\u6027\uff0c\u56e0\u6b64\u51c6\u786e\u9884\u6d4b\u70ed\u5e26\u6c14\u65cb\uff08TC\uff09\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u57fa\u4e8e\u4e00\u4e2a\u9884\u8bad\u7ec3-\u5fae\u8c03\u6d41\u7a0b\uff0c\u8be5\u6d41\u7a0b\u4ece\u5177\u6709\u5168\u7403\u8986\u76d6\u8303\u56f4\u7684\u591a\u4e2a\u536b\u661f\u5b66\u4e60\uff0c\u4ee5\u5c062D\u536b\u661f\u56fe\u50cf\u8f6c\u6362\u4e3a\u76f8\u5173\u4e91\u7279\u6027\u76843D\u4e91\u56fe\u3002", "result": "\u8be5\u6a21\u578b\u9996\u6b21\u521b\u5efa\u4e86\u5168\u7403\u77ac\u65f63D\u4e91\u56fe\uff0c\u5e76\u51c6\u786e\u5730\u91cd\u5efa\u4e86\u5f3a\u70c8\u98ce\u66b4\u76843D\u7ed3\u6784\u3002\u8be5\u6a21\u578b\u4e0d\u4ec5\u6269\u5c55\u4e86\u53ef\u7528\u7684\u536b\u661f\u89c2\u6d4b\u8d44\u6599\uff0c\u800c\u4e14\u8fd8\u5728\u5b8c\u5168\u7f3a\u5c11\u89c2\u6d4b\u8d44\u6599\u65f6\u63d0\u4f9b\u4e86\u4f30\u8ba1\u3002", "conclusion": "\u8be5\u6a21\u578b\u5bf9\u4e8e\u52a0\u6df1\u6211\u4eec\u5bf9TC\u589e\u5f3a\u7684\u7406\u89e3\u548c\u6539\u8fdb\u9884\u62a5\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2511.04751", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04751", "abs": "https://arxiv.org/abs/2511.04751", "authors": ["Matteo Cercola", "Michele Lomuscio", "Dario Piga", "Simone Formentin"], "title": "Regularized GLISp for sensor-guided human-in-the-loop optimization", "comment": null, "summary": "Human-in-the-loop calibration is often addressed via preference-based\noptimization, where algorithms learn from pairwise comparisons rather than\nexplicit cost evaluations. While effective, methods such as Preferential\nBayesian Optimization or Global optimization based on active preference\nlearning with radial basis functions (GLISp) treat the system as a black box\nand ignore informative sensor measurements. In this work, we introduce a\nsensor-guided regularized extension of GLISp that integrates measurable\ndescriptors into the preference-learning loop through a physics-informed\nhypothesis function and a least-squares regularization term. This injects\ngrey-box structure, combining subjective feedback with quantitative sensor\ninformation while preserving the flexibility of preference-based search.\nNumerical evaluations on an analytical benchmark and on a human-in-the-loop\nvehicle suspension tuning task show faster convergence and superior final\nsolutions compared to baseline GLISp.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f20\u611f\u5668\u5f15\u5bfc\u7684 GLISp \u6269\u5c55\u65b9\u6cd5\uff0c\u7528\u4e8e\u4eba\u673a\u56de\u8def\u6821\u51c6\uff0c\u8be5\u65b9\u6cd5\u96c6\u6210\u4e86\u53ef\u6d4b\u91cf\u7684\u63cf\u8ff0\u7b26\u5230\u504f\u597d\u5b66\u4e60\u5faa\u73af\u4e2d\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u504f\u597d\u7684\u4f18\u5316\u65b9\u6cd5\u5ffd\u7565\u4e86\u4fe1\u606f\u4e30\u5bcc\u7684\u4f20\u611f\u5668\u6d4b\u91cf\u3002", "method": "\u901a\u8fc7\u7269\u7406\u4fe1\u606f\u5047\u8bbe\u51fd\u6570\u548c\u6700\u5c0f\u4e8c\u4e58\u6b63\u5219\u5316\u9879\uff0c\u5c06\u53ef\u6d4b\u91cf\u7684\u63cf\u8ff0\u7b26\u96c6\u6210\u5230\u504f\u597d\u5b66\u4e60\u5faa\u73af\u4e2d\u3002", "result": "\u5728\u5206\u6790\u57fa\u51c6\u548c\u4eba\u673a\u56de\u8def\u8f66\u8f86\u60ac\u67b6\u8c03\u6574\u4efb\u52a1\u4e2d\u7684\u6570\u503c\u8bc4\u4f30\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf GLISp \u76f8\u6bd4\uff0c\u6536\u655b\u901f\u5ea6\u66f4\u5feb\uff0c\u6700\u7ec8\u89e3\u51b3\u65b9\u6848\u66f4\u4f18\u8d8a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u4e3b\u89c2\u53cd\u9988\u548c\u5b9a\u91cf\u4f20\u611f\u5668\u4fe1\u606f\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u57fa\u4e8e\u504f\u597d\u7684\u641c\u7d22\u7684\u7075\u6d3b\u6027\u3002"}}
{"id": "2511.04779", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04779", "abs": "https://arxiv.org/abs/2511.04779", "authors": ["Andrea Aspesi", "Andrea Simpsi", "Aaron Tognoli", "Simone Mentasti", "Luca Merigo", "Matteo Matteucci"], "title": "EETnet: a CNN for Gaze Detection and Tracking for Smart-Eyewear", "comment": "International Joint Conference on Neural Networks (IJCNN), 2025", "summary": "Event-based cameras are becoming a popular solution for efficient, low-power\neye tracking. Due to the sparse and asynchronous nature of event data, they\nrequire less processing power and offer latencies in the microsecond range.\nHowever, many existing solutions are limited to validation on powerful GPUs,\nwith no deployment on real embedded devices. In this paper, we present EETnet,\na convolutional neural network designed for eye tracking using purely\nevent-based data, capable of running on microcontrollers with limited\nresources. Additionally, we outline a methodology to train, evaluate, and\nquantize the network using a public dataset. Finally, we propose two versions\nof the architecture: a classification model that detects the pupil on a grid\nsuperimposed on the original image, and a regression model that operates at the\npixel level.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEETnet\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u4e13\u95e8\u7528\u4e8e\u4f7f\u7528\u7eaf\u7cb9\u7684\u57fa\u4e8e\u4e8b\u4ef6\u7684\u6570\u636e\u8fdb\u884c\u773c\u52a8\u8ffd\u8e2a\uff0c\u5e76\u4e14\u80fd\u591f\u5728\u8d44\u6e90\u6709\u9650\u7684\u5fae\u63a7\u5236\u5668\u4e0a\u8fd0\u884c\u3002", "motivation": "\u73b0\u6709\u7684\u773c\u52a8\u8ffd\u8e2a\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u9700\u8981\u5728\u5f3a\u5927\u7684GPU\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u65e0\u6cd5\u5728\u5b9e\u9645\u7684\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u90e8\u7f72\u3002\u57fa\u4e8e\u4e8b\u4ef6\u7684\u76f8\u673a\u867d\u7136\u5177\u6709\u9ad8\u6548\u3001\u4f4e\u529f\u8017\u7684\u4f18\u70b9\uff0c\u4f46\u5176\u7a00\u758f\u548c\u5f02\u6b65\u7684\u7279\u6027\u5bf9\u7b97\u6cd5\u63d0\u51fa\u4e86\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u773c\u52a8\u8ffd\u8e2a\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edcEETnet\uff0c\u5e76\u6982\u8ff0\u4e86\u4e00\u79cd\u4f7f\u7528\u516c\u5171\u6570\u636e\u96c6\u8bad\u7ec3\u3001\u8bc4\u4f30\u548c\u91cf\u5316\u8be5\u7f51\u7edc\u7684\u65b9\u6cd5\u3002\u63d0\u51fa\u4e86\u4e24\u79cd\u7248\u672c\u7684\u67b6\u6784\uff1a\u4e00\u79cd\u662f\u5728\u539f\u59cb\u56fe\u50cf\u4e0a\u53e0\u52a0\u7684\u7f51\u683c\u4e0a\u68c0\u6d4b\u77b3\u5b54\u7684\u5206\u7c7b\u6a21\u578b\uff0c\u53e6\u4e00\u79cd\u662f\u5728\u50cf\u7d20\u7ea7\u522b\u4e0a\u8fd0\u884c\u7684\u56de\u5f52\u6a21\u578b\u3002", "result": "\u8be5\u7f51\u7edc\u80fd\u591f\u5728\u8d44\u6e90\u6709\u9650\u7684\u5fae\u63a7\u5236\u5668\u4e0a\u8fd0\u884c\uff0c\u8868\u660e\u4e86\u5176\u5728\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u7684\u53ef\u884c\u6027\u3002", "conclusion": "EETnet\u7684\u63d0\u51fa\u4e3a\u57fa\u4e8e\u4e8b\u4ef6\u7684\u773c\u52a8\u8ffd\u8e2a\u5728\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2511.04760", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04760", "abs": "https://arxiv.org/abs/2511.04760", "authors": ["Vaibhav Singh", "Eugene Belilovsky", "Rahaf Aljundi"], "title": "When Data Falls Short: Grokking Below the Critical Threshold", "comment": "6 pages", "summary": "In this paper, we investigate the phenomenon of grokking, where models\nexhibit delayed generalization following overfitting on training data. We focus\non data-scarce regimes where the number of training samples falls below the\ncritical threshold, making grokking unobservable, and on practical scenarios\ninvolving distribution shift. We first show that Knowledge Distillation (KD)\nfrom a model that has already grokked on a distribution (p1) can induce and\naccelerate grokking on a different distribution (p2), even when the available\ndata lies below the critical threshold. This highlights the value of KD for\ndeployed models that must adapt to new distributions under limited data. We\nthen study training on the joint distribution (p1, p2) and demonstrate that\nwhile standard supervised training fails when either distribution has\ninsufficient data, distilling from models grokked on the individual\ndistributions enables generalization. Finally, we examine a continual\npretraining setup, where a grokked model transitions from p1 to p2, and find\nthat KD both accelerates generalization and mitigates catastrophic forgetting,\nachieving strong performance even with only 10% of the data. Together, our\nresults provide new insights into the mechanics of grokking under knowledge\ntransfer and underscore the central role of KD in enabling generalization in\nlow-data and evolving distribution settings.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728\u6570\u636e\u7a00\u7f3a\u548c\u5206\u5e03\u504f\u79fb\u60c5\u51b5\u4e0b\uff0c\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u5982\u4f55\u4fc3\u8fdb\u548c\u52a0\u901fGrokking\u73b0\u8c61\uff0c\u5373\u4f7f\u5728\u6570\u636e\u91cf\u4f4e\u4e8e\u4e34\u754c\u9608\u503c\u65f6\u4e5f\u80fd\u5b9e\u73b0\u6cdb\u5316\u3002", "motivation": "\u6a21\u578b\u5728\u8bad\u7ec3\u6570\u636e\u4e0a\u8fc7\u62df\u5408\u540e\u8868\u73b0\u51fa\u5ef6\u8fdf\u6cdb\u5316\uff08Grokking\uff09\u73b0\u8c61\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u7a00\u7f3a\u548c\u5b9e\u9645\u5206\u5e03\u504f\u79fb\u7684\u60c5\u51b5\u4e0b\u3002", "method": "1. \u4f7f\u7528\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u4ece\u5df2\u5728\u5206\u5e03p1\u4e0aGrokking\u7684\u6a21\u578b\u4e2d\uff0c\u8bf1\u5bfc\u548c\u52a0\u901f\u5728\u4e0d\u540c\u5206\u5e03p2\u4e0a\u7684Grokking\u3002\n2. \u7814\u7a76\u5728\u8054\u5408\u5206\u5e03\uff08p1, p2\uff09\u4e0a\u7684\u8bad\u7ec3\uff0c\u5229\u7528\u4ece\u5728\u5355\u72ec\u5206\u5e03\u4e0aGrokking\u7684\u6a21\u578b\u4e2d\u63d0\u53d6\u7684\u77e5\u8bc6\u8fdb\u884c\u6cdb\u5316\u3002\n3. \u8003\u5bdf\u6301\u7eed\u9884\u8bad\u7ec3\u8bbe\u7f6e\uff0c\u5176\u4e2dGrokking\u6a21\u578b\u4ecep1\u8fc7\u6e21\u5230p2\uff0c\u4f7f\u7528KD\u52a0\u901f\u6cdb\u5316\u5e76\u51cf\u8f7b\u707e\u96be\u6027\u9057\u5fd8\u3002", "result": "1. \u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u53ef\u4ee5\u8bf1\u5bfc\u548c\u52a0\u901fGrokking\uff0c\u5373\u4f7f\u5728\u6570\u636e\u4f4e\u4e8e\u4e34\u754c\u9608\u503c\u65f6\u3002\n2. \u4ece\u5728\u5355\u72ec\u5206\u5e03\u4e0aGrokking\u7684\u6a21\u578b\u4e2d\u8fdb\u884c\u84b8\u998f\uff0c\u80fd\u591f\u5728\u8054\u5408\u5206\u5e03\u4e0a\u5b9e\u73b0\u6cdb\u5316\u3002\n3. \u5728\u6301\u7eed\u9884\u8bad\u7ec3\u4e2d\uff0cKD\u52a0\u901f\u6cdb\u5316\u5e76\u51cf\u8f7b\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5373\u4f7f\u53ea\u670910%\u7684\u6570\u636e\u4e5f\u80fd\u5b9e\u73b0\u5f3a\u5927\u7684\u6027\u80fd\u3002", "conclusion": "\u77e5\u8bc6\u84b8\u998f\u5728\u4f4e\u6570\u636e\u548c\u4e0d\u65ad\u6f14\u53d8\u7684\u5206\u5e03\u73af\u5883\u4e2d\uff0c\u5bf9\u4e8e\u5b9e\u73b0\u6cdb\u5316\u8d77\u7740\u6838\u5fc3\u4f5c\u7528\u3002"}}
{"id": "2511.04698", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04698", "abs": "https://arxiv.org/abs/2511.04698", "authors": ["K M Sajjadul Islam", "John Fields", "Praveen Madiraju"], "title": "multiMentalRoBERTa: A Fine-tuned Multiclass Classifier for Mental Health Disorder", "comment": "Accepted in IEEE Big Data, 8-11 December, 2025 @ Macau SAR, China", "summary": "The early detection of mental health disorders from social media text is\ncritical for enabling timely support, risk assessment, and referral to\nappropriate resources. This work introduces multiMentalRoBERTa, a fine-tuned\nRoBERTa model designed for multiclass classification of common mental health\nconditions, including stress, anxiety, depression, post-traumatic stress\ndisorder (PTSD), suicidal ideation, and neutral discourse. Drawing on multiple\ncurated datasets, data exploration is conducted to analyze class overlaps,\nrevealing strong correlations between depression and suicidal ideation as well\nas anxiety and PTSD, while stress emerges as a broad, overlapping category.\nComparative experiments with traditional machine learning methods,\ndomain-specific transformers, and prompting-based large language models\ndemonstrate that multiMentalRoBERTa achieves superior performance, with macro\nF1-scores of 0.839 in the six-class setup and 0.870 in the five-class setup\n(excluding stress), outperforming both fine-tuned MentalBERT and baseline\nclassifiers. Beyond predictive accuracy, explainability methods, including\nLayer Integrated Gradients and KeyBERT, are applied to identify lexical cues\nthat drive classification, with a particular focus on distinguishing depression\nfrom suicidal ideation. The findings emphasize the effectiveness of fine-tuned\ntransformers for reliable and interpretable detection in sensitive contexts,\nwhile also underscoring the importance of fairness, bias mitigation, and\nhuman-in-the-loop safety protocols. Overall, multiMentalRoBERTa is presented as\na lightweight, robust, and deployable solution for enhancing support in mental\nhealth platforms.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a multiMentalRoBERTa \u7684\u5fae\u8c03 RoBERTa \u6a21\u578b\uff0c\u7528\u4e8e\u5bf9\u793e\u4ea4\u5a92\u4f53\u6587\u672c\u4e2d\u5e38\u89c1\u7684\u7cbe\u795e\u5065\u5eb7\u72b6\u51b5\u8fdb\u884c\u591a\u7c7b\u5206\u7c7b\u3002", "motivation": "\u53ca\u65f6\u53d1\u73b0\u793e\u4ea4\u5a92\u4f53\u6587\u672c\u4e2d\u7684\u7cbe\u795e\u5065\u5eb7\u969c\u788d\u5bf9\u4e8e\u63d0\u4f9b\u53ca\u65f6\u7684\u652f\u6301\u3001\u98ce\u9669\u8bc4\u4f30\u548c\u8f6c\u8bca\u81f3\u9002\u5f53\u7684\u8d44\u6e90\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5728\u591a\u4e2a\u7cbe\u9009\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03 RoBERTa \u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u6570\u636e\u63a2\u7d22\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u6765\u8bc6\u522b\u9a71\u52a8\u5206\u7c7b\u7684\u8bcd\u6c47\u7ebf\u7d22\u3002", "result": "multiMentalRoBERTa \u5728\u516d\u7c7b\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u4e86 0.839 \u7684\u5b8f F1 \u5206\u6570\uff0c\u5728\u4e94\u7c7b\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u4e86 0.870 \u7684\u5b8f F1 \u5206\u6570\uff0c\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5fae\u8c03 transformers \u5728\u654f\u611f\u73af\u5883\u4e2d\u8fdb\u884c\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u68c0\u6d4b\u7684\u6709\u6548\u6027\uff0c\u540c\u65f6\u4e5f\u5f3a\u8c03\u4e86\u516c\u5e73\u6027\u3001\u504f\u89c1\u7f13\u89e3\u548c\u4eba\u673a\u534f\u4f5c\u5b89\u5168\u534f\u8bae\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.04797", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04797", "abs": "https://arxiv.org/abs/2511.04797", "authors": ["Jim James", "Ben Wilson", "Simon Lucey", "James Hays"], "title": "3D Gaussian Point Encoders", "comment": "10 pages, 3 figures, 3 tables", "summary": "In this work, we introduce the 3D Gaussian Point Encoder, an explicit\nper-point embedding built on mixtures of learned 3D Gaussians. This explicit\ngeometric representation for 3D recognition tasks is a departure from widely\nused implicit representations such as PointNet. However, it is difficult to\nlearn 3D Gaussian encoders in end-to-end fashion with standard optimizers. We\ndevelop optimization techniques based on natural gradients and distillation\nfrom PointNets to find a Gaussian Basis that can reconstruct PointNet\nactivations. The resulting 3D Gaussian Point Encoders are faster and more\nparameter efficient than traditional PointNets. As in the 3D reconstruction\nliterature where there has been considerable interest in the move from implicit\n(e.g., NeRF) to explicit (e.g., Gaussian Splatting) representations, we can\ntake advantage of computational geometry heuristics to accelerate 3D Gaussian\nPoint Encoders further. We extend filtering techniques from 3D Gaussian\nSplatting to construct encoders that run 2.7 times faster as a comparable\naccuracy PointNet while using 46% less memory and 88% fewer FLOPs. Furthermore,\nwe demonstrate the effectiveness of 3D Gaussian Point Encoders as a component\nin Mamba3D, running 1.27 times faster and achieving a reduction in memory and\nFLOPs by 42% and 54% respectively. 3D Gaussian Point Encoders are lightweight\nenough to achieve high framerates on CPU-only devices.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u76843D\u9ad8\u65af\u70b9\u7f16\u7801\u5668\uff0c\u5b83\u6bd4\u4f20\u7edf\u7684PointNet\u66f4\u5feb\u3001\u66f4\u9ad8\u6548\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b33D\u8bc6\u522b\u4efb\u52a1\u4e2d\u9690\u5f0f\u8868\u793a\uff08\u5982PointNet\uff09\u7684\u5c40\u9650\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u81ea\u7136\u68af\u5ea6\u548cPointNet\u84b8\u998f\u7684\u4f18\u5316\u6280\u672f\uff0c\u4ee5\u627e\u5230\u80fd\u591f\u91cd\u5efaPointNet\u6fc0\u6d3b\u7684\u9ad8\u65af\u57fa\u3002", "result": "3D\u9ad8\u65af\u70b9\u7f16\u7801\u5668\u6bd4PointNet\u5feb2.7\u500d\uff0c\u5185\u5b58\u51cf\u5c1146%\uff0cFLOPs\u51cf\u5c1188%\u3002\u5728Mamba3D\u4e2d\uff0c\u901f\u5ea6\u63d0\u9ad81.27\u500d\uff0c\u5185\u5b58\u51cf\u5c1142%\uff0cFLOPs\u51cf\u5c1154%\u3002", "conclusion": "3D\u9ad8\u65af\u70b9\u7f16\u7801\u5668\u8db3\u591f\u8f7b\u91cf\uff0c\u53ef\u4ee5\u5728\u7eafCPU\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u5e27\u7387\u3002"}}
{"id": "2511.04768", "categories": ["cs.LG", "cs.AR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.04768", "abs": "https://arxiv.org/abs/2511.04768", "authors": ["Rubens Lacouture", "Nathan Zhang", "Ritvik Sharma", "Marco Siracusa", "Fredrik Kjolstad", "Kunle Olukotun", "Olivia Hsu"], "title": "FuseFlow: A Fusion-Centric Compilation Framework for Sparse Deep Learning on Streaming Dataflow", "comment": null, "summary": "As deep learning models scale, sparse computation and specialized dataflow\nhardware have emerged as powerful solutions to address efficiency. We propose\nFuseFlow, a compiler that converts sparse machine learning models written in\nPyTorch to fused sparse dataflow graphs for reconfigurable dataflow\narchitectures (RDAs). FuseFlow is the first compiler to support general\ncross-expression fusion of sparse operations. In addition to fusion across\nkernels (expressions), FuseFlow also supports optimizations like\nparallelization, dataflow ordering, and sparsity blocking. It targets a\ncycle-accurate dataflow simulator for microarchitectural analysis of fusion\nstrategies. We use FuseFlow for design-space exploration across four real-world\nmachine learning applications with sparsity, showing that full fusion (entire\ncross-expression fusion across all computation in an end-to-end model) is not\nalways optimal for sparse models-fusion granularity depends on the model\nitself. FuseFlow also provides a heuristic to identify and prune suboptimal\nconfigurations. Using Fuseflow, we achieve performance improvements, including\na ~2.7x speedup over an unfused baseline for GPT-3 with BigBird block-sparse\nattention.", "AI": {"tldr": "FuseFlow\u662f\u4e00\u4e2a\u7f16\u8bd1\u5668\uff0c\u53ef\u4ee5\u5c06PyTorch\u4e2d\u7f16\u5199\u7684\u7a00\u758f\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8f6c\u6362\u4e3a\u7528\u4e8e\u53ef\u91cd\u6784\u6570\u636e\u6d41\u67b6\u6784\uff08RDA\uff09\u7684\u878d\u5408\u7a00\u758f\u6570\u636e\u6d41\u56fe\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6269\u5c55\u5e26\u6765\u7684\u6548\u7387\u95ee\u9898\uff0c\u7a00\u758f\u8ba1\u7b97\u548c\u4e13\u7528\u6570\u636e\u6d41\u786c\u4ef6\u5e94\u8fd0\u800c\u751f\u3002", "method": "FuseFlow\u652f\u6301\u7a00\u758f\u64cd\u4f5c\u7684\u901a\u7528\u8de8\u8868\u8fbe\u5f0f\u878d\u5408\uff0c\u4ee5\u53ca\u5e76\u884c\u5316\u3001\u6570\u636e\u6d41\u6392\u5e8f\u548c\u7a00\u758f\u963b\u585e\u7b49\u4f18\u5316\u3002", "result": "\u5728\u56db\u4e2a\u5177\u6709\u7a00\u758f\u6027\u7684\u771f\u5b9e\u673a\u5668\u5b66\u4e60\u5e94\u7528\u7a0b\u5e8f\u4e2d\uff0c\u5b8c\u6574\u7684\u878d\u5408\u5e76\u4e0d\u603b\u662f\u7a00\u758f\u6a21\u578b\u7684\u6700\u4f73\u9009\u62e9\u3002FuseFlow\u8fd8\u63d0\u4f9b\u4e86\u4e00\u79cd\u542f\u53d1\u5f0f\u65b9\u6cd5\u6765\u8bc6\u522b\u548c\u4fee\u526a\u6b21\u4f18\u914d\u7f6e\u3002\u5bf9\u4e8e\u5177\u6709BigBird\u5757\u7a00\u758f\u6ce8\u610f\u529b\u7684GPT-3\uff0c\u4e0e\u672a\u878d\u5408\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e86\u7ea62.7\u500d\u7684\u52a0\u901f\u3002", "conclusion": "\u878d\u5408\u7c92\u5ea6\u53d6\u51b3\u4e8e\u6a21\u578b\u672c\u8eab\u3002"}}
{"id": "2511.04699", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04699", "abs": "https://arxiv.org/abs/2511.04699", "authors": ["Haneen Al-Homoud", "Asma Ibrahim", "Murtadha Al-Jubran", "Fahad Al-Otaibi", "Yazeed Al-Harbi", "Daulet Toibazar", "Kesen Wang", "Pedro J. Moreno"], "title": "Cross-Lingual SynthDocs: A Large-Scale Synthetic Corpus for Any to Arabic OCR and Document Understanding", "comment": null, "summary": "Cross-Lingual SynthDocs is a large-scale synthetic corpus designed to address\nthe scarcity of Arabic resources for Optical Character Recognition (OCR) and\nDocument Understanding (DU). The dataset comprises over 2.5 million of samples,\nincluding 1.5 million textual data, 270K fully annotated tables, and hundred\nthousands of real data based charts. Our pipeline leverages authentic scanned\nbackgrounds, bilingual layouts, and diacritic aware fonts to capture the\ntypographic and structural complexity of Arabic documents. In addition to text,\nthe corpus includes variety of rendered styles for charts and tables.\nFinetuning Qwen-2.5-VL on SynthDocs yields consistent improvements in Word\nError Rate (WER) and Character Error Rate (CER) in terms of OCR across multiple\npublic Arabic benchmarks, Tree-Edit Distance Similarity (TEDS) and Chart\nExtraction Score (CharTeX) improved as well in other modalities. SynthDocs\nprovides a scalable, visually realistic resource for advancing research in\nmultilingual document analysis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aCross-Lingual SynthDocs\u7684\u5927\u89c4\u6a21\u5408\u6210\u8bed\u6599\u5e93\uff0c\u65e8\u5728\u89e3\u51b3\u963f\u62c9\u4f2f\u8bedOCR\u548c\u6587\u6863\u7406\u89e3\u8d44\u6e90\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u963f\u62c9\u4f2f\u8bedOCR\u548c\u6587\u6863\u7406\u89e3\u8d44\u6e90\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528\u771f\u5b9e\u7684\u626b\u63cf\u80cc\u666f\u3001\u53cc\u8bed\u5e03\u5c40\u548c\u533a\u5206\u53d8\u97f3\u7b26\u53f7\u7684\u5b57\u4f53\u6765\u6355\u6349\u963f\u62c9\u4f2f\u8bed\u6587\u6863\u7684\u6392\u7248\u548c\u7ed3\u6784\u590d\u6742\u6027\u3002\u9664\u4e86\u6587\u672c\uff0c\u8bed\u6599\u5e93\u8fd8\u5305\u62ec\u5404\u79cd\u56fe\u8868\u548c\u8868\u683c\u7684\u6e32\u67d3\u6837\u5f0f\u3002", "result": "\u5728SynthDocs\u4e0a\u5fae\u8c03Qwen-2.5-VL\u53ef\u4ee5\u5728\u591a\u4e2a\u516c\u5171\u963f\u62c9\u4f2f\u8bed\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u63d0\u9ad8OCR\u7684\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\u548c\u5b57\u7b26\u9519\u8bef\u7387\uff08CER\uff09\uff0c\u5e76\u4e14\u5728\u5176\u4ed6\u6a21\u6001\u4e2d\uff0c\u6811\u7f16\u8f91\u8ddd\u79bb\u76f8\u4f3c\u5ea6\uff08TEDS\uff09\u548c\u56fe\u8868\u63d0\u53d6\u5206\u6570\uff08CharTeX\uff09\u4e5f\u5f97\u5230\u4e86\u63d0\u9ad8\u3002", "conclusion": "SynthDocs\u4e3a\u63a8\u8fdb\u591a\u8bed\u8a00\u6587\u6863\u5206\u6790\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u3001\u89c6\u89c9\u4e0a\u903c\u771f\u7684\u8d44\u6e90\u3002"}}
{"id": "2511.04803", "categories": ["cs.CV", "cs.AI", "cs.LG", "I.2.10; I.4.6"], "pdf": "https://arxiv.org/pdf/2511.04803", "abs": "https://arxiv.org/abs/2511.04803", "authors": ["Shuo Zhao", "Jianxu Chen"], "title": "Data Efficiency and Transfer Robustness in Biomedical Image Segmentation: A Study of Redundancy and Forgetting with Cellpose", "comment": "Accepted to IEEE BIBM 2025 Workshop; 6 pages; 4 figures; 5 tables;\n  IEEEtran class. Code: https://github.com/MMV-Lab/biomedseg-efficiency", "summary": "Generalist biomedical image segmentation models such as Cellpose are\nincreasingly applied across diverse imaging modalities and cell types. However,\ntwo critical challenges remain underexplored: (1) the extent of training data\nredundancy and (2) the impact of cross domain transfer on model retention. In\nthis study, we conduct a systematic empirical analysis of these challenges\nusing Cellpose as a case study. First, to assess data redundancy, we propose a\nsimple dataset quantization (DQ) strategy for constructing compact yet diverse\ntraining subsets. Experiments on the Cyto dataset show that image segmentation\nperformance saturates with only 10% of the data, revealing substantial\nredundancy and potential for training with minimal annotations. Latent space\nanalysis using MAE embeddings and t-SNE confirms that DQ selected patches\ncapture greater feature diversity than random sampling. Second, to examine\ncatastrophic forgetting, we perform cross domain finetuning experiments and\nobserve significant degradation in source domain performance, particularly when\nadapting from generalist to specialist domains. We demonstrate that selective\nDQ based replay reintroducing just 5-10% of the source data effectively\nrestores source performance, while full replay can hinder target adaptation.\nAdditionally, we find that training domain sequencing improves generalization\nand reduces forgetting in multi stage transfer. Our findings highlight the\nimportance of data centric design in biomedical image segmentation and suggest\nthat efficient training requires not only compact subsets but also retention\naware learning strategies and informed domain ordering. The code is available\nat https://github.com/MMV-Lab/biomedseg-efficiency.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578bCellpose\u7684\u8bad\u7ec3\u6570\u636e\u5197\u4f59\u548c\u8de8\u57df\u8fc1\u79fb\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u901a\u7528\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\u5728\u4e0d\u540c\u6210\u50cf\u65b9\u5f0f\u548c\u7ec6\u80de\u7c7b\u578b\u4e2d\u5e94\u7528\u8d8a\u6765\u8d8a\u5e7f\u6cdb\uff0c\u4f46\u8bad\u7ec3\u6570\u636e\u5197\u4f59\u548c\u8de8\u57df\u8fc1\u79fb\u5bf9\u6a21\u578b\u4fdd\u6301\u80fd\u529b\u7684\u5f71\u54cd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "1. \u63d0\u51fa\u6570\u636e\u96c6\u91cf\u5316(DQ)\u7b56\u7565\u6784\u5efa\u7d27\u51d1\u800c\u591a\u6837\u5316\u7684\u8bad\u7ec3\u5b50\u96c6\uff1b2. \u4f7f\u7528MAE\u5d4c\u5165\u548ct-SNE\u8fdb\u884c\u6f5c\u5728\u7a7a\u95f4\u5206\u6790\uff1b3. \u8fdb\u884c\u8de8\u57df\u5fae\u8c03\u5b9e\u9a8c\uff0c\u89c2\u5bdf\u6e90\u57df\u6027\u80fd\u7684\u4e0b\u964d\uff1b4. \u4f7f\u7528\u9009\u62e9\u6027DQ\u57fa\u4e8e\u91cd\u653e\uff0c\u91cd\u65b0\u5f15\u5165\u5c11\u91cf\u6e90\u6570\u636e\uff0c\u6062\u590d\u6e90\u6027\u80fd\uff1b5. \u63a2\u7d22\u8bad\u7ec3\u57df\u6392\u5e8f\u5bf9\u591a\u9636\u6bb5\u8fc1\u79fb\u7684\u5f71\u54cd\u3002", "result": "1. \u4ec5\u4f7f\u752810%\u7684\u6570\u636e\uff0c\u56fe\u50cf\u5206\u5272\u6027\u80fd\u8fbe\u5230\u9971\u548c\uff0c\u8868\u660e\u5b58\u5728\u5927\u91cf\u5197\u4f59\uff1b2. DQ\u9009\u62e9\u7684patch\u6bd4\u968f\u673a\u62bd\u6837\u6355\u83b7\u66f4\u5927\u7684\u7279\u5f81\u591a\u6837\u6027\uff1b3. \u8de8\u57df\u5fae\u8c03\u4f1a\u5bfc\u81f4\u6e90\u57df\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff1b4. \u91cd\u65b0\u5f15\u51655-10%\u7684\u6e90\u6570\u636e\u53ef\u6709\u6548\u6062\u590d\u6e90\u6027\u80fd\uff1b5. \u8bad\u7ec3\u57df\u6392\u5e8f\u53ef\u4ee5\u6539\u5584\u6cdb\u5316\u5e76\u51cf\u5c11\u591a\u9636\u6bb5\u8fc1\u79fb\u4e2d\u7684\u9057\u5fd8\u3002", "conclusion": "\u9ad8\u6548\u8bad\u7ec3\u4e0d\u4ec5\u9700\u8981\u7d27\u51d1\u7684\u5b50\u96c6\uff0c\u8fd8\u9700\u8981\u5177\u6709\u4fdd\u7559\u610f\u8bc6\u7684\u5b66\u4e60\u7b56\u7565\u548c\u77e5\u60c5\u7684\u9886\u57df\u6392\u5e8f\u3002"}}
{"id": "2511.04774", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.04774", "abs": "https://arxiv.org/abs/2511.04774", "authors": ["Liu Jiang", "Zerui Bao", "Shiqi Sheng", "Di Zhu"], "title": "SLOFetch: Compressed-Hierarchical Instruction Prefetching for Cloud Microservices", "comment": null, "summary": "Large-scale networked services rely on deep soft-ware stacks and microservice\norchestration, which increase instruction footprints and create frontend stalls\nthat inflate tail latency and energy. We revisit instruction prefetching for\nthese cloud workloads and present a design that aligns with SLO driven and self\noptimizing systems. Building on the Entangling Instruction Prefetcher (EIP), we\nintroduce a Compressed Entry that captures up to eight destinations around a\nbase using 36 bits by exploiting spatial clustering, and a Hierarchical\nMetadata Storage scheme that keeps only L1 resident and frequently queried\nentries on chip while virtualizing bulk metadata into lower levels. We further\nadd a lightweight Online ML Controller that scores prefetch profitability using\ncontext features and a bandit adjusted threshold. On data center applications,\nour approach preserves EIP like speedups with smaller on chip state and\nimproves efficiency for networked services in the ML era.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6307\u4ee4\u9884\u53d6\u8bbe\u8ba1\uff0c\u65e8\u5728\u63d0\u9ad8\u4e91\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6548\u7387\uff0c\u5e76\u51cf\u5c11\u5c3e\u90e8\u5ef6\u8fdf\u548c\u80fd\u6e90\u6d88\u8017\u3002", "motivation": "\u73b0\u6709\u7684\u4e91\u670d\u52a1\u4f9d\u8d56\u4e8e\u6df1\u5ea6\u8f6f\u4ef6\u6808\u548c\u5fae\u670d\u52a1\u7f16\u6392\uff0c\u5bfc\u81f4\u6307\u4ee4\u5360\u7528\u7a7a\u95f4\u589e\u52a0\uff0c\u524d\u7aef\u505c\u987f\uff0c\u5c3e\u90e8\u5ef6\u8fdf\u589e\u5927\uff0c\u80fd\u6e90\u6d88\u8017\u589e\u52a0\u3002", "method": "\u8be5\u8bbe\u8ba1\u57fa\u4e8eEntangling Instruction Prefetcher (EIP)\uff0c\u5f15\u5165\u4e86\u538b\u7f29\u6761\u76ee\uff08Compressed Entry\uff09\u548c\u5206\u5c42\u5143\u6570\u636e\u5b58\u50a8\u65b9\u6848\uff08Hierarchical Metadata Storage scheme\uff09\uff0c\u5e76\u589e\u52a0\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5728\u7ebf\u673a\u5668\u5b66\u4e60\u63a7\u5236\u5668\uff08Online ML Controller\uff09\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u4e2d\u5fc3\u5e94\u7528\u4e2d\u4fdd\u7559\u4e86EIP\u7c7b\u4f3c\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u5e76\u63d0\u9ad8\u4e86ML\u65f6\u4ee3\u7f51\u7edc\u670d\u52a1\u7684\u6548\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6307\u4ee4\u9884\u53d6\u8bbe\u8ba1\u80fd\u591f\u6709\u6548\u5730\u63d0\u9ad8\u4e91\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6548\u7387\uff0c\u5e76\u51cf\u5c11\u5c3e\u90e8\u5ef6\u8fdf\u548c\u80fd\u6e90\u6d88\u8017\u3002"}}
{"id": "2511.04700", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04700", "abs": "https://arxiv.org/abs/2511.04700", "authors": ["Song Wang", "Zihan Chen", "Peng Wang", "Zhepei Wei", "Zhen Tan", "Yu Meng", "Cong Shen", "Jundong Li"], "title": "Separate the Wheat from the Chaff: Winnowing Down Divergent Views in Retrieval Augmented Generation", "comment": "EMNLP Main 2025", "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nintegrating external knowledge sources to address their limitations in\naccessing up-to-date or specialized information. A natural strategy to increase\nthe likelihood of retrieving relevant information is to expand the number of\nretrieved documents. However, involving more documents could introduce\nsignificant noise, as many documents may be irrelevant or misleading, thereby\nreducing the overall accuracy of the generated responses. To overcome the\nchallenge associated with handling a larger number of documents, we propose\nWinnowRAG, a novel RAG framework designed to systematically filter out noisy\ndocuments while preserving valuable content -- a process we refer to as\nwinnowing. WinnowRAG operates in two stages: In Stage I, we perform query-aware\nclustering to group similar documents and form distinct topic clusters. Each\ncluster is assigned to an LLM agent for generating a unique answer. In Stage\nII, we perform winnowing, wherein a critic LLM evaluates the outputs of\nmultiple agents and iteratively separates useful documents from noisy ones. To\nretain useful documents when discarding agents, we propose two strategic\nmerging techniques to ensure that only relevant knowledge is used for\ngenerating the final response. Crucially, WinnowRAG is model-agnostic and does\nnot require any model fine-tuning, making it easily adaptable to various tasks.\nExtensive experiments on various realistic datasets demonstrate the\neffectiveness of WinnowRAG over state-of-the-art baselines.", "AI": {"tldr": "WinnowRAG\u901a\u8fc7\u8fc7\u6ee4\u566a\u58f0\u6587\u6863\u6765\u63d0\u5347\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u6548\u679c\uff0c\u65e0\u9700\u6a21\u578b\u5fae\u8c03\u3002", "motivation": "\u73b0\u6709RAG\u65b9\u6cd5\u5728\u589e\u52a0\u68c0\u7d22\u6587\u6863\u6570\u91cf\u65f6\u5f15\u5165\u566a\u58f0\uff0c\u964d\u4f4e\u751f\u6210\u7ed3\u679c\u7684\u51c6\u786e\u6027\u3002", "method": "WinnowRAG\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u8fdb\u884c\u67e5\u8be2\u611f\u77e5\u805a\u7c7b\uff0c\u5c06\u76f8\u4f3c\u6587\u6863\u5206\u7ec4\u5e76\u7531LLM\u751f\u6210\u7b54\u6848\uff1b\u7b2c\u4e8c\u9636\u6bb5\u8fdb\u884c\u8fc7\u6ee4\uff0c\u901a\u8fc7\u8bc4\u8bbaLLM\u8bc4\u4f30\u4ee3\u7406\u7684\u8f93\u51fa\uff0c\u533a\u5206\u6709\u7528\u548c\u566a\u58f0\u6587\u6863\uff0c\u5e76\u91c7\u7528\u5408\u5e76\u6280\u672f\u4fdd\u7559\u6709\u7528\u77e5\u8bc6\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cWinnowRAG\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002", "conclusion": "WinnowRAG\u662f\u4e00\u79cd\u6709\u6548\u7684RAG\u6846\u67b6\uff0c\u80fd\u591f\u7cfb\u7edf\u5730\u8fc7\u6ee4\u566a\u58f0\u6587\u6863\uff0c\u540c\u65f6\u4fdd\u7559\u6709\u4ef7\u503c\u7684\u5185\u5bb9\u3002"}}
{"id": "2511.04811", "categories": ["cs.CV", "cs.AI", "cs.LG", "68T07, 68U10", "I.2.10; I.4.6; J.3"], "pdf": "https://arxiv.org/pdf/2511.04811", "abs": "https://arxiv.org/abs/2511.04811", "authors": ["Shuo Zhao", "Yu Zhou", "Jianxu Chen"], "title": "An Active Learning Pipeline for Biomedical Image Instance Segmentation with Minimal Human Intervention", "comment": "6 pages, 4 figures, presented at Bildverarbeitung f\\\"ur die Medizin\n  (BVM) 2025, Wiesbaden, Germany", "summary": "Biomedical image segmentation is critical for precise structure delineation\nand downstream analysis. Traditional methods often struggle with noisy data,\nwhile deep learning models such as U-Net have set new benchmarks in\nsegmentation performance. nnU-Net further automates model configuration, making\nit adaptable across datasets without extensive tuning. However, it requires a\nsubstantial amount of annotated data for cross-validation, posing a challenge\nwhen only raw images but no labels are available. Large foundation models offer\nzero-shot generalizability, but may underperform on specific datasets with\nunique characteristics, limiting their direct use for analysis. This work\naddresses these bottlenecks by proposing a data-centric AI workflow that\nleverages active learning and pseudo-labeling to combine the strengths of\ntraditional neural networks and large foundation models while minimizing human\nintervention. The pipeline starts by generating pseudo-labels from a foundation\nmodel, which are then used for nnU-Net's self-configuration. Subsequently, a\nrepresentative core-set is selected for minimal manual annotation, enabling\neffective fine-tuning of the nnU-Net model. This approach significantly reduces\nthe need for manual annotations while maintaining competitive performance,\nproviding an accessible solution for biomedical researchers to apply\nstate-of-the-art AI techniques in their segmentation tasks. The code is\navailable at https://github.com/MMV-Lab/AL_BioMed_img_seg.", "AI": {"tldr": "This paper introduces a data-centric AI workflow for biomedical image segmentation that combines active learning and pseudo-labeling to reduce the need for manual annotations.", "motivation": "Traditional segmentation methods struggle with noisy data, and while deep learning models like U-Net and nnU-Net have improved performance, they require large amounts of annotated data or may underperform on specific datasets. Large foundation models offer zero-shot generalizability but may not be optimal for specialized datasets.", "method": "The proposed workflow generates pseudo-labels from a foundation model for nnU-Net self-configuration, selects a representative core-set for minimal manual annotation, and fine-tunes the nnU-Net model.", "result": "The approach reduces the need for manual annotations while maintaining competitive performance.", "conclusion": "The proposed method provides an accessible solution for biomedical researchers to apply state-of-the-art AI techniques in their segmentation tasks."}}
{"id": "2511.04789", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04789", "abs": "https://arxiv.org/abs/2511.04789", "authors": ["Xiaoda Wang", "Yuji Zhao", "Kaiqiao Han", "Xiao Luo", "Sanne van Rooij", "Jennifer Stevens", "Lifang He", "Liang Zhan", "Yizhou Sun", "Wei Wang", "Carl Yang"], "title": "Conditional Neural ODE for Longitudinal Parkinson's Disease Progression Forecasting", "comment": "Accepted to IEEE International Conference on Bioinformatics and\n  Biomedicine (BIBM) 2025", "summary": "Parkinson's disease (PD) shows heterogeneous, evolving brain-morphometry\npatterns. Modeling these longitudinal trajectories enables mechanistic insight,\ntreatment development, and individualized 'digital-twin' forecasting. However,\nexisting methods usually adopt recurrent neural networks and transformer\narchitectures, which rely on discrete, regularly sampled data while struggling\nto handle irregular and sparse magnetic resonance imaging (MRI) in PD cohorts.\nMoreover, these methods have difficulty capturing individual heterogeneity\nincluding variations in disease onset, progression rate, and symptom severity,\nwhich is a hallmark of PD. To address these challenges, we propose CNODE\n(Conditional Neural ODE), a novel framework for continuous, individualized PD\nprogression forecasting. The core of CNODE is to model morphological brain\nchanges as continuous temporal processes using a neural ODE model. In addition,\nwe jointly learn patient-specific initial time and progress speed to align\nindividual trajectories into a shared progression trajectory. We validate CNODE\non the Parkinson's Progression Markers Initiative (PPMI) dataset. Experimental\nresults show that our method outperforms state-of-the-art baselines in\nforecasting longitudinal PD progression.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7528\u4e8e\u8fde\u7eed\u3001\u4e2a\u4f53\u5316PD\u8fdb\u5c55\u9884\u6d4b\u7684\u6846\u67b6CNODE\uff08\u6761\u4ef6\u795e\u7ecfODE\uff09\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u548cTransformer\u67b6\u6784\uff0c\u8fd9\u4e9b\u67b6\u6784\u4f9d\u8d56\u4e8e\u79bb\u6563\u7684\u3001\u89c4\u5219\u91c7\u6837\u7684\u6570\u636e\uff0c\u540c\u65f6\u96be\u4ee5\u5904\u7406PD\u961f\u5217\u4e2d\u4e0d\u89c4\u5219\u548c\u7a00\u758f\u7684\u78c1\u5171\u632f\u6210\u50cf\uff08MRI\uff09\u3002\u6b64\u5916\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u4e2a\u4f53\u5f02\u8d28\u6027\uff0c\u5305\u62ec\u75be\u75c5\u53d1\u4f5c\u3001\u8fdb\u5c55\u901f\u5ea6\u548c\u75c7\u72b6\u4e25\u91cd\u7a0b\u5ea6\u7684\u53d8\u5316\uff0c\u8fd9\u662fPD\u7684\u6807\u5fd7\u3002", "method": "\u4f7f\u7528\u795e\u7ecfODE\u6a21\u578b\u5c06\u5f62\u6001\u8111\u53d8\u5316\u5efa\u6a21\u4e3a\u8fde\u7eed\u65f6\u95f4\u8fc7\u7a0b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5171\u540c\u5b66\u4e60\u60a3\u8005\u7279\u5b9a\u7684\u521d\u59cb\u65f6\u95f4\u548c\u8fdb\u5c55\u901f\u5ea6\uff0c\u4ee5\u5c06\u4e2a\u4f53\u8f68\u8ff9\u5bf9\u9f50\u5230\u5171\u4eab\u7684\u8fdb\u5c55\u8f68\u8ff9\u4e2d\u3002", "result": "\u5728\u5e15\u91d1\u68ee\u8fdb\u5c55\u6807\u8bb0\u7269\u5021\u8bae\uff08PPMI\uff09\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86CNODE\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u9884\u6d4b\u7eb5\u5411PD\u8fdb\u5c55\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "CNODE\u5728\u9884\u6d4b\u7eb5\u5411PD\u8fdb\u5c55\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3aPD\u7684\u4e2a\u4f53\u5316\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2511.04703", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04703", "abs": "https://arxiv.org/abs/2511.04703", "authors": ["Andrew M. Bean", "Ryan Othniel Kearns", "Angelika Romanou", "Franziska Sofia Hafner", "Harry Mayne", "Jan Batzner", "Negar Foroutan", "Chris Schmitz", "Karolina Korgul", "Hunar Batra", "Oishi Deb", "Emma Beharry", "Cornelius Emde", "Thomas Foster", "Anna Gausen", "Mar\u00eda Grandury", "Simeng Han", "Valentin Hofmann", "Lujain Ibrahim", "Hazel Kim", "Hannah Rose Kirk", "Fangru Lin", "Gabrielle Kaili-May Liu", "Lennart Luettgau", "Jabez Magomere", "Jonathan Rystr\u00f8m", "Anna Sotnikova", "Yushi Yang", "Yilun Zhao", "Adel Bibi", "Antoine Bosselut", "Ronald Clark", "Arman Cohan", "Jakob Foerster", "Yarin Gal", "Scott A. Hale", "Inioluwa Deborah Raji", "Christopher Summerfield", "Philip H. S. Torr", "Cozmin Ududec", "Luc Rocher", "Adam Mahdi"], "title": "Measuring what Matters: Construct Validity in Large Language Model Benchmarks", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Track on Datasets and Benchmarks", "summary": "Evaluating large language models (LLMs) is crucial for both assessing their\ncapabilities and identifying safety or robustness issues prior to deployment.\nReliably measuring abstract and complex phenomena such as 'safety' and\n'robustness' requires strong construct validity, that is, having measures that\nrepresent what matters to the phenomenon. With a team of 29 expert reviewers,\nwe conduct a systematic review of 445 LLM benchmarks from leading conferences\nin natural language processing and machine learning. Across the reviewed\narticles, we find patterns related to the measured phenomena, tasks, and\nscoring metrics which undermine the validity of the resulting claims. To\naddress these shortcomings, we provide eight key recommendations and detailed\nactionable guidance to researchers and practitioners in developing LLM\nbenchmarks.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8bc4\u4f30\u57fa\u51c6\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u56de\u987e\uff0c\u53d1\u73b0\u73b0\u6709\u57fa\u51c6\u5728\u6709\u6548\u6027\u65b9\u9762\u5b58\u5728\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u8bc4\u4f30LLM\u7684\u80fd\u529b\uff0c\u5e76\u5728\u90e8\u7f72\u524d\u8bc6\u522b\u5b89\u5168\u6216\u9c81\u68d2\u6027\u95ee\u9898\u81f3\u5173\u91cd\u8981\u3002\u53ef\u9760\u5730\u6d4b\u91cf\u62bd\u8c61\u548c\u590d\u6742\u7684\u73b0\u8c61\uff08\u5982\u201c\u5b89\u5168\u201d\u548c\u201c\u9c81\u68d2\u6027\u201d\uff09\u9700\u8981\u5f3a\u5927\u7684\u7ed3\u6784\u6548\u5ea6\u3002", "method": "\u4f5c\u8005\u7ec4\u7ec7\u4e8629\u4f4d\u4e13\u5bb6\uff0c\u7cfb\u7edf\u5730\u56de\u987e\u4e86\u6765\u81ea\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u9886\u5148\u4f1a\u8bae\u7684445\u4e2aLLM\u57fa\u51c6\u3002", "result": "\u53d1\u73b0\u73b0\u6709\u57fa\u51c6\u5728\u73b0\u8c61\u6d4b\u91cf\u3001\u4efb\u52a1\u548c\u8bc4\u5206\u6307\u6807\u65b9\u9762\u5b58\u5728\u6a21\u5f0f\uff0c\u8fd9\u4e9b\u6a21\u5f0f\u4f1a\u524a\u5f31\u7ed3\u679c\u58f0\u660e\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u7f3a\u70b9\uff0c\u4f5c\u8005\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u5728\u5f00\u53d1LLM\u57fa\u51c6\u65f6\u63d0\u4f9b\u4e86\u516b\u9879\u5173\u952e\u5efa\u8bae\u548c\u8be6\u7ec6\u7684\u53ef\u64cd\u4f5c\u6307\u5bfc\u3002"}}
{"id": "2511.04848", "categories": ["cs.CV", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.04848", "abs": "https://arxiv.org/abs/2511.04848", "authors": ["Manuel Wei\u00df", "Lukas Baumg\u00e4rtner", "Roland Herzog", "Stephan Schmidt"], "title": "Geometry Denoising with Preferred Normal Vectors", "comment": null, "summary": "We introduce a new paradigm for geometry denoising using prior knowledge\nabout the surface normal vector. This prior knowledge comes in the form of a\nset of preferred normal vectors, which we refer to as label vectors. A\nsegmentation problem is naturally embedded in the denoising process. The\nsegmentation is based on the similarity of the normal vector to the elements of\nthe set of label vectors. Regularization is achieved by a total variation term.\nWe formulate a split Bregman (ADMM) approach to solve the resulting\noptimization problem. The vertex update step is based on second-order shape\ncalculus.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u51e0\u4f55\u53bb\u566a\u8303\u4f8b\uff0c\u8be5\u8303\u4f8b\u4f7f\u7528\u5173\u4e8e\u8868\u9762\u6cd5\u5411\u91cf\u7684\u5148\u9a8c\u77e5\u8bc6\u3002", "motivation": "\u5148\u9a8c\u77e5\u8bc6\u6765\u81ea\u4e00\u7ec4\u9996\u9009\u6cd5\u5411\u91cf\uff08\u79f0\u4e3a\u6807\u7b7e\u5411\u91cf\uff09\u3002", "method": "\u53bb\u566a\u8fc7\u7a0b\u4e2d\u81ea\u7136\u5d4c\u5165\u4e86\u4e00\u4e2a\u5206\u5272\u95ee\u9898\u3002\u5206\u5272\u57fa\u4e8e\u6cd5\u5411\u91cf\u4e0e\u6807\u7b7e\u5411\u91cf\u96c6\u4e2d\u5143\u7d20\u7684\u76f8\u4f3c\u6027\u3002\u901a\u8fc7\u5168\u53d8\u5206\u9879\u5b9e\u73b0\u6b63\u5219\u5316\u3002\u6211\u4eec\u5236\u5b9a\u4e86\u4e00\u4e2a\u5206\u88c2Bregman\uff08ADMM\uff09\u65b9\u6cd5\u6765\u89e3\u51b3\u7531\u6b64\u4ea7\u751f\u7684\u4f18\u5316\u95ee\u9898\u3002\u9876\u70b9\u66f4\u65b0\u6b65\u9aa4\u57fa\u4e8e\u4e8c\u9636\u5f62\u72b6\u5fae\u79ef\u5206\u3002", "result": "\u672a\u63d0\u53ca", "conclusion": "\u672a\u63d0\u53ca"}}
