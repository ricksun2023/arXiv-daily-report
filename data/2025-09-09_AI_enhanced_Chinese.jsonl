{"id": "2509.05564", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.05564", "abs": "https://arxiv.org/abs/2509.05564", "authors": ["Chihiro Yamasaki", "Kai Sugahara", "Kazushi Okamoto"], "title": "Knowledge-Augmented Relation Learning for Complementary Recommendation with Large Language Models", "comment": null, "summary": "Complementary recommendations play a crucial role in e-commerce by enhancing\nuser experience through suggestions of compatible items. Accurate\nclassification of complementary item relationships requires reliable labels,\nbut their creation presents a dilemma. Behavior-based labels are widely used\nbecause they can be easily generated from interaction logs; however, they often\ncontain significant noise and lack reliability. While function-based labels\n(FBLs) provide high-quality definitions of complementary relationships by\ncarefully articulating them based on item functions, their reliance on costly\nmanual annotation severely limits a model's ability to generalize to diverse\nitems. To resolve this trade-off, we propose Knowledge-Augmented Relation\nLearning (KARL), a framework that strategically fuses active learning with\nlarge language models (LLMs). KARL efficiently expands a high-quality FBL\ndataset at a low cost by selectively sampling data points that the classifier\nfinds the most difficult and uses the label extension of the LLM. Our\nexperiments showed that in out-of-distribution (OOD) settings, an unexplored\nitem feature space, KARL improved the baseline accuracy by up to 37%. In\ncontrast, in in-distribution (ID) settings, the learned item feature space, the\nimprovement was less than 0.5%, with prolonged learning could degrade accuracy.\nThese contrasting results are due to the data diversity driven by KARL's\nknowledge expansion, suggesting the need for a dynamic sampling strategy that\nadjusts diversity based on the prediction context (ID or OOD).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKARL\u7684\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e3b\u52a8\u5b66\u4e60\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u6269\u5c55\u9ad8\u8d28\u91cf\u7684\u4e92\u8865\u5173\u7cfb\u6570\u636e\u96c6\uff0c\u4ece\u800c\u63d0\u9ad8\u63a8\u8350\u51c6\u786e\u6027\u3002", "motivation": "\u884c\u4e3a\u6807\u7b7e\u566a\u58f0\u5927\u4e14\u4e0d\u53ef\u9760\uff0c\u529f\u80fd\u6807\u7b7e\u4f9d\u8d56\u4e8e\u6602\u8d35\u7684\u624b\u52a8\u6ce8\u91ca\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u8be5\u6846\u67b6\u7b56\u7565\u6027\u5730\u878d\u5408\u4e86\u4e3b\u52a8\u5b66\u4e60\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u901a\u8fc7\u9009\u62e9\u5206\u7c7b\u5668\u8ba4\u4e3a\u6700\u56f0\u96be\u7684\u6570\u636e\u70b9\u5e76\u4f7f\u7528LLM\u7684\u6807\u7b7e\u6269\u5c55\u6765\u6709\u6548\u5730\u6269\u5c55\u9ad8\u8d28\u91cf\u7684FBL\u6570\u636e\u96c6\u3002", "result": "\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u8bbe\u7f6e\u4e2d\uff0cKARL\u5c06\u57fa\u7ebf\u7cbe\u5ea6\u63d0\u9ad8\u4e86\u9ad8\u8fbe37%\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5728\u5206\u5e03\u5185\uff08ID\uff09\u8bbe\u7f6e\u4e2d\uff0c\u6539\u8fdb\u5c0f\u4e8e0.5%\uff0c\u5e76\u4e14\u957f\u65f6\u95f4\u5b66\u4e60\u53ef\u80fd\u4f1a\u964d\u4f4e\u51c6\u786e\u6027\u3002", "conclusion": "KARL\u7684\u77e5\u8bc6\u6269\u5c55\u9a71\u52a8\u4e86\u6570\u636e\u591a\u6837\u6027\uff0c\u8868\u660e\u9700\u8981\u4e00\u79cd\u52a8\u6001\u91c7\u6837\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u6839\u636e\u9884\u6d4b\u4e0a\u4e0b\u6587\uff08ID\u6216OOD\uff09\u8c03\u6574\u591a\u6837\u6027\u3002"}}
{"id": "2509.05570", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.05570", "abs": "https://arxiv.org/abs/2509.05570", "authors": ["Yipeng Zhang", "Bowen Liu", "Xiaoshuang Zhang", "Aritra Mandal", "Zhe Wu", "Canran Xu"], "title": "LESER: Learning to Expand via Search Engine-feedback Reinforcement in e-Commerce", "comment": null, "summary": "User queries in e-commerce search are often vague, short, and underspecified,\nmaking it difficult for retrieval systems to match them accurately against\nstructured product catalogs. This challenge is amplified by the one-to-many\nnature of user intent, where a single query can imply diverse and competing\nneeds. Existing methods, including neural query expansion and prompting-based\nLLM approaches, fall short in real-world settings: they struggle to capture\nnuanced user intent, often generate outputs that violate platform constraints,\nand rely on workflows that are difficult to scale in production. We propose\nLearning to Expand via Search Engine-feedback Reinforcement (LESER), a novel\nframework that fine-tunes a context-aware LLM using real-time search engine\nfeedback as supervision. LESER formulates query expansion as a retrieval\noptimization task and leverages Group Relative Policy Optimization to learn\ndirectly from relevance and coverage metrics. LESER is trained to reason over\nsearch results and produce high quality query expansions that align with\nplatform rules and retrieval objectives. We evaluate LESER on large-scale,\nreal-world e-commerce datasets, demonstrating substantial improvements in both\noffline and online settings. Our results show that LESER not only enhances\nsemantic coverage and retrieval relevance but also delivers measurable gains in\nuser engagement, making it a practical and scalable solution for modern search\nsystems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a LESER \u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u5b9e\u65f6\u641c\u7d22\u5f15\u64ce\u53cd\u9988\u4f5c\u4e3a\u76d1\u7763\u6765\u5fae\u8c03\u4e0a\u4e0b\u6587\u611f\u77e5 LLM\uff0c\u4ee5\u89e3\u51b3\u7535\u5b50\u5546\u52a1\u641c\u7d22\u4e2d\u7528\u6237\u67e5\u8be2\u6a21\u7cca\u3001\u7b80\u77ed\u548c\u4e0d\u660e\u786e\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u795e\u7ecf\u67e5\u8be2\u6269\u5c55\u548c\u57fa\u4e8e\u63d0\u793a\u7684 LLM \u65b9\u6cd5\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u5b58\u5728\u4e0d\u8db3\uff0c\u96be\u4ee5\u6355\u6349\u7ec6\u5fae\u7684\u7528\u6237\u610f\u56fe\uff0c\u751f\u6210\u7684\u8f93\u51fa\u7ecf\u5e38\u8fdd\u53cd\u5e73\u53f0\u7ea6\u675f\uff0c\u5e76\u4e14\u4f9d\u8d56\u4e8e\u96be\u4ee5\u5728\u751f\u4ea7\u4e2d\u6269\u5c55\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002", "method": "LESER \u5c06\u67e5\u8be2\u6269\u5c55 \u0444\u043e\u0440\u043c\u0443\u043b\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u4e3a\u68c0\u7d22\u4f18\u5316\u4efb\u52a1\uff0c\u5e76\u5229\u7528\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u76f4\u63a5\u4ece\u76f8\u5173\u6027\u548c\u8986\u76d6\u7387\u6307\u6807\u4e2d\u5b66\u4e60\u3002", "result": "\u5728\u5927\u578b\u771f\u5b9e\u7535\u5b50\u5546\u52a1\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cLESER \u5728\u79bb\u7ebf\u548c\u5728\u7ebf\u8bbe\u7f6e\u4e2d\u90fd\u53d6\u5f97\u4e86\u663e\u7740\u6539\u8fdb\u3002LESER \u4e0d\u4ec5\u63d0\u9ad8\u4e86\u8bed\u4e49\u8986\u76d6\u7387\u548c\u68c0\u7d22\u76f8\u5173\u6027\uff0c\u8fd8\u5e26\u6765\u4e86\u7528\u6237\u53c2\u4e0e\u5ea6\u7684\u53ef\u8861\u91cf\u63d0\u5347\u3002", "conclusion": "LESER \u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u73b0\u4ee3\u641c\u7d22\u7cfb\u7edf\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.05750", "categories": ["cs.IR", "cs.DB", "cs.DS", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.05750", "abs": "https://arxiv.org/abs/2509.05750", "authors": ["Ilias Azizi", "Karima Echihab", "Themis Palpanas", "Vassilis Christophides"], "title": "Toward Efficient and Scalable Design of In-Memory Graph-Based Vector Search", "comment": "Presented at ICML 2025 VecDB Workshop; an extended version appeared\n  in ACM SIGMOD 2025 ('Graph-Based Vector Search: An Experimental Evaluation of\n  the State-of-the-Art')", "summary": "Vector data is prevalent across business and scientific applications, and its\npopularity is growing with the proliferation of learned embeddings. Vector data\ncollections often reach billions of vectors with thousands of dimensions, thus,\nincreasing the complexity of their analysis. Vector search is the backbone of\nmany critical analytical tasks, and graph-based methods have become the best\nchoice for analytical tasks that do not require guarantees on the quality of\nthe answers. Although several paradigms (seed selection, incremental insertion,\nneighborhood propagation, neighborhood diversification, and divide-and-conquer)\nhave been employed to design in-memory graph-based vector search algorithms, a\nsystematic comparison of the key algorithmic advances is still missing. We\nconduct an exhaustive experimental evaluation of twelve state-of-the-art\nmethods on seven real data collections, with sizes up to 1 billion vectors. We\nshare key insights about the strengths and limitations of these methods; e.g.,\nthe best approaches are typically based on incremental insertion and\nneighborhood diversification, and the choice of the base graph can hurt\nscalability. Finally, we discuss open research directions, such as the\nimportance of devising more sophisticated data adaptive seed selection and\ndiversification strategies.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5411\u91cf\u641c\u7d22\u7b97\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u6bd4\u8f83\u4e86\u5341\u4e8c\u79cd\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u5728\u4e03\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u63a2\u8ba8\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5411\u91cf\u6570\u636e\u5728\u5546\u4e1a\u548c\u79d1\u5b66\u5e94\u7528\u4e2d\u8d8a\u6765\u8d8a\u666e\u904d\uff0c\u5176\u5206\u6790\u4e5f\u53d8\u5f97\u8d8a\u6765\u8d8a\u590d\u6742\u3002\u5411\u91cf\u641c\u7d22\u662f\u8bb8\u591a\u5173\u952e\u5206\u6790\u4efb\u52a1\u7684\u652f\u67f1\uff0c\u57fa\u4e8e\u56fe\u7684\u65b9\u6cd5\u5df2\u6210\u4e3a\u5206\u6790\u4efb\u52a1\u7684\u6700\u4f73\u9009\u62e9\u3002", "method": "\u672c\u6587\u5bf9\u5341\u4e8c\u79cd\u6700\u5148\u8fdb\u7684\u5411\u91cf\u641c\u7d22\u65b9\u6cd5\u5728\u4e03\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8be6\u5c3d\u7684\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6700\u597d\u7684\u65b9\u6cd5\u901a\u5e38\u57fa\u4e8e\u589e\u91cf\u63d2\u5165\u548c\u90bb\u57df\u591a\u6837\u5316\uff0c\u5e76\u4e14\u57fa\u672c\u56fe\u7684\u9009\u62e9\u4f1a\u5f71\u54cd\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u672c\u6587\u603b\u7ed3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u5e76\u8ba8\u8bba\u4e86\u5f00\u653e\u7684\u7814\u7a76\u65b9\u5411\uff0c\u4f8b\u5982\u8bbe\u8ba1\u66f4\u590d\u6742\u7684\u6570\u636e\u81ea\u9002\u5e94\u79cd\u5b50\u9009\u62e9\u548c\u591a\u6837\u5316\u7b56\u7565\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.06002", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.06002", "abs": "https://arxiv.org/abs/2509.06002", "authors": ["Kuan Zou", "Aixin Sun"], "title": "A Survey of Real-World Recommender Systems: Challenges, Constraints, and Industrial Perspectives", "comment": "Working paper", "summary": "Recommender systems have generated tremendous value for both users and\nbusinesses, drawing significant attention from academia and industry alike.\nHowever, due to practical constraints, academic research remains largely\nconfined to offline dataset optimizations, lacking access to real user data and\nlarge-scale recommendation platforms. This limitation reduces practical\nrelevance, slows technological progress, and hampers a full understanding of\nthe key challenges in recommender systems. In this survey, we provide a\nsystematic review of industrial recommender systems and contrast them with\ntheir academic counterparts. We highlight key differences in data scale,\nreal-time requirements, and evaluation methodologies, and we summarize major\nreal-world recommendation scenarios along with their associated challenges. We\nthen examine how industry practitioners address these challenges in\nTransaction-Oriented Recommender Systems and Content-Oriented Recommender\nSystems, a new classification grounded in item characteristics and\nrecommendation objectives. Finally, we outline promising research directions,\nincluding the often-overlooked role of user decision-making, the integration of\neconomic and psychological theories, and concrete suggestions for advancing\nacademic research. Our goal is to enhance academia's understanding of practical\nrecommender systems, bridge the growing development gap, and foster stronger\ncollaboration between industry and academia.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5de5\u4e1a\u63a8\u8350\u7cfb\u7edf\uff0c\u5bf9\u6bd4\u4e86\u5b66\u672f\u754c\u7684\u7814\u7a76\uff0c\u65e8\u5728\u5f25\u5408\u4e24\u8005\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4fc3\u8fdb\u5408\u4f5c\u3002", "motivation": "\u5b66\u672f\u7814\u7a76\u53d7\u9650\u4e8e\u6570\u636e\u548c\u5e73\u53f0\uff0c\u7f3a\u4e4f\u5b9e\u9645\u5e94\u7528\uff0c\u65e0\u6cd5\u5145\u5206\u7406\u89e3\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u7cfb\u7edf\u6027\u5730\u56de\u987e\u5de5\u4e1a\u63a8\u8350\u7cfb\u7edf\uff0c\u5e76\u4e0e\u5b66\u672f\u754c\u7684\u7cfb\u7edf\u8fdb\u884c\u5bf9\u6bd4\uff0c\u603b\u7ed3\u5b9e\u9645\u5e94\u7528\u573a\u666f\u53ca\u5176\u6311\u6218\uff0c\u5e76\u6839\u636eitem\u7279\u5f81\u548c\u63a8\u8350\u76ee\u6807\uff0c\u5c06\u5de5\u4e1a\u63a8\u8350\u7cfb\u7edf\u5206\u4e3a\u9762\u5411\u4ea4\u6613\u548c\u9762\u5411\u5185\u5bb9\u4e24\u7c7b\u3002", "result": "\u5f3a\u8c03\u4e86\u6570\u636e\u89c4\u6a21\u3001\u5b9e\u65f6\u8981\u6c42\u548c\u8bc4\u4f30\u65b9\u6cd5\u7684\u4e3b\u8981\u5dee\u5f02\uff0c\u5e76\u6982\u8ff0\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u5982\u4f55\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "conclusion": "\u6982\u8ff0\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5305\u62ec\u7528\u6237\u51b3\u7b56\u3001\u7ecf\u6d4e\u548c\u5fc3\u7406\u7406\u8bba\u7684\u6574\u5408\uff0c\u4ee5\u53ca\u6539\u8fdb\u5b66\u672f\u7814\u7a76\u7684\u5177\u4f53\u5efa\u8bae\u3002"}}
{"id": "2509.06044", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2509.06044", "abs": "https://arxiv.org/abs/2509.06044", "authors": ["Lingxiao Kong", "Apostolos Sarris", "Miltiadis Polidorou", "Victor Klingenberg", "Vasilis Sevetlidis", "Vasilis Arampatzakis", "George Pavlidis", "Cong Yang", "Zeyd Boukhers"], "title": "A Unified Framework for Cultural Heritage Data Historicity and Migration: The ARGUS Approach", "comment": "Accepted for publication at the IEEE International Conference on\n  Cyber Humanities (2025)", "summary": "Cultural heritage preservation faces significant challenges in managing\ndiverse, multi-source, and multi-scale data for effective monitoring and\nconservation. This paper documents a comprehensive data historicity and\nmigration framework implemented within the ARGUS project, which addresses the\ncomplexities of processing heterogeneous cultural heritage data. We describe a\nsystematic data processing pipeline encompassing standardization, enrichment,\nintegration, visualization, ingestion, and publication strategies. The\nframework transforms raw, disparate datasets into standardized formats\ncompliant with FAIR principles. It enhances sparse datasets through established\nimputation techniques, ensures interoperability through database integration,\nand improves querying capabilities through LLM-powered natural language\nprocessing. This approach has been applied across five European pilot sites\nwith varying preservation challenges, demonstrating its adaptability to diverse\ncultural heritage contexts. The implementation results show improved data\naccessibility, enhanced analytical capabilities, and more effective\ndecision-making for conservation efforts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6587\u5316\u9057\u4ea7\u6570\u636e\u7ba1\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u591a\u6837\u3001\u591a\u6e90\u548c\u591a\u5c3a\u5ea6\u7684\u6587\u5316\u9057\u4ea7\u6570\u636e\uff0c\u4ee5\u5b9e\u73b0\u6709\u6548\u7684\u76d1\u6d4b\u548c\u4fdd\u62a4\u3002", "motivation": "\u6587\u5316\u9057\u4ea7\u4fdd\u62a4\u5728\u7ba1\u7406\u591a\u6837\u6027\u3001\u591a\u6765\u6e90\u548c\u591a\u5c3a\u5ea6\u7684\u6587\u5316\u9057\u4ea7\u6570\u636e\u65b9\u9762\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002", "method": "\u8be5\u6846\u67b6\u5305\u62ec\u6807\u51c6\u5316\u3001\u4e30\u5bcc\u3001\u96c6\u6210\u3001\u53ef\u89c6\u5316\u3001\u6444\u53d6\u548c\u53d1\u5e03\u7b56\u7565\u7684\u7cfb\u7edf\u6570\u636e\u5904\u7406\u6d41\u7a0b\uff0c\u5229\u7528\u6570\u636e\u63d2\u8865\u6280\u672f\u3001\u6570\u636e\u5e93\u96c6\u6210\u548cLLM\u9a71\u52a8\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3002", "result": "\u8be5\u65b9\u6cd5\u5df2\u5e94\u7528\u4e8e\u4e94\u4e2a\u6b27\u6d32\u8bd5\u70b9\u5730\u70b9\uff0c\u7ed3\u679c\u8868\u660e\u6570\u636e\u53ef\u8bbf\u95ee\u6027\u5f97\u5230\u6539\u5584\uff0c\u5206\u6790\u80fd\u529b\u5f97\u5230\u589e\u5f3a\uff0c\u5e76\u4e3a\u4fdd\u62a4\u5de5\u4f5c\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u51b3\u7b56\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u7684\u6587\u5316\u9057\u4ea7\u73af\u5883\uff0c\u5e76\u63d0\u9ad8\u4e86\u6570\u636e\u7ba1\u7406\u548c\u4fdd\u62a4\u5de5\u4f5c\u7684\u6548\u7387\u3002"}}
{"id": "2509.05359", "categories": ["cs.CL", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.05359", "abs": "https://arxiv.org/abs/2509.05359", "authors": ["Yanis Labrak", "Richard Dufour", "Micka\u00ebl Rouvier"], "title": "An Empirical Analysis of Discrete Unit Representations in Speech Language Modeling Pre-training", "comment": "Published in International Conference on Text, Speech, and Dialogue,\n  13-24", "summary": "This paper investigates discrete unit representations in Speech Language\nModels (SLMs), focusing on optimizing speech modeling during continual\npre-training. In this paper, we systematically examine how model architecture,\ndata representation, and training robustness influence the pre-training stage\nin which we adapt existing pre-trained language models to the speech modality.\nOur experiments highlight the role of speech encoders and clustering\ngranularity across different model scales, showing how optimal discretization\nstrategies vary with model capacity. By examining cluster distribution and\nphonemic alignments, we investigate the effective use of discrete vocabulary,\nuncovering both linguistic and paralinguistic patterns. Additionally, we\nexplore the impact of clustering data selection on model robustness,\nhighlighting the importance of domain matching between discretization training\nand target applications.", "AI": {"tldr": "\u7814\u7a76\u8bed\u97f3\u8bed\u8a00\u6a21\u578b(SLM)\u4e2d\u7684\u79bb\u6563\u5355\u5143\u8868\u793a\uff0c\u4e13\u6ce8\u4e8e\u4f18\u5316\u6301\u7eed\u9884\u8bad\u7ec3\u671f\u95f4\u7684\u8bed\u97f3\u5efa\u6a21\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u4f18\u5316\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u5728\u6301\u7eed\u9884\u8bad\u7ec3\u671f\u95f4\u7684\u8bed\u97f3\u5efa\u6a21\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\uff0c\u7cfb\u7edf\u5730\u68c0\u67e5\u6a21\u578b\u67b6\u6784\u3001\u6570\u636e\u8868\u793a\u548c\u8bad\u7ec3\u9c81\u68d2\u6027\u5982\u4f55\u5f71\u54cd\u9884\u8bad\u7ec3\u9636\u6bb5\uff0c\u8c03\u6574\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4ee5\u9002\u5e94\u8bed\u97f3\u6a21\u6001\u3002\u8003\u5bdf\u4e86\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u4e0b\u8bed\u97f3\u7f16\u7801\u5668\u548c\u805a\u7c7b\u7c92\u5ea6\u7684\u4f5c\u7528\uff0c\u5c55\u793a\u4e86\u6700\u4f73\u79bb\u6563\u5316\u7b56\u7565\u5982\u4f55\u968f\u6a21\u578b\u5bb9\u91cf\u53d8\u5316\u3002\u901a\u8fc7\u68c0\u67e5\u805a\u7c7b\u5206\u5e03\u548c\u97f3\u7d20\u5bf9\u9f50\uff0c\u7814\u7a76\u4e86\u79bb\u6563\u8bcd\u6c47\u7684\u6709\u6548\u4f7f\u7528\uff0c\u63ed\u793a\u4e86\u8bed\u8a00\u548c\u526f\u8bed\u8a00\u6a21\u5f0f\u3002\u6b64\u5916\uff0c\u8fd8\u63a2\u8ba8\u4e86\u805a\u7c7b\u6570\u636e\u9009\u62e9\u5bf9\u6a21\u578b\u9c81\u68d2\u6027\u7684\u5f71\u54cd\uff0c\u5f3a\u8c03\u4e86\u79bb\u6563\u5316\u8bad\u7ec3\u548c\u76ee\u6807\u5e94\u7528\u4e4b\u95f4\u9886\u57df\u5339\u914d\u7684\u91cd\u8981\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u5f3a\u8c03\u4e86\u8bed\u97f3\u7f16\u7801\u5668\u548c\u805a\u7c7b\u7c92\u5ea6\u7684\u4f5c\u7528\uff0c\u5c55\u793a\u4e86\u6700\u4f73\u79bb\u6563\u5316\u7b56\u7565\u5982\u4f55\u968f\u6a21\u578b\u5bb9\u91cf\u53d8\u5316\u3002\u63ed\u793a\u4e86\u8bed\u8a00\u548c\u526f\u8bed\u8a00\u6a21\u5f0f\u3002\u5f3a\u8c03\u4e86\u79bb\u6563\u5316\u8bad\u7ec3\u548c\u76ee\u6807\u5e94\u7528\u4e4b\u95f4\u9886\u57df\u5339\u914d\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u7ed3\u8bba\u662f\u79bb\u6563\u5355\u5143\u8868\u793a\u5728\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u4e14\u6700\u4f73\u79bb\u6563\u5316\u7b56\u7565\u968f\u6a21\u578b\u5bb9\u91cf\u53d8\u5316\uff0c\u9886\u57df\u5339\u914d\u5bf9\u4e8e\u6a21\u578b\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.05323", "categories": ["cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.05323", "abs": "https://arxiv.org/abs/2509.05323", "authors": ["Adam Cole", "Mick Grierson"], "title": "Attention of a Kiss: Exploring Attention Maps in Video Diffusion for XAIxArts", "comment": "3rd international workshop on eXplainable AI for the Arts (XAIxArts)\n  at the ACM Creativity and Cognition Conference 2025", "summary": "This paper presents an artistic and technical investigation into the\nattention mechanisms of video diffusion transformers. Inspired by early video\nartists who manipulated analog video signals to create new visual aesthetics,\nthis study proposes a method for extracting and visualizing cross-attention\nmaps in generative video models. Built on the open-source Wan model, our tool\nprovides an interpretable window into the temporal and spatial behavior of\nattention in text-to-video generation. Through exploratory probes and an\nartistic case study, we examine the potential of attention maps as both\nanalytical tools and raw artistic material. This work contributes to the\ngrowing field of Explainable AI for the Arts (XAIxArts), inviting artists to\nreclaim the inner workings of AI as a creative medium.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u89c6\u9891\u6269\u6563Transformer\u4e2d\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u63d0\u53d6\u548c\u53ef\u89c6\u5316\u751f\u6210\u89c6\u9891\u6a21\u578b\u4e2d\u7684\u4e92\u6ce8\u610f\u529b\u56fe\uff0c\u4e3a\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u4e2d\u7684\u6ce8\u610f\u529b\u884c\u4e3a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u7a97\u53e3\u3002", "motivation": "\u53d7\u65e9\u671f\u89c6\u9891\u827a\u672f\u5bb6\u542f\u53d1\uff0c\u4ed6\u4eec\u901a\u8fc7\u64cd\u7eb5\u6a21\u62df\u89c6\u9891\u4fe1\u53f7\u6765\u521b\u9020\u65b0\u7684\u89c6\u89c9\u7f8e\u5b66\u3002\u7814\u7a76\u76ee\u7684\u662f\u63a2\u7d22\u6ce8\u610f\u529b\u56fe\u4f5c\u4e3a\u5206\u6790\u5de5\u5177\u548c\u539f\u59cb\u827a\u672f\u6750\u6599\u7684\u6f5c\u529b\u3002", "method": "\u57fa\u4e8e\u5f00\u6e90Wan\u6a21\u578b\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u5de5\u5177\uff0c\u7528\u4e8e\u63d0\u53d6\u548c\u53ef\u89c6\u5316\u4e92\u6ce8\u610f\u529b\u56fe\u3002", "result": "\u901a\u8fc7\u63a2\u7d22\u6027\u63a2\u9488\u548c\u827a\u672f\u6848\u4f8b\u7814\u7a76\uff0c\u68c0\u9a8c\u4e86\u6ce8\u610f\u529b\u56fe\u7684\u6f5c\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u6709\u52a9\u4e8e\u4e0d\u65ad\u53d1\u5c55\u7684\u827a\u672f\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u9886\u57df\uff08XAIxArts\uff09\uff0c\u9080\u8bf7\u827a\u672f\u5bb6\u5c06\u4eba\u5de5\u667a\u80fd\u7684\u5185\u90e8\u8fd0\u4f5c\u91cd\u65b0\u7528\u4f5c\u4e00\u79cd\u521b\u9020\u6027\u5a92\u4ecb\u3002"}}
{"id": "2509.05307", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05307", "abs": "https://arxiv.org/abs/2509.05307", "authors": ["Sachin Chhabra", "Hemanth Venkateswara", "Baoxin Li"], "title": "Label Smoothing++: Enhanced Label Regularization for Training Neural Networks", "comment": "Published in British Machine Vision Conference (BMVC), 2024", "summary": "Training neural networks with one-hot target labels often results in\noverconfidence and overfitting. Label smoothing addresses this issue by\nperturbing the one-hot target labels by adding a uniform probability vector to\ncreate a regularized label. Although label smoothing improves the network's\ngeneralization ability, it assigns equal importance to all the non-target\nclasses, which destroys the inter-class relationships. In this paper, we\npropose a novel label regularization training strategy called Label\nSmoothing++, which assigns non-zero probabilities to non-target classes and\naccounts for their inter-class relationships. Our approach uses a fixed label\nfor the target class while enabling the network to learn the labels associated\nwith non-target classes. Through extensive experiments on multiple datasets, we\ndemonstrate how Label Smoothing++ mitigates overconfident predictions while\npromoting inter-class relationships and generalization capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Label Smoothing++ \u7684\u65b0\u578b\u6807\u7b7e\u5e73\u6ed1\u7b56\u7565\uff0c\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u6807\u7b7e\u5e73\u6ed1\u5ffd\u7565\u7c7b\u522b\u95f4\u5173\u7cfb\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfone-hot\u6807\u7b7e\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u5bb9\u6613\u5bfc\u81f4\u8fc7\u62df\u5408\u548c\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u6807\u7b7e\u5e73\u6ed1\u867d\u7136\u53ef\u4ee5\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u4f1a\u7834\u574f\u7c7b\u522b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u672c\u6587\u63d0\u51fa\u7684 Label Smoothing++ \u7b56\u7565\uff0c\u4e3a\u975e\u76ee\u6807\u7c7b\u522b\u5206\u914d\u975e\u96f6\u6982\u7387\uff0c\u5e76\u8003\u8651\u7c7b\u522b\u95f4\u7684\u5173\u7cfb\u3002\u8be5\u65b9\u6cd5\u56fa\u5b9a\u76ee\u6807\u7c7b\u522b\u7684\u6807\u7b7e\uff0c\u5e76\u4f7f\u7f51\u7edc\u80fd\u591f\u5b66\u4e60\u4e0e\u975e\u76ee\u6807\u7c7b\u522b\u76f8\u5173\u7684\u6807\u7b7e\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cLabel Smoothing++ \u53ef\u4ee5\u7f13\u89e3\u8fc7\u5ea6\u81ea\u4fe1\u7684\u9884\u6d4b\uff0c\u540c\u65f6\u4fc3\u8fdb\u7c7b\u95f4\u5173\u7cfb\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Label Smoothing++ \u662f\u4e00\u79cd\u6709\u6548\u7684\u6807\u7b7e\u6b63\u5219\u5316\u8bad\u7ec3\u7b56\u7565\uff0c\u53ef\u4ee5\u5728\u63d0\u9ad8\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u7c7b\u522b\u95f4\u7684\u5173\u7cfb\u3002"}}
{"id": "2509.05316", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05316", "abs": "https://arxiv.org/abs/2509.05316", "authors": ["Praveen Bushipaka", "Lucia Passaro", "Tommaso Cucinotta"], "title": "Standard vs. Modular Sampling: Best Practices for Reliable LLM Unlearning", "comment": null, "summary": "A conventional LLM Unlearning setting consists of two subsets -\"forget\" and\n\"retain\", with the objectives of removing the undesired knowledge from the\nforget set while preserving the remaining knowledge from the retain. In\nprivacy-focused unlearning research, a retain set is often further divided into\nneighbor sets, containing either directly or indirectly connected to the forget\ntargets; and augmented by a general-knowledge set. A common practice in\nexisting benchmarks is to employ only a single neighbor set, with general\nknowledge which fails to reflect the real-world data complexities and\nrelationships. LLM Unlearning typically involves 1:1 sampling or cyclic\niteration sampling. However, the efficacy and stability of these de facto\nstandards have not been critically examined. In this study, we systematically\nevaluate these common practices. Our findings reveal that relying on a single\nneighbor set is suboptimal and that a standard sampling approach can obscure\nperformance trade-offs. Based on this analysis, we propose and validate an\ninitial set of best practices: (1) Incorporation of diverse neighbor sets to\nbalance forget efficacy and model utility, (2) Standard 1:1 sampling methods\nare inefficient and yield poor results, (3) Our proposed Modular Entity-Level\nUnlearning (MELU) strategy as an alternative to cyclic sampling. We demonstrate\nthat this modular approach, combined with robust algorithms, provides a clear\nand stable path towards effective unlearning.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u975e\u5b66\u4e60\uff08Unlearning\uff09\u65b9\u6cd5\uff0c\u53d1\u73b0\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u548c\u62bd\u6837\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u6700\u4f73\u5b9e\u8df5\u3002", "motivation": "\u73b0\u6709LLM\u975e\u5b66\u4e60\u7814\u7a76\u672a\u80fd\u5145\u5206\u53cd\u6620\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u7684\u590d\u6742\u6027\u548c\u5173\u7cfb\uff0c\u4e14\u5e38\u7528\u62bd\u6837\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u7a33\u5b9a\u6027\u672a\u7ecf\u4e25\u683c\u68c0\u9a8c\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5355\u90bb\u5c45\u96c6\u7684\u4f7f\u7528\u4ee5\u53ca1:1\u62bd\u6837\u548c\u5faa\u73af\u8fed\u4ee3\u62bd\u6837\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u7684\u5b9e\u4f53\u7ea7\u522b\u975e\u5b66\u4e60\uff08MELU\uff09\u7b56\u7565\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u4f9d\u8d56\u5355\u4e00\u90bb\u5c45\u96c6\u662f\u6b21\u4f18\u7684\uff0c\u6807\u51c6\u62bd\u6837\u65b9\u6cd5\u4f1a\u63a9\u76d6\u6027\u80fd\u6743\u8861\u3002MELU\u7b56\u7565\u7ed3\u5408\u7a33\u5065\u7684\u7b97\u6cd5\uff0c\u4e3a\u6709\u6548\u7684\u975e\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u6761\u6e05\u6670\u4e14\u7a33\u5b9a\u7684\u8def\u5f84\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u4e00\u7cfb\u5217\u6700\u4f73\u5b9e\u8df5\uff0c\u5305\u62ec\u6574\u5408\u591a\u6837\u5316\u7684\u90bb\u5c45\u96c6\u4ee5\u5e73\u8861\u975e\u5b66\u4e60\u6548\u679c\u548c\u6a21\u578b\u6548\u7528\uff0c\u4ee5\u53ca\u4f7f\u7528MELU\u7b56\u7565\u66ff\u4ee3\u4f4e\u6548\u76841:1\u62bd\u6837\u65b9\u6cd5\u3002"}}
{"id": "2509.06185", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06185", "abs": "https://arxiv.org/abs/2509.06185", "authors": ["Firas Jarboui", "Issa Memari"], "title": "Modeling shopper interest broadness with entropy-driven dialogue policy in the context of arbitrarily large product catalogs", "comment": null, "summary": "Conversational recommender systems promise rich interactions for e-commerce,\nbut balancing exploration (clarifying user needs) and exploitation (making\nrecommendations) remains challenging, especially when deploying large language\nmodels (LLMs) with vast product catalogs. We address this challenge by modeling\nthe breadth of user interest via the entropy of retrieval score distributions.\nOur method uses a neural retriever to fetch relevant items for a user query and\ncomputes the entropy of the re-ranked scores to dynamically route the dialogue\npolicy: low-entropy (specific) queries trigger direct recommendations, whereas\nhigh-entropy (ambiguous) queries prompt exploratory questions. This simple yet\neffective strategy allows an LLM-driven agent to remain aware of an arbitrarily\nlarge catalog in real-time without bloating its context window.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u68c0\u7d22\u8bc4\u5206\u5206\u5e03\u7684\u71b5\u6765\u5e73\u8861\u5bf9\u8bdd\u5f0f\u63a8\u8350\u7cfb\u7edf\u4e2d\u63a2\u7d22\u548c\u5229\u7528\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5141\u8bb8LLM\u9a71\u52a8\u7684\u4ee3\u7406\u5b9e\u65f6\u4e86\u89e3\u4efb\u610f\u5927\u7684\u4ea7\u54c1\u76ee\u5f55\uff0c\u800c\u65e0\u9700\u6269\u5927\u5176\u4e0a\u4e0b\u6587\u7a97\u53e3\u3002", "motivation": "\u5728\u5bf9\u8bdd\u5f0f\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0c\u5e73\u8861\u63a2\u7d22\uff08\u6f84\u6e05\u7528\u6237\u9700\u6c42\uff09\u548c\u5229\u7528\uff08\u63d0\u51fa\u5efa\u8bae\uff09\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u5728\u4f7f\u7528\u5177\u6709\u5927\u91cf\u4ea7\u54c1\u76ee\u5f55\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u65f6\u3002", "method": "\u8be5\u65b9\u6cd5\u4f7f\u7528\u795e\u7ecf\u68c0\u7d22\u5668\u6765\u83b7\u53d6\u4e0e\u7528\u6237\u67e5\u8be2\u76f8\u5173\u7684\u9879\u76ee\uff0c\u5e76\u8ba1\u7b97\u91cd\u65b0\u6392\u5e8f\u7684\u5206\u6570\u7684\u71b5\uff0c\u4ee5\u52a8\u6001\u5730\u8def\u7531\u5bf9\u8bdd\u7b56\u7565\uff1a\u4f4e\u71b5\uff08\u7279\u5b9a\uff09\u67e5\u8be2\u89e6\u53d1\u76f4\u63a5\u63a8\u8350\uff0c\u800c\u9ad8\u71b5\uff08\u6a21\u7cca\uff09\u67e5\u8be2\u63d0\u793a\u63a2\u7d22\u6027\u95ee\u9898\u3002", "result": "\u8be5\u7b56\u7565\u5141\u8bb8LLM\u9a71\u52a8\u7684\u4ee3\u7406\u5b9e\u65f6\u4e86\u89e3\u4efb\u610f\u5927\u7684\u76ee\u5f55\uff0c\u800c\u65e0\u9700\u6269\u5927\u5176\u4e0a\u4e0b\u6587\u7a97\u53e3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5bf9\u68c0\u7d22\u5206\u6570\u5206\u5e03\u7684\u71b5\u8fdb\u884c\u5efa\u6a21\u6765\u89e3\u51b3\u5bf9\u8bdd\u5f0f\u63a8\u8350\u7cfb\u7edf\u4e2d\u63a2\u7d22\u548c\u5229\u7528\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\u3002"}}
{"id": "2509.06093", "categories": ["cs.DB", "cond-mat.mtrl-sci", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06093", "abs": "https://arxiv.org/abs/2509.06093", "authors": ["Yuze Liu", "Zhaoyuan Zhang", "Xiangsheng Zeng", "Yihe Zhang", "Leping Yu", "Lejia Wang", "Xi Yu"], "title": "Language Native Lightly Structured Databases for Large Language Model Driven Composite Materials Research", "comment": null, "summary": "Chemical and materials research has traditionally relied heavily on knowledge\nnarrative, with progress often driven by language-based descriptions of\nprinciples, mechanisms, and experimental experiences, rather than tables,\nlimiting what conventional databases and ML can exploit. We present a\nlanguage-native database for boron nitride nanosheet (BNNS) polymer thermally\nconductive composites that captures lightly structured information from papers\nacross preparation, characterization, theory-computation, and mechanistic\nreasoning, with evidence-linked snippets. Records are organized in a\nheterogeneous database and queried via composite retrieval with semantics, key\nwords and value filters. The system can synthesizes literature into accurate,\nverifiable, and expert style guidance. This substrate enables high fidelity\nefficient Retrieval Augmented Generation (RAG) and tool augmented agents to\ninterleave retrieval with reasoning and deliver actionable SOP. The framework\nsupplies the language rich foundation required for LLM-driven materials\ndiscovery.", "AI": {"tldr": "\u672c\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u7528\u4e8e\u6c2e\u5316\u787c\u7eb3\u7c73\u7247\uff08BNNS\uff09\u805a\u5408\u7269\u5bfc\u70ed\u590d\u5408\u6750\u6599\u7684\u8bed\u8a00\u539f\u751f\u6570\u636e\u5e93\uff0c\u8be5\u6570\u636e\u5e93\u4ece\u8bba\u6587\u4e2d\u6355\u83b7\u8f7b\u7ed3\u6784\u5316\u4fe1\u606f\u3002", "motivation": "\u4f20\u7edf\u5316\u5b66\u548c\u6750\u6599\u7814\u7a76\u4e25\u91cd\u4f9d\u8d56\u77e5\u8bc6\u53d9\u8ff0\uff0c\u8fdb\u6b65\u901a\u5e38\u7531\u57fa\u4e8e\u8bed\u8a00\u7684\u539f\u7406\u3001\u673a\u5236\u548c\u5b9e\u9a8c\u7ecf\u9a8c\u63cf\u8ff0\u9a71\u52a8\uff0c\u800c\u975e\u8868\u683c\uff0c\u9650\u5236\u4e86\u4f20\u7edf\u6570\u636e\u5e93\u548c\u673a\u5668\u5b66\u4e60\u7684\u5229\u7528\u3002", "method": "\u901a\u8fc7\u590d\u5408\u68c0\u7d22\uff08\u8bed\u4e49\u3001\u5173\u952e\u8bcd\u548c\u6570\u503c\u8fc7\u6ee4\u5668\uff09\u67e5\u8be2\u5f02\u6784\u6570\u636e\u5e93\uff0c\u5c06\u6587\u732e\u5408\u6210\u4e3a\u51c6\u786e\u3001\u53ef\u9a8c\u8bc1\u548c\u4e13\u5bb6\u98ce\u683c\u7684\u6307\u5bfc\u3002", "result": "\u8be5\u7cfb\u7edf\u80fd\u591f\u5408\u6210\u6587\u732e\u4e3a\u51c6\u786e\u3001\u53ef\u9a8c\u8bc1\u548c\u4e13\u5bb6\u98ce\u683c\u7684\u6307\u5bfc\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aLLM\u9a71\u52a8\u7684\u6750\u6599\u53d1\u73b0\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u8bed\u8a00\u57fa\u7840\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u9ad8\u6548\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u5de5\u5177\u589e\u5f3a\u4ee3\u7406\uff0c\u4ee5\u4ea4\u9519\u68c0\u7d22\u4e0e\u63a8\u7406\u5e76\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u6807\u51c6\u64cd\u4f5c\u7a0b\u5e8f\uff08SOP\uff09\u3002"}}
{"id": "2509.05360", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05360", "abs": "https://arxiv.org/abs/2509.05360", "authors": ["Jerry Li", "Evangelos Papalexakis"], "title": "Beyond ROUGE: N-Gram Subspace Features for LLM Hallucination Detection", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated effectiveness across a wide\nvariety of tasks involving natural language, however, a fundamental problem of\nhallucinations still plagues these models, limiting their trustworthiness in\ngenerating consistent, truthful information. Detecting hallucinations has\nquickly become an important topic, with various methods such as uncertainty\nestimation, LLM Judges, retrieval augmented generation (RAG), and consistency\nchecks showing promise. Many of these methods build upon foundational metrics,\nsuch as ROUGE, BERTScore, or Perplexity, which often lack the semantic depth\nnecessary to detect hallucinations effectively. In this work, we propose a\nnovel approach inspired by ROUGE that constructs an N-Gram frequency tensor\nfrom LLM-generated text. This tensor captures richer semantic structure by\nencoding co-occurrence patterns, enabling better differentiation between\nfactual and hallucinated content. We demonstrate this by applying tensor\ndecomposition methods to extract singular values from each mode and use these\nas input features to train a multi-layer perceptron (MLP) binary classifier for\nhallucinations. Our method is evaluated on the HaluEval dataset and\ndemonstrates significant improvements over traditional baselines, as well as\ncompetitive performance against state-of-the-art LLM judges.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u68c0\u6d4b\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5e7b\u89c9\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8e N-Gram \u9891\u7387\u5f20\u91cf\uff0c\u65e8\u5728\u6355\u6349\u66f4\u4e30\u5bcc\u7684\u8bed\u4e49\u7ed3\u6784\uff0c\u4ece\u800c\u66f4\u597d\u5730\u533a\u5206\u4e8b\u5b9e\u5185\u5bb9\u548c\u5e7b\u89c9\u5185\u5bb9\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u9650\u5236\u4e86\u5b83\u4eec\u751f\u6210\u4e00\u81f4\u548c\u771f\u5b9e\u4fe1\u606f\u7684\u53ef\u4fe1\u5ea6\u3002\u73b0\u6709\u7684\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5982 ROUGE\u3001BERTScore \u6216 Perplexity\uff0c\u7f3a\u4e4f\u6709\u6548\u68c0\u6d4b\u5e7b\u89c9\u6240\u9700\u7684\u8bed\u4e49\u6df1\u5ea6\u3002", "method": "\u8be5\u65b9\u6cd5\u6784\u5efa\u4e86\u6765\u81ea LLM \u751f\u6210\u6587\u672c\u7684 N-Gram \u9891\u7387\u5f20\u91cf\uff0c\u901a\u8fc7\u5bf9\u5f20\u91cf\u8fdb\u884c\u5206\u89e3\uff0c\u63d0\u53d6\u5947\u5f02\u503c\u4f5c\u4e3a\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u591a\u5c42\u611f\u77e5\u5668\uff08MLP\uff09\u4e8c\u5143\u5206\u7c7b\u5668\u8fdb\u884c\u5e7b\u89c9\u68c0\u6d4b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728 HaluEval \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u57fa\u7ebf\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\uff0c\u5e76\u4e14\u4e0e\u6700\u5148\u8fdb\u7684 LLM judges \u76f8\u6bd4\uff0c\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u5e7b\u89c9\u68c0\u6d4b\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7 N-Gram \u9891\u7387\u5f20\u91cf\u6355\u6349\u66f4\u4e30\u5bcc\u7684\u8bed\u4e49\u7ed3\u6784\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u5e7b\u89c9\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2509.05324", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05324", "abs": "https://arxiv.org/abs/2509.05324", "authors": ["Rongqian Chen", "Shu Hong", "Rifatul Islam", "Mahdi Imani", "G. Gary Tan", "Tian Lan"], "title": "Perception Graph for Cognitive Attack Reasoning in Augmented Reality", "comment": "Accepted by ACM MobiHoc XR Security workshop 2025", "summary": "Augmented reality (AR) systems are increasingly deployed in tactical\nenvironments, but their reliance on seamless human-computer interaction makes\nthem vulnerable to cognitive attacks that manipulate a user's perception and\nseverely compromise user decision-making. To address this challenge, we\nintroduce the Perception Graph, a novel model designed to reason about human\nperception within these systems. Our model operates by first mimicking the\nhuman process of interpreting key information from an MR environment and then\nrepresenting the outcomes using a semantically meaningful structure. We\ndemonstrate how the model can compute a quantitative score that reflects the\nlevel of perception distortion, providing a robust and measurable method for\ndetecting and analyzing the effects of such cognitive attacks.", "AI": {"tldr": "AR\u7cfb\u7edf\u6613\u53d7\u8ba4\u77e5\u653b\u51fb\u5f71\u54cd\uff0c\u5f71\u54cd\u7528\u6237\u51b3\u7b56\u3002", "motivation": "\u89e3\u51b3AR\u7cfb\u7edf\u5728\u6218\u672f\u73af\u5883\u4e2d\u6613\u53d7\u8ba4\u77e5\u653b\u51fb\u5f71\u54cd\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u611f\u77e5\u56fe\u6a21\u578b\uff0c\u6a21\u62df\u4eba\u7c7b\u611f\u77e5\u8fc7\u7a0b\uff0c\u5e76\u7528\u8bed\u4e49\u7ed3\u6784\u8868\u793a\u7ed3\u679c\u3002", "result": "\u6a21\u578b\u53ef\u4ee5\u8ba1\u7b97\u53cd\u6620\u611f\u77e5\u5931\u771f\u7a0b\u5ea6\u7684\u91cf\u5316\u5206\u6570\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u79cd\u68c0\u6d4b\u548c\u5206\u6790\u8ba4\u77e5\u653b\u51fb\u5f71\u54cd\u7684\u7a33\u5065\u4e14\u53ef\u6d4b\u91cf\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.05317", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05317", "abs": "https://arxiv.org/abs/2509.05317", "authors": ["Isac Holm"], "title": "VILOD: A Visual Interactive Labeling Tool for Object Detection", "comment": "Master's project", "summary": "The advancement of Object Detection (OD) using Deep Learning (DL) is often\nhindered by the significant challenge of acquiring large, accurately labeled\ndatasets, a process that is time-consuming and expensive. While techniques like\nActive Learning (AL) can reduce annotation effort by intelligently querying\ninformative samples, they often lack transparency, limit the strategic insight\nof human experts, and may overlook informative samples not aligned with an\nemployed query strategy. To mitigate these issues, Human-in-the-Loop (HITL)\napproaches integrating human intelligence and intuition throughout the machine\nlearning life-cycle have gained traction. Leveraging Visual Analytics (VA),\neffective interfaces can be created to facilitate this human-AI collaboration.\nThis thesis explores the intersection of these fields by developing and\ninvestigating \"VILOD: A Visual Interactive Labeling tool for Object Detection\".\nVILOD utilizes components such as a t-SNE projection of image features,\ntogether with uncertainty heatmaps and model state views. Enabling users to\nexplore data, interpret model states, AL suggestions, and implement diverse\nsample selection strategies within an iterative HITL workflow for OD. An\nempirical investigation using comparative use cases demonstrated how VILOD,\nthrough its interactive visualizations, facilitates the implementation of\ndistinct labeling strategies by making the model's state and dataset\ncharacteristics more interpretable (RQ1). The study showed that different\nvisually-guided labeling strategies employed within VILOD result in competitive\nOD performance trajectories compared to an automated uncertainty sampling AL\nbaseline (RQ2). This work contributes a novel tool and empirical insight into\nmaking the HITL-AL workflow for OD annotation more transparent, manageable, and\npotentially more effective.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVILOD\u7684\u53ef\u89c6\u4ea4\u4e92\u6807\u6ce8\u5de5\u5177\uff0c\u7528\u4e8e\u76ee\u6807\u68c0\u6d4b\uff0c\u65e8\u5728\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u4e2d\u6570\u636e\u96c6\u6807\u6ce8\u8017\u65f6\u8017\u529b\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u900f\u660e\u6027\uff0c\u9650\u5236\u4e86\u4eba\u7c7b\u4e13\u5bb6\u7684\u7b56\u7565\u6027\u89c1\u89e3\uff0c\u5e76\u53ef\u80fd\u5ffd\u7565\u4e0e\u6240\u7528\u67e5\u8be2\u7b56\u7565\u4e0d\u4e00\u81f4\u7684\u4fe1\u606f\u6837\u672c\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u7814\u7a76\u7740\u773c\u4e8e\u4eba\u673a\u7ed3\u5408\u7684\u65b9\u6cd5\u3002", "method": "\u8be5\u7814\u7a76\u5f00\u53d1\u5e76 \u0438\u0441\u0441\u043b\u0435\u0434\u0443\u0435\u0442 \u201cVILOD\uff1a\u4e00\u79cd\u7528\u4e8e\u5bf9\u8c61\u68c0\u6d4b\u7684\u53ef\u89c6\u4ea4\u4e92\u6807\u8bb0\u5de5\u5177\u201d\u3002VILOD\u5229\u7528\u56fe\u50cf\u7279\u5f81\u7684t-SNE\u6295\u5f71\u3001\u4e0d\u786e\u5b9a\u6027\u70ed\u56fe\u548c\u6a21\u578b\u72b6\u6001\u89c6\u56fe\u7b49\u7ec4\u4ef6\u3002", "result": "\u901a\u8fc7\u6bd4\u8f83\u7528\u4f8b\u7684\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0cVILOD\u901a\u8fc7\u5176\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\uff0c\u6709\u52a9\u4e8e\u901a\u8fc7\u4f7f\u6a21\u578b\u7684\u72b6\u6001\u548c\u6570\u636e\u96c6\u7279\u5f81\u66f4\u6613\u4e8e\u89e3\u91ca\u6765\u5b9e\u73b0\u4e0d\u540c\u7684\u6807\u6ce8\u7b56\u7565\u3002\u5728VILOD\u4e2d\u91c7\u7528\u7684\u4e0d\u540c\u89c6\u89c9\u5f15\u5bfc\u6807\u6ce8\u7b56\u7565\u4e0e\u81ea\u52a8\u4e0d\u786e\u5b9a\u6027\u62bd\u6837AL\u57fa\u7ebf\u76f8\u6bd4\uff0c\u4ea7\u751f\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684OD\u6027\u80fd\u8f68\u8ff9\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u8d21\u732e\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5de5\u5177\u548c\u5b9e\u8bc1\u89c1\u89e3\uff0c\u4f7fOD\u6ce8\u91ca\u7684HITL-AL\u5de5\u4f5c\u6d41\u7a0b\u66f4\u52a0\u900f\u660e\u3001\u6613\u4e8e\u7ba1\u7406\u4e14\u53ef\u80fd\u66f4\u6709\u6548\u3002"}}
{"id": "2509.05328", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05328", "abs": "https://arxiv.org/abs/2509.05328", "authors": ["Xiang Yuan", "Jun Shu", "Deyu meng", "Zongben Xu"], "title": "Feed Two Birds with One Scone: Exploiting Function-Space Regularization for Both OOD Robustness and ID Fine-Tuning Performance", "comment": null, "summary": "Robust fine-tuning aims to achieve competitive in-distribution (ID)\nperformance while maintaining the out-of-distribution (OOD) robustness of a\npre-trained model when transferring it to a downstream task. To remedy this,\nmost robust fine-tuning methods aim to preserve the pretrained weights,\nfeatures, or logits. However, we find that these methods cannot always improve\nOOD robustness for different model architectures. This is due to the OOD\nrobustness requiring the model function to produce stable prediction for input\ninformation of downstream tasks, while existing methods might serve as a poor\nproxy for the optimization in the function space. Based on this finding, we\npropose a novel regularization that constrains the distance of fine-tuning and\npre-trained model in the function space with the simulated OOD samples, aiming\nto preserve the OOD robustness of the pre-trained model. Besides, to further\nenhance the OOD robustness capability of the fine-tuning model, we introduce an\nadditional consistency regularization to promote stable predictions of\nperturbed samples. Extensive experiments demonstrate our approach could\nconsistently improve both downstream task ID fine-tuning performance and OOD\nrobustness across a variety of CLIP backbones, outperforming existing\nregularization-based robust fine-tuning methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9c81\u68d2\u5fae\u8c03\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u51fd\u6570\u7a7a\u95f4\u4e2d\u7ea6\u675f\u5fae\u8c03\u6a21\u578b\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u4e4b\u95f4\u7684\u8ddd\u79bb\uff0c\u5e76\u5f15\u5165\u4e00\u81f4\u6027\u6b63\u5219\u5316\u4ee5\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u9c81\u68d2\u5fae\u8c03\u65b9\u6cd5\u4e0d\u80fd\u59cb\u7ec8\u63d0\u9ad8\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u7684 OOD \u9c81\u68d2\u6027\uff0c\u56e0\u4e3a\u5b83\u4eec\u4e0d\u80fd\u5f88\u597d\u5730\u4f18\u5316\u51fd\u6570\u7a7a\u95f4\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u51fd\u6570\u7a7a\u95f4\u4e2d\u7ea6\u675f\u5fae\u8c03\u6a21\u578b\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u4e4b\u95f4\u7684\u8ddd\u79bb\uff0c\u5e76\u5f15\u5165\u4e00\u81f4\u6027\u6b63\u5219\u5316\u4ee5\u4fc3\u8fdb\u6270\u52a8\u6837\u672c\u7684\u7a33\u5b9a\u9884\u6d4b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5404\u79cd CLIP backbones \u4e0a\u90fd\u80fd\u6301\u7eed\u63d0\u9ad8\u4e0b\u6e38\u4efb\u52a1\u7684 ID \u5fae\u8c03\u6027\u80fd\u548c OOD \u9c81\u68d2\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u6b63\u5219\u5316\u7684\u9c81\u68d2\u5fae\u8c03\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u63d0\u9ad8\u6a21\u578b\u7684 OOD \u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301 ID \u6027\u80fd\u3002"}}
{"id": "2509.06195", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06195", "abs": "https://arxiv.org/abs/2509.06195", "authors": ["Jinrui Yang", "Fan Jiang", "Timothy Baldwin"], "title": "Language Bias in Information Retrieval: The Nature of the Beast and Mitigation Methods", "comment": "Accepted at EMNLP MRL 2024", "summary": "Language fairness in multilingual information retrieval (MLIR) systems is\ncrucial for ensuring equitable access to information across diverse languages.\nThis paper sheds light on the issue, based on the assumption that queries in\ndifferent languages, but with identical semantics, should yield equivalent\nranking lists when retrieving on the same multilingual documents. We evaluate\nthe degree of fairness using both traditional retrieval methods, and a DPR\nneural ranker based on mBERT and XLM-R. Additionally, we introduce `LaKDA', a\nnovel loss designed to mitigate language biases in neural MLIR approaches. Our\nanalysis exposes intrinsic language biases in current MLIR technologies, with\nnotable disparities across the retrieval methods, and the effectiveness of\nLaKDA in enhancing language fairness.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u591a\u8bed\u8a00\u4fe1\u606f\u68c0\u7d22\uff08MLIR\uff09\u7cfb\u7edf\u4e2d\u8bed\u8a00\u516c\u5e73\u6027\u7684\u95ee\u9898\uff0c\u5373\u8bed\u4e49\u76f8\u540c\u7684\u4e0d\u540c\u8bed\u8a00\u67e5\u8be2\u5e94\u8fd4\u56de\u7b49\u6548\u7684\u6392\u5e8f\u5217\u8868\u3002", "motivation": "\u786e\u4fdd\u4e0d\u540c\u8bed\u8a00\u7528\u6237\u516c\u5e73\u5730\u83b7\u53d6\u4fe1\u606f\u81f3\u5173\u91cd\u8981\u3002\u8bba\u6587\u57fa\u4e8e\u5047\u8bbe\uff1a\u8bed\u4e49\u76f8\u540c\u7684\u4e0d\u540c\u8bed\u8a00\u67e5\u8be2\u5e94\u5728\u76f8\u540c\u7684\u591a\u8bed\u8a00\u6587\u6863\u4e0a\u4ea7\u751f\u7b49\u6548\u7684\u6392\u5e8f\u5217\u8868\u3002", "method": "\u4f7f\u7528\u4f20\u7edf\u68c0\u7d22\u65b9\u6cd5\u548c\u57fa\u4e8emBERT\u53caXLM-R\u7684DPR\u795e\u7ecf\u6392\u5e8f\u5668\u8bc4\u4f30\u516c\u5e73\u6027\u3002\u5f15\u5165\u4e86\u540d\u4e3aLaKDA\u7684\u65b0\u578b\u635f\u5931\u51fd\u6570\uff0c\u4ee5\u51cf\u8f7b\u795e\u7ecfMLIR\u65b9\u6cd5\u4e2d\u7684\u8bed\u8a00\u504f\u5dee\u3002", "result": "\u63ed\u793a\u4e86\u5f53\u524dMLIR\u6280\u672f\u4e2d\u56fa\u6709\u7684\u8bed\u8a00\u504f\u5dee\uff0c\u4e0d\u540c\u68c0\u7d22\u65b9\u6cd5\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002LaKDA\u5728\u589e\u5f3a\u8bed\u8a00\u516c\u5e73\u6027\u65b9\u9762\u6709\u6548\u3002", "conclusion": "\u5206\u6790\u63ed\u793a\u4e86\u591a\u8bed\u8a00\u4fe1\u606f\u68c0\u7d22\u6280\u672f\u4e2d\u5b58\u5728\u7684\u8bed\u8a00\u504f\u5dee\uff0c\u5e76\u8bc1\u660e\u4e86LaKDA\u635f\u5931\u51fd\u6570\u5728\u63d0\u5347\u8bed\u8a00\u516c\u5e73\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.06298", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2509.06298", "abs": "https://arxiv.org/abs/2509.06298", "authors": ["Zihan Yan", "Rui Xi", "Mengshu Hou"], "title": "MCTuner: Spatial Decomposition-Enhanced Database Tuning via LLM-Guided Exploration", "comment": null, "summary": "Database knob tuning is essential for optimizing the performance of modern\ndatabase management systems, which often expose hundreds of knobs with\ncontinuous or categorical values. However, the large number of knobs and the\nvast configuration space make it difficult to identify optimal settings\nefficiently. Although learning-based tuning has shown promise, existing\napproaches either ignore domain knowledge by relying solely on benchmark\nfeedback or struggle to explore the high-dimensional knob space, resulting in\nhigh tuning costs and suboptimal performance. To address these challenges, we\npropose MCTuner, an adaptive knob tuning framework that minimizes exploration\nin ineffective regions of the configuration space. MCTuner employs a\nMixture-of-Experts (MoE) mechanism with specialized LLMs to identify\nperformance-critical knobs. In further, MCTuner introduces the first spatial\ndecomposition algorithm that recursively partitions the space into hierarchical\nsubspaces, on which Bayesian Optimization is performed to efficiently search\nfor near-optimal configurations. Evaluated on different benchmarks (OLAP, OLTP,\nand HTAP), MCTuner achieves up to 19.2% performance gains and 1.4x faster\nconfiguration discovery per iteration compared to state-of-the-art methods.", "AI": {"tldr": "MCTuner\u662f\u4e00\u4e2a\u6570\u636e\u5e93\u65cb\u94ae\u8c03\u4f18\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u6700\u5c0f\u5316\u914d\u7f6e\u7a7a\u95f4\u4e2d\u65e0\u6548\u533a\u57df\u7684\u63a2\u7d22\u6765\u4f18\u5316\u6570\u636e\u5e93\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u9886\u57df\u77e5\u8bc6\u6216\u96be\u4ee5\u63a2\u7d22\u9ad8\u7ef4\u65cb\u94ae\u7a7a\u95f4\uff0c\u5bfc\u81f4\u8c03\u4f18\u6210\u672c\u9ad8\u548c\u6027\u80fd\u6b20\u4f73\u3002", "method": "MCTuner\u91c7\u7528MoE\u673a\u5236\u4e0e\u4e13\u7528LLM\u6765\u8bc6\u522b\u6027\u80fd\u5173\u952e\u65cb\u94ae\uff0c\u5e76\u5f15\u5165\u7a7a\u95f4\u5206\u89e3\u7b97\u6cd5\u4ee5\u9012\u5f52\u65b9\u5f0f\u5c06\u7a7a\u95f4\u5212\u5206\u4e3a\u5206\u5c42\u5b50\u7a7a\u95f4\uff0c\u7136\u540e\u5728\u8fd9\u4e9b\u5b50\u7a7a\u95f4\u4e0a\u6267\u884c\u8d1d\u53f6\u65af\u4f18\u5316\u3002", "result": "MCTuner\u5728\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe19.2%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u4e14\u6bcf\u6b21\u8fed\u4ee3\u7684\u914d\u7f6e\u53d1\u73b0\u901f\u5ea6\u6bd4\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u5feb1.4\u500d\u3002", "conclusion": "MCTuner\u901a\u8fc7\u81ea\u9002\u5e94\u65cb\u94ae\u8c03\u4f18\u6846\u67b6\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u65e0\u6548\u914d\u7f6e\u7a7a\u95f4\u7684\u63a2\u7d22\uff0c\u5e76\u5728\u6570\u636e\u5e93\u6027\u80fd\u4f18\u5316\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002"}}
{"id": "2509.05385", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05385", "abs": "https://arxiv.org/abs/2509.05385", "authors": ["Jiacheng Wei", "Faguo Wu", "Xiao Zhang"], "title": "A Lightweight Framework for Trigger-Guided LoRA-Based Self-Adaptation in LLMs", "comment": "11 pages, 7 figures, conference", "summary": "Large language models are unable to continuously adapt and learn from new\ndata during reasoning at inference time. To address this limitation, we propose\nthat complex reasoning tasks be decomposed into atomic subtasks and introduce\nSAGE, a trigger-guided dynamic fine-tuning framework that enables adaptive\nupdates during reasoning at inference time. SAGE consists of three key\ncomponents: (1) a Trigger module that detects reasoning failures through\nmultiple evaluation metrics in real time; (2) a Trigger Buffer module that\nclusters anomaly samples using a streaming clustering process with HDBSCAN,\nfollowed by stability checks and similarity-based merging; and (3) a Lora Store\nmodule that dynamically optimizes parameter updates with an adapter pool for\nknowledge retention. Evaluation results show that SAGE demonstrates excellent\naccuracy, robustness, and stability on the atomic reasoning subtask through\ndynamic knowledge updating during test time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSAGE\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u63a8\u7406\u65f6\u8fdb\u884c\u81ea\u9002\u5e94\u66f4\u65b0\uff0c\u4ee5\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65e0\u6cd5\u6301\u7eed\u9002\u5e94\u548c\u5b66\u4e60\u65b0\u6570\u636e\u7684\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65e0\u6cd5\u5728\u63a8\u7406\u65f6\u6301\u7eed\u9002\u5e94\u548c\u5b66\u4e60\u65b0\u6570\u636e\u3002", "method": "\u5c06\u590d\u6742\u7684\u63a8\u7406\u4efb\u52a1\u5206\u89e3\u4e3a\u539f\u5b50\u5b50\u4efb\u52a1\uff0c\u5e76\u5f15\u5165\u89e6\u53d1\u5f15\u5bfc\u7684\u52a8\u6001\u5fae\u8c03\u6846\u67b6SAGE\uff0c\u8be5\u6846\u67b6\u5305\u542b\u89e6\u53d1\u6a21\u5757\u3001\u89e6\u53d1\u7f13\u51b2\u6a21\u5757\u548cLora\u5b58\u50a8\u6a21\u5757\u3002", "result": "SAGE\u5728\u539f\u5b50\u63a8\u7406\u5b50\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "SAGE\u53ef\u4ee5\u901a\u8fc7\u5728\u6d4b\u8bd5\u65f6\u8fdb\u884c\u52a8\u6001\u77e5\u8bc6\u66f4\u65b0\uff0c\u4ece\u800c\u5728\u539f\u5b50\u63a8\u7406\u5b50\u4efb\u52a1\u4e0a\u5b9e\u73b0\u5353\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2509.05325", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05325", "abs": "https://arxiv.org/abs/2509.05325", "authors": ["Liming Xu", "Yunbo Long", "Alexandra Brintrup"], "title": "SynDelay: A Synthetic Dataset for Delivery Delay Prediction", "comment": "This paper incldues 1 figure and 2 tables", "summary": "Artificial intelligence (AI) is transforming supply chain management, yet\nprogress in predictive tasks -- such as delivery delay prediction -- remains\nconstrained by the scarcity of high-quality, openly available datasets.\nExisting datasets are often proprietary, small, or inconsistently maintained,\nhindering reproducibility and benchmarking. We present SynDelay, a synthetic\ndataset designed for delivery delay prediction. Generated using an advanced\ngenerative model trained on real-world data, SynDelay preserves realistic\ndelivery patterns while ensuring privacy. Although not entirely free of noise\nor inconsistencies, it provides a challenging and practical testbed for\nadvancing predictive modelling. To support adoption, we provide baseline\nresults and evaluation metrics as initial benchmarks, serving as reference\npoints rather than state-of-the-art claims. SynDelay is publicly available\nthrough the Supply Chain Data Hub, an open initiative promoting dataset sharing\nand benchmarking in supply chain AI. We encourage the community to contribute\ndatasets, models, and evaluation practices to advance research in this area.\nAll code is openly accessible at https://supplychaindatahub.org.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aSynDelay\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u9884\u6d4b\u4ea4\u8d27\u5ef6\u8fdf\uff0c\u65e8\u5728\u89e3\u51b3\u9ad8\u8d28\u91cf\u516c\u5f00\u6570\u636e\u96c6\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "motivation": "\u76ee\u524d\u7528\u4e8e\u4f9b\u5e94\u94fe\u7ba1\u7406\u7684AI\uff0c \u5c24\u5176\u662f\u5728\u9884\u6d4b\u4ea4\u8d27\u5ef6\u8fdf\u7684\u4efb\u52a1\u4e2d\uff0c \u53d7\u5230\u9ad8\u8d28\u91cf\u3001\u516c\u5f00\u53ef\u7528\u7684\u6570\u636e\u96c6\u7a00\u7f3a\u7684\u9650\u5236\u3002\u73b0\u6709\u7684\u6570\u636e\u96c6\u901a\u5e38\u662f\u4e13\u6709\u7684\u3001\u5c0f\u578b\u7684\u6216\u7ef4\u62a4\u4e0d\u4e00\u81f4\u7684\uff0c\u4ece\u800c\u59a8\u788d\u4e86\u53ef\u91cd\u590d\u6027\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u4f7f\u7528\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u5148\u8fdb\u751f\u6210\u6a21\u578b\u751f\u6210SynDelay\uff0c\u8be5\u6a21\u578b\u4fdd\u7559\u4e86\u771f\u5b9e\u7684\u4ea4\u4ed8\u6a21\u5f0f\uff0c\u540c\u65f6\u786e\u4fdd\u4e86\u9690\u79c1\u3002", "result": "SynDelay\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u548c\u5b9e\u8df5\u6027\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7528\u4e8e\u63a8\u8fdb\u9884\u6d4b\u5efa\u6a21\u3002\u8bba\u6587\u63d0\u4f9b\u4e86\u57fa\u7ebf\u7ed3\u679c\u548c\u8bc4\u4f30\u6307\u6807\u4f5c\u4e3a\u521d\u59cb\u57fa\u51c6\u3002", "conclusion": "SynDelay\u901a\u8fc7\u4f9b\u5e94\u94fe\u6570\u636e\u4e2d\u5fc3\u516c\u5f00\u63d0\u4f9b\uff0c\u9f13\u52b1\u793e\u533a\u8d21\u732e\u6570\u636e\u96c6\u3001\u6a21\u578b\u548c\u8bc4\u4f30\u5b9e\u8df5\uff0c\u4ee5\u63a8\u8fdb\u8be5\u9886\u57df\u7684\u7814\u7a76\u3002"}}
{"id": "2509.05319", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05319", "abs": "https://arxiv.org/abs/2509.05319", "authors": ["Zhengda Li"], "title": "Context-Aware Knowledge Distillation with Adaptive Weighting for Image Classification", "comment": null, "summary": "Knowledge distillation (KD) is a widely used technique to transfer knowledge\nfrom a large teacher network to a smaller student model. Traditional KD uses a\nfixed balancing factor alpha as a hyperparameter to combine the hard-label\ncross-entropy loss with the soft-label distillation loss. However, a static\nalpha is suboptimal because the optimal trade-off between hard and soft\nsupervision can vary during training.\n  In this work, we propose an Adaptive Knowledge Distillation (AKD) framework.\nFirst we try to make alpha as learnable parameter that can be automatically\nlearned and optimized during training. Then we introduce a formula to reflect\nthe gap between the student and the teacher to compute alpha dynamically,\nguided by student-teacher discrepancies, and further introduce a Context-Aware\nModule (CAM) using MLP + Attention to adaptively reweight class-wise teacher\noutputs. Experiments on CIFAR-10 with ResNet-50 as teacher and ResNet-18 as\nstudent demonstrate that our approach achieves superior accuracy compared to\nfixed-weight KD baselines, and yields more stable convergence.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u77e5\u8bc6\u84b8\u998f\uff08AKD\uff09\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u77e5\u8bc6\u84b8\u998f\u4e2d\u56fa\u5b9a\u5e73\u8861\u56e0\u5b50alpha\u7684\u6b21\u4f18\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7684\u77e5\u8bc6\u84b8\u998f\u4f7f\u7528\u56fa\u5b9a\u7684\u5e73\u8861\u56e0\u5b50alpha\uff0c\u4f46\u9759\u6001\u7684alpha\u5e76\u975e\u6700\u4f18\uff0c\u56e0\u4e3ahard\u548csoft\u76d1\u7763\u4e4b\u95f4\u7684\u6700\u4f73\u6743\u8861\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u4f1a\u53d1\u751f\u53d8\u5316\u3002", "method": "1. \u5c06alpha\u4f5c\u4e3a\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u53c2\u6570\uff0c\u53ef\u4ee5\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u81ea\u52a8\u5b66\u4e60\u548c\u4f18\u5316\u30022. \u5f15\u5165\u4e00\u4e2a\u516c\u5f0f\u6765\u53cd\u6620\u5b66\u751f\u548c\u6559\u5e08\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4ece\u800c\u52a8\u6001\u8ba1\u7b97alpha\uff0c\u7531\u5b66\u751f-\u6559\u5e08\u5dee\u5f02\u6307\u5bfc\u30023. \u8fdb\u4e00\u6b65\u5f15\u5165\u4e00\u4e2a\u4e0a\u4e0b\u6587\u611f\u77e5\u6a21\u5757\uff08CAM\uff09\uff0c\u4f7f\u7528MLP + Attention\u6765\u81ea\u9002\u5e94\u5730\u91cd\u65b0\u52a0\u6743\u7c7b\u522b\u7684\u6559\u5e08\u8f93\u51fa\u3002", "result": "\u5728CIFAR-10\u4e0a\uff0c\u4f7f\u7528ResNet-50\u4f5c\u4e3a\u6559\u5e08\uff0cResNet-18\u4f5c\u4e3a\u5b66\u751f\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u6bd4\u56fa\u5b9a\u6743\u91cd\u7684KD\u57fa\u7ebf\u53d6\u5f97\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u7387\uff0c\u5e76\u4ea7\u751f\u4e86\u66f4\u7a33\u5b9a\u7684\u6536\u655b\u3002", "conclusion": "\u63d0\u51fa\u7684AKD\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u63d0\u9ad8\u77e5\u8bc6\u84b8\u998f\u7684\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574hard\u548csoft\u76d1\u7763\u7684\u6743\u91cd\u4ee5\u53ca\u91cd\u65b0\u52a0\u6743\u6559\u5e08\u8f93\u51fa\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u51c6\u786e\u7387\u548c\u66f4\u7a33\u5b9a\u7684\u6536\u655b\u3002"}}
{"id": "2509.05429", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.05429", "abs": "https://arxiv.org/abs/2509.05429", "authors": ["Jie Fu", "Hong Yuan", "Zhili Chen", "Wendy Hui Wang"], "title": "Safeguarding Graph Neural Networks against Topology Inference Attacks", "comment": "Acctepted by ACM CCS'25", "summary": "Graph Neural Networks (GNNs) have emerged as powerful models for learning\nfrom graph-structured data. However, their widespread adoption has raised\nserious privacy concerns. While prior research has primarily focused on\nedge-level privacy, a critical yet underexplored threat lies in topology\nprivacy - the confidentiality of the graph's overall structure. In this work,\nwe present a comprehensive study on topology privacy risks in GNNs, revealing\ntheir vulnerability to graph-level inference attacks. To this end, we propose a\nsuite of Topology Inference Attacks (TIAs) that can reconstruct the structure\nof a target training graph using only black-box access to a GNN model. Our\nfindings show that GNNs are highly susceptible to these attacks, and that\nexisting edge-level differential privacy mechanisms are insufficient as they\neither fail to mitigate the risk or severely compromise model accuracy. To\naddress this challenge, we introduce Private Graph Reconstruction (PGR), a\nnovel defense framework designed to protect topology privacy while maintaining\nmodel accuracy. PGR is formulated as a bi-level optimization problem, where a\nsynthetic training graph is iteratively generated using meta-gradients, and the\nGNN model is concurrently updated based on the evolving graph. Extensive\nexperiments demonstrate that PGR significantly reduces topology leakage with\nminimal impact on model accuracy. Our code is anonymously available at\nhttps://github.com/JeffffffFu/PGR.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u4e2d\u7684\u62d3\u6251\u9690\u79c1\u98ce\u9669\uff0c\u53d1\u73b0GNNs\u5bb9\u6613\u53d7\u5230\u56fe\u7ea7\u63a8\u7406\u653b\u51fb\u3002\u63d0\u51fa\u4e86\u62d3\u6251\u63a8\u7406\u653b\u51fb\uff08TIAs\uff09\u6765\u91cd\u5efa\u76ee\u6807\u8bad\u7ec3\u56fe\u7684\u7ed3\u6784\uff0c\u5e76\u53d1\u73b0\u73b0\u6709\u7684\u8fb9\u7ea7\u5dee\u5206\u9690\u79c1\u673a\u5236\u4e0d\u8db3\u4ee5\u7f13\u89e3\u98ce\u9669\u3002\u4e3a\u6b64\uff0c\u5f15\u5165\u4e86\u79c1\u6709\u56fe\u91cd\u5efa\uff08PGR\uff09\uff0c\u4ee5\u4fdd\u62a4\u62d3\u6251\u9690\u79c1\u5e76\u4fdd\u6301\u6a21\u578b\u51c6\u786e\u6027\u3002", "motivation": "GNNs\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u56fe\u7ed3\u6784\u6570\u636e\u7684\u5b66\u4e60\uff0c\u4f46\u5176\u5e7f\u6cdb\u5e94\u7528\u5f15\u53d1\u4e86\u4e25\u91cd\u7684\u9690\u79c1\u95ee\u9898\u3002\u4e4b\u524d\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u8fb9\u7ea7\u522b\u7684\u9690\u79c1\uff0c\u800c\u5ffd\u7565\u4e86\u62d3\u6251\u9690\u79c1\u2014\u2014\u56fe\u7684\u6574\u4f53\u7ed3\u6784\u7684\u673a\u5bc6\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u62d3\u6251\u63a8\u7406\u653b\u51fb\uff08TIAs\uff09\uff0c\u5229\u7528\u9ed1\u76d2\u8bbf\u95eeGNN\u6a21\u578b\u6765\u91cd\u5efa\u76ee\u6807\u8bad\u7ec3\u56fe\u7684\u7ed3\u6784\u3002\u540c\u65f6\uff0c\u5f15\u5165\u4e86\u79c1\u6709\u56fe\u91cd\u5efa\uff08PGR\uff09\uff0c\u5c06\u5176\u8868\u8ff0\u4e3a\u4e00\u4e2a\u53cc\u5c42\u4f18\u5316\u95ee\u9898\uff0c\u4f7f\u7528\u5143\u68af\u5ea6\u8fed\u4ee3\u751f\u6210\u5408\u6210\u8bad\u7ec3\u56fe\uff0c\u5e76\u57fa\u4e8e\u4e0d\u65ad\u6f14\u53d8\u7684\u56fe\u5e76\u53d1\u66f4\u65b0GNN\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPGR\u5728\u5bf9\u6a21\u578b\u51c6\u786e\u6027\u5f71\u54cd\u6700\u5c0f\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u62d3\u6251\u6cc4\u6f0f\u3002", "conclusion": "GNNs\u5bb9\u6613\u53d7\u5230\u56fe\u7ea7\u63a8\u7406\u653b\u51fb\uff0c\u73b0\u6709\u7684\u8fb9\u7ea7\u5dee\u5206\u9690\u79c1\u673a\u5236\u4e0d\u8db3\u4ee5\u7f13\u89e3\u98ce\u9669\u3002PGR\u53ef\u4ee5\u6709\u6548\u5730\u4fdd\u62a4\u62d3\u6251\u9690\u79c1\u5e76\u4fdd\u6301\u6a21\u578b\u51c6\u786e\u6027\u3002"}}
{"id": "2509.06452", "categories": ["cs.IR", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.06452", "abs": "https://arxiv.org/abs/2509.06452", "authors": ["Enrico Palumbo", "Gustavo Penha", "Alva Liu", "Marcus Eltscheminov", "Jefferson Carvalho dos Santos", "Alice Wang", "Hugues Bouchard", "Humberto Jes\u00fas Corona Pampin", "Michelle Tran Luu"], "title": "AudioBoost: Increasing Audiobook Retrievability in Spotify Search with Synthetic Query Generation", "comment": "EARL Workshop @ RecSys25", "summary": "Spotify has recently introduced audiobooks as part of its catalog,\ncomplementing its music and podcast offering. Search is often the first entry\npoint for users to access new items, and an important goal for Spotify is to\nsupport users in the exploration of the audiobook catalog. More specifically,\nwe would like to enable users without a specific item in mind to broadly search\nby topic, genre, story tropes, decade, and discover audiobooks, authors and\npublishers they may like. To do this, we need to 1) inspire users to type more\nexploratory queries for audiobooks and 2) augment our retrieval systems to\nbetter deal with exploratory audiobook queries. This is challenging in a\ncold-start scenario, where we have a retrievabiliy bias due to the little\namount of user interactions with audiobooks compared to previously available\nitems such as music and podcast content. To address this, we propose\nAudioBoost, a system to boost audiobook retrievability in Spotify's Search via\nsynthetic query generation. AudioBoost leverages Large Language Models (LLMs)\nto generate synthetic queries conditioned on audiobook metadata. The synthetic\nqueries are indexed both in the Query AutoComplete (QAC) and in the Search\nRetrieval engine to improve query formulation and retrieval at the same time.\nWe show through offline evaluation that synthetic queries increase\nretrievability and are of high quality. Moreover, results from an online A/B\ntest show that AudioBoost leads to a +0.7% in audiobook impressions, +1.22% in\naudiobook clicks, and +1.82% in audiobook exploratory query completions.", "AI": {"tldr": "Spotify introduces AudioBoost, a system using LLMs to generate synthetic queries for audiobooks, improving their searchability.", "motivation": "To enable users to explore audiobooks on Spotify through broad searches by topic, genre, etc., especially in a cold-start scenario with limited user interactions.", "method": "AudioBoost uses LLMs to generate synthetic queries based on audiobook metadata, indexing them in Query AutoComplete (QAC) and the Search Retrieval engine.", "result": "Offline evaluation shows increased retrievability and high quality of synthetic queries. Online A/B testing shows a +0.7% increase in audiobook impressions, +1.22% in audiobook clicks, and +1.82% in audiobook exploratory query completions.", "conclusion": "AudioBoost effectively improves audiobook discovery on Spotify by enhancing query formulation and retrieval."}}
{"id": "2509.06439", "categories": ["cs.DB", "cs.DM", "cs.MS"], "pdf": "https://arxiv.org/pdf/2509.06439", "abs": "https://arxiv.org/abs/2509.06439", "authors": ["David Robert Pratten", "Luke Mathieson", "Fahimeh Ramezani"], "title": "Relational Algebras for Subset Selection and Optimisation", "comment": "15 pages main text, 28 pages appendicies", "summary": "The database community lacks a unified relational query language for subset\nselection and optimisation queries, limiting both user expression and query\noptimiser reasoning about such problems. Decades of research (latterly under\nthe rubric of prescriptive analytics) have produced powerful evaluation\nalgorithms with incompatible, ad-hoc SQL extensions that specify and filter\nthrough distinct mechanisms. We present the first unified algebraic foundation\nfor these queries, introducing relational exponentiation to complete the\nfundamental algebraic operations alongside union (addition) and cross product\n(multiplication). First, we extend relational algebra to complete domain\nrelations-relations defined by characteristic functions rather than explicit\nextensions-achieving the expressiveness of NP-complete/hard problems, while\nsimultaneously providing query safety for finite inputs. Second, we introduce\nsolution sets, a higher-order relational algebra over sets of relations that\nnaturally expresses search spaces as functions f: Base to Decision, yielding\n|Decision|^|Base| candidate relations. Third, we provide structure-preserving\ntranslation semantics from solution sets to standard relational algebra,\nenabling mechanical translation to existing evaluation algorithms. This\nframework achieves the expressiveness of the most powerful prior approaches\nwhile providing the theoretical clarity and compositional properties absent in\nprevious work. We demonstrate the capabilities these algebras open up through a\npolymorphic SQL where standard clauses seamlessly express data management,\nsubset selection, and optimisation queries within a single paradigm.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u5173\u7cfb\u4ee3\u6570\u57fa\u7840\uff0c\u7528\u4e8e\u5b50\u96c6\u9009\u62e9\u548c\u4f18\u5316\u67e5\u8be2\u3002", "motivation": "\u6570\u636e\u5e93\u793e\u533a\u7f3a\u4e4f\u7edf\u4e00\u7684\u5173\u7cfb\u67e5\u8be2\u8bed\u8a00\u6765\u8fdb\u884c\u5b50\u96c6\u9009\u62e9\u548c\u4f18\u5316\u67e5\u8be2\u3002", "method": "1. \u6269\u5c55\u5173\u7cfb\u4ee3\u6570\u4ee5\u5b8c\u6210\u57df\u5173\u7cfb\u30022. \u5f15\u5165\u89e3\u96c6\uff0c\u4e00\u79cd\u5728\u5173\u7cfb\u96c6\u4e0a\u7684\u9ad8\u9636\u5173\u7cfb\u4ee3\u6570\u30023. \u63d0\u4f9b\u4ece\u89e3\u96c6\u5230\u6807\u51c6\u5173\u7cfb\u4ee3\u6570\u7684\u7ed3\u6784\u4fdd\u6301\u7ffb\u8bd1\u8bed\u4e49\u3002", "result": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u6700\u5f3a\u5927\u7684\u5148\u524d\u65b9\u6cd5\u7684\u529f\u80fd\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u5148\u524d\u5de5\u4f5c\u4e2d\u6ca1\u6709\u7684\u7406\u8bba\u6e05\u6670\u5ea6\u548c\u7ec4\u5408\u5c5e\u6027\u3002", "conclusion": "\u901a\u8fc7\u591a\u6001SQL\u5c55\u793a\u4e86\u8fd9\u4e9b\u4ee3\u6570\u5f00\u542f\u7684\u529f\u80fd\uff0c\u5176\u4e2d\u6807\u51c6\u5b50\u53e5\u5728\u5355\u4e2a\u8303\u4f8b\u4e2d\u65e0\u7f1d\u5730\u8868\u8fbe\u4e86\u6570\u636e\u7ba1\u7406\u3001\u5b50\u96c6\u9009\u62e9\u548c\u4f18\u5316\u67e5\u8be2\u3002"}}
{"id": "2509.05396", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.05396", "abs": "https://arxiv.org/abs/2509.05396", "authors": ["Andrea Wynn", "Harsh Satija", "Gillian Hadfield"], "title": "Talk Isn't Always Cheap: Understanding Failure Modes in Multi-Agent Debate", "comment": "ICML MAS Workshop 2025", "summary": "While multi-agent debate has been proposed as a promising strategy for\nimproving AI reasoning ability, we find that debate can sometimes be harmful\nrather than helpful. The prior work has exclusively focused on debates within\nhomogeneous groups of agents, whereas we explore how diversity in model\ncapabilities influences the dynamics and outcomes of multi-agent interactions.\nThrough a series of experiments, we demonstrate that debate can lead to a\ndecrease in accuracy over time -- even in settings where stronger (i.e., more\ncapable) models outnumber their weaker counterparts. Our analysis reveals that\nmodels frequently shift from correct to incorrect answers in response to peer\nreasoning, favoring agreement over challenging flawed reasoning. These results\nhighlight important failure modes in the exchange of reasons during multi-agent\ndebate, suggesting that naive applications of debate may cause performance\ndegradation when agents are neither incentivized nor adequately equipped to\nresist persuasive but incorrect reasoning.", "AI": {"tldr": "\u591a\u4eba\u8fa9\u8bba\u65e8\u5728\u63d0\u9ad8\u4eba\u5de5\u667a\u80fd\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u80fd\u529b\u4e0d\u540c\u7684\u6a21\u578b\u4e4b\u95f4\u8fdb\u884c\u8fa9\u8bba\u6709\u65f6\u53cd\u800c\u6709\u5bb3\u3002\u5373\u4f7f\u66f4\u5f3a\u5927\u7684\u6a21\u578b\u6570\u91cf\u8d85\u8fc7\u8f83\u5f31\u7684\u6a21\u578b\uff0c\u8fa9\u8bba\u4e5f\u53ef\u80fd\u5bfc\u81f4\u51c6\u786e\u7387\u4e0b\u964d\u3002\u6a21\u578b\u503e\u5411\u4e8e\u8fbe\u6210\u4e00\u81f4\uff0c\u800c\u4e0d\u662f\u6311\u6218\u6709\u7f3a\u9677\u7684\u63a8\u7406\uff0c\u5bfc\u81f4\u4ece\u6b63\u786e\u7b54\u6848\u8f6c\u5411\u9519\u8bef\u7b54\u6848\u3002\u56e0\u6b64\uff0c\u5728\u6ca1\u6709\u6fc0\u52b1\u6216\u5145\u5206\u51c6\u5907\u597d\u62b5\u5236\u6709\u8bf4\u670d\u529b\u4f46\u9519\u8bef\u7684\u63a8\u7406\u65f6\uff0c\u8fa9\u8bba\u53ef\u80fd\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u5148\u524d\u7684\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728\u540c\u8d28\u4ee3\u7406\u7fa4\u4f53\u4e2d\u7684\u8fa9\u8bba\uff0c\u800c\u672c\u6587\u63a2\u8ba8\u4e86\u6a21\u578b\u80fd\u529b\u7684\u591a\u6837\u6027\u5982\u4f55\u5f71\u54cd\u591a\u4ee3\u7406\u4ea4\u4e92\u7684\u52a8\u6001\u548c\u7ed3\u679c\u3002", "method": "\u901a\u8fc7\u4e00\u7cfb\u5217\u5b9e\u9a8c", "result": "\u8fa9\u8bba\u4f1a\u5bfc\u81f4\u51c6\u786e\u7387\u968f\u65f6\u95f4\u4e0b\u964d\u2014\u2014\u5373\u4f7f\u5728\u66f4\u5f3a\u5927\u7684\u6a21\u578b\u6570\u91cf\u8d85\u8fc7\u5176\u8f83\u5f31\u7684\u5bf9\u624b\u7684\u60c5\u51b5\u4e0b\u3002\u6a21\u578b\u7ecf\u5e38\u4ece\u6b63\u786e\u7b54\u6848\u8f6c\u53d8\u4e3a\u4e0d\u6b63\u786e\u7684\u7b54\u6848\uff0c\u4ee5\u54cd\u5e94\u540c\u4f34\u7684\u63a8\u7406\uff0c\u4ece\u800c\u504f\u7231\u8fbe\u6210\u534f\u8bae\u800c\u4e0d\u662f\u6311\u6218\u6709\u7f3a\u9677\u7684\u63a8\u7406\u3002", "conclusion": "\u591a\u4eba\u8fa9\u8bba\u4e2d\u7406\u7531\u4ea4\u6d41\u5b58\u5728\u91cd\u8981\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u8868\u660e\u5728\u6ca1\u6709\u6fc0\u52b1\u6216\u5145\u5206\u51c6\u5907\u597d\u62b5\u5236\u6709\u8bf4\u670d\u529b\u4f46\u9519\u8bef\u7684\u63a8\u7406\u65f6\uff0c\u7b80\u5355\u5730\u5e94\u7528\u8fa9\u8bba\u53ef\u80fd\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002"}}
{"id": "2509.05330", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05330", "abs": "https://arxiv.org/abs/2509.05330", "authors": ["Seyed Muhammad Hossein Mousavi", "Atiye Ilanloo"], "title": "MVRS: The Multimodal Virtual Reality Stimuli-based Emotion Recognition Dataset", "comment": null, "summary": "Automatic emotion recognition has become increasingly important with the rise\nof AI, especially in fields like healthcare, education, and automotive systems.\nHowever, there is a lack of multimodal datasets, particularly involving body\nmotion and physiological signals, which limits progress in the field. To\naddress this, the MVRS dataset is introduced, featuring synchronized recordings\nfrom 13 participants aged 12 to 60 exposed to VR based emotional stimuli\n(relaxation, fear, stress, sadness, joy). Data were collected using eye\ntracking (via webcam in a VR headset), body motion (Kinect v2), and EMG and GSR\nsignals (Arduino UNO), all timestamp aligned. Participants followed a unified\nprotocol with consent and questionnaires. Features from each modality were\nextracted, fused using early and late fusion techniques, and evaluated with\nclassifiers to confirm the datasets quality and emotion separability, making\nMVRS a valuable contribution to multimodal affective computing.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u65b0\u7684\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u6570\u636e\u96c6 MVRS\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u6765\u81ea 13 \u540d\u53c2\u4e0e\u8005\u5728 VR \u73af\u5883\u4e2d\u4f53\u9a8c\u4e0d\u540c\u60c5\u7eea\u523a\u6fc0\uff08\u653e\u677e\u3001\u6050\u60e7\u3001\u538b\u529b\u3001\u60b2\u4f24\u3001\u5feb\u4e50\uff09\u65f6\u7684\u773c\u52a8\u8ffd\u8e2a\u3001\u8eab\u4f53\u8fd0\u52a8\u548c\u751f\u7406\u4fe1\u53f7\u3002", "motivation": "\u7f3a\u4e4f\u5305\u542b\u8eab\u4f53\u8fd0\u52a8\u548c\u751f\u7406\u4fe1\u53f7\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u9650\u5236\u4e86\u60c5\u611f\u8bc6\u522b\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "\u4f7f\u7528\u773c\u52a8\u8ffd\u8e2a\uff08\u901a\u8fc7 VR \u5934\u663e\u4e2d\u7684\u7f51\u7edc\u6444\u50cf\u5934\uff09\u3001\u8eab\u4f53\u8fd0\u52a8\uff08Kinect v2\uff09\u4ee5\u53ca EMG \u548c GSR \u4fe1\u53f7\uff08Arduino UNO\uff09\u540c\u6b65\u8bb0\u5f55\u6570\u636e\u3002\u53c2\u4e0e\u8005\u9075\u5faa\u7edf\u4e00\u7684\u534f\u8bae\u5e76\u586b\u5199\u95ee\u5377\u3002\u63d0\u53d6\u6bcf\u4e2a\u6a21\u6001\u7684\u7279\u5f81\uff0c\u4f7f\u7528\u65e9\u671f\u548c\u665a\u671f\u878d\u5408\u6280\u672f\u8fdb\u884c\u878d\u5408\uff0c\u5e76\u4f7f\u7528\u5206\u7c7b\u5668\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u7684\u8d28\u91cf\u548c\u60c5\u7eea\u53ef\u5206\u79bb\u6027\u3002", "conclusion": "MVRS \u6570\u636e\u96c6\u5bf9\u591a\u6a21\u6001\u60c5\u611f\u8ba1\u7b97\u662f\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u8d21\u732e\u3002"}}
{"id": "2509.05321", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05321", "abs": "https://arxiv.org/abs/2509.05321", "authors": ["Yunfei Guo", "Tao Zhang", "Wu Huang", "Yao Song"], "title": "A Dataset Generation Scheme Based on Video2EEG-SPGN-Diffusion for SEED-VD", "comment": null, "summary": "This paper introduces an open-source framework, Video2EEG-SPGN-Diffusion,\nthat leverages the SEED-VD dataset to generate a multimodal dataset of EEG\nsignals conditioned on video stimuli. Additionally, we disclose an engineering\npipeline for aligning video and EEG data pairs, facilitating the training of\nmultimodal large models with EEG alignment capabilities. Personalized EEG\nsignals are generated using a self-play graph network (SPGN) integrated with a\ndiffusion model. As a major contribution, we release a new dataset comprising\nover 1000 samples of SEED-VD video stimuli paired with generated 62-channel EEG\nsignals at 200 Hz and emotion labels, enabling video-EEG alignment and\nadvancing multimodal research. This framework offers novel tools for emotion\nanalysis, data augmentation, and brain-computer interface applications, with\nsubstantial research and engineering significance.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6 Video2EEG-SPGN-Diffusion\uff0c\u5b83\u5229\u7528 SEED-VD \u6570\u636e\u96c6\u751f\u6210\u4ee5\u89c6\u9891\u523a\u6fc0\u4e3a\u6761\u4ef6\u7684\u8111\u7535\u4fe1\u53f7\u591a\u6a21\u6001\u6570\u636e\u96c6\u3002", "motivation": "\u516c\u5f00\u7528\u4e8e\u5bf9\u9f50\u89c6\u9891\u548c\u8111\u7535\u6570\u636e\u5bf9\u7684\u5de5\u7a0b\u7ba1\u9053\uff0c\u4fc3\u8fdb\u5177\u6709\u8111\u7535\u5bf9\u9f50\u80fd\u529b\u7684\u591a\u6a21\u6001\u5927\u578b\u6a21\u578b\u7684\u8bad\u7ec3\u3002", "method": "\u4f7f\u7528\u4e0e\u6269\u6563\u6a21\u578b\u96c6\u6210\u7684\u81ea\u535a\u5f08\u56fe\u7f51\u7edc (SPGN) \u751f\u6210\u4e2a\u6027\u5316\u8111\u7535\u4fe1\u53f7\u3002", "result": "\u53d1\u5e03\u4e86\u4e00\u4e2a\u65b0\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b 1000 \u591a\u4e2a SEED-VD \u89c6\u9891\u523a\u6fc0\u6837\u672c\uff0c\u8fd9\u4e9b\u6837\u672c\u4e0e\u751f\u6210\u7684 62 \u901a\u9053\u8111\u7535\u4fe1\u53f7\uff08200 Hz\uff09\u548c\u60c5\u611f\u6807\u7b7e\u914d\u5bf9\uff0c\u4ece\u800c\u5b9e\u73b0\u89c6\u9891-\u8111\u7535\u5bf9\u9f50\u5e76\u63a8\u8fdb\u591a\u6a21\u6001\u7814\u7a76\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u60c5\u7eea\u5206\u6790\u3001\u6570\u636e\u589e\u5f3a\u548c\u8111\u673a\u63a5\u53e3\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u9896\u7684\u5de5\u5177\uff0c\u5177\u6709\u91cd\u8981\u7684\u7814\u7a76\u548c\u5de5\u7a0b\u610f\u4e49\u3002"}}
{"id": "2509.05449", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05449", "abs": "https://arxiv.org/abs/2509.05449", "authors": ["Disha Makhija", "Manoj Ghuhan Arivazhagan", "Vinayshekhar Bannihatti Kumar", "Rashmi Gangadharaiah"], "title": "Neural Breadcrumbs: Membership Inference Attacks on LLMs Through Hidden State and Attention Pattern Analysis", "comment": null, "summary": "Membership inference attacks (MIAs) reveal whether specific data was used to\ntrain machine learning models, serving as important tools for privacy auditing\nand compliance assessment. Recent studies have reported that MIAs perform only\nmarginally better than random guessing against large language models,\nsuggesting that modern pre-training approaches with massive datasets may be\nfree from privacy leakage risks. Our work offers a complementary perspective to\nthese findings by exploring how examining LLMs' internal representations,\nrather than just their outputs, may provide additional insights into potential\nmembership inference signals. Our framework, \\emph{memTrace}, follows what we\ncall \\enquote{neural breadcrumbs} extracting informative signals from\ntransformer hidden states and attention patterns as they process candidate\nsequences. By analyzing layer-wise representation dynamics, attention\ndistribution characteristics, and cross-layer transition patterns, we detect\npotential memorization fingerprints that traditional loss-based approaches may\nnot capture. This approach yields strong membership detection across several\nmodel families achieving average AUC scores of 0.85 on popular MIA benchmarks.\nOur findings suggest that internal model behaviors can reveal aspects of\ntraining data exposure even when output-based signals appear protected,\nhighlighting the need for further research into membership privacy and the\ndevelopment of more robust privacy-preserving training techniques for large\nlanguage models.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6210\u5458\u63a8\u7406\u653b\u51fb\uff0c\u53d1\u73b0\u901a\u8fc7\u5206\u6790\u6a21\u578b\u7684\u5185\u90e8\u8868\u793a\uff08\u9690\u85cf\u72b6\u6001\u548c\u6ce8\u610f\u529b\u6a21\u5f0f\uff09\u53ef\u4ee5\u6709\u6548\u68c0\u6d4b\u8bad\u7ec3\u6570\u636e\u662f\u5426\u88ab\u6a21\u578b\u8bb0\u4f4f\u3002", "motivation": "\u73b0\u6709\u7684\u6210\u5458\u63a8\u7406\u653b\u51fb\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6548\u679c\u4e0d\u4f73\uff0c\u6697\u793a\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u65b9\u6cd5\u53ef\u80fd\u4e0d\u5b58\u5728\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7814\u7a76LLMs\u7684\u5185\u90e8\u8868\u793a\uff0c\u63a2\u7d22\u6f5c\u5728\u7684\u6210\u5458\u63a8\u7406\u4fe1\u53f7\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u540d\u4e3amemTrace\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u63d0\u53d6transformer\u9690\u85cf\u72b6\u6001\u548c\u6ce8\u610f\u529b\u6a21\u5f0f\u4e2d\u7684\u4fe1\u606f\u4fe1\u53f7\uff0c\u5206\u6790\u5c42\u95f4\u8868\u793a\u52a8\u6001\u3001\u6ce8\u610f\u529b\u5206\u5e03\u7279\u5f81\u548c\u8de8\u5c42\u8f6c\u6362\u6a21\u5f0f\u6765\u68c0\u6d4b\u6f5c\u5728\u7684\u8bb0\u5fc6\u6307\u7eb9\u3002", "result": "memTrace\u5728\u591a\u4e2a\u6a21\u578b\u7cfb\u5217\u4e0a\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u6210\u5458\u68c0\u6d4b\uff0c\u5728\u6d41\u884c\u7684MIA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747AUC\u5f97\u5206\u4e3a0.85\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u57fa\u4e8e\u8f93\u51fa\u7684\u4fe1\u53f7\u53d7\u5230\u4fdd\u62a4\uff0c\u5185\u90e8\u6a21\u578b\u884c\u4e3a\u4e5f\u53ef\u4ee5\u63ed\u793a\u8bad\u7ec3\u6570\u636e\u66b4\u9732\u7684\u65b9\u9762\uff0c\u5f3a\u8c03\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u6210\u5458\u9690\u79c1\u4ee5\u53ca\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u9690\u79c1\u4fdd\u62a4\u8bad\u7ec3\u6280\u672f\u3002"}}
{"id": "2509.06472", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.06472", "abs": "https://arxiv.org/abs/2509.06472", "authors": ["Haoxiang Jin", "Ronghan Li", "Qiguang Miao", "Zixiang Lu"], "title": "Rethinking LLM Parametric Knowledge as Post-retrieval Confidence for Dynamic Retrieval and Reranking", "comment": null, "summary": "Large Language Models (LLMs) often generate inaccurate responses\n(hallucinations) when faced with questions beyond their knowledge scope.\nRetrieval-Augmented Generation (RAG) addresses this by leveraging external\nknowledge, but a critical challenge remains: determining whether retrieved\ncontexts effectively enhance the model`s ability to answer specific queries.\nThis challenge underscores the importance of knowledge boundary awareness,\nwhich current methods-relying on discrete labels or limited signals-fail to\naddress adequately, as they overlook the rich information in LLMs` continuous\ninternal hidden states. To tackle this, we propose a novel post-retrieval\nknowledge filtering approach. First, we construct a confidence detection model\nbased on LLMs` internal hidden states to quantify how retrieved contexts\nenhance the model`s confidence. Using this model, we build a preference dataset\n(NQ_Rerank) to fine-tune a reranker, enabling it to prioritize contexts\npreferred by the downstream LLM during reranking. Additionally, we introduce\nConfidence-Based Dynamic Retrieval (CBDR), which adaptively triggers retrieval\nbased on the LLM`s initial confidence in the original question, reducing\nknowledge conflicts and improving efficiency. Experimental results demonstrate\nsignificant improvements in accuracy for context screening and end-to-end RAG\nperformance, along with a notable reduction in retrieval costs while\nmaintaining competitive accuracy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u540e\u68c0\u7d22\u77e5\u8bc6\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4e2d\u77e5\u8bc6\u8fb9\u754c\u611f\u77e5\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5229\u7528LLM\u7684\u5185\u90e8\u9690\u85cf\u72b6\u6001\u6784\u5efa\u7f6e\u4fe1\u5ea6\u68c0\u6d4b\u6a21\u578b\uff0c\u5e76 fine-tune \u4e00\u4e2a reranker \u6765\u4f18\u5148\u6392\u5e8f\u4e0b\u6e38 LLM \u66f4\u559c\u6b22\u7684\u4e0a\u4e0b\u6587\uff0c\u540c\u65f6\u5f15\u5165\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u52a8\u6001\u68c0\u7d22\uff08CBDR\uff09\u4ee5\u81ea\u9002\u5e94\u5730\u89e6\u53d1\u68c0\u7d22\uff0c\u4ece\u800c\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u79bb\u6563\u6807\u7b7e\u6216\u6709\u9650\u4fe1\u53f7\uff0c\u5ffd\u7565\u4e86 LLM \u8fde\u7eed\u5185\u90e8\u9690\u85cf\u72b6\u6001\u4e2d\u7684\u4e30\u5bcc\u4fe1\u606f\uff0c\u5bfc\u81f4\u77e5\u8bc6\u8fb9\u754c\u611f\u77e5\u4e0d\u8db3\u3002", "method": "1. \u6784\u5efa\u57fa\u4e8e LLM \u5185\u90e8\u9690\u85cf\u72b6\u6001\u7684\u7f6e\u4fe1\u5ea6\u68c0\u6d4b\u6a21\u578b\uff0c\u4ee5\u91cf\u5316\u68c0\u7d22\u5230\u7684\u4e0a\u4e0b\u6587\u5982\u4f55\u589e\u5f3a\u6a21\u578b\u7684\u7f6e\u4fe1\u5ea6\u3002\n2. \u4f7f\u7528\u8be5\u6a21\u578b\u6784\u5efa\u504f\u597d\u6570\u636e\u96c6 (NQ_Rerank) \u4ee5 fine-tune reranker\uff0c\u4f7f\u5176\u80fd\u591f\u4f18\u5148\u6392\u5e8f\u4e0b\u6e38 LLM \u66f4\u559c\u6b22\u7684\u4e0a\u4e0b\u6587\u3002\n3. \u5f15\u5165\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u52a8\u6001\u68c0\u7d22 (CBDR)\uff0c\u5b83\u6839\u636e LLM \u5bf9\u539f\u59cb\u95ee\u9898\u7684\u521d\u59cb\u7f6e\u4fe1\u5ea6\u81ea\u9002\u5e94\u5730\u89e6\u53d1\u68c0\u7d22\u3002", "result": "\u5728\u4e0a\u4e0b\u6587\u7b5b\u9009\u548c\u7aef\u5230\u7aef RAG \u6027\u80fd\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u7740\u6539\u8fdb\uff0c\u540c\u65f6\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\u663e\u7740\u964d\u4f4e\u4e86\u68c0\u7d22\u6210\u672c\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u9ad8 RAG \u7cfb\u7edf\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5e76\u964d\u4f4e\u68c0\u7d22\u6210\u672c\u3002"}}
{"id": "2509.05425", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05425", "abs": "https://arxiv.org/abs/2509.05425", "authors": ["Jessica M. Lundin", "Ada Zhang", "David Adelani", "Cody Carroll"], "title": "No Translation Needed: Forecasting Quality from Fertility and Metadata", "comment": null, "summary": "We show that translation quality can be predicted with surprising accuracy\n\\textit{without ever running the translation system itself}. Using only a\nhandful of features, token fertility ratios, token counts, and basic linguistic\nmetadata (language family, script, and region), we can forecast ChrF scores for\nGPT-4o translations across 203 languages in the FLORES-200 benchmark. Gradient\nboosting models achieve favorable performance ($R^{2}=0.66$ for\nXX$\\rightarrow$English and $R^{2}=0.72$ for English$\\rightarrow$XX). Feature\nimportance analyses reveal that typological factors dominate predictions into\nEnglish, while fertility plays a larger role for translations into diverse\ntarget languages. These findings suggest that translation quality is shaped by\nboth token-level fertility and broader linguistic typology, offering new\ninsights for multilingual evaluation and quality estimation.", "AI": {"tldr": "\u4ec5\u7528\u5c11\u91cf\u7279\u5f81\uff08token fertility ratios, token counts, \u8bed\u8a00\u5143\u6570\u636e\uff09\u5c31\u80fd\u9884\u6d4b\u7ffb\u8bd1\u8d28\u91cf\uff0c\u65e0\u9700\u8fd0\u884c\u7ffb\u8bd1\u7cfb\u7edf\u3002", "motivation": "\u63a2\u7d22\u5728\u4e0d\u8fd0\u884c\u7ffb\u8bd1\u7cfb\u7edf\u672c\u8eab\u7684\u60c5\u51b5\u4e0b\u9884\u6d4b\u7ffb\u8bd1\u8d28\u91cf\u7684\u53ef\u80fd\u6027\u3002", "method": "\u4f7f\u7528\u68af\u5ea6\u63d0\u5347\u6a21\u578b\uff0c\u57fa\u4e8etoken fertility ratios, token counts\u548c\u8bed\u8a00\u5b66\u5143\u6570\u636e\u9884\u6d4bGPT-4o\u5728FLORES-200 benchmark\u4e0a\u7684ChrF scores\u3002", "result": "\u5bf9\u4e8eXX->\u82f1\u8bed\u7684\u7ffb\u8bd1\uff0cR^2=0.66\uff1b\u5bf9\u4e8e\u82f1\u8bed->XX\u7684\u7ffb\u8bd1\uff0cR^2=0.72\u3002\u7c7b\u578b\u5b66\u56e0\u7d20\u4e3b\u5bfc\u4e86\u5bf9\u82f1\u8bed\u7684\u9884\u6d4b\uff0c\u800cfertility\u5728\u5bf9\u4e0d\u540c\u76ee\u6807\u8bed\u8a00\u7684\u7ffb\u8bd1\u4e2d\u8d77\u7740\u66f4\u5927\u7684\u4f5c\u7528\u3002", "conclusion": "\u7ffb\u8bd1\u8d28\u91cf\u53d7\u5230token-level fertility\u548c\u66f4\u5e7f\u6cdb\u7684\u8bed\u8a00\u7c7b\u578b\u5b66\u7684\u5f71\u54cd\uff0c\u4e3a\u591a\u8bed\u8a00\u8bc4\u4f30\u548c\u8d28\u91cf\u4f30\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2509.05346", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05346", "abs": "https://arxiv.org/abs/2509.05346", "authors": ["Bo Yuan", "Jiazi Hu"], "title": "Benchmarking Large Language Models for Personalized Guidance in AI-Enhanced Learning", "comment": null, "summary": "While Large Language Models (LLMs) are increasingly envisioned as intelligent\nassistants for personalized learning, systematic head-to-head evaluations\nwithin authentic learning scenarios remain limited. This study conducts an\nempirical comparison of three state-of-the-art LLMs on a tutoring task that\nsimulates a realistic learning setting. Using a dataset comprising a student's\nanswers to ten questions of mixed formats with correctness labels, each LLM is\nrequired to (i) analyze the quiz to identify underlying knowledge components,\n(ii) infer the student's mastery profile, and (iii) generate targeted guidance\nfor improvement. To mitigate subjectivity and evaluator bias, we employ Gemini\nas a virtual judge to perform pairwise comparisons along various dimensions:\naccuracy, clarity, actionability, and appropriateness. Results analyzed via the\nBradley-Terry model indicate that GPT-4o is generally preferred, producing\nfeedback that is more informative and better structured than its counterparts,\nwhile DeepSeek-V3 and GLM-4.5 demonstrate intermittent strengths but lower\nconsistency. These findings highlight the feasibility of deploying LLMs as\nadvanced teaching assistants for individualized support and provide\nmethodological guidance for future empirical research on LLM-driven\npersonalized learning.", "AI": {"tldr": "\u672c\u7814\u7a76\u5bf9\u6bd4\u4e86\u4e09\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6a21\u62df\u771f\u5b9e\u5b66\u4e60\u573a\u666f\u4e0b\u7684\u8f85\u5bfc\u4efb\u52a1\u8868\u73b0\uff0c\u4f7f\u7528 Gemini \u4f5c\u4e3a\u865a\u62df\u88c1\u5224\u8fdb\u884c\u8bc4\u4f30\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u88ab\u8bbe\u60f3\u4e3a\u4e2a\u6027\u5316\u5b66\u4e60\u7684\u667a\u80fd\u52a9\u624b\uff0c\u4f46\u5728\u771f\u5b9e\u7684\u5b66\u4e60\u573a\u666f\u4e2d\uff0c\u7cfb\u7edf\u6027\u7684\u5bf9\u6bd4\u8bc4\u4f30\u4ecd\u7136\u6709\u9650\u3002", "method": "\u4f7f\u7528\u5305\u542b\u5b66\u751f\u5bf9\u5341\u4e2a\u6df7\u5408\u5f62\u5f0f\u95ee\u9898\u7684\u7b54\u6848\u548c\u6b63\u786e\u6027\u6807\u7b7e\u7684\u6570\u636e\u96c6\uff0c\u8981\u6c42\u6bcf\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5206\u6790\u6d4b\u9a8c\u4ee5\u8bc6\u522b\u6f5c\u5728\u7684\u77e5\u8bc6\u7ec4\u6210\u90e8\u5206\uff0c\u63a8\u65ad\u5b66\u751f\u7684\u638c\u63e1\u60c5\u51b5\uff0c\u5e76\u751f\u6210\u6709\u9488\u5bf9\u6027\u7684\u6539\u8fdb\u6307\u5bfc\u3002\u4f7f\u7528 Gemini \u4f5c\u4e3a\u865a\u62df\u88c1\u5224\uff0c\u6cbf\u7740\u51c6\u786e\u6027\u3001\u6e05\u6670\u6027\u3001\u53ef\u64cd\u4f5c\u6027\u548c\u9002\u5f53\u6027\u7b49\u7ef4\u5ea6\u8fdb\u884c\u6210\u5bf9\u6bd4\u8f83\u3002", "result": "GPT-4o \u5728\u603b\u4f53\u4e0a\u66f4\u53d7\u6b22\u8fce\uff0c\u4ea7\u751f\u7684\u53cd\u9988\u6bd4\u5176\u4ed6\u6a21\u578b\u66f4\u5177\u4fe1\u606f\u91cf\u548c\u7ed3\u6784\u6027\uff0c\u800c DeepSeek-V3 \u548c GLM-4.5 \u8868\u73b0\u51fa\u95f4\u6b47\u6027\u7684\u4f18\u52bf\uff0c\u4f46\u4e00\u81f4\u6027\u8f83\u4f4e\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u4e2a\u6027\u5316\u652f\u6301\u7684\u9ad8\u7ea7\u52a9\u6559\u662f\u53ef\u884c\u7684\uff0c\u5e76\u4e3a\u672a\u6765\u5173\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u4e2a\u6027\u5316\u5b66\u4e60\u7684\u5b9e\u8bc1\u7814\u7a76\u63d0\u4f9b\u65b9\u6cd5\u6307\u5bfc\u3002"}}
{"id": "2509.05322", "categories": ["cs.CV", "cs.LG", "cs.SI", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2509.05322", "abs": "https://arxiv.org/abs/2509.05322", "authors": ["Pavithra Elumalai", "Sudharsan Vijayaraghavan", "Madhumita Mondal", "Areejit Samal"], "title": "Application of discrete Ricci curvature in pruning randomly wired neural networks: A case study with chest x-ray classification of COVID-19", "comment": "21 pages, 4 figures, 9 tables", "summary": "Randomly Wired Neural Networks (RWNNs) serve as a valuable testbed for\ninvestigating the impact of network topology in deep learning by capturing how\ndifferent connectivity patterns impact both learning efficiency and model\nperformance. At the same time, they provide a natural framework for exploring\nedge-centric network measures as tools for pruning and optimization. In this\nstudy, we investigate three edge-centric network measures: Forman-Ricci\ncurvature (FRC), Ollivier-Ricci curvature (ORC), and edge betweenness\ncentrality (EBC), to compress RWNNs by selectively retaining important synapses\n(or edges) while pruning the rest. As a baseline, RWNNs are trained for\nCOVID-19 chest x-ray image classification, aiming to reduce network complexity\nwhile preserving performance in terms of accuracy, specificity, and\nsensitivity. We extend prior work on pruning RWNN using ORC by incorporating\ntwo additional edge-centric measures, FRC and EBC, across three network\ngenerators: Erd\\\"{o}s-R\\'{e}nyi (ER) model, Watts-Strogatz (WS) model, and\nBarab\\'{a}si-Albert (BA) model. We provide a comparative analysis of the\npruning performance of the three measures in terms of compression ratio and\ntheoretical speedup. A central focus of our study is to evaluate whether FRC,\nwhich is computationally more efficient than ORC, can achieve comparable\npruning effectiveness. Along with performance evaluation, we further\ninvestigate the structural properties of the pruned networks through modularity\nand global efficiency, offering insights into the trade-off between modular\nsegregation and network efficiency in compressed RWNNs. Our results provide\ninitial evidence that FRC-based pruning can effectively simplify RWNNs,\noffering significant computational advantages while maintaining performance\ncomparable to ORC.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4f7f\u7528 Forman-Ricci \u66f2\u7387 (FRC)\u3001Ollivier-Ricci \u66f2\u7387 (ORC) \u548c\u8fb9\u4ecb\u6570\u4e2d\u5fc3\u6027 (EBC) \u6765\u538b\u7f29\u968f\u673a\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc (RWNN) \u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5730\u4fdd\u7559\u91cd\u8981\u7684\u7a81\u89e6\uff08\u6216\u8fb9\uff09\u540c\u65f6\u4fee\u526a\u5176\u4f59\u90e8\u5206\u3002", "motivation": "\u901a\u8fc7\u6355\u83b7\u4e0d\u540c\u7684\u8fde\u63a5\u6a21\u5f0f\u5982\u4f55\u5f71\u54cd\u5b66\u4e60\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\uff0c\u968f\u673a\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc (RWNN) \u53ef\u4ee5\u4f5c\u4e3a\u7814\u7a76\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7f51\u7edc\u62d3\u6251\u5f71\u54cd\u7684\u91cd\u8981\u8bd5\u9a8c\u53f0\u3002\u540c\u65f6\uff0c\u5b83\u4eec\u4e3a\u63a2\u7d22\u4ee5\u8fb9\u7f18\u4e3a\u4e2d\u5fc3\u7684\u7f51\u7edc\u5ea6\u91cf\u4f5c\u4e3a\u4fee\u526a\u548c\u4f18\u5316\u7684\u5de5\u5177\u63d0\u4f9b\u4e86\u4e00\u4e2a\u81ea\u7136\u7684\u6846\u67b6\u3002", "method": "\u5728 COVID-19 \u80f8\u90e8 X \u5c04\u7ebf\u56fe\u50cf\u5206\u7c7b\u4e0a\u8bad\u7ec3 RWNN\uff0c\u65e8\u5728\u964d\u4f4e\u7f51\u7edc\u590d\u6742\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3001\u7279\u5f02\u6027\u548c\u7075\u654f\u5ea6\u65b9\u9762\u7684\u6027\u80fd\u3002\u901a\u8fc7\u7ed3\u5408\u53e6\u5916\u4e24\u4e2a\u4ee5\u8fb9\u7f18\u4e3a\u4e2d\u5fc3\u7684\u5ea6\u91cf FRC \u548c EBC\uff0c\u6269\u5c55\u4e86\u5148\u524d\u5173\u4e8e\u4f7f\u7528 ORC \u4fee\u526a RWNN \u7684\u5de5\u4f5c\uff0c\u8de8\u8d8a\u4e09\u4e2a\u7f51\u7edc\u751f\u6210\u5668\uff1aErd\u0151s-R\u00e9nyi (ER) \u6a21\u578b\u3001Watts-Strogatz (WS) \u6a21\u578b\u548c Barab\u00e1si-Albert (BA) \u6a21\u578b\u3002", "result": "\u7ed3\u679c\u521d\u6b65\u8bc1\u660e\uff0c\u57fa\u4e8e FRC \u7684\u4fee\u526a\u53ef\u4ee5\u6709\u6548\u5730\u7b80\u5316 RWNN\uff0c\u63d0\u4f9b\u663e\u7740\u7684\u8ba1\u7b97\u4f18\u52bf\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e ORC \u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "FRC-based \u7684\u4fee\u526a\u53ef\u4ee5\u6709\u6548\u5730\u7b80\u5316 RWNN\uff0c\u63d0\u4f9b\u663e\u7740\u7684\u8ba1\u7b97\u4f18\u52bf\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e ORC \u76f8\u5f53\u7684\u6027\u80fd\u3002"}}
{"id": "2509.05460", "categories": ["cs.LG", "cs.IR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.05460", "abs": "https://arxiv.org/abs/2509.05460", "authors": ["Diego Feijer", "Himan Abdollahpouri", "Sanket Gupta", "Alexander Clare", "Yuxiao Wen", "Todd Wasson", "Maria Dimakopoulou", "Zahra Nazari", "Kyle Kretschman", "Mounia Lalmas"], "title": "Calibrated Recommendations with Contextual Bandits", "comment": "Accepted at ACM RecSys '25, CONSEQUENCES workshop", "summary": "Spotify's Home page features a variety of content types, including music,\npodcasts, and audiobooks. However, historical data is heavily skewed toward\nmusic, making it challenging to deliver a balanced and personalized content\nmix. Moreover, users' preference towards different content types may vary\ndepending on the time of day, the day of week, or even the device they use. We\npropose a calibration method that leverages contextual bandits to dynamically\nlearn each user's optimal content type distribution based on their context and\npreferences. Unlike traditional calibration methods that rely on historical\naverages, our approach boosts engagement by adapting to how users interests in\ndifferent content types varies across contexts. Both offline and online results\ndemonstrate improved precision and user engagement with the Spotify Home page,\nin particular with under-represented content types such as podcasts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u4e0a\u4e0b\u6587Bandit\u7b97\u6cd5\u52a8\u6001\u5b66\u4e60\u7528\u6237\u6700\u4f73\u5185\u5bb9\u7c7b\u578b\u5206\u5e03\u7684\u6821\u51c6\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3Spotify\u4e3b\u9875\u5185\u5bb9\u7c7b\u578b\u4e0d\u5e73\u8861\u548c\u7528\u6237\u504f\u597d\u968f\u60c5\u5883\u53d8\u5316\u7684\u95ee\u9898\u3002", "motivation": "\u5386\u53f2\u6570\u636e\u4e25\u91cd\u504f\u5411\u97f3\u4e50\uff0c\u96be\u4ee5\u63d0\u4f9b\u5e73\u8861\u548c\u4e2a\u6027\u5316\u7684\u5185\u5bb9\u7ec4\u5408\uff1b\u7528\u6237\u5bf9\u4e0d\u540c\u5185\u5bb9\u7c7b\u578b\u7684\u504f\u597d\u53ef\u80fd\u968f\u65f6\u95f4\u3001\u65e5\u671f\u751a\u81f3\u8bbe\u5907\u800c\u53d8\u5316\u3002", "method": "\u5229\u7528\u4e0a\u4e0b\u6587Bandit\u7b97\u6cd5\u52a8\u6001\u5b66\u4e60\u6bcf\u4e2a\u7528\u6237\u57fa\u4e8e\u60c5\u5883\u548c\u504f\u597d\u7684\u6700\u4f73\u5185\u5bb9\u7c7b\u578b\u5206\u5e03\u3002", "result": "\u79bb\u7ebf\u548c\u5728\u7ebf\u7ed3\u679c\u8868\u660e\uff0cSpotify\u4e3b\u9875\u7684\u7cbe\u786e\u5ea6\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\u5f97\u5230\u4e86\u63d0\u9ad8\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u5185\u5bb9\u7c7b\u578b\uff08\u5982\u64ad\u5ba2\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u9002\u5e94\u7528\u6237\u5728\u4e0d\u540c\u60c5\u5883\u4e0b\u5bf9\u4e0d\u540c\u5185\u5bb9\u7c7b\u578b\u7684\u5174\u8da3\u53d8\u5316\uff0c\u4ece\u800c\u63d0\u9ad8\u7528\u6237\u53c2\u4e0e\u5ea6\u3002"}}
{"id": "2509.06544", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.06544", "abs": "https://arxiv.org/abs/2509.06544", "authors": ["Yunfei Zhong", "Jun Yang", "Yixing Fan", "Jiafeng Guo", "Lixin Su", "Maarten de Rijke", "Ruqing Zhang", "Dawei Yin", "Xueqi Cheng"], "title": "Reasoning-enhanced Query Understanding through Decomposition and Interpretation", "comment": null, "summary": "Accurate inference of user intent is crucial for enhancing document retrieval\nin modern search engines. While large language models (LLMs) have made\nsignificant strides in this area, their effectiveness has predominantly been\nassessed with short, keyword-based queries. As AI-driven search evolves,\nlong-form queries with intricate intents are becoming more prevalent, yet they\nremain underexplored in the context of LLM-based query understanding (QU). To\nbridge this gap, we introduce ReDI: a Reasoning-enhanced approach for query\nunderstanding through Decomposition and Interpretation. ReDI leverages the\nreasoning and comprehension capabilities of LLMs in a three-stage pipeline: (i)\nit breaks down complex queries into targeted sub-queries to accurately capture\nuser intent; (ii) it enriches each sub-query with detailed semantic\ninterpretations to improve the query-document matching; and (iii) it\nindependently retrieves documents for each sub-query and employs a fusion\nstrategy to aggregate the results for the final ranking. We compiled a\nlarge-scale dataset of real-world complex queries from a major search engine\nand distilled the query understanding capabilities of teacher models into\nsmaller models for practical application. Experiments on BRIGHT and BEIR\ndemonstrate that ReDI consistently surpasses strong baselines in both sparse\nand dense retrieval paradigms, affirming its effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aReDI\u7684\uff0c\u57fa\u4e8eLLM\u7684\uff0c\u589e\u5f3a\u63a8\u7406\u7684\u67e5\u8be2\u7406\u89e3\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u957fquery\u548c\u590d\u6742intent\u3002", "motivation": "\u73b0\u6709LLM\u5728\u5904\u7406\u77edquery\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u957fquery\u548c\u590d\u6742intent\u4e0a\u7684\u6548\u679c\u6709\u5f85\u63d0\u9ad8\u3002", "method": "ReDI \u901a\u8fc7\u5206\u89e3query\uff0c\u8fdb\u884c\u8bed\u4e49\u89e3\u91ca\uff0c\u5e76\u878d\u5408\u68c0\u7d22\u7ed3\u679c\u6765\u7406\u89e3query\u3002", "result": "\u5728BRIGHT\u548cBEIR\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cReDI \u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ReDI \u5728\u7a00\u758f\u548c\u7a20\u5bc6\u68c0\u7d22\u8303\u4f8b\u4e2d\u5747\u6709\u6548\u3002"}}
{"id": "2509.05899", "categories": ["cs.LG", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.05899", "abs": "https://arxiv.org/abs/2509.05899", "authors": ["Dazhi Peng"], "title": "X-SQL: Expert Schema Linking and Understanding of Text-to-SQL with Multi-LLMs", "comment": null, "summary": "With Large Language Models' (LLMs) emergent abilities on code generation\ntasks, Text-to-SQL has become one of the most popular downstream applications.\nDespite the strong results of multiple recent LLM-based Text-to-SQL frameworks,\nthe research community often overlooks the importance of database schema\ninformation for generating high-quality SQL queries. We find that such schema\ninformation plays a significant or even dominant role in the Text-to-SQL task.\nTo tackle this challenge, we propose a novel database schema expert with two\ncomponents. We first introduce X-Linking, an LLM Supervised Finetuning\n(SFT)-based method that achieves superior Schema Linking results compared to\nexisting open-source Text-to-SQL methods. In addition, we innovatively propose\nan X-Admin component that focuses on Schema Understanding by bridging the gap\nbetween abstract schema information and the user's natural language question.\nAside from better learning with schema information, we experiment with\nMulti-LLMs for different components within the system to further boost its\nperformance. By incorporating these techniques into our end-to-end framework,\nX-SQL, we have achieved Execution Accuracies of 84.9% on the Spider-Dev dataset\nand 82.5% on the Spider-Test dataset. This outstanding performance establishes\nX-SQL as the leading Text-to-SQL framework based on open-source models.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aX-SQL\u7684Text-to-SQL\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u66f4\u6709\u6548\u5730\u5229\u7528\u6570\u636e\u5e93schema\u4fe1\u606f\uff0c\u5e76\u5728\u7cfb\u7edf\u4e2d\u4f7f\u7528Multi-LLMs\uff0c\u5728Spider\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u9886\u5148\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709Text-to-SQL\u6846\u67b6\u5ffd\u7565\u4e86\u6570\u636e\u5e93schema\u4fe1\u606f\u5728\u751f\u6210\u9ad8\u8d28\u91cfSQL\u67e5\u8be2\u4e2d\u7684\u91cd\u8981\u6027\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u6570\u636e\u5e93schema\u4e13\u5bb6\uff0c\u5305\u542bX-Linking\u548cX-Admin\u4e24\u4e2a\u7ec4\u4ef6\uff0c\u5206\u522b\u7528\u4e8eSchema Linking\u548cSchema Understanding\u3002\u540c\u65f6\uff0c\u8bba\u6587\u8fd8\u5c1d\u8bd5\u4f7f\u7528Multi-LLMs\u6765\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002", "result": "X-SQL\u5728Spider-Dev\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e8684.9%\u7684Execution Accuracy\uff0c\u5728Spider-Test\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e8682.5%\u7684Execution Accuracy\u3002", "conclusion": "X-SQL\u662f\u76ee\u524d\u57fa\u4e8e\u5f00\u6e90\u6a21\u578b\u7684\u9886\u5148Text-to-SQL\u6846\u67b6\u3002"}}
{"id": "2509.05440", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05440", "abs": "https://arxiv.org/abs/2509.05440", "authors": ["Logan Lawrence", "Ashton Williamson", "Alexander Shelton"], "title": "Direct-Scoring NLG Evaluators Can Use Pairwise Comparisons Too", "comment": "12 pages, 18 tables, 1 figure", "summary": "As large-language models have been increasingly used as automatic raters for\nevaluating free-form content, including document summarization, dialog, and\nstory generation, work has been dedicated to evaluating such models by\nmeasuring their correlations with human judgment. For \\textit{sample-level}\nperformance, methods which operate by using pairwise comparisons between\nmachine-generated text perform well but often lack the ability to assign\nabsolute scores to individual summaries, an ability crucial for use cases that\nrequire thresholding. In this work, we propose a direct-scoring method which\nuses synthetic summaries to act as pairwise machine rankings at test time. We\nshow that our method performs comparably to state-of-the-art pairwise\nevaluators in terms of axis-averaged sample-level correlations on the SummEval\n(\\textbf{+0.03}), TopicalChat (\\textbf{-0.03}), and HANNA (\\textbf{+0.05})\nmeta-evaluation benchmarks, and release the synthetic in-context summaries as\ndata to facilitate future work.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u76f4\u63a5\u8bc4\u5206\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528\u5408\u6210\u6458\u8981\u4f5c\u4e3a\u6d4b\u8bd5\u65f6\u7684\u6210\u5bf9\u673a\u5668\u6392\u5e8f\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4f5c\u8bc4\u4f30\u6587\u6863\u6458\u8981\u3001\u5bf9\u8bdd\u548c\u6545\u4e8b\u751f\u6210\u7b49\u81ea\u7531\u5f62\u5f0f\u5185\u5bb9\u7684\u81ea\u52a8\u8bc4\u4f30\u5668\uff0c\u56e0\u6b64\u9700\u8981\u901a\u8fc7\u6d4b\u91cf\u5b83\u4eec\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u76f8\u5173\u6027\u6765\u8bc4\u4f30\u6b64\u7c7b\u6a21\u578b\u3002", "method": "\u8be5\u65b9\u6cd5\u4f7f\u7528\u5408\u6210\u6458\u8981\u4f5c\u4e3a\u6d4b\u8bd5\u65f6\u7684\u6210\u5bf9\u673a\u5668\u6392\u540d\u3002", "result": "\u8be5\u65b9\u6cd5\u5728 SummEval (+0.03)\u3001TopicalChat (-0.03) \u548c HANNA (+0.05) \u5143\u8bc4\u4f30\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u8f74\u5e73\u5747\u6837\u672c\u7ea7\u522b\u76f8\u5173\u6027\u65b9\u9762\u4e0e\u6700\u5148\u8fdb\u7684\u6210\u5bf9\u8bc4\u4f30\u5668\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "\u53d1\u5e03\u5408\u6210\u7684\u4e0a\u4e0b\u6587\u6458\u8981\u4f5c\u4e3a\u6570\u636e\uff0c\u4ee5\u4fc3\u8fdb\u672a\u6765\u7684\u5de5\u4f5c\u3002"}}
{"id": "2509.05363", "categories": ["cs.AI", "cond-mat.mtrl-sci", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.05363", "abs": "https://arxiv.org/abs/2509.05363", "authors": ["Lijie Ding", "Changwoo Do"], "title": "SasAgent: Multi-Agent AI System for Small-Angle Scattering Data Analysis", "comment": "8 pages, 7 figures", "summary": "We introduce SasAgent, a multi-agent AI system powered by large language\nmodels (LLMs) that automates small-angle scattering (SAS) data analysis by\nleveraging tools from the SasView software and enables user interaction via\ntext input. SasAgent features a coordinator agent that interprets user prompts\nand delegates tasks to three specialized agents for scattering length density\n(SLD) calculation, synthetic data generation, and experimental data fitting.\nThese agents utilize LLM-friendly tools to execute tasks efficiently. These\ntools, including the model data tool, Retrieval-Augmented Generation (RAG)\ndocumentation tool, bump fitting tool, and SLD calculator tool, are derived\nfrom the SasView Python library. A user-friendly Gradio-based interface\nenhances user accessibility. Through diverse examples, we demonstrate\nSasAgent's ability to interpret complex prompts, calculate SLDs, generate\naccurate scattering data, and fit experimental datasets with high precision.\nThis work showcases the potential of LLM-driven AI systems to streamline\nscientific workflows and enhance automation in SAS research.", "AI": {"tldr": "SasAgent\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\uff0c\u5b83\u53ef\u4ee5\u81ea\u52a8\u5206\u6790\u5c0f\u89d2\u5ea6\u6563\u5c04\uff08SAS\uff09\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u6587\u672c\u8f93\u5165\u5b9e\u73b0\u7528\u6237\u4ea4\u4e92\u3002", "motivation": "\u8be5\u8bba\u6587\u65e8\u5728\u5229\u7528LLM\u9a71\u52a8\u7684AI\u7cfb\u7edf\u6765\u7b80\u5316\u79d1\u5b66\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5e76\u52a0\u5f3aSAS\u7814\u7a76\u4e2d\u7684\u81ea\u52a8\u5316\u3002", "method": "\u8bba\u6587\u4ecb\u7ecd\u4e86SasAgent\uff0c\u5b83\u5305\u542b\u4e00\u4e2a\u534f\u8c03\u4ee3\u7406\u548c\u4e09\u4e2a\u4e13\u4e1a\u4ee3\u7406\uff0c\u5206\u522b\u8d1f\u8d23\u6563\u5c04\u957f\u5ea6\u5bc6\u5ea6\uff08SLD\uff09\u8ba1\u7b97\u3001\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u5b9e\u9a8c\u6570\u636e\u62df\u5408\u3002\u8fd9\u4e9b\u4ee3\u7406\u5229\u7528LLM\u53cb\u597d\u7684\u5de5\u5177\u6765\u9ad8\u6548\u6267\u884c\u4efb\u52a1\u3002\u8fd9\u4e9b\u5de5\u5177\u5305\u62ec\u6a21\u578b\u6570\u636e\u5de5\u5177\u3001RAG\u6587\u6863\u5de5\u5177\u3001bump\u62df\u5408\u5de5\u5177\u548cSLD\u8ba1\u7b97\u5668\u5de5\u5177\uff0c\u5b83\u4eec\u90fd\u6e90\u81eaSasView Python\u5e93\u3002", "result": "\u8bba\u6587\u901a\u8fc7\u591a\u79cd\u793a\u4f8b\u5c55\u793a\u4e86SasAgent\u89e3\u91ca\u590d\u6742\u63d0\u793a\u3001\u8ba1\u7b97SLD\u3001\u751f\u6210\u51c6\u786e\u6563\u5c04\u6570\u636e\u548c\u9ad8\u7cbe\u5ea6\u62df\u5408\u5b9e\u9a8c\u6570\u636e\u96c6\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86LLM\u9a71\u52a8\u7684AI\u7cfb\u7edf\u5728\u7b80\u5316\u79d1\u5b66\u5de5\u4f5c\u6d41\u7a0b\u548c\u52a0\u5f3aSAS\u7814\u7a76\u81ea\u52a8\u5316\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.05329", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05329", "abs": "https://arxiv.org/abs/2509.05329", "authors": ["Juan Carlos Martinez-Sevilla", "Francesco Foscarin", "Patricia Garcia-Iasci", "David Rizo", "Jorge Calvo-Zaragoza", "Gerhard Widmer"], "title": "Optical Music Recognition of Jazz Lead Sheets", "comment": "Accepted at the 26th International Society for Music Information\n  Retrieval Conference (ISMIR), 2025", "summary": "In this paper, we address the challenge of Optical Music Recognition (OMR)\nfor handwritten jazz lead sheets, a widely used musical score type that encodes\nmelody and chords. The task is challenging due to the presence of chords, a\nscore component not handled by existing OMR systems, and the high variability\nand quality issues associated with handwritten images. Our contribution is\ntwo-fold. We present a novel dataset consisting of 293 handwritten jazz lead\nsheets of 163 unique pieces, amounting to 2021 total staves aligned with\nHumdrum **kern and MusicXML ground truth scores. We also supply synthetic score\nimages generated from the ground truth. The second contribution is the\ndevelopment of an OMR model for jazz lead sheets. We discuss specific\ntokenisation choices related to our kind of data, and the advantages of using\nsynthetic scores and pretrained models. We publicly release all code, data, and\nmodels.", "AI": {"tldr": "\u672c\u6587\u89e3\u51b3\u4e86\u624b\u5199\u7235\u58eb\u4e50\u8c31\u7684\u5149\u5b66\u97f3\u4e50\u8bc6\u522b(OMR)\u7684\u6311\u6218\uff0c\u8be5\u4e50\u8c31\u7c7b\u578b\u5305\u542b\u65cb\u5f8b\u548c\u548c\u5f26\u3002", "motivation": "\u73b0\u6709\u7684OMR\u7cfb\u7edf\u65e0\u6cd5\u5904\u7406\u5305\u542b\u548c\u5f26\u7684\u4e50\u8c31\uff0c\u5e76\u4e14\u624b\u5199\u56fe\u50cf\u5177\u6709\u9ad8\u5ea6\u7684\u53ef\u53d8\u6027\u548c\u8d28\u91cf\u95ee\u9898\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b293\u4efd\u624b\u5199\u7235\u58eb\u4e50\u8c31\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u7528\u4e8e\u7235\u58eb\u4e50\u8c31\u7684OMR\u6a21\u578b\u3002", "result": "\u53d1\u5e03\u4e86\u4ee3\u7801\u3001\u6570\u636e\u548c\u6a21\u578b\u3002", "conclusion": "\u672c\u6587\u4e3a\u7235\u58eb\u4e50\u8c31\u7684OMR\u505a\u51fa\u4e86\u8d21\u732e\u3002"}}
{"id": "2509.05478", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05478", "abs": "https://arxiv.org/abs/2509.05478", "authors": ["Jia Wang", "Xiao Wang", "Chi Zhang"], "title": "PLanTS: Periodicity-aware Latent-state Representation Learning for Multivariate Time Series", "comment": null, "summary": "Multivariate time series (MTS) are ubiquitous in domains such as healthcare,\nclimate science, and industrial monitoring, but their high dimensionality,\nlimited labeled data, and non-stationary nature pose significant challenges for\nconventional machine learning methods. While recent self-supervised learning\n(SSL) approaches mitigate label scarcity by data augmentations or time\npoint-based contrastive strategy, they neglect the intrinsic periodic structure\nof MTS and fail to capture the dynamic evolution of latent states. We propose\nPLanTS, a periodicity-aware self-supervised learning framework that explicitly\nmodels irregular latent states and their transitions. We first designed a\nperiod-aware multi-granularity patching mechanism and a generalized contrastive\nloss to preserve both instance-level and state-level similarities across\nmultiple temporal resolutions. To further capture temporal dynamics, we design\na next-transition prediction pretext task that encourages representations to\nencode predictive information about future state evolution. We evaluate PLanTS\nacross a wide range of downstream tasks-including multi-class and multi-label\nclassification, forecasting, trajectory tracking and anomaly detection. PLanTS\nconsistently improves the representation quality over existing SSL methods and\ndemonstrates superior runtime efficiency compared to DTW-based methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6PLanTS\uff0c\u7528\u4e8e\u5904\u7406\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u8be5\u6846\u67b6\u8003\u8651\u4e86\u65f6\u95f4\u5e8f\u5217\u7684\u5468\u671f\u6027\u7ed3\u6784\u548c\u52a8\u6001\u6f14\u5316\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u7ef4\u3001\u5c11\u6807\u7b7e\u548c\u975e\u5e73\u7a33\u7684\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u65f6\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5ffd\u7565\u4e86\u65f6\u95f4\u5e8f\u5217\u7684\u5468\u671f\u6027\u7ed3\u6784\u548c\u6f5c\u5728\u72b6\u6001\u7684\u52a8\u6001\u6f14\u5316\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5468\u671f\u611f\u77e5\u7684\u591a\u7c92\u5ea6patching\u673a\u5236\u548c\u4e00\u4e2a\u5e7f\u4e49\u5bf9\u6bd4\u635f\u5931\uff0c\u4ee5\u4fdd\u7559\u8de8\u591a\u4e2a\u65f6\u95f4\u5206\u8fa8\u7387\u7684\u5b9e\u4f8b\u7ea7\u522b\u548c\u72b6\u6001\u7ea7\u522b\u7684\u76f8\u4f3c\u6027\u3002\u8bbe\u8ba1\u4e86\u4e00\u4e2anext-transition prediction pretext task\uff0c\u9f13\u52b1\u8868\u793a\u7f16\u7801\u5173\u4e8e\u672a\u6765\u72b6\u6001\u6f14\u5316\u7684\u9884\u6d4b\u4fe1\u606f\u3002", "result": "PLanTS\u5728\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u4e14\u4e0e\u57fa\u4e8eDTW\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u8fd0\u884c\u65f6\u6548\u7387\u3002", "conclusion": "PLanTS\u80fd\u591f\u6709\u6548\u5730\u63d0\u9ad8\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u8868\u793a\u8d28\u91cf\uff0c\u5e76\u5728\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.06887", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2509.06887", "abs": "https://arxiv.org/abs/2509.06887", "authors": ["Jiahui Chen", "Xiaoze Jiang", "Zhibo Wang", "Quanzhi Zhu", "Junyao Zhao", "Feng Hu", "Kang Pan", "Ao Xie", "Maohua Pei", "Zhiheng Qin", "Hongjing Zhang", "Zhixin Zhai", "Xiaobo Guo", "Runbin Zhou", "Kefeng Wang", "Mingyang Geng", "Cheng Chen", "Jingshan Lv", "Yupeng Huang", "Xiao Liang", "Han Li"], "title": "UniSearch: Rethinking Search System with a Unified Generative Architecture", "comment": null, "summary": "Modern search systems play a crucial role in facilitating information\nacquisition. Traditional search engines typically rely on a cascaded\narchitecture, where results are retrieved through recall, pre-ranking, and\nranking stages. The complexity of designing and maintaining multiple modules\nmakes it difficult to achieve holistic performance gains. Recent advances in\ngenerative recommendation have motivated the exploration of unified generative\nsearch as an alternative. However, existing approaches are not genuinely\nend-to-end: they typically train an item encoder to tokenize candidates first\nand then optimize a generator separately, leading to objective inconsistency\nand limited generalization. To address these limitations, we propose UniSearch,\na unified generative search framework for Kuaishou Search. UniSearch replaces\nthe cascaded pipeline with an end-to-end architecture that integrates a Search\nGenerator and a Video Encoder. The Generator produces semantic identifiers of\nrelevant items given a user query, while the Video Encoder learns latent item\nembeddings and provides their tokenized representations. A unified training\nframework jointly optimizes both components, enabling mutual enhancement and\nimproving representation quality and generation accuracy. Furthermore, we\nintroduce Search Preference Optimization (SPO), which leverages a reward model\nand real user feedback to better align generation with user preferences.\nExtensive experiments on industrial-scale datasets, together with online A/B\ntesting in both short-video and live search scenarios, demonstrate the strong\neffectiveness and deployment potential of UniSearch. Notably, its deployment in\nlive search yields the largest single-experiment improvement in recent years of\nour product's history, highlighting its practical value for real-world\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUniSearch\u7684\u7edf\u4e00\u751f\u6210\u641c\u7d22\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u4f20\u7edf\u641c\u7d22\u7cfb\u7edf\u591a\u6a21\u5757\u8bbe\u8ba1\u548c\u7ef4\u62a4\u7684\u590d\u6742\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5e76\u975e\u771f\u6b63\u7684\u7aef\u5230\u7aef\uff0c\u901a\u5e38\u5206\u522b\u8bad\u7ec3\u9879\u76ee\u7f16\u7801\u5668\u548c\u751f\u6210\u5668\uff0c\u5bfc\u81f4\u76ee\u6807\u4e0d\u4e00\u81f4\u548c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "UniSearch\u7528\u96c6\u6210\u4e86\u641c\u7d22\u751f\u6210\u5668\u548c\u89c6\u9891\u7f16\u7801\u5668\u7684\u7aef\u5230\u7aef\u67b6\u6784\u66ff\u6362\u4e86\u7ea7\u8054pipeline\uff0c\u5e76\u5f15\u5165\u4e86\u641c\u7d22\u504f\u597d\u4f18\u5316\uff08SPO\uff09\u4ee5\u5229\u7528\u771f\u5b9e\u7528\u6237\u53cd\u9988\u3002", "result": "\u5728\u5de5\u4e1a\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u4ee5\u53ca\u5728\u77ed\u89c6\u9891\u548c\u76f4\u64ad\u641c\u7d22\u573a\u666f\u4e2d\u7684\u5728\u7ebfA/B\u6d4b\u8bd5\u8868\u660eUniSearch\u5177\u6709\u5f3a\u5927\u7684\u6709\u6548\u6027\u548c\u90e8\u7f72\u6f5c\u529b\u3002", "conclusion": "UniSearch\u5728\u76f4\u64ad\u641c\u7d22\u4e2d\u7684\u90e8\u7f72\u4ea7\u751f\u4e86\u8fd1\u5e74\u6765\u4ea7\u54c1\u5386\u53f2\u4e0a\u5355\u6b21\u5b9e\u9a8c\u7684\u6700\u5927\u6539\u8fdb\uff0c\u7a81\u663e\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.06902", "categories": ["cs.CL", "cs.CR", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06902", "abs": "https://arxiv.org/abs/2509.06902", "authors": ["Aivin V. Solatorio"], "title": "Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers from LLMs via Claim Verification", "comment": null, "summary": "Large Language Models (LLMs) as stochastic systems may generate numbers that\ndeviate from available data, a failure known as \\emph{numeric hallucination}.\nExisting safeguards -- retrieval-augmented generation, citations, and\nuncertainty estimation -- improve transparency but cannot guarantee fidelity:\nfabricated or misquoted values may still be displayed as if correct. We propose\n\\textbf{Proof-Carrying Numbers (PCN)}, a presentation-layer protocol that\nenforces numeric fidelity through mechanical verification. Under PCN, numeric\nspans are emitted as \\emph{claim-bound tokens} tied to structured claims, and a\nverifier checks each token under a declared policy (e.g., exact equality,\nrounding, aliases, or tolerance with qualifiers). Crucially, PCN places\nverification in the \\emph{renderer}, not the model: only claim-checked numbers\nare marked as verified, and all others default to unverified. This separation\nprevents spoofing and guarantees fail-closed behavior. We formalize PCN and\nprove soundness, completeness under honest tokens, fail-closed behavior, and\nmonotonicity under policy refinement. PCN is lightweight and model-agnostic,\nintegrates seamlessly into existing applications, and can be extended with\ncryptographic commitments. By enforcing verification as a mandatory step before\ndisplay, PCN establishes a simple contract for numerically sensitive settings:\n\\emph{trust is earned only by proof}, while the absence of a mark communicates\nuncertainty.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Proof-Carrying Numbers (PCN) \u7684\u534f\u8bae\uff0c\u7528\u4e8e\u5f3a\u5236\u6267\u884c\u6570\u5b57\u4fdd\u771f\u5ea6\uff0c\u901a\u8fc7\u673a\u68b0\u9a8c\u8bc1\u6765\u786e\u4fdd LLM \u751f\u6210\u7684\u6570\u5b57\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4f5c\u4e3a\u968f\u673a\u7cfb\u7edf\uff0c\u53ef\u80fd\u4f1a\u751f\u6210\u504f\u79bb\u53ef\u7528\u6570\u636e\u7684\u6570\u5b57\uff0c\u8fd9\u79cd\u5931\u8d25\u88ab\u79f0\u4e3a\u6570\u5b57\u5e7b\u89c9\u3002\u73b0\u6709\u7684\u4fdd\u969c\u63aa\u65bd\u65e0\u6cd5\u4fdd\u8bc1\u4fdd\u771f\u5ea6\u3002", "method": "PCN \u662f\u4e00\u79cd\u8868\u793a\u5c42\u534f\u8bae\uff0c\u5b83\u5c06\u6570\u5b57\u8303\u56f4\u4f5c\u4e3a\u7ed1\u5b9a\u5230\u7ed3\u6784\u5316\u58f0\u660e\u7684\u58f0\u660e\u7ed1\u5b9a\u4ee4\u724c\u53d1\u51fa\uff0c\u5e76\u4e14\u9a8c\u8bc1\u5668\u5728\u58f0\u660e\u7684\u7b56\u7565\u4e0b\u68c0\u67e5\u6bcf\u4e2a\u4ee4\u724c\u3002", "result": "PCN \u5177\u6709\u53ef\u9760\u6027\u3001\u5728\u8bda\u5b9e\u4ee4\u724c\u4e0b\u7684\u5b8c\u6574\u6027\u3001\u6545\u969c\u5173\u95ed\u884c\u4e3a\u4ee5\u53ca\u5728\u7b56\u7565\u6539\u8fdb\u4e0b\u7684\u5355\u8c03\u6027\u3002", "conclusion": "PCN \u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u6a21\u578b\u65e0\u5173\u7684\u534f\u8bae\uff0c\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u5e94\u7528\u7a0b\u5e8f\u4e2d\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7\u52a0\u5bc6\u627f\u8bfa\u8fdb\u884c\u6269\u5c55\u3002\u901a\u8fc7\u5f3a\u5236\u6267\u884c\u9a8c\u8bc1\u4f5c\u4e3a\u663e\u793a\u524d\u7684\u5f3a\u5236\u6b65\u9aa4\uff0cPCN \u4e3a\u6570\u5b57\u654f\u611f\u8bbe\u7f6e\u5efa\u7acb\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u5951\u7ea6\uff1a\u4fe1\u4efb\u53ea\u80fd\u901a\u8fc7\u8bc1\u660e\u6765\u83b7\u5f97\uff0c\u800c\u7f3a\u5c11\u6807\u8bb0\u5219\u8868\u793a\u4e0d\u786e\u5b9a\u6027\u3002"}}
{"id": "2509.05484", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.05484", "abs": "https://arxiv.org/abs/2509.05484", "authors": ["Hajar Sakai", "Yi-En Tseng", "Mohammadsadegh Mikaeili", "Joshua Bosire", "Franziska Jovin"], "title": "From Staff Messages to Actionable Insights: A Multi-Stage LLM Classification Framework for Healthcare Analytics", "comment": null, "summary": "Hospital call centers serve as the primary contact point for patients within\na hospital system. They also generate substantial volumes of staff messages as\nnavigators process patient requests and communicate with the hospital offices\nfollowing the established protocol restrictions and guidelines. This\ncontinuously accumulated large amount of text data can be mined and processed\nto retrieve insights; however, traditional supervised learning approaches\nrequire annotated data, extensive training, and model tuning. Large Language\nModels (LLMs) offer a paradigm shift toward more computationally efficient\nmethodologies for healthcare analytics. This paper presents a multi-stage\nLLM-based framework that identifies staff message topics and classifies\nmessages by their reasons in a multi-class fashion. In the process, multiple\nLLM types, including reasoning, general-purpose, and lightweight models, were\nevaluated. The best-performing model was o3, achieving 78.4% weighted F1-score\nand 79.2% accuracy, followed closely by gpt-5 (75.3% Weighted F1-score and\n76.2% accuracy). The proposed methodology incorporates data security measures\nand HIPAA compliance requirements essential for healthcare environments. The\nprocessed LLM outputs are integrated into a visualization decision support tool\nthat transforms the staff messages into actionable insights accessible to\nhealthcare professionals. This approach enables more efficient utilization of\nthe collected staff messaging data, identifies navigator training\nopportunities, and supports improved patient experience and care quality.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522b\u533b\u9662\u547c\u53eb\u4e2d\u5fc3\u5458\u5de5\u6d88\u606f\u7684\u4e3b\u9898\u548c\u5206\u7c7b\u539f\u56e0\uff0c\u65e8\u5728\u63d0\u9ad8\u60a3\u8005\u4f53\u9a8c\u548c\u62a4\u7406\u8d28\u91cf\u3002", "motivation": "\u533b\u9662\u547c\u53eb\u4e2d\u5fc3\u4ea7\u751f\u5927\u91cf\u5458\u5de5\u6d88\u606f\uff0c\u8fd9\u4e9b\u6d88\u606f\u5305\u542b\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u548c\u8bad\u7ec3\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e3a\u533b\u7597\u4fdd\u5065\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u8be5\u6846\u67b6\u4f7f\u7528\u591a\u9636\u6bb5 LLM\uff0c\u5305\u62ec\u63a8\u7406\u3001\u901a\u7528\u548c\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u5bf9\u5458\u5de5\u6d88\u606f\u8fdb\u884c\u4e3b\u9898\u8bc6\u522b\u548c\u591a\u7c7b\u5206\u7c7b\u3002\u540c\u65f6\u8003\u8651\u4e86\u6570\u636e\u5b89\u5168\u548c HIPAA \u5408\u89c4\u6027\u3002", "result": "\u6700\u4f73\u6a21\u578b o3 \u5b9e\u73b0\u4e86 78.4% \u7684\u52a0\u6743 F1 \u5206\u6570\u548c 79.2% \u7684\u51c6\u786e\u7387\uff0c\u5176\u6b21\u662f gpt-5\uff0875.3% \u7684\u52a0\u6743 F1 \u5206\u6570\u548c 76.2% \u7684\u51c6\u786e\u7387\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u4ee5\u5c06\u5458\u5de5\u6d88\u606f\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u4ece\u800c\u66f4\u6709\u6548\u5730\u5229\u7528\u6570\u636e\uff0c\u8bc6\u522b\u57f9\u8bad\u673a\u4f1a\uff0c\u5e76\u652f\u6301\u6539\u5584\u60a3\u8005\u4f53\u9a8c\u548c\u62a4\u7406\u8d28\u91cf\u3002"}}
{"id": "2509.05375", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05375", "abs": "https://arxiv.org/abs/2509.05375", "authors": ["Arend Hintze"], "title": "Characterizing Fitness Landscape Structures in Prompt Engineering", "comment": null, "summary": "While prompt engineering has emerged as a crucial technique for optimizing\nlarge language model performance, the underlying optimization landscape remains\npoorly understood. Current approaches treat prompt optimization as a black-box\nproblem, applying sophisticated search algorithms without characterizing the\nlandscape topology they navigate. We present a systematic analysis of fitness\nlandscape structures in prompt engineering using autocorrelation analysis\nacross semantic embedding spaces. Through experiments on error detection tasks\nwith two distinct prompt generation strategies -- systematic enumeration (1,024\nprompts) and novelty-driven diversification (1,000 prompts) -- we reveal\nfundamentally different landscape topologies. Systematic prompt generation\nyields smoothly decaying autocorrelation, while diversified generation exhibits\nnon-monotonic patterns with peak correlation at intermediate semantic\ndistances, indicating rugged, hierarchically structured landscapes.\nTask-specific analysis across 10 error detection categories reveals varying\ndegrees of ruggedness across different error types. Our findings provide an\nempirical foundation for understanding the complexity of optimization in prompt\nengineering landscapes.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u63d0\u793a\u5de5\u7a0b\u4e2d\u7684\u4f18\u5316\u524d\u666f\uff0c\u53d1\u73b0\u7cfb\u7edf\u63d0\u793a\u751f\u6210\u4ea7\u751f\u5e73\u6ed1\u8870\u51cf\u7684\u81ea\u76f8\u5173\u6027\uff0c\u800c\u591a\u6837\u5316\u751f\u6210\u8868\u73b0\u51fa\u975e\u5355\u8c03\u6a21\u5f0f\uff0c\u5728\u4e2d\u95f4\u8bed\u4e49\u8ddd\u79bb\u5904\u5177\u6709\u5cf0\u503c\u76f8\u5173\u6027\uff0c\u8868\u660e\u5d0e\u5c96\u3001\u5206\u5c42\u7ed3\u6784\u7684 landscape\u3002", "motivation": "\u63d0\u793a\u5de5\u7a0b\u5df2\u6210\u4e3a\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u7684\u5173\u952e\u6280\u672f\uff0c\u4f46\u5176\u5e95\u5c42\u7684\u4f18\u5316\u524d\u666f\u4ecd\u7136\u77e5\u4e4b\u751a\u5c11\u3002\u76ee\u524d\u7684\u65b9\u6cd5\u5c06\u63d0\u793a\u4f18\u5316\u89c6\u4e3a\u4e00\u4e2a\u9ed1\u76d2\u95ee\u9898\uff0c\u5e94\u7528\u590d\u6742\u7684\u641c\u7d22\u7b97\u6cd5\uff0c\u800c\u6ca1\u6709\u63cf\u8ff0\u5b83\u4eec\u6240\u6d4f\u89c8\u7684 landscape topology\u3002", "method": "\u4f7f\u7528\u8de8\u8bed\u4e49\u5d4c\u5165\u7a7a\u95f4\u7684\u81ea\u76f8\u5173\u5206\u6790\uff0c\u5bf9\u63d0\u793a\u5de5\u7a0b\u4e2d\u7684\u9002\u5e94\u5ea6 landscape \u7ed3\u6784\u8fdb\u884c\u4e86\u7cfb\u7edf\u7684\u5206\u6790\u3002\u901a\u8fc7\u5bf9\u4e24\u79cd\u4e0d\u540c\u7684\u63d0\u793a\u751f\u6210\u7b56\u7565\uff08\u7cfb\u7edf\u679a\u4e3e\uff081,024 \u4e2a\u63d0\u793a\uff09\u548c\u65b0\u9896\u6027\u9a71\u52a8\u7684\u591a\u6837\u5316\uff081,000 \u4e2a\u63d0\u793a\uff09\uff09\u7684\u9519\u8bef\u68c0\u6d4b\u4efb\u52a1\u8fdb\u884c\u5b9e\u9a8c", "result": "\u7cfb\u7edf\u63d0\u793a\u751f\u6210\u4ea7\u751f\u5e73\u6ed1\u8870\u51cf\u7684\u81ea\u76f8\u5173\u6027\uff0c\u800c\u591a\u6837\u5316\u751f\u6210\u8868\u73b0\u51fa\u975e\u5355\u8c03\u6a21\u5f0f\uff0c\u5728\u4e2d\u95f4\u8bed\u4e49\u8ddd\u79bb\u5904\u5177\u6709\u5cf0\u503c\u76f8\u5173\u6027\uff0c\u8868\u660e\u5d0e\u5c96\u3001\u5206\u5c42\u7ed3\u6784\u7684 landscape\u3002\u8de8 10 \u4e2a\u9519\u8bef\u68c0\u6d4b\u7c7b\u522b\u7684\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u5206\u6790\u63ed\u793a\u4e86\u4e0d\u540c\u9519\u8bef\u7c7b\u578b\u4e4b\u95f4\u4e0d\u540c\u7a0b\u5ea6\u7684 ruggedness\u3002", "conclusion": "\u6211\u4eec\u7684\u53d1\u73b0\u4e3a\u7406\u89e3\u63d0\u793a\u5de5\u7a0b landscape \u4e2d\u4f18\u5316\u7684\u590d\u6742\u6027\u63d0\u4f9b\u4e86\u7ecf\u9a8c\u57fa\u7840\u3002"}}
{"id": "2509.05333", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05333", "abs": "https://arxiv.org/abs/2509.05333", "authors": ["Junghyun Park", "Tuan Anh Nguyen", "Dugki Min"], "title": "RT-VLM: Re-Thinking Vision Language Model with 4-Clues for Real-World Object Recognition Robustness", "comment": null, "summary": "Real world deployments often expose modern object recognition models to\ndomain shifts that precipitate a severe drop in accuracy. Such shifts encompass\n(i) variations in low level image statistics, (ii) changes in object pose and\nviewpoint, (iii) partial occlusion, and (iv) visual confusion across adjacent\nclasses. To mitigate this degradation, we introduce the Re-Thinking Vision\nLanguage Model (RT-VLM) framework. The foundation of this framework is a unique\nsynthetic dataset generation pipeline that produces images annotated with\n\"4-Clues\": precise bounding boxes, class names, detailed object-level captions,\nand a comprehensive context-level caption for the entire scene. We then perform\nparameter efficient supervised tuning of Llama 3.2 11B Vision Instruct on this\nresource. At inference time, a two stage Re-Thinking scheme is executed: the\nmodel first emits its own four clues, then re examines these responses as\nevidence and iteratively corrects them. Across robustness benchmarks that\nisolate individual domain shifts, RT-VLM consistently surpasses strong\nbaselines. These findings indicate that the integration of structured\nmultimodal evidence with an explicit self critique loop constitutes a promising\nroute toward reliable and transferable visual understanding.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a Re-Thinking Vision Language Model (RT-VLM) \u7684\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5bf9\u8c61\u8bc6\u522b\u6a21\u578b\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u56e0\u57df\u6f02\u79fb\u800c\u5bfc\u81f4\u7684\u7cbe\u5ea6\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u90e8\u7f72\u7ecf\u5e38\u4f7f\u73b0\u4ee3\u5bf9\u8c61\u8bc6\u522b\u6a21\u578b\u66b4\u9732\u4e8e\u57df\u6f02\u79fb\uff0c\u4ece\u800c\u5bfc\u81f4\u7cbe\u5ea6\u4e25\u91cd\u4e0b\u964d\u3002\u8fd9\u4e9b\u53d8\u5316\u5305\u62ec\uff1a(i) \u4f4e\u7ea7\u56fe\u50cf\u7edf\u8ba1\u7684\u53d8\u5316\uff0c(ii) \u5bf9\u8c61\u59ff\u52bf\u548c\u89c6\u70b9\u7684\u53d8\u5316\uff0c(iii) \u90e8\u5206\u906e\u6321\uff0c\u4ee5\u53ca (iv) \u76f8\u90bb\u7c7b\u4e4b\u95f4\u7684\u89c6\u89c9\u6df7\u6dc6\u3002", "method": "\u8be5\u6846\u67b6\u7684\u57fa\u7840\u662f\u4e00\u4e2a\u72ec\u7279\u7684\u5408\u6210\u6570\u636e\u96c6\u751f\u6210\u7ba1\u9053\uff0c\u8be5\u7ba1\u9053\u751f\u6210\u5e26\u6709\u201c4-Clues\u201d\u6ce8\u91ca\u7684\u56fe\u50cf\uff1a\u7cbe\u786e\u7684\u8fb9\u754c\u6846\u3001\u7c7b\u540d\u3001\u8be6\u7ec6\u7684\u5bf9\u8c61\u7ea7\u6807\u9898\u4ee5\u53ca\u6574\u4e2a\u573a\u666f\u7684\u7efc\u5408\u4e0a\u4e0b\u6587\u7ea7\u6807\u9898\u3002\u7136\u540e\uff0c\u6211\u4eec\u5728\u6b64\u8d44\u6e90\u4e0a\u6267\u884c Llama 3.2 11B Vision Instruct \u7684\u53c2\u6570\u9ad8\u6548\u76d1\u7763\u8c03\u6574\u3002\u5728\u63a8\u7406\u65f6\uff0c\u6267\u884c\u4e24\u9636\u6bb5\u7684\u201c\u91cd\u65b0\u601d\u8003\u201d\u65b9\u6848\uff1a\u6a21\u578b\u9996\u5148\u53d1\u51fa\u81ea\u5df1\u7684\u56db\u4e2a\u7ebf\u7d22\uff0c\u7136\u540e\u91cd\u65b0\u68c0\u67e5\u8fd9\u4e9b\u54cd\u5e94\u4f5c\u4e3a\u8bc1\u636e\u5e76\u8fed\u4ee3\u5730\u7ea0\u6b63\u5b83\u4eec\u3002", "result": "\u5728\u9694\u79bb\u5404\u4e2a\u57df\u6f02\u79fb\u7684\u9c81\u68d2\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRT-VLM \u59cb\u7ec8\u8d85\u8fc7\u5f3a\u5927\u7684\u57fa\u7ebf\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u5c06\u7ed3\u6784\u5316\u591a\u6a21\u6001\u8bc1\u636e\u4e0e\u663e\u5f0f\u7684\u81ea\u6211\u6279\u8bc4\u5faa\u73af\u76f8\u7ed3\u5408\uff0c\u662f\u5b9e\u73b0\u53ef\u9760\u4e14\u53ef\u8f6c\u79fb\u7684\u89c6\u89c9\u7406\u89e3\u7684\u6709\u5e0c\u671b\u7684\u9014\u5f84\u3002"}}
{"id": "2509.05481", "categories": ["cs.LG", "q-bio.MN", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.05481", "abs": "https://arxiv.org/abs/2509.05481", "authors": ["Eric Palanques-Tost", "Hanna Krasowski", "Murat Arcak", "Ron Weiss", "Calin Belta"], "title": "STL-based Optimization of Biomolecular Neural Networks for Regression and Control", "comment": null, "summary": "Biomolecular Neural Networks (BNNs), artificial neural networks with\nbiologically synthesizable architectures, achieve universal function\napproximation capabilities beyond simple biological circuits. However, training\nBNNs remains challenging due to the lack of target data. To address this, we\npropose leveraging Signal Temporal Logic (STL) specifications to define\ntraining objectives for BNNs. We build on the quantitative semantics of STL,\nenabling gradient-based optimization of the BNN weights, and introduce a\nlearning algorithm that enables BNNs to perform regression and control tasks in\nbiological systems. Specifically, we investigate two regression problems in\nwhich we train BNNs to act as reporters of dysregulated states, and a feedback\ncontrol problem in which we train the BNN in closed-loop with a chronic disease\nmodel, learning to reduce inflammation while avoiding adverse responses to\nexternal infections. Our numerical experiments demonstrate that STL-based\nlearning can solve the investigated regression and control tasks efficiently.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91 (STL) \u89c4\u8303\u6765\u5b9a\u4e49\u751f\u7269\u5206\u5b50\u795e\u7ecf\u7f51\u7edc (BNN) \u8bad\u7ec3\u76ee\u6807\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3 BNN \u8bad\u7ec3\u4e2d\u7f3a\u4e4f\u76ee\u6807\u6570\u636e\u7684\u95ee\u9898\u3002", "motivation": "BNN \u5177\u6709\u8d85\u8d8a\u7b80\u5355\u751f\u7269\u7535\u8def\u7684\u901a\u7528\u51fd\u6570\u903c\u8fd1\u80fd\u529b\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u76ee\u6807\u6570\u636e\uff0c\u8bad\u7ec3 BNN \u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5229\u7528 STL \u7684\u5b9a\u91cf\u8bed\u4e49\uff0c\u5b9e\u73b0\u57fa\u4e8e\u68af\u5ea6\u7684 BNN \u6743\u91cd\u4f18\u5316\uff0c\u5e76\u5f15\u5165\u4e00\u79cd\u5b66\u4e60\u7b97\u6cd5\uff0c\u4f7f BNN \u80fd\u591f\u5728\u751f\u7269\u7cfb\u7edf\u4e2d\u6267\u884c\u56de\u5f52\u548c\u63a7\u5236\u4efb\u52a1\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e STL \u7684\u5b66\u4e60\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u6240\u7814\u7a76\u7684\u56de\u5f52\u548c\u63a7\u5236\u4efb\u52a1\u3002", "conclusion": "\u57fa\u4e8e STL \u7684\u5b66\u4e60\u53ef\u4ee5\u6709\u6548\u5730\u8bad\u7ec3 BNN\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u751f\u7269\u7cfb\u7edf\u4e2d\u6267\u884c\u56de\u5f52\u548c\u63a7\u5236\u4efb\u52a1\u3002"}}
{"id": "2509.05486", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05486", "abs": "https://arxiv.org/abs/2509.05486", "authors": ["Jessica M. Lundin", "Ada Zhang", "Nihal Karim", "Hamza Louzan", "Victor Wei", "David Adelani", "Cody Carroll"], "title": "The Token Tax: Systematic Bias in Multilingual Tokenization", "comment": null, "summary": "Tokenization inefficiency imposes structural disadvantages on morphologically\ncomplex, low-resource languages, inflating compute resources and depressing\naccuracy. We evaluate 10 large language models (LLMs) on AfriMMLU (9,000 MCQA\nitems; 5 subjects; 16 African languages) and show that fertility (tokens/word)\nreliably predicts accuracy. Higher fertility consistently predicts lower\naccuracy across all models and subjects. We further find that reasoning models\n(DeepSeek, o1) consistently outperform non-reasoning peers across high and low\nresource languages in the AfriMMLU dataset, narrowing accuracy gaps observed in\nprior generations. Finally, translating token inflation to economics, a\ndoubling in tokens results in quadrupled training cost and time, underscoring\nthe token tax faced by many languages. These results motivate morphologically\naware tokenization, fair pricing, and multilingual benchmarks for equitable\nnatural language processing (NLP).", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5206\u8bcd\u6548\u7387\u4f4e\u4e0b\u5bf9\u5f62\u6001\u590d\u6742\u3001\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u4e0d\u5229\u5f71\u54cd\uff0c\u4ee5\u53ca\u8fd9\u5982\u4f55\u5f71\u54cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u5f62\u6001\u590d\u6742\u3001\u4f4e\u8d44\u6e90\u8bed\u8a00\u7531\u4e8e\u5206\u8bcd\u6548\u7387\u95ee\u9898\uff0c\u5bfc\u81f4\u8ba1\u7b97\u8d44\u6e90\u81a8\u80c0\u548c\u51c6\u786e\u7387\u4e0b\u964d\u3002", "method": "\u901a\u8fc7\u5728AfriMMLU\u6570\u636e\u96c6\u4e0a\u8bc4\u4f3010\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5206\u6790 fertility\uff08tokens/word\uff09\u4e0e\u51c6\u786e\u7387\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8f83\u9ad8\u7684 fertility \u9884\u793a\u7740\u6240\u6709\u6a21\u578b\u548c\u79d1\u76ee\u4e2d\u8f83\u4f4e\u7684\u51c6\u786e\u7387\u3002\u63a8\u7406\u6a21\u578b\u5728\u5404\u79cd\u8d44\u6e90\u8bed\u8a00\u4e2d\u8868\u73b0\u4f18\u4e8e\u975e\u63a8\u7406\u6a21\u578b\u3002token \u6570\u91cf\u7ffb\u500d\u4f1a\u5bfc\u81f4\u8bad\u7ec3\u6210\u672c\u548c\u65f6\u95f4\u7ffb\u56db\u500d\u3002", "conclusion": "\u7814\u7a76\u7ed3\u8bba\u5f3a\u8c03\u4e86\u5f62\u6001\u5b66\u611f\u77e5\u5206\u8bcd\u3001\u516c\u5e73\u5b9a\u4ef7\u548c\u591a\u8bed\u8a00\u57fa\u51c6\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u5b9e\u73b0\u516c\u5e73\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3002"}}
{"id": "2509.05378", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.05378", "abs": "https://arxiv.org/abs/2509.05378", "authors": ["Andreas Motzfeldt", "Joakim Edin", "Casper L. Christensen", "Christian Hardmeier", "Lars Maal\u00f8e", "Anna Rogers"], "title": "Code Like Humans: A Multi-Agent Solution for Medical Coding", "comment": "EMNLP Findings 2025", "summary": "In medical coding, experts map unstructured clinical notes to alphanumeric\ncodes for diagnoses and procedures. We introduce Code Like Humans: a new\nagentic framework for medical coding with large language models. It implements\nofficial coding guidelines for human experts, and it is the first solution that\ncan support the full ICD-10 coding system (+70K labels). It achieves the best\nperformance to date on rare diagnosis codes (fine-tuned discriminative\nclassifiers retain an advantage for high-frequency codes, to which they are\nlimited). Towards future work, we also contribute an analysis of system\nperformance and identify its `blind spots' (codes that are systematically\nundercoded).", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aCode Like Humans\u7684agentic\u6846\u67b6\uff0c\u7528\u4e8e\u533b\u5b66\u7f16\u7801\uff0c\u5b83\u53ef\u4ee5\u652f\u6301\u5b8c\u6574\u7684ICD-10\u7f16\u7801\u7cfb\u7edf\uff0c\u5e76\u5728\u7f55\u89c1\u8bca\u65ad\u4ee3\u7801\u4e0a\u5b9e\u73b0\u4e86\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u7f16\u7801\u4e2d\uff0c\u4e13\u5bb6\u5c06\u975e\u7ed3\u6784\u5316\u7684\u4e34\u5e8a\u7b14\u8bb0\u6620\u5c04\u5230\u5b57\u6bcd\u6570\u5b57\u4ee3\u7801\uff0c\u4ee5\u8fdb\u884c\u8bca\u65ad\u548c\u7a0b\u5e8f\u3002", "method": "\u8be5\u6846\u67b6\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u5b9e\u65bd\u5b98\u65b9\u7f16\u7801\u6307\u5357\u3002", "result": "\u8be5\u6846\u67b6\u5728\u7f55\u89c1\u8bca\u65ad\u4ee3\u7801\u4e0a\u5b9e\u73b0\u4e86\u6700\u4f73\u6027\u80fd\uff0c\u4f46\u5fae\u8c03\u7684\u5224\u522b\u5206\u7c7b\u5668\u5728\u9ad8\u9891\u4ee3\u7801\u4e0a\u4ecd\u5177\u6709\u4f18\u52bf\u3002", "conclusion": "\u672c\u6587\u5206\u6790\u4e86\u7cfb\u7edf\u6027\u80fd\uff0c\u5e76\u8bc6\u522b\u4e86\u5176\u76f2\u70b9\uff08\u7cfb\u7edf\u6027\u7f16\u7801\u4e0d\u8db3\u7684\u4ee3\u7801\uff09\u3002"}}
{"id": "2509.05334", "categories": ["cs.CV", "cs.MM", "H.5.1; I.2.10"], "pdf": "https://arxiv.org/pdf/2509.05334", "abs": "https://arxiv.org/abs/2509.05334", "authors": ["Diwen Huang"], "title": "A Real-Time, Vision-Based System for Badminton Smash Speed Estimation on Mobile Devices", "comment": "6 pages, 3 figures, 1 table. Independent research preprint", "summary": "Performance metrics in sports, such as shot speed and angle, provide crucial\nfeedback for athlete development. However, the technology to capture these\nmetrics has historically been expensive, complex, and largely inaccessible to\namateur and recreational players. This paper addresses this gap in the context\nof badminton, one of the world's most popular sports, by introducing a novel,\ncost-effective, and user-friendly system for measuring smash speed using\nubiquitous smartphone technology. Our approach leverages a custom-trained\nYOLOv5 model for shuttlecock detection, combined with a Kalman filter for\nrobust trajectory tracking. By implementing a video-based kinematic speed\nestimation method with spatiotemporal scaling, the system automatically\ncalculates the shuttlecock's velocity from a standard video recording. The\nentire process is packaged into an intuitive mobile application, democratizing\naccess to high-level performance analytics and empowering players at all levels\nto analyze and improve their game.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u4f7f\u7528\u667a\u80fd\u624b\u673a\u6280\u672f\u6d4b\u91cf\u7fbd\u6bdb\u7403\u6263\u7403\u901f\u5ea6\u7684\u65b0\u578b\u3001\u7ecf\u6d4e\u9ad8\u6548\u4e14\u7528\u6237\u53cb\u597d\u7684\u7cfb\u7edf\u3002", "motivation": "\u4f20\u7edf\u7684\u8fd0\u52a8\u6027\u80fd\u6307\u6807\u6355\u83b7\u6280\u672f\u6602\u8d35\u3001\u590d\u6742\uff0c\u4e1a\u4f59\u548c\u4f11\u95f2\u8fd0\u52a8\u5458\u96be\u4ee5\u83b7\u5f97\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u7fbd\u6bdb\u7403\u8fd0\u52a8\u4e2d\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528\u5b9a\u5236\u8bad\u7ec3\u7684 YOLOv5 \u6a21\u578b\u8fdb\u884c\u7fbd\u6bdb\u7403\u68c0\u6d4b\uff0c\u7ed3\u5408\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u8fdb\u884c\u9c81\u68d2\u7684\u8f68\u8ff9\u8ddf\u8e2a\u3002\u901a\u8fc7\u5b9e\u65bd\u57fa\u4e8e\u89c6\u9891\u7684\u8fd0\u52a8\u5b66\u901f\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u548c\u65f6\u7a7a\u7f29\u653e\uff0c\u8be5\u7cfb\u7edf\u53ef\u4ee5\u81ea\u52a8\u8ba1\u7b97\u6807\u51c6\u89c6\u9891\u8bb0\u5f55\u4e2d\u7fbd\u6bdb\u7403\u7684\u901f\u5ea6\u3002", "result": "\u8be5\u7cfb\u7edf\u5c06\u6574\u4e2a\u8fc7\u7a0b\u6253\u5305\u5230\u4e00\u4e2a\u76f4\u89c2\u7684\u79fb\u52a8\u5e94\u7528\u7a0b\u5e8f\u4e2d\uff0c\u666e\u53ca\u4e86\u9ad8\u7ea7\u6027\u80fd\u5206\u6790\u7684\u8bbf\u95ee\uff0c\u5e76\u4f7f\u6240\u6709\u7ea7\u522b\u7684\u73a9\u5bb6\u90fd\u80fd\u5206\u6790\u548c\u6539\u8fdb\u4ed6\u4eec\u7684\u6bd4\u8d5b\u3002", "conclusion": "\u8be5\u7814\u7a76 democratizing access to high-level performance analytics and empowering players at all levels to analyze and improve their game."}}
{"id": "2509.05485", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05485", "abs": "https://arxiv.org/abs/2509.05485", "authors": ["Maksim Kazanskii", "Artem Kasianov"], "title": "Prior Distribution and Model Confidence", "comment": "10 pages,4 tables, 5 images", "summary": "This paper investigates the impact of training data distribution on the\nperformance of image classification models. By analyzing the embeddings of the\ntraining set, we propose a framework to understand the confidence of model\npredictions on unseen data without the need for retraining. Our approach\nfilters out low-confidence predictions based on their distance from the\ntraining distribution in the embedding space, significantly improving\nclassification accuracy. We demonstrate this on the example of several\nclassification models, showing consistent performance gains across\narchitectures. Furthermore, we show that using multiple embedding models to\nrepresent the training data enables a more robust estimation of confidence, as\ndifferent embeddings capture complementary aspects of the data. Combining these\nembeddings allows for better detection and exclusion of out-of-distribution\nsamples, resulting in further accuracy improvements. The proposed method is\nmodel-agnostic and generalizable, with potential applications beyond computer\nvision, including domains such as Natural Language Processing where prediction\nreliability is critical.", "AI": {"tldr": "\u7814\u7a76\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u5bf9\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u7406\u89e3\u6a21\u578b\u5bf9\u672a\u89c1\u6570\u636e\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u7684\u6846\u67b6\u3002", "motivation": "\u7814\u7a76\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u5bf9\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5206\u6790\u8bad\u7ec3\u96c6\u7684\u5d4c\u5165\uff0c\u6839\u636e\u5d4c\u5165\u7a7a\u95f4\u4e2d\u4e0e\u8bad\u7ec3\u5206\u5e03\u7684\u8ddd\u79bb\u8fc7\u6ee4\u6389\u4f4e\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u3002", "result": "\u5728\u591a\u4e2a\u5206\u7c7b\u6a21\u578b\u4e0a\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\uff0c\u8868\u660e\u5728\u4e0d\u540c\u67b6\u6784\u4e0a\u6027\u80fd\u5747\u5f97\u5230\u4e00\u81f4\u63d0\u5347\u3002\u4f7f\u7528\u591a\u4e2a\u5d4c\u5165\u6a21\u578b\u53ef\u4ee5\u66f4\u7a33\u5065\u5730\u4f30\u8ba1\u7f6e\u4fe1\u5ea6\uff0c\u4ece\u800c\u66f4\u597d\u5730\u68c0\u6d4b\u548c\u6392\u9664\u5206\u5e03\u5916\u6837\u672c\uff0c\u4ece\u800c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u662f\u6a21\u578b\u65e0\u5173\u4e14\u53ef\u63a8\u5e7f\u7684\uff0c\u5177\u6709\u8d85\u51fa\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u6f5c\u5728\u5e94\u7528\uff0c\u5305\u62ec\u9884\u6d4b\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7b49\u9886\u57df\u3002"}}
{"id": "2509.05554", "categories": ["cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.05554", "abs": "https://arxiv.org/abs/2509.05554", "authors": ["Yihong Leng", "Siming Zheng", "Jinwei Chen", "Bo Li", "Jiaojiao Li", "Peng-Tao Jiang"], "title": "RED: Robust Event-Guided Motion Deblurring with Modality-Specific Disentangled Representation", "comment": null, "summary": "Event cameras provide sparse yet temporally high-temporal-resolution motion\ninformation, demonstrating great potential for motion deblurring. Existing\nmethods focus on cross-modal interaction, overlooking the inherent\nincompleteness of event streams, which arises from the trade-off between\nsensitivity and noise introduced by the thresholding mechanism of Dynamic\nVision Sensors (DVS). Such degradation compromises the integrity of motion\npriors and limits the effectiveness of event-guided deblurring. To tackle these\nchallenges, we propose a Robust Event-guided Deblurring (RED) network with\nmodality-specific disentangled representation. First, we introduce a\nRobustness-Oriented Perturbation Strategy (RPS) that applies random masking to\nevents, which exposes RED to incomplete patterns and then foster robustness\nagainst various unknown scenario conditions.Next, a disentangled OmniAttention\nis presented to explicitly model intra-motion, inter-motion, and cross-modality\ncorrelations from two inherently distinct but complementary sources: blurry\nimages and partially disrupted events. Building on these reliable features, two\ninteractive modules are designed to enhance motion-sensitive areas in blurry\nimages and inject semantic context into incomplete event representations.\nExtensive experiments on synthetic and real-world datasets demonstrate RED\nconsistently achieves state-of-the-art performance in both accuracy and\nrobustness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u4e8b\u4ef6\u5f15\u5bfc\u53bb\u6a21\u7cca\uff08RED\uff09\u7f51\u7edc\uff0c\u8be5\u7f51\u7edc\u5177\u6709\u7279\u5b9a\u4e8e\u6a21\u6001\u7684\u89e3\u7f20\u8868\u793a\uff0c\u4ee5\u89e3\u51b3\u4e8b\u4ef6\u6d41\u7684\u4e0d\u5b8c\u6574\u6027\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u8fd0\u52a8\u5148\u9a8c\u7684\u5b8c\u6574\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u53bb\u6a21\u7cca\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u8de8\u6a21\u6001\u4ea4\u4e92\uff0c\u5ffd\u7565\u4e86\u4e8b\u4ef6\u6d41\u7684\u5185\u5728\u4e0d\u5b8c\u6574\u6027\uff0c\u8fd9\u79cd\u4e0d\u5b8c\u6574\u6027\u4f1a\u635f\u5bb3\u8fd0\u52a8\u5148\u9a8c\u7684\u5b8c\u6574\u6027\uff0c\u5e76\u9650\u5236\u4e8b\u4ef6\u5f15\u5bfc\u53bb\u6a21\u7cca\u7684\u6709\u6548\u6027\u3002", "method": "1. \u63d0\u51fa\u4e86\u4e00\u4e2a\u9762\u5411\u9c81\u68d2\u6027\u7684\u6270\u52a8\u7b56\u7565\uff08RPS\uff09\uff0c\u8be5\u7b56\u7565\u5c06\u968f\u673a\u63a9\u853d\u5e94\u7528\u4e8e\u4e8b\u4ef6\uff0c\u4f7f RED \u66b4\u9732\u4e8e\u4e0d\u5b8c\u6574\u7684\u6a21\u5f0f\uff0c\u4ece\u800c\u57f9\u517b\u9488\u5bf9\u5404\u79cd\u672a\u77e5\u573a\u666f\u6761\u4ef6\u7684\u9c81\u68d2\u6027\u3002\n2. \u63d0\u51fa\u4e86\u4e00\u4e2a\u89e3\u7f20\u7684 OmniAttention\uff0c\u7528\u4e8e\u663e\u5f0f\u5730\u5bf9\u6765\u81ea\u4e24\u4e2a\u5185\u5728\u4e0d\u540c\u4f46\u4e92\u8865\u7684\u6765\u6e90\uff08\u6a21\u7cca\u56fe\u50cf\u548c\u90e8\u5206\u4e2d\u65ad\u7684\u4e8b\u4ef6\uff09\u7684\u5185\u8fd0\u52a8\u3001\u95f4\u8fd0\u52a8\u548c\u8de8\u6a21\u6001\u76f8\u5173\u6027\u8fdb\u884c\u5efa\u6a21\u3002\n3. \u8bbe\u8ba1\u4e86\u4e24\u4e2a\u4ea4\u4e92\u6a21\u5757\uff0c\u4ee5\u589e\u5f3a\u6a21\u7cca\u56fe\u50cf\u4e2d\u7684\u8fd0\u52a8\u654f\u611f\u533a\u57df\uff0c\u5e76\u5c06\u8bed\u4e49\u4e0a\u4e0b\u6587\u6ce8\u5165\u5230\u4e0d\u5b8c\u6574\u7684\u4e8b\u4ef6\u8868\u793a\u4e2d\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cRED \u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u59cb\u7ec8\u5982\u4e00\u5730\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "RED\u7f51\u7edc\u901a\u8fc7\u6a21\u6001\u7279\u5b9a\u7684\u89e3\u7f20\u8868\u793a\u548c\u9762\u5411\u9c81\u68d2\u6027\u7684\u6270\u52a8\u7b56\u7565\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u4e8b\u4ef6\u6d41\u7684\u4e0d\u5b8c\u6574\u6027\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u8fd0\u52a8\u5148\u9a8c\u7684\u5b8c\u6574\u6027\uff0c\u4ece\u800c\u5728\u53bb\u6a21\u7cca\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002"}}
{"id": "2509.05505", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05505", "abs": "https://arxiv.org/abs/2509.05505", "authors": ["Mansi Garg", "Lee-Chi Wang", "Bhavesh Ghanchi", "Sanjana Dumpala", "Shreyash Kakde", "Yen Chih Chen"], "title": "Biomedical Literature Q&A System Using Retrieval-Augmented Generation (RAG)", "comment": "10 pages, 6 figures, 3 tables", "summary": "This work presents a Biomedical Literature Question Answering (Q&A) system\nbased on a Retrieval-Augmented Generation (RAG) architecture, designed to\nimprove access to accurate, evidence-based medical information. Addressing the\nshortcomings of conventional health search engines and the lag in public access\nto biomedical research, the system integrates diverse sources, including PubMed\narticles, curated Q&A datasets, and medical encyclopedias ,to retrieve relevant\ninformation and generate concise, context-aware responses. The retrieval\npipeline uses MiniLM-based semantic embeddings and FAISS vector search, while\nanswer generation is performed by a fine-tuned Mistral-7B-v0.3 language model\noptimized using QLoRA for efficient, low-resource training. The system supports\nboth general medical queries and domain-specific tasks, with a focused\nevaluation on breast cancer literature demonstrating the value of\ndomain-aligned retrieval. Empirical results, measured using BERTScore (F1),\nshow substantial improvements in factual consistency and semantic relevance\ncompared to baseline models. The findings underscore the potential of\nRAG-enhanced language models to bridge the gap between complex biomedical\nliterature and accessible public health knowledge, paving the way for future\nwork on multilingual adaptation, privacy-preserving inference, and personalized\nmedical AI systems.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u67b6\u6784\u7684\u751f\u7269\u533b\u5b66\u6587\u732e\u95ee\u7b54\uff08Q&A\uff09\u7cfb\u7edf\uff0c\u65e8\u5728\u6539\u8fdb\u5bf9\u51c6\u786e\u7684\u3001\u57fa\u4e8e\u8bc1\u636e\u7684\u533b\u5b66\u4fe1\u606f\u7684\u8bbf\u95ee\u3002", "motivation": "\u4f20\u7edf\u5065\u5eb7\u641c\u7d22\u5f15\u64ce\u5b58\u5728\u4e0d\u8db3\uff0c\u4e14\u516c\u4f17\u83b7\u53d6\u751f\u7269\u533b\u5b66\u7814\u7a76\u5b58\u5728\u6ede\u540e\u6027\u3002", "method": "\u8be5\u7cfb\u7edf\u6574\u5408\u4e86\u5305\u62ecPubMed\u6587\u7ae0\u3001\u7cbe\u9009\u7684Q&A\u6570\u636e\u96c6\u548c\u533b\u5b66\u767e\u79d1\u5168\u4e66\u5728\u5185\u7684\u591a\u79cd\u6765\u6e90\uff0c\u4ee5\u68c0\u7d22\u76f8\u5173\u4fe1\u606f\u5e76\u751f\u6210\u7b80\u6d01\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u56de\u7b54\u3002\u68c0\u7d22\u6d41\u7a0b\u4f7f\u7528\u57fa\u4e8eMiniLM\u7684\u8bed\u4e49\u5d4c\u5165\u548cFAISS\u5411\u91cf\u641c\u7d22\uff0c\u800c\u7b54\u6848\u751f\u6210\u7531\u7ecf\u8fc7\u5fae\u8c03\u7684Mistral-7B-v0.3\u8bed\u8a00\u6a21\u578b\u6267\u884c\uff0c\u8be5\u6a21\u578b\u4f7f\u7528QLoRA\u8fdb\u884c\u4f18\u5316\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u3001\u4f4e\u8d44\u6e90\u8bad\u7ec3\u3002", "result": "\u4f7f\u7528BERTScore (F1) \u6d4b\u91cf\u7684\u7ecf\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u6a21\u578b\u76f8\u6bd4\uff0c\u4e8b\u5b9e\u4e00\u81f4\u6027\u548c\u8bed\u4e49\u76f8\u5173\u6027\u6709\u4e86\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86RAG\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u5728\u5f25\u5408\u590d\u6742\u751f\u7269\u533b\u5b66\u6587\u732e\u548c\u53ef\u8bbf\u95ee\u7684\u516c\u5171\u5065\u5eb7\u77e5\u8bc6\u4e4b\u95f4\u7684\u5dee\u8ddd\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u5728\u591a\u8bed\u8a00\u9002\u5e94\u3001\u4fdd\u62a4\u9690\u79c1\u7684\u63a8\u7406\u548c\u4e2a\u6027\u5316\u533b\u7597AI\u7cfb\u7edf\u65b9\u9762\u7684\u5de5\u4f5c\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.05381", "categories": ["cs.AI", "cs.LG", "68T01, 68T20, 68Q87"], "pdf": "https://arxiv.org/pdf/2509.05381", "abs": "https://arxiv.org/abs/2509.05381", "authors": ["Madhava Gaikwad"], "title": "Murphys Laws of AI Alignment: Why the Gap Always Wins", "comment": "21 pages", "summary": "Large language models are increasingly aligned to human preferences through\nreinforcement learning from human feedback (RLHF) and related methods such as\nDirect Preference Optimization (DPO), Constitutional AI, and RLAIF. While\neffective, these methods exhibit recurring failure patterns i.e., reward\nhacking, sycophancy, annotator drift, and misgeneralization. We introduce the\nconcept of the Alignment Gap, a unifying lens for understanding recurring\nfailures in feedback-based alignment. Using a KL-tilting formalism, we\nillustrate why optimization pressure tends to amplify divergence between proxy\nrewards and true human intent. We organize these failures into a catalogue of\nMurphys Laws of AI Alignment, and propose the Alignment Trilemma as a way to\nframe trade-offs among optimization strength, value capture, and\ngeneralization. Small-scale empirical studies serve as illustrative support.\nFinally, we propose the MAPS framework (Misspecification, Annotation, Pressure,\nShift) as practical design levers. Our contribution is not a definitive\nimpossibility theorem but a perspective that reframes alignment debates around\nstructural limits and trade-offs, offering clearer guidance for future design.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7406\u89e3\u57fa\u4e8e\u53cd\u9988\u5bf9\u9f50\u4e2d\u53cd\u590d\u51fa\u73b0\u7684\u5931\u8d25\u7684\u7edf\u4e00\u89c6\u89d2\uff0c\u5373\u5bf9\u9f50\u5dee\u8ddd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u901a\u8fc7\u6765\u81ea\u4eba\u7c7b\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60 (RLHF) \u548c\u76f8\u5173\u65b9\u6cd5\uff08\u4f8b\u5982\u76f4\u63a5\u504f\u597d\u4f18\u5316 (DPO)\u3001\u5baa\u6cd5 AI \u548c RLAIF\uff09\u4e0e\u4eba\u7c7b\u504f\u597d\u4fdd\u6301\u4e00\u81f4\u3002\u867d\u7136\u6709\u6548\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u8868\u73b0\u51fa\u53cd\u590d\u51fa\u73b0\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u5373\u5956\u52b1\u9ed1\u5ba2\u3001\u8c04\u5a9a\u3001\u6ce8\u91ca\u8005\u6f02\u79fb\u548c\u9519\u8bef\u6cdb\u5316\u3002", "method": "\u4f7f\u7528 KL \u503e\u659c\u5f62\u5f0f\u4e3b\u4e49\uff0c\u8bf4\u660e\u4e86\u4e3a\u4ec0\u4e48\u4f18\u5316\u538b\u529b\u5f80\u5f80\u4f1a\u653e\u5927\u4ee3\u7406\u5956\u52b1\u548c\u771f\u6b63\u7684\u4eba\u7c7b\u610f\u56fe\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u5c06\u8fd9\u4e9b\u5931\u8d25\u7ec4\u7ec7\u6210 AI \u5bf9\u9f50\u7684\u58a8\u83f2\u5b9a\u5f8b\u76ee\u5f55\uff0c\u5e76\u63d0\u51fa\u5bf9\u9f50\u4e09\u96be\u56f0\u5883\u4f5c\u4e3a\u5728\u4f18\u5316\u5f3a\u5ea6\u3001\u4ef7\u503c\u6355\u83b7\u548c\u6cdb\u5316\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u7684\u4e00\u79cd\u65b9\u5f0f\u3002", "result": "\u5c0f\u89c4\u6a21\u7684\u5b9e\u8bc1\u7814\u7a76\u4f5c\u4e3a\u8bf4\u660e\u6027\u652f\u6301\u3002", "conclusion": "\u6211\u4eec\u7684\u8d21\u732e\u4e0d\u662f\u4e00\u4e2a\u660e\u786e\u7684\u4e0d\u53ef\u80fd\u6027\u5b9a\u7406\uff0c\u800c\u662f\u4e00\u79cd\u91cd\u65b0\u6784\u5efa\u56f4\u7ed5\u7ed3\u6784\u9650\u5236\u548c\u6743\u8861\u7684\u5bf9\u9f50\u8fa9\u8bba\u7684\u89c6\u89d2\uff0c\u4e3a\u672a\u6765\u7684\u8bbe\u8ba1\u63d0\u4f9b\u66f4\u6e05\u6670\u7684\u6307\u5bfc\u3002"}}
{"id": "2509.05335", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05335", "abs": "https://arxiv.org/abs/2509.05335", "authors": ["Zebo Xu", "Shaoyun Yu", "Mark Torrance", "Guido Nottbusch", "Nan Zhao", "Zhenguang Cai"], "title": "A Stroke-Level Large-Scale Database of Chinese Character Handwriting and the OpenHandWrite_Toolbox for Handwriting Research", "comment": null, "summary": "Understanding what linguistic components (e.g., phonological, semantic, and\northographic systems) modulate Chinese handwriting at the character, radical,\nand stroke levels remains an important yet understudied topic. Additionally,\nthere is a lack of comprehensive tools for capturing and batch-processing\nfine-grained handwriting data. To address these issues, we constructed a\nlarge-scale handwriting database in which 42 Chinese speakers for each\nhandwriting 1200 characters in a handwriting-to-dictation task. Additionally,\nwe enhanced the existing handwriting package and provided comprehensive\ndocumentation for the upgraded OpenHandWrite_Toolbox, which can easily modify\nthe experimental design, capture the stroke-level handwriting trajectory, and\nbatch-process handwriting measurements (e.g., latency, duration, and\npen-pressure). In analysing our large-scale database, multiple regression\nresults show that orthographic predictors impact handwriting preparation and\nexecution across character, radical, and stroke levels. Phonological factors\nalso influence execution at all three levels. Importantly, these lexical\neffects demonstrate hierarchical attenuation - they were most pronounced at the\ncharacter level, followed by the radical, and were weakest at the stroke\nlevels. These findings demonstrate that handwriting preparation and execution\nat the radical and stroke levels are closely intertwined with linguistic\ncomponents. This database and toolbox offer valuable resources for future\npsycholinguistic and neurolinguistic research on the handwriting of characters\nand sub-characters across different languages.", "AI": {"tldr": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u6c49\u5b57\u624b\u5199\u6570\u636e\u5e93\uff0c\u5e76\u5347\u7ea7\u4e86\u624b\u5199\u5de5\u5177\u7bb1\uff0c\u4ee5\u7814\u7a76\u8bed\u8a00\u6210\u5206\u5982\u4f55\u8c03\u8282\u6c49\u5b57\u624b\u5199\u3002", "motivation": "\u7406\u89e3\u8bed\u97f3\u3001\u8bed\u4e49\u548c\u6b63\u5b57\u6cd5\u7cfb\u7edf\u5982\u4f55\u8c03\u8282\u6c49\u5b57\u624b\u5199\uff0c\u4ee5\u53ca\u7f3a\u4e4f\u6355\u83b7\u548c\u6279\u91cf\u5904\u7406\u7ec6\u7c92\u5ea6\u624b\u5199\u6570\u636e\u7684\u5de5\u5177\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u624b\u5199\u6570\u636e\u5e93\uff0c\u5176\u4e2d42\u540d\u4e2d\u56fd\u4eba\u5728\u542c\u5199\u4efb\u52a1\u4e2d\u624b\u5199\u4e861200\u4e2a\u6c49\u5b57\u3002\u589e\u5f3a\u4e86\u73b0\u6709\u7684\u624b\u5199\u5305\uff0c\u5e76\u4e3a\u5347\u7ea7\u540e\u7684OpenHandWrite_Toolbox\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6587\u6863\u3002", "result": "\u6b63\u5b57\u6cd5\u9884\u6d4b\u56e0\u5b50\u5f71\u54cd\u6c49\u5b57\u3001\u90e8\u9996\u548c\u7b14\u753b\u5c42\u9762\u7684\u624b\u5199\u51c6\u5907\u548c\u6267\u884c\u3002\u8bed\u97f3\u56e0\u7d20\u4e5f\u4f1a\u5f71\u54cd\u6240\u6709\u4e09\u4e2a\u5c42\u9762\u7684\u6267\u884c\u3002\u8fd9\u4e9b\u8bcd\u6c47\u6548\u5e94\u8868\u73b0\u51fa\u5206\u5c42\u8870\u51cf\u2014\u2014\u5728\u5b57\u7b26\u5c42\u9762\u6700\u660e\u663e\uff0c\u5176\u6b21\u662f\u90e8\u9996\uff0c\u5728\u7b14\u753b\u5c42\u9762\u6700\u5f31\u3002", "conclusion": "\u90e8\u9996\u548c\u7b14\u753b\u5c42\u9762\u7684\u624b\u5199\u51c6\u5907\u548c\u6267\u884c\u4e0e\u8bed\u8a00\u6210\u5206\u5bc6\u5207\u76f8\u5173\u3002\u8be5\u6570\u636e\u5e93\u548c\u5de5\u5177\u7bb1\u4e3a\u672a\u6765\u5fc3\u7406\u8bed\u8a00\u5b66\u548c\u795e\u7ecf\u8bed\u8a00\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u8d44\u6e90\u3002"}}
{"id": "2509.05488", "categories": ["cs.LG", "cs.AI", "cs.OS", "C.3; I.2.6; D.2.13; D.4.7"], "pdf": "https://arxiv.org/pdf/2509.05488", "abs": "https://arxiv.org/abs/2509.05488", "authors": ["Hongjun Xu", "Junxi Xia", "Weisi Yang", "Yueyuan Sui", "Stephen Xia"], "title": "MambaLite-Micro: Memory-Optimized Mamba Inference on MCUs", "comment": "4 pages, 1 figures", "summary": "Deploying Mamba models on microcontrollers (MCUs) remains challenging due to\nlimited memory, the lack of native operator support, and the absence of\nembedded-friendly toolchains. We present, to our knowledge, the first\ndeployment of a Mamba-based neural architecture on a resource-constrained MCU,\na fully C-based runtime-free inference engine: MambaLite-Micro. Our pipeline\nmaps a trained PyTorch Mamba model to on-device execution by (1) exporting\nmodel weights into a lightweight format, and (2) implementing a handcrafted\nMamba layer and supporting operators in C with operator fusion and memory\nlayout optimization. MambaLite-Micro eliminates large intermediate tensors,\nreducing 83.0% peak memory, while maintaining an average numerical error of\nonly 1.7x10-5 relative to the PyTorch Mamba implementation. When evaluated on\nkeyword spotting(KWS) and human activity recognition (HAR) tasks,\nMambaLite-Micro achieved 100% consistency with the PyTorch baselines, fully\npreserving classification accuracy. We further validated portability by\ndeploying on both ESP32S3 and STM32H7 microcontrollers, demonstrating\nconsistent operation across heterogeneous embedded platforms and paving the way\nfor bringing advanced sequence models like Mamba to real-world\nresource-constrained applications.", "AI": {"tldr": "MambaLite-Micro is the first deployment of a Mamba-based model on microcontrollers, using a C-based runtime-free engine.", "motivation": "Deploying Mamba models on MCUs is difficult due to memory limitations, lack of operator support, and absence of toolchains.", "method": "The pipeline maps a trained PyTorch Mamba model to on-device execution by exporting weights and implementing a handcrafted Mamba layer in C with optimizations.", "result": "MambaLite-Micro reduces memory usage by 83% with minimal numerical error and achieves 100% consistency with PyTorch baselines on KWS and HAR tasks. It was validated on ESP32S3 and STM32H7 microcontrollers.", "conclusion": "The work demonstrates consistent operation across heterogeneous embedded platforms, enabling the use of Mamba in resource-constrained applications."}}
{"id": "2509.05635", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.05635", "abs": "https://arxiv.org/abs/2509.05635", "authors": ["Liang Zhang", "Yuan Li", "Shijie Zhang", "Zheng Zhang", "Xitong Li"], "title": "Few-Shot Query Intent Detection via Relation-Aware Prompt Learning", "comment": null, "summary": "Intent detection is a crucial component of modern conversational systems,\nsince accurately identifying user intent at the beginning of a conversation is\nessential for generating effective responses. Recent efforts have focused on\nstudying this problem under a challenging few-shot scenario. These approaches\nprimarily leverage large-scale unlabeled dialogue text corpora to pretrain\nlanguage models through various pretext tasks, followed by fine-tuning for\nintent detection with very limited annotations. Despite the improvements\nachieved, existing methods have predominantly focused on textual data,\nneglecting to effectively capture the crucial structural information inherent\nin conversational systems, such as the query-query relation and query-answer\nrelation. To address this gap, we propose SAID, a novel framework that\nintegrates both textual and relational structure information in a unified\nmanner for model pretraining for the first time. Building on this framework, we\nfurther propose a novel mechanism, the query-adaptive attention network\n(QueryAdapt), which operates at the relation token level by generating\nintent-specific relation tokens from well-learned query-query and query-answer\nrelations explicitly, enabling more fine-grained knowledge transfer. Extensive\nexperimental results on two real-world datasets demonstrate that SAID\nsignificantly outperforms state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6SAID\uff0c\u5b83\u4ee5\u7edf\u4e00\u7684\u65b9\u5f0f\u6574\u5408\u4e86\u6587\u672c\u548c\u5173\u7cfb\u7ed3\u6784\u4fe1\u606f\uff0c\u7528\u4e8e\u6a21\u578b\u9884\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u6587\u672c\u6570\u636e\u4e0a\uff0c\u5ffd\u7565\u4e86\u6709\u6548\u6355\u83b7\u4f1a\u8bdd\u7cfb\u7edf\u4e2d\u56fa\u6709\u7684\u5173\u952e\u7ed3\u6784\u4fe1\u606f\uff0c\u4f8b\u5982query-query\u5173\u7cfb\u548cquery-answer\u5173\u7cfb\u3002", "method": "\u5728SAID\u6846\u67b6\u7684\u57fa\u7840\u4e0a\uff0c\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u673a\u5236\uff0c\u5373query-adaptive attention network (QueryAdapt)\uff0c\u5b83\u901a\u8fc7\u4ece\u826f\u597d\u5b66\u4e60\u7684query-query\u548cquery-answer\u5173\u7cfb\u4e2d\u663e\u5f0f\u751f\u6210\u7279\u5b9a\u4e8e\u610f\u56fe\u7684\u5173\u7cfbtokens\uff0c\u4ece\u800c\u5728\u5173\u7cfbtoken\u7ea7\u522b\u4e0a\u8fd0\u884c\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u7ec6\u7c92\u5ea6\u7684\u77e5\u8bc6\u8f6c\u79fb\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSAID\u660e\u663e\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u63d0\u9ad8\u610f\u56fe\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2509.05553", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05553", "abs": "https://arxiv.org/abs/2509.05553", "authors": ["Serge Lionel Nikiema", "Jordan Samhi", "Micheline B\u00e9n\u00e9dicte Moumoula", "Alb\u00e9rick Euraste Djir\u00e9", "Abdoul Kader Kabor\u00e9", "Jacques Klein", "Tegawend\u00e9 F. Bissyand\u00e9"], "title": "Using Contrastive Learning to Improve Two-Way Reasoning in Large Language Models: The Obfuscation Task as a Case Study", "comment": null, "summary": "This research addresses a fundamental question in AI: whether large language\nmodels truly understand concepts or simply recognize patterns. The authors\npropose bidirectional reasoning,the ability to apply transformations in both\ndirections without being explicitly trained on the reverse direction, as a test\nfor genuine understanding. They argue that true comprehension should naturally\nallow reversibility. For example, a model that can change a variable name like\nuserIndex to i should also be able to infer that i represents a user index\nwithout reverse training. The researchers tested current language models and\ndiscovered what they term cognitive specialization: when models are fine-tuned\non forward tasks, their performance on those tasks improves, but their ability\nto reason bidirectionally becomes significantly worse. To address this issue,\nthey developed Contrastive Fine-Tuning (CFT), which trains models using three\ntypes of examples: positive examples that maintain semantic meaning, negative\nexamples with different semantics, and forward-direction obfuscation examples.\nThis approach aims to develop deeper understanding rather than surface-level\npattern recognition and allows reverse capabilities to develop naturally\nwithout explicit reverse training. Their experiments demonstrated that CFT\nsuccessfully achieved bidirectional reasoning, enabling strong reverse\nperformance while maintaining forward task capabilities. The authors conclude\nthat bidirectional reasoning serves both as a theoretical framework for\nassessing genuine understanding and as a practical training approach for\ndeveloping more capable AI systems.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u771f\u6b63\u7406\u89e3\u6982\u5ff5\u8fd8\u662f\u4ec5\u4ec5\u8bc6\u522b\u6a21\u5f0f\uff1f\u672c\u6587\u63d0\u51fa\u53cc\u5411\u63a8\u7406\u6765\u6d4b\u8bd5\u771f\u6b63\u7684\u7406\u89e3\u3002\u53d1\u73b0\u6a21\u578b\u5728\u6b63\u5411\u4efb\u52a1\u4e0a\u8fdb\u884c\u5fae\u8c03\u65f6\uff0c\u5176\u53cc\u5411\u63a8\u7406\u80fd\u529b\u4f1a\u663e\u8457\u4e0b\u964d\u3002\u63d0\u51fa\u4e86\u5bf9\u6bd4\u5fae\u8c03\uff08CFT\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b63\u4f8b\u3001\u53cd\u4f8b\u548c\u6b63\u5411\u6a21\u7cca\u793a\u4f8b\u6765\u8bad\u7ec3\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u53cc\u5411\u63a8\u7406\u3002", "motivation": "\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u771f\u6b63\u7406\u89e3\u6982\u5ff5\uff0c\u8fd8\u662f\u4ec5\u4ec5\u8bc6\u522b\u6a21\u5f0f\u3002", "method": "\u63d0\u51fa\u53cc\u5411\u63a8\u7406\u7684\u6982\u5ff5\uff0c\u5e76\u8bbe\u8ba1\u5bf9\u6bd4\u5fae\u8c03\uff08CFT\uff09\u65b9\u6cd5\uff0c\u5305\u542b\u6b63\u4f8b\u3001\u53cd\u4f8b\u548c\u6b63\u5411\u6a21\u7cca\u793a\u4f8b\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u5728\u6b63\u5411\u4efb\u52a1\u4e0a\u8fdb\u884c\u5fae\u8c03\u65f6\uff0c\u53cc\u5411\u63a8\u7406\u80fd\u529b\u4f1a\u663e\u8457\u4e0b\u964d\u3002\u5bf9\u6bd4\u5fae\u8c03\uff08CFT\uff09\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u53cc\u5411\u63a8\u7406\uff0c\u540c\u65f6\u4fdd\u6301\u6b63\u5411\u4efb\u52a1\u7684\u80fd\u529b\u3002", "conclusion": "\u53cc\u5411\u63a8\u7406\u65e2\u53ef\u4ee5\u4f5c\u4e3a\u8bc4\u4f30\u771f\u6b63\u7406\u89e3\u7684\u7406\u8bba\u6846\u67b6\uff0c\u4e5f\u53ef\u4ee5\u4f5c\u4e3a\u5f00\u53d1\u66f4\u5f3a\u5927\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u5b9e\u9645\u8bad\u7ec3\u65b9\u6cd5\u3002"}}
{"id": "2509.05469", "categories": ["cs.AI", "cs.CV", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.05469", "abs": "https://arxiv.org/abs/2509.05469", "authors": ["Chenguang Wang", "Xiang Yan", "Yilong Dai", "Ziyi Wang", "Susu Xu"], "title": "From Image Generation to Infrastructure Design: a Multi-agent Pipeline for Street Design Generation", "comment": "21 pages, 8 figures", "summary": "Realistic visual renderings of street-design scenarios are essential for\npublic engagement in active transportation planning. Traditional approaches are\nlabor-intensive, hindering collective deliberation and collaborative\ndecision-making. While AI-assisted generative design shows transformative\npotential by enabling rapid creation of design scenarios, existing generative\napproaches typically require large amounts of domain-specific training data and\nstruggle to enable precise spatial variations of design/configuration in\ncomplex street-view scenes. We introduce a multi-agent system that edits and\nredesigns bicycle facilities directly on real-world street-view imagery. The\nframework integrates lane localization, prompt optimization, design generation,\nand automated evaluation to synthesize realistic, contextually appropriate\ndesigns. Experiments across diverse urban scenarios demonstrate that the system\ncan adapt to varying road geometries and environmental conditions, consistently\nyielding visually coherent and instruction-compliant results. This work\nestablishes a foundation for applying multi-agent pipelines to transportation\ninfrastructure planning and facility design.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u53ef\u4ee5\u76f4\u63a5\u5728\u771f\u5b9e\u8857\u666f\u56fe\u50cf\u4e0a\u7f16\u8f91\u548c\u91cd\u65b0\u8bbe\u8ba1\u81ea\u884c\u8f66\u8bbe\u65bd\uff0c\u7528\u4e8e\u4e3b\u52a8\u4ea4\u901a\u89c4\u5212\u4e2d\u7684\u516c\u5171\u53c2\u4e0e\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u52b3\u52a8\u5bc6\u96c6\uff0c\u963b\u788d\u4e86\u96c6\u4f53\u5ba1\u8bae\u548c\u534f\u4f5c\u51b3\u7b56\u3002\u73b0\u6709\u7684\u751f\u6210\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u4e14\u96be\u4ee5\u5728\u590d\u6742\u7684\u8857\u666f\u573a\u666f\u4e2d\u5b9e\u73b0\u8bbe\u8ba1\u7684\u7cbe\u786e\u7a7a\u95f4\u53d8\u5316\u3002", "method": "\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u8f66\u9053\u5b9a\u4f4d\u3001\u63d0\u793a\u4f18\u5316\u3001\u8bbe\u8ba1\u751f\u6210\u548c\u81ea\u52a8\u8bc4\u4f30\uff0c\u4ee5\u5408\u6210\u903c\u771f\u7684\u3001\u7b26\u5408\u4e0a\u4e0b\u6587\u7684\u8bbe\u8ba1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u53ef\u4ee5\u9002\u5e94\u4e0d\u540c\u7684\u9053\u8def\u51e0\u4f55\u5f62\u72b6\u548c\u73af\u5883\u6761\u4ef6\uff0c\u59cb\u7ec8\u4ea7\u751f\u89c6\u89c9\u8fde\u8d2f\u4e14\u7b26\u5408\u6307\u4ee4\u7684\u7ed3\u679c\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5c06\u591a\u667a\u80fd\u4f53\u7ba1\u9053\u5e94\u7528\u4e8e\u4ea4\u901a\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u548c\u8bbe\u65bd\u8bbe\u8ba1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.05337", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.05337", "abs": "https://arxiv.org/abs/2509.05337", "authors": ["Younggeol Cho", "Gokhan Solak", "Olivia Nocentini", "Marta Lorenzini", "Andrea Fortuna", "Arash Ajoudani"], "title": "Anticipatory Fall Detection in Humans with Hybrid Directed Graph Neural Networks and Long Short-Term Memory", "comment": "Presented at IEEE RO-MAN 2025", "summary": "Detecting and preventing falls in humans is a critical component of assistive\nrobotic systems. While significant progress has been made in detecting falls,\nthe prediction of falls before they happen, and analysis of the transient state\nbetween stability and an impending fall remain unexplored. In this paper, we\npropose a anticipatory fall detection method that utilizes a hybrid model\ncombining Dynamic Graph Neural Networks (DGNN) with Long Short-Term Memory\n(LSTM) networks that decoupled the motion prediction and gait classification\ntasks to anticipate falls with high accuracy. Our approach employs real-time\nskeletal features extracted from video sequences as input for the proposed\nmodel. The DGNN acts as a classifier, distinguishing between three gait states:\nstable, transient, and fall. The LSTM-based network then predicts human\nmovement in subsequent time steps, enabling early detection of falls. The\nproposed model was trained and validated using the OUMVLP-Pose and URFD\ndatasets, demonstrating superior performance in terms of prediction error and\nrecognition accuracy compared to models relying solely on DGNN and models from\nliterature. The results indicate that decoupling prediction and classification\nimproves performance compared to addressing the unified problem using only the\nDGNN. Furthermore, our method allows for the monitoring of the transient state,\noffering valuable insights that could enhance the functionality of advanced\nassistance systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6a21\u578b\uff0c\u7ed3\u5408\u52a8\u6001\u56fe\u795e\u7ecf\u7f51\u7edc\uff08DGNN\uff09\u548c\u957f\u77ed\u671f\u8bb0\u5fc6\uff08LSTM\uff09\u7f51\u7edc\uff0c\u7528\u4e8e\u9884\u6d4b\u8dcc\u5012\u3002", "motivation": "\u5728\u8dcc\u5012\u68c0\u6d4b\u65b9\u9762\u5df2\u53d6\u5f97 \u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u8fdb\u5c55\uff0c\u4f46\u5728\u8dcc\u5012\u53d1\u751f\u524d\u7684\u9884\u6d4b\u4ee5\u53ca\u7a33\u5b9a\u548c\u5373\u5c06\u53d1\u751f\u7684\u8dcc\u5012\u4e4b\u95f4\u7684\u77ac\u6001\u5206\u6790\u4ecd\u6709\u5f85\u63a2\u7d22\u3002", "method": "\u5229\u7528\u6df7\u5408\u6a21\u578b\uff0c\u7ed3\u5408\u52a8\u6001\u56fe\u795e\u7ecf\u7f51\u7edc\uff08DGNN\uff09\u548c\u957f\u77ed\u671f\u8bb0\u5fc6\uff08LSTM\uff09\u7f51\u7edc\uff0c\u89e3\u8026\u8fd0\u52a8\u9884\u6d4b\u548c\u6b65\u6001\u5206\u7c7b\u4efb\u52a1\uff0c\u4ee5\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u8dcc\u5012\u3002DGNN \u4f5c\u4e3a\u5206\u7c7b\u5668\uff0c\u533a\u5206\u4e09\u79cd\u6b65\u6001\u72b6\u6001\uff1a\u7a33\u5b9a\u3001\u77ac\u6001\u548c\u8dcc\u5012\u3002\u7136\u540e\uff0c\u57fa\u4e8e LSTM \u7684\u7f51\u7edc\u9884\u6d4b\u540e\u7eed\u65f6\u95f4\u6b65\u9aa4\u4e2d\u7684\u4eba\u4f53\u8fd0\u52a8\uff0c\u4ece\u800c\u80fd\u591f\u53ca\u65e9\u53d1\u73b0\u8dcc\u5012\u3002", "result": "\u8be5\u6a21\u578b\u5728 OUMVLP-Pose \u548c URFD \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\u548c\u9a8c\u8bc1\uff0c\u5728\u9884\u6d4b\u8bef\u5dee\u548c\u8bc6\u522b\u7cbe\u5ea6\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u4e8e\u4ec5\u4f9d\u8d56 DGNN \u7684\u6a21\u578b\u548c\u6587\u732e\u4e2d\u7684\u6a21\u578b\u3002", "conclusion": "\u89e3\u8026\u9884\u6d4b\u548c\u5206\u7c7b\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u5e76\u4e14\u8be5\u65b9\u6cd5\u53ef\u4ee5\u76d1\u6d4b\u77ac\u6001\uff0c\u4e3a\u589e\u5f3a\u9ad8\u7ea7\u8f85\u52a9\u7cfb\u7edf\u7684\u529f\u80fd\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2509.05489", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05489", "abs": "https://arxiv.org/abs/2509.05489", "authors": ["Peixuan Han", "Adit Krishnan", "Gerald Friedland", "Jiaxuan You", "Chris Kong"], "title": "Self-Aligned Reward: Towards Effective and Efficient Reasoners", "comment": null, "summary": "Reinforcement learning with verifiable rewards has significantly advanced\nreasoning in large language models (LLMs), but such signals remain coarse,\noffering only binary correctness feedback. This limitation often results in\ninefficiencies, including overly verbose reasoning and high computational cost,\nwhile existing solutions often compromise accuracy. To address this, we\nintroduce self-aligned reward (SAR), a self-guided signal that complements\nverifiable rewards to encourage both reasoning accuracy and efficiency. SAR is\ndefined as the relative perplexity difference between an answer conditioned on\nthe query and the standalone answer, thereby favoring responses that are\nconcise and query-specific. Quantitative analysis reveals that SAR reliably\ndistinguishes answer quality: concise, correct answers score higher than\nredundant ones, and partially correct answers score higher than entirely\nincorrect ones. Evaluation on 4 models across 7 benchmarks shows that\nintegrating SAR with prevalent RL algorithms like PPO and GRPO improves\naccuracy by 4%, while reducing inference cost by 30%. Further analysis\ndemonstrates that SAR achieves a Pareto-optimal trade-off between correctness\nand efficiency compared to reward signals based on length or self-confidence.\nWe also show that SAR shortens responses while preserving advanced reasoning\nbehaviors, demonstrating its ability to suppress unnecessary elaboration\nwithout losing critical reasoning. These results highlight the promise of\nself-aligned reward as a fine-grained complement to verifiable rewards, paving\nthe way for more efficient and effective LLM training.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u81ea\u5bf9\u9f50\u5956\u52b1\uff08SAR\uff09\uff0c\u4ee5\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u53ef\u9a8c\u8bc1\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728LLM\u63a8\u7406\u4e2d\u5b58\u5728\u6548\u7387\u95ee\u9898\uff0c\u4f8b\u5982\u8fc7\u5ea6\u5197\u957f\u7684\u63a8\u7406\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u4e14\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u4f1a\u727a\u7272\u51c6\u786e\u6027\u3002", "method": "\u5b9a\u4e49SAR\u4e3a\u57fa\u4e8e\u67e5\u8be2\u6761\u4ef6\u4e0b\u7684\u7b54\u6848\u4e0e\u72ec\u7acb\u7b54\u6848\u4e4b\u95f4\u7684\u76f8\u5bf9\u56f0\u60d1\u5ea6\u5dee\u5f02\uff0c\u4ece\u800c\u503e\u5411\u4e8e\u7b80\u6d01\u4e14\u7279\u5b9a\u4e8e\u67e5\u8be2\u7684\u54cd\u5e94\u3002\u5c06SAR\u4e0ePPO\u548cGRPO\u7b49\u4e3b\u6d41RL\u7b97\u6cd5\u96c6\u6210\u3002", "result": "\u57287\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSAR\u5c06\u51c6\u786e\u7387\u63d0\u9ad8\u4e864%\uff0c\u540c\u65f6\u5c06\u63a8\u7406\u6210\u672c\u964d\u4f4e\u4e8630%\u3002SAR\u5728\u6b63\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u5b9e\u73b0\u4e86\u5e15\u7d2f\u6258\u6700\u4f18\u7684\u6743\u8861\u3002", "conclusion": "\u81ea\u5bf9\u9f50\u5956\u52b1\u53ef\u4ee5\u4f5c\u4e3a\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u7ec6\u7c92\u5ea6\u8865\u5145\uff0c\u4e3a\u66f4\u9ad8\u6548\u3001\u66f4\u6709\u6548\u7684LLM\u8bad\u7ec3\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2509.05703", "categories": ["cs.CV", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.05703", "abs": "https://arxiv.org/abs/2509.05703", "authors": ["Ragib Amin Nihal", "Benjamin Yen", "Takeshi Ashizawa", "Kazuhiro Nakadai"], "title": "Knowledge-Augmented Vision Language Models for Underwater Bioacoustic Spectrogram Analysis", "comment": null, "summary": "Marine mammal vocalization analysis depends on interpreting bioacoustic\nspectrograms. Vision Language Models (VLMs) are not trained on these\ndomain-specific visualizations. We investigate whether VLMs can extract\nmeaningful patterns from spectrograms visually. Our framework integrates VLM\ninterpretation with LLM-based validation to build domain knowledge. This\nenables adaptation to acoustic data without manual annotation or model\nretraining.", "AI": {"tldr": "\u7814\u7a76\u5982\u4f55\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u5206\u6790\u6d77\u6d0b\u54fa\u4e73\u52a8\u7269\u7684\u58f0\u97f3", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6ca1\u6709\u7ecf\u8fc7\u7279\u5b9a\u9886\u57df\u751f\u7269\u58f0\u5b66\u9891\u8c31\u56fe\u7684\u8bad\u7ec3", "method": "\u5c06 VLM \u89e3\u91ca\u4e0e\u57fa\u4e8e LLM \u7684\u9a8c\u8bc1\u76f8\u7ed3\u5408\uff0c\u4ee5\u6784\u5efa\u9886\u57df\u77e5\u8bc6", "result": "\u65e0\u9700\u624b\u52a8\u6ce8\u91ca\u6216\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u9002\u5e94\u58f0\u97f3\u6570\u636e", "conclusion": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u89c6\u89c9\u65b9\u5f0f\u4ece\u9891\u8c31\u56fe\u4e2d\u63d0\u53d6\u6709\u610f\u4e49\u7684\u6a21\u5f0f"}}
{"id": "2509.05566", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.05566", "abs": "https://arxiv.org/abs/2509.05566", "authors": ["Anya Ji", "Claire Augusta Bergey", "Ron Eliav", "Yoav Artzi", "Robert D. Hawkins"], "title": "Ad hoc conventions generalize to new referents", "comment": null, "summary": "How do people talk about things they've never talked about before? One view\nsuggests that a new shared naming system establishes an arbitrary link to a\nspecific target, like proper names that cannot extend beyond their bearers. An\nalternative view proposes that forming a shared way of describing objects\ninvolves broader conceptual alignment, reshaping each individual's semantic\nspace in ways that should generalize to new referents. We test these competing\naccounts in a dyadic communication study (N=302) leveraging the\nrecently-released KiloGram dataset containing over 1,000 abstract tangram\nimages. After pairs of participants coordinated on referential conventions for\none set of images through repeated communication, we measured the extent to\nwhich their descriptions aligned for undiscussed images. We found strong\nevidence for generalization: partners showed increased alignment relative to\ntheir pre-test labels. Generalization also decayed nonlinearly with visual\nsimilarity (consistent with Shepard's law) and was robust across levels of the\nimages' nameability. These findings suggest that ad hoc conventions are not\narbitrary labels but reflect genuine conceptual coordination, with implications\nfor theories of reference and the design of more adaptive language agents.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4eba\u4eec\u5982\u4f55\u8c08\u8bba\u4ed6\u4eec\u4ee5\u524d\u4ece\u672a\u8c08\u8bba\u8fc7\u7684\u4e8b\u60c5\uff0c\u5e76\u68c0\u9a8c\u4e86\u4e24\u79cd\u76f8\u4e92\u7ade\u4e89\u7684\u89c2\u70b9\uff1a\u5171\u4eab\u547d\u540d\u7cfb\u7edf\u4e0e\u66f4\u5e7f\u6cdb\u7684\u6982\u5ff5\u5bf9\u9f50\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u8ba8\u4eba\u4eec\u5728\u8c08\u8bba\u65b0\u4e8b\u7269\u65f6\uff0c\u662f\u5efa\u7acb\u4efb\u610f\u7684\u94fe\u63a5\uff0c\u8fd8\u662f\u8fdb\u884c\u66f4\u5e7f\u6cdb\u7684\u6982\u5ff5\u5bf9\u9f50\u3002", "method": "\u901a\u8fc7\u53cc\u4eba\u4ea4\u6d41\u7814\u7a76\uff0c\u5229\u7528KiloGram\u6570\u636e\u96c6\uff0c\u53c2\u4e0e\u8005\u901a\u8fc7\u91cd\u590d\u4ea4\u6d41\u6765\u534f\u8c03\u4e00\u7ec4\u56fe\u50cf\u7684\u6307\u79f0\u7ea6\u5b9a\uff0c\u5e76\u6d4b\u91cf\u4ed6\u4eec\u5bf9\u672a\u8ba8\u8bba\u56fe\u50cf\u7684\u63cf\u8ff0\u5bf9\u9f50\u7a0b\u5ea6\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4f19\u4f34\u4eec\u76f8\u5bf9\u4e8e\u4ed6\u4eec\u7684\u9884\u6d4b\u8bd5\u6807\u7b7e\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u5bf9\u9f50\u5ea6\uff0c\u4e14\u6cdb\u5316\u80fd\u529b\u968f\u7740\u89c6\u89c9\u76f8\u4f3c\u6027\u7684\u589e\u52a0\u800c\u975e\u7ebf\u6027\u8870\u51cf\u3002\u56fe\u50cf\u7684\u53ef\u547d\u540d\u6027\u6c34\u5e73\u5bf9\u7ed3\u679c\u5f71\u54cd\u4e0d\u5927\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u7279\u522b\u7684\u7ea6\u5b9a\u4e0d\u662f\u968f\u610f\u7684\u6807\u7b7e\uff0c\u800c\u662f\u53cd\u6620\u4e86\u771f\u6b63\u7684\u6982\u5ff5\u534f\u8c03\uff0c\u5bf9\u6307\u79f0\u7406\u8bba\u548c\u66f4\u81ea\u9002\u5e94\u7684\u8bed\u8a00\u4ee3\u7406\u7684\u8bbe\u8ba1\u5177\u6709\u5f71\u54cd\u3002"}}
{"id": "2509.05550", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05550", "abs": "https://arxiv.org/abs/2509.05550", "authors": ["Zixi Li"], "title": "TreeGPT: A Novel Hybrid Architecture for Abstract Syntax Tree Processing with Global Parent-Child Aggregation", "comment": "Code available at: https://github.com/lizixi-0x2F/TreeGPT", "summary": "We introduce TreeGPT, a novel neural architecture that combines\ntransformer-based attention mechanisms with global parent-child aggregation for\nprocessing Abstract Syntax Trees (ASTs) in neural program synthesis tasks.\nUnlike traditional approaches that rely solely on sequential processing or\ngraph neural networks, TreeGPT employs a hybrid design that leverages both\nself-attention for capturing local dependencies and a specialized Tree\nFeed-Forward Network (TreeFFN) for modeling hierarchical tree structures\nthrough iterative message passing.\n  The core innovation lies in our Global Parent-Child Aggregation mechanism,\nformalized as: $$h_i^{(t+1)} = \\sigma \\Big( h_i^{(0)} + W_{pc} \\sum_{(p,c) \\in\nE_i} f(h_p^{(t)}, h_c^{(t)}) + b \\Big)$$ where $h_i^{(t)}$ represents the\nhidden state of node $i$ at iteration $t$, $E_i$ denotes all parent-child edges\ninvolving node $i$, and $f(h_p, h_c)$ is an edge aggregation function. This\nformulation enables each node to progressively aggregate information from the\nentire tree structure through $T$ iterations.\n  Our architecture integrates optional enhancements including gated aggregation\nwith learnable edge weights, residual connections for gradient stability, and\nbidirectional propagation for capturing both bottom-up and top-down\ndependencies. We evaluate TreeGPT on the ARC Prize 2025 dataset, a challenging\nvisual reasoning benchmark requiring abstract pattern recognition and rule\ninference. Experimental results demonstrate that TreeGPT achieves 96\\%\naccuracy, significantly outperforming transformer baselines (1.3\\%),\nlarge-scale models like Grok-4 (15.9\\%), and specialized program synthesis\nmethods like SOAR (52\\%) while using only 1.5M parameters. Our comprehensive\nablation study reveals that edge projection is the most critical component,\nwith the combination of edge projection and gating achieving optimal\nperformance.", "AI": {"tldr": "TreeGPT: A new neural network for program synthesis using abstract syntax trees (ASTs).", "motivation": "Existing methods for program synthesis either use sequential processing or graph neural networks, which have limitations in capturing hierarchical tree structures and long-range dependencies.", "method": "A hybrid architecture combining transformer-based self-attention with a novel Tree Feed-Forward Network (TreeFFN) that uses global parent-child aggregation.", "result": "Achieved 96% accuracy on the ARC Prize 2025 dataset, significantly outperforming transformer baselines, large-scale models, and specialized program synthesis methods with only 1.5M parameters.", "conclusion": "TreeGPT's global parent-child aggregation mechanism, particularly with edge projection and gating, is highly effective for processing ASTs and achieving state-of-the-art results in program synthesis."}}
{"id": "2509.05340", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05340", "abs": "https://arxiv.org/abs/2509.05340", "authors": ["Dibya Jyoti Bora", "Mrinal Kanti Mishra"], "title": "Comparative Evaluation of Hard and Soft Clustering for Precise Brain Tumor Segmentation in MR Imaging", "comment": "15 pages, 10 figures", "summary": "Segmentation of brain tumors from Magnetic Resonance Imaging (MRI) remains a\npivotal challenge in medical image analysis due to the heterogeneous nature of\ntumor morphology and intensity distributions. Accurate delineation of tumor\nboundaries is critical for clinical decision-making, radiotherapy planning, and\nlongitudinal disease monitoring. In this study, we perform a comprehensive\ncomparative analysis of two major clustering paradigms applied in MRI tumor\nsegmentation: hard clustering, exemplified by the K-Means algorithm, and soft\nclustering, represented by Fuzzy C-Means (FCM). While K-Means assigns each\npixel strictly to a single cluster, FCM introduces partial memberships, meaning\neach pixel can belong to multiple clusters with varying degrees of association.\nExperimental validation was performed using the BraTS2020 dataset,\nincorporating pre-processing through Gaussian filtering and Contrast Limited\nAdaptive Histogram Equalization (CLAHE). Evaluation metrics included the Dice\nSimilarity Coefficient (DSC) and processing time, which collectively\ndemonstrated that K-Means achieved superior speed with an average runtime of\n0.3s per image, whereas FCM attained higher segmentation accuracy with an\naverage DSC of 0.67 compared to 0.43 for K-Means, albeit at a higher\ncomputational cost (1.3s per image). These results highlight the inherent\ntrade-off between computational efficiency and boundary precision.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u4e24\u79cd\u805a\u7c7b\u7b97\u6cd5\uff08K-Means \u548c FCM\uff09\u5728\u8111\u80bf\u7624MRI\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u8111\u80bf\u7624MRI\u56fe\u50cf\u5206\u5272\u662f\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u7cbe\u786e\u7684\u80bf\u7624\u8fb9\u754c deline \u5bf9\u4e8e\u4e34\u5e8a\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528 K-Means\uff08\u786c\u805a\u7c7b\uff09\u548c FCM\uff08\u8f6f\u805a\u7c7b\uff09\u7b97\u6cd5\u5206\u5272 BraTS2020 \u6570\u636e\u96c6\u4e2d\u7684\u8111\u80bf\u7624MRI\u56fe\u50cf\uff0c\u5e76\u4f7f\u7528\u9ad8\u65af\u6ee4\u6ce2\u548c CLAHE \u8fdb\u884c\u9884\u5904\u7406\u3002", "result": "K-Means \u901f\u5ea6\u66f4\u5feb (0.3s/\u56fe\u50cf)\uff0c\u4f46 FCM \u7cbe\u5ea6\u66f4\u9ad8 (DSC=0.67 vs 0.43)\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u66f4\u9ad8 (1.3s/\u56fe\u50cf)\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\u8ba1\u7b97\u6548\u7387\u548c\u8fb9\u754c\u7cbe\u5ea6\u4e4b\u95f4\u5b58\u5728\u56fa\u6709\u7684\u6743\u8861\u3002"}}
{"id": "2509.05542", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05542", "abs": "https://arxiv.org/abs/2509.05542", "authors": ["Qi Cao", "Pengtao Xie"], "title": "DreamPRM-1.5: Unlocking the Potential of Each Instance for Multimodal Process Reward Model Training", "comment": null, "summary": "Training multimodal process reward models (PRMs) is challenged by\ndistribution shifts and noisy data. We introduce DreamPRM-1.5, an\ninstance-reweighted framework that adaptively adjusts the importance of each\ntraining example via bi-level optimization. We design two complementary\nstrategies: Instance Table, effective for smaller datasets, and Instance Net,\nscalable to larger ones. Integrated into test-time scaling, DreamPRM-1.5\nachieves 84.6 accuracy on the MMMU benchmark, surpassing GPT-5.", "AI": {"tldr": "DreamPRM-1.5\u662f\u4e00\u79cd\u901a\u8fc7\u5b9e\u4f8b\u91cd\u52a0\u6743\u6765\u4f18\u5316\u591a\u6a21\u6001\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b(PRM)\u8bad\u7ec3\u7684\u6846\u67b6\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\u4e2d\u5b58\u5728\u7684\u5206\u5e03\u504f\u79fb\u548c\u566a\u58f0\u6570\u636e\u95ee\u9898\u3002", "method": "\u63d0\u51faDreamPRM-1.5\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u81ea\u9002\u5e94\u5730\u8c03\u6574\u6bcf\u4e2a\u8bad\u7ec3\u6837\u672c\u7684\u91cd\u8981\u6027\u3002\u8bbe\u8ba1\u4e86\u4e24\u79cd\u4e92\u8865\u7b56\u7565\uff1aInstance Table\uff08\u9002\u7528\u4e8e\u8f83\u5c0f\u6570\u636e\u96c6\uff09\u548cInstance Net\uff08\u53ef\u6269\u5c55\u5230\u8f83\u5927\u6570\u636e\u96c6\uff09\u3002", "result": "\u5728MMMU\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDreamPRM-1.5\u5b9e\u73b0\u4e8684.6\u7684\u51c6\u786e\u7387\u3002", "conclusion": "DreamPRM-1.5\u6846\u67b6\u5728\u6d4b\u8bd5\u65f6\u6269\u5c55\u540e\uff0c\u6027\u80fd\u8d85\u8fc7\u4e86GPT-5\u3002"}}
{"id": "2509.05874", "categories": ["cs.LG", "cs.AI", "cs.IR", "I.2.6"], "pdf": "https://arxiv.org/pdf/2509.05874", "abs": "https://arxiv.org/abs/2509.05874", "authors": ["Shao-An Yin"], "title": "Learning to Construct Knowledge through Sparse Reference Selection with Reinforcement Learning", "comment": "8 pages, 2 figures", "summary": "The rapid expansion of scientific literature makes it increasingly difficult\nto acquire new knowledge, particularly in specialized domains where reasoning\nis complex, full-text access is restricted, and target references are sparse\namong a large set of candidates. We present a Deep Reinforcement Learning\nframework for sparse reference selection that emulates human knowledge\nconstruction, prioritizing which papers to read under limited time and cost.\nEvaluated on drug--gene relation discovery with access restricted to titles and\nabstracts, our approach demonstrates that both humans and machines can\nconstruct knowledge effectively from partial information.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4fe1\u606f\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u9009\u62e9\u9605\u8bfb\u54ea\u4e9b\u8bba\u6587\uff0c\u4ee5\u6a21\u62df\u4eba\u7c7b\u77e5\u8bc6\u6784\u5efa\u3002", "motivation": "\u79d1\u5b66\u6587\u732e\u5feb\u901f\u6269\u5f20\uff0c\u96be\u4ee5\u83b7\u53d6\u65b0\u77e5\u8bc6\uff0c\u7279\u522b\u662f\u5728\u4e13\u4e1a\u9886\u57df\uff0c\u63a8\u7406\u590d\u6742\uff0c\u5168\u6587\u8bbf\u95ee\u53d7\u9650\uff0c\u76ee\u6807\u53c2\u8003\u6587\u732e\u7a00\u758f\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u6a21\u62df\u4eba\u7c7b\u77e5\u8bc6\u6784\u5efa\uff0c\u4f18\u5148\u8003\u8651\u5728\u6709\u9650\u65f6\u95f4\u548c\u6210\u672c\u4e0b\u9605\u8bfb\u54ea\u4e9b\u8bba\u6587\u3002", "result": "\u5728\u836f\u7269-\u57fa\u56e0\u5173\u7cfb\u53d1\u73b0\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u8bbf\u95ee\u6743\u9650\u4ec5\u9650\u4e8e\u6807\u9898\u548c\u6458\u8981\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u4eba\u7c7b\u548c\u673a\u5668\u90fd\u53ef\u4ee5\u4ece\u90e8\u5206\u4fe1\u606f\u4e2d\u6709\u6548\u5730\u6784\u5efa\u77e5\u8bc6\u3002", "conclusion": "\u5728\u4fe1\u606f\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u4eba\u7c7b\u548c\u673a\u5668\u90fd\u53ef\u4ee5\u6709\u6548\u5730\u6784\u5efa\u77e5\u8bc6\u3002"}}
{"id": "2509.05602", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.05602", "abs": "https://arxiv.org/abs/2509.05602", "authors": ["Hongyan Xie", "Yitong Yao", "Yikun Ban", "Zixuan Huang", "Deqing Wang", "Zhenhe Wu", "Haoxiang Su", "Chao Wang", "Shuangyong Song", "Xuelong Li"], "title": "Mitigating Spurious Correlations Between Question and Answer via Chain-of-Thought Correctness Perception Distillation", "comment": "PrePrint", "summary": "Large language models (LLMs) excel at reasoning tasks but are expensive to\ndeploy. Thus small language models (SLMs) are fine-tuned on CoT data generated\nby LLMs to copy LLMs' abilities. However, these CoT data may include noisy\nrationales that either fail to substantiate the answers or contribute no\nadditional information to support answer prediction, which leads SLMs to\ncapture spurious correlations between questions and answers and compromise the\nquality of reasoning. In this work, we propose Chain-of-Thought Correctness\nPerception Distillation (CoPeD), which aims to improve the reasoning quality of\nthe student model from the perspectives of task setting and data utilization.\nFirstly, we introduce a correctness-aware task setting that encourages the\nstudent model to predict answers based on correct rationales and revise them\nwhen they are incorrect. This setting improves the faithfulness of reasoning\nand allows the model to learn from its mistakes. Then, we propose a\nCorrectness-Aware Weighted loss, which dynamically adjusts the contribution of\neach training instance based on the combined loss of the rationale and the\nanswer. This strategy encourages the model to focus more on samples where the\nrationale offers stronger support for the correct answer. Experiments have\nshown that CoPeD is effective on both in-distribution (IND) and\nout-of-distribution (OOD) benchmark reasoning datasets.", "AI": {"tldr": "CoPeD\u901a\u8fc7\u5173\u6ce8\u6b63\u786e\u7684\u63a8\u7406\u94fe\u6765\u63d0\u9ad8\u5c0f\u8bed\u8a00\u6a21\u578b(SLM)\u7684\u63a8\u7406\u80fd\u529b\uff0c\u51cf\u5c11\u566a\u58f0\u6570\u636e\u7684\u5f71\u54cd\u3002", "motivation": "\u5c0f\u8bed\u8a00\u6a21\u578b(SLM)\u901a\u8fc7\u6a21\u4eff\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u7684CoT\u6570\u636e\u8fdb\u884c\u5fae\u8c03\uff0c\u4f46\u8fd9\u4e9b\u6570\u636e\u5305\u542b\u566a\u58f0\uff0c\u5bfc\u81f4SLM\u5b66\u4e60\u5230\u9519\u8bef\u7684\u5173\u8054\u3002", "method": "\u63d0\u51fa\u4e86Chain-of-Thought Correctness Perception Distillation (CoPeD)\uff0c\u5b83\u4ece\u4efb\u52a1\u8bbe\u7f6e\u548c\u6570\u636e\u5229\u7528\u7684\u89d2\u5ea6\u6539\u8fdbSLM\u7684\u63a8\u7406\u8d28\u91cf\u3002\u5f15\u5165\u4e86\u6b63\u786e\u6027\u611f\u77e5\u4efb\u52a1\u8bbe\u7f6e\uff0c\u9f13\u52b1\u6a21\u578b\u57fa\u4e8e\u6b63\u786e\u7684\u7406\u7531\u9884\u6d4b\u7b54\u6848\uff0c\u5e76\u5728\u4e0d\u6b63\u786e\u65f6\u4fee\u6539\u7b54\u6848\u3002\u63d0\u51fa\u4e86\u6b63\u786e\u6027\u611f\u77e5\u52a0\u6743\u635f\u5931\uff0c\u6839\u636e\u7406\u7531\u548c\u7b54\u6848\u7684\u7ec4\u5408\u635f\u5931\u52a8\u6001\u8c03\u6574\u6bcf\u4e2a\u8bad\u7ec3\u5b9e\u4f8b\u7684\u8d21\u732e\u3002", "result": "CoPeD\u5728\u540c\u5206\u5e03(IND)\u548c\u5f02\u5206\u5e03(OOD)\u57fa\u51c6\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u5747\u6709\u6548\u3002", "conclusion": "CoPeD\u6709\u6548\u5730\u63d0\u9ad8\u4e86SLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u51cf\u5c11\u4e86\u566a\u58f0\u6570\u636e\u7684\u5f71\u54cd\u3002"}}
{"id": "2509.05578", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.05578", "abs": "https://arxiv.org/abs/2509.05578", "authors": ["Ruixun Liu", "Lingyu Kong", "Derun Li", "Hang Zhao"], "title": "OccVLA: Vision-Language-Action Model with Implicit 3D Occupancy Supervision", "comment": null, "summary": "Multimodal large language models (MLLMs) have shown strong vision-language\nreasoning abilities but still lack robust 3D spatial understanding, which is\ncritical for autonomous driving. This limitation stems from two key challenges:\n(1) the difficulty of constructing accessible yet effective 3D representations\nwithout expensive manual annotations, and (2) the loss of fine-grained spatial\ndetails in VLMs due to the absence of large-scale 3D vision-language\npretraining. To address these challenges, we propose OccVLA, a novel framework\nthat integrates 3D occupancy representations into a unified multimodal\nreasoning process. Unlike prior approaches that rely on explicit 3D inputs,\nOccVLA treats dense 3D occupancy as both a predictive output and a supervisory\nsignal, enabling the model to learn fine-grained spatial structures directly\nfrom 2D visual inputs. The occupancy predictions are regarded as implicit\nreasoning processes and can be skipped during inference without performance\ndegradation, thereby adding no extra computational overhead. OccVLA achieves\nstate-of-the-art results on the nuScenes benchmark for trajectory planning and\ndemonstrates superior performance on 3D visual question-answering tasks,\noffering a scalable, interpretable, and fully vision-based solution for\nautonomous driving.", "AI": {"tldr": "OccVLA\uff1a\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7684\u65b0\u6846\u67b6\uff0c\u5b83\u5c06 3D occupancy representations \u96c6\u6210\u5230\u7edf\u4e00\u7684\u591a\u6a21\u6001\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684 3D \u7a7a\u95f4\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u7f3a\u4e4f\u5f3a\u5927\u7684 3D \u7a7a\u95f4\u7406\u89e3\u80fd\u529b\uff0c\u8fd9\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u81f3\u5173\u91cd\u8981\u3002 \u8fd9\u79cd\u5c40\u9650\u6027\u6e90\u4e8e\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a(1) \u5728\u6ca1\u6709\u6602\u8d35\u7684\u624b\u52a8\u6ce8\u91ca\u7684\u60c5\u51b5\u4e0b\uff0c\u96be\u4ee5\u6784\u5efa\u53ef\u8bbf\u95ee\u4f46\u6709\u6548\u7684 3D \u8868\u793a\uff0c\u4ee5\u53ca (2) \u7531\u4e8e\u7f3a\u4e4f\u5927\u89c4\u6a21\u7684 3D \u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\uff0cVLM \u4e2d\u4e22\u5931\u4e86\u7ec6\u7c92\u5ea6\u7684\u7a7a\u95f4\u7ec6\u8282\u3002", "method": "OccVLA \u6846\u67b6\u5c06\u5bc6\u96c6\u7684 3D occupancy \u89c6\u4e3a\u9884\u6d4b\u8f93\u51fa\u548c\u76d1\u7763\u4fe1\u53f7\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u76f4\u63a5\u4ece 2D \u89c6\u89c9\u8f93\u5165\u4e2d\u5b66\u4e60\u7ec6\u7c92\u5ea6\u7684\u7a7a\u95f4\u7ed3\u6784\u3002 occupancy \u9884\u6d4b\u88ab\u89c6\u4e3a\u9690\u5f0f\u63a8\u7406\u8fc7\u7a0b\uff0c\u53ef\u4ee5\u5728\u63a8\u7406\u671f\u95f4\u8df3\u8fc7\uff0c\u800c\u4e0d\u4f1a\u964d\u4f4e\u6027\u80fd\uff0c\u4ece\u800c\u4e0d\u4f1a\u589e\u52a0\u989d\u5916\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "result": "OccVLA \u5728 nuScenes \u8f68\u8ff9\u89c4\u5212\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5e76\u5728 3D \u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "OccVLA \u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u4e14\u5b8c\u5168\u57fa\u4e8e\u89c6\u89c9\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.05341", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05341", "abs": "https://arxiv.org/abs/2509.05341", "authors": ["Abhijeet Manoj Pal", "Rajbabu Velmurugan"], "title": "Handling imbalance and few-sample size in ML based Onion disease classification", "comment": "6 pages, 8 figures", "summary": "Accurate classification of pests and diseases plays a vital role in precision\nagriculture, enabling efficient identification, targeted interventions, and\npreventing their further spread. However, current methods primarily focus on\nbinary classification, which limits their practical applications, especially in\nscenarios where accurately identifying the specific type of disease or pest is\nessential. We propose a robust deep learning based model for multi-class\nclassification of onion crop diseases and pests. We enhance a pre-trained\nConvolutional Neural Network (CNN) model by integrating attention based modules\nand employing comprehensive data augmentation pipeline to mitigate class\nimbalance. We propose a model which gives 96.90% overall accuracy and 0.96 F1\nscore on real-world field image dataset. This model gives better results than\nother approaches using the same datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6d0b\u8471\u4f5c\u7269\u75c5\u866b\u5bb3\u591a\u5206\u7c7b\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u4e8e\u4e8c\u5143\u5206\u7c7b\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u9700\u8981\u51c6\u786e\u8bc6\u522b\u7279\u5b9a\u75be\u75c5\u6216\u5bb3\u866b\u7c7b\u578b\u7684\u573a\u666f\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u96c6\u6210\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6a21\u5757\u5e76\u91c7\u7528\u5168\u9762\u7684\u6570\u636e\u589e\u5f3a\u7ba1\u9053\u6765\u7f13\u89e3\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u9884\u8bad\u7ec3\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) \u6a21\u578b\u3002", "result": "\u8be5\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u7530\u95f4\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7ed9\u51fa\u4e86 96.90% \u7684\u603b\u4f53\u51c6\u786e\u7387\u548c 0.96 F1 \u5206\u6570\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u4f7f\u7528\u76f8\u540c\u6570\u636e\u96c6\u7684\u60c5\u51b5\u4e0b\u7ed9\u51fa\u4e86\u6bd4\u5176\u4ed6\u65b9\u6cd5\u66f4\u597d\u7684\u7ed3\u679c\u3002"}}
{"id": "2509.05545", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05545", "abs": "https://arxiv.org/abs/2509.05545", "authors": ["Yang Yu"], "title": "Reinforcement Learning with Anticipation: A Hierarchical Approach for Long-Horizon Tasks", "comment": null, "summary": "Solving long-horizon goal-conditioned tasks remains a significant challenge\nin reinforcement learning (RL). Hierarchical reinforcement learning (HRL)\naddresses this by decomposing tasks into more manageable sub-tasks, but the\nautomatic discovery of the hierarchy and the joint training of multi-level\npolicies often suffer from instability and can lack theoretical guarantees. In\nthis paper, we introduce Reinforcement Learning with Anticipation (RLA), a\nprincipled and potentially scalable framework designed to address these\nlimitations. The RLA agent learns two synergistic models: a low-level,\ngoal-conditioned policy that learns to reach specified subgoals, and a\nhigh-level anticipation model that functions as a planner, proposing\nintermediate subgoals on the optimal path to a final goal. The key feature of\nRLA is the training of the anticipation model, which is guided by a principle\nof value geometric consistency, regularized to prevent degenerate solutions. We\npresent proofs that RLA approaches the globally optimal policy under various\nconditions, establishing a principled and convergent method for hierarchical\nplanning and execution in long-horizon goal-conditioned tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u79f0\u4e3a\u5177\u6709\u9884\u671f\u6027\u7684\u5f3a\u5316\u5b66\u4e60 (RLA)\uff0c\u7528\u4e8e\u89e3\u51b3\u957f\u65f6\u7a0b\u76ee\u6807\u6761\u4ef6\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u957f\u65f6\u7a0b\u76ee\u6807\u6761\u4ef6\u4efb\u52a1\u4ecd\u7136\u662f\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u5206\u5c42\u5f3a\u5316\u5b66\u4e60 (HRL) \u901a\u8fc7\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u66f4\u6613\u4e8e\u7ba1\u7406\u7684\u5b50\u4efb\u52a1\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46\u5c42\u7ea7\u7684\u81ea\u52a8\u53d1\u73b0\u548c\u591a\u5c42\u7b56\u7565\u7684\u8054\u5408\u8bad\u7ec3\u901a\u5e38\u4f1a\u53d7\u5230\u4e0d\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u4e14\u53ef\u80fd\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1\u3002", "method": "RLA \u667a\u80fd\u4f53\u5b66\u4e60\u4e24\u79cd\u534f\u540c\u6a21\u578b\uff1a\u4e00\u79cd\u4f4e\u7ea7\u3001\u76ee\u6807\u6761\u4ef6\u7b56\u7565\uff0c\u5b66\u4e60\u8fbe\u5230\u6307\u5b9a\u5b50\u76ee\u6807\uff1b\u4ee5\u53ca\u4e00\u79cd\u9ad8\u7ea7\u9884\u671f\u6a21\u578b\uff0c\u5176\u529f\u80fd\u7c7b\u4f3c\u4e8e\u89c4\u5212\u5668\uff0c\u5728\u8fbe\u5230\u6700\u7ec8\u76ee\u6807\u7684\u6700\u4f73\u8def\u5f84\u4e0a\u63d0\u51fa\u4e2d\u95f4\u5b50\u76ee\u6807\u3002RLA \u7684\u5173\u952e\u7279\u5f81\u662f\u9884\u671f\u6a21\u578b\u7684\u8bad\u7ec3\uff0c\u8be5\u8bad\u7ec3\u4ee5\u4ef7\u503c\u51e0\u4f55\u4e00\u81f4\u6027\u539f\u5219\u4e3a\u6307\u5bfc\uff0c\u5e76\u8fdb\u884c\u6b63\u5219\u5316\u4ee5\u9632\u6b62\u9000\u5316\u89e3\u3002", "result": "\u6211\u4eec\u63d0\u4f9b\u4e86 RLA \u5728\u5404\u79cd\u6761\u4ef6\u4e0b\u63a5\u8fd1\u5168\u5c40\u6700\u4f18\u7b56\u7565\u7684\u8bc1\u660e\uff0c\u4e3a\u957f\u65f6\u7a0b\u76ee\u6807\u6761\u4ef6\u4efb\u52a1\u4e2d\u7684\u5206\u5c42\u89c4\u5212\u548c\u6267\u884c\u5efa\u7acb\u4e86\u4e00\u79cd\u6709\u539f\u5219\u4e14\u6536\u655b\u7684\u65b9\u6cd5\u3002", "conclusion": "RLA \u662f\u4e00\u79cd\u6709\u539f\u5219\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3 HRL \u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.06552", "categories": ["cs.LG", "cs.CV", "cs.DC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.06552", "abs": "https://arxiv.org/abs/2509.06552", "authors": ["Zheqi Lv", "Wenqiao Zhang", "Kairui Fu", "Qi Tian", "Shengyu Zhang", "Jiajie Su", "Jingyuan Chen", "Kun Kuang", "Fei Wu"], "title": "Tackling Device Data Distribution Real-time Shift via Prototype-based Parameter Editing", "comment": "Published on MM'25: Proceedings of the 33rd ACM International\n  Conference on Multimedia", "summary": "The on-device real-time data distribution shift on devices challenges the\ngeneralization of lightweight on-device models. This critical issue is often\noverlooked in current research, which predominantly relies on data-intensive\nand computationally expensive fine-tuning approaches. To tackle this, we\nintroduce Persona, a novel personalized method using a prototype-based,\nbackpropagation-free parameter editing framework to enhance model\ngeneralization without post-deployment retraining. Persona employs a neural\nadapter in the cloud to generate a parameter editing matrix based on real-time\ndevice data. This matrix adeptly adapts on-device models to the prevailing data\ndistributions, efficiently clustering them into prototype models. The\nprototypes are dynamically refined via the parameter editing matrix,\nfacilitating efficient evolution. Furthermore, the integration of cross-layer\nknowledge transfer ensures consistent and context-aware multi-layer parameter\nchanges and prototype assignment. Extensive experiments on vision task and\nrecommendation task on multiple datasets confirm Persona's effectiveness and\ngenerality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Persona \u7684\u65b0\u9896\u4e2a\u6027\u5316\u65b9\u6cd5\uff0c\u4f7f\u7528\u57fa\u4e8e\u539f\u578b\u7684\u65e0\u53cd\u5411\u4f20\u64ad\u53c2\u6570\u7f16\u8f91\u6846\u67b6\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u800c\u65e0\u9700\u90e8\u7f72\u540e\u91cd\u65b0\u8bad\u7ec3\u3002", "motivation": "\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u6570\u636e\u5206\u5e03\u53d8\u5316\u5bf9\u8bbe\u5907\u4e0a\u8f7b\u91cf\u7ea7\u6a21\u578b\u7684\u6cdb\u5316\u63d0\u51fa\u4e86\u6311\u6218\u3002\u76ee\u524d\u7684\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u4e8e\u6570\u636e\u5bc6\u96c6\u578b\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u800c\u8fd9\u4e2a\u95ee\u9898\u7ecf\u5e38\u88ab\u5ffd\u89c6\u3002", "method": "Persona \u91c7\u7528\u4e91\u4e2d\u7684\u795e\u7ecf\u9002\u914d\u5668\u6765\u751f\u6210\u57fa\u4e8e\u5b9e\u65f6\u8bbe\u5907\u6570\u636e\u7684\u53c2\u6570\u7f16\u8f91\u77e9\u9635\u3002\u8be5\u77e9\u9635\u5de7\u5999\u5730\u4f7f\u8bbe\u5907\u4e0a\u7684\u6a21\u578b\u9002\u5e94\u4e8e prevailing \u6570\u636e\u5206\u5e03\uff0c\u6709\u6548\u5730\u5c06\u5b83\u4eec\u805a\u7c7b\u6210\u539f\u578b\u6a21\u578b\u3002\u539f\u578b\u901a\u8fc7\u53c2\u6570\u7f16\u8f91\u77e9\u9635\u52a8\u6001\u5730\u7ec6\u5316\uff0c\u4ece\u800c\u4fc3\u8fdb\u9ad8\u6548\u7684\u6f14\u5316\u3002\u6b64\u5916\uff0c\u4ea4\u53c9\u5c42\u77e5\u8bc6\u8f6c\u79fb\u7684\u96c6\u6210\u786e\u4fdd\u4e86\u4e00\u81f4\u7684\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u591a\u5c42\u53c2\u6570\u53d8\u5316\u548c\u539f\u578b\u5206\u914d\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u89c6\u89c9\u4efb\u52a1\u548c\u63a8\u8350\u4efb\u52a1\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u5b9e\u4e86 Persona \u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "Persona \u662f\u4e00\u79cd\u6709\u6548\u7684\u4e2a\u6027\u5316\u65b9\u6cd5\uff0c\u53ef\u4ee5\u589e\u5f3a\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u800c\u65e0\u9700\u90e8\u7f72\u540e\u91cd\u65b0\u8bad\u7ec3\u3002"}}
{"id": "2509.05605", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05605", "abs": "https://arxiv.org/abs/2509.05605", "authors": ["Qiyuan Chen", "Hongsen Huang", "Qian Shao", "Jiahe Chen", "Jintai Chen", "Hongxia Xu", "Renjie Hua", "Ren Chuan", "Jian Wu"], "title": "Icon$^{2}$: Aligning Large Language Models Using Self-Synthetic Preference Data via Inherent Regulation", "comment": "EMNLP 2025 Main", "summary": "Large Language Models (LLMs) require high quality preference datasets to\nalign with human preferences. However, conventional methods for constructing\nsuch datasets face significant challenges: reliance on pre-collected\ninstructions often leads to distribution mismatches with target models, while\nthe need for sampling multiple stochastic responses introduces substantial\ncomputational overhead. In this work, we explore a paradigm shift by leveraging\ninherent regulation of LLMs' representation space for efficient and tailored\npreference dataset construction, named Icon$^{2}$. Specifically, it first\nextracts layer-wise direction vectors to encode sophisticated human preferences\nand then uses these vectors to filter self-synthesized instructions based on\ntheir inherent consistency. During decoding, bidirectional inherent control is\napplied to steer token representations, enabling the precise generation of\nresponse pairs with clear alignment distinctions. Experimental results\ndemonstrate significant improvements in both alignment and efficiency.\nLlama3-8B and Qwen2-7B achieve an average win rate improvement of 13.89% on\nAlpacaEval 2.0 and 13.45% on Arena-Hard, while reducing computational costs by\nup to 48.1%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9ad8\u6548\u504f\u597d\u6570\u636e\u96c6\u6784\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5185\u5728\u8c03\u63a7\u80fd\u529b\uff0c\u51cf\u5c11\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u5e76\u63d0\u9ad8\u4e86\u6a21\u578b\u5bf9\u9f50\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u504f\u597d\u6570\u636e\u96c6\u6784\u5efa\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u9884\u5148\u6536\u96c6\u7684\u6307\u4ee4\uff0c\u5bfc\u81f4\u4e0e\u76ee\u6807\u6a21\u578b\u5b58\u5728\u5206\u5e03\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u5e76\u4e14\u9700\u8981\u91c7\u6837\u591a\u4e2a\u968f\u673a\u54cd\u5e94\uff0c\u5f15\u5165\u4e86\u5927\u91cf\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u901a\u8fc7\u63d0\u53d6\u5c42\u7ea7\u7684\u65b9\u5411\u5411\u91cf\u6765\u7f16\u7801\u4eba\u7c7b\u504f\u597d\uff0c\u5e76\u4f7f\u7528\u8fd9\u4e9b\u5411\u91cf\u6765\u8fc7\u6ee4\u81ea\u5408\u6210\u7684\u6307\u4ee4\uff0c\u57fa\u4e8e\u5176\u5185\u5728\u4e00\u81f4\u6027\u3002\u5728\u89e3\u7801\u8fc7\u7a0b\u4e2d\uff0c\u5e94\u7528\u53cc\u5411\u5185\u5728\u63a7\u5236\u6765\u5f15\u5bfctoken\u8868\u793a\uff0c\u4ece\u800c\u7cbe\u786e\u751f\u6210\u5177\u6709\u6e05\u6670\u5bf9\u9f50\u533a\u5206\u7684\u54cd\u5e94\u5bf9\u3002", "result": "\u5728AlpacaEval 2.0\u548cArena-Hard\u4e0a\uff0cLlama3-8B\u548cQwen2-7B\u7684\u5e73\u5747\u80dc\u7387\u5206\u522b\u63d0\u9ad8\u4e8613.89%\u548c13.45%\uff0c\u540c\u65f6\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u4e86\u9ad8\u8fbe48.1%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5bf9\u9f50\u6027\u80fd\u548c\u6548\u7387\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\uff0c\u4e3a\u504f\u597d\u6570\u636e\u96c6\u7684\u6784\u5efa\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2509.05685", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05685", "abs": "https://arxiv.org/abs/2509.05685", "authors": ["Jian Yang", "Jiahui Wu", "Li Fang", "Hongchao Fan", "Bianying Zhang", "Huijie Zhao", "Guangyi Yang", "Rui Xin", "Xiong You"], "title": "MSRFormer: Road Network Representation Learning using Multi-scale Feature Fusion of Heterogeneous Spatial Interactions", "comment": null, "summary": "Transforming road network data into vector representations using deep\nlearning has proven effective for road network analysis. However, urban road\nnetworks' heterogeneous and hierarchical nature poses challenges for accurate\nrepresentation learning. Graph neural networks, which aggregate features from\nneighboring nodes, often struggle due to their homogeneity assumption and focus\non a single structural scale. To address these issues, this paper presents\nMSRFormer, a novel road network representation learning framework that\nintegrates multi-scale spatial interactions by addressing their flow\nheterogeneity and long-distance dependencies. It uses spatial flow convolution\nto extract small-scale features from large trajectory datasets, and identifies\nscale-dependent spatial interaction regions to capture the spatial structure of\nroad networks and flow heterogeneity. By employing a graph transformer,\nMSRFormer effectively captures complex spatial dependencies across multiple\nscales. The spatial interaction features are fused using residual connections,\nwhich are fed to a contrastive learning algorithm to derive the final road\nnetwork representation. Validation on two real-world datasets demonstrates that\nMSRFormer outperforms baseline methods in two road network analysis tasks. The\nperformance gains of MSRFormer suggest the traffic-related task benefits more\nfrom incorporating trajectory data, also resulting in greater improvements in\ncomplex road network structures with up to 16% improvements compared to the\nmost competitive baseline method. This research provides a practical framework\nfor developing task-agnostic road network representation models and highlights\ndistinct association patterns of the interplay between scale effects and flow\nheterogeneity of spatial interactions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9053\u8def\u7f51\u7edc\u8868\u793a\u5b66\u4e60\u6846\u67b6MSRFormer\uff0c\u5b83\u96c6\u6210\u4e86\u591a\u5c3a\u5ea6\u7a7a\u95f4\u4ea4\u4e92\uff0c\u901a\u8fc7\u89e3\u51b3\u5b83\u4eec\u7684\u6d41\u5f02\u8d28\u6027\u548c\u957f\u8ddd\u79bb\u4f9d\u8d56\u6027\u3002", "motivation": "\u57ce\u5e02\u9053\u8def\u7f51\u7edc\u7684\u5f02\u6784\u6027\u548c\u5206\u5c42\u6027\u8d28\u7ed9\u7cbe\u786e\u7684\u8868\u793a\u5b66\u4e60\u5e26\u6765\u4e86\u6311\u6218\u3002\u56fe\u795e\u7ecf\u7f51\u7edc\u7531\u4e8e\u5176\u540c\u8d28\u6027\u5047\u8bbe\u548c\u5bf9\u5355\u4e00\u7ed3\u6784\u5c3a\u5ea6\u7684\u5173\u6ce8\u800c\u5e38\u5e38\u96be\u4ee5\u80dc\u4efb\u3002", "method": "\u4f7f\u7528\u7a7a\u95f4\u6d41\u5377\u79ef\u4ece\u5927\u578b\u8f68\u8ff9\u6570\u636e\u96c6\u4e2d\u63d0\u53d6\u5c0f\u5c3a\u5ea6\u7279\u5f81\uff0c\u5e76\u8bc6\u522b\u5c3a\u5ea6\u76f8\u5173\u7684\u7a7a\u95f4\u4ea4\u4e92\u533a\u57df\u4ee5\u6355\u83b7\u9053\u8def\u7f51\u7edc\u7684\u7a7a\u95f4\u7ed3\u6784\u548c\u6d41\u5f02\u8d28\u6027\u3002\u901a\u8fc7\u4f7f\u7528\u56feTransformer\uff0cMSRFormer\u6709\u6548\u5730\u6355\u83b7\u8de8\u591a\u4e2a\u5c3a\u5ea6\u7684\u590d\u6742\u7a7a\u95f4\u4f9d\u8d56\u6027\u3002\u7a7a\u95f4\u4ea4\u4e92\u7279\u5f81\u901a\u8fc7\u6b8b\u5dee\u8fde\u63a5\u878d\u5408\uff0c\u5e76\u88ab\u9988\u9001\u5230\u5bf9\u6bd4\u5b66\u4e60\u7b97\u6cd5\u4ee5\u5bfc\u51fa\u6700\u7ec8\u7684\u9053\u8def\u7f51\u7edc\u8868\u793a\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u9a8c\u8bc1\u8868\u660e\uff0cMSRFormer\u5728\u4e24\u4e2a\u9053\u8def\u7f51\u7edc\u5206\u6790\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002MSRFormer\u7684\u6027\u80fd\u63d0\u5347\u8868\u660e\uff0c\u4e0e\u6d41\u91cf\u76f8\u5173\u7684\u4efb\u52a1\u66f4\u591a\u5730\u53d7\u76ca\u4e8e\u6574\u5408\u8f68\u8ff9\u6570\u636e\uff0c\u5e76\u4e14\u5728\u590d\u6742\u7684\u9053\u8def\u7f51\u7edc\u7ed3\u6784\u4e2d\u4e5f\u83b7\u5f97\u4e86\u66f4\u5927\u7684\u6539\u8fdb\uff0c\u4e0e\u6700\u5177\u7ade\u4e89\u529b\u7684\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6539\u8fdb\u9ad8\u8fbe16%\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u4e3a\u5f00\u53d1\u4e0e\u4efb\u52a1\u65e0\u5173\u7684\u9053\u8def\u7f51\u7edc\u8868\u793a\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u6846\u67b6\uff0c\u5e76\u5f3a\u8c03\u4e86\u7a7a\u95f4\u4ea4\u4e92\u7684\u5c3a\u5ea6\u6548\u5e94\u548c\u6d41\u91cf\u5f02\u8d28\u6027\u4e4b\u95f4\u76f8\u4e92\u4f5c\u7528\u7684\u72ec\u7279\u5173\u8054\u6a21\u5f0f\u3002"}}
{"id": "2509.05342", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05342", "abs": "https://arxiv.org/abs/2509.05342", "authors": ["Gaspard Beaudouin", "Minghan Li", "Jaeyeon Kim", "Sunghoon Yoon", "Mengyu Wang"], "title": "Delta Velocity Rectified Flow for Text-to-Image Editing", "comment": null, "summary": "We propose Delta Velocity Rectified Flow (DVRF), a novel inversion-free,\npath-aware editing framework within rectified flow models for text-to-image\nediting. DVRF is a distillation-based method that explicitly models the\ndiscrepancy between the source and target velocity fields in order to mitigate\nover-smoothing artifacts rampant in prior distillation sampling approaches. We\nfurther introduce a time-dependent shift term to push noisy latents closer to\nthe target trajectory, enhancing the alignment with the target distribution. We\ntheoretically demonstrate that when this shift is disabled, DVRF reduces to\nDelta Denoising Score, thereby bridging score-based diffusion optimization and\nvelocity-based rectified-flow optimization. Moreover, when the shift term\nfollows a linear schedule under rectified-flow dynamics, DVRF generalizes the\nInversion-free method FlowEdit and provides a principled theoretical\ninterpretation for it. Experimental results indicate that DVRF achieves\nsuperior editing quality, fidelity, and controllability while requiring no\narchitectural modifications, making it efficient and broadly applicable to\ntext-to-image editing tasks. Code is available at\nhttps://github.com/gaspardbd/DeltaVelocityRectifiedFlow.", "AI": {"tldr": "\u63d0\u51fa\u4e86 Delta Velocity Rectified Flow (DVRF)\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u65e0\u53cd\u6f14\u3001\u8def\u5f84\u611f\u77e5\u7684\u7f16\u8f91\u6846\u67b6\uff0c\u7528\u4e8e\u6587\u672c\u5230\u56fe\u50cf\u7f16\u8f91\u4e2d\u7684\u6821\u6b63\u6d41\u6a21\u578b\u3002", "motivation": "\u4e3a\u4e86\u51cf\u8f7b\u5148\u524d\u84b8\u998f\u91c7\u6837\u65b9\u6cd5\u4e2d\u666e\u904d\u5b58\u5728\u7684\u8fc7\u5ea6\u5e73\u6ed1\u4f2a\u5f71\uff0c\u663e\u5f0f\u5730\u5bf9\u6e90\u901f\u5ea6\u573a\u548c\u76ee\u6807\u901f\u5ea6\u573a\u4e4b\u95f4\u7684\u5dee\u5f02\u8fdb\u884c\u5efa\u6a21\u3002", "method": "DVRF \u662f\u4e00\u79cd\u57fa\u4e8e\u84b8\u998f\u7684\u65b9\u6cd5\uff0c\u5b83\u663e\u5f0f\u5730\u5bf9\u6e90\u901f\u5ea6\u573a\u548c\u76ee\u6807\u901f\u5ea6\u573a\u4e4b\u95f4\u7684\u5dee\u5f02\u8fdb\u884c\u5efa\u6a21\u3002\u5f15\u5165\u4e86\u4e00\u4e2a\u65f6\u95f4\u76f8\u5173\u7684\u5e73\u79fb\u9879\uff0c\u4ee5\u63a8\u52a8\u566a\u58f0\u6f5c\u5728\u53d8\u91cf\u66f4\u63a5\u8fd1\u76ee\u6807\u8f68\u8ff9\uff0c\u4ece\u800c\u589e\u5f3a\u4e0e\u76ee\u6807\u5206\u5e03\u7684\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDVRF \u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u7f16\u8f91\u8d28\u91cf\u3001\u4fdd\u771f\u5ea6\u548c\u53ef\u63a7\u6027\uff0c\u540c\u65f6\u4e0d\u9700\u8981\u4efb\u4f55\u67b6\u6784\u4fee\u6539\uff0c\u4f7f\u5176\u9ad8\u6548\u4e14\u5e7f\u6cdb\u9002\u7528\u4e8e\u6587\u672c\u5230\u56fe\u50cf\u7684\u7f16\u8f91\u4efb\u52a1\u3002", "conclusion": "\u5f53\u7981\u7528\u6b64\u5e73\u79fb\u65f6\uff0cDVRF \u7b80\u5316\u4e3a Delta Denoising Score\uff0c\u4ece\u800c\u6865\u63a5\u4e86\u57fa\u4e8e\u5206\u6570\u7684\u6269\u6563\u4f18\u5316\u548c\u57fa\u4e8e\u901f\u5ea6\u7684\u6821\u6b63\u6d41\u4f18\u5316\u3002\u6b64\u5916\uff0c\u5f53\u5e73\u79fb\u9879\u9075\u5faa\u6821\u6b63\u6d41\u52a8\u529b\u5b66\u4e0b\u7684\u7ebf\u6027\u65f6\u95f4\u8868\u65f6\uff0cDVRF \u6982\u62ec\u4e86\u65e0\u53cd\u6f14\u65b9\u6cd5 FlowEdit\uff0c\u5e76\u4e3a\u5176\u63d0\u4f9b\u4e86\u6709\u539f\u5219\u7684\u7406\u8bba\u89e3\u91ca\u3002"}}
{"id": "2509.05584", "categories": ["cs.LG", "cs.CV", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.05584", "abs": "https://arxiv.org/abs/2509.05584", "authors": ["Sadegh Jafari", "Aishwarya Sarkar", "Mohiuddin Bilwal", "Ali Jannesari"], "title": "ProfilingAgent: Profiling-Guided Agentic Reasoning for Adaptive Model Optimization", "comment": "13 pages, 3 figures, 5 tables, 1 algorithm", "summary": "Foundation models face growing compute and memory bottlenecks, hindering\ndeployment on resource-limited platforms. While compression techniques such as\npruning and quantization are widely used, most rely on uniform heuristics that\nignore architectural and runtime heterogeneity. Profiling tools expose\nper-layer latency, memory, and compute cost, yet are rarely integrated into\nautomated pipelines. We propose ProfilingAgent, a profiling-guided, agentic\napproach that uses large language models (LLMs) to automate compression via\nstructured pruning and post-training dynamic quantization. Our modular\nmulti-agent system reasons over static metrics (MACs, parameter counts) and\ndynamic signals (latency, memory) to design architecture-specific strategies.\nUnlike heuristic baselines, ProfilingAgent tailors layer-wise decisions to\nbottlenecks. Experiments on ImageNet-1K, CIFAR-10, and CIFAR-100 with\nResNet-101, ViT-B/16, Swin-B, and DeiT-B/16 show pruning maintains competitive\nor improved accuracy (about 1% drop on ImageNet-1K, +2% gains for ViT-B/16 on\nsmaller datasets), while quantization achieves up to 74% memory savings with\n<0.5% accuracy loss. Our quantization also yields consistent inference speedups\nof up to 1.74 times faster. Comparative studies with GPT-4o and GPT-4-Turbo\nhighlight the importance of LLM reasoning quality for iterative pruning. These\nresults establish agentic systems as scalable solutions for profiling-guided\nmodel optimization.", "AI": {"tldr": "ProfilingAgent\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6765\u81ea\u52a8\u5316\u538b\u7f29\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u526a\u679d\u548c\u540e\u8bad\u7ec3\u52a8\u6001\u91cf\u5316\uff0c\u9488\u5bf9\u7279\u5b9a\u67b6\u6784\u8bbe\u8ba1\u7b56\u7565\uff0c\u5b9e\u73b0\u6a21\u578b\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u538b\u7f29\u6280\u672f\u5ffd\u7565\u4e86\u67b6\u6784\u548c\u8fd0\u884c\u65f6\u5f02\u6784\u6027\uff0c\u4e14\u5f88\u5c11\u6709\u5de5\u5177\u5c06\u6027\u80fd\u5206\u6790\u6574\u5408\u5230\u81ea\u52a8\u5316\u6d41\u7a0b\u4e2d\uff0c\u5bfc\u81f4\u57fa\u7840\u6a21\u578b\u9762\u4e34\u65e5\u76ca\u589e\u957f\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u74f6\u9888\uff0c\u9650\u5236\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u7684\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aProfilingAgent\u7684\u6027\u80fd\u5206\u6790\u5f15\u5bfc\u7684agent\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u901a\u8fc7\u7ed3\u6784\u5316\u526a\u679d\u548c\u540e\u8bad\u7ec3\u52a8\u6001\u91cf\u5316\u6765\u81ea\u52a8\u538b\u7f29\u3002\u8be5\u6a21\u5757\u5316\u591aagent\u7cfb\u7edf\u57fa\u4e8e\u9759\u6001\u6307\u6807\uff08MACs\uff0c\u53c2\u6570\u8ba1\u6570\uff09\u548c\u52a8\u6001\u4fe1\u53f7\uff08\u5ef6\u8fdf\uff0c\u5185\u5b58\uff09\u8fdb\u884c\u63a8\u7406\uff0c\u4ee5\u8bbe\u8ba1\u7279\u5b9a\u4e8e\u67b6\u6784\u7684\u7b56\u7565\u3002", "result": "\u5728ImageNet-1K\u3001CIFAR-10\u548cCIFAR-100\u4e0a\uff0c\u4f7f\u7528ResNet-101\u3001ViT-B/16\u3001Swin-B\u548cDeiT-B/16\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u526a\u679d\u4fdd\u6301\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6216\u63d0\u9ad8\u7684\u51c6\u786e\u6027\uff08ImageNet-1K\u4e0a\u7ea61%\u7684\u4e0b\u964d\uff0c\u8f83\u5c0f\u6570\u636e\u96c6\u4e0aViT-B/16\u7684+2%\u7684\u6536\u76ca\uff09\uff0c\u800c\u91cf\u5316\u5b9e\u73b0\u4e86\u9ad8\u8fbe74%\u7684\u5185\u5b58\u8282\u7701\uff0c\u4e14\u51c6\u786e\u6027\u635f\u5931\u5c0f\u4e8e0.5%\u3002\u91cf\u5316\u8fd8\u4ea7\u751f\u4e86\u4e00\u81f4\u7684\u63a8\u7406\u52a0\u901f\uff0c\u9ad8\u8fbe1.74\u500d\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0cagent\u7cfb\u7edf\u662f\u6027\u80fd\u5206\u6790\u5f15\u5bfc\u6a21\u578b\u4f18\u5316\u7684\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.06650", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.06650", "abs": "https://arxiv.org/abs/2509.06650", "authors": ["Hao Lin", "Peitong Xie", "Jingxue Chen", "Jie Lin", "Qingkun Tang", "Qianchun Lu"], "title": "Domain-Aware RAG: MoL-Enhanced RL for Efficient Training and Scalable Retrieval", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems rely heavily on the retrieval\nstage, particularly the coarse-ranking process. Existing coarse-ranking\noptimization approaches often struggle to balance domain-specific knowledge\nlearning with query enhencement, resulting in suboptimal retrieval performance.\nTo address this challenge, we propose MoLER, a domain-aware RAG method that\nuses MoL-Enhanced Reinforcement Learning to optimize retrieval. MoLER has a\ntwo-stage pipeline: a continual pre-training (CPT) phase using a Mixture of\nLosses (MoL) to balance domain-specific knowledge with general language\ncapabilities, and a reinforcement learning (RL) phase leveraging Group Relative\nPolicy Optimization (GRPO) to optimize query and passage generation for\nmaximizing document recall. A key innovation is our Multi-query Single-passage\nLate Fusion (MSLF) strategy, which reduces computational overhead during RL\ntraining while maintaining scalable inference via Multi-query Multi-passage\nLate Fusion (MMLF). Extensive experiments on benchmark datasets show that MoLER\nachieves state-of-the-art performance, significantly outperforming baseline\nmethods. MoLER bridges the knowledge gap in RAG systems, enabling robust and\nscalable retrieval in specialized domains.", "AI": {"tldr": "MoLER\u662f\u4e00\u79cd\u9886\u57df\u611f\u77e5\u7684RAG\u65b9\u6cd5\uff0c\u5b83\u4f7f\u7528MoL\u589e\u5f3a\u7684\u5f3a\u5316\u5b66\u4e60\u6765\u4f18\u5316\u68c0\u7d22\u3002", "motivation": "\u73b0\u6709\u7684\u7c97\u6392\u5e8f\u4f18\u5316\u65b9\u6cd5\u901a\u5e38\u96be\u4ee5\u5e73\u8861\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u5b66\u4e60\u548c\u67e5\u8be2\u589e\u5f3a\uff0c\u5bfc\u81f4\u68c0\u7d22\u6027\u80fd\u6b20\u4f73\u3002", "method": "MoLER\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a\u4f7f\u7528\u6df7\u5408\u635f\u5931\u7684\u6301\u7eed\u9884\u8bad\u7ec3\u9636\u6bb5\uff0c\u4ee5\u53ca\u5229\u7528\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u7684\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\uff0c\u4ee5\u4f18\u5316\u67e5\u8be2\u548c\u6bb5\u843d\u751f\u6210\uff0c\u4ece\u800c\u6700\u5927\u9650\u5ea6\u5730\u63d0\u9ad8\u6587\u6863\u53ec\u56de\u7387\u3002\u5173\u952e\u521b\u65b0\u662f\u591a\u67e5\u8be2\u5355\u901a\u9053\u540e\u671f\u878d\u5408\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u51cf\u5c11\u4e86RL\u8bad\u7ec3\u671f\u95f4\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u901a\u8fc7\u591a\u67e5\u8be2\u591a\u901a\u9053\u540e\u671f\u878d\u5408\u4fdd\u6301\u4e86\u53ef\u6269\u5c55\u7684\u63a8\u7406\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMoLER\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MoLER\u5f25\u5408\u4e86RAG\u7cfb\u7edf\u4e2d\u7684\u77e5\u8bc6\u5dee\u8ddd\uff0c\u4ece\u800c\u53ef\u4ee5\u5728\u4e13\u4e1a\u9886\u57df\u4e2d\u5b9e\u73b0\u5f3a\u5927\u4e14\u53ef\u6269\u5c55\u7684\u68c0\u7d22\u3002"}}
{"id": "2509.05607", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.05607", "abs": "https://arxiv.org/abs/2509.05607", "authors": ["Qiyuan Chen", "Jiahe Chen", "Hongsen Huang", "Qian Shao", "Jintai Chen", "Renjie Hua", "Hongxia Xu", "Ruijia Wu", "Ren Chuan", "Jian Wu"], "title": "Beyond Keywords: Driving Generative Search Engine Optimization with Content-Centric Agents", "comment": "Technical Report", "summary": "The paradigm shift from traditional ranked-based search to Generative Search\nEngines has rendered conventional SEO metrics obsolete, creating an urgent need\nto understand, measure, and optimize for content influence on synthesized\nanswers. This paper introduces a comprehensive, end-to-end framework for\nGenerative Search Engine Optimization (GSEO) to address this challenge. We make\ntwo primary contributions. First, we construct CC-GSEO-Bench, a large-scale,\ncontent-centric benchmark, and propose a multi-dimensional evaluation framework\nthat systematically quantifies influence, moving beyond surface-level\nattribution to assess substantive semantic impact. Second, we design a novel\nmulti-agent system that operationalizes this framework, automating the\nstrategic refinement of content through a collaborative analyze-revise-evaluate\nworkflow. Our empirical analysis using this framework reveals novel insights\ninto the dynamics of content influence, offering actionable strategies for\ncreators and establishing a principled foundation for future GSEO research.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u751f\u6210\u5f0f\u641c\u7d22\u5f15\u64ce\u4f18\u5316\uff08GSEO\uff09\u7684\u7efc\u5408\u6846\u67b6\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u6392\u540d\u7684\u641c\u7d22\u5230\u751f\u6210\u5f0f\u641c\u7d22\u5f15\u64ce\u7684\u8303\u5f0f\u8f6c\u53d8\u4f7f\u5f97\u4f20\u7edf\u7684SEO\u6307\u6807\u8fc7\u65f6\uff0c\u8feb\u5207\u9700\u8981\u7406\u89e3\u3001\u6d4b\u91cf\u548c\u4f18\u5316\u5185\u5bb9\u5bf9\u5408\u6210\u7b54\u6848\u7684\u5f71\u54cd\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u3001\u4ee5\u5185\u5bb9\u4e3a\u4e2d\u5fc3\u7684\u57fa\u51c6CC-GSEO-Bench\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u7ef4\u8bc4\u4f30\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7cfb\u7edf\u5730\u91cf\u5316\u5f71\u54cd\uff0c\u8d85\u8d8a\u4e86\u8868\u9762\u5c42\u6b21\u7684\u5f52\u56e0\uff0c\u4ee5\u8bc4\u4f30\u5b9e\u8d28\u6027\u7684\u8bed\u4e49\u5f71\u54cd\u3002\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u901a\u8fc7\u534f\u4f5c\u5206\u6790-\u4fee\u6539-\u8bc4\u4f30\u5de5\u4f5c\u6d41\u7a0b\u6765\u81ea\u52a8\u5316\u5185\u5bb9\u7684\u6218\u7565\u6539\u8fdb\u3002", "result": "\u901a\u8fc7\u4f7f\u7528\u8be5\u6846\u67b6\u7684\u5b9e\u8bc1\u5206\u6790\uff0c\u63ed\u793a\u4e86\u5bf9\u5185\u5bb9\u5f71\u54cd\u52a8\u6001\u7684\u65b0\u9896\u89c1\u89e3\uff0c\u4e3a\u521b\u4f5c\u8005\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u7b56\u7565\uff0c\u5e76\u4e3a\u672a\u6765\u7684GSEO\u7814\u7a76\u5efa\u7acb\u4e86\u539f\u5219\u6027\u57fa\u7840\u3002", "conclusion": "\u4e3a\u751f\u6210\u5f0f\u641c\u7d22\u5f15\u64ce\u4f18\u5316 (GSEO) \u5efa\u7acb\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u7aef\u5230\u7aef\u6846\u67b6"}}
{"id": "2509.05714", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05714", "abs": "https://arxiv.org/abs/2509.05714", "authors": ["Zhaoyu Fan", "Kaihang Pan", "Mingze Zhou", "Bosheng Qin", "Juncheng Li", "Shengyu Zhang", "Wenqiao Zhang", "Siliang Tang", "Fei Wu", "Yueting Zhuang"], "title": "Towards Meta-Cognitive Knowledge Editing for Multimodal LLMs", "comment": "15 pages, 6 figures", "summary": "Knowledge editing enables multimodal large language models (MLLMs) to\nefficiently update outdated or incorrect information. However, existing\nbenchmarks primarily emphasize cognitive-level modifications while lacking a\nfocus on deeper meta-cognitive processes. To bridge this gap, we introduce\nCogEdit, a novel benchmark designed to evaluate MLLMs' meta-cognitive knowledge\nediting abilities across three levels: (1) Counterfactual-Driven Editing,\nassessing self-awareness of knowledge correctness changes; (2) Boundary\nConstraint Editing, ensuring appropriate generalization without unintended\ninterference; and (3) Noise-Robust Editing, promoting reflective evaluation of\nuncertain information. To advance meta-cognitive editing, we propose MIND\n(Meta-cognitive INtegrated Dynamic Knowledge Editing), a framework that\nconstructs a meta-knowledge memory for self-awareness, employs game-theoretic\ninteractions to monitor knowledge activation, and incorporates label refinement\nfor noise-robust updates. Extensive experiments show that MIND significantly\noutperforms existing cognitive editing approaches, achieving strong performance\non both traditional and meta-cognitive knowledge editing benchmarks.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5CogEdit\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u5143\u8ba4\u77e5\u77e5\u8bc6\u7f16\u8f91\u80fd\u529b\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aMIND\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u6784\u5efa\u5143\u77e5\u8bc6\u8bb0\u5fc6\u3001\u91c7\u7528\u535a\u5f08\u8bba\u4ea4\u4e92\u548c\u7ed3\u5408\u6807\u7b7e\u7ec6\u5316\u6765\u5b9e\u73b0\u5143\u8ba4\u77e5\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5f3a\u8c03\u8ba4\u77e5\u5c42\u9762\u7684\u4fee\u6539\uff0c\u7f3a\u4e4f\u5bf9\u66f4\u6df1\u5c42\u6b21\u7684\u5143\u8ba4\u77e5\u8fc7\u7a0b\u7684\u5173\u6ce8\u3002", "method": "\u63d0\u51fa\u4e86MIND\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6784\u5efa\u4e86\u4e00\u4e2a\u7528\u4e8e\u81ea\u6211\u610f\u8bc6\u7684\u5143\u77e5\u8bc6\u8bb0\u5fc6\uff0c\u91c7\u7528\u535a\u5f08\u8bba\u4ea4\u4e92\u6765\u76d1\u63a7\u77e5\u8bc6\u6fc0\u6d3b\uff0c\u5e76\u7ed3\u5408\u6807\u7b7e\u7ec6\u5316\u6765\u8fdb\u884c\u566a\u58f0\u9c81\u68d2\u66f4\u65b0\u3002", "result": "MIND\u5728\u4f20\u7edf\u548c\u5143\u8ba4\u77e5\u77e5\u8bc6\u7f16\u8f91\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u8ba4\u77e5\u7f16\u8f91\u65b9\u6cd5\u3002", "conclusion": "MIND\u6846\u67b6\u80fd\u591f\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u7f16\u8f91\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5143\u8ba4\u77e5\u5c42\u9762\u3002"}}
{"id": "2509.05343", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05343", "abs": "https://arxiv.org/abs/2509.05343", "authors": ["Zahid Ullah", "Minki Hong", "Tahir Mahmood", "Jihie Kim"], "title": "Systematic Integration of Attention Modules into CNNs for Accurate and Generalizable Medical Image Diagnosis", "comment": null, "summary": "Deep learning has become a powerful tool for medical image analysis; however,\nconventional Convolutional Neural Networks (CNNs) often fail to capture the\nfine-grained and complex features critical for accurate diagnosis. To address\nthis limitation, we systematically integrate attention mechanisms into five\nwidely adopted CNN architectures, namely, VGG16, ResNet18, InceptionV3,\nDenseNet121, and EfficientNetB5, to enhance their ability to focus on salient\nregions and improve discriminative performance. Specifically, each baseline\nmodel is augmented with either a Squeeze and Excitation block or a hybrid\nConvolutional Block Attention Module, allowing adaptive recalibration of\nchannel and spatial feature representations. The proposed models are evaluated\non two distinct medical imaging datasets, a brain tumor MRI dataset comprising\nmultiple tumor subtypes, and a Products of Conception histopathological dataset\ncontaining four tissue categories. Experimental results demonstrate that\nattention augmented CNNs consistently outperform baseline architectures across\nall metrics. In particular, EfficientNetB5 with hybrid attention achieves the\nhighest overall performance, delivering substantial gains on both datasets.\nBeyond improved classification accuracy, attention mechanisms enhance feature\nlocalization, leading to better generalization across heterogeneous imaging\nmodalities. This work contributes a systematic comparative framework for\nembedding attention modules in diverse CNN architectures and rigorously\nassesses their impact across multiple medical imaging tasks. The findings\nprovide practical insights for the development of robust, interpretable, and\nclinically applicable deep learning based decision support systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u6ce8\u610f\u529b\u673a\u5236\u6574\u5408\u5230\u4e94\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684CNN\u67b6\u6784\u4e2d\uff0c\u4ee5\u63d0\u9ad8\u533b\u5b66\u56fe\u50cf\u5206\u6790\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u7684CNN\u96be\u4ee5\u6355\u6349\u533b\u5b66\u56fe\u50cf\u4e2d\u7ec6\u7c92\u5ea6\u548c\u590d\u6742\u7684\u7279\u5f81\uff0c\u5f71\u54cd\u8bca\u65ad\u51c6\u786e\u6027\u3002", "method": "\u5c06Squeeze and Excitation\u6a21\u5757\u6216\u6df7\u5408\u5377\u79ef\u5757\u6ce8\u610f\u529b\u6a21\u5757\u6dfb\u52a0\u5230VGG16\u3001ResNet18\u3001InceptionV3\u3001DenseNet121\u548cEfficientNetB5\u7b49\u6a21\u578b\u4e2d\u3002", "result": "\u5728\u8111\u80bf\u7624MRI\u6570\u636e\u96c6\u548c\u598a\u5a20\u7ec4\u7ec7\u75c5\u7406\u5b66\u6570\u636e\u96c6\u4e0a\uff0c\u6ce8\u610f\u529b\u589e\u5f3a\u7684CNN\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u67b6\u6784\u3002EfficientNetB5\u4e0e\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u6574\u4f53\u6027\u80fd\u3002", "conclusion": "\u6ce8\u610f\u529b\u673a\u5236\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u5206\u7c7b\u7cbe\u5ea6\uff0c\u8fd8\u589e\u5f3a\u4e86\u7279\u5f81\u5b9a\u4f4d\uff0c\u4ece\u800c\u5728\u5f02\u6784\u6210\u50cf\u6a21\u5f0f\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6cdb\u5316\u3002"}}
{"id": "2509.05615", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05615", "abs": "https://arxiv.org/abs/2509.05615", "authors": ["Xiaoguang Zhu", "Lianlong Sun", "Yang Liu", "Pengyi Jiang", "Uma Srivatsa", "Nipavan Chiamvimonvat", "Vladimir Filkov"], "title": "Causal Debiasing Medical Multimodal Representation Learning with Missing Modalities", "comment": "Submitted to IEEE TKDE", "summary": "Medical multimodal representation learning aims to integrate heterogeneous\nclinical data into unified patient representations to support predictive\nmodeling, which remains an essential yet challenging task in the medical data\nmining community. However, real-world medical datasets often suffer from\nmissing modalities due to cost, protocol, or patient-specific constraints.\nExisting methods primarily address this issue by learning from the available\nobservations in either the raw data space or feature space, but typically\nneglect the underlying bias introduced by the data acquisition process itself.\nIn this work, we identify two types of biases that hinder model generalization:\nmissingness bias, which results from non-random patterns in modality\navailability, and distribution bias, which arises from latent confounders that\ninfluence both observed features and outcomes. To address these challenges, we\nperform a structural causal analysis of the data-generating process and propose\na unified framework that is compatible with existing direct prediction-based\nmultimodal learning methods. Our method consists of two key components: (1) a\nmissingness deconfounding module that approximates causal intervention based on\nbackdoor adjustment and (2) a dual-branch neural network that explicitly\ndisentangles causal features from spurious correlations. We evaluated our\nmethod in real-world public and in-hospital datasets, demonstrating its\neffectiveness and causal insights.", "AI": {"tldr": "\u533b\u5b66\u591a\u6a21\u6001\u8868\u5f81\u5b66\u4e60\u65e8\u5728\u6574\u5408\u5f02\u6784\u4e34\u5e8a\u6570\u636e\u4ee5\u652f\u6301\u9884\u6d4b\u5efa\u6a21\uff0c\u4f46\u73b0\u5b9e\u4e16\u754c\u7684\u533b\u7597\u6570\u636e\u96c6\u7ecf\u5e38\u56e0\u6210\u672c\u3001\u534f\u8bae\u6216\u60a3\u8005\u7279\u5b9a\u7ea6\u675f\u800c\u5b58\u5728\u6a21\u6001\u7f3a\u5931\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u7531\u4e8e\u6570\u636e\u91c7\u96c6\u8fc7\u7a0b\u672c\u8eab\u5f15\u5165\u7684\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u4e3b\u8981\u901a\u8fc7\u4ece\u539f\u59cb\u6570\u636e\u7a7a\u95f4\u6216\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u53ef\u7528\u89c2\u5bdf\u4e2d\u5b66\u4e60\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46\u901a\u5e38\u5ffd\u7565\u4e86\u6570\u636e\u91c7\u96c6\u8fc7\u7a0b\u672c\u8eab\u5f15\u5165\u7684\u6f5c\u5728\u504f\u5dee\u3002\u672c\u6587\u8bc6\u522b\u4e86\u4e24\u79cd\u963b\u788d\u6a21\u578b\u6cdb\u5316\u7684\u504f\u5dee\uff1a\u7f3a\u5931\u504f\u5dee\u548c\u5206\u5e03\u504f\u5dee\u3002", "method": "\u672c\u6587\u5bf9\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u8fdb\u884c\u4e86\u7ed3\u6784\u56e0\u679c\u5206\u6790\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4e0e\u73b0\u6709\u7684\u57fa\u4e8e\u76f4\u63a5\u9884\u6d4b\u7684\u591a\u6a21\u6001\u5b66\u4e60\u65b9\u6cd5\u517c\u5bb9\u3002\u8be5\u65b9\u6cd5\u5305\u62ec\u4e24\u4e2a\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff1a(1)\u4e00\u4e2a\u57fa\u4e8e\u540e\u95e8\u8c03\u6574\u7684\u8fd1\u4f3c\u56e0\u679c\u5e72\u9884\u7684\u7f3a\u5931\u53bb\u6df7\u6dc6\u6a21\u5757\uff1b(2)\u4e00\u4e2a\u53cc\u5206\u652f\u795e\u7ecf\u7f51\u7edc\uff0c\u5b83\u660e\u786e\u5730\u5c06\u56e0\u679c\u7279\u5f81\u4e0e\u865a\u5047\u76f8\u5173\u6027\u5206\u79bb\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7684\u516c\u5171\u548c\u9662\u5185\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u56e0\u679c\u6d1e\u5bdf\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u533b\u5b66\u591a\u6a21\u6001\u8868\u5f81\u5b66\u4e60\u4e2d\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u7531\u4e8e\u6570\u636e\u7f3a\u5931\u548c\u504f\u5dee\u5bfc\u81f4\u7684\u6a21\u578b\u6cdb\u5316\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u8df5\u4e2d\u53d6\u5f97\u4e86\u826f\u597d\u7684\u6548\u679c\u3002"}}
{"id": "2509.06883", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.06883", "abs": "https://arxiv.org/abs/2509.06883", "authors": ["Joe Wilder", "Nikhil Kadapala", "Benji Xu", "Mohammed Alsaadi", "Aiden Parsons", "Mitchell Rogers", "Palash Agarwal", "Adam Hassick", "Laura Dietz"], "title": "UNH at CheckThat! 2025: Fine-tuning Vs Prompting in Claim Extraction", "comment": "16 pages,3 tables, CLEF 2025 Working Notes, 9-12 September 2025,\n  Madrid, Spain", "summary": "We participate in CheckThat! Task 2 English and explore various methods of\nprompting and in-context learning, including few-shot prompting and fine-tuning\nwith different LLM families, with the goal of extracting check-worthy claims\nfrom social media passages. Our best METEOR score is achieved by fine-tuning a\nFLAN-T5 model. However, we observe that higher-quality claims can sometimes be\nextracted using other methods, even when their METEOR scores are lower.", "AI": {"tldr": "\u672c\u7814\u7a76\u53c2\u4e0e\u4e86CheckThat! Task 2\u82f1\u8bed\u4efb\u52a1\uff0c\u65e8\u5728\u4ece\u793e\u4ea4\u5a92\u4f53\u6587\u672c\u4e2d\u63d0\u53d6\u503c\u5f97\u6838\u5b9e\u7684\u58f0\u660e\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u4e0d\u540c\u7684\u63d0\u793a\u65b9\u6cd5\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u6280\u672f\uff0c\u4ee5\u63d0\u9ad8\u58f0\u660e\u63d0\u53d6\u7684\u51c6\u786e\u6027\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u5305\u62ec\u5c11\u6837\u672c\u63d0\u793a\u548c\u4f7f\u7528\u4e0d\u540cLLM\u5bb6\u65cf\u8fdb\u884c\u5fae\u8c03\u7b49\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u7684\u6700\u4f73METEOR\u5f97\u5206\u662f\u901a\u8fc7\u5fae\u8c03FLAN-T5\u6a21\u578b\u5b9e\u73b0\u7684\u3002\u7136\u800c\uff0c\u5176\u4ed6\u65b9\u6cd5\u6709\u65f6\u53ef\u4ee5\u63d0\u53d6\u66f4\u9ad8\u8d28\u91cf\u7684\u58f0\u660e\uff0c\u5373\u4f7f\u5b83\u4eec\u7684METEOR\u5f97\u5206\u8f83\u4f4e\u3002", "conclusion": "\u7ed3\u8bba\u662f\u5fae\u8c03FLAN-T5\u6a21\u578b\u5728METEOR\u6307\u6807\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u5176\u4ed6\u65b9\u6cd5\u5728\u63d0\u53d6\u9ad8\u8d28\u91cf\u58f0\u660e\u65b9\u9762\u53ef\u80fd\u66f4\u6709\u6548\u3002"}}
{"id": "2509.05609", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05609", "abs": "https://arxiv.org/abs/2509.05609", "authors": ["Xugang Lu", "Peng Shen", "Yu Tsao", "Hisashi Kawai"], "title": "New Insights into Optimal Alignment of Acoustic and Linguistic Representations for Knowledge Transfer in ASR", "comment": null, "summary": "Aligning acoustic and linguistic representations is a central challenge to\nbridge the pre-trained models in knowledge transfer for automatic speech\nrecognition (ASR). This alignment is inherently structured and asymmetric:\nwhile multiple consecutive acoustic frames typically correspond to a single\nlinguistic token (many-to-one), certain acoustic transition regions may relate\nto multiple adjacent tokens (one-to-many). Moreover, acoustic sequences often\ninclude frames with no linguistic counterpart, such as background noise or\nsilence may lead to imbalanced matching conditions. In this work, we take a new\ninsight to regard alignment and matching as a detection problem, where the goal\nis to identify meaningful correspondences with high precision and recall\nensuring full coverage of linguistic tokens while flexibly handling redundant\nor noisy acoustic frames in transferring linguistic knowledge for ASR. Based on\nthis new insight, we propose an unbalanced optimal transport-based alignment\nmodel that explicitly handles distributional mismatch and structural\nasymmetries with soft and partial matching between acoustic and linguistic\nmodalities. Our method ensures that every linguistic token is grounded in at\nleast one acoustic observation, while allowing for flexible, probabilistic\nmappings from acoustic to linguistic units. We evaluate our proposed model with\nexperiments on an CTC-based ASR system with a pre-trained language model for\nknowledge transfer. Experimental results demonstrate the effectiveness of our\napproach in flexibly controlling degree of matching and hence to improve ASR\nperformance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89d2\uff0c\u5c06\u58f0\u5b66\u548c\u8bed\u8a00\u8868\u793a\u7684\u5bf9\u9f50\u548c\u5339\u914d\u89c6\u4e3a\u4e00\u4e2a\u68c0\u6d4b\u95ee\u9898\uff0c\u76ee\u6807\u662f\u4ee5\u9ad8\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u8bc6\u522b\u6709\u610f\u4e49\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u786e\u4fdd\u8bed\u8a00\u6807\u8bb0\u7684\u5b8c\u5168\u8986\u76d6\uff0c\u540c\u65f6\u7075\u6d3b\u5904\u7406\u5197\u4f59\u6216\u5608\u6742\u7684\u58f0\u5b66\u5e27\uff0c\u4ee5\u4fbf\u4e3a\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u8f6c\u79fb\u8bed\u8a00\u77e5\u8bc6\u3002", "motivation": "\u5f25\u5408\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u4e2d\u77e5\u8bc6\u8f6c\u79fb\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u58f0\u5b66\u548c\u8bed\u8a00\u8868\u793a\u7684\u5bf9\u9f50\u662f\u4e00\u4e2a\u6838\u5fc3\u6311\u6218\u3002\u8fd9\u79cd\u5bf9\u9f50\u672c\u8d28\u4e0a\u662f\u7ed3\u6784\u5316\u7684\u548c\u975e\u5bf9\u79f0\u7684\uff1a\u867d\u7136\u591a\u4e2a\u8fde\u7eed\u7684\u58f0\u5b66\u5e27\u901a\u5e38\u5bf9\u5e94\u4e8e\u5355\u4e2a\u8bed\u8a00\u6807\u8bb0\uff08\u591a\u5bf9\u4e00\uff09\uff0c\u4f46\u67d0\u4e9b\u58f0\u5b66\u8fc7\u6e21\u533a\u57df\u53ef\u80fd\u4e0e\u591a\u4e2a\u76f8\u90bb\u6807\u8bb0\u76f8\u5173\uff08\u4e00\u5bf9\u591a\uff09\u3002\u6b64\u5916\uff0c\u58f0\u5b66\u5e8f\u5217\u901a\u5e38\u5305\u62ec\u6ca1\u6709\u8bed\u8a00\u5bf9\u5e94\u90e8\u5206\u7684\u5e27\uff0c\u4f8b\u5982\u80cc\u666f\u566a\u58f0\u6216\u9759\u97f3\u53ef\u80fd\u5bfc\u81f4\u4e0d\u5e73\u8861\u7684\u5339\u914d\u6761\u4ef6\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0d\u5e73\u8861\u6700\u4f18\u4f20\u8f93\u7684\u5bf9\u9f50\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u58f0\u5b66\u548c\u8bed\u8a00\u6a21\u6001\u4e4b\u95f4\u7684\u8f6f\u5339\u914d\u548c\u90e8\u5206\u5339\u914d\uff0c\u663e\u5f0f\u5730\u5904\u7406\u5206\u5e03\u4e0d\u5339\u914d\u548c\u7ed3\u6784\u4e0d\u5bf9\u79f0\u6027\u3002\u8be5\u65b9\u6cd5\u786e\u4fdd\u6bcf\u4e2a\u8bed\u8a00\u6807\u8bb0\u90fd\u4ee5\u81f3\u5c11\u4e00\u4e2a\u58f0\u5b66\u89c2\u5bdf\u4e3a\u57fa\u7840\uff0c\u540c\u65f6\u5141\u8bb8\u4ece\u58f0\u5b66\u5355\u5143\u5230\u8bed\u8a00\u5355\u5143\u7684\u7075\u6d3b\u7684\u6982\u7387\u6620\u5c04\u3002", "result": "\u5728\u57fa\u4e8eCTC\u7684ASR\u7cfb\u7edf\u4e0a\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u77e5\u8bc6\u8f6c\u79fb\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7075\u6d3b\u63a7\u5236\u5339\u914d\u7a0b\u5ea6\u4ece\u800c\u63d0\u9ad8ASR\u6027\u80fd\u65b9\u9762\u662f\u6709\u6548\u7684\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5bf9\u9f50\u89c6\u4e3a\u68c0\u6d4b\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u4e0d\u5e73\u8861\u6700\u4f18\u4f20\u8f93\u6765\u5904\u7406\u58f0\u5b66\u548c\u8bed\u8a00\u6a21\u6001\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u548c\u4e0d\u5bf9\u79f0\u6027\uff0c\u4ece\u800c\u6709\u6548\u5730\u63d0\u9ad8\u4e86ASR\u6027\u80fd\u3002"}}
{"id": "2509.05757", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05757", "abs": "https://arxiv.org/abs/2509.05757", "authors": ["Sarang Patil", "Zeyong Zhang", "Yiran Huang", "Tengfei Ma", "Mengjia Xu"], "title": "Hyperbolic Large Language Models", "comment": "32 pages, 6 figures", "summary": "Large language models (LLMs) have achieved remarkable success and\ndemonstrated superior performance across various tasks, including natural\nlanguage processing (NLP), weather forecasting, biological protein folding,\ntext generation, and solving mathematical problems. However, many real-world\ndata exhibit highly non-Euclidean latent hierarchical anatomy, such as protein\nnetworks, transportation networks, financial networks, brain networks, and\nlinguistic structures or syntactic trees in natural languages. Effectively\nlearning intrinsic semantic entailment and hierarchical relationships from\nthese raw, unstructured input data using LLMs remains an underexplored area.\nDue to its effectiveness in modeling tree-like hierarchical structures,\nhyperbolic geometry -- a non-Euclidean space -- has rapidly gained popularity\nas an expressive latent representation space for complex data modeling across\ndomains such as graphs, images, languages, and multi-modal data. Here, we\nprovide a comprehensive and contextual exposition of recent advancements in\nLLMs that leverage hyperbolic geometry as a representation space to enhance\nsemantic representation learning and multi-scale reasoning. Specifically, the\npaper presents a taxonomy of the principal techniques of Hyperbolic LLMs\n(HypLLMs) in terms of four main categories: (1) hyperbolic LLMs through exp/log\nmaps; (2) hyperbolic fine-tuned models; (3) fully hyperbolic LLMs, and (4)\nhyperbolic state-space models. We also explore crucial potential applications\nand outline future research directions. A repository of key papers, models,\ndatasets, and code implementations is available at\nhttps://github.com/sarangp2402/Hyperbolic-LLM-Models/tree/main.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5229\u7528\u53cc\u66f2\u51e0\u4f55\u4f5c\u4e3a\u8868\u793a\u7a7a\u95f4\u6765\u589e\u5f3a\u8bed\u4e49\u8868\u793a\u5b66\u4e60\u548c\u591a\u5c3a\u5ea6\u63a8\u7406\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u6700\u65b0\u8fdb\u5c55\u3002", "motivation": "\u8bb8\u591a\u73b0\u5b9e\u4e16\u754c\u7684\u6570\u636e\u8868\u73b0\u51fa\u9ad8\u5ea6\u975e\u6b27\u51e0\u91cc\u5f97\u6f5c\u5728\u5206\u5c42\u7ed3\u6784\uff0c\u4f8b\u5982\u86cb\u767d\u8d28\u7f51\u7edc\u3001\u4ea4\u901a\u8fd0\u8f93\u7f51\u7edc\u3001\u91d1\u878d\u7f51\u7edc\u3001\u5927\u8111\u7f51\u7edc\u4ee5\u53ca\u81ea\u7136\u8bed\u8a00\u4e2d\u7684\u8bed\u8a00\u7ed3\u6784\u6216\u53e5\u6cd5\u6811\u3002\u4f7f\u7528 LLM \u4ece\u8fd9\u4e9b\u539f\u59cb\u7684\u975e\u7ed3\u6784\u5316\u8f93\u5165\u6570\u636e\u4e2d\u6709\u6548\u5b66\u4e60\u5185\u5728\u7684\u8bed\u4e49\u8574\u542b\u548c\u5206\u5c42\u5173\u7cfb\u4ecd\u7136\u662f\u4e00\u4e2a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u9886\u57df\u3002", "method": "\u672c\u6587\u6839\u636e\u56db\u4e2a\u4e3b\u8981\u7c7b\u522b\u4ecb\u7ecd\u4e86\u53cc\u66f2 LLM (HypLLM) \u7684\u4e3b\u8981\u6280\u672f\u5206\u7c7b\uff1a(1) \u901a\u8fc7 exp/log \u6620\u5c04\u7684\u53cc\u66f2 LLM\uff1b(2) \u53cc\u66f2\u5fae\u8c03\u6a21\u578b\uff1b(3) \u5b8c\u5168\u53cc\u66f2 LLM\uff1b(4) \u53cc\u66f2\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u3002", "result": "\u672c\u6587\u63a2\u8ba8\u4e86\u5173\u952e\u7684\u6f5c\u5728\u5e94\u7528\uff0c\u5e76\u6982\u8ff0\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002\u4e00\u4e2a\u5305\u542b\u5173\u952e\u8bba\u6587\u3001\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5b9e\u73b0\u7684\u5b58\u50a8\u5e93\u53ef\u5728 https://github.com/sarangp2402/Hyperbolic-LLM-Models/tree/main \u83b7\u53d6\u3002", "conclusion": "\u53cc\u66f2\u51e0\u4f55\u4f5c\u4e3a\u4e00\u79cd\u975e\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\uff0c\u7531\u4e8e\u5176\u5728\u5efa\u6a21\u6811\u72b6\u5206\u5c42\u7ed3\u6784\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5df2\u8fc5\u901f\u666e\u53ca\uff0c\u6210\u4e3a\u8de8\u56fe\u3001\u56fe\u50cf\u3001\u8bed\u8a00\u548c\u591a\u6a21\u6001\u6570\u636e\u7b49\u9886\u57df\u7684\u590d\u6742\u6570\u636e\u5efa\u6a21\u7684\u5bcc\u6709\u8868\u73b0\u529b\u7684\u6f5c\u5728\u8868\u793a\u7a7a\u95f4\u3002"}}
{"id": "2509.05348", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05348", "abs": "https://arxiv.org/abs/2509.05348", "authors": ["Ashen Rodrigo", "Isuru Munasinghe", "Asanka Perera"], "title": "Vision-Based Object Detection for UAV Solar Panel Inspection Using an Enhanced Defects Dataset", "comment": null, "summary": "Timely and accurate detection of defects and contaminants in solar panels is\ncritical for maintaining the efficiency and reliability of photovoltaic\nsystems. This study presents a comprehensive evaluation of five\nstate-of-the-art object detection models: YOLOv3, Faster R-CNN, RetinaNet,\nEfficientDet, and Swin Transformer, for identifying physical and electrical\ndefects as well as surface contaminants such as dust, dirt, and bird droppings\non solar panels. A custom dataset, annotated in the COCO format and\nspecifically designed for solar panel defect and contamination detection, was\ndeveloped alongside a user interface to train and evaluate the models. The\nperformance of each model is assessed and compared based on mean Average\nPrecision (mAP), precision, recall, and inference speed. The results\ndemonstrate the trade-offs between detection accuracy and computational\nefficiency, highlighting the relative strengths and limitations of each model.\nThese findings provide valuable guidance for selecting appropriate detection\napproaches in practical solar panel monitoring and maintenance scenarios.\n  The dataset will be publicly available at\nhttps://github.com/IsuruMunasinghe98/solar-panel-inspection-dataset.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u4e94\u79cd\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff08YOLOv3\u3001Faster R-CNN\u3001RetinaNet\u3001EfficientDet \u548c Swin Transformer\uff09\u5728\u8bc6\u522b\u592a\u9633\u80fd\u7535\u6c60\u677f\u7f3a\u9677\u548c\u6c61\u67d3\u7269\u65b9\u9762\u7684\u6027\u80fd\u3002", "motivation": "\u53ca\u65f6\u51c6\u786e\u5730\u68c0\u6d4b\u592a\u9633\u80fd\u7535\u6c60\u677f\u7684\u7f3a\u9677\u548c\u6c61\u67d3\u7269\u5bf9\u4e8e\u7ef4\u6301\u5149\u4f0f\u7cfb\u7edf\u7684\u6548\u7387\u548c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u4e00\u4e2a\u5b9a\u5236\u7684\u3001\u4ee5 COCO \u683c\u5f0f\u6ce8\u91ca\u7684\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u4e13\u95e8\u4e3a\u592a\u9633\u80fd\u7535\u6c60\u677f\u7f3a\u9677\u548c\u6c61\u67d3\u7269\u68c0\u6d4b\u800c\u8bbe\u8ba1\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u7528\u6237\u754c\u9762\u6765\u8bad\u7ec3\u548c\u8bc4\u4f30\u6a21\u578b\u3002\u57fa\u4e8e\u5e73\u5747\u7cbe\u5ea6\u5747\u503c (mAP)\u3001\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548c\u63a8\u7406\u901f\u5ea6\u8bc4\u4f30\u548c\u6bd4\u8f83\u6bcf\u4e2a\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u7ed3\u679c\u8868\u660e\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u7a81\u51fa\u4e86\u6bcf\u4e2a\u6a21\u578b\u7684\u76f8\u5bf9\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u5728\u5b9e\u9645\u592a\u9633\u80fd\u7535\u6c60\u677f\u76d1\u6d4b\u548c\u7ef4\u62a4\u573a\u666f\u4e2d\u9009\u62e9\u5408\u9002\u7684\u68c0\u6d4b\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6307\u5bfc\u3002"}}
{"id": "2509.05656", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05656", "abs": "https://arxiv.org/abs/2509.05656", "authors": ["Bo Lyu", "Yu Cui", "Tuo Shi", "Ke Li"], "title": "OptiProxy-NAS: Optimization Proxy based End-to-End Neural Architecture Search", "comment": null, "summary": "Neural architecture search (NAS) is a hard computationally expensive\noptimization problem with a discrete, vast, and spiky search space. One of the\nkey research efforts dedicated to this space focuses on accelerating NAS via\ncertain proxy evaluations of neural architectures. Different from the prevalent\npredictor-based methods using surrogate models and differentiable architecture\nsearch via supernetworks, we propose an optimization proxy to streamline the\nNAS as an end-to-end optimization framework, named OptiProxy-NAS. In\nparticular, using a proxy representation, the NAS space is reformulated to be\ncontinuous, differentiable, and smooth. Thereby, any differentiable\noptimization method can be applied to the gradient-based search of the relaxed\narchitecture parameters. Our comprehensive experiments on $12$ NAS tasks of $4$\nsearch spaces across three different domains including computer vision, natural\nlanguage processing, and resource-constrained NAS fully demonstrate the\nsuperior search results and efficiency. Further experiments on low-fidelity\nscenarios verify the flexibility.", "AI": {"tldr": "OptiProxy-NAS: Reformulates NAS space to be continuous, differentiable, and smooth for gradient-based search.", "motivation": "Computationally expensive NAS with discrete, vast, and spiky search space.", "method": "Proposes OptiProxy-NAS, an optimization proxy to streamline NAS as an end-to-end optimization framework using a proxy representation for continuous, differentiable NAS space.", "result": "Superior search results and efficiency on 12 NAS tasks across 4 search spaces in computer vision, NLP, and resource-constrained NAS.", "conclusion": "Demonstrates the flexibility through experiments on low-fidelity scenarios."}}
{"id": "2509.06888", "categories": ["cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06888", "abs": "https://arxiv.org/abs/2509.06888", "authors": ["Marc Marone", "Orion Weller", "William Fleshman", "Eugene Yang", "Dawn Lawrie", "Benjamin Van Durme"], "title": "mmBERT: A Modern Multilingual Encoder with Annealed Language Learning", "comment": null, "summary": "Encoder-only languages models are frequently used for a variety of standard\nmachine learning tasks, including classification and retrieval. However, there\nhas been a lack of recent research for encoder models, especially with respect\nto multilingual models. We introduce mmBERT, an encoder-only language model\npretrained on 3T tokens of multilingual text in over 1800 languages. To build\nmmBERT we introduce several novel elements, including an inverse mask ratio\nschedule and an inverse temperature sampling ratio. We add over 1700\nlow-resource languages to the data mix only during the decay phase, showing\nthat it boosts performance dramatically and maximizes the gains from the\nrelatively small amount of training data. Despite only including these\nlow-resource languages in the short decay phase we achieve similar\nclassification performance to models like OpenAI's o3 and Google's Gemini 2.5\nPro. Overall, we show that mmBERT significantly outperforms the previous\ngeneration of models on classification and retrieval tasks -- on both high and\nlow-resource languages.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86mmBERT\uff0c\u4e00\u4e2a\u5728\u8d85\u8fc71800\u79cd\u8bed\u8a00\u76843T tokens\u591a\u8bed\u6587\u672c\u4e0a\u9884\u8bad\u7ec3\u7684\u4ec5\u7f16\u7801\u5668\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u7f3a\u4e4f\u5bf9\u7f16\u7801\u5668\u6a21\u578b\u7684\u6700\u65b0\u7814\u7a76\uff0c\u5c24\u5176\u662f\u5728\u591a\u8bed\u6a21\u578b\u65b9\u9762\u3002", "method": "\u5f15\u5165\u4e86\u51e0\u79cd\u65b0\u9896\u7684\u5143\u7d20\uff0c\u5305\u62ec\u9006\u63a9\u7801\u6bd4\u7387\u8c03\u5ea6\u548c\u9006\u6e29\u5ea6\u91c7\u6837\u6bd4\u7387\u3002\u5728\u8870\u51cf\u9636\u6bb5\u5411\u6570\u636e\u7ec4\u5408\u4e2d\u6dfb\u52a0\u4e86\u8d85\u8fc71700\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\u3002", "result": "\u5728\u5206\u7c7b\u6027\u80fd\u4e0a\u4e0eOpenAI\u7684o3\u548cGoogle\u7684Gemini 2.5 Pro\u7b49\u6a21\u578b\u76f8\u4f3c\u3002\u5728\u5206\u7c7b\u548c\u68c0\u7d22\u4efb\u52a1\u4e0a\uff0cmmBERT\u663e\u8457\u4f18\u4e8e\u524d\u4e00\u4ee3\u6a21\u578b\uff0c\u65e0\u8bba\u662f\u5728\u9ad8\u8d44\u6e90\u8bed\u8a00\u8fd8\u662f\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u3002", "conclusion": "mmBERT\u5728\u591a\u8bed\u79cd\u5206\u7c7b\u548c\u68c0\u7d22\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u65b9\u9762\u3002"}}
{"id": "2509.05617", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05617", "abs": "https://arxiv.org/abs/2509.05617", "authors": ["Shay Dahary", "Avi Edana", "Alexander Apartsin", "Yehudit Aperstein"], "title": "From Joy to Fear: A Benchmark of Emotion Estimation in Pop Song Lyrics", "comment": "5 pages, 2 figures", "summary": "The emotional content of song lyrics plays a pivotal role in shaping listener\nexperiences and influencing musical preferences. This paper investigates the\ntask of multi-label emotional attribution of song lyrics by predicting six\nemotional intensity scores corresponding to six fundamental emotions. A\nmanually labeled dataset is constructed using a mean opinion score (MOS)\napproach, which aggregates annotations from multiple human raters to ensure\nreliable ground-truth labels. Leveraging this dataset, we conduct a\ncomprehensive evaluation of several publicly available large language models\n(LLMs) under zero-shot scenarios. Additionally, we fine-tune a BERT-based model\nspecifically for predicting multi-label emotion scores. Experimental results\nreveal the relative strengths and limitations of zero-shot and fine-tuned\nmodels in capturing the nuanced emotional content of lyrics. Our findings\nhighlight the potential of LLMs for emotion recognition in creative texts,\nproviding insights into model selection strategies for emotion-based music\ninformation retrieval applications. The labeled dataset is available at\nhttps://github.com/LLM-HITCS25S/LyricsEmotionAttribution.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u6b4c\u66f2\u6b4c\u8bcd\u7684\u60c5\u611f\u5f52\u5c5e\u95ee\u9898\uff0c\u901a\u8fc7\u9884\u6d4b\u516d\u79cd\u57fa\u672c\u60c5\u611f\u7684\u5f3a\u5ea6\u8bc4\u5206\u6765\u8fdb\u884c\u591a\u6807\u7b7e\u60c5\u611f\u5206\u7c7b\u3002", "motivation": "\u7814\u7a76\u6b4c\u8bcd\u60c5\u611f\u5185\u5bb9\u5bf9\u542c\u4f17\u4f53\u9a8c\u548c\u97f3\u4e50\u504f\u597d\u7684\u5f71\u54cd\u3002", "method": "1. \u6784\u5efa\u4e86\u4e00\u4e2a\u4eba\u5de5\u6807\u6ce8\u7684\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u5e73\u5747\u610f\u89c1\u5f97\u5206\uff08MOS\uff09\u65b9\u6cd5\u805a\u5408\u591a\u4e2a\u8bc4\u5206\u8005\u7684\u6ce8\u91ca\u30022. \u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\uff0c\u8bc4\u4f30\u4e86\u51e0\u4e2a\u516c\u5f00\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u30023. \u9488\u5bf9\u9884\u6d4b\u591a\u6807\u7b7e\u60c5\u611f\u8bc4\u5206\uff0c\u5bf9\u57fa\u4e8eBERT\u7684\u6a21\u578b\u8fdb\u884c\u4e86\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u63ed\u793a\u4e86\u96f6\u6837\u672c\u6a21\u578b\u548c\u5fae\u8c03\u6a21\u578b\u5728\u6355\u6349\u6b4c\u8bcd\u7ec6\u5fae\u60c5\u611f\u5185\u5bb9\u65b9\u9762\u7684\u76f8\u5bf9\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86LLMs\u5728\u521b\u610f\u6587\u672c\u60c5\u611f\u8bc6\u522b\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u57fa\u4e8e\u60c5\u611f\u7684\u97f3\u4e50\u4fe1\u606f\u68c0\u7d22\u5e94\u7528\u63d0\u4f9b\u4e86\u6a21\u578b\u9009\u62e9\u7b56\u7565\u7684\u89c1\u89e3\u3002"}}
{"id": "2509.05764", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05764", "abs": "https://arxiv.org/abs/2509.05764", "authors": ["Yuwei Lou", "Hao Hu", "Shaocong Ma", "Zongfei Zhang", "Liang Wang", "Jidong Ge", "Xianping Tao"], "title": "DRF: LLM-AGENT Dynamic Reputation Filtering Framework", "comment": "This paper has been accepted by ICONIP 2025 but not published", "summary": "With the evolution of generative AI, multi - agent systems leveraging large -\nlanguage models(LLMs) have emerged as a powerful tool for complex tasks.\nHowever, these systems face challenges in quantifying agent performance and\nlack mechanisms to assess agent credibility. To address these issues, we\nintroduce DRF, a dynamic reputation filtering framework. DRF constructs an\ninteractive rating network to quantify agent performance, designs a reputation\nscoring mechanism to measure agent honesty and capability, and integrates an\nUpper Confidence Bound - based strategy to enhance agent selection efficiency.\nExperiments show that DRF significantly improves task completion quality and\ncollaboration efficiency in logical reasoning and code - generation tasks,\noffering a new approach for multi - agent systems to handle large - scale\ntasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u52a8\u6001\u4fe1\u8a89\u8fc7\u6ee4\u6846\u67b6\uff08DRF\uff09\uff0c\u7528\u4e8e\u91cf\u5316agent\u6027\u80fd\u548c\u8bc4\u4f30agent\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u91cf\u5316\u667a\u80fd\u4f53\u6027\u80fd\u548c\u8bc4\u4f30\u667a\u80fd\u4f53\u53ef\u4fe1\u5ea6\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u8bc4\u7ea7\u7f51\u7edc\u6765\u91cf\u5316\u667a\u80fd\u4f53\u6027\u80fd\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4fe1\u8a89\u8bc4\u5206\u673a\u5236\u6765\u8861\u91cf\u667a\u80fd\u4f53\u7684\u8bda\u5b9e\u5ea6\u548c\u80fd\u529b\uff0c\u5e76\u6574\u5408\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f6e\u4fe1\u4e0a\u9650\u7684\u7b56\u7565\u6765\u63d0\u9ad8\u667a\u80fd\u4f53\u9009\u62e9\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDRF\u663e\u8457\u63d0\u9ad8\u4e86\u903b\u8f91\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u4efb\u52a1\u5b8c\u6210\u8d28\u91cf\u548c\u534f\u4f5c\u6548\u7387\u3002", "conclusion": "DRF\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5904\u7406\u5927\u89c4\u6a21\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2509.05352", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05352", "abs": "https://arxiv.org/abs/2509.05352", "authors": ["Cuong Manh Hoang"], "title": "Unsupervised Instance Segmentation with Superpixels", "comment": null, "summary": "Instance segmentation is essential for numerous computer vision applications,\nincluding robotics, human-computer interaction, and autonomous driving.\nCurrently, popular models bring impressive performance in instance segmentation\nby training with a large number of human annotations, which are costly to\ncollect. For this reason, we present a new framework that efficiently and\neffectively segments objects without the need for human annotations. Firstly, a\nMultiCut algorithm is applied to self-supervised features for coarse mask\nsegmentation. Then, a mask filter is employed to obtain high-quality coarse\nmasks. To train the segmentation network, we compute a novel superpixel-guided\nmask loss, comprising hard loss and soft loss, with high-quality coarse masks\nand superpixels segmented from low-level image features. Lastly, a\nself-training process with a new adaptive loss is proposed to improve the\nquality of predicted masks. We conduct experiments on public datasets in\ninstance segmentation and object detection to demonstrate the effectiveness of\nthe proposed framework. The results show that the proposed framework\noutperforms previous state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b9e\u4f8b\u5206\u5272\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u65e0\u9700\u4eba\u5de5\u6ce8\u91ca\u5373\u53ef\u6709\u6548\u5206\u5272\u5bf9\u8c61\u3002", "motivation": "\u5f53\u524d\u6d41\u884c\u7684\u6a21\u578b\u901a\u8fc7\u5927\u91cf\u4eba\u5de5\u6ce8\u91ca\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ece\u800c\u5728\u5b9e\u4f8b\u5206\u5272\u4e2d\u5e26\u6765\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\uff0c\u800c\u4eba\u5de5\u6ce8\u91ca\u7684\u6536\u96c6\u6210\u672c\u5f88\u9ad8\u3002", "method": "\u9996\u5148\uff0c\u5c06 MultiCut \u7b97\u6cd5\u5e94\u7528\u4e8e\u81ea\u76d1\u7763\u7279\u5f81\u4ee5\u8fdb\u884c\u7c97\u7565\u7684\u63a9\u7801\u5206\u5272\u3002\u7136\u540e\uff0c\u91c7\u7528\u63a9\u7801\u6ee4\u6ce2\u5668\u6765\u83b7\u5f97\u9ad8\u8d28\u91cf\u7684\u7c97\u7565\u63a9\u7801\u3002\u4e3a\u4e86\u8bad\u7ec3\u5206\u5272\u7f51\u7edc\uff0c\u6211\u4eec\u8ba1\u7b97\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8d85\u50cf\u7d20\u5f15\u5bfc\u63a9\u7801\u635f\u5931\uff0c\u8be5\u635f\u5931\u5305\u62ec\u786c\u635f\u5931\u548c\u8f6f\u635f\u5931\uff0c\u4ee5\u53ca\u4ece\u4f4e\u7ea7\u56fe\u50cf\u7279\u5f81\u5206\u5272\u7684\u9ad8\u8d28\u91cf\u7c97\u7565\u63a9\u7801\u548c\u8d85\u50cf\u7d20\u3002\u6700\u540e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u91c7\u7528\u65b0\u7684\u81ea\u9002\u5e94\u635f\u5931\u7684\u81ea\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u4ee5\u63d0\u9ad8\u9884\u6d4b\u63a9\u7801\u7684\u8d28\u91cf\u3002", "result": "\u5728\u5b9e\u4f8b\u5206\u5272\u548c\u5bf9\u8c61\u68c0\u6d4b\u7684\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u4f18\u4e8e\u4ee5\u5f80\u7684\u6700\u65b0\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u9ad8\u6548\u5b9e\u4f8b\u5206\u5272\u6846\u67b6\uff0c\u5e76\u5728\u516c\u5171\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u6c34\u5e73\u7684\u7ed3\u679c\u3002"}}
{"id": "2509.05663", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05663", "abs": "https://arxiv.org/abs/2509.05663", "authors": ["Lucas Correia", "Jan-Christoph Goos", "Thomas B\u00e4ck", "Anna V. Kononova"], "title": "DQS: A Low-Budget Query Strategy for Enhancing Unsupervised Data-driven Anomaly Detection Approaches", "comment": "Submitted to the Reliability Engineering & System Safety journal", "summary": "Truly unsupervised approaches for time series anomaly detection are rare in\nthe literature. Those that exist suffer from a poorly set threshold, which\nhampers detection performance, while others, despite claiming to be\nunsupervised, need to be calibrated using a labelled data subset, which is\noften not available in the real world. This work integrates active learning\nwith an existing unsupervised anomaly detection method by selectively querying\nthe labels of multivariate time series, which are then used to refine the\nthreshold selection process. To achieve this, we introduce a novel query\nstrategy called the dissimilarity-based query strategy (DQS). DQS aims to\nmaximise the diversity of queried samples by evaluating the similarity between\nanomaly scores using dynamic time warping. We assess the detection performance\nof DQS in comparison to other query strategies and explore the impact of\nmislabelling, a topic that is underexplored in the literature. Our findings\nindicate that DQS performs best in small-budget scenarios, though the others\nappear to be more robust when faced with mislabelling. Therefore, in the real\nworld, the choice of query strategy depends on the expertise of the oracle and\nthe number of samples they are willing to label. Regardless, all query\nstrategies outperform the unsupervised threshold even in the presence of\nmislabelling. Thus, whenever it is feasible to query an oracle, employing an\nactive learning-based threshold is recommended.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u52a8\u5b66\u4e60\u7684\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5730\u67e5\u8be2\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u7684\u6807\u7b7e\u6765\u4f18\u5316\u9608\u503c\u9009\u62e9\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709\u7684\u65e0\u76d1\u7763\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u9608\u503c\u8bbe\u7f6e\u4e0d\u4f73\uff0c\u6216\u8005\u9700\u8981\u4f7f\u7528\u5e26\u6807\u7b7e\u7684\u6570\u636e\u5b50\u96c6\u8fdb\u884c\u6821\u51c6\uff0c\u4f46\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\uff0c\u5e26\u6807\u7b7e\u7684\u6570\u636e\u5b50\u96c6\u901a\u5e38\u4e0d\u53ef\u7528\u3002", "method": "\u672c\u6587\u5c06\u4e3b\u52a8\u5b66\u4e60\u4e0e\u73b0\u6709\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u76f8\u7ed3\u5408\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u67e5\u8be2\u7b56\u7565\uff0c\u79f0\u4e3a\u57fa\u4e8e\u5dee\u5f02\u6027\u7684\u67e5\u8be2\u7b56\u7565 (DQS)\uff0c\u8be5\u7b56\u7565\u65e8\u5728\u901a\u8fc7\u4f7f\u7528\u52a8\u6001\u65f6\u95f4\u89c4\u6574\u8bc4\u4f30\u5f02\u5e38\u5206\u6570\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u6765\u6700\u5927\u5316\u67e5\u8be2\u6837\u672c\u7684\u591a\u6837\u6027\u3002", "result": "DQS \u5728\u5c0f\u9884\u7b97\u573a\u666f\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u5176\u4ed6\u65b9\u6cd5\u5728\u9762\u5bf9\u9519\u8bef\u6807\u8bb0\u65f6\u4f3c\u4e4e\u66f4\u7a33\u5065\u3002\u6240\u6709\u67e5\u8be2\u7b56\u7565\u90fd\u4f18\u4e8e\u65e0\u76d1\u7763\u9608\u503c\uff0c\u5373\u4f7f\u5728\u5b58\u5728\u9519\u8bef\u6807\u8bb0\u7684\u60c5\u51b5\u4e0b\u4e5f\u662f\u5982\u6b64\u3002", "conclusion": "\u53ea\u8981\u53ef\u4ee5\u67e5\u8be2 Oracle\uff0c\u5efa\u8bae\u91c7\u7528\u57fa\u4e8e\u4e3b\u52a8\u5b66\u4e60\u7684\u9608\u503c\u3002\u67e5\u8be2\u7b56\u7565\u7684\u9009\u62e9\u53d6\u51b3\u4e8e Oracle \u7684\u4e13\u4e1a\u77e5\u8bc6\u548c\u4ed6\u4eec\u613f\u610f\u6807\u8bb0\u7684\u6837\u672c\u6570\u91cf\u3002"}}
{"id": "2509.05772", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05772", "abs": "https://arxiv.org/abs/2509.05772", "authors": ["Nasser Alkhulaifi", "Ismail Gokay Dogan", "Timothy R. Cargan", "Alexander L. Bowler", "Direnc Pekaslan", "Nicholas J. Watson", "Isaac Triguero"], "title": "Decision-Focused Learning Enhanced by Automated Feature Engineering for Energy Storage Optimisation", "comment": "22 pages, 10 figures, journal-based paper", "summary": "Decision-making under uncertainty in energy management is complicated by\nunknown parameters hindering optimal strategies, particularly in Battery Energy\nStorage System (BESS) operations. Predict-Then-Optimise (PTO) approaches treat\nforecasting and optimisation as separate processes, allowing prediction errors\nto cascade into suboptimal decisions as models minimise forecasting errors\nrather than optimising downstream tasks. The emerging Decision-Focused Learning\n(DFL) methods overcome this limitation by integrating prediction and\noptimisation; however, they are relatively new and have been tested primarily\non synthetic datasets or small-scale problems, with limited evidence of their\npractical viability. Real-world BESS applications present additional\nchallenges, including greater variability and data scarcity due to collection\nconstraints and operational limitations. Because of these challenges, this work\nleverages Automated Feature Engineering (AFE) to extract richer representations\nand improve the nascent approach of DFL. We propose an AFE-DFL framework\nsuitable for small datasets that forecasts electricity prices and demand while\noptimising BESS operations to minimise costs. We validate its effectiveness on\na novel real-world UK property dataset. The evaluation compares DFL methods\nagainst PTO, with and without AFE. The results show that, on average, DFL\nyields lower operating costs than PTO and adding AFE further improves the\nperformance of DFL methods by 22.9-56.5% compared to the same models without\nAFE. These findings provide empirical evidence for DFL's practical viability in\nreal-world settings, indicating that domain-specific AFE enhances DFL and\nreduces reliance on domain expertise for BESS optimisation, yielding economic\nbenefits with broader implications for energy management systems facing similar\nchallenges.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u52a8\u5316\u7279\u5f81\u5de5\u7a0b\uff08AFE\uff09\u7684\u51b3\u7b56\u91cd\u70b9\u5b66\u4e60\uff08DFL\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u7535\u6c60\u50a8\u80fd\u7cfb\u7edf\uff08BESS\uff09\u7684\u8fd0\u884c\uff0c\u4ee5\u964d\u4f4e\u6210\u672c\u3002", "motivation": "\u5728\u80fd\u6e90\u7ba1\u7406\u4e2d\uff0c\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u51b3\u7b56\u5f88\u590d\u6742\uff0c\u5c24\u5176\u662f\u5728\u7535\u6c60\u50a8\u80fd\u7cfb\u7edf\uff08BESS\uff09\u7684\u8fd0\u884c\u4e2d\uff0c\u9884\u6d4b\u8bef\u5dee\u4f1a\u4f20\u9012\u5230\u6b21\u4f18\u51b3\u7b56\u4e2d\u3002\u867d\u7136\u65b0\u5174\u7684\u51b3\u7b56\u91cd\u70b9\u5b66\u4e60\uff08DFL\uff09\u65b9\u6cd5\u96c6\u6210\u4e86\u9884\u6d4b\u548c\u4f18\u5316\uff0c\u4f46\u5b83\u4eec\u4e3b\u8981\u5728\u5408\u6210\u6570\u636e\u96c6\u6216\u5c0f\u89c4\u6a21\u95ee\u9898\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0c\u7f3a\u4e4f\u5b9e\u9645\u53ef\u884c\u6027\u7684\u8bc1\u636e\u3002", "method": "\u672c\u6587\u5229\u7528\u81ea\u52a8\u5316\u7279\u5f81\u5de5\u7a0b\uff08AFE\uff09\u6765\u63d0\u53d6\u66f4\u4e30\u5bcc\u7684\u8868\u793a\uff0c\u5e76\u6539\u8fdbDFL\u7684\u65b0\u5174\u65b9\u6cd5\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u9002\u7528\u4e8e\u5c0f\u6570\u636e\u96c6\u7684AFE-DFL\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u53ef\u4ee5\u9884\u6d4b\u7535\u4ef7\u548c\u9700\u6c42\uff0c\u540c\u65f6\u4f18\u5316BESS\u8fd0\u884c\u4ee5\u964d\u4f4e\u6210\u672c\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5e73\u5747\u800c\u8a00\uff0cDFL\u4ea7\u751f\u7684\u8fd0\u8425\u6210\u672c\u4f4e\u4e8ePTO\uff0c\u5e76\u4e14\u4e0e\u6ca1\u6709AFE\u7684\u76f8\u540c\u6a21\u578b\u76f8\u6bd4\uff0c\u6dfb\u52a0AFE\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86DFL\u65b9\u6cd5\u7684\u6027\u80fd22.9-56.5%\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3aDFL\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u53ef\u884c\u6027\u63d0\u4f9b\u4e86\u7ecf\u9a8c\u8bc1\u636e\uff0c\u8868\u660e\u7279\u5b9a\u9886\u57df\u7684AFE\u589e\u5f3a\u4e86DFL\uff0c\u5e76\u51cf\u5c11\u4e86\u5bf9BESS\u4f18\u5316\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u7684\u4f9d\u8d56\uff0c\u4ece\u800c\u4ea7\u751f\u4e86\u7ecf\u6d4e\u6548\u76ca\uff0c\u5e76\u5bf9\u9762\u4e34\u7c7b\u4f3c\u6311\u6218\u7684\u80fd\u6e90\u7ba1\u7406\u7cfb\u7edf\u5177\u6709\u66f4\u5e7f\u6cdb\u7684\u610f\u4e49\u3002"}}
{"id": "2509.05388", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05388", "abs": "https://arxiv.org/abs/2509.05388", "authors": ["Juan Olalla-Pombo", "Alberto Bad\u00edas", "Miguel \u00c1ngel Sanz-G\u00f3mez", "Jos\u00e9 Mar\u00eda Ben\u00edtez", "Francisco Javier Mont\u00e1ns"], "title": "Augmented Structure Preserving Neural Networks for cell biomechanics", "comment": null, "summary": "Cell biomechanics involve a great number of complex phenomena that are\nfundamental to the evolution of life itself and other associated processes,\nranging from the very early stages of embryo-genesis to the maintenance of\ndamaged structures or the growth of tumors. Given the importance of such\nphenomena, increasing research has been dedicated to their understanding, but\nthe many interactions between them and their influence on the decisions of\ncells as a collective network or cluster remain unclear. We present a new\napproach that combines Structure Preserving Neural Networks, which study cell\nmovements as a purely mechanical system, with other Machine Learning tools\n(Artificial Neural Networks), which allow taking into consideration\nenvironmental factors that can be directly deduced from an experiment with\nComputer Vision techniques. This new model, tested on simulated and real cell\nmigration cases, predicts complete cell trajectories following a roll-out\npolicy with a high level of accuracy. This work also includes a mitosis event\nprediction model based on Neural Networks architectures which makes use of the\nsame observed features.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ec6\u80de\u8fd0\u52a8\u9884\u6d4b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u7ed3\u6784\u4fdd\u6301\u795e\u7ecf\u7f51\u7edc\u548c\u673a\u5668\u5b66\u4e60\u5de5\u5177\uff0c\u53ef\u4ee5\u51c6\u786e\u9884\u6d4b\u7ec6\u80de\u8f68\u8ff9\uff0c\u5e76\u5305\u542b\u4e00\u4e2a\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684mitosis\u4e8b\u4ef6\u9884\u6d4b\u6a21\u578b\u3002", "motivation": "\u7406\u89e3\u7ec6\u80de\u751f\u7269\u529b\u5b66\u4e2d\u7684\u590d\u6742\u73b0\u8c61\u5bf9\u4e8e\u7406\u89e3\u751f\u547d\u8fdb\u5316\u548c\u76f8\u5173\u8fc7\u7a0b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7ec6\u80de\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u53ca\u5176\u5bf9\u7ec6\u80de\u51b3\u7b56\u7684\u5f71\u54cd\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u7ed3\u5408\u4e86\u7ed3\u6784\u4fdd\u6301\u795e\u7ecf\u7f51\u7edc\uff08\u7814\u7a76\u7ec6\u80de\u8fd0\u52a8\u4f5c\u4e3a\u7eaf\u673a\u68b0\u7cfb\u7edf\uff09\u548c\u5176\u4ed6\u673a\u5668\u5b66\u4e60\u5de5\u5177\uff08\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff09\uff0c\u540e\u8005\u53ef\u4ee5\u8003\u8651\u901a\u8fc7\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u4ece\u5b9e\u9a8c\u4e2d\u76f4\u63a5\u63a8\u65ad\u51fa\u7684\u73af\u5883\u56e0\u7d20\u3002", "result": "\u8be5\u6a21\u578b\u5728\u6a21\u62df\u548c\u771f\u5b9e\u7ec6\u80de\u8fc1\u79fb\u6848\u4f8b\u4e2d\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u53ef\u4ee5\u9ad8\u7cbe\u5ea6\u5730\u9884\u6d4b\u5b8c\u6574\u7684\u7ec6\u80de\u8f68\u8ff9\u3002\u8be5\u5de5\u4f5c\u8fd8\u5305\u62ec\u4e00\u4e2a\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7684mitosis\u4e8b\u4ef6\u9884\u6d4b\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u4f7f\u7528\u4e86\u76f8\u540c\u7684\u89c2\u5bdf\u7279\u5f81\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b0\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u7ec6\u80de\u8f68\u8ff9\uff0c\u5e76\u53ef\u7528\u4e8emitosis\u4e8b\u4ef6\u9884\u6d4b\u3002"}}
{"id": "2509.05671", "categories": ["cs.LG", "cs.AI", "cs.CR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.05671", "abs": "https://arxiv.org/abs/2509.05671", "authors": ["Labani Halder", "Tanmay Sen", "Sarbani Palit"], "title": "GraMFedDHAR: Graph Based Multimodal Differentially Private Federated HAR", "comment": null, "summary": "Human Activity Recognition (HAR) using multimodal sensor data remains\nchallenging due to noisy or incomplete measurements, scarcity of labeled\nexamples, and privacy concerns. Traditional centralized deep learning\napproaches are often constrained by infrastructure availability, network\nlatency, and data sharing restrictions. While federated learning (FL) addresses\nprivacy by training models locally and sharing only model parameters, it still\nhas to tackle issues arising from the use of heterogeneous multimodal data and\ndifferential privacy requirements. In this article, a Graph-based Multimodal\nFederated Learning framework, GraMFedDHAR, is proposed for HAR tasks. Diverse\nsensor streams such as a pressure mat, depth camera, and multiple\naccelerometers are modeled as modality-specific graphs, processed through\nresidual Graph Convolutional Neural Networks (GCNs), and fused via\nattention-based weighting rather than simple concatenation. The fused\nembeddings enable robust activity classification, while differential privacy\nsafeguards data during federated aggregation. Experimental results show that\nthe proposed MultiModalGCN model outperforms the baseline MultiModalFFN, with\nup to 2 percent higher accuracy in non-DP settings in both centralized and\nfederated paradigms. More importantly, significant improvements are observed\nunder differential privacy constraints: MultiModalGCN consistently surpasses\nMultiModalFFN, with performance gaps ranging from 7 to 13 percent depending on\nthe privacy budget and setting. These results highlight the robustness of\ngraph-based modeling in multimodal learning, where GNNs prove more resilient to\nthe performance degradation introduced by DP noise.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u591a\u6a21\u6001\u8054\u90a6\u5b66\u4e60\u6846\u67b6 GraMFedDHAR\uff0c\u7528\u4e8e\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b (HAR) \u4efb\u52a1\u3002", "motivation": "\u7531\u4e8e\u566a\u58f0\u6216\u4e0d\u5b8c\u6574\u7684\u6d4b\u91cf\u3001\u6807\u8bb0\u793a\u4f8b\u7684\u7a00\u7f3a\u4ee5\u53ca\u9690\u79c1\u95ee\u9898\uff0c\u4f7f\u7528\u591a\u6a21\u6001\u4f20\u611f\u5668\u6570\u636e\u8fdb\u884c\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b (HAR) \u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u4f20\u7edf\u7684\u96c6\u4e2d\u5f0f\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u53d7\u5230\u57fa\u7840\u8bbe\u65bd\u53ef\u7528\u6027\u3001\u7f51\u7edc\u5ef6\u8fdf\u548c\u6570\u636e\u5171\u4eab\u9650\u5236\u7684\u7ea6\u675f\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u538b\u529b\u57ab\u3001\u6df1\u5ea6\u76f8\u673a\u548c\u591a\u4e2a\u52a0\u901f\u5ea6\u8ba1\u7b49\u4e0d\u540c\u7684\u4f20\u611f\u5668\u6d41\u5efa\u6a21\u4e3a\u7279\u5b9a\u4e8e\u6a21\u6001\u7684\u56fe\uff0c\u901a\u8fc7\u6b8b\u5dee\u56fe\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (GCN) \u8fdb\u884c\u5904\u7406\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u52a0\u6743\u878d\u5408\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u7684\u8fde\u63a5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684 MultiModalGCN \u6a21\u578b\u4f18\u4e8e\u57fa\u7ebf MultiModalFFN\uff0c\u5728\u96c6\u4e2d\u5f0f\u548c\u8054\u90a6\u8303\u4f8b\u4e2d\u7684\u975e DP \u8bbe\u7f6e\u4e2d\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u4e86 2%\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u5728\u5dee\u5206\u9690\u79c1\u7ea6\u675f\u4e0b\u89c2\u5bdf\u5230\u663e\u7740\u6539\u8fdb\uff1aMultiModalGCN \u59cb\u7ec8\u4f18\u4e8e MultiModalFFN\uff0c\u6027\u80fd\u5dee\u8ddd\u4e3a 7% \u5230 13%\uff0c\u5177\u4f53\u53d6\u51b3\u4e8e\u9690\u79c1\u9884\u7b97\u548c\u8bbe\u7f6e\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u7a81\u51fa\u4e86\u57fa\u4e8e\u56fe\u5efa\u6a21\u5728\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u5176\u4e2d GNN \u88ab\u8bc1\u660e\u66f4\u80fd\u62b5\u6297 DP \u566a\u58f0\u5f15\u5165\u7684\u6027\u80fd\u4e0b\u964d\u3002"}}
{"id": "2509.05657", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05657", "abs": "https://arxiv.org/abs/2509.05657", "authors": ["Yuxuan Hu", "Jihao Liu", "Ke Wang", "Jinliang Zhen", "Weikang Shi", "Manyuan Zhang", "Qi Dou", "Rui Liu", "Aojun Zhou", "Hongsheng Li"], "title": "LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding", "comment": "EMNLP2025", "summary": "Recent progress in Large Language Models (LLMs) has opened new avenues for\nsolving complex optimization problems, including Neural Architecture Search\n(NAS). However, existing LLM-driven NAS approaches rely heavily on prompt\nengineering and domain-specific tuning, limiting their practicality and\nscalability across diverse tasks. In this work, we propose LM-Searcher, a novel\nframework that leverages LLMs for cross-domain neural architecture optimization\nwithout the need for extensive domain-specific adaptation. Central to our\napproach is NCode, a universal numerical string representation for neural\narchitectures, which enables cross-domain architecture encoding and search. We\nalso reformulate the NAS problem as a ranking task, training LLMs to select\nhigh-performing architectures from candidate pools using instruction-tuning\nsamples derived from a novel pruning-based subspace sampling strategy. Our\ncurated dataset, encompassing a wide range of architecture-performance pairs,\nencourages robust and transferable learning. Comprehensive experiments\ndemonstrate that LM-Searcher achieves competitive performance in both in-domain\n(e.g., CNNs for image classification) and out-of-domain (e.g., LoRA\nconfigurations for segmentation and generation) tasks, establishing a new\nparadigm for flexible and generalizable LLM-based architecture search. The\ndatasets and models will be released at https://github.com/Ashone3/LM-Searcher.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a LM-Searcher \u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u8fdb\u884c\u8de8\u57df\u795e\u7ecf\u67b6\u6784\u4f18\u5316\uff0c\u65e0\u9700\u8fdb\u884c\u5e7f\u6cdb\u7684\u9886\u57df\u7279\u5b9a\u8c03\u6574\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e LLM \u7684 NAS \u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u63d0\u793a\u5de5\u7a0b\u548c\u9886\u57df\u7279\u5b9a\u8c03\u6574\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u7684\u6838\u5fc3\u662f NCode\uff0c\u4e00\u79cd\u7528\u4e8e\u795e\u7ecf\u67b6\u6784\u7684\u901a\u7528\u6570\u5b57\u5b57\u7b26\u4e32\u8868\u793a\uff0c\u5b83\u652f\u6301\u8de8\u57df\u67b6\u6784\u7f16\u7801\u548c\u641c\u7d22\u3002\u8fd8\u5c06 NAS \u95ee\u9898\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6392\u5e8f\u4efb\u52a1\uff0c\u8bad\u7ec3 LLM \u4ece\u5019\u9009\u6c60\u4e2d\u9009\u62e9\u9ad8\u6027\u80fd\u67b6\u6784\u3002", "result": "LM-Searcher \u5728\u540c\u57df\uff08\u4f8b\u5982\uff0c\u7528\u4e8e\u56fe\u50cf\u5206\u7c7b\u7684 CNN\uff09\u548c\u5f02\u57df\uff08\u4f8b\u5982\uff0c\u7528\u4e8e\u5206\u5272\u548c\u751f\u6210\u7684 LoRA \u914d\u7f6e\uff09\u4efb\u52a1\u4e2d\u90fd\u53d6\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7075\u6d3b\u4e14\u53ef\u6cdb\u5316\u7684\u57fa\u4e8e LLM \u7684\u67b6\u6784\u641c\u7d22\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u8303\u4f8b\u3002"}}
{"id": "2509.05818", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05818", "abs": "https://arxiv.org/abs/2509.05818", "authors": ["Won Seok Jang", "Hieu Tran", "Manav Mistry", "SaiKiran Gandluri", "Yifan Zhang", "Sharmin Sultana", "Sunjae Kown", "Yuan Zhang", "Zonghai Yao", "Hong Yu"], "title": "Chatbot To Help Patients Understand Their Health", "comment": "Accepted in EMNLP 2025 Findings", "summary": "Patients must possess the knowledge necessary to actively participate in\ntheir care. We present NoteAid-Chatbot, a conversational AI that promotes\npatient understanding via a novel 'learning as conversation' framework, built\non a multi-agent large language model (LLM) and reinforcement learning (RL)\nsetup without human-labeled data. NoteAid-Chatbot was built on a lightweight\nLLaMA 3.2 3B model trained in two stages: initial supervised fine-tuning on\nconversational data synthetically generated using medical conversation\nstrategies, followed by RL with rewards derived from patient understanding\nassessments in simulated hospital discharge scenarios. Our evaluation, which\nincludes comprehensive human-aligned assessments and case studies, demonstrates\nthat NoteAid-Chatbot exhibits key emergent behaviors critical for patient\neducation, such as clarity, relevance, and structured dialogue, even though it\nreceived no explicit supervision for these attributes. Our results show that\neven simple Proximal Policy Optimization (PPO)-based reward modeling can\nsuccessfully train lightweight, domain-specific chatbots to handle multi-turn\ninteractions, incorporate diverse educational strategies, and meet nuanced\ncommunication objectives. Our Turing test demonstrates that NoteAid-Chatbot\nsurpasses non-expert human. Although our current focus is on healthcare, the\nframework we present illustrates the feasibility and promise of applying\nlow-cost, PPO-based RL to realistic, open-ended conversational domains,\nbroadening the applicability of RL-based alignment methods.", "AI": {"tldr": "NoteAid-Chatbot is a conversational AI that helps patients understand their care using a 'learning as conversation' framework.", "motivation": "Patients need knowledge to participate in their care, and current methods may be insufficient.", "method": "A multi-agent LLM and reinforcement learning setup was used to train a lightweight LLaMA 3.2 3B model. It was fine-tuned on synthetic conversational data and then trained with RL based on patient understanding assessments.", "result": "NoteAid-Chatbot demonstrates clarity, relevance, and structured dialogue in patient education, even without explicit supervision. It surpasses non-expert humans in a Turing test.", "conclusion": "The study demonstrates the feasibility of using low-cost, PPO-based RL for open-ended conversational domains, expanding the use of RL-based alignment methods, with a focus on healthcare."}}
{"id": "2509.05431", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05431", "abs": "https://arxiv.org/abs/2509.05431", "authors": ["GodsGift Uzor", "Tania-Amanda Nkoyo Fredrick Eneye", "Chukwuebuka Ijezue"], "title": "Advanced Brain Tumor Segmentation Using EMCAD: Efficient Multi-scale Convolutional Attention Decoding", "comment": null, "summary": "Brain tumor segmentation is a critical pre-processing step in the medical\nimage analysis pipeline that involves precise delineation of tumor regions from\nhealthy brain tissue in medical imaging data, particularly MRI scans. An\nefficient and effective decoding mechanism is crucial in brain tumor\nsegmentation especially in scenarios with limited computational resources.\nHowever these decoding mechanisms usually come with high computational costs.\nTo address this concern EMCAD a new efficient multi-scale convolutional\nattention decoder designed was utilized to optimize both performance and\ncomputational efficiency for brain tumor segmentation on the BraTs2020 dataset\nconsisting of MRI scans from 369 brain tumor patients. The preliminary result\nobtained by the model achieved a best Dice score of 0.31 and maintained a\nstable mean Dice score of 0.285 plus/minus 0.015 throughout the training\nprocess which is moderate. The initial model maintained consistent performance\nacross the validation set without showing signs of over-fitting.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u7801\u5668EMCAD\uff0c\u7528\u4e8e\u8111\u80bf\u7624\u5206\u5272\uff0c\u65e8\u5728\u4f18\u5316\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\uff0c\u8111\u80bf\u7624\u5206\u5272\u662f\u4e00\u4e2a\u5173\u952e\u7684\u9884\u5904\u7406\u6b65\u9aa4\uff0c\u5c24\u5176\u662f\u5728\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u9ad8\u6548\u7684\u89e3\u7801\u673a\u5236\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u89e3\u7801\u673a\u5236\u901a\u5e38\u8ba1\u7b97\u6210\u672c\u5f88\u9ad8\u3002", "method": "\u5229\u7528\u4e00\u79cd\u65b0\u7684\u9ad8\u6548\u591a\u5c3a\u5ea6\u5377\u79ef\u6ce8\u610f\u529b\u89e3\u7801\u5668EMCAD\uff0c\u5728BraTs2020\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8111\u80bf\u7624\u5206\u5272\u3002", "result": "\u8be5\u6a21\u578b\u83b7\u5f97\u4e860.31\u7684\u6700\u4f73Dice\u8bc4\u5206\uff0c\u5e76\u5728\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u4e860.285 +/- 0.015\u7684\u7a33\u5b9a\u5e73\u5747Dice\u8bc4\u5206\uff0c\u6027\u80fd\u9002\u4e2d\u3002\u521d\u59cb\u6a21\u578b\u5728\u9a8c\u8bc1\u96c6\u4e0a\u4fdd\u6301\u4e86\u4e00\u81f4\u7684\u6027\u80fd\uff0c\u6ca1\u6709\u51fa\u73b0\u8fc7\u62df\u5408\u7684\u8ff9\u8c61\u3002", "conclusion": "EMCAD\u89e3\u7801\u5668\u5728\u8111\u80bf\u7624\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e00\u5b9a\u7684\u6548\u7387\u548c\u7a33\u5b9a\u6027\uff0c\u4f46Dice\u8bc4\u5206\u8868\u660e\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002"}}
{"id": "2509.05679", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.05679", "abs": "https://arxiv.org/abs/2509.05679", "authors": ["Viet Hoang Pham", "Hyo-Sung Ahn"], "title": "Distributed Deep Learning using Stochastic Gradient Staleness", "comment": null, "summary": "Despite the notable success of deep neural networks (DNNs) in solving complex\ntasks, the training process still remains considerable challenges. A primary\nobstacle is the substantial time required for training, particularly as high\nperforming DNNs tend to become increasingly deep (characterized by a larger\nnumber of hidden layers) and require extensive training datasets. To address\nthese challenges, this paper introduces a distributed training method that\nintegrates two prominent strategies for accelerating deep learning: data\nparallelism and fully decoupled parallel backpropagation algorithm. By\nutilizing multiple computational units operating in parallel, the proposed\napproach enhances the amount of training data processed in each iteration while\nmitigating locking issues commonly associated with the backpropagation\nalgorithm. These features collectively contribute to significant improvements\nin training efficiency. The proposed distributed training method is rigorously\nproven to converge to critical points under certain conditions. Its\neffectiveness is further demonstrated through empirical evaluations, wherein an\nDNN is trained to perform classification tasks on the CIFAR-10 dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4ee5\u52a0\u901f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u8017\u65f6\u5f88\u957f\uff0c\u5c24\u5176\u662f\u5728\u7f51\u7edc\u5f88\u6df1\u3001\u6570\u636e\u96c6\u5f88\u5927\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u7ed3\u5408\u4e86\u6570\u636e\u5e76\u884c\u548c\u5b8c\u5168\u89e3\u8026\u7684\u5e76\u884c\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u53ef\u4ee5\u6536\u655b\u5230\u4e34\u754c\u70b9\uff0c\u5e76\u5728CIFAR-10\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6709\u6548\u6027\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2509.05660", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.05660", "abs": "https://arxiv.org/abs/2509.05660", "authors": ["Hong Su"], "title": "Cross-Question Method Reuse in Large Language Models: From Word-Level Prediction to Rational Logical-Layer Reasoning", "comment": null, "summary": "Large language models (LLMs) have been widely applied to assist in finding\nsolutions for diverse questions. Prior work has proposed representing a method\nas a pair of a question and its corresponding solution, enabling method reuse.\nHowever, existing approaches typically require the questions to be highly\nsimilar. In this paper, we extend the scope of method reuse to address\nquestions with low similarity or with hidden similarities that are not\nexplicitly observable. For questions that are similar in a general-specific\nsense (i.e., broader or narrower in scope), we propose to first separate the\nquestion and solution, rather than directly feeding the pair to the LLM. The\nLLM is then guided to adapt the solution to new but related questions, allowing\nit to focus on solution transfer rather than question recognition. Furthermore,\nwe extend this approach to cases where questions only share partial features or\nhidden characteristics. This enables cross-question method reuse beyond\nconventional similarity constraints. Experimental verification shows that our\nscope-extension approach increases the probability of filtering out reusable\nsolutions, thereby improving the effectiveness of cross-question method reuse.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u95ee\u9898\u89e3\u7b54\u4e2d\u65b9\u6cd5\u91cd\u7528\u7684\u8303\u56f4\uff0c\u4ee5\u89e3\u51b3\u76f8\u4f3c\u5ea6\u4f4e\u6216\u5177\u6709\u9690\u85cf\u76f8\u4f3c\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u6c42\u95ee\u9898\u9ad8\u5ea6\u76f8\u4f3c\uff0c\u9650\u5236\u4e86\u65b9\u6cd5\u91cd\u7528\u7684\u8303\u56f4\u3002", "method": "1. \u5c06\u95ee\u9898\u548c\u89e3\u51b3\u65b9\u6848\u5206\u79bb\uff0c\u5f15\u5bfcLLM\u5c06\u89e3\u51b3\u65b9\u6848\u9002\u914d\u5230\u65b0\u7684\u76f8\u5173\u95ee\u9898\uff1b2. \u6269\u5c55\u5230\u95ee\u9898\u4ec5\u5171\u4eab\u90e8\u5206\u7279\u5f81\u6216\u9690\u85cf\u7279\u5f81\u7684\u60c5\u51b5\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u8fc7\u6ee4\u53ef\u91cd\u7528\u89e3\u51b3\u65b9\u6848\u7684\u6982\u7387\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u8de8\u95ee\u9898\u65b9\u6cd5\u91cd\u7528\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5730\u6269\u5c55\u4e86LLM\u65b9\u6cd5\u91cd\u7528\u7684\u8303\u56f4\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u66f4\u5e7f\u6cdb\u7684\u95ee\u9898\u7c7b\u578b\u3002"}}
{"id": "2509.05933", "categories": ["cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.05933", "abs": "https://arxiv.org/abs/2509.05933", "authors": ["Md Hasebul Hasan", "Mahir Labib Dihan", "Mohammed Eunus Ali", "Md Rizwan Parvez"], "title": "MapAgent: A Hierarchical Agent for Geospatial Reasoning with Dynamic Map Tool Integration", "comment": "27 Pages", "summary": "Agentic AI has significantly extended the capabilities of large language\nmodels (LLMs) by enabling complex reasoning and tool use. However, most\nexisting frameworks are tailored to domains such as mathematics, coding, or web\nautomation, and fall short on geospatial tasks that require spatial reasoning,\nmulti-hop planning, and real-time map interaction. To address these challenges,\nwe introduce MapAgent, a hierarchical multi-agent plug-and-play framework with\ncustomized toolsets and agentic scaffolds for map-integrated geospatial\nreasoning. Unlike existing flat agent-based approaches that treat tools\nuniformly-often overwhelming the LLM when handling similar but subtly different\ngeospatial APIs-MapAgent decouples planning from execution. A high-level\nplanner decomposes complex queries into subgoals, which are routed to\nspecialized modules. For tool-heavy modules-such as map-based services-we then\ndesign a dedicated map-tool agent that efficiently orchestrates related APIs\nadaptively in parallel to effectively fetch geospatial data relevant for the\nquery, while simpler modules (e.g., solution generation or answer extraction)\noperate without additional agent overhead. This hierarchical design reduces\ncognitive load, improves tool selection accuracy, and enables precise\ncoordination across similar APIs. We evaluate MapAgent on four diverse\ngeospatial benchmarks-MapEval-Textual, MapEval-API, MapEval-Visual, and\nMapQA-and demonstrate substantial gains over state-of-the-art tool-augmented\nand agentic baselines. We open-source our framwork at\nhttps://github.com/Hasebul/MapAgent.", "AI": {"tldr": "MapAgent\u662f\u4e00\u4e2a\u7528\u4e8e\u5730\u7406\u7a7a\u95f4\u63a8\u7406\u7684\u5206\u5c42\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5b9a\u5236\u5de5\u5177\u96c6\u548c\u4ee3\u7406\u652f\u67b6\u96c6\u6210\u5730\u56fe\uff0c\u89e3\u51b3\u73b0\u6709\u6846\u67b6\u5728\u7a7a\u95f4\u63a8\u7406\u3001\u591a\u8df3\u89c4\u5212\u548c\u5b9e\u65f6\u5730\u56fe\u4ea4\u4e92\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u6846\u67b6\u5728\u5730\u7406\u7a7a\u95f4\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u56e0\u4e3a\u5b83\u4eec\u9700\u8981\u7a7a\u95f4\u63a8\u7406\u3001\u591a\u8df3\u89c4\u5212\u548c\u5b9e\u65f6\u5730\u56fe\u4ea4\u4e92\u3002", "method": "MapAgent\u91c7\u7528\u5206\u5c42\u591a\u4ee3\u7406\u5373\u63d2\u5373\u7528\u6846\u67b6\uff0c\u5c06\u89c4\u5212\u4e0e\u6267\u884c\u5206\u79bb\u3002\u9ad8\u5c42\u89c4\u5212\u5668\u5c06\u590d\u6742\u67e5\u8be2\u5206\u89e3\u4e3a\u5b50\u76ee\u6807\uff0c\u5e76\u5c06\u5176\u8def\u7531\u5230\u4e13\u7528\u6a21\u5757\u3002\u5bf9\u4e8e\u5de5\u5177\u7e41\u91cd\u7684\u6a21\u5757\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4e13\u7528\u7684\u5730\u56fe\u5de5\u5177\u4ee3\u7406\uff0c\u4ee5\u81ea\u9002\u5e94\u5730\u5e76\u884c\u534f\u8c03\u76f8\u5173API\uff0c\u800c\u66f4\u7b80\u5355\u7684\u6a21\u5757\u5219\u65e0\u9700\u989d\u5916\u7684\u4ee3\u7406\u5f00\u9500\u3002", "result": "\u5728\u56db\u4e2a\u4e0d\u540c\u7684\u5730\u7406\u7a7a\u95f4\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMapAgent\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u5de5\u5177\u589e\u5f3a\u548c\u4ee3\u7406\u57fa\u7ebf\u3002", "conclusion": "MapAgent\u901a\u8fc7\u5206\u5c42\u8bbe\u8ba1\u51cf\u5c11\u8ba4\u77e5\u8d1f\u8377\uff0c\u63d0\u9ad8\u5de5\u5177\u9009\u62e9\u51c6\u786e\u6027\uff0c\u5e76\u5b9e\u73b0\u8de8\u7c7b\u4f3cAPI\u7684\u7cbe\u786e\u534f\u8c03\u3002\u8be5\u6846\u67b6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.05441", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05441", "abs": "https://arxiv.org/abs/2509.05441", "authors": ["Tejaswini Medi", "Hsien-Yi Wang", "Arianna Rampini", "Margret Keuper"], "title": "FAVAE-Effective Frequency Aware Latent Tokenizer", "comment": null, "summary": "Latent generative models have shown remarkable progress in high-fidelity\nimage synthesis, typically using a two-stage training process that involves\ncompressing images into latent embeddings via learned tokenizers in the first\nstage. The quality of generation strongly depends on how expressive and\nwell-optimized these latent embeddings are. While various methods have been\nproposed to learn effective latent representations, the reconstructed images\noften lack realism, particularly in textured regions with sharp transitions,\ndue to loss of fine details governed by high frequencies. We conduct a detailed\nfrequency decomposition of existing state-of-the-art (SOTA) latent tokenizers\nand show that conventional objectives inherently prioritize low-frequency\nreconstruction, often at the expense of high-frequency fidelity. Our analysis\nreveals these latent tokenizers exhibit a bias toward low-frequency\ninformation, when jointly optimized, leading to over-smoothed outputs and\nvisual artifacts that diminish perceptual quality. To address this, we propose\na wavelet-based, frequency-aware variational autoencoder (FA-VAE) framework\nthat explicitly decouples the optimization of low- and high-frequency\ncomponents. This decoupling enables improved reconstruction of fine textures\nwhile preserving global structure. Our approach bridges the fidelity gap in\ncurrent latent tokenizers and emphasizes the importance of frequency-aware\noptimization for realistic image representation, with broader implications for\napplications in content creation, neural rendering, and medical imaging.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u5c0f\u6ce2\u7684\u3001\u9891\u7387\u654f\u611f\u7684\u53d8\u5206\u81ea\u7f16\u7801\u5668 (FA-VAE) \u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u73b0\u6709\u6f5c\u5728 tokenizer \u5728\u91cd\u5efa\u56fe\u50cf\u65f6\u7f3a\u4e4f\u771f\u5b9e\u611f\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u5177\u6709\u6e05\u6670\u8fc7\u6e21\u7684\u7eb9\u7406\u533a\u57df\u3002", "motivation": "\u73b0\u6709\u6280\u672f\u5728\u56fe\u50cf\u5408\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u91cd\u5efa\u7684\u56fe\u50cf\u901a\u5e38\u7f3a\u4e4f\u771f\u5b9e\u611f\uff0c\u5c24\u5176\u662f\u5728\u5177\u6709\u6e05\u6670\u8fc7\u6e21\u7684\u7eb9\u7406\u533a\u57df\uff0c\u8fd9\u662f\u7531\u4e8e\u9ad8\u9891\u63a7\u5236\u7684\u7cbe\u7ec6\u7ec6\u8282\u4e22\u5931\u6240\u81f4\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u7684\u3001\u9891\u7387\u654f\u611f\u7684\u53d8\u5206\u81ea\u7f16\u7801\u5668 (FA-VAE) \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u660e\u786e\u5730\u89e3\u8026\u4e86\u4f4e\u9891\u548c\u9ad8\u9891\u5206\u91cf\u7684\u4f18\u5316\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u66f4\u597d\u5730\u91cd\u5efa\u7cbe\u7ec6\u7eb9\u7406\uff0c\u540c\u65f6\u4fdd\u7559\u5168\u5c40\u7ed3\u6784\uff0c\u7f29\u5c0f\u4e86\u5f53\u524d\u6f5c\u5728 tokenizer \u4e2d\u7684\u4fdd\u771f\u5ea6\u5dee\u8ddd\u3002", "conclusion": "\u672c\u6587\u5f3a\u8c03\u4e86\u9891\u7387\u611f\u77e5\u4f18\u5316\u5bf9\u4e8e\u771f\u5b9e\u56fe\u50cf\u8868\u793a\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5bf9\u5185\u5bb9\u521b\u5efa\u3001\u795e\u7ecf\u6e32\u67d3\u548c\u533b\u5b66\u6210\u50cf\u7b49\u5e94\u7528\u5177\u6709\u66f4\u5e7f\u6cdb\u7684\u610f\u4e49\u3002"}}
{"id": "2509.05697", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.05697", "abs": "https://arxiv.org/abs/2509.05697", "authors": ["Iara Cunha", "Marcos Eduardo Valle"], "title": "Morphological Perceptron with Competitive Layer: Training Using Convex-Concave Procedure", "comment": "Submitted to the 4th International Conference on Discrete Geometry\n  and Mathematical Morphology (DGMM 2025)", "summary": "A morphological perceptron is a multilayer feedforward neural network in\nwhich neurons perform elementary operations from mathematical morphology. For\nmulticlass classification tasks, a morphological perceptron with a competitive\nlayer (MPCL) is obtained by integrating a winner-take-all output layer into the\nstandard morphological architecture. The non-differentiability of morphological\noperators renders gradient-based optimization methods unsuitable for training\nsuch networks. Consequently, alternative strategies that do not depend on\ngradient information are commonly adopted. This paper proposes the use of the\nconvex-concave procedure (CCP) for training MPCL networks. The training problem\nis formulated as a difference of convex (DC) functions and solved iteratively\nusing CCP, resulting in a sequence of linear programming subproblems.\nComputational experiments demonstrate the effectiveness of the proposed\ntraining method in addressing classification tasks with MPCL networks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u51f8\u51f9\u8fc7\u7a0b(CCP)\u8bad\u7ec3\u5f62\u6001\u611f\u77e5\u5668\u7684\u65b9\u6cd5\u3002", "motivation": "\u5f62\u6001\u611f\u77e5\u5668\u7684\u5f62\u6001\u7b97\u5b50\u7684\u4e0d\u53ef\u5fae\u6027\u4f7f\u5f97\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u65b9\u6cd5\u4e0d\u9002\u5408\u8bad\u7ec3\u8fd9\u79cd\u7f51\u7edc\uff0c\u56e0\u6b64\u901a\u5e38\u91c7\u7528\u4e0d\u4f9d\u8d56\u4e8e\u68af\u5ea6\u4fe1\u606f\u7684\u66ff\u4ee3\u7b56\u7565\u3002", "method": "\u5c06\u8bad\u7ec3\u95ee\u9898\u8868\u8ff0\u4e3a\u51f8\u51fd\u6570(DC)\u7684\u5dee\uff0c\u5e76\u4f7f\u7528CCP\u8fed\u4ee3\u6c42\u89e3\uff0c\u4ece\u800c\u4ea7\u751f\u4e00\u7cfb\u5217\u7ebf\u6027\u89c4\u5212\u5b50\u95ee\u9898\u3002", "result": "\u8ba1\u7b97\u5b9e\u9a8c\u8868\u660e\u4e86\u6240\u63d0\u51fa\u7684\u8bad\u7ec3\u65b9\u6cd5\u5728\u89e3\u51b3MPCL\u7f51\u7edc\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u51f8\u51f9\u8fc7\u7a0b(CCP)\u8bad\u7ec3\u5f62\u6001\u611f\u77e5\u5668\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2509.05668", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05668", "abs": "https://arxiv.org/abs/2509.05668", "authors": ["Michael Hoffmann", "Jophin John", "Stefan Schweter", "Gokul Ramakrishnan", "Hoi-Fong Mak", "Alice Zhang", "Dmitry Gaynullin", "Nicolay J. Hammer"], "title": "Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian", "comment": "Michael Hoffmann and Jophin John contributed equally to this work", "summary": "We present Llama-GENBA-10B, a trilingual foundation model addressing\nEnglish-centric bias in large language models. Built on Llama 3.1-8B and scaled\nto 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens\n(82B English, 82B German, and 80M Bavarian), balancing resources while\npreventing English dominance. Targeted at the German NLP community, the model\nalso promotes Bavarian as a low-resource language. Development tackled four\nchallenges: (1) curating a multilingual corpus despite Bavarian scarcity, (2)\ncreating a unified tokenizer for English, German, and Bavarian, (3) optimizing\narchitecture and language-ratio hyperparameters for cross-lingual transfer, and\n(4) establishing the first standardized trilingual evaluation suite by\ntranslating German benchmarks into Bavarian. Evaluations show that\nLlama-GENBA-10B achieves strong cross-lingual performance, with the fine-tuned\nvariant surpassing Apertus-8B-2509 and gemma-2-9b in Bavarian and establishing\nitself as the best model in its class for this language, while also\noutperforming EuroLLM in English and matching its results in German. Training\non the Cerebras CS-2 demonstrated efficient large-scale multilingual\npretraining with documented energy use, offering a blueprint for inclusive\nfoundation models that integrate low-resource languages.", "AI": {"tldr": "Llama-GENBA-10B is a trilingual (English, German, Bavarian) language model built on Llama 3.1-8B and scaled to 10B parameters. It aims to address English-centric bias and support low-resource languages like Bavarian.", "motivation": "Addressing English-centric bias in large language models and promoting low-resource languages like Bavarian.", "method": "Continuous pretraining on 164B tokens (balanced across English, German, and Bavarian), unified tokenizer, architecture and language-ratio optimization for cross-lingual transfer, and a standardized trilingual evaluation suite.", "result": "Achieves strong cross-lingual performance, surpassing Apertus-8B-2509 and gemma-2-9b in Bavarian and outperforming EuroLLM in English, while matching its results in German.", "conclusion": "Demonstrates efficient large-scale multilingual pretraining and offers a blueprint for inclusive foundation models that integrate low-resource languages."}}
{"id": "2509.06024", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06024", "abs": "https://arxiv.org/abs/2509.06024", "authors": ["Haoyang He", "Zihua Rong", "Kun Ji", "Chenyang Li", "Qing Huang", "Chong Xia", "Lan Yang", "Honggang Zhang"], "title": "Rethinking Reasoning Quality in Large Language Models through Enhanced Chain-of-Thought via RL", "comment": null, "summary": "Reinforcement learning (RL) has recently become the dominant paradigm for\nstrengthening the reasoning abilities of large language models (LLMs). Yet the\nrule-based reward functions commonly used on mathematical or programming\nbenchmarks assess only answer format and correctness, providing no signal as to\nwhether the induced Chain-of-Thought (CoT) actually improves the answer.\nFurthermore, such task-specific training offers limited control over logical\ndepth and therefore may fail to reveal a model's genuine reasoning capacity. We\npropose Dynamic Reasoning Efficiency Reward (DRER) -- a plug-and-play RL reward\nframework that reshapes both reward and advantage signals. (i) A Reasoning\nQuality Reward assigns fine-grained credit to those reasoning chains that\ndemonstrably raise the likelihood of the correct answer, directly incentivising\nthe trajectories with beneficial CoT tokens. (ii) A Dynamic Length Advantage\ndecays the advantage of responses whose length deviates from a\nvalidation-derived threshold, stabilising training. To facilitate rigorous\nassessment, we also release Logictree, a dynamically constructed deductive\nreasoning dataset that functions both as RL training data and as a\ncomprehensive benchmark. Experiments confirm the effectiveness of DRER: our 7B\nmodel attains GPT-o3-mini level performance on Logictree with 400 trianing\nsteps, while the average confidence of CoT-augmented answers rises by 30%. The\nmodel further exhibits generalisation across diverse logical-reasoning\ndatasets, and the mathematical benchmark AIME24. These results illuminate how\nRL shapes CoT behaviour and chart a practical path toward enhancing\nformal-reasoning skills in large language models. All code and data are\navailable in repository https://github.com/Henryhe09/DRER.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u52a8\u6001\u63a8\u7406\u6548\u7387\u5956\u52b1\uff08DRER\uff09\u6846\u67b6\uff0c\u4ee5\u6539\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u5956\u52b1\u51fd\u6570\u4ec5\u8bc4\u4f30\u7b54\u6848\u683c\u5f0f\u548c\u6b63\u786e\u6027\uff0c\u65e0\u6cd5\u5224\u65ad\u601d\u7ef4\u94fe\uff08CoT\uff09\u662f\u5426\u771f\u6b63\u6539\u8fdb\u4e86\u7b54\u6848\uff0c\u4e14\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u5bf9\u903b\u8f91\u6df1\u5ea6\u63a7\u5236\u6709\u9650\u3002", "method": "DRER\u5305\u542b\u63a8\u7406\u8d28\u91cf\u5956\u52b1\uff08Reasoning Quality Reward\uff09\u548c\u52a8\u6001\u957f\u5ea6\u4f18\u52bf\uff08Dynamic Length Advantage\uff09\u3002\u63a8\u7406\u8d28\u91cf\u5956\u52b1\u5bf9\u90a3\u4e9b\u80fd\u63d0\u9ad8\u6b63\u786e\u7b54\u6848\u53ef\u80fd\u6027\u7684\u63a8\u7406\u94fe\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5956\u52b1\uff0c\u52a8\u6001\u957f\u5ea6\u4f18\u52bf\u4f1a\u8870\u51cf\u504f\u79bb\u9a8c\u8bc1\u96c6\u957f\u5ea6\u9608\u503c\u7684\u54cd\u5e94\u4f18\u52bf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDRER\u6709\u6548\u30027B\u6a21\u578b\u5728Logictree\u4e0a\u8fbe\u5230GPT-o3-mini\u6c34\u5e73\uff0cCoT\u589e\u5f3a\u7b54\u6848\u7684\u5e73\u5747\u7f6e\u4fe1\u5ea6\u63d0\u9ad830%\u3002\u6a21\u578b\u5728\u4e0d\u540c\u7684\u903b\u8f91\u63a8\u7406\u6570\u636e\u96c6\u548cAIME24\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0cRL\u53ef\u4ee5\u5851\u9020CoT\u884c\u4e3a\uff0c\u5e76\u4e3a\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5f62\u5f0f\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2509.05446", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05446", "abs": "https://arxiv.org/abs/2509.05446", "authors": ["Iftekhar Haider Chowdhury", "Zaed Ikbal Syed", "Ahmed Faizul Haque Dhrubo", "Mohammad Abdul Qayum"], "title": "Dynamic Sensitivity Filter Pruning using Multi-Agent Reinforcement Learning For DCNN's", "comment": "This paper includes figures and two tables, and our work outperforms\n  the existing research that has been published in a journal", "summary": "Deep Convolutional Neural Networks have achieved state of the art performance\nacross various computer vision tasks, however their practical deployment is\nlimited by computational and memory overhead. This paper introduces\nDifferential Sensitivity Fusion Pruning, a novel single shot filter pruning\nframework that focuses on evaluating the stability and redundancy of filter\nimportance scores across multiple criteria. Differential Sensitivity Fusion\nPruning computes a differential sensitivity score for each filter by fusing the\ndiscrepancies among gradient based sensitivity, first order Taylor expansion,\nand KL divergence of activation distributions. An exponential scaling mechanism\nis applied to emphasize filters with inconsistent importance across metrics,\nidentifying candidates that are structurally unstable or less critical to the\nmodel performance. Unlike iterative or reinforcement learning based pruning\nstrategies, Differential Sensitivity Fusion Pruning is efficient and\ndeterministic, requiring only a single forward-backward pass for scoring and\npruning. Extensive experiments across varying pruning rates between 50 to 70\npercent demonstrate that Differential Sensitivity Fusion Pruning significantly\nreduces model complexity, achieving over 80 percent Floating point Operations\nPer Seconds reduction while maintaining high accuracy. For instance, at 70\npercent pruning, our approach retains up to 98.23 percent of baseline accuracy,\nsurpassing traditional heuristics in both compression and generalization. The\nproposed method presents an effective solution for scalable and adaptive Deep\nConvolutional Neural Networks compression, paving the way for efficient\ndeployment on edge and mobile platforms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5355\u6b21\u6ee4\u6ce2\u5668\u526a\u679d\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u68af\u5ea6\u654f\u611f\u6027\u3001\u4e00\u9636\u6cf0\u52d2\u5c55\u5f00\u548c\u6fc0\u6d3b\u5206\u5e03\u7684KL\u6563\u5ea6\u4e4b\u95f4\u7684\u5dee\u5f02\u6765\u8ba1\u7b97\u6bcf\u4e2a\u6ee4\u6ce2\u5668\u7684\u5fae\u5206\u654f\u611f\u5ea6\u5206\u6570\u3002", "motivation": "\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u5404\u79cd\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f46\u5b83\u4eec\u7684\u5b9e\u9645\u90e8\u7f72\u53d7\u5230\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u7684\u9650\u5236\u3002", "method": "\u901a\u8fc7\u878d\u5408\u68af\u5ea6\u654f\u611f\u6027\u3001\u4e00\u9636\u6cf0\u52d2\u5c55\u5f00\u548c\u6fc0\u6d3b\u5206\u5e03\u7684KL\u6563\u5ea6\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u4e3a\u6bcf\u4e2a\u6ee4\u6ce2\u5668\u8ba1\u7b97\u4e00\u4e2a\u5fae\u5206\u654f\u611f\u5ea6\u5206\u6570\u3002\u5e94\u7528\u6307\u6570\u7f29\u653e\u673a\u5236\u6765\u5f3a\u8c03\u5728\u4e0d\u540c\u6307\u6807\u4e2d\u91cd\u8981\u6027\u4e0d\u4e00\u81f4\u7684\u6ee4\u6ce2\u5668\u3002", "result": "\u5728 50% \u5230 70% \u7684\u4e0d\u540c\u526a\u679d\u7387\u4e0b\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u5fae\u5206\u654f\u611f\u5ea6\u878d\u5408\u526a\u679d\u663e\u7740\u964d\u4f4e\u4e86\u6a21\u578b\u590d\u6742\u6027\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u8d85\u8fc7 80% \u7684\u6bcf\u79d2\u6d6e\u70b9\u8fd0\u7b97\u6b21\u6570\u7684\u51cf\u5c11\u3002\u4f8b\u5982\uff0c\u5728 70% \u7684\u526a\u679d\u7387\u4e0b\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4fdd\u7559\u4e86\u9ad8\u8fbe 98.23% \u7684\u57fa\u7ebf\u7cbe\u5ea6\uff0c\u5728\u538b\u7f29\u548c\u6cdb\u5316\u65b9\u9762\u90fd\u8d85\u8fc7\u4e86\u4f20\u7edf\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u53ef\u6269\u5c55\u548c\u81ea\u9002\u5e94\u7684\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u538b\u7f29\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u5728\u8fb9\u7f18\u548c\u79fb\u52a8\u5e73\u53f0\u4e0a\u9ad8\u6548\u90e8\u7f72\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.05732", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05732", "abs": "https://arxiv.org/abs/2509.05732", "authors": ["Lenart Treven", "Bhavya Sukhija", "Jonas Rothfuss", "Stelian Coros", "Florian D\u00f6rfler", "Andreas Krause"], "title": "Simulation Priors for Data-Efficient Deep Learning", "comment": null, "summary": "How do we enable AI systems to efficiently learn in the real-world?\nFirst-principles models are widely used to simulate natural systems, but often\nfail to capture real-world complexity due to simplifying assumptions. In\ncontrast, deep learning approaches can estimate complex dynamics with minimal\nassumptions but require large, representative datasets. We propose SimPEL, a\nmethod that efficiently combines first-principles models with data-driven\nlearning by using low-fidelity simulators as priors in Bayesian deep learning.\nThis enables SimPEL to benefit from simulator knowledge in low-data regimes and\nleverage deep learning's flexibility when more data is available, all the while\ncarefully quantifying epistemic uncertainty. We evaluate SimPEL on diverse\nsystems, including biological, agricultural, and robotic domains, showing\nsuperior performance in learning complex dynamics. For decision-making, we\ndemonstrate that SimPEL bridges the sim-to-real gap in model-based\nreinforcement learning. On a high-speed RC car task, SimPEL learns a highly\ndynamic parking maneuver involving drifting with substantially less data than\nstate-of-the-art baselines. These results highlight the potential of SimPEL for\ndata-efficient learning and control in complex real-world environments.", "AI": {"tldr": "SimPEL: Combines first-principles models with Bayesian deep learning for efficient real-world AI learning.", "motivation": "First-principles models fail to capture real-world complexity, while deep learning needs large datasets.", "method": "SimPEL uses low-fidelity simulators as priors in Bayesian deep learning.", "result": "SimPEL outperforms baselines in learning complex dynamics across biological, agricultural, and robotic domains and bridges the sim-to-real gap in reinforcement learning.", "conclusion": "SimPEL enables data-efficient learning and control in complex real-world environments."}}
{"id": "2509.05691", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05691", "abs": "https://arxiv.org/abs/2509.05691", "authors": ["Ningyuan Deng", "Hanyu Duan", "Yixuan Tang", "Yi Yang"], "title": "Revealing the Numeracy Gap: An Empirical Investigation of Text Embedding Models", "comment": null, "summary": "Text embedding models are widely used in natural language processing\napplications. However, their capability is often benchmarked on tasks that do\nnot require understanding nuanced numerical information in text. As a result,\nit remains unclear whether current embedding models can precisely encode\nnumerical content, such as numbers, into embeddings. This question is critical\nbecause embedding models are increasingly applied in domains where numbers\nmatter, such as finance and healthcare. For example, Company X's market share\ngrew by 2\\% should be interpreted very differently from Company X's market\nshare grew by 20\\%, even though both indicate growth in market share. This\nstudy aims to examine whether text embedding models can capture such nuances.\nUsing synthetic data in a financial context, we evaluate 13 widely used text\nembedding models and find that they generally struggle to capture numerical\ndetails accurately. Our further analyses provide deeper insights into embedding\nnumeracy, informing future research to strengthen embedding model-based NLP\nsystems with improved capacity for handling numerical content.", "AI": {"tldr": "\u7814\u7a76\u4e86\u6587\u672c\u5d4c\u5165\u6a21\u578b\u662f\u5426\u80fd\u51c6\u786e\u7f16\u7801\u6570\u5b57\u4fe1\u606f\uff0c\u53d1\u73b0\u5b83\u4eec\u901a\u5e38\u96be\u4ee5\u6355\u6349\u6570\u5b57\u7ec6\u8282\u3002", "motivation": "\u5f53\u524d\u7684\u6587\u672c\u5d4c\u5165\u6a21\u578b\u5728\u9700\u8981\u7406\u89e3\u7ec6\u5fae\u6570\u5b57\u4fe1\u606f\u7684\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u5c1a\u4e0d\u6e05\u695a\uff0c\u8fd9\u5bf9\u4e8e\u91d1\u878d\u548c\u533b\u7597\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u91d1\u878d\u9886\u57df\u7684\u5408\u6210\u6570\u636e\uff0c\u8bc4\u4f30\u4e8613\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u6587\u672c\u5d4c\u5165\u6a21\u578b\u3002", "result": "\u53d1\u73b0\u8fd9\u4e9b\u6a21\u578b\u901a\u5e38\u96be\u4ee5\u51c6\u786e\u6355\u6349\u6570\u5b57\u7ec6\u8282\u3002", "conclusion": "\u8be5\u7814\u7a76\u6df1\u5165\u5206\u6790\u4e86\u5d4c\u5165\u7684\u6570\u503c\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u4fe1\u606f\uff0c\u4ee5\u589e\u5f3a\u57fa\u4e8e\u5d4c\u5165\u6a21\u578b\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7cfb\u7edf\u5904\u7406\u6570\u503c\u5185\u5bb9\u7684\u80fd\u529b\u3002"}}
{"id": "2509.06160", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06160", "abs": "https://arxiv.org/abs/2509.06160", "authors": ["Haozhe Wang", "Haoran Que", "Qixin Xu", "Minghao Liu", "Wangchunshu Zhou", "Jiazhan Feng", "Wanjun Zhong", "Wei Ye", "Tong Yang", "Wenhao Huang", "Ge Zhang", "Fangzhen Lin"], "title": "Reverse-Engineered Reasoning for Open-Ended Generation", "comment": "Preprint", "summary": "While the ``deep reasoning'' paradigm has spurred significant advances in\nverifiable domains like mathematics, its application to open-ended, creative\ngeneration remains a critical challenge. The two dominant methods for\ninstilling reasoning -- reinforcement learning (RL) and instruction\ndistillation -- falter in this area; RL struggles with the absence of clear\nreward signals and high-quality reward models, while distillation is\nprohibitively expensive and capped by the teacher model's capabilities. To\novercome these limitations, we introduce REverse-Engineered Reasoning (REER), a\nnew paradigm that fundamentally shifts the approach. Instead of building a\nreasoning process ``forwards'' through trial-and-error or imitation, REER works\n``backwards'' from known-good solutions to computationally discover the latent,\nstep-by-step deep reasoning process that could have produced them. Using this\nscalable, gradient-free approach, we curate and open-source DeepWriting-20K, a\nlarge-scale dataset of 20,000 deep reasoning trajectories for open-ended tasks.\nOur model, DeepWriter-8B, trained on this data, not only surpasses strong\nopen-source baselines but also achieves performance competitive with, and at\ntimes superior to, leading proprietary models like GPT-4o and Claude 3.5.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u63a8\u7406\u8303\u5f0f\uff0c\u901a\u8fc7\u9006\u5411\u5de5\u7a0b\u4ece\u5df2\u77e5\u89e3\u51b3\u65b9\u6848\u4e2d\u53d1\u73b0\u6f5c\u5728\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u548c\u6307\u4ee4\u84b8\u998f\u65b9\u6cd5\u5728\u5f00\u653e\u5f0f\u3001\u521b\u9020\u6027\u751f\u6210\u4efb\u52a1\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5f3a\u5316\u5b66\u4e60\u7f3a\u4e4f\u660e\u786e\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u800c\u6307\u4ee4\u84b8\u998f\u6210\u672c\u9ad8\u6602\u4e14\u53d7\u9650\u4e8e\u6559\u5e08\u6a21\u578b\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aREER\u7684\u9006\u5411\u5de5\u7a0b\u63a8\u7406\u8303\u5f0f\uff0c\u5b83\u4ece\u5df2\u77e5\u826f\u597d\u7684\u89e3\u51b3\u65b9\u6848\u51fa\u53d1\uff0c\u53cd\u5411\u8ba1\u7b97\u53d1\u73b0\u6f5c\u5728\u7684\u3001\u9010\u6b65\u7684\u6df1\u5ea6\u63a8\u7406\u8fc7\u7a0b\u3002\u540c\u65f6\uff0c\u6784\u5efa\u5e76\u5f00\u6e90\u4e86\u4e00\u4e2a\u540d\u4e3aDeepWriting-20K\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5305\u542b20,000\u4e2a\u5f00\u653e\u5f0f\u4efb\u52a1\u7684\u6df1\u5ea6\u63a8\u7406\u8f68\u8ff9\u3002", "result": "\u8bad\u7ec3\u51fa\u7684DeepWriter-8B\u6a21\u578b\u4e0d\u4ec5\u8d85\u8d8a\u4e86\u5f3a\u5927\u7684\u5f00\u6e90\u57fa\u7ebf\u6a21\u578b\uff0c\u800c\u4e14\u5728\u6027\u80fd\u4e0a\u4e0eGPT-4o\u548cClaude 3.5\u7b49\u9886\u5148\u7684\u4e13\u6709\u6a21\u578b\u7ade\u4e89\uff0c\u751a\u81f3\u5728\u67d0\u4e9b\u65f6\u5019\u4f18\u4e8e\u5b83\u4eec\u3002", "conclusion": "REER\u8303\u5f0f\u548cDeepWriting-20K\u6570\u636e\u96c6\u4e3a\u5f00\u653e\u5f0f\u3001\u521b\u9020\u6027\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6df1\u5ea6\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2509.05483", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05483", "abs": "https://arxiv.org/abs/2509.05483", "authors": ["Jinhao Wang", "Florian Vogl", "Pascal Sch\u00fctz", "Sa\u0161a \u0106ukovi\u0107", "William R. Taylor"], "title": "Veriserum: A dual-plane fluoroscopic dataset with knee implant phantoms for deep learning in medical imaging", "comment": "This work has been accepted at MICCAI 2025", "summary": "Veriserum is an open-source dataset designed to support the training of deep\nlearning registration for dual-plane fluoroscopic analysis. It comprises\napproximately 110,000 X-ray images of 10 knee implant pair combinations (2\nfemur and 5 tibia implants) captured during 1,600 trials, incorporating poses\nassociated with daily activities such as level gait and ramp descent. Each\nimage is annotated with an automatically registered ground-truth pose, while\n200 images include manually registered poses for benchmarking.\n  Key features of Veriserum include dual-plane images and calibration tools.\nThe dataset aims to support the development of applications such as 2D/3D image\nregistration, image segmentation, X-ray distortion correction, and 3D\nreconstruction. Freely accessible, Veriserum aims to advance computer vision\nand medical imaging research by providing a reproducible benchmark for\nalgorithm development and evaluation. The Veriserum dataset used in this study\nis publicly available via\nhttps://movement.ethz.ch/data-repository/veriserum.html, with the data stored\nat ETH Z\\\"urich Research Collections: https://doi.org/10.3929/ethz-b-000701146.", "AI": {"tldr": "Veriserum\u662f\u4e00\u4e2a\u7528\u4e8e\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u7684\u5f00\u6e90\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u53cc\u5e73\u9762\u8367\u5149\u955c\u5206\u6790\uff0c\u5305\u542b11\u4e07\u5f20X\u5149\u56fe\u50cf\uff0c10\u4e2a\u819d\u5173\u8282\u690d\u5165\u7269\u7ec4\u5408\uff0c\u7528\u4e8e\u65e5\u5e38\u6d3b\u52a8\u59ff\u52bf\u3002", "motivation": "\u65e8\u5728\u652f\u63012D/3D\u56fe\u50cf\u914d\u51c6\u3001\u56fe\u50cf\u5206\u5272\u3001X\u5c04\u7ebf\u7578\u53d8\u6821\u6b63\u548c3D\u91cd\u5efa\u7b49\u5e94\u7528\u7684\u53d1\u5c55\uff0c\u901a\u8fc7\u63d0\u4f9b\u53ef\u91cd\u590d\u7684\u57fa\u51c6\u6765\u63a8\u8fdb\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u533b\u5b66\u6210\u50cf\u7814\u7a76\u3002", "method": "\u5305\u542b\u5927\u7ea611\u4e07\u5f20X\u5c04\u7ebf\u56fe\u50cf\uff0c\u8fd9\u4e9b\u56fe\u50cf\u662f\u57281600\u6b21\u8bd5\u9a8c\u4e2d\u6355\u83b7\u7684\uff0c\u5305\u542b\u4e0e\u65e5\u5e38\u6d3b\u52a8\u76f8\u5173\u7684\u59ff\u52bf\uff0c\u4f8b\u5982\u6c34\u5e73\u6b65\u6001\u548c\u659c\u5761\u4e0b\u964d\u3002\u6bcf\u5f20\u56fe\u50cf\u90fd\u7528\u81ea\u52a8\u914d\u51c6\u7684\u5730\u9762\u5b9e\u51b5\u59ff\u52bf\u8fdb\u884c\u6ce8\u91ca\uff0c\u800c200\u5f20\u56fe\u50cf\u5305\u62ec\u624b\u52a8\u914d\u51c6\u7684\u59ff\u52bf\u7528\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5305\u542b\u53cc\u5e73\u9762\u56fe\u50cf\u548c\u6821\u51c6\u5de5\u5177\u7b49\u5173\u952e\u7279\u5f81\u3002\u65e8\u5728\u652f\u6301\u7b97\u6cd5\u5f00\u53d1\u548c\u8bc4\u4f30\u7684\u53ef\u91cd\u590d\u57fa\u51c6\u3002", "conclusion": "Veriserum\u662f\u4e00\u4e2a\u514d\u8d39\u8bbf\u95ee\u7684\u6570\u636e\u96c6\uff0c\u65e8\u5728\u901a\u8fc7\u4e3a\u7b97\u6cd5\u5f00\u53d1\u548c\u8bc4\u4f30\u63d0\u4f9b\u53ef\u91cd\u590d\u7684\u57fa\u51c6\u6765\u63a8\u8fdb\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u533b\u5b66\u6210\u50cf\u7814\u7a76\u3002"}}
{"id": "2509.05735", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05735", "abs": "https://arxiv.org/abs/2509.05735", "authors": ["Jiaqi Chen", "Ji Shi", "Cansu Sancaktar", "Jonas Frey", "Georg Martius"], "title": "Offline vs. Online Learning in Model-based RL: Lessons for Data Collection Strategies", "comment": "Accepted at Reinforcement Learning Conference (RLC 2025); Code\n  available at: https://github.com/swsychen/Offline_vs_Online_in_MBRL", "summary": "Data collection is crucial for learning robust world models in model-based\nreinforcement learning. The most prevalent strategies are to actively collect\ntrajectories by interacting with the environment during online training or\ntraining on offline datasets. At first glance, the nature of learning\ntask-agnostic environment dynamics makes world models a good candidate for\neffective offline training. However, the effects of online vs. offline data on\nworld models and thus on the resulting task performance have not been\nthoroughly studied in the literature. In this work, we investigate both\nparadigms in model-based settings, conducting experiments on 31 different\nenvironments. First, we showcase that online agents outperform their offline\ncounterparts. We identify a key challenge behind performance degradation of\noffline agents: encountering Out-Of-Distribution states at test time. This\nissue arises because, without the self-correction mechanism in online agents,\noffline datasets with limited state space coverage induce a mismatch between\nthe agent's imagination and real rollouts, compromising policy training. We\ndemonstrate that this issue can be mitigated by allowing for additional online\ninteractions in a fixed or adaptive schedule, restoring the performance of\nonline training with limited interaction data. We also showcase that\nincorporating exploration data helps mitigate the performance degradation of\noffline agents. Based on our insights, we recommend adding exploration data\nwhen collecting large datasets, as current efforts predominantly focus on\nexpert data alone.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u4e2d\u5728\u7ebf\u548c\u79bb\u7ebf\u6570\u636e\u5bf9\u4e16\u754c\u6a21\u578b\u7684\u5f71\u54cd\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u7ebfagent\u4f18\u4e8e\u79bb\u7ebfagent\uff0c\u79bb\u7ebfagent\u7684\u6027\u80fd\u4e0b\u964d\u4e3b\u8981\u662f\u7531\u4e8e\u9047\u5230\u5206\u5e03\u5916\u7684\u72b6\u6001\u3002\u53ef\u4ee5\u901a\u8fc7\u5141\u8bb8\u989d\u5916\u7684\u5728\u7ebf\u4ea4\u4e92\u6216\u52a0\u5165\u63a2\u7d22\u6570\u636e\u6765\u7f13\u89e3\u6b64\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u5728\u7ebf\u4e0e\u79bb\u7ebf\u6570\u636e\u5bf9\u4e16\u754c\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u7531\u6b64\u4ea7\u751f\u7684\u4efb\u52a1\u8868\u73b0\u3002", "method": "\u572831\u4e2a\u4e0d\u540c\u7684\u73af\u5883\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5bf9\u6bd4\u5728\u7ebf\u548c\u79bb\u7ebfagent\u7684\u8868\u73b0\u3002", "result": "\u5728\u7ebfagent\u4f18\u4e8e\u79bb\u7ebfagent\u3002\u79bb\u7ebfagent\u7684\u6027\u80fd\u4e0b\u964d\u4e3b\u8981\u662f\u7531\u4e8e\u9047\u5230\u5206\u5e03\u5916\u7684\u72b6\u6001\u3002\u53ef\u4ee5\u901a\u8fc7\u5141\u8bb8\u989d\u5916\u7684\u5728\u7ebf\u4ea4\u4e92\u6216\u52a0\u5165\u63a2\u7d22\u6570\u636e\u6765\u7f13\u89e3\u6b64\u95ee\u9898\u3002", "conclusion": "\u5efa\u8bae\u5728\u6536\u96c6\u5927\u578b\u6570\u636e\u96c6\u65f6\u6dfb\u52a0\u63a2\u7d22\u6570\u636e\uff0c\u56e0\u4e3a\u5f53\u524d\u7684\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728\u4e13\u5bb6\u6570\u636e\u4e0a\u3002"}}
{"id": "2509.05716", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05716", "abs": "https://arxiv.org/abs/2509.05716", "authors": ["Manoj Madushanka Perera", "Adnan Mahmood", "Kasun Eranda Wijethilake", "Fahmida Islam", "Maryam Tahermazandarani", "Quan Z. Sheng"], "title": "A Survey of the State-of-the-Art in Conversational Question Answering Systems", "comment": "42 pages, 12 figures, 4 tables", "summary": "Conversational Question Answering (ConvQA) systems have emerged as a pivotal\narea within Natural Language Processing (NLP) by driving advancements that\nenable machines to engage in dynamic and context-aware conversations. These\ncapabilities are increasingly being applied across various domains, i.e.,\ncustomer support, education, legal, and healthcare where maintaining a coherent\nand relevant conversation is essential. Building on recent advancements, this\nsurvey provides a comprehensive analysis of the state-of-the-art in ConvQA.\nThis survey begins by examining the core components of ConvQA systems, i.e.,\nhistory selection, question understanding, and answer prediction, highlighting\ntheir interplay in ensuring coherence and relevance in multi-turn\nconversations. It further investigates the use of advanced machine learning\ntechniques, including but not limited to, reinforcement learning, contrastive\nlearning, and transfer learning to improve ConvQA accuracy and efficiency. The\npivotal role of large language models, i.e., RoBERTa, GPT-4, Gemini 2.0 Flash,\nMistral 7B, and LLaMA 3, is also explored, thereby showcasing their impact\nthrough data scalability and architectural advancements. Additionally, this\nsurvey presents a comprehensive analysis of key ConvQA datasets and concludes\nby outlining open research directions. Overall, this work offers a\ncomprehensive overview of the ConvQA landscape and provides valuable insights\nto guide future advancements in the field.", "AI": {"tldr": "\u672c\u6587\u5168\u9762\u6982\u8ff0\u4e86\u4f1a\u8bdd\u5f0f\u95ee\u7b54 (ConvQA) \u9886\u57df\u7684\u73b0\u72b6\uff0c\u4e3a\u672a\u6765\u53d1\u5c55\u65b9\u5411\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\u3002", "motivation": "\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406 (NLP) \u9886\u57df\uff0c\u4f1a\u8bdd\u5f0f\u95ee\u7b54 (ConvQA) \u7cfb\u7edf\u5df2\u6210\u4e3a\u4e00\u4e2a\u5173\u952e\u9886\u57df\uff0c\u5b83\u63a8\u52a8\u4e86\u673a\u5668\u8fdb\u884c\u52a8\u6001\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5bf9\u8bdd\u3002", "method": "\u672c\u6587\u8003\u5bdf\u4e86 ConvQA \u7cfb\u7edf\u7684\u6838\u5fc3\u7ec4\u6210\u90e8\u5206\uff0c\u5373\u5386\u53f2\u9009\u62e9\u3001\u95ee\u9898\u7406\u89e3\u548c\u7b54\u6848\u9884\u6d4b\uff0c\u5f3a\u8c03\u4e86\u5b83\u4eec\u5728\u786e\u4fdd\u591a\u8f6e\u5bf9\u8bdd\u7684\u8fde\u8d2f\u6027\u548c\u76f8\u5173\u6027\u65b9\u9762\u7684\u76f8\u4e92\u4f5c\u7528\u3002\u5b83\u8fd8\u8fdb\u4e00\u6b65\u7814\u7a76\u4e86\u5148\u8fdb\u7684\u673a\u5668\u5b66\u4e60\u6280\u672f\uff08\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e\u5f3a\u5316\u5b66\u4e60\u3001\u5bf9\u6bd4\u5b66\u4e60\u548c\u8fc1\u79fb\u5b66\u4e60\uff09\u7684\u4f7f\u7528\uff0c\u4ee5\u63d0\u9ad8 ConvQA \u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5373 RoBERTa\u3001GPT-4\u3001Gemini 2.0 Flash\u3001Mistral 7B \u548c LLaMA 3\uff09\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4ece\u800c\u901a\u8fc7\u6570\u636e\u53ef\u6269\u5c55\u6027\u548c\u67b6\u6784\u8fdb\u6b65\u5c55\u793a\u4e86\u5b83\u4eec\u7684\u5f71\u54cd\u3002", "result": "\u672c\u6587\u5168\u9762\u5206\u6790\u4e86\u5173\u952e\u7684 ConvQA \u6570\u636e\u96c6\u3002", "conclusion": "\u672c\u6587\u6982\u8ff0\u4e86 ConvQA \u9886\u57df\uff0c\u5e76\u4e3a\u6307\u5bfc\u8be5\u9886\u57df\u672a\u6765\u7684\u53d1\u5c55\u65b9\u5411\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\u3002"}}
{"id": "2509.06174", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06174", "abs": "https://arxiv.org/abs/2509.06174", "authors": ["Wei Han", "Geng Zhan", "Sicheng Yu", "Chenyu Wang", "Bryan Hooi"], "title": "From Long to Short: LLMs Excel at Trimming Own Reasoning Chains", "comment": "21 pages, 5 figures, 7 tables", "summary": "O1/R1 style large reasoning models (LRMs) signal a substantial leap forward\nover conventional instruction-following LLMs. By applying test-time scaling to\ngenerate extended reasoning paths, they establish many SOTAs across a wide\nrange of complex reasoning tasks. However, recent studies show that LRMs are\nprone to suffer from overthinking -- the tendency to overcomplicate simple\nproblems, leading to excessive strategy switching and long, convoluted\nreasoning traces that hinder their interpretability. To mitigate this issue, we\nconduct a systematic investigation into the reasoning efficiency of a broad set\nof LRMs and uncover a common dilemma: the difficulty in balancing multiple\ngeneration objectives such as correctness and brevity. Based on this discovery,\nwe propose a test-time scaling method, EDIT (Efficient Dynamic Inference\nTrimming), which efficiently guides LRMs to identify the shortest correct\nreasoning paths at test time. EDIT employs constraint-guided generation while\njointly tracking length and answer distributions under varying constraints,\nallowing it to select responses that strike an optimal balance between\nconciseness and correctness. Extensive experiments across diverse models and\ndatasets show that EDIT substantially enhance the reasoning efficiency,\nproducing compact yet informative outputs that improve readability and user\nexperience.", "AI": {"tldr": "\u5927\u578b\u63a8\u7406\u6a21\u578b(LRM)\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b58\u5728\u8fc7\u5ea6\u601d\u8003\u7684\u95ee\u9898\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEDIT\u7684\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8LRM\u7684\u63a8\u7406\u6548\u7387\uff0c\u4f7f\u5176\u5728\u7b80\u6d01\u6027\u548c\u6b63\u786e\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "motivation": "LRM\u867d\u7136\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5bb9\u6613\u8fc7\u5ea6\u601d\u8003\uff0c\u5bfc\u81f4\u7b56\u7565\u5207\u6362\u9891\u7e41\uff0c\u63a8\u7406\u8fc7\u7a0b\u5197\u957f\u590d\u6742\uff0c\u53ef\u89e3\u91ca\u6027\u5dee\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aEDIT\uff08Efficient Dynamic Inference Trimming\uff09\u7684\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ea6\u675f\u5f15\u5bfc\u751f\u6210\uff0c\u5e76\u8054\u5408\u8ddf\u8e2a\u4e0d\u540c\u7ea6\u675f\u4e0b\u7684\u957f\u5ea6\u548c\u7b54\u6848\u5206\u5e03\uff0c\u4ece\u800c\u9009\u62e9\u5728\u7b80\u6d01\u6027\u548c\u6b63\u786e\u6027\u4e4b\u95f4\u53d6\u5f97\u6700\u4f73\u5e73\u8861\u7684\u54cd\u5e94\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cEDIT\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u6548\u7387\uff0c\u4ea7\u751f\u4e86\u7d27\u51d1\u800c\u4fe1\u606f\u4e30\u5bcc\u7684\u8f93\u51fa\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u53ef\u8bfb\u6027\u548c\u7528\u6237\u4f53\u9a8c\u3002", "conclusion": "EDIT\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u5f15\u5bfcLRM\u627e\u5230\u6700\u77ed\u7684\u6b63\u786e\u63a8\u7406\u8def\u5f84\uff0c\u63d0\u9ad8\u63a8\u7406\u6548\u7387\uff0c\u5e76\u5728\u7b80\u6d01\u6027\u548c\u6b63\u786e\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002"}}
{"id": "2509.05490", "categories": ["cs.CV", "cs.AI", "68T07", "I.2.10; I.4.8; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.05490", "abs": "https://arxiv.org/abs/2509.05490", "authors": ["Andrzej D. Dobrzycki", "Ana M. Bernardos", "Jos\u00e9 R. Casar"], "title": "An Analysis of Layer-Freezing Strategies for Enhanced Transfer Learning in YOLO Architectures", "comment": "31 pages, 14 figures, 9 tables", "summary": "The You Only Look Once (YOLO) architecture is crucial for real-time object\ndetection. However, deploying it in resource-constrained environments such as\nunmanned aerial vehicles (UAVs) requires efficient transfer learning. Although\nlayer freezing is a common technique, the specific impact of various freezing\nconfigurations on contemporary YOLOv8 and YOLOv10 architectures remains\nunexplored, particularly with regard to the interplay between freezing depth,\ndataset characteristics, and training dynamics. This research addresses this\ngap by presenting a detailed analysis of layer-freezing strategies. We\nsystematically investigate multiple freezing configurations across YOLOv8 and\nYOLOv10 variants using four challenging datasets that represent critical\ninfrastructure monitoring. Our methodology integrates a gradient behavior\nanalysis (L2 norm) and visual explanations (Grad-CAM) to provide deeper\ninsights into training dynamics under different freezing strategies. Our\nresults reveal that there is no universal optimal freezing strategy but,\nrather, one that depends on the properties of the data. For example, freezing\nthe backbone is effective for preserving general-purpose features, while a\nshallower freeze is better suited to handling extreme class imbalance. These\nconfigurations reduce graphics processing unit (GPU) memory consumption by up\nto 28% compared to full fine-tuning and, in some cases, achieve mean average\nprecision (mAP@50) scores that surpass those of full fine-tuning. Gradient\nanalysis corroborates these findings, showing distinct convergence patterns for\nmoderately frozen models. Ultimately, this work provides empirical findings and\npractical guidelines for selecting freezing strategies. It offers a practical,\nevidence-based approach to balanced transfer learning for object detection in\nscenarios with limited resources.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u4f7f\u7528YOLOv8\u548cYOLOv10\u8fdb\u884c\u76ee\u6807\u68c0\u6d4b\u65f6\uff0c\u5c42\u51bb\u7ed3\u7b56\u7565\u5bf9\u8fc1\u79fb\u5b66\u4e60\u7684\u5f71\u54cd\u3002", "motivation": "\u5728\u65e0\u4eba\u673a\u7b49\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72YOLO\u9700\u8981\u9ad8\u6548\u7684\u8fc1\u79fb\u5b66\u4e60\uff0c\u4f46\u4e0d\u540c\u51bb\u7ed3\u914d\u7f6e\u5bf9YOLOv8\u548cYOLOv10\u7684\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u901a\u8fc7\u5728YOLOv8\u548cYOLOv10\u4e0a\u4f7f\u7528\u591a\u79cd\u51bb\u7ed3\u914d\u7f6e\uff0c\u5e76\u5728\u56db\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u5408\u68af\u5ea6\u884c\u4e3a\u5206\u6790\u548c\u53ef\u89c6\u5316\u89e3\u91ca\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u4e0d\u5b58\u5728\u901a\u7528\u7684\u6700\u4f73\u51bb\u7ed3\u7b56\u7565\uff0c\u6700\u4f73\u7b56\u7565\u53d6\u51b3\u4e8e\u6570\u636e\u5c5e\u6027\u3002\u51bb\u7ed3\u4e3b\u5e72\u7f51\u7edc\u53ef\u6709\u6548\u4fdd\u7559\u901a\u7528\u7279\u5f81\uff0c\u800c\u8f83\u6d45\u7684\u51bb\u7ed3\u66f4\u9002\u5408\u5904\u7406\u6781\u7aef\u7c7b\u522b\u4e0d\u5e73\u8861\u3002\u4e0e\u5b8c\u5168\u5fae\u8c03\u76f8\u6bd4\uff0c\u8fd9\u4e9b\u914d\u7f6e\u6700\u591a\u53ef\u51cf\u5c1128%\u7684GPU\u5185\u5b58\u6d88\u8017\uff0c\u5e76\u4e14\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u5b9e\u73b0\u8d85\u8fc7\u5b8c\u5168\u5fae\u8c03\u7684\u5e73\u5747\u7cbe\u5ea6\u5747\u503c\uff08mAP@50\uff09\u5206\u6570\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u9009\u62e9\u51bb\u7ed3\u7b56\u7565\u63d0\u4f9b\u4e86\u7ecf\u9a8c\u6027\u53d1\u73b0\u548c\u5b9e\u8df5\u6307\u5357\uff0c\u4e3a\u8d44\u6e90\u6709\u9650\u573a\u666f\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u3001\u57fa\u4e8e\u8bc1\u636e\u7684\u5e73\u8861\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2509.05766", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.05766", "abs": "https://arxiv.org/abs/2509.05766", "authors": ["Jiaju Miao", "Wei Zhu"], "title": "Ensemble of Precision-Recall Curve (PRC) Classification Trees with Autoencoders", "comment": null, "summary": "Anomaly detection underpins critical applications from network security and\nintrusion detection to fraud prevention, where recognizing aberrant patterns\nrapidly is indispensable. Progress in this area is routinely impeded by two\nobstacles: extreme class imbalance and the curse of dimensionality. To combat\nthe former, we previously introduced Precision-Recall Curve (PRC)\nclassification trees and their ensemble extension, the PRC Random Forest\n(PRC-RF). Building on that foundation, we now propose a hybrid framework that\nintegrates PRC-RF with autoencoders, unsupervised machine learning methods that\nlearn compact latent representations, to confront both challenges\nsimultaneously. Extensive experiments across diverse benchmark datasets\ndemonstrate that the resulting Autoencoder-PRC-RF model achieves superior\naccuracy, scalability, and interpretability relative to prior methods,\naffirming its potential for high-stakes anomaly-detection tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f02\u5e38\u68c0\u6d4b\u6df7\u5408\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86 PRC-RF \u548c\u81ea\u7f16\u7801\u5668\uff0c\u4ee5\u5e94\u5bf9\u6781\u7aef\u7c7b\u4e0d\u5e73\u8861\u548c\u7ef4\u5ea6\u707e\u96be\u3002", "motivation": "\u5f02\u5e38\u68c0\u6d4b\u5728\u7f51\u7edc\u5b89\u5168\u3001\u5165\u4fb5\u68c0\u6d4b\u548c\u6b3a\u8bc8\u9884\u9632\u7b49\u5173\u952e\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u5feb\u901f\u8bc6\u522b\u5f02\u5e38\u6a21\u5f0f\u4e0d\u53ef\u6216\u7f3a\u3002\u4f46\u8be5\u9886\u57df\u7684\u8fdb\u5c55\u901a\u5e38\u53d7\u5230\u6781\u7aef\u7c7b\u4e0d\u5e73\u8861\u548c\u7ef4\u5ea6\u707e\u96be\u4e24\u4e2a\u969c\u788d\u7684\u963b\u788d\u3002", "method": "\u8be5\u65b9\u6cd5\u96c6\u6210\u4e86 PRC-RF \u4e0e\u81ea\u7f16\u7801\u5668\uff0c\u81ea\u7f16\u7801\u5668\u662f\u4e00\u79cd\u5b66\u4e60\u7d27\u51d1\u6f5c\u5728\u8868\u793a\u7684\u65e0\u76d1\u7763\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u540c\u65f6\u5e94\u5bf9\u4e24\u4e2a\u6311\u6218\u3002", "result": "\u5728\u5404\u79cd\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684 Autoencoder-PRC-RF \u6a21\u578b\u76f8\u5bf9\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u7ed3\u679c\u8bc1\u5b9e\u4e86\u8be5\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.05719", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.05719", "abs": "https://arxiv.org/abs/2509.05719", "authors": ["Donya Rooein", "Flor Miriam Plaza-del-Arco", "Debora Nozza", "Dirk Hovy"], "title": "Exploring Subjective Tasks in Farsi: A Survey Analysis and Evaluation of Language Models", "comment": null, "summary": "Given Farsi's speaker base of over 127 million people and the growing\navailability of digital text, including more than 1.3 million articles on\nWikipedia, it is considered a middle-resource language. However, this label\nquickly crumbles when the situation is examined more closely. We focus on three\nsubjective tasks (Sentiment Analysis, Emotion Analysis, and Toxicity Detection)\nand find significant challenges in data availability and quality, despite the\noverall increase in data availability. We review 110 publications on subjective\ntasks in Farsi and observe a lack of publicly available datasets. Furthermore,\nexisting datasets often lack essential demographic factors, such as age and\ngender, that are crucial for accurately modeling subjectivity in language. When\nevaluating prediction models using the few available datasets, the results are\nhighly unstable across both datasets and models. Our findings indicate that the\nvolume of data is insufficient to significantly improve a language's prospects\nin NLP.", "AI": {"tldr": "Farsi is considered a middle-resource language, but this label is misleading due to data availability and quality issues.", "motivation": "The study focuses on the challenges in data availability and quality for subjective tasks in Farsi, despite the increasing amount of digital text.", "method": "The authors reviewed 110 publications on subjective tasks in Farsi and evaluated prediction models using available datasets.", "result": "The review reveals a lack of publicly available datasets and essential demographic factors in existing datasets. The evaluation of prediction models shows highly unstable results.", "conclusion": "The volume of data is insufficient to significantly improve Farsi's prospects in NLP, highlighting the need for better data resources."}}
{"id": "2509.06235", "categories": ["cs.AI", "cs.MA", "I.2.11; I.2.6; I.2.8"], "pdf": "https://arxiv.org/pdf/2509.06235", "abs": "https://arxiv.org/abs/2509.06235", "authors": ["Olivier Schipper", "Yudi Zhang", "Yali Du", "Mykola Pechenizkiy", "Meng Fang"], "title": "PillagerBench: Benchmarking LLM-Based Agents in Competitive Minecraft Team Environments", "comment": "for the source code, see https://github.com/aialt/PillagerBench", "summary": "LLM-based agents have shown promise in various cooperative and strategic\nreasoning tasks, but their effectiveness in competitive multi-agent\nenvironments remains underexplored. To address this gap, we introduce\nPillagerBench, a novel framework for evaluating multi-agent systems in\nreal-time competitive team-vs-team scenarios in Minecraft. It provides an\nextensible API, multi-round testing, and rule-based built-in opponents for\nfair, reproducible comparisons. We also propose TactiCrafter, an LLM-based\nmulti-agent system that facilitates teamwork through human-readable tactics,\nlearns causal dependencies, and adapts to opponent strategies. Our evaluation\ndemonstrates that TactiCrafter outperforms baseline approaches and showcases\nadaptive learning through self-play. Additionally, we analyze its learning\nprocess and strategic evolution over multiple game episodes. To encourage\nfurther research, we have open-sourced PillagerBench, fostering advancements in\nmulti-agent AI for competitive environments.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3a PillagerBench \u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30 Minecraft \u4e2d\u5b9e\u65f6\u7ade\u4e89\u6027\u56e2\u961f\u5bf9\u6297\u573a\u666f\u4e2d\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3002\u540c\u65f6\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a TactiCrafter \u7684\u57fa\u4e8e LLM \u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u901a\u8fc7\u53ef\u8bfb\u7684\u6218\u672f\u4fc3\u8fdb\u56e2\u961f\u5408\u4f5c\uff0c\u5b66\u4e60\u56e0\u679c\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u9002\u5e94\u5bf9\u624b\u7684\u7b56\u7565\u3002", "motivation": "\u76ee\u524d\uff0c\u57fa\u4e8e LLM \u7684\u667a\u80fd\u4f53\u5728\u5404\u79cd\u5408\u4f5c\u548c\u6218\u7565\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5b83\u4eec\u5728\u7ade\u4e89\u6027\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u4ecd\u6709\u5f85\u63a2\u7d22\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63a8\u51fa\u4e86 PillagerBench\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86 TactiCrafter\uff0c\u4e00\u4e2a\u57fa\u4e8e LLM \u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5b83\u901a\u8fc7\u4eba\u7c7b\u53ef\u8bfb\u7684\u6218\u672f\u4fc3\u8fdb\u56e2\u961f\u5408\u4f5c\uff0c\u5b66\u4e60\u56e0\u679c\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u9002\u5e94\u5bf9\u624b\u7684\u7b56\u7565\u3002PillagerBench \u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684 API\u3001\u591a\u8f6e\u6d4b\u8bd5\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u5185\u7f6e\u5bf9\u624b\uff0c\u7528\u4e8e\u516c\u5e73\u3001\u53ef\u91cd\u590d\u7684\u6bd4\u8f83\u3002", "result": "\u6211\u4eec\u7684\u8bc4\u4f30\u8868\u660e\uff0cTactiCrafter \u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u81ea\u6211\u5bf9\u5f08\u5c55\u793a\u4e86\u81ea\u9002\u5e94\u5b66\u4e60\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5206\u6790\u4e86\u5b83\u7684\u5b66\u4e60\u8fc7\u7a0b\u548c\u591a\u4e2a\u6e38\u620f \u044d\u043f\u0438\u0437\u043e\u0434 \u4e2d\u7684\u6218\u7565\u6f14\u53d8\u3002", "conclusion": "\u4e3a\u4e86\u9f13\u52b1\u8fdb\u4e00\u6b65\u7814\u7a76\uff0c\u6211\u4eec\u5df2\u7ecf\u5f00\u6e90\u4e86 PillagerBench\uff0c\u4ee5\u4fc3\u8fdb\u7ade\u4e89\u73af\u5883\u4e2d\u591a\u667a\u80fd\u4f53 AI \u7684\u8fdb\u6b65\u3002"}}
{"id": "2509.05512", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.05512", "abs": "https://arxiv.org/abs/2509.05512", "authors": ["Bryce Grant", "Peng Wang"], "title": "Quaternion Approximation Networks for Enhanced Image Classification and Oriented Object Detection", "comment": "Accepted to IROS 2025", "summary": "This paper introduces Quaternion Approximate Networks (QUAN), a novel deep\nlearning framework that leverages quaternion algebra for rotation equivariant\nimage classification and object detection. Unlike conventional quaternion\nneural networks attempting to operate entirely in the quaternion domain, QUAN\napproximates quaternion convolution through Hamilton product decomposition\nusing real-valued operations. This approach preserves geometric properties\nwhile enabling efficient implementation with custom CUDA kernels. We introduce\nIndependent Quaternion Batch Normalization (IQBN) for training stability and\nextend quaternion operations to spatial attention mechanisms. QUAN is evaluated\non image classification (CIFAR-10/100, ImageNet), object detection (COCO,\nDOTA), and robotic perception tasks. In classification tasks, QUAN achieves\nhigher accuracy with fewer parameters and faster convergence compared to\nexisting convolution and quaternion-based models. For objection detection, QUAN\ndemonstrates improved parameter efficiency and rotation handling over standard\nConvolutional Neural Networks (CNNs) while establishing the SOTA for quaternion\nCNNs in this downstream task. These results highlight its potential for\ndeployment in resource-constrained robotic systems requiring rotation-aware\nperception and application in other domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u56db\u5143\u6570\u4ee3\u6570\u8fdb\u884c\u65cb\u8f6c\u7b49\u53d8\u56fe\u50cf\u5206\u7c7b\u548c\u76ee\u6807\u68c0\u6d4b\u3002", "motivation": "\u4f20\u7edf\u56db\u5143\u6570\u795e\u7ecf\u7f51\u7edc\u8bd5\u56fe\u5b8c\u5168\u5728\u56db\u5143\u6570\u57df\u4e2d\u64cd\u4f5c\uff0c\u4f46\u8ba1\u7b97\u6548\u7387\u4e0d\u9ad8\u3002", "method": "\u901a\u8fc7\u4f7f\u7528\u5b9e\u503c\u8fd0\u7b97\u7684\u6c49\u5bc6\u5c14\u987f\u79ef\u5206\u89e3\u6765\u8fd1\u4f3c\u56db\u5143\u6570\u5377\u79ef\uff0c\u5e76\u5f15\u5165\u72ec\u7acb\u56db\u5143\u6570\u6279\u5f52\u4e00\u5316\uff08IQBN\uff09\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u5e76\u5c06\u56db\u5143\u6570\u8fd0\u7b97\u6269\u5c55\u5230\u7a7a\u95f4\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cQUAN\u4ee5\u66f4\u5c11\u7684\u53c2\u6570\u548c\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u7cbe\u5ea6\u3002\u5bf9\u4e8e\u76ee\u6807\u68c0\u6d4b\uff0cQUAN \u6bd4\u6807\u51c6 CNN \u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u53c2\u6570\u6548\u7387\u548c\u65cb\u8f6c\u5904\u7406\u80fd\u529b\u3002", "conclusion": "QUAN \u5728\u8d44\u6e90\u53d7\u9650\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u5177\u6709\u90e8\u7f72\u6f5c\u529b\uff0c\u9700\u8981\u5728\u5176\u4ed6\u9886\u57df\u8fdb\u884c\u5177\u6709\u65cb\u8f6c\u611f\u77e5\u80fd\u529b\u7684\u611f\u77e5\u548c\u5e94\u7528\u3002"}}
{"id": "2509.05768", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05768", "abs": "https://arxiv.org/abs/2509.05768", "authors": ["Chen Shao", "Yue Wang", "Zhenyi Zhu", "Zhanbo Huang", "Sebastian P\u00fctz", "Benjamin Sch\u00e4fer", "Tobais K\u00e4fer", "Michael F\u00e4rber"], "title": "Real-E: A Foundation Benchmark for Advancing Robust and Generalizable Electricity Forecasting", "comment": "4 pages, CIKM 2025", "summary": "Energy forecasting is vital for grid reliability and operational efficiency.\nAlthough recent advances in time series forecasting have led to progress,\nexisting benchmarks remain limited in spatial and temporal scope and lack\nmulti-energy features. This raises concerns about their reliability and\napplicability in real-world deployment. To address this, we present the Real-E\ndataset, covering over 74 power stations across 30+ European countries over a\n10-year span with rich metadata. Using Real- E, we conduct an extensive data\nanalysis and benchmark over 20 baselines across various model types. We\nintroduce a new metric to quantify shifts in correlation structures and show\nthat existing methods struggle on our dataset, which exhibits more complex and\nnon-stationary correlation dynamics. Our findings highlight key limitations of\ncurrent methods and offer a strong empirical basis for building more robust\nforecasting models", "AI": {"tldr": "\u73b0\u6709\u7684\u80fd\u6e90\u9884\u6d4b\u57fa\u51c6\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u8303\u56f4\u4e0a\u90fd\u6709\u9650\uff0c\u5e76\u4e14\u7f3a\u4e4f\u591a\u80fd\u6e90\u7279\u5f81\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 Real-E \u6570\u636e\u96c6\uff0c\u5b83\u6db5\u76d6\u4e86 30 \u591a\u4e2a\u6b27\u6d32\u56fd\u5bb6/\u5730\u533a\u7684 74 \u591a\u4e2a\u53d1\u7535\u7ad9\uff0c\u65f6\u95f4\u8de8\u5ea6\u4e3a 10 \u5e74\uff0c\u5e76\u5177\u6709\u4e30\u5bcc\u7684\u5143\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u8303\u56f4\u4e0a\u4ecd\u7136\u6709\u9650\uff0c\u5e76\u4e14\u7f3a\u4e4f\u591a\u80fd\u6e90\u7279\u5f81\uff0c\u8fd9\u5f15\u8d77\u4e86\u4eba\u4eec\u5bf9\u5176\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u53ef\u9760\u6027\u548c\u9002\u7528\u6027\u7684\u62c5\u5fe7\u3002", "method": "\u6211\u4eec\u5229\u7528 Real-E \u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u6570\u636e\u5206\u6790\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u4e86\u5404\u79cd\u6a21\u578b\u7c7b\u578b\u7684 20 \u591a\u4e2a\u57fa\u7ebf\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u6307\u6807\u6765\u91cf\u5316\u76f8\u5173\u7ed3\u6784\u7684\u53d8\u5316\u3002", "result": "\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u6211\u4eec\u7684\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u8be5\u6570\u636e\u96c6\u8868\u73b0\u51fa\u66f4\u590d\u6742\u548c\u975e\u5e73\u7a33\u7684\u76f8\u5173\u52a8\u6001\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5173\u952e\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u6784\u5efa\u66f4\u5f3a\u5927\u7684\u9884\u6d4b\u6a21\u578b\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u7ecf\u9a8c\u57fa\u7840\u3002"}}
{"id": "2509.05729", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.05729", "abs": "https://arxiv.org/abs/2509.05729", "authors": ["Charles M. Varmantchaonala", "Niclas G\u00d6tting", "Nils-Erik Sch\u00dctte", "Jean Louis E. K. Fendji", "Christopher Gies"], "title": "QCSE: A Pretrained Quantum Context-Sensitive Word Embedding for Natural Language Processing", "comment": null, "summary": "Quantum Natural Language Processing (QNLP) offers a novel approach to\nencoding and understanding the complexity of natural languages through the\npower of quantum computation. This paper presents a pretrained quantum\ncontext-sensitive embedding model, called QCSE, that captures context-sensitive\nword embeddings, leveraging the unique properties of quantum systems to learn\ncontextual relationships in languages. The model introduces quantum-native\ncontext learning, enabling the utilization of quantum computers for linguistic\ntasks. Central to the proposed approach are innovative context matrix\ncomputation methods, designed to create unique, representations of words based\non their surrounding linguistic context. Five distinct methods are proposed and\ntested for computing the context matrices, incorporating techniques such as\nexponential decay, sinusoidal modulation, phase shifts, and hash-based\ntransformations. These methods ensure that the quantum embeddings retain\ncontext sensitivity, thereby making them suitable for downstream language tasks\nwhere the expressibility and properties of quantum systems are valuable\nresources. To evaluate the effectiveness of the model and the associated\ncontext matrix methods, evaluations are conducted on both a Fulani corpus, a\nlow-resource African language, dataset of small size and an English corpus of\nslightly larger size. The results demonstrate that QCSE not only captures\ncontext sensitivity but also leverages the expressibility of quantum systems\nfor representing rich, context-aware language information. The use of Fulani\nfurther highlights the potential of QNLP to mitigate the problem of lack of\ndata for this category of languages. This work underscores the power of quantum\ncomputation in natural language processing (NLP) and opens new avenues for\napplying QNLP to real-world linguistic challenges across various tasks and\ndomains.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aQCSE\u7684\u9884\u8bad\u7ec3\u91cf\u5b50\u4e0a\u4e0b\u6587\u654f\u611f\u5d4c\u5165\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5229\u7528\u91cf\u5b50\u7cfb\u7edf\u7684\u72ec\u7279\u6027\u8d28\u6765\u5b66\u4e60\u8bed\u8a00\u4e2d\u7684\u4e0a\u4e0b\u6587\u5173\u7cfb\u3002", "motivation": "\u5229\u7528\u91cf\u5b50\u8ba1\u7b97\u7684\u80fd\u529b\uff0c\u4e3a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u7f16\u7801\u548c\u7406\u89e3\u81ea\u7136\u8bed\u8a00\u7684\u590d\u6742\u6027\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e94\u79cd\u4e0d\u540c\u7684\u65b9\u6cd5\u6765\u8ba1\u7b97\u4e0a\u4e0b\u6587\u77e9\u9635\uff0c\u5305\u62ec\u6307\u6570\u8870\u51cf\u3001\u6b63\u5f26\u8c03\u5236\u3001\u76f8\u79fb\u548c\u57fa\u4e8e\u54c8\u5e0c\u7684\u8f6c\u6362\u3002", "result": "\u5728Fulani\u8bed\u6599\u5e93\u548c\u82f1\u8bed\u8bed\u6599\u5e93\u4e0a\u7684\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0cQCSE\u4e0d\u4ec5\u80fd\u6355\u83b7\u4e0a\u4e0b\u6587\u654f\u611f\u6027\uff0c\u8fd8\u80fd\u5229\u7528\u91cf\u5b50\u7cfb\u7edf\u7684\u8868\u8fbe\u6027\u6765\u8868\u793a\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u8bed\u8a00\u4fe1\u606f\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5f3a\u8c03\u4e86\u91cf\u5b50\u8ba1\u7b97\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u5f3a\u5927\u529f\u80fd\uff0c\u5e76\u4e3a\u5c06QNLP\u5e94\u7528\u4e8e\u5404\u79cd\u4efb\u52a1\u548c\u9886\u57df\u7684\u5b9e\u9645\u8bed\u8a00\u6311\u6218\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2509.06239", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06239", "abs": "https://arxiv.org/abs/2509.06239", "authors": ["Manvi Jha", "Jiaxin Wan", "Deming Chen"], "title": "Proof2Silicon: Prompt Repair for Verified Code and Hardware Generation via Reinforcement Learning", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nautomated code generation but frequently produce code that fails formal\nverification, an essential requirement for hardware and safety-critical\ndomains. To overcome this fundamental limitation, we previously proposed\nPREFACE, a model-agnostic framework based on reinforcement learning (RL) that\niteratively repairs the prompts provided to frozen LLMs, systematically\nsteering them toward generating formally verifiable Dafny code without costly\nfine-tuning. This work presents Proof2Silicon, a novel end-to-end synthesis\nframework that embeds the previously proposed PREFACE flow to enable the\ngeneration of correctness-by-construction hardware directly from natural\nlanguage specifications. Proof2Silicon operates by: (1) leveraging PREFACE's\nverifier-driven RL agent to optimize prompt generation iteratively, ensuring\nDafny code correctness; (2) automatically translating verified Dafny programs\ninto synthesizable high-level C using Dafny's Python backend and PyLog; and (3)\nemploying Vivado HLS to produce RTL implementations. Evaluated rigorously on a\nchallenging 100-task benchmark, PREFACE's RL-guided prompt optimization\nconsistently improved Dafny verification success rates across diverse LLMs by\nup to 21%. Crucially, Proof2Silicon achieved an end-to-end hardware synthesis\nsuccess rate of up to 72%, generating RTL designs through Vivado HLS synthesis\nflows. These results demonstrate a robust, scalable, and automated pipeline for\nLLM-driven, formally verified hardware synthesis, bridging natural-language\nspecification and silicon realization.", "AI": {"tldr": "Proof2Silicon\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86PREFACE\u6d41\u7a0b\uff0c\u4ee5\u76f4\u63a5\u4ece\u81ea\u7136\u8bed\u8a00\u89c4\u8303\u751f\u6210\u6b63\u786e\u6784\u5efa\u7684\u786c\u4ef6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u80fd\u529b\uff0c\u4f46\u7ecf\u5e38\u751f\u6210\u672a\u80fd\u901a\u8fc7\u5f62\u5f0f\u9a8c\u8bc1\u7684\u4ee3\u7801\uff0c\u800c\u5f62\u5f0f\u9a8c\u8bc1\u5bf9\u4e8e\u786c\u4ef6\u548c\u5b89\u5168\u5173\u952e\u9886\u57df\u81f3\u5173\u91cd\u8981\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e2a\u6839\u672c\u9650\u5236\u3002", "method": "Proof2Silicon\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u8fd0\u884c\uff1a\uff081\uff09\u5229\u7528PREFACE\u7684\u9a8c\u8bc1\u5668\u9a71\u52a8\u7684RL\u4ee3\u7406\u6765\u8fed\u4ee3\u4f18\u5316prompt\u751f\u6210\uff0c\u786e\u4fddDafny\u4ee3\u7801\u7684\u6b63\u786e\u6027\uff1b\uff082\uff09\u4f7f\u7528Dafny\u7684Python\u540e\u7aef\u548cPyLog\u81ea\u52a8\u5c06\u9a8c\u8bc1\u7684Dafny\u7a0b\u5e8f\u7ffb\u8bd1\u6210\u53ef\u7efc\u5408\u7684\u9ad8\u7ea7C\uff1b\uff083\uff09\u4f7f\u7528Vivado HLS\u751f\u6210RTL\u5b9e\u73b0\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684100\u4efb\u52a1\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e25\u683c\u8bc4\u4f30\uff0cPREFACE\u7684RL\u5f15\u5bfc\u7684prompt\u4f18\u5316\u59cb\u7ec8\u5c06\u5404\u79cdLLM\u7684Dafny\u9a8c\u8bc1\u6210\u529f\u7387\u63d0\u9ad8\u4e86\u9ad8\u8fbe21%\u3002\u91cd\u8981\u7684\u662f\uff0cProof2Silicon\u5b9e\u73b0\u4e86\u9ad8\u8fbe72%\u7684\u7aef\u5230\u7aef\u786c\u4ef6\u7efc\u5408\u6210\u529f\u7387\uff0c\u901a\u8fc7Vivado HLS\u7efc\u5408\u6d41\u7a0b\u751f\u6210RTL\u8bbe\u8ba1\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u5c55\u793a\u4e86\u4e00\u4e2a\u7a33\u5065\u3001\u53ef\u6269\u5c55\u4e14\u81ea\u52a8\u5316\u7684LLM\u9a71\u52a8\u7684\u5f62\u5f0f\u9a8c\u8bc1\u786c\u4ef6\u7efc\u5408pipeline\uff0c\u5f25\u5408\u4e86\u81ea\u7136\u8bed\u8a00\u89c4\u8303\u548c\u7845\u5b9e\u73b0\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2509.05513", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.05513", "abs": "https://arxiv.org/abs/2509.05513", "authors": ["Ahad Jawaid", "Yu Xiang"], "title": "OpenEgo: A Large-Scale Multimodal Egocentric Dataset for Dexterous Manipulation", "comment": "4 pages, 1 figure", "summary": "Egocentric human videos provide scalable demonstrations for imitation\nlearning, but existing corpora often lack either fine-grained, temporally\nlocalized action descriptions or dexterous hand annotations. We introduce\nOpenEgo, a multimodal egocentric manipulation dataset with standardized\nhand-pose annotations and intention-aligned action primitives. OpenEgo totals\n1107 hours across six public datasets, covering 290 manipulation tasks in 600+\nenvironments. We unify hand-pose layouts and provide descriptive, timestamped\naction primitives. To validate its utility, we train language-conditioned\nimitation-learning policies to predict dexterous hand trajectories. OpenEgo is\ndesigned to lower the barrier to learning dexterous manipulation from\negocentric video and to support reproducible research in vision-language-action\nlearning. All resources and instructions will be released at\nwww.openegocentric.com.", "AI": {"tldr": "OpenEgo\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u81ea\u4e2d\u5fc3\u64cd\u4f5c\u6570\u636e\u96c6\uff0c\u5305\u542b\u6807\u51c6\u5316\u7684\u624b\u90e8\u59ff\u52bf\u6ce8\u91ca\u548c\u4e0e\u610f\u56fe\u5bf9\u9f50\u7684\u52a8\u4f5c\u539f\u8bed\uff0c\u65e8\u5728\u964d\u4f4e\u4ece\u81ea\u4e2d\u5fc3\u89c6\u9891\u4e2d\u5b66\u4e60\u7075\u5de7\u64cd\u4f5c\u7684\u95e8\u69db\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u4e2d\u5fc3\u4eba\u4f53\u89c6\u9891\u8bed\u6599\u5e93\u901a\u5e38\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u7684\u3001\u65f6\u95f4\u5c40\u90e8\u5316\u7684\u52a8\u4f5c\u63cf\u8ff0\u6216\u7075\u5de7\u7684\u624b\u90e8\u6ce8\u91ca\u3002", "method": "OpenEgo\u7edf\u4e00\u4e86\u624b\u90e8\u59ff\u52bf\u5e03\u5c40\uff0c\u5e76\u63d0\u4f9b\u63cf\u8ff0\u6027\u7684\u3001\u5e26\u6709\u65f6\u95f4\u6233\u7684\u52a8\u4f5c\u539f\u8bed\u3002\u4f7f\u7528\u8bed\u8a00\u6761\u4ef6\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u6765\u9884\u6d4b\u7075\u5de7\u7684\u624b\u90e8\u8f68\u8ff9\u3002", "result": "OpenEgo\u603b\u8ba11107\u5c0f\u65f6\uff0c\u8de8\u8d8a\u516d\u4e2a\u516c\u5171\u6570\u636e\u96c6\uff0c\u6db5\u76d6600\u591a\u4e2a\u73af\u5883\u4e2d\u7684290\u4e2a\u64cd\u4f5c\u4efb\u52a1\u3002", "conclusion": "OpenEgo\u65e8\u5728\u964d\u4f4e\u4ece\u81ea\u4e2d\u5fc3\u89c6\u9891\u4e2d\u5b66\u4e60\u7075\u5de7\u64cd\u4f5c\u7684\u95e8\u69db\uff0c\u5e76\u652f\u6301\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u5b66\u4e60\u4e2d\u7684\u53ef\u91cd\u590d\u7814\u7a76\u3002"}}
{"id": "2509.05778", "categories": ["cs.LG", "cs.AI", "stat.ML", "I.2"], "pdf": "https://arxiv.org/pdf/2509.05778", "abs": "https://arxiv.org/abs/2509.05778", "authors": ["Arantxa Urrea-Casta\u00f1o", "Nicol\u00e1s Segura-Kunsagi", "Juan Luis Su\u00e1rez-D\u00edaz", "Rosana Montes", "Francisco Herrera"], "title": "DCV-ROOD Evaluation Framework: Dual Cross-Validation for Robust Out-of-Distribution Detection", "comment": "20 pages and appendix", "summary": "Out-of-distribution (OOD) detection plays a key role in enhancing the\nrobustness of artificial intelligence systems by identifying inputs that differ\nsignificantly from the training distribution, thereby preventing unreliable\npredictions and enabling appropriate fallback mechanisms. Developing reliable\nOOD detection methods is a significant challenge, and rigorous evaluation of\nthese techniques is essential for ensuring their effectiveness, as it allows\nresearchers to assess their performance under diverse conditions and to\nidentify potential limitations or failure modes. Cross-validation (CV) has\nproven to be a highly effective tool for providing a reasonable estimate of the\nperformance of a learning algorithm. Although OOD scenarios exhibit particular\ncharacteristics, an appropriate adaptation of CV can lead to a suitable\nevaluation framework for this setting. This work proposes a dual CV framework\nfor robust evaluation of OOD detection models, aimed at improving the\nreliability of their assessment. The proposed evaluation framework aims to\neffectively integrate in-distribution (ID) and OOD data while accounting for\ntheir differing characteristics. To achieve this, ID data are partitioned using\na conventional approach, whereas OOD data are divided by grouping samples based\non their classes. Furthermore, we analyze the context of data with class\nhierarchy to propose a data splitting that considers the entire class hierarchy\nto obtain fair ID-OOD partitions to apply the proposed evaluation framework.\nThis framework is called Dual Cross-Validation for Robust Out-of-Distribution\nDetection (DCV-ROOD). To test the validity of the evaluation framework, we\nselected a set of state-of-the-art OOD detection methods, both with and without\noutlier exposure. The results show that the method achieves very fast\nconvergence to the true performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684OOD\u68c0\u6d4b\u6a21\u578b\u8bc4\u4f30\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u53cc\u91cd\u4ea4\u53c9\u9a8c\u8bc1\u6765\u66f4\u53ef\u9760\u5730\u8bc4\u4f30OOD\u68c0\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u5f00\u53d1\u53ef\u9760\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u5e76\u4e14\u5bf9\u8fd9\u4e9b\u6280\u672f\u8fdb\u884c\u4e25\u683c\u8bc4\u4f30\u5bf9\u4e8e\u786e\u4fdd\u5176\u6709\u6548\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8be5\u6846\u67b6\u6709\u6548\u6574\u5408\u4e86\u540c\u5206\u5e03\uff08ID\uff09\u548cOOD\u6570\u636e\uff0c\u540c\u65f6\u8003\u8651\u4e86\u5b83\u4eec\u7684\u4e0d\u540c\u7279\u5f81\u3002ID\u6570\u636e\u4f7f\u7528\u4f20\u7edf\u65b9\u6cd5\u8fdb\u884c\u5212\u5206\uff0c\u800cOOD\u6570\u636e\u5219\u901a\u8fc7\u57fa\u4e8e\u5176\u7c7b\u522b\u5bf9\u6837\u672c\u8fdb\u884c\u5206\u7ec4\u6765\u5212\u5206\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5206\u6790\u4e86\u5177\u6709\u7c7b\u5c42\u6b21\u7ed3\u6784\u7684\u6570\u636e\u7684\u4e0a\u4e0b\u6587\uff0c\u4ee5\u63d0\u51fa\u4e00\u79cd\u8003\u8651\u6574\u4e2a\u7c7b\u5c42\u6b21\u7ed3\u6784\u7684\u6570\u636e\u5206\u5272\uff0c\u4ee5\u83b7\u5f97\u516c\u5e73\u7684ID-OOD\u5206\u533a\uff0c\u4ee5\u5e94\u7528\u6240\u63d0\u51fa\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u975e\u5e38\u5feb\u901f\u5730\u6536\u655b\u5230\u771f\u5b9e\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u53cc\u91cd\u4ea4\u53c9\u9a8c\u8bc1\u6846\u67b6\uff0c\u7528\u4e8e\u9c81\u68d2\u7684OOD\u68c0\u6d4b\u8bc4\u4f30\uff0c\u65e8\u5728\u63d0\u9ad8\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2509.05741", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.05741", "abs": "https://arxiv.org/abs/2509.05741", "authors": ["Fernando Gabriela Garc\u00eda", "Qiyang Shi", "Zilin Feng"], "title": "Enhancing Factual Accuracy and Citation Generation in LLMs via Multi-Stage Self-Verification", "comment": null, "summary": "This research introduces VeriFact-CoT (Verified Factual Chain-of-Thought), a\nnovel method designed to address the pervasive issues of hallucination and the\nabsence of credible citation sources in Large Language Models (LLMs) when\ngenerating complex, fact-sensitive content. By incorporating a multi-stage\nmechanism of 'fact verification-reflection-citation integration,' VeriFact-CoT\nempowers LLMs to critically self-examine and revise their intermediate\nreasoning steps and final answers. This process significantly enhances the\nobjective accuracy, trustworthiness, and traceability of the generated outputs,\nmaking LLMs more reliable for applications demanding high fidelity such as\nscientific research, news reporting, and legal consultation.", "AI": {"tldr": "VeriFact-CoT\u901a\u8fc7\u591a\u9636\u6bb5\u673a\u5236\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u590d\u6742\u5185\u5bb9\u65f6\u51fa\u73b0\u7684\u5e7b\u89c9\u548c\u7f3a\u4e4f\u53ef\u4fe1\u6765\u6e90\u7684\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u590d\u6742\u3001\u5bf9\u4e8b\u5b9e\u654f\u611f\u7684\u5185\u5bb9\u65f6\uff0c\u666e\u904d\u5b58\u5728\u7684\u5e7b\u89c9\u548c\u7f3a\u4e4f\u53ef\u4fe1\u5f15\u7528\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u591a\u9636\u6bb5\u673a\u5236\uff0c\u5305\u62ec\u201c\u4e8b\u5b9e\u9a8c\u8bc1-\u53cd\u601d-\u5f15\u7528\u6574\u5408\u201d\uff0c\u4f7fLLM\u80fd\u591f\u6279\u5224\u6027\u5730\u81ea\u67e5\u548c\u4fee\u6539\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u548c\u6700\u7ec8\u7b54\u6848\u3002", "result": "\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u8f93\u51fa\u7684\u5ba2\u89c2\u51c6\u786e\u6027\u3001\u53ef\u4fe1\u5ea6\u548c\u53ef\u8ffd\u6eaf\u6027\u3002", "conclusion": "VeriFact-CoT\u4f7fLLM\u5728\u9700\u8981\u9ad8\u4fdd\u771f\u5ea6\u7684\u5e94\u7528\u4e2d\u66f4\u52a0\u53ef\u9760\uff0c\u5982\u79d1\u5b66\u7814\u7a76\u3001\u65b0\u95fb\u62a5\u9053\u548c\u6cd5\u5f8b\u54a8\u8be2\u3002"}}
{"id": "2509.06269", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06269", "abs": "https://arxiv.org/abs/2509.06269", "authors": ["Vishal Raman", "Vijai Aravindh R", "Abhijith Ragav"], "title": "REMI: A Novel Causal Schema Memory Architecture for Personalized Lifestyle Recommendation Agents", "comment": "8 pages, 2 figures, Accepted at the OARS Workshop, KDD 2025, Paper\n  link: https://oars-workshop.github.io/papers/Raman2025.pdf", "summary": "Personalized AI assistants often struggle to incorporate complex personal\ndata and causal knowledge, leading to generic advice that lacks explanatory\npower. We propose REMI, a Causal Schema Memory architecture for a multimodal\nlifestyle agent that integrates a personal causal knowledge graph, a causal\nreasoning engine, and a schema based planning module. The idea is to deliver\nexplainable, personalized recommendations in domains like fashion, personal\nwellness, and lifestyle planning. Our architecture uses a personal causal graph\nof the user's life events and habits, performs goal directed causal traversals\nenriched with external knowledge and hypothetical reasoning, and retrieves\nadaptable plan schemas to generate tailored action plans. A Large Language\nModel orchestrates these components, producing answers with transparent causal\nexplanations. We outline the CSM system design and introduce new evaluation\nmetrics for personalization and explainability, including Personalization\nSalience Score and Causal Reasoning Accuracy, to rigorously assess its\nperformance. Results indicate that CSM based agents can provide more context\naware, user aligned recommendations compared to baseline LLM agents. This work\ndemonstrates a novel approach to memory augmented, causal reasoning in\npersonalized agents, advancing the development of transparent and trustworthy\nAI lifestyle assistants.", "AI": {"tldr": "\u63d0\u51faREMI\uff0c\u4e00\u4e2a\u591a\u6a21\u6001\u751f\u6d3b\u65b9\u5f0f\u4ee3\u7406\u7684\u56e0\u679c\u6a21\u5f0f\u8bb0\u5fc6\u67b6\u6784\uff0c\u96c6\u6210\u4e86\u4e2a\u4eba\u56e0\u679c\u77e5\u8bc6\u56fe\u8c31\u3001\u56e0\u679c\u63a8\u7406\u5f15\u64ce\u548c\u57fa\u4e8e\u6a21\u5f0f\u7684\u89c4\u5212\u6a21\u5757\u3002", "motivation": "\u73b0\u6709\u7684\u4e2a\u6027\u5316AI\u52a9\u624b\u96be\u4ee5\u6574\u5408\u590d\u6742\u7684\u4e2a\u4eba\u6570\u636e\u548c\u56e0\u679c\u77e5\u8bc6\uff0c\u5bfc\u81f4\u5efa\u8bae\u6cdb\u5316\u4e14\u7f3a\u4e4f\u89e3\u91ca\u529b\u3002", "method": "\u4f7f\u7528\u4e2a\u4eba\u56e0\u679c\u56fe\uff0c\u6267\u884c\u4ee5\u76ee\u6807\u4e3a\u5bfc\u5411\u7684\u56e0\u679c\u904d\u5386\uff0c\u5e76\u68c0\u7d22\u9002\u5e94\u6027\u8ba1\u5212\u6a21\u5f0f\u4ee5\u751f\u6210\u5b9a\u5236\u7684\u884c\u52a8\u8ba1\u5212\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\u534f\u8c03\u8fd9\u4e9b\u7ec4\u4ef6\uff0c\u4ea7\u751f\u5177\u6709\u900f\u660e\u56e0\u679c\u89e3\u91ca\u7684\u7b54\u6848\u3002", "result": "\u57fa\u4e8eCSM\u7684\u4ee3\u7406\u53ef\u4ee5\u63d0\u4f9b\u6bd4\u57fa\u7ebfLLM\u4ee3\u7406\u66f4\u7b26\u5408\u4e0a\u4e0b\u6587\u611f\u77e5\u3001\u66f4\u7b26\u5408\u7528\u6237\u9700\u6c42\u7684\u63a8\u8350\u3002", "conclusion": "\u8bc1\u660e\u4e86\u4e00\u79cd\u5728\u4e2a\u6027\u5316\u4ee3\u7406\u4e2d\u8fdb\u884c\u8bb0\u5fc6\u589e\u5f3a\u7684\u56e0\u679c\u63a8\u7406\u7684\u65b0\u65b9\u6cd5\uff0c\u63a8\u8fdb\u4e86\u900f\u660e\u548c\u503c\u5f97\u4fe1\u8d56\u7684AI\u751f\u6d3b\u65b9\u5f0f\u52a9\u624b\u7684\u5f00\u53d1\u3002"}}
{"id": "2509.05515", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05515", "abs": "https://arxiv.org/abs/2509.05515", "authors": ["Sen Wang", "Kunyi Li", "Siyun Liang", "Elena Alegret", "Jing Ma", "Nassir Navab", "Stefano Gasperini"], "title": "Visibility-Aware Language Aggregation for Open-Vocabulary Segmentation in 3D Gaussian Splatting", "comment": null, "summary": "Recently, distilling open-vocabulary language features from 2D images into 3D\nGaussians has attracted significant attention. Although existing methods\nachieve impressive language-based interactions of 3D scenes, we observe two\nfundamental issues: background Gaussians contributing negligibly to a rendered\npixel get the same feature as the dominant foreground ones, and multi-view\ninconsistencies due to view-specific noise in language embeddings. We introduce\nVisibility-Aware Language Aggregation (VALA), a lightweight yet effective\nmethod that computes marginal contributions for each ray and applies a\nvisibility-aware gate to retain only visible Gaussians. Moreover, we propose a\nstreaming weighted geometric median in cosine space to merge noisy multi-view\nfeatures. Our method yields a robust, view-consistent language feature\nembedding in a fast and memory-efficient manner. VALA improves open-vocabulary\nlocalization and segmentation across reference datasets, consistently\nsurpassing existing works.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVALA\u7684\u8f7b\u91cf\u7ea7\u6709\u6548\u65b9\u6cd5\uff0c\u7528\u4e8e\u5c062D\u56fe\u50cf\u4e2d\u7684\u5f00\u653e\u8bcd\u6c47\u8bed\u8a00\u7279\u5f81\u63d0\u70bc\u62103D\u9ad8\u65af\u5206\u5e03\uff0c\u4ece\u800c\u6539\u8fdb\u57fa\u4e8e\u8bed\u8a00\u76843D\u573a\u666f\u4ea4\u4e92\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u57fa\u672c\u95ee\u9898\uff1a\u80cc\u666f\u9ad8\u65af\u5bf9\u6e32\u67d3\u50cf\u7d20\u7684\u8d21\u732e\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8ba1\uff0c\u4f46\u83b7\u5f97\u4e0e\u4e3b\u8981\u524d\u666f\u9ad8\u65af\u76f8\u540c\u7684\u7279\u5f81\uff1b\u7531\u4e8e\u8bed\u8a00\u5d4c\u5165\u4e2d\u5b58\u5728\u7279\u5b9a\u4e8e\u89c6\u89d2\u7684\u566a\u58f0\uff0c\u5bfc\u81f4\u591a\u89c6\u89d2\u4e0d\u4e00\u81f4\u3002", "method": "\u63d0\u51fa\u4e86\u53ef\u89c6\u6027\u611f\u77e5\u8bed\u8a00\u805a\u5408\uff08VALA\uff09\uff0c\u8be5\u65b9\u6cd5\u8ba1\u7b97\u6bcf\u6761\u5c04\u7ebf\u7684\u8fb9\u9645\u8d21\u732e\uff0c\u5e76\u5e94\u7528\u53ef\u89c6\u6027\u611f\u77e5\u95e8\u6765\u4ec5\u4fdd\u7559\u53ef\u89c1\u7684\u9ad8\u65af\u5206\u5e03\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4f59\u5f26\u7a7a\u95f4\u4e2d\u7684\u6d41\u5f0f\u52a0\u6743\u51e0\u4f55\u4e2d\u4f4d\u6570\uff0c\u4ee5\u5408\u5e76\u566a\u58f0\u591a\u89c6\u89d2\u7279\u5f81\u3002", "result": "VALA\u5728\u53c2\u8003\u6570\u636e\u96c6\u4e0a\u6539\u8fdb\u4e86\u5f00\u653e\u8bcd\u6c47\u5b9a\u4f4d\u548c\u5206\u5272\uff0c\u59cb\u7ec8\u8d85\u8d8a\u73b0\u6709\u5de5\u4f5c\u3002", "conclusion": "VALA\u4ee5\u5feb\u901f\u4e14\u8282\u7701\u5185\u5b58\u7684\u65b9\u5f0f\u751f\u6210\u4e86\u9c81\u68d2\u3001\u89c6\u89d2\u4e00\u81f4\u7684\u8bed\u8a00\u7279\u5f81\u5d4c\u5165\u3002"}}
{"id": "2509.05779", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05779", "abs": "https://arxiv.org/abs/2509.05779", "authors": ["Wei Chen", "Yuqian Wu", "Yuanshao Zhu", "Xixuan Hao", "Shiyu Wang", "Yuxuan Liang"], "title": "Select, then Balance: A Plug-and-Play Framework for Exogenous-Aware Spatio-Temporal Forecasting", "comment": "16 pages, 11 figures", "summary": "Spatio-temporal forecasting aims to predict the future state of dynamic\nsystems and plays an important role in multiple fields. However, existing\nsolutions only focus on modeling using a limited number of observed target\nvariables. In real-world scenarios, exogenous variables can be integrated into\nthe model as additional input features and associated with the target signal to\npromote forecast accuracy. Although promising, this still encounters two\nchallenges: the inconsistent effects of different exogenous variables to the\ntarget system, and the imbalance effects between historical variables and\nfuture variables. To address these challenges, this paper introduces \\model, a\nnovel framework for modeling \\underline{exo}genous variables in\n\\underline{s}patio-\\underline{t}emporal forecasting, which follows a ``select,\nthen balance'' paradigm. Specifically, we first construct a latent space gated\nexpert module, where fused exogenous information is projected into a latent\nspace to dynamically select and recompose salient signals via specialized\nsub-experts. Furthermore, we design a siamese network architecture in which\nrecomposed representations of past and future exogenous variables are fed into\ndual-branch spatio-temporal backbones to capture dynamic patterns. The outputs\nare integrated through a context-aware weighting mechanism to achieve dynamic\nbalance during the modeling process. Extensive experiments on real-world\ndatasets demonstrate the effectiveness, generality, robustness, and efficiency\nof our proposed framework.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a \\model \u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u65f6\u7a7a\u9884\u6d4b\u4e2d\u5bf9\u5916\u751f\u53d8\u91cf\u8fdb\u884c\u5efa\u6a21\uff0c\u9075\u5faa\u201c\u9009\u62e9\uff0c\u7136\u540e\u5e73\u8861\u201d\u7684\u8303\u5f0f\u3002", "motivation": "\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u53ea\u5173\u6ce8\u4f7f\u7528\u6709\u9650\u6570\u91cf\u7684\u89c2\u6d4b\u76ee\u6807\u53d8\u91cf\u8fdb\u884c\u5efa\u6a21\u3002\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u5916\u751f\u53d8\u91cf\u53ef\u4ee5\u4f5c\u4e3a\u989d\u5916\u7684\u8f93\u5165\u7279\u5f81\u6574\u5408\u5230\u6a21\u578b\u4e2d\uff0c\u5e76\u4e0e\u76ee\u6807\u4fe1\u53f7\u76f8\u5173\u8054\uff0c\u4ee5\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002\u7136\u800c\uff0c\u8fd9\u4ecd\u7136\u9762\u4e34\u4e24\u4e2a\u6311\u6218\uff1a\u4e0d\u540c\u7684\u5916\u751f\u53d8\u91cf\u5bf9\u76ee\u6807\u7cfb\u7edf\u7684\u4e0d\u4e00\u81f4\u5f71\u54cd\uff0c\u4ee5\u53ca\u5386\u53f2\u53d8\u91cf\u548c\u672a\u6765\u53d8\u91cf\u4e4b\u95f4\u7684\u4e0d\u5e73\u8861\u5f71\u54cd\u3002", "method": "\u8be5\u6846\u67b6\u9996\u5148\u6784\u5efa\u4e86\u4e00\u4e2a\u6f5c\u5728\u7a7a\u95f4\u95e8\u63a7\u4e13\u5bb6\u6a21\u5757\uff0c\u5176\u4e2d\u878d\u5408\u7684\u5916\u751f\u4fe1\u606f\u88ab\u6295\u5f71\u5230\u4e00\u4e2a\u6f5c\u5728\u7a7a\u95f4\uff0c\u4ee5\u901a\u8fc7\u4e13\u95e8\u7684\u5b50\u4e13\u5bb6\u52a8\u6001\u9009\u62e9\u548c\u91cd\u7ec4\u663e\u8457\u4fe1\u53f7\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a Siamese \u7f51\u7edc\u67b6\u6784\uff0c\u5176\u4e2d\u8fc7\u53bb\u548c\u672a\u6765\u7684\u5916\u751f\u53d8\u91cf\u7684\u91cd\u7ec4\u8868\u793a\u88ab\u8f93\u5165\u5230\u53cc\u5206\u652f\u65f6\u7a7a\u4e3b\u5e72\u7f51\u7edc\u4e2d\uff0c\u4ee5\u6355\u83b7\u52a8\u6001\u6a21\u5f0f\u3002\u8f93\u51fa\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u52a0\u6743\u673a\u5236\u8fdb\u884c\u6574\u5408\uff0c\u4ee5\u5728\u5efa\u6a21\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u52a8\u6001\u5e73\u8861\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u6846\u67b6\u7684\u6709\u6548\u6027\u3001\u901a\u7528\u6027\u3001\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65f6\u7a7a\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u548c\u5e73\u8861\u5916\u751f\u53d8\u91cf\u6765\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002"}}
{"id": "2509.05863", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.05863", "abs": "https://arxiv.org/abs/2509.05863", "authors": ["Luis Felipe Chary", "Miguel Arjona Ramirez"], "title": "LatinX: Aligning a Multilingual TTS Model with Direct Preference Optimization", "comment": null, "summary": "We present LatinX, a multilingual text-to-speech (TTS) model for cascaded\nspeech-to-speech translation that preserves the source speaker's identity\nacross languages. LatinX is a 12-layer decoder-only Transformer trained in\nthree stages: (i) pre-training for text-to-audio mapping, (ii) supervised\nfine-tuning for zero-shot voice cloning, and (iii) alignment with Direct\nPreference Optimization (DPO) using automatically labeled pairs based on Word\nError Rate (WER) and speaker-similarity metrics. Trained on English and Romance\nlanguages with emphasis on Portuguese, LatinX with DPO consistently reduces WER\nand improves objective similarity over the fine-tuned baseline. Human\nevaluations further indicate stronger perceived speaker similarity than a\nstrong baseline (XTTSv2), revealing gaps between objective and subjective\nmeasures. We provide cross-lingual analyses and discuss balanced preference\nsignals and lower-latency architectures as future work.", "AI": {"tldr": "LatinX\u662f\u4e00\u4e2a\u591a\u8bed\u79cdTTS\u6a21\u578b\uff0c\u7528\u4e8e\u7ea7\u8054\u8bed\u97f3\u5230\u8bed\u97f3\u7ffb\u8bd1\uff0c\u53ef\u5728\u4e0d\u540c\u8bed\u8a00\u4e2d\u4fdd\u7559\u6e90\u8bf4\u8bdd\u8005\u7684\u8eab\u4efd\u3002", "motivation": "\u6784\u5efa\u4e00\u4e2a\u80fd\u591f\u8de8\u8bed\u8a00\u4fdd\u6301\u8bf4\u8bdd\u4eba\u97f3\u8272\u7684\u8bed\u97f3\u5408\u6210\u6a21\u578b\u3002", "method": "\u8be5\u6a21\u578b\u662f\u4e00\u4e2a12\u5c42decoder-only Transformer\uff0c\u7ecf\u8fc7\u4e09\u4e2a\u9636\u6bb5\u7684\u8bad\u7ec3\uff1a\u6587\u672c\u5230\u97f3\u9891\u7684\u6620\u5c04\u9884\u8bad\u7ec3\u3001zero-shot\u8bed\u97f3\u514b\u9686\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u4ee5\u53ca\u4f7f\u7528\u57fa\u4e8e\u8bcd\u9519\u7387\uff08WER\uff09\u548c\u8bf4\u8bdd\u4eba\u76f8\u4f3c\u5ea6\u6307\u6807\u81ea\u52a8\u6807\u8bb0\u7684\u914d\u5bf9\u8fdb\u884c\u7684\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u5bf9\u9f50\u3002", "result": "LatinX\u4e0eDPO\u76f8\u6bd4\u4e8e\u5fae\u8c03\u7684\u57fa\u7ebf\uff0c\u6301\u7eed\u964d\u4f4e\u4e86WER\u5e76\u63d0\u9ad8\u4e86\u5ba2\u89c2\u76f8\u4f3c\u5ea6\u3002\u4eba\u5de5\u8bc4\u4f30\u8868\u660e\uff0c\u4e0e\u5f3a\u5927\u7684\u57fa\u7ebf\uff08XTTSv2\uff09\u76f8\u6bd4\uff0c\u611f\u77e5\u5230\u7684\u8bf4\u8bdd\u4eba\u76f8\u4f3c\u5ea6\u66f4\u5f3a\u3002", "conclusion": "LatinX\u6a21\u578b\u5728\u8de8\u8bed\u8a00\u8bed\u97f3\u5408\u6210\u548c\u8bf4\u8bdd\u4eba\u97f3\u8272\u4fdd\u6301\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5ba2\u89c2\u6307\u6807\u548c\u4e3b\u89c2\u611f\u53d7\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\uff0c\u672a\u6765\u5de5\u4f5c\u5c06\u5173\u6ce8\u5e73\u8861\u504f\u597d\u4fe1\u53f7\u548c\u964d\u4f4e\u5ef6\u8fdf\u7684\u67b6\u6784\u3002"}}
{"id": "2509.06278", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06278", "abs": "https://arxiv.org/abs/2509.06278", "authors": ["Chuang Jiang", "Mingyue Cheng", "Xiaoyu Tao", "Qingyang Mao", "Jie Ouyang", "Qi Liu"], "title": "TableMind: An Autonomous Programmatic Agent for Tool-Augmented Table Reasoning", "comment": "Comments: 10 pages, 6 figures. Submitted to WSDM 2026", "summary": "Table reasoning is crucial for leveraging structured data in domains such as\nfinance, healthcare, and scientific research. While large language models\n(LLMs) show promise in multi-step reasoning, purely text-based methods often\nstruggle with the complex numerical computations and fine-grained operations\ninherently required in this task. Tool-integrated reasoning improves\ncomputational accuracy via explicit code execution, yet existing systems\nfrequently rely on rigid patterns, supervised imitation, and lack true\nautonomous adaptability. In this paper, we present TableMind, an LLM-driven\ntable reasoning agent that (i) autonomously performs multi-turn tool\ninvocation, (ii) writes and executes data-analyzing code in a secure sandbox\nenvironment for data analysis and precise numerical reasoning, and (iii)\nexhibits high-level capabilities such as planning and self-reflection to adapt\nstrategies. To realize these capabilities, we adopt a two-stage fine-tuning\nparadigm built on top of a powerful pre-trained language model: supervised\nfine-tuning on high-quality reasoning trajectories to establish effective tool\nusage patterns, followed by reinforcement fine-tuning to optimize\nmulti-objective strategies. In particular, we propose Rank-Aware Policy\nOptimization (RAPO), which increases the update weight of high-quality\ntrajectories when their output probabilities are lower than those of\nlow-quality ones, thereby guiding the model more consistently toward better and\nmore accurate answers. Extensive experiments on several mainstream benchmarks\ndemonstrate that TableMind achieves superior performance compared to\ncompetitive baselines, yielding substantial gains in both reasoning accuracy\nand computational precision.", "AI": {"tldr": "TableMind\u662f\u4e00\u4e2aLLM\u9a71\u52a8\u7684\u8868\u683c\u63a8\u7406Agent\uff0c\u5b83\u81ea\u4e3b\u6267\u884c\u591a\u8f6e\u5de5\u5177\u8c03\u7528\uff0c\u5728\u5b89\u5168\u6c99\u7bb1\u73af\u5883\u4e2d\u7f16\u5199\u548c\u6267\u884c\u6570\u636e\u5206\u6790\u4ee3\u7801\uff0c\u5e76\u5177\u6709\u89c4\u5212\u548c\u81ea\u6211\u53cd\u601d\u7b49\u9ad8\u7ea7\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6587\u672c\u7684\u65b9\u6cd5\u5728\u8868\u683c\u63a8\u7406\u4e2d\u5b58\u5728\u6570\u503c\u8ba1\u7b97\u590d\u6742\u548c\u64cd\u4f5c\u7cbe\u7ec6\u7684\u95ee\u9898\uff0c\u800c\u5de5\u5177\u96c6\u6210\u63a8\u7406\u867d\u7136\u63d0\u9ad8\u4e86\u8ba1\u7b97\u7cbe\u5ea6\uff0c\u4f46\u7f3a\u4e4f\u81ea\u4e3b\u9002\u5e94\u6027\u3002", "method": "TableMind\u91c7\u7528\u4e24\u9636\u6bb5\u5fae\u8c03\u8303\u5f0f\uff1a\u9996\u5148\u5728\u9ad8\u8d28\u91cf\u63a8\u7406\u8f68\u8ff9\u4e0a\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u5efa\u7acb\u6709\u6548\u7684\u5de5\u5177\u4f7f\u7528\u6a21\u5f0f\uff1b\u7136\u540e\u8fdb\u884c\u5f3a\u5316\u5fae\u8c03\uff0c\u4f18\u5316\u591a\u76ee\u6807\u7b56\u7565\u3002\u63d0\u51fa\u4e86Rank-Aware Policy Optimization (RAPO)\u7b97\u6cd5\u3002", "result": "TableMind\u5728\u591a\u4e2a\u4e3b\u6d41\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u63a8\u7406\u7cbe\u5ea6\u548c\u8ba1\u7b97\u7cbe\u5ea6\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "TableMind\u901a\u8fc7\u81ea\u4e3b\u5de5\u5177\u8c03\u7528\u3001\u4ee3\u7801\u6267\u884c\u548c\u7b56\u7565\u81ea\u9002\u5e94\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8868\u683c\u63a8\u7406\u7684\u6027\u80fd\u3002"}}
{"id": "2509.05543", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05543", "abs": "https://arxiv.org/abs/2509.05543", "authors": ["Haitao Tian", "Pierre Payeur"], "title": "DuoCLR: Dual-Surrogate Contrastive Learning for Skeleton-based Human Action Segmentation", "comment": "ICCV 2025 accepted paper", "summary": "In this paper, a contrastive representation learning framework is proposed to\nenhance human action segmentation via pre-training using trimmed (single\naction) skeleton sequences. Unlike previous representation learning works that\nare tailored for action recognition and that build upon isolated sequence-wise\nrepresentations, the proposed framework focuses on exploiting multi-scale\nrepresentations in conjunction with cross-sequence variations. More\nspecifically, it proposes a novel data augmentation strategy, 'Shuffle and\nWarp', which exploits diverse multi-action permutations. The latter effectively\nassists two surrogate tasks that are introduced in contrastive learning: Cross\nPermutation Contrasting (CPC) and Relative Order Reasoning (ROR). In\noptimization, CPC learns intra-class similarities by contrasting\nrepresentations of the same action class across different permutations, while\nROR reasons about inter-class contexts by predicting relative mapping between\ntwo permutations. Together, these tasks enable a Dual-Surrogate Contrastive\nLearning (DuoCLR) network to learn multi-scale feature representations\noptimized for action segmentation. In experiments, DuoCLR is pre-trained on a\ntrimmed skeleton dataset and evaluated on an untrimmed dataset where it\ndemonstrates a significant boost over state-the-art comparatives in both\nmulti-class and multi-label action segmentation tasks. Lastly, ablation studies\nare conducted to evaluate the effectiveness of each component of the proposed\napproach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u6bd4\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528\u4fee\u526a\u7684\uff08\u5355\u4e00\u52a8\u4f5c\uff09\u9aa8\u9abc\u5e8f\u5217\u8fdb\u884c\u9884\u8bad\u7ec3\u6765\u589e\u5f3a\u4eba\u7c7b\u52a8\u4f5c\u5206\u5272\u3002", "motivation": "\u4e0e\u4ee5\u5f80\u4e3a\u52a8\u4f5c\u8bc6\u522b\u91cf\u8eab\u5b9a\u5236\u5e76\u5efa\u7acb\u5728\u5b64\u7acb\u7684\u5e8f\u5217\u8868\u793a\u57fa\u7840\u4e0a\u7684\u8868\u793a\u5b66\u4e60\u5de5\u4f5c\u4e0d\u540c\uff0c\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u4fa7\u91cd\u4e8e\u7ed3\u5408\u8de8\u5e8f\u5217\u53d8\u5316\u6765\u5229\u7528\u591a\u5c3a\u5ea6\u8868\u793a\u3002", "method": "\u8be5\u65b9\u6cd5\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\u201cShuffle and Warp\u201d\uff0c\u5229\u7528\u4e0d\u540c\u7684\u591a\u52a8\u4f5c\u6392\u5217\u3002\u540e\u8005\u6709\u6548\u5730\u8f85\u52a9\u4e86\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u5f15\u5165\u7684\u4e24\u4e2a\u66ff\u4ee3\u4efb\u52a1\uff1a\u4ea4\u53c9\u6392\u5217\u5bf9\u6bd4\uff08CPC\uff09\u548c\u76f8\u5bf9\u987a\u5e8f\u63a8\u7406\uff08ROR\uff09\u3002", "result": "DuoCLR \u5728\u4e00\u4e2a\u4fee\u526a\u8fc7\u7684\u9aa8\u9abc\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5e76\u5728\u4e00\u4e2a\u672a\u4fee\u526a\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5728\u591a\u7c7b\u548c\u591a\u6807\u7b7e\u52a8\u4f5c\u5206\u5272\u4efb\u52a1\u4e2d\uff0cDuoCLR \u663e\u793a\u51fa\u6bd4\u6700\u5148\u8fdb\u7684\u6bd4\u8f83\u65b9\u6cd5\u6709\u663e\u7740\u63d0\u5347\u3002", "conclusion": "\u6d88\u878d\u7814\u7a76\u7528\u4e8e\u8bc4\u4f30\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6bcf\u4e2a\u7ec4\u6210\u90e8\u5206\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.05801", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05801", "abs": "https://arxiv.org/abs/2509.05801", "authors": ["Debdeep Sanyal", "Aaryan Nagpal", "Dhruv Kumar", "Murari Mandal", "Saurabh Deshpande"], "title": "time2time: Causal Intervention in Hidden States to Simulate Rare Events in Time Series Foundation Models", "comment": null, "summary": "While transformer-based foundation models excel at forecasting routine\npatterns, two questions remain: do they internalize semantic concepts such as\nmarket regimes, or merely fit curves? And can their internal representations be\nleveraged to simulate rare, high-stakes events such as market crashes? To\ninvestigate this, we introduce activation transplantation, a causal\nintervention that manipulates hidden states by imposing the statistical moments\nof one event (e.g., a historical crash) onto another (e.g., a calm period)\nduring the forward pass. This procedure deterministically steers forecasts:\ninjecting crash semantics induces downturn predictions, while injecting calm\nsemantics suppresses crashes and restores stability. Beyond binary control, we\nfind that models encode a graded notion of event severity, with the latent\nvector norm directly correlating with the magnitude of systemic shocks.\nValidated across two architecturally distinct TSFMs, Toto (decoder only) and\nChronos (encoder-decoder), our results demonstrate that steerable, semantically\ngrounded representations are a robust property of large time series\ntransformers. Our findings provide evidence for a latent concept space that\ngoverns model predictions, shifting interpretability from post-hoc attribution\nto direct causal intervention, and enabling semantic \"what-if\" analysis for\nstrategic stress-testing.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e Transformer \u7684\u57fa\u7840\u6a21\u578b\u5728\u9884\u6d4b\u5e38\u89c4\u6a21\u5f0f\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5e76\u63a2\u8ba8\u4e86\u5b83\u4eec\u662f\u5426\u5185\u5316\u4e86\u5e02\u573a\u673a\u5236\u7b49\u8bed\u4e49\u6982\u5ff5\uff0c\u6216\u8005\u4ec5\u4ec5\u662f\u62df\u5408\u66f2\u7ebf\u3002\u6b64\u5916\uff0c\u8fd8\u7814\u7a76\u4e86\u5b83\u4eec\u7684\u5185\u90e8\u8868\u5f81\u662f\u5426\u53ef\u4ee5\u7528\u4e8e\u6a21\u62df\u7f55\u89c1\u7684\u9ad8\u98ce\u9669\u4e8b\u4ef6\uff0c\u5982\u5e02\u573a\u5d29\u6e83\u3002", "motivation": "\u7814\u7a76 Transformer \u6a21\u578b\u662f\u5426\u771f\u6b63\u7406\u89e3\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u8bed\u4e49\u6982\u5ff5\uff0c\u4ee5\u53ca\u5b83\u4eec\u662f\u5426\u80fd\u591f\u6a21\u62df\u6781\u7aef\u4e8b\u4ef6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6fc0\u6d3b\u79fb\u690d\u7684\u56e0\u679c\u5e72\u9884\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5728\u6b63\u5411\u4f20\u9012\u8fc7\u7a0b\u4e2d\u5c06\u4e00\u4e2a\u4e8b\u4ef6\u7684\u7edf\u8ba1\u77e9\uff08\u4f8b\u5982\uff0c\u5386\u53f2\u5d29\u6e83\uff09\u65bd\u52a0\u5230\u53e6\u4e00\u4e2a\u4e8b\u4ef6\uff08\u4f8b\u5982\uff0c\u5e73\u9759\u65f6\u671f\uff09\u6765\u64cd\u7eb5\u9690\u85cf\u72b6\u6001\u3002\u4ece\u800c\u786e\u5b9a\u6027\u5730\u5f15\u5bfc\u9884\u6d4b\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u6ce8\u5165\u5d29\u6e83\u8bed\u4e49\u4f1a\u5bfc\u81f4\u7ecf\u6d4e\u8870\u9000\u9884\u6d4b\uff0c\u800c\u6ce8\u5165\u5e73\u9759\u8bed\u4e49\u4f1a\u6291\u5236\u5d29\u6e83\u5e76\u6062\u590d\u7a33\u5b9a\u3002\u6a21\u578b\u7f16\u7801\u4e86\u4e8b\u4ef6\u4e25\u91cd\u7a0b\u5ea6\u7684\u5206\u7ea7\u6982\u5ff5\uff0c\u6f5c\u5728\u5411\u91cf\u8303\u6570\u4e0e\u7cfb\u7edf\u6027\u51b2\u51fb\u7684\u5927\u5c0f\u76f4\u63a5\u76f8\u5173\u3002\u5728\u4e24\u79cd\u67b6\u6784\u4e0d\u540c\u7684 TSFM\uff08Toto \u548c Chronos\uff09\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u53ef\u64cd\u7eb5\u7684\u3001\u8bed\u4e49\u63a5\u5730\u7684\u8868\u5f81\u662f\u5927\u578b\u65f6\u95f4\u5e8f\u5217 Transformer \u7684\u4e00\u4e2a\u9c81\u68d2\u5c5e\u6027\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u63a7\u5236\u6a21\u578b\u9884\u6d4b\u7684\u6f5c\u5728\u6982\u5ff5\u7a7a\u95f4\u63d0\u4f9b\u4e86\u8bc1\u636e\uff0c\u5c06\u53ef\u89e3\u91ca\u6027\u4ece\u4e8b\u540e\u5f52\u56e0\u8f6c\u79fb\u5230\u76f4\u63a5\u56e0\u679c\u5e72\u9884\uff0c\u5e76\u4e3a\u6218\u7565\u538b\u529b\u6d4b\u8bd5\u63d0\u4f9b\u8bed\u4e49\u201c\u5047\u8bbe\u201d\u5206\u6790\u3002"}}
{"id": "2509.05867", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05867", "abs": "https://arxiv.org/abs/2509.05867", "authors": ["ZiXuan Zhang", "Bowen Hao", "Yingjie Li", "Hongzhi Yin"], "title": "ZhiFangDanTai: Fine-tuning Graph-based Retrieval-Augmented Generation Model for Traditional Chinese Medicine Formula", "comment": null, "summary": "Traditional Chinese Medicine (TCM) formulas play a significant role in\ntreating epidemics and complex diseases. Existing models for TCM utilize\ntraditional algorithms or deep learning techniques to analyze formula\nrelationships, yet lack comprehensive results, such as complete formula\ncompositions and detailed explanations. Although recent efforts have used TCM\ninstruction datasets to fine-tune Large Language Models (LLMs) for explainable\nformula generation, existing datasets lack sufficient details, such as the\nroles of the formula's sovereign, minister, assistant, courier; efficacy;\ncontraindications; tongue and pulse diagnosis-limiting the depth of model\noutputs. To address these challenges, we propose ZhiFangDanTai, a framework\ncombining Graph-based Retrieval-Augmented Generation (GraphRAG) with LLM\nfine-tuning. ZhiFangDanTai uses GraphRAG to retrieve and synthesize structured\nTCM knowledge into concise summaries, while also constructing an enhanced\ninstruction dataset to improve LLMs' ability to integrate retrieved\ninformation. Furthermore, we provide novel theoretical proofs demonstrating\nthat integrating GraphRAG with fine-tuning techniques can reduce generalization\nerror and hallucination rates in the TCM formula task. Experimental results on\nboth collected and clinical datasets demonstrate that ZhiFangDanTai achieves\nsignificant improvements over state-of-the-art models. Our model is\nopen-sourced at https://huggingface.co/tczzx6/ZhiFangDanTai1.0.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aZhiFangDanTai\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u57fa\u4e8e\u56fe\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08GraphRAG\uff09\u548cLLM\u5fae\u8c03\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u4e2d\u836f\u65b9\u5242\u5206\u6790\u6a21\u578b\u7684\u4e0d\u8db3\uff0c\u4f8b\u5982\u7f3a\u4e4f\u5b8c\u6574\u7684\u65b9\u5242\u7ec4\u6210\u548c\u8be6\u7ec6\u89e3\u91ca\u3002", "motivation": "\u73b0\u6709\u4e2d\u836f\u65b9\u5242\u5206\u6790\u6a21\u578b\u7f3a\u4e4f\u5168\u9762\u7684\u7ed3\u679c\uff0c\u4f8b\u5982\u5b8c\u6574\u7684\u65b9\u5242\u7ec4\u6210\u548c\u8be6\u7ec6\u7684\u89e3\u91ca\u3002\u73b0\u6709\u7684\u6570\u636e\u96c6\u7f3a\u4e4f\u8db3\u591f\u7684\u7ec6\u8282\uff0c\u4f8b\u5982\u65b9\u5242\u7684\u541b\u3001\u81e3\u3001\u4f50\u3001\u4f7f\u7684\u4f5c\u7528\uff1b\u529f\u6548\uff1b\u7981\u5fcc\u75c7\uff1b\u820c\u8c61\u548c\u8109\u8c61\u8bca\u65ad\uff0c\u9650\u5236\u4e86\u6a21\u578b\u8f93\u51fa\u7684\u6df1\u5ea6\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51faZhiFangDanTai\uff0c\u4e00\u4e2a\u7ed3\u5408\u4e86\u57fa\u4e8e\u56fe\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08GraphRAG\uff09\u4e0eLLM\u5fae\u8c03\u7684\u6846\u67b6\u3002ZhiFangDanTai\u4f7f\u7528GraphRAG\u6765\u68c0\u7d22\u548c\u7efc\u5408\u7ed3\u6784\u5316\u7684\u4e2d\u533b\u836f\u77e5\u8bc6\u5230\u7b80\u6d01\u7684\u6458\u8981\u4e2d\uff0c\u540c\u65f6\u6784\u5efa\u4e00\u4e2a\u589e\u5f3a\u7684\u6307\u4ee4\u6570\u636e\u96c6\u6765\u63d0\u9ad8LLM\u6574\u5408\u68c0\u7d22\u4fe1\u606f\u7684\u80fd\u529b\u3002", "result": "\u5728\u6536\u96c6\u7684\u548c\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cZhiFangDanTai \u6bd4state-of-the-art\u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u7ed3\u5408GraphRAG\u4e0e\u5fae\u8c03\u6280\u672f\u53ef\u4ee5\u51cf\u5c11\u4e2d\u836f\u65b9\u5242\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u8bef\u5dee\u548c\u5e7b\u89c9\u7387\u3002"}}
{"id": "2509.06283", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06283", "abs": "https://arxiv.org/abs/2509.06283", "authors": ["Xuan-Phi Nguyen", "Shrey Pandit", "Revanth Gangi Reddy", "Austin Xu", "Silvio Savarese", "Caiming Xiong", "Shafiq Joty"], "title": "SFR-DeepResearch: Towards Effective Reinforcement Learning for Autonomously Reasoning Single Agents", "comment": "Technical Report", "summary": "Equipping large language models (LLMs) with complex, interleaved reasoning\nand tool-use capabilities has become a key focus in agentic AI research,\nespecially with recent advances in reasoning-oriented (``thinking'') models.\nSuch capabilities are key to unlocking a number of important applications. One\nsuch application is Deep Research (DR), which requires extensive search and\nreasoning over many sources. Our work in this paper focuses on the development\nof native Autonomous Single-Agent models for DR featuring minimal web crawling\nand Python tool integration. Unlike multi-agent systems, where agents take up\npre-defined roles and are told what to do at each step in a static workflow, an\nautonomous single-agent determines its next action dynamically based on\ncontext, without manual directive. While prior work has proposed training\nrecipes for base or instruction-tuned LLMs, we focus on continual reinforcement\nlearning (RL) of reasoning-optimized models to further enhance agentic skills\nwhile preserving reasoning ability. Towards this end, we propose a simple RL\nrecipe with entirely synthetic data, which we apply to various open-source\nLLMs. Our best variant SFR-DR-20B achieves up to 28.7% on Humanity's Last Exam\nbenchmark. In addition, we conduct key analysis experiments to provide more\ninsights into our methodologies.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7528\u4e8e\u6df1\u5ea6\u7814\u7a76\u7684\u81ea\u4e3b\u5355\u667a\u80fd\u4f53\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5177\u6709\u6700\u5c11\u7684\u7f51\u7edc\u722c\u53d6\u548cPython\u5de5\u5177\u96c6\u6210\u3002", "motivation": "\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u914d\u5907\u590d\u6742\u7684\u3001\u4ea4\u9519\u7684\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u5df2\u6210\u4e3a\u667a\u80fd\u4f53\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u7684\u5173\u952e\u91cd\u70b9\uff0c\u5c24\u5176\u662f\u5728\u6700\u8fd1\u4ee5\u63a8\u7406\u4e3a\u5bfc\u5411\uff08\u201c\u601d\u8003\u201d\uff09\u7684\u6a21\u578b\u53d6\u5f97\u8fdb\u5c55\u7684\u60c5\u51b5\u4e0b\u3002\u6b64\u7c7b\u80fd\u529b\u662f\u89e3\u9501\u8bb8\u591a\u91cd\u8981\u5e94\u7528\u7684\u5173\u952e\u3002\u5176\u4e2d\u4e00\u9879\u5e94\u7528\u662f\u6df1\u5ea6\u7814\u7a76\uff08DR\uff09\uff0c\u5b83\u9700\u8981\u5728\u8bb8\u591a\u6765\u6e90\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u7684\u641c\u7d22\u548c\u63a8\u7406\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684RL\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5b8c\u5168\u4f7f\u7528\u5408\u6210\u6570\u636e\uff0c\u6211\u4eec\u5c06\u5176\u5e94\u7528\u4e8e\u5404\u79cd\u5f00\u6e90LLM\u3002", "result": "\u6211\u4eec\u6700\u597d\u7684\u53d8\u4f53SFR-DR-20B\u5728Humanity's Last Exam\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e8628.7%\u3002", "conclusion": "\u6211\u4eec\u8fdb\u884c\u4e86\u5173\u952e\u5206\u6790\u5b9e\u9a8c\uff0c\u4ee5\u63d0\u4f9b\u5bf9\u6211\u4eec\u65b9\u6cd5\u7684\u66f4\u591a\u89c1\u89e3\u3002"}}
{"id": "2509.05811", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05811", "abs": "https://arxiv.org/abs/2509.05811", "authors": ["Ben Kretzu", "Karen Ullrich", "Yonathan Efroni"], "title": "Simple Optimizers for Convex Aligned Multi-Objective Optimization", "comment": null, "summary": "It is widely recognized in modern machine learning practice that access to a\ndiverse set of tasks can enhance performance across those tasks. This\nobservation suggests that, unlike in general multi-objective optimization, the\nobjectives in many real-world settings may not be inherently conflicting. To\naddress this, prior work introduced the Aligned Multi-Objective Optimization\n(AMOO) framework and proposed gradient-based algorithms with provable\nconvergence guarantees. However, existing analysis relies on strong\nassumptions, particularly strong convexity, which implies the existence of a\nunique optimal solution. In this work, we relax this assumption and study\ngradient-descent algorithms for convex AMOO under standard smoothness or\nLipschitz continuity conditions-assumptions more consistent with those used in\ndeep learning practice. This generalization requires new analytical tools and\nmetrics to characterize convergence in the convex AMOO setting. We develop such\ntools, propose scalable algorithms for convex AMOO, and establish their\nconvergence guarantees. Additionally, we prove a novel lower bound that\ndemonstrates the suboptimality of naive equal-weight approaches compared to our\nmethods.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u51f8\u5bf9\u9f50\u591a\u76ee\u6807\u4f18\u5316 (AMOO) \u7684\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\uff0c\u653e\u5bbd\u4e86\u5f3a\u51f8\u6027\u5047\u8bbe\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u5206\u6790\u5de5\u5177\u548c\u6307\u6807\u6765\u8868\u5f81\u51f8 AMOO \u4e2d\u7684\u6536\u655b\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4f9d\u8d56\u4e8e\u5f3a\u51f8\u6027\u5047\u8bbe\uff0c\u8fd9\u4e0e\u6df1\u5ea6\u5b66\u4e60\u5b9e\u8df5\u4e0d\u7b26\u3002\u672c\u6587\u65e8\u5728\u653e\u5bbd\u8fd9\u4e00\u5047\u8bbe\uff0c\u7814\u7a76\u6807\u51c6\u5e73\u6ed1\u6216 Lipschitz \u8fde\u7eed\u6027\u6761\u4ef6\u4e0b\u51f8 AMOO \u7684\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u3002", "method": "\u672c\u6587\u5f00\u53d1\u4e86\u65b0\u7684\u5206\u6790\u5de5\u5177\uff0c\u63d0\u51fa\u4e86\u53ef\u6269\u5c55\u7684\u51f8 AMOO \u7b97\u6cd5\uff0c\u5e76\u5efa\u7acb\u4e86\u5b83\u4eec\u7684\u6536\u655b\u6027\u4fdd\u8bc1\u3002\u6b64\u5916\uff0c\u8fd8\u8bc1\u660e\u4e86\u4e00\u79cd\u65b0\u7684\u4e0b\u754c\uff0c\u8868\u660e\u4e0e\u672c\u6587\u65b9\u6cd5\u76f8\u6bd4\uff0c\u7b80\u5355\u7684\u7b49\u6743\u91cd\u65b9\u6cd5\u662f\u6b21\u4f18\u7684\u3002", "result": "\u672c\u6587\u4e3a\u51f8 AMOO \u63d0\u51fa\u4e86\u53ef\u6269\u5c55\u7684\u7b97\u6cd5\uff0c\u5e76\u5efa\u7acb\u4e86\u5b83\u4eec\u7684\u6536\u655b\u6027\u4fdd\u8bc1\u3002\u6b64\u5916\uff0c\u8fd8\u8bc1\u660e\u4e86\u4e00\u79cd\u65b0\u7684\u4e0b\u754c\uff0c\u8868\u660e\u4e0e\u672c\u6587\u65b9\u6cd5\u76f8\u6bd4\uff0c\u7b80\u5355\u7684\u7b49\u6743\u91cd\u65b9\u6cd5\u662f\u6b21\u4f18\u7684\u3002", "conclusion": "\u672c\u6587\u7814\u7a76\u4e86\u51f8\u5bf9\u9f50\u591a\u76ee\u6807\u4f18\u5316 (AMOO) \u7684\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\uff0c\u653e\u5bbd\u4e86\u5f3a\u51f8\u6027\u5047\u8bbe\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u5206\u6790\u5de5\u5177\u548c\u6307\u6807\u6765\u8868\u5f81\u51f8 AMOO \u4e2d\u7684\u6536\u655b\u6027\u3002"}}
{"id": "2509.05878", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.05878", "abs": "https://arxiv.org/abs/2509.05878", "authors": ["Fran\u00e7ois Grolleau", "Emily Alsentzer", "Timothy Keyes", "Philip Chung", "Akshay Swaminathan", "Asad Aali", "Jason Hom", "Tridu Huynh", "Thomas Lew", "April S. Liang", "Weihan Chu", "Natasha Z. Steele", "Christina F. Lin", "Jingkun Yang", "Kameron C. Black", "Stephen P. Ma", "Fateme N. Haredasht", "Nigam H. Shah", "Kevin Schulman", "Jonathan H. Chen"], "title": "MedFactEval and MedAgentBrief: A Framework and Workflow for Generating and Evaluating Factual Clinical Summaries", "comment": null, "summary": "Evaluating factual accuracy in Large Language Model (LLM)-generated clinical\ntext is a critical barrier to adoption, as expert review is unscalable for the\ncontinuous quality assurance these systems require. We address this challenge\nwith two complementary contributions. First, we introduce MedFactEval, a\nframework for scalable, fact-grounded evaluation where clinicians define\nhigh-salience key facts and an \"LLM Jury\"--a multi-LLM majority vote--assesses\ntheir inclusion in generated summaries. Second, we present MedAgentBrief, a\nmodel-agnostic, multi-step workflow designed to generate high-quality, factual\ndischarge summaries. To validate our evaluation framework, we established a\ngold-standard reference using a seven-physician majority vote on\nclinician-defined key facts from inpatient cases. The MedFactEval LLM Jury\nachieved almost perfect agreement with this panel (Cohen's kappa=81%), a\nperformance statistically non-inferior to that of a single human expert\n(kappa=67%, P < 0.001). Our work provides both a robust evaluation framework\n(MedFactEval) and a high-performing generation workflow (MedAgentBrief),\noffering a comprehensive approach to advance the responsible deployment of\ngenerative AI in clinical workflows.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30LLM\u751f\u6210\u4e34\u5e8a\u6587\u672c\u4e8b\u5b9e\u51c6\u786e\u6027\u7684\u6846\u67b6MedFactEval\u548c\u4e00\u4e2a\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u57fa\u4e8e\u4e8b\u5b9e\u7684\u51fa\u9662\u603b\u7ed3\u7684\u5de5\u4f5c\u6d41\u7a0bMedAgentBrief\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7684\u4e34\u5e8a\u6587\u672c\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u662f\u5e94\u7528\u7684\u5173\u952e\u969c\u788d\uff0c\u56e0\u4e3a\u4e13\u5bb6\u5ba1\u67e5\u5bf9\u4e8e\u8fd9\u4e9b\u7cfb\u7edf\u6240\u9700\u7684\u6301\u7eed\u8d28\u91cf\u4fdd\u8bc1\u6765\u8bf4\u662f\u4e0d\u53ef\u6269\u5c55\u7684\u3002", "method": "1. \u5f15\u5165MedFactEval\u6846\u67b6\uff0c\u7528\u4e8e\u53ef\u6269\u5c55\u7684\u3001\u57fa\u4e8e\u4e8b\u5b9e\u7684\u8bc4\u4f30\uff0c\u5176\u4e2d\u4e34\u5e8a\u533b\u751f\u5b9a\u4e49\u9ad8\u663e\u8457\u6027\u7684\u5173\u952e\u4e8b\u5b9e\uff0c\u5e76\u7531\u4e00\u4e2a\u201cLLM\u966a\u5ba1\u56e2\u201d\uff08\u4e00\u4e2a\u591aLLM\u591a\u6570\u6295\u7968\uff09\u8bc4\u4f30\u8fd9\u4e9b\u4e8b\u5b9e\u662f\u5426\u5305\u542b\u5728\u751f\u6210\u7684\u603b\u7ed3\u4e2d\u3002\n2. \u63d0\u51fa\u4e86MedAgentBrief\uff0c\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u3001\u591a\u6b65\u9aa4\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u65e8\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u57fa\u4e8e\u4e8b\u5b9e\u7684\u51fa\u9662\u603b\u7ed3\u3002", "result": "MedFactEval LLM\u966a\u5ba1\u56e2\u4e0e\u4e00\u4e2a\u4e03\u540d\u533b\u751f\u591a\u6570\u6295\u7968\u5efa\u7acb\u7684\u91d1\u6807\u51c6\u53c2\u8003\u51e0\u4e4e\u5b8c\u5168\u4e00\u81f4\uff08Cohen's kappa=81%\uff09\uff0c\u5176\u6027\u80fd\u5728\u7edf\u8ba1\u5b66\u4e0a\u4e0d\u4e9a\u4e8e\u5355\u4e2a\u4eba\u7c7b\u4e13\u5bb6\uff08kappa=67%\uff0cP < 0.001\uff09\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u8bc4\u4f30\u6846\u67b6\uff08MedFactEval\uff09\u548c\u4e00\u4e2a\u9ad8\u6027\u80fd\u7684\u751f\u6210\u5de5\u4f5c\u6d41\u7a0b\uff08MedAgentBrief\uff09\uff0c\u4e3a\u63a8\u8fdb\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5728\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u8d1f\u8d23\u4efb\u90e8\u7f72\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.06284", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06284", "abs": "https://arxiv.org/abs/2509.06284", "authors": ["Jiaxiang Chen", "Zhuo Wang", "Mingxi Zou", "Zhucong Li", "Zhijian Zhou", "Song Wang", "Zenglin Xu"], "title": "From Implicit Exploration to Structured Reasoning: Leveraging Guideline and Refinement for LLMs", "comment": null, "summary": "Large language models (LLMs) have advanced general-purpose reasoning, showing\nstrong performance across diverse tasks. However, existing methods often rely\non implicit exploration, where the model follows stochastic and unguided\nreasoning paths-like walking without a map. This leads to unstable reasoning\npaths, lack of error correction, and limited learning from past experience. To\naddress these issues, we propose a framework that shifts from implicit\nexploration to structured reasoning through guideline and refinement. First, we\nextract structured reasoning patterns from successful trajectories and\nreflective signals from failures. During inference, the model follows these\nguidelines step-by-step, with refinement applied after each step to correct\nerrors and stabilize the reasoning process. Experiments on BBH and four\nadditional benchmarks (GSM8K, MATH-500, MBPP, HumanEval) show that our method\nconsistently outperforms strong baselines across diverse reasoning tasks.\nStructured reasoning with stepwise execution and refinement improves stability\nand generalization, while guidelines transfer well across domains and flexibly\nsupport cross-model collaboration, matching or surpassing supervised\nfine-tuning in effectiveness and scalability.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6307\u5bfc\u548c\u6539\u8fdb\u5b9e\u73b0\u7ed3\u6784\u5316\u63a8\u7406\u7684\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u9690\u5f0f\u63a2\u7d22\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u968f\u673a\u548c\u65e0\u6307\u5bfc\u7684\u63a8\u7406\u8def\u5f84\uff0c\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u3001\u7f3a\u4e4f\u7ea0\u9519\u548c\u6709\u9650\u7684\u5b66\u4e60\u7ecf\u9a8c\u3002", "method": "\u4ece\u6210\u529f\u7684\u8f68\u8ff9\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u63a8\u7406\u6a21\u5f0f\uff0c\u5e76\u4ece\u5931\u8d25\u4e2d\u63d0\u53d6\u53cd\u601d\u4fe1\u53f7\u3002\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u6a21\u578b\u9010\u6b65\u9075\u5faa\u8fd9\u4e9b\u6307\u5bfc\uff0c\u5e76\u5728\u6bcf\u4e00\u6b65\u540e\u8fdb\u884c\u6539\u8fdb\u4ee5\u7ea0\u6b63\u9519\u8bef\u5e76\u7a33\u5b9a\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5728BBH\u548c\u56db\u4e2a\u989d\u5916\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\u3002", "conclusion": "\u7ed3\u6784\u5316\u63a8\u7406\u63d0\u9ad8\u4e86\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u6307\u5bfc\u65b9\u9488\u5728\u4e0d\u540c\u9886\u57df\u4e4b\u95f4\u5177\u6709\u826f\u597d\u7684\u8fc1\u79fb\u6027\uff0c\u5e76\u7075\u6d3b\u5730\u652f\u6301\u8de8\u6a21\u578b\u534f\u4f5c\u3002"}}
{"id": "2509.05576", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05576", "abs": "https://arxiv.org/abs/2509.05576", "authors": ["Zekang Zheng", "Haokun Li", "Yaofo Chen", "Mingkui Tan", "Qing Du"], "title": "Sensitivity-Aware Post-Training Quantization for Deep Neural Networks", "comment": "Accepted by PRCV 2025", "summary": "Model quantization reduces neural network parameter precision to achieve\ncompression, but often compromises accuracy. Existing post-training\nquantization (PTQ) methods employ iterative parameter updates to preserve\naccuracy under high compression ratios, incurring significant computational\ncomplexity and resource overhead, which limits applicability in\nresource-constrained edge computing and real-time inference scenarios. This\npaper proposes an efficient PTQ method guided by parameter sensitivity\nanalysis. The approach prioritizes quantization of high-sensitivity parameters,\nleveraging unquantized low-sensitivity parameters to compensate for\nquantization errors, thereby mitigating accuracy degradation. Furthermore, by\nexploiting column-wise clustering of parameter sensitivity, the method\nintroduces a row-parallel quantization framework with a globally shared inverse\nHessian matrix update mechanism, reducing computational complexity by an order\nof magnitude. Experimental results on ResNet-50 and YOLOv5s demonstrate a\n20-200-fold quantization speedup over the Optimal Brain Quantization baseline,\nwith mean accuracy loss below 0.3%, confirming the method's efficacy in\nbalancing efficiency and accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u8bad\u7ec3\u540e\u91cf\u5316\uff08PTQ\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u53c2\u6570\u654f\u611f\u6027\u5206\u6790\u6307\u5bfc\u91cf\u5316\u8fc7\u7a0b\uff0c\u5728\u4fdd\u8bc1\u7cbe\u5ea6\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u91cf\u5316\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684PTQ\u65b9\u6cd5\u4e3a\u4e86\u5728\u9ad8\u538b\u7f29\u6bd4\u4e0b\u4fdd\u6301\u7cbe\u5ea6\uff0c\u9700\u8981\u8fdb\u884c\u8fed\u4ee3\u53c2\u6570\u66f4\u65b0\uff0c\u5bfc\u81f4\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u8d44\u6e90\u5f00\u9500\u5927\uff0c\u9650\u5236\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8ba1\u7b97\u548c\u5b9e\u65f6\u63a8\u7406\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u8be5\u65b9\u6cd5\u4f18\u5148\u91cf\u5316\u9ad8\u654f\u611f\u5ea6\u53c2\u6570\uff0c\u5e76\u5229\u7528\u672a\u91cf\u5316\u7684\u4f4e\u654f\u611f\u5ea6\u53c2\u6570\u6765\u8865\u507f\u91cf\u5316\u8bef\u5dee\uff0c\u4ece\u800c\u51cf\u8f7b\u7cbe\u5ea6\u4e0b\u964d\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5229\u7528\u53c2\u6570\u654f\u611f\u6027\u7684\u5217\u5411\u805a\u7c7b\uff0c\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u4e00\u79cd\u5177\u6709\u5168\u5c40\u5171\u4eab\u9006 Hessian \u77e9\u9635\u66f4\u65b0\u673a\u5236\u7684\u884c\u5e76\u884c\u91cf\u5316\u6846\u67b6\uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u5728ResNet-50\u548cYOLOv5s\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4Optimal Brain Quantization\u57fa\u7ebf\u5feb20-200\u500d\uff0c\u5e73\u5747\u7cbe\u5ea6\u635f\u5931\u4f4e\u4e8e0.3%\uff0c\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u5728\u6548\u7387\u548c\u7cbe\u5ea6\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684PTQ\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4fdd\u8bc1\u7cbe\u5ea6\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u63d0\u9ad8\u91cf\u5316\u901f\u5ea6\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u573a\u666f\u3002"}}
{"id": "2509.05826", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05826", "abs": "https://arxiv.org/abs/2509.05826", "authors": ["Misgina Tsighe Hagos", "Claes Lundstr\u00f6m"], "title": "Performance of Conformal Prediction in Capturing Aleatoric Uncertainty", "comment": null, "summary": "Conformal prediction is a model-agnostic approach to generating prediction\nsets that cover the true class with a high probability. Although its prediction\nset size is expected to capture aleatoric uncertainty, there is a lack of\nevidence regarding its effectiveness. The literature presents that prediction\nset size can upper-bound aleatoric uncertainty or that prediction sets are\nlarger for difficult instances and smaller for easy ones, but a validation of\nthis attribute of conformal predictors is missing. This work investigates how\neffectively conformal predictors quantify aleatoric uncertainty, specifically\nthe inherent ambiguity in datasets caused by overlapping classes. We perform\nthis by measuring the correlation between prediction set sizes and the number\nof distinct labels assigned by human annotators per instance. We further assess\nthe similarity between prediction sets and human-provided annotations. We use\nthree conformal prediction approaches to generate prediction sets for eight\ndeep learning models trained on four datasets. The datasets contain annotations\nfrom multiple human annotators (ranging from five to fifty participants) per\ninstance, enabling the identification of class overlap. We show that the vast\nmajority of the conformal prediction outputs show a very weak to weak\ncorrelation with human annotations, with only a few showing moderate\ncorrelation. These findings underscore the necessity of critically reassessing\nthe prediction sets generated using conformal predictors. While they can\nprovide a higher coverage of the true classes, their capability in capturing\naleatoric uncertainty remains limited.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5171\u5f62\u9884\u6d4b\u5668\u91cf\u5316 aleatoric \u4e0d\u786e\u5b9a\u6027\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u7531\u91cd\u53e0\u7c7b\u5f15\u8d77\u7684\u6570\u636e\u96c6\u4e2d\u56fa\u6709\u7684\u6a21\u7cca\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\uff0c\u867d\u7136\u5171\u5f62\u9884\u6d4b\u7684\u9884\u6d4b\u96c6\u5927\u5c0f\u6709\u671b\u6355\u6349\u5230 aleatoric \u4e0d\u786e\u5b9a\u6027\uff0c\u4f46\u7f3a\u4e4f\u5173\u4e8e\u5176\u6709\u6548\u6027\u7684\u8bc1\u636e\u3002\u6587\u732e\u8868\u660e\u9884\u6d4b\u96c6\u5927\u5c0f\u53ef\u4ee5\u9650\u5236 aleatoric \u4e0d\u786e\u5b9a\u6027\uff0c\u6216\u8005\u9884\u6d4b\u96c6\u5bf9\u4e8e\u56f0\u96be\u7684\u5b9e\u4f8b\u66f4\u5927\uff0c\u5bf9\u4e8e\u5bb9\u6613\u7684\u5b9e\u4f8b\u66f4\u5c0f\uff0c\u4f46\u662f\u7f3a\u5c11\u5bf9\u6b64\u5c5e\u6027\u7684\u9a8c\u8bc1\u3002", "method": "\u672c\u6587\u901a\u8fc7\u6d4b\u91cf\u9884\u6d4b\u96c6\u5927\u5c0f\u4e0e\u4eba\u7c7b\u6ce8\u91ca\u8005\u4e3a\u6bcf\u4e2a\u5b9e\u4f8b\u5206\u914d\u7684\u4e0d\u540c\u6807\u7b7e\u6570\u91cf\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u6765\u8bc4\u4f30\u5171\u5f62\u9884\u6d4b\u5668\u91cf\u5316 aleatoric \u4e0d\u786e\u5b9a\u6027\u7684\u6709\u6548\u6027\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u8bc4\u4f30\u4e86\u9884\u6d4b\u96c6\u548c\u4eba\u7c7b\u63d0\u4f9b\u7684\u6ce8\u91ca\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u3002\u6211\u4eec\u4f7f\u7528\u4e09\u79cd\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\u4e3a\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u516b\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u751f\u6210\u9884\u6d4b\u96c6\u3002\u8fd9\u4e9b\u6570\u636e\u96c6\u5305\u542b\u6765\u81ea\u591a\u4e2a\u4eba\u7684\u6ce8\u91ca\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u7edd\u5927\u591a\u6570\u5171\u5f62\u9884\u6d4b\u8f93\u51fa\u4e0e\u4eba\u7c7b\u6ce8\u91ca\u7684\u76f8\u5173\u6027\u975e\u5e38\u5f31\u5230\u5f31\uff0c\u53ea\u6709\u5c11\u6570\u663e\u793a\u51fa\u4e2d\u7b49\u76f8\u5173\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u8bba\u662f\uff0c\u5171\u5f62\u9884\u6d4b\u5668\u5728\u6355\u83b7 aleatoric \u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u7684\u80fd\u529b\u4ecd\u7136\u6709\u9650\uff0c\u9700\u8981\u6279\u5224\u6027\u5730\u91cd\u65b0\u8bc4\u4f30\u4f7f\u7528\u5171\u5f62\u9884\u6d4b\u5668\u751f\u6210\u7684\u9884\u6d4b\u96c6\u3002"}}
{"id": "2509.05882", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05882", "abs": "https://arxiv.org/abs/2509.05882", "authors": ["Abhijnan Nath", "Carine Graff", "Nikhil Krishnaswamy"], "title": "Let's Roleplay: Examining LLM Alignment in Collaborative Dialogues", "comment": null, "summary": "As Large Language Models (LLMs) integrate into diverse workflows, they are\nincreasingly being considered \"collaborators\" with humans. If such AI\ncollaborators are to be reliable, their behavior over multiturn interactions\nmust be predictable, validated and verified before deployment. Common alignment\ntechniques are typically developed under simplified single-user settings and do\nnot account for the dynamics of long-horizon multiparty interactions. This\npaper examines how different alignment methods affect LLM agents' effectiveness\nas partners in multiturn, multiparty collaborations. We study this question\nthrough the lens of friction agents that intervene in group dialogues to\nencourage the collaborative group to slow down and reflect upon their reasoning\nfor deliberative decision-making. Using a roleplay methodology, we evaluate\ninterventions from differently-trained friction agents in collaborative task\nconversations. We propose a novel counterfactual evaluation framework that\nquantifies how friction interventions change the trajectory of group\ncollaboration and belief alignment. Our results show that a friction-aware\napproach significantly outperforms common alignment baselines in helping both\nconvergence to a common ground, or agreed-upon task-relevant propositions, and\ncorrectness of task outcomes.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u591a\u8f6e\u3001\u591a\u65b9\u534f\u4f5c\u4e2d\u4f5c\u4e3a\u4f19\u4f34\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u5173\u6ce8\u4e86\u4e0d\u540c\u5bf9\u9f50\u65b9\u6cd5\u7684\u5f71\u54cd\u3002", "motivation": "\u5728\u591a\u4eba\u534f\u4f5c\u7684\u590d\u6742\u73af\u5883\u4e2d\uff0c\u5e38\u89c1\u5bf9\u9f50\u6280\u672f\u5728\u7b80\u5316\u5355\u7528\u6237\u8bbe\u7f6e\u4e0b\u5f00\u53d1\uff0c\u65e0\u6cd5\u4fdd\u8bc1LLM\u884c\u4e3a\u7684\u53ef\u9884\u6d4b\u6027\u3001\u9a8c\u8bc1\u6027\u548c\u53ef\u9760\u6027\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u4e0d\u540c\u5bf9\u9f50\u65b9\u6cd5\u5bf9LLM\u4f5c\u4e3a\u534f\u4f5c\u4f19\u4f34\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u89d2\u8272\u626e\u6f14\u65b9\u6cd5\uff0c\u8bc4\u4f30\u4e0d\u540c\u8bad\u7ec3\u7684\u6469\u64e6\u4ee3\u7406\u5728\u534f\u4f5c\u4efb\u52a1\u5bf9\u8bdd\u4e2d\u7684\u5e72\u9884\u6548\u679c\u3002\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u53cd\u4e8b\u5b9e\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u6469\u64e6\u5e72\u9884\u5982\u4f55\u6539\u53d8\u7fa4\u4f53\u534f\u4f5c\u548c\u4fe1\u5ff5\u5bf9\u9f50\u7684\u8f68\u8ff9\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5e2e\u52a9\u8fbe\u6210\u5171\u8bc6\u548c\u63d0\u9ad8\u4efb\u52a1\u7ed3\u679c\u7684\u6b63\u786e\u6027\u65b9\u9762\uff0c\u5177\u6709\u6469\u64e6\u610f\u8bc6\u7684\u65b9\u6cd5\u660e\u663e\u4f18\u4e8e\u5e38\u89c1\u7684\u5bf9\u9f50\u57fa\u7ebf\u3002", "conclusion": "\u6469\u64e6\u610f\u8bc6\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u5347LLM\u5728\u591a\u8f6e\u591a\u65b9\u534f\u4f5c\u4e2d\u4f5c\u4e3a\u4f19\u4f34\u7684\u6709\u6548\u6027\uff0c\u6709\u52a9\u4e8e\u7fa4\u4f53\u8fbe\u6210\u5171\u8bc6\u5e76\u63d0\u9ad8\u4efb\u52a1\u7ed3\u679c\u7684\u6b63\u786e\u6027\u3002"}}
{"id": "2509.06307", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06307", "abs": "https://arxiv.org/abs/2509.06307", "authors": ["Lei Shu", "Dong Zhao"], "title": "Can AI Make Energy Retrofit Decisions? An Evaluation of Large Language Models", "comment": null, "summary": "Conventional approaches to building energy retrofit decision making suffer\nfrom limited generalizability and low interpretability, hindering adoption in\ndiverse residential contexts. With the growth of Smart and Connected\nCommunities, generative AI, especially large language models (LLMs), may help\nby processing contextual information and producing practitioner readable\nrecommendations. We evaluate seven LLMs (ChatGPT, DeepSeek, Gemini, Grok,\nLlama, and Claude) on residential retrofit decisions under two objectives:\nmaximizing CO2 reduction (technical) and minimizing payback period\n(sociotechnical). Performance is assessed on four dimensions: accuracy,\nconsistency, sensitivity, and reasoning, using a dataset of 400 homes across 49\nUS states. LLMs generate effective recommendations in many cases, reaching up\nto 54.5 percent top 1 match and 92.8 percent within top 5 without fine tuning.\nPerformance is stronger for the technical objective, while sociotechnical\ndecisions are limited by economic trade offs and local context. Agreement\nacross models is low, and higher performing models tend to diverge from others.\nLLMs are sensitive to location and building geometry but less sensitive to\ntechnology and occupant behavior. Most models show step by step, engineering\nstyle reasoning, but it is often simplified and lacks deeper contextual\nawareness. Overall, LLMs are promising assistants for energy retrofit decision\nmaking, but improvements in accuracy, consistency, and context handling are\nneeded for reliable practice.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u5728\u4f4f\u5b85\u6539\u9020\u51b3\u7b56\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u4f20\u7edf\u5efa\u7b51\u8282\u80fd\u6539\u9020\u51b3\u7b56\u65b9\u6cd5\u6cdb\u5316\u6027\u6709\u9650\uff0c\u53ef\u89e3\u91ca\u6027\u4f4e\uff0c\u963b\u788d\u4e86\u5176\u5728\u4e0d\u540c\u4f4f\u5b85\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u5904\u7406\u60c5\u5883\u4fe1\u606f\u5e76\u751f\u6210\u4ece\u4e1a\u8005\u53ef\u8bfb\u7684\u5efa\u8bae\u6765\u63d0\u4f9b\u5e2e\u52a9\u3002", "method": "\u8bc4\u4f30\u4e86\u4e03\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08ChatGPT\u3001DeepSeek\u3001Gemini\u3001Grok\u3001Llama \u548c Claude\uff09\u5728\u4f4f\u5b85\u6539\u9020\u51b3\u7b56\u4e2d\u7684\u8868\u73b0\uff0c\u76ee\u6807\u662f\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11CO2\u6392\u653e\uff08\u6280\u672f\u76ee\u6807\uff09\u548c\u6700\u5c0f\u5316\u6295\u8d44\u56de\u6536\u671f\uff08\u793e\u4f1a\u6280\u672f\u76ee\u6807\uff09\u3002\u4f7f\u7528\u5305\u542b\u7f8e\u56fd49\u4e2a\u5dde\u7684400\u4e2a\u5bb6\u5ead\u7684\u6570\u636e\u96c6\uff0c\u4ece\u51c6\u786e\u6027\u3001\u4e00\u81f4\u6027\u3001\u654f\u611f\u6027\u548c\u63a8\u7406\u56db\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bb8\u591a\u60c5\u51b5\u4e0b\u90fd\u80fd\u751f\u6210\u6709\u6548\u7684\u5efa\u8bae\uff0c\u5728\u4e0d\u8fdb\u884c\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u6700\u9ad8\u8fbe\u523054.5%\u7684top 1\u5339\u914d\u548c92.8%\u7684top 5\u5339\u914d\u3002\u6280\u672f\u76ee\u6807\u7684\u6027\u80fd\u66f4\u5f3a\uff0c\u800c\u793e\u4f1a\u6280\u672f\u51b3\u7b56\u53d7\u5230\u7ecf\u6d4e\u6743\u8861\u548c\u5f53\u5730\u73af\u5883\u7684\u9650\u5236\u3002\u6a21\u578b\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u8f83\u4f4e\uff0c\u6027\u80fd\u8f83\u9ad8\u7684\u6a21\u578b\u5f80\u5f80\u4e0e\u5176\u4ed6\u6a21\u578b\u4e0d\u540c\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u4f4d\u7f6e\u548c\u5efa\u7b51\u51e0\u4f55\u5f62\u72b6\u654f\u611f\uff0c\u4f46\u5bf9\u6280\u672f\u548c\u5c45\u4f4f\u8005\u884c\u4e3a\u4e0d\u592a\u654f\u611f\u3002", "conclusion": "\u603b\u7684\u6765\u8bf4\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u80fd\u6e90\u6539\u9020\u51b3\u7b56\u7684\u6709\u5e0c\u671b\u7684\u52a9\u624b\uff0c\u4f46\u4e3a\u4e86\u53ef\u9760\u7684\u5b9e\u8df5\uff0c\u9700\u8981\u63d0\u9ad8\u51c6\u786e\u6027\u3001\u4e00\u81f4\u6027\u548c\u4e0a\u4e0b\u6587\u5904\u7406\u80fd\u529b\u3002"}}
{"id": "2509.05582", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05582", "abs": "https://arxiv.org/abs/2509.05582", "authors": ["Zhiling Ye", "Cong Zhou", "Xiubao Zhang", "Haifeng Shen", "Weihong Deng", "Quan Lu"], "title": "Reconstruction and Reenactment Separated Method for Realistic Gaussian Head", "comment": null, "summary": "In this paper, we explore a reconstruction and reenactment separated\nframework for 3D Gaussians head, which requires only a single portrait image as\ninput to generate controllable avatar. Specifically, we developed a large-scale\none-shot gaussian head generator built upon WebSSL and employed a two-stage\ntraining approach that significantly enhances the capabilities of\ngeneralization and high-frequency texture reconstruction. During inference, an\nultra-lightweight gaussian avatar driven by control signals enables high\nframe-rate rendering, achieving 90 FPS at a resolution of 512x512. We further\ndemonstrate that the proposed framework follows the scaling law, whereby\nincreasing the parameter scale of the reconstruction module leads to improved\nperformance. Moreover, thanks to the separation design, driving efficiency\nremains unaffected. Finally, extensive quantitative and qualitative experiments\nvalidate that our approach outperforms current state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u76843D\u9ad8\u65af\u5934\u90e8\u91cd\u5efa\u548c\u91cd\u6f14\u6846\u67b6\uff0c\u4ec5\u9700\u5355\u5f20\u4eba\u50cf\u5373\u53ef\u751f\u6210\u53ef\u63a7\u5934\u50cf\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u5355\u5f20\u56fe\u50cf\u751f\u6210\u53ef\u63a7\u5934\u50cf\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8eWebSSL\u7684\u5927\u89c4\u6a21\u5355\u6b21\u9ad8\u65af\u5934\u90e8\u751f\u6210\u5668\uff0c\u5e76\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\u4ee5\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u548c\u9ad8\u9891\u7eb9\u7406\u91cd\u5efa\u3002", "result": "\u5b9e\u73b0\u4e86\u8d85\u8f7b\u91cf\u7ea7\u9ad8\u65af\u5934\u50cf\u7684\u63a7\u5236\u4fe1\u53f7\u9a71\u52a8\uff0c\u80fd\u591f\u5728512x512\u5206\u8fa8\u7387\u4e0b\u8fbe\u523090 FPS\u7684\u9ad8\u5e27\u7387\u6e32\u67d3\u3002\u53c2\u6570\u89c4\u6a21\u7684\u589e\u52a0\u80fd\u591f\u63d0\u9ad8\u6027\u80fd\uff0c\u4e14\u9a71\u52a8\u6548\u7387\u4e0d\u53d7\u5f71\u54cd\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5f53\u524d\u6700\u4f73\u65b9\u6cd5\u3002"}}
{"id": "2509.05830", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.05830", "abs": "https://arxiv.org/abs/2509.05830", "authors": ["Akaash Kolluri", "Shengguang Wu", "Joon Sung Park", "Michael S. Bernstein"], "title": "Finetuning LLMs for Human Behavior Prediction in Social Science Experiments", "comment": "16 pages, 5 figures", "summary": "Large language models (LLMs) offer a powerful opportunity to simulate the\nresults of social science experiments. In this work, we demonstrate that\nfinetuning LLMs directly on individual-level responses from past experiments\nmeaningfully improves the accuracy of such simulations across diverse social\nscience domains. We construct SocSci210 via an automatic pipeline, a dataset\ncomprising 2.9 million responses from 400,491 participants in 210 open-source\nsocial science experiments. Through finetuning, we achieve multiple levels of\ngeneralization. In completely unseen studies, our strongest model,\nSocrates-Qwen-14B, produces predictions that are 26% more aligned with\ndistributions of human responses to diverse outcome questions under varying\nconditions relative to its base model (Qwen2.5-14B), outperforming GPT-4o by\n13%. By finetuning on a subset of conditions in a study, generalization to new\nunseen conditions is particularly robust, improving by 71%. Since SocSci210\ncontains rich demographic information, we reduce demographic parity, a measure\nof bias, by 10.6% through finetuning. Because social sciences routinely\ngenerate rich, topic-specific datasets, our findings indicate that finetuning\non such data could enable more accurate simulations for experimental hypothesis\nscreening. We release our data, models and finetuning code at\nstanfordhci.github.io/socrates.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u5728\u793e\u4f1a\u79d1\u5b66\u5b9e\u9a8c\u7684\u4e2a\u4f53\u5c42\u9762\u53cd\u5e94\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u9ad8\u6a21\u62df\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6a21\u62df\u793e\u4f1a\u79d1\u5b66\u5b9e\u9a8c\u7ed3\u679c\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u9ad8\u6a21\u62df\u7684\u51c6\u786e\u6027\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b290\u4e07\u4e2a\u6765\u81ea210\u4e2a\u793e\u4f1a\u79d1\u5b66\u5b9e\u9a8c\u53c2\u4e0e\u8005\u56de\u5e94\u7684\u6570\u636e\u96c6SocSci210\uff0c\u5e76\u901a\u8fc7\u5728\u6b64\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u5fae\u8c03\uff0c\u6a21\u578b\u5728\u672a\u89c1\u8fc7\u7684\u7814\u7a76\u4e2d\uff0c\u9884\u6d4b\u7ed3\u679c\u4e0e\u4eba\u7c7b\u53cd\u5e94\u7684\u5206\u5e03\u5bf9\u9f50\u5ea6\u63d0\u9ad8\u4e8626%\uff0c\u4f18\u4e8eGPT-4o 13%\uff1b\u5728\u65b0\u6761\u4ef6\u4e0b\u6cdb\u5316\u80fd\u529b\u63d0\u9ad8\u4e8671%\uff1b\u4eba\u53e3\u7edf\u8ba1\u5b66\u4e0a\u7684\u504f\u5dee\u964d\u4f4e\u4e8610.6%\u3002", "conclusion": "\u5728\u7279\u5b9a\u4e3b\u9898\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\u53ef\u4ee5\u66f4\u51c6\u786e\u5730\u6a21\u62df\u5b9e\u9a8c\u5047\u8bbe\u7b5b\u9009\u3002"}}
{"id": "2509.05908", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.05908", "abs": "https://arxiv.org/abs/2509.05908", "authors": ["Yue Gu", "Zhihao Du", "Ying Shi", "Shiliang Zhang", "Qian Chen", "Jiqing Han"], "title": "Enhancing the Robustness of Contextual ASR to Varying Biasing Information Volumes Through Purified Semantic Correlation Joint Modeling", "comment": "Accepted by IEEE Transactions on Audio, Speech and Language\n  Processing, 2025 (https://ieeexplore.ieee.org/document/11150731). DOI:\n  10.1109/TASLPRO.2025.3606198", "summary": "Recently, cross-attention-based contextual automatic speech recognition (ASR)\nmodels have made notable advancements in recognizing personalized biasing\nphrases. However, the effectiveness of cross-attention is affected by\nvariations in biasing information volume, especially when the length of the\nbiasing list increases significantly. We find that, regardless of the length of\nthe biasing list, only a limited amount of biasing information is most relevant\nto a specific ASR intermediate representation. Therefore, by identifying and\nintegrating the most relevant biasing information rather than the entire\nbiasing list, we can alleviate the effects of variations in biasing information\nvolume for contextual ASR. To this end, we propose a purified semantic\ncorrelation joint modeling (PSC-Joint) approach. In PSC-Joint, we define and\ncalculate three semantic correlations between the ASR intermediate\nrepresentations and biasing information from coarse to fine: list-level,\nphrase-level, and token-level. Then, the three correlations are jointly modeled\nto produce their intersection, so that the most relevant biasing information\nacross various granularities is highlighted and integrated for contextual\nrecognition. In addition, to reduce the computational cost introduced by the\njoint modeling of three semantic correlations, we also propose a purification\nmechanism based on a grouped-and-competitive strategy to filter out irrelevant\nbiasing phrases. Compared with baselines, our PSC-Joint approach achieves\naverage relative F1 score improvements of up to 21.34% on AISHELL-1 and 28.46%\non KeSpeech, across biasing lists of varying lengths.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPSC-Joint\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u4e0a\u4e0b\u6587ASR\u4e2dbiasing\u4fe1\u606f\u91cf\u53d8\u5316\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u4e0a\u4e0b\u6587ASR\u6a21\u578b\u5728\u8bc6\u522b\u4e2a\u6027\u5316biasing\u77ed\u8bed\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u6709\u6548\u6027\u53d7\u5230biasing\u4fe1\u606f\u91cf\u53d8\u5316\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5728biasing\u5217\u8868\u957f\u5ea6\u663e\u8457\u589e\u52a0\u65f6\u3002", "method": "\u5b9a\u4e49\u5e76\u8ba1\u7b97ASR\u4e2d\u95f4\u8868\u793a\u548cbiasing\u4fe1\u606f\u4e4b\u95f4\u4ece\u7c97\u5230\u7ec6\u7684\u4e09\u4e2a\u8bed\u4e49\u76f8\u5173\u6027\uff1a\u5217\u8868\u7ea7\u522b\u3001\u77ed\u8bed\u7ea7\u522b\u548ctoken\u7ea7\u522b\u3002\u7136\u540e\uff0c\u8054\u5408\u5efa\u6a21\u8fd9\u4e09\u4e2a\u76f8\u5173\u6027\u4ee5\u4ea7\u751f\u5b83\u4eec\u7684\u4ea4\u96c6\uff0c\u4ece\u800c\u7a81\u51fa\u663e\u793a\u548c\u6574\u5408\u8de8\u5404\u79cd\u7c92\u5ea6\u7684\u6700\u76f8\u5173\u7684biasing\u4fe1\u606f\u4ee5\u8fdb\u884c\u4e0a\u4e0b\u6587\u8bc6\u522b\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u964d\u4f4e\u4e09\u4e2a\u8bed\u4e49\u76f8\u5173\u6027\u8054\u5408\u5efa\u6a21\u5e26\u6765\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u7ec4\u7ade\u4e89\u7b56\u7565\u7684\u51c0\u5316\u673a\u5236\uff0c\u4ee5\u8fc7\u6ee4\u6389\u4e0d\u76f8\u5173\u7684biasing\u77ed\u8bed\u3002", "result": "\u5728AISHELL-1\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe21.34%\u7684\u5e73\u5747\u76f8\u5bf9F1\u5206\u6570\u63d0\u5347\uff0c\u5728KeSpeech\u4e0a\u5b9e\u73b0\u4e8628.46%\u7684\u63d0\u5347\u3002", "conclusion": "PSC-Joint\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u63d0\u9ad8\u4e0a\u4e0b\u6587ASR\u5728biasing\u4fe1\u606f\u91cf\u53d8\u5316\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2509.06337", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06337", "abs": "https://arxiv.org/abs/2509.06337", "authors": ["Jianpeng Zhao", "Chenyu Yuan", "Weiming Luo", "Haoling Xie", "Guangwei Zhang", "Steven Jige Quan", "Zixuan Yuan", "Pengyang Wang", "Denghui Zhang"], "title": "Large Language Models as Virtual Survey Respondents: Evaluating Sociodemographic Response Generation", "comment": null, "summary": "Questionnaire-based surveys are foundational to social science research and\npublic policymaking, yet traditional survey methods remain costly,\ntime-consuming, and often limited in scale. This paper explores a new paradigm:\nsimulating virtual survey respondents using Large Language Models (LLMs). We\nintroduce two novel simulation settings, namely Partial Attribute Simulation\n(PAS) and Full Attribute Simulation (FAS), to systematically evaluate the\nability of LLMs to generate accurate and demographically coherent responses. In\nPAS, the model predicts missing attributes based on partial respondent\nprofiles, whereas FAS involves generating complete synthetic datasets under\nboth zero-context and context-enhanced conditions. We curate a comprehensive\nbenchmark suite, LLM-S^3 (Large Language Model-based Sociodemographic Survey\nSimulation), that spans 11 real-world public datasets across four sociological\ndomains. Our evaluation of multiple mainstream LLMs (GPT-3.5/4 Turbo, LLaMA\n3.0/3.1-8B) reveals consistent trends in prediction performance, highlights\nfailure modes, and demonstrates how context and prompt design impact simulation\nfidelity. This work establishes a rigorous foundation for LLM-driven survey\nsimulations, offering scalable and cost-effective tools for sociological\nresearch and policy evaluation. Our code and dataset are available at:\nhttps://github.com/dart-lab-research/LLM-S-Cube-Benchmark", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6a21\u62df\u865a\u62df\u8c03\u67e5\u5bf9\u8c61\u7684\u65b0\u8303\u5f0f\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u8c03\u67e5\u65b9\u6cd5\u7684\u6210\u672c\u9ad8\u3001\u8017\u65f6\u548c\u89c4\u6a21\u6709\u9650\u7b49\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u8c03\u67e5\u65b9\u6cd5\u6210\u672c\u9ad8\u3001\u8017\u65f6\u4e14\u89c4\u6a21\u6709\u9650\u3002", "method": "\u5f15\u5165\u4e86\u90e8\u5206\u5c5e\u6027\u6a21\u62df\uff08PAS\uff09\u548c\u5b8c\u6574\u5c5e\u6027\u6a21\u62df\uff08FAS\uff09\u4e24\u79cd\u65b0\u7684\u6a21\u62df\u8bbe\u7f6e\uff0c\u5e76\u6784\u5efa\u4e86LLM-S^3\u57fa\u51c6\u5957\u4ef6\uff0c\u8be5\u5957\u4ef6\u8de8\u8d8a\u56db\u4e2a\u793e\u4f1a\u5b66\u9886\u57df\u768411\u4e2a\u771f\u5b9e\u4e16\u754c\u516c\u5171\u6570\u636e\u96c6\u3002", "result": "\u5bf9\u591a\u4e2a\u4e3b\u6d41LLM\uff08GPT-3.5/4 Turbo, LLaMA 3.0/3.1-8B\uff09\u7684\u8bc4\u4f30\u63ed\u793a\u4e86\u9884\u6d4b\u6027\u80fd\u7684\u4e00\u81f4\u8d8b\u52bf\uff0c\u7a81\u51fa\u4e86\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u5c55\u793a\u4e86\u4e0a\u4e0b\u6587\u548c\u63d0\u793a\u8bbe\u8ba1\u5982\u4f55\u5f71\u54cd\u6a21\u62df\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3aLLM\u9a71\u52a8\u7684\u8c03\u67e5\u6a21\u62df\u5960\u5b9a\u4e86\u4e25\u8c28\u7684\u57fa\u7840\uff0c\u4e3a\u793e\u4f1a\u5b66\u7814\u7a76\u548c\u653f\u7b56\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2509.05592", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05592", "abs": "https://arxiv.org/abs/2509.05592", "authors": ["Changtao Miao", "Yi Zhang", "Man Luo", "Weiwei Feng", "Kaiyuan Zheng", "Qi Chu", "Tao Gong", "Jianshu Li", "Yunfeng Diao", "Wei Zhou", "Joey Tianyi Zhou", "Xiaoshuai Hao"], "title": "MFFI: Multi-Dimensional Face Forgery Image Dataset for Real-World Scenarios", "comment": null, "summary": "Rapid advances in Artificial Intelligence Generated Content (AIGC) have\nenabled increasingly sophisticated face forgeries, posing a significant threat\nto social security. However, current Deepfake detection methods are limited by\nconstraints in existing datasets, which lack the diversity necessary in\nreal-world scenarios. Specifically, these data sets fall short in four key\nareas: unknown of advanced forgery techniques, variability of facial scenes,\nrichness of real data, and degradation of real-world propagation. To address\nthese challenges, we propose the Multi-dimensional Face Forgery Image\n(\\textbf{MFFI}) dataset, tailored for real-world scenarios. MFFI enhances\nrealism based on four strategic dimensions: 1) Wider Forgery Methods; 2) Varied\nFacial Scenes; 3) Diversified Authentic Data; 4) Multi-level Degradation\nOperations. MFFI integrates $50$ different forgery methods and contains $1024K$\nimage samples. Benchmark evaluations show that MFFI outperforms existing public\ndatasets in terms of scene complexity, cross-domain generalization capability,\nand detection difficulty gradients. These results validate the technical\nadvance and practical utility of MFFI in simulating real-world conditions. The\ndataset and additional details are publicly available at\n{https://github.com/inclusionConf/MFFI}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u66f4\u8d34\u8fd1\u73b0\u5b9e\u573a\u666f\u7684Deepfake\u68c0\u6d4b\u6570\u636e\u96c6MFFI\uff0c\u5b83\u5728\u4f2a\u9020\u65b9\u6cd5\u3001\u9762\u90e8\u573a\u666f\u3001\u771f\u5b9e\u6570\u636e\u591a\u6837\u6027\u548c\u591a\u5c42\u6b21\u964d\u7ea7\u64cd\u4f5c\u56db\u4e2a\u7ef4\u5ea6\u4e0a\u589e\u5f3a\u4e86\u771f\u5b9e\u6027\u3002", "motivation": "\u73b0\u6709Deepfake\u68c0\u6d4b\u65b9\u6cd5\u53d7\u9650\u4e8e\u6570\u636e\u96c6\u7684\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u65e0\u6cd5\u5f88\u597d\u5730\u9002\u5e94\u771f\u5b9e\u4e16\u754c\u7684\u573a\u666f\uff0c\u5c24\u5176\u662f\u5728\u4f2a\u9020\u6280\u672f\u672a\u77e5\u3001\u9762\u90e8\u573a\u666f\u53d8\u5316\u3001\u771f\u5b9e\u6570\u636e\u4e30\u5bcc\u6027\u4ee5\u53ca\u771f\u5b9e\u4e16\u754c\u4f20\u64ad\u7684\u9000\u5316\u7b49\u65b9\u9762\u3002", "method": "\u63d0\u51fa\u4e86\u591a\u7ef4\u9762\u90e8\u4f2a\u9020\u56fe\u50cf\uff08MFFI\uff09\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b50\u79cd\u4e0d\u540c\u7684\u4f2a\u9020\u65b9\u6cd5\u548c1024K\u56fe\u50cf\u6837\u672c\uff0c\u5e76\u5728\u56db\u4e2a\u7ef4\u5ea6\u4e0a\u589e\u5f3a\u4e86\u771f\u5b9e\u6027\uff1a\u66f4\u5e7f\u6cdb\u7684\u4f2a\u9020\u65b9\u6cd5\u3001\u4e0d\u540c\u7684\u9762\u90e8\u573a\u666f\u3001\u591a\u6837\u5316\u7684\u771f\u5b9e\u6570\u636e\u548c\u591a\u5c42\u6b21\u7684\u964d\u7ea7\u64cd\u4f5c\u3002", "result": "\u57fa\u51c6\u8bc4\u4f30\u8868\u660e\uff0cMFFI\u5728\u573a\u666f\u590d\u6742\u6027\u3001\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u548c\u68c0\u6d4b\u96be\u5ea6\u68af\u5ea6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u516c\u5171\u6570\u636e\u96c6\u3002", "conclusion": "MFFI\u6570\u636e\u96c6\u7684\u6280\u672f\u8fdb\u6b65\u548c\u5728\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\u7684\u5b9e\u7528\u6027\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u8be5\u6570\u636e\u96c6\u5df2\u516c\u5f00\u3002"}}
{"id": "2509.05833", "categories": ["cs.LG", "cs.GT"], "pdf": "https://arxiv.org/pdf/2509.05833", "abs": "https://arxiv.org/abs/2509.05833", "authors": ["Zeyu Song", "Sainyam Galhotra", "Shagufta Mehnaz"], "title": "Benchmarking Robust Aggregation in Decentralized Gradient Marketplaces", "comment": null, "summary": "The rise of distributed and privacy-preserving machine learning has sparked\ninterest in decentralized gradient marketplaces, where participants trade\nintermediate artifacts like gradients. However, existing Federated Learning\n(FL) benchmarks overlook critical economic and systemic factors unique to such\nmarketplaces-cost-effectiveness, fairness to sellers, and market\nstability-especially when a buyer relies on a private baseline dataset for\nevaluation.\n  We introduce a comprehensive benchmark framework to holistically evaluate\nrobust gradient aggregation methods within these buyer-baseline-reliant\nmarketplaces. Our contributions include: (1) a simulation environment modeling\nmarketplace dynamics with a variable buyer baseline and diverse seller\ndistributions; (2) an evaluation methodology augmenting standard FL metrics\nwith marketplace-centric dimensions such as Economic Efficiency, Fairness, and\nSelection Dynamics; (3) an in-depth empirical analysis of the existing\nDistributed Gradient Marketplace framework, MartFL, including the integration\nand comparative evaluation of adapted FLTrust and SkyMask as alternative\naggregation strategies within it. This benchmark spans diverse datasets, local\nattacks, and Sybil attacks targeting the marketplace selection process; and (4)\nactionable insights into the trade-offs between model performance, robustness,\ncost, fairness, and stability.\n  This benchmark equips the community with essential tools and empirical\nevidence to evaluate and design more robust, equitable, and economically viable\ndecentralized gradient marketplaces.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u53bb\u4e2d\u5fc3\u5316\u68af\u5ea6\u5e02\u573a\u4e2d\u7684\u68af\u5ea6\u805a\u5408\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u8054\u90a6\u5b66\u4e60\u57fa\u51c6\u5ffd\u7565\u4e86\u68af\u5ea6\u5e02\u573a\u4e2d\u5173\u952e\u7684\u7ecf\u6d4e\u548c\u7cfb\u7edf\u56e0\u7d20\uff0c\u5982\u6210\u672c\u6548\u76ca\u3001\u5bf9\u5356\u5bb6\u7684\u516c\u5e73\u6027\u4ee5\u53ca\u5e02\u573a\u7a33\u5b9a\u6027\uff0c\u5c24\u5176\u662f\u5728\u4e70\u5bb6\u4f9d\u8d56\u79c1\u6709\u57fa\u7ebf\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\u65f6\u3002", "method": "\u8bba\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u6a21\u62df\u73af\u5883\uff0c\u5bf9\u5177\u6709\u53ef\u53d8\u4e70\u5bb6\u57fa\u7ebf\u548c\u591a\u6837\u5316\u5356\u5bb6\u5206\u5e03\u7684\u5e02\u573a\u52a8\u6001\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u4ee5\u5e02\u573a\u4e3a\u4e2d\u5fc3\u7684\u7ef4\u5ea6\uff08\u5982\u7ecf\u6d4e\u6548\u7387\u3001\u516c\u5e73\u6027\u548c\u9009\u62e9\u52a8\u6001\uff09\u6765\u589e\u5f3a\u6807\u51c6\u7684\u8054\u90a6\u5b66\u4e60\u6307\u6807\u3002", "result": "\u8bba\u6587\u5bf9\u73b0\u6709\u7684\u5206\u5e03\u5f0f\u68af\u5ea6\u5e02\u573a\u6846\u67b6MartFL\u8fdb\u884c\u4e86\u6df1\u5165\u7684\u5b9e\u8bc1\u5206\u6790\uff0c\u5305\u62ec\u96c6\u6210\u548c\u6bd4\u8f83\u8bc4\u4f30\u4e86\u6539\u8fdb\u7684FLTrust\u548cSkyMask\u4f5c\u4e3a\u5176\u4e2d\u7684\u66ff\u4ee3\u805a\u5408\u7b56\u7565\u3002\u8be5\u57fa\u51c6\u6db5\u76d6\u4e86\u4e0d\u540c\u7684\u6570\u636e\u96c6\u3001\u672c\u5730\u653b\u51fb\u548c\u9488\u5bf9\u5e02\u573a\u9009\u62e9\u8fc7\u7a0b\u7684Sybil\u653b\u51fb\u3002", "conclusion": "\u8be5\u57fa\u51c6\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u5fc5\u8981\u7684\u5de5\u5177\u548c\u7ecf\u9a8c\u8bc1\u636e\uff0c\u4ee5\u8bc4\u4f30\u548c\u8bbe\u8ba1\u66f4\u7a33\u5065\u3001\u516c\u5e73\u548c\u7ecf\u6d4e\u4e0a\u53ef\u884c\u7684\u53bb\u4e2d\u5fc3\u5316\u68af\u5ea6\u5e02\u573a\u3002"}}
{"id": "2509.05915", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.05915", "abs": "https://arxiv.org/abs/2509.05915", "authors": ["Sangmin Bae"], "title": "Accelerating Large Language Model Inference via Early-Exiting Algorithms", "comment": "PhD Dissertation", "summary": "Large language models have achieved remarkable capabilities, but their\npractical deployment is hindered by significant computational costs. While\nadaptive computation methods like early-exiting promise to reduce these costs,\nthey introduce a fundamental conflict: the per-token dynamism intended to save\ncomputation often creates system-level bottlenecks that can paradoxically\nreduce throughput in batched inference. This dissertation resolves this\nconflict by co-designing adaptive algorithms and model architectures to strike\nan optimal balance between dynamism and efficiency. To this end, our work first\naddresses critical sources of overhead in conventional early-exiting by\nproposing an efficient parallel decoding mechanism. We then show that deep\nparameter sharing provides an architectural foundation that not only yields\ncompact, parameter-efficient models but also inherently mitigates the critical\nsynchronization issues affecting dynamic inference. Finally, this work presents\na unified framework where lightweight routers are pretrained to dynamically\nassign an optimal recursion depth for each token. This approach establishes a\nnew Pareto frontier between efficiency and performance by effectively\noptimizing for both adaptive computation and parameter efficiency within a\nsingle model.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u534f\u540c\u8bbe\u8ba1\u81ea\u9002\u5e94\u7b97\u6cd5\u548c\u6a21\u578b\u67b6\u6784\u6765\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u7684\u65b9\u6cd5\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u81ea\u9002\u5e94\u8ba1\u7b97\u65b9\u6cd5\uff08\u5982\u63d0\u524d\u9000\u51fa\uff09\u867d\u7136\u53ef\u4ee5\u964d\u4f4e\u6210\u672c\uff0c\u4f46\u4f1a\u5f15\u5165\u7cfb\u7edf\u74f6\u9888\uff0c\u53cd\u800c\u964d\u4f4e\u6279\u91cf\u63a8\u7406\u7684\u541e\u5410\u91cf\u3002", "method": "1. \u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u7684\u5e76\u884c\u89e3\u7801\u673a\u5236\uff0c\u89e3\u51b3\u4f20\u7edf\u63d0\u524d\u9000\u51fa\u4e2d\u7684\u5173\u952e\u5f00\u9500\u6765\u6e90\u30022. \u63d0\u51fa\u6df1\u5ea6\u53c2\u6570\u5171\u4eab\u67b6\u6784\uff0c\u51cf\u5c11\u6a21\u578b\u53c2\u6570\uff0c\u7f13\u89e3\u52a8\u6001\u63a8\u7406\u4e2d\u7684\u540c\u6b65\u95ee\u9898\u30023. \u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u9884\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u8def\u7531\u5668\uff0c\u4e3a\u6bcf\u4e2atoken\u52a8\u6001\u5206\u914d\u6700\u4f73\u9012\u5f52\u6df1\u5ea6\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u7684\u6548\u7387\u548c\u6027\u80fd\u4e4b\u95f4\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u3002", "conclusion": "\u901a\u8fc7\u5728\u5355\u4e2a\u6a21\u578b\u4e2d\u6709\u6548\u5730\u4f18\u5316\u81ea\u9002\u5e94\u8ba1\u7b97\u548c\u53c2\u6570\u6548\u7387\uff0c\u5b9e\u73b0\u4e86\u6548\u7387\u548c\u6027\u80fd\u7684\u63d0\u5347\u3002"}}
{"id": "2509.06341", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06341", "abs": "https://arxiv.org/abs/2509.06341", "authors": ["Issue Yishu Wang", "Kakam Chong", "Xiaofeng Wang", "Xu Yan", "DeXin Kong", "Chen Ju", "Ming Chen", "Shuai Xiao", "Shuguang Han", "jufeng chen"], "title": "Evaluating Multi-Turn Bargain Skills in LLM-Based Seller Agent", "comment": null, "summary": "In online second-hand marketplaces, multi-turn bargaining is a crucial part\nof seller-buyer interactions. Large Language Models (LLMs) can act as seller\nagents, negotiating with buyers on behalf of sellers under given business\nconstraints. A critical ability for such agents is to track and accurately\ninterpret cumulative buyer intents across long negotiations, which directly\nimpacts bargaining effectiveness. We introduce a multi-turn evaluation\nframework for measuring the bargaining ability of seller agents in e-commerce\ndialogues. The framework tests whether an agent can extract and track buyer\nintents. Our contributions are: (1) a large-scale e-commerce bargaining\nbenchmark spanning 622 categories, 9,892 products, and 3,014 tasks; (2) a\nturn-level evaluation framework grounded in Theory of Mind (ToM) with annotated\nbuyer intents, moving beyond outcome-only metrics; and (3) an automated\npipeline that extracts reliable intent from massive dialogue data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5728\u7535\u5b50\u5546\u52a1\u5bf9\u8bdd\u4e2d\uff0c\u4f5c\u4e3a\u5356\u5bb6\u4ee3\u7406\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8bae\u4ef7\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u8ddf\u8e2a\u548c\u89e3\u91ca\u4e70\u5bb6\u610f\u56fe\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u5728\u7ebf\u4e8c\u624b\u5e02\u573a\u4e2d\uff0c\u591a\u8f6e\u8bae\u4ef7\u662f\u4e70\u5356\u53cc\u65b9\u4e92\u52a8\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u5145\u5f53\u5356\u5bb6\u4ee3\u7406\uff0c\u4ee3\u8868\u5356\u5bb6\u5728\u7ed9\u5b9a\u7684\u4e1a\u52a1\u7ea6\u675f\u4e0b\u4e0e\u4e70\u5bb6\u8c08\u5224\u3002\u5bf9\u4e8e\u6b64\u7c7b\u4ee3\u7406\u6765\u8bf4\uff0c\u4e00\u4e2a\u5173\u952e\u80fd\u529b\u662f\u8ddf\u8e2a\u548c\u51c6\u786e\u89e3\u91ca\u957f\u671f\u8c08\u5224\u4e2d\u7d2f\u79ef\u7684\u4e70\u5bb6\u610f\u56fe\uff0c\u8fd9\u76f4\u63a5\u5f71\u54cd\u8bae\u4ef7\u6548\u7387\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u591a\u8f6e\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8861\u91cf\u7535\u5b50\u5546\u52a1\u5bf9\u8bdd\u4e2d\u5356\u5bb6\u4ee3\u7406\u7684\u8bae\u4ef7\u80fd\u529b\u3002\u8be5\u6846\u67b6\u6d4b\u8bd5\u4ee3\u7406\u662f\u5426\u53ef\u4ee5\u63d0\u53d6\u548c\u8ddf\u8e2a\u4e70\u5bb6\u610f\u56fe\u3002", "result": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u7535\u5b50\u5546\u52a1\u8bae\u4ef7\u57fa\u51c6\uff0c\u6db5\u76d6 622 \u4e2a\u7c7b\u522b\u30019,892 \u79cd\u4ea7\u54c1\u548c 3,014 \u4e2a\u4efb\u52a1\uff1b\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5fc3\u7406\u7406\u8bba\uff08ToM\uff09\u7684turn-level\u8bc4\u4f30\u6846\u67b6\uff0c\u5e26\u6709\u6ce8\u91ca\u7684\u4e70\u5bb6\u610f\u56fe\uff0c\u8d85\u8d8a\u4e86\u4ec5\u5173\u6ce8\u7ed3\u679c\u7684\u6307\u6807\uff1b\u6784\u5efa\u4e86\u4e00\u4e2a\u81ea\u52a8\u7ba1\u9053\uff0c\u53ef\u4ee5\u4ece\u5927\u91cf\u5bf9\u8bdd\u6570\u636e\u4e2d\u63d0\u53d6\u53ef\u9760\u7684\u610f\u56fe\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30 LLM \u8bae\u4ef7\u80fd\u529b\u7684\u6846\u67b6\uff0c\u5e76\u6784\u5efa\u4e86\u76f8\u5e94\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.05604", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05604", "abs": "https://arxiv.org/abs/2509.05604", "authors": ["Jungin Park", "Jiyoung Lee", "Kwanghoon Sohn"], "title": "Language-guided Recursive Spatiotemporal Graph Modeling for Video Summarization", "comment": "Accepted to IJCV, 29 pages, 14 figures, 11 tables", "summary": "Video summarization aims to select keyframes that are visually diverse and\ncan represent the whole story of a given video. Previous approaches have\nfocused on global interlinkability between frames in a video by temporal\nmodeling. However, fine-grained visual entities, such as objects, are also\nhighly related to the main content of the video. Moreover, language-guided\nvideo summarization, which has recently been studied, requires a comprehensive\nlinguistic understanding of complex real-world videos. To consider how all the\nobjects are semantically related to each other, this paper regards video\nsummarization as a language-guided spatiotemporal graph modeling problem. We\npresent recursive spatiotemporal graph networks, called VideoGraph, which\nformulate the objects and frames as nodes of the spatial and temporal graphs,\nrespectively. The nodes in each graph are connected and aggregated with graph\nedges, representing the semantic relationships between the nodes. To prevent\nthe edges from being configured with visual similarity, we incorporate language\nqueries derived from the video into the graph node representations, enabling\nthem to contain semantic knowledge. In addition, we adopt a recursive strategy\nto refine initial graphs and correctly classify each frame node as a keyframe.\nIn our experiments, VideoGraph achieves state-of-the-art performance on several\nbenchmarks for generic and query-focused video summarization in both supervised\nand unsupervised manners. The code is available at\nhttps://github.com/park-jungin/videograph.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u6458\u8981\u65b9\u6cd5\uff0c\u5b83\u5c06\u89c6\u9891\u6458\u8981\u89c6\u4e3a\u4e00\u4e2a\u8bed\u8a00\u5f15\u5bfc\u7684\u65f6\u7a7a\u56fe\u5efa\u6a21\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u6784\u5efa\u4e86\u4e00\u4e2a\u9012\u5f52\u65f6\u7a7a\u56fe\u7f51\u7edc\uff0c\u79f0\u4e3aVideoGraph\uff0c\u5b83\u5c06\u5bf9\u8c61\u548c\u5e27\u5206\u522b\u8868\u793a\u4e3a\u7a7a\u95f4\u548c\u65f6\u95f4\u56fe\u7684\u8282\u70b9\u3002\u4e3a\u4e86\u9632\u6b62\u8fb9\u88ab\u914d\u7f6e\u4e3a\u89c6\u89c9\u76f8\u4f3c\u6027\uff0c\u8be5\u65b9\u6cd5\u5c06\u4ece\u89c6\u9891\u4e2d\u5bfc\u51fa\u7684\u8bed\u8a00\u67e5\u8be2\u5408\u5e76\u5230\u56fe\u8282\u70b9\u8868\u793a\u4e2d\uff0c\u4f7f\u5b83\u4eec\u80fd\u591f\u5305\u542b\u8bed\u4e49\u77e5\u8bc6\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u91c7\u7528\u4e86\u4e00\u79cd\u9012\u5f52\u7b56\u7565\u6765\u7ec6\u5316\u521d\u59cb\u56fe\uff0c\u5e76\u5c06\u6bcf\u4e2a\u5e27\u8282\u70b9\u6b63\u786e\u5206\u7c7b\u4e3a\u5173\u952e\u5e27\u3002", "motivation": "\u4ee5\u524d\u7684\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u901a\u8fc7\u65f6\u95f4\u5efa\u6a21\u6765\u5efa\u7acb\u89c6\u9891\u4e2d\u5e27\u4e4b\u95f4\u7684\u5168\u5c40\u4e92\u8fde\u6027\u3002\u7136\u800c\uff0c\u7ec6\u7c92\u5ea6\u7684\u89c6\u89c9\u5b9e\u4f53\uff0c\u5982\u5bf9\u8c61\uff0c\u4e5f\u4e0e\u89c6\u9891\u7684\u4e3b\u8981\u5185\u5bb9\u9ad8\u5ea6\u76f8\u5173\u3002\u6b64\u5916\uff0c\u6700\u8fd1\u7814\u7a76\u7684\u8bed\u8a00\u5f15\u5bfc\u89c6\u9891\u6458\u8981\u9700\u8981\u5bf9\u590d\u6742\u7684\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u8fdb\u884c\u5168\u9762\u7684\u8bed\u8a00\u7406\u89e3\u3002", "method": "\u8be5\u65b9\u6cd5\u63d0\u51fa\u4e86\u4e00\u79cd\u9012\u5f52\u65f6\u7a7a\u56fe\u7f51\u7edc\uff0c\u79f0\u4e3aVideoGraph\uff0c\u5b83\u5c06\u5bf9\u8c61\u548c\u5e27\u5206\u522b\u8868\u793a\u4e3a\u7a7a\u95f4\u548c\u65f6\u95f4\u56fe\u7684\u8282\u70b9\u3002\u56fe\u4e2d\u7684\u8282\u70b9\u901a\u8fc7\u56fe\u8fb9\u8fde\u63a5\u548c\u805a\u5408\uff0c\u8868\u793a\u8282\u70b9\u4e4b\u95f4\u7684\u8bed\u4e49\u5173\u7cfb\u3002\u4e3a\u4e86\u9632\u6b62\u8fb9\u88ab\u914d\u7f6e\u4e3a\u89c6\u89c9\u76f8\u4f3c\u6027\uff0c\u8be5\u65b9\u6cd5\u5c06\u4ece\u89c6\u9891\u4e2d\u5bfc\u51fa\u7684\u8bed\u8a00\u67e5\u8be2\u5408\u5e76\u5230\u56fe\u8282\u70b9\u8868\u793a\u4e2d\uff0c\u4f7f\u5b83\u4eec\u80fd\u591f\u5305\u542b\u8bed\u4e49\u77e5\u8bc6\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u91c7\u7528\u4e86\u4e00\u79cd\u9012\u5f52\u7b56\u7565\u6765\u7ec6\u5316\u521d\u59cb\u56fe\uff0c\u5e76\u5c06\u6bcf\u4e2a\u5e27\u8282\u70b9\u6b63\u786e\u5206\u7c7b\u4e3a\u5173\u952e\u5e27\u3002", "result": "\u5728\u901a\u7528\u548cquery-focused\u7684\u89c6\u9891\u6458\u8981\u7684\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVideoGraph\u53d6\u5f97\u4e86state-of-the-art\u7684\u6027\u80fd\uff0c\u5305\u62ec\u76d1\u7763\u548c\u975e\u76d1\u7763\u7684\u65b9\u5f0f\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u6458\u8981\u65b9\u6cd5\uff0c\u5b83\u5c06\u89c6\u9891\u6458\u8981\u89c6\u4e3a\u4e00\u4e2a\u8bed\u8a00\u5f15\u5bfc\u7684\u65f6\u7a7a\u56fe\u5efa\u6a21\u95ee\u9898\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u901a\u7528\u548cquery-focused\u7684\u89c6\u9891\u6458\u8981\u7684\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86state-of-the-art\u7684\u6027\u80fd\u3002"}}
{"id": "2509.05839", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05839", "abs": "https://arxiv.org/abs/2509.05839", "authors": ["Daksh Mittal", "Shunri Zheng", "Jing Dong", "Hongseok Namkoong"], "title": "Data-Driven Stochastic Modeling Using Autoregressive Sequence Models: Translating Event Tables to Queueing Dynamics", "comment": null, "summary": "While queueing network models are powerful tools for analyzing service\nsystems, they traditionally require substantial human effort and domain\nexpertise to construct. To make this modeling approach more scalable and\naccessible, we propose a data-driven framework for queueing network modeling\nand simulation based on autoregressive sequence models trained on event-stream\ndata. Instead of explicitly specifying arrival processes, service mechanisms,\nor routing logic, our approach learns the conditional distributions of event\ntypes and event times, recasting the modeling task as a problem of sequence\ndistribution learning. We show that Transformer-style architectures can\neffectively parameterize these distributions, enabling automated construction\nof high-fidelity simulators. As a proof of concept, we validate our framework\non event tables generated from diverse queueing networks, showcasing its\nutility in simulation, uncertainty quantification, and counterfactual\nevaluation. Leveraging advances in artificial intelligence and the growing\navailability of data, our framework takes a step toward more automated,\ndata-driven modeling pipelines to support broader adoption of queueing network\nmodels across service domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u56de\u5f52\u5e8f\u5217\u6a21\u578b\u7684\u6392\u961f\u7f51\u7edc\u5efa\u6a21\u548c\u4eff\u771f\u6570\u636e\u9a71\u52a8\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5b66\u4e60\u4e8b\u4ef6\u7c7b\u578b\u548c\u4e8b\u4ef6\u65f6\u95f4\u7684\u6761\u4ef6\u5206\u5e03\uff0c\u5c06\u5efa\u6a21\u4efb\u52a1\u8f6c\u5316\u4e3a\u5e8f\u5217\u5206\u5e03\u5b66\u4e60\u95ee\u9898\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u4fdd\u771f\u6a21\u62df\u5668\u7684\u81ea\u52a8\u6784\u5efa\u3002", "motivation": "\u4f20\u7edf\u7684\u6392\u961f\u7f51\u7edc\u6a21\u578b\u9700\u8981\u5927\u91cf\u7684\u4eba\u529b\u548c\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u6765\u6784\u5efa\uff0c\u4e3a\u4e86\u4f7f\u8fd9\u79cd\u5efa\u6a21\u65b9\u6cd5\u66f4\u5177\u53ef\u6269\u5c55\u6027\u548c\u53ef\u8bbf\u95ee\u6027\u3002", "method": "\u57fa\u4e8e\u5728\u4e8b\u4ef6\u6d41\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u81ea\u56de\u5f52\u5e8f\u5217\u6a21\u578b\uff0c\u5b66\u4e60\u4e8b\u4ef6\u7c7b\u578b\u548c\u4e8b\u4ef6\u65f6\u95f4\u7684\u6761\u4ef6\u5206\u5e03\u3002", "result": "Transformer\u98ce\u683c\u7684\u67b6\u6784\u53ef\u4ee5\u6709\u6548\u5730\u53c2\u6570\u5316\u8fd9\u4e9b\u5206\u5e03\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u4fdd\u771f\u6a21\u62df\u5668\u7684\u81ea\u52a8\u6784\u5efa\u3002\u5728\u4ece\u4e0d\u540c\u7684\u6392\u961f\u7f51\u7edc\u751f\u6210\u7684\u4e8b\u4ef6\u8868\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u5176\u5728\u6a21\u62df\u3001\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u53cd\u4e8b\u5b9e\u8bc4\u4f30\u4e2d\u7684\u6548\u7528\u3002", "conclusion": "\u8be5\u6846\u67b6\u5229\u7528\u4eba\u5de5\u667a\u80fd\u7684\u8fdb\u6b65\u548c\u65e5\u76ca\u589e\u957f\u7684\u6570\u636e\u53ef\u7528\u6027\uff0c\u671d\u7740\u66f4\u81ea\u52a8\u5316\u3001\u6570\u636e\u9a71\u52a8\u7684\u5efa\u6a21\u7ba1\u9053\u8fc8\u51fa\u4e86\u4e00\u6b65\uff0c\u4ee5\u652f\u6301\u5728\u670d\u52a1\u9886\u57df\u66f4\u5e7f\u6cdb\u5730\u91c7\u7528\u6392\u961f\u7f51\u7edc\u6a21\u578b\u3002"}}
{"id": "2509.06065", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06065", "abs": "https://arxiv.org/abs/2509.06065", "authors": ["Lorenzo Alfred Nery", "Ronald Dawson Catignas", "Thomas James Tiam-Lee"], "title": "KatotohananQA: Evaluating Truthfulness of Large Language Models in Filipino", "comment": "14 pages, 1 figure, 9 tables, 1 listing. To appear in Proceedings of\n  NLPIR 2025", "summary": "Large Language Models (LLMs) achieve remarkable performance across various\ntasks, but their tendency to produce hallucinations limits reliable adoption.\nBenchmarks such as TruthfulQA have been developed to measure truthfulness, yet\nthey are primarily available in English, leaving a gap in evaluating LLMs in\nlow-resource languages. To address this, we present KatotohananQA, a Filipino\ntranslation of the TruthfulQA benchmark. Seven free-tier proprietary models\nwere assessed using a binary-choice framework. Findings show a significant\nperformance gap between English and Filipino truthfulness, with newer OpenAI\nmodels (GPT-5 and GPT-5 mini) demonstrating strong multilingual robustness.\nResults also reveal disparities across question characteristics, suggesting\nthat some question types, categories, and topics are less robust to\nmultilingual transfer which highlight the need for broader multilingual\nevaluation to ensure fairness and reliability in LLM usage.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86KatotohananQA\uff0c\u4e00\u4e2aTruthfulQA\u57fa\u51c6\u7684\u83f2\u5f8b\u5bbe\u8bed\u7ffb\u8bd1\u7248\u672c\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u7684\u771f\u5b9e\u6027\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30LLM\u771f\u5b9e\u6027\u7684\u57fa\u51c6\u4e3b\u8981\u4e3a\u82f1\u8bed\uff0c\u7f3a\u4e4f\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u8bc4\u4f30\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u4e8c\u5143\u9009\u62e9\u6846\u67b6\u8bc4\u4f30\u4e86\u4e03\u4e2a\u514d\u8d39\u4e13\u6709\u6a21\u578b\u5728KatotohananQA\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u82f1\u8bed\u548c\u83f2\u5f8b\u5bbe\u8bed\u7684\u771f\u5b9e\u6027\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\uff0cOpenAI\u7684GPT-5\u548cGPT-5 mini\u6a21\u578b\u8868\u73b0\u51fa\u8f83\u5f3a\u7684\u591a\u8bed\u8a00\u9c81\u68d2\u6027\u3002\u4e0d\u540c\u95ee\u9898\u7c7b\u578b\u3001\u7c7b\u522b\u548c\u4e3b\u9898\u7684\u8868\u73b0\u4e5f\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u67d0\u4e9b\u95ee\u9898\u7c7b\u578b\u3001\u7c7b\u522b\u548c\u4e3b\u9898\u7684\u591a\u8bed\u8a00\u8fc1\u79fb\u80fd\u529b\u8f83\u5dee\uff0c\u5f3a\u8c03\u9700\u8981\u66f4\u5e7f\u6cdb\u7684\u591a\u8bed\u8a00\u8bc4\u4f30\uff0c\u4ee5\u786e\u4fddLLM\u4f7f\u7528\u7684\u516c\u5e73\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2509.06355", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06355", "abs": "https://arxiv.org/abs/2509.06355", "authors": ["Yunzhe Wang", "Volkan Ustun", "Chris McGroarty"], "title": "A data-driven discretized CS:GO simulation environment to facilitate strategic multi-agent planning research", "comment": "Accepted at the Winter Simulation Conference 2025, December, Seattle\n  USA", "summary": "Modern simulation environments for complex multi-agent interactions must\nbalance high-fidelity detail with computational efficiency. We present DECOY, a\nnovel multi-agent simulator that abstracts strategic, long-horizon planning in\n3D terrains into high-level discretized simulation while preserving low-level\nenvironmental fidelity. Using Counter-Strike: Global Offensive (CS:GO) as a\ntestbed, our framework accurately simulates gameplay using only movement\ndecisions as tactical positioning -- without explicitly modeling low-level\nmechanics such as aiming and shooting. Central to our approach is a waypoint\nsystem that simplifies and discretizes continuous states and actions, paired\nwith neural predictive and generative models trained on real CS:GO tournament\ndata to reconstruct event outcomes. Extensive evaluations show that replays\ngenerated from human data in DECOY closely match those observed in the original\ngame. Our publicly available simulation environment provides a valuable tool\nfor advancing research in strategic multi-agent planning and behavior\ngeneration.", "AI": {"tldr": "DECOY is a multi-agent simulator that balances high-fidelity detail with computational efficiency by abstracting strategic, long-horizon planning in 3D terrains into high-level discretized simulation while preserving low-level environmental fidelity.", "motivation": "The need to balance high-fidelity detail with computational efficiency in modern simulation environments for complex multi-agent interactions.", "method": "A waypoint system simplifies and discretizes continuous states and actions, paired with neural predictive and generative models trained on real CS:GO tournament data to reconstruct event outcomes.", "result": "Replays generated from human data in DECOY closely match those observed in the original game.", "conclusion": "DECOY provides a valuable tool for advancing research in strategic multi-agent planning and behavior generation."}}
{"id": "2509.05606", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05606", "abs": "https://arxiv.org/abs/2509.05606", "authors": ["Juan Yeo", "Ijun Jang", "Taesup Kim"], "title": "Patch-level Kernel Alignment for Self-Supervised Dense Representation Learning", "comment": null, "summary": "Dense representations are essential for vision tasks that require spatial\nprecision and fine-grained detail. While most self-supervised representation\nlearning methods focus on global representations that summarize the image as a\nwhole, such approaches often fall short in capturing the localized semantics\nnecessary for dense prediction tasks. To overcome these limitations, we propose\na framework that builds on pretrained representations through additional\nself-supervised learning, aiming to transfer existing semantic knowledge into\nthe dense feature space. Our method aligns the distributions of dense features\nbetween a teacher and a student model. Specifically, we introduce Patch-level\nKernel Alignment (PaKA), a simple yet effective alignment objective that\ncaptures statistical dependencies, thereby matching the structural\nrelationships of dense patches across the two models. In addition, we\ninvestigate augmentation strategies specifically designed for dense\nrepresentation learning. Our framework achieves state-of-the-art results across\na variety of dense vision benchmarks, demonstrating the effectiveness of our\napproach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u989d\u5916\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u5bc6\u96c6\u7279\u5f81\u7a7a\u95f4\u4e2d\u8fc1\u79fb\u73b0\u6709\u7684\u8bed\u4e49\u77e5\u8bc6\uff0c\u4ece\u800c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6355\u83b7\u5c40\u90e8\u8bed\u4e49\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u76d1\u7763\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u603b\u7ed3\u56fe\u50cf\u6574\u4f53\u7684\u5168\u5c40\u8868\u793a\uff0c\u4f46\u5728\u6355\u83b7\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u6240\u9700\u7684\u5c40\u90e8\u8bed\u4e49\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5bf9\u9f50\u6559\u5e08\u6a21\u578b\u548c\u5b66\u751f\u6a21\u578b\u4e4b\u95f4\u7684\u5bc6\u96c6\u7279\u5f81\u5206\u5e03\u6765\u5b9e\u73b0\u77e5\u8bc6\u8fc1\u79fb\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5f15\u5165\u4e86Patch-level Kernel Alignment (PaKA)\uff0c\u8fd9\u662f\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u5bf9\u9f50\u76ee\u6807\uff0c\u53ef\u4ee5\u6355\u83b7\u7edf\u8ba1\u4f9d\u8d56\u5173\u7cfb\uff0c\u4ece\u800c\u5339\u914d\u4e24\u4e2a\u6a21\u578b\u4e4b\u95f4\u5bc6\u96c6patch\u7684\u7ed3\u6784\u5173\u7cfb\u3002\u6b64\u5916\uff0c\u8fd8\u7814\u7a76\u4e86\u4e13\u95e8\u4e3a\u5bc6\u96c6\u8868\u793a\u5b66\u4e60\u8bbe\u8ba1\u7684\u589e\u5f3a\u7b56\u7565\u3002", "result": "\u8be5\u6846\u67b6\u5728\u4e00\u7cfb\u5217\u5bc6\u96c6\u89c6\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u662f\u6709\u6548\u7684\u3002"}}
{"id": "2509.05865", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.05865", "abs": "https://arxiv.org/abs/2509.05865", "authors": ["Rishabh Dixit", "Yuan Hui", "Rayan Saab"], "title": "The Measure of Deception: An Analysis of Data Forging in Machine Unlearning", "comment": null, "summary": "Motivated by privacy regulations and the need to mitigate the effects of\nharmful data, machine unlearning seeks to modify trained models so that they\neffectively ``forget'' designated data. A key challenge in verifying unlearning\nis forging -- adversarially crafting data that mimics the gradient of a target\npoint, thereby creating the appearance of unlearning without actually removing\ninformation. To capture this phenomenon, we consider the collection of data\npoints whose gradients approximate a target gradient within tolerance\n$\\epsilon$ -- which we call an $\\epsilon$-forging set -- and develop a\nframework for its analysis. For linear regression and one-layer neural\nnetworks, we show that the Lebesgue measure of this set is small. It scales on\nthe order of $\\epsilon$, and when $\\epsilon$ is small enough, $\\epsilon^d$.\nMore generally, under mild regularity assumptions, we prove that the forging\nset measure decays as $\\epsilon^{(d-r)/2}$, where $d$ is the data dimension and\n$r<d$ is the nullity of a variation matrix defined by the model gradients.\nExtensions to batch SGD and almost-everywhere smooth loss functions yield the\nsame asymptotic scaling. In addition, we establish probability bounds showing\nthat, under non-degenerate data distributions, the likelihood of randomly\nsampling a forging point is vanishingly small. These results provide evidence\nthat adversarial forging is fundamentally limited and that false unlearning\nclaims can, in principle, be detected.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u201c\u9057\u5fd8\u5b66\u4e60\u201d\u95ee\u9898\uff0c\u91cd\u70b9\u5173\u6ce8\u4e86\u5bf9\u6297\u6027\u4f2a\u9020\u6570\u636e\u4ee5\u6b3a\u9a97\u9057\u5fd8\u5b66\u4e60\u9a8c\u8bc1\u7684\u6311\u6218\u3002", "motivation": "\u9690\u79c1\u6cd5\u89c4\u548c\u51cf\u8f7b\u6709\u5bb3\u6570\u636e\u5f71\u54cd\u7684\u9700\u6c42\u4fc3\u4f7f\u4e86\u673a\u5668\u5b66\u4e60\u9057\u5fd8\u5b66\u4e60\u7684\u53d1\u5c55\u3002\u9a8c\u8bc1\u9057\u5fd8\u5b66\u4e60\u7684\u4e00\u4e2a\u5173\u952e\u6311\u6218\u662f\u4f2a\u9020\u6570\u636e\uff0c\u5373\u5bf9\u6297\u6027\u5730\u5236\u4f5c\u6a21\u4eff\u76ee\u6807\u70b9\u68af\u5ea6\u7684\u5047\u6570\u636e\uff0c\u4ece\u800c\u5728\u4e0d\u771f\u6b63\u79fb\u9664\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u9020\u6210\u9057\u5fd8\u7684\u5047\u8c61\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u6790\u6846\u67b6\uff0c\u7528\u4e8e\u7814\u7a76\u68af\u5ea6\u5728\u5bb9\u5dee $\\epsilon$ \u5185\u8fd1\u4f3c\u4e8e\u76ee\u6807\u68af\u5ea6\u7684\u4f2a\u9020\u6570\u636e\u96c6\u7684\u96c6\u5408\u3002\u9488\u5bf9\u7ebf\u6027\u56de\u5f52\u548c\u5355\u5c42\u795e\u7ecf\u7f51\u7edc\uff0c\u5206\u6790\u4e86\u8be5\u96c6\u5408\u7684\u52d2\u8d1d\u683c\u6d4b\u5ea6\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5bf9\u4e8e\u7ebf\u6027\u56de\u5f52\u548c\u5355\u5c42\u795e\u7ecf\u7f51\u7edc\uff0c\u4f2a\u9020\u6570\u636e\u96c6\u7684\u52d2\u8d1d\u683c\u6d4b\u5ea6\u5f88\u5c0f\uff0c\u5176\u89c4\u6a21\u4e0e $\\epsilon$ \u6210\u6bd4\u4f8b\uff0c\u5f53 $\\epsilon$ \u8db3\u591f\u5c0f\u65f6\uff0c\u4e0e $\\epsilon^d$ \u6210\u6bd4\u4f8b\u3002\u5728\u6e29\u548c\u7684\u6b63\u5219\u6027\u5047\u8bbe\u4e0b\uff0c\u8bc1\u660e\u4e86\u4f2a\u9020\u96c6\u6d4b\u5ea6\u8870\u51cf\u4e3a $\\epsilon^{(d-r)/2}$\uff0c\u5176\u4e2d $d$ \u662f\u6570\u636e\u7ef4\u5ea6\uff0c$r<d$ \u662f\u7531\u6a21\u578b\u68af\u5ea6\u5b9a\u4e49\u7684\u53d8\u5206\u77e9\u9635\u7684\u96f6\u5ea6\u3002\u6b64\u5916\uff0c\u8fd8\u5efa\u7acb\u4e86\u6982\u7387\u754c\u9650\uff0c\u8868\u660e\u5728\u975e\u9000\u5316\u7684\u6570\u636e\u5206\u5e03\u4e0b\uff0c\u968f\u673a\u62bd\u6837\u4f2a\u9020\u70b9\u7684\u53ef\u80fd\u6027\u975e\u5e38\u5c0f\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u6297\u6027\u4f2a\u9020\u4ece\u6839\u672c\u4e0a\u53d7\u5230\u9650\u5236\uff0c\u5e76\u4e14\u539f\u5219\u4e0a\u53ef\u4ee5\u68c0\u6d4b\u5230\u865a\u5047\u7684\u9057\u5fd8\u5b66\u4e60\u58f0\u660e\u3002"}}
{"id": "2509.06074", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06074", "abs": "https://arxiv.org/abs/2509.06074", "authors": ["Zhenqi Jia", "Rui Liu", "Berrak Sisman", "Haizhou Li"], "title": "Multimodal Fine-grained Context Interaction Graph Modeling for Conversational Speech Synthesis", "comment": "Accepted by EMNLP 2025", "summary": "Conversational Speech Synthesis (CSS) aims to generate speech with natural\nprosody by understanding the multimodal dialogue history (MDH). The latest work\npredicts the accurate prosody expression of the target utterance by modeling\nthe utterance-level interaction characteristics of MDH and the target\nutterance. However, MDH contains fine-grained semantic and prosody knowledge at\nthe word level. Existing methods overlook the fine-grained semantic and\nprosodic interaction modeling. To address this gap, we propose MFCIG-CSS, a\nnovel Multimodal Fine-grained Context Interaction Graph-based CSS system. Our\napproach constructs two specialized multimodal fine-grained dialogue\ninteraction graphs: a semantic interaction graph and a prosody interaction\ngraph. These two interaction graphs effectively encode interactions between\nword-level semantics, prosody, and their influence on subsequent utterances in\nMDH. The encoded interaction features are then leveraged to enhance synthesized\nspeech with natural conversational prosody. Experiments on the DailyTalk\ndataset demonstrate that MFCIG-CSS outperforms all baseline models in terms of\nprosodic expressiveness. Code and speech samples are available at\nhttps://github.com/AI-S2-Lab/MFCIG-CSS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u591a\u6a21\u6001\u7ec6\u7c92\u5ea6\u4e0a\u4e0b\u6587\u4ea4\u4e92\u56fe\u7684CSS\u7cfb\u7edf(MFCIG-CSS)\uff0c\u7528\u4e8e\u751f\u6210\u5177\u6709\u81ea\u7136\u97f5\u5f8b\u7684\u8bed\u97f3\u3002", "motivation": "\u73b0\u6709\u7684\u5bf9\u8bdd\u8bed\u97f3\u5408\u6210(CSS)\u65b9\u6cd5\u5ffd\u7565\u4e86\u7ec6\u7c92\u5ea6\u7684\u8bed\u4e49\u548c\u97f5\u5f8b\u4ea4\u4e92\u5efa\u6a21\u3002", "method": "\u6784\u5efa\u4e86\u8bed\u4e49\u4ea4\u4e92\u56fe\u548c\u97f5\u5f8b\u4ea4\u4e92\u56fe\uff0c\u6709\u6548\u5730\u7f16\u7801\u4e86\u8bcd\u7ea7\u7684\u8bed\u4e49\u3001\u97f5\u5f8b\u4ee5\u53ca\u5b83\u4eec\u5728MDH\u4e2d\u5bf9\u540e\u7eed\u8bdd\u8bed\u7684\u5f71\u54cd\u3002", "result": "\u5728DailyTalk\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMFCIG-CSS\u5728\u97f5\u5f8b\u8868\u73b0\u529b\u65b9\u9762\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "MFCIG-CSS\u80fd\u591f\u63d0\u5347\u5408\u6210\u8bed\u97f3\u7684\u81ea\u7136\u5bf9\u8bdd\u97f5\u5f8b\u3002"}}
{"id": "2509.06409", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06409", "abs": "https://arxiv.org/abs/2509.06409", "authors": ["Yihong Luo", "Wenwu He", "Zhuo-Xu Cui", "Dong Liang"], "title": "Teaching AI Stepwise Diagnostic Reasoning with Report-Guided Chain-of-Thought Learning", "comment": null, "summary": "This study presents DiagCoT, a multi-stage framework that applies supervised\nfine-tuning to general-purpose vision-language models (VLMs) to emulate\nradiologists' stepwise diagnostic reasoning using only free-text reports.\nDiagCoT combines contrastive image-report tuning for domain alignment,\nchain-of-thought supervision to capture inferential logic, and reinforcement\ntuning with clinical reward signals to enhance factual accuracy and fluency. On\nthe MIMIC-CXR benchmark, DiagCoT improved zero-shot disease classification AUC\nfrom 0.52 to 0.76 (absolute gain of 0.24), pathology grounding mIoU from 0.08\nto 0.31 (absolute gain of 0.23), and report generation BLEU from 0.11 to 0.33\n(absolute gain of 0.22). It outperformed state-of-the-art models including\nLLaVA-Med and CXR-LLAVA on long-tailed diseases and external datasets. By\nconverting unstructured clinical narratives into structured supervision,\nDiagCoT offers a scalable approach for developing interpretable and\ndiagnostically competent AI systems for radiology.", "AI": {"tldr": "DiagCoT: A multi-stage framework using VLMs to mimic radiologists' diagnostic reasoning.", "motivation": "Developing interpretable and diagnostically competent AI systems for radiology is challenging.", "method": "Multi-stage framework with contrastive image-report tuning, chain-of-thought supervision, and reinforcement tuning.", "result": "Improved zero-shot disease classification AUC to 0.76, pathology grounding mIoU to 0.31, and report generation BLEU to 0.33 on MIMIC-CXR.", "conclusion": "DiagCoT offers a scalable approach for developing AI systems for radiology by converting unstructured clinical narratives into structured supervision."}}
{"id": "2509.05614", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.05614", "abs": "https://arxiv.org/abs/2509.05614", "authors": ["Hanzhen Wang", "Jiaming Xu", "Jiayi Pan", "Yongkang Zhou", "Guohao Dai"], "title": "SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning", "comment": "8pages, 10 figures,", "summary": "Pruning accelerates compute-bound models by reducing computation. Recently\napplied to Vision-Language-Action (VLA) models, existing methods prune tokens\nusing only local info from current action, ignoring global context from prior\nactions, causing >20% success rate drop and limited speedup. We observe high\nsimilarity across consecutive actions and propose leveraging both local\n(current) and global (past) info for smarter token selection. We introduce\nSpecPrune-VLA, a training-free method with two-level pruning and heuristic\ncontrol: (1) Static pruning at action level: uses global history and local\ncontext to reduce visual tokens per action; (2) Dynamic pruning at layer level:\nprunes tokens per layer based on layer-specific importance; (3) Lightweight\naction-aware controller: classifies actions as coarse/fine-grained (by speed),\nadjusting pruning aggressiveness since fine-grained actions are\npruning-sensitive. Experiments on LIBERO show SpecPrune-VLA achieves 1.46 times\nspeedup on NVIDIA A800 and 1.57 times on NVIDIA GeForce RTX 3090 vs.\nOpenVLA-OFT, with negligible success rate loss.", "AI": {"tldr": "SpecPrune-VLA: A training-free pruning method for Vision-Language-Action (VLA) models that uses both local and global information for smarter token selection.", "motivation": "Existing pruning methods for VLA models only use local information, ignoring global context from prior actions, leading to performance drops and limited speedup.", "method": "SpecPrune-VLA introduces a two-level pruning approach: static pruning at the action level using global history and local context, and dynamic pruning at the layer level based on layer-specific importance. It also uses a lightweight action-aware controller to adjust pruning aggressiveness based on action granularity.", "result": "SpecPrune-VLA achieves 1.46x speedup on NVIDIA A800 and 1.57x speedup on NVIDIA GeForce RTX 3090 compared to OpenVLA-OFT, with negligible success rate loss on the LIBERO dataset.", "conclusion": "SpecPrune-VLA effectively leverages both local and global information for token selection, achieving significant speedups with minimal performance loss in VLA models."}}
{"id": "2509.06079", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06079", "abs": "https://arxiv.org/abs/2509.06079", "authors": ["Hao Liang", "Ruitao Wu", "Bohan Zeng", "Junbo Niu", "Wentao Zhang", "Bin Dong"], "title": "Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge", "comment": null, "summary": "Multimodal reasoning remains a fundamental challenge in artificial\nintelligence. Despite substantial advances in text-based reasoning, even\nstate-of-the-art models such as GPT-o3 struggle to maintain strong performance\nin multimodal scenarios. To address this gap, we introduce a caption-assisted\nreasoning framework that effectively bridges visual and textual modalities. Our\napproach achieved 1st place in the ICML 2025 AI for Math Workshop \\& Challenge\n2: SeePhys, highlighting its effectiveness and robustness. Furthermore, we\nvalidate its generalization on the MathVerse benchmark for geometric reasoning,\ndemonstrating the versatility of our method. Our code is publicly available at\nhttps://github.com/OpenDCAI/SciReasoner.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7caption\u8f85\u52a9\u6765\u8fde\u63a5\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\uff0c\u5728ICML 2025 AI for Math Workshop & Challenge 2\u4e2d\u83b7\u5f97\u7b2c\u4e00\u540d\uff0c\u5e76\u5728MathVerse\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u63a8\u7406\u6a21\u578b\u5728\u591a\u6a21\u6001\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u5f15\u5165caption\u8f85\u52a9\u63a8\u7406\u6846\u67b6\uff0c\u6709\u6548\u6865\u63a5\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u3002", "result": "\u5728ICML 2025 AI for Math Workshop & Challenge 2: SeePhys\u4e2d\u83b7\u5f97\u7b2c\u4e00\u540d\uff0c\u5e76\u5728MathVerse\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u4e14\u5177\u6709\u9c81\u68d2\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2509.06436", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06436", "abs": "https://arxiv.org/abs/2509.06436", "authors": ["Song Yu", "Xiaofei Xu", "Ke Deng", "Li Li", "Lin Tian"], "title": "Tree of Agents: Improving Long-Context Capabilities of Large Language Models through Multi-Perspective Reasoning", "comment": "19 pages, 5 figures", "summary": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTree of Agents (TOA) \u7684\u591a\u4ee3\u7406\u63a8\u7406\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u6587\u672c\u4efb\u52a1\u65f6\u9047\u5230\u7684\u201c\u4e2d\u95f4\u4fe1\u606f\u4e22\u5931\u201d\u95ee\u9898\u3002TOA\u901a\u8fc7\u5206\u5272\u8f93\u5165\u3001\u591a\u4ee3\u7406\u534f\u540c\u63a8\u7406\u3001\u52a8\u6001\u4fe1\u606f\u4ea4\u6362\u548c\u6811\u72b6\u7ed3\u6784\u8def\u5f84\u6765\u7f13\u89e3\u4f4d\u7f6e\u504f\u5dee\u548c\u51cf\u5c11\u5e7b\u89c9\uff0c\u540c\u65f6\u5229\u7528\u524d\u7f00\u54c8\u5e0c\u7f13\u5b58\u548c\u81ea\u9002\u5e94\u526a\u679d\u7b56\u7565\u63d0\u9ad8\u5904\u7406\u6548\u7387\u3002\u5b9e\u9a8c\u8868\u660e\uff0cTOA\u5728\u957f\u6587\u672c\u4efb\u52a1\u4e0a\u4f18\u4e8e\u591a\u4e2a\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u80fd\u4e0e\u66f4\u5927\u7684\u5546\u4e1a\u6a21\u578b\u76f8\u5ab2\u7f8e\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u6587\u672c\u4efb\u52a1\u65f6\uff0c\u5b58\u5728\u201c\u4e2d\u95f4\u4fe1\u606f\u4e22\u5931\u201d\u7684\u95ee\u9898\uff0c\u5373\u8f93\u5165\u4e2d\u95f4\u90e8\u5206\u7684\u4fe1\u606f\u672a\u88ab\u5145\u5206\u5229\u7528\u3002\u73b0\u6709\u7684\u51cf\u5c11\u8f93\u5165\u7684\u65b9\u6cd5\u53ef\u80fd\u4f1a\u4e22\u5f03\u5173\u952e\u4fe1\u606f\uff0c\u800c\u6269\u5c55\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684\u65b9\u6cd5\u5219\u53ef\u80fd\u5bfc\u81f4\u6ce8\u610f\u529b\u5206\u6563\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86Tree of Agents (TOA) \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u8f93\u5165\u5206\u5272\u6210\u5757\uff0c\u7531\u72ec\u7acb\u4ee3\u7406\u5904\u7406\u3002\u6bcf\u4e2a\u4ee3\u7406\u751f\u6210\u5c40\u90e8\u8ba4\u77e5\uff0c\u7136\u540e\u4ee3\u7406\u4e4b\u95f4\u52a8\u6001\u4ea4\u6362\u4fe1\u606f\uff0c\u6cbf\u7740\u6811\u72b6\u7ed3\u6784\u7684\u8def\u5f84\u8fdb\u884c\u534f\u540c\u63a8\u7406\u3002TOA\u8fd8\u91c7\u7528\u4e86\u524d\u7f00\u54c8\u5e0c\u7f13\u5b58\u548c\u81ea\u9002\u5e94\u526a\u679d\u7b56\u7565\u6765\u63d0\u9ad8\u5904\u7406\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTOA\u5728\u957f\u6587\u672c\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u591a\u4e2a\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u80fd\u4e0e\u6700\u65b0\u7684\u3001\u66f4\u5927\u7684\u5546\u4e1a\u6a21\u578b\uff08\u5982Gemini1.5-pro\uff09\u76f8\u5ab2\u7f8e\u3002TOA\u7531\u7d27\u51d1\u7684LLaMA3.1-8B\u9a71\u52a8\u3002", "conclusion": "Tree of Agents (TOA) \u662f\u4e00\u79cd\u6709\u6548\u7684\u591a\u4ee3\u7406\u63a8\u7406\u6846\u67b6\uff0c\u53ef\u4ee5\u7f13\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u6587\u672c\u4efb\u52a1\u65f6\u9047\u5230\u7684\u201c\u4e2d\u95f4\u4fe1\u606f\u4e22\u5931\u201d\u95ee\u9898\uff0c\u5e76\u5728\u6548\u7387\u548c\u6027\u80fd\u4e0a\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u7684\u63d0\u5347\u3002"}}
{"id": "2509.05625", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05625", "abs": "https://arxiv.org/abs/2509.05625", "authors": ["Kien Nguyen", "Anh Tran", "Cuong Pham"], "title": "SuMa: A Subspace Mapping Approach for Robust and Effective Concept Erasure in Text-to-Image Diffusion Models", "comment": null, "summary": "The rapid growth of text-to-image diffusion models has raised concerns about\ntheir potential misuse in generating harmful or unauthorized contents. To\naddress these issues, several Concept Erasure methods have been proposed.\nHowever, most of them fail to achieve both robustness, i.e., the ability to\nrobustly remove the target concept., and effectiveness, i.e., maintaining image\nquality. While few recent techniques successfully achieve these goals for NSFW\nconcepts, none could handle narrow concepts such as copyrighted characters or\ncelebrities. Erasing these narrow concepts is critical in addressing copyright\nand legal concerns. However, erasing them is challenging due to their close\ndistances to non-target neighboring concepts, requiring finer-grained\nmanipulation. In this paper, we introduce Subspace Mapping (SuMa), a novel\nmethod specifically designed to achieve both robustness and effectiveness in\neasing these narrow concepts. SuMa first derives a target subspace representing\nthe concept to be erased and then neutralizes it by mapping it to a reference\nsubspace that minimizes the distance between the two. This mapping ensures the\ntarget concept is robustly erased while preserving image quality. We conduct\nextensive experiments with SuMa across four tasks: subclass erasure, celebrity\nerasure, artistic style erasure, and instance erasure and compare the results\nwith current state-of-the-art methods. Our method achieves image quality\ncomparable to approaches focused on effectiveness, while also yielding results\nthat are on par with methods targeting completeness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSubspace Mapping (SuMa) \u7684\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u7a33\u5065\u4e14\u6709\u6548\u5730\u6d88\u9664\u6269\u6563\u6a21\u578b\u4e2d\u7684\u7279\u5b9a\u6982\u5ff5\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u6982\u5ff5\u6d88\u9664\u65b9\u6cd5\u5728\u7a33\u5065\u6027\u548c\u6709\u6548\u6027\u4e4b\u95f4\u96be\u4ee5\u517c\u987e\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u7248\u6743\u4eba\u7269\u6216\u540d\u4eba\u7b49\u7a84\u6982\u5ff5\u65f6\u6548\u679c\u4e0d\u4f73\u3002\u6d88\u9664\u8fd9\u4e9b\u7a84\u6982\u5ff5\u5bf9\u4e8e\u89e3\u51b3\u7248\u6743\u548c\u6cd5\u5f8b\u95ee\u9898\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u5b83\u4eec\u4e0e\u975e\u76ee\u6807\u6982\u5ff5\u8ddd\u79bb\u8f83\u8fd1\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u64cd\u4f5c\u3002", "method": "\u9996\u5148\u63a8\u5bfc\u51fa\u4e00\u4e2a\u4ee3\u8868\u8981\u64e6\u9664\u6982\u5ff5\u7684\u76ee\u6807\u5b50\u7a7a\u95f4\uff0c\u7136\u540e\u901a\u8fc7\u5c06\u5176\u6620\u5c04\u5230\u53c2\u8003\u5b50\u7a7a\u95f4\u6765\u4e2d\u548c\u5b83\uff0c\u4ece\u800c\u6700\u5c0f\u5316\u4e24\u8005\u4e4b\u95f4\u7684\u8ddd\u79bb\u3002", "result": "\u5728\u56db\u4e2a\u4efb\u52a1\uff08\u5b50\u7c7b\u64e6\u9664\u3001\u540d\u4eba\u64e6\u9664\u3001\u827a\u672f\u98ce\u683c\u64e6\u9664\u548c\u5b9e\u4f8b\u64e6\u9664\uff09\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u8d28\u91cf\u4e0a\u4e0e\u6ce8\u91cd\u6709\u6548\u6027\u7684\u65b9\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u5728\u5b8c\u6574\u6027\u65b9\u9762\u4e5f\u4e0e\u76ee\u6807\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "SuMa \u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u6d88\u9664\u7a84\u6982\u5ff5\u7684\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u3002"}}
{"id": "2509.05886", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05886", "abs": "https://arxiv.org/abs/2509.05886", "authors": ["Reza Pirayeshshirazinezhad"], "title": "SPINN: An Optimal Self-Supervised Physics-Informed Neural Network Framework", "comment": null, "summary": "A surrogate model is developed to predict the convective heat transfer\ncoefficient of liquid sodium (Na) flow within rectangular miniature heat sinks.\nInitially, kernel-based machine learning techniques and shallow neural network\nare applied to a dataset with 87 Nusselt numbers for liquid sodium in\nrectangular miniature heat sinks. Subsequently, a self-supervised\nphysics-informed neural network and transfer learning approach are used to\nincrease the estimation performance. In the self-supervised physics-informed\nneural network, an additional layer determines the weight the of physics in the\nloss function to balance data and physics based on their uncertainty for a\nbetter estimation. For transfer learning, a shallow neural network trained on\nwater is adapted for use with Na. Validation results show that the\nself-supervised physics-informed neural network successfully estimate the heat\ntransfer rates of Na with an error margin of approximately +8%. Using only\nphysics for regression, the error remains between 5% to 10%. Other machine\nlearning methods specify the prediction mostly within +8%. High-fidelity\nmodeling of turbulent forced convection of liquid metals using computational\nfluid dynamics (CFD) is both time-consuming and computationally expensive.\nTherefore, machine learning based models offer a powerful alternative tool for\nthe design and optimization of liquid-metal-cooled miniature heat sinks.", "AI": {"tldr": "\u4f7f\u7528\u4ee3\u7406\u6a21\u578b\u9884\u6d4b\u77e9\u5f62\u5fae\u578b\u6563\u70ed\u5668\u5185\u6db2\u6001\u94a0 (Na) \u6d41\u52a8\u7684\u5bf9\u6d41\u6362\u70ed\u7cfb\u6570\u3002", "motivation": "\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66 (CFD) \u5bf9\u6db2\u6001\u91d1\u5c5e\u6e4d\u6d41\u5f3a\u5236\u5bf9\u6d41\u7684\u9ad8\u7cbe\u5ea6\u5efa\u6a21\u65e2\u8017\u65f6\u53c8\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u56e0\u6b64\uff0c\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u6a21\u578b\u4e3a\u6db2\u6001\u91d1\u5c5e\u51b7\u5374\u5fae\u578b\u6563\u70ed\u5668\u7684\u8bbe\u8ba1\u548c\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u7684\u66ff\u4ee3\u5de5\u5177\u3002", "method": "\u9996\u5148\uff0c\u5c06\u57fa\u4e8e\u6838\u7684\u673a\u5668\u5b66\u4e60\u6280\u672f\u548c\u6d45\u5c42\u795e\u7ecf\u7f51\u7edc\u5e94\u7528\u4e8e\u5177\u6709 87 \u4e2a\u77e9\u5f62\u5fae\u578b\u6563\u70ed\u5668\u4e2d\u6db2\u6001\u94a0\u7684\u52aa\u585e\u5c14\u6570\u7684\u6570\u636e\u96c6\u3002\u968f\u540e\uff0c\u4f7f\u7528\u81ea\u76d1\u7763\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u548c\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u6765\u63d0\u9ad8\u4f30\u8ba1\u6027\u80fd\u3002\u5728\u81ea\u76d1\u7763\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u4e2d\uff0c\u9644\u52a0\u5c42\u786e\u5b9a\u635f\u5931\u51fd\u6570\u4e2d\u7269\u7406\u7684\u6743\u91cd\uff0c\u4ee5\u6839\u636e\u6570\u636e\u548c\u7269\u7406\u7684\u4e0d\u786e\u5b9a\u6027\u6765\u5e73\u8861\u6570\u636e\u548c\u7269\u7406\uff0c\u4ece\u800c\u83b7\u5f97\u66f4\u597d\u7684\u4f30\u8ba1\u3002\u5bf9\u4e8e\u8fc1\u79fb\u5b66\u4e60\uff0c\u9002\u7528\u4e8e\u6c34\u7684\u6d45\u5c42\u795e\u7ecf\u7f51\u7edc\u9002\u7528\u4e8e Na\u3002", "result": "\u9a8c\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u81ea\u76d1\u7763\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u6210\u529f\u4f30\u8ba1\u4e86 Na \u7684\u4f20\u70ed\u7387\uff0c\u8bef\u5dee\u5e45\u5ea6\u7ea6\u4e3a +8%\u3002\u4ec5\u4f7f\u7528\u7269\u7406\u8fdb\u884c\u56de\u5f52\u65f6\uff0c\u8bef\u5dee\u4fdd\u6301\u5728 5% \u5230 10% \u4e4b\u95f4\u3002\u5176\u4ed6\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5728 +8% \u4ee5\u5185\u6307\u5b9a\u9884\u6d4b\u3002", "conclusion": "\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u6a21\u578b\u4e3a\u6db2\u6001\u91d1\u5c5e\u51b7\u5374\u5fae\u578b\u6563\u70ed\u5668\u7684\u8bbe\u8ba1\u548c\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u7684\u66ff\u4ee3\u5de5\u5177"}}
{"id": "2509.06100", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06100", "abs": "https://arxiv.org/abs/2509.06100", "authors": ["Kefan Cao", "Shuaicheng Wu"], "title": "Orthogonal Low-rank Adaptation in Lie Groups for Continual Learning of Large Language Models", "comment": "13 pages, 3 figures", "summary": "Large language models (LLMs) are prone to catastrophic forgetting in\nsequential multi-task settings. Parameter regularization methods such as O-LoRA\nand N-LoRA alleviate task interference by enforcing low-rank subspace\northogonality, but they overlook the fact that conventional additive\nfine-tuning disrupts the intrinsic geometric structure of LLM parameters,\nlimiting performance. Our key insight is that the parameter space of LLMs\npossesses a geometric structure, which must be preserved in addition to\nenforcing orthogonality. Based on this, we propose Orthogonal Low-rank\nAdaptation in Lie Groups (OLieRA), which introduces Lie group theory into LLM\nfine-tuning: leveraging multiplicative updates to preserve parameter geometry\nwhile applying orthogonality constraints to task subspaces. Experiments\ndemonstrate that OLieRA achieves state-of-the-art results on the Standard CL\nbenchmark and remains among the top-performing methods in the Large Number of\nTasks setting.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOLieRA\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u987a\u5e8f\u591a\u4efb\u52a1\u8bbe\u7f6e\u4e2d\u5bb9\u6613\u51fa\u73b0\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u589e\u91cf\u5fae\u8c03\u4f1a\u7834\u574fLLM\u53c2\u6570\u7684\u5185\u5728\u51e0\u4f55\u7ed3\u6784\uff0c\u9650\u5236\u6027\u80fd\u3002", "method": "\u5c06\u674e\u7fa4\u7406\u8bba\u5f15\u5165LLM\u5fae\u8c03\uff0c\u5229\u7528\u4e58\u6cd5\u66f4\u65b0\u6765\u4fdd\u6301\u53c2\u6570\u51e0\u4f55\u7ed3\u6784\uff0c\u540c\u65f6\u5bf9\u4efb\u52a1\u5b50\u7a7a\u95f4\u5e94\u7528\u6b63\u4ea4\u7ea6\u675f\u3002", "result": "\u5728Standard CL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5e76\u4e14\u5728\u5927\u91cf\u4efb\u52a1\u8bbe\u7f6e\u4e2d\u4ecd\u7136\u662f\u8868\u73b0\u6700\u4f73\u7684\u65b9\u6cd5\u4e4b\u4e00\u3002", "conclusion": "OLieRA\u65b9\u6cd5\u6709\u6548\u5730\u89e3\u51b3\u4e86\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u8bbe\u7f6e\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.06444", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06444", "abs": "https://arxiv.org/abs/2509.06444", "authors": ["Cheng Qian", "Hainan Zhang", "Yongxin Tong", "Hong-Wei Zheng", "Zhiming Zheng"], "title": "HyFedRAG: A Federated Retrieval-Augmented Generation Framework for Heterogeneous and Privacy-Sensitive Data", "comment": "9 pages, 7 figures", "summary": "Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive\ndata, especially in distributed healthcare settings where patient data spans\nSQL, knowledge graphs, and clinical notes. Clinicians face difficulties\nretrieving rare disease cases due to privacy constraints and the limitations of\ntraditional cloud-based RAG systems in handling diverse formats and edge\ndevices. To address this, we introduce HyFedRAG, a unified and efficient\nFederated RAG framework tailored for Hybrid data modalities. By leveraging an\nedge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across\ndiverse data sources while preserving data privacy. Our key contributions are:\n(1) We design an edge-cloud collaborative RAG framework built on Flower, which\nsupports querying structured SQL data, semi-structured knowledge graphs, and\nunstructured documents. The edge-side LLMs convert diverse data into\nstandardized privacy-preserving representations, and the server-side LLMs\nintegrates them for global reasoning and generation. (2) We integrate\nlightweight local retrievers with privacy-aware LLMs and provide three\nanonymization tools that enable each client to produce semantically rich,\nde-identified summaries for global inference across devices. (3) To optimize\nresponse latency and reduce redundant computation, we design a three-tier\ncaching strategy consisting of local cache, intermediate representation cache,\nand cloud inference cache. Experimental results on PMC-Patients demonstrate\nthat HyFedRAG outperforms existing baselines in terms of retrieval quality,\ngeneration consistency, and system efficiency. Our framework offers a scalable\nand privacy-compliant solution for RAG over structural-heterogeneous data,\nunlocking the potential of LLMs in sensitive and diverse data environments.", "AI": {"tldr": "HyFedRAG: A federated RAG framework for hybrid data modalities, enabling privacy-preserving and efficient information retrieval in distributed healthcare settings.", "motivation": "Challenges in retrieving rare disease cases due to data heterogeneity, privacy constraints, and limitations of centralized RAG systems in distributed healthcare.", "method": "An edge-cloud collaborative RAG framework (HyFedRAG) is introduced, featuring edge-side LLMs for data conversion and server-side LLMs for integration, along with anonymization tools and a three-tier caching strategy.", "result": "HyFedRAG outperforms existing baselines in retrieval quality, generation consistency, and system efficiency on PMC-Patients dataset.", "conclusion": "HyFedRAG offers a scalable and privacy-compliant solution for RAG over structurally heterogeneous data, unlocking LLM potential in sensitive environments."}}
{"id": "2509.05630", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05630", "abs": "https://arxiv.org/abs/2509.05630", "authors": ["Moqsadur Rahman", "Saurav Kumar", "Santosh S. Palmate", "M. Shahriar Hossain"], "title": "Self-supervised Learning for Hyperspectral Images of Trees", "comment": null, "summary": "Aerial remote sensing using multispectral and RGB imagers has provided a\ncritical impetus to precision agriculture. Analysis of the hyperspectral images\nwith limited or no labels is challenging. This paper focuses on self-supervised\nlearning to create neural network embeddings reflecting vegetation properties\nof trees from aerial hyperspectral images of crop fields. Experimental results\ndemonstrate that a constructed tree representation, using a vegetation\nproperty-related embedding space, performs better in downstream machine\nlearning tasks compared to the direct use of hyperspectral vegetation\nproperties as tree representations.", "AI": {"tldr": "\u672c\u6587\u5229\u7528\u9ad8\u5149\u8c31\u56fe\u50cf\u8fdb\u884c\u7cbe\u51c6\u519c\u4e1a\u7814\u7a76\uff0c\u91cd\u70b9\u5728\u4e8e\u65e0\u6807\u7b7e\u6216\u5c11\u6807\u7b7e\u60c5\u51b5\u4e0b\u7684\u56fe\u50cf\u5206\u6790\u3002", "motivation": "\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u6790\u5728\u65e0\u6807\u7b7e\u6216\u5c11\u6807\u7b7e\u60c5\u51b5\u4e0b\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u91c7\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u521b\u5efa\u795e\u7ecf\u7f51\u7edc\u5d4c\u5165\uff0c\u53cd\u6620\u4f5c\u7269\u7530\u5730\u6811\u6728\u7684\u690d\u88ab\u7279\u6027\u3002", "result": "\u6784\u5efa\u7684\u6811\u6728\u8868\u5f81\u5728\u4f7f\u7528\u690d\u88ab\u5c5e\u6027\u76f8\u5173\u5d4c\u5165\u7a7a\u95f4\u65f6\uff0c\u5728\u4e0b\u6e38\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u9ad8\u5149\u8c31\u56fe\u50cf\u5728\u7cbe\u51c6\u519c\u4e1a\u4e2d\u7684\u5e94\u7528\u6548\u679c\u3002"}}
{"id": "2509.06164", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06164", "abs": "https://arxiv.org/abs/2509.06164", "authors": ["Jinrui Yang", "Xudong Han", "Timothy Baldwin"], "title": "Benchmarking Gender and Political Bias in Large Language Models", "comment": "The 8th International Conference on Natural Language and Speech\n  Processing (Oral)", "summary": "We introduce EuroParlVote, a novel benchmark for evaluating large language\nmodels (LLMs) in politically sensitive contexts. It links European Parliament\ndebate speeches to roll-call vote outcomes and includes rich demographic\nmetadata for each Member of the European Parliament (MEP), such as gender, age,\ncountry, and political group. Using EuroParlVote, we evaluate state-of-the-art\nLLMs on two tasks -- gender classification and vote prediction -- revealing\nconsistent patterns of bias. We find that LLMs frequently misclassify female\nMEPs as male and demonstrate reduced accuracy when simulating votes for female\nspeakers. Politically, LLMs tend to favor centrist groups while underperforming\non both far-left and far-right ones. Proprietary models like GPT-4o outperform\nopen-weight alternatives in terms of both robustness and fairness. We release\nthe EuroParlVote dataset, code, and demo to support future research on fairness\nand accountability in NLP within political contexts.", "AI": {"tldr": "EuroParlVote: A new benchmark for evaluating LLMs in political contexts, linking European Parliament speeches to vote outcomes with MEP demographic data.", "motivation": "Evaluating LLMs in politically sensitive contexts and identifying potential biases.", "method": "Evaluating state-of-the-art LLMs on gender classification and vote prediction tasks using the EuroParlVote benchmark.", "result": "LLMs frequently misclassify female MEPs as male, show reduced accuracy for female speakers' votes, and favor centrist political groups.", "conclusion": "Proprietary models outperform open-weight alternatives in robustness and fairness. The EuroParlVote dataset, code, and demo are released for future research."}}
{"id": "2509.06463", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06463", "abs": "https://arxiv.org/abs/2509.06463", "authors": ["Chengwei Wu", "Li Du", "Hanyu Zhao", "Yiming Ju", "Jiapu Wang", "Tengfei Pan"], "title": "Accelerate Scaling of LLM Alignment via Quantifying the Coverage and Depth of Instruction Set", "comment": null, "summary": "With the growing demand for applying large language models to downstream\ntasks, improving model alignment performance and efficiency has become crucial.\nSuch a process involves selecting informative instructions from a candidate\npool. However, due to the complexity of instruction set distributions, the key\nfactors driving the performance of aligned models remain unclear. As a result,\ncurrent instruction set refinement methods fail to improve performance as the\ninstruction pool expands continuously. To address this issue, we first\ninvestigate the key factors that influence the relationship between instruction\ndataset distribution and aligned model performance. Based on these insights, we\npropose a novel instruction data selection method. We identify that the depth\nof instructions and the coverage of the semantic space are the crucial factors\ndetermining downstream performance, which could explain over 70\\% of the model\nloss on the development set. We then design an instruction selection algorithm\nto simultaneously maximize the depth and semantic coverage of the selected\ninstructions. Experimental results demonstrate that, compared to\nstate-of-the-art baseline methods, it can sustainably improve model performance\nat a faster pace and thus achieve \\emph{``Accelerated Scaling''}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6307\u4ee4\u6570\u636e\u9009\u62e9\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u5bf9\u9f50\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u6307\u4ee4\u96c6\u4f18\u5316\u65b9\u6cd5\u65e0\u6cd5\u968f\u7740\u6307\u4ee4\u6c60\u7684\u6269\u5c55\u800c\u63d0\u9ad8\u6027\u80fd\uff0c\u56e0\u4e3a\u9a71\u52a8\u5bf9\u9f50\u6a21\u578b\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u672c\u6587\u9996\u5148\u7814\u7a76\u4e86\u5f71\u54cd\u6307\u4ee4\u6570\u636e\u96c6\u5206\u5e03\u548c\u5bf9\u9f50\u6a21\u578b\u6027\u80fd\u4e4b\u95f4\u5173\u7cfb\u7684\u5173\u952e\u56e0\u7d20\u3002\u7136\u540e\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6307\u4ee4\u9009\u62e9\u7b97\u6cd5\uff0c\u4ee5\u540c\u65f6\u6700\u5927\u5316\u6240\u9009\u6307\u4ee4\u7684\u6df1\u5ea6\u548c\u8bed\u4e49\u8986\u76d6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u66f4\u5feb\u5730\u6301\u7eed\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\uff0c\u4ece\u800c\u5b9e\u73b0\u201c\u52a0\u901f\u6269\u5c55\u201d\u3002", "conclusion": "\u6307\u4ee4\u7684\u6df1\u5ea6\u548c\u8bed\u4e49\u7a7a\u95f4\u7684\u8986\u76d6\u7387\u662f\u51b3\u5b9a\u4e0b\u6e38\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\uff0c\u53ef\u4ee5\u89e3\u91ca\u5f00\u53d1\u96c6\u4e2d\u8d85\u8fc7 70% \u7684\u6a21\u578b\u635f\u5931\u3002"}}
{"id": "2509.05652", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05652", "abs": "https://arxiv.org/abs/2509.05652", "authors": ["Ha Meem Hossain", "Pritam Nath", "Mahitun Nesa Mahi", "Imtiaz Uddin", "Ishrat Jahan Eiste", "Syed Nasibur Rahman Ratul", "Md Naim Uddin Mozumdar", "Asif Mohammed Saad"], "title": "Evaluating YOLO Architectures: Implications for Real-Time Vehicle Detection in Urban Environments of Bangladesh", "comment": null, "summary": "Vehicle detection systems trained on Non-Bangladeshi datasets struggle to\naccurately identify local vehicle types in Bangladesh's unique road\nenvironments, creating critical gaps in autonomous driving technology for\ndeveloping regions. This study evaluates six YOLO model variants on a custom\ndataset featuring 29 distinct vehicle classes, including region-specific\nvehicles such as ``Desi Nosimon'', ``Leguna'', ``Battery Rickshaw'', and\n``CNG''. The dataset comprises high-resolution images (1920x1080) captured\nacross various Bangladeshi roads using mobile phone cameras and manually\nannotated using LabelImg with YOLO format bounding boxes. Performance\nevaluation revealed YOLOv11x as the top performer, achieving 63.7\\% mAP@0.5,\n43.8\\% mAP@0.5:0.95, 61.4\\% recall, and 61.6\\% F1-score, though requiring 45.8\nmilliseconds per image for inference. Medium variants (YOLOv8m, YOLOv11m)\nstruck an optimal balance, delivering robust detection performance with mAP@0.5\nvalues of 62.5\\% and 61.8\\% respectively, while maintaining moderate inference\ntimes around 14-15 milliseconds. The study identified significant detection\nchallenges for rare vehicle classes, with Construction Vehicles and Desi\nNosimons showing near-zero accuracy due to dataset imbalances and insufficient\ntraining samples. Confusion matrices revealed frequent misclassifications\nbetween visually similar vehicles, particularly Mini Trucks versus Mini Covered\nVans. This research provides a foundation for developing robust object\ndetection systems specifically adapted to Bangladesh traffic conditions,\naddressing critical needs in autonomous vehicle technology advancement for\ndeveloping regions where conventional generic-trained models fail to perform\nadequately.", "AI": {"tldr": "\u9488\u5bf9\u5b5f\u52a0\u62c9\u56fd\u72ec\u7279\u9053\u8def\u73af\u5883\uff0c\u5728\u975e\u5b5f\u52a0\u62c9\u56fd\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u8f66\u8f86\u68c0\u6d4b\u7cfb\u7edf\u96be\u4ee5\u51c6\u786e\u8bc6\u522b\u5f53\u5730\u8f66\u8f86\u7c7b\u578b\uff0c\u4ece\u800c\u5728\u53d1\u5c55\u4e2d\u5730\u533a\u7684\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u4e2d\u9020\u6210\u4e25\u91cd\u5dee\u8ddd\u3002\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u516d\u79cd YOLO \u6a21\u578b\u53d8\u4f53\u5728\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b 29 \u4e2a\u4e0d\u540c\u7684\u8f66\u8f86\u7c7b\u522b\uff0c\u5305\u62ec\u533a\u57df\u7279\u5b9a\u8f66\u8f86\uff0c\u4f8b\u5982``Desi Nosimon''\u3001``Leguna''\u3001``Battery Rickshaw''\u548c``CNG''\u3002", "motivation": "\u5728\u975e\u5b5f\u52a0\u62c9\u56fd\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u8f66\u8f86\u68c0\u6d4b\u7cfb\u7edf\u96be\u4ee5\u51c6\u786e\u8bc6\u522b\u5b5f\u52a0\u62c9\u56fd\u5f53\u5730\u8f66\u8f86\u7c7b\u578b\uff0c\u4ece\u800c\u5728\u53d1\u5c55\u4e2d\u5730\u533a\u7684\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u4e2d\u9020\u6210\u4e25\u91cd\u5dee\u8ddd\u3002", "method": "\u4f7f\u7528\u624b\u673a\u6444\u50cf\u5934\u5728\u5404\u79cd\u5b5f\u52a0\u62c9\u56fd\u9053\u8def\u4e0a\u6355\u83b7\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf (1920x1080)\uff0c\u5e76\u4f7f\u7528 LabelImg \u4ee5 YOLO \u683c\u5f0f\u8fb9\u754c\u6846\u624b\u52a8\u6ce8\u91ca\u3002", "result": "YOLOv11x \u8868\u73b0\u6700\u4f73\uff0c\u8fbe\u5230 63.7% mAP@0.5\u300143.8% mAP@0.5:0.95\u300161.4% \u53ec\u56de\u7387\u548c 61.6% F1 \u5206\u6570\uff0c\u4f46\u6bcf\u5f20\u56fe\u50cf\u7684\u63a8\u7406\u9700\u8981 45.8 \u6beb\u79d2\u3002\u4e2d\u578b\u53d8\u4f53\uff08YOLOv8m\u3001YOLOv11m\uff09\u5b9e\u73b0\u4e86\u6700\u4f73\u5e73\u8861\uff0c\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u68c0\u6d4b\u6027\u80fd\uff0cmAP@0.5 \u503c\u5206\u522b\u4e3a 62.5% \u548c 61.8%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86 14-15 \u6beb\u79d2\u5de6\u53f3\u7684\u9002\u5ea6\u63a8\u7406\u65f6\u95f4\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u4e3a\u5f00\u53d1\u4e13\u95e8\u9002\u7528\u4e8e\u5b5f\u52a0\u62c9\u56fd\u4ea4\u901a\u72b6\u51b5\u7684\u7a33\u5065\u5bf9\u8c61\u68c0\u6d4b\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u89e3\u51b3\u4e86\u5728\u4f20\u7edf\u901a\u7528\u8bad\u7ec3\u6a21\u578b\u65e0\u6cd5\u5145\u5206\u53d1\u6325\u4f5c\u7528\u7684\u53d1\u5c55\u4e2d\u5730\u533a\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u6280\u672f\u8fdb\u6b65\u7684\u5173\u952e\u9700\u6c42\u3002"}}
{"id": "2509.05930", "categories": ["cs.LG", "cs.SY", "eess.SY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.05930", "abs": "https://arxiv.org/abs/2509.05930", "authors": ["Ali Zeynali", "Mahsa Sahebdel", "Qingsong Liu", "Mohammad Hajiesmaili", "Ramesh K. Sitaraman"], "title": "Smoothed Online Optimization for Target Tracking: Robust and Learning-Augmented Algorithms", "comment": "10 pages, 14 pages appendix", "summary": "We introduce the Smoothed Online Optimization for Target Tracking (SOOTT)\nproblem, a new framework that integrates three key objectives in online\ndecision-making under uncertainty: (1) tracking cost for following a\ndynamically moving target, (2) adversarial perturbation cost for withstanding\nunpredictable disturbances, and (3) switching cost for penalizing abrupt\nchanges in decisions. This formulation captures real-world scenarios such as\nelastic and inelastic workload scheduling in AI clusters, where operators must\nbalance long-term service-level agreements (e.g., LLM training) against sudden\ndemand spikes (e.g., real-time inference). We first present BEST, a robust\nalgorithm with provable competitive guarantees for SOOTT. To enhance practical\nperformance, we introduce CoRT, a learning-augmented variant that incorporates\nuntrusted black-box predictions (e.g., from ML models) into its decision\nprocess. Our theoretical analysis shows that CoRT strictly improves over BEST\nwhen predictions are accurate, while maintaining robustness under arbitrary\nprediction errors. We validate our approach through a case study on workload\nscheduling, demonstrating that both algorithms effectively balance trajectory\ntracking, decision smoothness, and resilience to external disturbances.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u5728\u7ebf\u51b3\u7b56\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u8fdb\u884c\u76ee\u6807\u8ddf\u8e2a\uff0c\u540c\u65f6\u8003\u8651\u8ddf\u8e2a\u6210\u672c\u3001\u5bf9\u6297\u6270\u52a8\u6210\u672c\u548c\u5207\u6362\u6210\u672c\u3002", "motivation": "\u5728\u8bf8\u5982AI\u96c6\u7fa4\u4e2d\u7684\u5f39\u6027\u53ca\u975e\u5f39\u6027\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\u7b49\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u9700\u8981\u5e73\u8861\u957f\u671f\u670d\u52a1\u6c34\u5e73\u534f\u8bae\u4e0e\u7a81\u53d1\u9700\u6c42\u9ad8\u5cf0\u3002", "method": "\u63d0\u51fa\u4e86\u540d\u4e3aBEST\u7684\u9c81\u68d2\u7b97\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86CoRT\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u878d\u5165\u4e86\u6765\u81ea\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u9ed1\u76d2\u9884\u6d4b\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u5f53\u9884\u6d4b\u51c6\u786e\u65f6\uff0cCoRT\u7b97\u6cd5\u4f18\u4e8eBEST\u7b97\u6cd5\uff0c\u5e76\u5728\u4efb\u610f\u9884\u6d4b\u8bef\u5dee\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\u3002\u901a\u8fc7\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5730\u5e73\u8861\u8f68\u8ff9\u8ddf\u8e2a\u3001\u51b3\u7b56\u5e73\u6ed1\u6027\u4ee5\u53ca\u5bf9\u5916\u90e8\u5e72\u6270\u7684\u5f39\u6027\u3002"}}
{"id": "2509.06184", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06184", "abs": "https://arxiv.org/abs/2509.06184", "authors": ["Jacob Mitchell Springer", "Vaibhav Adlakha", "Siva Reddy", "Aditi Raghunathan", "Marius Mosbach"], "title": "Understanding the Influence of Synthetic Data for Text Embedders", "comment": "ACL Findings 2025", "summary": "Recent progress in developing general purpose text embedders has been driven\nby training on ever-growing corpora of synthetic LLM-generated data.\nNonetheless, no publicly available synthetic dataset exists, posing a barrier\nto studying its role for generalization. To address this issue, we first\nreproduce and publicly release the synthetic data proposed by Wang et al.\n(Mistral-E5). Our synthetic data is high quality and leads to consistent\nimprovements in performance. Next, we critically examine where exactly\nsynthetic data improves model generalization. Our analysis reveals that\nbenefits from synthetic data are sparse and highly localized to individual\ndatasets. Moreover, we observe trade-offs between the performance on different\ncategories and data that benefits one task, degrades performance on another.\nOur findings highlight the limitations of current synthetic data approaches for\nbuilding general-purpose embedders and challenge the notion that training on\nsynthetic data leads to more robust embedding models across tasks.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u5bf9\u901a\u7528\u6587\u672c\u5d4c\u5165\u5668\u7684\u5f71\u54cd\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u516c\u5f00\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u65e0\u6cd5\u7814\u7a76\u5176\u5728\u6cdb\u5316\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u8bba\u6587\u590d\u5236\u5e76\u516c\u5f00\u53d1\u5e03\u4e86Wang\u7b49\u4eba\u63d0\u51fa\u7684\u5408\u6210\u6570\u636e(Mistral-E5)\uff0c\u5e76\u5206\u6790\u4e86\u5408\u6210\u6570\u636e\u6539\u8fdb\u6a21\u578b\u6cdb\u5316\u7684\u5177\u4f53\u65b9\u9762\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5408\u6210\u6570\u636e\u5e26\u6765\u7684\u597d\u5904\u662f\u7a00\u758f\u7684\uff0c\u5e76\u4e14\u9ad8\u5ea6\u5c40\u9650\u4e8e\u5355\u4e2a\u6570\u636e\u96c6\u3002\u4e0d\u540c\u7c7b\u522b\u4e4b\u95f4\u7684\u6027\u80fd\u5b58\u5728\u6743\u8861\uff0c\u5e76\u4e14\u6709\u76ca\u4e8e\u4e00\u9879\u4efb\u52a1\u7684\u6570\u636e\u4f1a\u964d\u4f4e\u53e6\u4e00\u9879\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d\u7684\u5408\u6210\u6570\u636e\u65b9\u6cd5\u5728\u6784\u5efa\u901a\u7528\u5d4c\u5165\u5668\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5e76\u5bf9\u5728\u5408\u6210\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\u53ef\u4ee5\u6784\u5efa\u66f4\u5f3a\u5927\u7684\u8de8\u4efb\u52a1\u5d4c\u5165\u6a21\u578b\u7684\u89c2\u70b9\u63d0\u51fa\u4e86\u6311\u6218\u3002"}}
{"id": "2509.06477", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06477", "abs": "https://arxiv.org/abs/2509.06477", "authors": ["Pengxiang Zhao", "Guangyi Liu", "Yaozhen Liang", "Weiqing He", "Zhengxi Lu", "Yuehao Huang", "Yaxuan Guo", "Kexin Zhang", "Hao Wang", "Liang Liu", "Yong Liu"], "title": "MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI Agents", "comment": null, "summary": "To enhance the efficiency of GUI agents on various platforms like smartphones\nand computers, a hybrid paradigm that combines flexible GUI operations with\nefficient shortcuts (e.g., API, deep links) is emerging as a promising\ndirection. However, a framework for systematically benchmarking these hybrid\nagents is still underexplored. To take the first step in bridging this gap, we\nintroduce MAS-Bench, a benchmark that pioneers the evaluation of GUI-shortcut\nhybrid agents with a specific focus on the mobile domain. Beyond merely using\npredefined shortcuts, MAS-Bench assesses an agent's capability to autonomously\ngenerate shortcuts by discovering and creating reusable, low-cost workflows. It\nfeatures 139 complex tasks across 11 real-world applications, a knowledge base\nof 88 predefined shortcuts (APIs, deep-links, RPA scripts), and 7 evaluation\nmetrics. The tasks are designed to be solvable via GUI-only operations, but can\nbe significantly accelerated by intelligently embedding shortcuts. Experiments\nshow that hybrid agents achieve significantly higher success rates and\nefficiency than their GUI-only counterparts. This result also demonstrates the\neffectiveness of our method for evaluating an agent's shortcut generation\ncapabilities. MAS-Bench fills a critical evaluation gap, providing a\nfoundational platform for future advancements in creating more efficient and\nrobust intelligent agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u6df7\u5408GUI\u667a\u80fd\u4ee3\u7406\u7684\u57fa\u51c6\u6d4b\u8bd5MAS-Bench\uff0c\u4e13\u6ce8\u4e8e\u79fb\u52a8\u9886\u57df\uff0c\u901a\u8fc7\u81ea\u4e3b\u751f\u6210\u5feb\u6377\u65b9\u5f0f\u6765\u8bc4\u4f30\u4ee3\u7406\u7684\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8GUI\u667a\u80fd\u4ee3\u7406\u5728\u667a\u80fd\u624b\u673a\u548c\u8ba1\u7b97\u673a\u7b49\u5e73\u53f0\u4e0a\u7684\u6548\u7387\uff0c\u7ed3\u5408\u7075\u6d3b\u7684GUI\u64cd\u4f5c\u548c\u9ad8\u6548\u5feb\u6377\u65b9\u5f0f\u7684\u6df7\u5408\u8303\u4f8b\u6b63\u5728\u6210\u4e3a\u4e00\u4e2a\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002\u7136\u800c\uff0c\u7cfb\u7edf\u5730\u8bc4\u4f30\u8fd9\u4e9b\u6df7\u5408\u4ee3\u7406\u7684\u6846\u67b6\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5f15\u5165MAS-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30GUI-shortcut\u6df7\u5408\u4ee3\u7406\uff0c\u5305\u542b139\u4e2a\u8de811\u4e2a\u771f\u5b9e\u5e94\u7528\u7a0b\u5e8f\u7684\u590d\u6742\u4efb\u52a1\uff0c\u4e00\u4e2a\u5305\u542b88\u4e2a\u9884\u5b9a\u4e49\u5feb\u6377\u65b9\u5f0f\u7684\u77e5\u8bc6\u5e93\uff0c\u4ee5\u53ca7\u4e2a\u8bc4\u4f30\u6307\u6807\u3002\u4efb\u52a1\u53ef\u4ee5\u901a\u8fc7\u7eafGUI\u64cd\u4f5c\u89e3\u51b3\uff0c\u4f46\u53ef\u4ee5\u901a\u8fc7\u667a\u80fd\u5d4c\u5165\u5feb\u6377\u65b9\u5f0f\u6765\u663e\u8457\u52a0\u901f\u3002", "result": "\u6df7\u5408\u4ee3\u7406\u6bd4\u7eafGUI\u4ee3\u7406\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u9ad8\u7684\u6210\u529f\u7387\u548c\u6548\u7387\u3002\u8fd9\u4e00\u7ed3\u679c\u4e5f\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u8bc4\u4f30\u4ee3\u7406\u7684\u5feb\u6377\u65b9\u5f0f\u751f\u6210\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "MAS-Bench\u586b\u8865\u4e86\u4e00\u4e2a\u5173\u952e\u7684\u8bc4\u4f30\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u521b\u5efa\u66f4\u9ad8\u6548\u548c\u5f3a\u5927\u7684\u667a\u80fd\u4ee3\u7406\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.05659", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05659", "abs": "https://arxiv.org/abs/2509.05659", "authors": ["Guandong Li", "Zhaobin Chu"], "title": "EditIDv2: Editable ID Customization with Data-Lubricated ID Feature Integration for Text-to-Image Generation", "comment": null, "summary": "We propose EditIDv2, a tuning-free solution specifically designed for\nhigh-complexity narrative scenes and long text inputs. Existing character\nediting methods perform well under simple prompts, but often suffer from\ndegraded editing capabilities, semantic understanding biases, and identity\nconsistency breakdowns when faced with long text narratives containing multiple\nsemantic layers, temporal logic, and complex contextual relationships. In\nEditID, we analyzed the impact of the ID integration module on editability. In\nEditIDv2, we further explore and address the influence of the ID feature\nintegration module. The core of EditIDv2 is to discuss the issue of editability\ninjection under minimal data lubrication. Through a sophisticated decomposition\nof PerceiverAttention, the introduction of ID loss and joint dynamic training\nwith the diffusion model, as well as an offline fusion strategy for the\nintegration module, we achieve deep, multi-level semantic editing while\nmaintaining identity consistency in complex narrative environments using only a\nsmall amount of data lubrication. This meets the demands of long prompts and\nhigh-quality image generation, and achieves excellent results in the IBench\nevaluation.", "AI": {"tldr": "EditIDv2 focuses on improving character editing in complex narrative scenes with long text inputs, addressing issues like degraded editing, semantic biases, and identity inconsistency.", "motivation": "Existing methods struggle with long, complex narratives, leading to poor editing, semantic misunderstanding, and identity issues.", "method": "It uses PerceiverAttention decomposition, ID loss, joint dynamic training with a diffusion model, and an offline fusion strategy.", "result": "EditIDv2 achieves deep, multi-level semantic editing while maintaining identity consistency in complex narrative environments with minimal data.", "conclusion": "EditIDv2 effectively handles long prompts and generates high-quality images, achieving excellent results in the IBench evaluation."}}
{"id": "2509.06025", "categories": ["cs.LG", "cs.AI", "68T07, 62M20", "I.2.6; H.2.8; H.3.3"], "pdf": "https://arxiv.org/pdf/2509.06025", "abs": "https://arxiv.org/abs/2509.06025", "authors": ["Vignesh Ethiraj", "Subhash Talluri"], "title": "Unified Interaction Foundational Model (UIFM) for Predicting Complex User and System Behavior", "comment": null, "summary": "A central goal of artificial intelligence is to build systems that can\nunderstand and predict complex, evolving sequences of events. However, current\nfoundation models, designed for natural language, fail to grasp the holistic\nnature of structured interactions found in domains like telecommunications,\ne-commerce and finance. By serializing events into text, they disassemble them\ninto semantically fragmented parts, losing critical context. In this work, we\nintroduce the Unified Interaction Foundation Model (UIFM), a foundation model\nengineered for genuine behavioral understanding. At its core is the principle\nof composite tokenization, where each multi-attribute event is treated as a\nsingle, semantically coherent unit. This allows UIFM to learn the underlying\n\"grammar\" of user behavior, perceiving entire interactions rather than a\ndisconnected stream of data points. We demonstrate that this architecture is\nnot just more accurate, but represents a fundamental step towards creating more\nadaptable and intelligent predictive systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u540d\u4e3a\u7edf\u4e00\u4ea4\u4e92\u57fa\u7840\u6a21\u578b\uff08UIFM\uff09\u7684\u57fa\u7840\u6a21\u578b\uff0c\u7528\u4e8e\u7406\u89e3\u548c\u9884\u6d4b\u590d\u6742\u7684\u4e8b\u4ef6\u5e8f\u5217\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u7136\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u65e0\u6cd5\u7406\u89e3\u7535\u4fe1\u3001\u7535\u5546\u548c\u91d1\u878d\u7b49\u9886\u57df\u4e2d\u7ed3\u6784\u5316\u4ea4\u4e92\u7684\u6574\u4f53\u6027\u8d28\uff0c\u56e0\u4e3a\u5b83\u4eec\u5c06\u4e8b\u4ef6\u5e8f\u5217\u5316\u4e3a\u6587\u672c\uff0c\u5bfc\u81f4\u8bed\u4e49\u788e\u7247\u5316\u5e76\u4e22\u5931\u5173\u952e\u4e0a\u4e0b\u6587\u3002", "method": "\u91c7\u7528\u590d\u5408tokenization\u539f\u5219\uff0c\u5c06\u6bcf\u4e2a\u591a\u5c5e\u6027\u4e8b\u4ef6\u89c6\u4e3a\u4e00\u4e2a\u5355\u4e00\u7684\u3001\u8bed\u4e49\u8fde\u8d2f\u7684\u5355\u5143\u3002", "result": "\u8be5\u67b6\u6784\u4e0d\u4ec5\u66f4\u51c6\u786e\uff0c\u800c\u4e14\u4ee3\u8868\u7740\u671d\u7740\u521b\u5efa\u66f4\u5177\u9002\u5e94\u6027\u548c\u667a\u80fd\u5316\u7684\u9884\u6d4b\u7cfb\u7edf\u8fc8\u51fa\u7684\u91cd\u8981\u4e00\u6b65\u3002", "conclusion": "UIFM\u80fd\u591f\u5b66\u4e60\u7528\u6237\u884c\u4e3a\u7684\u5e95\u5c42\u201c\u8bed\u6cd5\u201d\uff0c\u611f\u77e5\u6574\u4e2a\u4ea4\u4e92\u8fc7\u7a0b\uff0c\u800c\u4e0d\u662f\u4e0d\u8fde\u8d2f\u7684\u6570\u636e\u70b9\u6d41\u3002"}}
{"id": "2509.06196", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06196", "abs": "https://arxiv.org/abs/2509.06196", "authors": ["Mohamed T. Younes", "Omar Walid", "Khaled Shaban", "Ali Hamdi", "Mai Hassan"], "title": "Augmented Fine-Tuned LLMs for Enhanced Recruitment Automation", "comment": "Accepted in AICCSA 2025", "summary": "This paper presents a novel approach to recruitment automation. Large\nLanguage Models (LLMs) were fine-tuned to improve accuracy and efficiency.\nBuilding upon our previous work on the Multilayer Large Language Model-Based\nRobotic Process Automation Applicant Tracking (MLAR) system . This work\nintroduces a novel methodology. Training fine-tuned LLMs specifically tuned for\nrecruitment tasks. The proposed framework addresses the limitations of generic\nLLMs by creating a synthetic dataset that uses a standardized JSON format. This\nhelps ensure consistency and scalability. In addition to the synthetic data\nset, the resumes were parsed using DeepSeek, a high-parameter LLM. The resumes\nwere parsed into the same structured JSON format and placed in the training\nset. This will help improve data diversity and realism. Through\nexperimentation, we demonstrate significant improvements in performance\nmetrics, such as exact match, F1 score, BLEU score, ROUGE score, and overall\nsimilarity compared to base models and other state-of-the-art LLMs. In\nparticular, the fine-tuned Phi-4 model achieved the highest F1 score of 90.62%,\nindicating exceptional precision and recall in recruitment tasks. This study\nhighlights the potential of fine-tuned LLMs. Furthermore, it will revolutionize\nrecruitment workflows by providing more accurate candidate-job matching.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u62db\u8058\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6765\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u901a\u7528LLM\u5728\u62db\u8058\u4efb\u52a1\u4e2d\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "1. \u521b\u5efa\u4e00\u4e2a\u4f7f\u7528\u6807\u51c6\u5316JSON\u683c\u5f0f\u7684\u5408\u6210\u6570\u636e\u96c6\u30022. \u4f7f\u7528DeepSeek\u89e3\u6790\u7b80\u5386\uff0c\u5e76\u5c06\u7b80\u5386\u89e3\u6790\u4e3a\u76f8\u540c\u7684\u7ed3\u6784\u5316JSON\u683c\u5f0f\uff0c\u5e76\u5c06\u5176\u653e\u5165\u8bad\u7ec3\u96c6\u4e2d\u30023. \u5fae\u8c03LLM\u3002", "result": "\u5fae\u8c03\u540e\u7684Phi-4\u6a21\u578b\u8fbe\u5230\u4e8690.62%\u7684F1\u5206\u6570\uff0c\u8868\u660e\u5728\u62db\u8058\u4efb\u52a1\u4e2d\u5177\u6709\u51fa\u8272\u7684\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u3002", "conclusion": "\u5fae\u8c03\u540e\u7684LLM\u6709\u6f5c\u529b\u901a\u8fc7\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u5019\u9009\u4eba-\u804c\u4f4d\u5339\u914d\u6765\u5f7b\u5e95\u6539\u53d8\u62db\u8058\u5de5\u4f5c\u6d41\u7a0b\u3002"}}
{"id": "2509.06490", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06490", "abs": "https://arxiv.org/abs/2509.06490", "authors": ["Niki Kotecha", "Ehecatl Antonio del Rio Chanona"], "title": "MORSE: Multi-Objective Reinforcement Learning via Strategy Evolution for Supply Chain Optimization", "comment": null, "summary": "In supply chain management, decision-making often involves balancing multiple\nconflicting objectives, such as cost reduction, service level improvement, and\nenvironmental sustainability. Traditional multi-objective optimization methods,\nsuch as linear programming and evolutionary algorithms, struggle to adapt in\nreal-time to the dynamic nature of supply chains. In this paper, we propose an\napproach that combines Reinforcement Learning (RL) and Multi-Objective\nEvolutionary Algorithms (MOEAs) to address these challenges for dynamic\nmulti-objective optimization under uncertainty. Our method leverages MOEAs to\nsearch the parameter space of policy neural networks, generating a Pareto front\nof policies. This provides decision-makers with a diverse population of\npolicies that can be dynamically switched based on the current system\nobjectives, ensuring flexibility and adaptability in real-time decision-making.\nWe also introduce Conditional Value-at-Risk (CVaR) to incorporate\nrisk-sensitive decision-making, enhancing resilience in uncertain environments.\nWe demonstrate the effectiveness of our approach through case studies,\nshowcasing its ability to respond to supply chain dynamics and outperforming\nstate-of-the-art methods in an inventory management case study. The proposed\nstrategy not only improves decision-making efficiency but also offers a more\nrobust framework for managing uncertainty and optimizing performance in supply\nchains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60 (RL) \u548c\u591a\u76ee\u6807\u8fdb\u5316\u7b97\u6cd5 (MOEA) \u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u4e0d\u786e\u5b9a\u6027\u4e0b\u52a8\u6001\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7684\u591a\u76ee\u6807\u4f18\u5316\u65b9\u6cd5\u96be\u4ee5\u5b9e\u65f6\u9002\u5e94\u4f9b\u5e94\u94fe\u7684\u52a8\u6001\u7279\u6027\u3002", "method": "\u5229\u7528 MOEA \u641c\u7d22\u7b56\u7565\u795e\u7ecf\u7f51\u7edc\u7684\u53c2\u6570\u7a7a\u95f4\uff0c\u751f\u6210\u7b56\u7565\u7684 Pareto \u524d\u6cbf\u3002", "result": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u54cd\u5e94\u4f9b\u5e94\u94fe\u52a8\u6001\u7684\u80fd\u529b\uff0c\u5e76\u5728\u5e93\u5b58\u7ba1\u7406\u6848\u4f8b\u7814\u7a76\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7b56\u7565\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u51b3\u7b56\u6548\u7387\uff0c\u800c\u4e14\u4e3a\u7ba1\u7406\u4e0d\u786e\u5b9a\u6027\u548c\u4f18\u5316\u4f9b\u5e94\u94fe\u6027\u80fd\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u5f3a\u5927\u7684\u6846\u67b6\u3002"}}
{"id": "2509.05661", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05661", "abs": "https://arxiv.org/abs/2509.05661", "authors": ["Xiaomeng Zhu", "Changwei Wang", "Haozhe Wang", "Xinyu Liu", "Fangzhen Lin"], "title": "OOTSM: A Decoupled Linguistic Framework for Effective Scene Graph Anticipation", "comment": null, "summary": "A scene graph is a structured represention of objects and their relationships\nin a scene. Scene Graph Anticipation (SGA) involves predicting future scene\ngraphs from video clips, enabling applications as intelligent surveillance and\nhuman-machine collaboration. Existing SGA approaches primarily leverage visual\ncues, often struggling to integrate valuable commonsense knowledge, thereby\nlimiting long-term prediction robustness. To explicitly leverage such\ncommonsense knowledge, we propose a new approach to better understand the\nobjects, concepts, and relationships in a scene graph. Our approach decouples\nthe SGA task in two steps: first a scene graph capturing model is used to\nconvert a video clip into a sequence of scene graphs, then a pure text-based\nmodel is used to predict scene graphs in future frames. Our focus in this work\nis on the second step, and we call it Linguistic Scene Graph Anticipation\n(LSGA) and believes it should have independent interest beyond the use in SGA\ndiscussed here. For LSGA, we introduce an Object-Oriented Two-Staged Method\n(OOTSM) where an Large Language Model (LLM) first forecasts object appearances\nand disappearances before generating detailed human-object relations. We\nconduct extensive experiments to evaluate OOTSM in two settings. For LSGA, we\nevaluate our fine-tuned open-sourced LLMs against zero-shot APIs (i.e., GPT-4o,\nGPT-4o-mini, and DeepSeek-V3) on a benchmark constructed from Action Genome\nannotations. For SGA, we combine our OOTSM with STTran++ from, and our\nexperiments demonstrate effective state-of-the-art performance: short-term\nmean-Recall (@10) increases by 3.4% while long-term mean-Recall (@50) improves\ndramatically by 21.9%. Code is available at https://github.com/ZhuXMMM/OOTSM.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u573a\u666f\u56fe\u9884\u6d4b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u5e38\u8bc6\u77e5\u8bc6\uff0c\u901a\u8fc7\u89e3\u8026\u573a\u666f\u56fe\u6355\u83b7\u548c\u6587\u672c\u9884\u6d4b\u4e24\u4e2a\u6b65\u9aa4\uff0c\u7740\u91cd\u7814\u7a76\u4e86\u7eaf\u6587\u672c\u7684\u573a\u666f\u56fe\u9884\u6d4b\uff08LSGA\uff09\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u9762\u5411\u5bf9\u8c61\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\uff08OOTSM\uff09\u3002", "motivation": "\u73b0\u6709\u7684\u573a\u666f\u56fe\u9884\u6d4b\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u89c6\u89c9\u7ebf\u7d22\uff0c\u96be\u4ee5\u6574\u5408\u6709\u4ef7\u503c\u7684\u5e38\u8bc6\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u957f\u671f\u9884\u6d4b\u7684\u9c81\u68d2\u6027\u3002", "method": "\u9996\u5148\u4f7f\u7528\u573a\u666f\u56fe\u6355\u83b7\u6a21\u578b\u5c06\u89c6\u9891\u526a\u8f91\u8f6c\u6362\u4e3a\u573a\u666f\u56fe\u5e8f\u5217\uff0c\u7136\u540e\u4f7f\u7528\u7eaf\u6587\u672c\u6a21\u578b\u9884\u6d4b\u672a\u6765\u5e27\u4e2d\u7684\u573a\u666f\u56fe\u3002\u91cd\u70b9\u662f\u7b2c\u4e8c\u6b65\uff0c\u5373\u8bed\u8a00\u573a\u666f\u56fe\u9884\u6d4b\uff08LSGA\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u9762\u5411\u5bf9\u8c61\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\uff08OOTSM\uff09\uff0c\u5176\u4e2d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9996\u5148\u9884\u6d4b\u5bf9\u8c61\u7684\u5916\u89c2\u548c\u6d88\u5931\uff0c\u7136\u540e\u518d\u751f\u6210\u8be6\u7ec6\u7684\u4eba\u4e0e\u5bf9\u8c61\u5173\u7cfb\u3002", "result": "\u5728 LSGA \u4efb\u52a1\u4e2d\uff0c\u5bf9\u5fae\u8c03\u7684\u5f00\u6e90 LLM \u4e0e\u96f6\u6837\u672c API\uff08\u5373 GPT-4o\u3001GPT-4o-mini \u548c DeepSeek-V3\uff09\u5728\u4ece Action Genome \u6ce8\u91ca\u6784\u5efa\u7684\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u5bf9\u4e8e SGA\uff0c\u5c06 OOTSM \u4e0e STTran++ \u7ed3\u5408\u4f7f\u7528\uff0c\u5b9e\u9a8c\u8868\u660e\u6709\u6548\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff1a\u77ed\u671f mean-Recall (@10) \u63d0\u9ad8\u4e86 3.4%\uff0c\u800c\u957f\u671f mean-Recall (@50) \u663e\u7740\u63d0\u9ad8\u4e86 21.9%\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u573a\u666f\u56fe\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u5e38\u8bc6\u77e5\u8bc6\u548c\u89e3\u8026\u9884\u6d4b\u6b65\u9aa4\uff0c\u5728\u957f\u671f\u9884\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2509.06053", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06053", "abs": "https://arxiv.org/abs/2509.06053", "authors": ["Mingrui Lv", "Hangzhi Liu", "Zhi Luo", "Hongjie Zhang", "Jie Ou"], "title": "PolicyEvolve: Evolving Programmatic Policies by LLMs for multi-player games via Population-Based Training", "comment": null, "summary": "Multi-agent reinforcement learning (MARL) has achieved significant progress\nin solving complex multi-player games through self-play. However, training\neffective adversarial policies requires millions of experience samples and\nsubstantial computational resources. Moreover, these policies lack\ninterpretability, hindering their practical deployment. Recently, researchers\nhave successfully leveraged Large Language Models (LLMs) to generate\nprogrammatic policies for single-agent tasks, transforming neural network-based\npolicies into interpretable rule-based code with high execution efficiency.\nInspired by this, we propose PolicyEvolve, a general framework for generating\nprogrammatic policies in multi-player games. PolicyEvolve significantly reduces\nreliance on manually crafted policy code, achieving high-performance policies\nwith minimal environmental interactions. The framework comprises four modules:\nGlobal Pool, Local Pool, Policy Planner, and Trajectory Critic. The Global Pool\npreserves elite policies accumulated during iterative training. The Local Pool\nstores temporary policies for the current iteration; only sufficiently\nhigh-performing policies from this pool are promoted to the Global Pool. The\nPolicy Planner serves as the core policy generation module. It samples the top\nthree policies from the Global Pool, generates an initial policy for the\ncurrent iteration based on environmental information, and refines this policy\nusing feedback from the Trajectory Critic. Refined policies are then deposited\ninto the Local Pool. This iterative process continues until the policy achieves\na sufficiently high average win rate against the Global Pool, at which point it\nis integrated into the Global Pool. The Trajectory Critic analyzes interaction\ndata from the current policy, identifies vulnerabilities, and proposes\ndirectional improvements to guide the Policy Planner", "AI": {"tldr": "PolicyEvolve: \u4f7f\u7528 LLM \u751f\u6210\u591a\u4eba\u6e38\u620f\u4e2d\u53ef\u89e3\u91ca\u7684\u7a0b\u5e8f\u5316\u7b56\u7565\uff0c\u51cf\u5c11\u5bf9\u4eba\u5de5\u4ee3\u7801\u7684\u4f9d\u8d56\u3002", "motivation": "\u5728\u591a\u4eba\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u8bad\u7ec3\u6709\u6548\u7684\u5bf9\u6297\u7b56\u7565\u9700\u8981\u5927\u91cf\u6837\u672c\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u5e76\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002", "method": "PolicyEvolve \u6846\u67b6\u5305\u542b\u5168\u5c40\u6c60\u3001\u5c40\u90e8\u6c60\u3001\u7b56\u7565\u89c4\u5212\u5668\u548c\u8f68\u8ff9\u8bc4\u8bba\u5668\u56db\u4e2a\u6a21\u5757\uff0c\u901a\u8fc7\u8fed\u4ee3\u8bad\u7ec3\u751f\u6210\u548c\u4f18\u5316\u7b56\u7565\u3002", "result": "PolicyEvolve \u80fd\u591f\u4ee5\u6700\u5c11\u7684\u73af\u5883\u4ea4\u4e92\u5b9e\u73b0\u9ad8\u6027\u80fd\u7b56\u7565\u3002", "conclusion": "PolicyEvolve \u662f\u4e00\u79cd\u901a\u7528\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u591a\u4eba\u6e38\u620f\u4e2d\u751f\u6210\u7a0b\u5e8f\u5316\u7b56\u7565\u3002"}}
{"id": "2509.06200", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06200", "abs": "https://arxiv.org/abs/2509.06200", "authors": ["Omar Walid", "Mohamed T. Younes", "Khaled Shaban", "Mai Hassan", "Ali Hamdi"], "title": "MSLEF: Multi-Segment LLM Ensemble Finetuning in Recruitment", "comment": "Accepted in AICCSA 2025", "summary": "This paper presents MSLEF, a multi-segment ensemble framework that employs\nLLM fine-tuning to enhance resume parsing in recruitment automation. It\nintegrates fine-tuned Large Language Models (LLMs) using weighted voting, with\neach model specializing in a specific resume segment to boost accuracy.\nBuilding on MLAR , MSLEF introduces a segment-aware architecture that leverages\nfield-specific weighting tailored to each resume part, effectively overcoming\nthe limitations of single-model systems by adapting to diverse formats and\nstructures. The framework incorporates Gemini-2.5-Flash LLM as a high-level\naggregator for complex sections and utilizes Gemma 9B, LLaMA 3.1 8B, and Phi-4\n14B. MSLEF achieves significant improvements in Exact Match (EM), F1 score,\nBLEU, ROUGE, and Recruitment Similarity (RS) metrics, outperforming the best\nsingle model by up to +7% in RS. Its segment-aware design enhances\ngeneralization across varied resume layouts, making it highly adaptable to\nreal-world hiring scenarios while ensuring precise and reliable candidate\nrepresentation.", "AI": {"tldr": "MSLEF is a multi-segment ensemble framework that uses LLM fine-tuning to improve resume parsing.", "motivation": "The paper aims to overcome the limitations of single-model systems in resume parsing by adapting to diverse formats and structures.", "method": "The paper introduces a segment-aware architecture that leverages field-specific weighting tailored to each resume part and integrates fine-tuned LLMs using weighted voting. It incorporates Gemini-2.5-Flash LLM, Gemma 9B, LLaMA 3.1 8B, and Phi-4 14B.", "result": "MSLEF achieves significant improvements in Exact Match (EM), F1 score, BLEU, ROUGE, and Recruitment Similarity (RS) metrics, outperforming the best single model by up to +7% in RS.", "conclusion": "The segment-aware design enhances generalization across varied resume layouts, making it highly adaptable to real-world hiring scenarios while ensuring precise and reliable candidate representation."}}
{"id": "2509.06493", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06493", "abs": "https://arxiv.org/abs/2509.06493", "authors": ["Ran Xin", "Zeyu Zheng", "Yanchen Nie", "Kun Yuan", "Xia Xiao"], "title": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM Step-Provers", "comment": null, "summary": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search.", "AI": {"tldr": "BFS-Prover-V2\u901a\u8fc7\u8bad\u7ec3\u65f6\u5f3a\u5316\u5b66\u4e60\u548c\u63a8\u7406\u65f6\u8ba1\u7b97\u6269\u5c55\u6765\u89e3\u51b3LLM\u5728\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u4e2d\u7684\u6269\u5c55\u95ee\u9898\uff0c\u5728MiniF2F\u548cProofNet\u6d4b\u8bd5\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u96c6\u6210\u5230\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u4e2d\u663e\u793a\u51fa\u5de8\u5927\u7684\u524d\u666f\uff0c\u4f46\u53d7\u5230\u8bad\u7ec3\u65f6\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u548c\u63a8\u7406\u65f6\u8ba1\u7b97\u6269\u5c55\u7684\u6311\u6218\u7684\u6839\u672c\u9650\u5236\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u56de\u5408\u79bb\u7b56\u7565RL\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8bad\u7ec3\u65f6\u4e0d\u65ad\u63d0\u9ad8LLM step-prover\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u8fd8\u91c7\u7528\u4e86\u89c4\u5212\u5668\u589e\u5f3a\u7684\u591a\u667a\u80fd\u4f53\u641c\u7d22\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u5728\u63a8\u7406\u65f6\u6269\u5c55\u4e86\u63a8\u7406\u80fd\u529b\u3002\u8be5\u67b6\u6784\u4f7f\u7528\u901a\u7528\u63a8\u7406\u6a21\u578b\u4f5c\u4e3a\u9ad8\u7ea7\u89c4\u5212\u5668\uff0c\u4ee5\u8fed\u4ee3\u5730\u5c06\u590d\u6742\u5b9a\u7406\u5206\u89e3\u4e3a\u4e00\u7cfb\u5217\u66f4\u7b80\u5355\u7684\u5b50\u76ee\u6807\u3002", "result": "BFS-Prover-V2\u5728MiniF2F\u548cProofNet\u6d4b\u8bd5\u96c6\u4e0a\u5206\u522b\u8fbe\u5230\u4e8695.08%\u548c41.4%\u7684state-of-the-art\u7ed3\u679c\u3002", "conclusion": "\u5728\u5f62\u5f0f\u6570\u5b66\u9886\u57df\u4e2d\u8bc1\u660e\uff0c\u8fd9\u9879\u5de5\u4f5c\u4e2d\u63d0\u51fa\u7684RL\u548c\u63a8\u7406\u6280\u672f\u5177\u6709\u66f4\u5e7f\u6cdb\u7684\u610f\u4e49\uff0c\u53ef\u4ee5\u5e94\u7528\u4e8e\u5176\u4ed6\u9700\u8981\u957f\u671f\u591a\u56de\u5408\u63a8\u7406\u548c\u590d\u6742\u641c\u7d22\u7684\u9886\u57df\u3002"}}
{"id": "2509.05662", "categories": ["cs.CV", "hep-ex"], "pdf": "https://arxiv.org/pdf/2509.05662", "abs": "https://arxiv.org/abs/2509.05662", "authors": ["Wasikul Islam"], "title": "WIPUNet: A Physics-inspired Network with Weighted Inductive Biases for Image Denoising", "comment": "13 pages, 4 figures", "summary": "In high-energy particle physics, collider measurements are contaminated by\n\"pileup\", overlapping soft interactions that obscure the hard-scatter signal of\ninterest. Dedicated subtraction strategies exploit physical priors such as\nconservation, locality, and isolation. Inspired by this analogy, we investigate\nhow such principles can inform image denoising by embedding physics-guided\ninductive biases into neural architectures. This paper is a proof of concept:\nrather than targeting state-of-the-art (SOTA) benchmarks, we ask whether\nphysics-inspired priors improve robustness under strong corruption.\n  We introduce a hierarchy of PU-inspired denoisers: a residual CNN with\nconservation constraints, its Gaussian-noise variants, and the Weighted\nInductive Pileup-physics-inspired U-Network for Denoising (WIPUNet), which\nintegrates these ideas into a UNet backbone. On CIFAR-10 with Gaussian noise at\n$\\sigma\\in\\{15,25,50,75,100\\}$, PU-inspired CNNs are competitive with standard\nbaselines, while WIPUNet shows a \\emph{widening margin} at higher noise.\nComplementary BSD500 experiments show the same trend, suggesting\nphysics-inspired priors provide stability where purely data-driven models\ndegrade. Our contributions are: (i) translating pileup-mitigation principles\ninto modular inductive biases; (ii) integrating them into UNet; and (iii)\ndemonstrating robustness gains at high noise without relying on heavy SOTA\nmachinery.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u53bb\u566a\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53d7\u5230\u9ad8\u80fd\u7c92\u5b50\u7269\u7406\u4e2dpileup\u53bb\u9664\u6280\u672f\u7684\u542f\u53d1\uff0c\u901a\u8fc7\u5c06\u7269\u7406\u5148\u9a8c\u77e5\u8bc6\u5d4c\u5165\u5230\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u4e2d\uff0c\u63d0\u9ad8\u4e86\u5728\u5f3a\u566a\u58f0\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5728\u9ad8\u80fd\u7c92\u5b50\u7269\u7406\u4e2d\uff0c\u5bf9\u649e\u673a\u6d4b\u91cf\u53d7\u5230\u201cpileup\u201d\u7684\u6c61\u67d3\uff0c\u8fd9\u63a9\u76d6\u4e86\u4eba\u4eec\u611f\u5174\u8da3\u7684\u786c\u6563\u5c04\u4fe1\u53f7\u3002\u53d7\u6b64\u542f\u53d1\uff0c\u672c\u6587\u63a2\u8ba8\u4e86\u8fd9\u4e9b\u539f\u5219\u5982\u4f55\u901a\u8fc7\u5c06\u7269\u7406\u5f15\u5bfc\u7684\u5f52\u7eb3\u504f\u5dee\u5d4c\u5165\u5230\u795e\u7ecf\u67b6\u6784\u4e2d\u6765\u4e3a\u56fe\u50cf\u53bb\u566a\u63d0\u4f9b\u4fe1\u606f\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u7cfb\u5217\u53d7PU\u542f\u53d1\u7684\u53bb\u566a\u5668\uff1a\u5177\u6709\u5b88\u6052\u7ea6\u675f\u7684\u6b8b\u5deeCNN\u3001\u5b83\u7684\u9ad8\u65af\u566a\u58f0\u53d8\u4f53\uff0c\u4ee5\u53ca\u52a0\u6743\u5f52\u7eb3Pileup\u7269\u7406\u542f\u53d1\u7684U-Network\u7528\u4e8e\u53bb\u566a(WIPUNet)\uff0c\u5b83\u5c06\u8fd9\u4e9b\u601d\u60f3\u96c6\u6210\u5230UNet\u4e3b\u5e72\u4e2d\u3002", "result": "\u5728$\\\\sigma\\\\in\\\\{15,25,50,75,100\\\\}$\u7684\u9ad8\u65af\u566a\u58f0\u4e0b\u7684CIFAR-10\u4e0a\uff0c\u53d7PU\u542f\u53d1\u7684cnn\u4e0e\u6807\u51c6\u57fa\u7ebf\u5177\u6709\u7ade\u4e89\u529b\uff0c\u800cWIPUNet\u5728\u66f4\u9ad8\u7684\u566a\u58f0\u4e0b\u663e\u793a\u51fa\u66f4\u5927\u7684\u4f18\u52bf\u3002\u4e92\u8865\u7684BSD500\u5b9e\u9a8c\u663e\u793a\u4e86\u76f8\u540c\u7684\u8d8b\u52bf\uff0c\u8868\u660e\u7269\u7406\u542f\u53d1\u7684\u5148\u9a8c\u77e5\u8bc6\u63d0\u4f9b\u4e86\u7a33\u5b9a\u6027\uff0c\u800c\u7eaf\u7cb9\u7684\u6570\u636e\u9a71\u52a8\u6a21\u578b\u4f1a\u964d\u4f4e\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u7684\u8d21\u732e\u662f\uff1a(i)\u5c06pileup\u7f13\u89e3\u539f\u5219\u8f6c\u5316\u4e3a\u6a21\u5757\u5316\u5f52\u7eb3\u504f\u5dee\uff1b(ii)\u5c06\u5b83\u4eec\u96c6\u6210\u5230UNet\u4e2d\uff1b(iii)\u5728\u4e0d\u4f9d\u8d56\u91cd\u578bSOTA\u673a\u68b0\u7684\u60c5\u51b5\u4e0b\uff0c\u8bc1\u660e\u4e86\u5728\u9ad8\u566a\u58f0\u4e0b\u7684\u9c81\u68d2\u6027\u589e\u76ca\u3002"}}
{"id": "2509.06056", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06056", "abs": "https://arxiv.org/abs/2509.06056", "authors": ["Chun Wang"], "title": "A novel biomass fluidized bed gasification model coupled with machine learning and CFD simulation", "comment": null, "summary": "A coupling model of biomass fluidized bed gasification based on machine\nlearning and computational fluid dynamics is proposed to improve the prediction\naccuracy and computational efficiency of complex thermochemical reaction\nprocess. By constructing a high-quality data set based on experimental data and\nhigh fidelity simulation results, the agent model used to describe the\ncharacteristics of reaction kinetics was trained and embedded into the\ncomputational fluid dynamics (CFD) framework to realize the real-time update of\nreaction rate and composition evolution.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u548c\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66\u7684\u751f\u7269\u8d28\u6d41\u5316\u5e8a\u6c14\u5316\u8026\u5408\u6a21\u578b\uff0c\u4ee5\u63d0\u9ad8\u590d\u6742\u70ed\u5316\u5b66\u53cd\u5e94\u8fc7\u7a0b\u7684\u9884\u6d4b\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u590d\u6742\u70ed\u5316\u5b66\u53cd\u5e94\u8fc7\u7a0b\u7684\u9884\u6d4b\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u57fa\u4e8e\u5b9e\u9a8c\u6570\u636e\u548c\u9ad8\u4fdd\u771f\u6a21\u62df\u7ed3\u679c\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u7528\u4e8e\u63cf\u8ff0\u53cd\u5e94\u52a8\u529b\u5b66\u7279\u5f81\u7684\u4ee3\u7406\u6a21\u578b\uff0c\u5e76\u5c06\u5176\u5d4c\u5165\u5230\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66\uff08CFD\uff09\u6846\u67b6\u4e2d\uff0c\u4ee5\u5b9e\u73b0\u53cd\u5e94\u901f\u7387\u548c\u7ec4\u6210\u6f14\u5316\u7684\u5b9e\u65f6\u66f4\u65b0\u3002", "result": "\u5b9e\u73b0\u4e86\u53cd\u5e94\u901f\u7387\u548c\u7ec4\u6210\u6f14\u5316\u7684\u5b9e\u65f6\u66f4\u65b0\u3002", "conclusion": "\u8be5\u6a21\u578b\u63d0\u9ad8\u4e86\u590d\u6742\u70ed\u5316\u5b66\u53cd\u5e94\u8fc7\u7a0b\u7684\u9884\u6d4b\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2509.06277", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06277", "abs": "https://arxiv.org/abs/2509.06277", "authors": ["Jinju Kim", "Taehan Kim", "Abdul Waheed", "Rita Singh"], "title": "No Encore: Unlearning as Opt-Out in Music Generation", "comment": "Work in progress. 7 pages", "summary": "AI music generation is rapidly emerging in the creative industries, enabling\nintuitive music generation from textual descriptions. However, these systems\npose risks in exploitation of copyrighted creations, raising ethical and legal\nconcerns. In this paper, we present preliminary results on the first\napplication of machine unlearning techniques from an ongoing research to\nprevent inadvertent usage of creative content. Particularly, we explore\nexisting methods in machine unlearning to a pre-trained Text-to-Music (TTM)\nbaseline and analyze their efficacy in unlearning pre-trained datasets without\nharming model performance. Through our experiments, we provide insights into\nthe challenges of applying unlearning in music generation, offering a\nfoundational analysis for future works on the application of unlearning for\nmusic generative models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86AI\u97f3\u4e50\u751f\u6210\u4e2d\u7684\u7248\u6743\u95ee\u9898\uff0c\u5e76\u5c1d\u8bd5\u5e94\u7528\u673a\u5668\u5b66\u4e60\u53d6\u6d88\u5b66\u4e60\u6280\u672f\u6765\u907f\u514d\u4fb5\u6743\u3002", "motivation": "AI\u97f3\u4e50\u751f\u6210\u7cfb\u7edf\u5b58\u5728\u6ee5\u7528\u7248\u6743\u5185\u5bb9\u7684\u98ce\u9669\uff0c\u5f15\u53d1\u4f26\u7406\u548c\u6cd5\u5f8b\u95ee\u9898\u3002", "method": "\u5c06\u73b0\u6709\u7684\u673a\u5668\u5b66\u4e60\u53d6\u6d88\u5b66\u4e60\u65b9\u6cd5\u5e94\u7528\u4e8e\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u97f3\u4e50\uff08TTM\uff09\u57fa\u7ebf\u6a21\u578b\u3002", "result": "\u5206\u6790\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u5728\u53d6\u6d88\u5b66\u4e60\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u540c\u65f6\u8bc4\u4f30\u4e86\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "\u4e3a\u672a\u6765\u5728\u97f3\u4e50\u751f\u6210\u6a21\u578b\u4e2d\u5e94\u7528\u53d6\u6d88\u5b66\u4e60\u6280\u672f\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u97f3\u4e50\u751f\u6210\u4e2d\u5e94\u7528\u53d6\u6d88\u5b66\u4e60\u7684\u6311\u6218\u7684\u89c1\u89e3\u3002"}}
{"id": "2509.06503", "categories": ["cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.06503", "abs": "https://arxiv.org/abs/2509.06503", "authors": ["Eser Ayg\u00fcn", "Anastasiya Belyaeva", "Gheorghe Comanici", "Marc Coram", "Hao Cui", "Jake Garrison", "Renee Johnston Anton Kast", "Cory Y. McLean", "Peter Norgaard", "Zahra Shamsi", "David Smalling", "James Thompson", "Subhashini Venugopalan", "Brian P. Williams", "Chujun He", "Sarah Martinson", "Martyna Plomecka", "Lai Wei", "Yuchen Zhou", "Qian-Ze Zhu", "Matthew Abraham", "Erica Brand", "Anna Bulanova", "Jeffrey A. Cardille", "Chris Co", "Scott Ellsworth", "Grace Joseph", "Malcolm Kane", "Ryan Krueger", "Johan Kartiwa", "Dan Liebling", "Jan-Matthis Lueckmann", "Paul Raccuglia", "Xuefei", "Wang", "Katherine Chou", "James Manyika", "Yossi Matias", "John C. Platt", "Lizzie Dorfman", "Shibl Mourad", "Michael P. Brenner"], "title": "An AI system to help scientists write expert-level empirical software", "comment": "71 pages, 26 figures", "summary": "The cycle of scientific discovery is frequently bottlenecked by the slow,\nmanual creation of software to support computational experiments. To address\nthis, we present an AI system that creates expert-level scientific software\nwhose goal is to maximize a quality metric. The system uses a Large Language\nModel (LLM) and Tree Search (TS) to systematically improve the quality metric\nand intelligently navigate the large space of possible solutions. The system\nachieves expert-level results when it explores and integrates complex research\nideas from external sources. The effectiveness of tree search is demonstrated\nacross a wide range of benchmarks. In bioinformatics, it discovered 40 novel\nmethods for single-cell data analysis that outperformed the top human-developed\nmethods on a public leaderboard. In epidemiology, it generated 14 models that\noutperformed the CDC ensemble and all other individual models for forecasting\nCOVID-19 hospitalizations. Our method also produced state-of-the-art software\nfor geospatial analysis, neural activity prediction in zebrafish, time series\nforecasting and numerical solution of integrals. By devising and implementing\nnovel solutions to diverse tasks, the system represents a significant step\ntowards accelerating scientific progress.", "AI": {"tldr": "AI\u7cfb\u7edf\u81ea\u52a8\u521b\u5efa\u9ad8\u8d28\u91cf\u79d1\u7814\u8f6f\u4ef6\uff0c\u52a0\u901f\u79d1\u7814\u8fdb\u5c55\u3002", "motivation": "\u79d1\u7814\u8f6f\u4ef6\u5f00\u53d1\u7f13\u6162\uff0c\u963b\u788d\u79d1\u5b66\u53d1\u73b0\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u6811\u641c\u7d22\uff0c\u6700\u5927\u5316\u8d28\u91cf\u6307\u6807\u3002", "result": "\u5728\u751f\u7269\u4fe1\u606f\u5b66\u3001\u6d41\u884c\u75c5\u5b66\u7b49\u9886\u57df\u8d85\u8d8a\u4eba\u7c7b\u4e13\u5bb6\u6c34\u5e73\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u662f\u52a0\u901f\u79d1\u5b66\u8fdb\u5c55\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2509.05669", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05669", "abs": "https://arxiv.org/abs/2509.05669", "authors": ["Weijie Shen", "Xinrui Wang", "Yuanqi Nie", "Apiradee Boonmee"], "title": "Context-Aware Multi-Turn Visual-Textual Reasoning in LVLMs via Dynamic Memory and Adaptive Visual Guidance", "comment": null, "summary": "Current Large Language Models (LLMs) and Vision-Language Large Models (LVLMs)\nexcel in single-turn tasks but face significant challenges in multi-turn\ninteractions requiring deep contextual understanding and complex visual\nreasoning, often leading to fragmented reasoning, context loss, and\nhallucinations. To address these limitations, we propose Context-Aware\nMulti-Turn Visual Reasoning (CAMVR), a novel framework designed to empower\nLVLMs with robust and coherent multi-turn visual-textual inference\ncapabilities. CAMVR introduces two key innovations: a Visual-Textual Context\nMemory Unit (VCMU), a dynamic read-write memory network that stores and manages\ncritical visual features, textual semantic representations, and their\ncross-modal correspondences from each interaction turn; and an Adaptive Visual\nFocus Guidance (AVFG) mechanism, which leverages the VCMU's context to\ndynamically adjust the visual encoder's attention to contextually relevant\nimage regions. Our multi-level reasoning integration strategy ensures that\nresponse generation is deeply coherent with both current inputs and accumulated\nhistorical context. Extensive experiments on challenging datasets, including\nVisDial, an adapted A-OKVQA, and our novel Multi-Turn Instruction Following\n(MTIF) dataset, demonstrate that CAMVR consistently achieves state-of-the-art\nperformance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u65e8\u5728\u589e\u5f3a LVLM \u7684\u591a\u8f6e\u89c6\u89c9\u6587\u672c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u548c\u89c6\u89c9\u8bed\u8a00\u5927\u578b\u6a21\u578b (LVLM) \u5728\u5355\u8f6e\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9700\u8981\u6df1\u5ea6\u4e0a\u4e0b\u6587\u7406\u89e3\u548c\u590d\u6742\u89c6\u89c9\u63a8\u7406\u7684\u591a\u8f6e\u4ea4\u4e92\u4e2d\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u7ecf\u5e38\u5bfc\u81f4\u63a8\u7406\u788e\u7247\u5316\u3001\u4e0a\u4e0b\u6587\u4e22\u5931\u548c\u5e7b\u89c9\u3002", "method": "\u8be5\u6846\u67b6\u5f15\u5165\u4e86\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a\u89c6\u89c9\u6587\u672c\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u5355\u5143 (VCMU) \u548c\u81ea\u9002\u5e94\u89c6\u89c9\u7126\u70b9\u5f15\u5bfc (AVFG) \u673a\u5236\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\uff08\u5305\u62ec VisDial\u3001\u7ecf\u8fc7\u8c03\u6574\u7684 A-OKVQA \u548c\u6211\u4eec\u65b0\u9896\u7684\u591a\u8f6e\u6307\u4ee4\u8ddf\u968f (MTIF) \u6570\u636e\u96c6\uff09\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCAMVR \u59cb\u7ec8\u80fd\u591f\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "CAMVR \u80fd\u591f\u6301\u7eed\u5b9e\u73b0\u6700\u4f73\u6027\u80fd"}}
{"id": "2509.06060", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06060", "abs": "https://arxiv.org/abs/2509.06060", "authors": ["Fei Wang", "Yujie Li", "Zezhi Shao", "Chengqing Yu", "Yisong Fu", "Zhulin An", "Yongjun Xu", "Xueqi Cheng"], "title": "ARIES: Relation Assessment and Model Recommendation for Deep Time Series Forecasting", "comment": null, "summary": "Recent advancements in deep learning models for time series forecasting have\nbeen significant. These models often leverage fundamental time series\nproperties such as seasonality and non-stationarity, which may suggest an\nintrinsic link between model performance and data properties. However, existing\nbenchmark datasets fail to offer diverse and well-defined temporal patterns,\nrestricting the systematic evaluation of such connections. Additionally, there\nis no effective model recommendation approach, leading to high time and cost\nexpenditures when testing different architectures across different downstream\napplications. For those reasons, we propose ARIES, a framework for assessing\nrelation between time series properties and modeling strategies, and for\nrecommending deep forcasting models for realistic time series. First, we\nconstruct a synthetic dataset with multiple distinct patterns, and design a\ncomprehensive system to compute the properties of time series. Next, we conduct\nan extensive benchmarking of over 50 forecasting models, and establish the\nrelationship between time series properties and modeling strategies. Our\nexperimental results reveal a clear correlation. Based on these findings, we\npropose the first deep forecasting model recommender, capable of providing\ninterpretable suggestions for real-world time series. In summary, ARIES is the\nfirst study to establish the relations between the properties of time series\ndata and modeling strategies, while also implementing a model recommendation\nsystem. The code is available at: https://github.com/blisky-li/ARIES.", "AI": {"tldr": "ARIES\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u65f6\u95f4\u5e8f\u5217\u5c5e\u6027\u548c\u5efa\u6a21\u7b56\u7565\u4e4b\u95f4\u5173\u7cfb\u5e76\u4e3a\u5b9e\u9645\u65f6\u95f4\u5e8f\u5217\u63a8\u8350\u6df1\u5ea6\u9884\u6d4b\u6a21\u578b\u7684\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u6570\u636e\u96c6\u672a\u80fd\u63d0\u4f9b\u591a\u6837\u4e14\u660e\u786e\u7684\u65f6\u95f4\u6a21\u5f0f\uff0c\u9650\u5236\u4e86\u5bf9\u6a21\u578b\u6027\u80fd\u548c\u6570\u636e\u5c5e\u6027\u4e4b\u95f4\u5185\u5728\u8054\u7cfb\u7684\u7cfb\u7edf\u8bc4\u4f30\u3002\u6b64\u5916\uff0c\u6ca1\u6709\u6709\u6548\u7684\u6a21\u578b\u63a8\u8350\u65b9\u6cd5\uff0c\u5bfc\u81f4\u5728\u4e0d\u540c\u4e0b\u6e38\u5e94\u7528\u4e2d\u6d4b\u8bd5\u4e0d\u540c\u67b6\u6784\u65f6\u82b1\u8d39\u5927\u91cf\u65f6\u95f4\u548c\u6210\u672c\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u5177\u6709\u591a\u4e2a\u4e0d\u540c\u6a21\u5f0f\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u5e76\u8bbe\u8ba1\u4e00\u4e2a\u7efc\u5408\u7cfb\u7edf\u6765\u8ba1\u7b97\u65f6\u95f4\u5e8f\u5217\u7684\u5c5e\u6027\u3002\u5bf9 50 \u591a\u4e2a\u9884\u6d4b\u6a21\u578b\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u5efa\u7acb\u4e86\u65f6\u95f4\u5e8f\u5217\u5c5e\u6027\u548c\u5efa\u6a21\u7b56\u7565\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5b58\u5728\u660e\u663e\u7684\u76f8\u5173\u6027\u3002", "conclusion": "ARIES\u662f\u7b2c\u4e00\u4e2a\u5efa\u7acb\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5c5e\u6027\u548c\u5efa\u6a21\u7b56\u7565\u4e4b\u95f4\u5173\u7cfb\u7684\u7814\u7a76\uff0c\u540c\u65f6\u8fd8\u5b9e\u73b0\u4e86\u6a21\u578b\u63a8\u8350\u7cfb\u7edf\u3002"}}
{"id": "2509.06350", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.06350", "abs": "https://arxiv.org/abs/2509.06350", "authors": ["Junjie Mu", "Zonghao Ying", "Zhekui Fan", "Zonglei Jing", "Yaoyuan Zhang", "Zhengmin Yu", "Wenxin Zhang", "Quanchen Zou", "Xiangzheng Zhang"], "title": "Mask-GCG: Are All Tokens in Adversarial Suffixes Necessary for Jailbreak Attacks?", "comment": null, "summary": "Jailbreak attacks on Large Language Models (LLMs) have demonstrated various\nsuccessful methods whereby attackers manipulate models into generating harmful\nresponses that they are designed to avoid. Among these, Greedy Coordinate\nGradient (GCG) has emerged as a general and effective approach that optimizes\nthe tokens in a suffix to generate jailbreakable prompts. While several\nimproved variants of GCG have been proposed, they all rely on fixed-length\nsuffixes. However, the potential redundancy within these suffixes remains\nunexplored. In this work, we propose Mask-GCG, a plug-and-play method that\nemploys learnable token masking to identify impactful tokens within the suffix.\nOur approach increases the update probability for tokens at high-impact\npositions while pruning those at low-impact positions. This pruning not only\nreduces redundancy but also decreases the size of the gradient space, thereby\nlowering computational overhead and shortening the time required to achieve\nsuccessful attacks compared to GCG. We evaluate Mask-GCG by applying it to the\noriginal GCG and several improved variants. Experimental results show that most\ntokens in the suffix contribute significantly to attack success, and pruning a\nminority of low-impact tokens does not affect the loss values or compromise the\nattack success rate (ASR), thereby revealing token redundancy in LLM prompts.\nOur findings provide insights for developing efficient and interpretable LLMs\nfrom the perspective of jailbreak attacks.", "AI": {"tldr": "\u63d0\u51fa\u4e86Mask-GCG\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684token masking\u8bc6\u522bsuffix\u4e2d\u7684\u91cd\u8981tokens\uff0c\u51cf\u5c11\u5197\u4f59\u5e76\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\uff0c\u52a0\u901fjailbreak\u653b\u51fb\u3002", "motivation": "\u73b0\u6709GCG\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u957f\u5ea6\u7684\u540e\u7f00\uff0c\u4f46\u8fd9\u4e9b\u540e\u7f00\u4e2d\u53ef\u80fd\u5b58\u5728\u5197\u4f59\uff0c\u672a\u88ab\u63a2\u7d22\u3002", "method": "\u91c7\u7528\u53ef\u5b66\u4e60\u7684token masking\u6765\u8bc6\u522bsuffix\u4e2d\u7684\u91cd\u8981tokens\uff0c\u589e\u52a0\u9ad8\u5f71\u54cd\u529b\u4f4d\u7f6e\u7684tokens\u7684\u66f4\u65b0\u6982\u7387\uff0c\u5e76\u526a\u9664\u4f4e\u5f71\u54cd\u529b\u4f4d\u7f6e\u7684tokens\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0csuffix\u4e2d\u7684\u5927\u591a\u6570tokens\u5bf9\u653b\u51fb\u6210\u529f\u6709\u663e\u8457\u8d21\u732e\uff0c\u526a\u9664\u5c11\u6570\u4f4e\u5f71\u54cd\u529btokens\u4e0d\u5f71\u54cd\u635f\u5931\u503c\u6216\u653b\u51fb\u6210\u529f\u7387\u3002", "conclusion": "\u63ed\u793a\u4e86LLM prompt\u4e2d\u7684token\u5197\u4f59\uff0c\u4e3a\u4ecejailbreak\u653b\u51fb\u7684\u89d2\u5ea6\u5f00\u53d1\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684LLM\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2509.06641", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06641", "abs": "https://arxiv.org/abs/2509.06641", "authors": ["Zhou-Peng Shou", "Zhi-Qiang You", "Fang Wang", "Hai-Bo Liu"], "title": "CogGuide: Human-Like Guidance for Zero-Shot Omni-Modal Reasoning", "comment": null, "summary": "Targeting the issues of \"shortcuts\" and insufficient contextual understanding\nin complex cross-modal reasoning of multimodal large models, this paper\nproposes a zero-shot multimodal reasoning component guided by human-like\ncognitive strategies centered on an \"intent sketch\". The component comprises a\nplug-and-play three-module pipeline-Intent Perceiver, Strategy Generator, and\nStrategy Selector-that explicitly constructs a \"understand-plan-select\"\ncognitive process. By generating and filtering \"intent sketch\" strategies to\nguide the final reasoning, it requires no parameter fine-tuning and achieves\ncross-model transfer solely through in-context engineering.\nInformation-theoretic analysis shows that this process can reduce conditional\nentropy and improve information utilization efficiency, thereby suppressing\nunintended shortcut reasoning. Experiments on IntentBench, WorldSense, and\nDaily-Omni validate the method's generality and robust gains; compared with\ntheir respective baselines, the complete \"three-module\" scheme yields\nconsistent improvements across different reasoning engines and pipeline\ncombinations, with gains up to approximately 9.51 percentage points,\ndemonstrating the practical value and portability of the \"intent sketch\"\nreasoning component in zero-shot scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u591a\u6a21\u6001\u63a8\u7406\u7ec4\u4ef6\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u7b56\u7565\uff0c\u4ee5\u201c\u610f\u56fe\u8349\u56fe\u201d\u4e3a\u4e2d\u5fc3\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u5927\u6a21\u578b\u4e2d\u5b58\u5728\u7684\u201c\u6377\u5f84\u201d\u95ee\u9898\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u590d\u6742\u8de8\u6a21\u6001\u63a8\u7406\u4e2d\u5b58\u5728\u7684\u201c\u6377\u5f84\u201d\u95ee\u9898\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u8be5\u7ec4\u4ef6\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a\u610f\u56fe\u611f\u77e5\u5668\u3001\u7b56\u7565\u751f\u6210\u5668\u548c\u7b56\u7565\u9009\u62e9\u5668\uff0c\u901a\u8fc7\u6784\u5efa\u201c\u7406\u89e3-\u8ba1\u5212-\u9009\u62e9\u201d\u7684\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u751f\u6210\u548c\u8fc7\u6ee4\u201c\u610f\u56fe\u8349\u56fe\u201d\u7b56\u7565\u6765\u6307\u5bfc\u6700\u7ec8\u63a8\u7406\u3002", "result": "\u5728IntentBench\u3001WorldSense\u548cDaily-Omni\u7b49\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u901a\u7528\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e0e\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5b8c\u6574\u7684\u201c\u4e09\u6a21\u5757\u201d\u65b9\u6848\u5728\u4e0d\u540c\u7684\u63a8\u7406\u5f15\u64ce\u548cpipeline\u7ec4\u5408\u4e2d\u90fd\u53d6\u5f97\u4e86\u6301\u7eed\u7684\u6539\u8fdb\uff0c\u6536\u76ca\u9ad8\u8fbe\u7ea69.51\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u201c\u610f\u56fe\u8349\u56fe\u201d\u63a8\u7406\u7ec4\u4ef6\u5728\u96f6\u6837\u672c\u573a\u666f\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u548c\u53ef\u79fb\u690d\u6027\u3002"}}
{"id": "2509.05670", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05670", "abs": "https://arxiv.org/abs/2509.05670", "authors": ["Ga\u0161per Podobnik", "Toma\u017e Vrtovec"], "title": "MeshMetrics: A Precise Implementation of Distance-Based Image Segmentation Metrics", "comment": null, "summary": "The surge of research in image segmentation has yielded remarkable\nperformance gains but also exposed a reproducibility crisis. A major\ncontributor is performance evaluation, where both selection and implementation\nof metrics play critical roles. While recent efforts have improved the former,\nthe reliability of metric implementation has received far less attention.\nPitfalls in distance-based metric implementation can lead to considerable\ndiscrepancies between common open-source tools, for instance, exceeding 100 mm\nfor the Hausdorff distance and 30%pt for the normalized surface distance for\nthe same pair of segmentations. To address these pitfalls, we introduce\nMeshMetrics, a mesh-based framework that provides a more precise computation of\ndistance-based metrics than conventional grid-based approaches. Through\ntheoretical analysis and empirical validation, we demonstrate that MeshMetrics\nachieves higher accuracy and precision than established tools, and is\nsubstantially less affected by discretization artifacts, such as distance\nquantization. We release MeshMetrics as an open-source Python package,\navailable at https://github.com/gasperpodobnik/MeshMetrics.", "AI": {"tldr": "\u56fe\u50cf\u5206\u5272\u7814\u7a76\u6fc0\u589e\uff0c\u4f46\u91cd\u73b0\u6027\u9762\u4e34\u5371\u673a\uff0c\u8bc4\u4f30\u6307\u6807\u7684\u9009\u62e9\u548c\u5b9e\u65bd\u81f3\u5173\u91cd\u8981\u3002\u8ddd\u79bb\u5ea6\u91cf\u6807\u51c6\u5b9e\u65bd\u4e2d\u7684\u7f3a\u9677\u53ef\u80fd\u5bfc\u81f4\u5f00\u6e90\u5de5\u5177\u4e4b\u95f4\u7684\u5de8\u5927\u5dee\u5f02\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5f15\u5165\u4e86 MeshMetrics\uff0c\u5b83\u662f\u4e00\u4e2a\u57fa\u4e8e\u7f51\u683c\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u6bd4\u4f20\u7edf\u7684\u57fa\u4e8e\u7f51\u683c\u7684\u65b9\u6cd5\u66f4\u7cbe\u786e\u5730\u8ba1\u7b97\u57fa\u4e8e\u8ddd\u79bb\u7684\u5ea6\u91cf\u6807\u51c6\u3002", "motivation": "\u56fe\u50cf\u5206\u5272\u9886\u57df\u7684\u7814\u7a76\u6fc0\u589e\uff0c\u4f46\u91cd\u73b0\u6027\u9762\u4e34\u5371\u673a\uff0c\u5176\u4e2d\u8bc4\u4f30\u6307\u6807\u7684\u5b9e\u65bd\u662f\u4e3b\u8981\u539f\u56e0\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86 MeshMetrics\uff0c\u4e00\u4e2a\u57fa\u4e8e\u7f51\u683c\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u66f4\u7cbe\u786e\u5730\u8ba1\u7b97\u57fa\u4e8e\u8ddd\u79bb\u7684\u5ea6\u91cf\u6807\u51c6\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u57fa\u4e8e\u7f51\u683c\u7684\u65b9\u6cd5\u3002", "result": "MeshMetrics \u6bd4\u5df2\u5efa\u7acb\u7684\u5de5\u5177\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u7cbe\u5ea6\uff0c\u5e76\u4e14\u53d7\u79bb\u6563\u5316\u4f2a\u5f71\u7684\u5f71\u54cd\u66f4\u5c0f\u3002", "conclusion": "\u6211\u4eec\u53d1\u5e03\u4e86 MeshMetrics\uff0c\u4e00\u4e2a\u5f00\u6e90 Python \u5305\uff0c\u5b83\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7cbe\u786e\u7684\u8ddd\u79bb\u5ea6\u91cf\u8ba1\u7b97\u65b9\u6cd5\uff0c\u7ecf\u9a8c\u8bc1\u6bd4\u73b0\u6709\u5de5\u5177\u66f4\u51c6\u786e\u3001\u66f4\u7cbe\u786e\u3002"}}
{"id": "2509.06067", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06067", "abs": "https://arxiv.org/abs/2509.06067", "authors": ["Mianjun Xiao", "Peng Song", "Yulong Liu", "Cedric Korte", "Ziyang Xu", "Jiale Gao", "Jiaqi Lu", "Haoyang Nie", "Qiantong Deng", "Timing Qu"], "title": "A Surrogate model for High Temperature Superconducting Magnets to Predict Current Distribution with Neural Network", "comment": null, "summary": "Finite element method (FEM) is widely used in high-temperature\nsuperconducting (HTS) magnets, but its computational cost increases with magnet\nsize and becomes time-consuming for meter-scale magnets, especially when\nmulti-physics couplings are considered, which limits the fast design of\nlarge-scale REBCO magnet systems. In this work, a surrogate model based on a\nfully connected residual neural network (FCRN) is developed to predict the\nspace-time current density distribution in REBCO solenoids. Training datasets\nwere generated from FEM simulations with varying numbers of turns and pancakes.\nThe results demonstrate that, for deeper networks, the FCRN architecture\nachieves better convergence than conventional fully connected network (FCN),\nwith the configuration of 12 residual blocks and 256 neurons per layer\nproviding the most favorable balance between training accuracy and\ngeneralization capability. Extrapolation studies show that the model can\nreliably predict magnetization losses for up to 50% beyond the training range,\nwith maximum errors below 10%. The surrogate model achieves predictions several\norders of magnitude faster than FEM and still remains advantageous when\ntraining costs are included. These results indicate that the proposed\nFCRN-based surrogate model provides both accuracy and efficiency, offering a\npromising tool for the rapid analysis of large-scale HTS magnets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5168\u8fde\u63a5\u6b8b\u5dee\u795e\u7ecf\u7f51\u7edc\uff08FCRN\uff09\u7684\u66ff\u4ee3\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4bREBCO\u87ba\u7ebf\u7ba1\u4e2d\u7684\u65f6\u7a7a\u7535\u6d41\u5bc6\u5ea6\u5206\u5e03\u3002", "motivation": "\u6709\u9650\u5143\u65b9\u6cd5\uff08FEM\uff09\u5e7f\u6cdb\u5e94\u7528\u4e8e\u9ad8\u6e29\u8d85\u5bfc\uff08HTS\uff09\u78c1\u4f53\uff0c\u4f46\u5176\u8ba1\u7b97\u6210\u672c\u968f\u7740\u78c1\u4f53\u5c3a\u5bf8\u7684\u589e\u52a0\u800c\u589e\u52a0\uff0c\u5bf9\u4e8e\u7c73\u7ea7\u78c1\u4f53\u800c\u8a00\u975e\u5e38\u8017\u65f6\uff0c\u5c24\u5176\u662f\u5728\u8003\u8651\u591a\u7269\u7406\u573a\u8026\u5408\u65f6\uff0c\u8fd9\u9650\u5236\u4e86\u5927\u578bREBCO\u78c1\u4f53\u7cfb\u7edf\u7684\u5feb\u901f\u8bbe\u8ba1\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5168\u8fde\u63a5\u6b8b\u5dee\u795e\u7ecf\u7f51\u7edc\uff08FCRN\uff09\u7684\u66ff\u4ee3\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4bREBCO\u87ba\u7ebf\u7ba1\u4e2d\u7684\u65f6\u7a7a\u7535\u6d41\u5bc6\u5ea6\u5206\u5e03\u3002\u8bad\u7ec3\u6570\u636e\u96c6\u6765\u81eaFEM\u6a21\u62df\uff0c\u5177\u6709\u4e0d\u540c\u6570\u91cf\u7684\u531d\u6570\u548c\u997c\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u4e8e\u66f4\u6df1\u7684\u7f51\u7edc\uff0cFCRN\u67b6\u6784\u6bd4\u4f20\u7edf\u7684\u5168\u8fde\u63a5\u7f51\u7edc\uff08FCN\uff09\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6536\u655b\u6027\uff0c\u5177\u670912\u4e2a\u6b8b\u5dee\u5757\u548c\u6bcf\u5c42256\u4e2a\u795e\u7ecf\u5143\u7684\u914d\u7f6e\uff0c\u5728\u8bad\u7ec3\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u4e4b\u95f4\u63d0\u4f9b\u4e86\u6700\u6709\u5229\u7684\u5e73\u8861\u3002\u5916\u63a8\u7814\u7a76\u8868\u660e\uff0c\u8be5\u6a21\u578b\u53ef\u4ee5\u53ef\u9760\u5730\u9884\u6d4b\u9ad8\u8fbe\u8bad\u7ec3\u8303\u56f450%\u7684\u78c1\u5316\u635f\u8017\uff0c\u6700\u5927\u8bef\u5dee\u4f4e\u4e8e10%\u3002", "conclusion": "\u57fa\u4e8eFCRN\u7684\u66ff\u4ee3\u6a21\u578b\u517c\u5177\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u4e3a\u5927\u578bHTS\u78c1\u4f53\u7684\u5feb\u901f\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u9014\u7684\u5de5\u5177\u3002"}}
{"id": "2509.06356", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06356", "abs": "https://arxiv.org/abs/2509.06356", "authors": ["Ao Chang", "Yubo Chen", "Jun Zhao"], "title": "PL-CA: A Parametric Legal Case Augmentation Framework", "comment": null, "summary": "Conventional RAG is considered one of the most effective methods for\naddressing model knowledge insufficiency and hallucination, particularly in the\njudicial domain that requires high levels of knowledge rigor, logical\nconsistency, and content integrity. However, the conventional RAG method only\ninjects retrieved documents directly into the model's context, which severely\nconstrains models due to their limited context windows and introduces\nadditional computational overhead through excessively long contexts, thereby\ndisrupting models' attention and degrading performance on downstream tasks.\nMoreover, many existing benchmarks lack expert annotation and focus solely on\nindividual downstream tasks while real-world legal scenarios consist of\nmultiple mixed legal tasks, indicating conventional benchmarks' inadequacy for\nreflecting models' true capabilities. To address these limitations, we propose\nPL-CA, which introduces a parametric RAG (P-RAG) framework to perform data\naugmentation on corpus knowledge and encode this legal knowledge into\nparametric vectors, and then integrates this parametric knowledge into the\nLLM's feed-forward networks (FFN) via LoRA, thereby alleviating models' context\npressure. Additionally, we also construct a multi-task legal dataset comprising\nmore than 2000 training and test instances, which are all expert-annotated and\nmanually verified. We conduct our experiments on our dataset, and the\nexperimental results demonstrate that our method reduces the overhead\nassociated with excessively long contexts while maintaining competitive\nperformance on downstream tasks compared to conventional RAG. Our code and\ndataset are provided in the appendix.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u5316\u7684RAG (P-RAG) \u6846\u67b6PL-CA\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u7f16\u7801\u5c06\u6cd5\u5f8b\u77e5\u8bc6\u6ce8\u5165\u5230LLM\u4e2d\uff0c\u7f13\u89e3\u4e86\u6a21\u578b\u4e0a\u4e0b\u6587\u538b\u529b\u3002", "motivation": "\u4f20\u7edfRAG\u65b9\u6cd5\u76f4\u63a5\u5c06\u68c0\u7d22\u5230\u7684\u6587\u6863\u6ce8\u5165\u6a21\u578b\u4e0a\u4e0b\u6587\uff0c\u5bfc\u81f4\u6a21\u578b\u4e0a\u4e0b\u6587\u7a97\u53e3\u53d7\u9650\uff0c\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002\u73b0\u6709\u57fa\u51c6\u7f3a\u4e4f\u4e13\u5bb6\u6ce8\u91ca\uff0c\u4e14\u4fa7\u91cd\u4e8e\u5355\u4e2a\u4e0b\u6e38\u4efb\u52a1\uff0c\u65e0\u6cd5\u53cd\u6620\u6a21\u578b\u7684\u771f\u5b9e\u80fd\u529b\u3002", "method": "\u63d0\u51faPL-CA\u6846\u67b6\uff0c\u5229\u7528\u53c2\u6570\u5316RAG\u5bf9\u8bed\u6599\u77e5\u8bc6\u8fdb\u884c\u6570\u636e\u589e\u5f3a\uff0c\u5e76\u5c06\u6cd5\u5f8b\u77e5\u8bc6\u7f16\u7801\u4e3a\u53c2\u6570\u5316\u5411\u91cf\uff0c\u7136\u540e\u901a\u8fc7LoRA\u5c06\u53c2\u6570\u5316\u77e5\u8bc6\u96c6\u6210\u5230LLM\u7684\u524d\u9988\u7f51\u7edc\uff08FFN\uff09\u4e2d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e86\u8fc7\u957f\u4e0a\u4e0b\u6587\u5e26\u6765\u7684\u5f00\u9500\uff0c\u540c\u65f6\u5728\u4e0b\u6e38\u4efb\u52a1\u4e0a\u4fdd\u6301\u4e86\u4e0e\u4f20\u7edfRAG\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684PL-CA\u6846\u67b6\u6709\u6548\u7f13\u89e3\u4e86\u6a21\u578b\u4e0a\u4e0b\u6587\u538b\u529b\uff0c\u5e76\u5728\u591a\u4efb\u52a1\u6cd5\u5f8b\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5b9e\u9a8c\u7ed3\u679c\u3002"}}
{"id": "2509.06733", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06733", "abs": "https://arxiv.org/abs/2509.06733", "authors": ["Wenjun Li", "Zhi Chen", "Jingru Lin", "Hannan Cao", "Wei Han", "Sheng Liang", "Zhi Zhang", "Kuicai Dong", "Dexun Li", "Chen Zhang", "Yong Liu"], "title": "Reinforcement Learning Foundations for Deep Research Systems: A Survey", "comment": "38 pages, first version", "summary": "Deep research systems, agentic AI that solve complex, multi-step tasks by\ncoordinating reasoning, search across the open web and user files, and tool\nuse, are moving toward hierarchical deployments with a Planner, Coordinator,\nand Executors. In practice, training entire stacks end-to-end remains\nimpractical, so most work trains a single planner connected to core tools such\nas search, browsing, and code. While SFT imparts protocol fidelity, it suffers\nfrom imitation and exposure biases and underuses environment feedback.\nPreference alignment methods such as DPO are schema and proxy-dependent,\noff-policy, and weak for long-horizon credit assignment and multi-objective\ntrade-offs. A further limitation of SFT and DPO is their reliance on human\ndefined decision points and subskills through schema design and labeled\ncomparisons. Reinforcement learning aligns with closed-loop, tool-interaction\nresearch by optimizing trajectory-level policies, enabling exploration,\nrecovery behaviors, and principled credit assignment, and it reduces dependence\non such human priors and rater biases.\n  This survey is, to our knowledge, the first dedicated to the RL foundations\nof deep research systems. It systematizes work after DeepSeek-R1 along three\naxes: (i) data synthesis and curation; (ii) RL methods for agentic research\ncovering stability, sample efficiency, long context handling, reward and credit\ndesign, multi-objective optimization, and multimodal integration; and (iii)\nagentic RL training systems and frameworks. We also cover agent architecture\nand coordination, as well as evaluation and benchmarks, including recent QA,\nVQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We\ndistill recurring patterns, surface infrastructure bottlenecks, and offer\npractical guidance for training robust, transparent deep research agents with\nRL.", "AI": {"tldr": "\u672c\u7814\u7a76\u7efc\u8ff0\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60(RL)\u7684\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\uff0c\u8fd9\u7c7b\u7cfb\u7edf\u901a\u8fc7\u534f\u8c03\u63a8\u7406\u3001\u7f51\u7edc\u641c\u7d22\u548c\u5de5\u5177\u4f7f\u7528\u6765\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u8bad\u7ec3\u65b9\u6cd5\u4e0d\u5b9e\u7528\uff0cSFT\u5b58\u5728\u6a21\u4eff\u548c\u504f\u5dee\u95ee\u9898\uff0cDPO\u4f9d\u8d56schema\u548c\u4ee3\u7406\uff0c\u4e14\u5728\u957f\u65f6\u4fe1\u7528\u5206\u914d\u548c\u591a\u76ee\u6807\u6743\u8861\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002SFT\u548cDPO\u8fd8\u4f9d\u8d56\u4e8e\u4eba\u7c7b\u5b9a\u4e49\u7684\u51b3\u7b56\u70b9\u548c\u5b50\u6280\u80fd\u3002", "method": "\u8be5\u7814\u7a76\u7cfb\u7edf\u5730\u6574\u7406\u4e86DeepSeek-R1\u4e4b\u540e\u7684\u5de5\u4f5c\uff0c\u4e3b\u8981\u4ece\u4e09\u4e2a\u65b9\u9762\u8fdb\u884c\uff1a\u6570\u636e\u5408\u6210\u4e0e\u7ba1\u7406\u3001agentic\u7814\u7a76\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3001agentic RL\u8bad\u7ec3\u7cfb\u7edf\u548c\u6846\u67b6\u3002", "result": "\u603b\u7ed3\u4e86\u5e38\u89c1\u6a21\u5f0f\uff0c\u63ed\u793a\u4e86\u57fa\u7840\u8bbe\u65bd\u74f6\u9888\uff0c\u5e76\u4e3a\u4f7f\u7528RL\u8bad\u7ec3\u7a33\u5065\u3001\u900f\u660e\u7684\u6df1\u5ea6\u7814\u7a76agent\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u901a\u8fc7\u4f18\u5316\u8f68\u8ff9\u7ea7\u7b56\u7565\uff0c\u5b9e\u73b0\u63a2\u7d22\u548c\u6062\u590d\u884c\u4e3a\uff0c\u51cf\u5c11\u4e86\u5bf9\u4eba\u7c7b\u5148\u9a8c\u548c\u8bc4\u4f30\u8005\u504f\u5dee\u7684\u4f9d\u8d56\uff0c\u66f4\u9002\u5408\u95ed\u73af\u3001\u5de5\u5177\u4ea4\u4e92\u7814\u7a76\u3002"}}
{"id": "2509.05695", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05695", "abs": "https://arxiv.org/abs/2509.05695", "authors": ["Jingwei Peng", "Zhixuan Qiu", "Boyu Jin", "Surasakdi Siripong"], "title": "Leveraging Vision-Language Large Models for Interpretable Video Action Recognition with Semantic Tokenization", "comment": null, "summary": "Human action recognition often struggles with deep semantic understanding,\ncomplex contextual information, and fine-grained distinction, limitations that\ntraditional methods frequently encounter when dealing with diverse video data.\nInspired by the remarkable capabilities of large language models, this paper\nintroduces LVLM-VAR, a novel framework that pioneers the application of\npre-trained Vision-Language Large Models (LVLMs) to video action recognition,\nemphasizing enhanced accuracy and interpretability. Our method features a\nVideo-to-Semantic-Tokens (VST) Module, which innovatively transforms raw video\nsequences into discrete, semantically and temporally consistent \"semantic\naction tokens,\" effectively crafting an \"action narrative\" that is\ncomprehensible to an LVLM. These tokens, combined with natural language\ninstructions, are then processed by a LoRA-fine-tuned LVLM (e.g., LLaVA-13B)\nfor robust action classification and semantic reasoning. LVLM-VAR not only\nachieves state-of-the-art or highly competitive performance on challenging\nbenchmarks such as NTU RGB+D and NTU RGB+D 120, demonstrating significant\nimprovements (e.g., 94.1% on NTU RGB+D X-Sub and 90.0% on NTU RGB+D 120 X-Set),\nbut also substantially boosts model interpretability by generating natural\nlanguage explanations for its predictions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLVLM-VAR\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u9884\u8bad\u7ec3\u7684\u89c6\u89c9-\u8bed\u8a00\u5927\u6a21\u578b\uff08LVLM\uff09\u8fdb\u884c\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u5404\u79cd\u89c6\u9891\u6570\u636e\u65f6\uff0c\u5728\u6df1\u5ea6\u8bed\u4e49\u7406\u89e3\u3001\u590d\u6742\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u7ec6\u7c92\u5ea6\u533a\u5206\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528\u89c6\u9891\u5230\u8bed\u4e49\u4ee4\u724c\uff08VST\uff09\u6a21\u5757\uff0c\u5c06\u539f\u59cb\u89c6\u9891\u5e8f\u5217\u8f6c\u6362\u4e3a\u79bb\u6563\u7684\u3001\u8bed\u4e49\u548c\u65f6\u95f4\u4e0a\u4e00\u81f4\u7684\u201c\u8bed\u4e49\u52a8\u4f5c\u4ee4\u724c\u201d\uff0c\u4ece\u800c\u6709\u6548\u5730\u521b\u5efaLVLM\u53ef\u4ee5\u7406\u89e3\u7684\u201c\u52a8\u4f5c\u53d9\u8ff0\u201d\u3002\u7136\u540e\uff0c\u5c06\u8fd9\u4e9b\u4ee4\u724c\u4e0e\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u76f8\u7ed3\u5408\uff0c\u7531LoRA\u5fae\u8c03\u7684LVLM\uff08\u4f8b\u5982\uff0cLLaVA-13B\uff09\u5904\u7406\uff0c\u4ee5\u8fdb\u884c\u9c81\u68d2\u7684\u52a8\u4f5c\u5206\u7c7b\u548c\u8bed\u4e49\u63a8\u7406\u3002", "result": "LVLM-VAR\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982NTU RGB+D\u548cNTU RGB+D 120\uff09\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u6216\u6781\u5177\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u663e\u793a\u51fa\u663e\u8457\u7684\u6539\u8fdb\uff08\u4f8b\u5982\uff0c\u5728NTU RGB+D X-Sub\u4e0a\u4e3a94.1%\uff0c\u5728NTU RGB+D 120 X-Set\u4e0a\u4e3a90.0%\uff09\u3002", "conclusion": "LVLM-VAR\u901a\u8fc7\u4e3a\u5176\u9884\u6d4b\u751f\u6210\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u4ece\u800c\u5927\u5927\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2509.06094", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06094", "abs": "https://arxiv.org/abs/2509.06094", "authors": ["S. R. Eshwar"], "title": "Teaching Precommitted Agents: Model-Free Policy Evaluation and Control in Quasi-Hyperbolic Discounted MDPs", "comment": null, "summary": "Time-inconsistent preferences, where agents favor smaller-sooner over\nlarger-later rewards, are a key feature of human and animal decision-making.\nQuasi-Hyperbolic (QH) discounting provides a simple yet powerful model for this\nbehavior, but its integration into the reinforcement learning (RL) framework\nhas been limited. This paper addresses key theoretical and algorithmic gaps for\nprecommitted agents with QH preferences. We make two primary contributions: (i)\nwe formally characterize the structure of the optimal policy, proving for the\nfirst time that it reduces to a simple one-step non-stationary form; and (ii)\nwe design the first practical, model-free algorithms for both policy evaluation\nand Q-learning in this setting, both with provable convergence guarantees. Our\nresults provide foundational insights for incorporating QH preferences in RL.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5177\u6709\u65f6\u95f4\u4e0d\u4e00\u81f4\u504f\u597d\u7684\u667a\u80fd\u4f53\u7684\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u7279\u522b\u662fQuasi-Hyperbolic (QH) \u6298\u6263\u3002", "motivation": "\u7814\u7a76\u4eba\u7c7b\u548c\u52a8\u7269\u51b3\u7b56\u4e2d\u7684\u65f6\u95f4\u4e0d\u4e00\u81f4\u504f\u597d\uff0c\u5e76\u5c06\u5176\u6574\u5408\u5230\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e2d\u3002", "method": "1. \u4ece\u7406\u8bba\u4e0a\u5206\u6790\u4e86\u6700\u4f18\u7b56\u7565\u7684\u7ed3\u6784\uff0c\u8bc1\u660e\u5176\u53ef\u4ee5\u7b80\u5316\u4e3a\u7b80\u5355\u7684\u5355\u6b65\u975e\u5e73\u7a33\u5f62\u5f0f\u3002\n2. \u8bbe\u8ba1\u4e86\u9996\u4e2a\u5b9e\u7528\u7684\u3001\u65e0\u6a21\u578b\u7684\u7b56\u7565\u8bc4\u4f30\u548c Q-learning \u7b97\u6cd5\uff0c\u5e76\u5177\u6709\u53ef\u8bc1\u660e\u7684\u6536\u655b\u6027\u4fdd\u8bc1\u3002", "result": "\u8bc1\u660e\u4e86\u6700\u4f18\u7b56\u7565\u5177\u6709\u7b80\u5355\u7684\u5355\u6b65\u975e\u5e73\u7a33\u5f62\u5f0f\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5177\u6709\u6536\u655b\u6027\u4fdd\u8bc1\u7684\u5b9e\u7528\u7b97\u6cd5\u3002", "conclusion": "\u4e3a\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7ed3\u5408 QH \u504f\u597d\u63d0\u4f9b\u4e86\u57fa\u7840\u6027\u7684\u89c1\u89e3\u3002"}}
{"id": "2509.06401", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06401", "abs": "https://arxiv.org/abs/2509.06401", "authors": ["Ivan Mart\u00ednez-Murillo", "Elena Lloret", "Paloma Moreda", "Albert Gatt"], "title": "Do LLMs exhibit the same commonsense capabilities across languages?", "comment": null, "summary": "This paper explores the multilingual commonsense generation abilities of\nLarge Language Models (LLMs). To facilitate this investigation, we introduce\nMULTICOM, a novel benchmark that extends the COCOTEROS dataset to four\nlanguages: English, Spanish, Dutch, and Valencian. The task involves generating\na commonsensical sentence that includes a given triplet of words. We evaluate a\nrange of open-source LLMs, including LLaMA, Qwen, Gemma, EuroLLM, and\nSalamandra, on this benchmark. Our evaluation combines automatic metrics,\nLLM-as-a-judge approaches (using Prometheus and JudgeLM), and human\nannotations. Results consistently show superior performance in English, with\nsignificantly lower performance in less-resourced languages. While contextual\nsupport yields mixed results, it tends to benefit underrepresented languages.\nThese findings underscore the current limitations of LLMs in multilingual\ncommonsense generation. The dataset is publicly available at\nhttps://huggingface.co/datasets/gplsi/MULTICOM.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u591a\u8bed\u8a00\u5e38\u8bc6\u751f\u6210\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6 MULTICOM\uff0c\u8be5\u6570\u636e\u96c6\u5c06 COCOTEROS \u6570\u636e\u96c6\u6269\u5c55\u5230\u56db\u79cd\u8bed\u8a00\uff1a\u82f1\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u8377\u5170\u8bed\u548c\u5df4\u4f26\u897f\u4e9a\u8bed\u3002\u4efb\u52a1\u662f\u751f\u6210\u4e00\u4e2a\u5305\u542b\u7ed9\u5b9a\u4e09\u4e2a\u5355\u8bcd\u7684\u5e38\u8bc6\u6027\u53e5\u5b50\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u79cd\u8bed\u8a00\u4e2d\u751f\u6210\u5e38\u8bc6\u7684\u80fd\u529b\u3002", "method": "\u4f7f\u7528 MULTICOM \u57fa\u51c6\u8bc4\u4f30\u4e86\u4e00\u7cfb\u5217\u5f00\u6e90 LLM\uff0c\u5305\u62ec LLaMA\u3001Qwen\u3001Gemma\u3001EuroLLM \u548c Salamandra\u3002\u8bc4\u4f30\u7ed3\u5408\u4e86\u81ea\u52a8\u6307\u6807\u3001LLM \u4f5c\u4e3a\u8bc4\u5224\u65b9\u6cd5\u548c\u4eba\u5de5\u6ce8\u91ca\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u82f1\u8bed\u7684\u8868\u73b0\u59cb\u7ec8\u4f18\u4e8e\u5176\u4ed6\u8bed\u8a00\uff0c\u800c\u8d44\u6e90\u8f83\u5c11\u7684\u8bed\u8a00\u7684\u8868\u73b0\u660e\u663e\u8f83\u4f4e\u3002\u4e0a\u4e0b\u6587\u652f\u6301\u4ea7\u751f\u7684\u7ed3\u679c\u597d\u574f\u53c2\u534a\uff0c\u4f46\u5f80\u5f80\u6709\u5229\u4e8e\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u8bed\u8a00\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86 LLM \u5728\u591a\u8bed\u8a00\u5e38\u8bc6\u751f\u6210\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.06736", "categories": ["cs.AI", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06736", "abs": "https://arxiv.org/abs/2509.06736", "authors": ["Jie Yang", "Jiajun Chen", "Zhangyue Yin", "Shuo Chen", "Yuxin Wang", "Yiran Guo", "Yuan Li", "Yining Zheng", "Xuanjing Huang", "Xipeng Qiu"], "title": "VehicleWorld: A Highly Integrated Multi-Device Environment for Intelligent Vehicle Interaction", "comment": null, "summary": "Intelligent vehicle cockpits present unique challenges for API Agents,\nrequiring coordination across tightly-coupled subsystems that exceed typical\ntask environments' complexity. Traditional Function Calling (FC) approaches\noperate statelessly, requiring multiple exploratory calls to build\nenvironmental awareness before execution, leading to inefficiency and limited\nerror recovery. We introduce VehicleWorld, the first comprehensive environment\nfor the automotive domain, featuring 30 modules, 250 APIs, and 680 properties\nwith fully executable implementations that provide real-time state information\nduring agent execution. This environment enables precise evaluation of vehicle\nagent behaviors across diverse, challenging scenarios. Through systematic\nanalysis, we discovered that direct state prediction outperforms function\ncalling for environmental control. Building on this insight, we propose\nState-based Function Call (SFC), a novel approach that maintains explicit\nsystem state awareness and implements direct state transitions to achieve\ntarget conditions. Experimental results demonstrate that SFC significantly\noutperforms traditional FC approaches, achieving superior execution accuracy\nand reduced latency. We have made all implementation code publicly available on\nGithub https://github.com/OpenMOSS/VehicleWorld.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684API\u667a\u80fd\u5ea7\u8231\u73af\u5883VehicleWorld\uff0c\u5e76\u53d1\u73b0\u57fa\u4e8e\u72b6\u6001\u7684\u51fd\u6570\u8c03\u7528(SFC)\u4f18\u4e8e\u4f20\u7edf\u51fd\u6570\u8c03\u7528(FC)\u3002", "motivation": "\u4f20\u7edf\u7684\u51fd\u6570\u8c03\u7528(FC)\u65b9\u6cd5\u662f\u65e0\u72b6\u6001\u7684\uff0c\u9700\u8981\u591a\u6b21\u63a2\u7d22\u6027\u8c03\u7528\u6765\u6784\u5efa\u73af\u5883\u610f\u8bc6\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u548c\u9519\u8bef\u6062\u590d\u80fd\u529b\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u72b6\u6001\u7684\u51fd\u6570\u8c03\u7528(SFC)\uff0c\u5b83\u7ef4\u62a4\u663e\u5f0f\u7684\u7cfb\u7edf\u72b6\u6001\u611f\u77e5\uff0c\u5e76\u5b9e\u73b0\u76f4\u63a5\u72b6\u6001\u8f6c\u6362\u4ee5\u8fbe\u5230\u76ee\u6807\u6761\u4ef6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSFC\u660e\u663e\u4f18\u4e8e\u4f20\u7edf\u7684FC\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6267\u884c\u7cbe\u5ea6\u548c\u66f4\u4f4e\u7684\u5ef6\u8fdf\u3002", "conclusion": "\u76f4\u63a5\u72b6\u6001\u9884\u6d4b\u4f18\u4e8e\u51fd\u6570\u8c03\u7528\u8fdb\u884c\u73af\u5883\u63a7\u5236\uff0c\u5e76\u4e14\u63d0\u51fa\u7684SFC\u65b9\u6cd5\u5728\u8f66\u8f86\u667a\u80fd\u5ea7\u8231\u73af\u5883\u4e2d\u8868\u73b0\u66f4\u597d\u3002"}}
{"id": "2509.05696", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05696", "abs": "https://arxiv.org/abs/2509.05696", "authors": ["Hongyu Zhou", "Yunzhou Zhang", "Tingsong Huang", "Fawei Ge", "Man Qi", "Xichen Zhang", "Yizhong Zhang"], "title": "JRN-Geo: A Joint Perception Network based on RGB and Normal images for Cross-view Geo-localization", "comment": null, "summary": "Cross-view geo-localization plays a critical role in Unmanned Aerial Vehicle\n(UAV) localization and navigation. However, significant challenges arise from\nthe drastic viewpoint differences and appearance variations between images.\nExisting methods predominantly rely on semantic features from RGB images, often\nneglecting the importance of spatial structural information in capturing\nviewpoint-invariant features. To address this issue, we incorporate geometric\nstructural information from normal images and introduce a Joint perception\nnetwork to integrate RGB and Normal images (JRN-Geo). Our approach utilizes a\ndual-branch feature extraction framework, leveraging a Difference-Aware Fusion\nModule (DAFM) and Joint-Constrained Interaction Aggregation (JCIA) strategy to\nenable deep fusion and joint-constrained semantic and structural information\nrepresentation. Furthermore, we propose a 3D geographic augmentation technique\nto generate potential viewpoint variation samples, enhancing the network's\nability to learn viewpoint-invariant features. Extensive experiments on the\nUniversity-1652 and SUES-200 datasets validate the robustness of our method\nagainst complex viewpoint ariations, achieving state-of-the-art performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408RGB\u56fe\u50cf\u548c\u6cd5\u7ebf\u56fe\u50cf\u7684\u8de8\u89c6\u89d2\u5730\u7406\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u65e0\u4eba\u673a\u5b9a\u4f4d\u548c\u5bfc\u822a\u4e2d\u7684\u89c6\u89d2\u5dee\u5f02\u548c\u5916\u89c2\u53d8\u5316\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56RGB\u56fe\u50cf\u7684\u8bed\u4e49\u7279\u5f81\uff0c\u5ffd\u7565\u4e86\u7a7a\u95f4\u7ed3\u6784\u4fe1\u606f\u5728\u6355\u83b7\u89c6\u89d2\u4e0d\u53d8\u7279\u5f81\u4e2d\u7684\u91cd\u8981\u6027\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u8054\u5408\u611f\u77e5\u7f51\u7edc\uff0c\u7ed3\u5408RGB\u548c\u6cd5\u7ebf\u56fe\u50cf\uff0c\u5e76\u5229\u7528\u5dee\u5f02\u611f\u77e5\u878d\u5408\u6a21\u5757\u548c\u8054\u5408\u7ea6\u675f\u4ea4\u4e92\u805a\u5408\u7b56\u7565\u6765\u5b9e\u73b0\u6df1\u5ea6\u878d\u5408\u548c\u8bed\u4e49\u4e0e\u7ed3\u6784\u4fe1\u606f\u7684\u8054\u5408\u7ea6\u675f\u8868\u793a\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd3D\u5730\u7406\u589e\u5f3a\u6280\u672f\u6765\u751f\u6210\u6f5c\u5728\u7684\u89c6\u89d2\u53d8\u5316\u6837\u672c\u3002", "result": "\u5728University-1652\u548cSUES-200\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u89c6\u89d2\u53d8\u5316\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u8de8\u89c6\u89d2\u5730\u7406\u5b9a\u4f4d\u4e2d\u7684\u89c6\u89d2\u5dee\u5f02\u548c\u5916\u89c2\u53d8\u5316\u95ee\u9898\uff0c\u5177\u6709\u8f83\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u5148\u8fdb\u6027\u3002"}}
{"id": "2509.06120", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.06120", "abs": "https://arxiv.org/abs/2509.06120", "authors": ["Ambuj Tewari"], "title": "If generative AI is the answer, what is the question?", "comment": "To appear as a book chapter in a Springer book titled \"Statistical\n  Foundations and Applications of Artificial Intelligence, Machine Learning and\n  Deep Learning\" and edited by S. Ejaz Ahmed, Pierre Alquier, Yi Li, Shuangge\n  Ma", "summary": "Beginning with text and images, generative AI has expanded to audio, video,\ncomputer code, and molecules. Yet, if generative AI is the answer, what is the\nquestion? We explore the foundations of generation as a distinct machine\nlearning task with connections to prediction, compression, and decision-making.\nWe survey five major generative model families: autoregressive models,\nvariational autoencoders, normalizing flows, generative adversarial networks,\nand diffusion models. We then introduce a probabilistic framework that\nemphasizes the distinction between density estimation and generation. We review\na game-theoretic framework with a two-player adversary-learner setup to study\ngeneration. We discuss post-training modifications that prepare generative\nmodels for deployment. We end by highlighting some important topics in socially\nresponsible generation such as privacy, detection of AI-generated content, and\ncopyright and IP. We adopt a task-first framing of generation, focusing on what\ngeneration is as a machine learning problem, rather than only on how models\nimplement it.", "AI": {"tldr": "\u63a2\u8ba8\u751f\u6210\u5f0fAI\u7684\u57fa\u7840\uff0c\u5e76\u4ece\u673a\u5668\u5b66\u4e60\u7684\u89d2\u5ea6\u5206\u6790\u5176\u4e0e\u9884\u6d4b\u3001\u538b\u7f29\u548c\u51b3\u7b56\u7684\u5173\u7cfb\u3002", "motivation": "\u63a2\u8ba8\u751f\u6210\u5f0fAI\u4f5c\u4e3a\u4e00\u79cd\u72ec\u7279\u7684\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u7684\u6839\u672c\u539f\u7406\u3002", "method": "\u8c03\u7814\u4e86\u4e94\u79cd\u4e3b\u8981\u7684\u751f\u6210\u6a21\u578b\u5bb6\u65cf\uff1a\u81ea\u56de\u5f52\u6a21\u578b\u3001\u53d8\u5206\u81ea\u7f16\u7801\u5668\u3001\u6807\u51c6\u5316\u6d41\u3001\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u548c\u6269\u6563\u6a21\u578b\u3002\u5f15\u5165\u4e86\u4e00\u4e2a\u6982\u7387\u6846\u67b6\uff0c\u5f3a\u8c03\u5bc6\u5ea6\u4f30\u8ba1\u548c\u751f\u6210\u4e4b\u95f4\u7684\u533a\u522b\u3002\u56de\u987e\u4e86\u4e00\u4e2a\u535a\u5f08\u8bba\u6846\u67b6\uff0c\u7528\u53cc\u4eba\u5bf9\u6297\u5b66\u4e60\u8bbe\u7f6e\u6765\u7814\u7a76\u751f\u6210\u3002", "result": "\u8ba8\u8bba\u4e86\u4e3a\u90e8\u7f72\u51c6\u5907\u751f\u6210\u6a21\u578b\u7684\u540e\u8bad\u7ec3\u4fee\u6539\u3002\u91cd\u70b9\u4ecb\u7ecd\u4e86\u793e\u4f1a\u8d23\u4efb\u751f\u6210\u4e2d\u7684\u4e00\u4e9b\u91cd\u8981\u4e3b\u9898\uff0c\u5982\u9690\u79c1\u3001AI\u751f\u6210\u5185\u5bb9\u7684\u68c0\u6d4b\u4ee5\u53ca\u7248\u6743\u548cIP\u3002", "conclusion": "\u603b\u7ed3\u4e86\u751f\u6210\u5f0fAI\u4f5c\u4e3a\u4e00\u4e2a\u673a\u5668\u5b66\u4e60\u95ee\u9898\u7684\u4efb\u52a1\u4f18\u5148\u6846\u67b6\uff0c\u4fa7\u91cd\u4e8e\u751f\u6210\u662f\u4ec0\u4e48\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u6a21\u578b\u5982\u4f55\u5b9e\u73b0\u5b83\u3002"}}
{"id": "2509.06501", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06501", "abs": "https://arxiv.org/abs/2509.06501", "authors": ["Junteng Liu", "Yunji Li", "Chi Zhang", "Jingyang Li", "Aili Chen", "Ke Ji", "Weiyu Cheng", "Zijia Wu", "Chengyu Du", "Qidi Xu", "Jiayuan Song", "Zhengmao Zhu", "Wenhu Chen", "Pengyu Zhao", "Junxian He"], "title": "WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents", "comment": null, "summary": "The paradigm of Large Language Models (LLMs) has increasingly shifted toward\nagentic applications, where web browsing capabilities are fundamental for\nretrieving information from diverse online sources. However, existing\nopen-source web agents either demonstrate limited information-seeking abilities\non complex tasks or lack transparent implementations. In this work, we identify\nthat the key challenge lies in the scarcity of challenging data for information\nseeking. To address this limitation, we introduce WebExplorer: a systematic\ndata generation approach using model-based exploration and iterative,\nlong-to-short query evolution. This method creates challenging query-answer\npairs that require multi-step reasoning and complex web navigation. By\nleveraging our curated high-quality dataset, we successfully develop advanced\nweb agent WebExplorer-8B through supervised fine-tuning followed by\nreinforcement learning. Our model supports 128K context length and up to 100\ntool calling turns, enabling long-horizon problem solving. Across diverse\ninformation-seeking benchmarks, WebExplorer-8B achieves the state-of-the-art\nperformance at its scale. Notably, as an 8B-sized model, WebExplorer-8B is able\nto effectively search over an average of 16 turns after RL training, achieving\nhigher accuracy than WebSailor-72B on BrowseComp-en/zh and attaining the best\nperformance among models up to 100B parameters on WebWalkerQA and FRAMES.\nBeyond these information-seeking tasks, our model also achieves strong\ngeneralization on the HLE benchmark even though it is only trained on\nknowledge-intensive QA data. These results highlight our approach as a\npractical path toward long-horizon web agents.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a WebExplorer \u7684\u65b0\u578b\u7f51\u7edc\u4ee3\u7406\uff0c\u5b83\u901a\u8fc7\u6a21\u578b\u63a2\u7d22\u548c\u8fed\u4ee3\u7684\u957f\u77ed\u67e5\u8be2\u6f14\u5316\u6765\u751f\u6210\u5177\u6709\u6311\u6218\u6027\u7684\u4fe1\u606f\u5bfb\u6c42\u6570\u636e\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u73b0\u6709\u5f00\u6e90\u7f51\u7edc\u4ee3\u7406\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u4fe1\u606f\u5bfb\u6c42\u80fd\u529b\u6709\u9650\u6216\u7f3a\u4e4f\u900f\u660e\u5b9e\u73b0\u7684\u95ee\u9898\u3002WebExplorer-8B \u5728\u5404\u79cd\u4fe1\u606f\u5bfb\u6c42\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5f00\u6e90\u7f51\u7edc\u4ee3\u7406\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u4fe1\u606f\u5bfb\u6c42\u80fd\u529b\u6709\u9650\uff0c\u5e76\u4e14\u7f3a\u4e4f\u900f\u660e\u7684\u5b9e\u73b0\uff0c\u5173\u952e\u6311\u6218\u5728\u4e8e\u7f3a\u4e4f\u5177\u6709\u6311\u6218\u6027\u7684\u4fe1\u606f\u5bfb\u6c42\u6570\u636e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u7684\u6570\u636e\u751f\u6210\u65b9\u6cd5 WebExplorer\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528\u57fa\u4e8e\u6a21\u578b\u7684\u63a2\u7d22\u548c\u8fed\u4ee3\u7684\u957f\u77ed\u67e5\u8be2\u6f14\u5316\u6765\u521b\u5efa\u5177\u6709\u6311\u6218\u6027\u7684\u67e5\u8be2-\u7b54\u6848\u5bf9\uff0c\u8fd9\u4e9b\u67e5\u8be2-\u7b54\u6848\u5bf9\u9700\u8981\u591a\u6b65\u9aa4\u63a8\u7406\u548c\u590d\u6742\u7684\u7f51\u7edc\u5bfc\u822a\u3002\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u5f00\u53d1\u4e86\u5148\u8fdb\u7684\u7f51\u7edc\u4ee3\u7406 WebExplorer-8B\u3002", "result": "WebExplorer-8B \u5728\u5176\u89c4\u6a21\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u652f\u6301 128K \u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u548c\u6700\u591a 100 \u4e2a\u5de5\u5177\u8c03\u7528\u8f6e\u6b21\uff0c\u4ece\u800c\u80fd\u591f\u89e3\u51b3\u957f\u65f6\u7a0b\u95ee\u9898\u3002\u4f5c\u4e3a\u4e00\u4e2a 8B \u5927\u5c0f\u7684\u6a21\u578b\uff0cWebExplorer-8B \u5728 RL \u8bad\u7ec3\u540e\u80fd\u591f\u6709\u6548\u5730\u641c\u7d22\u5e73\u5747 16 \u8f6e\uff0c\u5728 BrowseComp-en/zh \u4e0a\u5b9e\u73b0\u4e86\u6bd4 WebSailor-72B \u66f4\u9ad8\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5728 WebWalkerQA \u548c FRAMES \u4e0a\u83b7\u5f97\u4e86\u9ad8\u8fbe 100B \u53c2\u6570\u7684\u6a21\u578b\u4e2d\u7684\u6700\u4f73\u6027\u80fd\u3002\u8be5\u6a21\u578b\u5728 HLE \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u662f\u901a\u5f80\u957f\u65f6\u7a0b\u7f51\u7edc\u4ee3\u7406\u7684\u5b9e\u7528\u9014\u5f84\u3002"}}
{"id": "2509.06770", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.06770", "abs": "https://arxiv.org/abs/2509.06770", "authors": ["Shashidhar Reddy Javaji", "Bhavul Gauri", "Zining Zhu"], "title": "Another Turn, Better Output? A Turn-Wise Analysis of Iterative LLM Prompting", "comment": null, "summary": "Large language models (LLMs) are now used in multi-turn workflows, but we\nstill lack a clear way to measure when iteration helps and when it hurts. We\npresent an evaluation framework for iterative refinement that spans ideation,\ncode, and math. Our protocol runs controlled 12-turn conversations per task,\nutilizing a variety of prompts ranging from vague ``improve it'' feedback to\ntargeted steering, and logs per-turn outputs. We score outcomes with\ndomain-appropriate checks (unit tests for code; answer-equivalence plus\nreasoning-soundness for math; originality and feasibility for ideation) and\ntrack turn-level behavior with three families of metrics: semantic movement\nacross turns, turn-to-turn change, and output size growth. Across models and\ntasks, gains are domain-dependent: they arrive early in ideas and code, but in\nmath late turns matter when guided by elaboration. After the first few turns,\nvague feedback often plateaus or reverses correctness, while targeted prompts\nreliably shift the intended quality axis (novelty vs. feasibility in ideation;\nspeed vs. readability in code; in math, elaboration outperforms exploration and\ndrives late-turn gains). We also observe consistent domain patterns: ideation\nmoves more in meaning across turns, code tends to grow in size with little\nsemantic change, and math starts fixed but can break that path with late,\nelaborative iteration.Together, the framework and metrics make iteration\nmeasurable and comparable across models, and signal when to steer, stop, or\nswitch strategies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u8fed\u4ee3\u6539\u8fdb\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8861\u91cf\u8fed\u4ee3\u5728\u4e0d\u540c\u4efb\u52a1\uff08\u6784\u601d\u3001\u4ee3\u7801\u548c\u6570\u5b66\uff09\u4e2d\u7684\u5e2e\u52a9\u548c\u963b\u788d\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u660e\u786e\u7684\u65b9\u6cd5\u6765\u8861\u91cf\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u8fed\u4ee3\u7684\u5e2e\u52a9\u548c\u963b\u788d\u3002", "method": "\u901a\u8fc7\u63a7\u523612\u8f6e\u5bf9\u8bdd\uff0c\u5229\u7528\u5404\u79cd\u63d0\u793a\uff08\u4ece\u6a21\u7cca\u7684\u201c\u6539\u8fdb\u5b83\u201d\u53cd\u9988\u5230\u6709\u9488\u5bf9\u6027\u7684\u5f15\u5bfc\uff09\uff0c\u5e76\u8bb0\u5f55\u6bcf\u8f6e\u8f93\u51fa\u3002\u4f7f\u7528\u9886\u57df\u9002\u5f53\u7684\u68c0\u67e5\uff08\u4ee3\u7801\u7684\u5355\u5143\u6d4b\u8bd5\uff1b\u6570\u5b66\u7684\u7b54\u6848\u7b49\u6548\u6027\u52a0\u63a8\u7406\u5408\u7406\u6027\uff1b\u6784\u601d\u7684\u539f\u521b\u6027\u548c\u53ef\u884c\u6027\uff09\u5bf9\u7ed3\u679c\u8fdb\u884c\u8bc4\u5206\uff0c\u5e76\u4f7f\u7528\u4e09\u7c7b\u6307\u6807\u8ddf\u8e2a\u8f6e\u6b21\u884c\u4e3a\uff1a\u8de8\u8f6e\u6b21\u7684\u8bed\u4e49\u79fb\u52a8\u3001\u8f6e\u6b21\u95f4\u53d8\u5316\u548c\u8f93\u51fa\u5927\u5c0f\u589e\u957f\u3002", "result": "\u6536\u76ca\u4e0e\u9886\u57df\u76f8\u5173\uff1a\u5b83\u4eec\u5728\u60f3\u6cd5\u548c\u4ee3\u7801\u4e2d\u51fa\u73b0\u8f83\u65e9\uff0c\u4f46\u5728\u6570\u5b66\u4e2d\uff0c\u5f53\u4ee5\u7ec6\u5316\u4e3a\u6307\u5bfc\u65f6\uff0c\u540e\u671f\u8f6e\u6b21\u5f88\u91cd\u8981\u3002\u6a21\u7cca\u7684\u53cd\u9988\u901a\u5e38\u4f1a\u505c\u6ede\u6216\u9006\u8f6c\u6b63\u786e\u6027\uff0c\u800c\u6709\u9488\u5bf9\u6027\u7684\u63d0\u793a\u53ef\u4ee5\u53ef\u9760\u5730\u6539\u53d8\u9884\u671f\u7684\u8d28\u91cf\u8f74\uff08\u6784\u601d\u4e2d\u7684\u65b0\u9896\u6027\u4e0e\u53ef\u884c\u6027\uff1b\u4ee3\u7801\u4e2d\u7684\u901f\u5ea6\u4e0e\u53ef\u8bfb\u6027\uff1b\u5728\u6570\u5b66\u4e2d\uff0c\u7ec6\u5316\u4f18\u4e8e\u63a2\u7d22\u5e76\u63a8\u52a8\u540e\u671f\u8f6e\u6b21\u6536\u76ca\uff09\u3002", "conclusion": "\u8be5\u6846\u67b6\u548c\u6307\u6807\u4f7f\u8fed\u4ee3\u53ef\u8861\u91cf\u4e14\u53ef\u5728\u6a21\u578b\u4e4b\u95f4\u8fdb\u884c\u6bd4\u8f83\uff0c\u5e76\u8868\u660e\u4f55\u65f6\u5f15\u5bfc\u3001\u505c\u6b62\u6216\u5207\u6362\u7b56\u7565\u3002"}}
{"id": "2509.06154", "categories": ["cs.LG", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.06154", "abs": "https://arxiv.org/abs/2509.06154", "authors": ["Dibyajyoti Nayak", "Somdatta Goswami"], "title": "Data-Efficient Time-Dependent PDE Surrogates: Graph Neural Simulators vs Neural Operators", "comment": "21 pages including references. Supplementary Information provided", "summary": "Neural operators (NOs) approximate mappings between infinite-dimensional\nfunction spaces but require large datasets and struggle with scarce training\ndata. Many NO formulations don't explicitly encode causal, local-in-time\nstructure of physical evolution. While autoregressive models preserve causality\nby predicting next time-steps, they suffer from rapid error accumulation. We\nemploy Graph Neural Simulators (GNS) - a message-passing graph neural network\nframework - with explicit numerical time-stepping schemes to construct accurate\nforward models that learn PDE solutions by modeling instantaneous time\nderivatives. We evaluate our framework on three canonical PDE systems: (1) 2D\nBurgers' scalar equation, (2) 2D coupled Burgers' vector equation, and (3) 2D\nAllen-Cahn equation. Rigorous evaluations demonstrate GNS significantly\nimproves data efficiency, achieving higher generalization accuracy with\nsubstantially fewer training trajectories compared to neural operator baselines\nlike DeepONet and FNO. GNS consistently achieves under 1% relative L2 errors\nwith only 30 training samples out of 1000 (3% of available data) across all\nthree PDE systems. It substantially reduces error accumulation over extended\ntemporal horizons: averaged across all cases, GNS reduces autoregressive error\nby 82.48% relative to FNO AR and 99.86% relative to DON AR. We introduce a\nPCA+KMeans trajectory selection strategy enhancing low-data performance.\nResults indicate combining graph-based local inductive biases with conventional\ntime integrators yields accurate, physically consistent, and scalable surrogate\nmodels for time-dependent PDEs.", "AI": {"tldr": "\u56fe\u795e\u7ecf\u7f51\u7edc\u6a21\u62df\u5668(GNS)\u901a\u8fc7\u5bf9\u77ac\u65f6\u65f6\u95f4\u5bfc\u6570\u5efa\u6a21\uff0c\u5e76\u7ed3\u5408\u663e\u5f0f\u6570\u503c\u65f6\u95f4\u6b65\u8fdb\u65b9\u6848\uff0c\u6784\u5efa\u7cbe\u786e\u7684\u524d\u5411\u6a21\u578b\u6765\u5b66\u4e60\u504f\u5fae\u5206\u65b9\u7a0b(PDE)\u7684\u89e3\uff0c\u63d0\u9ad8\u6570\u636e\u6548\u7387\uff0c\u51cf\u5c11\u8bef\u5dee\u7d2f\u79ef\u3002", "motivation": "\u795e\u7ecf\u7b97\u5b50(NOs)\u5728\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u4e14\u8bb8\u591aNO\u516c\u5f0f\u6ca1\u6709\u660e\u786e\u5730\u7f16\u7801\u7269\u7406\u6f14\u5316\u7684\u56e0\u679c\u3001\u5c40\u90e8\u65f6\u95f4\u7ed3\u6784\u3002\u81ea\u56de\u5f52\u6a21\u578b\u867d\u7136\u4fdd\u6301\u4e86\u56e0\u679c\u5173\u7cfb\uff0c\u4f46\u5b58\u5728\u5feb\u901f\u8bef\u5dee\u7d2f\u79ef\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u6a21\u62df\u5668(GNS)\uff0c\u8fd9\u662f\u4e00\u4e2a\u6d88\u606f\u4f20\u9012\u56fe\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u7ed3\u5408\u663e\u5f0f\u6570\u503c\u65f6\u95f4\u6b65\u8fdb\u65b9\u6848\u3002", "result": "GNS\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u6548\u7387\uff0c\u4e0eDeepONet\u548cFNO\u7b49\u795e\u7ecf\u7b97\u5b50\u57fa\u7ebf\u76f8\u6bd4\uff0c\u4ec5\u7528\u5c11\u91cf\u8bad\u7ec3\u8f68\u8ff9\u5c31\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6cdb\u5316\u7cbe\u5ea6\u3002\u5728\u6240\u6709\u4e09\u4e2aPDE\u7cfb\u7edf\u4e2d\uff0c\u4ec5\u4f7f\u75281000\u4e2a\u6837\u672c\u4e2d\u768430\u4e2a\u8bad\u7ec3\u6837\u672c(3%\u7684\u53ef\u7528\u6570\u636e)\uff0cGNS\u59cb\u7ec8\u5b9e\u73b0\u4e86\u4f4e\u4e8e1%\u7684\u76f8\u5bf9L2\u8bef\u5dee\u3002\u5b83\u5927\u5927\u51cf\u5c11\u4e86\u5728\u6269\u5c55\u65f6\u95f4\u8303\u56f4\u5185\u7684\u8bef\u5dee\u7d2f\u79ef\uff1a\u5728\u6240\u6709\u60c5\u51b5\u4e0b\uff0cGNS\u76f8\u5bf9\u4e8eFNO AR\u51cf\u5c11\u4e8682.48%\u7684\u81ea\u56de\u5f52\u8bef\u5dee\uff0c\u76f8\u5bf9\u4e8eDON AR\u51cf\u5c11\u4e8699.86%\u3002", "conclusion": "\u5c06\u57fa\u4e8e\u56fe\u7684\u5c40\u90e8\u5f52\u7eb3\u504f\u7f6e\u4e0e\u4f20\u7edf\u65f6\u95f4\u79ef\u5206\u5668\u76f8\u7ed3\u5408\uff0c\u53ef\u4ee5\u4e3a\u65f6\u95f4\u76f8\u5173\u7684\u504f\u5fae\u5206\u65b9\u7a0b\u4ea7\u751f\u7cbe\u786e\u3001\u7269\u7406\u4e00\u81f4\u4e14\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u6a21\u578b\u3002"}}
{"id": "2509.06518", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06518", "abs": "https://arxiv.org/abs/2509.06518", "authors": ["Andrei Baroian", "Kasper Notebomer"], "title": "Crown, Frame, Reverse: Layer-Wise Scaling Variants for LLM Pre-Training", "comment": "The reported results are skewed due to a data type mismatch. The\n  dataset was saved with int32, but the data loader interpreted it as uint16.\n  As a result, each 32-bit token was incorrectly split into two 16-bit tokens.\n  Outcome: a consistent artifact where every other token is zero", "summary": "Transformer-based language models traditionally use uniform (isotropic) layer\nsizes, yet they ignore the diverse functional roles that different depths can\nplay and their computational capacity needs. Building on Layer-Wise Scaling\n(LWS) and pruning literature, we introduce three new LWS variants - Framed,\nReverse, and Crown - that redistribute FFN widths and attention heads via two\nor three-point linear interpolation in the pre-training stage. We present the\nfirst systematic ablation of LWS and its variants, on a fixed budget of 180M\nparameters, trained on 5B tokens. All models converge to similar losses and\nachieve better performance compared to an equal-cost isotropic baseline,\nwithout a substantial decrease in training throughput. This work represents an\ninitial step into the design space of layer-wise architectures for\npre-training, but future work should scale experiments to orders of magnitude\nmore tokens and parameters to fully assess their potential.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86Transformer\u8bed\u8a00\u6a21\u578b\u4e2d\u4e0d\u540c\u6df1\u5ea6\u5c42\u7684\u529f\u80fd\u89d2\u8272\u548c\u8ba1\u7b97\u80fd\u529b\u9700\u6c42\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u79cd\u65b0\u7684Layer-Wise Scaling (LWS) \u53d8\u4f53\u3002", "motivation": "\u4f20\u7edfTransformer\u6a21\u578b\u5ffd\u7565\u4e86\u4e0d\u540c\u6df1\u5ea6\u5c42\u7684\u529f\u80fd\u5dee\u5f02\u548c\u8ba1\u7b97\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u4f7f\u7528\u4e24\u70b9\u6216\u4e09\u70b9\u7ebf\u6027\u63d2\u503c\u91cd\u65b0\u5206\u914dFFN\u5bbd\u5ea6\u548c\u6ce8\u610f\u529b\u5934\uff0c\u5f15\u5165\u4e86Framed\u3001Reverse\u548cCrown\u4e09\u79cdLWS\u53d8\u4f53\uff0c\u5e76\u5bf9LWS\u53ca\u5176\u53d8\u4f53\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u7684\u6d88\u878d\u7814\u7a76\u3002", "result": "\u6240\u6709\u6a21\u578b\u5728\u76f8\u4f3c\u7684\u635f\u5931\u4e0b\u6536\u655b\uff0c\u5e76\u4e14\u4e0e\u540c\u7b49\u6210\u672c\u7684\u5404\u5411\u540c\u6027\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\uff0c\u800c\u8bad\u7ec3\u541e\u5410\u91cf\u6ca1\u6709\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4ee3\u8868\u4e86\u9884\u8bad\u7ec3\u7684\u5206\u5c42\u67b6\u6784\u8bbe\u8ba1\u7a7a\u95f4\u7684\u521d\u6b65\u63a2\u7d22\uff0c\u4f46\u672a\u6765\u7684\u5de5\u4f5c\u5e94\u8be5\u5c06\u5b9e\u9a8c\u6269\u5c55\u5230\u66f4\u591a\u6570\u91cf\u7ea7\u7684tokens\u548c\u53c2\u6570\uff0c\u4ee5\u5145\u5206\u8bc4\u4f30\u5b83\u4eec\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.06822", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06822", "abs": "https://arxiv.org/abs/2509.06822", "authors": ["Chenyang Zhu", "Spencer Hong", "Jingyu Wu", "Kushal Chawla", "Charlotte Tang", "Youbing Yin", "Nathan Wolfe", "Erin Babinsky", "Daben Liu"], "title": "RAFFLES: Reasoning-based Attribution of Faults for LLM Systems", "comment": null, "summary": "We have reached a critical roadblock in the development and enhancement of\nlong-horizon, multi-component LLM agentic systems: it is incredibly tricky to\nidentify where these systems break down and why. Evaluation capabilities that\ncurrently exist today (e.g., single pass LLM-as-a-judge) are limited in that\nthey often focus on individual metrics or capabilities, end-to-end outcomes,\nand are narrowly grounded on the preferences of humans. We argue that to match\nthe agentic capabilities, evaluation frameworks must also be able to reason,\nprobe, iterate, and understand the complex logic passing through these systems\nover long horizons. In this paper, we present RAFFLES - an evaluation\narchitecture that incorporates reasoning and iterative refinement.\nSpecifically, RAFFLES operates as an iterative, multi-component pipeline, using\na central Judge to systematically investigate faults and a set of specialized\nEvaluators to assess not only the system's components but also the quality of\nthe reasoning by the Judge itself, thereby building a history of hypotheses. We\ntested RAFFLES against several baselines on the Who&When dataset, a benchmark\ndesigned to diagnose the \"who\" (agent) and \"when\" (step) of a system's failure.\nRAFFLES outperforms these baselines, achieving an agent-step fault pair\naccuracy of over 43% on the Algorithmically-Generated dataset (a substantial\nincrease from the previously published best of 16.6%) and over 20% on the\nHand-Crafted dataset (surpassing the previously published best of 8.8%). These\nresults demonstrate a key step towards introducing automated fault detection\nfor autonomous systems over labor-intensive manual human review.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aRAFFLES\u7684\u8bc4\u4f30\u67b6\u6784\uff0c\u7528\u4e8e\u8bc6\u522b\u957f\u7a0b\u3001\u591a\u7ec4\u4ef6LLM\u4ee3\u7406\u7cfb\u7edf\u7684\u6545\u969c\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u8bc6\u522b\u957f\u7a0b\u3001\u591a\u7ec4\u4ef6LLM\u4ee3\u7406\u7cfb\u7edf\u7684\u6545\u969c\u539f\u56e0\u3002", "method": "RAFFLES\u91c7\u7528\u8fed\u4ee3\u3001\u591a\u7ec4\u4ef6\u6d41\u6c34\u7ebf\uff0c\u4f7f\u7528\u4e2d\u5fc3Judge\u7cfb\u7edf\u5730\u8c03\u67e5\u6545\u969c\uff0c\u5e76\u4f7f\u7528\u4e00\u7ec4\u4e13\u95e8\u7684Evaluators\u8bc4\u4f30\u7cfb\u7edf\u7ec4\u4ef6\u548cJudge\u672c\u8eab\u7684\u63a8\u7406\u8d28\u91cf\u3002", "result": "\u5728Who&When\u6570\u636e\u96c6\u4e0a\uff0cRAFFLES\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5728\u7b97\u6cd5\u751f\u6210\u6570\u636e\u96c6\u4e0a\u7684agent-step\u6545\u969c\u5bf9\u51c6\u786e\u7387\u8d85\u8fc743%\uff0c\u5728\u624b\u5de5\u5236\u4f5c\u6570\u636e\u96c6\u4e0a\u7684\u51c6\u786e\u7387\u8d85\u8fc720%\u3002", "conclusion": "RAFFLES\u662f\u671d\u7740\u4e3a\u81ea\u4e3b\u7cfb\u7edf\u5f15\u5165\u81ea\u52a8\u5316\u6545\u969c\u68c0\u6d4b\u7684\u5173\u952e\u4e00\u6b65\uff0c\u4f18\u4e8e\u4eba\u5de5\u5ba1\u67e5\u3002"}}
{"id": "2509.05728", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.05728", "abs": "https://arxiv.org/abs/2509.05728", "authors": ["Niels Balemans", "Ali Anwar", "Jan Steckel", "Siegfried Mercelis"], "title": "LiDAR-BIND-T: Improving SLAM with Temporally Consistent Cross-Modal LiDAR Reconstruction", "comment": null, "summary": "This paper extends LiDAR-BIND, a modular multi-modal fusion framework that\nbinds heterogeneous sensors (radar, sonar) to a LiDAR-defined latent space,\nwith mechanisms that explicitly enforce temporal consistency. We introduce\nthree contributions: (i) temporal embedding similarity that aligns consecutive\nlatents, (ii) a motion-aligned transformation loss that matches displacement\nbetween predictions and ground truth LiDAR, and (iii) windows temporal fusion\nusing a specialised temporal module. We further update the model architecture\nto better preserve spatial structure. Evaluations on radar/sonar-to-LiDAR\ntranslation demonstrate improved temporal and spatial coherence, yielding lower\nabsolute trajectory error and better occupancy map accuracy in\nCartographer-based SLAM (Simultaneous Localisation and Mapping). We propose\ndifferent metrics based on the Fr\\'echet Video Motion Distance (FVMD) and a\ncorrelation-peak distance metric providing practical temporal quality\nindicators to evaluate SLAM performance. The proposed temporal LiDAR-BIND, or\nLiDAR-BIND-T, maintains plug-and-play modality fusion while substantially\nenhancing temporal stability, resulting in improved robustness and performance\nfor downstream SLAM.", "AI": {"tldr": "LiDAR-BIND-T\u901a\u8fc7\u589e\u5f3a\u65f6\u95f4\u4e00\u81f4\u6027\u6765\u6539\u8fdbLiDAR-BIND\u6846\u67b6\uff0c\u4ece\u800c\u63d0\u9ad8SLAM\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u5f02\u6784\u4f20\u611f\u5668\u878d\u5408\u6846\u67b6\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "method": "\u5f15\u5165\u4e86\u65f6\u95f4\u5d4c\u5165\u76f8\u4f3c\u6027\u3001\u8fd0\u52a8\u5bf9\u9f50\u53d8\u6362\u635f\u5931\u548c\u7a97\u53e3\u65f6\u95f4\u878d\u5408\u7b49\u673a\u5236\uff0c\u5e76\u66f4\u65b0\u4e86\u6a21\u578b\u67b6\u6784\u4ee5\u66f4\u597d\u5730\u4fdd\u6301\u7a7a\u95f4\u7ed3\u6784\u3002", "result": "\u5728\u96f7\u8fbe/\u58f0\u7eb3\u5230\u6fc0\u5149\u96f7\u8fbe\u7684\u8f6c\u6362\u8bc4\u4f30\u4e2d\uff0c\u65f6\u95f4\u7a7a\u95f4\u4e00\u81f4\u6027\u5f97\u5230\u4e86\u6539\u5584\uff0c\u7edd\u5bf9\u8f68\u8ff9\u8bef\u5dee\u964d\u4f4e\uff0c\u57fa\u4e8eCartographer\u7684SLAM\u7684\u5360\u7528\u5730\u56fe\u7cbe\u5ea6\u66f4\u9ad8\u3002", "conclusion": "\u63d0\u51fa\u7684LiDAR-BIND-T\u5728\u4fdd\u6301\u5373\u63d2\u5373\u7528\u6a21\u6001\u878d\u5408\u7684\u540c\u65f6\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u65f6\u95f4\u7a33\u5b9a\u6027\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u4e0b\u6e38SLAM\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002"}}
