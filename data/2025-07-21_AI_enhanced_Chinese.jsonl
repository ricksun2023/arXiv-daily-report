{"id": "2507.13354", "categories": ["cs.LG", "cs.AI", "cs.CL", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2507.13354", "abs": "https://arxiv.org/abs/2507.13354", "authors": ["Zeqian Chen"], "title": "Physical models realizing the transformer architecture of large language models", "comment": "6 pages", "summary": "The introduction of the transformer architecture in 2017 (cf.\\cite{VSP2017})\nmarked the most striking advancement in natural language processing. The\ntransformer is a model architecture relying entirely on an attention mechanism\nto draw global dependencies between input and output. However, we believe there\nis a gap in our theoretical understanding of what the transformer is, and why\nit works physically. In this paper, from a physical perspective on modern\nchips, we construct physical models in the Fock space over the Hilbert space of\ntokens realizing large language models based on a transformer architecture as\nopen quantum systems. Our physical models underlie the transformer architecture\nfor large language models.", "AI": {"tldr": "This paper constructs physical models for large language models based on the transformer architecture from a physical perspective.", "motivation": "There is a gap in our theoretical understanding of what the transformer is, and why it works physically.", "method": "We construct physical models in the Fock space over the Hilbert space of tokens realizing large language models based on a transformer architecture as open quantum systems.", "result": "Physical models in the Fock space over the Hilbert space of tokens realizing large language models based on a transformer architecture as open quantum systems.", "conclusion": "Physical models underlie the transformer architecture for large language models."}}
{"id": "2507.13383", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13383", "abs": "https://arxiv.org/abs/2507.13383", "authors": ["Charvi Rastogi", "Tian Huey Teh", "Pushkar Mishra", "Roma Patel", "Ding Wang", "Mark D\u00edaz", "Alicia Parrish", "Aida Mostafazadeh Davani", "Zoe Ashwood", "Michela Paganini", "Vinodkumar Prabhakaran", "Verena Rieser", "Lora Aroyo"], "title": "Whose View of Safety? A Deep DIVE Dataset for Pluralistic Alignment of Text-to-Image Models", "comment": "28 pages, 16 figures", "summary": "Current text-to-image (T2I) models often fail to account for diverse human\nexperiences, leading to misaligned systems. We advocate for pluralistic\nalignment, where an AI understands and is steerable towards diverse, and often\nconflicting, human values. Our work provides three core contributions to\nachieve this in T2I models. First, we introduce a novel dataset for Diverse\nIntersectional Visual Evaluation (DIVE) -- the first multimodal dataset for\npluralistic alignment. It enable deep alignment to diverse safety perspectives\nthrough a large pool of demographically intersectional human raters who\nprovided extensive feedback across 1000 prompts, with high replication,\ncapturing nuanced safety perceptions. Second, we empirically confirm\ndemographics as a crucial proxy for diverse viewpoints in this domain,\nrevealing significant, context-dependent differences in harm perception that\ndiverge from conventional evaluations. Finally, we discuss implications for\nbuilding aligned T2I models, including efficient data collection strategies,\nLLM judgment capabilities, and model steerability towards diverse perspectives.\nThis research offers foundational tools for more equitable and aligned T2I\nsystems. Content Warning: The paper includes sensitive content that may be\nharmful.", "AI": {"tldr": "This paper introduces a new dataset (DIVE) to improve the alignment of text-to-image models with diverse human values, finding that demographics are a crucial factor in harm perception. The paper also discusses implications for building aligned T2I models, including efficient data collection strategies, LLM judgment capabilities, and model steerability towards diverse perspectives.", "motivation": "Current text-to-image (T2I) models often fail to account for diverse human experiences, leading to misaligned systems. We advocate for pluralistic alignment, where an AI understands and is steerable towards diverse, and often conflicting, human values.", "method": "introducing a novel dataset for Diverse Intersectional Visual Evaluation (DIVE)", "result": "empirically confirm demographics as a crucial proxy for diverse viewpoints in this domain, revealing significant, context-dependent differences in harm perception that diverge from conventional evaluations.", "conclusion": "This research offers foundational tools for more equitable and aligned T2I systems."}}
{"id": "2507.13393", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13393", "abs": "https://arxiv.org/abs/2507.13393", "authors": ["Jakub Strawa", "Jarek Duda"], "title": "Improving KAN with CDF normalization to quantiles", "comment": "7 pages, 9 figures", "summary": "Data normalization is crucial in machine learning, usually performed by\nsubtracting the mean and dividing by standard deviation, or by rescaling to a\nfixed range. In copula theory, popular in finance, there is used normalization\nto approximately quantiles by transforming x to CDF(x) with estimated CDF\n(cumulative distribution function) to nearly uniform distribution in [0,1],\nallowing for simpler representations which are less likely to overfit. It seems\nnearly unknown in machine learning, therefore, we would like to present some\nits advantages on example of recently popular Kolmogorov-Arnold Networks\n(KANs), improving predictions from Legendre-KAN by just switching rescaling to\nCDF normalization. Additionally, in HCR interpretation, weights of such neurons\nare mixed moments providing local joint distribution models, allow to propagate\nalso probability distributions, and change propagation direction.", "AI": {"tldr": "CDF normalization, common in copula theory, improves KAN performance by simplifying representations and enabling probability distribution propagation.", "motivation": "Data normalization is crucial in machine learning. Copula theory uses CDF normalization, which is nearly unknown in machine learning, but it has advantages.", "method": "Using CDF normalization to transform data to approximately quantiles, resulting in a nearly uniform distribution [0,1].", "result": "Switching rescaling to CDF normalization improves predictions from Legendre-KAN. Weights of neurons are mixed moments providing local joint distribution models, allow to propagate also probability distributions, and change propagation direction.", "conclusion": "CDF normalization improves predictions in Kolmogorov-Arnold Networks (KANs) by providing simpler representations and reducing overfitting."}}
{"id": "2507.13710", "categories": ["cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13710", "abs": "https://arxiv.org/abs/2507.13710", "authors": ["Jing Chang", "Chang Liu", "Jinbin Huang", "Rui Mao", "Jianbin Qin"], "title": "CogniQ-H: A Soft Hierarchical Reinforcement Learning Paradigm for Automated Data Preparation", "comment": null, "summary": "Data preparation is a foundational yet notoriously challenging component of\nthe machine learning lifecycle, characterized by a vast combinatorial search\nspace of potential operator sequences. While reinforcement learning (RL) offers\na promising direction, existing approaches are inefficient as they fail to\ncapture the structured, hierarchical nature of the problem. We argue that\nHierarchical Reinforcement Learning (HRL), a paradigm that has been successful\nin other domains, provides a conceptually ideal yet previously unexplored\nframework for this task. However, a naive HRL implementation with a `hard\nhierarchy' is prone to suboptimal, irreversible decisions. To address this, we\nintroduce CogniQ-H, the first framework to implement a soft hierarchical\nparadigm for robust, end-to-end automated data preparation. CogniQ-H formulates\naction selection as a Bayesian inference problem. A high-level strategic prior,\ngenerated by a Large Language Model (LLM), guides exploration\nprobabilistically. This prior is synergistically combined with a fine-grained\noperator quality score from a supervised Learning-to-Rank (LTR) model and a\nlong-term value estimate from the agent's own Q-function. This hybrid\narchitecture allows CogniQ-H to balance strategic guidance with adaptive,\nevidence-based decision-making. Through extensive experiments on 18 diverse\ndatasets spanning multiple domains, we demonstrate that CogniQ-H achieves up to\n13.9\\% improvement in pipeline quality and 2.8$\\times$ faster convergence\ncompared to state-of-the-art RL-based methods.", "AI": {"tldr": "CogniQ-H\uff1a\u9996\u4e2a\u7528\u4e8e\u7a33\u5065\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u6570\u636e\u51c6\u5907\u7684\u8f6f\u5206\u5c42\u8303\u4f8b\u3002", "motivation": "\u6570\u636e\u51c6\u5907\u662f\u673a\u5668\u5b66\u4e60\u751f\u547d\u5468\u671f\u4e2d\u4e00\u4e2a\u57fa\u7840\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u7ec4\u6210\u90e8\u5206\uff0c\u5176\u7279\u70b9\u662f\u6f5c\u5728\u7b97\u5b50\u5e8f\u5217\u7684\u5de8\u5927\u7ec4\u5408\u641c\u7d22\u7a7a\u95f4\u3002\u5f3a\u5316\u5b66\u4e60(RL)\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u5e0c\u671b\u7684\u65b9\u5411\uff0c\u4f46\u73b0\u6709\u7684\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\uff0c\u56e0\u4e3a\u5b83\u4eec\u672a\u80fd\u6355\u6349\u5230\u95ee\u9898\u7684\u7ed3\u6784\u5316\u3001\u5206\u5c42\u6027\u8d28\u3002", "method": "CogniQ-H\u5c06\u52a8\u4f5c\u9009\u62e9\u6784\u5efa\u4e3a\u4e00\u4e2a\u8d1d\u53f6\u65af\u63a8\u65ad\u95ee\u9898\u3002\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u751f\u6210\u7684\u9ad8\u7ea7\u6218\u7565\u5148\u9a8c\u6982\u7387\u6027\u5730\u6307\u5bfc\u63a2\u7d22\u3002\u8be5\u5148\u9a8c\u4e0e\u6765\u81ea\u76d1\u7763\u5b66\u4e60\u6392\u5e8f(LTR)\u6a21\u578b\u7684\u7ec6\u7c92\u5ea6\u7b97\u5b50\u8d28\u91cf\u8bc4\u5206\u4ee5\u53ca\u6765\u81eaagent\u81ea\u8eabQ\u51fd\u6570\u7684\u957f\u671f\u4ef7\u503c\u4f30\u8ba1\u534f\u540c\u7ec4\u5408\u3002", "result": "CogniQ-H\u5728\u7ba1\u9053\u8d28\u91cf\u65b9\u9762\u63d0\u9ad8\u4e8613.9%\uff0c\u6536\u655b\u901f\u5ea6\u63d0\u9ad8\u4e862.8\u500d\u3002", "conclusion": "CogniQ-H\u5728\u591a\u4e2a\u9886\u57df18\u4e2a\u4e0d\u540c\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u4e8eRL\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cCogniQ-H\u5728\u7ba1\u9053\u8d28\u91cf\u65b9\u9762\u63d0\u9ad8\u4e8613.9%\uff0c\u6536\u655b\u901f\u5ea6\u63d0\u9ad8\u4e862.8\u500d\u3002"}}
{"id": "2507.13396", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13396", "abs": "https://arxiv.org/abs/2507.13396", "authors": ["Qingyun Sun", "Jiaqi Yuan", "Shan He", "Xiao Guan", "Haonan Yuan", "Xingcheng Fu", "Jianxin Li", "Philip S. Yu"], "title": "DyG-RAG: Dynamic Graph Retrieval-Augmented Generation with Event-Centric Reasoning", "comment": null, "summary": "Graph Retrieval-Augmented Generation has emerged as a powerful paradigm for\ngrounding large language models with external structured knowledge. However,\nexisting Graph RAG methods struggle with temporal reasoning, due to their\ninability to model the evolving structure and order of real-world events. In\nthis work, we introduce DyG-RAG, a novel event-centric dynamic graph\nretrieval-augmented generation framework designed to capture and reason over\ntemporal knowledge embedded in unstructured text. To eliminate temporal\nambiguity in traditional retrieval units, DyG-RAG proposes Dynamic Event Units\n(DEUs) that explicitly encode both semantic content and precise temporal\nanchors, enabling accurate and interpretable time-aware retrieval. To capture\ntemporal and causal dependencies across events, DyG-RAG constructs an event\ngraph by linking DEUs that share entities and occur close in time, supporting\nefficient and meaningful multi-hop reasoning. To ensure temporally consistent\ngeneration, DyG-RAG introduces an event timeline retrieval pipeline that\nretrieves event sequences via time-aware traversal, and proposes a Time\nChain-of-Thought strategy for temporally grounded answer generation. This\nunified pipeline enables DyG-RAG to retrieve coherent, temporally ordered event\nsequences and to answer complex, time-sensitive queries that standard RAG\nsystems cannot resolve. Extensive experiments on temporal QA benchmarks\ndemonstrate that DyG-RAG significantly improves the accuracy and recall of\nthree typical types of temporal reasoning questions, paving the way for more\nfaithful and temporal-aware generation. DyG-RAG is available at\nhttps://github.com/RingBDStack/DyG-RAG.", "AI": {"tldr": "DyG-RAG\u662f\u4e00\u79cd\u65b0\u9896\u7684\u4ee5\u4e8b\u4ef6\u4e3a\u4e2d\u5fc3\u7684\u52a8\u6001\u56fe\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u65e8\u5728\u6355\u83b7\u548c\u63a8\u7406\u5d4c\u5165\u5728\u975e\u7ed3\u6784\u5316\u6587\u672c\u4e2d\u7684\u65f6\u95f4\u77e5\u8bc6\u3002", "motivation": "\u73b0\u6709\u7684\u56feRAG\u65b9\u6cd5\u96be\u4ee5\u8fdb\u884c\u65f6\u95f4\u63a8\u7406\uff0c\u56e0\u4e3a\u5b83\u4eec\u65e0\u6cd5\u5bf9\u73b0\u5b9e\u4e16\u754c\u4e8b\u4ef6\u7684\u6f14\u53d8\u7ed3\u6784\u548c\u987a\u5e8f\u8fdb\u884c\u5efa\u6a21\u3002", "method": "DyG-RAG\u6784\u5efa\u4e86\u4e00\u4e2a\u4e8b\u4ef6\u56fe\uff0c\u901a\u8fc7\u8fde\u63a5\u5171\u4eab\u5b9e\u4f53\u5e76\u5728\u65f6\u95f4\u4e0a\u63a5\u8fd1\u7684DEU\u6765\u6355\u83b7\u4e8b\u4ef6\u4e4b\u95f4\u7684\u65f6\u95f4\u548c\u56e0\u679c\u4f9d\u8d56\u5173\u7cfb\uff0c\u652f\u6301\u9ad8\u6548\u548c\u6709\u610f\u4e49\u7684\u591a\u8df3\u63a8\u7406\u3002\u4e3a\u4e86\u786e\u4fdd\u65f6\u95f4\u4e0a\u4e00\u81f4\u7684\u751f\u6210\uff0cDyG-RAG\u5f15\u5165\u4e86\u4e00\u4e2a\u4e8b\u4ef6\u65f6\u95f4\u7ebf\u68c0\u7d22\u7ba1\u9053\uff0c\u8be5\u7ba1\u9053\u901a\u8fc7\u65f6\u95f4\u611f\u77e5\u904d\u5386\u6765\u68c0\u7d22\u4e8b\u4ef6\u5e8f\u5217\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65f6\u95f4\u94fe\u5f0f\u601d\u8003\u7b56\u7565\uff0c\u7528\u4e8e\u65f6\u95f4\u4e0a\u63a5\u5730\u7684\u7b54\u6848\u751f\u6210\u3002", "result": "DyG-RAG\u68c0\u7d22\u8fde\u8d2f\u7684\u3001\u65f6\u95f4\u4e0a\u6392\u5e8f\u7684\u4e8b\u4ef6\u5e8f\u5217\uff0c\u5e76\u56de\u7b54\u6807\u51c6RAG\u7cfb\u7edf\u65e0\u6cd5\u89e3\u51b3\u7684\u590d\u6742\u7684\u65f6\u95f4\u654f\u611f\u67e5\u8be2\u3002", "conclusion": "DyG-RAG\u663e\u8457\u63d0\u9ad8\u4e86\u4e09\u79cd\u5178\u578b\u7684\u65f6\u95f4\u63a8\u7406\u95ee\u9898\u7684\u51c6\u786e\u7387\u548c\u53ec\u56de\u7387\uff0c\u4e3a\u66f4\u5fe0\u5b9e\u548c\u65f6\u95f4\u611f\u77e5\u7684\u751f\u6210\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2507.13399", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13399", "abs": "https://arxiv.org/abs/2507.13399", "authors": ["Mert Sehri", "Zehui Hua", "Francisco de Assis Boldt", "Patrick Dumond"], "title": "Selective Embedding for Deep Learning", "comment": null, "summary": "Deep learning has revolutionized many industries by enabling models to\nautomatically learn complex patterns from raw data, reducing dependence on\nmanual feature engineering. However, deep learning algorithms are sensitive to\ninput data, and performance often deteriorates under nonstationary conditions\nand across dissimilar domains, especially when using time-domain data.\nConventional single-channel or parallel multi-source data loading strategies\neither limit generalization or increase computational costs. This study\nintroduces selective embedding, a novel data loading strategy, which alternates\nshort segments of data from multiple sources within a single input channel.\nDrawing inspiration from cognitive psychology, selective embedding mimics\nhuman-like information processing to reduce model overfitting, enhance\ngeneralization, and improve computational efficiency. Validation is conducted\nusing six time-domain datasets, demonstrating that the proposed method\nconsistently achieves high classification accuracy across various deep learning\narchitectures while significantly reducing training times. The approach proves\nparticularly effective for complex systems with multiple data sources, offering\na scalable and resource-efficient solution for real-world applications in\nhealthcare, heavy machinery, marine, railway, and agriculture, where robustness\nand adaptability are critical.", "AI": {"tldr": "Selective embedding, a novel data loading strategy, mimics human-like information processing to reduce model overfitting, enhance generalization, and improve computational efficiency on time-domain datasets.", "motivation": "Deep learning algorithms are sensitive to input data, and performance often deteriorates under nonstationary conditions and across dissimilar domains, especially when using time-domain data. Conventional single-channel or parallel multi-source data loading strategies either limit generalization or increase computational costs.", "method": "This study introduces selective embedding, a novel data loading strategy, which alternates short segments of data from multiple sources within a single input channel.", "result": "Validation is conducted using six time-domain datasets, demonstrating that the proposed method consistently achieves high classification accuracy across various deep learning architectures while significantly reducing training times.", "conclusion": "Selective embedding achieves high classification accuracy across various deep learning architectures while significantly reducing training times, especially for complex systems with multiple data sources."}}
{"id": "2507.13712", "categories": ["cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13712", "abs": "https://arxiv.org/abs/2507.13712", "authors": ["Jing Chang", "Chang Liu", "Jinbin Huang", "Rui Mao", "Jianbin Qin"], "title": "LLaPipe: LLM-Guided Reinforcement Learning for Automated Data Preparation Pipeline Construction", "comment": null, "summary": "Automated data preparation is crucial for democratizing machine learning, yet\nexisting reinforcement learning (RL) based approaches suffer from inefficient\nexploration in the vast space of possible preprocessing pipelines. We present\nLLaPipe, a novel framework that addresses this exploration bottleneck by\nintegrating Large Language Models (LLMs) as intelligent policy advisors. Unlike\ntraditional methods that rely solely on statistical features and blind\ntrial-and-error, LLaPipe leverages the semantic understanding capabilities of\nLLMs to provide contextually relevant exploration guidance. Our framework\nintroduces three key innovations: (1) an LLM Policy Advisor that analyzes\ndataset semantics and pipeline history to suggest promising preprocessing\noperations, (2) an Experience Distillation mechanism that mines successful\npatterns from past pipelines and transfers this knowledge to guide future\nexploration, and (3) an Adaptive Advisor Triggering strategy\n(Advisor\\textsuperscript{+}) that dynamically determines when LLM intervention\nis most beneficial, balancing exploration effectiveness with computational\ncost. Through extensive experiments on 18 diverse datasets spanning multiple\ndomains, we demonstrate that LLaPipe achieves up to 22.4\\% improvement in\npipeline quality and 2.3$\\times$ faster convergence compared to\nstate-of-the-art RL-based methods, while maintaining computational efficiency\nthrough selective LLM usage (averaging only 19.0\\% of total exploration steps).", "AI": {"tldr": "LLaPipe uses LLMs to guide exploration in automated data preparation, improving pipeline quality and convergence speed compared to RL-based methods.", "motivation": "Existing reinforcement learning (RL) based approaches suffer from inefficient exploration in the vast space of possible preprocessing pipelines.", "method": "LLaPipe, a novel framework that addresses this exploration bottleneck by integrating Large Language Models (LLMs) as intelligent policy advisors. It introduces three key innovations: (1) an LLM Policy Advisor, (2) an Experience Distillation mechanism, and (3) an Adaptive Advisor Triggering strategy.", "result": "LLaPipe achieves up to 22.4% improvement in pipeline quality and 2.3x faster convergence compared to state-of-the-art RL-based methods.", "conclusion": "LLaPipe achieves up to 22.4% improvement in pipeline quality and 2.3x faster convergence compared to state-of-the-art RL-based methods, while maintaining computational efficiency through selective LLM usage (averaging only 19.0% of total exploration steps)."}}
{"id": "2507.13525", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.13525", "abs": "https://arxiv.org/abs/2507.13525", "authors": ["Genki Kusano", "Kosuke Akimoto", "Kunihiro Takeoka"], "title": "Revisiting Prompt Engineering: A Comprehensive Evaluation for LLM-based Personalized Recommendation", "comment": "Accepted to ACM RecSys2025 reproducibility", "summary": "Large language models (LLMs) can perform recommendation tasks by taking\nprompts written in natural language as input. Compared to traditional methods\nsuch as collaborative filtering, LLM-based recommendation offers advantages in\nhandling cold-start, cross-domain, and zero-shot scenarios, as well as\nsupporting flexible input formats and generating explanations of user behavior.\nIn this paper, we focus on a single-user setting, where no information from\nother users is used. This setting is practical for privacy-sensitive or\ndata-limited applications. In such cases, prompt engineering becomes especially\nimportant for controlling the output generated by the LLM. We conduct a\nlarge-scale comparison of 23 prompt types across 8 public datasets and 12 LLMs.\nWe use statistical tests and linear mixed-effects models to evaluate both\naccuracy and inference cost. Our results show that for cost-efficient LLMs,\nthree types of prompts are especially effective: those that rephrase\ninstructions, consider background knowledge, and make the reasoning process\neasier to follow. For high-performance LLMs, simple prompts often outperform\nmore complex ones while reducing cost. In contrast, commonly used prompting\nstyles in natural language processing, such as step-by-step reasoning, or the\nuse of reasoning models often lead to lower accuracy. Based on these findings,\nwe provide practical suggestions for selecting prompts and LLMs depending on\nthe required balance between accuracy and cost.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u5bf9 LLM \u63a8\u8350\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u7b80\u5355\u63d0\u793a\u5bf9\u4e8e\u9ad8\u6027\u80fd LLM \u66f4\u6709\u6548\uff0c\u800c\u7279\u5b9a\u7c7b\u578b\u7684\u63d0\u793a\u5bf9\u6210\u672c\u6548\u76ca\u9ad8\u7684 LLM \u66f4\u6709\u6548\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u53ef\u4ee5\u901a\u8fc7\u91c7\u7528\u81ea\u7136\u8bed\u8a00\u7f16\u5199\u7684\u63d0\u793a\u4f5c\u4e3a\u8f93\u5165\u6765\u6267\u884c\u63a8\u8350\u4efb\u52a1\u3002\u4e0e\u534f\u540c\u8fc7\u6ee4\u7b49\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u57fa\u4e8e LLM \u7684\u63a8\u8350\u5728\u5904\u7406\u51b7\u542f\u52a8\u3001\u8de8\u9886\u57df\u548c\u96f6\u6837\u672c\u573a\u666f\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u5e76\u4e14\u652f\u6301\u7075\u6d3b\u7684\u8f93\u5165\u683c\u5f0f\u548c\u751f\u6210\u7528\u6237\u884c\u4e3a\u7684\u89e3\u91ca\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5173\u6ce8\u7684\u662f\u5355\u7528\u6237\u8bbe\u7f6e\uff0c\u5176\u4e2d\u4e0d\u4f7f\u7528\u6765\u81ea\u5176\u4ed6\u7528\u6237\u7684\u4fe1\u606f\u3002\u8fd9\u79cd\u8bbe\u7f6e\u5bf9\u4e8e\u9690\u79c1\u654f\u611f\u6216\u6570\u636e\u6709\u9650\u7684\u5e94\u7528\u7a0b\u5e8f\u6765\u8bf4\u662f\u5b9e\u7528\u7684\u3002", "method": "\u5927\u89c4\u6a21\u6bd4\u8f83\u4e86 23 \u79cd\u63d0\u793a\u7c7b\u578b\uff0c\u8de8\u8d8a 8 \u4e2a\u516c\u5171\u6570\u636e\u96c6\u548c 12 \u4e2a LLM\u3002\u6211\u4eec\u4f7f\u7528\u7edf\u8ba1\u6d4b\u8bd5\u548c\u7ebf\u6027\u6df7\u5408\u6548\u5e94\u6a21\u578b\u6765\u8bc4\u4f30\u51c6\u786e\u6027\u548c\u63a8\u7406\u6210\u672c\u3002", "result": "\u5bf9\u4e8e\u6210\u672c\u6548\u76ca\u9ad8\u7684 LLM\uff0c\u4e09\u79cd\u7c7b\u578b\u7684\u63d0\u793a\u7279\u522b\u6709\u6548\uff1a\u90a3\u4e9b\u6539\u5199\u6307\u4ee4\u3001\u8003\u8651\u80cc\u666f\u77e5\u8bc6\u5e76\u4f7f\u63a8\u7406\u8fc7\u7a0b\u66f4\u6613\u4e8e\u9075\u5faa\u7684\u63d0\u793a\u3002\u5bf9\u4e8e\u9ad8\u6027\u80fd LLM\uff0c\u7b80\u5355\u7684\u63d0\u793a\u901a\u5e38\u4f18\u4e8e\u66f4\u590d\u6742\u7684\u63d0\u793a\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u6210\u672c\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u5e38\u7528\u7684\u63d0\u793a\u98ce\u683c\uff0c\u5982\u9010\u6b65\u63a8\u7406\u6216\u63a8\u7406\u6a21\u578b\u7684\u4f7f\u7528\uff0c\u901a\u5e38\u4f1a\u5bfc\u81f4\u8f83\u4f4e\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u5bf9\u4e8e\u6210\u672c\u6548\u76ca\u9ad8\u7684LLM\uff0c\u4e09\u79cd\u7c7b\u578b\u7684\u63d0\u793a\u7279\u522b\u6709\u6548\uff1a\u90a3\u4e9b\u6539\u5199\u6307\u4ee4\u3001\u8003\u8651\u80cc\u666f\u77e5\u8bc6\u5e76\u4f7f\u63a8\u7406\u8fc7\u7a0b\u66f4\u6613\u4e8e\u9075\u5faa\u7684\u63d0\u793a\u3002\u5bf9\u4e8e\u9ad8\u6027\u80fdLLM\uff0c\u7b80\u5355\u7684\u63d0\u793a\u901a\u5e38\u4f18\u4e8e\u66f4\u590d\u6742\u7684\u63d0\u793a\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u6210\u672c\u3002"}}
{"id": "2507.13357", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13357", "abs": "https://arxiv.org/abs/2507.13357", "authors": ["Atharva Bhargude", "Ishan Gonehal", "Chandler Haney", "Dave Yoon", "Kevin Zhu", "Aaron Sandoval", "Sean O'Brien", "Kaustubh Vinnakota"], "title": "Adaptive Linguistic Prompting (ALP) Enhances Phishing Webpage Detection in Multimodal Large Language Models", "comment": "Published at ACL 2025 SRW, 9 pages, 3 figures", "summary": "Phishing attacks represent a significant cybersecurity threat, necessitating\nadaptive detection techniques. This study explores few-shot Adaptive Linguistic\nPrompting (ALP) in detecting phishing webpages through the multimodal\ncapabilities of state-of-the-art large language models (LLMs) such as GPT-4o\nand Gemini 1.5 Pro. ALP is a structured semantic reasoning method that guides\nLLMs to analyze textual deception by breaking down linguistic patterns,\ndetecting urgency cues, and identifying manipulative diction commonly found in\nphishing content. By integrating textual, visual, and URL-based analysis, we\npropose a unified model capable of identifying sophisticated phishing attempts.\nOur experiments demonstrate that ALP significantly enhances phishing detection\naccuracy by guiding LLMs through structured reasoning and contextual analysis.\nThe findings highlight the potential of ALP-integrated multimodal LLMs to\nadvance phishing detection frameworks, achieving an F1-score of 0.93,\nsurpassing traditional approaches. These results establish a foundation for\nmore robust, interpretable, and adaptive linguistic-based phishing detection\nsystems using LLMs.", "AI": {"tldr": "This study explores few-shot Adaptive Linguistic Prompting (ALP) with multimodal LLMs to detect phishing webpages, achieving superior accuracy compared to traditional methods.", "motivation": "Phishing attacks are a significant cybersecurity threat requiring adaptive detection techniques.", "method": "Few-shot Adaptive Linguistic Prompting (ALP) integrating textual, visual, and URL-based analysis with LLMs like GPT-4o and Gemini 1.5 Pro.", "result": "ALP significantly enhances phishing detection accuracy by guiding LLMs through structured reasoning and contextual analysis, achieving an F1-score of 0.93.", "conclusion": "ALP-integrated multimodal LLMs advance phishing detection, achieving an F1-score of 0.93, surpassing traditional approaches, and establishing a foundation for more robust, interpretable, and adaptive linguistic-based phishing detection systems using LLMs."}}
{"id": "2507.13511", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13511", "abs": "https://arxiv.org/abs/2507.13511", "authors": ["Nabil Abdelaziz Ferhat Taleb", "Abdolazim Rezaei", "Raj Atulkumar Patel", "Mehdi Sookhak"], "title": "GraphTrafficGPT: Enhancing Traffic Management Through Graph-Based AI Agent Coordination", "comment": null, "summary": "Large Language Models (LLMs) offer significant promise for intelligent\ntraffic management; however, current chain-based systems like TrafficGPT are\nhindered by sequential task execution, high token usage, and poor scalability,\nmaking them inefficient for complex, real-world scenarios. To address these\nlimitations, we propose GraphTrafficGPT, a novel graph-based architecture,\nwhich fundamentally redesigns the task coordination process for LLM-driven\ntraffic applications. GraphTrafficGPT represents tasks and their dependencies\nas nodes and edges in a directed graph, enabling efficient parallel execution\nand dynamic resource allocation. The main idea behind the proposed model is a\nBrain Agent that decomposes user queries, constructs optimized dependency\ngraphs, and coordinates a network of specialized agents for data retrieval,\nanalysis, visualization, and simulation. By introducing advanced context-aware\ntoken management and supporting concurrent multi-query processing, the proposed\narchitecture handles interdependent tasks typical of modern urban mobility\nenvironments. Experimental results demonstrate that GraphTrafficGPT reduces\ntoken consumption by 50.2% and average response latency by 19.0% compared to\nTrafficGPT, while supporting simultaneous multi-query execution with up to\n23.0% improvement in efficiency.", "AI": {"tldr": "GraphTrafficGPT\u662f\u4e00\u79cd\u65b0\u7684\u56fe\u67b6\u6784\uff0c\u5b83\u901a\u8fc7\u5e76\u884c\u5904\u7406\u548c\u4f18\u5316\u8d44\u6e90\u5206\u914d\uff0c\u63d0\u9ad8\u4e86LLM\u5728\u4ea4\u901a\u7ba1\u7406\u4e2d\u7684\u6548\u7387\uff0c\u663e\u8457\u4f18\u4e8eTrafficGPT\u3002", "motivation": "\u5f53\u524d\u7684\u57fa\u4e8e\u94fe\u7684\u7cfb\u7edf\u5728\u590d\u6742\u3001\u73b0\u5b9e\u4e16\u754c\u7684\u573a\u666f\u4e2d\u6548\u7387\u4f4e\u4e0b\uff0c\u56e0\u4e3a\u5b83\u4eec\u5b58\u5728\u987a\u5e8f\u4efb\u52a1\u6267\u884c\u3001\u9ad8\u4ee4\u724c\u4f7f\u7528\u548c\u5dee\u7684\u53ef\u6269\u5c55\u6027\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u65b0\u578b\u67b6\u6784GraphTrafficGPT\uff0c\u7528\u4e8eLLM\u9a71\u52a8\u7684\u4ea4\u901a\u5e94\u7528\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0eTrafficGPT\u76f8\u6bd4\uff0cGraphTrafficGPT\u51cf\u5c11\u4e8650.2%\u7684\u4ee4\u724c\u6d88\u8017\u548c19.0%\u7684\u5e73\u5747\u54cd\u5e94\u5ef6\u8fdf\uff0c\u540c\u65f6\u652f\u6301\u5e76\u53d1\u591a\u67e5\u8be2\u6267\u884c\uff0c\u6548\u7387\u63d0\u9ad8\u4e8623.0%\u3002", "conclusion": "GraphTrafficGPT\u901a\u8fc7\u5e76\u884c\u6267\u884c\u548c\u52a8\u6001\u8d44\u6e90\u5206\u914d\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4ea4\u901a\u7ba1\u7406\u4efb\u52a1\u7684\u6548\u7387\u548c\u54cd\u5e94\u901f\u5ea6\u3002"}}
{"id": "2507.13359", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13359", "abs": "https://arxiv.org/abs/2507.13359", "authors": ["Yang Zhou", "Junjie Li", "CongYang Ou", "Dawei Yan", "Haokui Zhang", "Xizhe Xue"], "title": "Open-Vocabulary Object Detection in UAV Imagery: A Review and Future Perspectives", "comment": "27 pages, 5 figures", "summary": "Due to its extensive applications, aerial image object detection has long\nbeen a hot topic in computer vision. In recent years, advancements in Unmanned\nAerial Vehicles (UAV) technology have further propelled this field to new\nheights, giving rise to a broader range of application requirements. However,\ntraditional UAV aerial object detection methods primarily focus on detecting\npredefined categories, which significantly limits their applicability. The\nadvent of cross-modal text-image alignment (e.g., CLIP) has overcome this\nlimitation, enabling open-vocabulary object detection (OVOD), which can\nidentify previously unseen objects through natural language descriptions. This\nbreakthrough significantly enhances the intelligence and autonomy of UAVs in\naerial scene understanding. This paper presents a comprehensive survey of OVOD\nin the context of UAV aerial scenes. We begin by aligning the core principles\nof OVOD with the unique characteristics of UAV vision, setting the stage for a\nspecialized discussion. Building on this foundation, we construct a systematic\ntaxonomy that categorizes existing OVOD methods for aerial imagery and provides\na comprehensive overview of the relevant datasets. This structured review\nenables us to critically dissect the key challenges and open problems at the\nintersection of these fields. Finally, based on this analysis, we outline\npromising future research directions and application prospects. This survey\naims to provide a clear road map and a valuable reference for both newcomers\nand seasoned researchers, fostering innovation in this rapidly evolving domain.\nWe keep tracing related works at\nhttps://github.com/zhouyang2002/OVOD-in-UVA-imagery", "AI": {"tldr": "This paper surveys open-vocabulary object detection in UAV aerial scenes, providing a taxonomy of methods, datasets, challenges, and future directions.", "motivation": "Traditional UAV aerial object detection methods primarily focus on detecting predefined categories, which significantly limits their applicability. The advent of cross-modal text-image alignment (e.g., CLIP) has overcome this limitation, enabling open-vocabulary object detection (OVOD).", "method": "The paper constructs a systematic taxonomy that categorizes existing OVOD methods for aerial imagery and provides a comprehensive overview of the relevant datasets.", "result": "The survey critically dissects the key challenges and open problems at the intersection of OVOD and UAV vision, and outlines promising future research directions and application prospects.", "conclusion": "This survey provides a roadmap and reference for researchers in the field of open-vocabulary object detection (OVOD) in UAV aerial scenes, fostering innovation in this domain."}}
{"id": "2507.13413", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13413", "abs": "https://arxiv.org/abs/2507.13413", "authors": ["Aleksey Lapin", "Igor Hromov", "Stanislav Chumakov", "Mile Mitrovic", "Dmitry Simakov", "Nikolay O. Nikitin", "Andrey V. Savchenko"], "title": "LightAutoDS-Tab: Multi-AutoML Agentic System for Tabular Data", "comment": "11 pages, 2 figures", "summary": "AutoML has advanced in handling complex tasks using the integration of LLMs,\nyet its efficiency remains limited by dependence on specific underlying tools.\nIn this paper, we introduce LightAutoDS-Tab, a multi-AutoML agentic system for\ntasks with tabular data, which combines an LLM-based code generation with\nseveral AutoML tools. Our approach improves the flexibility and robustness of\npipeline design, outperforming state-of-the-art open-source solutions on\nseveral data science tasks from Kaggle. The code of LightAutoDS-Tab is\navailable in the open repository https://github.com/sb-ai-lab/LADS", "AI": {"tldr": "LightAutoDS-Tab\u662f\u4e00\u79cd\u7528\u4e8e\u8868\u683c\u6570\u636e\u7684\u591aAutoML\u4ee3\u7406\u7cfb\u7edf\uff0c\u5b83\u7ed3\u5408\u4e86LLM\u548cAutoML\u5de5\u5177\uff0c\u4f18\u4e8eKaggle\u4e0a\u7684\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "AutoML\u5728\u5229\u7528LLM\u5904\u7406\u590d\u6742\u4efb\u52a1\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5176\u6548\u7387\u4ecd\u7136\u53d7\u5230\u5bf9\u7279\u5b9a\u5e95\u5c42\u5de5\u5177\u7684\u4f9d\u8d56\u6027\u7684\u9650\u5236\u3002", "method": "\u7ed3\u5408\u4e86\u57fa\u4e8eLLM\u7684\u4ee3\u7801\u751f\u6210\u548c\u591a\u79cdAutoML\u5de5\u5177\u7684\u591aAutoML\u4ee3\u7406\u7cfb\u7edf\u3002", "result": "\u63d0\u9ad8\u4e86pipeline\u8bbe\u8ba1\u7684\u7075\u6d3b\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "LightAutoDS-Tab\u5728\u591a\u4e2aKaggle\u6570\u636e\u79d1\u5b66\u4efb\u52a1\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u5f00\u6e90\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.13757", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.13757", "abs": "https://arxiv.org/abs/2507.13757", "authors": ["Joydeep Chandra", "Prabal Manhas"], "title": "Efficient and Scalable Self-Healing Databases Using Meta-Learning and Dependency-Driven Recovery", "comment": null, "summary": "This study explored the development of a novel self-healing framework for\ndatabases using meta-learning and reinforcement learning techniques. The\nprimary objective was to address the challenges of real-time adaptability and\nminimal retraining in dynamic workload environments. The proposed approach\nintegrated Model-Agnostic Meta-Learning (MAML) with reinforcement learning to\nenable anomaly detection and corrective actions that adapted swiftly to\nevolving database conditions. Multi-objective optimization was employed to\nbalance performance, resource utilization, and cost efficiency during the\nhealing process. Graph Neural Networks (GNNs) were incorporated to model\ninterdependencies within database components, ensuring holistic recovery\nstrategies. Data efficiency was enhanced through synthetic task augmentation\nand self-supervised learning, enabling effective training in sparse data\nregimes. To promote trust and transparency, explainable AI techniques were\nintegrated to provide interpretable insights into anomaly detection and healing\nactions. Federated meta-learning further enabled privacy-preserving\nadaptability in distributed database environments. The framework demonstrated\nsignificant improvements in adaptability, efficiency, and reliability,\ncontributing to advancements in database management and self-healing systems.", "AI": {"tldr": "A novel self-healing framework for databases using meta-learning and reinforcement learning techniques is proposed, which enables anomaly detection and corrective actions that adapted swiftly to evolving database conditions.", "motivation": "address the challenges of real-time adaptability and minimal retraining in dynamic workload environments", "method": "integrated Model-Agnostic Meta-Learning (MAML) with reinforcement learning, Multi-objective optimization, Graph Neural Networks (GNNs), synthetic task augmentation and self-supervised learning, explainable AI techniques, Federated meta-learning", "result": "significant improvements in adaptability, efficiency, and reliability", "conclusion": "The framework demonstrates significant improvements in adaptability, efficiency, and reliability, contributing to advancements in database management and self-healing systems."}}
{"id": "2507.13622", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.13622", "abs": "https://arxiv.org/abs/2507.13622", "authors": ["Youlin Wu", "Yuanyuan Sun", "Xiaokun Zhang", "Haoxi Zhan", "Bo Xu", "Liang Yang", "Hongfei Lin"], "title": "IP2: Entity-Guided Interest Probing for Personalized News Recommendation", "comment": "Accepted in RecSys 2025", "summary": "News recommender systems aim to provide personalized news reading experiences\nfor users based on their reading history. Behavioral science studies suggest\nthat screen-based news reading contains three successive steps: scanning, title\nreading, and then clicking. Adhering to these steps, we find that intra-news\nentity interest dominates the scanning stage, while the inter-news entity\ninterest guides title reading and influences click decisions. Unfortunately,\ncurrent methods overlook the unique utility of entities in news recommendation.\nTo this end, we propose a novel method called IP2 to probe entity-guided\nreading interest at both intra- and inter-news levels. At the intra-news level,\na Transformer-based entity encoder is devised to aggregate mentioned entities\nin the news title into one signature entity. Then, a signature entity-title\ncontrastive pre-training is adopted to initialize entities with proper meanings\nusing the news story context, which in the meantime facilitates us to probe for\nintra-news entity interest. As for the inter-news level, a dual tower user\nencoder is presented to capture inter-news reading interest from both the title\nmeaning and entity sides. In addition to highlighting the contribution of\ninter-news entity guidance, a cross-tower attention link is adopted to\ncalibrate title reading interest using inter-news entity interest, thus further\naligning with real-world behavior. Extensive experiments on two real-world\ndatasets demonstrate that our IP2 achieves state-of-the-art performance in news\nrecommendation.", "AI": {"tldr": "IP2\u6a21\u578b\u901a\u8fc7\u5728\u65b0\u95fb\u7684\u5185\u90e8\u548c\u5916\u90e8\u5c42\u9762\u4e0a\u63a2\u6d4b\u5b9e\u4f53\u5f15\u5bfc\u7684\u9605\u8bfb\u5174\u8da3\uff0c\u4ece\u800c\u6539\u8fdb\u65b0\u95fb\u63a8\u8350\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5ffd\u7565\u4e86\u5b9e\u4f53\u5728\u65b0\u95fb\u63a8\u8350\u4e2d\u7684\u72ec\u7279\u6548\u7528\u3002", "method": "Transformer-based\u5b9e\u4f53\u7f16\u7801\u5668\u548c\u53cc\u5854\u7528\u6237\u7f16\u7801\u5668\u3002", "result": "IP2\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "IP2\u5728\u65b0\u95fb\u63a8\u8350\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2507.13380", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13380", "abs": "https://arxiv.org/abs/2507.13380", "authors": ["Keito Inoshita", "Rushia Harada"], "title": "Persona-Based Synthetic Data Generation Using Multi-Stage Conditioning with Large Language Models for Emotion Recognition", "comment": null, "summary": "In the field of emotion recognition, the development of high-performance\nmodels remains a challenge due to the scarcity of high-quality, diverse\nemotional datasets. Emotional expressions are inherently subjective, shaped by\nindividual personality traits, socio-cultural backgrounds, and contextual\nfactors, making large-scale, generalizable data collection both ethically and\npractically difficult. To address this issue, we introduce PersonaGen, a novel\nframework for generating emotionally rich text using a Large Language Model\n(LLM) through multi-stage persona-based conditioning. PersonaGen constructs\nlayered virtual personas by combining demographic attributes, socio-cultural\nbackgrounds, and detailed situational contexts, which are then used to guide\nemotion expression generation. We conduct comprehensive evaluations of the\ngenerated synthetic data, assessing semantic diversity through clustering and\ndistributional metrics, human-likeness via LLM-based quality scoring, realism\nthrough comparison with real-world emotion corpora, and practical utility in\ndownstream emotion classification tasks. Experimental results show that\nPersonaGen significantly outperforms baseline methods in generating diverse,\ncoherent, and discriminative emotion expressions, demonstrating its potential\nas a robust alternative for augmenting or replacing real-world emotional\ndatasets.", "AI": {"tldr": "PersonaGen\u662f\u4e00\u79cd\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u60c5\u611f\u4e30\u5bcc\u6587\u672c\u7684\u65b0\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u591a\u9636\u6bb5\u7684\u57fa\u4e8e\u89d2\u8272\u7684\u6761\u4ef6\u53cd\u5c04\u6765\u89e3\u51b3\u60c5\u611f\u8bc6\u522b\u9886\u57df\u4e2d\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684\u60c5\u611f\u6570\u636e\u96c6\u7684\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684\u60c5\u611f\u6570\u636e\u96c6\u7684\u7a00\u7f3a\uff0c\u9ad8\u6027\u80fd\u6a21\u578b\u7684\u53d1\u5c55\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u60c5\u611f\u8868\u8fbe\u672c\u8d28\u4e0a\u662f\u4e3b\u89c2\u7684\uff0c\u53d7\u4e2a\u4eba\u6027\u683c\u7279\u5f81\u3001\u793e\u4f1a\u6587\u5316\u80cc\u666f\u548c\u60c5\u5883\u56e0\u7d20\u7684\u5f71\u54cd\uff0c\u4f7f\u5f97\u5927\u89c4\u6a21\u3001\u901a\u7528\u5316\u7684\u6570\u636e\u6536\u96c6\u5728\u4f26\u7406\u548c\u5b9e\u8df5\u4e0a\u90fd\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u901a\u8fc7\u591a\u9636\u6bb5\u57fa\u4e8e\u89d2\u8272\u7684\u6761\u4ef6\u53cd\u5c04\u751f\u6210\u60c5\u611f\u4e30\u5bcc\u7684\u6587\u672c\u3002", "result": "PersonaGen\u5728\u751f\u6210\u591a\u6837\u5316\u3001\u8fde\u8d2f\u548c\u53ef\u533a\u5206\u7684\u60c5\u611f\u8868\u8fbe\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "PersonaGen\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u591a\u6837\u3001\u8fde\u8d2f\u548c\u53ef\u533a\u5206\u7684\u60c5\u611f\u8868\u8fbe\uff0c\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3a\u589e\u5f3a\u6216\u66ff\u4ee3\u771f\u5b9e\u4e16\u754c\u60c5\u611f\u6570\u636e\u96c6\u7684\u5f3a\u5927\u66ff\u4ee3\u65b9\u6848\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.13541", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13541", "abs": "https://arxiv.org/abs/2507.13541", "authors": ["Shuyue Stella Li", "Melanie Sclar", "Hunter Lang", "Ansong Ni", "Jacqueline He", "Puxin Xu", "Andrew Cohen", "Chan Young Park", "Yulia Tsvetkov", "Asli Celikyilmaz"], "title": "PrefPalette: Personalized Preference Modeling with Latent Attributes", "comment": "17 pages, 6 tables, 5 figures", "summary": "Personalizing AI systems requires understanding not just what users prefer,\nbut the reasons that underlie those preferences - yet current preference models\ntypically treat human judgment as a black box. We introduce PrefPalette, a\nframework that decomposes preferences into attribute dimensions and tailors its\npreference prediction to distinct social community values in a\nhuman-interpretable manner. PrefPalette operationalizes a cognitive science\nprinciple known as multi-attribute decision making in two ways: (1) a scalable\ncounterfactual attribute synthesis step that involves generating synthetic\ntraining data to isolate for individual attribute effects (e.g., formality,\nhumor, cultural values), and (2) attention-based preference modeling that\nlearns how different social communities dynamically weight these attributes.\nThis approach moves beyond aggregate preference modeling to capture the diverse\nevaluation frameworks that drive human judgment. When evaluated on 45 social\ncommunities from the online platform Reddit, PrefPalette outperforms GPT-4o by\n46.6% in average prediction accuracy. Beyond raw predictive improvements,\nPrefPalette also shed light on intuitive, community-specific profiles:\nscholarly communities prioritize verbosity and stimulation, conflict-oriented\ncommunities value sarcasm and directness, and support-based communities\nemphasize empathy. By modeling the attribute-mediated structure of human\njudgment, PrefPalette delivers both superior preference modeling and\ntransparent, interpretable insights, and serves as a first step toward more\ntrustworthy, value-aware personalized applications.", "AI": {"tldr": "PrefPalette\u901a\u8fc7\u5c06\u504f\u597d\u5206\u89e3\u4e3a\u5c5e\u6027\u7ef4\u5ea6\u5e76\u6839\u636e\u793e\u4f1a\u793e\u533a\u4ef7\u503c\u89c2\u8fdb\u884c\u5b9a\u5236\uff0c\u4f18\u4e8eGPT-4o 46.6%\u7684\u5e73\u5747\u9884\u6d4b\u51c6\u786e\u7387\u3002", "motivation": "\u5f53\u524d\u504f\u597d\u6a21\u578b\u901a\u5e38\u5c06\u4eba\u7c7b\u5224\u65ad\u89c6\u4e3a\u9ed1\u76d2\uff0c\u4f46\u4e2a\u6027\u5316AI\u7cfb\u7edf\u4e0d\u4ec5\u9700\u8981\u7406\u89e3\u7528\u6237\u7684\u504f\u597d\uff0c\u8fd8\u9700\u8981\u7406\u89e3\u8fd9\u4e9b\u504f\u597d\u80cc\u540e\u7684\u539f\u56e0\u3002", "method": "PrefPalette\u6846\u67b6\u5c06\u504f\u597d\u5206\u89e3\u4e3a\u5c5e\u6027\u7ef4\u5ea6\uff0c\u5e76\u4ee5\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684\u65b9\u5f0f\u6839\u636e\u4e0d\u540c\u7684\u793e\u4f1a\u793e\u533a\u4ef7\u503c\u89c2\u5b9a\u5236\u5176\u504f\u597d\u9884\u6d4b\u3002\u5b83\u901a\u8fc7\u4ee5\u4e0b\u4e24\u79cd\u65b9\u5f0f\u5b9e\u73b0\u591a\u5c5e\u6027\u51b3\u7b56\uff1a(1) \u53ef\u6269\u5c55\u7684\u53cd\u4e8b\u5b9e\u5c5e\u6027\u5408\u6210\u6b65\u9aa4\uff0c\u6d89\u53ca\u751f\u6210\u5408\u6210\u8bad\u7ec3\u6570\u636e\u4ee5\u5206\u79bb\u4e2a\u4f53\u5c5e\u6027\u6548\u5e94\uff1b(2) \u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u504f\u597d\u5efa\u6a21\uff0c\u5b66\u4e60\u4e0d\u540c\u7684\u793e\u4f1a\u793e\u533a\u5982\u4f55\u52a8\u6001\u5730\u6743\u8861\u8fd9\u4e9b\u5c5e\u6027\u3002", "result": "\u5728\u5bf9\u6765\u81ea\u5728\u7ebf\u5e73\u53f0Reddit\u768445\u4e2a\u793e\u4f1a\u793e\u533a\u8fdb\u884c\u8bc4\u4f30\u65f6\uff0cPrefPalette\u7684\u5e73\u5747\u9884\u6d4b\u51c6\u786e\u7387\u6bd4GPT-4o\u9ad8\u51fa46.6%\u3002", "conclusion": "PrefPalette\u901a\u8fc7\u5bf9\u4eba\u7c7b\u5224\u65ad\u7684\u5c5e\u6027\u4e2d\u4ecb\u7ed3\u6784\u8fdb\u884c\u5efa\u6a21\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u504f\u597d\u5efa\u6a21\u548c\u900f\u660e\u3001\u53ef\u89e3\u91ca\u7684\u6d1e\u5bdf\u529b\uff0c\u662f\u671d\u7740\u66f4\u503c\u5f97\u4fe1\u8d56\u3001\u5177\u6709\u4ef7\u503c\u610f\u8bc6\u7684\u4e2a\u6027\u5316\u5e94\u7528\u8fc8\u51fa\u7684\u7b2c\u4e00\u6b65\u3002"}}
{"id": "2507.13360", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13360", "abs": "https://arxiv.org/abs/2507.13360", "authors": ["Le-Anh Tran", "Chung Nguyen Tran", "Ngoc-Luu Nguyen", "Nhan Cach Dang", "Jordi Carrabina", "David Castells-Rufas", "Minh Son Nguyen"], "title": "Low-Light Enhancement via Encoder-Decoder Network with Illumination Guidance", "comment": "6 pages, 3 figures, ICCCE 2025", "summary": "This paper introduces a novel deep learning framework for low-light image\nenhancement, named the Encoder-Decoder Network with Illumination Guidance\n(EDNIG). Building upon the U-Net architecture, EDNIG integrates an illumination\nmap, derived from Bright Channel Prior (BCP), as a guidance input. This\nillumination guidance helps the network focus on underexposed regions,\neffectively steering the enhancement process. To further improve the model's\nrepresentational power, a Spatial Pyramid Pooling (SPP) module is incorporated\nto extract multi-scale contextual features, enabling better handling of diverse\nlighting conditions. Additionally, the Swish activation function is employed to\nensure smoother gradient propagation during training. EDNIG is optimized within\na Generative Adversarial Network (GAN) framework using a composite loss\nfunction that combines adversarial loss, pixel-wise mean squared error (MSE),\nand perceptual loss. Experimental results show that EDNIG achieves competitive\nperformance compared to state-of-the-art methods in quantitative metrics and\nvisual quality, while maintaining lower model complexity, demonstrating its\nsuitability for real-world applications. The source code for this work is\navailable at https://github.com/tranleanh/ednig.", "AI": {"tldr": "EDNIG\u662f\u4e00\u79cd\u7528\u4e8e\u5f31\u5149\u56fe\u50cf\u589e\u5f3a\u7684\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5b83\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4e14\u6a21\u578b\u590d\u6742\u6027\u8f83\u4f4e\u3002", "motivation": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7528\u4e8e\u5f31\u5149\u56fe\u50cf\u589e\u5f3a\u7684\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u540d\u4e3a\u5177\u6709\u7167\u660e\u6307\u5bfc\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f51\u7edc\uff08EDNIG\uff09\u3002", "method": "\u57fa\u4e8eU-Net\u67b6\u6784\uff0cEDNIG\u96c6\u6210\u4e86\u4ece\u4eae\u901a\u9053\u5148\u9a8c\uff08BCP\uff09\u5bfc\u51fa\u7684\u7167\u660e\u56fe\u4f5c\u4e3a\u6307\u5bfc\u8f93\u5165\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6a21\u578b\u7684\u8868\u5f81\u80fd\u529b\uff0c\u7ed3\u5408\u4e86\u7a7a\u95f4\u91d1\u5b57\u5854\u6c60\u5316\uff08SPP\uff09\u6a21\u5757\u6765\u63d0\u53d6\u591a\u5c3a\u5ea6\u4e0a\u4e0b\u6587\u7279\u5f81\u3002\u6b64\u5916\uff0c\u91c7\u7528Swish\u6fc0\u6d3b\u51fd\u6570\u4ee5\u786e\u4fdd\u8bad\u7ec3\u671f\u95f4\u66f4\u5e73\u6ed1\u7684\u68af\u5ea6\u4f20\u64ad\u3002EDNIG\u5728\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u6846\u67b6\u5185\u4f7f\u7528\u590d\u5408\u635f\u5931\u51fd\u6570\u8fdb\u884c\u4f18\u5316\uff0c\u8be5\u51fd\u6570\u7ed3\u5408\u4e86\u5bf9\u6297\u635f\u5931\u3001\u50cf\u7d20\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u548c\u611f\u77e5\u635f\u5931\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cEDNIG\u5728\u5b9a\u91cf\u6307\u6807\u548c\u89c6\u89c9\u8d28\u91cf\u4e0a\u53d6\u5f97\u4e86\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u4f4e\u7684\u6a21\u578b\u590d\u6742\u6027\u3002", "conclusion": "EDNIG\u5728\u5b9a\u91cf\u6307\u6807\u548c\u89c6\u89c9\u8d28\u91cf\u4e0a\u53d6\u5f97\u4e86\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u4f4e\u7684\u6a21\u578b\u590d\u6742\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.13414", "categories": ["cs.LG", "cs.AI", "math.DG"], "pdf": "https://arxiv.org/pdf/2507.13414", "abs": "https://arxiv.org/abs/2507.13414", "authors": ["Alexander Strunk", "Roland Assam"], "title": "Gauge Flow Models", "comment": null, "summary": "This paper introduces Gauge Flow Models, a novel class of Generative Flow\nModels. These models incorporate a learnable Gauge Field within the Flow\nOrdinary Differential Equation (ODE). A comprehensive mathematical framework\nfor these models, detailing their construction and properties, is provided.\nExperiments using Flow Matching on Gaussian Mixture Models demonstrate that\nGauge Flow Models yields significantly better performance than traditional Flow\nModels of comparable or even larger size. Additionally, unpublished research\nindicates a potential for enhanced performance across a broader range of\ngenerative tasks.", "AI": {"tldr": "Gauge Flow Models incorporates a learnable Gauge Field within the Flow Ordinary Differential Equation (ODE) and yields significantly better performance than traditional Flow Models.", "motivation": "This paper introduces Gauge Flow Models, a novel class of Generative Flow Models", "method": "incorporate a learnable Gauge Field within the Flow Ordinary Differential Equation (ODE)", "result": "Experiments using Flow Matching on Gaussian Mixture Models demonstrate that Gauge Flow Models yields significantly better performance than traditional Flow Models", "conclusion": "Gauge Flow Models yields significantly better performance than traditional Flow Models. Additionally, a potential for enhanced performance across a broader range of generative tasks."}}
{"id": "2507.13892", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.13892", "abs": "https://arxiv.org/abs/2507.13892", "authors": ["Kevin M. Kramer", "Valerie Restat", "Sebastian Strasser", "Uta St\u00f6rl", "Meike Klettke"], "title": "Towards Next Generation Data Engineering Pipelines", "comment": null, "summary": "Data engineering pipelines are a widespread way to provide high-quality data\nfor all kinds of data science applications. However, numerous challenges still\nremain in the composition and operation of such pipelines. Data engineering\npipelines do not always deliver high-quality data. By default, they are also\nnot reactive to changes. When new data is coming in which deviates from prior\ndata, the pipeline could crash or output undesired results. We therefore\nenvision three levels of next generation data engineering pipelines: optimized\ndata pipelines, self-aware data pipelines, and self-adapting data pipelines.\nPipeline optimization addresses the composition of operators and their\nparametrization in order to achieve the highest possible data quality.\nSelf-aware data engineering pipelines enable a continuous monitoring of its\ncurrent state, notifying data engineers on significant changes. Self-adapting\ndata engineering pipelines are then even able to automatically react to those\nchanges. We propose approaches to achieve each of these levels.", "AI": {"tldr": "\u6570\u636e\u5de5\u7a0b\u6d41\u6c34\u7ebf\u5b58\u5728\u8d28\u91cf\u548c\u9002\u5e94\u6027\u95ee\u9898\u3002\u8bba\u6587\u63d0\u51fa\u4e86\u4e0b\u4e00\u4ee3\u5177\u6709\u4f18\u5316\u3001\u81ea\u611f\u77e5\u548c\u81ea\u9002\u5e94\u80fd\u529b\u7684\u6570\u636e\u6d41\u6c34\u7ebf\u3002", "motivation": "\u5f53\u524d\u7684\u6570\u636e\u5de5\u7a0b\u6d41\u6c34\u7ebf\u5728\u63d0\u4f9b\u9ad8\u8d28\u91cf\u6570\u636e\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u5e76\u4e14\u5bf9\u53d8\u5316\u4e0d\u654f\u611f\u3002\u5f53\u65b0\u6570\u636e\u504f\u79bb\u5148\u524d\u6570\u636e\u65f6\uff0c\u6d41\u6c34\u7ebf\u53ef\u80fd\u4f1a\u5d29\u6e83\u6216\u8f93\u51fa\u4e0d\u826f\u7ed3\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u4f18\u5316\u3001\u81ea\u611f\u77e5\u548c\u81ea\u9002\u5e94\u6570\u636e\u5de5\u7a0b\u6d41\u6c34\u7ebf\u7684\u65b9\u6cd5\u3002", "result": "\u8bbe\u60f3\u4e86\u4e0b\u4e00\u4ee3\u6570\u636e\u5de5\u7a0b\u6d41\u6c34\u7ebf\u7684\u4e09\u4e2a\u5c42\u6b21\uff1a\u4f18\u5316\u6570\u636e\u6d41\u6c34\u7ebf\u3001\u81ea\u611f\u77e5\u6570\u636e\u6d41\u6c34\u7ebf\u548c\u81ea\u9002\u5e94\u6570\u636e\u6d41\u6c34\u7ebf\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e0b\u4e00\u4ee3\u6570\u636e\u5de5\u7a0b\u6d41\u6c34\u7ebf\u7684\u4e09\u4e2a\u5c42\u6b21\uff1a\u4f18\u5316\u6570\u636e\u6d41\u6c34\u7ebf\u3001\u81ea\u611f\u77e5\u6570\u636e\u6d41\u6c34\u7ebf\u548c\u81ea\u9002\u5e94\u6570\u636e\u6d41\u6c34\u7ebf\uff0c\u5e76\u63d0\u51fa\u4e86\u5b9e\u73b0\u6bcf\u4e2a\u5c42\u6b21\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.13725", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13725", "abs": "https://arxiv.org/abs/2507.13725", "authors": ["Alejandro Bellog\u00edn", "Linus W. Dietz", "Francesco Ricci", "Pablo S\u00e1nchez"], "title": "Point of Interest Recommendation: Pitfalls and Viable Solutions", "comment": null, "summary": "Point of interest (POI) recommendation can play a pivotal role in enriching\ntourists' experiences by suggesting context-dependent and preference-matching\nlocations and activities, such as restaurants, landmarks, itineraries, and\ncultural attractions. Unlike some more common recommendation domains (e.g.,\nmusic and video), POI recommendation is inherently high-stakes: users invest\nsignificant time, money, and effort to search, choose, and consume these\nsuggested POIs. Despite the numerous research works in the area, several\nfundamental issues remain unresolved, hindering the real-world applicability of\nthe proposed approaches. In this paper, we discuss the current status of the\nPOI recommendation problem and the main challenges we have identified. The\nfirst contribution of this paper is a critical assessment of the current state\nof POI recommendation research and the identification of key shortcomings\nacross three main dimensions: datasets, algorithms, and evaluation\nmethodologies. We highlight persistent issues such as the lack of standardized\nbenchmark datasets, flawed assumptions in the problem definition and model\ndesign, and inadequate treatment of biases in the user behavior and system\nperformance. The second contribution is a structured research agenda that,\nstarting from the identified issues, introduces important directions for future\nwork related to multistakeholder design, context awareness, data collection,\ntrustworthiness, novel interactions, and real-world evaluation.", "AI": {"tldr": "The paper reviews POI recommendation, identifies shortcomings in datasets, algorithms, and evaluation, and suggests future research directions.", "motivation": "POI recommendation is inherently high-stakes, but several fundamental issues remain unresolved, hindering the real-world applicability.", "method": "critical assessment", "result": "Identified key shortcomings across three main dimensions: datasets, algorithms, and evaluation methodologies.", "conclusion": "This paper discusses the current status of POI recommendation and identifies key shortcomings across datasets, algorithms, and evaluation methodologies. It proposes a structured research agenda for future work."}}
{"id": "2507.13381", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13381", "abs": "https://arxiv.org/abs/2507.13381", "authors": ["Rafiq Kamel", "Filippo Guerranti", "Simon Geisler", "Stephan G\u00fcnnemann"], "title": "SAFT: Structure-Aware Fine-Tuning of LLMs for AMR-to-Text Generation", "comment": "Accepted at the KDD2025 Workshop on Structured Knowledge for LLMs", "summary": "Large Language Models (LLMs) are increasingly applied to tasks involving\nstructured inputs such as graphs. Abstract Meaning Representations (AMRs),\nwhich encode rich semantics as directed graphs, offer a rigorous testbed for\nevaluating LLMs on text generation from such structures. Yet, current methods\noften arbitrarily linearize AMRs, discarding key structural cues, or rely on\narchitectures incompatible with standard LLMs. We introduce SAFT, a\nstructure-aware fine-tuning approach that injects graph topology into\npretrained LLMs without architectural changes. We compute direction-sensitive\npositional encodings from the magnetic Laplacian of transformed AMRs and\nproject them into the embedding space of the LLM. While possibly applicable to\nany graph-structured inputs, we focus on AMR-to-text generation as a\nrepresentative and challenging benchmark. SAFT sets a new state-of-the-art on\nAMR 3.0 with a 3.5 BLEU improvement over baselines. Gains scale with graph\ncomplexity, highlighting the value of structure-aware representations in\nenhancing LLM performance. SAFT offers a general and effective pathway for\nbridging structured data and language models.", "AI": {"tldr": "SAFT is a structure-aware fine-tuning approach that injects graph topology into pretrained LLMs without architectural changes.  SAFT sets a new state-of-the-art on AMR 3.0 with a 3.5 BLEU improvement over baselines.", "motivation": "Large Language Models (LLMs) are increasingly applied to tasks involving structured inputs such as graphs. Abstract Meaning Representations (AMRs), which encode rich semantics as directed graphs, offer a rigorous testbed for evaluating LLMs on text generation from such structures. Yet, current methods often arbitrarily linearize AMRs, discarding key structural cues, or rely on architectures incompatible with standard LLMs.", "method": "a structure-aware fine-tuning approach that injects graph topology into pretrained LLMs without architectural changes. It computes direction-sensitive positional encodings from the magnetic Laplacian of transformed AMRs and project them into the embedding space of the LLM.", "result": "SAFT sets a new state-of-the-art on AMR 3.0 with a 3.5 BLEU improvement over baselines. Gains scale with graph complexity, highlighting the value of structure-aware representations in enhancing LLM performance.", "conclusion": "SAFT offers a general and effective pathway for bridging structured data and language models."}}
{"id": "2507.13550", "categories": ["cs.AI", "cs.CL", "cs.SC"], "pdf": "https://arxiv.org/pdf/2507.13550", "abs": "https://arxiv.org/abs/2507.13550", "authors": ["Eduardo C. Garrido-Merch\u00e1n", "Cristina Puente"], "title": "GOFAI meets Generative AI: Development of Expert Systems by means of Large Language Models", "comment": null, "summary": "The development of large language models (LLMs) has successfully transformed\nknowledge-based systems such as open domain question nswering, which can\nautomatically produce vast amounts of seemingly coherent information. Yet,\nthose models have several disadvantages like hallucinations or confident\ngeneration of incorrect or unverifiable facts. In this paper, we introduce a\nnew approach to the development of expert systems using LLMs in a controlled\nand transparent way. By limiting the domain and employing a well-structured\nprompt-based extraction approach, we produce a symbolic representation of\nknowledge in Prolog, which can be validated and corrected by human experts.\nThis approach also guarantees interpretability, scalability and reliability of\nthe developed expert systems. Via quantitative and qualitative experiments with\nClaude Sonnet 3.7 and GPT-4.1, we show strong adherence to facts and semantic\ncoherence on our generated knowledge bases. We present a transparent hybrid\nsolution that combines the recall capacity of LLMs with the precision of\nsymbolic systems, thereby laying the foundation for dependable AI applications\nin sensitive domains.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u4f7f\u7528 LLM \u4ee5\u53d7\u63a7\u548c\u900f\u660e\u7684\u65b9\u5f0f\u5f00\u53d1\u4e13\u5bb6\u7cfb\u7edf\u7684\u65b0\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u53d1\u5c55\u5df2\u6210\u529f\u8f6c\u53d8\u4e86\u57fa\u4e8e\u77e5\u8bc6\u7684\u7cfb\u7edf\uff0c\u4f8b\u5982\u5f00\u653e\u9886\u57df\u95ee\u9898\u89e3\u7b54\uff0c\u5b83\u53ef\u4ee5\u81ea\u52a8\u751f\u6210\u5927\u91cf\u770b\u4f3c\u8fde\u8d2f\u7684\u4fe1\u606f\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u6709\u51e0\u4e2a\u7f3a\u70b9\uff0c\u4f8b\u5982\u5e7b\u89c9\u6216\u81ea\u4fe1\u5730\u751f\u6210\u4e0d\u6b63\u786e\u6216\u65e0\u6cd5\u9a8c\u8bc1\u7684\u4e8b\u5b9e\u3002", "method": "\u901a\u8fc7\u9650\u5236\u9886\u57df\u548c\u91c7\u7528\u7ed3\u6784\u826f\u597d\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u63d0\u53d6\u65b9\u6cd5\uff0c\u6211\u4eec\u4ee5Prolog\u751f\u6210\u77e5\u8bc6\u7684\u7b26\u53f7\u8868\u793a\u3002", "result": "\u901a\u8fc7\u4f7f\u7528 Claude Sonnet 3.7 \u548c GPT-4.1 \u8fdb\u884c\u7684\u5b9a\u91cf\u548c\u5b9a\u6027\u5b9e\u9a8c\uff0c\u6211\u4eec\u8868\u660e\u6211\u4eec\u751f\u6210\u7684\u77e5\u8bc6\u5e93\u5bf9\u4e8b\u5b9e\u548c\u8bed\u4e49\u8fde\u8d2f\u6027\u6709\u5f88\u5f3a\u7684\u575a\u6301\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u900f\u660e\u7684\u6df7\u5408\u89e3\u51b3\u65b9\u6848\uff0c\u5b83\u7ed3\u5408\u4e86LLM\u7684\u53ec\u56de\u80fd\u529b\u548c\u7b26\u53f7\u7cfb\u7edf\u7684\u7cbe\u786e\u6027\uff0c\u4ece\u800c\u4e3a\u654f\u611f\u9886\u57df\u4e2d\u53ef\u9760\u7684AI\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.13361", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13361", "abs": "https://arxiv.org/abs/2507.13361", "authors": ["Shmuel Berman", "Jia Deng"], "title": "VLMs have Tunnel Vision: Evaluating Nonlocal Visual Reasoning in Leading VLMs", "comment": null, "summary": "Visual Language Models (VLMs) excel at complex visual tasks such as VQA and\nchart understanding, yet recent work suggests they struggle with simple\nperceptual tests. We present an evaluation that tests vision-language models'\ncapacity for nonlocal visual reasoning -- reasoning that requires chaining\nevidence collected from multiple, possibly distant, regions of an image. We\nisolate three distinct forms of non-local vision: comparative perception, which\ndemands holding two images in working memory and comparing them; saccadic\nsearch, which requires making discrete, evidence-driven jumps to locate\nsuccessive targets; and smooth visual search, which involves searching smoothly\nalong a continuous contour. Flagship models (e.g., Gemini 2.5 Pro, Claude\nVision 3.7, GPT-o4-mini), even those that perform well on prior\nprimitive-vision benchmarks, fail these tests and barely exceed random accuracy\non two variants of our tasks that are trivial for humans. Our structured\nevaluation suite allows us to test if VLMs can perform similar visual\nalgorithms to humans. Our findings show that despite gains in raw visual\nacuity, current models lack core visual reasoning capabilities.", "AI": {"tldr": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u975e\u5c40\u90e8\u89c6\u89c9\u63a8\u7406\u65b9\u9762\u5b58\u5728\u7f3a\u9677\uff0c\u5373\u4f7f\u5728\u7b80\u5355\u7684\u4efb\u52a1\u4e2d\u4e5f\u65e0\u6cd5\u8fbe\u5230\u4eba\u7c7b\u6c34\u5e73\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u64c5\u957f\u590d\u6742\u7684\u89c6\u89c9\u4efb\u52a1\uff0c\u4f8b\u5982 VQA \u548c\u56fe\u8868\u7406\u89e3\uff0c\u4f46\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u5b83\u4eec\u5728\u7b80\u5355\u7684\u611f\u77e5\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u9879\u8bc4\u4f30\uff0c\u6d4b\u8bd5\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u975e\u5c40\u90e8\u89c6\u89c9\u63a8\u7406\u7684\u80fd\u529b\uff0c\u8fd9\u79cd\u63a8\u7406\u9700\u8981\u94fe\u63a5\u4ece\u56fe\u50cf\u4e2d\u591a\u4e2a\u53ef\u80fd\u9065\u8fdc\u7684\u533a\u57df\u6536\u96c6\u7684\u8bc1\u636e\u3002\u6211\u4eec\u5206\u79bb\u51fa\u4e09\u79cd\u4e0d\u540c\u5f62\u5f0f\u7684\u975e\u5c40\u90e8\u89c6\u89c9\uff1a\u6bd4\u8f83\u611f\u77e5\uff0c\u626b\u89c6\u641c\u7d22\u548c\u5e73\u6ed1\u89c6\u89c9\u641c\u7d22\u3002", "result": "\u65d7\u8230\u6a21\u578b\uff08\u4f8b\u5982\uff0cGemini 2.5 Pro\u3001Claude Vision 3.7\u3001GPT-o4-mini\uff09\uff0c\u5373\u4f7f\u5728\u4e4b\u524d\u7684\u539f\u59cb\u89c6\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4e5f\u672a\u80fd\u901a\u8fc7\u8fd9\u4e9b\u6d4b\u8bd5\uff0c\u5e76\u4e14\u5728\u6211\u4eec\u4efb\u52a1\u7684\u4e24\u79cd\u53d8\u4f53\u4e2d\u51e0\u4e4e\u6ca1\u6709\u8d85\u8fc7\u968f\u673a\u51c6\u786e\u7387\uff0c\u800c\u8fd9\u4e9b\u53d8\u4f53\u5bf9\u4e8e\u4eba\u7c7b\u6765\u8bf4\u662f\u5fae\u4e0d\u8db3\u9053\u7684\u3002", "conclusion": "\u5c3d\u7ba1\u539f\u59cb\u89c6\u89c9\u654f\u9510\u5ea6\u6709\u6240\u63d0\u9ad8\uff0c\u4f46\u76ee\u524d\u7684\u6a21\u578b\u7f3a\u4e4f\u6838\u5fc3\u7684\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2507.13416", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13416", "abs": "https://arxiv.org/abs/2507.13416", "authors": ["Jiaxiang Yi", "Bernardo P. Ferreira", "Miguel A. Bessa"], "title": "Single- to multi-fidelity history-dependent learning with uncertainty quantification and disentanglement: application to data-driven constitutive modeling", "comment": "40 pages, 32 figures", "summary": "Data-driven learning is generalized to consider history-dependent\nmulti-fidelity data, while quantifying epistemic uncertainty and disentangling\nit from data noise (aleatoric uncertainty). This generalization is hierarchical\nand adapts to different learning scenarios: from training the simplest\nsingle-fidelity deterministic neural networks up to the proposed multi-fidelity\nvariance estimation Bayesian recurrent neural networks. The versatility and\ngenerality of the proposed methodology are demonstrated by applying it to\ndifferent data-driven constitutive modeling scenarios that include multiple\nfidelities with and without aleatoric uncertainty (noise). The method\naccurately predicts the response and quantifies model error while also\ndiscovering the noise distribution (when present). This opens opportunities for\nfuture real-world applications in diverse scientific and engineering domains;\nespecially, the most challenging cases involving design and analysis under\nuncertainty.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5e7f\u4e49\u7684\u5c42\u6b21\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u591a\u4fdd\u771f\u5ea6\u6570\u636e\uff0c\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u9002\u5e94\u4e0d\u540c\u7684\u5b66\u4e60\u573a\u666f\uff0c\u5e76\u901a\u8fc7\u5e94\u7528\u4e8e\u4e0d\u540c\u7684\u672c\u6784\u5efa\u6a21\u573a\u666f\u6765\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u6570\u636e\u9a71\u52a8\u7684\u5b66\u4e60\u88ab\u63a8\u5e7f\u5230\u8003\u8651\u5386\u53f2\u76f8\u5173\u7684\u591a\u4fdd\u771f\u5ea6\u6570\u636e\uff0c\u540c\u65f6\u91cf\u5316\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u5e76\u5c06\u5176\u4e0e\u6570\u636e\u566a\u58f0\uff08\u5076\u7136\u4e0d\u786e\u5b9a\u6027\uff09\u5206\u79bb\u3002", "method": "\u5206\u5c42\u6982\u62ec\u5e76\u9002\u5e94\u4e0d\u540c\u7684\u5b66\u4e60\u573a\u666f\uff1a\u4ece\u8bad\u7ec3\u6700\u7b80\u5355\u7684\u5355\u4fdd\u771f\u5ea6\u786e\u5b9a\u6027\u795e\u7ecf\u7f51\u7edc\u5230\u63d0\u51fa\u7684\u591a\u4fdd\u771f\u5ea6\u65b9\u5dee\u4f30\u8ba1\u8d1d\u53f6\u65af\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u3002", "result": "\u901a\u8fc7\u5c06\u5176\u5e94\u7528\u4e8e\u4e0d\u540c\u7684\u6570\u636e\u9a71\u52a8\u7684\u672c\u6784\u5efa\u6a21\u573a\u666f\u6765\u8bc1\u660e\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u591a\u529f\u80fd\u6027\u548c\u901a\u7528\u6027\uff0c\u8fd9\u4e9b\u573a\u666f\u5305\u62ec\u5177\u6709\u548c\u4e0d\u5177\u6709\u5076\u7136\u4e0d\u786e\u5b9a\u6027\uff08\u566a\u58f0\uff09\u7684\u591a\u4e2a\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u51c6\u786e\u5730\u9884\u6d4b\u4e86\u54cd\u5e94\u5e76\u91cf\u5316\u4e86\u6a21\u578b\u8bef\u5dee\uff0c\u540c\u65f6\u8fd8\u53d1\u73b0\u4e86\u566a\u58f0\u5206\u5e03\uff08\u5982\u679c\u5b58\u5728\uff09\u3002\u8fd9\u4e3a\u672a\u6765\u5728\u4e0d\u540c\u79d1\u5b66\u548c\u5de5\u7a0b\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u5f00\u8f9f\u4e86\u673a\u4f1a\uff1b\u7279\u522b\u662f\uff0c\u6d89\u53ca\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u8bbe\u8ba1\u548c\u5206\u6790\u7684\u6700\u5177\u6311\u6218\u6027\u7684\u60c5\u51b5\u3002"}}
{"id": "2507.14101", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.14101", "abs": "https://arxiv.org/abs/2507.14101", "authors": ["Diego Figueira", "Cibele Freire"], "title": "Project-connex Decompositions and Tractability of Aggregate Group-by Conjunctive Queries", "comment": "34 pages, 5 figures", "summary": "We introduce 'project-connex' tree-width as a measure of tractability for\ncounting and aggregate conjunctive queries over semirings with 'group-by'\nprojection (also known as 'AJAR' or 'FAQ' queries). This elementary measure\nallows to obtain comparable complexity bounds to the ones obtained by previous\nstructural conditions tailored for efficient evaluation of semiring aggregate\nqueries, enumeration algorithms of conjunctive queries, and tractability of\ncounting answers to conjunctive queries.\n  Project-connex tree decompositions are defined as the natural extension of\nthe known notion of 'free-connex' decompositions. They allow for a unified,\nsimple and intuitive algorithmic manipulation for evaluation of aggregate\nqueries and explain some existing tractability results on conjunctive query\nenumeration, counting conjunctive query evaluation, and evaluation of semiring\naggregate queries. Using this measure we also recover results relating\ntractable classes of counting conjunctive queries and bounded free-connex\ntree-width, or the constant-time delay enumeration of semiring aggregate\nqueries over bounded project-connex classes. We further show that\nproject-connex tree decompositions can be obtained via algorithms for computing\nclassical tree decompositions.", "AI": {"tldr": "Introduces project-connex tree-width as a measure of tractability for conjunctive queries with 'group-by' projection, unifying algorithmic manipulation and explaining existing tractability results.", "motivation": "To introduce a measure of tractability for counting and aggregate conjunctive queries over semirings with 'group-by' projection, allowing comparable complexity bounds to previous structural conditions.", "method": "Introduces 'project-connex' tree-width, defines project-connex tree decompositions as an extension of 'free-connex' decompositions, and uses algorithms for computing classical tree decompositions.", "result": "Obtains complexity bounds comparable to previous structural conditions, explains existing tractability results, recovers results relating tractable classes of counting conjunctive queries and bounded free-connex tree-width, and shows project-connex tree decompositions can be obtained via algorithms for computing classical tree decompositions.", "conclusion": "Project-connex tree decompositions provide a unified algorithmic approach for aggregate query evaluation and explain existing tractability results.  The measure recovers results relating tractable counting conjunctive queries and bounded free-connex tree-width, or constant-time delay enumeration of semiring aggregate queries over bounded project-connex classes. Project-connex tree decompositions can be obtained via algorithms for computing classical tree decompositions."}}
{"id": "2507.13822", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13822", "abs": "https://arxiv.org/abs/2507.13822", "authors": ["Shad Nygren", "Pinar Avci", "Andre Daniels", "Reza Rassol", "Afshin Beheshti", "Diego Galeano"], "title": "RAG-based Architectures for Drug Side Effect Retrieval in LLMs", "comment": null, "summary": "Drug side effects are a major global health concern, necessitating advanced\nmethods for their accurate detection and analysis. While Large Language Models\n(LLMs) offer promising conversational interfaces, their inherent limitations,\nincluding reliance on black-box training data, susceptibility to\nhallucinations, and lack of domain-specific knowledge, hinder their reliability\nin specialized fields like pharmacovigilance. To address this gap, we propose\ntwo architectures: Retrieval-Augmented Generation (RAG) and GraphRAG, which\nintegrate comprehensive drug side effect knowledge into a Llama 3 8B language\nmodel. Through extensive evaluations on 19,520 drug side effect associations\n(covering 976 drugs and 3,851 side effect terms), our results demonstrate that\nGraphRAG achieves near-perfect accuracy in drug side effect retrieval. This\nframework offers a highly accurate and scalable solution, signifying a\nsignificant advancement in leveraging LLMs for critical pharmacovigilance\napplications.", "AI": {"tldr": "The paper introduces GraphRAG, a novel method that integrates drug side effect knowledge into a Llama 3 8B language model, achieving near-perfect accuracy in drug side effect retrieval,signifying a significant advancement in leveraging LLMs for critical pharmacovigilance applications.", "motivation": "LLMs have limitations in specialized fields like pharmacovigilance due to reliance on black-box training data, susceptibility to hallucinations, and lack of domain-specific knowledge.", "method": "Retrieval-Augmented Generation (RAG) and GraphRAG, which integrate comprehensive drug side effect knowledge into a Llama 3 8B language model.", "result": "GraphRAG achieves near-perfect accuracy in drug side effect retrieval on 19,520 drug side effect associations (covering 976 drugs and 3,851 side effect terms).", "conclusion": "GraphRAG achieves near-perfect accuracy in drug side effect retrieval, offering a highly accurate and scalable solution for critical pharmacovigilance applications."}}
{"id": "2507.13382", "categories": ["cs.CL", "cs.LG", "05-05C12"], "pdf": "https://arxiv.org/pdf/2507.13382", "abs": "https://arxiv.org/abs/2507.13382", "authors": ["Chandrashekar Muniyappa", "Sirisha Velampalli"], "title": "Context-Based Fake News Detection using Graph Based Approach: ACOVID-19 Use-case", "comment": "CSAIDE '25: Proceedings of the 2025 4th International Conference on\n  Cyber Security, Artificial Intelligence and the Digital Economy", "summary": "In today\\'s digital world, fake news is spreading with immense speed. Its a\nsignificant concern to address. In this work, we addressed that challenge using\nnovel graph based approach. We took dataset from Kaggle that contains real and\nfake news articles. To test our approach we incorporated recent covid-19\nrelated news articles that contains both genuine and fake news that are\nrelevant to this problem. This further enhances the dataset as well instead of\nrelying completely on the original dataset. We propose a contextual graph-based\napproach to detect fake news articles. We need to convert news articles into\nappropriate schema, so we leverage Natural Language Processing (NLP) techniques\nto transform news articles into contextual graph structures. We then apply the\nMinimum Description Length (MDL)-based Graph-Based Anomaly Detection (GBAD)\nalgorithm for graph mining. Graph-based methods are particularly effective for\nhandling rich contextual data, as they enable the discovery of complex patterns\nthat traditional query-based or statistical techniques might overlook. Our\nproposed approach identifies normative patterns within the dataset and\nsubsequently uncovers anomalous patterns that deviate from these established\nnorms.", "AI": {"tldr": "This paper presents a contextual graph-based approach to detect fake news by transforming news articles into graph structures and applying anomaly detection.", "motivation": "The motivation is to address the rapid spread of fake news in the digital world.", "method": "The paper converts news articles into contextual graph structures using NLP techniques and applies the Minimum Description Length (MDL)-based Graph-Based Anomaly Detection (GBAD) algorithm for graph mining.", "result": "The proposed approach identifies normative patterns and uncovers anomalous patterns to detect fake news.", "conclusion": "This paper uses a contextual graph-based approach combined with NLP and MDL-based GBAD to detect fake news, identifying anomalous patterns that deviate from established norms."}}
{"id": "2507.13558", "categories": ["cs.AI", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13558", "abs": "https://arxiv.org/abs/2507.13558", "authors": ["David Poole"], "title": "Why Isn't Relational Learning Taking Over the World?", "comment": "10 pages (6 pages + references + appendices)", "summary": "AI seems to be taking over the world with systems that model pixels, words,\nand phonemes. The world is arguably made up, not of pixels, words, and phonemes\nbut of entities (objects, things, including events) with properties and\nrelations among them. Surely we should model these, not the perception or\ndescription of them. You might suspect that concentrating on modeling words and\npixels is because all of the (valuable) data in the world is in terms of text\nand images. If you look into almost any company you will find their most\nvaluable data is in spreadsheets, databases and other relational formats. These\nare not the form that are studied in introductory machine learning, but are\nfull of product numbers, student numbers, transaction numbers and other\nidentifiers that can't be interpreted naively as numbers. The field that\nstudies this sort of data has various names including relational learning,\nstatistical relational AI, and many others. This paper explains why relational\nlearning is not taking over the world -- except in a few cases with restricted\nrelations -- and what needs to be done to bring it to it's rightful prominence.", "AI": {"tldr": "This paper discusses why relational learning has not become more prominent despite the prevalence of relational data.", "motivation": "The world is arguably made up, not of pixels, words, and phonemes but of entities (objects, things, including events) with properties and relations among them. Surely we should model these, not the perception or description of them.", "method": "relational learning, statistical relational AI", "result": "relational learning is not taking over the world -- except in a few cases with restricted relations", "conclusion": "This paper explains why relational learning is not taking over the world -- except in a few cases with restricted relations -- and what needs to be done to bring it to it's rightful prominence."}}
{"id": "2507.13362", "categories": ["cs.CV", "cs.AI", "cs.CL", "I.2.10; I.4.8; I.2.6; I.2.7; I.5.4; I.5.1"], "pdf": "https://arxiv.org/pdf/2507.13362", "abs": "https://arxiv.org/abs/2507.13362", "authors": ["Binbin Ji", "Siddharth Agrawal", "Qiance Tang", "Yvonne Wu"], "title": "Enhancing Spatial Reasoning in Vision-Language Models via Chain-of-Thought Prompting and Reinforcement Learning", "comment": "10 pages, 5 figures, submitted to a conference (IEEE formate).\n  Authored by students from the Courant Institute, NYU", "summary": "This study investigates the spatial reasoning capabilities of vision-language\nmodels (VLMs) through Chain-of-Thought (CoT) prompting and reinforcement\nlearning. We begin by evaluating the impact of different prompting strategies\nand find that simple CoT formats, where the model generates a reasoning step\nbefore the answer, not only fail to help, but can even harm the model's\noriginal performance. In contrast, structured multi-stage prompting based on\nscene graphs (SceneGraph CoT) significantly improves spatial reasoning\naccuracy. Furthermore, to improve spatial reasoning ability, we fine-tune\nmodels using Group Relative Policy Optimization (GRPO) on the SAT dataset and\nevaluate their performance on CVBench. Compared to supervised fine-tuning\n(SFT), GRPO achieves higher accuracy on Pass@1 evaluations and demonstrates\nsuperior robustness under out-of-distribution (OOD) conditions. In particular,\nwe find that SFT overfits to surface-level linguistic patterns and may degrade\nperformance when test-time phrasing changes (e.g., from \"closer to\" to \"farther\nfrom\"). GRPO, on the other hand, generalizes more reliably and maintains stable\nperformance under such shifts. Our findings provide insights into how\nreinforcement learning and structured prompting improve the spatial reasoning\ncapabilities and generalization behavior of modern VLMs. All code is open\nsource at: https://github.com/Yvonne511/spatial-vlm-investigator", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u7ed3\u6784\u5316\u63d0\u793a\u6765\u63d0\u9ad8\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u601d\u7ef4\u94fe (CoT) \u63d0\u793a\u548c\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002\u4f7f\u7528 Group Relative Policy Optimization (GRPO) \u5728 SAT \u6570\u636e\u96c6\u4e0a\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u5728 CVBench \u4e0a\u8bc4\u4f30\u5b83\u4eec\u7684\u6027\u80fd\u3002", "result": "\u7b80\u5355\u7684 CoT \u683c\u5f0f\u65e0\u6cd5\u63d0\u4f9b\u5e2e\u52a9\uff0c\u751a\u81f3\u4f1a\u635f\u5bb3\u6a21\u578b\u539f\u6709\u7684\u6027\u80fd\u3002\u57fa\u4e8e\u573a\u666f\u56fe\u7684\u7ed3\u6784\u5316\u591a\u9636\u6bb5\u63d0\u793a (SceneGraph CoT) \u663e\u7740\u63d0\u9ad8\u4e86\u7a7a\u95f4\u63a8\u7406\u7684\u51c6\u786e\u6027\u3002GRPO \u5728 Pass@1 \u8bc4\u4f30\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\uff0c\u5e76\u4e14\u5728\u5206\u5e03\u5916 (OOD) \u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u9c81\u68d2\u6027\u3002SFT \u8fc7\u5ea6\u62df\u5408\u5230\u8868\u9762\u8bed\u8a00\u6a21\u5f0f\uff0c\u5e76\u4e14\u5f53\u6d4b\u8bd5\u65f6\u63aa\u8f9e\u53d1\u751f\u53d8\u5316\u65f6\u53ef\u80fd\u4f1a\u964d\u4f4e\u6027\u80fd\u3002GRPO \u53ef\u4ee5\u66f4\u53ef\u9760\u5730\u6cdb\u5316\uff0c\u5e76\u5728\u8fd9\u79cd\u53d8\u5316\u4e0b\u4fdd\u6301\u7a33\u5b9a\u7684\u6027\u80fd\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u548c\u7ed3\u6784\u5316\u63d0\u793a\u53ef\u4ee5\u63d0\u9ad8\u73b0\u4ee3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u548c\u6cdb\u5316\u884c\u4e3a\u3002"}}
{"id": "2507.13417", "categories": ["cs.LG", "cs.AI", "cs.DM"], "pdf": "https://arxiv.org/pdf/2507.13417", "abs": "https://arxiv.org/abs/2507.13417", "authors": ["Armel Soubeiga", "Thomas Guyet", "Violaine Antoine"], "title": "Soft-ECM: An extension of Evidential C-Means for complex data", "comment": null, "summary": "Clustering based on belief functions has been gaining increasing attention in\nthe machine learning community due to its ability to effectively represent\nuncertainty and/or imprecision. However, none of the existing algorithms can be\napplied to complex data, such as mixed data (numerical and categorical) or\nnon-tabular data like time series. Indeed, these types of data are, in general,\nnot represented in a Euclidean space and the aforementioned algorithms make use\nof the properties of such spaces, in particular for the construction of\nbarycenters. In this paper, we reformulate the Evidential C-Means (ECM) problem\nfor clustering complex data. We propose a new algorithm, Soft-ECM, which\nconsistently positions the centroids of imprecise clusters requiring only a\nsemi-metric. Our experiments show that Soft-ECM present results comparable to\nconventional fuzzy clustering approaches on numerical data, and we demonstrate\nits ability to handle mixed data and its benefits when combining fuzzy\nclustering with semi-metrics such as DTW for time series data.", "AI": {"tldr": "This paper introduces Soft-ECM, a new evidential clustering algorithm that can handle complex data like mixed data and time series by using a semi-metric, overcoming the limitations of existing methods.", "motivation": "Existing belief function-based clustering algorithms cannot be applied to complex data like mixed or non-tabular data because these data types are not represented in Euclidean space, which these algorithms rely on.", "method": "Reformulation of the Evidential C-Means (ECM) problem and a new algorithm, Soft-ECM, which positions centroids of imprecise clusters using a semi-metric.", "result": "Soft-ECM presents results comparable to conventional fuzzy clustering approaches on numerical data and demonstrates its ability to handle mixed data and benefits when combining fuzzy clustering with semi-metrics such as DTW for time series data.", "conclusion": "Soft-ECM achieves comparable results to fuzzy clustering on numerical data, handles mixed data, and benefits from combining fuzzy clustering with semi-metrics like DTW for time series data."}}
{"id": "2507.13859", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13859", "abs": "https://arxiv.org/abs/2507.13859", "authors": ["Aleksandr Gashkov", "Aleksandr Perevalov", "Maria Eltsova", "Andreas Both"], "title": "SPARQL Query Generation with LLMs: Measuring the Impact of Training Data Memorization and Knowledge Injection", "comment": "Winner of Best Paper Award at the 25th International Conference on\n  Web Engineering (ICWE 2025)", "summary": "Nowadays, the importance of software with natural-language user interfaces\ncannot be underestimated. In particular, in Question Answering (QA) systems,\ngenerating a SPARQL query for a given natural-language question (often named\nQuery Building) from the information retrieved from the same question is the\ncentral task of QA systems working over Knowledge Graphs (KGQA). Due to the\nrise of Large Language Models (LLMs), they are considered a well-suited method\nto increase the quality of the question-answering functionality, as there is\nstill a lot of room for improvement, aiming for enhanced quality and\ntrustworthiness. However, LLMs are trained on web data, where researchers have\nno control over whether the benchmark or the knowledge graph was already\nincluded in the training data. In this paper, we introduce a novel method that\nevaluates the quality of LLMs by generating a SPARQL query from a\nnatural-language question under various conditions: (1) zero-shot SPARQL\ngeneration, (2) with knowledge injection, and (3) with \"anonymized\" knowledge\ninjection. This enables us, for the first time, to estimate the influence of\nthe training data on the QA quality improved by LLMs. Ultimately, this will\nhelp to identify how portable a method is or whether good results might mostly\nbe achieved because a benchmark was already included in the training data (cf.\nLLM memorization). The developed method is portable, robust, and supports any\nknowledge graph; therefore, it could be easily applied to any KGQA or LLM,\ns.t., generating consistent insights into the actual LLM capabilities is\npossible.", "AI": {"tldr": "This paper presents a new method to test how well LLMs generate SPARQL queries, checking if they're actually smart or just memorizing training data.", "motivation": "The motivation is to address the lack of control over training data in LLMs and its potential impact on QA quality, especially concerning benchmark memorization.", "method": "The paper evaluates LLMs under three conditions: zero-shot SPARQL generation, with knowledge injection, and with anonymized knowledge injection.", "result": "The method enables estimation of the influence of training data on QA quality and identification of method portability.", "conclusion": "The paper introduces a portable and robust method to evaluate the quality of LLMs in generating SPARQL queries from natural language questions, applicable to any KGQA or LLM."}}
{"id": "2507.13390", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13390", "abs": "https://arxiv.org/abs/2507.13390", "authors": ["Kundeshwar Pundalik", "Piyush Sawarkar", "Nihar Sahoo", "Abhishek Shinde", "Prateek Chanda", "Vedant Goswami", "Ajay Nagpal", "Atul Singh", "Viraj Thakur", "Vijay Dewane", "Aamod Thakur", "Bhargav Patel", "Smita Gautam", "Bhagwan Panditi", "Shyam Pawar", "Madhav Kotcha", "Suraj Racha", "Saral Sureka", "Pankaj Singh", "Rishi Bal", "Rohit Saluja", "Ganesh Ramakrishnan"], "title": "PARAM-1 BharatGen 2.9B Model", "comment": null, "summary": "Large Language Models (LLMs) have emerged as powerful general-purpose\nreasoning systems, yet their development remains dominated by English-centric\ndata, architectures, and optimization paradigms. This exclusionary design\nresults in structural under-representation of linguistically diverse regions\nsuch as India, where over 20 official languages and 100+ dialects coexist\nalongside phenomena like code-switching and diglossia. We introduce PARAM-1, a\n2.9B parameter decoder-only, text-only language model trained from scratch with\nan explicit architectural and linguistic focus on Indian diversity. PARAM-1 is\ntrained on a bilingual dataset consisting of only Hindi and English,\nconstructed with a strong focus on fact-rich, high-quality content. It is\nguided by three core principles: equitable representation of Indic languages\nthrough a 25% corpus allocation; tokenization fairness via a SentencePiece\ntokenizer adapted to Indian morphological structures; and culturally aligned\nevaluation benchmarks across IndicQA, code-mixed reasoning, and\nsocio-linguistic robustness tasks. By embedding diversity at the pretraining\nlevel-rather than deferring it to post-hoc alignment-PARAM-1 offers a\ndesign-first blueprint for equitable foundation modeling. Our results\ndemonstrate that it serves as both a competent general-purpose model and a\nrobust baseline for India-centric applications.", "AI": {"tldr": "PARAM-1\u662f\u4e00\u4e2a29\u4ebf\u53c2\u6570\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u5b83\u4ece\u5934\u5f00\u59cb\u8fdb\u884c\u8bad\u7ec3\uff0c\u7279\u522b\u5173\u6ce8\u5370\u5ea6\u7684\u8bed\u8a00\u591a\u6837\u6027\uff0c\u5e76\u5728\u5370\u5730\u8bed\u548c\u82f1\u8bed\u53cc\u8bed\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5df2\u7ecf\u6210\u4e3a\u5f3a\u5927\u7684\u901a\u7528\u63a8\u7406\u7cfb\u7edf\uff0c\u4f46\u5176\u53d1\u5c55\u4ecd\u7136\u4ee5\u82f1\u8bed\u4e3a\u4e2d\u5fc3\u7684\u6570\u636e\u3001\u67b6\u6784\u548c\u4f18\u5316\u8303\u5f0f\u4e3a\u4e3b\u3002\u8fd9\u79cd\u6392\u4ed6\u6027\u7684\u8bbe\u8ba1\u5bfc\u81f4\u4e86\u5728\u8bed\u8a00\u591a\u6837\u7684\u5730\u533a\uff08\u5982\u5370\u5ea6\uff09\u7684\u7ed3\u6784\u6027\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u5370\u5ea6\u6709\u8d85\u8fc720\u79cd\u5b98\u65b9\u8bed\u8a00\u548c100\u591a\u79cd\u65b9\u8a00\uff0c\u4ee5\u53ca\u8bf8\u5982\u4ee3\u7801\u8f6c\u6362\u548c\u53cc\u8bed\u73b0\u8c61\u3002", "method": "\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u4e86\u4e00\u4e2a29\u4ebf\u53c2\u6570\u7684\u4ec5\u89e3\u7801\u5668\u3001\u4ec5\u6587\u672c\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5728\u67b6\u6784\u548c\u8bed\u8a00\u4e0a\u90fd\u660e\u786e\u5173\u6ce8\u5370\u5ea6\u7684\u591a\u6837\u6027\u3002\u4f7f\u7528\u4e00\u79cd\u660e\u786e\u5173\u6ce8\u5bcc\u542b\u4e8b\u5b9e\u3001\u9ad8\u8d28\u91cf\u5185\u5bb9\u7684\u53cc\u8bed\u6570\u636e\u96c6\uff08\u4ec5\u5305\u542b\u5370\u5730\u8bed\u548c\u82f1\u8bed\uff09\u8fdb\u884c\u8bad\u7ec3\u3002\u91c7\u7528SentencePiece\u5206\u8bcd\u5668", "result": "PARAM-1\u5728\u9884\u8bad\u7ec3\u5c42\u9762\u5d4c\u5165\u4e86\u591a\u6837\u6027\uff0c\u800c\u4e0d\u662f\u63a8\u8fdf\u5230\u4e8b\u540e\u8c03\u6574\uff0c\u8fd9\u4e3a\u516c\u5e73\u7684\u57fa\u7840\u5efa\u6a21\u63d0\u4f9b\u4e86design-first\u7684\u84dd\u56fe\u3002\u7ed3\u679c\u8868\u660e\uff0cPARAM-1\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u5408\u683c\u7684\u901a\u7528\u6a21\u578b\uff0c\u4e5f\u53ef\u4ee5\u4f5c\u4e3a\u4ee5\u5370\u5ea6\u4e3a\u4e2d\u5fc3\u7684\u5e94\u7528\u7684\u5f3a\u5927\u57fa\u7ebf\u3002", "conclusion": "PARAM-1\u4f5c\u4e3a\u4e00\u4e2a\u901a\u7528\u7684\u6a21\u578b\u548c\u5bf9\u4ee5\u5370\u5ea6\u4e3a\u4e2d\u5fc3\u7684\u5e94\u7528\u7684\u53ef\u9760\u7684\u57fa\u7ebf\u3002"}}
{"id": "2507.13625", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13625", "abs": "https://arxiv.org/abs/2507.13625", "authors": ["Yuxin Zhang", "Xi Wang", "Mo Hu", "Zhenyu Zhang"], "title": "BifrostRAG: Bridging Dual Knowledge Graphs for Multi-Hop Question Answering in Construction Safety", "comment": "19 pages, 13 figures", "summary": "Information retrieval and question answering from safety regulations are\nessential for automated construction compliance checking but are hindered by\nthe linguistic and structural complexity of regulatory text. Many\ncompliance-related queries are multi-hop, requiring synthesis of information\nacross interlinked clauses. This poses a challenge for traditional\nretrieval-augmented generation (RAG) systems. To overcome this, we introduce\nBifrostRAG: a dual-graph RAG-integrated system that explicitly models both\nlinguistic relationships (via an Entity Network Graph) and document structure\n(via a Document Navigator Graph). This architecture powers a hybrid retrieval\nmechanism that combines graph traversal with vector-based semantic search,\nenabling large language models to reason over both the meaning and the\nstructure of the text. Evaluation on a multi-hop question dataset shows that\nBifrostRAG achieves 92.8 percent precision, 85.5 percent recall, and an F1\nscore of 87.3 percent. These results significantly outperform vector-only and\ngraph-only RAG baselines that represent current leading approaches. Error\nanalysis further highlights the comparative advantages of our hybrid method\nover single-modality RAGs. These findings establish BifrostRAG as a robust\nknowledge engine for LLM-driven compliance checking. Its dual-graph, hybrid\nretrieval mechanism offers a transferable blueprint for navigating complex\ntechnical documents across knowledge-intensive engineering domains.", "AI": {"tldr": "BifrostRAG, a dual-graph RAG system, outperforms existing methods in retrieving information from complex regulatory texts for compliance checking by combining linguistic and structural analysis.", "motivation": "Information retrieval and question answering from safety regulations are essential for automated construction compliance checking but are hindered by the linguistic and structural complexity of regulatory text. Many compliance-related queries are multi-hop, requiring synthesis of information across interlinked clauses. This poses a challenge for traditional retrieval-augmented generation (RAG) systems.", "method": "a dual-graph RAG-integrated system that explicitly models both linguistic relationships and document structure, powering a hybrid retrieval mechanism that combines graph traversal with vector-based semantic search", "result": "BifrostRAG achieves 92.8 percent precision, 85.5 percent recall, and an F1 score of 87.3 percent. These results significantly outperform vector-only and graph-only RAG baselines.", "conclusion": "BifrostRAG is a robust knowledge engine for LLM-driven compliance checking, offering a transferable blueprint for navigating complex technical documents."}}
{"id": "2507.13363", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13363", "abs": "https://arxiv.org/abs/2507.13363", "authors": ["Atharv Goel", "Mehar Khurana"], "title": "Just Add Geometry: Gradient-Free Open-Vocabulary 3D Detection Without Human-in-the-Loop", "comment": null, "summary": "Modern 3D object detection datasets are constrained by narrow class\ntaxonomies and costly manual annotations, limiting their ability to scale to\nopen-world settings. In contrast, 2D vision-language models trained on\nweb-scale image-text pairs exhibit rich semantic understanding and support\nopen-vocabulary detection via natural language prompts. In this work, we\nleverage the maturity and category diversity of 2D foundation models to perform\nopen-vocabulary 3D object detection without any human-annotated 3D labels.\n  Our pipeline uses a 2D vision-language detector to generate text-conditioned\nproposals, which are segmented with SAM and back-projected into 3D using camera\ngeometry and either LiDAR or monocular pseudo-depth. We introduce a geometric\ninflation strategy based on DBSCAN clustering and Rotating Calipers to infer 3D\nbounding boxes without training. To simulate adverse real-world conditions, we\nconstruct Pseudo-nuScenes, a fog-augmented, RGB-only variant of the nuScenes\ndataset.\n  Experiments demonstrate that our method achieves competitive localization\nperformance across multiple settings, including LiDAR-based and purely RGB-D\ninputs, all while remaining training-free and open-vocabulary. Our results\nhighlight the untapped potential of 2D foundation models for scalable 3D\nperception. We open-source our code and resources at\nhttps://github.com/atharv0goel/open-world-3D-det.", "AI": {"tldr": "This paper introduces a training-free and open-vocabulary 3D object detection method that leverages 2D foundation models. It achieves competitive localization performance using a pipeline that combines a 2D vision-language detector, SAM segmentation, and geometric inflation strategies.", "motivation": "Modern 3D object detection datasets are constrained by narrow class taxonomies and costly manual annotations, limiting their ability to scale to open-world settings. In contrast, 2D vision-language models trained on web-scale image-text pairs exhibit rich semantic understanding and support open-vocabulary detection via natural language prompts. This work leverages the maturity and category diversity of 2D foundation models to perform open-vocabulary 3D object detection without any human-annotated 3D labels.", "method": "This pipeline uses a 2D vision-language detector to generate text-conditioned proposals, which are segmented with SAM and back-projected into 3D using camera geometry and either LiDAR or monocular pseudo-depth. A geometric inflation strategy based on DBSCAN clustering and Rotating Calipers is introduced to infer 3D bounding boxes without training.", "result": "The method achieves competitive localization performance across multiple settings, including LiDAR-based and purely RGB-D inputs, all while remaining training-free and open-vocabulary.", "conclusion": "This method achieves competitive localization performance across multiple settings, including LiDAR-based and purely RGB-D inputs, all while remaining training-free and open-vocabulary. The results highlight the untapped potential of 2D foundation models for scalable 3D perception."}}
{"id": "2507.13423", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13423", "abs": "https://arxiv.org/abs/2507.13423", "authors": ["Edward Henderson", "Dewi Gould", "Richard Everson", "George De Ath", "Nick Pepper"], "title": "Air Traffic Controller Task Demand via Graph Neural Networks: An Interpretable Approach to Airspace Complexity", "comment": "Author Accepted Manuscript version of paper at the AIAA AVIATION\n  Forum 2025", "summary": "Real-time assessment of near-term Air Traffic Controller (ATCO) task demand\nis a critical challenge in an increasingly crowded airspace, as existing\ncomplexity metrics often fail to capture nuanced operational drivers beyond\nsimple aircraft counts. This work introduces an interpretable Graph Neural\nNetwork (GNN) framework to address this gap. Our attention-based model predicts\nthe number of upcoming clearances, the instructions issued to aircraft by\nATCOs, from interactions within static traffic scenarios. Crucially, we derive\nan interpretable, per-aircraft task demand score by systematically ablating\naircraft and measuring the impact on the model's predictions. Our framework\nsignificantly outperforms an ATCO-inspired heuristic and is a more reliable\nestimator of scenario complexity than established baselines. The resulting tool\ncan attribute task demand to specific aircraft, offering a new way to analyse\nand understand the drivers of complexity for applications in controller\ntraining and airspace redesign.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u56fe\u795e\u7ecf\u7f51\u7edc (GNN) \u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u5373\u5c06\u5230\u6765\u7684\u8bb8\u53ef\u6570\u91cf\u3002", "motivation": "\u7531\u4e8e\u73b0\u6709\u7684\u590d\u6742\u6027\u6307\u6807\u901a\u5e38\u65e0\u6cd5\u6355\u6349\u5230\u9664\u7b80\u5355\u98de\u673a\u8ba1\u6570\u4e4b\u5916\u7684\u7ec6\u5fae\u64cd\u4f5c\u9a71\u52a8\u56e0\u7d20\uff0c\u56e0\u6b64\u5bf9\u8fd1\u671f\u7a7a\u4e2d\u4ea4\u901a\u7ba1\u5236\u5458 (ATCO) \u4efb\u52a1\u9700\u6c42\u8fdb\u884c\u5b9e\u65f6\u8bc4\u4f30\u662f\u65e5\u76ca\u62e5\u6324\u7684\u7a7a\u57df\u4e2d\u7684\u4e00\u9879\u5173\u952e\u6311\u6218\u3002", "method": "\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6a21\u578b\u9884\u6d4b\u5373\u5c06\u5230\u6765\u7684\u8bb8\u53ef\u6570\u91cf\uff0c\u5373\u7a7a\u4e2d\u4ea4\u901a\u7ba1\u5236\u5458\u5411\u98de\u673a\u53d1\u51fa\u7684\u6307\u4ee4\uff0c\u6765\u81ea\u9759\u6001\u4ea4\u901a\u573a\u666f\u4e2d\u7684\u4ea4\u4e92\u3002", "result": "\u8be5\u6846\u67b6\u663e\u7740\u4f18\u4e8e ATCO \u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5e76\u4e14\u662f\u6bd4\u5df2\u5efa\u7acb\u7684\u57fa\u7ebf\u66f4\u53ef\u9760\u7684\u573a\u666f\u590d\u6742\u6027\u4f30\u8ba1\u5668\u3002", "conclusion": "\u8be5\u5de5\u5177\u53ef\u4ee5\u5c06\u4efb\u52a1\u9700\u6c42\u5f52\u56e0\u4e8e\u7279\u5b9a\u98de\u673a\uff0c\u4e3a\u5206\u6790\u548c\u7406\u89e3\u63a7\u5236\u5668\u57f9\u8bad\u548c\u7a7a\u57df\u91cd\u65b0\u8bbe\u8ba1\u5e94\u7528\u7684\u590d\u6742\u6027\u9a71\u52a8\u56e0\u7d20\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2507.13910", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.13910", "abs": "https://arxiv.org/abs/2507.13910", "authors": ["Pranav Kasela", "Gabriella Pasi", "Raffaele Perego"], "title": "PARK: Personalized academic retrieval with knowledge-graphs", "comment": "Accepted in Information Systems. [17 May 2025]\n  https://doi.org/10.1016/j.is.2025.102574", "summary": "Academic Search is a search task aimed to manage and retrieve scientific\ndocuments like journal articles and conference papers. Personalization in this\ncontext meets individual researchers' needs by leveraging, through user\nprofiles, the user related information (e.g. documents authored by a\nresearcher), to improve search effectiveness and to reduce the information\noverload. While citation graphs are a valuable means to support the outcome of\nrecommender systems, their use in personalized academic search (with, e.g.\nnodes as papers and edges as citations) is still under-explored.\n  Existing personalized models for academic search often struggle to fully\ncapture users' academic interests. To address this, we propose a two-step\napproach: first, training a neural language model for retrieval, then\nconverting the academic graph into a knowledge graph and embedding it into a\nshared semantic space with the language model using translational embedding\ntechniques. This allows user models to capture both explicit relationships and\nhidden structures in citation graphs and paper content. We evaluate our\napproach in four academic search domains, outperforming traditional graph-based\nand personalized models in three out of four, with up to a 10\\% improvement in\nMAP@100 over the second-best model. This highlights the potential of knowledge\ngraph-based user models to enhance retrieval effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u5b66\u672f\u641c\u7d22\u4e2a\u6027\u5316\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u5b66\u672f\u641c\u7d22\u4e2a\u6027\u5316\u6a21\u578b\u901a\u5e38\u96be\u4ee5\u5b8c\u5168\u6355\u6349\u7528\u6237\u7684\u5b66\u672f\u5174\u8da3\u3002\u5f15\u6587\u56fe\u8c31\u662f\u652f\u6301\u63a8\u8350\u7cfb\u7edf\u7ed3\u679c\u7684\u4e00\u79cd\u6709\u4ef7\u503c\u7684\u624b\u6bb5\uff0c\u4f46\u5176\u5728\u4e2a\u6027\u5316\u5b66\u672f\u641c\u7d22\u4e2d\u7684\u5e94\u7528\u4ecd\u6709\u5f85\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u6b65\u6cd5\uff1a\u9996\u5148\uff0c\u8bad\u7ec3\u4e00\u4e2a\u7528\u4e8e\u68c0\u7d22\u7684\u795e\u7ecf\u8bed\u8a00\u6a21\u578b\uff0c\u7136\u540e\u5c06\u5b66\u672f\u56fe\u8f6c\u6362\u4e3a\u77e5\u8bc6\u56fe\u8c31\uff0c\u5e76\u4f7f\u7528\u7ffb\u8bd1\u5d4c\u5165\u6280\u672f\u5c06\u5176\u5d4c\u5165\u5230\u4e0e\u8bed\u8a00\u6a21\u578b\u5171\u4eab\u7684\u8bed\u4e49\u7a7a\u95f4\u4e2d\u3002", "result": "\u5728\u56db\u4e2a\u5b66\u672f\u641c\u7d22\u9886\u57df\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\uff0c\u5728\u56db\u4e2a\u9886\u57df\u4e2d\u7684\u4e09\u4e2a\u9886\u57df\u4f18\u4e8e\u4f20\u7edf\u7684\u57fa\u4e8e\u56fe\u7684\u6a21\u578b\u548c\u4e2a\u6027\u5316\u6a21\u578b\uff0c\u5728 MAP@100 \u65b9\u9762\u6bd4\u7b2c\u4e8c\u597d\u7684\u6a21\u578b\u63d0\u9ad8\u4e86 10%\u3002", "conclusion": "\u77e5\u8bc6\u56fe\u8c31\u7528\u6237\u6a21\u578b\u6709\u6f5c\u529b\u63d0\u9ad8\u68c0\u7d22\u6548\u7387\u3002"}}
{"id": "2507.13392", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13392", "abs": "https://arxiv.org/abs/2507.13392", "authors": ["Emil H\u00e4glund", "Johanna Bj\u00f6rklund"], "title": "TopicImpact: Improving Customer Feedback Analysis with Opinion Units for Topic Modeling and Star-Rating Prediction", "comment": null, "summary": "We improve the extraction of insights from customer reviews by restructuring\nthe topic modelling pipeline to operate on opinion units - distinct statements\nthat include relevant text excerpts and associated sentiment scores. Prior work\nhas demonstrated that such units can be reliably extracted using large language\nmodels. The result is a heightened performance of the subsequent topic\nmodeling, leading to coherent and interpretable topics while also capturing the\nsentiment associated with each topic. By correlating the topics and sentiments\nwith business metrics, such as star ratings, we can gain insights on how\nspecific customer concerns impact business outcomes. We present our system's\nimplementation, use cases, and advantages over other topic modeling and\nclassification solutions. We also evaluate its effectiveness in creating\ncoherent topics and assess methods for integrating topic and sentiment\nmodalities for accurate star-rating prediction.", "AI": {"tldr": "The paper introduces a new topic modeling pipeline that operates on opinion units, leading to better performance and interpretable topics with sentiment, which can be correlated with business metrics to gain insights on customer concerns and their impact on business outcomes.", "motivation": "Improve the extraction of insights from customer reviews.", "method": "Restructuring the topic modelling pipeline to operate on opinion units - distinct statements that include relevant text excerpts and associated sentiment scores.", "result": "Heightened performance of the subsequent topic modeling, leading to coherent and interpretable topics while also capturing the sentiment associated with each topic. Correlation of the topics and sentiments with business metrics, such as star ratings, provides insights on how specific customer concerns impact business outcomes.", "conclusion": "The system's implementation, use cases, and advantages over other topic modeling and classification solutions are presented. The effectiveness in creating coherent topics and methods for integrating topic and sentiment modalities for accurate star-rating prediction are evaluated."}}
{"id": "2507.13651", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13651", "abs": "https://arxiv.org/abs/2507.13651", "authors": ["Gerben van der Hoek", "Johan Jeuring", "Rogier Bos"], "title": "Buggy rule diagnosis for combined steps through final answer evaluation in stepwise tasks", "comment": null, "summary": "Many intelligent tutoring systems can support a student in solving a stepwise\ntask. When a student combines several steps in one step, the number of possible\npaths connecting consecutive inputs may be very large. This combinatorial\nexplosion makes error diagnosis hard. Using a final answer to diagnose a\ncombination of steps can mitigate the combinatorial explosion, because there\nare generally fewer possible (erroneous) final answers than (erroneous)\nsolution paths. An intermediate input for a task can be diagnosed by\nautomatically completing it according to the task solution strategy and\ndiagnosing this solution. This study explores the potential of automated error\ndiagnosis based on a final answer. We investigate the design of a service that\nprovides a buggy rule diagnosis when a student combines several steps. To\nvalidate the approach, we apply the service to an existing dataset (n=1939) of\nunique student steps when solving quadratic equations, which could not be\ndiagnosed by a buggy rule service that tries to connect consecutive inputs with\na single rule. Results show that final answer evaluation can diagnose 29,4% of\nthese steps. Moreover, a comparison of the generated diagnoses with teacher\ndiagnoses on a subset (n=115) shows that the diagnoses align in 97% of the\ncases. These results can be considered a basis for further exploration of the\napproach.", "AI": {"tldr": "Using a final answer to diagnose a combination of steps can mitigate the combinatorial explosion in error diagnosis.", "motivation": "error diagnosis is hard when a student combines several steps in one step, because the number of possible paths connecting consecutive inputs may be very large. Using a final answer to diagnose a combination of steps can mitigate the combinatorial explosion", "method": "design of a service that provides a buggy rule diagnosis when a student combines several steps, validated by applying the service to an existing dataset of unique student steps when solving quadratic equations", "result": "final answer evaluation can diagnose 29,4% of steps. Diagnoses align with teacher diagnoses in 97% of the cases.", "conclusion": "final answer evaluation can diagnose 29,4% of steps that could not be diagnosed by a buggy rule service, and the diagnoses align with teacher diagnoses in 97% of the cases."}}
{"id": "2507.13364", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13364", "abs": "https://arxiv.org/abs/2507.13364", "authors": ["Siddharth Srivastava", "Gaurav Sharma"], "title": "OmniVec2 -- A Novel Transformer based Network for Large Scale Multimodal and Multitask Learning", "comment": null, "summary": "We present a novel multimodal multitask network and associated training\nalgorithm. The method is capable of ingesting data from approximately 12\ndifferent modalities namely image, video, audio, text, depth, point cloud, time\nseries, tabular, graph, X-ray, infrared, IMU, and hyperspectral. The proposed\napproach utilizes modality specialized tokenizers, a shared transformer\narchitecture, and cross-attention mechanisms to project the data from different\nmodalities into a unified embedding space. It addresses multimodal and\nmultitask scenarios by incorporating modality-specific task heads for different\ntasks in respective modalities. We propose a novel pretraining strategy with\niterative modality switching to initialize the network, and a training\nalgorithm which trades off fully joint training over all modalities, with\ntraining on pairs of modalities at a time. We provide comprehensive evaluation\nacross 25 datasets from 12 modalities and show state of the art performances,\ndemonstrating the effectiveness of the proposed architecture, pretraining\nstrategy and adapted multitask training.", "AI": {"tldr": "a novel multimodal multitask network and a training algorithm are proposed, which achieves state-of-the-art performance on 25 datasets.", "motivation": "addresses multimodal and multitask scenarios by incorporating modality-specific task heads for different tasks in respective modalities", "method": "a novel multimodal multitask network with modality specialized tokenizers, a shared transformer architecture, and cross-attention mechanisms", "result": "project data from different modalities into a unified embedding space", "conclusion": "achieve state-of-the-art performance on 25 datasets from 12 modalities, demonstrating the effectiveness of the proposed architecture, pretraining strategy and adapted multitask training."}}
{"id": "2507.13482", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13482", "abs": "https://arxiv.org/abs/2507.13482", "authors": ["Seyyed Saeid Cheshmi", "Buyao Lyu", "Thomas Lisko", "Rajesh Rajamani", "Robert A. McGovern", "Yogatheesan Varatharajah"], "title": "Improving Out-of-distribution Human Activity Recognition via IMU-Video Cross-modal Representation Learning", "comment": null, "summary": "Human Activity Recognition (HAR) based on wearable inertial sensors plays a\ncritical role in remote health monitoring. In patients with movement disorders,\nthe ability to detect abnormal patient movements in their home environments can\nenable continuous optimization of treatments and help alert caretakers as\nneeded. Machine learning approaches have been proposed for HAR tasks using\nInertial Measurement Unit (IMU) data; however, most rely on\napplication-specific labels and lack generalizability to data collected in\ndifferent environments or populations. To address this limitation, we propose a\nnew cross-modal self-supervised pretraining approach to learn representations\nfrom large-sale unlabeled IMU-video data and demonstrate improved\ngeneralizability in HAR tasks on out of distribution (OOD) IMU datasets,\nincluding a dataset collected from patients with Parkinson's disease.\nSpecifically, our results indicate that the proposed cross-modal pretraining\napproach outperforms the current state-of-the-art IMU-video pretraining\napproach and IMU-only pretraining under zero-shot and few-shot evaluations.\nBroadly, our study provides evidence that in highly dynamic data modalities,\nsuch as IMU signals, cross-modal pretraining may be a useful tool to learn\ngeneralizable data representations. Our software is available at\nhttps://github.com/scheshmi/IMU-Video-OOD-HAR.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8de8\u6a21\u6001\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece IMU \u89c6\u9891\u6570\u636e\u4e2d\u5b66\u4e60\u8868\u5f81\uff0c\u5e76\u5728 HAR \u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6539\u8fdb\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u5e15\u91d1\u68ee\u75c5\u60a3\u8005\u7684\u6570\u636e\u96c6\u4e2d\u3002", "motivation": "\u57fa\u4e8e\u53ef\u7a7f\u6234\u60ef\u6027\u4f20\u611f\u5668\u7684 HAR \u5728\u8fdc\u7a0b\u5065\u5eb7\u76d1\u6d4b\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002\u8fd0\u52a8\u969c\u788d\u60a3\u8005\u5982\u679c\u80fd\u591f\u5728\u5bb6\u5ead\u73af\u5883\u4e2d\u68c0\u6d4b\u5230\u5f02\u5e38\u7684\u60a3\u8005\u52a8\u4f5c\uff0c\u5c31\u53ef\u4ee5\u6301\u7eed\u4f18\u5316\u6cbb\u7597\uff0c\u5e76\u5728\u9700\u8981\u65f6\u63d0\u9192\u770b\u62a4\u4eba\u5458\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7279\u5b9a\u4e8e\u5e94\u7528\u7a0b\u5e8f\u7684\u6807\u7b7e\uff0c\u5e76\u4e14\u7f3a\u4e4f\u5bf9\u5728\u4e0d\u540c\u73af\u5883\u6216\u4eba\u7fa4\u4e2d\u6536\u96c6\u7684\u6570\u636e\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8de8\u6a21\u6001\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4ee5\u4ece\u5927\u89c4\u6a21\u672a\u6807\u8bb0\u7684 IMU \u89c6\u9891\u6570\u636e\u4e2d\u5b66\u4e60\u8868\u5f81\u3002", "result": "\u8be5\u7814\u7a76\u7684\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u8de8\u6a21\u6001\u9884\u8bad\u7ec3\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u548c\u5c0f\u6837\u672c\u8bc4\u4f30\u4e0b\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684 IMU \u89c6\u9891\u9884\u8bad\u7ec3\u65b9\u6cd5\u548c\u4ec5 IMU \u9884\u8bad\u7ec3\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u5728\u9ad8\u5ea6\u52a8\u6001\u7684\u6570\u636e\u6a21\u5f0f\uff08\u5982 IMU \u4fe1\u53f7\uff09\u4e2d\uff0c\u8de8\u6a21\u6001\u9884\u8bad\u7ec3\u53ef\u80fd\u662f\u5b66\u4e60\u901a\u7528\u6570\u636e\u8868\u793a\u7684\u6709\u7528\u5de5\u5177\u3002"}}
{"id": "2507.13957", "categories": ["cs.IR", "cs.AI", "cs.LG", "68T05, 68T50, 62M45", "H.3.3; I.2.6; H.3.4; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.13957", "abs": "https://arxiv.org/abs/2507.13957", "authors": ["Yitong Li", "Raoul Grasman"], "title": "DUALRec: A Hybrid Sequential and Language Model Framework for Context-Aware Movie Recommendation", "comment": "10 pages, 5 figures", "summary": "The modern recommender systems are facing an increasing challenge of\nmodelling and predicting the dynamic and context-rich user preferences.\nTraditional collaborative filtering and content-based methods often struggle to\ncapture the temporal patternings and evolving user intentions. While Large\nLanguage Models (LLMs) have gained gradual attention in recent years, by their\nstrong semantic understanding and reasoning abilities, they are not inherently\ndesigned to model chronologically evolving user preference and intentions. On\nthe other hand, for sequential models like LSTM (Long-Short-Term-Memory) which\nis good at capturing the temporal dynamics of user behaviour and evolving user\npreference over time, but still lacks a rich semantic understanding for\ncomprehensive recommendation generation. In this study, we propose DUALRec\n(Dynamic User-Aware Language-based Recommender), a novel recommender that\nleverages the complementary strength of both models, which combines the\ntemporal modelling abilities of LSTM networks with semantic reasoning power of\nthe fine-tuned Large Language Models. The LSTM component will capture users\nevolving preference through their viewing history, while the fine-tuned LLM\nvariants will leverage these temporal user insights to generate next movies\nthat users might enjoy. Experimental results on MovieLens-1M dataset shows that\nthe DUALRec model outperforms a wide range of baseline models, with\ncomprehensive evaluation matrices of Hit Rate (HR@k), Normalized Discounted\nCumulative Gain (NDCG@k), and genre similarity metrics. This research proposes\na novel architecture that bridges the gap between temporal sequence modeling\nand semantic reasoning, and offers a promising direction for developing more\nintelligent and context-aware recommenders.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDUALRec\u7684\u65b0\u63a8\u8350\u6a21\u578b\uff0c\u5b83\u7ed3\u5408\u4e86LSTM\u548cLLM\u7684\u4f18\u52bf\uff0c\u4ee5\u66f4\u597d\u5730\u6355\u6349\u7528\u6237\u504f\u597d\u5e76\u63d0\u9ad8\u63a8\u8350\u8d28\u91cf\u3002", "motivation": "\u73b0\u4ee3\u63a8\u8350\u7cfb\u7edf\u9762\u4e34\u7740\u5bf9\u52a8\u6001\u548c\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u7528\u6237\u504f\u597d\u8fdb\u884c\u5efa\u6a21\u548c\u9884\u6d4b\u7684\u65e5\u76ca\u4e25\u5cfb\u7684\u6311\u6218\u3002\u4f20\u7edf\u7684\u534f\u540c\u8fc7\u6ee4\u548c\u57fa\u4e8e\u5185\u5bb9\u7684\u65b9\u6cd5\u901a\u5e38\u96be\u4ee5\u6355\u6349\u65f6\u95f4\u6a21\u5f0f\u548c\u4e0d\u65ad\u53d8\u5316\u7684\u7528\u6237\u610f\u56fe\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fd1\u5e74\u6765\u9010\u6e10\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u5b83\u4eec\u5e76\u975e\u5929\u751f\u8bbe\u8ba1\u7528\u4e8e\u5bf9\u6309\u65f6\u95f4\u987a\u5e8f\u6f14\u53d8\u7684\u7528\u6237\u504f\u597d\u548c\u610f\u56fe\u8fdb\u884c\u5efa\u6a21\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u5bf9\u4e8e\u50cfLSTM\uff08\u957f\u77ed\u671f\u8bb0\u5fc6\uff09\u8fd9\u6837\u7684\u5e8f\u5217\u6a21\u578b\uff0c\u5b83\u64c5\u957f\u6355\u6349\u7528\u6237\u884c\u4e3a\u7684\u65f6\u95f4\u52a8\u6001\u548c\u968f\u65f6\u95f4\u53d8\u5316\u7684\u7528\u6237\u504f\u597d\uff0c\u4f46\u4ecd\u7136\u7f3a\u4e4f\u5bf9\u5168\u9762\u63a8\u8350\u751f\u6210\u7684\u4e30\u5bcc\u7684\u8bed\u4e49\u7406\u89e3\u3002", "method": "\u63d0\u51faDUALRec\uff08\u52a8\u6001\u7528\u6237\u611f\u77e5\u8bed\u8a00\u63a8\u8350\u5668\uff09\uff0c\u8be5\u63a8\u8350\u5668\u7ed3\u5408\u4e86LSTM\u7f51\u7edc\u7684\u65f6\u95f4\u5efa\u6a21\u80fd\u529b\u548c\u5fae\u8c03\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDUALRec\u6a21\u578b\u4f18\u4e8e\u5404\u79cd\u57fa\u7ebf\u6a21\u578b\uff0c\u5177\u6709\u5168\u9762\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u5305\u62ec\u547d\u4e2d\u7387\uff08HR@k\uff09\u3001\u5f52\u4e00\u5316\u6298\u635f\u7d2f\u79ef\u589e\u76ca\uff08NDCG@k\uff09\u548c\u7c7b\u578b\u76f8\u4f3c\u6027\u6307\u6807\u3002", "conclusion": "DUALRec\u6a21\u578b\u5728MovieLens-1M\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u5404\u79cd\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u5728\u547d\u4e2d\u7387\uff08HR@k\uff09\u3001\u5f52\u4e00\u5316\u6298\u635f\u7d2f\u79ef\u589e\u76ca\uff08NDCG@k\uff09\u548c\u7c7b\u578b\u76f8\u4f3c\u6027\u6307\u6807\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u67b6\u6784\uff0c\u5f25\u5408\u4e86\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u548c\u8bed\u4e49\u63a8\u7406\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u5f00\u53d1\u66f4\u667a\u80fd\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u63a8\u8350\u5668\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002"}}
{"id": "2507.13395", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13395", "abs": "https://arxiv.org/abs/2507.13395", "authors": ["Xuanqi Gao", "Weipeng Jiang", "Juan Zhai", "Shiqing Ma", "Siyi Xie", "Xinyang Yin", "Chao Shen"], "title": "Mitigating Stylistic Biases of Machine Translation Systems via Monolingual Corpora Only", "comment": null, "summary": "The advent of neural machine translation (NMT) has revolutionized\ncross-lingual communication, yet preserving stylistic nuances remains a\nsignificant challenge. While existing approaches often require parallel corpora\nfor style preservation, we introduce Babel, a novel framework that enhances\nstylistic fidelity in NMT using only monolingual corpora. Babel employs two key\ncomponents: (1) a style detector based on contextual embeddings that identifies\nstylistic disparities between source and target texts, and (2) a\ndiffusion-based style applicator that rectifies stylistic inconsistencies while\nmaintaining semantic integrity. Our framework integrates with existing NMT\nsystems as a post-processing module, enabling style-aware translation without\nrequiring architectural modifications or parallel stylistic data. Extensive\nexperiments on five diverse domains (law, literature, scientific writing,\nmedicine, and educational content) demonstrate Babel's effectiveness: it\nidentifies stylistic inconsistencies with 88.21% precision and improves\nstylistic preservation by 150% while maintaining a high semantic similarity\nscore of 0.92. Human evaluation confirms that translations refined by Babel\nbetter preserve source text style while maintaining fluency and adequacy.", "AI": {"tldr": "Babel, a novel framework that enhances stylistic fidelity in NMT using only monolingual corpora.", "motivation": "preserving stylistic nuances remains a significant challenge for neural machine translation (NMT)", "method": "a style detector based on contextual embeddings and a diffusion-based style applicator", "result": "identifies stylistic inconsistencies with 88.21% precision and improves stylistic preservation by 150% while maintaining a high semantic similarity score of 0.92", "conclusion": "Babel can better preserve source text style while maintaining fluency and adequacy."}}
{"id": "2507.13652", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13652", "abs": "https://arxiv.org/abs/2507.13652", "authors": ["Gerben van der Hoek", "Johan Jeuring", "Rogier Bos"], "title": "Combining model tracing and constraint-based modeling for multistep strategy diagnoses", "comment": null, "summary": "Model tracing and constraint-based modeling are two approaches to diagnose\nstudent input in stepwise tasks. Model tracing supports identifying consecutive\nproblem-solving steps taken by a student, whereas constraint-based modeling\nsupports student input diagnosis even when several steps are combined into one\nstep. We propose an approach that merges both paradigms. By defining\nconstraints as properties that a student input has in common with a step of a\nstrategy, it is possible to provide a diagnosis when a student deviates from a\nstrategy even when the student combines several steps. In this study we explore\nthe design of a system for multistep strategy diagnoses, and evaluate these\ndiagnoses. As a proof of concept, we generate diagnoses for an existing dataset\ncontaining steps students take when solving quadratic equations (n=2136). To\ncompare with human diagnoses, two teachers coded a random sample of deviations\n(n=70) and applications of the strategy (n=70). Results show that that the\nsystem diagnosis aligned with the teacher coding in all of the 140 student\nsteps.", "AI": {"tldr": "merges model tracing and constraint-based modeling for multistep strategy diagnoses", "motivation": "Model tracing and constraint-based modeling are two approaches to diagnose student input in stepwise tasks. Model tracing supports identifying consecutive problem-solving steps taken by a student, whereas constraint-based modeling supports student input diagnosis even when several steps are combined into one step.", "method": "an approach that merges model tracing and constraint-based modeling paradigms by defining constraints as properties that a student input has in common with a step of a strategy", "result": "system diagnosis aligned with the teacher coding in all of the 140 student steps.", "conclusion": "The system diagnosis aligned with the teacher coding in all of the 140 student steps."}}
{"id": "2507.13371", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13371", "abs": "https://arxiv.org/abs/2507.13371", "authors": ["Yeming Cai", "Yang Wang", "Zhenglin Li"], "title": "Transformer-Based Framework for Motion Capture Denoising and Anomaly Detection in Medical Rehabilitation", "comment": null, "summary": "This paper proposes an end-to-end deep learning framework integrating optical\nmotion capture with a Transformer-based model to enhance medical\nrehabilitation. It tackles data noise and missing data caused by occlusion and\nenvironmental factors, while detecting abnormal movements in real time to\nensure patient safety. Utilizing temporal sequence modeling, our framework\ndenoises and completes motion capture data, improving robustness. Evaluations\non stroke and orthopedic rehabilitation datasets show superior performance in\ndata reconstruction and anomaly detection, providing a scalable, cost-effective\nsolution for remote rehabilitation with reduced on-site supervision.", "AI": {"tldr": "This paper introduces a deep learning framework using optical motion capture and a Transformer model for enhanced, scalable, and cost-effective remote medical rehabilitation, addressing data noise and ensuring patient safety with real-time anomaly detection.", "motivation": "The paper aims to tackle data noise and missing data in medical rehabilitation caused by occlusion and environmental factors, while detecting abnormal movements in real time to ensure patient safety.", "method": "The paper proposes an end-to-end deep learning framework integrating optical motion capture with a Transformer-based model.", "result": "The framework denoises and completes motion capture data, improving robustness.", "conclusion": "The proposed framework shows superior performance in data reconstruction and anomaly detection on stroke and orthopedic rehabilitation datasets, offering a scalable and cost-effective solution for remote rehabilitation with reduced on-site supervision."}}
{"id": "2507.13491", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.13491", "abs": "https://arxiv.org/abs/2507.13491", "authors": ["Thomas Banker", "Ali Mesbah"], "title": "Model-free Reinforcement Learning for Model-based Control: Towards Safe, Interpretable and Sample-efficient Agents", "comment": null, "summary": "Training sophisticated agents for optimal decision-making under uncertainty\nhas been key to the rapid development of modern autonomous systems across\nfields. Notably, model-free reinforcement learning (RL) has enabled\ndecision-making agents to improve their performance directly through system\ninteractions, with minimal prior knowledge about the system. Yet, model-free RL\nhas generally relied on agents equipped with deep neural network function\napproximators, appealing to the networks' expressivity to capture the agent's\npolicy and value function for complex systems. However, neural networks amplify\nthe issues of sample inefficiency, unsafe learning, and limited\ninterpretability in model-free RL. To this end, this work introduces\nmodel-based agents as a compelling alternative for control policy\napproximation, leveraging adaptable models of system dynamics, cost, and\nconstraints for safe policy learning. These models can encode prior system\nknowledge to inform, constrain, and aid in explaining the agent's decisions,\nwhile deficiencies due to model mismatch can be remedied with model-free RL. We\noutline the benefits and challenges of learning model-based agents --\nexemplified by model predictive control -- and detail the primary learning\napproaches: Bayesian optimization, policy search RL, and offline strategies,\nalong with their respective strengths. While model-free RL has long been\nestablished, its interplay with model-based agents remains largely unexplored,\nmotivating our perspective on their combined potentials for sample-efficient\nlearning of safe and interpretable decision-making agents.", "AI": {"tldr": "Model-based agents are introduced as a better alternative to model-free RL, addressing its shortcomings and offering safe, interpretable decision-making.", "motivation": "Model-free RL relies on deep neural networks, which suffer from sample inefficiency, unsafe learning, and limited interpretability.", "method": "Introduction of model-based agents as an alternative for control policy approximation, along with Bayesian optimization, policy search RL, and offline strategies.", "result": "Outlines the benefits and challenges of learning model-based agents and details the primary learning approaches.", "conclusion": "Model-based agents offer a compelling alternative for control policy approximation, leveraging adaptable models for safe policy learning, and their interplay with model-free RL holds potential for sample-efficient learning of safe and interpretable decision-making agents."}}
{"id": "2507.13374", "categories": ["cs.CV", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.13374", "abs": "https://arxiv.org/abs/2507.13374", "authors": ["Kevin Dela Rosa"], "title": "Smart Routing for Multimodal Video Retrieval: When to Search What", "comment": "Accepted to ICCV 2025 Multimodal Representation and Retrieval\n  Workshop", "summary": "We introduce ModaRoute, an LLM-based intelligent routing system that\ndynamically selects optimal modalities for multimodal video retrieval. While\ndense text captions can achieve 75.9% Recall@5, they require expensive offline\nprocessing and miss critical visual information present in 34% of clips with\nscene text not captured by ASR. By analyzing query intent and predicting\ninformation needs, ModaRoute reduces computational overhead by 41% while\nachieving 60.9% Recall@5. Our approach uses GPT-4.1 to route queries across ASR\n(speech), OCR (text), and visual indices, averaging 1.78 modalities per query\nversus exhaustive 3.0 modality search. Evaluation on 1.8M video clips\ndemonstrates that intelligent routing provides a practical solution for scaling\nmultimodal retrieval systems, reducing infrastructure costs while maintaining\ncompetitive effectiveness for real-world deployment.", "AI": {"tldr": "ModaRoute, an LLM-based intelligent routing system, dynamically selects optimal modalities for multimodal video retrieval, reducing computational overhead by 41% while achieving 60.9% Recall@5", "motivation": "dense text captions require expensive offline processing and miss critical visual information present in 34% of clips with scene text not captured by ASR", "method": "ModaRoute uses GPT-4.1 to route queries across ASR (speech), OCR (text), and visual indices", "result": "ModaRoute reduces computational overhead by 41% while achieving 60.9% Recall@5, averaging 1.78 modalities per query versus exhaustive 3.0 modality search", "conclusion": "intelligent routing provides a practical solution for scaling multimodal retrieval systems, reducing infrastructure costs while maintaining competitive effectiveness for real-world deployment"}}
{"id": "2507.13410", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13410", "abs": "https://arxiv.org/abs/2507.13410", "authors": ["Cheng-Ting Chou", "George Liu", "Jessica Sun", "Cole Blondin", "Kevin Zhu", "Vasu Sharma", "Sean O'Brien"], "title": "Causal Language Control in Multilingual Transformers via Sparse Feature Steering", "comment": null, "summary": "Deterministically controlling the target generation language of large\nmultilingual language models (LLMs) remains a fundamental challenge,\nparticularly in zero-shot settings where neither explicit language prompts nor\nfine-tuning are available. In this work, we investigate whether sparse\nautoencoder (SAE) features, previously shown to correlate with interpretable\nmodel behaviors, can be leveraged to steer the generated language of LLMs\nduring inference. Leveraging pretrained SAEs on the residual streams of\nGemma-2B and Gemma-9B, we identify features whose activations differ most\nsignificantly between English and four target languages: Chinese, Japanese,\nSpanish, and French. By modifying just a single SAE feature at one transformer\nlayer, we achieve controlled language shifts with up to 90\\% success, as\nmeasured by FastText language classification, while preserving semantic\nfidelity according to LaBSE (Language-Agnostic BERT Sentence Embedding)\nsimilarity. Our analysis reveals that language steering is most effective in\nmid-to-late transformer layers and is amplified by specific attention heads\ndisproportionately associated with language-sensitive SAE features. These\nresults demonstrate the promise of sparse feature steering as a lightweight and\ninterpretable mechanism for controllable multilingual generation.", "AI": {"tldr": "\u901a\u8fc7\u4fee\u6539\u7a00\u758f\u81ea\u7f16\u7801\u5668 (SAE) \u7279\u5f81\uff0c\u53ef\u4ee5\u5728\u5927\u578b\u591a\u8bed\u8a00\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5f15\u5bfc\u751f\u6210\u7684\u8bed\u8a00\u3002", "motivation": "\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e2d\uff0c\u786e\u5b9a\u6027\u5730\u63a7\u5236\u5927\u578b\u591a\u8bed\u8a00\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u76ee\u6807\u751f\u6210\u8bed\u8a00\u4ecd\u7136\u662f\u4e00\u4e2a\u6839\u672c\u6027\u7684\u6311\u6218\uff0c\u5728\u6ca1\u6709\u660e\u786e\u7684\u8bed\u8a00\u63d0\u793a\u6216\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u5c24\u5176\u5982\u6b64\u3002", "method": "\u5229\u7528\u5728 Gemma-2B \u548c Gemma-9B \u7684\u6b8b\u5dee\u6d41\u4e0a\u9884\u8bad\u7ec3\u7684 SAE\uff0c\u6211\u4eec\u8bc6\u522b\u51fa\u5176\u6fc0\u6d3b\u5728\u82f1\u8bed\u548c\u56db\u79cd\u76ee\u6807\u8bed\u8a00\uff08\u4e2d\u6587\u3001\u65e5\u8bed\u3001\u897f\u73ed\u7259\u8bed\u548c\u6cd5\u8bed\uff09\u4e4b\u95f4\u5dee\u5f02\u6700\u663e\u7740\u7684\u7279\u5f81\u3002\u901a\u8fc7\u4ec5\u4fee\u6539\u4e00\u4e2atransformer\u5c42\u4e2d\u7684\u5355\u4e2a SAE \u7279\u5f81\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86\u9ad8\u8fbe 90% \u6210\u529f\u7387\u7684\u53d7\u63a7\u8bed\u8a00\u8f6c\u6362\u3002", "result": "\u901a\u8fc7\u4fee\u6539\u4e00\u4e2atransformer\u5c42\u4e2d\u7684\u5355\u4e2a SAE \u7279\u5f81\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe 90% \u6210\u529f\u7387\u7684\u53d7\u63a7\u8bed\u8a00\u8f6c\u6362\uff0c\u540c\u65f6\u6839\u636e LaBSE\uff08\u4e0e\u8bed\u8a00\u65e0\u5173\u7684 BERT \u53e5\u5b50\u5d4c\u5165\uff09\u76f8\u4f3c\u6027\u4fdd\u7559\u4e86\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002\u8bed\u8a00\u63a7\u5236\u5728\u4e2d\u95f4\u5230\u540e\u9762\u7684transformer\u5c42\u4e2d\u6700\u6709\u6548\uff0c\u5e76\u4e14\u88ab\u4e0e\u8bed\u8a00\u654f\u611f\u7684 SAE \u7279\u5f81\u4e0d\u6210\u6bd4\u4f8b\u5730\u76f8\u5173\u7684\u7279\u5b9a\u6ce8\u610f\u529b\u5934\u653e\u5927\u3002", "conclusion": "\u7a00\u758f\u7279\u5f81\u63a7\u5236\u662f\u4e00\u79cd\u7528\u4e8e\u53ef\u63a7\u591a\u8bed\u8a00\u751f\u6210\u7684\u8f7b\u91cf\u7ea7\u548c\u53ef\u89e3\u91ca\u7684\u673a\u5236\u3002"}}
{"id": "2507.13737", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.13737", "abs": "https://arxiv.org/abs/2507.13737", "authors": ["Ye Tian", "Xiaoyuan Ren", "Zihao Wang", "Onat Gungor", "Xiaofan Yu", "Tajana Rosing"], "title": "DailyLLM: Context-Aware Activity Log Generation Using Multi-Modal Sensors and LLMs", "comment": null, "summary": "Rich and context-aware activity logs facilitate user behavior analysis and\nhealth monitoring, making them a key research focus in ubiquitous computing.\nThe remarkable semantic understanding and generation capabilities of Large\nLanguage Models (LLMs) have recently created new opportunities for activity log\ngeneration. However, existing methods continue to exhibit notable limitations\nin terms of accuracy, efficiency, and semantic richness. To address these\nchallenges, we propose DailyLLM. To the best of our knowledge, this is the\nfirst log generation and summarization system that comprehensively integrates\ncontextual activity information across four dimensions: location, motion,\nenvironment, and physiology, using only sensors commonly available on\nsmartphones and smartwatches. To achieve this, DailyLLM introduces a\nlightweight LLM-based framework that integrates structured prompting with\nefficient feature extraction to enable high-level activity understanding.\nExtensive experiments demonstrate that DailyLLM outperforms state-of-the-art\n(SOTA) log generation methods and can be efficiently deployed on personal\ncomputers and Raspberry Pi. Utilizing only a 1.5B-parameter LLM model, DailyLLM\nachieves a 17% improvement in log generation BERTScore precision compared to\nthe 70B-parameter SOTA baseline, while delivering nearly 10x faster inference\nspeed.", "AI": {"tldr": "DailyLLM is a new activity log generation system that uses a lightweight LLM to improve accuracy, efficiency, and semantic richness by integrating contextual activity information from smartphone and smartwatch sensors.", "motivation": "Existing activity log generation methods have limitations in accuracy, efficiency, and semantic richness.", "method": "A lightweight LLM-based framework integrating structured prompting with efficient feature extraction to understand high-level activity, using contextual activity information across location, motion, environment, and physiology from smartphone and smartwatch sensors.", "result": "DailyLLM achieves a 17% improvement in log generation BERTScore precision compared to a 70B-parameter baseline, while delivering nearly 10x faster inference speed, using only a 1.5B-parameter LLM model.", "conclusion": "DailyLLM outperforms existing log generation methods in accuracy and efficiency, achieving a 17% improvement in BERTScore precision with a much smaller model and faster inference speed."}}
{"id": "2507.13372", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13372", "abs": "https://arxiv.org/abs/2507.13372", "authors": ["Yeming Cai", "Zhenglin Li", "Yang Wang"], "title": "Enhancing Breast Cancer Detection with Vision Transformers and Graph Neural Networks", "comment": null, "summary": "Breast cancer is a leading cause of death among women globally, and early\ndetection is critical for improving survival rates. This paper introduces an\ninnovative framework that integrates Vision Transformers (ViT) and Graph Neural\nNetworks (GNN) to enhance breast cancer detection using the CBIS-DDSM dataset.\nOur framework leverages ViT's ability to capture global image features and\nGNN's strength in modeling structural relationships, achieving an accuracy of\n84.2%, outperforming traditional methods. Additionally, interpretable attention\nheatmaps provide insights into the model's decision-making process, aiding\nradiologists in clinical settings.", "AI": {"tldr": "Combines ViT and GNN for better breast cancer detection, achieving 84.2% accuracy and interpretable results.", "motivation": "Early detection of breast cancer is critical for improving survival rates.", "method": "Integrates Vision Transformers (ViT) and Graph Neural Networks (GNN).", "result": "Enhanced breast cancer detection using the CBIS-DDSM dataset with improved accuracy.", "conclusion": "Achieved 84.2% accuracy in breast cancer detection, outperforming traditional methods, and provides interpretable attention heatmaps for aiding radiologists."}}
{"id": "2507.13508", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.13508", "abs": "https://arxiv.org/abs/2507.13508", "authors": ["Agata Kaczmarek", "Dawid P\u0142udowski", "Piotr Wilczy\u0144ski", "Przemys\u0142aw Biecek", "Krzysztof Kotowski", "Ramez Shendy", "Jakub Nalepa", "Artur Janicki", "Evridiki Ntagiou"], "title": "Fake or Real: The Impostor Hunt in Texts for Space Operations", "comment": null, "summary": "The \"Fake or Real\" competition hosted on Kaggle\n(\\href{https://www.kaggle.com/competitions/fake-or-real-the-impostor-hunt}{https://www.kaggle.com/competitions/fake-or-real-the-impostor-hunt})\nis the second part of a series of follow-up competitions and hackathons related\nto the \"Assurance for Space Domain AI Applications\" project funded by the\nEuropean Space Agency\n(\\href{https://assurance-ai.space-codev.org/}{https://assurance-ai.space-codev.org/}).\nThe competition idea is based on two real-life AI security threats identified\nwithin the project -- data poisoning and overreliance in Large Language Models.\nThe task is to distinguish between the proper output from LLM and the output\ngenerated under malicious modification of the LLM. As this problem was not\nextensively researched, participants are required to develop new techniques to\naddress this issue or adjust already existing ones to this problem's statement.", "AI": {"tldr": "Kaggle competition focusing on identifying malicious modifications of LLM output, motivated by AI security threats in space applications.", "motivation": "The competition is motivated by real-life AI security threats (data poisoning and overreliance in LLMs) identified within the 'Assurance for Space Domain AI Applications' project.", "method": "Participants develop techniques to distinguish between proper LLM output and output generated under malicious modification.", "result": "The task is to distinguish between the proper output from LLM and the output generated under malicious modification of the LLM.", "conclusion": "Participants need to develop new or adjust existing techniques to distinguish between proper LLM output and output generated under malicious modification, as this problem lacks extensive research."}}
{"id": "2507.13608", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.13608", "abs": "https://arxiv.org/abs/2507.13608", "authors": ["Yudai Hayashi", "Shuhei Goda", "Yuta Saito"], "title": "Off-Policy Evaluation and Learning for Matching Markets", "comment": "RecSys'25", "summary": "Matching users based on mutual preferences is a fundamental aspect of\nservices driven by reciprocal recommendations, such as job search and dating\napplications. Although A/B tests remain the gold standard for evaluating new\npolicies in recommender systems for matching markets, it is costly and\nimpractical for frequent policy updates. Off-Policy Evaluation (OPE) thus plays\na crucial role by enabling the evaluation of recommendation policies using only\noffline logged data naturally collected on the platform. However, unlike\nconventional recommendation settings, the large scale and bidirectional nature\nof user interactions in matching platforms introduce variance issues and\nexacerbate reward sparsity, making standard OPE methods unreliable. To address\nthese challenges and facilitate effective offline evaluation, we propose novel\nOPE estimators, \\textit{DiPS} and \\textit{DPR}, specifically designed for\nmatching markets. Our methods combine elements of the Direct Method (DM),\nInverse Propensity Score (IPS), and Doubly Robust (DR) estimators while\nincorporating intermediate labels, such as initial engagement signals, to\nachieve better bias-variance control in matching markets. Theoretically, we\nderive the bias and variance of the proposed estimators and demonstrate their\nadvantages over conventional methods. Furthermore, we show that these\nestimators can be seamlessly extended to offline policy learning methods for\nimproving recommendation policies for making more matches. We empirically\nevaluate our methods through experiments on both synthetic data and A/B testing\nlogs from a real job-matching platform. The empirical results highlight the\nsuperiority of our approach over existing methods in off-policy evaluation and\nlearning tasks for a variety of configurations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u5339\u914d\u5e02\u573a\u7684\u79bb\u7ebf\u7b56\u7565\u8bc4\u4f30\u95ee\u9898\uff0c\u63d0\u51fa\u4e86DiPS\u548cDPR\u4e24\u79cd\u65b0\u7684OPE\u4f30\u8ba1\u5668\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5728\u5339\u914d\u5e02\u573a\u4e2d\uff0c\u7531\u4e8e\u7528\u6237\u4e92\u52a8\u7684\u89c4\u6a21\u548c\u53cc\u5411\u6027\uff0c\u6807\u51c6\u7684OPE\u65b9\u6cd5\u5b58\u5728\u65b9\u5dee\u95ee\u9898\u548c\u5956\u52b1\u7a00\u758f\u6027\u95ee\u9898\uff0c\u4f7f\u5f97\u5176\u4e0d\u53ef\u9760\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u5e76\u4fc3\u8fdb\u6709\u6548\u7684\u79bb\u7ebf\u8bc4\u4f30\u3002", "method": "\u8be5\u8bba\u6587\u7ed3\u5408\u4e86\u76f4\u63a5\u65b9\u6cd5\uff08DM\uff09\u3001\u9006\u503e\u5411\u8bc4\u5206\uff08IPS\uff09\u548c\u53cc\u91cd\u9c81\u68d2\uff08DR\uff09\u4f30\u8ba1\u5668\u7684\u5143\u7d20\uff0c\u5e76\u7ed3\u5408\u4e86\u521d\u59cb\u53c2\u4e0e\u4fe1\u53f7\u7b49\u4e2d\u95f4\u6807\u7b7e\uff0c\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684\u5339\u914d\u5e02\u573a\u4e2d\u7684\u504f\u5dee-\u65b9\u5dee\u63a7\u5236\u3002", "result": "\u8be5\u8bba\u6587\u5728\u7406\u8bba\u4e0a\u63a8\u5bfc\u4e86\u6240\u63d0\u51fa\u7684\u4f30\u8ba1\u5668\u7684\u504f\u5dee\u548c\u65b9\u5dee\uff0c\u5e76\u8bc1\u660e\u4e86\u5b83\u4eec\u76f8\u5bf9\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u4f18\u52bf\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u79bb\u7ebf\u7b56\u7565\u8bc4\u4f30\u548c\u5b66\u4e60\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u65b0\u7684OPE\u4f30\u8ba1\u5668DiPS\u548cDPR\uff0c\u5e76\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u62db\u8058\u5e73\u53f0\u7684A/B\u6d4b\u8bd5\u65e5\u5fd7\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u914d\u7f6e\u4e0b\u7684\u79bb\u7ebf\u7b56\u7565\u8bc4\u4f30\u548c\u5b66\u4e60\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.13411", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13411", "abs": "https://arxiv.org/abs/2507.13411", "authors": ["Nur A Zarin Nishat", "Andrea Coletta", "Luigi Bellomarini", "Kossi Amouzouvi", "Jens Lehmann", "Sahar Vahdati"], "title": "Aligning Knowledge Graphs and Language Models for Factual Accuracy", "comment": null, "summary": "Large language models like GPT-4, Gemini, and Claude have transformed natural\nlanguage processing (NLP) tasks such as question answering, dialogue\ngeneration, summarization, and so forth; yet their susceptibility to\nhallucination stands as one of the major challenges. Among numerous approaches\nto overcome this challenge, integration of Knowledge Graphs (KGs) into language\nmodels has emerged as a promising solution as it provides structured, reliable,\ndomain-specific, and up-to-date external information to the language models. In\nthis paper, we introduce ALIGNed-LLM, a simple yet effective approach to\nimprove language models' factuality via a lean strategy to infuse KGs into the\nlatent space of language models inspired by LLaVA where visual and textual\ninformation is infused. We use embeddings from a pre-trained Knowledge Graph\nEmbedding (KGE) model, such as TransE, and a trainable projection layer to\nalign entity and text embeddings. This alignment enables the language model to\ndistinguish between similar entities improving factual grounding and reducing\nhallucination. We tested our approach on three popular questions-answering\nbenchmark datasets alongside language models of varying sizes, showing\nsignificant improvement. Furthermore, we applied our approach to a real-world\nfinancial use case from a large central bank in Europe, which demands high\naccuracy and precision, demonstrating a substantial improvement of the LLM\nanswers.", "AI": {"tldr": "ALIGNed-LLM\u901a\u8fc7\u5bf9\u9f50\u77e5\u8bc6\u56fe\u8c31\u548c\u6587\u672c\u5d4c\u5165\uff0c\u63d0\u9ad8\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u4e8b\u5b9e\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u5c06\u77e5\u8bc6\u56fe\u8c31\u6574\u5408\u5230\u8bed\u8a00\u6a21\u578b\u4e2d\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u56e0\u4e3a\u5b83\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u3001\u53ef\u9760\u3001\u7279\u5b9a\u9886\u57df\u548c\u6700\u65b0\u7684\u5916\u90e8\u4fe1\u606f\u3002", "method": "\u63d0\u51faALIGNed-LLM\uff0c\u901a\u8fc7\u4e00\u4e2a\u53ef\u8bad\u7ec3\u7684\u6295\u5f71\u5c42\u5bf9\u9f50\u5b9e\u4f53\u548c\u6587\u672c\u5d4c\u5165\uff0c\u5c06\u77e5\u8bc6\u56fe\u8c31\u878d\u5165\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\u3002", "result": "\u5728\u4e09\u4e2a\u6d41\u884c\u7684\u95ee\u7b54\u57fa\u51c6\u6570\u636e\u96c6\u4ee5\u53ca\u4e0d\u540c\u89c4\u6a21\u7684\u8bed\u8a00\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u663e\u793a\u51fa\u663e\u8457\u7684\u6539\u8fdb\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8e\u6b27\u6d32\u4e00\u5bb6\u5927\u578b\u4e2d\u592e\u94f6\u884c\u7684\u5b9e\u9645\u91d1\u878d\u7528\u4f8b\uff0c\u5bf9LLM\u7684\u7b54\u6848\u8fdb\u884c\u4e86\u5927\u5e45\u6539\u8fdb\u3002", "conclusion": "ALIGNed-LLM\u901a\u8fc7\u5c06\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u878d\u5165\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u63d0\u9ad8\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u4e8b\u5b9e\u6027\u548c\u51c6\u786e\u6027\uff0c\u5e76\u5728\u591a\u4e2aQA\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9e\u9645\u91d1\u878d\u7528\u4f8b\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2507.13759", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13759", "abs": "https://arxiv.org/abs/2507.13759", "authors": ["Carlos Bobed", "Carlota Quintana", "Eduardo Mena", "Jorge Bobed", "Fernando Bobillo"], "title": "OntView: What you See is What you Meant", "comment": null, "summary": "In the field of knowledge management and computer science, ontologies provide\na structured framework for modeling domain-specific knowledge by defining\nconcepts and their relationships. However, the lack of tools that provide\neffective visualization is still a significant challenge. While numerous\nontology editors and viewers exist, most of them fail to graphically represent\nontology structures in a meaningful and non-overwhelming way, limiting users'\nability to comprehend dependencies and properties within large ontological\nframeworks.\n  In this paper, we present OntView, an ontology viewer that is designed to\nprovide users with an intuitive visual representation of ontology concepts and\ntheir formal definitions through a user-friendly interface. Building on the use\nof a DL reasoner, OntView follows a \"What you see is what you meant\" paradigm,\nshowing the actual inferred knowledge. One key aspect for this is its ability\nto visualize General Concept Inclusions (GCI), a feature absent in existing\nvisualization tools. Moreover, to avoid a possible information overload,\nOntView also offers different ways to show a simplified view of the ontology\nby: 1) creating ontology summaries by assessing the importance of the concepts\n(according to different available algorithms), 2) focusing the visualization on\nthe existing TBox elements between two given classes and 3) allowing to\nhide/show different branches in a dynamic way without losing the semantics.\nOntView has been released with an open-source license for the whole community.", "AI": {"tldr": "OntView is an open-source ontology viewer that provides an intuitive visual representation of ontology concepts, visualizes GCIs, and offers simplified views to avoid information overload.", "motivation": "The lack of tools that provide effective visualization is a significant challenge in ontology management, limiting users' ability to comprehend dependencies and properties within large ontological frameworks.", "method": "OntView uses a DL reasoner and offers different ways to show a simplified view of the ontology by creating ontology summaries, focusing on TBox elements between two given classes, and allowing users to hide/show different branches dynamically.", "result": "OntView visualizes General Concept Inclusions (GCI), a feature absent in existing visualization tools. It also offers different ways to show a simplified view of the ontology to avoid information overload.", "conclusion": "OntView, an open-source ontology viewer, provides an intuitive visual representation of ontology concepts and their formal definitions through a user-friendly interface, visualizing General Concept Inclusions (GCI) and offering simplified views to avoid information overload."}}
{"id": "2507.13373", "categories": ["cs.CV", "I.4.8; I.2.10; H.5.1; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.13373", "abs": "https://arxiv.org/abs/2507.13373", "authors": ["Xiaojian Lin", "Wenxin Zhang", "Yuchu Jiang", "Wangyu Wu", "Yiran Guo", "Kangxu Wang", "Zongzheng Zhang", "Guijin Wang", "Lei Jin", "Hao Zhao"], "title": "Butter: Frequency Consistency and Hierarchical Fusion for Autonomous Driving Object Detection", "comment": "10 pages, 6 figures. Supplementary material: 8 pages, 7 figures.\n  Accepted at ACM Multimedia 2025", "summary": "Hierarchical feature representations play a pivotal role in computer vision,\nparticularly in object detection for autonomous driving. Multi-level semantic\nunderstanding is crucial for accurately identifying pedestrians, vehicles, and\ntraffic signs in dynamic environments. However, existing architectures, such as\nYOLO and DETR, struggle to maintain feature consistency across different scales\nwhile balancing detection precision and computational efficiency. To address\nthese challenges, we propose Butter, a novel object detection framework\ndesigned to enhance hierarchical feature representations for improving\ndetection robustness. Specifically, Butter introduces two key innovations:\nFrequency-Adaptive Feature Consistency Enhancement (FAFCE) Component, which\nrefines multi-scale feature consistency by leveraging adaptive frequency\nfiltering to enhance structural and boundary precision, and Progressive\nHierarchical Feature Fusion Network (PHFFNet) Module, which progressively\nintegrates multi-level features to mitigate semantic gaps and strengthen\nhierarchical feature learning. Through extensive experiments on BDD100K, KITTI,\nand Cityscapes, Butter demonstrates superior feature representation\ncapabilities, leading to notable improvements in detection accuracy while\nreducing model complexity. By focusing on hierarchical feature refinement and\nintegration, Butter provides an advanced approach to object detection that\nachieves a balance between accuracy, deployability, and computational\nefficiency in real-time autonomous driving scenarios. Our model and\nimplementation are publicly available at https://github.com/Aveiro-Lin/Butter,\nfacilitating further research and validation within the autonomous driving\ncommunity.", "AI": {"tldr": "Butter\u901a\u8fc7\u589e\u5f3a\u5206\u5c42\u7279\u5f81\u8868\u793a\uff0c\u63d0\u9ad8\u4e86\u81ea\u52a8\u9a7e\u9a76\u76ee\u6807\u68c0\u6d4b\u7684\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u67b6\u6784\uff08\u5982YOLO\u548cDETR\uff09\u96be\u4ee5\u7ef4\u6301\u8de8\u4e0d\u540c\u5c3a\u5ea6\u7684\u7279\u5f81\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u5e73\u8861\u68c0\u6d4b\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Butter\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\uff0c\u65e8\u5728\u589e\u5f3a\u5206\u5c42\u7279\u5f81\u8868\u793a\uff0c\u4ee5\u63d0\u9ad8\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u3002\u8be5\u6846\u67b6\u5f15\u5165\u4e86\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a\u9891\u7387\u81ea\u9002\u5e94\u7279\u5f81\u4e00\u81f4\u6027\u589e\u5f3a\uff08FAFCE\uff09\u7ec4\u4ef6\u548c\u6e10\u8fdb\u5f0f\u5206\u5c42\u7279\u5f81\u878d\u5408\u7f51\u7edc\uff08PHFFNet\uff09\u6a21\u5757\u3002", "result": "\u5728BDD100K\u3001KITTI\u548cCityscapes\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cButter\u5177\u6709\u5353\u8d8a\u7684\u7279\u5f81\u8868\u793a\u80fd\u529b\uff0c\u4ece\u800c\u663e\u7740\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u6a21\u578b\u590d\u6742\u5ea6\u3002", "conclusion": "Butter\u5728\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u7cbe\u5ea6\u3001\u53ef\u90e8\u7f72\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u7684\u5e73\u8861\u3002"}}
{"id": "2507.13540", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13540", "abs": "https://arxiv.org/abs/2507.13540", "authors": ["Yongyi Yang", "Hidenori Tanaka", "Wei Hu"], "title": "Provable Low-Frequency Bias of In-Context Learning of Representations", "comment": null, "summary": "In-context learning (ICL) enables large language models (LLMs) to acquire new\nbehaviors from the input sequence alone without any parameter updates. Recent\nstudies have shown that ICL can surpass the original meaning learned in\npretraining stage through internalizing the structure the data-generating\nprocess (DGP) of the prompt into the hidden representations. However, the\nmechanisms by which LLMs achieve this ability is left open. In this paper, we\npresent the first rigorous explanation of such phenomena by introducing a\nunified framework of double convergence, where hidden representations converge\nboth over context and across layers. This double convergence process leads to\nan implicit bias towards smooth (low-frequency) representations, which we prove\nanalytically and verify empirically. Our theory explains several open empirical\nobservations, including why learned representations exhibit globally structured\nbut locally distorted geometry, and why their total energy decays without\nvanishing. Moreover, our theory predicts that ICL has an intrinsic robustness\ntowards high-frequency noise, which we empirically confirm. These results\nprovide new insights into the underlying mechanisms of ICL, and a theoretical\nfoundation to study it that hopefully extends to more general data\ndistributions and settings.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u53cc\u91cd\u6536\u655b\u6846\u67b6\u6765\u89e3\u91caICL\u7684\u673a\u5236\uff0c\u89e3\u91ca\u4e86\u5b9e\u9a8c\u89c2\u5bdf\u7ed3\u679c\uff0c\u5e76\u8bc1\u660e\u4e86ICL\u5bf9\u9ad8\u9891\u566a\u58f0\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u4f7f\u5176\u80fd\u591f\u4ec5\u4ece\u8f93\u5165\u5e8f\u5217\u4e2d\u83b7\u53d6\u65b0\u7684\u884c\u4e3a\uff0c\u800c\u65e0\u9700\u4efb\u4f55\u53c2\u6570\u66f4\u65b0\u3002\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0cICL\u53ef\u4ee5\u901a\u8fc7\u5c06\u63d0\u793a\u7684\u6570\u636e\u751f\u6210\u8fc7\u7a0b\uff08DGP\uff09\u7684\u7ed3\u6784\u5185\u5316\u5230\u9690\u85cf\u8868\u793a\u4e2d\uff0c\u4ece\u800c\u8d85\u8d8a\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u5b66\u4e60\u7684\u539f\u59cb\u542b\u4e49\u3002\u7136\u800c\uff0cLLM\u5b9e\u73b0\u8fd9\u79cd\u80fd\u529b\u7684\u673a\u5236\u4ecd\u7136\u662f\u5f00\u653e\u7684\u3002", "method": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u53cc\u91cd\u6536\u655b\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5176\u4e2d\u9690\u85cf\u7684\u8868\u793a\u5728\u4e0a\u4e0b\u6587\u548c\u5c42\u4e0a\u6536\u655b\u3002\u8fd9\u79cd\u53cc\u91cd\u6536\u655b\u8fc7\u7a0b\u5bfc\u81f4\u4e86\u5bf9\u5e73\u6ed1\uff08\u4f4e\u9891\uff09\u8868\u793a\u7684\u9690\u5f0f\u504f\u5dee\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u8bc1\u660e\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u8be5\u8bba\u6587\u7684\u7406\u8bba\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48\u5b66\u4e60\u7684\u8868\u793a\u8868\u73b0\u51fa\u5168\u5c40\u7ed3\u6784\u5316\u4f46\u5c40\u90e8\u626d\u66f2\u7684\u51e0\u4f55\u5f62\u72b6\uff0c\u4ee5\u53ca\u4e3a\u4ec0\u4e48\u5b83\u4eec\u7684\u603b\u80fd\u91cf\u8870\u51cf\u800c\u6ca1\u6709\u6d88\u5931\u3002\u6b64\u5916\uff0c\u8be5\u7406\u8bba\u9884\u6d4bICL\u5bf9\u9ad8\u9891\u566a\u58f0\u5177\u6709\u5185\u5728\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u8fd9\u4e00\u70b9\u3002", "conclusion": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u53cc\u91cd\u6536\u655b\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u89e3\u91ca\u4e86ICL\u7684\u673a\u5236\uff0c\u5e76\u89e3\u91ca\u4e86\u51e0\u4e2a\u5f00\u653e\u7684\u7ecf\u9a8c\u89c2\u5bdf\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u5b9e\u4e86ICL\u5bf9\u9ad8\u9891\u566a\u58f0\u5177\u6709\u5185\u5728\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.13705", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.13705", "abs": "https://arxiv.org/abs/2507.13705", "authors": ["Cedric Waterschoot", "Nava Tintarev", "Francesco Barile"], "title": "Consistent Explainers or Unreliable Narrators? Understanding LLM-generated Group Recommendations", "comment": "Short paper accepted at the Nineteenth ACM Conference on Recommender\n  Systems (RecSys '25). Cedric Waterschoot, Nava Tintarev, and Francesco\n  Barile. 2025. Consistent Explainers or Unreliable Narrators? Understanding\n  LLM-generated Group Recommendations. Proceedings of the Nineteenth ACM\n  Conference on Recommender Systems (RecSys '25), Prague, Czech Republic. doi:\n  10.1145/3705328.3748015", "summary": "Large Language Models (LLMs) are increasingly being implemented as joint\ndecision-makers and explanation generators for Group Recommender Systems (GRS).\nIn this paper, we evaluate these recommendations and explanations by comparing\nthem to social choice-based aggregation strategies. Our results indicate that\nLLM-generated recommendations often resembled those produced by Additive\nUtilitarian (ADD) aggregation. However, the explanations typically referred to\naveraging ratings (resembling but not identical to ADD aggregation). Group\nstructure, uniform or divergent, did not impact the recommendations.\nFurthermore, LLMs regularly claimed additional criteria such as user or item\nsimilarity, diversity, or used undefined popularity metrics or thresholds. Our\nfindings have important implications for LLMs in the GRS pipeline as well as\nstandard aggregation strategies. Additional criteria in explanations were\ndependent on the number of ratings in the group scenario, indicating potential\ninefficiency of standard aggregation methods at larger item set sizes.\nAdditionally, inconsistent and ambiguous explanations undermine transparency\nand explainability, which are key motivations behind the use of LLMs for GRS.", "AI": {"tldr": "LLMs\u5728GRS\u4e2d\u751f\u6210\u7684\u63a8\u8350\u7c7b\u4f3c\u4e8eADD\u805a\u5408\uff0c\u4f46\u5176\u89e3\u91ca\u4e0d\u4e00\u81f4\u4e14\u542b\u7cca\uff0c\u5f71\u54cd\u4e86\u900f\u660e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8d8a\u6765\u8d8a\u591a\u5730\u88ab\u7528\u4f5c\u7fa4\u4f53\u63a8\u8350\u7cfb\u7edf\uff08GRS\uff09\u7684\u8054\u5408\u51b3\u7b56\u8005\u548c\u89e3\u91ca\u751f\u6210\u5668\u3002", "method": "\u901a\u8fc7\u5c06LLM\u751f\u6210\u7684\u63a8\u8350\u548c\u89e3\u91ca\u4e0e\u57fa\u4e8e\u793e\u4f1a\u9009\u62e9\u7684\u805a\u5408\u7b56\u7565\u8fdb\u884c\u6bd4\u8f83\u6765\u8bc4\u4f30\u8fd9\u4e9b\u63a8\u8350\u548c\u89e3\u91ca\u3002", "result": "LLM\u751f\u6210\u7684\u63a8\u8350\u901a\u5e38\u7c7b\u4f3c\u4e8eADD\u805a\u5408\u4ea7\u751f\u7684\u63a8\u8350\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u89e3\u91ca\u901a\u5e38\u6307\u7684\u662f\u5e73\u5747\u8bc4\u5206\uff08\u7c7b\u4f3c\u4e8e\u4f46\u4e0d\u7b49\u540c\u4e8eADD\u805a\u5408\uff09\u3002\u7ec4\u7ed3\u6784\uff08\u7edf\u4e00\u6216\u53d1\u6563\uff09\u5bf9\u63a8\u8350\u6ca1\u6709\u5f71\u54cd\u3002\u6b64\u5916\uff0cLLMs\u7ecf\u5e38\u58f0\u79f0\u989d\u5916\u7684\u6807\u51c6\uff0c\u5982\u7528\u6237\u6216\u9879\u76ee\u76f8\u4f3c\u6027\u3001\u591a\u6837\u6027\uff0c\u6216\u4f7f\u7528\u672a\u5b9a\u4e49\u7684\u53d7\u6b22\u8fce\u7a0b\u5ea6\u6307\u6807\u6216\u9608\u503c\u3002\u89e3\u91ca\u4e2d\u7684\u9644\u52a0\u6807\u51c6\u53d6\u51b3\u4e8e\u7fa4\u4f53\u573a\u666f\u4e2d\u8bc4\u5206\u7684\u6570\u91cf\uff0c\u8fd9\u8868\u660e\u6807\u51c6\u805a\u5408\u65b9\u6cd5\u5728\u66f4\u5927\u7684\u9879\u76ee\u96c6\u5927\u5c0f\u4e0a\u53ef\u80fd\u6548\u7387\u4f4e\u4e0b\u3002", "conclusion": "LLMs\u5728GRS\u4e2d\u7684\u89e3\u91ca\u4e0d\u4e00\u81f4\u4e14\u542b\u7cca\uff0c\u8fd9\u524a\u5f31\u4e86\u900f\u660e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u800c\u8fd9\u6b63\u662f\u4f7f\u7528LLMs for GRS\u7684\u5173\u952e\u52a8\u673a\u3002"}}
{"id": "2507.13474", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13474", "abs": "https://arxiv.org/abs/2507.13474", "authors": ["Liang Lin", "Zhihao Xu", "Xuehai Tang", "Shi Liu", "Biyu Zhou", "Fuqing Zhu", "Jizhong Han", "Songlin Hu"], "title": "Paper Summary Attack: Jailbreaking LLMs through LLM Safety Papers", "comment": null, "summary": "The safety of large language models (LLMs) has garnered significant research\nattention. In this paper, we argue that previous empirical studies demonstrate\nLLMs exhibit a propensity to trust information from authoritative sources, such\nas academic papers, implying new possible vulnerabilities. To verify this\npossibility, a preliminary analysis is designed to illustrate our two findings.\nBased on this insight, a novel jailbreaking method, Paper Summary Attack\n(\\llmname{PSA}), is proposed. It systematically synthesizes content from either\nattack-focused or defense-focused LLM safety paper to construct an adversarial\nprompt template, while strategically infilling harmful query as adversarial\npayloads within predefined subsections. Extensive experiments show significant\nvulnerabilities not only in base LLMs, but also in state-of-the-art reasoning\nmodel like Deepseek-R1. PSA achieves a 97\\% attack success rate (ASR) on\nwell-aligned models like Claude3.5-Sonnet and an even higher 98\\% ASR on\nDeepseek-R1. More intriguingly, our work has further revealed diametrically\nopposed vulnerability bias across different base models, and even between\ndifferent versions of the same model, when exposed to either attack-focused or\ndefense-focused papers. This phenomenon potentially indicates future research\nclues for both adversarial methodologies and safety alignment.Code is available\nat https://github.com/233liang/Paper-Summary-Attack", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u5229\u7528llm\u5bf9\u5b66\u672f\u4fe1\u606f\u7684\u4fe1\u4efb\u503e\u5411\uff0c\u53d1\u73b0llm\u5b58\u5728\u5b89\u5168\u6f0f\u6d1e\uff0c\u5e76\u5728\u4e0d\u540c\u6a21\u578b\u4e2d\u89c2\u5bdf\u5230\u4e0d\u540c\u7684\u6f0f\u6d1e\u504f\u89c1\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u7684\u5b89\u5168\u6027\u53d7\u5230\u4e86\u5e7f\u6cdb\u7684\u7814\u7a76\u5173\u6ce8\u3002\u4e4b\u524d\u7684\u7ecf\u9a8c\u7814\u7a76\u8868\u660e\uff0cllm\u8868\u73b0\u51fa\u4fe1\u4efb\u6765\u81ea\u5b66\u672f\u8bba\u6587\u7b49\u6743\u5a01\u6765\u6e90\u7684\u4fe1\u606f\u7684\u503e\u5411\uff0c\u8fd9\u610f\u5473\u7740\u53ef\u80fd\u5b58\u5728\u65b0\u7684\u5b89\u5168\u6f0f\u6d1e\u3002\u672c\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u8fd9\u79cd\u53ef\u80fd\u6027\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u8bba\u6587\u6458\u8981\u653b\u51fb(psa)\u7684\u65b0\u578bjailbreaking\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u7cfb\u7edf\u5730\u5408\u6210\u4e86\u6765\u81ea\u4ee5\u653b\u51fb\u4e3a\u4e2d\u5fc3\u6216\u4ee5\u9632\u5fa1\u4e3a\u4e2d\u5fc3\u7684llm\u5b89\u5168\u8bba\u6587\u7684\u5185\u5bb9\uff0c\u4ee5\u6784\u5efa\u5bf9\u6297\u6027\u63d0\u793a\u6a21\u677f\uff0c\u540c\u65f6\u5728\u9884\u5b9a\u4e49\u7684\u5c0f\u8282\u4e2d\u7b56\u7565\u6027\u5730\u586b\u5145\u6709\u5bb3\u67e5\u8be2\u4f5c\u4e3a\u5bf9\u6297\u6027\u6709\u6548\u8f7d\u8377\u3002", "result": "\u8bba\u6587\u6458\u8981\u653b\u51fb(psa)\u5728\u50cfclaude3.5-sonnet\u8fd9\u6837\u5bf9\u9f50\u826f\u597d\u7684\u6a21\u578b\u4e0a\u8fbe\u5230\u4e8697%\u7684\u653b\u51fb\u6210\u529f\u7387(asr)\uff0c\u5728deepseek-r1\u4e0a\u8fbe\u5230\u4e86\u66f4\u9ad8\u768498%\u7684asr\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u8fd8\u63ed\u793a\u4e86\u4e0d\u540c\u57fa\u7840\u6a21\u578b\u4e4b\u95f4\uff0c\u751a\u81f3\u540c\u4e00\u6a21\u578b\u7684\u4e0d\u540c\u7248\u672c\u4e4b\u95f4\uff0c\u5728\u9762\u5bf9\u653b\u51fb\u6216\u9632\u5fa1\u6027\u8bba\u6587\u65f6\uff0c\u5b58\u5728\u622a\u7136\u76f8\u53cd\u7684\u6f0f\u6d1e\u504f\u89c1\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u5728\u9762\u5bf9\u6765\u81ea\u5b66\u672f\u8bba\u6587\u7b49\u6743\u5a01\u6765\u6e90\u7684\u4fe1\u606f\u65f6\uff0c\u5b58\u5728\u4fe1\u4efb\u503e\u5411\uff0c\u8fd9\u5bfc\u81f4\u4e86\u65b0\u7684\u5b89\u5168\u6f0f\u6d1e\u3002\u901a\u8fc7\u63d0\u51fa\u7684\u8bba\u6587\u6458\u8981\u653b\u51fb(psa)\u65b9\u6cd5\uff0c\u8be5\u7814\u7a76\u53d1\u73b0llm\u4e0d\u4ec5\u5728\u57fa\u7840\u6a21\u578b\u4e2d\u5b58\u5728\u6f0f\u6d1e\uff0c\u800c\u4e14\u5728\u6700\u5148\u8fdb\u7684\u63a8\u7406\u6a21\u578b(\u5982deepseek-r1)\u4e2d\u4e5f\u5b58\u5728\u6f0f\u6d1e\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u63ed\u793a\u4e86\u4e0d\u540c\u57fa\u7840\u6a21\u578b\u4e4b\u95f4\uff0c\u751a\u81f3\u540c\u4e00\u6a21\u578b\u7684\u4e0d\u540c\u7248\u672c\u4e4b\u95f4\uff0c\u5728\u9762\u5bf9\u653b\u51fb\u6216\u9632\u5fa1\u6027\u8bba\u6587\u65f6\uff0c\u5b58\u5728\u622a\u7136\u76f8\u53cd\u7684\u6f0f\u6d1e\u504f\u89c1\u3002"}}
{"id": "2507.13768", "categories": ["cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.13768", "abs": "https://arxiv.org/abs/2507.13768", "authors": ["Renato Ghisellini", "Remo Pareschi", "Marco Pedroni", "Giovanni Battista Raggi"], "title": "From Extraction to Synthesis: Entangled Heuristics for Agent-Augmented Strategic Reasoning", "comment": "Peer-reviewed full paper accepted through a double-blind review\n  process at the HAR 2025 conference (https://har-conf.eu/). The official\n  version will appear in a volume of the Lecture Notes in Computer Science\n  (LNCS) series", "summary": "We present a hybrid architecture for agent-augmented strategic reasoning,\ncombining heuristic extraction, semantic activation, and compositional\nsynthesis. Drawing on sources ranging from classical military theory to\ncontemporary corporate strategy, our model activates and composes multiple\nheuristics through a process of semantic interdependence inspired by research\nin quantum cognition. Unlike traditional decision engines that select the best\nrule, our system fuses conflicting heuristics into coherent and\ncontext-sensitive narratives, guided by semantic interaction modeling and\nrhetorical framing. We demonstrate the framework via a Meta vs. FTC case study,\nwith preliminary validation through semantic metrics. Limitations and\nextensions (e.g., dynamic interference tuning) are discussed.", "AI": {"tldr": "A hybrid architecture for agent-augmented strategic reasoning is presented, combining heuristic extraction, semantic activation, and compositional synthesis. The framework is demonstrated via a Meta vs. FTC case study.", "motivation": "To present a hybrid architecture for agent-augmented strategic reasoning, combining heuristic extraction, semantic activation, and compositional synthesis. Drawing on sources ranging from classical military theory to contemporary corporate strategy", "method": "The model activates and composes multiple heuristics through a process of semantic interdependence inspired by research in quantum cognition. Unlike traditional decision engines that select the best rule, our system fuses conflicting heuristics into coherent and context-sensitive narratives, guided by semantic interaction modeling and rhetorical framing.", "result": "The system fuses conflicting heuristics into coherent and context-sensitive narratives", "conclusion": "The framework is demonstrated via a Meta vs. FTC case study, with preliminary validation through semantic metrics. Limitations and extensions (e.g., dynamic interference tuning) are discussed."}}
{"id": "2507.13542", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13542", "abs": "https://arxiv.org/abs/2507.13542", "authors": ["Beka Begiashvili", "Carlos J. Fernandez-Candel", "Mat\u00edas P\u00e9rez Paredes"], "title": "Acoustic Index: A Novel AI-Driven Parameter for Cardiac Disease Risk Stratification Using Echocardiography", "comment": null, "summary": "Traditional echocardiographic parameters such as ejection fraction (EF) and\nglobal longitudinal strain (GLS) have limitations in the early detection of\ncardiac dysfunction. EF often remains normal despite underlying pathology, and\nGLS is influenced by load conditions and vendor variability. There is a growing\nneed for reproducible, interpretable, and operator-independent parameters that\ncapture subtle and global cardiac functional alterations.\n  We introduce the Acoustic Index, a novel AI-derived echocardiographic\nparameter designed to quantify cardiac dysfunction from standard ultrasound\nviews. The model combines Extended Dynamic Mode Decomposition (EDMD) based on\nKoopman operator theory with a hybrid neural network that incorporates clinical\nmetadata. Spatiotemporal dynamics are extracted from echocardiographic\nsequences to identify coherent motion patterns. These are weighted via\nattention mechanisms and fused with clinical data using manifold learning,\nresulting in a continuous score from 0 (low risk) to 1 (high risk).\n  In a prospective cohort of 736 patients, encompassing various cardiac\npathologies and normal controls, the Acoustic Index achieved an area under the\ncurve (AUC) of 0.89 in an independent test set. Cross-validation across five\nfolds confirmed the robustness of the model, showing that both sensitivity and\nspecificity exceeded 0.8 when evaluated on independent data. Threshold-based\nanalysis demonstrated stable trade-offs between sensitivity and specificity,\nwith optimal discrimination near this threshold.\n  The Acoustic Index represents a physics-informed, interpretable AI biomarker\nfor cardiac function. It shows promise as a scalable, vendor-independent tool\nfor early detection, triage, and longitudinal monitoring. Future directions\ninclude external validation, longitudinal studies, and adaptation to\ndisease-specific classifiers.", "AI": {"tldr": "The paper introduces the Acoustic Index, a novel AI-based tool for quantifying cardiac dysfunction from echocardiograms, showing high accuracy in detecting cardiac issues early on.", "motivation": "Traditional echocardiographic parameters have limitations in early detection of cardiac dysfunction, creating a need for more reproducible, interpretable, and operator-independent parameters.", "method": "The study introduces the Acoustic Index, an AI-derived echocardiographic parameter that combines Extended Dynamic Mode Decomposition (EDMD) with a hybrid neural network incorporating clinical metadata to quantify cardiac dysfunction.", "result": "In a prospective cohort of 736 patients, the Acoustic Index achieved an AUC of 0.89 in an independent test set, with sensitivity and specificity exceeding 0.8 in cross-validation.", "conclusion": "The Acoustic Index is a promising AI biomarker for cardiac function, offering potential for early detection, triage, and longitudinal monitoring. Future research will focus on external validation, longitudinal studies, and disease-specific adaptations."}}
{"id": "2507.13827", "categories": ["cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13827", "abs": "https://arxiv.org/abs/2507.13827", "authors": ["Hosein Azarbonyad", "Zi Long Zhu", "Georgios Cheirmpos", "Zubair Afzal", "Vikrant Yadav", "Georgios Tsatsaronis"], "title": "Question-Answer Extraction from Scientific Articles Using Knowledge Graphs and Large Language Models", "comment": "SIGIR 2025", "summary": "When deciding to read an article or incorporate it into their research,\nscholars often seek to quickly identify and understand its main ideas. In this\npaper, we aim to extract these key concepts and contributions from scientific\narticles in the form of Question and Answer (QA) pairs. We propose two distinct\napproaches for generating QAs. The first approach involves selecting salient\nparagraphs, using a Large Language Model (LLM) to generate questions, ranking\nthese questions by the likelihood of obtaining meaningful answers, and\nsubsequently generating answers. This method relies exclusively on the content\nof the articles. However, assessing an article's novelty typically requires\ncomparison with the existing literature. Therefore, our second approach\nleverages a Knowledge Graph (KG) for QA generation. We construct a KG by\nfine-tuning an Entity Relationship (ER) extraction model on scientific articles\nand using it to build the graph. We then employ a salient triplet extraction\nmethod to select the most pertinent ERs per article, utilizing metrics such as\nthe centrality of entities based on a triplet TF-IDF-like measure. This measure\nassesses the saliency of a triplet based on its importance within the article\ncompared to its prevalence in the literature. For evaluation, we generate QAs\nusing both approaches and have them assessed by Subject Matter Experts (SMEs)\nthrough a set of predefined metrics to evaluate the quality of both questions\nand answers. Our evaluations demonstrate that the KG-based approach effectively\ncaptures the main ideas discussed in the articles. Furthermore, our findings\nindicate that fine-tuning the ER extraction model on our scientific corpus is\ncrucial for extracting high-quality triplets from such documents.", "AI": {"tldr": "Two QA generation approaches (LLM and KG) are proposed to extract key concepts from scientific articles. KG approach performs better, and fine-tuning ER extraction model is crucial.", "motivation": "Scholars need to quickly identify and understand the main ideas of scientific articles.", "method": "Two approaches: 1) LLM to generate questions from salient paragraphs and rank them. 2) KG for QA generation using a fine-tuned ER extraction model and salient triplet extraction.", "result": "KG-based approach is effective; fine-tuning ER extraction model is crucial.", "conclusion": "KG-based approach effectively captures the main ideas discussed in the articles. Fine-tuning the ER extraction model on scientific corpus is crucial for extracting high-quality triplets."}}
{"id": "2507.13490", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13490", "abs": "https://arxiv.org/abs/2507.13490", "authors": ["Siqi Shen", "Mehar Singh", "Lajanugen Logeswaran", "Moontae Lee", "Honglak Lee", "Rada Mihalcea"], "title": "Revisiting LLM Value Probing Strategies: Are They Robust and Expressive?", "comment": null, "summary": "There has been extensive research on assessing the value orientation of Large\nLanguage Models (LLMs) as it can shape user experiences across demographic\ngroups. However, several challenges remain. First, while the Multiple Choice\nQuestion (MCQ) setting has been shown to be vulnerable to perturbations, there\nis no systematic comparison of probing methods for value probing. Second, it is\nunclear to what extent the probed values capture in-context information and\nreflect models' preferences for real-world actions. In this paper, we evaluate\nthe robustness and expressiveness of value representations across three widely\nused probing strategies. We use variations in prompts and options, showing that\nall methods exhibit large variances under input perturbations. We also\nintroduce two tasks studying whether the values are responsive to demographic\ncontext, and how well they align with the models' behaviors in value-related\nscenarios. We show that the demographic context has little effect on the\nfree-text generation, and the models' values only weakly correlate with their\npreference for value-based actions. Our work highlights the need for a more\ncareful examination of LLM value probing and awareness of its limitations.", "AI": {"tldr": "\u6211\u4eec\u8bc4\u4f30\u4e86\u4e09\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u63a2\u6d4b\u7b56\u7565\u7684\u4ef7\u503c\u8868\u793a\u7684\u7a33\u5065\u6027\u548c\u8868\u8fbe\u6027\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4ef7\u503c\u89c2\u53d6\u5411\u5df2\u7ecf\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u7814\u7a76\uff0c\u56e0\u4e3a\u5b83\u4f1a\u5f71\u54cd\u8de8\u4eba\u53e3\u7fa4\u4f53\u7684\u7528\u6237\u4f53\u9a8c\u3002\u7136\u800c\uff0c\u4ecd\u7136\u5b58\u5728\u4e00\u4e9b\u6311\u6218\u3002\u9996\u5148\uff0c\u867d\u7136\u591a\u9879\u9009\u62e9\u9898\uff08MCQ\uff09\u8bbe\u7f6e\u5df2\u88ab\u8bc1\u660e\u5bb9\u6613\u53d7\u5230\u6270\u52a8\u7684\u5f71\u54cd\uff0c\u4f46\u5bf9\u4e8e\u4ef7\u503c\u89c2\u63a2\u6d4b\u7684\u63a2\u6d4b\u65b9\u6cd5\u6ca1\u6709\u7cfb\u7edf\u7684\u6bd4\u8f83\u3002\u5176\u6b21\uff0c\u76ee\u524d\u8fd8\u4e0d\u6e05\u695a\u63a2\u6d4b\u5230\u7684\u4ef7\u503c\u89c2\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u6355\u6349\u4e86\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5e76\u53cd\u6620\u4e86\u6a21\u578b\u5bf9\u73b0\u5b9e\u4e16\u754c\u884c\u4e3a\u7684\u504f\u597d\u3002", "method": "\u6211\u4eec\u4f7f\u7528\u4e86\u63d0\u793a\u548c\u9009\u9879\u7684\u53d8\u5316\uff0c\u5c55\u793a\u4e86\u6240\u6709\u65b9\u6cd5\u5728\u8f93\u5165\u6270\u52a8\u4e0b\u90fd\u8868\u73b0\u51fa\u5f88\u5927\u7684\u5dee\u5f02\u3002\u6211\u4eec\u8fd8\u4ecb\u7ecd\u4e86\u4e24\u4e2a\u4efb\u52a1\uff0c\u7814\u7a76\u8fd9\u4e9b\u4ef7\u503c\u89c2\u662f\u5426\u5bf9\u4eba\u53e3\u80cc\u666f\u654f\u611f\uff0c\u4ee5\u53ca\u5b83\u4eec\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u4e0e\u6a21\u578b\u5728\u4e0e\u4ef7\u503c\u89c2\u76f8\u5173\u7684\u573a\u666f\u4e2d\u7684\u884c\u4e3a\u76f8\u4e00\u81f4\u3002", "result": "\u4eba\u53e3\u80cc\u666f\u5bf9\u81ea\u7531\u6587\u672c\u751f\u6210\u5f71\u54cd\u5f88\u5c0f\uff0c\u6a21\u578b\u7684\u4ef7\u503c\u89c2\u4e0e\u5176\u5bf9\u57fa\u4e8e\u4ef7\u503c\u89c2\u7684\u884c\u4e3a\u7684\u504f\u597d\u53ea\u6709\u5fae\u5f31\u7684\u76f8\u5173\u6027\u3002", "conclusion": "LLM\u7684\u4ef7\u503c\u89c2\u63a2\u7a76\u9700\u8981\u66f4\u8c28\u614e\u7684\u5ba1\u67e5\uff0c\u5e76\u610f\u8bc6\u5230\u5176\u5c40\u9650\u6027\u3002"}}
{"id": "2507.13825", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13825", "abs": "https://arxiv.org/abs/2507.13825", "authors": ["Haoyang Li", "Yuming Xu", "Yiming Li", "Hanmo Liu", "Darian Li", "Chen Jason Zhang", "Lei Chen", "Qing Li"], "title": "When Speed meets Accuracy: an Efficient and Effective Graph Model for Temporal Link Prediction", "comment": "Submitted in 2024. Accepted in 2025", "summary": "Temporal link prediction in dynamic graphs is a critical task with\napplications in diverse domains such as social networks, recommendation\nsystems, and e-commerce platforms. While existing Temporal Graph Neural\nNetworks (T-GNNs) have achieved notable success by leveraging complex\narchitectures to model temporal and structural dependencies, they often suffer\nfrom scalability and efficiency challenges due to high computational overhead.\nIn this paper, we propose EAGLE, a lightweight framework that integrates\nshort-term temporal recency and long-term global structural patterns. EAGLE\nconsists of a time-aware module that aggregates information from a node's most\nrecent neighbors to reflect its immediate preferences, and a structure-aware\nmodule that leverages temporal personalized PageRank to capture the influence\nof globally important nodes. To balance these attributes, EAGLE employs an\nadaptive weighting mechanism to dynamically adjust their contributions based on\ndata characteristics. Also, EAGLE eliminates the need for complex multi-hop\nmessage passing or memory-intensive mechanisms, enabling significant\nimprovements in efficiency. Extensive experiments on seven real-world temporal\ngraphs demonstrate that EAGLE consistently achieves superior performance\nagainst state-of-the-art T-GNNs in both effectiveness and efficiency,\ndelivering more than a 50x speedup over effective transformer-based T-GNNs.", "AI": {"tldr": "EAGLE\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u5b83\u96c6\u6210\u4e86\u77ed\u671f\u65f6\u95f4\u65b0\u8fd1\u5ea6\u548c\u957f\u671f\u5168\u5c40\u7ed3\u6784\u6a21\u5f0f\uff0c\u5728\u9884\u6d4b\u52a8\u6001\u56fe\u4e2d\u7684\u65f6\u95f4\u94fe\u63a5\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u56fe\u795e\u7ecf\u7f51\u7edc (T-GNN) \u901a\u8fc7\u5229\u7528\u590d\u6742\u7684\u67b6\u6784\u6765\u5efa\u6a21\u65f6\u95f4\u548c\u7ed3\u6784\u4f9d\u8d56\u6027\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u529f\uff0c\u4f46\u7531\u4e8e\u8ba1\u7b97\u5f00\u9500\u9ad8\uff0c\u5b83\u4eec\u901a\u5e38\u9762\u4e34\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u6311\u6218\u3002", "method": "EAGLE\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u77ed\u671f\u65f6\u95f4\u65b0\u8fd1\u5ea6\u548c\u957f\u671f\u5168\u5c40\u7ed3\u6784\u6a21\u5f0f\u3002EAGLE \u5305\u542b\u4e00\u4e2a\u65f6\u95f4\u611f\u77e5\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u805a\u5408\u6765\u81ea\u8282\u70b9\u6700\u8fd1\u90bb\u5c45\u7684\u4fe1\u606f\u4ee5\u53cd\u6620\u5176\u76f4\u63a5\u504f\u597d\uff0c\u4ee5\u53ca\u4e00\u4e2a\u7ed3\u6784\u611f\u77e5\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u5229\u7528\u65f6\u95f4\u4e2a\u6027\u5316 PageRank \u6765\u6355\u83b7\u5168\u5c40\u91cd\u8981\u8282\u70b9\u7684\u5f71\u54cd\u3002", "result": "EAGLE \u5728\u4e03\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u65f6\u95f4\u56fe\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8868\u660e EAGLE \u5728\u6709\u6548\u6027\u548c\u6548\u7387\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684 T-GNN\u3002", "conclusion": "EAGLE\u5728\u6548\u679c\u548c\u6548\u7387\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684 T-GNN\uff0c\u4e0e\u6709\u6548\u7684\u57fa\u4e8e Transformer \u7684 T-GNN \u76f8\u6bd4\uff0c\u901f\u5ea6\u63d0\u9ad8\u4e86 50 \u500d\u4ee5\u4e0a\u3002"}}
{"id": "2507.13378", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13378", "abs": "https://arxiv.org/abs/2507.13378", "authors": ["Yuqi Cheng", "Yunkang Cao", "Haiming Yao", "Wei Luo", "Cheng Jiang", "Hui Zhang", "Weiming Shen"], "title": "A Comprehensive Survey for Real-World Industrial Defect Detection: Challenges, Approaches, and Prospects", "comment": "27 pages, 7 figures", "summary": "Industrial defect detection is vital for upholding product quality across\ncontemporary manufacturing systems. As the expectations for precision,\nautomation, and scalability intensify, conventional inspection approaches are\nincreasingly found wanting in addressing real-world demands. Notable progress\nin computer vision and deep learning has substantially bolstered defect\ndetection capabilities across both 2D and 3D modalities. A significant\ndevelopment has been the pivot from closed-set to open-set defect detection\nframeworks, which diminishes the necessity for extensive defect annotations and\nfacilitates the recognition of novel anomalies. Despite such strides, a\ncohesive and contemporary understanding of industrial defect detection remains\nelusive. Consequently, this survey delivers an in-depth analysis of both\nclosed-set and open-set defect detection strategies within 2D and 3D\nmodalities, charting their evolution in recent years and underscoring the\nrising prominence of open-set techniques. We distill critical challenges\ninherent in practical detection environments and illuminate emerging trends,\nthereby providing a current and comprehensive vista of this swiftly progressing\nfield.", "AI": {"tldr": "This survey analyzes 2D and 3D defect detection strategies, focusing on open-set techniques, challenges, and trends in the rapidly advancing field.", "motivation": "Conventional inspection approaches are increasingly found wanting in addressing real-world demands. A cohesive and contemporary understanding of industrial defect detection remains elusive.", "method": "a survey of both closed-set and open-set defect detection strategies within 2D and 3D modalities", "result": "undisclosed", "conclusion": "This survey provides an in-depth analysis of both closed-set and open-set defect detection strategies within 2D and 3D modalities, charting their evolution in recent years and underscoring the rising prominence of open-set techniques. It also highlights critical challenges and emerging trends."}}
{"id": "2507.13556", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13556", "abs": "https://arxiv.org/abs/2507.13556", "authors": ["Rui Wang", "Steven Klee", "Alexis Roos"], "title": "Time Series Forecastability Measures", "comment": null, "summary": "This paper proposes using two metrics to quantify the forecastability of time\nseries prior to model development: the spectral predictability score and the\nlargest Lyapunov exponent. Unlike traditional model evaluation metrics, these\nmeasures assess the inherent forecastability characteristics of the data before\nany forecast attempts. The spectral predictability score evaluates the strength\nand regularity of frequency components in the time series, whereas the Lyapunov\nexponents quantify the chaos and stability of the system generating the data.\nWe evaluated the effectiveness of these metrics on both synthetic and\nreal-world time series from the M5 forecast competition dataset. Our results\ndemonstrate that these two metrics can correctly reflect the inherent\nforecastability of a time series and have a strong correlation with the actual\nforecast performance of various models. By understanding the inherent\nforecastability of time series before model training, practitioners can focus\ntheir planning efforts on products and supply chain levels that are more\nforecastable, while setting appropriate expectations or seeking alternative\nstrategies for products with limited forecastability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5728\u6a21\u578b\u8bad\u7ec3\u524d\u7528\u8c31\u53ef\u9884\u6d4b\u6027\u8bc4\u5206\u548c\u6700\u5927 Lyapunov \u6307\u6570\u6765\u91cf\u5316\u65f6\u95f4\u5e8f\u5217\u7684\u53ef\u9884\u6d4b\u6027\uff0c\u7ed3\u679c\u8868\u660e\u8fd9\u4e9b\u6307\u6807\u4e0e\u5b9e\u9645\u9884\u6d4b\u6027\u80fd\u5bc6\u5207\u76f8\u5173\u3002", "motivation": "\u4e0e\u4f20\u7edf\u7684\u6a21\u578b\u8bc4\u4f30\u6307\u6807\u4e0d\u540c\uff0c\u8fd9\u4e9b\u6307\u6807\u5728\u4efb\u4f55\u9884\u6d4b\u5c1d\u8bd5\u4e4b\u524d\u8bc4\u4f30\u6570\u636e\u7684\u56fa\u6709\u53ef\u9884\u6d4b\u6027\u7279\u5f81\u3002", "method": "\u63d0\u51fa\u4e86\u4f7f\u7528\u4e24\u4e2a\u6307\u6807\u6765\u91cf\u5316\u6a21\u578b\u5f00\u53d1\u524d\u7684\u65f6\u95f4\u5e8f\u5217\u7684\u53ef\u9884\u6d4b\u6027\uff1a\u8c31\u53ef\u9884\u6d4b\u6027\u8bc4\u5206\u548c\u6700\u5927 Lyapunov \u6307\u6570\u3002", "result": "\u8fd9\u4e24\u4e2a\u6307\u6807\u53ef\u4ee5\u6b63\u786e\u53cd\u6620\u65f6\u95f4\u5e8f\u5217\u7684\u56fa\u6709\u53ef\u9884\u6d4b\u6027\uff0c\u5e76\u4e14\u4e0e\u5404\u79cd\u6a21\u578b\u7684\u5b9e\u9645\u9884\u6d4b\u6027\u80fd\u5bc6\u5207\u76f8\u5173\u3002", "conclusion": "\u4f7f\u7528\u8fd9\u4e24\u4e2a\u6307\u6807\u53ef\u4ee5\u6b63\u786e\u53cd\u6620\u65f6\u95f4\u5e8f\u5217\u7684\u56fa\u6709\u53ef\u9884\u6d4b\u6027\uff0c\u5e76\u4e14\u4e0e\u5404\u79cd\u6a21\u578b\u7684\u5b9e\u9645\u9884\u6d4b\u6027\u80fd\u5bc6\u5207\u76f8\u5173\u3002"}}
{"id": "2507.14079", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14079", "abs": "https://arxiv.org/abs/2507.14079", "authors": ["Garapati Keerthana", "Manik Gupta"], "title": "DENSE: Longitudinal Progress Note Generation with Temporal Modeling of Heterogeneous Clinical Notes Across Hospital Visits", "comment": null, "summary": "Progress notes are among the most clinically meaningful artifacts in an\nElectronic Health Record (EHR), offering temporally grounded insights into a\npatient's evolving condition, treatments, and care decisions. Despite their\nimportance, they are severely underrepresented in large-scale EHR datasets. For\ninstance, in the widely used Medical Information Mart for Intensive Care III\n(MIMIC-III) dataset, only about $8.56\\%$ of hospital visits include progress\nnotes, leaving gaps in longitudinal patient narratives. In contrast, the\ndataset contains a diverse array of other note types, each capturing different\naspects of care.\n  We present DENSE (Documenting Evolving Progress Notes from Scattered\nEvidence), a system designed to align with clinical documentation workflows by\nsimulating how physicians reference past encounters while drafting progress\nnotes. The system introduces a fine-grained note categorization and a temporal\nalignment mechanism that organizes heterogeneous notes across visits into\nstructured, chronological inputs. At its core, DENSE leverages a clinically\ninformed retrieval strategy to identify temporally and semantically relevant\ncontent from both current and prior visits. This retrieved evidence is used to\nprompt a large language model (LLM) to generate clinically coherent and\ntemporally aware progress notes.\n  We evaluate DENSE on a curated cohort of patients with multiple visits and\ncomplete progress note documentation. The generated notes demonstrate strong\nlongitudinal fidelity, achieving a temporal alignment ratio of $1.089$,\nsurpassing the continuity observed in original notes. By restoring narrative\ncoherence across fragmented documentation, our system supports improved\ndownstream tasks such as summarization, predictive modeling, and clinical\ndecision support, offering a scalable solution for LLM-driven note synthesis in\nreal-world healthcare settings.", "AI": {"tldr": "DENSE is a system designed to generate progress notes by retrieving relevant information from heterogeneous notes across visits and using it to prompt a large language model (LLM).", "motivation": "Progress notes are severely underrepresented in large-scale EHR datasets, leaving gaps in longitudinal patient narratives.", "method": "DENSE introduces a fine-grained note categorization and a temporal alignment mechanism that organizes heterogeneous notes across visits into structured, chronological inputs. At its core, DENSE leverages a clinically informed retrieval strategy to identify temporally and semantically relevant content from both current and prior visits. This retrieved evidence is used to prompt a large language model (LLM) to generate clinically coherent and temporally aware progress notes.", "result": "The generated notes demonstrate strong longitudinal fidelity, achieving a temporal alignment ratio of $1.089$, surpassing the continuity observed in original notes.", "conclusion": "DENSE can restore narrative coherence across fragmented documentation, supporting improved downstream tasks such as summarization, predictive modeling, and clinical decision support, offering a scalable solution for LLM-driven note synthesis in real-world healthcare settings."}}
{"id": "2507.13501", "categories": ["cs.CL", "math.RA", "q-bio.NC", "91F20, 16Y60, 16T05, 92C20"], "pdf": "https://arxiv.org/pdf/2507.13501", "abs": "https://arxiv.org/abs/2507.13501", "authors": ["Matilde Marcolli", "Robert C. Berwick"], "title": "Encoding syntactic objects and Merge operations in function spaces", "comment": "40 pages, LaTeX, 4 png figures", "summary": "We provide a mathematical argument showing that, given a representation of\nlexical items as functions (wavelets, for instance) in some function space, it\nis possible to construct a faithful representation of arbitrary syntactic\nobjects in the same function space. This space can be endowed with a\ncommutative non-associative semiring structure built using the second Renyi\nentropy. The resulting representation of syntactic objects is compatible with\nthe magma structure. The resulting set of functions is an algebra over an\noperad, where the operations in the operad model circuits that transform the\ninput wave forms into a combined output that encodes the syntactic structure.\nThe action of Merge on workspaces is faithfully implemented as action on these\ncircuits, through a coproduct and a Hopf algebra Markov chain. The results\nobtained here provide a constructive argument showing the theoretical\npossibility of a neurocomputational realization of the core computational\nstructure of syntax. We also present a particular case of this general\nconstruction where this type of realization of Merge is implemented as a cross\nfrequency phase synchronization on sinusoidal waves. This also shows that Merge\ncan be expressed in terms of the successor function of a semiring, thus\nclarifying the well known observation of its similarities with the successor\nfunction of arithmetic.", "AI": {"tldr": "The paper provides a mathematical argument for a neurocomputational realization of syntax, showing how syntactic structures can be represented in a function space and how Merge can be implemented as cross-frequency phase synchronization.", "motivation": "To provide a mathematical argument showing that it is possible to construct a faithful representation of arbitrary syntactic objects in some function space.", "method": "The authors construct a faithful representation of arbitrary syntactic objects in a function space endowed with a commutative non-associative semiring structure built using the second Renyi entropy. The action of Merge on workspaces is faithfully implemented as action on these circuits, through a coproduct and a Hopf algebra Markov chain. A particular case of this general construction is presented where this type of realization of Merge is implemented as a cross frequency phase synchronization on sinusoidal waves.", "result": "The resulting representation of syntactic objects is compatible with the magma structure. The resulting set of functions is an algebra over an operad. Merge can be expressed in terms of the successor function of a semiring.", "conclusion": "This paper shows the theoretical possibility of a neurocomputational realization of the core computational structure of syntax."}}
{"id": "2507.13846", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13846", "abs": "https://arxiv.org/abs/2507.13846", "authors": ["Kathrin Korte", "Christian Medeiros Adriano", "Sona Ghahremani", "Holger Giese"], "title": "Causal Knowledge Transfer for Multi-Agent Reinforcement Learning in Dynamic Environments", "comment": null, "summary": "[Context] Multi-agent reinforcement learning (MARL) has achieved notable\nsuccess in environments where agents must learn coordinated behaviors. However,\ntransferring knowledge across agents remains challenging in non-stationary\nenvironments with changing goals. [Problem] Traditional knowledge transfer\nmethods in MARL struggle to generalize, and agents often require costly\nretraining to adapt. [Approach] This paper introduces a causal knowledge\ntransfer framework that enables RL agents to learn and share compact causal\nrepresentations of paths within a non-stationary environment. As the\nenvironment changes (new obstacles), agents' collisions require adaptive\nrecovery strategies. We model each collision as a causal intervention\ninstantiated as a sequence of recovery actions (a macro) whose effect\ncorresponds to a causal knowledge of how to circumvent the obstacle while\nincreasing the chances of achieving the agent's goal (maximizing cumulative\nreward). This recovery action macro is transferred online from a second agent\nand is applied in a zero-shot fashion, i.e., without retraining, just by\nquerying a lookup model with local context information (collisions). [Results]\nOur findings reveal two key insights: (1) agents with heterogeneous goals were\nable to bridge about half of the gap between random exploration and a fully\nretrained policy when adapting to new environments, and (2) the impact of\ncausal knowledge transfer depends on the interplay between environment\ncomplexity and agents' heterogeneous goals.", "AI": {"tldr": "Introduces a causal knowledge transfer framework that enables RL agents to learn and share compact causal representations of paths within a non-stationary environment. Agents with heterogeneous goals were able to bridge about half of the gap between random exploration and a fully retrained policy when adapting to new environments", "motivation": "transferring knowledge across agents remains challenging in non-stationary environments with changing goals. Traditional knowledge transfer methods in MARL struggle to generalize, and agents often require costly retraining to adapt.", "method": "a causal knowledge transfer framework that enables RL agents to learn and share compact causal representations of paths within a non-stationary environment. We model each collision as a causal intervention instantiated as a sequence of recovery actions (a macro) whose effect corresponds to a causal knowledge of how to circumvent the obstacle while increasing the chances of achieving the agent's goal (maximizing cumulative reward). This recovery action macro is transferred online from a second agent and is applied in a zero-shot fashion", "result": "agents with heterogeneous goals were able to bridge about half of the gap between random exploration and a fully retrained policy when adapting to new environments, and the impact of causal knowledge transfer depends on the interplay between environment complexity and agents' heterogeneous goals.", "conclusion": "agents with heterogeneous goals were able to bridge about half of the gap between random exploration and a fully retrained policy when adapting to new environments, and the impact of causal knowledge transfer depends on the interplay between environment complexity and agents' heterogeneous goals."}}
{"id": "2507.13385", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13385", "abs": "https://arxiv.org/abs/2507.13385", "authors": ["Arjun Rao", "Esther Rolf"], "title": "Using Multiple Input Modalities Can Improve Data-Efficiency and O.O.D. Generalization for ML with Satellite Imagery", "comment": "17 pages, 9 figures, 7 tables. Accepted to TerraBytes@ICML 2025", "summary": "A large variety of geospatial data layers is available around the world\nranging from remotely-sensed raster data like satellite imagery, digital\nelevation models, predicted land cover maps, and human-annotated data, to data\nderived from environmental sensors such as air temperature or wind speed data.\nA large majority of machine learning models trained on satellite imagery\n(SatML), however, are designed primarily for optical input modalities such as\nmulti-spectral satellite imagery. To better understand the value of using other\ninput modalities alongside optical imagery in supervised learning settings, we\ngenerate augmented versions of SatML benchmark tasks by appending additional\ngeographic data layers to datasets spanning classification, regression, and\nsegmentation. Using these augmented datasets, we find that fusing additional\ngeographic inputs with optical imagery can significantly improve SatML model\nperformance. Benefits are largest in settings where labeled data are limited\nand in geographic out-of-sample settings, suggesting that multi-modal inputs\nmay be especially valuable for data-efficiency and out-of-sample performance of\nSatML models. Surprisingly, we find that hard-coded fusion strategies\noutperform learned variants, with interesting implications for future work.", "AI": {"tldr": "fusing additional geographic inputs with optical imagery can significantly improve SatML model performance, especially when labeled data are limited", "motivation": "To better understand the value of using other input modalities alongside optical imagery in supervised learning settings", "method": "generate augmented versions of SatML benchmark tasks by appending additional geographic data layers to datasets spanning classification, regression, and segmentation", "result": "hard-coded fusion strategies outperform learned variants", "conclusion": "fusing additional geographic inputs with optical imagery can significantly improve SatML model performance. Benefits are largest in settings where labeled data are limited and in geographic out-of-sample settings"}}
{"id": "2507.13569", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13569", "abs": "https://arxiv.org/abs/2507.13569", "authors": ["Mrinal Mathur", "Mike Doan", "Barak Pearlmutter", "Sergey Plis"], "title": "Change of Thought: Adaptive Test-Time Computation", "comment": null, "summary": "Transformers evaluated in a single, fixed-depth pass are provably limited in\nexpressive power to the constant-depth circuit class TC0. Running a Transformer\nautoregressively removes that ceiling -- first in next-token prediction and,\nmore recently, in chain-of-thought reasoning. Both regimes rely on feedback\nloops that decode internal states into tokens only to re-encode them in\nsubsequent steps. While this \"thinking aloud\" mirrors human reasoning,\nbiological brains iterate without externalising intermediate states as\nlanguage. To boost the expressive power of encoder Transformers without\nresorting to token-level autoregression, we introduce the SELF-Transformer: an\nencoder layer that iteratively refines its own attention weights to a fixed\npoint. Instead of producing -- in one pass -- the alignment matrix that remixes\nthe input sequence, the SELF-Transformer iteratively updates that matrix\ninternally, scaling test-time computation with input difficulty. This\nadaptivity yields up to 20\\% accuracy gains on encoder-style benchmarks without\nincreasing parameter count, demonstrating that input-adaptive alignment at test\ntime offers substantial benefits for only a modest extra compute budget.\nSelf-Transformers thus recover much of the expressive power of iterative\nreasoning while preserving the simplicity of pure encoder architectures.", "AI": {"tldr": "SELF-Transformer \u662f\u4e00\u79cd\u65b0\u7684\u7f16\u7801\u5668\u5c42\uff0c\u5b83\u901a\u8fc7\u8fed\u4ee3\u7ec6\u5316\u6ce8\u610f\u529b\u6743\u91cd\u6765\u63d0\u9ad8\u8868\u8fbe\u80fd\u529b\uff0c\u4ece\u800c\u5728\u4e0d\u589e\u52a0\u53c2\u6570\u6570\u91cf\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u4e3a\u4e86\u5728\u4e0d\u4f9d\u8d56\u4e8e token \u7ea7\u522b\u81ea\u56de\u5f52\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u9ad8\u7f16\u7801\u5668 Transformer \u7684\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86 SELF-Transformer\uff1a\u4e00\u4e2a\u8fed\u4ee3\u5730\u7ec6\u5316\u5176\u81ea\u8eab\u6ce8\u610f\u529b\u6743\u91cd\u5230\u56fa\u5b9a\u70b9\u7684\u7f16\u7801\u5668\u5c42\u3002", "result": "\u5728\u7f16\u7801\u5668\u98ce\u683c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u4e86 20%\u3002", "conclusion": "Self-Transformers \u901a\u8fc7\u5728\u6d4b\u8bd5\u65f6\u8fdb\u884c\u8f93\u5165\u81ea\u9002\u5e94\u5bf9\u9f50\uff0c\u5728\u4e0d\u589e\u52a0\u53c2\u6570\u6570\u91cf\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u9ad8\u4e86\u7f16\u7801\u5668\u98ce\u683c\u57fa\u51c6\u6d4b\u8bd5\u7684\u51c6\u786e\u6027\uff0c\u5e76\u6062\u590d\u4e86\u8fed\u4ee3\u63a8\u7406\u7684\u5927\u90e8\u5206\u8868\u8fbe\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u7eaf\u7f16\u7801\u5668\u67b6\u6784\u7684\u7b80\u5355\u6027\u3002"}}
{"id": "2507.14096", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.14096", "abs": "https://arxiv.org/abs/2507.14096", "authors": ["Brian Ondov", "William Xia", "Kush Attal", "Ishita Unde", "Jerry He", "Hoa Dang", "Ian Soboroff", "Dina Demner-Fushman"], "title": "Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track", "comment": null, "summary": "Objective: Recent advances in language models have shown potential to adapt\nprofessional-facing biomedical literature to plain language, making it\naccessible to patients and caregivers. However, their unpredictability,\ncombined with the high potential for harm in this domain, means rigorous\nevaluation is necessary. Our goals with this track were to stimulate research\nand to provide high-quality evaluation of the most promising systems.\n  Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts\n(PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included\ncomplete, sentence-level, rewriting of abstracts (Task 1) as well as\nidentifying and replacing difficult terms (Task 2). For automatic evaluation of\nTask 1, we developed a four-fold set of professionally-written references.\nSubmissions for both Tasks 1 and 2 were provided extensive manual evaluation\nfrom biomedical experts.\n  Results: Twelve teams spanning twelve countries participated in the track,\nwith models from multilayer perceptrons to large pretrained transformers. In\nmanual judgments of Task 1, top-performing models rivaled human levels of\nfactual accuracy and completeness, but not simplicity or brevity. Automatic,\nreference-based metrics generally did not correlate well with manual judgments.\nIn Task 2, systems struggled with identifying difficult terms and classifying\nhow to replace them. When generating replacements, however, LLM-based systems\ndid well in manually judged accuracy, completeness, and simplicity, though not\nin brevity.\n  Conclusion: The PLABA track showed promise for using Large Language Models to\nadapt biomedical literature for the general public, while also highlighting\ntheir deficiencies and the need for improved automatic benchmarking tools.", "AI": {"tldr": "This paper introduces the PLABA track, which investigates the use of language models for simplifying biomedical text. The results show promise but also highlight challenges in simplicity, brevity, and automatic evaluation.", "motivation": "Recent advances in language models have shown potential to adapt professional-facing biomedical literature to plain language, making it accessible to patients and caregivers. However, their unpredictability, combined with the high potential for harm in this domain, means rigorous evaluation is necessary. Our goals with this track were to stimulate research and to provide high-quality evaluation of the most promising systems.", "method": "We hosted the Plain Language Adaptation of Biomedical Abstracts (PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included complete, sentence-level, rewriting of abstracts (Task 1) as well as identifying and replacing difficult terms (Task 2). For automatic evaluation of Task 1, we developed a four-fold set of professionally-written references. Submissions for both Tasks 1 and 2 were provided extensive manual evaluation from biomedical experts.", "result": "Twelve teams spanning twelve countries participated in the track, with models from multilayer perceptrons to large pretrained transformers. In manual judgments of Task 1, top-performing models rivaled human levels of factual accuracy and completeness, but not simplicity or brevity. Automatic, reference-based metrics generally did not correlate well with manual judgments. In Task 2, systems struggled with identifying difficult terms and classifying how to replace them. When generating replacements, however, LLM-based systems did well in manually judged accuracy, completeness, and simplicity, though not in brevity.", "conclusion": "PLABA track showed promise for using Large Language Models to adapt biomedical literature for the general public, while also highlighting their deficiencies and the need for improved automatic benchmarking tools."}}
{"id": "2507.13544", "categories": ["cs.CL", "68T50, 05C85, 68T05, 68R10", "I.2.7; I.2.4; H.3.3; I.5.0"], "pdf": "https://arxiv.org/pdf/2507.13544", "abs": "https://arxiv.org/abs/2507.13544", "authors": ["Mohamed Achref Ben Ammar", "Mohamed Taha Bennani"], "title": "A Computational Approach to Modeling Conversational Systems: Analyzing Large-Scale Quasi-Patterned Dialogue Flows", "comment": null, "summary": "The analysis of conversational dynamics has gained increasing importance with\nthe rise of large language model-based systems, which interact with users\nacross diverse contexts. In this work, we propose a novel computational\nframework for constructing conversational graphs that capture the flow and\nstructure of loosely organized dialogues, referred to as quasi-patterned\nconversations. We introduce the Filter & Reconnect method, a novel graph\nsimplification technique that minimizes noise while preserving semantic\ncoherence and structural integrity of conversational graphs. Through\ncomparative analysis, we demonstrate that the use of large language models\ncombined with our graph simplification technique has resulted in semantic\nmetric S increasing by a factor of 2.06 compared to previous approaches while\nsimultaneously enforcing a tree-like structure with 0 {\\delta}-hyperbolicity,\nensuring optimal clarity in conversation modeling. This work provides a\ncomputational method for analyzing large-scale dialogue datasets, with\npractical applications related to monitoring automated systems such as\nchatbots, dialogue management tools, and user behavior analytics.", "AI": {"tldr": "propose a novel computational framework for constructing conversational graphs that capture the flow and structure of loosely organized dialogues, referred to as quasi-patterned conversations. We introduce the Filter & Reconnect method, a novel graph simplification technique that minimizes noise while preserving semantic coherence and structural integrity of conversational graphs.", "motivation": "The analysis of conversational dynamics has gained increasing importance with the rise of large language model-based systems, which interact with users across diverse contexts.", "method": "We introduce the Filter & Reconnect method, a novel graph simplification technique that minimizes noise while preserving semantic coherence and structural integrity of conversational graphs.", "result": "the use of large language models combined with our graph simplification technique has resulted in semantic metric S increasing by a factor of 2.06 compared to previous approaches while simultaneously enforcing a tree-like structure with 0 {\\&}delta-hyperbolicity, ensuring optimal clarity in conversation modeling.", "conclusion": "This work provides a computational method for analyzing large-scale dialogue datasets, with practical applications related to monitoring automated systems such as chatbots, dialogue management tools, and user behavior analytics."}}
{"id": "2507.13874", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13874", "abs": "https://arxiv.org/abs/2507.13874", "authors": ["Mateusz Bystro\u0144ski", "Miko\u0142aj Ho\u0142ysz", "Grzegorz Piotrowski", "Nitesh V. Chawla", "Tomasz Kajdanowicz"], "title": "Large Language Models as Innovators: A Framework to Leverage Latent Space Exploration for Novelty Discovery", "comment": null, "summary": "Innovative idea generation remains a core challenge in AI, as large language\nmodels (LLMs) often struggle to produce outputs that are both novel and\nrelevant. Despite their fluency, LLMs tend to replicate patterns seen during\ntraining, limiting their ability to diverge creatively without extensive prompt\nengineering. Prior work has addressed this through domain-specific heuristics\nand structured prompting pipelines, but such solutions are brittle and\ndifficult to generalize. In this paper, we propose a model-agnostic\nlatent-space ideation framework that enables controlled, scalable creativity by\nnavigating the continuous embedding space of ideas. Unlike prior methods, our\nframework requires no handcrafted rules and adapts easily to different domains,\ninput formats, and creative tasks. This paper introduces an early-stage\nprototype of our method, outlining the conceptual framework and preliminary\nresults highlighting its potential as a general-purpose co-ideator for human-AI\ncollaboration.", "AI": {"tldr": "This paper proposes a model-agnostic latent-space ideation framework that enables controlled, scalable creativity by navigating the continuous embedding space of ideas. It requires no handcrafted rules and adapts easily to different domains, input formats, and creative tasks.", "motivation": "Innovative idea generation remains a core challenge in AI, as large language models (LLMs) often struggle to produce outputs that are both novel and relevant. Despite their fluency, LLMs tend to replicate patterns seen during training, limiting their ability to diverge creatively without extensive prompt engineering. Prior work has addressed this through domain-specific heuristics and structured prompting pipelines, but such solutions are brittle and difficult to generalize.", "method": "propose a model-agnostic latent-space ideation framework that enables controlled, scalable creativity by navigating the continuous embedding space of ideas", "result": "This paper introduces an early-stage prototype of our method, outlining the conceptual framework and preliminary results highlighting its potential as a general-purpose co-ideator for human-AI collaboration.", "conclusion": "This paper introduces an early-stage prototype of a model-agnostic latent-space ideation framework, outlining the conceptual framework and preliminary results highlighting its potential as a general-purpose co-ideator for human-AI collaboration."}}
{"id": "2507.13386", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13386", "abs": "https://arxiv.org/abs/2507.13386", "authors": ["Yang Zhang", "Er Jin", "Yanfei Dong", "Yixuan Wu", "Philip Torr", "Ashkan Khakzar", "Johannes Stegmaier", "Kenji Kawaguchi"], "title": "Minimalist Concept Erasure in Generative Models", "comment": "ICML2025", "summary": "Recent advances in generative models have demonstrated remarkable\ncapabilities in producing high-quality images, but their reliance on\nlarge-scale unlabeled data has raised significant safety and copyright\nconcerns. Efforts to address these issues by erasing unwanted concepts have\nshown promise. However, many existing erasure methods involve excessive\nmodifications that compromise the overall utility of the model. In this work,\nwe address these issues by formulating a novel minimalist concept erasure\nobjective based \\emph{only} on the distributional distance of final generation\noutputs. Building on our formulation, we derive a tractable loss for\ndifferentiable optimization that leverages backpropagation through all\ngeneration steps in an end-to-end manner. We also conduct extensive analysis to\nshow theoretical connections with other models and methods. To improve the\nrobustness of the erasure, we incorporate neuron masking as an alternative to\nmodel fine-tuning. Empirical evaluations on state-of-the-art flow-matching\nmodels demonstrate that our method robustly erases concepts without degrading\noverall model performance, paving the way for safer and more responsible\ngenerative models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6781\u7b80\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7a33\u5065\u5730\u64e6\u9664\u6982\u5ff5\uff0c\u540c\u65f6\u4e0d\u964d\u4f4e\u6574\u4f53\u6a21\u578b\u6027\u80fd\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u5b89\u5168\u3001\u66f4\u8d1f\u8d23\u7684\u751f\u6210\u6a21\u578b\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u4f9d\u8d56\u4e8e\u5927\u89c4\u6a21\u65e0\u6807\u7b7e\u6570\u636e\uff0c\u5f15\u53d1\u4e86\u4e25\u91cd\u7684\u5b89\u5168\u548c\u7248\u6743\u95ee\u9898\u3002\u73b0\u6709\u7684\u64e6\u9664\u65b9\u6cd5\u6d89\u53ca\u8fc7\u591a\u7684\u4fee\u6539\uff0c\u4ece\u800c\u635f\u5bb3\u4e86\u6a21\u578b\u7684\u6574\u4f53\u6548\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6781\u7b80\u6982\u5ff5\u64e6\u9664\u76ee\u6807\uff0c\u8be5\u76ee\u6807\u4ec5\u57fa\u4e8e\u6700\u7ec8\u751f\u6210\u8f93\u51fa\u7684\u5206\u5e03\u8ddd\u79bb\u3002\u5229\u7528\u901a\u8fc7\u6240\u6709\u751f\u6210\u6b65\u9aa4\u7684\u53cd\u5411\u4f20\u64ad\u8fdb\u884c\u7aef\u5230\u7aef\u7684\u53ef\u5fae\u4f18\u5316\uff0c\u63a8\u5bfc\u51fa\u4e00\u4e2a\u6613\u4e8e\u5904\u7406\u7684\u635f\u5931\u3002\u4e3a\u4e86\u63d0\u9ad8\u64e6\u9664\u7684\u9c81\u68d2\u6027\uff0c\u91c7\u7528\u795e\u7ecf\u5143\u63a9\u853d\u4f5c\u4e3a\u6a21\u578b\u5fae\u8c03\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "result": "\u5728\u6700\u5148\u8fdb\u7684flow-matching\u6a21\u578b\u4e0a\u7684\u7ecf\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u7a33\u5065\u5730\u64e6\u9664\u6982\u5ff5\uff0c\u540c\u65f6\u4e0d\u964d\u4f4e\u6574\u4f53\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u7a33\u5065\u5730\u64e6\u9664\u6982\u5ff5\uff0c\u540c\u65f6\u4e0d\u964d\u4f4e\u6574\u4f53\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u66f4\u5b89\u5168\u3001\u66f4\u8d1f\u8d23\u7684\u751f\u6210\u6a21\u578b\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2507.13575", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13575", "abs": "https://arxiv.org/abs/2507.13575", "authors": ["Hanzhi Zhou", "Erik Hornberger", "Pengsheng Guo", "Xiyou Zhou", "Saiwen Wang", "Xin Wang", "Yifei He", "Xuankai Chang", "Rene Rauch", "Louis D'hauwe", "John Peebles", "Alec Doane", "Kohen Chia", "Jenna Thibodeau", "Zi-Yi Dou", "Yuanyang Zhang", "Ruoming Pang", "Reed Li", "Zhifeng Chen", "Jeremy Warner", "Zhaoyang Xu", "Sophy Lee", "David Mizrahi", "Ramsey Tantawi", "Chris Chaney", "Kelsey Peterson", "Jun Qin", "Alex Dombrowski", "Mira Chiang", "Aiswarya Raghavan", "Gerard Casamayor", "Qibin Chen", "Aonan Zhang", "Nathalie Tran", "Jianyu Wang", "Hang Su", "Thomas Voice", "Alessandro Pappalardo", "Brycen Wershing", "Prasanth Yadla", "Rui Li", "Priyal Chhatrapati", "Ismael Fernandez", "Yusuf Goren", "Xin Zheng", "Forrest Huang", "Tao Lei", "Eray Yildiz", "Alper Kokmen", "Gokul Santhanam", "Areeba Kamal", "Kaan Elgin", "Dian Ang Yap", "Jeremy Liu", "Peter Gray", "Howard Xing", "Kieran Liu", "Matteo Ronchi", "Moritz Schwarzer-Becker", "Yun Zhu", "Mandana Saebi", "Jeremy Snow", "David Griffiths", "Guillaume Tartavel", "Erin Feldman", "Simon Lehnerer", "Fernando Berm\u00fadez-Medina", "Hans Han", "Joe Zhou", "Xiaoyi Ren", "Sujeeth Reddy", "Zirui Wang", "Tom Gunter", "Albert Antony", "Yuanzhi Li", "John Dennison", "Tony Sun", "Yena Han", "Yi Qin", "Sam Davarnia", "Jeffrey Bigham", "Wayne Shan", "Hannah Gillis Coleman", "Guillaume Klein", "Peng Liu", "Muyang Yu", "Jack Cackler", "Yuan Gao", "Crystal Xiao", "Binazir Karimzadeh", "Zhengdong Zhang", "Felix Bai", "Albin Madappally Jose", "Feng Nan", "Nazir Kamaldin", "Dong Yin", "Hans Hao", "Yanchao Sun", "Yi Hua", "Charles Maalouf", "Alex Guillen Garcia", "Guoli Yin", "Lezhi Li", "Mohana Prasad Sathya Moorthy", "Hongbin Gao", "Jay Tang", "Joanna Arreaza-Taylor", "Faye Lao", "Carina Peng", "Josh Shaffer", "Dan Masi", "Sushma Rao", "Tommi Vehvilainen", "Senyu Tong", "Dongcai Shen", "Yang Zhao", "Chris Bartels", "Peter Fu", "Qingqing Cao", "Christopher Neubauer", "Ethan Li", "Mingfei Gao", "Rebecca Callahan", "Richard Wei", "Patrick Dong", "Alex Braunstein", "Sachin Ravi", "Adolfo Lopez Mendez", "Kaiwei Huang", "Kun Duan", "Haoshuo Huang", "Rui Qian", "Stefano Ligas", "Jordan Huffaker", "Dongxu Li", "Bailin Wang", "Nanzhu Wang", "Anuva Agarwal", "Tait Madsen", "Josh Newnham", "Abhishek Sharma", "Zhile Ren", "Deepak Gopinath", "Erik Daxberger", "Saptarshi Guha", "Oron Levy", "Jing Lu", "Nan Dun", "Marc Kirchner", "Yinfei Yang", "Manjot Bilkhu", "Dave Nelson", "Anthony Spalvieri-Kruse", "Juan Lao Tebar", "Yang Xu", "Phani Mutyala", "Gabriel Jacoby-Cooper", "Yingbo Wang", "Karla Vega", "Vishaal Mahtani", "Darren Botten", "Eric Wang", "Hanli Li", "Matthias Paulik", "Haoran Yan", "Navid Shiee", "Yihao Qian", "Bugu Wu", "Qi Zhu", "Ob Adaranijo", "Bhuwan Dhingra", "Zhe Gan", "Nicholas Seidl", "Grace Duanmu", "Rong Situ", "Yiping Ma", "Yin Xia", "David Riazati", "Vasileios Saveris", "Anh Nguyen", "Michael", "Lee", "Patrick Sonnenberg", "Chinguun Erdenebileg", "Yanghao Li", "Vivian Ma", "James Chou", "Isha Garg", "Mark Lee", "Keen You", "Yuhong Li", "Ransen Niu", "Nandhitha Raghuram", "Pulkit Agrawal", "Henry Mason", "Sumeet Singh", "Keyu He", "Hong-You Chen", "Lucas Guibert", "Shiyu Li", "Varsha Paidi", "Narendran Raghavan", "Mingze Xu", "Yuli Yang", "Sergiu Sima", "Irina Belousova", "Sprite Chu", "Afshin Dehghan", "Philipp Dufter", "David Haldimann", "Zhen Yang", "Margit Bowler", "Chang Liu", "Ying-Chang Cheng", "Vivek Rathod", "Syd Evans", "Wilson Tsao", "Dustin Withers", "Haitian Sun", "Biyao Wang", "Peter Grasch", "Walker Cheng", "Yihao Feng", "Vivek Kumar", "Frank Chu", "Victoria M\u00f6nchJuan Haladjian", "Doug Kang", "Jiarui Lu", "Ciro Sannino", "Max Lam", "Floris Weers", "Bowen Pan", "Kenneth Jung", "Dhaval Doshi", "Fangping Shi", "Olli Saarikivi", "Alp Aygar", "Josh Elman", "Cheng Leong", "Eshan Verma", "Matthew Lei", "Jeff Nichols", "Jiulong Shan", "Donald Zhang", "Lawrence Zhou", "Stephen Murphy", "Xianzhi Du", "Chang Lan", "Ankur Jain", "Elmira Amirloo", "Marcin Eichner", "Naomy Sabo", "Anupama Mann Anupama", "David Qiu", "Zhao Meng", "Michael FitzMaurice", "Peng Zhang", "Simon Yeung", "Chen Chen", "Marco Zuliani", "Andrew Hansen", "Yang Lu", "Brent Ramerth", "Ziyi Zhong", "Parsa Mazaheri", "Matthew Hopkins", "Mengyu Li", "Simon Wang", "David Chen", "Farzin Rasteh", "Chong Wang", "Josh Gardner", "Asaf Liberman", "Haoxuan You", "Andrew Walkingshaw", "Xingyu Zhou", "Jinhao Lei", "Yan Meng", "Quentin Keunebroek", "Sam Wiseman", "Anders Boesen Lindbo Larsen", "Yi Zhang", "Zaid Ahmed", "Haiming Gang", "Aaron Franklin", "Kelvin Zou", "Guillaume Seguin", "Jonathan Janke", "Rachel Burger", "Co Giang", "Cheng Shen", "Jen Liu", "Sanskruti Shah", "Xiang Kong", "Yiran Fei", "TJ Collins", "Chen Zhang", "Zhiyun Lu", "Michael Booker", "Qin Ba", "Yasutaka Tanaka", "Andres Romero Mier Y Teran", "Federico Scozzafava", "Regan Poston", "Jane Li", "Eduardo Jimenez", "Bas Straathof", "Karanjeet Singh", "Lindsay Hislop", "Rajat Arora", "Deepa Seshadri", "Boyue Li", "Colorado Reed", "Zhen Li", "TJ Lu", "Yi Wang", "Kaelen Haag", "Nicholas Lusskin", "Raunak Sinha", "Rahul Nair", "Eldon Schoop", "Mary Beth Kery", "Mehrdad Farajtbar", "Brenda Yang", "George Horrell", "Shiwen Zhao", "Dhruti Shah", "Cha Chen", "Bowen Zhang", "Chang Gao", "Devi Krishna", "Jennifer Mallalieu", "Javier Movellan", "Di Feng", "Emily Zhang", "Sam Xu", "Junting Pan", "Dominik Moritz", "Suma Jayaram", "Kevin Smith", "Dongseong Hwang", "Daniel Parilla", "Jiaming Hu", "You-Cyuan Jhang", "Emad Soroush", "Fred Hohman", "Nan Du", "Emma Wang", "Sam Dodge", "Pragnya Sridhar", "Joris Pelemans", "Wei Fang", "Nina Wenzel", "Joseph Yitan Cheng", "Hadas Kotek", "Chung-Cheng Chiu", "Meng Cao", "Haijing Fu", "Ruixuan Hou", "Ke Ye", "Diane Zhu", "Nikhil Bhendawade", "Joseph Astrauskas", "Jian Liu", "Sai Aitharaju", "Wentao Wu", "Artsiom Peshko", "Hyunjik Kim", "Nilesh Shahdadpuri", "Andy De Wang", "Qi Shan", "Piotr Maj", "Raul Rea Menacho", "Justin Lazarow", "Eric Liang Yang", "Arsalan Farooq", "Donghan Yu", "David G\u00fcera", "Minsik Cho", "Kavya Nerella", "Yongqiang Wang", "Tao Jia", "John Park", "Jeff Lai", "Haotian Zhang", "Futang Peng", "Daniele Molinari", "Aparna Rajamani", "Tyler Johnson", "Lauren Gardiner", "Chao Jia", "Violet Yao", "Wojciech Kryscinski", "Xiujun Li", "Shang-Chen Wu"], "title": "Apple Intelligence Foundation Language Models: Tech Report 2025", "comment": null, "summary": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute.", "AI": {"tldr": "Apple introduces two new multilingual, multimodal foundation language models for Apple Intelligence: an on-device model and a scalable server model. Both models are trained on large-scale datasets and refined with supervised fine-tuning and reinforcement learning. The models support multiple languages, understand images, and execute tool calls. A Swift framework is provided for developers to integrate these capabilities.", "motivation": "Introduce two multilingual, multimodal foundation language models that power Apple Intelligence features across Apple devices and services", "method": "i a 3B-parameter on-device model optimized for Apple silicon through architectural innovations such as KV-cache sharing and 2-bit quantization-aware training; and ii a scalable server model built on a novel Parallel-Track Mixture-of-Experts PT-MoE transformer that combines track parallelism, mixture-of-experts sparse computation, and interleaved global-local attention", "result": "The resulting models support several additional languages while understanding images and executing tool calls. A new Swift-centric Foundation Models framework exposes guided generation, constrained tool calling, and LoRA adapter fine-tuning, allowing developers to integrate these capabilities with a few lines of code.", "conclusion": "Both the server model and the on-device model match or surpass comparably sized open baselines.The latest advancements in Apple Intelligence models are grounded in our Responsible AI approach with safeguards like content filtering and locale-specific evaluation, as well as our commitment to protecting our users' privacy with innovations like Private Cloud Compute."}}
{"id": "2507.14107", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.14107", "abs": "https://arxiv.org/abs/2507.14107", "authors": ["Viraj Nishesh Darji", "Callie C. Liao", "Duoduo Liao"], "title": "Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment", "comment": null, "summary": "Bridge maintenance and safety are essential for transportation authorities,\nand Non-Destructive Evaluation (NDE) techniques are critical to assessing\nstructural integrity. However, interpreting NDE data can be time-consuming and\nrequires expertise, potentially delaying decision-making. Recent advancements\nin Large Language Models (LLMs) offer new ways to automate and improve this\nanalysis. This pilot study introduces a holistic assessment of LLM capabilities\nfor interpreting NDE contour maps and demonstrates the effectiveness of LLMs in\nproviding detailed bridge condition analyses. It establishes a framework for\nintegrating LLMs into bridge inspection workflows, indicating that LLM-assisted\nanalysis can enhance efficiency without compromising accuracy. In this study,\nseveral LLMs are explored with prompts specifically designed to enhance the\nquality of image descriptions, which are applied to interpret five different\nNDE contour maps obtained through technologies for assessing bridge conditions.\nEach LLM model is evaluated based on its ability to produce detailed\ndescriptions, identify defects, provide actionable recommendations, and\ndemonstrate overall accuracy. The research indicates that four of the nine\nmodels provide better image descriptions, effectively covering a wide range of\ntopics related to the bridge's condition. The outputs from these four models\nare summarized using five different LLMs to form a comprehensive overview of\nthe bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more\neffective summaries. The findings suggest that LLMs have the potential to\nsignificantly improve efficiency and accuracy. This pilot study presents an\ninnovative approach that leverages LLMs for image captioning in parallel and\nsummarization, enabling faster decision-making in bridge maintenance and\nenhancing infrastructure management and safety assessments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u5728\u89e3\u91ca\u6865\u6881\u975e\u7834\u574f\u6027\u8bc4\u4f30(nde)\u6570\u636e\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5b83\u4eec\u53ef\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u4ece\u800c\u52a0\u5feb\u6865\u6881\u7ef4\u62a4\u51b3\u7b56\u3002", "motivation": "\u6865\u6881\u7ef4\u62a4\u548c\u5b89\u5168\u5bf9\u4ea4\u901a\u90e8\u95e8\u81f3\u5173\u91cd\u8981\uff0c\u65e0\u635f\u8bc4\u4f30(nde)\u6280\u672f\u5bf9\u8bc4\u4f30\u7ed3\u6784\u5b8c\u6574\u6027\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u89e3\u91cande\u6570\u636e\u53ef\u80fd\u975e\u5e38\u8017\u65f6\uff0c\u5e76\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u4ece\u800c\u53ef\u80fd\u5ef6\u8bef\u51b3\u7b56\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u5bf9\u901a\u8fc7\u8bc4\u4f30\u6865\u6881\u72b6\u51b5\u7684\u6280\u672f\u83b7\u5f97\u7684\u4e94\u4e2a\u4e0d\u540c\u7684\u975e\u7834\u574f\u6027\u8bc4\u4f30\u7b49\u9ad8\u7ebf\u56fe\u8fdb\u884c\u89e3\u91ca\u3002", "result": "\u4e5d\u4e2a\u6a21\u578b\u4e2d\u6709\u56db\u4e2a\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u56fe\u50cf\u63cf\u8ff0\uff0c\u6709\u6548\u5730\u6db5\u76d6\u4e86\u4e0e\u6865\u6881\u72b6\u51b5\u76f8\u5173\u7684\u5e7f\u6cdb\u4e3b\u9898\u3002LLMs ChatGPT-4\u548cClaude 3.5 Sonnet\u751f\u6210\u4e86\u66f4\u6709\u6548\u7684\u6458\u8981\u3002", "conclusion": "LLMs\u5177\u6709\u663e\u8457\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u5e76\u884c\u56fe\u50cf\u63cf\u8ff0\u548c\u603b\u7ed3\uff0c\u5b9e\u73b0\u66f4\u5feb\u7684\u51b3\u7b56\uff0c\u5e76\u52a0\u5f3a\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u548c\u5b89\u5168\u8bc4\u4f30\u3002"}}
{"id": "2507.13551", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13551", "abs": "https://arxiv.org/abs/2507.13551", "authors": ["Feng Chen", "Weizhe Xu", "Changye Li", "Serguei Pakhomov", "Alex Cohen", "Simran Bhola", "Sandy Yin", "Sunny X Tang", "Michael Mackinley", "Lena Palaniyappan", "Dror Ben-Zeev", "Trevor Cohen"], "title": "Reading Between the Lines: Combining Pause Dynamics and Semantic Coherence for Automated Assessment of Thought Disorder", "comment": null, "summary": "Formal thought disorder (FTD), a hallmark of schizophrenia spectrum\ndisorders, manifests as incoherent speech and poses challenges for clinical\nassessment. Traditional clinical rating scales, though validated, are\nresource-intensive and lack scalability. Automated speech analysis with\nautomatic speech recognition (ASR) allows for objective quantification of\nlinguistic and temporal features of speech, offering scalable alternatives. The\nuse of utterance timestamps in ASR captures pause dynamics, which are thought\nto reflect the cognitive processes underlying speech production. However, the\nutility of integrating these ASR-derived features for assessing FTD severity\nrequires further evaluation. This study integrates pause features with semantic\ncoherence metrics across three datasets: naturalistic self-recorded diaries\n(AVH, n = 140), structured picture descriptions (TOPSY, n = 72), and dream\nnarratives (PsyCL, n = 43). We evaluated pause related features alongside\nestablished coherence measures, using support vector regression (SVR) to\npredict clinical FTD scores. Key findings demonstrate that pause features alone\nrobustly predict the severity of FTD. Integrating pause features with semantic\ncoherence metrics enhanced predictive performance compared to semantic-only\nmodels, with integration of independent models achieving correlations up to\n\\r{ho} = 0.649 and AUC = 83.71% for severe cases detection (TOPSY, with best\n\\r{ho} = 0.584 and AUC = 79.23% for semantic-only models). The performance\ngains from semantic and pause features integration held consistently across all\ncontexts, though the nature of pause patterns was dataset-dependent. These\nfindings suggest that frameworks combining temporal and semantic analyses\nprovide a roadmap for refining the assessment of disorganized speech and\nadvance automated speech analysis in psychosis.", "AI": {"tldr": "This study demonstrates that pause features, when combined with semantic coherence metrics, improve the prediction of formal thought disorder (FTD) severity using automated speech analysis.", "motivation": "Traditional clinical rating scales are resource-intensive and lack scalability. Automated speech analysis with automatic speech recognition (ASR) allows for objective quantification of linguistic and temporal features of speech, offering scalable alternatives. The utility of integrating these ASR-derived features for assessing FTD severity requires further evaluation.", "method": "integrates pause features with semantic coherence metrics across three datasets: naturalistic self-recorded diaries, structured picture descriptions, and dream narratives. support vector regression (SVR) is used to predict clinical FTD scores.", "result": "pause features alone robustly predict the severity of FTD. Integrating pause features with semantic coherence metrics enhanced predictive performance compared to semantic-only models. Performance gains held consistently across all contexts, though the nature of pause patterns was dataset-dependent.", "conclusion": "Integrating pause features with semantic coherence metrics enhanced predictive performance compared to semantic-only models, with integration of independent models achieving correlations up to \r{ho} = 0.649 and AUC = 83.71% for severe cases detection. The performance gains from semantic and pause features integration held consistently across all contexts, though the nature of pause patterns was dataset-dependent. These findings suggest that frameworks combining temporal and semantic analyses provide a roadmap for refining the assessment of disorganized speech and advance automated speech analysis in psychosis."}}
{"id": "2507.13956", "categories": ["cs.AI", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.13956", "abs": "https://arxiv.org/abs/2507.13956", "authors": ["Yutao Jin", "Haowen Xiao", "Jielei Chu", "Fengmao Lv", "Yuxiao Li", "Tianrui Li"], "title": "Cross-modal Causal Intervention for Alzheimer's Disease Prediction", "comment": null, "summary": "Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's\nDisease (AD), where early identification and intervention can effectively slow\nthe progression to dementia. However, diagnosing AD remains a significant\nchallenge in neurology due to the confounders caused mainly by the selection\nbias of multimodal data and the complex relationships between variables. To\naddress these issues, we propose a novel visual-language causal intervention\nframework named Alzheimer's Disease Prediction with Cross-modal Causal\nIntervention (ADPC) for diagnostic assistance. Our ADPC employs large language\nmodel (LLM) to summarize clinical data under strict templates, maintaining\nstructured text outputs even with incomplete or unevenly distributed datasets.\nThe ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)\nimages and textual data generated by LLM to classify participants into\nCognitively Normal (CN), MCI, and AD categories. Because of the presence of\nconfounders, such as neuroimaging artifacts and age-related biomarkers,\nnon-causal models are likely to capture spurious input-output correlations,\ngenerating less reliable results. Our framework implicitly eliminates\nconfounders through causal intervention. Experimental results demonstrate the\noutstanding performance of our method in distinguishing CN/MCI/AD cases,\nachieving state-of-the-art (SOTA) metrics across most evaluation metrics. The\nstudy showcases the potential of integrating causal reasoning with multi-modal\nlearning for neurological disease diagnosis.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53ef\u89c6\u8bed\u8a00\u56e0\u679c\u5e72\u9884\u6846\u67b6 ADPC\uff0c\u7528\u4e8e\u8bca\u65ad\u8f85\u52a9\uff0c\u901a\u8fc7\u56e0\u679c\u5e72\u9884\u9690\u5f0f\u6d88\u9664\u6df7\u6dc6\u56e0\u7d20\uff0c\u5728\u533a\u5206 CN/MCI/AD \u75c5\u4f8b\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u89e3\u51b3\u7531\u591a\u6a21\u6001\u6570\u636e\u7684\u9009\u62e9\u504f\u5dee\u548c\u53d8\u91cf\u4e4b\u95f4\u590d\u6742\u5173\u7cfb\u5f15\u8d77\u7684\u6df7\u6dc6\u56e0\u7d20\uff0c\u8fd9\u4e9b\u56e0\u7d20\u5bfc\u81f4\u795e\u7ecf\u5185\u79d1\u5728\u8bca\u65ad AD \u65b9\u9762\u4ecd\u7136\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002", "method": "Alzheimer's Disease Prediction with Cross-modal Causal Intervention (ADPC)", "result": "\u8be5\u65b9\u6cd5\u5728\u533a\u5206 CN/MCI/AD \u75c5\u4f8b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5728\u5927\u591a\u6570\u8bc4\u4f30\u6307\u6807\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb (SOTA) \u7684\u6307\u6807\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u5c06\u56e0\u679c\u63a8\u7406\u4e0e\u591a\u6a21\u6001\u5b66\u4e60\u76f8\u7ed3\u5408\u4ee5\u8fdb\u884c\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\u8bca\u65ad\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.13387", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.13387", "abs": "https://arxiv.org/abs/2507.13387", "authors": ["Chihiro Noguchi", "Takaki Yamamoto"], "title": "From Binary to Semantic: Utilizing Large-Scale Binary Occupancy Data for 3D Semantic Occupancy Prediction", "comment": "Accepted to ICCV Workshop 2025", "summary": "Accurate perception of the surrounding environment is essential for safe\nautonomous driving. 3D occupancy prediction, which estimates detailed 3D\nstructures of roads, buildings, and other objects, is particularly important\nfor vision-centric autonomous driving systems that do not rely on LiDAR\nsensors. However, in 3D semantic occupancy prediction -- where each voxel is\nassigned a semantic label -- annotated LiDAR point clouds are required, making\ndata acquisition costly. In contrast, large-scale binary occupancy data, which\nonly indicate occupied or free space without semantic labels, can be collected\nat a lower cost. Despite their availability, the potential of leveraging such\ndata remains unexplored. In this study, we investigate the utilization of\nlarge-scale binary occupancy data from two perspectives: (1) pre-training and\n(2) learning-based auto-labeling. We propose a novel binary occupancy-based\nframework that decomposes the prediction process into binary and semantic\noccupancy modules, enabling effective use of binary occupancy data. Our\nexperimental results demonstrate that the proposed framework outperforms\nexisting methods in both pre-training and auto-labeling tasks, highlighting its\neffectiveness in enhancing 3D semantic occupancy prediction. The code is\navailable at https://github.com/ToyotaInfoTech/b2s-occupancy", "AI": {"tldr": "This paper introduces a framework that utilizes large-scale binary occupancy data to improve 3D semantic occupancy prediction, outperforming existing methods in pre-training and auto-labeling.", "motivation": "annotated LiDAR point clouds are required, making data acquisition costly. In contrast, large-scale binary occupancy data, which only indicate occupied or free space without semantic labels, can be collected at a lower cost. Despite their availability, the potential of leveraging such data remains unexplored", "method": "a novel binary occupancy-based framework that decomposes the prediction process into binary and semantic occupancy modules, enabling effective use of binary occupancy data", "result": "demonstrate that the proposed framework outperforms existing methods in both pre-training and auto-labeling tasks", "conclusion": "The proposed framework outperforms existing methods in both pre-training and auto-labeling tasks, highlighting its effectiveness in enhancing 3D semantic occupancy prediction."}}
{"id": "2507.13579", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13579", "abs": "https://arxiv.org/abs/2507.13579", "authors": ["Hyunji Nam", "Yanming Wan", "Mickel Liu", "Jianxun Lian", "Natasha Jaques"], "title": "Learning Pluralistic User Preferences through Reinforcement Learning Fine-tuned Summaries", "comment": "20 pages", "summary": "As everyday use cases of large language model (LLM) AI assistants have\nexpanded, it is becoming increasingly important to personalize responses to\nalign to different users' preferences and goals. While reinforcement learning\nfrom human feedback (RLHF) is effective at improving LLMs to be generally more\nhelpful and fluent, it does not account for variability across users, as it\nmodels the entire user population with a single reward model. We present a\nnovel framework, Preference Learning Using Summarization (PLUS), that learns\ntext-based summaries of each user's preferences, characteristics, and past\nconversations. These summaries condition the reward model, enabling it to make\npersonalized predictions about the types of responses valued by each user. We\ntrain the user-summarization model with reinforcement learning, and update the\nreward model simultaneously, creating an online co-adaptation loop. We show\nthat in contrast with prior personalized RLHF techniques or with in-context\nlearning of user information, summaries produced by PLUS capture meaningful\naspects of a user's preferences. Across different pluralistic user datasets, we\nshow that our method is robust to new users and diverse conversation topics.\nAdditionally, we demonstrate that the textual summaries generated about users\ncan be transferred for zero-shot personalization of stronger, proprietary\nmodels like GPT-4. The resulting user summaries are not only concise and\nportable, they are easy for users to interpret and modify, allowing for more\ntransparency and user control in LLM alignment.", "AI": {"tldr": "The paper introduces PLUS, a framework that learns text-based summaries of user preferences to personalize LLM responses, outperforming existing methods and allowing for user control and transparency.", "motivation": "It is becoming increasingly important to personalize responses of large language model (LLM) AI assistants to align to different users' preferences and goals. Reinforcement learning from human feedback (RLHF) does not account for variability across users, as it models the entire user population with a single reward model.", "method": "The paper presents a novel framework, Preference Learning Using Summarization (PLUS), that learns text-based summaries of each user's preferences, characteristics, and past conversations. The user-summarization model is trained with reinforcement learning, and the reward model is updated simultaneously, creating an online co-adaptation loop.", "result": "The paper shows that PLUS captures meaningful aspects of a user's preferences, is robust to new users and diverse conversation topics, and can be transferred for zero-shot personalization of stronger, proprietary models like GPT-4.", "conclusion": "The paper demonstrates that textual summaries generated about users can be transferred for zero-shot personalization of stronger models like GPT-4, and these summaries are concise, portable, easy to interpret and modify, allowing for more transparency and user control in LLM alignment."}}
{"id": "2507.13563", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.13563", "abs": "https://arxiv.org/abs/2507.13563", "authors": ["Kirill Borodin", "Nikita Vasiliev", "Vasiliy Kudryavtsev", "Maxim Maslov", "Mikhail Gorodnichev", "Oleg Rogov", "Grach Mkrtchian"], "title": "A Data-Centric Framework for Addressing Phonetic and Prosodic Challenges in Russian Speech Generative Models", "comment": "The work is still in progress", "summary": "Russian speech synthesis presents distinctive challenges, including vowel\nreduction, consonant devoicing, variable stress patterns, homograph ambiguity,\nand unnatural intonation. This paper introduces Balalaika, a novel dataset\ncomprising more than 2,000 hours of studio-quality Russian speech with\ncomprehensive textual annotations, including punctuation and stress markings.\nExperimental results show that models trained on Balalaika significantly\noutperform those trained on existing datasets in both speech synthesis and\nenhancement tasks. We detail the dataset construction pipeline, annotation\nmethodology, and results of comparative evaluations.", "AI": {"tldr": "Balalaika dataset improves Russian speech synthesis and enhancement.", "motivation": "Russian speech synthesis presents distinctive challenges, including vowel reduction, consonant devoicing, variable stress patterns, homograph ambiguity, and unnatural intonation.", "method": "A novel dataset comprising more than 2,000 hours of studio-quality Russian speech with comprehensive textual annotations.", "result": "Models trained on Balalaika significantly outperform those trained on existing datasets in both speech synthesis and enhancement tasks.", "conclusion": "Models trained on Balalaika outperform those trained on existing datasets in speech synthesis and enhancement."}}
{"id": "2507.13958", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.13958", "abs": "https://arxiv.org/abs/2507.13958", "authors": ["Pedro Cabalar", "Mart\u00edn Di\u00e9guez", "Fran\u00e7ois Olivier", "Torsten Schaub", "Igor St\u00e9phan"], "title": "Towards Constraint Temporal Answer Set Programming", "comment": null, "summary": "Reasoning about dynamic systems with a fine-grained temporal and numeric\nresolution presents significant challenges for logic-based approaches like\nAnswer Set Programming (ASP). To address this, we introduce and elaborate upon\na novel temporal and constraint-based extension of the logic of Here-and-There\nand its nonmonotonic equilibrium extension, representing, to the best of our\nknowledge, the first approach to nonmonotonic temporal reasoning with\nconstraints specifically tailored for ASP. This expressive system is achieved\nby a synergistic combination of two foundational ASP extensions: the\nlinear-time logic of Here-and-There, providing robust nonmonotonic temporal\nreasoning capabilities, and the logic of Here-and-There with constraints,\nenabling the direct integration and manipulation of numeric constraints, among\nothers. This work establishes the foundational logical framework for tackling\ncomplex dynamic systems with high resolution within the ASP paradigm.", "AI": {"tldr": "Introduces a new ASP approach for temporal reasoning with constraints, combining linear-time logic and constraint logic for handling complex dynamic systems.", "motivation": "Reasoning about dynamic systems with a fine-grained temporal and numeric resolution presents significant challenges for logic-based approaches like Answer Set Programming (ASP).", "method": "a novel temporal and constraint-based extension of the logic of Here-and-There and its nonmonotonic equilibrium extension", "result": "This expressive system is achieved by a synergistic combination of two foundational ASP extensions: the linear-time logic of Here-and-There, providing robust nonmonotonic temporal reasoning capabilities, and the logic of Here-and-There with constraints, enabling the direct integration and manipulation of numeric constraints, among others.", "conclusion": "This work establishes the foundational logical framework for tackling complex dynamic systems with high resolution within the ASP paradigm."}}
{"id": "2507.13397", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13397", "abs": "https://arxiv.org/abs/2507.13397", "authors": ["Kaiyuan Zhai", "Juan Chen", "Chao Wang", "Zeyi Xu"], "title": "InSyn: Modeling Complex Interactions for Pedestrian Trajectory Prediction", "comment": null, "summary": "Accurate pedestrian trajectory prediction is crucial for intelligent\napplications, yet it remains highly challenging due to the complexity of\ninteractions among pedestrians. Previous methods have primarily relied on\nrelative positions to model pedestrian interactions; however, they tend to\noverlook specific interaction patterns such as paired walking or conflicting\nbehaviors, limiting the prediction accuracy in crowded scenarios. To address\nthis issue, we propose InSyn (Interaction-Synchronization Network), a novel\nTransformer-based model that explicitly captures diverse interaction patterns\n(e.g., walking in sync or conflicting) while effectively modeling\ndirection-sensitive social behaviors. Additionally, we introduce a training\nstrategy termed Seq-Start of Seq (SSOS), designed to alleviate the common issue\nof initial-step divergence in numerical time-series prediction. Experiments on\nthe ETH and UCY datasets demonstrate that our model outperforms recent\nbaselines significantly, especially in high-density scenarios. Furthermore, the\nSSOS strategy proves effective in improving sequential prediction performance,\nreducing the initial-step prediction error by approximately 6.58%.", "AI": {"tldr": "This paper introduces InSyn, a Transformer-based model, and a training strategy called SSOS for pedestrian trajectory prediction. InSyn captures diverse interaction patterns and SSOS alleviates initial-step divergence. The model outperforms baselines, especially in crowded scenarios.", "motivation": "Accurate pedestrian trajectory prediction is crucial for intelligent applications, yet it remains highly challenging due to the complexity of interactions among pedestrians. Previous methods have primarily relied on relative positions to model pedestrian interactions; however, they tend to overlook specific interaction patterns such as paired walking or conflicting behaviors, limiting the prediction accuracy in crowded scenarios.", "method": "We propose InSyn (Interaction-Synchronization Network), a novel Transformer-based model that explicitly captures diverse interaction patterns while effectively modeling direction-sensitive social behaviors. Additionally, we introduce a training strategy termed Seq-Start of Seq (SSOS).", "result": "Our model outperforms recent baselines significantly, especially in high-density scenarios. Furthermore, the SSOS strategy proves effective in improving sequential prediction performance, reducing the initial-step prediction error by approximately 6.58%.", "conclusion": "The proposed InSyn model outperforms recent baselines significantly, especially in high-density scenarios. The SSOS strategy proves effective in improving sequential prediction performance, reducing the initial-step prediction error by approximately 6.58%."}}
{"id": "2507.13614", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13614", "abs": "https://arxiv.org/abs/2507.13614", "authors": ["Sergio E. Zanotto", "Segun Aroyehun"], "title": "Linguistic and Embedding-Based Profiling of Texts generated by Humans and Large Language Models", "comment": "arXiv admin note: text overlap with arXiv:2412.03025", "summary": "The rapid advancements in large language models (LLMs) have significantly\nimproved their ability to generate natural language, making texts generated by\nLLMs increasingly indistinguishable from human-written texts. While recent\nresearch has primarily focused on using LLMs to classify text as either\nhuman-written and machine-generated texts, our study focus on characterizing\nthese texts using a set of linguistic features across different linguistic\nlevels such as morphology, syntax, and semantics. We select a dataset of\nhuman-written and machine-generated texts spanning 8 domains and produced by 11\ndifferent LLMs. We calculate different linguistic features such as dependency\nlength and emotionality and we use them for characterizing human-written and\nmachine-generated texts along with different sampling strategies, repetition\ncontrols and model release date. Our statistical analysis reveals that\nhuman-written texts tend to exhibit simpler syntactic structures and more\ndiverse semantic content. Furthermore, we calculate the variability of our set\nof features across models and domains. Both human and machine texts show\nstylistic diversity across domains, with humans displaying greater variation in\nour features. Finally, we apply style embeddings to further test variability\namong human-written and machine-generated texts. Notably, newer models output\ntext that is similarly variable, pointing to an homogenization of\nmachine-generated texts.", "AI": {"tldr": "This paper characterizes texts generated by LLMs using linguistic features, finding differences in syntactic structure and semantic content between human and machine-generated texts. Newer models show homogenization.", "motivation": "While recent research has primarily focused on using LLMs to classify text as either human-written and machine-generated texts, our study focus on characterizing these texts", "method": "characterizing these texts using a set of linguistic features across different linguistic levels such as morphology, syntax, and semantics. We calculate different linguistic features such as dependency length and emotionality and we use them for characterizing human-written and machine-generated texts along with different sampling strategies, repetition controls and model release date. Furthermore, we calculate the variability of our set of features across models and domains. Finally, we apply style embeddings to further test variability among human-written and machine-generated texts.", "result": "human-written texts tend to exhibit simpler syntactic structures and more diverse semantic content. Both human and machine texts show stylistic diversity across domains, with humans displaying greater variation in our features.", "conclusion": "Newer models output text that is similarly variable, pointing to an homogenization of machine-generated texts."}}
{"id": "2507.14032", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14032", "abs": "https://arxiv.org/abs/2507.14032", "authors": ["Lam Nguyen", "Erika Barcelos", "Roger French", "Yinghui Wu"], "title": "KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models", "comment": "Accepted to the 24th International Semantic Web Conference Research\n  Track (ISWC 2025)", "summary": "Ontology Matching (OM) is a cornerstone task of semantic interoperability,\nyet existing systems often rely on handcrafted rules or specialized models with\nlimited adaptability. We present KROMA, a novel OM framework that harnesses\nLarge Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)\npipeline to dynamically enrich the semantic context of OM tasks with\nstructural, lexical, and definitional knowledge. To optimize both performance\nand efficiency, KROMA integrates a bisimilarity-based concept matching and a\nlightweight ontology refinement step, which prune candidate concepts and\nsubstantially reduce the communication overhead from invoking LLMs. Through\nexperiments on multiple benchmark datasets, we show that integrating knowledge\nretrieval with context-augmented LLMs significantly enhances ontology matching,\noutperforming both classic OM systems and cutting-edge LLM-based approaches\nwhile keeping communication overhead comparable. Our study highlights the\nfeasibility and benefit of the proposed optimization techniques (targeted\nknowledge retrieval, prompt enrichment, and ontology refinement) for ontology\nmatching at scale.", "AI": {"tldr": "KROMA, a novel OM framework, harnesses Large Language Models (LLMs) within a Retrieval-Augmented Generation (RAG) pipeline to dynamically enrich the semantic context of OM tasks. It optimizes performance and efficiency through bisimilarity-based concept matching and a lightweight ontology refinement step.", "motivation": "Existing Ontology Matching (OM) systems often rely on handcrafted rules or specialized models with limited adaptability.", "method": "KROMA integrates a bisimilarity-based concept matching and a lightweight ontology refinement step, which prune candidate concepts and substantially reduce the communication overhead from invoking LLMs.", "result": "KROMA outperforms both classic OM systems and cutting-edge LLM-based approaches while keeping communication overhead comparable.", "conclusion": "Integrating knowledge retrieval with context-augmented LLMs significantly enhances ontology matching, outperforming both classic OM systems and cutting-edge LLM-based approaches while keeping communication overhead comparable. The study highlights the feasibility and benefit of the proposed optimization techniques (targeted knowledge retrieval, prompt enrichment, and ontology refinement) for ontology matching at scale."}}
{"id": "2507.13401", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13401", "abs": "https://arxiv.org/abs/2507.13401", "authors": ["Shreya Kadambi", "Risheek Garrepalli", "Shubhankar Borse", "Munawar Hyatt", "Fatih Porikli"], "title": "MADI: Masking-Augmented Diffusion with Inference-Time Scaling for Visual Editing", "comment": "26 pages", "summary": "Despite the remarkable success of diffusion models in text-to-image\ngeneration, their effectiveness in grounded visual editing and compositional\ncontrol remains challenging. Motivated by advances in self-supervised learning\nand in-context generative modeling, we propose a series of simple yet powerful\ndesign choices that significantly enhance diffusion model capacity for\nstructured, controllable generation and editing. We introduce Masking-Augmented\nDiffusion with Inference-Time Scaling (MADI), a framework that improves the\neditability, compositionality and controllability of diffusion models through\ntwo core innovations. First, we introduce Masking-Augmented gaussian Diffusion\n(MAgD), a novel training strategy with dual corruption process which combines\nstandard denoising score matching and masked reconstruction by masking noisy\ninput from forward process. MAgD encourages the model to learn discriminative\nand compositional visual representations, thus enabling localized and\nstructure-aware editing. Second, we introduce an inference-time capacity\nscaling mechanism based on Pause Tokens, which act as special placeholders\ninserted into the prompt for increasing computational capacity at inference\ntime. Our findings show that adopting expressive and dense prompts during\ntraining further enhances performance, particularly for MAgD. Together, these\ncontributions in MADI substantially enhance the editability of diffusion\nmodels, paving the way toward their integration into more general-purpose,\nin-context generative diffusion architectures.", "AI": {"tldr": "MADI\u901a\u8fc7\u65b0\u9896\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u63a8\u7406\u65f6\u5bb9\u91cf\u7f29\u653e\u673a\u5236\uff0c\u63d0\u9ad8\u4e86\u6269\u6563\u6a21\u578b\u7684\u53ef\u7f16\u8f91\u6027\u3001\u7ec4\u5408\u6027\u548c\u53ef\u63a7\u6027\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5176\u5728\u6709\u6839\u636e\u7684\u89c6\u89c9\u7f16\u8f91\u548c\u7ec4\u5408\u63a7\u5236\u65b9\u9762\u7684\u6709\u6548\u6027\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u53d7\u5230\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u4e0a\u4e0b\u6587\u751f\u6210\u5efa\u6a21\u8fdb\u5c55\u7684\u63a8\u52a8\u3002", "method": "Masking-Augmented Diffusion with Inference-Time Scaling (MADI)", "result": "MADI\u663e\u8457\u63d0\u9ad8\u4e86\u6269\u6563\u6a21\u578b\u7684\u53ef\u7f16\u8f91\u6027\u3002", "conclusion": "MADI\u901a\u8fc7MAgD\u548c\u57fa\u4e8ePause Tokens\u7684\u63a8\u7406\u65f6\u5bb9\u91cf\u7f29\u653e\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6269\u6563\u6a21\u578b\u7684\u53ef\u7f16\u8f91\u6027\uff0c\u4e3a\u5c06\u5176\u96c6\u6210\u5230\u66f4\u901a\u7528\u7684\u3001\u4e0a\u4e0b\u6587\u751f\u6210\u6269\u6563\u67b6\u6784\u4e2d\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2507.13620", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13620", "abs": "https://arxiv.org/abs/2507.13620", "authors": ["Binxiong Li", "Yuefei Wang", "Xu Xiang", "Xue Li", "Binyu Zhao", "Heyang Gao", "Qinyu Zhao", "Xi Yu"], "title": "Tri-Learn Graph Fusion Network for Attributed Graph Clustering", "comment": "The source code for this study is available at\n  https://github.com/YF-W/Tri-GFN", "summary": "In recent years, models based on Graph Convolutional Networks (GCN) have made\nsignificant strides in the field of graph data analysis. However, challenges\nsuch as over-smoothing and over-compression remain when handling large-scale\nand complex graph datasets, leading to a decline in clustering quality.\nAlthough the Graph Transformer architecture has mitigated some of these issues,\nits performance is still limited when processing heterogeneous graph data. To\naddress these challenges, this study proposes a novel deep clustering framework\nthat comprising GCN, Autoencoder (AE), and Graph Transformer, termed the\nTri-Learn Graph Fusion Network (Tri-GFN). This framework enhances the\ndifferentiation and consistency of global and local information through a\nunique tri-learning mechanism and feature fusion enhancement strategy. The\nframework integrates GCN, AE, and Graph Transformer modules. These components\nare meticulously fused by a triple-channel enhancement module, which maximizes\nthe use of both node attributes and topological structures, ensuring robust\nclustering representation. The tri-learning mechanism allows mutual learning\namong these modules, while the feature fusion strategy enables the model to\ncapture complex relationships, yielding highly discriminative representations\nfor graph clustering. It surpasses many state-of-the-art methods, achieving an\naccuracy improvement of approximately 0.87% on the ACM dataset, 14.14 % on the\nReuters dataset, and 7.58 % on the USPS dataset. Due to its outstanding\nperformance on the Reuters dataset, Tri-GFN can be applied to automatic news\nclassification, topic retrieval, and related fields.", "AI": {"tldr": "\u63d0\u51fa\u4e86Tri-GFN\u6a21\u578b\uff0c\u901a\u8fc7\u878d\u5408GCN\u3001AE\u548cGraph Transformer\uff0c\u5229\u7528\u4e09\u5b66\u4e60\u673a\u5236\u548c\u7279\u5f81\u878d\u5408\u7b56\u7565\uff0c\u5728\u56fe\u805a\u7c7b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86state-of-the-art\u7684\u7ed3\u679c\u3002", "motivation": "\u57fa\u4e8e\u56fe\u5377\u79ef\u7f51\u7edc(GCN)\u7684\u6a21\u578b\u5728\u56fe\u6570\u636e\u5206\u6790\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002\u7136\u800c\uff0c\u5728\u5904\u7406\u5927\u89c4\u6a21\u548c\u590d\u6742\u56fe\u6570\u636e\u96c6\u65f6\uff0c\u4ecd\u7136\u5b58\u5728\u8fc7\u5ea6\u5e73\u6ed1\u548c\u8fc7\u5ea6\u538b\u7f29\u7b49\u6311\u6218\uff0c\u5bfc\u81f4\u805a\u7c7b\u8d28\u91cf\u4e0b\u964d\u3002\u867d\u7136Graph Transformer\u67b6\u6784\u7f13\u89e3\u4e86\u5176\u4e2d\u4e00\u4e9b\u95ee\u9898\uff0c\u4f46\u5728\u5904\u7406\u5f02\u6784\u56fe\u6570\u636e\u65f6\uff0c\u5176\u6027\u80fd\u4ecd\u7136\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u805a\u7c7b\u6846\u67b6\uff0c\u5305\u542bGCN\u3001Autoencoder (AE)\u548cGraph Transformer\uff0c\u79f0\u4e3aTri-Learn Graph Fusion Network (Tri-GFN)\u3002", "result": "\u5728ACM\u6570\u636e\u96c6\u4e0a\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u4e86\u7ea60.87%\uff1b\u5728\u8def\u900f\u793e\u6570\u636e\u96c6\u4e0a\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u4e8614.14%\uff1b\u5728USPS\u6570\u636e\u96c6\u4e0a\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u4e867.58%\u3002", "conclusion": "Tri-GFN\u5728ACM\u3001Reuters\u548cUSPS\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728Reuters\u6570\u636e\u96c6\u4e0a\u7684\u7a81\u51fa\u6027\u80fd\u4f7f\u5176\u53ef\u5e94\u7528\u4e8e\u81ea\u52a8\u65b0\u95fb\u5206\u7c7b\u3001\u4e3b\u9898\u68c0\u7d22\u548c\u76f8\u5173\u9886\u57df\u3002"}}
{"id": "2507.13618", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13618", "abs": "https://arxiv.org/abs/2507.13618", "authors": ["Shanbo Cheng", "Yu Bao", "Qian Cao", "Luyang Huang", "Liyan Kang", "Zhicheng Liu", "Yu Lu", "Wenhao Zhu", "Zhichao Huang", "Tao Li", "Sitong Liu", "Ningxin Peng", "Shuaijie She", "Lu Xu", "Nuo Xu", "Sen Yang", "Runsheng Yu", "Yiming Yu", "Liehao Zou", "Hang Li", "Lu Lu", "Yuxuan Wang", "Yonghui Wu"], "title": "Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters", "comment": null, "summary": "Multilingual translation stands as a challenging task for large language\nmodels (LLMs) to handle intricate language patterns and stilted translations\nthat arise in automated translations. In this paper, we introduce Seed-X, a\nfamily of open-source LLMs comprising instruct and reasoning models, pushing\nthe limits of translation capability with 7B parameter size. The base model is\npre-trained on a diverse, high-quality dataset encompassing both monolingual\nand bilingual content across 28 languages, harnessing the full potential of\nmultilingual data. The instruct model is then finetuned to translate by\nChain-of-Thought (CoT) reasoning and further enhanced through reinforcement\nlearning (RL) to achieve better generalization across diverse language pairs.\nSeed-X achieves performance comparable to leading closed-source models,\nincluding Gemini-2.5 and GPT-4o, across 28 languages, and significantly\noutperforms larger open-source models in both automatic metrics and human\nevaluations. We share the best practices through our optimization process, and\nmake the parameter public available for advancing translation research and\napplications.", "AI": {"tldr": "Seed-X, a 7B parameter open-source LLM, achieves translation performance comparable to Gemini-2.5 and GPT-4o across 28 languages using CoT reasoning and RL finetuning.", "motivation": "Multilingual translation stands as a challenging task for large language models (LLMs) to handle intricate language patterns and stilted translations that arise in automated translations.", "method": "The instruct model is finetuned to translate by Chain-of-Thought (CoT) reasoning and further enhanced through reinforcement learning (RL) to achieve better generalization across diverse language pairs.", "result": "Seed-X, a family of open-source LLMs comprising instruct and reasoning models, pushing the limits of translation capability with 7B parameter size. The base model is pre-trained on a diverse, high-quality dataset encompassing both monolingual and bilingual content across 28 languages, harnessing the full potential of multilingual data.", "conclusion": "Seed-X achieves performance comparable to leading closed-source models, including Gemini-2.5 and GPT-4o, across 28 languages, and significantly outperforms larger open-source models in both automatic metrics and human evaluations. The parameter is made publicly available."}}
{"id": "2507.14077", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14077", "abs": "https://arxiv.org/abs/2507.14077", "authors": ["Temiloluwa Prioleau", "Baiying Lu", "Yanjun Cui"], "title": "Glucose-ML: A collection of longitudinal diabetes datasets for development of robust AI solutions", "comment": "19 pages, 3 figures, 6 tables", "summary": "Artificial intelligence (AI) algorithms are a critical part of\nstate-of-the-art digital health technology for diabetes management. Yet, access\nto large high-quality datasets is creating barriers that impede development of\nrobust AI solutions. To accelerate development of transparent, reproducible,\nand robust AI solutions, we present Glucose-ML, a collection of 10 publicly\navailable diabetes datasets, released within the last 7 years (i.e., 2018 -\n2025). The Glucose-ML collection comprises over 300,000 days of continuous\nglucose monitor (CGM) data with a total of 38 million glucose samples collected\nfrom 2500+ people across 4 countries. Participants include persons living with\ntype 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support\nresearchers and innovators with using this rich collection of diabetes\ndatasets, we present a comparative analysis to guide algorithm developers with\ndata selection. Additionally, we conduct a case study for the task of blood\nglucose prediction - one of the most common AI tasks within the field. Through\nthis case study, we provide a benchmark for short-term blood glucose prediction\nacross all 10 publicly available diabetes datasets within the Glucose-ML\ncollection. We show that the same algorithm can have significantly different\nprediction results when developed/evaluated with different datasets. Findings\nfrom this study are then used to inform recommendations for developing robust\nAI solutions within the diabetes or broader health domain. We provide direct\nlinks to each longitudinal diabetes dataset in the Glucose-ML collection and\nopenly provide our code.", "AI": {"tldr": "\u63d0\u51fa\u4e86Glucose-ML\u7cd6\u5c3f\u75c5\u6570\u636e\u96c6\uff0c\u5305\u542b10\u4e2a\u6570\u636e\u96c6\uff0c\u5e76\u8fdb\u884c\u4e86\u77ed\u671f\u8840\u7cd6\u9884\u6d4b\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u83b7\u53d6\u5927\u578b\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u6b63\u5728\u5f62\u6210\u58c1\u5792\uff0c\u963b\u788d\u4e86\u7a33\u5065\u7684AI\u89e3\u51b3\u65b9\u6848\u7684\u5f00\u53d1\u3002\u4e3a\u4e86\u52a0\u901f\u5f00\u53d1\u900f\u660e\u3001\u53ef\u91cd\u590d\u548c\u7a33\u5065\u7684AI\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86Glucose-ML\uff0c\u4e00\u4e2a\u5305\u542b10\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u7cd6\u5c3f\u75c5\u6570\u636e\u96c6\u7684\u96c6\u5408\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u662f\u5728\u8fc7\u53bb7\u5e74\u5185\u53d1\u5e03\u7684\uff08\u53732018-2025\u5e74\uff09\u3002", "result": "Glucose-ML\u96c6\u5408\u5305\u542b\u8d85\u8fc7300,000\u5929\u7684\u8fde\u7eed\u8840\u7cd6\u76d1\u6d4b\uff08CGM\uff09\u6570\u636e\uff0c\u603b\u5171\u4ece\u6765\u81ea4\u4e2a\u56fd\u5bb6\u76842500\u591a\u4eba\u6536\u96c6\u4e863800\u4e07\u4e2a\u8461\u8404\u7cd6\u6837\u672c\u3002\u6211\u4eec\u5c55\u793a\u4e86\u76f8\u540c\u7684\u7b97\u6cd5\u5728\u7528\u4e0d\u540c\u7684\u6570\u636e\u96c6\u5f00\u53d1/\u8bc4\u4f30\u65f6\uff0c\u53ef\u4ee5\u6709\u663e\u8457\u4e0d\u540c\u7684\u9884\u6d4b\u7ed3\u679c\u3002", "conclusion": "\u540c\u4e00\u7b97\u6cd5\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u5f00\u53d1/\u8bc4\u4f30\u65f6\uff0c\u53ef\u80fd\u4ea7\u751f\u663e\u8457\u4e0d\u540c\u7684\u9884\u6d4b\u7ed3\u679c\u3002\u672c\u7814\u7a76\u7684\u53d1\u73b0\u53ef\u4e3a\u5728\u7cd6\u5c3f\u75c5\u6216\u66f4\u5e7f\u6cdb\u7684\u5065\u5eb7\u9886\u57df\u5f00\u53d1\u7a33\u5065\u7684AI\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u53c2\u8003\u5efa\u8bae\u3002"}}
{"id": "2507.13403", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13403", "abs": "https://arxiv.org/abs/2507.13403", "authors": ["Morteza Bodaghi", "Majid Hosseini", "Raju Gottumukkala", "Ravi Teja Bhupatiraju", "Iftikhar Ahmad", "Moncef Gabbouj"], "title": "UL-DD: A Multimodal Drowsiness Dataset Using Video, Biometric Signals, and Behavioral Data", "comment": null, "summary": "In this study, we present a comprehensive public dataset for driver\ndrowsiness detection, integrating multimodal signals of facial, behavioral, and\nbiometric indicators. Our dataset includes 3D facial video using a depth\ncamera, IR camera footage, posterior videos, and biometric signals such as\nheart rate, electrodermal activity, blood oxygen saturation, skin temperature,\nand accelerometer data. This data set provides grip sensor data from the\nsteering wheel and telemetry data from the American truck simulator game to\nprovide more information about drivers' behavior while they are alert and\ndrowsy. Drowsiness levels were self-reported every four minutes using the\nKarolinska Sleepiness Scale (KSS). The simulation environment consists of three\nmonitor setups, and the driving condition is completely like a car. Data were\ncollected from 19 subjects (15 M, 4 F) in two conditions: when they were fully\nalert and when they exhibited signs of sleepiness. Unlike other datasets, our\nmultimodal dataset has a continuous duration of 40 minutes for each data\ncollection session per subject, contributing to a total length of 1,400\nminutes, and we recorded gradual changes in the driver state rather than\ndiscrete alert/drowsy labels. This study aims to create a comprehensive\nmultimodal dataset of driver drowsiness that captures a wider range of\nphysiological, behavioral, and driving-related signals. The dataset will be\navailable upon request to the corresponding author.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u9a7e\u9a76\u5458\u56f0\u5026\u68c0\u6d4b\u7684\u7efc\u5408\u516c\u5171\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u96c6\u6210\u4e86\u9762\u90e8\u3001\u884c\u4e3a\u548c\u751f\u7269\u8bc6\u522b\u6307\u6807\u7684\u591a\u6a21\u6001\u4fe1\u53f7\u3002", "motivation": "\u73b0\u6709\u7684\u9a7e\u9a76\u5458\u56f0\u5026\u68c0\u6d4b\u6570\u636e\u96c6\u4e0d\u8db3\u4ee5\u6355\u83b7\u9a7e\u9a76\u5458\u72b6\u6001\u7684\u6e10\u8fdb\u53d8\u5316\u548c\u5404\u79cd\u751f\u7406\u3001\u884c\u4e3a\u548c\u9a7e\u9a76\u76f8\u5173\u4fe1\u53f7\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u76f8\u673a\u3001\u7ea2\u5916\u76f8\u673a\u7d20\u6750\u3001\u540e\u89c6\u9891\u548c\u751f\u7269\u7279\u5f81\u4fe1\u53f7\uff08\u5982\u5fc3\u7387\u3001\u76ae\u80a4\u7535\u6d3b\u52a8\u3001\u8840\u6c27\u9971\u548c\u5ea6\u3001\u76ae\u80a4\u6e29\u5ea6\u548c\u52a0\u901f\u5ea6\u8ba1\u6570\u636e\uff09\u96c6\u6210\u9762\u90e8\u3001\u884c\u4e3a\u548c\u751f\u7269\u8bc6\u522b\u6307\u6807\u7684\u591a\u6a21\u6001\u4fe1\u53f7\u3002", "result": "\u8be5\u6570\u636e\u96c6\u5305\u62ec\u6765\u81ea 19 \u540d\u53d7\u8bd5\u8005\uff0815 \u540d\u7537\u6027\uff0c4 \u540d\u5973\u6027\uff09\u7684\u6570\u636e\uff0c\u8fd9\u4e9b\u6570\u636e\u662f\u5728\u4e24\u79cd\u60c5\u51b5\u4e0b\u6536\u96c6\u7684\uff1a\u5f53\u4ed6\u4eec\u5b8c\u5168\u6e05\u9192\u65f6\u4ee5\u53ca\u5f53\u4ed6\u4eec\u8868\u73b0\u51fa\u56f0\u5026\u8ff9\u8c61\u65f6\u3002\u4e0e\u5176\u5b83\u6570\u636e\u96c6\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u5bf9\u4e8e\u6bcf\u4e2a\u53d7\u8bd5\u8005\u7684\u6bcf\u6b21\u6570\u636e\u6536\u96c6\u4f1a\u8bdd\u5177\u6709 40 \u5206\u949f\u7684\u8fde\u7eed\u65f6\u957f\uff0c\u603b\u957f\u5ea6\u4e3a 1,400 \u5206\u949f\uff0c\u5e76\u4e14\u6211\u4eec\u8bb0\u5f55\u4e86\u9a7e\u9a76\u5458\u72b6\u6001\u7684\u9010\u6e10\u53d8\u5316\uff0c\u800c\u4e0d\u662f\u79bb\u6563\u7684\u6e05\u9192/\u56f0\u5026\u6807\u7b7e\u3002", "conclusion": "\u672c\u7814\u7a76\u65e8\u5728\u521b\u5efa\u4e00\u4e2a\u5168\u9762\u7684\u9a7e\u9a76\u5458\u56f0\u5026\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u6355\u83b7\u66f4\u5e7f\u6cdb\u7684\u751f\u7406\u3001\u884c\u4e3a\u548c\u9a7e\u9a76\u76f8\u5173\u4fe1\u53f7\u3002\u8be5\u6570\u636e\u96c6\u5c06\u5e94\u8981\u6c42\u63d0\u4f9b\u7ed9\u901a\u8baf\u4f5c\u8005\u3002"}}
{"id": "2507.13624", "categories": ["cs.LG", "cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.13624", "abs": "https://arxiv.org/abs/2507.13624", "authors": ["Daniel Commey", "Kamel Abbad", "Garth V. Crosby", "Lyes Khoukhi"], "title": "FedSkipTwin: Digital-Twin-Guided Client Skipping for Communication-Efficient Federated Learning", "comment": null, "summary": "Communication overhead remains a primary bottleneck in federated learning\n(FL), particularly for applications involving mobile and IoT devices with\nconstrained bandwidth. This work introduces FedSkipTwin, a novel\nclient-skipping algorithm driven by lightweight, server-side digital twins.\nEach twin, implemented as a simple LSTM, observes a client's historical\nsequence of gradient norms to forecast both the magnitude and the epistemic\nuncertainty of its next update. The server leverages these predictions,\nrequesting communication only when either value exceeds a predefined threshold;\notherwise, it instructs the client to skip the round, thereby saving bandwidth.\nExperiments are conducted on the UCI-HAR and MNIST datasets with 10 clients\nunder a non-IID data distribution. The results demonstrate that FedSkipTwin\nreduces total communication by 12-15.5% across 20 rounds while simultaneously\nimproving final model accuracy by up to 0.5 percentage points compared to the\nstandard FedAvg algorithm. These findings establish that prediction-guided\nskipping is a practical and effective strategy for resource-aware FL in\nbandwidth-constrained edge environments.", "AI": {"tldr": "FedSkipTwin, a client-skipping algorithm, reduces communication overhead and improves accuracy in federated learning by using server-side digital twins to predict client updates.", "motivation": "Communication overhead remains a primary bottleneck in federated learning (FL), particularly for applications involving mobile and IoT devices with constrained bandwidth.", "method": "a novel client-skipping algorithm driven by lightweight, server-side digital twins.", "result": "FedSkipTwin reduces total communication by 12-15.5% across 20 rounds while simultaneously improving final model accuracy by up to 0.5 percentage points compared to the standard FedAvg algorithm.", "conclusion": " prediction-guided skipping is a practical and effective strategy for resource-aware FL in bandwidth-constrained edge environments."}}
{"id": "2507.13655", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13655", "abs": "https://arxiv.org/abs/2507.13655", "authors": ["Teerapong Panboonyuen"], "title": "CU-ICU: Customizing Unsupervised Instruction-Finetuned Language Models for ICU Datasets via Text-to-Text Transfer Transformer", "comment": "12 pages", "summary": "Integrating large language models into specialized domains like healthcare\npresents unique challenges, including domain adaptation and limited labeled\ndata. We introduce CU-ICU, a method for customizing unsupervised\ninstruction-finetuned language models for ICU datasets by leveraging the\nText-to-Text Transfer Transformer (T5) architecture. CU-ICU employs a sparse\nfine-tuning approach that combines few-shot prompting with selective parameter\nupdates, enabling efficient adaptation with minimal supervision. Our evaluation\nacross critical ICU tasks--early sepsis detection, mortality prediction, and\nclinical note generation--demonstrates that CU-ICU consistently improves\npredictive accuracy and interpretability over standard fine-tuning methods.\nNotably, CU-ICU achieves up to a 15% increase in sepsis detection accuracy and\na 20% enhancement in generating clinically relevant explanations while updating\nfewer than 1% of model parameters in its most efficient configuration. These\nresults establish CU-ICU as a scalable, low-overhead solution for delivering\naccurate and interpretable clinical decision support in real-world ICU\nenvironments.", "AI": {"tldr": "CU-ICU improves predictive accuracy and interpretability for ICU tasks with minimal overhead.", "motivation": "Integrating large language models into specialized domains like healthcare presents unique challenges, including domain adaptation and limited labeled data.", "method": "CU-ICU: a method for customizing unsupervised instruction-finetuned language models for ICU datasets by leveraging the Text-to-Text Transfer Transformer (T5) architecture, employs a sparse fine-tuning approach that combines few-shot prompting with selective parameter updates", "result": "CU-ICU consistently improves predictive accuracy and interpretability over standard fine-tuning methods. achieves up to a 15% increase in sepsis detection accuracy and a 20% enhancement in generating clinically relevant explanations while updating fewer than 1% of model parameters", "conclusion": "CU-ICU is a scalable, low-overhead solution for accurate and interpretable clinical decision support in real-world ICU environments."}}
{"id": "2507.14097", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14097", "abs": "https://arxiv.org/abs/2507.14097", "authors": ["Hari Iyer", "Neel Macwan", "Atharva Jitendra Hude", "Heejin Jeong", "Shenghan Guo"], "title": "Generative AI-Driven High-Fidelity Human Motion Simulation", "comment": null, "summary": "Human motion simulation (HMS) supports cost-effective evaluation of worker\nbehavior, safety, and productivity in industrial tasks. However, existing\nmethods often suffer from low motion fidelity. This study introduces\nGenerative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and\ntext-to-motion models to enhance simulation quality for physical tasks.\nG-AI-HMS tackles two key challenges: (1) translating task descriptions into\nmotion-aware language using Large Language Models aligned with MotionGPT's\ntraining vocabulary, and (2) validating AI-enhanced motions against real human\nmovements using computer vision. Posture estimation algorithms are applied to\nreal-time videos to extract joint landmarks, and motion similarity metrics are\nused to compare them with AI-enhanced sequences. In a case study involving\neight tasks, the AI-enhanced motions showed lower error than human created\ndescriptions in most scenarios, performing better in six tasks based on spatial\naccuracy, four tasks based on alignment after pose normalization, and seven\ntasks based on overall temporal similarity. Statistical analysis showed that\nAI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and\ntemporal misalignment while retaining comparable posture accuracy.", "AI": {"tldr": "This study introduces Generative-AI-Enabled HMS (G-AI-HMS) to enhance simulation quality for physical tasks.", "motivation": "existing methods often suffer from low motion fidelity.", "method": "integrates text-to-text and text-to-motion models and validating AI-enhanced motions against real human movements using computer vision", "result": "AI-enhanced motions showed lower error than human created descriptions in most scenarios", "conclusion": "AI-enhanced motions significantly reduce joint error and temporal misalignment while retaining comparable posture accuracy."}}
{"id": "2507.13404", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13404", "abs": "https://arxiv.org/abs/2507.13404", "authors": ["Delin An", "Pan Du", "Jian-Xun Wang", "Chaoli Wang"], "title": "AortaDiff: Volume-Guided Conditional Diffusion Models for Multi-Branch Aortic Surface Generation", "comment": null, "summary": "Accurate 3D aortic construction is crucial for clinical diagnosis,\npreoperative planning, and computational fluid dynamics (CFD) simulations, as\nit enables the estimation of critical hemodynamic parameters such as blood flow\nvelocity, pressure distribution, and wall shear stress. Existing construction\nmethods often rely on large annotated training datasets and extensive manual\nintervention. While the resulting meshes can serve for visualization purposes,\nthey struggle to produce geometrically consistent, well-constructed surfaces\nsuitable for downstream CFD analysis. To address these challenges, we introduce\nAortaDiff, a diffusion-based framework that generates smooth aortic surfaces\ndirectly from CT/MRI volumes. AortaDiff first employs a volume-guided\nconditional diffusion model (CDM) to iteratively generate aortic centerlines\nconditioned on volumetric medical images. Each centerline point is then\nautomatically used as a prompt to extract the corresponding vessel contour,\nensuring accurate boundary delineation. Finally, the extracted contours are\nfitted into a smooth 3D surface, yielding a continuous, CFD-compatible mesh\nrepresentation. AortaDiff offers distinct advantages over existing methods,\nincluding an end-to-end workflow, minimal dependency on large labeled datasets,\nand the ability to generate CFD-compatible aorta meshes with high geometric\nfidelity. Experimental results demonstrate that AortaDiff performs effectively\neven with limited training data, successfully constructing both normal and\npathologically altered aorta meshes, including cases with aneurysms or\ncoarctation. This capability enables the generation of high-quality\nvisualizations and positions AortaDiff as a practical solution for\ncardiovascular research.", "AI": {"tldr": "AortaDiff \u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u6846\u67b6\uff0c\u53ef\u76f4\u63a5\u4ece CT/MRI \u5bb9\u79ef\u751f\u6210\u5e73\u6ed1\u7684\u4e3b\u52a8\u8109\u8868\u9762\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5b83\u5bf9\u5927\u578b\u6807\u8bb0\u6570\u636e\u96c6\u7684\u4f9d\u8d56\u6027\u6700\u5c0f\uff0c\u5e76\u4e14\u80fd\u591f\u751f\u6210\u5177\u6709\u9ad8\u51e0\u4f55\u4fdd\u771f\u5ea6\u7684 CFD \u517c\u5bb9\u4e3b\u52a8\u8109\u7f51\u683c\u3002", "motivation": "\u7cbe\u786e\u7684 3D \u4e3b\u52a8\u8109\u6784\u5efa\u5bf9\u4e8e\u4e34\u5e8a\u8bca\u65ad\u3001\u672f\u524d\u8ba1\u5212\u548c\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66 (CFD) \u6a21\u62df\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u5b83\u80fd\u591f\u4f30\u8ba1\u5173\u952e\u7684\u8840\u6d41\u52a8\u529b\u5b66\u53c2\u6570\uff0c\u4f8b\u5982\u8840\u6d41\u901f\u5ea6\u3001\u538b\u529b\u5206\u5e03\u548c\u58c1\u526a\u5e94\u529b\u3002\u73b0\u6709\u7684\u6784\u5efa\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u5927\u578b\u5e26\u6ce8\u91ca\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u5e7f\u6cdb\u7684\u4eba\u5de5\u5e72\u9884\u3002\u867d\u7136\u751f\u6210\u7684\u7f51\u683c\u53ef\u4ee5\u7528\u4e8e\u53ef\u89c6\u5316\u76ee\u7684\uff0c\u4f46\u5b83\u4eec\u96be\u4ee5\u751f\u6210\u51e0\u4f55\u4e00\u81f4\u3001\u7ed3\u6784\u826f\u597d\u7684\u8868\u9762\uff0c\u9002\u7528\u4e8e\u4e0b\u6e38 CFD \u5206\u6790\u3002", "method": "AortaDiff \u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u6846\u67b6\uff0c\u53ef\u76f4\u63a5\u4ece CT/MRI \u5bb9\u79ef\u751f\u6210\u5e73\u6ed1\u7684\u4e3b\u52a8\u8109\u8868\u9762\u3002AortaDiff \u9996\u5148\u91c7\u7528\u5bb9\u79ef\u5f15\u5bfc\u7684\u6761\u4ef6\u6269\u6563\u6a21\u578b (CDM) \u4ee5\u8fed\u4ee3\u65b9\u5f0f\u751f\u6210\u4ee5\u5bb9\u79ef\u533b\u5b66\u56fe\u50cf\u4e3a\u6761\u4ef6\u7684\u4e2d\u5fc3\u7ebf\u3002\u7136\u540e\uff0c\u6bcf\u4e2a\u4e2d\u5fc3\u7ebf\u70b9\u81ea\u52a8\u7528\u4f5c\u63d0\u53d6\u76f8\u5e94\u8840\u7ba1\u8f6e\u5ed3\u7684\u63d0\u793a\uff0c\u786e\u4fdd\u51c6\u786e\u7684\u8fb9\u754c\u63cf\u7ed8\u3002\u6700\u540e\uff0c\u5c06\u63d0\u53d6\u7684\u8f6e\u5ed3\u62df\u5408\u6210\u5e73\u6ed1\u7684 3D \u8868\u9762\uff0c\u4ece\u800c\u751f\u6210\u8fde\u7eed\u7684\u3001CFD \u517c\u5bb9\u7684\u7f51\u683c\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u6709\u9650\u7684\u8bad\u7ec3\u6570\u636e\u4e0b\uff0cAortaDiff \u4e5f\u80fd\u6709\u6548\u6267\u884c\uff0c\u6210\u529f\u6784\u5efa\u6b63\u5e38\u548c\u75c5\u7406\u6539\u53d8\u7684\u4e3b\u52a8\u8109\u7f51\u683c\uff0c\u5305\u62ec\u52a8\u8109\u7624\u6216\u7f29\u7a84\u75c5\u4f8b\u3002AortaDiff \u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5305\u62ec\u7aef\u5230\u7aef\u5de5\u4f5c\u6d41\u7a0b\u3001\u5bf9\u5927\u578b\u6807\u8bb0\u6570\u636e\u96c6\u7684\u6700\u5c0f\u4f9d\u8d56\u6027\u4ee5\u53ca\u751f\u6210\u5177\u6709\u9ad8\u51e0\u4f55\u4fdd\u771f\u5ea6\u7684 CFD \u517c\u5bb9\u4e3b\u52a8\u8109\u7f51\u683c\u7684\u80fd\u529b\u3002", "conclusion": "AortaDiff \u80fd\u591f\u6709\u6548\u5730\u6784\u5efa\u6b63\u5e38\u548c\u75c5\u7406\u6539\u53d8\u7684\u4e3b\u52a8\u8109\u7f51\u683c\uff0c\u5305\u62ec\u52a8\u8109\u7624\u6216\u4e3b\u52a8\u8109\u7f29\u7a84\u7684\u75c5\u4f8b\u3002\u8fd9\u4f7f\u5f97\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u53ef\u89c6\u5316\u6548\u679c\uff0c\u5e76\u5c06 AortaDiff \u5b9a\u4f4d\u4e3a\u5fc3\u8840\u7ba1\u7814\u7a76\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.13646", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.13646", "abs": "https://arxiv.org/abs/2507.13646", "authors": ["Nimisha Ghosh", "Daniele Santoni", "Debaleena Nawn", "Eleonora Ottaviani", "Giovanni Felici"], "title": "A Comprehensive Review of Transformer-based language models for Protein Sequence Analysis and Design", "comment": null, "summary": "The impact of Transformer-based language models has been unprecedented in\nNatural Language Processing (NLP). The success of such models has also led to\ntheir adoption in other fields including bioinformatics. Taking this into\naccount, this paper discusses recent advances in Transformer-based models for\nprotein sequence analysis and design. In this review, we have discussed and\nanalysed a significant number of works pertaining to such applications. These\napplications encompass gene ontology, functional and structural protein\nidentification, generation of de novo proteins and binding of proteins. We\nattempt to shed light on the strength and weaknesses of the discussed works to\nprovide a comprehensive insight to readers. Finally, we highlight shortcomings\nin existing research and explore potential avenues for future developments. We\nbelieve that this review will help researchers working in this field to have an\noverall idea of the state of the art in this field, and to orient their future\nstudies.", "AI": {"tldr": "This paper reviews recent advances in Transformer-based models for protein sequence analysis and design, covering applications, strengths, weaknesses, and future directions.", "motivation": "The success of Transformer-based language models in NLP has led to their adoption in bioinformatics, prompting this review of recent advances.", "method": "Review and analysis of a significant number of works pertaining to applications of Transformer-based models in protein sequence analysis and design.", "result": "Discussion and analysis of applications like gene ontology, functional/structural protein identification, de novo protein generation, and protein binding, along with strengths and weaknesses of existing works. Identification of shortcomings and potential future developments.", "conclusion": "This review helps researchers understand the current state-of-the-art and orient future studies in Transformer-based models for protein sequence analysis and design."}}
{"id": "2507.13666", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13666", "abs": "https://arxiv.org/abs/2507.13666", "authors": ["Woo-Chan Kim", "Ji-Hoon Park", "Seong-Whan Lee"], "title": "KiC: Keyword-inspired Cascade for Cost-Efficient Text Generation with LLMs", "comment": null, "summary": "Large language models (LLMs) have demonstrated state-of-the-art performance\nacross a wide range of natural language processing tasks. However,\nhigh-performing models are typically accessible only via APIs, incurring\nsubstantial inference costs. Cascade methods address this by initially\nemploying a cheaper model and escalating to a stronger one only when necessary.\nNevertheless, existing cascade approaches struggle to select a reliable\nrepresentative response and assess the overall reliability of free-form\noutputs, as they rely on exact text matching. To overcome these limitations, we\npropose Keyword-inspired Cascade (KiC), a novel framework for cost-efficient\nfree-form text generation. KiC identifies the most representative answer among\nmultiple outputs from a weaker model and evaluates the semantic alignment of\nother responses with it. Based on the degree of alignment, KiC determines\nwhether to accept the weaker model's output or escalate to a stronger model.\nExperiments on three free-form text generation benchmarks show that KiC\nachieves 97.53 percent of GPT-4's accuracy while reducing API costs by 28.81\npercent on average, and even outperforms GPT-4 in a specific benchmark.", "AI": {"tldr": "KiC is a cost-efficient framework for free-form text generation that achieves high accuracy while reducing API costs.", "motivation": "Existing cascade approaches struggle to select a reliable representative response and assess the overall reliability of free-form outputs, as they rely on exact text matching.", "method": "Keyword-inspired Cascade (KiC)", "result": "KiC achieves 97.53 percent of GPT-4's accuracy while reducing API costs by 28.81 percent on average, and even outperforms GPT-4 in a specific benchmark.", "conclusion": "KiC achieves 97.53 percent of GPT-4's accuracy while reducing API costs by 28.81 percent on average, and even outperforms GPT-4 in a specific benchmark."}}
{"id": "2507.13405", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13405", "abs": "https://arxiv.org/abs/2507.13405", "authors": ["Ishant Chintapatla", "Kazuma Choji", "Naaisha Agarwal", "Andrew Lin", "Hannah You", "Charles Duong", "Kevin Zhu", "Sean O'Brien", "Vasu Sharma"], "title": "COREVQA: A Crowd Observation and Reasoning Entailment Visual Question Answering Benchmark", "comment": null, "summary": "Recently, many benchmarks and datasets have been developed to evaluate\nVision-Language Models (VLMs) using visual question answering (VQA) pairs, and\nmodels have shown significant accuracy improvements. However, these benchmarks\nrarely test the model's ability to accurately complete visual entailment, for\ninstance, accepting or refuting a hypothesis based on the image. To address\nthis, we propose COREVQA (Crowd Observations and Reasoning Entailment), a\nbenchmark of 5608 image and synthetically generated true/false statement pairs,\nwith images derived from the CrowdHuman dataset, to provoke visual entailment\nreasoning on challenging crowded images. Our results show that even the\ntop-performing VLMs achieve accuracy below 80%, with other models performing\nsubstantially worse (39.98%-69.95%). This significant performance gap reveals\nkey limitations in VLMs' ability to reason over certain types of image-question\npairs in crowded scenes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u62e5\u6324\u573a\u666f\u4e2d\u8fdb\u884c\u89c6\u89c9\u8574\u6db5\u63a8\u7406\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u5f88\u5c11\u6d4b\u8bd5\u6a21\u578b\u51c6\u786e\u5b8c\u6210\u89c6\u89c9\u8574\u6db5\u7684\u80fd\u529b\uff0c\u4f8b\u5982\uff0c\u57fa\u4e8e\u56fe\u50cf\u63a5\u53d7\u6216\u53cd\u9a73\u5047\u8bbe\u3002", "method": "\u63d0\u51fa\u4e86COREVQA\uff08Crowd Observations and Reasoning Entailment\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b5608\u4e2a\u56fe\u50cf\u548c\u5408\u6210\u751f\u6210\u7684\u771f/\u5047\u8bed\u53e5\u5bf9\u7684\u57fa\u51c6\uff0c\u56fe\u50cf\u6765\u81eaCrowdHuman\u6570\u636e\u96c6\uff0c\u65e8\u5728\u6fc0\u53d1\u5bf9\u5177\u6709\u6311\u6218\u6027\u7684\u62e5\u6324\u56fe\u50cf\u7684\u89c6\u89c9\u8574\u6db5\u63a8\u7406\u3002", "result": "\u5373\u4f7f\u662f\u6700\u4f18\u79c0\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u7684\u51c6\u786e\u7387\u4e5f\u4f4e\u4e8e80%\uff0c\u5176\u4ed6\u6a21\u578b\u7684\u8868\u73b0\u5219\u66f4\u5dee(39.98%-69.95%)\u3002", "conclusion": "\u5373\u4f7f\u662f\u6700\u4f18\u79c0\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u7684\u51c6\u786e\u7387\u4e5f\u4f4e\u4e8e80%\uff0c\u5176\u4ed6\u6a21\u578b\u7684\u8868\u73b0\u5219\u66f4\u5dee(39.98%-69.95%)\u3002\u8fd9\u4e00\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\u63ed\u793a\u4e86VLMs\u5728\u5bf9\u62e5\u6324\u573a\u666f\u4e2d\u7684\u67d0\u4e9b\u7c7b\u578b\u7684\u56fe\u50cf-\u95ee\u9898\u5bf9\u8fdb\u884c\u63a8\u7406\u65f6\u7684\u5173\u952e\u5c40\u9650\u6027\u3002"}}
{"id": "2507.13685", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13685", "abs": "https://arxiv.org/abs/2507.13685", "authors": ["Yue Yang", "Zihan Su", "Ying Zhang", "Chang Chuan Goh", "Yuxiang Lin", "Anthony Graham Bellotti", "Boon Giin Lee"], "title": "Kolmogorov-Arnold Networks-based GRU and LSTM for Loan Default Early Prediction", "comment": null, "summary": "This study addresses a critical challenge in time series anomaly detection:\nenhancing the predictive capability of loan default models more than three\nmonths in advance to enable early identification of default events, helping\nfinancial institutions implement preventive measures before risk events\nmaterialize. Existing methods have significant drawbacks, such as their lack of\naccuracy in early predictions and their dependence on training and testing\nwithin the same year and specific time frames. These issues limit their\npractical use, particularly with out-of-time data. To address these, the study\nintroduces two innovative architectures, GRU-KAN and LSTM-KAN, which merge\nKolmogorov-Arnold Networks (KAN) with Gated Recurrent Units (GRU) and Long\nShort-Term Memory (LSTM) networks. The proposed models were evaluated against\nthe baseline models (LSTM, GRU, LSTM-Attention, and LSTM-Transformer) in terms\nof accuracy, precision, recall, F1 and AUC in different lengths of feature\nwindow, sample sizes, and early prediction intervals. The results demonstrate\nthat the proposed model achieves a prediction accuracy of over 92% three months\nin advance and over 88% eight months in advance, significantly outperforming\nexisting baselines.", "AI": {"tldr": "This paper introduces GRU-KAN and LSTM-KAN models for early loan default prediction, achieving over 92% accuracy three months in advance, outperforming existing methods.", "motivation": "Existing methods lack accuracy in early predictions and depend on training and testing within the same year and specific time frames, limiting their practical use with out-of-time data.", "method": "The study introduces two innovative architectures, GRU-KAN and LSTM-KAN, which merge Kolmogorov-Arnold Networks (KAN) with Gated Recurrent Units (GRU) and Long Short-Term Memory (LSTM) networks.", "result": "The proposed model achieves a prediction accuracy of over 92% three months in advance and over 88% eight months in advance, significantly outperforming existing baselines.", "conclusion": "The GRU-KAN and LSTM-KAN models significantly outperform existing baselines in predicting loan defaults, achieving high accuracy even 8 months in advance."}}
{"id": "2507.13681", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13681", "abs": "https://arxiv.org/abs/2507.13681", "authors": ["Haoyang Li", "Zhanchao Xu", "Yiming Li", "Xuejia Chen", "Darian Li", "Anxin Tian", "Qingfa Xiao", "Cheng Deng", "Jun Wang", "Qing Li", "Lei Chen", "Mingxuan Yuan"], "title": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues", "comment": null, "summary": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. In this paper,\nwe present LoopServe, an adaptive dual-phase inference acceleration framework\nfor large language models in multi-turn dialogues. LoopServe introduces two\nmain innovations. First, it performs online sparsification during the\nprefilling phase by dynamically selecting the most important parts of the\nattention matrix for each new input. Second, it uses progressive key value\ncompression during decoding by adaptively maintaining a relevant and efficient\ncache based on the most recently generated output tokens. We also propose a\n\\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new\nbenchmark} with eleven multi-turn datasets that reflect realistic query\npositions and conversational dependencies. Extensive experiments demonstrate\nthat LoopServe consistently achieves superior effectiveness compared to\nexisting baselines and significantly accelerates LLM inference across a wide\nrange of long-context dialogue tasks.", "AI": {"tldr": "LoopServe is an adaptive framework that accelerates large language model inference in multi-turn dialogues through online sparsification and progressive key value compression.", "motivation": "existing large language models face increasing computational and memory challenges in multi-turn dialogues, and current acceleration methods do not adapt well to dynamic conversational patterns", "method": "adaptive dual-phase inference acceleration framework with online sparsification and progressive key value compression", "result": "LoopServe achieves superior effectiveness and significantly accelerates LLM inference", "conclusion": "LoopServe consistently achieves superior effectiveness compared to existing baselines and significantly accelerates LLM inference across a wide range of long-context dialogue tasks."}}
{"id": "2507.14111", "categories": ["cs.AI", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14111", "abs": "https://arxiv.org/abs/2507.14111", "authors": ["Xiaoya Li", "Xiaofei Sun", "Albert Wang", "Jiwei Li", "Chris Shum"], "title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning", "comment": "Preprint Version", "summary": "The exponential growth in demand for GPU computing resources, driven by the\nrapid advancement of Large Language Models, has created an urgent need for\nautomated CUDA optimization strategies. While recent advances in LLMs show\npromise for code generation, current SOTA models (e.g. R1, o1) achieve low\nsuccess rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an\nautomated reinforcement learning framework for CUDA optimization.\n  CUDA-L1 achieves performance improvements on the CUDA optimization task:\ntrained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250\nCUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the\nmodel also demonstrates excellent portability across GPU architectures,\nachieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,\nx14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.\nBeyond these benchmark results, CUDA-L1 demonstrates several remarkable\nproperties: 1) Discovers a variety of CUDA optimization techniques and learns\nto combine them strategically to achieve optimal performance; 2) Uncovers\nfundamental principles of CUDA optimization; 3) Identifies non-obvious\nperformance bottlenecks and rejects seemingly beneficial optimizations that\nharm performance.\n  The capabilities of CUDA-L1 demonstrate that reinforcement learning can\ntransform an initially poor-performing LLM into an effective CUDA optimizer\nthrough speedup-based reward signals alone, without human expertise or domain\nknowledge. More importantly, the trained RL model extend the acquired reasoning\nabilities to new kernels. This paradigm opens possibilities for automated\noptimization of CUDA operations, and holds promise to substantially promote GPU\nefficiency and alleviate the rising pressure on GPU computing resources.", "AI": {"tldr": "Introduces CUDA-L1, a reinforcement learning framework for CUDA optimization, achieving significant speedups and portability across GPUs, demonstrating the potential of RL in automated CUDA optimization.", "motivation": "The exponential growth in demand for GPU computing resources, driven by the rapid advancement of Large Language Models, has created an urgent need for automated CUDA optimization strategies. While recent advances in LLMs show promise for code generation, current SOTA models achieve low success rates in improving CUDA speed.", "method": "an automated reinforcement learning framework for CUDA optimization", "result": "CUDA-L1 achieves performance improvements on the CUDA optimization task: trained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250 CUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the model also demonstrates excellent portability across GPU architectures, achieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40, x14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.", "conclusion": "Reinforcement learning can transform an initially poor-performing LLM into an effective CUDA optimizer through speedup-based reward signals alone, without human expertise or domain knowledge. The trained RL model extend the acquired reasoning abilities to new kernels. This paradigm opens possibilities for automated optimization of CUDA operations, and holds promise to substantially promote GPU efficiency and alleviate the rising pressure on GPU computing resources."}}
{"id": "2507.13407", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.13407", "abs": "https://arxiv.org/abs/2507.13407", "authors": ["Vinu Sankar Sadasivan", "Mehrdad Saberi", "Soheil Feizi"], "title": "IConMark: Robust Interpretable Concept-Based Watermark For AI Images", "comment": "Accepted at ICLR 2025 Workshop on GenAI Watermarking (WMARK)", "summary": "With the rapid rise of generative AI and synthetic media, distinguishing\nAI-generated images from real ones has become crucial in safeguarding against\nmisinformation and ensuring digital authenticity. Traditional watermarking\ntechniques have shown vulnerabilities to adversarial attacks, undermining their\neffectiveness in the presence of attackers. We propose IConMark, a novel\nin-generation robust semantic watermarking method that embeds interpretable\nconcepts into AI-generated images, as a first step toward interpretable\nwatermarking. Unlike traditional methods, which rely on adding noise or\nperturbations to AI-generated images, IConMark incorporates meaningful semantic\nattributes, making it interpretable to humans and hence, resilient to\nadversarial manipulation. This method is not only robust against various image\naugmentations but also human-readable, enabling manual verification of\nwatermarks. We demonstrate a detailed evaluation of IConMark's effectiveness,\ndemonstrating its superiority in terms of detection accuracy and maintaining\nimage quality. Moreover, IConMark can be combined with existing watermarking\ntechniques to further enhance and complement its robustness. We introduce\nIConMark+SS and IConMark+TM, hybrid approaches combining IConMark with\nStegaStamp and TrustMark, respectively, to further bolster robustness against\nmultiple types of image manipulations. Our base watermarking technique\n(IConMark) and its variants (+TM and +SS) achieve 10.8%, 14.5%, and 15.9%\nhigher mean area under the receiver operating characteristic curve (AUROC)\nscores for watermark detection, respectively, compared to the best baseline on\nvarious datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIConMark\u7684AI\u751f\u6210\u56fe\u50cf\u6c34\u5370\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5d4c\u5165\u53ef\u89e3\u91ca\u7684\u6982\u5ff5\u6765\u5b9e\u73b0\u9c81\u68d2\u6027\u548c\u53ef\u8bfb\u6027\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u533a\u5206AI\u751f\u6210\u7684\u56fe\u50cf\u548c\u771f\u5b9e\u56fe\u50cf\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u4f20\u7edf\u6c34\u5370\u6280\u672f\u5728\u5bf9\u6297\u6027\u653b\u51fb\u4e2d\u663e\u793a\u51fa\u6f0f\u6d1e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u751f\u6210\u5f0f\u9c81\u68d2\u8bed\u4e49\u6c34\u5370\u65b9\u6cd5IConMark\uff0c\u8be5\u65b9\u6cd5\u5c06\u53ef\u89e3\u91ca\u7684\u6982\u5ff5\u5d4c\u5165\u5230\u4eba\u5de5\u667a\u80fd\u751f\u6210\u7684\u56fe\u50cf\u4e2d\u3002", "result": "IConMark\u4e0d\u4ec5\u5bf9\u5404\u79cd\u56fe\u50cf\u589e\u5f3a\u5177\u6709\u9c81\u68d2\u6027\uff0c\u800c\u4e14\u662f\u4eba\u7c7b\u53ef\u8bfb\u7684\uff0c\u80fd\u591f\u624b\u52a8\u9a8c\u8bc1\u6c34\u5370\u3002\u8be6\u7ec6\u7684\u8bc4\u4f30\u8868\u660e\u4e86\u5176\u5728\u68c0\u6d4b\u7cbe\u5ea6\u548c\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "IConMark\u53ca\u5176\u53d8\u4f53\u5728\u6c34\u5370\u68c0\u6d4b\u65b9\u9762\u7684AUROC\u5f97\u5206\u6bd4\u6700\u4f73\u57fa\u7ebf\u5206\u522b\u9ad8\u51fa10.8%\u300114.5%\u548c15.9%\u3002"}}
{"id": "2507.13703", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13703", "abs": "https://arxiv.org/abs/2507.13703", "authors": ["Martin Krutsk\u00fd", "Gustav \u0160\u00edr", "Vyacheslav Kungurtsev", "Georgios Korpas"], "title": "Binarizing Physics-Inspired GNNs for Combinatorial Optimization", "comment": "Accepted to the 28th European Conference on Artificial Intelligence\n  (ECAI 2025). This archival version includes supplementary appendices", "summary": "Physics-inspired graph neural networks (PI-GNNs) have been utilized as an\nefficient unsupervised framework for relaxing combinatorial optimization\nproblems encoded through a specific graph structure and loss, reflecting\ndependencies between the problem's variables. While the framework has yielded\npromising results in various combinatorial problems, we show that the\nperformance of PI-GNNs systematically plummets with an increasing density of\nthe combinatorial problem graphs. Our analysis reveals an interesting phase\ntransition in the PI-GNNs' training dynamics, associated with degenerate\nsolutions for the denser problems, highlighting a discrepancy between the\nrelaxed, real-valued model outputs and the binary-valued problem solutions. To\naddress the discrepancy, we propose principled alternatives to the naive\nstrategy used in PI-GNNs by building on insights from fuzzy logic and binarized\nneural networks. Our experiments demonstrate that the portfolio of proposed\nmethods significantly improves the performance of PI-GNNs in increasingly dense\nsettings.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0 PI-GNNs \u5728\u5bc6\u96c6\u56fe\u4e0a\u6027\u80fd\u4e0b\u964d\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6539\u8fdb\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u867d\u7136 PI-GNNs \u6846\u67b6\u5728\u5404\u79cd\u7ec4\u5408\u95ee\u9898\u4e2d\u90fd\u53d6\u5f97\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\uff0c\u4f46\u672c\u6587\u8868\u660e\uff0cPI-GNNs \u7684\u6027\u80fd\u968f\u7740\u7ec4\u5408\u95ee\u9898\u56fe\u5bc6\u5ea6\u7684\u589e\u52a0\u800c\u7cfb\u7edf\u6027\u5730\u4e0b\u964d\u3002\u5206\u6790\u63ed\u793a\u4e86 PI-GNNs \u8bad\u7ec3\u52a8\u6001\u4e2d\u4e00\u4e2a\u6709\u8da3\u7684\u76f8\u53d8\uff0c\u8fd9\u4e0e\u66f4\u5bc6\u96c6\u95ee\u9898\u7684\u9000\u5316\u89e3\u76f8\u5173\uff0c\u7a81\u51fa\u4e86\u677e\u5f1b\u7684\u5b9e\u503c\u6a21\u578b\u8f93\u51fa\u4e0e\u4e8c\u503c\u95ee\u9898\u89e3\u4e4b\u95f4\u7684\u5dee\u5f02\u3002", "method": "\u672c\u6587\u901a\u8fc7\u6784\u5efa\u6a21\u7cca\u903b\u8f91\u548c\u4e8c\u503c\u5316\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u51fa\u4e86 \u043f\u0440\u0438\u043d\u0446\u0438\u043f\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u66ff\u4ee3\u7b56\u7565\uff0c\u4ee5\u6539\u8fdb PI-GNNs\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u7ec4\u5408\u663e\u8457\u63d0\u9ad8\u4e86 PI-GNNs \u5728\u65e5\u76ca\u5bc6\u96c6\u7684\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u57fa\u4e8e\u6a21\u7cca\u903b\u8f91\u548c\u4e8c\u503c\u5316\u795e\u7ecf\u7f51\u7edc\u7684\u539f\u5219\u6027\u66ff\u4ee3\u65b9\u6848\uff0c\u4ee5\u89e3\u51b3PI-GNNs\u5728\u5bc6\u96c6\u56fe\u4e0a\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347PI-GNNs\u5728\u65e5\u76ca\u5bc6\u96c6\u73af\u5883\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.12898", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12898", "abs": "https://arxiv.org/abs/2507.12898", "authors": ["Yao Feng", "Hengkai Tan", "Xinyi Mao", "Guodong Liu", "Shuhe Huang", "Chendong Xiang", "Hang Su", "Jun Zhu"], "title": "Generalist Bimanual Manipulation via Foundation Video Diffusion Models", "comment": null, "summary": "Bimanual robotic manipulation, which involves the coordinated control of two\nrobotic arms, is foundational for solving challenging tasks. Despite recent\nprogress in general-purpose manipulation, data scarcity and embodiment\nheterogeneity remain serious obstacles to further scaling up in bimanual\nsettings. In this paper, we introduce VIdeo Diffusion for Action Reasoning\n(VIDAR), a two-stage framework that leverages large-scale, diffusion-based\nvideo pre-training and a novel masked inverse dynamics model for action\nprediction. We pre-train the video diffusion model on 750K multi-view videos\nfrom three real-world bimanual robot platforms, utilizing a unified observation\nspace that encodes robot, camera, task, and scene contexts. Our masked inverse\ndynamics model learns masks to extract action-relevant information from\ngenerated trajectories without requiring pixel-level labels, and the masks can\neffectively generalize to unseen backgrounds. Our experiments demonstrate that\nwith only 20 minutes of human demonstrations on an unseen robot platform (only\n1% of typical data requirements), VIDAR generalizes to unseen tasks and\nbackgrounds with strong semantic understanding, surpassing state-of-the-art\nmethods. Our findings highlight the potential of video foundation models,\ncoupled with masked action prediction, to enable scalable and generalizable\nrobotic manipulation in diverse real-world settings.", "AI": {"tldr": "VIDAR\uff1a\u4e00\u79cd\u7528\u4e8e\u52a8\u4f5c\u63a8\u7406\u7684\u89c6\u9891\u6269\u6563\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u5927\u89c4\u6a21\u89c6\u9891\u9884\u8bad\u7ec3\u548c\u63a9\u853d\u9006\u52a8\u529b\u5b66\u6a21\u578b\u6765\u5b9e\u73b0\u53ef\u6269\u5c55\u548c\u901a\u7528\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u3002", "motivation": "\u53cc\u81c2\u673a\u5668\u4eba\u64cd\u4f5c\u662f\u89e3\u51b3\u5177\u6709\u6311\u6218\u6027\u4efb\u52a1\u7684\u57fa\u7840\u3002\u5c3d\u7ba1\u901a\u7528\u64cd\u4f5c\u6700\u8fd1\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u6570\u636e\u7a00\u7f3a\u548c\u5177\u4f53\u5316\u5f02\u8d28\u6027\u4ecd\u7136\u662f\u5728\u53cc\u81c2\u73af\u5883\u4e2d\u8fdb\u4e00\u6b65\u6269\u5c55\u7684\u4e25\u91cd\u969c\u788d\u3002", "method": "VIDAR\uff0c\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u5229\u7528\u57fa\u4e8e\u6269\u6563\u7684\u5927\u89c4\u6a21\u89c6\u9891\u9884\u8bad\u7ec3\u548c\u4e00\u4e2a\u7528\u4e8e\u52a8\u4f5c\u9884\u6d4b\u7684\u65b0\u578b\u63a9\u853d\u9006\u52a8\u529b\u5b66\u6a21\u578b\u3002", "result": "VIDAR\u63a8\u5e7f\u5230\u672a\u89c1\u8fc7\u7684\u4efb\u52a1\u548c\u80cc\u666f\uff0c\u5177\u6709\u5f3a\u5927\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u8d85\u8fc7\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "VIDAR\u5728\u672a\u89c1\u8fc7\u7684\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u4ec5\u752820\u5206\u949f\u7684\u4eba\u5de5\u6f14\u793a\uff08\u4ec5\u4e3a\u5178\u578b\u6570\u636e\u9700\u6c42\u76841%\uff09\u5373\u53ef\u63a8\u5e7f\u5230\u672a\u89c1\u8fc7\u7684\u4efb\u52a1\u548c\u80cc\u666f\uff0c\u5177\u6709\u5f3a\u5927\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u8d85\u8fc7\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u89c6\u9891\u57fa\u7840\u6a21\u578b\u4e0e\u63a9\u853d\u52a8\u4f5c\u9884\u6d4b\u76f8\u7ed3\u5408\u7684\u6f5c\u529b\uff0c\u4ece\u800c\u53ef\u4ee5\u5728\u5404\u79cd\u73b0\u5b9e\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u6269\u5c55\u548c\u901a\u7528\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u3002"}}
{"id": "2507.13408", "categories": ["cs.CV", "cs.AI", "68T07", "I.2.10"], "pdf": "https://arxiv.org/pdf/2507.13408", "abs": "https://arxiv.org/abs/2507.13408", "authors": ["Hemanth Kumar M", "Karthika M", "Saianiruth M", "Vasanthakumar Venugopal", "Anandakumar D", "Revathi Ezhumalai", "Charulatha K", "Kishore Kumar J", "Dayana G", "Kalyan Sivasailam", "Bargava Subramanian"], "title": "A Deep Learning-Based Ensemble System for Automated Shoulder Fracture Detection in Clinical Radiographs", "comment": "12 pages, 2 figures", "summary": "Background: Shoulder fractures are often underdiagnosed, especially in\nemergency and high-volume clinical settings. Studies report up to 10% of such\nfractures may be missed by radiologists. AI-driven tools offer a scalable way\nto assist early detection and reduce diagnostic delays. We address this gap\nthrough a dedicated AI system for shoulder radiographs. Methods: We developed a\nmulti-model deep learning system using 10,000 annotated shoulder X-rays.\nArchitectures include Faster R-CNN (ResNet50-FPN, ResNeXt), EfficientDet, and\nRF-DETR. To enhance detection, we applied bounding box and classification-level\nensemble techniques such as Soft-NMS, WBF, and NMW fusion. Results: The NMW\nensemble achieved 95.5% accuracy and an F1-score of 0.9610, outperforming\nindividual models across all key metrics. It demonstrated strong recall and\nlocalization precision, confirming its effectiveness for clinical fracture\ndetection in shoulder X-rays. Conclusion: The results show ensemble-based AI\ncan reliably detect shoulder fractures in radiographs with high clinical\nrelevance. The model's accuracy and deployment readiness position it well for\nintegration into real-time diagnostic workflows. The current model is limited\nto binary fracture detection, reflecting its design for rapid screening and\ntriage support rather than detailed orthopedic classification.", "AI": {"tldr": "Developed an AI system using deep learning and ensemble techniques to reliably detect shoulder fractures in X-rays, achieving high accuracy and F1-score.", "motivation": "Shoulder fractures are often underdiagnosed, especially in emergency and high-volume clinical settings. Studies report up to 10% of such fractures may be missed by radiologists. AI-driven tools offer a scalable way to assist early detection and reduce diagnostic delays. Addressed this gap through a dedicated AI system for shoulder radiographs.", "method": "Developed a multi-model deep learning system using 10,000 annotated shoulder X-rays. Architectures include Faster R-CNN (ResNet50-FPN, ResNeXt), EfficientDet, and RF-DETR. Enhanced detection using Soft-NMS, WBF, and NMW fusion.", "result": "The NMW ensemble achieved 95.5% accuracy and an F1-score of 0.9610, outperforming individual models across all key metrics. It demonstrated strong recall and localization precision, confirming its effectiveness for clinical fracture detection in shoulder X-rays.", "conclusion": "Ensemble-based AI can reliably detect shoulder fractures in radiographs with high clinical relevance. The model's accuracy and deployment readiness position it well for integration into real-time diagnostic workflows. The current model is limited to binary fracture detection, reflecting its design for rapid screening and triage support rather than detailed orthopedic classification."}}
{"id": "2507.13704", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.13704", "abs": "https://arxiv.org/abs/2507.13704", "authors": ["Anabel Yong", "Austin Tripp", "Layla Hosseini-Gerami", "Brooks Paige"], "title": "Bayesian Optimization for Molecules Should Be Pareto-Aware", "comment": null, "summary": "Multi-objective Bayesian optimization (MOBO) provides a principled framework\nfor navigating trade-offs in molecular design. However, its empirical\nadvantages over scalarized alternatives remain underexplored. We benchmark a\nsimple Pareto-based MOBO strategy -- Expected Hypervolume Improvement (EHVI) --\nagainst a simple fixed-weight scalarized baseline using Expected Improvement\n(EI), under a tightly controlled setup with identical Gaussian Process\nsurrogates and molecular representations. Across three molecular optimization\ntasks, EHVI consistently outperforms scalarized EI in terms of Pareto front\ncoverage, convergence speed, and chemical diversity. While scalarization\nencompasses flexible variants -- including random or adaptive schemes -- our\nresults show that even strong deterministic instantiations can underperform in\nlow-data regimes. These findings offer concrete evidence for the practical\nadvantages of Pareto-aware acquisition in de novo molecular optimization,\nespecially when evaluation budgets are limited and trade-offs are nontrivial.", "AI": {"tldr": "MOBO (EHVI) outperforms scalarized EI in molecular optimization, especially when data is limited.", "motivation": "The empirical advantages of multi-objective Bayesian optimization (MOBO) over scalarized alternatives in molecular design remain underexplored.", "method": "Benchmarked Expected Hypervolume Improvement (EHVI) against fixed-weight scalarized Expected Improvement (EI) using identical Gaussian Process surrogates and molecular representations.", "result": "EHVI consistently outperforms scalarized EI in terms of Pareto front coverage, convergence speed, and chemical diversity across three molecular optimization tasks.", "conclusion": "Pareto-aware acquisition functions like EHVI outperform scalarized approaches in de novo molecular optimization, especially in low-data regimes with nontrivial trade-offs."}}
{"id": "2507.13732", "categories": ["cs.CL", "cs.LG", "J.1; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.13732", "abs": "https://arxiv.org/abs/2507.13732", "authors": ["Guillaume Zambrano"], "title": "The Judge Variable: Challenging Judge-Agnostic Legal Judgment Prediction", "comment": "23 pages, 24 figures shorter version submitted to JURIX 2025", "summary": "This study examines the role of human judges in legal decision-making by\nusing machine learning to predict child physical custody outcomes in French\nappellate courts. Building on the legal realism-formalism debate, we test\nwhether individual judges' decision-making patterns significantly influence\ncase outcomes, challenging the assumption that judges are neutral variables\nthat apply the law uniformly. To ensure compliance with French privacy laws, we\nimplement a strict pseudonymization process. Our analysis uses 18,937 living\narrangements rulings extracted from 10,306 cases. We compare models trained on\nindividual judges' past rulings (specialist models) with a judge-agnostic model\ntrained on aggregated data (generalist models). The prediction pipeline is a\nhybrid approach combining large language models (LLMs) for structured feature\nextraction and ML models for outcome prediction (RF, XGB and SVC). Our results\nshow that specialist models consistently achieve higher predictive accuracy\nthan the general model, with top-performing models reaching F1 scores as high\nas 92.85%, compared to the generalist model's 82.63% trained on 20x to 100x\nmore samples. Specialist models capture stable individual patterns that are not\ntransferable to other judges. In-Domain and Cross-Domain validity tests provide\nempirical support for legal realism, demonstrating that judicial identity plays\na measurable role in legal outcomes. All data and code used will be made\navailable.", "AI": {"tldr": "This study uses machine learning to predict child custody outcomes, finding that individual judges' patterns significantly influence decisions, supporting legal realism.", "motivation": "To test whether individual judges' decision-making patterns significantly influence case outcomes, challenging the assumption that judges are neutral variables that apply the law uniformly.", "method": "A hybrid approach combining large language models (LLMs) for structured feature extraction and ML models for outcome prediction (RF, XGB and SVC).", "result": "Specialist models consistently achieve higher predictive accuracy than the general model, with top-performing models reaching F1 scores as high as 92.85%, compared to the generalist model's 82.63%.", "conclusion": "Specialist models consistently achieve higher predictive accuracy than the general model, with top-performing models reaching F1 scores as high as 92.85%, compared to the generalist model's 82.63% trained on 20x to 100x more samples. Specialist models capture stable individual patterns that are not transferable to other judges. In-Domain and Cross-Domain validity tests provide empirical support for legal realism, demonstrating that judicial identity plays a measurable role in legal outcomes."}}
{"id": "2507.13420", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13420", "abs": "https://arxiv.org/abs/2507.13420", "authors": ["Alessandro Pistola", "Valentina Orru'", "Nicolo' Marchetti", "Marco Roccetti"], "title": "AI-ming backwards: Vanishing archaeological landscapes in Mesopotamia and automatic detection of sites on CORONA imagery", "comment": "25 pages, 9 Figures", "summary": "By upgrading an existing deep learning model with the knowledge provided by\none of the oldest sets of grayscale satellite imagery, known as CORONA, we\nimproved the AI model attitude towards the automatic identification of\narchaeological sites in an environment which has been completely transformed in\nthe last five decades, including the complete destruction of many of those same\nsites. The initial Bing based convolutional network model was retrained using\nCORONA satellite imagery for the district of Abu Ghraib, west of Baghdad,\ncentral Mesopotamian floodplain. The results were twofold and surprising.\nFirst, the detection precision obtained on the area of interest increased\nsensibly: in particular, the Intersection over Union (IoU) values, at the image\nsegmentation level, surpassed 85 percent, while the general accuracy in\ndetecting archeological sites reached 90 percent. Second, our retrained model\nallowed the identification of four new sites of archaeological interest\n(confirmed through field verification), previously not identified by\narchaeologists with traditional techniques. This has confirmed the efficacy of\nusing AI techniques and the CORONA imagery from the 1960 to discover\narchaeological sites currently no longer visible, a concrete breakthrough with\nsignificant consequences for the study of landscapes with vanishing\narchaeological evidence induced by anthropization", "AI": {"tldr": "\u901a\u8fc7\u4f7f\u7528\u65e7\u7684\u536b\u661f\u56fe\u50cf\u91cd\u65b0\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u8be5\u7814\u7a76\u63d0\u9ad8\u4e86\u8003\u53e4\u9057\u5740\u7684\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u5e76\u53d1\u73b0\u4e86\u56db\u4e2a\u65b0\u7684\u8003\u53e4\u9057\u5740\u3002", "motivation": "\u5229\u7528CORONA\u7684\u77e5\u8bc6\u6765\u5347\u7ea7\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4ee5\u6539\u8fdbAI\u6a21\u578b\u5bf9\u8003\u53e4\u9057\u5740\u7684\u81ea\u52a8\u8bc6\u522b\u80fd\u529b\uff0c\u56e0\u4e3a\u5728\u8fc7\u53bb\u7684\u4e94\u5341\u5e74\u4e2d\uff0c\u73af\u5883\u5df2\u7ecf\u53d1\u751f\u4e86\u5f7b\u5e95\u7684\u6539\u53d8\uff0c\u5305\u62ec\u8bb8\u591a\u9057\u5740\u7684\u5b8c\u5168\u7834\u574f\u3002", "method": "\u4f7f\u7528CORONA\u536b\u661f\u56fe\u50cf\u5bf9\u57fa\u4e8eBing\u7684\u5377\u79ef\u7f51\u7edc\u6a21\u578b\u8fdb\u884c\u518d\u8bad\u7ec3\u3002", "result": "\u68c0\u6d4b\u7cbe\u5ea6\u663e\u8457\u63d0\u9ad8\uff1a\u5728\u56fe\u50cf\u5206\u5272\u5c42\u9762\uff0c\u4ea4\u5e76\u6bd4\uff08IoU\uff09\u503c\u8d85\u8fc785%\uff0c\u68c0\u6d4b\u8003\u53e4\u9057\u5740\u7684\u603b\u4f53\u51c6\u786e\u7387\u8fbe\u523090%\u3002\u91cd\u65b0\u8bad\u7ec3\u7684\u6a21\u578b\u80fd\u591f\u8bc6\u522b\u51fa\u56db\u4e2a\u65b0\u7684\u8003\u53e4\u9057\u5740\uff08\u901a\u8fc7\u73b0\u573a\u9a8c\u8bc1\u786e\u8ba4\uff09\uff0c\u8fd9\u4e9b\u9057\u5740\u4ee5\u524d\u672a\u88ab\u8003\u53e4\u5b66\u5bb6\u7528\u4f20\u7edf\u6280\u672f\u8bc6\u522b\u51fa\u6765\u3002", "conclusion": "\u4f7f\u7528AI\u6280\u672f\u548c1960\u5e74\u7684CORONA\u56fe\u50cf\u53ef\u4ee5\u53d1\u73b0\u76ee\u524d\u5df2\u4e0d\u53ef\u89c1\u7684\u8003\u53e4\u9057\u5740\uff0c\u5bf9\u4e8e\u7814\u7a76\u56e0\u4eba\u4e3a\u56e0\u7d20\u5bfc\u81f4\u8003\u53e4\u8bc1\u636e\u6d88\u5931\u7684\u666f\u89c2\u5177\u6709\u91cd\u5927\u610f\u4e49\u3002"}}
{"id": "2507.13707", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13707", "abs": "https://arxiv.org/abs/2507.13707", "authors": ["Hao Wang", "Yu Liu", "Daniel Biggs", "Haoru Wang", "Jiandong Yu", "Ping Huang"], "title": "Learning Deformable Body Interactions With Adaptive Spatial Tokenization", "comment": "21 pages, 15 figures", "summary": "Simulating interactions between deformable bodies is vital in fields like\nmaterial science, mechanical design, and robotics. While learning-based methods\nwith Graph Neural Networks (GNNs) are effective at solving complex physical\nsystems, they encounter scalability issues when modeling deformable body\ninteractions. To model interactions between objects, pairwise global edges have\nto be created dynamically, which is computationally intensive and impractical\nfor large-scale meshes. To overcome these challenges, drawing on insights from\ngeometric representations, we propose an Adaptive Spatial Tokenization (AST)\nmethod for efficient representation of physical states. By dividing the\nsimulation space into a grid of cells and mapping unstructured meshes onto this\nstructured grid, our approach naturally groups adjacent mesh nodes. We then\napply a cross-attention module to map the sparse cells into a compact,\nfixed-length embedding, serving as tokens for the entire physical state.\nSelf-attention modules are employed to predict the next state over these tokens\nin latent space. This framework leverages the efficiency of tokenization and\nthe expressive power of attention mechanisms to achieve accurate and scalable\nsimulation results. Extensive experiments demonstrate that our method\nsignificantly outperforms state-of-the-art approaches in modeling deformable\nbody interactions. Notably, it remains effective on large-scale simulations\nwith meshes exceeding 100,000 nodes, where existing methods are hindered by\ncomputational limitations. Additionally, we contribute a novel large-scale\ndataset encompassing a wide range of deformable body interactions to support\nfuture research in this area.", "AI": {"tldr": "This paper introduces an Adaptive Spatial Tokenization (AST) method to improve the scalability of GNNs for simulating deformable body interactions. It uses a grid-based approach and attention mechanisms to efficiently model large-scale meshes, outperforming existing methods and providing a new dataset for future research.", "motivation": "Learning-based methods with Graph Neural Networks (GNNs) encounter scalability issues when modeling deformable body interactions, especially with large-scale meshes, due to the computational intensity of creating pairwise global edges dynamically.", "method": "The paper proposes an Adaptive Spatial Tokenization (AST) method that divides the simulation space into a grid of cells and maps unstructured meshes onto this structured grid. A cross-attention module maps sparse cells into a compact, fixed-length embedding, and self-attention modules predict the next state over these tokens in latent space.", "result": "The proposed method achieves accurate and scalable simulation results and outperforms state-of-the-art approaches. It remains effective on large-scale simulations with meshes exceeding 100,000 nodes. The authors also contribute a novel large-scale dataset.", "conclusion": "The proposed method significantly outperforms state-of-the-art approaches in modeling deformable body interactions and remains effective on large-scale simulations with meshes exceeding 100,000 nodes."}}
