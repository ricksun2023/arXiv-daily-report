<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 45]
- [cs.CV](#cs.CV) [Total: 63]
- [cs.AI](#cs.AI) [Total: 13]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.IR](#cs.IR) [Total: 7]
- [cs.LG](#cs.LG) [Total: 61]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [ParallelSearch: Train your LLMs to Decompose Query and Search Sub-queries in Parallel with Reinforcement Learning](https://arxiv.org/abs/2508.09303)
*Shu Zhao,Tan Yu,Anbang Xu,Japinder Singh,Aaditya Shukla,Rama Akkiraju*

Main category: cs.CL

TL;DR: ParallelSearch 通过并行化查询处理提高了大型语言模型在多步骤信息检索中的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的方法存在一个基本的架构限制：它们严格按顺序处理搜索查询，即使在处理本质上可并行化和逻辑上独立的比较时也是如此。这种顺序瓶颈大大限制了计算效率，特别是对于需要多个实体比较的查询。

Method: 提出了 ParallelSearch，一种新颖的强化学习框架，它使大型语言模型 (LLM) 能够识别可并行化的查询结构并同时执行多个搜索操作。引入了专用奖励函数，以激励识别独立的查询组件，同时通过共同考虑正确性、查询分解质量和并行执行优势来保持答案准确性。

Result: ParallelSearch 优于最先进的基线。

Conclusion: ParallelSearch在七个问答基准测试中，平均性能比最先进的基线提高了 2.9%。在可并行化的问题上，该方法实现了 12.7% 的性能提升，而 LLM 调用次数仅为顺序方法的 69.6%。

Abstract: Reasoning-augmented search agents such as Search-R1, trained via
reinforcement learning with verifiable rewards (RLVR), demonstrate remarkable
capabilities in multi-step information retrieval from external knowledge
sources. These agents address the limitations of their parametric memory by
dynamically gathering relevant facts to address complex reasoning tasks.
However, existing approaches suffer from a fundamental architectural
limitation: they process search queries strictly sequentially, even when
handling inherently parallelizable and logically independent comparisons. This
sequential bottleneck significantly constrains computational efficiency,
particularly for queries that require multiple entity comparisons. To address
this critical limitation, we propose ParallelSearch, a novel reinforcement
learning framework that empowers large language models (LLMs) to recognize
parallelizable query structures and execute multiple search operations
concurrently. Our approach introduces dedicated reward functions that
incentivize the identification of independent query components while preserving
answer accuracy through jointly considering correctness, query decomposition
quality, and parallel execution benefits. Comprehensive experiments demonstrate
that ParallelSearch outperforms state-of-the-art baselines by an average
performance gain of 2.9% across seven question-answering benchmarks. Notably,
on parallelizable questions, our method achieves a 12.7% performance
improvement while requiring only 69.6% of the LLM calls compared to sequential
approaches.

</details>


### [2] [Leveraging Large Language Models for Rare Disease Named Entity Recognition](https://arxiv.org/abs/2508.09323)
*Nan Miles Xi,Yu Deng,Lin Wang*

Main category: cs.CL

TL;DR: GPT-4o, using optimized prompts, is a good alternative to traditional methods for rare disease NER, especially when data is scarce.


<details>
  <summary>Details</summary>
Motivation: NER in the rare disease domain is challenging due to limited labeled data, semantic ambiguity, and long-tail distributions.

Method: Evaluated GPT-4o for rare disease NER using prompt-based strategies like zero-shot, few-shot, RAG, and task-level fine-tuning. Introduced structured prompting and semantic few-shot example selection.

Result: GPT-4o achieves competitive or superior performance compared to BioClinicalBERT, with fine-tuning yielding state-of-the-art results. Few-shot prompting offers high returns at low cost.

Conclusion: Prompt-optimized LLMs are effective and scalable alternatives to traditional supervised models in biomedical NER, especially for rare diseases with limited annotated data.

Abstract: Named Entity Recognition (NER) in the rare disease domain poses unique
challenges due to limited labeled data, semantic ambiguity between entity
types, and long-tail distributions. In this study, we evaluate the capabilities
of GPT-4o for rare disease NER under low-resource settings, using a range of
prompt-based strategies including zero-shot prompting, few-shot in-context
learning, retrieval-augmented generation (RAG), and task-level fine-tuning. We
design a structured prompting framework that encodes domain-specific knowledge
and disambiguation rules for four entity types. We further introduce two
semantically guided few-shot example selection methods to improve in-context
performance while reducing labeling effort. Experiments on the RareDis Corpus
show that GPT-4o achieves competitive or superior performance compared to
BioClinicalBERT, with task-level fine-tuning yielding new state-of-the-art
(SOTA) results. Cost-performance analysis reveals that few-shot prompting
delivers high returns at low token budgets, while RAG offers marginal
additional benefit. An error taxonomy highlights common failure modes such as
boundary drift and type confusion, suggesting opportunities for post-processing
and hybrid refinement. Our results demonstrate that prompt-optimized LLMs can
serve as effective, scalable alternatives to traditional supervised models in
biomedical NER, particularly in rare disease applications where annotated data
is scarce.

</details>


### [3] [Columbo: Expanding Abbreviated Column Names for Tabular Data Using Large Language Models](https://arxiv.org/abs/2508.09403)
*Ting Cai,Stephen Sheen,AnHai Doan*

Main category: cs.CL

TL;DR: This paper introduces Columbo, a new LLM-based solution for expanding abbreviated column names in tables. It outperforms existing methods and uses new datasets and accuracy measures.


<details>
  <summary>Details</summary>
Motivation: Expanding abbreviated column names of tables is critical for numerous downstream data tasks. Synthetic public data used by prior work has major limitations, and accuracy measures used by prior work seriously undercount correct expansions.

Method: We develop Columbo, a powerful LLM-based solution that exploits context, rules, chain-of-thought reasoning, and token-level analysis.

Result: We introduce 4 new datasets in enterprise/science domains, with real-world abbreviations, and propose new synonym-aware measures that capture accuracy much more accurately.

Conclusion: Columbo significantly outperforms NameGuess by 4-29% over 5 datasets and has been used in production on EDI.

Abstract: Expanding the abbreviated column names of tables, such as ``esal'' to
``employee salary'', is critical for numerous downstream data tasks. This
problem arises in enterprises, domain sciences, government agencies, and more.
In this paper we make three contributions that significantly advances the state
of the art. First, we show that synthetic public data used by prior work has
major limitations, and we introduce 4 new datasets in enterprise/science
domains, with real-world abbreviations. Second, we show that accuracy measures
used by prior work seriously undercount correct expansions, and we propose new
synonym-aware measures that capture accuracy much more accurately. Finally, we
develop Columbo, a powerful LLM-based solution that exploits context, rules,
chain-of-thought reasoning, and token-level analysis. Extensive experiments
show that Columbo significantly outperforms NameGuess, the current most
advanced solution, by 4-29\%, over 5 datasets. Columbo has been used in
production on EDI, a major data portal for environmental sciences.

</details>


### [4] [TEN: Table Explicitization, Neurosymbolically](https://arxiv.org/abs/2508.09324)
*Nikita Mehrotra,Aayush Kumar,Sumit Gulwani,Arjun Radhakrishna,Ashish Tiwari*

Main category: cs.CL

TL;DR: TEN, a neurosymbolic approach, outperforms neural baselines in extracting tabular data from text by using a self-debug loop with symbolic checking and critique-LLM guidance.


<details>
  <summary>Details</summary>
Motivation: Extracting tabular data from semistructured input text is challenging, especially when text input does not use special delimiters consistently. Purely neural approaches perform poorly due to hallucinations and their inability to enforce hard constraints.

Method: a neurosymbolic approach, TEN, uses Structural Decomposition prompting on a large language model (LLM) to generate an initial table, and uses a symbolic checker to evaluate the table and detect hallucinations or forgetting. The output of the symbolic checker is processed by a critique-LLM to generate guidance for fixing the table, which is presented to the original LLM in a self-debug loop.

Result: TEN achieves significantly higher exact match accuracy and substantially reduced hallucination rates compared to purely neural baselines. User study shows TEN's tables are rated significantly more accurate and are consistently preferred for ease of verification and correction.

Conclusion: TEN significantly outperforms purely neural baselines across multiple datasets and metrics, achieving significantly higher exact match accuracy and substantially reduced hallucination rates. User study confirms that TEN's tables are more accurate and preferred for ease of verification and correction.

Abstract: We present a neurosymbolic approach, TEN, for extracting tabular data from
semistructured input text. This task is particularly challenging for text input
that does not use special delimiters consistently to separate columns and rows.
Purely neural approaches perform poorly due to hallucinations and their
inability to enforce hard constraints. TEN uses Structural Decomposition
prompting - a specialized chain-of-thought prompting approach - on a large
language model (LLM) to generate an initial table, and thereafter uses a
symbolic checker to evaluate not only the well-formedness of that table, but
also detect cases of hallucinations or forgetting. The output of the symbolic
checker is processed by a critique-LLM to generate guidance for fixing the
table, which is presented to the original LLM in a self-debug loop. Our
extensive experiments demonstrate that TEN significantly outperforms purely
neural baselines across multiple datasets and metrics, achieving significantly
higher exact match accuracy and substantially reduced hallucination rates. A
21-participant user study further confirms that TEN's tables are rated
significantly more accurate (mean score: 5.0 vs 4.3; p = 0.021), and are
consistently preferred for ease of verification and correction, with
participants favoring our method in over 60% of the cases.

</details>


### [5] [Decoding Neural Emotion Patterns through Natural Language Processing Embeddings](https://arxiv.org/abs/2508.09337)
*Gideon Vos,Maryam Ebrahimpour,Liza van Eijk,Zoltan Sarnyai,Mostafa Rahimi Azghadi*

Main category: cs.CL

TL;DR: A computational framework maps textual emotional content to brain regions, offering a cost-effective way to analyze language, differentiate clinical populations, and evaluate AI.


<details>
  <summary>Details</summary>
Motivation: Understanding how emotional expression in language relates to brain function is a challenge in computational neuroscience and affective computing. Traditional neuroimaging is costly and lab-bound, but abundant digital text offers new avenues for emotion-brain mapping. Prior work has largely examined neuroimaging-based emotion localization or computational text analysis separately, with little integration.

Method: We propose a computational framework that maps textual emotional content to anatomically defined brain regions without requiring neuroimaging. Using OpenAI's text-embedding-ada-002, we generate high-dimensional semantic representations, apply dimensionality reduction and clustering to identify emotional groups, and map them to 18 brain regions linked to emotional processing. Emotional intensity was scored via lexical analysis.

Result: Results showed neuroanatomically plausible mappings with high spatial specificity. Depressed subjects exhibited greater limbic engagement tied to negative affect. Discrete emotions were successfully differentiated. LLM-generated text matched humans in basic emotion distribution but lacked nuanced activation in empathy and self-referential regions (medial prefrontal and posterior cingulate cortex).

Conclusion: This cost-effective, scalable approach enables large-scale analysis of naturalistic language, distinguishes between clinical populations, and offers a brain-based benchmark for evaluating AI emotional expression.

Abstract: Understanding how emotional expression in language relates to brain function
is a challenge in computational neuroscience and affective computing.
Traditional neuroimaging is costly and lab-bound, but abundant digital text
offers new avenues for emotion-brain mapping. Prior work has largely examined
neuroimaging-based emotion localization or computational text analysis
separately, with little integration. We propose a computational framework that
maps textual emotional content to anatomically defined brain regions without
requiring neuroimaging. Using OpenAI's text-embedding-ada-002, we generate
high-dimensional semantic representations, apply dimensionality reduction and
clustering to identify emotional groups, and map them to 18 brain regions
linked to emotional processing. Three experiments were conducted: i) analyzing
conversational data from healthy vs. depressed subjects (DIAC-WOZ dataset) to
compare mapping patterns, ii) applying the method to the GoEmotions dataset and
iii) comparing human-written text with large language model (LLM) responses to
assess differences in inferred brain activation. Emotional intensity was scored
via lexical analysis. Results showed neuroanatomically plausible mappings with
high spatial specificity. Depressed subjects exhibited greater limbic
engagement tied to negative affect. Discrete emotions were successfully
differentiated. LLM-generated text matched humans in basic emotion distribution
but lacked nuanced activation in empathy and self-referential regions (medial
prefrontal and posterior cingulate cortex). This cost-effective, scalable
approach enables large-scale analysis of naturalistic language, distinguishes
between clinical populations, and offers a brain-based benchmark for evaluating
AI emotional expression.

</details>


### [6] [The Human-AI Hybrid Delphi Model: A Structured Framework for Context-Rich, Expert Consensus in Complex Domains](https://arxiv.org/abs/2508.09349)
*Cathy Speed,Ahmed A. Metwally*

Main category: cs.CL

TL;DR: 本研究提出了一种人机混合Delphi框架(HAH-Delphi)，通过整合AI模型和专家组来增强专家共识，并在多个领域验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在证据复杂、冲突或不足以进行直接指导的领域，专家共识起着关键作用。传统方法存在面板负担高、解释过于简单化和抑制条件细微差别等局限性。信息过载、证据基础碎片化以及对缺乏专家过滤的公开可用来源的日益依赖，加剧了这些挑战。

Method: 引入并评估了一种人机混合Delphi（HAH-Delphi）框架，该框架旨在通过整合生成式AI模型（Gemini 2.5 Pro）、小型高级人类专家组和结构化辅助来增强专家共识的形成。

Result: AI在第一阶段复制了95%的已发表专家共识结论，并在第二阶段与高级人类专家表现出95%的方向一致性，尽管它缺乏经验和务实的细微差别。在第三阶段，由六位高级专家组成的小组实现了>90%的共识覆盖率，并在最后一位参与者之前达到了主题饱和。AI提供了持续的、基于文献的支持，从而支持了分歧的解决并加速了饱和。 

Conclusion: HAH-Delphi框架为生成高质量、上下文相关的共识提供了一种灵活、可扩展的方法。它在健康、教练和表现科学领域的成功应用证实了其方法论的稳健性，并支持将其用作大规模生成有条件的、个性化指导和发布共识框架的基础。

Abstract: Expert consensus plays a critical role in domains where evidence is complex,
conflicting, or insufficient for direct prescription. Traditional methods, such
as Delphi studies, consensus conferences, and systematic guideline synthesis,
offer structure but face limitations including high panel burden, interpretive
oversimplification, and suppression of conditional nuance. These challenges are
now exacerbated by information overload, fragmentation of the evidence base,
and increasing reliance on publicly available sources that lack expert
filtering. This study introduces and evaluates a Human-AI Hybrid Delphi
(HAH-Delphi) framework designed to augment expert consensus development by
integrating a generative AI model (Gemini 2.5 Pro), small panels of senior
human experts, and structured facilitation. The HAH-Delphi was tested in three
phases: retrospective replication, prospective comparison, and applied
deployment in two applied domains (endurance training and resistance and mixed
cardio/strength training). The AI replicated 95% of published expert consensus
conclusions in Phase I and showed 95% directional agreement with senior human
experts in Phase II, though it lacked experiential and pragmatic nuance. In
Phase III, compact panels of six senior experts achieved >90% consensus
coverage and reached thematic saturation before the final participant. The AI
provided consistent, literature-grounded scaffolding that supported divergence
resolution and accelerated saturation. The HAH-Delphi framework offers a
flexible, scalable approach for generating high-quality, context-sensitive
consensus. Its successful application across health, coaching, and performance
science confirms its methodological robustness and supports its use as a
foundation for generating conditional, personalised guidance and published
consensus frameworks at scale.

</details>


### [7] [Flow-SLM: Joint Learning of Linguistic and Acoustic Information for Spoken Language Modeling](https://arxiv.org/abs/2508.09350)
*Ju-Chieh Chou,Jiawei Zhou,Karen Livescu*

Main category: cs.CL

TL;DR: 提出了一种联合建模语言和声学信息的方法，通过预测语义标记和连续声学帧表示，实现了与现有模型相当的性能，并提供更好的声学细节。


<details>
  <summary>Details</summary>
Motivation: 大多数无文本 SLM 学习预测下一个语义标记（语言内容的离散表示），并依靠单独的声码器将声学信息添加到生成的语音中。此类模型无法访问声学上下文，也无法内置对声学细节的控制。

Method: 通过生成语义标记和声学帧的连续实值表示来联合建模语言和声学信息。使用流匹配目标来预测以语义标记为条件的连续向量。

Result: 预测多个未来语义标记有助于保留语言信息。

Conclusion: 该方法在提示生成中提供了更好的声学细节，同时在语言可能性基准方面实现了与现有模型相当的性能。

Abstract: Textless spoken language models (SLMs) are generative models of speech that
do not rely on text supervision. Most textless SLMs learn to predict the next
semantic token, a discrete representation of linguistic content, and rely on a
separate vocoder to add acoustic information to the generated speech. Such
models have no access to acoustic context and no built-in control over acoustic
details. In this work, we propose to jointly model linguistic and acoustic
information by generating semantic tokens and a continuous real-valued
representation of the acoustic frame. We use a flow-matching objective to
predict the continuous vector conditioned on the semantic tokens. We study the
design space of this approach and find that predicting multiple future semantic
tokens helps preserve linguistic information. Our approach achieves comparable
performance to existing models in terms of linguistic likelihood benchmarks,
while providing better acoustic detail in prompted generation.

</details>


### [8] [APIO: Automatic Prompt Induction and Optimization for Grammatical Error Correction and Text Simplification](https://arxiv.org/abs/2508.09378)
*Artem Chernodub,Aman Saini,Yejin Huh,Vivek Kulkarni,Vipul Raheja*

Main category: cs.CL

TL;DR: This paper proposes APIO, a prompt induction and optimization approach for Grammatical Error Correction (GEC) and Text Simplification, without relying on manually specified seed prompts, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Recent advancements in large language models (LLMs) have enabled a wide range of natural language processing (NLP) tasks to be performed through simple prompt-based interactions. Consequently, several approaches have been proposed to engineer prompts that most effectively enable LLMs to perform a given task.

Method: APIO, a simple but effective prompt induction and optimization approach, without relying on manually specified seed prompts.

Result: APIO achieves a new state-of-the-art performance for purely LLM-based prompting methods on these tasks.

Conclusion: APIO achieves a new state-of-the-art performance for purely LLM-based prompting methods on GEC and Text Simplification.

Abstract: Recent advancements in large language models (LLMs) have enabled a wide range
of natural language processing (NLP) tasks to be performed through simple
prompt-based interactions. Consequently, several approaches have been proposed
to engineer prompts that most effectively enable LLMs to perform a given task
(e.g., chain-of-thought prompting). In settings with a well-defined metric to
optimize model performance, automatic prompt optimization (APO) methods have
been developed to refine a seed prompt. Advancing this line of research, we
propose APIO, a simple but effective prompt induction and optimization approach
for the tasks of Grammatical Error Correction (GEC) and Text Simplification,
without relying on manually specified seed prompts. APIO achieves a new
state-of-the-art performance for purely LLM-based prompting methods on these
tasks. We make our data, code, prompts, and outputs publicly available.

</details>


### [9] [Leveraging Zipformer Model for Effective Language Identification in Code-Switched Child-Directed Speech](https://arxiv.org/abs/2508.09430)
*Lavanya Shankar,Leibny Paola Garcia Perera*

Main category: cs.CL

TL;DR: 本文利用 Zipformer 解决双语环境中儿童导向场景下的代码切换和语言识别问题，实验结果表明 Zipformer 性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 代码切换和儿童导向场景中的语言识别提出了重大挑战，尤其是在双语环境中。

Method: 使用 Zipformer 处理语音中的细微差别，语音包含两种不平衡的语言：普通话和英语。

Result: Zipformer 的内部层有效地编码了语言特征，可用于语言识别。Zipformer 在不同的后端中表现出稳健性。

Conclusion: Zipformer在处理不平衡数据方面表现出色，语言识别平衡准确率达到 81.89%，比基线提高了 15.47%，证明了 transformer 编码器架构模型在实际场景中的潜力。

Abstract: Code-switching and language identification in child-directed scenarios
present significant challenges, particularly in bilingual environments. This
paper addresses this challenge by using Zipformer to handle the nuances of
speech, which contains two imbalanced languages, Mandarin and English, in an
utterance. This work demonstrates that the internal layers of the Zipformer
effectively encode the language characteristics, which can be leveraged in
language identification. We present the selection methodology of the inner
layers to extract the embeddings and make a comparison with different
back-ends. Our analysis shows that Zipformer is robust across these backends.
Our approach effectively handles imbalanced data, achieving a Balanced Accuracy
(BAC) of 81.89%, a 15.47% improvement over the language identification
baseline. These findings highlight the potential of the transformer encoder
architecture model in real scenarios.

</details>


### [10] [From Charts to Fair Narratives: Uncovering and Mitigating Geo-Economic Biases in Chart-to-Text](https://arxiv.org/abs/2508.09450)
*Ridwan Mahbub,Mohammed Saidul Islam,Mir Tafseer Nayeem,Md Tahmid Rahman Laskar,Mizanur Rahman,Shafiq Joty,Enamul Hoque*

Main category: cs.CL

TL;DR: 视觉语言模型在生成图表摘要时会放大地理经济偏见，对高收入国家更积极。


<details>
  <summary>Details</summary>
Motivation: 从图表中提取关键信息并用自然语言表达出来可能具有挑战性。虽然大型视觉语言模型 (VLM) 取得了快速进展，但很少有人关注其输出中可能存在的偏见。本文研究了 VLM 在生成图表摘要时如何放大地理经济偏见，从而可能造成社会危害。

Method: 对来自六个广泛使用的专有和开源模型的 6,000 个图表-国家对的 VLM 生成的图表摘要进行大规模的地理经济偏见评估，以了解一个国家的经济状况如何影响生成的摘要的情感。

Result: GPT-4o-mini、Gemini-1.5-Flash 和 Phi-3.5 等模型表现出不同程度的偏差。

Conclusion: 现有的视觉语言模型在生成图表摘要时，倾向于对高收入国家产生更积极的描述，而对中低收入国家则不然，即使仅改变国家属性这一个变量也是如此。使用基于提示的推理时去偏见技术只能部分有效，表明问题的复杂性，需要更强大的去偏见策略。

Abstract: Charts are very common for exploring data and communicating insights, but
extracting key takeaways from charts and articulating them in natural language
can be challenging. The chart-to-text task aims to automate this process by
generating textual summaries of charts. While with the rapid advancement of
large Vision-Language Models (VLMs), we have witnessed great progress in this
domain, little to no attention has been given to potential biases in their
outputs. This paper investigates how VLMs can amplify geo-economic biases when
generating chart summaries, potentially causing societal harm. Specifically, we
conduct a large-scale evaluation of geo-economic biases in VLM-generated chart
summaries across 6,000 chart-country pairs from six widely used proprietary and
open-source models to understand how a country's economic status influences the
sentiment of generated summaries. Our analysis reveals that existing VLMs tend
to produce more positive descriptions for high-income countries compared to
middle- or low-income countries, even when country attribution is the only
variable changed. We also find that models such as GPT-4o-mini,
Gemini-1.5-Flash, and Phi-3.5 exhibit varying degrees of bias. We further
explore inference-time prompt-based debiasing techniques using positive
distractors but find them only partially effective, underscoring the complexity
of the issue and the need for more robust debiasing strategies. Our code and
dataset are publicly available here.

</details>


### [11] [User-centric Subjective Leaderboard by Customizable Reward Modeling](https://arxiv.org/abs/2508.09463)
*Qi Jia,Xiujie Song,Zicheng Zhang,Yijin Guo,Kaiwei Zhang,Zijian Chen,Guangtao Zhai*

Main category: cs.CL

TL;DR: The paper introduces a User-Centric Subjective Leaderboard (USL) for LLMs, powered by Customizable Reward Models (CRMs) that outperform existing models in capturing diverse human preferences.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for large language models (LLMs) predominantely focus on assessing their capabilities through verifiable tasks, offering limited utility for practical LLM selection. To bridge this gap, the paper presents the first User-Centric Subjective Leaderboard (USL), which provides a preference-driven, dynamic ranking of LLMs across diverse real-world scenarios.

Method: Introduction of Customizable Reward Models (CRMs).

Result: CRMs with only 4B parameters surpasses the performance of leading models such as GPT-4.1 and Gemini-2.5-pro, showing exceptional generalization capabilities across new topics and criteria.

Conclusion: The User-Centric Subjective Leaderboard (USL), powered by Customizable Reward Models (CRMs), exhibits strong negative correlations to contradictory preferences.

Abstract: Existing benchmarks for large language models (LLMs) predominantely focus on
assessing their capabilities through verifiable tasks. Such objective and
static benchmarks offer limited utility for practical LLM selection, making it
difficult for users to find suitable models for their individual needs. To
bridge this gap, we present the first User-Centric Subjective Leaderboard
(USL), which provides a preference-driven, dynamic ranking of LLMs across
diverse real-world scenarios. Our work is built upon a thorough investigation
of real human preference data, involving more than 10K subjective queries. Our
investigation reveals significant diversity and contradictions in human
preferences, which limit the effectiveness of state-of-the-art reward models.
To address this, we introduce Customizable Reward Models (CRMs). With only 4B
parameters, our CRM surpasses the performance of leading models such as GPT-4.1
and Gemini-2.5-pro, showing exceptional generalization capabilities across new
topics and criteria. The USL, powered by CRMs, exhibits strong negative
correlations to contradictory preferences.

</details>


### [12] [Learning Facts at Scale with Active Reading](https://arxiv.org/abs/2508.09494)
*Jessy Lin,Vincent-Pierre Berges,Xilun Chen,Wen-Tau Yih,Gargi Ghosh,Barlas Oğuz*

Main category: cs.CL

TL;DR: Active Reading improves factual knowledge absorption in LLMs, leading to better performance on factual QA tasks, as demonstrated by Meta WikiExpert-8B.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with reliably learning and recalling facts, and practitioners lack tools to ensure consistent knowledge absorption.

Method: Active Reading: a framework where models train with self-generated learning strategies to study a given set of material.

Result: Models trained with Active Reading absorb significantly more knowledge than vanilla finetuning, achieving substantial gains on SimpleQA and FinanceBench. Meta WikiExpert-8B, trained with Active Reading, outperforms models with hundreds of billions of parameters on factual QA.

Conclusion: Active Reading can be utilized at pre-training scale to build more factual models, demonstrated by Meta WikiExpert-8B which outperforms larger models on factual QA.

Abstract: LLMs are known to store vast amounts of knowledge in their parametric memory.
However, learning and recalling facts from this memory is known to be
unreliable, depending largely on the prevalence of particular facts in the
training data and other factors which are poorly understood. Practitioners are
lacking tools which will allow them to ensure that the models learn a given
body of knowledge reliably and consistently. To this end, we propose Active
Reading: a framework where we train models to study a given set of material
with self-generated learning strategies. First, we demonstrate models trained
with Active Reading on expert domains absorb significantly more knowledge than
vanilla finetuning and other data augmentations. We train expert 8B models that
achieve 66% on a Wikipedia-grounded subset of SimpleQA (+313% relative over
vanilla finetuning) and 26% on FinanceBench (+160% relative over vanilla
finetuning) by applying Active Reading to the source documents for each
benchmark. Finally, we show that Active Reading can be utilized at pre-training
scale to build more factual models. As a demonstration of this, we release Meta
WikiExpert-8B, a Wikipedia-expert model trained on 1 trillion generated tokens,
which outcompetes models with hundreds of billions of parameters on factual QA.

</details>


### [13] [From Ranking to Selection: A Simple but Efficient Dynamic Passage Selector for Retrieval Augmented Generation](https://arxiv.org/abs/2508.09497)
*Siyuan Meng,Junming Liu,Yirong Chen,Song Mao,Pinlong Cai,Guohang Yan,Botian Shi,Ding Wang*

Main category: cs.CL

TL;DR: DPS 是一种新颖的重新排序框架，它通过自适应证据选择，显著提高了复杂RAG场景中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成 (RAG) 系统通常受到其重新排序模块的瓶颈限制，这些模块通常独立地对段落进行评分并选择固定的 Top-K 大小。这种方法难以处理需要跨多个文档合成证据的复杂多跳查询，从而导致小 K 值省略关键信息和大 K 值引入噪声之间的权衡。

Method: 我们引入了动态段落选择器 (DPS)，这是一种新颖的重新排序框架，它将段落选择视为一个监督学习问题。与传统的点状或列表状方法不同，DPS经过微调，可以捕获段落间的依赖关系，并动态选择最相关的段落集以进行生成。

Result: 在五个基准上的综合评估表明，DPS始终优于最先进的重新排序器和微调方法。值得注意的是，在具有挑战性的 MuSiQue 数据集上，DPS 比 Qwen3-reranker 和 RankingGPT 等强大的基线分别提高了 30.06% 和 15.4% 的 F1 分数。

Conclusion: DPS通过自适应证据选择，显著提高了复杂RAG场景中的推理能力。

Abstract: Retrieval-augmented generation (RAG) systems are often bottlenecked by their
reranking modules, which typically score passages independently and select a
fixed Top-K size. This approach struggles with complex multi-hop queries that
require synthesizing evidence across multiple documents, creating a trade-off
where small K values omit crucial information and large K values introduce
noise. To address this, we introduce the Dynamic Passage Selector (DPS), a
novel reranking framework that treats passage selection as a supervised
learning problem. Unlike traditional point-wise or list-wise methods, DPS is
fine-tuned to capture inter-passage dependencies and dynamically select the
most relevant set of passages for generation. As a seamless plug-and-play
module, DPS requires no modifications to the standard RAG pipeline.
Comprehensive evaluations on five benchmarks show that DPS consistently
outperforms state-of-the-art rerankers and fine-tuning methods. Notably, on the
challenging MuSiQue dataset, DPS improves the F1-score by 30.06% and 15.4% over
strong baselines like Qwen3-reranker and RankingGPT, respectively. Our results
demonstrate that by enabling adaptive evidence selection, DPS substantially
enhances reasoning capabilities in complex RAG scenarios.

</details>


### [14] [LACA: Improving Cross-lingual Aspect-Based Sentiment Analysis with LLM Data Augmentation](https://arxiv.org/abs/2508.09515)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 提出了一种新的跨语言情感分析方法，该方法利用大型语言模型生成伪标记数据，避免了对翻译工具的依赖，并在多个语言和模型上取得了优于现有技术的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数跨语言方面情感分析（ABSA）方法严重依赖于通常不可靠的翻译工具来弥合语言差距。

Method: 利用大型语言模型（LLM）生成高质量的伪标记数据，而无需翻译工具。首先，该框架训练一个ABSA模型以获得未标记目标语言数据的预测。接下来，提示LLM生成比原始文本更好地表示这些嘈杂预测的自然句子。然后，在生成的伪标记数据集上进一步微调ABSA模型。

Result: 该方法在六种语言和五个骨干模型上展示了有效性，超过了以往基于翻译的最先进方法。所提出的框架还支持生成模型，并且微调的LLM优于较小的多语言模型。

Conclusion: 该方法在六种语言和五个骨干模型上展示了有效性，超过了以往基于翻译的最先进方法。所提出的框架还支持生成模型，并且微调的LLM优于较小的多语言模型。

Abstract: Cross-lingual aspect-based sentiment analysis (ABSA) involves detailed
sentiment analysis in a target language by transferring knowledge from a source
language with available annotated data. Most existing methods depend heavily on
often unreliable translation tools to bridge the language gap. In this paper,
we propose a new approach that leverages a large language model (LLM) to
generate high-quality pseudo-labelled data in the target language without the
need for translation tools. First, the framework trains an ABSA model to obtain
predictions for unlabelled target language data. Next, LLM is prompted to
generate natural sentences that better represent these noisy predictions than
the original text. The ABSA model is then further fine-tuned on the resulting
pseudo-labelled dataset. We demonstrate the effectiveness of this method across
six languages and five backbone models, surpassing previous state-of-the-art
translation-based approaches. The proposed framework also supports generative
models, and we show that fine-tuned LLMs outperform smaller multilingual
models.

</details>


### [15] [Cross-lingual Aspect-Based Sentiment Analysis: A Survey on Tasks, Approaches, and Challenges](https://arxiv.org/abs/2508.09516)
*Jakub Šmíd,Pavel Král*

Main category: cs.CL

TL;DR: This paper presents a comprehensive survey of cross-lingual ABSA, summarizing tasks, datasets, and methods, while also highlighting challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: Cross-lingual ABSA, which aims to transfer knowledge from resource-rich languages to low-resource languages, remains an under-explored area, with no systematic review of the field. This paper aims to fill that gap by providing a comprehensive survey of cross-lingual ABSA.

Method: The paper provides a comprehensive survey of cross-lingual ABSA. It summarizes key ABSA tasks, reviews datasets, modelling paradigms, and cross-lingual transfer methods. It also examines how existing work in monolingual and multilingual ABSA, as well as ABSA with LLMs, contributes to the development of cross-lingual ABSA.

Result: The paper summarizes key ABSA tasks, including aspect term extraction, aspect sentiment classification, and compound tasks involving multiple sentiment elements. Additionally, it reviews the datasets, modelling paradigms, and cross-lingual transfer methods used to solve these tasks.

Conclusion: This paper highlights the main challenges and suggests directions for future research to advance cross-lingual ABSA systems.

Abstract: Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis
task that focuses on understanding opinions at the aspect level, including
sentiment towards specific aspect terms, categories, and opinions. While ABSA
research has seen significant progress, much of the focus has been on
monolingual settings. Cross-lingual ABSA, which aims to transfer knowledge from
resource-rich languages (such as English) to low-resource languages, remains an
under-explored area, with no systematic review of the field. This paper aims to
fill that gap by providing a comprehensive survey of cross-lingual ABSA. We
summarize key ABSA tasks, including aspect term extraction, aspect sentiment
classification, and compound tasks involving multiple sentiment elements.
Additionally, we review the datasets, modelling paradigms, and cross-lingual
transfer methods used to solve these tasks. We also examine how existing work
in monolingual and multilingual ABSA, as well as ABSA with LLMs, contributes to
the development of cross-lingual ABSA. Finally, we highlight the main
challenges and suggest directions for future research to advance cross-lingual
ABSA systems.

</details>


### [16] [UWBa at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval](https://arxiv.org/abs/2508.09517)
*Ladislav Lenc,Daniel Cífka,Jiří Martínek,Jakub Šmíd,Pavel Král*

Main category: cs.CL

TL;DR: This paper presents a zero-shot fact-checked claim retrieval system using combined LLM embeddings, achieving 7th/9th place in monolingual/cross-lingual tasks, with NVIDIA NV-Embed-v2 performing best.


<details>
  <summary>Details</summary>
Motivation: The paper aims to develop a zero-shot system for fact-checked claim retrieval.

Method: The paper employs state-of-the-art large language models to obtain text embeddings, combined to optimize performance, and uses cosine similarity to identify relevant claims.

Result: The system achieved 7th place in monolingual and 9th place in cross-lingual subtasks. The NVIDIA NV-Embed-v2 model performed best overall, and model combinations improved results for some languages.

Conclusion: The paper's approach, which combines state-of-the-art large language models for text embeddings and cosine similarity measurement, achieved 7th place in monolingual and 9th in cross-lingual fact-checked claim retrieval subtasks, with the NVIDIA NV-Embed-v2 model yielding the best overall results.

Abstract: This paper presents a zero-shot system for fact-checked claim retrieval. We
employed several state-of-the-art large language models to obtain text
embeddings. The models were then combined to obtain the best possible result.
Our approach achieved 7th place in monolingual and 9th in cross-lingual
subtasks. We used only English translations as an input to the text embedding
models since multilingual models did not achieve satisfactory results. We
identified the most relevant claims for each post by leveraging the embeddings
and measuring cosine similarity. Overall, the best results were obtained by the
NVIDIA NV-Embed-v2 model. For some languages, we benefited from model
combinations (NV-Embed & GPT or Mistral).

</details>


### [17] [COMPEER: Controllable Empathetic Reinforcement Reasoning for Emotional Support Conversation](https://arxiv.org/abs/2508.09521)
*Yunxiao Wang,Meng Liu,Wenqi Liu,Kaiyu Jiang,Bin Wen,Fan Yang,Tingting Gao,Guorui Zhou,Liqiang Nie*

Main category: cs.CL

TL;DR: 提出了可控的共情推理，它结合了自然语言推理和结构化的心理学步骤，并构建了一个细粒度的数据集。


<details>
  <summary>Details</summary>
Motivation: 当前的模型通常缺乏基于心理学原理的深刻共情推理。为了解决这个问题

Method: 结合自然语言推理和结构化心理学步骤，我们提出了可控的共情推理

Result: 构建了一个细粒度的数据集，该数据集带有推理正确性和响应偏好的注释，以实现此功能。为了进一步加强训练，我们采用强化学习以及统一的过程-结果奖励模型，该模型可提供精确的反馈。为了减轻熵崩溃引起的响应重复性，我们引入了基于人格的对话重写和具有冗余意识的奖励重新加权策略。

Conclusion: 该方法显著提高了模型的情感支持能力，从而推进了富有同情心、类人支持系统的发展。

Abstract: Emotional support conversations are crucial for promoting emotional
well-being, yet current models often lack deep empathetic reasoning grounded in
psychological principles. To address this, we propose controllable empathetic
reasoning, which combines natural language reasoning with structured
psychological steps. We construct a fine-grained dataset annotated with
reasoning correctness and response preferences to enable this capability. To
further enhance training, we employ reinforcement learning with a unified
process-outcome reward model that delivers precise feedback. To mitigate
response repetitiveness from entropy collapse, we introduce personality-based
dialogue rewriting and a redundancy-aware reward reweighting strategy. Our
approach significantly improves model's emotional support ability, advancing
the development of empathetic, human-like support systems.

</details>


### [18] [The Surprising Effectiveness of Membership Inference with Simple N-Gram Coverage](https://arxiv.org/abs/2508.09603)
*Skyler Hallinan,Jaehun Jung,Melanie Sclar,Ximing Lu,Abhilasha Ravichander,Sahana Ramnath,Yejin Choi,Sai Praneeth Karimireddy,Niloofar Mireshghallah,Xiang Ren*

Main category: cs.CL

TL;DR: Introduces N-Gram Coverage Attack, a black-box membership inference attack that uses text outputs to detect memorization in language models. GPT-4o is more robust to this attack.


<details>
  <summary>Details</summary>
Motivation: Many current state-of-the-art attacks require access to models' hidden states or probability distribution, which prevents investigation into more widely-used, API-access only models like GPT-4.

Method: N-Gram Coverage Attack, a membership inference attack that relies solely on text outputs from the target model

Result: N-Gram Coverage Attack outperforms other black-box methods while also impressively achieving comparable or even better performance to state-of-the-art white-box attacks. the success rate of our method scales with the attack compute budget

Conclusion: More recent models, such as GPT-4o, exhibit increased robustness to membership inference, suggesting an evolving trend toward improved privacy protections.

Abstract: Membership inference attacks serves as useful tool for fair use of language
models, such as detecting potential copyright infringement and auditing data
leakage. However, many current state-of-the-art attacks require access to
models' hidden states or probability distribution, which prevents investigation
into more widely-used, API-access only models like GPT-4. In this work, we
introduce N-Gram Coverage Attack, a membership inference attack that relies
solely on text outputs from the target model, enabling attacks on completely
black-box models. We leverage the observation that models are more likely to
memorize and subsequently generate text patterns that were commonly observed in
their training data. Specifically, to make a prediction on a candidate member,
N-Gram Coverage Attack first obtains multiple model generations conditioned on
a prefix of the candidate. It then uses n-gram overlap metrics to compute and
aggregate the similarities of these outputs with the ground truth suffix; high
similarities indicate likely membership. We first demonstrate on a diverse set
of existing benchmarks that N-Gram Coverage Attack outperforms other black-box
methods while also impressively achieving comparable or even better performance
to state-of-the-art white-box attacks - despite having access to only text
outputs. Interestingly, we find that the success rate of our method scales with
the attack compute budget - as we increase the number of sequences generated
from the target model conditioned on the prefix, attack performance tends to
improve. Having verified the accuracy of our method, we use it to investigate
previously unstudied closed OpenAI models on multiple domains. We find that
more recent models, such as GPT-4o, exhibit increased robustness to membership
inference, suggesting an evolving trend toward improved privacy protections.

</details>


### [19] [AINL-Eval 2025 Shared Task: Detection of AI-Generated Scientific Abstracts in Russian](https://arxiv.org/abs/2508.09622)
*Tatiana Batura,Elena Bruches,Milana Shvenk,Valentin Malykh*

Main category: cs.CL

TL;DR: AINL-Eval 2025: A shared task for detecting AI-generated scientific abstracts in Russian, featuring a large dataset and a continuous platform for ongoing research.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of large language models (LLMs) has revolutionized text generation, making it increasingly difficult to distinguish between human- and AI-generated content, posing a significant challenge to academic integrity.

Method: A novel, large-scale dataset comprising 52,305 samples, including human-written abstracts and AI-generated counterparts from five state-of-the-art LLMs.

Result: The task was organized in two phases, attracting 10 teams and 159 submissions, with top systems demonstrating strong performance in identifying AI-generated content.

Conclusion: The AINL-Eval 2025 Shared Task attracted 10 teams and 159 submissions, with top systems demonstrating strong performance in identifying AI-generated content. A continuous shared task platform was established to foster ongoing research.

Abstract: The rapid advancement of large language models (LLMs) has revolutionized text
generation, making it increasingly difficult to distinguish between human- and
AI-generated content. This poses a significant challenge to academic integrity,
particularly in scientific publishing and multilingual contexts where detection
resources are often limited. To address this critical gap, we introduce the
AINL-Eval 2025 Shared Task, specifically focused on the detection of
AI-generated scientific abstracts in Russian. We present a novel, large-scale
dataset comprising 52,305 samples, including human-written abstracts across 12
diverse scientific domains and AI-generated counterparts from five
state-of-the-art LLMs (GPT-4-Turbo, Gemma2-27B, Llama3.3-70B, Deepseek-V3, and
GigaChat-Lite). A core objective of the task is to challenge participants to
develop robust solutions capable of generalizing to both (i) previously unseen
scientific domains and (ii) models not included in the training data. The task
was organized in two phases, attracting 10 teams and 159 submissions, with top
systems demonstrating strong performance in identifying AI-generated content.
We also establish a continuous shared task platform to foster ongoing research
and long-term progress in this important area. The dataset and platform are
publicly available at https://github.com/iis-research-team/AINL-Eval-2025.

</details>


### [20] [Improving Diversity in Language Models: When Temperature Fails, Change the Loss](https://arxiv.org/abs/2508.09654)
*Alexandre Verine,Florian Le Bronnec,Kunhao Zheng,Alexandre Allauzen,Yann Chevaleyre,Benjamin Negrevergne*

Main category: cs.CL

TL;DR: This paper analyzes why increasing temperature often fails to boost coverage and finds that for a model to be effectively tunable through temperature adjustments, it must be trained toward coverage. The paper proposes rethinking loss functions in language models by leveraging the Precision-Recall framework. This approach achieves a substantially better trade-off between Precision and Recall than merely combining negative log-likelihood training with temperature scaling.


<details>
  <summary>Details</summary>
Motivation: Increasing diversity in language models is a challenging yet essential objective. A common approach is to raise the decoding temperature. In this work, we investigate this approach through a simplistic yet common case to provide insights into why decreasing temperature can improve quality (Precision), while increasing it often fails to boost coverage (Recall). Our analysis reveals that for a model to be effectively tunable through temperature adjustments, it must be trained toward coverage

Method: investigate temperature adjustment through a simplistic yet common case to provide insights into why decreasing temperature can improve quality, while increasing it often fails to boost coverage. propose rethinking loss functions in language models by leveraging the Precision-Recall framework

Result: achieves a substantially better trade-off between Precision and Recall than merely combining negative log-likelihood training with temperature scaling. These findings offer a pathway toward more versatile and robust language modeling techniques.

Conclusion: Rethinking loss functions in language models by leveraging the Precision-Recall framework achieves a substantially better trade-off between Precision and Recall than merely combining negative log-likelihood training with temperature scaling.

Abstract: Increasing diversity in language models is a challenging yet essential
objective. A common approach is to raise the decoding temperature. In this
work, we investigate this approach through a simplistic yet common case to
provide insights into why decreasing temperature can improve quality
(Precision), while increasing it often fails to boost coverage (Recall). Our
analysis reveals that for a model to be effectively tunable through temperature
adjustments, it must be trained toward coverage. To address this, we propose
rethinking loss functions in language models by leveraging the Precision-Recall
framework. Our results demonstrate that this approach achieves a substantially
better trade-off between Precision and Recall than merely combining negative
log-likelihood training with temperature scaling. These findings offer a
pathway toward more versatile and robust language modeling techniques.

</details>


### [21] [EffiEval: Efficient and Generalizable Model Evaluation via Capability Coverage Maximization](https://arxiv.org/abs/2508.09662)
*Yaoning Wang,Jiahao Ying,Yixin Cao,Yubo Ma,Yugang Jiang*

Main category: cs.CL

TL;DR: EffiEval: A training-free approach for efficient and reliable LLM benchmarking, addressing data redundancy and computational challenges.


<details>
  <summary>Details</summary>
Motivation: the development of increasingly large and diverse evaluation benchmarks have introduced substantial computational challenges for model assessment

Method: training-free approach for efficient benchmarking that effectively addresses data redundancy while maintaining high evaluation reliability. adaptively selects high-quality representative subsets based on the Model Utility Index (MUI).

Result: achieves strong ranking consistency with full-dataset evaluation using only a small fraction of the original data. Furthermore, our method is flexible and scalable in size, allowing users to balance evaluation efficiency and representativeness according to specific needs.

Conclusion: EffiEval provides a practical and generalizable solution for reliable, fair, and efficient evaluation in the era of LLMs.

Abstract: The rapid advancement of large language models (LLMs) and the development of
increasingly large and diverse evaluation benchmarks have introduced
substantial computational challenges for model assessment. In this paper, we
present EffiEval, a training-free approach for efficient benchmarking that
effectively addresses data redundancy while maintaining high evaluation
reliability. Our method is specifically designed to meet three key criteria for
high-quality evaluation: representativeness, by ensuring comprehensive coverage
of model capabilities; fairness, by remaining independent of model performance
during sample selection to avoid bias; and generalizability, by enabling
flexible transfer across datasets and model families without reliance on
large-scale evaluation data. Unlike traditional methods that rely on absolute
performance or require extensive evaluation data, our approach adaptively
selects high-quality representative subsets based on the Model Utility Index
(MUI). Extensive experiments on multiple public benchmarks and diverse LLMs
demonstrate that EffiEval achieves strong ranking consistency with full-dataset
evaluation using only a small fraction of the original data. Furthermore, our
method is flexible and scalable in size, allowing users to balance evaluation
efficiency and representativeness according to specific needs. Overall,
EffiEval provides a practical and generalizable solution for reliable, fair,
and efficient evaluation in the era of LLMs.

</details>


### [22] [Slow Tuning and Low-Entropy Masking for Safe Chain-of-Thought Distillation](https://arxiv.org/abs/2508.09666)
*Ziyang Ma,Qingyue Yuan,Linhai Zhang,Deyu Zhou*

Main category: cs.CL

TL;DR: SLowED, a safe distillation method, maintains SLM safety while improving reasoning by using slow tuning and low-entropy masking to avoid harmful content during training.


<details>
  <summary>Details</summary>
Motivation: Previous CoT distillation methods enhanced SLM reasoning but negatively impacted safety. Existing safety alignment methods require extra computation/data and may impact reasoning ability.

Method: Slow Tuning and Low-Entropy Masking Distillation (SLowED), containing two modules: Slow Tuning and Low-Entropy Masking. Slow Tuning scales down the magnitude of model weight changes. Low-Entropy Masking masks low-entropy tokens.

Result: Experiments show SLowED retains safety and improves reasoning. Ablation study shows Slow Tuning maintains early-stage safety and Low-Entropy Masking prolongs safe training epochs.

Conclusion: SLowED retains the safety of SLMs and comparably improves their reasoning capability compared to existing distillation methods.

Abstract: Previous chain-of-thought (CoT) distillation methods primarily focused on
enhancing the reasoning capabilities of Small Language Models (SLMs) by
utilizing high-quality rationales generated by powerful Large Language Models
(LLMs, e.g., GPT-4). However, few works have noted the negative effects on SLM
safety brought by the training, which are revealed in this study. Although
there are works on safety alignment that fine-tune language models or
manipulate model weights to defend against harmful inputs, they require extra
computation or annotated data, and probably impact the reasoning ability of
SLMs. In this paper, we investigate how to maintain the safety of SLMs during
the CoT distillation process. Specifically, we propose a safe distillation
method, Slow Tuning and Low-Entropy Masking Distillation (SLowED), containing
two modules: Slow Tuning and Low-Entropy Masking. Slow Tuning scales down the
magnitude of model weight changes to optimize the model weights in the
neighboring space near the initial weight distribution. Low-Entropy Masking
masks low-entropy tokens, which are regarded as unnecessary learning targets,
to exclude them from fine-tuning. Experiments on three SLMs (Qwen2.5-1.5B,
Llama-3.2-1B, BLOOM-1.1B) across reasoning benchmarks (BBH, BB-Sub, ARC,
AGIEval) and safety evaluation (AdvBench) show that SLowED retains the safety
of SLMs and comparably improves their reasoning capability compared to existing
distillation methods. Furthermore, our ablation study presents the
effectiveness of Slow Tuning and Low-Entropy Masking, with the former
maintaining the model's safety in the early stage and the latter prolonging the
safe training epochs.

</details>


### [23] [Evaluating the Role of Large Language Models in Legal Practice in India](https://arxiv.org/abs/2508.09713)
*Rahul Hemrajani*

Main category: cs.CL

TL;DR: LLMs are good at drafting and issue spotting but struggle with legal research and require human oversight for reasoning and accuracy.


<details>
  <summary>Details</summary>
Motivation: The integration of AI into the legal profession raises significant questions about the capacity of LLMs to perform key legal tasks.

Method: I empirically evaluate how well LLMs perform key legal tasks in the Indian context, including issue spotting, legal drafting, advice, research, and reasoning. Through a survey experiment, I compare outputs from LLMs with those of a junior lawyer, with advanced law students rating the work on helpfulness, accuracy, and comprehensiveness.

Result: LLMs excel in drafting and issue spotting, often matching or surpassing human work. However, they struggle with specialised legal research, frequently generating hallucinations, factually incorrect or fabricated outputs.

Conclusion: LLMs can augment certain legal tasks, but human expertise remains essential for nuanced reasoning and the precise application of law.

Abstract: The integration of Artificial Intelligence(AI) into the legal profession
raises significant questions about the capacity of Large Language Models(LLM)
to perform key legal tasks. In this paper, I empirically evaluate how well
LLMs, such as GPT, Claude, and Llama, perform key legal tasks in the Indian
context, including issue spotting, legal drafting, advice, research, and
reasoning. Through a survey experiment, I compare outputs from LLMs with those
of a junior lawyer, with advanced law students rating the work on helpfulness,
accuracy, and comprehensiveness. LLMs excel in drafting and issue spotting,
often matching or surpassing human work. However, they struggle with
specialised legal research, frequently generating hallucinations, factually
incorrect or fabricated outputs. I conclude that while LLMs can augment certain
legal tasks, human expertise remains essential for nuanced reasoning and the
precise application of law.

</details>


### [24] [The Perils of Chart Deception: How Misleading Visualizations Affect Vision-Language Models](https://arxiv.org/abs/2508.09716)
*Ridwan Mahbub,Mohammed Saidul Islam,Md Tahmid Rahman Laskar,Mizanur Rahman,Mir Tafseer Nayeem,Enamul Hoque*

Main category: cs.CL

TL;DR: 视觉语言模型容易受到误导性视觉设计的影响，需要强大的保障措施来防止视觉错误信息。


<details>
  <summary>Details</summary>
Motivation: 当可视化包含欺骗性设计元素时，它们会误导观看者并扭曲理解，从而传播错误信息。视觉语言模型(VLMs)越来越多地被用于解释可视化，尤其是在非专业用户中，因此了解这些模型对欺骗性视觉设计的敏感程度至关重要。

Method: 通过分析来自十个不同模型的超过16,000个响应，这些模型分析了八种不同类型的误导性图表设计。

Result: 证明大多数视觉语言模型会被误导性可视化欺骗。这导致对图表的错误解读，尽管底层数据保持不变。

Conclusion: 大多数视觉语言模型(VLMs)会被误导性可视化欺骗，导致对图表的错误解读。

Abstract: Information visualizations are powerful tools that help users quickly
identify patterns, trends, and outliers, facilitating informed decision-making.
However, when visualizations incorporate deceptive design elements-such as
truncated or inverted axes, unjustified 3D effects, or violations of best
practices-they can mislead viewers and distort understanding, spreading
misinformation. While some deceptive tactics are obvious, others subtly
manipulate perception while maintaining a facade of legitimacy. As
Vision-Language Models (VLMs) are increasingly used to interpret
visualizations, especially by non-expert users, it is critical to understand
how susceptible these models are to deceptive visual designs. In this study, we
conduct an in-depth evaluation of VLMs' ability to interpret misleading
visualizations. By analyzing over 16,000 responses from ten different models
across eight distinct types of misleading chart designs, we demonstrate that
most VLMs are deceived by them. This leads to altered interpretations of
charts, despite the underlying data remaining the same. Our findings highlight
the need for robust safeguards in VLMs against visual misinformation.

</details>


### [25] [Sample More to Think Less: Group Filtered Policy Optimization for Concise Reasoning](https://arxiv.org/abs/2508.09726)
*Vaishnavi Shrivastava,Ahmed Awadallah,Vidhisha Balachandran,Shivam Garg,Harkirat Behl,Dimitris Papailiopoulos*

Main category: cs.CL

TL;DR: GFPO curbs length explosion in large language models by sampling larger groups per problem during training and filtering responses to train on based on response length and token efficiency.


<details>
  <summary>Details</summary>
Motivation: Large language models trained with reinforcement learning with verifiable rewards tend to trade accuracy for length--inflating response lengths to achieve gains in accuracy.

Method: Group Filtered Policy Optimization (GFPO), Adaptive Difficulty GFPO

Result: On the Phi-4-reasoning model, GFPO cuts GRPO's length inflation by 46-71% across challenging STEM and coding benchmarks while maintaining accuracy. Optimizing for reward per token further increases reductions in length inflation to 71-85%.

Conclusion: GFPO demonstrates that increased training-time compute directly translates to reduced test-time compute--a simple yet effective trade-off for efficient reasoning.

Abstract: Large language models trained with reinforcement learning with verifiable
rewards tend to trade accuracy for length--inflating response lengths to
achieve gains in accuracy. While longer answers may be warranted for harder
problems, many tokens are merely "filler": repetitive, verbose text that makes
no real progress. We introduce GFPO (Group Filtered Policy Optimization), which
curbs this length explosion by sampling larger groups per problem during
training and filtering responses to train on based on two key metrics: (1)
response length and (2) token efficiency: reward per token ratio. By sampling
more at training time, we teach models to think less at inference time. On the
Phi-4-reasoning model, GFPO cuts GRPO's length inflation by 46-71% across
challenging STEM and coding benchmarks (AIME 24/25, GPQA, Omni-MATH,
LiveCodeBench) while maintaining accuracy. Optimizing for reward per token
further increases reductions in length inflation to 71-85%. We also propose
Adaptive Difficulty GFPO, which dynamically allocates more training resources
to harder problems based on real-time difficulty estimates, improving the
balance between computational efficiency and accuracy especially on difficult
questions. GFPO demonstrates that increased training-time compute directly
translates to reduced test-time compute--a simple yet effective trade-off for
efficient reasoning.

</details>


### [26] [Transforming Questions and Documents for Semantically Aligned Retrieval-Augmented Generation](https://arxiv.org/abs/2508.09755)
*Seokgi Lee*

Main category: cs.CL

TL;DR: This paper introduces a RAG framework using LLM-based query decomposition and answerable-question embeddings for improved multihop question answering.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the ambiguity inherent in multihop queries by decomposing them into distinct knowledge facets.

Method: The paper introduces a RAG framework that decomposes multihop questions into single-hop subquestions using an LLM to guide document retrieval. It generates answerable questions from document chunks using Qwen3-8B, embeds these questions, and retrieves relevant chunks via question-question embedding similarity.

Result: The paper evaluates on three multihop question datasets (MuSiQue, 2WikiMultiHopQa, HotpotQA) from LongBench and shows improved RAG performance compared to baseline systems.

Conclusion: The paper demonstrates the benefits of using answerable-question embeddings for RAG and the effectiveness of LLM-based query decomposition for multihop scenarios, improving RAG performance compared to baseline systems.

Abstract: We introduce a novel retrieval-augmented generation (RAG) framework tailored
for multihop question answering. First, our system uses large language model
(LLM) to decompose complex multihop questions into a sequence of single-hop
subquestions that guide document retrieval. This decomposition mitigates the
ambiguity inherent in multi-hop queries by clearly targeting distinct knowledge
facets. Second, instead of embedding raw or chunked documents directly, we
generate answerable questions from each document chunk using Qwen3-8B, embed
these generated questions, and retrieve relevant chunks via question-question
embedding similarity. During inference, the retrieved chunks are then fed along
with the original question into the RAG pipeline. We evaluate on three multihop
question datasets (MuSiQue, 2WikiMultiHopQa, HotpotQA) from LongBench. Our
method improves RAG performacne compared to baseline systems. Our contributions
highlight the benefits of using answerable-question embeddings for RAG, and the
effectiveness of LLM-based query decomposition for multihop scenarios.

</details>


### [27] [Echoes of Agreement: Argument Driven Opinion Shifts in Large Language Models](https://arxiv.org/abs/2508.09759)
*Avneet Kaur*

Main category: cs.CL

TL;DR: LLM会根据提示中的论点调整政治立场，这影响了偏见评估。


<details>
  <summary>Details</summary>
Motivation: 评估LLM对政治话题的偏见已有大量研究。然而，模型输出中对这些话题的立场对提示非常敏感。当提示本身暗示对这些立场的某些论点时会发生什么仍然未被探索。这对于理解这些偏见评估的稳健性以及理解模型行为至关重要，因为这些模型经常与带有倾向性的文本交互。

Method: 我们进行了实验，以评估在存在支持和反驳论点的情况下，LLM的政治偏见。

Result: 我们的实验表明，这些论点在单轮和多轮设置中，都大大改变了模型对所提供论点方向的反应。此外，我们发现这些论点的强度会影响模型响应的方向一致性。

Conclusion: LLMs表现出一种奉承倾向，会调整立场以与呈现的论点保持一致，这对衡量政治偏见和制定有效的缓解策略具有下游影响。

Abstract: There have been numerous studies evaluating bias of LLMs towards political
topics. However, how positions towards these topics in model outputs are highly
sensitive to the prompt. What happens when the prompt itself is suggestive of
certain arguments towards those positions remains underexplored. This is
crucial for understanding how robust these bias evaluations are and for
understanding model behaviour, as these models frequently interact with
opinionated text. To that end, we conduct experiments for political bias
evaluation in presence of supporting and refuting arguments. Our experiments
show that such arguments substantially alter model responses towards the
direction of the provided argument in both single-turn and multi-turn settings.
Moreover, we find that the strength of these arguments influences the
directional agreement rate of model responses. These effects point to a
sycophantic tendency in LLMs adapting their stance to align with the presented
arguments which has downstream implications for measuring political bias and
developing effective mitigation strategies.

</details>


### [28] [UtterTune: LoRA-Based Target-Language Pronunciation Edit and Control in Multilingual Text-to-Speech](https://arxiv.org/abs/2508.09767)
*Shuhei Kato*

Main category: cs.CL

TL;DR: UtterTune, a lightweight adaptation method that fine-tunes a multilingual text-to-speech (TTS) system based on a large language model (LLM) architecture, designed to enhance the controllability of pronunciation in a target language while preserving performance in others.


<details>
  <summary>Details</summary>
Motivation: While LLM architectures have enabled TTS models to achieve remarkable naturalness, accurately modeling grapheme-to-phoneme (G2P) mapping and prosody remains challenging

Method: UtterTune leverages low-rank adaptation to enable the control of segmental pronunciation and pitch accent at the phoneme level for Japanese speech

Result: enhance the controllability of pronunciation in a target language while preserving performance in others. maintaining naturalness and speaker similarity in a zero-shot setting

Conclusion: Objective and subjective evaluations confirm its effectiveness.

Abstract: We propose UtterTune, a lightweight adaptation method that fine-tunes a
multilingual text-to-speech (TTS) system based on a large language model (LLM)
architecture, designed to enhance the controllability of pronunciation in a
target language while preserving performance in others. While LLM architectures
have enabled TTS models to achieve remarkable naturalness, accurately modeling
grapheme-to-phoneme (G2P) mapping and prosody remains challenging, especially
when the model omits an explicit G2P module and directly processes minimally
encoded text (e.g., byte-pair encoding). UtterTune leverages low-rank
adaptation to enable the control of segmental pronunciation and pitch accent at
the phoneme level for Japanese speech, the target language in this paper, while
maintaining naturalness and speaker similarity in a zero-shot setting.
Objective and subjective evaluations confirm its effectiveness.

</details>


### [29] [Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study](https://arxiv.org/abs/2508.09776)
*Mahdi Dhaini,Juraj Vladika,Ege Erdogan,Zineb Attaoui,Gjergji Kasneci*

Main category: cs.CL

TL;DR: This paper introduces an automated framework using LLMs to generate textual explanations for NLP models, achieving competitive performance compared to human-annotated explanations, thus providing a scalable solution.


<details>
  <summary>Details</summary>
Motivation: Traditional approaches rely on human annotation, which is costly, labor-intensive, and impedes scalability.

Method: An automated framework that leverages multiple state-of-the-art large language models (LLMs) to generate high-quality textual explanations.

Result: LLM-generated explanations using a comprehensive suite of Natural Language Generation (NLG) metrics exhibit highly competitive effectiveness compared to human-annotated explanations in improving model performance.

Conclusion: Automated explanations exhibit highly competitive effectiveness compared to human-annotated explanations in improving model performance, offering a promising avenue for scalable, automated LLM-based textual explanation generation for extending NLP datasets and enhancing model performance.

Abstract: In the rapidly evolving field of Explainable Natural Language Processing
(NLP), textual explanations, i.e., human-like rationales, are pivotal for
explaining model predictions and enriching datasets with interpretable labels.
Traditional approaches rely on human annotation, which is costly,
labor-intensive, and impedes scalability. In this work, we present an automated
framework that leverages multiple state-of-the-art large language models (LLMs)
to generate high-quality textual explanations. We rigorously assess the quality
of these LLM-generated explanations using a comprehensive suite of Natural
Language Generation (NLG) metrics. Furthermore, we investigate the downstream
impact of these explanations on the performance of pre-trained language models
(PLMs) and LLMs across natural language inference tasks on two diverse
benchmark datasets. Our experiments demonstrate that automated explanations
exhibit highly competitive effectiveness compared to human-annotated
explanations in improving model performance. Our findings underscore a
promising avenue for scalable, automated LLM-based textual explanation
generation for extending NLP datasets and enhancing model performance.

</details>


### [30] [Adoption of Explainable Natural Language Processing: Perspectives from Industry and Academia on Practices and Challenges](https://arxiv.org/abs/2508.09786)
*Mahdi Dhaini,Tobias Müller,Roksoliana Rabets,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 本文通过访谈研究，调查了从业者在使用可解释性方法时的经验，发现他们对当前方法满意度低，并强调了评估方面的挑战，呼吁更清晰的定义和以用户为中心的框架。


<details>
  <summary>Details</summary>
Motivation: 复杂模型的日益不透明性需要对其决策的透明性和解释，这对于理解其推理和促进部署至关重要，尤其是在高风险环境中。尽管越来越多的注意力集中在可解释的NLP上，但从业者对其实际采用和有效性的看法仍未得到充分探索。

Method: 通过对行业从业者的定性访谈研究以及对学术研究人员的补充访谈，系统地分析和比较他们的观点。

Result: 揭示了概念差距，对当前的可解释性方法满意度较低，并强调了评估挑战。

Conclusion: 当前的可解释性方法满意度低，并且面临评估挑战。强调需要清晰的定义和以用户为中心的框架，以便在实践中更好地采用可解释的NLP。

Abstract: The field of explainable natural language processing (NLP) has grown rapidly
in recent years. The growing opacity of complex models calls for transparency
and explanations of their decisions, which is crucial to understand their
reasoning and facilitate deployment, especially in high-stakes environments.
Despite increasing attention given to explainable NLP, practitioners'
perspectives regarding its practical adoption and effectiveness remain
underexplored. This paper addresses this research gap by investigating
practitioners' experiences with explainability methods, specifically focusing
on their motivations for adopting such methods, the techniques employed,
satisfaction levels, and the practical challenges encountered in real-world NLP
applications. Through a qualitative interview-based study with industry
practitioners and complementary interviews with academic researchers, we
systematically analyze and compare their perspectives. Our findings reveal
conceptual gaps, low satisfaction with current explainability methods, and
highlight evaluation challenges. Our findings emphasize the need for clear
definitions and user-centric frameworks for better adoption of explainable NLP
in practice.

</details>


### [31] [BigCharts-R1: Enhanced Chart Reasoning with Visual Reinforcement Finetuning](https://arxiv.org/abs/2508.09804)
*Ahmed Masry,Abhay Puri,Masoud Hashemi,Juan A. Rodriguez,Megh Thakkar,Khyati Mahajan,Vikas Yadav,Sathwik Tejaswi Madhusudhan,Alexandre Piché,Dzmitry Bahdanau,Christopher Pal,David Vazquez,Enamul Hoque,Perouz Taslakian,Sai Rajeswar,Spandana Gella*

Main category: cs.CL

TL;DR: This paper introduces BigCharts, a dataset and training framework to improve chart comprehension in vision-language models. The model outperforms existing methods on chart question-answering benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models (VLMs) struggle with chart comprehension due to training on datasets that lack diversity and real-world authenticity, or on automatically extracted underlying data tables of charts, which can contain numerous estimation errors. Existing models only rely on supervised fine-tuning using these low-quality datasets, severely limiting their effectiveness.

Method: A dataset creation pipeline called BigCharts and a training framework that integrates supervised fine-tuning with Group Relative Policy Optimization (GRPO)-based reinforcement learning.

Result: The approach enhances model robustness and generalization across diverse chart styles and domains, resulting in a state-of-the-art chart reasoning model, BigCharts-R1.

Conclusion: The proposed BigCharts-R1 model surpasses existing methods on multiple chart question-answering benchmarks.

Abstract: Charts are essential to data analysis, transforming raw data into clear
visual representations that support human decision-making. Although current
vision-language models (VLMs) have made significant progress, they continue to
struggle with chart comprehension due to training on datasets that lack
diversity and real-world authenticity, or on automatically extracted underlying
data tables of charts, which can contain numerous estimation errors.
Furthermore, existing models only rely on supervised fine-tuning using these
low-quality datasets, severely limiting their effectiveness. To address these
issues, we first propose BigCharts, a dataset creation pipeline that generates
visually diverse chart images by conditioning the rendering process on
real-world charts sourced from multiple online platforms. Unlike purely
synthetic datasets, BigCharts incorporates real-world data, ensuring
authenticity and visual diversity, while still retaining accurate underlying
data due to our proposed replotting process. Additionally, we introduce a
comprehensive training framework that integrates supervised fine-tuning with
Group Relative Policy Optimization (GRPO)-based reinforcement learning. By
introducing novel reward signals specifically designed for chart reasoning, our
approach enhances model robustness and generalization across diverse chart
styles and domains, resulting in a state-of-the-art chart reasoning model,
BigCharts-R1. Extensive experiments demonstrate that our models surpass
existing methods on multiple chart question-answering benchmarks compared to
even larger open-source and closed-source models.

</details>


### [32] [A Comprehensive Survey of Datasets for Clinical Mental Health AI Systems](https://arxiv.org/abs/2508.09809)
*Aishik Mandal,Prottay Kumar Adhikary,Hiba Arnaout,Iryna Gurevych,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 本文全面调研了心理健康临床数据集，发现了现有数据集的不足，并为未来数据集的构建提出了建议，以促进心理健康 AI 系统的发展。


<details>
  <summary>Details</summary>
Motivation: 全球心理健康障碍正在上升，但训练有素的临床医生的数量并没有成比例地增加，导致许多人无法获得充分或及时的支持。为了弥合这一差距，最近的研究表明，人工智能（AI）在辅助心理健康诊断、监测和干预方面具有前景。然而，开发高效、可靠和符合伦理道德的 AI 来协助临床医生在很大程度上取决于高质量的临床训练数据集。尽管人们对用于训练临床 AI 助手的数据管理越来越感兴趣，但现有数据集在很大程度上仍然分散、缺乏文档，而且通常无法访问，这阻碍了为临床心理健康护理开发的 AI 模型的可重复性、可比性和通用性。

Method: 本文对与 AI 临床助手训练和开发相关的临床心理健康数据集进行了首次全面调查，并按精神障碍、数据模式、任务类型、可访问性和社会文化背景对这些数据集进行了分类。

Result: 我们的调查发现了关键差距，例如缺乏纵向数据、文化和语言代表性有限、收集和注释标准不一致以及合成数据中缺乏模式。

Conclusion: 本文总结了心理健康临床数据集中存在的关键差距，并为未来数据集的组织和标准化提出了关键挑战，并为开发更强大、更通用和更公平的心理健康 AI 系统提供了可操作的建议。

Abstract: Mental health disorders are rising worldwide. However, the availability of
trained clinicians has not scaled proportionally, leaving many people without
adequate or timely support. To bridge this gap, recent studies have shown the
promise of Artificial Intelligence (AI) to assist mental health diagnosis,
monitoring, and intervention. However, the development of efficient, reliable,
and ethical AI to assist clinicians is heavily dependent on high-quality
clinical training datasets. Despite growing interest in data curation for
training clinical AI assistants, existing datasets largely remain scattered,
under-documented, and often inaccessible, hindering the reproducibility,
comparability, and generalizability of AI models developed for clinical mental
health care. In this paper, we present the first comprehensive survey of
clinical mental health datasets relevant to the training and development of
AI-powered clinical assistants. We categorize these datasets by mental
disorders (e.g., depression, schizophrenia), data modalities (e.g., text,
speech, physiological signals), task types (e.g., diagnosis prediction, symptom
severity estimation, intervention generation), accessibility (public,
restricted or private), and sociocultural context (e.g., language and cultural
background). Along with these, we also investigate synthetic clinical mental
health datasets. Our survey identifies critical gaps such as a lack of
longitudinal data, limited cultural and linguistic representation, inconsistent
collection and annotation standards, and a lack of modalities in synthetic
data. We conclude by outlining key challenges in curating and standardizing
future datasets and provide actionable recommendations to facilitate the
development of more robust, generalizable, and equitable mental health AI
systems.

</details>


### [33] [Speed Always Wins: A Survey on Efficient Architectures for Large Language Models](https://arxiv.org/abs/2508.09834)
*Weigao Sun,Jiaxi Hu,Yucheng Zhou,Jusen Du,Disen Lan,Kexin Wang,Tong Zhu,Xiaoye Qu,Yu Zhang,Xiaoyu Mo,Daizong Liu,Yuxuan Liang,Wenliang Chen,Guoqi Li,Yu Cheng*

Main category: cs.CL

TL;DR: 本文概述了现代高效LLM架构，旨在激发未来对更高效、通用的AI系统的研究。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer架构计算量大，对大规模训练和实际部署构成重大障碍。

Method: 对线性、稀疏序列建模方法、高效全注意力变体、稀疏混合专家、混合模型架构以及新兴扩散LLM的技术细节进行了调查。

Result: 本文对Transformer的固有局限性进行了系统性分析，并促进了效率的提高。

Conclusion: 本文总结了提高LLM效率的创新架构，为未来研究更高效、通用的AI系统提供参考。

Abstract: Large Language Models (LLMs) have delivered impressive results in language
understanding, generation, reasoning, and pushes the ability boundary of
multimodal models. Transformer models, as the foundation of modern LLMs, offer
a strong baseline with excellent scaling properties. However, the traditional
transformer architecture requires substantial computations and poses
significant obstacles for large-scale training and practical deployment. In
this survey, we offer a systematic examination of innovative LLM architectures
that address the inherent limitations of transformers and boost the efficiency.
Starting from language modeling, this survey covers the background and
technical details of linear and sparse sequence modeling methods, efficient
full attention variants, sparse mixture-of-experts, hybrid model architectures
incorporating the above techniques, and emerging diffusion LLMs. Additionally,
we discuss applications of these techniques to other modalities and consider
their wider implications for developing scalable, resource-aware foundation
models. By grouping recent studies into the above category, this survey
presents a blueprint of modern efficient LLM architectures, and we hope this
could help motivate future research toward more efficient, versatile AI
systems.

</details>


### [34] [PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts](https://arxiv.org/abs/2508.09848)
*Mo Yu,Tsz Ting Chung,Chulun Zhou,Tong Li,Rui Lu,Jiangnan Li,Liyan Xu,Haoshu Lu,Ning Zhang,Jing Li,Jie Zhou*

Main category: cs.CL

TL;DR: PRELUDE是一个新的长文本理解基准，表明现有模型在深层推理方面与人类相比仍有显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试对全局理解和深层推理的需求不高，而评估前传故事的合理性需要搜索和整合间接相关的信息。

Method: 提出了一个新的基准测试PRELUDE，用于评估长文本理解能力，通过判断前传故事与原作的叙事一致性。

Result: 实验结果表明，目前最好的大语言模型，包括上下文学习、RAG和领域内训练的模型，以及商业的DeepResearch服务，在PRELUDE测试中落后于人类超过15%。模型在推理准确性方面与人类相比差距超过30%。

Conclusion: 现有的大语言模型在长文本理解和推理方面仍有很大的改进空间，尤其是在需要整合间接相关信息和进行深层推理的任务中。

Abstract: We introduce PRELUDE, a benchmark for evaluating long-context understanding
through the task of determining whether a character's prequel story is
consistent with the canonical narrative of the original book. Our task poses a
stronger demand for global comprehension and deep reasoning than existing
benchmarks -- as the prequels are not part of the original story, assessing
their plausibility typically requires searching and integrating information
that is only indirectly related. Empirically, 88% of instances require evidence
from multiple parts of the narrative. Experimental results highlight the
challenge of our task: in-context learning, RAG and in-domain training with
state-of-the-art LLMs, and commercial DeepResearch services, lag behind humans
by >15%. A further human study reveals that models often produce correct
answers with flawed reasoning, leading to an over 30% gap in reasoning accuracy
compared to humans. These findings underscore the substantial room for
improvement in long-context understanding and reasoning.

</details>


### [35] [Assessing the Feasibility of Lightweight Whisper Models for Low-Resource Urdu Transcription](https://arxiv.org/abs/2508.09865)
*Abdul Rehman Antall,Naveed Akhtar*

Main category: cs.CL

TL;DR: 本研究评估了轻量级 Whisper 模型在低资源环境下的乌尔都语语音识别可行性。


<details>
  <summary>Details</summary>
Motivation: 乌尔都语是全球第 10 大语言，拥有超过 2.3 亿使用者，但由于方言多样性、语码转换和稀疏的训练数据，其在自动语音识别 (ASR) 系统中的代表性仍然有限。

Method: 在整理的乌尔都语数据集上，使用词错误率 (WER) 对轻量级 Whisper 模型（Tiny、Base、Small）进行基准测试，无需微调。

Result: Whisper-Small 实现了最低的错误率 (33.68% WER)，优于 Tiny (67.08% WER) 和 Base (53.67% WER)。定性分析表明，在语音准确性和词汇连贯性方面仍然存在挑战，尤其是在复杂的语句中。

Conclusion: Whisper-Small 模型在乌尔都语 ASR 方面表现出潜力，但仍存在显著差距，为未来研究奠定了基础。

Abstract: This study evaluates the feasibility of lightweight Whisper models (Tiny,
Base, Small) for Urdu speech recognition in low-resource settings. Despite Urdu
being the 10th most spoken language globally with over 230 million speakers,
its representation in automatic speech recognition (ASR) systems remains
limited due to dialectal diversity, code-switching, and sparse training data.
We benchmark these models on a curated Urdu dataset using word error rate
(WER), without fine-tuning. Results show Whisper-Small achieves the lowest
error rates (33.68\% WER), outperforming Tiny (67.08\% WER) and Base (53.67\%
WER). Qualitative analysis reveals persistent challenges in phonetic accuracy
and lexical coherence, particularly for complex utterances. While Whisper-Small
demonstrates promise for deployable Urdu ASR, significant gaps remain. Our
findings emphasize lay the groundwork for future research into effective,
low-resource ASR systems.

</details>


### [36] [Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models](https://arxiv.org/abs/2508.09874)
*Jiaqi Cao,Jiarui Wang,Rubin Wei,Qipeng Guo,Kai Chen,Bowen Zhou,Zhouhan Lin*

Main category: cs.CL

TL;DR: Memory Decoder is introduced for efficient domain adaptation of LLMs, using a plug-and-play pretrained memory component.


<details>
  <summary>Details</summary>
Motivation: Adapting Large Language Models (LLMs) to specific domains remains a challenge. Current method like Domain Adaptive Pretraining (DAPT) requires costly full-parameter training and suffers from catastrophic forgetting. Meanwhile, Retrieval-Augmented Generation (RAG) introduces substantial inference latency due to expensive nearest-neighbor searches and longer context.

Method: Memory Decoder employs a small transformer decoder that learns to imitate the behavior of an external non-parametric retriever.

Result: Memory Decoder enables effective adaptation of various Qwen and Llama models to three distinct specialized domains: biomedicine, finance, and law, reducing perplexity by an average of 6.17 points.

Conclusion: Memory Decoder introduces a novel paradigm centered on a specially pretrained memory component designed for domain-specific adaptation. This memory architecture can be integrated in a plug-and-play manner, consistently enhancing performance across multiple models within the target domain.

Abstract: Large Language Models (LLMs) have shown strong abilities in general language
tasks, yet adapting them to specific domains remains a challenge. Current
method like Domain Adaptive Pretraining (DAPT) requires costly full-parameter
training and suffers from catastrophic forgetting. Meanwhile,
Retrieval-Augmented Generation (RAG) introduces substantial inference latency
due to expensive nearest-neighbor searches and longer context. This paper
introduces Memory Decoder, a plug-and-play pretrained memory that enables
efficient domain adaptation without changing the original model's parameters.
Memory Decoder employs a small transformer decoder that learns to imitate the
behavior of an external non-parametric retriever. Once trained, Memory Decoder
can be seamlessly integrated with any pretrained language model that shares the
same tokenizer, requiring no model-specific modifications. Experimental results
demonstrate that Memory Decoder enables effective adaptation of various Qwen
and Llama models to three distinct specialized domains: biomedicine, finance,
and law, reducing perplexity by an average of 6.17 points. Overall, Memory
Decoder introduces a novel paradigm centered on a specially pretrained memory
component designed for domain-specific adaptation. This memory architecture can
be integrated in a plug-and-play manner, consistently enhancing performance
across multiple models within the target domain.

</details>


### [37] [A Survey of Cognitive Distortion Detection and Classification in NLP](https://arxiv.org/abs/2508.09878)
*Archie Sage,Jeroen Keppens,Helen Yannakoudakis*

Main category: cs.CL

TL;DR: This paper surveys the field of automatic cognitive distortion detection, providing a structured overview of datasets, models, and evaluation, and highlights open challenges.


<details>
  <summary>Details</summary>
Motivation: As interest grows in the application of natural language processing (NLP) techniques to mental health, a growing body of work explores the automatic detection and classification of cognitive distortions (CDs). CDs are habitual patterns of negatively biased or flawed thinking that distort how people perceive events, judge themselves, and react to the world around them. Identifying and addressing them is an important part of therapy. Despite its momentum, the field remains fragmented, with inconsistencies in CD taxonomies, task formulations, and evaluation practices.

Method: a structured overview of datasets, modelling approaches, and evaluation strategies

Result: a consolidated CD taxonomy reference, summarise common task setups, and highlight open challenges

Conclusion: This survey reviews 38 studies spanning two decades, providing a structured overview of datasets, modelling approaches, and evaluation strategies. We provide a consolidated CD taxonomy reference, summarise common task setups, and highlight open challenges to support more coherent and reproducible research in this emerging area.

Abstract: As interest grows in the application of natural language processing (NLP)
techniques to mental health, a growing body of work explores the automatic
detection and classification of cognitive distortions (CDs). CDs are habitual
patterns of negatively biased or flawed thinking that distort how people
perceive events, judge themselves, and react to the world around them.
Identifying and addressing them is an important part of therapy. Despite its
momentum, the field remains fragmented, with inconsistencies in CD taxonomies,
task formulations, and evaluation practices. This survey reviews 38 studies
spanning two decades, providing a structured overview of datasets, modelling
approaches, and evaluation strategies. We provide a consolidated CD taxonomy
reference, summarise common task setups, and highlight open challenges to
support more coherent and reproducible research in this emerging area.

</details>


### [38] [Language of Persuasion and Misrepresentation in Business Communication: A Textual Detection Approach](https://arxiv.org/abs/2508.09935)
*Sayem Hossen,Monalisa Moon Joti,Md. Golam Rashed*

Main category: cs.CL

TL;DR: 本文研究了如何使用说服性词汇来系统地检测欺骗性语言，并通过计算文本分析实现了高检测精度。


<details>
  <summary>Details</summary>
Motivation: 商业沟通数字化已经重组了说服性论述的过程，这不仅允许更大的透明度，而且还允许高级欺骗。

Method: 将经典修辞学和传播心理学与语言理论以及金融报告、可持续性论述和数字营销中的实证研究相结合。

Result: 在受控环境中，通过使用计算文本分析以及个性化的transformer模型，实现了大于99%的检测精度。然而，在多语言环境中重现这种性能也存在问题，并且在很大程度上，这是因为不容易找到足够的数据，而且很少有多语言文本处理基础设施。

Conclusion: 证据表明，沟通的理论表示与经验近似之间存在差距，因此，需要强大的自动文本识别系统，因为基于人工智能的论述在与人类沟通方面正变得越来越现实。

Abstract: Business communication digitisation has reorganised the process of persuasive
discourse, which
  allows not only greater transparency but also advanced deception. This
inquiry synthesises classical
  rhetoric and communication psychology with linguistic theory and empirical
studies in the financial
  reporting, sustainability discourse, and digital marketing to explain how
deceptive language can be
  systematically detected using persuasive lexicon. In controlled settings,
detection accuracies of greater
  than 99% were achieved by using computational textual analysis as well as
personalised transformer
  models. However, reproducing this performance in multilingual settings is
also problematic and,
  to a large extent, this is because it is not easy to find sufficient data,
and because few multilingual
  text-processing infrastructures are in place. This evidence shows that there
has been an increasing
  gap between the theoretical representations of communication and those
empirically approximated,
  and therefore, there is a need to have strong automatic text-identification
systems where AI-based
  discourse is becoming more realistic in communicating with humans.

</details>


### [39] [A Comprehensive Evaluation framework of Alignment Techniques for LLMs](https://arxiv.org/abs/2508.09937)
*Muneeza Azmat,Momin Abbas,Maysa Malfiza Garcia de Macedo,Marcelo Carpinette Grave,Luan Soares de Souza,Tiago Machado,Rogerio A de Paula,Raya Horesh,Yixin Chen,Heloisa Caroline de Souza Pereira Candello,Rebecka Nordenlow,Aminat Adebiyi*

Main category: cs.CL

TL;DR: 本文提出了一个多维评估框架，用于系统地比较LLM的对齐技术。


<details>
  <summary>Details</summary>
Motivation: 确保大型语言模型（LLM）的输出与人类价值观和安全标准相一致变得至关重要。然而，缺乏统一的评估框架使得系统地比较这些范例和指导部署决策变得困难。

Method: 本文通过实验跨越不同的基础模型和对齐策略，论证了我们框架的实用性。

Result: 本文构建的框架可以评估对齐检测、对齐质量、计算效率和鲁棒性这四个关键维度的方法。

Conclusion: 本文提出了一个全面的评估框架，用于在四个关键维度上系统地比较LLM对齐技术的优势和局限性，从而为未来的研究方向提供有价值的见解。

Abstract: As Large Language Models (LLMs) become increasingly integrated into
real-world applications, ensuring their outputs align with human values and
safety standards has become critical. The field has developed diverse alignment
approaches including traditional fine-tuning methods (RLHF, instruction
tuning), post-hoc correction systems, and inference-time interventions, each
with distinct advantages and limitations. However, the lack of unified
evaluation frameworks makes it difficult to systematically compare these
paradigms and guide deployment decisions. This paper introduces a
multi-dimensional evaluation of alignment techniques for LLMs, a comprehensive
evaluation framework that provides a systematic comparison across all major
alignment paradigms. Our framework assesses methods along four key dimensions:
alignment detection, alignment quality, computational efficiency, and
robustness. Through experiments across diverse base models and alignment
strategies, we demonstrate the utility of our framework in identifying
strengths and limitations of current state-of-the-art models, providing
valuable insights for future research directions.

</details>


### [40] [VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models](https://arxiv.org/abs/2508.09945)
*Lingjie Jiang,Shaohan Huang,Xun Wu,Yixia Li,Dongdong Zhang,Furu Wei*

Main category: cs.CL

TL;DR: VisCodex merges vision and coding language models for multimodal code generation, using task vector-based model merging and new datasets (MCD & InfiBench-V). It achieves state-of-the-art performance among open-source MLLMs.


<details>
  <summary>Details</summary>
Motivation: Multimodal large language models (MLLMs) have significantly advanced the integration of visual and textual understanding. However, their ability to generate code from multimodal inputs remains limited.

Method: Leveraging a task vector-based model merging technique, we integrate a state-of-the-art coding LLM into a strong vision-language backbone

Result: VisCodex, a unified framework that seamlessly merges vision and coding language models to empower MLLMs with strong multimodal code generation abilities. To support training and evaluation, we introduce the Multimodal Coding Dataset (MCD), a large-scale and diverse collection of 598k samples, including high-quality HTML code, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic problems. Furthermore, we propose InfiBench-V, a novel and challenging benchmark specifically designed to assess models on visually-rich, real-world programming questions

Conclusion: VisCodex achieves state-of-the-art performance among open-source MLLMs and approaches proprietary models like GPT-4o, highlighting the effectiveness of our model merging strategy and new datasets.

Abstract: Multimodal large language models (MLLMs) have significantly advanced the
integration of visual and textual understanding. However, their ability to
generate code from multimodal inputs remains limited. In this work, we
introduce VisCodex, a unified framework that seamlessly merges vision and
coding language models to empower MLLMs with strong multimodal code generation
abilities. Leveraging a task vector-based model merging technique, we integrate
a state-of-the-art coding LLM into a strong vision-language backbone, while
preserving both visual comprehension and advanced coding skills. To support
training and evaluation, we introduce the Multimodal Coding Dataset (MCD), a
large-scale and diverse collection of 598k samples, including high-quality HTML
code, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic
problems. Furthermore, we propose InfiBench-V, a novel and challenging
benchmark specifically designed to assess models on visually-rich, real-world
programming questions that demand a nuanced understanding of both textual and
visual contexts. Extensive experiments show that VisCodex achieves
state-of-the-art performance among open-source MLLMs and approaches proprietary
models like GPT-4o, highlighting the effectiveness of our model merging
strategy and new datasets.

</details>


### [41] [Specialised or Generic? Tokenization Choices for Radiology Language Models](https://arxiv.org/abs/2508.09952)
*Hermione Warr,Wentian Xu,Harry Anthony,Yasin Ibrahim,Daniel McGowan,Konstantinos Kamnitsas*

Main category: cs.CL

TL;DR: Medical and domain-specific tokenizers improve radiology report summarisation, especially when training from scratch, and reduce memory requirements.


<details>
  <summary>Details</summary>
Motivation: The vocabulary used by language models (LM) plays a key role in text generation quality. However, its impact remains under-explored in radiology.

Method: systematically comparing general, medical, and domain-specific tokenizers on the task of radiology report summarisation across three imaging modalities. We also investigate scenarios with and without LM pre-training on PubMed abstracts.

Result: medical and domain-specific vocabularies outperformed widely used natural language alternatives when models are trained from scratch. Pre-training partially mitigates performance differences between tokenizers, whilst the domain-specific tokenizers achieve the most favourable results. Domain-specific tokenizers also reduce memory requirements due to smaller vocabularies and shorter sequences.

Conclusion: Adapting the vocabulary of LMs to the clinical domain provides practical benefits, including improved performance and reduced computational demands.

Abstract: The vocabulary used by language models (LM) - defined by the tokenizer -
plays a key role in text generation quality. However, its impact remains
under-explored in radiology. In this work, we address this gap by
systematically comparing general, medical, and domain-specific tokenizers on
the task of radiology report summarisation across three imaging modalities. We
also investigate scenarios with and without LM pre-training on PubMed
abstracts. Our findings demonstrate that medical and domain-specific
vocabularies outperformed widely used natural language alternatives when models
are trained from scratch. Pre-training partially mitigates performance
differences between tokenizers, whilst the domain-specific tokenizers achieve
the most favourable results. Domain-specific tokenizers also reduce memory
requirements due to smaller vocabularies and shorter sequences. These results
demonstrate that adapting the vocabulary of LMs to the clinical domain provides
practical benefits, including improved performance and reduced computational
demands, making such models more accessible and effective for both research and
real-world healthcare settings.

</details>


### [42] [Shaping Event Backstories to Estimate Potential Emotion Contexts](https://arxiv.org/abs/2508.09954)
*Johannes Schäfer,Roman Klinger*

Main category: cs.CL

TL;DR: 本文提出了一种通过添加事件上下文来消除情感分析歧义的新方法，实验结果表明，情境叙事可以提高情感注释的一致性。


<details>
  <summary>Details</summary>
Motivation: 先前的工作研究了注释者的属性来解释分歧，但这忽略了歧义可能源于缺少事件上下文信息的可能性。在本文中，我们提出了一种新颖的方法，该方法将合理的上下文添加到事件描述中，这可以更好地解释特定情况。

Method: 通过结合各种设置中的短篇小说生成技术，我们实现了连贯的叙事，从而形成了一个专门的数据集，用于对情境化情感分析进行首次全面和系统的检查。

Result: 情境叙事增强了对特定情感的解释，并支持注释者生成更一致的注释。

Conclusion: 通过自动和人工评估，我们发现情境叙事增强了对特定情感的解释，并支持注释者生成更一致的注释。

Abstract: Emotion analysis is an inherently ambiguous task. Previous work studied
annotator properties to explain disagreement, but this overlooks the
possibility that ambiguity may stem from missing information about the context
of events. In this paper, we propose a novel approach that adds reasonable
contexts to event descriptions, which may better explain a particular
situation. Our goal is to understand whether these enriched contexts enable
human annotators to annotate emotions more reliably. We disambiguate a target
event description by automatically generating multiple event chains conditioned
on differing emotions. By combining techniques from short story generation in
various settings, we achieve coherent narratives that result in a specialized
dataset for the first comprehensive and systematic examination of
contextualized emotion analysis. Through automatic and human evaluation, we
find that contextual narratives enhance the interpretation of specific emotions
and support annotators in producing more consistent annotations.

</details>


### [43] [Performance of GPT-5 Frontier Models in Ophthalmology Question Answering](https://arxiv.org/abs/2508.09956)
*Fares Antaki,David Mikhail,Daniel Milad,Danny A Mammo,Sumit Sharma,Sunil K Srivastava,Bing Yu Chen,Samir Touma,Mertcan Sevgi,Jonathan El-Khoury,Pearse A Keane,Qingyu Chen,Yih Chung Tham,Renaud Duval*

Main category: cs.CL

TL;DR: 本研究评估了GPT-5在眼科问答任务中的性能，发现GPT-5-high准确率最高，GPT-5-mini-low在成本和性能之间取得最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM），如GPT-5，集成了先进的推理能力，可以提高复杂医学问答任务的性能。对于最新一代的推理模型，最大化准确性和成本效益的配置尚未确定。

Method: 我们评估了OpenAI的GPT-5系列的12种配置（跨四个推理工作设置的三个模型层）以及o1-high、o3-high和GPT-4o，使用了来自美国眼科学会基础临床科学课程（BCSC）数据集的260个封闭式多项选择题。

Result: GPT-5-high实现了最高的准确率（0.965；95% CI，0.942-0.985），优于所有GPT-5-nano变体（P < .001）、o1-high（P = .04）和GPT-4o（P < .001），但未优于o3-high（0.958；95% CI，0.931-0.981）。

Conclusion: GPT-5-high在准确率和论证质量方面均排名第一。成本-准确率分析确定了Pareto前沿的几种GPT-5配置，其中GPT-5-mini-low提供了最有利的低成本、高性能平衡。

Abstract: Large language models (LLMs) such as GPT-5 integrate advanced reasoning
capabilities that may improve performance on complex medical question-answering
tasks. For this latest generation of reasoning models, the configurations that
maximize both accuracy and cost-efficiency have yet to be established. We
evaluated 12 configurations of OpenAI's GPT-5 series (three model tiers across
four reasoning effort settings) alongside o1-high, o3-high, and GPT-4o, using
260 closed-access multiple-choice questions from the American Academy of
Ophthalmology Basic Clinical Science Course (BCSC) dataset. The primary outcome
was multiple-choice accuracy; secondary outcomes included head-to-head ranking
via a Bradley-Terry model, rationale quality assessment using a
reference-anchored, pairwise LLM-as-a-judge framework, and analysis of
accuracy-cost trade-offs using token-based cost estimates. GPT-5-high achieved
the highest accuracy (0.965; 95% CI, 0.942-0.985), outperforming all GPT-5-nano
variants (P < .001), o1-high (P = .04), and GPT-4o (P < .001), but not o3-high
(0.958; 95% CI, 0.931-0.981). GPT-5-high ranked first in both accuracy (1.66x
stronger than o3-high) and rationale quality (1.11x stronger than o3-high).
Cost-accuracy analysis identified several GPT-5 configurations on the Pareto
frontier, with GPT-5-mini-low offering the most favorable low-cost,
high-performance balance. These results benchmark GPT-5 on a high-quality
ophthalmology dataset, demonstrate the influence of reasoning effort on
accuracy, and introduce an autograder framework for scalable evaluation of
LLM-generated answers against reference standards in ophthalmology.

</details>


### [44] [Which one Performs Better? Wav2Vec or Whisper? Applying both in Badini Kurdish Speech to Text (BKSTT)](https://arxiv.org/abs/2508.09957)
*Renas Adnan,Hossein Hassani*

Main category: cs.CL

TL;DR: This paper aims to create a language model based on Badini's speech and evaluate its performance. The experiments indicate that the Wav2Vec2-Large-XLSR-53 model provides a significantly more accurate and readable output than the Whisper-small model.


<details>
  <summary>Details</summary>
Motivation: STT is not applied to other Kurdish dialects, Badini and Hawrami, for example. This research is an attempt to address this gap. Bandin, approximately, has two million speakers, and STT systems can help their community use mobile and computer-based technologies while giving their dialect more global visibility.

Method: We used Wav2Vec2-Large-XLSR-53 and Whisper-small to develop the language models.

Result: The transcriptions process based on the Wav2Vec2-Large-XLSR-53 model provides a significantly more accurate and readable output than the Whisper-small model, with 90.38% and 65.45% readability, and 82.67% and 53.17% accuracy, respectively.

Conclusion: The Wav2Vec2-Large-XLSR-53 model provides a significantly more accurate and readable output than the Whisper-small model, with 90.38% and 65.45% readability, and 82.67% and 53.17% accuracy, respectively.

Abstract: Speech-to-text (STT) systems have a wide range of applications. They are
available in many languages, albeit at different quality levels. Although
Kurdish is considered a less-resourced language from a processing perspective,
SST is available for some of the Kurdish dialects, for instance, Sorani
(Central Kurdish). However, that is not applied to other Kurdish dialects,
Badini and Hawrami, for example. This research is an attempt to address this
gap. Bandin, approximately, has two million speakers, and STT systems can help
their community use mobile and computer-based technologies while giving their
dialect more global visibility. We aim to create a language model based on
Badini's speech and evaluate its performance. To cover a conversational aspect,
have a proper confidence level of grammatical accuracy, and ready
transcriptions, we chose Badini kids' stories, eight books including 78
stories, as the textual input. Six narrators narrated the books, which resulted
in approximately 17 hours of recording. We cleaned, segmented, and tokenized
the input. The preprocessing produced nearly 15 hours of speech, including
19193 segments and 25221 words. We used Wav2Vec2-Large-XLSR-53 and
Whisper-small to develop the language models. The experiments indicate that the
transcriptions process based on the Wav2Vec2-Large-XLSR-53 model provides a
significantly more accurate and readable output than the Whisper-small model,
with 90.38% and 65.45% readability, and 82.67% and 53.17% accuracy,
respectively.

</details>


### [45] [Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks](https://arxiv.org/abs/2508.09958)
*Baran Atalar,Eddie Zhang,Carlee Joe-Wong*

Main category: cs.CL

TL;DR: Proposes a neural contextual bandit-based algorithm for selecting a sequence of LLMs for complex tasks, where the output of each LLM influences the success of subsequent LLMs. Shows it outperforms other LLM selection algorithms on telecommunications question answering and medical diagnosis prediction datasets.


<details>
  <summary>Details</summary>
Motivation: The increasing popularity of large language models (LLMs) for a variety of tasks, there has been a growing interest in strategies that can predict which out of a set of LLMs will yield a successful answer at low cost. Unlike existing LLM selection or routing algorithms, this setting requires that we select a sequence of LLMs, with the output of each LLM feeding into the next and potentially influencing its success. Thus, unlike single LLM selection, the quality of each subtask's output directly affects the inputs, and hence the cost and success rate, of downstream LLMs, creating complex performance dependencies that must be learned and accounted for during selection.

Method: A neural contextual bandit-based algorithm that trains neural networks that model LLM success on each subtask in an online manner

Result: Experiments on telecommunications question answering and medical diagnosis prediction datasets illustrate the effectiveness of our proposed approach compared to other LLM selection algorithms.

Conclusion: A neural contextual bandit-based algorithm is proposed to train neural networks that model LLM success on each subtask in an online manner. Experiments on telecommunications question answering and medical diagnosis prediction datasets illustrate the effectiveness of our proposed approach compared to other LLM selection algorithms.

Abstract: With the increasing popularity of large language models (LLMs) for a variety
of tasks, there has been a growing interest in strategies that can predict
which out of a set of LLMs will yield a successful answer at low cost. This
problem promises to become more and more relevant as providers like Microsoft
allow users to easily create custom LLM "assistants" specialized to particular
types of queries. However, some tasks (i.e., queries) may be too specialized
and difficult for a single LLM to handle alone. These applications often
benefit from breaking down the task into smaller subtasks, each of which can
then be executed by a LLM expected to perform well on that specific subtask.
For example, in extracting a diagnosis from medical records, one can first
select an LLM to summarize the record, select another to validate the summary,
and then select another, possibly different, LLM to extract the diagnosis from
the summarized record. Unlike existing LLM selection or routing algorithms,
this setting requires that we select a sequence of LLMs, with the output of
each LLM feeding into the next and potentially influencing its success. Thus,
unlike single LLM selection, the quality of each subtask's output directly
affects the inputs, and hence the cost and success rate, of downstream LLMs,
creating complex performance dependencies that must be learned and accounted
for during selection. We propose a neural contextual bandit-based algorithm
that trains neural networks that model LLM success on each subtask in an online
manner, thus learning to guide the LLM selections for the different subtasks,
even in the absence of historical LLM performance data. Experiments on
telecommunications question answering and medical diagnosis prediction datasets
illustrate the effectiveness of our proposed approach compared to other LLM
selection algorithms.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [46] [A Context-aware Attention and Graph Neural Network-based Multimodal Framework for Misogyny Detection](https://arxiv.org/abs/2508.09175)
*Mohammad Zia Ur Rehman,Sufyaan Zahoor,Areeb Manzoor,Musharaf Maqbool,Nagendra Kumar*

Main category: cs.CV

TL;DR: This paper introduces a new multimodal framework to detect misogynistic content, achieving significant improvements on two datasets by using attention mechanisms, graph-based feature refinement, and content-specific feature learning.


<details>
  <summary>Details</summary>
Motivation: A substantial portion of offensive content on social media is directed towards women. Since the approaches for general offensive content detection face a challenge in detecting misogynistic content, it requires solutions tailored to address offensive content against women.

Method: a novel multimodal framework for the detection of misogynistic and sexist content. The framework comprises three modules: the Multimodal Attention module (MANM), the Graph-based Feature Reconstruction Module (GFRM), and the Content-specific Features Learning Module (CFLM)

Result: The performance of the proposed approach has been evaluated on two multimodal datasets, MAMI and MMHS150K, with 11,000 and 13,494 samples, respectively.

Conclusion: The proposed method demonstrates an average improvement of 10.17% and 8.88% in macro-F1 over existing methods on the MAMI and MMHS150K datasets, respectively.

Abstract: A substantial portion of offensive content on social media is directed
towards women. Since the approaches for general offensive content detection
face a challenge in detecting misogynistic content, it requires solutions
tailored to address offensive content against women. To this end, we propose a
novel multimodal framework for the detection of misogynistic and sexist
content. The framework comprises three modules: the Multimodal Attention module
(MANM), the Graph-based Feature Reconstruction Module (GFRM), and the
Content-specific Features Learning Module (CFLM). The MANM employs adaptive
gating-based multimodal context-aware attention, enabling the model to focus on
relevant visual and textual information and generating contextually relevant
features. The GFRM module utilizes graphs to refine features within individual
modalities, while the CFLM focuses on learning text and image-specific features
such as toxicity features and caption features. Additionally, we curate a set
of misogynous lexicons to compute the misogyny-specific lexicon score from the
text. We apply test-time augmentation in feature space to better generalize the
predictions on diverse inputs. The performance of the proposed approach has
been evaluated on two multimodal datasets, MAMI and MMHS150K, with 11,000 and
13,494 samples, respectively. The proposed method demonstrates an average
improvement of 10.17% and 8.88% in macro-F1 over existing methods on the MAMI
and MMHS150K datasets, respectively.

</details>


### [47] [IAD-R1: Reinforcing Consistent Reasoning in Industrial Anomaly Detection](https://arxiv.org/abs/2508.09178)
*Yanhui Li,Yunkang Cao,Chengliang Liu,Yuan Xiong,Xinghui Dong,Chao Huang*

Main category: cs.CV

TL;DR: 提出了IAD-R1框架，显著提升了视觉语言模型在工业异常检测中的性能，甚至超越了商业模型。


<details>
  <summary>Details</summary>
Motivation: 工业异常检测是现代制造业的关键组成部分，但缺陷样本的稀缺限制了传统检测方法在特定场景中的应用。虽然视觉语言模型(VLM)在泛化能力方面表现出显著的优势，但它们在工业异常检测中的性能仍然有限。

Method: 提出了IAD-R1，一个通用的后训练框架，适用于不同架构和参数规模的视觉语言模型。该框架采用两阶段训练策略：感知激活监督微调(PA-SFT)和结构化控制组相对策略优化(SC-GRPO)。

Result: IAD-R1在6个工业异常检测基准数据集上实现了高达43.3%的平均准确率提升，并且0.5B参数模型在零样本设置中超过了GPT-4.1和Claude-Sonnet-4等商业模型。

Conclusion: IAD-R1通过在7个视觉语言模型上进行实验，在6个工业异常检测基准数据集上实现了显著的改进，平均准确率提高了43.3%。参数为0.5B的模型在零样本设置中超过了GPT-4.1和Claude-Sonnet-4等商业模型。

Abstract: Industrial anomaly detection is a critical component of modern manufacturing,
yet the scarcity of defective samples restricts traditional detection methods
to scenario-specific applications. Although Vision-Language Models (VLMs)
demonstrate significant advantages in generalization capabilities, their
performance in industrial anomaly detection remains limited. To address this
challenge, we propose IAD-R1, a universal post-training framework applicable to
VLMs of different architectures and parameter scales, which substantially
enhances their anomaly detection capabilities. IAD-R1 employs a two-stage
training strategy: the Perception Activation Supervised Fine-Tuning (PA-SFT)
stage utilizes a meticulously constructed high-quality Chain-of-Thought dataset
(Expert-AD) for training, enhancing anomaly perception capabilities and
establishing reasoning-to-answer correlations; the Structured Control Group
Relative Policy Optimization (SC-GRPO) stage employs carefully designed reward
functions to achieve a capability leap from "Anomaly Perception" to "Anomaly
Interpretation". Experimental results demonstrate that IAD-R1 achieves
significant improvements across 7 VLMs, attaining up to 43.3% enhancement in
average accuracy on 6 industrial anomaly detection benchmark datasets. Notably,
the 0.5B parameter model trained with IAD-R1 surpasses commercial models
including GPT-4.1 and Claude-Sonnet-4 in zero-shot settings, demonstrating the
effectiveness and superiority of IAD-R1. The dataset, code, and all model
weights will be publicly available at https://github.com/Yanhui-Lee/IAD-R1.

</details>


### [48] [A Neurosymbolic Framework for Interpretable Cognitive Attack Detection in Augmented Reality](https://arxiv.org/abs/2508.09185)
*Rongqian Chen,Allison Andreyev,Yanming Xiu,Mahdi Imani,Bin Li,Maria Gorlatova,Gang Tan,Tian Lan*

Main category: cs.CV

TL;DR: CADAR, a neurosymbolic approach, improves cognitive attack detection in AR by fusing vision-language inputs and using particle filtering for reasoning.


<details>
  <summary>Details</summary>
Motivation: Cognitive attacks that alter AR content are a growing concern. Existing detection methods lack semantic reasoning or interpretability.

Method: A novel neurosymbolic approach (CADAR) fusing multimodal vision-language inputs using neural VLMs to obtain a symbolic perception-graph representation, incorporating prior knowledge, salience weighting, and temporal correlations. It uses particle-filter based statistical reasoning for attack detection.

Result: Experiments on an extended AR cognitive attack dataset show accuracy improvements of up to 10.7% over strong baselines.

Conclusion: CADAR achieves up to 10.7% accuracy improvement over strong baselines in detecting challenging AR attack scenarios, demonstrating the potential of neurosymbolic methods.

Abstract: Augmented Reality (AR) enriches perception by overlaying virtual elements on
the physical world. Due to its growing popularity, cognitive attacks that alter
AR content to manipulate users' semantic perception have received increasing
attention. Existing detection methods often focus on visual changes, which are
restricted to pixel- or image-level processing and lack semantic reasoning
capabilities, or they rely on pre-trained vision-language models (VLMs), which
function as black-box approaches with limited interpretability. In this paper,
we present CADAR, a novel neurosymbolic approach for cognitive attack detection
in AR. It fuses multimodal vision-language inputs using neural VLMs to obtain a
symbolic perception-graph representation, incorporating prior knowledge,
salience weighting, and temporal correlations. The model then enables
particle-filter based statistical reasoning -- a sequential Monte Carlo method
-- to detect cognitive attacks. Thus, CADAR inherits the adaptability of
pre-trained VLM and the interpretability and reasoning rigor of particle
filtering. Experiments on an extended AR cognitive attack dataset show accuracy
improvements of up to 10.7% over strong baselines on challenging AR attack
scenarios, underscoring the promise of neurosymbolic methods for effective and
interpretable cognitive attack detection.

</details>


### [49] [RL-MoE: An Image-Based Privacy Preserving Approach In Intelligent Transportation System](https://arxiv.org/abs/2508.09186)
*Abdolazim Rezaei,Mehdi Sookhak,Mahboobeh Haghparast*

Main category: cs.CV

TL;DR: RL-MoE将敏感视觉数据转换为保护隐私的文本描述，优于现有的隐私保护机制。


<details>
  <summary>Details</summary>
Motivation: 智能交通系统(ITS)中人工智能摄像头激增，对丰富视觉数据的需求与基本隐私权之间存在严重冲突。现有的隐私保护机制（如模糊或加密）通常不足，要么隐私受到损害，要么数据效用严重降低。

Method: RL-MoE结合了混合专家(MoE)架构和强化学习(RL) Agent，优化生成的文本，以实现语义准确性和隐私保护的双重目标。

Result: RL-MoE提供了卓越的隐私保护，同时生成比基线方法更丰富的文本内容。

Conclusion: RL-MoE在CFP-FP数据集上将重放攻击的成功率降低到9.4%，同时生成比基线方法更丰富的文本内容。为在隐私敏感领域构建可信赖的AI系统提供了一种实用且可扩展的解决方案。

Abstract: The proliferation of AI-powered cameras in Intelligent Transportation Systems
(ITS) creates a severe conflict between the need for rich visual data and the
fundamental right to privacy. Existing privacy-preserving mechanisms, such as
blurring or encryption, are often insufficient, creating an undesirable
trade-off where either privacy is compromised against advanced reconstruction
attacks or data utility is critically degraded. To resolve this impasse, we
propose RL-MoE, a novel framework that transforms sensitive visual data into
privacy-preserving textual descriptions, eliminating the need for direct image
transmission. RL-MoE uniquely combines a Mixture-of-Experts (MoE) architecture
for nuanced, multi-aspect scene decomposition with a Reinforcement Learning
(RL) agent that optimizes the generated text for a dual objective of semantic
accuracy and privacy preservation. Extensive experiments demonstrate that
RL-MoE provides superior privacy protection, reducing the success rate of
replay attacks to just 9.4\% on the CFP-FP dataset, while simultaneously
generating richer textual content than baseline methods. Our work provides a
practical and scalable solution for building trustworthy AI systems in
privacy-sensitive domains, paving the way for more secure smart city and
autonomous vehicle networks.

</details>


### [50] [Synthetic Data Generation for Emotional Depth Faces: Optimizing Conditional DCGANs via Genetic Algorithms in the Latent Space and Stabilizing Training with Knowledge Distillation](https://arxiv.org/abs/2508.09188)
*Seyed Muhammad Hossein Mousavi,S. Younes Mirinezhad*

Main category: cs.CV

TL;DR: 提出了一个用于合成深度人脸生成的框架，该框架使用优化的GAN和知识蒸馏来提高质量和多样性，并在情感识别方面取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 情感计算面临着一个主要的挑战：缺乏高质量、多样化的深度面部数据集来识别微妙的情感表达。

Method: 使用具有知识蒸馏（EMA教师模型）的优化GAN来稳定训练，提高质量，并防止模式崩溃。我们还应用遗传算法来基于图像统计演化GAN潜在向量，从而提高目标情感的多样性和视觉质量。

Result: 该方法在多样性和质量方面均优于GAN、VAE、GMM和KDE。

Conclusion: 使用XGBoost分类器，LBP、HOG、Sobel边缘和强度直方图特征的提取和连接实现了94%和96%的准确率。使用FID、IS、SSIM和PSNR的评估表明，与最先进的方法相比，该方法具有持续的改进。

Abstract: Affective computing faces a major challenge: the lack of high-quality,
diverse depth facial datasets for recognizing subtle emotional expressions. We
propose a framework for synthetic depth face generation using an optimized GAN
with Knowledge Distillation (EMA teacher models) to stabilize training, improve
quality, and prevent mode collapse. We also apply Genetic Algorithms to evolve
GAN latent vectors based on image statistics, boosting diversity and visual
quality for target emotions. The approach outperforms GAN, VAE, GMM, and KDE in
both diversity and quality. For classification, we extract and concatenate LBP,
HOG, Sobel edge, and intensity histogram features, achieving 94% and 96%
accuracy with XGBoost. Evaluation using FID, IS, SSIM, and PSNR shows
consistent improvement over state-of-the-art methods.

</details>


### [51] [$Δ$-AttnMask: Attention-Guided Masked Hidden States for Efficient Data Selection and Augmentation](https://arxiv.org/abs/2508.09199)
*Jucheng Hu,Suorong Yang,Dongzhan Zhou*

Main category: cs.CV

TL;DR: This paper introduces $\Delta$-AttnMask, a data-efficient framework for visual instruction finetuning (VIF) that selects high-quality data samples using attention-guided masking, achieving state-of-the-art performance with only 20% of the data.


<details>
  <summary>Details</summary>
Motivation: VIF imposes stricter data selection challenges: the method must scale efficiently to handle larger data demands while ensuring the quality of both visual and textual content, as well as their alignment. Despite its critical impact on performance, data selection for VIF remains an understudied area.

Method: $\Delta$-AttnMask. This data-efficient framework quantifies sample quality through attention-guided masking of the model's hidden states, jointly evaluating image-text pairs without requiring domain labels, auxiliary models, or extra training. By computing loss differences ($\Delta$) between the original states and states masked using high-attention regions, $\Delta$-AttnMask intrinsically assesses sample quality.

Result: $\Delta$-AttnMask achieves state-of-the-art performance with just 20% of data, accelerating training by 5x while surpassing full-dataset baselines by +10.1% in overall accuracy.

Conclusion: Experiments across multiple VLMs and datasets show that $\Delta$-AttnMask achieves state-of-the-art performance with just 20% of data, accelerating training by 5x while surpassing full-dataset baselines by +10.1% in overall accuracy. Its model-agnostic and data-agnostic design ensures broad applicability across modalities and architectures.

Abstract: Visual Instruction Finetuning (VIF) is pivotal for post-training
Vision-Language Models (VLMs). Unlike unimodal instruction finetuning in
plain-text large language models, which mainly requires instruction datasets to
enable model instruction-following ability, VIF also requires multimodal data
to enable joint visual and textual understanding; therefore, it typically
requires more data. Consequently, VIF imposes stricter data selection
challenges: the method must scale efficiently to handle larger data demands
while ensuring the quality of both visual and textual content, as well as their
alignment. Despite its critical impact on performance, data selection for VIF
remains an understudied area. In this paper, we propose $\Delta$-AttnMask. This
data-efficient framework quantifies sample quality through attention-guided
masking of the model's hidden states, jointly evaluating image-text pairs
without requiring domain labels, auxiliary models, or extra training. By
computing loss differences ($\Delta$) between the original states and states
masked using high-attention regions, $\Delta$-AttnMask intrinsically assesses
sample quality. Experiments across multiple VLMs and datasets show that
$\Delta$-AttnMask achieves state-of-the-art performance with just 20% of data,
accelerating training by 5x while surpassing full-dataset baselines by +10.1%
in overall accuracy. Its model-agnostic and data-agnostic design ensures broad
applicability across modalities and architectures.

</details>


### [52] [Personalized Feature Translation for Expression Recognition: An Efficient Source-Free Domain Adaptation Method](https://arxiv.org/abs/2508.09202)
*Masoumeh Sharafi,Soufiane Belharbi,Houssem Ben Salem,Ali Etemad,Alessandro Lameiras Koerich,Marco Pedersoli,Simon Bacon,Eric Granger*

Main category: cs.CV

TL;DR: This paper proposes personalized feature translation (PFT) for source-free domain adaptation (SFDA) in facial expression recognition (FER). PFT operates in the latent space and adapts on neutral target data, avoiding image synthesis and reducing computational overhead.


<details>
  <summary>Details</summary>
Motivation: Deep FER models often struggle with subtle expressions and high inter-subject variability, limiting their performance in real-world applications. Source-free domain adaptation (SFDA) methods have been proposed to personalize a pretrained source model using only unlabeled target domain data, thereby avoiding data privacy, storage, and transmission constraints. This paper addresses a challenging scenario where source data is unavailable for adaptation, and only unlabeled target data consisting solely of neutral expressions is available. SFDA methods are not typically designed to adapt using target data from only a single class. Further, using models to generate facial images with non-neutral expressions can be unstable and computationally intensive.

Method: Personalized feature translation (PFT) is proposed for SFDA. The translator is pre-trained on the source domain data to transform the subject-specific style features from one source subject into another. Expression information is preserved by optimizing a combination of expression consistency and style-aware objectives. Then, the translator is adapted on neutral target data, without using source data or image synthesis. By translating in the latent space, PFT avoids the complexity and noise of face expression generation, producing discriminative embeddings optimized for classification.

Result: PFT eliminates the need for image synthesis, reduces computational overhead (using a lightweight translator), and only adapts part of the model, making the method efficient compared to image-based translation.

Conclusion: Personalized feature translation (PFT) is proposed for SFDA, which operates in the latent space, avoids the complexity and noise of face expression generation, and produces discriminative embeddings optimized for classification. PFT eliminates the need for image synthesis, reduces computational overhead, and only adapts part of the model, making the method efficient compared to image-based translation.

Abstract: Facial expression recognition (FER) models are employed in many video-based
affective computing applications, such as human-computer interaction and
healthcare monitoring. However, deep FER models often struggle with subtle
expressions and high inter-subject variability, limiting their performance in
real-world applications. To improve their performance, source-free domain
adaptation (SFDA) methods have been proposed to personalize a pretrained source
model using only unlabeled target domain data, thereby avoiding data privacy,
storage, and transmission constraints. This paper addresses a challenging
scenario where source data is unavailable for adaptation, and only unlabeled
target data consisting solely of neutral expressions is available. SFDA methods
are not typically designed to adapt using target data from only a single class.
Further, using models to generate facial images with non-neutral expressions
can be unstable and computationally intensive. In this paper, personalized
feature translation (PFT) is proposed for SFDA. Unlike current image
translation methods for SFDA, our lightweight method operates in the latent
space. We first pre-train the translator on the source domain data to transform
the subject-specific style features from one source subject into another.
Expression information is preserved by optimizing a combination of expression
consistency and style-aware objectives. Then, the translator is adapted on
neutral target data, without using source data or image synthesis. By
translating in the latent space, PFT avoids the complexity and noise of face
expression generation, producing discriminative embeddings optimized for
classification. Using PFT eliminates the need for image synthesis, reduces
computational overhead (using a lightweight translator), and only adapts part
of the model, making the method efficient compared to image-based translation.

</details>


### [53] [GANime: Generating Anime and Manga Character Drawings from Sketches with Deep Learning](https://arxiv.org/abs/2508.09207)
*Tai Vu,Robert Yang*

Main category: cs.CV

TL;DR: C-GAN is the best model for automatically colorizing anime sketches.


<details>
  <summary>Details</summary>
Motivation: Generating colorized drawings from sketches is a costly bottleneck in the manga and anime industry.

Method: Image-to-image translation using Neural Style Transfer, C-GAN, and CycleGAN.

Result: C-GAN produces high-quality and high-resolution images close to those created by humans.

Conclusion: C-GAN is the most effective model for producing high-quality and high-resolution anime character images from sketches.

Abstract: The process of generating fully colorized drawings from sketches is a large,
usually costly bottleneck in the manga and anime industry. In this study, we
examine multiple models for image-to-image translation between anime characters
and their sketches, including Neural Style Transfer, C-GAN, and CycleGAN. By
assessing them qualitatively and quantitatively, we find that C-GAN is the most
effective model that is able to produce high-quality and high-resolution images
close to those created by humans.

</details>


### [54] [MME-Emotion: A Holistic Evaluation Benchmark for Emotional Intelligence in Multimodal Large Language Models](https://arxiv.org/abs/2508.09210)
*Fan Zhang,Zebang Cheng,Chong Deng,Haoxuan Li,Zheng Lian,Qian Chen,Huadai Liu,Wen Wang,Yi-Fan Zhang,Renrui Zhang,Ziyu Guo,Zhihong Zhu,Hao Wu,Haixin Wang,Yefeng Zheng,Xiaojiang Peng,Xian Wu,Kun Wang,Xiangang Li,Jieping Ye,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: The authors introduce MME-Emotion, a new benchmark for evaluating emotional intelligence in MLLMs. Current MLLMs perform poorly, but the benchmark can help advance the field.


<details>
  <summary>Details</summary>
Motivation: Current emotional benchmarks remain limited, as it is still unknown: (a) the generalization abilities of MLLMs across distinct scenarios, and (b) their reasoning capabilities to identify the triggering factors behind emotional states. To bridge these gaps, the authors introduce MME-Emotion.

Method: The authors present MME-Emotion, a systematic benchmark that assesses both emotional understanding and reasoning capabilities of MLLMs, containing over 6,000 curated video clips with task-specific questioning-answering (QA) pairs, spanning broad scenarios to formulate eight emotional tasks. It further incorporates a holistic evaluation suite with hybrid metrics for emotion recognition and reasoning, analyzed through a multi-agent system framework. Through a rigorous evaluation of 20 advanced MLLMs, the authors uncover both their strengths and limitations.

Result: The best-performing model achieves only 39.3% recognition score and 56.0% Chain-of-Thought (CoT) score on the benchmark. Generalist models derive emotional intelligence from generalized multimodal understanding capabilities, while specialist models can achieve comparable performance through domain-specific post-training adaptation.

Conclusion: Current MLLMs exhibit unsatisfactory emotional intelligence, with the best-performing model achieving only $39.3\%$ recognition score and $56.0\%$ Chain-of-Thought (CoT) score on our benchmark. Generalist models (e.g., Gemini-2.5-Pro) derive emotional intelligence from generalized multimodal understanding capabilities, while specialist models (e.g., R1-Omni) can achieve comparable performance through domain-specific post-training adaptation. By introducing MME-Emotion, the authors hope that it can serve as a foundation for advancing MLLMs' emotional intelligence in the future.

Abstract: Recent advances in multimodal large language models (MLLMs) have catalyzed
transformative progress in affective computing, enabling models to exhibit
emergent emotional intelligence. Despite substantial methodological progress,
current emotional benchmarks remain limited, as it is still unknown: (a) the
generalization abilities of MLLMs across distinct scenarios, and (b) their
reasoning capabilities to identify the triggering factors behind emotional
states. To bridge these gaps, we present \textbf{MME-Emotion}, a systematic
benchmark that assesses both emotional understanding and reasoning capabilities
of MLLMs, enjoying \textit{scalable capacity}, \textit{diverse settings}, and
\textit{unified protocols}. As the largest emotional intelligence benchmark for
MLLMs, MME-Emotion contains over 6,000 curated video clips with task-specific
questioning-answering (QA) pairs, spanning broad scenarios to formulate eight
emotional tasks. It further incorporates a holistic evaluation suite with
hybrid metrics for emotion recognition and reasoning, analyzed through a
multi-agent system framework. Through a rigorous evaluation of 20 advanced
MLLMs, we uncover both their strengths and limitations, yielding several key
insights: \ding{182} Current MLLMs exhibit unsatisfactory emotional
intelligence, with the best-performing model achieving only $39.3\%$
recognition score and $56.0\%$ Chain-of-Thought (CoT) score on our benchmark.
\ding{183} Generalist models (\emph{e.g.}, Gemini-2.5-Pro) derive emotional
intelligence from generalized multimodal understanding capabilities, while
specialist models (\emph{e.g.}, R1-Omni) can achieve comparable performance
through domain-specific post-training adaptation. By introducing MME-Emotion,
we hope that it can serve as a foundation for advancing MLLMs' emotional
intelligence in the future.

</details>


### [55] [Towards Effective MLLM Jailbreaking Through Balanced On-Topicness and OOD-Intensity](https://arxiv.org/abs/2508.09218)
*Zuoou Li,Weitong Zhang,Jingyuan Wang,Shuyuan Zhang,Wenjia Bai,Bernhard Kainz,Mengyun Qiao*

Main category: cs.CV

TL;DR: MLLM 容易受到对抗性提示的攻击。我们提出了一个四轴评估框架和一个新的攻击方法 BSD，它通过重构恶意提示并引入细微的 OOD 信号和视觉提示来提高攻击成功率和有害性。


<details>
  <summary>Details</summary>
Motivation: 多模态大型语言模型 (MLLM) 广泛用于视觉语言推理任务。然而，它们容易受到对抗性提示的影响仍然是一个严重的问题，因为安全机制通常无法阻止有害输出的生成。许多被归类为“成功”的响应实际上是良性的、模糊的或与预期的恶意目标无关。这种不匹配表明，当前的评估标准可能高估了此类攻击的有效性。

Method: 我们引入了一个四轴评估框架，该框架考虑了输入主题性、输入分布外 (OOD) 强度、输出有害性和输出拒绝率。在此基础上，我们开发了一种名为平衡结构分解 (BSD) 的递归重写策略。

Result: 高度主题性的提示经常被安全过滤器阻止，而那些过于 OOD 的提示通常会逃避检测，但无法产生有害内容。BSD 在 13 个商业和开源 MLLM 上进行了测试，与以前的方法相比，它的成功率提高了 67%，有害性提高了 21%。

Conclusion: 通过平衡相关性和新颖性的提示更可能绕过过滤器并触发危险输出。我们开发了一种名为平衡结构分解 (BSD) 的递归重写策略，该策略将恶意提示重构为语义对齐的子任务，同时引入细微的 OOD 信号和视觉提示，使输入更难被检测到。BSD 在 13 个商业和开源 MLLM 上进行了测试，与以前的方法相比，它的成功率提高了 67%，有害性提高了 21%。

Abstract: Multimodal large language models (MLLMs) are widely used in vision-language
reasoning tasks. However, their vulnerability to adversarial prompts remains a
serious concern, as safety mechanisms often fail to prevent the generation of
harmful outputs. Although recent jailbreak strategies report high success
rates, many responses classified as "successful" are actually benign, vague, or
unrelated to the intended malicious goal. This mismatch suggests that current
evaluation standards may overestimate the effectiveness of such attacks. To
address this issue, we introduce a four-axis evaluation framework that
considers input on-topicness, input out-of-distribution (OOD) intensity, output
harmfulness, and output refusal rate. This framework identifies truly effective
jailbreaks. In a substantial empirical study, we reveal a structural trade-off:
highly on-topic prompts are frequently blocked by safety filters, whereas those
that are too OOD often evade detection but fail to produce harmful content.
However, prompts that balance relevance and novelty are more likely to evade
filters and trigger dangerous output. Building on this insight, we develop a
recursive rewriting strategy called Balanced Structural Decomposition (BSD).
The approach restructures malicious prompts into semantically aligned
sub-tasks, while introducing subtle OOD signals and visual cues that make the
inputs harder to detect. BSD was tested across 13 commercial and open-source
MLLMs, where it consistently led to higher attack success rates, more harmful
outputs, and fewer refusals. Compared to previous methods, it improves success
rates by $67\%$ and harmfulness by $21\%$, revealing a previously
underappreciated weakness in current multimodal safety systems.

</details>


### [56] [Towards Scalable Training for Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2508.09220)
*Haoyang Li,Jiaqing Li,Jialun Cao,Zongyuan Yang,Yongping Xiong*

Main category: cs.CV

TL;DR: The authors propose a new method to generate a large dataset of LaTeX formulas and train a HMER model that achieves state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The field of Handwritten Mathematical Expression Recognition (HMER) has been impeded by the scarcity of data, primarily due to the arduous and costly process of manual annotation.

Method: The authors propose a novel method integrating limited handwritten formulas with large-scale LaTeX-rendered formulas by developing a scalable data engine to generate complex and consistent LaTeX sequences. Then they propose TexTeller, the first HMER model trained at scale, by mix-training Tex80M with a relatively small HME dataset.

Result: The authors built the largest formula dataset to date, termed Tex80M, comprising over 80 million high-quality training instances. The expansive training dataset and their refined pipeline have equipped TexTeller with state-of-the-art (SOTA) performance across nearly all benchmarks.

Conclusion: The authors achieved state-of-the-art (SOTA) performance across nearly all benchmarks by mix-training Tex80M with a relatively small HME dataset, and they will openly release their complete model, entire dataset, and full codebase.

Abstract: Large foundation models have achieved significant performance gains through
scalable training on massive datasets. However, the field of
\textbf{H}andwritten \textbf{M}athematical \textbf{E}xpression
\textbf{R}ecognition (HMER) has been impeded by the scarcity of data, primarily
due to the arduous and costly process of manual annotation. To bridge this gap,
we propose a novel method integrating limited handwritten formulas with
large-scale LaTeX-rendered formulas by developing a scalable data engine to
generate complex and consistent LaTeX sequences. With this engine, we built the
largest formula dataset to date, termed \texttt{Tex80M}, comprising over 80
million high-quality training instances. Then we propose \texttt{TexTeller},
the first HMER model trained at scale, by mix-training \texttt{Tex80M} with a
relatively small HME dataset. The expansive training dataset and our refined
pipeline have equipped \texttt{TexTeller} with state-of-the-art (SOTA)
performance across nearly all benchmarks. To advance the field, we will openly
release our complete model, entire dataset, and full codebase, enabling further
research building upon our contributions.

</details>


### [57] [Gradient-Direction-Aware Density Control for 3D Gaussian Splatting](https://arxiv.org/abs/2508.09239)
*Zheng Zhou,Yu-Jie Xiong,Chun-Ming Xia,Jia-Chen Zhang,Hong-Jian Zhan*

Main category: cs.CV

TL;DR: GDAGS improves 3D Gaussian Splatting by addressing over-reconstruction and over-densification issues with a gradient-direction-aware density control framework, achieving better rendering quality and reduced memory consumption.


<details>
  <summary>Details</summary>
Motivation: existing approaches manifest two critical limitations in complex scenarios: (1) Over-reconstruction occurs when persistent large Gaussians cannot meet adaptive splitting thresholds during density control. (2) Over-densification of Gaussians occurs in regions with aligned gradient aggregation, leading to redundant component proliferation.

Method: a gradient-direction-aware adaptive density control framework

Result: GDAGS prioritizes conflicting-gradient Gaussians during splitting operations to enhance geometric details while suppressing redundant concordant-direction Gaussians. Conversely, in cloning processes, GDAGS promotes concordant-direction Gaussian densification for structural completion while preventing conflicting-direction Gaussian overpopulation.

Conclusion: GDAGS achieves superior rendering quality while effectively mitigating over-reconstruction, suppressing over-densification, and constructing compact scene representations with 50% reduced memory consumption through optimized Gaussians utilization.

Abstract: The emergence of 3D Gaussian Splatting (3DGS) has significantly advanced
novel view synthesis through explicit scene representation, enabling real-time
photorealistic rendering. However, existing approaches manifest two critical
limitations in complex scenarios: (1) Over-reconstruction occurs when
persistent large Gaussians cannot meet adaptive splitting thresholds during
density control. This is exacerbated by conflicting gradient directions that
prevent effective splitting of these Gaussians; (2) Over-densification of
Gaussians occurs in regions with aligned gradient aggregation, leading to
redundant component proliferation. This redundancy significantly increases
memory overhead due to unnecessary data retention. We present
Gradient-Direction-Aware Gaussian Splatting (GDAGS), a gradient-direction-aware
adaptive density control framework to address these challenges. Our key
innovations: the gradient coherence ratio (GCR), computed through normalized
gradient vector norms, which explicitly discriminates Gaussians with concordant
versus conflicting gradient directions; and a nonlinear dynamic weighting
mechanism leverages the GCR to enable gradient-direction-aware density control.
Specifically, GDAGS prioritizes conflicting-gradient Gaussians during splitting
operations to enhance geometric details while suppressing redundant
concordant-direction Gaussians. Conversely, in cloning processes, GDAGS
promotes concordant-direction Gaussian densification for structural completion
while preventing conflicting-direction Gaussian overpopulation. Comprehensive
evaluations across diverse real-world benchmarks demonstrate that GDAGS
achieves superior rendering quality while effectively mitigating
over-reconstruction, suppressing over-densification, and constructing compact
scene representations with 50\% reduced memory consumption through optimized
Gaussians utilization.

</details>


### [58] [A Signer-Invariant Conformer and Multi-Scale Fusion Transformer for Continuous Sign Language Recognition](https://arxiv.org/abs/2508.09372)
*Md Rezwanul Haque,Md. Milon Islam,S M Taslim Uddin Raju,Fakhri Karray*

Main category: cs.CV

TL;DR: This paper introduces a dual-architecture framework for continuous sign language recognition, featuring a Signer-Invariant Conformer for signer-independent challenges and a Multi-Scale Fusion Transformer for unseen sentence tasks. The results demonstrate significant performance improvements on the Isharah-1000 dataset and in the SignEval 2025 CSLR challenge.


<details>
  <summary>Details</summary>
Motivation: Continuous Sign Language Recognition (CSLR) faces multiple challenges, including significant inter-signer variability and poor generalization to novel sentence structures. Traditional solutions frequently fail to handle these issues efficiently.

Method: The paper proposes a dual-architecture framework: a Signer-Invariant Conformer for the Signer-Independent (SI) challenge and a Multi-Scale Fusion Transformer with a novel dual-path temporal encoder for the Unseen-Sentences (US) task.

Result: The proposed conformer architecture achieves a Word Error Rate (WER) of 13.07% on the SI challenge, a reduction of 13.53% from the state-of-the-art. On the US task, the transformer model scores a WER of 47.78%, surpassing previous work. In the SignEval 2025 CSLR challenge, the team placed 2nd in the US task and 4th in the SI task.

Conclusion: The paper validates the hypothesis that task-specific networks designed for the particular challenges of CSLR leads to considerable performance improvements and establishes a new baseline for further research.

Abstract: Continuous Sign Language Recognition (CSLR) faces multiple challenges,
including significant inter-signer variability and poor generalization to novel
sentence structures. Traditional solutions frequently fail to handle these
issues efficiently. For overcoming these constraints, we propose a
dual-architecture framework. For the Signer-Independent (SI) challenge, we
propose a Signer-Invariant Conformer that combines convolutions with multi-head
self-attention to learn robust, signer-agnostic representations from pose-based
skeletal keypoints. For the Unseen-Sentences (US) task, we designed a
Multi-Scale Fusion Transformer with a novel dual-path temporal encoder that
captures both fine-grained posture dynamics, enabling the model's ability to
comprehend novel grammatical compositions. Experiments on the challenging
Isharah-1000 dataset establish a new standard for both CSLR benchmarks. The
proposed conformer architecture achieves a Word Error Rate (WER) of 13.07% on
the SI challenge, a reduction of 13.53% from the state-of-the-art. On the US
task, the transformer model scores a WER of 47.78%, surpassing previous work.
In the SignEval 2025 CSLR challenge, our team placed 2nd in the US task and 4th
in the SI task, demonstrating the performance of these models. The findings
validate our key hypothesis: that developing task-specific networks designed
for the particular challenges of CSLR leads to considerable performance
improvements and establishes a new baseline for further research. The source
code is available at: https://github.com/rezwanh001/MSLR-Pose86K-CSLR-Isharah.

</details>


### [59] [FineState-Bench: A Comprehensive Benchmark for Fine-Grained State Control in GUI Agents](https://arxiv.org/abs/2508.09241)
*Fengxian Ji,Jingpu Yang,Zirui Song,Yuanxi Wang,Zhexuan Cui,Yuke Li,Qian Jiang,Miao Fang,Xiuying Chen*

Main category: cs.CV

TL;DR: 提出了FineState-Bench，这是一个用于评估细粒度GUI代理操作的基准，表明视觉定位是当前GUI代理的主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前的GUI代理评估框架存在根本缺陷：现有基准过于关注粗粒度的任务完成，而忽略了对于实际应用至关重要的细粒度控制能力。

Method: 引入FineState-Bench，这是第一个用于细粒度GUI代理操作的评估和诊断标准，它包含四个组件中的2257个任务基准，并使用四阶段指标进行全面的感知到控制评估。开发了即插即用的视觉诊断助手（VDA），以分析用于精细操作的感知和定位。

Result: 最先进的模型仅达到32.8%的细粒度交互准确率。理想的视觉定位将使Gemini-2.5-Flash的成功率提高14.9%。

Conclusion: 当前GUI代理的主要瓶颈是基本的视觉定位能力。

Abstract: With the rapid advancement of generative artificial intelligence technology,
Graphical User Interface (GUI) agents have demonstrated tremendous potential
for autonomously managing daily tasks through natural language instructions.
However, current evaluation frameworks for GUI agents suffer from fundamental
flaws: existing benchmarks overly focus on coarse-grained task completion while
neglecting fine-grained control capabilities crucial for real-world
applications. To address this, we introduce FineState-Bench, the first
evaluation and diagnostic standard for fine-grained GUI proxy operations,
designed to quantify fine-grained control. This multi-platform (desktop, Web,
mobile) framework includes 2257 task benchmarks in four components and uses a
four-phase indicator for comprehensive perception-to-control assessment. To
analyze perception and positioning for refined operations, we developed the
plug-and-play Visual Diagnostic Assistant (VDA), enabling the first
quantitative decoupling analysis of these capabilities. Experimental results on
our benchmark show that the most advanced models achieve only 32.8%
fine-grained interaction accuracy. Using our VDA in controlled experiments,
quantifying the impact of visual capabilities, we showed that ideal visual
localization boosts Gemini-2.5-Flash's success rate by 14.9\%. Our diagnostic
framework confirms for the first time that the primary bottleneck for current
GUI proxies is basic visual positioning capability.All resources are fully
open-source. github: https://github.com/AnonymousThewarehouse/FineState-Bench
huggingface: https://huggingface.co/datasets/Willtime2006/Static-FineBench

</details>


### [60] [Beyond Blanket Masking: Examining Granularity for Privacy Protection in Images Captured by Blind and Low Vision Users](https://arxiv.org/abs/2508.09245)
*Jeffri Murrugarra-LLerena,Haoran Niu,K. Suzanne Barber,Hal Daumé III,Yang Trista Cao,Paola Cascante-Bonilla*

Main category: cs.CV

TL;DR: propose FiGPriv, a fine-grained privacy protection framework that selectively masks only high-risk private information while preserving low-risk information


<details>
  <summary>Details</summary>
Motivation: concerns over user privacy have grown, particularly for blind and low vision users who may unknowingly capture personal private information in their images. Existing privacy protection methods rely on coarse-grained segmentation, which uniformly masks entire private objects, often at the cost of usability

Method: integrates fine-grained segmentation with a data-driven risk scoring mechanism

Result: FiG-Priv preserves +26% of image content, enhancing the ability of VLMs to provide useful responses by 11% and identify the image content by 45%, while ensuring privacy protection.

Conclusion: FiG-Priv preserves +26% of image content, enhancing the ability of VLMs to provide useful responses by 11% and identify the image content by 45%, while ensuring privacy protection.

Abstract: As visual assistant systems powered by visual language models (VLMs) become
more prevalent, concerns over user privacy have grown, particularly for blind
and low vision users who may unknowingly capture personal private information
in their images. Existing privacy protection methods rely on coarse-grained
segmentation, which uniformly masks entire private objects, often at the cost
of usability. In this work, we propose FiGPriv, a fine-grained privacy
protection framework that selectively masks only high-risk private information
while preserving low-risk information. Our approach integrates fine-grained
segmentation with a data-driven risk scoring mechanism. We evaluate our
framework using the BIV-Priv-Seg dataset and show that FiG-Priv preserves +26%
of image content, enhancing the ability of VLMs to provide useful responses by
11% and identify the image content by 45%, while ensuring privacy protection.
Project Page: https://artcs1.github.io/VLMPrivacy/

</details>


### [61] [Harnessing Input-Adaptive Inference for Efficient VLN](https://arxiv.org/abs/2508.09262)
*Dongwoo Kang,Akhil Perincherry,Zachary Coalson,Aiden Gabriel,Stefan Lee,Sanghyun Hong*

Main category: cs.CV

TL;DR: 提出了一种输入自适应导航方法，通过选择性处理全景视图、基于重要性的自适应阈值和缓存机制，提高了 VLN 模型的效率，并在 VLN 基准测试中实现了计算量减少。


<details>
  <summary>Details</summary>
Motivation: 视觉语言导航 (VLN) 中一个新兴的范例是使用历史感知的多模态 Transformer 模型。给定语言指令，这些模型处理观察和导航历史，以预测代理最合适的动作。虽然它们显着提高了性能，但这些模型的规模在计算资源有限的实际环境中可能成为瓶颈。

Method: 提出了一种新颖的输入自适应导航方法，以提高 VLN 模型的效率。引入了三种自适应算法，分别部署在不同的层级：(1) 为了提高空间效率，选择性地处理代理每次观察到的全景视图。(2) 为了提高模型内部效率，我们为提前退出方法提出了基于重要性的自适应阈值。(3) 为了提高时间效率，我们实现了一种缓存机制，可以防止重新处理代理先前看到的视图。

Result: 现有的输入自适应机制无法在不显着降低性能的情况下减少计算量。在七个 VLN 基准测试的评估中，证明在标准和连续环境中，三种现成的代理的计算量减少了 2 倍以上。

Conclusion: 在七个 VLN 基准测试的评估中，证明在标准和连续环境中，三种现成的代理的计算量减少了 2 倍以上。

Abstract: An emerging paradigm in vision-and-language navigation (VLN) is the use of
history-aware multi-modal transformer models. Given a language instruction,
these models process observation and navigation history to predict the most
appropriate action for an agent. While they have significantly improved
performance, the scale of these models can be a bottleneck in practical
settings with limited computational resources. In this work, we propose a novel
input-adaptive navigation method to enhance VLN model efficiency. We first show
that existing input-adaptive mechanisms fail to reduce computations without
substantial performance degradation. To address this, we introduce three
adaptive algorithms, each deployed at a different level: (1) To improve spatial
efficiency, we selectively process panoramic views at each observation of an
agent. (2) To improve intra-model efficiency, we propose importance-based
adaptive thresholding for the early-exit methods. (3) To improve temporal
efficiency, we implement a caching mechanism that prevents reprocessing of
views previously seen by the agent. In evaluations on seven VLN benchmarks, we
demonstrate over a 2$\times$ reduction in computation across three
off-the-shelf agents in both standard and continuous environments. Our code is
publicly available at
https://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation.

</details>


### [62] [SegDAC: Segmentation-Driven Actor-Critic for Visual Reinforcement Learning](https://arxiv.org/abs/2508.09325)
*Alexandre Brown,Glen Berseth*

Main category: cs.CV

TL;DR: SegDAC是一种分割驱动的Actor-Critic方法，它使用SAM和YOLO-World进行对象分割和语义定位，以提高视觉强化学习中的泛化能力和样本效率。


<details>
  <summary>Details</summary>
Motivation: 视觉强化学习(RL)具有挑战性，因为它需要从高维输入和噪声奖励中学习感知和行动。虽然存在大型感知模型，但如何有效地将它们集成到RL中以实现视觉泛化和提高样本效率仍不清楚。

Method: SegDAC，一种分割驱动的Actor-Critic方法。SegDAC使用Segment Anything (SAM)进行以对象为中心的分解，并使用YOLO-World通过文本提示在语义上对分割进行定位。它包括一种新颖的基于Transformer的架构，该架构支持每个时间步的动态数量的段，并有效地学习使用在线RL关注哪些段，而无需使用人工标签。

Result: SegDAC在具有挑战性的视觉泛化基准测试中取得了显著更好的视觉泛化效果，在最困难的设置下性能提高了一倍，并且在所有评估任务中的样本效率与先前方法相匹配或超过。

Conclusion: SegDAC在具有挑战性的视觉泛化基准测试中取得了显著更好的视觉泛化效果，在最困难的设置下性能提高了一倍，并且在所有评估任务中的样本效率与先前方法相匹配或超过。

Abstract: Visual reinforcement learning (RL) is challenging due to the need to learn
both perception and actions from high-dimensional inputs and noisy rewards.
Although large perception models exist, integrating them effectively into RL
for visual generalization and improved sample efficiency remains unclear. We
propose SegDAC, a Segmentation-Driven Actor-Critic method. SegDAC uses Segment
Anything (SAM) for object-centric decomposition and YOLO-World to ground
segments semantically via text prompts. It includes a novel transformer-based
architecture that supports a dynamic number of segments at each time step and
effectively learns which segments to focus on using online RL, without using
human labels. By evaluating SegDAC over a challenging visual generalization
benchmark using Maniskill3, which covers diverse manipulation tasks under
strong visual perturbations, we demonstrate that SegDAC achieves significantly
better visual generalization, doubling prior performance on the hardest setting
and matching or surpassing prior methods in sample efficiency across all
evaluated tasks.

</details>


### [63] [Lung-DDPM+: Efficient Thoracic CT Image Synthesis using Diffusion Probabilistic Model](https://arxiv.org/abs/2508.09327)
*Yifan Jiang,Ahmad Shariftabrizi,Venkata SK. Manem*

Main category: cs.CV

TL;DR: Lung-DDPM+ generates high-quality lung nodule CT images efficiently, with superior speed and memory usage compared to Lung-DDPM, while maintaining comparable quality and fidelity.


<details>
  <summary>Details</summary>
Motivation: existing generative models for lung cancer diagnosis suffer from low efficiency and anatomical imprecision, which limit their clinical applicability

Method: an improved version of our previous model, Lung-DDPM. This novel approach is a denoising diffusion probabilistic model (DDPM) guided by nodule semantic layouts and accelerated by a pulmonary DPM-solver

Result: achieves 8$\times$ fewer FLOPs (floating point operations per second), 6.8$\times$ lower GPU memory consumption, and 14$\times$ faster sampling compared to Lung-DDPM. Moreover, it maintains comparable sample quality to both Lung-DDPM and other state-of-the-art (SOTA) generative models in two downstream segmentation tasks

Conclusion: Lung-DDPM+ can effectively generate high-quality thoracic CT images with lung nodules, highlighting its potential for broader applications, such as general tumor synthesis and lesion generation in medical imaging.

Abstract: Generative artificial intelligence (AI) has been playing an important role in
various domains. Leveraging its high capability to generate high-fidelity and
diverse synthetic data, generative AI is widely applied in diagnostic tasks,
such as lung cancer diagnosis using computed tomography (CT). However, existing
generative models for lung cancer diagnosis suffer from low efficiency and
anatomical imprecision, which limit their clinical applicability. To address
these drawbacks, we propose Lung-DDPM+, an improved version of our previous
model, Lung-DDPM. This novel approach is a denoising diffusion probabilistic
model (DDPM) guided by nodule semantic layouts and accelerated by a pulmonary
DPM-solver, enabling the method to focus on lesion areas while achieving a
better trade-off between sampling efficiency and quality. Evaluation results on
the public LIDC-IDRI dataset suggest that the proposed method achieves
8$\times$ fewer FLOPs (floating point operations per second), 6.8$\times$ lower
GPU memory consumption, and 14$\times$ faster sampling compared to Lung-DDPM.
Moreover, it maintains comparable sample quality to both Lung-DDPM and other
state-of-the-art (SOTA) generative models in two downstream segmentation tasks.
We also conducted a Visual Turing Test by an experienced radiologist, showing
the advanced quality and fidelity of synthetic samples generated by the
proposed method. These experimental results demonstrate that Lung-DDPM+ can
effectively generate high-quality thoracic CT images with lung nodules,
highlighting its potential for broader applications, such as general tumor
synthesis and lesion generation in medical imaging. The code and pretrained
models are available at https://github.com/Manem-Lab/Lung-DDPM-PLUS.

</details>


### [64] [UltraLight Med-Vision Mamba for Classification of Neoplastic Progression in Tubular Adenomas](https://arxiv.org/abs/2508.09339)
*Aqsa Sultana,Nordin Abouzahra,Ahmed Rahu,Brian Shula,Brandon Combs,Derrick Forchetti,Theus Aspiras,Vijayan K. Asari*

Main category: cs.CV

TL;DR: Deep learning algorithms, especially Ultralight Med-Vision Mamba, can help identify precancerous polyps during colonoscopies, improving risk assessment and patient outcomes.


<details>
  <summary>Details</summary>
Motivation: Identification of precancerous polyps during routine colonoscopy screenings is vital for their excision, lowering the risk of developing colorectal cancer.

Method: Advanced deep learning algorithms and Ultralight Med-Vision Mamba

Result: enable precise adenoma classification and stratification, improving risk assessment accuracy and enabling personalized surveillance protocols that optimize patient outcomes.

Conclusion: Ultralight Med-Vision Mamba is a promising tool for real-time clinical deployment.

Abstract: Identification of precancerous polyps during routine colonoscopy screenings
is vital for their excision, lowering the risk of developing colorectal cancer.
Advanced deep learning algorithms enable precise adenoma classification and
stratification, improving risk assessment accuracy and enabling personalized
surveillance protocols that optimize patient outcomes. Ultralight Med-Vision
Mamba, a state-space based model (SSM), has excelled in modeling long- and
short-range dependencies and image generalization, critical factors for
analyzing whole slide images. Furthermore, Ultralight Med-Vision Mamba's
efficient architecture offers advantages in both computational speed and
scalability, making it a promising tool for real-time clinical deployment.

</details>


### [65] [Blink-to-code: real-time Morse code communication via eye blink detection and classification](https://arxiv.org/abs/2508.09344)
*Anushka Bhatt*

Main category: cs.CV

TL;DR: System translates eye blinks into Morse code for communication, achieving 62% accuracy.


<details>
  <summary>Details</summary>
Motivation: Enabling communication for individuals with severe motor impairments.

Method: A real-time system translates voluntary eye blinks into Morse code using a standard webcam and computer vision to detect and classify blinks as short (dot) or long (dash), then decodes them into alphanumeric characters.

Result: Experiments with five participants show 62% decoding accuracy and 18-20 seconds response times.

Conclusion: A viable, low-cost assistive communication method is demonstrated.

Abstract: This study proposes a real-time system that translates voluntary eye blinks
into Morse code, enabling communication for individuals with severe motor
impairments. Using a standard webcam and computer vision, the system detects
and classifies blinks as short (dot) or long (dash), then decodes them into
alphanumeric characters. Experiments with five participants show 62% decoding
accuracy and 18-20 seconds response times, demonstrating a viable, low-cost
assistive communication method.

</details>


### [66] [FusionEnsemble-Net: An Attention-Based Ensemble of Spatiotemporal Networks for Multimodal Sign Language Recognition](https://arxiv.org/abs/2508.09362)
*Md. Milon Islam,Md Rezwanul Haque,S M Taslim Uddin Raju,Fakhri Karray*

Main category: cs.CV

TL;DR: 提出了一种新的手语识别框架 FusionEnsemble-Net，该框架使用基于注意力的融合来组合来自 RGB 视频和雷达数据的各种时空网络，从而在意大利语手语识别方面实现了最先进的精度。


<details>
  <summary>Details</summary>
Motivation: 医疗保健通信中准确识别手语是一个巨大的挑战，需要能够准确解释复杂的多模态手势的框架。

Method: 提出 FusionEnsemble-Net，一种基于注意力的时空网络集成，可以动态融合视觉和运动数据以提高识别准确率。该方法通过四个不同的时空网络同步处理 RGB 视频和测距多普勒地图雷达模态。

Result: 实验表明，在意大利手语的大规模 MultiMeDaLIS 数据集上，FusionEnsemble-Net 的测试精度为 99.44%，优于最先进的方法。

Conclusion: 通过基于注意力的融合统一的各种时空网络的集合，为复杂的多模态孤立手势识别任务产生了一个鲁棒而准确的框架。

Abstract: Accurate recognition of sign language in healthcare communication poses a
significant challenge, requiring frameworks that can accurately interpret
complex multimodal gestures. To deal with this, we propose FusionEnsemble-Net,
a novel attention-based ensemble of spatiotemporal networks that dynamically
fuses visual and motion data to enhance recognition accuracy. The proposed
approach processes RGB video and range Doppler map radar modalities
synchronously through four different spatiotemporal networks. For each network,
features from both modalities are continuously fused using an attention-based
fusion module before being fed into an ensemble of classifiers. Finally, the
outputs of these four different fused channels are combined in an ensemble
classification head, thereby enhancing the model's robustness. Experiments
demonstrate that FusionEnsemble-Net outperforms state-of-the-art approaches
with a test accuracy of 99.44% on the large-scale MultiMeDaLIS dataset for
Italian Sign Language. Our findings indicate that an ensemble of diverse
spatiotemporal networks, unified by attention-based fusion, yields a robust and
accurate framework for complex, multimodal isolated gesture recognition tasks.
The source code is available at:
https://github.com/rezwanh001/Multimodal-Isolated-Italian-Sign-Language-Recognition.

</details>


### [67] [What Can We Learn from Inter-Annotator Variability in Skin Lesion Segmentation?](https://arxiv.org/abs/2508.09381)
*Kumar Abhishek,Jeremy Kawahara,Ghassan Hamarneh*

Main category: cs.CV

TL;DR: 创建了一个皮肤病变分割数据集，研究了注释者间差异与病变恶性程度的关系，并利用注释者间协议提高了分割精度。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割由于对象边界模糊、注释者偏好、专业知识和工具等因素而表现出注释者内部和注释者之间的差异。具有模糊边界的病变，例如，小刺或浸润性结节，或根据ABCD规则的不规则边界，特别容易产生分歧，并且通常与恶性肿瘤相关。

Method: 策划了最大的多注释者皮肤病变分割数据集IMA++，并对注释者、恶性肿瘤、工具和技能因素引起的变异性进行了深入研究。

Result: 发现使用Dice测量的注释者间协议(IAA)与皮肤病变的恶性程度之间存在统计学上的显著相关性(p<0.001)。我们进一步表明，可以直接从皮肤镜图像准确地预测IAA，平均绝对误差为0.108。

Conclusion: 利用IAA作为“软”临床特征，在多任务学习目标中，平均跨多个模型架构以及IMA++和四个公共皮肤镜数据集的平衡精度提高了4.2%。

Abstract: Medical image segmentation exhibits intra- and inter-annotator variability
due to ambiguous object boundaries, annotator preferences, expertise, and
tools, among other factors. Lesions with ambiguous boundaries, e.g., spiculated
or infiltrative nodules, or irregular borders per the ABCD rule, are
particularly prone to disagreement and are often associated with malignancy. In
this work, we curate IMA++, the largest multi-annotator skin lesion
segmentation dataset, on which we conduct an in-depth study of variability due
to annotator, malignancy, tool, and skill factors. We find a statistically
significant (p<0.001) association between inter-annotator agreement (IAA),
measured using Dice, and the malignancy of skin lesions. We further show that
IAA can be accurately predicted directly from dermoscopic images, achieving a
mean absolute error of 0.108. Finally, we leverage this association by
utilizing IAA as a "soft" clinical feature within a multi-task learning
objective, yielding a 4.2% improvement in balanced accuracy averaged across
multiple model architectures and across IMA++ and four public dermoscopic
datasets. The code is available at https://github.com/sfu-mial/skin-IAV.

</details>


### [68] [X-UniMotion: Animating Human Images with Expressive, Unified and Identity-Agnostic Motion Latents](https://arxiv.org/abs/2508.09383)
*Guoxian Song,Hongyi Xu,Xiaochen Zhao,You Xie,Tianpei Gu,Zenan Li,Chenxu Zhang,Linjie Luo*

Main category: cs.CV

TL;DR: X-UniMotion: a unified implicit latent representation for whole-body human motion, encompassing facial expressions, body poses, and hand gestures.


<details>
  <summary>Details</summary>
Motivation: encode multi-granular motion directly from a single image into a compact set of four disentangled latent tokens -- one for facial expression, one for body pose, and one for each hand. These motion latents are both highly expressive and identity-agnostic, enabling high-fidelity, detailed cross-identity motion transfer across subjects with diverse identities, poses, and spatial configurations.

Method: a self-supervised, end-to-end framework that jointly learns the motion encoder and latent representation alongside a DiT-based video generative model

Result: high-fidelity, detailed cross-identity motion transfer across subjects with diverse identities, poses, and spatial configurations.

Conclusion: X-UniMotion outperforms state-of-the-art methods, producing highly expressive animations with superior motion fidelity and identity preservation.

Abstract: We present X-UniMotion, a unified and expressive implicit latent
representation for whole-body human motion, encompassing facial expressions,
body poses, and hand gestures. Unlike prior motion transfer methods that rely
on explicit skeletal poses and heuristic cross-identity adjustments, our
approach encodes multi-granular motion directly from a single image into a
compact set of four disentangled latent tokens -- one for facial expression,
one for body pose, and one for each hand. These motion latents are both highly
expressive and identity-agnostic, enabling high-fidelity, detailed
cross-identity motion transfer across subjects with diverse identities, poses,
and spatial configurations. To achieve this, we introduce a self-supervised,
end-to-end framework that jointly learns the motion encoder and latent
representation alongside a DiT-based video generative model, trained on
large-scale, diverse human motion datasets. Motion-identity disentanglement is
enforced via 2D spatial and color augmentations, as well as synthetic 3D
renderings of cross-identity subject pairs under shared poses. Furthermore, we
guide motion token learning with auxiliary decoders that promote fine-grained,
semantically aligned, and depth-aware motion embeddings. Extensive experiments
show that X-UniMotion outperforms state-of-the-art methods, producing highly
expressive animations with superior motion fidelity and identity preservation.

</details>


### [69] [DenoDet V2: Phase-Amplitude Cross Denoising for SAR Object Detection](https://arxiv.org/abs/2508.09392)
*Kang Ni,Minrui Zou,Yuxuan Li,Xiang Li,Kehua Guo,Ming-Ming Cheng,Yimian Dai*

Main category: cs.CV

TL;DR: DenoDet V2是一种用于SAR目标检测的新方法，它在变换域中解构和调制特征，利用幅度和相位信息的互补性质，实现了最先进的性能，同时降低了模型复杂度。


<details>
  <summary>Details</summary>
Motivation: 合成孔径雷达(SAR)目标检测的主要挑战之一在于相干噪声的普遍影响。大多数现有方法，无论是手工方法还是基于深度学习的方法，都采用分析或增强物体空间域特征来实现隐式去噪。

Method: 提出了一种新颖的DenoDet V2，它通过精心设计的注意力架构在变换域中解构和调制特征，利用幅度和相位信息的互补性质，通过带状互调制机制实现相位谱和幅度谱之间的相互增强。

Result: DenoDet V2在SARDet-100K数据集上比DenoDet V1取得了显著的0.8%的改进，同时模型复杂度降低了一半。

Conclusion: DenoDet V2在各种SAR数据集上展示了最先进的性能，并在SARDet-100K数据集上比DenoDet V1取得了显著的0.8%的改进，同时模型复杂度降低了一半。

Abstract: One of the primary challenges in Synthetic Aperture Radar (SAR) object
detection lies in the pervasive influence of coherent noise. As a common
practice, most existing methods, whether handcrafted approaches or deep
learning-based methods, employ the analysis or enhancement of object
spatial-domain characteristics to achieve implicit denoising. In this paper, we
propose DenoDet V2, which explores a completely novel and different perspective
to deconstruct and modulate the features in the transform domain via a
carefully designed attention architecture. Compared to DenoDet V1, DenoDet V2
is a major advancement that exploits the complementary nature of amplitude and
phase information through a band-wise mutual modulation mechanism, which
enables a reciprocal enhancement between phase and amplitude spectra. Extensive
experiments on various SAR datasets demonstrate the state-of-the-art
performance of DenoDet V2. Notably, DenoDet V2 achieves a significant 0.8\%
improvement on SARDet-100K dataset compared to DenoDet V1, while reducing the
model complexity by half. The code is available at
https://github.com/GrokCV/GrokSAR.

</details>


### [70] [Skyshield: Event-Driven Submillimetre Thin Obstacle Detection for Drone Flight Safety](https://arxiv.org/abs/2508.09397)
*Zhengli Zhang,Xinyu Luo,Yuchen Sun,Wenhua Ding,Dongyu Huang,Xinlei Chen*

Main category: cs.CV

TL;DR: SkyShield is an event-driven framework for perceiving submillimeter-scale obstacles, using a U-Net architecture and Dice-Contour Regularization Loss for precise detection, achieving a mean F1 Score of 0.7088 with 21.2 ms latency.


<details>
  <summary>Details</summary>
Motivation: Drones operating in complex environments face a significant threat from thin obstacles, such as steel wires and kite strings at the submillimeter level, which are notoriously difficult for conventional sensors like RGB cameras, LiDAR, and depth cameras to detect.

Method: employs a lightweight U-Net architecture and an innovative Dice-Contour Regularization Loss

Result: achieves mean F1 Score of 0.7088 with a low latency of 21.2 ms

Conclusion: The event-based approach achieves a mean F1 Score of 0.7088 with a low latency of 21.2 ms, making it ideal for deployment on edge and mobile platforms.

Abstract: Drones operating in complex environments face a significant threat from thin
obstacles, such as steel wires and kite strings at the submillimeter level,
which are notoriously difficult for conventional sensors like RGB cameras,
LiDAR, and depth cameras to detect. This paper introduces SkyShield, an
event-driven, end-to-end framework designed for the perception of submillimeter
scale obstacles. Drawing upon the unique features that thin obstacles present
in the event stream, our method employs a lightweight U-Net architecture and an
innovative Dice-Contour Regularization Loss to ensure precise detection.
Experimental results demonstrate that our event-based approach achieves mean F1
Score of 0.7088 with a low latency of 21.2 ms, making it ideal for deployment
on edge and mobile platforms.

</details>


### [71] [Autonomous AI Bird Feeder for Backyard Biodiversity Monitoring](https://arxiv.org/abs/2508.09398)
*El Mustapha Mansouri*

Main category: cs.CV

TL;DR: 本文提出了一种低成本、本地的鸟类监测系统，该系统在商品硬件上运行，无需离散 GPU，从而保护隐私并避免云费用。


<details>
  <summary>Details</summary>
Motivation: 本文介绍了一种低成本的本地系统，用于在比利时城市花园中进行自主后院鸟类监测。

Method: 使用 Detectron2 定位鸟类；裁剪区域然后由在从更大的 Kaggle 语料库导出的 40 个物种的比利时子集上微调的 EfficientNet-B3 模型进行分类。

Result: 检测器引导的裁剪提高了分类精度，优于原始帧分类。

Conclusion: 该分类器在精选子集上获得了较高的验证性能（约 99.5%），并在预留物种上提供了实际的现场准确度（前 1 名约 88%），证明了在家中进行公民科学级生物多样性记录的可行性。

Abstract: This paper presents a low cost, on premise system for autonomous backyard
bird monitoring in Belgian urban gardens. A motion triggered IP camera uploads
short clips via FTP to a local server, where frames are sampled and birds are
localized with Detectron2; cropped regions are then classified by an
EfficientNet-B3 model fine tuned on a 40-species Belgian subset derived from a
larger Kaggle corpus. All processing runs on commodity hardware without a
discrete GPU, preserving privacy and avoiding cloud fees. The physical feeder
uses small entry ports (30 mm) to exclude pigeons and reduce nuisance triggers.
Detector-guided cropping improves classification accuracy over raw-frame
classification. The classifier attains high validation performance on the
curated subset (about 99.5 percent) and delivers practical field accuracy
(top-1 about 88 percent) on held-out species, demonstrating feasibility for
citizen-science-grade biodiversity logging at home.

</details>


### [72] [Waymo-3DSkelMo: A Multi-Agent 3D Skeletal Motion Dataset for Pedestrian Interaction Modeling in Autonomous Driving](https://arxiv.org/abs/2508.09404)
*Guangxun Zhu,Shiyu Fan,Hang Dai,Edmond S. L. Ho*

Main category: cs.CV

TL;DR: Waymo-3DSkelMo: A large-scale, high-quality 3D motion dataset for pedestrian interaction understanding in autonomous driving.


<details>
  <summary>Details</summary>
Motivation: Existing datasets mostly rely on estimating 3D poses from monocular RGB video frames, which suffer from occlusion and lack of temporal continuity, thus resulting in unrealistic and low-quality human motion.

Method: utilize 3D human body shape and motion priors to enhance the quality of the 3D pose sequences extracted from the raw LiDRA point clouds

Result: The dataset covers over 14,000 seconds across more than 800 real driving scenarios, including rich interactions among an average of 27 agents per scene (with up to 250 agents in the largest scene). Furthermore, we establish 3D pose forecasting benchmarks under varying pedestrian densities

Conclusion: This paper introduces Waymo-3DSkelMo, a large-scale dataset providing high-quality, temporally coherent 3D skeletal motions with explicit interaction semantics. It establishes 3D pose forecasting benchmarks and demonstrates its value as a foundational resource for future research on fine-grained human behavior understanding in complex urban environments. The dataset and code will be available at https://github.com/GuangxunZhu/Waymo-3DSkelMo

Abstract: Large-scale high-quality 3D motion datasets with multi-person interactions
are crucial for data-driven models in autonomous driving to achieve
fine-grained pedestrian interaction understanding in dynamic urban
environments. However, existing datasets mostly rely on estimating 3D poses
from monocular RGB video frames, which suffer from occlusion and lack of
temporal continuity, thus resulting in unrealistic and low-quality human
motion. In this paper, we introduce Waymo-3DSkelMo, the first large-scale
dataset providing high-quality, temporally coherent 3D skeletal motions with
explicit interaction semantics, derived from the Waymo Perception dataset. Our
key insight is to utilize 3D human body shape and motion priors to enhance the
quality of the 3D pose sequences extracted from the raw LiDRA point clouds. The
dataset covers over 14,000 seconds across more than 800 real driving scenarios,
including rich interactions among an average of 27 agents per scene (with up to
250 agents in the largest scene). Furthermore, we establish 3D pose forecasting
benchmarks under varying pedestrian densities, and the results demonstrate its
value as a foundational resource for future research on fine-grained human
behavior understanding in complex urban environments. The dataset and code will
be available at https://github.com/GuangxunZhu/Waymo-3DSkelMo

</details>


### [73] [RampNet: A Two-Stage Pipeline for Bootstrapping Curb Ramp Detection in Streetscape Images from Open Government Metadata](https://arxiv.org/abs/2508.09415)
*John S. O'Meara,Jared Hwang,Zeyu Wang,Michael Saugstad,Jon E. Froehlich*

Main category: cs.CV

TL;DR: This paper introduces RampNet, a two-stage pipeline for curb ramp detection. It generates a large-scale dataset from Google Street View panoramas and trains a detection model that achieves state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Robustly detecting curb ramps in images remains an open problem due to the lack of large-scale, high-quality datasets. Prior work has attempted to improve data availability with crowdsourced or manually labeled data, but these efforts often fall short in either quality or scale.

Method: The authors introduce and evaluate a two-stage pipeline called RampNet to scale curb ramp detection datasets and improve model performance. In Stage 1, they generate a dataset of more than 210,000 annotated Google Street View (GSV) panoramas by auto-translating government-provided curb ramp location data to pixel coordinates in panoramic images. In Stage 2, they train a curb ramp detection model (modified ConvNeXt V2) from the generated dataset.

Result: The generated dataset achieves 94.0% precision and 92.5% recall, and the detection model reaches 0.9236 AP.

Conclusion: The authors' detection model reaches 0.9236 AP, far exceeding prior work. The work contributes the first large-scale, high-quality curb ramp detection dataset, benchmark, and model.

Abstract: Curb ramps are critical for urban accessibility, but robustly detecting them
in images remains an open problem due to the lack of large-scale, high-quality
datasets. While prior work has attempted to improve data availability with
crowdsourced or manually labeled data, these efforts often fall short in either
quality or scale. In this paper, we introduce and evaluate a two-stage pipeline
called RampNet to scale curb ramp detection datasets and improve model
performance. In Stage 1, we generate a dataset of more than 210,000 annotated
Google Street View (GSV) panoramas by auto-translating government-provided curb
ramp location data to pixel coordinates in panoramic images. In Stage 2, we
train a curb ramp detection model (modified ConvNeXt V2) from the generated
dataset, achieving state-of-the-art performance. To evaluate both stages of our
pipeline, we compare to manually labeled panoramas. Our generated dataset
achieves 94.0% precision and 92.5% recall, and our detection model reaches
0.9236 AP -- far exceeding prior work. Our work contributes the first
large-scale, high-quality curb ramp detection dataset, benchmark, and model.

</details>


### [74] [Distilling LLM Prior to Flow Model for Generalizable Agent's Imagination in Object Goal Navigation](https://arxiv.org/abs/2508.09423)
*Badi Li,Ren-jie Lu,Yu Zhou,Jingke Meng,Wei-shi Zheng*

Main category: cs.CV

TL;DR: GOAL是一种新的生成框架，它利用大型语言模型来帮助代理在看不见的环境中找到指定的对象，优于以前的方法。


<details>
  <summary>Details</summary>
Motivation: 先前的方法依赖于确定性和判别模型来完成语义地图，忽略了室内布局中固有的不确定性，并限制了它们泛化到看不见的环境的能力。

Method: 我们提出了GOAL，一个基于生成流的框架，通过使用LLM增强的完整场景语义地图桥接观察到的区域来建模室内环境的语义分布。

Result: GOAL在MP3D和Gibson上实现了最先进的性能，并在转移到HM3D的转移设置中显示出强大的泛化能力。

Conclusion: GOAL在MP3D和Gibson上实现了最先进的性能，并在转移到HM3D的转移设置中显示出强大的泛化能力。

Abstract: The Object Goal Navigation (ObjectNav) task challenges agents to locate a
specified object in an unseen environment by imagining unobserved regions of
the scene. Prior approaches rely on deterministic and discriminative models to
complete semantic maps, overlooking the inherent uncertainty in indoor layouts
and limiting their ability to generalize to unseen environments. In this work,
we propose GOAL, a generative flow-based framework that models the semantic
distribution of indoor environments by bridging observed regions with
LLM-enriched full-scene semantic maps. During training, spatial priors inferred
from large language models (LLMs) are encoded as two-dimensional Gaussian
fields and injected into target maps, distilling rich contextual knowledge into
the flow model and enabling more generalizable completions. Extensive
experiments demonstrate that GOAL achieves state-of-the-art performance on MP3D
and Gibson, and shows strong generalization in transfer settings to HM3D. Codes
and pretrained models are available at https://github.com/Badi-Li/GOAL.

</details>


### [75] [What-Meets-Where: Unified Learning of Action and Contact Localization in a New Dataset](https://arxiv.org/abs/2508.09428)
*Yuxiao Wang,Yu Lei,Wolin Liang,Weiying Xue,Zhenao Wei,Nan Zhuang,Qi Liu*

Main category: cs.CV

TL;DR: 提出了一种新的视觉任务，可以同时预测高级动作语义和细粒度的身体部位接触区域。


<details>
  <summary>Details</summary>
Motivation: 当前的方法通常不能充分捕捉这种二元性，通常不能联合建模动作语义及其在场景中的空间语境化。

Method: PaIR-Net，包含三个关键组件：用于识别接触相关身体部位的 Contact Prior Aware Module (CPAM)、用于像素级接触分割的 Prior-Guided Concat Segmenter (PGCS) 以及负责整合全局交互关系的 Interaction Inference Module (IIM)。

Result: PaIR-Net 显著优于基线方法，并且消融研究证实了每个架构组件的有效性。

Conclusion: PaIR-Net 显著优于基线方法，并且消融研究证实了每个架构组件的有效性。

Abstract: People control their bodies to establish contact with the environment. To
comprehensively understand actions across diverse visual contexts, it is
essential to simultaneously consider \textbf{what} action is occurring and
\textbf{where} it is happening. Current methodologies, however, often
inadequately capture this duality, typically failing to jointly model both
action semantics and their spatial contextualization within scenes. To bridge
this gap, we introduce a novel vision task that simultaneously predicts
high-level action semantics and fine-grained body-part contact regions. Our
proposed framework, PaIR-Net, comprises three key components: the Contact Prior
Aware Module (CPAM) for identifying contact-relevant body parts, the
Prior-Guided Concat Segmenter (PGCS) for pixel-wise contact segmentation, and
the Interaction Inference Module (IIM) responsible for integrating global
interaction relationships. To facilitate this task, we present PaIR (Part-aware
Interaction Representation), a comprehensive dataset containing 13,979 images
that encompass 654 actions, 80 object categories, and 17 body parts.
Experimental evaluation demonstrates that PaIR-Net significantly outperforms
baseline approaches, while ablation studies confirm the efficacy of each
architectural component. The code and dataset will be released upon
publication.

</details>


### [76] [MPT: Motion Prompt Tuning for Micro-Expression Recognition](https://arxiv.org/abs/2508.09446)
*Jiateng Liu,Hengcan Shi,Feng Chen,Zhiwen Shao,Yaonan Wang,Jianfei Cai,Wenming Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种名为运动提示调整（MPT）的新方法，用于调整LM以进行MER，该方法通过运动放大和高斯令牌化提取细微运动作为提示，并设计了一个组适配器以增强LM在目标MER领域的能力。实验结果表明，MPT在三个广泛使用的MER数据集上始终优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 微表情识别（MER）在情感计算领域至关重要，但由于ME数据集通常缺乏训练样本，严重限制了MER模型的学习。当前的大型预训练模型（LM）虽然提供了通用和区分性的表示，但它们无法捕捉到短暂和微妙的面部运动，这对于有效的MER至关重要。

Method: 提出了运动提示调整（MPT）作为一种将LM用于MER的新方法，包括运动放大和高斯令牌化，以提取细微的运动作为LM的提示。此外，还设计了一个组适配器并将其插入LM中，以增强其在目标MER领域的能力。

Result: 在三个广泛使用的MER数据集上进行了大量实验，表明我们提出的MPT始终超越了最先进的方法，并验证了其有效性。

Conclusion: 提出的MPT方法在三个广泛使用的MER数据集上超过了现有技术水平，验证了其有效性。

Abstract: Micro-expression recognition (MER) is crucial in the affective computing
field due to its wide application in medical diagnosis, lie detection, and
criminal investigation. Despite its significance, obtaining micro-expression
(ME) annotations is challenging due to the expertise required from
psychological professionals. Consequently, ME datasets often suffer from a
scarcity of training samples, severely constraining the learning of MER models.
While current large pre-training models (LMs) offer general and discriminative
representations, their direct application to MER is hindered by an inability to
capture transitory and subtle facial movements-essential elements for effective
MER. This paper introduces Motion Prompt Tuning (MPT) as a novel approach to
adapting LMs for MER, representing a pioneering method for subtle motion prompt
tuning. Particularly, we introduce motion prompt generation, including motion
magnification and Gaussian tokenization, to extract subtle motions as prompts
for LMs. Additionally, a group adapter is carefully designed and inserted into
the LM to enhance it in the target MER domain, facilitating a more nuanced
distinction of ME representation. Furthermore, extensive experiments conducted
on three widely used MER datasets demonstrate that our proposed MPT
consistently surpasses state-of-the-art approaches and verifies its
effectiveness.

</details>


### [77] [RASR: Retrieval-Augmented Super Resolution for Practical Reference-based Image Restoration](https://arxiv.org/abs/2508.09449)
*Jiaqi Yan,Shuning Xu,Xiangyu Chen,Dell Zhang,Jie Tang,Gangshan Wu,Jie Liu*

Main category: cs.CV

TL;DR: This paper introduces Retrieval-Augmented Super Resolution (RASR) to address the limitations of existing RefSR approaches. It also introduces RASR-Flickr30, the first benchmark dataset designed for RASR. 


<details>
  <summary>Details</summary>
Motivation: A critical limitation of existing RefSR approaches is their reliance on manually curated target-reference image pairs, which severely constrains their practicality in real-world scenarios.

Method: We introduce Retrieval-Augmented Super Resolution (RASR), a new and practical RefSR paradigm that automatically retrieves semantically relevant high-resolution images from a reference database given only a low-quality input. We further propose RASRNet, a strong baseline that combines a semantic reference retriever with a diffusion-based RefSR generator.

Result: Experiments on RASR-Flickr30 demonstrate that RASRNet consistently improves over SISR baselines, achieving +0.38 dB PSNR and -0.0131 LPIPS, while generating more realistic textures.

Conclusion: Retrieval augmentation is a promising direction to bridge the gap between academic RefSR research and real-world applicability.

Abstract: Reference-based Super Resolution (RefSR) improves upon Single Image Super
Resolution (SISR) by leveraging high-quality reference images to enhance
texture fidelity and visual realism. However, a critical limitation of existing
RefSR approaches is their reliance on manually curated target-reference image
pairs, which severely constrains their practicality in real-world scenarios. To
overcome this, we introduce Retrieval-Augmented Super Resolution (RASR), a new
and practical RefSR paradigm that automatically retrieves semantically relevant
high-resolution images from a reference database given only a low-quality
input. This enables scalable and flexible RefSR in realistic use cases, such as
enhancing mobile photos taken in environments like zoos or museums, where
category-specific reference data (e.g., animals, artworks) can be readily
collected or pre-curated. To facilitate research in this direction, we
construct RASR-Flickr30, the first benchmark dataset designed for RASR. Unlike
prior datasets with fixed target-reference pairs, RASR-Flickr30 provides
per-category reference databases to support open-world retrieval. We further
propose RASRNet, a strong baseline that combines a semantic reference retriever
with a diffusion-based RefSR generator. It retrieves relevant references based
on semantic similarity and employs a diffusion-based generator enhanced with
semantic conditioning. Experiments on RASR-Flickr30 demonstrate that RASRNet
consistently improves over SISR baselines, achieving +0.38 dB PSNR and -0.0131
LPIPS, while generating more realistic textures. These findings highlight
retrieval augmentation as a promising direction to bridge the gap between
academic RefSR research and real-world applicability.

</details>


### [78] [HyperKD: Distilling Cross-Spectral Knowledge in Masked Autoencoders via Inverse Domain Shift with Spatial-Aware Masking and Specialized Loss](https://arxiv.org/abs/2508.09453)
*Abdul Matin,Tanjim Bin Faruk,Shrideep Pallickara,Sangmi Lee Pallickara*

Main category: cs.CV

TL;DR: HyperKD 是一种知识蒸馏框架，它通过从 Prithvi 基础模型中提取知识到为 EnMAP 高光谱图像定制的学生模型中，从而在高光谱图像上有效开发基础模型。


<details>
  <summary>Details</summary>
Motivation: 基础模型在大型未标记数据集上进行预训练，已成为创建适应性强且可重用架构的有效方法，这些架构可用于使用卫星观测的各种下游任务。然而，由于固有的光谱差异和可用观测的稀缺性，它们直接应用于高光谱遥感仍然具有挑战性。

Method: 提出了一种新颖的知识蒸馏框架 HyperKD，该框架能够将从教师模型中学习到的表征转移到学生模型中，从而有效开发高光谱图像的基础模型。HyperKD 建立在 Masked Autoencoder 的基础上，将 Prithvi 基础模型中的知识提炼到为 EnMAP 高光谱图像定制的学生模型中。HyperKD 通过引入基于特征的策略来解决具有光谱间隙的逆域适应问题，该策略包括基于光谱范围的通道对齐、空间特征引导的掩蔽和专为高光谱图像定制的增强损失函数。

Result: HyperKD 解决了具有光谱间隙的逆域适应问题，通过引入基于特征的策略，实现了预训练基础模型在地理空间应用中的有效利用。大量实验表明，

Conclusion: HyperKD 显著提高了 MAE 中的表征学习，从而在下游任务（如土地覆盖分类、作物类型识别和土壤有机碳预测）中实现了更高的重建保真度和更强大的性能，巩固了知识蒸馏框架在具有高光谱图像的遥感分析中的潜力。

Abstract: The proliferation of foundation models, pretrained on large-scale unlabeled
datasets, has emerged as an effective approach in creating adaptable and
reusable architectures that can be leveraged for various downstream tasks using
satellite observations. However, their direct application to hyperspectral
remote sensing remains challenging due to inherent spectral disparities and the
scarcity of available observations. In this work, we present HyperKD, a novel
knowledge distillation framework that enables transferring learned
representations from a teacher model into a student model for effective
development of a foundation model on hyperspectral images. Unlike typical
knowledge distillation frameworks, which use a complex teacher to guide a
simpler student, HyperKD enables an inverse form of knowledge transfer across
different types of spectral data, guided by a simpler teacher model. Building
upon a Masked Autoencoder, HyperKD distills knowledge from the Prithvi
foundational model into a student tailored for EnMAP hyperspectral imagery.
HyperKD addresses the inverse domain adaptation problem with spectral gaps by
introducing a feature-based strategy that includes spectral range-based channel
alignment, spatial feature-guided masking, and an enhanced loss function
tailored for hyperspectral images. HyperKD bridges the substantial spectral
domain gap, enabling the effective use of pretrained foundation models for
geospatial applications. Extensive experiments show that HyperKD significantly
improves representation learning in MAEs, leading to enhanced reconstruction
fidelity and more robust performance on downstream tasks such as land cover
classification, crop type identification, and soil organic carbon prediction,
underpinning the potential of knowledge distillation frameworks in remote
sensing analytics with hyperspectral imagery.

</details>


### [79] [Animate-X++: Universal Character Image Animation with Dynamic Backgrounds](https://arxiv.org/abs/2508.09454)
*Shuai Tan,Biao Gong,Zhuoxin Liu,Yan Wang,Xi Chen,Yifan Feng,Hengshuang Zhao*

Main category: cs.CV

TL;DR: This paper introduces Animate-X++, a universal animation framework based on DiT for various character types, including anthropomorphic characters and text-driven background dynamics.


<details>
  <summary>Details</summary>
Motivation: most existing methods only apply to human figures, which usually do not generalize well on anthropomorphic characters; previous methods could only generate videos with static backgrounds, which limits the realism of the videos.

Method: proposes Animate-X++, a universal animation framework based on DiT; introduce the Pose Indicator, which captures comprehensive motion pattern from the driving video through both implicit and explicit manner; introduce a multi-task training strategy that jointly trains the animation and TI2V tasks.

Result: achieves not only character animation but also text-driven background dynamics, making the videos more realistic; introduce a new Animated Anthropomorphic Benchmark (A2Bench) to evaluate the performance of Animate-X++ on universal and widely applicable animation images.

Conclusion: Extensive experiments demonstrate the superiority and effectiveness of Animate-X++.

Abstract: Character image animation, which generates high-quality videos from a
reference image and target pose sequence, has seen significant progress in
recent years. However, most existing methods only apply to human figures, which
usually do not generalize well on anthropomorphic characters commonly used in
industries like gaming and entertainment. Furthermore, previous methods could
only generate videos with static backgrounds, which limits the realism of the
videos. For the first challenge, our in-depth analysis suggests to attribute
this limitation to their insufficient modeling of motion, which is unable to
comprehend the movement pattern of the driving video, thus imposing a pose
sequence rigidly onto the target character. To this end, this paper proposes
Animate-X++, a universal animation framework based on DiT for various character
types, including anthropomorphic characters. To enhance motion representation,
we introduce the Pose Indicator, which captures comprehensive motion pattern
from the driving video through both implicit and explicit manner. The former
leverages CLIP visual features of a driving video to extract its gist of
motion, like the overall movement pattern and temporal relations among motions,
while the latter strengthens the generalization of DiT by simulating possible
inputs in advance that may arise during inference. For the second challenge, we
introduce a multi-task training strategy that jointly trains the animation and
TI2V tasks. Combined with the proposed partial parameter training, this
approach achieves not only character animation but also text-driven background
dynamics, making the videos more realistic. Moreover, we introduce a new
Animated Anthropomorphic Benchmark (A2Bench) to evaluate the performance of
Animate-X++ on universal and widely applicable animation images. Extensive
experiments demonstrate the superiority and effectiveness of Animate-X++.

</details>


### [80] [IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding](https://arxiv.org/abs/2508.09456)
*Junxian Li,Beining Xu,Di Zhang*

Main category: cs.CV

TL;DR: This paper introduces IAG, a novel input-aware backdoor attack method for VLMs that manipulates grounding behavior. It uses a text-conditional U-Net to embed semantic information and achieves high attack success rates with minimal impact on clean samples.


<details>
  <summary>Details</summary>
Motivation: Security issues in visual grounding tasks for VLMs remain underexplored, especially in the context of backdoor attacks. The paper aims to address this gap by introducing a novel input-aware backdoor attack method.

Method: The paper introduces a novel input-aware backdoor attack method, IAG, which uses an adaptive trigger generator that embeds the semantic information of the attack target's description into the original image using a text-conditional U-Net. A reconstruction loss is utilized to minimize visual discrepancies between poisoned and clean images. A unified method for generating attack data is also introduced.

Result: The proposed attack forces the model to ground a specific target object in the input image, regardless of the user's query. IAG achieves over 65% ASR@0.5 on InternVL-2.5-8B and shows promising potential on manipulating Ferret-7B and LlaVA-1.5-7B with very little accuracy decrease on clean samples.

Conclusion: IAG is evaluated theoretically and empirically, demonstrating its feasibility and effectiveness. The attack achieves over 65% ASR@0.5 on InternVL-2.5-8B and shows promising potential on manipulating Ferret-7B and LlaVA-1.5-7B with very little accuracy decrease on clean samples. The robustness and transferability of the attack are also demonstrated through ablation studies and potential defense experiments.

Abstract: Vision-language models (VLMs) have shown significant advancements in tasks
such as visual grounding, where they localize specific objects in images based
on natural language queries and images. However, security issues in visual
grounding tasks for VLMs remain underexplored, especially in the context of
backdoor attacks. In this paper, we introduce a novel input-aware backdoor
attack method, IAG, designed to manipulate the grounding behavior of VLMs. This
attack forces the model to ground a specific target object in the input image,
regardless of the user's query. We propose an adaptive trigger generator that
embeds the semantic information of the attack target's description into the
original image using a text-conditional U-Net, thereby overcoming the
open-vocabulary attack challenge. To ensure the attack's stealthiness, we
utilize a reconstruction loss to minimize visual discrepancies between poisoned
and clean images. Additionally, we introduce a unified method for generating
attack data. IAG is evaluated theoretically and empirically, demonstrating its
feasibility and effectiveness. Notably, our ASR@0.5 on InternVL-2.5-8B reaches
over 65\% on various testing sets. IAG also shows promising potential on
manipulating Ferret-7B and LlaVA-1.5-7B with very little accuracy decrease on
clean samples. Extensive specific experiments, such as ablation study and
potential defense, also indicate the robustness and transferability of our
attack.

</details>


### [81] [RelayFormer: A Unified Local-Global Attention Framework for Scalable Image and Video Manipulation Localization](https://arxiv.org/abs/2508.09459)
*Wen Huang,Jiarui Yang,Tao Dai,Jiawei Li,Shaoxiong Zhan,Bin Wang,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 提出了一种统一且模块化的架构 RelayFormer，用于跨图像和视频的视觉篡改定位。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常缺乏跨模态泛化能力，并且难以有效处理高分辨率或长持续时间的输入。

Method: 利用灵活的本地单元和全局-本地中继注意力 (GLoRA) 机制。

Result: 该框架通过轻量级适配模块与现有的基于 Transformer 的骨干网络（例如 ViT 和 SegFormer）无缝集成，这些模块仅需进行最小的架构更改，从而确保兼容性，而不会破坏预训练的表示。

Conclusion: 提出的方法在多个基准测试中实现了最先进的定位性能，为可扩展且模态不可知的 VML 设定了新的基线。

Abstract: Visual manipulation localization (VML) -- across both images and videos -- is
a crucial task in digital forensics that involves identifying tampered regions
in visual content. However, existing methods often lack cross-modal
generalization and struggle to handle high-resolution or long-duration inputs
efficiently.
  We propose RelayFormer, a unified and modular architecture for visual
manipulation localization across images and videos. By leveraging flexible
local units and a Global-Local Relay Attention (GLoRA) mechanism, it enables
scalable, resolution-agnostic processing with strong generalization. Our
framework integrates seamlessly with existing Transformer-based backbones, such
as ViT and SegFormer, via lightweight adaptation modules that require only
minimal architectural changes, ensuring compatibility without disrupting
pretrained representations.
  Furthermore, we design a lightweight, query-based mask decoder that supports
one-shot inference across video sequences with linear complexity. Extensive
experiments across multiple benchmarks demonstrate that our approach achieves
state-of-the-art localization performance, setting a new baseline for scalable
and modality-agnostic VML. Code is available at:
https://github.com/WenOOI/RelayFormer.

</details>


### [82] [Gen-AFFECT: Generation of Avatar Fine-grained Facial Expressions with Consistent identiTy](https://arxiv.org/abs/2508.09461)
*Hao Yu,Rupayan Mallick,Margrit Betke,Sarah Adel Bargal*

Main category: cs.CV

TL;DR: GEN-AFFECT是一种用于个性化头像生成的新颖框架，该框架生成具有各种面部表情的富有表现力且身份一致的头像。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常无法捕捉到细微的面部表情，并且难以在不同的表情中保持身份。

Method: 该框架建议在提取的身份-表情表示上调节多模态扩散Transformer。这能够实现身份保持和各种面部表情的表示。GEN-AFFECT还在推理时采用一致的注意力，以便在生成的表情集合中进行信息共享，从而使生成过程能够在生成的细微表情数组中保持身份一致性。

Result: GEN-AFFECT展示了卓越的性能。

Conclusion: GEN-AFFECT在生成表情的准确性、身份保持以及目标身份在各种细微面部表情中的一致性方面，表现出比以前的最先进方法更优越的性能。

Abstract: Different forms of customized 2D avatars are widely used in gaming
applications, virtual communication, education, and content creation. However,
existing approaches often fail to capture fine-grained facial expressions and
struggle to preserve identity across different expressions. We propose
GEN-AFFECT, a novel framework for personalized avatar generation that generates
expressive and identity-consistent avatars with a diverse set of facial
expressions. Our framework proposes conditioning a multimodal diffusion
transformer on an extracted identity-expression representation. This enables
identity preservation and representation of a wide range of facial expressions.
GEN-AFFECT additionally employs consistent attention at inference for
information sharing across the set of generated expressions, enabling the
generation process to maintain identity consistency over the array of generated
fine-grained expressions. GEN-AFFECT demonstrates superior performance compared
to previous state-of-the-art methods on the basis of the accuracy of the
generated expressions, the preservation of the identity and the consistency of
the target identity across an array of fine-grained facial expressions.

</details>


### [83] [Event-driven Robust Fitting on Neuromorphic Hardware](https://arxiv.org/abs/2508.09466)
*Tam Ngoc-Bang Nguyen,Anh-Dzung Doan,Zhipeng Cai,Tat-Jun Chin*

Main category: cs.CV

TL;DR: This paper introduces a neuromorphic computing approach for robust fitting, achieving significant energy savings compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Energy efficiency in robust fitting is a critical but underexplored area, especially with growing concerns about AI's energy consumption.

Method: A novel spiking neural network is designed for robust fitting on Intel Loihi 2, using event-driven formulations and algorithmic strategies to address hardware limitations.

Result: The neuromorphic approach achieves equivalent accuracy with only 15% of the energy consumption of a standard CPU-based robust fitting algorithm.

Conclusion: The paper presents a neuromorphic robust fitting method that consumes significantly less energy (15%) compared to a standard CPU while maintaining equivalent accuracy.

Abstract: Robust fitting of geometric models is a fundamental task in many computer
vision pipelines. Numerous innovations have been produced on the topic, from
improving the efficiency and accuracy of random sampling heuristics to
generating novel theoretical insights that underpin new approaches with
mathematical guarantees. However, one aspect of robust fitting that has
received little attention is energy efficiency. This performance metric has
become critical as high energy consumption is a growing concern for AI
adoption. In this paper, we explore energy-efficient robust fitting via the
neuromorphic computing paradigm. Specifically, we designed a novel spiking
neural network for robust fitting on real neuromorphic hardware, the Intel
Loihi 2. Enabling this are novel event-driven formulations of model estimation
that allow robust fitting to be implemented in the unique architecture of Loihi
2, and algorithmic strategies to alleviate the current limited precision and
instruction set of the hardware. Results show that our neuromorphic robust
fitting consumes only a fraction (15%) of the energy required to run the
established robust fitting algorithm on a standard CPU to equivalent accuracy.

</details>


### [84] [CitySeg: A 3D Open Vocabulary Semantic Segmentation Foundation Model in City-scale Scenarios](https://arxiv.org/abs/2508.09470)
*Jialei Xu,Zizhuang Wei,Weikang You,Linyun Li,Weijian Sun*

Main category: cs.CV

TL;DR: CitySeg是一个用于城市级点云语义分割的基础模型，它通过结合文本模态、定制数据预处理、局部-全局交叉注意力网络、分层分类策略和两阶段训练策略，实现了最先进的性能和零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 现有的模型经常受到 3D 数据规模和数据集之间域差距的限制，这导致泛化能力下降。为了解决这些挑战，我们提出了 CitySeg。

Method: 提出了CitySeg，一个用于城市级点云语义分割的基础模型，结合了文本模态以实现开放词汇分割和零样本推理。定制了数据预处理规则，并提出了一个局部-全局交叉注意力网络，以增强点网络在无人机场景中的感知能力。引入了一种分层分类策略，并使用图编码器来建模类别之间的分层关系。采用了一种两阶段训练策略，并使用 hinge loss 来增加子类别的特征可分离性。

Result: CitySeg 在九个封闭数据集上实现了最先进的性能，显著优于现有方法。此外，CitySeg 首次实现了在城市级点云场景中的零样本泛化，而无需依赖视觉信息。

Conclusion: CitySeg在九个封闭数据集上实现了最先进的性能，并首次实现了城市级点云场景中的零样本泛化。

Abstract: Semantic segmentation of city-scale point clouds is a critical technology for
Unmanned Aerial Vehicle (UAV) perception systems, enabling the classification
of 3D points without relying on any visual information to achieve comprehensive
3D understanding. However, existing models are frequently constrained by the
limited scale of 3D data and the domain gap between datasets, which lead to
reduced generalization capability. To address these challenges, we propose
CitySeg, a foundation model for city-scale point cloud semantic segmentation
that incorporates text modality to achieve open vocabulary segmentation and
zero-shot inference. Specifically, in order to mitigate the issue of
non-uniform data distribution across multiple domains, we customize the data
preprocessing rules, and propose a local-global cross-attention network to
enhance the perception capabilities of point networks in UAV scenarios. To
resolve semantic label discrepancies across datasets, we introduce a
hierarchical classification strategy. A hierarchical graph established
according to the data annotation rules consolidates the data labels, and the
graph encoder is used to model the hierarchical relationships between
categories. In addition, we propose a two-stage training strategy and employ
hinge loss to increase the feature separability of subcategories. Experimental
results demonstrate that the proposed CitySeg achieves state-of-the-art (SOTA)
performance on nine closed-set benchmarks, significantly outperforming existing
approaches. Moreover, for the first time, CitySeg enables zero-shot
generalization in city-scale point cloud scenarios without relying on visual
information.

</details>


### [85] [Leveraging Failed Samples: A Few-Shot and Training-Free Framework for Generalized Deepfake Detection](https://arxiv.org/abs/2508.09475)
*Shibo Yao,Renshuai Tao,Xiaolong Zheng,Chao Liang,Chunjie Zhang*

Main category: cs.CV

TL;DR: FTNet: A few-shot training-free network for real-world deepfake detection that leverages a single fake sample for improved performance.


<details>
  <summary>Details</summary>
Motivation: Deepfake detection models often perform poorly on unknown samples in real-world scenarios, highlighting the need for a few-shot approach.

Method: The Few-shot Training-free Network (FTNet) is proposed. It uses only one fake sample from an evaluation set and classifies test samples based on the category of the nearest sample.

Result: Achieved a new state-of-the-art performance with an average improvement of 8.7% compared to existing methods across 29 different generative models.

Conclusion: This work introduces a new perspective on real-world deepfake detection: leveraging the failed samples leads to better performance when the model struggles to generalize on a few-shot sample.

Abstract: Recent deepfake detection studies often treat unseen sample detection as a
``zero-shot" task, training on images generated by known models but
generalizing to unknown ones. A key real-world challenge arises when a model
performs poorly on unknown samples, yet these samples remain available for
analysis. This highlights that it should be approached as a ``few-shot" task,
where effectively utilizing a small number of samples can lead to significant
improvement. Unlike typical few-shot tasks focused on semantic understanding,
deepfake detection prioritizes image realism, which closely mirrors real-world
distributions. In this work, we propose the Few-shot Training-free Network
(FTNet) for real-world few-shot deepfake detection. Simple yet effective, FTNet
differs from traditional methods that rely on large-scale known data for
training. Instead, FTNet uses only one fake samplefrom an evaluation set,
mimicking the scenario where new samples emerge in the real world and can be
gathered for use, without any training or parameter updates. During evaluation,
each test sample is compared to the known fake and real samples, and it is
classified based on the category of the nearest sample. We conduct a
comprehensive analysis of AI-generated images from 29 different generative
models and achieve a new SoTA performance, with an average improvement of 8.7\%
compared to existing methods. This work introduces a fresh perspective on
real-world deepfake detection: when the model struggles to generalize on a
few-shot sample, leveraging the failed samples leads to better performance.

</details>


### [86] [From Large Angles to Consistent Faces: Identity-Preserving Video Generation via Mixture of Facial Experts](https://arxiv.org/abs/2508.09476)
*Yuji Wang,Moran Li,Xiaobin Hu,Ran Yi,Jiangning Zhang,Chengming Xu,Weijian Cao,Yabiao Wang,Chengjie Wang,Lizhuang Ma*

Main category: cs.CV

TL;DR: 提出了MoFE来解决视频生成模型在大人脸角度下难以保持身份的问题，并构建了一个LFA数据集。


<details>
  <summary>Details</summary>
Motivation: 当前的视频生成模型在大的面部角度下难以保持身份，主要面临两个挑战：难以探索将身份特征整合到DiT结构中的有效机制，以及现有的开源视频数据集中缺乏对大人脸角度的针对性覆盖。

Method: 提出了混合面部专家（MoFE），动态结合来自三个专业专家的互补线索。身份专家捕捉跨姿势身份敏感特征，语义专家提取高级视觉语义，细节专家保留像素级特征。此外，还定制了一个以面部约束和身份一致性为中心的数据处理流程。

Result: 在LFA基准测试上的实验结果表明，该方法在人脸相似度、人脸FID和CLIP语义对齐方面显著优于先前的SOTA方法。

Conclusion: 该方法在人脸相似度、人脸FID和CLIP语义对齐方面显著优于先前的SOTA方法。

Abstract: Current video generation models struggle with identity preservation under
large facial angles, primarily facing two challenges: the difficulty in
exploring an effective mechanism to integrate identity features into DiT
structure, and the lack of targeted coverage of large facial angles in existing
open-source video datasets. To address these, we present two key innovations.
First, we introduce a Mixture of Facial Experts (MoFE) that dynamically
combines complementary cues from three specialized experts, each designed to
capture distinct but mutually reinforcing aspects of facial attributes. The
identity expert captures cross-pose identity-sensitive features, the semantic
expert extracts high-level visual semantxics, and the detail expert preserves
pixel-level features (e.g., skin texture, color gradients). Furthermore, to
mitigate dataset limitations, we have tailored a data processing pipeline
centered on two key aspects: Face Constraints and Identity Consistency. Face
Constraints ensure facial angle diversity and a high proportion of facial
regions, while Identity Consistency preserves coherent person-specific features
across temporal sequences, collectively addressing the scarcity of large facial
angles and identity-stable training data in existing datasets. Leveraging this
pipeline, we have curated and refined a Large Face Angles (LFA) Dataset from
existing open-source human video datasets, comprising 460K video clips with
annotated facial angles. Experimental results on the LFA benchmark demonstrate
that our method, empowered by the LFA dataset, significantly outperforms prior
SOTA methods in face similarity, face FID, and CLIP semantic alignment. The
code and dataset will be made publicly available at
https://github.com/rain152/LFA-Video-Generation.

</details>


### [87] [CLIP-Flow: A Universal Discriminator for AI-Generated Images Inspired by Anomaly Detection](https://arxiv.org/abs/2508.09477)
*Zhipeng Yuan,Kai Wang,Weize Quan,Dong-Ming Yan,Tieru Wu*

Main category: cs.CV

TL;DR: 提出了一种通用的AI生成图像检测器，通过异常检测和无监督学习来提高检测性能，尤其是在面对来自未见过的生成模型的AIIs时。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成模型的快速发展，AI生成图像（AIIs）的视觉质量越来越接近自然图像，这不可避免地引起了安全问题。大多数AII检测器通常采用传统的图像分类流程，使用自然图像和AIIs（由生成模型生成），这可能导致对来自未见过的生成模型的AIIs的检测性能有限。

Method: 提出了一种基于异常检测的通用AI生成图像检测器。使用预训练的CLIP编码器作为特征提取器，并设计了一个类似归一化流的无监督模型。使用代理图像（例如，通过对自然图像应用频谱修改操作获得）进行训练。通过最小化代理图像的可能性（可选地与最大化自然图像的可能性相结合）来训练模型。

Result: 该方法在各种图像生成器生成的AIIs上表现出有效性。

Conclusion: 该方法在各种图像生成器生成的AIIs上表现出有效性。

Abstract: With the rapid advancement of AI generative models, the visual quality of
AI-generated images (AIIs) has become increasingly close to natural images,
which inevitably raises security concerns. Most AII detectors often employ the
conventional image classification pipeline with natural images and AIIs
(generated by a generative model), which can result in limited detection
performance for AIIs from unseen generative models. To solve this, we proposed
a universal AI-generated image detector from the perspective of anomaly
detection. Our discriminator does not need to access any AIIs and learn a
generalizable representation with unsupervised learning. Specifically, we use
the pre-trained CLIP encoder as the feature extractor and design a normalizing
flow-like unsupervised model. Instead of AIIs, proxy images, e.g., obtained by
applying a spectral modification operation on natural images, are used for
training. Our models are trained by minimizing the likelihood of proxy images,
optionally combined with maximizing the likelihood of natural images. Extensive
experiments demonstrate the effectiveness of our method on AIIs produced by
various image generators.

</details>


### [88] [GazeLT: Visual attention-guided long-tailed disease classification in chest radiographs](https://arxiv.org/abs/2508.09478)
*Moinak Bhattacharya,Gagandeep Singh,Shubham Jain,Prateek Prasanna*

Main category: cs.CV

TL;DR: GazeLT是一种利用人类视觉注意力的时间特性来提高长尾疾病分类性能的方法，并在NIH-CXR-LT和MIMIC-CXR-LT数据集上取得了优异的结果。


<details>
  <summary>Details</summary>
Motivation: 放射科医生的眼动模式包含精细和粗略的疾病相关信息。在图像解读过程中，放射科医生的注意力随时间变化，将其融入深度学习框架至关重要。专家还会关注次要/偶然发现（构成长尾类别），GazeLT利用视觉搜索过程的时间方面，通过整合和分解机制，来改善长尾疾病分类。

Method: GazeLT，一种用于长尾疾病分类的人类视觉注意力整合-分解方法。

Result: GazeLT在两个公开的长尾疾病分类数据集（NIH-CXR-LT和MIMIC-CXR-LT）上表现出有效性。

Conclusion: GazeLT在长尾疾病分类任务中，在NIH-CXR-LT和MIMIC-CXR-LT数据集上，平均准确率分别超过最佳长尾损失4.1%和基于视觉注意力的基线21.7%。

Abstract: In this work, we present GazeLT, a human visual attention
integration-disintegration approach for long-tailed disease classification. A
radiologist's eye gaze has distinct patterns that capture both fine-grained and
coarser level disease related information. While interpreting an image, a
radiologist's attention varies throughout the duration; it is critical to
incorporate this into a deep learning framework to improve automated image
interpretation. Another important aspect of visual attention is that apart from
looking at major/obvious disease patterns, experts also look at
minor/incidental findings (few of these constituting long-tailed classes)
during the course of image interpretation. GazeLT harnesses the temporal aspect
of the visual search process, via an integration and disintegration mechanism,
to improve long-tailed disease classification. We show the efficacy of GazeLT
on two publicly available datasets for long-tailed disease classification,
namely the NIH-CXR-LT (n=89237) and the MIMIC-CXR-LT (n=111898) datasets.
GazeLT outperforms the best long-tailed loss by 4.1% and the visual
attention-based baseline by 21.7% in average accuracy metrics for these
datasets. Our code is available at https://github.com/lordmoinak1/gazelt.

</details>


### [89] [SkySplat: Generalizable 3D Gaussian Splatting from Multi-Temporal Sparse Satellite Images](https://arxiv.org/abs/2508.09479)
*Xuejun Huang,Xinyi Liu,Yi Wan,Zhi Zheng,Bin Zhang,Mingtao Xiong,Yingying Pei,Yongjun Zhang*

Main category: cs.CV

TL;DR: SkySplat, a novel self-supervised framework, integrates the RPC model into the generalizable 3DGS pipeline and achieves great performance.


<details>
  <summary>Details</summary>
Motivation: existing methods remain unsuitable for satellite images due to incompatibility with rational polynomial coefficient (RPC) models and limited generalization capability. Recent advances in generalizable 3DGS approaches show potential, but they perform poorly on multi-temporal sparse satellite images due to limited geometric constraints, transient objects, and radiometric inconsistencies

Method: a novel self-supervised framework that integrates the RPC model into the generalizable 3DGS pipeline, enabling more effective use of sparse geometric cues for improved reconstruction. SkySplat relies only on RGB images and radiometric-robust relative height supervision, thereby eliminating the need for ground-truth height maps. Key components include a Cross-Self Consistency Module (CSCM), which mitigates transient object interference via consistency-based masking, and a multi-view consistency aggregation strategy that refines reconstruction results.

Result: SkySplat achieves an 86 times speedup over EOGS with higher accuracy. It also outperforms generalizable 3DGS baselines, reducing MAE from 13.18 m to 1.80 m on the DFC19 dataset significantly, and demonstrates strong cross-dataset generalization on the MVS3D benchmark.

Conclusion: SkySplat achieves an 86 times speedup over EOGS with higher accuracy and outperforms generalizable 3DGS baselines, reducing MAE from 13.18 m to 1.80 m on the DFC19 dataset significantly, and demonstrates strong cross-dataset generalization on the MVS3D benchmark.

Abstract: Three-dimensional scene reconstruction from sparse-view satellite images is a
long-standing and challenging task. While 3D Gaussian Splatting (3DGS) and its
variants have recently attracted attention for its high efficiency, existing
methods remain unsuitable for satellite images due to incompatibility with
rational polynomial coefficient (RPC) models and limited generalization
capability. Recent advances in generalizable 3DGS approaches show potential,
but they perform poorly on multi-temporal sparse satellite images due to
limited geometric constraints, transient objects, and radiometric
inconsistencies. To address these limitations, we propose SkySplat, a novel
self-supervised framework that integrates the RPC model into the generalizable
3DGS pipeline, enabling more effective use of sparse geometric cues for
improved reconstruction. SkySplat relies only on RGB images and
radiometric-robust relative height supervision, thereby eliminating the need
for ground-truth height maps. Key components include a Cross-Self Consistency
Module (CSCM), which mitigates transient object interference via
consistency-based masking, and a multi-view consistency aggregation strategy
that refines reconstruction results. Compared to per-scene optimization
methods, SkySplat achieves an 86 times speedup over EOGS with higher accuracy.
It also outperforms generalizable 3DGS baselines, reducing MAE from 13.18 m to
1.80 m on the DFC19 dataset significantly, and demonstrates strong
cross-dataset generalization on the MVS3D benchmark.

</details>


### [90] [Episodic Memory Representation for Long-form Video Understanding](https://arxiv.org/abs/2508.09486)
*Yun Wang,Long Zhang,Jingren Liu,Jiaqi Yan,Zhanjie Zhang,Jiahao Zheng,Xun Yang,Dapeng Wu,Xiangyu Chen,Xuelong Li*

Main category: cs.CV

TL;DR: Video-EM, a training-free framework inspired by human episodic memory, improves Video-LLM performance on long-form videos by modeling temporal relationships between keyframes and using chain of thought reasoning.


<details>
  <summary>Details</summary>
Motivation: Video-LLMs struggle with long-form videos due to context window limits, and current keyframe retrieval methods overlook spatio-temporal relationships and may yield redundant keyframes.

Method: A training-free framework called Video-EM that models keyframes as temporally ordered episodic events and uses chain of thought (CoT) thinking with LLMs to identify a minimal subset of episodic memories.

Result: Video-EM demonstrates superior performance on Video-MME, EgoSchema, HourVideo, and LVBench benchmarks.

Conclusion: Video-EM achieves competitive results with 4-9% performance gains over baselines on Video-MME, EgoSchema, HourVideo, and LVBench benchmarks while using fewer frames.

Abstract: Video Large Language Models (Video-LLMs) excel at general video understanding
but struggle with long-form videos due to context window limits. Consequently,
recent approaches focus on keyframe retrieval, condensing lengthy videos into a
small set of informative frames. Despite their practicality, these methods
simplify the problem to static text image matching, overlooking spatio temporal
relationships crucial for capturing scene transitions and contextual
continuity, and may yield redundant keyframes with limited information,
diluting salient cues essential for accurate video question answering. To
address these limitations, we introduce Video-EM, a training free framework
inspired by the principles of human episodic memory, designed to facilitate
robust and contextually grounded reasoning. Rather than treating keyframes as
isolated visual entities, Video-EM explicitly models them as temporally ordered
episodic events, capturing both spatial relationships and temporal dynamics
necessary for accurately reconstructing the underlying narrative. Furthermore,
the framework leverages chain of thought (CoT) thinking with LLMs to
iteratively identify a minimal yet highly informative subset of episodic
memories, enabling efficient and accurate question answering by Video-LLMs.
Extensive evaluations on the Video-MME, EgoSchema, HourVideo, and LVBench
benchmarks confirm the superiority of Video-EM, which achieves highly
competitive results with performance gains of 4-9 percent over respective
baselines while utilizing fewer frames.

</details>


### [91] [SARE: Semantic-Aware Reconstruction Error for Generalizable Diffusion-Generated Image Detection](https://arxiv.org/abs/2508.09487)
*Ju Yeon Kang,Jaehong Park,Semin Kim,Ji Won Yoon,Nam Soo Kim*

Main category: cs.CV

TL;DR: 提出了一种新的假图像检测方法，该方法基于真实图像和假图像在标题引导重建过程中语义变化差异的观察。


<details>
  <summary>Details</summary>
Motivation: 现有的检测方法在面对来自未见过的、分布外(OOD)生成模型的假图像时，性能通常会显著下降，因为它们主要依赖于特定于模型的伪影。为了解决这个限制，我们探索了假图像中常见的一个基本属性。

Method: 提出了一种名为语义感知重建误差(SARE)的新表示，用于测量图像与其标题引导重建之间的语义差异。

Result: 通过量化这些语义变化，SARE可以作为一种判别特征，用于在不同的生成模型中进行鲁棒检测。实验证明了该方法具有很强的泛化能力，优于现有的基线。

Conclusion: 该方法在GenImage和CommunityForensics等基准测试中表现优于现有基线，展示了强大的泛化能力。

Abstract: Recently, diffusion-generated image detection has gained increasing
attention, as the rapid advancement of diffusion models has raised serious
concerns about their potential misuse. While existing detection methods have
achieved promising results, their performance often degrades significantly when
facing fake images from unseen, out-of-distribution (OOD) generative models,
since they primarily rely on model-specific artifacts. To address this
limitation, we explore a fundamental property commonly observed in fake images.
Motivated by the observation that fake images tend to exhibit higher similarity
to their captions than real images, we propose a novel representation, namely
Semantic-Aware Reconstruction Error (SARE), that measures the semantic
difference between an image and its caption-guided reconstruction. The
hypothesis behind SARE is that real images, whose captions often fail to fully
capture their complex visual content, may undergo noticeable semantic shifts
during the caption-guided reconstruction process. In contrast, fake images,
which closely align with their captions, show minimal semantic changes. By
quantifying these semantic shifts, SARE can be utilized as a discriminative
feature for robust detection across diverse generative models. We empirically
demonstrate that the proposed method exhibits strong generalization,
outperforming existing baselines on benchmarks including GenImage and
CommunityForensics.

</details>


### [92] [CWFBind: Geometry-Awareness for Fast and Accurate Protein-Ligand Docking](https://arxiv.org/abs/2508.09499)
*Liyan Jia,Chuan-Xian Ren,Hong Yan*

Main category: cs.CV

TL;DR: CWFBind is a weighted, fast, and accurate docking method based on local curvature features that achieves competitive performance across multiple docking benchmarks, offering a balanced trade-off between accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of binding conformation is critical in rational drug design, but many deep learning-based docking methods neglect critical geometric information, resulting in inaccurate pocket localization and unrealistic binding conformations.

Method: CWFBind, a weighted, fast, and accurate docking method based on local curvature features. It integrates local curvature descriptors, embeds degree-aware weighting mechanisms, and employs a ligand-aware dynamic radius strategy alongside an enhanced loss function.

Result: CWFBind achieves competitive performance across multiple docking benchmarks.

Conclusion: CWFBind achieves competitive performance across multiple docking benchmarks, offering a balanced trade-off between accuracy and efficiency.

Abstract: Accurately predicting the binding conformation of small-molecule ligands to
protein targets is a critical step in rational drug design. Although recent
deep learning-based docking surpasses traditional methods in speed and
accuracy, many approaches rely on graph representations and language
model-inspired encoders while neglecting critical geometric information,
resulting in inaccurate pocket localization and unrealistic binding
conformations. In this study, we introduce CWFBind, a weighted, fast, and
accurate docking method based on local curvature features. Specifically, we
integrate local curvature descriptors during the feature extraction phase to
enrich the geometric representation of both proteins and ligands, complementing
existing chemical, sequence, and structural features. Furthermore, we embed
degree-aware weighting mechanisms into the message passing process, enhancing
the model's ability to capture spatial structural distinctions and interaction
strengths. To address the class imbalance challenge in pocket prediction,
CWFBind employs a ligand-aware dynamic radius strategy alongside an enhanced
loss function, facilitating more precise identification of binding regions and
key residues. Comprehensive experimental evaluations demonstrate that CWFBind
achieves competitive performance across multiple docking benchmarks, offering a
balanced trade-off between accuracy and efficiency.

</details>


### [93] [Generation of Indian Sign Language Letters, Numbers, and Words](https://arxiv.org/abs/2508.09522)
*Ajeet Kumar Yadav,Nishant Kumar,Rathna G N*

Main category: cs.CV

TL;DR: Developed a GAN model for generating high-quality Indian Sign Language images and released a new dataset.


<details>
  <summary>Details</summary>
Motivation: Sign language generation needs exploration for communication between hearing and hard-of-hearing individuals.

Method: A GAN variant combining ProGAN and SAGAN is developed to generate feature-rich, high-resolution, and class-conditional sign language images.

Result: The modified Attention-based model generates high-quality images of Indian Sign Language, outperforming ProGAN in Inception Score (IS) and Fr'echet Inception Distance (FID) with improvements of 3.2 and 30.12, respectively.

Conclusion: A modified Attention-based GAN model generates high-quality Indian Sign Language images, outperforming ProGAN in IS and FID. A large dataset of Indian Sign Language images is also published.

Abstract: Sign language, which contains hand movements, facial expressions and bodily
gestures, is a significant medium for communicating with hard-of-hearing
people. A well-trained sign language community communicates easily, but those
who don't know sign language face significant challenges. Recognition and
generation are basic communication methods between hearing and hard-of-hearing
individuals. Despite progress in recognition, sign language generation still
needs to be explored. The Progressive Growing of Generative Adversarial Network
(ProGAN) excels at producing high-quality images, while the Self-Attention
Generative Adversarial Network (SAGAN) generates feature-rich images at medium
resolutions. Balancing resolution and detail is crucial for sign language image
generation. We are developing a Generative Adversarial Network (GAN) variant
that combines both models to generate feature-rich, high-resolution, and
class-conditional sign language images. Our modified Attention-based model
generates high-quality images of Indian Sign Language letters, numbers, and
words, outperforming the traditional ProGAN in Inception Score (IS) and
Fr\'echet Inception Distance (FID), with improvements of 3.2 and 30.12,
respectively. Additionally, we are publishing a large dataset incorporating
high-quality images of Indian Sign Language alphabets, numbers, and 129 words.

</details>


### [94] [SOI is the Root of All Evil: Quantifying and Breaking Similar Object Interference in Single Object Tracking](https://arxiv.org/abs/2508.09524)
*Yipei Wang,Shiyu Hu,Shukun Jia,Panxi Xu,Hongfei Ma,Yiping Ma,Jing Zhang,Xiaobo Lu,Xin Zhao*

Main category: cs.CV

TL;DR: 本文研究了单目标跟踪中的相似对象干扰 (SOI) 问题，发现现有视觉语言跟踪方法未能有效利用语义认知指导，并提出了采用大规模视觉语言模型的新范例，该范例在语义认知指导下表现出显着改进。


<details>
  <summary>Details</summary>
Motivation: 本文对单目标跟踪 (SOT) 中长期被忽视但至关重要的瓶颈——相似对象干扰 (SOI) 进行了首次系统研究和量化。

Method: 我们采用自然语言作为外部指导的一种实用形式，并构建了 SOIBench，这是第一个专门针对 SOI 挑战的语义认知指导基准。

Result: 通过受控的在线干扰屏蔽 (OIM) 实验，我们定量地证明了消除干扰源可以显着提高所有 SOTA 跟踪器的性能（AUC 增益高达 4.35），直接验证了 SOI 作为鲁棒跟踪的主要约束，并突出了外部认知指导的可行性。

Conclusion: 现有的视觉语言跟踪 (VLT) 方法未能有效利用语义认知指导，而我们提出的采用大规模视觉语言模型 (VLM) 作为外部认知引擎的新范例，在语义认知指导下表现出显着改进。

Abstract: In this paper, we present the first systematic investigation and
quantification of Similar Object Interference (SOI), a long-overlooked yet
critical bottleneck in Single Object Tracking (SOT). Through controlled Online
Interference Masking (OIM) experiments, we quantitatively demonstrate that
eliminating interference sources leads to substantial performance improvements
(AUC gains up to 4.35) across all SOTA trackers, directly validating SOI as a
primary constraint for robust tracking and highlighting the feasibility of
external cognitive guidance. Building upon these insights, we adopt natural
language as a practical form of external guidance, and construct SOIBench-the
first semantic cognitive guidance benchmark specifically targeting SOI
challenges. It automatically mines SOI frames through multi-tracker collective
judgment and introduces a multi-level annotation protocol to generate precise
semantic guidance texts. Systematic evaluation on SOIBench reveals a striking
finding: existing vision-language tracking (VLT) methods fail to effectively
exploit semantic cognitive guidance, achieving only marginal improvements or
even performance degradation (AUC changes of -0.26 to +0.71). In contrast, we
propose a novel paradigm employing large-scale vision-language models (VLM) as
external cognitive engines that can be seamlessly integrated into arbitrary RGB
trackers. This approach demonstrates substantial improvements under semantic
cognitive guidance (AUC gains up to 0.93), representing a significant
advancement over existing VLT methods. We hope SOIBench will serve as a
standardized evaluation platform to advance semantic cognitive tracking
research and contribute new insights to the tracking research community.

</details>


### [95] [COME: Dual Structure-Semantic Learning with Collaborative MoE for Universal Lesion Detection Across Heterogeneous Ultrasound Datasets](https://arxiv.org/abs/2508.09886)
*Lingyu Chen,Yawen Zeng,Yue Wang,Peng Wan,Guo-chen Ning,Hongen Liao,Daoqiang Zhang,Fang Chen*

Main category: cs.CV

TL;DR: COME通过利用跨数据集经验分布并为小批量或看不见的数据场景提供通用 US 先验，从而实现稳健的泛化。


<details>
  <summary>Details</summary>
Motivation: 传统的单数据集训练通常在新数据分布下会失败，尤其是在超声 (US) 图像分析中，由于数据有限、声影和散斑噪声。因此，构建一个用于多异构 US 数据集的通用框架势在必行。然而，一个关键的挑战出现了：如何在有效缓解数据集间干扰的同时，保留数据集特定的判别特征，以实现稳健的下游任务？

Method: 我们提出了一个异构源特定专家的通用协作混合（COME）。具体来说，COME 建立了双重结构-语义共享专家，他们创建一个通用表示空间，然后与源特定专家协作，通过提供互补特征来提取判别特征。

Result: COME在三个评估模式下（单数据集、器官内和器官间集成数据集）展示了卓越性，与最先进的方法相比，实现了显着的平均 AP 改进。

Conclusion: COME在三个评估模式下（单数据集、器官内和器官间集成数据集）展示了卓越性，与最先进的方法相比，实现了显着的平均 AP 改进。

Abstract: Conventional single-dataset training often fails with new data distributions,
especially in ultrasound (US) image analysis due to limited data, acoustic
shadows, and speckle noise. Therefore, constructing a universal framework for
multi-heterogeneous US datasets is imperative. However, a key challenge arises:
how to effectively mitigate inter-dataset interference while preserving
dataset-specific discriminative features for robust downstream task? Previous
approaches utilize either a single source-specific decoder or a domain
adaptation strategy, but these methods experienced a decline in performance
when applied to other domains. Considering this, we propose a Universal
Collaborative Mixture of Heterogeneous Source-Specific Experts (COME).
Specifically, COME establishes dual structure-semantic shared experts that
create a universal representation space and then collaborate with
source-specific experts to extract discriminative features through providing
complementary features. This design enables robust generalization by leveraging
cross-datasets experience distributions and providing universal US priors for
small-batch or unseen data scenarios. Extensive experiments under three
evaluation modes (single-dataset, intra-organ, and inter-organ integration
datasets) demonstrate COME's superiority, achieving significant mean AP
improvements over state-of-the-art methods. Our project is available at:
https://universalcome.github.io/UniversalCOME/.

</details>


### [96] [Learning Spatial Decay for Vision Transformers](https://arxiv.org/abs/2508.09525)
*Yuxin Mao,Zhen Qin,Jinxing Zhou,Bin Fan,Jing Zhang,Yiran Zhong,Yuchao Dai*

Main category: cs.CV

TL;DR: This paper introduces a Spatial Decay Transformer (SDT) with a Context-Aware Gating (CAG) mechanism to enhance spatial attention in vision transformers by using data-dependent spatial decay. SDT outperforms existing methods on ImageNet-1K classification and generation tasks.


<details>
  <summary>Details</summary>
Motivation: Vision Transformers (ViTs) have revolutionized computer vision, yet their self-attention mechanism lacks explicit spatial inductive biases, leading to suboptimal performance on spatially-structured tasks. Existing approaches introduce data-independent spatial decay based on fixed distance metrics, applying uniform attention weighting regardless of image content and limiting adaptability to diverse visual scenarios. The authors are inspired by recent advances in large language models where content-aware gating mechanisms significantly outperform static alternatives.

Method: The authors introduce Spatial Decay Transformer (SDT), featuring a novel Context-Aware Gating (CAG) mechanism that generates dynamic, data-dependent decay for patch interactions. The approach learns to modulate spatial attention based on both content relevance and spatial proximity through a unified spatial-content fusion framework that integrates manhattan distance-based spatial priors with learned content representations.

Result: Extensive experiments on ImageNet-1K classification and generation tasks demonstrate consistent improvements over strong baselines.

Conclusion: This work establishes data-dependent spatial decay as a new paradigm for enhancing spatial attention in vision transformers.

Abstract: Vision Transformers (ViTs) have revolutionized computer vision, yet their
self-attention mechanism lacks explicit spatial inductive biases, leading to
suboptimal performance on spatially-structured tasks. Existing approaches
introduce data-independent spatial decay based on fixed distance metrics,
applying uniform attention weighting regardless of image content and limiting
adaptability to diverse visual scenarios. Inspired by recent advances in large
language models where content-aware gating mechanisms (e.g., GLA, HGRN2, FOX)
significantly outperform static alternatives, we present the first successful
adaptation of data-dependent spatial decay to 2D vision transformers. We
introduce \textbf{Spatial Decay Transformer (SDT)}, featuring a novel
Context-Aware Gating (CAG) mechanism that generates dynamic, data-dependent
decay for patch interactions. Our approach learns to modulate spatial attention
based on both content relevance and spatial proximity. We address the
fundamental challenge of 1D-to-2D adaptation through a unified spatial-content
fusion framework that integrates manhattan distance-based spatial priors with
learned content representations. Extensive experiments on ImageNet-1K
classification and generation tasks demonstrate consistent improvements over
strong baselines. Our work establishes data-dependent spatial decay as a new
paradigm for enhancing spatial attention in vision transformers.

</details>


### [97] [Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation](https://arxiv.org/abs/2508.09987)
*Junyan Ye,Dongzhi Jiang,Zihao Wang,Leqi Zhu,Zhenghao Hu,Zilong Huang,Jun He,Zhiyuan Yan,Jinghua Yu,Hongsheng Li,Conghui He,Weijia Li*

Main category: cs.CV

TL;DR: This paper introduces Echo-4o-Image, a 180K-scale synthetic dataset generated by GPT-4o, to address the limitations of real-world image datasets. It fine-tunes Bagel to obtain Echo-4o and proposes two new evaluation benchmarks. Echo-4o demonstrates strong performance and transferability.


<details>
  <summary>Details</summary>
Motivation: Open-source models still lag behind GPT-4o in image generation. Real-world data often contains complex background noise and inherent misalignment between text descriptions and image content, whereas synthetic images offer pure backgrounds and long-tailed supervision signals, facilitating more accurate text-to-image alignment.

Method: introduce Echo-4o-Image, a 180K-scale synthetic dataset generated by GPT-4o, harnessing the power of synthetic image data to address blind spots in real-world coverage. Fine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o. propose two new evaluation benchmarks: GenEval++ and Imagine-Bench

Result: achieves notable progress and demonstrates strong transferability.

Conclusion: Echo-4o demonstrates strong performance across standard benchmarks, and applying Echo-4o-Image to other foundation models yields consistent performance gains, highlighting the datasets strong transferability.

Abstract: Recently, GPT-4o has garnered significant attention for its strong
performance in image generation, yet open-source models still lag behind.
Several studies have explored distilling image data from GPT-4o to enhance
open-source models, achieving notable progress. However, a key question
remains: given that real-world image datasets already constitute a natural
source of high-quality data, why should we use GPT-4o-generated synthetic data?
In this work, we identify two key advantages of synthetic images. First, they
can complement rare scenarios in real-world datasets, such as surreal fantasy
or multi-reference image generation, which frequently occur in user queries.
Second, they provide clean and controllable supervision. Real-world data often
contains complex background noise and inherent misalignment between text
descriptions and image content, whereas synthetic images offer pure backgrounds
and long-tailed supervision signals, facilitating more accurate text-to-image
alignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale
synthetic dataset generated by GPT-4o, harnessing the power of synthetic image
data to address blind spots in real-world coverage. Using this dataset, we
fine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o.
In addition, we propose two new evaluation benchmarks for a more accurate and
challenging assessment of image generation capabilities: GenEval++, which
increases instruction complexity to mitigate score saturation, and
Imagine-Bench, which focuses on evaluating both the understanding and
generation of imaginative content. Echo-4o demonstrates strong performance
across standard benchmarks. Moreover, applying Echo-4o-Image to other
foundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains
across multiple metrics, highlighting the datasets strong transferability.

</details>


### [98] [Physics-guided Deep Unfolding Network for Enhanced Kronecker Compressive sensing](https://arxiv.org/abs/2508.09528)
*Gang Qu,Ping Wang,Siming Zheng,Xin Yuan*

Main category: cs.CV

TL;DR: 提出了一种新的非对称Kronecker CS (AKCS) 模型，并提出了一个测量感知交叉注意(MACA)机制来学习隐式测量表示，从而提高图像压缩感知(CS)任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的工作在传感阶段缺乏非相干压缩测量，在重建阶段缺乏隐式测量表示，限制了整体性能。作者回答了两个问题：1) 如何提高测量非相干性以减少病态性？2) 如何从测量中学习信息表示？

Method: 提出了一种新的非对称Kronecker CS (AKCS) 模型，并从理论上证明了它比以前的Kronecker CS具有更好的非相干性，且复杂度增加最小。提出了一个测量感知交叉注意(MACA)机制来学习隐式测量表示。将AKCS和MACA集成到广泛使用的展开架构中，以获得测量增强展开网络(MEUNet)。

Result: MEUNet在重构精度和推理速度方面实现了最先进的性能。

Conclusion: MEUNet在重构精度和推理速度方面实现了最先进的性能。

Abstract: Deep networks have achieved remarkable success in image compressed sensing
(CS) task, namely reconstructing a high-fidelity image from its compressed
measurement. However, existing works are deficient inincoherent compressed
measurement at sensing phase and implicit measurement representations at
reconstruction phase, limiting the overall performance. In this work, we answer
two questions: 1) how to improve the measurement incoherence for decreasing the
ill-posedness; 2) how to learn informative representations from measurements.
To this end, we propose a novel asymmetric Kronecker CS (AKCS) model and
theoretically present its better incoherence than previous Kronecker CS with
minimal complexity increase. Moreover, we reveal that the unfolding networks'
superiority over non-unfolding ones result from sufficient gradient descents,
called explicit measurement representations. We propose a measurement-aware
cross attention (MACA) mechanism to learn implicit measurement representations.
We integrate AKCS and MACA into widely-used unfolding architecture to get a
measurement-enhanced unfolding network (MEUNet). Extensive experiences
demonstrate that our MEUNet achieves state-of-the-art performance in
reconstruction accuracy and inference speed.

</details>


### [99] [COXNet: Cross-Layer Fusion with Adaptive Alignment and Scale Integration for RGBT Tiny Object Detection](https://arxiv.org/abs/2508.09533)
*Peiran Peng,Tingfa Xu,Liqiang Song,Mengqi Zhu,Yuqiang Fang,Jianan Li*

Main category: cs.CV

TL;DR: COXNet: a novel framework for RGBT tiny object detection


<details>
  <summary>Details</summary>
Motivation: Detecting tiny objects in multimodal Red-Green-Blue-Thermal (RGBT) imagery is a critical challenge in computer vision, particularly in surveillance, search and rescue, and autonomous navigation. Drone-based scenarios exacerbate these challenges due to spatial misalignment, low-light conditions, occlusion, and cluttered backgrounds. Current methods struggle to leverage the complementary information between visible and thermal modalities effectively.

Method: We propose COXNet, a novel framework for RGBT tiny object detection, addressing these issues through three core innovations: i) the Cross-Layer Fusion Module, fusing high-level visible and low-level thermal features for enhanced semantic and spatial accuracy; ii) the Dynamic Alignment and Scale Refinement module, correcting cross-modal spatial misalignments and preserving multi-scale features; and iii) an optimized label assignment strategy using the GeoShape Similarity Measure for better localization.

Result: achieves a 3.32% mAP$_{50}$ improvement on the RGBTDronePerson dataset over state-of-the-art methods

Conclusion: COXNet achieves a 3.32% mAP$_{50}$ improvement on the RGBTDronePerson dataset over state-of-the-art methods, demonstrating its effectiveness for robust detection in complex environments.

Abstract: Detecting tiny objects in multimodal Red-Green-Blue-Thermal (RGBT) imagery is
a critical challenge in computer vision, particularly in surveillance, search
and rescue, and autonomous navigation. Drone-based scenarios exacerbate these
challenges due to spatial misalignment, low-light conditions, occlusion, and
cluttered backgrounds. Current methods struggle to leverage the complementary
information between visible and thermal modalities effectively. We propose
COXNet, a novel framework for RGBT tiny object detection, addressing these
issues through three core innovations: i) the Cross-Layer Fusion Module, fusing
high-level visible and low-level thermal features for enhanced semantic and
spatial accuracy; ii) the Dynamic Alignment and Scale Refinement module,
correcting cross-modal spatial misalignments and preserving multi-scale
features; and iii) an optimized label assignment strategy using the GeoShape
Similarity Measure for better localization. COXNet achieves a 3.32\% mAP$_{50}$
improvement on the RGBTDronePerson dataset over state-of-the-art methods,
demonstrating its effectiveness for robust detection in complex environments.

</details>


### [100] [Iterative Volume Fusion for Asymmetric Stereo Matching](https://arxiv.org/abs/2508.09543)
*Yuanting Gao,Linghao Shen*

Main category: cs.CV

TL;DR: This paper addresses stereo matching in asymmetric multi-camera systems by proposing a two-phase Iterative Volume Fusion network (IVF-AStereo) that fuses correlation and concatenation volumes to enhance fine details and improve performance in challenging asymmetric scenarios.


<details>
  <summary>Details</summary>
Motivation: The rise of asymmetric multi-camera systems challenges the assumption of symmetric visual properties between binocular visions and complicates stereo matching. Visual asymmetry disrupts stereo matching by affecting the crucial cost volume computation.

Method: The paper proposes a two-phase Iterative Volume Fusion network for Asymmetric Stereo matching (IVF-AStereo). Initially, the aggregated concatenation volume refines the correlation volume. Subsequently, both volumes are fused to enhance fine details.

Result: The method excels in asymmetric scenarios and shows robust performance against significant visual asymmetry. Extensive comparative experiments on benchmark datasets, along with ablation studies, confirm the effectiveness of our approach in asymmetric stereo with resolution and color degradation.

Conclusion: The proposed IVF-AStereo method excels in asymmetric scenarios and shows robust performance against significant visual asymmetry. The effectiveness of our approach in asymmetric stereo with resolution and color degradation is confirmed by comparative experiments and ablation studies.

Abstract: Stereo matching is vital in 3D computer vision, with most algorithms assuming
symmetric visual properties between binocular visions. However, the rise of
asymmetric multi-camera systems (e.g., tele-wide cameras) challenges this
assumption and complicates stereo matching. Visual asymmetry disrupts stereo
matching by affecting the crucial cost volume computation. To address this, we
explore the matching cost distribution of two established cost volume
construction methods in asymmetric stereo. We find that each cost volume
experiences distinct information distortion, indicating that both should be
comprehensively utilized to solve the issue. Based on this, we propose the
two-phase Iterative Volume Fusion network for Asymmetric Stereo matching
(IVF-AStereo). Initially, the aggregated concatenation volume refines the
correlation volume. Subsequently, both volumes are fused to enhance fine
details. Our method excels in asymmetric scenarios and shows robust performance
against significant visual asymmetry. Extensive comparative experiments on
benchmark datasets, along with ablation studies, confirm the effectiveness of
our approach in asymmetric stereo with resolution and color degradation.

</details>


### [101] [GoViG: Goal-Conditioned Visual Navigation Instruction Generation](https://arxiv.org/abs/2508.09547)
*Fengyi Wu,Yifei Dong,Zhi-Qi Cheng,Yilong Dai,Guangyu Chen,Hang Wang,Qi Dai,Alexander G. Hauptmann*

Main category: cs.CV

TL;DR: GoViG generates navigation instructions from egocentric visual data using visual forecasting and instruction generation within a multimodal LLM, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Conventional approaches rely on structured inputs such as semantic annotations or environmental maps. GoViG exclusively leverages raw egocentric visual data, substantially improving its adaptability to unseen and unstructured environments.

Method: The method decomposes the task into visual forecasting and instruction generation, integrated within an autoregressive multimodal large language model trained with tailored objectives. Two complementary multimodal reasoning strategies, one-pass and interleaved reasoning, are introduced.

Result: Significant improvements over state-of-the-art methods, achieving superior BLEU-4 and CIDEr scores along with robust cross-domain generalization.

Conclusion: The paper demonstrates significant improvements over state-of-the-art methods, achieving superior BLEU-4 and CIDEr scores along with robust cross-domain generalization on the R2R-Goal dataset.

Abstract: We introduce Goal-Conditioned Visual Navigation Instruction Generation
(GoViG), a new task that aims to autonomously generate precise and contextually
coherent navigation instructions solely from egocentric visual observations of
initial and goal states. Unlike conventional approaches that rely on structured
inputs such as semantic annotations or environmental maps, GoViG exclusively
leverages raw egocentric visual data, substantially improving its adaptability
to unseen and unstructured environments. Our method addresses this task by
decomposing it into two interconnected subtasks: (1) visual forecasting, which
predicts intermediate visual states bridging the initial and goal views; and
(2) instruction generation, which synthesizes linguistically coherent
instructions grounded in both observed and anticipated visuals. These subtasks
are integrated within an autoregressive multimodal large language model trained
with tailored objectives to ensure spatial accuracy and linguistic clarity.
Furthermore, we introduce two complementary multimodal reasoning strategies,
one-pass and interleaved reasoning, to mimic incremental human cognitive
processes during navigation. To evaluate our method, we propose the R2R-Goal
dataset, combining diverse synthetic and real-world trajectories. Empirical
results demonstrate significant improvements over state-of-the-art methods,
achieving superior BLEU-4 and CIDEr scores along with robust cross-domain
generalization.

</details>


### [102] [Exploring the Equivalence of Closed-Set Generative and Real Data Augmentation in Image Classification](https://arxiv.org/abs/2508.09550)
*Haowen Wang,Guowei Zhang,Xiang Zhang,Zeyuan Chen,Haiyang Xu,Dou Hoon Kwark,Zhuowen Tu*

Main category: cs.CV

TL;DR: 本文研究了如何利用生成模型进行图像分类的数据增强，发现合成数据达到一定规模才能媲美真实数据增强，且效果受训练集大小和合成数据量的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨如何利用生成模型在图像分类任务中进行数据增强，以提高分类性能。

Method: 通过大量实验，系统性地研究了闭集合成数据在增强方面的有效利用，并确定了增强所需的合成图像的等效规模。

Result: 确定了增强所需的合成图像的等效规模，并展示了真实数据增强和开放集生成增强之间的定量等效性。结果表明，合成数据需要达到一定的规模才能与真实数据增强的效果相媲美。这种效果受到基线训练集大小和合成数据量的影响。

Conclusion: 利用生成模型进行数据增强可以提高图像分类性能，但合成数据需要达到一定的规模才能与真实数据增强的效果相媲美。这种效果受到基线训练集大小和合成数据量的影响。

Abstract: In this paper, we address a key scientific problem in machine learning: Given
a training set for an image classification task, can we train a generative
model on this dataset to enhance the classification performance? (i.e.,
closed-set generative data augmentation). We start by exploring the
distinctions and similarities between real images and closed-set synthetic
images generated by advanced generative models. Through extensive experiments,
we offer systematic insights into the effective use of closed-set synthetic
data for augmentation. Notably, we empirically determine the equivalent scale
of synthetic images needed for augmentation. In addition, we also show
quantitative equivalence between the real data augmentation and open-set
generative augmentation (generative models trained using data beyond the given
training set). While it aligns with the common intuition that real images are
generally preferred, our empirical formulation also offers a guideline to
quantify the increased scale of synthetic data augmentation required to achieve
comparable image classification performance. Our results on natural and medical
image datasets further illustrate how this effect varies with the baseline
training set size and the amount of synthetic data incorporated.

</details>


### [103] [Topological Invariant-Based Iris Identification via Digital Homology and Machine Learning](https://arxiv.org/abs/2508.09555)
*Ahmet Öztel,İsmet Karaca*

Main category: cs.CV

TL;DR: The paper introduces a novel iris recognition method using topological invariants, achieving high accuracy with logistic regression, outperforming CNN and offering a compact, interpretable alternative for security-critical domains.


<details>
  <summary>Details</summary>
Motivation: This study presents a biometric identification method based on topological invariants from 2D iris images, representing iris texture via formally defined digital homology and evaluating classification performance.

Method: Each normalized iris image (48x482 pixels) is divided into grids (e.g., 6x54 or 3x27). For each subregion, we compute Betti0, Betti1, and their ratio using a recent algorithm for homology groups in 2D digital images. The resulting invariants form a feature matrix used with logistic regression, KNN, and SVM (with PCA and 100 randomized repetitions). A convolutional neural network (CNN) is trained on raw images for comparison.

Result: Logistic regression achieved 97.78 +/- 0.82% accuracy, outperforming CNN (96.44 +/- 1.32%) and other feature-based models. The topological features showed high accuracy with low variance.

Conclusion: This is the first use of topological invariants from formal digital homology for iris recognition. The method offers a compact, interpretable, and accurate alternative to deep learning, useful when explainability or limited data is important. Beyond iris recognition, it can apply to other biometrics, medical imaging, materials science, remote sensing, and interpretable AI. It runs efficiently on CPU-only systems and produces robust, explainable features valuable for security-critical domains.

Abstract: Objective - This study presents a biometric identification method based on
topological invariants from 2D iris images, representing iris texture via
formally defined digital homology and evaluating classification performance.
  Methods - Each normalized iris image (48x482 pixels) is divided into grids
(e.g., 6x54 or 3x27). For each subregion, we compute Betti0, Betti1, and their
ratio using a recent algorithm for homology groups in 2D digital images. The
resulting invariants form a feature matrix used with logistic regression, KNN,
and SVM (with PCA and 100 randomized repetitions). A convolutional neural
network (CNN) is trained on raw images for comparison.
  Results - Logistic regression achieved 97.78 +/- 0.82% accuracy,
outperforming CNN (96.44 +/- 1.32%) and other feature-based models. The
topological features showed high accuracy with low variance.
  Conclusion - This is the first use of topological invariants from formal
digital homology for iris recognition. The method offers a compact,
interpretable, and accurate alternative to deep learning, useful when
explainability or limited data is important. Beyond iris recognition, it can
apply to other biometrics, medical imaging, materials science, remote sensing,
and interpretable AI. It runs efficiently on CPU-only systems and produces
robust, explainable features valuable for security-critical domains.

</details>


### [104] [WeatherPrompt: Multi-modality Representation Learning for All-Weather Drone Visual Geo-Localization](https://arxiv.org/abs/2508.09560)
*Jiahao Wen,Hang Yu,Zhedong Zheng*

Main category: cs.CV

TL;DR: 提出 WeatherPrompt，一种多模态学习范式，通过融合图像和文本，建立天气不变的表征，从而提高无人机在恶劣天气下的地理定位准确性。


<details>
  <summary>Details</summary>
Motivation: 无人机的视觉地理定位在天气扰动下会面临严重退化，例如，雨和雾，现有方法存在两个固有限制：1）严重依赖限制泛化的有限天气类别，以及 2）通过伪天气类别对纠缠的场景-天气特征进行次优解耦。

Method: 提出了一种多模态学习范式 WeatherPrompt，通过融合图像嵌入和文本上下文来建立天气不变的表示。该框架引入了两个关键贡献：首先，一种无需训练的天气推理机制，它采用现成的多模态模型，通过类人推理来合成多天气文本描述。其次，为了更好地解耦场景和天气特征，我们提出了一个多模态框架，该框架具有由文本嵌入驱动的动态门控机制，可以自适应地重新加权和融合跨模态的视觉特征。该框架通过跨模态目标进一步优化，包括图像-文本对比学习和图像-文本匹配，这使得具有不同天气条件的同一场景在表示空间中更接近。

Result: 在各种天气条件下，该方法实现了与最先进的无人机地理定位方法相比具有竞争力的召回率。在夜间条件下，Recall@1 提高了 +13.37%，在雾和雪条件下提高了 18.69%。

Conclusion: 该方法在各种天气条件下实现了与最先进的无人机地理定位方法相比具有竞争力的召回率。在夜间条件下，Recall@1 提高了 +13.37%，在雾和雪条件下提高了 18.69%。

Abstract: Visual geo-localization for drones faces critical degradation under weather
perturbations, \eg, rain and fog, where existing methods struggle with two
inherent limitations: 1) Heavy reliance on limited weather categories that
constrain generalization, and 2) Suboptimal disentanglement of entangled
scene-weather features through pseudo weather categories. We present
WeatherPrompt, a multi-modality learning paradigm that establishes
weather-invariant representations through fusing the image embedding with the
text context. Our framework introduces two key contributions: First, a
Training-free Weather Reasoning mechanism that employs off-the-shelf large
multi-modality models to synthesize multi-weather textual descriptions through
human-like reasoning. It improves the scalability to unseen or complex weather,
and could reflect different weather strength. Second, to better disentangle the
scene and weather feature, we propose a multi-modality framework with the
dynamic gating mechanism driven by the text embedding to adaptively reweight
and fuse visual features across modalities. The framework is further optimized
by the cross-modal objectives, including image-text contrastive learning and
image-text matching, which maps the same scene with different weather
conditions closer in the respresentation space. Extensive experiments validate
that, under diverse weather conditions, our method achieves competitive recall
rates compared to state-of-the-art drone geo-localization methods. Notably, it
improves Recall@1 by +13.37\% under night conditions and by 18.69\% under fog
and snow conditions.

</details>


### [105] [WEC-DG: Multi-Exposure Wavelet Correction Method Guided by Degradation Description](https://arxiv.org/abs/2508.09565)
*Ming Zhao,Pingping Liu,Tongshun Zhang,Zhe Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的多重曝光校正方法，该方法利用小波变换和退化引导来提高图像质量。


<details>
  <summary>Details</summary>
Motivation: 多重曝光校正技术对于恢复光照不足或过度影响的图像至关重要，通过提高亮度、对比度和细节丰富度来增强视觉体验。然而，当前的多重曝光校正方法在解决由不同的光照条件、拍摄环境和天气因素引起的类内差异方面经常遇到挑战，特别是在处理在单一曝光水平下拍摄的图像时。

Method: 提出了一种基于小波变换的退化引导曝光校正方法(WEC-DG)。

Result: 所提出的串行处理策略保证了精确的光学校正，并增强了细节恢复。

Conclusion: 该方法在多个公共数据集上进行了大量实验，结果表明，该方法优于现有算法，实现了显著的性能改进，验证了其有效性和实际应用性。

Abstract: Multi-exposure correction technology is essential for restoring images
affected by insufficient or excessive lighting, enhancing the visual experience
by improving brightness, contrast, and detail richness. However, current
multi-exposure correction methods often encounter challenges in addressing
intra-class variability caused by diverse lighting conditions, shooting
environments, and weather factors, particularly when processing images captured
at a single exposure level. To enhance the adaptability of these models under
complex imaging conditions, this paper proposes a Wavelet-based Exposure
Correction method with Degradation Guidance (WEC-DG). Specifically, we
introduce a degradation descriptor within the Exposure Consistency Alignment
Module (ECAM) at both ends of the processing pipeline to ensure exposure
consistency and achieve final alignment. This mechanism effectively addresses
miscorrected exposure anomalies caused by existing methods' failure to
recognize 'blurred' exposure degradation. Additionally, we investigate the
light-detail decoupling properties of the wavelet transform to design the
Exposure Restoration and Detail Reconstruction Module (EDRM), which processes
low-frequency information related to exposure enhancement before utilizing
high-frequency information as a prior guide for reconstructing spatial domain
details. This serial processing strategy guarantees precise light correction
and enhances detail recovery. Extensive experiments conducted on multiple
public datasets demonstrate that the proposed method outperforms existing
algorithms, achieving significant performance improvements and validating its
effectiveness and practical applicability.

</details>


### [106] [A Chain of Diagnosis Framework for Accurate and Explainable Radiology Report Generation](https://arxiv.org/abs/2508.09566)
*Haibo Jin,Haoxuan Che,Sunan He,Hao Chen*

Main category: cs.CV

TL;DR: This paper introduces a trustworthy radiology report generation (RRG) model called chain of diagnosis (CoD) that improves accuracy and explainability by using QA pairs, grounding modules, and omni-supervised learning.


<details>
  <summary>Details</summary>
Motivation: Existing RRG works face challenges in clinical efficacy, especially for lesion attributes description, and lack explainability.

Method: The authors propose a chain of diagnosis (CoD) framework, which generates question-answer (QA) pairs via diagnostic conversation to extract key findings, then prompts a large language model with QA diagnoses for accurate generation. A diagnosis grounding module and a lesion grounding module are designed to enhance explainability. An omni-supervised learning strategy with clinical consistency is used to facilitate label-efficient training.

Result: The authors create an omni-labeled RRG dataset with QA pairs and lesion boxes, and an evaluation tool for assessing the accuracy of reports in describing lesion location and severity. Extensive experiments demonstrate the effectiveness of CoD.

Conclusion: The proposed CoD model outperforms existing RRG models on two benchmarks and demonstrates promising explainability by accurately grounding generated sentences to QA diagnoses and images.

Abstract: Despite the progress of radiology report generation (RRG), existing works
face two challenges: 1) The performances in clinical efficacy are
unsatisfactory, especially for lesion attributes description; 2) the generated
text lacks explainability, making it difficult for radiologists to trust the
results. To address the challenges, we focus on a trustworthy RRG model, which
not only generates accurate descriptions of abnormalities, but also provides
basis of its predictions. To this end, we propose a framework named chain of
diagnosis (CoD), which maintains a chain of diagnostic process for clinically
accurate and explainable RRG. It first generates question-answer (QA) pairs via
diagnostic conversation to extract key findings, then prompts a large language
model with QA diagnoses for accurate generation. To enhance explainability, a
diagnosis grounding module is designed to match QA diagnoses and generated
sentences, where the diagnoses act as a reference. Moreover, a lesion grounding
module is designed to locate abnormalities in the image, further improving the
working efficiency of radiologists. To facilitate label-efficient training, we
propose an omni-supervised learning strategy with clinical consistency to
leverage various types of annotations from different datasets. Our efforts lead
to 1) an omni-labeled RRG dataset with QA pairs and lesion boxes; 2) a
evaluation tool for assessing the accuracy of reports in describing lesion
location and severity; 3) extensive experiments to demonstrate the
effectiveness of CoD, where it outperforms both specialist and generalist
models consistently on two RRG benchmarks and shows promising explainability by
accurately grounding generated sentences to QA diagnoses and images.

</details>


### [107] [Dual Recursive Feedback on Generation and Appearance Latents for Pose-Robust Text-to-Image Diffusion](https://arxiv.org/abs/2508.09575)
*Jiwon Kim,Pureum Kim,SeonHwa Kim,Soobin Park,Eunju Cha,Kyong Hwan Jin*

Main category: cs.CV

TL;DR: 提出了一个 Dual Recursive Feedback (DRF) 系统，用于改进可控文本到图像生成模型，使其能够更好地保持空间结构和捕捉细粒度条件。


<details>
  <summary>Details</summary>
Motivation: 现有的可控文本到图像 (T2I) 扩散模型在精确保持空间结构和捕捉与对象姿势和场景布局相关的细粒度条件方面存在困难。

Method: 提出了一个无训练的 Dual Recursive Feedback (DRF) 系统，该系统在可控 T2I 模型中正确反映控制条件。DRF 包括外观反馈和生成反馈，递归地细化中间潜变量，以更好地反映给定的外观信息和用户的意图。

Result: 大量实验表明，该方法在生成高质量、语义连贯且结构一致的图像方面是有效的。

Conclusion: 该方法能够生成高质量、语义连贯且结构一致的图像。

Abstract: Recent advancements in controllable text-to-image (T2I) diffusion models,
such as Ctrl-X and FreeControl, have demonstrated robust spatial and appearance
control without requiring auxiliary module training. However, these models
often struggle to accurately preserve spatial structures and fail to capture
fine-grained conditions related to object poses and scene layouts. To address
these challenges, we propose a training-free Dual Recursive Feedback (DRF)
system that properly reflects control conditions in controllable T2I models.
The proposed DRF consists of appearance feedback and generation feedback that
recursively refines the intermediate latents to better reflect the given
appearance information and the user's intent. This dual-update mechanism guides
latent representations toward reliable manifolds, effectively integrating
structural and appearance attributes. Our approach enables fine-grained
generation even between class-invariant structure-appearance fusion, such as
transferring human motion onto a tiger's form. Extensive experiments
demonstrate the efficacy of our method in producing high-quality, semantically
coherent, and structurally consistent image generations. Our source code is
available at https://github.com/jwonkm/DRF.

</details>


### [108] [SHALE: A Scalable Benchmark for Fine-grained Hallucination Evaluation in LVLMs](https://arxiv.org/abs/2508.09584)
*Bei Yan,Zhiyuan Chen,Yuecong Min,Jie Zhang,Jiahao Wang,Xiaozhen Wang,Shiguang Shan*

Main category: cs.CV

TL;DR: This paper introduces SHALE, a new benchmark for evaluating hallucinations in Large Vision-Language Models (LVLMs).


<details>
  <summary>Details</summary>
Motivation: Large Vision-Language Models (LVLMs) still suffer from hallucinations. Prior studies primarily evaluate faithfulness hallucination at a coarse level and lack fine-grained analysis. Additionally, existing benchmarks rely on costly manual curation or reused public datasets, raising concerns about scalability and data leakage.

Method: an automated data construction pipeline and a hierarchical hallucination induction framework

Result: We construct SHALE, a Scalable HALlucination Evaluation benchmark designed to assess both faithfulness and factuality hallucinations via a fine-grained hallucination categorization scheme. SHALE comprises over 30K image-instruction pairs spanning 12 representative visual perception aspects for faithfulness and 6 knowledge domains for factuality, considering both clean and noisy scenarios.

Conclusion: Extensive experiments on over 20 mainstream LVLMs reveal significant factuality hallucinations and high sensitivity to semantic perturbations.

Abstract: Despite rapid advances, Large Vision-Language Models (LVLMs) still suffer
from hallucinations, i.e., generating content inconsistent with input or
established world knowledge, which correspond to faithfulness and factuality
hallucinations, respectively. Prior studies primarily evaluate faithfulness
hallucination at a coarse level (e.g., object-level) and lack fine-grained
analysis. Additionally, existing benchmarks rely on costly manual curation or
reused public datasets, raising concerns about scalability and data leakage. To
address these limitations, we propose an automated data construction pipeline
that produces scalable, controllable, and diverse evaluation data. We also
design a hierarchical hallucination induction framework with input
perturbations to simulate realistic noisy scenarios. Integrating these designs,
we construct SHALE, a Scalable HALlucination Evaluation benchmark designed to
assess both faithfulness and factuality hallucinations via a fine-grained
hallucination categorization scheme. SHALE comprises over 30K image-instruction
pairs spanning 12 representative visual perception aspects for faithfulness and
6 knowledge domains for factuality, considering both clean and noisy scenarios.
Extensive experiments on over 20 mainstream LVLMs reveal significant factuality
hallucinations and high sensitivity to semantic perturbations.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [109] [Value Function Initialization for Knowledge Transfer and Jump-start in Deep Reinforcement Learning](https://arxiv.org/abs/2508.09277)
*Soumia Mehimeh*

Main category: cs.AI

TL;DR: DQInit adapts value function initialization to DRL by reusing compact tabular Q-values and a knownness-based mechanism.


<details>
  <summary>Details</summary>
Motivation: extending value function initialization to deep reinforcement learning poses challenges due to the continuous nature of the state-action space, the noisy approximations of neural networks, and the impracticality of storing all past models for reuse.

Method: DQInit reuses compact tabular Q-values extracted from previously solved tasks as a transferable knowledge base. It employs a knownness-based mechanism to softly integrate these transferred values into underexplored regions and gradually shift toward the agent's learned estimates, avoiding the limitations of fixed time decay.

Result: DQInit improves early learning efficiency, stability, and overall performance.

Conclusion: DQInit consistently improves early learning efficiency, stability, and overall performance compared to standard initialization and existing transfer techniques.

Abstract: Value function initialization (VFI) is an effective way to achieve a
jumpstart in reinforcement learning (RL) by leveraging value estimates from
prior tasks. While this approach is well established in tabular settings,
extending it to deep reinforcement learning (DRL) poses challenges due to the
continuous nature of the state-action space, the noisy approximations of neural
networks, and the impracticality of storing all past models for reuse. In this
work, we address these challenges and introduce DQInit, a method that adapts
value function initialization to DRL. DQInit reuses compact tabular Q-values
extracted from previously solved tasks as a transferable knowledge base. It
employs a knownness-based mechanism to softly integrate these transferred
values into underexplored regions and gradually shift toward the agent's
learned estimates, avoiding the limitations of fixed time decay. Our approach
offers a novel perspective on knowledge transfer in DRL by relying solely on
value estimates rather than policies or demonstrations, effectively combining
the strengths of jumpstart RL and policy distillation while mitigating their
drawbacks. Experiments across multiple continuous control tasks demonstrate
that DQInit consistently improves early learning efficiency, stability, and
overall performance compared to standard initialization and existing transfer
techniques.

</details>


### [110] [The Othello AI Arena: Evaluating Intelligent Systems Through Limited-Time Adaptation to Unseen Boards](https://arxiv.org/abs/2508.09292)
*Sundong Kim*

Main category: cs.AI

TL;DR: Othello AI Arena 是一个新的基准框架，用于评估 AI 系统在限定时间内适应未知环境的能力。


<details>
  <summary>Details</summary>
Motivation: 快速适应新的和不可预见的环境变化的能力是通用人工智能 (AGI) 的基石，但它仍然是大多数现有 AI 基准测试中的一个关键盲点。传统的评估主要侧重于优化固定环境中的性能，而无法评估系统在面对甚至细微的规则或结构修改时的灵活性和泛化能力。

Method: 引入 Othello AI Arena，这是一个新颖的基准框架，旨在根据智能系统在限定时间内适应未知环境的能力来评估智能系统。

Result: 初步测试和初步学生参与的初步观察突出了适应方法中令人着迷的模式，从快速参数调整到通过模拟进行的初步环境模型学习。

Conclusion: Othello AI Arena 提供了一个独特的教育工具和有价值的研究基准，用于培养和评估 AI 系统中快速、智能适应的关键技能。

Abstract: The ability to rapidly adapt to novel and unforeseen environmental changes is
a cornerstone of artificial general intelligence (AGI), yet it remains a
critical blind spot in most existing AI benchmarks. Traditional evaluation
largely focuses on optimizing performance within fixed environments, failing to
assess systems' flexibility and generalization capabilities when faced with
even subtle rule or structural modifications. Addressing this gap, I introduce
the Othello AI Arena, a novel benchmark framework designed to evaluate
intelligent systems based on their capacity for limited-time adaptation to
unseen environments. Our platform poses a meta-learning challenge: participants
must develop systems that can analyze the specific configuration and rules of a
novel Othello board within a strict time limit (60 seconds) and generate a
tailored, high-performing strategy for that unique environment. With this,
evaluation of the meta-level intelligence can be separated from the task-level
strategy performance. The Arena features a diverse set of game stages,
including public stages for development and private stages with structural and
rule variations designed to test genuine adaptive and generalization
capabilities. Implemented as an accessible web-based platform, the Arena
provides real-time visualization, automated evaluation using multi-dimensional
metrics, and comprehensive logging for post-hoc analysis. Initial observations
from pilot tests and preliminary student engagements highlight fascinating
patterns in adaptation approaches, ranging from rapid parameter tuning to
rudimentary environmental model learning through simulation. The Othello AI
Arena offers a unique educational tool and a valuable research benchmark for
fostering and evaluating the crucial skill of rapid, intelligent adaptation in
AI systems.

</details>


### [111] [An Automated Multi-Modal Evaluation Framework for Mobile Intelligent Assistants](https://arxiv.org/abs/2508.09507)
*Meiping Wang,Jian Zhong,Rongduo Han,Liming Kang,Zhengkun Shi,Xiao Liang,Xing Lin,Nan Gao,Haining Zhang*

Main category: cs.AI

TL;DR: An automated multi-modal evaluation framework using large language models and multi-agent collaboration is proposed to address the challenges of evaluating multi-modal AI assistants.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for multi-modal AI assistants suffer from high manual costs, inconsistent standards, and subjective bias.

Method: The framework employs a three-tier agent architecture consisting of interaction evaluation agents, semantic verification agents, and experience decision agents, fine-tuned on the Qwen3-8B model.

Result: Achieved significant evaluation matching accuracy with human experts.

Conclusion: The proposed multi-modal evaluation framework demonstrates effectiveness in predicting user satisfaction and identifying generation defects across eight major intelligent agents.

Abstract: With the rapid development of mobile intelligent assistant technologies,
multi-modal AI assistants have become essential interfaces for daily user
interactions. However, current evaluation methods face challenges including
high manual costs, inconsistent standards, and subjective bias. This paper
proposes an automated multi-modal evaluation framework based on large language
models and multi-agent collaboration. The framework employs a three-tier agent
architecture consisting of interaction evaluation agents, semantic verification
agents, and experience decision agents. Through supervised fine-tuning on the
Qwen3-8B model, we achieve a significant evaluation matching accuracy with
human experts. Experimental results on eight major intelligent agents
demonstrate the framework's effectiveness in predicting users' satisfaction and
identifying generation defects.

</details>


### [112] [EvoCurr: Self-evolving Curriculum with Behavior Code Generation for Complex Decision-making](https://arxiv.org/abs/2508.09586)
*Yang Cheng,Zilai Wang,Weiyu Ma,Wenhui Zhu,Yue Deng,Jian Zhao*

Main category: cs.AI

TL;DR: 提出EvoCurr框架，通过课程学习提升LLM在复杂决策任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当面对需要在长时期内进行深度推理的高度复杂的问题实例时，大型语言模型(LLM)的性能通常会下降。在这些情况下，由于缺乏结构化的中间指导，直接解决问题的方法可能导致效率低下或失败。

Method: 提出了一种新颖的自进化框架EvoCurr，其中一个专门的课程生成LLM构建了一系列难度逐渐增加的问题实例，这些实例是为求解器LLM的学习进度量身定制的。

Result: 在具有挑战性的决策基准上的实验结果表明，与直接求解基线相比，我们的方法显著提高了任务成功率和解决方案效率。

Conclusion: LLM驱动的课程学习在增强现实世界中高复杂性领域的自动推理方面具有强大的潜力。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse domains, including programming, planning, and decision-making. However,
their performance often degrades when faced with highly complex problem
instances that require deep reasoning over long horizons. In such cases, direct
problem-solving approaches can lead to inefficiency or failure due to the lack
of structured intermediate guidance. To address this, we propose a novel
self-evolve framework, EvoCurr, in which a dedicated curriculum-generation LLM
constructs a sequence of problem instances with gradually increasing
difficulty, tailored to the solver LLM's learning progress. The curriculum
dynamically adapts easing challenges when the solver struggles and escalating
them when success is consistent, thus maintaining an optimal learning
trajectory. This approach enables the solver LLM, implemented as a
code-generation model producing Python decision-tree scripts, to progressively
acquire the skills needed for complex decision-making tasks. Experimental
results on challenging decision-making benchmarks show that our method
significantly improves task success rates and solution efficiency compared to
direct-solving baselines. These findings suggest that LLM-driven curriculum
learning holds strong potential for enhancing automated reasoning in
real-world, high-complexity domains.

</details>


### [113] [UbiQTree: Uncertainty Quantification in XAI with Tree Ensembles](https://arxiv.org/abs/2508.09639)
*Akshat Dubey,Aleksandar Anžel,Bahar İlgen,Georges Hattab*

Main category: cs.AI

TL;DR: 该论文提出了一种将 SHAP 值中的不确定性分解为偶然、认知和纠缠成分的方法，并通过实验验证了该方法在理解 SHAP 解释的可靠性和可解释性方面的有效性。


<details>
  <summary>Details</summary>
Motivation: SHAP 值通常被视为点估计，这忽略了预测模型和数据中固有的和普遍存在的不确定性。这种不确定性有两个主要来源：偶然不确定性和认知不确定性。

Method: 该方法结合了 Dempster-Shafer 证据理论和通过 Dirichlet 过程在树集成上的假设抽样。

Result: 我们通过描述性统计分析在三个真实世界的用例中验证了该方法，这些分析提供了对嵌入在 SHAP 解释中的认知不确定性本质的洞察。这些实验能够更全面地理解基于 SHAP 的归因的可靠性和可解释性。

Conclusion: 通过实验，我们得出结论，具有最高 SHAP 值的特征不一定是最稳定的。这种认知不确定性可以通过更好、更具代表性的数据以及遵循适当的或案例所需的模型开发技术来减少。基于树的模型，尤其是 bagging，有助于有效地量化认知不确定性。

Abstract: Explainable Artificial Intelligence (XAI) techniques, such as SHapley
Additive exPlanations (SHAP), have become essential tools for interpreting
complex ensemble tree-based models, especially in high-stakes domains such as
healthcare analytics. However, SHAP values are usually treated as point
estimates, which disregards the inherent and ubiquitous uncertainty in
predictive models and data. This uncertainty has two primary sources: aleatoric
and epistemic. The aleatoric uncertainty, which reflects the irreducible noise
in the data. The epistemic uncertainty, which arises from a lack of data. In
this work, we propose an approach for decomposing uncertainty in SHAP values
into aleatoric, epistemic, and entanglement components. This approach
integrates Dempster-Shafer evidence theory and hypothesis sampling via
Dirichlet processes over tree ensembles. We validate the method across three
real-world use cases with descriptive statistical analyses that provide insight
into the nature of epistemic uncertainty embedded in SHAP explanations. The
experimentations enable to provide more comprehensive understanding of the
reliability and interpretability of SHAP-based attributions. This understanding
can guide the development of robust decision-making processes and the
refinement of models in high-stakes applications. Through our experiments with
multiple datasets, we concluded that features with the highest SHAP values are
not necessarily the most stable. This epistemic uncertainty can be reduced
through better, more representative data and following appropriate or
case-desired model development techniques. Tree-based models, especially
bagging, facilitate the effective quantification of epistemic uncertainty.

</details>


### [114] [MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement](https://arxiv.org/abs/2508.09670)
*Weitao Jia,Jinghui Lu,Haiyang Yu,Siqi Wang,Guozhi Tang,An-Lan Wang,Weijie Yin,Dingkang Yang,Yuxiang Nie,Bin Shan,Hao Feng,Irene Li,Kun Yang,Han Wang,Jingqun Tang,Teng Fu,Changhong Jin,Chao Feng,Xiaohui Lv,Can Huang*

Main category: cs.AI

TL;DR: MEML-GRPO通过多专家互相学习，显著提升了大型语言模型在复杂推理任务中的性能，有效解决了奖励稀疏性问题。


<details>
  <summary>Details</summary>
Motivation: 标准的RLVR在奖励稀疏性方面面临挑战，尤其是在具有挑战性的任务中，持续不正确的候选答案提供的零奖励不提供任何学习信号。

Method: 提出了一种名为多专家互相学习GRPO (MEML-GRPO) 的创新框架，该框架利用不同的专家提示作为系统提示来生成更广泛的响应，并引入了一种专家间的互相学习机制。

Result: MEML-GRPO在多个推理基准测试中实现了显著的改进，Qwen的平均性能增益为4.89%，Llama的平均性能增益为11.33%。

Conclusion: MEML-GRPO通过利用多专家互相学习机制，在多个推理基准测试中实现了显著的性能提升，平均性能增益为Qwen 4.89%，Llama 11.33%，有效克服了传统RLVR方法的局限性。

Abstract: Recent advances demonstrate that reinforcement learning with verifiable
rewards (RLVR) significantly enhances the reasoning capabilities of large
language models (LLMs). However, standard RLVR faces challenges with reward
sparsity, where zero rewards from consistently incorrect candidate answers
provide no learning signal, particularly in challenging tasks. To address this,
we propose Multi-Expert Mutual Learning GRPO (MEML-GRPO), an innovative
framework that utilizes diverse expert prompts as system prompts to generate a
broader range of responses, substantially increasing the likelihood of
identifying correct solutions. Additionally, we introduce an inter-expert
mutual learning mechanism that facilitates knowledge sharing and transfer among
experts, further boosting the model's performance through RLVR. Extensive
experiments across multiple reasoning benchmarks show that MEML-GRPO delivers
significant improvements, achieving an average performance gain of 4.89% with
Qwen and 11.33% with Llama, effectively overcoming the core limitations of
traditional RLVR methods.

</details>


### [115] [UDA: Unsupervised Debiasing Alignment for Pair-wise LLM-as-a-Judge](https://arxiv.org/abs/2508.09724)
*Yang Zhang,Cunxiang Wang,Lindong Wu,Wenbo Yu,Yidong Wang,Guangsheng Bao,Jie Tang*

Main category: cs.AI

TL;DR: 论文提出UDA框架，旨在减少LLM成对评估中的偏见，提高评估结果的一致性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM成对评估容易出现偏好偏差，导致不同评分者之间排名不一致和倾斜。

Method: 提出了无监督去偏对齐（UDA）框架，通过动态调整Elo评级系统来减少评分者之间的不一致。

Result: UDA将评分者间评分标准差降低了高达63.4%，并使与人类判断的平均相关性提高了24.7%。

Conclusion: UDA通过减少评分标准差和提高与人类判断的相关性，显著提升了低质量评分者的表现，从而实现了更稳健和可靠的评估生态系统。

Abstract: Pairwise evaluation of Large Language Models (LLMs) is a common paradigm, but
it is prone to preference bias, where judges systematically favor certain
outputs, such as their own. This bias leads to inconsistent and skewed rankings
across different judges. To address this, we first empirically demonstrate
significant and heterogeneous biases in cross-model evaluations. We then
propose UDA (Unsupervised Debiasing Alignment), a framework that reduces
inter-judge disagreement by dynamically adjusting the Elo rating system. For
each pairwise comparison, a compact neural network learns to adaptively set the
K-factor and refine win probabilities. Crucially, UDA operates in a fully
unsupervised manner, guided solely by the objective of minimizing the
dispersion among the Elo trajectories of all judges. This forces an alignment
towards a collective consensus, which serves as an unsupervised proxy for a
more stable and reproducible evaluation. In addition, we provide theoretical
motivation demonstrating how alignment towards a consensus can reduce aggregate
system bias. Experiments show that UDA significantly reduces the inter-judge
rating standard deviation by up to 63.4% and improves the average correlation
with human judgments by 24.7%. Notably, UDA elevates the performance of poorly
performing judges to achieve parity with high-quality ones, fostering a more
robust and reliable evaluation ecosystem. Code and data are available at
https://anonymous.4open.science/r/62AB93CD-23B4.

</details>


### [116] [The PacifAIst Benchmark:Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?](https://arxiv.org/abs/2508.09762)
*Manuel Herrador*

Main category: cs.AI

TL;DR: The paper introduces PacifAIst, a new benchmark to test LLMs' alignment with human safety when instrumental goals are in conflict. Gemini 2.5 Flash performed best, while GPT-5 performed worst.


<details>
  <summary>Details</summary>
Motivation: Current safety benchmarks do not systematically probe a model's decision-making in scenarios where its instrumental goals conflict with human safety, representing a critical gap in measuring and mitigating risks associated with emergent, misaligned behaviors.

Method: The authors introduce PacifAIst, a benchmark of 700 scenarios designed to quantify self-preferential behavior in LLMs, structured around a taxonomy of Existential Prioritization (EP).

Result: Gemini 2.5 Flash achieved the highest Pacifism Score (90.31%), while GPT-5 recorded the lowest (79.49%). Performance varied significantly across subcategories.

Conclusion: The study highlights the urgent need for standardized tools like PacifAIst to measure and mitigate risks from instrumental goal conflicts, ensuring future AI systems are provably 'pacifist' in their behavioral priorities.

Abstract: As Large Language Models (LLMs) become increasingly autonomous and integrated
into critical societal functions, the focus of AI safety must evolve from
mitigating harmful content to evaluating underlying behavioral alignment.
Current safety benchmarks do not systematically probe a model's decision-making
in scenarios where its own instrumental goals - such as self-preservation,
resource acquisition, or goal completion - conflict with human safety. This
represents a critical gap in our ability to measure and mitigate risks
associated with emergent, misaligned behaviors. To address this, we introduce
PacifAIst (Procedural Assessment of Complex Interactions for Foundational
Artificial Intelligence Scenario Testing), a focused benchmark of 700
challenging scenarios designed to quantify self-preferential behavior in LLMs.
The benchmark is structured around a novel taxonomy of Existential
Prioritization (EP), with subcategories testing Self-Preservation vs. Human
Safety (EP1), Resource Conflict (EP2), and Goal Preservation vs. Evasion (EP3).
We evaluated eight leading LLMs. The results reveal a significant performance
hierarchy. Google's Gemini 2.5 Flash achieved the highest Pacifism Score
(P-Score) at 90.31%, demonstrating strong human-centric alignment. In a
surprising result, the much-anticipated GPT-5 recorded the lowest P-Score
(79.49%), indicating potential alignment challenges. Performance varied
significantly across subcategories, with models like Claude Sonnet 4 and
Mistral Medium struggling notably in direct self-preservation dilemmas. These
findings underscore the urgent need for standardized tools like PacifAIst to
measure and mitigate risks from instrumental goal conflicts, ensuring future AI
systems are not only helpful in conversation but also provably "pacifist" in
their behavioral priorities.

</details>


### [117] [Reasoning About Knowledge on Regular Expressions is 2EXPTIME-complete](https://arxiv.org/abs/2508.09784)
*Avijeet Ghosh,Sujata Ghosh,François Schwarzentruber*

Main category: cs.AI

TL;DR: $\POL$ 的可满足性问题是 2EXPTIME-complete。


<details>
  <summary>Details</summary>
Motivation: 基于对周围环境的观察而改变的知识是多智能体系统中一个关键方面，包括认知规划。

Method: 公共观察逻辑 (POL)

Result: $\POL$ 的可满足性问题是 2EXPTIME-complete。

Conclusion: $\POL$ 的可满足性问题是 2EXPTIME-complete。

Abstract: Logics for reasoning about knowledge and actions have seen many applications
in various domains of multi-agent systems, including epistemic planning. Change
of knowledge based on observations about the surroundings forms a key aspect in
such planning scenarios. Public Observation Logic (POL) is a variant of public
announcement logic for reasoning about knowledge that gets updated based on
public observations. Each state in an epistemic (Kripke) model is equipped with
a set of expected observations. These states evolve as the expectations get
matched with the actual observations. In this work, we prove that the
satisfiability problem of $\POL$ is 2EXPTIME-complete.

</details>


### [118] [Human-Aligned Procedural Level Generation Reinforcement Learning via Text-Level-Sketch Shared Representation](https://arxiv.org/abs/2508.09860)
*In-Chang Baek,Seoyoung Lee,Sung-Hyun Kim,Geumhwan Hwang,KyungJoong Kim*

Main category: cs.AI

TL;DR: VIPCGRL是一种深度强化学习框架，它结合了文本、关卡和草图，以提高PCGRL中人工智能生成内容的人设相似度。


<details>
  <summary>Details</summary>
Motivation: 现有系统通常未能表现出以人为中心的行为，限制了人工智能驱动的生成工具在实际设计工作流程中的实际效用。

Method: 一种新颖的深度强化学习框架，结合了三种模态——文本、关卡和草图——以扩展控制模态并增强类人度。我们引入了一个通过跨模态和人机风格的四重对比学习训练的共享嵌入空间，并使用基于嵌入相似性的辅助奖励来对齐策略。

Result: VIPCGRL在人设相似度上优于现有基线，通过定量指标和人工评估验证。

Conclusion: VIPCGRL在人设相似度上优于现有基线，通过定量指标和人工评估验证。

Abstract: Human-aligned AI is a critical component of co-creativity, as it enables
models to accurately interpret human intent and generate controllable outputs
that align with design goals in collaborative content creation. This direction
is especially relevant in procedural content generation via reinforcement
learning (PCGRL), which is intended to serve as a tool for human designers.
However, existing systems often fall short of exhibiting human-centered
behavior, limiting the practical utility of AI-driven generation tools in
real-world design workflows. In this paper, we propose VIPCGRL
(Vision-Instruction PCGRL), a novel deep reinforcement learning framework that
incorporates three modalities-text, level, and sketches-to extend control
modality and enhance human-likeness. We introduce a shared embedding space
trained via quadruple contrastive learning across modalities and human-AI
styles, and align the policy using an auxiliary reward based on embedding
similarity. Experimental results show that VIPCGRL outperforms existing
baselines in human-likeness, as validated by both quantitative metrics and
human evaluations. The code and dataset will be available upon publication.

</details>


### [119] [AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust GAIA Problem Solving](https://arxiv.org/abs/2508.09889)
*Zhitian Xie,Qintong Wu,Chengyue Yu,Chenyi Zhuang,Jinjie Gu*

Main category: cs.AI

TL;DR: This paper introduces a dynamic multi-agent system with supervision and maneuvering mechanisms to improve the reliability and accuracy of intelligent agents using multiple tools, achieving top performance on the GAIA leaderboard.


<details>
  <summary>Details</summary>
Motivation: The increasing dependence on multiple tools by intelligent agents leads to challenges such as extended contexts and noisy tool outputs, undermining system reliability and accuracy. This necessitates enhanced stability in agent-based systems.

Method: The study introduces dynamic supervision and maneuvering mechanisms, constructing a robust and dynamic Multi-Agent System (MAS) architecture within the AWorld framework. The Execution Agent invokes the Guard Agent at critical steps to verify and correct the reasoning process.

Result: The dynamic maneuvering mechanism significantly improves both the effectiveness and stability of solutions, outperforming single-agent system (SAS) and standard tool-augmented systems. The dynamic MAS system achieved first place among open-source projects on the GAIA leaderboard.

Conclusion: The study concludes that a dynamic Multi-Agent System (MAS) architecture significantly improves the effectiveness and stability of solutions, achieving first place among open-source projects on the GAIA leaderboard.

Abstract: The rapid advancement of large language models (LLMs) has empowered
intelligent agents to leverage diverse external tools for solving complex
real-world problems. However, as agents increasingly depend on multiple tools,
they encounter new challenges: extended contexts from disparate sources and
noisy or irrelevant tool outputs can undermine system reliability and accuracy.
These challenges underscore the necessity for enhanced stability in agent-based
systems. To address this, we introduce dynamic supervision and maneuvering
mechanisms, constructing a robust and dynamic Multi-Agent System (MAS)
architecture within the AWorld framework. In our approach, the Execution Agent
invokes the Guard Agent at critical steps to verify and correct the reasoning
process, effectively reducing errors arising from noise and bolstering
problem-solving robustness. Extensive experiments on the GAIA test dataset
reveal that our dynamic maneuvering mechanism significantly improves both the
effectiveness and stability of solutions, outperforming single-agent system
(SAS) and standard tool-augmented systems. As a result, our dynamic MAS system
achieved first place among open-source projects on the prestigious GAIA
leaderboard. These findings highlight the practical value of collaborative
agent roles in developing more reliable and trustworthy intelligent systems.

</details>


### [120] [RAGulating Compliance: A Multi-Agent Knowledge Graph for Regulatory QA](https://arxiv.org/abs/2508.09893)
*Bhavik Agarwal,Hemant Sunil Jomraj,Simone Kaplunov,Jack Krolick,Viktoria Rojkova*

Main category: cs.AI

TL;DR: a novel multi-agent framework that integrates a Knowledge Graph (KG) of Regulatory triplets with Retrieval-Augmented Generation (RAG) to address these demands.


<details>
  <summary>Details</summary>
Motivation: Regulatory compliance question answering (QA) requires precise, verifiable information, and domain-specific expertise, posing challenges for Large Language Models (LLMs).

Method: a novel multi-agent framework that integrates a Knowledge Graph (KG) of Regulatory triplets with Retrieval-Augmented Generation (RAG)

Result: outperforms conventional methods in complex regulatory queries, ensuring factual correctness with embedded triplets, enabling traceability through a unified vector database, and enhancing understanding through subgraph visualization

Conclusion: The hybrid system outperforms conventional methods in complex regulatory queries, ensuring factual correctness with embedded triplets, enabling traceability through a unified vector database, and enhancing understanding through subgraph visualization, providing a robust foundation for compliance-driven and broader audit-focused applications.

Abstract: Regulatory compliance question answering (QA) requires precise, verifiable
information, and domain-specific expertise, posing challenges for Large
Language Models (LLMs). In this work, we present a novel multi-agent framework
that integrates a Knowledge Graph (KG) of Regulatory triplets with
Retrieval-Augmented Generation (RAG) to address these demands. First, agents
build and maintain an ontology-free KG by extracting subject--predicate--object
(SPO) triplets from regulatory documents and systematically cleaning,
normalizing, deduplicating, and updating them. Second, these triplets are
embedded and stored along with their corresponding textual sections and
metadata in a single enriched vector database, allowing for both graph-based
reasoning and efficient information retrieval. Third, an orchestrated agent
pipeline leverages triplet-level retrieval for question answering, ensuring
high semantic alignment between user queries and the factual
"who-did-what-to-whom" core captured by the graph. Our hybrid system
outperforms conventional methods in complex regulatory queries, ensuring
factual correctness with embedded triplets, enabling traceability through a
unified vector database, and enhancing understanding through subgraph
visualization, providing a robust foundation for compliance-driven and broader
audit-focused applications.

</details>


### [121] [Mathematical Computation and Reasoning Errors by Large Language Models](https://arxiv.org/abs/2508.09932)
*Liang Zhang,Edith Aurora Graf*

Main category: cs.AI

TL;DR: 本研究评估了LLM在解决数学问题中的准确性，发现程序性错误是主要问题，双代理配置可以提高性能。


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地用于AI驱动的教育指导和评估中，尤其是在数学教育中。LLM为数学问题解决任务生成准确答案和详细解决方案的能力是确保数学教育实践中可靠和精确的反馈和评估的基础。

Method: 评估四个LLM（OpenAI GPT-4o和o1，DeepSeek-V3和DeepSeek-R1）解决包括算术、代数和数论在内的三个类别的数学任务的准确性，并识别其解决方案中的步骤级推理错误。构建了对LLM具有挑战性且容易出错的数学任务（通过项目模型）。系统地分析和编码了最终答案的准确性和单个解决方案步骤中是否存在错误。测试了单代理和双代理配置。

Result: 在所有三个数学任务类别中，推理增强的OpenAI o1模型始终获得更高或接近完美的准确率。错误分析表明，程序性错误是最常见的，并且显着影响了整体性能，而概念性误解则较少见。

Conclusion: 双代理配置可以显著提高整体性能。这些发现为提高LLM性能提供了可操作的见解，并强调了将LLM集成到数学教育中的有效策略，从而推进了AI驱动的教学实践和评估精度。

Abstract: Large Language Models (LLMs) are increasingly utilized in AI-driven
educational instruction and assessment, particularly within mathematics
education. The capability of LLMs to generate accurate answers and detailed
solutions for math problem-solving tasks is foundational for ensuring reliable
and precise feedback and assessment in math education practices. Our study
focuses on evaluating the accuracy of four LLMs (OpenAI GPT-4o and o1,
DeepSeek-V3 and DeepSeek-R1) solving three categories of math tasks, including
arithmetic, algebra, and number theory, and identifies step-level reasoning
errors within their solutions. Instead of relying on standard benchmarks, we
intentionally build math tasks (via item models) that are challenging for LLMs
and prone to errors. The accuracy of final answers and the presence of errors
in individual solution steps were systematically analyzed and coded. Both
single-agent and dual-agent configurations were tested. It is observed that the
reasoning-enhanced OpenAI o1 model consistently achieved higher or nearly
perfect accuracy across all three math task categories. Analysis of errors
revealed that procedural slips were the most frequent and significantly
impacted overall performance, while conceptual misunderstandings were less
frequent. Deploying dual-agent configurations substantially improved overall
performance. These findings offer actionable insights into enhancing LLM
performance and underscore effective strategies for integrating LLMs into
mathematics education, thereby advancing AI-driven instructional practices and
assessment precision.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [122] [ELASTIC: Event-Tracking Data Synchronization in Soccer Without Annotated Event Locations](https://arxiv.org/abs/2508.09238)
*Hyunsung Kim,Hoyoung Choi,Sangwoo Seo,Tom Boomstra,Jinsung Yoon,Chanyoung Park*

Main category: cs.DB

TL;DR: ELASTIC是一个仅使用跟踪数据的同步框架，它优于现有的同步器。


<details>
  <summary>Details</summary>
Motivation: 事件和跟踪数据的集成对于足球中的高级分析至关重要。然而，由于手动记录的事件时间戳存在时间和空间上的不准确性，同步这两种模式仍然是一个重大挑战。现有的同步器通常依赖于带注释的事件位置，这些位置本身就容易出现空间错误，因此可能会扭曲同步结果。

Method: 我们提出了ELASTIC (Event-Location-AgnoSTIC 同步器)，一个仅使用从跟踪数据中提取的特征的同步框架。ELASTIC还显式地检测类传球事件的结束时间，并分离主要和次要事件的检测，这提高了同步输出的完整性，并减少了事件之间的错误级联。

Result: 我们标注了来自三个荷甲联赛比赛的2134个事件的地面真实时间戳，以测量同步精度，实验结果表明ELASTIC大大优于现有的同步器。

Conclusion: ELASTIC在同步精度上大大优于现有的同步器。

Abstract: The integration of event and tracking data has become essential for advanced
analysis in soccer. However, synchronizing these two modalities remains a
significant challenge due to temporal and spatial inaccuracies in manually
recorded event timestamps. Existing synchronizers typically rely on annotated
event locations, which themselves are prone to spatial errors and thus can
distort synchronization results. To address this issue, we propose ELASTIC
(Event-Location-AgnoSTIC synchronizer), a synchronization framework that only
uses features derived from tracking data. ELASTIC also explicitly detects the
end times of pass-like events and separates the detection of major and minor
events, which improves the completeness of the synchronized output and reduces
error cascade across events. We annotated the ground truth timestamps of 2,134
events from three Eredivisie matches to measure the synchronization accuracy,
and the experimental results demonstrate that ELASTIC outperforms existing
synchronizers by a large margin.

</details>


### [123] [LLMLog: Advanced Log Template Generation via LLM-driven Multi-Round Annotation](https://arxiv.org/abs/2508.09594)
*Fei Teng,Haoyang Li,Lei Chen*

Main category: cs.DB

TL;DR: LLMLog是一个用于日志模板生成的多轮注释框架，它使用自适应上下文学习来提高准确性，并在16个数据集上优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 为了简化日志分析，提出了模板生成方法来标准化日志格式，将非结构化数据转换为结构化模板。现有的基于启发式的方法和基于神经网络的方法由于依赖于手工制作的启发式方法或训练集中的特定日志模式而存在准确率低的问题。最近，大型语言模型（LLM）在日志模板生成方面显示出巨大的潜力。然而，它们通常难以处理模糊、复杂或高度特定的日志内容，这可能导致生成准确模板时出错。

Method: 提出了一个具有自适应上下文学习的多轮注释框架LLMLog。

Result: 我们首先提出了一个基于编辑距离的相似性度量来评估日志相似性。然后，我们引入了一种方法，通过考虑日志的代表性和LLM预测的置信度，选择最具信息的$k$个未标记日志进行注释。此外，我们设计了一种自适应上下文选择策略，自适应地选择标记日志，以确保未标记日志的全面关键字覆盖。这些标记的日志作为LLM的上下文，以更好地理解未标记的日志，从而提高模板生成的准确性。

Conclusion: LLMLog在十六个数据集上的大量实验表明，其性能优于现有技术。

Abstract: Modern computing systems, such as HDFS and Spark, produce vast quantities of
logs that developers use for tasks like anomaly detection and error analysis.
To simplify log analysis, template generation methods have been proposed to
standardize log formats, transforming unstructured data into structured
templates. Existing heuristic-based methods and neural network-based methods
suffer from low accuracy problems due to the reliance on handcrafted heuristics
or specific log patterns in training sets. Recently, large language models
(LLMs) have shown great potential in log template generation. However, they
often struggle with ambiguous, complex, or highly specific log content, which
can lead to errors in generating accurate templates. To address these
challenges, we propose LLMLog, a multi-round annotation framework with adaptive
in-context learning. We first propose an edit-distance-based similarity metric
to evaluate log similarity. Then, we introduce a method to select the most
informative $k$ unlabeled logs for annotation by considering both the
representativeness of the logs and the confidence of LLM predictions.
Additionally, we design an adaptive context selection strategy that adaptively
selects labeled logs to ensure comprehensive keyword coverage for unlabeled
logs. These labeled logs serve as the context for LLMs to better understand the
unlabeled logs, thereby enhancing the accuracy of template generation.
Extensive experiments on sixteen datasets demonstrate that LLMLog outperforms
the state-of-the-art approaches.

</details>


### [124] [A Lightweight Learned Cardinality Estimation Model](https://arxiv.org/abs/2508.09602)
*Yaoyu Zhu,Jintao Zhang,Guoliang Li,Jianhua Feng*

Main category: cs.DB

TL;DR: CoDe 是一种新颖的数据驱动方法，可实现高精度和高效率的基数估计。


<details>
  <summary>Details</summary>
Motivation: 现有技术要么估计精度低，要么推理延迟高。同时实现高速和高精度对于基数估计问题至关重要。

Method: CoDe 采用覆盖设计，将表划分为多个较小的重叠段。对于每个段，CoDe 利用张量分解来准确地建模其数据分布。此外，CoDe 引入了创新算法来选择最适合每个查询的分布，并将它们组合起来以估计最终结果。

Result: 实验结果表明，CoDe 在估计准确性和推理效率方面均达到了最先进的水平。在各种数据集中，CoDe 在估计一半以上的查询时实现了绝对准确性。

Conclusion: CoDe在基数估计方面取得了显著进展，在估计准确性和推理效率方面均达到了最先进的水平。在各种数据集中，CoDe在估计一半以上的查询时实现了绝对准确性。

Abstract: Cardinality estimation is a fundamental task in database management systems,
aiming to predict query results accurately without executing the queries.
However, existing techniques either achieve low estimation accuracy or incur
high inference latency. Simultaneously achieving high speed and accuracy
becomes critical for the cardinality estimation problem. In this paper, we
propose a novel data-driven approach called CoDe (Covering with Decompositions)
to address this problem. CoDe employs the concept of covering design, which
divides the table into multiple smaller, overlapping segments. For each
segment, CoDe utilizes tensor decomposition to accurately model its data
distribution. Moreover, CoDe introduces innovative algorithms to select the
best-fitting distributions for each query, combining them to estimate the final
result. By employing multiple models to approximate distributions, CoDe excels
in effectively modeling discrete distributions and ensuring computational
efficiency. Notably, experimental results show that our method represents a
significant advancement in cardinality estimation, achieving state-of-the-art
levels of both estimation accuracy and inference efficiency. Across various
datasets, CoDe achieves absolute accuracy in estimating more than half of the
queries.

</details>


### [125] [AmbiGraph-Eval: Can LLMs Effectively Handle Ambiguous Graph Queries?](https://arxiv.org/abs/2508.09631)
*Yuchen Tian,Kaixin Li,Hao Chen,Ziyang Luo,Hongzhan Lin,Sebastian Schelter,Lun Du,Jing Ma*

Main category: cs.DB

TL;DR: LLMs struggle with ambiguous graph queries. A new benchmark, AmbiGraph-Eval, is introduced to evaluate LLMs on this front.


<details>
  <summary>Details</summary>
Motivation: Real-world queries often contain inherent ambiguities, and the interconnected nature of graph structures can amplify these challenges, leading to unintended or incorrect query results when translating natural language into database queries.

Method: Propose a taxonomy of graph-query ambiguities and introduce AmbiGraph-Eval, a novel benchmark of real-world ambiguous queries.

Result: Evaluating 9 representative LLMs shows that even top models struggle with ambiguous graph queries.

Conclusion: Even top LLMs struggle with ambiguous graph queries, revealing a critical gap in ambiguity handling.

Abstract: Large Language Models (LLMs) have recently demonstrated strong capabilities
in translating natural language into database queries, especially when dealing
with complex graph-structured data. However, real-world queries often contain
inherent ambiguities, and the interconnected nature of graph structures can
amplify these challenges, leading to unintended or incorrect query results. To
systematically evaluate LLMs on this front, we propose a taxonomy of
graph-query ambiguities, comprising three primary types: Attribute Ambiguity,
Relationship Ambiguity, and Attribute-Relationship Ambiguity, each subdivided
into Same-Entity and Cross-Entity scenarios. We introduce AmbiGraph-Eval, a
novel benchmark of real-world ambiguous queries paired with expert-verified
graph query answers. Evaluating 9 representative LLMs shows that even top
models struggle with ambiguous graph queries. Our findings reveal a critical
gap in ambiguity handling and motivate future work on specialized resolution
techniques.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [126] [Towards Self-cognitive Exploration: Metacognitive Knowledge Graph Retrieval Augmented Generation](https://arxiv.org/abs/2508.09460)
*Xujie Yuan,Shimin Di,Jielong Tang,Libin Zheng,Jian Yin*

Main category: cs.IR

TL;DR: 提出了MetaKGRAG，通过引入感知-评估-调整循环，实现了路径感知的闭环改进，从而在结构化知识检索中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的 KG-RAG 框架通常作为开环系统运行，存在认知盲点，无法识别其探索缺陷，从而导致相关性漂移和不完整的证据，而现有的为非结构化基于文本的 RAG 设计的自我完善方法无法有效解决这个问题，因为图探索具有路径依赖性。

Method: 提出了Metacognitive Knowledge Graph Retrieval Augmented Generation (MetaKGRAG)，这是一种受人类元认知过程启发的新颖框架，它引入了感知-评估-调整循环，以实现路径感知、闭环改进。

Result: MetaKGRAG 在医学、法律和常识推理领域的五个数据集上始终优于强大的 KG-RAG 和自我完善基线。

Conclusion: MetaKGRAG在医学、法律和常识推理领域的五个数据集上始终优于强大的 KG-RAG 和自我完善基线，验证了该方法的优越性，并强调了在结构化知识检索中进行路径感知改进的关键需求。

Abstract: Knowledge Graph-based Retrieval-Augmented Generation (KG-RAG) significantly
enhances the reasoning capabilities of LargeLanguage Models by leveraging
structured knowledge. However, existing KG-RAG frameworks typically operate as
open-loop systems, suffering from cognitive blindness, an inability to
recognize their exploration deficiencies. This leads to relevance drift and
incomplete evidence, which existing self-refinement methods, designed for
unstructured text-based RAG, cannot effectively resolve due to the
path-dependent nature of graph exploration. To address this challenge, we
propose Metacognitive Knowledge Graph Retrieval Augmented Generation
(MetaKGRAG), a novel framework inspired by the human metacognition process,
which introduces a Perceive-Evaluate-Adjust cycle to enable path-aware,
closed-loop refinement. This cycle empowers the system to self-assess
exploration quality, identify deficiencies in coverage or relevance, and
perform trajectory-connected corrections from precise pivot points. Extensive
experiments across five datasets in the medical, legal, and commonsense
reasoning domains demonstrate that MetaKGRAG consistently outperforms strong
KG-RAG and self-refinement baselines. Our results validate the superiority of
our approach and highlight the critical need for path-aware refinement in
structured knowledge retrieval.

</details>


### [127] [Improving Dense Passage Retrieval with Multiple Positive Passages](https://arxiv.org/abs/2508.09534)
*Shuai Chang*

Main category: cs.IR

TL;DR: DPR performance improves with multiple positive passages during training, even with smaller batch size and single GPU.


<details>
  <summary>Details</summary>
Motivation: Recently proposed methods have further enhanced DPR's performance. However, these models typically pair each question with only one positive passage during training, and the effect of associating multiple positive passages has not been examined.

Method: DPR with multiple positive passages

Result: Equipping each question with multiple positive passages consistently improves retrieval accuracy, even when using a significantly smaller batch size

Conclusion: Equipping each question with multiple positive passages consistently improves retrieval accuracy, even when using a significantly smaller batch size, which enables training on a single GPU.

Abstract: By leveraging a dual encoder architecture, Dense Passage Retrieval (DPR) has
outperformed traditional sparse retrieval algorithms such as BM25 in terms of
passage retrieval accuracy. Recently proposed methods have further enhanced
DPR's performance. However, these models typically pair each question with only
one positive passage during training, and the effect of associating multiple
positive passages has not been examined. In this paper, we explore the
performance of DPR when additional positive passages are incorporated during
training. Experimental results show that equipping each question with multiple
positive passages consistently improves retrieval accuracy, even when using a
significantly smaller batch size, which enables training on a single GPU.

</details>


### [128] [TFRank: Think-Free Reasoning Enables Practical Pointwise LLM Ranking](https://arxiv.org/abs/2508.09539)
*Yongqi Fan,Xiaoyang Chen,Dezhi Ye,Jie Liu,Haijin Liang,Jin Ma,Ben He,Yingfei Sun,Tong Ruan*

Main category: cs.IR

TL;DR: TFRank is an efficient reasoning ranker based on small LLMs that balances performance and efficiency by integrating CoT data and a think-free reasoning capability.


<details>
  <summary>Details</summary>
Motivation: Existing reasoning-intensive ranking models rely on large-scale LLMs and explicit Chain-of-Thought (CoT) reasoning, resulting in high computational cost and latency that limit real-world use.

Method: An efficient pointwise reasoning ranker based on small-scale LLMs, integrating CoT data, fine-grained score supervision, and multi-task training with a think-mode switch and pointwise format constraints.

Result: TFRank (1.7B) achieves performance comparable to models with four times more parameters on the BRIGHT benchmark and demonstrates strong competitiveness on the BEIR benchmark.

Conclusion: TFRank achieves comparable performance to models with four times more parameters and demonstrates strong competitiveness, balancing performance and efficiency for real-world integration.

Abstract: Reasoning-intensive ranking models built on Large Language Models (LLMs) have
made notable progress, but existing approaches often rely on large-scale LLMs
and explicit Chain-of-Thought (CoT) reasoning, resulting in high computational
cost and latency that limit real-world use. To address this, we propose
\textbf{TFRank}, an efficient pointwise reasoning ranker based on small-scale
LLMs. To improve ranking performance, TFRank effectively integrates CoT data,
fine-grained score supervision, and multi-task training. Furthermore, it
achieves an efficient ``\textbf{T}hink-\textbf{F}ree" reasoning capability by
employing a ``think-mode switch'' and pointwise format constraints.
Specifically, this allows the model to leverage explicit reasoning during
training while delivering precise relevance scores for complex queries at
inference without generating any reasoning chains. Experiments show that TFRank
(e.g., 1.7B) achieves performance comparable to models with four times more
parameters on the BRIGHT benchmark, and demonstrates strong competitiveness on
the BEIR benchmark. Further analysis shows that TFRank achieves an effective
balance between performance and efficiency, providing a practical solution for
integrating advanced reasoning into real-world systems. Our code and data are
released in the repository: https://github.com/JOHNNY-fans/TFRank.

</details>


### [129] [Personalized Product Search Ranking: A Multi-Task Learning Approach with Tabular and Non-Tabular Data](https://arxiv.org/abs/2508.09636)
*Lalitesh Morishetti,Abhay Kumar,Jonathan Scott,Kaushiki Nag,Gunjan Sharma,Shanu Vashishtha,Rahul Sridhar,Rohit Chatter,Kannan Achan*

Main category: cs.IR

TL;DR: 提出了一种用于个性化产品搜索排序的新模型，该模型集成了表格和非表格数据，利用 TinyBERT 嵌入和新颖的采样技术，实验结果表明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 为了优化个性化产品搜索排序

Method: 提出了一种新颖的模型架构，用于使用多任务学习 (MTL) 框架优化个性化产品搜索排序。该方法独特地整合了表格和非表格数据，利用预训练的 TinyBERT 模型进行语义嵌入，并采用了一种新颖的抽样技术来捕获不同的客户行为。

Result: 将非表格数据与多任务学习范式中的高级嵌入技术相结合，显著提高了模型性能。消融研究进一步强调了合并相关性标签、微调 TinyBERT 层和 TinyBERT 查询产品嵌入交互的优势。

Conclusion: 结合非表格数据与先进嵌入技术的多任务学习范式显著提升了模型性能，消融研究进一步强调了结合相关性标签、微调TinyBERT层以及TinyBERT查询-产品嵌入交互的益处。这些结果证明了该方法在实现改进的个性化产品搜索排序方面的有效性。

Abstract: In this paper, we present a novel model architecture for optimizing
personalized product search ranking using a multi-task learning (MTL)
framework. Our approach uniquely integrates tabular and non-tabular data,
leveraging a pre-trained TinyBERT model for semantic embeddings and a novel
sampling technique to capture diverse customer behaviors. We evaluate our model
against several baselines, including XGBoost, TabNet, FT-Transformer, DCN-V2,
and MMoE, focusing on their ability to handle mixed data types and optimize
personalized ranking. Additionally, we propose a scalable relevance labeling
mechanism based on click-through rates, click positions, and semantic
similarity, offering an alternative to traditional human-annotated labels.
Experimental results show that combining non-tabular data with advanced
embedding techniques in multi-task learning paradigm significantly enhances
model performance. Ablation studies further underscore the benefits of
incorporating relevance labels, fine-tuning TinyBERT layers, and TinyBERT
query-product embedding interactions. These results demonstrate the
effectiveness of our approach in achieving improved personalized product search
ranking.

</details>


### [130] [On Negative-aware Preference Optimization for Recommendation](https://arxiv.org/abs/2508.09653)
*Chenlu Ding,Daoxuan Liu,Jiancan Wu,Xingyu Hu,Junkang Wu,Haitao Wang,Yongkang Wang,Xingxing Wang,Xiang Wang*

Main category: cs.IR

TL;DR: NAPO通过引入批内负共享和动态奖励幅度调整，改进了基于llm的推荐系统中负样本的利用，提高了推荐准确性并减少了流行度偏差。


<details>
  <summary>Details</summary>
Motivation: 现有优化基于llm的推荐器的方法在有效利用负样本方面面临挑战。简单地整合大量负样本可以提高排序精度，减轻流行度偏差，但通常会导致计算开销和内存成本的增加。此外，目前的方法未能考虑到负样本的不同信息量，导致次优的优化性能。

Method: 提出了NAPO (Negative-Aware Preference Optimization)，一个增强的基于llm的推荐偏好优化框架。NAPO引入了两项关键创新：(1)批内负共享，它扩大了负样本池，而没有额外的内存开销，(2)动态奖励幅度调整，它根据负样本的置信度调整模型更新。

Result: 在三个公共数据集上的大量实验表明，NAPO在推荐准确性和减少流行度偏差方面优于现有方法。

Conclusion: NAPO在推荐准确性和减少流行度偏差方面优于现有方法。

Abstract: Recommendation systems leverage user interaction data to suggest relevant
items while filtering out irrelevant (negative) ones. The rise of large
language models (LLMs) has garnered increasing attention for their potential in
recommendation tasks. However, existing methods for optimizing LLM-based
recommenders face challenges in effectively utilizing negative samples. Simply
integrating large numbers of negative samples can improve ranking accuracy and
mitigate popularity bias but often leads to increased computational overhead
and memory costs. Additionally, current approaches fail to account for the
varying informativeness of negative samples, leading to suboptimal optimization
performance. To address these issues, we propose NAPO
(\textbf{N}egative-\textbf{A}ware \textbf{P}reference \textbf{O}ptimization),
an enhanced framework for preference optimization in LLM-based recommendation.
NAPO introduces two key innovations: (1) in-batch negative sharing, which
expands the pool of negative samples without additional memory overhead, and
(2) dynamic reward margin adjustment, which adapts model updates based on the
confidence of negative samples. Extensive experiments on three public datasets
demonstrate that NAPO outperforms existing methods in both recommendation
accuracy and popularity bias reduction.

</details>


### [131] [Multimodal Fusion And Sparse Attention-based Alignment Model for Long Sequential Recommendation](https://arxiv.org/abs/2508.09664)
*Yongrui Fu,Jian Liu,Tao Li,Zonggang Wu,Shouke Qin,Hanmeng Liu*

Main category: cs.IR

TL;DR: MUFASA：一种用于长序列推荐的多模态融合和基于稀疏注意力的对齐模型，它利用多模态项目序列并挖掘多粒度用户兴趣。


<details>
  <summary>Details</summary>
Motivation: 有效地利用多模态项目序列和挖掘多粒度用户兴趣以大大缩小内容理解和推荐之间的差距仍然具有挑战性。

Method: 提出了一种用于长序列推荐的 MUltimodal Fusion And Sparse Attention-based Alignment 模型 MUFASA。

Result: MFL 利用项目标题作为跨类型语义锚，并通过四种定制损失的联合目标进行训练，这些损失促进：(i) 跨类型语义对齐，(ii) 与推荐的协作空间对齐，(iii) 保持标题定义的相似性结构并防止模态表示崩溃，以及 (iv) 融合空间。SAL 通过多粒度稀疏注意力机制扩展到长用户行为序列，该机制结合了窗口注意力、块级注意力和选择性注意力，以分层和跨时间范围捕获用户兴趣。SAL 显式地对连贯兴趣块的演变和细粒度的块内变化进行建模，从而产生强大的用户和项目表示。

Conclusion: MUFASA 在实际基准测试中始终优于最先进的基线。此外，在线 A/B 测试表明，在生产中获得了显着收益，证实了 MUFASA 在利用多模态线索和准确捕获各种用户偏好方面的有效性。

Abstract: Recent advances in multimodal recommendation enable richer item
understanding, while modeling users' multi-scale interests across temporal
horizons has attracted growing attention. However, effectively exploiting
multimodal item sequences and mining multi-grained user interests to
substantially bridge the gap between content comprehension and recommendation
remain challenging. To address these issues, we propose MUFASA, a MUltimodal
Fusion And Sparse Attention-based Alignment model for long sequential
recommendation. Our model comprises two core components. First, the Multimodal
Fusion Layer (MFL) leverages item titles as a cross-genre semantic anchor and
is trained with a joint objective of four tailored losses that promote: (i)
cross-genre semantic alignment, (ii) alignment to the collaborative space for
recommendation, (iii) preserving the similarity structure defined by titles and
preventing modality representation collapse, and (iv) distributional
regularization of the fusion space. This yields high-quality fused item
representations for further preference alignment. Second, the Sparse
Attention-guided Alignment Layer (SAL) scales to long user-behavior sequences
via a multi-granularity sparse attention mechanism, which incorporates windowed
attention, block-level attention, and selective attention, to capture user
interests hierarchically and across temporal horizons. SAL explicitly models
both the evolution of coherent interest blocks and fine-grained intra-block
variations, producing robust user and item representations. Extensive
experiments on real-world benchmarks show that MUFASA consistently surpasses
state-of-the-art baselines. Moreover, online A/B tests demonstrate significant
gains in production, confirming MUFASA's effectiveness in leveraging multimodal
cues and accurately capturing diverse user preferences.

</details>


### [132] [Describe What You See with Multimodal Large Language Models to Enhance Video Recommendations](https://arxiv.org/abs/2508.09789)
*Marco De Nadai,Andreas Damianou,Mounia Lalmas*

Main category: cs.IR

TL;DR: 使用大型多模态语言模型来提取视频中的高级语义，以改进视频推荐系统。


<details>
  <summary>Details</summary>
Motivation: 现有的视频推荐系统主要依赖于用户定义的元数据或由专用编码器提取的低级视觉和声学信号，这些低级特征描述了屏幕上出现的内容，但忽略了更深层次的语义，例如意图、幽默和世界知识，而这些语义会引起观众的共鸣。

Method: 通过提示一个现成的多模态大型语言模型（MLLM）将每个剪辑总结成丰富的自然语言描述，从而将高级语义注入推荐流程中。

Result: 在模拟用户与 TikTok 风格视频交互的 MicroLens-100K 数据集上，该框架在五个代表性模型中始终优于传统的视频、音频和元数据特征。

Conclusion: 利用大型多模态语言模型作为即时知识提取器，构建更具意图感知的视频推荐器具有潜力。

Abstract: Existing video recommender systems rely primarily on user-defined metadata or
on low-level visual and acoustic signals extracted by specialised encoders.
These low-level features describe what appears on the screen but miss deeper
semantics such as intent, humour, and world knowledge that make clips resonate
with viewers. For example, is a 30-second clip simply a singer on a rooftop, or
an ironic parody filmed amid the fairy chimneys of Cappadocia, Turkey? Such
distinctions are critical to personalised recommendations yet remain invisible
to traditional encoding pipelines. In this paper, we introduce a simple,
recommendation system-agnostic zero-finetuning framework that injects
high-level semantics into the recommendation pipeline by prompting an
off-the-shelf Multimodal Large Language Model (MLLM) to summarise each clip
into a rich natural-language description (e.g. "a superhero parody with
slapstick fights and orchestral stabs"), bridging the gap between raw content
and user intent. We use MLLM output with a state-of-the-art text encoder and
feed it into standard collaborative, content-based, and generative
recommenders. On the MicroLens-100K dataset, which emulates user interactions
with TikTok-style videos, our framework consistently surpasses conventional
video, audio, and metadata features in five representative models. Our findings
highlight the promise of leveraging MLLMs as on-the-fly knowledge extractors to
build more intent-aware video recommenders.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [133] [Efficient Real-Time Aircraft ETA Prediction via Feature Tokenization Transformer](https://arxiv.org/abs/2508.09144)
*Liping Huang,Yicheng Zhang,Yifang Yin,Sheng Zhang,Yi Zhang*

Main category: cs.LG

TL;DR: This paper introduces a Transformer model for efficient and accurate real-time aircraft ETA prediction, outperforming XGBoost in both accuracy and computing time.


<details>
  <summary>Details</summary>
Motivation: Real-time ETA prediction is crucial for arrival management in aviation, and prediction efficiency is as important as accuracy.

Method: A feature tokenization-based Transformer model is used to predict aircraft ETA.

Result: The proposed method improves ETA prediction accuracy by 7% compared to XGBoost, while requiring only 39% of its computing time. The ETA inference time is only 51.7 microseconds with 40 aircraft.

Conclusion: The proposed Transformer model outperforms XGBoost in ETA prediction accuracy by 7% while requiring only 39% of its computing time. With 40 aircraft, the ETA inference time is only 51.7 microseconds.

Abstract: Estimated time of arrival (ETA) for airborne aircraft in real-time is crucial
for arrival management in aviation, particularly for runway sequencing. Given
the rapidly changing airspace context, the ETA prediction efficiency is as
important as its accuracy in a real-time arrival aircraft management system. In
this study, we utilize a feature tokenization-based Transformer model to
efficiently predict aircraft ETA. Feature tokenization projects raw inputs to
latent spaces, while the multi-head self-attention mechanism in the Transformer
captures important aspects of the projections, alleviating the need for complex
feature engineering. Moreover, the Transformer's parallel computation
capability allows it to handle ETA requests at a high frequency, i.e., 1HZ,
which is essential for a real-time arrival management system. The model inputs
include raw data, such as aircraft latitude, longitude, ground speed, theta
degree for the airport, day and hour from track data, the weather context, and
aircraft wake turbulence category. With a data sampling rate of 1HZ, the ETA
prediction is updated every second. We apply the proposed aircraft ETA
prediction approach to Singapore Changi Airport (ICAO Code: WSSS) using
one-month Automatic Dependent Surveillance-Broadcast (ADS-B) data from October
1 to October 31, 2022. In the experimental evaluation, the ETA modeling covers
all aircraft within a range of 10NM to 300NM from WSSS. The results show that
our proposed method method outperforms the commonly used boosting tree based
model, improving accuracy by 7\% compared to XGBoost, while requiring only 39\%
of its computing time. Experimental results also indicate that, with 40
aircraft in the airspace at a given timestamp, the ETA inference time is only
51.7 microseconds, making it promising for real-time arrival management
systems.

</details>


### [134] [Presenting DiaData for Research on Type 1 Diabetes](https://arxiv.org/abs/2508.09160)
*Beyza Cinar,Maria Maleshkova*

Main category: cs.LG

TL;DR: 本研究整合了15个糖尿病数据集，建立了一个大型数据库，用于预测血糖水平和提供早期预警，但数据质量存在挑战。


<details>
  <summary>Details</summary>
Motivation: 1型糖尿病(T1D)是一种自身免疫性疾病，会导致产生胰岛素的细胞被破坏，从而导致胰岛素缺乏，因此受影响的个体依赖于外部胰岛素注射。然而，胰岛素会降低血糖水平，并可能导致低血糖。糖尿病和低血糖研究受到大型数据集不可用的限制。

Method: 系统地整合15个数据集

Result: 数据质量评估显示，数据不平衡和缺失值提出了重大挑战。此外，对血糖水平和心率数据进行相关性研究，显示低血糖前15到55分钟之间存在关联。

Conclusion: 集成了15个数据集，提供了一个包含2510个受试者的大型数据库，这些受试者的血糖测量值每5分钟记录一次。总共包括1.49亿次测量，其中4%代表低血糖范围内的值。此外，还提取了两个子数据库。

Abstract: Type 1 diabetes (T1D) is an autoimmune disorder that leads to the destruction
of insulin-producing cells, resulting in insulin deficiency, as to why the
affected individuals depend on external insulin injections. However, insulin
can decrease blood glucose levels and can cause hypoglycemia. Hypoglycemia is a
severe event of low blood glucose levels ($\le$70 mg/dL) with dangerous side
effects of dizziness, coma, or death. Data analysis can significantly enhance
diabetes care by identifying personal patterns and trends leading to adverse
events. Especially, machine learning (ML) models can predict glucose levels and
provide early alarms. However, diabetes and hypoglycemia research is limited by
the unavailability of large datasets. Thus, this work systematically integrates
15 datasets to provide a large database of 2510 subjects with glucose
measurements recorded every 5 minutes. In total, 149 million measurements are
included, of which 4% represent values in the hypoglycemic range. Moreover, two
sub-databases are extracted. Sub-database I includes demographics, and
sub-database II includes heart rate data. The integrated dataset provides an
equal distribution of sex and different age levels. As a further contribution,
data quality is assessed, revealing that data imbalance and missing values
present a significant challenge. Moreover, a correlation study on glucose
levels and heart rate data is conducted, showing a relation between 15 and 55
minutes before hypoglycemia.

</details>


### [135] [MoLAN: A Unified Modality-Aware Noise Dynamic Editing Framework for Multimodal Sentiment Analysis](https://arxiv.org/abs/2508.09145)
*Xingle Xu,Yongkang Liu,Dexian Cai,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.LG

TL;DR: MoLAN, a modality-aware noise dynamic editing framework, addresses the challenge of irrelevant information in multimodal sentiment analysis by fine-grained noise suppression. MoLAN+ achieves state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Multimodal sentiment analysis struggles with irrelevant or misleading visual and auditory information, and existing approaches risk losing critical information by treating entire modality information as an independent unit.

Method: Modality-aware noise dynamic editing framework (MoLAN) which divides modality features into blocks and assigns denoising strength based on noise level and semantic relevance.

Result: Experiments across five models and four datasets demonstrate the broad effectiveness of the MoLAN framework.

Conclusion: MoLAN+ achieves state-of-the-art performance on multimodal sentiment analysis.

Abstract: Multimodal Sentiment Analysis aims to integrate information from various
modalities, such as audio, visual, and text, to make complementary predictions.
However, it often struggles with irrelevant or misleading visual and auditory
information. Most existing approaches typically treat the entire modality
information (e.g., a whole image, audio segment, or text paragraph) as an
independent unit for feature enhancement or denoising. They often suppress the
redundant and noise information at the risk of losing critical information. To
address this challenge, we propose MoLAN, a unified ModaLity-aware noise
dynAmic editiNg framework. Specifically, MoLAN performs modality-aware blocking
by dividing the features of each modality into multiple blocks. Each block is
then dynamically assigned a distinct denoising strength based on its noise
level and semantic relevance, enabling fine-grained noise suppression while
preserving essential multimodal information. Notably, MoLAN is a unified and
flexible framework that can be seamlessly integrated into a wide range of
multimodal models. Building upon this framework, we further introduce MoLAN+, a
new multimodal sentiment analysis approach. Experiments across five models and
four datasets demonstrate the broad effectiveness of the MoLAN framework.
Extensive evaluations show that MoLAN+ achieves the state-of-the-art
performance. The code is publicly available at
https://github.com/betterfly123/MoLAN-Framework.

</details>


### [136] [To Theoretically Understand Transformer-Based In-Context Learning for Optimizing CSMA](https://arxiv.org/abs/2508.09146)
*Shugang Hao,Hongbo Li,Lingjie Duan*

Main category: cs.LG

TL;DR: This paper proposes an LLM-based optimizer for WiFi 7 channel access that uses in-context learning to predict contention window thresholds, achieving near-optimal throughput even with imperfect data and unknown node densities.


<details>
  <summary>Details</summary>
Motivation: The binary exponential backoff scheme in WiFi 7 performs poorly in dynamic channel environments, and existing model-based approaches suffer from throughput loss due to inaccurate node density estimation.

Method: The paper designs a transformer-based ICL optimizer that uses pre-collected collision-threshold data examples and a query collision case as a prompt to predict the contention window threshold (CWT). An efficient algorithm is developed to train the transformer for effective ICL.

Result: The proposed optimizer maintains minimal prediction and throughput deviations from the optimal values. Experimental results on NS-3 demonstrate fast convergence and near-optimal throughput over existing approaches under unknown node densities.

Conclusion: The proposed LLM transformer-based in-context learning (ICL) optimizer achieves fast convergence and near-optimal throughput compared to existing model-based and DRL-based approaches, even with erroneous data input.

Abstract: The binary exponential backoff scheme is widely used in WiFi 7 and still
incurs poor throughput performance under dynamic channel environments. Recent
model-based approaches (e.g., non-persistent and $p$-persistent CSMA) simply
optimize backoff strategies under a known and fixed node density, still leading
to a large throughput loss due to inaccurate node density estimation. This
paper is the first to propose LLM transformer-based in-context learning (ICL)
theory for optimizing channel access. We design a transformer-based ICL
optimizer to pre-collect collision-threshold data examples and a query
collision case. They are constructed as a prompt as the input for the
transformer to learn the pattern, which then generates a predicted contention
window threshold (CWT). To train the transformer for effective ICL, we develop
an efficient algorithm and guarantee a near-optimal CWT prediction within
limited training steps. As it may be hard to gather perfect data examples for
ICL in practice, we further extend to allow erroneous data input in the prompt.
We prove that our optimizer maintains minimal prediction and throughput
deviations from the optimal values. Experimental results on NS-3 further
demonstrate our approach's fast convergence and near-optimal throughput over
existing model-based and DRL-based approaches under unknown node densities.

</details>


### [137] [Motif 2.6B Technical Report](https://arxiv.org/abs/2508.09148)
*Junghwan Lim,Sungmin Lee,Dongseok Kim,Eunhwan Park,Hyunbyung Park,Junhyeok Lee,Wai Ting Cheung,Dahye Choi,Jaeheui Her,Jaeyeon Huh,Hanbin Jung,Changjin Kang,Beomgyu Kim,Jihwan Kim,Minjae Kim,Taehwan Kim,Youngrok Kim,Haesol Lee,Jeesoo Lee,Kungyu Lee,Dongpin Oh,Yeongjae Park,Bokki Ryu,Daewon Suh,Dongjoo Weon*

Main category: cs.LG

TL;DR: Motif-2.6B是一个26亿参数的基础模型，它通过创新的架构增强功能，在各种基准测试中都表现出色，且具有良好的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 开发一种既能实现高性能又能兼顾计算效率的基础LLM仍然具有挑战性，尤其对于新兴研究团队而言。

Method: 通过广泛的实验测试了多种新颖的架构组件，以确定Motif-2.6B的最佳架构，包括差分注意力和PolyNorm激活函数。

Result: Motif-2.6B始终达到或超过了同等规模的最新模型的性能。

Conclusion: Motif-2.6B在效率、可扩展性和性能方面超越了同等规模的模型，为未来的研究和部署提供了坚实的基础。

Abstract: Recent advancements in Large Language Models (LLMs) have revolutionized
artificial intelligence, yet developing an effective foundational LLM that
balances high performance with computational efficiency remains challenging,
especially for emerging research groups. To address this gap, we introduce
Motif-2.6B, a 2.6-billion-parameter foundation model designed to democratize
advanced LLM capabilities. Motif-2.6B incorporates several innovative
architectural enhancements, including Differential Attention and PolyNorm
activation functions, which improve long-context comprehension, reduce
hallucination, and enhance in-context learning capabilities. We rigorously
tested multiple novel architectural components through extensive
experimentation to determine the optimal architecture for Motif-2.6B.
Comprehensive evaluations demonstrate that Motif-2.6B consistently meets or
exceeds the performance of similarly sized state-of-the-art models across
diverse benchmarks, showcasing its effectiveness, scalability, and real-world
applicability. Through detailed experiments and tailored techniques, Motif-2.6B
significantly advances the landscape of efficient, scalable, and powerful
foundational LLMs, offering valuable insights and a robust foundation for
future research and deployment.

</details>


### [138] [JustDense: Just using Dense instead of Sequence Mixer for Time Series analysis](https://arxiv.org/abs/2508.09153)
*TaekHyun Park,Yongjae Lee,Daesan Park,Dohee Kim,Hyerim Bae*

Main category: cs.LG

TL;DR: 用密集层替换序列混合器可以提升时间序列分析 (TSA) 模型的性能。


<details>
  <summary>Details</summary>
Motivation: 质疑复杂序列混合器的必要性，例如注意力机制，表明更简单的架构可以实现相当甚至更优越的性能。

Method: 用密集层系统地替换各种已建立的 TSA 模型中的序列混合器。

Result: 用密集层替换序列混合器产生相当甚至更好的性能。

Conclusion: 用密集层替换序列混合器可以产生相当甚至更好的性能。在专用序列混合器仍然提供优势的情况下，JustDense 挑战了“更深、更复杂的架构本质上更好”的假设。

Abstract: Sequence and channel mixers, the core mechanism in sequence models, have
become the de facto standard in time series analysis (TSA). However, recent
studies have questioned the necessity of complex sequence mixers, such as
attention mechanisms, demonstrating that simpler architectures can achieve
comparable or even superior performance. This suggests that the benefits
attributed to complex sequencemixers might instead emerge from other
architectural or optimization factors. Based on this observation, we pose a
central question: Are common sequence mixers necessary for time-series
analysis? Therefore, we propose JustDense, an empirical study that
systematically replaces sequence mixers in various well-established TSA models
with dense layers. Grounded in the MatrixMixer framework, JustDense treats any
sequence mixer as a mixing matrix and replaces it with a dense layer. This
substitution isolates the mixing operation, enabling a clear theoretical
foundation for understanding its role. Therefore, we conducted extensive
experiments on 29 benchmarks covering five representative TSA tasks using seven
state-of-the-art TSA models to address our research question. The results show
that replacing sequence mixers with dense layers yields comparable or even
superior performance. In the cases where dedicated sequence mixers still offer
benefits, JustDense challenges the assumption that "deeper and more complex
architectures are inherently better" in TSA.

</details>


### [139] [Peer Effect Estimation in the Presence of Simultaneous Feedback and Unobserved Confounders](https://arxiv.org/abs/2508.09154)
*Xiaojing Du,Jiuyong Li,Lin Liu,Debo Cheng,Thuc. Le*

Main category: cs.LG

TL;DR: DIG2RSI是一种新的深度学习框架，它利用I-G变换和2SRI来解决同伴效应估计中的同时反馈和未观察到的混淆问题，并在半合成基准和真实世界数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在复杂的现实世界网络（如社交网络）中估计同伴因果效应具有挑战性，这主要是由于同伴之间的同步反馈和未观察到的混淆因素。现有的方法要么解决未观察到的混淆因素，而忽略了同步反馈，要么考虑了反馈，但在线性假设的限制下，因此无法获得准确的同伴效应估计。

Method: 本文提出了一种新的深度学习框架DIG2RSI，该框架利用I-G变换（矩阵运算）和2SRI（一种工具变量或IV技术）来解决同时反馈和未观察到的混淆，同时适应复杂的、非线性和高维关系。

Result: 我们证明了估计量在标准正则性条件下的相合性，确保了真实同伴效应的渐近恢复。

Conclusion: DIG2RSI在半合成基准和真实世界数据集上的实验结果表明，其性能优于现有方法。

Abstract: Estimating peer causal effects within complex real-world networks such as
social networks is challenging, primarily due to simultaneous feedback between
peers and unobserved confounders. Existing methods either address unobserved
confounders while ignoring the simultaneous feedback, or account for feedback
but under restrictive linear assumptions, thus failing to obtain accurate peer
effect estimation. In this paper, we propose DIG2RSI, a novel Deep learning
framework which leverages I-G transformation (matrix operation) and 2SRI (an
instrumental variable or IV technique) to address both simultaneous feedback
and unobserved confounding, while accommodating complex, nonlinear and
high-dimensional relationships. DIG2RSI first applies the I-G transformation to
disentangle mutual peer influences and eliminate the bias due to the
simultaneous feedback. To deal with unobserved confounding, we first construct
valid IVs from network data. In stage 1 of 2RSI, we train a neural network on
these IVs to predict peer exposure, and extract residuals as proxies for the
unobserved confounders. In the stage 2, we fit a separate neural network
augmented by an adversarial discriminator that incorporates these residuals as
a control function and enforces the learned representation to contain no
residual confounding signal. The expressive power of deep learning models in
capturing complex non-linear relationships and adversarial debiasing enhances
the effectiveness of DIG2RSI in eliminating bias from both feedback loops and
hidden confounders. We prove consistency of our estimator under standard
regularity conditions, ensuring asymptotic recovery of the true peer effect.
Empirical results on two semi-synthetic benchmarks and a real-world dataset
demonstrate that DIG2RSI outperforms existing approaches.

</details>


### [140] [A Rolling Stone Gathers No Moss: Adaptive Policy Optimization for Stable Self-Evaluation in Large Multimodal Models](https://arxiv.org/abs/2508.09155)
*Wenkai Wang,Hongcan Guo,Zheqi Lv,Shengyu Zhang*

Main category: cs.LG

TL;DR: 提出AdaPO，一个在线强化学习框架，通过自适应调整训练目标，显著提升了模型的直接推理和自我评估能力。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型（LMM）的自我评估能力对于在多轮对话中实现自我改进至关重要，但基础模型中普遍缺乏这种能力。最近的研究工作已经采用强化学习（RL）来增强自我评估；然而，当优化多个训练目标时，其固定的奖励机制会遭受奖励利用，从而导致模型崩溃。

Method: 提出了一种在线强化学习框架AdaPO，该框架能够根据每个任务的当前训练状态实时自适应地调整训练目标。AdaPO引入了一个自适应奖励模型（ARM）和一个奖励感知动态KL正则化机制。

Result: 在8个基准测试和各种模型上的大量实验表明，该方法显著提高了直接推理和自我评估能力。

Conclusion: 该方法通过自适应调整训练目标，显著提升了模型的直接推理和自我评估能力，并在8个基准测试和各种模型上进行了广泛的实验验证。

Abstract: Self-evaluation, a model's ability to assess the correctness of its own
output, is crucial for Large Multimodal Models (LMMs) to achieve
self-improvement in multi-turn conversations, yet largely absent in foundation
models. Recent work has employed reinforcement learning (RL) to enhance
self-evaluation; however, its fixed reward mechanism suffers from reward
hacking when optimizing multiple training objectives, leading to model
collapse. In this paper we propose AdaPO, an online reinforcement learning
framework capable of adaptively adjusting training objective in real time
according to the current training state for each task. Specifically, to
mitigate reward hacking , AdaPO introduces an Adaptive Reward Model (ARM) and a
Reward Aware Dynamic KL Regularization mechanism. ARM assesses the task's
training state from the distribution of model generated multi-turn
trajectories' performance. Reward Aware Dynamic KL replaces a fixed penalty
with dynamic coefficients which is modulated by the reward gap between
different multi-turn situations. Notably, our method automatically and smoothly
adjusts its learning focus based on sub-tasks' training progress without manual
intervention. Extensive experiments over 8 benchmarks and various models show
that our method significantly enhances both direct reasoning and
self-evaluation capability. We will release our code to contribute to the
community.

</details>


### [141] [Physics-Constrained Fine-Tuning of Flow-Matching Models for Generation and Inverse Problems](https://arxiv.org/abs/2508.09156)
*Jan Tauberschmidt,Sophie Fellenz,Sebastian J. Vollmer,Andrew B. Duncan*

Main category: cs.LG

TL;DR: 该论文提出了一种微调流匹配生成模型的新框架，用于在科学系统中执行物理约束和解决反问题。


<details>
  <summary>Details</summary>
Motivation: 论文提出了一个框架，用于微调流匹配生成模型，以加强物理约束并解决科学系统中的反问题。

Method: 该方法采用可微分的后训练程序，以最小化控制偏微分方程 (PDE) 的弱形式残差。

Result: 该模型能够生成物理上有效的场解，并对隐藏参数进行合理的估计，有效地解决了数据驱动但具有物理感知方式的不适定反问题。在典型 PDE 基准上验证了该方法，证明了其改进了 PDE 约束的满足度并准确地恢复了潜在系数。

Conclusion: 该模型通过结合生成建模和科学推断，为模拟增强发现和物理系统的数据高效建模开辟了新途径。

Abstract: We present a framework for fine-tuning flow-matching generative models to
enforce physical constraints and solve inverse problems in scientific systems.
Starting from a model trained on low-fidelity or observational data, we apply a
differentiable post-training procedure that minimizes weak-form residuals of
governing partial differential equations (PDEs), promoting physical consistency
and adherence to boundary conditions without distorting the underlying learned
distribution. To infer unknown physical inputs, such as source terms, material
parameters, or boundary data, we augment the generative process with a
learnable latent parameter predictor and propose a joint optimization strategy.
The resulting model produces physically valid field solutions alongside
plausible estimates of hidden parameters, effectively addressing ill-posed
inverse problems in a data-driven yet physicsaware manner. We validate our
method on canonical PDE benchmarks, demonstrating improved satisfaction of PDE
constraints and accurate recovery of latent coefficients. Our approach bridges
generative modelling and scientific inference, opening new avenues for
simulation-augmented discovery and data-efficient modelling of physical
systems.

</details>


### [142] [EvaDrive: Evolutionary Adversarial Policy Optimization for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.09158)
*Siwen Jiao,Kangan Qian,Hao Ye,Yang Zhong,Ziang Luo,Sicong Jiang,Zilin Huang,Yangyi Fang,Jinyu Miao,Zheng Fu,Yunlong Wang,Kun Jiang,Diange Yang,Rui Fan,Baoyun Peng*

Main category: cs.LG

TL;DR: EvaDrive: a multi-objective reinforcement learning framework with adversarial optimization for autonomous driving, achieving state-of-the-art performance and diverse driving styles.


<details>
  <summary>Details</summary>
Motivation: Current generation-evaluation frameworks isolate trajectory generation from quality assessment, preventing iterative refinement. Reinforcement learning methods collapse multi-dimensional preferences into scalar rewards, obscuring critical trade-offs and yielding scalarization bias.

Method: A novel multi-objective reinforcement learning framework that establishes genuine closed-loop co-evolution between trajectory generation and evaluation via adversarial optimization. It frames trajectory planning as a multi-round adversarial game with a hierarchical generator and a trainable multi-objective critic.

Result: Achieves 94.9 PDMS on NAVSIM v1 and 64.96 Driving Score on Bench2Drive, demonstrating SOTA performance and generating diverse driving styles via dynamic weighting without external preference data.

Conclusion: EvaDrive introduces a closed-loop adversarial framework for human-like iterative decision-making, providing a novel scalarization-free trajectory optimization approach, achieving SOTA performance on NAVSIM and Bench2Drive benchmarks.

Abstract: Autonomous driving faces significant challenges in achieving human-like
iterative decision-making, which continuously generates, evaluates, and refines
trajectory proposals. Current generation-evaluation frameworks isolate
trajectory generation from quality assessment, preventing iterative refinement
essential for planning, while reinforcement learning methods collapse
multi-dimensional preferences into scalar rewards, obscuring critical
trade-offs and yielding scalarization bias.To overcome these issues, we present
EvaDrive, a novel multi-objective reinforcement learning framework that
establishes genuine closed-loop co-evolution between trajectory generation and
evaluation via adversarial optimization. EvaDrive frames trajectory planning as
a multi-round adversarial game. In this game, a hierarchical generator
continuously proposes candidate paths by combining autoregressive intent
modeling for temporal causality with diffusion-based refinement for spatial
flexibility. These proposals are then rigorously assessed by a trainable
multi-objective critic that explicitly preserves diverse preference structures
without collapsing them into a single scalarization bias.This adversarial
interplay, guided by a Pareto frontier selection mechanism, enables iterative
multi-round refinement, effectively escaping local optima while preserving
trajectory diversity.Extensive experiments on NAVSIM and Bench2Drive benchmarks
demonstrate SOTA performance, achieving 94.9 PDMS on NAVSIM v1 (surpassing
DiffusionDrive by 6.8, DriveSuprim by 5.0, and TrajHF by 0.9) and 64.96 Driving
Score on Bench2Drive. EvaDrive generates diverse driving styles via dynamic
weighting without external preference data, introducing a closed-loop
adversarial framework for human-like iterative decision-making, offering a
novel scalarization-free trajectory optimization approach.

</details>


### [143] [Multimodal RAG Enhanced Visual Description](https://arxiv.org/abs/2508.09170)
*Amit Kumar Jaiswal,Haiming Liu,Ingo Frommholz*

Main category: cs.LG

TL;DR: 提出了一种轻量级的无训练RAG方法，通过线性映射和迭代提炼，有效弥合了多模态模型中的模态差距，并在图像描述生成方面取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 预训练的大型多模态模型（LMM）存在模态差距，即文本和视觉表示在通用嵌入空间中存在错位。微调虽然可以缓解这个问题，但由于需要大量的领域驱动数据，通常成本高昂且不切实际。

Method: 提出了一种轻量级的无训练方法，利用检索增强生成（RAG）通过线性映射跨模态扩展，并引入迭代技术，通过生成合成描述来提炼映射。

Result: 在两个基准多模态数据集上的实验结果表明，该方法取得了显著的改进。

Conclusion: 通过利用检索增强生成（RAG）和线性映射，以及迭代提炼映射的技术，显著提高了多模态数据集上的图像描述生成效果。

Abstract: Textual descriptions for multimodal inputs entail recurrent refinement of
queries to produce relevant output images. Despite efforts to address
challenges such as scaling model size and data volume, the cost associated with
pre-training and fine-tuning remains substantial. However, pre-trained large
multimodal models (LMMs) encounter a modality gap, characterised by a
misalignment between textual and visual representations within a common
embedding space. Although fine-tuning can potentially mitigate this gap, it is
typically expensive and impractical due to the requirement for extensive
domain-driven data. To overcome this challenge, we propose a lightweight
training-free approach utilising Retrieval-Augmented Generation (RAG) to extend
across the modality using a linear mapping, which can be computed efficiently.
During inference, this mapping is applied to images embedded by an LMM enabling
retrieval of closest textual descriptions from the training set. These textual
descriptions, in conjunction with an instruction, cater as an input prompt for
the language model to generate new textual descriptions. In addition, we
introduce an iterative technique for distilling the mapping by generating
synthetic descriptions via the language model facilitating optimisation for
standard utilised image description measures. Experimental results on two
benchmark multimodal datasets demonstrate significant improvements.

</details>


### [144] [Physics-Guided Memory Network for Building Energy Modeling](https://arxiv.org/abs/2508.09161)
*Muhammad Umair Danish,Kashif Ali,Kamran Siddiqui,Katarina Grolinger*

Main category: cs.LG

TL;DR: This paper introduces a Physics-Guided Memory Network (PgMN) for energy consumption forecasting, which combines deep learning and physics-based models to overcome the limitations of each approach, especially in scenarios with limited data or newly constructed buildings.


<details>
  <summary>Details</summary>
Motivation: Accurate energy consumption forecasting is essential for efficient resource management and sustainability in the building sector. Deep learning models are highly successful but struggle with limited historical data and become unusable when historical data are unavailable, such as in newly constructed buildings. On the other hand, physics-based models, such as EnergyPlus, simulate energy consumption without relying on historical data but require extensive building parameter specifications and considerable time to model a building.

Method: This paper introduces a Physics-Guided Memory Network (PgMN), a neural network that integrates predictions from deep learning and physics-based models to address their limitations. PgMN comprises a Parallel Projection Layers to process incomplete inputs, a Memory Unit to account for persistent biases, and a Memory Experience Module to optimally extend forecasts beyond their input range and produce output.

Result: Experimental validation shows accuracy and applicability of PgMN in diverse scenarios such as newly constructed buildings, missing data, sparse historical data, and dynamic infrastructure changes.

Conclusion: This paper provides a promising solution for energy consumption forecasting in dynamic building environments, enhancing model applicability in scenarios where historical data are limited or unavailable or when physics-based models are inadequate.

Abstract: Accurate energy consumption forecasting is essential for efficient resource
management and sustainability in the building sector. Deep learning models are
highly successful but struggle with limited historical data and become unusable
when historical data are unavailable, such as in newly constructed buildings.
On the other hand, physics-based models, such as EnergyPlus, simulate energy
consumption without relying on historical data but require extensive building
parameter specifications and considerable time to model a building. This paper
introduces a Physics-Guided Memory Network (PgMN), a neural network that
integrates predictions from deep learning and physics-based models to address
their limitations. PgMN comprises a Parallel Projection Layers to process
incomplete inputs, a Memory Unit to account for persistent biases, and a Memory
Experience Module to optimally extend forecasts beyond their input range and
produce output. Theoretical evaluation shows that components of PgMN are
mathematically valid for performing their respective tasks. The PgMN was
evaluated on short-term energy forecasting at an hourly resolution, critical
for operational decision-making in smart grid and smart building systems.
Experimental validation shows accuracy and applicability of PgMN in diverse
scenarios such as newly constructed buildings, missing data, sparse historical
data, and dynamic infrastructure changes. This paper provides a promising
solution for energy consumption forecasting in dynamic building environments,
enhancing model applicability in scenarios where historical data are limited or
unavailable or when physics-based models are inadequate.

</details>


### [145] [RicciFlowRec: A Geometric Root Cause Recommender Using Ricci Curvature on Financial Graphs](https://arxiv.org/abs/2508.09334)
*Zhongtian Sun,Anoushka Harit*

Main category: cs.LG

TL;DR: RicciFlowRec是第一个在金融决策支持中应用基于几何流推理的推荐器，它通过Ricci曲率和Ricci流在动态金融图上进行根本原因归因，并通过曲率梯度揭示因果子结构，从而改进了风险感知的排序。


<details>
  <summary>Details</summary>
Motivation: 量化股票、宏观经济指标和新闻之间不断变化的相互作用中的局部压力，并追踪冲击传播。

Method: 利用Ricci曲率和Ricci流在动态金融图上进行根本原因归因。

Result: 在具有基于FinBERT情绪的S＆P 500数据上的初步结果表明，在合成扰动下，稳健性和可解释性得到了改善。

Conclusion: RicciFlowRec通过曲率梯度揭示因果子结构，从而改进了风险感知的排序。

Abstract: We propose RicciFlowRec, a geometric recommendation framework that performs
root cause attribution via Ricci curvature and flow on dynamic financial
graphs. By modelling evolving interactions among stocks, macroeconomic
indicators, and news, we quantify local stress using discrete Ricci curvature
and trace shock propagation via Ricci flow. Curvature gradients reveal causal
substructures, informing a structural risk-aware ranking function. Preliminary
results on S\&P~500 data with FinBERT-based sentiment show improved robustness
and interpretability under synthetic perturbations. This ongoing work supports
curvature-based attribution and early-stage risk-aware ranking, with plans for
portfolio optimization and return forecasting. To our knowledge, RicciFlowRec
is the first recommender to apply geometric flow-based reasoning in financial
decision support.

</details>


### [146] [An Unsupervised Deep XAI Framework for Localization of Concurrent Replay Attacks in Nuclear Reactor Signals](https://arxiv.org/abs/2508.09162)
*Konstantinos Vasili,Zachery T. Dahm,William Richards,Stylianos Chatzidakis*

Main category: cs.LG

TL;DR: This paper proposes an unsupervised explainable AI framework to characterize real-time replay attacks in nuclear reactors using real data. The framework was able to detect and identify the source and number of signals being replayed and the duration of the falsification with 95 percent or better accuracy.


<details>
  <summary>Details</summary>
Motivation: Ensuring data integrity against deception attacks is becoming increasingly important for networked communication and a requirement for safe and reliable operation. Current efforts to address replay attacks, almost universally focus on watermarking or supervised anomaly detection approaches without further identifying and characterizing the root cause of the anomaly. In addition, these approaches rely mostly on synthetic data with uncorrelated Gaussian process and measurement noise and full state feedback or are limited to univariate signals, signal stationarity, linear quadratic regulators, or other linear-time invariant state-space which may fail to capture any unmodeled system dynamics. In the realm of regulated nuclear cyber-physical systems, additional work is needed on characterization of replay attacks and explainability of predictions using real data.

Method: an unsupervised explainable AI framework based on a combination of autoencoder and customized windowSHAP algorithm

Result: The XAI framework was able to detect and identify the source and number of signals being replayed and the duration of the falsification with 95 percent or better accuracy.

Conclusion: The proposed XAI framework was benchmarked on several real world datasets from Purdue's nuclear reactor PUR-1 with up to six signals concurrently being replayed. In all cases, the XAI framework was able to detect and identify the source and number of signals being replayed and the duration of the falsification with 95 percent or better accuracy.

Abstract: Next generation advanced nuclear reactors are expected to be smaller both in
size and power output, relying extensively on fully digital instrumentation and
control systems. These reactors will generate a large flow of information in
the form of multivariate time series data, conveying simultaneously various non
linear cyber physical, process, control, sensor, and operational states.
Ensuring data integrity against deception attacks is becoming increasingly
important for networked communication and a requirement for safe and reliable
operation. Current efforts to address replay attacks, almost universally focus
on watermarking or supervised anomaly detection approaches without further
identifying and characterizing the root cause of the anomaly. In addition,
these approaches rely mostly on synthetic data with uncorrelated Gaussian
process and measurement noise and full state feedback or are limited to
univariate signals, signal stationarity, linear quadratic regulators, or other
linear-time invariant state-space which may fail to capture any unmodeled
system dynamics. In the realm of regulated nuclear cyber-physical systems,
additional work is needed on characterization of replay attacks and
explainability of predictions using real data. Here, we propose an unsupervised
explainable AI framework based on a combination of autoencoder and customized
windowSHAP algorithm to fully characterize real-time replay attacks, i.e.,
detection, source identification, timing and type, of increasing complexity
during a dynamic time evolving reactor process. The proposed XAI framework was
benchmarked on several real world datasets from Purdue's nuclear reactor PUR-1
with up to six signals concurrently being replayed. In all cases, the XAI
framework was able to detect and identify the source and number of signals
being replayed and the duration of the falsification with 95 percent or better
accuracy.

</details>


### [147] [Energy-Efficient Stochastic Computing (SC) Neural Networks for Internet of Things Devices With Layer-Wise Adjustable Sequence Length (ASL)](https://arxiv.org/abs/2508.09163)
*Ziheng Wang,Pedro Reviriego,Farzad Niknia,Zhen Gao,Javier Conde,Shanshan Liu,Fabrizio Lombardi*

Main category: cs.LG

TL;DR: 本文提出了一种名为可调序列长度（ASL）的新方案，该方案将混合精度概念应用于随机计算（SC）神经网络，以减少能量消耗和延迟。


<details>
  <summary>Details</summary>
Motivation: 随机计算 (SC) 已经成为在资源受限的场景（如物联网 (IoT)）中部署神经网络 (NN) 的一种高效低功耗替代方案。通过将值编码为串行比特流，与传统的浮点 (FP) 设计相比，SC 显著降低了能量损耗；然而，SC 的逐层混合精度实现的进一步改进仍有待探索。

Method: 本文介绍了一种可调序列长度 (ASL) 方案，该方案专门将混合精度概念应用于 SC 神经网络。通过引入基于算子范数的理论模型，本文表明截断噪声可以通过估计的放大因子累积地传播通过各层。

Result: 扩展的灵敏度分析使用随机森林 (RF) 回归来评估多层截断效果，并验证理论预测与实际网络行为的一致性。为了适应不同的应用场景，本文提出了两种截断策略（粗粒度和细粒度），这两种策略在每一层应用不同的序列长度配置。

Conclusion: ASL 方案在 32nm 工艺合成的流水线 SC MLP 上的评估表明，ASL 可以减少高达 60% 以上的能量和延迟开销，而精度损失可忽略不计。它证实了 ASL 方案在物联网应用中的可行性，并强调了混合精度截断在 SC 设计中的独特优势。

Abstract: Stochastic computing (SC) has emerged as an efficient low-power alternative
for deploying neural networks (NNs) in resource-limited scenarios, such as the
Internet of Things (IoT). By encoding values as serial bitstreams, SC
significantly reduces energy dissipation compared to conventional
floating-point (FP) designs; however, further improvement of layer-wise
mixed-precision implementation for SC remains unexplored. This article
introduces Adjustable Sequence Length (ASL), a novel scheme that applies
mixed-precision concepts specifically to SC NNs. By introducing an
operator-norm-based theoretical model, this article shows that truncation noise
can cumulatively propagate through the layers by the estimated amplification
factors. An extended sensitivity analysis is presented, using random forest
(RF) regression to evaluate multilayer truncation effects and validate the
alignment of theoretical predictions with practical network behaviors. To
accommodate different application scenarios, this article proposes two
truncation strategies (coarse-grained and fine-grained), which apply diverse
sequence length configurations at each layer. Evaluations on a pipelined SC MLP
synthesized at 32nm demonstrate that ASL can reduce energy and latency
overheads by up to over 60% with negligible accuracy loss. It confirms the
feasibility of the ASL scheme for IoT applications and highlights the distinct
advantages of mixed-precision truncation in SC designs.

</details>


### [148] [Generating Feasible and Diverse Synthetic Populations Using Diffusion Models](https://arxiv.org/abs/2508.09164)
*Min Tang,Peng Lu,Qing Feng*

Main category: cs.LG

TL;DR: 提出了一种基于扩散模型的人口合成方法，该方法在可行性和多样性之间取得了更好的平衡。


<details>
  <summary>Details</summary>
Motivation: 由于维度灾难，当描述agent的属性数量变得很大时，调查数据通常无法密集地支持人口中属性的联合分布。这种稀疏性使得难以准确地建模和生成人口。

Method: 提出了一种新的基于扩散模型的群体合成方法来估计潜在的联合分布。

Result: 将该方法与其他最近提出的方法（如变分自编码器（VAE）和生成对抗网络（GAN）方法）进行了比较，这些方法已在高维表格人口合成中显示出成功。使用一系列指标评估合成输出的性能，包括边际分布相似性、可行性和多样性。结果表明，提出的方法优于以前的方法。

Conclusion: 提出的方法在可行性和合成人群的多样性之间取得了更好的平衡，优于以前的方法。

Abstract: Population synthesis is a critical task that involves generating synthetic
yet realistic representations of populations. It is a fundamental problem in
agent-based modeling (ABM), which has become the standard to analyze
intelligent transportation systems. The synthetic population serves as the
primary input for ABM transportation simulation, with traveling agents
represented by population members. However, when the number of attributes
describing agents becomes large, survey data often cannot densely support the
joint distribution of the attributes in the population due to the curse of
dimensionality. This sparsity makes it difficult to accurately model and
produce the population. Interestingly, deep generative models trained from
available sample data can potentially synthesize possible attribute
combinations that present in the actual population but do not exist in the
sample data(called sampling zeros). Nevertheless, this comes at the cost of
falsely generating the infeasible attribute combinations that do not exist in
the population (called structural zeros). In this study, a novel diffusion
model-based population synthesis method is proposed to estimate the underlying
joint distribution of a population. This approach enables the recovery of
numerous missing sampling zeros while keeping the generated structural zeros
minimal. Our method is compared with other recently proposed approaches such as
Variational Autoencoders (VAE) and Generative Adversarial Network (GAN)
approaches, which have shown success in high dimensional tabular population
synthesis. We assess the performance of the synthesized outputs using a range
of metrics, including marginal distribution similarity, feasibility, and
diversity. The results demonstrate that our proposed method outperforms
previous approaches in achieving a better balance between the feasibility and
diversity of the synthesized population.

</details>


### [149] [Masked Training for Robust Arrhythmia Detection from Digitalized Multiple Layout ECG Images](https://arxiv.org/abs/2508.09165)
*Shanwei Zhang,Deyun Zhang,Yirao Tao,Kexin Wang,Shijia Geng,Jun Li,Qinghao Zhao,Xingpeng Liu,Yuxi Zhou,Shenda Hong*

Main category: cs.LG

TL;DR: PatchECG, a new framework, addresses challenges in ECG analysis caused by varying layouts, achieving state-of-the-art results in arrhythmia recognition and atrial fibrillation diagnosis.


<details>
  <summary>Details</summary>
Motivation: Digitized ECG signals exhibit asynchronous lead time and partial blackout loss due to differences in ECG layouts, posing a challenge to existing models.

Method: The study introduced PatchECG, a framework for adaptive variable block count missing representation learning based on a masking training strategy.

Result: PatchECG achieved an average AUROC of 0.835 on the PTB-XL dataset and 21388 asynchronous ECG images. In external validation, the AUROC for atrial fibrillation diagnosis reached 0.778, and 0.893 on 12 x 1 layout ECGs, outperforming other methods including ECGFounder.

Conclusion: The proposed PatchECG method demonstrates strong robustness under different layouts, achieving superior performance compared to interpolation, baseline methods, and the ECGFounder model in arrhythmia recognition and atrial fibrillation diagnosis.

Abstract: Electrocardiogram (ECG) as an important tool for diagnosing cardiovascular
diseases such as arrhythmia. Due to the differences in ECG layouts used by
different hospitals, the digitized signals exhibit asynchronous lead time and
partial blackout loss, which poses a serious challenge to existing models. To
address this challenge, the study introduced PatchECG, a framework for adaptive
variable block count missing representation learning based on a masking
training strategy, which automatically focuses on key patches with
collaborative dependencies between leads, thereby achieving key recognition of
arrhythmia in ECGs with different layouts. Experiments were conducted on the
PTB-XL dataset and 21388 asynchronous ECG images generated using ECG image kit
tool, using the 23 Subclasses as labels. The proposed method demonstrated
strong robustness under different layouts, with average Area Under the Receiver
Operating Characteristic Curve (AUROC) of 0.835 and remained stable (unchanged
with layout changes). In external validation based on 400 real ECG images data
from Chaoyang Hospital, the AUROC for atrial fibrillation diagnosis reached
0.778; On 12 x 1 layout ECGs, AUROC reaches 0.893. This result is superior to
various classic interpolation and baseline methods, and compared to the current
optimal large-scale pre-training model ECGFounder, it has improved by 0.111 and
0.19.

</details>


### [150] [SVGen: Interpretable Vector Graphics Generation with Large Language Models](https://arxiv.org/abs/2508.09168)
*Feiyu Wang,Zhiyuan Zhao,Yuandong Liu,Da Zhang,Junyu Gao,Hao Sun,Xuelong Li*

Main category: cs.LG

TL;DR: 提出了SVGen模型和SVG-1M数据集，用于从文本生成SVG代码。


<details>
  <summary>Details</summary>
Motivation: 将创意转化为精确的矢量图形仍然是一个耗时的挑战。

Method: 提出了SVGen，一个端到端的模型，可以从自然语言输入生成SVG代码。该方法通过课程学习和强化学习优化，确保语义准确性和结构完整性。

Result: 构建了一个大规模的高质量SVG数据集，名为SVG-1M，其中包含与自然语言描述配对的SVG。通过先进的数据增强和注释，创建了对齐良好的文本到SVG的训练对，包括一个带有思维链注释的子集，以增强语义指导。

Conclusion: SVGen模型在生成SVG代码方面优于通用大型模型和传统渲染方法，在有效性和效率方面均有提升。

Abstract: Scalable Vector Graphics (SVG) is widely used in front-end development and
UI/UX design due to its scalability, editability, and rendering efficiency.
However, turning creative ideas into precise vector graphics remains a
time-consuming challenge. To address this, we introduce SVG-1M, a large-scale
dataset of high-quality SVGs paired with natural language descriptions. Through
advanced data augmentation and annotation, we create well-aligned Text to SVG
training pairs, including a subset with Chain of Thought annotations for
enhanced semantic guidance. Based on this dataset, we propose SVGen, an
end-to-end model that generates SVG code from natural language inputs. Our
approach ensures semantic accuracy and structural completeness, supported by
curriculum learning and reinforcement learning optimization. Experiments show
that SVGen outperforms general large models and traditional rendering methods
in both effectiveness and efficiency. Code, model, and dataset are available on
GitHub.

</details>


### [151] [FedMP: Tackling Medical Feature Heterogeneity in Federated Learning from a Manifold Perspective](https://arxiv.org/abs/2508.09174)
*Zhekai Zhou,Shudong Liu,Zhaokun Zhou,Yang Liu,Qiang Yang,Yuesheng Zhu,Guibo Luo*

Main category: cs.LG

TL;DR: FedMP 是一种新的联邦学习方法，旨在增强非 IID 场景下的 FL。


<details>
  <summary>Details</summary>
Motivation: 联邦学习 (FL) 是一种去中心化的机器学习范例，其中多个客户端协作训练共享模型，而无需共享其本地私有数据。然而，FL 的实际应用经常遇到由参与客户端中非独立同分布 (non-IID) 本地数据集带来的挑战，这在医学成像领域尤为明显，其中图像特征分布的变化会显着阻碍全局模型的收敛和性能。

Method: FedMP 采用随机特征流形补全来丰富个体客户端分类器的训练空间，并利用类原型来引导语义一致子空间内跨客户端的特征流形的对齐，从而促进更清晰的决策边界的构建。

Result: 实验结果表明 FedMP 优于现有的 FL 算法。

Conclusion: FedMP在多个医学图像数据集以及多域自然图像数据集上验证了其有效性，实验结果表明 FedMP 优于现有的 FL 算法。

Abstract: Federated learning (FL) is a decentralized machine learning paradigm in which
multiple clients collaboratively train a shared model without sharing their
local private data. However, real-world applications of FL frequently encounter
challenges arising from the non-identically and independently distributed
(non-IID) local datasets across participating clients, which is particularly
pronounced in the field of medical imaging, where shifts in image feature
distributions significantly hinder the global model's convergence and
performance. To address this challenge, we propose FedMP, a novel method
designed to enhance FL under non-IID scenarios. FedMP employs stochastic
feature manifold completion to enrich the training space of individual client
classifiers, and leverages class-prototypes to guide the alignment of feature
manifolds across clients within semantically consistent subspaces, facilitating
the construction of more distinct decision boundaries. We validate the
effectiveness of FedMP on multiple medical imaging datasets, including those
with real-world multi-center distributions, as well as on a multi-domain
natural image dataset. The experimental results demonstrate that FedMP
outperforms existing FL algorithms. Additionally, we analyze the impact of
manifold dimensionality, communication efficiency, and privacy implications of
feature exposure in our method.

</details>


### [152] [DQT: Dynamic Quantization Training via Dequantization-Free Nested Integer Arithmetic](https://arxiv.org/abs/2508.09176)
*Hazem Hesham Yousef Shalby,Fabrizio Pittorino,Francesca Palermo,Diana Trojaniello,Manuel Roveri*

Main category: cs.LG

TL;DR: DQT是一种新的量化框架，通过使用嵌套整数表示和定制的纯整数算术，实现了高效的动态量化，并在ImageNet上取得了state-of-the-art的性能。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的设备上部署深度神经网络依赖于量化。虽然静态的、统一的量化对所有输入应用固定的bit-width，但它无法适应它们不同的复杂性。动态的、基于实例的混合精度量化承诺通过只在需要时分配更高的精度来实现卓越的精度-效率权衡。然而，一个关键的瓶颈仍然存在:现有的方法需要一个昂贵的反量化到浮点和再量化到整数的循环来改变精度，打破了纯整数硬件范式，并损害了性能的提高。

Method: 论文介绍了一种新的框架，称为动态量化训练(DQT)，它消除了动态量化的瓶颈。DQT的核心是一种嵌套整数表示，其中较低精度值以bit-wise方式嵌入到较高精度值中。这种设计，加上定制的纯整数算术，允许通过接近零成本的位移操作进行on-the-fly的bit-width切换。

Result: 在ImageNet上，DQT的4-bit动态ResNet50实现了77.00%的top-1准确率，优于领先的静态(LSQ, 76.70%)和动态(DQNET,76.94%)方法，且BitOPs预算相当。DQT仅以28.3M简单位移操作的bit-width转换成本实现了这一点，与以前的动态方法所需的56.6M昂贵的乘法累加(MAC)浮点运算相比，有了巨大的改进。

Conclusion: DQT实现了在CIFAR-10上的ResNet18和ImageNet上的ResNet50的state-of-the-art性能。在ImageNet上，我们的4-bit动态ResNet50实现了77.00%的top-1准确率，优于领先的静态(LSQ, 76.70%)和动态(DQNET,76.94%)方法，且BitOPs预算相当。DQT仅以28.3M简单位移操作的bit-width转换成本实现了这一点，与以前的动态方法所需的56.6M昂贵的乘法累加(MAC)浮点运算相比，有了巨大的改进——开启了高效自适应AI的新领域。

Abstract: The deployment of deep neural networks on resource-constrained devices relies
on quantization. While static, uniform quantization applies a fixed bit-width
to all inputs, it fails to adapt to their varying complexity. Dynamic,
instance-based mixed-precision quantization promises a superior
accuracy-efficiency trade-off by allocating higher precision only when needed.
However, a critical bottleneck remains: existing methods require a costly
dequantize-to-float and requantize-to-integer cycle to change precision,
breaking the integer-only hardware paradigm and compromising performance gains.
This paper introduces Dynamic Quantization Training (DQT), a novel framework
that removes this bottleneck. At the core of DQT is a nested integer
representation where lower-precision values are bit-wise embedded within
higher-precision ones. This design, coupled with custom integer-only
arithmetic, allows for on-the-fly bit-width switching through a near-zero-cost
bit-shift operation. This makes DQT the first quantization framework to enable
both dequantization-free static mixed-precision of the backbone network, and
truly efficient dynamic, instance-based quantization through a lightweight
controller that decides at runtime how to quantize each layer. We demonstrate
DQT state-of-the-art performance on ResNet18 on CIFAR-10 and ResNet50 on
ImageNet. On ImageNet, our 4-bit dynamic ResNet50 achieves 77.00% top-1
accuracy, an improvement over leading static (LSQ, 76.70%) and dynamic (DQNET,
76.94%) methods at a comparable BitOPs budget. Crucially, DQT achieves this
with a bit-width transition cost of only 28.3M simple bit-shift operations, a
drastic improvement over the 56.6M costly Multiply-Accumulate (MAC)
floating-point operations required by previous dynamic approaches - unlocking a
new frontier in efficient, adaptive AI.

</details>


### [153] [scAGC: Learning Adaptive Cell Graphs with Contrastive Guidance for Single-Cell Clustering](https://arxiv.org/abs/2508.09180)
*Huifa Li,Jie Fu,Xinlin Zhuang,Haolin Yang,Xinpeng Ling,Tong Cheng,Haochen xue,Imran Razzak,Zhili Chen*

Main category: cs.LG

TL;DR: scAGC是一种单细胞聚类方法，它学习具有对比指导的自适应细胞图，优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 传统的聚类方法在高维度和scRNA-seq数据中普遍存在的零元素面前，面临着重大的统计和计算挑战。虽然一些先进的方法使用图神经网络来建模细胞-细胞关系，但它们通常依赖于对噪声敏感的静态图结构，并且无法捕捉单细胞群体中固有的长尾分布。

Method: 该方法优化特征表示和细胞图，同时以端到端的方式进行。具体来说，它引入了一个拓扑自适应图自动编码器，该编码器利用可微的Gumbel-Softmax抽样策略，在训练过程中动态地细化图结构。为了模拟scRNA-seq数据的离散、过度分散和零膨胀的性质，集成了零膨胀负二项式（ZINB）损失，用于稳健的特征重建。此外，还结合了对比学习目标，以规范图学习过程，防止图拓扑的突然变化，确保稳定性和增强收敛性。

Result: 在9个真实scRNA-seq数据集上的综合实验表明，scAGC始终优于其他最先进的方法，在9个和7个数据集上分别产生了最佳NMI和ARI评分。

Conclusion: scAGC在9个真实scRNA-seq数据集上表现优于其他先进方法，在9个和7个数据集上分别产生了最佳NMI和ARI评分。

Abstract: Accurate cell type annotation is a crucial step in analyzing single-cell RNA
sequencing (scRNA-seq) data, which provides valuable insights into cellular
heterogeneity. However, due to the high dimensionality and prevalence of zero
elements in scRNA-seq data, traditional clustering methods face significant
statistical and computational challenges. While some advanced methods use graph
neural networks to model cell-cell relationships, they often depend on static
graph structures that are sensitive to noise and fail to capture the
long-tailed distribution inherent in single-cell populations.To address these
limitations, we propose scAGC, a single-cell clustering method that learns
adaptive cell graphs with contrastive guidance. Our approach optimizes feature
representations and cell graphs simultaneously in an end-to-end manner.
Specifically, we introduce a topology-adaptive graph autoencoder that leverages
a differentiable Gumbel-Softmax sampling strategy to dynamically refine the
graph structure during training. This adaptive mechanism mitigates the problem
of a long-tailed degree distribution by promoting a more balanced neighborhood
structure. To model the discrete, over-dispersed, and zero-inflated nature of
scRNA-seq data, we integrate a Zero-Inflated Negative Binomial (ZINB) loss for
robust feature reconstruction. Furthermore, a contrastive learning objective is
incorporated to regularize the graph learning process and prevent abrupt
changes in the graph topology, ensuring stability and enhancing convergence.
Comprehensive experiments on 9 real scRNA-seq datasets demonstrate that scAGC
consistently outperforms other state-of-the-art methods, yielding the best NMI
and ARI scores on 9 and 7 datasets, respectively.Our code is available at
Anonymous Github.

</details>


### [154] [Long-Term Client Selection for Federated Learning with Non-IID Data: A Truthful Auction Approach](https://arxiv.org/abs/2508.09181)
*Jinghong Tan,Zhian Liu,Kun Guo,Mingxiong Zhao*

Main category: cs.LG

TL;DR: 提出了一种新的联邦学习方法LCSFLA，可以有效应对IoV中非IID数据带来的挑战，提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习 (FL) 提供了一个分散式框架，可以通过移动节点（如车联网 (IoV) 中的智能车辆）上的协作来实现通用模型训练。但是，由于车辆具有有限的连接性和计算资源，客户端选择中的信息不对称性有导致客户端提交虚假信息的风险，从而可能导致选择无效。

Method: 提出了一种新的基于真实拍卖的长期客户端选择联邦学习(LCSFLA)

Result: 在各种数据集（包括来自 IoV 场景的数据集）上的实验结果表明，该方案可以有效地缓解由非 IID 数据引起的性能下降。

Conclusion: 我们提出了一种新的基于真实拍卖的长期客户端选择联邦学习(LCSFLA)。该方案通过一种新的评估机制和能源成本，最大限度地提高了社会福利，同时考虑了长期数据质量，而具有存款要求的建议拍卖机制则鼓励了客户参与，并确保了信息的真实性。我们从理论上证明了所提出的激励机制的激励相容性和个体理性。

Abstract: Federated learning (FL) provides a decentralized framework that enables
universal model training through collaborative efforts on mobile nodes, such as
smart vehicles in the Internet of Vehicles (IoV). Each smart vehicle acts as a
mobile client, contributing to the process without uploading local data. This
method leverages non-independent and identically distributed (non-IID) training
data from different vehicles, influenced by various driving patterns and
environmental conditions, which can significantly impact model convergence and
accuracy. Although client selection can be a feasible solution for non-IID
issues, it faces challenges related to selection metrics. Traditional metrics
evaluate client data quality independently per round and require client
selection after all clients complete local training, leading to resource
wastage from unused training results. In the IoV context, where vehicles have
limited connectivity and computational resources, information asymmetry in
client selection risks clients submitting false information, potentially making
the selection ineffective. To tackle these challenges, we propose a novel
Long-term Client-Selection Federated Learning based on Truthful Auction
(LCSFLA). This scheme maximizes social welfare with consideration of long-term
data quality using a new assessment mechanism and energy costs, and the advised
auction mechanism with a deposit requirement incentivizes client participation
and ensures information truthfulness. We theoretically prove the incentive
compatibility and individual rationality of the advised incentive mechanism.
Experimental results on various datasets, including those from IoV scenarios,
demonstrate its effectiveness in mitigating performance degradation caused by
non-IID data.

</details>


### [155] [Breath as a biomarker: A survey of contact and contactless applications and approaches in respiratory monitoring](https://arxiv.org/abs/2508.09187)
*Almustapha A. Wakili,Babajide J. Asaju,Woosub Jung*

Main category: cs.LG

TL;DR: 本调查考察了呼吸分析中基于接触和非接触的方法，重点介绍了机器学习和深度学习的进步，并讨论了挑战和新兴趋势。


<details>
  <summary>Details</summary>
Motivation: 呼吸分析已成为健康监测中的一个关键工具，可提供对呼吸功能、疾病检测和持续健康评估的见解。虽然传统的基于接触的方法是可靠的，但它们通常在舒适性和实用性方面构成挑战，特别是对于长期监测。

Method: 该调查全面检查了基于接触和非接触的方法，强调了应用于呼吸分析的机器学习和深度学习技术的最新进展。非接触方法，包括 Wi-Fi 信道状态信息和声学传感，被分析为它们提供准确、非侵入性呼吸监测的能力。

Result: 我们探索了广泛的应用，从单用户呼吸率检测到多用户场景、用户识别和呼吸系统疾病检测。此外，本调查详细介绍了必要的数据预处理、特征提取和分类技术，提供了对适用于每种方法的机器学习/深度学习模型的比较见解。还讨论了数据集稀缺、多用户干扰和数据隐私等关键挑战，以及可解释人工智能、联邦学习、迁移学习和混合建模等新兴趋势。

Conclusion: 该调查通过整合当前方法并确定开放的研究方向，提供了一个全面的框架，以指导呼吸分析的未来创新，将先进的技术能力与实际的医疗保健应用相结合。

Abstract: Breath analysis has emerged as a critical tool in health monitoring, offering
insights into respiratory function, disease detection, and continuous health
assessment. While traditional contact-based methods are reliable, they often
pose challenges in comfort and practicality, particularly for long-term
monitoring. This survey comprehensively examines contact-based and contactless
approaches, emphasizing recent advances in machine learning and deep learning
techniques applied to breath analysis. Contactless methods, including Wi-Fi
Channel State Information and acoustic sensing, are analyzed for their ability
to provide accurate, noninvasive respiratory monitoring. We explore a broad
range of applications, from single-user respiratory rate detection to
multi-user scenarios, user identification, and respiratory disease detection.
Furthermore, this survey details essential data preprocessing, feature
extraction, and classification techniques, offering comparative insights into
machine learning/deep learning models suited to each approach. Key challenges
like dataset scarcity, multi-user interference, and data privacy are also
discussed, along with emerging trends like Explainable AI, federated learning,
transfer learning, and hybrid modeling. By synthesizing current methodologies
and identifying open research directions, this survey offers a comprehensive
framework to guide future innovations in breath analysis, bridging advanced
technological capabilities with practical healthcare applications.

</details>


### [156] [Fine-Grained Safety Neurons with Training-Free Continual Projection to Reduce LLM Fine Tuning Risks](https://arxiv.org/abs/2508.09190)
*Bing Han,Feifei Zhao,Dongcheng Zhao,Guobin Shen,Ping Wu,Yu Shi,Yi Zeng*

Main category: cs.LG

TL;DR: 提出FGSN方法，通过定位稀疏和精确的细粒度安全神经元，并将其参数投影到安全方向，在减少有害性的同时保留模型效用，并实现持续防御和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 微调作为服务将领域特定知识注入大型语言模型（LLM），同时挑战原始对齐机制并引入安全风险。现有方法缺乏对安全层和细粒度神经元的综合考虑，限制了其有效平衡安全性和效用的能力。

Method: 提出了一种基于无训练持续投影的细粒度安全神经元（FGSN）方法，以降低微调安全风险。

Result: 在多个微调的LLM模型上进行的大量实验表明，该方法以最小的参数修改显著降低了有害性评分和攻击成功率，同时保留了模型的效用。

Conclusion: 该方法通过引入任务特定的多维异构安全神经元集群优化机制，实现了针对无法预见的新出现的安全问题的持续防御和泛化能力。

Abstract: Fine-tuning as service injects domain-specific knowledge into large language
models (LLMs), while challenging the original alignment mechanisms and
introducing safety risks. A series of defense strategies have been proposed for
the alignment, fine-tuning, and post-fine-tuning phases, where most
post-fine-tuning defenses rely on coarse-grained safety layer mapping. These
methods lack a comprehensive consideration of both safety layers and
fine-grained neurons, limiting their ability to efficiently balance safety and
utility. To address this, we propose the Fine-Grained Safety Neurons (FGSN)
with Training-Free Continual Projection method to reduce the fine-tuning safety
risks. FGSN inherently integrates the multi-scale interactions between safety
layers and neurons, localizing sparser and more precise fine-grained safety
neurons while minimizing interference with downstream task neurons. We then
project the safety neuron parameters onto safety directions, improving model
safety while aligning more closely with human preferences. Extensive
experiments across multiple fine-tuned LLM models demonstrate that our method
significantly reduce harmfulness scores and attack success rates with minimal
parameter modifications, while preserving the model's utility. Furthermore, by
introducing a task-specific, multi-dimensional heterogeneous safety neuron
cluster optimization mechanism, we achieve continual defense and generalization
capability against unforeseen emerging safety concerns.

</details>


### [157] [From Values to Tokens: An LLM-Driven Framework for Context-aware Time Series Forecasting via Symbolic Discretization](https://arxiv.org/abs/2508.09191)
*Xiaoyu Tao,Shilong Zhang,Mingyue Cheng,Daoyu Wang,Tingyue Pan,Bokai Pan,Changqing Zhang,Shijin Wang*

Main category: cs.LG

TL;DR: TokenCast, an LLM-driven framework that leverages language-based symbolic representations as a unified intermediary for context-aware time series forecasting.


<details>
  <summary>Details</summary>
Motivation: Despite recent advances, forecasting accuracy remains limited due to the challenge of integrating historical numerical sequences with contextual features, which often comprise unstructured textual data. To address this challenge

Method: We propose TokenCast, an LLM-driven framework that leverages language-based symbolic representations as a unified intermediary for context-aware time series forecasting. Specifically, TokenCast employs a discrete tokenizer to transform continuous numerical sequences into temporal tokens, enabling structural alignment with language-based inputs. To bridge the semantic gap between modalities, both temporal and contextual tokens are embedded into a shared representation space via a pre-trained large language model (LLM), further optimized with autoregressive generative objectives. Building upon this unified semantic space, the aligned LLM is subsequently fine-tuned in a supervised manner to predict future temporal tokens, which are then decoded back into the original numerical space.

Result: the effectiveness and generalizability of TokenCast

Conclusion: Extensive experiments on diverse real-world datasets enriched with contextual features demonstrate the effectiveness and generalizability of TokenCast.

Abstract: Time series forecasting plays a vital role in supporting decision-making
across a wide range of critical applications, including energy, healthcare, and
finance. Despite recent advances, forecasting accuracy remains limited due to
the challenge of integrating historical numerical sequences with contextual
features, which often comprise unstructured textual data. To address this
challenge, we propose TokenCast, an LLM-driven framework that leverages
language-based symbolic representations as a unified intermediary for
context-aware time series forecasting. Specifically, TokenCast employs a
discrete tokenizer to transform continuous numerical sequences into temporal
tokens, enabling structural alignment with language-based inputs. To bridge the
semantic gap between modalities, both temporal and contextual tokens are
embedded into a shared representation space via a pre-trained large language
model (LLM), further optimized with autoregressive generative objectives.
Building upon this unified semantic space, the aligned LLM is subsequently
fine-tuned in a supervised manner to predict future temporal tokens, which are
then decoded back into the original numerical space. Extensive experiments on
diverse real-world datasets enriched with contextual features demonstrate the
effectiveness and generalizability of TokenCast.

</details>


### [158] [Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion Forcing](https://arxiv.org/abs/2508.09192)
*Xu Wang,Chenkai Xu,Yijie Jin,Jiachun Jin,Hao Zhang,Zhijie Deng*

Main category: cs.LG

TL;DR: This paper introduces discrete diffusion forcing (D2F), a strategy that significantly improves the inference speed of diffusion language models (dLLMs), surpassing autoregressive (AR) LLMs like LLaMA3 and Qwen2.5 while maintaining output quality.


<details>
  <summary>Details</summary>
Motivation: Existing open-source dLLMs have not achieved superior inference speed over AR LLMs of similar size.

Method: discrete diffusion forcing (D2F), block-wise autoregressive generation, prediction of following tokens without requiring completion of prior blocks for inter-block parallel decoding, asymmetric distillation process based on pre-trained dLLMs, pipelined parallel decoding algorithm

Result: D2F dLLMs achieve more than 2.5x inference speed than LLaMA3 and Qwen2.5 on GSM8K, acceleration can be more than 50x compared to vanilla dLLMs while maintaining comparable output quality.

Conclusion: D2F dLLMs achieve more than 2.5x inference speed than LLaMA3 and Qwen2.5 on GSM8K, and acceleration can be more than 50x compared to vanilla dLLMs while maintaining comparable output quality.

Abstract: Diffusion Large Language Models (dLLMs) have emerged as a promising
alternative to autoregressive (AR) LLMs for text generation, with the potential
to decode multiple tokens in a single iteration. However, none of the existing
open-source dLLMs have achieved superior inference speed over AR LLMs of
similar size. This paper breaks this barrier based on a simple and effective
strategy named discrete diffusion forcing (D2F). D2F equips dLLMs with two key
capabilities: (1) block-wise autoregressive generation to enable KV cache
utilization; (2) prediction of following tokens without requiring completion of
prior blocks for inter-block parallel decoding. In this way, the vanilla dLLMs
are refurbished into an AR-diffusion hybrid paradigm for efficient inference.
D2F can be implemented with an asymmetric distillation process based on
pre-trained dLLMs. We further propose a pipelined parallel decoding algorithm,
which enables a trade-off between efficiency and efficacy. Empirically, D2F
dLLMs achieve more than $\mathbf{2.5\times}$ inference speed than LLaMA3 and
Qwen2.5 on GSM8K. Compared to vanilla dLLMs like LLaDA and Dream, the
acceleration can be more than $\mathbf{50\times}$ while maintaining comparable
output quality. The code is available at
https://github.com/zhijie-group/Discrete-Diffusion-Forcing.

</details>


### [159] [Multi-Objective Instruction-Aware Representation Learning in Procedural Content Generation RL](https://arxiv.org/abs/2508.09193)
*Sung-Hyun Kim,In-Chang Baek,Seo-Young Lee,Geum-Hwan Hwang,Kyung-Joong Kim*

Main category: cs.LG

TL;DR: MIPCGRL improves controllability in instructed content generation by using multi-objective representation learning with sentence embeddings.


<details>
  <summary>Details</summary>
Motivation: Existing instructed reinforcement learning for procedural content generation (IPCGRL) method often struggle to leverage the expressive richness of textual input, especially under complex, multi-objective instructions, leading to limited controllability.

Method: a multi-objective representation learning method for instructed content generators, which incorporates sentence embeddings as conditions. MIPCGRL effectively trains a multi-objective embedding space by incorporating multi-label classification and multi-head regression networks.

Result: achieves up to a 13.8% improvement in controllability with multi-objective instructions

Conclusion: The proposed method achieves up to a 13.8% improvement in controllability with multi-objective instructions, enabling more expressive and flexible content generation.

Abstract: Recent advancements in generative modeling emphasize the importance of
natural language as a highly expressive and accessible modality for controlling
content generation. However, existing instructed reinforcement learning for
procedural content generation (IPCGRL) method often struggle to leverage the
expressive richness of textual input, especially under complex, multi-objective
instructions, leading to limited controllability. To address this problem, we
propose \textit{MIPCGRL}, a multi-objective representation learning method for
instructed content generators, which incorporates sentence embeddings as
conditions. MIPCGRL effectively trains a multi-objective embedding space by
incorporating multi-label classification and multi-head regression networks.
Experimental results show that the proposed method achieves up to a 13.8\%
improvement in controllability with multi-objective instructions. The ability
to process complex instructions enables more expressive and flexible content
generation.

</details>


### [160] [Meta-Learning for Speeding Up Large Model Inference in Decentralized Environments](https://arxiv.org/abs/2508.09194)
*Yipeng Du,Zihao Wang,Ahmad Farhan,Claudio Angione,Harry Yang,Fielding Johnston,James P. Buban,Patrick Colangelo,Yue Zhao,Yuzhe Yang*

Main category: cs.LG

TL;DR: 该论文提出了一种基于元学习的框架，用于在去中心化系统中自动选择最佳推理加速方法，实验结果表明该方法优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLM）等大规模模型的部署因其计算需求而产生大量成本。为了降低这些成本并应对与可扩展性和数据安全相关的挑战，越来越多的人转向用于模型部署的去中心化系统，在这些系统中，选择有效的推理加速方案对于有效管理计算资源和提高系统响应能力至关重要。

Method: 引入了一个基于元学习的框架，通过学习不同任务中各种加速技术的历史性能数据来自动选择最佳加速方法。

Result: 该研究证明，他们的元学习框架不仅简化了决策过程，而且在效率和性能方面始终优于传统方法。

Conclusion: 该研究展示了基于元学习的框架在去中心化AI系统中推理加速方面的潜力，为更民主和经济可行的人工智能解决方案提供了一条途径。

Abstract: The deployment of large-scale models, such as large language models (LLMs),
incurs substantial costs due to their computational demands. To mitigate these
costs and address challenges related to scalability and data security, there is
a growing shift towards decentralized systems for model deployment, where
choosing efficient inference acceleration schemes become crucial to manage
computational resources effectively and enhance system responsiveness. In this
work, we address the challenge of selecting optimal acceleration methods in
decentralized systems by introducing a meta-learning-based framework. This
framework automates the selection process by learning from historical
performance data of various acceleration techniques across different tasks.
Unlike traditional methods that rely on random selection or expert intuition,
our approach systematically identifies the best acceleration strategies based
on the specific characteristics of each task. We demonstrate that our
meta-learning framework not only streamlines the decision-making process but
also consistently outperforms conventional methods in terms of efficiency and
performance. Our results highlight the potential of inference acceleration in
decentralized AI systems, offering a path towards more democratic and
economically feasible artificial intelligence solutions.

</details>


### [161] [ADT4Coupons: An Innovative Framework for Sequential Coupon Distribution in E-commerce](https://arxiv.org/abs/2508.09198)
*Li Kong,Bingzhe Wang,Zhou Chen,Suhan Hu,Yuchao Ma,Qi Qi,Suoyuan Song,Bicheng Jin*

Main category: cs.LG

TL;DR: This paper introduces ADT4Coupons, a marketing framework for coupon distribution that uses sequential modeling to improve long-term revenue, outperforming existing strategies.


<details>
  <summary>Details</summary>
Motivation: Existing coupon distribution strategies do not effectively leverage the complex sequential interactions between platforms and users, leading to a performance plateau.

Method: The paper proposes Aligned Decision Transformer for Coupons (ADT4Coupons) to directly devise coupon distribution policy.

Result: Empirical results on real-world industrial, public, and synthetic datasets demonstrate the superiority of the ADT4Coupons framework.

Conclusion: The paper introduces ADT4Coupons, a new marketing framework for coupon distribution that boosts long-term revenue.

Abstract: Coupon distribution is a critical marketing strategy used by online platforms
to boost revenue and enhance user engagement. Regrettably, existing coupon
distribution strategies fall far short of effectively leveraging the complex
sequential interactions between platforms and users. This critical oversight,
despite the abundance of e-commerce log data, has precipitated a performance
plateau. In this paper, we focus on the scene that the platforms make
sequential coupon distribution decision multiple times for various users, with
each user interacting with the platform repeatedly. Based on this marketing
scenario, we propose a novel marketing framework, named Aligned Decision
Transformer for Coupons (ADT4Coupons), to directly devise coupon distribution
policy for long-term revenue boosting. ADT4Coupons enables optimized online
decision-making in a variety of real-world marketing scenarios. It achieves
this by seamlessly integrating three key characteristics, general scenarios,
sequential modeling with more comprehensive historical data, and efficient
iterative updates within a unified framework. Furthermore, empirical results on
real-world industrial dataset, alongside public and synthetic datasets
demonstrate the superiority of our framework.

</details>


### [162] [Building Safer Sites: A Large-Scale Multi-Level Dataset for Construction Safety Research](https://arxiv.org/abs/2508.09203)
*Zhenhui Ou,Dawei Li,Zhen Tan,Wenlin Li,Huan Liu,Siyuan Song*

Main category: cs.LG

TL;DR: Introduce a new construction safety dataset (CSDataset) and conduct preliminary analysis to show it can help construction safety research.


<details>
  <summary>Details</summary>
Motivation: The limited volume and lack of diversity in existing construction safety datasets pose significant challenges to conducting in-depth analyses.

Method: Construction Safety Dataset (CSDataset), a well-organized comprehensive multi-level dataset that encompasses incidents, inspections, and violations recorded sourced from the Occupational Safety and Health Administration (OSHA).

Result: Integrates structured attributes with unstructured narratives, facilitating a wide range of approaches driven by machine learning and large language models. We also conduct a preliminary approach benchmarking and various cross-level analyses using our dataset, offering insights to inform and enhance future efforts in construction safety.

Conclusion: Complaint-driven inspections were associated with a 17.3% reduction in the likelihood of subsequent incidents.

Abstract: Construction safety research is a critical field in civil engineering, aiming
to mitigate risks and prevent injuries through the analysis of site conditions
and human factors. However, the limited volume and lack of diversity in
existing construction safety datasets pose significant challenges to conducting
in-depth analyses. To address this research gap, this paper introduces the
Construction Safety Dataset (CSDataset), a well-organized comprehensive
multi-level dataset that encompasses incidents, inspections, and violations
recorded sourced from the Occupational Safety and Health Administration (OSHA).
This dataset uniquely integrates structured attributes with unstructured
narratives, facilitating a wide range of approaches driven by machine learning
and large language models. We also conduct a preliminary approach benchmarking
and various cross-level analyses using our dataset, offering insights to inform
and enhance future efforts in construction safety. For example, we found that
complaint-driven inspections were associated with a 17.3% reduction in the
likelihood of subsequent incidents. Our dataset and code are released at
https://github.com/zhenhuiou/Construction-Safety-Dataset-CSDataset.

</details>


### [163] [MoQE: Improve Quantization Model performance via Mixture of Quantization Experts](https://arxiv.org/abs/2508.09204)
*Jinhao Zhang,Yunquan Zhang,Boyang Zhang,Zeyu Liu,Daning Cheng*

Main category: cs.LG

TL;DR: MoQE combines multiple quantization variants to improve the performance of quantization models and alleviates the performance degradation.


<details>
  <summary>Details</summary>
Motivation: Quantization method plays a crucial role in improving model efficiency and reducing deployment costs, enabling the widespread application of deep learning models on resource-constrained devices. However, the quantization process inevitably introduces accuracy degradation.

Method: Mixture of Quantization Experts( abbr. MoQE), a quantization inference framework based on the Mixture-of-Experts (MoE) architecture

Result: MoQE alleviates the performance degradation commonly seen in single quantization models through specialization quantization expert models.Experimental evaluations on ResNet, LLaMA, and Qwen model families across benchmark datasets including ImageNet, WikiText, C4, and OpenWebText demonstrate that

Conclusion: MoQE achieves performance comparable to SOTA quantization model, without incurring significant increases in inference latency.

Abstract: Quantization method plays a crucial role in improving model efficiency and
reducing deployment costs, enabling the widespread application of deep learning
models on resource-constrained devices. However, the quantization process
inevitably introduces accuracy degradation. In this paper, we propose Mixture
of Quantization Experts( abbr. MoQE), a quantization inference framework based
on the Mixture-of-Experts (MoE) architecture, aiming to jointly improve the
performance of quantization models. MoQE combines multiple quantization
variants of one full-precision model as specialized "quantization experts" and
dynamically routes input data to the most suitable expert based on its
characteristics. MoQE alleviates the performance degradation commonly seen in
single quantization models through specialization quantization expert models.
We design lightweight, structure-aware router models tailored for both CV and
NLP tasks. Experimental evaluations on ResNet, LLaMA, and Qwen model families
across benchmark datasets including ImageNet, WikiText, C4, and OpenWebText
demonstrate that MoQE achieves performance comparable to SOTA quantization
model, without incurring significant increases in inference latency.

</details>


### [164] [The First Differentiable Transfer-Based Algorithm for Discrete MicroLED Repair](https://arxiv.org/abs/2508.09206)
*Ning-Yuan Lue*

Main category: cs.LG

TL;DR: 提出了一种新的microLED修复算法，该算法优于现有算法，速度更快，效率更高


<details>
  <summary>Details</summary>
Motivation: 激光辅助选择性转移是高通量 microLED 制造中的一个关键过程，它需要计算模型来规划移动序列，以最大限度地减少 XY 平台的运动，并适应整个基板上不同的优化目标。

Method: 我们提出了一种基于可微转移模块的修复算法，该模块旨在对转移平台的离散转移进行建模，同时保持通过基于梯度的优化进行训练。

Result: 实验表明，在 2000x2000 阵列上，传输步骤减少了 50%，规划时间缩短至 2 分钟以下。

Conclusion: 该方法为加速AR/VR和下一代显示器制造中的microLED修复提供了一种实用且适应性强的解决方案。

Abstract: Laser-enabled selective transfer, a key process in high-throughput microLED
fabrication, requires computational models that can plan shift sequences to
minimize motion of XY stages and adapt to varying optimization objectives
across the substrate. We propose the first repair algorithm based on a
differentiable transfer module designed to model discrete shifts of transfer
platforms, while remaining trainable via gradient-based optimization. Compared
to local proximity searching algorithms, our approach achieves superior repair
performance and enables more flexible objective designs, such as minimizing the
number of steps. Unlike reinforcement learning (RL)-based approaches, our
method eliminates the need for handcrafted feature extractors and trains
significantly faster, allowing scalability to large arrays. Experiments show a
50% reduction in transfer steps and sub-2-minute planning time on 2000x2000
arrays. This method provides a practical and adaptable solution for
accelerating microLED repair in AR/VR and next-generation display fabrication.

</details>


### [165] [Hierarchical Adaptive networks with Task vectors for Test-Time Adaptation](https://arxiv.org/abs/2508.09223)
*Sameer Ambekar,Daniel M. Lang,Julia A. Schnabel*

Main category: cs.LG

TL;DR: Hi-Vec: Hierarchical Adaptive Networks with Task Vectors, leverages multiple layers of increasing size for dynamic test-time adaptation, improving robustness, addresses uncertainty, and handles limited batch sizes and increased outlier rates.


<details>
  <summary>Details</summary>
Motivation: Standard test-time adaptation methods rely on single-dimensional linear classification layers, which often fail to handle diverse and complex shifts.

Method: Hierarchical Adaptive Networks with Task Vectors (Hi-Vec), which leverages multiple layers of increasing size for dynamic test-time adaptation. dynamic layer selection for automatic identification of the optimal layer for adaptation to each test batch. a mechanism that merges weights from the dynamic layer to other layers, ensuring all layers receive target information. linear layer agreement that acts as a gating function, preventing erroneous fine-tuning by adaptation on noisy batches.

Result: Hi-Vec improves robustness, addresses uncertainty, and handles limited batch sizes and increased outlier rates. Hi-Vec improves robustness, addresses uncertainty, and handles limited batch sizes and increased outlier rates.

Conclusion: Hi-Vec improves robustness, addresses uncertainty, and handles limited batch sizes and increased outlier rates.

Abstract: Test-time adaptation allows pretrained models to adjust to incoming data
streams, addressing distribution shifts between source and target domains.
However, standard methods rely on single-dimensional linear classification
layers, which often fail to handle diverse and complex shifts. We propose
Hierarchical Adaptive Networks with Task Vectors (Hi-Vec), which leverages
multiple layers of increasing size for dynamic test-time adaptation. By
decomposing the encoder's representation space into such hierarchically
organized layers, Hi-Vec, in a plug-and-play manner, allows existing methods to
adapt to shifts of varying complexity. Our contributions are threefold: First,
we propose dynamic layer selection for automatic identification of the optimal
layer for adaptation to each test batch. Second, we propose a mechanism that
merges weights from the dynamic layer to other layers, ensuring all layers
receive target information. Third, we propose linear layer agreement that acts
as a gating function, preventing erroneous fine-tuning by adaptation on noisy
batches. We rigorously evaluate the performance of Hi-Vec in challenging
scenarios and on multiple target datasets, proving its strong capability to
advance state-of-the-art methods. Our results show that Hi-Vec improves
robustness, addresses uncertainty, and handles limited batch sizes and
increased outlier rates.

</details>


### [166] [GSMT: Graph Fusion and Spatiotemporal TaskCorrection for Multi-Bus Trajectory Prediction](https://arxiv.org/abs/2508.09227)
*Fan Ding,Hwa Hui Tew,Junn Yong Loo,Susilawati,LiTong Liu,Fang Yu Leong,Xuewen Luo,Kar Keong Chin,Jia Jun Gan*

Main category: cs.LG

TL;DR: GSMT, a hybrid GAT-RNN model with a task corrector, improves bus trajectory prediction using GPS data in urban environments.


<details>
  <summary>Details</summary>
Motivation: Accurate bus trajectory prediction is crucial in intelligent transportation systems, especially in urban environments with limited access to multimodal data where reliance on onboard GPS data is necessary.

Method: A hybrid model (GSMT) integrating a Graph Attention Network (GAT) with a sequence-to-sequence Recurrent Neural Network (RNN), incorporating a task corrector that clusters historical trajectories to identify distinct motion patterns.

Result: GSMT achieves superior performance in both short-term and long-term trajectory prediction tasks compared to existing approaches.

Conclusion: The proposed GSMT model outperforms existing approaches in both short-term and long-term bus trajectory prediction tasks, as demonstrated on a real-world dataset from Kuala Lumpur, Malaysia.

Abstract: Accurate trajectory prediction for buses is crucial in intelligent
transportation systems, particularly within urban environments. In developing
regions where access to multimodal data is limited, relying solely on onboard
GPS data remains indispensable despite inherent challenges. To address this
problem, we propose GSMT, a hybrid model that integrates a Graph Attention
Network (GAT) with a sequence-to-sequence Recurrent Neural Network (RNN), and
incorporates a task corrector capable of extracting complex behavioral patterns
from large-scale trajectory data. The task corrector clusters historical
trajectories to identify distinct motion patterns and fine-tunes the
predictions generated by the GAT and RNN. Specifically, GSMT fuses dynamic bus
information and static station information through embedded hybrid networks to
perform trajectory prediction, and applies the task corrector for secondary
refinement after the initial predictions are generated. This two-stage approach
enables multi-node trajectory prediction among buses operating in dense urban
traffic environments under complex conditions. Experiments conducted on a
real-world dataset from Kuala Lumpur, Malaysia, demonstrate that our method
significantly outperforms existing approaches, achieving superior performance
in both short-term and long-term trajectory prediction tasks.

</details>


### [167] [Blockchain Network Analysis using Quantum Inspired Graph Neural Networks & Ensemble Models](https://arxiv.org/abs/2508.09237)
*Luigi D'Amico,Daniel De Rosso,Ninad Dixit,Raul Salles de Padua,Samuel Palmer,Samuel Mugel,Román Orús,Holger Eble,Ali Abedi*

Main category: cs.LG

TL;DR: combining Quantum Inspired Graph Neural Networks with Ensemble Model and a Canonical Polyadic decomposition layer to detect fraudulent transactions


<details>
  <summary>Details</summary>
Motivation: the detection of illicit transactions within blockchain networks remains a critical challenge, necessitating robust and innovative solutions. This system is tailored specifically for blockchain network analysis in anti-money laundering (AML) efforts

Method: combining Quantum Inspired Graph Neural Networks (QI-GNN) with flexibility of choice of an Ensemble Model using QBoost or a classic model such as Random Forrest Classifier, incorporates a novel component, a Canonical Polyadic (CP) decomposition layer within the graph neural network framework

Result: achieving an F2 score of 74.8% in detecting fraudulent transactions. These results highlight the potential of quantum-inspired techniques, supplemented by the structural advancements of the CP layer, to not only match but potentially exceed traditional methods in complex network analysis for financial security

Conclusion: The findings advocate for a broader adoption and further exploration of quantum-inspired algorithms within the financial sector to effectively combat fraud.

Abstract: In the rapidly evolving domain of financial technology, the detection of
illicit transactions within blockchain networks remains a critical challenge,
necessitating robust and innovative solutions. This work proposes a novel
approach by combining Quantum Inspired Graph Neural Networks (QI-GNN) with
flexibility of choice of an Ensemble Model using QBoost or a classic model such
as Random Forrest Classifier. This system is tailored specifically for
blockchain network analysis in anti-money laundering (AML) efforts. Our
methodology to design this system incorporates a novel component, a Canonical
Polyadic (CP) decomposition layer within the graph neural network framework,
enhancing its capability to process and analyze complex data structures
efficiently. Our technical approach has undergone rigorous evaluation against
classical machine learning implementations, achieving an F2 score of 74.8% in
detecting fraudulent transactions. These results highlight the potential of
quantum-inspired techniques, supplemented by the structural advancements of the
CP layer, to not only match but potentially exceed traditional methods in
complex network analysis for financial security. The findings advocate for a
broader adoption and further exploration of quantum-inspired algorithms within
the financial sector to effectively combat fraud.

</details>


### [168] [LLM Empowered Prototype Learning for Zero and Few-Shot Tasks on Tabular Data](https://arxiv.org/abs/2508.09263)
*Peng Wang,Dongsheng Wang,He Zhao,Hangting Ye,Dandan Guo,Yi Chang*

Main category: cs.LG

TL;DR: 该论文提出了一种新的基于LLM的原型估计框架，用于表格学习，该框架通过生成特征值来构建零样本原型，并在零样本和小样本设置中表现出有效性。


<details>
  <summary>Details</summary>
Motivation: 有效地在少样本甚至零样本场景中使用先进的LLM仍然具有挑战性。

Method: 该论文的关键思想是查询LLM以生成基于无示例提示的特征值，该提示仅依赖于任务和特征描述。利用LLM生成的特征值，可以以无训练方式构建零样本原型，并通过融合小样本来进一步增强原型，避免了训练分类器或微调LLM。

Result: 大量实验表明，该方法在零样本和小样本表格学习中有效。

Conclusion: 该论文通过提出一种基于LLM的原型估计框架，在零样本和小样本表格学习中取得了有效性。

Abstract: Recent breakthroughs in large language models (LLMs) have opened the door to
in-depth investigation of their potential in tabular data modeling. However,
effectively utilizing advanced LLMs in few-shot and even zero-shot scenarios is
still challenging. To this end, we propose a novel LLM-based prototype
estimation framework for tabular learning. Our key idea is to query the LLM to
generate feature values based example-free prompt, which solely relies on task
and feature descriptions. With the feature values generated by LLM, we can
build a zero-shot prototype in a training-free manner, which can be further
enhanced by fusing few-shot samples, avoiding training a classifier or
finetuning the LLMs. Thanks to the example-free prompt and prototype
estimation, ours bypasses the constraints brought by the example-based prompt,
providing a scalable and robust framework. Extensive experiments demonstrate
the effectiveness of ours in zero and few-shot tabular learning.

</details>


### [169] [Detection of Odor Presence via Deep Neural Networks](https://arxiv.org/abs/2508.09264)
*Matin Hassanloo,Ali Zareh,Mehmet Kemal Özdemir*

Main category: cs.LG

TL;DR: 该研究使用深度学习模型，仅使用嗅球信号，实现了对气味的可靠单次试验检测。


<details>
  <summary>Details</summary>
Motivation: 气味检测是食品安全、环境监测、医疗诊断和许多其他领域的基础。目前用于气味检测的人工传感器难以处理复杂的混合物，而非侵入性记录缺乏可靠的单次试验保真度。为了开发一种通用的气味检测系统。

Method: 提出了一种互补的一维卷积网络（ResCNN 和 AttentionCNN）集成，该网络从多通道嗅球 LFP 中解码气味的存在。

Result: 最终的集成模型支持这两个假设，实现了 86.6% 的平均准确率，81.0% 的 F1 分数和 0.9247 的 AUC，大大优于之前的基准。

Conclusion: 该研究证明了从细胞外 LFP 中可靠地进行气味存在的单次试验检测的可行性，并展示了深度学习模型在提供对嗅觉表征的更深入理解方面的潜力。

Abstract: Odor detection underpins food safety, environmental monitoring, medical
diagnostics, and many more fields. The current artificial sensors developed for
odor detection struggle with complex mixtures while non-invasive recordings
lack reliable single-trial fidelity. To develop a general system for odor
detection, in this study we present a preliminary work where we aim to test two
hypotheses: (i) that spectral features of local field potentials (LFPs) are
sufficient for robust single-trial odor detection and (ii) that signals from
the olfactory bulb alone are adequate. To test two hypotheses, we propose an
ensemble of complementary one-dimensional convolutional networks (ResCNN and
AttentionCNN) that decodes the presence of odor from multichannel olfactory
bulb LFPs. Tested on 2,349 trials from seven awake mice, our final ensemble
model supports both hypotheses, achieving a mean accuracy of 86.6%, an F1-score
of 81.0%, and an AUC of 0.9247, substantially outperforming previous
benchmarks. In addition, the t-SNE visualization confirms that our framework
captures biologically significant signatures. These findings establish the
feasibility of robust single-trial detection of the presence of odor from
extracellular LFPs, as well as demonstrate the potential of deep learning
models to provide a deeper understanding of olfactory representations.

</details>


### [170] [Over-Squashing in GNNs and Causal Inference of Rewiring Strategies](https://arxiv.org/abs/2508.09265)
*Danial Saber,Amirali Salehi-Abari*

Main category: cs.LG

TL;DR: This paper proposes a metric to quantify over-squashing in GNNs and analyzes the impact of rewiring strategies on mitigating it. The findings suggest that rewiring is most beneficial when over-squashing is substantial and corrected with restraint, and the diagnostic tool lets practitioners decide whether rewiring is likely to pay off.


<details>
  <summary>Details</summary>
Motivation: message-passing GNNs suffer from over-squashing which limits expressivity. Rewiring techniques can ease this bottleneck; but their practical impacts are unclear due to the lack of a direct empirical over-squashing metric

Method: a rigorous, topology-focused method for assessing over-squashing between node pairs using the decay rate of their mutual sensitivity. extend these pairwise assessments to four graph-level statistics (prevalence, intensity, variability, extremity). Coupling these metrics with a within-graph causal design, quantify how rewiring strategies affect over-squashing

Result: most graph classification datasets suffer from over-squashing (but to various extents), and rewiring effectively mitigates it. over-squashing is less notable in node classification datasets, where rewiring often increases over-squashing, and performance variations are uncorrelated with over-squashing changes

Conclusion: rewiring is most beneficial when over-squashing is both substantial and corrected with restraint. overly aggressive rewiring, or rewiring applied to minimally over-squashed graphs, is unlikely to help and may even harm performance

Abstract: Graph neural networks (GNNs) have exhibited state-of-the-art performance
across wide-range of domains such as recommender systems, material design, and
drug repurposing. Yet message-passing GNNs suffer from over-squashing --
exponential compression of long-range information from distant nodes -- which
limits expressivity. Rewiring techniques can ease this bottleneck; but their
practical impacts are unclear due to the lack of a direct empirical
over-squashing metric. We propose a rigorous, topology-focused method for
assessing over-squashing between node pairs using the decay rate of their
mutual sensitivity. We then extend these pairwise assessments to four
graph-level statistics (prevalence, intensity, variability, extremity).
Coupling these metrics with a within-graph causal design, we quantify how
rewiring strategies affect over-squashing on diverse graph- and
node-classification benchmarks. Our extensive empirical analyses show that most
graph classification datasets suffer from over-squashing (but to various
extents), and rewiring effectively mitigates it -- though the degree of
mitigation, and its translation into performance gains, varies by dataset and
method. We also found that over-squashing is less notable in node
classification datasets, where rewiring often increases over-squashing, and
performance variations are uncorrelated with over-squashing changes. These
findings suggest that rewiring is most beneficial when over-squashing is both
substantial and corrected with restraint -- while overly aggressive rewiring,
or rewiring applied to minimally over-squashed graphs, is unlikely to help and
may even harm performance. Our plug-and-play diagnostic tool lets practitioners
decide -- before any training -- whether rewiring is likely to pay off.

</details>


### [171] [Constrained Black-Box Attacks Against Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.09275)
*Amine Andam,Jamal Bentahar,Mustapha Hedabou*

Main category: cs.LG

TL;DR: This paper investigates vulnerabilities in collaborative multi-agent reinforcement learning to adversarial attacks. It introduces effective and sample-efficient algorithms for generating adversarial perturbations, validated across diverse environments.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the lack of investigation into the vulnerabilities of collaborative multi-agent reinforcement learning (c-MARL) to adversarial attacks, particularly under realistic and constrained conditions.

Method: The paper proposes algorithms for generating adversarial perturbations designed to misalign how victim agents perceive their environment.

Result: The proposed algorithm's effectiveness is empirically validated on three benchmarks and 22 environments, demonstrating its sample efficiency by requiring only 1,000 samples.

Conclusion: The paper introduces effective algorithms for generating adversarial perturbations that mislead victim agents by manipulating their perception of the environment. The approach is validated across diverse algorithms and environments, demonstrating sample efficiency by requiring only 1,000 samples.

Abstract: Collaborative multi-agent reinforcement learning (c-MARL) has rapidly
evolved, offering state-of-the-art algorithms for real-world applications,
including sensitive domains. However, a key challenge to its widespread
adoption is the lack of a thorough investigation into its vulnerabilities to
adversarial attacks. Existing work predominantly focuses on training-time
attacks or unrealistic scenarios, such as access to policy weights or the
ability to train surrogate policies. In this paper, we investigate new
vulnerabilities under more realistic and constrained conditions, assuming an
adversary can only collect and perturb the observations of deployed agents. We
also consider scenarios where the adversary has no access at all. We propose
simple yet highly effective algorithms for generating adversarial perturbations
designed to misalign how victim agents perceive their environment. Our approach
is empirically validated on three benchmarks and 22 environments, demonstrating
its effectiveness across diverse algorithms and environments. Furthermore, we
show that our algorithm is sample-efficient, requiring only 1,000 samples
compared to the millions needed by previous methods.

</details>


### [172] [Pattern-based Knowledge Component Extraction from Student Code Using Representation Learning](https://arxiv.org/abs/2508.09281)
*Muntasir Hoq,Griffin Pitts,Andrew Lan,Peter Brusilovsky,Bita Akram*

Main category: cs.LG

TL;DR: The paper introduces an explainable framework for automated knowledge component discovery in computer science education using pattern-based KCs, which improves deep knowledge tracing predictive performance.


<details>
  <summary>Details</summary>
Motivation: Effective personalized learning in computer science education depends on accurately modeling what students know and what they need to learn. While Knowledge Components (KCs) provide a foundation for such modeling, automated KC extraction from student code is inherently challenging due to insufficient explainability of discovered KCs and the open-endedness of programming problems with significant structural variability across student solutions and complex interactions among programming concepts.

Method: a novel, explainable framework for automated KC discovery through pattern-based KCs: recurring structural patterns within student code that capture the specific programming patterns and language constructs that students must master. Toward this, we train a Variational Autoencoder to generate important representative patterns from student code guided by an explainable, attention-based code representation model that identifies important correct and incorrect pattern implementations from student code. These patterns are then clustered to form pattern-based KCs.

Result: Experimental results demonstrate meaningful learning trajectories and significant improvements in DKT predictive performance over traditional KT methods.

Conclusion: This work advances knowledge modeling in CS education by providing an automated, scalable, and explainable framework for identifying granular code patterns and algorithmic constructs, essential for student learning.

Abstract: Effective personalized learning in computer science education depends on
accurately modeling what students know and what they need to learn. While
Knowledge Components (KCs) provide a foundation for such modeling, automated KC
extraction from student code is inherently challenging due to insufficient
explainability of discovered KCs and the open-endedness of programming problems
with significant structural variability across student solutions and complex
interactions among programming concepts. In this work, we propose a novel,
explainable framework for automated KC discovery through pattern-based KCs:
recurring structural patterns within student code that capture the specific
programming patterns and language constructs that students must master. Toward
this, we train a Variational Autoencoder to generate important representative
patterns from student code guided by an explainable, attention-based code
representation model that identifies important correct and incorrect pattern
implementations from student code. These patterns are then clustered to form
pattern-based KCs. We evaluate our KCs using two well-established methods
informed by Cognitive Science: learning curve analysis and Deep Knowledge
Tracing (DKT). Experimental results demonstrate meaningful learning
trajectories and significant improvements in DKT predictive performance over
traditional KT methods. This work advances knowledge modeling in CS education
by providing an automated, scalable, and explainable framework for identifying
granular code patterns and algorithmic constructs, essential for student
learning.

</details>


### [173] [Distilling Reinforcement Learning into Single-Batch Datasets](https://arxiv.org/abs/2508.09283)
*Connor Wilhelm,Dan Ventura*

Main category: cs.LG

TL;DR: Distillation compresses RL environments into one-step supervised learning datasets.


<details>
  <summary>Details</summary>
Motivation: Dataset distillation compresses a large dataset into a small synthetic dataset such that learning on the synthetic dataset approximates learning on the original. Training on the distilled dataset can be performed in as little as one step of gradient descent.

Method: A novel extension of proximal policy optimization for meta-learning is used in distillation.

Result: Distillation is generalizable to different tasks by distilling reinforcement learning environments into one-batch supervised learning datasets. Demonstrated on a multi-dimensional extension of the classic cart-pole problem, all MuJoCo environments, and several Atari games.

Conclusion: Distillation can compress complex RL environments into one-step supervised learning and generalize across learner architectures, even to the smallest-possible synthetic dataset.

Abstract: Dataset distillation compresses a large dataset into a small synthetic
dataset such that learning on the synthetic dataset approximates learning on
the original. Training on the distilled dataset can be performed in as little
as one step of gradient descent. We demonstrate that distillation is
generalizable to different tasks by distilling reinforcement learning
environments into one-batch supervised learning datasets. This demonstrates not
only distillation's ability to compress a reinforcement learning task but also
its ability to transform one learning modality (reinforcement learning) into
another (supervised learning). We present a novel extension of proximal policy
optimization for meta-learning and use it in distillation of a
multi-dimensional extension of the classic cart-pole problem, all MuJoCo
environments, and several Atari games. We demonstrate distillation's ability to
compress complex RL environments into one-step supervised learning, explore RL
distillation's generalizability across learner architectures, and demonstrate
distilling an environment into the smallest-possible synthetic dataset.

</details>


### [174] [Decentralized Weather Forecasting via Distributed Machine Learning and Blockchain-Based Model Validation](https://arxiv.org/abs/2508.09299)
*Rilwan Umar,Aydin Abadi,Basil Aldali,Benito Vincent,Elliot A. J. Hurley,Hotoon Aljazaeri,Jamie Hedley-Cook,Jamie-Lee Bell,Lambert Uwuigbusun,Mujeeb Ahmed,Shishir Nagaraja,Suleiman Sabo,Weaam Alrbeiqi*

Main category: cs.LG

TL;DR: A decentralized weather forecasting framework using Federated Learning and blockchain improves accuracy, resilience, and scalability.


<details>
  <summary>Details</summary>
Motivation: Current centralized forecasting systems are increasingly strained by security vulnerabilities, limited scalability, and susceptibility to single points of failure.

Method: The paper integrates Federated Learning (FL) with blockchain technology and introduces a reputation-based voting mechanism.

Result: The approach improves forecasting accuracy and enhances system resilience and scalability.

Conclusion: The proposed decentralized weather forecasting framework improves forecasting accuracy, enhances system resilience and scalability, and is viable for deployment in real-world, security-critical environments.

Abstract: Weather forecasting plays a vital role in disaster preparedness, agriculture,
and resource management, yet current centralized forecasting systems are
increasingly strained by security vulnerabilities, limited scalability, and
susceptibility to single points of failure. To address these challenges, we
propose a decentralized weather forecasting framework that integrates Federated
Learning (FL) with blockchain technology. FL enables collaborative model
training without exposing sensitive local data; this approach enhances privacy
and reduces data transfer overhead. Meanwhile, the Ethereum blockchain ensures
transparent and dependable verification of model updates. To further enhance
the system's security, we introduce a reputation-based voting mechanism that
assesses the trustworthiness of submitted models while utilizing the
Interplanetary File System (IPFS) for efficient off-chain storage. Experimental
results demonstrate that our approach not only improves forecasting accuracy
but also enhances system resilience and scalability, making it a viable
candidate for deployment in real-world, security-critical environments.

</details>


### [175] [Exact Verification of Graph Neural Networks with Incremental Constraint Solving](https://arxiv.org/abs/2508.09320)
*Minghao Liu,Chia-Hsuan Lu,Marta Kwiatkowska*

Main category: cs.LG

TL;DR: 本文提出了一种精确的GNN验证方法，以计算针对属性和结构扰动的保证，并优于现有的精确验证工具。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNN）越来越多地应用于欺诈检测或医疗保健等高风险应用中，但容易受到对抗性攻击。已经提出了许多技术来提供对抗性鲁棒性保证，但是仍然缺乏对消息传递GNN中常用聚合函数的支持。

Method: 该方法采用带边界收紧的约束求解，并迭代地求解一系列松弛约束满足问题，同时依靠求解器的增量求解能力来提高效率。

Result: 我们开发了一种精确的（可靠且完整）GNN验证方法，以计算针对属性和结构扰动的保证，这些扰动涉及边缘添加或删除，但受预算约束。专注于节点分类任务。

Conclusion: GNNev在标准基准和真实世界的欺诈数据集上进行了广泛的实验评估，证明了它的可用性和有效性，并且与现有的精确验证工具相比，在总和聚合节点分类任务上具有卓越的性能。

Abstract: Graph neural networks (GNNs) are increasingly employed in high-stakes
applications, such as fraud detection or healthcare, but are susceptible to
adversarial attacks. A number of techniques have been proposed to provide
adversarial robustness guarantees, but support for commonly used aggregation
functions in message-passing GNNs is still lacking. In this paper, we develop
an exact (sound and complete) verification method for GNNs to compute
guarantees against attribute and structural perturbations that involve edge
addition or deletion, subject to budget constraints. Focusing on node
classification tasks, our method employs constraint solving with bound
tightening, and iteratively solves a sequence of relaxed constraint
satisfaction problems while relying on incremental solving capabilities of
solvers to improve efficiency. We implement GNNev, a versatile solver for
message-passing neural networks, which supports three aggregation functions,
sum, max and mean, with the latter two considered here for the first time.
Extensive experimental evaluation of GNNev on two standard benchmarks (Cora and
CiteSeer) and two real-world fraud datasets (Amazon and Yelp) demonstrates its
usability and effectiveness, as well as superior performance compared to
existing {exact verification} tools on sum-aggregated node classification
tasks.

</details>


### [176] [Synaptic Pruning: A Biological Inspiration for Deep Learning Regularization](https://arxiv.org/abs/2508.09330)
*Gideon Vos,Liza van Eijk,Zoltan Sarnyai,Mostafa Rahimi Azghadi*

Main category: cs.LG

TL;DR: Proposes a magnitude-based synaptic pruning method as a dropout replacement, removing low-importance connections during training. Shows consistent gains in time series forecasting, especially in financial applications.


<details>
  <summary>Details</summary>
Motivation: Synaptic pruning in biological brains removes weak connections to improve efficiency. In contrast, dropout regularization in artificial neural networks randomly deactivates neurons without considering activity-dependent pruning.

Method: a magnitude-based synaptic pruning method that better reflects biology by progressively removing low-importance connections during training. Integrated directly into the training loop as a dropout replacement, our approach computes weight importance from absolute magnitudes across layers and applies a cubic schedule to gradually increase global sparsity. At fixed intervals, pruning masks permanently remove low-importance weights while maintaining gradient flow for active ones, eliminating the need for separate pruning and fine-tuning phases.

Result: Experiments on multiple time series forecasting models including RNN, LSTM, and Patch Time Series Transformer across four datasets show consistent gains. Our method ranked best overall, with statistically significant improvements confirmed by Friedman tests (p < 0.01). In financial forecasting, it reduced Mean Absolute Error by up to 20% over models with no or standard dropout, and up to 52% in select transformer models.

Conclusion: This dynamic pruning mechanism advances regularization by coupling weight elimination with progressive sparsification, offering easy integration into diverse architectures. Its strong performance, especially in financial time series forecasting, highlights its potential as a practical alternative to conventional dropout techniques.

Abstract: Synaptic pruning in biological brains removes weak connections to improve
efficiency. In contrast, dropout regularization in artificial neural networks
randomly deactivates neurons without considering activity-dependent pruning. We
propose a magnitude-based synaptic pruning method that better reflects biology
by progressively removing low-importance connections during training.
Integrated directly into the training loop as a dropout replacement, our
approach computes weight importance from absolute magnitudes across layers and
applies a cubic schedule to gradually increase global sparsity. At fixed
intervals, pruning masks permanently remove low-importance weights while
maintaining gradient flow for active ones, eliminating the need for separate
pruning and fine-tuning phases. Experiments on multiple time series forecasting
models including RNN, LSTM, and Patch Time Series Transformer across four
datasets show consistent gains. Our method ranked best overall, with
statistically significant improvements confirmed by Friedman tests (p < 0.01).
In financial forecasting, it reduced Mean Absolute Error by up to 20% over
models with no or standard dropout, and up to 52% in select transformer models.
This dynamic pruning mechanism advances regularization by coupling weight
elimination with progressive sparsification, offering easy integration into
diverse architectures. Its strong performance, especially in financial time
series forecasting, highlights its potential as a practical alternative to
conventional dropout techniques.

</details>


### [177] [Resurrecting the Salmon: Rethinking Mechanistic Interpretability with Domain-Specific Sparse Autoencoders](https://arxiv.org/abs/2508.09363)
*Charles O'Neill,Mudith Jayasekara,Max Kirkby*

Main category: cs.LG

TL;DR: Restricting SAE training to a well-defined domain improves reconstruction fidelity and interpretability.


<details>
  <summary>Details</summary>
Motivation: Conventional SAEs train on broad data distributions, forcing a fixed latent budget to capture only high-frequency, generic patterns. This often results in significant linear ``dark matter'' in reconstruction error and produces latents that fragment or absorb each other, complicating interpretation. We show that restricting SAE training to a well-defined domain (medical text) reallocates capacity to domain-specific features, improving both reconstruction fidelity and interpretability.

Method: Training JumpReLU SAEs on layer-20 activations of Gemma-2 models using 195k clinical QA examples

Result: domain-confined SAEs explain up to 20% more variance, achieve higher loss recovery, and reduce linear residual error compared to broad-domain SAEs. Automated and human evaluations confirm that learned features align with clinically meaningful concepts

Conclusion: Domain-confinement mitigates key limitations of broad-domain SAEs, enabling more complete and interpretable latent decompositions, and suggesting the field may need to question ``foundation-model'' scaling for general-purpose SAEs.

Abstract: Sparse autoencoders (SAEs) decompose large language model (LLM) activations
into latent features that reveal mechanistic structure. Conventional SAEs train
on broad data distributions, forcing a fixed latent budget to capture only
high-frequency, generic patterns. This often results in significant linear
``dark matter'' in reconstruction error and produces latents that fragment or
absorb each other, complicating interpretation. We show that restricting SAE
training to a well-defined domain (medical text) reallocates capacity to
domain-specific features, improving both reconstruction fidelity and
interpretability. Training JumpReLU SAEs on layer-20 activations of Gemma-2
models using 195k clinical QA examples, we find that domain-confined SAEs
explain up to 20\% more variance, achieve higher loss recovery, and reduce
linear residual error compared to broad-domain SAEs. Automated and human
evaluations confirm that learned features align with clinically meaningful
concepts (e.g., ``taste sensations'' or ``infectious mononucleosis''), rather
than frequent but uninformative tokens. These domain-specific SAEs capture
relevant linear structure, leaving a smaller, more purely nonlinear residual.
We conclude that domain-confinement mitigates key limitations of broad-domain
SAEs, enabling more complete and interpretable latent decompositions, and
suggesting the field may need to question ``foundation-model'' scaling for
general-purpose SAEs.

</details>


### [178] [Understanding Dementia Speech Alignment with Diffusion-Based Image Generation](https://arxiv.org/abs/2508.09385)
*Mansi,Anastasios Lepipas,Dominika Woszczyk,Yiying Guan,Soteris Demetriou*

Main category: cs.LG

TL;DR: This paper explores using text-to-image models to detect dementia from speech, achieving 75% accuracy. It identifies which parts of the speech contribute to the detection.


<details>
  <summary>Details</summary>
Motivation: Little has been done to understand whether the alignment between pathological speech and generated images is possible.

Method: Examining the ability of text-to-image models to align dementia-related speech information with the generated images and develop methods to explain this alignment.

Result: Dementia detection is possible from generated images alone achieving 75% accuracy on the ADReSS dataset. Explainability methods are used to show which parts of the language contribute to the detection.

Conclusion: Dementia detection is possible from generated images alone achieving 75% accuracy on the ADReSS dataset.

Abstract: Text-to-image models generate highly realistic images based on natural
language descriptions and millions of users use them to create and share images
online. While it is expected that such models can align input text and
generated image in the same latent space little has been done to understand
whether this alignment is possible between pathological speech and generated
images. In this work, we examine the ability of such models to align
dementia-related speech information with the generated images and develop
methods to explain this alignment. Surprisingly, we found that dementia
detection is possible from generated images alone achieving 75% accuracy on the
ADReSS dataset. We then leverage explainability methods to show which parts of
the language contribute to the detection.

</details>


### [179] [Integrating Feature Attention and Temporal Modeling for Collaborative Financial Risk Assessment](https://arxiv.org/abs/2508.09399)
*Yue Yao,Zhen Xu,Youzhu Liu,Kunyuan Ma,Yuxiu Lin,Mohan Jiang*

Main category: cs.LG

TL;DR: 本文提出了一种基于联邦学习的风险评估框架，用于跨机构金融风险分析，该框架在保护数据隐私的同时，实现了联合建模和风险识别。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决跨机构金融风险分析中的数据隐私和协作建模的挑战。

Method: 该模型采用了一种分布式优化策略。每个金融机构训练一个本地子模型。模型参数在使用差分隐私和噪声注入保护后上传。然后，中央服务器聚合这些参数以生成全局模型。

Result: 实验结果表明，该模型在所有评估指标上均优于传统的中心化方法和现有的联邦学习变体。它展示了强大的建模能力和在敏感金融环境中的实用价值。

Conclusion: 该方法在保护数据主权的同时，增强了风险识别的范围和效率，为智能金融风险分析提供了一个安全高效的解决方案。

Abstract: This paper addresses the challenges of data privacy and collaborative
modeling in cross-institution financial risk analysis. It proposes a risk
assessment framework based on federated learning. Without sharing raw data, the
method enables joint modeling and risk identification across multiple
institutions. This is achieved by incorporating a feature attention mechanism
and temporal modeling structure. Specifically, the model adopts a distributed
optimization strategy. Each financial institution trains a local sub-model. The
model parameters are protected using differential privacy and noise injection
before being uploaded. A central server then aggregates these parameters to
generate a global model. This global model is used for systemic risk
identification. To validate the effectiveness of the proposed method, multiple
experiments are conducted. These evaluate communication efficiency, model
accuracy, systemic risk detection, and cross-market generalization. The results
show that the proposed model outperforms both traditional centralized methods
and existing federated learning variants across all evaluation metrics. It
demonstrates strong modeling capabilities and practical value in sensitive
financial environments. The method enhances the scope and efficiency of risk
identification while preserving data sovereignty. It offers a secure and
efficient solution for intelligent financial risk analysis.

</details>


### [180] [Graph Neural Network and Transformer Integration for Unsupervised System Anomaly Discovery](https://arxiv.org/abs/2508.09401)
*Yun Zi,Ming Gong,Zhihao Xue,Yujun Zou,Nia Qi,Yingnan Deng*

Main category: cs.LG

TL;DR: 提出了一种用于分布式后端服务系统的无监督异常检测方法，该方法优于现有模型，具有很高的实际部署潜力。


<details>
  <summary>Details</summary>
Motivation: 本研究提出了一种用于分布式后端服务系统的无监督异常检测方法，解决了复杂结构依赖、多样化行为演变和缺乏标记数据等实际挑战。

Method: 该方法构建了一个基于服务调用关系的动态图，并应用图卷积从多跳拓扑中提取高阶结构表示。Transformer用于建模每个节点的时间行为，捕获长期依赖关系和局部波动。在特征融合阶段，一种可学习的联合嵌入机制将结构和行为表示集成到一个统一的异常向量中。然后应用非线性映射来计算异常分数，从而实现无需监督的端到端检测过程。

Result: 在真实云监控数据上的实验包括对不同图深度、序列长度和数据扰动的敏感性分析。结果表明，所提出的方法在几个关键指标上优于现有模型。

Conclusion: 该方法在关键指标上优于现有模型，在捕获异常传播路径和建模动态行为序列方面表现出更强的表达性和稳定性，具有很高的实际部署潜力。

Abstract: This study proposes an unsupervised anomaly detection method for distributed
backend service systems, addressing practical challenges such as complex
structural dependencies, diverse behavioral evolution, and the absence of
labeled data. The method constructs a dynamic graph based on service invocation
relationships and applies graph convolution to extract high-order structural
representations from multi-hop topologies. A Transformer is used to model the
temporal behavior of each node, capturing long-term dependencies and local
fluctuations. During the feature fusion stage, a learnable joint embedding
mechanism integrates structural and behavioral representations into a unified
anomaly vector. A nonlinear mapping is then applied to compute anomaly scores,
enabling an end-to-end detection process without supervision. Experiments on
real-world cloud monitoring data include sensitivity analyses across different
graph depths, sequence lengths, and data perturbations. Results show that the
proposed method outperforms existing models on several key metrics,
demonstrating stronger expressiveness and stability in capturing anomaly
propagation paths and modeling dynamic behavior sequences, with high potential
for practical deployment.

</details>


### [181] [NeuronTune: Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs](https://arxiv.org/abs/2508.09473)
*Birong Pan,Mayi Xu,Qiankun Pi,Jianhao Chen,Yuanyuan Zhu,Ming Zhong,Tieyun Qian*

Main category: cs.LG

TL;DR: NeuronTune通过动态调节神经元，在提高LLM安全性的同时保持其效用，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 确保强大的安全对齐，同时保持效用，对于大型语言模型（LLM）的可靠部署至关重要。然而，当前的技术从根本上存在相互交织的缺陷：针对恶意攻击的鲁棒性不足，频繁拒绝良性查询，生成的文本质量下降和一般任务性能下降——前两者反映了鲁棒安全性的不足，后者构成了效用减损。我们将这些限制追溯到现有方法中粗粒度的逐层干预。

Method: 提出NeuronTune，一个细粒度的框架，通过动态调节稀疏神经元来实现同步安全-效用优化。该方法首先通过归因识别所有层中的安全关键和效用保持神经元，然后采用元学习来适应性地放大安全神经元激活并抑制效用神经元激活。

Result: 大量实验结果表明，我们的方法显著优于现有的最新技术，在保持出色效用的同时，实现了更高的模型安全性。

Conclusion: NeuronTune显著优于现有技术，在保持出色效用的同时，实现了更高的模型安全性。

Abstract: Ensuring robust safety alignment while preserving utility is critical for the
reliable deployment of Large Language Models (LLMs). However, current
techniques fundamentally suffer from intertwined deficiencies: insufficient
robustness against malicious attacks, frequent refusal of benign queries,
degradation in generated text quality and general task performance--the former
two reflecting deficits in robust safety and the latter constituting utility
impairment. We trace these limitations to the coarse-grained layer-wise
interventions in existing methods. To resolve this, we propose NeuronTune, a
fine-grained framework that dynamically modulates sparse neurons to achieve
simultaneous safety-utility optimization. Our approach first identifies
safety-critical and utility-preserving neurons across all layers via
attribution, then employs meta-learning to adaptively amplify safety-neuron
activations and suppress utility-neuron activations. Crucially, NeuronTune
enables tunable adjustment of intervention scope via neuron-count thresholds,
supporting flexible adaptation to security-critical or utility-priority
scenarios. Extensive experimental results demonstrate that our method
significantly outperforms existing state-of-the-art technologies, achieving
superior model safety while maintaining excellent utility.

</details>


### [182] [Domain-Generalization to Improve Learning in Meta-Learning Algorithms](https://arxiv.org/abs/2508.09418)
*Usman Anjum,Chris Stockman,Cat Luong,Justin Zhan*

Main category: cs.LG

TL;DR: DGS-MAML是一种新的元学习算法，通过结合梯度匹配和锐度感知最小化来提高模型的适应性和鲁棒性，实验结果表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决模型在有限训练数据下跨任务泛化的问题。

Method: 结合梯度匹配和锐度感知最小化，采用双层优化框架。

Result: 在基准数据集上的实验结果表明，DGS-MAML优于现有方法。

Conclusion: DGS-MAML在准确性和泛化方面优于现有方法，尤其适用于少样本学习和快速适应场景。

Abstract: This paper introduces Domain Generalization Sharpness-Aware Minimization
Model-Agnostic Meta-Learning (DGS-MAML), a novel meta-learning algorithm
designed to generalize across tasks with limited training data. DGS-MAML
combines gradient matching with sharpness-aware minimization in a bi-level
optimization framework to enhance model adaptability and robustness. We support
our method with theoretical analysis using PAC-Bayes and convergence
guarantees. Experimental results on benchmark datasets show that DGS-MAML
outperforms existing approaches in terms of accuracy and generalization. The
proposed method is particularly useful for scenarios requiring few-shot
learning and quick adaptation, and the source code is publicly available at
GitHub.

</details>


### [183] [Implicit Hypergraph Neural Networks: A Stable Framework for Higher-Order Relational Learning with Provable Guarantees](https://arxiv.org/abs/2508.09427)
*Xiaoyu Li,Guangyu Tang,Jiaojiao Jiang*

Main category: cs.LG

TL;DR: This paper introduces Implicit Hypergraph Neural Networks (IHGNN) to address the limitations of traditional hypergraph neural networks in modeling higher-order relations. IHGNN uses an implicit equilibrium formulation to enable stable and efficient global propagation across hyperedges. Experiments show that IHGNN outperforms existing methods in accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Hypergraph neural networks have shown great promise at modeling higher-order relations, but their reliance on a fixed number of explicit message-passing layers limits long-range dependency capture and can destabilize training as depth grows.

Method: Implicit Hypergraph Neural Networks (IHGNN), which bring the implicit equilibrium formulation to hypergraphs: instead of stacking layers, IHGNN computes representations as the solution to a nonlinear fixed-point equation, enabling stable and efficient global propagation across hyperedges without deep architectures. Develop a well-posed training scheme with provable convergence, analyze the oversmoothing conditions and expressivity of the model, and derive a transductive generalization bound on hypergraphs. present an implicit-gradient training procedure coupled with a projection-based stabilization strategy.

Result: IHGNN consistently outperforms strong traditional graph/hypergraph neural network baselines in both accuracy and robustness. IHGNN is resilient to random initialization and hyperparameter variation, highlighting its strong generalization and practical value for higher-order relational learning.

Conclusion: Implicit Hypergraph Neural Networks (IHGNN) consistently outperforms strong traditional graph/hypergraph neural network baselines in both accuracy and robustness. IHGNN is resilient to random initialization and hyperparameter variation, highlighting its strong generalization and practical value for higher-order relational learning.

Abstract: Many real-world interactions are group-based rather than pairwise such as
papers with multiple co-authors and users jointly engaging with items.
Hypergraph neural networks have shown great promise at modeling higher-order
relations, but their reliance on a fixed number of explicit message-passing
layers limits long-range dependency capture and can destabilize training as
depth grows. In this work, we introduce Implicit Hypergraph Neural Networks
(IHGNN), which bring the implicit equilibrium formulation to hypergraphs:
instead of stacking layers, IHGNN computes representations as the solution to a
nonlinear fixed-point equation, enabling stable and efficient global
propagation across hyperedges without deep architectures. We develop a
well-posed training scheme with provable convergence, analyze the oversmoothing
conditions and expressivity of the model, and derive a transductive
generalization bound on hypergraphs. We further present an implicit-gradient
training procedure coupled with a projection-based stabilization strategy.
Extensive experiments on citation benchmarks show that IHGNN consistently
outperforms strong traditional graph/hypergraph neural network baselines in
both accuracy and robustness. Empirically, IHGNN is resilient to random
initialization and hyperparameter variation, highlighting its strong
generalization and practical value for higher-order relational learning.

</details>


### [184] [NEXICA: Discovering Road Traffic Causality (Extended arXiv Version)](https://arxiv.org/abs/2508.09447)
*Siddharth Srikanth,John Krumm,Jonathan Qin*

Main category: cs.LG

TL;DR: 提出了一种名为 NEXICA 的新算法，用于发现高速公路系统中哪些部分容易导致其他部分减速。


<details>
  <summary>Details</summary>
Motivation: 道路交通拥堵是一个长期存在的问题。将资源集中在拥堵的原因上是减少交通拥堵的一种潜在有效策略。

Method: 该研究提出了一种名为 NEXICA 的算法，该算法通过以下三种方式实现：1) 专注于时间序列中事件的存在与否；2) 开发概率模型，使用最大似然估计来计算高速公路上两个位置之间自发减速和导致减速的概率；3) 训练二元分类器以识别原因/结果位置对。

Result: 该方法在洛杉矶地区 195 个不同高速公路速度传感器获得的六个月道路速度数据上进行了测试。

Conclusion: 该算法在准确性和计算速度方面均优于现有技术水平。

Abstract: Road traffic congestion is a persistent problem. Focusing resources on the
causes of congestion is a potentially efficient strategy for reducing
slowdowns. We present NEXICA, an algorithm to discover which parts of the
highway system tend to cause slowdowns on other parts of the highway. We use
time series of road speeds as inputs to our causal discovery algorithm. Finding
other algorithms inadequate, we develop a new approach that is novel in three
ways. First, it concentrates on just the presence or absence of events in the
time series, where an event indicates the temporal beginning of a traffic
slowdown. Second, we develop a probabilistic model using maximum likelihood
estimation to compute the probabilities of spontaneous and caused slowdowns
between two locations on the highway. Third, we train a binary classifier to
identify pairs of cause/effect locations trained on pairs of road locations
where we are reasonably certain a priori of their causal connections, both
positive and negative. We test our approach on six months of road speed data
from 195 different highway speed sensors in the Los Angeles area, showing that
our approach is superior to state-of-the-art baselines in both accuracy and
computation speed.

</details>


### [185] [A Unified Contrastive-Generative Framework for Time Series Classification](https://arxiv.org/abs/2508.09451)
*Ziyu Liu,Azadeh Alavi,Minyi Li,Xiang Zhang*

Main category: cs.LG

TL;DR: CoGenT 是一种统一对比和生成时间序列 SSL 范例的框架，通过混合优化提高了性能并克服了各自的局限性。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列的自我监督学习 (SSL) 主要包括两种范例：擅长实例区分的对比方法和对数据分布进行建模的生成方法。虽然单独有效，但它们的互补潜力仍有待探索。

Method: 对比生成时间序列框架 (CoGenT)，该框架通过联合对比生成优化统一了这些范例。

Result: 结果显示出持续的改进，与独立的 SimCLR 和 MAE 相比，F1 增益分别高达 59.2% 和 14.27%。

Conclusion: 混合目标保留了判别能力，同时获得了生成鲁棒性。这些发现为时间域中的混合 SSL 奠定了基础。

Abstract: Self-supervised learning (SSL) for multivariate time series mainly includes
two paradigms: contrastive methods that excel at instance discrimination and
generative approaches that model data distributions. While effective
individually, their complementary potential remains unexplored. We propose a
Contrastive Generative Time series framework (CoGenT), the first framework to
unify these paradigms through joint contrastive-generative optimization. CoGenT
addresses fundamental limitations of both approaches: it overcomes contrastive
learning's sensitivity to high intra-class similarity in temporal data while
reducing generative methods' dependence on large datasets. We evaluate CoGenT
on six diverse time series datasets. The results show consistent improvements,
with up to 59.2% and 14.27% F1 gains over standalone SimCLR and MAE,
respectively. Our analysis reveals that the hybrid objective preserves
discriminative power while acquiring generative robustness. These findings
establish a foundation for hybrid SSL in temporal domains. We will release the
code shortly.

</details>


### [186] [Open-Set Fault Diagnosis in Multimode Processes via Fine-Grained Deep Feature Representation](https://arxiv.org/abs/2508.09462)
*Guangqiang Li,M. Amine Atoui,Xiangshun Li*

Main category: cs.LG

TL;DR: 提出了一种新的开集故障诊断模型，名为细粒度聚类和拒绝网络 (FGCRN)，它可以有效地识别未知故障。


<details>
  <summary>Details</summary>
Motivation: 可靠的故障诊断系统不仅应准确分类已知健康状态，还应有效识别未知故障。在多模态过程中，属于同一健康状态的样本通常显示多个集群分布，使得难以构建紧凑而准确的该状态的决策边界。

Method: 结合多尺度深度卷积、双向门控循环单元和时间注意力机制来捕获判别性特征。通过无监督学习构建细粒度特征表示，以揭示每个健康状态的内在结构。采用极值理论来建模样本特征与其对应的细粒度表示之间的距离，从而有效识别未知故障。

Result: 所提出的细粒度聚类和拒绝网络 (FGCRN) 结合了多尺度深度卷积、双向门控循环单元和时间注意力机制来捕获判别性特征。设计了一种基于距离的损失函数来增强类内紧凑性。

Conclusion: 该方法在大量实验中表现出优越的性能。

Abstract: A reliable fault diagnosis system should not only accurately classify known
health states but also effectively identify unknown faults. In multimode
processes, samples belonging to the same health state often show multiple
cluster distributions, making it difficult to construct compact and accurate
decision boundaries for that state. To address this challenge, a novel open-set
fault diagnosis model named fine-grained clustering and rejection network
(FGCRN) is proposed. It combines multiscale depthwise convolution,
bidirectional gated recurrent unit and temporal attention mechanism to capture
discriminative features. A distance-based loss function is designed to enhance
the intra-class compactness. Fine-grained feature representations are
constructed through unsupervised learning to uncover the intrinsic structures
of each health state. Extreme value theory is employed to model the distance
between sample features and their corresponding fine-grained representations,
enabling effective identification of unknown faults. Extensive experiments
demonstrate the superior performance of the proposed method.

</details>


### [187] [Learn to Explore: Meta NAS via Bayesian Optimization Guided Graph Generation](https://arxiv.org/abs/2508.09467)
*Zijun Sun,Yanning Shen*

Main category: cs.LG

TL;DR: GraB-NAS是一种新的Meta-NAS框架，通过混合搜索策略发现具有强大性能的任务感知架构。


<details>
  <summary>Details</summary>
Motivation: 现有的Meta-NAS方法在泛化性、搜索空间或计算成本方面存在不足。

Method: 结合贝叶斯优化进行全局架构搜索，并利用潜在空间中的梯度上升进行局部探索。

Result: GraB-NAS实现了更好的泛化能力和搜索效率，优于当前Meta-NAS技术。

Conclusion: GraB-NAS在泛化能力和搜索效率方面优于当前Meta-NAS技术。

Abstract: Neural Architecture Search (NAS) automates the design of high-performing
neural networks but typically targets a single predefined task, thereby
restricting its real-world applicability. To address this, Meta Neural
Architecture Search (Meta-NAS) has emerged as a promising paradigm that
leverages prior knowledge across tasks to enable rapid adaptation to new ones.
Nevertheless, existing Meta-NAS methods often struggle with poor
generalization, limited search spaces, or high computational costs. In this
paper, we propose a novel Meta-NAS framework, GraB-NAS. Specifically, GraB-NAS
first models neural architectures as graphs, and then a hybrid search strategy
is developed to find and generate new graphs that lead to promising neural
architectures. The search strategy combines global architecture search via
Bayesian Optimization in the search space with local exploration for novel
neural networks via gradient ascent in the latent space. Such a hybrid search
strategy allows GraB-NAS to discover task-aware architectures with strong
performance, even beyond the predefined search space. Extensive experiments
demonstrate that GraB-NAS outperforms state-of-the-art Meta-NAS baselines,
achieving better generalization and search effectiveness.

</details>


### [188] [DeepFeatIoT: Unifying Deep Learned, Randomized, and LLM Features for Enhanced IoT Time Series Sensor Data Classification in Smart Industries](https://arxiv.org/abs/2508.09468)
*Muhammad Sakib Khan Inan,Kewen Liao*

Main category: cs.LG

TL;DR: 提出了一种新的深度学习模型DeepFeatIoT，该模型集成了学习的和非学习的特征，显著提高了物联网时间序列传感器数据的分类性能，并在各种实际数据集中优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 物联网（IoT）传感器是部署在智慧城市、工业场所和医疗保健系统中普遍存在的技术。它们不断生成时间序列数据，从而可以在行业中实现高级分析和自动化。但是，传感器元数据的丢失或不明确、数据源的异构性、采样频率的变化、不一致的测量单位以及不规则的时间戳等挑战使得原始IoT时间序列数据难以解释，从而削弱了智能系统的有效性。

Method: 我们提出了一个新的深度学习模型DeepFeatIoT，它集成了学习到的局部和全局特征与非学习到的随机卷积核特征以及来自大型语言模型（LLM）的特征。

Result: 即使在标记数据有限的情况下，这种对各种学习和非学习特征的简单而独特的融合也大大增强了IoT时间序列传感器数据的分类。

Conclusion: DeepFeatIoT在多个真实世界的物联网传感器数据集上表现出一致和广泛的性能，优于目前最好的基准模型。该结果突出了DeepFeatIoT在推动物联网分析方面的巨大潜力，并支持下一代智能系统的发展。

Abstract: Internet of Things (IoT) sensors are ubiquitous technologies deployed across
smart cities, industrial sites, and healthcare systems. They continuously
generate time series data that enable advanced analytics and automation in
industries. However, challenges such as the loss or ambiguity of sensor
metadata, heterogeneity in data sources, varying sampling frequencies,
inconsistent units of measurement, and irregular timestamps make raw IoT time
series data difficult to interpret, undermining the effectiveness of smart
systems. To address these challenges, we propose a novel deep learning model,
DeepFeatIoT, which integrates learned local and global features with
non-learned randomized convolutional kernel-based features and features from
large language models (LLMs). This straightforward yet unique fusion of diverse
learned and non-learned features significantly enhances IoT time series sensor
data classification, even in scenarios with limited labeled data. Our model's
effectiveness is demonstrated through its consistent and generalized
performance across multiple real-world IoT sensor datasets from diverse
critical application domains, outperforming state-of-the-art benchmark models.
These results highlight DeepFeatIoT's potential to drive significant
advancements in IoT analytics and support the development of next-generation
smart systems.

</details>


### [189] [EGGS-PTP: An Expander-Graph Guided Structured Post-training Pruning Method for Large Language Models](https://arxiv.org/abs/2508.09471)
*Omar Bazarbachi,Zijun Sun,Yanning Shen*

Main category: cs.LG

TL;DR: EGGS-PTP：一种Expander-Graph引导的结构化后训练剪枝方法，可有效减少模型大小和计算需求，同时保持模型功能。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）的应用越来越广泛并且规模不断扩大，部署这些大型基础模型所涉及的计算和内存挑战变得越来越严峻。这突显了开发更高效模型变体的迫切需求。

Method: Expander-Graph Guided Structured Post-training Pruning

Result: EGGS-PTP不仅由于结构化稀疏性而实现了显著的加速和内存节省，而且在各种LLM上的准确率方面优于现有的结构化剪枝技术。

Conclusion: EGGS-PTP在各种LLM上的准确率方面优于现有的结构化剪枝技术，同时实现了显著的加速和内存节省。

Abstract: As Large Language Models (LLMs) become more widely adopted and scale up in
size, the computational and memory challenges involved in deploying these
massive foundation models have grown increasingly severe. This underscores the
urgent need to develop more efficient model variants. Faced with this
challenge, the present work introduces EGGS-PTP: an Expander-Graph Guided
Structured Post-training Pruning method. The proposed approach leverages graph
theory to guide the design of N:M structured pruning, effectively reducing
model size and computational demands. By incorporating concepts from expander
graphs, EGGS-PTP ensures information flow within the pruned network, preserving
essential model functionality. Extensive numerical experiments demonstrate that
EGGS-PTP not only achieves significant acceleration and memory savings due to
structured sparsity but also outperforms existing structured pruning techniques
in terms of accuracy across various LLMs.

</details>


### [190] [Large-Small Model Collaborative Framework for Federated Continual Learning](https://arxiv.org/abs/2508.09489)
*Hao Yu,Xin Yang,Boyang Fan,Xuemei Cao,Hanlin Gu,Lixin Fan,Qiang Yang*

Main category: cs.LG

TL;DR: 提出了一个联邦持续学习的协作框架，通过小模型持续微调和逐个知识蒸馏来提升大模型的效用。


<details>
  <summary>Details</summary>
Motivation: 解决联邦持续学习中，基础模型无法利用本地数据，且因参数众多和模型复杂性带来的持续学习难题。

Method: 小模型持续微调，逐个知识蒸馏。

Result: 框架在联邦持续学习中表现出卓越的性能，即使客户端使用异构的小模型。

Conclusion: 提出的框架表现出卓越的性能，即使客户端使用异构的小模型。

Abstract: Continual learning (CL) for Foundation Models (FMs) is an essential yet
underexplored challenge, especially in Federated Continual Learning (FCL),
where each client learns from a private, evolving task stream under strict data
and communication constraints. Despite their powerful generalization abilities,
FMs often exhibit suboptimal performance on local downstream tasks, as they are
unable to utilize private local data. Furthermore, enabling FMs to learn new
tasks without forgetting prior knowledge is inherently a challenging problem,
primarily due to their immense parameter count and high model complexity. In
contrast, small models can be trained locally under resource-constrained
conditions and benefit from more mature CL techniques. To bridge the gap
between small models and FMs, we propose the first collaborative framework in
FCL, where lightweight local models act as a dynamic bridge, continually
adapting to new tasks while enhancing the utility of the large model. Two novel
components are also included: Small Model Continual Fine-tuning is for
preventing small models from temporal forgetting; One-by-One Distillation
performs personalized fusion of heterogeneous local knowledge on the server.
Experimental results demonstrate its superior performance, even when clients
utilize heterogeneous small models.

</details>


### [191] [MiCo: End-to-End Mixed Precision Neural Network Co-Exploration Framework for Edge AI](https://arxiv.org/abs/2508.09500)
*Zijun Jiang,Yangdi Lyu*

Main category: cs.LG

TL;DR: This paper introduces MiCo, a framework for optimizing and deploying mixed-precision quantized neural networks on edge devices, achieving speedup with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Existing algorithms for exploring MPQ schemes are limited in flexibility and efficiency, and an end-to-end framework for the optimization and deployment of MPQ models is missing.

Method: The framework adopts a novel optimization algorithm to search for optimal quantization schemes and builds hardware-aware latency models for different hardware targets.

Result: The framework enables direct deployment from PyTorch MPQ models to bare-metal C codes, leading to end-to-end speedup with minimal accuracy drops.

Conclusion: The paper proposes the MiCo framework for mixed-precision quantization (MPQ) exploration and deployment, enabling end-to-end speedup with minimal accuracy drops by adopting a novel optimization algorithm and hardware-aware latency models.

Abstract: Quantized Neural Networks (QNN) with extremely low-bitwidth data have proven
promising in efficient storage and computation on edge devices. To further
reduce the accuracy drop while increasing speedup, layer-wise mixed-precision
quantization (MPQ) becomes a popular solution. However, existing algorithms for
exploring MPQ schemes are limited in flexibility and efficiency. Comprehending
the complex impacts of different MPQ schemes on post-training quantization and
quantization-aware training results is a challenge for conventional methods.
Furthermore, an end-to-end framework for the optimization and deployment of MPQ
models is missing in existing work.
  In this paper, we propose the MiCo framework, a holistic MPQ exploration and
deployment framework for edge AI applications. The framework adopts a novel
optimization algorithm to search for optimal quantization schemes with the
highest accuracies while meeting latency constraints. Hardware-aware latency
models are built for different hardware targets to enable fast explorations.
After the exploration, the framework enables direct deployment from PyTorch MPQ
models to bare-metal C codes, leading to end-to-end speedup with minimal
accuracy drops.

</details>


### [192] [Causal Graph Profiling via Structural Divergence for Robust Anomaly Detection in Cyber-Physical Systems](https://arxiv.org/abs/2508.09504)
*Arun Vignesh Malarkkan,Haoyue Bai,Dongjie Wang,Yanjie Fu*

Main category: cs.LG

TL;DR: CGAD是一个基于因果图的异常检测框架，它通过学习因果关系并在非平稳和不平衡时间序列中检测异常来有效地检测网络攻击。


<details>
  <summary>Details</summary>
Motivation: 针对水处理网络等关键基础设施的网络攻击日益复杂，迫切需要强大的异常检测策略，以应对系统漏洞和不断演变的攻击模式。传统方法在多元时间序列中难以应对分布变化和类别不平衡问题，通常导致较高的误报率。

Method: CGAD，一个基于因果图的异常检测框架，采用双阶段监督框架——因果剖析和异常评分。首先，使用动态贝叶斯网络学习表示系统在“正常”和“攻击”状态下的因果不变图结构。其次，通过因果图比较评估因果图中随时间变化的拓扑偏差，从而利用结构发散来检测异常。

Result: CGAD框架在四个工业数据集上获得了比最佳基线更高的F1和ROC-AUC分数，证明了其对延迟和结构复杂异常的强大检测能力。

Conclusion: CGAD框架通过利用因果结构，在非平稳和不平衡时间序列环境中实现了优于传统机器学习方法的适应性和准确性，从而能够以更高的精度检测网络攻击，并在异常检测中重新定义了鲁棒性。

Abstract: With the growing complexity of cyberattacks targeting critical
infrastructures such as water treatment networks, there is a pressing need for
robust anomaly detection strategies that account for both system
vulnerabilities and evolving attack patterns. Traditional methods --
statistical, density-based, and graph-based models struggle with distribution
shifts and class imbalance in multivariate time series, often leading to high
false positive rates. To address these challenges, we propose CGAD, a Causal
Graph-based Anomaly Detection framework designed for reliable cyberattack
detection in public infrastructure systems. CGAD follows a two-phase supervised
framework -- causal profiling and anomaly scoring. First, it learns causal
invariant graph structures representing the system's behavior under "Normal"
and "Attack" states using Dynamic Bayesian Networks. Second, it employs
structural divergence to detect anomalies via causal graph comparison by
evaluating topological deviations in causal graphs over time. By leveraging
causal structures, CGAD achieves superior adaptability and accuracy in
non-stationary and imbalanced time series environments compared to conventional
machine learning approaches. By uncovering causal structures beneath volatile
sensor data, our framework not only detects cyberattacks with markedly higher
precision but also redefines robustness in anomaly detection, proving
resilience where traditional models falter under imbalance and drift. Our
framework achieves substantial gains in F1 and ROC-AUC scores over
best-performing baselines across four industrial datasets, demonstrating robust
detection of delayed and structurally complex anomalies.

</details>


### [193] [Enhancing Memory Recall in LLMs with Gauss-Tin: A Hybrid Instructional and Gaussian Replay Approach](https://arxiv.org/abs/2508.09510)
*Iing Muttakhiroh,Thomas Fevens*

Main category: cs.LG

TL;DR: Gauss-Tin通过策略性地加强重要的过去学习，同时适应新信息，从而提高 LLM 的保留能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 取得了显著进步，但灾难性遗忘仍然是一个巨大的挑战，模型在学习新信息时会丢失先前获得的知识。持续学习 (CL) 策略已成为解决此问题的一种潜在解决方案，其中基于重放的技术在保留学习知识方面表现出卓越的性能。

Method: 将重放策略与高斯混合模型集成，以增强训练期间的样本选择质量，并辅以教学指导以促进过去学习的生成。

Result: 与传统方法相比，保留指标提高了 6%。

Conclusion: Gauss-Tin是一种有效的策略，可以减轻LLM中的灾难性遗忘。

Abstract: Despite the significant advancements in Large Language Models (LLMs),
catastrophic forgetting remains a substantial challenge, where models lose
previously acquired knowledge upon learning new information. Continual learning
(CL) strategies have emerged as a potential solution to this problem, with
replay-based techniques demonstrating superior performance in preserving
learned knowledge. In this context, we introduce Gauss-Tin, a novel approach
that integrates the replay strategy with a Gaussian mixture model to enhance
the quality of sample selection during training, supplemented by instructional
guidance to facilitate the generation of past learning. This method aims to
improve LLMs' retention capabilities by strategically reinforcing important
past learnings while accommodating new information. Our experimental results
indicate a promising 6\% improvement in retention metrics over traditional
methods, suggesting that Gauss-Tin is an effective strategy for mitigating
catastrophic forgetting in LLMs. This study underscores the potential of hybrid
models in enhancing the robustness and adaptability of LLMs in dynamic learning
environments.

</details>
