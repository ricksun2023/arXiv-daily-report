{"id": "2510.26852", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.26852", "abs": "https://arxiv.org/abs/2510.26852", "authors": ["Lingyue Fu", "Xin Ding", "Yaoming Zhu", "Shao Zhang", "Lin Qiu", "Weiwen Liu", "Weinan Zhang", "Xuezhi Cao", "Xunliang Cai", "Jiaxin Ding", "Yong Yu"], "title": "CATArena: Evaluation of LLM Agents through Iterative Tournament Competitions", "comment": null, "summary": "Large Language Model (LLM) agents have evolved from basic text generation to\nautonomously completing complex tasks through interaction with external tools.\nHowever, current benchmarks mainly assess end-to-end performance in fixed\nscenarios, restricting evaluation to specific skills and suffering from score\nsaturation and growing dependence on expert annotation as agent capabilities\nimprove. In this work, we emphasize the importance of learning ability,\nincluding both self-improvement and peer-learning, as a core driver for agent\nevolution toward human-level intelligence. We propose an iterative, competitive\npeer-learning framework, which allows agents to refine and optimize their\nstrategies through repeated interactions and feedback, thereby systematically\nevaluating their learning capabilities. To address the score saturation issue\nin current benchmarks, we introduce CATArena, a tournament-style evaluation\nplatform featuring four diverse board and card games with open-ended scoring.\nBy providing tasks without explicit upper score limits, CATArena enables\ncontinuous and dynamic evaluation of rapidly advancing agent capabilities.\nExperimental results and analyses involving both minimal and commercial code\nagents demonstrate that CATArena provides reliable, stable, and scalable\nbenchmarking for core agent abilities, particularly learning ability and\nstrategy coding.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u5b66\u4e60\u80fd\u529b\u7684\u8fed\u4ee3\u5f0f\u3001\u7ade\u4e89\u6027\u540c\u4f34\u5b66\u4e60\u6846\u67b6CATArena\u3002CATArena\u901a\u8fc7\u63d0\u4f9b\u5177\u6709\u5f00\u653e\u5f0f\u8bc4\u5206\u7684\u68cb\u76d8\u548c\u7eb8\u724c\u6e38\u620f\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u6570\u9971\u548c\u7684\u95ee\u9898\uff0c\u5e76\u80fd\u591f\u6301\u7eed\u548c\u52a8\u6001\u5730\u8bc4\u4f30\u5feb\u901f\u53d1\u5c55\u7684\u4ee3\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u56fa\u5b9a\u573a\u666f\u4e2d\u7684\u7aef\u5230\u7aef\u6027\u80fd\uff0c\u9650\u5236\u4e86\u5bf9\u7279\u5b9a\u6280\u80fd\u7684\u8bc4\u4f30\uff0c\u5e76\u4e14\u968f\u7740\u4ee3\u7406\u80fd\u529b\u7684\u63d0\u9ad8\uff0c\u9762\u4e34\u5206\u6570\u9971\u548c\u548c\u5bf9\u4e13\u5bb6\u6ce8\u91ca\u4f9d\u8d56\u6027\u589e\u52a0\u7684\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u8bba\u6587\u5f3a\u8c03\u4e86\u5b66\u4e60\u80fd\u529b\uff08\u5305\u62ec\u81ea\u6211\u63d0\u5347\u548c\u540c\u4f34\u5b66\u4e60\uff09\u4f5c\u4e3a\u4ee3\u7406\u5411\u4eba\u7c7b\u6c34\u5e73\u667a\u80fd\u6f14\u8fdb\u7684\u6838\u5fc3\u9a71\u52a8\u529b\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8fed\u4ee3\u7684\u3001\u7ade\u4e89\u6027\u7684\u540c\u4f34\u5b66\u4e60\u6846\u67b6\uff0c\u5141\u8bb8\u4ee3\u7406\u901a\u8fc7\u91cd\u590d\u7684\u4ea4\u4e92\u548c\u53cd\u9988\u6765\u6539\u8fdb\u548c\u4f18\u5316\u4ed6\u4eec\u7684\u7b56\u7565\uff0c\u4ece\u800c\u7cfb\u7edf\u5730\u8bc4\u4f30\u4ed6\u4eec\u7684\u5b66\u4e60\u80fd\u529b\u3002\u540c\u65f6\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u9526\u6807\u8d5b\u5f0f\u7684\u8bc4\u4f30\u5e73\u53f0CATArena\uff0c\u5176\u4e2d\u5305\u542b\u56db\u4e2a\u5177\u6709\u5f00\u653e\u5f0f\u8bc4\u5206\u7684\u591a\u6837\u5316\u68cb\u76d8\u548c\u7eb8\u724c\u6e38\u620f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u548c\u5206\u6790\u8868\u660e\uff0cCATArena\u4e3a\u6838\u5fc3\u4ee3\u7406\u80fd\u529b\uff08\u7279\u522b\u662f\u5b66\u4e60\u80fd\u529b\u548c\u7b56\u7565\u7f16\u7801\uff09\u63d0\u4f9b\u4e86\u53ef\u9760\u3001\u7a33\u5b9a\u548c\u53ef\u6269\u5c55\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "CATArena\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u6709\u6548\u8bc4\u4f30LLM\u4ee3\u7406\u5b66\u4e60\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.26854", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26854", "abs": "https://arxiv.org/abs/2510.26854", "authors": ["Yu Li", "Yuan Huang", "Tao Wang", "Caiyu Fan", "Xiansheng Cai", "Sihan Hu", "Xinzijian Liu", "Cheng Shi", "Mingjun Xu", "Zhen Wang", "Yan Wang", "Xiangqi Jin", "Tianhan Zhang", "Linfeng Zhang", "Lei Wang", "Youjin Deng", "Pan Zhang", "Weijie Sun", "Xingyu Li", "Weinan E", "Linfeng Zhang", "Zhiyuan Yao", "Kun Chen"], "title": "Inverse Knowledge Search over Verifiable Reasoning: Synthesizing a Scientific Encyclopedia from a Long Chains-of-Thought Knowledge Base", "comment": "43 pages, 4 figures", "summary": "Most scientific materials compress reasoning, presenting conclusions while\nomitting the derivational chains that justify them. This compression hinders\nverification by lacking explicit, step-wise justifications and inhibits\ncross-domain links by collapsing the very pathways that establish the logical\nand causal connections between concepts. We introduce a scalable framework that\ndecompresses scientific reasoning, constructing a verifiable Long\nChain-of-Thought (LCoT) knowledge base and projecting it into an emergent\nencyclopedia, SciencePedia. Our pipeline operationalizes an endpoint-driven,\nreductionist strategy: a Socratic agent, guided by a curriculum of around 200\ncourses, generates approximately 3 million first-principles questions. To\nensure high fidelity, multiple independent solver models generate LCoTs, which\nare then rigorously filtered by prompt sanitization and cross-model answer\nconsensus, retaining only those with verifiable endpoints. This verified corpus\npowers the Brainstorm Search Engine, which performs inverse knowledge search --\nretrieving diverse, first-principles derivations that culminate in a target\nconcept. This engine, in turn, feeds the Plato synthesizer, which narrates\nthese verified chains into coherent articles. The initial SciencePedia\ncomprises approximately 200,000 fine-grained entries spanning mathematics,\nphysics, chemistry, biology, engineering, and computation. In evaluations\nacross six disciplines, Plato-synthesized articles (conditioned on retrieved\nLCoTs) exhibit substantially higher knowledge-point density and significantly\nlower factual error rates than an equally-prompted baseline without retrieval\n(as judged by an external LLM). Built on this verifiable LCoT knowledge base,\nthis reasoning-centric approach enables trustworthy, cross-domain scientific\nsynthesis at scale and establishes the foundation for an ever-expanding\nencyclopedia.", "AI": {"tldr": "This paper introduces a framework to decompress scientific reasoning by constructing a verifiable Long Chain-of-Thought (LCoT) knowledge base and projecting it into an encyclopedia, SciencePedia.", "motivation": "Current scientific materials compress reasoning, hindering verification and cross-domain links.", "method": "A Socratic agent generates questions, multiple solver models generate LCoTs, and these are filtered by prompt sanitization and cross-model answer consensus. A search engine retrieves derivations, and a synthesizer narrates these into articles.", "result": "SciencePedia comprises 200,000 entries and exhibits higher knowledge-point density and lower factual error rates compared to a baseline.", "conclusion": "The reasoning-centric approach enables trustworthy, cross-domain scientific synthesis at scale and establishes the foundation for an expanding encyclopedia."}}
{"id": "2510.26887", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.26887", "abs": "https://arxiv.org/abs/2510.26887", "authors": ["Francisco Villaescusa-Navarro", "Boris Bolliet", "Pablo Villanueva-Domingo", "Adrian E. Bayer", "Aidan Acquah", "Chetana Amancharla", "Almog Barzilay-Siegal", "Pablo Bermejo", "Camille Bilodeau", "Pablo C\u00e1rdenas Ram\u00edrez", "Miles Cranmer", "Urbano L. Fran\u00e7a", "ChangHoon Hahn", "Yan-Fei Jiang", "Raul Jimenez", "Jun-Young Lee", "Antonio Lerario", "Osman Mamun", "Thomas Meier", "Anupam A. Ojha", "Pavlos Protopapas", "Shimanto Roy", "David N. Spergel", "Pedro Taranc\u00f3n-\u00c1lvarez", "Ujjwal Tiwari", "Matteo Viel", "Digvijay Wadekar", "Chi Wang", "Bonny Y. Wang", "Licong Xu", "Yossi Yovel", "Shuwen Yue", "Wen-Han Zhou", "Qiyao Zhu", "Jiajun Zou", "\u00cd\u00f1igo Zubeldia"], "title": "The Denario project: Deep knowledge AI agents for scientific discovery", "comment": "272 pages. Examples of 11 AI-generated paper drafts from different\n  scientific disciplines. Code publicly available at\n  https://github.com/AstroPilot-AI/Denario", "summary": "We present Denario, an AI multi-agent system designed to serve as a\nscientific research assistant. Denario can perform many different tasks, such\nas generating ideas, checking the literature, developing research plans,\nwriting and executing code, making plots, and drafting and reviewing a\nscientific paper. The system has a modular architecture, allowing it to handle\nspecific tasks, such as generating an idea, or carrying out end-to-end\nscientific analysis using Cmbagent as a deep-research backend. In this work, we\ndescribe in detail Denario and its modules, and illustrate its capabilities by\npresenting multiple AI-generated papers generated by it in many different\nscientific disciplines such as astrophysics, biology, biophysics, biomedical\ninformatics, chemistry, material science, mathematical physics, medicine,\nneuroscience and planetary science. Denario also excels at combining ideas from\ndifferent disciplines, and we illustrate this by showing a paper that applies\nmethods from quantum physics and machine learning to astrophysical data. We\nreport the evaluations performed on these papers by domain experts, who\nprovided both numerical scores and review-like feedback. We then highlight the\nstrengths, weaknesses, and limitations of the current system. Finally, we\ndiscuss the ethical implications of AI-driven research and reflect on how such\ntechnology relates to the philosophy of science. We publicly release the code\nat https://github.com/AstroPilot-AI/Denario. A Denario demo can also be run\ndirectly on the web at https://huggingface.co/spaces/astropilot-ai/Denario, and\nthe full app will be deployed on the cloud.", "AI": {"tldr": "Denario is an AI multi-agent system that acts as a scientific research assistant.", "motivation": "To create an AI system that can perform various research tasks and generate scientific papers across multiple disciplines.", "method": "A modular architecture is used, with Cmbagent as a deep-research backend. The system generates papers in various scientific disciplines and combines ideas from different fields.", "result": "Denario generated papers in multiple scientific disciplines, which were evaluated by domain experts. The system's strengths, weaknesses, and limitations were highlighted.", "conclusion": "The paper discusses the ethical implications of AI-driven research and reflects on the technology's relation to the philosophy of science. The code is publicly released."}}
{"id": "2510.26912", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.26912", "abs": "https://arxiv.org/abs/2510.26912", "authors": ["Hyunji Lee", "Wenhao Yu", "Hongming Zhang", "Kaixin Ma", "Jiyeon Kim", "Dong Yu", "Minjoon Seo"], "title": "Understanding and Enhancing Mamba-Transformer Hybrids for Memory Recall and Language Modeling", "comment": null, "summary": "Hybrid models that combine state space models (SSMs) with attention\nmechanisms have shown strong performance by leveraging the efficiency of SSMs\nand the high recall ability of attention. However, the architectural design\nchoices behind these hybrid models remain insufficiently understood. In this\nwork, we analyze hybrid architectures through the lens of memory utilization\nand overall performance, and propose a complementary method to further enhance\ntheir effectiveness. We first examine the distinction between sequential and\nparallel integration of SSM and attention layers. Our analysis reveals several\ninteresting findings, including that sequential hybrids perform better on\nshorter contexts, whereas parallel hybrids are more effective for longer\ncontexts. We also introduce a data-centric approach of continually training on\ndatasets augmented with paraphrases, which further enhances recall while\npreserving other capabilities. It generalizes well across different base models\nand outperforms architectural modifications aimed at enhancing recall. Our\nfindings provide a deeper understanding of hybrid SSM-attention models and\noffer practical guidance for designing architectures tailored to various use\ncases. Our findings provide a deeper understanding of hybrid SSM-attention\nmodels and offer practical guidance for designing architectures tailored to\nvarious use cases.", "AI": {"tldr": "\u5206\u6790\u4e86\u6df7\u5408SSM-Attention\u6a21\u578b\u7684\u67b6\u6784\u8bbe\u8ba1\u9009\u62e9\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u4e92\u8865\u65b9\u6cd5\u6765\u63d0\u9ad8\u5176\u6709\u6548\u6027\u3002", "motivation": "\u6df7\u5408\u6a21\u578b\u7ed3\u5408\u4e86SSM\u7684\u6548\u7387\u548cAttention\u7684\u9ad8\u53ec\u56de\u80fd\u529b\uff0c\u4f46\u5176\u80cc\u540e\u7684\u67b6\u6784\u8bbe\u8ba1\u9009\u62e9\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u901a\u8fc7\u5185\u5b58\u5229\u7528\u7387\u548c\u6574\u4f53\u6027\u80fd\u7684\u89d2\u5ea6\u5206\u6790\u6df7\u5408\u67b6\u6784\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u6301\u7eed\u8bad\u7ec3\u6570\u636e\u96c6\u589e\u5f3a\u7684\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u987a\u5e8f\u6df7\u5408\u6a21\u578b\u5728\u8f83\u77ed\u4e0a\u4e0b\u6587\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u800c\u5e76\u884c\u6df7\u5408\u6a21\u578b\u5728\u8f83\u957f\u4e0a\u4e0b\u6587\u4e2d\u66f4\u6709\u6548\u3002\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u53ec\u56de\u7387\uff0c\u4e14\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u66f4\u6df1\u5165\u5730\u7406\u89e3\u4e86\u6df7\u5408SSM-Attention\u6a21\u578b\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u9002\u7528\u4e8e\u5404\u79cd\u7528\u4f8b\u7684\u67b6\u6784\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6307\u5bfc\u3002"}}
{"id": "2510.26905", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26905", "abs": "https://arxiv.org/abs/2510.26905", "authors": ["Pedro Antonio Alarc\u00f3n Granadeno", "Arturo Miguel Bernal Russell", "Sofia Nelson", "Demetrius Hernandez", "Maureen Petterson", "Michael Murphy", "Walter J. Scheirer", "Jane Cleland-Huang"], "title": "Cognition Envelopes for Bounded AI Reasoning in Autonomous UAS Operations", "comment": "10.5 pages, 9 figures", "summary": "Cyber-physical systems increasingly rely on Foundational Models such as Large\nLanguage Models (LLMs) and Vision-Language Models (VLMs) to increase autonomy\nthrough enhanced perception, inference, and planning. However, these models\nalso introduce new types of errors, such as hallucinations,\novergeneralizations, and context misalignments, resulting in incorrect and\nflawed decisions. To address this, we introduce the concept of Cognition\nEnvelopes, designed to establish reasoning boundaries that constrain\nAI-generated decisions while complementing the use of meta-cognition and\ntraditional safety envelopes. As with safety envelopes, Cognition Envelopes\nrequire practical guidelines and systematic processes for their definition,\nvalidation, and assurance.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u8ba4\u77e5\u5305\u7edc\u7684\u6982\u5ff5\uff0c\u65e8\u5728\u7ea6\u675f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\u4e2d\u7684\u51b3\u7b56\uff0c\u4ee5\u51cf\u5c11\u5e7b\u89c9\u3001\u8fc7\u5ea6\u6982\u62ec\u548c\u4e0a\u4e0b\u6587\u9519\u4f4d\u7b49\u9519\u8bef\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u4f1a\u5f15\u5165\u65b0\u7684\u9519\u8bef\u7c7b\u578b\uff0c\u5bfc\u81f4\u9519\u8bef\u7684\u51b3\u7b56\u3002", "method": "\u63d0\u51fa\u8ba4\u77e5\u5305\u7edc\u7684\u6982\u5ff5\uff0c\u7528\u4e8e\u5efa\u7acb\u63a8\u7406\u8fb9\u754c\uff0c\u7ea6\u675fAI\u751f\u6210\u7684\u51b3\u7b56\u3002", "result": "\u8ba4\u77e5\u5305\u7edc\u9700\u8981\u50cf\u5b89\u5168\u5305\u7edc\u4e00\u6837\uff0c\u5236\u5b9a\u5b9e\u7528\u7684\u6307\u5357\u548c\u7cfb\u7edf\u7684\u6d41\u7a0b\u6765\u8fdb\u884c\u5b9a\u4e49\u3001\u9a8c\u8bc1\u548c\u4fdd\u8bc1\u3002", "conclusion": "\u8ba4\u77e5\u5305\u7edc\u53ef\u4ee5\u4f5c\u4e3a\u5143\u8ba4\u77e5\u548c\u4f20\u7edf\u5b89\u5168\u5305\u7edc\u7684\u8865\u5145\uff0c\u63d0\u9ad8\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\u7684\u81ea\u4e3b\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2510.26969", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26969", "abs": "https://arxiv.org/abs/2510.26969", "authors": ["L\u00edvia Dutra", "Arthur Lorenzi", "La\u00eds Berno", "Franciany Campos", "Karoline Biscardi", "Kenneth Brown", "Marcelo Viridiano", "Frederico Belcavello", "Ely Matos", "Ol\u00edvia Guaranha", "Erik Santos", "Sofia Reinach", "Tiago Timponi Torrent"], "title": "Frame Semantic Patterns for Identifying Underreporting of Notifiable Events in Healthcare: The Case of Gender-Based Violence", "comment": null, "summary": "We introduce a methodology for the identification of notifiable events in the\ndomain of healthcare. The methodology harnesses semantic frames to define\nfine-grained patterns and search them in unstructured data, namely, open-text\nfields in e-medical records. We apply the methodology to the problem of\nunderreporting of gender-based violence (GBV) in e-medical records produced\nduring patients' visits to primary care units. A total of eight patterns are\ndefined and searched on a corpus of 21 million sentences in Brazilian\nPortuguese extracted from e-SUS APS. The results are manually evaluated by\nlinguists and the precision of each pattern measured. Our findings reveal that\nthe methodology effectively identifies reports of violence with a precision of\n0.726, confirming its robustness. Designed as a transparent, efficient,\nlow-carbon, and language-agnostic pipeline, the approach can be easily adapted\nto other health surveillance contexts, contributing to the broader, ethical,\nand explainable use of NLP in public health systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u533b\u7597\u9886\u57df\u8bc6\u522b\u5e94\u62a5\u544a\u4e8b\u4ef6\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u8bed\u4e49\u6846\u67b6\u5728\u975e\u7ed3\u6784\u5316\u6570\u636e\u4e2d\u641c\u7d22\u7ec6\u7c92\u5ea6\u6a21\u5f0f\u3002", "motivation": "\u89e3\u51b3\u7535\u5b50\u75c5\u5386\u4e2d\u6027\u522b\u66b4\u529b\uff08GBV\uff09\u6f0f\u62a5\u95ee\u9898\u3002", "method": "\u5b9a\u4e49\u4e86\u516b\u79cd\u6a21\u5f0f\uff0c\u5e76\u5728\u4ecee-SUS APS\u63d0\u53d6\u76842100\u4e07\u53e5\u8461\u8404\u7259\u8bed\u8bed\u6599\u5e93\u4e2d\u641c\u7d22\u8fd9\u4e9b\u6a21\u5f0f\u3002\u7ed3\u679c\u7531\u8bed\u8a00\u5b66\u5bb6\u624b\u52a8\u8bc4\u4f30\uff0c\u5e76\u6d4b\u91cf\u6bcf\u79cd\u6a21\u5f0f\u7684\u7cbe\u5ea6\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8bc6\u522b\u66b4\u529b\u62a5\u544a\uff0c\u7cbe\u5ea6\u4e3a0.726\uff0c\u8bc1\u5b9e\u4e86\u5176\u7a33\u5065\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u900f\u660e\u3001\u9ad8\u6548\u3001\u4f4e\u78b3\u548c\u8bed\u8a00\u65e0\u5173\u7684\u7279\u70b9\uff0c\u53ef\u8f7b\u677e\u9002\u5e94\u5176\u4ed6\u5065\u5eb7\u76d1\u6d4b\u73af\u5883\uff0c\u6709\u52a9\u4e8e\u5728\u516c\u5171\u536b\u751f\u7cfb\u7edf\u4e2d\u66f4\u5e7f\u6cdb\u3001\u5408\u4e4e\u9053\u5fb7\u548c\u53ef\u89e3\u91ca\u5730\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3002"}}
{"id": "2510.26865", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26865", "abs": "https://arxiv.org/abs/2510.26865", "authors": ["Fenfen Lin", "Yesheng Liu", "Haiyu Xu", "Chen Yue", "Zheqi He", "Mingxuan Zhao", "Miguel Hu Chen", "Jiakang Liu", "JG Yao", "Xi Yang"], "title": "Do Vision-Language Models Measure Up? Benchmarking Visual Measurement Reading with MeasureBench", "comment": "Project page: https://flageval-baai.github.io/MeasureBenchPage/", "summary": "Reading measurement instruments is effortless for humans and requires\nrelatively little domain expertise, yet it remains surprisingly challenging for\ncurrent vision-language models (VLMs) as we find in preliminary evaluation. In\nthis work, we introduce MeasureBench, a benchmark on visual measurement reading\ncovering both real-world and synthesized images of various types of\nmeasurements, along with an extensible pipeline for data synthesis. Our\npipeline procedurally generates a specified type of gauge with controllable\nvisual appearance, enabling scalable variation in key details such as pointers,\nscales, fonts, lighting, and clutter. Evaluation on popular proprietary and\nopen-weight VLMs shows that even the strongest frontier VLMs struggle\nmeasurement reading in general. A consistent failure mode is indicator\nlocalization: models can read digits or labels but misidentify the key\npositions of pointers or alignments, leading to big numeric errors despite\nplausible textual reasoning. We have also conducted preliminary experiments\nwith reinforcement learning over synthetic data, and find encouraging results\non in-domain synthetic subset but less promising for real-world images. Our\nanalysis highlights a fundamental limitation of current VLMs in fine-grained\nspatial grounding. We hope this resource can help future advances on visually\ngrounded numeracy and precise spatial perception of VLMs, bridging the gap\nbetween recognizing numbers and measuring the world.", "AI": {"tldr": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u5728\u6d4b\u91cf\u4eea\u5668\u8bfb\u6570\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002\u8bba\u6587\u4ecb\u7ecd\u4e86 MeasureBench\uff0c\u4e00\u4e2a\u7528\u4e8e\u89c6\u89c9\u6d4b\u91cf\u8bfb\u6570\u7684\u57fa\u51c6\uff0c\u6db5\u76d6\u771f\u5b9e\u548c\u5408\u6210\u56fe\u50cf\uff0c\u4ee5\u53ca\u7528\u4e8e\u6570\u636e\u5408\u6210\u7684\u53ef\u6269\u5c55\u6d41\u6c34\u7ebf\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6d4b\u91cf\u4eea\u5668\u8bfb\u6570\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86 MeasureBench \u57fa\u51c6\uff0c\u5305\u542b\u771f\u5b9e\u4e16\u754c\u548c\u5408\u6210\u7684\u6d4b\u91cf\u56fe\u50cf\uff0c\u4ee5\u53ca\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6570\u636e\u5408\u6210\u6d41\u6c34\u7ebf\uff0c\u8be5\u6d41\u6c34\u7ebf\u53ef\u4ee5\u7a0b\u5e8f\u5316\u751f\u6210\u7279\u5b9a\u7c7b\u578b\u7684\u4eea\u8868\uff0c\u5e76\u63a7\u5236\u89c6\u89c9\u5916\u89c2\u3002", "result": "\u5bf9\u73b0\u6709 VLM \u7684\u8bc4\u4f30\u8868\u660e\uff0c\u5373\u4f7f\u662f\u6700\u5f3a\u5927\u7684\u6a21\u578b\u5728\u6d4b\u91cf\u8bfb\u6570\u65b9\u9762\u4e5f\u5b58\u5728\u56f0\u96be\u3002\u4e00\u4e2a\u5e38\u89c1\u7684\u5931\u8d25\u6a21\u5f0f\u662f\u6307\u793a\u5668\u5b9a\u4f4d\u9519\u8bef\u3002\u5728\u5408\u6210\u6570\u636e\u4e0a\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u7684\u521d\u6b65\u5b9e\u9a8c\u5728\u57df\u5185\u5408\u6210\u5b50\u96c6\u4e0a\u53d6\u5f97\u4e86\u4ee4\u4eba\u9f13\u821e\u7684\u7ed3\u679c\uff0c\u4f46\u5728\u771f\u5b9e\u56fe\u50cf\u4e0a\u6548\u679c\u4e0d\u4f73\u3002", "conclusion": "\u76ee\u524d\u7684 VLM \u5728\u7ec6\u7c92\u5ea6\u7684\u7a7a\u95f4\u5b9a\u4f4d\u65b9\u9762\u5b58\u5728\u6839\u672c\u9650\u5236\u3002MeasureBench \u6709\u52a9\u4e8e\u672a\u6765\u5728\u89c6\u89c9\u57fa\u7840\u4e0a\u7684\u6570\u5b57\u80fd\u529b\u548c VLM \u7cbe\u786e\u7a7a\u95f4\u611f\u77e5\u65b9\u9762\u7684\u8fdb\u6b65\u3002"}}
{"id": "2510.26829", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.26829", "abs": "https://arxiv.org/abs/2510.26829", "authors": ["Svetlana Churina", "Niranjan Chebrolu", "Kokil Jaidka"], "title": "Layer of Truth: Probing Belief Shifts under Continual Pre-Training Poisoning", "comment": null, "summary": "Large language models (LLMs) continually evolve through pre-training on\never-expanding web data, but this adaptive process also exposes them to subtle\nforms of misinformation. While prior work has explored data poisoning during\nstatic pre-training, the effects of such manipulations under continual\npre-training remain largely unexplored. Drawing inspiration from the illusory\ntruth effect in human cognition - where repeated exposure to falsehoods\nincreases belief in their accuracy - we ask whether LLMs exhibit a similar\nvulnerability. We investigate whether repeated exposure to false but\nconfidently stated facts can shift a model's internal representation away from\nthe truth.\n  We introduce Layer of Truth, a framework and dataset for probing belief\ndynamics in continually trained LLMs. By injecting controlled amounts of\npoisoned data and probing intermediate representations across checkpoints,\nmodel scales, and question types, we quantify when and how factual beliefs\nshift. Our findings reveal that even minimal exposure can induce persistent\nrepresentational drift in well-established facts, with susceptibility varying\nacross layers and model sizes. These results highlight an overlooked\nvulnerability of continually updated LLMs: their capacity to internalize\nmisinformation analogously to humans, underscoring the need for robust\nmonitoring of factual integrity during model updates.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u5728\u4e0d\u65ad\u6269\u5c55\u7684\u7f51\u7edc\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u4e0d\u65ad\u53d1\u5c55\uff0c\u4f46\u4e5f\u5bb9\u6613\u53d7\u5230\u5fae\u5999\u7684\u9519\u8bef\u4fe1\u606f\u7684\u5f71\u54cd\u3002\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u662f\u5c11\u91cf\u7684\u9519\u8bef\u4fe1\u606f\u4e5f\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u4e2d\u5df2\u5efa\u7acb\u7684\u4e8b\u5b9e\u53d1\u751f\u6301\u4e45\u7684\u8868\u5f81\u6f02\u79fb\uff0c\u8fd9\u79cd\u5f71\u54cd\u56e0\u5c42\u548c\u6a21\u578b\u5927\u5c0f\u800c\u5f02\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6301\u7eed\u9884\u8bad\u7ec3\u4e2d\uff0c\u662f\u5426\u4f1a\u56e0\u4e3a\u91cd\u590d\u63a5\u89e6\u865a\u5047\u4fe1\u606f\u800c\u6539\u53d8\u5176\u5185\u90e8\u8868\u5f81\uff0c\u4ece\u800c\u504f\u79bb\u4e8b\u5b9e\uff0c\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u8ba4\u77e5\u4e2d\u7684\u865a\u5e7b\u771f\u7406\u6548\u5e94\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3a\u201c\u771f\u7406\u5c42\u201d\u7684\u6846\u67b6\u548c\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u63a2\u6d4b\u6301\u7eed\u8bad\u7ec3\u7684LLM\u4e2d\u7684\u4fe1\u5ff5\u52a8\u6001\u3002\u901a\u8fc7\u6ce8\u5165\u53ef\u63a7\u6570\u91cf\u7684\u9519\u8bef\u6570\u636e\uff0c\u5e76\u63a2\u6d4b\u8de8\u68c0\u67e5\u70b9\u3001\u6a21\u578b\u89c4\u6a21\u548c\u95ee\u9898\u7c7b\u578b\u7684\u4e2d\u95f4\u8868\u793a\uff0c\u91cf\u5316\u4e8b\u5b9e\u4fe1\u5ff5\u4f55\u65f6\u4ee5\u53ca\u5982\u4f55\u8f6c\u53d8\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u662f\u6700\u5c0f\u7a0b\u5ea6\u7684\u63a5\u89e6\u4e5f\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u4e2d\u5df2\u5efa\u7acb\u7684\u4e8b\u5b9e\u53d1\u751f\u6301\u4e45\u7684\u8868\u5f81\u6f02\u79fb\uff0c\u5e76\u4e14\u8fd9\u79cd\u654f\u611f\u6027\u56e0\u5c42\u548c\u6a21\u578b\u5927\u5c0f\u800c\u5f02\u3002", "conclusion": "\u6301\u7eed\u66f4\u65b0\u7684LLM\u5b58\u5728\u4e00\u4e2a\u88ab\u5ffd\u89c6\u7684\u6f0f\u6d1e\uff1a\u5b83\u4eec\u5177\u6709\u50cf\u4eba\u7c7b\u4e00\u6837\u5185\u5316\u9519\u8bef\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u5bf9\u6a21\u578b\u66f4\u65b0\u8fc7\u7a0b\u4e2d\u7684\u4e8b\u5b9e\u5b8c\u6574\u6027\u8fdb\u884c\u7a33\u5065\u7684\u76d1\u63a7\u3002"}}
{"id": "2510.26861", "categories": ["cs.IR", "cs.CL", "H.3.3; I.2.7; I.2.10"], "pdf": "https://arxiv.org/pdf/2510.26861", "abs": "https://arxiv.org/abs/2510.26861", "authors": ["Teerapol Saengsukhiran", "Peerawat Chomphooyod", "Narabodee Rodjananant", "Chompakorn Chaksangchaichot", "Patawee Prakrankamanant", "Witthawin Sripheanpol", "Pak Lovichit", "SarChaksaana Nutanong", "Ekapol Chuangsuwanich"], "title": "Evaluating Perspectival Biases in Cross-Modal Retrieval", "comment": null, "summary": "Multimodal retrieval systems are expected to operate in a semantic space,\nagnostic to the language or cultural origin of the query. In practice, however,\nretrieval outcomes systematically reflect perspectival biases: deviations\nshaped by linguistic prevalence and cultural associations. We study two such\nbiases. First, prevalence bias refers to the tendency to favor entries from\nprevalent languages over semantically faithful entries in image-to-text\nretrieval. Second, association bias refers to the tendency to favor images\nculturally associated with the query over semantically correct ones in\ntext-to-image retrieval. Results show that explicit alignment is a more\neffective strategy for mitigating prevalence bias. However, association bias\nremains a distinct and more challenging problem. These findings suggest that\nachieving truly equitable multimodal systems requires targeted strategies\nbeyond simple data scaling and that bias arising from cultural association may\nbe treated as a more challenging problem than one arising from linguistic\nprevalence.", "AI": {"tldr": "\u591a\u6a21\u6001\u68c0\u7d22\u7cfb\u7edf\u5b58\u5728\u89c6\u89d2\u504f\u5dee\uff0c\u53d7\u5230\u8bed\u8a00\u548c\u6587\u5316\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u591a\u6a21\u6001\u68c0\u7d22\u7cfb\u7edf\u4e2d\u5b58\u5728\u7684\u4e24\u79cd\u504f\u5dee\uff1a\u8bed\u8a00\u6d41\u884c\u5ea6\u548c\u6587\u5316\u8054\u60f3\u504f\u5dee\u3002", "method": "\u5206\u6790\u56fe\u50cf\u5230\u6587\u672c\u68c0\u7d22\u4e2d\u7684\u6d41\u884c\u5ea6\u504f\u5dee\u548c\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\u4e2d\u7684\u8054\u60f3\u504f\u5dee\u3002", "result": "\u663e\u5f0f\u5bf9\u9f50\u80fd\u6709\u6548\u7f13\u89e3\u6d41\u884c\u5ea6\u504f\u5dee\uff0c\u4f46\u8054\u60f3\u504f\u5dee\u66f4\u96be\u89e3\u51b3\u3002", "conclusion": "\u5b9e\u73b0\u516c\u5e73\u7684\u591a\u6a21\u6001\u7cfb\u7edf\u9700\u8981\u6709\u9488\u5bf9\u6027\u7684\u7b56\u7565\uff0c\u6587\u5316\u8054\u60f3\u504f\u5dee\u6bd4\u8bed\u8a00\u6d41\u884c\u5ea6\u504f\u5dee\u66f4\u5177\u6311\u6218\u6027\u3002"}}
{"id": "2510.26835", "categories": ["cs.DB", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26835", "abs": "https://arxiv.org/abs/2510.26835", "authors": ["Chen Wang", "Xunzhuo Liu", "Yue Zhu", "Alaa Youssef", "Priya Nagpurkar", "Huamin Chen"], "title": "Category-Aware Semantic Caching for Heterogeneous LLM Workloads", "comment": "13 pages including reference, position paper", "summary": "LLM serving systems process heterogeneous query workloads where different\ncategories exhibit different characteristics. Code queries cluster densely in\nembedding space while conversational queries distribute sparsely. Content\nstaleness varies from minutes (stock data) to months (code patterns). Query\nrepetition patterns range from power-law (code) to uniform (conversation),\nproducing long tail cache hit rate distributions: high-repetition categories\nachieve 40-60% hit rates while low-repetition or volatile categories achieve\n5-15% hit rates. Vector databases must exclude the long tail because remote\nsearch costs (30ms) require 15--20% hit rates to break even, leaving 20-30% of\nproduction traffic uncached. Uniform cache policies compound this problem:\nfixed thresholds cause false positives in dense spaces and miss valid\nparaphrases in sparse spaces; fixed TTLs waste memory or serve stale data. This\npaper presents category-aware semantic caching where similarity thresholds,\nTTLs, and quotas vary by query category. We present a hybrid architecture\nseparating in-memory HNSW search from external document storage, reducing miss\ncost from 30ms to 2ms. This reduction makes low-hit-rate categories\neconomically viable (break-even at 3-5% versus 15-20%), enabling cache coverage\nacross the entire workload distribution. Adaptive load-based policies extend\nthis framework to respond to downstream model load, dynamically adjusting\nthresholds and TTLs to reduce traffic to overloaded models by 9-17% in\ntheoretical projections.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cdcategory-aware\u7684\u8bed\u4e49\u7f13\u5b58\u65b9\u6cd5\uff0c\u901a\u8fc7\u533a\u5206\u4e0d\u540c\u67e5\u8be2\u7c7b\u522b\u6765\u4f18\u5316\u7f13\u5b58\u6027\u80fd\uff0c\u4ece\u800c\u63d0\u5347LLM serving\u7cfb\u7edf\u7684\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u5411\u91cf\u6570\u636e\u5e93\u7f13\u5b58\u7b56\u7565\u65e0\u6cd5\u6709\u6548\u5904\u7406\u4e0d\u540c\u7c7b\u522b\u67e5\u8be2\u7684\u5dee\u5f02\u6027\uff0c\u5bfc\u81f4\u7f13\u5b58\u547d\u4e2d\u7387\u4f4e\uff0c\u5927\u91cf\u6d41\u91cf\u65e0\u6cd5\u88ab\u7f13\u5b58\u3002", "method": "\u8be5\u65b9\u6cd5\u6839\u636e\u67e5\u8be2\u7c7b\u522b\u8c03\u6574\u76f8\u4f3c\u5ea6\u9608\u503c\u3001TTL\u548c\u914d\u989d\uff0c\u5e76\u91c7\u7528\u6df7\u5408\u67b6\u6784\u5206\u79bb\u5185\u5b58HNSW\u641c\u7d22\u548c\u5916\u90e8\u6587\u6863\u5b58\u50a8\uff0c\u964d\u4f4e\u672a\u547d\u4e2d\u6210\u672c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u7f13\u5b58\u8986\u76d6\u7387\uff0c\u5e76\u80fd\u6839\u636e\u4e0b\u6e38\u6a21\u578b\u8d1f\u8f7d\u52a8\u6001\u8c03\u6574\u9608\u503c\u548cTTL\uff0c\u4ece\u800c\u51cf\u5c11\u6a21\u578b\u8fc7\u8f7d\u60c5\u51b5\u3002", "conclusion": "\u63d0\u51fa\u7684category-aware\u8bed\u4e49\u7f13\u5b58\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347LLM serving\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u5e76\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.26989", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.26989", "abs": "https://arxiv.org/abs/2510.26989", "authors": ["Agorakis Bompotas", "Konstantinos Koutras", "Nikitas Rigas Kalogeropoulos", "Panagiotis Kechagias", "Dimitra Gariza", "Athanasios P. Kalogeras", "Christos Alexakos"], "title": "SUSTAINABLE Platform: Seamless Smart Farming Integration Towards Agronomy Automation", "comment": "Accepted for presentation to 11th IEEE International Smart Cities\n  Conference (ISC2 2025)", "summary": "The global agricultural sector is undergoing a transformative shift, driven\nby increasing food demands, climate variability and the need for sustainable\npractices. SUSTAINABLE is a smart farming platform designed to integrate IoT,\nAI, satellite imaging, and role-based task orchestration to enable efficient,\ntraceable, and sustainable agriculture with a pilot usecase in viticulture.\nThis paper explores current smart agriculture solutions, presents a comparative\nevaluation, and introduces SUSTAINABLE's key features, including satellite\nindex integration, real-time environmental data, and role-aware task management\ntailored to Mediterranean vineyards.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86SUSTAINABLE\uff0c\u4e00\u4e2a\u65e8\u5728\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u8ffd\u6eaf\u548c\u53ef\u6301\u7eed\u519c\u4e1a\u7684\u667a\u80fd\u519c\u4e1a\u5e73\u53f0\u3002", "motivation": "\u5168\u7403\u519c\u4e1a\u90e8\u95e8\u6b63\u5728\u7ecf\u5386\u8f6c\u578b\uff0c\u53d7\u5230\u4e0d\u65ad\u589e\u957f\u7684\u7cae\u98df\u9700\u6c42\u3001\u6c14\u5019\u53d8\u5316\u548c\u5bf9\u53ef\u6301\u7eed\u53d1\u5c55\u7684\u9700\u6c42\u9a71\u52a8\u3002", "method": "\u63a2\u7d22\u4e86\u5f53\u524d\u7684\u667a\u80fd\u519c\u4e1a\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u6bd4\u8f83\u8bc4\u4f30\uff0c\u5e76\u4ecb\u7ecd\u4e86SUSTAINABLE\u7684\u5173\u952e\u7279\u6027\uff0c\u5305\u62ec\u536b\u661f\u6307\u6570\u96c6\u6210\u3001\u5b9e\u65f6\u73af\u5883\u6570\u636e\u548c\u4e3a\u5730\u4e2d\u6d77\u8461\u8404\u56ed\u91cf\u8eab\u5b9a\u5236\u7684\u57fa\u4e8e\u89d2\u8272\u7684\u4efb\u52a1\u7ba1\u7406\u3002", "result": "\u5c55\u793aSUSTAINABLE\u7684\u5173\u952e\u7279\u6027\uff0c\u5305\u62ec\u536b\u661f\u6307\u6570\u96c6\u6210\u3001\u5b9e\u65f6\u73af\u5883\u6570\u636e\u548c\u57fa\u4e8e\u89d2\u8272\u7684\u4efb\u52a1\u7ba1\u7406\u3002", "conclusion": "SUSTAINABLE\u662f\u4e00\u4e2a\u667a\u80fd\u519c\u4e1a\u5e73\u53f0\uff0c\u65e8\u5728\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u8ffd\u6eaf\u548c\u53ef\u6301\u7eed\u7684\u519c\u4e1a\uff0c\u5e76\u5728\u8461\u8404\u683d\u57f9\u4e2d\u8fdb\u884c\u4e86\u8bd5\u70b9\u7528\u4f8b\u3002"}}
{"id": "2510.26974", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26974", "abs": "https://arxiv.org/abs/2510.26974", "authors": ["Jean-Philippe Corbeil", "Asma Ben Abacha", "Jerome Tremblay", "Phillip Swazinna", "Akila Jeeson Daniel", "Miguel Del-Agua", "Francois Beaulieu"], "title": "Overview of the MEDIQA-OE 2025 Shared Task on Medical Order Extraction from Doctor-Patient Consultations", "comment": null, "summary": "Clinical documentation increasingly uses automatic speech recognition and\nsummarization, yet converting conversations into actionable medical orders for\nElectronic Health Records remains unexplored. A solution to this problem can\nsignificantly reduce the documentation burden of clinicians and directly impact\ndownstream patient care. We introduce the MEDIQA-OE 2025 shared task, the first\nchallenge on extracting medical orders from doctor-patient conversations. Six\nteams participated in the shared task and experimented with a broad range of\napproaches, and both closed- and open-weight large language models (LLMs). In\nthis paper, we describe the MEDIQA-OE task, dataset, final leaderboard ranking,\nand participants' solutions.", "AI": {"tldr": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u65b0\u7684\u5171\u4eab\u4efb\u52a1\uff0c\u65e8\u5728\u4ece\u533b\u60a3\u5bf9\u8bdd\u4e2d\u63d0\u53d6\u533b\u5631\uff0c\u4ee5\u51cf\u8f7b\u4e34\u5e8a\u533b\u751f\u7684\u6587\u6863\u8d1f\u62c5\u5e76\u6539\u5584\u60a3\u8005\u62a4\u7406\u3002", "motivation": "\u4e34\u5e8a\u6587\u6863\u8d8a\u6765\u8d8a\u591a\u5730\u4f7f\u7528\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u548c\u603b\u7ed3\uff0c\u4f46\u5c06\u5bf9\u8bdd\u8f6c\u6362\u4e3a\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u7684\u53ef\u64cd\u4f5c\u533b\u7597\u533b\u5631\u4ecd\u672a\u88ab\u63a2\u7d22\u3002", "method": "\u4ecb\u7ecd\u4e86MEDIQA-OE 2025 \u5171\u4eab\u4efb\u52a1\uff0c\u8fd9\u662f\u4e00\u4e2a\u5173\u4e8e\u4ece\u533b\u60a3\u5bf9\u8bdd\u4e2d\u63d0\u53d6\u533b\u7597\u533b\u5631\u7684\u9996\u6b21\u6311\u6218\u3002\u516d\u4e2a\u56e2\u961f\u53c2\u4e0e\u4e86\u5171\u4eab\u4efb\u52a1\uff0c\u5e76\u5c1d\u8bd5\u4e86\u5404\u79cd\u65b9\u6cd5\uff0c\u5305\u62ec\u5c01\u95ed\u6743\u91cd\u548c\u5f00\u653e\u6743\u91cd\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM)\u3002", "result": "\u4ecb\u7ecd\u4e86MEDIQA-OE\u4efb\u52a1\u3001\u6570\u636e\u96c6\u3001\u6700\u7ec8\u6392\u884c\u699c\u6392\u540d\u548c\u53c2\u4e0e\u8005\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u672c\u6587\u6982\u8ff0\u4e86MEDIQA-OE 2025 \u5171\u4eab\u4efb\u52a1\u7684\u5404\u4e2a\u65b9\u9762\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u533b\u5631\u63d0\u53d6\u548c\u4e34\u5e8a\u6587\u6863\u81ea\u52a8\u5316\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.26903", "categories": ["cs.CV", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2510.26903", "abs": "https://arxiv.org/abs/2510.26903", "authors": ["Rochak Dhakal", "Chen Zhao", "Zixin Shi", "Joyce H. Keyak", "Tadashi S. Kaneko", "Kuan-Jui Su", "Hui Shen", "Hong-Wen Deng", "Weihua Zhou"], "title": "PF-DAformer: Proximal Femur Segmentation via Domain Adaptive Transformer for Dual-Center QCT", "comment": "22 Pages, 5 Tables, 10 Figures. The combination of GRL and MMD\n  achieved the most balanced performance, reducing contour deviations and\n  enhancing surface smoothness", "summary": "Quantitative computed tomography (QCT) plays a crucial role in assessing bone\nstrength and fracture risk by enabling volumetric analysis of bone density\ndistribution in the proximal femur. However, deploying automated segmentation\nmodels in practice remains difficult because deep networks trained on one\ndataset often fail when applied to another. This failure stems from domain\nshift, where scanners, reconstruction settings, and patient demographics vary\nacross institutions, leading to unstable predictions and unreliable\nquantitative metrics. Overcoming this barrier is essential for multi-center\nosteoporosis research and for ensuring that radiomics and structural finite\nelement analysis results remain reproducible across sites. In this work, we\ndeveloped a domain-adaptive transformer segmentation framework tailored for\nmulti-institutional QCT. Our model is trained and validated on one of the\nlargest hip fracture related research cohorts to date, comprising 1,024 QCT\nimages scans from Tulane University and 384 scans from Rochester, Minnesota for\nproximal femur segmentation. To address domain shift, we integrate two\ncomplementary strategies within a 3D TransUNet backbone: adversarial alignment\nvia Gradient Reversal Layer (GRL), which discourages the network from encoding\nsite-specific cues, and statistical alignment via Maximum Mean Discrepancy\n(MMD), which explicitly reduces distributional mismatches between institutions.\nThis dual mechanism balances invariance and fine-grained alignment, enabling\nscanner-agnostic feature learning while preserving anatomical detail.", "AI": {"tldr": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u591a\u4e2d\u5fc3\u5b9a\u91cf\u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf\uff08QCT\uff09\u4e2d\u7531\u4e8e\u9886\u57df\u8f6c\u79fb\u5bfc\u81f4\u9aa8\u9abc\u5206\u5272\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5f00\u53d1\u4e00\u79cd\u9886\u57df\u81ea\u9002\u5e94\u7684Transformer\u5206\u5272\u6846\u67b6\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u4e0d\u540c\u673a\u6784QCT\u56fe\u50cf\u4e0a\u7684\u5206\u5272\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u5728\u591a\u4e2d\u5fc3QCT\u7814\u7a76\u4e2d\uff0c\u7531\u4e8e\u626b\u63cf\u4eea\u3001\u91cd\u5efa\u8bbe\u7f6e\u548c\u60a3\u8005\u4eba\u53e3\u7edf\u8ba1\u5b66\u7b49\u56e0\u7d20\u7684\u5dee\u5f02\uff0c\u5bfc\u81f4\u9886\u57df\u8f6c\u79fb\uff0c\u4f7f\u5f97\u5728\u4e00\u4e2a\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6df1\u5ea6\u7f51\u7edc\u5728\u5e94\u7528\u4e8e\u5176\u4ed6\u6570\u636e\u96c6\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u8fd9\u963b\u788d\u4e86\u9aa8\u8d28\u758f\u677e\u75c7\u7684\u591a\u4e2d\u5fc3\u7814\u7a76\u4ee5\u53ca\u5f71\u50cf\u7ec4\u5b66\u548c\u7ed3\u6784\u6709\u9650\u5143\u5206\u6790\u7ed3\u679c\u7684\u8de8\u7ad9\u70b9\u91cd\u73b0\u6027\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9886\u57df\u81ea\u9002\u5e94\u7684Transformer\u5206\u5272\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u57283D TransUNet\u9aa8\u5e72\u7f51\u7edc\u4e2d\u96c6\u6210\u4e86\u4e24\u79cd\u4e92\u8865\u7b56\u7565\uff1a\u901a\u8fc7\u68af\u5ea6\u53cd\u8f6c\u5c42\uff08GRL\uff09\u8fdb\u884c\u5bf9\u6297\u5bf9\u9f50\uff0c\u4ee5\u6291\u5236\u7f51\u7edc\u7f16\u7801\u7279\u5b9a\u4e8e\u7ad9\u70b9\u7684\u7ebf\u7d22\uff1b\u4ee5\u53ca\u901a\u8fc7\u6700\u5927\u5747\u503c\u5dee\u5f02\uff08MMD\uff09\u8fdb\u884c\u7edf\u8ba1\u5bf9\u9f50\uff0c\u4ee5\u663e\u5f0f\u51cf\u5c11\u673a\u6784\u4e4b\u95f4\u7684\u5206\u5e03\u4e0d\u5339\u914d\u3002\u8fd9\u79cd\u53cc\u91cd\u673a\u5236\u5e73\u8861\u4e86\u4e0d\u53d8\u6027\u548c\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u3002", "result": "\u8be5\u6a21\u578b\u5728\u5305\u542b\u6765\u81ea\u675c\u5170\u5927\u5b66\u76841,024\u5f20QCT\u56fe\u50cf\u626b\u63cf\u548c\u6765\u81ea\u660e\u5c3c\u82cf\u8fbe\u5dde\u7f57\u5207\u65af\u7279\u7684384\u5f20\u626b\u63cf\u7684\u5927\u578b\u9acb\u90e8\u9aa8\u6298\u76f8\u5173\u7814\u7a76\u961f\u5217\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\u548c\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u9886\u57df\u81ea\u9002\u5e94\u7684Transformer\u5206\u5272\u6846\u67b6\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u591a\u4e2d\u5fc3QCT\u4e2d\u7531\u4e8e\u9886\u57df\u8f6c\u79fb\u5bfc\u81f4\u7684\u9aa8\u9abc\u5206\u5272\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u4e0d\u540c\u673a\u6784QCT\u56fe\u50cf\u4e0a\u7684\u5206\u5272\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2510.26830", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.26830", "abs": "https://arxiv.org/abs/2510.26830", "authors": ["Guangzhi Su", "Shuchang Huang", "Yutong Ke", "Zhuohang Liu", "Long Qian", "Kaizhu Huang"], "title": "SmoothGuard: Defending Multimodal Large Language Models with Noise Perturbation and Clustering Aggregation", "comment": null, "summary": "Multimodal large language models (MLLMs) have achieved impressive performance\nacross diverse tasks by jointly reasoning over textual and visual inputs.\nDespite their success, these models remain highly vulnerable to adversarial\nmanipulations, raising concerns about their safety and reliability in\ndeployment. In this work, we first generalize an approach for generating\nadversarial images within the HuggingFace ecosystem and then introduce\nSmoothGuard, a lightweight and model-agnostic defense framework that enhances\nthe robustness of MLLMs through randomized noise injection and clustering-based\nprediction aggregation. Our method perturbs continuous modalities (e.g., images\nand audio) with Gaussian noise, generates multiple candidate outputs, and\napplies embedding-based clustering to filter out adversarially influenced\npredictions. The final answer is selected from the majority cluster, ensuring\nstable responses even under malicious perturbations. Extensive experiments on\nPOPE, LLaVA-Bench (In-the-Wild), and MM-SafetyBench demonstrate that\nSmoothGuard improves resilience to adversarial attacks while maintaining\ncompetitive utility. Ablation studies further identify an optimal noise range\n(0.1-0.2) that balances robustness and utility.", "AI": {"tldr": "MLLMs\u6613\u53d7\u5bf9\u6297\u653b\u51fb\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86SmoothGuard\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7\u566a\u58f0\u6ce8\u5165\u548c\u805a\u7c7b\u9884\u6d4b\u805a\u5408\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "motivation": "MLLMs\u5728\u5904\u7406\u6587\u672c\u548c\u89c6\u89c9\u8f93\u5165\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u653b\u51fb\uff0c\u5bf9\u5176\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u6784\u6210\u5a01\u80c1\u3002", "method": "\u63d0\u51fa\u4e86SmoothGuard\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u6a21\u578b\u65e0\u5173\u7684\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7\u968f\u673a\u566a\u58f0\u6ce8\u5165\u548c\u57fa\u4e8e\u805a\u7c7b\u7684\u9884\u6d4b\u805a\u5408\u6765\u589e\u5f3aMLLMs\u7684\u9c81\u68d2\u6027\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u9ad8\u65af\u566a\u58f0\u6270\u52a8\u8fde\u7eed\u6a21\u6001\uff08\u4f8b\u5982\uff0c\u56fe\u50cf\u548c\u97f3\u9891\uff09\uff0c\u751f\u6210\u591a\u4e2a\u5019\u9009\u8f93\u51fa\uff0c\u5e76\u5e94\u7528\u57fa\u4e8e\u5d4c\u5165\u7684\u805a\u7c7b\u6765\u8fc7\u6ee4\u6389\u53d7\u5bf9\u6297\u5f71\u54cd\u7684\u9884\u6d4b\u3002\u4ece\u591a\u6570\u96c6\u7fa4\u4e2d\u9009\u62e9\u6700\u7ec8\u7b54\u6848\uff0c\u786e\u4fdd\u5728\u6076\u610f\u6270\u52a8\u4e0b\u4e5f\u80fd\u83b7\u5f97\u7a33\u5b9a\u7684\u54cd\u5e94\u3002", "result": "\u5728POPE\u3001LLaVA-Bench (In-the-Wild)\u548cMM-SafetyBench\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSmoothGuard\u63d0\u9ad8\u4e86\u5bf9\u6297\u653b\u51fb\u7684\u62b5\u6297\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7ade\u4e89\u6027\u7684\u6548\u7528\u3002\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u786e\u5b9a\u4e86\u5e73\u8861\u9c81\u68d2\u6027\u548c\u6548\u7528\u7684\u6700\u4f73\u566a\u58f0\u8303\u56f4 (0.1-0.2)\u3002", "conclusion": "SmoothGuard \u901a\u8fc7\u5728 MLLM \u4e2d\u6ce8\u5165\u566a\u58f0\u5e76\u5e94\u7528\u805a\u7c7b\u6765\u63d0\u9ad8\u5bf9\u6297\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u6027\u7684\u6027\u80fd\u3002"}}
{"id": "2510.27157", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.27157", "abs": "https://arxiv.org/abs/2510.27157", "authors": ["Min Hou", "Le Wu", "Yuxin Liao", "Yonghui Yang", "Zhen Zhang", "Changlong Zheng", "Han Wu", "Richang Hong"], "title": "A Survey on Generative Recommendation: Data, Model, and Tasks", "comment": null, "summary": "Recommender systems serve as foundational infrastructure in modern\ninformation ecosystems, helping users navigate digital content and discover\nitems aligned with their preferences. At their core, recommender systems\naddress a fundamental problem: matching users with items. Over the past\ndecades, the field has experienced successive paradigm shifts, from\ncollaborative filtering and matrix factorization in the machine learning era to\nneural architectures in the deep learning era. Recently, the emergence of\ngenerative models, especially large language models (LLMs) and diffusion\nmodels, have sparked a new paradigm: generative recommendation, which\nreconceptualizes recommendation as a generation task rather than discriminative\nscoring. This survey provides a comprehensive examination through a unified\ntripartite framework spanning data, model, and task dimensions. Rather than\nsimply categorizing works, we systematically decompose approaches into\noperational stages-data augmentation and unification, model alignment and\ntraining, task formulation and execution. At the data level, generative models\nenable knowledge-infused augmentation and agent-based simulation while unifying\nheterogeneous signals. At the model level, we taxonomize LLM-based methods,\nlarge recommendation models, and diffusion approaches, analyzing their\nalignment mechanisms and innovations. At the task level, we illuminate new\ncapabilities including conversational interaction, explainable reasoning, and\npersonalized content generation. We identify five key advantages: world\nknowledge integration, natural language understanding, reasoning capabilities,\nscaling laws, and creative generation. We critically examine challenges in\nbenchmark design, model robustness, and deployment efficiency, while charting a\nroadmap toward intelligent recommendation assistants that fundamentally reshape\nhuman-information interaction.", "AI": {"tldr": "\u751f\u6210\u5f0f\u63a8\u8350\u7cfb\u7edf\u901a\u8fc7\u5c06\u63a8\u8350\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u751f\u6210\u4efb\u52a1\uff0c\u4ece\u800c\u6539\u53d8\u4e86\u4f20\u7edf\u7684\u5224\u522b\u8bc4\u5206\u65b9\u6cd5\u3002\u672c\u7efc\u8ff0\u5168\u9762\u8003\u5bdf\u4e86\u6570\u636e\u3001\u6a21\u578b\u548c\u4efb\u52a1\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u63a2\u8ba8\u4e86\u751f\u6210\u6a21\u578b\u5728\u77e5\u8bc6\u589e\u5f3a\u3001\u7edf\u4e00\u5f02\u6784\u4fe1\u53f7\u3001\u5bf9\u8bdd\u4ea4\u4e92\u3001\u53ef\u89e3\u91ca\u63a8\u7406\u548c\u4e2a\u6027\u5316\u5185\u5bb9\u751f\u6210\u7b49\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u5e76 \u043a\u0440\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u5206\u6790\u4e86\u57fa\u51c6\u8bbe\u8ba1\u3001\u6a21\u578b\u9c81\u68d2\u6027\u548c\u90e8\u7f72\u6548\u7387\u7b49\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u4ee3\u63a8\u8350\u7cfb\u7edf\u9762\u4e34\u7740\u5982\u4f55\u66f4\u597d\u5730\u5339\u914d\u7528\u6237\u548c\u7269\u54c1\u7684\u6839\u672c\u95ee\u9898\u3002\u968f\u7740\u751f\u6210\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u63a8\u8350\u7cfb\u7edf\u8fce\u6765\u4e86\u751f\u6210\u5f0f\u63a8\u8350\u7684\u65b0\u8303\u5f0f\u3002", "method": "\u8be5\u7814\u7a76\u901a\u8fc7\u7edf\u4e00\u7684\u4e09\u65b9\u6846\u67b6\uff0c\u4ece\u6570\u636e\u589e\u5f3a\u4e0e\u7edf\u4e00\u3001\u6a21\u578b\u5bf9\u9f50\u4e0e\u8bad\u7ec3\u3001\u4efb\u52a1\u516c\u5f0f\u5316\u4e0e\u6267\u884c\u7b49\u64cd\u4f5c\u9636\u6bb5\u5bf9\u65b9\u6cd5\u8fdb\u884c\u5206\u89e3\uff0c\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u751f\u6210\u5f0f\u63a8\u8350\u3002", "result": "\u603b\u7ed3\u4e86\u751f\u6210\u5f0f\u63a8\u8350\u7684\u4e94\u4e2a\u5173\u952e\u4f18\u52bf\uff1a\u4e16\u754c\u77e5\u8bc6\u6574\u5408\u3001\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u3001\u63a8\u7406\u80fd\u529b\u3001\u6269\u5c55\u5b9a\u5f8b\u548c\u521b\u9020\u6027\u751f\u6210\u3002", "conclusion": "\u751f\u6210\u5f0f\u63a8\u8350\u4e3a\u91cd\u5851\u4eba\u673a\u4fe1\u606f\u4ea4\u4e92\u7684\u667a\u80fd\u63a8\u8350\u52a9\u624b\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u4f46\u4e5f\u9762\u4e34\u7740\u57fa\u51c6\u8bbe\u8ba1\u3001\u6a21\u578b\u9c81\u68d2\u6027\u548c\u90e8\u7f72\u6548\u7387\u7b49\u65b9\u9762\u7684\u6311\u6218\u3002"}}
{"id": "2510.26840", "categories": ["cs.DB", "cs.AI", "cs.FL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.26840", "abs": "https://arxiv.org/abs/2510.26840", "authors": ["Rocky Klopfenstein", "Yang He", "Andrew Tremante", "Yuepeng Wang", "Nina Narodytska", "Haoze Wu"], "title": "SpotIt: Evaluating Text-to-SQL Evaluation with Formal Verification", "comment": null, "summary": "Community-driven Text-to-SQL evaluation platforms play a pivotal role in\ntracking the state of the art of Text-to-SQL performance. The reliability of\nthe evaluation process is critical for driving progress in the field. Current\nevaluation methods are largely test-based, which involves comparing the\nexecution results of a generated SQL query and a human-labeled ground-truth on\na static test database. Such an evaluation is optimistic, as two queries can\ncoincidentally produce the same output on the test database while actually\nbeing different. In this work, we propose a new alternative evaluation\npipeline, called SpotIt, where a formal bounded equivalence verification engine\nactively searches for a database that differentiates the generated and\nground-truth SQL queries. We develop techniques to extend existing verifiers to\nsupport a richer SQL subset relevant to Text-to-SQL. A performance evaluation\nof ten Text-to-SQL methods on the high-profile BIRD dataset suggests that\ntest-based methods can often overlook differences between the generated query\nand the ground-truth. Further analysis of the verification results reveals a\nmore complex picture of the current Text-to-SQL evaluation.", "AI": {"tldr": "\u5f53\u524dText-to-SQL\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6d4b\u8bd5\u6570\u636e\u5e93\uff0c\u53ef\u80fd\u65e0\u6cd5\u51c6\u786e\u8bc4\u4f30\u67e5\u8be2\u7684\u5dee\u5f02\u3002", "motivation": "\u73b0\u6709Text-to-SQL\u8bc4\u4f30\u65b9\u6cd5\u4e0d\u591f\u53ef\u9760\uff0c\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6d41\u7a0bSpotIt\uff0c\u4f7f\u7528\u5f62\u5f0f\u5316\u9a8c\u8bc1\u5f15\u64ce\u5bfb\u627e\u533a\u5206\u751f\u6210SQL\u67e5\u8be2\u548c\u6807\u51c6\u7b54\u6848SQL\u67e5\u8be2\u7684\u6570\u636e\u5e93\u3002", "result": "\u5728BIRD\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u6d4b\u8bd5\u7684\u8bc4\u4f30\u65b9\u6cd5\u7ecf\u5e38\u5ffd\u7565\u751f\u6210\u67e5\u8be2\u548c\u6807\u51c6\u7b54\u6848\u4e4b\u95f4\u7684\u5dee\u5f02\u3002", "conclusion": "\u9a8c\u8bc1\u7ed3\u679c\u63ed\u793a\u4e86\u5f53\u524dText-to-SQL\u8bc4\u4f30\u7684\u590d\u6742\u6027\u3002"}}
{"id": "2510.27009", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.27009", "abs": "https://arxiv.org/abs/2510.27009", "authors": ["Jared Junkin", "Samuel Nathanson"], "title": "Causal Masking on Spatial Data: An Information-Theoretic Case for Learning Spatial Datasets with Unimodal Language Models", "comment": "8 pages, NeurIPS 2025", "summary": "Language models are traditionally designed around causal masking. In domains\nwith spatial or relational structure, causal masking is often viewed as\ninappropriate, and sequential linearizations are instead used. Yet the question\nof whether it is viable to accept the information loss introduced by causal\nmasking on nonsequential data has received little direct study, in part because\nfew domains offer both spatial and sequential representations of the same\ndataset. In this work, we investigate this issue in the domain of chess, which\nnaturally supports both representations. We train language models with\nbidirectional and causal self-attention mechanisms on both spatial\n(board-based) and sequential (move-based) data. Our results show that models\ntrained on spatial board states - \\textit{even with causal masking} -\nconsistently achieve stronger playing strength than models trained on\nsequential data. While our experiments are conducted on chess, our results are\nmethodological and may have broader implications: applying causal masking to\nspatial data is a viable procedure for training unimodal LLMs on spatial data,\nand in some domains is even preferable to sequentialization.", "AI": {"tldr": "\u63a2\u8ba8\u4e86\u5728\u5177\u6709\u7a7a\u95f4\u6216\u5173\u7cfb\u7ed3\u6784\u7684\u9886\u57df\u4e2d\uff0c\u56e0\u679c\u63a9\u853d\uff08causal masking\uff09\u662f\u5426\u9002\u7528\u4e8e\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5728\u56fd\u9645\u8c61\u68cb\u9886\u57df\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e86\u5728\u7a7a\u95f4\uff08\u68cb\u76d8\uff09\u548c\u5e8f\u5217\uff08\u8d70\u6cd5\uff09\u6570\u636e\u4e0a\u4f7f\u7528\u53cc\u5411\u548c\u56e0\u679c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u8bad\u7ec3\u7684\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5728\u5177\u6709\u7a7a\u95f4\u6216\u5173\u7cfb\u7ed3\u6784\u7684\u9886\u57df\u4e2d\uff0c\u4f20\u7edf\u4e0a\u8ba4\u4e3a\u56e0\u679c\u63a9\u853d\u4e0d\u9002\u7528\uff0c\u901a\u5e38\u4f7f\u7528\u5e8f\u5217\u7ebf\u6027\u5316\u65b9\u6cd5\u3002\u4f46\u63a5\u53d7\u56e0\u679c\u63a9\u853d\u5e26\u6765\u7684\u4fe1\u606f\u635f\u5931\u662f\u5426\u53ef\u884c\uff0c\u7f3a\u4e4f\u76f4\u63a5\u7814\u7a76\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5728\u56fd\u9645\u8c61\u68cb\u9886\u57df\uff0c\u4f7f\u7528\u7a7a\u95f4\uff08\u68cb\u76d8\uff09\u548c\u5e8f\u5217\uff08\u8d70\u6cd5\uff09\u6570\u636e\uff0c\u8bad\u7ec3\u5177\u6709\u53cc\u5411\u548c\u56e0\u679c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5728\u7a7a\u95f4\u68cb\u76d8\u72b6\u6001\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\uff08\u5373\u4f7f\u4f7f\u7528\u56e0\u679c\u63a9\u853d\uff09\u59cb\u7ec8\u6bd4\u5728\u5e8f\u5217\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u5177\u6709\u66f4\u5f3a\u7684\u5bf9\u5f08\u80fd\u529b\u3002", "conclusion": "\u5728\u7a7a\u95f4\u6570\u636e\u4e0a\u5e94\u7528\u56e0\u679c\u63a9\u853d\u662f\u8bad\u7ec3\u5355\u6a21\u6001LLM\u7684\u53ef\u884c\u65b9\u6cd5\uff0c\u5728\u67d0\u4e9b\u9886\u57df\u751a\u81f3\u4f18\u4e8e\u5e8f\u5217\u5316\u3002"}}
