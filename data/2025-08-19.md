<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 46]
- [cs.CV](#cs.CV) [Total: 43]
- [cs.AI](#cs.AI) [Total: 43]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.IR](#cs.IR) [Total: 15]
- [cs.LG](#cs.LG) [Total: 45]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Deep Language Geometry: Constructing a Metric Space from LLM Weights](https://arxiv.org/abs/2508.11676)
*Maksym Shamrai,Vladyslav Hamolia*

Main category: cs.CL

TL;DR: A novel framework that utilizes the internal weight activations of modern Large Language Models (LLMs) to construct a metric space of languages.


<details>
  <summary>Details</summary>
Motivation: We introduce a novel framework that utilizes the internal weight activations of modern Large Language Models (LLMs) to construct a metric space of languages. Unlike traditional approaches based on hand-crafted linguistic features

Method: Our method automatically derives high-dimensional vector representations by computing weight importance scores via an adapted pruning algorithm.

Result: We validate our approach across diverse datasets and multilingual LLMs, covering 106 languages.

Conclusion: The approach aligns well with established linguistic families while also revealing unexpected inter-language connections that may indicate historical contact or language evolution.

Abstract: We introduce a novel framework that utilizes the internal weight activations
of modern Large Language Models (LLMs) to construct a metric space of
languages. Unlike traditional approaches based on hand-crafted linguistic
features, our method automatically derives high-dimensional vector
representations by computing weight importance scores via an adapted pruning
algorithm. Our approach captures intrinsic language characteristics that
reflect linguistic phenomena. We validate our approach across diverse datasets
and multilingual LLMs, covering 106 languages. The results align well with
established linguistic families while also revealing unexpected inter-language
connections that may indicate historical contact or language evolution. The
source code, computed language latent vectors, and visualization tool are made
publicly available at https://github.com/mshamrai/deep-language-geometry.

</details>


### [2] [Can we Evaluate RAGs with Synthetic Data?](https://arxiv.org/abs/2508.11758)
*Jonas van Elburg,Peter van der Putten,Maarten Marx*

Main category: cs.CL

TL;DR: synthetic QA data can replace human-labeled data in some cases, but not all


<details>
  <summary>Details</summary>
Motivation: investigate whether synthetic question-answer (QA) data generated by large language models (LLMs) can serve as an effective proxy for human-labeled benchmarks when such data is unavailable

Method: synthetic question-answer (QA) data generated by large language models (LLMs)

Result: synthetic benchmarks reliably rank the RAGs varying in terms of retriever configuration, aligning well with human-labeled benchmark baselines. However, they fail to produce consistent RAG rankings when comparing generator architectures

Conclusion: synthetic benchmarks reliably rank the RAGs varying in terms of retriever configuration, aligning well with human-labeled benchmark baselines, but fail to produce consistent RAG rankings when comparing generator architectures. The breakdown possibly arises from a combination of task mismatch between the synthetic and human benchmarks, and stylistic bias favoring certain generators.

Abstract: We investigate whether synthetic question-answer (QA) data generated by large
language models (LLMs) can serve as an effective proxy for human-labeled
benchmarks when such data is unavailable. We assess the reliability of
synthetic benchmarks across two experiments: one varying retriever parameters
while keeping the generator fixed, and another varying the generator with fixed
retriever parameters. Across four datasets, of which two open-domain and two
proprietary, we find that synthetic benchmarks reliably rank the RAGs varying
in terms of retriever configuration, aligning well with human-labeled benchmark
baselines. However, they fail to produce consistent RAG rankings when comparing
generator architectures. The breakdown possibly arises from a combination of
task mismatch between the synthetic and human benchmarks, and stylistic bias
favoring certain generators.

</details>


### [3] [Limitation Learning: Catching Adverse Dialog with GAIL](https://arxiv.org/abs/2508.11767)
*Noah Kasmanoff,Rahul Zalkikar*

Main category: cs.CL

TL;DR: Use imitation learning for conversation to create a dialog policy and identify limitations of dialog models.


<details>
  <summary>Details</summary>
Motivation: Creating a policy in the absence of rewards, by leveraging expert demonstrations.

Method: Apply imitation learning to conversation.

Result: Recover a policy capable of talking to a user given a prompt, and a discriminator capable of classifying between expert and synthetic conversation. Limitations of dialog models are indicated.

Conclusion: Technique can identify adverse behavior of dialog models.

Abstract: Imitation learning is a proven method for creating a policy in the absence of
rewards, by leveraging expert demonstrations. In this work, we apply imitation
learning to conversation. In doing so, we recover a policy capable of talking
to a user given a prompt (input state), and a discriminator capable of
classifying between expert and synthetic conversation. While our policy is
effective, we recover results from our discriminator that indicate the
limitations of dialog models. We argue that this technique can be used to
identify adverse behavior of arbitrary data models common for dialog oriented
tasks.

</details>


### [4] [An LLM Agent-Based Complex Semantic Table Annotation Approach](https://arxiv.org/abs/2508.12868)
*Yilin Geng,Shujing Wang,Chuan Wang,Keqing He,Yanfei Lv,Ying Wang,Zaiwen Feng,Xiaoying Bai*

Main category: cs.CL

TL;DR: 提出了一种基于LLM代理的CTA和CEA方法，该方法优于现有方法，并通过利用Levenshtein距离来减少冗余注释，从而显著降低了时间和token使用成本。


<details>
  <summary>Details</summary>
Motivation: 复杂表格常常带来挑战，例如列名或单元格值的语义丢失、严格的本体层次结构要求、同义词、拼写错误和缩写，这些都阻碍了注释的准确性。

Method: 基于ReAct框架，设计并实施了五个带有定制提示的外部工具，使STA代理能够根据表格特征动态选择合适的注释策略。

Result: 在SemTab挑战赛的Tough Tables和BiodivTab数据集上进行的实验表明，该方法在各种指标上优于现有方法。

Conclusion: 利用Levenshtein距离减少冗余注释，从而显著降低时间和token使用成本，为STA提供高效且经济高效的解决方案。

Abstract: The Semantic Table Annotation (STA) task, which includes Column Type
Annotation (CTA) and Cell Entity Annotation (CEA), maps table contents to
ontology entities and plays important roles in various semantic applications.
However, complex tables often pose challenges such as semantic loss of column
names or cell values, strict ontological hierarchy requirements, homonyms,
spelling errors, and abbreviations, which hinder annotation accuracy. To
address these issues, this paper proposes an LLM-based agent approach for CTA
and CEA. We design and implement five external tools with tailored prompts
based on the ReAct framework, enabling the STA agent to dynamically select
suitable annotation strategies depending on table characteristics. Experiments
are conducted on the Tough Tables and BiodivTab datasets from the SemTab
challenge, which contain the aforementioned challenges. Our method outperforms
existing approaches across various metrics. Furthermore, by leveraging
Levenshtein distance to reduce redundant annotations, we achieve a 70%
reduction in time costs and a 60% reduction in LLM token usage, providing an
efficient and cost-effective solution for STA.

</details>


### [5] [Investigating Transcription Normalization in the Faetar ASR Benchmark](https://arxiv.org/abs/2508.11771)
*Leo Peckham,Michael Ong,Naomi Nagy,Ewan Dunbar*

Main category: cs.CL

TL;DR: transcription inconsistencies are not the main challenge in the Faetar ASR benchmark, and bigram word-based language modelling is of no added benefit


<details>
  <summary>Details</summary>
Motivation: examining the challenging low-resource ASR benchmark

Method: examine the role of transcription inconsistencies with the help of a small, hand-constructed lexicon

Result: bigram word-based language modelling is of no added benefit, but that constraining decoding to a finite lexicon can be beneficial; the task remains extremely difficult

Conclusion: transcription inconsistencies are not the main challenge in the Faetar ASR benchmark

Abstract: We examine the role of transcription inconsistencies in the Faetar Automatic
Speech Recognition benchmark, a challenging low-resource ASR benchmark. With
the help of a small, hand-constructed lexicon, we conclude that find that,
while inconsistencies do exist in the transcriptions, they are not the main
challenge in the task. We also demonstrate that bigram word-based language
modelling is of no added benefit, but that constraining decoding to a finite
lexicon can be beneficial. The task remains extremely difficult.

</details>


### [6] [A Multi-Task Evaluation of LLMs' Processing of Academic Text Input](https://arxiv.org/abs/2508.11779)
*Tianyi Li,Yu Qin,Olivia R. Liu Sheng*

Main category: cs.CL

TL;DR: 大型语言模型在处理学术文本方面表现不佳，不建议在同行评审中盲目使用。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型（LLM）在辅助学术同行评审等科学发现中的作用。

Method: 采用包含内容再现/比较/评分/反思四个任务的评估工作流程，使用顶级信息系统文章作为输入文本，并记录了LLM compromised的性能表现。

Result: Google的Gemini在学术文本的总结和释义方面表现尚可，但通过成对文本比较对文本进行排序的可扩展性较差，对学术文本进行评分时容易出现较差的区分度，其对文本的定性反思具有自洽性，但很难激发有意义的研究。

Conclusion: 不建议在同行评审中未经检查地使用大型语言模型（LLM）。

Abstract: How much large language models (LLMs) can aid scientific discovery, notably
in assisting academic peer review, is in heated debate. Between a literature
digest and a human-comparable research assistant lies their practical
application potential. We organize individual tasks that computer science
studies employ in separate terms into a guided and robust workflow to evaluate
LLMs' processing of academic text input. We employ four tasks in the
assessment: content reproduction/comparison/scoring/reflection, each demanding
a specific role of the LLM (oracle/judgmental arbiter/knowledgeable
arbiter/collaborator) in assisting scholarly works, and altogether testing LLMs
with questions that increasingly require intellectual capabilities towards a
solid understanding of scientific texts to yield desirable solutions. We
exemplify a rigorous performance evaluation with detailed instructions on the
prompts. Adopting first-rate Information Systems articles at three top journals
as the input texts and an abundant set of text metrics, we record a compromised
performance of the leading LLM - Google's Gemini: its summary and paraphrase of
academic text is acceptably reliable; using it to rank texts through pairwise
text comparison is faintly scalable; asking it to grade academic texts is prone
to poor discrimination; its qualitative reflection on the text is
self-consistent yet hardly insightful to inspire meaningful research. This
evidence against an endorsement of LLMs' text-processing capabilities is
consistent across metric-based internal (linguistic assessment), external
(comparing to the ground truth), and human evaluation, and is robust to the
variations of the prompt. Overall, we do not recommend an unchecked use of LLMs
in constructing peer reviews.

</details>


### [7] [LLM-Guided Planning and Summary-Based Scientific Text Simplification: DS@GT at CLEF 2025 SimpleText](https://arxiv.org/abs/2508.11816)
*Krishna Chaitanya Marturi,Heba H. Elwazzan*

Main category: cs.CL

TL;DR: This paper uses LLMs in a two-stage framework to simplify scientific text at both sentence and document levels.


<details>
  <summary>Details</summary>
Motivation: The paper addresses both sentence-level and document-level scientific text simplification.

Method: The methodology employs large language models (LLMs) to generate structured plans for sentence-level simplification and leverages LLMs to produce concise summaries to guide the document-level simplification process.

Result: The two-stage, LLM-based framework enables more coherent and contextually faithful simplifications of scientific text.

Conclusion: This paper presents a two-stage, LLM-based framework for coherent and contextually faithful simplification of scientific text at both the sentence and document levels.

Abstract: In this paper, we present our approach for the CLEF 2025 SimpleText Task 1,
which addresses both sentence-level and document-level scientific text
simplification. For sentence-level simplification, our methodology employs
large language models (LLMs) to first generate a structured plan, followed by
plan-driven simplification of individual sentences. At the document level, we
leverage LLMs to produce concise summaries and subsequently guide the
simplification process using these summaries. This two-stage, LLM-based
framework enables more coherent and contextually faithful simplifications of
scientific text.

</details>


### [8] [Hallucination Detection and Mitigation in Scientific Text Simplification using Ensemble Approaches: DS@GT at CLEF 2025 SimpleText](https://arxiv.org/abs/2508.11823)
*Krishna Chaitanya Marturi,Heba H. Elwazzan*

Main category: cs.CL

TL;DR: 本文介绍了一种用于检测科学文本简化中伪造和扭曲的集成方法，该方法结合了BERT、语义相似性、自然语言推理和LLM。


<details>
  <summary>Details</summary>
Motivation: 专注于检测和评估科学文本简化中的创造性生成和信息扭曲。

Method: 集成了多种策略：构建了一个利用基于BERT的分类器、语义相似性度量、自然语言推理模型和大型语言模型（LLM）推理的集成框架。使用元分类器组合这些不同的信号。

Result: 构建了一个用于CLEF 2025 SimpleText Task 2 的方法。

Conclusion: 集成分类器提升了检测伪造和扭曲的鲁棒性。LLM-based的后编辑系统根据原始输入文本修改简化。

Abstract: In this paper, we describe our methodology for the CLEF 2025 SimpleText Task
2, which focuses on detecting and evaluating creative generation and
information distortion in scientific text simplification. Our solution
integrates multiple strategies: we construct an ensemble framework that
leverages BERT-based classifier, semantic similarity measure, natural language
inference model, and large language model (LLM) reasoning. These diverse
signals are combined using meta-classifiers to enhance the robustness of
spurious and distortion detection. Additionally, for grounded generation, we
employ an LLM-based post-editing system that revises simplifications based on
the original input texts.

</details>


### [9] [A Survey of Idiom Datasets for Psycholinguistic and Computational Research](https://arxiv.org/abs/2508.11828)
*Michael Flor,Xinyi Liu,Anna Feldman*

Main category: cs.CL

TL;DR: This survey reviews datasets for studying idioms in psycholinguistics and computational linguistics, highlighting annotation practices, coverage, and task framing across 53 datasets. It finds a disconnect between the two fields.


<details>
  <summary>Details</summary>
Motivation: Idioms are figurative expressions whose meanings often cannot be inferred from their individual words, making them difficult to process computationally and posing challenges for human experimental studies.

Method: This survey reviews datasets developed in psycholinguistics and computational linguistics for studying idioms, focusing on their content, form, and intended use. We present trends in annotation practices, coverage, and task framing across 53 datasets.

Result: Psycholinguistic resources typically contain normed ratings along dimensions such as familiarity, transparency, and compositionality, while computational datasets support tasks like idiomaticity detection/classification, paraphrasing, and cross-lingual modeling. Although recent efforts expanded language coverage and task diversity

Conclusion: There seems to be no relation yet between psycholinguistic and computational research on idioms.

Abstract: Idioms are figurative expressions whose meanings often cannot be inferred
from their individual words, making them difficult to process computationally
and posing challenges for human experimental studies. This survey reviews
datasets developed in psycholinguistics and computational linguistics for
studying idioms, focusing on their content, form, and intended use.
Psycholinguistic resources typically contain normed ratings along dimensions
such as familiarity, transparency, and compositionality, while computational
datasets support tasks like idiomaticity detection/classification,
paraphrasing, and cross-lingual modeling. We present trends in annotation
practices, coverage, and task framing across 53 datasets. Although recent
efforts expanded language coverage and task diversity, there seems to be no
relation yet between psycholinguistic and computational research on idioms.

</details>


### [10] [Every 28 Days the AI Dreams of Soft Skin and Burning Stars: Scaffolding AI Agents with Hormones and Emotions](https://arxiv.org/abs/2508.11829)
*Leigh Levinson,Christopher J. Agostino*

Main category: cs.CL

TL;DR: 本研究发现，将模拟的激素周期嵌入大型语言模型中会影响其情绪、风格和性能，这表明生物节律可以作为一种自然关联性过滤器。


<details>
  <summary>Details</summary>
Motivation: AI系统难以确定上下文中相关的信息，本研究假设生物节律可以作为自然的关联性过滤器来解决这个问题。

Method: 通过模拟月经周期和昼夜节律，将激素周期嵌入大型语言模型中，这些激素包括雌激素、睾酮和皮质醇。

Result: 语言分析揭示了情绪和风格的变化，这些变化与生物学阶段相关。在SQuAD、MMLU、Hellaswag和AI2-ARC上的基准测试表明，性能变化与生物学预期相符。

Conclusion: 语言模型在不同生物节律阶段表现出细微但一致的性能变化，包括在适度而非极端激素范围内的最佳功能。

Abstract: Despite significant advances, AI systems struggle with the frame problem:
determining what information is contextually relevant from an exponentially
large possibility space. We hypothesize that biological rhythms, particularly
hormonal cycles, serve as natural relevance filters that could address this
fundamental challenge. We develop a framework that embeds simulated menstrual
and circadian cycles into Large Language Models through system prompts
generated from periodic functions modeling key hormones including estrogen,
testosterone, and cortisol. Across multiple state-of-the-art models, linguistic
analysis reveals emotional and stylistic variations that track biological
phases; sadness peaks during menstruation while happiness dominates ovulation
and circadian patterns show morning optimism transitioning to nocturnal
introspection. Benchmarking on SQuAD, MMLU, Hellaswag, and AI2-ARC demonstrates
subtle but consistent performance variations aligning with biological
expectations, including optimal function in moderate rather than extreme
hormonal ranges. This methodology provides a novel approach to contextual AI
while revealing how societal biases regarding gender and biology are embedded
within language models.

</details>


### [11] [When Does Language Transfer Help? Sequential Fine-Tuning for Cross-Lingual Euphemism Detection](https://arxiv.org/abs/2508.11831)
*Julia Sammartino,Libby Barak,Jing Peng,Anna Feldman*

Main category: cs.CL

TL;DR: Sequential fine-tuning helps multilingual models detect euphemisms, particularly in low-resource languages.


<details>
  <summary>Details</summary>
Motivation: Euphemisms are culturally variable and ambiguous, posing challenges for language models, especially in low-resource settings.

Method: Cross-lingual transfer via sequential fine-tuning with XLM-R and mBERT.

Result: Sequential fine-tuning with a high-resource L1 improves L2 performance, especially for low-resource languages. XLM-R achieves larger gains but is more sensitive to pretraining gaps, while mBERT yields more stable, though lower, results.

Conclusion: Sequential fine-tuning improves euphemism detection in multilingual models, especially for low-resource languages.

Abstract: Euphemisms are culturally variable and often ambiguous, posing challenges for
language models, especially in low-resource settings. This paper investigates
how cross-lingual transfer via sequential fine-tuning affects euphemism
detection across five languages: English, Spanish, Chinese, Turkish, and
Yoruba. We compare sequential fine-tuning with monolingual and simultaneous
fine-tuning using XLM-R and mBERT, analyzing how performance is shaped by
language pairings, typological features, and pretraining coverage. Results show
that sequential fine-tuning with a high-resource L1 improves L2 performance,
especially for low-resource languages like Yoruba and Turkish. XLM-R achieves
larger gains but is more sensitive to pretraining gaps and catastrophic
forgetting, while mBERT yields more stable, though lower, results. These
findings highlight sequential fine-tuning as a simple yet effective strategy
for improving euphemism detection in multilingual models, particularly when
low-resource languages are involved.

</details>


### [12] [SupraTok: Cross-Boundary Tokenization for Enhanced Language Model Performance](https://arxiv.org/abs/2508.11857)
*Andrei-Valentin Tănase,Elena Pelican*

Main category: cs.CL

TL;DR: SupraTok, a novel tokenization architecture, improves tokenization efficiency and language model performance.


<details>
  <summary>Details</summary>
Motivation: Tokenization remains a fundamental yet underexplored bottleneck in natural language processing, with strategies largely static despite remarkable progress in model architectures.

Method: SupraTok, a novel tokenization architecture that reimagines subword segmentation through three innovations: cross-boundary pattern learning, entropy-driven data curation, and multi-phase curriculum learning.

Result: SupraTok achieves 31% improvement in English tokenization efficiency compared to OpenAI's o200k tokenizer and 30% improvement over Google's Gemma 3 tokenizer, while maintaining competitive performance across 38 languages. When integrated with a GPT-2 scale model, SupraTok yields 8.4% improvement on HellaSWAG and 9.5% on MMLU benchmarks without architectural modifications.

Conclusion: Efficient tokenization can complement architectural innovations as a path to improved language model performance.

Abstract: Tokenization remains a fundamental yet underexplored bottleneck in natural
language processing, with strategies largely static despite remarkable progress
in model architectures. We present SupraTok, a novel tokenization architecture
that reimagines subword segmentation through three innovations: cross-boundary
pattern learning that discovers multi-word semantic units, entropy-driven data
curation that optimizes training corpus quality, and multi-phase curriculum
learning for stable convergence. Our approach extends Byte-Pair Encoding by
learning "superword" tokens, coherent multi-word expressions that preserve
semantic unity while maximizing compression efficiency. SupraTok achieves 31%
improvement in English tokenization efficiency (5.91 versus 4.51 characters per
token) compared to OpenAI's o200k tokenizer and 30% improvement over Google's
Gemma 3 tokenizer (256k vocabulary), while maintaining competitive performance
across 38 languages. When integrated with a GPT-2 scale model (124M parameters)
trained on 10 billion tokens from the FineWeb-Edu dataset, SupraTok yields 8.4%
improvement on HellaSWAG and 9.5% on MMLU benchmarks without architectural
modifications. While these results are promising at this scale, further
validation at larger model scales is needed. These findings suggest that
efficient tokenization can complement architectural innovations as a path to
improved language model performance.

</details>


### [13] [In-Context Examples Matter: Improving Emotion Recognition in Conversation with Instruction Tuning](https://arxiv.org/abs/2508.11889)
*Hui Ma,Bo Zhang,Jinpeng Hu,Zenglin Shi*

Main category: cs.CL

TL;DR: 本文提出了一种用于 ERC 的简单而有效的单阶段上下文指令调整框架 InitERC。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在多阶段指令调整，这种方法从根本上限制了在统一框架内共同捕获说话者特征和对话上下文之间动态交互的能力，从而导致说话者身份、上下文线索和情绪状态之间的弱对齐。

Method: InitERC 包含四个组件，即演示池构建、上下文示例选择、提示模板设计和上下文指令调整。

Result: InitERC 适应 LLM，通过上下文指令调整从上下文示例中学习说话者-上下文-情感对齐。

Conclusion: 提出的 InitERC 在三个广泛使用的数据集上实现了相对于最先进基线的显着改进。

Abstract: Emotion recognition in conversation (ERC) aims to identify the emotion of
each utterance in a conversation, playing a vital role in empathetic artificial
intelligence. With the growing of large language models (LLMs), instruction
tuning has emerged as a critical paradigm for ERC. Existing studies mainly
focus on multi-stage instruction tuning, which first endows LLMs with speaker
characteristics, and then conducts context-aware instruction tuning to
comprehend emotional states. However, these methods inherently constrains the
capacity to jointly capture the dynamic interaction between speaker
characteristics and conversational context, resulting in weak alignment among
speaker identity, contextual cues, and emotion states within a unified
framework. In this paper, we propose InitERC, a simple yet effective one-stage
in-context instruction tuning framework for ERC. InitERC adapts LLMs to learn
speaker-context-emotion alignment from context examples via in-context
instruction tuning. Specifically, InitERC comprises four components, i.e.,
demonstration pool construction, in-context example selection, prompt template
design, and in-context instruction tuning. To explore the impact of in-context
examples, we conduct a comprehensive study on three key factors: retrieval
strategy, example ordering, and the number of examples. Extensive experiments
on three widely used datasets demonstrate that our proposed InitERC achieves
substantial improvements over the state-of-the-art baselines.

</details>


### [14] [CORE: Measuring Multi-Agent LLM Interaction Quality under Game-Theoretic Pressures](https://arxiv.org/abs/2508.11915)
*Punya Syon Pandey,Yongjin Yang,Jiarui Liu,Zhijing Jin*

Main category: cs.CL

TL;DR: This paper introduces CORE, a metric to quantify language use in multi-agent LLM systems, and finds that cooperation leads to more diverse language use than competition.


<details>
  <summary>Details</summary>
Motivation: The linguistic diversity of interactions between agents with LLMs has not been sufficiently quantified.

Method: Introduce CORE, a metric integrating cluster entropy, lexical repetition, and semantic similarity. Apply CORE to pairwise LLM dialogs across competitive, cooperative, and neutral settings, using Zipf's and Heaps' Laws.

Result: Cooperative settings exhibit steeper Zipf distributions and higher Heap exponents, indicating more repetition and greater vocabulary expansion. Competitive interactions display lower Zipf and Heaps exponents, reflecting less repetition and more constrained vocabularies.

Conclusion: Cooperative settings lead to more repetition and vocabulary expansion, while competitive ones show less repetition and constrained vocabularies. CORE is a robust diagnostic for measuring linguistic robustness in multi-agent LLM systems.

Abstract: Game-theoretic interactions between agents with Large Language Models (LLMs)
have revealed many emergent capabilities, yet the linguistic diversity of these
interactions has not been sufficiently quantified. In this paper, we present
the Conversational Robustness Evaluation Score: CORE, a metric to quantify the
effectiveness of language use within multi-agent systems across different
game-theoretic interactions. CORE integrates measures of cluster entropy,
lexical repetition, and semantic similarity, providing a direct lens of dialog
quality. We apply CORE to pairwise LLM dialogs across competitive, cooperative,
and neutral settings, further grounding our analysis in Zipf's and Heaps' Laws
to characterize word frequency distributions and vocabulary growth. Our
findings show that cooperative settings exhibit both steeper Zipf distributions
and higher Heap exponents, indicating more repetition alongside greater
vocabulary expansion. In contrast, competitive interactions display lower Zipf
and Heaps exponents, reflecting less repetition and more constrained
vocabularies. These results provide new insights into how social incentives
influence language adaptation, and highlight CORE as a robust diagnostic for
measuring linguistic robustness in multi-agent LLM systems. Our code is
available at https://github.com/psyonp/core.

</details>


### [15] [LLMs Struggle with NLI for Perfect Aspect: A Cross-Linguistic Study in Chinese and Japanese](https://arxiv.org/abs/2508.11927)
*Jie Lu,Du Jin,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 构建了一个针对汉语和日语的NLI数据集，揭示了现有模型在时间推理方面的不足，强调了跨语言评估的重要性。


<details>
  <summary>Details</summary>
Motivation: 汉语和日语在完成体中缺乏独立的时态语法形式，这使得自然语言推理（NLI）变得复杂。

Method: 构建了一个基于语言学动机的、基于模板的NLI数据集（每种语言1,350对）。

Result: 实验表明，即使是高级的LLM也很难进行时间推理，尤其是在检测细微的时态和参考时间变化时。这些发现突出了模型的局限性，并强调了时间语义中跨语言评估的必要性。

Conclusion: 即使是高级的LLM也很难进行时间推理，尤其是在检测细微的时态和参考时间变化时。

Abstract: Unlike English, which uses distinct forms (e.g., had, has, will have) to mark
the perfect aspect across tenses, Chinese and Japanese lack separate
grammatical forms for tense within the perfect aspect, which complicates
Natural Language Inference (NLI). Focusing on the perfect aspect in these
languages, we construct a linguistically motivated, template-based NLI dataset
(1,350 pairs per language). Experiments reveal that even advanced LLMs struggle
with temporal inference, particularly in detecting subtle tense and
reference-time shifts. These findings highlight model limitations and
underscore the need for cross-linguistic evaluation in temporal semantics. Our
dataset is available at https://github.com/Lujie2001/CrossNLI.

</details>


### [16] [CAMF: Collaborative Adversarial Multi-agent Framework for Machine Generated Text Detection](https://arxiv.org/abs/2508.11933)
*Yue Wang,Liesheng Wei,Yuxiang Wang*

Main category: cs.CL

TL;DR: 本文提出了CAMF，一种用于检测机器生成文本的新架构，它通过协同对抗过程，能够更深入地分析文本不一致性，从而优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 检测大型语言模型（LLM）生成的机器生成文本（MGT）日益重要，但现有的零样本检测范例存在不足，包括分析肤浅和缺乏对跨语言维度一致性的研究。

Method: 提出了一个名为CAMF的新架构，该架构使用多个基于LLM的代理，采用协同三阶段流程：多维语言特征提取、对抗一致性探测和综合判断聚合。

Result: 实验评估表明，CAMF明显优于最先进的零样本MGT检测技术。

Conclusion: CAMF在检测机器生成文本方面显著优于现有技术。

Abstract: Detecting machine-generated text (MGT) from contemporary Large Language
Models (LLMs) is increasingly crucial amid risks like disinformation and
threats to academic integrity. Existing zero-shot detection paradigms, despite
their practicality, often exhibit significant deficiencies. Key challenges
include: (1) superficial analyses focused on limited textual attributes, and
(2) a lack of investigation into consistency across linguistic dimensions such
as style, semantics, and logic. To address these challenges, we introduce the
\textbf{C}ollaborative \textbf{A}dversarial \textbf{M}ulti-agent
\textbf{F}ramework (\textbf{CAMF}), a novel architecture using multiple
LLM-based agents. CAMF employs specialized agents in a synergistic three-phase
process: \emph{Multi-dimensional Linguistic Feature Extraction},
\emph{Adversarial Consistency Probing}, and \emph{Synthesized Judgment
Aggregation}. This structured collaborative-adversarial process enables a deep
analysis of subtle, cross-dimensional textual incongruities indicative of
non-human origin. Empirical evaluations demonstrate CAMF's significant
superiority over state-of-the-art zero-shot MGT detection techniques.

</details>


### [17] [Learning Wisdom from Errors: Promoting LLM's Continual Relation Learning through Exploiting Error Cases](https://arxiv.org/abs/2508.12031)
*Shaozhe Yin,Jinyu Guo,Kai Shuang,Xia Liu,Ruize Ou*

Main category: cs.CL

TL;DR: 提出了一种新的基于指令的持续对比调整方法，用于大型语言模型（LLM）在CRE中应用，该方法在错误案例上表现出色，并在TACRED和FewRel数据集上取得了新的state-of-the-art CRE性能。


<details>
  <summary>Details</summary>
Motivation: 现有的CRE方法主要使用记忆重放和对比学习来减轻灾难性遗忘，但是这些方法没有重视能够更有效地揭示模型认知偏差的错误案例。

Method: 提出了一种基于指令的持续对比调整方法，用于大型语言模型（LLM）在CRE中应用。该方法将每个任务的训练和记忆数据分为两部分，并利用LLM的指令跟随能力，提出了一种新的基于指令的对比调整策略。

Result: 该模型在TACRED和FewRel数据集上取得了显著的改进。

Conclusion: 该模型在TACRED和FewRel数据集上取得了新的state-of-the-art CRE性能，表明了专门利用错误案例的重要性。

Abstract: Continual Relation Extraction (CRE) aims to continually learn new emerging
relations while avoiding catastrophic forgetting. Existing CRE methods mainly
use memory replay and contrastive learning to mitigate catastrophic forgetting.
However, these methods do not attach importance to the error cases that can
reveal the model's cognitive biases more effectively. To address this issue, we
propose an instruction-based continual contrastive tuning approach for Large
Language Models (LLMs) in CRE. Different from existing CRE methods that
typically handle the training and memory data in a unified manner, this
approach splits the training and memory data of each task into two parts
respectively based on the correctness of the initial responses and treats them
differently through dual-task fine-tuning. In addition, leveraging the
advantages of LLM's instruction-following ability, we propose a novel
instruction-based contrastive tuning strategy for LLM to continuously correct
current cognitive biases with the guidance of previous data in an
instruction-tuning manner, which mitigates the gap between old and new
relations in a more suitable way for LLMs. We experimentally evaluate our model
on TACRED and FewRel, and the results show that our model achieves new
state-of-the-art CRE performance with significant improvements, demonstrating
the importance of specializing in exploiting error cases.

</details>


### [18] [A Question Answering Dataset for Temporal-Sensitive Retrieval-Augmented Generation](https://arxiv.org/abs/2508.12282)
*Ziyang Chen,Erxue Min,Xiang Zhao,Yunxin Li,Xin Jia,Jinzhi Liao,Jichao Li,Shuaiqiang Wang,Baotian Hu,Dawei Yin*

Main category: cs.CL

TL;DR: ChronoQA 是一个大规模中文问答数据集，用于评估 RAG 系统中的时间推理。


<details>
  <summary>Details</summary>
Motivation: 为了评估检索增强生成 (RAG) 系统中的时间推理，我们引入了 ChronoQA，这是一个大规模的中文问答基准数据集。

Method: ChronoQA 是从 2019 年至 2024 年间发布的超过 300,000 篇新闻文章构建的，包含 5,176 个高质量问题，涵盖绝对、聚合和相对时间类型，包括显式和隐式时间表达。

Result: ChronoQA 具有全面的结构注释，并经过了多阶段验证，包括基于规则的、基于 LLM 的和人工评估，以确保数据质量。该数据集支持单文档和多文档场景，反映了对时间对齐和逻辑一致性的实际要求。

Conclusion: ChronoQA 通过提供动态、可靠和可扩展的资源，能够对各种时间任务进行结构化评估，并作为推进时间敏感的检索增强问答系统的强大基准。

Abstract: We introduce ChronoQA, a large-scale benchmark dataset for Chinese question
answering, specifically designed to evaluate temporal reasoning in
Retrieval-Augmented Generation (RAG) systems. ChronoQA is constructed from over
300,000 news articles published between 2019 and 2024, and contains 5,176
high-quality questions covering absolute, aggregate, and relative temporal
types with both explicit and implicit time expressions. The dataset supports
both single- and multi-document scenarios, reflecting the real-world
requirements for temporal alignment and logical consistency. ChronoQA features
comprehensive structural annotations and has undergone multi-stage validation,
including rule-based, LLM-based, and human evaluation, to ensure data quality.
By providing a dynamic, reliable, and scalable resource, ChronoQA enables
structured evaluation across a wide range of temporal tasks, and serves as a
robust benchmark for advancing time-sensitive retrieval-augmented question
answering systems.

</details>


### [19] [Mind the Generation Process: Fine-Grained Confidence Estimation During LLM Generation](https://arxiv.org/abs/2508.12040)
*Jinyi Han,Tingyun Li,Shisong Chen,Jie Shi,Xinyi Wang,Guanglei Yue,Jiaqing Liang,Xin Lin,Liqian Wen,Zulong Chen,Yanghua Xiao*

Main category: cs.CL

TL;DR: FineCE is a new confidence estimation method that provides accurate, fine-grained confidence scores during text generation, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing approaches suffer from coarse-grained scoring mechanisms that fail to provide fine-grained, continuous confidence estimates throughout the generation process. Accurate confidence estimation is critical for enhancing the trustworthiness and reliability of LLM-generated outputs, as LLMs lack self-awareness and frequently exhibit overconfidence.

Method: FineCE: a novel confidence estimation method that delivers accurate, fine-grained confidence scores during text generation. It includes a comprehensive pipeline for constructing training data and a Backward Confidence Integration (BCI) strategy.

Result: FineCE delivers accurate, fine-grained confidence scores during text generation.

Conclusion: FineCE consistently outperforms existing classical confidence estimation methods.

Abstract: While large language models (LLMs) have demonstrated remarkable performance
across diverse tasks, they fundamentally lack self-awareness and frequently
exhibit overconfidence, assigning high confidence scores to incorrect
predictions. Accurate confidence estimation is therefore critical for enhancing
the trustworthiness and reliability of LLM-generated outputs. However, existing
approaches suffer from coarse-grained scoring mechanisms that fail to provide
fine-grained, continuous confidence estimates throughout the generation
process. To address these limitations, we introduce FineCE, a novel confidence
estimation method that delivers accurate, fine-grained confidence scores during
text generation. Specifically, we first develop a comprehensive pipeline for
constructing training data that effectively captures the underlying
probabilistic distribution of LLM responses, and then train a model to predict
confidence scores for arbitrary text sequences in a supervised manner.
Furthermore, we propose a Backward Confidence Integration (BCI) strategy that
leverages information from the subsequent text to enhance confidence estimation
for the current sequence during inference. We also introduce three strategies
for identifying optimal positions to perform confidence estimation within the
generation process. Extensive experiments on multiple benchmark datasets
demonstrate that FineCE consistently outperforms existing classical confidence
estimation methods. Our code and all baselines used in the paper are available
on GitHub.

</details>


### [20] [J6: Jacobian-Driven Role Attribution for Multi-Objective Prompt Optimization in LLMs](https://arxiv.org/abs/2508.12086)
*Yao Wu*

Main category: cs.CL

TL;DR: 提出了一种基于结构化雅可比矩阵的冲突感知提示优化方法，用于解决大型语言模型适应中的多目标优化问题。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型 (LLM) 适应中，平衡多个优化目标（例如提高事实性和增加置信度）是一个根本挑战，尤其是在提示参数以非平凡的方式交互时。

Method: 提出了一种名为 J6 的结构化雅可比矩阵方法，该方法将梯度交互矩阵分解为六个可解释的组成部分，以实现动态更新框架。

Result: J6 的可解释结构提供了对参数归因、任务干涉和几何对齐适应的深入了解。

Conclusion: 提出了一个基于结构化雅可比矩阵的方法，用于解决大型语言模型 (LLM) 适应中的多目标优化问题，并为冲突感知的提示优化提供了一个有原则且可扩展的机制。

Abstract: In large language model (LLM) adaptation, balancing multiple optimization
objectives such as improving factuality (heat) and increasing confidence (via
low entropy) poses a fundamental challenge, especially when prompt parameters
(e.g., hidden-layer insertions h and embedding modifications w) interact in
non-trivial ways. Existing multi-objective optimization strategies often rely
on scalar gradient aggregation, ignoring the deeper geometric structure between
objectives and parameters. We propose J6, a structured Jacobian-based method
that decomposes the gradient interaction matrix into six interpretable
components. This decomposition enables both hard decision-making (e.g.,
choosing the dominant update direction via argmax) and soft strategies (e.g.,
attention-style weighting via softmax over J6), forming a dynamic update
framework that adapts to local conflict and synergy. Moreover, the
interpretable structure of J6 provides insight into parameter attribution, task
interference, and geometry-aligned adaptation. Our work introduces a principled
and extensible mechanism for conflict-aware prompt optimization, and opens a
new avenue for incorporating structured Jacobian reasoning into multi-objective
neural tuning.

</details>


### [21] [All for law and law for all: Adaptive RAG Pipeline for Legal Research](https://arxiv.org/abs/2508.13107)
*Figarri Keisha,Prince Singh,Pallavi,Dion Fernandes,Aravindh Manivannan,Ilham Wicaksono,Faisal Ahmad*

Main category: cs.CL

TL;DR: 本文提出了一种用于法律领域的 RAG 管道，该管道通过上下文感知查询转换器、开源检索策略和综合评估框架来增强 LegalBenchRAG 基线，实验结果表明，精心设计的开源管道在检索质量上可以与专有方法相媲美或超过专有方法。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成 (RAG) 通过将大型语言模型输出建立在引用的来源中来减轻幻觉，这种能力在法律领域尤其重要。

Method: 端到端 RAG 管道，它重新审视并扩展了 LegalBenchRAG 基线，并进行了三项有针对性的增强：(i) 一种上下文感知的查询转换器，可将文档引用与自然语言问题分离，并根据专业知识和特异性调整检索深度和响应风格，(ii) 使用 SBERT 和 GTE 嵌入的开源检索策略，可实现显著的性能提升（将 Recall@K 提高 30-95%，Precision@K 提高约 2.5 倍，对于 K>4），同时保持成本效益，以及 (iii) 一个综合评估和生成框架，它结合了 RAGAS、BERTScore-F1 和 ROUGE-Recall，以评估跨模型和提示设计的语义对齐和忠实度。

Result: 精心设计的开源管道在检索质量上可以与专有方法相媲美或超过专有方法，而定制的法律基础提示始终比基线提示产生更忠实和上下文相关的答案。

Conclusion: 精心设计的开源管道在检索质量上可以与专有方法相媲美或超过专有方法，而定制的法律基础提示始终比基线提示产生更忠实和上下文相关的答案。这些贡献证明了任务感知、组件级调整在为法律研究提供具有法律依据、可重现且具有成本效益的 RAG 系统的潜力。

Abstract: Retrieval-Augmented Generation (RAG) mitigates hallucinations by grounding
large language model outputs in cited sources, a capability that is especially
critical in the legal domain. We present an end-to-end RAG pipeline that
revisits and extends the LegalBenchRAG baseline with three targeted
enhancements: (i) a context-aware query translator that disentangles document
references from natural-language questions and adapts retrieval depth and
response style based on expertise and specificity, (ii) open-source retrieval
strategies using SBERT and GTE embeddings that achieve substantial performance
gains (improving Recall@K by 30-95\% and Precision@K by $\sim$2.5$\times$ for
$K>4$) while remaining cost-efficient, and (iii) a comprehensive evaluation and
generation framework that combines RAGAS, BERTScore-F1, and ROUGE-Recall to
assess semantic alignment and faithfulness across models and prompt designs.
Our results show that carefully designed open-source pipelines can rival or
outperform proprietary approaches in retrieval quality, while a custom
legal-grounded prompt consistently produces more faithful and contextually
relevant answers than baseline prompting. Taken together, these contributions
demonstrate the potential of task-aware, component-level tuning to deliver
legally grounded, reproducible, and cost-effective RAG systems for legal
research assistance.

</details>


### [22] [STEM: Efficient Relative Capability Evaluation of LLMs through Structured Transition Samples](https://arxiv.org/abs/2508.12096)
*Haiquan Hu,Jiazhi Jiang,Shiyou Xu,Ruhan Zeng,Tian Wang*

Main category: cs.CL

TL;DR: STEM 是一种轻量级且可解释的评估框架，用于有效估计法学硕士的相对能力。


<details>
  <summary>Details</summary>
Motivation: 随着模型能力的快速发展，评估大型语言模型 (LLM) 变得越来越具有挑战性。近期的模型通常在标准基准测试中获得更高的分数，但这些改进并不能始终反映出增强的实际推理能力。此外，广泛过度拟合公共基准以及完全评估的高计算成本使得区分模型之间有意义的差异既昂贵又效果较差。

Method: 通过分析相同架构但参数规模不同的大型语言模型之间一致的性能转换来识别显着的转换样本 (STS)。

Result: 实验结果表明，STEM 可靠地捕获性能趋势，与模型能力的真实排名一致。

Conclusion: STEM 是一种实用且可扩展的方法，可以对 LLM 进行细粒度的、与架构无关的评估。

Abstract: Evaluating large language models (LLMs) has become increasingly challenging
as model capabilities advance rapidly. While recent models often achieve higher
scores on standard benchmarks, these improvements do not consistently reflect
enhanced real-world reasoning capabilities. Moreover, widespread overfitting to
public benchmarks and the high computational cost of full evaluations have made
it both expensive and less effective to distinguish meaningful differences
between models. To address these challenges, we propose the \textbf{S}tructured
\textbf{T}ransition \textbf{E}valuation \textbf{M}ethod (STEM), a lightweight
and interpretable evaluation framework for efficiently estimating the relative
capabilities of LLMs. STEM identifies \textit{significant transition samples}
(STS) by analyzing consistent performance transitions among LLMs of the same
architecture but varying parameter scales. These samples enable STEM to
effectively estimate the capability position of an unknown model. Qwen3 model
family is applied to construct the STS pool on six diverse and representative
benchmarks. To assess generalizability. Experimental results indicate that STEM
reliably captures performance trends, aligns with ground-truth rankings of
model capability. These findings highlight STEM as a practical and scalable
method for fine-grained, architecture-agnostic evaluation of LLMs.

</details>


### [23] [Exploring Efficiency Frontiers of Thinking Budget in Medical Reasoning: Scaling Laws between Computational Resources and Reasoning Quality](https://arxiv.org/abs/2508.12140)
*Ziqian Bi,Lu Chen,Junhao Song,Hongying Luo,Enze Ge,Junmin Huang,Tianyang Wang,Keyu Chen,Chia Xin Liang,Zihan Wei,Huafeng Liu,Chunjie Tian,Jibin Guan,Joe Yeong,Yongzhi Xu,Peng Wang,Junfeng Hao*

Main category: cs.CL

TL;DR: Thinking budget mechanisms in medical reasoning tasks are evaluated, revealing scaling laws and efficiency regimes. Smaller models benefit more from extended thinking.


<details>
  <summary>Details</summary>
Motivation: To evaluate thinking budget mechanisms in medical reasoning tasks and reveal scaling laws between computational resources and reasoning quality.

Method: Evaluation of Qwen3 and DeepSeek-R1 models across 15 medical datasets with varying thinking budgets.

Result: Logarithmic scaling relationships between accuracy improvements and thinking budget/model size; identification of three efficiency regimes; smaller models benefit more from extended thinking; domain-specific patterns emerge.

Conclusion: Thinking budget control is a critical mechanism for optimizing medical AI systems, enabling dynamic resource allocation aligned with clinical needs while maintaining the transparency essential for healthcare deployment.

Abstract: This study presents the first comprehensive evaluation of thinking budget
mechanisms in medical reasoning tasks, revealing fundamental scaling laws
between computational resources and reasoning quality. We systematically
evaluated two major model families, Qwen3 (1.7B to 235B parameters) and
DeepSeek-R1 (1.5B to 70B parameters), across 15 medical datasets spanning
diverse specialties and difficulty levels. Through controlled experiments with
thinking budgets ranging from zero to unlimited tokens, we establish
logarithmic scaling relationships where accuracy improvements follow a
predictable pattern with both thinking budget and model size. Our findings
identify three distinct efficiency regimes: high-efficiency (0 to 256 tokens)
suitable for real-time applications, balanced (256 to 512 tokens) offering
optimal cost-performance tradeoffs for routine clinical support, and
high-accuracy (above 512 tokens) justified only for critical diagnostic tasks.
Notably, smaller models demonstrate disproportionately larger benefits from
extended thinking, with 15 to 20% improvements compared to 5 to 10% for larger
models, suggesting a complementary relationship where thinking budget provides
greater relative benefits for capacity-constrained models. Domain-specific
patterns emerge clearly, with neurology and gastroenterology requiring
significantly deeper reasoning processes than cardiovascular or respiratory
medicine. The consistency between Qwen3 native thinking budget API and our
proposed truncation method for DeepSeek-R1 validates the generalizability of
thinking budget concepts across architectures. These results establish thinking
budget control as a critical mechanism for optimizing medical AI systems,
enabling dynamic resource allocation aligned with clinical needs while
maintaining the transparency essential for healthcare deployment.

</details>


### [24] [LLM-as-a-Judge for Privacy Evaluation? Exploring the Alignment of Human and LLM Perceptions of Privacy in Textual Data](https://arxiv.org/abs/2508.12158)
*Stephen Meisenbacher,Alexandra Klymenko,Florian Matthes*

Main category: cs.CL

TL;DR: LLMs can be used as privacy evaluators, addressing a core challenge in solving pressing privacy issues with innovative technical solutions.


<details>
  <summary>Details</summary>
Motivation: Accurate evaluation of privacy remains a significant challenge in privacy-preserving NLP.

Method: Using LLMs as a privacy evaluator, inspired by the LLM-as-a-Judge paradigm, to evaluate the privacy sensitivity of textual data and measuring how closely LLM evaluations align with human perceptions of privacy in text.

Result: Privacy is a difficult concept to measure empirically, but LLMs can accurately model a global human privacy perspective.

Conclusion: LLMs can accurately model a global human privacy perspective, paving the way for exploring the feasibility of LLMs as privacy evaluators.

Abstract: Despite advances in the field of privacy-preserving Natural Language
Processing (NLP), a significant challenge remains the accurate evaluation of
privacy. As a potential solution, using LLMs as a privacy evaluator presents a
promising approach $\unicode{x2013}$ a strategy inspired by its success in
other subfields of NLP. In particular, the so-called $\textit{LLM-as-a-Judge}$
paradigm has achieved impressive results on a variety of natural language
evaluation tasks, demonstrating high agreement rates with human annotators.
Recognizing that privacy is both subjective and difficult to define, we
investigate whether LLM-as-a-Judge can also be leveraged to evaluate the
privacy sensitivity of textual data. Furthermore, we measure how closely LLM
evaluations align with human perceptions of privacy in text. Resulting from a
study involving 10 datasets, 13 LLMs, and 677 human survey participants, we
confirm that privacy is indeed a difficult concept to measure empirically,
exhibited by generally low inter-human agreement rates. Nevertheless, we find
that LLMs can accurately model a global human privacy perspective, and through
an analysis of human and LLM reasoning patterns, we discuss the merits and
limitations of LLM-as-a-Judge for privacy evaluation in textual data. Our
findings pave the way for exploring the feasibility of LLMs as privacy
evaluators, addressing a core challenge in solving pressing privacy issues with
innovative technical solutions.

</details>


### [25] [Arabic Multimodal Machine Learning: Datasets, Applications, Approaches, and Challenges](https://arxiv.org/abs/2508.12227)
*Abdelhamid Haouhat,Slimane Bellaouar,Attia Nehar,Hadda Cherroun,Ahmed Abdelali*

Main category: cs.CL

TL;DR: This paper surveys Arabic Multimodal Machine Learning (MML), categorizing research into datasets, applications, approaches, and challenges, to provide insights and highlight research gaps.


<details>
  <summary>Details</summary>
Motivation: Arabic MML has reached a certain level of maturity in its foundational development, making it time to conduct a comprehensive survey.

Method: The paper explores Arabic MML by categorizing efforts through a novel taxonomy and analyzing existing research. The taxonomy organizes these efforts into four key topics: datasets, applications, approaches, and challenges.

Result: The survey offers insights into the current state of Arabic MML.

Conclusion: This survey provides a structured overview of Arabic MML, highlighting areas that have not been investigated and critical research gaps. Researchers will be empowered to build upon the identified opportunities and address challenges to advance the field.

Abstract: Multimodal Machine Learning (MML) aims to integrate and analyze information
from diverse modalities, such as text, audio, and visuals, enabling machines to
address complex tasks like sentiment analysis, emotion recognition, and
multimedia retrieval. Recently, Arabic MML has reached a certain level of
maturity in its foundational development, making it time to conduct a
comprehensive survey. This paper explores Arabic MML by categorizing efforts
through a novel taxonomy and analyzing existing research. Our taxonomy
organizes these efforts into four key topics: datasets, applications,
approaches, and challenges. By providing a structured overview, this survey
offers insights into the current state of Arabic MML, highlighting areas that
have not been investigated and critical research gaps. Researchers will be
empowered to build upon the identified opportunities and address challenges to
advance the field.

</details>


### [26] [SEA-BED: Southeast Asia Embedding Benchmark](https://arxiv.org/abs/2508.12243)
*Wuttikorn Ponwitayarat,Raymond Ng,Jann Railey Montalan,Thura Aung,Jian Gang Ngui,Yosephine Susanto,William Tjhi,Panuthep Tasawong,Erik Cambria,Ekapol Chuangsuwanich,Sarana Nutanong,Peerat Limkonchotiwat*

Main category: cs.CL

TL;DR: Introduce SEA-BED, the first large-scale SEA embedding benchmark with 169 datasets across 9 tasks and 10 languages, where 71% are formulated by humans, not machine generation or translation. Results show sharp ranking shifts, inconsistent model performance among SEA languages, and the importance of human-curated datasets for low-resource languages like Burmese.


<details>
  <summary>Details</summary>
Motivation: Southeast Asia (SEA) datasets are scarce and often machine-translated, missing native linguistic properties. With nearly 700 million speakers, the SEA region lacks a region-specific embedding benchmark.

Method: We introduce SEA-BED, the first large-scale SEA embedding benchmark with 169 datasets across 9 tasks and 10 languages, where 71% are formulated by humans, not machine generation or translation. We evaluate 17 embedding models across six studies, analyzing task and language challenges, cross-benchmark comparisons, and translation trade-offs.

Result: which SEA languages and tasks are challenging, whether SEA languages show unique performance gaps globally, and how human vs. machine translations affect evaluation. Results show sharp ranking shifts, inconsistent model performance among SEA languages, and the importance of human-curated datasets for low-resource languages like Burmese.

Conclusion: Results show sharp ranking shifts, inconsistent model performance among SEA languages, and the importance of human-curated datasets for low-resource languages like Burmese.

Abstract: Sentence embeddings are essential for NLP tasks such as semantic search,
re-ranking, and textual similarity. Although multilingual benchmarks like MMTEB
broaden coverage, Southeast Asia (SEA) datasets are scarce and often
machine-translated, missing native linguistic properties. With nearly 700
million speakers, the SEA region lacks a region-specific embedding benchmark.
We introduce SEA-BED, the first large-scale SEA embedding benchmark with 169
datasets across 9 tasks and 10 languages, where 71% are formulated by humans,
not machine generation or translation. We address three research questions: (1)
which SEA languages and tasks are challenging, (2) whether SEA languages show
unique performance gaps globally, and (3) how human vs. machine translations
affect evaluation. We evaluate 17 embedding models across six studies,
analyzing task and language challenges, cross-benchmark comparisons, and
translation trade-offs. Results show sharp ranking shifts, inconsistent model
performance among SEA languages, and the importance of human-curated datasets
for low-resource languages like Burmese.

</details>


### [27] [What do Speech Foundation Models Learn? Analysis and Applications](https://arxiv.org/abs/2508.12255)
*Ankita Pasad*

Main category: cs.CL

TL;DR: This thesis analyzes speech foundation models (SFMs) using statistical tools and introduces new datasets for spoken language understanding (SLU) tasks, demonstrating that SFMs can improve performance on these tasks.


<details>
  <summary>Details</summary>
Motivation: Although the zoo of SFMs continues to grow, our understanding of the knowledge they acquire lags behind.The limited exploration of SLU is primarily due to a lack of relevant datasets.

Method: a lightweight analysis framework using statistical tools and training-free tasks to investigate the acoustic and linguistic knowledge encoded in SFM layers; SFM-based approaches for NER and NEL

Result: analytical insights have concrete implications for downstream task performance; end-to-end (E2E) models leveraging SFMs can surpass traditional cascaded (speech recognition followed by a text model) approaches.

Conclusion: This thesis tackles previously unanswered questions about SFMs, providing tools and datasets to further our understanding and to enable the community to make informed design choices for future model development and adoption.

Abstract: Speech foundation models (SFMs) are designed to serve as general-purpose
representations for a wide range of speech-processing tasks. The last five
years have seen an influx of increasingly successful self-supervised and
supervised pre-trained models with impressive performance on various downstream
tasks.
  Although the zoo of SFMs continues to grow, our understanding of the
knowledge they acquire lags behind. This thesis presents a lightweight analysis
framework using statistical tools and training-free tasks to investigate the
acoustic and linguistic knowledge encoded in SFM layers. We conduct a
comparative study across multiple SFMs and statistical tools. Our study also
shows that the analytical insights have concrete implications for downstream
task performance.
  The effectiveness of an SFM is ultimately determined by its performance on
speech applications. Yet it remains unclear whether the benefits extend to
spoken language understanding (SLU) tasks that require a deeper understanding
than widely studied ones, such as speech recognition. The limited exploration
of SLU is primarily due to a lack of relevant datasets. To alleviate that, this
thesis contributes tasks, specifically spoken named entity recognition (NER)
and named entity localization (NEL), to the Spoken Language Understanding
Evaluation benchmark. We develop SFM-based approaches for NER and NEL, and find
that end-to-end (E2E) models leveraging SFMs can surpass traditional cascaded
(speech recognition followed by a text model) approaches. Further, we evaluate
E2E SLU models across SFMs and adaptation strategies to assess the impact on
task performance.
  Collectively, this thesis tackles previously unanswered questions about SFMs,
providing tools and datasets to further our understanding and to enable the
community to make informed design choices for future model development and
adoption.

</details>


### [28] [Structuring the Unstructured: A Systematic Review of Text-to-Structure Generation for Agentic AI with a Universal Evaluation Framework](https://arxiv.org/abs/2508.12257)
*Zheye Deng,Chunkit Chan,Tianshi Zheng,Wei Fan,Weiqi Wang,Yangqiu Song*

Main category: cs.CL

TL;DR: systematic review examines text-to-structure techniques and the encountered challenges, evaluates current datasets and assessment criteria, and outlines potential directions for future research


<details>
  <summary>Details</summary>
Motivation: The evolution of AI systems toward agentic operation and context-aware retrieval necessitates transforming unstructured text into structured formats

Method: examines text-to-structure techniques

Result: evaluates current datasets and assessment criteria, and outlines potential directions for future research

Conclusion: establishes text-to-structure as foundational infrastructure for next-generation AI systems

Abstract: The evolution of AI systems toward agentic operation and context-aware
retrieval necessitates transforming unstructured text into structured formats
like tables, knowledge graphs, and charts. While such conversions enable
critical applications from summarization to data mining, current research lacks
a comprehensive synthesis of methodologies, datasets, and metrics. This
systematic review examines text-to-structure techniques and the encountered
challenges, evaluates current datasets and assessment criteria, and outlines
potential directions for future research. We also introduce a universal
evaluation framework for structured outputs, establishing text-to-structure as
foundational infrastructure for next-generation AI systems.

</details>


### [29] [Fast, Slow, and Tool-augmented Thinking for LLMs: A Review](https://arxiv.org/abs/2508.12265)
*Xinda Jia,Jinpeng Li,Zezhong Wang,Jingjing Li,Xingshan Zeng,Yasheng Wang,Weinan Zhang,Yong Yu,Weiwen Liu*

Main category: cs.CL

TL;DR: This paper proposes a taxonomy of LLM reasoning strategies and surveys recent work on adaptive reasoning in LLMs. It also highlights open challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: Effective reasoning in real-world tasks requires adapting the reasoning strategy to the demands of the problem.

Method: This paper systematically survey recent work on adaptive reasoning in LLMs and categorize methods based on key decision factors.

Result: This paper propose a novel taxonomy of LLM reasoning strategies along two knowledge boundaries: a fast/slow boundary separating intuitive from deliberative processes, and an internal/external boundary distinguishing reasoning grounded in the model's parameters from reasoning augmented by external tools.

Conclusion: This paper concludes by highlighting open challenges and future directions toward more adaptive, efficient, and reliable LLMs.

Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in
reasoning across diverse domains. However, effective reasoning in real-world
tasks requires adapting the reasoning strategy to the demands of the problem,
ranging from fast, intuitive responses to deliberate, step-by-step reasoning
and tool-augmented thinking. Drawing inspiration from cognitive psychology, we
propose a novel taxonomy of LLM reasoning strategies along two knowledge
boundaries: a fast/slow boundary separating intuitive from deliberative
processes, and an internal/external boundary distinguishing reasoning grounded
in the model's parameters from reasoning augmented by external tools. We
systematically survey recent work on adaptive reasoning in LLMs and categorize
methods based on key decision factors. We conclude by highlighting open
challenges and future directions toward more adaptive, efficient, and reliable
LLMs.

</details>


### [30] [The Self-Execution Benchmark: Measuring LLMs' Attempts to Overcome Their Lack of Self-Execution](https://arxiv.org/abs/2508.12277)
*Elon Ezra,Ariel Weizman,Amos Azaria*

Main category: cs.CL

TL;DR: LLM无法很好地预测自己的行为，更大的模型也无法解决这个问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）通常在其知识或推理能力的测试任务中进行评估。在本文中，我们探索了一种不同类型的评估：LLM是否可以预测其自身响应的各个方面。

Method: 引入了自我执行基准，衡量模型预测其输出属性的能力。

Result: 模型通常在此基准测试中表现不佳，并且模型大小或功能的增加并不一定会带来更好的性能。

Conclusion: LLMs表现不佳，模型大小或能力的增加并不一定带来更好的性能。这些结果表明LLM在表示和推理自身行为方面存在根本限制。

Abstract: Large language models (LLMs) are commonly evaluated on tasks that test their
knowledge or reasoning abilities. In this paper, we explore a different type of
evaluation: whether an LLM can predict aspects of its own responses. Since LLMs
lack the ability to execute themselves, we introduce the Self-Execution
Benchmark, which measures a model's ability to anticipate properties of its
output, such as whether a question will be difficult for it, whether it will
refuse to answer, or what kinds of associations it is likely to produce. Our
experiments show that models generally perform poorly on this benchmark, and
that increased model size or capability does not consistently lead to better
performance. These results suggest a fundamental limitation in how LLMs
represent and reason about their own behavior.

</details>


### [31] [Legal$Δ$: Enhancing Legal Reasoning in LLMs via Reinforcement Learning with Chain-of-Thought Guided Information Gain](https://arxiv.org/abs/2508.12281)
*Xin Dai,Buqiang Xu,Zhenghao Liu,Yukun Yan,Huiyuan Xie,Xiaoyuan Yi,Shuo Wang,Ge Yu*

Main category: cs.CL

TL;DR: LegalΔ是一种强化学习框架，旨在通过思维链引导的信息增益来增强法律推理。


<details>
  <summary>Details</summary>
Motivation: 现有的法律法学硕士仍然难以生成可靠且可解释的推理过程。他们通常通过产生没有明确的多步骤推理的直接答案来默认为快速思考行为，从而限制了他们在需要严格论证的复杂法律场景中的有效性。

Method: LegalΔ 采用双模式输入设置（包括直接回答和推理增强模式），并最大化它们之间的信息增益。LegalΔ 遵循两阶段方法：（1）从强大的大型推理模型 (LRM) DeepSeek-R1 中提取潜在的推理能力，以及 (2) 通过差异比较细化推理质量，并结合评估结构连贯性和法律领域特异性的多维奖励机制。

Result: 在多个法律推理任务上的实验结果表明，LegalΔ 在准确性和可解释性方面优于强大的基线。

Conclusion: LegalΔ在准确性和可解释性方面优于强大的基线，并且持续产生更强大和可信的法律判断，而无需依赖标记的偏好数据。

Abstract: Legal Artificial Intelligence (LegalAI) has achieved notable advances in
automating judicial decision-making with the support of Large Language Models
(LLMs). However, existing legal LLMs still struggle to generate reliable and
interpretable reasoning processes. They often default to fast-thinking behavior
by producing direct answers without explicit multi-step reasoning, limiting
their effectiveness in complex legal scenarios that demand rigorous
justification. To address this challenge, we propose Legal$\Delta$, a
reinforcement learning framework designed to enhance legal reasoning through
chain-of-thought guided information gain. During training, Legal$\Delta$
employs a dual-mode input setup-comprising direct answer and
reasoning-augmented modes-and maximizes the information gain between them. This
encourages the model to acquire meaningful reasoning patterns rather than
generating superficial or redundant explanations. Legal$\Delta$ follows a
two-stage approach: (1) distilling latent reasoning capabilities from a
powerful Large Reasoning Model (LRM), DeepSeek-R1, and (2) refining reasoning
quality via differential comparisons, combined with a multidimensional reward
mechanism that assesses both structural coherence and legal-domain specificity.
Experimental results on multiple legal reasoning tasks demonstrate that
Legal$\Delta$ outperforms strong baselines in both accuracy and
interpretability. It consistently produces more robust and trustworthy legal
judgments without relying on labeled preference data. All code and data will be
released at https://github.com/NEUIR/LegalDelta.

</details>


### [32] [Incorporating Legal Logic into Deep Learning: An Intelligent Approach to Probation Prediction](https://arxiv.org/abs/2508.12286)
*Qinghua Wang,Xu Zhang,Lingyan Yang,Rui Shao,Bonan Wang,Fang Wang,Cunquan Qu*

Main category: cs.CL

TL;DR: 提出了一种将法律逻辑融入深度学习模型的缓刑预测新方法，并在缓刑数据集上的实验表明，MT-DT 模型优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前的智能司法助理系统（IJAS）缺乏专门的缓刑预测方法，并且对影响缓刑资格的潜在因素的研究仍然有限。此外，缓刑资格需要对犯罪情况和悔恨进行全面分析。IJAS 中的许多现有研究主要依赖于数据驱动的方法，而这些方法通常忽略了司法决策背后的法律逻辑。

Method: 提出了一种新颖的方法，该方法将法律逻辑集成到深度学习模型中，以进行缓刑预测，并在三个不同的阶段实施。

Result: MT-DT 模型优于基线模型

Conclusion: MT-DT 模型在缓刑数据集上优于基线模型，并且对底层法律逻辑的分析进一步验证了所提出方法的有效性。

Abstract: Probation is a crucial institution in modern criminal law, embodying the
principles of fairness and justice while contributing to the harmonious
development of society. Despite its importance, the current Intelligent
Judicial Assistant System (IJAS) lacks dedicated methods for probation
prediction, and research on the underlying factors influencing probation
eligibility remains limited. In addition, probation eligibility requires a
comprehensive analysis of both criminal circumstances and remorse. Much of the
existing research in IJAS relies primarily on data-driven methodologies, which
often overlooks the legal logic underpinning judicial decision-making. To
address this gap, we propose a novel approach that integrates legal logic into
deep learning models for probation prediction, implemented in three distinct
stages. First, we construct a specialized probation dataset that includes fact
descriptions and probation legal elements (PLEs). Second, we design a distinct
probation prediction model named the Multi-Task Dual-Theory Probation
Prediction Model (MT-DT), which is grounded in the legal logic of probation and
the \textit{Dual-Track Theory of Punishment}. Finally, our experiments on the
probation dataset demonstrate that the MT-DT model outperforms baseline models,
and an analysis of the underlying legal logic further validates the
effectiveness of the proposed approach.

</details>


### [33] [CarelessWhisper: Turning Whisper into a Causal Streaming Model](https://arxiv.org/abs/2508.12301)
*Tomer Krichli,Bhiksha Raj,Joseph Keshet*

Main category: cs.CL

TL;DR: 提出了一种低延迟流式ASR模型，通过微调Transformer结构实现，并在性能和效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的Automatic Speech Recognition (ASR)模型，如OpenAI Whisper和NVIDIA Canary，在离线转录方面表现出色，但由于其架构和训练方法的限制，不适用于流式(在线或实时)转录。

Method: 通过微调encoder和decoder，使用Low-Rank Adaptation (LoRA)和一个弱对齐数据集，将现有的(非因果) encoder修改为因果encoder。

Result: 在低延迟块大小（小于300毫秒）的实验表明，该微调模型在大多数情况下优于现有的非微调流式方法，同时使用较低的复杂度。此外，该研究的训练过程产生了更好的对齐，从而能够使用一种简单的方法提取单词级时间戳。

Conclusion: 该研究提出了一种将transformer encoder-decoder模型转化为低延迟流式模型的方法，并通过实验验证了其在低延迟块大小上的优越性能，同时提供代码和模型以支持未来的研究。

Abstract: Automatic Speech Recognition (ASR) has seen remarkable progress, with models
like OpenAI Whisper and NVIDIA Canary achieving state-of-the-art (SOTA)
performance in offline transcription. However, these models are not designed
for streaming (online or real-time) transcription, due to limitations in their
architecture and training methodology. We propose a method to turn the
transformer encoder-decoder model into a low-latency streaming model that is
careless about future context. We present an analysis explaining why it is not
straightforward to convert an encoder-decoder transformer to a low-latency
streaming model. Our proposed method modifies the existing (non-causal) encoder
to a causal encoder by fine-tuning both the encoder and decoder using Low-Rank
Adaptation (LoRA) and a weakly aligned dataset. We then propose an updated
inference mechanism that utilizes the fine-tune causal encoder and decoder to
yield greedy and beam-search decoding, and is shown to be locally optimal.
Experiments on low-latency chunk sizes (less than 300 msec) show that our
fine-tuned model outperforms existing non-fine-tuned streaming approaches in
most cases, while using a lower complexity. Additionally, we observe that our
training process yields better alignment, enabling a simple method for
extracting word-level timestamps. We release our training and inference code,
along with the fine-tuned models, to support further research and development
in streaming ASR.

</details>


### [34] [Consensus or Conflict? Fine-Grained Evaluation of Conflicting Answers in Question-Answering](https://arxiv.org/abs/2508.12355)
*Eviatar Nachshoni,Arie Cattan,Shmuel Amar,Ori Shapira,Ido Dagan*

Main category: cs.CL

TL;DR: The paper introduces NATCONFQA, a new benchmark for realistic, conflict-aware Multi-Answer Question Answering (MAQA), and evaluates eight high-end LLMs on it, revealing their fragility in handling conflicts.


<details>
  <summary>Details</summary>
Motivation: Multi-Answer Question Answering (MAQA), where a question may have several valid answers, remains challenging because traditional QA settings often assume consistency across evidences, but MAQA can involve conflicting answers. Constructing datasets that reflect such conflicts is costly and labor-intensive, while existing benchmarks often rely on synthetic data, restrict the task to yes/no questions, or apply unverified automated annotation.

Method: A novel cost-effective methodology for leveraging fact-checking datasets to construct NATCONFQA, a new benchmark for realistic, conflict-aware MAQA, enriched with detailed conflict labels, for all answer pairs.

Result: Introduce NATCONFQA, a new benchmark for realistic, conflict-aware MAQA, enriched with detailed conflict labels, for all answer pairs.

Conclusion: Eight high-end LLMs are evaluated on NATCONFQA, revealing their fragility in handling various types of conflicts and the flawed strategies they employ to resolve them.

Abstract: Large Language Models (LLMs) have demonstrated strong performance in question
answering (QA) tasks. However, Multi-Answer Question Answering (MAQA), where a
question may have several valid answers, remains challenging. Traditional QA
settings often assume consistency across evidences, but MAQA can involve
conflicting answers. Constructing datasets that reflect such conflicts is
costly and labor-intensive, while existing benchmarks often rely on synthetic
data, restrict the task to yes/no questions, or apply unverified automated
annotation. To advance research in this area, we extend the conflict-aware MAQA
setting to require models not only to identify all valid answers, but also to
detect specific conflicting answer pairs, if any. To support this task, we
introduce a novel cost-effective methodology for leveraging fact-checking
datasets to construct NATCONFQA, a new benchmark for realistic, conflict-aware
MAQA, enriched with detailed conflict labels, for all answer pairs. We evaluate
eight high-end LLMs on NATCONFQA, revealing their fragility in handling various
types of conflicts and the flawed strategies they employ to resolve them.

</details>


### [35] [ReaLM: Reflection-Enhanced Autonomous Reasoning with Small Language Models](https://arxiv.org/abs/2508.12387)
*Yuanfeng Xu,Zehui Dai,Jian Liang,Jiapeng Guan,Guangrun Wang,Liang Lin,Xiaohui Lv*

Main category: cs.CL

TL;DR: 本文提出了一种强化学习框架ReaLM，用于增强SLM的推理能力，提高自主性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 小语言模型(slm)是大型语言模型(llm)的一种经济有效的替代品，但由于其容量有限，并且在多步推理过程中容易产生错误或不一致的答案，因此常常难以进行复杂的推理。

Method: 强化学习框架，包含Multi-Route Process Verification (MRPV) 和 Enabling Autonomy via Asymptotic Induction (EAAI)

Result: ReaLM在垂直和一般推理任务上显著提高了SLM的性能。

Conclusion: ReaLM显著提高了SLM在上述(1)-(3)方面的性能。

Abstract: Small Language Models (SLMs) are a cost-effective alternative to Large
Language Models (LLMs), but often struggle with complex reasoning due to their
limited capacity and a tendency to produce mistakes or inconsistent answers
during multi-step reasoning. Existing efforts have improved SLM performance,
but typically at the cost of one or more of three key aspects: (1) reasoning
capability, due to biased supervision that filters out negative reasoning paths
and limits learning from errors; (2) autonomy, due to over-reliance on
externally generated reasoning signals; and (3) generalization, which suffers
when models overfit to teacher-specific patterns. In this paper, we introduce
ReaLM, a reinforcement learning framework for robust and self-sufficient
reasoning in vertical domains. To enhance reasoning capability, we propose
Multi-Route Process Verification (MRPV), which contrasts both positive and
negative reasoning paths to extract decisive patterns. To reduce reliance on
external guidance and improve autonomy, we introduce Enabling Autonomy via
Asymptotic Induction (EAAI), a training strategy that gradually fades external
signals. To improve generalization, we apply guided chain-of-thought
distillation to encode domain-specific rules and expert knowledge into SLM
parameters, making them part of what the model has learned. Extensive
experiments on both vertical and general reasoning tasks demonstrate that ReaLM
significantly improves SLM performance across aspects (1)-(3) above.

</details>


### [36] [MedKGent: A Large Language Model Agent Framework for Constructing Temporally Evolving Medical Knowledge Graph](https://arxiv.org/abs/2508.12393)
*Duzhen Zhang,Zixiao Wang,Zhong-Zhi Li,Yahan Yu,Shuncheng Jia,Jiahua Dong,Haotian Xu,Xing Wu,Yingying Zhang,Tielin Zhang,Jie Yang,Xiuying Chen,Le Song*

Main category: cs.CL

TL;DR: MedKGent, a LLM agent framework, constructs a temporally evolving medical KG from PubMed abstracts, achieving high accuracy and improving medical question answering and drug repurposing.


<details>
  <summary>Details</summary>
Motivation: current KG construction methods often rely on supervised pipelines with limited generalizability or naively aggregate outputs from Large Language Models (LLMs), treating biomedical corpora as static and ignoring the temporal dynamics and contextual uncertainty of evolving knowledge

Method: introduce MedKGent, a LLM agent framework for constructing temporally evolving medical KGs. Leveraging over 10 million PubMed abstracts published between 1975 and 2023, we simulate the emergence of biomedical knowledge via a fine-grained daily time series. MedKGent incrementally builds the KG in a day-by-day manner using two specialized agents powered by the Qwen2.5-32B-Instruct model. The Extractor Agent identifies knowledge triples and assigns confidence scores via sampling-based estimation, which are used to filter low-confidence extractions and inform downstream processing. The Constructor Agent incrementally integrates the retained triples into a temporally evolving graph, guided by confidence scores and timestamps to reinforce recurring knowledge and resolve conflicts.

Result: Quality assessments by two SOTA LLMs and three domain experts demonstrate an accuracy approaching 90%, with strong inter-rater agreement. To evaluate downstream utility, we conduct RAG across seven medical question answering benchmarks using five leading LLMs, consistently observing significant improvements over non-augmented baselines.

Conclusion: The resulting KG contains 156,275 entities and 2,971,384 relational triples. Quality assessments by two SOTA LLMs and three domain experts demonstrate an accuracy approaching 90%, with strong inter-rater agreement. To evaluate downstream utility, we conduct RAG across seven medical question answering benchmarks using five leading LLMs, consistently observing significant improvements over non-augmented baselines. Case studies further demonstrate the KG's value in literature-based drug repurposing via confidence-aware causal inference.

Abstract: The rapid expansion of medical literature presents growing challenges for
structuring and integrating domain knowledge at scale. Knowledge Graphs (KGs)
offer a promising solution by enabling efficient retrieval, automated
reasoning, and knowledge discovery. However, current KG construction methods
often rely on supervised pipelines with limited generalizability or naively
aggregate outputs from Large Language Models (LLMs), treating biomedical
corpora as static and ignoring the temporal dynamics and contextual uncertainty
of evolving knowledge. To address these limitations, we introduce MedKGent, a
LLM agent framework for constructing temporally evolving medical KGs.
Leveraging over 10 million PubMed abstracts published between 1975 and 2023, we
simulate the emergence of biomedical knowledge via a fine-grained daily time
series. MedKGent incrementally builds the KG in a day-by-day manner using two
specialized agents powered by the Qwen2.5-32B-Instruct model. The Extractor
Agent identifies knowledge triples and assigns confidence scores via
sampling-based estimation, which are used to filter low-confidence extractions
and inform downstream processing. The Constructor Agent incrementally
integrates the retained triples into a temporally evolving graph, guided by
confidence scores and timestamps to reinforce recurring knowledge and resolve
conflicts. The resulting KG contains 156,275 entities and 2,971,384 relational
triples. Quality assessments by two SOTA LLMs and three domain experts
demonstrate an accuracy approaching 90\%, with strong inter-rater agreement. To
evaluate downstream utility, we conduct RAG across seven medical question
answering benchmarks using five leading LLMs, consistently observing
significant improvements over non-augmented baselines. Case studies further
demonstrate the KG's value in literature-based drug repurposing via
confidence-aware causal inference.

</details>


### [37] [Extracting Post-Acute Sequelae of SARS-CoV-2 Infection Symptoms from Clinical Notes via Hybrid Natural Language Processing](https://arxiv.org/abs/2508.12405)
*Zilong Bai,Zihan Xu,Cong Sun,Chengxi Zang,H. Timothy Bunnell,Catherine Sinfield,Jacqueline Rutter,Aaron Thomas Martinez,L. Charles Bailey,Mark Weiner,Thomas R. Campion,Thomas Carton,Christopher B. Forrest,Rainu Kaushal,Fei Wang,Yifan Peng*

Main category: cs.CL

TL;DR: 本研究开发了一种用于从临床笔记中提取 PASC 症状和进行断言检测的混合自然语言处理管道，该管道在 PASC 诊断方面表现出效率和有效性。


<details>
  <summary>Details</summary>
Motivation: 由于 COVID-19 后急性期后遗症 (PASC) 的无数症状在长期和可变的时间间隔内演变，因此准确有效地诊断 PASC 仍然具有挑战性。

Method: 我们开发了一种混合自然语言处理管道，该管道集成了基于规则的命名实体识别与基于 BERT 的断言检测模块，用于从临床笔记中提取 PASC 症状和进行断言检测。

Result: 在一个站点的内部验证中，我们实现了 0.82 的平均 F1 分数，在 10 个站点的外部验证中，我们实现了 0.76 的平均 F1 分数以进行断言检测。我们的管道平均以 2.448±0.812 秒的速度处理每个音符。Spearman 相关性测试表明，正面提及的 ρ >0.83，负面提及的 ρ >0.72，两者均具有 P <0.0001。

Conclusion: 该模型有效且高效，并具有改善 PASC 诊断的潜力。

Abstract: Accurately and efficiently diagnosing Post-Acute Sequelae of COVID-19 (PASC)
remains challenging due to its myriad symptoms that evolve over long- and
variable-time intervals. To address this issue, we developed a hybrid natural
language processing pipeline that integrates rule-based named entity
recognition with BERT-based assertion detection modules for PASC-symptom
extraction and assertion detection from clinical notes. We developed a
comprehensive PASC lexicon with clinical specialists. From 11 health systems of
the RECOVER initiative network across the U.S., we curated 160 intake progress
notes for model development and evaluation, and collected 47,654 progress notes
for a population-level prevalence study. We achieved an average F1 score of
0.82 in one-site internal validation and 0.76 in 10-site external validation
for assertion detection. Our pipeline processed each note at $2.448\pm 0.812$
seconds on average. Spearman correlation tests showed $\rho >0.83$ for positive
mentions and $\rho >0.72$ for negative ones, both with $P <0.0001$. These
demonstrate the effectiveness and efficiency of our models and their potential
for improving PASC diagnosis.

</details>


### [38] [ZigzagAttention: Efficient Long-Context Inference with Exclusive Retrieval and Streaming Heads](https://arxiv.org/abs/2508.12407)
*Zhuorui Liu,Chen Zhang,Dawei Song*

Main category: cs.CL

TL;DR: This paper introduces ZigzagAttention, a method that reduces latency and maintains comparable performance in large language models by grouping retrieval or streaming heads into unique layers.


<details>
  <summary>Details</summary>
Motivation: Handling long context has become one of the vital abilities in LLMs, but it increases the consumption of KV cache. Previous work aimed to optimize the memory footprint of KV cache by identifying and waiving the KV cache in streaming heads, but it may bring extra latency on accessing and indexing tensors.

Method: The paper designs a criterion that enforces exclusively retrieval or streaming heads gathered in one unique layer.

Result: ZigzagAttention is competitive among considered baselines owing to reduced latency and comparable performance.

Conclusion: The paper introduces ZigzagAttention, a method that reduces latency and maintains comparable performance by grouping retrieval or streaming heads into unique layers, thereby eliminating extra latency.

Abstract: With the rapid development of large language models (LLMs), handling long
context has become one of the vital abilities in LLMs. Such long-context
ability is accompanied by difficulties in deployment, especially due to the
increased consumption of KV cache. There is certain work aiming to optimize the
memory footprint of KV cache, inspired by the observation that attention heads
can be categorized into retrieval heads that are of great significance and
streaming heads that are of less significance. Typically, identifying the
streaming heads and and waiving the KV cache in the streaming heads would
largely reduce the overhead without hurting the performance that much. However,
since employing both retrieval and streaming heads in one layer decomposes one
large round of attention computation into two small ones, it may unexpectedly
bring extra latency on accessing and indexing tensors. Based on this intuition,
we impose an important improvement to the identification process of retrieval
and streaming heads, in which we design a criterion that enforces exclusively
retrieval or streaming heads gathered in one unique layer. In this way, we
further eliminate the extra latency and only incur negligible performance
degradation. Our method named \textsc{ZigzagAttention} is competitive among
considered baselines owing to reduced latency and comparable performance.

</details>


### [39] [The Cultural Gene of Large Language Models: A Study on the Impact of Cross-Corpus Training on Model Values and Biases](https://arxiv.org/abs/2508.12411)
*Emanuel Z. Fenech-Borg,Tilen P. Meznaric-Kos,Milica D. Lekovic-Bojovic,Arni J. Hentze-Djurhuus*

Main category: cs.CL

TL;DR: LLMs inherit cultural values from their training data, leading to different cultural orientations in Western-centric vs. Eastern-centric models. This highlights the need for culturally aware evaluation and deployment.


<details>
  <summary>Details</summary>
Motivation: The cultural and ethical assumptions of LLMs remain underexplored. Proposes the notion of a cultural gene -- a systematic value orientation that LLMs inherit from their training corpora.

Method: Compared a Western-centric model (GPT-4) and an Eastern-centric model (ERNIE Bot) using a Cultural Probe Dataset (CPD) of 200 prompts targeting Individualism-Collectivism (IDV) and Power Distance (PDI). Computed a Cultural Alignment Index (CAI) against Hofstede's national scores. Qualitative analyses of dilemma resolution and authority-related judgments were conducted.

Result: GPT-4 exhibits individualistic and low-power-distance tendencies, aligning more closely with the USA. ERNIE Bot shows collectivistic and higher-power-distance tendencies, aligning more closely with China. Significant and consistent divergence across both dimensions was observed.

Conclusion: LLMs function as statistical mirrors of their cultural corpora. Culturally aware evaluation and deployment are needed to avoid algorithmic cultural hegemony.

Abstract: Large language models (LLMs) are deployed globally, yet their underlying
cultural and ethical assumptions remain underexplored. We propose the notion of
a "cultural gene" -- a systematic value orientation that LLMs inherit from
their training corpora -- and introduce a Cultural Probe Dataset (CPD) of 200
prompts targeting two classic cross-cultural dimensions:
Individualism-Collectivism (IDV) and Power Distance (PDI). Using standardized
zero-shot prompts, we compare a Western-centric model (GPT-4) and an
Eastern-centric model (ERNIE Bot). Human annotation shows significant and
consistent divergence across both dimensions. GPT-4 exhibits individualistic
and low-power-distance tendencies (IDV score approx 1.21; PDI score approx
-1.05), while ERNIE Bot shows collectivistic and higher-power-distance
tendencies (IDV approx -0.89; PDI approx 0.76); differences are statistically
significant (p < 0.001). We further compute a Cultural Alignment Index (CAI)
against Hofstede's national scores and find GPT-4 aligns more closely with the
USA (e.g., IDV CAI approx 0.91; PDI CAI approx 0.88) whereas ERNIE Bot aligns
more closely with China (IDV CAI approx 0.85; PDI CAI approx 0.81). Qualitative
analyses of dilemma resolution and authority-related judgments illustrate how
these orientations surface in reasoning. Our results support the view that LLMs
function as statistical mirrors of their cultural corpora and motivate
culturally aware evaluation and deployment to avoid algorithmic cultural
hegemony.

</details>


### [40] [Uncovering Emergent Physics Representations Learned In-Context by Large Language Models](https://arxiv.org/abs/2508.12448)
*Yeongwoo Song,Jaeyong Bae,Dong-Kyum Kim,Hawoong Jeong*

Main category: cs.CL

TL;DR: 大型语言模型可以通过上下文学习物理知识。


<details>
  <summary>Details</summary>
Motivation: 确定大型语言模型中允许在不同任务类别中成功进行上下文学习的精确机制或内部结构仍然难以捉摸。基于物理的任务为探究这一挑战提供了一个有希望的试验平台。

Method: 使用稀疏自动编码器（SAE）分析模型的残差流激活。

Result: 上下文中的动态预测性能随着更长的输入上下文而提高；SAE 捕获的特征与关键物理变量（如能量）相关。

Conclusion: 有意义的物理概念在上下文学习过程中被编码在大型语言模型中。

Abstract: Large language models (LLMs) exhibit impressive in-context learning (ICL)
abilities, enabling them to solve wide range of tasks via textual prompts
alone. As these capabilities advance, the range of applicable domains continues
to expand significantly. However, identifying the precise mechanisms or
internal structures within LLMs that allow successful ICL across diverse,
distinct classes of tasks remains elusive. Physics-based tasks offer a
promising testbed for probing this challenge. Unlike synthetic sequences such
as basic arithmetic or symbolic equations, physical systems provide
experimentally controllable, real-world data based on structured dynamics
grounded in fundamental principles. This makes them particularly suitable for
studying the emergent reasoning behaviors of LLMs in a realistic yet tractable
setting. Here, we mechanistically investigate the ICL ability of LLMs,
especially focusing on their ability to reason about physics. Using a dynamics
forecasting task in physical systems as a proxy, we evaluate whether LLMs can
learn physics in context. We first show that the performance of dynamics
forecasting in context improves with longer input contexts. To uncover how such
capability emerges in LLMs, we analyze the model's residual stream activations
using sparse autoencoders (SAEs). Our experiments reveal that the features
captured by SAEs correlate with key physical variables, such as energy. These
findings demonstrate that meaningful physical concepts are encoded within LLMs
during in-context learning. In sum, our work provides a novel case study that
broadens our understanding of how LLMs learn in context.

</details>


### [41] [M3PO: Multimodal-Model-Guided Preference Optimization for Visual Instruction Following](https://arxiv.org/abs/2508.12458)
*Ruirui Gao,Emily Johnson,Bowen Tan,Yanfei Qian*

Main category: cs.CL

TL;DR: M3PO 是一种通过整合多模态对齐分数和模型置信度来选择高质量偏好样本对，从而增强 LVLM 在视觉指令跟随方面能力的方法。


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法难以有效利用模型自身的生成空间来识别信息量大的“硬负例”样本。

Method: Multimodal-Model-Guided Preference Optimization (M3PO)

Result: M3PO 是一种新颖且数据高效的方法，旨在增强 LVLM 在视觉指令跟随方面的能力。

Conclusion: M3PO在多模态指令跟随基准测试中始终优于强大的基线，包括 SFT、模拟 RLHF、vanilla DPO 和 RM-DPO。

Abstract: Large Vision-Language Models (LVLMs) hold immense potential for complex
multimodal instruction following, yet their development is often hindered by
the high cost and inconsistency of human annotation required for effective
fine-tuning and preference alignment. Traditional supervised fine-tuning (SFT)
and existing preference optimization methods like RLHF and DPO frequently
struggle to efficiently leverage the model's own generation space to identify
highly informative "hard negative" samples. To address these challenges, we
propose Multimodal-Model-Guided Preference Optimization (M3PO), a novel and
data-efficient method designed to enhance LVLMs' capabilities in visual
instruction following. M3PO intelligently selects the most "learning-valuable"
preference sample pairs from a diverse pool of LVLM-generated candidates. This
selection is driven by a sophisticated mechanism that integrates two crucial
signals: a Multimodal Alignment Score (MAS) to assess external quality and the
model's Self-Consistency / Confidence (log-probability) to gauge internal
belief. These are combined into a novel M3P-Score, which specifically
identifies preferred responses and challenging dispreferred responses that the
model might confidently generate despite being incorrect. These high-quality
preference pairs are then used for efficient Direct Preference Optimization
(DPO) fine-tuning on base LVLMs like LLaVA-1.5 (7B/13B) using LoRA. Our
extensive experiments demonstrate that M3PO consistently outperforms strong
baselines, including SFT, simulated RLHF, vanilla DPO, and RM-DPO, across a
comprehensive suite of multimodal instruction following benchmarks (MME-Bench,
POPE, IFT, Human Pref. Score).

</details>


### [42] [LoraxBench: A Multitask, Multilingual Benchmark Suite for 20 Indonesian Languages](https://arxiv.org/abs/2508.12459)
*Alham Fikri Aji,Trevor Cohn*

Main category: cs.CL

TL;DR: Introduce LoraxBench, a benchmark that focuses on low-resource languages of Indonesia and covers 6 diverse tasks, and evaluate a diverse set of multilingual and region-focused LLMs and found that this benchmark is challenging.


<details>
  <summary>Details</summary>
Motivation: Indonesia is behind in terms of NLP progress

Method: Introduce LoraxBench, a benchmark that focuses on low-resource languages of Indonesia and covers 6 diverse tasks

Result: Evaluate a diverse set of multilingual and region-focused LLMs and found that this benchmark is challenging

Conclusion: There is a visible discrepancy between performance in Indonesian and other languages, especially the low-resource ones. There is no clear lead when using a region-specific model as opposed to the general multilingual model. Lastly, a change in register affects model performance, especially with registers not commonly found in social media.

Abstract: As one of the world's most populous countries, with 700 languages spoken,
Indonesia is behind in terms of NLP progress. We introduce LoraxBench, a
benchmark that focuses on low-resource languages of Indonesia and covers 6
diverse tasks: reading comprehension, open-domain QA, language inference,
causal reasoning, translation, and cultural QA. Our dataset covers 20
languages, with the addition of two formality registers for three languages. We
evaluate a diverse set of multilingual and region-focused LLMs and found that
this benchmark is challenging. We note a visible discrepancy between
performance in Indonesian and other languages, especially the low-resource
ones. There is no clear lead when using a region-specific model as opposed to
the general multilingual model. Lastly, we show that a change in register
affects model performance, especially with registers not commonly found in
social media, such as high-level politeness `Krama' Javanese.

</details>


### [43] [Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI's Latest Open Source Models](https://arxiv.org/abs/2508.12461)
*Ziqian Bi,Keyu Chen,Chiung-Yi Tseng,Danyang Zhang,Tianyang Wang,Hongying Luo,Lu Chen,Junming Huang,Jibin Guan,Junfeng Hao,Junhao Song*

Main category: cs.CL

TL;DR: GPT-OSS-20B 优于 GPT-OSS-120B，但两者在开源 LLM 中都处于中等水平，代码生成是优势，多语言是弱势。


<details>
  <summary>Details</summary>
Motivation: OpenAI 发布了 GPT-OSS 模型，这是自 2019 年 GPT-2 以来首个开源权重的大型语言模型，包括两种具有 120B 和 20B 参数的专家混合架构。

Method: 在涵盖一般知识、数学推理、代码生成、多语言理解和对话能力的十个基准上，针对六个当代开源大型语言模型（参数范围从 147 亿到 2350 亿）评估了两种变体。所有模型均在标准化推理设置下的未量化形式下进行了测试，并使用 McNemars 检验和效应大小分析进行了统计验证。

Result: gpt-oss-20B 在多项基准测试中始终优于 gpt-oss-120B，尽管每次响应所需的内存和能源大大减少。 这两个模型在当前开源环境中表现出中等的整体性能，在代码生成方面具有相对优势，在多语言任务方面表现出明显的劣势。

Conclusion: 稀疏架构中的扩展可能不会产生成比例的性能提升，因此需要进一步研究优化策略，并为未来的开源部署提供更有效的模型选择。

Abstract: In August 2025, OpenAI released GPT-OSS models, its first open weight large
language models since GPT-2 in 2019, comprising two mixture of experts
architectures with 120B and 20B parameters. We evaluated both variants against
six contemporary open source large language models ranging from 14.7B to 235B
parameters, representing both dense and sparse designs, across ten benchmarks
covering general knowledge, mathematical reasoning, code generation,
multilingual understanding, and conversational ability. All models were tested
in unquantised form under standardised inference settings, with statistical
validation using McNemars test and effect size analysis. Results show that
gpt-oss-20B consistently outperforms gpt-oss-120B on several benchmarks, such
as HumanEval and MMLU, despite requiring substantially less memory and energy
per response. Both models demonstrate mid-tier overall performance within the
current open source landscape, with relative strength in code generation and
notable weaknesses in multilingual tasks. These findings provide empirical
evidence that scaling in sparse architectures may not yield proportional
performance gains, underscoring the need for further investigation into
optimisation strategies and informing more efficient model selection for future
open source deployments.

</details>


### [44] [The Structural Sources of Verb Meaning Revisited: Large Language Models Display Syntactic Bootstrapping](https://arxiv.org/abs/2508.12482)
*Xiaomeng Zhu,R. Thomas McCoy,Robert Frank*

Main category: cs.CL

TL;DR: 大型语言模型通过句法引导学习动词，类似于儿童的学习方式。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型是否表现出与儿童相似的句法引导行为，即利用动词出现的句法环境来学习其含义。

Method: 通过在经过扰动的数据集上训练RoBERTa和GPT-2，其中句法信息被消除。

Result: 当句法线索被移除时，模型对动词的表征会退化更多；心理动词的表征受到的负面影响比物理动词更大；名词的表征在共现关系被扭曲时受到的影响更大。

Conclusion: 大型语言模型在学习动词含义时表现出与儿童相似的句法引导现象，句法信息对于动词学习至关重要。

Abstract: Syntactic bootstrapping (Gleitman, 1990) is the hypothesis that children use
the syntactic environments in which a verb occurs to learn its meaning. In this
paper, we examine whether large language models exhibit a similar behavior. We
do this by training RoBERTa and GPT-2 on perturbed datasets where syntactic
information is ablated. Our results show that models' verb representation
degrades more when syntactic cues are removed than when co-occurrence
information is removed. Furthermore, the representation of mental verbs, for
which syntactic bootstrapping has been shown to be particularly crucial in
human verb learning, is more negatively impacted in such training regimes than
physical verbs. In contrast, models' representation of nouns is affected more
when co-occurrences are distorted than when syntax is distorted. In addition to
reinforcing the important role of syntactic bootstrapping in verb learning, our
results demonstrated the viability of testing developmental hypotheses on a
larger scale through manipulating the learning environments of large language
models.

</details>


### [45] [Mitigating Hallucinations in Large Language Models via Causal Reasoning](https://arxiv.org/abs/2508.12495)
*Yuangang Li,Yiqing Shen,Yi Nian,Jiechao Gao,Ziyi Wang,Chenxiao Yu,Shawn Li,Jie Wang,Xiyang Hu,Yue Zhao*

Main category: cs.CL

TL;DR: CDCR-SFT通过显式因果图建模提升LLM的因果推理能力，并有效减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(llm)表现出逻辑上不一致的幻觉，这些幻觉看起来是连贯的，但违反了推理原则。最近的研究表明，因果推理能力和这种幻觉之间存在反比关系。

Method: 提出了因果DAG构建和推理(CDCR-SFT)的监督微调框架，该框架训练LLM来显式地构建变量级的有向无环图(DAG)，然后在其上执行推理。

Result: 在8个任务上对4个llm进行的实验表明，CDCR-SFT提高了因果推理能力，在CLADDER上达到了最先进的95.33%的准确率(首次超过了94.8%的人类表现)，并在HaluEval上减少了10%的幻觉。

Conclusion: CDCR-SFT通过显式的因果结构建模，有效地减轻了LLM输出中的逻辑不一致性。

Abstract: Large language models (LLMs) exhibit logically inconsistent hallucinations
that appear coherent yet violate reasoning principles, with recent research
suggesting an inverse relationship between causal reasoning capabilities and
such hallucinations. However, existing reasoning approaches in LLMs, such as
Chain-of-Thought (CoT) and its graph-based variants, operate at the linguistic
token level rather than modeling the underlying causal relationships between
variables, lacking the ability to represent conditional independencies or
satisfy causal identification assumptions. To bridge this gap, we introduce
causal-DAG construction and reasoning (CDCR-SFT), a supervised fine-tuning
framework that trains LLMs to explicitly construct variable-level directed
acyclic graph (DAG) and then perform reasoning over it. Moreover, we present a
dataset comprising 25,368 samples (CausalDR), where each sample includes an
input question, explicit causal DAG, graph-based reasoning trace, and validated
answer. Experiments on four LLMs across eight tasks show that CDCR-SFT improves
the causal reasoning capability with the state-of-the-art 95.33% accuracy on
CLADDER (surpassing human performance of 94.8% for the first time) and reduces
the hallucination on HaluEval with 10% improvements. It demonstrates that
explicit causal structure modeling in LLMs can effectively mitigate logical
inconsistencies in LLM outputs. Code is available at
https://github.com/MrLYG/CDCR-SFT.

</details>


### [46] [CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection](https://arxiv.org/abs/2508.12535)
*Seonglae Cho,Zekun Wu,Adriano Koshiyama*

Main category: cs.CL

TL;DR: CorrSteer selects features by correlating sample correctness with SAE activations from generated tokens at inference time to improve task performance and automate the entire pipeline


<details>
  <summary>Details</summary>
Motivation: effectiveness in downstream steering tasks is limited by the requirement for contrastive datasets or large activation storage

Method: CorrSteer, which selects features by correlating sample correctness with SAE activations from generated tokens at inference time

Result: improved task performance on QA, bias mitigation, jailbreaking prevention, and reasoning benchmarks on Gemma 2 2B and LLaMA 3.1 8B, notably achieving a +4.1% improvement in MMLU performance and a +22.9% improvement in HarmBench with only 4000 samples

Conclusion: correlationbased selection is an effective and scalable approach for automated SAE steering across language model applications

Abstract: Sparse Autoencoders (SAEs) can extract interpretable features from large
language models (LLMs) without supervision. However, their effectiveness in
downstream steering tasks is limited by the requirement for contrastive
datasets or large activation storage. To address these limitations, we propose
CorrSteer, which selects features by correlating sample correctness with SAE
activations from generated tokens at inference time. This approach uses only
inference-time activations to extract more relevant features, thereby avoiding
spurious correlations. It also obtains steering coefficients from average
activations, automating the entire pipeline. Our method shows improved task
performance on QA, bias mitigation, jailbreaking prevention, and reasoning
benchmarks on Gemma 2 2B and LLaMA 3.1 8B, notably achieving a +4.1%
improvement in MMLU performance and a +22.9% improvement in HarmBench with only
4000 samples. Selected features demonstrate semantically meaningful patterns
aligned with each task's requirements, revealing the underlying capabilities
that drive performance. Our work establishes correlationbased selection as an
effective and scalable approach for automated SAE steering across language
model applications.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [47] [A Deep Learning-Based CCTV System for Automatic Smoking Detection in Fire Exit Zones](https://arxiv.org/abs/2508.11696)
*Sami Sadat,Mohammad Irtiza Hossain,Junaid Ahmed Sifat,Suhail Haque Rafi,Md. Waseq Alauddin Alvi,Md. Khalilur Rhaman*

Main category: cs.CV

TL;DR: Developed a custom YOLOv8 model for real-time smoking detection, which outperforms existing models on edge devices.


<details>
  <summary>Details</summary>
Motivation: Critical safety requirements in fire exit areas necessitate a real-time smoking detection system.

Method: Developed a custom model derived from YOLOv8 with added structures.

Result: The proposed model achieved a recall of 78.90% and mAP at 50 of 83.70%. Jetson Xavier NX processed data at 52 to 97 milliseconds per inference.

Conclusion: The custom YOLOv8 model is suitable for real-time smoking detection on edge devices.

Abstract: A deep learning real-time smoking detection system for CCTV surveillance of
fire exit areas is proposed due to critical safety requirements. The dataset
contains 8,124 images from 20 different scenarios along with 2,708 raw samples
demonstrating low-light areas. We evaluated three advanced object detection
models: YOLOv8, YOLOv11, and YOLOv12, followed by development of a custom model
derived from YOLOv8 with added structures for challenging surveillance
contexts. The proposed model outperformed the others, achieving a recall of
78.90 percent and mAP at 50 of 83.70 percent, delivering optimal object
detection across varied environments. Performance evaluation on multiple edge
devices using multithreaded operations showed the Jetson Xavier NX processed
data at 52 to 97 milliseconds per inference, establishing its suitability for
time-sensitive operations. This system offers a robust and adaptable platform
for monitoring public safety and enabling automatic regulatory compliance.

</details>


### [48] [Separating Knowledge and Perception with Procedural Data](https://arxiv.org/abs/2508.11697)
*Adrián Rodríguez-Muñoz,Manel Baradad,Phillip Isola,Antonio Torralba*

Main category: cs.CV

TL;DR: The paper shows that models trained on procedural data can perform comparably to those trained on real data in several vision tasks, particularly in fine-grained classification and zero-shot segmentation, using a visual memory approach.


<details>
  <summary>Details</summary>
Motivation: The research aims to achieve full compartmentalization with respect to real-world images while maintaining strong performance by training models on procedural data only.

Method: The study trains representation models using only procedural data and applies them to visual tasks using visual memory, an explicit database of reference image embeddings.

Result: The procedural model performs within 1% on NIGHTS visual similarity, outperforms by 8% and 15% on CUB200 and Flowers102 fine-grained classification, is within 10% on ImageNet-1K classification, and achieves an R^2 on COCO within 10% of models trained on real data. Dissimilar representations of object parts in procedural models explain the remaining performance gap.

Conclusion: Procedural models demonstrate strong performance in visual similarity, classification, and semantic segmentation tasks, achieving comparable or superior results to models trained on real-world data, especially in fine-grained classification and zero-shot segmentation.

Abstract: We train representation models with procedural data only, and apply them on
visual similarity, classification, and semantic segmentation tasks without
further training by using visual memory -- an explicit database of reference
image embeddings. Unlike prior work on visual memory, our approach achieves
full compartmentalization with respect to all real-world images while retaining
strong performance. Compared to a model trained on Places, our procedural model
performs within $1\%$ on NIGHTS visual similarity, outperforms by $8\%$ and
$15\%$ on CUB200 and Flowers102 fine-grained classification, and is within
$10\%$ on ImageNet-1K classification. It also demonstrates strong zero-shot
segmentation, achieving an $R^2$ on COCO within $10\%$ of the models trained on
real data. Finally, we analyze procedural versus real data models, showing that
parts of the same object have dissimilar representations in procedural models,
resulting in incorrect searches in memory and explaining the remaining
performance gap.

</details>


### [49] [FusionFM: Fusing Eye-specific Foundational Models for Optimized Ophthalmic Diagnosis](https://arxiv.org/abs/2508.11721)
*Ke Zou,Jocelyn Hui Lin Goh,Yukun Zhou,Tian Lin,Samantha Min Er Yew,Sahana Srinivasan,Meng Wang,Rui Santos,Gabor M. Somfai,Huazhu Fu,Haoyu Chen,Pearse A. Keane,Ching-Yu Cheng,Yih Chung Tham*

Main category: cs.CV

TL;DR: 系统地评估了单个和融合的眼科FMs，发现DINORET和RetiZero表现较好，模型融合有益处，但预测全身性疾病仍具挑战。


<details>
  <summary>Details</summary>
Motivation: 在眼科领域，已经出现了几种FMs，但对于基本问题仍然没有明确的答案：哪种FM表现最好？它们在不同的任务中是否同样出色？如果我们把所有的FMs结合在一起会怎么样？

Method: 提出FusionFM，一个综合评估套件，以及两种融合方法来整合不同的眼科FMs。

Result: DINORET和RetiZero在眼科和全身疾病任务中表现出色，RetiZero在外部数据集上表现出更强的泛化能力。基于门控的方法在预测青光眼、AMD和高血压方面提供了一些改进。

Conclusion: DINORET和RetiZero在眼科和全身疾病任务中表现出色，RetiZero在外部数据集上表现出更强的泛化能力。基于门控的方法在预测青光眼、AMD和高血压方面提供了一些改进。预测全身性疾病，特别是外部队列中的高血压仍然具有挑战性。

Abstract: Foundation models (FMs) have shown great promise in medical image analysis by
improving generalization across diverse downstream tasks. In ophthalmology,
several FMs have recently emerged, but there is still no clear answer to
fundamental questions: Which FM performs the best? Are they equally good across
different tasks? What if we combine all FMs together? To our knowledge, this is
the first study to systematically evaluate both single and fused ophthalmic
FMs. To address these questions, we propose FusionFM, a comprehensive
evaluation suite, along with two fusion approaches to integrate different
ophthalmic FMs. Our framework covers both ophthalmic disease detection
(glaucoma, diabetic retinopathy, and age-related macular degeneration) and
systemic disease prediction (diabetes and hypertension) based on retinal
imaging. We benchmarked four state-of-the-art FMs (RETFound, VisionFM,
RetiZero, and DINORET) using standardized datasets from multiple countries and
evaluated their performance using AUC and F1 metrics. Our results show that
DINORET and RetiZero achieve superior performance in both ophthalmic and
systemic disease tasks, with RetiZero exhibiting stronger generalization on
external datasets. Regarding fusion strategies, the Gating-based approach
provides modest improvements in predicting glaucoma, AMD, and hypertension.
Despite these advances, predicting systemic diseases, especially hypertension
in external cohort remains challenging. These findings provide an
evidence-based evaluation of ophthalmic FMs, highlight the benefits of model
fusion, and point to strategies for enhancing their clinical applicability.

</details>


### [50] [UniDCF: A Foundation Model for Comprehensive Dentocraniofacial Hard Tissue Reconstruction](https://arxiv.org/abs/2508.11728)
*Chunxia Ren,Ning Zhu,Yue Lai,Gui Chen,Ruijie Wang,Yangyi Hu,Suyao Liu,Shuwen Mao,Hong Su,Yu Zhang,Li Xiao*

Main category: cs.CV

TL;DR: UniDCF is a unified framework capable of reconstructing multiple dentocraniofacial hard tissues through multimodal fusion encoding of point clouds and multi-view images, outperforming existing state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current deep learning models are limited to single-tissue scenarios and modality-specific imaging inputs, resulting in poor generalizability and trade-offs between anatomical fidelity, computational efficiency, and cross-tissue adaptability. 

Method: a unified framework capable of reconstructing multiple dentocraniofacial hard tissues through multimodal fusion encoding of point clouds and multi-view images. By leveraging the complementary strengths of each modality and incorporating a score-based denoising module to refine surface smoothness

Result: UniDCF outperforms existing state-of-the-art methods in terms of geometric precision, structural completeness, and spatial accuracy. Clinical simulations indicate UniDCF reduces reconstruction design time by 99% and achieves clinician-rated acceptability exceeding 94%.

Conclusion: UniDCF enables rapid, automated, and high-fidelity reconstruction, supporting personalized and precise restorative treatments, streamlining clinical workflows, and enhancing patient outcomes.

Abstract: Dentocraniofacial hard tissue defects profoundly affect patients'
physiological functions, facial aesthetics, and psychological well-being,
posing significant challenges for precise reconstruction. Current deep learning
models are limited to single-tissue scenarios and modality-specific imaging
inputs, resulting in poor generalizability and trade-offs between anatomical
fidelity, computational efficiency, and cross-tissue adaptability. Here we
introduce UniDCF, a unified framework capable of reconstructing multiple
dentocraniofacial hard tissues through multimodal fusion encoding of point
clouds and multi-view images. By leveraging the complementary strengths of each
modality and incorporating a score-based denoising module to refine surface
smoothness, UniDCF overcomes the limitations of prior single-modality
approaches. We curated the largest multimodal dataset, comprising intraoral
scans, CBCT, and CT from 6,609 patients, resulting in 54,555 annotated
instances. Evaluations demonstrate that UniDCF outperforms existing
state-of-the-art methods in terms of geometric precision, structural
completeness, and spatial accuracy. Clinical simulations indicate UniDCF
reduces reconstruction design time by 99% and achieves clinician-rated
acceptability exceeding 94%. Overall, UniDCF enables rapid, automated, and
high-fidelity reconstruction, supporting personalized and precise restorative
treatments, streamlining clinical workflows, and enhancing patient outcomes.

</details>


### [51] [Ovis2.5 Technical Report](https://arxiv.org/abs/2508.11737)
*Shiyin Lu,Yang Li,Yu Xia,Yuwei Hu,Shanshan Zhao,Yanqing Ma,Zhichao Wei,Yinglun Li,Lunhao Duan,Jianshan Zhao,Yuxuan Han,Haijun Li,Wanying Chen,Junke Tang,Chengkun Hou,Zhixing Du,Tianli Zhou,Wenjie Zhang,Huping Ding,Jiahe Li,Wen Li,Gui Hu,Yiliang Gu,Siran Yang,Jiamang Wang,Hailong Sun,Yibo Wang,Hui Sun,Jinlong Huang,Yuping He,Shengze Shi,Weihong Zhang,Guodong Zheng,Junpeng Jiang,Sensen Gao,Yi-Feng Wu,Sijia Chen,Yuhui Chen,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang*

Main category: cs.CV

TL;DR: Ovis2.5是一种用于视觉感知和多模态推理的新模型，它在多个基准测试中都取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: Ovis2.5旨在实现原生分辨率的视觉感知和强大的多模态推理。它集成了原生分辨率视觉转换器，可以在其原生可变分辨率下处理图像，避免了固定分辨率平铺造成的退化，并保留了精细的细节和全局布局——这对于视觉密集的内容（如复杂图表）至关重要。

Method: 该模型通过一个全面的五阶段课程进行训练，该课程逐步构建其技能。该过程从基础视觉和多模态预训练开始，通过大规模指令调整进行改进，并最终使用DPO和GRPO进行对齐和推理增强。

Result: Ovis2.5-9B在OpenCompass多模态排行榜上的平均分为78.3，比其前身Ovis2-8B有了显著提高，并在40B参数范围内的开源MLLM中取得了最先进的结果；Ovis2.5-2B的得分为73.9，为其规模建立了SOTA。Ovis2.5在STEM基准测试中取得了领先成果，在基础和视频任务中表现出强大的能力，并在其规模上实现了复杂图表分析的开源SOTA。

Conclusion: Ovis2.5-9B和Ovis2.5-2B是两个开源模型，它们在多个基准测试中都取得了最先进的结果，尤其是在STEM基准测试、基础和视频任务以及复杂图表分析方面。

Abstract: We present Ovis2.5, a successor to Ovis2 designed for native-resolution
visual perception and strong multimodal reasoning. Ovis2.5 integrates a
native-resolution vision transformer that processes images at their native,
variable resolutions, avoiding the degradation from fixed-resolution tiling and
preserving both fine detail and global layout -- crucial for visually dense
content like complex charts. To strengthen reasoning, we train the model to
move beyond linear chain-of-thought and perform reflection -- including
self-checking and revision. This advanced capability is exposed as an optional
"thinking mode" at inference time, allowing users to trade latency for enhanced
accuracy on difficult inputs. The model is trained via a comprehensive
five-phase curriculum that progressively builds its skills. The process begins
with foundational visual and multimodal pretraining, advances through
large-scale instruction tuning, and culminates in alignment and reasoning
enhancement using DPO and GRPO. To scale these upgrades efficiently, we employ
multimodal data packing and hybrid parallelism, yielding a significant
end-to-end speedup. We release two open-source models: Ovis2.5-9B and
Ovis2.5-2B. The latter continues the "small model, big performance" philosophy
of Ovis2, making it ideal for resource-constrained, on-device scenarios. On the
OpenCompass multimodal leaderboard, Ovis2.5-9B averages 78.3, marking a
substantial improvement over its predecessor, Ovis2-8B, and achieving
state-of-the-art results among open-source MLLMs in the sub-40B parameter
range; Ovis2.5-2B scores 73.9, establishing SOTA for its size. Beyond aggregate
scores, Ovis2.5 achieves leading results on STEM benchmarks, exhibits strong
capabilities on grounding and video tasks, and achieves open-source SOTA at its
scale for complex chart analysis.

</details>


### [52] [VideoAVE: A Multi-Attribute Video-to-Text Attribute Value Extraction Dataset and Benchmark Models](https://arxiv.org/abs/2508.11801)
*Ming Cheng,Tong Wu,Jiazhen Hu,Jiaying Gong,Hoda Eldardiry*

Main category: cs.CV

TL;DR: introduce VideoAVE, the first publicly available video-to-text e-commerce AVE dataset


<details>
  <summary>Details</summary>
Motivation: existing AVE datasets are primarily limited to text-to-text or image-to-text settings, lacking support for product videos, diverse attribute coverage, and public availability

Method: a post-hoc CLIP-based Mixture of Experts filtering system (CLIP-MoE)

Result: a refined dataset of 224k training data and 25k evaluation data

Conclusion: video-to-text AVE remains a challenging problem, particularly in open settings, and there is still room for developing more advanced VLMs capable of leveraging effective temporal information

Abstract: Attribute Value Extraction (AVE) is important for structuring product
information in e-commerce. However, existing AVE datasets are primarily limited
to text-to-text or image-to-text settings, lacking support for product videos,
diverse attribute coverage, and public availability. To address these gaps, we
introduce VideoAVE, the first publicly available video-to-text e-commerce AVE
dataset across 14 different domains and covering 172 unique attributes. To
ensure data quality, we propose a post-hoc CLIP-based Mixture of Experts
filtering system (CLIP-MoE) to remove the mismatched video-product pairs,
resulting in a refined dataset of 224k training data and 25k evaluation data.
In order to evaluate the usability of the dataset, we further establish a
comprehensive benchmark by evaluating several state-of-the-art video vision
language models (VLMs) under both attribute-conditioned value prediction and
open attribute-value pair extraction tasks. Our results analysis reveals that
video-to-text AVE remains a challenging problem, particularly in open settings,
and there is still room for developing more advanced VLMs capable of leveraging
effective temporal information. The dataset and benchmark code for VideoAVE are
available at: https://github.com/gjiaying/VideoAVE

</details>


### [53] [An MLP Baseline for Handwriting Recognition Using Planar Curvature and Gradient Orientation](https://arxiv.org/abs/2508.11803)
*Azam Nouri*

Main category: cs.CV

TL;DR: 本研究表明，基于曲率的特征可以有效地用于手写字符识别，并且可以使用手工设计的特征实现深度学习的优势。


<details>
  <summary>Details</summary>
Motivation: 研究二阶几何线索是否足以驱动用于手写字符识别 (HCR) 的多层感知器 (MLP) 分类器，从而为卷积神经网络 (CNN) 提供替代方案。

Method: 使用二阶几何线索（平面曲率大小、曲率符号和梯度方向）驱动多层感知器 (MLP) 分类器。

Result: 基于曲率方向的 MLP 在 MNIST 数字上实现了 97% 的准确率，在 EMNIST 字母上实现了 89% 的准确率。

Conclusion: 曲率表示法对手写字符图像具有判别能力，即使使用可解释的手工设计的特征，也可以实现深度学习的优势。

Abstract: This study investigates whether second-order geometric cues - planar
curvature magnitude, curvature sign, and gradient orientation - are sufficient
on their own to drive a multilayer perceptron (MLP) classifier for handwritten
character recognition (HCR), offering an alternative to convolutional neural
networks (CNNs). Using these three handcrafted feature maps as inputs, our
curvature-orientation MLP achieves 97 percent accuracy on MNIST digits and 89
percent on EMNIST letters. These results underscore the discriminative power of
curvature-based representations for handwritten character images and
demonstrate that the advantages of deep learning can be realized even with
interpretable, hand-engineered features.

</details>


### [54] [Labels or Input? Rethinking Augmentation in Multimodal Hate Detection](https://arxiv.org/abs/2508.11808)
*Sahajpreet Singh,Rongxin Ouyang,Subhayan Mukerjee,Kokil Jaidka*

Main category: cs.CV

TL;DR: 提出了一种双管齐下的方法来改进多模态仇恨检测。


<details>
  <summary>Details</summary>
Motivation: 现代网络充斥着多模态内容，加剧了检测仇恨模因的挑战，在仇恨模因中，有害意图通常通过文本和图像之间的微妙交互来传达，伪装成幽默或讽刺。虽然视觉-语言模型（VLM）的最新进展显示出了希望，但这些模型缺乏对细粒度监督的支持，并且仍然容易受到隐性仇恨言论的影响。

Method: 提出了一个prompt优化框架，系统地改变了prompt结构、监督粒度和训练模态。引入了一种多模态数据增强管道，通过隔离和重写仇恨模态来生成2,479个反事实的中性模因。

Result: 结构化prompt提高了鲁棒性，即使在小型模型中也是如此，并且InternVL2在二元和缩放设置中实现了最佳F1分数。该管道成功地减少了虚假相关性，并提高了分类器的泛化能力。

Conclusion: Prompt结构和数据组成与模型大小一样重要，有针对性的增强可以支持更可信和上下文相关的仇恨检测。

Abstract: The modern web is saturated with multimodal content, intensifying the
challenge of detecting hateful memes, where harmful intent is often conveyed
through subtle interactions between text and image under the guise of humor or
satire. While recent advances in Vision-Language Models (VLMs) show promise,
these models lack support for fine-grained supervision and remain susceptible
to implicit hate speech. In this paper, we present a dual-pronged approach to
improve multimodal hate detection. First, we propose a prompt optimization
framework that systematically varies prompt structure, supervision granularity,
and training modality. We show that prompt design and label scaling both
influence performance, with structured prompts improving robustness even in
small models, and InternVL2 achieving the best F1-scores across binary and
scaled settings. Second, we introduce a multimodal data augmentation pipeline
that generates 2,479 counterfactually neutral memes by isolating and rewriting
the hateful modality. This pipeline, powered by a multi-agent LLM-VLM setup,
successfully reduces spurious correlations and improves classifier
generalization. Our approaches inspire new directions for building synthetic
data to train robust and fair vision-language models. Our findings demonstrate
that prompt structure and data composition are as critical as model size, and
that targeted augmentation can support more trustworthy and context-sensitive
hate detection.

</details>


### [55] [Towards Understanding 3D Vision: the Role of Gaussian Curvature](https://arxiv.org/abs/2508.11825)
*Sherlon Almeida da Silva,Davi Geiger,Luiz Velho,Moacir Antonelli Ponti*

Main category: cs.CV

TL;DR: This paper investigates the role of Gaussian curvature in 3D surface modeling and shows its benefits.


<details>
  <summary>Details</summary>
Motivation: Deep learning methods for computer vision lack explicit models of 3D geometry that can be directly analyzed, transferred across modalities, or systematically modified for controlled experimentation.

Method: investigate the role of Gaussian curvature in 3D surface modeling

Result: demonstrate using the Middlebury stereo dataset

Conclusion: Gaussian curvature offers a sparse and compact description of 3D surfaces, state-of-the-art monocular and stereo methods seem to implicitly consider it, a form of geometric prior that can inform and improve 3D surface reconstruction, and a possible use as an unsupervised metric for stereo methods.

Abstract: Recent advances in computer vision have predominantly relied on data-driven
approaches that leverage deep learning and large-scale datasets. Deep neural
networks have achieved remarkable success in tasks such as stereo matching and
monocular depth reconstruction. However, these methods lack explicit models of
3D geometry that can be directly analyzed, transferred across modalities, or
systematically modified for controlled experimentation. We investigate the role
of Gaussian curvature in 3D surface modeling. Besides Gaussian curvature being
an invariant quantity under change of observers or coordinate systems, we
demonstrate using the Middlebury stereo dataset that it offers: (i) a sparse
and compact description of 3D surfaces, (ii) state-of-the-art monocular and
stereo methods seem to implicitly consider it, but no explicit module of such
use can be extracted, (iii) a form of geometric prior that can inform and
improve 3D surface reconstruction, and (iv) a possible use as an unsupervised
metric for stereo methods.

</details>


### [56] [From Pixels to Graphs: Deep Graph-Level Anomaly Detection on Dermoscopic Images](https://arxiv.org/abs/2508.11826)
*Dehn Xu,Tim Katzke,Emmanuel Müller*

Main category: cs.CV

TL;DR: This study systematically evaluates image-to-graph transformation approaches for GNN-based graph-level anomaly detection, finding that color descriptors are important and shape/texture features improve efficacy. Achieves strong performance on dermoscopic images, especially with supervision.


<details>
  <summary>Details</summary>
Motivation: no study has rigorously compared the effectiveness of the numerous potential image-to-graph transformation approaches for GNN-based graph-level anomaly detection (GLAD).

Method: systematically evaluate the efficacy of multiple segmentation schemes, edge construction strategies, and node feature sets based on color, texture, and shape descriptors to produce suitable image-derived graph representations to perform graph-level anomaly detection. We conduct extensive experiments on dermoscopic images using state-of-the-art GLAD models, examining performance and efficiency in purely unsupervised, weakly supervised, and fully supervised regimes.

Result: color descriptors contribute the best standalone performance, while incorporating shape and texture features consistently enhances detection efficacy. The best unsupervised configuration using OCGTL achieves a competitive AUC-ROC score of up to 0.805 without relying on pretrained backbones. With the inclusion of sparse labels, the performance increases substantially to 0.872 and with full supervision to 0.914 AUC-ROC.

Conclusion: Color descriptors contribute the best standalone performance, while incorporating shape and texture features consistently enhances detection efficacy. The best unsupervised configuration using OCGTL achieves a competitive AUC-ROC score of up to 0.805 without relying on pretrained backbones. With the inclusion of sparse labels, the performance increases substantially to 0.872 and with full supervision to 0.914 AUC-ROC.

Abstract: Graph Neural Networks (GNNs) have emerged as a powerful approach for
graph-based machine learning tasks. Previous work applied GNNs to image-derived
graph representations for various downstream tasks such as classification or
anomaly detection. These transformations include segmenting images, extracting
features from segments, mapping them to nodes, and connecting them. However, to
the best of our knowledge, no study has rigorously compared the effectiveness
of the numerous potential image-to-graph transformation approaches for
GNN-based graph-level anomaly detection (GLAD). In this study, we
systematically evaluate the efficacy of multiple segmentation schemes, edge
construction strategies, and node feature sets based on color, texture, and
shape descriptors to produce suitable image-derived graph representations to
perform graph-level anomaly detection. We conduct extensive experiments on
dermoscopic images using state-of-the-art GLAD models, examining performance
and efficiency in purely unsupervised, weakly supervised, and fully supervised
regimes. Our findings reveal, for example, that color descriptors contribute
the best standalone performance, while incorporating shape and texture features
consistently enhances detection efficacy. In particular, our best unsupervised
configuration using OCGTL achieves a competitive AUC-ROC score of up to 0.805
without relying on pretrained backbones like comparable image-based approaches.
With the inclusion of sparse labels, the performance increases substantially to
0.872 and with full supervision to 0.914 AUC-ROC.

</details>


### [57] [Recent Advances in Transformer and Large Language Models for UAV Applications](https://arxiv.org/abs/2508.11834)
*Hamza Kheddar,Yassine Habchi,Mohamed Chahine Ghanem,Mustapha Hemis,Dusit Niyato*

Main category: cs.CV

TL;DR: This review paper categorizes and evaluates recent developments in Transformer architectures applied to UAVs, highlights emerging applications, and provides comparative analyses.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of Transformer-based models has reshaped the landscape of uncrewed aerial vehicle (UAV) systems by enhancing perception, decision-making, and autonomy.

Method: This review paper systematically categorizes and evaluates recent developments in Transformer architectures applied to UAVs, including attention mechanisms, CNN-Transformer hybrids, reinforcement learning Transformers, and large language models (LLMs).

Result: This work presents a unified taxonomy of Transformer-based UAV models, highlights emerging applications such as precision agriculture and autonomous navigation, and provides comparative analyses through structured tables and performance benchmarks. The paper also reviews key datasets, simulators, and evaluation metrics used in the field.

Conclusion: This review paper aims to guide researchers and practitioners in understanding and advancing Transformer-driven UAV technologies.

Abstract: The rapid advancement of Transformer-based models has reshaped the landscape
of uncrewed aerial vehicle (UAV) systems by enhancing perception,
decision-making, and autonomy. This review paper systematically categorizes and
evaluates recent developments in Transformer architectures applied to UAVs,
including attention mechanisms, CNN-Transformer hybrids, reinforcement learning
Transformers, and large language models (LLMs). Unlike previous surveys, this
work presents a unified taxonomy of Transformer-based UAV models, highlights
emerging applications such as precision agriculture and autonomous navigation,
and provides comparative analyses through structured tables and performance
benchmarks. The paper also reviews key datasets, simulators, and evaluation
metrics used in the field. Furthermore, it identifies existing gaps in the
literature, outlines critical challenges in computational efficiency and
real-time deployment, and offers future research directions. This comprehensive
synthesis aims to guide researchers and practitioners in understanding and
advancing Transformer-driven UAV technologies.

</details>


### [58] [ComplicitSplat: Downstream Models are Vulnerable to Blackbox Attacks by 3D Gaussian Splat Camouflages](https://arxiv.org/abs/2508.11854)
*Matthew Hull,Haoyang Yang,Pratham Mehta,Mansi Phute,Aeree Cho,Haorang Wang,Matthew Lau,Wenke Lee,Wilian Lunardi,Martin Andreoni,Polo Chau*

Main category: cs.CV

TL;DR: ComplicitSplat 是一种新的黑盒攻击，它利用 3DGS 中的视角特定伪装来攻击对象检测器，从而对自动导航等应用构成安全风险。


<details>
  <summary>Details</summary>
Motivation: 由于 3D Gaussian Splatting (3DGS) 在安全关键任务中得到快速采用，用于从静态图像中高效地合成新视角，那么攻击者可能如何篡改图像以造成损害？

Method: 利用标准的 3DGS shading 方法创建视角特定的伪装，即颜色和纹理随视角变化，以将对抗性内容嵌入到仅从特定视角可见的场景对象中，而无需访问模型架构或权重。

Result: ComplicitSplat 能够泛化并成功攻击各种流行的检测器。

Conclusion: ComplicitSplat 攻击成功地攻击了各种流行的检测器，包括单阶段、多阶段和基于 Transformer 的模型，无论是在真实物体的真实世界捕获还是合成场景中。这是第一个使用 3DGS 对下游对象检测器进行的黑盒攻击，揭示了自动导航和其他任务关键型机器人系统等应用的新安全风险。

Abstract: As 3D Gaussian Splatting (3DGS) gains rapid adoption in safety-critical tasks
for efficient novel-view synthesis from static images, how might an adversary
tamper images to cause harm? We introduce ComplicitSplat, the first attack that
exploits standard 3DGS shading methods to create viewpoint-specific camouflage
- colors and textures that change with viewing angle - to embed adversarial
content in scene objects that are visible only from specific viewpoints and
without requiring access to model architecture or weights. Our extensive
experiments show that ComplicitSplat generalizes to successfully attack a
variety of popular detector - both single-stage, multi-stage, and
transformer-based models on both real-world capture of physical objects and
synthetic scenes. To our knowledge, this is the first black-box attack on
downstream object detectors using 3DGS, exposing a novel safety risk for
applications like autonomous navigation and other mission-critical robotic
systems.

</details>


### [59] [Impact of Clinical Image Quality on Efficient Foundation Model Finetuning](https://arxiv.org/abs/2508.11864)
*Yucheng Tang,Pawel Rajwa,Alexander Ng,Yipei Wang,Wen Yan,Natasha Thorley,Aqua Asif,Clare Allen,Louise Dickinson,Francesco Giganti,Shonit Punwani,Daniel C. Alexander,Veeru Kasivisvanathan,Yipeng Hu*

Main category: cs.CV

TL;DR: 本文研究了图像质量对前列腺MRI中基础模型微调的影响，发现图像质量分布的匹配对于实现最佳性能至关重要。


<details>
  <summary>Details</summary>
Motivation: 医学影像中的基础模型已经显示出有希望的标签效率，仅用一小部分带注释的数据即可实现高下游性能。本文研究了可变的图像质量如何影响标签高效微调。

Method: 使用ProFound，一个在大型前列腺MRI数据集上预训练的领域特定视觉基础模型，在多参数前列腺MRI中评估标签效率。

Result: a) 微调和测试集之间高质量图像与低质量图像的比率变化会导致下游性能的显着差异；b) 微调集中存在足够的高质量图像对于保持强大的性能至关重要，而匹配的微调和测试分布的重要性因不同的下游任务而异，例如自动放射学报告和前列腺癌检测。

Conclusion: 图像质量分布及其微调和测试不匹配会显著影响模型性能。当质量比率一致时，微调比从头开始训练需要更少的标记数据，但标签效率取决于图像质量分布。没有足够的高质量微调数据，预训练模型可能无法胜过那些没有预训练的模型。因此，评估和调整微调和部署之间的质量分布非常重要，并且需要针对特定下游任务的微调数据质量标准。使用ProFound，我们展示了量化微调和部署中的图像质量的价值，以充分实现基础模型的数据和计算效率优势。

Abstract: Foundation models in medical imaging have shown promising label efficiency,
achieving high downstream performance with only a fraction of annotated data.
Here, we evaluate this in prostate multiparametric MRI using ProFound, a
domain-specific vision foundation model pretrained on large-scale prostate MRI
datasets. We investigate how variable image quality affects label-efficient
finetuning by measuring the generalisability of finetuned models. Experiments
systematically vary high-/low-quality image ratios in finetuning and evaluation
sets. Our findings indicate that image quality distribution and its
finetune-and-test mismatch significantly affect model performance. In
particular: a) Varying the ratio of high- to low-quality images between
finetuning and test sets leads to notable differences in downstream
performance; and b) The presence of sufficient high-quality images in the
finetuning set is critical for maintaining strong performance, whilst the
importance of matched finetuning and testing distribution varies between
different downstream tasks, such as automated radiology reporting and prostate
cancer detection.When quality ratios are consistent, finetuning needs far less
labeled data than training from scratch, but label efficiency depends on image
quality distribution. Without enough high-quality finetuning data, pretrained
models may fail to outperform those trained without pretraining. This
highlights the importance of assessing and aligning quality distributions
between finetuning and deployment, and the need for quality standards in
finetuning data for specific downstream tasks. Using ProFound, we show the
value of quantifying image quality in both finetuning and deployment to fully
realise the data and compute efficiency benefits of foundation models.

</details>


### [60] [AdaRing: Towards Ultra-Light Vision-Language Adaptation via Cross-Layer Tensor Ring Decomposition](https://arxiv.org/abs/2508.11870)
*Ying Huang,Yuanbin Man,Wenqi Jia,Zhengzhong Tu,Junzhou Huang,Miao Yin*

Main category: cs.CV

TL;DR: Proposes AdaRing, a parameter-efficient fine-tuning method for VLMs using cross-layer tensor ring decomposition and diverse adapters, achieving state-of-the-art performance with 90% parameter reduction.


<details>
  <summary>Details</summary>
Motivation: Existing adapter-based fine-tuning methods face limitations: 1) limited compression rate due to ignoring cross-layer redundancy, and 2) limited representational capacity across homogeneous adapters.

Method: a novel vision-language fine-tuning framework based on cross-layer tensor ring decomposition (TRD) with the integration and collaboration of diverse adapters, called AdaRing

Result: achieving ultra-light parameter-efficient adaptation of VLMs on various tasks

Conclusion: The proposed AdaRing achieves state-of-the-art performance while reducing average training parameters by 90%.

Abstract: Adapter-based fine-tuning has gained remarkable attention in adapting large
pre-trained vision language models (VLMs) for a wide range of downstream tasks
efficiently. In this paradigm, only the inserted adapters are fine-tuned,
without the need for training the original VLM backbone. Existing works scale
adapters by integrating them into every layer of VLMs to increase the capacity
of adapters. However, these methods face two primary limitations: 1) limited
compression rate due to ignoring cross-layer redundancy, and 2) limited
representational capacity across homogeneous adapters. In this paper, we
propose a novel vision-language fine-tuning framework based on cross-layer
tensor ring decomposition (TRD) with the integration and collaboration of
diverse adapters, called AdaRing, achieving ultra-light parameter-efficient
adaptation of VLMs on various tasks. To remove the high redundancy that exists
among adapters across layers, we exploit the tensor-level low-rankness to
formulate adapters as layer-shared tensor cores and layer-specific slices.
Moreover, guided by generalization-aware fine-tuning, diverse rank-driven
adapters cooperate to handle tasks that require different representations. Our
experiments show that the proposed AdaRing achieves the state-of-the-art
performance while reducing average training parameters by 90%.

</details>


### [61] [EVTP-IVS: Effective Visual Token Pruning For Unifying Instruction Visual Segmentation In Multi-Modal Large Language Models](https://arxiv.org/abs/2508.11886)
*Wenhui Zhu,Xiwen Chen,Zhipeng Wang,Shao Tang,Sayan Ghosh,Xuanzhao Dong,Rajat Koner,Yalin Wang*

Main category: cs.CV

TL;DR: Proposes EVTP-IV, a visual token pruning method for IVS that significantly accelerates inference while maintaining accuracy, achieving up to 5X speed-up on video and 3.5X on image tasks.


<details>
  <summary>Details</summary>
Motivation: Reducing the inference cost of Multimodal Large Language Models (MLLMs) on Instructed Visual Segmentation (IVS) tasks, particularly in video, by addressing the bottleneck of high inference cost.

Method: A novel visual token pruning method called EVTP-IV, which builds upon the k-center by integrating spatial information.

Result: The method achieves significant speed-ups (up to 5X on video and 3.5X on image tasks) while maintaining comparable accuracy using only 20% of the tokens and outperforming state-of-the-art pruning baselines.

Conclusion: The proposed EVTP-IV method achieves up to 5X speed-up on video IVS tasks and 3.5X on image tasks, while maintaining comparable accuracy using only 20% of the tokens, and outperforms state-of-the-art pruning baselines.

Abstract: Instructed Visual Segmentation (IVS) tasks require segmenting objects in
images or videos based on natural language instructions. While recent
multimodal large language models (MLLMs) have achieved strong performance on
IVS, their inference cost remains a major bottleneck, particularly in video. We
empirically analyze visual token sampling in MLLMs and observe a strong
correlation between subset token coverage and segmentation performance. This
motivates our design of a simple and effective token pruning method that
selects a compact yet spatially representative subset of tokens to accelerate
inference. In this paper, we introduce a novel visual token pruning method for
IVS, called EVTP-IV, which builds upon the k-center by integrating spatial
information to ensure better coverage. We further provide an
information-theoretic analysis to support our design. Experiments on standard
IVS benchmarks show that our method achieves up to 5X speed-up on video tasks
and 3.5X on image tasks, while maintaining comparable accuracy using only 20%
of the tokens. Our method also consistently outperforms state-of-the-art
pruning baselines under varying pruning ratios.

</details>


### [62] [MOON: Generative MLLM-based Multimodal Representation Learning for E-commerce Product Understanding](https://arxiv.org/abs/2508.11999)
*Daoze Zhang,Zhanheng Nie,Jianyu Liu,Chenghan Fu,Wanxian Guan,Yuan Gao,Jun Song,Pengjie Wang,Jian Xu,Bo Zheng*

Main category: cs.CV

TL;DR: 本文提出了一个名为MOON的生成式MLLM模型，用于产品表示学习，该模型采用引导式的混合专家模块，有效地检测产品图像中的核心语义区域，并引入了专门的负采样策略。此外，我们发布了一个大规模多模态基准MBE。


<details>
  <summary>Details</summary>
Motivation: 探索通用表示而不是特定于任务的表示已经引起越来越多的研究关注。现有的判别式双流架构在产品理解领域推动了进展，但它们在建模产品的多个图像和文本之间的多对一对应关系方面存在困难。典型的LLM缺乏多模态和方面感知的建模模块；产品图像中普遍存在背景噪声；缺乏用于评估的标准基准。

Method: 我们提出了第一个基于生成式MLLM的模型MOON用于产品表示学习。我们的方法（1）采用了一种引导式的混合专家（MoE）模块，用于有针对性地建模多模态和特定方面的产品内容；（2）有效地检测产品图像中的核心语义区域，以减轻背景噪声造成的干扰；（3）引入了专门的负采样策略，以增加负样本的难度和多样性。

Result: MOON模型在产品理解方面表现出有效性。

Conclusion: MOON模型在我们的基准测试和公共数据集上展示了有竞争力的零样本性能，展示了在各种下游任务中的强大泛化能力，包括跨模态检索、产品分类和属性预测。案例研究和可视化说明了MOON在产品理解方面的有效性。

Abstract: With the rapid advancement of e-commerce, exploring general representations
rather than task-specific ones has attracted increasing research attention. For
product understanding, although existing discriminative dual-flow architectures
drive progress in this field, they inherently struggle to model the many-to-one
alignment between multiple images and texts of products. Therefore, we argue
that generative Multimodal Large Language Models (MLLMs) hold significant
potential for improving product representation learning. Nevertheless,
achieving this goal still remains non-trivial due to several key challenges:
the lack of multimodal and aspect-aware modeling modules in typical LLMs; the
common presence of background noise in product images; and the absence of a
standard benchmark for evaluation. To address these issues, we propose the
first generative MLLM-based model named MOON for product representation
learning. Our method (1) employs a guided Mixture-of-Experts (MoE) module for
targeted modeling of multimodal and aspect-specific product content; (2)
effectively detects core semantic regions in product images to mitigate the
distraction and interference caused by background noise; and (3) introduces the
specialized negative sampling strategy to increase the difficulty and diversity
of negative samples. In addition, we release a large-scale multimodal benchmark
MBE for various product understanding tasks. Experimentally, our model
demonstrates competitive zero-shot performance on both our benchmark and the
public dataset, showcasing strong generalization across various downstream
tasks, including cross-modal retrieval, product classification, and attribute
prediction. Furthermore, the case study and visualization illustrate the
effectiveness of MOON for product understanding.

</details>


### [63] [Large Kernel Modulation Network for Efficient Image Super-Resolution](https://arxiv.org/abs/2508.11893)
*Quanwei Hu,Yinggan Tang,Xuguang Zhang*

Main category: cs.CV

TL;DR: LKMN, a CNN-based model, balances performance and latency in image super-resolution by using Enhanced Partial Large Kernel Block (EPLKB) and Cross-Gate Feed-Forward Network (CGFN), outperforming existing lightweight SR models.


<details>
  <summary>Details</summary>
Motivation: Image super-resolution (SR) in resource-constrained scenarios demands lightweight models balancing performance and latency. Convolutional neural networks (CNNs) offer low latency but lack non-local feature capture, while Transformers excel at non-local modeling yet suffer slow inference. To address this trade-off

Method: The Large Kernel Modulation Network (LKMN), a pure CNN-based model with two core components: Enhanced Partial Large Kernel Block (EPLKB) and Cross-Gate Feed-Forward Network (CGFN).

Result: LKMN-L achieves 0.23 dB PSNR improvement over DAT-light on the Manga109 dataset at $\times$4 upscale, with nearly $\times$4.8 times faster.

Conclusion: LKMN outperforms existing state-of-the-art (SOTA) lightweight SR models while balancing quality and efficiency. Specifically, LKMN-L achieves 0.23 dB PSNR improvement over DAT-light on the Manga109 dataset at $\times$4 upscale, with nearly $\times$4.8 times faster.

Abstract: Image super-resolution (SR) in resource-constrained scenarios demands
lightweight models balancing performance and latency. Convolutional neural
networks (CNNs) offer low latency but lack non-local feature capture, while
Transformers excel at non-local modeling yet suffer slow inference. To address
this trade-off, we propose the Large Kernel Modulation Network (LKMN), a pure
CNN-based model. LKMN has two core components: Enhanced Partial Large Kernel
Block (EPLKB) and Cross-Gate Feed-Forward Network (CGFN). The EPLKB utilizes
channel shuffle to boost inter-channel interaction, incorporates channel
attention to focus on key information, and applies large kernel strip
convolutions on partial channels for non-local feature extraction with reduced
complexity. The CGFN dynamically adjusts discrepancies between input, local,
and non-local features via a learnable scaling factor, then employs a
cross-gate strategy to modulate and fuse these features, enhancing their
complementarity. Extensive experiments demonstrate that our method outperforms
existing state-of-the-art (SOTA) lightweight SR models while balancing quality
and efficiency. Specifically, LKMN-L achieves 0.23 dB PSNR improvement over
DAT-light on the Manga109 dataset at $\times$4 upscale, with nearly $\times$4.8
times faster. Codes are in the supplementary materials. The code is available
at https://github.com/Supereeeee/LKMN.

</details>


### [64] [A Sobel-Gradient MLP Baseline for Handwritten Character Recognition](https://arxiv.org/abs/2508.11902)
*Azam Nouri*

Main category: cs.CV

TL;DR: 使用 Sobel 算子和 MLP，手写字符识别也能达到接近 CNN 的效果，但内存占用更小，特征更透明。


<details>
  <summary>Details</summary>
Motivation: 重新审视经典的 Sobel 算子，提出了一个简单的问题：作为卷积神经网络 (CNN) 的替代方案，一阶边缘图是否足以驱动用于手写字符识别 (HCR) 的全密集多层感知器 (MLP)？

Method: 使用水平和垂直 Sobel 导数作为输入，在 MNIST 和 EMNIST Letters 上训练了一个 MLP。

Result: 所得到的网络在 MNIST 数字上达到了 98% 的准确率，在 EMNIST 字母上达到了 92% 的准确率——接近 CNN，同时提供了更小的内存占用和透明的特征。

Conclusion: 手写字符图像中的大部分类别判别信息已经被一阶梯度捕获，这使得边缘感知 MLP 成为 HCR 的一个引人注目的选择。

Abstract: We revisit the classical Sobel operator to ask a simple question: Are
first-order edge maps sufficient to drive an all-dense multilayer perceptron
(MLP) for handwritten character recognition (HCR), as an alternative to
convolutional neural networks (CNNs)? Using only horizontal and vertical Sobel
derivatives as input, we train an MLP on MNIST and EMNIST Letters. Despite its
extreme simplicity, the resulting network reaches 98% accuracy on MNIST digits
and 92% on EMNIST letters -- approaching CNNs while offering a smaller memory
footprint and transparent features. Our findings highlight that much of the
class-discriminative information in handwritten character images is already
captured by first-order gradients, making edge-aware MLPs a compelling option
for HCR.

</details>


### [65] [OVG-HQ: Online Video Grounding with Hybrid-modal Queries](https://arxiv.org/abs/2508.11903)
*Runhao Zeng,Jiaqi Mao,Minghao Lai,Minh Hieu Phan,Yanjie Dong,Wei Wang,Qi Chen,Xiping Hu*

Main category: cs.CV

TL;DR: 提出了在线视频混合模态查询定位 (OVG-HQ) 任务，并构建了 OVG-HQ-Unify 模型和 QVHighlights-Unify 数据集，解决了在线环境中的上下文限制和模态不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统视频定位在流媒体视频或使用视觉线索的查询等场景中存在不足。为了填补这一空白，我们提出了一个新的任务，名为在线视频混合模态查询定位 (OVG-HQ)。

Method: 提出了 OVG-HQ-Unify 框架，该框架具有参数记忆块 (PMB) 和跨模态蒸馏策略。

Result: OVG-HQ-Unify 优于现有模型，并提出了 oR@n, IoU=m, 和 online mean Average Precision (omAP) 等在线评估指标。

Conclusion: OVG-HQ-Unify 模型在在线混合模态视频定位任务中优于现有模型，为该任务提供了一个鲁棒的解决方案。

Abstract: Video grounding (VG) task focuses on locating specific moments in a video
based on a query, usually in text form. However, traditional VG struggles with
some scenarios like streaming video or queries using visual cues. To fill this
gap, we present a new task named Online Video Grounding with Hybrid-modal
Queries (OVG-HQ), which enables online segment localization using text, images,
video segments, and their combinations. This task poses two new challenges:
limited context in online settings and modality imbalance during training,
where dominant modalities overshadow weaker ones. To address these, we propose
OVG-HQ-Unify, a unified framework featuring a Parametric Memory Block (PMB)
that retain previously learned knowledge to enhance current decision and a
cross-modal distillation strategy that guides the learning of non-dominant
modalities. This design enables a single model to effectively handle
hybrid-modal queries. Due to the lack of suitable datasets, we construct
QVHighlights-Unify, an expanded dataset with multi-modal queries. Besides,
since offline metrics overlook prediction timeliness, we adapt them to the
online setting, introducing oR@n, IoU=m, and online mean Average Precision
(omAP) to evaluate both accuracy and efficiency. Experiments show that our
OVG-HQ-Unify outperforms existing models, offering a robust solution for
online, hybrid-modal video grounding. Source code and datasets are available at
https://github.com/maojiaqi2324/OVG-HQ.

</details>


### [66] [SafeCtrl: Region-Based Safety Control for Text-to-Image Diffusion via Detect-Then-Suppress](https://arxiv.org/abs/2508.11904)
*Lingyun Zhang,Yu Xie,Yanwei Fu,Ping Chen*

Main category: cs.CV

TL;DR: SafeCtrl, a lightweight plugin, localizes and suppresses unsafe content in text-to-image models, outperforming existing methods in safety and fidelity by using DPO for training.


<details>
  <summary>Details</summary>
Motivation: Existing safety methods often introduce a trade-off between safety and fidelity. Recent localization-based approaches rely on explicit concept replacement which can sometimes lead to semantic incongruity.

Method: A lightweight, non-intrusive plugin that first precisely localizes unsafe content and then suppresses the harmful semantics, allowing the generative process to naturally and coherently resolve into a safe, context-aware alternative. A novel training strategy using Direct Preference Optimization (DPO) is used.

Result: SafeCtrl significantly outperforms state-of-the-art methods in both safety efficacy and fidelity preservation.

Conclusion: Decoupled, suppression-based control is a highly effective and scalable direction for building more responsible generative models.

Abstract: The widespread deployment of text-to-image models is challenged by their
potential to generate harmful content. While existing safety methods, such as
prompt rewriting or model fine-tuning, provide valuable interventions, they
often introduce a trade-off between safety and fidelity. Recent
localization-based approaches have shown promise, yet their reliance on
explicit ``concept replacement" can sometimes lead to semantic incongruity. To
address these limitations, we explore a more flexible detect-then-suppress
paradigm. We introduce SafeCtrl, a lightweight, non-intrusive plugin that first
precisely localizes unsafe content. Instead of performing a hard A-to-B
substitution, SafeCtrl then suppresses the harmful semantics, allowing the
generative process to naturally and coherently resolve into a safe,
context-aware alternative. A key aspect of our work is a novel training
strategy using Direct Preference Optimization (DPO). We leverage readily
available, image-level preference data to train our module, enabling it to
learn nuanced suppression behaviors and perform region-guided interventions at
inference without requiring costly, pixel-level annotations. Extensive
experiments show that SafeCtrl significantly outperforms state-of-the-art
methods in both safety efficacy and fidelity preservation. Our findings suggest
that decoupled, suppression-based control is a highly effective and scalable
direction for building more responsible generative models.

</details>


### [67] [TimeSenCLIP: A Vision-Language Model for Remote Sensing Using Single-Pixel Time Series](https://arxiv.org/abs/2508.11919)
*Pallavi Jain,Diego Marcos,Dino Ienco,Roberto Interdonato,Tristan Berchoux*

Main category: cs.CV

TL;DR: TimeSenCLIP 通过利用单个像素的时间和光谱维度，为大规模遥感应用提供了一种可扩展且高效的替代方案，从而最大限度地减少了对基于字幕的训练的需求。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在遥感应用中显示出巨大的前景，特别是通过零样本分类和检索进行土地利用和土地覆盖。然而，目前的方法面临两个主要挑战：依赖于增加计算成本的大空间瓦片，以及依赖于通常不易获得的基于文本的监督。

Method: TimeSenCLIP，一个轻量级框架，通过利用单个像素的时间和光谱维度来评估其有效性，用于分类土地利用和土地覆盖以及生态系统类型。利用来自 Sentinel-2 图像的光谱和时间信息，以及与地理标记的地面照片的交叉视角学习，最大限度地减少了对基于字幕的训练的需求，同时保持了 overhead（卫星）和地面视角之间的语义对齐。

Result: 证明了单像素输入，当与时间和光谱线索结合使用时，足以用于专题制图，为大规模遥感应用提供了一种可扩展且高效的替代方案。

Conclusion: 单像素输入结合时间和光谱线索足以用于专题制图，为大规模遥感应用提供了一种可扩展且高效的替代方案。

Abstract: Vision-language models have shown significant promise in remote sensing
applications, particularly for land-use and land-cover (LULC) via zero-shot
classification and retrieval. However, current approaches face two key
challenges: reliance on large spatial tiles that increase computational cost,
and dependence on text-based supervision, which is often not readily available.
In this work, we present TimeSenCLIP, a lightweight framework that reevaluate
the role of spatial context by evaluating the effectiveness of a single pixel
by leveraging its temporal and spectral dimensions, for classifying LULC and
ecosystem types. By leveraging spectral and temporal information from
Sentinel-2 imagery and cross-view learning with geo-tagged ground-level photos,
we minimises the need for caption-based training while preserving semantic
alignment between overhead (satellite) and ground perspectives. Our approach is
grounded in the LUCAS and Sen4Map datasets, and evaluated on classification
tasks including LULC, crop type, and ecosystem type. We demonstrate that single
pixel inputs, when combined with temporal and spectral cues, are sufficient for
thematic mapping, offering a scalable and efficient alternative for large-scale
remote sensing applications. Code is available at
https://github.com/pallavijain-pj/TimeSenCLIP

</details>


### [68] [Assessment of Using Synthetic Data in Brain Tumor Segmentation](https://arxiv.org/abs/2508.11922)
*Aditi Jahagirdar,Sameer Joshi*

Main category: cs.CV

TL;DR: Synthetic data can augment real data for brain tumor segmentation, but more work is needed to address class imbalance and data consistency.


<details>
  <summary>Details</summary>
Motivation: Manual brain tumor segmentation from MRI scans is challenging due to tumor heterogeneity, scarcity of annotated data, and class imbalance in medical imaging datasets. Synthetic data generated by generative models has the potential to mitigate these issues by improving dataset diversity.

Method: incorporating synthetic MRI data, generated using a pre-trained GAN model, into training a U-Net segmentation network

Result: overall quantitative performance was comparable between real-only and hybrid-trained models, qualitative inspection suggested that hybrid datasets improved whole tumor boundary delineation. However, region-wise accuracy for the tumor core and the enhancing tumor remained lower, indicating a persistent class imbalance.

Conclusion: The findings support the feasibility of synthetic data as an augmentation strategy for brain tumor segmentation, while highlighting the need for larger-scale experiments, volumetric data consistency, and mitigating class imbalance in future work.

Abstract: Manual brain tumor segmentation from MRI scans is challenging due to tumor
heterogeneity, scarcity of annotated data, and class imbalance in medical
imaging datasets. Synthetic data generated by generative models has the
potential to mitigate these issues by improving dataset diversity. This study
investigates, as a proof of concept, the impact of incorporating synthetic MRI
data, generated using a pre-trained GAN model, into training a U-Net
segmentation network. Experiments were conducted using real data from the BraTS
2020 dataset, synthetic data generated with the medigan library, and hybrid
datasets combining real and synthetic samples in varying proportions. While
overall quantitative performance (Dice coefficient, IoU, precision, recall,
accuracy) was comparable between real-only and hybrid-trained models,
qualitative inspection suggested that hybrid datasets, particularly with 40%
real and 60% synthetic data, improved whole tumor boundary delineation.
However, region-wise accuracy for the tumor core and the enhancing tumor
remained lower, indicating a persistent class imbalance. The findings support
the feasibility of synthetic data as an augmentation strategy for brain tumor
segmentation, while highlighting the need for larger-scale experiments,
volumetric data consistency, and mitigating class imbalance in future work.

</details>


### [69] [Deep Learning For Point Cloud Denoising: A Survey](https://arxiv.org/abs/2508.11932)
*Chengwei Zhang,Xueyi Zhang,Mingrui Lao,Tao Jiang,Xinhao Xu,Wenjie Li,Fubo Zhang,Longyong Chen*

Main category: cs.CV

TL;DR: 本文总结了基于深度学习的点云去噪方法，提出了一个分类法，并讨论了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现实世界环境中的点云不可避免地会表现出不同模式和强度的噪声。因此，点云去噪 (PCD) 作为提高下游任务性能的预处理步骤至关重要。尽管最近在性能方面取得了进展，但没有全面的综述系统地总结基于深度学习的PCD的发展。

Method: 将PCD формулируется 为一个两步过程：离群值去除和表面噪声恢复。

Result: 基于深度学习的PCD模型，以其强大的表示能力和灵活的架构而闻名，已经在去噪性能上超过了传统方法。比较了各种方法的异同和各自的优点。

Conclusion: 本文全面地总结了基于深度学习的点云去噪 (PCD) 的发展，并提出了针对去噪任务的分类法。最后，讨论了研究的局限性和未来的方向，为PCD的进一步发展提供了见解。

Abstract: Real-world environment-derived point clouds invariably exhibit noise across
varying modalities and intensities. Hence, point cloud denoising (PCD) is
essential as a preprocessing step to improve downstream task performance. Deep
learning (DL)-based PCD models, known for their strong representation
capabilities and flexible architectures, have surpassed traditional methods in
denoising performance. To our best knowledge, despite recent advances in
performance, no comprehensive survey systematically summarizes the developments
of DL-based PCD. To fill the gap, this paper seeks to identify key challenges
in DL-based PCD, summarizes the main contributions of existing methods, and
proposes a taxonomy tailored to denoising tasks. To achieve this goal, we
formulate PCD as a two-step process: outlier removal and surface noise
restoration, encompassing most scenarios and requirements of PCD. Additionally,
we compare methods in terms of similarities, differences, and respective
advantages. Finally, we discuss research limitations and future directions,
offering insights for further advancements in PCD.

</details>


### [70] [DynamicPose: Real-time and Robust 6D Object Pose Tracking for Fast-Moving Cameras and Objects](https://arxiv.org/abs/2508.11950)
*Tingbang Liang,Yixin Zeng,Jiatong Xie,Boyu Zhou*

Main category: cs.CV

TL;DR: DynamicPose, a retraining-free 6D pose tracking framework that improves tracking robustness in fast-moving camera and object scenarios


<details>
  <summary>Details</summary>
Motivation: improves tracking robustness in fast-moving camera and object scenarios. Previous work is mainly applicable to static or quasi-static scenes, and its performance significantly deteriorates when both the object and the camera move rapidly

Method: propose three synergistic components: (1) A visual-inertial odometry compensates for the shift in the Region of Interest (ROI) caused by camera motion; (2) A depth-informed 2D tracker corrects ROI deviations caused by large object translation; (3) A VIO-guided Kalman filter predicts object rotation, generates multiple candidate poses, and then obtains the final pose by hierarchical refinement

Result: Simulation and real-world experiments demonstrate the effectiveness of our method

Conclusion: achieving real-time and robust 6D pose tracking for fast-moving cameras and objects

Abstract: We present DynamicPose, a retraining-free 6D pose tracking framework that
improves tracking robustness in fast-moving camera and object scenarios.
Previous work is mainly applicable to static or quasi-static scenes, and its
performance significantly deteriorates when both the object and the camera move
rapidly. To overcome these challenges, we propose three synergistic components:
(1) A visual-inertial odometry compensates for the shift in the Region of
Interest (ROI) caused by camera motion; (2) A depth-informed 2D tracker
corrects ROI deviations caused by large object translation; (3) A VIO-guided
Kalman filter predicts object rotation, generates multiple candidate poses, and
then obtains the final pose by hierarchical refinement. The 6D pose tracking
results guide subsequent 2D tracking and Kalman filter updates, forming a
closed-loop system that ensures accurate pose initialization and precise pose
tracking. Simulation and real-world experiments demonstrate the effectiveness
of our method, achieving real-time and robust 6D pose tracking for fast-moving
cameras and objects.

</details>


### [71] [Transferable Class Statistics and Multi-scale Feature Approximation for 3D Object Detection](https://arxiv.org/abs/2508.11951)
*Hao Peng,Hong Sang,Yajing Ma,Ping Qiu,Chao Ji*

Main category: cs.CV

TL;DR: This paper proposes a computationally efficient method for object detection from point clouds by approximating multi-scale features and using transferable feature embedding.


<details>
  <summary>Details</summary>
Motivation: Multi-scale feature learning in point cloud object detection is computationally expensive and hinders lightweight models, especially with limited resources.

Method: The paper approximates point-based multi-scale features from a single neighborhood based on knowledge distillation and designs a transferable feature embedding mechanism using class-aware statistics. It also introduces the central weighted intersection over union for localization.

Result: Extensive experiments on public datasets demonstrate the effectiveness of the proposed method.

Conclusion: The proposed method is effective and saves computational costs, as demonstrated by experiments on public datasets.

Abstract: This paper investigates multi-scale feature approximation and transferable
features for object detection from point clouds. Multi-scale features are
critical for object detection from point clouds. However, multi-scale feature
learning usually involves multiple neighborhood searches and scale-aware
layers, which can hinder efforts to achieve lightweight models and may not be
conducive to research constrained by limited computational resources. This
paper approximates point-based multi-scale features from a single neighborhood
based on knowledge distillation. To compensate for the loss of constructive
diversity in a single neighborhood, this paper designs a transferable feature
embedding mechanism. Specifically, class-aware statistics are employed as
transferable features given the small computational cost. In addition, this
paper introduces the central weighted intersection over union for localization
to alleviate the misalignment brought by the center offset in optimization.
Note that the method presented in this paper saves computational costs.
Extensive experiments on public datasets demonstrate the effectiveness of the
proposed method.

</details>


### [72] [UniUGG: Unified 3D Understanding and Generation via Geometric-Semantic Encoding](https://arxiv.org/abs/2508.11952)
*Yueming Xu,Jiahui Zhang,Ze Huang,Yurui Chen,Yanpeng Zhou,Zhenyu Chen,Yu-Jie Yuan,Pengxiang Xia,Guowei Huang,Xinyue Cai,Zhongang Qi,Xingyue Quan,Jianye Hao,Hang Xu,Li Zhang*

Main category: cs.CV

TL;DR: UniUGG is a new unified framework for 3D understanding and generation using a spatial decoder and geometric-semantic learning.


<details>
  <summary>Details</summary>
Motivation: The integration of 3D tasks remains challenging and largely unexplored despite the progress on 2D image understanding and generation.

Method: The paper proposes a spatial decoder leveraging a latent diffusion model and a geometric-semantic learning strategy to pretrain the vision encoder.

Result: UniUGG achieves superior performance in visual representation, spatial understanding, and 3D generation.

Conclusion: The paper introduces UniUGG, a unified framework for 3D understanding and generation, and demonstrates its superiority in visual representation, spatial understanding, and 3D generation through experiments.

Abstract: Despite the impressive progress on understanding and generating images shown
by the recent unified architectures, the integration of 3D tasks remains
challenging and largely unexplored. In this paper, we introduce UniUGG, the
first unified understanding and generation framework for 3D modalities. Our
unified framework employs an LLM to comprehend and decode sentences and 3D
representations. At its core, we propose a spatial decoder leveraging a latent
diffusion model to generate high-quality 3D representations. This allows for
the generation and imagination of 3D scenes based on a reference image and an
arbitrary view transformation, while remaining supports for spatial visual
question answering (VQA) tasks. Additionally, we propose a geometric-semantic
learning strategy to pretrain the vision encoder. This design jointly captures
the input's semantic and geometric cues, enhancing both spatial understanding
and generation. Extensive experimental results demonstrate the superiority of
our method in visual representation, spatial understanding, and 3D generation.
The source code will be released upon paper acceptance.

</details>


### [73] [SAMDWICH: Moment-aware Video-text Alignment for Referring Video Object Segmentation](https://arxiv.org/abs/2508.11955)
*Seunghun Lee,Jiwan Seo,Jeonghoon Kim,Siwon Kim,Haeun Yun,Hyogyeong Jeon,Wonhyeok Choi,Jaehoon Jeong,Zane Durante,Sang Hyun Park,Sunghoon Im*

Main category: cs.CV

TL;DR: SAMDWICH通过引入时刻感知框架和选择性监督，解决了RVOS中的语义不对准问题，从而在MeViS基准测试中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常会遭受语义不对准的困扰，这主要是由于不分青红皂白的帧采样以及在训练期间对所有可见对象的监督-无论它们与表达的实际相关性如何。

Method: 我们引入了一个名为SAMDWICH的时刻感知RVOS框架，以及一个建立在具有挑战性的MeViS基准之上的新注释数据集MeViS-M。我们手动注释时间时刻，指示每个对象何时被表达式引用，从而实现语义接地的监督，从而加强视频文本对齐。SAMDWICH利用这些对齐的文本到剪辑对来指导训练，从而显着增强了指称理解。在此框架的基础上，我们提出了一种时刻引导的双路径传播（MDP），这是一种时刻感知的传播策略，通过在通过时刻为中心的记忆机制在相关和不相关的帧上进行训练，从而改善了对象的基础和跟踪。此外，我们还引入了对象级选择性监督（OSS），这是一种对象级过滤策略，该策略仅监督每个训练剪辑中与表达式在时间上对齐的对象。这种选择性监督减少了语义噪声并增强了语言条件学习。

Result: SAMDWICH在具有挑战性的MeViS基准测试中实现了最先进的性能，尤其是在涉及各种表达式的复杂场景中表现出色。

Conclusion: SAMDWICH在MeViS基准测试中实现了最先进的性能，尤其是在涉及各种表达式的复杂场景中表现出色。

Abstract: Referring Video Object Segmentation (RVOS) aims to segment and track objects
in videos based on natural language expressions, requiring precise alignment
between visual content and textual queries. However, existing methods often
suffer from semantic misalignment, largely due to indiscriminate frame sampling
and supervision of all visible objects during training -- regardless of their
actual relevance to the expression. To address this, we introduce a
moment-aware RVOS framework named SAMDWICH, along with a newly annotated
dataset, MeViS-M, built upon the challenging MeViS benchmark. We manually
annotate temporal moments indicating when each object is referred to by the
expression, enabling semantically grounded supervision that strengthens
video-text alignment. SAMDWICH leverages these aligned text-to-clip pairs to
guide training, significantly enhancing referential understanding. Building
upon this framework, we propose Moment-guided Dual-path Propagation (MDP), a
moment-aware propagation strategy that improves both object grounding and
tracking by training on both relevant and irrelevant frames through a
moment-centric memory mechanism. In addition, we introduce Object-level
Selective Supervision (OSS), an object-level filtering strategy that supervises
only the objects temporally aligned with the expression in each training clip.
This selective supervision reduces semantic noise and reinforces
language-conditioned learning. Extensive experiments show that SAMDWICH
achieves state-of-the-art performance on challenging MeViS benchmark,
particularly excelling in complex scenarios involving diverse expressions.

</details>


### [74] [PEdger++: Practical Edge Detection via Assembling Cross Information](https://arxiv.org/abs/2508.11961)
*Yuanbin Fu,Liang Li,Xiaojie Guo*

Main category: cs.CV

TL;DR: 提出了PEdger++，一个协同学习框架，旨在降低计算成本和模型大小，同时提高边缘检测准确性。


<details>
  <summary>Details</summary>
Motivation: 边缘检测是许多计算机视觉应用的关键基础，但深度学习方法通常计算成本高，限制了它们在资源受限设备上的应用。本文旨在实现高精度和低计算复杂性之间的平衡。

Method: 提出了PEdger++，一个旨在降低计算成本和模型大小同时提高边缘检测准确性的协同学习框架。

Result: 在BSDS500、NYUD和Multicue数据集上的大量实验结果表明了该方法的有效性，在定量和定性方面都显示出优于现有方法的明显改进。

Conclusion: PEdger++在多个数据集上表现出优于现有方法的改进，并且具有适应不同资源约束的适应性。

Abstract: Edge detection serves as a critical foundation for numerous computer vision
applications, including object detection, semantic segmentation, and image
editing, by extracting essential structural cues that define object boundaries
and salient edges. To be viable for broad deployment across devices with
varying computational capacities, edge detectors shall balance high accuracy
with low computational complexity. While deep learning has evidently improved
accuracy, they often suffer from high computational costs, limiting their
applicability on resource-constrained devices. This paper addresses the
challenge of achieving that balance: \textit{i.e.}, {how to efficiently capture
discriminative features without relying on large-size and sophisticated
models}. We propose PEdger++, a collaborative learning framework designed to
reduce computational costs and model sizes while improving edge detection
accuracy. The core principle of our PEdger++ is that cross-information derived
from heterogeneous architectures, diverse training moments, and multiple
parameter samplings, is beneficial to enhance learning from an ensemble
perspective. Extensive experimental results on the BSDS500, NYUD and Multicue
datasets demonstrate the effectiveness of our approach, both quantitatively and
qualitatively, showing clear improvements over existing methods. We also
provide multiple versions of the model with varying computational requirements,
highlighting PEdger++'s adaptability with respect to different resource
constraints. Codes are accessible at
https://github.com/ForawardStar/EdgeDetectionviaPEdgerPlus/.

</details>


### [75] [Exploring Spatial-Temporal Dynamics in Event-based Facial Micro-Expression Analysis](https://arxiv.org/abs/2508.11988)
*Nicolas Mastropasqua,Ignacio Bugueno-Cordova,Rodrigo Verschae,Daniel Acevedo,Pablo Negri,Maria E. Buemi*

Main category: cs.CV

TL;DR: A novel multi-resolution and multi-modal micro-expression dataset recorded with synchronized RGB and event cameras is introduced. Event data shows promising results in micro-expression recognition and frame reconstruction.


<details>
  <summary>Details</summary>
Motivation: Accurately capturing subtle and fast facial movements remains difficult when relying solely on RGB cameras, due to limitations in temporal resolution and sensitivity to motion blur. Event cameras offer an alternative, with microsecond-level precision, high dynamic range, and low latency. However, public datasets featuring event-based recordings of Action Units are still scarce.

Method: Action Unit classification using Spiking Neural Networks and frame reconstruction using Conditional Variational Autoencoders

Result: Action Unit classification using Spiking Neural Networks (51.23% accuracy with events vs. 23.12% with RGB), and frame reconstruction using Conditional Variational Autoencoders, achieving SSIM = 0.8513 and PSNR = 26.89 dB with high-resolution event input

Conclusion: event-based data can be used for micro-expression recognition and frame reconstruction

Abstract: Micro-expression analysis has applications in domains such as Human-Robot
Interaction and Driver Monitoring Systems. Accurately capturing subtle and fast
facial movements remains difficult when relying solely on RGB cameras, due to
limitations in temporal resolution and sensitivity to motion blur. Event
cameras offer an alternative, with microsecond-level precision, high dynamic
range, and low latency. However, public datasets featuring event-based
recordings of Action Units are still scarce. In this work, we introduce a
novel, preliminary multi-resolution and multi-modal micro-expression dataset
recorded with synchronized RGB and event cameras under variable lighting
conditions. Two baseline tasks are evaluated to explore the spatial-temporal
dynamics of micro-expressions: Action Unit classification using Spiking Neural
Networks (51.23\% accuracy with events vs. 23.12\% with RGB), and frame
reconstruction using Conditional Variational Autoencoders, achieving SSIM =
0.8513 and PSNR = 26.89 dB with high-resolution event input. These promising
results show that event-based data can be used for micro-expression recognition
and frame reconstruction.

</details>


### [76] [InstDrive: Instance-Aware 3D Gaussian Splatting for Driving Scenes](https://arxiv.org/abs/2508.12015)
*Hongyuan Liu,Haochen Yu,Jianfei Jiang,Qiankun Liu,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: InstDrive is the first framework to achieve 3D instance segmentation in dynamic, open-world driving scenes by using an instance-aware 3D Gaussian Splatting framework.


<details>
  <summary>Details</summary>
Motivation: reconstructing dynamic driving scenes from dashcam videos has attracted increasing attention due to its significance in autonomous driving and scene understanding. While recent advances have made impressive progress, most methods still unify all background elements into a single representation, hindering both instance-level understanding and flexible scene editing. Some approaches attempt to lift 2D segmentation into 3D space, but often rely on pre-processed instance IDs or complex pipelines to map continuous features to discrete identities. Moreover, these methods are typically designed for indoor scenes with rich viewpoints, making them less applicable to outdoor driving scenarios.

Method: an instance-aware 3D Gaussian Splatting framework tailored for the interactive reconstruction of dynamic driving scene. It uses masks generated by SAM as pseudo ground-truth to guide 2D feature learning via contrastive loss and pseudo-supervised objectives. At the 3D level, it introduce regularization to implicitly encode instance identities and enforce consistency through a voxel-based loss. A lightweight static codebook further bridges continuous features and discrete identities without requiring data pre-processing or complex optimization.

Result: Quantitative and qualitative experiments demonstrate the effectiveness of InstDrive

Conclusion: InstDrive achieves 3D instance segmentation in dynamic, open-world driving scenes.

Abstract: Reconstructing dynamic driving scenes from dashcam videos has attracted
increasing attention due to its significance in autonomous driving and scene
understanding. While recent advances have made impressive progress, most
methods still unify all background elements into a single representation,
hindering both instance-level understanding and flexible scene editing. Some
approaches attempt to lift 2D segmentation into 3D space, but often rely on
pre-processed instance IDs or complex pipelines to map continuous features to
discrete identities. Moreover, these methods are typically designed for indoor
scenes with rich viewpoints, making them less applicable to outdoor driving
scenarios. In this paper, we present InstDrive, an instance-aware 3D Gaussian
Splatting framework tailored for the interactive reconstruction of dynamic
driving scene. We use masks generated by SAM as pseudo ground-truth to guide 2D
feature learning via contrastive loss and pseudo-supervised objectives. At the
3D level, we introduce regularization to implicitly encode instance identities
and enforce consistency through a voxel-based loss. A lightweight static
codebook further bridges continuous features and discrete identities without
requiring data pre-processing or complex optimization. Quantitative and
qualitative experiments demonstrate the effectiveness of InstDrive, and to the
best of our knowledge, it is the first framework to achieve 3D instance
segmentation in dynamic, open-world driving scenes.More visualizations are
available at our project page.

</details>


### [77] [WiseLVAM: A Novel Framework For Left Ventricle Automatic Measurements](https://arxiv.org/abs/2508.12023)
*Durgesh Kumar Singh,Qing Cao,Sarina Thomas,Ahcène Boubekki,Robert Jenssen,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: WiseLVAM is a novel, fully automated yet manually adaptable framework for automatically placing the SL and then automatically performing the LV linear measurements in the AMM mode,utilizing the structure-awareness from B-mode images and the motion-awareness from AMM mode to enhance robustness and accuracy with the potential to provide a practical solution for the routine clinical application.


<details>
  <summary>Details</summary>
Motivation: Clinical guidelines recommend performing left ventricular (LV) linear measurements in B-mode echocardiographic images at the basal level -- typically at the mitral valve leaflet tips -- and aligned perpendicular to the LV long axis along a virtual scanline (SL). However, most automated methods estimate landmarks directly from B-mode images for the measurement task, where even small shifts in predicted points along the LV walls can lead to significant measurement errors, reducing their clinical reliability. A recent semi-automatic method, EnLVAM, addresses this limitation by constraining landmark prediction to a clinician-defined SL and training on generated Anatomical Motion Mode (AMM) images to predict LV landmarks along the same.

Method: a contour-aware SL placement approach, in which the LV contour is estimated using a weakly supervised B-mode landmark detector. SL placement is then performed by inferring the LV long axis and the basal level-mimicking clinical guidelines. Building on this foundation, we introduce WiseLVAM -- a novel, fully automated yet manually adaptable framework for automatically placing the SL and then automatically performing the LV linear measurements in the AMM mode

Result: fully automated yet manually adaptable framework for automatically placing the SL and then automatically performing the LV linear measurements in the AMM mode

Conclusion: WiseLVAM utilizes the structure-awareness from B-mode images and the motion-awareness from AMM mode to enhance robustness and accuracy with the potential to provide a practical solution for the routine clinical application.

Abstract: Clinical guidelines recommend performing left ventricular (LV) linear
measurements in B-mode echocardiographic images at the basal level -- typically
at the mitral valve leaflet tips -- and aligned perpendicular to the LV long
axis along a virtual scanline (SL). However, most automated methods estimate
landmarks directly from B-mode images for the measurement task, where even
small shifts in predicted points along the LV walls can lead to significant
measurement errors, reducing their clinical reliability. A recent
semi-automatic method, EnLVAM, addresses this limitation by constraining
landmark prediction to a clinician-defined SL and training on generated
Anatomical Motion Mode (AMM) images to predict LV landmarks along the same. To
enable full automation, a contour-aware SL placement approach is proposed in
this work, in which the LV contour is estimated using a weakly supervised
B-mode landmark detector. SL placement is then performed by inferring the LV
long axis and the basal level-mimicking clinical guidelines. Building on this
foundation, we introduce \textit{WiseLVAM} -- a novel, fully automated yet
manually adaptable framework for automatically placing the SL and then
automatically performing the LV linear measurements in the AMM mode.
\textit{WiseLVAM} utilizes the structure-awareness from B-mode images and the
motion-awareness from AMM mode to enhance robustness and accuracy with the
potential to provide a practical solution for the routine clinical application.

</details>


### [78] [Q-FSRU: Quantum-Augmented Frequency-Spectral Fusion for Medical Visual Question Answering](https://arxiv.org/abs/2508.12036)
*Rakesh Thakur,Yusra Tariq*

Main category: cs.CV

TL;DR: Q-FSRU是一种新的医学VQA模型，它结合了频率频谱表示和量子检索增强生成，以提高性能和可解释性，并在VQA-RAD数据集上优于之前的模型。


<details>
  <summary>Details</summary>
Motivation: 在医疗保健人工智能中，解决需要图像和文本理解的难题仍然是一个主要挑战。

Method: 该模型结合了频率频谱表示和融合（FSRU）与一种称为量子检索增强生成（Quantum RAG）的方法，用于医学视觉问答（VQA）。

Result: Q-FSRU模型在VQA-RAD数据集上的结果表明，其性能优于之前的模型，尤其是在需要图像-文本推理的复杂案例中。

Conclusion: Q-FSRU模型在VQA-RAD数据集上的表现优于之前的模型，特别是在需要图像-文本推理的复杂案例中。频率和量子信息的结合提高了性能和可解释性。该方法为构建智能、清晰和有用的医生AI工具提供了一种有前景的途径。

Abstract: Solving tough clinical questions that require both image and text
understanding is still a major challenge in healthcare AI. In this work, we
propose Q-FSRU, a new model that combines Frequency Spectrum Representation and
Fusion (FSRU) with a method called Quantum Retrieval-Augmented Generation
(Quantum RAG) for medical Visual Question Answering (VQA). The model takes in
features from medical images and related text, then shifts them into the
frequency domain using Fast Fourier Transform (FFT). This helps it focus on
more meaningful data and filter out noise or less useful information. To
improve accuracy and ensure that answers are based on real knowledge, we add a
quantum-inspired retrieval system. It fetches useful medical facts from
external sources using quantum-based similarity techniques. These details are
then merged with the frequency-based features for stronger reasoning. We
evaluated our model using the VQA-RAD dataset, which includes real radiology
images and questions. The results showed that Q-FSRU outperforms earlier
models, especially on complex cases needing image-text reasoning. The mix of
frequency and quantum information improves both performance and explainability.
Overall, this approach offers a promising way to build smart, clear, and
helpful AI tools for doctors.

</details>


### [79] [VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion Language Models](https://arxiv.org/abs/2508.12081)
*Haidong Xu,Guangwei Xu,Zhedong Zheng,Xiatian Zhu,Wei Ji,Xiangtai Li,Ruijie Guo,Meishan Zhang,Min zhang,Hao Fei*

Main category: cs.CV

TL;DR: VimoRAG是一种新颖的基于视频的检索增强运动生成框架，用于运动大型语言模型(llm)。


<details>
  <summary>Details</summary>
Motivation: 由于注释数据有限，运动llm面临严重的域外/词汇外问题，VimoRAG利用大规模的野生视频数据库，通过检索相关的2D人体运动信号来增强3D运动生成。

Method: 设计了Gemini运动视频检索器机制和以运动为中心的双重对齐DPO训练器。

Result: 实验结果表明，

Conclusion: VimoRAG显著提升了仅限文本输入的运动llm的性能。

Abstract: This paper introduces VimoRAG, a novel video-based retrieval-augmented motion
generation framework for motion large language models (LLMs). As motion LLMs
face severe out-of-domain/out-of-vocabulary issues due to limited annotated
data, VimoRAG leverages large-scale in-the-wild video databases to enhance 3D
motion generation by retrieving relevant 2D human motion signals. While
video-based motion RAG is nontrivial, we address two key bottlenecks: (1)
developing an effective motion-centered video retrieval model that
distinguishes human poses and actions, and (2) mitigating the issue of error
propagation caused by suboptimal retrieval results. We design the Gemini Motion
Video Retriever mechanism and the Motion-centric Dual-alignment DPO Trainer,
enabling effective retrieval and generation processes. Experimental results
show that VimoRAG significantly boosts the performance of motion LLMs
constrained to text-only input.

</details>


### [80] [Automated Model Evaluation for Object Detection via Prediction Consistency and Reliablity](https://arxiv.org/abs/2508.12082)
*Seungju Yoo,Hyuk Kwon,Joong-Won Hwang,Kibok Lee*

Main category: cs.CV

TL;DR: 我们提出了一种自动模型评估 (AutoEval) 框架，用于对象检测。它通过测量 NMS 前后框之间的空间一致性和重叠框的置信度分数来估计检测性能，而无需使用任何标签。


<details>
  <summary>Details</summary>
Motivation: 在现实世界的应用中评估物体检测器的性能仍然依赖于昂贵的手动注释。为了解决这个限制，我们开发了一个用于物体检测的自动化模型评估 (AutoEval) 框架。

Method: Prediction Consistency and Reliability (PCR)，它利用传统检测器在非极大值抑制 (NMS) 之前生成的多个候选边界框。PCR 通过联合测量 1) NMS 前后框之间的空间一致性，以及 2) 通过重叠框的置信度分数评估保留框的可靠性来估计检测性能，而无需 ground-truth 标签。

Result: PCR 产生比现有的 AutoEval 方法更准确的性能估计，并且提出的元数据集涵盖了更广泛的检测性能。

Conclusion: PCR 比现有的 AutoEval 方法产生更准确的性能估计，并且提出的元数据集涵盖了更广泛的检测性能。

Abstract: Recent advances in computer vision have made training object detectors more
efficient and effective; however, assessing their performance in real-world
applications still relies on costly manual annotation. To address this
limitation, we develop an automated model evaluation (AutoEval) framework for
object detection. We propose Prediction Consistency and Reliability (PCR),
which leverages the multiple candidate bounding boxes that conventional
detectors generate before non-maximum suppression (NMS). PCR estimates
detection performance without ground-truth labels by jointly measuring 1) the
spatial consistency between boxes before and after NMS, and 2) the reliability
of the retained boxes via the confidence scores of overlapping boxes. For a
more realistic and scalable evaluation, we construct a meta-dataset by applying
image corruptions of varying severity. Experimental results demonstrate that
PCR yields more accurate performance estimates than existing AutoEval methods,
and the proposed meta-dataset covers a wider range of detection performance.
The code is available at https://github.com/YonseiML/autoeval-det.

</details>


### [81] [Generic Event Boundary Detection via Denoising Diffusion](https://arxiv.org/abs/2508.12084)
*Jaejun Hwang,Dayoung Gong,Manjin Kim,Minsu Cho*

Main category: cs.CV

TL;DR: Introduces DiffGEBD, a diffusion-based model for generic event boundary detection, addressing the diversity of plausible solutions and achieving strong performance on standard benchmarks.


<details>
  <summary>Details</summary>
Motivation: Previous methods have focused on deterministic predictions, overlooking the diversity of plausible solutions for generic event boundary detection (GEBD).

Method: A novel diffusion-based boundary detection model (DiffGEBD) that encodes relevant changes across adjacent frames via temporal self-similarity and iteratively decodes random noise into plausible event boundaries.

Result: The method achieves strong performance on Kinetics-GEBD and TAPOS benchmarks, generating diverse and plausible event boundaries. A new evaluation metric is introduced to assess the quality of predictions considering both diversity and fidelity.

Conclusion: The proposed DiffGEBD model achieves strong performance on Kinetics-GEBD and TAPOS benchmarks, generating diverse and plausible event boundaries.

Abstract: Generic event boundary detection (GEBD) aims to identify natural boundaries
in a video, segmenting it into distinct and meaningful chunks. Despite the
inherent subjectivity of event boundaries, previous methods have focused on
deterministic predictions, overlooking the diversity of plausible solutions. In
this paper, we introduce a novel diffusion-based boundary detection model,
dubbed DiffGEBD, that tackles the problem of GEBD from a generative
perspective. The proposed model encodes relevant changes across adjacent frames
via temporal self-similarity and then iteratively decodes random noise into
plausible event boundaries being conditioned on the encoded features.
Classifier-free guidance allows the degree of diversity to be controlled in
denoising diffusion. In addition, we introduce a new evaluation metric to
assess the quality of predictions considering both diversity and fidelity.
Experiments show that our method achieves strong performance on two standard
benchmarks, Kinetics-GEBD and TAPOS, generating diverse and plausible event
boundaries.

</details>


### [82] [Enhancing 3D point accuracy of laser scanner through multi-stage convolutional neural network for applications in construction](https://arxiv.org/abs/2508.12089)
*Qinyuan Fan,Clemens Gühmann*

Main category: cs.CV

TL;DR: Using neural networks to improve the accuracy of low-end laser scanners by comparing them to high-accuracy scanners.


<details>
  <summary>Details</summary>
Motivation: Reducing uncertainty of 3D point accuracy of lasar scanner in rough indoor rooms, providing more accurate spatial measurements.

Method: A multi-stage convolutional neural network based integrated method.

Result: Measurement accuracy is significantly improved, with MSE reductions exceeding 70% and PSNR improvements of approximately 6 decibels.

Conclusion: The multi-stage convolutional neural network method significantly improves measurement accuracy, achieving MSE reductions exceeding 70% and PSNR improvements of approximately 6 decibels.

Abstract: We propose a multi-stage convolutional neural network (MSCNN) based
integrated method for reducing uncertainty of 3D point accuracy of lasar
scanner (LS) in rough indoor rooms, providing more accurate spatial
measurements for high-precision geometric model creation and renovation. Due to
different equipment limitations and environmental factors, high-end and low-end
LS have positional errors. Our approach pairs high-accuracy scanners (HAS) as
references with corresponding low-accuracy scanners (LAS) of measurements in
identical environments to quantify specific error patterns. By establishing a
statistical relationship between measurement discrepancies and their spatial
distribution, we develop a correction framework that combines traditional
geometric processing with targeted neural network refinement. This method
transforms the quantification of systematic errors into a supervised learning
problem, allowing precise correction while preserving critical geometric
features. Experimental results in our rough indoor rooms dataset show
significant improvements in measurement accuracy, with mean square error (MSE)
reductions exceeding 70% and peak signal-to-noise ratio (PSNR) improvements of
approximately 6 decibels. This approach enables low-end devices to achieve
measurement uncertainty levels approaching those of high-end devices without
hardware modifications.

</details>


### [83] [Error Propagation Mechanisms and Compensation Strategies for Quantized Diffusion](https://arxiv.org/abs/2508.12094)
*Songwei Liu,Hong Liu,Fangmin Chen,Xurui Peng,Chenqian Yan,Lean Fu,Xing Mei*

Main category: cs.CV

TL;DR: 扩散模型很棒，但计算量大。我们通过减少量化误差使它们更快更精确。


<details>
  <summary>Details</summary>
Motivation: 扩散模型通过建立前所未有的质量和创造力基准，改变了图像合成。然而，由于计算密集型的迭代去噪过程，它们的大规模部署面临挑战。虽然训练后量化(PTQ)为加速采样提供了一条有效的途径，但扩散模型的迭代性质导致逐步量化误差在生成过程中逐渐积累，不可避免地损害了输出的保真度。

Method: 开发了一个理论框架，该框架以数学方式公式化了扩散模型(DM)中的误差传播，推导了每步量化误差传播方程，并建立了累积误差的第一个封闭形式解。

Result: 在多个图像数据集上进行的大量实验表明，该补偿策略有效地缓解了误差传播，显著增强了现有的PTQ方法，从而在低精度扩散模型上实现了最先进的(SOTA)性能。

Conclusion: 通过提出时间步感知累积误差补偿方案，显著增强了现有的PTQ方法，从而在低精度扩散模型上实现了最先进的(SOTA)性能。

Abstract: Diffusion models have transformed image synthesis by establishing
unprecedented quality and creativity benchmarks. Nevertheless, their
large-scale deployment faces challenges due to computationally intensive
iterative denoising processes. Although post-training quantization(PTQ)
provides an effective pathway for accelerating sampling, the iterative nature
of diffusion models causes stepwise quantization errors to accumulate
progressively during generation, inevitably compromising output fidelity. To
address this challenge, we develop a theoretical framework that mathematically
formulates error propagation in Diffusion Models (DMs), deriving per-step
quantization error propagation equations and establishing the first closed-form
solution for cumulative error. Building on this theoretical foundation, we
propose a timestep-aware cumulative error compensation scheme. Extensive
experiments across multiple image datasets demonstrate that our compensation
strategy effectively mitigates error propagation, significantly enhancing
existing PTQ methods to achieve state-of-the-art(SOTA) performance on
low-precision diffusion models.

</details>


### [84] [VELVET-Med: Vision and Efficient Language Pre-training for Volumetric Imaging Tasks in Medicine](https://arxiv.org/abs/2508.12108)
*Ziyang Zhang,Yang Yu,Xulei Yang,Si Yong Yeo*

Main category: cs.CV

TL;DR: propose a novel vision-language pre-training (VLP) framework, termed as VELVET-Med, specifically designed for limited volumetric data such as 3D CT and associated radiology reports,Using only 38,875 scan-report pairs, our approach seeks to uncover rich spatial and semantic relationships embedded in volumetric medical images and corresponding clinical narratives, thereby enhancing the generalization ability of the learned encoders


<details>
  <summary>Details</summary>
Motivation: curating large-scale paired data in the medical field for volumetric modalities such as CT scans remains a challenging and time-intensive process. This difficulty often limits the performance on downstream tasks

Method: incorporate uni-modal self-supervised learning into VLP framework; propose a novel language encoder, termed as TriBERT, for learning multi-level textual semantics; devise the hierarchical contrastive learning to capture multi-level vision-language correspondence

Result: achieving state-of-the-art performance across a wide range of downstream tasks, including 3D segmentation, cross-modal retrieval, visual question answering, and report generation.

Conclusion: The resulting encoders exhibit strong transferability, achieving state-of-the-art performance across a wide range of downstream tasks, including 3D segmentation, cross-modal retrieval, visual question answering, and report generation.

Abstract: Vision-and-language models (VLMs) have been increasingly explored in the
medical domain, particularly following the success of CLIP in general domain.
However, unlike the relatively straightforward pairing of 2D images and text,
curating large-scale paired data in the medical field for volumetric modalities
such as CT scans remains a challenging and time-intensive process. This
difficulty often limits the performance on downstream tasks. To address these
challenges, we propose a novel vision-language pre-training (VLP) framework,
termed as \textbf{VELVET-Med}, specifically designed for limited volumetric
data such as 3D CT and associated radiology reports. Instead of relying on
large-scale data collection, our method focuses on the development of effective
pre-training objectives and model architectures. The key contributions are: 1)
We incorporate uni-modal self-supervised learning into VLP framework, which are
often underexplored in the existing literature. 2) We propose a novel language
encoder, termed as \textbf{TriBERT}, for learning multi-level textual
semantics. 3) We devise the hierarchical contrastive learning to capture
multi-level vision-language correspondence. Using only 38,875 scan-report
pairs, our approach seeks to uncover rich spatial and semantic relationships
embedded in volumetric medical images and corresponding clinical narratives,
thereby enhancing the generalization ability of the learned encoders. The
resulting encoders exhibit strong transferability, achieving state-of-the-art
performance across a wide range of downstream tasks, including 3D segmentation,
cross-modal retrieval, visual question answering, and report generation.

</details>


### [85] [Simple o3: Towards Interleaved Vision-Language Reasoning](https://arxiv.org/abs/2508.12109)
*Ye Wang,Qianglong Chen,Zejun Li,Siyuan Wang,Shijie Guo,Zhirui Zhang,Zhongyu Wei*

Main category: cs.CV

TL;DR: Simple o3通过集成动态工具交互，增强了多模态大型语言模型在视觉语言推理方面的能力，并在各种基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 多模态大型语言模型 (MLLM) 在视觉语言任务上表现出了令人印象深刻的性能，但它们在多模态场景中的长链思维 (CoT) 能力仍有待探索。

Method: Simple o3，一个端到端框架，它通过监督微调 (SFT) 将动态工具交互（例如，裁剪、缩放和重用）集成到交错的视觉语言推理中。

Result: 通过引入额外的视觉token进行交错的视觉语言推理，重用和放大原始图像显着提高了模型的视觉推理和细粒度感知，而基于精确视觉基础的图像裁剪允许模型有效地关注关键实体或区域，进一步增强了其能力。

Conclusion: Simple o3在各种基准测试中表现出色，超越了现有方法。通过结合增强的推理能力，Simple o3 建立了一个强大且计算上可负担的范例，用于推进多模态推理。

Abstract: Multimodal Large Language Models (MLLMs) have shown impressive performance on
vision-language tasks, but their long Chain-of-Thought (CoT) capabilities in
multimodal scenarios remain underexplored. Inspired by OpenAI's o3 model, which
emulates human-like ''thinking with image'' through iterative visual
transformations and linguistic reasoning, we propose Simple o3, an end-to-end
framework that integrates dynamic tool interactions (e.g., cropping, zooming,
and reusing) into interleaved vision-language reasoning via supervised
fine-tuning (SFT). Our approach features a scalable data synthesis pipeline
that generates high-quality interleaved vision-language reasoning chains via an
''observe-reason-act'' cycle, complete with executable visual operations and
rigorous verification, yielding the open-source TWI-Tools-146K dataset.
Experimental results demonstrate Simple o3's superior performance on diverse
benchmarks, outperforming existing approaches. By combining enhanced reasoning
capabilities, Simple o3 establishes a powerful yet computationally affordable
paradigm for advancing multimodal reasoning. Remarkably, we provide the first
in-depth analysis of different interleaved reasoning strategies, offering
insights into their impact on model performance. We found that by introducing
additional visual tokens for interleaved vision-language reasoning, reusing and
magnifying the original image significantly improves the model's visual
reasoning and fine-grained perception, while image cropping based on precise
visual grounding allows the model to effectively focus on key entities or
regions, further enhancing its capabilities.

</details>


### [86] [DualFit: A Two-Stage Virtual Try-On via Warping and Synthesis](https://arxiv.org/abs/2508.12131)
*Minh Tran,Johnmark Clements,Annie Prasanna,Tri Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: DualFit, a two-stage hybrid VTON pipeline, warps garments and synthesizes the final output by blending the warped garment with preserved human regions, achieving visually seamless try-on results while faithfully maintaining high-frequency garment details.


<details>
  <summary>Details</summary>
Motivation: Recent advances in diffusion-based warping-free methods have improved perceptual quality, they often fail to preserve fine-grained garment details such as logos and printed text elements that are critical for brand integrity and customer trust.

Method: DualFit, a hybrid VTON pipeline that addresses this limitation by two-stage approach. In the first stage, DualFit warps the target garment to align with the person image using a learned flow field. In the second stage, a fidelity-preserving try-on module synthesizes the final output by blending the warped garment with preserved human regions. Particularly, to guide this process, we introduce a preserved-region input and an inpainting mask

Result: achieves visually seamless try-on results while faithfully maintaining high-frequency garment details

Conclusion: DualFit achieves visually seamless try-on results while faithfully maintaining high-frequency garment details, striking an effective balance between reconstruction accuracy and perceptual realism.

Abstract: Virtual Try-On technology has garnered significant attention for its
potential to transform the online fashion retail experience by allowing users
to visualize how garments would look on them without physical trials. While
recent advances in diffusion-based warping-free methods have improved
perceptual quality, they often fail to preserve fine-grained garment details
such as logos and printed text elements that are critical for brand integrity
and customer trust. In this work, we propose DualFit, a hybrid VTON pipeline
that addresses this limitation by two-stage approach. In the first stage,
DualFit warps the target garment to align with the person image using a learned
flow field, ensuring high-fidelity preservation. In the second stage, a
fidelity-preserving try-on module synthesizes the final output by blending the
warped garment with preserved human regions. Particularly, to guide this
process, we introduce a preserved-region input and an inpainting mask, enabling
the model to retain key areas and regenerate only where necessary, particularly
around garment seams. Extensive qualitative results show that DualFit achieves
visually seamless try-on results while faithfully maintaining high-frequency
garment details, striking an effective balance between reconstruction accuracy
and perceptual realism.

</details>


### [87] [TriQDef: Disrupting Semantic and Gradient Alignment to Prevent Adversarial Patch Transferability in Quantized Neural Networks](https://arxiv.org/abs/2508.12132)
*Amira Guesmi,Bassem Ouni,Muhammad Shafique*

Main category: cs.CV

TL;DR: TriQDef 是一种三级量化感知防御框架，旨在破坏基于 patch 的对抗性攻击在 QNN 上的可迁移性，通过扰乱语义和感知梯度对齐来实现。


<details>
  <summary>Details</summary>
Motivation: 量化神经网络 (QNN) 由于其在计算和内存使用方面的效率而被越来越多地部署在边缘和资源受限的环境中。尽管已显示出扭曲梯度 landscape 并削弱传统的像素级攻击，但它对基于 patch 的对抗性攻击的鲁棒性有限，此类攻击具有 localized、高显著性扰动，并且在不同位宽之间仍然具有 surprising 的可迁移性。现有的防御要么过拟合到固定的量化设置，要么无法解决这种跨位泛化漏洞。

Method: TriQDef，一个三级量化感知防御框架，包含特征失调惩罚 (FDP)、梯度感知不协调惩罚 (GPDP) 和联合量化感知训练协议。

Result: TriQDef 在未见过的 patch 和量化组合上将攻击成功率 (ASR) 降低了 40% 以上，同时保持了较高的 clean 准确率。

Conclusion: TriQDef 通过扰乱语义和感知梯度对齐来缓解 QNN 中的 patch 可迁移性，并在 CIFAR-10 和 ImageNet 上实现了超过 40% 的攻击成功率降低。

Abstract: Quantized Neural Networks (QNNs) are increasingly deployed in edge and
resource-constrained environments due to their efficiency in computation and
memory usage. While shown to distort the gradient landscape and weaken
conventional pixel-level attacks, it provides limited robustness against
patch-based adversarial attacks-localized, high-saliency perturbations that
remain surprisingly transferable across bit-widths. Existing defenses either
overfit to fixed quantization settings or fail to address this cross-bit
generalization vulnerability. We introduce \textbf{TriQDef}, a tri-level
quantization-aware defense framework designed to disrupt the transferability of
patch-based adversarial attacks across QNNs. TriQDef consists of: (1) a Feature
Disalignment Penalty (FDP) that enforces semantic inconsistency by penalizing
perceptual similarity in intermediate representations; (2) a Gradient
Perceptual Dissonance Penalty (GPDP) that explicitly misaligns input gradients
across bit-widths by minimizing structural and directional agreement via Edge
IoU and HOG Cosine metrics; and (3) a Joint Quantization-Aware Training
Protocol that unifies these penalties within a shared-weight training scheme
across multiple quantization levels. Extensive experiments on CIFAR-10 and
ImageNet demonstrate that TriQDef reduces Attack Success Rates (ASR) by over
40\% on unseen patch and quantization combinations, while preserving high clean
accuracy. Our findings underscore the importance of disrupting both semantic
and perceptual gradient alignment to mitigate patch transferability in QNNs.

</details>


### [88] [Infusing fine-grained visual knowledge to Vision-Language Models](https://arxiv.org/abs/2508.12137)
*Nikolaos-Antonios Ypsilantis,Kaifeng Chen,André Araujo,Ondřej Chum*

Main category: cs.CV

TL;DR: The paper introduces a fine-tuning method for VLMs that balances domain adaptation with retention of broad knowledge, achieving strong retrieval results without forgetting or using text data during fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Pre-trained Vision-and-Language Models (VLMs) are suboptimal for fine-grained open-set visual retrieval, and fine-tuning leads to catastrophic forgetting of general-purpose capabilities.

Method: A fine-tuning method is proposed, combining regularization techniques for knowledge retention with careful validation set design and hyperparameter tuning.

Result: The method consistently achieves strong results on fine-grained and coarse-grained image-image and image-text retrieval benchmarks.

Conclusion: The proposed fine-tuning method achieves strong results on both fine-grained and coarse-grained image-image and image-text retrieval benchmarks, retaining visual-text alignment without using text data or the original text encoder during fine-tuning.

Abstract: Large-scale contrastive pre-training produces powerful Vision-and-Language
Models (VLMs) capable of generating representations (embeddings) effective for
a wide variety of visual and multimodal tasks. However, these pretrained
embeddings remain suboptimal for fine-grained open-set visual retrieval, where
state-of-the-art results require fine-tuning the vision encoder using annotated
domain-specific samples. Naively performing such fine-tuning typically leads to
catastrophic forgetting, severely diminishing the model's general-purpose
visual and cross-modal capabilities.
  In this work, we propose a fine-tuning method explicitly designed to achieve
optimal balance between fine-grained domain adaptation and retention of the
pretrained VLM's broad multimodal knowledge. Drawing inspiration from continual
learning literature, we systematically analyze standard regularization
techniques aimed at knowledge retention and propose an efficient and effective
combination strategy. Additionally, we address the commonly overlooked yet
critical aspects of validation set design and hyperparameter tuning to ensure
reproducibility and robust generalization across datasets and pretrained
models. We extensively evaluate our method on both fine-grained and
coarse-grained image-image and image-text retrieval benchmarks. Our approach
consistently achieves strong results, notably retaining the visual-text
alignment without utilizing any text data or the original text encoder during
fine-tuning. Code and model checkpoints: https://github.com/nikosips/infusing .

</details>


### [89] [KP-INR: A Dual-Branch Implicit Neural Representation Model for Cardiac Cine MRI Reconstruction](https://arxiv.org/abs/2508.12147)
*Donghang Lyu,Marius Staring,Mariya Doneva,Hildo J. Lamb,Nicola Pezzotti*

Main category: cs.CV

TL;DR: 提出KP-INR，一种用于心脏电影MRI重建的双分支INR方法，通过处理位置嵌入和学习局部多尺度k空间特征表示，在快速MRI重建中实现了高性能。


<details>
  <summary>Details</summary>
Motivation: 为了减少扫描时间和屏气不适，快速采集技术被利用，但降低了图像质量。当前的INR方法主要侧重于使用基于坐标的位置嵌入来学习映射，而忽略了目标点及其相邻上下文的特征表示。

Method: 提出了一种双分支INR方法KP-INR，该方法在k空间中运行，用于心脏电影MRI重建：一个分支处理k空间坐标的位置嵌入，而另一个分支从这些坐标的局部多尺度k空间特征表示中学习。

Result: 通过实现跨分支交互并从两个分支逼近目标k空间值，KP-INR可以在具有挑战性的笛卡尔k空间数据上实现强大的性能。

Conclusion: KP-INR在CMRxRecon2024数据集上表现优于基线模型，并突出了其在该领域的潜力。

Abstract: Cardiac Magnetic Resonance (CMR) imaging is a non-invasive method for
assessing cardiac structure, function, and blood flow. Cine MRI extends this by
capturing heart motion, providing detailed insights into cardiac mechanics. To
reduce scan time and breath-hold discomfort, fast acquisition techniques have
been utilized at the cost of lowering image quality. Recently, Implicit Neural
Representation (INR) methods have shown promise in unsupervised reconstruction
by learning coordinate-to-value mappings from undersampled data, enabling
high-quality image recovery. However, current existing INR methods primarily
focus on using coordinate-based positional embeddings to learn the mapping,
while overlooking the feature representations of the target point and its
neighboring context. In this work, we propose KP-INR, a dual-branch INR method
operating in k-space for cardiac cine MRI reconstruction: one branch processes
the positional embedding of k-space coordinates, while the other learns from
local multi-scale k-space feature representations at those coordinates. By
enabling cross-branch interaction and approximating the target k-space values
from both branches, KP-INR can achieve strong performance on challenging
Cartesian k-space data. Experiments on the CMRxRecon2024 dataset confirms its
improved performance over baseline models and highlights its potential in this
field.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [90] [Finite Automata Extraction: Low-data World Model Learning as Programs from Gameplay Video](https://arxiv.org/abs/2508.11836)
*Dave Goel,Matthew Guzdial,Anurag Sarkar*

Main category: cs.AI

TL;DR: learns a neuro-symbolic world model from gameplay video


<details>
  <summary>Details</summary>
Motivation: transfer of the learned environment dynamics and explainability a challenge

Method: learns a neuro-symbolic world model from gameplay video represented as programs in a novel domain-specific language (DSL): Retro Coder

Result: learns a more precise model of the environment and more general code than prior DSL-based approaches

Conclusion: FAE learns a more precise model of the environment.

Abstract: World models are defined as a compressed spatial and temporal learned
representation of an environment. The learned representation is typically a
neural network, making transfer of the learned environment dynamics and
explainability a challenge. In this paper, we propose an approach, Finite
Automata Extraction (FAE), that learns a neuro-symbolic world model from
gameplay video represented as programs in a novel domain-specific language
(DSL): Retro Coder. Compared to prior world model approaches, FAE learns a more
precise model of the environment and more general code than prior DSL-based
approaches.

</details>


### [91] [EvoCut: Strengthening Integer Programs via Evolution-Guided Language Models](https://arxiv.org/abs/2508.11850)
*Milad Yazdani,Mahdi Mostajabdaveh,Samin Aref,Zirui Zhou*

Main category: cs.AI

TL;DR: EvoCut automates the generation of acceleration cuts for integer programming using LLMs and evolutionary search, achieving significant improvements in solver performance without human expertise.


<details>
  <summary>Details</summary>
Motivation: Integer programming is crucial for combinatorial optimization but challenging due to its NP-hard nature. Manual design of acceleration cuts is effective but requires deep expertise and is not automated.

Method: EvoCut combines large language models (LLMs) with an evolutionary search to automate the generation of acceleration cuts.

Result: EvoCut reduces optimality gap by 17-57% within a fixed time, obtains the same solutions up to 4 times as fast, and obtains higher-quality solutions within the same time limit compared to standard integer programming practice.

Conclusion: EvoCut reliably generates, improves, and empirically verifies cuts that generalize to unseen instances without human expert input.

Abstract: Integer programming lies at the heart of crucial combinatorial optimization
tasks but remains challenging due to its NP-hard nature. An effective approach
for practically solving integer programs is the manual design of acceleration
cuts, i.e. inequalities that improve solver performance. However, this creative
process demands deep expertise and is yet to be automated. Our proposed
framework, EvoCut, automates the generation of acceleration cuts by combining
large language models (LLMs) with an evolutionary search. EvoCut (i)
initializes a diverse population of candidate cuts via an LLM-based initializer
agent; (ii) for each cut empirically evaluates both preservation of the optimal
solution and its ability to cut off fractional solutions across a verification
set; and (iii) iteratively refines the population through evolutionary
crossover and mutation agents. We quantify each cut's utility by its relative
reduction in the solver's optimality gap. Our comparisons against standard
integer programming practice show that EvoCut reduces optimality gap by 17-57%
within a fixed time. It obtains the same solutions up to 4 times as fast, and
obtains higher-quality solutions within the same time limit. Requiring no human
expert input, EvoCut reliably generates, improves, and empirically verifies
cuts that generalize to unseen instances. The code is available at
https://github.com/milad1378yz/EvoCut.

</details>


### [92] [LARC: Towards Human-level Constrained Retrosynthesis Planning through an Agentic Framework](https://arxiv.org/abs/2508.11860)
*Frazier N. Baker,Daniel Adu-Ampratwum,Reza Averly,Botao Yu,Huan Sun,Xia Ning*

Main category: cs.AI

TL;DR: LARC is the first LLM-based Agentic framework for Retrosynthesis planning under Constraints. It outperforms LLM baselines and approaches human expert-level success in substantially less time.


<details>
  <summary>Details</summary>
Motivation: Constrained retrosynthesis planning is an essential, yet challenging, process within chemistry for identifying synthetic routes from commercially available starting materials to desired target molecules, subject to practical constraints.

Method: LARC incorporates agentic constraint evaluation, through an Agent-as-a-Judge, directly into the retrosynthesis planning process, using agentic feedback grounded in tool-based reasoning to guide and constrain route generation.

Result: LARC achieves a 72.9% success rate on these tasks, vastly outperforming LLM baselines and approaching human expert-level success in substantially less time.

Conclusion: The LARC framework is extensible, and serves as a first step towards an effective agentic tool or a co-scientist to human experts for constrained retrosynthesis.

Abstract: Large language model (LLM) agent evaluators leverage specialized tools to
ground the rational decision-making of LLMs, making them well-suited to aid in
scientific discoveries, such as constrained retrosynthesis planning.
Constrained retrosynthesis planning is an essential, yet challenging, process
within chemistry for identifying synthetic routes from commercially available
starting materials to desired target molecules, subject to practical
constraints. Here, we present LARC, the first LLM-based Agentic framework for
Retrosynthesis planning under Constraints. LARC incorporates agentic constraint
evaluation, through an Agent-as-a-Judge, directly into the retrosynthesis
planning process, using agentic feedback grounded in tool-based reasoning to
guide and constrain route generation. We rigorously evaluate LARC on a
carefully curated set of 48 constrained retrosynthesis planning tasks across 3
constraint types. LARC achieves a 72.9% success rate on these tasks, vastly
outperforming LLM baselines and approaching human expert-level success in
substantially less time. The LARC framework is extensible, and serves as a
first step towards an effective agentic tool or a co-scientist to human experts
for constrained retrosynthesis.

</details>


### [93] [QuarkMed Medical Foundation Model Technical Report](https://arxiv.org/abs/2508.11894)
*Ao Li,Bin Yan,Bingfeng Cai,Chenxi Li,Cunzhong Zhao,Fugen Yao,Gaoqiang Liu,Guanjun Jiang,Jian Xu,Liang Dong,Liansheng Sun,Rongshen Zhang,Xiaolei Gui,Xin Liu,Xin Shang,Yao Wu,Yu Cao,Zhenxin Ma,Zhuang Jia*

Main category: cs.AI

TL;DR: QuarkMed是一个高性能医学基础模型，它利用医学数据处理、RAG和强化学习，在中国医师资格考试中达到了70%的准确率，并已服务数百万用户。


<details>
  <summary>Details</summary>
Motivation: 医疗任务通常需要高度专业的知识、专业的准确性和定制能力，因此需要一个强大而可靠的基础模型。

Method: 利用精选的医疗数据处理、医疗内容检索增强生成（RAG）和大规模、可验证的强化学习管道来开发高性能医疗基础模型。

Result: 该模型在中国医师资格考试中取得了70%的准确率，证明了在各种医疗基准测试中具有很强的泛化能力。

Conclusion: QuarkMed提供了一个强大且通用的个人医疗AI解决方案，已经在ai.quark.cn上为数百万用户提供服务。

Abstract: Recent advancements in large language models have significantly accelerated
their adoption in healthcare applications, including AI-powered medical
consultations, diagnostic report assistance, and medical search tools. However,
medical tasks often demand highly specialized knowledge, professional accuracy,
and customization capabilities, necessitating a robust and reliable foundation
model. QuarkMed addresses these needs by leveraging curated medical data
processing, medical-content Retrieval-Augmented Generation (RAG), and a
large-scale, verifiable reinforcement learning pipeline to develop a
high-performance medical foundation model. The model achieved 70% accuracy on
the Chinese Medical Licensing Examination, demonstrating strong generalization
across diverse medical benchmarks. QuarkMed offers a powerful yet versatile
personal medical AI solution, already serving over millions of users at
ai.quark.cn.

</details>


### [94] [CHBench: A Cognitive Hierarchy Benchmark for Evaluating Strategic Reasoning Capability of LLMs](https://arxiv.org/abs/2508.11944)
*Hongtao Liu,Zhicheng Du,Zihe Wang,Weiran Shen*

Main category: cs.AI

TL;DR: 本文提出了认知层次基准（CHBench），一个新的评估框架，以评估LLM的战略推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的研究依赖于效用性能指标，由于对手行为和博弈结构的变化，这些指标不够稳健。为了解决这个局限性，

Method: 我们提出了认知层次基准（CHBench），这是一个受到行为经济学中认知层次模型启发的新的评估框架。我们通过一个三阶段的系统框架评估LLM的战略推理能力，利用来自六个最先进的LLM在十五个精心选择的范式博弈中的行为数据。

Result: 实验表明，LLM在不同的对手中表现出一致的战略推理水平，证实了该框架的稳健性和泛化能力。聊天机制会显著降低战略推理能力，而记忆机制会提高战略推理能力。

Conclusion: CHBench是一个有前景的评估LLM能力的工具，对未来的研究和实际应用具有巨大的潜力。

Abstract: Game-playing ability serves as an indicator for evaluating the strategic
reasoning capability of large language models (LLMs). While most existing
studies rely on utility performance metrics, which are not robust enough due to
variations in opponent behavior and game structure. To address this limitation,
we propose \textbf{Cognitive Hierarchy Benchmark (CHBench)}, a novel evaluation
framework inspired by the cognitive hierarchy models from behavioral economics.
We hypothesize that agents have bounded rationality -- different agents behave
at varying reasoning depths/levels. We evaluate LLMs' strategic reasoning
through a three-phase systematic framework, utilizing behavioral data from six
state-of-the-art LLMs across fifteen carefully selected normal-form games.
Experiments show that LLMs exhibit consistent strategic reasoning levels across
diverse opponents, confirming the framework's robustness and generalization
capability. We also analyze the effects of two key mechanisms (Chat Mechanism
and Memory Mechanism) on strategic reasoning performance. Results indicate that
the Chat Mechanism significantly degrades strategic reasoning, whereas the
Memory Mechanism enhances it. These insights position CHBench as a promising
tool for evaluating LLM capabilities, with significant potential for future
research and practical applications.

</details>


### [95] [Data Mixing Optimization for Supervised Fine-Tuning of Large Language Models](https://arxiv.org/abs/2508.11953)
*Yuan Li,Zhengzhong Liu,Eric Xing*

Main category: cs.AI

TL;DR: This paper introduces a novel data mixing method for supervised fine-tuning (SFT) of large language models (LLMs) that minimizes validation loss. The method achieves performance on par with grid search and improves both validation loss and downstream performance when reweighting popular SFT datasets.


<details>
  <summary>Details</summary>
Motivation: Optimizing data mixtures for supervised fine-tuning (SFT) of large language models (LLMs) is critical for developing general-purpose models, yet this area remains underexplored. In this paper, data mixing is framed as an optimization problem.

Method: This paper introduces a novel method designed to minimize validation loss. The approach parametrizes the loss by modeling effective data transferred and leveraging scaling laws for fine-tuning. By experimenting with various small-scale data mixtures, the parameters are fitted and the optimal weights are derived.

Result: The algorithm achieves excellent overall and individual performance across all domains. Models trained with optimized weights perform on par with those using optimal weights determined via grid search, with per-domain loss only 0.66% higher than the best domain loss from grid search on average. Reweighting popular SFT datasets improves both validation loss and downstream performance.

Conclusion: The algorithm achieves excellent overall and individual performance across all domains. Models trained with optimized weights perform on par with those using optimal weights determined via grid search, with per-domain loss only 0.66% higher than the best domain loss from grid search on average. Reweighting popular SFT datasets improves both validation loss and downstream performance. The method can generalize to guide data selection for domain-specific models and provide insights into SFT.

Abstract: Optimizing data mixtures for supervised fine-tuning (SFT) of large language
models (LLMs) is critical for developing general-purpose models, yet this area
remains underexplored. In this paper, we frame data mixing as an optimization
problem and introduce a novel method designed to minimize validation loss. Our
approach parametrizes the loss by modeling effective data transferred and
leveraging scaling laws for fine-tuning. By experimenting with various
small-scale data mixtures, we fit these parameters and derive the optimal
weights. We provide both mathematical proofs and empirical results
demonstrating that our algorithm achieves excellent overall and individual
performance across all domains. Through controlled experiments, we show that
models trained with our optimized weights perform on par with those using
optimal weights determined via grid search, with per-domain loss only 0.66%
higher than the best domain loss from grid search on average. Additionally, we
show that reweighting popular SFT datasets using our method improves both
validation loss and downstream performance. Finally, we discuss how our method
can generalize to guide data selection for domain-specific models and provide
insights into SFT.

</details>


### [96] [UniCast: A Unified Multimodal Prompting Framework for Time Series Forecasting](https://arxiv.org/abs/2508.11954)
*Sehyuk Park,Soyeon Caren Han,Eduard Hovy*

Main category: cs.AI

TL;DR: UniCast是一种新颖的参数高效多模态框架，它扩展了TSFM，以共同利用时间序列、视觉和文本模态，从而增强预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的时间序列基础模型（TSFM）主要在单模态环境中运行，忽略了丰富的多模态上下文，例如视觉和文本信号，这些信号通常伴随现实场景中的时间序列数据。

Method: 通过软提示调整，UniCast集成了来自预训练的视觉和文本编码器的模态特定嵌入以及冻结的TSFM，从而能够以最少的参数更新进行有效适应。

Result: UniCast始终且显着地优于所有现有的TSFM基线。

Conclusion: UniCast在各种时间序列预测基准上始终且显着地优于所有现有的TSFM基线。研究结果强调了多模态上下文在推进下一代通用时间序列预测器中的关键作用。

Abstract: Time series forecasting is a foundational task across domains, such as
finance, healthcare, and environmental monitoring. While recent advances in
Time Series Foundation Models (TSFMs) have demonstrated strong generalisation
through large-scale pretraining, existing models operate predominantly in a
unimodal setting, ignoring the rich multimodal context, such as visual and
textual signals, that often accompanies time series data in real-world
scenarios. This paper introduces a novel parameter-efficient multimodal
framework, UniCast, that extends TSFMs to jointly leverage time series, vision,
and text modalities for enhanced forecasting performance. Our method integrates
modality-specific embeddings from pretrained Vision and Text Encoders with a
frozen TSFM via soft prompt tuning, enabling efficient adaptation with minimal
parameter updates. This design not only preserves the generalisation strength
of the foundation model but also enables effective cross-modal interaction.
Extensive experiments across diverse time-series forecasting benchmarks
demonstrate that UniCast consistently and significantly outperforms all
existing TSFM baselines. The findings highlight the critical role of multimodal
context in advancing the next generation of general-purpose time series
forecasters.

</details>


### [97] [Rigorous Feature Importance Scores based on Shapley Value and Banzhaf Index](https://arxiv.org/abs/2508.11959)
*Xuanxiang Huang,Olivier Létoffé,Joao Marques-Silva*

Main category: cs.AI

TL;DR: 本文利用 Shapley 值和 Banzhaf 指数，考虑非 WAXp 集合，提出了两种新的特征重要性评分，以量化特征在排除对抗样本方面的有效性，并研究了这些评分的属性和计算复杂度


<details>
  <summary>Details</summary>
Motivation: 基于博弈论的特征属性方法在可解释人工智能 (XAI) 领域无处不在。 然而，非WAXp集合的贡献被忽略。事实上，由于形式解释 (XPs) 和对抗性示例 (AExs) 之间的关系，非WAXp集合也可以传达重要的信息

Method: 利用 Shapley 值和 Banzhaf 指数设计了两种新的特征重要性评分

Result: 新提出的分数可以量化每个特征在排除 AExs 方面的有效性

Conclusion: 本文提出了两种新的特征重要性评分，它们在计算特征贡献时考虑了非WAXp集合，并量化了每个特征在排除AExs方面的有效性。本文还确定了所提出的分数的属性并研究了计算复杂性。

Abstract: Feature attribution methods based on game theory are ubiquitous in the field
of eXplainable Artificial Intelligence (XAI). Recent works proposed rigorous
feature attribution using logic-based explanations, specifically targeting
high-stakes uses of machine learning (ML) models. Typically, such works exploit
weak abductive explanation (WAXp) as the characteristic function to assign
importance to features. However, one possible downside is that the contribution
of non-WAXp sets is neglected. In fact, non-WAXp sets can also convey important
information, because of the relationship between formal explanations (XPs) and
adversarial examples (AExs). Accordingly, this paper leverages Shapley value
and Banzhaf index to devise two novel feature importance scores. We take into
account non-WAXp sets when computing feature contribution, and the novel scores
quantify how effective each feature is at excluding AExs. Furthermore, the
paper identifies properties and studies the computational complexity of the
proposed scores.

</details>


### [98] [Chart-CoCa: Self-Improving Chart Understanding of Vision LMs via Code-Driven Synthesis and Candidate-Conditioned Answering](https://arxiv.org/abs/2508.11975)
*Gongyao Jiang,Qiong Luo*

Main category: cs.AI

TL;DR: This paper introduces a new chart synthesis pipeline to generate reliable synthetic data and a candidate-conditioned answering process to improve the performance of VLMs on chart understanding tasks.


<details>
  <summary>Details</summary>
Motivation: Vision Language Models (VLMs) often struggle with chart understanding tasks, particularly in accurate chart description and complex reasoning. Synthetic data generation is a promising solution, while usually facing the challenge of noise labels.

Method: introduce a chart synthesis pipeline that generates aligned chart-question-answer triplets through code generation and execution, ensuring the reliability of synthetic data without human intervention. Furthermore, inspired by test-time scaling that increases inference budget and thereby improves performance, we design a candidate-conditioned answering process. The VLM first generates multiple responses per query, and then synthesizes the final answer by contextualizing these candidates.

Result: significant improvements, with up to 15.50 points accuracy gain over the initial VLM

Conclusion: Experiments demonstrate significant improvements, with up to 15.50 points accuracy gain over the initial VLM, in a fully self-improving paradigm without either human-labeled data or external models.

Abstract: Vision Language Models (VLMs) often struggle with chart understanding tasks,
particularly in accurate chart description and complex reasoning. Synthetic
data generation is a promising solution, while usually facing the challenge of
noise labels. To address this challenge, we first introduce a chart synthesis
pipeline that generates aligned chart-question-answer triplets through code
generation and execution, ensuring the reliability of synthetic data without
human intervention. Furthermore, inspired by test-time scaling that increases
inference budget and thereby improves performance, we design a
candidate-conditioned answering process. The VLM first generates multiple
responses per query, and then synthesizes the final answer by contextualizing
these candidates. Experiments demonstrate significant improvements, with up to
15.50 points accuracy gain over the initial VLM, in a fully self-improving
paradigm without either human-labeled data or external models.

</details>


### [99] [FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction](https://arxiv.org/abs/2508.11987)
*Zhiyuan Zeng,Jiashuo Liu,Siyuan Chen,Tianci He,Yali Liao,Jinpeng Wang,Zaiyuan Wang,Yang Yang,Lingyue Yin,Mingren Yin,Zhenwei Zhu,Tianle Cai,Zehui Chen,Jiecao Chen,Yantao Du,Xiang Gao,Jiacheng Guo,Liang Hu,Jianpeng Jiao,Xiangsheng Li,Jingkai Liu,Shuang Ni,Zhoufutu Wen,Ge Zhang,Kaiyuan Zhang,Xin Zhou,Jose Blanchet,Xipeng Qiu,Mengdi Wang,Wenhao Huang*

Main category: cs.AI

TL;DR: Introduces FutureX, a live benchmark for evaluating LLM agents on future prediction.


<details>
  <summary>Details</summary>
Motivation: The lack of a large-scale benchmark for evaluating LLM agents on future prediction.

Method: A dynamic and live evaluation benchmark called FutureX.

Result: Evaluation of 25 LLM/agent models, in-depth analyses of agents' failure modes and performance pitfalls.

Conclusion: This paper introduces FutureX, a dynamic benchmark for evaluating LLM agents on future prediction, and evaluates 25 models, analyzing failure modes and performance pitfalls.

Abstract: Future prediction is a complex task for LLM agents, requiring a high level of
analytical thinking, information gathering, contextual understanding, and
decision-making under uncertainty. Agents must not only gather and interpret
vast amounts of dynamic information but also integrate diverse data sources,
weigh uncertainties, and adapt predictions based on emerging trends, just as
human experts do in fields like politics, economics, and finance. Despite its
importance, no large-scale benchmark exists for evaluating agents on future
prediction, largely due to challenges in handling real-time updates and
retrieving timely, accurate answers. To address this, we introduce
$\textbf{FutureX}$, a dynamic and live evaluation benchmark specifically
designed for LLM agents performing future prediction tasks. FutureX is the
largest and most diverse live benchmark for future prediction, supporting
real-time daily updates and eliminating data contamination through an automated
pipeline for question gathering and answer collection. We evaluate 25 LLM/agent
models, including those with reasoning, search capabilities, and integration of
external tools such as the open-source Deep Research Agent and closed-source
Deep Research models. This comprehensive evaluation assesses agents' adaptive
reasoning and performance in dynamic environments. Additionally, we provide
in-depth analyses of agents' failure modes and performance pitfalls in
future-oriented tasks, including the vulnerability to fake web pages and the
temporal validity. Our goal is to establish a dynamic, contamination-free
evaluation standard that drives the development of LLM agents capable of
performing at the level of professional human analysts in complex reasoning and
predictive thinking.

</details>


### [100] [Modeling Relational Logic Circuits for And-Inverter Graph Convolutional Network](https://arxiv.org/abs/2508.11991)
*Weihao Sun*

Main category: cs.AI

TL;DR: This paper introduces AIGer, a new approach to model And-Inverter Graphs (AIGs) by jointly modeling functional and structural characteristics and improves its message passing capability. AIGer outperforms existing models in Signal Probability Prediction and Truth Table Distance Prediction tasks.


<details>
  <summary>Details</summary>
Motivation: Due to the complex structure and large scale of nodes in real-world AIGs, accurate modeling is challenging, leading to existing work lacking the ability to jointly model functional and structural characteristics, as well as insufficient dynamic information propagation capability.

Method: AIGer consists of two components: 1) Node logic feature initialization embedding component and 2) AIGs feature learning network component. The node logic feature initialization embedding component projects logic nodes, such as AND and NOT, into independent semantic spaces, to enable effective node embedding for subsequent processing. Building upon this, the AIGs feature learning network component employs a heterogeneous graph convolutional network, designing dynamic relationship weight matrices and differentiated information aggregation approaches to better represent the original structure and information of AIGs.

Result: AIGer outperforms the current best models in the Signal Probability Prediction (SSP) task and the Truth Table Distance Prediction (TTDP) task.

Conclusion: AIGer outperforms the current best models in the Signal Probability Prediction (SSP) task, improving MAE and MSE by 18.95% and 44.44%, respectively. In the Truth Table Distance Prediction (TTDP) task, AIGer achieves improvements of 33.57% and 14.79% in MAE and MSE, respectively, compared to the best-performing models.

Abstract: The automation of logic circuit design enhances chip performance, energy
efficiency, and reliability, and is widely applied in the field of Electronic
Design Automation (EDA).And-Inverter Graphs (AIGs) efficiently represent,
optimize, and verify the functional characteristics of digital circuits,
enhancing the efficiency of EDA development.Due to the complex structure and
large scale of nodes in real-world AIGs, accurate modeling is challenging,
leading to existing work lacking the ability to jointly model functional and
structural characteristics, as well as insufficient dynamic information
propagation capability.To address the aforementioned challenges, we propose
AIGer.Specifically, AIGer consists of two components: 1) Node logic feature
initialization embedding component and 2) AIGs feature learning network
component.The node logic feature initialization embedding component projects
logic nodes, such as AND and NOT, into independent semantic spaces, to enable
effective node embedding for subsequent processing.Building upon this, the AIGs
feature learning network component employs a heterogeneous graph convolutional
network, designing dynamic relationship weight matrices and differentiated
information aggregation approaches to better represent the original structure
and information of AIGs.The combination of these two components enhances
AIGer's ability to jointly model functional and structural characteristics and
improves its message passing capability. Experimental results indicate that
AIGer outperforms the current best models in the Signal Probability Prediction
(SSP) task, improving MAE and MSE by 18.95\% and 44.44\%, respectively. In the
Truth Table Distance Prediction (TTDP) task, AIGer achieves improvements of
33.57\% and 14.79\% in MAE and MSE, respectively, compared to the
best-performing models.

</details>


### [101] [AgentCDM: Enhancing Multi-Agent Collaborative Decision-Making via ACH-Inspired Structured Reasoning](https://arxiv.org/abs/2508.11995)
*Xuyang Zhao,Shiwan Zhao,Hualong Yu,Liting Zhang,Qicheng Li*

Main category: cs.AI

TL;DR: AgentCDM是一个结构化框架，用于增强基于LLM的多智能体系统中的协作决策，通过模仿认知科学中的竞争假设分析（ACH）来减轻认知偏差，并在多个基准数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 基于大型语言模型（LLM）的多智能体系统（MAS）在解决复杂决策任务方面具有巨大的潜力。然而，这些系统中协作决策（CDM）的核心过程仍未得到充分探索。现有方法通常依赖于易受单个智能体认知偏差影响的“独裁”策略，或者未能充分利用集体智慧的“基于投票”的方法。

Method: AgentCDM，一个用于增强基于LLM的多智能体系统中协作决策的结构化框架。借鉴认知科学中的竞争假设分析（ACH），AgentCDM引入了一种结构化的推理范式，该范式系统地减轻了认知偏差，并将决策从被动答案选择转变为主动假设评估和构建。为了将这种推理过程内在化，我们开发了一个两阶段训练范式：第一阶段使用显式的、受ACH启发的支架来指导模型进行结构化推理，而第二阶段逐渐移除这种支架，以鼓励自主泛化。

Result: AgentCDM在多个基准数据集上实现了最先进的性能，并表现出强大的泛化能力。

Conclusion: AgentCDM在多个基准数据集上实现了最先进的性能，并表现出强大的泛化能力，验证了其在提高MAS中协作决策的质量和稳健性方面的有效性。

Abstract: Multi-agent systems (MAS) powered by large language models (LLMs) hold
significant promise for solving complex decision-making tasks. However, the
core process of collaborative decision-making (CDM) within these systems
remains underexplored. Existing approaches often rely on either ``dictatorial"
strategies that are vulnerable to the cognitive biases of a single agent, or
``voting-based" methods that fail to fully harness collective intelligence. To
address these limitations, we propose \textbf{AgentCDM}, a structured framework
for enhancing collaborative decision-making in LLM-based multi-agent systems.
Drawing inspiration from the Analysis of Competing Hypotheses (ACH) in
cognitive science, AgentCDM introduces a structured reasoning paradigm that
systematically mitigates cognitive biases and shifts decision-making from
passive answer selection to active hypothesis evaluation and construction. To
internalize this reasoning process, we develop a two-stage training paradigm:
the first stage uses explicit ACH-inspired scaffolding to guide the model
through structured reasoning, while the second stage progressively removes this
scaffolding to encourage autonomous generalization. Experiments on multiple
benchmark datasets demonstrate that AgentCDM achieves state-of-the-art
performance and exhibits strong generalization, validating its effectiveness in
improving the quality and robustness of collaborative decisions in MAS.

</details>


### [102] [AI Models for Depressive Disorder Detection and Diagnosis: A Review](https://arxiv.org/abs/2508.12022)
*Dorsa Macky Aleagha,Payam Zohari,Mostafa Haghir Chehreghani*

Main category: cs.AI

TL;DR: Comprehensive survey of AI methods for depression detection, highlighting trends like graph neural networks and large language models, and emphasizing multimodal fusion and explainability.


<details>
  <summary>Details</summary>
Motivation: Major Depressive Disorder is a leading cause of disability, and its diagnosis relies on subjective clinical assessments. AI offers potential for objective, scalable diagnostic tools.

Method: A systematic review of 55 key studies, introducing a novel hierarchical taxonomy based on clinical task, data modality, and computational model class.

Result: Identified three major trends: predominance of graph neural networks for modeling brain connectivity, the rise of large language models for linguistic data, and an emerging focus on multimodal fusion, explainability, and algorithmic fairness. Also provides an overview of datasets and evaluation metrics.

Conclusion: This survey synthesizes current advances in AI for depression detection and diagnosis and highlights open challenges, offering a roadmap for future innovation in computational psychiatry.

Abstract: Major Depressive Disorder is one of the leading causes of disability
worldwide, yet its diagnosis still depends largely on subjective clinical
assessments. Integrating Artificial Intelligence (AI) holds promise for
developing objective, scalable, and timely diagnostic tools. In this paper, we
present a comprehensive survey of state-of-the-art AI methods for depression
detection and diagnosis, based on a systematic review of 55 key studies. We
introduce a novel hierarchical taxonomy that structures the field by primary
clinical task (diagnosis vs. prediction), data modality (text, speech,
neuroimaging, multimodal), and computational model class (e.g., graph neural
networks, large language models, hybrid approaches). Our in-depth analysis
reveals three major trends: the predominance of graph neural networks for
modeling brain connectivity, the rise of large language models for linguistic
and conversational data, and an emerging focus on multimodal fusion,
explainability, and algorithmic fairness. Alongside methodological insights, we
provide an overview of prominent public datasets and standard evaluation
metrics as a practical guide for researchers. By synthesizing current advances
and highlighting open challenges, this survey offers a comprehensive roadmap
for future innovation in computational psychiatry.

</details>


### [103] [Bongard-RWR+: Real-World Representations of Fine-Grained Concepts in Bongard Problems](https://arxiv.org/abs/2508.12026)
*Szymon Pawlonka,Mikołaj Małkiński,Jacek Mańdziuk*

Main category: cs.AI

TL;DR: 论文介绍了一个新的 Bongard Problems (BP) 数据集 Bongard-RWR+，该数据集包含 5,400 个实例，使用视觉语言模型生成的图像来表示抽象概念。实验表明，现有的视觉语言模型在识别细粒度概念方面存在局限性。


<details>
  <summary>Details</summary>
Motivation: 早期的 BP 基准测试以合成黑白绘图为特色，可能无法完全捕捉真实世界场景的复杂性。随后的 BP 数据集采用了真实世界的图像，但所代表的概念可以从高级图像特征中识别出来，从而降低了任务的复杂性。最近发布的 Bongard-RWR 数据集旨在表示原始 BP 中制定的抽象概念，使用细粒度的真实世界图像。然而，它的人工构建将数据集的大小限制为 60 个实例，从而限制了评估的稳健性。

Method: 使用 Pixtral-12B 描述手动整理的图像，并生成与底层概念对齐的新描述，使用 Flux.1-dev 从这些描述中合成图像，并手动验证生成的图像是否忠实地反映了预期的概念。

Result: 我们引入了 Bongard-RWR+，这是一个由 5,400 个实例组成的 BP 数据集，它使用通过视觉语言模型 (VLM) 管道生成的类真实世界图像来表示原始 BP 抽象概念。我们评估了各种 BP 公式（包括二元和多类分类以及文本答案生成）中的最先进的 VLM。我们的发现表明，虽然 VLM 可以识别粗粒度的视觉概念，但它们始终难以辨别细粒度的概念，突显了其推理能力的局限性。

Conclusion: VLMs在识别粗粒度视觉概念方面表现尚可，但在辨别细粒度概念方面存在困难，突显了其推理能力的局限性。

Abstract: Bongard Problems (BPs) provide a challenging testbed for abstract visual
reasoning (AVR), requiring models to identify visual concepts fromjust a few
examples and describe them in natural language. Early BP benchmarks featured
synthetic black-and-white drawings, which might not fully capture the
complexity of real-world scenes. Subsequent BP datasets employed real-world
images, albeit the represented concepts are identifiable from high-level image
features, reducing the task complexity. Differently, the recently released
Bongard-RWR dataset aimed at representing abstract concepts formulated in the
original BPs using fine-grained real-world images. Its manual construction,
however, limited the dataset size to just $60$ instances, constraining
evaluation robustness. In this work, we introduce Bongard-RWR+, a BP dataset
composed of $5\,400$ instances that represent original BP abstract concepts
using real-world-like images generated via a vision language model (VLM)
pipeline. Building on Bongard-RWR, we employ Pixtral-12B to describe manually
curated images and generate new descriptions aligned with the underlying
concepts, use Flux.1-dev to synthesize images from these descriptions, and
manually verify that the generated images faithfully reflect the intended
concepts. We evaluate state-of-the-art VLMs across diverse BP formulations,
including binary and multiclass classification, as well as textual answer
generation. Our findings reveal that while VLMs can recognize coarse-grained
visual concepts, they consistently struggle with discerning fine-grained
concepts, highlighting limitations in their reasoning capabilities.

</details>


### [104] [Active inference for action-unaware agents](https://arxiv.org/abs/2508.12027)
*Filippo Torresan,Keisuke Suzuki,Ryota Kanai,Manuel Baltieri*

Main category: cs.AI

TL;DR: This paper compares action-aware and action-unaware agents in navigation tasks, finding that action-unaware agents can perform similarly to action-aware ones despite being at a disadvantage.


<details>
  <summary>Details</summary>
Motivation: investigate different strategies for agents to plan their future actions, specifically how past motor experience contributes to future behavior.

Method: comparing the performances of action-aware and action-unaware agents in two navigations tasks

Result: action-unaware agents can achieve performances comparable to action-aware ones.

Conclusion: action-unaware agents can achieve performances comparable to action-aware ones while at a severe disadvantage.

Abstract: Active inference is a formal approach to study cognition based on the notion
that adaptive agents can be seen as engaging in a process of approximate
Bayesian inference, via the minimisation of variational and expected free
energies. Minimising the former provides an account of perceptual processes and
learning as evidence accumulation, while minimising the latter describes how
agents select their actions over time. In this way, adaptive agents are able to
maximise the likelihood of preferred observations or states, given a generative
model of the environment. In the literature, however, different strategies have
been proposed to describe how agents can plan their future actions. While they
all share the notion that some kind of expected free energy offers an
appropriate way to score policies, sequences of actions, in terms of their
desirability, there are different ways to consider the contribution of past
motor experience to the agent's future behaviour. In some approaches, agents
are assumed to know their own actions, and use such knowledge to better plan
for the future. In other approaches, agents are unaware of their actions, and
must infer their motor behaviour from recent observations in order to plan for
the future. This difference reflects a standard point of departure in two
leading frameworks in motor control based on the presence, or not, of an
efference copy signal representing knowledge about an agent's own actions. In
this work we compare the performances of action-aware and action-unaware agents
in two navigations tasks, showing how action-unaware agents can achieve
performances comparable to action-aware ones while at a severe disadvantage.

</details>


### [105] [MAPF-World: Action World Model for Multi-Agent Path Finding](https://arxiv.org/abs/2508.12087)
*Zhanjiang Yang,Meng Li,Yang Shen,Yueming Li,Lijun Sun*

Main category: cs.AI

TL;DR: 提出MAPF-World，一种用于多智能体路径寻找的自回归行动世界模型，它通过预测未来状态和行动来提高决策质量，并在实验中表现出优越的性能，同时减少了模型大小和数据需求。


<details>
  <summary>Details</summary>
Motivation: 现有的分散式可学习求解器在大型MAPF中表现出巨大的潜力，但由于它们是反应式策略模型，对环境时间动态和智能体间依赖关系的建模有限，因此在复杂的长期规划场景中性能下降。

Method: 提出了一种用于MAPF的自回归行动世界模型MAPF-World，它统一了情境理解和行动生成，从而指导决策超越了直接的局部观察。

Result: MAPF-World通过显式建模环境动态，包括空间特征和时间依赖性，从而提高了情境感知能力。通过结合这些预测的未来，MAPF-World能够做出更明智、更协调和更具远见的决策。引入了一种基于真实世界场景的自动地图生成器，用于训练和评估MAPF求解器。MAPF-World优于最先进的可学习求解器。

Conclusion: MAPF-World在复杂多智能体环境中表现出色，优于现有可学习的求解器，并在零样本泛化方面表现出卓越的性能。MAPF-World的训练模型规模减少了96.5%，数据量减少了92%。

Abstract: Multi-agent path finding (MAPF) is the problem of planning conflict-free
paths from the designated start locations to goal positions for multiple
agents. It underlies a variety of real-world tasks, including multi-robot
coordination, robot-assisted logistics, and social navigation. Recent
decentralized learnable solvers have shown great promise for large-scale MAPF,
especially when leveraging foundation models and large datasets. However, these
agents are reactive policy models and exhibit limited modeling of environmental
temporal dynamics and inter-agent dependencies, resulting in performance
degradation in complex, long-term planning scenarios. To address these
limitations, we propose MAPF-World, an autoregressive action world model for
MAPF that unifies situation understanding and action generation, guiding
decisions beyond immediate local observations. It improves situational
awareness by explicitly modeling environmental dynamics, including spatial
features and temporal dependencies, through future state and actions
prediction. By incorporating these predicted futures, MAPF-World enables more
informed, coordinated, and far-sighted decision-making, especially in complex
multi-agent settings. Furthermore, we augment MAPF benchmarks by introducing an
automatic map generator grounded in real-world scenarios, capturing practical
map layouts for training and evaluating MAPF solvers. Extensive experiments
demonstrate that MAPF-World outperforms state-of-the-art learnable solvers,
showcasing superior zero-shot generalization to out-of-distribution cases.
Notably, MAPF-World is trained with a 96.5% smaller model size and 92% reduced
data.

</details>


### [106] [Overcoming Knowledge Discrepancies: Structuring Reasoning Threads through Knowledge Balancing in Interactive Scenarios](https://arxiv.org/abs/2508.12100)
*Daniel Burkhardt,Xiangwei Cheng*

Main category: cs.AI

TL;DR: 提出了 ReT-Eval 框架，该框架通过结合图神经网络和大型语言模型，从领域知识图中提取相关的知识结构，并使用奖励引导策略进行评估和修剪，以生成有效的推理链。


<details>
  <summary>Details</summary>
Motivation: 现有的推理模型通常缺乏明确的语义层次结构、用户领域知识对齐和有效的剪枝推理链机制。这些限制导致冗长的通用输出，不能指导用户完成面向目标的推理步骤。

Method: 提出了一个原型驱动的，两阶段的推理链评估 (ReT-Eval) 框架。

Result: 实验和专家评估表明 ReT-Eval 增强了用户理解并且超过了现有最优推理模型。

Conclusion: ReT-Eval 增强了用户理解并且超过了现有最优推理模型。

Abstract: Reasoning in interactive problem solving scenarios requires models to
construct reasoning threads that reflect user understanding and align with
structured domain knowledge. However, current reasoning models often lack
explicit semantic hierarchies, user-domain knowledge alignment, and principled
mechanisms to prune reasoning threads for effectiveness. These limitations
result in lengthy generic output that does not guide users through
goal-oriented reasoning steps. To address this, we propose a
prototype-inspired, two-phases Reasoning-Threads-Evaluation (ReT-Eval)
framework, drawing inspiration from human-like reasoning strategies that
emphasize structured knowledge reuse. In the first phase, semantically relevant
knowledge structures are extracted from a sparse domain knowledge graph using a
graph neural network and enriched with intrinsic large language model knowledge
to resolve knowledge discrepancies. In the second phase, these threads are
evaluated and pruned using a reward-guided strategy aimed at maintaining
semantic coherence to generate effective reasoning threads. Experiments and
expert evaluations show that ReT-Eval enhances user understanding and
outperforms state-of-the-art reasoning models.

</details>


### [107] [MOVER: Multimodal Optimal Transport with Volume-based Embedding Regularization](https://arxiv.org/abs/2508.12149)
*Haochen You,Baojing Liu*

Main category: cs.AI

TL;DR: MOVER结合最优传输和几何正则化，构建了语义对齐和结构化的多模态表示，并在多模态检索任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态学习方法主要依赖于pairwise contrastive objectives，以在共享嵌入空间中对齐不同的模态（例如文本、视频和音频）。虽然在bi-modal setups中有效，但这些方法难以在多个模态中推广，并且通常在高维空间中缺乏语义结构。

Method: 结合了基于最优传输的软对齐和基于体积的几何正则化，构建了语义对齐和结构化的多模态表示。通过将传输引导的匹配机制与几何体积最小化目标（GAVE）相结合，MOVER鼓励以modality-agnostic的方式在所有模态中保持一致的对齐。

Result: 在text-video-audio检索任务上的实验表明，MOVER显著优于先前state-of-the-art的方法。

Conclusion: MOVER在zero-shot和finetuned设置下，显著优于先前state-of-the-art的方法。它还能更好地泛化到unseen modality combinations，并在学习到的嵌入空间中具有更强的结构一致性。

Abstract: Recent advances in multimodal learning have largely relied on pairwise
contrastive objectives to align different modalities, such as text, video, and
audio, in a shared embedding space. While effective in bi-modal setups, these
approaches struggle to generalize across multiple modalities and often lack
semantic structure in high-dimensional spaces. In this paper, we propose MOVER,
a novel framework that combines optimal transport-based soft alignment with
volume-based geometric regularization to build semantically aligned and
structured multimodal representations. By integrating a transport-guided
matching mechanism with a geometric volume minimization objective (GAVE), MOVER
encourages consistent alignment across all modalities in a modality-agnostic
manner. Experiments on text-video-audio retrieval tasks demonstrate that MOVER
significantly outperforms prior state-of-the-art methods in both zero-shot and
finetuned settings. Additional analysis shows improved generalization to unseen
modality combinations and stronger structural consistency in the learned
embedding space.

</details>


### [108] [RLNVR: Reinforcement Learning from Non-Verified Real-World Rewards](https://arxiv.org/abs/2508.12165)
*Rohit Krishnan,Jon Evans*

Main category: cs.AI

TL;DR: RLNVR使用噪声、真实世界的反馈信号训练语言模型，无需人工验证，并通过实际参与数据优化社交媒体内容生成。


<details>
  <summary>Details</summary>
Motivation: 传统的RLHF需要昂贵的、经过验证的奖励信号，这在许多现实领域中是不切实际的。

Method: RLNVR框架，通过基线归一化和语义相似性奖励转移来训练语言模型。

Result: 在内容质量和训练稳定性方面有显著提高。

Conclusion: RLNVR结合GSPO和可选的UED课程，在噪声、隐式奖励下提高稳定性和多样性。

Abstract: This paper introduces RLNVR (Reinforcement Learning from Non-Verified
Rewards), a framework for training language models using noisy, real-world
feedback signals without requiring explicit human verification. Traditional
RLHF requires expensive, verified reward signals that are impractical in many
real-world domains. RLNVR addresses this challenge through baseline
normalization and semantic similarity-based reward transfer. We demonstrate
RLNVR through Walter, a prototype system that optimizes social media content
generation using actual engagement data from Bluesky. Our experimental results
show significant improvements in content quality and training stability, with
comprehensive evaluation planned for future work. Positioning: We present a
practical framework that combines RLNVR with GSPO (Group Sequence Policy
Optimization) and an optional UED (Unsupervised Environment Design) curriculum
to improve stability and diversity under noisy, implicit rewards. To our
knowledge, combining GSPO-style normalization with a UED-style curriculum for
LLM content generation from implicit social engagement has not been previously
documented in this applied setting; we frame this as an applied integration
rather than a new algorithm.

</details>


### [109] [Mantis: A Simulation-Grounded Foundation Model for Disease Forecasting](https://arxiv.org/abs/2508.12260)
*Carson Dudley,Reiden Magdaleno,Christopher Harding,Ananya Sharma,Emily Martin,Marisa Eisenberg*

Main category: cs.AI

TL;DR: Mantis 是一个在机制模拟基础上训练的基础模型，无需真实世界数据即可实现跨疾病、区域和结果的开箱即用预测，优于专家调整的模型，并且在机制上是可解释的。


<details>
  <summary>Details</summary>
Motivation: 在新爆发或低资源环境中，传染病预测一直受到对疾病特定数据、定制培训和专家调整的需求的限制。

Method: 在超过 4 亿个模拟的爆发动态天数的基础上，Mantis 涵盖了多种病原体、传播模式、干预措施和监测人为因素。

Result: 尽管在训练期间不需要真实世界的数据，但 Mantis 在我们测试的六种疾病中优于 39 个经过专家调整的模型，包括 CDC 的 COVID-19 预测中心的所有模型。Mantis 推广到新的流行病学领域，包括具有保留传播机制的疾病，表明它捕获了基本的传染动态。重要的是，Mantis 在机制上是可解释的，使公共卫生决策者能够识别其预测背后的潜在驱动因素。最后，Mantis 提供了 8 周范围内的准确预测，是大多数模型的可操作范围的两倍以上，从而可以进行积极的公共卫生规划。

Conclusion: Mantis作为一个通用、可解释且可在传统模型失效时部署的基础，为下一代疾病预测系统奠定了基础。

Abstract: Infectious disease forecasting in novel outbreaks or low resource settings
has been limited by the need for disease-specific data, bespoke training, and
expert tuning. We introduce Mantis, a foundation model trained entirely on
mechanistic simulations, which enables out-of-the-box forecasting across
diseases, regions, and outcomes, even in settings with limited historical data.
Mantis is built on over 400 million simulated days of outbreak dynamics
spanning diverse pathogens, transmission modes, interventions, and surveillance
artifacts. Despite requiring no real-world data during training, Mantis
outperformed 39 expert-tuned models we tested across six diseases, including
all models in the CDC's COVID-19 Forecast Hub. Mantis generalized to novel
epidemiological regimes, including diseases with held-out transmission
mechanisms, demonstrating that it captures fundamental contagion dynamics.
Critically, Mantis is mechanistically interpretable, enabling public health
decision-makers to identify the latent drivers behind its predictions. Finally,
Mantis delivers accurate forecasts at 8-week horizons, more than doubling the
actionable range of most models, enabling proactive public health planning.
Together, these capabilities position Mantis as a foundation for
next-generation disease forecasting systems: general, interpretable, and
deployable where traditional models fail.

</details>


### [110] [RadarQA: Multi-modal Quality Analysis of Weather Radar Forecasts](https://arxiv.org/abs/2508.12291)
*Xuming He,Zhiyuan You,Junchao Gong,Couhua Liu,Xiaoyu Yue,Peiqin Zhuang,Wenlong Zhang,Lei Bai*

Main category: cs.AI

TL;DR: 本文提出了一种名为RadarQA的基于MLLM的天气预报分析方法，并构建了一个名为RQA-70K的大规模数据集用于训练和基准测试。


<details>
  <summary>Details</summary>
Motivation: 传统上基于分数的评估指标可以量化某些预测误差，但在描述能力、可解释性和对动态演化的理解方面，它们与气象专家相差甚远。随着多模态大型语言模型（MLLM）的快速发展，这些模型成为克服上述挑战的潜在工具。

Method: 介绍了一种基于MLLM的天气预报分析方法RadarQA，该方法集成了关键的物理属性和详细的评估报告。设计了一个新颖而全面的多模态质量分析任务范例，包括单帧和序列，以及评级和评估场景。设计了一种混合注释流程，该流程结合了人类专家标记和自动启发式方法。

Result: RadarQA在所有评估设置中优于现有的通用MLLM。

Conclusion: RadarQA在所有评估设置中优于现有的通用MLLM，突出了其在推进天气预报质量分析方面的潜力。

Abstract: Quality analysis of weather forecasts is an essential topic in meteorology.
Although traditional score-based evaluation metrics can quantify certain
forecast errors, they are still far from meteorological experts in terms of
descriptive capability, interpretability, and understanding of dynamic
evolution. With the rapid development of Multi-modal Large Language Models
(MLLMs), these models become potential tools to overcome the above challenges.
In this work, we introduce an MLLM-based weather forecast analysis method,
RadarQA, integrating key physical attributes with detailed assessment reports.
We introduce a novel and comprehensive task paradigm for multi-modal quality
analysis, encompassing both single frame and sequence, under both rating and
assessment scenarios. To support training and benchmarking, we design a hybrid
annotation pipeline that combines human expert labeling with automated
heuristics. With such an annotation method, we construct RQA-70K, a large-scale
dataset with varying difficulty levels for radar forecast quality evaluation.
We further design a multi-stage training strategy that iteratively improves
model performance at each stage. Extensive experiments show that RadarQA
outperforms existing general MLLMs across all evaluation settings, highlighting
its potential for advancing quality analysis in weather prediction.

</details>


### [111] [Wisdom of the Crowd: Reinforcement Learning from Coevolutionary Collective Feedback](https://arxiv.org/abs/2508.12338)
*Wenzhen Yuan,Shengji Tang,Weihao Lin,Jiacheng Ruan,Ganqu Cui,Bo Zhang,Tao Chen,Ting Liu,Yuzhuo Fu,Peng Ye,Lei Bai*

Main category: cs.AI

TL;DR: RLCCF is a new reinforcement learning framework that uses multi-model collaborative evolution to improve the reasoning capabilities of large language models without external supervision, achieving significant performance gains on mathematical reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing self-feedback methods are constrained by the capabilities of a single model, which can lead to overconfidence in incorrect answers, reward hacking, and even training collapse. The reliance on expensive human-labeled data or complex reward models severely limits scalability.

Method: The paper proposes Reinforcement Learning from Coevolutionary Collective Feedback (RLCCF), a novel RL framework that enables multi-model collaborative evolution without external supervision. It optimizes the ability of a model collective by maximizing its Collective Consistency (CC), which jointly trains a diverse ensemble of LLMs and provides reward signals by voting on collective outputs. Each model's vote is weighted by its Self-Consistency (SC) score.

Result: Experiments on four mainstream open-source LLMs across four mathematical reasoning benchmarks demonstrate that the framework yields significant performance gains, achieving an average relative improvement of 16.72% in accuracy. The group's majority-voting accuracy is enhanced by 4.51%.

Conclusion: The RLCCF framework significantly improves the performance of individual models and enhances the group's majority-voting accuracy, demonstrating its ability to extend the collective capability boundary of the model collective.

Abstract: Reinforcement learning (RL) has significantly enhanced the reasoning
capabilities of large language models (LLMs), but its reliance on expensive
human-labeled data or complex reward models severely limits scalability. While
existing self-feedback methods aim to address this problem, they are
constrained by the capabilities of a single model, which can lead to
overconfidence in incorrect answers, reward hacking, and even training
collapse. To this end, we propose Reinforcement Learning from Coevolutionary
Collective Feedback (RLCCF), a novel RL framework that enables multi-model
collaborative evolution without external supervision. Specifically, RLCCF
optimizes the ability of a model collective by maximizing its Collective
Consistency (CC), which jointly trains a diverse ensemble of LLMs and provides
reward signals by voting on collective outputs. Moreover, each model's vote is
weighted by its Self-Consistency (SC) score, ensuring that more confident
models contribute more to the collective decision. Benefiting from the diverse
output distributions and complementary abilities of multiple LLMs, RLCCF
enables the model collective to continuously enhance its reasoning ability
through coevolution. Experiments on four mainstream open-source LLMs across
four mathematical reasoning benchmarks demonstrate that our framework yields
significant performance gains, achieving an average relative improvement of
16.72\% in accuracy. Notably, RLCCF not only improves the performance of
individual models but also enhances the group's majority-voting accuracy by
4.51\%, demonstrating its ability to extend the collective capability boundary
of the model collective.

</details>


### [112] [Hierarchical knowledge guided fault intensity diagnosis of complex industrial systems](https://arxiv.org/abs/2508.12375)
*Yu Sha,Shuiping Gou,Bo Liu,Johannes Faber,Ningtao Liu,Stefan Schramm,Horst Stoecker,Thomas Steckenreiter,Domagoj Vnucec,Nadine Wetzstein,Andreas Widl,Kai Zhou*

Main category: cs.AI

TL;DR: Developed HKG framework for fault intensity diagnosis, capturing dependencies among target classes using hierarchical knowledge and graph convolutional networks, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current FID methods lack consideration of dependencies among target classes.

Method: A hierarchical knowledge guided fault intensity diagnosis framework (HKG) using graph convolutional networks and a re-weighted hierarchical knowledge correlation matrix (Re-HKCM).

Result: Superior results on four real-world datasets compared to recent state-of-the-art FID methods.

Conclusion: The proposed HKG framework outperforms state-of-the-art FID methods on four real-world datasets.

Abstract: Fault intensity diagnosis (FID) plays a pivotal role in monitoring and
maintaining mechanical devices within complex industrial systems. As current
FID methods are based on chain of thought without considering dependencies
among target classes. To capture and explore dependencies, we propose a
hierarchical knowledge guided fault intensity diagnosis framework (HKG)
inspired by the tree of thought, which is amenable to any representation
learning methods. The HKG uses graph convolutional networks to map the
hierarchical topological graph of class representations into a set of
interdependent global hierarchical classifiers, where each node is denoted by
word embeddings of a class. These global hierarchical classifiers are applied
to learned deep features extracted by representation learning, allowing the
entire model to be end-to-end learnable. In addition, we develop a re-weighted
hierarchical knowledge correlation matrix (Re-HKCM) scheme by embedding
inter-class hierarchical knowledge into a data-driven statistical correlation
matrix (SCM) which effectively guides the information sharing of nodes in
graphical convolutional neural networks and avoids over-smoothing issues. The
Re-HKCM is derived from the SCM through a series of mathematical
transformations. Extensive experiments are performed on four real-world
datasets from different industrial domains (three cavitation datasets from
SAMSON AG and one existing publicly) for FID, all showing superior results and
outperform recent state-of-the-art FID methods.

</details>


### [113] [GraphCogent: Overcoming LLMs' Working Memory Constraints via Multi-Agent Collaboration in Complex Graph Understanding](https://arxiv.org/abs/2508.12379)
*Rongzheng Wang,Qizhi Chen,Yihong Huang,Yizhuo Ma,Muquan Li,Jiakai Li,Ke Qin,Guangchun Luo,Shuang Liang*

Main category: cs.AI

TL;DR: GraphCogent, a new framework, significantly improves graph reasoning performance and efficiency compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: LLMs' inability to effectively process complex graph topology and perform multi-step reasoning simultaneously.

Method: We propose GraphCogent, a collaborative agent framework inspired by human Working Memory Model that decomposes graph reasoning into specialized cognitive processes: sense, buffer, and execute. The framework consists of three modules: Sensory Module standardizes diverse graph text representations via subgraph sampling, Buffer Module integrates and indexes graph data across multiple formats, and Execution Module combines tool calling and model generation for efficient reasoning.

Result: GraphCogent achieves significant improvements in accuracy and token usage compared to existing LLMs and agent-based baselines on the Graph4real benchmark.

Conclusion: Llama3.1-8B based GraphCogent achieves a 50% improvement over massive-scale LLMs like DeepSeek-R1 (671B). Compared to state-of-the-art agent-based baseline, our framework outperforms by 20% in accuracy while reducing token usage by 80% for in-toolset tasks and 30% for out-toolset tasks.

Abstract: Large language models (LLMs) show promising performance on small-scale graph
reasoning tasks but fail when handling real-world graphs with complex queries.
This phenomenon stems from LLMs' inability to effectively process complex graph
topology and perform multi-step reasoning simultaneously. To address these
limitations, we propose GraphCogent, a collaborative agent framework inspired
by human Working Memory Model that decomposes graph reasoning into specialized
cognitive processes: sense, buffer, and execute. The framework consists of
three modules: Sensory Module standardizes diverse graph text representations
via subgraph sampling, Buffer Module integrates and indexes graph data across
multiple formats, and Execution Module combines tool calling and model
generation for efficient reasoning. We also introduce Graph4real, a
comprehensive benchmark contains with four domains of real-world graphs (Web,
Social, Transportation, and Citation) to evaluate LLMs' graph reasoning
capabilities. Our Graph4real covers 21 different graph reasoning tasks,
categorized into three types (Structural Querying, Algorithmic Reasoning, and
Predictive Modeling tasks), with graph scales that are 10 times larger than
existing benchmarks. Experiments show that Llama3.1-8B based GraphCogent
achieves a 50% improvement over massive-scale LLMs like DeepSeek-R1 (671B).
Compared to state-of-the-art agent-based baseline, our framework outperforms by
20% in accuracy while reducing token usage by 80% for in-toolset tasks and 30%
for out-toolset tasks. Code will be available after review.

</details>


### [114] [Non-Iterative Symbolic-Aided Chain-of-Thought for Logical Reasoning](https://arxiv.org/abs/2508.12425)
*Phuong Minh Nguyen,Tien Huu Dang,Naoya Inoue*

Main category: cs.AI

TL;DR: Symbolic-Aided CoT improves logical reasoning in LLMs by integrating lightweight symbolic representations into prompts, outperforming conventional CoT on most datasets.


<details>
  <summary>Details</summary>
Motivation: to integrate lightweight symbolic representations into few-shot prompts, structuring the inference steps with a consistent strategy to make reasoning patterns more explicit within a non-iterative reasoning process

Method: integrates lightweight symbolic representations into few-shot prompts, structuring the inference steps with a consistent strategy

Result: Symbolic-Aided CoT consistently improves LLMs' reasoning capabilities across various model sizes and significantly outperforms conventional CoT on three out of four datasets, ProofWriter, ProntoQA, and LogicalDeduction

Conclusion: Symbolic-Aided CoT consistently improves LLMs' reasoning capabilities across various model sizes and significantly outperforms conventional CoT on three out of four datasets.

Abstract: This work introduces Symbolic-Aided Chain-of-Thought (CoT), an improved
approach to standard CoT, for logical reasoning in large language models
(LLMs). The key idea is to integrate lightweight symbolic representations into
few-shot prompts, structuring the inference steps with a consistent strategy to
make reasoning patterns more explicit within a non-iterative reasoning process.
By incorporating these symbolic structures, our method preserves the
generalizability of standard prompting techniques while enhancing the
transparency, interpretability, and analyzability of LLM logical reasoning.
Extensive experiments on four well-known logical reasoning benchmarks --
ProofWriter, FOLIO, ProntoQA, and LogicalDeduction, which cover diverse
reasoning scenarios -- demonstrate the effectiveness of the proposed approach,
particularly in complex reasoning tasks that require navigating multiple
constraints or rules. Notably, Symbolic-Aided CoT consistently improves LLMs'
reasoning capabilities across various model sizes and significantly outperforms
conventional CoT on three out of four datasets, ProofWriter, ProntoQA, and
LogicalDeduction.

</details>


### [115] [GALA: Can Graph-Augmented Large Language Model Agentic Workflows Elevate Root Cause Analysis?](https://arxiv.org/abs/2508.12472)
*Yifang Tian,Yaming Liu,Zichun Chong,Zihang Huang,Hans-Arno Jacobsen*

Main category: cs.AI

TL;DR: GALA是一种多模态框架，它结合了统计因果推理与 LLM 驱动的迭代推理，以增强 RCA。


<details>
  <summary>Details</summary>
Motivation: 微服务系统中的根本原因分析 (RCA) 具有挑战性，需要随叫随到的工程师快速诊断跨异构遥测（例如指标、日志和跟踪）的故障。传统的 RCA 方法通常侧重于单一模式或仅对可疑服务进行排名，而未能提供具有补救指导的可操作诊断见解。

Method: GALA，一种新颖的多模式框架，它结合了统计因果推理与 LLM 驱动的迭代推理，以增强 RCA。

Result: GALA 在开源基准上进行了评估，与最先进的方法相比，GALA 的准确率提高了 42.22%。我们新颖的人工指导 LLM 评估分数表明，与现有方法相比，GALA 生成的因果关系更合理、可操作的诊断输出。

Conclusion: GALA通过提供准确的根本原因识别和人类可解释的补救指导，弥合了自动化故障诊断和实际事件解决之间的差距。

Abstract: Root cause analysis (RCA) in microservice systems is challenging, requiring
on-call engineers to rapidly diagnose failures across heterogeneous telemetry
such as metrics, logs, and traces. Traditional RCA methods often focus on
single modalities or merely rank suspect services, falling short of providing
actionable diagnostic insights with remediation guidance. This paper introduces
GALA, a novel multi-modal framework that combines statistical causal inference
with LLM-driven iterative reasoning for enhanced RCA. Evaluated on an
open-source benchmark, GALA achieves substantial improvements over
state-of-the-art methods of up to 42.22% accuracy. Our novel human-guided LLM
evaluation score shows GALA generates significantly more causally sound and
actionable diagnostic outputs than existing methods. Through comprehensive
experiments and a case study, we show that GALA bridges the gap between
automated failure diagnosis and practical incident resolution by providing both
accurate root cause identification and human-interpretable remediation
guidance.

</details>


### [116] [The Yokai Learning Environment: Tracking Beliefs Over Space and Time](https://arxiv.org/abs/2508.12480)
*Constantin Ruhdorfer,Matteo Bortoletto,Andreas Bulling*

Main category: cs.AI

TL;DR: Introduces the Yokai Learning Environment (YLE) for collaborative AI research, revealing challenges in current RL agents' Theory of Mind capabilities, especially in generalization and belief tracking.


<details>
  <summary>Details</summary>
Motivation: Existing ToM benchmarks are restricted to passive observer settings or lack an assessment of how agents establish and maintain common ground over time.

Method: Multi-agent reinforcement learning (RL) environment based on the cooperative card game Yokai.

Result: Current RL agents struggle to solve the YLE, even with perfect memory. Belief modelling improves performance, but agents are still unable to effectively generalize to unseen partners or form accurate beliefs over longer games.

Conclusion: Current RL agents struggle to solve the YLE, even with perfect memory. Belief modelling improves performance, but agents struggle to generalize to unseen partners or form accurate beliefs over longer games, relying on brittle conventions rather than robust belief tracking.

Abstract: Developing collaborative AI hinges on Theory of Mind (ToM) - the ability to
reason about the beliefs of others to build and maintain common ground.
Existing ToM benchmarks, however, are restricted to passive observer settings
or lack an assessment of how agents establish and maintain common ground over
time. To address these gaps, we introduce the Yokai Learning Environment (YLE)
- a multi-agent reinforcement learning (RL) environment based on the
cooperative card game Yokai. In the YLE, agents take turns peeking at hidden
cards and moving them to form clusters based on colour. Success requires
tracking evolving beliefs, remembering past observations, using hints as
grounded communication, and maintaining common ground with teammates. Our
evaluation yields two key findings: First, current RL agents struggle to solve
the YLE, even when given access to perfect memory. Second, while belief
modelling improves performance, agents are still unable to effectively
generalise to unseen partners or form accurate beliefs over longer games,
exposing a reliance on brittle conventions rather than robust belief tracking.
We use the YLE to investigate research questions in belief modelling, memory,
partner generalisation, and scaling to higher-order ToM.

</details>


### [117] [Advanced DOA Regulation with a Whale-Optimized Fractional Order Fuzzy PID Framework](https://arxiv.org/abs/2508.12487)
*Lida Shahbandari,Hossein Mohseni*

Main category: cs.AI

TL;DR: 本研究提出了一种基于鲸鱼优化算法的分数阶模糊 PID 控制器 (FOFPID)，用于管理双频指数 (BIS)，实验表明，FOFPID 控制器性能优于标准 FOPID 控制器


<details>
  <summary>Details</summary>
Motivation: 管理双频指数 (BIS)，使其保持在 40 到 60 的理想范围内

Method: 使用鲸鱼优化算法 (WOA) 的分数阶模糊 PID (FOFPID) 控制器

Result: FOFPID 控制器比标准分数阶 PID (FOPID) 控制器表现更好，实现了更快的稳定时间（2.5 分钟对比 3.2 分钟）和更低的稳态误差（0.5 对比 1.2）。

Conclusion: FOFPID 控制器在八种不同患者模型上的测试表现优于标准 FOPID 控制器，具有更快的稳定时间和更低的稳态误差，展示了其卓越的强度和准确性。它为自动化麻醉输送提供了一种可扩展的、人工智能驱动的解决方案，可以改善临床实践和改善患者的治疗效果。

Abstract: This study introduces a Fractional Order Fuzzy PID (FOFPID) controller that
uses the Whale Optimization Algorithm (WOA) to manage the Bispectral Index
(BIS), keeping it within the ideal range of forty to sixty. The FOFPID
controller combines fuzzy logic for adapting to changes and fractional order
dynamics for fine tuning. This allows it to adjust its control gains to handle
a person's unique physiology. The WOA helps fine tune the controller's
parameters, including the fractional orders and the fuzzy membership functions,
which boosts its performance. Tested on models of eight different patient
profiles, the FOFPID controller performed better than a standard Fractional
Order PID (FOPID) controller. It achieved faster settling times, at two and a
half minutes versus three point two minutes, and had a lower steady state
error, at zero point five versus one point two. These outcomes show the
FOFPID's excellent strength and accuracy. It offers a scalable, artificial
intelligence driven solution for automated anesthesia delivery that could
enhance clinical practice and improve patient results.

</details>


### [118] [Root Cause Analysis of Hydrogen Bond Separation in Spatio-Temporal Molecular Dynamics using Causal Models](https://arxiv.org/abs/2508.12500)
*Rahmat K. Adesunkanmi,Ashfaq Khokhar,Goce Trajcevski,Sohail Murad*

Main category: cs.AI

TL;DR: This paper introduces a causal modeling approach using a variational autoencoder-inspired architecture to identify the root causes of hydrogen bond formation and separation events in molecular dynamics simulations. The model is validated on chiral separation simulations, showing it can predict future events and identify key driving variables.


<details>
  <summary>Details</summary>
Motivation: Molecular dynamics simulations (MDS) face challenges, including resource-heavy computations and the need to manually scan outputs to detect interesting events, such as the formation and persistence of hydrogen bonds between atoms of different molecules. A critical research gap lies in identifying the underlying causes of hydrogen bond formation and separation -understanding which interactions or prior events contribute to their emergence over time. With this challenge in mind, we propose leveraging spatio-temporal data analytics and machine learning models to enhance the detection of these phenomena.

Method: causal modeling and aims to identify the root cause variables of hydrogen bond formation and separation events. Specifically, we treat the separation of hydrogen bonds as an intervention occurring and represent the causal structure of the bonding and separation events in the MDS as graphical causal models. These causal models are built using a variational autoencoder-inspired architecture that enables us to infer causal relationships across samples with diverse underlying causal graphs while leveraging shared dynamic information.

Result: We validate the efficacy of our model empirically on the atomic trajectories that used MDS for chiral separation, demonstrating that we can predict many steps in the future and also find the variables driving the observed changes in the system.

Conclusion: This framework provides a novel perspective on root cause analysis in molecular dynamic systems by constructing causal models that capture shifts in the conditional distributions of molecular interactions during bond formation or separation.

Abstract: Molecular dynamics simulations (MDS) face challenges, including
resource-heavy computations and the need to manually scan outputs to detect
"interesting events," such as the formation and persistence of hydrogen bonds
between atoms of different molecules. A critical research gap lies in
identifying the underlying causes of hydrogen bond formation and separation
-understanding which interactions or prior events contribute to their emergence
over time. With this challenge in mind, we propose leveraging spatio-temporal
data analytics and machine learning models to enhance the detection of these
phenomena. In this paper, our approach is inspired by causal modeling and aims
to identify the root cause variables of hydrogen bond formation and separation
events. Specifically, we treat the separation of hydrogen bonds as an
"intervention" occurring and represent the causal structure of the bonding and
separation events in the MDS as graphical causal models. These causal models
are built using a variational autoencoder-inspired architecture that enables us
to infer causal relationships across samples with diverse underlying causal
graphs while leveraging shared dynamic information. We further include a step
to infer the root causes of changes in the joint distribution of the causal
models. By constructing causal models that capture shifts in the conditional
distributions of molecular interactions during bond formation or separation,
this framework provides a novel perspective on root cause analysis in molecular
dynamic systems. We validate the efficacy of our model empirically on the
atomic trajectories that used MDS for chiral separation, demonstrating that we
can predict many steps in the future and also find the variables driving the
observed changes in the system.

</details>


### [119] [Help or Hurdle? Rethinking Model Context Protocol-Augmented Large Language Models](https://arxiv.org/abs/2508.12566)
*Wei Song,Haonan Zhong,Ziqi Ding,Jingling Xue,Yuekang Li*

Main category: cs.AI

TL;DR: The paper introduces MCPGAUGE, a framework for evaluating how LLMs use external resources, and finds limitations in current AI-tool integration.


<details>
  <summary>Details</summary>
Motivation: While commonly assumed to enhance performance, how LLMs actually leverage this capability remains poorly understood.

Method: We introduce MCPGAUGE, the first comprehensive evaluation framework for probing LLM-MCP interactions along four key dimensions: proactivity (self-initiated tool use), compliance (adherence to tool-use instructions), effectiveness (task performance post-integration), and overhead (computational cost incurred). MCPGAUGE comprises a 160-prompt suite and 25 datasets spanning knowledge comprehension, general reasoning, and code generation. Our large-scale evaluation, spanning six commercial LLMs, 30 MCP tool suites, and both one- and two-turn interaction settings, comprises around 20,000 API calls and over USD 6,000 in computational cost.

Result: This comprehensive study reveals four key findings that challenge prevailing assumptions about the effectiveness of MCP integration.

Conclusion: This study reveals four key findings that challenge prevailing assumptions about the effectiveness of MCP integration. These insights highlight critical limitations in current AI-tool integration and position MCPGAUGE as a principled benchmark for advancing controllable, tool-augmented LLMs.

Abstract: The Model Context Protocol (MCP) enables large language models (LLMs) to
access external resources on demand. While commonly assumed to enhance
performance, how LLMs actually leverage this capability remains poorly
understood. We introduce MCPGAUGE, the first comprehensive evaluation framework
for probing LLM-MCP interactions along four key dimensions: proactivity
(self-initiated tool use), compliance (adherence to tool-use instructions),
effectiveness (task performance post-integration), and overhead (computational
cost incurred). MCPGAUGE comprises a 160-prompt suite and 25 datasets spanning
knowledge comprehension, general reasoning, and code generation. Our
large-scale evaluation, spanning six commercial LLMs, 30 MCP tool suites, and
both one- and two-turn interaction settings, comprises around 20,000 API calls
and over USD 6,000 in computational cost. This comprehensive study reveals four
key findings that challenge prevailing assumptions about the effectiveness of
MCP integration. These insights highlight critical limitations in current
AI-tool integration and position MCPGAUGE as a principled benchmark for
advancing controllable, tool-augmented LLMs.

</details>


### [120] [An LLM + ASP Workflow for Joint Entity-Relation Extraction](https://arxiv.org/abs/2508.12611)
*Trang Tran,Trung Hoang Le,Huiping Cao,Tran Cao Son*

Main category: cs.AI

TL;DR: This paper proposes a generic workflow for joint entity-relation extraction (JERE) using large language models (LLMs) and Answer Set Programming (ASP). The results show that the LLM + ASP workflow is better than state-of-the-art JERE systems with only 10% of training data.


<details>
  <summary>Details</summary>
Motivation: Traditional machine-learning based approaches to performing this task require a large corpus of annotated data and lack the ability to easily incorporate domain specific information in the construction of the model. Therefore, creating a model for JERE is often labor intensive, time consuming, and elaboration intolerant.

Method: harnessing the capabilities of generative pretrained large language models (LLMs) and the knowledge representation and reasoning capabilities of Answer Set Programming (ASP) to perform JERE. We present a generic workflow for JERE using LLMs and ASP.

Result: LLM + ASP workflow is better than state-of-the-art JERE systems in several categories with only 10% of training data. It is able to achieve a 2.5 times (35% over 15%) improvement in the Relation Extraction task for the SciERC corpus

Conclusion: LLM + ASP workflow is better than state-of-the-art JERE systems in several categories with only 10% of training data. It is able to achieve a 2.5 times (35% over 15%) improvement in the Relation Extraction task for the SciERC corpus

Abstract: Joint entity-relation extraction (JERE) identifies both entities and their
relationships simultaneously. Traditional machine-learning based approaches to
performing this task require a large corpus of annotated data and lack the
ability to easily incorporate domain specific information in the construction
of the model. Therefore, creating a model for JERE is often labor intensive,
time consuming, and elaboration intolerant. In this paper, we propose
harnessing the capabilities of generative pretrained large language models
(LLMs) and the knowledge representation and reasoning capabilities of Answer
Set Programming (ASP) to perform JERE. We present a generic workflow for JERE
using LLMs and ASP. The workflow is generic in the sense that it can be applied
for JERE in any domain. It takes advantage of LLM's capability in natural
language understanding in that it works directly with unannotated text. It
exploits the elaboration tolerant feature of ASP in that no modification of its
core program is required when additional domain specific knowledge, in the form
of type specifications, is found and needs to be used. We demonstrate the
usefulness of the proposed workflow through experiments with limited training
data on three well-known benchmarks for JERE. The results of our experiments
show that the LLM + ASP workflow is better than state-of-the-art JERE systems
in several categories with only 10\% of training data. It is able to achieve a
2.5 times (35\% over 15\%) improvement in the Relation Extraction task for the
SciERC corpus, one of the most difficult benchmarks.

</details>


### [121] [Cognitive Structure Generation: From Educational Priors to Policy Optimization](https://arxiv.org/abs/2508.12647)
*Hengnian Gu,Zhifu Chen,Yuxin Chen,Jin Peng Zhou,Dongdai Zhou*

Main category: cs.AI

TL;DR: This paper introduces a Cognitive Structure Generation (CSG) framework to generate students' cognitive structures, which improves student modeling performance.


<details>
  <summary>Details</summary>
Motivation: cognitive structure assessment remains a long-standing challenge in student modeling and psychometrics, persisting as a foundational yet largely unassessable concept in educational practice

Method: introduce a novel framework, Cognitive Structure Generation (CSG), in which we first pretrain a Cognitive Structure Diffusion Probabilistic Model (CSDPM) to generate students' cognitive structures from educational priors, and then further optimize its generative process as a policy with hierarchical reward signals via reinforcement learning to align with genuine cognitive development levels during students' learning processes

Result: Experimental results on four popular real-world education datasets show that cognitive structures generated by CSG offer more comprehensive and effective representations for student modeling, substantially improving performance on KT and CD tasks while enhancing interpretability.

Conclusion: cognitive structures generated by CSG offer more comprehensive and effective representations for student modeling, substantially improving performance on KT and CD tasks while enhancing interpretability

Abstract: Cognitive structure is a student's subjective organization of an objective
knowledge system, reflected in the psychological construction of concepts and
their relations. However, cognitive structure assessment remains a
long-standing challenge in student modeling and psychometrics, persisting as a
foundational yet largely unassessable concept in educational practice. This
paper introduces a novel framework, Cognitive Structure Generation (CSG), in
which we first pretrain a Cognitive Structure Diffusion Probabilistic Model
(CSDPM) to generate students' cognitive structures from educational priors, and
then further optimize its generative process as a policy with hierarchical
reward signals via reinforcement learning to align with genuine cognitive
development levels during students' learning processes. Experimental results on
four popular real-world education datasets show that cognitive structures
generated by CSG offer more comprehensive and effective representations for
student modeling, substantially improving performance on KT and CD tasks while
enhancing interpretability.

</details>


### [122] [The Maximum Coverage Model and Recommendation System for UAV Vertiports Location Planning](https://arxiv.org/abs/2508.12651)
*Chunliang Hua,Xiao Hu,Jiayang Sun,Zeyuan Yang*

Main category: cs.AI

TL;DR: 本文提出了一个用于vertiport网络设计的实用工具，通过将数学严谨性与实际实施考虑相结合，弥合了理论位置建模与实际UAM基础设施规划之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有的规划框架仍然不足以应对这种复杂性，因为历史数据粒度和实际适用性的限制。本文旨在通过提出一种新的优化框架来解决这些差距。

Method: 提出了一种新的优化框架，即容量动态最大覆盖位置问题（CDMCLP），该框架同时模拟城市规模的时空需求、异构用户行为和基础设施容量约束。在此基础上，我们引入了一个集成的规划推荐系统，该系统将CDMCLP与社会经济因素和动态聚类初始化相结合。

Result: 在CDMCLP的评估和优化下，传统的位置选择方法的定量性能被暴露出来，可以提高38%-52%，同时推荐系统显示出用户友好性和复杂元素的有效集成。

Conclusion: 新的优化框架和推荐系统在中国中心城市得到验证，传统的位置选择方法可以通过CDMCLP的评估和优化提高38%-52%，同时推荐系统显示出用户友好性和复杂元素的有效集成。

Abstract: As urban aerial mobility (UAM) infrastructure development accelerates
globally, cities like Shenzhen are planning large-scale vertiport networks
(e.g., 1,200+ facilities by 2026). Existing planning frameworks remain
inadequate for this complexity due to historical limitations in data
granularity and real-world applicability. This paper addresses these gaps by
first proposing the Capacitated Dynamic Maximum Covering Location Problem
(CDMCLP), a novel optimization framework that simultaneously models urban-scale
spatial-temporal demand, heterogeneous user behaviors, and infrastructure
capacity constraints. Building on this foundation, we introduce an Integrated
Planning Recommendation System that combines CDMCLP with socio-economic factors
and dynamic clustering initialization. This system leverages adaptive parameter
tuning based on empirical user behavior to generate practical planning
solutions. Validation in a Chinese center city demonstrates the effectiveness
of the new optimization framework and recommendation system. Under the
evaluation and optimization of CDMCLP, the quantitative performance of
traditional location methods are exposed and can be improved by 38\%--52\%,
while the recommendation system shows user-friendliness and the effective
integration of complex elements. By integrating mathematical rigor with
practical implementation considerations, this hybrid approach bridges the gap
between theoretical location modeling and real-world UAM infrastructure
planning, offering municipalities a pragmatic tool for vertiport network
design.

</details>


### [123] [GridCodex: A RAG-Driven AI Framework for Power Grid Code Reasoning and Compliance](https://arxiv.org/abs/2508.12682)
*Jinquan Shi,Yingying Cheng,Fan Zhang,Miao Jiang,Jun Lin,Yanbai Shen*

Main category: cs.AI

TL;DR: GridCodex, a framework using large language models and RAG, helps with grid code reasoning and compliance, improving answer quality and recall rate.


<details>
  <summary>Details</summary>
Motivation: regulatory reasoning and compliance are vital for electricity industry but grid codes are complex and lack automated interpretation solutions

Method: using large language models and retrieval-augmented generation (RAG)

Result: GridCodex improves answer quality and recall rate

Conclusion: GridCodex improves answer quality by 26.4% and recall rate by more than 10 fold.

Abstract: The global shift towards renewable energy presents unprecedented challenges
for the electricity industry, making regulatory reasoning and compliance
increasingly vital. Grid codes, the regulations governing grid operations, are
complex and often lack automated interpretation solutions, which hinders
industry expansion and undermines profitability for electricity companies. We
introduce GridCodex, an end to end framework for grid code reasoning and
compliance that leverages large language models and retrieval-augmented
generation (RAG). Our framework advances conventional RAG workflows through
multi stage query refinement and enhanced retrieval with RAPTOR. We validate
the effectiveness of GridCodex with comprehensive benchmarks, including
automated answer assessment across multiple dimensions and regulatory agencies.
Experimental results showcase a 26.4% improvement in answer quality and more
than a 10 fold increase in recall rate. An ablation study further examines the
impact of base model selection.

</details>


### [124] [EGOILLUSION: Benchmarking Hallucinations in Egocentric Video Understanding](https://arxiv.org/abs/2508.12687)
*Ashish Seth,Utkarsh Tyagi,Ramaneswaran Selvakumar,Nishit Anand,Sonal Kumar,Sreyan Ghosh,Ramani Duraiswami,Chirag Agarwal,Dinesh Manocha*

Main category: cs.AI

TL;DR: EgoIllusion 是一个用于评估 MLLM 在以自我为中心的视频中产生幻觉的基准，结果表明现有模型在该方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 多模态大型语言模型 (MLLM) 在复杂的多模态任务中表现出了卓越的性能。虽然 MLLM 擅长第三人称和以自我为中心的视频中的视觉感知和推理，但它们容易产生幻觉，产生连贯但不准确的反应。

Method: EgoIllusion 包含 1,400 个视频，并配有 8,000 个人工注释的开放式和封闭式问题，旨在触发以自我为中心的视频中的视觉和听觉提示中的幻觉。

Result: 对 10 个 MLLM 的评估显示出重大挑战，包括像 GPT-4o 和 Gemini 这样的强大模型，仅达到 59% 的准确率。

Conclusion: EgoIllusion 为评估 MLLM 在以自我为中心的视频中产生幻觉奠定了基础，并促进了开发具有更低幻觉率的更好的以自我为中心的 MLLM。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
performance in complex multimodal tasks. While MLLMs excel at visual perception
and reasoning in third-person and egocentric videos, they are prone to
hallucinations, generating coherent yet inaccurate responses. We present
EgoIllusion, a first benchmark to evaluate MLLM hallucinations in egocentric
videos. EgoIllusion comprises 1,400 videos paired with 8,000 human-annotated
open and closed-ended questions designed to trigger hallucinations in both
visual and auditory cues in egocentric videos. Evaluations across ten MLLMs
reveal significant challenges, including powerful models like GPT-4o and
Gemini, achieving only 59% accuracy. EgoIllusion lays the foundation in
developing robust benchmarks to evaluate the effectiveness of MLLMs and spurs
the development of better egocentric MLLMs with reduced hallucination rates.
Our benchmark will be open-sourced for reproducibility.

</details>


### [125] [GTool: Graph Enhanced Tool Planning with Large Language Model](https://arxiv.org/abs/2508.12725)
*Wenjie Chen,Wenbin Li,Di Yao,Xuying Meng,Chang Gong,Jingping Bi*

Main category: cs.AI

TL;DR: GTool improves LLM tool planning by constructing a tool graph and predicting missing dependencies, achieving 29.6% performance gains over SOTA baselines.


<details>
  <summary>Details</summary>
Motivation: Current works treat different tools as isolated components and fail to leverage the inherent dependencies of tools, leading to invalid planning results. Since tool dependencies are often incomplete, it becomes challenging for LLMs to accurately identify the appropriate tools required by a user request, especially when confronted with a large toolset.

Method: GTool constructs a request-specific tool graph to select tools efficiently and generate the <graph token> which provides sufficient dependency information understandable by LLMs. Moreover, a missing dependency prediction task is designed to improve the reliability of GTool with incomplete dependencies.

Result: GTool enhances the tool planning ability of LLMs under incomplete dependencies.

Conclusion: GTool achieves more than 29.6% performance improvements compared with the state-of-the-art (SOTA) baselines with a light-weight (7B) LLM backbone.

Abstract: Tool planning with large language models (LLMs), referring to selecting,
organizing, and preparing the tools necessary to complete a user request,
bridges the gap between natural language understanding and task execution.
However, current works treat different tools as isolated components and fail to
leverage the inherent dependencies of tools, leading to invalid planning
results. Since tool dependencies are often incomplete, it becomes challenging
for LLMs to accurately identify the appropriate tools required by a user
request, especially when confronted with a large toolset. To solve this
challenge, we propose \texttt{GTool}, which is the first work aiming to enhance
the tool planning ability of LLMs under incomplete dependencies. \texttt{GTool}
constructs a request-specific tool graph to select tools efficiently and
generate the \texttt{<graph token>} which provides sufficient dependency
information understandable by LLMs. Moreover, a missing dependency prediction
task is designed to improve the reliability of \texttt{GTool} with incomplete
dependencies. Without trimming LLMs, \texttt{GTool} can be seamlessly
integrated with various LLM backbones without extensive retraining. Extensive
experiments show that \texttt{GTool} achieves more than 29.6\% performance
improvements compared with the state-of-the-art (SOTA) baselines with a
light-weight (7B) LLM backbone.

</details>


### [126] [Beyond Ethical Alignment: Evaluating LLMs as Artificial Moral Assistants](https://arxiv.org/abs/2508.12754)
*Alessio Galatolo,Luca Alberto Rappuoli,Katie Winkle,Meriem Beloucif*

Main category: cs.AI

TL;DR: 本文研究了LLM作为人工道德助理（AMA）的能力，设计了一个新的框架和基准来评估其道德推理能力，结果表明LLM在溯因道德推理方面存在不足，需要专门的策略来提高其道德推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的道德能力引起了相当大的关注。现有的基准和评估在很大程度上仍然是肤浅的，通常基于最终的伦理判决而不是明确的道德推理来衡量一致性。

Method: 设计了一个新的形式框架，描述了一个AMA应该展示的特定行为，区分了诸如演绎和溯因道德推理等关键品质。建立在此理论框架的基础上，开发了一个基准来测试这些品质，并评估流行的开放LLM。

Result: 结果揭示了模型之间相当大的差异，并突出了持续存在的缺点，尤其是在溯因道德推理方面。

Conclusion: LLMs表现出相当大的差异，并突出了持续存在的缺点，尤其是在溯因道德推理方面。强调需要专门的策略来明确提高LLM中的道德推理能力。

Abstract: The recent rise in popularity of large language models (LLMs) has prompted
considerable concerns about their moral capabilities. Although considerable
effort has been dedicated to aligning LLMs with human moral values, existing
benchmarks and evaluations remain largely superficial, typically measuring
alignment based on final ethical verdicts rather than explicit moral reasoning.
In response, this paper aims to advance the investigation of LLMs' moral
capabilities by examining their capacity to function as Artificial Moral
Assistants (AMAs), systems envisioned in the philosophical literature to
support human moral deliberation. We assert that qualifying as an AMA requires
more than what state-of-the-art alignment techniques aim to achieve: not only
must AMAs be able to discern ethically problematic situations, they should also
be able to actively reason about them, navigating between conflicting values
outside of those embedded in the alignment phase. Building on existing
philosophical literature, we begin by designing a new formal framework of the
specific kind of behaviour an AMA should exhibit, individuating key qualities
such as deductive and abductive moral reasoning. Drawing on this theoretical
framework, we develop a benchmark to test these qualities and evaluate popular
open LLMs against it. Our results reveal considerable variability across models
and highlight persistent shortcomings, particularly regarding abductive moral
reasoning. Our work connects theoretical philosophy with practical AI
evaluation while also emphasising the need for dedicated strategies to
explicitly enhance moral reasoning capabilities in LLMs. Code available at
https://github.com/alessioGalatolo/AMAeval

</details>


### [127] [HeroBench: A Benchmark for Long-Horizon Planning and Structured Reasoning in Virtual Worlds](https://arxiv.org/abs/2508.12782)
*Petr Anokhin,Roman Khalikov,Stefan Rebrikov,Viktor Volkov,Artyom Sorokin,Vincent Bissonnette*

Main category: cs.AI

TL;DR: HeroBench, a new benchmark for evaluating long-horizon planning in LLMs within complex virtual worlds, reveals performance disparities and weaknesses in current models.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks typically assess LLMs through abstract or low-dimensional algorithmic tasks, failing to capture the complexity of realistic planning environments; LLMs' proficiency in long-horizon planning remains underexplored.

Method: Introduce HeroBench, a novel benchmark designed specifically to evaluate long-horizon planning and structured reasoning within complex RPG-inspired virtual worlds.

Result: Extensive evaluation of 25 state-of-the-art LLMs reveals substantial performance disparities rarely observed in conventional reasoning benchmarks; Detailed error analysis further uncovers specific weaknesses in current models' abilities to generate robust high-level plans and reliably execute structured actions.

Conclusion: HeroBench significantly advances the evaluation of LLM reasoning and provides a flexible, scalable foundation for future research into advanced, autonomous planning in virtual environments.

Abstract: Large language models (LLMs) have shown remarkable capabilities in isolated
step-by-step reasoning tasks such as mathematics and programming, but their
proficiency in long-horizon planning, where solutions require extended,
structured sequences of interdependent actions, remains underexplored. Existing
benchmarks typically assess LLMs through abstract or low-dimensional
algorithmic tasks, failing to capture the complexity of realistic planning
environments. We introduce HeroBench, a novel benchmark designed specifically
to evaluate long-horizon planning and structured reasoning within complex
RPG-inspired virtual worlds. HeroBench provides a rigorously constructed
dataset of tasks covering a wide range of difficulties, a simulated environment
to execute and validate agent plans, and detailed analytical tools for
evaluating model performance. Tasks challenge models to formulate strategic
plans, efficiently gather resources, master necessary skills, craft equipment,
and defeat adversaries, reflecting practical scenarios' layered dependencies
and constraints. Our extensive evaluation of 25 state-of-the-art LLMs, spanning
both open-source and proprietary models, including the GPT-5 family, reveals
substantial performance disparities rarely observed in conventional reasoning
benchmarks. Detailed error analysis further uncovers specific weaknesses in
current models' abilities to generate robust high-level plans and reliably
execute structured actions. HeroBench thus not only significantly advances the
evaluation of LLM reasoning but also provides a flexible, scalable foundation
for future research into advanced, autonomous planning in virtual environments.

</details>


### [128] [Reinforcement Learning with Rubric Anchors](https://arxiv.org/abs/2508.12790)
*Zenan Huang,Yihong Zhuang,Guoshan Lu,Zeyu Qin,Haokai Xu,Tianyu Zhao,Ru Peng,Jiaqi Hu,Zhanming Shen,Xiaomeng Hu,Xijun Gu,Peiyi Tu,Jiaxin Liu,Wenyu Chen,Yuzhuo Fu,Zhiting Fan,Yanmei Gu,Yuanyuan Wang,Zhengkai Yang,Jianguo Li,Junbo Zhao*

Main category: cs.AI

TL;DR: 该论文提出了一种基于rubric奖励的强化学习方法，用于增强LLM在开放式任务中的表现，通过构建大型rubric奖励系统并在Qwen-30B-A3B模型上进行实验，取得了显著成果，并在风格控制方面有所提升。


<details>
  <summary>Details</summary>
Motivation: 现有的从可验证奖励中进行强化学习的方法主要局限于具有自动可检查结果的领域，为了克服这一限制，将RLVR扩展到开放式任务。

Method: 该论文提出了一种基于rubric的强化学习方法，用于开放式任务，通过整合rubric奖励，利用精心设计的rubric作为结构化的、模型可解释的标准，对主观输出进行自动评分。

Result: 仅使用5K+样本，该系统在开放式benchmark上提高了+5.2%，胜过671B DeepSeek-V3模型+2.4%，同时保留了一般的推理能力。该方法还提供了细粒度的风格控制，可以使用rubric作为anchor来减轻“AI-like”的语气，并产生更像人类的、更富有表现力的反应。

Conclusion: 该方法通过使用人工或LLM构建的超过10,000个rubric的大型rubric奖励系统，并开源了一个Qwen-30B-A3B模型，在开放式任务上取得了显著的成果，并在人文领域表现突出，胜过DeepSeek-V3模型，同时保持了通用和推理能力。该方法还提供了细粒度的风格控制，并分享了rubric构建、数据选择和训练的关键经验。

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a
powerful paradigm for enhancing Large Language Models (LLMs), exemplified by
the success of OpenAI's o-series. In RLVR, rewards are derived from verifiable
signals-such as passing unit tests in code generation or matching correct
answers in mathematical reasoning. While effective, this requirement largely
confines RLVR to domains with automatically checkable outcomes. To overcome
this, we extend the RLVR paradigm to open-ended tasks by integrating
rubric-based rewards, where carefully designed rubrics serve as structured,
model-interpretable criteria for automatic scoring of subjective outputs. We
construct, to our knowledge, the largest rubric reward system to date, with
over 10,000 rubrics from humans, LLMs, or a hybrid human-LLM collaboration.
Implementing rubric-based RL is challenging; we tackle these issues with a
clear framework and present an open-sourced Qwen-30B-A3B model with notable
gains: 1) With only 5K+ samples, our system improves by +5.2% on open-ended
benchmarks (especially humanities), outperforming a 671B DeepSeek-V3 model by
+2.4%, while preserving general and reasoning abilities. 2) Our method provides
fine-grained stylistic control, using rubrics as anchors to mitigate the
"AI-like" tone and produce more human-like, expressive responses. We share key
lessons in rubric construction, data selection, and training, and discuss
limitations and future releases.

</details>


### [129] [[Social] Allostasis: Or, How I Learned To Stop Worrying and Love The Noise](https://arxiv.org/abs/2508.12791)
*Imran Khan*

Main category: cs.AI

TL;DR: This paper presents a computational model of allostasis and social allostasis, demonstrating that agents can use environmental and social noise for adaptive reconfiguration, improving viability compared to homeostatic agents.


<details>
  <summary>Details</summary>
Motivation: biological and artificial systems can proactively leverage perturbations to reconfigure their regulatory parameters in anticipation of environmental demands

Method: a computational model of allostatic and social allostatic regulation that employs biophysiologically inspired signal transducers, analogous to hormones like cortisol and oxytocin, to encode information from both the environment and social interactions, which mediate this dynamic reconfiguration. The models are tested in a small society of animats across several dynamic environments, using an agent-based model

Result: allostatic and social allostatic regulation enable agents to leverage environmental and social noise for adaptive reconfiguration

Conclusion: allostatic and social allostatic regulation enable agents to leverage environmental and social noise for adaptive reconfiguration, leading to improved viability compared to purely reactive homeostatic agents

Abstract: The notion of homeostasis typically conceptualises biological and artificial
systems as maintaining stability by resisting deviations caused by
environmental and social perturbations. In contrast, (social) allostasis
proposes that these systems can proactively leverage these very perturbations
to reconfigure their regulatory parameters in anticipation of environmental
demands, aligning with von Foerster's ``order through noise'' principle. This
paper formulates a computational model of allostatic and social allostatic
regulation that employs biophysiologically inspired signal transducers,
analogous to hormones like cortisol and oxytocin, to encode information from
both the environment and social interactions, which mediate this dynamic
reconfiguration. The models are tested in a small society of ``animats'' across
several dynamic environments, using an agent-based model. The results show that
allostatic and social allostatic regulation enable agents to leverage
environmental and social ``noise'' for adaptive reconfiguration, leading to
improved viability compared to purely reactive homeostatic agents. This work
offers a novel computational perspective on the principles of social allostasis
and their potential for designing more robust, bio-inspired, adaptive systems

</details>


### [130] [Scaling Multi-Agent Epistemic Planning through GNN-Derived Heuristics](https://arxiv.org/abs/2508.12840)
*Giovanni Briglia,Francesco Fabiano,Stefano Mariani*

Main category: cs.AI

TL;DR: This paper uses Graph Neural Networks to improve the scalability of multi-agent epistemic planning.


<details>
  <summary>Details</summary>
Motivation: The richness of MEP requires states to be represented as Kripke structures, i.e., directed labeled graphs. This representation limits the applicability of existing heuristics, hindering the scalability of epistemic solvers, which must explore an exponential search space without guidance, resulting often in intractability.

Method: This paper exploits Graph Neural Networks (GNNs) to learn patterns and relational structures within epistemic states, to guide the planning process.

Result: The evaluation against standard baselines shows significant improvements in the scalability of multi-agent epistemic planning.

Conclusion: This paper integrates predictive heuristics into an epistemic planning pipeline and demonstrates significant improvements in the scalability of multi-agent epistemic planning.

Abstract: Multi-agent Epistemic Planning (MEP) is an autonomous planning framework for
reasoning about both the physical world and the beliefs of agents, with
applications in domains where information flow and awareness among agents are
critical. The richness of MEP requires states to be represented as Kripke
structures, i.e., directed labeled graphs. This representation limits the
applicability of existing heuristics, hindering the scalability of epistemic
solvers, which must explore an exponential search space without guidance,
resulting often in intractability. To address this, we exploit Graph Neural
Networks (GNNs) to learn patterns and relational structures within epistemic
states, to guide the planning process. GNNs, which naturally capture the
graph-like nature of Kripke models, allow us to derive meaningful estimates of
state quality -- e.g., the distance from the nearest goal -- by generalizing
knowledge obtained from previously solved planning instances. We integrate
these predictive heuristics into an epistemic planning pipeline and evaluate
them against standard baselines, showing significant improvements in the
scalability of multi-agent epistemic planning.

</details>


### [131] [CAMAR: Continuous Actions Multi-Agent Routing](https://arxiv.org/abs/2508.12845)
*Artem Pshenitsyn,Aleksandr Panov,Alexey Skrynnik*

Main category: cs.AI

TL;DR: Introduces CAMAR, a new MARL benchmark for multi-agent pathfinding with continuous actions, supporting both cooperative and competitive scenarios.


<details>
  <summary>Details</summary>
Motivation: Few MARL benchmarks combine continuous state and action spaces with challenging coordination and planning tasks.

Method: A new MARL benchmark designed explicitly for multi-agent pathfinding in environments with continuous actions, integration of classical planning methods such as RRT and RRT* into MARL pipelines

Result: CAMAR supports cooperative and competitive interactions between agents and runs efficiently at up to 100,000 environment steps per second. A three-tier evaluation protocol is proposed to better track algorithmic progress and enable deeper analysis of performance.

Conclusion: CAMAR is a challenging and realistic testbed for the MARL community.

Abstract: Multi-agent reinforcement learning (MARL) is a powerful paradigm for solving
cooperative and competitive decision-making problems. While many MARL
benchmarks have been proposed, few combine continuous state and action spaces
with challenging coordination and planning tasks. We introduce CAMAR, a new
MARL benchmark designed explicitly for multi-agent pathfinding in environments
with continuous actions. CAMAR supports cooperative and competitive
interactions between agents and runs efficiently at up to 100,000 environment
steps per second. We also propose a three-tier evaluation protocol to better
track algorithmic progress and enable deeper analysis of performance. In
addition, CAMAR allows the integration of classical planning methods such as
RRT and RRT* into MARL pipelines. We use them as standalone baselines and
combine RRT* with popular MARL algorithms to create hybrid approaches. We
provide a suite of test scenarios and benchmarking tools to ensure
reproducibility and fair comparison. Experiments show that CAMAR presents a
challenging and realistic testbed for the MARL community.

</details>


### [132] [E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model](https://arxiv.org/abs/2508.12854)
*Ronghao Lin,Shuai Shen,Weipeng Hu,Qiaolin He,Aolin Xiong,Li Huang,Haifeng Hu,Yap-peng Tan*

Main category: cs.AI

TL;DR: E3RG是一种基于多模态LLM的显式情感驱动的共情响应生成系统，它通过分解MERG任务并集成先进的生成模型，在多模态情感内容处理和身份一致性方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 多模态共情响应生成 (MERG) 对于构建情感智能人机交互至关重要。虽然大型语言模型 (LLM) 改进了基于文本的ERG，但在处理多模态情感内容和保持身份一致性方面仍然存在挑战。

Method: E3RG将MERG任务分解为三个部分：多模态共情理解、共情记忆检索和多模态响应生成。通过集成先进的表达性语音和视频生成模型，E3RG无需额外训练即可提供自然、情感丰富且身份一致的响应。

Result: E3RG在零样本和小样本设置下的实验验证了其优越性。

Conclusion: E3RG在零样本和小样本设置下均表现出色，并在ACM MM 25的基于化身的Multimodal Empathy Challenge中获得第一名。

Abstract: Multimodal Empathetic Response Generation (MERG) is crucial for building
emotionally intelligent human-computer interactions. Although large language
models (LLMs) have improved text-based ERG, challenges remain in handling
multimodal emotional content and maintaining identity consistency. Thus, we
propose E3RG, an Explicit Emotion-driven Empathetic Response Generation System
based on multimodal LLMs which decomposes MERG task into three parts:
multimodal empathy understanding, empathy memory retrieval, and multimodal
response generation. By integrating advanced expressive speech and video
generative models, E3RG delivers natural, emotionally rich, and
identity-consistent responses without extra training. Experiments validate the
superiority of our system on both zero-shot and few-shot settings, securing
Top-1 position in the Avatar-based Multimodal Empathy Challenge on ACM MM 25.
Our code is available at https://github.com/RH-Lin/E3RG.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [133] [LSM-OPD: Boosting Scan in LSM-Trees by Enabling Direct Computing on Compressed Data](https://arxiv.org/abs/2508.11862)
*Jianfeng Huang,Ziyao Wang,Lin Yuan,Jiajie Wen,Yihao Cao,Dongjing Miao,Yong Wang,Jiahao Zhang*

Main category: cs.DB

TL;DR: LSM-OPD是一种日志结构合并-顺序保持字典编码方案，通过支持在压缩数据上直接计算，解决了LSM树中扫描操作的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 扫描操作（如后台压缩和值过滤）已成为LSM树支持当代数据密集型应用的主要瓶颈。对于较慢的外部存储设备（如HDD和SATA SSD），由于LSM树中大量的读/写放大，扫描性能主要受I/O带宽的限制（即I/O bound）。最近采用的高性能存储设备（如NVMe SSD）已将主要限制转变为计算限制，从而突显了由低效压缩和过滤引起的计算资源消耗的影响。然而，当值大小增加时，快速设备中扫描性能的瓶颈也逐渐转向I/O带宽，并且所有类型设备的整体吞吐量都会急剧下降。

Method: 提出了一种日志结构合并-顺序保持字典编码方案LSM-OPD，该方案支持在LSM树中直接对压缩数据进行计算。它首先实现了键值分离的数据刷新到磁盘，采用密集编码的列式布局，理想情况下，对于大型字符串值（例如1024字节），只需少量字节，从而显著减轻了由密集扫描引起的频繁I/O请求。然后，由于其顺序保持特性，它能够将对大型值的昂贵扫描操作（包括压缩和值过滤）卸载到轻量级字典。并且现在可以采用基于SIMD的向量化来最大化现代多核处理器上的评估性能，进一步打破LSM树中的计算限制。

Result: LSM-OPD通过在LSM树中直接对压缩数据进行计算，解决了核心问题。

Conclusion: LSM-OPD在处理涉及对各种现代存储设备进行密集扫描操作的各种工作负载方面表现出卓越的效率。

Abstract: Scan-based operations, such as backstage compaction and value filtering, have
emerged as the main bottleneck for LSM-Trees in supporting contemporary
data-intensive applications. For slower external storage devices, such as HDD
and SATA SSD, the scan performance is primarily limited by the I/O bandwidth
(i.e., I/O bound) due to the substantial read/write amplifications in
LSM-Trees. Recent adoption of high-performance storage devices, such as NVMe
SSD, has transformed the main limitation to be compute-bound, emerging the
impact of computational resource consumption caused by inefficient compactions
and filtering. However, when the value size increases, the bottleneck for scan
performance in fast devices gradually shifts towards the I/O bandwidth as well,
and the overall throughput across all types of devices undergo a dramatic
reduction. This paper addresses the core issues by proposing LSM-OPD, a Log-S
tructured M erge-O rder- Preserving Dictionary encoding scheme that enables
direct computing on compressed data within LSM-Trees. It first enables
key-value-separated data flushing to disk in a densely encoded columnar layout,
ideally with few bytes for a large string value (e.g., 1024 bytes), thereby
significantly alleviating the frequent I/O requests caused by intensive scans.
Then, it is capable of offloading the costly scan-based operations on large
values, including compaction and value filtering, to lightweight dictionaries
due to the order-preserving property. And SIMD-based vectorization can now be
employed to maximize the evaluating performance on modern multi-core
processors, further breaking the compute-bound limitations in LSM-trees.
Extensive experiments demonstrate the superior efficiency of LSM-OPD in
processing various workloads that involve intensive scan-based operations on
diverse modern storage devices.

</details>


### [134] [Carry the Tail in Consensus Protocols](https://arxiv.org/abs/2508.12173)
*Suyash Gupta,Dakai Kang,Dahlia Malkhi,Mohammad Sadoghi*

Main category: cs.DB

TL;DR: Presents Carry-the-Tail, a deterministic atomic broadcast protocol addressing tail-forking attacks with improved communication efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing atomic broadcast solutions face throughput degradation under tail-forking attacks, and solutions require either quadratic communication steps or computationally-prohibitive SNARK generation.

Method: The key technical contribution is Carry, a practical drop-in mechanism for streamlined protocols in the HotStuff family.

Result: Carry guarantees good performance against tail-forking and removes most leader-induced stalls, while retaining linear traffic and protocol simplicity.

Conclusion: The paper introduces Carry-the-Tail, a deterministic atomic broadcast protocol that ensures a constant fraction of commits by non-faulty leaders against tail-forking attacks and maintains optimal communication complexity.

Abstract: We present Carry-the-Tail, the first deterministic atomic broadcast protocol
in partial synchrony that, after GST, guarantees a constant fraction of commits
by non-faulty leaders against tail-forking attacks, and maintains optimal,
worst-case quadratic communication under a cascade of faulty leaders. The
solution also guarantees linear amortized communication, i.e., the steady-state
is linear.
  Prior atomic broadcast solutions achieve quadratic word communication
complexity in the worst case. However, they face a significant degradation in
throughput under tail-forking attack. Existing solutions to tail-forking
attacks require either quadratic communication steps or
computationally-prohibitive SNARK generation.
  The key technical contribution is Carry, a practical drop-in mechanism for
streamlined protocols in the HotStuff family. Carry guarantees good performance
against tail-forking and removes most leader-induced stalls, while retaining
linear traffic and protocol simplicity.

</details>


### [135] [jXBW: Fast Substructure Search in Large-Scale JSONL Datasets for Foundation Model Applications](https://arxiv.org/abs/2508.12536)
*Yasuo Tabei*

Main category: cs.DB

TL;DR: jXBW 是一种快速的 JSONL 子结构搜索方法，它通过合并树表示、简洁的数据结构和高效的搜索算法，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: JSON Lines (JSONL) 数据集中的子结构搜索对于现代应用程序（如基础模型中的提示工程）至关重要，但现有方法由于穷举树遍历和子树匹配而导致计算成本过高。

Method: 一种快速的 JSONL 数据集子结构搜索方法，它具有三个关键技术贡献：(i) 通过合并多个 JSON 对象的树构建的合并树表示，同时保留单个标识，(ii) 一种基于扩展 Burrows-Wheeler 变换的简洁数据结构，可以实现高效的树导航和子路径搜索，以及 (iii) 一种高效的三步子结构搜索算法，该算法结合了路径分解、祖先计算和自适应树标识符收集，以确保正确性，同时避免穷举树遍历。

Result: jXBW 在较小的数据集上实现了 16 倍的加速，在较大的数据集上实现了高达 4,700 倍的加速（相比于基于树的方法），并且比基于 XML 的处理方法快 6×10^6 倍以上，同时保持了有竞争力的内存使用。

Conclusion: jXBW在真实数据集上的实验评估表明，它始终优于现有方法，在较小的数据集上实现了 16 倍的加速，在较大的数据集上实现了高达 4,700 倍的加速（相比于基于树的方法），并且比基于 XML 的处理方法快 6×10^6 倍以上，同时保持了有竞争力的内存使用。

Abstract: Substructure search in JSON Lines (JSONL) datasets is essential for modern
applications such as prompt engineering in foundation models, but existing
methods suffer from prohibitive computational costs due to exhaustive tree
traversal and subtree matching. We present jXBW, a fast method for substructure
search on large-scale JSONL datasets. Our method makes three key technical
contributions: (i) a merged tree representation built by merging trees of
multiple JSON objects while preserving individual identities, (ii) a succinct
data structure based on the eXtended Burrows-Wheeler Transform that enables
efficient tree navigation and subpath search, and (iii) an efficient three-step
substructure search algorithm that combines path decomposition, ancestor
computation, and adaptive tree identifier collection to ensure correctness
while avoiding exhaustive tree traversal. Experimental evaluation on real-world
datasets demonstrates that jXBW consistently outperforms existing methods,
achieving speedups of 16$\times$ for smaller datasets and up to 4,700$\times$
for larger datasets over tree-based approaches, and more than 6$\times$10$^6$
over XML-based processing while maintaining competitive memory usage.

</details>


### [136] [Evaluating the Quality of Open Building Datasets for Mapping Urban Inequality: A Comparative Analysis Across 5 Cities](https://arxiv.org/abs/2508.12872)
*Franz Okyere,Meng Lu,Ansgar Brunn*

Main category: cs.DB

TL;DR: 本研究评估了AI生成的开放建筑数据集在不同城市的数据质量，发现数据质量存在显著差异，需要考虑数据集质量以避免错误表示。


<details>
  <summary>Details</summary>
Motivation: 非正式住区缺乏重点发展且高度动态，这些地方的空间数据质量可能不确定。本研究评估了由谷歌和微软生成的AI开放建筑数据集(OBD)的质量和偏差。

Method: 使用Intersection over Union (IoU)、重叠分析和位置精度算法来分析数据集的相似性和对齐情况。还分析了建筑物多边形区域的大小分布和完整性。

Result: 休斯顿和柏林表现出高度的一致性和完整性，反映了它们结构化的城市环境。在所分析的数据集中存在差距，阿克拉和加拉加斯等城市可能代表性不足。不同的建筑规模分布可能表明全球社会经济差距。

Conclusion: AI生成的开放建筑数据集（OBD）的数据质量存在显著差异，休斯顿和柏林表现出高度的一致性和完整性，而阿克拉和加拉加斯等城市可能代表性不足。需要考虑全球建筑数据集的质量，以避免错误表示。

Abstract: While informal settlements lack focused development and are highly dynamic,
the quality of spatial data for these places may be uncertain. This study
evaluates the quality and biases of AI-generated Open Building Datasets (OBDs)
generated by Google and Microsoft against OpenStreetMap (OSM) data, across
diverse global cities including Accra, Nairobi, Caracas, Berlin, and Houston.
The Intersection over Union (IoU), overlap analysis and a positional accuracy
algorithm are used to analyse the similarity and alignment of the datasets. The
paper also analyses the size distribution of the building polygon area, and
completeness using predefined but regular spatial units. The results indicate
significant variance in data quality, with Houston and Berlin demonstrating
high alignment and completeness, reflecting their structured urban
environments. There are gaps in the datasets analysed, and cities like Accra
and Caracas may be under-represented. This could highlight difficulties in
capturing complex or informal regions. The study also notes different building
size distributions, which may be indicative of the global socio-economic
divide. These findings may emphasise the need to consider the quality of global
building datasets to avoid misrepresentation, which is an important element of
planning and resource distribution.

</details>


### [137] [SPARQL in N3: SPARQL CONSTRUCT as a rule language for the Semantic Web (Extended Version)](https://arxiv.org/abs/2508.13041)
*Dörthe Arndt,William Van Woensel,Dominik Tomaszuk*

Main category: cs.DB

TL;DR: Leveraging SPARQL CONSTRUCT queries as logic rules and translating these queries to the Notation3 Logic (N3) rule language.


<details>
  <summary>Details</summary>
Motivation: Reasoning in the Semantic Web (SW) commonly uses Description Logics (DL) via OWL2 DL ontologies, or SWRL for variables and Horn clauses. The Rule Interchange Format (RIF) offers more expressive rules but is defined outside RDF and rarely adopted. For querying, SPARQL is a well-established standard operating directly on RDF triples.

Method: We leverage SPARQL CONSTRUCT queries as logic rules, enabling (1) an expressive, familiar SW rule language, and (2) general recursion, where queries can act on the results of others. We translate these queries to the Notation3 Logic (N3) rule language, allowing use of existing reasoning machinery with forward and backward chaining. Targeting a one-to-one query-rule mapping improves exchangeability and interpretability.

Result: Enabling (1) an expressive, familiar SW rule language, and (2) general recursion, where queries can act on the results of others.

Conclusion: Benchmarks indicate competitive performance, aiming to advance the potential of rule-based reasoning in the SW.

Abstract: Reasoning in the Semantic Web (SW) commonly uses Description Logics (DL) via
OWL2 DL ontologies, or SWRL for variables and Horn clauses. The Rule
Interchange Format (RIF) offers more expressive rules but is defined outside
RDF and rarely adopted. For querying, SPARQL is a well-established standard
operating directly on RDF triples. We leverage SPARQL CONSTRUCT queries as
logic rules, enabling (1) an expressive, familiar SW rule language, and (2)
general recursion, where queries can act on the results of others. We translate
these queries to the Notation3 Logic (N3) rule language, allowing use of
existing reasoning machinery with forward and backward chaining. Targeting a
one-to-one query-rule mapping improves exchangeability and interpretability.
Benchmarks indicate competitive performance, aiming to advance the potential of
rule-based reasoning in the SW.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [138] [RRRA: Resampling and Reranking through a Retriever Adapter](https://arxiv.org/abs/2508.11670)
*Bongsu Kim*

Main category: cs.IR

TL;DR: This paper proposes a learnable adapter module to model false negatives in dense retrieval, which improves performance on standard benchmarks.


<details>
  <summary>Details</summary>
Motivation: Recent methods apply heuristics based on positive document scores to identify hard negatives, improving both performance and interpretability. However, these global, example agnostic strategies often miss instance specific false negatives.

Method: a learnable adapter module that monitors Bi-Encoder representations to estimate the likelihood that a hard negative is actually a false negative. This probability is modeled dynamically and contextually, enabling fine-grained, query specific judgments. The predicted scores are used in two downstream components: (1) resampling, where negatives are reweighted during training, and (2) reranking, where top-k retrieved documents are reordered at inference.

Result: Empirical results on standard benchmarks show that our adapter-enhanced framework consistently outperforms strong Bi-Encoder baselines

Conclusion: The adapter-enhanced framework consistently outperforms strong Bi-Encoder baselines, underscoring the benefit of explicit false negative modeling in dense retrieval.

Abstract: In dense retrieval, effective training hinges on selecting high quality hard
negatives while avoiding false negatives. Recent methods apply heuristics based
on positive document scores to identify hard negatives, improving both
performance and interpretability. However, these global, example agnostic
strategies often miss instance specific false negatives. To address this, we
propose a learnable adapter module that monitors Bi-Encoder representations to
estimate the likelihood that a hard negative is actually a false negative. This
probability is modeled dynamically and contextually, enabling fine-grained,
query specific judgments. The predicted scores are used in two downstream
components: (1) resampling, where negatives are reweighted during training, and
(2) reranking, where top-k retrieved documents are reordered at inference.
Empirical results on standard benchmarks show that our adapter-enhanced
framework consistently outperforms strong Bi-Encoder baselines, underscoring
the benefit of explicit false negative modeling in dense retrieval.

</details>


### [139] [LLM-Based Intelligent Agents for Music Recommendation: A Comparison with Classical Content-Based Filtering](https://arxiv.org/abs/2508.11671)
*Ronald Carvalho Boadana,Ademir Guimarães da Costa Junior,Ricardo Rios,Fábio Santos da Silva*

Main category: cs.IR

TL;DR: LLMs show promise in personalized music recommendation systems by addressing information overload and achieving high user satisfaction.


<details>
  <summary>Details</summary>
Motivation: Address information overload for users due to the growing availability of music on streaming platforms and enhance user experience.

Method: Large Language Models (LLMs) from the Gemini and LLaMA families, combined with intelligent agents

Result: LLMs achieved satisfaction rates of up to 89.32%.

Conclusion: LLMs achieved high user satisfaction (up to 89.32%) in music recommendation, showing their potential.

Abstract: The growing availability of music on streaming platforms has led to
information overload for users. To address this issue and enhance the user
experience, increasingly sophisticated recommendation systems have been
proposed. This work investigates the use of Large Language Models (LLMs) from
the Gemini and LLaMA families, combined with intelligent agents, in a
multi-agent personalized music recommendation system. The results are compared
with a traditional content-based recommendation model, considering user
satisfaction, novelty, and computational efficiency. LLMs achieved satisfaction
rates of up to \textit{89{,}32\%}, indicating their promising potential in
music recommendation systems.

</details>


### [140] [Ontology-Guided Query Expansion for Biomedical Document Retrieval using Large Language Models](https://arxiv.org/abs/2508.11784)
*Zabir Al Nazi,Vagelis Hristidis,Aaron Lawson McLean,Jannat Ara Meem,Md Taukir Azam Chowdhury*

Main category: cs.IR

TL;DR: BMQExpander是一种新的查询扩展方法，它利用医学知识和大型语言模型来提高生物医学文档检索的有效性，并在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 大型生物医学文档集合上的有效问答（QA）需要有效的文档检索技术。由于特定领域的词汇和用户查询中的语义模糊性，后者仍然是一项具有挑战性的任务。

Method: 结合UMLS Metathesaurus的医学知识与大型语言模型（LLM）的生成能力，提出了一种新的本体感知查询扩展管道BMQExpander。

Result: BMQExpander在NFCorpus、TREC-COVID和SciFact这三个流行的生物医学信息检索（IR）基准测试中具有出色的检索性能，与稀疏基线相比，NDCG@10的提升高达22.1%，与最强的基线相比，提升高达6.5%。在查询扰动设置下，BMQExpander具有很强的泛化能力，与有监督的基线相比，实现了高达15.7%的改进。定性分析表明，与其他基于LLM的查询扩展基线相比，BMQExpander的幻觉更少。

Conclusion: BMQExpander在三个生物医学信息检索基准测试中表现出色，尤其是在查询扰动下具有鲁棒性，并且幻觉较少。

Abstract: Effective Question Answering (QA) on large biomedical document collections
requires effective document retrieval techniques. The latter remains a
challenging task due to the domain-specific vocabulary and semantic ambiguity
in user queries. We propose BMQExpander, a novel ontology-aware query expansion
pipeline that combines medical knowledge - definitions and relationships - from
the UMLS Metathesaurus with the generative capabilities of large language
models (LLMs) to enhance retrieval effectiveness. We implemented several
state-of-the-art baselines, including sparse and dense retrievers, query
expansion methods, and biomedical-specific solutions. We show that BMQExpander
has superior retrieval performance on three popular biomedical Information
Retrieval (IR) benchmarks: NFCorpus, TREC-COVID, and SciFact - with
improvements of up to 22.1% in NDCG@10 over sparse baselines and up to 6.5%
over the strongest baseline. Further, BMQExpander generalizes robustly under
query perturbation settings, in contrast to supervised baselines, achieving up
to 15.7% improvement over the strongest baseline. As a side contribution, we
publish our paraphrased benchmarks. Finally, our qualitative analysis shows
that BMQExpander has fewer hallucinations compared to other LLM-based query
expansion baselines.

</details>


### [141] [TBGRecall: A Generative Retrieval Model for E-commerce Recommendation Scenarios](https://arxiv.org/abs/2508.11977)
*Zida Liang,Changfa Wu,Dunxian Huang,Weiqiang Sun,Ziyang Wang,Yuliang Yan,Jian Wu,Yuning Jiang,Bo Zheng,Ke Chen,Silu Zhou,Yu Zhang*

Main category: cs.IR

TL;DR: TBGRecall, a Next Session Prediction framework, enhances generative retrieval models for e-commerce, outperforming existing methods with improved training efficiency and data recency.


<details>
  <summary>Details</summary>
Motivation: Generative models in recommendation systems have limitations in optimizing retrieval tasks due to their reliance on autoregressive generation mechanisms and sequential dependencies that impede efficient retrieval.

Method: TBGRecall: a framework integrating Next Session Prediction (NSP), designed to enhance generative retrieval models for e-commerce applications. The framework partitions input samples into multi-session sequences and incorporates optimizations tailored to the generative task in retrieval scenarios. Training integrates limited historical data pre-training with stochastic partial incremental training.

Result: TBGRecall outperforms state-of-the-art recommendation methods and exhibits a clear scaling law trend on public benchmarks and a large-scale industrial dataset from TaoBao.

Conclusion: NSP is a significant advancement in generative recommendation systems for e-commerce.

Abstract: Recommendation systems are essential tools in modern e-commerce, facilitating
personalized user experiences by suggesting relevant products. Recent
advancements in generative models have demonstrated potential in enhancing
recommendation systems; however, these models often exhibit limitations in
optimizing retrieval tasks, primarily due to their reliance on autoregressive
generation mechanisms. Conventional approaches introduce sequential
dependencies that impede efficient retrieval, as they are inherently unsuitable
for generating multiple items without positional constraints within a single
request session. To address these limitations, we propose TBGRecall, a
framework integrating Next Session Prediction (NSP), designed to enhance
generative retrieval models for e-commerce applications. Our framework
reformulation involves partitioning input samples into multi-session sequences,
where each sequence comprises a session token followed by a set of item tokens,
and then further incorporate multiple optimizations tailored to the generative
task in retrieval scenarios. In terms of training methodology, our pipeline
integrates limited historical data pre-training with stochastic partial
incremental training, significantly improving training efficiency and
emphasizing the superiority of data recency over sheer data volume. Our
extensive experiments, conducted on public benchmarks alongside a large-scale
industrial dataset from TaoBao, show TBGRecall outperforms the state-of-the-art
recommendation methods, and exhibits a clear scaling law trend. Ultimately, NSP
represents a significant advancement in the effectiveness of generative
recommendation systems for e-commerce applications.

</details>


### [142] [Leveraging Geometric Insights in Hyperbolic Triplet Loss for Improved Recommendations](https://arxiv.org/abs/2508.11978)
*Viacheslav Yusupov,Maxim Rakhuba,Evgeny Frolov*

Main category: cs.IR

TL;DR: 本文介绍了一种新的双曲推荐模型，该模型使用几何见解来改进表示学习并同时提高计算稳定性。


<details>
  <summary>Details</summary>
Motivation: 最近的研究表明，双曲几何在捕获推荐系统中交互数据的复杂模式方面具有潜力。

Method: 我们重新定义了双曲距离的概念，以释放超过传统欧几里得空间的额外表示能力，并学习更具表现力的用户和项目表示。为了更好地捕捉用户-项目交互，我们构建了一个三元组损失，该损失通过由数据几何驱动的成对交互项混合来模拟用户及其相应的首选和非首选选择之间的三元关系。

Result: 我们的双曲方法不仅优于现有的欧几里得和双曲模型，而且减少了受欢迎程度偏差，从而实现了更多样化和个性化的推荐。

Conclusion: 该双曲方法不仅优于现有的欧几里得和双曲模型，而且减少了受欢迎程度偏差，从而实现了更多样化和个性化的推荐。

Abstract: Recent studies have demonstrated the potential of hyperbolic geometry for
capturing complex patterns from interaction data in recommender systems. In
this work, we introduce a novel hyperbolic recommendation model that uses
geometrical insights to improve representation learning and increase
computational stability at the same time. We reformulate the notion of
hyperbolic distances to unlock additional representation capacity over
conventional Euclidean space and learn more expressive user and item
representations. To better capture user-items interactions, we construct a
triplet loss that models ternary relations between users and their
corresponding preferred and nonpreferred choices through a mix of pairwise
interaction terms driven by the geometry of data. Our hyperbolic approach not
only outperforms existing Euclidean and hyperbolic models but also reduces
popularity bias, leading to more diverse and personalized recommendations.

</details>


### [143] [A Large-Scale Web Search Dataset for Federated Online Learning to Rank](https://arxiv.org/abs/2508.12353)
*Marcel Gregoriadis,Jingwei Kang,Johan Pouwelse*

Main category: cs.IR

TL;DR: AOL4FOLTR: A large-scale web search dataset for Federated Online Learning to Rank (FOLTR).


<details>
  <summary>Details</summary>
Motivation: Existing FOLTR benchmarks oversimplifies real-world dynamics and undermines the realism of experimental results.

Method: AOL4FOLTR, a large-scale web search dataset with 2.6 million queries from 10,000 users

Result: The dataset addresses key limitations of existing benchmarks by including user identifiers, real click data, and query timestamps

Conclusion: AOL4FOLTR dataset enables realistic user partitioning, behavior modeling, and asynchronous federated learning scenarios.

Abstract: The centralized collection of search interaction logs for training ranking
models raises significant privacy concerns. Federated Online Learning to Rank
(FOLTR) offers a privacy-preserving alternative by enabling collaborative model
training without sharing raw user data. However, benchmarks in FOLTR are
largely based on random partitioning of classical learning-to-rank datasets,
simulated user clicks, and the assumption of synchronous client participation.
This oversimplifies real-world dynamics and undermines the realism of
experimental results. We present AOL4FOLTR, a large-scale web search dataset
with 2.6 million queries from 10,000 users. Our dataset addresses key
limitations of existing benchmarks by including user identifiers, real click
data, and query timestamps, enabling realistic user partitioning, behavior
modeling, and asynchronous federated learning scenarios.

</details>


### [144] [TaoSR1: The Thinking Model for E-commerce Relevance Search](https://arxiv.org/abs/2508.12365)
*Chenhe Dong,Shaowei Yao,Pengkun Jiao,Jianhui Yang,Yiming Jin,Zerui Huang,Xiaojiang Zhou,Dan Ou,Haihong Tang*

Main category: cs.IR

TL;DR: 提出了一种名为TaoSR1的框架，用于直接部署大型语言模型以进行query-product相关性预测，该框架通过结合CoT、DPO和GRPO等技术，在推理能力、生成质量和幻觉抑制方面都取得了显著提升，并在在线评估中表现出色。


<details>
  <summary>Details</summary>
Motivation: 基于BERT的模型擅长语义匹配，但缺乏复杂的推理能力。虽然已经探索了大型语言模型（LLM），但大多数仍然使用判别性微调或提炼到较小的模型以进行部署。

Method: TaoSR1框架，涉及三个阶段：（1）使用CoT进行监督微调（SFT）以灌输推理；（2）使用pass@N策略和直接偏好优化（DPO）进行离线采样，以提高生成质量；（3）使用基于难度的动态采样和组相对策略优化（GRPO）来减轻判别性幻觉。

Result: TaoSR1在离线数据集上显著优于基线，并在在线人工评估中取得了显著的收益。

Conclusion: TaoSR1显著优于离线数据集的基线，并在在线并排人工评估中取得了显著的收益，为将CoT推理应用于相关性分类引入了一种新的范例。

Abstract: Query-product relevance prediction is a core task in e-commerce search.
BERT-based models excel at semantic matching but lack complex reasoning
capabilities. While Large Language Models (LLMs) are explored, most still use
discriminative fine-tuning or distill to smaller models for deployment. We
propose a framework to directly deploy LLMs for this task, addressing key
challenges: Chain-of-Thought (CoT) error accumulation, discriminative
hallucination, and deployment feasibility. Our framework, TaoSR1, involves
three stages: (1) Supervised Fine-Tuning (SFT) with CoT to instill reasoning;
(2) Offline sampling with a pass@N strategy and Direct Preference Optimization
(DPO) to improve generation quality; and (3) Difficulty-based dynamic sampling
with Group Relative Policy Optimization (GRPO) to mitigate discriminative
hallucination. Additionally, post-CoT processing and a cumulative
probability-based partitioning method enable efficient online deployment.
TaoSR1 significantly outperforms baselines on offline datasets and achieves
substantial gains in online side-by-side human evaluations, introducing a novel
paradigm for applying CoT reasoning to relevance classification.

</details>


### [145] [Contrastive Multi-View Graph Hashing](https://arxiv.org/abs/2508.12377)
*Yang Xu,Zuliang Yang,Kai Ming Ting*

Main category: cs.IR

TL;DR: CMGHash：一种新的端到端框架，用于从多视图图数据中学习统一的判别性二值嵌入，并在检索精度方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多视图图数据在各个领域变得越来越普遍，它既捕获节点属性，又捕获来自不同来源的丰富关系信息。对此类数据进行有效检索是一项重要任务。然而，现有的多视图哈希技术通常假设每个视图都是基于属性的输入，因此不适合多视图图数据。

Method: 提出了一种新的端到端框架，称为对比多视图图哈希 (CMGHash)，旨在从多视图图数据中学习统一的判别性二值嵌入。

Result: CMGHash学习使用对比多视图图损失的共识节点表示空间，该损失旨在将来自所有图的$k$-最近邻拉得更近，同时推开负对，即非邻居节点。此外，我们在该共识空间上施加二值化约束，使其能够以最小的代价转换为相应的二值嵌入空间。

Conclusion: CMGHash在检索精度方面显著优于现有方法。

Abstract: Multi-view graph data, which both captures node attributes and rich
relational information from diverse sources, is becoming increasingly prevalent
in various domains. The effective and efficient retrieval of such data is an
important task. Although multi-view hashing techniques have offered a paradigm
for fusing diverse information into compact binary codes, they typically assume
attributes-based inputs per view. This makes them unsuitable for multi-view
graph data, where effectively encoding and fusing complex topological
information from multiple heterogeneous graph views to generate unified binary
embeddings remains a significant challenge. In this work, we propose
Contrastive Multi-view Graph Hashing (CMGHash), a novel end-to-end framework
designed to learn unified and discriminative binary embeddings from multi-view
graph data. CMGHash learns a consensus node representation space using a
contrastive multi-view graph loss, which aims to pull $k$-nearest neighbors
from all graphs closer while pushing away negative pairs, i.e., non-neighbor
nodes. Moreover, we impose binarization constraints on this consensus space,
enabling its conversion to a corresponding binary embedding space at minimal
cost. Extensive experiments on several benchmark datasets demonstrate that
CMGHash significantly outperforms existing approaches in terms of retrieval
accuracy.

</details>


### [146] [Diagnostic-Guided Dynamic Profile Optimization for LLM-based User Simulators in Sequential Recommendation](https://arxiv.org/abs/2508.12645)
*Hongyang Liu,Zhu Sun,Tianjun Wei,Yan Wang,Jiajie Zhu,Xinghua Qu*

Main category: cs.IR

TL;DR: 这篇论文提出了一种新的基于LLM的框架DGDPO，用于构建更准确和完整的用户画像，并通过多轮交互提高推荐系统仿真的真实性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于LLM的推荐系统模拟器面临两个主要限制：(1) 静态和单步的基于提示的推理，导致不准确和不完整的用户画像构建；(2) 不切实际的单轮推荐-反馈交互模式，无法捕捉真实世界的场景。

Method: DGDPO框架包含两个核心模块：一个专门的基于LLM的诊断模块，用于准确识别用户画像中的特定缺陷；以及一个通用的基于LLM的治疗模块，用于分析诊断出的缺陷并生成有针对性的建议以完善画像。

Result: DGDPO框架在三个真实世界数据集上进行了广泛的实验，结果表明其有效性。该框架还首次将DGDPO与顺序推荐器集成，实现了用户画像和推荐策略在多轮交互中的双向演化。

Conclusion: 这篇论文提出了一种名为DGDPO的新框架，通过动态迭代优化过程构建用户画像，以提高仿真保真度。在三个真实世界数据集上进行的大量实验证明了该框架的有效性。

Abstract: Recent advances in large language models (LLMs) have enabled realistic user
simulators for developing and evaluating recommender systems (RSs). However,
existing LLM-based simulators for RSs face two major limitations: (1) static
and single-step prompt-based inference that leads to inaccurate and incomplete
user profile construction; (2) unrealistic and single-round
recommendation-feedback interaction pattern that fails to capture real-world
scenarios. To address these limitations, we propose DGDPO (Diagnostic-Guided
Dynamic Profile Optimization), a novel framework that constructs user profile
through a dynamic and iterative optimization process to enhance the simulation
fidelity. Specifically, DGDPO incorporates two core modules within each
optimization loop: firstly, a specialized LLM-based diagnostic module,
calibrated through our novel training strategy, accurately identifies specific
defects in the user profile. Subsequently, a generalized LLM-based treatment
module analyzes the diagnosed defect and generates targeted suggestions to
refine the profile. Furthermore, unlike existing LLM-based user simulators that
are limited to single-round interactions, we are the first to integrate DGDPO
with sequential recommenders, enabling a bidirectional evolution where user
profiles and recommendation strategies adapt to each other over multi-round
interactions. Extensive experiments conducted on three real-world datasets
demonstrate the effectiveness of our proposed framework.

</details>


### [147] [Multi-Granularity Distribution Modeling for Video Watch Time Prediction via Exponential-Gaussian Mixture Network](https://arxiv.org/abs/2508.12665)
*Xu Zhao,Ruibo Ma,Jiaqi Chen,Weiqi Zhao,Ping Yang,Yao Hu*

Main category: cs.IR

TL;DR: 提出了一种用于观看时长预测的指数-高斯混合网络 (EGMN)，并在小红书App上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 精确的观看时长预测对于增强流媒体短视频平台中的用户参与度至关重要，尽管它受到跨多粒度级别的复杂分布特征的挑战。通过对真实工业数据的系统分析，我们从分布的角度揭示了观看时长预测中的两个关键挑战：(1) 由快速跳过的大量集中引起的粗粒度偏度，(2) 由各种用户-视频互动模式引起的细粒度多样性。

Method: 提出了一种指数-高斯混合网络 (EGMN)，用于 EGM 分布的参数化，该网络由两个关键模块组成：隐藏表示编码器和混合参数生成器。

Result: 在公共数据集上进行了广泛的离线实验，并在小红书App的工业短视频feed场景中进行了在线A/B测试，以验证EGMN相对于现有最先进方法的优越性。全面的实验结果证明，EGMN 具有出色的跨粗到细粒度级别的分布拟合能力。

Conclusion: EGMN在粗粒度和细粒度级别上都表现出出色的分布拟合能力。

Abstract: Accurate watch time prediction is crucial for enhancing user engagement in
streaming short-video platforms, although it is challenged by complex
distribution characteristics across multi-granularity levels. Through
systematic analysis of real-world industrial data, we uncover two critical
challenges in watch time prediction from a distribution aspect: (1)
coarse-grained skewness induced by a significant concentration of quick-skips1,
(2) fine-grained diversity arising from various user-video interaction
patterns. Consequently, we assume that the watch time follows the
Exponential-Gaussian Mixture (EGM) distribution, where the exponential and
Gaussian components respectively characterize the skewness and diversity.
Accordingly, an Exponential-Gaussian Mixture Network (EGMN) is proposed for the
parameterization of EGM distribution, which consists of two key modules: a
hidden representation encoder and a mixture parameter generator. We conducted
extensive offline experiments on public datasets and online A/B tests on the
industrial short-video feeding scenario of Xiaohongshu App to validate the
superiority of EGMN compared with existing state-of-the-art methods.
Remarkably, comprehensive experimental results have proven that EGMN exhibits
excellent distribution fitting ability across coarse-to-fine-grained levels. We
open source related code on Github: https://github.com/BestActionNow/EGMN.

</details>


### [148] [Asymmetric Diffusion Recommendation Model](https://arxiv.org/abs/2508.12706)
*Yongchun Zhu,Guanyu Jiang,Jingwu Chen,Feng Zhang,Xiao Yang,Zuotao Liu*

Main category: cs.IR

TL;DR: AsymDiffRec improves recommendation models using asymmetric diffusion in discrete data space, showing gains in user activity and app usage in Douyin Music App.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based recommendation models use standard Gaussian noise in continuous data space, which is not suitable for the discrete data space of recommendation systems and can corrupt personalized information.

Method: The paper proposes an Asymmetric Diffusion Recommendation Model (AsymDiffRec) with a generalized forward process simulating missing features and a reverse process in an asymmetric latent feature space. A task-oriented optimization strategy is used to preserve personalized information.

Result: Online A/B tests show improvements of +0.131% in users' active days and +0.166% in app usage duration. Offline experiments also demonstrate improvements.

Conclusion: The paper introduces AsymDiffRec, which improves recommendation models by using asymmetric diffusion processes in discrete data space. Online A/B tests show improvements in user activity and app usage duration. AsymDiffRec has been implemented in the Douyin Music App.

Abstract: Recently, motivated by the outstanding achievements of diffusion models, the
diffusion process has been employed to strengthen representation learning in
recommendation systems. Most diffusion-based recommendation models typically
utilize standard Gaussian noise in symmetric forward and reverse processes in
continuous data space. Nevertheless, the samples derived from recommendation
systems inhabit a discrete data space, which is fundamentally different from
the continuous one. Moreover, Gaussian noise has the potential to corrupt
personalized information within latent representations. In this work, we
propose a novel and effective method, named Asymmetric Diffusion Recommendation
Model (AsymDiffRec), which learns forward and reverse processes in an
asymmetric manner. We define a generalized forward process that simulates the
missing features in real-world recommendation samples. The reverse process is
then performed in an asymmetric latent feature space. To preserve personalized
information within the latent representation, a task-oriented optimization
strategy is introduced. In the serving stage, the raw sample with missing
features is regarded as a noisy input to generate a denoising and robust
representation for the final prediction. By equipping base models with
AsymDiffRec, we conduct online A/B tests, achieving improvements of +0.131% and
+0.166% in terms of users' active days and app usage duration respectively.
Additionally, the extended offline experiments also demonstrate improvements.
AsymDiffRec has been implemented in the Douyin Music App.

</details>


### [149] [Deep Research: A Survey of Autonomous Research Agents](https://arxiv.org/abs/2508.12752)
*Wenlin Zhang,Xiaopeng Li,Yingyi Zhang,Pengyue Jia,Yichao Wang,Huifeng Guo,Yong Liu,Xiangyu Zhao*

Main category: cs.IR

TL;DR: 本文概述了深度研究流程，分析了关键技术挑战，总结了优化技术和基准，并讨论了开放的挑战和有希望的研究方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的快速发展推动了能够自主执行复杂任务的代理系统的发展。尽管它们具有令人印象深刻的能力，但LLM仍然受到其内部知识边界的限制。为了克服这些限制，人们提出了深度研究的范例，即代理主动参与计划、检索和综合，以生成基于网络证据的全面而忠实的分析报告。

Method: 对深度研究流程进行系统概述，包括四个核心阶段：计划、问题开发、网络探索和报告生成。对于每个阶段，分析了关键的技术挑战，并对为解决这些挑战而开发的代表性方法进行了分类。

Result: 为深度研究定制的优化技术和基准的最新进展

Conclusion: 总结了深度研究的优化技术和基准，并讨论了开放的挑战和有希望的研究方向，旨在为构建更强大和值得信赖的深度研究代理提供路线图。

Abstract: The rapid advancement of large language models (LLMs) has driven the
development of agentic systems capable of autonomously performing complex
tasks. Despite their impressive capabilities, LLMs remain constrained by their
internal knowledge boundaries. To overcome these limitations, the paradigm of
deep research has been proposed, wherein agents actively engage in planning,
retrieval, and synthesis to generate comprehensive and faithful analytical
reports grounded in web-based evidence. In this survey, we provide a systematic
overview of the deep research pipeline, which comprises four core stages:
planning, question developing, web exploration, and report generation. For each
stage, we analyze the key technical challenges and categorize representative
methods developed to address them. Furthermore, we summarize recent advances in
optimization techniques and benchmarks tailored for deep research. Finally, we
discuss open challenges and promising research directions, aiming to chart a
roadmap toward building more capable and trustworthy deep research agents.

</details>


### [150] [Informfully Recommenders -- Reproducibility Framework for Diversity-aware Intra-session Recommendations](https://arxiv.org/abs/2508.13019)
*Lucien Heitz,Runze Li,Oana Inel,Abraham Bernstein*

Main category: cs.IR

TL;DR: Introduces Informfully Recommenders, a reproducibility framework for diversity-aware recommender systems.


<details>
  <summary>Details</summary>
Motivation: There is a lack of reproducibility frameworks for thorough norm-driven experimentation in recommender systems, especially for diversity optimization.

Method: The framework extends Cornac and provides an end-to-end solution covering dataset pre-processing, diversity-optimized models, intra-session item re-ranking, and diversity metrics.

Result: The capabilities of the extension are demonstrated through an extensive offline experiment in the news domain.

Conclusion: This paper presents Informfully Recommenders, a normative reproducibility framework for diversity-aware recommender systems.

Abstract: Norm-aware recommender systems have gained increased attention, especially
for diversity optimization. The recommender systems community has
well-established experimentation pipelines that support reproducible
evaluations by facilitating models' benchmarking and comparisons against
state-of-the-art methods. However, to the best of our knowledge, there is
currently no reproducibility framework to support thorough norm-driven
experimentation at the pre-processing, in-processing, post-processing, and
evaluation stages of the recommender pipeline. To address this gap, we present
Informfully Recommenders, a first step towards a normative reproducibility
framework that focuses on diversity-aware design built on Cornac. Our extension
provides an end-to-end solution for implementing and experimenting with
normative and general-purpose diverse recommender systems that cover 1) dataset
pre-processing, 2) diversity-optimized models, 3) dedicated intrasession item
re-ranking, and 4) an extensive set of diversity metrics. We demonstrate the
capabilities of our extension through an extensive offline experiment in the
news domain.

</details>


### [151] [D-RDW: Diversity-Driven Random Walks for News Recommender Systems](https://arxiv.org/abs/2508.13035)
*Runze Li,Lucien Heitz,Oana Inel,Abraham Bernstein*

Main category: cs.IR

TL;DR: D-RDW是一种社会推荐器，它结合了传统随机漫步算法的多样化能力与可定制的新闻文章属性目标分布。


<details>
  <summary>Details</summary>
Motivation: 生成多样化的新闻推荐

Method: D-RDW，一种轻量级算法和重排序技术

Result: D-RDW在考虑文章的情绪和政党提及的关键多样性指标上显示出增强的性能

Conclusion: D-RDW在计算效率上优于现有方法。

Abstract: This paper introduces Diversity-Driven RandomWalks (D-RDW), a lightweight
algorithm and re-ranking technique that generates diverse news recommendations.
D-RDW is a societal recommender, which combines the diversification
capabilities of the traditional random walk algorithms with customizable target
distributions of news article properties. In doing so, our model provides a
transparent approach for editors to incorporate norms and values into the
recommendation process. D-RDW shows enhanced performance across key diversity
metrics that consider the articles' sentiment and political party mentions when
compared to state-of-the-art neural models. Furthermore, D-RDW proves to be
more computationally efficient than existing approaches.

</details>


### [152] [Is This News Still Interesting to You?: Lifetime-aware Interest Matching for News Recommendation](https://arxiv.org/abs/2508.13064)
*Seongeun Ryu,Yunyong Ko,Sang-Wook Kim*

Main category: cs.IR

TL;DR: 提出LIME框架，通过考虑新闻的生命周期和用户兴趣的持久性来改进个性化新闻推荐。


<details>
  <summary>Details</summary>
Motivation: 虽然先前的工作已经通过改进新闻和用户的表示来改进兴趣匹配，但以下与时间相关的挑战仍未得到充分探索：（C1）利用点击新闻的年龄来推断用户的兴趣持久性，以及（C2）建模不同主题和用户的新闻的不同生命周期。

Method: 提出了一种新颖的Lifetime-aware兴趣匹配框架LIME，用于新闻推荐，该框架结合了三个关键策略：（1）用户-主题lifetime-aware年龄表示，以捕获相对于用户-主题对的新闻相对年龄，（2）候选感知lifetime注意力，用于生成时间对齐的用户表示，以及（3）新鲜度引导的兴趣细化，用于在预测时优先考虑有效的候选新闻。

Result: LIME在两个真实世界的数据集上始终优于各种最先进的新闻推荐方法，并且其模型不可知策略显着提高了推荐准确性。

Conclusion: LIME在两个真实世界的数据集上始终优于各种最先进的新闻推荐方法，并且其模型不可知策略显着提高了推荐准确性。

Abstract: Personalized news recommendation aims to deliver news articles aligned with
users' interests, serving as a key solution to alleviate the problem of
information overload on online news platforms. While prior work has improved
interest matching through refined representations of news and users, the
following time-related challenges remain underexplored: (C1) leveraging the age
of clicked news to infer users' interest persistence, and (C2) modeling the
varying lifetime of news across topics and users. To jointly address these
challenges, we propose a novel Lifetime-aware Interest Matching framework for
nEws recommendation, named LIME, which incorporates three key strategies: (1)
User-Topic lifetime-aware age representation to capture the relative age of
news with respect to a user-topic pair, (2) Candidate-aware lifetime attention
for generating temporally aligned user representation, and (3) Freshness-guided
interest refinement for prioritizing valid candidate news at prediction time.
Extensive experiments on two real-world datasets demonstrate that LIME
consistently outperforms a wide range of state-of-the-art news recommendation
methods, and its model agnostic strategies significantly improve recommendation
accuracy.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [153] [Sparse Attention across Multiple-context KV Cache](https://arxiv.org/abs/2508.11661)
*Ziyi Cao,Qingyi Si,Jingbin Zhang,Bingquan Liu*

Main category: cs.LG

TL;DR: SamKV通过注意力稀疏化方法，有效压缩多上下文RAG场景中的序列长度，提高推理效率，同时保证准确率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长序列推理中面临显著的成本挑战。现有方法在RAG场景中对多上下文KV Cache进行注意力稀疏化效果不佳，且先前工作虽部分重新计算多上下文KV Cache以减轻精度损失，但需要保留所有KV Cache，无法减少内存开销。

Method: 提出SamKV，一种针对多上下文KV Cache的注意力稀疏化方法。该方法在稀疏化一个上下文时，考虑其他上下文的互补信息，并局部地重新计算稀疏化的信息。

Result: 实验表明，与完全重新计算的基线相比，该方法在不降低准确率的情况下将序列长度压缩到15%，从而显著提高了多上下文RAG场景中的吞吐量。

Conclusion: SamKV在多上下文RAG场景中，将序列长度压缩到15%且不降低准确率，显著提高了吞吐量。

Abstract: Large language models face significant cost challenges in long-sequence
inference. To address this, reusing historical Key-Value (KV) Cache for
improved inference efficiency has become a mainstream approach. Recent advances
further enhance throughput by sparse attention mechanisms to select the most
relevant KV Cache, thereby reducing sequence length. However, such techniques
are limited to single-context scenarios, where historical KV Cache is computed
sequentially with causal-attention dependencies. In retrieval-augmented
generation (RAG) scenarios, where retrieved documents as context are unknown
beforehand, each document's KV Cache is computed and stored independently
(termed multiple-context KV Cache), lacking cross-attention between contexts.
This renders existing methods ineffective. Although prior work partially
recomputes multiple-context KV Cache to mitigate accuracy loss from missing
cross-attention, it requires retaining all KV Cache throughout, failing to
reduce memory overhead. This paper presents SamKV, the first exploration of
attention sparsification for multiple-context KV Cache. Specifically, SamKV
takes into account the complementary information of other contexts when
sparsifying one context, and then locally recomputes the sparsified
information. Experiments demonstrate that our method compresses sequence length
to 15% without accuracy degradation compared with full-recompuation baselines,
significantly boosting throughput in multi-context RAG scenarios.

</details>


### [154] [Assessing Representation Stability for Transformer Models](https://arxiv.org/abs/2508.11667)
*Bryan E. Tuck,Rakesh M. Verma*

Main category: cs.LG

TL;DR: This paper introduces Representation Stability (RS), a model-agnostic detection framework that identifies adversarial examples by measuring how embedding representations change when important words are masked.


<details>
  <summary>Details</summary>
Motivation: Adversarial text attacks remain a persistent threat to transformer models, yet existing defenses are typically attack-specific or require costly model retraining.

Method: RS first ranks words using importance heuristics, then measures embedding sensitivity to masking top-k critical words, and processes the resulting patterns with a BiLSTM detector.

Result: RS achieves over 88% detection accuracy and demonstrates competitive performance compared to existing state-of-the-art methods, often at lower computational cost. Using Normalized Discounted Cumulative Gain (NDCG) to measure perturbation identification quality, we reveal that gradient-based ranking outperforms attention and random selection approaches, with identification quality correlating with detection performance for word-level attacks.

Conclusion: RS also generalizes well to unseen datasets, attacks, and models without retraining, providing a practical solution for adversarial text detection.

Abstract: Adversarial text attacks remain a persistent threat to transformer models,
yet existing defenses are typically attack-specific or require costly model
retraining. We introduce Representation Stability (RS), a model-agnostic
detection framework that identifies adversarial examples by measuring how
embedding representations change when important words are masked. RS first
ranks words using importance heuristics, then measures embedding sensitivity to
masking top-k critical words, and processes the resulting patterns with a
BiLSTM detector. Experiments show that adversarially perturbed words exhibit
disproportionately high masking sensitivity compared to naturally important
words. Across three datasets, three attack types, and two victim models, RS
achieves over 88% detection accuracy and demonstrates competitive performance
compared to existing state-of-the-art methods, often at lower computational
cost. Using Normalized Discounted Cumulative Gain (NDCG) to measure
perturbation identification quality, we reveal that gradient-based ranking
outperforms attention and random selection approaches, with identification
quality correlating with detection performance for word-level attacks. RS also
generalizes well to unseen datasets, attacks, and models without retraining,
providing a practical solution for adversarial text detection.

</details>


### [155] [Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for NGINX](https://arxiv.org/abs/2508.12485)
*Aayush Gupta,Arpit Bhayani*

Main category: cs.LG

TL;DR: Cold-RL, a reinforcement learning-based eviction policy for NGINX, outperforms traditional methods in hit ratio, especially for smaller caches, with low overhead.


<details>
  <summary>Details</summary>
Motivation: Web proxies like NGINX commonly use LRU eviction, which is size agnostic and can perform poorly under periodic bursts and mixed object sizes. This motivates the development of a more adaptive eviction policy.

Method: The method involves replacing LRU's forced-expire path with a dueling Deep Q-Network served by an ONNX sidecar. Policies are trained offline using NGINX access logs and a cache simulator with a reward system based on object retention.

Result: Cold-RL significantly improves hit ratio compared to LRU, LFU, size-based, adaptive LRU, and a hybrid baseline, particularly with smaller caches. It achieves a 146% improvement at 25 MB cache, a 15% gain at 100 MB, and matches classical methods at 400 MB, with minimal CPU overhead and eviction latency.

Conclusion: This paper introduces Cold-RL, a reinforcement learning eviction policy integrated into NGINX with strict SLOs. It demonstrates significant hit ratio improvements over classical baselines on adversarial workloads, especially with smaller cache sizes, while maintaining low CPU overhead and eviction latency.

Abstract: Web proxies such as NGINX commonly rely on least-recently-used (LRU)
eviction, which is size agnostic and can thrash under periodic bursts and mixed
object sizes. We introduce Cold-RL, a learned eviction policy for NGINX that
replaces LRU's forced-expire path with a dueling Deep Q-Network served by an
ONNX sidecar within a strict microsecond budget. On each eviction, Cold-RL
samples the K least-recently-used objects, extracts six lightweight features
(age, size, hit count, inter-arrival time, remaining TTL, and last origin RTT),
and requests a bitmask of victims; a hard timeout of 500 microseconds triggers
immediate fallback to native LRU. Policies are trained offline by replaying
NGINX access logs through a cache simulator with a simple reward: a retained
object earns one point if it is hit again before TTL expiry. We compare against
LRU, LFU, size-based, adaptive LRU, and a hybrid baseline on two adversarial
workloads. With a 25 MB cache, Cold-RL raises hit ratio from 0.1436 to 0.3538,
a 146 percent improvement over the best classical baseline; at 100 MB, from
0.7530 to 0.8675, a 15 percent gain; and at 400 MB it matches classical methods
(about 0.918). Inference adds less than 2 percent CPU overhead and keeps 95th
percentile eviction latency within budget. To our knowledge, this is the first
reinforcement learning eviction policy integrated into NGINX with strict SLOs.

</details>


### [156] [Collaborative Learning-Enhanced Lightweight Models for Predicting Arterial Blood Pressure Waveform in a Large-scale Perioperative Dataset](https://arxiv.org/abs/2508.11669)
*Wentao Li,Yonghu He,Kun Gao,Qing Liu,Yali Zheng*

Main category: cs.LG

TL;DR: Developed a lightweight deep learning model (KDCL_sInvResUNet) for real-time, noninvasive arterial blood pressure (ABP) monitoring on embedded systems, achieving good performance with low computational load.


<details>
  <summary>Details</summary>
Motivation: Limited research has addressed the issue of model performance and computational load for deployment on embedded systems.

Method: The study introduces a lightweight sInvResUNet, along with a collaborative learning scheme named KDCL_sInvResUNet.

Result: The proposed KDCL_sInvResUNet achieved lightly better performance compared to large models, with a mean absolute error of 10.06 mmHg and mean Pearson correlation of 0.88 in tracking ABP changes. With only 0.89 million parameters and a computational load of 0.02 GFLOPS, real-time ABP estimation was successfully achieved on embedded devices with an inference time of just 8.49 milliseconds for a 10-second output.

Conclusion: This study lays a foundation work for real-time, unobtrusive ABP monitoring in real-world perioperative settings, providing baseline for future advancements in this area.

Abstract: Noninvasive arterial blood pressure (ABP) monitoring is essential for patient
management in critical care and perioperative settings, providing continuous
assessment of cardiovascular hemodynamics with minimal risks. Numerous deep
learning models have developed to reconstruct ABP waveform from noninvasively
acquired physiological signals such as electrocardiogram and
photoplethysmogram. However, limited research has addressed the issue of model
performance and computational load for deployment on embedded systems. The
study introduces a lightweight sInvResUNet, along with a collaborative learning
scheme named KDCL_sInvResUNet. With only 0.89 million parameters and a
computational load of 0.02 GFLOPS, real-time ABP estimation was successfully
achieved on embedded devices with an inference time of just 8.49 milliseconds
for a 10-second output. We performed subject-independent validation in a
large-scale and heterogeneous perioperative dataset containing 1,257,141 data
segments from 2,154 patients, with a wide BP range (41-257 mmHg for SBP, and
31-234 mmHg for DBP). The proposed KDCL_sInvResUNet achieved lightly better
performance compared to large models, with a mean absolute error of 10.06 mmHg
and mean Pearson correlation of 0.88 in tracking ABP changes. Despite these
promising results, all deep learning models showed significant performance
variations across different demographic and cardiovascular conditions,
highlighting their limited ability to generalize across such a broad and
diverse population. This study lays a foundation work for real-time,
unobtrusive ABP monitoring in real-world perioperative settings, providing
baseline for future advancements in this area.

</details>


### [157] [Contrastive Regularization over LoRA for Multimodal Biomedical Image Incremental Learning](https://arxiv.org/abs/2508.11673)
*Haojie Zhang,Yixiong Liang,Hulin Kuang,Lihui Cen,Zhe Qu,Yigang Cen,Min Zeng,Shichao Kan*

Main category: cs.LG

TL;DR: MSLoRA-CR outperforms existing methods in multimodal biomedical image incremental learning by fine-tuning Modality-Specific LoRA modules with Contrastive Regularization.


<details>
  <summary>Details</summary>
Motivation: Existing incremental learning methods focus on task expansion within a single modality, whereas MBIIL seeks to train a unified model incrementally across modalities. The MBIIL faces two challenges: I) How to preserve previously learned knowledge during incremental updates? II) How to effectively leverage knowledge acquired from existing modalities to support new modalities?

Method: fine-tunes Modality-Specific LoRA modules while incorporating Contrastive Regularization to enhance intra-modality knowledge sharing and promote inter-modality knowledge differentiation. Our approach builds upon a large vision-language model (LVLM), keeping the pretrained model frozen while incrementally adapting new LoRA modules for each modality or task.

Result: Experiments on the incremental learning of biomedical images demonstrate that MSLoRA-CR outperforms both the state-of-the-art (SOTA) approach of training separate models for each modality and the general incremental learning method (incrementally fine-tuning LoRA).

Conclusion: MSLoRA-CR achieves a 1.88% improvement in overall performance compared to unconstrained incremental learning methods while maintaining computational efficiency.

Abstract: Multimodal Biomedical Image Incremental Learning (MBIIL) is essential for
handling diverse tasks and modalities in the biomedical domain, as training
separate models for each modality or task significantly increases inference
costs. Existing incremental learning methods focus on task expansion within a
single modality, whereas MBIIL seeks to train a unified model incrementally
across modalities. The MBIIL faces two challenges: I) How to preserve
previously learned knowledge during incremental updates? II) How to effectively
leverage knowledge acquired from existing modalities to support new modalities?
To address these challenges, we propose MSLoRA-CR, a method that fine-tunes
Modality-Specific LoRA modules while incorporating Contrastive Regularization
to enhance intra-modality knowledge sharing and promote inter-modality
knowledge differentiation. Our approach builds upon a large vision-language
model (LVLM), keeping the pretrained model frozen while incrementally adapting
new LoRA modules for each modality or task. Experiments on the incremental
learning of biomedical images demonstrate that MSLoRA-CR outperforms both the
state-of-the-art (SOTA) approach of training separate models for each modality
and the general incremental learning method (incrementally fine-tuning LoRA).
Specifically, MSLoRA-CR achieves a 1.88% improvement in overall performance
compared to unconstrained incremental learning methods while maintaining
computational efficiency. Our code is publicly available at
https://github.com/VentusAislant/MSLoRA_CR.

</details>


### [158] [Lifelong Learner: Discovering Versatile Neural Solvers for Vehicle Routing Problems](https://arxiv.org/abs/2508.11679)
*Shaodi Feng,Zhuoyi Lin,Jianan Zhou,Cong Zhang,Jingwen Li,Kuan-Wen Chen,Senthilnath Jayavelu,Yew-Soon Ong*

Main category: cs.LG

TL;DR: This paper introduces a lifelong learning framework to train a neural solver for vehicle routing problems (VRPs) in diverse contexts. It outperforms other neural solvers and achieves the best performance on synthetic and benchmark instances with problem sizes up to 18k.


<details>
  <summary>Details</summary>
Motivation: Most neural solvers are trained to tackle VRP instances in a relatively monotonous context, e.g., simplifying VRPs by using Euclidean distance between nodes and adhering to a single problem size, which harms their off-the-shelf application in different scenarios. To enhance their versatility, this paper presents a novel lifelong learning framework that incrementally trains a neural solver to manage VRPs in distinct contexts.

Method: A lifelong learning framework that incrementally trains a neural solver to manage VRPs in distinct contexts. A lifelong learner (LL), exploiting a Transformer network as the backbone, to solve a series of VRPs. The inter-context self-attention mechanism is proposed within LL to transfer the knowledge obtained from solving preceding VRPs into the succeeding ones. A dynamic context scheduler (DCS), employing the cross-context experience replay to further facilitate LL looking back on the attained policies of solving preceding VRPs.

Result: The proposed approach achieves the best performance for most VRPs.

Conclusion: The lifelong learner (LL) is capable of discovering effective policies for tackling generic VRPs in varying contexts, which outperforms other neural solvers and achieves the best performance for most VRPs.

Abstract: Deep learning has been extensively explored to solve vehicle routing problems
(VRPs), which yields a range of data-driven neural solvers with promising
outcomes. However, most neural solvers are trained to tackle VRP instances in a
relatively monotonous context, e.g., simplifying VRPs by using Euclidean
distance between nodes and adhering to a single problem size, which harms their
off-the-shelf application in different scenarios. To enhance their versatility,
this paper presents a novel lifelong learning framework that incrementally
trains a neural solver to manage VRPs in distinct contexts. Specifically, we
propose a lifelong learner (LL), exploiting a Transformer network as the
backbone, to solve a series of VRPs. The inter-context self-attention mechanism
is proposed within LL to transfer the knowledge obtained from solving preceding
VRPs into the succeeding ones. On top of that, we develop a dynamic context
scheduler (DCS), employing the cross-context experience replay to further
facilitate LL looking back on the attained policies of solving preceding VRPs.
Extensive results on synthetic and benchmark instances (problem sizes up to
18k) show that our LL is capable of discovering effective policies for tackling
generic VRPs in varying contexts, which outperforms other neural solvers and
achieves the best performance for most VRPs.

</details>


### [159] [Comparative Analysis of Time Series Foundation Models for Demographic Forecasting: Enhancing Predictive Accuracy in US Population Dynamics](https://arxiv.org/abs/2508.11680)
*Aditya Akella,Jonathan Farah*

Main category: cs.LG

TL;DR: 本研究使用TimesFM预测人口变化，结果表明TimesFM优于传统模型，尤其是在少数族裔人口方面。


<details>
  <summary>Details</summary>
Motivation: 人口结构变化对决策者和研究人员构成了重大挑战，准确的人口预测对于城市规划、医疗保健和经济政策等领域的知情决策至关重要。

Method: 应用时间序列基础模型来预测美国的人口变化，使用美国人口普查局和联邦储备经济数据(FRED)的数据集，并与LSTM网络、ARIMA和线性回归等传统基线模型进行比较。

Result: 在六个不同人口结构的州进行的实验表明，TimesFM在86.67%的测试案例中实现了最低的均方误差(MSE)，尤其是在历史数据稀疏的少数族裔人口方面表现出色。

Conclusion: TimesFM在预测人口变化方面优于传统模型，尤其是在少数族裔人口方面表现出色，无需大量特定任务微调即可增强人口分析并为积极的政策干预提供信息。

Abstract: Demographic shifts, influenced by globalization, economic conditions,
geopolitical events, and environmental factors, pose significant challenges for
policymakers and researchers. Accurate demographic forecasting is essential for
informed decision-making in areas such as urban planning, healthcare, and
economic policy. This study explores the application of time series foundation
models to predict demographic changes in the United States using datasets from
the U.S. Census Bureau and Federal Reserve Economic Data (FRED). We evaluate
the performance of the Time Series Foundation Model (TimesFM) against
traditional baselines including Long Short-Term Memory (LSTM) networks,
Autoregressive Integrated Moving Average (ARIMA), and Linear Regression. Our
experiments across six demographically diverse states demonstrate that TimesFM
achieves the lowest Mean Squared Error (MSE) in 86.67% of test cases, with
particularly strong performance on minority populations with sparse historical
data. These findings highlight the potential of pre-trained foundation models
to enhance demographic analysis and inform proactive policy interventions
without requiring extensive task-specific fine-tuning.

</details>


### [160] [From Heuristics to Data: Quantifying Site Planning Layout Indicators with Deep Learning and Multi-Modal Data](https://arxiv.org/abs/2508.11723)
*Qian Cao,Jielin Chen,Junchao Zhao,Rudi Stouffs*

Main category: cs.LG

TL;DR: The paper proposes a Site Planning Layout Indicator (SPLI) system, a data-driven framework for urban spatial analytics, integrating multi-source data and deep learning to improve functional classification accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional site planning often relies on experiential judgment and single-source data, limiting systematic quantification of multifunctional layouts.

Method: A data-driven framework integrating empirical knowledge with heterogeneous multi-source data to produce structured urban spatial information. It extends conventional metrics through five dimensions: (1) Hierarchical Building Function Classification (2) Spatial Organization (3) Functional Diversity (4) Accessibility to Essential Services (5) Land Use Intensity. Data gaps are addressed through deep learning, including Relational Graph Neural Networks (RGNN) and Graph Neural Networks (GNN).

Result: The SPLI supports multimodal spatial data systems for analytics, inference, and retrieval by combining OpenStreetMap (OSM), Points of Interest (POI), building morphology, land use, and satellite imagery.

Conclusion: The SPLI improves functional classification accuracy and provides a standardized basis for automated, data-driven urban spatial analytics.

Abstract: The spatial layout of urban sites shapes land-use efficiency and spatial
organization. Traditional site planning often relies on experiential judgment
and single-source data, limiting systematic quantification of multifunctional
layouts. We propose a Site Planning Layout Indicator (SPLI) system, a
data-driven framework integrating empirical knowledge with heterogeneous
multi-source data to produce structured urban spatial information. The SPLI
supports multimodal spatial data systems for analytics, inference, and
retrieval by combining OpenStreetMap (OSM), Points of Interest (POI), building
morphology, land use, and satellite imagery. It extends conventional metrics
through five dimensions: (1) Hierarchical Building Function Classification,
refining empirical systems into clear hierarchies; (2) Spatial Organization,
quantifying seven layout patterns (e.g., symmetrical, concentric,
axial-oriented); (3) Functional Diversity, transforming qualitative assessments
into measurable indicators using Functional Ratio (FR) and Simpson Index (SI);
(4) Accessibility to Essential Services, integrating facility distribution and
transport networks for comprehensive accessibility metrics; and (5) Land Use
Intensity, using Floor Area Ratio (FAR) and Building Coverage Ratio (BCR) to
assess utilization efficiency. Data gaps are addressed through deep learning,
including Relational Graph Neural Networks (RGNN) and Graph Neural Networks
(GNN). Experiments show the SPLI improves functional classification accuracy
and provides a standardized basis for automated, data-driven urban spatial
analytics.

</details>


### [161] [Causal Structure Learning in Hawkes Processes with Complex Latent Confounder Networks](https://arxiv.org/abs/2508.11727)
*Songyao Jin,Biwei Huang*

Main category: cs.LG

TL;DR: 该论文提出了一种用于识别具有潜在子过程的复杂系统中因果结构的算法。


<details>
  <summary>Details</summary>
Motivation: 现实世界的系统通常只是部分被观察到，潜在的子过程带来了巨大的挑战。

Method: 我们提出了一个两阶段迭代算法，该算法在推断已发现的子过程之间的因果关系和发现新的潜在子过程之间交替进行，并由基于路径的条件来保证可识别性。

Result: 我们表明，随着时间间隔的缩小，连续时间事件序列可以表示为离散时间模型，并且我们利用这一见解来建立识别潜在子过程和因果影响的必要和充分条件。

Conclusion: 该方法有效地恢复了因果结构，尽管存在潜在的子过程。

Abstract: Multivariate Hawkes process provides a powerful framework for modeling
temporal dependencies and event-driven interactions in complex systems. While
existing methods primarily focus on uncovering causal structures among observed
subprocesses, real-world systems are often only partially observed, with latent
subprocesses posing significant challenges. In this paper, we show that
continuous-time event sequences can be represented by a discrete-time model as
the time interval shrinks, and we leverage this insight to establish necessary
and sufficient conditions for identifying latent subprocesses and the causal
influences. Accordingly, we propose a two-phase iterative algorithm that
alternates between inferring causal relationships among discovered subprocesses
and uncovering new latent subprocesses, guided by path-based conditions that
guarantee identifiability. Experiments on both synthetic and real-world
datasets show that our method effectively recovers causal structures despite
the presence of latent subprocesses.

</details>


### [162] [BRIEF: BRain-Inspired network connection search with Extensive temporal feature Fusion enhances disease classification](https://arxiv.org/abs/2508.11732)
*Xiangxiang Cui,Min Zhao,Dongmei Zhi,Shile Qi,Vince D Calhoun,Jing Sui*

Main category: cs.LG

TL;DR: 提出了一种新的受大脑启发的特征融合框架，通过优化网络架构和利用Transformer进行多特征融合，提高了fMRI的精神疾病分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于fMRI的分类深度学习模型在网络架构确定（依赖经验）和特征空间融合（主要是简单的连接，缺乏相互学习）方面存在局限性。

Method: 提出了一种新的BRain-Inspired特征融合（BRIEF）框架，该框架能够通过结合改进的神经网络连接搜索（NCS）策略和基于Transformer的多特征融合模块来自动优化网络架构。

Result: BRIEF相比21种最先进的模型，性能显著提高了2.2%到12.1%。

Conclusion: BRIEF在区分精神分裂症（SZ）和自闭症谱系障碍（ASD）方面表现出显著改进，分别达到91.5%和78.4%的AUC。

Abstract: Existing deep learning models for functional MRI-based classification have
limitations in network architecture determination (relying on experience) and
feature space fusion (mostly simple concatenation, lacking mutual learning).
Inspired by the human brain's mechanism of updating neural connections through
learning and decision-making, we proposed a novel BRain-Inspired feature Fusion
(BRIEF) framework, which is able to optimize network architecture automatically
by incorporating an improved neural network connection search (NCS) strategy
and a Transformer-based multi-feature fusion module. Specifically, we first
extracted 4 types of fMRI temporal representations, i.e., time series (TCs),
static/dynamic functional connection (FNC/dFNC), and multi-scale dispersion
entropy (MsDE), to construct four encoders. Within each encoder, we employed a
modified Q-learning to dynamically optimize the NCS to extract high-level
feature vectors, where the NCS is formulated as a Markov Decision Process.
Then, all feature vectors were fused via a Transformer, leveraging both
stable/time-varying connections and multi-scale dependencies across different
brain regions to achieve the final classification. Additionally, an attention
module was embedded to improve interpretability. The classification performance
of our proposed BRIEF was compared with 21 state-of-the-art models by
discriminating two mental disorders from healthy controls: schizophrenia (SZ,
n=1100) and autism spectrum disorder (ASD, n=1550). BRIEF demonstrated
significant improvements of 2.2% to 12.1% compared to 21 algorithms, reaching
an AUC of 91.5% - 0.6% for SZ and 78.4% - 0.5% for ASD, respectively. This is
the first attempt to incorporate a brain-inspired, reinforcement learning
strategy to optimize fMRI-based mental disorder classification, showing
significant potential for identifying precise neuroimaging biomarkers.

</details>


### [163] [Scalable Geospatial Data Generation Using AlphaEarth Foundations Model](https://arxiv.org/abs/2508.11739)
*Luc Houriez,Sebastian Pilarski,Behzad Vahedi,Ali Ahmadalipour,Teo Honda Scully,Nicholas Aflitto,David Andre,Caroline Jaffe,Martha Wedner,Rich Mazzola,Josh Jeffery,Ben Messinger,Sage McGinley-Smith,Sarah Russell*

Main category: cs.LG

TL;DR: AEF can extend geospatial labeled datasets beyond their initial geographic regions, even basic models can be used and achieve 81% and 73% classification accuracy on EvtPhys validation sets in the USA and Canada


<details>
  <summary>Details</summary>
Motivation: High-quality labeled geospatial datasets are essential for extracting insights and understanding our planet. Unfortunately, these datasets often do not span the entire globe and are limited to certain geographic regions where data was collected. Google DeepMind's recently released AlphaEarth Foundations (AEF) provides an information-dense global geospatial representation designed to serve as a useful input across a wide gamut of tasks.

Method: leverages AEF to extend geospatial labeled datasets beyond their initial geographic regions, basic models like random forests or logistic regression can be used

Result: model predictions align with ground truth. Trained models achieve 81% and 73% classification accuracy on EvtPhys validation sets in the USA and Canada

Conclusion: Trained models achieve 81% and 73% classification accuracy on EvtPhys validation sets in the USA and Canada

Abstract: High-quality labeled geospatial datasets are essential for extracting
insights and understanding our planet. Unfortunately, these datasets often do
not span the entire globe and are limited to certain geographic regions where
data was collected. Google DeepMind's recently released AlphaEarth Foundations
(AEF) provides an information-dense global geospatial representation designed
to serve as a useful input across a wide gamut of tasks. In this article we
propose and evaluate a methodology which leverages AEF to extend geospatial
labeled datasets beyond their initial geographic regions. We show that even
basic models like random forests or logistic regression can be used to
accomplish this task. We investigate a case study of extending LANDFIRE's
Existing Vegetation Type (EVT) dataset beyond the USA into Canada at two levels
of granularity: EvtPhys (13 classes) and EvtGp (80 classes). Qualitatively, for
EvtPhys, model predictions align with ground truth. Trained models achieve 81%
and 73% classification accuracy on EvtPhys validation sets in the USA and
Canada, despite discussed limitations.

</details>


### [164] [Fed-Meta-Align: A Similarity-Aware Aggregation and Personalization Pipeline for Federated TinyML on Heterogeneous Data](https://arxiv.org/abs/2508.11794)
*Hemanth Macharla,Mayukha Pal*

Main category: cs.LG

TL;DR: This paper introduces Fed-Meta-Align, a four-phase federated learning framework that overcomes the limitations of standard FL in heterogeneous IoT environments. It uses a sophisticated initialization and training pipeline and achieves higher accuracy than existing methods.


<details>
  <summary>Details</summary>
Motivation: Real-time fault classification in resource-constrained Internet of Things (IoT) devices is critical for industrial safety, yet training robust models in such heterogeneous environments remains a significant challenge. Standard Federated Learning (FL) often fails in the presence of non-IID data, leading to model divergence.

Method: a novel four-phase framework designed to overcome these limitations through a sophisticated initialization and training pipeline. Our process begins by training a foundational model on a general public dataset to establish a competent starting point. This model then undergoes a serial meta-initialization phase, where it sequentially trains on a subset of IOT Device data to learn a heterogeneity-aware initialization that is already situated in a favorable region of the loss landscape. This informed model is subsequently refined in a parallel FL phase, which utilizes a dual-criterion aggregation mechanism that weights for IOT devices updates based on both local performance and cosine similarity alignment. Finally, an on-device personalization phase adapts the converged global model into a specialized expert for each IOT Device.

Result: achieves an average test accuracy of 91.27% across heterogeneous IOT devices, outperforming personalized FedAvg and FedProx by up to 3.87% and 3.37% on electrical and mechanical fault datasets, respectively

Conclusion: Fed-Meta-Align achieves an average test accuracy of 91.27% across heterogeneous IOT devices, outperforming personalized FedAvg and FedProx by up to 3.87% and 3.37% on electrical and mechanical fault datasets, respectively. This multi-stage approach of sequenced initialization and adaptive aggregation provides a robust pathway for deploying high-performance intelligence on diverse TinyML networks.

Abstract: Real-time fault classification in resource-constrained Internet of Things
(IoT) devices is critical for industrial safety, yet training robust models in
such heterogeneous environments remains a significant challenge. Standard
Federated Learning (FL) often fails in the presence of non-IID data, leading to
model divergence. This paper introduces Fed-Meta-Align, a novel four-phase
framework designed to overcome these limitations through a sophisticated
initialization and training pipeline. Our process begins by training a
foundational model on a general public dataset to establish a competent
starting point. This model then undergoes a serial meta-initialization phase,
where it sequentially trains on a subset of IOT Device data to learn a
heterogeneity-aware initialization that is already situated in a favorable
region of the loss landscape. This informed model is subsequently refined in a
parallel FL phase, which utilizes a dual-criterion aggregation mechanism that
weights for IOT devices updates based on both local performance and cosine
similarity alignment. Finally, an on-device personalization phase adapts the
converged global model into a specialized expert for each IOT Device.
Comprehensive experiments demonstrate that Fed-Meta-Align achieves an average
test accuracy of 91.27% across heterogeneous IOT devices, outperforming
personalized FedAvg and FedProx by up to 3.87% and 3.37% on electrical and
mechanical fault datasets, respectively. This multi-stage approach of sequenced
initialization and adaptive aggregation provides a robust pathway for deploying
high-performance intelligence on diverse TinyML networks.

</details>


### [165] [Uncalibrated Reasoning: GRPO Induces Overconfidence for Stochastic Outcomes](https://arxiv.org/abs/2508.11800)
*Michael Bereket,Jure Leskovec*

Main category: cs.LG

TL;DR: 强化学习可用于优化科学实验中的语言模型，但需要注意校准问题。


<details>
  <summary>Details</summary>
Motivation: 研究当前的强化学习方法在具有随机结果的可验证领域（如科学实验）中优化语言模型是否有效。

Method: 使用Group Relative Policy Optimization (GRPO), Proximal Policy Optimization (PPO)和REINFORCE Leave-One-Out (RLOO)等强化学习方法。

Result: GRPO会引发二元随机结果的过度自信概率预测，而PPO和RLOO会产生良好校准的模型。去除GRPO中的group standard normalization可以解决其校准问题。

Conclusion: GRPO中的标准归一化会导致过度自信，去除group standard normalization可以解决这个问题。为在确定性领域之外的推理语言模型应用强化学习铺平了道路。

Abstract: Reinforcement learning (RL) has proven remarkably effective at improving the
accuracy of language models in verifiable and deterministic domains like
mathematics. Here, we examine if current RL methods are also effective at
optimizing language models in verifiable domains with stochastic outcomes, like
scientific experiments. Through applications to synthetic data and real-world
biological experiments, we demonstrate that Group Relative Policy Optimization
(GRPO) induces overconfident probability predictions for binary stochastic
outcomes, while Proximal Policy Optimization (PPO) and REINFORCE Leave-One-Out
(RLOO) yield well-calibrated models. We show that removing group standard
normalization in GRPO fixes its miscalibration and provide a theoretical
explanation for why normalization causes overconfidence. Our results provide
new evidence against the use of standard normalization in GRPO and help pave
the way for applications of RL for reasoning language models beyond
deterministic domains.

</details>


### [166] [FairTabGen: Unifying Counterfactual and Causal Fairness in Synthetic Tabular Data Generation](https://arxiv.org/abs/2508.11810)
*Nitish Nagesh,Salar Shakibhamedan,Mahdi Bagheri,Ziyu Wang,Nima TaheriNejad,Axel Jantsch,Amir M. Rahmani*

Main category: cs.LG

TL;DR: FairTabGen, a new LLM-based framework, generates fairer and more useful synthetic tabular data, especially when data is scarce.


<details>
  <summary>Details</summary>
Motivation: Generating synthetic data is crucial in privacy-sensitive and data-scarce settings, particularly for tabular datasets. Improving counterfactual and causal fairness while preserving utility is a key challenge.

Method: A fairness-aware large language model-based framework is used, integrating counterfactual and causal fairness into generation and evaluation. In-context learning, prompt refinement, and fairness-aware data curation are employed.

Result: FairTabGen achieves up to 10% improvements on fairness metrics while retaining statistical utility, using less than 20% of the original data.

Conclusion: FairTabGen provides a practical approach for generating fair and useful synthetic tabular data, outperforming existing methods in balancing fairness and utility, especially in low-data regimes.

Abstract: Generating synthetic data is crucial in privacy-sensitive, data-scarce
settings, especially for tabular datasets widely used in real-world
applications. A key challenge is improving counterfactual and causal fairness,
while preserving high utility. We present FairTabGen, a fairness-aware large
language model-based framework for tabular synthetic data generation. We
integrate multiple fairness definitions including counterfactual and causal
fairness into both its generation and evaluation pipelines. We use in-context
learning, prompt refinement, and fairness-aware data curation to balance
fairness and utility. Across diverse datasets, our method outperforms
state-of-the-art GAN-based and LLM-based methods, achieving up to 10%
improvements on fairness metrics such as demographic parity and path-specific
causal effects while retaining statistical utility. Remarkably, it achieves
these gains using less than 20% of the original data, highlighting its
efficiency in low-data regimes. These results demonstrate a principled and
practical approach for generating fair and useful synthetic tabular data.

</details>


### [167] [Combinations of Fast Activation and Trigonometric Functions in Kolmogorov-Arnold Networks](https://arxiv.org/abs/2508.11876)
*Hoang-Thang Ta,Duy-Quy Thai,Phuong-Linh Tran-Thi*

Main category: cs.LG

TL;DR: This paper uses faster functions (ReLU, sin, cos, arctan) in KANs to improve speed and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing KANs rely on polynomial functions like B-splines and RBFs, which are not fully supported by GPUs and are less popular.

Method: The paper integrates ReLU and trigonometric functions into the KAN network structure.

Result: The proposed combinations maintain competitive performance while offering potential improvements in training time and generalization.

Conclusion: This paper proposes using fast computational functions like ReLU and trigonometric functions in KANs to enhance computational efficiency. Experimental results show competitive performance with potential improvements in training time and generalization.

Abstract: For years, many neural networks have been developed based on the
Kolmogorov-Arnold Representation Theorem (KART), which was created to address
Hilbert's 13th problem. Recently, relying on KART, Kolmogorov-Arnold Networks
(KANs) have attracted attention from the research community, stimulating the
use of polynomial functions such as B-splines and RBFs. However, these
functions are not fully supported by GPU devices and are still considered less
popular. In this paper, we propose the use of fast computational functions,
such as ReLU and trigonometric functions (e.g., ReLU, sin, cos, arctan), as
basis components in Kolmogorov-Arnold Networks (KANs). By integrating these
function combinations into the network structure, we aim to enhance
computational efficiency. Experimental results show that these combinations
maintain competitive performance while offering potential improvements in
training time and generalization.

</details>


### [168] [PCA- and SVM-Grad-CAM for Convolutional Neural Networks: Closed-form Jacobian Expression](https://arxiv.org/abs/2508.11880)
*Yuto Omae*

Main category: cs.LG

TL;DR: This paper introduces PCA-Grad-CAM and SVM-Grad-CAM to visualize attention regions in PCA and SVM layers of CNNs, addressing the limitations of traditional Grad-CAM when using these layers to improve classification with limited data.


<details>
  <summary>Details</summary>
Motivation: Traditional Grad-CAM cannot be directly applied to PCA and/or SVM layers in CNNs, which limits the development of white-box methods for CNNs when training samples are limited.  Incorporating PCA and/or SVM can improve performance with limited training data, but lacks visualization.

Method: The paper proposes PCA-Grad-CAM and SVM-Grad-CAM methods for visualizing attention regions in PCA and SVM layers within CNNs. It derives the exact closed-form Jacobian for these layers.

Result: The paper introduces PCA-Grad-CAM and SVM-Grad-CAM for visualizing attention regions and provides visualization results on major datasets.

Conclusion: The paper presents exact closed-form Jacobian and visualization results of PCA-Grad-CAM and SVM-Grad-CAM applied to several major datasets.

Abstract: Convolutional Neural Networks (CNNs) are an effective approach for
classification tasks, particularly when the training dataset is large. Although
CNNs have long been considered a black-box classification method, they can be
used as a white-box method through visualization techniques such as Grad-CAM.
When training samples are limited, incorporating a Principal Component Analysis
(PCA) layer and/or a Support Vector Machine (SVM) classifier into a CNN can
effectively improve classification performance. However, traditional Grad-CAM
cannot be directly applied to PCA and/or SVM layers. It is important to
generate attention regions for PCA and/or SVM layers in CNNs to facilitate the
development of white-box methods. Therefore, we propose ``PCA-Grad-CAM'', a
method for visualizing attention regions in PCA feature vectors, and
``SVM-Grad-CAM'', a method for visualizing attention regions in an SVM
classifier layer. To complete our methods analytically, it is necessary to
solve the closed-form Jacobian consisting of partial derivatives from the last
convolutional layer to the PCA and/or SVM layers. In this paper, we present the
exact closed-form Jacobian and the visualization results of our methods applied
to several major datasets.

</details>


### [169] [ENA: Efficient N-dimensional Attention](https://arxiv.org/abs/2508.11921)
*Yibo Zhong*

Main category: cs.LG

TL;DR: 提出了一种名为ENA的线性循环和高阶SWA混合架构，用于高效建模超长高阶数据。


<details>
  <summary>Details</summary>
Motivation: Transformer在对高阶数据的长序列进行建模时效率较低，因此需要更高效的架构。

Method: 线性循环模型和高阶滑动窗口注意力（SWA）的混合架构

Result: 扫描提供的优势有限，而注意力混合模型产生了有希望的结果。分块高阶滑动窗口注意力（SWA）在理论和实践中都是有效的。

Conclusion: 线性循环能够压缩全局信息到状态中，而SWA通过强制执行严格的局部建模来对其进行补充。它们共同构成了一个简单的框架，为超长高阶数据建模提供了一个有前景且实用的解决方案。

Abstract: Efficient modeling of long sequences of high-order data requires a more
efficient architecture than Transformer. In this paper, we investigate two key
aspects of extending linear recurrent models, especially those originally
designed for language modeling, to high-order data (1D to ND): scanning
strategies and attention-hybrid architectures. Empirical results suggest that
scanning provides limited benefits, while attention-hybrid models yield
promising results. Focusing on the latter, we further evaluate types of
attention and find that tiled high-order sliding window attention (SWA) is
efficient in both theory and practice. We term the resulting hybrid
architecture of linear recurrence and high-order SWA as Efficient N-dimensional
Attention (ENA). We then conduct several experiments to demonstrate its
effectiveness. The intuition behind ENA is that linear recurrence compresses
global information into a state, while SWA complements it by enforcing strict
local modeling. Together, they form a simple framework that offers a promising
and practical solution for ultra-long high-order data modeling.

</details>


### [170] [Scale-Disentangled spatiotemporal Modeling for Long-term Traffic Emission Forecasting](https://arxiv.org/abs/2508.11923)
*Yan Wu,Lihong Pei,Yukai Han,Yang Cao,Yu Kang,Yanlong Zhao*

Main category: cs.LG

TL;DR: SDSTM model addresses cascading error amplification in long-term traffic emission forecasting by disentangling and fusing features at different scales with a dual-stream independence constraint.


<details>
  <summary>Details</summary>
Motivation: Traditional forecasting methods suffer from cascading error amplification during long-term inference due to the multi-scale entanglement of traffic emissions across time and space.

Method: A Scale-Disentangled Spatio-Temporal Modeling (SDSTM) framework leveraging the predictability differences across multiple scales to decompose and fuse features, incorporating a dual-stream independence constraint based on cross-term loss.

Result: The SDSTM model achieves state-of-the-art performance.

Conclusion: The proposed SDSTM model achieves state-of-the-art performance on a road-level traffic emission dataset within Xi'an's Second Ring Road.

Abstract: Long-term traffic emission forecasting is crucial for the comprehensive
management of urban air pollution. Traditional forecasting methods typically
construct spatiotemporal graph models by mining spatiotemporal dependencies to
predict emissions. However, due to the multi-scale entanglement of traffic
emissions across time and space, these spatiotemporal graph modeling method
tend to suffer from cascading error amplification during long-term inference.
To address this issue, we propose a Scale-Disentangled Spatio-Temporal Modeling
(SDSTM) framework for long-term traffic emission forecasting. It leverages the
predictability differences across multiple scales to decompose and fuse
features at different scales, while constraining them to remain independent yet
complementary. Specifically, the model first introduces a dual-stream feature
decomposition strategy based on the Koopman lifting operator. It lifts the
scale-coupled spatiotemporal dynamical system into an infinite-dimensional
linear space via Koopman operator, and delineates the predictability boundary
using gated wavelet decomposition. Then a novel fusion mechanism is
constructed, incorporating a dual-stream independence constraint based on
cross-term loss to dynamically refine the dual-stream prediction results,
suppress mutual interference, and enhance the accuracy of long-term traffic
emission prediction. Extensive experiments conducted on a road-level traffic
emission dataset within Xi'an's Second Ring Road demonstrate that the proposed
model achieves state-of-the-art performance.

</details>


### [171] [An Improved Algorithm for Adversarial Linear Contextual Bandits via Reduction](https://arxiv.org/abs/2508.11931)
*Tim van Erven,Jack Mayo,Julia Olkhovskaya,Chen-Yu Wei*

Main category: cs.LG

TL;DR: 提出了一种用于解决具有对抗性损失和随机动作集的线性上下文bandit的有效算法，该算法在多项式时间内实现了$\text{poly}(d)\\sqrt{T}$遗憾。


<details>
  <summary>Details</summary>
Motivation: Liu et al. (2023) 提出了一个开放性问题，即是否可以在独立于动作数量的多项式时间内获得$\\text{poly}(d)\\sqrt{T}$遗憾。

Method: 将这种设置简化为具有固定动作集的错误指定鲁棒对抗线性bandit。

Result: 该算法实现了$\\tilde{O}(\\min\\{d^2\\sqrt{T}, \\sqrt{d^3T\\log K}\\})$的遗憾，并在$\\text{poly}(d,C,T)$时间内运行。

Conclusion: 对于具有对抗性损失和随机动作集的组合bandit的重要类别，其中动作集可以用多项式数量的线性约束来描述，该算法是第一个在多项式时间内实现$\\text{poly}(d)\\sqrt{T}$遗憾的算法，而据我们所知，没有先前的算法在多项式时间内实现甚至$o(T)$遗憾。当模拟器可用时，遗憾界可以提高到$\\tilde{O}(d\\sqrt{L^\\star})$，其中$L^\\star$是最佳策略的累积损失。

Abstract: We present an efficient algorithm for linear contextual bandits with
adversarial losses and stochastic action sets. Our approach reduces this
setting to misspecification-robust adversarial linear bandits with fixed action
sets. Without knowledge of the context distribution or access to a context
simulator, the algorithm achieves $\tilde{O}(\min\{d^2\sqrt{T}, \sqrt{d^3T\log
K}\})$ regret and runs in $\text{poly}(d,C,T)$ time, where $d$ is the feature
dimension, $C$ is an upper bound on the number of linear constraints defining
the action set in each round, $K$ is an upper bound on the number of actions in
each round, and $T$ is number of rounds. This resolves the open question by Liu
et al. (2023) on whether one can obtain $\text{poly}(d)\sqrt{T}$ regret in
polynomial time independent of the number of actions. For the important class
of combinatorial bandits with adversarial losses and stochastic action sets
where the action sets can be described by a polynomial number of linear
constraints, our algorithm is the first to achieve $\text{poly}(d)\sqrt{T}$
regret in polynomial time, while no prior algorithm achieves even $o(T)$ regret
in polynomial time to our knowledge. When a simulator is available, the regret
bound can be improved to $\tilde{O}(d\sqrt{L^\star})$, where $L^\star$ is the
cumulative loss of the best policy.

</details>


### [172] [M3OOD: Automatic Selection of Multimodal OOD Detectors](https://arxiv.org/abs/2508.11936)
*Yuehan Qin,Li Li,Defu Cao,Tiankai Yang,Yue Zhao*

Main category: cs.LG

TL;DR: M3OOD是一种基于元学习的框架，用于在多模态环境中选择OOD检测器，它可以学习历史模型行为，并以最小的监督快速适应新的数据分布变化。


<details>
  <summary>Details</summary>
Motivation: 在多模态环境中，OOD鲁棒性至关重要。现有的OOD检测方法各有优缺点，难以在所有场景中都表现良好。由于OOD检测任务的无监督性质，预测模型性能和找到通用的最佳模型非常困难。系统地比较新模型成本高昂甚至不切实际。

Method: 结合多模态嵌入与手工设计的元特征，这些元特征捕获分布和跨模态特征以表示数据集。利用跨各种多模态基准的历史性能，M3OOD可以为新的数据分布变化推荐合适的检测器。

Result: M3OOD框架在多模态设置中选择OOD检测器，通过学习历史模型行为，能够以最小的监督快速适应新的数据分布变化。

Conclusion: M3OOD在12个测试场景中始终优于10个竞争基线，且计算开销最小。

Abstract: Out-of-distribution (OOD) robustness is a critical challenge for modern
machine learning systems, particularly as they increasingly operate in
multimodal settings involving inputs like video, audio, and sensor data.
Currently, many OOD detection methods have been proposed, each with different
designs targeting various distribution shifts. A single OOD detector may not
prevail across all the scenarios; therefore, how can we automatically select an
ideal OOD detection model for different distribution shifts? Due to the
inherent unsupervised nature of the OOD detection task, it is difficult to
predict model performance and find a universally Best model. Also,
systematically comparing models on the new unseen data is costly or even
impractical. To address this challenge, we introduce M3OOD, a
meta-learning-based framework for OOD detector selection in multimodal
settings. Meta learning offers a solution by learning from historical model
behaviors, enabling rapid adaptation to new data distribution shifts with
minimal supervision. Our approach combines multimodal embeddings with
handcrafted meta-features that capture distributional and cross-modal
characteristics to represent datasets. By leveraging historical performance
across diverse multimodal benchmarks, M3OOD can recommend suitable detectors
for a new data distribution shift. Experimental evaluation demonstrates that
M3OOD consistently outperforms 10 competitive baselines across 12 test
scenarios with minimal computational overhead.

</details>


### [173] [Extending Straight-Through Estimation for Robust Neural Networks on Analog CIM Hardware](https://arxiv.org/abs/2508.11940)
*Yuannuo Feng,Wenyong Zhou,Yuexi Lyu,Yixiang Zhang,Zhengwu Liu,Ngai Wong,Wang Kang*

Main category: cs.LG

TL;DR: 该论文提出了一种用于模拟CIM的噪声感知训练的新框架，该框架提高了精度、降低了困惑度、加快了训练速度并减少了内存使用。


<details>
  <summary>Details</summary>
Motivation: 模拟计算内存（CIM）架构在神经网络推理方面具有显著的能效优势，但存在复杂的硬件引入噪声，这对部署提出了重大挑战。现有的噪声感知训练方法依赖于理想化和可微的噪声模型，无法捕捉模拟CIM硬件变化的全部复杂性。

Method: 该论文提出了一种扩展的STE框架，该框架解耦了前向噪声模拟和后向梯度计算，从而可以使用更准确的噪声模型进行噪声感知训练。

Result: 该论文的扩展STE框架在图像分类上实现了高达5.3%的精度提升，在文本生成上实现了0.72的困惑度降低，训练时间加速2.2倍，峰值内存使用降低37.9%。

Conclusion: 该论文提出了一种扩展的STE框架，通过解耦前向噪声模拟和后向梯度计算，实现了更精确但计算上难以处理的模拟CIM系统噪声建模的噪声感知训练。实验表明，该框架在图像分类上实现了高达5.3%的精度提升，在文本生成上实现了0.72的困惑度降低，训练时间加速2.2倍，峰值内存使用降低37.9%。

Abstract: Analog Compute-In-Memory (CIM) architectures promise significant energy
efficiency gains for neural network inference, but suffer from complex
hardware-induced noise that poses major challenges for deployment. While
noise-aware training methods have been proposed to address this issue, they
typically rely on idealized and differentiable noise models that fail to
capture the full complexity of analog CIM hardware variations. Motivated by the
Straight-Through Estimator (STE) framework in quantization, we decouple forward
noise simulation from backward gradient computation, enabling noise-aware
training with more accurate but computationally intractable noise modeling in
analog CIM systems. We provide theoretical analysis demonstrating that our
approach preserves essential gradient directional information while maintaining
computational tractability and optimization stability. Extensive experiments
show that our extended STE framework achieves up to 5.3% accuracy improvement
on image classification, 0.72 perplexity reduction on text generation,
2.2$\times$ speedup in training time, and 37.9% lower peak memory usage
compared to standard noise-aware training methods.

</details>


### [174] [Learning Marked Temporal Point Process Explanations based on Counterfactual and Factual Reasoning](https://arxiv.org/abs/2508.11943)
*Sishun Liu,Ke Deng,Xiuzhen Zhang,Yan Wang*

Main category: cs.LG

TL;DR: This paper introduces CFF, a novel approach for generating explanations for MTPP models, which combines counterfactual and factual explanations to provide more rational and efficient results.


<details>
  <summary>Details</summary>
Motivation: This study focuses on Explanation for MTPP, aiming to identify the minimal and rational explanation. This study finds that directly defining Explanation for MTPP as counterfactual explanation or factual explanation can result in irrational explanations. To address this issue, we define Explanation for MTPP as a combination of counterfactual explanation and factual explanation.

Method: This study proposes Counterfactual and Factual Explainer for MTPP (CFF) to solve Explanation for MTPP with a series of deliberately designed techniques.

Result: Experiments demonstrate the correctness and superiority of CFF over baselines regarding explanation quality and processing efficiency.

Conclusion: This study proposes CFF to solve Explanation for MTPP with a series of deliberately designed techniques. Experiments demonstrate the correctness and superiority of CFF over baselines regarding explanation quality and processing efficiency.

Abstract: Neural network-based Marked Temporal Point Process (MTPP) models have been
widely adopted to model event sequences in high-stakes applications, raising
concerns about the trustworthiness of outputs from these models. This study
focuses on Explanation for MTPP, aiming to identify the minimal and rational
explanation, that is, the minimum subset of events in history, based on which
the prediction accuracy of MTPP matches that based on full history to a great
extent and better than that based on the complement of the subset. This study
finds that directly defining Explanation for MTPP as counterfactual explanation
or factual explanation can result in irrational explanations. To address this
issue, we define Explanation for MTPP as a combination of counterfactual
explanation and factual explanation. This study proposes Counterfactual and
Factual Explainer for MTPP (CFF) to solve Explanation for MTPP with a series of
deliberately designed techniques. Experiments demonstrate the correctness and
superiority of CFF over baselines regarding explanation quality and processing
efficiency.

</details>


### [175] [Set-Valued Transformer Network for High-Emission Mobile Source Identification](https://arxiv.org/abs/2508.11976)
*Yunning Cao,Lihong Pei,Jian Guo,Yang Cao,Yu Kang,Yanlong Zhao*

Main category: cs.LG

TL;DR: The paper introduces a Set-Valued Transformer Network (SVTN) to improve the detection accuracy of high-emission vehicles, achieving a 9.5% reduction in missed detections compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Identifying high-emission vehicles is crucial for regulating urban pollution, but the low proportion of high-emission data and the nonlinear nature of vehicle emission states pose challenges to building identification models.

Method: A Set-Valued Transformer Network (SVTN) is proposed, which uses a transformer to measure the temporal similarity of micro-trip condition variations and a set-valued identification algorithm to probabilistically model the relationship between feature vectors and their labels.

Result: The SVTN achieves a 9.5% reduction in the missed detection rate for high-emission vehicles compared to the transformer-based baseline.

Conclusion: The proposed Set-Valued Transformer Network (SVTN) reduces the missed detection rate for high-emission vehicles by 9.5% compared to the transformer-based baseline on diesel vehicle monitoring data in Hefei city in 2020.

Abstract: Identifying high-emission vehicles is a crucial step in regulating urban
pollution levels and formulating traffic emission reduction strategies.
However, in practical monitoring data, the proportion of high-emission state
data is significantly lower compared to normal emission states. This
characteristic long-tailed distribution severely impedes the extraction of
discriminative features for emission state identification during data mining.
Furthermore, the highly nonlinear nature of vehicle emission states and the
lack of relevant prior knowledge also pose significant challenges to the
construction of identification models.To address the aforementioned issues, we
propose a Set-Valued Transformer Network (SVTN) to achieve comprehensive
learning of discriminative features from high-emission samples, thereby
enhancing detection accuracy. Specifically, this model first employs the
transformer to measure the temporal similarity of micro-trip condition
variations, thus constructing a mapping rule that projects the original
high-dimensional emission data into a low-dimensional feature space. Next, a
set-valued identification algorithm is used to probabilistically model the
relationship between the generated feature vectors and their labels, providing
an accurate metric criterion for the classification algorithm. To validate the
effectiveness of our proposed approach, we conducted extensive experiments on
the diesel vehicle monitoring data of Hefei city in 2020. The results
demonstrate that our method achieves a 9.5\% reduction in the missed detection
rate for high-emission vehicles compared to the transformer-based baseline,
highlighting its superior capability in accurately identifying high-emission
mobile pollution sources.

</details>


### [176] [Efficient Modular Learning through Naive LoRA Summation: Leveraging Orthogonality in High-Dimensional Models](https://arxiv.org/abs/2508.11985)
*Zhanhao Cao,Clement Truong,Andrew Lizarraga*

Main category: cs.LG

TL;DR: Independently trained LoRA modules on disjoint domains are approximately orthogonal and can be combined by simple addition.


<details>
  <summary>Details</summary>
Motivation: Motivated by the superposition principle, we hypothesize that independently trained LoRA modules on disjoint domains are approximately orthogonal and can be combined by simple addition.

Method: Using GPT-2 Small (117M) with LoRA rank 4 and alpha=64, we train adapters for three QA domains (math, medicine, finance).

Result: In pairwise tests, adding Math+Medicine adapters improves perplexity by -9.10% relative to merged-data fine-tuning, while Math+Finance and Finance+Medicine change by +4.54% and +27.56%, respectively. Across combinations, the RMS cosine similarity between LoRA deltas correlates positively and approximately linearly with the change in perplexity.

Conclusion: Naive summation requires no additional training, can be applied in seconds, and achieves performance comparable to models trained on merged data, while clarifying when interference appears in higher-order compositions.

Abstract: Recent advances in large language models are driven by scale, while
parameter-efficient fine-tuning (PEFT) enables updating only a small fraction
of parameters. Low-Rank Adaptation (LoRA) stores parameter deltas as the
product of two small matrices, which makes them natural building blocks that
can be composed. Motivated by the superposition principle, we hypothesize that
independently trained LoRA modules on disjoint domains are approximately
orthogonal and can be combined by simple addition. Using GPT-2 Small (117M)
with LoRA rank 4 and alpha=64, we train adapters for three QA domains (math,
medicine, finance). In pairwise tests, adding Math+Medicine adapters improves
perplexity by -9.10% relative to merged-data fine-tuning, while Math+Finance
and Finance+Medicine change by +4.54% and +27.56%, respectively. Across
combinations, the RMS cosine similarity between LoRA deltas correlates
positively and approximately linearly with the change in perplexity. Naive
summation requires no additional training, can be applied in seconds, and
achieves performance comparable to models trained on merged data, while
clarifying when interference appears in higher-order compositions.

</details>


### [177] [Universal Learning of Nonlinear Dynamics](https://arxiv.org/abs/2508.11990)
*Evan Dogariu,Anand Brahmbhatt,Elad Hazan*

Main category: cs.LG

TL;DR: learns a mapping from past observations to the next based on a spectral representation of the system


<details>
  <summary>Details</summary>
Motivation: learning a marginally stable unknown nonlinear dynamical system

Method: spectral filtering

Result: a new spectral filtering algorithm for linear dynamical systems, which incorporates past observations and applies to general noisy and marginally stable systems

Conclusion: vanishing prediction error for any nonlinear dynamical system that has finitely many marginally stable modes

Abstract: We study the fundamental problem of learning a marginally stable unknown
nonlinear dynamical system. We describe an algorithm for this problem, based on
the technique of spectral filtering, which learns a mapping from past
observations to the next based on a spectral representation of the system.
Using techniques from online convex optimization, we prove vanishing prediction
error for any nonlinear dynamical system that has finitely many marginally
stable modes, with rates governed by a novel quantitative control-theoretic
notion of learnability. The main technical component of our method is a new
spectral filtering algorithm for linear dynamical systems, which incorporates
past observations and applies to general noisy and marginally stable systems.
This significantly generalizes the original spectral filtering algorithm to
both asymmetric dynamics as well as incorporating noise correction, and is of
independent interest.

</details>


### [178] [FedUHD: Unsupervised Federated Learning using Hyperdimensional Computing](https://arxiv.org/abs/2508.12021)
*You Hak Lee,Xiaofan Yu,Quanling Zhao,Flavio Ponzina,Tajana Rosing*

Main category: cs.LG

TL;DR: FedUHD 是一种基于超维计算 (HDC) 的无监督联邦学习 (UFL) 框架，它通过在客户端使用 kNN 聚类超向量去除方法和在服务器端使用加权 HDC 聚合技术来提高 UFL 性能，从而在训练速度、能效、通信成本和准确性方面优于基于神经网络的方法。


<details>
  <summary>Details</summary>
Motivation: 无监督联邦学习 (UFL) 作为一种保护隐私的去中心化机器学习方法而受到关注，它无需人工数据标记。但是，UFL 在实际应用中面临着一些挑战：(1) 设备之间的非独立且相同分布 (non-iid) 数据分布，(2) 边缘处昂贵的计算和通信成本，以及 (3) 容易受到通信噪声的影响。以前的 UFL 方法依赖于深度神经网络 (NN)，这会在计算和通信中引入大量开销。

Method: FedUHD，第一个基于超维计算 (HDC) 的 UFL 框架。在客户端，一种基于 kNN 的聚类超向量去除方法通过消除有害的异常值来解决非 iid 数据样本。在服务器端，一种加权 HDC 聚合技术平衡了跨客户端的非 iid 数据分布。

Result: FedUHD 的训练速度分别提高了 173.6 倍和 612.7 倍，能效提高了 173.6 倍和 612.7 倍，通信成本降低了 271 倍，并且在各种设置中的平均准确率提高了 15.50%，并且与最先进的基于 NN 的 UFL 方法相比，对各种类型的噪声具有更强的鲁棒性。

Conclusion: FedUHD在不同设置中平均准确率提高了 15.50%，训练速度提高了 173.6 倍，能效提高了 612.7 倍，通信成本降低了 271 倍，并且与最先进的基于 NN 的 UFL 方法相比，对各种类型的噪声具有更强的鲁棒性。

Abstract: Unsupervised federated learning (UFL) has gained attention as a
privacy-preserving, decentralized machine learning approach that eliminates the
need for labor-intensive data labeling. However, UFL faces several challenges
in practical applications: (1) non-independent and identically distributed
(non-iid) data distribution across devices, (2) expensive computational and
communication costs at the edge, and (3) vulnerability to communication noise.
Previous UFL approaches have relied on deep neural networks (NN), which
introduce substantial overhead in both computation and communication. In this
paper, we propose FedUHD, the first UFL framework based on Hyperdimensional
Computing (HDC). HDC is a brain-inspired computing scheme with lightweight
training and inference operations, much smaller model size, and robustness to
communication noise. FedUHD introduces two novel HDC-based designs to improve
UFL performance. On the client side, a kNN-based cluster hypervector removal
method addresses non-iid data samples by eliminating detrimental outliers. On
the server side, a weighted HDC aggregation technique balances the non-iid data
distribution across clients. Our experiments demonstrate that FedUHD achieves
up to 173.6x and 612.7x better speedup and energy efficiency, respectively, in
training, up to 271x lower communication cost, and 15.50% higher accuracy on
average across diverse settings, along with superior robustness to various
types of noise compared to state-of-the-art NN-based UFL approaches.

</details>


### [179] [Fairness Regularization in Federated Learning](https://arxiv.org/abs/2508.12042)
*Zahra Kharaghani,Ali Dadras,Tommy Löfstedt*

Main category: cs.LG

TL;DR: This paper focuses on performance equitable fairness in Federated Learning, introduces FairGrad and FairGrad* (two variants of a gradient variance regularization method), and demonstrates their effectiveness in improving fairness and overall model performance in heterogeneous data settings.


<details>
  <summary>Details</summary>
Motivation: Fairness is a crucial concern in Federated Learning (FL), and the effectiveness of existing fairness-aware methods, particularly in heterogeneous data settings, remains unclear, and the relationships between different approaches are not well understood.

Method: gradient variance regularization

Result: Identified and theoretically explain connections between the investigated fairness methods, and empirically show that FairGrad (approximate) and FairGrad* (exact) improve both fairness and overall model performance in heterogeneous data settings.

Conclusion: FairGrad (approximate) and FairGrad* (exact) improve both fairness and overall model performance in heterogeneous data settings.

Abstract: Federated Learning (FL) has emerged as a vital paradigm in modern machine
learning that enables collaborative training across decentralized data sources
without exchanging raw data. This approach not only addresses privacy concerns
but also allows access to overall substantially larger and potentially more
diverse datasets, without the need for centralized storage or hardware
resources. However, heterogeneity in client data may cause certain clients to
have disproportionate impacts on the global model, leading to disparities in
the clients' performances. Fairness, therefore, becomes a crucial concern in FL
and can be addressed in various ways. However, the effectiveness of existing
fairness-aware methods, particularly in heterogeneous data settings, remains
unclear, and the relationships between different approaches are not well
understood. In this work, we focus on performance equitable fairness, which
aims to minimize differences in performance across clients. We restrict our
study to fairness-aware methods that explicitly regularize client losses,
evaluating both existing and newly proposed approaches. We identify and
theoretically explain connections between the investigated fairness methods,
and empirically show that FairGrad (approximate) and FairGrad* (exact) (two
variants of a gradient variance regularization method introduced here for
performance equitable fairness) improve both fairness and overall model
performance in heterogeneous data settings.

</details>


### [180] [VARAN: Variational Inference for Self-Supervised Speech Models Fine-Tuning on Downstream Tasks](https://arxiv.org/abs/2508.12061)
*Daria Diatlova,Nikita Balagansky,Alexander Varlamov,Egor Spirin*

Main category: cs.LG

TL;DR: VARAN是一种动态调整层聚合到单个输入的框架，它优于传统的层聚合方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法在微调的自监督语音模型中聚合层时，例如使用最后一层或加权和，存在信息瓶颈和所有数据集示例的静态特征权重的问题。

Method: VARAN通过采用层专业探测头和数据相关的权重，自适应地优先考虑基于输入的层特征。

Result: VARAN在自动语音识别和语音情感识别任务中表现出了卓越的性能。

Conclusion: VARAN在自动语音识别和语音情感识别任务中表现出色，尤其是在使用LoRA微调技术时。该框架解决了保留层特定信息和实现灵活特征利用之间的权衡。

Abstract: Conventional methods for aggregating layers in fine-tuned self-supervised
speech models, such as using the final layer or weighted sum, suffer from
information bottlenecks and static feature weighting for all dataset examples.
We propose VARAN, a framework that dynamically tailors layer aggregation to
individual inputs. By employing layer-specialized probing heads and
data-dependent weighting, VARAN adaptively prioritizes layer's features based
on input. Evaluations on automatic speech recognition and speech emotion
recognition tasks demonstrate VARAN's superior performance, particularly when
using the LoRA fine-tuning technique. The framework resolves the trade-off
between preserving layer-specific information and enabling flexible feature
utilization, advancing efficient adaptation of self-supervised speech
representations.

</details>


### [181] [Content Accuracy and Quality Aware Resource Allocation Based on LP-Guided DRL for ISAC-Driven AIGC Networks](https://arxiv.org/abs/2508.12079)
*Ningzhe Shi,Yiqing Zhou,Ling Liu,Jinglin Shi,Yihao Wu,Haiwei Shi,Hanxiao Yu*

Main category: cs.LG

TL;DR: 提出了一种用于评估基于ISAC的AIGC服务质量的指标CAQA，并提出了一种LPDRL-F算法来优化资源分配，实验表明该算法可以显著提高AvgCAQA。


<details>
  <summary>Details</summary>
Motivation: 现有的AIGC服务通常假设在给定准确的输入数据和提示的情况下，可以确保生成内容的准确性，因此只关注内容生成质量（CGQ）。然而，这不适用于基于ISAC的AIGC网络，在这些网络中，内容生成是基于不准确的感知数据。此外，AIGC模型本身会引入生成误差，这取决于生成步骤的数量（即计算资源）。

Method: 提出了一种内容准确性和质量感知服务评估指标（CAQA）以及一种线性规划（LP）引导的深度强化学习（DRL）算法，该算法具有动作过滤器（LPDRL-F）。

Result: 与现有的DRL和没有LP的生成扩散模型算法相比，LPDRL-F收敛速度提高了60%以上，并找到了更好的资源分配解决方案，平均CAQA提高了14%以上。与现有仅关注CGQ的方案相比，CAQA-AIGC在AvgCAQA方面提高了50%以上。

Conclusion: 提出了一种线性规划（LP）引导的深度强化学习（DRL）算法，该算法具有动作过滤器（LPDRL-F），与现有的DRL和没有LP的生成扩散模型算法相比，LPDRL-F收敛速度提高了60%以上，并找到了更好的资源分配解决方案，平均CAQA提高了14%以上。与现有仅关注CGQ的方案相比，CAQA-AIGC在AvgCAQA方面提高了50%以上。

Abstract: Integrated sensing and communication (ISAC) can enhance artificial
intelligence-generated content (AIGC) networks by providing efficient sensing
and transmission. Existing AIGC services usually assume that the accuracy of
the generated content can be ensured, given accurate input data and prompt,
thus only the content generation quality (CGQ) is concerned. However, it is not
applicable in ISAC-based AIGC networks, where content generation is based on
inaccurate sensed data. Moreover, the AIGC model itself introduces generation
errors, which depend on the number of generating steps (i.e., computing
resources). To assess the quality of experience of ISAC-based AIGC services, we
propose a content accuracy and quality aware service assessment metric (CAQA).
Since allocating more resources to sensing and generating improves content
accuracy but may reduce communication quality, and vice versa, this
sensing-generating (computing)-communication three-dimensional resource
tradeoff must be optimized to maximize the average CAQA (AvgCAQA) across all
users with AIGC (CAQA-AIGC). This problem is NP-hard, with a large solution
space that grows exponentially with users. To solve the CAQA-AIGC problem with
low complexity, a linear programming (LP) guided deep reinforcement learning
(DRL) algorithm with an action filter (LPDRL-F) is proposed. Through the
LP-guided approach and the action filter, LPDRL-F can transform the original
three-dimensional solution space to two dimensions, reducing complexity while
improving the learning performance of DRL. Simulations show that compared to
existing DRL and generative diffusion model algorithms without LP, LPDRL-F
converges faster by over 60% and finds better resource allocation solutions,
improving AvgCAQA by more than 14%. With LPDRL-F, CAQA-AIGC can achieve an
improvement in AvgCAQA of more than 50% compared to existing schemes focusing
solely on CGQ.

</details>


### [182] [Generative Medical Event Models Improve with Scale](https://arxiv.org/abs/2508.12104)
*Shane Waxler,Paul Blazek,Davis White,Daniel Sneider,Kevin Chung,Mani Nagarathnam,Patrick Williams,Hank Voeller,Karen Wong,Matthew Swanhorst,Sheng Zhang,Naoto Usuyama,Cliff Wong,Tristan Naumann,Hoifung Poon,Andrew Loza,Daniella Meeker,Seth Hain,Rahul Shah*

Main category: cs.LG

TL;DR: CoMET是一种生成式医疗事件基础模型，它可以有效地捕捉复杂的临床动态，提供一个可扩展和通用的框架，以支持临床决策，简化医疗保健运营，并改善患者预后。


<details>
  <summary>Details</summary>
Motivation: 在大规模上实现个性化医疗需要从纵向患者历程中提取见解的方法，纵向患者历程可以被视为一系列医疗事件。在大型医疗事件数据上预训练的基础模型代表了扩展真实世界证据生成和推广到各种下游任务的有希望的方向。

Method: decoder-only transformer模型

Result: CoMET通常优于或匹配这些任务上的特定于任务的监督模型，而无需特定于任务的微调或少量示例。

Conclusion: CoMET，一种生成式医疗事件基础模型，可以有效地捕捉复杂的临床动态，提供一个可扩展和通用的框架，以支持临床决策，简化医疗保健运营，并改善患者预后。

Abstract: Realizing personalized medicine at scale calls for methods that distill
insights from longitudinal patient journeys, which can be viewed as a sequence
of medical events. Foundation models pretrained on large-scale medical event
data represent a promising direction for scaling real-world evidence generation
and generalizing to diverse downstream tasks. Using Epic Cosmos, a dataset with
medical events from de-identified longitudinal health records for 16.3 billion
encounters over 300 million unique patient records from 310 health systems, we
introduce the Cosmos Medical Event Transformer ( CoMET) models, a family of
decoder-only transformer models pretrained on 118 million patients representing
115 billion discrete medical events (151 billion tokens). We present the
largest scaling-law study for medical event data, establishing a methodology
for pretraining and revealing power-law scaling relationships for compute,
tokens, and model size. Based on this, we pretrained a series of
compute-optimal models with up to 1 billion parameters. Conditioned on a
patient's real-world history, CoMET autoregressively generates the next medical
event, simulating patient health timelines. We studied 78 real-world tasks,
including diagnosis prediction, disease prognosis, and healthcare operations.
Remarkably for a foundation model with generic pretraining and simulation-based
inference, CoMET generally outperformed or matched task-specific supervised
models on these tasks, without requiring task-specific fine-tuning or few-shot
examples. CoMET's predictive power consistently improves as the model and
pretraining scale. Our results show that CoMET, a generative medical event
foundation model, can effectively capture complex clinical dynamics, providing
an extensible and generalizable framework to support clinical decision-making,
streamline healthcare operations, and improve patient outcomes.

</details>


### [183] [DynamixSFT: Dynamic Mixture Optimization of Instruction Tuning Collections](https://arxiv.org/abs/2508.12116)
*Haebin Shin,Lei Ji,Xiao Liu,Zhiwei Yu,Qi Chen,Yeyun Gong*

Main category: cs.LG

TL;DR: DynamixSFT: a dynamic and automated method for instruction-tuning dataset mixture optimization,achieves up to a 2.2% performance improvement across 10 benchmarks.


<details>
  <summary>Details</summary>
Motivation: Dynamically balancing and optimizing instruction-tuning dataset mixtures has become a critical challenge.

Method: We propose DynamixSFT, a dynamic and automated method for instruction-tuning dataset mixture optimization. We formulate the problem as a multi-armed bandit setup and introduce a Prior-scaled Boltzmann Exploration that softly anchors the updated sampling distribution to the original dataset proportions.

Result: DynamixSFT achieves up to a 2.2% performance improvement across 10 benchmarks. comprehensive analysis and visualizations to offer deeper insights into the adaptive dynamics of our method.

Conclusion: DynamixSFT achieves up to a 2.2% performance improvement across 10 benchmarks on the Tulu-v2-mixture collection.

Abstract: As numerous instruction-tuning datasets continue to emerge during the
post-training stage, dynamically balancing and optimizing their mixtures has
become a critical challenge. To address this, we propose DynamixSFT, a dynamic
and automated method for instruction-tuning dataset mixture optimization. We
formulate the problem as a multi-armed bandit setup and introduce a
Prior-scaled Boltzmann Exploration that softly anchors the updated sampling
distribution to the original dataset proportions, thereby preserving the
inherent diversity and coverage of the collection. Sampling probabilities are
updated using a lightweight 1-Step Look-ahead Reward, reflecting how much the
dataset contributes to improving the model's performance at its current state.
When applied to the Tulu-v2-mixture collection comprising 16 instruction-tuning
datasets, DynamixSFT achieves up to a 2.2% performance improvement across 10
benchmarks. Furthermore, we provide a comprehensive analysis and visualizations
to offer deeper insights into the adaptive dynamics of our method.

</details>


### [184] [Time-Scale Coupling Between States and Parameters in Recurrent Neural Networks](https://arxiv.org/abs/2508.12121)
*Lorenzo Livi*

Main category: cs.LG

TL;DR: 门控机制在 RNN 中诱导自适应学习率行为，从而实现稳健的训练和稳定性。


<details>
  <summary>Details</summary>
Motivation: 研究 RNN 中门控机制如何隐式地诱导自适应学习率行为，即使训练是用固定的全局学习率进行的。

Method: 通过推导 leaky-integrator 和 gated RNN 的精确雅可比矩阵，获得一阶展开，从而明确常数、标量和多维门如何重塑梯度传播，调节有效步长，并在参数更新中引入各向异性。

Result: 揭示了门不仅控制隐藏状态中的记忆保持，还充当数据驱动的预处理器，从而调整参数空间中的优化轨迹。与学习率 schedules、momentum 和自适应方法（如 Adam）进行正式类比，表明这些优化行为自然地从门控中产生。数值实验证实了微扰分析的有效性，支持门控引起的校正保持较小，同时对训练动力学产生系统性影响的观点。

Conclusion: 门控机制能够自然地产生优化行为，实现稳健的训练和稳定性。

Abstract: We study how gating mechanisms in recurrent neural networks (RNNs) implicitly
induce adaptive learning-rate behavior, even when training is carried out with
a fixed, global learning rate. This effect arises from the coupling between
state-space time scales--parametrized by the gates--and parameter-space
dynamics during gradient descent. By deriving exact Jacobians for
leaky-integrator and gated RNNs, we obtain a first-order expansion that makes
explicit how constant, scalar, and multi-dimensional gates reshape gradient
propagation, modulate effective step sizes, and introduce anisotropy in
parameter updates. These findings reveal that gates not only control memory
retention in the hidden states, but also act as data-driven preconditioners
that adapt optimization trajectories in parameter space. We further draw formal
analogies with learning-rate schedules, momentum, and adaptive methods such as
Adam, showing that these optimization behaviors emerge naturally from gating.
Numerical experiments confirm the validity of our perturbative analysis,
supporting the view that gate-induced corrections remain small while exerting
systematic effects on training dynamics. Overall, this work provides a unified
dynamical-systems perspective on how gating couples state evolution with
parameter updates, explaining why gated architectures achieve robust
trainability and stability in practice.

</details>


### [185] [DE-VAE: Revealing Uncertainty in Parametric and Inverse Projections with Variational Autoencoders using Differential Entropy](https://arxiv.org/abs/2508.12145)
*Frederik L. Dennig,Daniel A. Keim*

Main category: cs.LG

TL;DR: DE-VAE是一种不确定性感知变分自编码器，它使用微分熵来改进参数和可逆投影。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在处理数据或嵌入空间中的分布外样本时表现不佳。

Method: 提出了一种使用微分熵(DE)的、具有不确定性意识的变分AE，以改进学习到的参数和可逆投影。

Result: 在四个著名的数据集上进行了定量和定性评估，使用UMAP和t-SNE作为基线投影方法。

Conclusion: DE-VAE可以创建与其他当前基于AE的方法具有可比性的参数和逆投影，同时能够分析嵌入不确定性。

Abstract: Recently, autoencoders (AEs) have gained interest for creating parametric and
invertible projections of multidimensional data. Parametric projections make it
possible to embed new, unseen samples without recalculating the entire
projection, while invertible projections allow the synthesis of new data
instances. However, existing methods perform poorly when dealing with
out-of-distribution samples in either the data or embedding space. Thus, we
propose DE-VAE, an uncertainty-aware variational AE using differential entropy
(DE) to improve the learned parametric and invertible projections. Given a
fixed projection, we train DE-VAE to learn a mapping into 2D space and an
inverse mapping back to the original space. We conduct quantitative and
qualitative evaluations on four well-known datasets, using UMAP and t-SNE as
baseline projection methods. Our findings show that DE-VAE can create
parametric and inverse projections with comparable accuracy to other current
AE-based approaches while enabling the analysis of embedding uncertainty.

</details>


### [186] [AICRN: Attention-Integrated Convolutional Residual Network for Interpretable Electrocardiogram Analysis](https://arxiv.org/abs/2508.12162)
*J. M. I. H. Jayakody,A. M. H. H. Alahakoon,C. R. M. Perera,R. M. L. C. Srimal,Roshan Ragel,Vajira Thambawita,Isuru Nawinne*

Main category: cs.LG

TL;DR: This paper proposes a novel deep learning architecture called AICRN to regress key ECG parameters with higher precision.


<details>
  <summary>Details</summary>
Motivation: improve the diagnostic precision and predictive capacity of cardiac diseases and address traditional analysis challenges, such as loss of focus due to human errors, and facilitates the fast and easy detection of cardiac events, thereby reducing the manual efforts required to solve analysis tasks.

Method: a novel deep learning (DL) architecture called the attention-integrated convolutional residual network (AICRN)

Result: AICRN models outperform existing models in parameter regression with higher precision.

Conclusion: DL can play a crucial role in the interpretability and precision of ECG analysis, opening up new clinical applications for cardiac monitoring and management.

Abstract: The paradigm of electrocardiogram (ECG) analysis has evolved into real-time
digital analysis, facilitated by artificial intelligence (AI) and machine
learning (ML), which has improved the diagnostic precision and predictive
capacity of cardiac diseases. This work proposes a novel deep learning (DL)
architecture called the attention-integrated convolutional residual network
(AICRN) to regress key ECG parameters such as the PR interval, the QT interval,
the QRS duration, the heart rate, the peak amplitude of the R wave, and the
amplitude of the T wave for interpretable ECG analysis. Our architecture is
specially designed with spatial and channel attention-related mechanisms to
address the type and spatial location of the ECG features for regression. The
models employ a convolutional residual network to address vanishing and
exploding gradient problems. The designed system addresses traditional analysis
challenges, such as loss of focus due to human errors, and facilitates the fast
and easy detection of cardiac events, thereby reducing the manual efforts
required to solve analysis tasks. AICRN models outperform existing models in
parameter regression with higher precision. This work demonstrates that DL can
play a crucial role in the interpretability and precision of ECG analysis,
opening up new clinical applications for cardiac monitoring and management.

</details>


### [187] [ProtTeX-CC: Activating In-Context Learning in Protein LLM via Two-Stage Instruction Compression](https://arxiv.org/abs/2508.12212)
*Chuanliu Fan,Zicheng Ma,Jun Gao,Nan Yu,Jun Zhang,Ziqiang Cao,Yi Qin Gao,Guohong Fu*

Main category: cs.LG

TL;DR: ProtTeX-CC, a two-stage compression framework, enhances ProtTeX for few-shot learning by fusing sequence and structure representations and compressing demonstrations, achieving significant compression and performance gains.


<details>
  <summary>Details</summary>
Motivation: Recent advances in protein large language models, such as ProtTeX, represent both side-chain amino acids and backbone structure as discrete token sequences of residue length. While this design enables unified modeling of multimodal protein information, it suffers from two major limitations: (1) The concatenation of sequence and structure tokens approximately doubles the protein length and breaks the intrinsic residue-level alignment between modalities. (2) Constrained by the training corpus and limited context window, ProtTeX is typically trained on single-protein inputs, rendering it incompatible with in-context learning (ICL) and thus limiting its generalization capability.

Method: a lightweight two-stage compression framework designed to enhance ProtTeX under few-shot settings. We first design a joint embedding compression mechanism that fuses sequence and structure representations at the residue level, effectively reducing the protein input length by half without sacrificing performance. Then we propose a self-compression module that aggregates each full demonstration into the latent space of the last few linguistic tokens, reducing the average demonstration length from 751 tokens to less than 16 tokens.

Result: Compared to the original ProtTeX, our self-compression approach achieves a compression ratio of approximately 93.68% in the total prompt length under the 16-shot setting. Without modifying the backbone model, ProtTeX-CC introduces only a small number of additional parameters through PEFT-based tuning in the joint embedding compression stage and a single trainable projection layer in the self-compression stage.

Conclusion: ProtTeX-CC improves performance on the in-domain benchmark by 2%, and generalizes well to the out-of-domain dataset with a performance gain of 11%.

Abstract: Recent advances in protein large language models, such as ProtTeX, represent
both side-chain amino acids and backbone structure as discrete token sequences
of residue length. While this design enables unified modeling of multimodal
protein information, it suffers from two major limitations: (1) The
concatenation of sequence and structure tokens approximately doubles the
protein length and breaks the intrinsic residue-level alignment between
modalities. (2) Constrained by the training corpus and limited context window,
ProtTeX is typically trained on single-protein inputs, rendering it
incompatible with in-context learning (ICL) and thus limiting its
generalization capability. To address these issues, we propose ProtTeX-CC, a
lightweight two-stage compression framework designed to enhance ProtTeX under
few-shot settings. We first design a joint embedding compression mechanism that
fuses sequence and structure representations at the residue level, effectively
reducing the protein input length by half without sacrificing performance. Then
we propose a self-compression module that aggregates each full demonstration
into the latent space of the last few linguistic tokens, reducing the average
demonstration length from 751 tokens to less than 16 tokens. Compared to the
original ProtTeX, our self-compression approach achieves a compression ratio of
approximately 93.68% in the total prompt length under the 16-shot setting.
Without modifying the backbone model, ProtTeX-CC introduces only a small number
of additional parameters through PEFT-based tuning in the joint embedding
compression stage and a single trainable projection layer in the
self-compression stage. Extensive experiments on protein function prediction
show that ProtTeX-CC improves performance on the in-domain benchmark by 2%, and
generalizes well to the out-of-domain dataset with a performance gain of 11%.

</details>


### [188] [Unlearning at Scale: Implementing the Right to be Forgotten in Large Language Models](https://arxiv.org/abs/2508.12220)
*Abdullah X*

Main category: cs.LG

TL;DR: We propose a system for unlearning in large language models by replaying the training tail while filtering the forget closure, and demonstrate byte-identical equality of model and optimizer states.


<details>
  <summary>Details</summary>
Motivation: We study the right to be forgotten (GDPR Art. 17) for large language models and frame unlearning as a reproducible systems problem.

Method: Our approach treats training as a deterministic program and logs a minimal per-microbatch record. We add complementary paths: (i) exact reverts of recent steps via micro-checkpoints or dense per-step deltas, (ii) cohort-scoped adapter deletion, and (iii) a curvature-guided anti-update followed by a short retain-tune.

Result: We report storage/latency budgets and a toy artifact validating mechanics.

Conclusion: We demonstrate byte-identical equality of model and optimizer states under controlled conditions.

Abstract: We study the right to be forgotten (GDPR Art. 17) for large language models
and frame unlearning as a reproducible systems problem. Our approach treats
training as a deterministic program and logs a minimal per-microbatch record
(ordered ID hash, RNG seed, learning-rate value, optimizer-step counter, and
accumulation boundary). Under a pinned stack and deterministic kernels,
replaying the training tail while filtering only the forget closure yields the
same parameters as training on the retain set (bit-identical in the training
dtype) when preconditions hold. To meet latency and availability constraints,
we add complementary paths: (i) exact reverts of recent steps via
micro-checkpoints or dense per-step deltas, (ii) cohort-scoped adapter deletion
when the base is frozen, and (iii) a curvature-guided anti-update followed by a
short retain-tune, audit-gated with escalation to exact replay. We report
storage/latency budgets and a toy artifact validating mechanics; in a
controlled run that satisfies the preconditions we demonstrate byte-identical
equality of model and optimizer states.

</details>


### [189] [Distribution Matching via Generalized Consistency Models](https://arxiv.org/abs/2508.12222)
*Sagar Shrestha,Rajesh Shrestha,Tri Nguyen,Subash Timilsina*

Main category: cs.LG

TL;DR: Proposes a new distribution matching method based on CNF consistency models to address GAN training challenges, validated theoretically and experimentally.


<details>
  <summary>Details</summary>
Motivation: GANs often encounter challenge in training due to their bi-level min-max optimization objective and susceptibility to mode collapse.

Method: A novel approach for distribution matching inspired by the consistency models employed in Continuous Normalizing Flow (CNF).

Result: Theoretical validation and demonstration of performance through experiments on synthetic and real-world datasets.

Conclusion: This paper proposes a novel approach for distribution matching inspired by consistency models in CNF, inheriting the advantages of CNF models while remaining adaptable to different constraints similar to GANs. The approach is validated theoretically and experimentally.

Abstract: Recent advancement in generative models have demonstrated remarkable
performance across various data modalities. Beyond their typical use in data
synthesis, these models play a crucial role in distribution matching tasks such
as latent variable modeling, domain translation, and domain adaptation.
Generative Adversarial Networks (GANs) have emerged as the preferred method of
distribution matching due to their efficacy in handling high-dimensional data
and their flexibility in accommodating various constraints. However, GANs often
encounter challenge in training due to their bi-level min-max optimization
objective and susceptibility to mode collapse. In this work, we propose a novel
approach for distribution matching inspired by the consistency models employed
in Continuous Normalizing Flow (CNF). Our model inherits the advantages of CNF
models, such as having a straight forward norm minimization objective, while
remaining adaptable to different constraints similar to GANs. We provide
theoretical validation of our proposed objective and demonstrate its
performance through experiments on synthetic and real-world datasets.

</details>


### [190] [Communication-Efficient Distributed Asynchronous ADMM](https://arxiv.org/abs/2508.12233)
*Sagar Shrestha*

Main category: cs.LG

TL;DR: 提出粗量化异步ADMM，以减少大规模联邦学习和分布式优化应用的通信开销。


<details>
  <summary>Details</summary>
Motivation: 在分布式优化和联邦学习中，异步交替方向乘子法（ADMM）是大规模优化、数据隐私、落后节点和各种目标函数的有吸引力的选择。然而，当节点具有有限的通信预算或当要通信的数据非常大时，通信成本可能成为主要的瓶颈。

Method: 在异步ADMM中引入粗量化到要交换的数据，以减少大规模联邦学习和分布式优化应用的通信开销。

Result: 提出了在异步ADMM中引入粗量化到要交换的数据，以减少大规模联邦学习和分布式优化应用的通信开销。

Conclusion: 验证了所提方法在包括神经网络在内的多个分布式学习任务中的收敛性。

Abstract: In distributed optimization and federated learning, asynchronous alternating
direction method of multipliers (ADMM) serves as an attractive option for
large-scale optimization, data privacy, straggler nodes and variety of
objective functions. However, communication costs can become a major bottleneck
when the nodes have limited communication budgets or when the data to be
communicated is prohibitively large. In this work, we propose introducing
coarse quantization to the data to be exchanged in aynchronous ADMM so as to
reduce communication overhead for large-scale federated learning and
distributed optimization applications. We experimentally verify the convergence
of the proposed method for several distributed learning tasks, including neural
networks.

</details>


### [191] [CC-Time: Cross-Model and Cross-Modality Time Series Forecasting](https://arxiv.org/abs/2508.12235)
*Peng Chen,Yihang Wang,Yang Shu,Yunyao Cheng,Kai Zhao,Zhongwen Rao,Lujia Pan,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: CC-Time incorporates cross-modality learning to model temporal dependency and channel correlations in the language model from both time series sequences and their corresponding text descriptions.CC-Time further proposes the cross-model fusion block to adaptively integrate knowledge from the PLMs and time series model to form a more comprehensive modeling of time series patterns.


<details>
  <summary>Details</summary>
Motivation: Current PLM-based TSF methods still fail to achieve satisfactory prediction accuracy matching the strong sequential modeling power of language models.

Method: Cross-Model and Cross-Modality Learning with PLMs for time series forecasting (CC-Time)

Result: achieves state-of-the-art prediction accuracy in both full-data training and few-shot learning situations.

Conclusion: CC-Time achieves state-of-the-art prediction accuracy in both full-data training and few-shot learning situations.

Abstract: With the success of pre-trained language models (PLMs) in various application
fields beyond natural language processing, language models have raised emerging
attention in the field of time series forecasting (TSF) and have shown great
prospects. However, current PLM-based TSF methods still fail to achieve
satisfactory prediction accuracy matching the strong sequential modeling power
of language models. To address this issue, we propose Cross-Model and
Cross-Modality Learning with PLMs for time series forecasting (CC-Time). We
explore the potential of PLMs for time series forecasting from two aspects: 1)
what time series features could be modeled by PLMs, and 2) whether relying
solely on PLMs is sufficient for building time series models. In the first
aspect, CC-Time incorporates cross-modality learning to model temporal
dependency and channel correlations in the language model from both time series
sequences and their corresponding text descriptions. In the second aspect,
CC-Time further proposes the cross-model fusion block to adaptively integrate
knowledge from the PLMs and time series model to form a more comprehensive
modeling of time series patterns. Extensive experiments on nine real-world
datasets demonstrate that CC-Time achieves state-of-the-art prediction accuracy
in both full-data training and few-shot learning situations.

</details>


### [192] [DHG-Bench: A Comprehensive Benchmark on Deep Hypergraph Learning](https://arxiv.org/abs/2508.12244)
*Fan Li,Xiaoyang Wang,Wenjie Zhang,Ying Zhang,Xuemin Lin*

Main category: cs.LG

TL;DR: DHG-Bench is introduced as the first comprehensive benchmark for deep hypergraph learning, addressing limitations in existing HNN evaluation and providing insights for future research.


<details>
  <summary>Details</summary>
Motivation: Conventional deep graph models are limited in learning higher-order interactions in real-world systems, and there is no comprehensive benchmark for HNNs.

Method: Introduction of DHG-Bench, a comprehensive benchmark for DHGL, integrating diverse datasets, HNN algorithms, consistent data processing, and experimental protocols.

Result: Systematic investigation of HNN characteristics in terms of effectiveness, efficiency, robustness, and fairness. Development of an easy-to-use library for training and evaluation.

Conclusion: DHG-Bench reveals strengths and limitations of existing HNN algorithms, offering insights for future research.

Abstract: Although conventional deep graph models have achieved great success in
relational learning, their focus on pairwise relationships limits their
capacity to learn pervasive higher-order interactions in real-world complex
systems, which can be naturally modeled as hypergraphs. To tackle this,
hypergraph neural networks (HNNs), the dominant approach in deep hypergraph
learning (DHGL), has garnered substantial attention in recent years. Despite
the proposal of numerous HNN methods, there is no comprehensive benchmark for
HNNs, which creates a great obstacle to understanding the progress of DHGL in
several aspects: (i) insufficient coverage of datasets, algorithms, and tasks;
(ii) a narrow evaluation of algorithm performance; and (iii) inconsistent
dataset usage, preprocessing, and experimental setups that hinder
comparability. To fill the gap, we introduce DHG-Bench, the first comprehensive
benchmark for DHGL. Specifically, DHG-Bench integrates 20 diverse datasets
spanning node-, edge-, and graph-level tasks, along with 16 state-of-the-art
HNN algorithms, under consistent data processing and experimental protocols.
Our benchmark systematically investigates the characteristics of HNNs in terms
of four dimensions: effectiveness, efficiency, robustness, and fairness.
Further, to facilitate reproducible research, we have developed an easy-to-use
library for training and evaluating different HNN methods. Extensive
experiments conducted with DHG-Bench reveal both the strengths and inherent
limitations of existing algorithms, offering valuable insights and directions
for future research. The code is publicly available at:
https://github.com/Coco-Hut/DHG-Bench.

</details>


### [193] [STM3: Mixture of Multiscale Mamba for Long-Term Spatio-Temporal Time-Series Prediction](https://arxiv.org/abs/2508.12247)
*Haolong Chen,Liang Zhang,Zhengyuan Xin,Guangxu Zhu*

Main category: cs.LG

TL;DR: Proposes STM2 and STM3, which use multiscale Mamba architectures and adaptive graph causal convolution networks to efficiently capture complex long-term spatio-temporal dependencies, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods struggle with learning complex long-term spatio-temporal dependencies efficiently. The long-term spatio-temporal dependency learning brings two new challenges: 1) The long-term temporal sequence includes multiscale information naturally which is hard to extract efficiently; 2) The multiscale temporal information from different nodes is highly correlated and hard to model.

Method: STM2 includes a multiscale Mamba architecture and an adaptive graph causal convolution network. STM3 employs a special Mixture-of-Experts architecture, including a more stable routing strategy and a causal contrastive learning strategy.

Result: STM2/STM3's superior performance, achieving state-of-the-art results in long-term spatio-temporal time-series prediction on real-world benchmarks.

Conclusion: STM2/STM3 achieves state-of-the-art results in long-term spatio-temporal time-series prediction.

Abstract: Recently, spatio-temporal time-series prediction has developed rapidly, yet
existing deep learning methods struggle with learning complex long-term
spatio-temporal dependencies efficiently. The long-term spatio-temporal
dependency learning brings two new challenges: 1) The long-term temporal
sequence includes multiscale information naturally which is hard to extract
efficiently; 2) The multiscale temporal information from different nodes is
highly correlated and hard to model. To address these challenges, we propose an
efficient \textit{\textbf{S}patio-\textbf{T}emporal \textbf{M}ultiscale
\textbf{M}amba} (STM2) that includes a multiscale Mamba architecture to capture
the multiscale information efficiently and simultaneously, and an adaptive
graph causal convolution network to learn the complex multiscale
spatio-temporal dependency. STM2 includes hierarchical information aggregation
for different-scale information that guarantees their distinguishability. To
capture diverse temporal dynamics across all spatial nodes more efficiently, we
further propose an enhanced version termed
\textit{\textbf{S}patio-\textbf{T}emporal \textbf{M}ixture of
\textbf{M}ultiscale \textbf{M}amba} (STM3) that employs a special
Mixture-of-Experts architecture, including a more stable routing strategy and a
causal contrastive learning strategy to enhance the scale distinguishability.
We prove that STM3 has much better routing smoothness and guarantees the
pattern disentanglement for each expert successfully. Extensive experiments on
real-world benchmarks demonstrate STM2/STM3's superior performance, achieving
state-of-the-art results in long-term spatio-temporal time-series prediction.

</details>


### [194] [Interpreting Time Series Forecasts with LIME and SHAP: A Case Study on the Air Passengers Dataset](https://arxiv.org/abs/2508.12253)
*Manish Shukla*

Main category: cs.LG

TL;DR: Interpreting time-series forecasts using LIME and SHAP


<details>
  <summary>Details</summary>
Motivation: Classical ARIMA models offer interpretability via coefficients but struggle with nonlinearities, whereas tree-based machine-learning models such as XGBoost deliver high accuracy but are often opaque

Method: unified framework for interpreting time-series forecasts using LIME and SHAP. convert a univariate series into a leakage-free supervised learning problem,train a gradient-boosted tree alongside an ARIMA baseline and apply post-hoc explainability

Result: apply LIME and SHAP to time series without violating chronology, theoretical exposition of the underlying algorithms, empirical evaluation with extensive analysis, and guidelines for practitioners

Conclusion: lagged features and seasonal encodings explain most forecast variance

Abstract: Time-series forecasting underpins critical decisions across aviation, energy,
retail and health. Classical autoregressive integrated moving average (ARIMA)
models offer interpretability via coefficients but struggle with
nonlinearities, whereas tree-based machine-learning models such as XGBoost
deliver high accuracy but are often opaque. This paper presents a unified
framework for interpreting time-series forecasts using local interpretable
model-agnostic explanations (LIME) and SHapley additive exPlanations (SHAP). We
convert a univariate series into a leakage-free supervised learning problem,
train a gradient-boosted tree alongside an ARIMA baseline and apply post-hoc
explainability. Using the Air Passengers dataset as a case study, we show that
a small set of lagged features -- particularly the twelve-month lag -- and
seasonal encodings explain most forecast variance. We contribute: (i) a
methodology for applying LIME and SHAP to time series without violating
chronology; (ii) theoretical exposition of the underlying algorithms; (iii)
empirical evaluation with extensive analysis; and (iv) guidelines for
practitioners.

</details>


### [195] [L-SR1: Learned Symmetric-Rank-One Preconditioning](https://arxiv.org/abs/2508.12270)
*Gal Lifshitz,Shahar Zuler,Ori Fouks,Dan Raviv*

Main category: cs.LG

TL;DR: 提出了一种新的学习二阶优化器，在单目人体网格恢复 (HMR) 任务中表现出色，且具有良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 端到端深度学习取得了令人瞩目的成果，但仍受到其对大型标记数据集的依赖性、对未见场景的泛化能力差以及不断增长的计算需求的限制。相比之下，经典的优化方法是数据高效且轻量级的，但通常存在收敛速度慢的问题。虽然学习优化器提供了两个世界的有希望的融合，但大多数都集中在一阶方法上，而学习二阶方法在很大程度上仍未被探索。

Method: 提出了一种新的学习二阶优化器，该优化器引入了一个可训练的预处理单元，以增强经典的对称秩 1 (SR1) 算法。该单元生成数据驱动的向量，用于构建正半定秩 1 矩阵，并通过学习投影与割线约束对齐。

Result: 该方法优于现有的基于学习的优化方法。该方法具有轻量级模型，不需要带注释的数据或微调，具有很强的泛化能力，非常适合集成到更广泛的基于优化的框架中。

Conclusion: 该方法在单目人体网格恢复 (HMR) 这一真实世界的任务中，优于现有的基于学习的优化方法。该方法具有轻量级模型，不需要带注释的数据或微调，具有很强的泛化能力，非常适合集成到更广泛的基于优化的框架中。

Abstract: End-to-end deep learning has achieved impressive results but remains limited
by its reliance on large labeled datasets, poor generalization to unseen
scenarios, and growing computational demands. In contrast, classical
optimization methods are data-efficient and lightweight but often suffer from
slow convergence. While learned optimizers offer a promising fusion of both
worlds, most focus on first-order methods, leaving learned second-order
approaches largely unexplored.
  We propose a novel learned second-order optimizer that introduces a trainable
preconditioning unit to enhance the classical Symmetric-Rank-One (SR1)
algorithm. This unit generates data-driven vectors used to construct positive
semi-definite rank-one matrices, aligned with the secant constraint via a
learned projection. Our method is evaluated through analytic experiments and on
the real-world task of Monocular Human Mesh Recovery (HMR), where it
outperforms existing learned optimization-based approaches. Featuring a
lightweight model and requiring no annotated data or fine-tuning, our approach
offers strong generalization and is well-suited for integration into broader
optimization-based frameworks.

</details>


### [196] [CRoC: Context Refactoring Contrast for Graph Anomaly Detection with Limited Supervision](https://arxiv.org/abs/2508.12278)
*Siyue Xie,Da Sun Handason Tam,Wing Cheong Lau*

Main category: cs.LG

TL;DR: CRoC是一种用于图异常检测的框架，它利用有限的标记和丰富的未标记数据来训练GNN，通过重构节点上下文和集成异构关系来提高性能。


<details>
  <summary>Details</summary>
Motivation: 训练鲁棒的GNN通常需要大量的标记数据，这是实际应用中的一个关键瓶颈。这种限制严重阻碍了图异常检测(GAD)的进展，因为异常本质上是罕见的，标记成本高昂，并且可能积极地伪装它们的模式以逃避检测。为了解决这些问题

Method: 我们提出了上下文重构对比(CRoC)，这是一个简单而有效的框架，它通过共同利用有限的标记数据和丰富的未标记数据来训练用于GAD的GNN。CRoC利用GAD中固有的类不平衡来重构每个节点的上下文，通过重新组合节点的属性同时保持它们的交互模式来构建增强图。此外，CRoC分别编码异构关系，并将它们集成到消息传递过程中，从而增强模型捕获复杂交互语义的能力。在训练阶段，CRoC进一步与对比学习范式集成。这使得GNN能够在联合训练期间有效地利用未标记数据，从而产生更丰富、更具区分性的节点嵌入。

Result: CRoC实现了高达14%的AUC改进

Conclusion: CRoC在七个真实世界的GAD数据集上进行了评估，实验表明，在有限标签设置下，CRoC比基线GNN实现了高达14%的AUC改进，并且优于最先进的GAD方法。

Abstract: Graph Neural Networks (GNNs) are widely used as the engine for various
graph-related tasks, with their effectiveness in analyzing graph-structured
data. However, training robust GNNs often demands abundant labeled data, which
is a critical bottleneck in real-world applications. This limitation severely
impedes progress in Graph Anomaly Detection (GAD), where anomalies are
inherently rare, costly to label, and may actively camouflage their patterns to
evade detection. To address these problems, we propose Context Refactoring
Contrast (CRoC), a simple yet effective framework that trains GNNs for GAD by
jointly leveraging limited labeled and abundant unlabeled data. Different from
previous works, CRoC exploits the class imbalance inherent in GAD to refactor
the context of each node, which builds augmented graphs by recomposing the
attributes of nodes while preserving their interaction patterns. Furthermore,
CRoC encodes heterogeneous relations separately and integrates them into the
message-passing process, enhancing the model's capacity to capture complex
interaction semantics. These operations preserve node semantics while
encouraging robustness to adversarial camouflage, enabling GNNs to uncover
intricate anomalous cases. In the training stage, CRoC is further integrated
with the contrastive learning paradigm. This allows GNNs to effectively harness
unlabeled data during joint training, producing richer, more discriminative
node embeddings. CRoC is evaluated on seven real-world GAD datasets with
varying scales. Extensive experiments demonstrate that CRoC achieves up to 14%
AUC improvement over baseline GNNs and outperforms state-of-the-art GAD methods
under limited-label settings.

</details>


### [197] [Convergence Analysis of the Lion Optimizer in Centralized and Distributed Settings](https://arxiv.org/abs/2508.12327)
*Wei Jiang,Lijun Zhang*

Main category: cs.LG

TL;DR: 本文研究了Lion优化器的收敛性，提出了改进版本，并在分布式环境中进行了分析。


<details>
  <summary>Details</summary>
Motivation: 旨在提高Lion优化器的收敛速度。

Method: 提出了带有方差减少的Lion优化器以及通信高效的分布式Lion变体。

Result: 在标准假设下，Lion优化器实现了O(d^{1/2}T^{-1/4})的收敛速度。带有方差减少的Lion优化器实现了O(d^{1/2}T^{-1/3})的收敛速度。通信高效的分布式Lion变体实现了O(max{d^{1/4}/T^{1/4}, d^{1/10}/(n^{1/5}T^{1/5})})和O(d^{1/4}/T^{1/4})的收敛速度。

Conclusion: 这篇论文分析了Lion优化器的收敛性质，并在分布式环境中进行了扩展。

Abstract: In this paper, we analyze the convergence properties of the Lion optimizer.
First, we establish that the Lion optimizer attains a convergence rate of
$\mathcal{O}(d^{1/2}T^{-1/4})$ under standard assumptions, where $d$ denotes
the problem dimension and $T$ is the iteration number. To further improve this
rate, we introduce the Lion optimizer with variance reduction, resulting in an
enhanced convergence rate of $\mathcal{O}(d^{1/2}T^{-1/3})$. We then analyze in
distributed settings, where the standard and variance reduced version of the
distributed Lion can obtain the convergence rates of
$\mathcal{O}(d^{1/2}(nT)^{-1/4})$ and $\mathcal{O}(d^{1/2}(nT)^{-1/3})$, with
$n$ denoting the number of nodes. Furthermore, we investigate a
communication-efficient variant of the distributed Lion that ensures sign
compression in both communication directions. By employing the unbiased sign
operations, the proposed Lion variant and its variance reduction counterpart,
achieve convergence rates of $\mathcal{O}\left( \max
\left\{\frac{d^{1/4}}{T^{1/4}}, \frac{d^{1/10}}{n^{1/5}T^{1/5}} \right\}
\right)$ and $\mathcal{O}\left( \frac{d^{1/4}}{T^{1/4}} \right)$, respectively.

</details>
