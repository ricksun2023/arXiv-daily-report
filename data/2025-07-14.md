<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [RepeaTTS: Towards Feature Discovery through Repeated Fine-Tuning](https://arxiv.org/abs/2507.08012)
*Atli Sigurgeirsson,Simon King*

Main category: cs.CL

TL;DR: 通过利用模型固有的不可控方差，该研究提出了一种新颖的微调方案，以提高基于提示的文本到语音模型的可控性。


<details>
  <summary>Details</summary>
Motivation: 基于提示的文本到语音模型允许用户通过自然语言指令控制语音的不同方面，例如语速和感知性别。虽然用户友好，但这种方法一方面受到限制：控制仅限于训练期间暴露给模型的声学特征，另一方面过于灵活：相同的输入会产生反映在语料库统计数据中的不可控变化。

Method: 利用模型不可控的方差，通过主成分分析确定潜在特征，并将其作为二次微调的新标签。

Result: 在冰岛语语音语料库上训练的两个模型（一个具有情感披露，一个没有情感披露）上评估了所提出的方法。在没有情感披露的模型的情况下，该方法产生连续和离散的特征，提高了模型的整体可控性。

Conclusion: 通过对数千个合成样本的主成分分析，我们确定了潜在特征，这些特征解释了输出方差的最大比例，并将它们作为二次微调的新标签。对于没有情感披露的模型，该方法产生连续和离散的特征，提高了模型的整体可控性。

Abstract: A Prompt-based Text-To-Speech model allows a user to control different
aspects of speech, such as speaking rate and perceived gender, through natural
language instruction. Although user-friendly, such approaches are on one hand
constrained: control is limited to acoustic features exposed to the model
during training, and too flexible on the other: the same inputs yields
uncontrollable variation that are reflected in the corpus statistics.
  We investigate a novel fine-tuning regime to address both of these issues at
the same time by exploiting the uncontrollable variance of the model. Through
principal component analysis of thousands of synthesised samples, we determine
latent features that account for the highest proportion of the output variance
and incorporate them as new labels for secondary fine-tuning. We evaluate the
proposed methods on two models trained on an expressive Icelandic speech
corpus, one with emotional disclosure and one without. In the case of the model
without emotional disclosure, the method yields both continuous and discrete
features that improve overall controllability of the model.

</details>
