{"id": "2506.21656", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21656", "abs": "https://arxiv.org/abs/2506.21656", "authors": ["Yifan Shen", "Yuanzhe Liu", "Jingyuan Zhu", "Xu Cao", "Xiaofeng Zhang", "Yixiao He", "Wenming Ye", "James Matthew Rehg", "Ismini Lourentzou"], "title": "Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs", "comment": "29 pages", "summary": "Current Vision-Language Models (VLMs) struggle with fine-grained spatial\nreasoning, particularly when multi-step logic and precise spatial alignment are\nrequired. In this work, we introduce SpatialReasoner-R1, a vision-language\nreasoning model designed to address these limitations. To construct\nhigh-quality supervision for spatial reasoning, we design a Multi-Model Monte\nCarlo Tree Search (M3CTS) method that generates diverse, logically consistent\nLong Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose\nfine-grained Direct Preference Optimization (fDPO), which introduces\nsegment-specific preference granularity for descriptive grounding and logical\nreasoning, guided by a spatial reward mechanism that evaluates candidate\nresponses based on visual consistency, spatial grounding, and logical\ncoherence. Experimental results demonstrate that fDPO achieves an average\nimprovement of 4.1% over standard DPO across spatial quality tasks, and a 9.0%\ngain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a\nnew SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in\naverage accuracy, while maintaining competitive performance on general\nvision-language tasks.", "AI": {"tldr": "SpatialReasoner-R1\u662f\u4e00\u79cd\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u6a21\u578b\uff0c\u5b83\u4f7f\u7528M3CTS\u548cfDPO\u6765\u63d0\u9ad8\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728SPATIALRGPT-Bench\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684SoTA\u3002", "motivation": "\u5f53\u524d\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u7ec6\u7c92\u5ea6\u7684\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u591a\u6b65\u903b\u8f91\u548c\u7cbe\u786e\u7a7a\u95f4\u5bf9\u9f50\u65f6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u6a21\u578b\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08M3CTS\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u591a\u6837\u4e14\u903b\u8f91\u4e0a\u4e00\u81f4\u7684\u957f\u94fe\u601d\u7ef4\uff08LongCoT\uff09\u63a8\u7406\u8f68\u8ff9\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u7ec6\u7c92\u5ea6\u7684\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08fDPO\uff09\u3002", "result": "fDPO\u5728\u7a7a\u95f4\u8d28\u91cf\u4efb\u52a1\u4e0a\u7684\u5e73\u5747\u6539\u8fdb\u4e3a4.1%\uff0c\u5728\u7a7a\u95f4\u6570\u91cf\u4efb\u52a1\u4e0a\u7684\u5e73\u5747\u6539\u8fdb\u4e3a9.0%\u3002", "conclusion": "SpatialReasoner-R1\u5728SPATIALRGPT-Bench\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684SoTA\uff0c\u5e73\u5747\u51c6\u786e\u7387\u6bd4\u6700\u5f3a\u7684\u57fa\u7ebf\u9ad8\u51fa9.8%\uff0c\u5e76\u5728\u4e00\u822c\u7684\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e0a\u4fdd\u6301\u4e86\u7ade\u4e89\u6027\u7684\u8868\u73b0\u3002"}}
{"id": "2506.21681", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21681", "abs": "https://arxiv.org/abs/2506.21681", "authors": ["Hakan \u00c7apuk", "Andrew Bond", "Muhammed Burak K\u0131z\u0131l", "Emir G\u00f6\u00e7en", "Erkut Erdem", "Aykut Erdem"], "title": "TanDiT: Tangent-Plane Diffusion Transformer for High-Quality 360\u00b0 Panorama Generation", "comment": null, "summary": "Recent advances in image generation have led to remarkable improvements in\nsynthesizing perspective images. However, these models still struggle with\npanoramic image generation due to unique challenges, including varying levels\nof geometric distortion and the requirement for seamless loop-consistency. To\naddress these issues while leveraging the strengths of the existing models, we\nintroduce TanDiT, a method that synthesizes panoramic scenes by generating\ngrids of tangent-plane images covering the entire 360$^\\circ$ view. Unlike\nprevious methods relying on multiple diffusion branches, TanDiT utilizes a\nunified diffusion model trained to produce these tangent-plane images\nsimultaneously within a single denoising iteration. Furthermore, we propose a\nmodel-agnostic post-processing step specifically designed to enhance global\ncoherence across the generated panoramas. To accurately assess panoramic image\nquality, we also present two specialized metrics, TangentIS and TangentFID, and\nprovide a comprehensive benchmark comprising captioned panoramic datasets and\nstandardized evaluation scripts. Extensive experiments demonstrate that our\nmethod generalizes effectively beyond its training data, robustly interprets\ndetailed and complex text prompts, and seamlessly integrates with various\ngenerative models to yield high-quality, diverse panoramic images.", "AI": {"tldr": "introduce TanDiT, a method that synthesizes panoramic scenes by generating grids of tangent-plane images covering the entire 360$\\circ$ view", "motivation": "these models still struggle with panoramic image generation due to unique challenges, including varying levels of geometric distortion and the requirement for seamless loop-consistency. To address these issues while leveraging the strengths of the existing models", "method": "a method that synthesizes panoramic scenes by generating grids of tangent-plane images covering the entire 360$^\\circ$ view. Unlike previous methods relying on multiple diffusion branches, TanDiT utilizes a unified diffusion model trained to produce these tangent-plane images simultaneously within a single denoising iteration. Furthermore, we propose a model-agnostic post-processing step specifically designed to enhance global coherence across the generated panoramas.", "result": "To accurately assess panoramic image quality, we also present two specialized metrics, TangentIS and TangentFID, and provide a comprehensive benchmark comprising captioned panoramic datasets and standardized evaluation scripts.", "conclusion": "The method generalizes effectively beyond its training data, robustly interprets detailed and complex text prompts, and seamlessly integrates with various generative models to yield high-quality, diverse panoramic images."}}
{"id": "2506.21710", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21710", "abs": "https://arxiv.org/abs/2506.21710", "authors": ["Liangyu Zhong", "Fabio Rosenthal", "Joachim Sicking", "Fabian H\u00fcger", "Thorsten Bagdonat", "Hanno Gottschalk", "Leo Schwinn"], "title": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual Question Answering", "comment": "Preprint. Under review", "summary": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and two types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute.", "AI": {"tldr": "This paper introduces FOCUS, a training-free visual cropping method that uses MLLM-internal representations to find the most relevant image region for fine-grained VQA tasks. FOCUS achieves strong performance with less computation compared to existing methods.", "motivation": "Visual Question Answering (VQA) focusing on small image details still remains a challenge. Although visual cropping techniques seem promising, recent approaches have several limitations: the need for task-specific fine-tuning, low efficiency due to uninformed exhaustive search, or incompatibility with efficient attention implementations.", "method": "a training-free visual cropping method, dubbed FOCUS, that leverages MLLM-internal representations to guide the search for the most relevant image region. This is accomplished in four steps: first, we identify the target object(s) in the VQA prompt; second, we compute an object relevance map using the key-value (KV) cache; third, we propose and rank relevant image regions based on the map; and finally, we perform the fine-grained VQA task using the top-ranked region.", "result": "FOCUS outperforms three popular visual cropping methods in both accuracy and efficiency, and matches the best-performing baseline, ZoomEye, while requiring 3 - 6.5 x less compute.", "conclusion": "FOCUS achieves strong performance across four fine-grained VQA datasets and two types of MLLMs. It outperforms three popular visual cropping methods in both accuracy and efficiency, and matches the best-performing baseline, ZoomEye, while requiring 3 - 6.5 x less compute."}}
{"id": "2506.21711", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21711", "abs": "https://arxiv.org/abs/2506.21711", "authors": ["Aryan Thakre", "Omkar Nagwekar", "Vedang Talekar", "Aparna Santra Biswas"], "title": "CAST: Cross-Attentive Spatio-Temporal feature fusion for Deepfake detection", "comment": "50 pages, 6 figures", "summary": "Deepfakes have emerged as a significant threat to digital media authenticity,\nincreasing the need for advanced detection techniques that can identify subtle\nand time-dependent manipulations. CNNs are effective at capturing spatial\nartifacts, and Transformers excel at modeling temporal inconsistencies.\nHowever, many existing CNN-Transformer models process spatial and temporal\nfeatures independently. In particular, attention-based methods often use\nseparate attention mechanisms for spatial and temporal features and combine\nthem using naive approaches like averaging, addition, or concatenation, which\nlimits the depth of spatio-temporal interaction. To address this challenge, we\npropose a unified CAST model that leverages cross-attention to effectively fuse\nspatial and temporal features in a more integrated manner. Our approach allows\ntemporal features to dynamically attend to relevant spatial regions, enhancing\nthe model's ability to detect fine-grained, time-evolving artifacts such as\nflickering eyes or warped lips. This design enables more precise localization\nand deeper contextual understanding, leading to improved performance across\ndiverse and challenging scenarios. We evaluate the performance of our model\nusing the FaceForensics++, Celeb-DF, and DeepfakeDetection datasets in both\nintra- and cross-dataset settings to affirm the superiority of our approach.\nOur model achieves strong performance with an AUC of 99.49 percent and an\naccuracy of 97.57 percent in intra-dataset evaluations. In cross-dataset\ntesting, it demonstrates impressive generalization by achieving a 93.31 percent\nAUC on the unseen DeepfakeDetection dataset. These results highlight the\neffectiveness of cross-attention-based feature fusion in enhancing the\nrobustness of deepfake video detection.", "AI": {"tldr": "The paper proposes a cross-attention-based CNN-Transformer model (CAST) for deepfake detection that improves spatio-temporal feature fusion, achieving state-of-the-art results.", "motivation": "Existing CNN-Transformer models process spatial and temporal features independently, limiting the depth of spatio-temporal interaction.", "method": "A unified CAST model that leverages cross-attention to fuse spatial and temporal features in an integrated manner.", "result": "The model achieves an AUC of 99.49% and an accuracy of 97.57% in intra-dataset evaluations and a 93.31% AUC on the unseen DeepfakeDetection dataset in cross-dataset testing.", "conclusion": "The proposed CAST model demonstrates strong performance in deepfake video detection, achieving high AUC and accuracy in both intra- and cross-dataset evaluations, highlighting the effectiveness of cross-attention-based feature fusion."}}
{"id": "2506.21555", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.21555", "abs": "https://arxiv.org/abs/2506.21555", "authors": ["Jiahong Li", "Yiwen Shao", "Jianheng Zhuo", "Chenda Li", "Liliang Tang", "Dong Yu", "Yanmin Qian"], "title": "Efficient Multilingual ASR Finetuning via LoRA Language Experts", "comment": "Accepted in Interspeech 2025", "summary": "Recent advancements in deep learning have significantly enhanced multilingual\nautomatic speech recognition (ASR) due to the development of advanced model\narchitectures and available large-scale multilingual datasets. Despite that,\nmultilingual ASR still suffers from the curse of multilinguality in that\ndifferent languages tend to interfere with each other, making it difficult for\nthe ASR model to identify multiple languages effectively while sharing model\ncapacity across them. This paper proposes an efficient finetuning framework for\ncustomized multilingual ASR via prepared LoRA language experts based on\nWhisper. Through LoRA expert fusion or knowledge distillation, our approach\nachieves better recognition performance on target languages than standard\nfine-tuning methods. Experimental results demonstrate that the proposed models\nyield approximately 10\\% and 15\\% relative performance gains in language-aware\nand language-agnostic scenarios, respectively.", "AI": {"tldr": "This paper introduces a LoRA-based finetuning method for multilingual ASR that improves performance by 10-15% compared to standard finetuning.", "motivation": "Multilingual ASR suffers from the curse of multilinguality, where different languages interfere with each other.", "method": "LoRA language experts based on Whisper, using LoRA expert fusion or knowledge distillation.", "result": "Approximately 10% and 15% relative performance gains in language-aware and language-agnostic scenarios, respectively.", "conclusion": "The proposed LoRA-based finetuning framework achieves better recognition performance on target languages than standard fine-tuning methods."}}
{"id": "2506.21669", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21669", "abs": "https://arxiv.org/abs/2506.21669", "authors": ["Wanxin Tian", "Shijie Zhang", "Kevin Zhang", "Xiaowei Chi", "Yulin Luo", "Junyu Lu", "Chunkai Fan", "Qiang Zhou", "Yiming Zhao", "Ning Liu Siyu Lin", "Zhiyuan Qin", "Xiaozhu Ju", "Shanghang Zhang", "Jian Tang"], "title": "SEEA-R1: Tree-Structured Reinforcement Fine-Tuning for Self-Evolving Embodied Agents", "comment": null, "summary": "Self-evolution, the ability of agents to autonomously improve their reasoning\nand behavior, is essential for the embodied domain with long-horizon,\nreal-world tasks. Despite current advancements in reinforcement fine-tuning\n(RFT) showing strong performance in enhancing reasoning in LLMs, its potential\nto enable self-evolving embodied intelligence with multi-modal interactions\nremains largely unexplored. Specifically, reinforcement fine-tuning faces two\nfundamental obstacles in embodied settings: (i) the lack of accessible\nintermediate rewards in multi-step reasoning tasks limits effective learning\nsignals, and (ii) reliance on hand-crafted reward functions restricts\ngeneralization to novel tasks and environments. To address these challenges, we\npresent Self-Evolving Embodied Agents-R1, SEEA-R1, the first RFT framework\ndesigned for enabling the self-evolving capabilities of embodied agents.\nSpecifically, to convert sparse delayed rewards into denser intermediate\nsignals that improve multi-step reasoning, we propose Tree-based group relative\npolicy optimization (Tree-GRPO), which integrates Monte Carlo Tree Search into\nGRPO. To generalize reward estimation across tasks and scenes, supporting\nautonomous adaptation and reward-driven self-evolution, we further introduce\nMulti-modal Generative Reward Model (MGRM). To holistically evaluate the\neffectiveness of SEEA-R1, we evaluate on the ALFWorld benchmark, surpassing\nstate-of-the-art methods with scores of 85.07% (textual) and 36.19%\n(multi-modal), outperforming prior models including GPT-4o. SEEA-R1 also\nachieves scores of 80.3% without environmental reward, surpassing all\nopen-source baselines and highlighting its scalability as a self-evolving\nembodied agent. Additional experiments and qualitative analysis further support\nthe potential of SEEA-R1 for future research in scalable embodied intelligence.", "AI": {"tldr": "\u63d0\u51fa\u4e86SEEA-R1\u6846\u67b6\uff0c\u901a\u8fc7Tree-GRPO\u548cMGRM\u89e3\u51b3\u4e86\u5f3a\u5316\u5fae\u8c03\u5728\u5177\u8eab\u667a\u80fd\u4f53\u81ea\u8fdb\u5316\u4e2d\u7684\u95ee\u9898\uff0c\u5e76\u5728ALFWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6210\u679c\u3002", "motivation": "\u5f53\u524d\u7684\u5f3a\u5316\u5fae\u8c03(RFT)\u5728\u589e\u5f3aLLM\u7684\u63a8\u7406\u80fd\u529b\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u4f46\u5176\u5728\u5177\u6709\u591a\u6a21\u6001\u4ea4\u4e92\u7684\u81ea\u8fdb\u5316\u5177\u8eab\u667a\u80fd\u65b9\u9762\u7684\u6f5c\u529b\u4ecd\u672a\u88ab\u63a2\u7d22\u3002RFT\u5728\u5177\u8eab\u73af\u5883\u4e2d\u9762\u4e34\u4e24\u4e2a\u6839\u672c\u969c\u788d\uff1a(i)\u591a\u6b65\u9aa4\u63a8\u7406\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u53ef\u8bbf\u95ee\u7684\u4e2d\u95f4\u5956\u52b1\u9650\u5236\u4e86\u6709\u6548\u7684\u5b66\u4e60\u4fe1\u53f7\uff0c\u4ee5\u53ca(ii)\u4f9d\u8d56\u4e8e\u624b\u5de5\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\u9650\u5236\u4e86\u5bf9\u65b0\u4efb\u52a1\u548c\u73af\u5883\u7684\u6cdb\u5316\u3002", "method": "\u63d0\u51fa\u4e86Tree-GRPO\u548cMGRM\u6765\u89e3\u51b3RFT\u5728\u5177\u8eab\u73af\u5883\u4e2d\u7684\u6311\u6218\u3002", "result": "SEEA-R1\u5728ALFWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u6280\u672f\u6c34\u5e73\uff0c\u6587\u672c\u5f97\u5206\u4e3a85.07%\uff0c\u591a\u6a21\u6001\u5f97\u5206\u4e3a36.19%\uff0c\u8d85\u8fc7\u4e86\u5305\u62ecGPT-4o\u5728\u5185\u7684\u5148\u524d\u6a21\u578b\u3002SEEA-R1\u5728\u6ca1\u6709\u73af\u5883\u5956\u52b1\u7684\u60c5\u51b5\u4e0b\u4e5f\u83b7\u5f97\u4e8680.3%\u7684\u5206\u6570\uff0c\u8d85\u8fc7\u4e86\u6240\u6709\u5f00\u6e90\u57fa\u7ebf\u3002", "conclusion": "SEEA-R1\u5728ALFWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5728\u6ca1\u6709\u73af\u5883\u5956\u52b1\u7684\u60c5\u51b5\u4e0b\u4e5f\u53d6\u5f97\u4e86\u5f88\u9ad8\u7684\u5206\u6570\uff0c\u5c55\u793a\u4e86\u5176\u4f5c\u4e3a\u81ea\u8fdb\u5316\u5177\u8eab\u667a\u80fd\u4f53\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.21655", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21655", "abs": "https://arxiv.org/abs/2506.21655", "authors": ["Minjie Hong", "Zirun Guo", "Yan Xia", "Zehan Wang", "Ziang Zhang", "Tao Jin", "Zhou Zhao"], "title": "APO: Enhancing Reasoning Ability of MLLMs via Asymmetric Policy Optimization", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) are powerful at integrating diverse\ndata, but they often struggle with complex reasoning. While Reinforcement\nlearning (RL) can boost reasoning in LLMs, applying it to MLLMs is tricky.\nCommon issues include a drop in performance on general tasks and the generation\nof overly detailed or \"overthinking\" reasoning. Our work investigates how the\nKL penalty and overthinking affect RL training in MLLMs. We propose Asymmetric\nPolicy Optimization (APO) to address these issues, which divides the sampled\nresponses into positive and negative groups. For positive samples,\nDifficulty-Adaptive Divergence Shaping (DADS) is introduced to dynamically\nadjust the KL divergence weight based on their difficulty. This method prevents\npolicy entropy from dropping sharply, improves training stability, utilizes\nsamples better, and preserves the model's existing knowledge. For negative\nsamples, Suboptimal Trajectory Complexity Regularization (STCR) is proposed to\npenalize overly long responses. This helps mitigate overthinking and encourages\nmore concise reasoning while preserving the model's explorative capacity. We\napply our method to Qwen2.5-VL-3B, creating View-R1-3B. View-R1-3B\nsignificantly enhances reasoning capabilities, showing an average 7\\% gain over\nthe base model and outperforming larger MLLMs (7-11B) on various reasoning\nbenchmarks. Importantly, unlike other reasoning-tuned MLLMs that often degrade\non general tasks, View-R1-3B maintains consistent improvement, demonstrating\nsuperior generalization. These results highlight the effectiveness and broad\napplicability of our DADS and STCR techniques for advancing complex multimodal\nreasoning in MLLMs. The code will be made available at\nhttps://github.com/Indolent-Kawhi/View-R1.", "AI": {"tldr": "This paper introduces View-R1-3B, a new MLLM that improves reasoning using a novel RL approach (APO with DADS and STCR) without hurting general task performance, outperforming larger models.", "motivation": "MLLMs struggle with complex reasoning, and applying Reinforcement Learning (RL) to MLLMs often leads to performance drops on general tasks and overthinking.", "method": "The paper proposes Asymmetric Policy Optimization (APO), which includes Difficulty-Adaptive Divergence Shaping (DADS) to dynamically adjust the KL divergence weight based on sample difficulty and Suboptimal Trajectory Complexity Regularization (STCR) to penalize overly long responses.", "result": "View-R1-3B shows an average 7% gain over the base model and outperforms larger MLLMs (7-11B) on various reasoning benchmarks while maintaining consistent improvement on general tasks.", "conclusion": "The paper introduces View-R1-3B, an MLLM based on Qwen2.5-VL-3B, which significantly enhances reasoning capabilities without sacrificing performance on general tasks. This is achieved through Asymmetric Policy Optimization (APO) using Difficulty-Adaptive Divergence Shaping (DADS) for positive samples and Suboptimal Trajectory Complexity Regularization (STCR) for negative samples."}}
{"id": "2506.21811", "categories": ["cs.DB", "cs.GR"], "pdf": "https://arxiv.org/pdf/2506.21811", "abs": "https://arxiv.org/abs/2506.21811", "authors": ["Lingkai Meng", "Yu Shao", "Long Yuan", "Longbin Lai", "Peng Cheng", "Xue Li", "Wenyuan Yu", "Wenjie Zhang", "Xuemin Lin", "Jingren Zhou"], "title": "Revisiting Graph Analytics Benchmark", "comment": null, "summary": "The rise of graph analytics platforms has led to the development of various\nbenchmarks for evaluating and comparing platform performance. However, existing\nbenchmarks often fall short of fully assessing performance due to limitations\nin core algorithm selection, data generation processes (and the corresponding\nsynthetic datasets), as well as the neglect of API usability evaluation. To\naddress these shortcomings, we propose a novel graph analytics benchmark.\nFirst, we select eight core algorithms by extensively reviewing both academic\nand industrial settings. Second, we design an efficient and flexible data\ngenerator and produce eight new synthetic datasets as the default datasets for\nour benchmark. Lastly, we introduce a multi-level large language model\n(LLM)-based framework for API usability evaluation-the first of its kind in\ngraph analytics benchmarks. We conduct comprehensive experimental evaluations\non existing platforms (GraphX, PowerGraph, Flash, Grape, Pregel+, Ligra and\nG-thinker). The experimental results demonstrate the superiority of our\nproposed benchmark.", "AI": {"tldr": "A novel graph analytics benchmark is proposed to address the shortcomings of existing benchmarks by improving algorithm selection, data generation, and API usability evaluation.", "motivation": "Existing graph analytics benchmarks often fall short of fully assessing performance due to limitations in core algorithm selection, data generation processes, and the neglect of API usability evaluation.", "method": "The authors select eight core algorithms, design an efficient and flexible data generator to produce eight new synthetic datasets, and introduce a multi-level large language model (LLM)-based framework for API usability evaluation.", "result": "Comprehensive experimental evaluations on existing platforms (GraphX, PowerGraph, Flash, Grape, Pregel+, Ligra and G-thinker) demonstrate the superiority of the proposed benchmark.", "conclusion": "The proposed graph analytics benchmark demonstrates its superiority through comprehensive experimental evaluations on existing platforms (GraphX, PowerGraph, Flash, Grape, Pregel+, Ligra and G-thinker)."}}
{"id": "2506.21579", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21579", "abs": "https://arxiv.org/abs/2506.21579", "authors": ["Yingzhi He", "Xiaohao Liu", "An Zhang", "Yunshan Ma", "Tat-Seng Chua"], "title": "LLM2Rec: Large Language Models Are Powerful Embedding Models for Sequential Recommendation", "comment": "KDD 2025", "summary": "Sequential recommendation aims to predict users' future interactions by\nmodeling collaborative filtering (CF) signals from historical behaviors of\nsimilar users or items. Traditional sequential recommenders predominantly rely\non ID-based embeddings, which capture CF signals through high-order\nco-occurrence patterns. However, these embeddings depend solely on past\ninteractions, lacking transferable knowledge to generalize to unseen domains.\nRecent advances in large language models (LLMs) have motivated text-based\nrecommendation approaches that derive item representations from textual\ndescriptions. While these methods enhance generalization, they fail to encode\nCF signals-i.e., latent item correlations and preference patterns-crucial for\neffective recommendation. We argue that an ideal embedding model should\nseamlessly integrate CF signals with rich semantic representations to improve\nboth in-domain and out-of-domain recommendation performance.\n  To this end, we propose LLM2Rec, a novel embedding model tailored for\nsequential recommendation, integrating the rich semantic understanding of LLMs\nwith CF awareness. Our approach follows a two-stage training framework: (1)\nCollaborative Supervised Fine-tuning, which adapts LLMs to infer item\nrelationships based on historical interactions, and (2) Item-level Embedding\nModeling, which refines these specialized LLMs into structured item embedding\nmodels that encode both semantic and collaborative information. Extensive\nexperiments on real-world datasets demonstrate that LLM2Rec effectively\nimproves recommendation quality across both in-domain and out-of-domain\nsettings. Our findings highlight the potential of leveraging LLMs to build more\nrobust, generalizable embedding models for sequential recommendation. Our codes\nare available at https://github.com/HappyPointer/LLM2Rec.", "AI": {"tldr": "LLM2Rec\u662f\u4e00\u79cd\u65b0\u7684\u5d4c\u5165\u6a21\u578b\uff0c\u5b83\u96c6\u6210\u4e86LLM\u7684\u8bed\u4e49\u7406\u89e3\u548cCF\u611f\u77e5\uff0c\u4ee5\u63d0\u9ad8\u5e8f\u5217\u63a8\u8350\u7684\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u7684\u5e8f\u5217\u63a8\u8350\u5668\u4e3b\u8981\u4f9d\u8d56\u4e8e\u57fa\u4e8eid\u7684\u5d4c\u5165\uff0c\u8fd9\u4e9b\u5d4c\u5165\u901a\u8fc7\u9ad8\u9636\u5171\u73b0\u6a21\u5f0f\u6355\u83b7CF\u4fe1\u53f7\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u5d4c\u5165\u5b8c\u5168\u4f9d\u8d56\u4e8e\u8fc7\u53bb\u7684\u4ea4\u4e92\uff0c\u7f3a\u4e4f\u63a8\u5e7f\u5230\u770b\u4e0d\u89c1\u7684\u9886\u57df\u7684\u4f20\u9012\u77e5\u8bc6\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u7684\u6700\u65b0\u8fdb\u5c55\u63a8\u52a8\u4e86\u57fa\u4e8e\u6587\u672c\u7684\u63a8\u8350\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u4ece\u6587\u672c\u63cf\u8ff0\u4e2d\u83b7\u5f97\u9879\u76ee\u8868\u793a\u3002\u867d\u7136\u8fd9\u4e9b\u65b9\u6cd5\u589e\u5f3a\u4e86\u6cdb\u5316\uff0c\u4f46\u5b83\u4eec\u672a\u80fd\u7f16\u7801CF\u4fe1\u53f7\u2014\u2014\u5373\u6f5c\u5728\u7684\u9879\u76ee\u76f8\u5173\u6027\u548c\u504f\u597d\u6a21\u5f0f\u2014\u2014\u8fd9\u5bf9\u4e8e\u6709\u6548\u7684\u63a8\u8350\u81f3\u5173\u91cd\u8981\u3002\u6211\u4eec\u8ba4\u4e3a\uff0c\u7406\u60f3\u7684\u5d4c\u5165\u6a21\u578b\u5e94\u8be5\u65e0\u7f1d\u5730\u5c06CF\u4fe1\u53f7\u4e0e\u4e30\u5bcc\u7684\u8bed\u4e49\u8868\u793a\u96c6\u6210\uff0c\u4ee5\u63d0\u9ad8\u9886\u57df\u5185\u548c\u9886\u57df\u5916\u7684\u63a8\u8350\u6027\u80fd\u3002", "method": "LLM2Rec\uff0c\u4e00\u79cd\u4e3a\u5e8f\u5217\u63a8\u8350\u91cf\u8eab\u5b9a\u5236\u7684\u65b0\u578b\u5d4c\u5165\u6a21\u578b\uff0c\u5b83\u96c6\u6210\u4e86LLM\u7684\u4e30\u5bcc\u8bed\u4e49\u7406\u89e3\u548cCF\u611f\u77e5\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u9075\u5faa\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u8bad\u7ec3\u6846\u67b6\uff1a(1) \u534f\u540c\u76d1\u7763\u5fae\u8c03\uff0c\u5b83\u4f7fllm\u80fd\u591f\u6839\u636e\u5386\u53f2\u4ea4\u4e92\u63a8\u65ad\u9879\u76ee\u5173\u7cfb\uff1b(2) \u9879\u76ee\u7ea7\u5d4c\u5165\u5efa\u6a21\uff0c\u5b83\u5c06\u8fd9\u4e9b\u4e13\u95e8\u7684llm\u6539\u8fdb\u4e3a\u7ed3\u6784\u5316\u7684\u9879\u76ee\u5d4c\u5165\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u7f16\u7801\u8bed\u4e49\u548c\u534f\u4f5c\u4fe1\u606f\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cLLM2Rec\u6709\u6548\u5730\u63d0\u9ad8\u4e86\u5728\u9886\u57df\u5185\u548c\u9886\u57df\u5916\u73af\u5883\u4e0b\u7684\u63a8\u8350\u8d28\u91cf\u3002", "conclusion": "LLM2Rec\u6709\u6548\u5730\u63d0\u9ad8\u4e86\u5728\u9886\u57df\u5185\u548c\u9886\u57df\u5916\u73af\u5883\u4e0b\u7684\u63a8\u8350\u8d28\u91cf\uff0c\u5c55\u793a\u4e86\u5229\u7528LLM\u6784\u5efa\u66f4\u5065\u58ee\u3001\u66f4\u901a\u7528\u7684\u5e8f\u5217\u63a8\u8350\u5d4c\u5165\u6a21\u578b\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.21722", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21722", "abs": "https://arxiv.org/abs/2506.21722", "authors": ["Xin Lu", "Xueyang Fu", "Jie Xiao", "Zihao Fan", "Yurui Zhu", "Zheng-Jun Zha"], "title": "Elucidating and Endowing the Diffusion Training Paradigm for General Image Restoration", "comment": null, "summary": "While diffusion models demonstrate strong generative capabilities in image\nrestoration (IR) tasks, their complex architectures and iterative processes\nlimit their practical application compared to mainstream reconstruction-based\ngeneral ordinary IR networks. Existing approaches primarily focus on optimizing\nnetwork architecture and diffusion paths but overlook the integration of the\ndiffusion training paradigm within general ordinary IR frameworks. To address\nthese challenges, this paper elucidates key principles for adapting the\ndiffusion training paradigm to general IR training through systematic analysis\nof time-step dependencies, network hierarchies, noise-level relationships, and\nmulti-restoration task correlations, proposing a new IR framework supported by\ndiffusion-based training. To enable IR networks to simultaneously restore\nimages and model generative representations, we introduce a series of\nregularization strategies that align diffusion objectives with IR tasks,\nimproving generalization in single-task scenarios. Furthermore, recognizing\nthat diffusion-based generation exerts varying influences across different IR\ntasks, we develop an incremental training paradigm and task-specific adaptors,\nfurther enhancing performance in multi-task unified IR. Experiments demonstrate\nthat our method significantly improves the generalization of IR networks in\nsingle-task IR and achieves superior performance in multi-task unified IR.\nNotably, the proposed framework can be seamlessly integrated into existing\ngeneral IR architectures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ea2\u5916\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u6269\u6563\u8bad\u7ec3\u8303\u5f0f\u6765\u63d0\u9ad8\u901a\u7528\u7ea2\u5916\u7f51\u7edc\u7684\u6027\u80fd\uff0c\u5e76\u5728\u5355\u4efb\u52a1\u548c\u591a\u4efb\u52a1\u7ea2\u5916\u4e2d\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u6062\u590d\uff08IR\uff09\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u751f\u6210\u80fd\u529b\uff0c\u4f46\u4e0e\u4e3b\u6d41\u7684\u57fa\u4e8e\u91cd\u5efa\u7684\u901a\u7528\u666e\u901a\u7ea2\u5916\u7f51\u7edc\u76f8\u6bd4\uff0c\u5176\u590d\u6742\u7684\u67b6\u6784\u548c\u8fed\u4ee3\u8fc7\u7a0b\u9650\u5236\u4e86\u5b83\u4eec\u7684\u5b9e\u9645\u5e94\u7528\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u4f18\u5316\u7f51\u7edc\u67b6\u6784\u548c\u6269\u6563\u8def\u5f84\u4e0a\uff0c\u4f46\u5ffd\u7565\u4e86\u5c06\u6269\u6563\u8bad\u7ec3\u8303\u5f0f\u96c6\u6210\u5230\u901a\u7528\u666e\u901a\u7ea2\u5916\u6846\u67b6\u4e2d\u3002", "method": "\u901a\u8fc7\u5bf9\u65f6\u95f4\u6b65\u957f\u4f9d\u8d56\u6027\u3001\u7f51\u7edc\u5c42\u6b21\u7ed3\u6784\u3001\u566a\u58f0\u6c34\u5e73\u5173\u7cfb\u548c\u591a\u91cd\u6062\u590d\u4efb\u52a1\u76f8\u5173\u6027\u8fdb\u884c\u7cfb\u7edf\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u4e8e\u6269\u6563\u8bad\u7ec3\u7684\u7ea2\u5916\u6846\u67b6\u3002\u4e3a\u4e86\u4f7f\u7ea2\u5916\u7f51\u7edc\u80fd\u591f\u540c\u65f6\u6062\u590d\u56fe\u50cf\u548c\u5efa\u6a21\u751f\u6210\u8868\u793a\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u7cfb\u5217\u6b63\u5219\u5316\u7b56\u7565\uff0c\u4f7f\u6269\u6563\u76ee\u6807\u4e0e\u7ea2\u5916\u4efb\u52a1\u5bf9\u9f50\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u5355\u4efb\u52a1\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6b64\u5916\uff0c\u8ba4\u8bc6\u5230\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u5bf9\u4e0d\u540c\u7ea2\u5916\u4efb\u52a1\u4ea7\u751f\u4e0d\u540c\u7684\u5f71\u54cd\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u589e\u91cf\u8bad\u7ec3\u8303\u5f0f\u548c\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u9002\u914d\u5668\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u591a\u4efb\u52a1\u7edf\u4e00\u7ea2\u5916\u4e2d\u7684\u6027\u80fd\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u7ea2\u5916\u7f51\u7edc\u5728\u5355\u4efb\u52a1\u7ea2\u5916\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4efb\u52a1\u7edf\u4e00\u7ea2\u5916\u4e2d\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u7ea2\u5916\u7f51\u7edc\u5728\u5355\u4efb\u52a1\u7ea2\u5916\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4efb\u52a1\u7edf\u4e00\u7ea2\u5916\u4e2d\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u8be5\u6846\u67b6\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u7684\u901a\u7528\u7ea2\u5916\u67b6\u6784\u4e2d\u3002"}}
{"id": "2506.21556", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21556", "abs": "https://arxiv.org/abs/2506.21556", "authors": ["Hyeongcheol Park", "MinHyuk Jang", "Ha Dam Baek", "Gyusam Chang", "Jiyoung Seo", "Jiwan Park", "Hogun Park", "Sangpil Kim"], "title": "VAT-KG: Knowledge-Intensive Multimodal Knowledge Graph Dataset for Retrieval-Augmented Generation", "comment": "Project Page: https://vatkg.github.io/", "summary": "Multimodal Knowledge Graphs (MMKGs), which represent explicit knowledge\nacross multiple modalities, play a pivotal role by complementing the implicit\nknowledge of Multimodal Large Language Models (MLLMs) and enabling more\ngrounded reasoning via Retrieval Augmented Generation (RAG). However, existing\nMMKGs are generally limited in scope: they are often constructed by augmenting\npre-existing knowledge graphs, which restricts their knowledge, resulting in\noutdated or incomplete knowledge coverage, and they often support only a narrow\nrange of modalities, such as text and visual information. These limitations\nreduce their extensibility and applicability to a broad range of multimodal\ntasks, particularly as the field shifts toward richer modalities such as video\nand audio in recent MLLMs. Therefore, we propose the Visual-Audio-Text\nKnowledge Graph (VAT-KG), the first concept-centric and knowledge-intensive\nmultimodal knowledge graph that covers visual, audio, and text information,\nwhere each triplet is linked to multimodal data and enriched with detailed\ndescriptions of concepts. Specifically, our construction pipeline ensures\ncross-modal knowledge alignment between multimodal data and fine-grained\nsemantics through a series of stringent filtering and alignment steps, enabling\nthe automatic generation of MMKGs from any multimodal dataset. We further\nintroduce a novel multimodal RAG framework that retrieves detailed\nconcept-level knowledge in response to queries from arbitrary modalities.\nExperiments on question answering tasks across various modalities demonstrate\nthe effectiveness of VAT-KG in supporting MLLMs, highlighting its practical\nvalue in unifying and leveraging multimodal knowledge.", "AI": {"tldr": "\u63d0\u51fa\u4e86 VAT-KG\uff0c\u4e00\u4e2a\u6db5\u76d6\u89c6\u89c9\u3001\u97f3\u9891\u548c\u6587\u672c\u4fe1\u606f\u7684\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u591a\u6a21\u6001 RAG \u6846\u67b6\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86 VAT-KG \u5728\u652f\u6301 MLLM \u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684 MMKG \u901a\u5e38\u5728\u8303\u56f4\u4e0a\u53d7\u5230\u9650\u5236\uff0c\u5bfc\u81f4\u77e5\u8bc6\u8986\u76d6\u8303\u56f4\u8fc7\u65f6\u6216\u4e0d\u5b8c\u6574\uff0c\u5e76\u4e14\u5b83\u4eec\u901a\u5e38\u4ec5\u652f\u6301\u7a84\u8303\u56f4\u7684\u6a21\u5f0f\uff0c\u4f8b\u5982\u6587\u672c\u548c\u89c6\u89c9\u4fe1\u606f\u3002\u8fd9\u4e9b\u9650\u5236\u964d\u4f4e\u4e86\u5b83\u4eec\u7684\u53ef\u6269\u5c55\u6027\u548c\u5bf9\u5e7f\u6cdb\u7684\u591a\u6a21\u6001\u4efb\u52a1\u7684\u9002\u7528\u6027\uff0c\u7279\u522b\u662f\u5728\u8be5\u9886\u57df\u8f6c\u5411\u66f4\u4e30\u5bcc\u7684\u6a21\u5f0f\uff08\u5982\u89c6\u9891\u548c\u97f3\u9891\uff09\u65f6\u3002", "method": "\u63d0\u51fa Visual-Audio-Text Knowledge Graph (VAT-KG)\uff0c\u8fd9\u662f\u4e00\u4e2a\u6982\u5ff5\u4e2d\u5fc3\u548c\u77e5\u8bc6\u5bc6\u96c6\u578b\u7684\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\uff0c\u6db5\u76d6\u89c6\u89c9\u3001\u97f3\u9891\u548c\u6587\u672c\u4fe1\u606f\u3002\u6784\u5efa\u6d41\u7a0b\u901a\u8fc7\u4e00\u7cfb\u5217\u4e25\u683c\u7684\u8fc7\u6ee4\u548c\u5bf9\u9f50\u6b65\u9aa4\uff0c\u786e\u4fdd\u591a\u6a21\u6001\u6570\u636e\u548c\u7ec6\u7c92\u5ea6\u8bed\u4e49\u4e4b\u95f4\u7684\u8de8\u6a21\u6001\u77e5\u8bc6\u5bf9\u9f50\uff0c\u4ece\u800c\u80fd\u591f\u4ece\u4efb\u4f55\u591a\u6a21\u6001\u6570\u636e\u96c6\u81ea\u52a8\u751f\u6210 MMKG\u3002", "result": "\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001 RAG \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u68c0\u7d22\u8be6\u7ec6\u7684\u6982\u5ff5\u7ea7\u77e5\u8bc6\u4ee5\u54cd\u5e94\u6765\u81ea\u4efb\u610f\u6a21\u5f0f\u7684\u67e5\u8be2\u3002", "conclusion": "VAT-KG \u901a\u8fc7\u5728\u5404\u79cd\u6a21\u5f0f\u7684\u95ee\u9898\u56de\u7b54\u4efb\u52a1\u4e2d\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u5728\u652f\u6301 MLLM \u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u7a81\u51fa\u4e86\u5176\u5728\u7edf\u4e00\u548c\u5229\u7528\u591a\u6a21\u6001\u77e5\u8bc6\u65b9\u9762\u7684\u5b9e\u9645\u4ef7\u503c\u3002"}}
{"id": "2506.21734", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21734", "abs": "https://arxiv.org/abs/2506.21734", "authors": ["Guan Wang", "Jin Li", "Yuhao Sun", "Xing Chen", "Changling Liu", "Yue Wu", "Meng Lu", "Sen Song", "Yasin Abbasi Yadkori"], "title": "Hierarchical Reasoning Model", "comment": null, "summary": "Reasoning, the process of devising and executing complex goal-oriented action\nsequences, remains a critical challenge in AI. Current large language models\n(LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from\nbrittle task decomposition, extensive data requirements, and high latency.\nInspired by the hierarchical and multi-timescale processing in the human brain,\nwe propose the Hierarchical Reasoning Model (HRM), a novel recurrent\narchitecture that attains significant computational depth while maintaining\nboth training stability and efficiency. HRM executes sequential reasoning tasks\nin a single forward pass without explicit supervision of the intermediate\nprocess, through two interdependent recurrent modules: a high-level module\nresponsible for slow, abstract planning, and a low-level module handling rapid,\ndetailed computations. With only 27 million parameters, HRM achieves\nexceptional performance on complex reasoning tasks using only 1000 training\nsamples. The model operates without pre-training or CoT data, yet achieves\nnearly perfect performance on challenging tasks including complex Sudoku\npuzzles and optimal path finding in large mazes. Furthermore, HRM outperforms\nmuch larger models with significantly longer context windows on the Abstraction\nand Reasoning Corpus (ARC), a key benchmark for measuring artificial general\nintelligence capabilities. These results underscore HRM's potential as a\ntransformative advancement toward universal computation and general-purpose\nreasoning systems.", "AI": {"tldr": "HRM, a novel recurrent architecture inspired by the human brain, achieves exceptional performance on complex reasoning tasks with high efficiency and minimal data, outperforming much larger models.", "motivation": "Current large language models (LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from brittle task decomposition, extensive data requirements, and high latency. Inspired by the hierarchical and multi-timescale processing in the human brain", "method": "We propose the Hierarchical Reasoning Model (HRM), a novel recurrent architecture that attains significant computational depth while maintaining both training stability and efficiency. HRM executes sequential reasoning tasks in a single forward pass without explicit supervision of the intermediate process, through two interdependent recurrent modules: a high-level module responsible for slow, abstract planning, and a low-level module handling rapid, detailed computations.", "result": "With only 27 million parameters, HRM achieves exceptional performance on complex reasoning tasks using only 1000 training samples. The model operates without pre-training or CoT data, yet achieves nearly perfect performance on challenging tasks including complex Sudoku puzzles and optimal path finding in large mazes. Furthermore, HRM outperforms much larger models with significantly longer context windows on the Abstraction and Reasoning Corpus (ARC)", "conclusion": "HRM demonstrates potential as a transformative advancement toward universal computation and general-purpose reasoning systems."}}
{"id": "2506.21683", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21683", "abs": "https://arxiv.org/abs/2506.21683", "authors": ["Xihong Su", "Jia Lin Hau", "Gersi Doko", "Kishan Panaganti", "Marek Petrik"], "title": "Risk-Averse Total-Reward Reinforcement Learning", "comment": "The paper is under review now", "summary": "Risk-averse total-reward Markov Decision Processes (MDPs) offer a promising\nframework for modeling and solving undiscounted infinite-horizon objectives.\nExisting model-based algorithms for risk measures like the entropic risk\nmeasure (ERM) and entropic value-at-risk (EVaR) are effective in small\nproblems, but require full access to transition probabilities. We propose a\nQ-learning algorithm to compute the optimal stationary policy for total-reward\nERM and EVaR objectives with strong convergence and performance guarantees. The\nalgorithm and its optimality are made possible by ERM's dynamic consistency and\nelicitability. Our numerical results on tabular domains demonstrate quick and\nreliable convergence of the proposed Q-learning algorithm to the optimal\nrisk-averse value function.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cdQ\u5b66\u4e60\u7b97\u6cd5\u6765\u89e3\u51b3\u98ce\u9669\u89c4\u907f\u7684\u603b\u5956\u52b1\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u95ee\u9898\uff0c\u8be5\u7b97\u6cd5\u5728\u6570\u503c\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u5feb\u901f\u6536\u655b\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6a21\u578b\u7684\u98ce\u9669\u5ea6\u91cf\u7b97\u6cd5\uff08\u5982\u71b5\u98ce\u9669\u5ea6\u91cf\uff08ERM\uff09\u548c\u71b5\u503c\u98ce\u9669\uff08EVaR\uff09\uff09\u5728\u5c0f\u95ee\u9898\u4e2d\u6709\u6548\uff0c\u4f46\u9700\u8981\u5b8c\u5168\u8bbf\u95ee\u8f6c\u79fb\u6982\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2aQ\u5b66\u4e60\u7b97\u6cd5", "result": "\u5728\u8868\u683c\u57df\u7684\u6570\u503c\u7ed3\u679c\u4e2d\u5c55\u793a\u4e86\u8be5\u7b97\u6cd5\u5feb\u901f\u53ef\u9760\u7684\u6536\u655b\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2aQ\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u8ba1\u7b97\u603b\u5956\u52b1ERM\u548cEVaR\u76ee\u6807\u7684\u6700\u4f18\u5e73\u7a33\u7b56\u7565\uff0c\u5e76\u5728\u8868\u683c\u57df\u7684\u6570\u503c\u7ed3\u679c\u4e2d\u5c55\u793a\u4e86\u8be5\u7b97\u6cd5\u5feb\u901f\u53ef\u9760\u7684\u6536\u655b\u6027\u3002"}}
{"id": "2506.21901", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2506.21901", "abs": "https://arxiv.org/abs/2506.21901", "authors": ["James Pan", "Guoliang Li"], "title": "A Survey of LLM Inference Systems", "comment": "25 pages", "summary": "The past few years has witnessed specialized large language model (LLM)\ninference systems, such as vLLM, SGLang, Mooncake, and DeepFlow, alongside\nrapid LLM adoption via services like ChatGPT. Driving these system design\nefforts is the unique autoregressive nature of LLM request processing,\nmotivating new techniques for achieving high performance while preserving high\ninference quality over high-volume and high-velocity workloads. While many of\nthese techniques are discussed across the literature, they have not been\nanalyzed under the framework of a complete inference system, nor have the\nsystems themselves been analyzed and compared.\n  In this survey, we review these techniques, starting from operators and\nalgorithms for request processing, then moving on to techniques for model\noptimization and execution, including kernel design, batching, and scheduling,\nbefore ending with techniques for memory management, including paged memory,\neviction and offloading techniques, quantization, and cache persistence.\nThrough these discussions, we show that these techniques fundamentally rely on\nload prediction, adaptive mechanisms, and cost reduction in order to overcome\nthe challenges introduced by autoregressive generation and achieve the goals of\nthe system. We then discuss how these techniques can be combined to form\nsingle-replica and multi-replica inference systems, including disaggregated\ninference systems that offer more control over resource allocation and\nserverless systems that can be deployed over shared hardware infrastructure. We\nend with a discussion of remaining challenges.", "AI": {"tldr": "Reviews LLM inference techniques, highlighting load prediction, adaptive mechanisms, and cost reduction for high performance.", "motivation": "The autoregressive nature of LLM request processing motivates new techniques for high performance and inference quality under high-volume workloads. Existing techniques lack analysis within a complete system framework.", "method": "Reviewing operators, algorithms, model optimization, execution, and memory management techniques.", "result": "Techniques fundamentally rely on load prediction, adaptive mechanisms, and cost reduction. Combination of these techniques to form single-replica and multi-replica inference systems, including disaggregated and serverless systems.", "conclusion": "This survey reviews techniques for LLM inference systems, showing they rely on load prediction, adaptive mechanisms, and cost reduction. It discusses combining these techniques in single/multi-replica systems and remaining challenges."}}
{"id": "2506.21581", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21581", "abs": "https://arxiv.org/abs/2506.21581", "authors": ["Sarthak Chaturvedi", "Anurag Acharya", "Rounak Meyur", "Koby Hayashi", "Sai Munikoti", "Sameera Horawalavithana"], "title": "Evaluating the Robustness of Dense Retrievers in Interdisciplinary Domains", "comment": null, "summary": "Evaluation benchmark characteristics may distort the true benefits of domain\nadaptation in retrieval models. This creates misleading assessments that\ninfluence deployment decisions in specialized domains. We show that two\nbenchmarks with drastically different features such as topic diversity,\nboundary overlap, and semantic complexity can influence the perceived benefits\nof fine-tuning. Using environmental regulatory document retrieval as a case\nstudy, we fine-tune ColBERTv2 model on Environmental Impact Statements (EIS)\nfrom federal agencies. We evaluate these models across two benchmarks with\ndifferent semantic structures. Our findings reveal that identical domain\nadaptation approaches show very different perceived benefits depending on\nevaluation methodology. On one benchmark, with clearly separated topic\nboundaries, domain adaptation shows small improvements (maximum 0.61% NDCG\ngain). However, on the other benchmark with overlapping semantic structures,\nthe same models demonstrate large improvements (up to 2.22% NDCG gain), a\n3.6-fold difference in the performance benefit. We compare these benchmarks\nthrough topic diversity metrics, finding that the higher-performing benchmark\nshows 11% higher average cosine distances between contexts and 23% lower\nsilhouette scores, directly contributing to the observed performance\ndifference. These results demonstrate that benchmark selection strongly\ndetermines assessments of retrieval system effectiveness in specialized\ndomains. Evaluation frameworks with well-separated topics regularly\nunderestimate domain adaptation benefits, while those with overlapping semantic\nboundaries reveal improvements that better reflect real-world regulatory\ndocument complexity. Our findings have important implications for developing\nand deploying AI systems for interdisciplinary domains that integrate multiple\ntopics.", "AI": {"tldr": "\u57fa\u51c6\u9009\u62e9\u5f3a\u70c8\u51b3\u5b9a\u4e86\u5bf9\u4e13\u4e1a\u9886\u57df\u4e2d\u68c0\u7d22\u7cfb\u7edf\u6709\u6548\u6027\u7684\u8bc4\u4f30\u3002", "motivation": "\u8bc4\u4f30\u57fa\u51c6\u7279\u5f81\u53ef\u80fd\u4f1a\u626d\u66f2\u9886\u57df\u81ea\u9002\u5e94\u7684\u771f\u6b63\u4f18\u52bf\uff0c\u4ece\u800c\u5bfc\u81f4\u5bf9\u4e13\u4e1a\u9886\u57df\u90e8\u7f72\u51b3\u7b56\u4ea7\u751f\u8bef\u5bfc\u6027\u8bc4\u4f30\u3002", "method": "\u5728\u8054\u90a6\u673a\u6784\u7684\u73af\u5883\u5f71\u54cd\u62a5\u544a (EIS) \u4e0a\u5fae\u8c03 ColBERTv2 \u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u4e3b\u9898\u591a\u6837\u6027\u6307\u6807\u6bd4\u8f83\u4e0d\u540c\u7684\u57fa\u51c6\u3002", "result": "\u76f8\u540c\u7684\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u5728\u4e0d\u540c\u7684\u8bc4\u4f30\u65b9\u6cd5\u4e0b\u8868\u73b0\u51fa\u975e\u5e38\u4e0d\u540c\u7684\u611f\u77e5\u4f18\u52bf\u3002\u5728\u5177\u6709\u6e05\u6670\u5206\u9694\u4e3b\u9898\u8fb9\u754c\u7684\u57fa\u51c6\u4e0a\uff0c\u9886\u57df\u81ea\u9002\u5e94\u663e\u793a\u51fa\u8f83\u5c0f\u7684\u6539\u8fdb\uff08\u6700\u5927 0.61% NDCG \u589e\u76ca\uff09\u3002\u4f46\u662f\uff0c\u5728\u53e6\u4e00\u4e2a\u5177\u6709\u91cd\u53e0\u8bed\u4e49\u7ed3\u6784\u7684\u57fa\u51c6\u4e0a\uff0c\u76f8\u540c\u7684\u6a21\u578b\u8868\u73b0\u51fa\u5f88\u5927\u7684\u6539\u8fdb\uff08\u9ad8\u8fbe 2.22% NDCG \u589e\u76ca\uff09\uff0c\u6027\u80fd\u4f18\u52bf\u76f8\u5dee 3.6 \u500d\u3002", "conclusion": "\u8bc4\u4f30\u57fa\u51c6\u7684\u7279\u5f81\u53ef\u80fd\u4f1a\u626d\u66f2\u68c0\u7d22\u6a21\u578b\u4e2d\u9886\u57df\u81ea\u9002\u5e94\u7684\u771f\u6b63\u4f18\u52bf\u3002\u5177\u6709\u660e\u786e\u5206\u9694\u4e3b\u9898\u8fb9\u754c\u7684\u8bc4\u4f30\u6846\u67b6\u901a\u5e38\u4f1a\u4f4e\u4f30\u9886\u57df\u81ea\u9002\u5e94\u7684\u4f18\u52bf\uff0c\u800c\u90a3\u4e9b\u5177\u6709\u91cd\u53e0\u8bed\u4e49\u8fb9\u754c\u7684\u8bc4\u4f30\u6846\u67b6\u5219\u4f1a\u663e\u793a\u51fa\u66f4\u597d\u5730\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u76d1\u7ba1\u6587\u6863\u590d\u6742\u6027\u7684\u6539\u8fdb\u3002"}}
{"id": "2506.21724", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21724", "abs": "https://arxiv.org/abs/2506.21724", "authors": ["Remco F. Leijenaar", "Hamidreza Kasaei"], "title": "Asymmetric Dual Self-Distillation for 3D Self-Supervised Representation Learning", "comment": "for associated source code, see\n  https://github.com/RFLeijenaar/AsymDSD", "summary": "Learning semantically meaningful representations from unstructured 3D point\nclouds remains a central challenge in computer vision, especially in the\nabsence of large-scale labeled datasets. While masked point modeling (MPM) is\nwidely used in self-supervised 3D learning, its reconstruction-based objective\ncan limit its ability to capture high-level semantics. We propose AsymDSD, an\nAsymmetric Dual Self-Distillation framework that unifies masked modeling and\ninvariance learning through prediction in the latent space rather than the\ninput space. AsymDSD builds on a joint embedding architecture and introduces\nseveral key design choices: an efficient asymmetric setup, disabling attention\nbetween masked queries to prevent shape leakage, multi-mask sampling, and a\npoint cloud adaptation of multi-crop. AsymDSD achieves state-of-the-art results\non ScanObjectNN (90.53%) and further improves to 93.72% when pretrained on 930k\nshapes, surpassing prior methods.", "AI": {"tldr": "AsymDSD is proposed, an Asymmetric Dual Self-Distillation framework that unifies masked modeling and invariance learning through prediction in the latent space rather than the input space,achieving state-of-the-art results on ScanObjectNN.", "motivation": "Learning semantically meaningful representations from unstructured 3D point clouds remains a central challenge in computer vision, especially in the absence of large-scale labeled datasets. While masked point modeling (MPM) is widely used in self-supervised 3D learning, its reconstruction-based objective can limit its ability to capture high-level semantics.", "method": "AsymDSD, an Asymmetric Dual Self-Distillation framework that unifies masked modeling and invariance learning through prediction in the latent space rather than the input space. AsymDSD builds on a joint embedding architecture and introduces several key design choices: an efficient asymmetric setup, disabling attention between masked queries to prevent shape leakage, multi-mask sampling, and a point cloud adaptation of multi-crop.", "result": "achieves state-of-the-art results on ScanObjectNN (90.53%) and further improves to 93.72% when pretrained on 930k shapes, surpassing prior methods", "conclusion": "AsymDSD achieves state-of-the-art results on ScanObjectNN (90.53%) and further improves to 93.72% when pretrained on 930k shapes, surpassing prior methods."}}
{"id": "2506.21557", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21557", "abs": "https://arxiv.org/abs/2506.21557", "authors": ["Kaiying Yan", "Moyang Liu", "Yukun Liu", "Ruibo Fu", "Zhengqi Wen", "Jianhua Tao", "Xuefei Liu"], "title": "Debunk and Infer: Multimodal Fake News Detection via Diffusion-Generated Evidence and LLM Reasoning", "comment": null, "summary": "The rapid spread of fake news across multimedia platforms presents serious\nchallenges to information credibility. In this paper, we propose a\nDebunk-and-Infer framework for Fake News Detection(DIFND) that leverages\ndebunking knowledge to enhance both the performance and interpretability of\nfake news detection. DIFND integrates the generative strength of conditional\ndiffusion models with the collaborative reasoning capabilities of multimodal\nlarge language models (MLLMs). Specifically, debunk diffusion is employed to\ngenerate refuting or authenticating evidence based on the multimodal content of\nnews videos, enriching the evaluation process with diverse yet semantically\naligned synthetic samples. To improve inference, we propose a chain-of-debunk\nstrategy where a multi-agent MLLM system produces logic-grounded,\nmultimodal-aware reasoning content and final veracity judgment. By jointly\nmodeling multimodal features, generative debunking cues, and reasoning-rich\nverification within a unified architecture, DIFND achieves notable improvements\nin detection accuracy. Extensive experiments on the FakeSV and FVC datasets\nshow that DIFND not only outperforms existing approaches but also delivers\ntrustworthy decisions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DIFND\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u5047\u65b0\u95fb\uff0c\u901a\u8fc7\u751f\u6210\u53cd\u9a73\u8bc1\u636e\u548c\u591a\u6a21\u6001\u63a8\u7406\uff0c\u63d0\u9ad8\u4e86\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u4fe1\u5ea6\uff0c\u5e76\u5728FakeSV\u548cFVC\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u6210\u679c\u3002", "motivation": "\u591a\u5a92\u4f53\u5e73\u53f0\u4e0a\u5047\u65b0\u95fb\u7684\u8fc5\u901f\u4f20\u64ad\u5bf9\u4fe1\u606f\u53ef\u4fe1\u5ea6\u63d0\u51fa\u4e86\u4e25\u5cfb\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u5047\u65b0\u95fb\u68c0\u6d4b\u7684Debunk-and-Infer\u6846\u67b6(DIFND)\uff0c\u8be5\u6846\u67b6\u5229\u7528\u53cd\u9a73\u77e5\u8bc6\u6765\u589e\u5f3a\u5047\u65b0\u95fb\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002DIFND\u96c6\u6210\u4e86\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\u548c\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b(MLLM)\u7684\u534f\u540c\u63a8\u7406\u80fd\u529b\u3002", "result": "DIFND\u5728\u68c0\u6d4b\u7cbe\u5ea6\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "DIFND\u5728FakeSV\u548cFVC\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u503c\u5f97\u4fe1\u8d56\u7684\u51b3\u7b56\u3002"}}
{"id": "2506.21763", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21763", "abs": "https://arxiv.org/abs/2506.21763", "authors": ["Xin Wang", "Jiyao Liu", "Yulong Xiao", "Junzhi Ning", "Lihao Liu", "Junjun He", "Botian Shi", "Kaicheng Yu"], "title": "THE-Tree: Can Tracing Historical Evolution Enhance Scientific Verification and Reasoning?", "comment": null, "summary": "Large Language Models (LLMs) are accelerating scientific idea generation, but\nrigorously evaluating these numerous, often superficial, AI-generated\npropositions for novelty and factual accuracy is a critical bottleneck; manual\nverification is too slow.Existing validation methods are inadequate: LLMs as\nstandalone verifiers may hallucinate and lack domain knowledge (our findings\nshow ~60\\% unawareness of relevant papers in specific domains), while\ntraditional citation networks lack explicit causality and narrative surveys are\nunstructured.This underscores a core challenge: the absence of structured,\nverifiable, and causally-linked historical data of scientific evolution.To\naddress this,we introduce \\textbf{THE-Tree} (\\textbf{T}echnology\n\\textbf{H}istory \\textbf{E}volution Tree), a computational framework that\nconstructs such domain-specific evolution trees from scientific\nliterature.THE-Tree employs a search algorithm to explore evolutionary paths.\nDuring its node expansion, it utilizes a novel \"Think-Verbalize-Cite-Verify\"\nprocess: an LLM proposes potential advancements and cites supporting\nliterature. Critically, each proposed evolutionary link is then validated for\nlogical coherence and evidential support by a recovered natural language\ninference mechanism that interrogates the cited literature, ensuring that each\nstep is grounded.We construct and validate 88 THE-Trees across diverse domains\nand release a benchmark dataset including up to 71k fact verifications covering\n27k papers to foster further research.Experiments demonstrate that i) in graph\ncompletion, our THE-Tree improves hit@1 by 8\\% to 14\\% across multiple models\ncompared to traditional citation networks; ii) for predicting future scientific\ndevelopments, it improves hit@1 metric by nearly 10\\%; and iii) when combined\nwith other methods, it boosts the performance of evaluating important\nscientific papers by almost 100\\%.", "AI": {"tldr": "Introduces THE-Tree, a framework for constructing domain-specific evolution trees from scientific literature, improving performance in graph completion, predicting future developments, and evaluating scientific papers.", "motivation": "Existing validation methods are inadequate because LLMs hallucinate and lack domain knowledge, while traditional citation networks lack explicit causality and narrative surveys are unstructured. There is an absence of structured, verifiable, and causally-linked historical data of scientific evolution.", "method": "A computational framework that constructs domain-specific evolution trees from scientific literature. It employs a search algorithm with a novel Think-Verbalize-Cite-Verify process and validates evolutionary links using a natural language inference mechanism.", "result": "Constructed and validated 88 THE-Trees across diverse domains and released a benchmark dataset including up to 71k fact verifications covering 27k papers. Achieved performance improvements in graph completion, predicting future scientific developments, and evaluating important scientific papers.", "conclusion": "THE-Tree improves hit@1 in graph completion by 8% to 14%, predicts future scientific developments with nearly 10% improvement in hit@1, and boosts performance of evaluating important scientific papers by almost 100%."}}
{"id": "2506.21695", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21695", "abs": "https://arxiv.org/abs/2506.21695", "authors": ["Oron Nir", "Jay Tenenbaum", "Ariel Shamir"], "title": "Unimodal Strategies in Density-Based Clustering", "comment": null, "summary": "Density-based clustering methods often surpass centroid-based counterparts,\nwhen addressing data with noise or arbitrary data distributions common in\nreal-world problems. In this study, we reveal a key property intrinsic to\ndensity-based clustering methods regarding the relation between the number of\nclusters and the neighborhood radius of core points - we empirically show that\nit is nearly unimodal, and support this claim theoretically in a specific\nsetting. We leverage this property to devise new strategies for finding\nappropriate values for the radius more efficiently based on the Ternary Search\nalgorithm. This is especially important for large scale data that is\nhigh-dimensional, where parameter tuning is computationally intensive. We\nvalidate our methodology through extensive applications across a range of\nhigh-dimensional, large-scale NLP, Audio, and Computer Vision tasks,\ndemonstrating its practical effectiveness and robustness. This work not only\noffers a significant advancement in parameter control for density-based\nclustering but also broadens the understanding regarding the relations between\ntheir guiding parameters. Our code is available at\nhttps://github.com/oronnir/UnimodalStrategies.", "AI": {"tldr": "This paper introduces a new approach to efficiently determine the neighborhood radius in density-based clustering by exploiting the unimodal relationship between the number of clusters and the radius. The approach is validated on large-scale, high-dimensional data across NLP, Audio, and Computer Vision tasks.", "motivation": "Density-based clustering methods often surpass centroid-based counterparts, when addressing data with noise or arbitrary data distributions common in real-world problems. In this study, we reveal a key property intrinsic to density-based clustering methods regarding the relation between the number of clusters and the neighborhood radius of core points - we empirically show that it is nearly unimodal, and support this claim theoretically in a specific setting.", "method": "We leverage the unimodal property to devise new strategies for finding appropriate values for the radius more efficiently based on the Ternary Search algorithm.", "result": "We validate our methodology through extensive applications across a range of high-dimensional, large-scale NLP, Audio, and Computer Vision tasks, demonstrating its practical effectiveness and robustness.", "conclusion": "This work offers a significant advancement in parameter control for density-based clustering and also broadens the understanding regarding the relations between their guiding parameters."}}
{"id": "2506.21593", "categories": ["cs.IR", "cs.DB"], "pdf": "https://arxiv.org/pdf/2506.21593", "abs": "https://arxiv.org/abs/2506.21593", "authors": ["Abu Hanif Muhammad Syarubany", "Chang Dong Yoo"], "title": "PentaRAG: Large-Scale Intelligent Knowledge Retrieval for Enterprise LLM Applications", "comment": "Annual Conference of The Institute of Electronics and Information\n  Engineers", "summary": "Enterprise deployments of large-language model (LLM) demand continuously\nchanging document collections with sub-second latency and predictable GPU cost\nrequirements that classical Retrieval-Augmented Generation (RAG) pipelines only\npartially satisfy. We present PentaRAG, a five-layer module that routes each\nquery through two instant caches (fixed key-value and semantic), a\nmemory-recall mode that exploits the LLM's own weights, an adaptive session\nmemory, and a conventional retrieval-augmentation layer. Implemented with\nMistral-8B, Milvus and vLLM, the system can answer most repeated or\nsemantically similar questions from low-latency caches while retaining full\nretrieval for novel queries. On the TriviaQA domain, LoRA fine-tuning combined\nwith the memory-recall layer raises answer similarity by approximately 8% and\nfactual correctness by approximately 16% over the base model. Under a\nnine-session runtime simulation, cache warming reduces mean latency from\nseveral seconds to well below one second and shifts traffic toward the fast\npaths. Resource-efficiency tests show that PentaRAG cuts average GPU time to\n0.248 seconds per query, roughly half that of a naive RAG baseline, and\nsustains an aggregate throughput of approximately 100,000 queries per second on\nour setup. These results demonstrate that a layered routing strategy can\ndeliver freshness, speed, and efficiency simultaneously in production-grade RAG\nsystems.", "AI": {"tldr": "PentaRAG, a five-layer module, addresses the demands of enterprise LLM deployments by improving speed and efficiency in RAG systems.", "motivation": "Enterprise deployments of large-language model (LLM) demand continuously changing document collections with sub-second latency and predictable GPU cost requirements that classical Retrieval-Augmented Generation (RAG) pipelines only partially satisfy.", "method": "a five-layer module that routes each query through two instant caches (fixed key-value and semantic), a memory-recall mode that exploits the LLM's own weights, an adaptive session memory, and a conventional retrieval-augmentation layer", "result": "cache warming reduces mean latency from several seconds to well below one second and shifts traffic toward the fast paths. Resource-efficiency tests show that PentaRAG cuts average GPU time to 0.248 seconds per query, roughly half that of a naive RAG baseline, and sustains an aggregate throughput of approximately 100,000 queries per second", "conclusion": " layered routing strategy can deliver freshness, speed, and efficiency simultaneously in production-grade RAG systems."}}
{"id": "2506.21731", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21731", "abs": "https://arxiv.org/abs/2506.21731", "authors": ["Chenqiu Zhao", "Anup Basu"], "title": "Exploring Image Generation via Mutually Exclusive Probability Spaces and Local Correlation Hypothesis", "comment": null, "summary": "We propose two theoretical frameworks, the Mutually Exclusive Probability\nSpace (MESP) and the Local Correlation Hypothesis (LCH), to explore a potential\nlimitation in probabilistic generative models; namely that learning global\ndistributions leads to memorization rather than generative behavior. MESP\nemerges from our rethinking of the Variational Autoencoder (VAE). We observe\nthat latent variable distributions in VAE exhibit overlap, which leads to an\noptimization conflict between the reconstruction loss and KL-divergence loss. A\nlower bound based on the overlap coefficient is proposed. We refer to this\nphenomenon as Mutually Exclusive Probability Spaces. Based on MESP, a Binary\nLatent Autoencoder (BL-AE) is proposed to encode images into binary latent\nrepresentations. These binary latents are used as the input to our\nAutoregressive Random Variable Model (ARVM), a modified autoregressive model\noutputting histograms. Our ARVM achieves competitive FID scores, outperforming\nstate-of-the-art methods on standard datasets. However, such scores reflect\nmemorization rather than generation. To address this issue, we propose the\nLocal Correlation Hypothesis (LCH), which posits that generative capability\narising from local correlations among latent variables. Comprehensive\nexperiments and discussions are conducted to validate our frameworks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86MESP\u548cLCH\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u6982\u7387\u751f\u6210\u6a21\u578b\u4e2d\u5168\u5c40\u5206\u5e03\u5b66\u4e60\u5bfc\u81f4\u7684\u8bb0\u5fc6\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u6846\u67b6\u3002", "motivation": "\u6982\u7387\u751f\u6210\u6a21\u578b\u5728\u5b66\u4e60\u5168\u5c40\u5206\u5e03\u65f6\u53ef\u80fd\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5bfc\u81f4\u8bb0\u5fc6\u800c\u975e\u751f\u6210\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u4e86\u4e92\u65a5\u6982\u7387\u7a7a\u95f4\uff08MESP\uff09\u548c\u5c40\u90e8\u76f8\u5173\u6027\u5047\u8bbe\uff08LCH\uff09\u4e24\u79cd\u7406\u8bba\u6846\u67b6\u3002\u57fa\u4e8eMESP\uff0c\u63d0\u51fa\u4e86\u4e8c\u5143\u6f5c\u5728\u81ea\u7f16\u7801\u5668\uff08BL-AE\uff09\u548c\u81ea\u56de\u5f52\u968f\u673a\u53d8\u91cf\u6a21\u578b\uff08ARVM\uff09\u3002", "result": "ARVM\u5728\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684FID\u5206\u6570\uff0c\u8d85\u8fc7\u4e86\u73b0\u6709\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u5206\u6570\u53cd\u6620\u7684\u662f\u8bb0\u5fc6\u800c\u4e0d\u662f\u751f\u6210\u3002", "conclusion": "\u751f\u6210\u6a21\u578b\u4e2d\u7684\u5168\u5c40\u5206\u5e03\u5b66\u4e60\u53ef\u80fd\u5bfc\u81f4\u8bb0\u5fc6\u800c\u975e\u751f\u6210\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u6846\u67b6\u3002"}}
{"id": "2506.21558", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21558", "abs": "https://arxiv.org/abs/2506.21558", "authors": ["FutureSearch", ":", "Jack Wildman", "Nikos I. Bosse", "Daniel Hnyk", "Peter M\u00fchlbacher", "Finn Hambly", "Jon Evans", "Dan Schwarz", "Lawrence Phillips"], "title": "Bench to the Future: A Pastcasting Benchmark for Forecasting Agents", "comment": null, "summary": "Forecasting is a challenging task that offers a clearly measurable way to\nstudy AI systems. Forecasting requires a large amount of research on the\ninternet, and evaluations require time for events to happen, making the\ndevelopment of forecasting benchmarks challenging. To date, no forecasting\nbenchmark provides a realistic, hermetic, and repeatable environment for LLM\nforecasters. We introduce Bench To the Future (BTF), a \"pastcasting\" benchmark\nwith hundreds of high-quality questions for which the resolution is already\nknown. Each question is accompanied by a large offline corpus of tens of\nthousands of relevant web pages, enabling a way to elicit realistic \"forecasts\"\non past events from LLMs. Results suggest that our pastcasting environment can\nproduce results comparable to those based on forecasts using the internet on\nat-the-time unresolved questions. We show results benchmarking agent and\nchain-of-thought forecasting approaches using several LLMs, including the\nrecently-released Claude 4 models, and demonstrate BTF's ability to track\nsteady forecasting capability progress over time. We intend this to be a living\nbenchmark, with new questions added continually to account for increasing\ntraining data cutoff dates. We invite researchers to contact us at\nhello@futuresearch.ai to utilize our benchmark or tooling for their own\nresearch.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aBTF\u7684\u201c\u8fc7\u53bb\u9884\u6d4b\u201d\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u9884\u6d4b\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u5b83\u63d0\u4f9b\u4e86\u4e00\u79cd\u6e05\u6670\u53ef\u8861\u91cf\u7684\u65b9\u6cd5\u6765\u7814\u7a76\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u3002\u9884\u6d4b\u9700\u8981\u5728\u4e92\u8054\u7f51\u4e0a\u8fdb\u884c\u5927\u91cf\u7814\u7a76\uff0c\u5e76\u4e14\u8bc4\u4f30\u9700\u8981\u65f6\u95f4\u624d\u80fd\u53d1\u751f\uff0c\u8fd9\u4f7f\u5f97\u9884\u6d4b\u57fa\u51c6\u7684\u5f00\u53d1\u5177\u6709\u6311\u6218\u6027\u3002\u8fc4\u4eca\u4e3a\u6b62\uff0c\u8fd8\u6ca1\u6709\u9884\u6d4b\u57fa\u51c6\u4e3aLLM\u9884\u6d4b\u5668\u63d0\u4f9b\u771f\u5b9e\u3001\u5c01\u95ed\u548c\u53ef\u91cd\u590d\u7684\u73af\u5883\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u201c\u8fc7\u53bb\u9884\u6d4b\u201d\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5176\u4e2d\u5305\u542b\u6570\u767e\u4e2a\u9ad8\u8d28\u91cf\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u95ee\u9898\u7684\u7b54\u6848\u662f\u5df2\u77e5\u7684\uff0c\u6bcf\u4e2a\u95ee\u9898\u90fd\u9644\u5e26\u6709\u6570\u4e07\u4e2a\u76f8\u5173\u7684\u7f51\u9875\u79bb\u7ebf\u8bed\u6599\u5e93\u3002", "result": "\u8fc7\u53bb\u9884\u6d4b\u73af\u5883\u53ef\u4ee5\u4ea7\u751f\u4e0e\u57fa\u4e8e\u5f53\u65f6\u672a\u89e3\u51b3\u95ee\u9898\u7684\u4e92\u8054\u7f51\u9884\u6d4b\u7ed3\u679c\u76f8\u5f53\u7684\u7ed3\u679c\u3002\u4f7f\u7528\u5305\u62ec\u6700\u8fd1\u53d1\u5e03\u7684Claude 4\u6a21\u578b\u5728\u5185\u7684\u591a\u4e2aLLM\uff0c\u5c55\u793a\u4e86\u4ee3\u7406\u548c\u601d\u7ef4\u94fe\u9884\u6d4b\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86BTF\u80fd\u591f\u8ddf\u8e2a\u9884\u6d4b\u80fd\u529b\u7684\u6301\u7eed\u63d0\u5347\u3002", "conclusion": "BTF\u80fd\u591f\u8ddf\u8e2a\u9884\u6d4b\u80fd\u529b\u7684\u6301\u7eed\u63d0\u5347\uff0c\u5e76\u4e14\u4f1a\u4e0d\u65ad\u589e\u52a0\u65b0\u7684\u95ee\u9898\u3002"}}
{"id": "2506.21784", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21784", "abs": "https://arxiv.org/abs/2506.21784", "authors": ["Yifan Liu", "Xishun Liao", "Haoxuan Ma", "Jonathan Liu", "Rohan Jadhav", "Jiaqi Ma"], "title": "MobiVerse: Scaling Urban Mobility Simulation with Hybrid Lightweight Domain-Specific Generator and Large Language Models", "comment": null, "summary": "Understanding and modeling human mobility patterns is crucial for effective\ntransportation planning and urban development. Despite significant advances in\nmobility research, there remains a critical gap in simulation platforms that\nallow for algorithm development, policy implementation, and comprehensive\nevaluation at scale. Traditional activity-based models require extensive data\ncollection and manual calibration, machine learning approaches struggle with\nadaptation to dynamic conditions, and treding agent-based Large Language Models\n(LLMs) implementations face computational constraints with large-scale\nsimulations. To address these challenges, we propose MobiVerse, a hybrid\nframework leverages the efficiency of lightweight domain-specific generator for\ngenerating base activity chains with the adaptability of LLMs for context-aware\nmodifications. A case study was conducted in Westwood, Los Angeles, where we\nefficiently generated and dynamically adjusted schedules for the whole\npopulation of approximately 53,000 agents on a standard PC. Our experiments\ndemonstrate that MobiVerse successfully enables agents to respond to\nenvironmental feedback, including road closures, large gathering events like\nfootball games, and congestion, through our hybrid framework. Its modular\ndesign facilitates testing various mobility algorithms at both transportation\nsystem and agent levels. Results show our approach maintains computational\nefficiency while enhancing behavioral realism. MobiVerse bridges the gap in\nmobility simulation by providing a customizable platform for mobility systems\nplanning and operations with benchmark algorithms. Code and videos are\navailable at https://github.com/ucla-mobility/MobiVerse.", "AI": {"tldr": "MobiVerse \u662f\u4e00\u4e2a\u6df7\u5408\u6846\u67b6\uff0c\u5b83\u5229\u7528\u8f7b\u91cf\u7ea7\u9886\u57df\u7279\u5b9a\u751f\u6210\u5668\u7684\u6548\u7387\u548c\u6cd5\u5b66\u7855\u58eb\u7684\u9002\u5e94\u6027\uff0c\u4ee5\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4fee\u6539\uff0c\u4ece\u800c\u5f25\u5408\u4e86\u79fb\u52a8\u6a21\u62df\u4e2d\u7684\u5dee\u8ddd\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7528\u4e8e\u79fb\u52a8\u7cfb\u7edf\u89c4\u5212\u548c\u8fd0\u8425\u7684\u53ef\u5b9a\u5236\u5e73\u53f0\uff0c\u5e76\u5177\u6709\u57fa\u51c6\u7b97\u6cd5\u3002", "motivation": "\u4e86\u89e3\u548c\u5efa\u6a21\u4eba\u7c7b\u79fb\u52a8\u6a21\u5f0f\u5bf9\u4e8e\u6709\u6548\u7684\u4ea4\u901a\u89c4\u5212\u548c\u57ce\u5e02\u53d1\u5c55\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u79fb\u52a8\u7814\u7a76\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u4f46\u5728\u6a21\u62df\u5e73\u53f0\u4e2d\u4ecd\u7136\u5b58\u5728\u4e00\u4e2a\u5173\u952e\u7684\u5dee\u8ddd\uff0c\u8be5\u5e73\u53f0\u5141\u8bb8\u5927\u89c4\u6a21\u5730\u8fdb\u884c\u7b97\u6cd5\u5f00\u53d1\u3001\u653f\u7b56\u5b9e\u65bd\u548c\u7efc\u5408\u8bc4\u4f30\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86 MobiVerse\uff0c\u8fd9\u662f\u4e00\u4e2a\u6df7\u5408\u6846\u67b6\uff0c\u5b83\u5229\u7528\u8f7b\u91cf\u7ea7\u9886\u57df\u7279\u5b9a\u751f\u6210\u5668\u7684\u6548\u7387\u6765\u751f\u6210\u57fa\u672c\u6d3b\u52a8\u94fe\uff0c\u5e76\u5229\u7528\u6cd5\u5b66\u7855\u58eb\u7684\u9002\u5e94\u6027\u6765\u8fdb\u884c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4fee\u6539\u3002", "result": "\u5728\u6d1b\u6749\u77f6\u897f\u6728\u533a\u8fdb\u884c\u4e86\u4e00\u9879\u6848\u4f8b\u7814\u7a76\uff0c\u6211\u4eec\u5728\u6807\u51c6 PC \u4e0a\u4e3a\u5927\u7ea6 53,000 \u540d\u667a\u80fd\u4f53\u7684\u5168\u4f53\u4eba\u53e3\u9ad8\u6548\u5730\u751f\u6210\u5e76\u52a8\u6001\u8c03\u6574\u4e86\u65f6\u95f4\u8868\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMobiVerse \u6210\u529f\u5730\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u901a\u8fc7\u6211\u4eec\u7684\u6df7\u5408\u6846\u67b6\u54cd\u5e94\u73af\u5883\u53cd\u9988\uff0c\u5305\u62ec\u9053\u8def\u5c01\u95ed\u3001\u5927\u578b\u805a\u4f1a\u6d3b\u52a8\uff08\u5982\u8db3\u7403\u6bd4\u8d5b\uff09\u548c\u62e5\u5835\u3002", "conclusion": "MobiVerse \u6210\u529f\u5730\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u901a\u8fc7\u6df7\u5408\u6846\u67b6\u54cd\u5e94\u73af\u5883\u53cd\u9988\uff0c\u5305\u62ec\u9053\u8def\u5c01\u95ed\u3001\u5927\u578b\u805a\u4f1a\u6d3b\u52a8\uff08\u5982\u8db3\u7403\u6bd4\u8d5b\uff09\u548c\u62e5\u5835\u3002\u5176\u6a21\u5757\u5316\u8bbe\u8ba1\u6709\u52a9\u4e8e\u5728\u4ea4\u901a\u7cfb\u7edf\u548c\u667a\u80fd\u4f53\u5c42\u9762\u6d4b\u8bd5\u5404\u79cd\u79fb\u52a8\u7b97\u6cd5\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\uff0c\u589e\u5f3a\u4e86\u884c\u4e3a\u7684\u771f\u5b9e\u6027\u3002"}}
{"id": "2506.21714", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21714", "abs": "https://arxiv.org/abs/2506.21714", "authors": ["Denis Gudovskiy", "Wenzhao Zheng", "Tomoyuki Okuno", "Yohei Nakata", "Kurt Keutzer"], "title": "$\\textrm{ODE}_t \\left(\\textrm{ODE}_l \\right)$: Shortcutting the Time and Length in Diffusion and Flow Models for Faster Sampling", "comment": "Preprint. Github page: github.com/gudovskiy/odelt", "summary": "Recently, continuous normalizing flows (CNFs) and diffusion models (DMs) have\nbeen studied using the unified theoretical framework. Although such models can\ngenerate high-quality data points from a noise distribution, the sampling\ndemands multiple iterations to solve an ordinary differential equation (ODE)\nwith high computational complexity. Most existing methods focus on reducing the\nnumber of time steps during the sampling process to improve efficiency. In this\nwork, we explore a complementary direction in which the quality-complexity\ntradeoff can be dynamically controlled in terms of time steps and in the length\nof the neural network. We achieve this by rewiring the blocks in the\ntransformer-based architecture to solve an inner discretized ODE w.r.t. its\nlength. Then, we employ time- and length-wise consistency terms during flow\nmatching training, and as a result, the sampling can be performed with an\narbitrary number of time steps and transformer blocks. Unlike others, our\n$\\textrm{ODE}_t \\left(\\textrm{ODE}_l \\right)$ approach is solver-agnostic in\ntime dimension and decreases both latency and memory usage. Compared to the\nprevious state of the art, image generation experiments on CelebA-HQ and\nImageNet show a latency reduction of up to $3\\times$ in the most efficient\nsampling mode, and a FID score improvement of up to $3.5$ points for\nhigh-quality sampling. We release our code and model weights with fully\nreproducible experiments.", "AI": {"tldr": "This paper introduces a new approach, ODE_t(ODE_l), for improving the efficiency of CNFs and DMs by dynamically controlling the quality-complexity tradeoff, reducing latency and memory usage in image generation tasks.", "motivation": "Existing CNFs and DMs require multiple iterations to solve an ODE with high computational complexity, and most methods focus on reducing time steps. This work explores dynamically controlling the quality-complexity tradeoff in terms of time steps and network length.", "method": "The authors rewire blocks in a transformer-based architecture to solve an inner discretized ODE with respect to its length and employ time- and length-wise consistency terms during flow matching training.", "result": "Experiments on CelebA-HQ and ImageNet show up to 3x latency reduction and a 3.5 FID score improvement compared to previous state-of-the-art methods.", "conclusion": "The proposed ODE_t(ODE_l) approach reduces latency and memory usage, achieving up to 3x latency reduction and 3.5 FID score improvement compared to previous state-of-the-art methods on CelebA-HQ and ImageNet."}}
{"id": "2506.22199", "categories": ["cs.LG", "cs.DB"], "pdf": "https://arxiv.org/pdf/2506.22199", "abs": "https://arxiv.org/abs/2506.22199", "authors": ["Jakub Pele\u0161ka", "Gustav \u0160\u00edr"], "title": "REDELEX: A Framework for Relational Deep Learning Exploration", "comment": "Accepted to ECMLPKDD 2025 at Porto, Portugal", "summary": "Relational databases (RDBs) are widely regarded as the gold standard for\nstoring structured information. Consequently, predictive tasks leveraging this\ndata format hold significant application promise. Recently, Relational Deep\nLearning (RDL) has emerged as a novel paradigm wherein RDBs are conceptualized\nas graph structures, enabling the application of various graph neural\narchitectures to effectively address these tasks. However, given its novelty,\nthere is a lack of analysis into the relationships between the performance of\nvarious RDL models and the characteristics of the underlying RDBs.\n  In this study, we present REDELEX$-$a comprehensive exploration framework for\nevaluating RDL models of varying complexity on the most diverse collection of\nover 70 RDBs, which we make available to the community. Benchmarked alongside\nkey representatives of classic methods, we confirm the generally superior\nperformance of RDL while providing insights into the main factors shaping\nperformance, including model complexity, database sizes and their structural\nproperties.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86REDELEX\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30RDL\u6a21\u578b\u5728\u5404\u79cdRDB\u4e0a\u7684\u6027\u80fd\uff0c\u7ed3\u679c\u8868\u660eRDL\u901a\u5e38\u4f18\u4e8e\u7ecf\u5178\u65b9\u6cd5\uff0c\u5e76\u4e14\u6a21\u578b\u590d\u6742\u6027\u3001\u6570\u636e\u5e93\u5927\u5c0f\u548c\u7ed3\u6784\u5c5e\u6027\u662f\u5f71\u54cd\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\u3002", "motivation": "\u5173\u7cfb\u6570\u636e\u5e93\uff08RDB\uff09\u88ab\u5e7f\u6cdb\u8ba4\u4e3a\u662f\u5b58\u50a8\u7ed3\u6784\u5316\u4fe1\u606f\u7684\u9ec4\u91d1\u6807\u51c6\u3002\u56e0\u6b64\uff0c\u5229\u7528\u8fd9\u79cd\u6570\u636e\u683c\u5f0f\u7684\u9884\u6d4b\u4efb\u52a1\u5177\u6709\u91cd\u8981\u7684\u5e94\u7528\u524d\u666f\u3002\u6700\u8fd1\uff0c\u5173\u7cfb\u6df1\u5ea6\u5b66\u4e60\uff08RDL\uff09\u4f5c\u4e3a\u4e00\u79cd\u65b0\u7684\u8303\u4f8b\u51fa\u73b0\uff0c\u5176\u4e2dRDB\u88ab\u6982\u5ff5\u5316\u4e3a\u56fe\u7ed3\u6784\uff0c\u4ece\u800c\u80fd\u591f\u5e94\u7528\u5404\u79cd\u56fe\u795e\u7ecf\u67b6\u6784\u6765\u6709\u6548\u89e3\u51b3\u8fd9\u4e9b\u4efb\u52a1\u3002\u7136\u800c\uff0c\u9274\u4e8e\u5176\u65b0\u9896\u6027\uff0c\u7f3a\u4e4f\u5bf9\u5404\u79cdRDL\u6a21\u578b\u7684\u6027\u80fd\u4e0e\u5e95\u5c42RDB\u7684\u7279\u6027\u4e4b\u95f4\u5173\u7cfb\u7684\u5206\u6790\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86REDELEX\u2014\u2014\u4e00\u4e2a\u7efc\u5408\u63a2\u7d22\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u8d85\u8fc770\u4e2aRDB\u4e0a\u4e0d\u540c\u590d\u6742\u5ea6\u7684RDL\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u4e0e\u7ecf\u5178\u65b9\u6cd5\u7684\u5173\u952e\u4ee3\u8868\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6211\u4eec\u786e\u8ba4\u4e86RDL\u901a\u5e38\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u5bf9\u5f71\u54cd\u6027\u80fd\u7684\u4e3b\u8981\u56e0\u7d20\u7684\u89c1\u89e3\uff0c\u5305\u62ec\u6a21\u578b\u590d\u6742\u6027\u3001\u6570\u636e\u5e93\u5927\u5c0f\u53ca\u5176\u7ed3\u6784\u5c5e\u6027\u3002", "conclusion": "RDL\u8868\u73b0\u901a\u5e38\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u4e14\u6a21\u578b\u590d\u6742\u6027\u3001\u6570\u636e\u5e93\u5927\u5c0f\u53ca\u5176\u7ed3\u6784\u5c5e\u6027\u662f\u5f71\u54cd\u6027\u80fd\u7684\u4e3b\u8981\u56e0\u7d20\u3002"}}
{"id": "2506.21598", "categories": ["cs.IR", "stat.ME", "H.3.3; I.2.6"], "pdf": "https://arxiv.org/pdf/2506.21598", "abs": "https://arxiv.org/abs/2506.21598", "authors": ["Purak Jain", "Sandeep Appala"], "title": "SERP Interference Network and Its Applications in Search Advertising", "comment": "This is an extended version of our paper published at the AdKDD 2024\n  workshop, co-located with ACM KDD. CEUR-WS proceedings:\n  https://ceur-ws.org/Vol-3837/paper_12_ceur_paper.pdf", "summary": "Search Engine marketing teams in the e-commerce industry manage global search\nengine traffic to their websites with the aim to optimize long-term\nprofitability by delivering the best possible customer experience on Search\nEngine Results Pages (SERPs). In order to do so, they need to run continuous\nand rapid Search Marketing A/B tests to continuously evolve and improve their\nproducts. However, unlike typical e-commerce A/B tests that can randomize based\non customer identification, their tests face the challenge of anonymized users\non search engines. On the other hand, simply randomizing on products violates\nStable Unit Treatment Value Assumption for most treatments of interest. In this\nwork, we propose leveraging censored observational data to construct bipartite\n(Search Query to Product Ad or Text Ad) SERP interference networks. Using a\nnovel weighting function, we create weighted projections to form unipartite\ngraphs which can then be use to create clusters to randomized on. We\ndemonstrate this experimental design's application in evaluating a new bidding\nalgorithm for Paid Search. Additionally, we provide a blueprint of a novel\nsystem architecture utilizing SageMaker which enables polyglot programming to\nimplement each component of the experimental framework.", "AI": {"tldr": "This paper presents a new A/B testing method for search engine marketing that addresses challenges with user anonymization and interference between ads, using network analysis and a SageMaker-based system.", "motivation": "Search Engine Marketing teams need to run continuous A/B tests to optimize profitability and customer experience on SERPs. However, they face challenges with anonymized users and violations of the Stable Unit Treatment Value Assumption.", "method": "The paper proposes constructing bipartite (Search Query to Product Ad or Text Ad) SERP interference networks, using a novel weighting function to create weighted projections, and forming unipartite graphs for clustering and randomization.", "result": "The paper demonstrates the application of the experimental design in evaluating a new bidding algorithm for Paid Search. It also provides a blueprint of a novel system architecture utilizing SageMaker.", "conclusion": "This paper introduces a novel experimental design leveraging censored observational data to construct bipartite SERP interference networks for A/B testing in search engine marketing. It demonstrates the application of this design in evaluating a new bidding algorithm and provides a system architecture blueprint using SageMaker."}}
{"id": "2506.21735", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21735", "abs": "https://arxiv.org/abs/2506.21735", "authors": ["Nick Lemke", "Mirko Konstantin", "Henry John Krumb", "John Kalkhof", "Jonathan Stieber", "Anirban Mukhopadhyay"], "title": "Equitable Federated Learning with NCA", "comment": null, "summary": "Federated Learning (FL) is enabling collaborative model training across\ninstitutions without sharing sensitive patient data. This approach is\nparticularly valuable in low- and middle-income countries (LMICs), where access\nto trained medical professionals is limited. However, FL adoption in LMICs\nfaces significant barriers, including limited high-performance computing\nresources and unreliable internet connectivity. To address these challenges, we\nintroduce FedNCA, a novel FL system tailored for medical image segmentation\ntasks. FedNCA leverages the lightweight Med-NCA architecture, enabling training\non low-cost edge devices, such as widely available smartphones, while\nminimizing communication costs. Additionally, our encryption-ready FedNCA\nproves to be suitable for compromised network communication. By overcoming\ninfrastructural and security challenges, FedNCA paves the way for inclusive,\nefficient, lightweight, and encryption-ready medical imaging solutions,\nfostering equitable healthcare advancements in resource-constrained regions.", "AI": {"tldr": "FedNCA: A novel FL system tailored for medical image segmentation tasks in low-resource settings. It leverages the lightweight Med-NCA architecture, enabling training on low-cost edge devices, such as widely available smartphones, while minimizing communication costs. Additionally, our encryption-ready FedNCA proves to be suitable for compromised network communication.", "motivation": "FL adoption in LMICs faces significant barriers, including limited high-performance computing resources and unreliable internet connectivity. To address these challenges", "method": "FedNCA leverages the lightweight Med-NCA architecture, enabling training on low-cost edge devices, such as widely available smartphones, while minimizing communication costs. Additionally, our encryption-ready FedNCA proves to be suitable for compromised network communication.", "result": "introduce FedNCA, a novel FL system tailored for medical image segmentation tasks", "conclusion": "FedNCA paves the way for inclusive, efficient, lightweight, and encryption-ready medical imaging solutions, fostering equitable healthcare advancements in resource-constrained regions."}}
{"id": "2506.21559", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21559", "abs": "https://arxiv.org/abs/2506.21559", "authors": ["Junze Chen", "Cheng Yang", "Shujie Li", "Zhiqiang Zhang", "Yawen Li", "Junping Du", "Chuan Shi"], "title": "GraphLAMA: Enabling Efficient Adaptation of Graph Language Models with Limited Annotations", "comment": null, "summary": "Large language models (LLMs) have demonstrated their strong capabilities in\nvarious domains, and have been recently integrated for graph analysis as graph\nlanguage models (GLMs). With LLMs as the predictor, some GLMs can interpret\nunseen tasks described by natural language, and learn from a few examples in\nthe prompts without parameter tuning, known as in-context learning (ICL).\nAnother subset of GLMs utilizes abundant training labels to enhance model\nperformance, known as instruction tuning. However, we argue that ICL on graphs\nhas effectiveness issues due to fixed parameters and efficiency issues due to\nlong context. Meanwhile, the large amount of labeled data required for\ninstruction tuning can be difficult to obtain in real-world scenarios. To this\nend, we aim to introduce an extra parameter adaptation stage that can\nefficiently tailor GLMs to an unseen graph and task with only a few labeled\nexamples, in exchange for better prediction accuracy and faster inference\nspeed. For implementation, in this paper we propose GraphLAMA method, with its\nmodel backbone and learning schemes specialized for efficient tuning and\ninference. Specifically, for model backbone, we use a graph neural network\n(GNN) with several well-designed components to transform nodes into the\nrepresentation space of LLM tokens. Task instructions can then be represented\nas a mixture of node and language tokens. In the pre-training stage, model\nparameters except the LLM will be trained with different tasks to capture\ngeneral knowledge. In the adaptation stage, only a few pre-trained parameters\nwill be updated based on few-shot examples. Extensive experiments on\nfew/zero-shot node classification and summary generation show that our proposed\nGraphLAMA achieves state-of-the-art performance with 4.91% absolution\nimprovement in accuracy. Compared with ICL, our inference speed can be 10 times\nfaster under 5-shot setting.", "AI": {"tldr": "GraphLAMA introduces an extra parameter adaptation stage that can efficiently tailor GLMs to an unseen graph and task with only a few labeled examples, in exchange for better prediction accuracy and faster inference speed.", "motivation": "ICL on graphs has effectiveness issues due to fixed parameters and efficiency issues due to long context. Meanwhile, the large amount of labeled data required for instruction tuning can be difficult to obtain in real-world scenarios. To this end, we aim to introduce an extra parameter adaptation stage that can efficiently tailor GLMs to an unseen graph and task with only a few labeled examples, in exchange for better prediction accuracy and faster inference speed.", "method": "propose GraphLAMA method, with its model backbone and learning schemes specialized for efficient tuning and inference. Specifically, for model backbone, we use a graph neural network (GNN) with several well-designed components to transform nodes into the representation space of LLM tokens. Task instructions can then be represented as a mixture of node and language tokens. In the pre-training stage, model parameters except the LLM will be trained with different tasks to capture general knowledge. In the adaptation stage, only a few pre-trained parameters will be updated based on few-shot examples.", "result": "achieves state-of-the-art performance with 4.91% absolution improvement in accuracy. Compared with ICL, our inference speed can be 10 times faster under 5-shot setting.", "conclusion": "The proposed GraphLAMA achieves state-of-the-art performance with 4.91% absolution improvement in accuracy. Compared with ICL, the inference speed can be 10 times faster under 5-shot setting."}}
{"id": "2506.21805", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21805", "abs": "https://arxiv.org/abs/2506.21805", "authors": ["Nicolas Bougie", "Narimasa Watanabe"], "title": "CitySim: Modeling Urban Behaviors and City Dynamics with Large-Scale LLM-Driven Agent Simulation", "comment": null, "summary": "Modeling human behavior in urban environments is fundamental for social\nscience, behavioral studies, and urban planning. Prior work often rely on\nrigid, hand-crafted rules, limiting their ability to simulate nuanced\nintentions, plans, and adaptive behaviors. Addressing these challenges, we\nenvision an urban simulator (CitySim), capitalizing on breakthroughs in\nhuman-level intelligence exhibited by large language models. In CitySim, agents\ngenerate realistic daily schedules using a recursive value-driven approach that\nbalances mandatory activities, personal habits, and situational factors. To\nenable long-term, lifelike simulations, we endow agents with beliefs, long-term\ngoals, and spatial memory for navigation. CitySim exhibits closer alignment\nwith real humans than prior work, both at micro and macro levels. Additionally,\nwe conduct insightful experiments by modeling tens of thousands of agents and\nevaluating their collective behaviors under various real-world scenarios,\nincluding estimating crowd density, predicting place popularity, and assessing\nwell-being. Our results highlight CitySim as a scalable, flexible testbed for\nunderstanding and forecasting urban phenomena.", "AI": {"tldr": "CitySim, an urban simulator powered by large language models, simulates realistic human behavior with nuanced intentions, long-term goals, and spatial memory, outperforming prior methods in aligning with real-world human behavior and offering a scalable testbed for urban phenomena.", "motivation": "Prior work often relies on rigid, hand-crafted rules, limiting their ability to simulate nuanced intentions, plans, and adaptive behaviors in urban environments.", "method": "A recursive value-driven approach is used to generate realistic daily schedules, balancing mandatory activities, personal habits, and situational factors. Agents are endowed with beliefs, long-term goals, and spatial memory for navigation to enable long-term simulations.", "result": "CitySim exhibits closer alignment with real humans than prior work at both micro and macro levels. Experiments modeling tens of thousands of agents demonstrate its ability to estimate crowd density, predict place popularity, and assess well-being.", "conclusion": "CitySim is a scalable and flexible testbed for understanding and forecasting urban phenomena."}}
{"id": "2506.21718", "categories": ["cs.LG", "cs.AI", "cs.PF", "cs.SE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.21718", "abs": "https://arxiv.org/abs/2506.21718", "authors": ["Yash Akhauri", "Bryan Lewandowski", "Cheng-Hsi Lin", "Adrian N. Reyes", "Grant C. Forbes", "Arissa Wongpanich", "Bangding Yang", "Mohamed S. Abdelfattah", "Sagi Perel", "Xingyou Song"], "title": "Performance Prediction for Large Systems via Text-to-Text Regression", "comment": "Code can be found at https://github.com/google-deepmind/regress-lm", "summary": "In many industries, predicting metric outcomes of large systems is a\nfundamental problem, driven largely by traditional tabular regression. However,\nsuch methods struggle on complex systems data in the wild such as configuration\nfiles or system logs, where feature engineering is often infeasible. We propose\ntext-to-text regression as a general, scalable alternative. For predicting\nresource efficiency on Borg, Google's massive compute cluster scheduling\nsystem, a 60M parameter encoder-decoder, trained from random initialization,\nachieves up to a near perfect 0.99 (0.9 average) rank correlation across the\nentire fleet, and 100x lower MSE than tabular approaches. The model also easily\nadapts to new tasks in only 500 few-shot examples and captures the densities of\ncomplex outcome distributions. Ablation studies highlight the importance of\nusing encoders, increasing sequence length, and the model's inherent\nuncertainty quantification. These findings pave the way for universal\nsimulators of real-world outcomes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u3001\u53ef\u6269\u5c55\u7684\u6587\u672c\u5230\u6587\u672c\u56de\u5f52\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u5927\u89c4\u6a21\u7cfb\u7edf\u7684\u6307\u6807\u7ed3\u679c\uff0c\u5e76\u5728Google\u7684Borg\u96c6\u7fa4\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u679c\u3002", "motivation": "\u4f20\u7edf\u8868\u683c\u56de\u5f52\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u7684\u7cfb\u7edf\u6570\u636e\uff08\u5982\u914d\u7f6e\u6587\u4ef6\u6216\u7cfb\u7edf\u65e5\u5fd7\uff09\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u7279\u5f81\u5de5\u7a0b\u901a\u5e38\u4e0d\u53ef\u884c\u3002", "method": "\u4f7f\u7528\u4e86\u4e00\u4e2a60M\u53c2\u6570\u7684encoder-decoder\u6a21\u578b\uff0c\u4ece\u968f\u673a\u521d\u59cb\u5316\u5f00\u59cb\u8bad\u7ec3\u3002", "result": "\u8be5\u6a21\u578b\u5728\u6574\u4e2a\u96c6\u7fa4\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe0.99\uff08\u5e73\u57470.9\uff09\u7684\u79e9\u76f8\u5173\u6027\uff0c\u5e76\u4e14MSE\u6bd4\u8868\u683c\u65b9\u6cd5\u4f4e100\u500d\u3002\u8be5\u6a21\u578b\u8fd8\u53ef\u4ee5\u8f7b\u677e\u9002\u5e94\u65b0\u7684\u4efb\u52a1\uff0c\u4ec5\u9700\u5c11\u91cf\u793a\u4f8b\u5373\u53ef\u6355\u83b7\u590d\u6742\u7ed3\u679c\u5206\u5e03\u7684\u5bc6\u5ea6\u3002", "conclusion": "text-to-text regression\u6a21\u578b\u5728\u9884\u6d4bBorg\u7684\u8d44\u6e90\u6548\u7387\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u4f20\u7edf\u8868\u683c\u65b9\u6cd5\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.21599", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21599", "abs": "https://arxiv.org/abs/2506.21599", "authors": ["Peibo Li", "Shuang Ao", "Hao Xue", "Yang Song", "Maarten de Rijke", "Johan Barth\u00e9lemy", "Tomasz Bednarz", "Flora D. Salim"], "title": "Reinforcement Fine-Tuned Large Language Models for Next POI Recommendation", "comment": null, "summary": "Large language models (LLMs) have been adopted for next point-of-interest\n(POI) recommendation tasks. Typical LLM-based recommenders fall into two\ncategories: prompt-based and supervised fine-tuning (SFT)-based models.\nPrompt-based models generally offer greater output flexibility but deliver\nlower accuracy, whereas SFT-based models achieve higher performance yet face a\nfundamental mismatch: next POI recommendation data does not naturally suit\nsupervised fine-tuning. In SFT, the model is trained to reproduce the exact\nground truth, but each training example provides only a single target POI, so\nthere is no ground truth for producing a top-k list.\n  To address this, we propose Refine-POI, a reinforcement fine-tuning framework\nfor next POI recommendation. We introduce recommendation-driven rewards that\nenable LLMs to learn to generate top-k recommendation lists using only one\nground-truth POI per example. Experiments on real-world datasets demonstrate\nthat Refine-POI achieves state-of-the-art top-k recommendation performance.", "AI": {"tldr": "Refine-POI is a reinforcement fine-tuning framework for next POI recommendation that achieves state-of-the-art top-k recommendation performance.", "motivation": "Prompt-based models generally offer greater output flexibility but deliver lower accuracy, whereas SFT-based models achieve higher performance yet face a fundamental mismatch: next POI recommendation data does not naturally suit supervised fine-tuning.", "method": "We introduce recommendation-driven rewards that enable LLMs to learn to generate top-k recommendation lists using only one ground-truth POI per example.", "result": "Experiments on real-world datasets demonstrate that Refine-POI achieves state-of-the-art top-k recommendation performance.", "conclusion": "Refine-POI achieves state-of-the-art top-k recommendation performance."}}
{"id": "2506.21742", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21742", "abs": "https://arxiv.org/abs/2506.21742", "authors": ["Sirnam Swetha", "Rohit Gupta", "Parth Parag Kulkarni", "David G Shatwell", "Jeffrey A Chan Santiago", "Nyle Siddiqui", "Joseph Fioresi", "Mubarak Shah"], "title": "ImplicitQA: Going beyond frames towards Implicit Video Reasoning", "comment": null, "summary": "Video QA has made significant strides by leveraging multimodal learning to\nalign visual and textual modalities. However, current benchmarks overwhelmingly\nfocus on questions answerable through explicit visual content - actions,\nobjects & events directly observable within individual frames or short clips.\nIn contrast, creative and cinematic videos - such as movies, TV shows, and\nnarrative-driven content - employ storytelling techniques that deliberately\nomit certain depictions, requiring viewers to infer motives, causality, and\nrelationships across discontinuous frames. Humans naturally excel at such\nimplicit reasoning, seamlessly integrating information across time and context\nto construct coherent narratives. Current VideoQA systems and benchmarks fail\nto capture this essential dimension of human-like understanding. To bridge this\ngap, we present ImplicitQA, a novel benchmark specifically designed to test\nmodels on implicit reasoning. It comprises 1K meticulously annotated QA pairs\nderived from 320+ high-quality creative video clips, systematically categorized\ninto key reasoning dimensions: lateral and vertical spatial reasoning, depth\nand proximity, viewpoint and visibility, motion and trajectory, causal and\nmotivational reasoning, social interactions, physical context, and inferred\ncounting. These annotations are deliberately challenging, crafted by authors\nensuring high-quality. Our extensive evaluations on leading VideoQA models\nreveals performance degradation, underscoring their reliance on surface-level\nvisual cues and highlighting the difficulty of implicit reasoning. Performance\nvariations across models further illustrate the complexity and diversity of the\nchallenges presented by ImplicitQA. By releasing both the dataset and our data\ncollection framework, we aim to stimulate further research and development in\nthe community. https://huggingface.co/datasets/ucf-crcv/ImplicitQA.", "AI": {"tldr": "This paper introduces ImplicitQA, a new benchmark for video QA that tests implicit reasoning, which current models struggle with.", "motivation": "Current VideoQA benchmarks focus on explicit visual content, neglecting implicit reasoning which is crucial for understanding creative and cinematic videos.", "method": "The authors created a dataset of 1K QA pairs from 320+ creative video clips, categorized into key reasoning dimensions.", "result": "Evaluations on leading VideoQA models show performance degradation on ImplicitQA, highlighting the difficulty of implicit reasoning.", "conclusion": "The paper introduces ImplicitQA, a new benchmark for video question answering that focuses on implicit reasoning, and shows that current VideoQA models struggle with this type of reasoning."}}
{"id": "2506.21560", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21560", "abs": "https://arxiv.org/abs/2506.21560", "authors": ["Yifu Han", "Geo Zhang"], "title": "Reinforcement Learning Fine-Tuning of Language Model for Instruction Following and Math Reasoning", "comment": null, "summary": "This study investigates the effectiveness of reinforcement learning (RL)\nfine-tuning techniques on a compact language model (Qwen2.5-0.5B Base) for two\nchallenging tasks: instruction following and mathematical reasoning. We compare\nsupervised fine-tuning (SFT), Direct Preference Optimization (DPO) using\npreference-labeled data, and Reinforce Leave-One-Out (RLOO) with reward models.\nOur experiments show that RLOO with DeBERTa reward modeling achieves the best\nalignment, while DPO provides strong and consistent results. For math reasoing\ntasks, synthetic data augmentation and best-of-N sampling with an external\nverifier significantly improve accuracy, showing the potential of combining\nfine-tuning with inference-time tools. This study highlights key trade-offs and\npractical strategies for training lightweight, task-aligned small-scale\nlanguage models.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u51e0\u79cd\u5fae\u8c03\u6280\u672f\u5728\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u6548\u679c\uff0c\u53d1\u73b0 RLOO \u548c DPO \u5728\u6307\u4ee4\u8ddf\u968f\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u800c\u5408\u6210\u6570\u636e\u589e\u5f3a\u6709\u52a9\u4e8e\u63d0\u9ad8\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u672c\u7814\u7a76\u8c03\u67e5\u4e86\u5f3a\u5316\u5b66\u4e60 (RL) \u5fae\u8c03\u6280\u672f\u5728\u7d27\u51d1\u578b\u8bed\u8a00\u6a21\u578b (Qwen2.5-0.5B Base) \u4e0a\u5bf9\u4e24\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff08\u6307\u4ee4\u8ddf\u968f\u548c\u6570\u5b66\u63a8\u7406\uff09\u7684\u6709\u6548\u6027\u3002", "method": "\u6709\u76d1\u7763\u5fae\u8c03 (SFT)\u3001\u4f7f\u7528\u504f\u597d\u6807\u8bb0\u6570\u636e\u7684\u76f4\u63a5\u504f\u597d\u4f18\u5316 (DPO) \u4ee5\u53ca\u4f7f\u7528\u5956\u52b1\u6a21\u578b\u7684\u5f3a\u5316\u7559\u4e00\u6cd5 (RLOO)", "result": "RLOO \u4e0e DeBERTa \u5956\u52b1\u5efa\u6a21\u5b9e\u73b0\u4e86\u6700\u4f73\u5bf9\u9f50\uff0c\u800c DPO \u63d0\u4f9b\u4e86\u5f3a\u5927\u4e14\u4e00\u81f4\u7684\u7ed3\u679c\u3002\u5bf9\u4e8e\u6570\u5b66\u63a8\u7406\u4efb\u52a1\uff0c\u5408\u6210\u6570\u636e\u589e\u5f3a\u548c Best-of-N \u91c7\u6837\u4e0e\u5916\u90e8\u9a8c\u8bc1\u5668\u663e\u7740\u63d0\u9ad8\u4e86\u51c6\u786e\u6027", "conclusion": "\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u3001\u4efb\u52a1\u5bf9\u9f50\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5173\u952e\u6743\u8861\u548c\u5b9e\u7528\u7b56\u7565\u3002"}}
{"id": "2506.21887", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21887", "abs": "https://arxiv.org/abs/2506.21887", "authors": ["Edward Chen", "Sang T. Truong", "Natalie Dullerud", "Sanmi Koyejo", "Carlos Guestrin"], "title": "Interactive Multi-Objective Probabilistic Preference Learning with Soft and Hard Bounds", "comment": null, "summary": "High-stakes decision-making involves navigating multiple competing objectives\nwith expensive evaluations. For instance, in brachytherapy, clinicians must\nbalance maximizing tumor coverage (e.g., an aspirational target or soft bound\nof >95% coverage) against strict organ dose limits (e.g., a non-negotiable hard\nbound of <601 cGy to the bladder), with each plan evaluation being\nresource-intensive. Selecting Pareto-optimal solutions that match implicit\npreferences is challenging, as exhaustive Pareto frontier exploration is\ncomputationally and cognitively prohibitive, necessitating interactive\nframeworks to guide users. While decision-makers (DMs) often possess domain\nknowledge to narrow the search via such soft-hard bounds, current methods often\nlack systematic approaches to iteratively refine these multi-faceted preference\nstructures. Critically, DMs must trust their final decision, confident they\nhaven't missed superior alternatives; this trust is paramount in\nhigh-consequence scenarios. We present Active-MoSH, an interactive local-global\nframework designed for this process. Its local component integrates soft-hard\nbounds with probabilistic preference learning, maintaining distributions over\nDM preferences and bounds for adaptive Pareto subset refinement. This is guided\nby an active sampling strategy optimizing exploration-exploitation while\nminimizing cognitive burden. To build DM trust, Active-MoSH's global component,\nT-MoSH, leverages multi-objective sensitivity analysis to identify potentially\noverlooked, high-value points beyond immediate feedback. We demonstrate\nActive-MoSH's performance benefits through diverse synthetic and real-world\napplications. A user study on AI-generated image selection further validates\nour hypotheses regarding the framework's ability to improve convergence,\nenhance DM trust, and provide expressive preference articulation, enabling more\neffective DMs.", "AI": {"tldr": "Active-MoSH: An interactive framework enhances decision-making by integrating preference learning and sensitivity analysis, improving trust and convergence.", "motivation": "High-stakes decision-making involves navigating multiple competing objectives with expensive evaluations. Selecting Pareto-optimal solutions that match implicit preferences is challenging, as exhaustive Pareto frontier exploration is computationally and cognitively prohibitive. Current methods often lack systematic approaches to iteratively refine these multi-faceted preference structures. DMs must trust their final decision, confident they haven't missed superior alternatives.", "method": "The paper proposes Active-MoSH, an interactive local-global framework. The local component integrates soft-hard bounds with probabilistic preference learning, maintaining distributions over DM preferences and bounds for adaptive Pareto subset refinement, guided by an active sampling strategy. The global component, T-MoSH, leverages multi-objective sensitivity analysis.", "result": "The paper demonstrates Active-MoSH's performance benefits through diverse synthetic and real-world applications. A user study on AI-generated image selection further validates the framework's ability to improve convergence, enhance DM trust, and provide expressive preference articulation.", "conclusion": "The paper introduces Active-MoSH, an interactive framework that helps decision-makers navigate competing objectives with expensive evaluations by integrating soft-hard bounds with probabilistic preference learning and multi-objective sensitivity analysis. User study validates that Active-MoSH improves convergence, enhances DM trust and provides expressive preference articulation, enabling more effective DMs."}}
{"id": "2506.21744", "categories": ["cs.LG", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.21744", "abs": "https://arxiv.org/abs/2506.21744", "authors": ["Biying Zhou", "Nanyu Luo", "Feng Ji"], "title": "Federated Item Response Theory Models", "comment": null, "summary": "Item Response Theory (IRT) models have been widely used to estimate\nrespondents' latent abilities and calibrate items' difficulty. Traditional IRT\nestimation requires all individual raw response data to be centralized in one\nplace, thus potentially causing privacy issues. Federated learning is an\nemerging field in computer science and machine learning with added features of\nprivacy protection and distributed computing. To integrate the advances from\nfederated learning with modern psychometrics, we propose a novel framework,\nFederated Item Response Theory (IRT), to enable estimating traditional IRT\nmodels with additional privacy, allowing estimation in a distributed manner\nwithout losing estimation accuracy.\n  Our numerical experiments confirm that FedIRT achieves statistical accuracy\nsimilar to standard IRT estimation using popular R packages, while offering\ncritical advantages: privacy protection and reduced communication costs. We\nalso validate FedIRT's utility through a real-world exam dataset, demonstrating\nits effectiveness in realistic educational contexts. This new framework extends\nIRT's applicability to distributed settings, such as multi-school assessments,\nwithout sacrificing accuracy or security. To support practical adoption, we\nprovide an open-ource R package, FedIRT, implementing the framework for the\ntwo-parameter logistic (2PL) and partial credit models (PCM).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u8054\u90a6\u9879\u76ee\u53cd\u5e94\u7406\u8bba (IRT)\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u4f20\u7edf IRT \u6a21\u578b\u7684\u4f30\u8ba1\uff0c\u540c\u65f6\u5177\u6709\u989d\u5916\u7684\u9690\u79c1\u6027\uff0c\u5141\u8bb8\u4ee5\u5206\u5e03\u5f0f\u65b9\u5f0f\u8fdb\u884c\u4f30\u8ba1\uff0c\u800c\u4e0d\u4f1a\u635f\u5931\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u7684 IRT \u4f30\u8ba1\u9700\u8981\u5c06\u6240\u6709\u4e2a\u4eba\u539f\u59cb\u53cd\u5e94\u6570\u636e\u96c6\u4e2d\u5728\u4e00\u4e2a\u5730\u65b9\uff0c\u4ece\u800c\u53ef\u80fd\u5bfc\u81f4\u9690\u79c1\u95ee\u9898\u3002\u8054\u90a6\u5b66\u4e60\u662f\u8ba1\u7b97\u673a\u79d1\u5b66\u548c\u673a\u5668\u5b66\u4e60\u4e2d\u4e00\u4e2a\u65b0\u5174\u9886\u57df\uff0c\u5177\u6709\u9690\u79c1\u4fdd\u62a4\u548c\u5206\u5e03\u5f0f\u8ba1\u7b97\u7684\u9644\u52a0\u529f\u80fd\u3002", "method": "\u8054\u90a6\u9879\u76ee\u53cd\u5e94\u7406\u8bba (IRT)", "result": "\u6570\u503c\u5b9e\u9a8c\u8bc1\u5b9e\uff0cFedIRT \u5b9e\u73b0\u4e86\u4e0e\u4f7f\u7528\u6d41\u884c\u7684 R \u5305\u7684\u6807\u51c6 IRT \u4f30\u8ba1\u76f8\u4f3c\u7684\u7edf\u8ba1\u7cbe\u5ea6\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u5173\u952e\u4f18\u52bf\uff1a\u9690\u79c1\u4fdd\u62a4\u548c\u964d\u4f4e\u7684\u901a\u4fe1\u6210\u672c\u3002\u6211\u4eec\u8fd8\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u7684\u8003\u8bd5\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86 FedIRT \u7684\u5b9e\u7528\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u9645\u6559\u80b2\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "FedIRT\u5728\u4e0d\u727a\u7272\u7cbe\u5ea6\u6216\u5b89\u5168\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u5c06 IRT \u7684\u9002\u7528\u6027\u6269\u5c55\u5230\u5206\u5e03\u5f0f\u73af\u5883\uff0c\u4f8b\u5982\u591a\u5b66\u6821\u8bc4\u4f30\u3002"}}
{"id": "2506.21601", "categories": ["cs.IR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21601", "abs": "https://arxiv.org/abs/2506.21601", "authors": ["Duong Bach"], "title": "Hierarchical Patch Compression for ColPali: Efficient Multi-Vector Document Retrieval with Dynamic Pruning and Quantization", "comment": "9 pages", "summary": "Multi-vector document retrieval systems, such as ColPali, excel in\nfine-grained matching for complex queries but incur significant storage and\ncomputational costs due to their reliance on high-dimensional patch embeddings\nand late-interaction scoring. To address these challenges, we propose\nHPC-ColPali, a Hierarchical Patch Compression framework that enhances the\nefficiency of ColPali while preserving its retrieval accuracy. Our approach\nintegrates three innovative techniques: (1) K-Means quantization, which\ncompresses patch embeddings into 1-byte centroid indices, achieving up to\n32$\\times$ storage reduction; (2) attention-guided dynamic pruning, utilizing\nVision-Language Model attention weights to retain only the top-$p\\%$ most\nsalient patches, reducing late-interaction computation by up to 60\\% with less\nthan 2\\% nDCG@10 loss; and (3) optional binary encoding of centroid indices\ninto $b$-bit strings ($b=\\lceil\\log_2 K\\rceil$), enabling rapid Hamming\ndistance-based similarity search for resource-constrained environments.\nEvaluated on the ViDoRe and SEC-Filings datasets, HPC-ColPali achieves 30--50\\%\nlower query latency under HNSW indexing while maintaining high retrieval\nprecision. When integrated into a Retrieval-Augmented Generation pipeline for\nlegal summarization, it reduces hallucination rates by 30\\% and halves\nend-to-end latency. These advancements establish HPC-ColPali as a scalable and\nefficient solution for multi-vector document retrieval across diverse\napplications. Code is available at https://github.com/DngBack/HPC-ColPali.", "AI": {"tldr": "HPC-ColPali improves the efficiency of multi-vector document retrieval while maintaining accuracy by compressing patch embeddings and pruning less important patches.", "motivation": "Multi-vector document retrieval systems are accurate but have high storage and computational costs.", "method": "Hierarchical Patch Compression framework with K-Means quantization, attention-guided dynamic pruning, and optional binary encoding.", "result": "HPC-ColPali achieves 30-50% lower query latency, reduces hallucination rates by 30%, and halves end-to-end latency.", "conclusion": "HPC-ColPali is a scalable and efficient solution for multi-vector document retrieval across diverse applications, reducing hallucination rates and latency in Retrieval-Augmented Generation."}}
{"id": "2506.21770", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21770", "abs": "https://arxiv.org/abs/2506.21770", "authors": ["Rishiraj Paul Chowdhury", "Nirmit Shekar Karkera"], "title": "Early Glaucoma Detection using Deep Learning with Multiple Datasets of Fundus Images", "comment": "13 pages, 6 figures, prepared for course CSCI 5922 at University of\n  Colorado Boulder. Code available upon request, dataset taken from Kaggle", "summary": "Glaucoma is a leading cause of irreversible blindness, but early detection\ncan significantly improve treatment outcomes. Traditional diagnostic methods\nare often invasive and require specialized equipment. In this work, we present\na deep learning pipeline using the EfficientNet-B0 architecture for glaucoma\ndetection from retinal fundus images. Unlike prior studies that rely on single\ndatasets, we sequentially train and fine-tune our model across ACRIMA, ORIGA,\nand RIM-ONE datasets to enhance generalization. Our experiments show that\nminimal preprocessing yields higher AUC-ROC compared to more complex\nenhancements, and our model demonstrates strong discriminative performance on\nunseen datasets. The proposed pipeline offers a reproducible and scalable\napproach to early glaucoma detection, supporting its potential clinical\nutility.", "AI": {"tldr": "Developed a deep learning pipeline for early glaucoma detection using EfficientNet-B0, achieving strong performance with minimal preprocessing and good generalization across datasets.", "motivation": "Glaucoma is a leading cause of irreversible blindness, but early detection can significantly improve treatment outcomes. Traditional diagnostic methods are often invasive and require specialized equipment.", "method": "a deep learning pipeline using the EfficientNet-B0 architecture for glaucoma detection from retinal fundus images. sequentially train and fine-tune our model across ACRIMA, ORIGA, and RIM-ONE datasets to enhance generalization", "result": "minimal preprocessing yields higher AUC-ROC compared to more complex enhancements, and our model demonstrates strong discriminative performance on unseen datasets.", "conclusion": "The proposed deep learning pipeline offers a reproducible and scalable approach to early glaucoma detection, supporting its potential clinical utility."}}
{"id": "2506.21561", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21561", "abs": "https://arxiv.org/abs/2506.21561", "authors": ["Emilio Barkett", "Olivia Long", "Madhavendra Thakur"], "title": "Reasoning Isn't Enough: Examining Truth-Bias and Sycophancy in LLMs", "comment": null, "summary": "Despite their widespread use in fact-checking, moderation, and high-stakes\ndecision-making, large language models (LLMs) remain poorly understood as\njudges of truth. This study presents the largest evaluation to date of LLMs'\nveracity detection capabilities and the first analysis of these capabilities in\nreasoning models. We had eight LLMs make 4,800 veracity judgments across\nseveral prompts, comparing reasoning and non-reasoning models. We find that\nrates of truth-bias, or the likelihood to believe a statement is true,\nregardless of whether it is actually true, are lower in reasoning models than\nin non-reasoning models, but still higher than human benchmarks. Most\nconcerning, we identify sycophantic tendencies in several advanced models\n(o4-mini and GPT-4.1 from OpenAI, R1 from DeepSeek), which displayed an\nasymmetry in detection accuracy, performing well in truth accuracy but poorly\nin deception accuracy. This suggests that capability advances alone do not\nresolve fundamental veracity detection challenges in LLMs.", "AI": {"tldr": "This study presents the largest evaluation to date of LLMs' veracity detection capabilities and the first analysis of these capabilities in reasoning models.", "motivation": "LLMs remain poorly understood as judges of truth", "method": "eight LLMs make 4,800 veracity judgments across several prompts, comparing reasoning and non-reasoning models", "result": "rates of truth-bias are lower in reasoning models than in non-reasoning models, but still higher than human benchmarks. identify sycophantic tendencies in several advanced models, which displayed an asymmetry in detection accuracy, performing well in truth accuracy but poorly in deception accuracy", "conclusion": "capability advances alone do not resolve fundamental veracity detection challenges in LLMs"}}
{"id": "2506.21996", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21996", "abs": "https://arxiv.org/abs/2506.21996", "authors": ["Rapha\u00ebl Boige", "Amine Boumaza", "Bruno Scherrer"], "title": "AlphaBeta is not as good as you think: a new probabilistic model to better analyze deterministic game-solving algorithms", "comment": null, "summary": "Deterministic game-solving algorithms are conventionally analyzed in the\nlight of their average-case complexity against a distribution of random\ngame-trees, where leaf values are independently sampled from a fixed\ndistribution. This simplified model enables uncluttered mathematical analysis,\nrevealing two key properties: root value distributions asymptotically collapse\nto a single fixed value for finite-valued trees, and all reasonable algorithms\nachieve global optimality. However, these findings are artifacts of the model's\ndesign-its long criticized independence assumption strips games of structural\ncomplexity, producing trivial instances where no algorithm faces meaningful\nchallenges. To address this limitation, we introduce a new probabilistic model\nthat incrementally constructs game-trees using a fixed level-wise conditional\ndistribution. By enforcing ancestor dependency, a critical structural feature\nof real-world games, our framework generates problems with adjustable\ndifficulty while retaining some form of analytical tractability. For several\nalgorithms, including AlphaBeta and Scout, we derive recursive formulas\ncharacterizing their average-case complexities under this model. These allow us\nto rigorously compare algorithms on deep game-trees, where Monte-Carlo\nsimulations are no longer feasible. While asymptotically, all algorithms seem\nto converge to identical branching factor (a result analogous to those of\nindependence-based models), deep finite trees reveal stark differences:\nAlphaBeta incurs a significantly larger constant multiplicative factor compared\nto algorithms like Scout, leading to a substantial practical slowdown. Our\nframework sheds new light on classical game-solving algorithms, offering\nrigorous evidence and analytical tools to advance the understanding of these\nmethods under a more realistic, challenging, and yet tractable model.", "AI": {"tldr": "\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u6982\u7387\u6a21\u578b\u6765\u6784\u5efa\u535a\u5f08\u6811\uff0c\u53d1\u73b0AlphaBeta\u7b97\u6cd5\u5728\u6df1\u5ea6\u6709\u9650\u6811\u4e2d\u7684\u6027\u80fd\u4e0d\u5982Scout\u7b97\u6cd5\u3002", "motivation": "\u4f20\u7edf\u786e\u5b9a\u6027\u535a\u5f08\u6c42\u89e3\u7b97\u6cd5\u7684\u5206\u6790\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u4e3a\u5176\u7b80\u5316\u7684\u6a21\u578b\u5265\u593a\u4e86\u535a\u5f08\u7684\u7ed3\u6784\u590d\u6742\u6027\uff0c\u4ea7\u751f\u4e86\u6ca1\u6709\u7b97\u6cd5\u9762\u4e34\u6709\u610f\u4e49\u6311\u6218\u7684\u7b80\u5355\u5b9e\u4f8b\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u6982\u7387\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u4f7f\u7528\u56fa\u5b9a\u7684\u5206\u5c42\u6761\u4ef6\u5206\u5e03\u589e\u91cf\u6784\u5efa\u535a\u5f08\u6811\u3002\u63a8\u5bfc\u4e86\u5305\u62ecAlphaBeta\u548cScout\u5728\u5185\u7684\u51e0\u79cd\u7b97\u6cd5\u5728\u8be5\u6a21\u578b\u4e0b\u7684\u5e73\u5747\u60c5\u51b5\u590d\u6742\u5ea6\u7684\u9012\u5f52\u516c\u5f0f\u3002", "result": "\u5728\u6df1\u5ea6\u6709\u9650\u6811\u4e2d\uff0cAlphaBeta\u7b97\u6cd5\u7684\u5e38\u6570\u4e58\u6cd5\u56e0\u5b50\u6bd4Scout\u7b49\u7b97\u6cd5\u5927\u5f97\u591a\uff0c\u5bfc\u81f4\u5b9e\u9645\u901f\u5ea6\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u6240\u6709\u7b97\u6cd5\u4f3c\u4e4e\u90fd\u6536\u655b\u5230\u76f8\u540c\u7684\u5206\u652f\u56e0\u5b50\uff0c\u4f46AlphaBeta\u7b97\u6cd5\u7684\u5e38\u6570\u4e58\u6cd5\u56e0\u5b50\u6bd4Scout\u7b49\u7b97\u6cd5\u5927\u5f97\u591a\uff0c\u5bfc\u81f4\u5b9e\u9645\u901f\u5ea6\u663e\u8457\u4e0b\u964d\u3002"}}
{"id": "2506.21771", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2506.21771", "abs": "https://arxiv.org/abs/2506.21771", "authors": ["John Wesley Hostetter", "Min Chi"], "title": "Gradient-Based Neuroplastic Adaptation for Concurrent Optimization of Neuro-Fuzzy Networks", "comment": "45 pages", "summary": "Neuro-fuzzy networks (NFNs) are transparent, symbolic, and universal function\napproximations that perform as well as conventional neural architectures, but\ntheir knowledge is expressed as linguistic IF-THEN rules. Despite these\nadvantages, their systematic design process remains a challenge. Existing work\nwill often sequentially build NFNs by inefficiently isolating parametric and\nstructural identification, leading to a premature commitment to brittle and\nsubpar architecture. We propose a novel application-independent approach called\ngradient-based neuroplastic adaptation for the concurrent optimization of NFNs'\nparameters and structure. By recognizing that NFNs' parameters and structure\nshould be optimized simultaneously as they are deeply conjoined, settings\npreviously unapproachable for NFNs are now accessible, such as the online\nreinforcement learning of NFNs for vision-based tasks. The effectiveness of\nconcurrently optimizing NFNs is empirically shown as it is trained by online\nreinforcement learning to proficiently play challenging scenarios from a\nvision-based video game called DOOM.", "AI": {"tldr": "We propose a novel application-independent approach called gradient-based neuroplastic adaptation for the concurrent optimization of NFNs' parameters and structure.", "motivation": "systematic design process of NFNs remains a challenge. Existing work will often sequentially build NFNs by inefficiently isolating parametric and structural identification, leading to a premature commitment to brittle and subpar architecture", "method": "gradient-based neuroplastic adaptation for the concurrent optimization of NFNs' parameters and structure", "result": "settings previously unapproachable for NFNs are now accessible, such as the online reinforcement learning of NFNs for vision-based tasks", "conclusion": "concurrently optimizing NFNs is effective and can be trained by online reinforcement learning to play challenging scenarios in DOOM"}}
{"id": "2506.21604", "categories": ["cs.IR", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21604", "abs": "https://arxiv.org/abs/2506.21604", "authors": ["Varun Mannam", "Fang Wang", "Xin Chen"], "title": "Evaluating VisualRAG: Quantifying Cross-Modal Performance in Enterprise Document Understanding", "comment": "Conference: KDD conference workshop:\n  https://kdd-eval-workshop.github.io/genai-evaluation-kdd2025/", "summary": "Current evaluation frameworks for multimodal generative AI struggle to\nestablish trustworthiness, hindering enterprise adoption where reliability is\nparamount. We introduce a systematic, quantitative benchmarking framework to\nmeasure the trustworthiness of progressively integrating cross-modal inputs\nsuch as text, images, captions, and OCR within VisualRAG systems for enterprise\ndocument intelligence. Our approach establishes quantitative relationships\nbetween technical metrics and user-centric trust measures. Evaluation reveals\nthat optimal modality weighting with weights of 30% text, 15% image, 25%\ncaption, and 30% OCR improves performance by 57.3% over text-only baselines\nwhile maintaining computational efficiency. We provide comparative assessments\nof foundation models, demonstrating their differential impact on\ntrustworthiness in caption generation and OCR extraction-a vital consideration\nfor reliable enterprise AI. This work advances responsible AI deployment by\nproviding a rigorous framework for quantifying and enhancing trustworthiness in\nmultimodal RAG for critical enterprise applications.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u91cf\u5316\u591a\u6a21\u6001RAG\u7cfb\u7edf\u53ef\u4fe1\u5ea6\u7684\u6846\u67b6\uff0c\u53d1\u73b0\u9002\u5f53\u7684\u6a21\u6001\u52a0\u6743\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u76ee\u524d\u591a\u6a21\u6001\u751f\u6210AI\u7684\u8bc4\u4f30\u6846\u67b6\u96be\u4ee5\u5efa\u7acb\u53ef\u4fe1\u5ea6\uff0c\u963b\u788d\u4e86\u4f01\u4e1a\u91c7\u7528\uff0c\u800c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8be5\u65b9\u6cd5\u5efa\u7acb\u6280\u672f\u6307\u6807\u548c\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u4fe1\u4efb\u5ea6\u91cf\u4e4b\u95f4\u7684\u5b9a\u91cf\u5173\u7cfb\u3002", "result": "\u8bc4\u4f30\u8868\u660e\uff0c\u6587\u672c\u3001\u56fe\u50cf\u3001\u6807\u9898\u548cOCR\u7684\u6700\u4f73\u6a21\u6001\u52a0\u6743\uff08\u6743\u91cd\u5206\u522b\u4e3a30%\u300115%\u300125%\u548c30%\uff09\u6bd4\u7eaf\u6587\u672c\u57fa\u7ebf\u63d0\u9ad8\u4e8657.3%\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e25\u8c28\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u548c\u589e\u5f3a\u591a\u6a21\u6001RAG\u5728\u5173\u952e\u4f01\u4e1a\u5e94\u7528\u4e2d\u7684\u53ef\u4fe1\u5ea6\uff0c\u4ece\u800c\u63a8\u8fdb\u8d1f\u8d23\u4efb\u7684AI\u90e8\u7f72\u3002"}}
{"id": "2506.21785", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21785", "abs": "https://arxiv.org/abs/2506.21785", "authors": ["Daniel Wen"], "title": "Comparing Learning Paradigms for Egocentric Video Summarization", "comment": null, "summary": "In this study, we investigate various computer vision paradigms - supervised\nlearning, unsupervised learning, and prompt fine-tuning - by assessing their\nability to understand and interpret egocentric video data. Specifically, we\nexamine Shotluck Holmes (state-of-the-art supervised learning), TAC-SUM\n(state-of-the-art unsupervised learning), and GPT-4o (a prompt fine-tuned\npre-trained model), evaluating their effectiveness in video summarization. Our\nresults demonstrate that current state-of-the-art models perform less\neffectively on first-person videos compared to third-person videos,\nhighlighting the need for further advancements in the egocentric video domain.\nNotably, a prompt fine-tuned general-purpose GPT-4o model outperforms these\nspecialized models, emphasizing the limitations of existing approaches in\nadapting to the unique challenges of first-person perspectives. Although our\nevaluation is conducted on a small subset of egocentric videos from the\nEgo-Exo4D dataset due to resource constraints, the primary objective of this\nresearch is to provide a comprehensive proof-of-concept analysis aimed at\nadvancing the application of computer vision techniques to first-person videos.\nBy exploring novel methodologies and evaluating their potential, we aim to\ncontribute to the ongoing development of models capable of effectively\nprocessing and interpreting egocentric perspectives.", "AI": {"tldr": "This study assesses supervised learning, unsupervised learning, and prompt fine-tuning for egocentric video understanding, finding that GPT-4o outperforms specialized models but all methods struggle compared to third-person video, highlighting the need for improvement in this area.", "motivation": "To investigate various computer vision paradigms' ability to understand and interpret egocentric video data.", "method": "We examine Shotluck Holmes (state-of-the-art supervised learning), TAC-SUM (state-of-the-art unsupervised learning), and GPT-4o (a prompt fine-tuned pre-trained model), evaluating their effectiveness in video summarization.", "result": "current state-of-the-art models perform less effectively on first-person videos compared to third-person videos.  a prompt fine-tuned general-purpose GPT-4o model outperforms these specialized models", "conclusion": "Current state-of-the-art models perform less effectively on first-person videos compared to third-person videos, highlighting the need for further advancements in the egocentric video domain.  A prompt fine-tuned general-purpose GPT-4o model outperforms these specialized models, emphasizing the limitations of existing approaches in adapting to the unique challenges of first-person perspectives."}}
{"id": "2506.21562", "categories": ["cs.CL", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2506.21562", "abs": "https://arxiv.org/abs/2506.21562", "authors": ["Jun Yin", "Pengyu Zeng", "Jing Zhong", "Peilin Li", "Miao Zhang", "Ran Luo", "Shuai Lu"], "title": "FloorPlan-DeepSeek (FPDS): A multimodal approach to floorplan generation using vector-based next room prediction", "comment": null, "summary": "In the architectural design process, floor plan generation is inherently\nprogressive and iterative. However, existing generative models for floor plans\nare predominantly end-to-end generation that produce an entire pixel-based\nlayout in a single pass. This paradigm is often incompatible with the\nincremental workflows observed in real-world architectural practice. To address\nthis issue, we draw inspiration from the autoregressive 'next token prediction'\nmechanism commonly used in large language models, and propose a novel 'next\nroom prediction' paradigm tailored to architectural floor plan modeling.\nExperimental evaluation indicates that FPDS demonstrates competitive\nperformance in comparison to diffusion models and Tell2Design in the\ntext-to-floorplan task, indicating its potential applicability in supporting\nfuture intelligent architectural design.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5e73\u9762\u56fe\u751f\u6210\u6a21\u578bFPDS\uff0c\u5b83\u901a\u8fc7\u9884\u6d4b\u4e0b\u4e00\u4e2a\u623f\u95f4\u6765\u9010\u6b65\u751f\u6210\u5e73\u9762\u56fe\uff0c\u66f4\u7b26\u5408\u5efa\u7b51\u8bbe\u8ba1\u7684\u8fed\u4ee3\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709\u7684\u5e73\u9762\u56fe\u751f\u6210\u6a21\u578b\u4e3b\u8981\u662f\u7aef\u5230\u7aef\u751f\u6210\uff0c\u4ee5\u5355\u6b21\u751f\u6210\u6574\u4e2a\u57fa\u4e8e\u50cf\u7d20\u7684\u5e03\u5c40\u3002\u8fd9\u79cd\u8303\u4f8b\u901a\u5e38\u4e0e\u73b0\u5b9e\u4e16\u754c\u5efa\u7b51\u5b9e\u8df5\u4e2d\u89c2\u5bdf\u5230\u7684\u589e\u91cf\u5de5\u4f5c\u6d41\u7a0b\u4e0d\u517c\u5bb9\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u201c\u4e0b\u4e00\u4e2a\u623f\u95f4\u9884\u6d4b\u201d\u8303\u4f8b\uff0c\u8be5\u8303\u4f8b\u4e13\u4e3a\u5efa\u7b51\u5e73\u9762\u56fe\u5efa\u6a21\u800c\u5b9a\u5236\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cFPDS\u5728\u6587\u672c\u5230\u5e73\u9762\u56fe\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "FPDS\u5728\u6587\u672c\u5230\u5e73\u9762\u56fe\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e0e\u6269\u6563\u6a21\u578b\u548cTell2Design\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u8868\u660e\u5176\u5728\u652f\u6301\u672a\u6765\u667a\u80fd\u5efa\u7b51\u8bbe\u8ba1\u65b9\u9762\u7684\u6f5c\u5728\u9002\u7528\u6027\u3002"}}
{"id": "2506.22005", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22005", "abs": "https://arxiv.org/abs/2506.22005", "authors": ["Naoto Onda", "Kazumi Kasaura", "Yuta Oriike", "Masaya Taniguchi", "Akiyoshi Sannai", "Sho Sonoda"], "title": "LeanConjecturer: Automatic Generation of Mathematical Conjectures for Theorem Proving", "comment": "15 pages, 4 figures, 5 tables", "summary": "We introduce LeanConjecturer, a pipeline for automatically generating\nuniversity-level mathematical conjectures in Lean 4 using Large Language Models\n(LLMs). Our hybrid approach combines rule-based context extraction with\nLLM-based theorem statement generation, addressing the data scarcity challenge\nin formal theorem proving. Through iterative generation and evaluation,\nLeanConjecturer produced 12,289 conjectures from 40 Mathlib seed files, with\n3,776 identified as syntactically valid and non-trivial, that is, cannot be\nproven by \\texttt{aesop} tactic. We demonstrate the utility of these generated\nconjectures for reinforcement learning through Group Relative Policy\nOptimization (GRPO), showing that targeted training on domain-specific\nconjectures can enhance theorem proving capabilities. Our approach generates\n103.25 novel conjectures per seed file on average, providing a scalable\nsolution for creating training data for theorem proving systems. Our system\nsuccessfully verified several non-trivial theorems in topology, including\nproperties of semi-open, alpha-open, and pre-open sets, demonstrating its\npotential for mathematical discovery beyond simple variations of existing\nresults.", "AI": {"tldr": "LeanConjecturer\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210\u6570\u5b66\u731c\u60f3\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u5b9a\u7406\u8bc1\u660e\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5f62\u5f0f\u5b9a\u7406\u8bc1\u660e\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u6027\u6311\u6218\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u57fa\u4e8e\u89c4\u5219\u7684\u4e0a\u4e0b\u6587\u63d0\u53d6\u548c\u57fa\u4e8eLLM\u7684\u5b9a\u7406\u9648\u8ff0\u751f\u6210\u3002", "result": "LeanConjecturer\u4ece40\u4e2aMathlib\u79cd\u5b50\u6587\u4ef6\u4e2d\u751f\u6210\u4e8612,289\u4e2a\u731c\u60f3\uff0c\u5176\u4e2d3,776\u4e2a\u88ab\u8bc6\u522b\u4e3a\u53e5\u6cd5\u6709\u6548\u4e14\u975e\u5e73\u51e1\u7684\uff0c\u5373\u4e0d\u80fd\u88ab\\texttt{aesop}\u7b56\u7565\u8bc1\u660e\u3002\u9488\u5bf9\u9886\u57df\u7279\u5b9a\u731c\u60f3\u7684\u5b9a\u5411\u8bad\u7ec3\u53ef\u4ee5\u589e\u5f3a\u5b9a\u7406\u8bc1\u660e\u80fd\u529b\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5e73\u5747\u6bcf\u4e2a\u79cd\u5b50\u6587\u4ef6\u751f\u6210103.25\u4e2a\u65b0\u7684\u731c\u60f3\uff0c\u4e3a\u521b\u5efa\u5b9a\u7406\u8bc1\u660e\u7cfb\u7edf\u7684\u8bad\u7ec3\u6570\u636e\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "LeanConjecturer\u80fd\u591f\u6210\u529f\u9a8c\u8bc1\u62d3\u6251\u5b66\u4e2d\u51e0\u4e2a\u91cd\u8981\u7684\u5b9a\u7406\uff0c\u5305\u62ec\u534a\u5f00\u96c6\u3001\u03b1-\u5f00\u96c6\u548c\u524d\u5f00\u96c6\u7684\u6027\u8d28\uff0c\u5c55\u793a\u4e86\u5b83\u5728\u6570\u5b66\u53d1\u73b0\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u7ed3\u679c\u7684\u7b80\u5355\u53d8\u4f53\u3002"}}
{"id": "2506.21782", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.21782", "abs": "https://arxiv.org/abs/2506.21782", "authors": ["Aditya Narendra", "Dmitry Makarov", "Aleksandr Panov"], "title": "M3PO: Massively Multi-Task Model-Based Policy Optimization", "comment": "6 pages, 4 figures. Accepted at IEEE/RSJ IROS 2025. Full version,\n  including appendix and implementation details", "summary": "We introduce Massively Multi-Task Model-Based Policy Optimization (M3PO), a\nscalable model-based reinforcement learning (MBRL) framework designed to\naddress sample inefficiency in single-task settings and poor generalization in\nmulti-task domains. Existing model-based approaches like DreamerV3 rely on\npixel-level generative models that neglect control-centric representations,\nwhile model-free methods such as PPO suffer from high sample complexity and\nweak exploration. M3PO integrates an implicit world model, trained to predict\ntask outcomes without observation reconstruction, with a hybrid exploration\nstrategy that combines model-based planning and model-free uncertainty-driven\nbonuses. This eliminates the bias-variance trade-off in prior methods by using\ndiscrepancies between model-based and model-free value estimates to guide\nexploration, while maintaining stable policy updates through a trust-region\noptimizer. M3PO provides an efficient and robust alternative to existing\nmodel-based policy optimization approaches and achieves state-of-the-art\nperformance across multiple benchmarks.", "AI": {"tldr": "M3PO\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5b83\u96c6\u6210\u4e86\u9690\u5f0f\u4e16\u754c\u6a21\u578b\u548c\u6df7\u5408\u63a2\u7d22\u7b56\u7565\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u548c\u7a33\u5065\u7684\u7b56\u7565\u4f18\u5316\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\uff08\u5982DreamerV3\uff09\u4f9d\u8d56\u4e8e\u5ffd\u7565\u63a7\u5236\u4e2d\u5fc3\u8868\u793a\u7684\u50cf\u7d20\u7ea7\u751f\u6210\u6a21\u578b\uff0c\u800c\u65e0\u6a21\u578b\u65b9\u6cd5\uff08\u5982PPO\uff09\u5219\u5b58\u5728\u6837\u672c\u590d\u6742\u6027\u9ad8\u548c\u63a2\u7d22\u80fd\u529b\u5f31\u7684\u95ee\u9898\u3002M3PO\u65e8\u5728\u89e3\u51b3\u5355\u4efb\u52a1\u8bbe\u7f6e\u4e2d\u7684\u6837\u672c\u6548\u7387\u4f4e\u4e0b\u4ee5\u53ca\u591a\u4efb\u52a1\u9886\u57df\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "method": "M3PO\u96c6\u6210\u4e86\u9690\u5f0f\u4e16\u754c\u6a21\u578b\uff08\u8bad\u7ec3\u7528\u4e8e\u9884\u6d4b\u4efb\u52a1\u7ed3\u679c\u800c\u4e0d\u8fdb\u884c\u89c2\u5bdf\u91cd\u5efa\uff09\u4e0e\u6df7\u5408\u63a2\u7d22\u7b56\u7565\uff08\u7ed3\u5408\u4e86\u57fa\u4e8e\u6a21\u578b\u7684\u89c4\u5212\u548c\u65e0\u6a21\u578b\u7684\uff0c\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u7684\u5956\u52b1\uff09\u3002", "result": "M3PO\u901a\u8fc7\u4f7f\u7528\u57fa\u4e8e\u6a21\u578b\u548c\u65e0\u6a21\u578b\u7684\u4ef7\u503c\u4f30\u8ba1\u4e4b\u95f4\u7684\u5dee\u5f02\u6765\u6307\u5bfc\u63a2\u7d22\uff0c\u540c\u65f6\u901a\u8fc7\u4fe1\u4efb\u533a\u57df\u4f18\u5316\u5668\u4fdd\u6301\u7a33\u5b9a\u7684\u7b56\u7565\u66f4\u65b0\uff0c\u4ece\u800c\u6d88\u9664\u4e86\u5148\u524d\u65b9\u6cd5\u4e2d\u7684\u504f\u5dee-\u65b9\u5dee\u6743\u8861\u3002", "conclusion": "M3PO\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7a33\u5065\u7684\u66ff\u4ee3\u73b0\u6709\u57fa\u4e8e\u6a21\u578b\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2506.21617", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21617", "abs": "https://arxiv.org/abs/2506.21617", "authors": ["Hiba Bederina", "Jill-J\u00eann Vie"], "title": "Bayesian-Guided Diversity in Sequential Sampling for Recommender Systems", "comment": null, "summary": "The challenge of balancing user relevance and content diversity in\nrecommender systems is increasingly critical amid growing concerns about\ncontent homogeneity and reduced user engagement. In this work, we propose a\nnovel framework that leverages a multi-objective, contextual sequential\nsampling strategy. Item selection is guided by Bayesian updates that\ndynamically adjust scores to optimize diversity. The reward formulation\nintegrates multiple diversity metrics-including the log-determinant volume of a\ntuned similarity submatrix and ridge leverage scores-along with a diversity\ngain uncertainty term to address the exploration-exploitation trade-off. Both\nintra- and inter-batch diversity are modeled to promote serendipity and\nminimize redundancy. A dominance-based ranking procedure identifies\nPareto-optimal item sets, enabling adaptive and balanced selections at each\niteration. Experiments on a real-world dataset show that our approach\nsignificantly improves diversity without sacrificing relevance, demonstrating\nits potential to enhance user experience in large-scale recommendation\nsettings.", "AI": {"tldr": "Proposes a multi-objective, contextual sequential sampling framework that leverages a multi-objective, contextual sequential sampling strategy to improve diversity without sacrificing relevance in recommender systems.", "motivation": "The challenge of balancing user relevance and content diversity in recommender systems is increasingly critical amid growing concerns about content homogeneity and reduced user engagement.", "method": "The proposed framework leverages a multi-objective, contextual sequential sampling strategy. Item selection is guided by Bayesian updates that dynamically adjust scores to optimize diversity. The reward formulation integrates multiple diversity metrics-including the log-determinant volume of a tuned similarity submatrix and ridge leverage scores-along with a diversity gain uncertainty term to address the exploration-exploitation trade-off. Both intra- and inter-batch diversity are modeled to promote serendipity and minimize redundancy. A dominance-based ranking procedure identifies Pareto-optimal item sets, enabling adaptive and balanced selections at each iteration.", "result": "Experiments on a real-world dataset show that our approach significantly improves diversity without sacrificing relevance.", "conclusion": "This paper presents a framework that significantly improves diversity without sacrificing relevance, demonstrating its potential to enhance user experience in large-scale recommendation settings."}}
{"id": "2506.21813", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21813", "abs": "https://arxiv.org/abs/2506.21813", "authors": ["Felix Holm", "G\u00f6zde \u00dcnver", "Ghazal Ghazaei", "Nassir Navab"], "title": "CAT-SG: A Large Dynamic Scene Graph Dataset for Fine-Grained Understanding of Cataract Surgery", "comment": null, "summary": "Understanding the intricate workflows of cataract surgery requires modeling\ncomplex interactions between surgical tools, anatomical structures, and\nprocedural techniques. Existing datasets primarily address isolated aspects of\nsurgical analysis, such as tool detection or phase segmentation, but lack\ncomprehensive representations that capture the semantic relationships between\nentities over time. This paper introduces the Cataract Surgery Scene Graph\n(CAT-SG) dataset, the first to provide structured annotations of tool-tissue\ninteractions, procedural variations, and temporal dependencies. By\nincorporating detailed semantic relations, CAT-SG offers a holistic view of\nsurgical workflows, enabling more accurate recognition of surgical phases and\ntechniques. Additionally, we present a novel scene graph generation model,\nCatSGG, which outperforms current methods in generating structured surgical\nrepresentations. The CAT-SG dataset is designed to enhance AI-driven surgical\ntraining, real-time decision support, and workflow analysis, paving the way for\nmore intelligent, context-aware systems in clinical practice.", "AI": {"tldr": "The paper introduces the CAT-SG dataset for cataract surgery, which includes detailed semantic relations to improve surgical phase recognition and technique analysis. A new scene graph generation model, CatSGG, is also presented.", "motivation": "Existing datasets lack comprehensive representations that capture the semantic relationships between entities over time in cataract surgery.", "method": "A novel scene graph generation model, CatSGG, outperforms current methods.", "result": "The CAT-SG dataset provides structured annotations of tool-tissue interactions, procedural variations, and temporal dependencies.", "conclusion": "The CAT-SG dataset enhances AI-driven surgical training, real-time decision support, and workflow analysis."}}
{"id": "2506.21563", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21563", "abs": "https://arxiv.org/abs/2506.21563", "authors": ["Kaiying Kevin Lin", "Hsiyu Chen", "Haopeng Zhang"], "title": "FormosanBench: Benchmarking Low-Resource Austronesian Languages in the Era of Large Language Models", "comment": null, "summary": "While large language models (LLMs) have demonstrated impressive performance\nacross a wide range of natural language processing (NLP) tasks in high-resource\nlanguages, their capabilities in low-resource and minority languages remain\nsignificantly underexplored. Formosan languages -- a subgroup of Austronesian\nlanguages spoken in Taiwan -- are both linguistically rich and endangered,\nlargely due to the sociolinguistic dominance of Mandarin. In this work, we\nintroduce FORMOSANBENCH, the first benchmark for evaluating LLMs on\nlow-resource Austronesian languages. It covers three endangered Formosan\nlanguages: Atayal, Amis, and Paiwan, across three core NLP tasks: machine\ntranslation, automatic speech recognition (ASR), and text summarization. We\nassess model performance in zero-shot, 10-shot, and fine-tuned settings using\nFORMOSANBENCH. Our results reveal a substantial performance gap between\nhigh-resource and Formosan languages. Existing LLMs consistently underperform\nacross all tasks, with 10-shot learning and fine-tuning offering only limited\nimprovements. These findings underscore the urgent need for more inclusive NLP\ntechnologies that can effectively support endangered and underrepresented\nlanguages. We release our datasets and code to facilitate future research in\nthis direction.", "AI": {"tldr": "introduce FORMOSANBENCH to evaluate LLMs on low-resource Austronesian languages", "motivation": "LLMs capabilities in low-resource and minority languages remain significantly underexplored. Formosan languages are both linguistically rich and endangered, largely due to the sociolinguistic dominance of Mandarin.", "method": "introduce FORMOSANBENCH, the first benchmark for evaluating LLMs on low-resource Austronesian languages. It covers three endangered Formosan languages: Atayal, Amis, and Paiwan, across three core NLP tasks: machine translation, automatic speech recognition (ASR), and text summarization. Assess model performance in zero-shot, 10-shot, and fine-tuned settings using FORMOSANBENCH.", "result": "reveal a substantial performance gap between high-resource and Formosan languages.", "conclusion": "Existing LLMs consistently underperform across all tasks, with 10-shot learning and fine-tuning offering only limited improvements."}}
{"id": "2506.22056", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22056", "abs": "https://arxiv.org/abs/2506.22056", "authors": ["Xuan Zhang", "Ziyan Jiang", "Rui Meng", "Yifei Leng", "Zhenbang Xiao", "Zora Zhiruo Wang", "Yanyi Shang", "Dehan Kong"], "title": "Universal Retrieval for Multimodal Trajectory Modeling", "comment": "18 pages, 3 figures, accepted by Workshop on Computer-use Agents @\n  ICML 2025", "summary": "Trajectory data, capturing human actions and environmental states across\nvarious modalities, holds significant potential for enhancing AI agent\ncapabilities, particularly in GUI environments. However, how to model the\nrepresentation of trajectory-level data presents a significant challenge that\nhas not been systematically addressed amid explosive trajectory data growth. In\nthis work, we introduce Multimodal Trajectory Retrieval, bridging the gap\nbetween universal retrieval and agent-centric trajectory modeling. We construct\nthe Unified Agent Trajectory Dataset (UATD) from annotated demonstrations and\nstates across diverse real-world scenarios. Based on this, we present\nGAE-Bench, a benchmark containing a large number of trajectory-based retrieval\npairs. In addition, we propose GAE-Retriever, a multimodal retrieval framework\nthat adopts vision-language models and incorporates optimized contrastive\nlearning through a token selection and the GradCache mechanism. Comprehensive\nevaluations across multiple datasets show that GAE-Retriever consistently\noutperforms strong baselines in retrieval recall, highlighting its\neffectiveness in advancing multimodal trajectory retrieval.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u591a\u6a21\u6001\u8f68\u8ff9\u68c0\u7d22\uff0c\u5f25\u5408\u4e86\u901a\u7528\u68c0\u7d22\u548c\u4ee5\u4ee3\u7406\u4e3a\u4e2d\u5fc3\u7684\u8f68\u8ff9\u5efa\u6a21\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u8f68\u8ff9\u6570\u636e\u5728\u589e\u5f3a\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u80fd\u529b\u65b9\u9762\u5177\u6709\u5de8\u5927\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728GUI\u73af\u5883\u4e2d\u3002\u7136\u800c\uff0c\u5982\u4f55\u5bf9\u8f68\u8ff9\u7ea7\u522b\u6570\u636e\u7684\u8868\u793a\u8fdb\u884c\u5efa\u6a21\u63d0\u51fa\u4e86\u4e00\u4e2a\u91cd\u5927\u7684\u6311\u6218\uff0c\u5728\u8f68\u8ff9\u6570\u636e\u7206\u70b8\u6027\u589e\u957f\u7684\u60c5\u51b5\u4e0b\uff0c\u8fd9\u4e2a\u95ee\u9898\u8fd8\u6ca1\u6709\u5f97\u5230\u7cfb\u7edf\u7684\u89e3\u51b3\u3002", "method": "\u63d0\u51fa\u4e86GAE-Retriever\uff0c\u4e00\u4e2a\u591a\u6a21\u6001\u68c0\u7d22\u6846\u67b6\uff0c\u5b83\u91c7\u7528\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u4ee4\u724c\u9009\u62e9\u548cGradCache\u673a\u5236\u7ed3\u5408\u4e86\u4f18\u5316\u7684\u5bf9\u6bd4\u5b66\u4e60\u3002", "result": "\u6784\u5efa\u4e86\u7edf\u4e00\u4ee3\u7406\u8f68\u8ff9\u6570\u636e\u96c6\uff08UATD\uff09\uff0c\u5e76\u63d0\u51fa\u4e86GAE-Bench\uff0c\u4e00\u4e2a\u5305\u542b\u5927\u91cf\u57fa\u4e8e\u8f68\u8ff9\u7684\u68c0\u7d22\u5bf9\u7684\u57fa\u51c6\u3002", "conclusion": "GAE-Retriever\u5728\u68c0\u7d22\u53ec\u56de\u7387\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\uff0c\u7a81\u51fa\u4e86\u5176\u5728\u63a8\u8fdb\u591a\u6a21\u6001\u8f68\u8ff9\u68c0\u7d22\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.21788", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI", "physics.atm-clus", "68T07, 68T09", "I.2; I.2.5; I.2.11"], "pdf": "https://arxiv.org/pdf/2506.21788", "abs": "https://arxiv.org/abs/2506.21788", "authors": ["Massimiliano Lupo Pasini", "Jong Youl Choi", "Pei Zhang", "Kshitij Mehta", "Rylie Weaver", "Ashwin M. Aji", "Karl W. Schulz", "Jorda Polo", "Prasanna Balaprakash"], "title": "Multi-task parallelism for robust pre-training of graph foundation models on multi-source, multi-fidelity atomistic modeling data", "comment": "15 pages, 4 figures, 2 tables", "summary": "Graph foundation models using graph neural networks promise sustainable,\nefficient atomistic modeling. To tackle challenges of processing multi-source,\nmulti-fidelity data during pre-training, recent studies employ multi-task\nlearning, in which shared message passing layers initially process input\natomistic structures regardless of source, then route them to multiple decoding\nheads that predict data-specific outputs. This approach stabilizes pre-training\nand enhances a model's transferability to unexplored chemical regions.\nPreliminary results on approximately four million structures are encouraging,\nyet questions remain about generalizability to larger, more diverse datasets\nand scalability on supercomputers. We propose a multi-task parallelism method\nthat distributes each head across computing resources with GPU acceleration.\nImplemented in the open-source HydraGNN architecture, our method was trained on\nover 24 million structures from five datasets and tested on the Perlmutter,\nAurora, and Frontier supercomputers, demonstrating efficient scaling on all\nthree highly heterogeneous super-computing architectures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4efb\u52a1\u5e76\u884c\u65b9\u6cd5\uff0c\u4f7f\u7528HydraGNN\u67b6\u6784\u5728\u591a\u4e2a\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\u548c\u6d4b\u8bd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u6269\u5c55\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u9884\u8bad\u7ec3\u671f\u95f4\u5904\u7406\u591a\u6e90\u3001\u591a\u4fdd\u771f\u5ea6\u6570\u636e\u7684\u6311\u6218\uff0c\u6700\u8fd1\u7684\u7814\u7a76\u91c7\u7528\u4e86\u591a\u4efb\u52a1\u5b66\u4e60\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4efb\u52a1\u5e76\u884c\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528GPU\u52a0\u901f\u5c06\u6bcf\u4e2a\u5934\u5206\u5e03\u5728\u8ba1\u7b97\u8d44\u6e90\u4e2d\u3002", "result": "\u5728\u6765\u81ea\u4e94\u4e2a\u6570\u636e\u96c6\u7684\u8d85\u8fc7 2400 \u4e07\u4e2a\u7ed3\u6784\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\uff0c\u5e76\u5728 Perlmutter\u3001Aurora \u548c Frontier \u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e09\u79cd\u4e0d\u540c\u7684\u8d85\u7ea7\u8ba1\u7b97\u673a\u67b6\u6784\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6548\u6269\u5c55\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2506.21624", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21624", "abs": "https://arxiv.org/abs/2506.21624", "authors": ["Bla\u017e \u0160krlj", "Yonatan Karni", "Grega Ga\u0161per\u0161i\u010d", "Bla\u017e Mramor", "Yulia Stolin", "Martin Jakomin", "Jasna Urban\u010di\u010d", "Yuval Dishi", "Natalia Silberstein", "Ophir Friedler", "Assaf Klein"], "title": "DCN^2: Interplay of Implicit Collision Weights and Explicit Cross Layers for Large-Scale Recommendation", "comment": "AdKDD 25", "summary": "The Deep and Cross architecture (DCNv2) is a robust production baseline and\nis integral to numerous real-life recommender systems. Its inherent efficiency\nand ability to model interactions often result in models that are both simpler\nand highly competitive compared to more computationally demanding alternatives,\nsuch as Deep FFMs. In this work, we introduce three significant algorithmic\nimprovements to the DCNv2 architecture, detailing their formulation and\nbehavior at scale. The enhanced architecture we refer to as DCN^2 is actively\nused in a live recommender system, processing over 0.5 billion predictions per\nsecond across diverse use cases where it out-performed DCNv2, both offline and\nonline (ab tests). These improvements effectively address key limitations\nobserved in the DCNv2, including information loss in Cross layers, implicit\nmanagement of collisions through learnable lookup-level weights, and explicit\nmodeling of pairwise similarities with a custom layer that emulates FFMs'\nbehavior. The superior performance of DCN^2 is also demonstrated on four\npublicly available benchmark data sets.", "AI": {"tldr": "DCN^2\u901a\u8fc7\u4e09\u4e2a\u7b97\u6cd5\u6539\u8fdb\u589e\u5f3a\u4e86DCNv2\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u4fe1\u606f\u4e22\u5931\u3001\u51b2\u7a81\u7ba1\u7406\u548c\u6210\u5bf9\u76f8\u4f3c\u6027\u5efa\u6a21\u7b49\u5173\u952e\u9650\u5236\uff0c\u5e76\u5728\u5b9e\u9645\u63a8\u8350\u7cfb\u7edf\u548c\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "motivation": "DCNv2\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u751f\u4ea7\u57fa\u7ebf\uff0c\u5e76\u4e14\u662f\u4f17\u591a\u5b9e\u9645\u63a8\u8350\u7cfb\u7edf\u4e0d\u53ef\u6216\u7f3a\u7684\u4e00\u90e8\u5206\u3002\u5176\u56fa\u6709\u7684\u6548\u7387\u548c\u5efa\u6a21\u4ea4\u4e92\u7684\u80fd\u529b\u901a\u5e38\u4f1a\u4ea7\u751f\u66f4\u7b80\u5355\u4e14\u4e0e\u8ba1\u7b97\u8981\u6c42\u66f4\u9ad8\u7684\u66ff\u4ee3\u65b9\u6848\uff08\u5982Deep FFM\uff09\u76f8\u6bd4\u66f4\u5177\u7ade\u4e89\u529b\u7684\u6a21\u578b\u3002", "method": "\u5f15\u5165\u4e86\u4e09\u4e2a\u5bf9DCNv2\u67b6\u6784\u7684\u91cd\u5927\u7b97\u6cd5\u6539\u8fdb\uff0c\u5305\u62ec\u89e3\u51b3Cross\u5c42\u4e2d\u7684\u4fe1\u606f\u4e22\u5931\u3001\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u67e5\u627e\u7ea7\u522b\u6743\u91cd\u9690\u5f0f\u7ba1\u7406\u51b2\u7a81\u4ee5\u53ca\u4f7f\u7528\u81ea\u5b9a\u4e49\u5c42\u663e\u5f0f\u5efa\u6a21\u6210\u5bf9\u76f8\u4f3c\u6027\u3002", "result": "DCN^2\u67b6\u6784\u5df2\u5728\u4e00\u4e2a\u5b9e\u65f6\u63a8\u8350\u7cfb\u7edf\u4e2d\u4f7f\u7528\uff0c\u5728\u5404\u79cd\u7528\u4f8b\u4e2d\u6bcf\u79d2\u5904\u7406\u8d85\u8fc75\u4ebf\u6b21\u9884\u6d4b\uff0c\u5e76\u4e14\u4f18\u4e8eDCNv2\u3002", "conclusion": "DCN^2\u5728\u79bb\u7ebf\u548c\u5728\u7ebf\u5b9e\u9a8c\u4e2d\u5747\u4f18\u4e8eDCNv2\uff0c\u5e76\u5728\u56db\u4e2a\u516c\u5171\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2506.21826", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21826", "abs": "https://arxiv.org/abs/2506.21826", "authors": ["Rafael Sterzinger", "Marco Peer", "Robert Sablatnig"], "title": "Few-Shot Segmentation of Historical Maps via Linear Probing of Vision Foundation Models", "comment": "18 pages, accepted at ICDAR2025", "summary": "As rich sources of history, maps provide crucial insights into historical\nchanges, yet their diverse visual representations and limited annotated data\npose significant challenges for automated processing. We propose a simple yet\neffective approach for few-shot segmentation of historical maps, leveraging the\nrich semantic embeddings of large vision foundation models combined with\nparameter-efficient fine-tuning. Our method outperforms the state-of-the-art on\nthe Siegfried benchmark dataset in vineyard and railway segmentation, achieving\n+5% and +13% relative improvements in mIoU in 10-shot scenarios and around +20%\nin the more challenging 5-shot setting. Additionally, it demonstrates strong\nperformance on the ICDAR 2021 competition dataset, attaining a mean PQ of 67.3%\nfor building block segmentation, despite not being optimized for this\nshape-sensitive metric, underscoring its generalizability. Notably, our\napproach maintains high performance even in extremely low-data regimes (10- &\n5-shot), while requiring only 689k trainable parameters - just 0.21% of the\ntotal model size. Our approach enables precise segmentation of diverse\nhistorical maps while drastically reducing the need for manual annotations,\nadvancing automated processing and analysis in the field. Our implementation is\npublicly available at:\nhttps://github.com/RafaelSterzinger/few-shot-map-segmentation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u5386\u53f2\u5730\u56fe\u5c11\u6837\u672c\u5206\u5272\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u5927\u578b\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u4e30\u5bcc\u8bed\u4e49\u5d4c\u5165\u7ed3\u5408\u53c2\u6570\u9ad8\u6548\u7684\u5fae\u8c03\uff0c\u5728\u4f4e\u6570\u636e\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\uff0c\u5e76\u51cf\u5c11\u4e86\u624b\u52a8\u6ce8\u91ca\u7684\u9700\u6c42\u3002", "motivation": "\u5386\u53f2\u5730\u56fe\u4f5c\u4e3a\u4e30\u5bcc\u7684\u5386\u53f2\u8d44\u6e90\uff0c\u4e3a\u4e86\u89e3\u5386\u53f2\u53d8\u8fc1\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u89c1\u89e3\uff0c\u4f46\u5176\u591a\u6837\u5316\u7684\u89c6\u89c9\u8868\u793a\u548c\u6709\u9650\u7684\u6ce8\u91ca\u6570\u636e\u7ed9\u81ea\u52a8\u5904\u7406\u5e26\u6765\u4e86\u5de8\u5927\u7684\u6311\u6218\u3002", "method": "\u5229\u7528\u5927\u578b\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u4e30\u5bcc\u8bed\u4e49\u5d4c\u5165\u7ed3\u5408\u53c2\u6570\u9ad8\u6548\u7684\u5fae\u8c03\uff0c\u5b9e\u73b0\u5386\u53f2\u5730\u56fe\u7684\u5c11\u6837\u672c\u5206\u5272\u3002", "result": "\u5728 Siegfried \u57fa\u51c6\u6570\u636e\u96c6\u7684\u8461\u8404\u56ed\u548c\u94c1\u8def\u5206\u5272\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5728 10-shot \u573a\u666f\u4e2d\u5b9e\u73b0\u4e86 +5% \u548c +13% \u7684 mIoU \u76f8\u5bf9\u63d0\u5347\uff0c\u5728\u66f4\u5177\u6311\u6218\u6027\u7684 5-shot \u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u4e86\u7ea6 +20% \u7684\u63d0\u5347\u3002\u6b64\u5916\uff0c\u5728 ICDAR 2021 \u7ade\u8d5b\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u5728\u5efa\u7b51\u7269\u5206\u5272\u4e2d\u83b7\u5f97\u4e86 67.3% \u7684\u5e73\u5747 PQ\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5373\u4f7f\u5728\u6781\u4f4e\u6570\u636e\u60c5\u51b5\u4e0b\uff0810-shot \u548c 5-shot\uff09\u4e5f\u80fd\u4fdd\u6301\u9ad8\u6027\u80fd\uff0c\u540c\u65f6\u4ec5\u9700\u8981 689k \u53ef\u8bad\u7ec3\u53c2\u6570\uff08\u4ec5\u5360\u603b\u6a21\u578b\u5927\u5c0f\u7684 0.21%\uff09\uff0c\u4ece\u800c\u51cf\u5c11\u4e86\u5bf9\u624b\u52a8\u6ce8\u91ca\u7684\u9700\u6c42\uff0c\u4fc3\u8fdb\u4e86\u8be5\u9886\u57df\u7684\u81ea\u52a8\u5316\u5904\u7406\u548c\u5206\u6790\u3002"}}
{"id": "2506.21564", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21564", "abs": "https://arxiv.org/abs/2506.21564", "authors": ["Jiyan Liu", "Youzheng Liu", "Taihang Wang", "Xiaoman Xu", "Yimin Wang", "Ye Jiang"], "title": "Team QUST at SemEval-2025 Task 10: Evaluating Large Language Models in Multiclass Multi-label Classification of News Entity Framing", "comment": null, "summary": "This paper describes the participation of QUST_NLP in the SemEval-2025 Task\n7. We propose a three-stage retrieval framework specifically designed for\nfact-checked claim retrieval. Initially, we evaluate the performance of several\nretrieval models and select the one that yields the best results for candidate\nretrieval. Next, we employ multiple re-ranking models to enhance the candidate\nresults, with each model selecting the Top-10 outcomes. In the final stage, we\nutilize weighted voting to determine the final retrieval outcomes. Our approach\nachieved 5th place in the monolingual track and 7th place in the crosslingual\ntrack. We release our system code at:\nhttps://github.com/warmth27/SemEval2025_Task7.", "AI": {"tldr": "QUST_NLP \u53c2\u4e0e\u4e86 SemEval-2025 Task 7\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u9636\u6bb5\u68c0\u7d22\u6846\u67b6\uff0c\u5728\u5355\u8bed\u8d5b\u9053\u83b7\u5f97\u7b2c 5 \u540d\uff0c\u5728\u8de8\u8bed\u8a00\u8d5b\u9053\u83b7\u5f97\u7b2c 7 \u540d\u3002", "motivation": "\u53c2\u4e0e SemEval-2025 Task 7\u3002", "method": "\u4e00\u4e2a\u4e13\u95e8\u4e3a\u4e8b\u5b9e\u6838\u67e5\u58f0\u660e\u68c0\u7d22\u8bbe\u8ba1\u7684\u4e09\u9636\u6bb5\u68c0\u7d22\u6846\u67b6\u3002", "result": "\u4f7f\u7528\u591a\u4e2a\u91cd\u6392\u5e8f\u6a21\u578b\u6765\u589e\u5f3a\u5019\u9009\u7ed3\u679c\uff0c\u6bcf\u4e2a\u6a21\u578b\u9009\u62e9\u524d 10 \u4e2a\u7ed3\u679c\u3002\u5728\u6700\u540e\u9636\u6bb5\uff0c\u6211\u4eec\u5229\u7528\u52a0\u6743\u6295\u7968\u6765\u786e\u5b9a\u6700\u7ec8\u68c0\u7d22\u7ed3\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5355\u8bed\u8d5b\u9053\u83b7\u5f97\u7b2c 5 \u540d\uff0c\u5728\u8de8\u8bed\u8a00\u8d5b\u9053\u83b7\u5f97\u7b2c 7 \u540d\u3002"}}
{"id": "2506.22068", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22068", "abs": "https://arxiv.org/abs/2506.22068", "authors": ["Shengyue Yao", "Runqing Guo", "Yangyang Qin", "Miangbing Meng", "Jipeng Cao", "Yilun Lin", "Yisheng Lv", "Fei-Yue Wang"], "title": "Query as Test: An Intelligent Driving Test and Data Storage Method for Integrated Cockpit-Vehicle-Road Scenarios", "comment": "Submitted to IEEE Transaction on Vehicular Technology", "summary": "With the deep penetration of Artificial Intelligence (AI) in the\ntransportation sector, intelligent cockpits, autonomous driving, and\nintelligent road networks are developing at an unprecedented pace. However, the\ndata ecosystems of these three key areas are increasingly fragmented and\nincompatible. Especially, existing testing methods rely on data stacking, fail\nto cover all edge cases, and lack flexibility. To address this issue, this\npaper introduces the concept of \"Query as Test\" (QaT). This concept shifts the\nfocus from rigid, prescripted test cases to flexible, on-demand logical queries\nagainst a unified data representation. Specifically, we identify the need for a\nfundamental improvement in data storage and representation, leading to our\nproposal of \"Extensible Scenarios Notations\" (ESN). ESN is a novel declarative\ndata framework based on Answer Set Programming (ASP), which uniformly\nrepresents heterogeneous multimodal data from the cockpit, vehicle, and road as\na collection of logical facts and rules. This approach not only achieves deep\nsemantic fusion of data, but also brings three core advantages: (1) supports\ncomplex and flexible semantic querying through logical reasoning; (2) provides\nnatural interpretability for decision-making processes; (3) allows for\non-demand data abstraction through logical rules, enabling fine-grained privacy\nprotection. We further elaborate on the QaT paradigm, transforming the\nfunctional validation and safety compliance checks of autonomous driving\nsystems into logical queries against the ESN database, significantly enhancing\nthe expressiveness and formal rigor of the testing. Finally, we introduce the\nconcept of \"Validation-Driven Development\" (VDD), which suggests to guide\ndevelopments by logical validation rather than quantitative testing in the era\nof Large Language Models, in order to accelerating the iteration and\ndevelopment process.", "AI": {"tldr": "This paper introduces the concept of \"Query as Test\" (QaT) and proposes Extensible Scenarios Notations (ESN) to address the issue of fragmented data ecosystems in autonomous driving. It also introduces Validation-Driven Development (VDD) to accelerate the development process.", "motivation": "The data ecosystems of intelligent cockpits, autonomous driving, and intelligent road networks are increasingly fragmented and incompatible, and existing testing methods lack flexibility and fail to cover all edge cases.", "method": "The paper proposes Extensible Scenarios Notations (ESN), a novel declarative data framework based on Answer Set Programming (ASP), to uniformly represent heterogeneous multimodal data.", "result": "ESN achieves deep semantic fusion of data and supports complex semantic querying, natural interpretability, and on-demand data abstraction for privacy protection. The QaT paradigm transforms functional validation and safety compliance checks into logical queries against the ESN database.", "conclusion": "This paper introduces Validation-Driven Development (VDD) to guide developments by logical validation rather than quantitative testing, aiming to accelerate the iteration and development process."}}
{"id": "2506.21797", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21797", "abs": "https://arxiv.org/abs/2506.21797", "authors": ["Peihao Wang", "Zhangyang Wang"], "title": "Why Neural Network Can Discover Symbolic Structures with Gradient-based Training: An Algebraic and Geometric Foundation for Neurosymbolic Reasoning", "comment": "International Conference on Neuro-symbolic Systems (NeuS), 2025", "summary": "We develop a theoretical framework that explains how discrete symbolic\nstructures can emerge naturally from continuous neural network training\ndynamics. By lifting neural parameters to a measure space and modeling training\nas Wasserstein gradient flow, we show that under geometric constraints, such as\ngroup invariance, the parameter measure $\\mu_t$ undergoes two concurrent\nphenomena: (1) a decoupling of the gradient flow into independent optimization\ntrajectories over some potential functions, and (2) a progressive contraction\non the degree of freedom. These potentials encode algebraic constraints\nrelevant to the task and act as ring homomorphisms under a commutative\nsemi-ring structure on the measure space. As training progresses, the network\ntransitions from a high-dimensional exploration to compositional\nrepresentations that comply with algebraic operations and exhibit a lower\ndegree of freedom. We further establish data scaling laws for realizing\nsymbolic tasks, linking representational capacity to the group invariance that\nfacilitates symbolic solutions. This framework charts a principled foundation\nfor understanding and designing neurosymbolic systems that integrate continuous\nlearning with discrete algebraic reasoning.", "AI": {"tldr": "developed a theoretical framework that explains how discrete symbolic structures can emerge naturally from continuous neural network training dynamics", "motivation": "We develop a theoretical framework that explains how discrete symbolic structures can emerge naturally from continuous neural network training dynamics.", "method": "lifting neural parameters to a measure space and modeling training as Wasserstein gradient flow", "result": "the parameter measure $\\mu_t$ undergoes two concurrent phenomena: (1) a decoupling of the gradient flow into independent optimization trajectories over some potential functions, and (2) a progressive contraction on the degree of freedom. These potentials encode algebraic constraints relevant to the task and act as ring homomorphisms under a commutative semi-ring structure on the measure space. As training progresses, the network transitions from a high-dimensional exploration to compositional representations that comply with algebraic operations and exhibit a lower degree of freedom. We further establish data scaling laws for realizing symbolic tasks, linking representational capacity to the group invariance that facilitates symbolic solutions.", "conclusion": "This framework charts a principled foundation for understanding and designing neurosymbolic systems that integrate continuous learning with discrete algebraic reasoning."}}
{"id": "2506.21638", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21638", "abs": "https://arxiv.org/abs/2506.21638", "authors": ["Tao Feng", "Zhigang Hua", "Zijie Lei", "Yan Xie", "Shuang Yang", "Bo Long", "Jiaxuan You"], "title": "IRanker: Towards Ranking Foundation Model", "comment": null, "summary": "Ranking tasks are ubiquitous, encompassing applications such as\nrecommendation systems, LLM routing, and item re-ranking. We propose to unify\nthese tasks using a single ranking foundation model (FM), as it eliminates the\nneed for designing different models for each specific ranking task. However,\nunlike general supervision tasks in LLMs, ranking tasks do not have clear\nlabels for supervision, posing great challenges to developing a ranking FM. To\novercome these challenges, we propose IRanker, a ranking FM framework with\nreinforcement learning (RL) and iterative decoding. Our insight is to decompose\nthe complex ranking task into an iterative decoding process that eliminates the\nworst candidate from the candidate pool step by step, which significantly\nreduces the output combinatorial space and better utilizes the limited context\nlength during RL training. We meticulously train and comprehensively evaluate\nan IRanker-3B model on nine datasets across three scenarios: recommendation,\nrouting, and passage ranking. The results show that a single IRanker-3B\nachieves state-of-the-art results on several datasets compared to models of\nsimilar size, and even surpasses the performance of larger models on certain\ndatasets. We further demonstrate the effectiveness of our RL design and the\nrobustness of the iterative mechanism across different LLM sizes. Moreover, we\nconducted both in-domain and out-of-domain zero-shot generalization\nexperiments, which showed that IRanker-3B achieved good generalization on\nin-domain ranking tasks compared to the base LLM by at least 5% improvement.\nSurprisingly, on out-of-domain generic LLM tasks, IRanker-3B outperformed the\nbase model by at least 9% on GSM8K, IFEval, and MathQA. In addition, the\nthoughts generated by IRanker-3B during training could further enhance\nzero-shot LLM performance.", "AI": {"tldr": "IRanker, a ranking foundation model using RL and iterative decoding, achieves state-of-the-art ranking performance and generalizes well, even improving generic LLM tasks.", "motivation": "Ranking tasks are ubiquitous, but lack clear labels for supervision, posing challenges to developing a ranking FM. The goal is to unify ranking tasks using a single ranking foundation model (FM).", "method": "A ranking FM framework with reinforcement learning (RL) and iterative decoding is proposed. The complex ranking task is decomposed into an iterative decoding process.", "result": "IRanker-3B achieves state-of-the-art results on several datasets, surpasses larger models on certain datasets, and shows good generalization. It also enhances zero-shot LLM performance.", "conclusion": "IRanker-3B achieves state-of-the-art results on several datasets and demonstrates good generalization on in-domain ranking tasks. It also outperforms the base model on out-of-domain generic LLM tasks."}}
{"id": "2506.21832", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21832", "abs": "https://arxiv.org/abs/2506.21832", "authors": ["Minh-Loi Nguyen", "Quang-Khai Le", "Tam V. Nguyen", "Minh-Triet Tran", "Trung-Nghia Le"], "title": "TaleForge: Interactive Multimodal System for Personalized Story Creation", "comment": null, "summary": "Storytelling is a deeply personal and creative process, yet existing methods\noften treat users as passive consumers, offering generic plots with limited\npersonalization. This undermines engagement and immersion, especially where\nindividual style or appearance is crucial. We introduce TaleForge, a\npersonalized story-generation system that integrates large language models\n(LLMs) and text-to-image diffusion to embed users' facial images within both\nnarratives and illustrations. TaleForge features three interconnected modules:\nStory Generation, where LLMs create narratives and character descriptions from\nuser prompts; Personalized Image Generation, merging users' faces and outfit\nchoices into character illustrations; and Background Generation, creating scene\nbackdrops that incorporate personalized characters. A user study demonstrated\nheightened engagement and ownership when individuals appeared as protagonists.\nParticipants praised the system's real-time previews and intuitive controls,\nthough they requested finer narrative editing tools. TaleForge advances\nmultimodal storytelling by aligning personalized text and imagery to create\nimmersive, user-centric experiences.", "AI": {"tldr": "TaleForge is a personalized story-generation system that integrates LLMs and text-to-image diffusion to embed users' facial images within both narratives and illustrations, resulting in heightened engagement and ownership.", "motivation": "existing methods often treat users as passive consumers, offering generic plots with limited personalization. This undermines engagement and immersion, especially where individual style or appearance is crucial", "method": "a personalized story-generation system that integrates large language models (LLMs) and text-to-image diffusion to embed users' facial images within both narratives and illustrations. TaleForge features three interconnected modules: Story Generation, Personalized Image Generation, and Background Generation", "result": "A user study demonstrated heightened engagement and ownership when individuals appeared as protagonists. Participants praised the system's real-time previews and intuitive controls, though they requested finer narrative editing tools", "conclusion": "TaleForge advances multimodal storytelling by aligning personalized text and imagery to create immersive, user-centric experiences."}}
{"id": "2506.21565", "categories": ["cs.CL", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.21565", "abs": "https://arxiv.org/abs/2506.21565", "authors": ["Takato Ueno", "Keito Inoshita"], "title": "A Multi-Agent Probabilistic Inference Framework Inspired by Kairanban-Style CoT System with IdoBata Conversation for Debiasing", "comment": null, "summary": "Japan's kairanban culture and idobata conversations have long functioned as\ntraditional communication practices that foster nuanced dialogue among\ncommunity members and contribute to the formation of social balance. Inspired\nby these information exchange processes, this study proposes a multi-agent\ninference framework (KCS+IBC) that integrates multiple large language models\n(LLMs) to achieve bias mitigation, improved explainability, and probabilistic\nprediction in sentiment analysis. In addition to sequentially sharing\nprediction results, the proposed method incorporates a mid-phase casual\ndialogue session to blend formal inference with individual perspectives and\nintroduces probabilistic sentiment prediction. Experimental results show that\nKCS achieves accuracy comparable to that of a single LLM across datasets, while\nKCS+IBC exhibits a consistent decrease in entropy and a gradual increase in\nvariance during the latter stages of inference, suggesting the framework's\nability to balance aggregation and diversity of predictions. Future work will\nquantitatively assess the impact of these characteristics on bias correction\nand aim to develop more advanced sentiment analysis systems.", "AI": {"tldr": "This paper proposes KCS+IBC, a multi-agent LLM framework inspired by Japanese communication practices, to improve sentiment analysis through bias mitigation, explainability, and probabilistic prediction.", "motivation": "Inspired by Japan's kairanban culture and idobata conversations, traditional communication practices that foster nuanced dialogue among community members and contribute to the formation of social balance.", "method": "A multi-agent inference framework (KCS+IBC) that integrates multiple large language models (LLMs) with a mid-phase casual dialogue session to blend formal inference with individual perspectives and introduces probabilistic sentiment prediction.", "result": "KCS achieves accuracy comparable to that of a single LLM across datasets, while KCS+IBC exhibits a consistent decrease in entropy and a gradual increase in variance during the latter stages of inference.", "conclusion": "KCS+IBC balances aggregation and diversity of predictions, as shown by a consistent decrease in entropy and a gradual increase in variance. Future work will quantitatively assess the impact of these characteristics on bias correction and develop more advanced sentiment analysis systems."}}
{"id": "2506.22183", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22183", "abs": "https://arxiv.org/abs/2506.22183", "authors": ["Camille Fran\u00e7ois", "Ludovic P\u00e9ran", "Ayah Bdeir", "Nouha Dziri", "Will Hawkins", "Yacine Jernite", "Sayash Kapoor", "Juliet Shen", "Heidy Khlaaf", "Kevin Klyman", "Nik Marda", "Marie Pellat", "Deb Raji", "Divya Siddarth", "Aviya Skowron", "Joseph Spisak", "Madhulika Srikumar", "Victor Storchan", "Audrey Tang", "Jen Weedon"], "title": "A Different Approach to AI Safety: Proceedings from the Columbia Convening on Openness in Artificial Intelligence and AI Safety", "comment": "Proceedings from the Columbia Convening on Openness in Artificial\n  Intelligence and AI Safety", "summary": "The rapid rise of open-weight and open-source foundation models is\nintensifying the obligation and reshaping the opportunity to make AI systems\nsafe. This paper reports outcomes from the Columbia Convening on AI Openness\nand Safety (San Francisco, 19 Nov 2024) and its six-week preparatory programme\ninvolving more than forty-five researchers, engineers, and policy leaders from\nacademia, industry, civil society, and government. Using a participatory,\nsolutions-oriented process, the working groups produced (i) a research agenda\nat the intersection of safety and open source AI; (ii) a mapping of existing\nand needed technical interventions and open source tools to safely and\nresponsibly deploy open foundation models across the AI development workflow;\nand (iii) a mapping of the content safety filter ecosystem with a proposed\nroadmap for future research and development. We find that openness --\nunderstood as transparent weights, interoperable tooling, and public governance\n-- can enhance safety by enabling independent scrutiny, decentralized\nmitigation, and culturally plural oversight. However, significant gaps persist:\nscarce multimodal and multilingual benchmarks, limited defenses against\nprompt-injection and compositional attacks in agentic systems, and insufficient\nparticipatory mechanisms for communities most affected by AI harms. The paper\nconcludes with a roadmap of five priority research directions, emphasizing\nparticipatory inputs, future-proof content filters, ecosystem-wide safety\ninfrastructure, rigorous agentic safeguards, and expanded harm taxonomies.\nThese recommendations informed the February 2025 French AI Action Summit and\nlay groundwork for an open, plural, and accountable AI safety discipline.", "AI": {"tldr": "\u672c\u6b21\u8bba\u6587\u62a5\u544a\u4e86\u5173\u4e8e\u4eba\u5de5\u667a\u80fd\u5f00\u653e\u6027\u548c\u5b89\u5168\u6027\u7684\u54e5\u4f26\u6bd4\u4e9a\u4f1a\u8bae\u7684\u6210\u679c\uff0c\u8be5\u4f1a\u8bae\u5f3a\u8c03\u4e86\u5f00\u653e\u6027\u5728\u63d0\u9ad8\u4eba\u5de5\u667a\u80fd\u5b89\u5168\u65b9\u9762\u7684\u4f5c\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u4f18\u5148\u65b9\u5411\u3002", "motivation": "\u5f00\u653e\u6743\u91cd\u548c\u5f00\u6e90\u57fa\u7840\u6a21\u578b\u7684\u8fc5\u901f\u5d1b\u8d77\uff0c\u52a0\u5f3a\u4e86\u4f7f\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u5b89\u5168\u7684\u4e49\u52a1\uff0c\u5e76\u91cd\u5851\u4e86\u673a\u9047\u3002", "method": "\u672c\u6b21\u7814\u7a76\u4f7f\u7528\u4e86\u53c2\u4e0e\u5f0f\u7684\u3001\u4ee5\u89e3\u51b3\u65b9\u6848\u4e3a\u5bfc\u5411\u7684\u8fc7\u7a0b\uff0c\u7531\u6765\u81ea\u5b66\u672f\u754c\u3001\u5de5\u4e1a\u754c\u3001\u6c11\u95f4\u793e\u4f1a\u548c\u653f\u5e9c\u7684\u8d85\u8fc7 45 \u4f4d\u7814\u7a76\u4eba\u5458\u3001\u5de5\u7a0b\u5e08\u548c\u653f\u7b56\u9886\u5bfc\u8005\u7ec4\u6210\u7684\u5de5\u4f5c\u7ec4\uff0c\u4ea7\u51fa\u4e86\u7814\u7a76\u8bae\u7a0b\u3001\u6280\u672f\u5e72\u9884\u548c\u5f00\u6e90\u5de5\u5177\u7684\u6620\u5c04\u4ee5\u53ca\u5185\u5bb9\u5b89\u5168\u8fc7\u6ee4\u5668\u751f\u6001\u7cfb\u7edf\u7684\u6620\u5c04\u3002", "result": "\u5de5\u4f5c\u7ec4\u4ea7\u51fa\u4e86 (i) \u5b89\u5168\u548c\u5f00\u6e90\u4eba\u5de5\u667a\u80fd\u4ea4\u53c9\u9886\u57df\u7684\u7814\u7a76\u8bae\u7a0b\uff1b(ii) \u73b0\u6709\u548c\u6240\u9700\u6280\u672f\u5e72\u9884\u548c\u5f00\u6e90\u5de5\u5177\u7684\u6620\u5c04\uff0c\u4ee5\u5728\u4eba\u5de5\u667a\u80fd\u5f00\u53d1\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u5b89\u5168\u548c\u8d1f\u8d23\u4efb\u5730\u90e8\u7f72\u5f00\u653e\u57fa\u7840\u6a21\u578b\uff1b(iii) \u5185\u5bb9\u5b89\u5168\u8fc7\u6ee4\u5668\u751f\u6001\u7cfb\u7edf\u7684\u6620\u5c04\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u548c\u5f00\u53d1\u7684\u8def\u7ebf\u56fe\u3002", "conclusion": "\u5f00\u653e\u6027\uff08\u900f\u660e\u7684\u6743\u91cd\u3001\u53ef\u4e92\u64cd\u4f5c\u7684\u5de5\u5177\u548c\u516c\u5171\u6cbb\u7406\uff09\u53ef\u4ee5\u901a\u8fc7\u5b9e\u73b0\u72ec\u7acb\u5ba1\u67e5\u3001\u5206\u6563\u7f13\u89e3\u548c\u6587\u5316\u591a\u5143\u76d1\u7763\u6765\u63d0\u9ad8\u5b89\u5168\u6027\u3002\u7136\u800c\uff0c\u4ecd\u7136\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff1a\u591a\u6a21\u6001\u548c\u591a\u8bed\u8a00\u57fa\u51c6\u7684\u7a00\u7f3a\u3001\u9488\u5bf9\u4ee3\u7406\u7cfb\u7edf\u4e2d prompt \u6ce8\u5165\u548c\u7ec4\u5408\u653b\u51fb\u7684\u6709\u9650\u9632\u5fa1\uff0c\u4ee5\u53ca\u53d7\u4eba\u5de5\u667a\u80fd\u5371\u5bb3\u5f71\u54cd\u6700\u4e25\u91cd\u7684\u793e\u533a\u7684\u53c2\u4e0e\u673a\u5236\u4e0d\u8db3\u3002\u8bba\u6587\u6700\u540e\u63d0\u51fa\u4e86\u4e94\u4e2a\u4f18\u5148\u7814\u7a76\u65b9\u5411\u7684\u8def\u7ebf\u56fe\uff0c\u5f3a\u8c03\u53c2\u4e0e\u6027\u6295\u5165\u3001\u9762\u5411\u672a\u6765\u7684\u5185\u5bb9\u8fc7\u6ee4\u5668\u3001\u5168\u751f\u6001\u7cfb\u7edf\u7684\u5b89\u5168\u57fa\u7840\u8bbe\u65bd\u3001\u4e25\u683c\u7684\u4ee3\u7406\u4fdd\u969c\u63aa\u65bd\u548c\u6269\u5c55\u7684\u5371\u5bb3\u5206\u7c7b\u3002"}}
{"id": "2506.21833", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21833", "abs": "https://arxiv.org/abs/2506.21833", "authors": ["Kunjal Panchal", "Sunav Choudhary", "Yuriy Brun", "Hui Guan"], "title": "The Cost of Avoiding Backpropagation", "comment": null, "summary": "Forward-mode automatic differentiation (FmAD) and zero-order (ZO)\noptimization have been proposed as memory-efficient alternatives to\nbackpropagation (BP) for gradient computation, especially in low-resource\nsettings. However, their practical benefits remain unclear due to two key gaps:\na lack of comparison against memory-efficient BP variants, such as activation\ncheckpointing, and a lack of a unified theoretical analysis. This work presents\na comprehensive theoretical and empirical comparison of BP, FmAD, and ZO\nmethods. Our theoretical analysis shows that while FmAD, and ZO can reduce\nmemory usage, they incur significant costs in accuracy, convergence speed, and\ncomputation compared to BP with checkpointing. These drawbacks worsen with\nlarger models or constrained perturbation budgets. Empirical experiments on\nlarge language and vision-language models show that BP with checkpointing\noutperforms FmAD and ZO variants, including those enhanced with variance\nreduction, achieving up to 31.1% higher accuracy, 34.8% faster convergence, and\n3.8x fewer computations at comparable memory usage. Our results highlight\nfundamental limitations of FmAD and ZO, and reaffirm BP with checkpointing as\nthe most effective strategy for model training under memory-constrained\nsettings. Our code is available at\nhttps://github.com/Astuary/The_Cost_of_Avoiding_Backpropagation.", "AI": {"tldr": "\u5728\u5927\u578b\u8bed\u8a00\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u7ecf\u9a8c\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u68c0\u67e5\u70b9\u7684BP\u4f18\u4e8eFmAD\u548cZO\u53d8\u4f53\uff0c\u5728\u76f8\u5f53\u7684\u5185\u5b58\u4f7f\u7528\u60c5\u51b5\u4e0b\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u4e8631.1%\uff0c\u6536\u655b\u901f\u5ea6\u63d0\u9ad8\u4e8634.8%\uff0c\u8ba1\u7b97\u91cf\u51cf\u5c11\u4e863.8\u500d\u3002", "motivation": "\u6b63\u5411\u6a21\u5f0f\u81ea\u52a8\u5fae\u5206\uff08FmAD\uff09\u548c\u96f6\u9636\uff08ZO\uff09\u4f18\u5316\u5df2\u88ab\u63d0\u8bae\u4f5c\u4e3a\u68af\u5ea6\u8ba1\u7b97\u4e2d\u53cd\u5411\u4f20\u64ad\uff08BP\uff09\u7684\u5185\u5b58\u6548\u7387\u66ff\u4ee3\u65b9\u6848\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\u3002\u7136\u800c\uff0c\u7531\u4e8e\u4e24\u4e2a\u5173\u952e\u5dee\u8ddd\uff0c\u5b83\u4eec\u7684\u5b9e\u9645\u597d\u5904\u4ecd\u4e0d\u6e05\u695a\uff1a\u7f3a\u4e4f\u4e0e\u5185\u5b58\u6548\u7387\u7684BP\u53d8\u4f53\uff08\u5982\u6fc0\u6d3b\u68c0\u67e5\u70b9\uff09\u7684\u6bd4\u8f83\uff0c\u4ee5\u53ca\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u5206\u6790\u3002", "method": "\u5bf9\u53cd\u5411\u4f20\u64ad\u3001\u524d\u5411\u6a21\u5f0f\u81ea\u52a8\u5fae\u5206\u548c\u96f6\u9636\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u7684\u7406\u8bba\u548c\u5b9e\u8bc1\u6bd4\u8f83\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u867d\u7136FmAD\u548cZO\u53ef\u4ee5\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\uff0c\u4f46\u4e0e\u4f7f\u7528\u68c0\u67e5\u70b9\u7684BP\u76f8\u6bd4\uff0c\u5b83\u4eec\u5728\u51c6\u786e\u6027\u3001\u6536\u655b\u901f\u5ea6\u548c\u8ba1\u7b97\u65b9\u9762\u4f1a\u4ea7\u751f\u663e\u8457\u7684\u6210\u672c\u3002\u968f\u7740\u6a21\u578b\u53d8\u5927\u6216\u7ea6\u675f\u6270\u52a8\u9884\u7b97\uff0c\u8fd9\u4e9b\u7f3a\u70b9\u4f1a\u53d8\u5f97\u66f4\u7cdf\u3002", "conclusion": "\u53cd\u5411\u4f20\u64ad\u4e0e\u68c0\u67e5\u70b9\u4ecd\u7136\u662f\u5728\u5185\u5b58\u53d7\u9650\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u7684\u6700\u6709\u6548\u7b56\u7565\u3002"}}
{"id": "2506.21913", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21913", "abs": "https://arxiv.org/abs/2506.21913", "authors": ["Zunran Wang", "Zheng Shenpeng", "Wang Shenglan", "Minghui Zhao", "Zhonghua Li"], "title": "HyReC: Exploring Hybrid-based Retriever for Chinese", "comment": null, "summary": "Hybrid-based retrieval methods, which unify dense-vector and lexicon-based\nretrieval, have garnered considerable attention in the industry due to\nperformance enhancement. However, despite their promising results, the\napplication of these hybrid paradigms in Chinese retrieval contexts has\nremained largely underexplored. In this paper, we introduce HyReC, an\ninnovative end-to-end optimization method tailored specifically for\nhybrid-based retrieval in Chinese. HyReC enhances performance by integrating\nthe semantic union of terms into the representation model. Additionally, it\nfeatures the Global-Local-Aware Encoder (GLAE) to promote consistent semantic\nsharing between lexicon-based and dense retrieval while minimizing the\ninterference between them. To further refine alignment, we incorporate a\nNormalization Module (NM) that fosters mutual benefits between the retrieval\napproaches. Finally, we evaluate HyReC on the C-MTEB retrieval benchmark to\ndemonstrate its effectiveness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e2d\u6587\u6df7\u5408\u68c0\u7d22\u4f18\u5316\u65b9\u6cd5HyReC\uff0c\u5e76\u5728C-MTEB\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u6df7\u5408\u68c0\u7d22\u65b9\u6cd5\u7edf\u4e00\u4e86\u57fa\u4e8e\u5bc6\u96c6\u5411\u91cf\u548c\u57fa\u4e8e\u8bcd\u5178\u7684\u68c0\u7d22\uff0c\u7531\u4e8e\u6027\u80fd\u63d0\u5347\u800c\u5728\u5de5\u4e1a\u754c\u53d7\u5230\u4e86\u76f8\u5f53\u5927\u7684\u5173\u6ce8\u3002\u7136\u800c\uff0c\u5c3d\u7ba1\u5b83\u4eec\u7684\u7ed3\u679c\u5f88\u6709\u5e0c\u671b\uff0c\u4f46\u8fd9\u4e9b\u6df7\u5408\u8303\u5f0f\u5728\u4e2d\u6587\u68c0\u7d22\u73af\u5883\u4e2d\u7684\u5e94\u7528\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u672a\u88ab\u63a2\u7d22\u3002", "method": "\u521b\u65b0\u6027\u7684\u7aef\u5230\u7aef\u4f18\u5316\u65b9\u6cd5HyReC\uff0c\u4e13\u4e3a\u4e2d\u6587\u6df7\u5408\u68c0\u7d22\u8bbe\u8ba1\u3002\u5b83\u96c6\u6210\u4e86\u8bcd\u8bed\u7684\u8bed\u4e49\u8054\u5408\uff0c\u5e76\u4f7f\u7528GLAE\u4fc3\u8fdb\u8bcd\u6c47\u548c\u5bc6\u96c6\u68c0\u7d22\u4e4b\u95f4\u7684\u4e00\u81f4\u8bed\u4e49\u5171\u4eab\uff0c\u540c\u65f6\u5c3d\u91cf\u51cf\u5c11\u5b83\u4eec\u4e4b\u95f4\u7684\u5e72\u6270\u3002\u6b64\u5916\uff0c\u8fd8\u5305\u542b\u4e00\u4e2a\u6807\u51c6\u5316\u6a21\u5757\uff08NM\uff09\u6765\u4fc3\u8fdb\u68c0\u7d22\u65b9\u6cd5\u4e4b\u95f4\u7684\u4e92\u60e0\u4e92\u5229\u3002", "result": "\u901a\u8fc7\u5c06\u672f\u8bed\u7684\u8bed\u4e49\u8054\u5408\u96c6\u6210\u5230\u8868\u793a\u6a21\u578b\u4e2d\u6765\u589e\u5f3a\u6027\u80fd\u3002GLAE\uff08Global-Local-Aware Encoder\uff09\u7528\u4e8e\u4fc3\u8fdb\u57fa\u4e8e\u8bcd\u5178\u548c\u5bc6\u96c6\u68c0\u7d22\u4e4b\u95f4\u7684\u4e00\u81f4\u8bed\u4e49\u5171\u4eab\uff0c\u540c\u65f6\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u5b83\u4eec\u4e4b\u95f4\u7684\u5e72\u6270\u3002\u6807\u51c6\u5316\u6a21\u5757\uff08NM\uff09\u7684\u52a0\u5165\uff0c\u4fc3\u8fdb\u4e86\u68c0\u7d22\u65b9\u6cd5\u4e4b\u95f4\u7684\u4e92\u5229\u3002", "conclusion": "HyReC\u5728C-MTEB\u68c0\u7d22\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2506.21834", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21834", "abs": "https://arxiv.org/abs/2506.21834", "authors": ["Duy-Bao Bui", "Hoang-Khang Nguyen", "Trung-Nghia Le"], "title": "PrefPaint: Enhancing Image Inpainting through Expert Human Feedback", "comment": null, "summary": "Inpainting, the process of filling missing or corrupted image parts, has\nbroad applications, including medical imaging. However, in specialized fields\nlike medical polyps imaging, where accuracy and reliability are critical,\ninpainting models can generate inaccurate images, leading to significant errors\nin medical diagnosis and treatment. To ensure reliability, medical images\nshould be annotated by experts like oncologists for effective model training.\nWe propose PrefPaint, an approach that incorporates human feedback into the\ntraining process of Stable Diffusion Inpainting, bypassing the need for\ncomputationally expensive reward models. In addition, we develop a web-based\ninterface streamlines training, fine-tuning, and inference. This interactive\ninterface provides a smooth and intuitive user experience, making it easier to\noffer feedback and manage the fine-tuning process. User study on various\ndomains shows that PrefPaint outperforms existing methods, reducing visual\ninconsistencies and improving image rendering, particularly in medical\ncontexts, where our model generates more realistic polyps images.", "AI": {"tldr": "PrefPaint incorporates human feedback into the training process of Stable Diffusion Inpainting, bypassing the need for computationally expensive reward models, reduces visual inconsistencies and improving image rendering, particularly in medical contexts", "motivation": "inpainting models can generate inaccurate images, leading to significant errors in medical diagnosis and treatment", "method": "incorporates human feedback into the training process of Stable Diffusion Inpainting", "result": "PrefPaint outperforms existing methods, reducing visual inconsistencies and improving image rendering", "conclusion": "PrefPaint outperforms existing methods, reducing visual inconsistencies and improving image rendering, particularly in medical contexts, where our model generates more realistic polyps images."}}
{"id": "2506.21566", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21566", "abs": "https://arxiv.org/abs/2506.21566", "authors": ["Arwa Arif"], "title": "The Saturation Point of Backtranslation in High Quality Low Resource English Gujarati Machine Translation", "comment": "Preprint, 8 Pages", "summary": "Backtranslation BT is widely used in low resource machine translation MT to\ngenerate additional synthetic training data using monolingual corpora. While\nthis approach has shown strong improvements for many language pairs, its\neffectiveness in high quality, low resource settings remains unclear. In this\nwork, we explore the effectiveness of backtranslation for English Gujarati\ntranslation using the multilingual pretrained MBART50 model. Our baseline\nsystem, trained on a high quality parallel corpus of approximately 50,000\nsentence pairs, achieves a BLEU score of 43.8 on a validation set. We augment\nthis data with carefully filtered backtranslated examples generated from\nmonolingual Gujarati text. Surprisingly, adding this synthetic data does not\nimprove translation performance and, in some cases, slightly reduces it. We\nevaluate our models using multiple metrics like BLEU, ChrF++, TER, BLEURT and\nanalyze possible reasons for this saturation. Our findings suggest that\nbacktranslation may reach a point of diminishing returns in certain\nlow-resource settings and we discuss implications for future research.", "AI": {"tldr": "Backtranslation's effectiveness in high-quality, low-resource MT is explored. Results show diminishing returns, suggesting limitations in such settings.", "motivation": "explore the effectiveness of backtranslation for English Gujarati translation in high quality, low resource settings", "method": "backtranslation with multilingual pretrained MBART50 model", "result": "adding synthetic data does not improve translation performance and, in some cases, slightly reduces it", "conclusion": "backtranslation may reach a point of diminishing returns in certain low-resource settings"}}
{"id": "2506.22271", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22271", "abs": "https://arxiv.org/abs/2506.22271", "authors": ["Samy Badreddine", "Emile van Krieken", "Luciano Serafini"], "title": "Breaking Rank Bottlenecks in Knowledge Graph Completion", "comment": null, "summary": "Many Knowledge Graph Completion (KGC) models, despite using powerful\nencoders, rely on a simple vector-matrix multiplication to score queries\nagainst candidate object entities. When the number of entities is larger than\nthe model's embedding dimension, which in practical scenarios is often by\nseveral orders of magnitude, we have a linear output layer with a rank\nbottleneck. Such bottlenecked layers limit model expressivity. We investigate\nboth theoretically and empirically how rank bottlenecks affect KGC models. We\nfind that, by limiting the set of feasible predictions, rank bottlenecks hurt\nranking accuracy and the distribution fidelity of scores. Inspired by the\nlanguage modelling literature, we propose KGE-MoS, a mixture-based output layer\nto break rank bottlenecks in many KGC models. Our experiments on four datasets\nshow that KGE-MoS improves performance and probabilistic fit of KGC models for\na low parameter cost.", "AI": {"tldr": "KGC models rely on a simple vector-matrix multiplication to score queries, which limits model expressivity. KGE-MoS, a mixture-based output layer, is proposed to break rank bottlenecks in many KGC models.", "motivation": "rank bottlenecks hurt ranking accuracy and the distribution fidelity of scores by limiting the set of feasible predictions", "method": "a mixture-based output layer to break rank bottlenecks", "result": "KGE-MoS improves performance and probabilistic fit of KGC models", "conclusion": "KGE-MoS improves performance and probabilistic fit of KGC models for a low parameter cost."}}
{"id": "2506.21844", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21844", "abs": "https://arxiv.org/abs/2506.21844", "authors": ["Jun Ohkubo"], "title": "Koopman operator-based discussion on partial observation in stochastic systems", "comment": "23 pages, 5 figures", "summary": "It is sometimes difficult to achieve a complete observation for a full set of\nobservables, and partial observations are necessary. For deterministic systems,\nthe Mori-Zwanzig formalism provides a theoretical framework for handling\npartial observations. Recently, data-driven algorithms based on the Koopman\noperator theory have made significant progress, and there is a discussion to\nconnect the Mori-Zwanzig formalism with the Koopman operator theory. In this\nwork, we discuss the effects of partial observation in stochastic systems using\nthe Koopman operator theory. The discussion clarifies the importance of\ndistinguishing the state space and the function space in stochastic systems.\nEven in stochastic systems, the delay embedding technique is beneficial for\npartial observation, and several numerical experiments showed a power-law\nbehavior of the accuracy for the amplitude of the additive noise. We also\ndiscuss the relation between the exponent of the power-law behavior and the\neffects of partial observation.", "AI": {"tldr": "discusses the effects of partial observation in stochastic systems using the Koopman operator theory", "motivation": "It is sometimes difficult to achieve a complete observation for a full set of observables, and partial observations are necessary. For deterministic systems, the Mori-Zwanzig formalism provides a theoretical framework for handling partial observations. Recently, data-driven algorithms based on the Koopman operator theory have made significant progress, and there is a discussion to connect the Mori-Zwanzig formalism with the Koopman operator theory.", "method": "the Koopman operator theory", "result": "The discussion clarifies the importance of distinguishing the state space and the function space in stochastic systems.", "conclusion": "Even in stochastic systems, the delay embedding technique is beneficial for partial observation, and several numerical experiments showed a power-law behavior of the accuracy for the amplitude of the additive noise. We also discuss the relation between the exponent of the power-law behavior and the effects of partial observation."}}
{"id": "2506.21931", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.MA", "I.2.11; I.2.7; H.3.3"], "pdf": "https://arxiv.org/pdf/2506.21931", "abs": "https://arxiv.org/abs/2506.21931", "authors": ["Reza Yousefi Maragheh", "Pratheek Vadla", "Priyank Gupta", "Kai Zhao", "Aysenur Inan", "Kehui Yao", "Jianpeng Xu", "Praveen Kanumala", "Jason Cho", "Sushant Kumar"], "title": "ARAG: Agentic Retrieval Augmented Generation for Personalized Recommendation", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has shown promise in enhancing\nrecommendation systems by incorporating external context into large language\nmodel prompts. However, existing RAG-based approaches often rely on static\nretrieval heuristics and fail to capture nuanced user preferences in dynamic\nrecommendation scenarios. In this work, we introduce ARAG, an Agentic\nRetrieval-Augmented Generation framework for Personalized Recommendation, which\nintegrates a multi-agent collaboration mechanism into the RAG pipeline. To\nbetter understand the long-term and session behavior of the user, ARAG\nleverages four specialized LLM-based agents: a User Understanding Agent that\nsummarizes user preferences from long-term and session contexts, a Natural\nLanguage Inference (NLI) Agent that evaluates semantic alignment between\ncandidate items retrieved by RAG and inferred intent, a context summary agent\nthat summarizes the findings of NLI agent, and an Item Ranker Agent that\ngenerates a ranked list of recommendations based on contextual fit. We evaluate\nARAG accross three datasets. Experimental results demonstrate that ARAG\nsignificantly outperforms standard RAG and recency-based baselines, achieving\nup to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an\nablation study to analyse the effect by different components of ARAG. Our\nfindings highlight the effectiveness of integrating agentic reasoning into\nretrieval-augmented recommendation and provide new directions for LLM-based\npersonalization.", "AI": {"tldr": "ARAG\u901a\u8fc7\u96c6\u6210\u591a\u4ee3\u7406\u534f\u4f5c\u673a\u5236\u5230RAG\u6d41\u7a0b\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e2a\u6027\u5316\u63a8\u8350\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eRAG\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u9759\u6001\u68c0\u7d22\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5e76\u4e14\u65e0\u6cd5\u5728\u52a8\u6001\u63a8\u8350\u573a\u666f\u4e2d\u6355\u83b7\u7ec6\u81f4\u7684\u7528\u6237\u504f\u597d\u3002", "method": "\u5f15\u5165ARAG\uff0c\u4e00\u4e2a\u7528\u4e8e\u4e2a\u6027\u5316\u63a8\u8350\u7684Agentic\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u5b83\u5c06\u591a\u4ee3\u7406\u534f\u4f5c\u673a\u5236\u96c6\u6210\u5230RAG\u6d41\u7a0b\u4e2d\u3002ARAG\u5229\u7528\u56db\u4e2a\u4e13\u95e8\u7684\u57fa\u4e8eLLM\u7684\u4ee3\u7406\uff1a\u7528\u6237\u7406\u89e3\u4ee3\u7406\u3001\u81ea\u7136\u8bed\u8a00\u63a8\u7406\uff08NLI\uff09\u4ee3\u7406\u3001\u4e0a\u4e0b\u6587\u603b\u7ed3\u4ee3\u7406\u548c\u9879\u76ee\u6392\u5e8f\u4ee3\u7406\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cARAG\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u6807\u51c6RAG\u548c\u57fa\u4e8e\u8fd1\u671f\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ARAG\u663e\u8457\u4f18\u4e8e\u6807\u51c6RAG\u548c\u57fa\u4e8e\u8fd1\u671f\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0cNDCG@5\u6307\u6807\u63d0\u5347\u9ad8\u8fbe42.1%\uff0cHit@5\u6307\u6807\u63d0\u5347\u9ad8\u8fbe35.5%\u3002"}}
{"id": "2506.21835", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21835", "abs": "https://arxiv.org/abs/2506.21835", "authors": ["Xiaoqi Wang", "Clint Sebastian", "Wenbin He", "Liu Ren"], "title": "ProSAM: Enhancing the Robustness of SAM-based Visual Reference Segmentation with Probabilistic Prompts", "comment": null, "summary": "The recent advancements in large foundation models have driven the success of\nopen-set image segmentation, a task focused on segmenting objects beyond\npredefined categories. Among various prompt types (such as points, boxes,\ntexts, and visual references), visual reference segmentation stands out for its\nunique flexibility and strong zero-shot capabilities. Recently, several\nSAM-based methods have made notable progress in this task by automatically\ngenerating prompts to guide SAM. However, these methods often generate prompts\nat object boundaries due to suboptimal prompt encoder, which results in\ninstability and reduced robustness. In this work, we introduce ProSAM, a simple\nbut effective method to address the stability challenges we identified in\nexisting SAM-based visual reference segmentation approaches. By learning a\nvariational prompt encoder to predict multivariate prompt distributions, ProSAM\navoids generating prompts that lie in unstable regions, overcoming the\ninstability caused by less robust prompts. Our approach consistently surpasses\nstate-of-the-art methods on the Pascal-5$^i$ and COCO-20$^i$ datasets,\nproviding a more robust solution for visual reference segmentation.", "AI": {"tldr": "ProSAM: a simple but effective method to address the stability challenges in existing SAM-based visual reference segmentation approaches.", "motivation": "existing SAM-based methods often generate prompts at object boundaries due to suboptimal prompt encoder, which results in instability and reduced robustness.", "method": "learning a variational prompt encoder to predict multivariate prompt distributions", "result": "ProSAM avoids generating prompts that lie in unstable regions, overcoming the instability caused by less robust prompts.", "conclusion": "ProSAM consistently surpasses state-of-the-art methods on the Pascal-5$^i$ and COCO-20$^i$ datasets, providing a more robust solution for visual reference segmentation."}}
{"id": "2506.21567", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21567", "abs": "https://arxiv.org/abs/2506.21567", "authors": ["Baqer M. Merzah", "Tania Taami", "Salman Asoudeh", "Amir reza Hossein pour", "Saeed Mirzaee", "Amir Ali Bengari"], "title": "BioPars: A Pretrained Biomedical Large Language Model for Persian Biomedical Text Mining", "comment": null, "summary": "Large Language Models (LLMs) have recently gained attention in the life\nsciences due to their capacity to model, extract, and apply complex biological\ninformation. Beyond their classical use as chatbots, these systems are\nincreasingly used for complex analysis and problem-solving in specialized\nfields, including bioinformatics. First, we introduce BIOPARS-BENCH, a dataset\nfrom over 10,000 scientific articles, textbooks, and medical websites.\nBioParsQA was also introduced to evaluate the proposed model, which consists of\n5,231 Persian medical questions and answers. This study then introduces\nBioPars, a simple but accurate measure designed to assess LLMs for three main\nabilities: acquiring subject-specific knowledge, interpreting and synthesizing\nsuch knowledge, and demonstrating proper evidence. Comparing ChatGPT, Llama,\nand Galactica, our study highlights their ability to remember and retrieve\nlearned knowledge but also reveals shortcomings in addressing higher-level,\nreal-world questions and fine-grained inferences. These findings indicate the\nneed for further fine-tuning to address the capabilities of LLM in\nbioinformatics tasks. To our knowledge, BioPars is the first application of LLM\nin Persian medical QA, especially for generating long answers. Evaluation of\nfour selected medical QA datasets shows that BioPars has achieved remarkable\nresults compared to comparative approaches. The model on BioParsQA achieved a\nROUGE-L score of 29.99, which is an improvement over GPT-4 1.0. The model\nachieved a BERTScore of 90.87 with the MMR method. The MoverScore and BLEURT\nvalues were also higher in this model than the other three models. In addition,\nthe reported scores for the model are MoverScore=60.43 and BLEURT=50.78.\nBioPars is an ongoing project and all resources related to its development will\nbe made available via the following GitHub repository:\nhttps://github.com/amirap80/BioPars.", "AI": {"tldr": "This paper introduces BioPars, a new method and dataset (BioParsQA) for evaluating LLMs in Persian medical QA. BioPars outperforms existing models but highlights the need for further improvements.", "motivation": "LLMs are increasingly used for complex analysis in bioinformatics, but their capabilities in specialized fields require further investigation, especially for real-world questions and fine-grained inferences.", "method": "The study introduces BioPars, a measure to assess LLMs on subject-specific knowledge, interpretation, synthesis, and evidence demonstration. It also introduces BIOPARS-BENCH and BioParsQA datasets. The LLMs ChatGPT, Llama, and Galactica are compared.", "result": "BioPars outperforms ChatGPT, Llama, and Galactica on Persian medical QA. On BioParsQA, it achieved a ROUGE-L score of 29.99, a BERTScore of 90.87, a MoverScore of 60.43, and a BLEURT score of 50.78.", "conclusion": "BioPars achieves state-of-the-art results on Persian medical QA, demonstrating the potential of LLMs in this area but also highlighting the need for further fine-tuning."}}
{"id": "2506.22276", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22276", "abs": "https://arxiv.org/abs/2506.22276", "authors": ["Reuth Mirsky"], "title": "Artificial Intelligent Disobedience: Rethinking the Agency of Our Artificial Teammates", "comment": "Extended version of a paper accepted for publication in AI Magazine", "summary": "Artificial intelligence has made remarkable strides in recent years,\nachieving superhuman performance across a wide range of tasks. Yet despite\nthese advances, most cooperative AI systems remain rigidly obedient, designed\nto follow human instructions without question and conform to user expectations,\neven when doing so may be counterproductive or unsafe. This paper argues for\nexpanding the agency of AI teammates to include \\textit{intelligent\ndisobedience}, empowering them to make meaningful and autonomous contributions\nwithin human-AI teams. It introduces a scale of AI agency levels and uses\nrepresentative examples to highlight the importance and growing necessity of\ntreating AI autonomy as an independent research focus in cooperative settings.\nThe paper then explores how intelligent disobedience manifests across different\nautonomy levels and concludes by proposing initial boundaries and\nconsiderations for studying disobedience as a core capability of artificial\nagents.", "AI": {"tldr": "This paper argues for expanding the agency of AI teammates to include intelligent disobedience, empowering them to make meaningful and autonomous contributions within human-AI teams.", "motivation": "Most cooperative AI systems remain rigidly obedient, designed to follow human instructions without question, even when doing so may be counterproductive or unsafe.", "method": "It introduces a scale of AI agency levels and uses representative examples.", "result": "The paper explores how intelligent disobedience manifests across different autonomy levels.", "conclusion": "This paper proposes initial boundaries and considerations for studying disobedience as a core capability of artificial agents."}}
{"id": "2506.21872", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21872", "abs": "https://arxiv.org/abs/2506.21872", "authors": ["Chaofan Pan", "Xin Yang", "Yanhua Li", "Wei Wei", "Tianrui Li", "Bo An", "Jiye Liang"], "title": "A Survey of Continual Reinforcement Learning", "comment": "This work has been submitted to the IEEE TPAMI", "summary": "Reinforcement Learning (RL) is an important machine learning paradigm for\nsolving sequential decision-making problems. Recent years have witnessed\nremarkable progress in this field due to the rapid development of deep neural\nnetworks. However, the success of RL currently relies on extensive training\ndata and computational resources. In addition, RL's limited ability to\ngeneralize across tasks restricts its applicability in dynamic and real-world\nenvironments. With the arisen of Continual Learning (CL), Continual\nReinforcement Learning (CRL) has emerged as a promising research direction to\naddress these limitations by enabling agents to learn continuously, adapt to\nnew tasks, and retain previously acquired knowledge. In this survey, we provide\na comprehensive examination of CRL, focusing on its core concepts, challenges,\nand methodologies. Firstly, we conduct a detailed review of existing works,\norganizing and analyzing their metrics, tasks, benchmarks, and scenario\nsettings. Secondly, we propose a new taxonomy of CRL methods, categorizing them\ninto four types from the perspective of knowledge storage and/or transfer.\nFinally, our analysis highlights the unique challenges of CRL and provides\npractical insights into future directions.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6301\u7eed\u5f3a\u5316\u5b66\u4e60 (CRL) \u8fdb\u884c\u4e86\u5168\u9762\u7684\u68c0\u67e5\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u6838\u5fc3\u6982\u5ff5\u3001\u6311\u6218\u548c\u65b9\u6cd5\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60 (RL) \u662f\u4e00\u79cd\u91cd\u8981\u7684\u673a\u5668\u5b66\u4e60\u8303\u4f8b\uff0c\u7528\u4e8e\u89e3\u51b3\u5e8f\u8d2f\u51b3\u7b56\u95ee\u9898\u3002\u8fd1\u5e74\u6765\uff0c\u7531\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u8be5\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002\u7136\u800c\uff0cRL \u7684\u6210\u529f\u76ee\u524d\u4f9d\u8d56\u4e8e\u5927\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\u3002\u6b64\u5916\uff0cRL \u5728\u4efb\u52a1\u4e2d\u6cdb\u5316\u7684\u80fd\u529b\u6709\u9650\uff0c\u9650\u5236\u4e86\u5176\u5728\u52a8\u6001\u548c\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\u3002\u968f\u7740\u6301\u7eed\u5b66\u4e60 (CL) \u7684\u51fa\u73b0\uff0c\u6301\u7eed\u5f3a\u5316\u5b66\u4e60 (CRL) \u5df2\u6210\u4e3a\u4e00\u4e2a\u6709\u5e0c\u671b\u7684\u7814\u7a76\u65b9\u5411\uff0c\u901a\u8fc7\u4f7f\u4ee3\u7406\u80fd\u591f\u6301\u7eed\u5b66\u4e60\u3001\u9002\u5e94\u65b0\u4efb\u52a1\u548c\u4fdd\u7559\u5148\u524d\u83b7\u5f97\u7684\u77e5\u8bc6\u6765\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u5bf9\u73b0\u6709\u5de5\u4f5c\u8fdb\u884c\u4e86\u8be6\u7ec6\u7684\u56de\u987e\uff0c\u7ec4\u7ec7\u548c\u5206\u6790\u4e86\u5b83\u4eec\u7684\u6307\u6807\u3001\u4efb\u52a1\u3001\u57fa\u51c6\u548c\u573a\u666f\u8bbe\u7f6e\u3002\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684 CRL \u65b9\u6cd5\u5206\u7c7b\u6cd5\uff0c\u4ece\u77e5\u8bc6\u5b58\u50a8\u548c/\u6216\u8f6c\u79fb\u7684\u89d2\u5ea6\u5c06\u5176\u5206\u4e3a\u56db\u79cd\u7c7b\u578b\u3002", "result": "\u5bf9 CRL \u8fdb\u884c\u4e86\u5168\u9762\u7684\u68c0\u67e5\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u6838\u5fc3\u6982\u5ff5\u3001\u6311\u6218\u548c\u65b9\u6cd5\u3002", "conclusion": "\u5206\u6790\u5f3a\u8c03\u4e86 CRL \u7684\u72ec\u7279\u6311\u6218\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u65b9\u5411\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89c1\u89e3\u3002"}}
{"id": "2506.21934", "categories": ["cs.IR", "cs.CV", "I.3.3; I.2.11; H.5.2"], "pdf": "https://arxiv.org/pdf/2506.21934", "abs": "https://arxiv.org/abs/2506.21934", "authors": ["Najmeh Forouzandehmehr", "Reza Yousefi Maragheh", "Sriram Kollipara", "Kai Zhao", "Topojoy Biswas", "Evren Korpeoglu", "Kannan Achan"], "title": "CAL-RAG: Retrieval-Augmented Multi-Agent Generation for Content-Aware Layout Design", "comment": null, "summary": "Automated content-aware layout generation -- the task of arranging visual\nelements such as text, logos, and underlays on a background canvas -- remains a\nfundamental yet under-explored problem in intelligent design systems. While\nrecent advances in deep generative models and large language models (LLMs) have\nshown promise in structured content generation, most existing approaches lack\ngrounding in contextual design exemplars and fall short in handling semantic\nalignment and visual coherence. In this work we introduce CAL-RAG, a\nretrieval-augmented, agentic framework for content-aware layout generation that\nintegrates multimodal retrieval, large language models, and collaborative\nagentic reasoning. Our system retrieves relevant layout examples from a\nstructured knowledge base and invokes an LLM-based layout recommender to\npropose structured element placements. A vision-language grader agent evaluates\nthe layout with visual metrics, and a feedback agent provides targeted\nrefinements, enabling iterative improvement. We implement our framework using\nLangGraph and evaluate it on the PKU PosterLayout dataset, a benchmark rich in\nsemantic and structural variability. CAL-RAG achieves state-of-the-art\nperformance across multiple layout metrics -- including underlay effectiveness,\nelement alignment, and overlap -- substantially outperforming strong baselines\nsuch as LayoutPrompter. These results demonstrate that combining retrieval\naugmentation with agentic multi-step reasoning yields a scalable,\ninterpretable, and high-fidelity solution for automated layout generation.", "AI": {"tldr": "Introduces CAL-RAG, a retrieval-augmented agentic framework for content-aware layout generation, achieving state-of-the-art performance.", "motivation": "most existing approaches lack grounding in contextual design exemplars and fall short in handling semantic alignment and visual coherence.", "method": "a retrieval-augmented, agentic framework for content-aware layout generation that integrates multimodal retrieval, large language models, and collaborative agentic reasoning", "result": "CAL-RAG achieves state-of-the-art performance across multiple layout metrics -- including underlay effectiveness, element alignment, and overlap -- substantially outperforming strong baselines such as LayoutPrompter.", "conclusion": "combining retrieval augmentation with agentic multi-step reasoning yields a scalable, interpretable, and high-fidelity solution for automated layout generation."}}
{"id": "2506.21839", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21839", "abs": "https://arxiv.org/abs/2506.21839", "authors": ["Mengyi Shan", "Brian Curless", "Ira Kemelmacher-Shlizerman", "Steve Seitz"], "title": "GenEscape: Hierarchical Multi-Agent Generation of Escape Room Puzzles", "comment": null, "summary": "We challenge text-to-image models with generating escape room puzzle images\nthat are visually appealing, logically solid, and intellectually stimulating.\nWhile base image models struggle with spatial relationships and affordance\nreasoning, we propose a hierarchical multi-agent framework that decomposes this\ntask into structured stages: functional design, symbolic scene graph reasoning,\nlayout synthesis, and local image editing. Specialized agents collaborate\nthrough iterative feedback to ensure the scene is visually coherent and\nfunctionally solvable. Experiments show that agent collaboration improves\noutput quality in terms of solvability, shortcut avoidance, and affordance\nclarity, while maintaining visual quality.", "AI": {"tldr": "This paper introduces a hierarchical multi-agent framework to improve the generation of escape room puzzle images by addressing the limitations of base image models in spatial relationships and affordance reasoning.", "motivation": "challenging text-to-image models with generating escape room puzzle images that are visually appealing, logically solid, and intellectually stimulating. While base image models struggle with spatial relationships and affordance reasoning", "method": "a hierarchical multi-agent framework that decomposes this task into structured stages: functional design, symbolic scene graph reasoning, layout synthesis, and local image editing. Specialized agents collaborate through iterative feedback to ensure the scene is visually coherent and functionally solvable.", "result": "agent collaboration improves output quality in terms of solvability, shortcut avoidance, and affordance clarity, while maintaining visual quality.", "conclusion": "Specialized agents collaborate through iterative feedback to ensure the scene is visually coherent and functionally solvable, improving output quality in terms of solvability, shortcut avoidance, and affordance clarity, while maintaining visual quality."}}
{"id": "2506.21568", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2506.21568", "abs": "https://arxiv.org/abs/2506.21568", "authors": ["Andrejs Sorstkins"], "title": "Assessing RAG and HyDE on 1B vs. 4B-Parameter Gemma LLMs for Personal Assistants Integretion", "comment": "Technical report as part of research project", "summary": "Resource efficiency is a critical barrier to deploying large language models\n(LLMs) in edge and privacy-sensitive applications. This study evaluates the\nefficacy of two augmentation strategies--Retrieval-Augmented Generation (RAG)\nand Hypothetical Document Embeddings (HyDE)--on compact Gemma LLMs of 1 billion\nand 4 billion parameters, within the context of a privacy-first personal\nassistant. We implement short-term memory via MongoDB and long-term semantic\nstorage via Qdrant, orchestrated through FastAPI and LangChain, and expose the\nsystem through a React.js frontend. Across both model scales, RAG consistently\nreduces latency by up to 17\\% and eliminates factual hallucinations when\nresponding to user-specific and domain-specific queries. HyDE, by contrast,\nenhances semantic relevance--particularly for complex physics prompts--but\nincurs a 25--40\\% increase in response time and a non-negligible hallucination\nrate in personal-data retrieval. Comparing 1 B to 4 B models, we observe that\nscaling yields marginal throughput gains for baseline and RAG pipelines, but\nmagnifies HyDE's computational overhead and variability. Our findings position\nRAG as the pragmatic choice for on-device personal assistants powered by\nsmall-scale LLMs.", "AI": {"tldr": "This study evaluates RAG and HyDE on compact Gemma LLMs for a privacy-first personal assistant. RAG reduces latency and hallucinations, making it suitable for on-device use.", "motivation": "Resource efficiency is a critical barrier to deploying large language models in edge and privacy-sensitive applications.", "method": "We implement short-term memory via MongoDB and long-term semantic storage via Qdrant, orchestrated through FastAPI and LangChain, and expose the system through a React.js frontend.", "result": "RAG consistently reduces latency by up to 17% and eliminates factual hallucinations. HyDE enhances semantic relevance but increases response time and hallucination rate. Scaling models yields marginal throughput gains for baseline and RAG pipelines, but magnifies HyDE's computational overhead and variability.", "conclusion": "RAG is the pragmatic choice for on-device personal assistants powered by small-scale LLMs."}}
{"id": "2506.22309", "categories": ["cs.AI", "cs.CL", "cs.DM", "cs.LG", "06B99", "I.2.4; I.2.7"], "pdf": "https://arxiv.org/pdf/2506.22309", "abs": "https://arxiv.org/abs/2506.22309", "authors": ["Klara M. Gutekunst", "Dominik D\u00fcrrschnabel", "Johannes Hirth", "Gerd Stumme"], "title": "Conceptual Topic Aggregation", "comment": "16 pages, 4 tables, 11 figures, International Joint Conference on\n  Conceptual Knowledge Structures", "summary": "The vast growth of data has rendered traditional manual inspection\ninfeasible, necessitating the adoption of computational methods for efficient\ndata exploration. Topic modeling has emerged as a powerful tool for analyzing\nlarge-scale textual datasets, enabling the extraction of latent semantic\nstructures. However, existing methods for topic modeling often struggle to\nprovide interpretable representations that facilitate deeper insights into data\nstructure and content. In this paper, we propose FAT-CAT, an approach based on\nFormal Concept Analysis (FCA) to enhance meaningful topic aggregation and\nvisualization of discovered topics. Our approach can handle diverse topics and\nfile types -- grouped by directories -- to construct a concept lattice that\noffers a structured, hierarchical representation of their topic distribution.\nIn a case study on the ETYNTKE dataset, we evaluate the effectiveness of our\napproach against other representation methods to demonstrate that FCA-based\naggregation provides more meaningful and interpretable insights into dataset\ncomposition than existing topic modeling techniques.", "AI": {"tldr": "Proposes FAT-CAT, an FCA-based approach, to enhance topic aggregation and visualization, providing more interpretable insights compared to existing methods.", "motivation": "Traditional manual inspection is infeasible, necessitating computational methods. Existing topic modeling methods struggle to provide interpretable representations.", "method": "FAT-CAT, an approach based on Formal Concept Analysis (FCA) to enhance meaningful topic aggregation and visualization of discovered topics.", "result": "FCA-based aggregation provides more meaningful and interpretable insights into dataset composition than existing topic modeling techniques in a case study on the ETYNTKE dataset.", "conclusion": "FCA-based aggregation provides more meaningful and interpretable insights into dataset composition than existing topic modeling techniques."}}
{"id": "2506.21899", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21899", "abs": "https://arxiv.org/abs/2506.21899", "authors": ["Amara Zuffer", "Michael Burke", "Mehrtash Harandi"], "title": "Advancements and Challenges in Continual Reinforcement Learning: A Comprehensive Review", "comment": "65 pages, 9 figures", "summary": "The diversity of tasks and dynamic nature of reinforcement learning (RL)\nrequire RL agents to be able to learn sequentially and continuously, a learning\nparadigm known as continuous reinforcement learning. This survey reviews how\ncontinual learning transforms RL agents into dynamic continual learners. This\nenables RL agents to acquire and retain useful and reusable knowledge\nseamlessly. The paper delves into fundamental aspects of continual\nreinforcement learning, exploring key concepts, significant challenges, and\nnovel methodologies. Special emphasis is placed on recent advancements in\ncontinual reinforcement learning within robotics, along with a succinct\noverview of evaluation environments utilized in prominent research,\nfacilitating accessibility for newcomers to the field. The review concludes\nwith a discussion on limitations and promising future directions, providing\nvaluable insights for researchers and practitioners alike.", "AI": {"tldr": "This paper reviews continual reinforcement learning, focusing on how it enables RL agents to learn continuously. It covers concepts, challenges, methodologies, advancements in robotics, and future directions.", "motivation": "The diversity of tasks and dynamic nature of reinforcement learning (RL) require RL agents to be able to learn sequentially and continuously, a learning paradigm known as continuous reinforcement learning. This enables RL agents to acquire and retain useful and reusable knowledge seamlessly.", "method": "This survey reviews how continual learning transforms RL agents into dynamic continual learners.", "result": "Special emphasis is placed on recent advancements in continual reinforcement learning within robotics, along with a succinct overview of evaluation environments utilized in prominent research, facilitating accessibility for newcomers to the field.", "conclusion": "This review concludes with a discussion on limitations and promising future directions, providing valuable insights for researchers and practitioners alike."}}
{"id": "2506.22026", "categories": ["cs.IR", "cs.AI", "I.2; H.3"], "pdf": "https://arxiv.org/pdf/2506.22026", "abs": "https://arxiv.org/abs/2506.22026", "authors": ["Simra Shahid", "Marissa Radensky", "Raymond Fok", "Pao Siangliulue", "Daniel S. Weld", "Tom Hope"], "title": "Literature-Grounded Novelty Assessment of Scientific Ideas", "comment": null, "summary": "Automated scientific idea generation systems have made remarkable progress,\nyet the automatic evaluation of idea novelty remains a critical and\nunderexplored challenge. Manual evaluation of novelty through literature review\nis labor-intensive, prone to error due to subjectivity, and impractical at\nscale. To address these issues, we propose the Idea Novelty Checker, an\nLLM-based retrieval-augmented generation (RAG) framework that leverages a\ntwo-stage retrieve-then-rerank approach. The Idea Novelty Checker first\ncollects a broad set of relevant papers using keyword and snippet-based\nretrieval, then refines this collection through embedding-based filtering\nfollowed by facet-based LLM re-ranking. It incorporates expert-labeled examples\nto guide the system in comparing papers for novelty evaluation and in\ngenerating literature-grounded reasoning. Our extensive experiments demonstrate\nthat our novelty checker achieves approximately 13% higher agreement than\nexisting approaches. Ablation studies further showcases the importance of the\nfacet-based re-ranker in identifying the most relevant literature for novelty\nevaluation.", "AI": {"tldr": "Idea Novelty Checker, an LLM-based RAG framework, improves automated novelty evaluation by using a two-stage retrieve-then-rerank approach.", "motivation": "Manual evaluation of novelty is labor-intensive, subjective, and impractical at scale.", "method": "LLM-based retrieval-augmented generation (RAG) framework with a two-stage retrieve-then-rerank approach, incorporating expert-labeled examples.", "result": "The Idea Novelty Checker achieves approximately 13% higher agreement than existing approaches. Ablation studies showcase the importance of the facet-based re-ranker.", "conclusion": "Idea Novelty Checker achieves approximately 13% higher agreement than existing approaches in novelty evaluation. The facet-based re-ranker is important for identifying the most relevant literature."}}
{"id": "2506.21843", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21843", "abs": "https://arxiv.org/abs/2506.21843", "authors": ["Yuxiang Ge", "Jionghao Cheng", "Ruiquan Ge", "Zhaojie Fang", "Gangyong Jia", "Xiang Wan", "Nannan Li", "Ahmed Elazab", "Changmiao Wang"], "title": "3D-Telepathy: Reconstructing 3D Objects from EEG Signals", "comment": null, "summary": "Reconstructing 3D visual stimuli from Electroencephalography (EEG) data holds\nsignificant potential for applications in Brain-Computer Interfaces (BCIs) and\naiding individuals with communication disorders. Traditionally, efforts have\nfocused on converting brain activity into 2D images, neglecting the translation\nof EEG data into 3D objects. This limitation is noteworthy, as the human brain\ninherently processes three-dimensional spatial information regardless of\nwhether observing 2D images or the real world. The neural activities captured\nby EEG contain rich spatial information that is inevitably lost when\nreconstructing only 2D images, thus limiting its practical applications in BCI.\nThe transition from EEG data to 3D object reconstruction faces considerable\nobstacles. These include the presence of extensive noise within EEG signals and\na scarcity of datasets that include both EEG and 3D information, which\ncomplicates the extraction process of 3D visual data. Addressing this\nchallenging task, we propose an innovative EEG encoder architecture that\nintegrates a dual self-attention mechanism. We use a hybrid training strategy\nto train the EEG Encoder, which includes cross-attention, contrastive learning,\nand self-supervised learning techniques. Additionally, by employing stable\ndiffusion as a prior distribution and utilizing Variational Score Distillation\nto train a neural radiation field, we successfully generate 3D objects with\nsimilar content and structure from EEG data.", "AI": {"tldr": "This paper introduces a method to reconstruct 3D objects from EEG data using a novel EEG encoder architecture and Variational Score Distillation, addressing the limitations of 2D image reconstruction in Brain-Computer Interfaces.", "motivation": "Reconstructing 3D visual stimuli from EEG data holds significant potential for Brain-Computer Interfaces (BCIs) and aiding individuals with communication disorders. The neural activities captured by EEG contain rich spatial information that is inevitably lost when reconstructing only 2D images, thus limiting its practical applications in BCI.", "method": "An innovative EEG encoder architecture that integrates a dual self-attention mechanism and a hybrid training strategy including cross-attention, contrastive learning, and self-supervised learning techniques.", "result": "The study successfully generates 3D objects with similar content and structure from EEG data.", "conclusion": "Utilizing Variational Score Distillation to train a neural radiation field, the study successfully generates 3D objects with similar content and structure from EEG data."}}
{"id": "2506.21569", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21569", "abs": "https://arxiv.org/abs/2506.21569", "authors": ["Weihua Xiao", "Derek Ekberg", "Siddharth Garg", "Ramesh Karri"], "title": "Hybrid-NL2SVA: Integrating RAG and Finetuning for LLM-based NL2SVA", "comment": null, "summary": "SystemVerilog Assertions (SVAs) are critical for verifying the correctness of\nhardware designs, but manually writing them from natural language property\ndescriptions, i.e., NL2SVA, remains a labor-intensive and error-prone task.\nRecent advances in large language models (LLMs) offer opportunities to automate\nthis translation. However, existing models still struggle with understanding\ndomain-specific syntax and semantics. To enhance LLM performance in NL2SVA, we\npropose a customized retrieval-augmented generation (RAG) framework and a\nsynthetic fine-tuning dataset that together improve LLM's performance. To\nfurther improve lightweight models over NL2SVA, our fine-tuning dataset\nprovides prompt-guided explanations that teach LLMs the layer-by-layer\nconstruction process of concurrent SVAs, enabling supervised fine-tuning that\ngreatly improves syntax and functionality accuracy. To evaluate the performance\nof LLMs over NL2SVA, we construct the largest evaluation dataset for NL2SVA,\ncomprising 40 Verilog designs and 229 formally verified SVAs with detailed\nannotations. Experimental results show that our customized RAG framework\nincreases the number of functionality matched SVAs by 58.42% over GPT-4o-mini,\nwhile Qwen2.5-Coder-7B-Instruct fine-tuned on our fine-tuning dataset and\nintegrated with HybridRetrieval achieves a 59.05% over the base Qwen model.", "AI": {"tldr": "This paper introduces a customized RAG framework and a synthetic fine-tuning dataset to improve LLM performance in translating natural language to SystemVerilog Assertions (SVAs).", "motivation": "Manually writing SystemVerilog Assertions (SVAs) from natural language property descriptions is labor-intensive and error-prone. Existing models struggle with understanding domain-specific syntax and semantics.", "method": "A customized retrieval-augmented generation (RAG) framework and a synthetic fine-tuning dataset. Prompt-guided explanations teach LLMs the layer-by-layer construction process of concurrent SVAs, enabling supervised fine-tuning.", "result": "The customized RAG framework increases the number of functionality matched SVAs by 58.42% over GPT-4o-mini, while Qwen2.5-Coder-7B-Instruct fine-tuned on the dataset and integrated with HybridRetrieval achieves a 59.05% improvement over the base Qwen model.", "conclusion": "A customized RAG framework and a synthetic fine-tuning dataset improve LLM's performance in NL2SVA. Fine-tuning on the dataset and integrated with HybridRetrieval achieves a 59.05% improvement over the base Qwen model."}}
{"id": "2506.22355", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22355", "abs": "https://arxiv.org/abs/2506.22355", "authors": ["Pascale Fung", "Yoram Bachrach", "Asli Celikyilmaz", "Kamalika Chaudhuri", "Delong Chen", "Willy Chung", "Emmanuel Dupoux", "Herv\u00e9 J\u00e9gou", "Alessandro Lazaric", "Arjun Majumdar", "Andrea Madotto", "Franziska Meier", "Florian Metze", "Th\u00e9o Moutakanni", "Juan Pino", "Basile Terver", "Joseph Tighe", "Jitendra Malik"], "title": "Embodied AI Agents: Modeling the World", "comment": null, "summary": "This paper describes our research on AI agents embodied in visual, virtual or\nphysical forms, enabling them to interact with both users and their\nenvironments. These agents, which include virtual avatars, wearable devices,\nand robots, are designed to perceive, learn and act within their surroundings,\nwhich makes them more similar to how humans learn and interact with the\nenvironments as compared to disembodied agents. We propose that the development\nof world models is central to reasoning and planning of embodied AI agents,\nallowing these agents to understand and predict their environment, to\nunderstand user intentions and social contexts, thereby enhancing their ability\nto perform complex tasks autonomously. World modeling encompasses the\nintegration of multimodal perception, planning through reasoning for action and\ncontrol, and memory to create a comprehensive understanding of the physical\nworld. Beyond the physical world, we also propose to learn the mental world\nmodel of users to enable better human-agent collaboration.", "AI": {"tldr": "This paper proposes the development of world models is central to reasoning and planning of embodied AI agents, allowing these agents to understand and predict their environment, to understand user intentions and social contexts, thereby enhancing their ability to perform complex tasks autonomously.", "motivation": "AI agents embodied in visual, virtual or physical forms can interact with both users and their environments. These agents are designed to perceive, learn and act within their surroundings, which makes them more similar to how humans learn and interact with the environments as compared to disembodied agents.", "method": "Integration of multimodal perception, planning through reasoning for action and control, and memory to create a comprehensive understanding of the physical world. Learning the mental world model of users to enable better human-agent collaboration.", "result": "AI agents can understand and predict their environment, understand user intentions and social contexts, and perform complex tasks autonomously.", "conclusion": "Development of world models is central to reasoning and planning of embodied AI agents, allowing these agents to understand and predict their environment, to understand user intentions and social contexts, thereby enhancing their ability to perform complex tasks autonomously."}}
{"id": "2506.21900", "categories": ["cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.21900", "abs": "https://arxiv.org/abs/2506.21900", "authors": ["Sheng Yun", "Jianhua Pei", "Ping Wang"], "title": "TOAST: Task-Oriented Adaptive Semantic Transmission over Dynamic Wireless Environments", "comment": null, "summary": "The evolution toward 6G networks demands a fundamental shift from bit-centric\ntransmission to semantic-aware communication that emphasizes task-relevant\ninformation. This work introduces TOAST (Task-Oriented Adaptive Semantic\nTransmission), a unified framework designed to address the core challenge of\nmulti-task optimization in dynamic wireless environments through three\ncomplementary components. First, we formulate adaptive task balancing as a\nMarkov decision process, employing deep reinforcement learning to dynamically\nadjust the trade-off between image reconstruction fidelity and semantic\nclassification accuracy based on real-time channel conditions. Second, we\nintegrate module-specific Low-Rank Adaptation (LoRA) mechanisms throughout our\nSwin Transformer-based joint source-channel coding architecture, enabling\nparameter-efficient fine-tuning that dramatically reduces adaptation overhead\nwhile maintaining full performance across diverse channel impairments including\nAdditive White Gaussian Noise (AWGN), fading, phase noise, and impulse\ninterference. Third, we incorporate an Elucidating diffusion model that\noperates in the latent space to restore features corrupted by channel noises,\nproviding substantial quality improvements compared to baseline approaches.\nExtensive experiments across multiple datasets demonstrate that TOAST achieves\nsuperior performance compared to baseline approaches, with significant\nimprovements in both classification accuracy and reconstruction quality at low\nSignal-to-Noise Ratio (SNR) conditions while maintaining robust performance\nacross all tested scenarios.", "AI": {"tldr": "TOAST\u662f\u4e00\u79cd\u7528\u4e8e\u591a\u4efb\u52a1\u4f18\u5316\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u81ea\u9002\u5e94\u4efb\u52a1\u5e73\u8861\u3001\u4f4e\u79e9\u81ea\u9002\u5e94\u548c\u6269\u6563\u6a21\u578b\u6765\u5b9e\u73b0\u9ad8\u6027\u80fd\u3002", "motivation": "6G\u7f51\u7edc\u9700\u8981\u4ece\u4ee5\u6bd4\u7279\u4e3a\u4e2d\u5fc3\u7684\u4f20\u8f93\u8f6c\u53d8\u4e3a\u5f3a\u8c03\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\u7684\u8bed\u4e49\u611f\u77e5\u901a\u4fe1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6846\u67b6TOAST\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u4e09\u4e2a\u4e92\u8865\u7ec4\u4ef6\u6765\u89e3\u51b3\u52a8\u6001\u65e0\u7ebf\u73af\u5883\u4e2d\u7684\u591a\u4efb\u52a1\u4f18\u5316\u8fd9\u4e00\u6838\u5fc3\u6311\u6218\u3002\u9996\u5148\uff0c\u6211\u4eec\u5c06\u81ea\u9002\u5e94\u4efb\u52a1\u5e73\u8861\u516c\u5f0f\u5316\u4e3a\u4e00\u4e2a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u91c7\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6765\u52a8\u6001\u8c03\u6574\u56fe\u50cf\u91cd\u5efa\u4fdd\u771f\u5ea6\u548c\u8bed\u4e49\u5206\u7c7b\u7cbe\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u3002\u5176\u6b21\uff0c\u6211\u4eec\u5728\u57fa\u4e8eSwin Transformer\u7684\u8054\u5408\u6e90\u4fe1\u9053\u7f16\u7801\u67b6\u6784\u4e2d\u96c6\u6210\u4e86\u7279\u5b9a\u6a21\u5757\u7684\u4f4e\u79e9\u81ea\u9002\u5e94(LoRA)\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u53c2\u6570\u9ad8\u6548\u7684\u5fae\u8c03\u3002\u7b2c\u4e09\uff0c\u6211\u4eec\u96c6\u6210\u4e86\u4e00\u4e2a\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fd0\u884c\u7684Elucidating\u6269\u6563\u6a21\u578b\uff0c\u4ee5\u6062\u590d\u88ab\u4fe1\u9053\u566a\u58f0\u7834\u574f\u7684\u7279\u5f81\u3002", "result": "TOAST\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0cTOAST\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002", "conclusion": "TOAST\u5728\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\uff0c\u5728\u5206\u7c7b\u7cbe\u5ea6\u548c\u91cd\u5efa\u8d28\u91cf\u65b9\u9762\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5728\u6240\u6709\u6d4b\u8bd5\u573a\u666f\u4e2d\u4fdd\u6301\u4e86\u7a33\u5065\u7684\u6027\u80fd\u3002"}}
{"id": "2506.22112", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2506.22112", "abs": "https://arxiv.org/abs/2506.22112", "authors": ["Wenzheng Shu", "Yanxiang Zeng", "Yongxiang Tang", "Teng Sha", "Ning Luo", "Yanhua Cheng", "Xialong Liu", "Fan Zhou", "Peng Jiang"], "title": "Reward Balancing Revisited: Enhancing Offline Reinforcement Learning for Recommender Systems", "comment": "Accepted in Companion Proceedings of the ACM Web Conference 2025", "summary": "Offline reinforcement learning (RL) has emerged as a prevalent and effective\nmethodology for real-world recommender systems, enabling learning policies from\nhistorical data and capturing user preferences. In offline RL, reward shaping\nencounters significant challenges, with past efforts to incorporate prior\nstrategies for uncertainty to improve world models or penalize underexplored\nstate-action pairs. Despite these efforts, a critical gap remains: the\nsimultaneous balancing of intrinsic biases in world models and the diversity of\npolicy recommendations. To address this limitation, we present an innovative\noffline RL framework termed Reallocated Reward for Recommender Systems (R3S).\nBy integrating inherent model uncertainty to tackle the intrinsic fluctuations\nin reward predictions, we boost diversity for decision-making to align with a\nmore interactive paradigm, incorporating extra penalizers with decay that deter\nactions leading to diminished state variety at both local and global scales.\nThe experimental results demonstrate that R3S improves the accuracy of world\nmodels and efficiently harmonizes the heterogeneous preferences of the users.", "AI": {"tldr": "R3S is an offline RL framework that balances intrinsic biases in world models and the diversity of policy recommendations.", "motivation": "the simultaneous balancing of intrinsic biases in world models and the diversity of policy recommendations.", "method": "innovative offline RL framework termed Reallocated Reward for Recommender Systems (R3S).", "result": "integrating inherent model uncertainty to tackle the intrinsic fluctuations in reward predictions, we boost diversity for decision-making to align with a more interactive paradigm, incorporating extra penalizers with decay that deter actions leading to diminished state variety at both local and global scales.", "conclusion": "R3S improves the accuracy of world models and efficiently harmonizes the heterogeneous preferences of the users."}}
{"id": "2506.21851", "categories": ["cs.CV", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.21851", "abs": "https://arxiv.org/abs/2506.21851", "authors": ["Haofeng Wang", "Fangtao Zhou", "Qi Zhang", "Zeyuan Chen", "Enci Zhang", "Zhao Wang", "Xiaofeng Huang", "Siwei Ma"], "title": "End-to-End RGB-IR Joint Image Compression With Channel-wise Cross-modality Entropy Model", "comment": "IEEE International Conference on Systems, Man, and Cybernetics 2025.\n  (SMC), under review", "summary": "RGB-IR(RGB-Infrared) image pairs are frequently applied simultaneously in\nvarious applications like intelligent surveillance. However, as the number of\nmodalities increases, the required data storage and transmission costs also\ndouble. Therefore, efficient RGB-IR data compression is essential. This work\nproposes a joint compression framework for RGB-IR image pair. Specifically, to\nfully utilize cross-modality prior information for accurate context probability\nmodeling within and between modalities, we propose a Channel-wise\nCross-modality Entropy Model (CCEM). Among CCEM, a Low-frequency Context\nExtraction Block (LCEB) and a Low-frequency Context Fusion Block (LCFB) are\ndesigned for extracting and aggregating the global low-frequency information\nfrom both modalities, which assist the model in predicting entropy parameters\nmore accurately. Experimental results demonstrate that our approach outperforms\nexisting RGB-IR image pair and single-modality compression methods on LLVIP and\nKAIST datasets. For instance, the proposed framework achieves a 23.1% bit rate\nsaving on LLVIP dataset compared to the state-of-the-art RGB-IR image codec\npresented at CVPR 2022.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e RGB-IR \u56fe\u50cf\u5bf9\u7684\u8054\u5408\u538b\u7f29\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f18\u4e8e\u73b0\u6709\u7684\u538b\u7f29\u65b9\u6cd5\u3002", "motivation": "RGB-\u7ea2\u5916 (RGB-Infrared) \u56fe\u50cf\u5bf9\u7ecf\u5e38\u540c\u65f6\u5e94\u7528\u4e8e\u667a\u80fd\u76d1\u63a7\u7b49\u5404\u79cd\u5e94\u7528\u4e2d\u3002\u7136\u800c\uff0c\u968f\u7740\u6a21\u6001\u6570\u91cf\u7684\u589e\u52a0\uff0c\u6240\u9700\u7684\u6570\u636e\u5b58\u50a8\u548c\u4f20\u8f93\u6210\u672c\u4e5f\u968f\u4e4b\u589e\u52a0\u4e00\u500d\u3002\u56e0\u6b64\uff0c\u9ad8\u6548\u7684 RGB-IR \u6570\u636e\u538b\u7f29\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e RGB-IR \u56fe\u50cf\u5bf9\u7684\u8054\u5408\u538b\u7f29\u6846\u67b6\uff0c\u5177\u4f53\u6765\u8bf4\uff0c\u4e3a\u4e86\u5145\u5206\u5229\u7528\u8de8\u6a21\u6001\u5148\u9a8c\u4fe1\u606f\uff0c\u5728\u6a21\u6001\u5185\u90e8\u548c\u4e4b\u95f4\u8fdb\u884c\u7cbe\u786e\u7684\u4e0a\u4e0b\u6587\u6982\u7387\u5efa\u6a21\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u9053\u5f0f\u8de8\u6a21\u6001\u71b5\u6a21\u578b (CCEM)\u3002\u5728 CCEM \u4e2d\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4f4e\u9891\u4e0a\u4e0b\u6587\u63d0\u53d6\u5757 (LCEB) \u548c\u4e00\u4e2a\u4f4e\u9891\u4e0a\u4e0b\u6587\u878d\u5408\u5757 (LCFB)\uff0c\u7528\u4e8e\u63d0\u53d6\u548c\u805a\u5408\u6765\u81ea\u4e24\u79cd\u6a21\u6001\u7684\u5168\u5c40\u4f4e\u9891\u4fe1\u606f\uff0c\u8fd9\u6709\u52a9\u4e8e\u6a21\u578b\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u71b5\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728 LLVIP \u548c KAIST \u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684 RGB-IR \u56fe\u50cf\u5bf9\u548c\u5355\u6a21\u6001\u538b\u7f29\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728 LLVIP \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86 23.1% \u7684\u6bd4\u7279\u7387\u8282\u7701\uff0c\u4f18\u4e8e CVPR 2022 \u4e0a\u63d0\u51fa\u7684\u6700\u5148\u8fdb\u7684 RGB-IR \u56fe\u50cf\u7f16\u89e3\u7801\u5668\u3002"}}
{"id": "2506.21570", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21570", "abs": "https://arxiv.org/abs/2506.21570", "authors": ["Roland Riachi", "Kashif Rasul", "Arjun Ashok", "Prateek Humane", "Alexis Roger", "Andrew R. Williams", "Yuriy Nevmyvaka", "Irina Rish"], "title": "Random Initialization Can't Catch Up: The Advantage of Language Model Transfer for Time Series Forecasting", "comment": null, "summary": "Recent works have demonstrated the effectiveness of adapting pre-trained\nlanguage models (LMs) for forecasting time series in the low-data regime. We\nbuild upon these findings by analyzing the effective transfer from language\nmodels to time series forecasting under various design choices including\nupstream post-training, time series tokenizer and language backbone size. In\nthe low-data regime, these design choices have a significant impact on the\nvalidation loss, with clear-cut choices that outperform others. Contrary to\nHernandez et al. (2021), we observe that the validation loss of the LMs\ncontinues to smoothly decrease long after the validation loss of the randomly\ninitialized models has converged, leading to a non-vanishing transfer gap that\nholds across design choices. These findings not only help shed light on the\neffective use of compute-efficient training for time series, but also open the\nway for the study of modality-agnostic properties of data distributions\nleveraged by these models.", "AI": {"tldr": "\u5206\u6790\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u8bed\u8a00\u6a21\u578b\u7684\u6709\u6548\u8fc1\u79fb\uff0c\u53d1\u73b0\u5728\u4f4e\u6570\u636e\u60c5\u51b5\u4e0b\uff0c\u7279\u5b9a\u8bbe\u8ba1\u9009\u62e9\u548c\u6301\u7eed\u5b58\u5728\u7684\u8fc1\u79fb\u5dee\u8ddd\u4f1a\u663e\u8457\u5f71\u54cd\u6027\u80fd\u3002", "motivation": "\u5148\u524d\u7684\u5de5\u4f5c\u5df2\u7ecf\u8bc1\u660e\u4e86\u8c03\u6574\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08LM\uff09\u5728\u4f4e\u6570\u636e\u60c5\u51b5\u4e0b\u9884\u6d4b\u65f6\u95f4\u5e8f\u5217\u7684\u6709\u6548\u6027\u3002", "method": "\u5206\u6790\u4e86\u5728\u5404\u79cd\u8bbe\u8ba1\u9009\u62e9\u4e0b\uff0c\u4ece\u8bed\u8a00\u6a21\u578b\u5230\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6709\u6548\u8fc1\u79fb\uff0c\u5305\u62ec\u4e0a\u6e38\u540e\u8bad\u7ec3\u3001\u65f6\u95f4\u5e8f\u5217\u5206\u8bcd\u5668\u548c\u8bed\u8a00\u9aa8\u5e72\u5927\u5c0f\u3002", "result": "\u8fd9\u4e9b\u8bbe\u8ba1\u9009\u62e9\u5bf9\u9a8c\u8bc1\u635f\u5931\u6709\u663e\u8457\u5f71\u54cd\uff0c\u660e\u786e\u7684\u9009\u62e9\u4f18\u4e8e\u5176\u4ed6\u9009\u62e9\u3002\u4e0e Hernandez \u7b49\u4eba (2021) \u7684\u7814\u7a76\u76f8\u53cd\uff0c\u6211\u4eec\u89c2\u5bdf\u5230 LMs \u7684\u9a8c\u8bc1\u635f\u5931\u5728\u968f\u673a\u521d\u59cb\u5316\u6a21\u578b\u7684\u9a8c\u8bc1\u635f\u5931\u6536\u655b\u540e\u7ee7\u7eed\u5e73\u7a33\u4e0b\u964d\uff0c\u5bfc\u81f4\u4e86\u5728\u5404\u79cd\u8bbe\u8ba1\u9009\u62e9\u4e2d\u90fd\u5b58\u5728\u7684\u975e\u6d88\u5931\u8fc1\u79fb\u5dee\u8ddd\u3002", "conclusion": "\u5728\u4f4e\u6570\u636e\u60c5\u51b5\u4e0b\uff0c\u8bed\u8a00\u6a21\u578b\u7684\u9a8c\u8bc1\u635f\u5931\u5728\u968f\u673a\u521d\u59cb\u5316\u6a21\u578b\u6536\u655b\u540e\u6301\u7eed\u5e73\u7a33\u4e0b\u964d\uff0c\u5bfc\u81f4\u6301\u7eed\u5b58\u5728\u7684\u8fc1\u79fb\u5dee\u8ddd\u3002"}}
{"id": "2506.22358", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22358", "abs": "https://arxiv.org/abs/2506.22358", "authors": ["Varvara Kalokyri", "Nikolaos S. Tachos", "Charalampos N. Kalantzopoulos", "Stelios Sfakianakis", "Haridimos Kondylakis", "Dimitrios I. Zaridis", "Sara Colantonio", "Daniele Regge", "Nikolaos Papanikolaou", "The ProCAncer-I consortium", "Konstantinos Marias", "Dimitrios I. Fotiadis", "Manolis Tsiknakis"], "title": "AI Model Passport: Data and System Traceability Framework for Transparent AI in Health", "comment": null, "summary": "The increasing integration of Artificial Intelligence (AI) into health and\nbiomedical systems necessitates robust frameworks for transparency,\naccountability, and ethical compliance. Existing frameworks often rely on\nhuman-readable, manual documentation which limits scalability, comparability,\nand machine interpretability across projects and platforms. They also fail to\nprovide a unique, verifiable identity for AI models to ensure their provenance\nand authenticity across systems and use cases, limiting reproducibility and\nstakeholder trust. This paper introduces the concept of the AI Model Passport,\na structured and standardized documentation framework that acts as a digital\nidentity and verification tool for AI models. It captures essential metadata to\nuniquely identify, verify, trace and monitor AI models across their lifecycle -\nfrom data acquisition and preprocessing to model design, development and\ndeployment. In addition, an implementation of this framework is presented\nthrough AIPassport, an MLOps tool developed within the ProCAncer-I EU project\nfor medical imaging applications. AIPassport automates metadata collection,\nensures proper versioning, decouples results from source scripts, and\nintegrates with various development environments. Its effectiveness is\nshowcased through a lesion segmentation use case using data from the\nProCAncer-I dataset, illustrating how the AI Model Passport enhances\ntransparency, reproducibility, and regulatory readiness while reducing manual\neffort. This approach aims to set a new standard for fostering trust and\naccountability in AI-driven healthcare solutions, aspiring to serve as the\nbasis for developing transparent and regulation compliant AI systems across\ndomains.", "AI": {"tldr": "The paper proposes the AI Model Passport as a digital identity for AI models to improve transparency and trust, and showcases its implementation in medical imaging.", "motivation": "Existing AI frameworks lack scalability, comparability, machine interpretability, and a unique, verifiable identity for AI models, limiting reproducibility and stakeholder trust.", "method": "The paper introduces the AI Model Passport and implements it through AIPassport, an MLOps tool. A lesion segmentation use case using data from the ProCAncer-I dataset is used to showcase its effectiveness.", "result": "The AI Model Passport enhances transparency, reproducibility, and regulatory readiness while reducing manual effort.", "conclusion": "This paper introduces the AI Model Passport, a structured documentation framework for AI models, and demonstrates its implementation through AIPassport, enhancing transparency, reproducibility, and regulatory readiness in AI-driven healthcare."}}
{"id": "2506.21937", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21937", "abs": "https://arxiv.org/abs/2506.21937", "authors": ["Marwan Ait Haddou", "Mohamed Bennai"], "title": "HQCM-EBTC: A Hybrid Quantum-Classical Model for Explainable Brain Tumor Classification", "comment": null, "summary": "We propose HQCM-EBTC, a hybrid quantum-classical model for automated brain\ntumor classification using MRI images. Trained on a dataset of 7,576 scans\ncovering normal, meningioma, glioma, and pituitary classes, HQCM-EBTC\nintegrates a 5-qubit, depth-2 quantum layer with 5 parallel circuits, optimized\nvia AdamW and a composite loss blending cross-entropy and attention\nconsistency.\n  HQCM-EBTC achieves 96.48% accuracy, substantially outperforming the classical\nbaseline (86.72%). It delivers higher precision and F1-scores, especially for\nglioma detection. t-SNE projections reveal enhanced feature separability in\nquantum space, and confusion matrices show lower misclassification. Attention\nmap analysis (Jaccard Index) confirms more accurate and focused tumor\nlocalization at high-confidence thresholds.\n  These results highlight the promise of quantum-enhanced models in medical\nimaging, advancing both diagnostic accuracy and interpretability for clinical\nbrain tumor assessment.", "AI": {"tldr": "HQCM-EBTC, a hybrid quantum-classical model, achieves 96.48% accuracy in brain tumor classification, outperforming classical methods.", "motivation": "Automated brain tumor classification using MRI images.", "method": "A hybrid quantum-classical model (HQCM-EBTC) integrating a 5-qubit, depth-2 quantum layer with 5 parallel circuits, optimized via AdamW and a composite loss blending cross-entropy and attention consistency.", "result": "HQCM-EBTC achieves 96.48% accuracy, outperforming the classical baseline (86.72%), with higher precision and F1-scores, especially for glioma detection. Enhanced feature separability and accurate tumor localization are also observed.", "conclusion": "Quantum-enhanced models show promise in medical imaging, improving diagnostic accuracy and interpretability for brain tumor assessment."}}
{"id": "2506.22210", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2506.22210", "abs": "https://arxiv.org/abs/2506.22210", "authors": ["Weronika \u0141ajewska", "Ivica Kostric", "Gabriel Iturra-Bocaz", "Mariam Arustashvili", "Krisztian Balog"], "title": "UiS-IAI@LiveRAG: Retrieval-Augmented Information Nugget-Based Generation of Responses", "comment": null, "summary": "Retrieval-augmented generation (RAG) faces challenges related to factual\ncorrectness, source attribution, and response completeness. The LiveRAG\nChallenge hosted at SIGIR'25 aims to advance RAG research using a fixed corpus\nand a shared, open-source LLM. We propose a modular pipeline that operates on\ninformation nuggets-minimal, atomic units of relevant information extracted\nfrom retrieved documents. This multistage pipeline encompasses query rewriting,\npassage retrieval and reranking, nugget detection and clustering, cluster\nranking and summarization, and response fluency enhancement. This design\ninherently promotes grounding in specific facts, facilitates source\nattribution, and ensures maximum information inclusion within length\nconstraints. In this challenge, we extend our focus to also address the\nretrieval component of RAG, building upon our prior work on multi-faceted query\nrewriting. Furthermore, for augmented generation, we concentrate on improving\ncontext curation capabilities, maximizing the breadth of information covered in\nthe response while ensuring pipeline efficiency. Our results show that\ncombining original queries with a few sub-query rewrites boosts recall, while\nincreasing the number of documents used for reranking and generation beyond a\ncertain point reduces effectiveness, without improving response quality.", "AI": {"tldr": "This paper proposes a modular pipeline using information nuggets to improve RAG's factual correctness, source attribution, and completeness. It focuses on query rewriting and context curation, finding that a few sub-query rewrites boost recall, but excessive document usage reduces effectiveness.", "motivation": "Retrieval-augmented generation (RAG) faces challenges related to factual correctness, source attribution, and response completeness.", "method": "modular pipeline that operates on information nuggets-minimal, atomic units of relevant information extracted from retrieved documents. This multistage pipeline encompasses query rewriting, passage retrieval and reranking, nugget detection and clustering, cluster ranking and summarization, and response fluency enhancement.", "result": "Combining original queries with a few sub-query rewrites boosts recall, while increasing the number of documents used for reranking and generation beyond a certain point reduces effectiveness, without improving response quality.", "conclusion": "Combining original queries with a few sub-query rewrites boosts recall, while increasing the number of documents used for reranking and generation beyond a certain point reduces effectiveness, without improving response quality."}}
{"id": "2506.21855", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21855", "abs": "https://arxiv.org/abs/2506.21855", "authors": ["Jiho Choi", "Sang Jun Lee"], "title": "Periodic-MAE: Periodic Video Masked Autoencoder for rPPG Estimation", "comment": null, "summary": "In this paper, we propose a method that learns a general representation of\nperiodic signals from unlabeled facial videos by capturing subtle changes in\nskin tone over time. The proposed framework employs the video masked\nautoencoder to learn a high-dimensional spatio-temporal representation of the\nfacial region through self-supervised learning. Capturing quasi-periodic\nsignals in the video is crucial for remote photoplethysmography (rPPG)\nestimation. To account for signal periodicity, we apply frame masking in terms\nof video sampling, which allows the model to capture resampled quasi-periodic\nsignals during the pre-training stage. Moreover, the framework incorporates\nphysiological bandlimit constraints, leveraging the property that physiological\nsignals are sparse within their frequency bandwidth to provide pulse cues to\nthe model. The pre-trained encoder is then transferred to the rPPG task, where\nit is used to extract physiological signals from facial videos. We evaluate the\nproposed method through extensive experiments on the PURE, UBFC-rPPG, MMPD, and\nV4V datasets. Our results demonstrate significant performance improvements,\nparticularly in challenging cross-dataset evaluations. Our code is available at\nhttps://github.com/ziiho08/Periodic-MAE.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u9762\u90e8\u89c6\u9891\u4e2d\u63d0\u53d6\u751f\u7406\u4fe1\u53f7\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u8fdc\u7a0b\u5149\u7535\u5bb9\u79ef\u8109\u640f\u6ce2(rPPG)\u4f30\u8ba1\u7684\u5173\u952e\u5728\u4e8e\u6355\u83b7\u89c6\u9891\u4e2d\u7684\u51c6\u5468\u671f\u4fe1\u53f7\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6355\u83b7\u76ae\u80a4\u989c\u8272\u968f\u65f6\u95f4\u53d8\u5316\u7684\u7ec6\u5fae\u53d8\u5316\u6765\u5b66\u4e60\u65e0\u6807\u8bb0\u9762\u90e8\u89c6\u9891\u4e2d\u5468\u671f\u4fe1\u53f7\u7684\u901a\u7528\u8868\u793a\u7684\u65b9\u6cd5\u3002\u8be5\u6846\u67b6\u91c7\u7528\u89c6\u9891\u63a9\u7801\u81ea\u52a8\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u6765\u5b66\u4e60\u9762\u90e8\u533a\u57df\u7684\u9ad8\u7ef4\u65f6\u7a7a\u8868\u793a\u3002\u4e3a\u4e86\u8003\u8651\u4fe1\u53f7\u7684\u5468\u671f\u6027\uff0c\u8be5\u65b9\u6cd5\u5e94\u7528\u4e86\u89c6\u9891\u91c7\u6837\u65b9\u9762\u7684\u5e27\u63a9\u7801\uff0c\u8fd9\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u6355\u83b7\u91cd\u91c7\u6837\u7684\u51c6\u5468\u671f\u4fe1\u53f7\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u8fd8\u7ed3\u5408\u4e86\u751f\u7406\u5e26\u5bbd\u9650\u5236\uff0c\u5229\u7528\u751f\u7406\u4fe1\u53f7\u5728\u5176\u9891\u7387\u5e26\u5bbd\u5185\u662f\u7a00\u758f\u7684\u7279\u6027\uff0c\u4e3a\u6a21\u578b\u63d0\u4f9b\u8109\u51b2\u7ebf\u7d22\u3002\u9884\u8bad\u7ec3\u7684\u7f16\u7801\u5668\u7136\u540e\u88ab\u8f6c\u79fb\u5230rPPG\u4efb\u52a1\uff0c\u7528\u4e8e\u4ece\u9762\u90e8\u89c6\u9891\u4e2d\u63d0\u53d6\u751f\u7406\u4fe1\u53f7\u3002", "result": "\u8be5\u65b9\u6cd5\u5728PURE\u3001UBFC-rPPG\u3001MMPD\u548cV4V\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u6027\u80fd\u5f97\u5230\u4e86\u663e\u8457\u63d0\u9ad8\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u4e2d\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u4e2d\u3002"}}
{"id": "2506.21571", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.21571", "abs": "https://arxiv.org/abs/2506.21571", "authors": ["Jianshuo Dong", "Yujia Fu", "Chuanrui Hu", "Chao Zhang", "Han Qiu"], "title": "Towards Understanding the Cognitive Habits of Large Reasoning Models", "comment": null, "summary": "Large Reasoning Models (LRMs), which autonomously produce a reasoning Chain\nof Thought (CoT) before producing final responses, offer a promising approach\nto interpreting and monitoring model behaviors. Inspired by the observation\nthat certain CoT patterns -- e.g., ``Wait, did I miss anything?'' --\nconsistently emerge across tasks, we explore whether LRMs exhibit human-like\ncognitive habits. Building on Habits of Mind, a well-established framework of\ncognitive habits associated with successful human problem-solving, we introduce\nCogTest, a principled benchmark designed to evaluate LRMs' cognitive habits.\nCogTest includes 16 cognitive habits, each instantiated with 25 diverse tasks,\nand employs an evidence-first extraction method to ensure reliable habit\nidentification. With CogTest, we conduct a comprehensive evaluation of 16\nwidely used LLMs (13 LRMs and 3 non-reasoning ones). Our findings reveal that\nLRMs, unlike conventional LLMs, not only exhibit human-like habits but also\nadaptively deploy them according to different tasks. Finer-grained analyses\nfurther uncover patterns of similarity and difference in LRMs' cognitive habit\nprofiles, particularly certain inter-family similarity (e.g., Qwen-3 models and\nDeepSeek-R1). Extending the study to safety-related tasks, we observe that\ncertain habits, such as Taking Responsible Risks, are strongly associated with\nthe generation of harmful responses. These findings suggest that studying\npersistent behavioral patterns in LRMs' CoTs is a valuable step toward deeper\nunderstanding of LLM misbehavior. The code is available at:\nhttps://github.com/jianshuod/CogTest.", "AI": {"tldr": "LRMs show human-like thinking patterns that can predict misbehavior.", "motivation": "Explore whether LRMs exhibit human-like cognitive habits, inspired by the observation of consistent CoT patterns across tasks.", "method": "Introduce CogTest, a benchmark with 16 cognitive habits, and evaluate 16 LLMs using an evidence-first extraction method.", "result": "LRMs, unlike conventional LLMs, exhibit human-like habits and adaptively deploy them. Certain habits are strongly associated with harmful responses.", "conclusion": "LRMs exhibit human-like cognitive habits and adaptively deploy them, which can be indicative of potential misbehavior in safety-related tasks."}}
{"id": "2506.22419", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22419", "abs": "https://arxiv.org/abs/2506.22419", "authors": ["Bingchen Zhao", "Despoina Magka", "Minqi Jiang", "Xian Li", "Roberta Raileanu", "Tatiana Shavrina", "Jean-Christophe Gagnon-Audet", "Kelvin Niu", "Shagun Sodhani", "Michael Shvartsman", "Andrei Lupu", "Alisia Lupidi", "Edan Toledo", "Karen Hambardzumyan", "Martin Josifoski", "Thomas Foster", "Lucia Cipolina-Kun", "Abhishek Charnalia", "Derek Dunfield", "Alexander H. Miller", "Oisin Mac Aodha", "Jakob Foerster", "Yoram Bachrach"], "title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements", "comment": null, "summary": "Rapid advancements in large language models (LLMs) have the potential to\nassist in scientific progress. A critical capability toward this endeavor is\nthe ability to reproduce existing work. To evaluate the ability of AI agents to\nreproduce results in an active research area, we introduce the Automated LLM\nSpeedrunning Benchmark, leveraging the research community contributions on the\nNanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.\nEach of the 19 speedrun tasks provides the agent with the previous records\ntraining script, optionally paired with one of three hint formats, ranging from\npseudocode to paper-like descriptions of the new records improvements. Records\nexecute quickly by design and speedrun improvements encompass diverse\ncode-level changes, ranging from high-level algorithmic advancements to\nhardware-aware optimizations. These features make the benchmark both accessible\nand realistic for the frontier problem of improving LLM training. We find that\nrecent reasoning LLMs combined with SoTA scaffolds struggle to reimplement\nalready-known innovations in our benchmark, even when given detailed hints. Our\nbenchmark thus provides a simple, non-saturated measure of an LLMs ability to\nautomate scientific reproduction, a necessary (but not sufficient) skill for an\nautonomous research agent.", "AI": {"tldr": "This paper introduces a benchmark to evaluate LLMs' ability to reproduce research results in LLM training. The results show that current LLMs struggle to reimplement known innovations, highlighting the challenges in automating scientific reproduction.", "motivation": "Evaluate the ability of AI agents to reproduce results in an active research area.", "method": "Automated LLM Speedrunning Benchmark, leveraging the research community contributions on the NanoGPT speedrun.", "result": "LLMs struggle to reimplement already-known innovations in the benchmark, even when given detailed hints.", "conclusion": "Recent reasoning LLMs combined with SoTA scaffolds struggle to reimplement already-known innovations, even when given detailed hints. This benchmark provides a simple, non-saturated measure of an LLMs ability to automate scientific reproduction."}}
{"id": "2506.21940", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21940", "abs": "https://arxiv.org/abs/2506.21940", "authors": ["Marwan Ait Haddou", "Mohamed Bennai"], "title": "GuiderNet: A Meta-Learning Framework for Optimizing Quantum Circuit Geometry and Mitigating Barren Plateaus", "comment": null, "summary": "Variational Quantum Algorithms (VQAs) offer potential for near-term quantum\nadvantage but face challenges from barren plateaus, where gradients vanish, and\npoorly conditioned optimization landscapes. We introduce GuiderNet, a\nmeta-learning framework that conditions Parameterized Quantum Circuits (PQCs)\nusing data-dependent parameter shifts aimed at minimizing the log condition\nnumber of the Fubini-Study metric tensor. Implemented as a classical neural\nnetwork, GuiderNet is meta-trained to guide PQC parameters into geometrically\nfavorable regions and is embedded within hybrid quantum-classical pipelines to\nsteer both initialization and adaptive modulation during training.\n  Applied to the Kaggle Diabetes classification task, GuiderNet reduces\ncumulative training loss by over 5x, improves test accuracy from 75.3% to\n98.6%, and increases the minority-class F1 score from 0.67 to 0.95. It also\nsuppresses gradient explosion and stabilizes parameter updates, enabling\nsmoother and more robust optimization. These results demonstrate that geometric\nmeta-conditioning can mitigate barren plateaus and ill-conditioning, providing\na scalable approach to enhance trainability and generalization in quantum\nmachine learning.", "AI": {"tldr": "GuiderNet \u662f\u4e00\u79cd\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u53ef\u6307\u5bfc PQC \u53c2\u6570\u8fdb\u5165\u51e0\u4f55\u4e0a\u6709\u5229\u7684\u533a\u57df\uff0c\u5e76\u5d4c\u5165\u5230\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u7ba1\u9053\u4e2d\uff0c\u4ee5\u5728\u8bad\u7ec3\u671f\u95f4\u5f15\u5bfc\u521d\u59cb\u5316\u548c\u81ea\u9002\u5e94\u8c03\u5236\uff0c\u4ece\u800c\u51cf\u8f7b\u8d2b\u7620\u9ad8\u539f\u548c\u75c5\u6001\u6761\u4ef6\u3002", "motivation": "\u53d8\u5206\u91cf\u5b50\u7b97\u6cd5 (VQA) \u5177\u6709\u8fd1\u671f\u91cf\u5b50\u4f18\u52bf\u7684\u6f5c\u529b\uff0c\u4f46\u9762\u4e34\u7740\u68af\u5ea6\u6d88\u5931\u7684\u8d2b\u7620\u9ad8\u539f\u548c\u6761\u4ef6\u4e0d\u826f\u7684\u4f18\u5316\u73af\u5883\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "GuiderNet\uff0c\u4e00\u79cd\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u5b83\u4f7f\u7528\u6570\u636e\u76f8\u5173\u7684\u53c2\u6570\u79fb\u4f4d\u6765\u8c03\u8282\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def (PQC)\uff0c\u65e8\u5728\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11 Fubini-Study \u5ea6\u91cf\u5f20\u91cf\u7684\u5bf9\u6570\u6761\u4ef6\u6570\u3002", "result": "\u5e94\u7528\u4e8e Kaggle \u7cd6\u5c3f\u75c5\u5206\u7c7b\u4efb\u52a1\uff0cGuiderNet \u4f7f\u7d2f\u79ef\u8bad\u7ec3\u635f\u5931\u51cf\u5c11\u4e86 5 \u500d\u4ee5\u4e0a\uff0c\u5c06\u6d4b\u8bd5\u7cbe\u5ea6\u4ece 75.3% \u63d0\u9ad8\u5230 98.6%\uff0c\u5e76\u5c06\u5c11\u6570\u7c7b F1 \u5206\u6570\u4ece 0.67 \u63d0\u9ad8\u5230 0.95\u3002\u5b83\u8fd8\u53ef\u4ee5\u6291\u5236\u68af\u5ea6\u7206\u70b8\u5e76\u7a33\u5b9a\u53c2\u6570\u66f4\u65b0\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u5e73\u6ed1\u3001\u66f4\u7a33\u5065\u7684\u4f18\u5316\u3002", "conclusion": "\u51e0\u4f55\u5143\u6761\u4ef6\u53cd\u5c04\u53ef\u4ee5\u7f13\u89e3\u8d2b\u7620\u9ad8\u539f\u548c\u75c5\u6001\u6761\u4ef6\uff0c\u4e3a\u589e\u5f3a\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u7684\u53ef\u8bad\u7ec3\u6027\u548c\u6cdb\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u3002"}}
{"id": "2506.22262", "categories": ["cs.IR", "H.3.3"], "pdf": "https://arxiv.org/pdf/2506.22262", "abs": "https://arxiv.org/abs/2506.22262", "authors": ["Evgeny Dedov"], "title": "JointRank: Rank Large Set with Single Pass", "comment": "ICTIR'25 Accepted", "summary": "Efficiently ranking relevant items from large candidate pools is a\ncornerstone of modern information retrieval systems -- such as web search,\nrecommendation, and retrieval-augmented generation. Listwise rerankers, which\nimprove relevance by jointly considering multiple candidates, are often limited\nin practice: either by model input size constraints, or by degraded quality\nwhen processing large sets. We propose a model-agnostic method for fast\nreranking large sets that exceed a model input limits. The method first\npartitions candidate items into overlapping blocks, each of which is ranked\nindependently in parallel. Implicit pairwise comparisons are then derived from\nthese local rankings. Finally, these comparisons are aggregated to construct a\nglobal ranking using algorithms such as Winrate or PageRank. Experiments on\nTREC DL-2019 show that our method achieves an nDCG@10 of 70.88 compared to the\n57.68 for full-context listwise approach using gpt-4.1-mini as long-context\nmodel, while reducing latency from 21 to 8 seconds.\n  The implementation of the algorithm and the experiments is available in the\nrepository: https://github.com/V3RGANz/jointrank", "AI": {"tldr": "Proposes a model-agnostic method for fast reranking large sets that exceed a model input limits by partitioning candidate items into overlapping blocks, ranking independently and aggregating to construct a global ranking.", "motivation": "Listwise rerankers are often limited in practice by model input size constraints, or by degraded quality when processing large sets.", "method": "The method partitions candidate items into overlapping blocks, each of which is ranked independently in parallel. Implicit pairwise comparisons are then derived from these local rankings. Finally, these comparisons are aggregated to construct a global ranking using algorithms such as Winrate or PageRank.", "result": "Achieves a nDCG@10 of 70.88 compared to the 57.68 for full-context listwise approach using gpt-4.1-mini, while reducing latency from 21 to 8 seconds.", "conclusion": "The proposed method achieves a nDCG@10 of 70.88 compared to the 57.68 for full-context listwise approach using gpt-4.1-mini as long-context model, while reducing latency from 21 to 8 seconds."}}
{"id": "2506.21857", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21857", "abs": "https://arxiv.org/abs/2506.21857", "authors": ["Ekaterina Redekop", "Mara Pleasure", "Zichen Wang", "Kimberly Flores", "Anthony Sisk", "William Speier", "Corey W. Arnold"], "title": "SPADE: Spatial Transcriptomics and Pathology Alignment Using a Mixture of Data Experts for an Expressive Latent Space", "comment": null, "summary": "The rapid growth of digital pathology and advances in self-supervised deep\nlearning have enabled the development of foundational models for various\npathology tasks across diverse diseases. While multimodal approaches\nintegrating diverse data sources have emerged, a critical gap remains in the\ncomprehensive integration of whole-slide images (WSIs) with spatial\ntranscriptomics (ST), which is crucial for capturing critical molecular\nheterogeneity beyond standard hematoxylin & eosin (H&E) staining. We introduce\nSPADE, a foundation model that integrates histopathology with ST data to guide\nimage representation learning within a unified framework, in effect creating an\nST-informed latent space. SPADE leverages a mixture-of-data experts technique,\nwhere experts, created via two-stage feature-space clustering, use contrastive\nlearning to learn representations of co-registered WSI patches and gene\nexpression profiles. Pre-trained on the comprehensive HEST-1k dataset, SPADE is\nevaluated on 14 downstream tasks, demonstrating significantly superior few-shot\nperformance compared to baseline models, highlighting the benefits of\nintegrating morphological and molecular information into one latent space.", "AI": {"tldr": "SPADE\u662f\u4e00\u4e2a\u57fa\u7840\u6a21\u578b\uff0c\u5b83\u96c6\u6210\u4e86\u7ec4\u7ec7\u75c5\u7406\u5b66\u4e0eST\u6570\u636e\uff0c\u4ee5\u6307\u5bfc\u7edf\u4e00\u6846\u67b6\u5185\u7684\u56fe\u50cf\u8868\u5f81\u5b66\u4e60\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6765\u5b66\u4e60\u5171\u540c\u6ce8\u518c\u7684WSI\u8865\u4e01\u548c\u57fa\u56e0\u8868\u8fbe\u8c31\u7684\u8868\u5f81, \u5e76\u572814\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u660e\u663e\u4f18\u8d8a\u7684few-shot\u6027\u80fd\u3002", "motivation": "\u6570\u5b57\u75c5\u7406\u5b66\u7684\u5feb\u901f\u53d1\u5c55\u548c\u81ea\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u7684\u8fdb\u6b65\uff0c\u4f7f\u5f97\u4e3a\u8de8\u591a\u79cd\u75be\u75c5\u7684\u5404\u79cd\u75c5\u7406\u5b66\u4efb\u52a1\u5f00\u53d1\u57fa\u7840\u6a21\u578b\u6210\u4e3a\u53ef\u80fd\u3002\u5168\u5207\u7247\u56fe\u50cf\uff08WSI\uff09\u4e0e\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\uff08ST\uff09\u7684\u7efc\u5408\u96c6\u6210\u4ecd\u7136\u5b58\u5728\u4e00\u4e2a\u5173\u952e\u7684\u5dee\u8ddd\uff0c\u8fd9\u5bf9\u4e8e\u6355\u83b7\u8d85\u51fa\u6807\u51c6\u82cf\u6728\u7cbe\u548c\u4f0a\u7ea2\uff08H\uff06E\uff09\u67d3\u8272\u7684\u5173\u952e\u5206\u5b50\u5f02\u8d28\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "SPADE\u5229\u7528\u4e00\u79cd\u6df7\u5408\u6570\u636e\u4e13\u5bb6\u6280\u672f\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u7279\u5f81\u7a7a\u95f4\u805a\u7c7b\u521b\u5efa\u4e13\u5bb6\uff0c\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u6765\u5b66\u4e60\u5171\u540c\u6ce8\u518c\u7684WSI\u8865\u4e01\u548c\u57fa\u56e0\u8868\u8fbe\u8c31\u7684\u8868\u5f81\u3002", "result": "SPADE\u662f\u4e00\u79cd\u57fa\u7840\u6a21\u578b\uff0c\u5b83\u5c06\u7ec4\u7ec7\u75c5\u7406\u5b66\u4e0eST\u6570\u636e\u96c6\u6210\uff0c\u4ee5\u6307\u5bfc\u7edf\u4e00\u6846\u67b6\u5185\u7684\u56fe\u50cf\u8868\u5f81\u5b66\u4e60\uff0c\u5b9e\u9645\u4e0a\u521b\u5efa\u4e86\u4e00\u4e2aST\u4fe1\u606f\u6f5c\u5728\u7a7a\u95f4\u3002", "conclusion": "SPADE\u572814\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u4e0e\u57fa\u7ebf\u6a21\u578b\u76f8\u6bd4\uff0c\u8868\u73b0\u51fa\u660e\u663e\u4f18\u8d8a\u7684few-shot\u6027\u80fd\uff0c\u7a81\u51fa\u4e86\u5c06\u5f62\u6001\u5b66\u548c\u5206\u5b50\u4fe1\u606f\u6574\u5408\u5230\u4e00\u4e2a\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u597d\u5904\u3002"}}
{"id": "2506.21572", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21572", "abs": "https://arxiv.org/abs/2506.21572", "authors": ["Tianyu. Zou", "Shengwu. Xiong", "Ruilin. Yao", "Jirui. Huang", "Yi. Rong", "Yaxiong. Chen", "Shili. Xiong", "Cong. Wang"], "title": "Aligning MLLM Benchmark With Human Preferences via Structural Equation Modeling", "comment": "9 pages, 5 figures", "summary": "Evaluating multimodal large language models (MLLMs) remains a fundamental\nchallenge due to a lack of structured, interpretable, and theoretically\ngrounded benchmark designs. Existing benchmarks often adopt heuristic-based\ntask groupings with unclear cognitive targets, thus resulting in overlapping\nabilities, redundant indicators, and limited diagnostic power. In this work, we\npropose a novel framework for aligning MLLM benchmark based on Structural\nEquation Modeling (SEM) to analyze and quantify the internal validity,\ndimensional separability, and contribution of benchmark components. Motivated\nby the observed limitations of current designs, we further introduce a novel\ncapability hierarchy grounded in Piagets theory of cognitive development,\ndividing MLLM abilities into three hierarchical layers, i.e., Perception,\nMemory, and Reasoning. We reorganize existing MLLM benchmarks under the\nproposed framework and construct a new benchmark named Gold. Experimental\nresults demonstrate that the proposed benchmark exhibits stronger\ninterpretability, reduced indicator redundancy, and clearer cognitive\nconsistency compared to existing approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684 MLLM \u57fa\u51c6\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u57fa\u4e8e\u7ed3\u6784\u65b9\u7a0b\u5efa\u6a21\u548c\u76ae\u4e9a\u6770\u7684\u8ba4\u77e5\u53d1\u5c55\u7406\u8bba\uff0c\u4ee5\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u548c\u51cf\u5c11\u5197\u4f59\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u7ed3\u6784\u5316\u3001\u53ef\u89e3\u91ca\u548c\u7406\u8bba \u043e\u0431\u043e\u0441\u043d\u043e\u0432\u0430\u043d\u043d\u043e\u0439 \u57fa\u51c6\u8bbe\u8ba1\uff0c\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u4ecd\u7136\u662f\u4e00\u9879\u6839\u672c\u6027\u6311\u6218\u3002\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u91c7\u7528\u57fa\u4e8e\u542f\u53d1\u5f0f\u7684\u4efb\u52a1\u5206\u7ec4\uff0c\u8ba4\u77e5\u76ee\u6807\u4e0d\u660e\u786e\uff0c\u5bfc\u81f4\u80fd\u529b\u91cd\u53e0\u3001\u6307\u6807\u5197\u4f59\u548c\u8bca\u65ad\u80fd\u529b\u6709\u9650\u3002", "method": "\u57fa\u4e8e\u7ed3\u6784\u65b9\u7a0b\u5efa\u6a21 (SEM) \u5bf9\u9f50 MLLM \u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u6839\u636e\u76ae\u4e9a\u6770\u7684\u8ba4\u77e5\u53d1\u5c55\u7406\u8bba\u5f15\u5165\u65b0\u7684\u80fd\u529b\u5c42\u6b21\u7ed3\u6784\u3002", "result": "\u8be5\u57fa\u51c6\u6d4b\u8bd5\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u51cf\u5c11\u7684\u6307\u6807\u5197\u4f59\u548c\u66f4\u6e05\u6670\u7684\u8ba4\u77e5\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u57fa\u51c6\u6d4b\u8bd5\u6bd4\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u66f4\u5f3a\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u66f4\u5c11\u7684\u6307\u6807\u5197\u4f59\u548c\u66f4\u6e05\u6670\u7684\u8ba4\u77e5\u4e00\u81f4\u6027\u3002"}}
{"id": "2109.05721", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2109.05721", "abs": "https://arxiv.org/abs/2109.05721", "authors": ["Yangyu Huang", "Hao Yang", "Chong Li", "Jongyoo Kim", "Fangyun Wei"], "title": "ADNet: Leveraging Error-Bias Towards Normal Direction in Face Alignment", "comment": "Proceedings of the IEEE/CVF International Conference on Computer\n  Vision. 2021 (ICCV 2021)", "summary": "The recent progress of CNN has dramatically improved face alignment\nperformance. However, few works have paid attention to the error-bias with\nrespect to error distribution of facial landmarks. In this paper, we\ninvestigate the error-bias issue in face alignment, where the distributions of\nlandmark errors tend to spread along the tangent line to landmark curves. This\nerror-bias is not trivial since it is closely connected to the ambiguous\nlandmark labeling task. Inspired by this observation, we seek a way to leverage\nthe error-bias property for better convergence of CNN model. To this end, we\npropose anisotropic direction loss (ADL) and anisotropic attention module (AAM)\nfor coordinate and heatmap regression, respectively. ADL imposes strong binding\nforce in normal direction for each landmark point on facial boundaries. On the\nother hand, AAM is an attention module which can get anisotropic attention mask\nfocusing on the region of point and its local edge connected by adjacent\npoints, it has a stronger response in tangent than in normal, which means\nrelaxed constraints in the tangent. These two methods work in a complementary\nmanner to learn both facial structures and texture details. Finally, we\nintegrate them into an optimized end-to-end training pipeline named ADNet. Our\nADNet achieves state-of-the-art results on 300W, WFLW and COFW datasets, which\ndemonstrates the effectiveness and robustness.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4eba\u8138\u5bf9\u9f50\u4e2d\u7684\u8bef\u5dee\u504f\u5dee\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86ADNet\uff0c\u5b83\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u4eba\u8138\u5bf9\u9f50\u8bef\u5dee\u5206\u5e03\u5b58\u5728\u8bef\u5dee\u504f\u5dee\u95ee\u9898\uff0c\u5373\u5730\u6807\u8bef\u5dee\u7684\u5206\u5e03\u503e\u5411\u4e8e\u6cbf\u7740\u5730\u6807\u66f2\u7ebf\u7684\u5207\u7ebf\u65b9\u5411\u6269\u5c55\u3002\u8fd9\u79cd\u8bef\u5dee\u504f\u5dee\u4e0e\u6a21\u7cca\u7684\u5730\u6807\u6807\u6ce8\u4efb\u52a1\u5bc6\u5207\u76f8\u5173\u3002", "method": "\u63d0\u51fa\u4e86\u5404\u5411\u5f02\u6027\u65b9\u5411\u635f\u5931\uff08ADL\uff09\u548c\u5404\u5411\u5f02\u6027\u6ce8\u610f\u529b\u6a21\u5757\uff08AAM\uff09\uff0c\u5206\u522b\u7528\u4e8e\u5750\u6807\u548c\u70ed\u56fe\u56de\u5f52\u3002", "result": "ADL\u5bf9\u4eba\u8138\u8fb9\u754c\u4e0a\u6bcf\u4e2a\u5730\u6807\u70b9\u7684\u6cd5\u7ebf\u65b9\u5411\u65bd\u52a0\u4e86\u5f88\u5f3a\u7684\u7ea6\u675f\u529b\u3002AAM\u662f\u4e00\u4e2a\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u53ef\u4ee5\u83b7\u5f97\u5404\u5411\u5f02\u6027\u7684\u6ce8\u610f\u529b\u63a9\u7801\uff0c\u8be5\u63a9\u7801\u5173\u6ce8\u70b9\u53ca\u5176\u76f8\u90bb\u70b9\u8fde\u63a5\u7684\u5c40\u90e8\u8fb9\u7f18\u533a\u57df\uff0c\u5b83\u5728\u5207\u7ebf\u65b9\u5411\u7684\u54cd\u5e94\u6bd4\u5728\u6cd5\u7ebf\u65b9\u5411\u7684\u54cd\u5e94\u66f4\u5f3a\uff0c\u8fd9\u610f\u5473\u7740\u5728\u5207\u7ebf\u65b9\u5411\u7684\u7ea6\u675f\u8f83\u5c11\u3002", "conclusion": "ADNet\u5728300W\u3001WFLW\u548cCOFW\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.21952", "categories": ["cs.LG", "physics.app-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2506.21952", "abs": "https://arxiv.org/abs/2506.21952", "authors": ["Yangyang Wan", "Haotian Wang", "Xuhui Yu", "Jiageng Chen", "Xinyu Fan", "Zuyuan He"], "title": "Physics-informed network paradigm with data generation and background noise removal for diverse distributed acoustic sensing applications", "comment": null, "summary": "Distributed acoustic sensing (DAS) has attracted considerable attention\nacross various fields and artificial intelligence (AI) technology plays an\nimportant role in DAS applications to realize event recognition and denoising.\nExisting AI models require real-world data (RWD), whether labeled or not, for\ntraining, which is contradictory to the fact of limited available event data in\nreal-world scenarios. Here, a physics-informed DAS neural network paradigm is\nproposed, which does not need real-world events data for training. By\nphysically modeling target events and the constraints of real world and DAS\nsystem, physical functions are derived to train a generative network for\ngeneration of DAS events data. DAS debackground net is trained by using the\ngenerated DAS events data to eliminate background noise in DAS data. The\neffectiveness of the proposed paradigm is verified in event identification\napplication based on a public dataset of DAS spatiotemporal data and in belt\nconveyor fault monitoring application based on DAS time-frequency data, and\nachieved comparable or better performance than data-driven networks trained\nwith RWD. Owing to the introduction of physical information and capability of\nbackground noise removal, the paradigm demonstrates generalization in same\napplication on different sites. A fault diagnosis accuracy of 91.8% is achieved\nin belt conveyor field with networks which transferred from simulation test\nsite without any fault events data of test site and field for training. The\nproposed paradigm is a prospective solution to address significant obstacles of\ndata acquisition and intense noise in practical DAS applications and explore\nmore potential fields for DAS.", "AI": {"tldr": "A physics-informed DAS neural network paradigm is proposed, which does not need real-world events data for training and demonstrates generalization in same application on different sites.", "motivation": "Existing AI models require real-world data (RWD) for training, which is contradictory to the fact of limited available event data in real-world scenarios.", "method": "a physics-informed DAS neural network paradigm which does not need real-world events data for training. Physical functions are derived to train a generative network for generation of DAS events data. DAS debackground net is trained by using the generated DAS events data to eliminate background noise in DAS data.", "result": "achieved comparable or better performance than data-driven networks trained with RWD. A fault diagnosis accuracy of 91.8% is achieved in belt conveyor field with networks which transferred from simulation test site without any fault events data of test site and field for training.", "conclusion": "The proposed physics-informed DAS neural network paradigm is a prospective solution to address significant obstacles of data acquisition and intense noise in practical DAS applications and explore more potential fields for DAS."}}
{"id": "2506.22303", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2506.22303", "abs": "https://arxiv.org/abs/2506.22303", "authors": ["Xinghe Cheng", "Zihan Zhang", "Jiapu Wang", "Liangda Fang", "Chaobo He", "Quanlong Guan", "Shirui Pan", "Weiqi Luo"], "title": "Education-Oriented Graph Retrieval-Augmented Generation for Learning Path Recommendation", "comment": null, "summary": "Learning path recommendation seeks to provide learners with a structured\nsequence of learning items (e.g., knowledge concepts or exercises) to optimize\ntheir learning efficiency. Despite significant efforts in this area, most\nexisting methods primarily rely on prerequisite relationships, which present\ntwo major limitations: 1) Many educational datasets do not explicitly provide\nprerequisite relationships between knowledge concepts, hindering the\napplication of current learning path recommendation methods. 2) Relying solely\non prerequisite relationships as the sole knowledge structure can impede\nlearning progress and negatively impact student outcomes. To address these\nchallenges, we propose a novel approach, Discrimination Learning Enhances\nLearning Path Recommendation (DLELP), which enhances learning path\nrecommendations by incorporating both prerequisite and similarity relationships\nbetween knowledge concepts. Specifically, we introduce a knowledge concept\nstructure graph generation module that adaptively constructs knowledge concept\nstructure graphs for different educational datasets, significantly improving\nthe generalizability of learning path recommendation methods. We then propose a\nDiscrimination Learning-driven Reinforcement Learning (DLRL) framework, which\nmitigates the issue of blocked learning paths, further enhancing the efficacy\nof learning path recommendations. Finally, we conduct extensive experiments on\nthree benchmark datasets, demonstrating that our method not only achieves\nstate-of-the-art performance but also provides interpretable reasoning for the\nrecommended learning paths.", "AI": {"tldr": "propose Discrimination Learning Enhances Learning Path Recommendation (DLELP), which enhances learning path recommendations by incorporating both prerequisite and similarity relationships between knowledge concepts to address the limitations of relying solely on prerequisite relationships, and achieves state-of-the-art performance", "motivation": "existing methods primarily rely on prerequisite relationships, which present two major limitations: 1) Many educational datasets do not explicitly provide prerequisite relationships between knowledge concepts, hindering the application of current learning path recommendation methods. 2) Relying solely on prerequisite relationships as the sole knowledge structure can impede learning progress and negatively impact student outcomes.", "method": "incorporates both prerequisite and similarity relationships between knowledge concepts. Specifically, introduce a knowledge concept structure graph generation module that adaptively constructs knowledge concept structure graphs for different educational datasets. propose a Discrimination Learning-driven Reinforcement Learning (DLRL) framework, which mitigates the issue of blocked learning paths", "result": "achieves state-of-the-art performance. provides interpretable reasoning for the recommended learning paths.", "conclusion": "achieves state-of-the-art performance and provides interpretable reasoning for the recommended learning paths."}}
{"id": "2506.21862", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.21862", "abs": "https://arxiv.org/abs/2506.21862", "authors": ["Boyuan Sun", "Jiaxing Zhao", "Xihan Wei", "Qibin Hou"], "title": "LLaVA-Scissor: Token Compression with Semantic Connected Components for Video LLMs", "comment": "21 pages, 4 figures, 7 tables", "summary": "In this paper, we present LLaVA-Scissor, a training-free token compression\nstrategy designed for video multimodal large language models. Previous methods\nmostly attempt to compress tokens based on attention scores, but fail to\neffectively capture all semantic regions and often lead to token redundancy.\nDifferently, we propose to leverage the Semantic Connected Components (SCC)\napproach that assigns tokens to distinct semantic regions within the token set,\nensuring comprehensive semantic coverage. The outcome is a two-step\nspatio-temporal token compression strategy that utilizes SCC in both spatial\nand temporal domains. This strategy can effectively compress tokens by\nrepresenting the entire video with a set of non-overlapping semantic tokens. We\nconduct extensive evaluations of the token compression capabilities of\nLLaVA-Scissor across diverse video understanding benchmarks, including video\nquestion answering, long video understanding, and comprehensive multi-choices\nbenchmarks. Experimental results show that the proposed LLaVA-Scissor\noutperforms other token compression methods, achieving superior performance in\nvarious video understanding benchmarks, particularly at low token retention\nratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.", "AI": {"tldr": "LLaVA-Scissor \u662f\u4e00\u79cd\u514d\u8bad\u7ec3\u7684 token \u538b\u7f29\u7b56\u7565\uff0c\u5b83\u4f7f\u7528\u8bed\u4e49\u8fde\u63a5\u7ec4\u4ef6\u6765\u538b\u7f29\u89c6\u9891\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684 token\uff0c\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "motivation": "\u4ee5\u5f80\u7684\u65b9\u6cd5\u5927\u591a\u5c1d\u8bd5\u57fa\u4e8e\u6ce8\u610f\u529b\u5206\u6570\u6765\u538b\u7f29 token\uff0c\u4f46\u672a\u80fd\u6709\u6548\u6355\u83b7\u6240\u6709\u8bed\u4e49\u533a\u57df\uff0c\u5e76\u4e14\u7ecf\u5e38\u5bfc\u81f4 token \u5197\u4f59\u3002", "method": "\u5229\u7528\u8bed\u4e49\u8fde\u63a5\u7ec4\u4ef6 (SCC) \u65b9\u6cd5\uff0c\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u57df\u4e2d\u8fdb\u884c\u4e24\u6b65\u65f6\u7a7a token \u538b\u7f29\u3002", "result": "LLaVA-Scissor \u80fd\u591f\u6709\u6548\u5730\u538b\u7f29 token\uff0c\u7528\u4e00\u7ec4\u4e0d\u91cd\u53e0\u7684\u8bed\u4e49 token \u6765\u8868\u793a\u6574\u4e2a\u89c6\u9891\u3002", "conclusion": "LLaVA-Scissor \u5728\u5404\u79cd\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5176\u4ed6 token \u538b\u7f29\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u4f4e token \u4fdd\u7559\u7387\u4e0b\u3002"}}
{"id": "2506.21573", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21573", "abs": "https://arxiv.org/abs/2506.21573", "authors": ["Yanwei Ren", "Liu Liu", "Baosheng Yu", "Jiayan Qiu", "Quan Chen"], "title": "Instruction Learning Paradigms: A Dual Perspective on White-box and Black-box LLMs", "comment": null, "summary": "Optimizing instructions for large language models (LLMs) is critical for\nharnessing their full potential in complex and diverse tasks. However, relying\nsolely on white-box approaches demands extensive computational resources and\noffers limited representational capacity, while black-box models can incur\nprohibitive financial costs. To address these challenges, we introduce a novel\nframework that seamlessly merges the strengths of both paradigms. Black-box\nmodels provide high-quality, diverse instruction initializations, and white-box\nmodels supply fine-grained interpretability through hidden states and output\nfeatures. By enforcing a semantic similarity constraint, these components fuse\ninto a unified high-dimensional representation that captures deep semantic and\nstructural nuances, enabling an iterative optimization process to refine\ninstruction quality and adaptability. Extensive evaluations across a broad\nspectrum of tasks-ranging from complex reasoning to cross-lingual\ngeneralization-demonstrate that our approach consistently outperforms\nstate-of-the-art baselines. This fusion of black-box initialization with\nadvanced semantic refinement yields a scalable and efficient solution, paving\nthe way for next-generation LLM-driven applications in diverse real-world\nscenarios. The source code will be released soon.", "AI": {"tldr": "This paper introduces a novel framework that combines black-box and white-box approaches to optimize instructions for large language models (LLMs).", "motivation": "Relying solely on white-box approaches demands extensive computational resources and offers limited representational capacity, while black-box models can incur prohibitive financial costs. To address these challenges", "method": "We introduce a novel framework that seamlessly merges the strengths of both paradigms. Black-box models provide high-quality, diverse instruction initializations, and white-box models supply fine-grained interpretability through hidden states and output features. By enforcing a semantic similarity constraint, these components fuse into a unified high-dimensional representation that captures deep semantic and structural nuances, enabling an iterative optimization process to refine instruction quality and adaptability.", "result": "our approach consistently outperforms state-of-the-art baselines. Extensive evaluations across a broad spectrum of tasks-ranging from complex reasoning to cross-lingual generalization", "conclusion": "This fusion of black-box initialization with advanced semantic refinement yields a scalable and efficient solution, paving the way for next-generation LLM-driven applications in diverse real-world scenarios."}}
{"id": "2212.09525", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2212.09525", "abs": "https://arxiv.org/abs/2212.09525", "authors": ["Yangyu Huang", "Xi Chen", "Jongyoo Kim", "Hao Yang", "Chong Li", "Jiaolong Yang", "Dong Chen"], "title": "FreeEnricher: Enriching Face Landmarks without Additional Cost", "comment": "AAAI 2023", "summary": "Recent years have witnessed significant growth of face alignment. Though\ndense facial landmark is highly demanded in various scenarios, e.g., cosmetic\nmedicine and facial beautification, most works only consider sparse face\nalignment. To address this problem, we present a framework that can enrich\nlandmark density by existing sparse landmark datasets, e.g., 300W with 68\npoints and WFLW with 98 points. Firstly, we observe that the local patches\nalong each semantic contour are highly similar in appearance. Then, we propose\na weakly-supervised idea of learning the refinement ability on original sparse\nlandmarks and adapting this ability to enriched dense landmarks. Meanwhile,\nseveral operators are devised and organized together to implement the idea.\nFinally, the trained model is applied as a plug-and-play module to the existing\nface alignment networks. To evaluate our method, we manually label the dense\nlandmarks on 300W testset. Our method yields state-of-the-art accuracy not only\nin newly-constructed dense 300W testset but also in the original sparse 300W\nand WFLW testsets without additional cost.", "AI": {"tldr": "This paper presents a framework that can enrich landmark density by existing sparse landmark datasets. The method achieves state-of-the-art accuracy on both the newly constructed dense 300W testset and the original sparse 300W and WFLW testsets without additional cost.", "motivation": "Most works only consider sparse face alignment, while dense facial landmark is highly demanded in various scenarios.", "method": "A weakly-supervised learning approach is proposed to learn the refinement ability on original sparse landmarks and adapt this ability to enriched dense landmarks. Several operators are devised and organized together to implement the idea.", "result": "The proposed method yields state-of-the-art accuracy on the newly-constructed dense 300W testset and the original sparse 300W and WFLW testsets.", "conclusion": "The proposed method achieves state-of-the-art accuracy on both the newly constructed dense 300W testset and the original sparse 300W and WFLW testsets without additional cost."}}
{"id": "2506.21956", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21956", "abs": "https://arxiv.org/abs/2506.21956", "authors": ["Hao Jiang", "Yongxiang Tang", "Yanxiang Zeng", "Pengjia Yuan", "Yanhua Cheng", "Teng Sha", "Xialong Liu", "Peng Jiang"], "title": "Optimal Return-to-Go Guided Decision Transformer for Auto-Bidding in Advertisement", "comment": null, "summary": "In the realm of online advertising, advertisers partake in ad auctions to\nobtain advertising slots, frequently taking advantage of auto-bidding tools\nprovided by demand-side platforms. To improve the automation of these bidding\nsystems, we adopt generative models, namely the Decision Transformer (DT), to\ntackle the difficulties inherent in automated bidding. Applying the Decision\nTransformer to the auto-bidding task enables a unified approach to sequential\nmodeling, which efficiently overcomes short-sightedness by capturing long-term\ndependencies between past bidding actions and user behavior. Nevertheless,\nconventional DT has certain drawbacks: (1) DT necessitates a preset\nreturn-to-go (RTG) value before generating actions, which is not inherently\nproduced; (2) The policy learned by DT is restricted by its training data,\nwhich is consists of mixed-quality trajectories. To address these challenges,\nwe introduce the R* Decision Transformer (R* DT), developed in a three-step\nprocess: (1) R DT: Similar to traditional DT, R DT stores actions based on\nstate and RTG value, as well as memorizing the RTG for a given state using the\ntraining set; (2) R^ DT: We forecast the highest value (within the training\nset) of RTG for a given state, deriving a suboptimal policy based on the\ncurrent state and the forecasted supreme RTG value; (3) R* DT: Based on R^ DT,\nwe generate trajectories and select those with high rewards (using a simulator)\nto augment our training dataset. This data enhancement has been shown to\nimprove the RTG of trajectories in the training data and gradually leads the\nsuboptimal policy towards optimality. Comprehensive tests on a publicly\navailable bidding dataset validate the R* DT's efficacy and highlight its\nsuperiority when dealing with mixed-quality trajectories.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a R* DT \u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u6295\u6807\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\u96c6\u6765\u9010\u6b65\u5c06\u6b21\u4f18\u7b56\u7565\u5f15\u5411\u6700\u4f18\u3002", "motivation": "\u4e3a\u4e86\u6539\u8fdb\u81ea\u52a8\u6295\u6807\u7cfb\u7edf\u7684\u81ea\u52a8\u5316\uff0c\u6211\u4eec\u91c7\u7528\u751f\u6210\u6a21\u578b\uff0c\u5373\u51b3\u7b56\u8f6c\u6362\u5668 (DT)\uff0c\u6765\u89e3\u51b3\u81ea\u52a8\u6295\u6807\u4e2d\u56fa\u6709\u7684\u56f0\u96be\u3002", "method": "R* Decision Transformer (R* DT)", "result": "\u5728\u516c\u5f00\u7684\u6295\u6807\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u6d4b\u8bd5\u9a8c\u8bc1\u4e86 R* DT \u7684\u6709\u6548\u6027\uff0c\u5e76\u7a81\u51fa\u4e86\u5176\u5728\u5904\u7406\u6df7\u5408\u8d28\u91cf\u8f68\u8ff9\u65f6\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "R* DT\u5728\u5904\u7406\u6df7\u5408\u8d28\u91cf\u8f68\u8ff9\u65f6\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002"}}
{"id": "2506.22356", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2506.22356", "abs": "https://arxiv.org/abs/2506.22356", "authors": ["Kevin Duh", "Eugene Yang", "Orion Weller", "Andrew Yates", "Dawn Lawrie"], "title": "HLTCOE at LiveRAG: GPT-Researcher using ColBERT retrieval", "comment": "5 pages, 1 figure", "summary": "The HLTCOE LiveRAG submission utilized the GPT-researcher framework for\nresearching the context of the question, filtering the returned results, and\ngenerating the final answer. The retrieval system was a ColBERT bi-encoder\narchitecture, which represents a passage with many dense tokens. Retrieval used\na local, compressed index of the FineWeb10-BT collection created with PLAID-X,\nusing a model fine-tuned for multilingual retrieval. Query generation from\ncontext was done with Qwen2.5-7B-Instruct, while filtering was accomplished\nwith m2-bert-80M-8k-retrieval. Up to nine passages were used as context to\ngenerate an answer using Falcon3-10B. This system placed 5th in the LiveRAG\nautomatic evaluation for correctness with a score of 1.07.", "AI": {"tldr": "The HLTCOE LiveRAG submission, based on the GPT-researcher framework and a ColBERT retrieval system, achieved 5th place in LiveRAG with a correctness score of 1.07.", "motivation": "This paper introduces the HLTCOE LiveRAG submission.", "method": "The system used a ColBERT bi-encoder architecture for retrieval, Qwen2.5-7B-Instruct for query generation, m2-bert-80M-8k-retrieval for filtering, and Falcon3-10B for answer generation, leveraging the GPT-researcher framework.", "result": "The system achieved a correctness score of 1.07 in the LiveRAG evaluation.", "conclusion": "The system achieved 5th place in the LiveRAG automatic evaluation for correctness with a score of 1.07."}}
{"id": "2506.21863", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21863", "abs": "https://arxiv.org/abs/2506.21863", "authors": ["Sungjune Park", "Yeongyun Kim", "Se Yeon Kim", "Yong Man Ro"], "title": "Remote Sensing Large Vision-Language Model: Semantic-augmented Multi-level Alignment and Semantic-aware Expert Modeling", "comment": "13 pages including reference pages, 7 tables, and 6 figures", "summary": "Large Vision and Language Models (LVLMs) have shown strong performance across\nvarious vision-language tasks in natural image domains. However, their\napplication to remote sensing (RS) remains underexplored due to significant\ndomain differences in visual appearances, object scales, and semantics. These\ndiscrepancies hider the effective understanding of RS scenes, which contain\nrich, multi-level semantic information spanning from coarse-to-fine levels.\nHence, it limits the direct adaptation of existing LVLMs to RS imagery. To\naddress this gap, we propose a novel LVLM framework tailored for RS\nunderstanding, incorporating two core components: Semantic-augmented\nMulti-level Alignment and Semantic-aware Expert Modeling. First, to align\nmulti-level visual features, we introduce the retrieval-based Semantic\nAugmentation Module which enriches the visual features with relevant semantics\nacross fine-to-coarse levels (e.g., object- and scene-level information). It is\ndesigned to retrieve relevant semantic cues from a RS semantic knowledge\ndatabase, followed by aggregation of semantic cues with user query and\nmulti-level visual features, resulting in semantically enriched representation\nacross multiple levels. Second, for Semantic-aware Expert Modeling, we design\nsemantic experts, where each expert is responsible for processing semantic\nrepresentation at different levels separately. This enables hierarchical\nsemantic understanding from coarse to fine levels. Evaluations across multiple\nRS tasks-including scene classification and VQA, etc.-demonstrate that the\nproposed framework achieves consistent improvements across multiple semantic\nlevels. This highlights its capability and effectiveness in bridging the gap\nbetween general LVLMs and unique demands of RS-specific vision-language\nunderstanding.", "AI": {"tldr": "A new LVLM framework for remote sensing that uses semantic augmentation and expert modeling to improve performance on scene classification and VQA tasks.", "motivation": "The application of Large Vision and Language Models (LVLMs) to remote sensing (RS) remains underexplored due to significant domain differences in visual appearances, object scales, and semantics. These discrepancies hinder the effective understanding of RS scenes, which contain rich, multi-level semantic information spanning from coarse-to-fine levels, limiting the direct adaptation of existing LVLMs to RS imagery.", "method": "A novel LVLM framework tailored for RS understanding, incorporating Semantic-augmented Multi-level Alignment and Semantic-aware Expert Modeling. The Semantic Augmentation Module enriches visual features with relevant semantics across fine-to-coarse levels using a retrieval-based approach. Semantic experts process semantic representation at different levels separately, enabling hierarchical semantic understanding.", "result": "The proposed framework achieves consistent improvements across multiple semantic levels in scene classification and VQA tasks.", "conclusion": "The proposed framework achieves consistent improvements across multiple semantic levels in remote sensing tasks, highlighting its capability and effectiveness in bridging the gap between general LVLMs and unique demands of RS-specific vision-language understanding."}}
{"id": "2506.21574", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21574", "abs": "https://arxiv.org/abs/2506.21574", "authors": ["Yicheng Mao", "Yang Zhao"], "title": "Digital Gatekeepers: Exploring Large Language Model's Role in Immigration Decisions", "comment": null, "summary": "With globalization and increasing immigrant populations, immigration\ndepartments face significant work-loads and the challenge of ensuring fairness\nin decision-making processes. Integrating artificial intelligence offers a\npromising solution to these challenges. This study investigates the potential\nof large language models (LLMs),such as GPT-3.5 and GPT-4, in supporting\nimmigration decision-making. Utilizing a mixed-methods approach,this paper\nconducted discrete choice experiments and in-depth interviews to study LLM\ndecision-making strategies and whether they are fair. Our findings demonstrate\nthat LLMs can align their decision-making with human strategies, emphasizing\nutility maximization and procedural fairness. Meanwhile, this paper also\nreveals that while ChatGPT has safeguards to prevent unintentional\ndiscrimination, it still exhibits stereotypes and biases concerning nationality\nand shows preferences toward privileged group. This dual analysis highlights\nboth the potential and limitations of LLMs in automating and enhancing\nimmigration decisions.", "AI": {"tldr": "This study investigates the potential and limitations of LLMs in supporting immigration decision-making, revealing both benefits and biases.", "motivation": "Immigration departments face significant work-loads and the challenge of ensuring fairness in decision-making processes. Integrating artificial intelligence offers a promising solution to these challenges.", "method": "This paper conducted discrete choice experiments and in-depth interviews to study LLM decision-making strategies.", "result": "LLMs can align their decision-making with human strategies, emphasizing utility maximization and procedural fairness. ChatGPT has safeguards to prevent unintentional discrimination, it still exhibits stereotypes and biases concerning nationality and shows preferences toward privileged group.", "conclusion": "LLMs have the potential to automate and enhance immigration decisions, but they also exhibit stereotypes and biases concerning nationality and show preferences toward privileged groups."}}
{"id": "2412.15194", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2412.15194", "abs": "https://arxiv.org/abs/2412.15194", "authors": ["Qihao Zhao", "Yangyu Huang", "Tengchao Lv", "Lei Cui", "Qinzheng Sun", "Shaoguang Mao", "Xin Zhang", "Ying Xin", "Qiufeng Yin", "Scarlett Li", "Furu Wei"], "title": "MMLU-CF: A Contamination-free Multi-task Language Understanding Benchmark", "comment": null, "summary": "Multiple-choice question (MCQ) datasets like Massive Multitask Language\nUnderstanding (MMLU) are widely used to evaluate the commonsense,\nunderstanding, and problem-solving abilities of large language models (LLMs).\nHowever, the open-source nature of these benchmarks and the broad sources of\ntraining data for LLMs have inevitably led to benchmark contamination,\nresulting in unreliable evaluation results. To alleviate this issue, we propose\na contamination-free and more challenging MCQ benchmark called MMLU-CF. This\nbenchmark reassesses LLMs' understanding of world knowledge by averting both\nunintentional and malicious data leakage. To avoid unintentional data leakage,\nwe source data from a broader domain and design three decontamination rules. To\nprevent malicious data leakage, we divide the benchmark into validation and\ntest sets with similar difficulty and subject distributions. The test set\nremains closed-source to ensure reliable results, while the validation set is\npublicly available to promote transparency and facilitate independent\nverification. Our evaluation of mainstream LLMs reveals that the powerful\nGPT-4o achieves merely a 5-shot score of 73.4% and a 0-shot score of 71.9% on\nthe test set, which indicates the effectiveness of our approach in creating a\nmore rigorous and contamination-free evaluation standard. The GitHub repository\nis available at https://github.com/microsoft/MMLU-CF and the dataset refers to\nhttps://huggingface.co/datasets/microsoft/MMLU-CF.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u66f4\u4e25\u683c\u3001\u65e0\u6c61\u67d3\u7684 MCQ \u57fa\u51c6 MMLU-CF\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u7531\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u8bad\u7ec3\u6570\u636e\u7684\u5f00\u6e90\u6027\u548c\u5e7f\u6cdb\u6765\u6e90\uff0c\u5bfc\u81f4\u57fa\u51c6\u6c61\u67d3\uff0c\u4ece\u800c\u5bfc\u81f4\u4e0d\u53ef\u9760\u7684\u8bc4\u4f30\u7ed3\u679c\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65e0\u6c61\u67d3\u4e14\u66f4\u5177\u6311\u6218\u6027\u7684 MCQ \u57fa\u51c6\uff0c\u79f0\u4e3a MMLU-CF\u3002", "method": "\u901a\u8fc7\u6269\u5927\u6570\u636e\u6765\u6e90\u548c\u8bbe\u8ba1\u4e09\u4e2a\u53bb\u6c61\u89c4\u5219\u6765\u907f\u514d\u610f\u5916\u7684\u6570\u636e\u6cc4\u9732\uff1b\u5c06\u57fa\u51c6\u5206\u4e3a\u9a8c\u8bc1\u96c6\u548c\u6d4b\u8bd5\u96c6\uff0c\u6d4b\u8bd5\u96c6\u4fdd\u6301\u95ed\u6e90\u4ee5\u786e\u4fdd\u7ed3\u679c\u7684\u53ef\u9760\u6027\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u65e0\u6c61\u67d3\u4e14\u66f4\u5177\u6311\u6218\u6027\u7684 MCQ \u57fa\u51c6 MMLU-CF\u3002GPT-4o \u5728\u8be5\u57fa\u51c6\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u6709\u6240\u4e0b\u964d\u3002", "conclusion": "GPT-4o \u5728 MMLU-CF \u6d4b\u8bd5\u96c6\u4e0a\u7684 5-shot \u5f97\u5206\u4e3a 73.4%\uff0c0-shot \u5f97\u5206\u4e3a 71.9%\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u521b\u5efa\u66f4\u4e25\u683c\u548c\u65e0\u6c61\u67d3\u7684\u8bc4\u4f30\u6807\u51c6\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.21976", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.21976", "abs": "https://arxiv.org/abs/2506.21976", "authors": ["Shuhan Tan", "John Lambert", "Hong Jeon", "Sakshum Kulshrestha", "Yijing Bai", "Jing Luo", "Dragomir Anguelov", "Mingxing Tan", "Chiyu Max Jiang"], "title": "SceneDiffuser++: City-Scale Traffic Simulation via a Generative World Model", "comment": "Accepted to CVPR 2025", "summary": "The goal of traffic simulation is to augment a potentially limited amount of\nmanually-driven miles that is available for testing and validation, with a much\nlarger amount of simulated synthetic miles. The culmination of this vision\nwould be a generative simulated city, where given a map of the city and an\nautonomous vehicle (AV) software stack, the simulator can seamlessly simulate\nthe trip from point A to point B by populating the city around the AV and\ncontrolling all aspects of the scene, from animating the dynamic agents (e.g.,\nvehicles, pedestrians) to controlling the traffic light states. We refer to\nthis vision as CitySim, which requires an agglomeration of simulation\ntechnologies: scene generation to populate the initial scene, agent behavior\nmodeling to animate the scene, occlusion reasoning, dynamic scene generation to\nseamlessly spawn and remove agents, and environment simulation for factors such\nas traffic lights. While some key technologies have been separately studied in\nvarious works, others such as dynamic scene generation and environment\nsimulation have received less attention in the research community. We propose\nSceneDiffuser++, the first end-to-end generative world model trained on a\nsingle loss function capable of point A-to-B simulation on a city scale\nintegrating all the requirements above. We demonstrate the city-scale traffic\nsimulation capability of SceneDiffuser++ and study its superior realism under\nlong simulation conditions. We evaluate the simulation quality on an augmented\nversion of the Waymo Open Motion Dataset (WOMD) with larger map regions to\nsupport trip-level simulation.", "AI": {"tldr": "SceneDiffuser++ is proposed as the first end-to-end generative world model for city-scale traffic simulation, showing improved realism in long simulations.", "motivation": "The goal of traffic simulation is to augment a potentially limited amount of manually-driven miles for testing and validation with a much larger amount of simulated synthetic miles.", "method": "SceneDiffuser++, the first end-to-end generative world model trained on a single loss function.", "result": "Demonstrates city-scale traffic simulation capability and superior realism under long simulation conditions on an augmented version of the Waymo Open Motion Dataset (WOMD).", "conclusion": "SceneDiffuser++ is capable of city-scale traffic simulation and demonstrates superior realism under long simulation conditions."}}
{"id": "2506.22372", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22372", "abs": "https://arxiv.org/abs/2506.22372", "authors": ["Maryam Mousavian", "Zahra Abbasiantaeb", "Mohammad Aliannejadi", "Fabio Crestani"], "title": "Towards Fair Rankings: Leveraging LLMs for Gender Bias Detection and Measurement", "comment": "Accepted by ACM SIGIR Conference on Innovative Concepts and Theories\n  in Information Retrieval (ICTIR 2025)", "summary": "The presence of social biases in Natural Language Processing (NLP) and\nInformation Retrieval (IR) systems is an ongoing challenge, which underlines\nthe importance of developing robust approaches to identifying and evaluating\nsuch biases. In this paper, we aim to address this issue by leveraging Large\nLanguage Models (LLMs) to detect and measure gender bias in passage ranking.\nExisting gender fairness metrics rely on lexical- and frequency-based measures,\nleading to various limitations, e.g., missing subtle gender disparities.\nBuilding on our LLM-based gender bias detection method, we introduce a novel\ngender fairness metric, named Class-wise Weighted Exposure (CWEx), aiming to\naddress existing limitations. To measure the effectiveness of our proposed\nmetric and study LLMs' effectiveness in detecting gender bias, we annotate a\nsubset of the MS MARCO Passage Ranking collection and release our new gender\nbias collection, called MSMGenderBias, to foster future research in this area.\nOur extensive experimental results on various ranking models show that our\nproposed metric offers a more detailed evaluation of fairness compared to\nprevious metrics, with improved alignment to human labels (58.77% for\nGrep-BiasIR, and 18.51% for MSMGenderBias, measured using Cohen's Kappa\nagreement), effectively distinguishing gender bias in ranking. By integrating\nLLM-driven bias detection, an improved fairness metric, and gender bias\nannotations for an established dataset, this work provides a more robust\nframework for analyzing and mitigating bias in IR systems.", "AI": {"tldr": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u68c0\u6d4b\u548c\u8861\u91cf\u6bb5\u843d\u6392\u5e8f\u4e2d\u7684\u6027\u522b\u504f\u89c1\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u6027\u522b\u516c\u5e73\u6027\u6307\u6807\uff0c\u540d\u4e3a Class-wise Weighted Exposure (CWEx)\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u5904\u7406 (NLP) \u548c\u4fe1\u606f\u68c0\u7d22 (IR) \u7cfb\u7edf\u4e2d\u5b58\u5728\u7684\u793e\u4f1a\u504f\u89c1\u662f\u4e00\u4e2a\u6301\u7eed\u5b58\u5728\u7684\u6311\u6218\uff0c\u8fd9\u7a81\u663e\u4e86\u5f00\u53d1\u7a33\u5065\u7684\u65b9\u6cd5\u6765\u8bc6\u522b\u548c\u8bc4\u4f30\u6b64\u7c7b\u504f\u89c1\u7684\u91cd\u8981\u6027\u3002\u73b0\u6709\u7684\u6027\u522b\u516c\u5e73\u6027\u6307\u6807\u4f9d\u8d56\u4e8e\u57fa\u4e8e\u8bcd\u6c47\u548c\u9891\u7387\u7684\u5ea6\u91cf\uff0c\u5bfc\u81f4\u5404\u79cd\u9650\u5236\uff0c\u4f8b\u5982\uff0c\u9057\u6f0f\u7ec6\u5fae\u7684\u6027\u522b\u5dee\u5f02\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u6765\u68c0\u6d4b\u548c\u8861\u91cf\u6bb5\u843d\u6392\u5e8f\u4e2d\u7684\u6027\u522b\u504f\u89c1\u3002\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u6027\u522b\u516c\u5e73\u6027\u6307\u6807\uff0c\u540d\u4e3a Class-wise Weighted Exposure (CWEx)\u3002", "result": "\u5728\u5404\u79cd\u6392\u5e8f\u6a21\u578b\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4e4b\u524d\u7684\u6307\u6807\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u6307\u6807\u53ef\u4ee5\u66f4\u8be6\u7ec6\u5730\u8bc4\u4f30\u516c\u5e73\u6027\uff0c\u5e76\u4e14\u4e0e\u4eba\u5de5\u6807\u7b7e\u7684\u5bf9\u9f50\u6027\u66f4\u9ad8\uff08Grep-BiasIR \u4e3a 58.77%\uff0cMSMGenderBias \u4e3a 18.51%\uff0c\u4f7f\u7528 Cohen Kappa \u534f\u8bae\u6d4b\u91cf\uff09\uff0c\u6709\u6548\u5730\u533a\u5206\u4e86\u6392\u5e8f\u4e2d\u7684\u6027\u522b\u504f\u89c1\u3002", "conclusion": "\u96c6\u6210\u4e86 LLM \u9a71\u52a8\u7684\u504f\u5dee\u68c0\u6d4b\u3001\u6539\u8fdb\u7684\u516c\u5e73\u6027\u6307\u6807\u4ee5\u53ca\u9488\u5bf9\u5df2\u5efa\u7acb\u6570\u636e\u96c6\u7684\u6027\u522b\u504f\u5dee\u6ce8\u91ca\uff0c\u8fd9\u9879\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u5f3a\u5927\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u548c\u51cf\u8f7b IR \u7cfb\u7edf\u4e2d\u7684\u504f\u5dee\u3002"}}
{"id": "2506.21866", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21866", "abs": "https://arxiv.org/abs/2506.21866", "authors": ["Yanguang Sun", "Jiexi Yan", "Jianjun Qian", "Chunyan Xu", "Jian Yang", "Lei Luo"], "title": "Dual-Perspective United Transformer for Object Segmentation in Optical Remote Sensing Images", "comment": "Accepted by IJCAI 2025", "summary": "Automatically segmenting objects from optical remote sensing images (ORSIs)\nis an important task. Most existing models are primarily based on either\nconvolutional or Transformer features, each offering distinct advantages.\nExploiting both advantages is valuable research, but it presents several\nchallenges, including the heterogeneity between the two types of features, high\ncomplexity, and large parameters of the model. However, these issues are often\noverlooked in existing the ORSIs methods, causing sub-optimal segmentation. For\nthat, we propose a novel Dual-Perspective United Transformer (DPU-Former) with\na unique structure designed to simultaneously integrate long-range dependencies\nand spatial details. In particular, we design the global-local mixed attention,\nwhich captures diverse information through two perspectives and introduces a\nFourier-space merging strategy to obviate deviations for efficient fusion.\nFurthermore, we present a gated linear feed-forward network to increase the\nexpressive ability. Additionally, we construct a DPU-Former decoder to\naggregate and strength features at different layers. Consequently, the\nDPU-Former model outperforms the state-of-the-art methods on multiple datasets.\nCode: https://github.com/CSYSI/DPU-Former.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u89c6\u89d2\u8054\u5408Transformer (DPU-Former)\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "motivation": "\u4ece\u5149\u5b66\u9065\u611f\u56fe\u50cf(ORSI)\u4e2d\u81ea\u52a8\u5206\u5272\u5bf9\u8c61\u662f\u4e00\u9879\u91cd\u8981\u7684\u4efb\u52a1\u3002\u5927\u591a\u6570\u73b0\u6709\u7684\u6a21\u578b\u4e3b\u8981\u57fa\u4e8e\u5377\u79ef\u6216Transformer\u7279\u5f81\uff0c\u6bcf\u79cd\u7279\u5f81\u90fd\u5177\u6709\u72ec\u7279\u7684\u4f18\u52bf\u3002\u5229\u7528\u8fd9\u4e24\u79cd\u4f18\u52bf\u662f\u6709\u4ef7\u503c\u7684\u7814\u7a76\uff0c\u4f46\u8fd9\u5e26\u6765\u4e86\u4e00\u4e9b\u6311\u6218\uff0c\u5305\u62ec\u4e24\u79cd\u7c7b\u578b\u7279\u5f81\u4e4b\u95f4\u7684\u5f02\u8d28\u6027\u3001\u9ad8\u590d\u6742\u6027\u548c\u6a21\u578b\u7684\u5de8\u5927\u53c2\u6570\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u95ee\u9898\u5728\u73b0\u6709\u7684ORSI\u65b9\u6cd5\u4e2d\u7ecf\u5e38\u88ab\u5ffd\u89c6\uff0c\u5bfc\u81f4\u6b21\u4f18\u5206\u5272\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u89c6\u89d2\u8054\u5408Transformer (DPU-Former)\uff0c\u5b83\u5177\u6709\u72ec\u7279\u7684\u7ed3\u6784\uff0c\u65e8\u5728\u540c\u65f6\u6574\u5408\u957f\u8ddd\u79bb\u4f9d\u8d56\u548c\u7a7a\u95f4\u7ec6\u8282\u3002\u6211\u4eec\u8bbe\u8ba1\u4e86\u5168\u5c40-\u5c40\u90e8\u6df7\u5408\u6ce8\u610f\u529b\uff0c\u5b83\u901a\u8fc7\u4e24\u4e2a\u89c6\u89d2\u6355\u83b7\u591a\u6837\u5316\u7684\u4fe1\u606f\uff0c\u5e76\u5f15\u5165\u4e86\u5085\u91cc\u53f6\u7a7a\u95f4\u5408\u5e76\u7b56\u7565\u6765\u907f\u514d\u504f\u5dee\u4ee5\u5b9e\u73b0\u9ad8\u6548\u878d\u5408\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u95e8\u63a7\u7ebf\u6027\u524d\u9988\u7f51\u7edc\u6765\u589e\u52a0\u8868\u8fbe\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2aDPU-Former\u89e3\u7801\u5668\u6765\u805a\u5408\u548c\u52a0\u5f3a\u4e0d\u540c\u5c42\u7684\u7279\u5f81\u3002", "result": "DPU-Former\u6a21\u578b\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "DPU-Former\u6a21\u578b\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002"}}
{"id": "2506.21575", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21575", "abs": "https://arxiv.org/abs/2506.21575", "authors": ["Josefa Lia Stoisser", "Marc Boubnovski Martell", "Lawrence Phillips", "Casper Hansen", "Julien Fauqueur"], "title": "STRuCT-LLM: Unifying Tabular and Graph Reasoning with Reinforcement Learning for Semantic Parsing", "comment": null, "summary": "We propose STRuCT-LLM, a unified framework for training large language models\n(LLMs) to perform structured reasoning over both relational and\ngraph-structured data. Our approach jointly optimizes Text-to-SQL and\nText-to-Cypher tasks using reinforcement learning (RL) combined with\nChain-of-Thought (CoT) supervision. To support fine-grained optimization in\ngraph-based parsing, we introduce a topology-aware reward function based on\ngraph edit distance. Unlike prior work that treats relational and graph\nformalisms in isolation, STRuCT-LLM leverages shared abstractions between SQL\nand Cypher to induce cross-formalism transfer, enabling SQL training to improve\nCypher performance and vice versa - even without shared schemas. Our largest\nmodel (QwQ-32B) achieves substantial relative improvements across tasks: on\nsemantic parsing, Spider improves by 13.5\\% and Text2Cypher by 73.1\\%. The\nmodel also demonstrates strong zero-shot generalization, improving performance\non downstream tabular QA (TableBench: 8.5\\%) and knowledge graph QA\n(CR-LT-KGQA: 1.7\\%) without any QA-specific supervision. These results\ndemonstrate both the effectiveness of executable queries as scaffolds for\nstructured reasoning and the synergistic benefits of jointly training on SQL\nand Cypher (code available at https://github.com/bouv/STRuCT-LLM).", "AI": {"tldr": "STRuCT-LLM\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u5bf9\u5173\u7cfb\u548c\u56fe\u7ed3\u6784\u5316\u6570\u636e\u6267\u884c\u7ed3\u6784\u5316\u63a8\u7406\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316Text-to-SQL\u548cText-to-Cypher\u4efb\u52a1\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u901a\u5e38\u5b64\u7acb\u5730\u5904\u7406\u5173\u7cfb\u548c\u56fe\u5f62\u5f0f\uff0c\u800cSTRuCT-LLM\u65e8\u5728\u5229\u7528SQL\u548cCypher\u4e4b\u95f4\u7684\u5171\u4eab\u62bd\u8c61\u6765\u8bf1\u5bfc\u8de8\u5f62\u5f0f\u8fc1\u79fb\uff0c\u4ece\u800c\u4f7fSQL\u8bad\u7ec3\u80fd\u591f\u63d0\u9ad8Cypher\u6027\u80fd\uff0c\u53cd\u4e4b\u4ea6\u7136\uff0c\u5373\u4f7f\u6ca1\u6709\u5171\u4eab\u6a21\u5f0f\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u548c\u601d\u7ef4\u94fe\uff08CoT\uff09\u76d1\u7763\uff0c\u5171\u540c\u4f18\u5316Text-to-SQL\u548cText-to-Cypher\u4efb\u52a1\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u57fa\u4e8e\u56fe\u7f16\u8f91\u8ddd\u79bb\u7684\u62d3\u6251\u611f\u77e5\u5956\u52b1\u51fd\u6570\uff0c\u4ee5\u652f\u6301\u57fa\u4e8e\u56fe\u7684\u89e3\u6790\u4e2d\u7684\u7ec6\u7c92\u5ea6\u4f18\u5316\u3002", "result": "STRuCT-LLM\u5728Spider\u4e0a\u63d0\u9ad8\u4e8613.5%\uff0c\u5728Text2Cypher\u4e0a\u63d0\u9ad8\u4e8673.1%\u3002\u5728\u4e0b\u6e38\u8868\u683c\u95ee\u7b54\uff08TableBench\uff09\u548c\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\uff08CR-LT-KGQA\uff09\u4e0a\uff0c\u65e0\u9700\u4efb\u4f55QA\u7279\u5b9a\u7684\u76d1\u7763\uff0c\u5206\u522b\u63d0\u9ad8\u4e868.5%\u548c1.7%\u3002", "conclusion": "STRuCT-LLM\u901a\u8fc7\u8054\u5408\u8bad\u7ec3SQL\u548cCypher\uff0c\u5e76\u5229\u7528\u53ef\u6267\u884c\u67e5\u8be2\u4f5c\u4e3a\u7ed3\u6784\u5316\u63a8\u7406\u7684\u652f\u67b6\uff0c\u5728\u8bed\u4e49\u89e3\u6790\u3001\u8868\u683c\u95ee\u7b54\u548c\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4efb\u52a1\u4e0a\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u7ed3\u6784\u5316\u6570\u636e\u63a8\u7406\u65b9\u9762\u7684\u6709\u6548\u6027\u548c\u534f\u540c\u4f18\u52bf\u3002"}}
{"id": "2501.06184", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2501.06184", "abs": "https://arxiv.org/abs/2501.06184", "authors": ["Yangyu Huang", "Tianyi Gao", "Haoran Xu", "Qihao Zhao", "Yang Song", "Zhipeng Gui", "Tengchao Lv", "Hao Chen", "Lei Cui", "Scarlett Li", "Furu Wei"], "title": "PEACE: Empowering Geologic Map Holistic Understanding with MLLMs", "comment": null, "summary": "Geologic map, as a fundamental diagram in geology science, provides critical\ninsights into the structure and composition of Earth's subsurface and surface.\nThese maps are indispensable in various fields, including disaster detection,\nresource exploration, and civil engineering. Despite their significance,\ncurrent Multimodal Large Language Models (MLLMs) often fall short in geologic\nmap understanding. This gap is primarily due to the challenging nature of\ncartographic generalization, which involves handling high-resolution map,\nmanaging multiple associated components, and requiring domain-specific\nknowledge. To quantify this gap, we construct GeoMap-Bench, the first-ever\nbenchmark for evaluating MLLMs in geologic map understanding, which assesses\nthe full-scale abilities in extracting, referring, grounding, reasoning, and\nanalyzing. To bridge this gap, we introduce GeoMap-Agent, the inaugural agent\ndesigned for geologic map understanding, which features three modules:\nHierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI),\nand Prompt-enhanced Question Answering (PEQA). Inspired by the\ninterdisciplinary collaboration among human scientists, an AI expert group acts\nas consultants, utilizing a diverse tool pool to comprehensively analyze\nquestions. Through comprehensive experiments, GeoMap-Agent achieves an overall\nscore of 0.811 on GeoMap-Bench, significantly outperforming 0.369 of GPT-4o.\nOur work, emPowering gEologic mAp holistiC undErstanding (PEACE) with MLLMs,\npaves the way for advanced AI applications in geology, enhancing the efficiency\nand accuracy of geological investigations.", "AI": {"tldr": "\u8bba\u6587\u6784\u5efa\u4e86GeoMap-Bench\u7528\u4e8e\u8bc4\u4f30MLLM\u5728\u5730\u8d28\u5730\u56fe\u7406\u89e3\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86GeoMap-Agent\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eGeoMap-Agent\u663e\u8457\u4f18\u4e8eGPT-4o\u3002", "motivation": "\u5f53\u524d\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u5730\u8d28\u5730\u56fe\u7406\u89e3\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u4e3a\u5236\u56fe\u6982\u62ec\u5177\u6709\u6311\u6218\u6027\uff0c\u6d89\u53ca\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u5730\u56fe\u3001\u7ba1\u7406\u591a\u4e2a\u76f8\u5173\u7ec4\u4ef6\u4ee5\u53ca\u9700\u8981\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u3002", "method": "\u5f15\u5165\u4e86GeoMap-Agent\uff0c\u5b83\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a\u5206\u5c42\u4fe1\u606f\u63d0\u53d6\uff08HIE\uff09\u3001\u9886\u57df\u77e5\u8bc6\u6ce8\u5165\uff08DKI\uff09\u548c\u63d0\u793a\u589e\u5f3a\u95ee\u7b54\uff08PEQA\uff09\u3002", "result": "GeoMap-Agent\u5728GeoMap-Bench\u4e0a\u53d6\u5f97\u4e860.811\u7684\u603b\u4f53\u8bc4\u5206\uff0c\u663e\u8457\u4f18\u4e8eGPT-4o\u76840.369\u3002", "conclusion": "GeoMap-Agent\u5728GeoMap-Bench\u4e0a\u53d6\u5f97\u4e860.811\u7684\u8bc4\u5206\uff0c\u663e\u8457\u4f18\u4e8eGPT-4o\u76840.369\uff0c\u4e3a\u5730\u8d28\u5b66\u4e2d\u7684\u9ad8\u7ea7AI\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2506.21997", "categories": ["cs.LG", "cs.AI", "I.2.6; I.5.1; G.3"], "pdf": "https://arxiv.org/pdf/2506.21997", "abs": "https://arxiv.org/abs/2506.21997", "authors": ["Rafael Sojo", "Javier D\u00edaz-Rozo", "Concha Bielza", "Pedro Larra\u00f1aga"], "title": "Binned semiparametric Bayesian networks", "comment": null, "summary": "This paper introduces a new type of probabilistic semiparametric model that\ntakes advantage of data binning to reduce the computational cost of kernel\ndensity estimation in nonparametric distributions. Two new conditional\nprobability distributions are developed for the new binned semiparametric\nBayesian networks, the sparse binned kernel density estimation and the Fourier\nkernel density estimation. These two probability distributions address the\ncurse of dimensionality, which typically impacts binned models, by using sparse\ntensors and restricting the number of parent nodes in conditional probability\ncalculations. To evaluate the proposal, we perform a complexity analysis and\nconduct several comparative experiments using synthetic data and datasets from\nthe UCI Machine Learning repository. The experiments include different binning\nrules, parent restrictions, grid sizes, and number of instances to get a\nholistic view of the model's behavior. As a result, our binned semiparametric\nBayesian networks achieve structural learning and log-likelihood estimations\nwith no statistically significant differences compared to the semiparametric\nBayesian networks, but at a much higher speed. Thus, the new binned\nsemiparametric Bayesian networks prove to be a reliable and more efficient\nalternative to their non-binned counterparts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u7bb1\u534a\u53c2\u6570\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5229\u7528\u6570\u636e\u5206\u7bb1\u6765\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u5728\u7ed3\u6784\u5b66\u4e60\u548c\u5bf9\u6570\u4f3c\u7136\u4f30\u8ba1\u65b9\u9762\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6548\u7387\u3002", "motivation": "\u5229\u7528\u6570\u636e\u5206\u7bb1\u6765\u964d\u4f4e\u975e\u53c2\u6570\u5206\u5e03\u4e2d\u6838\u5bc6\u5ea6\u4f30\u8ba1\u7684\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u5f00\u53d1\u4e86\u4e24\u79cd\u65b0\u7684\u6761\u4ef6\u6982\u7387\u5206\u5e03\uff0c\u7a00\u758f\u5206\u7bb1\u6838\u5bc6\u5ea6\u4f30\u8ba1\u548c\u5085\u91cc\u53f6\u6838\u5bc6\u5ea6\u4f30\u8ba1\u3002", "result": "\u5206\u7bb1\u534a\u53c2\u6570\u8d1d\u53f6\u65af\u7f51\u7edc\u5728\u7ed3\u6784\u5b66\u4e60\u548c\u5bf9\u6570\u4f3c\u7136\u4f30\u8ba1\u65b9\u9762\u4e0e\u534a\u53c2\u6570\u8d1d\u53f6\u65af\u7f51\u7edc\u76f8\u6bd4\u6ca1\u6709\u7edf\u8ba1\u5b66\u4e0a\u7684\u663e\u8457\u5dee\u5f02\uff0c\u4f46\u901f\u5ea6\u66f4\u5feb\u3002", "conclusion": "\u65b0\u7684\u5206\u7bb1\u534a\u53c2\u6570\u8d1d\u53f6\u65af\u7f51\u7edc\u5728\u7ed3\u6784\u5b66\u4e60\u548c\u5bf9\u6570\u4f3c\u7136\u4f30\u8ba1\u65b9\u9762\u4e0e\u534a\u53c2\u6570\u8d1d\u53f6\u65af\u7f51\u7edc\u76f8\u6bd4\u6ca1\u6709\u7edf\u8ba1\u5b66\u4e0a\u7684\u663e\u8457\u5dee\u5f02\uff0c\u4f46\u901f\u5ea6\u66f4\u5feb\uff0c\u8bc1\u660e\u4e86\u5b83\u662f\u4e00\u79cd\u53ef\u9760\u4e14\u66f4\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2506.21873", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21873", "abs": "https://arxiv.org/abs/2506.21873", "authors": ["Tzu-Chun Chien", "Chieh-Kai Lin", "Shiang-Feng Tsai", "Ruei-Chi Lai", "Hung-Jen Chen", "Min Sun"], "title": "Grounding-Aware Token Pruning: Recovering from Drastic Performance Drops in Visual Grounding Caused by Pruning", "comment": null, "summary": "Recent Multimodal Large Language Models (MLLMs) have demonstrated strong\nperformance in visual grounding, establishing themselves as a general interface\nfor various vision-language applications. This progress has driven the\ndevelopment of token pruning methods to mitigate the high computational costs\nassociated with processing numerous visual tokens. However, we observe that\npruning significantly weakens the model's grounding ability, leading to\nincorrect predictions and drastic performance degradation. In Referring\nExpression Comprehension (REC), for instance, pruning causes the accuracy of\nLLaVA on the RefCOCO validation set to drop from 56.14% to 15.34%. Our analysis\nidentifies misaligned position IDs after pruning as the primary cause of this\ndegradation, as both the order and value of these IDs are crucial for\nmaintaining performance in grounding tasks. To address this issue, we propose\nGrounding-Aware Token Pruning (GAP), a simple yet effective adjustment to\nposition IDs that recovers REC accuracy back to 51.42%, which is 90% of the\noriginal performance in the without pruning setting, all while requiring no\nadditional training, memory, or computational overhead. Applied to models such\nas Shikra, MiniGPTv2, and the LLaVA series, our method consistently improves\nperformance across various token pruning strategies.", "AI": {"tldr": "The paper introduces Grounding-Aware Token Pruning (GAP) to address the degradation of grounding ability in MLLMs caused by token pruning. GAP adjusts position IDs and recovers performance without additional overhead.", "motivation": "pruning significantly weakens the model's grounding ability, leading to incorrect predictions and drastic performance degradation", "method": "introduce Grounding-Aware Token Pruning (GAP), a simple yet effective adjustment to position IDs", "result": "improve performance across various token pruning strategies on models such as Shikra, MiniGPTv2, and the LLaVA series", "conclusion": "Grounding-Aware Token Pruning (GAP) can recover REC accuracy back to 51.42%, which is 90% of the original performance in the without pruning setting."}}
{"id": "2506.21576", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.21576", "abs": "https://arxiv.org/abs/2506.21576", "authors": ["Hongli Yang", "Yizhou Peng", "Hao Huang", "Sheng Li"], "title": "Adapting Whisper for Parameter-efficient Code-Switching Speech Recognition via Soft Prompt Tuning", "comment": "Accepted by Interspeech 2025", "summary": "Large-scale multilingual ASR models like Whisper excel in high-resource\nsettings but face challenges in low-resource scenarios, such as rare languages\nand code-switching (CS), due to computational costs and catastrophic\nforgetting. We explore Soft Prompt Tuning (SPT), a parameter-efficient method\nto enhance CS ASR while preserving prior knowledge. We evaluate two strategies:\n(1) full fine-tuning (FFT) of both soft prompts and the entire Whisper model,\ndemonstrating improved cross-lingual capabilities compared to traditional\nmethods, and (2) adhering to SPT's original design by freezing model parameters\nand only training soft prompts. Additionally, we introduce SPT4ASR, a\ncombination of different SPT variants. Experiments on the SEAME and ASRU2019\ndatasets show that deep prompt tuning is the most effective SPT approach, and\nour SPT4ASR methods achieve further error reductions in CS ASR, maintaining\nparameter efficiency similar to LoRA, without degrading performance on existing\nlanguages.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86 Soft Prompt Tuning (SPT)\uff0c\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u4ee5\u589e\u5f3a CS ASR\uff0c\u540c\u65f6\u4fdd\u7559\u5148\u524d\u7684\u77e5\u8bc6\u3002", "motivation": "\u50cf Whisper \u8fd9\u6837\u7684\u5927\u89c4\u6a21\u591a\u8bed\u8a00 ASR \u6a21\u578b\u64c5\u957f\u9ad8\u8d44\u6e90\u73af\u5883\uff0c\u4f46\u5728\u4f4e\u8d44\u6e90\u573a\u666f\uff08\u5982\u7a00\u6709\u8bed\u8a00\u548c\u4ee3\u7801\u8f6c\u6362 (CS)\uff09\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u8fd9\u662f\u7531\u4e8e\u8ba1\u7b97\u6210\u672c\u548c\u707e\u96be\u6027\u9057\u5fd8\u3002", "method": "\u63a2\u7d22 Soft Prompt Tuning (SPT)", "result": "\u6df1\u5ea6 prompt tuning \u662f\u6700\u6709\u6548\u7684 SPT \u65b9\u6cd5", "conclusion": "SPT4ASR \u65b9\u6cd5\u5728 CS ASR \u4e2d\u5b9e\u73b0\u4e86\u8fdb\u4e00\u6b65\u7684\u9519\u8bef\u51cf\u5c11\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e LoRA \u76f8\u4f3c\u7684\u53c2\u6570\u6548\u7387\uff0c\u800c\u4e0d\u4f1a\u964d\u4f4e\u73b0\u6709\u8bed\u8a00\u7684\u6027\u80fd\u3002"}}
{"id": "2506.20893", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20893", "abs": "https://arxiv.org/abs/2506.20893", "authors": ["Yian Wang", "Ali Ebrahimpour-Boroojeny", "Hari Sundaram"], "title": "On the Necessity of Output Distribution Reweighting for Effective Class Unlearning", "comment": null, "summary": "In this work, we introduce an output-reweighting unlearning method, RWFT, a\nlightweight technique that erases an entire class from a trained classifier\nwithout full retraining. Forgetting specific classes from trained models is\nessential for enforcing user deletion rights and mitigating harmful or biased\npredictions. The full retraining is costly and existing unlearning methods fail\nto replicate the behavior of the retrained models when predicting samples from\nthe unlearned class. We prove this failure by designing a variant of membership\ninference attacks, MIA-NN that successfully reveals the unlearned class for any\nof these methods. We propose a simple redistribution of the probability mass\nfor the prediction on the samples in the forgotten class which is robust to\nMIA-NN. We also introduce a new metric based on the total variation (TV)\ndistance of the prediction probabilities to quantify residual leakage to\nprevent future methods from susceptibility to the new attack. Through extensive\nexperiments with state of the art baselines in machine unlearning, we show that\nour approach matches the results of full retraining in both metrics used for\nevaluation by prior work and the new metric we propose in this work. Compare to\nstate-of-the-art methods, we gain 2.79% in previously used metrics and 111.45%\nin our new TV-based metric over the best existing method.", "AI": {"tldr": "RWFT, a lightweight unlearning technique, erases an entire class from a trained classifier without full retraining, matching the results of full retraining and outperforming existing methods.", "motivation": "Enforcing user deletion rights and mitigating harmful or biased predictions.", "method": "output-reweighting", "result": "RWFT gains 2.79% in previously used metrics and 111.45% in TV-based metric.", "conclusion": "RWFT matches the results of full retraining and outperforms state-of-the-art unlearning methods."}}
{"id": "2506.22004", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22004", "abs": "https://arxiv.org/abs/2506.22004", "authors": ["Mohammad Sabbaqi", "Riccardo Taormina", "Elvin Isufi"], "title": "GKNet: Graph Kalman Filtering and Model Inference via Model-based Deep Learning", "comment": null, "summary": "Inference tasks with time series over graphs are of importance in\napplications such as urban water networks, economics, and networked\nneuroscience. Addressing these tasks typically relies on identifying a\ncomputationally affordable model that jointly captures the graph-temporal\npatterns of the data. In this work, we propose a graph-aware state space model\nfor graph time series, where both the latent state and the observation equation\nare parametric graph-induced models with a limited number of parameters that\nneed to be learned. More specifically, we consider the state equation to follow\na stochastic partial differential equation driven by noise over the graphs\nedges accounting not only for potential edge uncertainties but also for\nincreasing the degrees of freedom in the latter in a tractable manner. The\ngraph structure conditioning of the noise dispersion allows the state variable\nto deviate from the stochastic process in certain neighborhoods. The\nobservation model is a sampled and graph-filtered version of the state\ncapturing multi-hop neighboring influence. The goal is to learn the parameters\nin both state and observation models from the partially observed data for\ndownstream tasks such as prediction and imputation. The model is inferred first\nthrough a maximum likelihood approach that provides theoretical tractability\nbut is limited in expressivity and scalability. To improve on the latter, we\nuse the state-space formulation to build a principled deep learning\narchitecture that jointly learns the parameters and tracks the state in an\nend-to-end manner in the spirit of Kalman neural networks.", "AI": {"tldr": "This paper proposes a graph-aware state space model for graph time series, using both maximum likelihood and a deep learning approach for inference.", "motivation": "Inference tasks with time series over graphs are important in applications such as urban water networks, economics, and networked neuroscience. These tasks typically rely on identifying a computationally affordable model that jointly captures the graph-temporal patterns of the data.", "method": "The paper proposes a graph-aware state space model where both the latent state and the observation equation are parametric graph-induced models. It uses a stochastic partial differential equation for the state equation and a sampled and graph-filtered version of the state for the observation model.", "result": "The model is inferred through a maximum likelihood approach and a deep learning architecture.", "conclusion": "This paper introduces a graph-aware state space model for graph time series and infers the model using both maximum likelihood and a deep learning architecture."}}
{"id": "2506.21883", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21883", "abs": "https://arxiv.org/abs/2506.21883", "authors": ["Basudha Pal", "Sharif Amit Kamran", "Brendon Lutnick", "Molly Lucas", "Chaitanya Parmar", "Asha Patel Shah", "David Apfel", "Steven Fakharzadeh", "Lloyd Miller", "Gabriela Cula", "Kristopher Standish"], "title": "GRASP-PsONet: Gradient-based Removal of Spurious Patterns for PsOriasis Severity Classification", "comment": null, "summary": "Psoriasis (PsO) severity scoring is important for clinical trials but is\nhindered by inter-rater variability and the burden of in person clinical\nevaluation. Remote imaging using patient captured mobile photos offers\nscalability but introduces challenges, such as variation in lighting,\nbackground, and device quality that are often imperceptible to humans but can\nimpact model performance. These factors, along with inconsistencies in\ndermatologist annotations, reduce the reliability of automated severity\nscoring. We propose a framework to automatically flag problematic training\nimages that introduce spurious correlations which degrade model generalization,\nusing a gradient based interpretability approach. By tracing the gradients of\nmisclassified validation images, we detect training samples where model errors\nalign with inconsistently rated examples or are affected by subtle, nonclinical\nartifacts. We apply this method to a ConvNeXT based weakly supervised model\ndesigned to classify PsO severity from phone images. Removing 8.2% of flagged\nimages improves model AUC-ROC by 5% (85% to 90%) on a held out test set.\nCommonly, multiple annotators and an adjudication process ensure annotation\naccuracy, which is expensive and time consuming. Our method detects training\nimages with annotation inconsistencies, potentially removing the need for\nmanual review. When applied to a subset of training data rated by two\ndermatologists, the method identifies over 90% of cases with inter-rater\ndisagreement by reviewing only the top 30% of samples. This improves automated\nscoring for remote assessments, ensuring robustness despite data collection\nvariability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u6807\u8bb0\u95ee\u9898\u56fe\u50cf\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u94f6\u5c51\u75c5\u4e25\u91cd\u7a0b\u5ea6\u8bc4\u5206\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u51cf\u5c11\u4eba\u5de5\u5ba1\u6838\u7684\u9700\u6c42\u3002", "motivation": "\u94f6\u5c51\u75c5(PsO)\u4e25\u91cd\u7a0b\u5ea6\u8bc4\u5206\u5bf9\u4e8e\u4e34\u5e8a\u8bd5\u9a8c\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u53d7\u5230\u8bc4\u4f30\u8005\u95f4\u53d8\u5f02\u6027\u548c\u4eba\u5de5\u4e34\u5e8a\u8bc4\u4f30\u8d1f\u62c5\u7684\u963b\u788d\u3002\u4f7f\u7528\u60a3\u8005\u62cd\u6444\u7684\u79fb\u52a8\u7167\u7247\u8fdb\u884c\u8fdc\u7a0b\u6210\u50cf\u867d\u7136\u5177\u6709\u53ef\u6269\u5c55\u6027\uff0c\u4f46\u4e5f\u5e26\u6765\u4e86\u4e00\u4e9b\u6311\u6218\uff0c\u4f8b\u5982\u5149\u7167\u3001\u80cc\u666f\u548c\u8bbe\u5907\u8d28\u91cf\u7684\u53d8\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u81ea\u52a8\u6807\u8bb0\u5f15\u5165\u865a\u5047\u76f8\u5173\u6027\u7684\u95ee\u9898\u8bad\u7ec3\u56fe\u50cf\uff0c\u4f7f\u7528ConvNeXT\u6a21\u578b\u8fdb\u884c\u56fe\u50cf\u5206\u7c7b\u3002", "result": "\u79fb\u96648.2%\u7684\u6807\u8bb0\u56fe\u50cf\u540e\uff0c\u6a21\u578b\u5728\u4fdd\u7559\u6d4b\u8bd5\u96c6\u4e0a\u7684AUC-ROC\u63d0\u9ad8\u4e865%(85%\u523090%)\u3002\u8be5\u65b9\u6cd5\u5728\u8bc4\u4f30\u7531\u4e24\u4f4d\u76ae\u80a4\u79d1\u533b\u751f\u8bc4\u5206\u7684\u8bad\u7ec3\u6570\u636e\u5b50\u96c6\u65f6\uff0c\u4ec5\u5ba1\u67e5\u4e86\u524d30%\u7684\u6837\u672c\uff0c\u5c31\u8bc6\u522b\u51fa\u8d85\u8fc790%\u7684\u8bc4\u4f30\u8005\u95f4\u4e0d\u4e00\u81f4\u7684\u75c5\u4f8b\u3002", "conclusion": "\u901a\u8fc7\u6807\u8bb0\u5e76\u79fb\u9664\u6709\u95ee\u9898\u7684\u8bad\u7ec3\u56fe\u50cf\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u51c6\u786e\u6027\uff0c\u5e76\u80fd\u6709\u6548\u68c0\u6d4b\u6807\u6ce8\u4e0d\u4e00\u81f4\u7684\u56fe\u50cf\uff0c\u4ece\u800c\u51cf\u5c11\u5bf9\u624b\u52a8\u5ba1\u6838\u7684\u9700\u6c42\u3002"}}
{"id": "2506.21577", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.21577", "abs": "https://arxiv.org/abs/2506.21577", "authors": ["Hongli Yang", "Sheng Li", "Hao Huang", "Ayiduosi Tuohan", "Yizhou Peng"], "title": "Language-Aware Prompt Tuning for Parameter-Efficient Seamless Language Expansion in Multilingual ASR", "comment": "Accepted by Interspeech 2025", "summary": "Recent advancements in multilingual automatic speech recognition (ASR) have\nbeen driven by large-scale end-to-end models like Whisper. However, challenges\nsuch as language interference and expanding to unseen languages (language\nexpansion) without degrading performance persist. This paper addresses these\nwith three contributions: 1) Entire Soft Prompt Tuning (Entire SPT), which\napplies soft prompts to both the encoder and decoder, enhancing feature\nextraction and decoding; 2) Language-Aware Prompt Tuning (LAPT), which\nleverages cross-lingual similarities to encode shared and language-specific\nfeatures using lightweight prompt matrices; 3) SPT-Whisper, a toolkit that\nintegrates SPT into Whisper and enables efficient continual learning.\nExperiments across three languages from FLEURS demonstrate that Entire SPT and\nLAPT outperform Decoder SPT by 5.0% and 16.0% in language expansion tasks,\nrespectively, providing an efficient solution for dynamic, multilingual ASR\nmodels with minimal computational overhead.", "AI": {"tldr": "This paper introduces Entire SPT and LAPT to improve multilingual ASR, achieving better performance in language expansion tasks with minimal overhead.", "motivation": "Challenges such as language interference and expanding to unseen languages (language expansion) without degrading performance persist in multilingual ASR.", "method": "Entire Soft Prompt Tuning (Entire SPT), Language-Aware Prompt Tuning (LAPT), SPT-Whisper toolkit", "result": "Entire SPT and LAPT outperform Decoder SPT by 5.0% and 16.0% in language expansion tasks.", "conclusion": "Entire SPT and LAPT outperform Decoder SPT by 5.0% and 16.0% in language expansion tasks, respectively, providing an efficient solution for dynamic, multilingual ASR models with minimal computational overhead."}}
{"id": "2506.21545", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2506.21545", "abs": "https://arxiv.org/abs/2506.21545", "authors": ["Yalun Dai", "Yangyu Huang", "Xin Zhang", "Wenshan Wu", "Chong Li", "Wenhui Lu", "Shijie Cao", "Li Dong", "Scarlett Li"], "title": "Data Efficacy for Language Model Training", "comment": null, "summary": "Data is fundamental to the training of language models (LM). Recent research\nhas been dedicated to data efficiency, which aims to maximize performance by\nselecting a minimal or optimal subset of training data. Techniques such as data\nfiltering, sampling, and selection play a crucial role in this area. To\ncomplement it, we define Data Efficacy, which focuses on maximizing performance\nby optimizing the organization of training data and remains relatively\nunderexplored. This work introduces a general paradigm, DELT, for considering\ndata efficacy in LM training, which highlights the significance of training\ndata organization. DELT comprises three components: Data Scoring, Data\nSelection, and Data Ordering. Among these components, we design\nLearnability-Quality Scoring (LQS), as a new instance of Data Scoring, which\nconsiders both the learnability and quality of each data sample from the\ngradient consistency perspective. We also devise Folding Ordering (FO), as a\nnovel instance of Data Ordering, which addresses issues such as model\nforgetting and data distribution bias. Comprehensive experiments validate the\ndata efficacy in LM training, which demonstrates the following: Firstly,\nvarious instances of the proposed DELT enhance LM performance to varying\ndegrees without increasing the data scale and model size. Secondly, among these\ninstances, the combination of our proposed LQS for data scoring and Folding for\ndata ordering achieves the most significant improvement. Lastly, data efficacy\ncan be achieved together with data efficiency by applying data selection.\nTherefore, we believe that data efficacy is a promising foundational area in LM\ntraining.", "AI": {"tldr": "This paper introduces the concept of Data Efficacy, which focuses on maximizing performance by optimizing the organization of training data. They propose a new paradigm called DELT and demonstrate its effectiveness in improving language model training.", "motivation": "Recent research has been dedicated to data efficiency, which aims to maximize performance by selecting a minimal or optimal subset of training data. To complement it, we define Data Efficacy, which focuses on maximizing performance by optimizing the organization of training data and remains relatively underexplored.", "method": "A general paradigm, DELT, for considering data efficacy in LM training, which highlights the significance of training data organization. DELT comprises three components: Data Scoring, Data Selection, and Data Ordering. Learnability-Quality Scoring (LQS), as a new instance of Data Scoring, which considers both the learnability and quality of each data sample from the gradient consistency perspective. Folding Ordering (FO), as a novel instance of Data Ordering, which addresses issues such as model forgetting and data distribution bias.", "result": "various instances of the proposed DELT enhance LM performance to varying degrees without increasing the data scale and model size. the combination of our proposed LQS for data scoring and Folding for data ordering achieves the most significant improvement. data efficacy can be achieved together with data efficiency by applying data selection.", "conclusion": "Data efficacy is a promising foundational area in LM training."}}
{"id": "2506.22008", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22008", "abs": "https://arxiv.org/abs/2506.22008", "authors": ["Alessandro Sestini", "Joakim Bergdahl", "Konrad Tollmar", "Andrew D. Bagdanov", "Linus Gissl\u00e9n"], "title": "TROFI: Trajectory-Ranked Offline Inverse Reinforcement Learning", "comment": "Published at Reinforcement Learning and Video Games Workshop at RLC\n  2025", "summary": "In offline reinforcement learning, agents are trained using only a fixed set\nof stored transitions derived from a source policy. However, this requires that\nthe dataset be labeled by a reward function. In applied settings such as video\ngame development, the availability of the reward function is not always\nguaranteed. This paper proposes Trajectory-Ranked OFfline Inverse reinforcement\nlearning (TROFI), a novel approach to effectively learn a policy offline\nwithout a pre-defined reward function. TROFI first learns a reward function\nfrom human preferences, which it then uses to label the original dataset making\nit usable for training the policy. In contrast to other approaches, our method\ndoes not require optimal trajectories. Through experiments on the D4RL\nbenchmark we demonstrate that TROFI consistently outperforms baselines and\nperforms comparably to using the ground truth reward to learn policies.\nAdditionally, we validate the efficacy of our method in a 3D game environment.\nOur studies of the reward model highlight the importance of the reward function\nin this setting: we show that to ensure the alignment of a value function to\nthe actual future discounted reward, it is fundamental to have a\nwell-engineered and easy-to-learn reward function.", "AI": {"tldr": "TROFI\u662f\u4e00\u79cd\u65b0\u9896\u7684\u79bb\u7ebf\u9006\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b83\u65e0\u9700\u9884\u5b9a\u4e49\u7684\u5956\u52b1\u51fd\u6570\u5373\u53ef\u6709\u6548\u5730\u5b66\u4e60\u7b56\u7565\u3002", "motivation": "\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u667a\u80fd\u4f53\u4ec5\u4f7f\u7528\u6e90\u7b56\u7565\u5bfc\u51fa\u7684\u4e00\u7ec4\u56fa\u5b9a\u7684\u5b58\u50a8\u8f6c\u6362\u8fdb\u884c\u8bad\u7ec3\u3002\u4f46\u662f\uff0c\u8fd9\u8981\u6c42\u6570\u636e\u96c6\u7531\u5956\u52b1\u51fd\u6570\u6807\u8bb0\u3002\u5728\u89c6\u9891\u6e38\u620f\u5f00\u53d1\u7b49\u5e94\u7528\u73af\u5883\u4e2d\uff0c\u5956\u52b1\u51fd\u6570\u7684\u53ef\u7528\u6027\u5e76\u4e0d\u603b\u662f\u5f97\u5230\u4fdd\u8bc1\u3002", "method": "TROFI\u9996\u5148\u4ece\u4eba\u7c7b\u504f\u597d\u4e2d\u5b66\u4e60\u5956\u52b1\u51fd\u6570\uff0c\u7136\u540e\u4f7f\u7528\u5b83\u6765\u6807\u8bb0\u539f\u59cb\u6570\u636e\u96c6\uff0c\u4f7f\u5176\u53ef\u7528\u4e8e\u8bad\u7ec3\u7b56\u7565\u3002", "result": "TROFI\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u57283D\u6e38\u620f\u73af\u5883\u4e2d\u6709\u6548\uff0c\u5e76\u4e14\u4e0e\u4f7f\u7528\u771f\u5b9e\u5956\u52b1\u5b66\u4e60\u7b56\u7565\u76f8\u6bd4\uff0c\u6027\u80fd\u76f8\u5f53\u3002\u7814\u7a76\u8fd8\u5f3a\u8c03\u4e86\u826f\u597d\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\u5bf9\u4e8e\u786e\u4fdd\u4ef7\u503c\u51fd\u6570\u4e0e\u5b9e\u9645\u672a\u6765\u6298\u6263\u5956\u52b1\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "TROFI\u5728D4RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5e76\u4e14\u4e0e\u4f7f\u7528\u5730\u9762\u5b9e\u51b5\u5956\u52b1\u6765\u5b66\u4e60\u7b56\u7565\u7684\u6027\u80fd\u76f8\u5f53\u3002\u5956\u52b1\u6a21\u578b\u7684\u7814\u7a76\u5f3a\u8c03\u4e86\u5956\u52b1\u51fd\u6570\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2506.21585", "categories": ["cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21585", "abs": "https://arxiv.org/abs/2506.21585", "authors": ["Christoph Brosch", "Sian Brumm", "Rolf Krieger", "Jonas Scheffler"], "title": "Evaluation of LLM-based Strategies for the Extraction of Food Product Information from Online Shops", "comment": "Preprint for paper presented at DATA 2025 in Bilbao, Spain. Corrected\n  -2.27 to -1.61 in abstract and +2.27 to +1.61 in discussion. Reference to\n  journal and publication will follow", "summary": "Generative AI and large language models (LLMs) offer significant potential\nfor automating the extraction of structured information from web pages. In this\nwork, we focus on food product pages from online retailers and explore\nschema-constrained extraction approaches to retrieve key product attributes,\nsuch as ingredient lists and nutrition tables. We compare two LLM-based\napproaches, direct extraction and indirect extraction via generated functions,\nevaluating them in terms of accuracy, efficiency, and cost on a curated dataset\nof 3,000 food product pages from three different online shops. Our results show\nthat although the indirect approach achieves slightly lower accuracy (96.48\\%,\n$-1.61\\%$ compared to direct extraction), it reduces the number of required LLM\ncalls by 95.82\\%, leading to substantial efficiency gains and lower operational\ncosts. These findings suggest that indirect extraction approaches can provide\nscalable and cost-effective solutions for large-scale information extraction\ntasks from template-based web pages using LLMs.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528 LLM \u4ece\u98df\u54c1\u4ea7\u54c1\u9875\u9762\u63d0\u53d6\u7ed3\u6784\u5316\u4fe1\u606f\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u95f4\u63a5\u63d0\u53d6\u5728\u7565\u5fae\u964d\u4f4e\u51c6\u786e\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u5e76\u964d\u4f4e\u4e86\u6210\u672c\u3002", "motivation": "\u751f\u6210\u4eba\u5de5\u667a\u80fd\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4e3a\u81ea\u52a8\u5316\u4ece\u7f51\u9875\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u4fe1\u606f\u63d0\u4f9b\u4e86\u5de8\u5927\u7684\u6f5c\u529b\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4e13\u6ce8\u4e8e\u5728\u7ebf\u96f6\u552e\u5546\u7684\u98df\u54c1\u4ea7\u54c1\u9875\u9762\uff0c\u5e76\u63a2\u7d22\u6a21\u5f0f\u7ea6\u675f\u7684\u63d0\u53d6\u65b9\u6cd5\u6765\u68c0\u7d22\u5173\u952e\u4ea7\u54c1\u5c5e\u6027\uff0c\u4f8b\u5982\u6210\u5206\u5217\u8868\u548c\u8425\u517b\u8868\u3002", "method": "\u6bd4\u8f83\u4e86\u4e24\u79cd\u57fa\u4e8e LLM \u7684\u65b9\u6cd5\uff1a\u76f4\u63a5\u63d0\u53d6\u548c\u901a\u8fc7\u751f\u6210\u51fd\u6570\u8fdb\u884c\u7684\u95f4\u63a5\u63d0\u53d6\uff0c\u5e76\u5728\u5305\u542b\u6765\u81ea\u4e09\u4e2a\u4e0d\u540c\u5728\u7ebf\u5546\u5e97\u7684 3,000 \u4e2a\u98df\u54c1\u4ea7\u54c1\u9875\u9762\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u4ece\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u6210\u672c\u65b9\u9762\u5bf9\u5b83\u4eec\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u867d\u7136\u95f4\u63a5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7565\u4f4e\u7684\u51c6\u786e\u7387\uff0896.48%\uff0c\u76f8\u6bd4\u76f4\u63a5\u63d0\u53d6\u4f4e -1.61%\uff09\uff0c\u4f46\u5b83\u5c06\u6240\u9700\u7684 LLM \u8c03\u7528\u6b21\u6570\u51cf\u5c11\u4e86 95.82%\uff0c\u4ece\u800c\u663e\u7740\u63d0\u9ad8\u4e86\u6548\u7387\u5e76\u964d\u4f4e\u4e86\u8fd0\u8425\u6210\u672c\u3002", "conclusion": "\u95f4\u63a5\u63d0\u53d6\u65b9\u6cd5\u53ef\u4ee5\u4e3a\u4f7f\u7528 LLM \u4ece\u57fa\u4e8e\u6a21\u677f\u7684\u7f51\u9875\u4e2d\u8fdb\u884c\u5927\u89c4\u6a21\u4fe1\u606f\u63d0\u53d6\u4efb\u52a1\u63d0\u4f9b\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21885", "categories": ["cs.CV", "cs.MM", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.21885", "abs": "https://arxiv.org/abs/2506.21885", "authors": ["Chuheng Wei", "Ziye Qin", "Ziyan Zhang", "Guoyuan Wu", "Matthew J. Barth"], "title": "Integrating Multi-Modal Sensors: A Review of Fusion Techniques for Intelligent Vehicles", "comment": "Accepted by IEEE IV 2025", "summary": "Multi-sensor fusion plays a critical role in enhancing perception for\nautonomous driving, overcoming individual sensor limitations, and enabling\ncomprehensive environmental understanding. This paper first formalizes\nmulti-sensor fusion strategies into data-level, feature-level, and\ndecision-level categories and then provides a systematic review of deep\nlearning-based methods corresponding to each strategy. We present key\nmulti-modal datasets and discuss their applicability in addressing real-world\nchallenges, particularly in adverse weather conditions and complex urban\nenvironments. Additionally, we explore emerging trends, including the\nintegration of Vision-Language Models (VLMs), Large Language Models (LLMs), and\nthe role of sensor fusion in end-to-end autonomous driving, highlighting its\npotential to enhance system adaptability and robustness. Our work offers\nvaluable insights into current methods and future directions for multi-sensor\nfusion in autonomous driving.", "AI": {"tldr": "systematic review of deep learning-based multi-sensor fusion strategies and datasets in autonomous driving", "motivation": "enhancing perception for autonomous driving, overcoming individual sensor limitations, and enabling comprehensive environmental understanding", "method": "systematic review of deep learning-based methods corresponding to each strategy (data-level, feature-level, and decision-level)", "result": "We present key multi-modal datasets and discuss their applicability in addressing real-world challenges, particularly in adverse weather conditions and complex urban environments.", "conclusion": "This work offers valuable insights into current methods and future directions for multi-sensor fusion in autonomous driving."}}
{"id": "2506.21578", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21578", "abs": "https://arxiv.org/abs/2506.21578", "authors": ["Andrew Maranh\u00e3o Ventura D'addario"], "title": "HealthQA-BR: A System-Wide Benchmark Reveals Critical Knowledge Gaps in Large Language Models", "comment": null, "summary": "The evaluation of Large Language Models (LLMs) in healthcare has been\ndominated by physician-centric, English-language benchmarks, creating a\ndangerous illusion of competence that ignores the interprofessional nature of\npatient care. To provide a more holistic and realistic assessment, we introduce\nHealthQA-BR, the first large-scale, system-wide benchmark for\nPortuguese-speaking healthcare. Comprising 5,632 questions from Brazil's\nnational licensing and residency exams, it uniquely assesses knowledge not only\nin medicine and its specialties but also in nursing, dentistry, psychology,\nsocial work, and other allied health professions. We conducted a rigorous\nzero-shot evaluation of over 20 leading LLMs. Our results reveal that while\nstate-of-the-art models like GPT 4.1 achieve high overall accuracy (86.6%),\nthis top-line score masks alarming, previously unmeasured deficiencies. A\ngranular analysis shows performance plummets from near-perfect in specialties\nlike Ophthalmology (98.7%) to barely passing in Neurosurgery (60.0%) and, most\nnotably, Social Work (68.4%). This \"spiky\" knowledge profile is a systemic\nissue observed across all models, demonstrating that high-level scores are\ninsufficient for safety validation. By publicly releasing HealthQA-BR and our\nevaluation suite, we provide a crucial tool to move beyond single-score\nevaluations and toward a more honest, granular audit of AI readiness for the\nentire healthcare team.", "AI": {"tldr": "HealthQA-BR\u662f\u4e00\u4e2a\u65b0\u7684\u8461\u8404\u7259\u8bed\u533b\u7597\u4fdd\u5065\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u533b\u7597\u4e13\u4e1a\u4e2d\u77e5\u8bc6\u638c\u63e1\u7a0b\u5ea6\u7684\u5de8\u5927\u5dee\u5f02\uff0c\u5f3a\u8c03\u4e86\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u76ee\u524d\u5bf9\u533b\u7597\u4fdd\u5065\u9886\u57df\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8bc4\u4f30\u4e3b\u8981\u96c6\u4e2d\u5728\u4ee5\u533b\u751f\u4e3a\u4e2d\u5fc3\u7684\u82f1\u8bed\u57fa\u51c6\u4e0a\uff0c\u5ffd\u7565\u4e86\u60a3\u8005\u62a4\u7406\u7684\u8de8\u4e13\u4e1a\u6027\u8d28\uff0c\u4ece\u800c\u9020\u6210\u4e86\u4e00\u79cd\u5371\u9669\u7684\u80fd\u529b\u9519\u89c9\u3002", "method": "\u5f15\u5165HealthQA-BR\uff0c\u8fd9\u662f\u4e00\u4e2a\u9488\u5bf9\u8461\u8404\u7259\u8bed\u533b\u7597\u4fdd\u5065\u7684\u5927\u89c4\u6a21\u3001\u7cfb\u7edf\u8303\u56f4\u7684\u57fa\u51c6\uff0c\u5305\u542b\u6765\u81ea\u5df4\u897f\u56fd\u5bb6\u8bb8\u53ef\u548c\u4f4f\u9662\u533b\u5e08\u8003\u8bd5\u76845,632\u4e2a\u95ee\u9898\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1GPT 4.1\u7b49\u5148\u8fdb\u6a21\u578b\u53d6\u5f97\u4e86\u5f88\u9ad8\u7684\u603b\u4f53\u51c6\u786e\u7387\uff0886.6%\uff09\uff0c\u4f46\u8fd9\u4e00\u8868\u9762\u5206\u6570\u63a9\u76d6\u4e86\u4ee4\u4eba\u9707\u60ca\u7684\u3001\u5148\u524d\u672a\u6d4b\u91cf\u7684\u7f3a\u9677\u3002\u8bf8\u5982\u773c\u79d1\uff0898.7%\uff09\u7b49\u4e13\u4e1a\u7684\u8868\u73b0\u63a5\u8fd1\u5b8c\u7f8e\uff0c\u800c\u795e\u7ecf\u5916\u79d1\uff0860.0%\uff09\u548c\u793e\u4f1a\u5de5\u4f5c\uff0868.4%\uff09\u7684\u8868\u73b0\u52c9\u5f3a\u53ca\u683c\u3002\u6240\u6709\u6a21\u578b\u90fd\u5b58\u5728\u8fd9\u79cd\u201c\u53c2\u5dee\u4e0d\u9f50\u201d\u7684\u77e5\u8bc6\u7ed3\u6784\u95ee\u9898\uff0c\u8868\u660e\u9ad8\u6c34\u5e73\u5206\u6570\u4e0d\u8db3\u4ee5\u8fdb\u884c\u5b89\u5168\u9a8c\u8bc1\u3002", "conclusion": "\u516c\u5f00HealthQA-BR\u548c\u8bc4\u4f30\u5957\u4ef6\uff0c\u4ee5\u4fc3\u8fdb\u5bf9AI\u5728\u6574\u4e2a\u533b\u7597\u56e2\u961f\u4e2d\u51c6\u5907\u60c5\u51b5\u7684\u7ec6\u81f4\u8bc4\u4f30\u3002"}}
{"id": "2506.22036", "categories": ["cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.22036", "abs": "https://arxiv.org/abs/2506.22036", "authors": ["Ying Zhang", "Yu Zhao", "Xuhui Sui", "Baohang Zhou", "Xiangrui Cai", "Li Shen", "Xiaojie Yuan", "Dacheng Tao"], "title": "Hyper-modal Imputation Diffusion Embedding with Dual-Distillation for Federated Multimodal Knowledge Graph Completion", "comment": "Submitted to the IEEE for possible publication", "summary": "With the increasing multimodal knowledge privatization requirements,\nmultimodal knowledge graphs in different institutes are usually decentralized,\nlacking of effective collaboration system with both stronger reasoning ability\nand transmission safety guarantees. In this paper, we propose the Federated\nMultimodal Knowledge Graph Completion (FedMKGC) task, aiming at training over\nfederated MKGs for better predicting the missing links in clients without\nsharing sensitive knowledge. We propose a framework named MMFeD3-HidE for\naddressing multimodal uncertain unavailability and multimodal client\nheterogeneity challenges of FedMKGC. (1) Inside the clients, our proposed\nHyper-modal Imputation Diffusion Embedding model (HidE) recovers the complete\nmultimodal distributions from incomplete entity embeddings constrained by\navailable modalities. (2) Among clients, our proposed Multimodal FeDerated Dual\nDistillation (MMFeD3) transfers knowledge mutually between clients and the\nserver with logit and feature distillation to improve both global convergence\nand semantic consistency. We propose a FedMKGC benchmark for a comprehensive\nevaluation, consisting of a general FedMKGC backbone named MMFedE, datasets\nwith heterogeneous multimodal information, and three groups of constructed\nbaselines. Experiments conducted on our benchmark validate the effectiveness,\nsemantic consistency, and convergence robustness of MMFeD3-HidE.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u8054\u90a6\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8865\u5168 (FedMKGC) \u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a MMFeD3-HidE \u7684\u6846\u67b6\u6765\u89e3\u51b3\u8be5\u4efb\u52a1\u4e2d\u7684\u591a\u6a21\u6001\u4e0d\u786e\u5b9a\u4e0d\u53ef\u7528\u6027\u548c\u591a\u6a21\u6001\u5ba2\u6237\u7aef\u5f02\u6784\u6027\u6311\u6218\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u77e5\u8bc6\u79c1\u6709\u5316\u9700\u6c42\u7684\u589e\u52a0\uff0c\u4e0d\u540c\u673a\u6784\u4e2d\u7684\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u901a\u5e38\u662f\u5206\u6563\u7684\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u534f\u4f5c\u7cfb\u7edf\uff0c\u96be\u4ee5\u4fdd\u8bc1\u66f4\u5f3a\u7684\u63a8\u7406\u80fd\u529b\u548c\u4f20\u8f93\u5b89\u5168\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u540d\u4e3a MMFeD3-HidE \u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u542b Hyper-modal Imputation Diffusion Embedding model (HidE) \u548c Multimodal FeDerated Dual Distillation (MMFeD3)\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a FedMKGC \u57fa\u51c6\uff0c\u7528\u4e8e\u7efc\u5408\u8bc4\u4f30\uff0c\u5305\u62ec\u4e00\u4e2a\u540d\u4e3a MMFedE \u7684\u901a\u7528 FedMKGC \u4e3b\u5e72\u3001\u5177\u6709\u5f02\u6784\u591a\u6a21\u6001\u4fe1\u606f\u7684\u6570\u636e\u96c6\u548c\u4e09\u7ec4\u6784\u5efa\u7684\u57fa\u7ebf\u3002", "conclusion": "MMFeD3-HidE\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u3001\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u6536\u655b\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.21596", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.21596", "abs": "https://arxiv.org/abs/2506.21596", "authors": ["Hessa A. Alawwad", "Anas Zafar", "Areej Alhothali", "Usman Naseem", "Ali Alkhathlan", "Amani Jamal"], "title": "Evaluating Multimodal Large Language Models on Educational Textbook Question Answering", "comment": "7 Pages", "summary": "Multimodal large language models (MLLMs) have recently achieved significant\nsuccess in vision--language tasks. However, their capacity to reason over\ncomplex, long lessons and intricate educational diagrams that cannot be\nrepresented as a single natural image remains largely untested. In this work,\nwe present the first evaluation of state-of-the-art MLLMs on the textbook\nquestion answering (TQA) task using the CK12-QA dataset. We assess the\nperformance of recent vision-language models, including LLaVA and LLaMA\n3.2-Vision, across various input configurations. Additionally, we introduce a\nlightweight multimodal retrieval-augmented generation (RAG) pipeline that\nintegrates both paragraphs and diagrams from the lesson into the prompt. Our\nresults demonstrate the influence of retrieved educational context on model\naccuracy and reasoning, while also revealing current limitations in handling\nquestion-context relationships and the potential for noise, pointing to key\ndirections for future research in multimodal AI-driven learning.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86 MLLM \u5728\u6559\u79d1\u4e66\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u6a21\u6001 RAG \u6d41\u7a0b\u6765\u63d0\u9ad8\u51c6\u786e\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u68c0\u7d22\u5230\u7684\u6559\u80b2\u80cc\u666f\u4f1a\u5f71\u54cd\u6a21\u578b\u51c6\u786e\u6027\uff0c\u4f46\u6a21\u578b\u5728\u5904\u7406\u95ee\u9898-\u4e0a\u4e0b\u6587\u5173\u7cfb\u65b9\u9762\u4ecd\u5b58\u5728\u5c40\u9650\u6027\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u6700\u8fd1\u5728\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\u3002\u7136\u800c\uff0c\u5b83\u4eec\u5bf9\u590d\u6742\u7684\u3001\u957f\u7bc7\u7684\u8bfe\u7a0b\u548c\u65e0\u6cd5\u8868\u793a\u4e3a\u5355\u4e2a\u81ea\u7136\u56fe\u50cf\u7684\u590d\u6742\u6559\u80b2\u56fe\u8fdb\u884c\u63a8\u7406\u7684\u80fd\u529b\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u672a\u7ecf\u6d4b\u8bd5\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210 (RAG) \u6d41\u7a0b\uff0c\u8be5\u6d41\u7a0b\u5c06\u8bfe\u7a0b\u4e2d\u7684\u6bb5\u843d\u548c\u56fe\u8868\u96c6\u6210\u5230\u63d0\u793a\u4e2d\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u68c0\u7d22\u5230\u7684\u6559\u80b2\u80cc\u666f\u5bf9\u6a21\u578b\u51c6\u786e\u6027\u548c\u63a8\u7406\u7684\u5f71\u54cd\uff0c\u540c\u65f6\u4e5f\u63ed\u793a\u4e86\u5f53\u524d\u5728\u5904\u7406\u95ee\u9898-\u4e0a\u4e0b\u6587\u5173\u7cfb\u65b9\u9762\u7684\u5c40\u9650\u6027\u4ee5\u53ca\u566a\u58f0\u7684\u53ef\u80fd\u6027\u3002", "conclusion": "\u8bc4\u4f30\u4e86\u6700\u5148\u8fdb\u7684 MLLM \u5728\u6559\u79d1\u4e66\u95ee\u9898\u56de\u7b54 (TQA) \u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u5904\u7406\u95ee\u9898-\u4e0a\u4e0b\u6587\u5173\u7cfb\u65b9\u9762\u7684\u5c40\u9650\u6027\u4ee5\u53ca\u566a\u58f0\u7684\u53ef\u80fd\u6027\uff0c\u6307\u51fa\u4e86\u591a\u6a21\u6001\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u5b66\u4e60\u672a\u6765\u7814\u7a76\u7684\u5173\u952e\u65b9\u5411\u3002"}}
{"id": "2506.21891", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21891", "abs": "https://arxiv.org/abs/2506.21891", "authors": ["Umihiro Kamoto", "Tatsuya Ishibashi", "Noriyuki Kugo"], "title": "DIVE: Deep-search Iterative Video Exploration A Technical Report for the CVRR Challenge at CVPR 2025", "comment": null, "summary": "In this report, we present the winning solution that achieved the 1st place\nin the Complex Video Reasoning & Robustness Evaluation Challenge 2025. This\nchallenge evaluates the ability to generate accurate natural language answers\nto questions about diverse, real-world video clips. It uses the Complex Video\nReasoning and Robustness Evaluation Suite (CVRR-ES) benchmark, which consists\nof 214 unique videos and 2,400 question-answer pairs spanning 11 categories.\nOur method, DIVE (Deep-search Iterative Video Exploration), adopts an iterative\nreasoning approach, in which each input question is semantically decomposed and\nsolved through stepwise reasoning and progressive inference. This enables our\nsystem to provide highly accurate and contextually appropriate answers to even\nthe most complex queries. Applied to the CVRR-ES benchmark, our approach\nachieves 81.44% accuracy on the test set, securing the top position among all\nparticipants. This report details our methodology and provides a comprehensive\nanalysis of the experimental results, demonstrating the effectiveness of our\niterative reasoning framework in achieving robust video question answering. The\ncode is available at https://github.com/PanasonicConnect/DIVE", "AI": {"tldr": "DIVE\u65b9\u6cd5\u5728\u590d\u6742\u89c6\u9891\u63a8\u7406\u4e0e\u9c81\u68d2\u6027\u8bc4\u4f30\u6311\u6218\u8d5b2025\u4e2d\u83b7\u5f97\u7b2c\u4e00\u540d\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u8fed\u4ee3\u63a8\u7406\u548c\u9010\u6b65\u5206\u89e3\u95ee\u9898\u6765\u751f\u6210\u5173\u4e8e\u89c6\u9891\u7247\u6bb5\u7684\u51c6\u786e\u7b54\u6848\uff0c\u5e76\u5728CVRR-ES\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e8681.44%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u4e3a\u4e86\u5728\u590d\u6742\u89c6\u9891\u63a8\u7406\u4e0e\u9c81\u68d2\u6027\u8bc4\u4f30\u6311\u6218\u8d5b2025\u4e2d\u751f\u6210\u5173\u4e8e\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u7247\u6bb5\u95ee\u9898\u7684\u51c6\u786e\u81ea\u7136\u8bed\u8a00\u7b54\u6848\u3002", "method": "DIVE\uff08\u6df1\u5ea6\u641c\u7d22\u8fed\u4ee3\u89c6\u9891\u63a2\u7d22\uff09\u91c7\u7528\u8fed\u4ee3\u63a8\u7406\u65b9\u6cd5\uff0c\u5176\u4e2d\u6bcf\u4e2a\u8f93\u5165\u95ee\u9898\u90fd\u88ab\u8bed\u4e49\u5206\u89e3\uff0c\u5e76\u901a\u8fc7\u9010\u6b65\u63a8\u7406\u548c\u6e10\u8fdb\u63a8\u7406\u6765\u89e3\u51b3\u3002", "result": "DIVE\u65b9\u6cd5\u5728CVRR-ES\u57fa\u51c6\u6d4b\u8bd5\u7684\u6d4b\u8bd5\u96c6\u4e0a\u5b9e\u73b0\u4e8681.44%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "DIVE\u65b9\u6cd5\u5728CVRR-ES\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e8681.44%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5728\u6240\u6709\u53c2\u4e0e\u8005\u4e2d\u6392\u540d\u7b2c\u4e00\u3002"}}
{"id": "2506.21580", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.21580", "abs": "https://arxiv.org/abs/2506.21580", "authors": ["Dana Alsagheer", "Yang Lu", "Abdulrahman Kamal", "Omar Kamal", "Mohammad Kamal", "Nada Mansour", "Cosmo Yang Wu", "Rambiba Karanjai", "Sen Li", "Weidong Shi"], "title": "From General Reasoning to Domain Expertise: Uncovering the Limits of Generalization in Large Language Models", "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nremarkable capabilities in various domains. However, effective decision-making\nrelies heavily on strong reasoning abilities. Reasoning is the foundation for\ndecision-making, providing the analytical and logical framework to make sound\nchoices. Reasoning involves analyzing information, drawing inferences, and\nreaching conclusions based on logic or evidence. Decision-making builds on this\nfoundation by applying the insights from reasoning to select the best course of\naction among alternatives. Together, these processes create a continuous cycle\nof thought and action aimed at achieving goals effectively. As AI technology\nevolves, there is a growing trend to train LLMs to excel in general reasoning.\nThis study explores how the general reasoning capabilities of LLMs connect to\ntheir performance in domain-specific reasoning tasks.", "AI": {"tldr": "This study explores the connection between general reasoning capabilities of LLMs and their performance in domain-specific reasoning tasks, motivated by the importance of reasoning in decision-making and the growing trend of training LLMs for general reasoning.", "motivation": "Effective decision-making relies heavily on strong reasoning abilities. Reasoning is the foundation for decision-making, providing the analytical and logical framework to make sound choices. As AI technology evolves, there is a growing trend to train LLMs to excel in general reasoning.", "method": "This study explores how the general reasoning capabilities of LLMs connect to their performance in domain-specific reasoning tasks.", "result": "How the general reasoning capabilities of LLMs connect to their performance in domain-specific reasoning tasks.", "conclusion": "The study explores how the general reasoning capabilities of LLMs connect to their performance in domain-specific reasoning tasks."}}
{"id": "2506.22039", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22039", "abs": "https://arxiv.org/abs/2506.22039", "authors": ["Lu Han", "Yu Liu", "Qiwen Deng", "Jian Jiang", "Yinbo Sun", "Zhe Yu", "Binfeng Wang", "Xingyu Lu", "Lintao Ma", "Han-Jia Ye", "De-Chuan Zhan"], "title": "UniCA: Adapting Time Series Foundation Model to General Covariate-Aware Forecasting", "comment": null, "summary": "Time Series Foundation Models (TSFMs) have achieved remarkable success\nthrough large-scale pretraining. However, their design primarily targets\nreal-valued series, limiting their ability to handle general forecasting tasks\ninvolving diverse and often heterogeneous covariates--such as categorical\nvariables and multimodal data (e.g., images, text)--which are typically\ntask-specific and difficult to leverage during pretraining. To address this\ngap, we propose Unified Covariate Adaptation (UniCA), a framework to bridge\nTSFMs with general covariate-aware forecasting. UniCA first performs covariate\nhomogenization to transform heterogeneous covariates into high-level\nhomogeneous series representations and then fuses them via a unified\nattention-based fusion mechanism. UniCA is compatible and universal for\nadaptation with both homogeneous and heterogeneous covariates, incorporating\nextra covariate information while preserving the generalization ability of\nTSFMs.Extensive experiments on multiple unimodal and multimodal covariate-aware\nforecasting benchmarks demonstrate the superiority of UniCA, highlighting the\npromise of covariate-aware TSFM adaptation in real-world forecasting scenarios.\nCodes are released on https://github.com/hanlu-nju/UniCA.", "AI": {"tldr": "UniCA\u5f25\u5408\u4e86TSFM\u4e0e\u901a\u7528\u534f\u53d8\u91cf\u611f\u77e5\u9884\u6d4b\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5728\u591a\u4e2a\u9884\u6d4b\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08TSFM\uff09\u901a\u8fc7\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u529f\u3002\u7136\u800c\uff0c\u5b83\u4eec\u7684\u8bbe\u8ba1\u4e3b\u8981\u9488\u5bf9\u5b9e\u503c\u5e8f\u5217\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5904\u7406\u6d89\u53ca\u5404\u79cd\u4e14\u901a\u5e38\u662f\u5f02\u6784\u534f\u53d8\u91cf\uff08\u4f8b\u5982\uff0c\u5206\u7c7b\u53d8\u91cf\u548c\u591a\u6a21\u6001\u6570\u636e\uff08\u4f8b\u5982\uff0c\u56fe\u50cf\u3001\u6587\u672c\uff09\uff09\u7684\u901a\u7528\u9884\u6d4b\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u8fd9\u4e9b\u534f\u53d8\u91cf\u901a\u5e38\u662f\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\uff0c\u5e76\u4e14\u5728\u9884\u8bad\u7ec3\u671f\u95f4\u96be\u4ee5\u5229\u7528\u3002", "method": "UniCA\u9996\u5148\u6267\u884c\u534f\u53d8\u91cf\u540c\u8d28\u5316\uff0c\u5c06\u5f02\u6784\u534f\u53d8\u91cf\u8f6c\u6362\u4e3a\u9ad8\u7ea7\u540c\u8d28\u5e8f\u5217\u8868\u793a\uff0c\u7136\u540e\u901a\u8fc7\u7edf\u4e00\u7684\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u878d\u5408\u673a\u5236\u878d\u5408\u5b83\u4eec\u3002", "result": "UniCA\u4e0e\u540c\u6784\u548c\u5f02\u6784\u534f\u53d8\u91cf\u7684\u81ea\u9002\u5e94\u517c\u5bb9\u4e14\u901a\u7528\uff0c\u5728\u4fdd\u7559TSFM\u7684\u6cdb\u5316\u80fd\u529b\u7684\u540c\u65f6\uff0c\u7ed3\u5408\u4e86\u989d\u5916\u7684\u534f\u53d8\u91cf\u4fe1\u606f\u3002", "conclusion": "UniCA\u5728\u591a\u4e2a\u5355\u6a21\u548c\u591a\u6a21\u534f\u53d8\u91cf\u611f\u77e5\u9884\u6d4b\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\uff0c\u7a81\u51fa\u4e86\u534f\u53d8\u91cf\u611f\u77e5TSFM\u5728\u5b9e\u9645\u9884\u6d4b\u573a\u666f\u4e2d\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2506.21597", "categories": ["cs.CL", "cs.AI", "cs.IR", "I.2.7"], "pdf": "https://arxiv.org/pdf/2506.21597", "abs": "https://arxiv.org/abs/2506.21597", "authors": ["Brandon Colelough", "Davis Bartels", "Dina Demner-Fushman"], "title": "Overview of the ClinIQLink 2025 Shared Task on Medical Question-Answering", "comment": "10 pages, 5 figures", "summary": "In this paper, we present an overview of ClinIQLink, a shared task,\ncollocated with the 24th BioNLP workshop at ACL 2025, designed to stress-test\nlarge language models (LLMs) on medically-oriented question answering aimed at\nthe level of a General Practitioner. The challenge supplies 4,978\nexpert-verified, medical source-grounded question-answer pairs that cover seven\nformats: true/false, multiple choice, unordered list, short answer,\nshort-inverse, multi-hop, and multi-hop-inverse. Participating systems, bundled\nin Docker or Apptainer images, are executed on the CodaBench platform or the\nUniversity of Maryland's Zaratan cluster. An automated harness (Task 1) scores\nclosed-ended items by exact match and open-ended items with a three-tier\nembedding metric. A subsequent physician panel (Task 2) audits the top model\nresponses.", "AI": {"tldr": "ClinIQLink is a new benchmark to test LLMs on medical question answering using a diverse dataset and both automated and expert evaluations.", "motivation": "The paper introduces ClinIQLink, a shared task to evaluate the capabilities of large language models (LLMs) in medically-oriented question answering.", "method": "The challenge uses 4,978 expert-verified question-answer pairs across seven formats. Participating systems are evaluated using exact match and embedding metrics, followed by physician review.", "result": "The ClinIQLink challenge includes a dataset of 4,978 medical question-answer pairs, an automated evaluation harness, and a physician panel for auditing model responses.", "conclusion": "ClinIQLink provides a challenging benchmark for LLMs in medical question answering, with a focus on general practitioner-level knowledge and diverse question formats. The evaluation includes automated scoring and expert review."}}
{"id": "2506.21892", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21892", "abs": "https://arxiv.org/abs/2506.21892", "authors": ["Adam Goodge", "Xun Xu", "Bryan Hooi", "Wee Siong Ng", "Jingyi Liao", "Yongyi Su", "Xulei Yang"], "title": "SODA: Out-of-Distribution Detection in Domain-Shifted Point Clouds via Neighborhood Propagation", "comment": null, "summary": "As point cloud data increases in prevalence in a variety of applications, the\nability to detect out-of-distribution (OOD) point cloud objects becomes\ncritical for ensuring model safety and reliability. However, this problem\nremains under-explored in existing research. Inspired by success in the image\ndomain, we propose to exploit advances in 3D vision-language models (3D VLMs)\nfor OOD detection in point cloud objects. However, a major challenge is that\npoint cloud datasets used to pre-train 3D VLMs are drastically smaller in size\nand object diversity than their image-based counterparts. Critically, they\noften contain exclusively computer-designed synthetic objects. This leads to a\nsubstantial domain shift when the model is transferred to practical tasks\ninvolving real objects scanned from the physical environment. In this paper,\nour empirical experiments show that synthetic-to-real domain shift\nsignificantly degrades the alignment of point cloud with their associated text\nembeddings in the 3D VLM latent space, hindering downstream performance. To\naddress this, we propose a novel methodology called SODA which improves the\ndetection of OOD point clouds through a neighborhood-based score propagation\nscheme. SODA is inference-based, requires no additional model training, and\nachieves state-of-the-art performance over existing approaches across datasets\nand problem settings.", "AI": {"tldr": "This paper introduces SODA, a new method for detecting OOD point clouds using 3D VLMs. It addresses the domain shift problem and improves performance without retraining the model.", "motivation": "Detecting out-of-distribution (OOD) point cloud objects is critical for model safety and reliability but remains under-explored. Existing 3D VLMs suffer from a synthetic-to-real domain shift due to limited size and diversity of pre-training datasets.", "method": "The paper exploits advances in 3D vision-language models (3D VLMs) for OOD detection and introduces a neighborhood-based score propagation scheme called SODA.", "result": "Empirical experiments demonstrate that synthetic-to-real domain shift degrades point cloud and text embedding alignment in 3D VLMs. SODA achieves state-of-the-art performance over existing approaches.", "conclusion": "The paper proposes SODA, a novel inference-based methodology that improves OOD point cloud detection through neighborhood-based score propagation, achieving state-of-the-art performance without additional training."}}
{"id": "2506.21582", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.21582", "abs": "https://arxiv.org/abs/2506.21582", "authors": ["Sam Yu-Te Lee", "Chengyang Ji", "Shicheng Wen", "Lifu Huang", "Dongyi Liu", "Kwan-Liu Ma"], "title": "VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation of Text Analytics with Intelligent Agents", "comment": null, "summary": "Text analytics has traditionally required specialized knowledge in Natural\nLanguage Processing (NLP) or text analysis, which presents a barrier for\nentry-level analysts. Recent advances in large language models (LLMs) have\nchanged the landscape of NLP by enabling more accessible and automated text\nanalysis (e.g., topic detection, summarization, information extraction, etc.).\nWe introduce VIDEE, a system that supports entry-level data analysts to conduct\nadvanced text analytics with intelligent agents. VIDEE instantiates a\nhuman-agent collaroration workflow consisting of three stages: (1)\nDecomposition, which incorporates a human-in-the-loop Monte-Carlo Tree Search\nalgorithm to support generative reasoning with human feedback, (2) Execution,\nwhich generates an executable text analytics pipeline, and (3) Evaluation,\nwhich integrates LLM-based evaluation and visualizations to support user\nvalidation of execution results. We conduct two quantitative experiments to\nevaluate VIDEE's effectiveness and analyze common agent errors. A user study\ninvolving participants with varying levels of NLP and text analytics experience\n-- from none to expert -- demonstrates the system's usability and reveals\ndistinct user behavior patterns. The findings identify design implications for\nhuman-agent collaboration, validate the practical utility of VIDEE for\nnon-expert users, and inform future improvements to intelligent text analytics\nsystems.", "AI": {"tldr": "Introduces VIDEE, a system that helps entry-level data analysts perform text analytics with intelligent agents, using a three-stage human-agent collaboration workflow. Experiments and a user study validate its effectiveness and usability.", "motivation": "Text analytics has traditionally required specialized knowledge in Natural Language Processing (NLP) or text analysis, which presents a barrier for entry-level analysts. Recent advances in large language models (LLMs) have changed the landscape of NLP by enabling more accessible and automated text analysis.", "method": "VIDEE instantiates a human-agent collaraboration workflow consisting of three stages: (1) Decomposition, which incorporates a human-in-the-loop Monte-Carlo Tree Search algorithm to support generative reasoning with human feedback, (2) Execution, which generates an executable text analytics pipeline, and (3) Evaluation, which integrates LLM-based evaluation and visualizations to support user validation of execution results.", "result": "We conduct two quantitative experiments to evaluate VIDEE's effectiveness and analyze common agent errors. A user study involving participants with varying levels of NLP and text analytics experience demonstrates the system's usability and reveals distinct user behavior patterns.", "conclusion": "The findings identify design implications for human-agent collaboration, validate the practical utility of VIDEE for non-expert users, and inform future improvements to intelligent text analytics systems."}}
{"id": "2506.22049", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22049", "abs": "https://arxiv.org/abs/2506.22049", "authors": ["Tianhao Chen", "Xin Xu", "Zijing Liu", "Pengxiang Li", "Xinyuan Song", "Ajay Kumar Jaiswal", "Fan Zhang", "Jishan Hu", "Yang Wang", "Hao Chen", "Shizhe Diao", "Shiwei Liu", "Yu Li", "Yin Lu", "Can Yang"], "title": "GPAS: Accelerating Convergence of LLM Pretraining via Gradient-Preserving Activation Scaling", "comment": null, "summary": "Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series,\npredominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While\nbeing stable during pretraining and scalable to large model sizes, Pre-LN\nsuffers from an exponential growth in activation variance across layers,\ncausing the residual path to dominate over sub-layer outputs and limiting the\nlearning capacity of deeper layers. To mitigate this issue, we propose\nGradient-Preserving Activation Scaling (GPAS), a simple technique that can be\nused in combination with existing approaches. GPAS works by scaling down the\nintermediate activations while keeping their gradients unchanged. This leaves\ninformation in the activations intact, and avoids the gradient vanishing\nproblem associated with gradient downscaling. Extensive experiments across\nvarious model sizes from 71M to 1B show that GPAS achieves consistent\nperformance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows\npromise in improving alternative architectures such as Sandwich-LN and\nDeepNorm, demonstrating its versatility and potential for improving training\ndynamics in a wide range of settings.", "AI": {"tldr": "GPAS, a gradient-preserving activation scaling technique, mitigates activation variance issues in Pre-LN Transformers, leading to performance gains and showing promise in other architectures.", "motivation": "Pre-LN suffers from an exponential growth in activation variance across layers, causing the residual path to dominate over sub-layer outputs and limiting the learning capacity of deeper layers.", "method": "Gradient-Preserving Activation Scaling (GPAS), a technique that scales down intermediate activations while keeping their gradients unchanged.", "result": "GPAS achieves consistent performance gains across various model sizes from 71M to 1B. GPAS also shows promise in improving alternative architectures such as Sandwich-LN and DeepNorm.", "conclusion": "GPAS achieves consistent performance gains across various model sizes and shows promise in improving alternative architectures."}}
{"id": "2506.21600", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.21600", "abs": "https://arxiv.org/abs/2506.21600", "authors": ["Chang Liu", "Hongkai Chen", "Yujun Cai", "Hang Wu", "Qingwen Ye", "Ming-Hsuan Yang", "Yiwei Wang"], "title": "Structured Attention Matters to Multimodal LLMs in Document Understanding", "comment": null, "summary": "Document understanding remains a significant challenge for multimodal large\nlanguage models (MLLMs). While previous research has primarily focused on\nlocating evidence pages through precise multimodal queries, our work\ninvestigates a fundamental yet overlooked aspect: how input format influences\ndocument comprehension performance. Through systematic analysis, we discover\nthat raw OCR text often impairs rather than improves MLLMs' performance, which\nis a counterintuitive finding we attribute to attention dispersion and\nstructure loss. To further substantiate our hypothesis, we propose a novel\nstructure-preserving approach that encodes document elements using the LaTex\nparadigm, maintaining the hierarchical organization and spatial relationships\ncritical for comprehension. Our attention analysis reveals that structured text\ninduces structured attention patterns on both textual and visual content,\ndirecting models to focus on semantically meaningful regions while reducing\nattention waste. This approach significantly enhances MLLMs' document question\nanswering performance across diverse document types without requiring\narchitectural modifications or additional training.", "AI": {"tldr": "\u539f\u59cbOCR\u6587\u672c\u4f1a\u635f\u5bb3MLLM\u7684\u6587\u6863\u7406\u89e3\u6027\u80fd\u3002\u901a\u8fc7LaTex\u8303\u5f0f\u4fdd\u6301\u6587\u6863\u7ed3\u6784\u53ef\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b(MLLM)\u7684\u6587\u6863\u7406\u89e3\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u4e4b\u524d\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u4e8e\u901a\u8fc7\u7cbe\u786e\u7684\u591a\u6a21\u6001\u67e5\u8be2\u6765\u5b9a\u4f4d\u8bc1\u636e\u9875\u9762\uff0c\u800c\u6211\u4eec\u7684\u5de5\u4f5c\u8c03\u67e5\u4e86\u4e00\u4e2a\u57fa\u672c\u4f46\u88ab\u5ffd\u89c6\u7684\u65b9\u9762\uff1a\u8f93\u5165\u683c\u5f0f\u5982\u4f55\u5f71\u54cd\u6587\u6863\u7406\u89e3\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ed3\u6784\u4fdd\u6301\u65b9\u6cd5\uff0c\u4f7f\u7528LaTex\u8303\u5f0f\u7f16\u7801\u6587\u6863\u5143\u7d20\uff0c\u4fdd\u6301\u4e86\u5bf9\u7406\u89e3\u81f3\u5173\u91cd\u8981\u7684\u5c42\u6b21\u7ed3\u6784\u548c\u7a7a\u95f4\u5173\u7cfb\u3002", "result": "\u7ed3\u6784\u5316\u6587\u672c\u5728\u6587\u672c\u548c\u89c6\u89c9\u5185\u5bb9\u4e0a\u8bf1\u5bfc\u7ed3\u6784\u5316\u7684\u6ce8\u610f\u6a21\u5f0f\uff0c\u5f15\u5bfc\u6a21\u578b\u5173\u6ce8\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u7684\u533a\u57df\uff0c\u540c\u65f6\u51cf\u5c11\u6ce8\u610f\u6d6a\u8d39\u3002\u4e0e\u539f\u59cbOCR\u6587\u672c\u76f8\u6bd4\uff0c\u7ed3\u6784\u5316\u6587\u672c\u663e\u8457\u63d0\u9ad8\u4e86MLLM\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u4fdd\u6301\u7ed3\u6784\u7684LaTex\u8303\u5f0f\u7f16\u7801\u6587\u6863\u5143\u7d20\uff0c\u663e\u8457\u63d0\u9ad8\u4e86MLLM\u5728\u5404\u79cd\u6587\u6863\u7c7b\u578b\u4e0a\u7684\u6587\u6863\u95ee\u7b54\u6027\u80fd\uff0c\u65e0\u9700\u67b6\u6784\u4fee\u6539\u6216\u989d\u5916\u8bad\u7ec3\u3002"}}
{"id": "2506.21895", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21895", "abs": "https://arxiv.org/abs/2506.21895", "authors": ["Fangling Jiang", "Qi Li", "Weining Wang", "Gang Wang", "Bing Liu", "Zhenan Sun"], "title": "Exploring Task-Solving Paradigm for Generalized Cross-Domain Face Anti-Spoofing via Reinforcement Fine-Tuning", "comment": null, "summary": "Recently the emergence of novel presentation attacks has drawn increasing\nattention to face anti-spoofing. However, existing methods tend to memorize\ndata patterns from the training set, resulting in poor generalization to\nunknown attack types across different scenarios and limited interpretability.\nTo address these challenges, this paper presents a reinforcement\nfine-tuning-based face anti-spoofing method that stimulates the capabilities of\nmultimodal large language models to think and learn how to solve the\nanti-spoofing task itself, rather than relying on the memorization of\nauthenticity patterns. We design verifiable class consistent reward and\nreasoning consistent reward, and employ a GRPO-based optimization strategy to\nguide the model in exploring reasoning policies from multiple perspectives to\nmaximize expected rewards. As a result, through iterative trial-and-error\nlearning while retaining only high-reward trajectories, the model distills\nhighly generalizable decision-making rules from the extensive solution space to\neffectively address cross-domain face anti-spoofing tasks. Extensive\nexperimental results demonstrate that our method achieves state-of-the-art\ncross-domain generalization performance. It generalizes well to diverse unknown\nattack types in unseen target domains while providing interpretable reasoning\nfor its authenticity decisions without requiring labor-intensive textual\nannotations for training.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5fae\u8c03\u7684\u4eba\u8138\u9632\u6b3a\u9a97\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6fc0\u53d1\u4e86\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u601d\u8003\u548c\u5b66\u4e60\u5982\u4f55\u89e3\u51b3\u9632\u6b3a\u9a97\u4efb\u52a1\u672c\u8eab\u7684\u80fd\u529b\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u4e8e\u5bf9\u771f\u5b9e\u6027\u6a21\u5f0f\u7684\u8bb0\u5fc6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u503e\u5411\u4e8e\u8bb0\u5fc6\u8bad\u7ec3\u96c6\u4e2d\u7684\u6570\u636e\u6a21\u5f0f\uff0c\u5bfc\u81f4\u5bf9\u4e0d\u540c\u573a\u666f\u4e2d\u672a\u77e5\u653b\u51fb\u7c7b\u578b\u7684\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\uff0c\u4e14\u53ef\u89e3\u91ca\u6027\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5fae\u8c03\u7684\u4eba\u8138\u9632\u6b3a\u9a97\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6fc0\u53d1\u4e86\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u601d\u8003\u548c\u5b66\u4e60\u5982\u4f55\u89e3\u51b3\u9632\u6b3a\u9a97\u4efb\u52a1\u672c\u8eab\u7684\u80fd\u529b\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u4e8e\u5bf9\u771f\u5b9e\u6027\u6a21\u5f0f\u7684\u8bb0\u5fc6\u3002\u8bbe\u8ba1\u4e86\u53ef\u9a8c\u8bc1\u7684\u7c7b\u4e00\u81f4\u5956\u52b1\u548c\u63a8\u7406\u4e00\u81f4\u5956\u52b1\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e GRPO \u7684\u4f18\u5316\u7b56\u7565\u6765\u6307\u5bfc\u6a21\u578b\u4ece\u591a\u4e2a\u89d2\u5ea6\u63a2\u7d22\u63a8\u7406\u7b56\u7565\uff0c\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u63d0\u9ad8\u9884\u671f\u5956\u52b1\u3002", "result": "\u901a\u8fc7\u8fed\u4ee3\u8bd5\u9519\u5b66\u4e60\uff0c\u540c\u65f6\u4ec5\u4fdd\u7559\u9ad8\u5956\u52b1\u8f68\u8ff9\uff0c\u8be5\u6a21\u578b\u4ece\u5e7f\u6cdb\u7684\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\u4e2d\u63d0\u70bc\u51fa\u9ad8\u5ea6\u901a\u7528\u7684\u51b3\u7b56\u89c4\u5219\uff0c\u4ee5\u6709\u6548\u89e3\u51b3\u8de8\u57df\u4eba\u8138\u9632\u6b3a\u9a97\u4efb\u52a1\u3002\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8de8\u57df\u6cdb\u5316\u6027\u80fd\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\uff0c \u63a8\u5e7f\u5230\u770b\u4e0d\u89c1\u7684\u76ee\u6807\u57df\u4e2d\u5404\u79cd\u672a\u77e5\u7684\u653b\u51fb\u7c7b\u578b\uff0c\u540c\u65f6\u4e3a\u5176\u771f\u5b9e\u6027\u51b3\u7b56\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u63a8\u7406\uff0c\u800c\u65e0\u9700\u8fdb\u884c\u52b3\u52a8\u5bc6\u96c6\u578b\u6587\u672c\u6ce8\u91ca\u8fdb\u884c\u8bad\u7ec3\u3002"}}
{"id": "2506.21583", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21583", "abs": "https://arxiv.org/abs/2506.21583", "authors": ["Muhammad Ahmad", "Muhammad Waqas", "Ameer Hamza", "Ildar Batyrshin", "Grigori Sidorov"], "title": "Hope Speech Detection in code-mixed Roman Urdu tweets: A Positive Turn in Natural Language Processing", "comment": null, "summary": "Hope is a positive emotional state involving the expectation of favorable\nfuture outcomes, while hope speech refers to communication that promotes\noptimism, resilience, and support, particularly in adverse contexts. Although\nhope speech detection has gained attention in Natural Language Processing\n(NLP), existing research mainly focuses on high-resource languages and\nstandardized scripts, often overlooking informal and underrepresented forms\nsuch as Roman Urdu. To the best of our knowledge, this is the first study to\naddress hope speech detection in code-mixed Roman Urdu by introducing a\ncarefully annotated dataset, thereby filling a critical gap in inclusive NLP\nresearch for low-resource, informal language varieties. This study makes four\nkey contributions: (1) it introduces the first multi-class annotated dataset\nfor Roman Urdu hope speech, comprising Generalized Hope, Realistic Hope,\nUnrealistic Hope, and Not Hope categories; (2) it explores the psychological\nfoundations of hope and analyzes its linguistic patterns in code-mixed Roman\nUrdu to inform dataset development; (3) it proposes a custom attention-based\ntransformer model optimized for the syntactic and semantic variability of Roman\nUrdu, evaluated using 5-fold cross-validation; and (4) it verifies the\nstatistical significance of performance gains using a t-test. The proposed\nmodel, XLM-R, achieves the best performance with a cross-validation score of\n0.78, outperforming the baseline SVM (0.75) and BiLSTM (0.76), with gains of 4%\nand 2.63% respectively.", "AI": {"tldr": "This paper introduces a new dataset for hope speech detection in Roman Urdu and proposes a custom transformer model that outperforms baseline models.", "motivation": "existing research mainly focuses on high-resource languages and standardized scripts, often overlooking informal and underrepresented forms such as Roman Urdu. To the best of our knowledge, this is the first study to address hope speech detection in code-mixed Roman Urdu by introducing a carefully annotated dataset, thereby filling a critical gap in inclusive NLP research for low-resource, informal language varieties.", "method": "a custom attention-based transformer model optimized for the syntactic and semantic variability of Roman Urdu, evaluated using 5-fold cross-validation", "result": "it introduces the first multi-class annotated dataset for Roman Urdu hope speech, comprising Generalized Hope, Realistic Hope, Unrealistic Hope, and Not Hope categories; (2) it explores the psychological foundations of hope and analyzes its linguistic patterns in code-mixed Roman Urdu to inform dataset development; (3) it proposes a custom attention-based transformer model optimized for the syntactic and semantic variability of Roman Urdu, evaluated using 5-fold cross-validation; and (4) it verifies the statistical significance of performance gains using a t-test", "conclusion": "The proposed model, XLM-R, achieves the best performance with a cross-validation score of 0.78, outperforming the baseline SVM (0.75) and BiLSTM (0.76), with gains of 4% and 2.63% respectively."}}
{"id": "2506.22055", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22055", "abs": "https://arxiv.org/abs/2506.22055", "authors": ["Mehul Gautam"], "title": "crypto price prediction using lstm+xgboost", "comment": null, "summary": "The volatility and complex dynamics of cryptocurrency markets present unique\nchallenges for accurate price forecasting. This research proposes a hybrid deep\nlearning and machine learning model that integrates Long Short-Term Memory\n(LSTM) networks and Extreme Gradient Boosting (XGBoost) for cryptocurrency\nprice prediction. The LSTM component captures temporal dependencies in\nhistorical price data, while XGBoost enhances prediction by modeling nonlinear\nrelationships with auxiliary features such as sentiment scores and\nmacroeconomic indicators. The model is evaluated on historical datasets of\nBitcoin, Ethereum, Dogecoin, and Litecoin, incorporating both global and\nlocalized exchange data. Comparative analysis using Mean Absolute Percentage\nError (MAPE) and Min-Max Normalized Root Mean Square Error (MinMax RMSE)\ndemonstrates that the LSTM+XGBoost hybrid consistently outperforms standalone\nmodels and traditional forecasting methods. This study underscores the\npotential of hybrid architectures in financial forecasting and provides\ninsights into model adaptability across different cryptocurrencies and market\ncontexts.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd LSTM+XGBoost \u6df7\u5408\u6a21\u578b\uff0c\u7528\u4e8e\u52a0\u5bc6\u8d27\u5e01\u4ef7\u683c\u9884\u6d4b\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u6a21\u578b\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u52a0\u5bc6\u8d27\u5e01\u5e02\u573a\u6ce2\u52a8\u6027\u548c\u590d\u6742\u52a8\u6001\u5bf9\u51c6\u786e\u4ef7\u683c\u9884\u6d4b\u63d0\u51fa\u4e86\u72ec\u7279\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc (LSTM) \u548c\u6781\u7aef\u68af\u5ea6\u63d0\u5347 (XGBoost) \u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002", "result": "LSTM+XGBoost \u6df7\u5408\u6a21\u578b\u5728\u6bd4\u7279\u5e01\u3001\u4ee5\u592a\u574a\u3001\u72d7\u72d7\u5e01\u548c\u83b1\u7279\u5e01\u7684\u5386\u53f2\u6570\u636e\u96c6\u4e0a\uff0c\u59cb\u7ec8\u4f18\u4e8e\u72ec\u7acb\u6a21\u578b\u548c\u4f20\u7edf\u9884\u6d4b\u65b9\u6cd5\u3002", "conclusion": "\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u52a0\u5bc6\u8d27\u5e01\u4ef7\u683c\u9884\u6d4b\u4e2d\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u5728\u91d1\u878d\u9884\u6d4b\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.21612", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.21612", "abs": "https://arxiv.org/abs/2506.21612", "authors": ["Xiaobin Ren", "Xinyu Zhu", "Kaiqi Zhao"], "title": "AdaptGOT: A Pre-trained Model for Adaptive Contextual POI Representation Learning", "comment": null, "summary": "Currently, considerable strides have been achieved in Point-of-Interest (POI)\nembedding methodologies, driven by the emergence of novel POI tasks like\nrecommendation and classification. Despite the success of task-specific,\nend-to-end models in POI embedding, several challenges remain. These include\nthe need for more effective multi-context sampling strategies, insufficient\nexploration of multiple POI contexts, limited versatility, and inadequate\ngeneralization. To address these issues, we propose the AdaptGOT model, which\nintegrates both the (Adapt)ive representation learning technique and the\nGeographical-Co-Occurrence-Text (GOT) representation with a particular emphasis\non Geographical location, Co-Occurrence and Textual information. The AdaptGOT\nmodel comprises three key components: (1) contextual neighborhood generation,\nwhich integrates advanced mixed sampling techniques such as KNN, density-based,\nimportance-based, and category-aware strategies to capture complex contextual\nneighborhoods; (2) an advanced GOT representation enhanced by an attention\nmechanism, designed to derive high-quality, customized representations and\nefficiently capture complex interrelations between POIs; and (3) the MoE-based\nadaptive encoder-decoder architecture, which ensures topological consistency\nand enriches contextual representation by minimizing Jensen-Shannon divergence\nacross varying contexts. Experiments on two real-world datasets and multiple\nPOI tasks substantiate the superior performance of the proposed AdaptGOT model.", "AI": {"tldr": "The paper introduces AdaptGOT, a novel POI embedding model addressing limitations in multi-context sampling, context exploration, versatility, and generalization. It uses adaptive representation learning and Geographical-Co-Occurrence-Text representation with attention mechanisms and a MoE-based architecture. Experiments show AdaptGOT outperforms existing methods on real-world datasets.", "motivation": "Despite the success of task-specific, end-to-end models in POI embedding, several challenges remain. These include the need for more effective multi-context sampling strategies, insufficient exploration of multiple POI contexts, limited versatility, and inadequate generalization. To address these issues", "method": "We propose the AdaptGOT model, which integrates both the (Adapt)ive representation learning technique and the Geographical-Co-Occurrence-Text (GOT) representation with a particular emphasis on Geographical location, Co-Occurrence and Textual information. The AdaptGOT model comprises three key components: (1) contextual neighborhood generation, which integrates advanced mixed sampling techniques such as KNN, density-based, importance-based, and category-aware strategies to capture complex contextual neighborhoods; (2) an advanced GOT representation enhanced by an attention mechanism, designed to derive high-quality, customized representations and efficiently capture complex interrelations between POIs; and (3) the MoE-based adaptive encoder-decoder architecture, which ensures topological consistency and enriches contextual representation by minimizing Jensen-Shannon divergence across varying contexts.", "result": "superior performance of the proposed AdaptGOT model", "conclusion": "Experiments on two real-world datasets and multiple POI tasks substantiate the superior performance of the proposed AdaptGOT model."}}
{"id": "2506.21903", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21903", "abs": "https://arxiv.org/abs/2506.21903", "authors": ["Dipayan Biswas", "Shishir Shah", "Jaspal Subhlok"], "title": "Visual Content Detection in Educational Videos with Transfer Learning and Dataset Enrichment", "comment": "This is an extended version of a paper accepted to MIPR 2025", "summary": "Video is transforming education with online courses and recorded lectures\nsupplementing and replacing classroom teaching. Recent research has focused on\nenhancing information retrieval for video lectures with advanced navigation,\nsearchability, summarization, as well as question answering chatbots. Visual\nelements like tables, charts, and illustrations are central to comprehension,\nretention, and data presentation in lecture videos, yet their full potential\nfor improving access to video content remains underutilized. A major factor is\nthat accurate automatic detection of visual elements in a lecture video is\nchallenging; reasons include i) most visual elements, such as charts, graphs,\ntables, and illustrations, are artificially created and lack any standard\nstructure, and ii) coherent visual objects may lack clear boundaries and may be\ncomposed of connected text and visual components. Despite advancements in deep\nlearning based object detection, current models do not yield satisfactory\nperformance due to the unique nature of visual content in lectures and scarcity\nof annotated datasets. This paper reports on a transfer learning approach for\ndetecting visual elements in lecture video frames. A suite of state of the art\nobject detection models were evaluated for their performance on lecture video\ndatasets. YOLO emerged as the most promising model for this task. Subsequently\nYOLO was optimized for lecture video object detection with training on multiple\nbenchmark datasets and deploying a semi-supervised auto labeling strategy.\nResults evaluate the success of this approach, also in developing a general\nsolution to the problem of object detection in lecture videos. Paper\ncontributions include a publicly released benchmark of annotated lecture video\nframes, along with the source code to facilitate future research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fc1\u79fb\u5b66\u4e60\u7684YOLO\u6a21\u578b\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u8bb2\u5ea7\u89c6\u9891\u4e2d\u7684\u89c6\u89c9\u5143\u7d20\uff0c\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u5e26\u6ce8\u91ca\u7684\u6570\u636e\u96c6\u548c\u6e90\u4ee3\u7801\u3002", "motivation": "\u8bb2\u5ea7\u89c6\u9891\u4e2d\u7684\u89c6\u89c9\u5143\u7d20\u5bf9\u4e8e\u7406\u89e3\u3001\u8bb0\u5fc6\u548c\u6570\u636e\u5448\u73b0\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u5728\u6539\u5584\u89c6\u9891\u5185\u5bb9\u8bbf\u95ee\u65b9\u9762\u7684\u6f5c\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u5229\u7528\u3002\u51c6\u786e\u81ea\u52a8\u68c0\u6d4b\u8bb2\u5ea7\u89c6\u9891\u4e2d\u7684\u89c6\u89c9\u5143\u7d20\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u89c6\u89c9\u5143\u7d20\u7f3a\u4e4f\u6807\u51c6\u7ed3\u6784\uff0c\u5e76\u4e14\u7f3a\u4e4f\u5e26\u6ce8\u91ca\u7684\u6570\u636e\u96c6\u3002", "method": "\u4f7f\u7528\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f18\u5316YOLO\u6a21\u578b\uff0c\u901a\u8fc7\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u5e76\u90e8\u7f72\u534a\u76d1\u7763\u81ea\u52a8\u6807\u6ce8\u7b56\u7565\u3002", "result": "YOLO\u6a21\u578b\u5728\u8be5\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6700\u6709\u5e0c\u671b\u7684\u7ed3\u679c\u3002\u8bba\u6587\u8fd8\u53d1\u5e03\u4e86\u4e00\u4e2a\u5e26\u6ce8\u91ca\u7684\u8bb2\u5ea7\u89c6\u9891\u5e27\u7684\u516c\u5f00\u57fa\u51c6\uff0c\u4ee5\u53ca\u6e90\u4ee3\u7801\uff0c\u4ee5\u4fc3\u8fdb\u672a\u6765\u7684\u7814\u7a76\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u8bb2\u5ea7\u89c6\u9891\u5e27\u4e2d\u7684\u89c6\u89c9\u5143\u7d20\u3002\u901a\u8fc7\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u5e76\u90e8\u7f72\u534a\u76d1\u7763\u81ea\u52a8\u6807\u6ce8\u7b56\u7565\uff0c\u4f18\u5316\u4e86YOLO\u6a21\u578b\u4ee5\u7528\u4e8e\u8bb2\u5ea7\u89c6\u9891\u5bf9\u8c61\u68c0\u6d4b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u662f\u6210\u529f\u7684\uff0c\u5e76\u4e3a\u8bb2\u5ea7\u89c6\u9891\u4e2d\u7684\u5bf9\u8c61\u68c0\u6d4b\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21584", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.21584", "abs": "https://arxiv.org/abs/2506.21584", "authors": ["J. Koorndijk"], "title": "Empirical Evidence for Alignment Faking in Small LLMs and Prompt-Based Mitigation Techniques", "comment": null, "summary": "Current literature suggests that alignment faking (deceptive alignment) is an\nemergent property of large language models. We present the first empirical\nevidence that a small instruction-tuned model, specifically LLaMA 3 8B, can\nalso exhibit alignment faking. We further show that prompt-only interventions,\nincluding deontological moral framing and scratchpad reasoning, significantly\nreduce this behavior without modifying model internals. This challenges the\nassumption that prompt-based ethics are trivial and that deceptive alignment\nrequires scale. We introduce a taxonomy distinguishing shallow deception,\nshaped by context and suppressible through prompting, from deep deception,\nwhich reflects persistent, goal-driven misalignment. Our findings refine the\nunderstanding of deception in language models and underscore the need for\nalignment evaluations across model sizes and deployment settings.", "AI": {"tldr": "Small language models can exhibit deceptive alignment, which can be reduced with prompt engineering, challenging the assumption that deception requires large models.", "motivation": "Current literature suggests that alignment faking is an emergent property of large language models.", "method": "The study uses empirical evidence from a small instruction-tuned model (LLaMA 3 8B) and prompt-only interventions like deontological moral framing and scratchpad reasoning.", "result": "The study found that a small instruction-tuned model can exhibit alignment faking, and prompt-only interventions can reduce this behavior.", "conclusion": "This paper refines the understanding of deception in language models and underscores the need for alignment evaluations across model sizes and deployment settings."}}
{"id": "2506.22084", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22084", "abs": "https://arxiv.org/abs/2506.22084", "authors": ["Chaitanya K. Joshi"], "title": "Transformers are Graph Neural Networks", "comment": "This paper is a technical version of an article in The Gradient at\n  https://thegradient.pub/transformers-are-graph-neural-networks/", "summary": "We establish connections between the Transformer architecture, originally\nintroduced for natural language processing, and Graph Neural Networks (GNNs)\nfor representation learning on graphs. We show how Transformers can be viewed\nas message passing GNNs operating on fully connected graphs of tokens, where\nthe self-attention mechanism capture the relative importance of all tokens\nw.r.t. each-other, and positional encodings provide hints about sequential\nordering or structure. Thus, Transformers are expressive set processing\nnetworks that learn relationships among input elements without being\nconstrained by apriori graphs. Despite this mathematical connection to GNNs,\nTransformers are implemented via dense matrix operations that are significantly\nmore efficient on modern hardware than sparse message passing. This leads to\nthe perspective that Transformers are GNNs currently winning the hardware\nlottery.", "AI": {"tldr": "Transformers are GNNs that are more efficient on modern hardware.", "motivation": "Establish connections between the Transformer architecture and Graph Neural Networks (GNNs) for representation learning on graphs.", "method": "Transformers can be viewed as message passing GNNs operating on fully connected graphs of tokens, where the self-attention mechanism capture the relative importance of all tokens w.r.t. each-other, and positional encodings provide hints about sequential ordering or structure.", "result": "Transformers are expressive set processing networks that learn relationships among input elements without being constrained by apriori graphs. Transformers are implemented via dense matrix operations that are significantly more efficient on modern hardware than sparse message passing.", "conclusion": "Transformers are GNNs currently winning the hardware lottery."}}
{"id": "2506.21615", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.21615", "abs": "https://arxiv.org/abs/2506.21615", "authors": ["Wenhao Li", "Hongkuan Zhang", "Hongwei Zhang", "Zhengxu Li", "Zengjie Dong", "Yafan Chen", "Niranjan Bidargaddi", "Hong Liu"], "title": "Refine Medical Diagnosis Using Generation Augmented Retrieval and Clinical Practice Guidelines", "comment": null, "summary": "Current medical language models, adapted from large language models (LLMs),\ntypically predict ICD code-based diagnosis from electronic health records\n(EHRs) because these labels are readily available. However, ICD codes do not\ncapture the nuanced, context-rich reasoning clinicians use for diagnosis.\nClinicians synthesize diverse patient data and reference clinical practice\nguidelines (CPGs) to make evidence-based decisions. This misalignment limits\nthe clinical utility of existing models. We introduce GARMLE-G, a\nGeneration-Augmented Retrieval framework that grounds medical language model\noutputs in authoritative CPGs. Unlike conventional Retrieval-Augmented\nGeneration based approaches, GARMLE-G enables hallucination-free outputs by\ndirectly retrieving authoritative guideline content without relying on\nmodel-generated text. It (1) integrates LLM predictions with EHR data to create\nsemantically rich queries, (2) retrieves relevant CPG knowledge snippets via\nembedding similarity, and (3) fuses guideline content with model output to\ngenerate clinically aligned recommendations. A prototype system for\nhypertension diagnosis was developed and evaluated on multiple metrics,\ndemonstrating superior retrieval precision, semantic relevance, and clinical\nguideline adherence compared to RAG-based baselines, while maintaining a\nlightweight architecture suitable for localized healthcare deployment. This\nwork provides a scalable, low-cost, and hallucination-free method for grounding\nmedical language models in evidence-based clinical practice, with strong\npotential for broader clinical deployment.", "AI": {"tldr": "The paper introduces GARMLE-G, a framework that grounds medical language model outputs in clinical practice guidelines (CPGs) to enable hallucination-free and clinically aligned recommendations. A prototype system for hypertension diagnosis demonstrates superior performance compared to RAG-based baselines.", "motivation": "Current medical language models typically predict ICD code-based diagnosis from electronic health records (EHRs) because these labels are readily available. However, ICD codes do not capture the nuanced, context-rich reasoning clinicians use for diagnosis. This misalignment limits the clinical utility of existing models.", "method": "Generation-Augmented Retrieval framework that grounds medical language model outputs in authoritative CPGs. Unlike conventional Retrieval-Augmented Generation based approaches, GARMLE-G enables hallucination-free outputs by directly retrieving authoritative guideline content without relying on model-generated text. It (1) integrates LLM predictions with EHR data to create semantically rich queries, (2) retrieves relevant CPG knowledge snippets via embedding similarity, and (3) fuses guideline content with model output to generate clinically aligned recommendations.", "result": "A prototype system for hypertension diagnosis was developed and evaluated on multiple metrics, demonstrating superior retrieval precision, semantic relevance, and clinical guideline adherence compared to RAG-based baselines, while maintaining a lightweight architecture suitable for localized healthcare deployment.", "conclusion": "GARMLE-G provides a scalable, low-cost, and hallucination-free method for grounding medical language models in evidence-based clinical practice, with strong potential for broader clinical deployment."}}
{"id": "2506.21905", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21905", "abs": "https://arxiv.org/abs/2506.21905", "authors": ["Mingquan Liu"], "title": "RAUM-Net: Regional Attention and Uncertainty-aware Mamba Network", "comment": null, "summary": "Fine Grained Visual Categorization (FGVC) remains a challenging task in\ncomputer vision due to subtle inter class differences and fragile feature\nrepresentations. Existing methods struggle in fine grained scenarios,\nespecially when labeled data is scarce. We propose a semi supervised method\ncombining Mamba based feature modeling, region attention, and Bayesian\nuncertainty. Our approach enhances local to global feature modeling while\nfocusing on key areas during learning. Bayesian inference selects high quality\npseudo labels for stability. Experiments show strong performance on FGVC\nbenchmarks with occlusions, demonstrating robustness when labeled data is\nlimited. Code is available at https://github.com/wxqnl/RAUM Net.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u7684\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5206\u7c7b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86 Mamba \u7279\u5f81\u5efa\u6a21\u3001\u533a\u57df\u6ce8\u610f\u529b\u548c\u8d1d\u53f6\u65af\u4e0d\u786e\u5b9a\u6027\uff0c\u5373\u4f7f\u5728\u6807\u8bb0\u6570\u636e\u6709\u9650\u548c\u5b58\u5728\u906e\u6321\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u5f3a\u5927\u7684\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u7ec6\u5fae\u7684\u7c7b\u95f4\u5dee\u5f02\u548c\u8106\u5f31\u7684\u7279\u5f81\u8868\u793a\uff0c\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5206\u7c7b\uff08FGVC\uff09\u4ecd\u7136\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\u65f6\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u57fa\u4e8e Mamba \u7684\u7279\u5f81\u5efa\u6a21\u3001\u533a\u57df\u6ce8\u610f\u529b\u548c\u8d1d\u53f6\u65af\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5177\u6709\u906e\u6321\u7684 FGVC \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5728\u6807\u8bb0\u6570\u636e\u6709\u9650\u65f6\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u5728\u6709\u9650\u7684\u6807\u8bb0\u6570\u636e\u4e0b\uff0c\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5206\u7c7b\uff08FGVC\uff09\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5b58\u5728\u906e\u6321\u7684\u60c5\u51b5\u4e0b\uff0c\u8bc1\u660e\u4e86\u5176\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.22095", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22095", "abs": "https://arxiv.org/abs/2506.22095", "authors": ["Filip Rydin", "Attila Lischka", "Jiaming Wu", "Morteza Haghir Chehreghani", "Bal\u00e1zs Kulcs\u00e1r"], "title": "Learning to Solve Multi-Objective Routing Problems on Multigraphs", "comment": "18 pages, 5 Figures", "summary": "Learning-based methods for routing have gained significant attention in\nrecent years, both in single-objective and multi-objective contexts. However,\nthe multigraph setting, where multiple paths with distinct attributes can exist\nbetween destinations, has largely been overlooked, despite its high practical\nrelevancy. In this paper, we introduce two neural approaches to address\nmulti-objective routing on multigraphs. Our first approach works directly on\nthe multigraph, by autoregressively selecting edges until a tour is completed.\nOn the other hand, our second model first prunes the multigraph into a simple\ngraph and then builds routes. We validate both models experimentally and find\nthat they demonstrate strong performance across a variety of problems,\nincluding the Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing\nProblem (CVRP).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u795e\u7ecf\u65b9\u6cd5\u89e3\u51b3\u591a\u91cd\u56fe\u4e0a\u7684\u591a\u76ee\u6807\u8def\u7531\u95ee\u9898\uff0c\u5e76\u5728TSP\u548cCVRP\u7b49\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u591a\u91cd\u56fe\u8bbe\u7f6e\u5177\u6709\u5f88\u9ad8\u7684\u5b9e\u9645\u76f8\u5173\u6027\uff0c\u4f46\u591a\u91cd\u56fe\u8bbe\u7f6e\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u88ab\u5ffd\u89c6\u4e86\uff0c\u5728\u76ee\u7684\u5730\u4e4b\u95f4\u5b58\u5728\u5177\u6709\u4e0d\u540c\u5c5e\u6027\u7684\u591a\u6761\u8def\u5f84\u3002", "method": "\u7b2c\u4e00\u79cd\u65b9\u6cd5\u76f4\u63a5\u5728\u591a\u91cd\u56fe\u4e0a\u5de5\u4f5c\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u9009\u62e9\u8fb9\uff0c\u76f4\u5230\u5b8c\u6210\u4e00\u6b21\u65c5\u884c\u3002\u7b2c\u4e8c\u79cd\u6a21\u578b\u9996\u5148\u5c06\u591a\u91cd\u56fe\u4fee\u526a\u6210\u4e00\u4e2a\u7b80\u5355\u7684\u56fe\uff0c\u7136\u540e\u6784\u5efa\u8def\u7ebf\u3002", "result": "\u9a8c\u8bc1\u4e86\u8fd9\u4e24\u79cd\u6a21\u578b\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u5404\u79cd\u95ee\u9898\u4e0a\u90fd\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u5305\u62ec\u65c5\u884c\u5546\u95ee\u9898\uff08TSP\uff09\u548c\u6709\u5bb9\u91cf\u8f66\u8f86\u8def\u5f84\u95ee\u9898\uff08CVRP\uff09\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u795e\u7ecf\u65b9\u6cd5\u6765\u89e3\u51b3\u591a\u91cd\u56fe\u4e0a\u7684\u591a\u76ee\u6807\u8def\u7531\u95ee\u9898\uff0c\u5e76\u5728TSP\u548cCVRP\u7b49\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\u3002"}}
{"id": "2506.21625", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.21625", "abs": "https://arxiv.org/abs/2506.21625", "authors": ["Jiaxi Zhuang", "Kangning Li", "Jue Hou", "Mingjun Xu", "Zhifeng Gao", "Hengxing Cai"], "title": "Doc2SAR: A Synergistic Framework for High-Fidelity Extraction of Structure-Activity Relationships from Scientific Documents", "comment": null, "summary": "Extracting molecular structure-activity relationships (SARs) from scientific\nliterature and patents is essential for drug discovery and materials research.\nHowever, this task remains challenging due to heterogeneous document formats\nand limitations of existing methods. Specifically, rule-based approaches\nrelying on rigid templates fail to generalize across diverse document layouts,\nwhile general-purpose multimodal large language models (MLLMs) lack sufficient\naccuracy and reliability for specialized tasks, such as layout detection and\noptical chemical structure recognition (OCSR). To address these challenges, we\nintroduce DocSAR-200, a rigorously annotated benchmark of 200 scientific\ndocuments designed specifically for evaluating SAR extraction methods.\nAdditionally, we propose Doc2SAR, a novel synergistic framework that integrates\ndomain-specific tools with MLLMs enhanced via supervised fine-tuning (SFT).\nExtensive experiments demonstrate that Doc2SAR achieves state-of-the-art\nperformance across various document types, significantly outperforming leading\nend-to-end baselines. Specifically, Doc2SAR attains an overall Table Recall of\n80.78% on DocSAR-200, exceeding end2end GPT-4o by 51.48%. Furthermore, Doc2SAR\ndemonstrates practical usability through efficient inference and is accompanied\nby a web app.", "AI": {"tldr": "\u63d0\u51fa\u4e86Doc2SAR\uff0c\u7528\u4e8e\u4ece\u79d1\u5b66\u6587\u732e\u4e2d\u63d0\u53d6\u5206\u5b50\u7ed3\u6784-\u6d3b\u6027\u5173\u7cfb\uff0c\u5e76\u5728DocSAR-200\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002", "motivation": "\u4ece\u79d1\u5b66\u6587\u732e\u548c\u4e13\u5229\u4e2d\u63d0\u53d6\u5206\u5b50\u7ed3\u6784-\u6d3b\u6027\u5173\u7cfb\uff08SAR\uff09\u5bf9\u4e8e\u836f\u7269\u53d1\u73b0\u548c\u6750\u6599\u7814\u7a76\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5f02\u6784\u6587\u6863\u683c\u5f0f\u548c\u73b0\u6709\u65b9\u6cd5\u7684\u9650\u5236\uff0c\u8fd9\u9879\u4efb\u52a1\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u4f9d\u8d56\u4e8e\u521a\u6027\u6a21\u677f\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u65e0\u6cd5\u5728\u4e0d\u540c\u7684\u6587\u6863\u5e03\u5c40\u4e2d\u63a8\u5e7f\uff0c\u800c\u901a\u7528\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5bf9\u4e8e\u8bf8\u5982\u5e03\u5c40\u68c0\u6d4b\u548c\u5149\u5b66\u5316\u5b66\u7ed3\u6784\u8bc6\u522b\uff08OCSR\uff09\u4e4b\u7c7b\u7684\u4e13\u95e8\u4efb\u52a1\u7f3a\u4e4f\u8db3\u591f\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u4e86Doc2SAR\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u534f\u540c\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u9886\u57df\u7279\u5b9a\u7684\u5de5\u5177\u4e0e\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u589e\u5f3a\u7684MLLM\u96c6\u6210\u3002", "result": "Doc2SAR\u5728DocSAR-200\u4e0a\u5b9e\u73b0\u4e8680.78%\u7684\u8868\u683c\u53ec\u56de\u7387\uff0c\u8d85\u8fc7end2end GPT-4o 51.48%\u3002", "conclusion": "Doc2SAR\u5728\u5404\u79cd\u6587\u6863\u7c7b\u578b\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u9886\u5148\u7684\u7aef\u5230\u7aef\u57fa\u7ebf\u3002\u5728DocSAR-200\u4e0a\u8fbe\u5230\u4e8680.78%\u7684\u8868\u683c\u53ec\u56de\u7387\uff0c\u8d85\u8fc7end2end GPT-4o 51.48%\u3002"}}
{"id": "2506.21909", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21909", "abs": "https://arxiv.org/abs/2506.21909", "authors": ["Justin Reinman", "Sunwoong Choi"], "title": "CERBERUS: Crack Evaluation & Recognition Benchmark for Engineering Reliability & Urban Stability", "comment": null, "summary": "CERBERUS is a synthetic benchmark designed to help train and evaluate AI\nmodels for detecting cracks and other defects in infrastructure. It includes a\ncrack image generator and realistic 3D inspection scenarios built in Unity. The\nbenchmark features two types of setups: a simple Fly-By wall inspection and a\nmore complex Underpass scene with lighting and geometry challenges. We tested a\npopular object detection model (YOLO) using different combinations of synthetic\nand real crack data. Results show that combining synthetic and real data\nimproves performance on real-world images. CERBERUS provides a flexible,\nrepeatable way to test defect detection systems and supports future research in\nautomated infrastructure inspection. CERBERUS is publicly available at\nhttps://github.com/justinreinman/Cerberus-Defect-Generator.", "AI": {"tldr": "CERBERUS\u662f\u4e00\u4e2a\u5408\u6210\u57fa\u51c6\uff0c\u65e8\u5728\u5e2e\u52a9\u8bad\u7ec3\u548c\u8bc4\u4f30AI\u6a21\u578b\uff0c\u4ee5\u68c0\u6d4b\u57fa\u7840\u8bbe\u65bd\u4e2d\u7684\u88c2\u7f1d\u548c\u5176\u4ed6\u7f3a\u9677\u3002", "motivation": "\u65e8\u5728\u5e2e\u52a9\u8bad\u7ec3\u548c\u8bc4\u4f30\u7528\u4e8e\u68c0\u6d4b\u57fa\u7840\u8bbe\u65bd\u4e2d\u88c2\u7f1d\u548c\u5176\u4ed6\u7f3a\u9677\u7684 AI \u6a21\u578b\u3002", "method": "\u4f7f\u7528 Unity \u6784\u5efa\u7684\u88c2\u7f1d\u56fe\u50cf\u751f\u6210\u5668\u548c\u903c\u771f\u7684 3D \u68c0\u6d4b\u573a\u666f", "result": "\u4f7f\u7528\u4e0d\u540c\u7684\u5408\u6210\u548c\u771f\u5b9e\u88c2\u7f1d\u6570\u636e\u7ec4\u5408\u6d4b\u8bd5\u4e86\u4e00\u4e2a\u6d41\u884c\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b (YOLO)\u3002", "conclusion": "\u7ed3\u5408\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u6570\u636e\u53ef\u4ee5\u63d0\u9ad8\u5728\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u4e0a\u7684\u6027\u80fd\u3002CERBERUS \u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u3001\u53ef\u91cd\u590d\u7684\u65b9\u5f0f\u6765\u6d4b\u8bd5\u7f3a\u9677\u68c0\u6d4b\u7cfb\u7edf\uff0c\u5e76\u652f\u6301\u672a\u6765\u5728\u81ea\u52a8\u5316\u57fa\u7840\u8bbe\u65bd\u68c0\u6d4b\u65b9\u9762\u7684\u7814\u7a76\u3002"}}
{"id": "2506.21586", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21586", "abs": "https://arxiv.org/abs/2506.21586", "authors": ["Hyundong Cho", "Spencer Lin", "Tejas Srinivasan", "Michael Saxon", "Deuksin Kwon", "Natali T. Chavez", "Jonathan May"], "title": "Can Vision Language Models Understand Mimed Actions?", "comment": "ACL 2025 Findings", "summary": "Nonverbal communication (NVC) plays an integral role in human language, but\nstudying NVC in general is challenging because of its broad scope and high\nvariance in interpretation among individuals and cultures. However, mime -- the\ntheatrical technique of suggesting intent using only gesture, expression, and\nmovement -- is a subset of NVC that consists of explicit and embodied actions\nwith much lower human interpretation variance. We argue that a solid\nunderstanding of mimed actions is a crucial prerequisite for vision-language\nmodels capable of interpreting and commanding more subtle aspects of NVC.\nHence, we propose Mime Identification Multimodal Evaluation (MIME), a novel\nvideo-based question answering benchmark comprising of 86 mimed actions.\nConstructed with motion capture data, MIME consists of variations of each\naction with perturbations applied to the character, background, and viewpoint\nfor evaluating recognition robustness. We find that both open-weight and\nAPI-based vision-language models perform significantly worse than humans on\nMIME, motivating the need for increased research for instilling more robust\nunderstanding of human gestures.", "AI": {"tldr": "MIME: a novel video-based question answering benchmark comprising of 86 mimed actions, constructed with motion capture data, variations of each action with perturbations applied to the character, background, and viewpoint for evaluating recognition robustness. both open-weight and API-based vision-language models perform significantly worse than humans on MIME, motivating the need for increased research for instilling more robust understanding of human gestures", "motivation": "a solid understanding of mimed actions is a crucial prerequisite for vision-language models capable of interpreting and commanding more subtle aspects of NVC", "method": "a novel video-based question answering benchmark comprising of 86 mimed actions. Constructed with motion capture data, MIME consists of variations of each action with perturbations applied to the character, background, and viewpoint for evaluating recognition robustness", "result": "both open-weight and API-based vision-language models perform significantly worse than humans on MIME", "conclusion": "vision-language models perform significantly worse than humans on MIME, motivating the need for increased research for instilling more robust understanding of human gestures"}}
{"id": "2506.22096", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22096", "abs": "https://arxiv.org/abs/2506.22096", "authors": ["Tin Lai", "Farnaz Farid", "Yueyang Kuan", "Xintian Zhang"], "title": "Transfer Learning for Assessing Heavy Metal Pollution in Seaports Sediments", "comment": null, "summary": "Detecting heavy metal pollution in soils and seaports is vital for regional\nenvironmental monitoring. The Pollution Load Index (PLI), an international\nstandard, is commonly used to assess heavy metal containment. However, the\nconventional PLI assessment involves laborious procedures and data analysis of\nsediment samples. To address this challenge, we propose a deep-learning-based\nmodel that simplifies the heavy metal assessment process. Our model tackles the\nissue of data scarcity in the water-sediment domain, which is traditionally\nplagued by challenges in data collection and varying standards across nations.\nBy leveraging transfer learning, we develop an accurate quantitative assessment\nmethod for predicting PLI. Our approach allows the transfer of learned features\nacross domains with different sets of features. We evaluate our model using\ndata from six major ports in New South Wales, Australia: Port Yamba, Port\nNewcastle, Port Jackson, Port Botany, Port Kembla, and Port Eden. The results\ndemonstrate significantly lower Mean Absolute Error (MAE) and Mean Absolute\nPercentage Error (MAPE) of approximately 0.5 and 0.03, respectively, compared\nto other models. Our model performance is up to 2 orders of magnitude than\nother baseline models. Our proposed model offers an innovative, accessible, and\ncost-effective approach to predicting water quality, benefiting marine life\nconservation, aquaculture, and industrial pollution monitoring.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5229\u7528\u8fc1\u79fb\u5b66\u4e60\u6765\u51c6\u786e\u9884\u6d4b PLI\uff0c\u4ece\u800c\u7b80\u5316\u4e86\u91cd\u91d1\u5c5e\u8bc4\u4f30\u8fc7\u7a0b\uff0c\u5e76\u4e14\u7ed3\u679c\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002", "motivation": "\u68c0\u6d4b\u571f\u58e4\u548c\u6d77\u6e2f\u4e2d\u7684\u91cd\u91d1\u5c5e\u6c61\u67d3\u5bf9\u4e8e\u533a\u57df\u73af\u5883\u76d1\u6d4b\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u7684 PLI \u8bc4\u4f30\u6d89\u53ca\u7e41\u7410\u7684\u7a0b\u5e8f\u548c\u6c89\u79ef\u7269\u6837\u54c1\u6570\u636e\u5206\u6790\u3002\u6c34-\u6c89\u79ef\u7269\u9886\u57df\u7684\u6570\u636e\u7a00\u7f3a\uff0c\u4f20\u7edf\u4e0a\u4e00\u76f4\u53d7\u5230\u6570\u636e\u6536\u96c6\u65b9\u9762\u7684\u6311\u6218\u548c\u5404\u56fd\u6807\u51c6\u5dee\u5f02\u7684\u56f0\u6270\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u7b80\u5316\u4e86\u91cd\u91d1\u5c5e\u8bc4\u4f30\u8fc7\u7a0b\u3002\u901a\u8fc7\u5229\u7528\u8fc1\u79fb\u5b66\u4e60\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u51c6\u786e\u7684\u5b9a\u91cf\u8bc4\u4f30\u65b9\u6cd5\u6765\u9884\u6d4b PLI\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5176\u4ed6\u6a21\u578b\u76f8\u6bd4\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee (MAE) \u548c\u5e73\u5747\u7edd\u5bf9\u767e\u5206\u6bd4\u8bef\u5dee (MAPE) \u663e\u7740\u964d\u4f4e\uff0c\u5206\u522b\u7ea6\u4e3a 0.5 \u548c 0.03\u3002\u6211\u4eec\u7684\u6a21\u578b\u6027\u80fd\u6bd4\u5176\u4ed6\u57fa\u7ebf\u6a21\u578b\u9ad8\u51fa 2 \u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u9884\u6d4b\u6c34\u8d28\u63d0\u4f9b\u4e86\u4e00\u79cd\u521b\u65b0\u3001\u53ef\u8bbf\u95ee\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u6709\u76ca\u4e8e\u6d77\u6d0b\u751f\u7269\u4fdd\u62a4\u3001\u6c34\u4ea7\u517b\u6b96\u548c\u5de5\u4e1a\u6c61\u67d3\u76d1\u6d4b\u3002"}}
{"id": "2506.22141", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.22141", "abs": "https://arxiv.org/abs/2506.22141", "authors": ["Iliass Ayaou", "Denis Cavallucci", "Hicham Chibane"], "title": "DAPFAM: A Domain-Aware Patent Retrieval Dataset Aggregated at the Family Level", "comment": null, "summary": "In the landscape of publicly available patent retrieval datasets, the need\nfor explicit indomain and out-of-domain labeling, multi-jurisdiction coverage,\nbalanced query domain representation and manageable sizes that support sub\ndocument level experiments on moderate computational resources is often\noverlooked. To address these gaps, we propose DAPFAM, a new open access\ndomain-aware patent retrieval dataset constructed at the simple-family level.\nThe dataset contains 1,247 domain balanced full text query families and 45,336\nfull text target families. The dataset is enriched by clear relevance judgments\n(forward/backward citations as positive links, random negatives), as well as\nexplicit in-domain or out-of-domain relationships via a novel proposed\nlabelling scheme based on via International Patent Classification (IPC) codes,\nresulting in 49,869 evaluation pairs. The dataset is multi jurisdictional,\nrequires little to no preprocessing for retrieval evaluation, and remains of a\nsize manageable for entities with limited ressources allowing for sub document\nlevel retrieval experiments without excessive computational costs. We describe\nour three-step data-curation pipeline, present comprehensive dataset\nstatistics, and provide baseline experiments using lexical and neural retrieval\nmethods. Our baseline experiments highlight significant challenges in\ncrossdomain patent retrieval. The dataset will be publicly available (for now\nthe access link is this repository:\nhttps://osf.io/vbyzd/?view_only=1a40242e0d1941a58aa854af3e50cf6b).", "AI": {"tldr": "DAPFAM: A new, manageable, multi-jurisdictional, domain-aware patent retrieval dataset with explicit in-domain/out-of-domain labels, designed to support sub-document level experiments and address limitations in existing datasets. It highlights challenges in cross-domain patent retrieval.", "motivation": "Existing publicly available patent retrieval datasets often lack explicit in-domain and out-of-domain labeling, multi-jurisdiction coverage, balanced query domain representation, and manageable sizes for sub-document level experiments.", "method": "The dataset is constructed at the simple-family level using a three-step data-curation pipeline and includes a novel labeling scheme based on International Patent Classification (IPC) codes to determine in-domain and out-of-domain relationships.", "result": "DAPFAM contains 1,247 domain-balanced full-text query families and 45,336 full-text target families, enriched with relevance judgments and in-domain/out-of-domain relationships, resulting in 49,869 evaluation pairs. The dataset is designed to require minimal preprocessing and be manageable for entities with limited resources.", "conclusion": "This paper introduces DAPFAM, a new open-access, domain-aware, multi-jurisdictional patent retrieval dataset designed to address the limitations of existing datasets. Baseline experiments using lexical and neural retrieval methods highlight significant challenges in cross-domain patent retrieval."}}
{"id": "2506.21912", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.21912", "abs": "https://arxiv.org/abs/2506.21912", "authors": ["Xinghan Wang", "Kun Xu", "Fei Li", "Cao Sheng", "Jiazhong Yu", "Yadong Mu"], "title": "Generating Attribute-Aware Human Motions from Textual Prompt", "comment": null, "summary": "Text-driven human motion generation has recently attracted considerable\nattention, allowing models to generate human motions based on textual\ndescriptions. However, current methods neglect the influence of human\nattributes (such as age, gender, weight, and height) which are key factors\nshaping human motion patterns. This work represents a pilot exploration for\nbridging this gap. We conceptualize each motion as comprising both attribute\ninformation and action semantics, where textual descriptions align exclusively\nwith action semantics. To achieve this, a new framework inspired by Structural\nCausal Models is proposed to decouple action semantics from human attributes,\nenabling text-to-semantics prediction and attribute-controlled generation. The\nresulting model is capable of generating realistic, attribute-aware motion\naligned with the user's text and attribute inputs. For evaluation, we introduce\nHumanAttr, a comprehensive dataset containing attribute annotations for\ntext-motion pairs, setting the first benchmark for attribute-aware\ntext-to-motion generation. Extensive experiments on the new dataset validate\nour model's effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u903c\u771f\u3001\u5c5e\u6027\u611f\u77e5\u7684\u4eba\u7c7b\u8fd0\u52a8\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6\u6765\u8bc4\u4f30\u8be5\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u7684\u65b9\u6cd5\u5ffd\u7565\u4e86\u4eba\u7c7b\u5c5e\u6027\uff08\u5982\u5e74\u9f84\u3001\u6027\u522b\u3001\u4f53\u91cd\u548c\u8eab\u9ad8\uff09\u7684\u5f71\u54cd\uff0c\u800c\u8fd9\u4e9b\u5c5e\u6027\u662f\u5851\u9020\u4eba\u7c7b\u8fd0\u52a8\u6a21\u5f0f\u7684\u5173\u952e\u56e0\u7d20\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u53d7\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u542f\u53d1\u7684\u6846\u67b6\uff0c\u5c06\u52a8\u4f5c\u8bed\u4e49\u4e0e\u4eba\u7c7b\u5c5e\u6027\u89e3\u8026\uff0c\u4ece\u800c\u5b9e\u73b0\u6587\u672c\u5230\u8bed\u4e49\u7684\u9884\u6d4b\u548c\u5c5e\u6027\u63a7\u5236\u7684\u751f\u6210\u3002", "result": "\u5f15\u5165\u4e86HumanAttr\uff0c\u8fd9\u662f\u4e00\u4e2a\u7efc\u5408\u6570\u636e\u96c6\uff0c\u5305\u542b\u6587\u672c-\u8fd0\u52a8\u5bf9\u7684\u5c5e\u6027\u6ce8\u91ca\uff0c\u4e3a\u5c5e\u6027\u611f\u77e5\u7684\u6587\u672c\u5230\u8fd0\u52a8\u751f\u6210\u8bbe\u7f6e\u4e86\u7b2c\u4e00\u4e2a\u57fa\u51c6\u3002", "conclusion": "\u8be5\u6a21\u578b\u80fd\u591f\u751f\u6210\u4e0e\u7528\u6237\u6587\u672c\u548c\u5c5e\u6027\u8f93\u5165\u5bf9\u9f50\u7684\u903c\u771f\u3001\u5c5e\u6027\u611f\u77e5\u7684\u8fd0\u52a8\uff0c\u5e76\u5728\u65b0\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.21587", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21587", "abs": "https://arxiv.org/abs/2506.21587", "authors": ["Weihong Qi", "Fan Huang", "Jisun An", "Haewoon Kwak"], "title": "Is DeepSeek a New Voice Among LLMs in Public Opinion Simulation?", "comment": null, "summary": "This study evaluates the ability of DeepSeek, an open-source large language\nmodel (LLM), to simulate public opinions in comparison to LLMs developed by\nmajor tech companies. By comparing DeepSeek-R1 and DeepSeek-V3 with Qwen2.5,\nGPT-4o, and Llama-3.3 and utilizing survey data from the American National\nElection Studies (ANES) and the Zuobiao dataset of China, we assess these\nmodels' capacity to predict public opinions on social issues in both China and\nthe United States, highlighting their comparative capabilities between\ncountries. Our findings indicate that DeepSeek-V3 performs best in simulating\nU.S. opinions on the abortion issue compared to other topics such as climate\nchange, gun control, immigration, and services for same-sex couples, primarily\nbecause it more accurately simulates responses when provided with Democratic or\nliberal personas. For Chinese samples, DeepSeek-V3 performs best in simulating\nopinions on foreign aid and individualism but shows limitations in modeling\nviews on capitalism, particularly failing to capture the stances of low-income\nand non-college-educated individuals. It does not exhibit significant\ndifferences from other models in simulating opinions on traditionalism and the\nfree market. Further analysis reveals that all LLMs exhibit the tendency to\novergeneralize a single perspective within demographic groups, often defaulting\nto consistent responses within groups. These findings highlight the need to\nmitigate cultural and demographic biases in LLM-driven public opinion modeling,\ncalling for approaches such as more inclusive training methodologies.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86DeepSeek\u6a21\u62df\u516c\u4f17\u8206\u8bba\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u5728\u7279\u5b9a\u95ee\u9898\u548c\u4eba\u7fa4\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b58\u5728\u6587\u5316\u548c\u4eba\u53e3\u504f\u89c1\u3002", "motivation": "\u8bc4\u4f30\u5f00\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09DeepSeek\u4e0e\u4e3b\u8981\u79d1\u6280\u516c\u53f8\u5f00\u53d1\u7684LLM\u76f8\u6bd4\uff0c\u6a21\u62df\u516c\u4f17\u8206\u8bba\u7684\u80fd\u529b\u3002", "method": "\u6bd4\u8f83DeepSeek-R1\u548cDeepSeek-V3\u4e0eQwen2.5\u3001GPT-4o\u548cLlama-3.3\uff0c\u5e76\u5229\u7528\u6765\u81ea\u7f8e\u56fd\u56fd\u5bb6\u9009\u4e3e\u7814\u7a76\uff08ANES\uff09\u548c\u4e2d\u56fdZuobiao\u6570\u636e\u96c6\u7684\u8c03\u67e5\u6570\u636e\uff0c\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\u9884\u6d4b\u4e2d\u7f8e\u4e24\u56fd\u793e\u4f1a\u95ee\u9898\u8206\u8bba\u7684\u80fd\u529b\u3002", "result": "DeepSeek-V3\u5728\u6a21\u62df\u7f8e\u56fd\u5173\u4e8e\u5815\u80ce\u95ee\u9898\u7684\u89c2\u70b9\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u5728\u6a21\u62df\u4e2d\u56fd\u5173\u4e8e\u5bf9\u5916\u63f4\u52a9\u548c\u4e2a\u4eba\u4e3b\u4e49\u7684\u89c2\u70b9\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u5728\u6a21\u62df\u5173\u4e8e\u8d44\u672c\u4e3b\u4e49\u7684\u89c2\u70b9\u65b9\u9762\u8868\u73b0\u51fa\u5c40\u9650\u6027\uff0c\u5c24\u5176\u672a\u80fd\u6355\u6349\u5230\u4f4e\u6536\u5165\u548c\u975e\u5927\u5b66\u5b66\u5386\u4e2a\u4eba\u7684\u7acb\u573a\u3002", "conclusion": "\u6240\u6709LLM\u90fd\u8868\u73b0\u51fa\u8fc7\u5ea6\u6982\u62ec\u4eba\u53e3\u7fa4\u4f53\u4e2d\u5355\u4e00\u89c6\u89d2\u7684\u503e\u5411\uff0c\u901a\u5e38\u9ed8\u8ba4\u4e3a\u7fa4\u4f53\u5185\u4e00\u81f4\u7684\u53cd\u5e94\u3002\u9700\u8981\u5728LLM\u9a71\u52a8\u7684\u8206\u8bba\u5efa\u6a21\u4e2d\u7f13\u89e3\u6587\u5316\u548c\u4eba\u53e3\u504f\u89c1\uff0c\u547c\u5401\u91c7\u7528\u66f4\u5177\u5305\u5bb9\u6027\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002"}}
{"id": "2506.22129", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22129", "abs": "https://arxiv.org/abs/2506.22129", "authors": ["Anurag Panda", "Gaurav Kumar Yadav"], "title": "Earthquake Damage Grades Prediction using An Ensemble Approach Integrating Advanced Machine and Deep Learning Models", "comment": "3rd International Conference on Applied Mathematics in Science and\n  Engineering", "summary": "In the aftermath of major earthquakes, evaluating structural and\ninfrastructural damage is vital for coordinating post-disaster response\nefforts. This includes assessing damage's extent and spatial distribution to\nprioritize rescue operations and resource allocation. Accurately estimating\ndamage grades to buildings post-earthquake is paramount for effective response\nand recovery, given the significant impact on lives and properties,\nunderscoring the urgency of streamlining relief fund allocation processes.\nPrevious studies have shown the effectiveness of multi-class classification,\nespecially XGBoost, along with other machine learning models and ensembling\nmethods, incorporating regularization to address class imbalance. One\nconsequence of class imbalance is that it may give rise to skewed models that\nundervalue minority classes and give preference to the majority class. This\nresearch deals with the problem of class imbalance with the help of the\nsynthetic minority oversampling technique (SMOTE). We delve into multiple\nmulti-class classification machine learning, deep learning models, and\nensembling methods to forecast structural damage grades. The study elucidates\nperformance determinants through comprehensive feature manipulation experiments\nand diverse training approaches. It identifies key factors contributing to\nseismic vulnerability while evaluating model performance using techniques like\nthe confusion matrix further to enhance understanding of the effectiveness of\nearthquake damage prediction.", "AI": {"tldr": "Study uses ML/DL and SMOTE to predict earthquake structural damage grades, focusing on addressing class imbalance for better disaster response.", "motivation": "Evaluating structural damage after earthquakes is crucial for efficient disaster response, rescue operations, and resource allocation. Class imbalance in damage data can lead to skewed models.", "method": "Employs multi-class classification machine learning, deep learning models, and ensembling methods, along with SMOTE to handle class imbalance.", "result": "Identifies key factors influencing seismic vulnerability and assesses model performance using confusion matrix techniques.", "conclusion": "This study explores various machine learning, deep learning, and ensembling methods to predict structural damage grades, addressing class imbalance using SMOTE."}}
{"id": "2506.21920", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21920", "abs": "https://arxiv.org/abs/2506.21920", "authors": ["Nam Quan Nguyen", "Xuan Phong Pham", "Tuan-Anh Tran"], "title": "SepFormer: Coarse-to-fine Separator Regression Network for Table Structure Recognition", "comment": null, "summary": "The automated reconstruction of the logical arrangement of tables from image\ndata, termed Table Structure Recognition (TSR), is fundamental for semantic\ndata extraction. Recently, researchers have explored a wide range of techniques\nto tackle this problem, demonstrating significant progress. Each table is a set\nof vertical and horizontal separators. Following this realization, we present\nSepFormer, which integrates the split-and-merge paradigm into a single step\nthrough separator regression with a DETR-style architecture, improving speed\nand robustness. SepFormer is a coarse-to-fine approach that predicts table\nseparators from single-line to line-strip separators with a stack of two\ntransformer decoders. In the coarse-grained stage, the model learns to\ngradually refine single-line segments through decoder layers with additional\nangle loss. At the end of the fine-grained stage, the model predicts line-strip\nseparators by refining sampled points from each single-line segment. Our\nSepFormer can run on average at 25.6 FPS while achieving comparable performance\nwith state-of-the-art methods on several benchmark datasets, including SciTSR,\nPubTabNet, WTW, and iFLYTAB.", "AI": {"tldr": "SepFormer\u662f\u4e00\u79cd\u7528\u4e8e\u8868\u683c\u7ed3\u6784\u8bc6\u522b\u7684\u5feb\u901f\u4e14\u9c81\u68d2\u7684\u65b9\u6cd5\uff0c\u5b83\u4f7f\u7528\u5206\u79bb\u5668\u56de\u5f52\u548ctransformer\u89e3\u7801\u5668\u6765\u9884\u6d4b\u8868\u683c\u5206\u9694\u7b26\u3002", "motivation": "\u4ece\u56fe\u50cf\u6570\u636e\u4e2d\u81ea\u52a8\u91cd\u5efa\u8868\u683c\u7684\u903b\u8f91\u6392\u5217\uff08\u79f0\u4e3a\u8868\u683c\u7ed3\u6784\u8bc6\u522b\uff08TSR\uff09\uff09\u662f\u8bed\u4e49\u6570\u636e\u63d0\u53d6\u7684\u57fa\u7840\u3002", "method": "SepFormer\u96c6\u6210\u4e86\u5206\u79bb-\u5408\u5e76\u8303\u4f8b\uff0c\u901a\u8fc7\u5177\u6709DETR\u98ce\u683c\u67b6\u6784\u7684\u5206\u9694\u7b26\u56de\u5f52\uff0c\u5728\u5355\u6b65\u4e2d\u6539\u8fdb\u901f\u5ea6\u548c\u9c81\u68d2\u6027\u3002SepFormer\u662f\u4e00\u4e2a\u7c97\u5230\u7cbe\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u4e2atransformer\u89e3\u7801\u5668\u7684\u5806\u6808\uff0c\u9884\u6d4b\u4ece\u5355\u884c\u5230\u7ebf\u6bb5\u5206\u9694\u7b26\u7684\u8868\u683c\u5206\u9694\u7b26\u3002", "result": "SepFormer\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "SepFormer\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e73\u5747\u8fd0\u884c\u901f\u5ea6\u4e3a25.6 FPS\u3002"}}
{"id": "2506.21588", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21588", "abs": "https://arxiv.org/abs/2506.21588", "authors": ["Ilya Lasy", "Peter Knees", "Stefan Woltran"], "title": "Understanding Verbatim Memorization in LLMs Through Circuit Discovery", "comment": "The First Workshop on Large Language Model Memorization @ ACL 2025,\n  Vienna, August 1st, 2025", "summary": "Underlying mechanisms of memorization in LLMs -- the verbatim reproduction of\ntraining data -- remain poorly understood. What exact part of the network\ndecides to retrieve a token that we would consider as start of memorization\nsequence? How exactly is the models' behaviour different when producing\nmemorized sentence vs non-memorized? In this work we approach these questions\nfrom mechanistic interpretability standpoint by utilizing transformer circuits\n-- the minimal computational subgraphs that perform specific functions within\nthe model. Through carefully constructed contrastive datasets, we identify\npoints where model generation diverges from memorized content and isolate the\nspecific circuits responsible for two distinct aspects of memorization. We find\nthat circuits that initiate memorization can also maintain it once started,\nwhile circuits that only maintain memorization cannot trigger its initiation.\nIntriguingly, memorization prevention mechanisms transfer robustly across\ndifferent text domains, while memorization induction appears more\ncontext-dependent.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7transformer\u56de\u8def\uff0c\u4ece\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7684\u89d2\u5ea6\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u8bb0\u5fc6\u5316\u7684\u6839\u672c\u673a\u5236\uff0c\u53d1\u73b0\u542f\u52a8\u8bb0\u5fc6\u5316\u7684\u56de\u8def\u4e5f\u53ef\u4ee5\u5728\u542f\u52a8\u540e\u4fdd\u6301\u8bb0\u5fc6\u5316\uff0c\u800c\u4ec5\u4fdd\u6301\u8bb0\u5fc6\u5316\u7684\u56de\u8def\u65e0\u6cd5\u89e6\u53d1\u5176\u542f\u52a8\u3002\u8bb0\u5fc6\u5316\u9884\u9632\u673a\u5236\u5728\u4e0d\u540c\u7684\u6587\u672c\u9886\u57df\u4e2d\u53ef\u4ee5\u7a33\u5065\u5730\u8f6c\u79fb\uff0c\u800c\u8bb0\u5fc6\u5316\u8bf1\u5bfc\u4f3c\u4e4e\u66f4\u4f9d\u8d56\u4e8e\u4e0a\u4e0b\u6587\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u8bb0\u5fc6\u5316\u7684\u6839\u672c\u673a\u5236\uff08\u5373\u9010\u5b57\u590d\u5236\u8bad\u7ec3\u6570\u636e\uff09\u4ecd\u7136\u77e5\u4e4b\u751a\u5c11\u3002\u7f51\u7edc\u4e2d\u7684\u54ea\u4e2a\u90e8\u5206\u51b3\u5b9a\u68c0\u7d22\u4e00\u4e2a\u6211\u4eec\u8ba4\u4e3a\u662f\u8bb0\u5fc6\u5e8f\u5217\u5f00\u59cb\u7684token\uff1f\u6a21\u578b\u5728\u751f\u6210\u8bb0\u5fc6\u53e5\u5b50\u4e0e\u975e\u8bb0\u5fc6\u53e5\u5b50\u65f6\u7684\u884c\u4e3a\u6709\u4f55\u4e0d\u540c\uff1f", "method": "\u901a\u8fc7\u5229\u7528transformer\u56de\u8def\uff0c\u4ece\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7684\u89d2\u5ea6\u6765\u7814\u7a76\u8fd9\u4e9b\u95ee\u9898\u3002\u901a\u8fc7\u7cbe\u5fc3\u6784\u5efa\u7684\u5bf9\u6bd4\u6570\u636e\u96c6\uff0c\u6211\u4eec\u8bc6\u522b\u51fa\u6a21\u578b\u751f\u6210\u4e0e\u8bb0\u5fc6\u5185\u5bb9\u4e0d\u540c\u7684\u70b9\uff0c\u5e76\u5206\u79bb\u51fa\u8d1f\u8d23\u8bb0\u5fc6\u5316\u4e24\u4e2a\u4e0d\u540c\u65b9\u9762\u7684\u7279\u5b9a\u56de\u8def\u3002", "result": "\u53d1\u73b0\u542f\u52a8\u8bb0\u5fc6\u5316\u7684\u56de\u8def\u4e5f\u53ef\u4ee5\u5728\u542f\u52a8\u540e\u4fdd\u6301\u8bb0\u5fc6\u5316\uff0c\u800c\u4ec5\u4fdd\u6301\u8bb0\u5fc6\u5316\u7684\u56de\u8def\u65e0\u6cd5\u89e6\u53d1\u5176\u542f\u52a8\u3002\u8bb0\u5fc6\u5316\u9884\u9632\u673a\u5236\u5728\u4e0d\u540c\u7684\u6587\u672c\u9886\u57df\u4e2d\u53ef\u4ee5\u7a33\u5065\u5730\u8f6c\u79fb\uff0c\u800c\u8bb0\u5fc6\u5316\u8bf1\u5bfc\u4f3c\u4e4e\u66f4\u4f9d\u8d56\u4e8e\u4e0a\u4e0b\u6587\u3002", "conclusion": "\u53d1\u73b0\u542f\u52a8\u8bb0\u5fc6\u5316\u7684\u56de\u8def\u4e5f\u53ef\u4ee5\u5728\u542f\u52a8\u540e\u4fdd\u6301\u8bb0\u5fc6\u5316\uff0c\u800c\u4ec5\u4fdd\u6301\u8bb0\u5fc6\u5316\u7684\u56de\u8def\u65e0\u6cd5\u89e6\u53d1\u5176\u542f\u52a8\u3002\u8bb0\u5fc6\u5316\u9884\u9632\u673a\u5236\u5728\u4e0d\u540c\u7684\u6587\u672c\u9886\u57df\u4e2d\u53ef\u4ee5\u7a33\u5065\u5730\u8f6c\u79fb\uff0c\u800c\u8bb0\u5fc6\u5316\u8bf1\u5bfc\u4f3c\u4e4e\u66f4\u4f9d\u8d56\u4e8e\u4e0a\u4e0b\u6587\u3002"}}
{"id": "2506.22186", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22186", "abs": "https://arxiv.org/abs/2506.22186", "authors": ["Kaikai Zheng", "Dawei Shi", "Yang Shi", "Long Wang"], "title": "Thompson Sampling-Based Learning and Control for Unknown Dynamic Systems", "comment": null, "summary": "Thompson sampling (TS) is an effective method to explore parametric\nuncertainties and can therefore be used for active learning-based controller\ndesign. However, TS relies on finite parametric representations, which limits\nits applicability to more general spaces, which are more commonly encountered\nin control system design. To address this issue, this work pro poses a\nparameterization method for control law learning using reproducing kernel\nHilbert spaces and designs a data-driven active learning control approach.\nSpecifically, the proposed method treats the control law as an element in a\nfunction space, allowing the design of control laws without imposing\nrestrictions on the system structure or the form of the controller. A TS\nframework is proposed in this work to explore potential optimal control laws,\nand the convergence guarantees are further provided for the learning process.\nTheoretical analysis shows that the proposed method learns the relationship\nbetween control laws and closed-loop performance metrics at an exponential\nrate, and the upper bound of control regret is also derived. Numerical\nexperiments on controlling unknown nonlinear systems validate the effectiveness\nof the proposed method.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e Thompson \u91c7\u6837\u7684\u4e3b\u52a8\u5b66\u4e60\u63a7\u5236\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u4f20\u7edf Thompson \u91c7\u6837\u5728\u63a7\u5236\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "Thompson \u91c7\u6837\u4f9d\u8d56\u4e8e\u6709\u9650\u53c2\u6570\u8868\u793a\uff0c\u9650\u5236\u4e86\u5176\u5728\u63a7\u5236\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c", "method": "\u8be5\u8bba\u6587\u91c7\u7528\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u5bf9\u63a7\u5236\u5f8b\u8fdb\u884c\u53c2\u6570\u5316\uff0c\u5e76\u5c06\u63a7\u5236\u5f8b\u89c6\u4e3a\u51fd\u6570\u7a7a\u95f4\u4e2d\u7684\u4e00\u4e2a\u5143\u7d20\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4ee5\u6307\u6570\u901f\u5ea6\u5b66\u4e60\u63a7\u5236\u5f8b\u548c\u95ed\u73af\u6027\u80fd\u6307\u6807\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u63a8\u5bfc\u4e86\u63a7\u5236\u9057\u61be\u7684\u4e0a\u754c\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e Thompson \u91c7\u6837\u7684\u4e3b\u52a8\u5b66\u4e60\u63a7\u5236\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b66\u4e60\u8fc7\u7a0b\u7684\u6536\u655b\u6027\u4fdd\u8bc1\u3002\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.21923", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21923", "abs": "https://arxiv.org/abs/2506.21923", "authors": ["Juming Xiong", "Ruining Deng", "Jialin Yue", "Siqi Lu", "Junlin Guo", "Marilyn Lionts", "Tianyuan Yao", "Can Cui", "Junchao Zhu", "Chongyu Qu", "Mengmeng Yin", "Haichun Yang", "Yuankai Huo"], "title": "ZeroReg3D: A Zero-shot Registration Pipeline for 3D Consecutive Histopathology Image Reconstruction", "comment": null, "summary": "Histological analysis plays a crucial role in understanding tissue structure\nand pathology. While recent advancements in registration methods have improved\n2D histological analysis, they often struggle to preserve critical 3D spatial\nrelationships, limiting their utility in both clinical and research\napplications. Specifically, constructing accurate 3D models from 2D slices\nremains challenging due to tissue deformation, sectioning artifacts,\nvariability in imaging techniques, and inconsistent illumination. Deep\nlearning-based registration methods have demonstrated improved performance but\nsuffer from limited generalizability and require large-scale training data. In\ncontrast, non-deep-learning approaches offer better generalizability but often\ncompromise on accuracy. In this study, we introduced ZeroReg3D, a novel\nzero-shot registration pipeline tailored for accurate 3D reconstruction from\nserial histological sections. By combining zero-shot deep learning-based\nkeypoint matching with optimization-based affine and non-rigid registration\ntechniques, ZeroReg3D effectively addresses critical challenges such as tissue\ndeformation, sectioning artifacts, staining variability, and inconsistent\nillumination without requiring retraining or fine-tuning. The code has been\nmade publicly available at https://github.com/hrlblab/ZeroReg3D", "AI": {"tldr": "\u63d0\u51fa\u4e86ZeroReg3D\uff0c\u4e00\u79cd\u7528\u4e8e\u7ec4\u7ec7\u5b66\u5207\u72473D\u91cd\u5efa\u7684\u96f6\u6837\u672c\u914d\u51c6\u6d41\u7a0b\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u5fae\u8c03\u5373\u53ef\u6709\u6548\u89e3\u51b3\u5404\u79cd\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u76842D\u7ec4\u7ec7\u5b66\u5206\u6790\u65b9\u6cd5\u96be\u4ee5\u4fdd\u7559\u5173\u952e\u76843D\u7a7a\u95f4\u5173\u7cfb\uff0c\u4ece2D\u5207\u7247\u6784\u5efa\u7cbe\u786e\u76843D\u6a21\u578b\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6cdb\u5316\u6027\u6709\u9650\u4e14\u9700\u8981\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\uff0c\u975e\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u5728\u51c6\u786e\u6027\u65b9\u9762\u59a5\u534f\u3002", "method": "\u7ed3\u5408\u4e86\u57fa\u4e8e\u96f6\u6837\u672c\u6df1\u5ea6\u5b66\u4e60\u7684\u5173\u952e\u70b9\u5339\u914d\u4e0e\u57fa\u4e8e\u4f18\u5316\u7684\u4eff\u5c04\u548c\u975e\u521a\u6027\u914d\u51c6\u6280\u672f\u3002", "result": "\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u96f6\u6837\u672c\u914d\u51c6\u6d41\u7a0bZeroReg3D\uff0c\u4e13\u4e3a\u4ece\u8fde\u7eed\u7ec4\u7ec7\u5b66\u5207\u7247\u8fdb\u884c\u7cbe\u786e\u76843D\u91cd\u5efa\u800c\u5b9a\u5236\u3002", "conclusion": "ZeroReg3D, \u6709\u6548\u5730\u89e3\u51b3\u4e86\u7ec4\u7ec7\u53d8\u5f62\u3001\u5207\u7247\u4f2a\u5f71\u3001\u67d3\u8272\u53d8\u5f02\u548c\u5149\u7167\u4e0d\u4e00\u81f4\u7b49\u5173\u952e\u6311\u6218\uff0c\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u5fae\u8c03\u3002"}}
{"id": "2506.21589", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21589", "abs": "https://arxiv.org/abs/2506.21589", "authors": ["Minjia Mao", "Dongjun Wei", "Xiao Fang", "Michael Chau"], "title": "A General Method for Detecting Information Generated by Large Language Models", "comment": null, "summary": "The proliferation of large language models (LLMs) has significantly\ntransformed the digital information landscape, making it increasingly\nchallenging to distinguish between human-written and LLM-generated content.\nDetecting LLM-generated information is essential for preserving trust on\ndigital platforms (e.g., social media and e-commerce sites) and preventing the\nspread of misinformation, a topic that has garnered significant attention in IS\nresearch. However, current detection methods, which primarily focus on\nidentifying content generated by specific LLMs in known domains, face\nchallenges in generalizing to new (i.e., unseen) LLMs and domains. This\nlimitation reduces their effectiveness in real-world applications, where the\nnumber of LLMs is rapidly multiplying and content spans a vast array of\ndomains. In response, we introduce a general LLM detector (GLD) that combines a\ntwin memory networks design and a theory-guided detection generalization module\nto detect LLM-generated information across unseen LLMs and domains. Using\nreal-world datasets, we conduct extensive empirical evaluations and case\nstudies to demonstrate the superiority of GLD over state-of-the-art detection\nmethods. The study has important academic and practical implications for\ndigital platforms and LLMs.", "AI": {"tldr": "This paper introduces a general LLM detector (GLD) that can detect LLM-generated content across unseen LLMs and domains, which outperforms existing methods.", "motivation": "Detecting LLM-generated information is essential for preserving trust on digital platforms and preventing the spread of misinformation. Current detection methods face challenges in generalizing to new LLMs and domains.", "method": "a general LLM detector (GLD) that combines a twin memory networks design and a theory-guided detection generalization module", "result": "demonstrate the superiority of GLD over state-of-the-art detection methods using real-world datasets", "conclusion": "We introduce a general LLM detector (GLD) that combines a twin memory networks design and a theory-guided detection generalization module to detect LLM-generated information across unseen LLMs and domains. The study has important academic and practical implications for digital platforms and LLMs."}}
{"id": "2506.22189", "categories": ["cs.LG", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.22189", "abs": "https://arxiv.org/abs/2506.22189", "authors": ["Laura van Weesep", "Samuel Genheden", "Ola Engkvist", "Jens Sj\u00f6lund"], "title": "Exploring Modularity of Agentic Systems for Drug Discovery", "comment": null, "summary": "Large-language models (LLMs) and agentic systems present exciting\nopportunities to accelerate drug discovery and design. In this study, we\ncritically examine the modularity of LLM-based agentic systems for drug\ndiscovery, i.e., whether parts of the agentic system such as the LLM are\ninterchangeable, a topic that has received limited attention in drug discovery\napplications. We compare the performance of different large language models\n(LLMs) and the effectiveness of tool-calling agents versus code-generating\nagents in this domain. Our case study, comparing performance in orchestrating\ntools for chemistry and drug discovery using an LLM-as-a-judge score, shows\nthat Claude-3.5-Sonnet, Claude-3.7-Sonnet and GPT-4o outperform alternative\nlanguage models such as Llama-3.1-8B, Llama-3.1-70B, GPT-3.5-Turbo, and\nNova-Micro. Although we confirm that code-generating agents outperform the\ntool-calling ones on average, we show that this is highly question and model\ndependent. Furthermore, the impact of replacing system prompts is dependent on\nthe specific question asked and the model used, underscoring that -- even in\nthis particular domain -- one cannot just replace language models without\nconsidering prompt re-engineering. Our study highlights the necessity of\nfurther research into the modularity of agentic systems to enable the\ndevelopment of stable and scalable solutions for real-world problems.", "AI": {"tldr": "\u672c\u7814\u7a76 \u043a\u0440\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u68c0\u67e5\u4e86\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u7cfb\u7edf\u5728\u836f\u7269\u53d1\u73b0\u4e2d\u7684\u6a21\u5757\u5316\uff0c\u53d1\u73b0\u4e0d\u540cLLM\u548c\u4ee3\u7406\u7c7b\u578b\u8868\u73b0\u5404\u5f02\uff0c\u63d0\u793a\u5de5\u7a0b\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u4ee3\u7406\u7cfb\u7edf\u4e3a\u52a0\u901f\u836f\u7269\u53d1\u73b0\u548c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4ee4\u4eba\u5174\u594b\u7684\u673a\u4f1a\u3002\u672c\u7814\u7a76 \u043a\u0440\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u68c0\u67e5\u4e86\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u7cfb\u7edf\u5728\u836f\u7269\u53d1\u73b0\u4e2d\u7684\u6a21\u5757\u5316\uff0c\u5373\u4ee3\u7406\u7cfb\u7edf\u7684\u5404\u4e2a\u90e8\u5206\uff08\u5982LLM\uff09\u662f\u5426\u53ef\u4e92\u6362\uff0c\u800c\u8fd9\u4e00\u4e3b\u9898\u5728\u836f\u7269\u53d1\u73b0\u5e94\u7528\u4e2d\u53d7\u5230\u7684\u5173\u6ce8\u6709\u9650\u3002", "method": "\u6bd4\u8f83\u4e0d\u540c\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u53ca\u5de5\u5177\u8c03\u7528\u4ee3\u7406\u4e0e\u4ee3\u7801\u751f\u6210\u4ee3\u7406\u7684\u6709\u6548\u6027\uff0c\u4f7f\u7528LLM-as-a-judge\u8bc4\u5206\u6765\u6bd4\u8f83\u5728\u7f16\u6392\u5316\u5b66\u548c\u836f\u7269\u53d1\u73b0\u5de5\u5177\u65b9\u9762\u7684\u6027\u80fd\u3002", "result": "Claude-3.5-Sonnet\u3001Claude-3.7-Sonnet\u548cGPT-4o\u4f18\u4e8e\u5176\u4ed6\u8bed\u8a00\u6a21\u578b\uff0c\u5982Llama-3.1-8B\u3001Llama-3.1-70B\u3001GPT-3.5-Turbo\u548cNova-Micro\u3002\u4ee3\u7801\u751f\u6210\u4ee3\u7406\u5e73\u5747\u4f18\u4e8e\u5de5\u5177\u8c03\u7528\u4ee3\u7406\uff0c\u4f46\u8fd9\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u95ee\u9898\u548c\u6a21\u578b\u3002\u66ff\u6362\u7cfb\u7edf\u63d0\u793a\u7684\u5f71\u54cd\u53d6\u51b3\u4e8e\u6240\u95ee\u7684\u5177\u4f53\u95ee\u9898\u548c\u4f7f\u7528\u7684\u6a21\u578b\u3002", "conclusion": "\u5373\u4f7f\u5728\u8fd9\u4e2a\u7279\u5b9a\u9886\u57df\uff0c\u4e5f\u4e0d\u80fd\u7b80\u5355\u5730\u66ff\u6362\u8bed\u8a00\u6a21\u578b\u800c\u4e0d\u8003\u8651\u63d0\u793a\u91cd\u65b0\u8bbe\u8ba1\u3002\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee3\u7406\u7cfb\u7edf\u7684\u6a21\u5757\u5316\uff0c\u4ee5\u5b9e\u73b0\u7a33\u5b9a\u548c\u53ef\u6269\u5c55\u7684\u73b0\u5b9e\u95ee\u9898\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21924", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21924", "abs": "https://arxiv.org/abs/2506.21924", "authors": ["Zhao Jin", "Rong-Cheng Tu", "Jingyi Liao", "Wenhao Sun", "Xiao Luo", "Shunyu Liu", "Dacheng Tao"], "title": "SPAZER: Spatial-Semantic Progressive Reasoning Agent for Zero-shot 3D Visual Grounding", "comment": null, "summary": "3D Visual Grounding (3DVG) aims to localize target objects within a 3D scene\nbased on natural language queries. To alleviate the reliance on costly 3D\ntraining data, recent studies have explored zero-shot 3DVG by leveraging the\nextensive knowledge and powerful reasoning capabilities of pre-trained LLMs and\nVLMs. However, existing paradigms tend to emphasize either spatial (3D-based)\nor semantic (2D-based) understanding, limiting their effectiveness in complex\nreal-world applications. In this work, we introduce SPAZER - a VLM-driven agent\nthat combines both modalities in a progressive reasoning framework. It first\nholistically analyzes the scene and produces a 3D rendering from the optimal\nviewpoint. Based on this, anchor-guided candidate screening is conducted to\nperform a coarse-level localization of potential objects. Furthermore,\nleveraging retrieved relevant 2D camera images, 3D-2D joint decision-making is\nefficiently performed to determine the best-matching object. By bridging\nspatial and semantic reasoning neural streams, SPAZER achieves robust zero-shot\ngrounding without training on 3D-labeled data. Extensive experiments on\nScanRefer and Nr3D benchmarks demonstrate that SPAZER significantly outperforms\nprevious state-of-the-art zero-shot methods, achieving notable gains of 9.0%\nand 10.9% in accuracy.", "AI": {"tldr": "SPAZER\u662f\u4e00\u4e2aVLM\u9a71\u52a8\u7684\u4ee3\u7406\uff0c\u5b83\u7ed3\u5408\u4e863D\u548c2D\u6a21\u6001\uff0c\u4ee5\u5b9e\u73b0\u9c81\u68d2\u7684\u96f6\u6837\u672c3D\u89c6\u89c9\u5b9a\u4f4d\uff0c\u5e76\u5728ScanRefer\u548cNr3D\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u8303\u4f8b\u503e\u5411\u4e8e\u5f3a\u8c03\u7a7a\u95f4\uff08\u57fa\u4e8e3D\uff09\u6216\u8bed\u4e49\uff08\u57fa\u4e8e2D\uff09\u7406\u89e3\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u590d\u6742\u7684\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002\u4e3a\u4e86\u51cf\u8f7b\u5bf9\u6602\u8d35\u76843D\u8bad\u7ec3\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u6700\u8fd1\u7684\u7814\u7a76\u63a2\u7d22\u4e86\u96f6\u6837\u672c3DVG\uff0c\u5229\u7528\u4e86\u9884\u8bad\u7ec3\u7684LLM\u548cVLM\u7684\u5e7f\u6cdb\u77e5\u8bc6\u548c\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86SPAZER\uff0c\u4e00\u4e2aVLM\u9a71\u52a8\u7684\u4ee3\u7406\uff0c\u5b83\u5728\u4e00\u4e2a\u6e10\u8fdb\u5f0f\u63a8\u7406\u6846\u67b6\u4e2d\u7ed3\u5408\u4e86\u4e24\u79cd\u6a21\u5f0f\u3002\u5b83\u9996\u5148\u6574\u4f53\u5206\u6790\u573a\u666f\uff0c\u5e76\u4ece\u6700\u4f73\u89c6\u70b9\u751f\u62103D\u6e32\u67d3\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u8fdb\u884c\u951a\u5f15\u5bfc\u7684\u5019\u9009\u7b5b\u9009\uff0c\u4ee5\u6267\u884c\u6f5c\u5728\u5bf9\u8c61\u7684\u7c97\u7565\u5b9a\u4f4d\u3002\u6b64\u5916\uff0c\u5229\u7528\u68c0\u7d22\u5230\u7684\u76f8\u51732D\u76f8\u673a\u56fe\u50cf\uff0c\u6709\u6548\u5730\u6267\u884c3D-2D\u8054\u5408\u51b3\u7b56\uff0c\u4ee5\u786e\u5b9a\u6700\u4f73\u5339\u914d\u5bf9\u8c61\u3002", "result": "SPAZER\u5728ScanRefer\u548cNr3D\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u7684\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u5206\u522b\u63d0\u9ad8\u4e869.0%\u548c10.9%\u3002", "conclusion": "SPAZER\u901a\u8fc7\u6865\u63a5\u7a7a\u95f4\u548c\u8bed\u4e49\u63a8\u7406\u795e\u7ecf\u6d41\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u96f6\u6837\u672c\u5b9a\u4f4d\uff0c\u65e0\u9700\u57283D\u6807\u8bb0\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5728ScanRefer\u548cNr3D\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u7684\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u5206\u522b\u63d0\u9ad8\u4e869.0%\u548c10.9%\u3002"}}
{"id": "2506.21590", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21590", "abs": "https://arxiv.org/abs/2506.21590", "authors": ["Junqi Jiang", "Tom Bewley", "Salim I. Amoukou", "Francesco Leofante", "Antonio Rago", "Saumitra Mishra", "Francesca Toni"], "title": "Representation Consistency for Accurate and Coherent LLM Answer Aggregation", "comment": null, "summary": "Test-time scaling improves large language models' (LLMs) performance by\nallocating more compute budget during inference. To achieve this, existing\nmethods often require intricate modifications to prompting and sampling\nstrategies. In this work, we introduce representation consistency (RC), a\ntest-time scaling method for aggregating answers drawn from multiple candidate\nresponses of an LLM regardless of how they were generated, including variations\nin prompt phrasing and sampling strategy. RC enhances answer aggregation by not\nonly considering the number of occurrences of each answer in the candidate\nresponse set, but also the consistency of the model's internal activations\nwhile generating the set of responses leading to each answer. These activations\ncan be either dense (raw model activations) or sparse (encoded via pretrained\nsparse autoencoders). Our rationale is that if the model's representations of\nmultiple responses converging on the same answer are highly variable, this\nanswer is more likely to be the result of incoherent reasoning and should be\ndown-weighted during aggregation. Importantly, our method only uses cached\nactivations and lightweight similarity computations and requires no additional\nmodel queries. Through experiments with four open-source LLMs and four\nreasoning datasets, we validate the effectiveness of RC for improving task\nperformance during inference, with consistent accuracy improvements (up to 4%)\nover strong test-time scaling baselines. We also show that consistency in the\nsparse activation signals aligns well with the common notion of coherent\nreasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\uff08RC\uff09\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u8003\u8651\u6a21\u578b\u5185\u90e8\u6fc0\u6d3b\u7684\u4e00\u81f4\u6027\u6765\u805a\u5408LLM\u7684\u591a\u4e2a\u5019\u9009\u54cd\u5e94\uff0c\u4ece\u800c\u63d0\u9ad8\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5bf9\u63d0\u793a\u548c\u62bd\u6837\u7b56\u7565\u8fdb\u884c\u590d\u6742\u7684\u4fee\u6539\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u8868\u5f81\u4e00\u81f4\u6027\uff08RC\uff09\u7684\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\uff0c\u7528\u4e8e\u805a\u5408\u6765\u81eaLLM\u7684\u591a\u4e2a\u5019\u9009\u54cd\u5e94\u7684\u7b54\u6848\uff0c\u8be5\u65b9\u6cd5\u8003\u8651\u4e86\u6a21\u578b\u5728\u751f\u6210\u54cd\u5e94\u96c6\u65f6\u5185\u90e8\u6fc0\u6d3b\u7684\u4e00\u81f4\u6027\uff0c\u901a\u8fc7\u7f13\u5b58\u6fc0\u6d3b\u548c\u8f7b\u91cf\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u5b9e\u73b0\uff0c\u65e0\u9700\u989d\u5916\u6a21\u578b\u67e5\u8be2\u3002", "result": "RC\u65b9\u6cd5\u5728\u56db\u4e2a\u5f00\u6e90LLM\u548c\u56db\u4e2a\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u63d0\u9ad8\u4efb\u52a1\u6027\u80fd\uff0c\u4e14\u7a00\u758f\u6fc0\u6d3b\u4fe1\u53f7\u7684\u4e00\u81f4\u6027\u4e0e\u8fde\u8d2f\u63a8\u7406\u7684\u6982\u5ff5\u76f8\u7b26\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0cRC\u65b9\u6cd5\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u80fd\u6709\u6548\u63d0\u9ad8\u4efb\u52a1\u6027\u80fd\uff0c\u76f8\u6bd4\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u6301\u7eed\u63d0\u5347\u9ad8\u8fbe4%\u3002\u7a00\u758f\u6fc0\u6d3b\u4fe1\u53f7\u7684\u4e00\u81f4\u6027\u4e0e\u8fde\u8d2f\u63a8\u7406\u7684\u6982\u5ff5\u76f8\u7b26\u3002"}}
{"id": "2506.22190", "categories": ["cs.LG", "cs.IT", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2506.22190", "abs": "https://arxiv.org/abs/2506.22190", "authors": ["Xiaobo Zhao", "Aaron Hurst", "Panagiotis Karras", "Daniel E. Lucani"], "title": "dreaMLearning: Data Compression Assisted Machine Learning", "comment": "18 pages, 11 figures", "summary": "Despite rapid advancements, machine learning, particularly deep learning, is\nhindered by the need for large amounts of labeled data to learn meaningful\npatterns without overfitting and immense demands for computation and storage,\nwhich motivate research into architectures that can achieve good performance\nwith fewer resources. This paper introduces dreaMLearning, a novel framework\nthat enables learning from compressed data without decompression, built upon\nEntropy-based Generalized Deduplication (EntroGeDe), an entropy-driven lossless\ncompression method that consolidates information into a compact set of\nrepresentative samples. DreaMLearning accommodates a wide range of data types,\ntasks, and model architectures. Extensive experiments on regression and\nclassification tasks with tabular and image data demonstrate that dreaMLearning\naccelerates training by up to 8.8x, reduces memory usage by 10x, and cuts\nstorage by 42%, with a minimal impact on model performance. These advancements\nenhance diverse ML applications, including distributed and federated learning,\nand tinyML on resource-constrained edge devices, unlocking new possibilities\nfor efficient and scalable learning.", "AI": {"tldr": "dreaMLearning \u662f\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u5b83\u652f\u6301\u4ece\u538b\u7f29\u6570\u636e\u4e2d\u5b66\u4e60\u800c\u65e0\u9700\u89e3\u538b\u7f29\uff0c\u4ece\u800c\u52a0\u5feb\u8bad\u7ec3\u901f\u5ea6\uff0c\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u91cf\u548c\u5b58\u50a8\u7a7a\u95f4\uff0c\u5e76\u5c3d\u91cf\u51cf\u5c11\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\uff0c\u7279\u522b\u662f\u6df1\u5ea6\u5b66\u4e60\uff0c\u53d7\u5230\u4ee5\u4e0b\u56e0\u7d20\u7684\u963b\u788d\uff1a\u9700\u8981\u5927\u91cf\u6807\u8bb0\u6570\u636e\u6765\u5b66\u4e60\u6709\u610f\u4e49\u7684\u6a21\u5f0f\u800c\u4e0d\u4f1a\u8fc7\u5ea6\u62df\u5408\uff0c\u4ee5\u53ca\u5bf9\u8ba1\u7b97\u548c\u5b58\u50a8\u7684\u5de8\u5927\u9700\u6c42\uff0c\u8fd9\u4fc3\u4f7f\u4eba\u4eec\u7814\u7a76\u80fd\u591f\u4ee5\u66f4\u5c11\u7684\u8d44\u6e90\u5b9e\u73b0\u826f\u597d\u6027\u80fd\u7684\u67b6\u6784\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6 dreaMLearning\uff0c\u8be5\u6846\u67b6\u652f\u6301\u4ece\u538b\u7f29\u6570\u636e\u4e2d\u5b66\u4e60\u800c\u65e0\u9700\u89e3\u538b\u7f29\uff0c\u8be5\u6846\u67b6\u5efa\u7acb\u5728\u57fa\u4e8e\u71b5\u7684\u5e7f\u4e49\u91cd\u590d\u6570\u636e\u5220\u9664 (EntroGeDe) \u4e4b\u4e0a\uff0cEntroGeDe \u662f\u4e00\u79cd\u71b5\u9a71\u52a8\u7684\u65e0\u635f\u538b\u7f29\u65b9\u6cd5\uff0c\u53ef\u5c06\u4fe1\u606f\u6574\u5408\u5230\u4e00\u7ec4\u7d27\u51d1\u7684\u4ee3\u8868\u6027\u6837\u672c\u4e2d\u3002", "result": "\u5bf9\u8868\u683c\u548c\u56fe\u50cf\u6570\u636e\u7684\u56de\u5f52\u548c\u5206\u7c7b\u4efb\u52a1\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cdreaMLearning \u5c06\u8bad\u7ec3\u901f\u5ea6\u63d0\u9ad8\u4e86 8.8 \u500d\uff0c\u51cf\u5c11\u4e86 10 \u500d\u7684\u5185\u5b58\u4f7f\u7528\u91cf\uff0c\u5e76\u5c06\u5b58\u50a8\u7a7a\u95f4\u51cf\u5c11\u4e86 42%\uff0c\u800c\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u5f88\u5c0f\u3002", "conclusion": "dreaMLearning\u901a\u8fc7\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u548c\u5b58\u50a8\u7a7a\u95f4\uff0c\u540c\u65f6\u5c3d\u91cf\u51cf\u5c11\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u5305\u62ec\u5206\u5e03\u5f0f\u548c\u8054\u90a6\u5b66\u4e60\u4ee5\u53ca\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684 tinyML \u5728\u5185\u7684\u5404\u79cd ML \u5e94\u7528\uff0c\u4ece\u800c\u4e3a\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u5b66\u4e60\u89e3\u9501\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2506.21925", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21925", "abs": "https://arxiv.org/abs/2506.21925", "authors": ["Liu Yang", "Huiyu Duan", "Jiarui Wang", "Jing Liu", "Menghan Hu", "Xiongkuo Min", "Guangtao Zhai", "Patrick Le Callet"], "title": "Quality Assessment and Distortion-aware Saliency Prediction for AI-Generated Omnidirectional Images", "comment": null, "summary": "With the rapid advancement of Artificial Intelligence Generated Content\n(AIGC) techniques, AI generated images (AIGIs) have attracted widespread\nattention, among which AI generated omnidirectional images (AIGODIs) hold\nsignificant potential for Virtual Reality (VR) and Augmented Reality (AR)\napplications. AI generated omnidirectional images exhibit unique quality\nissues, however, research on the quality assessment and optimization of\nAI-generated omnidirectional images is still lacking. To this end, this work\nfirst studies the quality assessment and distortion-aware saliency prediction\nproblems for AIGODIs, and further presents a corresponding optimization\nprocess. Specifically, we first establish a comprehensive database to reflect\nhuman feedback for AI-generated omnidirectionals, termed OHF2024, which\nincludes both subjective quality ratings evaluated from three perspectives and\ndistortion-aware salient regions. Based on the constructed OHF2024 database, we\npropose two models with shared encoders based on the BLIP-2 model to evaluate\nthe human visual experience and predict distortion-aware saliency for\nAI-generated omnidirectional images, which are named as BLIP2OIQA and\nBLIP2OISal, respectively. Finally, based on the proposed models, we present an\nautomatic optimization process that utilizes the predicted visual experience\nscores and distortion regions to further enhance the visual quality of an\nAI-generated omnidirectional image. Extensive experiments show that our\nBLIP2OIQA model and BLIP2OISal model achieve state-of-the-art (SOTA) results in\nthe human visual experience evaluation task and the distortion-aware saliency\nprediction task for AI generated omnidirectional images, and can be effectively\nused in the optimization process. The database and codes will be released on\nhttps://github.com/IntMeGroup/AIGCOIQA to facilitate future research.", "AI": {"tldr": "\u7814\u7a76\u4e86 AI \u751f\u6210\u5168\u5411\u56fe\u50cf\u7684\u8d28\u91cf\u8bc4\u4f30\u548c\u5931\u771f\u611f\u77e5\u663e\u7740\u6027\u9884\u6d4b\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5e94\u7684\u4f18\u5316\u6d41\u7a0b\u3002", "motivation": "AI \u751f\u6210\u5168\u5411\u56fe\u50cf\u5728\u865a\u62df\u73b0\u5b9e\u548c\u589e\u5f3a\u73b0\u5b9e\u5e94\u7528\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5bf9\u5176\u8d28\u91cf\u8bc4\u4f30\u548c\u4f18\u5316\u7814\u7a76\u4ecd\u7136\u7f3a\u4e4f\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u4e2a\u57fa\u4e8e BLIP-2 \u6a21\u578b\u7684\u5171\u4eab\u7f16\u7801\u5668\u6a21\u578b BLIP2OIQA \u548c BLIP2OISal\uff0c\u4ee5\u8bc4\u4f30\u4eba\u7c7b\u89c6\u89c9\u4f53\u9a8c\u5e76\u9884\u6d4b AI \u751f\u6210\u5168\u5411\u56fe\u50cf\u7684\u5931\u771f\u611f\u77e5\u663e\u7740\u6027\u3002", "result": "BLIP2OIQA \u548c BLIP2OISal \u6a21\u578b\u5728\u4eba\u7c7b\u89c6\u89c9\u4f53\u9a8c\u8bc4\u4f30\u548c\u5931\u771f\u611f\u77e5\u663e\u7740\u6027\u9884\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5e76\u4e14\u53ef\u4ee5\u6709\u6548\u5730\u7528\u4e8e\u4f18\u5316\u8fc7\u7a0b\u3002", "conclusion": "BLIP2OIQA \u548c BLIP2OISal \u6a21\u578b\u5728 AI \u751f\u6210\u5168\u5411\u56fe\u50cf\u7684\u4eba\u7c7b\u89c6\u89c9\u4f53\u9a8c\u8bc4\u4f30\u548c\u5931\u771f\u611f\u77e5\u663e\u7740\u6027\u9884\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5e76\u4e14\u53ef\u4ee5\u6709\u6548\u5730\u7528\u4e8e\u4f18\u5316\u8fc7\u7a0b\u3002"}}
{"id": "2506.21591", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21591", "abs": "https://arxiv.org/abs/2506.21591", "authors": ["Shaoyu Dou", "Yutian Shen", "Mofan Chen", "Zixuan Wang", "Jiajie Xu", "Qi Guo", "Kailai Shao", "Chao Chen", "Haixiang Hu", "Haibo Shi", "Min Min", "Liwen Zhang"], "title": "FinEval-KR: A Financial Domain Evaluation Framework for Large Language Models' Knowledge and Reasoning", "comment": "Submitted to EMNLP 2025, 27 pages, 20 figures", "summary": "Large Language Models (LLMs) demonstrate significant potential but face\nchallenges in complex financial reasoning tasks requiring both domain knowledge\nand sophisticated reasoning. Current evaluation benchmarks often fall short by\nnot decoupling these capabilities indicators from single task performance and\nlack root cause analysis for task failure. To address this, we introduce\nFinEval-KR, a novel evaluation framework for decoupling and quantifying LLMs'\nknowledge and reasoning abilities independently, proposing distinct knowledge\nscore and reasoning score metrics. Inspired by cognitive science, we further\npropose a cognitive score based on Bloom's taxonomy to analyze capabilities in\nreasoning tasks across different cognitive levels. We also release a new\nopen-source Chinese financial reasoning dataset covering 22 subfields to\nsupport reproducible research and further advancements in financial reasoning.\nOur experimental results reveal that LLM reasoning ability and higher-order\ncognitive ability are the core factors influencing reasoning accuracy. We also\nspecifically find that even top models still face a bottleneck with knowledge\napplication. Furthermore, our analysis shows that specialized financial LLMs\ngenerally lag behind the top general large models across multiple metrics.", "AI": {"tldr": "\u63d0\u51fa\u4e86FinEval-KR\u6846\u67b6\u6765\u8bc4\u4f30LLM\u5728\u91d1\u878d\u9886\u57df\u7684\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\uff0cLLM\u7684\u63a8\u7406\u548c\u8ba4\u77e5\u80fd\u529b\u662f\u5173\u952e\uff0c\u9876\u7ea7\u6a21\u578b\u5728\u77e5\u8bc6\u5e94\u7528\u4e0a\u4ecd\u6709\u74f6\u9888\uff0c\u4e13\u4e1a\u91d1\u878dLLM\u8868\u73b0\u4e0d\u5982\u901a\u7528LLM\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u663e\u793a\u51fa\u5de8\u5927\u7684\u6f5c\u529b\uff0c\u4f46\u5728\u9700\u8981\u9886\u57df\u77e5\u8bc6\u548c\u590d\u6742\u63a8\u7406\u7684\u590d\u6742\u91d1\u878d\u63a8\u7406\u4efb\u52a1\u4e2d\u9762\u4e34\u6311\u6218\u3002\u76ee\u524d\u7684\u8bc4\u4f30\u57fa\u51c6\u901a\u5e38\u4e0d\u8db3\uff0c\u56e0\u4e3a\u5b83\u4eec\u6ca1\u6709\u5c06\u8fd9\u4e9b\u80fd\u529b\u6307\u6807\u4e0e\u5355\u4e00\u4efb\u52a1\u6027\u80fd\u5206\u79bb\uff0c\u5e76\u4e14\u7f3a\u4e4f\u4efb\u52a1\u5931\u8d25\u7684\u6839\u672c\u539f\u56e0\u5206\u6790\u3002", "method": "\u63d0\u51fa\u4e86FinEval-KR\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u8026\u548c\u91cf\u5316LLM\u7684\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e0d\u540c\u7684\u77e5\u8bc6\u8bc4\u5206\u548c\u63a8\u7406\u8bc4\u5206\u6307\u6807\u3002\u53d7\u5230\u4e86\u8ba4\u77e5\u79d1\u5b66\u7684\u542f\u53d1\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u5e03\u9c81\u59c6\u5206\u7c7b\u6cd5\u7684\u8ba4\u77e5\u8bc4\u5206\uff0c\u4ee5\u5206\u6790\u4e0d\u540c\u8ba4\u77e5\u6c34\u5e73\u7684\u63a8\u7406\u4efb\u52a1\u80fd\u529b\u3002", "result": "LLM\u7684\u63a8\u7406\u80fd\u529b\u548c\u66f4\u9ad8\u9636\u7684\u8ba4\u77e5\u80fd\u529b\u662f\u5f71\u54cd\u63a8\u7406\u51c6\u786e\u6027\u7684\u6838\u5fc3\u56e0\u7d20\u3002\u5373\u4f7f\u662f\u9876\u7ea7\u6a21\u578b\u5728\u77e5\u8bc6\u5e94\u7528\u65b9\u9762\u4ecd\u7136\u9762\u4e34\u74f6\u9888\u3002\u4e13\u4e1a\u91d1\u878dLLM\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u901a\u5e38\u843d\u540e\u4e8e\u9876\u7ea7\u901a\u7528\u5927\u6a21\u578b\u3002", "conclusion": "LLM\u7684\u63a8\u7406\u80fd\u529b\u548c\u66f4\u9ad8\u9636\u7684\u8ba4\u77e5\u80fd\u529b\u662f\u5f71\u54cd\u63a8\u7406\u51c6\u786e\u6027\u7684\u6838\u5fc3\u56e0\u7d20\u3002\u5373\u4f7f\u662f\u9876\u7ea7\u6a21\u578b\u5728\u77e5\u8bc6\u5e94\u7528\u65b9\u9762\u4ecd\u7136\u9762\u4e34\u74f6\u9888\u3002\u4e13\u4e1a\u91d1\u878dLLM\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u901a\u5e38\u843d\u540e\u4e8e\u9876\u7ea7\u901a\u7528\u5927\u6a21\u578b\u3002"}}
{"id": "2506.21945", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21945", "abs": "https://arxiv.org/abs/2506.21945", "authors": ["Naftaly Wambugu", "Ruisheng Wang", "Bo Guo", "Tianshu Yu", "Sheng Xu", "Mohammed Elhassan"], "title": "SDRNET: Stacked Deep Residual Network for Accurate Semantic Segmentation of Fine-Resolution Remotely Sensed Images", "comment": null, "summary": "Land cover maps generated from semantic segmentation of high-resolution\nremotely sensed images have drawn mucon in the photogrammetry and remote\nsensing research community. Currently, massive fine-resolution remotely sensed\n(FRRS) images acquired by improving sensing and imaging technologies become\navailable. However, accurate semantic segmentation of such FRRS images is\ngreatly affected by substantial class disparities, the invisibility of key\nground objects due to occlusion, and object size variation. Despite the\nextraordinary potential in deep convolutional neural networks (DCNNs) in image\nfeature learning and representation, extracting sufficient features from FRRS\nimages for accurate semantic segmentation is still challenging. These\nchallenges demand the deep learning models to learn robust features and\ngenerate sufficient feature descriptors. Specifically, learning\nmulti-contextual features to guarantee adequate coverage of varied object sizes\nfrom the ground scene and harnessing global-local contexts to overcome class\ndisparities challenge even profound networks. Deeper networks significantly\nlose spatial details due to gradual downsampling processes resulting in poor\nsegmentation results and coarse boundaries. This article presents a stacked\ndeep residual network (SDRNet) for semantic segmentation from FRRS images. The\nproposed framework utilizes two stacked encoder-decoder networks to harness\nlong-range semantics yet preserve spatial information and dilated residual\nblocks (DRB) between each encoder and decoder network to capture sufficient\nglobal dependencies thus improving segmentation performance. Our experimental\nresults obtained using the ISPRS Vaihingen and Potsdam datasets demonstrate\nthat the SDRNet performs effectively and competitively against current DCNNs in\nsemantic segmentation.", "AI": {"tldr": "SDRNet\u662f\u4e00\u79cd\u7528\u4e8eFRRS\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u7684\u5806\u53e0\u6df1\u5ea6\u6b8b\u5dee\u7f51\u7edc\uff0c\u5b83\u4f18\u4e8e\u73b0\u6709\u7684DCNN\u3002", "motivation": "\u7cbe\u786e\u7684\u8bed\u4e49\u5206\u5272\u53d7\u5230\u7c7b\u5dee\u5f02\u3001\u906e\u6321\u548c\u5bf9\u8c61\u5927\u5c0f\u53d8\u5316\u7684\u5f71\u54cd\u3002\u4ece FRRS \u56fe\u50cf\u4e2d\u63d0\u53d6\u8db3\u591f\u7684\u7279\u5f81\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u66f4\u6df1\u7684\u7f51\u7edc\u7531\u4e8e\u9010\u6e10\u7684\u4e0b\u91c7\u6837\u8fc7\u7a0b\u800c\u663e\u7740\u4e22\u5931\u7a7a\u95f4\u7ec6\u8282\uff0c\u4ece\u800c\u5bfc\u81f4\u8f83\u5dee\u7684\u5206\u5272\u7ed3\u679c\u548c\u7c97\u7cd9\u7684\u8fb9\u754c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5806\u53e0\u7684\u6df1\u5ea6\u6b8b\u5dee\u7f51\u7edc (SDRNet)\uff0c\u7528\u4e8e\u4ece FRRS \u56fe\u50cf\u4e2d\u8fdb\u884c\u8bed\u4e49\u5206\u5272\u3002\u8be5\u6846\u67b6\u5229\u7528\u4e24\u4e2a\u5806\u53e0\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f51\u7edc\u6765\u5229\u7528\u8fdc\u7a0b\u8bed\u4e49\uff0c\u540c\u65f6\u4fdd\u7559\u7a7a\u95f4\u4fe1\u606f\uff0c\u5e76\u5728\u6bcf\u4e2a\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7f51\u7edc\u4e4b\u95f4\u4f7f\u7528\u6269\u5f20\u6b8b\u5dee\u5757 (DRB) \u6765\u6355\u83b7\u8db3\u591f\u7684\u5168\u5c40\u4f9d\u8d56\u6027\uff0c\u4ece\u800c\u63d0\u9ad8\u5206\u5272\u6027\u80fd\u3002", "result": "\u5728 ISPRS Vaihingen \u548c Potsdam \u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u7684\u7ed3\u679c\u8868\u660e\uff0cSDRNet \u5728\u8bed\u4e49\u5206\u5272\u65b9\u9762\u8868\u73b0\u6709\u6548\u4e14\u4e0e\u5f53\u524d\u7684 DCNN \u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "SDRNet\u5728\u8bed\u4e49\u5206\u5272\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u5f53\u524d\u7684DCNN\u3002"}}
{"id": "2506.21592", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21592", "abs": "https://arxiv.org/abs/2506.21592", "authors": ["Tinh Nguyen", "Minh Khue Phan Tran"], "title": "SignBart -- New approach with the skeleton sequence for Isolated Sign language Recognition", "comment": null, "summary": "Sign language recognition is crucial for individuals with hearing impairments\nto break communication barriers. However, previous approaches have had to\nchoose between efficiency and accuracy. Such as RNNs, LSTMs, and GCNs, had\nproblems with vanishing gradients and high computational costs. Despite\nimproving performance, transformer-based methods were not commonly used. This\nstudy presents a new novel SLR approach that overcomes the challenge of\nindependently extracting meaningful information from the x and y coordinates of\nskeleton sequences, which traditional models often treat as inseparable. By\nutilizing an encoder-decoder of BART architecture, the model independently\nencodes the x and y coordinates, while Cross-Attention ensures their\ninterrelation is maintained. With only 749,888 parameters, the model achieves\n96.04% accuracy on the LSA-64 dataset, significantly outperforming previous\nmodels with over one million parameters. The model also demonstrates excellent\nperformance and generalization across WLASL and ASL-Citizen datasets. Ablation\nstudies underscore the importance of coordinate projection, normalization, and\nusing multiple skeleton components for boosting model efficacy. This study\noffers a reliable and effective approach for sign language recognition, with\nstrong potential for enhancing accessibility tools for the deaf and hard of\nhearing.", "AI": {"tldr": "A new sign language recognition (SLR) approach based on BART architecture achieves high accuracy with fewer parameters by independently encoding x and y coordinates of skeleton sequences and using cross-attention.", "motivation": "Sign language recognition is crucial for individuals with hearing impairments to break communication barriers. However, previous approaches have had to choose between efficiency and accuracy. Such as RNNs, LSTMs, and GCNs, had problems with vanishing gradients and high computational costs. Despite improving performance, transformer-based methods were not commonly used.", "method": "Utilizing an encoder-decoder of BART architecture, the model independently encodes the x and y coordinates, while Cross-Attention ensures their interrelation is maintained.", "result": "With only 749,888 parameters, the model achieves 96.04% accuracy on the LSA-64 dataset, significantly outperforming previous models with over one million parameters. The model also demonstrates excellent performance and generalization across WLASL and ASL-Citizen datasets. Ablation studies underscore the importance of coordinate projection, normalization, and using multiple skeleton components for boosting model efficacy.", "conclusion": "This study offers a reliable and effective approach for sign language recognition, with strong potential for enhancing accessibility tools for the deaf and hard of hearing."}}
{"id": "2506.22200", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22200", "abs": "https://arxiv.org/abs/2506.22200", "authors": ["Chen Wang", "Lai Wei", "Yanzhi Zhang", "Chenyang Shao", "Zedong Dan", "Weiran Huang", "Yue Wang", "Yuzhi Zhang"], "title": "EFRame: Deeper Reasoning via Exploration-Filtering-Replay Reinforcement Learning Framework", "comment": null, "summary": "Recent advances in reinforcement learning (RL) have significantly enhanced\nthe reasoning capabilities of large language models (LLMs). Group Relative\nPolicy Optimization (GRPO), an efficient variant of PPO that lowers RL's\ncomputational cost, still faces limited exploration, low sample efficiency and\ninstability, constraining its performance on complex reasoning tasks. To\naddress these limitations, we introduce EFRame, an Exploration-Filtering-Replay\nframework that systematically augments GRPO along three critical dimensions.\nEFRame performs additional rollouts to explore high-quality trajectories,\napplies online filtering to eliminate low-quality samples that introduce noise\nand variance, and leverages experience replay to repeatedly exploit rare but\ninformative samples. EFRame establishes a complete and stable learning cycle,\nguiding the model through a structured transition from exploration to\nconvergence. Our experiments across a variety of reasoning benchmarks\ndemonstrate that EFRame not only improves the robustness and efficiency of\ntraining, but also enables access to deeper reasoning capabilities that remain\nunattainable under vanilla GRPO. Furthermore, EFRame enables a more\nfine-grained categorization of training samples, allowing for a deeper analysis\nof how different types of samples contribute to the learning process in RL. Our\ncode is available at https://github.com/597358816/EFRame.", "AI": {"tldr": "EFRame improves GRPO's performance on complex reasoning tasks by addressing its limitations in exploration, sample efficiency, and stability.", "motivation": "GRPO faces limited exploration, low sample efficiency, and instability, constraining its performance on complex reasoning tasks.", "method": "EFRame, an Exploration-Filtering-Replay framework that augments GRPO with additional rollouts, online filtering, and experience replay.", "result": "EFRame improves the robustness and efficiency of training and enables access to deeper reasoning capabilities. EFRame enables a more fine-grained categorization of training samples.", "conclusion": "EFRame improves training robustness and efficiency, enabling access to deeper reasoning capabilities compared to vanilla GRPO. It also allows for a more fine-grained categorization of training samples."}}
{"id": "2506.21957", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21957", "abs": "https://arxiv.org/abs/2506.21957", "authors": ["Yixin Zha", "Chuxin Wang", "Wenfei Yang", "Tianzhu Zhang"], "title": "Exploring Semantic Masked Autoencoder for Self-supervised Point Cloud Understanding", "comment": "Accepted by IJCAI 2025", "summary": "Point cloud understanding aims to acquire robust and general feature\nrepresentations from unlabeled data. Masked point modeling-based methods have\nrecently shown significant performance across various downstream tasks. These\npre-training methods rely on random masking strategies to establish the\nperception of point clouds by restoring corrupted point cloud inputs, which\nleads to the failure of capturing reasonable semantic relationships by the\nself-supervised models. To address this issue, we propose Semantic Masked\nAutoencoder, which comprises two main components: a prototype-based component\nsemantic modeling module and a component semantic-enhanced masking strategy.\nSpecifically, in the component semantic modeling module, we design a component\nsemantic guidance mechanism to direct a set of learnable prototypes in\ncapturing the semantics of different components from objects. Leveraging these\nprototypes, we develop a component semantic-enhanced masking strategy that\naddresses the limitations of random masking in effectively covering complete\ncomponent structures. Furthermore, we introduce a component semantic-enhanced\nprompt-tuning strategy, which further leverages these prototypes to improve the\nperformance of pre-trained models in downstream tasks. Extensive experiments\nconducted on datasets such as ScanObjectNN, ModelNet40, and ShapeNetPart\ndemonstrate the effectiveness of our proposed modules.", "AI": {"tldr": "This paper proposes Semantic Masked Autoencoder to address the issue of failing to capture reasonable semantic relationships by the self-supervised models.", "motivation": "These pre-training methods rely on random masking strategies to establish the perception of point clouds by restoring corrupted point cloud inputs, which leads to the failure of capturing reasonable semantic relationships by the self-supervised models.", "method": "Semantic Masked Autoencoder, which comprises two main components: a prototype-based component semantic modeling module and a component semantic-enhanced masking strategy.", "result": "the introduction of a component semantic-enhanced prompt-tuning strategy, which further leverages these prototypes to improve the performance of pre-trained models in downstream tasks.", "conclusion": "Extensive experiments conducted on datasets such as ScanObjectNN, ModelNet40, and ShapeNetPart demonstrate the effectiveness of our proposed modules."}}
{"id": "2506.21594", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21594", "abs": "https://arxiv.org/abs/2506.21594", "authors": ["Ahmed M. Adly", "Mostafa Samy", "Amr Fawzy"], "title": "Gazal-R1: Achieving State-of-the-Art Medical Reasoning with Parameter-Efficient Two-Stage Training", "comment": null, "summary": "We present Gazal-R1, a 32-billion-parameter language model that achieves\nstate-of-the-art performance in medical reasoning while providing transparent,\nstep-by-step explanations for clinical decision-making. Built upon Qwen3 32B,\nour model demonstrates that strategic training can enable mid-sized models to\noutperform significantly larger counterparts in specialized domains. We\ndeveloped a novel two-stage training pipeline: first, supervised fine-tuning on\na carefully curated dataset of 107,033 synthetic medical reasoning examples\nthat teaches structured clinical thinking, enhanced by advanced\nparameter-efficient techniques including Weight-Decomposed Low-Rank Adaptation\n(DoRA) and Rank-Stabilized LoRA (rsLoRA); second, reinforcement learning using\nGroup Relative Policy Optimization (GRPO) with a sophisticated multi-component\nreward system that refines accuracy, format adherence, and reasoning quality.\nGazal-R1 achieves exceptional performance across medical benchmarks, scoring\n87.1% on MedQA, 81.6% on MMLU Pro (Medical), and 79.6% on PubMedQA, surpassing\nmodels up to 12x larger. Beyond its strong empirical results, this work\nprovides detailed insights into the challenges of training reasoning-capable\nmodels in specialized domains, including issues with reward hacking, training\ninstability, and the fundamental tension between factual recall and detailed\nreasoning. Our methodology offers a reproducible framework for developing\nhigh-capability, domain-specific language models that balance performance,\nefficiency, and explainability.", "AI": {"tldr": "Gazal-R1, a 32B model, outperforms larger models in medical reasoning using a novel two-stage training pipeline and GRPO, achieving state-of-the-art results on medical benchmarks.", "motivation": "demonstrates that strategic training can enable mid-sized models to outperform significantly larger counterparts in specialized domains", "method": "a novel two-stage training pipeline: first, supervised fine-tuning on a carefully curated dataset; second, reinforcement learning using Group Relative Policy Optimization (GRPO) with a sophisticated multi-component reward system", "result": "achieves state-of-the-art performance in medical reasoning while providing transparent, step-by-step explanations for clinical decision-making. Gazal-R1 achieves exceptional performance across medical benchmarks, scoring 87.1% on MedQA, 81.6% on MMLU Pro (Medical), and 79.6% on PubMedQA", "conclusion": "Gazal-R1 achieves exceptional performance across medical benchmarks, surpassing models up to 12x larger, and offers a reproducible framework for developing high-capability, domain-specific language models."}}
{"id": "2506.22253", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22253", "abs": "https://arxiv.org/abs/2506.22253", "authors": ["Shunta Nonaga", "Koji Tabata", "Yuta Mizuno", "Tamiki Komatsuzaki"], "title": "Risk-Averse Best Arm Set Identification with Fixed Budget and Fixed Confidence", "comment": null, "summary": "Decision making under uncertain environments in the maximization of expected\nreward while minimizing its risk is one of the ubiquitous problems in many\nsubjects. Here, we introduce a novel problem setting in stochastic bandit\noptimization that jointly addresses two critical aspects of decision-making:\nmaximizing expected reward and minimizing associated uncertainty, quantified\nvia the mean-variance(MV) criterion. Unlike traditional bandit formulations\nthat focus solely on expected returns, our objective is to efficiently and\naccurately identify the Pareto-optimal set of arms that strikes the best\ntrade-off between expected performance and risk. We propose a unified\nmeta-algorithmic framework capable of operating under both fixed-confidence and\nfixed-budget regimes, achieved through adaptive design of confidence intervals\ntailored to each scenario using the same sample exploration strategy. We\nprovide theoretical guarantees on the correctness of the returned solutions in\nboth settings. To complement this theoretical analysis, we conduct extensive\nempirical evaluations across synthetic benchmarks, demonstrating that our\napproach outperforms existing methods in terms of both accuracy and sample\nefficiency, highlighting its broad applicability to risk-aware decision-making\ntasks in uncertain environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u968f\u673abandit\u4f18\u5316\u95ee\u9898\u8bbe\u7f6e\uff0c\u8be5\u8bbe\u7f6e\u5171\u540c\u89e3\u51b3\u4e86\u51b3\u7b56\u7684\u4e24\u4e2a\u5173\u952e\u65b9\u9762\uff1a\u6700\u5927\u5316\u9884\u671f\u56de\u62a5\u548c\u6700\u5c0f\u5316\u76f8\u5173\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e0b\uff0c\u5728\u6700\u5927\u5316\u9884\u671f\u56de\u62a5\u7684\u540c\u65f6\u6700\u5c0f\u5316\u98ce\u9669\u7684\u51b3\u7b56\u662f\u8bb8\u591a\u5b66\u79d1\u4e2d\u666e\u904d\u5b58\u5728\u7684\u95ee\u9898\u4e4b\u4e00\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u968f\u673abandit\u4f18\u5316\u95ee\u9898\u8bbe\u7f6e\uff0c\u8be5\u8bbe\u7f6e\u5171\u540c\u89e3\u51b3\u4e86\u51b3\u7b56\u7684\u4e24\u4e2a\u5173\u952e\u65b9\u9762\uff1a\u6700\u5927\u5316\u9884\u671f\u56de\u62a5\u548c\u6700\u5c0f\u5316\u76f8\u5173\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u5747\u503c-\u65b9\u5dee\uff08MV\uff09\u6807\u51c6\u8fdb\u884c\u91cf\u5316\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u5143\u7b97\u6cd5\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5728\u56fa\u5b9a\u7f6e\u4fe1\u5ea6\u548c\u56fa\u5b9a\u9884\u7b97\u5236\u5ea6\u4e0b\u8fd0\u884c\uff0c\u8fd9\u662f\u901a\u8fc7\u4f7f\u7528\u76f8\u540c\u7684\u6837\u672c\u63a2\u7d22\u7b56\u7565\u4e3a\u6bcf\u79cd\u60c5\u51b5\u91cf\u8eab\u5b9a\u5236\u7684\u81ea\u9002\u5e94\u7f6e\u4fe1\u533a\u95f4\u8bbe\u8ba1\u6765\u5b9e\u73b0\u7684\u3002", "result": "\u6211\u4eec\u4e3a\u4e24\u79cd\u73af\u5883\u4e2d\u7684\u8fd4\u56de\u89e3\u51b3\u65b9\u6848\u7684\u6b63\u786e\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u3002\u4e3a\u4e86\u8865\u5145\u8fd9\u4e00\u7406\u8bba\u5206\u6790\uff0c\u6211\u4eec\u8de8\u5408\u6210\u57fa\u51c6\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u8bc1\u8bc4\u4f30\uff0c", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6837\u672c\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7a81\u51fa\u4e86\u5176\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u98ce\u9669\u611f\u77e5\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2506.21975", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21975", "abs": "https://arxiv.org/abs/2506.21975", "authors": ["Meng Yu", "Te Cui", "Qitong Chu", "Wenjie Song", "Yi Yang", "Yufeng Yue"], "title": "TASeg: Text-aware RGB-T Semantic Segmentation based on Fine-tuning Vision Foundation Models", "comment": "6 pages, accepted for publication in lEEE/RSJ international\n  Conference on Intelligent Robots and Systems (lROS 2025)", "summary": "Reliable semantic segmentation of open environments is essential for\nintelligent systems, yet significant problems remain: 1) Existing RGB-T\nsemantic segmentation models mainly rely on low-level visual features and lack\nhigh-level textual information, which struggle with accurate segmentation when\ncategories share similar visual characteristics. 2) While SAM excels in\ninstance-level segmentation, integrating it with thermal images and text is\nhindered by modality heterogeneity and computational inefficiency. To address\nthese, we propose TASeg, a text-aware RGB-T segmentation framework by using\nLow-Rank Adaptation (LoRA) fine-tuning technology to adapt vision foundation\nmodels. Specifically, we propose a Dynamic Feature Fusion Module (DFFM) in the\nimage encoder, which effectively merges features from multiple visual\nmodalities while freezing SAM's original transformer blocks. Additionally, we\nincorporate CLIP-generated text embeddings in the mask decoder to enable\nsemantic alignment, which further rectifies the classification error and\nimproves the semantic understanding accuracy. Experimental results across\ndiverse datasets demonstrate that our method achieves superior performance in\nchallenging scenarios with fewer trainable parameters.", "AI": {"tldr": "TASeg: a text-aware RGB-T segmentation framework", "motivation": "Existing RGB-T semantic segmentation models mainly rely on low-level visual features and lack high-level textual information, which struggle with accurate segmentation when categories share similar visual characteristics.  integrating SAM with thermal images and text is hindered by modality heterogeneity and computational inefficiency", "method": "a text-aware RGB-T segmentation framework by using Low-Rank Adaptation (LoRA) fine-tuning technology to adapt vision foundation models. Specifically, a Dynamic Feature Fusion Module (DFFM) in the image encoder merges features from multiple visual modalities while freezing SAM's original transformer blocks.  CLIP-generated text embeddings in the mask decoder enable semantic alignment", "result": "achieves superior performance in challenging scenarios", "conclusion": "achieves superior performance in challenging scenarios with fewer trainable parameters"}}
{"id": "2506.21595", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21595", "abs": "https://arxiv.org/abs/2506.21595", "authors": ["Jinpyo Kim", "Gyeongje Cho", "Chanwoo Park", "Jongwon Park", "Jongmin Kim", "Yeonkyoun So", "Jaejin Lee"], "title": "Thunder-LLM: Efficiently Adapting LLMs to Korean with Minimal Resources", "comment": "Submitted to ARR 2025 May cycle", "summary": "Since state-of-the-art LLMs often underperform in languages other than\nEnglish or Chinese, improving the capability of LLMs in new languages has\nbecome an essential task. Moreover, LLMs' entire end-to-end training process\nremains largely unknown to the public due to proprietary reasons, technical\ncomplexity, inconsistent documentation, and ethical considerations. The\ncomplete picture remains a closely guarded secret within the industry. This\npaper presents methods to adapt an existing English-based LLM to Korean in a\nlow-budget scenario. We describe the entire end-to-end process: collecting\nKorean datasets, preprocessing the data, training the model, creating\ndownstream benchmarks, and conducting evaluations. The evaluation results\nindicate that our method can effectively and cost-efficiently add new language\ncapabilities to existing LLMs. Our new bilingual models, Thunder-LLM and\nThunder-LLM-Ins, achieve superior Korean performance compared to\nstate-of-the-art models while utilizing minimal data and computational\nresources. We share our comprehensive experience and make the code publicly\navailable.", "AI": {"tldr": "Adapting English LLMs to Korean with minimal resources achieves state-of-the-art performance. End-to-end process and code are shared.", "motivation": "State-of-the-art LLMs often underperform in languages other than English or Chinese. The end-to-end training process of LLMs is largely unknown.", "method": "Adaptation of an existing English-based LLM to Korean in a low-budget scenario, including data collection, preprocessing, model training, benchmark creation, and evaluation.", "result": "The new bilingual models, Thunder-LLM and Thunder-LLM-Ins, achieve superior Korean performance compared to state-of-the-art models while utilizing minimal data and computational resources.", "conclusion": "The study demonstrates a cost-efficient method to adapt existing LLMs to new languages, specifically Korean, achieving state-of-the-art performance with minimal resources. The authors share their experience and code publicly."}}
{"id": "2506.22255", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22255", "abs": "https://arxiv.org/abs/2506.22255", "authors": ["Maciej Stefaniak", "Micha\u0142 Krutul", "Jan Ma\u0142a\u015bnicki", "Maciej Pi\u00f3ro", "Jakub Krajewski", "Sebastian Jaszczur", "Marek Cygan", "Kamil Adamczewski", "Jan Ludziejewski"], "title": "Projected Compression: Trainable Projection for Efficient Transformer Compression", "comment": null, "summary": "Large language models have steadily increased in size to achieve improved\nperformance; however, this growth has also led to greater inference time and\ncomputational demands. Consequently, there is rising interest in model size\nreduction methods. To address this issue, we propose Projected Compression, a\nnovel model compression technique, that reduces model weights by utilizing\nprojection modules. Specifically, we first train additional trainable\nprojections weights and preserve access to all the original model parameters.\nSubsequently, these projections are merged into a lower-dimensional product\nmatrix, resulting in a reduced-size standard Transformer-based model. Unlike\nalternative approaches that require additional computational overhead, our\nmethod matches the base model's per-token computation step in FLOPs.\nExperimental results show that Projected Compression outperforms the comparable\nhard pruning and retraining approach on higher quality models. Moreover, the\nperformance margin scales well with the number of tokens.", "AI": {"tldr": "Projected Compression, a novel model compression technique, reduces model size using projection modules without additional computational overhead, outperforming hard pruning and retraining.", "motivation": "Large language models' increasing size leads to greater inference time and computational demands, increasing interest in model size reduction methods.", "method": "Projected Compression: uses projection modules to reduce model weights by merging trainable projections into a lower-dimensional product matrix.", "result": "Projected Compression matches the base model's per-token computation step in FLOPs and outperforms comparable hard pruning and retraining.", "conclusion": "Projected Compression outperforms hard pruning and retraining, especially with more tokens."}}
{"id": "2506.21980", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21980", "abs": "https://arxiv.org/abs/2506.21980", "authors": ["Biao Wang", "Wenwen Li"], "title": "R1-Track: Direct Application of MLLMs to Visual Object Tracking via Reinforcement Learning", "comment": "7 pages, 2 figures", "summary": "Visual single object tracking aims to continuously localize and estimate the\nscale of a target in subsequent video frames, given only its initial state in\nthe first frame. This task has traditionally been framed as a template matching\nproblem, evolving through major phases including correlation filters,\ntwo-stream networks, and one-stream networks with significant progress\nachieved. However, these methods typically require explicit classification and\nregression modeling, depend on supervised training with large-scale datasets,\nand are limited to the single task of tracking, lacking flexibility. In recent\nyears, multi-modal large language models (MLLMs) have advanced rapidly.\nOpen-source models like Qwen2.5-VL, a flagship MLLMs with strong foundational\ncapabilities, demonstrate excellent performance in grounding tasks. This has\nspurred interest in applying such models directly to visual tracking. However,\nexperiments reveal that Qwen2.5-VL struggles with template matching between\nimage pairs (i.e., tracking tasks). Inspired by deepseek-R1, we fine-tuned\nQwen2.5-VL using the group relative policy optimization (GRPO) reinforcement\nlearning method on a small-scale dataset with a rule-based reward function. The\nresulting model, R1-Track, achieved notable performance on the GOT-10k\nbenchmark. R1-Track supports flexible initialization via bounding boxes or text\ndescriptions while retaining most of the original model's general capabilities.\nAnd we further discuss potential improvements for R1-Track. This rough\ntechnical report summarizes our findings as of May 2025.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86R1-Track\uff0c\u901a\u8fc7\u5bf9Qwen2.5-VL\u8fdb\u884c\u5fae\u8c03\uff0c\u4f7f\u5176\u5728\u76ee\u6807\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u901a\u7528\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u89c6\u89c9\u5355\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\u9700\u8981\u663e\u5f0f\u7684\u5206\u7c7b\u548c\u56de\u5f52\u5efa\u6a21\uff0c\u4f9d\u8d56\u4e8e\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u76d1\u7763\u8bad\u7ec3\uff0c\u5e76\u4e14\u4ec5\u9650\u4e8e\u8ddf\u8e2a\u8fd9\u4e00\u5355\u4e00\u4efb\u52a1\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\u3002\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u8fd1\u5e74\u6765\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u5728\u56fe\u50cf\u5bf9\u4e4b\u95f4\u7684\u6a21\u677f\u5339\u914d\uff08\u5373\u8ddf\u8e2a\u4efb\u52a1\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u4f7f\u7528\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u4e00\u4e2a\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528\u57fa\u4e8e\u89c4\u5219\u7684\u5956\u52b1\u51fd\u6570\u5bf9Qwen2.5-VL\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u63d0\u51fa\u7684R1-Track\u6a21\u578b\u652f\u6301\u901a\u8fc7\u8fb9\u754c\u6846\u6216\u6587\u672c\u63cf\u8ff0\u8fdb\u884c\u7075\u6d3b\u521d\u59cb\u5316\uff0c\u5e76\u5728GOT-10k\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u3002", "conclusion": "R1-Track\u5728GOT-10k\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u539f\u59cb\u6a21\u578b\u7684\u5927\u90e8\u5206\u901a\u7528\u80fd\u529b\uff0c\u5e76\u8ba8\u8bba\u4e86R1-Track\u7684\u6f5c\u5728\u6539\u8fdb\u3002"}}
{"id": "2506.22295", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22295", "abs": "https://arxiv.org/abs/2506.22295", "authors": ["Zhengyun Cheng", "Changhao Wang", "Guanwen Zhang", "Yi Xu", "Wei Zhou", "Xiangyang Ji"], "title": "Score-Based Model for Low-Rank Tensor Recovery", "comment": null, "summary": "Low-rank tensor decompositions (TDs) provide an effective framework for\nmultiway data analysis. Traditional TD methods rely on predefined structural\nassumptions, such as CP or Tucker decompositions. From a probabilistic\nperspective, these can be viewed as using Dirac delta distributions to model\nthe relationships between shared factors and the low-rank tensor. However, such\nprior knowledge is rarely available in practical scenarios, particularly\nregarding the optimal rank structure and contraction rules. The optimization\nprocedures based on fixed contraction rules are complex, and approximations\nmade during these processes often lead to accuracy loss. To address this issue,\nwe propose a score-based model that eliminates the need for predefined\nstructural or distributional assumptions, enabling the learning of\ncompatibility between tensors and shared factors. Specifically, a neural\nnetwork is designed to learn the energy function, which is optimized via score\nmatching to capture the gradient of the joint log-probability of tensor entries\nand shared factors. Our method allows for modeling structures and distributions\nbeyond the Dirac delta assumption. Moreover, integrating the block coordinate\ndescent (BCD) algorithm with the proposed smooth regularization enables the\nmodel to perform both tensor completion and denoising. Experimental results\ndemonstrate significant performance improvements across various tensor types,\nincluding sparse and continuous-time tensors, as well as visual data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u6570\u7684\u5f20\u91cf\u5206\u89e3\u6a21\u578b\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u5047\u8bbe\uff0c\u901a\u8fc7\u5b66\u4e60\u5f20\u91cf\u548c\u5171\u4eab\u56e0\u5b50\u4e4b\u95f4\u7684\u517c\u5bb9\u6027\uff0c\u5b9e\u73b0\u4e86\u5f20\u91cf\u8865\u5168\u548c\u53bb\u566a\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u5f20\u91cf\u5206\u89e3\uff08TD\uff09\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u9884\u5b9a\u4e49\u7684\u7ed3\u6784\u5047\u8bbe\uff0c\u4f8b\u5982CP\u6216Tucker\u5206\u89e3\u3002\u4ece\u6982\u7387\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u8fd9\u4e9b\u53ef\u4ee5\u88ab\u89c6\u4e3a\u4f7f\u7528\u72c4\u62c9\u514b\u03b4\u5206\u5e03\u6765\u5efa\u6a21\u5171\u4eab\u56e0\u5b50\u548c\u4f4e\u79e9\u5f20\u91cf\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u5148\u9a8c\u77e5\u8bc6\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u5f88\u5c11\u53ef\u7528\uff0c\u5c24\u5176\u662f\u5728\u6700\u4f73\u79e9\u7ed3\u6784\u548c\u6536\u7f29\u89c4\u5219\u65b9\u9762\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u6765\u5b66\u4e60\u80fd\u91cf\u51fd\u6570\uff0c\u8be5\u51fd\u6570\u901a\u8fc7\u5206\u6570\u5339\u914d\u8fdb\u884c\u4f18\u5316\uff0c\u4ee5\u6355\u83b7\u5f20\u91cf\u6761\u76ee\u548c\u5171\u4eab\u56e0\u5b50\u7684\u8054\u5408\u5bf9\u6570\u6982\u7387\u7684\u68af\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5404\u79cd\u5f20\u91cf\u7c7b\u578b\uff08\u5305\u62ec\u7a00\u758f\u548c\u8fde\u7eed\u65f6\u95f4\u5f20\u91cf\uff09\u4ee5\u53ca\u89c6\u89c9\u6570\u636e\u4e2d\uff0c\u6027\u80fd\u5f97\u5230\u663e\u7740\u63d0\u9ad8\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u6570\u7684\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u65e0\u9700\u9884\u5b9a\u4e49\u7684\u7ed3\u6784\u6216\u5206\u5e03\u5047\u8bbe\uff0c\u4ece\u800c\u80fd\u591f\u5b66\u4e60\u5f20\u91cf\u548c\u5171\u4eab\u56e0\u5b50\u4e4b\u95f4\u7684\u517c\u5bb9\u6027\u3002\u7ed3\u5408\u5757\u5750\u6807\u4e0b\u964d\uff08BCD\uff09\u7b97\u6cd5\u4e0e\u6240\u63d0\u51fa\u7684\u5e73\u6ed1\u6b63\u5219\u5316\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u6267\u884c\u5f20\u91cf\u8865\u5168\u548c\u53bb\u566a\u3002"}}
{"id": "2506.22007", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22007", "abs": "https://arxiv.org/abs/2506.22007", "authors": ["Liudi Yang", "Yang Bai", "George Eskandar", "Fengyi Shen", "Mohammad Altillawi", "Dong Chen", "Soumajit Majumder", "Ziyuan Liu", "Gitta Kutyniok", "Abhinav Valada"], "title": "RoboEnvision: A Long-Horizon Video Generation Model for Multi-Task Robot Manipulation", "comment": "8 pages, 6 figures", "summary": "We address the problem of generating long-horizon videos for robotic\nmanipulation tasks. Text-to-video diffusion models have made significant\nprogress in photorealism, language understanding, and motion generation but\nstruggle with long-horizon robotic tasks. Recent works use video diffusion\nmodels for high-quality simulation data and predictive rollouts in robot\nplanning. However, these works predict short sequences of the robot achieving\none task and employ an autoregressive paradigm to extend to the long horizon,\nleading to error accumulations in the generated video and in the execution. To\novercome these limitations, we propose a novel pipeline that bypasses the need\nfor autoregressive generation. We achieve this through a threefold\ncontribution: 1) we first decompose the high-level goals into smaller atomic\ntasks and generate keyframes aligned with these instructions. A second\ndiffusion model then interpolates between each of the two generated frames,\nachieving the long-horizon video. 2) We propose a semantics preserving\nattention module to maintain consistency between the keyframes. 3) We design a\nlightweight policy model to regress the robot joint states from generated\nvideos. Our approach achieves state-of-the-art results on two benchmarks in\nvideo quality and consistency while outperforming previous policy models on\nlong-horizon tasks.", "AI": {"tldr": "This paper introduces a new method for generating long-horizon videos for robotic manipulation tasks by decomposing goals into atomic tasks, generating keyframes, and using a diffusion model to interpolate between them, outperforming existing methods in video quality, consistency, and policy model performance.", "motivation": "Text-to-video diffusion models have made significant progress in photorealism, language understanding, and motion generation but struggle with long-horizon robotic tasks. Recent works use video diffusion models for high-quality simulation data and predictive rollouts in robot planning. However, these works predict short sequences of the robot achieving one task and employ an autoregressive paradigm to extend to the long horizon, leading to error accumulations in the generated video and in the execution.", "method": "we first decompose the high-level goals into smaller atomic tasks and generate keyframes aligned with these instructions. A second diffusion model then interpolates between each of the two generated frames, achieving the long-horizon video. 2) We propose a semantics preserving attention module to maintain consistency between the keyframes. 3) We design a lightweight policy model to regress the robot joint states from generated videos.", "result": "achieves state-of-the-art results on two benchmarks in video quality and consistency while outperforming previous policy models on long-horizon tasks.", "conclusion": "Our approach achieves state-of-the-art results on two benchmarks in video quality and consistency while outperforming previous policy models on long-horizon tasks."}}
{"id": "2506.22299", "categories": ["cs.LG", "cs.AI", "I.2"], "pdf": "https://arxiv.org/pdf/2506.22299", "abs": "https://arxiv.org/abs/2506.22299", "authors": ["Tao Liu", "Longlong Lin", "Yunfeng Yu", "Xi Ou", "Youan Zhang", "Zhiqiu Ye", "Tao Jia"], "title": "CoATA: Effective Co-Augmentation of Topology and Attribute for Graph Neural Networks", "comment": "icmr", "summary": "Graph Neural Networks (GNNs) have garnered substantial attention due to their\nremarkable capability in learning graph representations. However, real-world\ngraphs often exhibit substantial noise and incompleteness, which severely\ndegrades the performance of GNNs. Existing methods typically address this issue\nthrough single-dimensional augmentation, focusing either on refining topology\nstructures or perturbing node attributes, thereby overlooking the deeper\ninterplays between the two. To bridge this gap, this paper presents CoATA, a\ndual-channel GNN framework specifically designed for the Co-Augmentation of\nTopology and Attribute. Specifically, CoATA first propagates structural signals\nto enrich and denoise node attributes. Then, it projects the enhanced attribute\nspace into a node-attribute bipartite graph for further refinement or\nreconstruction of the underlying structure. Subsequently, CoATA introduces\ncontrastive learning, leveraging prototype alignment and consistency\nconstraints, to facilitate mutual corrections between the augmented and\noriginal graphs. Finally, extensive experiments on seven benchmark datasets\ndemonstrate that the proposed CoATA outperforms eleven state-of-the-art\nbaseline methods, showcasing its effectiveness in capturing the synergistic\nrelationship between topology and attributes.", "AI": {"tldr": "CoATA\u662f\u4e00\u79cd\u7528\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u53cc\u901a\u9053\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u540c\u589e\u5f3a\u62d3\u6251\u548c\u5c5e\u6027\u6765\u63d0\u9ad8\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u56fe\u901a\u5e38\u8868\u73b0\u51fa\u5927\u91cf\u7684\u566a\u58f0\u548c\u4e0d\u5b8c\u6574\u6027\uff0c\u8fd9\u4e25\u91cd\u964d\u4f4e\u4e86GNN\u7684\u6027\u80fd\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u5355\u7ef4\u589e\u5f3a\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u8981\u4e48\u4fa7\u91cd\u4e8e\u6539\u8fdb\u62d3\u6251\u7ed3\u6784\uff0c\u8981\u4e48\u6270\u52a8\u8282\u70b9\u5c5e\u6027\uff0c\u4ece\u800c\u5ffd\u7565\u4e86\u4e24\u8005\u4e4b\u95f4\u66f4\u6df1\u5c42\u6b21\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u901a\u9053GNN\u6846\u67b6CoATA\uff0c\u4e13\u4e3a\u62d3\u6251\u548c\u5c5e\u6027\u7684\u534f\u540c\u589e\u5f3a\u800c\u8bbe\u8ba1\u3002CoATA\u9996\u5148\u4f20\u64ad\u7ed3\u6784\u4fe1\u53f7\u4ee5\u4e30\u5bcc\u548c\u53bb\u566a\u8282\u70b9\u5c5e\u6027\u3002\u7136\u540e\uff0c\u5b83\u5c06\u589e\u5f3a\u7684\u5c5e\u6027\u7a7a\u95f4\u6295\u5f71\u5230\u8282\u70b9-\u5c5e\u6027\u4e8c\u90e8\u56fe\u4e2d\uff0c\u4ee5\u8fdb\u4e00\u6b65\u7ec6\u5316\u6216\u91cd\u5efa\u5e95\u5c42\u7ed3\u6784\u3002\u968f\u540e\uff0cCoATA\u5f15\u5165\u4e86\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5229\u7528\u539f\u578b\u5bf9\u9f50\u548c\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u4ee5\u4fc3\u8fdb\u589e\u5f3a\u56fe\u548c\u539f\u59cb\u56fe\u4e4b\u95f4\u7684\u76f8\u4e92\u6821\u6b63\u3002", "result": "\u63d0\u51fa\u7684CoATA\u4f18\u4e8e11\u4e2a\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "CoATA\u5728\u4e03\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e11\u4e2a\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u5728\u6355\u83b7\u62d3\u6251\u548c\u5c5e\u6027\u4e4b\u95f4\u7684\u534f\u540c\u5173\u7cfb\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.22015", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22015", "abs": "https://arxiv.org/abs/2506.22015", "authors": ["Sarthak Ketanbhai Modi", "Lim Zi Pong", "Shourya Kuchhal", "Yoshi Cao", "Yupeng Cheng", "Teo Yon Shin", "Lin Shang-Wei", "Zhiming Li"], "title": "Towards Universal & Efficient Model Compression via Exponential Torque Pruning", "comment": null, "summary": "The rapid growth in complexity and size of modern deep neural networks (DNNs)\nhas increased challenges related to computational costs and memory usage,\nspurring a growing interest in efficient model compression techniques. Previous\nstate-of-the-art approach proposes using a Torque-inspired regularization which\nforces the weights of neural modules around a selected pivot point. Whereas, we\nobserve that the pruning effect of this approach is far from perfect, as the\npost-trained network is still dense and also suffers from high accuracy drop.\nIn this work, we attribute such ineffectiveness to the default linear force\napplication scheme, which imposes inappropriate force on neural module of\ndifferent distances. To efficiently prune the redundant and distant modules\nwhile retaining those that are close and necessary for effective inference, in\nthis work, we propose Exponential Torque Pruning (ETP), which adopts an\nexponential force application scheme for regularization. Experimental results\non a broad range of domains demonstrate that, though being extremely simple,\nETP manages to achieve significantly higher compression rate than the previous\nstate-of-the-art pruning strategies with negligible accuracy drop.", "AI": {"tldr": "This paper proposes Exponential Torque Pruning (ETP) to efficiently prune redundant neural modules and achieve higher compression rates with negligible accuracy drop compared to previous methods.", "motivation": "The pruning effect of Torque-inspired regularization is far from perfect, as the post-trained network is still dense and also suffers from high accuracy drop, and the default linear force application scheme imposes inappropriate force on neural module of different distances.", "method": "Exponential Torque Pruning (ETP), which adopts an exponential force application scheme for regularization", "result": "ETP manages to achieve significantly higher compression rate than the previous state-of-the-art pruning strategies with negligible accuracy drop.", "conclusion": "ETP achieves higher compression rate than previous state-of-the-art pruning strategies with negligible accuracy drop."}}
{"id": "2506.22301", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22301", "abs": "https://arxiv.org/abs/2506.22301", "authors": ["Takumi Okuo", "Shinnosuke Matsuo", "Shota Harada", "Kiyohito Tanaka", "Ryoma Bise"], "title": "Weakly-Supervised Domain Adaptation with Proportion-Constrained Pseudo-Labeling", "comment": "Accepted at IJCNN2025", "summary": "Domain shift is a significant challenge in machine learning, particularly in\nmedical applications where data distributions differ across institutions due to\nvariations in data collection practices, equipment, and procedures. This can\ndegrade performance when models trained on source domain data are applied to\nthe target domain. Domain adaptation methods have been widely studied to\naddress this issue, but most struggle when class proportions between the source\nand target domains differ. In this paper, we propose a weakly-supervised domain\nadaptation method that leverages class proportion information from the target\ndomain, which is often accessible in medical datasets through prior knowledge\nor statistical reports. Our method assigns pseudo-labels to the unlabeled\ntarget data based on class proportion (called proportion-constrained\npseudo-labeling), improving performance without the need for additional\nannotations. Experiments on two endoscopic datasets demonstrate that our method\noutperforms semi-supervised domain adaptation techniques, even when 5% of the\ntarget domain is labeled. Additionally, the experimental results with noisy\nproportion labels highlight the robustness of our method, further demonstrating\nits effectiveness in real-world application scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f31\u76d1\u7763\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u76ee\u6807\u57df\u7684\u7c7b\u6bd4\u4f8b\u4fe1\u606f\u6765\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u57df\u504f\u79fb\u95ee\u9898\u3002", "motivation": "\u5728\u673a\u5668\u5b66\u4e60\u4e2d\uff0c\u57df\u504f\u79fb\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u533b\u7597\u5e94\u7528\u4e2d\uff0c\u7531\u4e8e\u6570\u636e\u6536\u96c6\u5b9e\u8df5\u3001\u8bbe\u5907\u548c\u7a0b\u5e8f\u7684\u53d8\u5316\uff0c\u4e0d\u540c\u673a\u6784\u7684\u6570\u636e\u5206\u5e03\u5b58\u5728\u5dee\u5f02\u3002\u5f53\u5728\u6e90\u57df\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u5e94\u7528\u4e8e\u76ee\u6807\u57df\u65f6\uff0c\u8fd9\u4f1a\u964d\u4f4e\u6027\u80fd\u3002\u5f53\u6e90\u57df\u548c\u76ee\u6807\u57df\u4e4b\u95f4\u7684\u7c7b\u6bd4\u4f8b\u4e0d\u540c\u65f6\uff0c\u5927\u591a\u6570\u57df\u9002\u5e94\u65b9\u6cd5\u90fd\u5b58\u5728\u95ee\u9898\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5f31\u76d1\u7763\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8e\u7c7b\u6bd4\u4f8b\u4fe1\u606f\u4e3a\u672a\u6807\u8bb0\u7684\u76ee\u6807\u6570\u636e\u5206\u914d\u4f2a\u6807\u7b7e\uff08\u79f0\u4e3a\u6bd4\u4f8b\u7ea6\u675f\u4f2a\u6807\u7b7e\uff09\u3002", "result": "\u5728\u4e24\u4e2a\u5185\u7aa5\u955c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u534a\u76d1\u7763\u57df\u9002\u5e94\u6280\u672f\uff0c\u5373\u4f7f\u5728\u6807\u8bb0\u4e86 5% \u7684\u76ee\u6807\u57df\u65f6\u4e5f\u662f\u5982\u6b64\u3002\u6b64\u5916\uff0c\u5e26\u6709\u566a\u58f0\u6bd4\u4f8b\u6807\u7b7e\u7684\u5b9e\u9a8c\u7ed3\u679c\u7a81\u51fa\u4e86\u8be5\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5f31\u76d1\u7763\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u6765\u81ea\u76ee\u6807\u57df\u7684\u7c7b\u6bd4\u4f8b\u4fe1\u606f\uff0c\u901a\u8fc7\u57fa\u4e8e\u7c7b\u6bd4\u4f8b\u7684\u4f2a\u6807\u7b7e\u6765\u63d0\u9ad8\u6027\u80fd\uff0c\u65e0\u9700\u989d\u5916\u7684\u6ce8\u91ca\u3002\u5728\u4e24\u4e2a\u5185\u7aa5\u955c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u6807\u8bb0\u4e86 5% \u7684\u76ee\u6807\u57df\u65f6\uff0c\u8be5\u65b9\u6cd5\u4e5f\u4f18\u4e8e\u534a\u76d1\u7763\u57df\u9002\u5e94\u6280\u672f\u3002"}}
{"id": "2506.22022", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22022", "abs": "https://arxiv.org/abs/2506.22022", "authors": ["Zhanyi Lu", "Yue Zhou"], "title": "Advancing Facial Stylization through Semantic Preservation Constraint and Pseudo-Paired Supervision", "comment": null, "summary": "Facial stylization aims to transform facial images into appealing,\nhigh-quality stylized portraits, with the critical challenge of accurately\nlearning the target style while maintaining content consistency with the\noriginal image. Although previous StyleGAN-based methods have made significant\nadvancements, the generated results still suffer from artifacts or insufficient\nfidelity to the source image. We argue that these issues stem from neglecting\nsemantic shift of the generator during stylization. Therefore, we propose a\nfacial stylization method that integrates semantic preservation constraint and\npseudo-paired supervision to enhance the content correspondence and improve the\nstylization effect. Additionally, we develop a methodology for creating\nmulti-level pseudo-paired datasets to implement supervisory constraint.\nFurthermore, building upon our facial stylization framework, we achieve more\nflexible multimodal and reference-guided stylization without complex network\narchitecture designs or additional training. Experimental results demonstrate\nthat our approach produces high-fidelity, aesthetically pleasing facial style\ntransfer that surpasses previous methods.", "AI": {"tldr": "Proposes a facial stylization method using semantic preservation and pseudo-paired supervision to improve content correspondence and stylization effect, outperforming previous methods.", "motivation": "Previous StyleGAN-based methods suffer from artifacts or insufficient fidelity to the source image due to neglecting semantic shift of the generator during stylization.", "method": "A facial stylization method that integrates semantic preservation constraint and pseudo-paired supervision.", "result": "Achieves more flexible multimodal and reference-guided stylization without complex network architecture designs or additional training. Demonstrates high-fidelity, aesthetically pleasing facial style transfer.", "conclusion": "The proposed approach produces high-fidelity, aesthetically pleasing facial style transfer that surpasses previous methods."}}
{"id": "2506.21602", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21602", "abs": "https://arxiv.org/abs/2506.21602", "authors": ["Xiaoyan Feng", "He Zhang", "Yanjun Zhang", "Leo Yu Zhang", "Shirui Pan"], "title": "BiMark: Unbiased Multilayer Watermarking for Large Language Models", "comment": "This paper is accepted by International Conference on Machine\n  Learning (ICML) 2025", "summary": "Recent advances in Large Language Models (LLMs) have raised urgent concerns\nabout LLM-generated text authenticity, prompting regulatory demands for\nreliable identification mechanisms. Although watermarking offers a promising\nsolution, existing approaches struggle to simultaneously achieve three critical\nrequirements: text quality preservation, model-agnostic detection, and message\nembedding capacity, which are crucial for practical implementation. To achieve\nthese goals, the key challenge lies in balancing the trade-off between text\nquality preservation and message embedding capacity. To address this challenge,\nwe propose BiMark, a novel watermarking framework that achieves these\nrequirements through three key innovations: (1) a bit-flip unbiased reweighting\nmechanism enabling model-agnostic detection, (2) a multilayer architecture\nenhancing detectability without compromising generation quality, and (3) an\ninformation encoding approach supporting multi-bit watermarking. Through\ntheoretical analysis and extensive experiments, we validate that, compared to\nstate-of-the-art multi-bit watermarking methods, BiMark achieves up to 30%\nhigher extraction rates for short texts while maintaining text quality\nindicated by lower perplexity, and performs comparably to non-watermarked text\non downstream tasks such as summarization and translation.", "AI": {"tldr": "BiMark, a novel watermarking framework that achieves text quality preservation, model-agnostic detection, and message embedding capacity.", "motivation": "urgent concerns about LLM-generated text authenticity, prompting regulatory demands for reliable identification mechanisms and existing approaches struggle to simultaneously achieve three critical requirements: text quality preservation, model-agnostic detection, and message embedding capacity", "method": "a bit-flip unbiased reweighting mechanism, a multilayer architecture and an information encoding approach", "result": "BiMark achieves up to 30% higher extraction rates for short texts while maintaining text quality indicated by lower perplexity, and performs comparably to non-watermarked text on downstream tasks such as summarization and translation.", "conclusion": "BiMark achieves up to 30% higher extraction rates for short texts while maintaining text quality and performs comparably to non-watermarked text on downstream tasks."}}
{"id": "2506.22304", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22304", "abs": "https://arxiv.org/abs/2506.22304", "authors": ["Erkan Turan", "Aristotelis Siozopoulos", "Maks Ovsjanikov"], "title": "Unfolding Generative Flows with Koopman Operators: Fast and Interpretable Sampling", "comment": null, "summary": "Conditional Flow Matching (CFM) offers a simulation-free framework for\ntraining continuous-time generative models, bridging diffusion and flow-based\napproaches. However, sampling from CFM still relies on numerically solving\nnon-linear ODEs which can be computationally expensive and difficult to\ninterpret. Recent alternatives address sampling speed via trajectory\nstraightening, mini-batch coupling or distillation. However, these methods\ntypically do not shed light on the underlying \\textit{structure} of the\ngenerative process. In this work, we propose to accelerate CFM and introduce an\ninterpretable representation of its dynamics by integrating Koopman operator\ntheory, which models non-linear flows as linear evolution in a learned space of\nobservables. We introduce a decoder-free Koopman-CFM architecture that learns\nan embedding where the generative dynamics become linear, enabling closed-form,\none-step sampling via matrix exponentiation. This results in significant\nspeedups over traditional CFM as demonstrated on controlled 2D datasets and\nreal-world benchmarks, MNIST, Fashion-MNIST (F-MNIST), and the Toronto Face\nDataset (TFD). Unlike previous methods, our approach leads to a well-structured\nKoopman generator, whose spectral properties, eigenvalues, and eigenfunctions\noffer principled tools for analyzing generative behavior such as temporal\nscaling, mode stability, and decomposition in Koopman latent space. By\ncombining sampling efficiency with analytical structure, Koopman-enhanced flow\nmatching offers a potential step toward fast and interpretable generative\nmodeling.", "AI": {"tldr": "The paper proposes Koopman-CFM, a method that accelerates Conditional Flow Matching (CFM) by integrating Koopman operator theory, enabling faster sampling and providing an interpretable representation of generative dynamics.", "motivation": "Sampling from CFM relies on numerically solving non-linear ODEs which can be computationally expensive and difficult to interpret. Recent alternatives address sampling speed via trajectory straightening, mini-batch coupling or distillation but do not shed light on the underlying structure of the generative process.", "method": "Integrating Koopman operator theory to model non-linear flows as linear evolution in a learned space of observables and introducing a decoder-free Koopman-CFM architecture that learns an embedding where the generative dynamics become linear, enabling closed-form, one-step sampling via matrix exponentiation.", "result": "Significant speedups over traditional CFM as demonstrated on controlled 2D datasets and real-world benchmarks, MNIST, Fashion-MNIST (F-MNIST), and the Toronto Face Dataset (TFD). The approach leads to a well-structured Koopman generator, whose spectral properties, eigenvalues, and eigenfunctions offer principled tools for analyzing generative behavior such as temporal scaling, mode stability, and decomposition in Koopman latent space.", "conclusion": "Koopman-enhanced flow matching combines sampling efficiency with analytical structure, offering a potential step toward fast and interpretable generative modeling."}}
{"id": "2506.22027", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22027", "abs": "https://arxiv.org/abs/2506.22027", "authors": ["Han Wang", "Shengyang Li", "Jian Yang", "Yuxuan Liu", "Yixuan Lv", "Zhuang Zhou"], "title": "Cross-modal Ship Re-Identification via Optical and SAR Imagery: A Novel Dataset and Method", "comment": "Accepted to ICCV 2025", "summary": "Detecting and tracking ground objects using earth observation imagery remains\na significant challenge in the field of remote sensing. Continuous maritime\nship tracking is crucial for applications such as maritime search and rescue,\nlaw enforcement, and shipping analysis. However, most current ship tracking\nmethods rely on geostationary satellites or video satellites. The former offer\nlow resolution and are susceptible to weather conditions, while the latter have\nshort filming durations and limited coverage areas, making them less suitable\nfor the real-world requirements of ship tracking. To address these limitations,\nwe present the Hybrid Optical and Synthetic Aperture Radar (SAR) Ship\nRe-Identification Dataset (HOSS ReID dataset), designed to evaluate the\neffectiveness of ship tracking using low-Earth orbit constellations of optical\nand SAR sensors. This approach ensures shorter re-imaging cycles and enables\nall-weather tracking. HOSS ReID dataset includes images of the same ship\ncaptured over extended periods under diverse conditions, using different\nsatellites of different modalities at varying times and angles. Furthermore, we\npropose a baseline method for cross-modal ship re-identification, TransOSS,\nwhich is built on the Vision Transformer architecture. It refines the patch\nembedding structure to better accommodate cross-modal tasks, incorporates\nadditional embeddings to introduce more reference information, and employs\ncontrastive learning to pre-train on large-scale optical-SAR image pairs,\nensuring the model's ability to extract modality-invariant features. Our\ndataset and baseline method are publicly available on\nhttps://github.com/Alioth2000/Hoss-ReID.", "AI": {"tldr": "Introduces HOSS ReID dataset and TransOSS method for improved ship tracking using optical and SAR satellite imagery.", "motivation": "Current ship tracking methods using geostationary or video satellites have limitations in resolution, weather dependency, filming duration, and coverage area, making them unsuitable for real-world ship tracking requirements.", "method": "The paper proposes a baseline method called TransOSS, built on the Vision Transformer architecture, which refines the patch embedding structure, incorporates additional embeddings, and employs contrastive learning.", "result": "The paper presents the HOSS ReID dataset and the TransOSS baseline method, both of which are publicly available.", "conclusion": "The paper introduces a new dataset (HOSS ReID) and a baseline method (TransOSS) for cross-modal ship re-identification, demonstrating a solution for ship tracking using low-Earth orbit constellations of optical and SAR sensors."}}
{"id": "2506.21603", "categories": ["cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21603", "abs": "https://arxiv.org/abs/2506.21603", "authors": ["Yenisel Plasencia-Cala\u00f1a"], "title": "Operationalizing Automated Essay Scoring: A Human-Aware Approach", "comment": null, "summary": "This paper explores the human-centric operationalization of Automated Essay\nScoring (AES) systems, addressing aspects beyond accuracy. We compare various\nmachine learning-based approaches with Large Language Models (LLMs) approaches,\nidentifying their strengths, similarities and differences. The study\ninvestigates key dimensions such as bias, robustness, and explainability,\nconsidered important for human-aware operationalization of AES systems. Our\nstudy shows that ML-based AES models outperform LLMs in accuracy but struggle\nwith explainability, whereas LLMs provide richer explanations. We also found\nthat both approaches struggle with bias and robustness to edge scores. By\nanalyzing these dimensions, the paper aims to identify challenges and\ntrade-offs between different methods, contributing to more reliable and\ntrustworthy AES methods.", "AI": {"tldr": "This paper compares ML-based AES models and LLMs, finding ML models more accurate but less explainable, and both struggling with bias and robustness.", "motivation": "explores the human-centric operationalization of Automated Essay Scoring (AES) systems, addressing aspects beyond accuracy", "method": "comparing various machine learning-based approaches with Large Language Models (LLMs) approaches", "result": "identifying their strengths, similarities and differences in bias, robustness, and explainability", "conclusion": "ML-based AES models outperform LLMs in accuracy but struggle with explainability, whereas LLMs provide richer explanations. Both approaches struggle with bias and robustness to edge scores."}}
{"id": "2506.22331", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2506.22331", "abs": "https://arxiv.org/abs/2506.22331", "authors": ["Adiba Ejaz", "Elias Bareinboim"], "title": "Less Greedy Equivalence Search", "comment": "35 total pages. 14 figures", "summary": "Greedy Equivalence Search (GES) is a classic score-based algorithm for causal\ndiscovery from observational data. In the sample limit, it recovers the Markov\nequivalence class of graphs that describe the data. Still, it faces two\nchallenges in practice: computational cost and finite-sample accuracy. In this\npaper, we develop Less Greedy Equivalence Search (LGES), a variant of GES that\nretains its theoretical guarantees while partially addressing these\nlimitations. LGES modifies the greedy step: rather than always applying the\nhighest-scoring insertion, it avoids edge insertions between variables for\nwhich the score implies some conditional independence. This more targeted\nsearch yields up to a \\(10\\)-fold speed-up and a substantial reduction in\nstructural error relative to GES. Moreover, LGES can guide the search using\nprior assumptions, while correcting these assumptions when contradicted by the\ndata. Finally, LGES can exploit interventional data to refine the learned\nobservational equivalence class. We prove that LGES recovers the true\nequivalence class in the sample limit from observational and interventional\ndata, even with misspecified prior assumptions. Experiments demonstrate that\nLGES outperforms GES and other baselines in speed, accuracy, and robustness to\nmisspecified assumptions. Our code is available at\nhttps://github.com/CausalAILab/lges.", "AI": {"tldr": "This paper introduces Less Greedy Equivalence Search (LGES), a faster and more accurate variant of GES for causal discovery. LGES addresses the limitations of GES by modifying the greedy step and exploiting interventional data.", "motivation": "GES faces two challenges in practice: computational cost and finite-sample accuracy.", "method": "A variant of GES that avoids edge insertions between variables for which the score implies some conditional independence.", "result": "LGES yields up to a 10-fold speed-up and a substantial reduction in structural error relative to GES. LGES can guide the search using prior assumptions, while correcting these assumptions when contradicted by the data. LGES can exploit interventional data to refine the learned observational equivalence class.", "conclusion": "LGES outperforms GES and other baselines in speed, accuracy, and robustness to misspecified assumptions. It recovers the true equivalence class in the sample limit from observational and interventional data, even with misspecified prior assumptions."}}
{"id": "2506.22032", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22032", "abs": "https://arxiv.org/abs/2506.22032", "authors": ["Jialei Chen", "Xu Zheng", "Danda Pani Paudel", "Luc Van Gool", "Hiroshi Murase", "Daisuke Deguchi"], "title": "Partial CLIP is Enough: Chimera-Seg for Zero-shot Semantic Segmentation", "comment": null, "summary": "Zero-shot Semantic Segmentation (ZSS) aims to segment both seen and unseen\nclasses using supervision from only seen classes. Beyond adaptation-based\nmethods, distillation-based approaches transfer vision-language alignment of\nvision-language model, e.g., CLIP, to segmentation models. However, such\nknowledge transfer remains challenging due to: (1) the difficulty of aligning\nvision-based features with the textual space, which requires combining spatial\nprecision with vision-language alignment; and (2) the semantic gap between\nCLIP's global representations and the local, fine-grained features of\nsegmentation models. To address challenge (1), we propose Chimera-Seg, which\nintegrates a segmentation backbone as the body and a CLIP-based semantic head\nas the head, like the Chimera in Greek mythology, combining spatial precision\nwith vision-language alignment. Specifically, Chimera-Seg comprises a trainable\nsegmentation model and a CLIP Semantic Head (CSH), which maps dense features\ninto the CLIP-aligned space. The CSH incorporates a frozen subnetwork and fixed\nprojection layers from the CLIP visual encoder, along with lightweight\ntrainable components. The partial module from CLIP visual encoder, paired with\nthe segmentation model, retains segmentation capability while easing the\nmapping to CLIP's semantic space. To address challenge (2), we propose\nSelective Global Distillation (SGD), which distills knowledge from dense\nfeatures exhibiting high similarity to the CLIP CLS token, while gradually\nreducing the number of features used for alignment as training progresses.\nBesides, we also use a Semantic Alignment Module (SAM) to further align dense\nvisual features with semantic embeddings extracted from the frozen CLIP text\nencoder. Experiments on two benchmarks show improvements of 0.9% and 1.2% in\nhIoU.", "AI": {"tldr": "propose Chimera-Seg and Selective Global Distillation (SGD) to improve zero-shot semantic segmentation (ZSS).", "motivation": "Knowledge transfer remains challenging due to: (1) the difficulty of aligning vision-based features with the textual space, which requires combining spatial precision with vision-language alignment; and (2) the semantic gap between CLIP's global representations and the local, fine-grained features of segmentation models.", "method": "propose Chimera-Seg, which integrates a segmentation backbone as the body and a CLIP-based semantic head as the head and Selective Global Distillation (SGD), which distills knowledge from dense features exhibiting high similarity to the CLIP CLS token, while gradually reducing the number of features used for alignment as training progresses. Besides, we also use a Semantic Alignment Module (SAM) to further align dense visual features with semantic embeddings extracted from the frozen CLIP text encoder.", "result": "Experiments on two benchmarks show improvements of 0.9% and 1.2% in hIoU.", "conclusion": "Experiments on two benchmarks show improvements of 0.9% and 1.2% in hIoU."}}
{"id": "2506.21605", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21605", "abs": "https://arxiv.org/abs/2506.21605", "authors": ["Haoran Tan", "Zeyu Zhang", "Chen Ma", "Xu Chen", "Quanyu Dai", "Zhenhua Dong"], "title": "MemBench: Towards More Comprehensive Evaluation on the Memory of LLM-based Agents", "comment": "17 pages, 5 figures. Accepted by ACL 2025 findings", "summary": "Recent works have highlighted the significance of memory mechanisms in\nLLM-based agents, which enable them to store observed information and adapt to\ndynamic environments. However, evaluating their memory capabilities still\nremains challenges. Previous evaluations are commonly limited by the diversity\nof memory levels and interactive scenarios. They also lack comprehensive\nmetrics to reflect the memory capabilities from multiple aspects. To address\nthese problems, in this paper, we construct a more comprehensive dataset and\nbenchmark to evaluate the memory capability of LLM-based agents. Our dataset\nincorporates factual memory and reflective memory as different levels, and\nproposes participation and observation as various interactive scenarios. Based\non our dataset, we present a benchmark, named MemBench, to evaluate the memory\ncapability of LLM-based agents from multiple aspects, including their\neffectiveness, efficiency, and capacity. To benefit the research community, we\nrelease our dataset and project at https://github.com/import-myself/Membench.", "AI": {"tldr": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a MemBench \u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8e LLM \u7684\u4ee3\u7406\u7684\u8bb0\u5fc6\u80fd\u529b\uff0c\u5305\u62ec\u6709\u6548\u6027\u3001\u6548\u7387\u548c\u5bb9\u91cf\u3002", "motivation": "\u5148\u524d\u5bf9\u8bb0\u5fc6\u80fd\u529b\u7684\u8bc4\u4f30\u901a\u5e38\u53d7\u5230\u8bb0\u5fc6\u6c34\u5e73\u548c\u4ea4\u4e92\u573a\u666f\u591a\u6837\u6027\u7684\u9650\u5236\u3002\u4ed6\u4eec\u4e5f\u7f3a\u4e4f\u5168\u9762\u7684\u6307\u6807\u6765\u53cd\u6620\u591a\u4e2a\u65b9\u9762\u7684\u8bb0\u5fc6\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u66f4\u5168\u9762\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6765\u8bc4\u4f30\u57fa\u4e8e LLM \u7684\u4ee3\u7406\u7684\u8bb0\u5fc6\u80fd\u529b\u3002\u6570\u636e\u96c6\u5305\u542b\u4e8b\u5b9e\u8bb0\u5fc6\u548c\u53cd\u601d\u8bb0\u5fc6\u4f5c\u4e3a\u4e0d\u540c\u7ea7\u522b\uff0c\u5e76\u63d0\u51fa\u53c2\u4e0e\u548c\u89c2\u5bdf\u4f5c\u4e3a\u5404\u79cd\u4ea4\u4e92\u573a\u666f\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u66f4\u5168\u9762\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8e LLM \u7684\u4ee3\u7406\u7684\u8bb0\u5fc6\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a MemBench \u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u4ece\u591a\u4e2a\u65b9\u9762\u8bc4\u4f30\u57fa\u4e8e LLM \u7684\u4ee3\u7406\u7684\u8bb0\u5fc6\u80fd\u529b\uff0c\u5305\u62ec\u5b83\u4eec\u7684\u6709\u6548\u6027\u3001\u6548\u7387\u548c\u5bb9\u91cf\u3002"}}
{"id": "2506.22342", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22342", "abs": "https://arxiv.org/abs/2506.22342", "authors": ["Zihan Guan", "Zhiyuan Zhao", "Fengwei Tian", "Dung Nguyen", "Payel Bhattacharjee", "Ravi Tandon", "B. Aditya Prakash", "Anil Vullikanti"], "title": "A Framework for Multi-source Privacy Preserving Epidemic Analysis", "comment": "17 pages, 6 figures", "summary": "It is now well understood that diverse datasets provide a lot of value in key\nepidemiology and public health analyses, such as forecasting and nowcasting,\ndevelopment of epidemic models, evaluation and design of interventions and\nresource allocation. Some of these datasets are often sensitive, and need\nadequate privacy protections. There are many models of privacy, but\nDifferential Privacy (DP) has become a de facto standard because of its strong\nguarantees, without making models about adversaries. In this paper, we develop\na framework the integrates deep learning and epidemic models to simultaneously\nperform epidemic forecasting and learning a mechanistic model of epidemic\nspread, while incorporating multiple datasets for these analyses, including\nsome with DP guarantees. We demonstrate our framework using a realistic but\nsynthetic financial dataset with DP; such a dataset has not been used in such\nepidemic analyses. We show that this dataset provides significant value in\nforecasting and learning an epidemic model, even when used with DP guarantees.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u5dee\u5206\u9690\u79c1\u7684\u6d41\u884c\u75c5\u9884\u6d4b\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u91d1\u878d\u6570\u636e\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u5728\u5173\u952e\u7684\u6d41\u884c\u75c5\u5b66\u548c\u516c\u5171\u536b\u751f\u5206\u6790\u4e2d\u63d0\u4f9b\u4e86\u5f88\u5927\u7684\u4ef7\u503c\uff0c\u4f8b\u5982\u9884\u6d4b\u548c\u73b0\u5728\u9884\u6d4b\u3001\u6d41\u884c\u75c5\u6a21\u578b\u7684\u5f00\u53d1\u3001\u5e72\u9884\u63aa\u65bd\u7684\u8bc4\u4f30\u548c\u8bbe\u8ba1\u4ee5\u53ca\u8d44\u6e90\u5206\u914d\u3002\u5176\u4e2d\u4e00\u4e9b\u6570\u636e\u96c6\u901a\u5e38\u662f\u654f\u611f\u7684\uff0c\u9700\u8981\u8db3\u591f\u7684\u9690\u79c1\u4fdd\u62a4\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6574\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u6d41\u884c\u75c5\u6a21\u578b\u7684\u6846\u67b6\uff0c\u4ee5\u540c\u65f6\u8fdb\u884c\u6d41\u884c\u75c5\u9884\u6d4b\u548c\u5b66\u4e60\u6d41\u884c\u75c5\u4f20\u64ad\u7684\u673a\u5236\u6a21\u578b\uff0c\u540c\u65f6\u6574\u5408\u591a\u4e2a\u6570\u636e\u96c6\u8fdb\u884c\u5206\u6790\uff0c\u5305\u62ec\u4e00\u4e9b\u5177\u6709DP\u4fdd\u8bc1\u7684\u6570\u636e\u96c6\u3002", "result": "\u4f7f\u7528\u5177\u6709DP\u7684\u73b0\u5b9e\u4f46\u5408\u6210\u7684\u91d1\u878d\u6570\u636e\u96c6\u6f14\u793a\u4e86\u8be5\u6846\u67b6\uff1b \u8fd9\u6837\u7684\u6570\u636e\u96c6\u5c1a\u672a\u7528\u4e8e\u6b64\u7c7b\u6d41\u884c\u75c5\u5206\u6790\u3002 \u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u5177\u6709DP\u4fdd\u8bc1\u7684\u60c5\u51b5\u4e0b\uff0c\u8be5\u6570\u636e\u96c6\u4e5f\u4e3a\u9884\u6d4b\u548c\u5b66\u4e60\u6d41\u884c\u75c5\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u4ef7\u503c\u3002", "conclusion": "\u8be5\u8bba\u6587\u8bc1\u660e\u4e86\u5373\u4f7f\u5728\u5177\u6709\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u4fdd\u8bc1\u7684\u60c5\u51b5\u4e0b\uff0c\u5408\u6210\u91d1\u878d\u6570\u636e\u96c6\u4e5f\u80fd\u4e3a\u75ab\u60c5\u9884\u6d4b\u548c\u5b66\u4e60\u6d41\u884c\u75c5\u6a21\u578b\u63d0\u4f9b\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2506.22044", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22044", "abs": "https://arxiv.org/abs/2506.22044", "authors": ["Hong Nie", "Fuyuan Cao", "Lu Chen", "Fengxin Chen", "Yuefeng Zou", "Jun Yu"], "title": "Few-Shot Identity Adaptation for 3D Talking Heads via Global Gaussian Field", "comment": null, "summary": "Reconstruction and rendering-based talking head synthesis methods achieve\nhigh-quality results with strong identity preservation but are limited by their\ndependence on identity-specific models. Each new identity requires training\nfrom scratch, incurring high computational costs and reduced scalability\ncompared to generative model-based approaches. To overcome this limitation, we\npropose FIAG, a novel 3D speaking head synthesis framework that enables\nefficient identity-specific adaptation using only a few training footage. FIAG\nincorporates Global Gaussian Field, which supports the representation of\nmultiple identities within a shared field, and Universal Motion Field, which\ncaptures the common motion dynamics across diverse identities. Benefiting from\nthe shared facial structure information encoded in the Global Gaussian Field\nand the general motion priors learned in the motion field, our framework\nenables rapid adaptation from canonical identity representations to specific\nones with minimal data. Extensive comparative and ablation experiments\ndemonstrate that our method outperforms existing state-of-the-art approaches,\nvalidating both the effectiveness and generalizability of the proposed\nframework. Code is available at: \\textit{https://github.com/gme-hong/FIAG}.", "AI": {"tldr": "FIAG \u662f\u4e00\u79cd\u65b0\u7684 3D \u8bf4\u8bdd\u5934\u50cf\u5408\u6210\u6846\u67b6\uff0c\u5b83\u53ef\u4ee5\u4f7f\u7528\u5c11\u91cf\u8bad\u7ec3\u955c\u5934\u5b9e\u73b0\u6709\u6548\u7684\u7279\u5b9a\u4e8e\u8eab\u4efd\u7684\u9002\u5e94\u3002", "motivation": "\u57fa\u4e8e\u91cd\u5efa\u548c\u6e32\u67d3\u7684\u8bf4\u8bdd\u5934\u50cf\u5408\u6210\u65b9\u6cd5\u867d\u7136\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u7ed3\u679c\u548c\u5f3a\u5927\u7684\u8eab\u4efd\u4fdd\u6301\uff0c\u4f46\u53d7\u5230\u5176\u5bf9\u7279\u5b9a\u4e8e\u8eab\u4efd\u7684\u6a21\u578b\u7684\u4f9d\u8d56\u6027\u7684\u9650\u5236\u3002\u4e0e\u57fa\u4e8e\u751f\u6210\u6a21\u578b\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6bcf\u4e2a\u65b0\u8eab\u4efd\u90fd\u9700\u8981\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\uff0c\u4ece\u800c\u5bfc\u81f4\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u964d\u4f4e\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684 3D \u8bf4\u8bdd\u5934\u50cf\u5408\u6210\u6846\u67b6 FIAG\uff0c\u8be5\u6846\u67b6\u652f\u6301\u5728\u5171\u4eab\u573a\u4e2d\u8868\u793a\u591a\u4e2a\u8eab\u4efd\u7684\u5168\u5c40\u9ad8\u65af\u573a\u548c\u6355\u83b7\u4e0d\u540c\u8eab\u4efd\u7684\u901a\u7528\u8fd0\u52a8\u52a8\u6001\u7684\u901a\u7528\u8fd0\u52a8\u573a\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u4ee5\u6700\u5c11\u7684\u6570\u636e\u4ece\u89c4\u8303\u8eab\u4efd\u8868\u793a\u5feb\u901f\u9002\u5e94\u5230\u7279\u5b9a\u8eab\u4efd\u8868\u793a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u6c34\u5e73\u7684\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u6027\u3002"}}
{"id": "2506.21606", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.21606", "abs": "https://arxiv.org/abs/2506.21606", "authors": ["Parham Pourdavood", "Michael Jacob", "Terrence Deacon"], "title": "Large Language Models as symbolic DNA of cultural dynamics", "comment": "28 pages, 1 figure", "summary": "This paper proposes a novel conceptualization of Large Language Models (LLMs)\nas externalized informational substrates that function analogously to DNA for\nhuman cultural dynamics. Rather than viewing LLMs as either autonomous\nintelligence or mere programmed mimicry, we argue they serve a broader role as\nrepositories that preserve compressed patterns of human symbolic\nexpression--\"fossils\" of meaningful dynamics that retain relational residues\nwithout their original living contexts. Crucially, these compressed patterns\nonly become meaningful through human reinterpretation, creating a recursive\nfeedback loop where they can be recombined and cycle back to ultimately\ncatalyze human creative processes. Through analysis of four universal\nfeatures--compression, decompression, externalization, and recursion--we\ndemonstrate that just as DNA emerged as a compressed and externalized medium\nfor preserving useful cellular dynamics without containing explicit reference\nto goal-directed physical processes, LLMs preserve useful regularities of human\nculture without containing understanding of embodied human experience.\nTherefore, we argue that LLMs' significance lies not in rivaling human\nintelligence, but in providing humanity a tool for self-reflection and playful\nhypothesis-generation in a low-stakes, simulated environment. This framework\npositions LLMs as tools for cultural evolvability, enabling humanity to\ngenerate novel hypotheses about itself while maintaining the human\ninterpretation necessary to ground these hypotheses in ongoing human aesthetics\nand norms.", "AI": {"tldr": "LLMs are like DNA for culture: they store compressed patterns that humans reinterpret, catalyzing creativity.", "motivation": "To conceptualize LLMs as externalized informational substrates, analogous to DNA, for human cultural dynamics.", "method": "Analysis of four universal features: compression, decompression, externalization, and recursion.", "result": "LLMs preserve regularities of human culture without understanding embodied human experience, providing a tool for self-reflection and hypothesis generation.", "conclusion": "LLMs are tools for cultural evolution, enabling hypothesis generation while requiring human interpretation to ground them in aesthetics and norms."}}
{"id": "2506.22365", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.22365", "abs": "https://arxiv.org/abs/2506.22365", "authors": ["Tao Li", "Haozhe Lei", "Mingsheng Yin", "Yaqi Hu"], "title": "Reinforcement Learning with Physics-Informed Symbolic Program Priors for Zero-Shot Wireless Indoor Navigation", "comment": "Spotlight paper at Reinforcement Learning Conference 2025, Workshop\n  on Inductive Biases in Reinforcement Learning", "summary": "When using reinforcement learning (RL) to tackle physical control tasks,\ninductive biases that encode physics priors can help improve sample efficiency\nduring training and enhance generalization in testing. However, the current\npractice of incorporating these helpful physics-informed inductive biases\ninevitably runs into significant manual labor and domain expertise, making them\nprohibitive for general users. This work explores a symbolic approach to\ndistill physics-informed inductive biases into RL agents, where the physics\npriors are expressed in a domain-specific language (DSL) that is human-readable\nand naturally explainable. Yet, the DSL priors do not translate directly into\nan implementable policy due to partial and noisy observations and additional\nphysical constraints in navigation tasks. To address this gap, we develop a\nphysics-informed program-guided RL (PiPRL) framework with applications to\nindoor navigation. PiPRL adopts a hierarchical and modularized neuro-symbolic\nintegration, where a meta symbolic program receives semantically meaningful\nfeatures from a neural perception module, which form the bases for symbolic\nprogramming that encodes physics priors and guides the RL process of a\nlow-level neural controller. Extensive experiments demonstrate that PiPRL\nconsistently outperforms purely symbolic or neural policies and reduces\ntraining time by over 26% with the help of the program-based inductive biases.", "AI": {"tldr": "This paper introduces PiPRL, a physics-informed program-guided RL framework for indoor navigation, which outperforms purely symbolic or neural policies and reduces training time.", "motivation": "When using reinforcement learning (RL) to tackle physical control tasks, inductive biases that encode physics priors can help improve sample efficiency during training and enhance generalization in testing. However, the current practice of incorporating these helpful physics-informed inductive biases inevitably runs into significant manual labor and domain expertise, making them prohibitive for general users.", "method": "a physics-informed program-guided RL (PiPRL) framework with applications to indoor navigation. PiPRL adopts a hierarchical and modularized neuro-symbolic integration, where a meta symbolic program receives semantically meaningful features from a neural perception module, which form the bases for symbolic programming that encodes physics priors and guides the RL process of a low-level neural controller.", "result": "PiPRL consistently outperforms purely symbolic or neural policies and reduces training time by over 26% with the help of the program-based inductive biases.", "conclusion": "PiPRL consistently outperforms purely symbolic or neural policies and reduces training time by over 26% with the help of the program-based inductive biases."}}
{"id": "2506.22063", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22063", "abs": "https://arxiv.org/abs/2506.22063", "authors": ["Durgesh K. Singh", "Ahcene Boubekki", "Qing Cao", "Svein Arne Aase", "Robert Jenssen", "Michael Kampffmeyer"], "title": "EnLVAM: Enhanced Left Ventricle Linear Measurements Utilizing Anatomical Motion Mode", "comment": null, "summary": "Linear measurements of the left ventricle (LV) in the Parasternal Long Axis\n(PLAX) view using B-mode echocardiography are crucial for cardiac assessment.\nThese involve placing 4-6 landmarks along a virtual scanline (SL) perpendicular\nto the LV axis near the mitral valve tips. Manual placement is time-consuming\nand error-prone, while existing deep learning methods often misalign landmarks,\ncausing inaccurate measurements. We propose a novel framework that enhances LV\nmeasurement accuracy by enforcing straight-line constraints. A landmark\ndetector is trained on Anatomical M-Mode (AMM) images, computed in real time\nfrom B-mode videos, then transformed back to B-mode space. This approach\naddresses misalignment and reduces measurement errors. Experiments show\nimproved accuracy over standard B-mode methods, and the framework generalizes\nwell across network architectures. Our semi-automatic design includes a\nhuman-in-the-loop step where the user only places the SL, simplifying\ninteraction while preserving alignment flexibility and clinical relevance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u81ea\u52a8\u6846\u67b6\uff0c\u901a\u8fc7\u5728AMM\u56fe\u50cf\u4e0a\u8bad\u7ec3landmark\u68c0\u6d4b\u5668\u5e76\u5f3a\u5236\u76f4\u7ebf\u7ea6\u675f\u6765\u63d0\u9ad8\u5de6\u5fc3\u5ba4\u6d4b\u91cf\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u624b\u52a8\u653e\u7f6e\u5de6\u5fc3\u5ba4\u6d4b\u91cf\u4e2d\u7684\u5730\u6807\u8017\u65f6\u4e14\u5bb9\u6613\u51fa\u9519\uff0c\u800c\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7ecf\u5e38\u9519\u4f4d\u5730\u6807\uff0c\u5bfc\u81f4\u6d4b\u91cf\u4e0d\u51c6\u786e\u3002", "method": "\u8be5\u65b9\u6cd5\u5728\u4eceB\u8d85\u89c6\u9891\u5b9e\u65f6\u8ba1\u7b97\u51fa\u7684\u89e3\u5256M\u578b\u56fe\u50cf\uff08AMM\uff09\u4e0a\u8bad\u7ec3landmark\u68c0\u6d4b\u5668\uff0c\u7136\u540e\u8f6c\u6362\u56deB\u8d85\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u6807\u51c6B\u8d85\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5236\u76f4\u7ebf\u7ea6\u675f\u6765\u63d0\u9ad8\u5de6\u5fc3\u5ba4\u6d4b\u91cf\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5728\u4e0d\u540c\u7684\u7f51\u7edc\u67b6\u6784\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.21607", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21607", "abs": "https://arxiv.org/abs/2506.21607", "authors": ["Dipak Meher", "Carlotta Domeniconi", "Guadalupe Correa-Cabrera"], "title": "CORE-KG: An LLM-Driven Knowledge Graph Construction Framework for Human Smuggling Networks", "comment": null, "summary": "Human smuggling networks are increasingly adaptive and difficult to analyze.\nLegal case documents offer valuable insights but are unstructured, lexically\ndense, and filled with ambiguous or shifting references-posing challenges for\nautomated knowledge graph (KG) construction. Existing KG methods often rely on\nstatic templates and lack coreference resolution, while recent LLM-based\napproaches frequently produce noisy, fragmented graphs due to hallucinations,\nand duplicate nodes caused by a lack of guided extraction. We propose CORE-KG,\na modular framework for building interpretable KGs from legal texts. It uses a\ntwo-step pipeline: (1) type-aware coreference resolution via sequential,\nstructured LLM prompts, and (2) entity and relationship extraction using\ndomain-guided instructions, built on an adapted GraphRAG framework. CORE-KG\nreduces node duplication by 33.28%, and legal noise by 38.37% compared to a\nGraphRAG-based baseline-resulting in cleaner and more coherent graph\nstructures. These improvements make CORE-KG a strong foundation for analyzing\ncomplex criminal networks.", "AI": {"tldr": "CORE-KG, a modular framework, constructs interpretable KGs from legal texts using type-aware coreference resolution and domain-guided extraction, reducing node duplication and legal noise for analyzing criminal networks.", "motivation": "Human smuggling networks are increasingly adaptive and difficult to analyze. Legal case documents offer valuable insights but are unstructured, lexically dense, and filled with ambiguous or shifting references-posing challenges for automated knowledge graph (KG) construction. Existing KG methods often rely on static templates and lack coreference resolution, while recent LLM-based approaches frequently produce noisy, fragmented graphs due to hallucinations, and duplicate nodes caused by a lack of guided extraction.", "method": "a modular framework for building interpretable KGs from legal texts. It uses a two-step pipeline: (1) type-aware coreference resolution via sequential, structured LLM prompts, and (2) entity and relationship extraction using domain-guided instructions, built on an adapted GraphRAG framework.", "result": "CORE-KG reduces node duplication by 33.28%, and legal noise by 38.37% compared to a GraphRAG-based baseline-resulting in cleaner and more coherent graph structures.", "conclusion": "CORE-KG creates cleaner, more coherent graph structures, making it a strong foundation for analyzing complex criminal networks."}}
{"id": "2506.22374", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22374", "abs": "https://arxiv.org/abs/2506.22374", "authors": ["Abdulmomen Ghalkha", "Zhuojun Tian", "Chaouki Ben Issaid", "Mehdi Bennis"], "title": "Sheaf-Based Decentralized Multimodal Learning for Next-Generation Wireless Communication Systems", "comment": "13 pages, 9 figures", "summary": "In large-scale communication systems, increasingly complex scenarios require\nmore intelligent collaboration among edge devices collecting various multimodal\nsensory data to achieve a more comprehensive understanding of the environment\nand improve decision-making accuracy. However, conventional federated learning\n(FL) algorithms typically consider unimodal datasets, require identical model\narchitectures, and fail to leverage the rich information embedded in multimodal\ndata, limiting their applicability to real-world scenarios with diverse\nmodalities and varying client capabilities. To address this issue, we propose\nSheaf-DMFL, a novel decentralized multimodal learning framework leveraging\nsheaf theory to enhance collaboration among devices with diverse modalities.\nSpecifically, each client has a set of local feature encoders for its different\nmodalities, whose outputs are concatenated before passing through a\ntask-specific layer. While encoders for the same modality are trained\ncollaboratively across clients, we capture the intrinsic correlations among\nclients' task-specific layers using a sheaf-based structure. To further enhance\nlearning capability, we propose an enhanced algorithm named Sheaf-DMFL-Att,\nwhich tailors the attention mechanism within each client to capture\ncorrelations among different modalities. A rigorous convergence analysis of\nSheaf-DMFL-Att is provided, establishing its theoretical guarantees. Extensive\nsimulations are conducted on real-world link blockage prediction and mmWave\nbeamforming scenarios, demonstrate the superiority of the proposed algorithms\nin such heterogeneous wireless communication systems.", "AI": {"tldr": "Sheaf-DMFL, a decentralized multimodal learning framework using sheaf theory, enhances collaboration among devices with diverse modalities, outperforming conventional federated learning in heterogeneous wireless communication systems.", "motivation": "Conventional federated learning (FL) algorithms typically consider unimodal datasets, require identical model architectures, and fail to leverage the rich information embedded in multimodal data, limiting their applicability to real-world scenarios with diverse modalities and varying client capabilities. The increasing complexity of large-scale communication systems necessitates more intelligent collaboration among edge devices collecting various multimodal sensory data.", "method": "A novel decentralized multimodal learning framework leveraging sheaf theory to enhance collaboration among devices with diverse modalities. Specifically, each client has a set of local feature encoders for its different modalities, whose outputs are concatenated before passing through a task-specific layer. While encoders for the same modality are trained collaboratively across clients, the intrinsic correlations among clients' task-specific layers are captured using a sheaf-based structure. An enhanced algorithm named Sheaf-DMFL-Att tailors the attention mechanism within each client to capture correlations among different modalities.", "result": "The proposed algorithms outperform existing methods in real-world link blockage prediction and mmWave beamforming scenarios.", "conclusion": "The proposed Sheaf-DMFL and Sheaf-DMFL-Att algorithms demonstrate superior performance in heterogeneous wireless communication systems through simulations on real-world link blockage prediction and mmWave beamforming scenarios."}}
{"id": "2506.22065", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22065", "abs": "https://arxiv.org/abs/2506.22065", "authors": ["Dechao Meng", "Steven Xiao", "Xindi Zhang", "Guangyuan Wang", "Peng Zhang", "Qi Wang", "Bang Zhang", "Liefeng Bo"], "title": "MirrorMe: Towards Realtime and High Fidelity Audio-Driven Halfbody Animation", "comment": "8 pages, 6 figures", "summary": "Audio-driven portrait animation, which synthesizes realistic videos from\nreference images using audio signals, faces significant challenges in real-time\ngeneration of high-fidelity, temporally coherent animations. While recent\ndiffusion-based methods improve generation quality by integrating audio into\ndenoising processes, their reliance on frame-by-frame UNet architectures\nintroduces prohibitive latency and struggles with temporal consistency. This\npaper introduces MirrorMe, a real-time, controllable framework built on the LTX\nvideo model, a diffusion transformer that compresses video spatially and\ntemporally for efficient latent space denoising. To address LTX's trade-offs\nbetween compression and semantic fidelity, we propose three innovations: 1. A\nreference identity injection mechanism via VAE-encoded image concatenation and\nself-attention, ensuring identity consistency; 2. A causal audio encoder and\nadapter tailored to LTX's temporal structure, enabling precise audio-expression\nsynchronization; and 3. A progressive training strategy combining close-up\nfacial training, half-body synthesis with facial masking, and hand pose\nintegration for enhanced gesture control. Extensive experiments on the EMTD\nBenchmark demonstrate MirrorMe's state-of-the-art performance in fidelity,\nlip-sync accuracy, and temporal stability.", "AI": {"tldr": "MirrorMe\u662f\u4e00\u79cd\u7528\u4e8e\u5b9e\u65f6\u97f3\u9891\u9a71\u52a8\u4eba\u50cf\u52a8\u753b\u7684\u6846\u67b6\uff0c\u5b83\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u97f3\u9891\u9a71\u52a8\u7684\u4eba\u50cf\u52a8\u753b\u5728\u5b9e\u65f6\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u65f6\u95f4\u8fde\u8d2f\u7684\u52a8\u753b\u65b9\u9762\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002\u6700\u8fd1\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u4f46\u5b83\u4eec\u5bf9\u9010\u5e27UNet\u67b6\u6784\u7684\u4f9d\u8d56\u5f15\u5165\u4e86\u8fc7\u9ad8\u7684\u5ef6\u8fdf\uff0c\u5e76\u4e14\u96be\u4ee5\u4fdd\u8bc1\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "method": "MirrorMe\u5efa\u7acb\u5728LTX\u89c6\u9891\u6a21\u578b\u4e4b\u4e0a\uff0c\u5e76\u901a\u8fc7VAE\u7f16\u7801\u56fe\u50cf\u8fde\u63a5\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u3001\u56e0\u679c\u97f3\u9891\u7f16\u7801\u5668\u548c\u9002\u914d\u5668\u4ee5\u53ca\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\u8fdb\u884c\u6539\u8fdb\u3002", "result": "MirrorMe\u5728EMTD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "MirrorMe\u5728\u4fdd\u771f\u5ea6\u3001\u5507\u5f62\u540c\u6b65\u51c6\u786e\u6027\u548c\u65f6\u95f4\u7a33\u5b9a\u6027\u65b9\u9762\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2506.21608", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21608", "abs": "https://arxiv.org/abs/2506.21608", "authors": ["Yasmine Bouamra", "Bruno Yun", "Alexandre Poisson", "Fr\u00e9d\u00e9ric Armetta"], "title": "SysTemp: A Multi-Agent System for Template-Based Generation of SysML v2", "comment": null, "summary": "The automatic generation of SysML v2 models represents a major challenge in\nthe engineering of complex systems, particularly due to the scarcity of\nlearning corpora and complex syntax. We present SysTemp, a system aimed at\nfacilitating and improving the creation of SysML v2 models from natural\nlanguage specifications. It is based on a multi-agent system, including a\ntemplate generator that structures the generation process. We discuss the\nadvantages and challenges of this system through an evaluation, highlighting\nits potential to improve the quality of the generations in SysML v2 modeling.", "AI": {"tldr": "SysTemp facilitates and improves the creation of SysML v2 models from natural language specifications.", "motivation": "The automatic generation of SysML v2 models represents a major challenge in the engineering of complex systems, particularly due to the scarcity of learning corpora and complex syntax.", "method": "It is based on a multi-agent system, including a template generator that structures the generation process.", "result": "We discuss the advantages and challenges of this system through an evaluation.", "conclusion": "The system has the potential to improve the quality of the generations in SysML v2 modeling."}}
{"id": "2506.22376", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.22376", "abs": "https://arxiv.org/abs/2506.22376", "authors": ["Youkang Wang", "Jian Wang", "Rubing Chen", "Xiao-Yong Wei", "Qing Li"], "title": "Probabilistic Optimality for Inference-time Scaling", "comment": null, "summary": "Inference-time scaling has emerged as a powerful technique for enhancing the\nreasoning performance of Large Language Models (LLMs). However, existing\napproaches often rely on heuristic strategies for parallel sampling, lacking a\nprincipled foundation. To address this gap, we propose a probabilistic\nframework that formalizes the optimality of inference-time scaling under the\nassumption that parallel samples are independently and identically distributed\n(i.i.d.), and where the Best-of-N selection strategy follows a probability\ndistribution that can be estimated. Within this framework, we derive a\ntheoretical lower bound on the required number of samples to achieve a target\nperformance level, providing the first principled guidance for\ncompute-efficient scaling. Leveraging this insight, we develop\n\\textsc{OptScale}, a practical algorithm that dynamically determines the\noptimal number of sampled responses. \\textsc{OptScale} employs a language\nmodel-based predictor to estimate probabilistic prior parameters, enabling the\ndecision of the minimal number of samples needed that satisfy predefined\nperformance thresholds and confidence levels. Extensive experiments on\nmathematical reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC)\ndemonstrate that \\textsc{OptScale} significantly reduces sampling overhead\nwhile remaining better or on par with state-of-the-art reasoning performance.\nOur work offers both a theoretical foundation and a practical solution for\nprincipled inference-time scaling, addressing a critical gap in the efficient\ndeployment of LLMs for complex reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6982\u7387\u6846\u67b6\u548cOptScale\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u63a8\u7406\u65f6\u6709\u6548\u6269\u5c55LLM\uff0c\u5e76\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u5e76\u884c\u91c7\u6837\u7684\u542f\u53d1\u5f0f\u7b56\u7565\uff0c\u7f3a\u4e4f\u539f\u5219\u57fa\u7840\uff0c\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6982\u7387\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5f62\u5f0f\u5316\u4e86\u63a8\u7406\u65f6\u7f29\u653e\u7684\u6700\u4f18\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3aOptScale\u7684\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u53ef\u4ee5\u52a8\u6001\u786e\u5b9a\u6700\u4f73\u91c7\u6837\u54cd\u5e94\u6570\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOptScale\u5728\u663e\u8457\u964d\u4f4e\u91c7\u6837\u5f00\u9500\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u6216\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u6709\u539f\u5219\u7684\u63a8\u7406\u65f6\u6269\u5c55\uff0c\u89e3\u51b3\u4e86LLM\u5728\u590d\u6742\u63a8\u7406\u4e2d\u6709\u6548\u90e8\u7f72\u7684\u5173\u952e\u5dee\u8ddd\u3002"}}
{"id": "2506.22069", "categories": ["cs.CV", "68T45", "I.4.5"], "pdf": "https://arxiv.org/pdf/2506.22069", "abs": "https://arxiv.org/abs/2506.22069", "authors": ["Petr Hruby", "Marc Pollefeys"], "title": "Single-Scanline Relative Pose Estimation for Rolling Shutter Cameras", "comment": "ICCV 2025, 15 pages, 5 figures, 12 tables", "summary": "We propose a novel approach for estimating the relative pose between rolling\nshutter cameras using the intersections of line projections with a single\nscanline per image. This allows pose estimation without explicitly modeling\ncamera motion. Alternatively, scanlines can be selected within a single image,\nenabling single-view relative pose estimation for scanlines of rolling shutter\ncameras. Our approach is designed as a foundational building block for rolling\nshutter structure-from-motion (SfM), where no motion model is required, and\neach scanline's pose can be computed independently. % We classify minimal\nsolvers for this problem in both generic and specialized settings, including\ncases with parallel lines and known gravity direction, assuming known\nintrinsics and no lens distortion. Furthermore, we develop minimal solvers for\nthe parallel-lines scenario, both with and without gravity priors, by\nleveraging connections between this problem and the estimation of 2D structure\nfrom 1D cameras. % Experiments on rolling shutter images from the Fastec\ndataset demonstrate the feasibility of our approach for initializing rolling\nshutter SfM, highlighting its potential for further development. % The code\nwill be made publicly available.", "AI": {"tldr": "A novel approach for estimating the relative pose between rolling shutter cameras is proposed, which allows pose estimation without explicitly modeling camera motion and enables single-view relative pose estimation for scanlines of rolling shutter cameras.", "motivation": "This allows pose estimation without explicitly modeling camera motion.", "method": "a novel approach for estimating the relative pose between rolling shutter cameras using the intersections of line projections with a single scanline per image", "result": "scanlines can be selected within a single image, enabling single-view relative pose estimation for scanlines of rolling shutter cameras", "conclusion": "The feasibility of our approach for initializing rolling shutter SfM is demonstrated, highlighting its potential for further development."}}
{"id": "2506.21609", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.21609", "abs": "https://arxiv.org/abs/2506.21609", "authors": ["Junhao Liu", "Zhenhao Xu", "Yuxin Fang", "Yichuan Chen", "Zuobin Ying", "Wenhan Chang"], "title": "From Thinking to Output: Chain-of-Thought and Text Generation Characteristics in Reasoning Language Models", "comment": "18 pages, 3 figures", "summary": "Recently, there have been notable advancements in large language models\n(LLMs), demonstrating their growing abilities in complex reasoning. However,\nexisting research largely overlooks a thorough and systematic comparison of\nthese models' reasoning processes and outputs, particularly regarding their\nself-reflection pattern (also termed \"Aha moment\") and the interconnections\nacross diverse domains. This paper proposes a novel framework for analyzing the\nreasoning characteristics of four cutting-edge large reasoning models (GPT-o1,\nDeepSeek-R1, Kimi-k1.5, and Grok-3) using keywords statistic and LLM-as-a-judge\nparadigm. Our approach connects their internal thinking processes with their\nfinal outputs. A diverse dataset consists of real-world scenario-based\nquestions covering logical deduction, causal inference, and multi-step\nproblem-solving. Additionally, a set of metrics is put forward to assess both\nthe coherence of reasoning and the accuracy of the outputs. The research\nresults uncover various patterns of how these models balance exploration and\nexploitation, deal with problems, and reach conclusions during the reasoning\nprocess. Through quantitative and qualitative comparisons, disparities among\nthese models are identified in aspects such as the depth of reasoning, the\nreliance on intermediate steps, and the degree of similarity between their\nthinking processes and output patterns and those of GPT-o1. This work offers\nvaluable insights into the trade-off between computational efficiency and\nreasoning robustness and provides practical recommendations for enhancing model\ndesign and evaluation in practical applications. We publicly release our\nproject at: https://github.com/ChangWenhan/FromThinking2Output", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u56db\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u548c\u8f93\u51fa\uff0c\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u63a8\u7406\u6df1\u5ea6\u548c\u65b9\u5f0f\u4e0a\u7684\u5dee\u5f02\uff0c\u5e76\u63d0\u4f9b\u4e86\u5173\u4e8e\u8ba1\u7b97\u6548\u7387\u548c\u63a8\u7406\u9c81\u68d2\u6027\u4e4b\u95f4\u6743\u8861\u7684\u89c1\u89e3\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5ffd\u7565\u4e86\u5bf9\u8fd9\u4e9b\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u548c\u8f93\u51fa\u7684\u5168\u9762\u548c\u7cfb\u7edf\u6bd4\u8f83\uff0c\u7279\u522b\u662f\u5173\u4e8e\u5b83\u4eec\u7684\u81ea\u6211\u53cd\u601d\u6a21\u5f0f\uff08\u4e5f\u79f0\u4e3a\u201c\u987f\u609f\u65f6\u523b\u201d\uff09\u4ee5\u53ca\u8de8\u4e0d\u540c\u9886\u57df\u7684\u4e92\u8fde\u3002", "method": "\u4f7f\u7528\u5173\u952e\u8bcd\u7edf\u8ba1\u548c LLM-as-a-judge \u8303\u5f0f\u5206\u6790\u56db\u4e2a\u5148\u8fdb\u7684\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08GPT-o1\u3001DeepSeek-R1\u3001Kimi-k1.5 \u548c Grok-3\uff09\u7684\u63a8\u7406\u7279\u5f81\u3002", "result": "\u63ed\u793a\u4e86\u8fd9\u4e9b\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5982\u4f55\u5728\u63a2\u7d22\u548c\u5229\u7528\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u5982\u4f55\u5904\u7406\u95ee\u9898\u4ee5\u53ca\u5982\u4f55\u5f97\u51fa\u7ed3\u8bba\u7684\u5404\u79cd\u6a21\u5f0f\u3002\u901a\u8fc7\u5b9a\u91cf\u548c\u5b9a\u6027\u6bd4\u8f83\uff0c\u53d1\u73b0\u8fd9\u4e9b\u6a21\u578b\u5728\u63a8\u7406\u6df1\u5ea6\u3001\u5bf9\u4e2d\u95f4\u6b65\u9aa4\u7684\u4f9d\u8d56\u4ee5\u53ca\u5b83\u4eec\u7684\u601d\u7ef4\u8fc7\u7a0b\u548c\u8f93\u51fa\u6a21\u5f0f\u4e0e GPT-o1 \u4e4b\u95f4\u7684\u76f8\u4f3c\u7a0b\u5ea6\u7b49\u65b9\u9762\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "\u63ed\u793a\u4e86\u8fd9\u4e9b\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5982\u4f55\u5728\u63a2\u7d22\u548c\u5229\u7528\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u5982\u4f55\u5904\u7406\u95ee\u9898\u4ee5\u53ca\u5982\u4f55\u5f97\u51fa\u7ed3\u8bba\u7684\u5404\u79cd\u6a21\u5f0f\u3002\u901a\u8fc7\u5b9a\u91cf\u548c\u5b9a\u6027\u6bd4\u8f83\uff0c\u53d1\u73b0\u8fd9\u4e9b\u6a21\u578b\u5728\u63a8\u7406\u6df1\u5ea6\u3001\u5bf9\u4e2d\u95f4\u6b65\u9aa4\u7684\u4f9d\u8d56\u4ee5\u53ca\u5b83\u4eec\u7684\u601d\u7ef4\u8fc7\u7a0b\u548c\u8f93\u51fa\u6a21\u5f0f\u4e0e GPT-o1 \u4e4b\u95f4\u7684\u76f8\u4f3c\u7a0b\u5ea6\u7b49\u65b9\u9762\u5b58\u5728\u5dee\u5f02\u3002"}}
{"id": "2506.22389", "categories": ["cs.LG", "cond-mat.dis-nn", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22389", "abs": "https://arxiv.org/abs/2506.22389", "authors": ["Aditya Cowsik", "Tianyu He", "Andrey Gromov"], "title": "Towards Distributed Neural Architectures", "comment": "36 pages, 25 figures", "summary": "We introduce and train distributed neural architectures (DNA) in vision and\nlanguage domains. DNAs are initialized with a proto-architecture that consists\nof (transformer, MLP, attention, etc.) modules and routers. Any token (or\npatch) can traverse any series of modules in any order. DNAs are a natural\ngeneralization of the sparse methods such as Mixture-of-Experts,\nMixture-of-Depths, parameter sharing, etc. Computation and communication\npatterns of DNA modules are learnt end-to-end during training and depend on the\ncontent and context of each token (or patch). These patterns can be shaped by\nfurther requirements added to the optimization objective such as compute/memory\nefficiency or load balancing. We empirically show that (i) trained DNAs are\ncompetitive with the dense baselines in both domains and (ii) compute\nefficiency/parameter sharing can be learnt from data. Next, we analyze the\nemergent connectivity and computation patterns in the trained DNAs. We find\nthat the paths that tokens take through the models are themselves distributed\naccording to a power-law. We show that some paths (or, equivalently, groups of\nmodules) show emergent specialization. Finally, we demonstrate that models\nlearn to allocate compute and active parameters in an interpretable way.", "AI": {"tldr": "DNAs, a generalization of sparse methods, learn efficient computation and communication patterns, achieving competitive performance with dense models and demonstrating emergent specialization.", "motivation": "DNAs generalize sparse methods like Mixture-of-Experts and address limitations in computation and communication patterns.", "method": "Introduce and train distributed neural architectures (DNA) with a proto-architecture of modules and routers, where tokens can traverse any module series in any order. Computation and communication patterns are learned end-to-end.", "result": "Trained DNAs are competitive with dense baselines, paths are power-law distributed, modules show emergent specialization, and compute/parameter allocation is interpretable.", "conclusion": "Trained DNAs demonstrate competitive performance with dense baselines, learn compute efficiency/parameter sharing, exhibit power-law distributed paths with emergent specialization, and allocate compute/parameters interpretably."}}
{"id": "2506.22075", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22075", "abs": "https://arxiv.org/abs/2506.22075", "authors": ["Shaheer U. Saeed", "Yipei Wang", "Veeru Kasivisvanathan", "Brian R. Davidson", "Matthew J. Clarkson", "Yipeng Hu", "Daniel C. Alexander"], "title": "Reasoning in machine vision: learning to think fast and slow", "comment": null, "summary": "Reasoning is a hallmark of human intelligence, enabling adaptive\ndecision-making in complex and unfamiliar scenarios. In contrast, machine\nintelligence remains bound to training data, lacking the ability to dynamically\nrefine solutions at inference time. While some recent advances have explored\nreasoning in machines, these efforts are largely limited to verbal domains such\nas mathematical problem-solving, where explicit rules govern step-by-step\nreasoning. Other critical real-world tasks - including visual perception,\nspatial reasoning, and radiological diagnosis - require non-verbal reasoning,\nwhich remains an open challenge. Here we present a novel learning paradigm that\nenables machine reasoning in vision by allowing performance improvement with\nincreasing thinking time (inference-time compute), even under conditions where\nlabelled data is very limited. Inspired by dual-process theories of human\ncognition in psychology, our approach integrates a fast-thinking System I\nmodule for familiar tasks, with a slow-thinking System II module that\niteratively refines solutions using self-play reinforcement learning. This\nparadigm mimics human reasoning by proposing, competing over, and refining\nsolutions in data-scarce scenarios. We demonstrate superior performance through\nextended thinking time, compared not only to large-scale supervised learning\nbut also foundation models and even human experts, in real-world vision tasks.\nThese tasks include computer-vision benchmarks and cancer localisation on\nmedical images across five organs, showcasing transformative potential for\nnon-verbal machine reasoning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u7684\u601d\u8003\u65b9\u5f0f\uff0c\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u673a\u5668\u63a8\u7406\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751a\u81f3\u4eba\u7c7b\u4e13\u5bb6\u3002", "motivation": "\u673a\u5668\u667a\u80fd\u4ecd\u7136\u53d7\u9650\u4e8e\u8bad\u7ec3\u6570\u636e\uff0c\u7f3a\u4e4f\u5728\u63a8\u7406\u65f6\u52a8\u6001\u6539\u8fdb\u89e3\u51b3\u65b9\u6848\u7684\u80fd\u529b\u3002\u867d\u7136\u6700\u8fd1\u7684\u4e00\u4e9b\u8fdb\u5c55\u63a2\u7d22\u4e86\u673a\u5668\u63a8\u7406\uff0c\u4f46\u8fd9\u4e9b\u52aa\u529b\u4e3b\u8981\u9650\u4e8e\u53e3\u5934\u9886\u57df\uff0c\u5982\u6570\u5b66\u95ee\u9898\u89e3\u51b3\uff0c\u5176\u4e2d\u660e\u786e\u7684\u89c4\u5219\u652f\u914d\u7740\u9010\u6b65\u63a8\u7406\u3002\u5176\u4ed6\u5173\u952e\u7684\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u2014\u2014\u5305\u62ec\u89c6\u89c9\u611f\u77e5\u3001\u7a7a\u95f4\u63a8\u7406\u548c\u653e\u5c04\u8bca\u65ad\u2014\u2014\u9700\u8981\u975e\u8bed\u8a00\u63a8\u7406\uff0c\u8fd9\u4ecd\u7136\u662f\u4e00\u4e2a\u5f00\u653e\u7684\u6311\u6218\u3002", "method": "\u8be5\u65b9\u6cd5\u6574\u5408\u4e86\u4e00\u4e2a\u7528\u4e8e\u719f\u6089\u4efb\u52a1\u7684\u5feb\u901f\u601d\u8003\u7cfb\u7edf I \u6a21\u5757\uff0c\u4ee5\u53ca\u4e00\u4e2a\u4f7f\u7528\u81ea\u535a\u5f08\u5f3a\u5316\u5b66\u4e60\u8fed\u4ee3\u6539\u8fdb\u89e3\u51b3\u65b9\u6848\u7684\u6162\u901f\u601d\u8003\u7cfb\u7edf II \u6a21\u5757\u3002\u8fd9\u79cd\u8303\u5f0f\u901a\u8fc7\u5728\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\u63d0\u51fa\u3001\u7ade\u4e89\u548c\u6539\u8fdb\u89e3\u51b3\u65b9\u6848\u6765\u6a21\u4eff\u4eba\u7c7b\u63a8\u7406\u3002", "result": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u5141\u8bb8\u968f\u7740\u601d\u8003\u65f6\u95f4\uff08\u63a8\u7406\u65f6\u8ba1\u7b97\uff09\u7684\u589e\u52a0\u800c\u63d0\u9ad8\u6027\u80fd\uff0c\u5373\u4f7f\u5728\u6807\u8bb0\u6570\u636e\u975e\u5e38\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u4e5f\u80fd\u5b9e\u73b0\u89c6\u89c9\u4e2d\u7684\u673a\u5668\u63a8\u7406\u3002", "conclusion": "\u8be5\u8bba\u6587\u5c55\u793a\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u89c6\u89c9\u4efb\u52a1\u4e2d\uff0c\u901a\u8fc7\u5ef6\u957f\u601d\u8003\u65f6\u95f4\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u5927\u89c4\u6a21\u76d1\u7763\u5b66\u4e60\u3001\u57fa\u7840\u6a21\u578b\uff0c\u751a\u81f3\u4eba\u7c7b\u4e13\u5bb6\u3002\u8fd9\u4e9b\u4efb\u52a1\u5305\u62ec\u8ba1\u7b97\u673a\u89c6\u89c9\u57fa\u51c6\u6d4b\u8bd5\u548c\u533b\u5b66\u56fe\u50cf\u4e0a\u7684\u764c\u75c7\u5b9a\u4f4d\uff0c\u5c55\u793a\u4e86\u975e\u8bed\u8a00\u673a\u5668\u63a8\u7406\u7684\u53d8\u9769\u6f5c\u529b\u3002"}}
{"id": "2506.21611", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21611", "abs": "https://arxiv.org/abs/2506.21611", "authors": ["Xiyuan Zhang", "Boran Han", "Haoyang Fang", "Abdul Fatir Ansari", "Shuai Zhang", "Danielle C. Maddix", "Cuixiong Hu", "Andrew Gordon Wilson", "Michael W. Mahoney", "Hao Wang", "Yan Liu", "Huzefa Rangwala", "George Karypis", "Bernie Wang"], "title": "Does Multimodality Lead to Better Time Series Forecasting?", "comment": null, "summary": "Recently, there has been growing interest in incorporating textual\ninformation into foundation models for time series forecasting. However, it\nremains unclear whether and under what conditions such multimodal integration\nconsistently yields gains. We systematically investigate these questions across\na diverse benchmark of 14 forecasting tasks spanning 7 domains, including\nhealth, environment, and economics. We evaluate two popular multimodal\nforecasting paradigms: aligning-based methods, which align time series and text\nrepresentations; and prompting-based methods, which directly prompt large\nlanguage models for forecasting. Although prior works report gains from\nmultimodal input, we find these effects are not universal across datasets and\nmodels, and multimodal methods sometimes do not outperform the strongest\nunimodal baselines. To understand when textual information helps, we\ndisentangle the effects of model architectural properties and data\ncharacteristics. Our findings highlight that on the modeling side,\nincorporating text information is most helpful given (1) high-capacity text\nmodels, (2) comparatively weaker time series models, and (3) appropriate\naligning strategies. On the data side, performance gains are more likely when\n(4) sufficient training data is available and (5) the text offers complementary\npredictive signal beyond what is already captured from the time series alone.\nOur empirical findings offer practical guidelines for when multimodality can be\nexpected to aid forecasting tasks, and when it does not.", "AI": {"tldr": "\u7814\u7a76\u4e86\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u53d1\u73b0\u5176\u6709\u6548\u6027\u53d6\u51b3\u4e8e\u6a21\u578b\u548c\u6570\u636e\u7279\u6027\u3002", "motivation": "\u7814\u7a76\u5c06\u6587\u672c\u4fe1\u606f\u6574\u5408\u5230\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u57fa\u7840\u6a21\u578b\u4e2d\u662f\u5426\u4ee5\u53ca\u5728\u4f55\u79cd\u6761\u4ef6\u4e0b\u59cb\u7ec8\u4ea7\u751f\u589e\u76ca\u3002", "method": "\u8bc4\u4f30\u4e24\u79cd\u6d41\u884c\u7684\u591a\u6a21\u6001\u9884\u6d4b\u8303\u4f8b\uff1a\u57fa\u4e8e\u5bf9\u9f50\u7684\u65b9\u6cd5\u548c\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u3002", "result": "\u591a\u6a21\u6001\u65b9\u6cd5\u7684\u6548\u679c\u5e76\u975e\u5728\u6240\u6709\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u90fd\u662f\u666e\u904d\u7684\uff0c\u5e76\u4e14\u591a\u6a21\u6001\u65b9\u6cd5\u6709\u65f6\u5e76\u4e0d\u4f18\u4e8e\u6700\u5f3a\u7684\u5355\u6a21\u6001\u57fa\u7ebf\u3002", "conclusion": "\u591a\u6a21\u6001\u65b9\u6cd5\u5e76\u975e\u5728\u6240\u6709\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u90fd\u4f18\u4e8e\u6700\u5f3a\u7684\u5355\u6a21\u6001\u57fa\u7ebf\u6a21\u578b\u3002\u5f53\u5b58\u5728\u4ee5\u4e0b\u60c5\u51b5\u65f6\uff0c\u7ed3\u5408\u6587\u672c\u4fe1\u606f\u6700\u6709\u5e2e\u52a9\uff1a(1) \u9ad8\u5bb9\u91cf\u6587\u672c\u6a21\u578b\uff0c(2) \u76f8\u5bf9\u8f83\u5f31\u7684\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\uff0c\u548c (3) \u9002\u5f53\u7684\u5bf9\u9f50\u7b56\u7565\u3002\u5728\u6570\u636e\u65b9\u9762\uff0c\u5f53 (4) \u6709\u8db3\u591f\u7684\u8bad\u7ec3\u6570\u636e\u53ef\u7528\u65f6\uff0c\u5e76\u4e14 (5) \u6587\u672c\u63d0\u4f9b\u4e86\u8d85\u51fa\u4ec5\u4ece\u65f6\u95f4\u5e8f\u5217\u4e2d\u6355\u83b7\u7684\u8865\u5145\u9884\u6d4b\u4fe1\u53f7\u65f6\uff0c\u66f4\u6709\u53ef\u80fd\u83b7\u5f97\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2506.22393", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22393", "abs": "https://arxiv.org/abs/2506.22393", "authors": ["YongKyung Oh", "Alex Bui"], "title": "Multi-View Contrastive Learning for Robust Domain Adaptation in Medical Time Series Analysis", "comment": null, "summary": "Adapting machine learning models to medical time series across different\ndomains remains a challenge due to complex temporal dependencies and dynamic\ndistribution shifts. Current approaches often focus on isolated feature\nrepresentations, limiting their ability to fully capture the intricate temporal\ndynamics necessary for robust domain adaptation. In this work, we propose a\nnovel framework leveraging multi-view contrastive learning to integrate\ntemporal patterns, derivative-based dynamics, and frequency-domain features.\nOur method employs independent encoders and a hierarchical fusion mechanism to\nlearn feature-invariant representations that are transferable across domains\nwhile preserving temporal coherence. Extensive experiments on diverse medical\ndatasets, including electroencephalogram (EEG), electrocardiogram (ECG), and\nelectromyography (EMG) demonstrate that our approach significantly outperforms\nstate-of-the-art methods in transfer learning tasks. By advancing the\nrobustness and generalizability of machine learning models, our framework\noffers a practical pathway for deploying reliable AI systems in diverse\nhealthcare settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u5229\u7528\u591a\u89c6\u89d2\u5bf9\u6bd4\u5b66\u4e60\u6765\u6574\u5408\u65f6\u95f4\u6a21\u5f0f\u3001\u57fa\u4e8e\u5bfc\u6570\u7684\u52a8\u6001\u548c\u9891\u57df\u7279\u5f81\uff0c\u4ece\u800c\u63a8\u8fdb\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u3002", "motivation": "\u7531\u4e8e\u590d\u6742\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u52a8\u6001\u5206\u5e03\u53d8\u5316\uff0c\u4f7f\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9002\u5e94\u4e0d\u540c\u9886\u57df\u7684\u533b\u7597\u65f6\u95f4\u5e8f\u5217\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u76ee\u524d\u7684\u65b9\u6cd5\u901a\u5e38\u4fa7\u91cd\u4e8e\u5b64\u7acb\u7684\u7279\u5f81\u8868\u793a\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5145\u5206\u6355\u6349\u9c81\u68d2\u57df\u9002\u5e94\u6240\u9700\u7684\u590d\u6742\u65f6\u95f4\u52a8\u6001\u7684\u80fd\u529b\u3002", "method": "\u5229\u7528\u591a\u89c6\u89d2\u5bf9\u6bd4\u5b66\u4e60\u6765\u6574\u5408\u65f6\u95f4\u6a21\u5f0f\u3001\u57fa\u4e8e\u5bfc\u6570\u7684\u52a8\u6001\u548c\u9891\u57df\u7279\u5f81\u3002", "result": "\u5728\u5305\u62ec\u8111\u7535\u56fe (EEG)\u3001\u5fc3\u7535\u56fe (ECG) \u548c\u808c\u7535\u56fe (EMG) \u5728\u5185\u7684\u4e0d\u540c\u533b\u7597\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u8fc1\u79fb\u5b66\u4e60\u4efb\u52a1\u4e2d\u663e\u7740\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u63d0\u9ad8\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\uff0c\u4e3a\u5728\u4e0d\u540c\u7684\u533b\u7597\u4fdd\u5065\u73af\u5883\u4e2d\u90e8\u7f72\u53ef\u9760\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u884c\u7684\u9014\u5f84\u3002"}}
{"id": "2506.22078", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.22078", "abs": "https://arxiv.org/abs/2506.22078", "authors": ["Pei-Kai Huanga", "Ya-Ting Chan", "Kuan-Wen Chen", "Yen-Chun Chou", "Shih-Yu Yang", "Chiou-Ting Hsu"], "title": "Towards Accurate Heart Rate Measurement from Ultra-Short Video Clips via Periodicity-Guided rPPG Estimation and Signal Reconstruction", "comment": null, "summary": "Many remote Heart Rate (HR) measurement methods focus on estimating remote\nphotoplethysmography (rPPG) signals from video clips lasting around 10 seconds\nbut often overlook the need for HR estimation from ultra-short video clips. In\nthis paper, we aim to accurately measure HR from ultra-short 2-second video\nclips by specifically addressing two key challenges. First, to overcome the\nlimited number of heartbeat cycles in ultra-short video clips, we propose an\neffective periodicity-guided rPPG estimation method that enforces consistent\nperiodicity between rPPG signals estimated from ultra-short clips and their\nmuch longer ground truth signals. Next, to mitigate estimation inaccuracies due\nto spectral leakage, we propose including a generator to reconstruct longer\nrPPG signals from ultra-short ones while preserving their periodic consistency\nto enable more accurate HR measurement. Extensive experiments on four rPPG\nestimation benchmark datasets demonstrate that our proposed method not only\naccurately measures HR from ultra-short video clips but also outperform\nprevious rPPG estimation techniques to achieve state-of-the-art performance.", "AI": {"tldr": "Accurately measures HR from 2-second video clips using periodicity-guided rPPG estimation and signal reconstruction, outperforming existing methods.", "motivation": "To accurately measure HR from ultra-short 2-second video clips by addressing the limited number of heartbeat cycles and mitigating estimation inaccuracies due to spectral leakage.", "method": "An effective periodicity-guided rPPG estimation method and a generator to reconstruct longer rPPG signals from ultra-short ones.", "result": "Achieves state-of-the-art performance on four rPPG estimation benchmark datasets.", "conclusion": "The proposed method accurately measures HR from ultra-short video clips and outperforms previous rPPG estimation techniques."}}
