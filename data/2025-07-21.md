<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 25]
- [cs.CV](#cs.CV) [Total: 22]
- [cs.AI](#cs.AI) [Total: 20]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.IR](#cs.IR) [Total: 8]
- [cs.LG](#cs.LG) [Total: 27]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Adaptive Linguistic Prompting (ALP) Enhances Phishing Webpage Detection in Multimodal Large Language Models](https://arxiv.org/abs/2507.13357)
*Atharva Bhargude,Ishan Gonehal,Chandler Haney,Dave Yoon,Kevin Zhu,Aaron Sandoval,Sean O'Brien,Kaustubh Vinnakota*

Main category: cs.CL

TL;DR: This study explores few-shot Adaptive Linguistic Prompting (ALP) with multimodal LLMs to detect phishing webpages, achieving superior accuracy compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Phishing attacks are a significant cybersecurity threat requiring adaptive detection techniques.

Method: Few-shot Adaptive Linguistic Prompting (ALP) integrating textual, visual, and URL-based analysis with LLMs like GPT-4o and Gemini 1.5 Pro.

Result: ALP significantly enhances phishing detection accuracy by guiding LLMs through structured reasoning and contextual analysis, achieving an F1-score of 0.93.

Conclusion: ALP-integrated multimodal LLMs advance phishing detection, achieving an F1-score of 0.93, surpassing traditional approaches, and establishing a foundation for more robust, interpretable, and adaptive linguistic-based phishing detection systems using LLMs.

Abstract: Phishing attacks represent a significant cybersecurity threat, necessitating
adaptive detection techniques. This study explores few-shot Adaptive Linguistic
Prompting (ALP) in detecting phishing webpages through the multimodal
capabilities of state-of-the-art large language models (LLMs) such as GPT-4o
and Gemini 1.5 Pro. ALP is a structured semantic reasoning method that guides
LLMs to analyze textual deception by breaking down linguistic patterns,
detecting urgency cues, and identifying manipulative diction commonly found in
phishing content. By integrating textual, visual, and URL-based analysis, we
propose a unified model capable of identifying sophisticated phishing attempts.
Our experiments demonstrate that ALP significantly enhances phishing detection
accuracy by guiding LLMs through structured reasoning and contextual analysis.
The findings highlight the potential of ALP-integrated multimodal LLMs to
advance phishing detection frameworks, achieving an F1-score of 0.93,
surpassing traditional approaches. These results establish a foundation for
more robust, interpretable, and adaptive linguistic-based phishing detection
systems using LLMs.

</details>


### [2] [Persona-Based Synthetic Data Generation Using Multi-Stage Conditioning with Large Language Models for Emotion Recognition](https://arxiv.org/abs/2507.13380)
*Keito Inoshita,Rushia Harada*

Main category: cs.CL

TL;DR: PersonaGen是一种使用大型语言模型生成情感丰富文本的新框架，它通过多阶段的基于角色的条件反射来解决情感识别领域中高质量、多样化的情感数据集的稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 由于高质量、多样化的情感数据集的稀缺，高性能模型的发展仍然是一个挑战。情感表达本质上是主观的，受个人性格特征、社会文化背景和情境因素的影响，使得大规模、通用化的数据收集在伦理和实践上都具有挑战性。

Method: 使用大型语言模型(LLM)通过多阶段基于角色的条件反射生成情感丰富的文本。

Result: PersonaGen在生成多样化、连贯和可区分的情感表达方面显著优于基线方法。

Conclusion: PersonaGen显著优于基线方法，能够生成多样、连贯和可区分的情感表达，证明了其作为增强或替代真实世界情感数据集的强大替代方案的潜力。

Abstract: In the field of emotion recognition, the development of high-performance
models remains a challenge due to the scarcity of high-quality, diverse
emotional datasets. Emotional expressions are inherently subjective, shaped by
individual personality traits, socio-cultural backgrounds, and contextual
factors, making large-scale, generalizable data collection both ethically and
practically difficult. To address this issue, we introduce PersonaGen, a novel
framework for generating emotionally rich text using a Large Language Model
(LLM) through multi-stage persona-based conditioning. PersonaGen constructs
layered virtual personas by combining demographic attributes, socio-cultural
backgrounds, and detailed situational contexts, which are then used to guide
emotion expression generation. We conduct comprehensive evaluations of the
generated synthetic data, assessing semantic diversity through clustering and
distributional metrics, human-likeness via LLM-based quality scoring, realism
through comparison with real-world emotion corpora, and practical utility in
downstream emotion classification tasks. Experimental results show that
PersonaGen significantly outperforms baseline methods in generating diverse,
coherent, and discriminative emotion expressions, demonstrating its potential
as a robust alternative for augmenting or replacing real-world emotional
datasets.

</details>


### [3] [SAFT: Structure-Aware Fine-Tuning of LLMs for AMR-to-Text Generation](https://arxiv.org/abs/2507.13381)
*Rafiq Kamel,Filippo Guerranti,Simon Geisler,Stephan Günnemann*

Main category: cs.CL

TL;DR: SAFT is a structure-aware fine-tuning approach that injects graph topology into pretrained LLMs without architectural changes.  SAFT sets a new state-of-the-art on AMR 3.0 with a 3.5 BLEU improvement over baselines.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) are increasingly applied to tasks involving structured inputs such as graphs. Abstract Meaning Representations (AMRs), which encode rich semantics as directed graphs, offer a rigorous testbed for evaluating LLMs on text generation from such structures. Yet, current methods often arbitrarily linearize AMRs, discarding key structural cues, or rely on architectures incompatible with standard LLMs.

Method: a structure-aware fine-tuning approach that injects graph topology into pretrained LLMs without architectural changes. It computes direction-sensitive positional encodings from the magnetic Laplacian of transformed AMRs and project them into the embedding space of the LLM.

Result: SAFT sets a new state-of-the-art on AMR 3.0 with a 3.5 BLEU improvement over baselines. Gains scale with graph complexity, highlighting the value of structure-aware representations in enhancing LLM performance.

Conclusion: SAFT offers a general and effective pathway for bridging structured data and language models.

Abstract: Large Language Models (LLMs) are increasingly applied to tasks involving
structured inputs such as graphs. Abstract Meaning Representations (AMRs),
which encode rich semantics as directed graphs, offer a rigorous testbed for
evaluating LLMs on text generation from such structures. Yet, current methods
often arbitrarily linearize AMRs, discarding key structural cues, or rely on
architectures incompatible with standard LLMs. We introduce SAFT, a
structure-aware fine-tuning approach that injects graph topology into
pretrained LLMs without architectural changes. We compute direction-sensitive
positional encodings from the magnetic Laplacian of transformed AMRs and
project them into the embedding space of the LLM. While possibly applicable to
any graph-structured inputs, we focus on AMR-to-text generation as a
representative and challenging benchmark. SAFT sets a new state-of-the-art on
AMR 3.0 with a 3.5 BLEU improvement over baselines. Gains scale with graph
complexity, highlighting the value of structure-aware representations in
enhancing LLM performance. SAFT offers a general and effective pathway for
bridging structured data and language models.

</details>


### [4] [Context-Based Fake News Detection using Graph Based Approach: ACOVID-19 Use-case](https://arxiv.org/abs/2507.13382)
*Chandrashekar Muniyappa,Sirisha Velampalli*

Main category: cs.CL

TL;DR: This paper presents a contextual graph-based approach to detect fake news by transforming news articles into graph structures and applying anomaly detection.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the rapid spread of fake news in the digital world.

Method: The paper converts news articles into contextual graph structures using NLP techniques and applies the Minimum Description Length (MDL)-based Graph-Based Anomaly Detection (GBAD) algorithm for graph mining.

Result: The proposed approach identifies normative patterns and uncovers anomalous patterns to detect fake news.

Conclusion: This paper uses a contextual graph-based approach combined with NLP and MDL-based GBAD to detect fake news, identifying anomalous patterns that deviate from established norms.

Abstract: In today\'s digital world, fake news is spreading with immense speed. Its a
significant concern to address. In this work, we addressed that challenge using
novel graph based approach. We took dataset from Kaggle that contains real and
fake news articles. To test our approach we incorporated recent covid-19
related news articles that contains both genuine and fake news that are
relevant to this problem. This further enhances the dataset as well instead of
relying completely on the original dataset. We propose a contextual graph-based
approach to detect fake news articles. We need to convert news articles into
appropriate schema, so we leverage Natural Language Processing (NLP) techniques
to transform news articles into contextual graph structures. We then apply the
Minimum Description Length (MDL)-based Graph-Based Anomaly Detection (GBAD)
algorithm for graph mining. Graph-based methods are particularly effective for
handling rich contextual data, as they enable the discovery of complex patterns
that traditional query-based or statistical techniques might overlook. Our
proposed approach identifies normative patterns within the dataset and
subsequently uncovers anomalous patterns that deviate from these established
norms.

</details>


### [5] [PARAM-1 BharatGen 2.9B Model](https://arxiv.org/abs/2507.13390)
*Kundeshwar Pundalik,Piyush Sawarkar,Nihar Sahoo,Abhishek Shinde,Prateek Chanda,Vedant Goswami,Ajay Nagpal,Atul Singh,Viraj Thakur,Vijay Dewane,Aamod Thakur,Bhargav Patel,Smita Gautam,Bhagwan Panditi,Shyam Pawar,Madhav Kotcha,Suraj Racha,Saral Sureka,Pankaj Singh,Rishi Bal,Rohit Saluja,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: PARAM-1是一个29亿参数的语言模型，它从头开始进行训练，特别关注印度的语言多样性，并在印地语和英语双语数据集上进行训练。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）已经成为强大的通用推理系统，但其发展仍然以英语为中心的数据、架构和优化范式为主。这种排他性的设计导致了在语言多样的地区（如印度）的结构性代表性不足，印度有超过20种官方语言和100多种方言，以及诸如代码转换和双语现象。

Method: 从头开始训练了一个29亿参数的仅解码器、仅文本的语言模型，该模型在架构和语言上都明确关注印度的多样性。使用一种明确关注富含事实、高质量内容的双语数据集（仅包含印地语和英语）进行训练。采用SentencePiece分词器

Result: PARAM-1在预训练层面嵌入了多样性，而不是推迟到事后调整，这为公平的基础建模提供了design-first的蓝图。结果表明，PARAM-1可以作为一种合格的通用模型，也可以作为以印度为中心的应用的强大基线。

Conclusion: PARAM-1作为一个通用的模型和对以印度为中心的应用的可靠的基线。

Abstract: Large Language Models (LLMs) have emerged as powerful general-purpose
reasoning systems, yet their development remains dominated by English-centric
data, architectures, and optimization paradigms. This exclusionary design
results in structural under-representation of linguistically diverse regions
such as India, where over 20 official languages and 100+ dialects coexist
alongside phenomena like code-switching and diglossia. We introduce PARAM-1, a
2.9B parameter decoder-only, text-only language model trained from scratch with
an explicit architectural and linguistic focus on Indian diversity. PARAM-1 is
trained on a bilingual dataset consisting of only Hindi and English,
constructed with a strong focus on fact-rich, high-quality content. It is
guided by three core principles: equitable representation of Indic languages
through a 25% corpus allocation; tokenization fairness via a SentencePiece
tokenizer adapted to Indian morphological structures; and culturally aligned
evaluation benchmarks across IndicQA, code-mixed reasoning, and
socio-linguistic robustness tasks. By embedding diversity at the pretraining
level-rather than deferring it to post-hoc alignment-PARAM-1 offers a
design-first blueprint for equitable foundation modeling. Our results
demonstrate that it serves as both a competent general-purpose model and a
robust baseline for India-centric applications.

</details>


### [6] [TopicImpact: Improving Customer Feedback Analysis with Opinion Units for Topic Modeling and Star-Rating Prediction](https://arxiv.org/abs/2507.13392)
*Emil Häglund,Johanna Björklund*

Main category: cs.CL

TL;DR: The paper introduces a new topic modeling pipeline that operates on opinion units, leading to better performance and interpretable topics with sentiment, which can be correlated with business metrics to gain insights on customer concerns and their impact on business outcomes.


<details>
  <summary>Details</summary>
Motivation: Improve the extraction of insights from customer reviews.

Method: Restructuring the topic modelling pipeline to operate on opinion units - distinct statements that include relevant text excerpts and associated sentiment scores.

Result: Heightened performance of the subsequent topic modeling, leading to coherent and interpretable topics while also capturing the sentiment associated with each topic. Correlation of the topics and sentiments with business metrics, such as star ratings, provides insights on how specific customer concerns impact business outcomes.

Conclusion: The system's implementation, use cases, and advantages over other topic modeling and classification solutions are presented. The effectiveness in creating coherent topics and methods for integrating topic and sentiment modalities for accurate star-rating prediction are evaluated.

Abstract: We improve the extraction of insights from customer reviews by restructuring
the topic modelling pipeline to operate on opinion units - distinct statements
that include relevant text excerpts and associated sentiment scores. Prior work
has demonstrated that such units can be reliably extracted using large language
models. The result is a heightened performance of the subsequent topic
modeling, leading to coherent and interpretable topics while also capturing the
sentiment associated with each topic. By correlating the topics and sentiments
with business metrics, such as star ratings, we can gain insights on how
specific customer concerns impact business outcomes. We present our system's
implementation, use cases, and advantages over other topic modeling and
classification solutions. We also evaluate its effectiveness in creating
coherent topics and assess methods for integrating topic and sentiment
modalities for accurate star-rating prediction.

</details>


### [7] [Mitigating Stylistic Biases of Machine Translation Systems via Monolingual Corpora Only](https://arxiv.org/abs/2507.13395)
*Xuanqi Gao,Weipeng Jiang,Juan Zhai,Shiqing Ma,Siyi Xie,Xinyang Yin,Chao Shen*

Main category: cs.CL

TL;DR: Babel, a novel framework that enhances stylistic fidelity in NMT using only monolingual corpora.


<details>
  <summary>Details</summary>
Motivation: preserving stylistic nuances remains a significant challenge for neural machine translation (NMT)

Method: a style detector based on contextual embeddings and a diffusion-based style applicator

Result: identifies stylistic inconsistencies with 88.21% precision and improves stylistic preservation by 150% while maintaining a high semantic similarity score of 0.92

Conclusion: Babel can better preserve source text style while maintaining fluency and adequacy.

Abstract: The advent of neural machine translation (NMT) has revolutionized
cross-lingual communication, yet preserving stylistic nuances remains a
significant challenge. While existing approaches often require parallel corpora
for style preservation, we introduce Babel, a novel framework that enhances
stylistic fidelity in NMT using only monolingual corpora. Babel employs two key
components: (1) a style detector based on contextual embeddings that identifies
stylistic disparities between source and target texts, and (2) a
diffusion-based style applicator that rectifies stylistic inconsistencies while
maintaining semantic integrity. Our framework integrates with existing NMT
systems as a post-processing module, enabling style-aware translation without
requiring architectural modifications or parallel stylistic data. Extensive
experiments on five diverse domains (law, literature, scientific writing,
medicine, and educational content) demonstrate Babel's effectiveness: it
identifies stylistic inconsistencies with 88.21% precision and improves
stylistic preservation by 150% while maintaining a high semantic similarity
score of 0.92. Human evaluation confirms that translations refined by Babel
better preserve source text style while maintaining fluency and adequacy.

</details>


### [8] [Causal Language Control in Multilingual Transformers via Sparse Feature Steering](https://arxiv.org/abs/2507.13410)
*Cheng-Ting Chou,George Liu,Jessica Sun,Cole Blondin,Kevin Zhu,Vasu Sharma,Sean O'Brien*

Main category: cs.CL

TL;DR: 通过修改稀疏自编码器 (SAE) 特征，可以在大型多语言语言模型 (LLM) 的推理过程中引导生成的语言。


<details>
  <summary>Details</summary>
Motivation: 在零样本设置中，确定性地控制大型多语言语言模型 (LLM) 的目标生成语言仍然是一个根本性的挑战，在没有明确的语言提示或微调的情况下尤其如此。

Method: 利用在 Gemma-2B 和 Gemma-9B 的残差流上预训练的 SAE，我们识别出其激活在英语和四种目标语言（中文、日语、西班牙语和法语）之间差异最显着的特征。通过仅修改一个transformer层中的单个 SAE 特征，我们实现了高达 90% 成功率的受控语言转换。

Result: 通过修改一个transformer层中的单个 SAE 特征，实现了高达 90% 成功率的受控语言转换，同时根据 LaBSE（与语言无关的 BERT 句子嵌入）相似性保留了语义保真度。语言控制在中间到后面的transformer层中最有效，并且被与语言敏感的 SAE 特征不成比例地相关的特定注意力头放大。

Conclusion: 稀疏特征控制是一种用于可控多语言生成的轻量级和可解释的机制。

Abstract: Deterministically controlling the target generation language of large
multilingual language models (LLMs) remains a fundamental challenge,
particularly in zero-shot settings where neither explicit language prompts nor
fine-tuning are available. In this work, we investigate whether sparse
autoencoder (SAE) features, previously shown to correlate with interpretable
model behaviors, can be leveraged to steer the generated language of LLMs
during inference. Leveraging pretrained SAEs on the residual streams of
Gemma-2B and Gemma-9B, we identify features whose activations differ most
significantly between English and four target languages: Chinese, Japanese,
Spanish, and French. By modifying just a single SAE feature at one transformer
layer, we achieve controlled language shifts with up to 90\% success, as
measured by FastText language classification, while preserving semantic
fidelity according to LaBSE (Language-Agnostic BERT Sentence Embedding)
similarity. Our analysis reveals that language steering is most effective in
mid-to-late transformer layers and is amplified by specific attention heads
disproportionately associated with language-sensitive SAE features. These
results demonstrate the promise of sparse feature steering as a lightweight and
interpretable mechanism for controllable multilingual generation.

</details>


### [9] [Aligning Knowledge Graphs and Language Models for Factual Accuracy](https://arxiv.org/abs/2507.13411)
*Nur A Zarin Nishat,Andrea Coletta,Luigi Bellomarini,Kossi Amouzouvi,Jens Lehmann,Sahar Vahdati*

Main category: cs.CL

TL;DR: ALIGNed-LLM通过对齐知识图谱和文本嵌入，提高了语言模型的事实性和准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型容易产生幻觉，将知识图谱整合到语言模型中是一种有前景的解决方案，因为它提供了结构化、可靠、特定领域和最新的外部信息。

Method: 提出ALIGNed-LLM，通过一个可训练的投影层对齐实体和文本嵌入，将知识图谱融入语言模型的潜在空间。

Result: 在三个流行的问答基准数据集以及不同规模的语言模型上进行了测试，显示出显著的改进。此外，该方法应用于欧洲一家大型中央银行的实际金融用例，对LLM的答案进行了大幅改进。

Conclusion: ALIGNed-LLM通过将知识图谱嵌入融入语言模型的潜在空间，提高了语言模型的事实性和准确性，并在多个QA基准测试和实际金融用例中表现出显著改进。

Abstract: Large language models like GPT-4, Gemini, and Claude have transformed natural
language processing (NLP) tasks such as question answering, dialogue
generation, summarization, and so forth; yet their susceptibility to
hallucination stands as one of the major challenges. Among numerous approaches
to overcome this challenge, integration of Knowledge Graphs (KGs) into language
models has emerged as a promising solution as it provides structured, reliable,
domain-specific, and up-to-date external information to the language models. In
this paper, we introduce ALIGNed-LLM, a simple yet effective approach to
improve language models' factuality via a lean strategy to infuse KGs into the
latent space of language models inspired by LLaVA where visual and textual
information is infused. We use embeddings from a pre-trained Knowledge Graph
Embedding (KGE) model, such as TransE, and a trainable projection layer to
align entity and text embeddings. This alignment enables the language model to
distinguish between similar entities improving factual grounding and reducing
hallucination. We tested our approach on three popular questions-answering
benchmark datasets alongside language models of varying sizes, showing
significant improvement. Furthermore, we applied our approach to a real-world
financial use case from a large central bank in Europe, which demands high
accuracy and precision, demonstrating a substantial improvement of the LLM
answers.

</details>


### [10] [Consistent Explainers or Unreliable Narrators? Understanding LLM-generated Group Recommendations](https://arxiv.org/abs/2507.13705)
*Cedric Waterschoot,Nava Tintarev,Francesco Barile*

Main category: cs.CL

TL;DR: LLMs在GRS中生成的推荐类似于ADD聚合，但其解释不一致且含糊，影响了透明性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）越来越多地被用作群体推荐系统（GRS）的联合决策者和解释生成器。

Method: 通过将LLM生成的推荐和解释与基于社会选择的聚合策略进行比较来评估这些推荐和解释。

Result: LLM生成的推荐通常类似于ADD聚合产生的推荐。然而，这些解释通常指的是平均评分（类似于但不等同于ADD聚合）。组结构（统一或发散）对推荐没有影响。此外，LLMs经常声称额外的标准，如用户或项目相似性、多样性，或使用未定义的受欢迎程度指标或阈值。解释中的附加标准取决于群体场景中评分的数量，这表明标准聚合方法在更大的项目集大小上可能效率低下。

Conclusion: LLMs在GRS中的解释不一致且含糊，这削弱了透明性和可解释性，而这正是使用LLMs for GRS的关键动机。

Abstract: Large Language Models (LLMs) are increasingly being implemented as joint
decision-makers and explanation generators for Group Recommender Systems (GRS).
In this paper, we evaluate these recommendations and explanations by comparing
them to social choice-based aggregation strategies. Our results indicate that
LLM-generated recommendations often resembled those produced by Additive
Utilitarian (ADD) aggregation. However, the explanations typically referred to
averaging ratings (resembling but not identical to ADD aggregation). Group
structure, uniform or divergent, did not impact the recommendations.
Furthermore, LLMs regularly claimed additional criteria such as user or item
similarity, diversity, or used undefined popularity metrics or thresholds. Our
findings have important implications for LLMs in the GRS pipeline as well as
standard aggregation strategies. Additional criteria in explanations were
dependent on the number of ratings in the group scenario, indicating potential
inefficiency of standard aggregation methods at larger item set sizes.
Additionally, inconsistent and ambiguous explanations undermine transparency
and explainability, which are key motivations behind the use of LLMs for GRS.

</details>


### [11] [Paper Summary Attack: Jailbreaking LLMs through LLM Safety Papers](https://arxiv.org/abs/2507.13474)
*Liang Lin,Zhihao Xu,Xuehai Tang,Shi Liu,Biyu Zhou,Fuqing Zhu,Jizhong Han,Songlin Hu*

Main category: cs.CL

TL;DR: 该论文提出了一种新的攻击方法，利用llm对学术信息的信任倾向，发现llm存在安全漏洞，并在不同模型中观察到不同的漏洞偏见。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(llm)的安全性受到了广泛的研究关注。之前的经验研究表明，llm表现出信任来自学术论文等权威来源的信息的倾向，这意味着可能存在新的安全漏洞。本研究旨在验证这种可能性。

Method: 该研究提出了一种名为论文摘要攻击(psa)的新型jailbreaking方法。该方法系统地合成了来自以攻击为中心或以防御为中心的llm安全论文的内容，以构建对抗性提示模板，同时在预定义的小节中策略性地填充有害查询作为对抗性有效载荷。

Result: 论文摘要攻击(psa)在像claude3.5-sonnet这样对齐良好的模型上达到了97%的攻击成功率(asr)，在deepseek-r1上达到了更高的98%的asr。此外，该研究还揭示了不同基础模型之间，甚至同一模型的不同版本之间，在面对攻击或防御性论文时，存在截然相反的漏洞偏见。

Conclusion: 该研究揭示了大型语言模型(llm)在面对来自学术论文等权威来源的信息时，存在信任倾向，这导致了新的安全漏洞。通过提出的论文摘要攻击(psa)方法，该研究发现llm不仅在基础模型中存在漏洞，而且在最先进的推理模型(如deepseek-r1)中也存在漏洞。此外，研究还揭示了不同基础模型之间，甚至同一模型的不同版本之间，在面对攻击或防御性论文时，存在截然相反的漏洞偏见。

Abstract: The safety of large language models (LLMs) has garnered significant research
attention. In this paper, we argue that previous empirical studies demonstrate
LLMs exhibit a propensity to trust information from authoritative sources, such
as academic papers, implying new possible vulnerabilities. To verify this
possibility, a preliminary analysis is designed to illustrate our two findings.
Based on this insight, a novel jailbreaking method, Paper Summary Attack
(\llmname{PSA}), is proposed. It systematically synthesizes content from either
attack-focused or defense-focused LLM safety paper to construct an adversarial
prompt template, while strategically infilling harmful query as adversarial
payloads within predefined subsections. Extensive experiments show significant
vulnerabilities not only in base LLMs, but also in state-of-the-art reasoning
model like Deepseek-R1. PSA achieves a 97\% attack success rate (ASR) on
well-aligned models like Claude3.5-Sonnet and an even higher 98\% ASR on
Deepseek-R1. More intriguingly, our work has further revealed diametrically
opposed vulnerability bias across different base models, and even between
different versions of the same model, when exposed to either attack-focused or
defense-focused papers. This phenomenon potentially indicates future research
clues for both adversarial methodologies and safety alignment.Code is available
at https://github.com/233liang/Paper-Summary-Attack

</details>


### [12] [Question-Answer Extraction from Scientific Articles Using Knowledge Graphs and Large Language Models](https://arxiv.org/abs/2507.13827)
*Hosein Azarbonyad,Zi Long Zhu,Georgios Cheirmpos,Zubair Afzal,Vikrant Yadav,Georgios Tsatsaronis*

Main category: cs.CL

TL;DR: Two QA generation approaches (LLM and KG) are proposed to extract key concepts from scientific articles. KG approach performs better, and fine-tuning ER extraction model is crucial.


<details>
  <summary>Details</summary>
Motivation: Scholars need to quickly identify and understand the main ideas of scientific articles.

Method: Two approaches: 1) LLM to generate questions from salient paragraphs and rank them. 2) KG for QA generation using a fine-tuned ER extraction model and salient triplet extraction.

Result: KG-based approach is effective; fine-tuning ER extraction model is crucial.

Conclusion: KG-based approach effectively captures the main ideas discussed in the articles. Fine-tuning the ER extraction model on scientific corpus is crucial for extracting high-quality triplets.

Abstract: When deciding to read an article or incorporate it into their research,
scholars often seek to quickly identify and understand its main ideas. In this
paper, we aim to extract these key concepts and contributions from scientific
articles in the form of Question and Answer (QA) pairs. We propose two distinct
approaches for generating QAs. The first approach involves selecting salient
paragraphs, using a Large Language Model (LLM) to generate questions, ranking
these questions by the likelihood of obtaining meaningful answers, and
subsequently generating answers. This method relies exclusively on the content
of the articles. However, assessing an article's novelty typically requires
comparison with the existing literature. Therefore, our second approach
leverages a Knowledge Graph (KG) for QA generation. We construct a KG by
fine-tuning an Entity Relationship (ER) extraction model on scientific articles
and using it to build the graph. We then employ a salient triplet extraction
method to select the most pertinent ERs per article, utilizing metrics such as
the centrality of entities based on a triplet TF-IDF-like measure. This measure
assesses the saliency of a triplet based on its importance within the article
compared to its prevalence in the literature. For evaluation, we generate QAs
using both approaches and have them assessed by Subject Matter Experts (SMEs)
through a set of predefined metrics to evaluate the quality of both questions
and answers. Our evaluations demonstrate that the KG-based approach effectively
captures the main ideas discussed in the articles. Furthermore, our findings
indicate that fine-tuning the ER extraction model on our scientific corpus is
crucial for extracting high-quality triplets from such documents.

</details>


### [13] [Revisiting LLM Value Probing Strategies: Are They Robust and Expressive?](https://arxiv.org/abs/2507.13490)
*Siqi Shen,Mehar Singh,Lajanugen Logeswaran,Moontae Lee,Honglak Lee,Rada Mihalcea*

Main category: cs.CL

TL;DR: 我们评估了三种广泛使用的探测策略的价值表示的稳健性和表达性。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLM）的价值观取向已经进行了广泛的研究，因为它会影响跨人口群体的用户体验。然而，仍然存在一些挑战。首先，虽然多项选择题（MCQ）设置已被证明容易受到扰动的影响，但对于价值观探测的探测方法没有系统的比较。其次，目前还不清楚探测到的价值观在多大程度上捕捉了上下文信息，并反映了模型对现实世界行为的偏好。

Method: 我们使用了提示和选项的变化，展示了所有方法在输入扰动下都表现出很大的差异。我们还介绍了两个任务，研究这些价值观是否对人口背景敏感，以及它们在多大程度上与模型在与价值观相关的场景中的行为相一致。

Result: 人口背景对自由文本生成影响很小，模型的价值观与其对基于价值观的行为的偏好只有微弱的相关性。

Conclusion: LLM的价值观探究需要更谨慎的审查，并意识到其局限性。

Abstract: There has been extensive research on assessing the value orientation of Large
Language Models (LLMs) as it can shape user experiences across demographic
groups. However, several challenges remain. First, while the Multiple Choice
Question (MCQ) setting has been shown to be vulnerable to perturbations, there
is no systematic comparison of probing methods for value probing. Second, it is
unclear to what extent the probed values capture in-context information and
reflect models' preferences for real-world actions. In this paper, we evaluate
the robustness and expressiveness of value representations across three widely
used probing strategies. We use variations in prompts and options, showing that
all methods exhibit large variances under input perturbations. We also
introduce two tasks studying whether the values are responsive to demographic
context, and how well they align with the models' behaviors in value-related
scenarios. We show that the demographic context has little effect on the
free-text generation, and the models' values only weakly correlate with their
preference for value-based actions. Our work highlights the need for a more
careful examination of LLM value probing and awareness of its limitations.

</details>


### [14] [DENSE: Longitudinal Progress Note Generation with Temporal Modeling of Heterogeneous Clinical Notes Across Hospital Visits](https://arxiv.org/abs/2507.14079)
*Garapati Keerthana,Manik Gupta*

Main category: cs.CL

TL;DR: DENSE is a system designed to generate progress notes by retrieving relevant information from heterogeneous notes across visits and using it to prompt a large language model (LLM).


<details>
  <summary>Details</summary>
Motivation: Progress notes are severely underrepresented in large-scale EHR datasets, leaving gaps in longitudinal patient narratives.

Method: DENSE introduces a fine-grained note categorization and a temporal alignment mechanism that organizes heterogeneous notes across visits into structured, chronological inputs. At its core, DENSE leverages a clinically informed retrieval strategy to identify temporally and semantically relevant content from both current and prior visits. This retrieved evidence is used to prompt a large language model (LLM) to generate clinically coherent and temporally aware progress notes.

Result: The generated notes demonstrate strong longitudinal fidelity, achieving a temporal alignment ratio of $1.089$, surpassing the continuity observed in original notes.

Conclusion: DENSE can restore narrative coherence across fragmented documentation, supporting improved downstream tasks such as summarization, predictive modeling, and clinical decision support, offering a scalable solution for LLM-driven note synthesis in real-world healthcare settings.

Abstract: Progress notes are among the most clinically meaningful artifacts in an
Electronic Health Record (EHR), offering temporally grounded insights into a
patient's evolving condition, treatments, and care decisions. Despite their
importance, they are severely underrepresented in large-scale EHR datasets. For
instance, in the widely used Medical Information Mart for Intensive Care III
(MIMIC-III) dataset, only about $8.56\%$ of hospital visits include progress
notes, leaving gaps in longitudinal patient narratives. In contrast, the
dataset contains a diverse array of other note types, each capturing different
aspects of care.
  We present DENSE (Documenting Evolving Progress Notes from Scattered
Evidence), a system designed to align with clinical documentation workflows by
simulating how physicians reference past encounters while drafting progress
notes. The system introduces a fine-grained note categorization and a temporal
alignment mechanism that organizes heterogeneous notes across visits into
structured, chronological inputs. At its core, DENSE leverages a clinically
informed retrieval strategy to identify temporally and semantically relevant
content from both current and prior visits. This retrieved evidence is used to
prompt a large language model (LLM) to generate clinically coherent and
temporally aware progress notes.
  We evaluate DENSE on a curated cohort of patients with multiple visits and
complete progress note documentation. The generated notes demonstrate strong
longitudinal fidelity, achieving a temporal alignment ratio of $1.089$,
surpassing the continuity observed in original notes. By restoring narrative
coherence across fragmented documentation, our system supports improved
downstream tasks such as summarization, predictive modeling, and clinical
decision support, offering a scalable solution for LLM-driven note synthesis in
real-world healthcare settings.

</details>


### [15] [Encoding syntactic objects and Merge operations in function spaces](https://arxiv.org/abs/2507.13501)
*Matilde Marcolli,Robert C. Berwick*

Main category: cs.CL

TL;DR: The paper provides a mathematical argument for a neurocomputational realization of syntax, showing how syntactic structures can be represented in a function space and how Merge can be implemented as cross-frequency phase synchronization.


<details>
  <summary>Details</summary>
Motivation: To provide a mathematical argument showing that it is possible to construct a faithful representation of arbitrary syntactic objects in some function space.

Method: The authors construct a faithful representation of arbitrary syntactic objects in a function space endowed with a commutative non-associative semiring structure built using the second Renyi entropy. The action of Merge on workspaces is faithfully implemented as action on these circuits, through a coproduct and a Hopf algebra Markov chain. A particular case of this general construction is presented where this type of realization of Merge is implemented as a cross frequency phase synchronization on sinusoidal waves.

Result: The resulting representation of syntactic objects is compatible with the magma structure. The resulting set of functions is an algebra over an operad. Merge can be expressed in terms of the successor function of a semiring.

Conclusion: This paper shows the theoretical possibility of a neurocomputational realization of the core computational structure of syntax.

Abstract: We provide a mathematical argument showing that, given a representation of
lexical items as functions (wavelets, for instance) in some function space, it
is possible to construct a faithful representation of arbitrary syntactic
objects in the same function space. This space can be endowed with a
commutative non-associative semiring structure built using the second Renyi
entropy. The resulting representation of syntactic objects is compatible with
the magma structure. The resulting set of functions is an algebra over an
operad, where the operations in the operad model circuits that transform the
input wave forms into a combined output that encodes the syntactic structure.
The action of Merge on workspaces is faithfully implemented as action on these
circuits, through a coproduct and a Hopf algebra Markov chain. The results
obtained here provide a constructive argument showing the theoretical
possibility of a neurocomputational realization of the core computational
structure of syntax. We also present a particular case of this general
construction where this type of realization of Merge is implemented as a cross
frequency phase synchronization on sinusoidal waves. This also shows that Merge
can be expressed in terms of the successor function of a semiring, thus
clarifying the well known observation of its similarities with the successor
function of arithmetic.

</details>


### [16] [Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track](https://arxiv.org/abs/2507.14096)
*Brian Ondov,William Xia,Kush Attal,Ishita Unde,Jerry He,Hoa Dang,Ian Soboroff,Dina Demner-Fushman*

Main category: cs.CL

TL;DR: This paper introduces the PLABA track, which investigates the use of language models for simplifying biomedical text. The results show promise but also highlight challenges in simplicity, brevity, and automatic evaluation.


<details>
  <summary>Details</summary>
Motivation: Recent advances in language models have shown potential to adapt professional-facing biomedical literature to plain language, making it accessible to patients and caregivers. However, their unpredictability, combined with the high potential for harm in this domain, means rigorous evaluation is necessary. Our goals with this track were to stimulate research and to provide high-quality evaluation of the most promising systems.

Method: We hosted the Plain Language Adaptation of Biomedical Abstracts (PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included complete, sentence-level, rewriting of abstracts (Task 1) as well as identifying and replacing difficult terms (Task 2). For automatic evaluation of Task 1, we developed a four-fold set of professionally-written references. Submissions for both Tasks 1 and 2 were provided extensive manual evaluation from biomedical experts.

Result: Twelve teams spanning twelve countries participated in the track, with models from multilayer perceptrons to large pretrained transformers. In manual judgments of Task 1, top-performing models rivaled human levels of factual accuracy and completeness, but not simplicity or brevity. Automatic, reference-based metrics generally did not correlate well with manual judgments. In Task 2, systems struggled with identifying difficult terms and classifying how to replace them. When generating replacements, however, LLM-based systems did well in manually judged accuracy, completeness, and simplicity, though not in brevity.

Conclusion: PLABA track showed promise for using Large Language Models to adapt biomedical literature for the general public, while also highlighting their deficiencies and the need for improved automatic benchmarking tools.

Abstract: Objective: Recent advances in language models have shown potential to adapt
professional-facing biomedical literature to plain language, making it
accessible to patients and caregivers. However, their unpredictability,
combined with the high potential for harm in this domain, means rigorous
evaluation is necessary. Our goals with this track were to stimulate research
and to provide high-quality evaluation of the most promising systems.
  Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts
(PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included
complete, sentence-level, rewriting of abstracts (Task 1) as well as
identifying and replacing difficult terms (Task 2). For automatic evaluation of
Task 1, we developed a four-fold set of professionally-written references.
Submissions for both Tasks 1 and 2 were provided extensive manual evaluation
from biomedical experts.
  Results: Twelve teams spanning twelve countries participated in the track,
with models from multilayer perceptrons to large pretrained transformers. In
manual judgments of Task 1, top-performing models rivaled human levels of
factual accuracy and completeness, but not simplicity or brevity. Automatic,
reference-based metrics generally did not correlate well with manual judgments.
In Task 2, systems struggled with identifying difficult terms and classifying
how to replace them. When generating replacements, however, LLM-based systems
did well in manually judged accuracy, completeness, and simplicity, though not
in brevity.
  Conclusion: The PLABA track showed promise for using Large Language Models to
adapt biomedical literature for the general public, while also highlighting
their deficiencies and the need for improved automatic benchmarking tools.

</details>


### [17] [A Computational Approach to Modeling Conversational Systems: Analyzing Large-Scale Quasi-Patterned Dialogue Flows](https://arxiv.org/abs/2507.13544)
*Mohamed Achref Ben Ammar,Mohamed Taha Bennani*

Main category: cs.CL

TL;DR: propose a novel computational framework for constructing conversational graphs that capture the flow and structure of loosely organized dialogues, referred to as quasi-patterned conversations. We introduce the Filter & Reconnect method, a novel graph simplification technique that minimizes noise while preserving semantic coherence and structural integrity of conversational graphs.


<details>
  <summary>Details</summary>
Motivation: The analysis of conversational dynamics has gained increasing importance with the rise of large language model-based systems, which interact with users across diverse contexts.

Method: We introduce the Filter & Reconnect method, a novel graph simplification technique that minimizes noise while preserving semantic coherence and structural integrity of conversational graphs.

Result: the use of large language models combined with our graph simplification technique has resulted in semantic metric S increasing by a factor of 2.06 compared to previous approaches while simultaneously enforcing a tree-like structure with 0 {\&}delta-hyperbolicity, ensuring optimal clarity in conversation modeling.

Conclusion: This work provides a computational method for analyzing large-scale dialogue datasets, with practical applications related to monitoring automated systems such as chatbots, dialogue management tools, and user behavior analytics.

Abstract: The analysis of conversational dynamics has gained increasing importance with
the rise of large language model-based systems, which interact with users
across diverse contexts. In this work, we propose a novel computational
framework for constructing conversational graphs that capture the flow and
structure of loosely organized dialogues, referred to as quasi-patterned
conversations. We introduce the Filter & Reconnect method, a novel graph
simplification technique that minimizes noise while preserving semantic
coherence and structural integrity of conversational graphs. Through
comparative analysis, we demonstrate that the use of large language models
combined with our graph simplification technique has resulted in semantic
metric S increasing by a factor of 2.06 compared to previous approaches while
simultaneously enforcing a tree-like structure with 0 {\delta}-hyperbolicity,
ensuring optimal clarity in conversation modeling. This work provides a
computational method for analyzing large-scale dialogue datasets, with
practical applications related to monitoring automated systems such as
chatbots, dialogue management tools, and user behavior analytics.

</details>


### [18] [Reading Between the Lines: Combining Pause Dynamics and Semantic Coherence for Automated Assessment of Thought Disorder](https://arxiv.org/abs/2507.13551)
*Feng Chen,Weizhe Xu,Changye Li,Serguei Pakhomov,Alex Cohen,Simran Bhola,Sandy Yin,Sunny X Tang,Michael Mackinley,Lena Palaniyappan,Dror Ben-Zeev,Trevor Cohen*

Main category: cs.CL

TL;DR: This study demonstrates that pause features, when combined with semantic coherence metrics, improve the prediction of formal thought disorder (FTD) severity using automated speech analysis.


<details>
  <summary>Details</summary>
Motivation: Traditional clinical rating scales are resource-intensive and lack scalability. Automated speech analysis with automatic speech recognition (ASR) allows for objective quantification of linguistic and temporal features of speech, offering scalable alternatives. The utility of integrating these ASR-derived features for assessing FTD severity requires further evaluation.

Method: integrates pause features with semantic coherence metrics across three datasets: naturalistic self-recorded diaries, structured picture descriptions, and dream narratives. support vector regression (SVR) is used to predict clinical FTD scores.

Result: pause features alone robustly predict the severity of FTD. Integrating pause features with semantic coherence metrics enhanced predictive performance compared to semantic-only models. Performance gains held consistently across all contexts, though the nature of pause patterns was dataset-dependent.

Conclusion: Integrating pause features with semantic coherence metrics enhanced predictive performance compared to semantic-only models, with integration of independent models achieving correlations up to {ho} = 0.649 and AUC = 83.71% for severe cases detection. The performance gains from semantic and pause features integration held consistently across all contexts, though the nature of pause patterns was dataset-dependent. These findings suggest that frameworks combining temporal and semantic analyses provide a roadmap for refining the assessment of disorganized speech and advance automated speech analysis in psychosis.

Abstract: Formal thought disorder (FTD), a hallmark of schizophrenia spectrum
disorders, manifests as incoherent speech and poses challenges for clinical
assessment. Traditional clinical rating scales, though validated, are
resource-intensive and lack scalability. Automated speech analysis with
automatic speech recognition (ASR) allows for objective quantification of
linguistic and temporal features of speech, offering scalable alternatives. The
use of utterance timestamps in ASR captures pause dynamics, which are thought
to reflect the cognitive processes underlying speech production. However, the
utility of integrating these ASR-derived features for assessing FTD severity
requires further evaluation. This study integrates pause features with semantic
coherence metrics across three datasets: naturalistic self-recorded diaries
(AVH, n = 140), structured picture descriptions (TOPSY, n = 72), and dream
narratives (PsyCL, n = 43). We evaluated pause related features alongside
established coherence measures, using support vector regression (SVR) to
predict clinical FTD scores. Key findings demonstrate that pause features alone
robustly predict the severity of FTD. Integrating pause features with semantic
coherence metrics enhanced predictive performance compared to semantic-only
models, with integration of independent models achieving correlations up to
\r{ho} = 0.649 and AUC = 83.71% for severe cases detection (TOPSY, with best
\r{ho} = 0.584 and AUC = 79.23% for semantic-only models). The performance
gains from semantic and pause features integration held consistently across all
contexts, though the nature of pause patterns was dataset-dependent. These
findings suggest that frameworks combining temporal and semantic analyses
provide a roadmap for refining the assessment of disorganized speech and
advance automated speech analysis in psychosis.

</details>


### [19] [A Data-Centric Framework for Addressing Phonetic and Prosodic Challenges in Russian Speech Generative Models](https://arxiv.org/abs/2507.13563)
*Kirill Borodin,Nikita Vasiliev,Vasiliy Kudryavtsev,Maxim Maslov,Mikhail Gorodnichev,Oleg Rogov,Grach Mkrtchian*

Main category: cs.CL

TL;DR: Balalaika dataset improves Russian speech synthesis and enhancement.


<details>
  <summary>Details</summary>
Motivation: Russian speech synthesis presents distinctive challenges, including vowel reduction, consonant devoicing, variable stress patterns, homograph ambiguity, and unnatural intonation.

Method: A novel dataset comprising more than 2,000 hours of studio-quality Russian speech with comprehensive textual annotations.

Result: Models trained on Balalaika significantly outperform those trained on existing datasets in both speech synthesis and enhancement tasks.

Conclusion: Models trained on Balalaika outperform those trained on existing datasets in speech synthesis and enhancement.

Abstract: Russian speech synthesis presents distinctive challenges, including vowel
reduction, consonant devoicing, variable stress patterns, homograph ambiguity,
and unnatural intonation. This paper introduces Balalaika, a novel dataset
comprising more than 2,000 hours of studio-quality Russian speech with
comprehensive textual annotations, including punctuation and stress markings.
Experimental results show that models trained on Balalaika significantly
outperform those trained on existing datasets in both speech synthesis and
enhancement tasks. We detail the dataset construction pipeline, annotation
methodology, and results of comparative evaluations.

</details>


### [20] [Linguistic and Embedding-Based Profiling of Texts generated by Humans and Large Language Models](https://arxiv.org/abs/2507.13614)
*Sergio E. Zanotto,Segun Aroyehun*

Main category: cs.CL

TL;DR: This paper characterizes texts generated by LLMs using linguistic features, finding differences in syntactic structure and semantic content between human and machine-generated texts. Newer models show homogenization.


<details>
  <summary>Details</summary>
Motivation: While recent research has primarily focused on using LLMs to classify text as either human-written and machine-generated texts, our study focus on characterizing these texts

Method: characterizing these texts using a set of linguistic features across different linguistic levels such as morphology, syntax, and semantics. We calculate different linguistic features such as dependency length and emotionality and we use them for characterizing human-written and machine-generated texts along with different sampling strategies, repetition controls and model release date. Furthermore, we calculate the variability of our set of features across models and domains. Finally, we apply style embeddings to further test variability among human-written and machine-generated texts.

Result: human-written texts tend to exhibit simpler syntactic structures and more diverse semantic content. Both human and machine texts show stylistic diversity across domains, with humans displaying greater variation in our features.

Conclusion: Newer models output text that is similarly variable, pointing to an homogenization of machine-generated texts.

Abstract: The rapid advancements in large language models (LLMs) have significantly
improved their ability to generate natural language, making texts generated by
LLMs increasingly indistinguishable from human-written texts. While recent
research has primarily focused on using LLMs to classify text as either
human-written and machine-generated texts, our study focus on characterizing
these texts using a set of linguistic features across different linguistic
levels such as morphology, syntax, and semantics. We select a dataset of
human-written and machine-generated texts spanning 8 domains and produced by 11
different LLMs. We calculate different linguistic features such as dependency
length and emotionality and we use them for characterizing human-written and
machine-generated texts along with different sampling strategies, repetition
controls and model release date. Our statistical analysis reveals that
human-written texts tend to exhibit simpler syntactic structures and more
diverse semantic content. Furthermore, we calculate the variability of our set
of features across models and domains. Both human and machine texts show
stylistic diversity across domains, with humans displaying greater variation in
our features. Finally, we apply style embeddings to further test variability
among human-written and machine-generated texts. Notably, newer models output
text that is similarly variable, pointing to an homogenization of
machine-generated texts.

</details>


### [21] [Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters](https://arxiv.org/abs/2507.13618)
*Shanbo Cheng,Yu Bao,Qian Cao,Luyang Huang,Liyan Kang,Zhicheng Liu,Yu Lu,Wenhao Zhu,Zhichao Huang,Tao Li,Sitong Liu,Ningxin Peng,Shuaijie She,Lu Xu,Nuo Xu,Sen Yang,Runsheng Yu,Yiming Yu,Liehao Zou,Hang Li,Lu Lu,Yuxuan Wang,Yonghui Wu*

Main category: cs.CL

TL;DR: Seed-X, a 7B parameter open-source LLM, achieves translation performance comparable to Gemini-2.5 and GPT-4o across 28 languages using CoT reasoning and RL finetuning.


<details>
  <summary>Details</summary>
Motivation: Multilingual translation stands as a challenging task for large language models (LLMs) to handle intricate language patterns and stilted translations that arise in automated translations.

Method: The instruct model is finetuned to translate by Chain-of-Thought (CoT) reasoning and further enhanced through reinforcement learning (RL) to achieve better generalization across diverse language pairs.

Result: Seed-X, a family of open-source LLMs comprising instruct and reasoning models, pushing the limits of translation capability with 7B parameter size. The base model is pre-trained on a diverse, high-quality dataset encompassing both monolingual and bilingual content across 28 languages, harnessing the full potential of multilingual data.

Conclusion: Seed-X achieves performance comparable to leading closed-source models, including Gemini-2.5 and GPT-4o, across 28 languages, and significantly outperforms larger open-source models in both automatic metrics and human evaluations. The parameter is made publicly available.

Abstract: Multilingual translation stands as a challenging task for large language
models (LLMs) to handle intricate language patterns and stilted translations
that arise in automated translations. In this paper, we introduce Seed-X, a
family of open-source LLMs comprising instruct and reasoning models, pushing
the limits of translation capability with 7B parameter size. The base model is
pre-trained on a diverse, high-quality dataset encompassing both monolingual
and bilingual content across 28 languages, harnessing the full potential of
multilingual data. The instruct model is then finetuned to translate by
Chain-of-Thought (CoT) reasoning and further enhanced through reinforcement
learning (RL) to achieve better generalization across diverse language pairs.
Seed-X achieves performance comparable to leading closed-source models,
including Gemini-2.5 and GPT-4o, across 28 languages, and significantly
outperforms larger open-source models in both automatic metrics and human
evaluations. We share the best practices through our optimization process, and
make the parameter public available for advancing translation research and
applications.

</details>


### [22] [CU-ICU: Customizing Unsupervised Instruction-Finetuned Language Models for ICU Datasets via Text-to-Text Transfer Transformer](https://arxiv.org/abs/2507.13655)
*Teerapong Panboonyuen*

Main category: cs.CL

TL;DR: CU-ICU improves predictive accuracy and interpretability for ICU tasks with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Integrating large language models into specialized domains like healthcare presents unique challenges, including domain adaptation and limited labeled data.

Method: CU-ICU: a method for customizing unsupervised instruction-finetuned language models for ICU datasets by leveraging the Text-to-Text Transfer Transformer (T5) architecture, employs a sparse fine-tuning approach that combines few-shot prompting with selective parameter updates

Result: CU-ICU consistently improves predictive accuracy and interpretability over standard fine-tuning methods. achieves up to a 15% increase in sepsis detection accuracy and a 20% enhancement in generating clinically relevant explanations while updating fewer than 1% of model parameters

Conclusion: CU-ICU is a scalable, low-overhead solution for accurate and interpretable clinical decision support in real-world ICU environments.

Abstract: Integrating large language models into specialized domains like healthcare
presents unique challenges, including domain adaptation and limited labeled
data. We introduce CU-ICU, a method for customizing unsupervised
instruction-finetuned language models for ICU datasets by leveraging the
Text-to-Text Transfer Transformer (T5) architecture. CU-ICU employs a sparse
fine-tuning approach that combines few-shot prompting with selective parameter
updates, enabling efficient adaptation with minimal supervision. Our evaluation
across critical ICU tasks--early sepsis detection, mortality prediction, and
clinical note generation--demonstrates that CU-ICU consistently improves
predictive accuracy and interpretability over standard fine-tuning methods.
Notably, CU-ICU achieves up to a 15% increase in sepsis detection accuracy and
a 20% enhancement in generating clinically relevant explanations while updating
fewer than 1% of model parameters in its most efficient configuration. These
results establish CU-ICU as a scalable, low-overhead solution for delivering
accurate and interpretable clinical decision support in real-world ICU
environments.

</details>


### [23] [KiC: Keyword-inspired Cascade for Cost-Efficient Text Generation with LLMs](https://arxiv.org/abs/2507.13666)
*Woo-Chan Kim,Ji-Hoon Park,Seong-Whan Lee*

Main category: cs.CL

TL;DR: KiC is a cost-efficient framework for free-form text generation that achieves high accuracy while reducing API costs.


<details>
  <summary>Details</summary>
Motivation: Existing cascade approaches struggle to select a reliable representative response and assess the overall reliability of free-form outputs, as they rely on exact text matching.

Method: Keyword-inspired Cascade (KiC)

Result: KiC achieves 97.53 percent of GPT-4's accuracy while reducing API costs by 28.81 percent on average, and even outperforms GPT-4 in a specific benchmark.

Conclusion: KiC achieves 97.53 percent of GPT-4's accuracy while reducing API costs by 28.81 percent on average, and even outperforms GPT-4 in a specific benchmark.

Abstract: Large language models (LLMs) have demonstrated state-of-the-art performance
across a wide range of natural language processing tasks. However,
high-performing models are typically accessible only via APIs, incurring
substantial inference costs. Cascade methods address this by initially
employing a cheaper model and escalating to a stronger one only when necessary.
Nevertheless, existing cascade approaches struggle to select a reliable
representative response and assess the overall reliability of free-form
outputs, as they rely on exact text matching. To overcome these limitations, we
propose Keyword-inspired Cascade (KiC), a novel framework for cost-efficient
free-form text generation. KiC identifies the most representative answer among
multiple outputs from a weaker model and evaluates the semantic alignment of
other responses with it. Based on the degree of alignment, KiC determines
whether to accept the weaker model's output or escalate to a stronger model.
Experiments on three free-form text generation benchmarks show that KiC
achieves 97.53 percent of GPT-4's accuracy while reducing API costs by 28.81
percent on average, and even outperforms GPT-4 in a specific benchmark.

</details>


### [24] [LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues](https://arxiv.org/abs/2507.13681)
*Haoyang Li,Zhanchao Xu,Yiming Li,Xuejia Chen,Darian Li,Anxin Tian,Qingfa Xiao,Cheng Deng,Jun Wang,Qing Li,Lei Chen,Mingxuan Yuan*

Main category: cs.CL

TL;DR: LoopServe is an adaptive framework that accelerates large language model inference in multi-turn dialogues through online sparsification and progressive key value compression.


<details>
  <summary>Details</summary>
Motivation: existing large language models face increasing computational and memory challenges in multi-turn dialogues, and current acceleration methods do not adapt well to dynamic conversational patterns

Method: adaptive dual-phase inference acceleration framework with online sparsification and progressive key value compression

Result: LoopServe achieves superior effectiveness and significantly accelerates LLM inference

Conclusion: LoopServe consistently achieves superior effectiveness compared to existing baselines and significantly accelerates LLM inference across a wide range of long-context dialogue tasks.

Abstract: Multi-turn dialogues are essential in many real-world applications of large
language models, such as chatbots and virtual assistants. As conversation
histories become longer, existing large language models face increasing
computational and memory challenges, which hinder their ability to provide
efficient and responsive interactions. Most current acceleration methods either
compress the context or optimize key value caching, but they often rely on
fixed or position-based heuristics that do not adapt well to the dynamic and
unpredictable patterns found in actual multi-turn conversations. In this paper,
we present LoopServe, an adaptive dual-phase inference acceleration framework
for large language models in multi-turn dialogues. LoopServe introduces two
main innovations. First, it performs online sparsification during the
prefilling phase by dynamically selecting the most important parts of the
attention matrix for each new input. Second, it uses progressive key value
compression during decoding by adaptively maintaining a relevant and efficient
cache based on the most recently generated output tokens. We also propose a
\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new
benchmark} with eleven multi-turn datasets that reflect realistic query
positions and conversational dependencies. Extensive experiments demonstrate
that LoopServe consistently achieves superior effectiveness compared to
existing baselines and significantly accelerates LLM inference across a wide
range of long-context dialogue tasks.

</details>


### [25] [The Judge Variable: Challenging Judge-Agnostic Legal Judgment Prediction](https://arxiv.org/abs/2507.13732)
*Guillaume Zambrano*

Main category: cs.CL

TL;DR: This study uses machine learning to predict child custody outcomes, finding that individual judges' patterns significantly influence decisions, supporting legal realism.


<details>
  <summary>Details</summary>
Motivation: To test whether individual judges' decision-making patterns significantly influence case outcomes, challenging the assumption that judges are neutral variables that apply the law uniformly.

Method: A hybrid approach combining large language models (LLMs) for structured feature extraction and ML models for outcome prediction (RF, XGB and SVC).

Result: Specialist models consistently achieve higher predictive accuracy than the general model, with top-performing models reaching F1 scores as high as 92.85%, compared to the generalist model's 82.63%.

Conclusion: Specialist models consistently achieve higher predictive accuracy than the general model, with top-performing models reaching F1 scores as high as 92.85%, compared to the generalist model's 82.63% trained on 20x to 100x more samples. Specialist models capture stable individual patterns that are not transferable to other judges. In-Domain and Cross-Domain validity tests provide empirical support for legal realism, demonstrating that judicial identity plays a measurable role in legal outcomes.

Abstract: This study examines the role of human judges in legal decision-making by
using machine learning to predict child physical custody outcomes in French
appellate courts. Building on the legal realism-formalism debate, we test
whether individual judges' decision-making patterns significantly influence
case outcomes, challenging the assumption that judges are neutral variables
that apply the law uniformly. To ensure compliance with French privacy laws, we
implement a strict pseudonymization process. Our analysis uses 18,937 living
arrangements rulings extracted from 10,306 cases. We compare models trained on
individual judges' past rulings (specialist models) with a judge-agnostic model
trained on aggregated data (generalist models). The prediction pipeline is a
hybrid approach combining large language models (LLMs) for structured feature
extraction and ML models for outcome prediction (RF, XGB and SVC). Our results
show that specialist models consistently achieve higher predictive accuracy
than the general model, with top-performing models reaching F1 scores as high
as 92.85%, compared to the generalist model's 82.63% trained on 20x to 100x
more samples. Specialist models capture stable individual patterns that are not
transferable to other judges. In-Domain and Cross-Domain validity tests provide
empirical support for legal realism, demonstrating that judicial identity plays
a measurable role in legal outcomes. All data and code used will be made
available.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [26] [Open-Vocabulary Object Detection in UAV Imagery: A Review and Future Perspectives](https://arxiv.org/abs/2507.13359)
*Yang Zhou,Junjie Li,CongYang Ou,Dawei Yan,Haokui Zhang,Xizhe Xue*

Main category: cs.CV

TL;DR: This paper surveys open-vocabulary object detection in UAV aerial scenes, providing a taxonomy of methods, datasets, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: Traditional UAV aerial object detection methods primarily focus on detecting predefined categories, which significantly limits their applicability. The advent of cross-modal text-image alignment (e.g., CLIP) has overcome this limitation, enabling open-vocabulary object detection (OVOD).

Method: The paper constructs a systematic taxonomy that categorizes existing OVOD methods for aerial imagery and provides a comprehensive overview of the relevant datasets.

Result: The survey critically dissects the key challenges and open problems at the intersection of OVOD and UAV vision, and outlines promising future research directions and application prospects.

Conclusion: This survey provides a roadmap and reference for researchers in the field of open-vocabulary object detection (OVOD) in UAV aerial scenes, fostering innovation in this domain.

Abstract: Due to its extensive applications, aerial image object detection has long
been a hot topic in computer vision. In recent years, advancements in Unmanned
Aerial Vehicles (UAV) technology have further propelled this field to new
heights, giving rise to a broader range of application requirements. However,
traditional UAV aerial object detection methods primarily focus on detecting
predefined categories, which significantly limits their applicability. The
advent of cross-modal text-image alignment (e.g., CLIP) has overcome this
limitation, enabling open-vocabulary object detection (OVOD), which can
identify previously unseen objects through natural language descriptions. This
breakthrough significantly enhances the intelligence and autonomy of UAVs in
aerial scene understanding. This paper presents a comprehensive survey of OVOD
in the context of UAV aerial scenes. We begin by aligning the core principles
of OVOD with the unique characteristics of UAV vision, setting the stage for a
specialized discussion. Building on this foundation, we construct a systematic
taxonomy that categorizes existing OVOD methods for aerial imagery and provides
a comprehensive overview of the relevant datasets. This structured review
enables us to critically dissect the key challenges and open problems at the
intersection of these fields. Finally, based on this analysis, we outline
promising future research directions and application prospects. This survey
aims to provide a clear road map and a valuable reference for both newcomers
and seasoned researchers, fostering innovation in this rapidly evolving domain.
We keep tracing related works at
https://github.com/zhouyang2002/OVOD-in-UVA-imagery

</details>


### [27] [Low-Light Enhancement via Encoder-Decoder Network with Illumination Guidance](https://arxiv.org/abs/2507.13360)
*Le-Anh Tran,Chung Nguyen Tran,Ngoc-Luu Nguyen,Nhan Cach Dang,Jordi Carrabina,David Castells-Rufas,Minh Son Nguyen*

Main category: cs.CV

TL;DR: EDNIG是一种用于弱光图像增强的新型深度学习框架，它优于现有技术，且模型复杂性较低。


<details>
  <summary>Details</summary>
Motivation: 本文介绍了一种用于弱光图像增强的新型深度学习框架，名为具有照明指导的编码器-解码器网络（EDNIG）。

Method: 基于U-Net架构，EDNIG集成了从亮通道先验（BCP）导出的照明图作为指导输入。为了进一步提高模型的表征能力，结合了空间金字塔池化（SPP）模块来提取多尺度上下文特征。此外，采用Swish激活函数以确保训练期间更平滑的梯度传播。EDNIG在生成对抗网络（GAN）框架内使用复合损失函数进行优化，该函数结合了对抗损失、像素均方误差（MSE）和感知损失。

Result: 实验结果表明，EDNIG在定量指标和视觉质量上取得了与现有技术相比具有竞争力的性能，同时保持较低的模型复杂性。

Conclusion: EDNIG在定量指标和视觉质量上取得了与现有技术相比具有竞争力的性能，同时保持较低的模型复杂性，证明了其适用于实际应用。

Abstract: This paper introduces a novel deep learning framework for low-light image
enhancement, named the Encoder-Decoder Network with Illumination Guidance
(EDNIG). Building upon the U-Net architecture, EDNIG integrates an illumination
map, derived from Bright Channel Prior (BCP), as a guidance input. This
illumination guidance helps the network focus on underexposed regions,
effectively steering the enhancement process. To further improve the model's
representational power, a Spatial Pyramid Pooling (SPP) module is incorporated
to extract multi-scale contextual features, enabling better handling of diverse
lighting conditions. Additionally, the Swish activation function is employed to
ensure smoother gradient propagation during training. EDNIG is optimized within
a Generative Adversarial Network (GAN) framework using a composite loss
function that combines adversarial loss, pixel-wise mean squared error (MSE),
and perceptual loss. Experimental results show that EDNIG achieves competitive
performance compared to state-of-the-art methods in quantitative metrics and
visual quality, while maintaining lower model complexity, demonstrating its
suitability for real-world applications. The source code for this work is
available at https://github.com/tranleanh/ednig.

</details>


### [28] [VLMs have Tunnel Vision: Evaluating Nonlocal Visual Reasoning in Leading VLMs](https://arxiv.org/abs/2507.13361)
*Shmuel Berman,Jia Deng*

Main category: cs.CV

TL;DR: 视觉语言模型在非局部视觉推理方面存在缺陷，即使在简单的任务中也无法达到人类水平。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型 (VLM) 擅长复杂的视觉任务，例如 VQA 和图表理解，但最近的研究表明，它们在简单的感知测试中表现不佳。

Method: 我们提出了一项评估，测试视觉语言模型进行非局部视觉推理的能力，这种推理需要链接从图像中多个可能遥远的区域收集的证据。我们分离出三种不同形式的非局部视觉：比较感知，扫视搜索和平滑视觉搜索。

Result: 旗舰模型（例如，Gemini 2.5 Pro、Claude Vision 3.7、GPT-o4-mini），即使在之前的原始视觉基准测试中表现良好，也未能通过这些测试，并且在我们任务的两种变体中几乎没有超过随机准确率，而这些变体对于人类来说是微不足道的。

Conclusion: 尽管原始视觉敏锐度有所提高，但目前的模型缺乏核心的视觉推理能力。

Abstract: Visual Language Models (VLMs) excel at complex visual tasks such as VQA and
chart understanding, yet recent work suggests they struggle with simple
perceptual tests. We present an evaluation that tests vision-language models'
capacity for nonlocal visual reasoning -- reasoning that requires chaining
evidence collected from multiple, possibly distant, regions of an image. We
isolate three distinct forms of non-local vision: comparative perception, which
demands holding two images in working memory and comparing them; saccadic
search, which requires making discrete, evidence-driven jumps to locate
successive targets; and smooth visual search, which involves searching smoothly
along a continuous contour. Flagship models (e.g., Gemini 2.5 Pro, Claude
Vision 3.7, GPT-o4-mini), even those that perform well on prior
primitive-vision benchmarks, fail these tests and barely exceed random accuracy
on two variants of our tasks that are trivial for humans. Our structured
evaluation suite allows us to test if VLMs can perform similar visual
algorithms to humans. Our findings show that despite gains in raw visual
acuity, current models lack core visual reasoning capabilities.

</details>


### [29] [Enhancing Spatial Reasoning in Vision-Language Models via Chain-of-Thought Prompting and Reinforcement Learning](https://arxiv.org/abs/2507.13362)
*Binbin Ji,Siddharth Agrawal,Qiance Tang,Yvonne Wu*

Main category: cs.CV

TL;DR: 本研究通过强化学习和结构化提示来提高视觉语言模型的空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 研究视觉语言模型 (VLM) 的空间推理能力。

Method: 使用思维链 (CoT) 提示和强化学习研究视觉语言模型 (VLM) 的空间推理能力。使用 Group Relative Policy Optimization (GRPO) 在 SAT 数据集上对模型进行微调，并在 CVBench 上评估它们的性能。

Result: 简单的 CoT 格式无法提供帮助，甚至会损害模型原有的性能。基于场景图的结构化多阶段提示 (SceneGraph CoT) 显着提高了空间推理的准确性。GRPO 在 Pass@1 评估中实现了更高的准确性，并且在分布外 (OOD) 条件下表现出卓越的鲁棒性。SFT 过度拟合到表面语言模式，并且当测试时措辞发生变化时可能会降低性能。GRPO 可以更可靠地泛化，并在这种变化下保持稳定的性能。

Conclusion: 强化学习和结构化提示可以提高现代视觉语言模型的空间推理能力和泛化行为。

Abstract: This study investigates the spatial reasoning capabilities of vision-language
models (VLMs) through Chain-of-Thought (CoT) prompting and reinforcement
learning. We begin by evaluating the impact of different prompting strategies
and find that simple CoT formats, where the model generates a reasoning step
before the answer, not only fail to help, but can even harm the model's
original performance. In contrast, structured multi-stage prompting based on
scene graphs (SceneGraph CoT) significantly improves spatial reasoning
accuracy. Furthermore, to improve spatial reasoning ability, we fine-tune
models using Group Relative Policy Optimization (GRPO) on the SAT dataset and
evaluate their performance on CVBench. Compared to supervised fine-tuning
(SFT), GRPO achieves higher accuracy on Pass@1 evaluations and demonstrates
superior robustness under out-of-distribution (OOD) conditions. In particular,
we find that SFT overfits to surface-level linguistic patterns and may degrade
performance when test-time phrasing changes (e.g., from "closer to" to "farther
from"). GRPO, on the other hand, generalizes more reliably and maintains stable
performance under such shifts. Our findings provide insights into how
reinforcement learning and structured prompting improve the spatial reasoning
capabilities and generalization behavior of modern VLMs. All code is open
source at: https://github.com/Yvonne511/spatial-vlm-investigator

</details>


### [30] [Just Add Geometry: Gradient-Free Open-Vocabulary 3D Detection Without Human-in-the-Loop](https://arxiv.org/abs/2507.13363)
*Atharv Goel,Mehar Khurana*

Main category: cs.CV

TL;DR: This paper introduces a training-free and open-vocabulary 3D object detection method that leverages 2D foundation models. It achieves competitive localization performance using a pipeline that combines a 2D vision-language detector, SAM segmentation, and geometric inflation strategies.


<details>
  <summary>Details</summary>
Motivation: Modern 3D object detection datasets are constrained by narrow class taxonomies and costly manual annotations, limiting their ability to scale to open-world settings. In contrast, 2D vision-language models trained on web-scale image-text pairs exhibit rich semantic understanding and support open-vocabulary detection via natural language prompts. This work leverages the maturity and category diversity of 2D foundation models to perform open-vocabulary 3D object detection without any human-annotated 3D labels.

Method: This pipeline uses a 2D vision-language detector to generate text-conditioned proposals, which are segmented with SAM and back-projected into 3D using camera geometry and either LiDAR or monocular pseudo-depth. A geometric inflation strategy based on DBSCAN clustering and Rotating Calipers is introduced to infer 3D bounding boxes without training.

Result: The method achieves competitive localization performance across multiple settings, including LiDAR-based and purely RGB-D inputs, all while remaining training-free and open-vocabulary.

Conclusion: This method achieves competitive localization performance across multiple settings, including LiDAR-based and purely RGB-D inputs, all while remaining training-free and open-vocabulary. The results highlight the untapped potential of 2D foundation models for scalable 3D perception.

Abstract: Modern 3D object detection datasets are constrained by narrow class
taxonomies and costly manual annotations, limiting their ability to scale to
open-world settings. In contrast, 2D vision-language models trained on
web-scale image-text pairs exhibit rich semantic understanding and support
open-vocabulary detection via natural language prompts. In this work, we
leverage the maturity and category diversity of 2D foundation models to perform
open-vocabulary 3D object detection without any human-annotated 3D labels.
  Our pipeline uses a 2D vision-language detector to generate text-conditioned
proposals, which are segmented with SAM and back-projected into 3D using camera
geometry and either LiDAR or monocular pseudo-depth. We introduce a geometric
inflation strategy based on DBSCAN clustering and Rotating Calipers to infer 3D
bounding boxes without training. To simulate adverse real-world conditions, we
construct Pseudo-nuScenes, a fog-augmented, RGB-only variant of the nuScenes
dataset.
  Experiments demonstrate that our method achieves competitive localization
performance across multiple settings, including LiDAR-based and purely RGB-D
inputs, all while remaining training-free and open-vocabulary. Our results
highlight the untapped potential of 2D foundation models for scalable 3D
perception. We open-source our code and resources at
https://github.com/atharv0goel/open-world-3D-det.

</details>


### [31] [OmniVec2 -- A Novel Transformer based Network for Large Scale Multimodal and Multitask Learning](https://arxiv.org/abs/2507.13364)
*Siddharth Srivastava,Gaurav Sharma*

Main category: cs.CV

TL;DR: a novel multimodal multitask network and a training algorithm are proposed, which achieves state-of-the-art performance on 25 datasets.


<details>
  <summary>Details</summary>
Motivation: addresses multimodal and multitask scenarios by incorporating modality-specific task heads for different tasks in respective modalities

Method: a novel multimodal multitask network with modality specialized tokenizers, a shared transformer architecture, and cross-attention mechanisms

Result: project data from different modalities into a unified embedding space

Conclusion: achieve state-of-the-art performance on 25 datasets from 12 modalities, demonstrating the effectiveness of the proposed architecture, pretraining strategy and adapted multitask training.

Abstract: We present a novel multimodal multitask network and associated training
algorithm. The method is capable of ingesting data from approximately 12
different modalities namely image, video, audio, text, depth, point cloud, time
series, tabular, graph, X-ray, infrared, IMU, and hyperspectral. The proposed
approach utilizes modality specialized tokenizers, a shared transformer
architecture, and cross-attention mechanisms to project the data from different
modalities into a unified embedding space. It addresses multimodal and
multitask scenarios by incorporating modality-specific task heads for different
tasks in respective modalities. We propose a novel pretraining strategy with
iterative modality switching to initialize the network, and a training
algorithm which trades off fully joint training over all modalities, with
training on pairs of modalities at a time. We provide comprehensive evaluation
across 25 datasets from 12 modalities and show state of the art performances,
demonstrating the effectiveness of the proposed architecture, pretraining
strategy and adapted multitask training.

</details>


### [32] [Transformer-Based Framework for Motion Capture Denoising and Anomaly Detection in Medical Rehabilitation](https://arxiv.org/abs/2507.13371)
*Yeming Cai,Yang Wang,Zhenglin Li*

Main category: cs.CV

TL;DR: This paper introduces a deep learning framework using optical motion capture and a Transformer model for enhanced, scalable, and cost-effective remote medical rehabilitation, addressing data noise and ensuring patient safety with real-time anomaly detection.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle data noise and missing data in medical rehabilitation caused by occlusion and environmental factors, while detecting abnormal movements in real time to ensure patient safety.

Method: The paper proposes an end-to-end deep learning framework integrating optical motion capture with a Transformer-based model.

Result: The framework denoises and completes motion capture data, improving robustness.

Conclusion: The proposed framework shows superior performance in data reconstruction and anomaly detection on stroke and orthopedic rehabilitation datasets, offering a scalable and cost-effective solution for remote rehabilitation with reduced on-site supervision.

Abstract: This paper proposes an end-to-end deep learning framework integrating optical
motion capture with a Transformer-based model to enhance medical
rehabilitation. It tackles data noise and missing data caused by occlusion and
environmental factors, while detecting abnormal movements in real time to
ensure patient safety. Utilizing temporal sequence modeling, our framework
denoises and completes motion capture data, improving robustness. Evaluations
on stroke and orthopedic rehabilitation datasets show superior performance in
data reconstruction and anomaly detection, providing a scalable, cost-effective
solution for remote rehabilitation with reduced on-site supervision.

</details>


### [33] [Smart Routing for Multimodal Video Retrieval: When to Search What](https://arxiv.org/abs/2507.13374)
*Kevin Dela Rosa*

Main category: cs.CV

TL;DR: ModaRoute, an LLM-based intelligent routing system, dynamically selects optimal modalities for multimodal video retrieval, reducing computational overhead by 41% while achieving 60.9% Recall@5


<details>
  <summary>Details</summary>
Motivation: dense text captions require expensive offline processing and miss critical visual information present in 34% of clips with scene text not captured by ASR

Method: ModaRoute uses GPT-4.1 to route queries across ASR (speech), OCR (text), and visual indices

Result: ModaRoute reduces computational overhead by 41% while achieving 60.9% Recall@5, averaging 1.78 modalities per query versus exhaustive 3.0 modality search

Conclusion: intelligent routing provides a practical solution for scaling multimodal retrieval systems, reducing infrastructure costs while maintaining competitive effectiveness for real-world deployment

Abstract: We introduce ModaRoute, an LLM-based intelligent routing system that
dynamically selects optimal modalities for multimodal video retrieval. While
dense text captions can achieve 75.9% Recall@5, they require expensive offline
processing and miss critical visual information present in 34% of clips with
scene text not captured by ASR. By analyzing query intent and predicting
information needs, ModaRoute reduces computational overhead by 41% while
achieving 60.9% Recall@5. Our approach uses GPT-4.1 to route queries across ASR
(speech), OCR (text), and visual indices, averaging 1.78 modalities per query
versus exhaustive 3.0 modality search. Evaluation on 1.8M video clips
demonstrates that intelligent routing provides a practical solution for scaling
multimodal retrieval systems, reducing infrastructure costs while maintaining
competitive effectiveness for real-world deployment.

</details>


### [34] [Enhancing Breast Cancer Detection with Vision Transformers and Graph Neural Networks](https://arxiv.org/abs/2507.13372)
*Yeming Cai,Zhenglin Li,Yang Wang*

Main category: cs.CV

TL;DR: Combines ViT and GNN for better breast cancer detection, achieving 84.2% accuracy and interpretable results.


<details>
  <summary>Details</summary>
Motivation: Early detection of breast cancer is critical for improving survival rates.

Method: Integrates Vision Transformers (ViT) and Graph Neural Networks (GNN).

Result: Enhanced breast cancer detection using the CBIS-DDSM dataset with improved accuracy.

Conclusion: Achieved 84.2% accuracy in breast cancer detection, outperforming traditional methods, and provides interpretable attention heatmaps for aiding radiologists.

Abstract: Breast cancer is a leading cause of death among women globally, and early
detection is critical for improving survival rates. This paper introduces an
innovative framework that integrates Vision Transformers (ViT) and Graph Neural
Networks (GNN) to enhance breast cancer detection using the CBIS-DDSM dataset.
Our framework leverages ViT's ability to capture global image features and
GNN's strength in modeling structural relationships, achieving an accuracy of
84.2%, outperforming traditional methods. Additionally, interpretable attention
heatmaps provide insights into the model's decision-making process, aiding
radiologists in clinical settings.

</details>


### [35] [Butter: Frequency Consistency and Hierarchical Fusion for Autonomous Driving Object Detection](https://arxiv.org/abs/2507.13373)
*Xiaojian Lin,Wenxin Zhang,Yuchu Jiang,Wangyu Wu,Yiran Guo,Kangxu Wang,Zongzheng Zhang,Guijin Wang,Lei Jin,Hao Zhao*

Main category: cs.CV

TL;DR: Butter通过增强分层特征表示，提高了自动驾驶目标检测的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有架构（如YOLO和DETR）难以维持跨不同尺度的特征一致性，同时平衡检测精度和计算效率。

Method: 该论文提出了Butter，一种新颖的目标检测框架，旨在增强分层特征表示，以提高检测的鲁棒性。该框架引入了两个关键创新：频率自适应特征一致性增强（FAFCE）组件和渐进式分层特征融合网络（PHFFNet）模块。

Result: 在BDD100K、KITTI和Cityscapes上的大量实验表明，Butter具有卓越的特征表示能力，从而显着提高了检测精度，同时降低了模型复杂度。

Conclusion: Butter在自动驾驶场景中实现了精度、可部署性和计算效率之间的平衡。

Abstract: Hierarchical feature representations play a pivotal role in computer vision,
particularly in object detection for autonomous driving. Multi-level semantic
understanding is crucial for accurately identifying pedestrians, vehicles, and
traffic signs in dynamic environments. However, existing architectures, such as
YOLO and DETR, struggle to maintain feature consistency across different scales
while balancing detection precision and computational efficiency. To address
these challenges, we propose Butter, a novel object detection framework
designed to enhance hierarchical feature representations for improving
detection robustness. Specifically, Butter introduces two key innovations:
Frequency-Adaptive Feature Consistency Enhancement (FAFCE) Component, which
refines multi-scale feature consistency by leveraging adaptive frequency
filtering to enhance structural and boundary precision, and Progressive
Hierarchical Feature Fusion Network (PHFFNet) Module, which progressively
integrates multi-level features to mitigate semantic gaps and strengthen
hierarchical feature learning. Through extensive experiments on BDD100K, KITTI,
and Cityscapes, Butter demonstrates superior feature representation
capabilities, leading to notable improvements in detection accuracy while
reducing model complexity. By focusing on hierarchical feature refinement and
integration, Butter provides an advanced approach to object detection that
achieves a balance between accuracy, deployability, and computational
efficiency in real-time autonomous driving scenarios. Our model and
implementation are publicly available at https://github.com/Aveiro-Lin/Butter,
facilitating further research and validation within the autonomous driving
community.

</details>


### [36] [A Comprehensive Survey for Real-World Industrial Defect Detection: Challenges, Approaches, and Prospects](https://arxiv.org/abs/2507.13378)
*Yuqi Cheng,Yunkang Cao,Haiming Yao,Wei Luo,Cheng Jiang,Hui Zhang,Weiming Shen*

Main category: cs.CV

TL;DR: This survey analyzes 2D and 3D defect detection strategies, focusing on open-set techniques, challenges, and trends in the rapidly advancing field.


<details>
  <summary>Details</summary>
Motivation: Conventional inspection approaches are increasingly found wanting in addressing real-world demands. A cohesive and contemporary understanding of industrial defect detection remains elusive.

Method: a survey of both closed-set and open-set defect detection strategies within 2D and 3D modalities

Result: undisclosed

Conclusion: This survey provides an in-depth analysis of both closed-set and open-set defect detection strategies within 2D and 3D modalities, charting their evolution in recent years and underscoring the rising prominence of open-set techniques. It also highlights critical challenges and emerging trends.

Abstract: Industrial defect detection is vital for upholding product quality across
contemporary manufacturing systems. As the expectations for precision,
automation, and scalability intensify, conventional inspection approaches are
increasingly found wanting in addressing real-world demands. Notable progress
in computer vision and deep learning has substantially bolstered defect
detection capabilities across both 2D and 3D modalities. A significant
development has been the pivot from closed-set to open-set defect detection
frameworks, which diminishes the necessity for extensive defect annotations and
facilitates the recognition of novel anomalies. Despite such strides, a
cohesive and contemporary understanding of industrial defect detection remains
elusive. Consequently, this survey delivers an in-depth analysis of both
closed-set and open-set defect detection strategies within 2D and 3D
modalities, charting their evolution in recent years and underscoring the
rising prominence of open-set techniques. We distill critical challenges
inherent in practical detection environments and illuminate emerging trends,
thereby providing a current and comprehensive vista of this swiftly progressing
field.

</details>


### [37] [Using Multiple Input Modalities Can Improve Data-Efficiency and O.O.D. Generalization for ML with Satellite Imagery](https://arxiv.org/abs/2507.13385)
*Arjun Rao,Esther Rolf*

Main category: cs.CV

TL;DR: fusing additional geographic inputs with optical imagery can significantly improve SatML model performance, especially when labeled data are limited


<details>
  <summary>Details</summary>
Motivation: To better understand the value of using other input modalities alongside optical imagery in supervised learning settings

Method: generate augmented versions of SatML benchmark tasks by appending additional geographic data layers to datasets spanning classification, regression, and segmentation

Result: hard-coded fusion strategies outperform learned variants

Conclusion: fusing additional geographic inputs with optical imagery can significantly improve SatML model performance. Benefits are largest in settings where labeled data are limited and in geographic out-of-sample settings

Abstract: A large variety of geospatial data layers is available around the world
ranging from remotely-sensed raster data like satellite imagery, digital
elevation models, predicted land cover maps, and human-annotated data, to data
derived from environmental sensors such as air temperature or wind speed data.
A large majority of machine learning models trained on satellite imagery
(SatML), however, are designed primarily for optical input modalities such as
multi-spectral satellite imagery. To better understand the value of using other
input modalities alongside optical imagery in supervised learning settings, we
generate augmented versions of SatML benchmark tasks by appending additional
geographic data layers to datasets spanning classification, regression, and
segmentation. Using these augmented datasets, we find that fusing additional
geographic inputs with optical imagery can significantly improve SatML model
performance. Benefits are largest in settings where labeled data are limited
and in geographic out-of-sample settings, suggesting that multi-modal inputs
may be especially valuable for data-efficiency and out-of-sample performance of
SatML models. Surprisingly, we find that hard-coded fusion strategies
outperform learned variants, with interesting implications for future work.

</details>


### [38] [Minimalist Concept Erasure in Generative Models](https://arxiv.org/abs/2507.13386)
*Yang Zhang,Er Jin,Yanfei Dong,Yixuan Wu,Philip Torr,Ashkan Khakzar,Johannes Stegmaier,Kenji Kawaguchi*

Main category: cs.CV

TL;DR: 提出了一种新的极简概念擦除方法，该方法稳健地擦除概念，同时不降低整体模型性能，从而实现更安全、更负责的生成模型。


<details>
  <summary>Details</summary>
Motivation: 生成模型依赖于大规模无标签数据，引发了严重的安全和版权问题。现有的擦除方法涉及过多的修改，从而损害了模型的整体效用。

Method: 提出了一种新的极简概念擦除目标，该目标仅基于最终生成输出的分布距离。利用通过所有生成步骤的反向传播进行端到端的可微优化，推导出一个易于处理的损失。为了提高擦除的鲁棒性，采用神经元掩蔽作为模型微调的替代方案。

Result: 在最先进的flow-matching模型上的经验评估表明，该方法能够稳健地擦除概念，同时不降低整体模型性能。

Conclusion: 该方法能够稳健地擦除概念，同时不降低整体模型性能，为更安全、更负责的生成模型铺平了道路。

Abstract: Recent advances in generative models have demonstrated remarkable
capabilities in producing high-quality images, but their reliance on
large-scale unlabeled data has raised significant safety and copyright
concerns. Efforts to address these issues by erasing unwanted concepts have
shown promise. However, many existing erasure methods involve excessive
modifications that compromise the overall utility of the model. In this work,
we address these issues by formulating a novel minimalist concept erasure
objective based \emph{only} on the distributional distance of final generation
outputs. Building on our formulation, we derive a tractable loss for
differentiable optimization that leverages backpropagation through all
generation steps in an end-to-end manner. We also conduct extensive analysis to
show theoretical connections with other models and methods. To improve the
robustness of the erasure, we incorporate neuron masking as an alternative to
model fine-tuning. Empirical evaluations on state-of-the-art flow-matching
models demonstrate that our method robustly erases concepts without degrading
overall model performance, paving the way for safer and more responsible
generative models.

</details>


### [39] [From Binary to Semantic: Utilizing Large-Scale Binary Occupancy Data for 3D Semantic Occupancy Prediction](https://arxiv.org/abs/2507.13387)
*Chihiro Noguchi,Takaki Yamamoto*

Main category: cs.CV

TL;DR: This paper introduces a framework that utilizes large-scale binary occupancy data to improve 3D semantic occupancy prediction, outperforming existing methods in pre-training and auto-labeling.


<details>
  <summary>Details</summary>
Motivation: annotated LiDAR point clouds are required, making data acquisition costly. In contrast, large-scale binary occupancy data, which only indicate occupied or free space without semantic labels, can be collected at a lower cost. Despite their availability, the potential of leveraging such data remains unexplored

Method: a novel binary occupancy-based framework that decomposes the prediction process into binary and semantic occupancy modules, enabling effective use of binary occupancy data

Result: demonstrate that the proposed framework outperforms existing methods in both pre-training and auto-labeling tasks

Conclusion: The proposed framework outperforms existing methods in both pre-training and auto-labeling tasks, highlighting its effectiveness in enhancing 3D semantic occupancy prediction.

Abstract: Accurate perception of the surrounding environment is essential for safe
autonomous driving. 3D occupancy prediction, which estimates detailed 3D
structures of roads, buildings, and other objects, is particularly important
for vision-centric autonomous driving systems that do not rely on LiDAR
sensors. However, in 3D semantic occupancy prediction -- where each voxel is
assigned a semantic label -- annotated LiDAR point clouds are required, making
data acquisition costly. In contrast, large-scale binary occupancy data, which
only indicate occupied or free space without semantic labels, can be collected
at a lower cost. Despite their availability, the potential of leveraging such
data remains unexplored. In this study, we investigate the utilization of
large-scale binary occupancy data from two perspectives: (1) pre-training and
(2) learning-based auto-labeling. We propose a novel binary occupancy-based
framework that decomposes the prediction process into binary and semantic
occupancy modules, enabling effective use of binary occupancy data. Our
experimental results demonstrate that the proposed framework outperforms
existing methods in both pre-training and auto-labeling tasks, highlighting its
effectiveness in enhancing 3D semantic occupancy prediction. The code is
available at https://github.com/ToyotaInfoTech/b2s-occupancy

</details>


### [40] [InSyn: Modeling Complex Interactions for Pedestrian Trajectory Prediction](https://arxiv.org/abs/2507.13397)
*Kaiyuan Zhai,Juan Chen,Chao Wang,Zeyi Xu*

Main category: cs.CV

TL;DR: This paper introduces InSyn, a Transformer-based model, and a training strategy called SSOS for pedestrian trajectory prediction. InSyn captures diverse interaction patterns and SSOS alleviates initial-step divergence. The model outperforms baselines, especially in crowded scenarios.


<details>
  <summary>Details</summary>
Motivation: Accurate pedestrian trajectory prediction is crucial for intelligent applications, yet it remains highly challenging due to the complexity of interactions among pedestrians. Previous methods have primarily relied on relative positions to model pedestrian interactions; however, they tend to overlook specific interaction patterns such as paired walking or conflicting behaviors, limiting the prediction accuracy in crowded scenarios.

Method: We propose InSyn (Interaction-Synchronization Network), a novel Transformer-based model that explicitly captures diverse interaction patterns while effectively modeling direction-sensitive social behaviors. Additionally, we introduce a training strategy termed Seq-Start of Seq (SSOS).

Result: Our model outperforms recent baselines significantly, especially in high-density scenarios. Furthermore, the SSOS strategy proves effective in improving sequential prediction performance, reducing the initial-step prediction error by approximately 6.58%.

Conclusion: The proposed InSyn model outperforms recent baselines significantly, especially in high-density scenarios. The SSOS strategy proves effective in improving sequential prediction performance, reducing the initial-step prediction error by approximately 6.58%.

Abstract: Accurate pedestrian trajectory prediction is crucial for intelligent
applications, yet it remains highly challenging due to the complexity of
interactions among pedestrians. Previous methods have primarily relied on
relative positions to model pedestrian interactions; however, they tend to
overlook specific interaction patterns such as paired walking or conflicting
behaviors, limiting the prediction accuracy in crowded scenarios. To address
this issue, we propose InSyn (Interaction-Synchronization Network), a novel
Transformer-based model that explicitly captures diverse interaction patterns
(e.g., walking in sync or conflicting) while effectively modeling
direction-sensitive social behaviors. Additionally, we introduce a training
strategy termed Seq-Start of Seq (SSOS), designed to alleviate the common issue
of initial-step divergence in numerical time-series prediction. Experiments on
the ETH and UCY datasets demonstrate that our model outperforms recent
baselines significantly, especially in high-density scenarios. Furthermore, the
SSOS strategy proves effective in improving sequential prediction performance,
reducing the initial-step prediction error by approximately 6.58%.

</details>


### [41] [MADI: Masking-Augmented Diffusion with Inference-Time Scaling for Visual Editing](https://arxiv.org/abs/2507.13401)
*Shreya Kadambi,Risheek Garrepalli,Shubhankar Borse,Munawar Hyatt,Fatih Porikli*

Main category: cs.CV

TL;DR: MADI通过新颖的训练策略和推理时容量缩放机制，提高了扩散模型的可编辑性、组合性和可控性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在文本到图像生成方面取得了显著成功，但其在有根据的视觉编辑和组合控制方面的有效性仍然具有挑战性。受到自监督学习和上下文生成建模进展的推动。

Method: Masking-Augmented Diffusion with Inference-Time Scaling (MADI)

Result: MADI显著提高了扩散模型的可编辑性。

Conclusion: MADI通过MAgD和基于Pause Tokens的推理时容量缩放机制，显著提高了扩散模型的可编辑性，为将其集成到更通用的、上下文生成扩散架构中铺平了道路。

Abstract: Despite the remarkable success of diffusion models in text-to-image
generation, their effectiveness in grounded visual editing and compositional
control remains challenging. Motivated by advances in self-supervised learning
and in-context generative modeling, we propose a series of simple yet powerful
design choices that significantly enhance diffusion model capacity for
structured, controllable generation and editing. We introduce Masking-Augmented
Diffusion with Inference-Time Scaling (MADI), a framework that improves the
editability, compositionality and controllability of diffusion models through
two core innovations. First, we introduce Masking-Augmented gaussian Diffusion
(MAgD), a novel training strategy with dual corruption process which combines
standard denoising score matching and masked reconstruction by masking noisy
input from forward process. MAgD encourages the model to learn discriminative
and compositional visual representations, thus enabling localized and
structure-aware editing. Second, we introduce an inference-time capacity
scaling mechanism based on Pause Tokens, which act as special placeholders
inserted into the prompt for increasing computational capacity at inference
time. Our findings show that adopting expressive and dense prompts during
training further enhances performance, particularly for MAgD. Together, these
contributions in MADI substantially enhance the editability of diffusion
models, paving the way toward their integration into more general-purpose,
in-context generative diffusion architectures.

</details>


### [42] [UL-DD: A Multimodal Drowsiness Dataset Using Video, Biometric Signals, and Behavioral Data](https://arxiv.org/abs/2507.13403)
*Morteza Bodaghi,Majid Hosseini,Raju Gottumukkala,Ravi Teja Bhupatiraju,Iftikhar Ahmad,Moncef Gabbouj*

Main category: cs.CV

TL;DR: 本研究提出了一个用于驾驶员困倦检测的综合公共数据集，该数据集集成了面部、行为和生物识别指标的多模态信号。


<details>
  <summary>Details</summary>
Motivation: 现有的驾驶员困倦检测数据集不足以捕获驾驶员状态的渐进变化和各种生理、行为和驾驶相关信号。

Method: 使用深度相机、红外相机素材、后视频和生物特征信号（如心率、皮肤电活动、血氧饱和度、皮肤温度和加速度计数据）集成面部、行为和生物识别指标的多模态信号。

Result: 该数据集包括来自 19 名受试者（15 名男性，4 名女性）的数据，这些数据是在两种情况下收集的：当他们完全清醒时以及当他们表现出困倦迹象时。与其它数据集不同，我们的多模态数据集对于每个受试者的每次数据收集会话具有 40 分钟的连续时长，总长度为 1,400 分钟，并且我们记录了驾驶员状态的逐渐变化，而不是离散的清醒/困倦标签。

Conclusion: 本研究旨在创建一个全面的驾驶员困倦多模态数据集，该数据集捕获更广泛的生理、行为和驾驶相关信号。该数据集将应要求提供给通讯作者。

Abstract: In this study, we present a comprehensive public dataset for driver
drowsiness detection, integrating multimodal signals of facial, behavioral, and
biometric indicators. Our dataset includes 3D facial video using a depth
camera, IR camera footage, posterior videos, and biometric signals such as
heart rate, electrodermal activity, blood oxygen saturation, skin temperature,
and accelerometer data. This data set provides grip sensor data from the
steering wheel and telemetry data from the American truck simulator game to
provide more information about drivers' behavior while they are alert and
drowsy. Drowsiness levels were self-reported every four minutes using the
Karolinska Sleepiness Scale (KSS). The simulation environment consists of three
monitor setups, and the driving condition is completely like a car. Data were
collected from 19 subjects (15 M, 4 F) in two conditions: when they were fully
alert and when they exhibited signs of sleepiness. Unlike other datasets, our
multimodal dataset has a continuous duration of 40 minutes for each data
collection session per subject, contributing to a total length of 1,400
minutes, and we recorded gradual changes in the driver state rather than
discrete alert/drowsy labels. This study aims to create a comprehensive
multimodal dataset of driver drowsiness that captures a wider range of
physiological, behavioral, and driving-related signals. The dataset will be
available upon request to the corresponding author.

</details>


### [43] [AortaDiff: Volume-Guided Conditional Diffusion Models for Multi-Branch Aortic Surface Generation](https://arxiv.org/abs/2507.13404)
*Delin An,Pan Du,Jian-Xun Wang,Chaoli Wang*

Main category: cs.CV

TL;DR: AortaDiff 是一种基于扩散的框架，可直接从 CT/MRI 容积生成平滑的主动脉表面，与现有方法相比，它对大型标记数据集的依赖性最小，并且能够生成具有高几何保真度的 CFD 兼容主动脉网格。


<details>
  <summary>Details</summary>
Motivation: 精确的 3D 主动脉构建对于临床诊断、术前计划和计算流体动力学 (CFD) 模拟至关重要，因为它能够估计关键的血流动力学参数，例如血流速度、压力分布和壁剪应力。现有的构建方法通常依赖于大型带注释的训练数据集和广泛的人工干预。虽然生成的网格可以用于可视化目的，但它们难以生成几何一致、结构良好的表面，适用于下游 CFD 分析。

Method: AortaDiff 是一种基于扩散的框架，可直接从 CT/MRI 容积生成平滑的主动脉表面。AortaDiff 首先采用容积引导的条件扩散模型 (CDM) 以迭代方式生成以容积医学图像为条件的中心线。然后，每个中心线点自动用作提取相应血管轮廓的提示，确保准确的边界描绘。最后，将提取的轮廓拟合成平滑的 3D 表面，从而生成连续的、CFD 兼容的网格表示。

Result: 实验结果表明，即使在有限的训练数据下，AortaDiff 也能有效执行，成功构建正常和病理改变的主动脉网格，包括动脉瘤或缩窄病例。AortaDiff 优于现有方法，包括端到端工作流程、对大型标记数据集的最小依赖性以及生成具有高几何保真度的 CFD 兼容主动脉网格的能力。

Conclusion: AortaDiff 能够有效地构建正常和病理改变的主动脉网格，包括动脉瘤或主动脉缩窄的病例。这使得能够生成高质量的可视化效果，并将 AortaDiff 定位为心血管研究的实用解决方案。

Abstract: Accurate 3D aortic construction is crucial for clinical diagnosis,
preoperative planning, and computational fluid dynamics (CFD) simulations, as
it enables the estimation of critical hemodynamic parameters such as blood flow
velocity, pressure distribution, and wall shear stress. Existing construction
methods often rely on large annotated training datasets and extensive manual
intervention. While the resulting meshes can serve for visualization purposes,
they struggle to produce geometrically consistent, well-constructed surfaces
suitable for downstream CFD analysis. To address these challenges, we introduce
AortaDiff, a diffusion-based framework that generates smooth aortic surfaces
directly from CT/MRI volumes. AortaDiff first employs a volume-guided
conditional diffusion model (CDM) to iteratively generate aortic centerlines
conditioned on volumetric medical images. Each centerline point is then
automatically used as a prompt to extract the corresponding vessel contour,
ensuring accurate boundary delineation. Finally, the extracted contours are
fitted into a smooth 3D surface, yielding a continuous, CFD-compatible mesh
representation. AortaDiff offers distinct advantages over existing methods,
including an end-to-end workflow, minimal dependency on large labeled datasets,
and the ability to generate CFD-compatible aorta meshes with high geometric
fidelity. Experimental results demonstrate that AortaDiff performs effectively
even with limited training data, successfully constructing both normal and
pathologically altered aorta meshes, including cases with aneurysms or
coarctation. This capability enables the generation of high-quality
visualizations and positions AortaDiff as a practical solution for
cardiovascular research.

</details>


### [44] [COREVQA: A Crowd Observation and Reasoning Entailment Visual Question Answering Benchmark](https://arxiv.org/abs/2507.13405)
*Ishant Chintapatla,Kazuma Choji,Naaisha Agarwal,Andrew Lin,Hannah You,Charles Duong,Kevin Zhu,Sean O'Brien,Vasu Sharma*

Main category: cs.CV

TL;DR: 提出了一个新的基准测试，用于评估视觉语言模型在拥挤场景中进行视觉蕴涵推理的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试很少测试模型准确完成视觉蕴涵的能力，例如，基于图像接受或反驳假设。

Method: 提出了COREVQA（Crowd Observations and Reasoning Entailment），这是一个包含5608个图像和合成生成的真/假语句对的基准，图像来自CrowdHuman数据集，旨在激发对具有挑战性的拥挤图像的视觉蕴涵推理。

Result: 即使是最优秀的视觉语言模型(VLMs)的准确率也低于80%，其他模型的表现则更差(39.98%-69.95%)。

Conclusion: 即使是最优秀的视觉语言模型(VLMs)的准确率也低于80%，其他模型的表现则更差(39.98%-69.95%)。这一显著的性能差距揭示了VLMs在对拥挤场景中的某些类型的图像-问题对进行推理时的关键局限性。

Abstract: Recently, many benchmarks and datasets have been developed to evaluate
Vision-Language Models (VLMs) using visual question answering (VQA) pairs, and
models have shown significant accuracy improvements. However, these benchmarks
rarely test the model's ability to accurately complete visual entailment, for
instance, accepting or refuting a hypothesis based on the image. To address
this, we propose COREVQA (Crowd Observations and Reasoning Entailment), a
benchmark of 5608 image and synthetically generated true/false statement pairs,
with images derived from the CrowdHuman dataset, to provoke visual entailment
reasoning on challenging crowded images. Our results show that even the
top-performing VLMs achieve accuracy below 80%, with other models performing
substantially worse (39.98%-69.95%). This significant performance gap reveals
key limitations in VLMs' ability to reason over certain types of image-question
pairs in crowded scenes.

</details>


### [45] [IConMark: Robust Interpretable Concept-Based Watermark For AI Images](https://arxiv.org/abs/2507.13407)
*Vinu Sankar Sadasivan,Mehrdad Saberi,Soheil Feizi*

Main category: cs.CV

TL;DR: 提出了一种名为IConMark的AI生成图像水印方法，该方法通过嵌入可解释的概念来实现鲁棒性和可读性，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 区分AI生成的图像和真实图像变得至关重要，传统水印技术在对抗性攻击中显示出漏洞。

Method: 提出了一种新颖的生成式鲁棒语义水印方法IConMark，该方法将可解释的概念嵌入到人工智能生成的图像中。

Result: IConMark不仅对各种图像增强具有鲁棒性，而且是人类可读的，能够手动验证水印。详细的评估表明了其在检测精度和保持图像质量方面的优越性。

Conclusion: IConMark及其变体在水印检测方面的AUROC得分比最佳基线分别高出10.8%、14.5%和15.9%。

Abstract: With the rapid rise of generative AI and synthetic media, distinguishing
AI-generated images from real ones has become crucial in safeguarding against
misinformation and ensuring digital authenticity. Traditional watermarking
techniques have shown vulnerabilities to adversarial attacks, undermining their
effectiveness in the presence of attackers. We propose IConMark, a novel
in-generation robust semantic watermarking method that embeds interpretable
concepts into AI-generated images, as a first step toward interpretable
watermarking. Unlike traditional methods, which rely on adding noise or
perturbations to AI-generated images, IConMark incorporates meaningful semantic
attributes, making it interpretable to humans and hence, resilient to
adversarial manipulation. This method is not only robust against various image
augmentations but also human-readable, enabling manual verification of
watermarks. We demonstrate a detailed evaluation of IConMark's effectiveness,
demonstrating its superiority in terms of detection accuracy and maintaining
image quality. Moreover, IConMark can be combined with existing watermarking
techniques to further enhance and complement its robustness. We introduce
IConMark+SS and IConMark+TM, hybrid approaches combining IConMark with
StegaStamp and TrustMark, respectively, to further bolster robustness against
multiple types of image manipulations. Our base watermarking technique
(IConMark) and its variants (+TM and +SS) achieve 10.8%, 14.5%, and 15.9%
higher mean area under the receiver operating characteristic curve (AUROC)
scores for watermark detection, respectively, compared to the best baseline on
various datasets.

</details>


### [46] [A Deep Learning-Based Ensemble System for Automated Shoulder Fracture Detection in Clinical Radiographs](https://arxiv.org/abs/2507.13408)
*Hemanth Kumar M,Karthika M,Saianiruth M,Vasanthakumar Venugopal,Anandakumar D,Revathi Ezhumalai,Charulatha K,Kishore Kumar J,Dayana G,Kalyan Sivasailam,Bargava Subramanian*

Main category: cs.CV

TL;DR: Developed an AI system using deep learning and ensemble techniques to reliably detect shoulder fractures in X-rays, achieving high accuracy and F1-score.


<details>
  <summary>Details</summary>
Motivation: Shoulder fractures are often underdiagnosed, especially in emergency and high-volume clinical settings. Studies report up to 10% of such fractures may be missed by radiologists. AI-driven tools offer a scalable way to assist early detection and reduce diagnostic delays. Addressed this gap through a dedicated AI system for shoulder radiographs.

Method: Developed a multi-model deep learning system using 10,000 annotated shoulder X-rays. Architectures include Faster R-CNN (ResNet50-FPN, ResNeXt), EfficientDet, and RF-DETR. Enhanced detection using Soft-NMS, WBF, and NMW fusion.

Result: The NMW ensemble achieved 95.5% accuracy and an F1-score of 0.9610, outperforming individual models across all key metrics. It demonstrated strong recall and localization precision, confirming its effectiveness for clinical fracture detection in shoulder X-rays.

Conclusion: Ensemble-based AI can reliably detect shoulder fractures in radiographs with high clinical relevance. The model's accuracy and deployment readiness position it well for integration into real-time diagnostic workflows. The current model is limited to binary fracture detection, reflecting its design for rapid screening and triage support rather than detailed orthopedic classification.

Abstract: Background: Shoulder fractures are often underdiagnosed, especially in
emergency and high-volume clinical settings. Studies report up to 10% of such
fractures may be missed by radiologists. AI-driven tools offer a scalable way
to assist early detection and reduce diagnostic delays. We address this gap
through a dedicated AI system for shoulder radiographs. Methods: We developed a
multi-model deep learning system using 10,000 annotated shoulder X-rays.
Architectures include Faster R-CNN (ResNet50-FPN, ResNeXt), EfficientDet, and
RF-DETR. To enhance detection, we applied bounding box and classification-level
ensemble techniques such as Soft-NMS, WBF, and NMW fusion. Results: The NMW
ensemble achieved 95.5% accuracy and an F1-score of 0.9610, outperforming
individual models across all key metrics. It demonstrated strong recall and
localization precision, confirming its effectiveness for clinical fracture
detection in shoulder X-rays. Conclusion: The results show ensemble-based AI
can reliably detect shoulder fractures in radiographs with high clinical
relevance. The model's accuracy and deployment readiness position it well for
integration into real-time diagnostic workflows. The current model is limited
to binary fracture detection, reflecting its design for rapid screening and
triage support rather than detailed orthopedic classification.

</details>


### [47] [AI-ming backwards: Vanishing archaeological landscapes in Mesopotamia and automatic detection of sites on CORONA imagery](https://arxiv.org/abs/2507.13420)
*Alessandro Pistola,Valentina Orru',Nicolo' Marchetti,Marco Roccetti*

Main category: cs.CV

TL;DR: 通过使用旧的卫星图像重新训练深度学习模型，该研究提高了考古遗址的检测精度，并发现了四个新的考古遗址。


<details>
  <summary>Details</summary>
Motivation: 利用CORONA的知识来升级现有的深度学习模型，以改进AI模型对考古遗址的自动识别能力，因为在过去的五十年中，环境已经发生了彻底的改变，包括许多遗址的完全破坏。

Method: 使用CORONA卫星图像对基于Bing的卷积网络模型进行再训练。

Result: 检测精度显著提高：在图像分割层面，交并比（IoU）值超过85%，检测考古遗址的总体准确率达到90%。重新训练的模型能够识别出四个新的考古遗址（通过现场验证确认），这些遗址以前未被考古学家用传统技术识别出来。

Conclusion: 使用AI技术和1960年的CORONA图像可以发现目前已不可见的考古遗址，对于研究因人为因素导致考古证据消失的景观具有重大意义。

Abstract: By upgrading an existing deep learning model with the knowledge provided by
one of the oldest sets of grayscale satellite imagery, known as CORONA, we
improved the AI model attitude towards the automatic identification of
archaeological sites in an environment which has been completely transformed in
the last five decades, including the complete destruction of many of those same
sites. The initial Bing based convolutional network model was retrained using
CORONA satellite imagery for the district of Abu Ghraib, west of Baghdad,
central Mesopotamian floodplain. The results were twofold and surprising.
First, the detection precision obtained on the area of interest increased
sensibly: in particular, the Intersection over Union (IoU) values, at the image
segmentation level, surpassed 85 percent, while the general accuracy in
detecting archeological sites reached 90 percent. Second, our retrained model
allowed the identification of four new sites of archaeological interest
(confirmed through field verification), previously not identified by
archaeologists with traditional techniques. This has confirmed the efficacy of
using AI techniques and the CORONA imagery from the 1960 to discover
archaeological sites currently no longer visible, a concrete breakthrough with
significant consequences for the study of landscapes with vanishing
archaeological evidence induced by anthropization

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [48] [GraphTrafficGPT: Enhancing Traffic Management Through Graph-Based AI Agent Coordination](https://arxiv.org/abs/2507.13511)
*Nabil Abdelaziz Ferhat Taleb,Abdolazim Rezaei,Raj Atulkumar Patel,Mehdi Sookhak*

Main category: cs.AI

TL;DR: GraphTrafficGPT是一种新的图架构，它通过并行处理和优化资源分配，提高了LLM在交通管理中的效率，显著优于TrafficGPT。


<details>
  <summary>Details</summary>
Motivation: 当前的基于链的系统在复杂、现实世界的场景中效率低下，因为它们存在顺序任务执行、高令牌使用和差的可扩展性等问题。

Method: 提出了一种基于图的新型架构GraphTrafficGPT，用于LLM驱动的交通应用。

Result: 实验结果表明，与TrafficGPT相比，GraphTrafficGPT减少了50.2%的令牌消耗和19.0%的平均响应延迟，同时支持并发多查询执行，效率提高了23.0%。

Conclusion: GraphTrafficGPT通过并行执行和动态资源分配，显著提高了交通管理任务的效率和响应速度。

Abstract: Large Language Models (LLMs) offer significant promise for intelligent
traffic management; however, current chain-based systems like TrafficGPT are
hindered by sequential task execution, high token usage, and poor scalability,
making them inefficient for complex, real-world scenarios. To address these
limitations, we propose GraphTrafficGPT, a novel graph-based architecture,
which fundamentally redesigns the task coordination process for LLM-driven
traffic applications. GraphTrafficGPT represents tasks and their dependencies
as nodes and edges in a directed graph, enabling efficient parallel execution
and dynamic resource allocation. The main idea behind the proposed model is a
Brain Agent that decomposes user queries, constructs optimized dependency
graphs, and coordinates a network of specialized agents for data retrieval,
analysis, visualization, and simulation. By introducing advanced context-aware
token management and supporting concurrent multi-query processing, the proposed
architecture handles interdependent tasks typical of modern urban mobility
environments. Experimental results demonstrate that GraphTrafficGPT reduces
token consumption by 50.2% and average response latency by 19.0% compared to
TrafficGPT, while supporting simultaneous multi-query execution with up to
23.0% improvement in efficiency.

</details>


### [49] [PrefPalette: Personalized Preference Modeling with Latent Attributes](https://arxiv.org/abs/2507.13541)
*Shuyue Stella Li,Melanie Sclar,Hunter Lang,Ansong Ni,Jacqueline He,Puxin Xu,Andrew Cohen,Chan Young Park,Yulia Tsvetkov,Asli Celikyilmaz*

Main category: cs.AI

TL;DR: PrefPalette通过将偏好分解为属性维度并根据社会社区价值观进行定制，优于GPT-4o 46.6%的平均预测准确率。


<details>
  <summary>Details</summary>
Motivation: 当前偏好模型通常将人类判断视为黑盒，但个性化AI系统不仅需要理解用户的偏好，还需要理解这些偏好背后的原因。

Method: PrefPalette框架将偏好分解为属性维度，并以人类可解释的方式根据不同的社会社区价值观定制其偏好预测。它通过以下两种方式实现多属性决策：(1) 可扩展的反事实属性合成步骤，涉及生成合成训练数据以分离个体属性效应；(2) 基于注意力的偏好建模，学习不同的社会社区如何动态地权衡这些属性。

Result: 在对来自在线平台Reddit的45个社会社区进行评估时，PrefPalette的平均预测准确率比GPT-4o高出46.6%。

Conclusion: PrefPalette通过对人类判断的属性中介结构进行建模，实现了卓越的偏好建模和透明、可解释的洞察力，是朝着更值得信赖、具有价值意识的个性化应用迈出的第一步。

Abstract: Personalizing AI systems requires understanding not just what users prefer,
but the reasons that underlie those preferences - yet current preference models
typically treat human judgment as a black box. We introduce PrefPalette, a
framework that decomposes preferences into attribute dimensions and tailors its
preference prediction to distinct social community values in a
human-interpretable manner. PrefPalette operationalizes a cognitive science
principle known as multi-attribute decision making in two ways: (1) a scalable
counterfactual attribute synthesis step that involves generating synthetic
training data to isolate for individual attribute effects (e.g., formality,
humor, cultural values), and (2) attention-based preference modeling that
learns how different social communities dynamically weight these attributes.
This approach moves beyond aggregate preference modeling to capture the diverse
evaluation frameworks that drive human judgment. When evaluated on 45 social
communities from the online platform Reddit, PrefPalette outperforms GPT-4o by
46.6% in average prediction accuracy. Beyond raw predictive improvements,
PrefPalette also shed light on intuitive, community-specific profiles:
scholarly communities prioritize verbosity and stimulation, conflict-oriented
communities value sarcasm and directness, and support-based communities
emphasize empathy. By modeling the attribute-mediated structure of human
judgment, PrefPalette delivers both superior preference modeling and
transparent, interpretable insights, and serves as a first step toward more
trustworthy, value-aware personalized applications.

</details>


### [50] [GOFAI meets Generative AI: Development of Expert Systems by means of Large Language Models](https://arxiv.org/abs/2507.13550)
*Eduardo C. Garrido-Merchán,Cristina Puente*

Main category: cs.AI

TL;DR: 本文介绍了一种使用 LLM 以受控和透明的方式开发专家系统的新方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 的发展已成功转变了基于知识的系统，例如开放领域问题解答，它可以自动生成大量看似连贯的信息。然而，这些模型有几个缺点，例如幻觉或自信地生成不正确或无法验证的事实。

Method: 通过限制领域和采用结构良好的基于提示的提取方法，我们以Prolog生成知识的符号表示。

Result: 通过使用 Claude Sonnet 3.7 和 GPT-4.1 进行的定量和定性实验，我们表明我们生成的知识库对事实和语义连贯性有很强的坚持。

Conclusion: 我们提出了一个透明的混合解决方案，它结合了LLM的召回能力和符号系统的精确性，从而为敏感领域中可靠的AI应用奠定了基础。

Abstract: The development of large language models (LLMs) has successfully transformed
knowledge-based systems such as open domain question nswering, which can
automatically produce vast amounts of seemingly coherent information. Yet,
those models have several disadvantages like hallucinations or confident
generation of incorrect or unverifiable facts. In this paper, we introduce a
new approach to the development of expert systems using LLMs in a controlled
and transparent way. By limiting the domain and employing a well-structured
prompt-based extraction approach, we produce a symbolic representation of
knowledge in Prolog, which can be validated and corrected by human experts.
This approach also guarantees interpretability, scalability and reliability of
the developed expert systems. Via quantitative and qualitative experiments with
Claude Sonnet 3.7 and GPT-4.1, we show strong adherence to facts and semantic
coherence on our generated knowledge bases. We present a transparent hybrid
solution that combines the recall capacity of LLMs with the precision of
symbolic systems, thereby laying the foundation for dependable AI applications
in sensitive domains.

</details>


### [51] [Why Isn't Relational Learning Taking Over the World?](https://arxiv.org/abs/2507.13558)
*David Poole*

Main category: cs.AI

TL;DR: This paper discusses why relational learning has not become more prominent despite the prevalence of relational data.


<details>
  <summary>Details</summary>
Motivation: The world is arguably made up, not of pixels, words, and phonemes but of entities (objects, things, including events) with properties and relations among them. Surely we should model these, not the perception or description of them.

Method: relational learning, statistical relational AI

Result: relational learning is not taking over the world -- except in a few cases with restricted relations

Conclusion: This paper explains why relational learning is not taking over the world -- except in a few cases with restricted relations -- and what needs to be done to bring it to it's rightful prominence.

Abstract: AI seems to be taking over the world with systems that model pixels, words,
and phonemes. The world is arguably made up, not of pixels, words, and phonemes
but of entities (objects, things, including events) with properties and
relations among them. Surely we should model these, not the perception or
description of them. You might suspect that concentrating on modeling words and
pixels is because all of the (valuable) data in the world is in terms of text
and images. If you look into almost any company you will find their most
valuable data is in spreadsheets, databases and other relational formats. These
are not the form that are studied in introductory machine learning, but are
full of product numbers, student numbers, transaction numbers and other
identifiers that can't be interpreted naively as numbers. The field that
studies this sort of data has various names including relational learning,
statistical relational AI, and many others. This paper explains why relational
learning is not taking over the world -- except in a few cases with restricted
relations -- and what needs to be done to bring it to it's rightful prominence.

</details>


### [52] [BifrostRAG: Bridging Dual Knowledge Graphs for Multi-Hop Question Answering in Construction Safety](https://arxiv.org/abs/2507.13625)
*Yuxin Zhang,Xi Wang,Mo Hu,Zhenyu Zhang*

Main category: cs.AI

TL;DR: BifrostRAG, a dual-graph RAG system, outperforms existing methods in retrieving information from complex regulatory texts for compliance checking by combining linguistic and structural analysis.


<details>
  <summary>Details</summary>
Motivation: Information retrieval and question answering from safety regulations are essential for automated construction compliance checking but are hindered by the linguistic and structural complexity of regulatory text. Many compliance-related queries are multi-hop, requiring synthesis of information across interlinked clauses. This poses a challenge for traditional retrieval-augmented generation (RAG) systems.

Method: a dual-graph RAG-integrated system that explicitly models both linguistic relationships and document structure, powering a hybrid retrieval mechanism that combines graph traversal with vector-based semantic search

Result: BifrostRAG achieves 92.8 percent precision, 85.5 percent recall, and an F1 score of 87.3 percent. These results significantly outperform vector-only and graph-only RAG baselines.

Conclusion: BifrostRAG is a robust knowledge engine for LLM-driven compliance checking, offering a transferable blueprint for navigating complex technical documents.

Abstract: Information retrieval and question answering from safety regulations are
essential for automated construction compliance checking but are hindered by
the linguistic and structural complexity of regulatory text. Many
compliance-related queries are multi-hop, requiring synthesis of information
across interlinked clauses. This poses a challenge for traditional
retrieval-augmented generation (RAG) systems. To overcome this, we introduce
BifrostRAG: a dual-graph RAG-integrated system that explicitly models both
linguistic relationships (via an Entity Network Graph) and document structure
(via a Document Navigator Graph). This architecture powers a hybrid retrieval
mechanism that combines graph traversal with vector-based semantic search,
enabling large language models to reason over both the meaning and the
structure of the text. Evaluation on a multi-hop question dataset shows that
BifrostRAG achieves 92.8 percent precision, 85.5 percent recall, and an F1
score of 87.3 percent. These results significantly outperform vector-only and
graph-only RAG baselines that represent current leading approaches. Error
analysis further highlights the comparative advantages of our hybrid method
over single-modality RAGs. These findings establish BifrostRAG as a robust
knowledge engine for LLM-driven compliance checking. Its dual-graph, hybrid
retrieval mechanism offers a transferable blueprint for navigating complex
technical documents across knowledge-intensive engineering domains.

</details>


### [53] [Buggy rule diagnosis for combined steps through final answer evaluation in stepwise tasks](https://arxiv.org/abs/2507.13651)
*Gerben van der Hoek,Johan Jeuring,Rogier Bos*

Main category: cs.AI

TL;DR: Using a final answer to diagnose a combination of steps can mitigate the combinatorial explosion in error diagnosis.


<details>
  <summary>Details</summary>
Motivation: error diagnosis is hard when a student combines several steps in one step, because the number of possible paths connecting consecutive inputs may be very large. Using a final answer to diagnose a combination of steps can mitigate the combinatorial explosion

Method: design of a service that provides a buggy rule diagnosis when a student combines several steps, validated by applying the service to an existing dataset of unique student steps when solving quadratic equations

Result: final answer evaluation can diagnose 29,4% of steps. Diagnoses align with teacher diagnoses in 97% of the cases.

Conclusion: final answer evaluation can diagnose 29,4% of steps that could not be diagnosed by a buggy rule service, and the diagnoses align with teacher diagnoses in 97% of the cases.

Abstract: Many intelligent tutoring systems can support a student in solving a stepwise
task. When a student combines several steps in one step, the number of possible
paths connecting consecutive inputs may be very large. This combinatorial
explosion makes error diagnosis hard. Using a final answer to diagnose a
combination of steps can mitigate the combinatorial explosion, because there
are generally fewer possible (erroneous) final answers than (erroneous)
solution paths. An intermediate input for a task can be diagnosed by
automatically completing it according to the task solution strategy and
diagnosing this solution. This study explores the potential of automated error
diagnosis based on a final answer. We investigate the design of a service that
provides a buggy rule diagnosis when a student combines several steps. To
validate the approach, we apply the service to an existing dataset (n=1939) of
unique student steps when solving quadratic equations, which could not be
diagnosed by a buggy rule service that tries to connect consecutive inputs with
a single rule. Results show that final answer evaluation can diagnose 29,4% of
these steps. Moreover, a comparison of the generated diagnoses with teacher
diagnoses on a subset (n=115) shows that the diagnoses align in 97% of the
cases. These results can be considered a basis for further exploration of the
approach.

</details>


### [54] [Combining model tracing and constraint-based modeling for multistep strategy diagnoses](https://arxiv.org/abs/2507.13652)
*Gerben van der Hoek,Johan Jeuring,Rogier Bos*

Main category: cs.AI

TL;DR: merges model tracing and constraint-based modeling for multistep strategy diagnoses


<details>
  <summary>Details</summary>
Motivation: Model tracing and constraint-based modeling are two approaches to diagnose student input in stepwise tasks. Model tracing supports identifying consecutive problem-solving steps taken by a student, whereas constraint-based modeling supports student input diagnosis even when several steps are combined into one step.

Method: an approach that merges model tracing and constraint-based modeling paradigms by defining constraints as properties that a student input has in common with a step of a strategy

Result: system diagnosis aligned with the teacher coding in all of the 140 student steps.

Conclusion: The system diagnosis aligned with the teacher coding in all of the 140 student steps.

Abstract: Model tracing and constraint-based modeling are two approaches to diagnose
student input in stepwise tasks. Model tracing supports identifying consecutive
problem-solving steps taken by a student, whereas constraint-based modeling
supports student input diagnosis even when several steps are combined into one
step. We propose an approach that merges both paradigms. By defining
constraints as properties that a student input has in common with a step of a
strategy, it is possible to provide a diagnosis when a student deviates from a
strategy even when the student combines several steps. In this study we explore
the design of a system for multistep strategy diagnoses, and evaluate these
diagnoses. As a proof of concept, we generate diagnoses for an existing dataset
containing steps students take when solving quadratic equations (n=2136). To
compare with human diagnoses, two teachers coded a random sample of deviations
(n=70) and applications of the strategy (n=70). Results show that that the
system diagnosis aligned with the teacher coding in all of the 140 student
steps.

</details>


### [55] [DailyLLM: Context-Aware Activity Log Generation Using Multi-Modal Sensors and LLMs](https://arxiv.org/abs/2507.13737)
*Ye Tian,Xiaoyuan Ren,Zihao Wang,Onat Gungor,Xiaofan Yu,Tajana Rosing*

Main category: cs.AI

TL;DR: DailyLLM is a new activity log generation system that uses a lightweight LLM to improve accuracy, efficiency, and semantic richness by integrating contextual activity information from smartphone and smartwatch sensors.


<details>
  <summary>Details</summary>
Motivation: Existing activity log generation methods have limitations in accuracy, efficiency, and semantic richness.

Method: A lightweight LLM-based framework integrating structured prompting with efficient feature extraction to understand high-level activity, using contextual activity information across location, motion, environment, and physiology from smartphone and smartwatch sensors.

Result: DailyLLM achieves a 17% improvement in log generation BERTScore precision compared to a 70B-parameter baseline, while delivering nearly 10x faster inference speed, using only a 1.5B-parameter LLM model.

Conclusion: DailyLLM outperforms existing log generation methods in accuracy and efficiency, achieving a 17% improvement in BERTScore precision with a much smaller model and faster inference speed.

Abstract: Rich and context-aware activity logs facilitate user behavior analysis and
health monitoring, making them a key research focus in ubiquitous computing.
The remarkable semantic understanding and generation capabilities of Large
Language Models (LLMs) have recently created new opportunities for activity log
generation. However, existing methods continue to exhibit notable limitations
in terms of accuracy, efficiency, and semantic richness. To address these
challenges, we propose DailyLLM. To the best of our knowledge, this is the
first log generation and summarization system that comprehensively integrates
contextual activity information across four dimensions: location, motion,
environment, and physiology, using only sensors commonly available on
smartphones and smartwatches. To achieve this, DailyLLM introduces a
lightweight LLM-based framework that integrates structured prompting with
efficient feature extraction to enable high-level activity understanding.
Extensive experiments demonstrate that DailyLLM outperforms state-of-the-art
(SOTA) log generation methods and can be efficiently deployed on personal
computers and Raspberry Pi. Utilizing only a 1.5B-parameter LLM model, DailyLLM
achieves a 17% improvement in log generation BERTScore precision compared to
the 70B-parameter SOTA baseline, while delivering nearly 10x faster inference
speed.

</details>


### [56] [OntView: What you See is What you Meant](https://arxiv.org/abs/2507.13759)
*Carlos Bobed,Carlota Quintana,Eduardo Mena,Jorge Bobed,Fernando Bobillo*

Main category: cs.AI

TL;DR: OntView is an open-source ontology viewer that provides an intuitive visual representation of ontology concepts, visualizes GCIs, and offers simplified views to avoid information overload.


<details>
  <summary>Details</summary>
Motivation: The lack of tools that provide effective visualization is a significant challenge in ontology management, limiting users' ability to comprehend dependencies and properties within large ontological frameworks.

Method: OntView uses a DL reasoner and offers different ways to show a simplified view of the ontology by creating ontology summaries, focusing on TBox elements between two given classes, and allowing users to hide/show different branches dynamically.

Result: OntView visualizes General Concept Inclusions (GCI), a feature absent in existing visualization tools. It also offers different ways to show a simplified view of the ontology to avoid information overload.

Conclusion: OntView, an open-source ontology viewer, provides an intuitive visual representation of ontology concepts and their formal definitions through a user-friendly interface, visualizing General Concept Inclusions (GCI) and offering simplified views to avoid information overload.

Abstract: In the field of knowledge management and computer science, ontologies provide
a structured framework for modeling domain-specific knowledge by defining
concepts and their relationships. However, the lack of tools that provide
effective visualization is still a significant challenge. While numerous
ontology editors and viewers exist, most of them fail to graphically represent
ontology structures in a meaningful and non-overwhelming way, limiting users'
ability to comprehend dependencies and properties within large ontological
frameworks.
  In this paper, we present OntView, an ontology viewer that is designed to
provide users with an intuitive visual representation of ontology concepts and
their formal definitions through a user-friendly interface. Building on the use
of a DL reasoner, OntView follows a "What you see is what you meant" paradigm,
showing the actual inferred knowledge. One key aspect for this is its ability
to visualize General Concept Inclusions (GCI), a feature absent in existing
visualization tools. Moreover, to avoid a possible information overload,
OntView also offers different ways to show a simplified view of the ontology
by: 1) creating ontology summaries by assessing the importance of the concepts
(according to different available algorithms), 2) focusing the visualization on
the existing TBox elements between two given classes and 3) allowing to
hide/show different branches in a dynamic way without losing the semantics.
OntView has been released with an open-source license for the whole community.

</details>


### [57] [From Extraction to Synthesis: Entangled Heuristics for Agent-Augmented Strategic Reasoning](https://arxiv.org/abs/2507.13768)
*Renato Ghisellini,Remo Pareschi,Marco Pedroni,Giovanni Battista Raggi*

Main category: cs.AI

TL;DR: A hybrid architecture for agent-augmented strategic reasoning is presented, combining heuristic extraction, semantic activation, and compositional synthesis. The framework is demonstrated via a Meta vs. FTC case study.


<details>
  <summary>Details</summary>
Motivation: To present a hybrid architecture for agent-augmented strategic reasoning, combining heuristic extraction, semantic activation, and compositional synthesis. Drawing on sources ranging from classical military theory to contemporary corporate strategy

Method: The model activates and composes multiple heuristics through a process of semantic interdependence inspired by research in quantum cognition. Unlike traditional decision engines that select the best rule, our system fuses conflicting heuristics into coherent and context-sensitive narratives, guided by semantic interaction modeling and rhetorical framing.

Result: The system fuses conflicting heuristics into coherent and context-sensitive narratives

Conclusion: The framework is demonstrated via a Meta vs. FTC case study, with preliminary validation through semantic metrics. Limitations and extensions (e.g., dynamic interference tuning) are discussed.

Abstract: We present a hybrid architecture for agent-augmented strategic reasoning,
combining heuristic extraction, semantic activation, and compositional
synthesis. Drawing on sources ranging from classical military theory to
contemporary corporate strategy, our model activates and composes multiple
heuristics through a process of semantic interdependence inspired by research
in quantum cognition. Unlike traditional decision engines that select the best
rule, our system fuses conflicting heuristics into coherent and
context-sensitive narratives, guided by semantic interaction modeling and
rhetorical framing. We demonstrate the framework via a Meta vs. FTC case study,
with preliminary validation through semantic metrics. Limitations and
extensions (e.g., dynamic interference tuning) are discussed.

</details>


### [58] [When Speed meets Accuracy: an Efficient and Effective Graph Model for Temporal Link Prediction](https://arxiv.org/abs/2507.13825)
*Haoyang Li,Yuming Xu,Yiming Li,Hanmo Liu,Darian Li,Chen Jason Zhang,Lei Chen,Qing Li*

Main category: cs.AI

TL;DR: EAGLE是一种轻量级框架，它集成了短期时间新近度和长期全局结构模式，在预测动态图中的时间链接方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有时间图神经网络 (T-GNN) 通过利用复杂的架构来建模时间和结构依赖性，取得了显著的成功，但由于计算开销高，它们通常面临可扩展性和效率挑战。

Method: EAGLE，一个轻量级框架，集成了短期时间新近度和长期全局结构模式。EAGLE 包含一个时间感知模块，该模块聚合来自节点最近邻居的信息以反映其直接偏好，以及一个结构感知模块，该模块利用时间个性化 PageRank 来捕获全局重要节点的影响。

Result: EAGLE 在七个真实世界的时间图上进行了广泛的实验，表明 EAGLE 在有效性和效率方面始终优于最先进的 T-GNN。

Conclusion: EAGLE在效果和效率上始终优于最先进的 T-GNN，与有效的基于 Transformer 的 T-GNN 相比，速度提高了 50 倍以上。

Abstract: Temporal link prediction in dynamic graphs is a critical task with
applications in diverse domains such as social networks, recommendation
systems, and e-commerce platforms. While existing Temporal Graph Neural
Networks (T-GNNs) have achieved notable success by leveraging complex
architectures to model temporal and structural dependencies, they often suffer
from scalability and efficiency challenges due to high computational overhead.
In this paper, we propose EAGLE, a lightweight framework that integrates
short-term temporal recency and long-term global structural patterns. EAGLE
consists of a time-aware module that aggregates information from a node's most
recent neighbors to reflect its immediate preferences, and a structure-aware
module that leverages temporal personalized PageRank to capture the influence
of globally important nodes. To balance these attributes, EAGLE employs an
adaptive weighting mechanism to dynamically adjust their contributions based on
data characteristics. Also, EAGLE eliminates the need for complex multi-hop
message passing or memory-intensive mechanisms, enabling significant
improvements in efficiency. Extensive experiments on seven real-world temporal
graphs demonstrate that EAGLE consistently achieves superior performance
against state-of-the-art T-GNNs in both effectiveness and efficiency,
delivering more than a 50x speedup over effective transformer-based T-GNNs.

</details>


### [59] [Causal Knowledge Transfer for Multi-Agent Reinforcement Learning in Dynamic Environments](https://arxiv.org/abs/2507.13846)
*Kathrin Korte,Christian Medeiros Adriano,Sona Ghahremani,Holger Giese*

Main category: cs.AI

TL;DR: Introduces a causal knowledge transfer framework that enables RL agents to learn and share compact causal representations of paths within a non-stationary environment. Agents with heterogeneous goals were able to bridge about half of the gap between random exploration and a fully retrained policy when adapting to new environments


<details>
  <summary>Details</summary>
Motivation: transferring knowledge across agents remains challenging in non-stationary environments with changing goals. Traditional knowledge transfer methods in MARL struggle to generalize, and agents often require costly retraining to adapt.

Method: a causal knowledge transfer framework that enables RL agents to learn and share compact causal representations of paths within a non-stationary environment. We model each collision as a causal intervention instantiated as a sequence of recovery actions (a macro) whose effect corresponds to a causal knowledge of how to circumvent the obstacle while increasing the chances of achieving the agent's goal (maximizing cumulative reward). This recovery action macro is transferred online from a second agent and is applied in a zero-shot fashion

Result: agents with heterogeneous goals were able to bridge about half of the gap between random exploration and a fully retrained policy when adapting to new environments, and the impact of causal knowledge transfer depends on the interplay between environment complexity and agents' heterogeneous goals.

Conclusion: agents with heterogeneous goals were able to bridge about half of the gap between random exploration and a fully retrained policy when adapting to new environments, and the impact of causal knowledge transfer depends on the interplay between environment complexity and agents' heterogeneous goals.

Abstract: [Context] Multi-agent reinforcement learning (MARL) has achieved notable
success in environments where agents must learn coordinated behaviors. However,
transferring knowledge across agents remains challenging in non-stationary
environments with changing goals. [Problem] Traditional knowledge transfer
methods in MARL struggle to generalize, and agents often require costly
retraining to adapt. [Approach] This paper introduces a causal knowledge
transfer framework that enables RL agents to learn and share compact causal
representations of paths within a non-stationary environment. As the
environment changes (new obstacles), agents' collisions require adaptive
recovery strategies. We model each collision as a causal intervention
instantiated as a sequence of recovery actions (a macro) whose effect
corresponds to a causal knowledge of how to circumvent the obstacle while
increasing the chances of achieving the agent's goal (maximizing cumulative
reward). This recovery action macro is transferred online from a second agent
and is applied in a zero-shot fashion, i.e., without retraining, just by
querying a lookup model with local context information (collisions). [Results]
Our findings reveal two key insights: (1) agents with heterogeneous goals were
able to bridge about half of the gap between random exploration and a fully
retrained policy when adapting to new environments, and (2) the impact of
causal knowledge transfer depends on the interplay between environment
complexity and agents' heterogeneous goals.

</details>


### [60] [Large Language Models as Innovators: A Framework to Leverage Latent Space Exploration for Novelty Discovery](https://arxiv.org/abs/2507.13874)
*Mateusz Bystroński,Mikołaj Hołysz,Grzegorz Piotrowski,Nitesh V. Chawla,Tomasz Kajdanowicz*

Main category: cs.AI

TL;DR: This paper proposes a model-agnostic latent-space ideation framework that enables controlled, scalable creativity by navigating the continuous embedding space of ideas. It requires no handcrafted rules and adapts easily to different domains, input formats, and creative tasks.


<details>
  <summary>Details</summary>
Motivation: Innovative idea generation remains a core challenge in AI, as large language models (LLMs) often struggle to produce outputs that are both novel and relevant. Despite their fluency, LLMs tend to replicate patterns seen during training, limiting their ability to diverge creatively without extensive prompt engineering. Prior work has addressed this through domain-specific heuristics and structured prompting pipelines, but such solutions are brittle and difficult to generalize.

Method: propose a model-agnostic latent-space ideation framework that enables controlled, scalable creativity by navigating the continuous embedding space of ideas

Result: This paper introduces an early-stage prototype of our method, outlining the conceptual framework and preliminary results highlighting its potential as a general-purpose co-ideator for human-AI collaboration.

Conclusion: This paper introduces an early-stage prototype of a model-agnostic latent-space ideation framework, outlining the conceptual framework and preliminary results highlighting its potential as a general-purpose co-ideator for human-AI collaboration.

Abstract: Innovative idea generation remains a core challenge in AI, as large language
models (LLMs) often struggle to produce outputs that are both novel and
relevant. Despite their fluency, LLMs tend to replicate patterns seen during
training, limiting their ability to diverge creatively without extensive prompt
engineering. Prior work has addressed this through domain-specific heuristics
and structured prompting pipelines, but such solutions are brittle and
difficult to generalize. In this paper, we propose a model-agnostic
latent-space ideation framework that enables controlled, scalable creativity by
navigating the continuous embedding space of ideas. Unlike prior methods, our
framework requires no handcrafted rules and adapts easily to different domains,
input formats, and creative tasks. This paper introduces an early-stage
prototype of our method, outlining the conceptual framework and preliminary
results highlighting its potential as a general-purpose co-ideator for human-AI
collaboration.

</details>


### [61] [Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment](https://arxiv.org/abs/2507.14107)
*Viraj Nishesh Darji,Callie C. Liao,Duoduo Liao*

Main category: cs.AI

TL;DR: 本研究探讨了大型语言模型(llm)在解释桥梁非破坏性评估(nde)数据方面的能力，发现它们可以提高效率和准确性，从而加快桥梁维护决策。


<details>
  <summary>Details</summary>
Motivation: 桥梁维护和安全对交通部门至关重要，无损评估(nde)技术对评估结构完整性至关重要。然而，解释nde数据可能非常耗时，并且需要专业知识，从而可能延误决策。

Method: 利用大型语言模型(llm)对通过评估桥梁状况的技术获得的五个不同的非破坏性评估等高线图进行解释。

Result: 九个模型中有四个模型提供了更好的图像描述，有效地涵盖了与桥梁状况相关的广泛主题。LLMs ChatGPT-4和Claude 3.5 Sonnet生成了更有效的摘要。

Conclusion: LLMs具有显著提高效率和准确性的潜力，通过并行图像描述和总结，实现更快的决策，并加强基础设施管理和安全评估。

Abstract: Bridge maintenance and safety are essential for transportation authorities,
and Non-Destructive Evaluation (NDE) techniques are critical to assessing
structural integrity. However, interpreting NDE data can be time-consuming and
requires expertise, potentially delaying decision-making. Recent advancements
in Large Language Models (LLMs) offer new ways to automate and improve this
analysis. This pilot study introduces a holistic assessment of LLM capabilities
for interpreting NDE contour maps and demonstrates the effectiveness of LLMs in
providing detailed bridge condition analyses. It establishes a framework for
integrating LLMs into bridge inspection workflows, indicating that LLM-assisted
analysis can enhance efficiency without compromising accuracy. In this study,
several LLMs are explored with prompts specifically designed to enhance the
quality of image descriptions, which are applied to interpret five different
NDE contour maps obtained through technologies for assessing bridge conditions.
Each LLM model is evaluated based on its ability to produce detailed
descriptions, identify defects, provide actionable recommendations, and
demonstrate overall accuracy. The research indicates that four of the nine
models provide better image descriptions, effectively covering a wide range of
topics related to the bridge's condition. The outputs from these four models
are summarized using five different LLMs to form a comprehensive overview of
the bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more
effective summaries. The findings suggest that LLMs have the potential to
significantly improve efficiency and accuracy. This pilot study presents an
innovative approach that leverages LLMs for image captioning in parallel and
summarization, enabling faster decision-making in bridge maintenance and
enhancing infrastructure management and safety assessments.

</details>


### [62] [Cross-modal Causal Intervention for Alzheimer's Disease Prediction](https://arxiv.org/abs/2507.13956)
*Yutao Jin,Haowen Xiao,Jielei Chu,Fengmao Lv,Yuxiao Li,Tianrui Li*

Main category: cs.AI

TL;DR: 本研究提出了一种新颖的可视语言因果干预框架 ADPC，用于诊断辅助，通过因果干预隐式消除混淆因素，在区分 CN/MCI/AD 病例方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决由多模态数据的选择偏差和变量之间复杂关系引起的混淆因素，这些因素导致神经内科在诊断 AD 方面仍然面临重大挑战。

Method: Alzheimer's Disease Prediction with Cross-modal Causal Intervention (ADPC)

Result: 该方法在区分 CN/MCI/AD 病例方面表现出色，在大多数评估指标上实现了最先进 (SOTA) 的指标。

Conclusion: 该研究展示了将因果推理与多模态学习相结合以进行神经系统疾病诊断的潜力。

Abstract: Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's
Disease (AD), where early identification and intervention can effectively slow
the progression to dementia. However, diagnosing AD remains a significant
challenge in neurology due to the confounders caused mainly by the selection
bias of multimodal data and the complex relationships between variables. To
address these issues, we propose a novel visual-language causal intervention
framework named Alzheimer's Disease Prediction with Cross-modal Causal
Intervention (ADPC) for diagnostic assistance. Our ADPC employs large language
model (LLM) to summarize clinical data under strict templates, maintaining
structured text outputs even with incomplete or unevenly distributed datasets.
The ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)
images and textual data generated by LLM to classify participants into
Cognitively Normal (CN), MCI, and AD categories. Because of the presence of
confounders, such as neuroimaging artifacts and age-related biomarkers,
non-causal models are likely to capture spurious input-output correlations,
generating less reliable results. Our framework implicitly eliminates
confounders through causal intervention. Experimental results demonstrate the
outstanding performance of our method in distinguishing CN/MCI/AD cases,
achieving state-of-the-art (SOTA) metrics across most evaluation metrics. The
study showcases the potential of integrating causal reasoning with multi-modal
learning for neurological disease diagnosis.

</details>


### [63] [Towards Constraint Temporal Answer Set Programming](https://arxiv.org/abs/2507.13958)
*Pedro Cabalar,Martín Diéguez,François Olivier,Torsten Schaub,Igor Stéphan*

Main category: cs.AI

TL;DR: Introduces a new ASP approach for temporal reasoning with constraints, combining linear-time logic and constraint logic for handling complex dynamic systems.


<details>
  <summary>Details</summary>
Motivation: Reasoning about dynamic systems with a fine-grained temporal and numeric resolution presents significant challenges for logic-based approaches like Answer Set Programming (ASP).

Method: a novel temporal and constraint-based extension of the logic of Here-and-There and its nonmonotonic equilibrium extension

Result: This expressive system is achieved by a synergistic combination of two foundational ASP extensions: the linear-time logic of Here-and-There, providing robust nonmonotonic temporal reasoning capabilities, and the logic of Here-and-There with constraints, enabling the direct integration and manipulation of numeric constraints, among others.

Conclusion: This work establishes the foundational logical framework for tackling complex dynamic systems with high resolution within the ASP paradigm.

Abstract: Reasoning about dynamic systems with a fine-grained temporal and numeric
resolution presents significant challenges for logic-based approaches like
Answer Set Programming (ASP). To address this, we introduce and elaborate upon
a novel temporal and constraint-based extension of the logic of Here-and-There
and its nonmonotonic equilibrium extension, representing, to the best of our
knowledge, the first approach to nonmonotonic temporal reasoning with
constraints specifically tailored for ASP. This expressive system is achieved
by a synergistic combination of two foundational ASP extensions: the
linear-time logic of Here-and-There, providing robust nonmonotonic temporal
reasoning capabilities, and the logic of Here-and-There with constraints,
enabling the direct integration and manipulation of numeric constraints, among
others. This work establishes the foundational logical framework for tackling
complex dynamic systems with high resolution within the ASP paradigm.

</details>


### [64] [KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models](https://arxiv.org/abs/2507.14032)
*Lam Nguyen,Erika Barcelos,Roger French,Yinghui Wu*

Main category: cs.AI

TL;DR: KROMA, a novel OM framework, harnesses Large Language Models (LLMs) within a Retrieval-Augmented Generation (RAG) pipeline to dynamically enrich the semantic context of OM tasks. It optimizes performance and efficiency through bisimilarity-based concept matching and a lightweight ontology refinement step.


<details>
  <summary>Details</summary>
Motivation: Existing Ontology Matching (OM) systems often rely on handcrafted rules or specialized models with limited adaptability.

Method: KROMA integrates a bisimilarity-based concept matching and a lightweight ontology refinement step, which prune candidate concepts and substantially reduce the communication overhead from invoking LLMs.

Result: KROMA outperforms both classic OM systems and cutting-edge LLM-based approaches while keeping communication overhead comparable.

Conclusion: Integrating knowledge retrieval with context-augmented LLMs significantly enhances ontology matching, outperforming both classic OM systems and cutting-edge LLM-based approaches while keeping communication overhead comparable. The study highlights the feasibility and benefit of the proposed optimization techniques (targeted knowledge retrieval, prompt enrichment, and ontology refinement) for ontology matching at scale.

Abstract: Ontology Matching (OM) is a cornerstone task of semantic interoperability,
yet existing systems often rely on handcrafted rules or specialized models with
limited adaptability. We present KROMA, a novel OM framework that harnesses
Large Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)
pipeline to dynamically enrich the semantic context of OM tasks with
structural, lexical, and definitional knowledge. To optimize both performance
and efficiency, KROMA integrates a bisimilarity-based concept matching and a
lightweight ontology refinement step, which prune candidate concepts and
substantially reduce the communication overhead from invoking LLMs. Through
experiments on multiple benchmark datasets, we show that integrating knowledge
retrieval with context-augmented LLMs significantly enhances ontology matching,
outperforming both classic OM systems and cutting-edge LLM-based approaches
while keeping communication overhead comparable. Our study highlights the
feasibility and benefit of the proposed optimization techniques (targeted
knowledge retrieval, prompt enrichment, and ontology refinement) for ontology
matching at scale.

</details>


### [65] [Glucose-ML: A collection of longitudinal diabetes datasets for development of robust AI solutions](https://arxiv.org/abs/2507.14077)
*Temiloluwa Prioleau,Baiying Lu,Yanjun Cui*

Main category: cs.AI

TL;DR: 提出了Glucose-ML糖尿病数据集，包含10个数据集，并进行了短期血糖预测的基准测试。


<details>
  <summary>Details</summary>
Motivation: 获取大型高质量数据集正在形成壁垒，阻碍了稳健的AI解决方案的开发。为了加速开发透明、可重复和稳健的AI解决方案。

Method: 提出了Glucose-ML，一个包含10个公开可用的糖尿病数据集的集合，这些数据集是在过去7年内发布的（即2018-2025年）。

Result: Glucose-ML集合包含超过300,000天的连续血糖监测（CGM）数据，总共从来自4个国家的2500多人收集了3800万个葡萄糖样本。我们展示了相同的算法在用不同的数据集开发/评估时，可以有显著不同的预测结果。

Conclusion: 同一算法在不同数据集上开发/评估时，可能产生显著不同的预测结果。本研究的发现可为在糖尿病或更广泛的健康领域开发稳健的AI解决方案提供参考建议。

Abstract: Artificial intelligence (AI) algorithms are a critical part of
state-of-the-art digital health technology for diabetes management. Yet, access
to large high-quality datasets is creating barriers that impede development of
robust AI solutions. To accelerate development of transparent, reproducible,
and robust AI solutions, we present Glucose-ML, a collection of 10 publicly
available diabetes datasets, released within the last 7 years (i.e., 2018 -
2025). The Glucose-ML collection comprises over 300,000 days of continuous
glucose monitor (CGM) data with a total of 38 million glucose samples collected
from 2500+ people across 4 countries. Participants include persons living with
type 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support
researchers and innovators with using this rich collection of diabetes
datasets, we present a comparative analysis to guide algorithm developers with
data selection. Additionally, we conduct a case study for the task of blood
glucose prediction - one of the most common AI tasks within the field. Through
this case study, we provide a benchmark for short-term blood glucose prediction
across all 10 publicly available diabetes datasets within the Glucose-ML
collection. We show that the same algorithm can have significantly different
prediction results when developed/evaluated with different datasets. Findings
from this study are then used to inform recommendations for developing robust
AI solutions within the diabetes or broader health domain. We provide direct
links to each longitudinal diabetes dataset in the Glucose-ML collection and
openly provide our code.

</details>


### [66] [Generative AI-Driven High-Fidelity Human Motion Simulation](https://arxiv.org/abs/2507.14097)
*Hari Iyer,Neel Macwan,Atharva Jitendra Hude,Heejin Jeong,Shenghan Guo*

Main category: cs.AI

TL;DR: This study introduces Generative-AI-Enabled HMS (G-AI-HMS) to enhance simulation quality for physical tasks.


<details>
  <summary>Details</summary>
Motivation: existing methods often suffer from low motion fidelity.

Method: integrates text-to-text and text-to-motion models and validating AI-enhanced motions against real human movements using computer vision

Result: AI-enhanced motions showed lower error than human created descriptions in most scenarios

Conclusion: AI-enhanced motions significantly reduce joint error and temporal misalignment while retaining comparable posture accuracy.

Abstract: Human motion simulation (HMS) supports cost-effective evaluation of worker
behavior, safety, and productivity in industrial tasks. However, existing
methods often suffer from low motion fidelity. This study introduces
Generative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and
text-to-motion models to enhance simulation quality for physical tasks.
G-AI-HMS tackles two key challenges: (1) translating task descriptions into
motion-aware language using Large Language Models aligned with MotionGPT's
training vocabulary, and (2) validating AI-enhanced motions against real human
movements using computer vision. Posture estimation algorithms are applied to
real-time videos to extract joint landmarks, and motion similarity metrics are
used to compare them with AI-enhanced sequences. In a case study involving
eight tasks, the AI-enhanced motions showed lower error than human created
descriptions in most scenarios, performing better in six tasks based on spatial
accuracy, four tasks based on alignment after pose normalization, and seven
tasks based on overall temporal similarity. Statistical analysis showed that
AI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and
temporal misalignment while retaining comparable posture accuracy.

</details>


### [67] [CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning](https://arxiv.org/abs/2507.14111)
*Xiaoya Li,Xiaofei Sun,Albert Wang,Jiwei Li,Chris Shum*

Main category: cs.AI

TL;DR: Introduces CUDA-L1, a reinforcement learning framework for CUDA optimization, achieving significant speedups and portability across GPUs, demonstrating the potential of RL in automated CUDA optimization.


<details>
  <summary>Details</summary>
Motivation: The exponential growth in demand for GPU computing resources, driven by the rapid advancement of Large Language Models, has created an urgent need for automated CUDA optimization strategies. While recent advances in LLMs show promise for code generation, current SOTA models achieve low success rates in improving CUDA speed.

Method: an automated reinforcement learning framework for CUDA optimization

Result: CUDA-L1 achieves performance improvements on the CUDA optimization task: trained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250 CUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the model also demonstrates excellent portability across GPU architectures, achieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40, x14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.

Conclusion: Reinforcement learning can transform an initially poor-performing LLM into an effective CUDA optimizer through speedup-based reward signals alone, without human expertise or domain knowledge. The trained RL model extend the acquired reasoning abilities to new kernels. This paradigm opens possibilities for automated optimization of CUDA operations, and holds promise to substantially promote GPU efficiency and alleviate the rising pressure on GPU computing resources.

Abstract: The exponential growth in demand for GPU computing resources, driven by the
rapid advancement of Large Language Models, has created an urgent need for
automated CUDA optimization strategies. While recent advances in LLMs show
promise for code generation, current SOTA models (e.g. R1, o1) achieve low
success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an
automated reinforcement learning framework for CUDA optimization.
  CUDA-L1 achieves performance improvements on the CUDA optimization task:
trained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250
CUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the
model also demonstrates excellent portability across GPU architectures,
achieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,
x14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.
Beyond these benchmark results, CUDA-L1 demonstrates several remarkable
properties: 1) Discovers a variety of CUDA optimization techniques and learns
to combine them strategically to achieve optimal performance; 2) Uncovers
fundamental principles of CUDA optimization; 3) Identifies non-obvious
performance bottlenecks and rejects seemingly beneficial optimizations that
harm performance.
  The capabilities of CUDA-L1 demonstrate that reinforcement learning can
transform an initially poor-performing LLM into an effective CUDA optimizer
through speedup-based reward signals alone, without human expertise or domain
knowledge. More importantly, the trained RL model extend the acquired reasoning
abilities to new kernels. This paradigm opens possibilities for automated
optimization of CUDA operations, and holds promise to substantially promote GPU
efficiency and alleviate the rising pressure on GPU computing resources.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [68] [CogniQ-H: A Soft Hierarchical Reinforcement Learning Paradigm for Automated Data Preparation](https://arxiv.org/abs/2507.13710)
*Jing Chang,Chang Liu,Jinbin Huang,Rui Mao,Jianbin Qin*

Main category: cs.DB

TL;DR: CogniQ-H：首个用于稳健的端到端自动数据准备的软分层范例。


<details>
  <summary>Details</summary>
Motivation: 数据准备是机器学习生命周期中一个基础但具有挑战性的组成部分，其特点是潜在算子序列的巨大组合搜索空间。强化学习(RL)提供了一个有希望的方向，但现有的方法效率低下，因为它们未能捕捉到问题的结构化、分层性质。

Method: CogniQ-H将动作选择构建为一个贝叶斯推断问题。由大型语言模型(LLM)生成的高级战略先验概率性地指导探索。该先验与来自监督学习排序(LTR)模型的细粒度算子质量评分以及来自agent自身Q函数的长期价值估计协同组合。

Result: CogniQ-H在管道质量方面提高了13.9%，收敛速度提高了2.8倍。

Conclusion: CogniQ-H在多个领域18个不同的数据集上进行了广泛的实验，结果表明，与最先进的基于RL的方法相比，CogniQ-H在管道质量方面提高了13.9%，收敛速度提高了2.8倍。

Abstract: Data preparation is a foundational yet notoriously challenging component of
the machine learning lifecycle, characterized by a vast combinatorial search
space of potential operator sequences. While reinforcement learning (RL) offers
a promising direction, existing approaches are inefficient as they fail to
capture the structured, hierarchical nature of the problem. We argue that
Hierarchical Reinforcement Learning (HRL), a paradigm that has been successful
in other domains, provides a conceptually ideal yet previously unexplored
framework for this task. However, a naive HRL implementation with a `hard
hierarchy' is prone to suboptimal, irreversible decisions. To address this, we
introduce CogniQ-H, the first framework to implement a soft hierarchical
paradigm for robust, end-to-end automated data preparation. CogniQ-H formulates
action selection as a Bayesian inference problem. A high-level strategic prior,
generated by a Large Language Model (LLM), guides exploration
probabilistically. This prior is synergistically combined with a fine-grained
operator quality score from a supervised Learning-to-Rank (LTR) model and a
long-term value estimate from the agent's own Q-function. This hybrid
architecture allows CogniQ-H to balance strategic guidance with adaptive,
evidence-based decision-making. Through extensive experiments on 18 diverse
datasets spanning multiple domains, we demonstrate that CogniQ-H achieves up to
13.9\% improvement in pipeline quality and 2.8$\times$ faster convergence
compared to state-of-the-art RL-based methods.

</details>


### [69] [LLaPipe: LLM-Guided Reinforcement Learning for Automated Data Preparation Pipeline Construction](https://arxiv.org/abs/2507.13712)
*Jing Chang,Chang Liu,Jinbin Huang,Rui Mao,Jianbin Qin*

Main category: cs.DB

TL;DR: LLaPipe uses LLMs to guide exploration in automated data preparation, improving pipeline quality and convergence speed compared to RL-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing reinforcement learning (RL) based approaches suffer from inefficient exploration in the vast space of possible preprocessing pipelines.

Method: LLaPipe, a novel framework that addresses this exploration bottleneck by integrating Large Language Models (LLMs) as intelligent policy advisors. It introduces three key innovations: (1) an LLM Policy Advisor, (2) an Experience Distillation mechanism, and (3) an Adaptive Advisor Triggering strategy.

Result: LLaPipe achieves up to 22.4% improvement in pipeline quality and 2.3x faster convergence compared to state-of-the-art RL-based methods.

Conclusion: LLaPipe achieves up to 22.4% improvement in pipeline quality and 2.3x faster convergence compared to state-of-the-art RL-based methods, while maintaining computational efficiency through selective LLM usage (averaging only 19.0% of total exploration steps).

Abstract: Automated data preparation is crucial for democratizing machine learning, yet
existing reinforcement learning (RL) based approaches suffer from inefficient
exploration in the vast space of possible preprocessing pipelines. We present
LLaPipe, a novel framework that addresses this exploration bottleneck by
integrating Large Language Models (LLMs) as intelligent policy advisors. Unlike
traditional methods that rely solely on statistical features and blind
trial-and-error, LLaPipe leverages the semantic understanding capabilities of
LLMs to provide contextually relevant exploration guidance. Our framework
introduces three key innovations: (1) an LLM Policy Advisor that analyzes
dataset semantics and pipeline history to suggest promising preprocessing
operations, (2) an Experience Distillation mechanism that mines successful
patterns from past pipelines and transfers this knowledge to guide future
exploration, and (3) an Adaptive Advisor Triggering strategy
(Advisor\textsuperscript{+}) that dynamically determines when LLM intervention
is most beneficial, balancing exploration effectiveness with computational
cost. Through extensive experiments on 18 diverse datasets spanning multiple
domains, we demonstrate that LLaPipe achieves up to 22.4\% improvement in
pipeline quality and 2.3$\times$ faster convergence compared to
state-of-the-art RL-based methods, while maintaining computational efficiency
through selective LLM usage (averaging only 19.0\% of total exploration steps).

</details>


### [70] [Efficient and Scalable Self-Healing Databases Using Meta-Learning and Dependency-Driven Recovery](https://arxiv.org/abs/2507.13757)
*Joydeep Chandra,Prabal Manhas*

Main category: cs.DB

TL;DR: A novel self-healing framework for databases using meta-learning and reinforcement learning techniques is proposed, which enables anomaly detection and corrective actions that adapted swiftly to evolving database conditions.


<details>
  <summary>Details</summary>
Motivation: address the challenges of real-time adaptability and minimal retraining in dynamic workload environments

Method: integrated Model-Agnostic Meta-Learning (MAML) with reinforcement learning, Multi-objective optimization, Graph Neural Networks (GNNs), synthetic task augmentation and self-supervised learning, explainable AI techniques, Federated meta-learning

Result: significant improvements in adaptability, efficiency, and reliability

Conclusion: The framework demonstrates significant improvements in adaptability, efficiency, and reliability, contributing to advancements in database management and self-healing systems.

Abstract: This study explored the development of a novel self-healing framework for
databases using meta-learning and reinforcement learning techniques. The
primary objective was to address the challenges of real-time adaptability and
minimal retraining in dynamic workload environments. The proposed approach
integrated Model-Agnostic Meta-Learning (MAML) with reinforcement learning to
enable anomaly detection and corrective actions that adapted swiftly to
evolving database conditions. Multi-objective optimization was employed to
balance performance, resource utilization, and cost efficiency during the
healing process. Graph Neural Networks (GNNs) were incorporated to model
interdependencies within database components, ensuring holistic recovery
strategies. Data efficiency was enhanced through synthetic task augmentation
and self-supervised learning, enabling effective training in sparse data
regimes. To promote trust and transparency, explainable AI techniques were
integrated to provide interpretable insights into anomaly detection and healing
actions. Federated meta-learning further enabled privacy-preserving
adaptability in distributed database environments. The framework demonstrated
significant improvements in adaptability, efficiency, and reliability,
contributing to advancements in database management and self-healing systems.

</details>


### [71] [Towards Next Generation Data Engineering Pipelines](https://arxiv.org/abs/2507.13892)
*Kevin M. Kramer,Valerie Restat,Sebastian Strasser,Uta Störl,Meike Klettke*

Main category: cs.DB

TL;DR: 数据工程流水线存在质量和适应性问题。论文提出了下一代具有优化、自感知和自适应能力的数据流水线。


<details>
  <summary>Details</summary>
Motivation: 当前的数据工程流水线在提供高质量数据方面存在挑战，并且对变化不敏感。当新数据偏离先前数据时，流水线可能会崩溃或输出不良结果。

Method: 提出了优化、自感知和自适应数据工程流水线的方法。

Result: 设想了下一代数据工程流水线的三个层次：优化数据流水线、自感知数据流水线和自适应数据流水线。

Conclusion: 提出了下一代数据工程流水线的三个层次：优化数据流水线、自感知数据流水线和自适应数据流水线，并提出了实现每个层次的方法。

Abstract: Data engineering pipelines are a widespread way to provide high-quality data
for all kinds of data science applications. However, numerous challenges still
remain in the composition and operation of such pipelines. Data engineering
pipelines do not always deliver high-quality data. By default, they are also
not reactive to changes. When new data is coming in which deviates from prior
data, the pipeline could crash or output undesired results. We therefore
envision three levels of next generation data engineering pipelines: optimized
data pipelines, self-aware data pipelines, and self-adapting data pipelines.
Pipeline optimization addresses the composition of operators and their
parametrization in order to achieve the highest possible data quality.
Self-aware data engineering pipelines enable a continuous monitoring of its
current state, notifying data engineers on significant changes. Self-adapting
data engineering pipelines are then even able to automatically react to those
changes. We propose approaches to achieve each of these levels.

</details>


### [72] [Project-connex Decompositions and Tractability of Aggregate Group-by Conjunctive Queries](https://arxiv.org/abs/2507.14101)
*Diego Figueira,Cibele Freire*

Main category: cs.DB

TL;DR: Introduces project-connex tree-width as a measure of tractability for conjunctive queries with 'group-by' projection, unifying algorithmic manipulation and explaining existing tractability results.


<details>
  <summary>Details</summary>
Motivation: To introduce a measure of tractability for counting and aggregate conjunctive queries over semirings with 'group-by' projection, allowing comparable complexity bounds to previous structural conditions.

Method: Introduces 'project-connex' tree-width, defines project-connex tree decompositions as an extension of 'free-connex' decompositions, and uses algorithms for computing classical tree decompositions.

Result: Obtains complexity bounds comparable to previous structural conditions, explains existing tractability results, recovers results relating tractable classes of counting conjunctive queries and bounded free-connex tree-width, and shows project-connex tree decompositions can be obtained via algorithms for computing classical tree decompositions.

Conclusion: Project-connex tree decompositions provide a unified algorithmic approach for aggregate query evaluation and explain existing tractability results.  The measure recovers results relating tractable counting conjunctive queries and bounded free-connex tree-width, or constant-time delay enumeration of semiring aggregate queries over bounded project-connex classes. Project-connex tree decompositions can be obtained via algorithms for computing classical tree decompositions.

Abstract: We introduce 'project-connex' tree-width as a measure of tractability for
counting and aggregate conjunctive queries over semirings with 'group-by'
projection (also known as 'AJAR' or 'FAQ' queries). This elementary measure
allows to obtain comparable complexity bounds to the ones obtained by previous
structural conditions tailored for efficient evaluation of semiring aggregate
queries, enumeration algorithms of conjunctive queries, and tractability of
counting answers to conjunctive queries.
  Project-connex tree decompositions are defined as the natural extension of
the known notion of 'free-connex' decompositions. They allow for a unified,
simple and intuitive algorithmic manipulation for evaluation of aggregate
queries and explain some existing tractability results on conjunctive query
enumeration, counting conjunctive query evaluation, and evaluation of semiring
aggregate queries. Using this measure we also recover results relating
tractable classes of counting conjunctive queries and bounded free-connex
tree-width, or the constant-time delay enumeration of semiring aggregate
queries over bounded project-connex classes. We further show that
project-connex tree decompositions can be obtained via algorithms for computing
classical tree decompositions.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [73] [DyG-RAG: Dynamic Graph Retrieval-Augmented Generation with Event-Centric Reasoning](https://arxiv.org/abs/2507.13396)
*Qingyun Sun,Jiaqi Yuan,Shan He,Xiao Guan,Haonan Yuan,Xingcheng Fu,Jianxin Li,Philip S. Yu*

Main category: cs.IR

TL;DR: DyG-RAG是一种新颖的以事件为中心的动态图检索增强生成框架，旨在捕获和推理嵌入在非结构化文本中的时间知识。


<details>
  <summary>Details</summary>
Motivation: 现有的图RAG方法难以进行时间推理，因为它们无法对现实世界事件的演变结构和顺序进行建模。

Method: DyG-RAG构建了一个事件图，通过连接共享实体并在时间上接近的DEU来捕获事件之间的时间和因果依赖关系，支持高效和有意义的多跳推理。为了确保时间上一致的生成，DyG-RAG引入了一个事件时间线检索管道，该管道通过时间感知遍历来检索事件序列，并提出了一个时间链式思考策略，用于时间上接地的答案生成。

Result: DyG-RAG检索连贯的、时间上排序的事件序列，并回答标准RAG系统无法解决的复杂的时间敏感查询。

Conclusion: DyG-RAG显著提高了三种典型的时间推理问题的准确率和召回率，为更忠实和时间感知的生成铺平了道路。

Abstract: Graph Retrieval-Augmented Generation has emerged as a powerful paradigm for
grounding large language models with external structured knowledge. However,
existing Graph RAG methods struggle with temporal reasoning, due to their
inability to model the evolving structure and order of real-world events. In
this work, we introduce DyG-RAG, a novel event-centric dynamic graph
retrieval-augmented generation framework designed to capture and reason over
temporal knowledge embedded in unstructured text. To eliminate temporal
ambiguity in traditional retrieval units, DyG-RAG proposes Dynamic Event Units
(DEUs) that explicitly encode both semantic content and precise temporal
anchors, enabling accurate and interpretable time-aware retrieval. To capture
temporal and causal dependencies across events, DyG-RAG constructs an event
graph by linking DEUs that share entities and occur close in time, supporting
efficient and meaningful multi-hop reasoning. To ensure temporally consistent
generation, DyG-RAG introduces an event timeline retrieval pipeline that
retrieves event sequences via time-aware traversal, and proposes a Time
Chain-of-Thought strategy for temporally grounded answer generation. This
unified pipeline enables DyG-RAG to retrieve coherent, temporally ordered event
sequences and to answer complex, time-sensitive queries that standard RAG
systems cannot resolve. Extensive experiments on temporal QA benchmarks
demonstrate that DyG-RAG significantly improves the accuracy and recall of
three typical types of temporal reasoning questions, paving the way for more
faithful and temporal-aware generation. DyG-RAG is available at
https://github.com/RingBDStack/DyG-RAG.

</details>


### [74] [Revisiting Prompt Engineering: A Comprehensive Evaluation for LLM-based Personalized Recommendation](https://arxiv.org/abs/2507.13525)
*Genki Kusano,Kosuke Akimoto,Kunihiro Takeoka*

Main category: cs.IR

TL;DR: 本文比较了不同提示策略对 LLM 推荐性能的影响，发现简单提示对于高性能 LLM 更有效，而特定类型的提示对成本效益高的 LLM 更有效。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 可以通过采用自然语言编写的提示作为输入来执行推荐任务。与协同过滤等传统方法相比，基于 LLM 的推荐在处理冷启动、跨领域和零样本场景方面具有优势，并且支持灵活的输入格式和生成用户行为的解释。在本文中，我们关注的是单用户设置，其中不使用来自其他用户的信息。这种设置对于隐私敏感或数据有限的应用程序来说是实用的。

Method: 大规模比较了 23 种提示类型，跨越 8 个公共数据集和 12 个 LLM。我们使用统计测试和线性混合效应模型来评估准确性和推理成本。

Result: 对于成本效益高的 LLM，三种类型的提示特别有效：那些改写指令、考虑背景知识并使推理过程更易于遵循的提示。对于高性能 LLM，简单的提示通常优于更复杂的提示，同时降低了成本。相比之下，自然语言处理中常用的提示风格，如逐步推理或推理模型的使用，通常会导致较低的准确性。

Conclusion: 对于成本效益高的LLM，三种类型的提示特别有效：那些改写指令、考虑背景知识并使推理过程更易于遵循的提示。对于高性能LLM，简单的提示通常优于更复杂的提示，同时降低了成本。

Abstract: Large language models (LLMs) can perform recommendation tasks by taking
prompts written in natural language as input. Compared to traditional methods
such as collaborative filtering, LLM-based recommendation offers advantages in
handling cold-start, cross-domain, and zero-shot scenarios, as well as
supporting flexible input formats and generating explanations of user behavior.
In this paper, we focus on a single-user setting, where no information from
other users is used. This setting is practical for privacy-sensitive or
data-limited applications. In such cases, prompt engineering becomes especially
important for controlling the output generated by the LLM. We conduct a
large-scale comparison of 23 prompt types across 8 public datasets and 12 LLMs.
We use statistical tests and linear mixed-effects models to evaluate both
accuracy and inference cost. Our results show that for cost-efficient LLMs,
three types of prompts are especially effective: those that rephrase
instructions, consider background knowledge, and make the reasoning process
easier to follow. For high-performance LLMs, simple prompts often outperform
more complex ones while reducing cost. In contrast, commonly used prompting
styles in natural language processing, such as step-by-step reasoning, or the
use of reasoning models often lead to lower accuracy. Based on these findings,
we provide practical suggestions for selecting prompts and LLMs depending on
the required balance between accuracy and cost.

</details>


### [75] [IP2: Entity-Guided Interest Probing for Personalized News Recommendation](https://arxiv.org/abs/2507.13622)
*Youlin Wu,Yuanyuan Sun,Xiaokun Zhang,Haoxi Zhan,Bo Xu,Liang Yang,Hongfei Lin*

Main category: cs.IR

TL;DR: IP2模型通过在新闻的内部和外部层面上探测实体引导的阅读兴趣，从而改进新闻推荐。


<details>
  <summary>Details</summary>
Motivation: 当前方法忽略了实体在新闻推荐中的独特效用。

Method: Transformer-based实体编码器和双塔用户编码器。

Result: IP2在两个真实世界的数据集上实现了最先进的性能。

Conclusion: IP2在新闻推荐方面取得了最先进的性能。

Abstract: News recommender systems aim to provide personalized news reading experiences
for users based on their reading history. Behavioral science studies suggest
that screen-based news reading contains three successive steps: scanning, title
reading, and then clicking. Adhering to these steps, we find that intra-news
entity interest dominates the scanning stage, while the inter-news entity
interest guides title reading and influences click decisions. Unfortunately,
current methods overlook the unique utility of entities in news recommendation.
To this end, we propose a novel method called IP2 to probe entity-guided
reading interest at both intra- and inter-news levels. At the intra-news level,
a Transformer-based entity encoder is devised to aggregate mentioned entities
in the news title into one signature entity. Then, a signature entity-title
contrastive pre-training is adopted to initialize entities with proper meanings
using the news story context, which in the meantime facilitates us to probe for
intra-news entity interest. As for the inter-news level, a dual tower user
encoder is presented to capture inter-news reading interest from both the title
meaning and entity sides. In addition to highlighting the contribution of
inter-news entity guidance, a cross-tower attention link is adopted to
calibrate title reading interest using inter-news entity interest, thus further
aligning with real-world behavior. Extensive experiments on two real-world
datasets demonstrate that our IP2 achieves state-of-the-art performance in news
recommendation.

</details>


### [76] [Point of Interest Recommendation: Pitfalls and Viable Solutions](https://arxiv.org/abs/2507.13725)
*Alejandro Bellogín,Linus W. Dietz,Francesco Ricci,Pablo Sánchez*

Main category: cs.IR

TL;DR: The paper reviews POI recommendation, identifies shortcomings in datasets, algorithms, and evaluation, and suggests future research directions.


<details>
  <summary>Details</summary>
Motivation: POI recommendation is inherently high-stakes, but several fundamental issues remain unresolved, hindering the real-world applicability.

Method: critical assessment

Result: Identified key shortcomings across three main dimensions: datasets, algorithms, and evaluation methodologies.

Conclusion: This paper discusses the current status of POI recommendation and identifies key shortcomings across datasets, algorithms, and evaluation methodologies. It proposes a structured research agenda for future work.

Abstract: Point of interest (POI) recommendation can play a pivotal role in enriching
tourists' experiences by suggesting context-dependent and preference-matching
locations and activities, such as restaurants, landmarks, itineraries, and
cultural attractions. Unlike some more common recommendation domains (e.g.,
music and video), POI recommendation is inherently high-stakes: users invest
significant time, money, and effort to search, choose, and consume these
suggested POIs. Despite the numerous research works in the area, several
fundamental issues remain unresolved, hindering the real-world applicability of
the proposed approaches. In this paper, we discuss the current status of the
POI recommendation problem and the main challenges we have identified. The
first contribution of this paper is a critical assessment of the current state
of POI recommendation research and the identification of key shortcomings
across three main dimensions: datasets, algorithms, and evaluation
methodologies. We highlight persistent issues such as the lack of standardized
benchmark datasets, flawed assumptions in the problem definition and model
design, and inadequate treatment of biases in the user behavior and system
performance. The second contribution is a structured research agenda that,
starting from the identified issues, introduces important directions for future
work related to multistakeholder design, context awareness, data collection,
trustworthiness, novel interactions, and real-world evaluation.

</details>


### [77] [RAG-based Architectures for Drug Side Effect Retrieval in LLMs](https://arxiv.org/abs/2507.13822)
*Shad Nygren,Pinar Avci,Andre Daniels,Reza Rassol,Afshin Beheshti,Diego Galeano*

Main category: cs.IR

TL;DR: The paper introduces GraphRAG, a novel method that integrates drug side effect knowledge into a Llama 3 8B language model, achieving near-perfect accuracy in drug side effect retrieval,signifying a significant advancement in leveraging LLMs for critical pharmacovigilance applications.


<details>
  <summary>Details</summary>
Motivation: LLMs have limitations in specialized fields like pharmacovigilance due to reliance on black-box training data, susceptibility to hallucinations, and lack of domain-specific knowledge.

Method: Retrieval-Augmented Generation (RAG) and GraphRAG, which integrate comprehensive drug side effect knowledge into a Llama 3 8B language model.

Result: GraphRAG achieves near-perfect accuracy in drug side effect retrieval on 19,520 drug side effect associations (covering 976 drugs and 3,851 side effect terms).

Conclusion: GraphRAG achieves near-perfect accuracy in drug side effect retrieval, offering a highly accurate and scalable solution for critical pharmacovigilance applications.

Abstract: Drug side effects are a major global health concern, necessitating advanced
methods for their accurate detection and analysis. While Large Language Models
(LLMs) offer promising conversational interfaces, their inherent limitations,
including reliance on black-box training data, susceptibility to
hallucinations, and lack of domain-specific knowledge, hinder their reliability
in specialized fields like pharmacovigilance. To address this gap, we propose
two architectures: Retrieval-Augmented Generation (RAG) and GraphRAG, which
integrate comprehensive drug side effect knowledge into a Llama 3 8B language
model. Through extensive evaluations on 19,520 drug side effect associations
(covering 976 drugs and 3,851 side effect terms), our results demonstrate that
GraphRAG achieves near-perfect accuracy in drug side effect retrieval. This
framework offers a highly accurate and scalable solution, signifying a
significant advancement in leveraging LLMs for critical pharmacovigilance
applications.

</details>


### [78] [SPARQL Query Generation with LLMs: Measuring the Impact of Training Data Memorization and Knowledge Injection](https://arxiv.org/abs/2507.13859)
*Aleksandr Gashkov,Aleksandr Perevalov,Maria Eltsova,Andreas Both*

Main category: cs.IR

TL;DR: This paper presents a new method to test how well LLMs generate SPARQL queries, checking if they're actually smart or just memorizing training data.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of control over training data in LLMs and its potential impact on QA quality, especially concerning benchmark memorization.

Method: The paper evaluates LLMs under three conditions: zero-shot SPARQL generation, with knowledge injection, and with anonymized knowledge injection.

Result: The method enables estimation of the influence of training data on QA quality and identification of method portability.

Conclusion: The paper introduces a portable and robust method to evaluate the quality of LLMs in generating SPARQL queries from natural language questions, applicable to any KGQA or LLM.

Abstract: Nowadays, the importance of software with natural-language user interfaces
cannot be underestimated. In particular, in Question Answering (QA) systems,
generating a SPARQL query for a given natural-language question (often named
Query Building) from the information retrieved from the same question is the
central task of QA systems working over Knowledge Graphs (KGQA). Due to the
rise of Large Language Models (LLMs), they are considered a well-suited method
to increase the quality of the question-answering functionality, as there is
still a lot of room for improvement, aiming for enhanced quality and
trustworthiness. However, LLMs are trained on web data, where researchers have
no control over whether the benchmark or the knowledge graph was already
included in the training data. In this paper, we introduce a novel method that
evaluates the quality of LLMs by generating a SPARQL query from a
natural-language question under various conditions: (1) zero-shot SPARQL
generation, (2) with knowledge injection, and (3) with "anonymized" knowledge
injection. This enables us, for the first time, to estimate the influence of
the training data on the QA quality improved by LLMs. Ultimately, this will
help to identify how portable a method is or whether good results might mostly
be achieved because a benchmark was already included in the training data (cf.
LLM memorization). The developed method is portable, robust, and supports any
knowledge graph; therefore, it could be easily applied to any KGQA or LLM,
s.t., generating consistent insights into the actual LLM capabilities is
possible.

</details>


### [79] [PARK: Personalized academic retrieval with knowledge-graphs](https://arxiv.org/abs/2507.13910)
*Pranav Kasela,Gabriella Pasi,Raffaele Perego*

Main category: cs.IR

TL;DR: 提出了一种基于知识图谱的学术搜索个性化方法，该方法优于现有的模型。


<details>
  <summary>Details</summary>
Motivation: 现有的学术搜索个性化模型通常难以完全捕捉用户的学术兴趣。引文图谱是支持推荐系统结果的一种有价值的手段，但其在个性化学术搜索中的应用仍有待探索。

Method: 提出了一种两步法：首先，训练一个用于检索的神经语言模型，然后将学术图转换为知识图谱，并使用翻译嵌入技术将其嵌入到与语言模型共享的语义空间中。

Result: 在四个学术搜索领域评估了该方法，在四个领域中的三个领域优于传统的基于图的模型和个性化模型，在 MAP@100 方面比第二好的模型提高了 10%。

Conclusion: 知识图谱用户模型有潜力提高检索效率。

Abstract: Academic Search is a search task aimed to manage and retrieve scientific
documents like journal articles and conference papers. Personalization in this
context meets individual researchers' needs by leveraging, through user
profiles, the user related information (e.g. documents authored by a
researcher), to improve search effectiveness and to reduce the information
overload. While citation graphs are a valuable means to support the outcome of
recommender systems, their use in personalized academic search (with, e.g.
nodes as papers and edges as citations) is still under-explored.
  Existing personalized models for academic search often struggle to fully
capture users' academic interests. To address this, we propose a two-step
approach: first, training a neural language model for retrieval, then
converting the academic graph into a knowledge graph and embedding it into a
shared semantic space with the language model using translational embedding
techniques. This allows user models to capture both explicit relationships and
hidden structures in citation graphs and paper content. We evaluate our
approach in four academic search domains, outperforming traditional graph-based
and personalized models in three out of four, with up to a 10\% improvement in
MAP@100 over the second-best model. This highlights the potential of knowledge
graph-based user models to enhance retrieval effectiveness.

</details>


### [80] [DUALRec: A Hybrid Sequential and Language Model Framework for Context-Aware Movie Recommendation](https://arxiv.org/abs/2507.13957)
*Yitong Li,Raoul Grasman*

Main category: cs.IR

TL;DR: 提出了一种名为DUALRec的新推荐模型，它结合了LSTM和LLM的优势，以更好地捕捉用户偏好并提高推荐质量。


<details>
  <summary>Details</summary>
Motivation: 现代推荐系统面临着对动态和上下文丰富的用户偏好进行建模和预测的日益严峻的挑战。传统的协同过滤和基于内容的方法通常难以捕捉时间模式和不断变化的用户意图。大型语言模型（LLM）近年来逐渐受到关注，但它们并非天生设计用于对按时间顺序演变的用户偏好和意图进行建模。另一方面，对于像LSTM（长短期记忆）这样的序列模型，它擅长捕捉用户行为的时间动态和随时间变化的用户偏好，但仍然缺乏对全面推荐生成的丰富的语义理解。

Method: 提出DUALRec（动态用户感知语言推荐器），该推荐器结合了LSTM网络的时间建模能力和微调的大型语言模型的语义推理能力。

Result: 实验结果表明，DUALRec模型优于各种基线模型，具有全面的评估指标，包括命中率（HR@k）、归一化折损累积增益（NDCG@k）和类型相似性指标。

Conclusion: DUALRec模型在MovieLens-1M数据集上优于各种基线模型，并在命中率（HR@k）、归一化折损累积增益（NDCG@k）和类型相似性指标方面表现出色。该研究提出了一种新的架构，弥合了时间序列建模和语义推理之间的差距，为开发更智能和上下文感知的推荐器提供了有希望的方向。

Abstract: The modern recommender systems are facing an increasing challenge of
modelling and predicting the dynamic and context-rich user preferences.
Traditional collaborative filtering and content-based methods often struggle to
capture the temporal patternings and evolving user intentions. While Large
Language Models (LLMs) have gained gradual attention in recent years, by their
strong semantic understanding and reasoning abilities, they are not inherently
designed to model chronologically evolving user preference and intentions. On
the other hand, for sequential models like LSTM (Long-Short-Term-Memory) which
is good at capturing the temporal dynamics of user behaviour and evolving user
preference over time, but still lacks a rich semantic understanding for
comprehensive recommendation generation. In this study, we propose DUALRec
(Dynamic User-Aware Language-based Recommender), a novel recommender that
leverages the complementary strength of both models, which combines the
temporal modelling abilities of LSTM networks with semantic reasoning power of
the fine-tuned Large Language Models. The LSTM component will capture users
evolving preference through their viewing history, while the fine-tuned LLM
variants will leverage these temporal user insights to generate next movies
that users might enjoy. Experimental results on MovieLens-1M dataset shows that
the DUALRec model outperforms a wide range of baseline models, with
comprehensive evaluation matrices of Hit Rate (HR@k), Normalized Discounted
Cumulative Gain (NDCG@k), and genre similarity metrics. This research proposes
a novel architecture that bridges the gap between temporal sequence modeling
and semantic reasoning, and offers a promising direction for developing more
intelligent and context-aware recommenders.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [81] [Physical models realizing the transformer architecture of large language models](https://arxiv.org/abs/2507.13354)
*Zeqian Chen*

Main category: cs.LG

TL;DR: This paper constructs physical models for large language models based on the transformer architecture from a physical perspective.


<details>
  <summary>Details</summary>
Motivation: There is a gap in our theoretical understanding of what the transformer is, and why it works physically.

Method: We construct physical models in the Fock space over the Hilbert space of tokens realizing large language models based on a transformer architecture as open quantum systems.

Result: Physical models in the Fock space over the Hilbert space of tokens realizing large language models based on a transformer architecture as open quantum systems.

Conclusion: Physical models underlie the transformer architecture for large language models.

Abstract: The introduction of the transformer architecture in 2017 (cf.\cite{VSP2017})
marked the most striking advancement in natural language processing. The
transformer is a model architecture relying entirely on an attention mechanism
to draw global dependencies between input and output. However, we believe there
is a gap in our theoretical understanding of what the transformer is, and why
it works physically. In this paper, from a physical perspective on modern
chips, we construct physical models in the Fock space over the Hilbert space of
tokens realizing large language models based on a transformer architecture as
open quantum systems. Our physical models underlie the transformer architecture
for large language models.

</details>


### [82] [Whose View of Safety? A Deep DIVE Dataset for Pluralistic Alignment of Text-to-Image Models](https://arxiv.org/abs/2507.13383)
*Charvi Rastogi,Tian Huey Teh,Pushkar Mishra,Roma Patel,Ding Wang,Mark Díaz,Alicia Parrish,Aida Mostafazadeh Davani,Zoe Ashwood,Michela Paganini,Vinodkumar Prabhakaran,Verena Rieser,Lora Aroyo*

Main category: cs.LG

TL;DR: This paper introduces a new dataset (DIVE) to improve the alignment of text-to-image models with diverse human values, finding that demographics are a crucial factor in harm perception. The paper also discusses implications for building aligned T2I models, including efficient data collection strategies, LLM judgment capabilities, and model steerability towards diverse perspectives.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image (T2I) models often fail to account for diverse human experiences, leading to misaligned systems. We advocate for pluralistic alignment, where an AI understands and is steerable towards diverse, and often conflicting, human values.

Method: introducing a novel dataset for Diverse Intersectional Visual Evaluation (DIVE)

Result: empirically confirm demographics as a crucial proxy for diverse viewpoints in this domain, revealing significant, context-dependent differences in harm perception that diverge from conventional evaluations.

Conclusion: This research offers foundational tools for more equitable and aligned T2I systems.

Abstract: Current text-to-image (T2I) models often fail to account for diverse human
experiences, leading to misaligned systems. We advocate for pluralistic
alignment, where an AI understands and is steerable towards diverse, and often
conflicting, human values. Our work provides three core contributions to
achieve this in T2I models. First, we introduce a novel dataset for Diverse
Intersectional Visual Evaluation (DIVE) -- the first multimodal dataset for
pluralistic alignment. It enable deep alignment to diverse safety perspectives
through a large pool of demographically intersectional human raters who
provided extensive feedback across 1000 prompts, with high replication,
capturing nuanced safety perceptions. Second, we empirically confirm
demographics as a crucial proxy for diverse viewpoints in this domain,
revealing significant, context-dependent differences in harm perception that
diverge from conventional evaluations. Finally, we discuss implications for
building aligned T2I models, including efficient data collection strategies,
LLM judgment capabilities, and model steerability towards diverse perspectives.
This research offers foundational tools for more equitable and aligned T2I
systems. Content Warning: The paper includes sensitive content that may be
harmful.

</details>


### [83] [Improving KAN with CDF normalization to quantiles](https://arxiv.org/abs/2507.13393)
*Jakub Strawa,Jarek Duda*

Main category: cs.LG

TL;DR: CDF normalization, common in copula theory, improves KAN performance by simplifying representations and enabling probability distribution propagation.


<details>
  <summary>Details</summary>
Motivation: Data normalization is crucial in machine learning. Copula theory uses CDF normalization, which is nearly unknown in machine learning, but it has advantages.

Method: Using CDF normalization to transform data to approximately quantiles, resulting in a nearly uniform distribution [0,1].

Result: Switching rescaling to CDF normalization improves predictions from Legendre-KAN. Weights of neurons are mixed moments providing local joint distribution models, allow to propagate also probability distributions, and change propagation direction.

Conclusion: CDF normalization improves predictions in Kolmogorov-Arnold Networks (KANs) by providing simpler representations and reducing overfitting.

Abstract: Data normalization is crucial in machine learning, usually performed by
subtracting the mean and dividing by standard deviation, or by rescaling to a
fixed range. In copula theory, popular in finance, there is used normalization
to approximately quantiles by transforming x to CDF(x) with estimated CDF
(cumulative distribution function) to nearly uniform distribution in [0,1],
allowing for simpler representations which are less likely to overfit. It seems
nearly unknown in machine learning, therefore, we would like to present some
its advantages on example of recently popular Kolmogorov-Arnold Networks
(KANs), improving predictions from Legendre-KAN by just switching rescaling to
CDF normalization. Additionally, in HCR interpretation, weights of such neurons
are mixed moments providing local joint distribution models, allow to propagate
also probability distributions, and change propagation direction.

</details>


### [84] [Selective Embedding for Deep Learning](https://arxiv.org/abs/2507.13399)
*Mert Sehri,Zehui Hua,Francisco de Assis Boldt,Patrick Dumond*

Main category: cs.LG

TL;DR: Selective embedding, a novel data loading strategy, mimics human-like information processing to reduce model overfitting, enhance generalization, and improve computational efficiency on time-domain datasets.


<details>
  <summary>Details</summary>
Motivation: Deep learning algorithms are sensitive to input data, and performance often deteriorates under nonstationary conditions and across dissimilar domains, especially when using time-domain data. Conventional single-channel or parallel multi-source data loading strategies either limit generalization or increase computational costs.

Method: This study introduces selective embedding, a novel data loading strategy, which alternates short segments of data from multiple sources within a single input channel.

Result: Validation is conducted using six time-domain datasets, demonstrating that the proposed method consistently achieves high classification accuracy across various deep learning architectures while significantly reducing training times.

Conclusion: Selective embedding achieves high classification accuracy across various deep learning architectures while significantly reducing training times, especially for complex systems with multiple data sources.

Abstract: Deep learning has revolutionized many industries by enabling models to
automatically learn complex patterns from raw data, reducing dependence on
manual feature engineering. However, deep learning algorithms are sensitive to
input data, and performance often deteriorates under nonstationary conditions
and across dissimilar domains, especially when using time-domain data.
Conventional single-channel or parallel multi-source data loading strategies
either limit generalization or increase computational costs. This study
introduces selective embedding, a novel data loading strategy, which alternates
short segments of data from multiple sources within a single input channel.
Drawing inspiration from cognitive psychology, selective embedding mimics
human-like information processing to reduce model overfitting, enhance
generalization, and improve computational efficiency. Validation is conducted
using six time-domain datasets, demonstrating that the proposed method
consistently achieves high classification accuracy across various deep learning
architectures while significantly reducing training times. The approach proves
particularly effective for complex systems with multiple data sources, offering
a scalable and resource-efficient solution for real-world applications in
healthcare, heavy machinery, marine, railway, and agriculture, where robustness
and adaptability are critical.

</details>


### [85] [LightAutoDS-Tab: Multi-AutoML Agentic System for Tabular Data](https://arxiv.org/abs/2507.13413)
*Aleksey Lapin,Igor Hromov,Stanislav Chumakov,Mile Mitrovic,Dmitry Simakov,Nikolay O. Nikitin,Andrey V. Savchenko*

Main category: cs.LG

TL;DR: LightAutoDS-Tab是一种用于表格数据的多AutoML代理系统，它结合了LLM和AutoML工具，优于Kaggle上的现有解决方案。


<details>
  <summary>Details</summary>
Motivation: AutoML在利用LLM处理复杂任务方面取得了进展，但其效率仍然受到对特定底层工具的依赖性的限制。

Method: 结合了基于LLM的代码生成和多种AutoML工具的多AutoML代理系统。

Result: 提高了pipeline设计的灵活性和鲁棒性。

Conclusion: LightAutoDS-Tab在多个Kaggle数据科学任务中优于最先进的开源解决方案。

Abstract: AutoML has advanced in handling complex tasks using the integration of LLMs,
yet its efficiency remains limited by dependence on specific underlying tools.
In this paper, we introduce LightAutoDS-Tab, a multi-AutoML agentic system for
tasks with tabular data, which combines an LLM-based code generation with
several AutoML tools. Our approach improves the flexibility and robustness of
pipeline design, outperforming state-of-the-art open-source solutions on
several data science tasks from Kaggle. The code of LightAutoDS-Tab is
available in the open repository https://github.com/sb-ai-lab/LADS

</details>


### [86] [Gauge Flow Models](https://arxiv.org/abs/2507.13414)
*Alexander Strunk,Roland Assam*

Main category: cs.LG

TL;DR: Gauge Flow Models incorporates a learnable Gauge Field within the Flow Ordinary Differential Equation (ODE) and yields significantly better performance than traditional Flow Models.


<details>
  <summary>Details</summary>
Motivation: This paper introduces Gauge Flow Models, a novel class of Generative Flow Models

Method: incorporate a learnable Gauge Field within the Flow Ordinary Differential Equation (ODE)

Result: Experiments using Flow Matching on Gaussian Mixture Models demonstrate that Gauge Flow Models yields significantly better performance than traditional Flow Models

Conclusion: Gauge Flow Models yields significantly better performance than traditional Flow Models. Additionally, a potential for enhanced performance across a broader range of generative tasks.

Abstract: This paper introduces Gauge Flow Models, a novel class of Generative Flow
Models. These models incorporate a learnable Gauge Field within the Flow
Ordinary Differential Equation (ODE). A comprehensive mathematical framework
for these models, detailing their construction and properties, is provided.
Experiments using Flow Matching on Gaussian Mixture Models demonstrate that
Gauge Flow Models yields significantly better performance than traditional Flow
Models of comparable or even larger size. Additionally, unpublished research
indicates a potential for enhanced performance across a broader range of
generative tasks.

</details>


### [87] [Single- to multi-fidelity history-dependent learning with uncertainty quantification and disentanglement: application to data-driven constitutive modeling](https://arxiv.org/abs/2507.13416)
*Jiaxiang Yi,Bernardo P. Ferreira,Miguel A. Bessa*

Main category: cs.LG

TL;DR: 提出了一种广义的层次方法，用于处理多保真度数据，量化不确定性，并适应不同的学习场景，并通过应用于不同的本构建模场景来证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的学习被推广到考虑历史相关的多保真度数据，同时量化认知不确定性并将其与数据噪声（偶然不确定性）分离。

Method: 分层概括并适应不同的学习场景：从训练最简单的单保真度确定性神经网络到提出的多保真度方差估计贝叶斯循环神经网络。

Result: 通过将其应用于不同的数据驱动的本构建模场景来证明所提出的方法的多功能性和通用性，这些场景包括具有和不具有偶然不确定性（噪声）的多个保真度。

Conclusion: 该方法准确地预测了响应并量化了模型误差，同时还发现了噪声分布（如果存在）。这为未来在不同科学和工程领域的实际应用开辟了机会；特别是，涉及不确定性下的设计和分析的最具挑战性的情况。

Abstract: Data-driven learning is generalized to consider history-dependent
multi-fidelity data, while quantifying epistemic uncertainty and disentangling
it from data noise (aleatoric uncertainty). This generalization is hierarchical
and adapts to different learning scenarios: from training the simplest
single-fidelity deterministic neural networks up to the proposed multi-fidelity
variance estimation Bayesian recurrent neural networks. The versatility and
generality of the proposed methodology are demonstrated by applying it to
different data-driven constitutive modeling scenarios that include multiple
fidelities with and without aleatoric uncertainty (noise). The method
accurately predicts the response and quantifies model error while also
discovering the noise distribution (when present). This opens opportunities for
future real-world applications in diverse scientific and engineering domains;
especially, the most challenging cases involving design and analysis under
uncertainty.

</details>


### [88] [Soft-ECM: An extension of Evidential C-Means for complex data](https://arxiv.org/abs/2507.13417)
*Armel Soubeiga,Thomas Guyet,Violaine Antoine*

Main category: cs.LG

TL;DR: This paper introduces Soft-ECM, a new evidential clustering algorithm that can handle complex data like mixed data and time series by using a semi-metric, overcoming the limitations of existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing belief function-based clustering algorithms cannot be applied to complex data like mixed or non-tabular data because these data types are not represented in Euclidean space, which these algorithms rely on.

Method: Reformulation of the Evidential C-Means (ECM) problem and a new algorithm, Soft-ECM, which positions centroids of imprecise clusters using a semi-metric.

Result: Soft-ECM presents results comparable to conventional fuzzy clustering approaches on numerical data and demonstrates its ability to handle mixed data and benefits when combining fuzzy clustering with semi-metrics such as DTW for time series data.

Conclusion: Soft-ECM achieves comparable results to fuzzy clustering on numerical data, handles mixed data, and benefits from combining fuzzy clustering with semi-metrics like DTW for time series data.

Abstract: Clustering based on belief functions has been gaining increasing attention in
the machine learning community due to its ability to effectively represent
uncertainty and/or imprecision. However, none of the existing algorithms can be
applied to complex data, such as mixed data (numerical and categorical) or
non-tabular data like time series. Indeed, these types of data are, in general,
not represented in a Euclidean space and the aforementioned algorithms make use
of the properties of such spaces, in particular for the construction of
barycenters. In this paper, we reformulate the Evidential C-Means (ECM) problem
for clustering complex data. We propose a new algorithm, Soft-ECM, which
consistently positions the centroids of imprecise clusters requiring only a
semi-metric. Our experiments show that Soft-ECM present results comparable to
conventional fuzzy clustering approaches on numerical data, and we demonstrate
its ability to handle mixed data and its benefits when combining fuzzy
clustering with semi-metrics such as DTW for time series data.

</details>


### [89] [Air Traffic Controller Task Demand via Graph Neural Networks: An Interpretable Approach to Airspace Complexity](https://arxiv.org/abs/2507.13423)
*Edward Henderson,Dewi Gould,Richard Everson,George De Ath,Nick Pepper*

Main category: cs.LG

TL;DR: 本文介绍了一种可解释的图神经网络 (GNN) 框架，用于预测即将到来的许可数量。


<details>
  <summary>Details</summary>
Motivation: 由于现有的复杂性指标通常无法捕捉到除简单飞机计数之外的细微操作驱动因素，因此对近期空中交通管制员 (ATCO) 任务需求进行实时评估是日益拥挤的空域中的一项关键挑战。

Method: 基于注意力的模型预测即将到来的许可数量，即空中交通管制员向飞机发出的指令，来自静态交通场景中的交互。

Result: 该框架显着优于 ATCO 启发式方法，并且是比已建立的基线更可靠的场景复杂性估计器。

Conclusion: 该工具可以将任务需求归因于特定飞机，为分析和理解控制器培训和空域重新设计应用的复杂性驱动因素提供了一种新方法。

Abstract: Real-time assessment of near-term Air Traffic Controller (ATCO) task demand
is a critical challenge in an increasingly crowded airspace, as existing
complexity metrics often fail to capture nuanced operational drivers beyond
simple aircraft counts. This work introduces an interpretable Graph Neural
Network (GNN) framework to address this gap. Our attention-based model predicts
the number of upcoming clearances, the instructions issued to aircraft by
ATCOs, from interactions within static traffic scenarios. Crucially, we derive
an interpretable, per-aircraft task demand score by systematically ablating
aircraft and measuring the impact on the model's predictions. Our framework
significantly outperforms an ATCO-inspired heuristic and is a more reliable
estimator of scenario complexity than established baselines. The resulting tool
can attribute task demand to specific aircraft, offering a new way to analyse
and understand the drivers of complexity for applications in controller
training and airspace redesign.

</details>


### [90] [Improving Out-of-distribution Human Activity Recognition via IMU-Video Cross-modal Representation Learning](https://arxiv.org/abs/2507.13482)
*Seyyed Saeid Cheshmi,Buyao Lyu,Thomas Lisko,Rajesh Rajamani,Robert A. McGovern,Yogatheesan Varatharajah*

Main category: cs.LG

TL;DR: 该论文提出了一种新的跨模态自监督预训练方法，用于从 IMU 视频数据中学习表征，并在 HAR 任务中表现出改进的泛化能力，尤其是在帕金森病患者的数据集中。


<details>
  <summary>Details</summary>
Motivation: 基于可穿戴惯性传感器的 HAR 在远程健康监测中起着关键作用。运动障碍患者如果能够在家庭环境中检测到异常的患者动作，就可以持续优化治疗，并在需要时提醒看护人员。然而，大多数机器学习方法依赖于特定于应用程序的标签，并且缺乏对在不同环境或人群中收集的数据的泛化能力。

Method: 提出了一种新的跨模态自监督预训练方法，以从大规模未标记的 IMU 视频数据中学习表征。

Result: 该研究的结果表明，所提出的跨模态预训练方法在零样本和小样本评估下优于当前最先进的 IMU 视频预训练方法和仅 IMU 预训练方法。

Conclusion: 该研究表明，在高度动态的数据模式（如 IMU 信号）中，跨模态预训练可能是学习通用数据表示的有用工具。

Abstract: Human Activity Recognition (HAR) based on wearable inertial sensors plays a
critical role in remote health monitoring. In patients with movement disorders,
the ability to detect abnormal patient movements in their home environments can
enable continuous optimization of treatments and help alert caretakers as
needed. Machine learning approaches have been proposed for HAR tasks using
Inertial Measurement Unit (IMU) data; however, most rely on
application-specific labels and lack generalizability to data collected in
different environments or populations. To address this limitation, we propose a
new cross-modal self-supervised pretraining approach to learn representations
from large-sale unlabeled IMU-video data and demonstrate improved
generalizability in HAR tasks on out of distribution (OOD) IMU datasets,
including a dataset collected from patients with Parkinson's disease.
Specifically, our results indicate that the proposed cross-modal pretraining
approach outperforms the current state-of-the-art IMU-video pretraining
approach and IMU-only pretraining under zero-shot and few-shot evaluations.
Broadly, our study provides evidence that in highly dynamic data modalities,
such as IMU signals, cross-modal pretraining may be a useful tool to learn
generalizable data representations. Our software is available at
https://github.com/scheshmi/IMU-Video-OOD-HAR.

</details>


### [91] [Model-free Reinforcement Learning for Model-based Control: Towards Safe, Interpretable and Sample-efficient Agents](https://arxiv.org/abs/2507.13491)
*Thomas Banker,Ali Mesbah*

Main category: cs.LG

TL;DR: Model-based agents are introduced as a better alternative to model-free RL, addressing its shortcomings and offering safe, interpretable decision-making.


<details>
  <summary>Details</summary>
Motivation: Model-free RL relies on deep neural networks, which suffer from sample inefficiency, unsafe learning, and limited interpretability.

Method: Introduction of model-based agents as an alternative for control policy approximation, along with Bayesian optimization, policy search RL, and offline strategies.

Result: Outlines the benefits and challenges of learning model-based agents and details the primary learning approaches.

Conclusion: Model-based agents offer a compelling alternative for control policy approximation, leveraging adaptable models for safe policy learning, and their interplay with model-free RL holds potential for sample-efficient learning of safe and interpretable decision-making agents.

Abstract: Training sophisticated agents for optimal decision-making under uncertainty
has been key to the rapid development of modern autonomous systems across
fields. Notably, model-free reinforcement learning (RL) has enabled
decision-making agents to improve their performance directly through system
interactions, with minimal prior knowledge about the system. Yet, model-free RL
has generally relied on agents equipped with deep neural network function
approximators, appealing to the networks' expressivity to capture the agent's
policy and value function for complex systems. However, neural networks amplify
the issues of sample inefficiency, unsafe learning, and limited
interpretability in model-free RL. To this end, this work introduces
model-based agents as a compelling alternative for control policy
approximation, leveraging adaptable models of system dynamics, cost, and
constraints for safe policy learning. These models can encode prior system
knowledge to inform, constrain, and aid in explaining the agent's decisions,
while deficiencies due to model mismatch can be remedied with model-free RL. We
outline the benefits and challenges of learning model-based agents --
exemplified by model predictive control -- and detail the primary learning
approaches: Bayesian optimization, policy search RL, and offline strategies,
along with their respective strengths. While model-free RL has long been
established, its interplay with model-based agents remains largely unexplored,
motivating our perspective on their combined potentials for sample-efficient
learning of safe and interpretable decision-making agents.

</details>


### [92] [Fake or Real: The Impostor Hunt in Texts for Space Operations](https://arxiv.org/abs/2507.13508)
*Agata Kaczmarek,Dawid Płudowski,Piotr Wilczyński,Przemysław Biecek,Krzysztof Kotowski,Ramez Shendy,Jakub Nalepa,Artur Janicki,Evridiki Ntagiou*

Main category: cs.LG

TL;DR: Kaggle competition focusing on identifying malicious modifications of LLM output, motivated by AI security threats in space applications.


<details>
  <summary>Details</summary>
Motivation: The competition is motivated by real-life AI security threats (data poisoning and overreliance in LLMs) identified within the 'Assurance for Space Domain AI Applications' project.

Method: Participants develop techniques to distinguish between proper LLM output and output generated under malicious modification.

Result: The task is to distinguish between the proper output from LLM and the output generated under malicious modification of the LLM.

Conclusion: Participants need to develop new or adjust existing techniques to distinguish between proper LLM output and output generated under malicious modification, as this problem lacks extensive research.

Abstract: The "Fake or Real" competition hosted on Kaggle
(\href{https://www.kaggle.com/competitions/fake-or-real-the-impostor-hunt}{https://www.kaggle.com/competitions/fake-or-real-the-impostor-hunt})
is the second part of a series of follow-up competitions and hackathons related
to the "Assurance for Space Domain AI Applications" project funded by the
European Space Agency
(\href{https://assurance-ai.space-codev.org/}{https://assurance-ai.space-codev.org/}).
The competition idea is based on two real-life AI security threats identified
within the project -- data poisoning and overreliance in Large Language Models.
The task is to distinguish between the proper output from LLM and the output
generated under malicious modification of the LLM. As this problem was not
extensively researched, participants are required to develop new techniques to
address this issue or adjust already existing ones to this problem's statement.

</details>


### [93] [Off-Policy Evaluation and Learning for Matching Markets](https://arxiv.org/abs/2507.13608)
*Yudai Hayashi,Shuhei Goda,Yuta Saito*

Main category: cs.LG

TL;DR: 该论文针对匹配市场的离线策略评估问题，提出了DiPS和DPR两种新的OPE估计器，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在匹配市场中，由于用户互动的规模和双向性，标准的OPE方法存在方差问题和奖励稀疏性问题，使得其不可靠。为了解决这些挑战并促进有效的离线评估。

Method: 该论文结合了直接方法（DM）、逆倾向评分（IPS）和双重鲁棒（DR）估计器的元素，并结合了初始参与信号等中间标签，以实现更好的匹配市场中的偏差-方差控制。

Result: 该论文在理论上推导了所提出的估计器的偏差和方差，并证明了它们相对于传统方法的优势。实验结果表明，该方法在离线策略评估和学习任务中优于现有方法。

Conclusion: 该论文提出了新的OPE估计器DiPS和DPR，并在合成数据和真实招聘平台的A/B测试日志上进行了实验验证，结果表明该方法在各种配置下的离线策略评估和学习任务中优于现有方法。

Abstract: Matching users based on mutual preferences is a fundamental aspect of
services driven by reciprocal recommendations, such as job search and dating
applications. Although A/B tests remain the gold standard for evaluating new
policies in recommender systems for matching markets, it is costly and
impractical for frequent policy updates. Off-Policy Evaluation (OPE) thus plays
a crucial role by enabling the evaluation of recommendation policies using only
offline logged data naturally collected on the platform. However, unlike
conventional recommendation settings, the large scale and bidirectional nature
of user interactions in matching platforms introduce variance issues and
exacerbate reward sparsity, making standard OPE methods unreliable. To address
these challenges and facilitate effective offline evaluation, we propose novel
OPE estimators, \textit{DiPS} and \textit{DPR}, specifically designed for
matching markets. Our methods combine elements of the Direct Method (DM),
Inverse Propensity Score (IPS), and Doubly Robust (DR) estimators while
incorporating intermediate labels, such as initial engagement signals, to
achieve better bias-variance control in matching markets. Theoretically, we
derive the bias and variance of the proposed estimators and demonstrate their
advantages over conventional methods. Furthermore, we show that these
estimators can be seamlessly extended to offline policy learning methods for
improving recommendation policies for making more matches. We empirically
evaluate our methods through experiments on both synthetic data and A/B testing
logs from a real job-matching platform. The empirical results highlight the
superiority of our approach over existing methods in off-policy evaluation and
learning tasks for a variety of configurations.

</details>


### [94] [Provable Low-Frequency Bias of In-Context Learning of Representations](https://arxiv.org/abs/2507.13540)
*Yongyi Yang,Hidenori Tanaka,Wei Hu*

Main category: cs.LG

TL;DR: 这篇论文提出了一个双重收敛框架来解释ICL的机制，解释了实验观察结果，并证明了ICL对高频噪声的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）中的上下文学习（ICL）使其能够仅从输入序列中获取新的行为，而无需任何参数更新。最近的研究表明，ICL可以通过将提示的数据生成过程（DGP）的结构内化到隐藏表示中，从而超越在预训练阶段学习的原始含义。然而，LLM实现这种能力的机制仍然是开放的。

Method: 该论文介绍了一个双重收敛的统一框架，其中隐藏的表示在上下文和层上收敛。这种双重收敛过程导致了对平滑（低频）表示的隐式偏差，并通过分析证明和实验验证。

Result: 该论文的理论解释了为什么学习的表示表现出全局结构化但局部扭曲的几何形状，以及为什么它们的总能量衰减而没有消失。此外，该理论预测ICL对高频噪声具有内在的鲁棒性，并通过实验证实了这一点。

Conclusion: 这篇论文提出了一个双重收敛的统一框架，解释了ICL的机制，并解释了几个开放的经验观察，并通过实验证实了ICL对高频噪声具有内在的鲁棒性。

Abstract: In-context learning (ICL) enables large language models (LLMs) to acquire new
behaviors from the input sequence alone without any parameter updates. Recent
studies have shown that ICL can surpass the original meaning learned in
pretraining stage through internalizing the structure the data-generating
process (DGP) of the prompt into the hidden representations. However, the
mechanisms by which LLMs achieve this ability is left open. In this paper, we
present the first rigorous explanation of such phenomena by introducing a
unified framework of double convergence, where hidden representations converge
both over context and across layers. This double convergence process leads to
an implicit bias towards smooth (low-frequency) representations, which we prove
analytically and verify empirically. Our theory explains several open empirical
observations, including why learned representations exhibit globally structured
but locally distorted geometry, and why their total energy decays without
vanishing. Moreover, our theory predicts that ICL has an intrinsic robustness
towards high-frequency noise, which we empirically confirm. These results
provide new insights into the underlying mechanisms of ICL, and a theoretical
foundation to study it that hopefully extends to more general data
distributions and settings.

</details>


### [95] [Acoustic Index: A Novel AI-Driven Parameter for Cardiac Disease Risk Stratification Using Echocardiography](https://arxiv.org/abs/2507.13542)
*Beka Begiashvili,Carlos J. Fernandez-Candel,Matías Pérez Paredes*

Main category: cs.LG

TL;DR: The paper introduces the Acoustic Index, a novel AI-based tool for quantifying cardiac dysfunction from echocardiograms, showing high accuracy in detecting cardiac issues early on.


<details>
  <summary>Details</summary>
Motivation: Traditional echocardiographic parameters have limitations in early detection of cardiac dysfunction, creating a need for more reproducible, interpretable, and operator-independent parameters.

Method: The study introduces the Acoustic Index, an AI-derived echocardiographic parameter that combines Extended Dynamic Mode Decomposition (EDMD) with a hybrid neural network incorporating clinical metadata to quantify cardiac dysfunction.

Result: In a prospective cohort of 736 patients, the Acoustic Index achieved an AUC of 0.89 in an independent test set, with sensitivity and specificity exceeding 0.8 in cross-validation.

Conclusion: The Acoustic Index is a promising AI biomarker for cardiac function, offering potential for early detection, triage, and longitudinal monitoring. Future research will focus on external validation, longitudinal studies, and disease-specific adaptations.

Abstract: Traditional echocardiographic parameters such as ejection fraction (EF) and
global longitudinal strain (GLS) have limitations in the early detection of
cardiac dysfunction. EF often remains normal despite underlying pathology, and
GLS is influenced by load conditions and vendor variability. There is a growing
need for reproducible, interpretable, and operator-independent parameters that
capture subtle and global cardiac functional alterations.
  We introduce the Acoustic Index, a novel AI-derived echocardiographic
parameter designed to quantify cardiac dysfunction from standard ultrasound
views. The model combines Extended Dynamic Mode Decomposition (EDMD) based on
Koopman operator theory with a hybrid neural network that incorporates clinical
metadata. Spatiotemporal dynamics are extracted from echocardiographic
sequences to identify coherent motion patterns. These are weighted via
attention mechanisms and fused with clinical data using manifold learning,
resulting in a continuous score from 0 (low risk) to 1 (high risk).
  In a prospective cohort of 736 patients, encompassing various cardiac
pathologies and normal controls, the Acoustic Index achieved an area under the
curve (AUC) of 0.89 in an independent test set. Cross-validation across five
folds confirmed the robustness of the model, showing that both sensitivity and
specificity exceeded 0.8 when evaluated on independent data. Threshold-based
analysis demonstrated stable trade-offs between sensitivity and specificity,
with optimal discrimination near this threshold.
  The Acoustic Index represents a physics-informed, interpretable AI biomarker
for cardiac function. It shows promise as a scalable, vendor-independent tool
for early detection, triage, and longitudinal monitoring. Future directions
include external validation, longitudinal studies, and adaptation to
disease-specific classifiers.

</details>


### [96] [Time Series Forecastability Measures](https://arxiv.org/abs/2507.13556)
*Rui Wang,Steven Klee,Alexis Roos*

Main category: cs.LG

TL;DR: 本文提出在模型训练前用谱可预测性评分和最大 Lyapunov 指数来量化时间序列的可预测性，结果表明这些指标与实际预测性能密切相关。


<details>
  <summary>Details</summary>
Motivation: 与传统的模型评估指标不同，这些指标在任何预测尝试之前评估数据的固有可预测性特征。

Method: 提出了使用两个指标来量化模型开发前的时间序列的可预测性：谱可预测性评分和最大 Lyapunov 指数。

Result: 这两个指标可以正确反映时间序列的固有可预测性，并且与各种模型的实际预测性能密切相关。

Conclusion: 使用这两个指标可以正确反映时间序列的固有可预测性，并且与各种模型的实际预测性能密切相关。

Abstract: This paper proposes using two metrics to quantify the forecastability of time
series prior to model development: the spectral predictability score and the
largest Lyapunov exponent. Unlike traditional model evaluation metrics, these
measures assess the inherent forecastability characteristics of the data before
any forecast attempts. The spectral predictability score evaluates the strength
and regularity of frequency components in the time series, whereas the Lyapunov
exponents quantify the chaos and stability of the system generating the data.
We evaluated the effectiveness of these metrics on both synthetic and
real-world time series from the M5 forecast competition dataset. Our results
demonstrate that these two metrics can correctly reflect the inherent
forecastability of a time series and have a strong correlation with the actual
forecast performance of various models. By understanding the inherent
forecastability of time series before model training, practitioners can focus
their planning efforts on products and supply chain levels that are more
forecastable, while setting appropriate expectations or seeking alternative
strategies for products with limited forecastability.

</details>


### [97] [Change of Thought: Adaptive Test-Time Computation](https://arxiv.org/abs/2507.13569)
*Mrinal Mathur,Mike Doan,Barak Pearlmutter,Sergey Plis*

Main category: cs.LG

TL;DR: SELF-Transformer 是一种新的编码器层，它通过迭代细化注意力权重来提高表达能力，从而在不增加参数数量的情况下提高准确率。


<details>
  <summary>Details</summary>
Motivation: 为了在不依赖于 token 级别自回归的情况下，提高编码器 Transformer 的表达能力。

Method: 提出了 SELF-Transformer：一个迭代地细化其自身注意力权重到固定点的编码器层。

Result: 在编码器风格基准测试中，准确率提高了 20%。

Conclusion: Self-Transformers 通过在测试时进行输入自适应对齐，在不增加参数数量的情况下，提高了编码器风格基准测试的准确性，并恢复了迭代推理的大部分表达能力，同时保留了纯编码器架构的简单性。

Abstract: Transformers evaluated in a single, fixed-depth pass are provably limited in
expressive power to the constant-depth circuit class TC0. Running a Transformer
autoregressively removes that ceiling -- first in next-token prediction and,
more recently, in chain-of-thought reasoning. Both regimes rely on feedback
loops that decode internal states into tokens only to re-encode them in
subsequent steps. While this "thinking aloud" mirrors human reasoning,
biological brains iterate without externalising intermediate states as
language. To boost the expressive power of encoder Transformers without
resorting to token-level autoregression, we introduce the SELF-Transformer: an
encoder layer that iteratively refines its own attention weights to a fixed
point. Instead of producing -- in one pass -- the alignment matrix that remixes
the input sequence, the SELF-Transformer iteratively updates that matrix
internally, scaling test-time computation with input difficulty. This
adaptivity yields up to 20\% accuracy gains on encoder-style benchmarks without
increasing parameter count, demonstrating that input-adaptive alignment at test
time offers substantial benefits for only a modest extra compute budget.
Self-Transformers thus recover much of the expressive power of iterative
reasoning while preserving the simplicity of pure encoder architectures.

</details>


### [98] [Apple Intelligence Foundation Language Models: Tech Report 2025](https://arxiv.org/abs/2507.13575)
*Hanzhi Zhou,Erik Hornberger,Pengsheng Guo,Xiyou Zhou,Saiwen Wang,Xin Wang,Yifei He,Xuankai Chang,Rene Rauch,Louis D'hauwe,John Peebles,Alec Doane,Kohen Chia,Jenna Thibodeau,Zi-Yi Dou,Yuanyang Zhang,Ruoming Pang,Reed Li,Zhifeng Chen,Jeremy Warner,Zhaoyang Xu,Sophy Lee,David Mizrahi,Ramsey Tantawi,Chris Chaney,Kelsey Peterson,Jun Qin,Alex Dombrowski,Mira Chiang,Aiswarya Raghavan,Gerard Casamayor,Qibin Chen,Aonan Zhang,Nathalie Tran,Jianyu Wang,Hang Su,Thomas Voice,Alessandro Pappalardo,Brycen Wershing,Prasanth Yadla,Rui Li,Priyal Chhatrapati,Ismael Fernandez,Yusuf Goren,Xin Zheng,Forrest Huang,Tao Lei,Eray Yildiz,Alper Kokmen,Gokul Santhanam,Areeba Kamal,Kaan Elgin,Dian Ang Yap,Jeremy Liu,Peter Gray,Howard Xing,Kieran Liu,Matteo Ronchi,Moritz Schwarzer-Becker,Yun Zhu,Mandana Saebi,Jeremy Snow,David Griffiths,Guillaume Tartavel,Erin Feldman,Simon Lehnerer,Fernando Bermúdez-Medina,Hans Han,Joe Zhou,Xiaoyi Ren,Sujeeth Reddy,Zirui Wang,Tom Gunter,Albert Antony,Yuanzhi Li,John Dennison,Tony Sun,Yena Han,Yi Qin,Sam Davarnia,Jeffrey Bigham,Wayne Shan,Hannah Gillis Coleman,Guillaume Klein,Peng Liu,Muyang Yu,Jack Cackler,Yuan Gao,Crystal Xiao,Binazir Karimzadeh,Zhengdong Zhang,Felix Bai,Albin Madappally Jose,Feng Nan,Nazir Kamaldin,Dong Yin,Hans Hao,Yanchao Sun,Yi Hua,Charles Maalouf,Alex Guillen Garcia,Guoli Yin,Lezhi Li,Mohana Prasad Sathya Moorthy,Hongbin Gao,Jay Tang,Joanna Arreaza-Taylor,Faye Lao,Carina Peng,Josh Shaffer,Dan Masi,Sushma Rao,Tommi Vehvilainen,Senyu Tong,Dongcai Shen,Yang Zhao,Chris Bartels,Peter Fu,Qingqing Cao,Christopher Neubauer,Ethan Li,Mingfei Gao,Rebecca Callahan,Richard Wei,Patrick Dong,Alex Braunstein,Sachin Ravi,Adolfo Lopez Mendez,Kaiwei Huang,Kun Duan,Haoshuo Huang,Rui Qian,Stefano Ligas,Jordan Huffaker,Dongxu Li,Bailin Wang,Nanzhu Wang,Anuva Agarwal,Tait Madsen,Josh Newnham,Abhishek Sharma,Zhile Ren,Deepak Gopinath,Erik Daxberger,Saptarshi Guha,Oron Levy,Jing Lu,Nan Dun,Marc Kirchner,Yinfei Yang,Manjot Bilkhu,Dave Nelson,Anthony Spalvieri-Kruse,Juan Lao Tebar,Yang Xu,Phani Mutyala,Gabriel Jacoby-Cooper,Yingbo Wang,Karla Vega,Vishaal Mahtani,Darren Botten,Eric Wang,Hanli Li,Matthias Paulik,Haoran Yan,Navid Shiee,Yihao Qian,Bugu Wu,Qi Zhu,Ob Adaranijo,Bhuwan Dhingra,Zhe Gan,Nicholas Seidl,Grace Duanmu,Rong Situ,Yiping Ma,Yin Xia,David Riazati,Vasileios Saveris,Anh Nguyen,Michael,Lee,Patrick Sonnenberg,Chinguun Erdenebileg,Yanghao Li,Vivian Ma,James Chou,Isha Garg,Mark Lee,Keen You,Yuhong Li,Ransen Niu,Nandhitha Raghuram,Pulkit Agrawal,Henry Mason,Sumeet Singh,Keyu He,Hong-You Chen,Lucas Guibert,Shiyu Li,Varsha Paidi,Narendran Raghavan,Mingze Xu,Yuli Yang,Sergiu Sima,Irina Belousova,Sprite Chu,Afshin Dehghan,Philipp Dufter,David Haldimann,Zhen Yang,Margit Bowler,Chang Liu,Ying-Chang Cheng,Vivek Rathod,Syd Evans,Wilson Tsao,Dustin Withers,Haitian Sun,Biyao Wang,Peter Grasch,Walker Cheng,Yihao Feng,Vivek Kumar,Frank Chu,Victoria MönchJuan Haladjian,Doug Kang,Jiarui Lu,Ciro Sannino,Max Lam,Floris Weers,Bowen Pan,Kenneth Jung,Dhaval Doshi,Fangping Shi,Olli Saarikivi,Alp Aygar,Josh Elman,Cheng Leong,Eshan Verma,Matthew Lei,Jeff Nichols,Jiulong Shan,Donald Zhang,Lawrence Zhou,Stephen Murphy,Xianzhi Du,Chang Lan,Ankur Jain,Elmira Amirloo,Marcin Eichner,Naomy Sabo,Anupama Mann Anupama,David Qiu,Zhao Meng,Michael FitzMaurice,Peng Zhang,Simon Yeung,Chen Chen,Marco Zuliani,Andrew Hansen,Yang Lu,Brent Ramerth,Ziyi Zhong,Parsa Mazaheri,Matthew Hopkins,Mengyu Li,Simon Wang,David Chen,Farzin Rasteh,Chong Wang,Josh Gardner,Asaf Liberman,Haoxuan You,Andrew Walkingshaw,Xingyu Zhou,Jinhao Lei,Yan Meng,Quentin Keunebroek,Sam Wiseman,Anders Boesen Lindbo Larsen,Yi Zhang,Zaid Ahmed,Haiming Gang,Aaron Franklin,Kelvin Zou,Guillaume Seguin,Jonathan Janke,Rachel Burger,Co Giang,Cheng Shen,Jen Liu,Sanskruti Shah,Xiang Kong,Yiran Fei,TJ Collins,Chen Zhang,Zhiyun Lu,Michael Booker,Qin Ba,Yasutaka Tanaka,Andres Romero Mier Y Teran,Federico Scozzafava,Regan Poston,Jane Li,Eduardo Jimenez,Bas Straathof,Karanjeet Singh,Lindsay Hislop,Rajat Arora,Deepa Seshadri,Boyue Li,Colorado Reed,Zhen Li,TJ Lu,Yi Wang,Kaelen Haag,Nicholas Lusskin,Raunak Sinha,Rahul Nair,Eldon Schoop,Mary Beth Kery,Mehrdad Farajtbar,Brenda Yang,George Horrell,Shiwen Zhao,Dhruti Shah,Cha Chen,Bowen Zhang,Chang Gao,Devi Krishna,Jennifer Mallalieu,Javier Movellan,Di Feng,Emily Zhang,Sam Xu,Junting Pan,Dominik Moritz,Suma Jayaram,Kevin Smith,Dongseong Hwang,Daniel Parilla,Jiaming Hu,You-Cyuan Jhang,Emad Soroush,Fred Hohman,Nan Du,Emma Wang,Sam Dodge,Pragnya Sridhar,Joris Pelemans,Wei Fang,Nina Wenzel,Joseph Yitan Cheng,Hadas Kotek,Chung-Cheng Chiu,Meng Cao,Haijing Fu,Ruixuan Hou,Ke Ye,Diane Zhu,Nikhil Bhendawade,Joseph Astrauskas,Jian Liu,Sai Aitharaju,Wentao Wu,Artsiom Peshko,Hyunjik Kim,Nilesh Shahdadpuri,Andy De Wang,Qi Shan,Piotr Maj,Raul Rea Menacho,Justin Lazarow,Eric Liang Yang,Arsalan Farooq,Donghan Yu,David Güera,Minsik Cho,Kavya Nerella,Yongqiang Wang,Tao Jia,John Park,Jeff Lai,Haotian Zhang,Futang Peng,Daniele Molinari,Aparna Rajamani,Tyler Johnson,Lauren Gardiner,Chao Jia,Violet Yao,Wojciech Kryscinski,Xiujun Li,Shang-Chen Wu*

Main category: cs.LG

TL;DR: Apple introduces two new multilingual, multimodal foundation language models for Apple Intelligence: an on-device model and a scalable server model. Both models are trained on large-scale datasets and refined with supervised fine-tuning and reinforcement learning. The models support multiple languages, understand images, and execute tool calls. A Swift framework is provided for developers to integrate these capabilities.


<details>
  <summary>Details</summary>
Motivation: Introduce two multilingual, multimodal foundation language models that power Apple Intelligence features across Apple devices and services

Method: i a 3B-parameter on-device model optimized for Apple silicon through architectural innovations such as KV-cache sharing and 2-bit quantization-aware training; and ii a scalable server model built on a novel Parallel-Track Mixture-of-Experts PT-MoE transformer that combines track parallelism, mixture-of-experts sparse computation, and interleaved global-local attention

Result: The resulting models support several additional languages while understanding images and executing tool calls. A new Swift-centric Foundation Models framework exposes guided generation, constrained tool calling, and LoRA adapter fine-tuning, allowing developers to integrate these capabilities with a few lines of code.

Conclusion: Both the server model and the on-device model match or surpass comparably sized open baselines.The latest advancements in Apple Intelligence models are grounded in our Responsible AI approach with safeguards like content filtering and locale-specific evaluation, as well as our commitment to protecting our users' privacy with innovations like Private Cloud Compute.

Abstract: We introduce two multilingual, multimodal foundation language models that
power Apple Intelligence features across Apple devices and services: i a
3B-parameter on-device model optimized for Apple silicon through architectural
innovations such as KV-cache sharing and 2-bit quantization-aware training; and
ii a scalable server model built on a novel Parallel-Track Mixture-of-Experts
PT-MoE transformer that combines track parallelism, mixture-of-experts sparse
computation, and interleaved global-local attention to deliver high quality
with competitive cost on Apple's Private Cloud Compute platform. Both models
are trained on large-scale multilingual and multimodal datasets sourced via
responsible web crawling, licensed corpora, and high-quality synthetic data,
then further refined with supervised fine-tuning and reinforcement learning on
a new asynchronous platform. The resulting models support several additional
languages while understanding images and executing tool calls. In public
benchmarks and human evaluations, both the server model and the on-device model
match or surpass comparably sized open baselines.
  A new Swift-centric Foundation Models framework exposes guided generation,
constrained tool calling, and LoRA adapter fine-tuning, allowing developers to
integrate these capabilities with a few lines of code. The latest advancements
in Apple Intelligence models are grounded in our Responsible AI approach with
safeguards like content filtering and locale-specific evaluation, as well as
our commitment to protecting our users' privacy with innovations like Private
Cloud Compute.

</details>


### [99] [Learning Pluralistic User Preferences through Reinforcement Learning Fine-tuned Summaries](https://arxiv.org/abs/2507.13579)
*Hyunji Nam,Yanming Wan,Mickel Liu,Jianxun Lian,Natasha Jaques*

Main category: cs.LG

TL;DR: The paper introduces PLUS, a framework that learns text-based summaries of user preferences to personalize LLM responses, outperforming existing methods and allowing for user control and transparency.


<details>
  <summary>Details</summary>
Motivation: It is becoming increasingly important to personalize responses of large language model (LLM) AI assistants to align to different users' preferences and goals. Reinforcement learning from human feedback (RLHF) does not account for variability across users, as it models the entire user population with a single reward model.

Method: The paper presents a novel framework, Preference Learning Using Summarization (PLUS), that learns text-based summaries of each user's preferences, characteristics, and past conversations. The user-summarization model is trained with reinforcement learning, and the reward model is updated simultaneously, creating an online co-adaptation loop.

Result: The paper shows that PLUS captures meaningful aspects of a user's preferences, is robust to new users and diverse conversation topics, and can be transferred for zero-shot personalization of stronger, proprietary models like GPT-4.

Conclusion: The paper demonstrates that textual summaries generated about users can be transferred for zero-shot personalization of stronger models like GPT-4, and these summaries are concise, portable, easy to interpret and modify, allowing for more transparency and user control in LLM alignment.

Abstract: As everyday use cases of large language model (LLM) AI assistants have
expanded, it is becoming increasingly important to personalize responses to
align to different users' preferences and goals. While reinforcement learning
from human feedback (RLHF) is effective at improving LLMs to be generally more
helpful and fluent, it does not account for variability across users, as it
models the entire user population with a single reward model. We present a
novel framework, Preference Learning Using Summarization (PLUS), that learns
text-based summaries of each user's preferences, characteristics, and past
conversations. These summaries condition the reward model, enabling it to make
personalized predictions about the types of responses valued by each user. We
train the user-summarization model with reinforcement learning, and update the
reward model simultaneously, creating an online co-adaptation loop. We show
that in contrast with prior personalized RLHF techniques or with in-context
learning of user information, summaries produced by PLUS capture meaningful
aspects of a user's preferences. Across different pluralistic user datasets, we
show that our method is robust to new users and diverse conversation topics.
Additionally, we demonstrate that the textual summaries generated about users
can be transferred for zero-shot personalization of stronger, proprietary
models like GPT-4. The resulting user summaries are not only concise and
portable, they are easy for users to interpret and modify, allowing for more
transparency and user control in LLM alignment.

</details>


### [100] [Tri-Learn Graph Fusion Network for Attributed Graph Clustering](https://arxiv.org/abs/2507.13620)
*Binxiong Li,Yuefei Wang,Xu Xiang,Xue Li,Binyu Zhao,Heyang Gao,Qinyu Zhao,Xi Yu*

Main category: cs.LG

TL;DR: 提出了Tri-GFN模型，通过融合GCN、AE和Graph Transformer，利用三学习机制和特征融合策略，在图聚类任务上取得了state-of-the-art的结果。


<details>
  <summary>Details</summary>
Motivation: 基于图卷积网络(GCN)的模型在图数据分析领域取得了显著进展。然而，在处理大规模和复杂图数据集时，仍然存在过度平滑和过度压缩等挑战，导致聚类质量下降。虽然Graph Transformer架构缓解了其中一些问题，但在处理异构图数据时，其性能仍然有限。

Method: 提出了一种新的深度聚类框架，包含GCN、Autoencoder (AE)和Graph Transformer，称为Tri-Learn Graph Fusion Network (Tri-GFN)。

Result: 在ACM数据集上，准确率提高了约0.87%；在路透社数据集上，准确率提高了14.14%；在USPS数据集上，准确率提高了7.58%。

Conclusion: Tri-GFN在ACM、Reuters和USPS数据集上表现出色，尤其在Reuters数据集上的突出性能使其可应用于自动新闻分类、主题检索和相关领域。

Abstract: In recent years, models based on Graph Convolutional Networks (GCN) have made
significant strides in the field of graph data analysis. However, challenges
such as over-smoothing and over-compression remain when handling large-scale
and complex graph datasets, leading to a decline in clustering quality.
Although the Graph Transformer architecture has mitigated some of these issues,
its performance is still limited when processing heterogeneous graph data. To
address these challenges, this study proposes a novel deep clustering framework
that comprising GCN, Autoencoder (AE), and Graph Transformer, termed the
Tri-Learn Graph Fusion Network (Tri-GFN). This framework enhances the
differentiation and consistency of global and local information through a
unique tri-learning mechanism and feature fusion enhancement strategy. The
framework integrates GCN, AE, and Graph Transformer modules. These components
are meticulously fused by a triple-channel enhancement module, which maximizes
the use of both node attributes and topological structures, ensuring robust
clustering representation. The tri-learning mechanism allows mutual learning
among these modules, while the feature fusion strategy enables the model to
capture complex relationships, yielding highly discriminative representations
for graph clustering. It surpasses many state-of-the-art methods, achieving an
accuracy improvement of approximately 0.87% on the ACM dataset, 14.14 % on the
Reuters dataset, and 7.58 % on the USPS dataset. Due to its outstanding
performance on the Reuters dataset, Tri-GFN can be applied to automatic news
classification, topic retrieval, and related fields.

</details>


### [101] [FedSkipTwin: Digital-Twin-Guided Client Skipping for Communication-Efficient Federated Learning](https://arxiv.org/abs/2507.13624)
*Daniel Commey,Kamel Abbad,Garth V. Crosby,Lyes Khoukhi*

Main category: cs.LG

TL;DR: FedSkipTwin, a client-skipping algorithm, reduces communication overhead and improves accuracy in federated learning by using server-side digital twins to predict client updates.


<details>
  <summary>Details</summary>
Motivation: Communication overhead remains a primary bottleneck in federated learning (FL), particularly for applications involving mobile and IoT devices with constrained bandwidth.

Method: a novel client-skipping algorithm driven by lightweight, server-side digital twins.

Result: FedSkipTwin reduces total communication by 12-15.5% across 20 rounds while simultaneously improving final model accuracy by up to 0.5 percentage points compared to the standard FedAvg algorithm.

Conclusion:  prediction-guided skipping is a practical and effective strategy for resource-aware FL in bandwidth-constrained edge environments.

Abstract: Communication overhead remains a primary bottleneck in federated learning
(FL), particularly for applications involving mobile and IoT devices with
constrained bandwidth. This work introduces FedSkipTwin, a novel
client-skipping algorithm driven by lightweight, server-side digital twins.
Each twin, implemented as a simple LSTM, observes a client's historical
sequence of gradient norms to forecast both the magnitude and the epistemic
uncertainty of its next update. The server leverages these predictions,
requesting communication only when either value exceeds a predefined threshold;
otherwise, it instructs the client to skip the round, thereby saving bandwidth.
Experiments are conducted on the UCI-HAR and MNIST datasets with 10 clients
under a non-IID data distribution. The results demonstrate that FedSkipTwin
reduces total communication by 12-15.5% across 20 rounds while simultaneously
improving final model accuracy by up to 0.5 percentage points compared to the
standard FedAvg algorithm. These findings establish that prediction-guided
skipping is a practical and effective strategy for resource-aware FL in
bandwidth-constrained edge environments.

</details>


### [102] [A Comprehensive Review of Transformer-based language models for Protein Sequence Analysis and Design](https://arxiv.org/abs/2507.13646)
*Nimisha Ghosh,Daniele Santoni,Debaleena Nawn,Eleonora Ottaviani,Giovanni Felici*

Main category: cs.LG

TL;DR: This paper reviews recent advances in Transformer-based models for protein sequence analysis and design, covering applications, strengths, weaknesses, and future directions.


<details>
  <summary>Details</summary>
Motivation: The success of Transformer-based language models in NLP has led to their adoption in bioinformatics, prompting this review of recent advances.

Method: Review and analysis of a significant number of works pertaining to applications of Transformer-based models in protein sequence analysis and design.

Result: Discussion and analysis of applications like gene ontology, functional/structural protein identification, de novo protein generation, and protein binding, along with strengths and weaknesses of existing works. Identification of shortcomings and potential future developments.

Conclusion: This review helps researchers understand the current state-of-the-art and orient future studies in Transformer-based models for protein sequence analysis and design.

Abstract: The impact of Transformer-based language models has been unprecedented in
Natural Language Processing (NLP). The success of such models has also led to
their adoption in other fields including bioinformatics. Taking this into
account, this paper discusses recent advances in Transformer-based models for
protein sequence analysis and design. In this review, we have discussed and
analysed a significant number of works pertaining to such applications. These
applications encompass gene ontology, functional and structural protein
identification, generation of de novo proteins and binding of proteins. We
attempt to shed light on the strength and weaknesses of the discussed works to
provide a comprehensive insight to readers. Finally, we highlight shortcomings
in existing research and explore potential avenues for future developments. We
believe that this review will help researchers working in this field to have an
overall idea of the state of the art in this field, and to orient their future
studies.

</details>


### [103] [Kolmogorov-Arnold Networks-based GRU and LSTM for Loan Default Early Prediction](https://arxiv.org/abs/2507.13685)
*Yue Yang,Zihan Su,Ying Zhang,Chang Chuan Goh,Yuxiang Lin,Anthony Graham Bellotti,Boon Giin Lee*

Main category: cs.LG

TL;DR: This paper introduces GRU-KAN and LSTM-KAN models for early loan default prediction, achieving over 92% accuracy three months in advance, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack accuracy in early predictions and depend on training and testing within the same year and specific time frames, limiting their practical use with out-of-time data.

Method: The study introduces two innovative architectures, GRU-KAN and LSTM-KAN, which merge Kolmogorov-Arnold Networks (KAN) with Gated Recurrent Units (GRU) and Long Short-Term Memory (LSTM) networks.

Result: The proposed model achieves a prediction accuracy of over 92% three months in advance and over 88% eight months in advance, significantly outperforming existing baselines.

Conclusion: The GRU-KAN and LSTM-KAN models significantly outperform existing baselines in predicting loan defaults, achieving high accuracy even 8 months in advance.

Abstract: This study addresses a critical challenge in time series anomaly detection:
enhancing the predictive capability of loan default models more than three
months in advance to enable early identification of default events, helping
financial institutions implement preventive measures before risk events
materialize. Existing methods have significant drawbacks, such as their lack of
accuracy in early predictions and their dependence on training and testing
within the same year and specific time frames. These issues limit their
practical use, particularly with out-of-time data. To address these, the study
introduces two innovative architectures, GRU-KAN and LSTM-KAN, which merge
Kolmogorov-Arnold Networks (KAN) with Gated Recurrent Units (GRU) and Long
Short-Term Memory (LSTM) networks. The proposed models were evaluated against
the baseline models (LSTM, GRU, LSTM-Attention, and LSTM-Transformer) in terms
of accuracy, precision, recall, F1 and AUC in different lengths of feature
window, sample sizes, and early prediction intervals. The results demonstrate
that the proposed model achieves a prediction accuracy of over 92% three months
in advance and over 88% eight months in advance, significantly outperforming
existing baselines.

</details>


### [104] [Binarizing Physics-Inspired GNNs for Combinatorial Optimization](https://arxiv.org/abs/2507.13703)
*Martin Krutský,Gustav Šír,Vyacheslav Kungurtsev,Georgios Korpas*

Main category: cs.LG

TL;DR: 本文发现 PI-GNNs 在密集图上性能下降，并提出了改进方法，实验验证了改进方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 虽然 PI-GNNs 框架在各种组合问题中都取得了有希望的结果，但本文表明，PI-GNNs 的性能随着组合问题图密度的增加而系统性地下降。分析揭示了 PI-GNNs 训练动态中一个有趣的相变，这与更密集问题的退化解相关，突出了松弛的实值模型输出与二值问题解之间的差异。

Method: 本文通过构建模糊逻辑和二值化神经网络，提出了 принципиальные 替代策略，以改进 PI-GNNs。

Result: 实验表明，所提出的方法组合显著提高了 PI-GNNs 在日益密集的环境中的性能。

Conclusion: 本文提出了一系列基于模糊逻辑和二值化神经网络的原则性替代方案，以解决PI-GNNs在密集图上的性能下降问题，并通过实验验证了这些方法能显著提升PI-GNNs在日益密集环境下的性能。

Abstract: Physics-inspired graph neural networks (PI-GNNs) have been utilized as an
efficient unsupervised framework for relaxing combinatorial optimization
problems encoded through a specific graph structure and loss, reflecting
dependencies between the problem's variables. While the framework has yielded
promising results in various combinatorial problems, we show that the
performance of PI-GNNs systematically plummets with an increasing density of
the combinatorial problem graphs. Our analysis reveals an interesting phase
transition in the PI-GNNs' training dynamics, associated with degenerate
solutions for the denser problems, highlighting a discrepancy between the
relaxed, real-valued model outputs and the binary-valued problem solutions. To
address the discrepancy, we propose principled alternatives to the naive
strategy used in PI-GNNs by building on insights from fuzzy logic and binarized
neural networks. Our experiments demonstrate that the portfolio of proposed
methods significantly improves the performance of PI-GNNs in increasingly dense
settings.

</details>


### [105] [Generalist Bimanual Manipulation via Foundation Video Diffusion Models](https://arxiv.org/abs/2507.12898)
*Yao Feng,Hengkai Tan,Xinyi Mao,Guodong Liu,Shuhe Huang,Chendong Xiang,Hang Su,Jun Zhu*

Main category: cs.LG

TL;DR: VIDAR：一种用于动作推理的视频扩散方法，它利用大规模视频预训练和掩蔽逆动力学模型来实现可扩展和通用的机器人操作。


<details>
  <summary>Details</summary>
Motivation: 双臂机器人操作是解决具有挑战性任务的基础。尽管通用操作最近取得了进展，但数据稀缺和具体化异质性仍然是在双臂环境中进一步扩展的严重障碍。

Method: VIDAR，一个两阶段框架，利用基于扩散的大规模视频预训练和一个用于动作预测的新型掩蔽逆动力学模型。

Result: VIDAR推广到未见过的任务和背景，具有强大的语义理解能力，超过了最先进的方法。

Conclusion: VIDAR在未见过的机器人平台上仅用20分钟的人工演示（仅为典型数据需求的1%）即可推广到未见过的任务和背景，具有强大的语义理解能力，超过了最先进的方法。研究结果强调了视频基础模型与掩蔽动作预测相结合的潜力，从而可以在各种现实环境中实现可扩展和通用的机器人操作。

Abstract: Bimanual robotic manipulation, which involves the coordinated control of two
robotic arms, is foundational for solving challenging tasks. Despite recent
progress in general-purpose manipulation, data scarcity and embodiment
heterogeneity remain serious obstacles to further scaling up in bimanual
settings. In this paper, we introduce VIdeo Diffusion for Action Reasoning
(VIDAR), a two-stage framework that leverages large-scale, diffusion-based
video pre-training and a novel masked inverse dynamics model for action
prediction. We pre-train the video diffusion model on 750K multi-view videos
from three real-world bimanual robot platforms, utilizing a unified observation
space that encodes robot, camera, task, and scene contexts. Our masked inverse
dynamics model learns masks to extract action-relevant information from
generated trajectories without requiring pixel-level labels, and the masks can
effectively generalize to unseen backgrounds. Our experiments demonstrate that
with only 20 minutes of human demonstrations on an unseen robot platform (only
1% of typical data requirements), VIDAR generalizes to unseen tasks and
backgrounds with strong semantic understanding, surpassing state-of-the-art
methods. Our findings highlight the potential of video foundation models,
coupled with masked action prediction, to enable scalable and generalizable
robotic manipulation in diverse real-world settings.

</details>


### [106] [Bayesian Optimization for Molecules Should Be Pareto-Aware](https://arxiv.org/abs/2507.13704)
*Anabel Yong,Austin Tripp,Layla Hosseini-Gerami,Brooks Paige*

Main category: cs.LG

TL;DR: MOBO (EHVI) outperforms scalarized EI in molecular optimization, especially when data is limited.


<details>
  <summary>Details</summary>
Motivation: The empirical advantages of multi-objective Bayesian optimization (MOBO) over scalarized alternatives in molecular design remain underexplored.

Method: Benchmarked Expected Hypervolume Improvement (EHVI) against fixed-weight scalarized Expected Improvement (EI) using identical Gaussian Process surrogates and molecular representations.

Result: EHVI consistently outperforms scalarized EI in terms of Pareto front coverage, convergence speed, and chemical diversity across three molecular optimization tasks.

Conclusion: Pareto-aware acquisition functions like EHVI outperform scalarized approaches in de novo molecular optimization, especially in low-data regimes with nontrivial trade-offs.

Abstract: Multi-objective Bayesian optimization (MOBO) provides a principled framework
for navigating trade-offs in molecular design. However, its empirical
advantages over scalarized alternatives remain underexplored. We benchmark a
simple Pareto-based MOBO strategy -- Expected Hypervolume Improvement (EHVI) --
against a simple fixed-weight scalarized baseline using Expected Improvement
(EI), under a tightly controlled setup with identical Gaussian Process
surrogates and molecular representations. Across three molecular optimization
tasks, EHVI consistently outperforms scalarized EI in terms of Pareto front
coverage, convergence speed, and chemical diversity. While scalarization
encompasses flexible variants -- including random or adaptive schemes -- our
results show that even strong deterministic instantiations can underperform in
low-data regimes. These findings offer concrete evidence for the practical
advantages of Pareto-aware acquisition in de novo molecular optimization,
especially when evaluation budgets are limited and trade-offs are nontrivial.

</details>


### [107] [Learning Deformable Body Interactions With Adaptive Spatial Tokenization](https://arxiv.org/abs/2507.13707)
*Hao Wang,Yu Liu,Daniel Biggs,Haoru Wang,Jiandong Yu,Ping Huang*

Main category: cs.LG

TL;DR: This paper introduces an Adaptive Spatial Tokenization (AST) method to improve the scalability of GNNs for simulating deformable body interactions. It uses a grid-based approach and attention mechanisms to efficiently model large-scale meshes, outperforming existing methods and providing a new dataset for future research.


<details>
  <summary>Details</summary>
Motivation: Learning-based methods with Graph Neural Networks (GNNs) encounter scalability issues when modeling deformable body interactions, especially with large-scale meshes, due to the computational intensity of creating pairwise global edges dynamically.

Method: The paper proposes an Adaptive Spatial Tokenization (AST) method that divides the simulation space into a grid of cells and maps unstructured meshes onto this structured grid. A cross-attention module maps sparse cells into a compact, fixed-length embedding, and self-attention modules predict the next state over these tokens in latent space.

Result: The proposed method achieves accurate and scalable simulation results and outperforms state-of-the-art approaches. It remains effective on large-scale simulations with meshes exceeding 100,000 nodes. The authors also contribute a novel large-scale dataset.

Conclusion: The proposed method significantly outperforms state-of-the-art approaches in modeling deformable body interactions and remains effective on large-scale simulations with meshes exceeding 100,000 nodes.

Abstract: Simulating interactions between deformable bodies is vital in fields like
material science, mechanical design, and robotics. While learning-based methods
with Graph Neural Networks (GNNs) are effective at solving complex physical
systems, they encounter scalability issues when modeling deformable body
interactions. To model interactions between objects, pairwise global edges have
to be created dynamically, which is computationally intensive and impractical
for large-scale meshes. To overcome these challenges, drawing on insights from
geometric representations, we propose an Adaptive Spatial Tokenization (AST)
method for efficient representation of physical states. By dividing the
simulation space into a grid of cells and mapping unstructured meshes onto this
structured grid, our approach naturally groups adjacent mesh nodes. We then
apply a cross-attention module to map the sparse cells into a compact,
fixed-length embedding, serving as tokens for the entire physical state.
Self-attention modules are employed to predict the next state over these tokens
in latent space. This framework leverages the efficiency of tokenization and
the expressive power of attention mechanisms to achieve accurate and scalable
simulation results. Extensive experiments demonstrate that our method
significantly outperforms state-of-the-art approaches in modeling deformable
body interactions. Notably, it remains effective on large-scale simulations
with meshes exceeding 100,000 nodes, where existing methods are hindered by
computational limitations. Additionally, we contribute a novel large-scale
dataset encompassing a wide range of deformable body interactions to support
future research in this area.

</details>
