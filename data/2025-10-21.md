<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 17]
- [cs.CV](#cs.CV) [Total: 16]
- [cs.AI](#cs.AI) [Total: 18]
- [cs.DB](#cs.DB) [Total: 7]
- [cs.IR](#cs.IR) [Total: 11]
- [cs.LG](#cs.LG) [Total: 18]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Quantum NLP models on Natural Language Inference](https://arxiv.org/abs/2510.15972)
*Ling Sun,Peter Sullivan,Michael Martin,Yun Zhou*

Main category: cs.CL

TL;DR: 本文探讨了量子自然语言处理（QNLP）模型在自然语言推理（NLI）任务中的应用，并比较了量子、混合和基于经典Transformer的模型在受限的少样本设置下的性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索QNLP在语义建模方面的潜力，特别是在资源受限的情况下。

Method: 该研究使用lambeq库和DisCoCat框架构建参数化的量子电路，用于句子对的语义相关性和推理分类。此外，还提出了一种新颖的基于聚类的架构，以提高泛化能力。

Result: 结果表明，量子模型在参数数量显著减少的情况下，性能与经典基线相当。量子模型在推理方面优于随机初始化的Transformer，并在相关性任务上实现了较低的测试误差。量子模型还表现出更高的单参数学习效率。

Conclusion: 研究结论是QNLP在低资源、结构敏感的环境中具有潜力，并强调了其更高的单参数学习效率。提出的基于聚类的架构通过将门参数与学习的词簇联系起来，改善了泛化能力。

Abstract: Quantum natural language processing (QNLP) offers a novel approach to
semantic modeling by embedding compositional structure directly into quantum
circuits. This paper investigates the application of QNLP models to the task of
Natural Language Inference (NLI), comparing quantum, hybrid, and classical
transformer-based models under a constrained few-shot setting. Using the lambeq
library and the DisCoCat framework, we construct parameterized quantum circuits
for sentence pairs and train them for both semantic relatedness and inference
classification. To assess efficiency, we introduce a novel
information-theoretic metric, Information Gain per Parameter (IGPP), which
quantifies learning dynamics independent of model size. Our results demonstrate
that quantum models achieve performance comparable to classical baselines while
operating with dramatically fewer parameters. The Quantum-based models
outperform randomly initialized transformers in inference and achieve lower
test error on relatedness tasks. Moreover, quantum models exhibit significantly
higher per-parameter learning efficiency (up to five orders of magnitude more
than classical counterparts), highlighting the promise of QNLP in low-resource,
structure-sensitive settings. To address circuit-level isolation and promote
parameter sharing, we also propose a novel cluster-based architecture that
improves generalization by tying gate parameters to learned word clusters
rather than individual tokens.

</details>


### [2] [Fusion-Augmented Large Language Models: Boosting Diagnostic Trustworthiness via Model Consensus](https://arxiv.org/abs/2510.16057)
*Md Kamrul Siam,Md Jobair Hossain Faruk,Jerry Q. Cheng,Huanying Gu*

Main category: cs.CL

TL;DR: 本研究提出了一种新颖的多模型融合框架，利用 ChatGPT 和 Claude 两种先进的大型语言模型 (LLM) 来提高 CheXpert 数据集上胸部 X 光片判读的可靠性。


<details>
  <summary>Details</summary>
Motivation: 利用大型语言模型来提高胸部X光片判读的可靠性，减少诊断错误。

Method: 使用图像提示评估 ChatGPT 和 Claude 的单模态性能，并采用基于相似性的共识方法。通过配对图像和合成文本，评估多模态输入的影响。

Result: ChatGPT 和 Claude 在单模态设置下的诊断准确率分别为 62.8% 和 76.9%。在多模态队列中，ChatGPT 的性能提高到 84%，Claude 的性能提高到 76%，共识准确率达到 91.3%。

Conclusion: 整合互补模态和使用输出级共识可以提高 AI 辅助放射诊断的可信度和临床实用性，并以最小的计算开销提供减少诊断错误的实用途径。

Abstract: This study presents a novel multi-model fusion framework leveraging two
state-of-the-art large language models (LLMs), ChatGPT and Claude, to enhance
the reliability of chest X-ray interpretation on the CheXpert dataset. From the
full CheXpert corpus of 224,316 chest radiographs, we randomly selected 234
radiologist-annotated studies to evaluate unimodal performance using image-only
prompts. In this setting, ChatGPT and Claude achieved diagnostic accuracies of
62.8% and 76.9%, respectively. A similarity-based consensus approach, using a
95% output similarity threshold, improved accuracy to 77.6%. To assess the
impact of multimodal inputs, we then generated synthetic clinical notes
following the MIMIC-CXR template and evaluated a separate subset of 50 randomly
selected cases paired with both images and synthetic text. On this multimodal
cohort, performance improved to 84% for ChatGPT and 76% for Claude, while
consensus accuracy reached 91.3%. Across both experimental conditions,
agreement-based fusion consistently outperformed individual models. These
findings highlight the utility of integrating complementary modalities and
using output-level consensus to improve the trustworthiness and clinical
utility of AI-assisted radiological diagnosis, offering a practical path to
reduce diagnostic errors with minimal computational overhead.

</details>


### [3] [Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs](https://arxiv.org/abs/2510.16062)
*Guiyao Tie,Zenghui Yuan,Zeli Zhao,Chaoran Hu,Tianhe Gu,Ruihang Zhang,Sizhe Zhang,Junran Wu,Xiaoyue Tu,Ming Jin,Qingsong Wen,Lixing Chen,Pan Zhou,Lichao Sun*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLM）的自纠正能力，并提出了一个名为CorrectBench的基准来评估不同自纠正策略的有效性。


<details>
  <summary>Details</summary>
Motivation: 评估现有自纠正方法，并探究LLM是否能真正纠正自身错误。

Method: 通过CorrectBench基准，评估内在、外在和微调等自纠正策略在常识推理、数学推理和代码生成三个任务上的表现。

Result: 自纠正方法可以提高准确性，尤其是在复杂推理任务中；混合不同的自纠正策略可以进一步提高性能，但会降低效率；推理LLM在额外的自纠正方法下优化有限，且时间成本高；简单的CoT基线表现出具有竞争力的准确性和效率。

Conclusion: 自纠正具有增强LLM推理性能的潜力，但提高效率仍然是一个挑战。未来的研究应侧重于优化推理能力和运营效率之间的平衡。

Abstract: Self-correction of large language models (LLMs) emerges as a critical
component for enhancing their reasoning performance. Although various
self-correction methods have been proposed, a comprehensive evaluation of these
methods remains largely unexplored, and the question of whether LLMs can truly
correct themselves is a matter of significant interest and concern. In this
study, we introduce CorrectBench, a benchmark developed to evaluate the
effectiveness of self-correction strategies, including intrinsic, external, and
fine-tuned approaches, across three tasks: commonsense reasoning, mathematical
reasoning, and code generation. Our findings reveal that: 1) Self-correction
methods can improve accuracy, especially for complex reasoning tasks; 2) Mixing
different self-correction strategies yields further improvements, though it
reduces efficiency; 3) Reasoning LLMs (e.g., DeepSeek-R1) have limited
optimization under additional self-correction methods and have high time costs.
Interestingly, a comparatively simple chain-of-thought (CoT) baseline
demonstrates competitive accuracy and efficiency. These results underscore the
potential of self-correction to enhance LLM's reasoning performance while
highlighting the ongoing challenge of improving their efficiency. Consequently,
we advocate for further research focused on optimizing the balance between
reasoning capabilities and operational efficiency. Project Page:
https://correctbench.github.io/

</details>


### [4] [EvolveR: Self-Evolving LLM Agents through an Experience-Driven Lifecycle](https://arxiv.org/abs/2510.16079)
*Rong Wu,Xiaoman Wang,Jianbiao Mei,Pinlong Cai,Daocheng Fu,Cheng Yang,Licheng Wen,Xuemeng Yang,Yufan Shen,Yuxin Wang,Botian Shi*

Main category: cs.CL

TL;DR: EvolveR框架通过离线自我提炼和在线互动，使LLM Agent能够从自身经验中系统地学习和改进问题解决策略。


<details>
  <summary>Details</summary>
Motivation: 现有LLM Agent在工具使用上表现出色，但缺乏从自身经验中系统学习的关键能力，无法迭代优化解决问题的策略。

Method: 引入EvolveR框架，包含离线自我提炼（将交互轨迹合成为可重用的策略原则）和在线互动（检索提炼的原则来指导决策）两个阶段，并通过策略强化机制迭代更新Agent。

Result: 在复杂的多跳问答基准测试中，EvolveR优于强大的Agent基线。

Conclusion: EvolveR为Agent提供了一个全面的蓝图，使其不仅可以从外部数据中学习，还可以从自身行为的后果中学习，为更自主和持续改进的系统铺平了道路。

Abstract: Current Large Language Model (LLM) agents show strong performance in tool
use, but lack the crucial capability to systematically learn from their own
experiences. While existing frameworks mainly focus on mitigating external
knowledge gaps, they fail to address a more fundamental limitation: the
inability to iteratively refine problem-solving strategies. In this work, we
introduce EvolveR, a framework designed to enable agent to self-improve through
a complete, closed-loop experience lifecycle. This lifecycle comprises two key
stages: (1) Offline Self-Distillation, where the agent's interaction
trajectories are synthesized into a structured repository of abstract, reusable
strategic principles; (2) Online Interaction, where the agent interacts with
tasks and actively retrieves distilled principles to guide its decision-making,
accumulating a diverse set of behavioral trajectories. This loop employs a
policy reinforcement mechanism to iteratively update the agent based on its
performance. We demonstrate the effectiveness of EvolveR on complex multi-hop
question-answering benchmarks, where it achieves superior performance over
strong agentic baselines. Our work presents a comprehensive blueprint for
agents that learn not only from external data but also from the consequences of
their own actions, paving the way for more autonomous and continuously
improving systems. Code is available at https://github.com/Edaizi/EvolveR.

</details>


### [5] [Evaluating Prompting Strategies and Large Language Models in Systematic Literature Review Screening: Relevance and Task-Stage Classification](https://arxiv.org/abs/2510.16091)
*Binglan Han,Anuradha Mathrani,Teo Susnjak*

Main category: cs.CL

TL;DR: 本研究量化了提示策略与大型语言模型（LLM）之间的相互作用，以实现系统文献综述（SLR）筛选阶段的自动化。


<details>
  <summary>Details</summary>
Motivation: 旨在评估不同LLM和提示策略在文献筛选任务中的性能，并为实际应用提供指导。

Method: 研究评估了六个LLM在五种提示类型下的表现，使用了准确率、精确率、召回率和F1等指标。

Result: CoT-few-shot提示策略在精确率和召回率之间取得了最可靠的平衡；zero-shot提示策略最大化了召回率；self-reflection提示策略表现不佳。GPT-4o和DeepSeek表现稳健，GPT-4o-mini在较低成本下表现出竞争力。

Conclusion: 研究结果表明LLM在文献筛选自动化方面具有不均衡但有前景的潜力，并为任务自适应的LLM部署提供了比较基准和实践指导。

Abstract: This study quantifies how prompting strategies interact with large language
models (LLMs) to automate the screening stage of systematic literature reviews
(SLRs). We evaluate six LLMs (GPT-4o, GPT-4o-mini, DeepSeek-Chat-V3,
Gemini-2.5-Flash, Claude-3.5-Haiku, Llama-4-Maverick) under five prompt types
(zero-shot, few-shot, chain-of-thought (CoT), CoT-few-shot, self-reflection)
across relevance classification and six Level-2 tasks, using accuracy,
precision, recall, and F1. Results show pronounced model-prompt interaction
effects: CoT-few-shot yields the most reliable precision-recall balance;
zero-shot maximizes recall for high-sensitivity passes; and self-reflection
underperforms due to over-inclusivity and instability across models. GPT-4o and
DeepSeek provide robust overall performance, while GPT-4o-mini performs
competitively at a substantially lower dollar cost. A cost-performance analysis
for relevance classification (per 1,000 abstracts) reveals large absolute
differences among model-prompt pairings; GPT-4o-mini remains low-cost across
prompts, and structured prompts (CoT/CoT-few-shot) on GPT-4o-mini offer
attractive F1 at a small incremental cost. We recommend a staged workflow that
(1) deploys low-cost models with structured prompts for first-pass screening
and (2) escalates only borderline cases to higher-capacity models. These
findings highlight LLMs' uneven but promising potential to automate literature
screening. By systematically analyzing prompt-model interactions, we provide a
comparative benchmark and practical guidance for task-adaptive LLM deployment.

</details>


### [6] [Facts in Stats: Impacts of Pretraining Diversity on Language Model Generalization](https://arxiv.org/abs/2510.16096)
*Tina Behnia,Puneesh Deora,Christos Thrampoulidis*

Main category: cs.CL

TL;DR: 这篇论文介绍了一个灵活的合成测试平台，用于研究语言模型中统计规律和事实关联之间的相互作用如何影响泛化能力。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对语言模型中统计规律和事实关联之间相互作用的系统分析，而这种相互作用会影响模型的泛化能力。

Method: 该论文构建了一个合成测试平台，该平台结合了通用token的统计流和源-目标token对的抽象事实流，从而可以精细地控制它们之间的相互作用。通过控制流的组成（上下文结构）来控制多样性的性质，并通过改变每个事实出现的统计流来控制多样性水平。

Result: 研究发现，虽然较高的上下文多样性会延迟分布内 (ID) 事实准确性，但它对分布外 (OOD) 事实泛化的影响主要取决于上下文结构。在某些情况下，OOD 性能与 ID 遵循相同的趋势，但在其他情况下，多样性对于非平凡的事实召回至关重要。即使低多样性禁止事实召回，最佳多样性水平也取决于训练持续时间。除了事实召回失败之外，我们还发现了统计泛化独立失败的结构，以及两种能力都会下降的其他结构。通过对模型组件进行一系列受控干预，我们将 OOD 失败追溯到不同的优化瓶颈，突出了嵌入和非嵌入层的重要性。

Conclusion: 上下文设计和多样性水平之间的相互作用会影响不同的泛化方面。该合成框架允许我们分离在大型研究中会混淆的影响，从而为未来的研究提供一个受控的测试平台。

Abstract: Language models are pretrained on sequences that blend statistical
regularities (making text fluent) with factual associations between specific
tokens (knowledge of facts). While recent work suggests that the variability of
their interaction, such as paraphrases of factual associations, critically
determines generalization ability, we lack a systematic analysis of these
impacts. This paper introduces a flexible synthetic testbed that combines a
statistical stream of generic tokens with an abstract factual stream of
source-target token pairs, enabling fine-grained control over their
interaction. The design enables the independent control of diversity nature by
manipulating stream composition (contextual structure) and the diversity level
by varying which statistical streams each fact appears in. Through controlled
experiments, we find that while higher contextual diversity delays
in-distribution (ID) factual accuracy, its impact on out-of-distribution (OOD)
factual generalization depends critically on contextual structure. In some
cases, OOD performance follows the same trend as ID, but in others, diversity
becomes essential for non-trivial factual recall. Even when low diversity
prohibits factual recall, optimal diversity levels depend on training duration.
Beyond factual recall failures, we identify structures where statistical
generalization fails independently, and others where both capabilities degrade.
This shows how the interplay between contextual design and diversity level
impacts different generalization aspects. Further, through a series of
controlled interventions on the model components, we trace the OOD failures to
distinct optimization bottlenecks, highlighting the importance of the embedding
and unembedding layers. Our synthetic framework allows us to isolate effects
that would be confounded in large-scale studies, offering a controlled testbed
for future investigations.

</details>


### [7] [In Generative AI We (Dis)Trust? Computational Analysis of Trust and Distrust in Reddit Discussions](https://arxiv.org/abs/2510.16173)
*Aria Pessianzadeh,Naima Sultana,Hildegarde Van den Bulck,David Gefen,Shahin Jabari,Rezvaneh Rezapour*

Main category: cs.CL

TL;DR: 本文对生成式人工智能 (GenAI) 的信任和不信任进行了首次计算研究，使用了一个多年的 Reddit 数据集。


<details>
  <summary>Details</summary>
Motivation: 负责任地采用和管理生成式人工智能系统，理解公众对它们的信任至关重要。以往对人工智能信任的研究大多来自心理学和人机交互领域，但缺乏对 GenAI 和大型语言模型 (LLM) 进行大规模、长期信任和不信任测量的计算方法。

Method: 结合众包注释和一个具有代表性的样本的分类模型来扩展分析，使用了一个多年的 Reddit 数据集 (2022--2025)，跨越 39 个 subreddit 和 197,618 个帖子。

Result: 信任和不信任随着时间的推移几乎是平衡的，并且在主要模型发布前后发生了变化。技术性能和可用性是主要的维度，而个人经验是影响态度的最常见原因。在信任者 (例如，专家、伦理学家、普通用户) 之间也出现了不同的模式。

Conclusion: 我们的结果为大规模信任分析提供了一个方法框架，并深入了解了公众对 GenAI 不断变化的看法。

Abstract: The rise of generative AI (GenAI) has impacted many aspects of human life. As
these systems become embedded in everyday practices, understanding public trust
in them also becomes essential for responsible adoption and governance. Prior
work on trust in AI has largely drawn from psychology and human-computer
interaction, but there is a lack of computational, large-scale, and
longitudinal approaches to measuring trust and distrust in GenAI and large
language models (LLMs). This paper presents the first computational study of
Trust and Distrust in GenAI, using a multi-year Reddit dataset (2022--2025)
spanning 39 subreddits and 197,618 posts. Crowd-sourced annotations of a
representative sample were combined with classification models to scale
analysis. We find that Trust and Distrust are nearly balanced over time, with
shifts around major model releases. Technical performance and usability
dominate as dimensions, while personal experience is the most frequent reason
shaping attitudes. Distinct patterns also emerge across trustors (e.g.,
experts, ethicists, general users). Our results provide a methodological
framework for large-scale Trust analysis and insights into evolving public
perceptions of GenAI.

</details>


### [8] [EgMM-Corpus: A Multimodal Vision-Language Dataset for Egyptian Culture](https://arxiv.org/abs/2510.16198)
*Mohamed Gamil,Abdelrahman Elsayed,Abdelrahman Lila,Ahmed Gad,Hesham Abdelgawad,Mohamed Aref,Ahmed Fares*

Main category: cs.CL

TL;DR: 提出了一个名为 EgMM-Corpus 的多模态数据集，专门用于埃及文化，包含 3000 多张图像，涵盖地标、食物和民俗等 313 个概念。


<details>
  <summary>Details</summary>
Motivation: 现有的 AI 技术在多模态文化多样性数据集方面仍然有限，特别是在中东和非洲地区。

Method: 设计并运行了一个新的数据收集管道，手动验证了文化真实性和多模态一致性。

Result: 在 EgMM-Corpus 上评估了 Contrastive Language-Image Pre-training CLIP 的 zero-shot 性能，Top-1 准确率为 21.2%，Top-5 准确率为 36.4%。

Conclusion: 强调了大规模视觉语言模型中存在的文化偏见，并证明了 EgMM-Corpus 作为开发具有文化意识的模型基准的重要性。

Abstract: Despite recent advances in AI, multimodal culturally diverse datasets are
still limited, particularly for regions in the Middle East and Africa. In this
paper, we introduce EgMM-Corpus, a multimodal dataset dedicated to Egyptian
culture. By designing and running a new data collection pipeline, we collected
over 3,000 images, covering 313 concepts across landmarks, food, and folklore.
Each entry in the dataset is manually validated for cultural authenticity and
multimodal coherence. EgMM-Corpus aims to provide a reliable resource for
evaluating and training vision-language models in an Egyptian cultural context.
We further evaluate the zero-shot performance of Contrastive Language-Image
Pre-training CLIP on EgMM-Corpus, on which it achieves 21.2% Top-1 accuracy and
36.4% Top-5 accuracy in classification. These results underscore the existing
cultural bias in large-scale vision-language models and demonstrate the
importance of EgMM-Corpus as a benchmark for developing culturally aware
models.

</details>


### [9] [What Can String Probability Tell Us About Grammaticality?](https://arxiv.org/abs/2510.16227)
*Jennifer Hu,Ethan Gotlieb Wilcox,Siyuan Song,Kyle Mahowald,Roger P. Levy*

Main category: cs.CL

TL;DR: 本研究探讨了语言模型(LM)在语法方面的学习情况，并分析了字符串概率与LM潜在语法知识之间的关系。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探讨语言模型是否以及如何学习语法，这对于语言理论具有重要意义。以往的研究对于如何通过字符串概率来揭示语言模型的语法知识存在争议。

Method: 该研究基于语料库数据的生成过程，提出了一个语法、语义和字符串概率之间关系的理论框架。该框架提出了三个预测，并使用英语和汉语的28万个句子对进行实证验证。

Result: 实证结果验证了理论框架的三个预测：(1) 最小对中字符串概率之间存在相关性；(2) 模型和人类在最小对中的差异之间存在相关性；(3) 未配对的语法和非语法字符串在概率空间中的分离度较差。

Conclusion: 该研究为使用概率来了解LM的结构知识提供了理论基础，并为LM语法评估的未来工作指明了方向。

Abstract: What have language models (LMs) learned about grammar? This question remains
hotly debated, with major ramifications for linguistic theory. However, since
probability and grammaticality are distinct notions in linguistics, it is not
obvious what string probabilities can reveal about an LM's underlying
grammatical knowledge. We present a theoretical analysis of the relationship
between grammar, meaning, and string probability, based on simple assumptions
about the generative process of corpus data. Our framework makes three
predictions, which we validate empirically using 280K sentence pairs in English
and Chinese: (1) correlation between the probability of strings within minimal
pairs, i.e., string pairs with minimal semantic differences; (2) correlation
between models' and humans' deltas within minimal pairs; and (3) poor
separation in probability space between unpaired grammatical and ungrammatical
strings. Our analyses give theoretical grounding for using probability to learn
about LMs' structural knowledge, and suggest directions for future work in LM
grammatical evaluation.

</details>


### [10] [Towards Low-Resource Alignment to Diverse Perspectives with Sparse Feedback](https://arxiv.org/abs/2510.16257)
*Chu Fei Luo,Samuel Dahan,Xiaodan Zhu*

Main category: cs.CL

TL;DR: 这篇论文旨在提高语言模型在低资源环境下的多元化对齐能力，通过两种方法：多元化解码和模型引导。


<details>
  <summary>Details</summary>
Motivation: 确保语言模型与不同的观点对齐，并能够反映人类价值观的细微差别非常重要。然而，现代语言模型最流行的训练范式通常假设每个查询都有一个最佳答案，从而导致泛化的响应和较差的对齐。

Method: 提出了两种方法：多元化解码和模型引导。

Result: 实验证明，仅使用 50 个带注释的样本，模型引导就比零样本和小样本基线提供了持续的改进。所提出的方法减少了在仇恨言论检测和错误信息检测等几个高风险任务中的假阳性，并提高了在全球观点问答中对人类价值观的分布对齐。

Conclusion: 这项工作强调了多样性的重要性，以及如何调整语言模型以考虑细微的观点。

Abstract: As language models have a greater impact on society, it is important to
ensure they are aligned to a diverse range of perspectives and are able to
reflect nuance in human values. However, the most popular training paradigms
for modern language models often assume there is one optimal answer for every
query, leading to generic responses and poor alignment. In this work, we aim to
enhance pluralistic alignment of language models in a low-resource setting with
two methods: pluralistic decoding and model steering. We empirically
demonstrate that model steering offers consistent improvement over zero-shot
and few-shot baselines with only 50 annotated samples. Our proposed methods
decrease false positives in several high-stakes tasks such as hate speech
detection and misinformation detection, and improves the distributional
alignment to human values in GlobalOpinionQA. We hope our work highlights the
importance of diversity and how language models can be adapted to consider
nuanced perspectives.

</details>


### [11] [Instant Personalized Large Language Model Adaptation via Hypernetwork](https://arxiv.org/abs/2510.16282)
*Zhaoxuan Tan,Zixuan Zhang,Haoyang Wen,Zheng Li,Rongzhi Zhang,Pei Chen,Fengran Mo,Zheyuan Liu,Qingkai Zeng,Qingyu Yin,Meng Jiang*

Main category: cs.CL

TL;DR: 提出了一种新的LLM个性化框架，通过超网络将用户profile映射到adapter参数，无需为每个用户单独训练。


<details>
  <summary>Details</summary>
Motivation: 现有的PEFT方法（如OPPU）需要为每个用户训练单独的adapter，计算成本高，不适合实时更新。

Method: 使用一个端到端训练的超网络，将用户的编码profile直接映射到一整套adapter参数（例如，LoRA）。

Result: 实验结果表明，该方法优于基于prompt的个性化和OPPU，同时在部署时使用更少的计算资源。该框架对分布外的用户表现出很强的泛化能力，并在不同的用户活动水平和不同的embedding backbone上保持了鲁棒性。

Conclusion: 所提出的Profile-to-PEFT框架实现了高效、可扩展和自适应的LLM个性化，适用于大规模应用。

Abstract: Personalized large language models (LLMs) tailor content to individual
preferences using user profiles or histories. However, existing
parameter-efficient fine-tuning (PEFT) methods, such as the
``One-PEFT-Per-User'' (OPPU) paradigm, require training a separate adapter for
each user, making them computationally expensive and impractical for real-time
updates. We introduce Profile-to-PEFT, a scalable framework that employs a
hypernetwork, trained end-to-end, to map a user's encoded profile directly to a
full set of adapter parameters (e.g., LoRA), eliminating per-user training at
deployment. This design enables instant adaptation, generalization to unseen
users, and privacy-preserving local deployment. Experimental results
demonstrate that our method outperforms both prompt-based personalization and
OPPU while using substantially fewer computational resources at deployment. The
framework exhibits strong generalization to out-of-distribution users and
maintains robustness across varying user activity levels and different
embedding backbones. The proposed Profile-to-PEFT framework enables efficient,
scalable, and adaptive LLM personalization suitable for large-scale
applications.

</details>


### [12] [Thinking About Thinking: Evaluating Reasoning in Post-Trained Language Models](https://arxiv.org/abs/2510.16340)
*Pratham Singla,Shivank Garg,Ayush Singh,Ishan Garg,Ketan Suhaas Saichandran*

Main category: cs.CL

TL;DR: 大型语言模型可以通过生成补充规划 tokens 来增强处理复杂、逻辑密集型任务的能力。本文研究了这些模型是否了解它们“学习”和“思考”的内容。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型是否了解它们通过后训练技术“学习”和“思考”的内容，以及它们是否意识到它们所学习的潜在策略。

Method: 通过定义三个核心能力：(1) 对学习到的潜在策略的意识，(2) 这些策略在不同领域的泛化，(3) 内部推理轨迹和最终输出之间的一致性，并在多个任务上进行实证评估。对比了通过监督微调 (SFT)、直接策略优化 (DPO) 和组相对策略优化 (GRPO) 进行后训练的模型的表现。

Result: RL 训练的模型不仅比 SFT 模型表现出更强的对学习行为的意识和对新的、结构相似的任务的泛化能力，而且通常在推理轨迹和最终输出之间表现出较弱的一致性，GRPO 训练的模型中这种效应最为明显。

Conclusion: RL 训练的模型比 SFT 模型更了解它们的行为，但 GRPO 训练的模型在推理和输出之间的一致性较差。

Abstract: Recent advances in post-training techniques have endowed Large Language
Models (LLMs) with enhanced capabilities for tackling complex, logic-intensive
tasks through the generation of supplementary planning tokens. This development
raises a fundamental question: Are these models aware of what they "learn" and
"think"? To address this, we define three core competencies: (1) awareness of
learned latent policies, (2) generalization of these policies across domains,
and (3) alignment between internal reasoning traces and final outputs. We
empirically evaluate these abilities on several tasks, each designed to require
learning a distinct policy. Furthermore, we contrast the profiles of models
post-trained via Supervised Fine-Tuning (SFT), Direct Policy Optimization
(DPO), and Group Relative Policy Optimization (GRPO). Our findings indicate
that RL-trained models not only demonstrate greater awareness of their learned
behaviors and stronger generalizability to novel, structurally similar tasks
than SFT models but also often exhibit weak alignment between their reasoning
traces and final outputs, an effect most pronounced in GRPO-trained models.

</details>


### [13] [Utilising Large Language Models for Generating Effective Counter Arguments to Anti-Vaccine Tweets](https://arxiv.org/abs/2510.16359)
*Utsav Dhanuka,Soham Poddar,Saptarshi Ghosh*

Main category: cs.CL

TL;DR: 本文探讨了利用大型语言模型（LLM）生成针对疫苗错误信息的反驳论点的能力，旨在对抗社交媒体上的疫苗怀疑论和错误信息。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的错误疫苗信息影响公共健康，需要生成有针对性的反驳论点以消除这些信息。

Method: 通过实验不同的提示策略和微调方法，优化反驳论点的生成。同时，训练分类器将反疫苗推文分类为多个类别，以便提供更符合语境的反驳。

Result: 通过人工判断、基于LLM的评估和自动指标进行评估，结果显示，整合标签描述和结构化微调可以提高反驳论点的有效性。

Conclusion: 研究表明，利用LLM生成反驳论点是一种有前景的大规模缓解疫苗错误信息的方法。

Abstract: In an era where public health is increasingly influenced by information
shared on social media, combatting vaccine skepticism and misinformation has
become a critical societal goal. Misleading narratives around vaccination have
spread widely, creating barriers to achieving high immunisation rates and
undermining trust in health recommendations. While efforts to detect
misinformation have made significant progress, the generation of real time
counter-arguments tailored to debunk such claims remains an insufficiently
explored area. In this work, we explore the capabilities of LLMs to generate
sound counter-argument rebuttals to vaccine misinformation. Building on prior
research in misinformation debunking, we experiment with various prompting
strategies and fine-tuning approaches to optimise counter-argument generation.
Additionally, we train classifiers to categorise anti-vaccine tweets into
multi-labeled categories such as concerns about vaccine efficacy, side effects,
and political influences allowing for more context aware rebuttals. Our
evaluation, conducted through human judgment, LLM based assessments, and
automatic metrics, reveals strong alignment across these methods. Our findings
demonstrate that integrating label descriptions and structured fine-tuning
enhances counter-argument effectiveness, offering a promising approach for
mitigating vaccine misinformation at scale.

</details>


### [14] [End-to-End Argument Mining through Autoregressive Argumentative Structure Prediction](https://arxiv.org/abs/2510.16363)
*Nilmadhab Das,Vishal Vaibhav,Yash Sunil Choudhary,V. Vijaya Saradhi,Ashish Anand*

Main category: cs.CL

TL;DR: 提出了一种新的论证挖掘（AM）框架，即自回归论证结构预测（AASP），以端到端的方式联合执行AM的关键任务。


<details>
  <summary>Details</summary>
Motivation: 现有的论证挖掘方法通常将论证结构扁平化，忽略了论证成分（AC）和论证关系（AR）之间的依赖关系，难以捕捉论证推理的复杂性。

Method: AASP框架基于自回归结构预测，利用条件预训练语言模型，将论证结构建模为受约束的预定义动作集，通过逐步构建论证结构来捕捉论证推理的流程。

Result: 在三个标准AM基准数据集上的大量实验表明，AASP在两个基准测试中实现了最先进的结果，并在一个基准测试中取得了优异的成绩。

Conclusion: AASP框架能够有效地捕捉论证推理的流程，并在论证挖掘任务中取得了优异的性能。

Abstract: Argument Mining (AM) helps in automating the extraction of complex
argumentative structures such as Argument Components (ACs) like Premise, Claim
etc. and Argumentative Relations (ARs) like Support, Attack etc. in an
argumentative text. Due to the inherent complexity of reasoning involved with
this task, modelling dependencies between ACs and ARs is challenging. Most of
the recent approaches formulate this task through a generative paradigm by
flattening the argumentative structures. In contrast to that, this study
jointly formulates the key tasks of AM in an end-to-end fashion using
Autoregressive Argumentative Structure Prediction (AASP) framework. The
proposed AASP framework is based on the autoregressive structure prediction
framework that has given good performance for several NLP tasks. AASP framework
models the argumentative structures as constrained pre-defined sets of actions
with the help of a conditional pre-trained language model. These actions build
the argumentative structures step-by-step in an autoregressive manner to
capture the flow of argumentative reasoning in an efficient way. Extensive
experiments conducted on three standard AM benchmarks demonstrate that AASP
achieves state-of-theart (SoTA) results across all AM tasks in two benchmarks
and delivers strong results in one benchmark.

</details>


### [15] [Rethinking On-policy Optimization for Query Augmentation](https://arxiv.org/abs/2510.17139)
*Zhichao Xu,Shengyao Zhuang,Xueguang Ma,Bingsen Chen,Yijun Tian,Fengran Mo,Jie Cao,Vivek Srikumar*

Main category: cs.CL

TL;DR: 对比了基于提示和基于强化学习的查询扩展方法，发现简单的、无训练的查询扩展通常与更昂贵的基于强化学习的方法性能相当甚至更好。提出了On-policy Pseudo-document Query Expansion (OPQE)，该方法结合了提示的灵活性和生成结构以及强化学习的针对性优化，优于单独的提示和基于强化学习的重写。


<details>
  <summary>Details</summary>
Motivation: 信息检索领域对查询扩展的兴趣激增，主要有两种方法：基于提示的方法和基于强化学习的方法。这两种方法各有优缺点，但尚未在一致的实验条件下进行比较。

Method: 对基于提示和基于强化学习的查询扩展进行了系统的比较，并提出了On-policy Pseudo-document Query Expansion (OPQE)方法。

Result: 简单的、无训练的查询扩展通常与更昂贵的基于强化学习的方法性能相当甚至更好。OPQE优于单独的提示和基于强化学习的重写。

Conclusion: 结合提示的灵活性和生成结构以及强化学习的针对性优化，可以获得最佳结果。

Abstract: Recent advances in large language models (LLMs) have led to a surge of
interest in query augmentation for information retrieval (IR). Two main
approaches have emerged. The first prompts LLMs to generate answers or
pseudo-documents that serve as new queries, relying purely on the model's
parametric knowledge or contextual information. The second applies
reinforcement learning (RL) to fine-tune LLMs for query rewriting, directly
optimizing retrieval metrics. While having respective advantages and
limitations, the two approaches have not been compared under consistent
experimental conditions. In this work, we present the first systematic
comparison of prompting-based and RL-based query augmentation across diverse
benchmarks, including evidence-seeking, ad hoc, and tool retrieval. Our key
finding is that simple, training-free query augmentation often performs on par
with, or even surpasses, more expensive RL-based counterparts, especially when
using powerful LLMs. Motivated by this discovery, we introduce a novel hybrid
method, On-policy Pseudo-document Query Expansion (OPQE), which, instead of
rewriting a query, the LLM policy learns to generate a pseudo-document that
maximizes retrieval performance, thus merging the flexibility and generative
structure of prompting with the targeted optimization of RL. We show OPQE
outperforms both standalone prompting and RL-based rewriting, demonstrating
that a synergistic approach yields the best results. Our implementation is made
available to facilitate reproducibility.

</details>


### [16] [Navigating through the hidden embedding space: steering LLMs to improve mental health assessment](https://arxiv.org/abs/2510.16373)
*Federico Ravenda,Seyed Ali Bahrainian,Andrea Raballo,Antonietta Mira*

Main category: cs.CL

TL;DR: 提出了一种轻量级方法，通过线性变换和steering vectors来提升LLM在心理健康评估方面的能力，无需大量计算。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型(llm)发展迅速，但在特定领域的应用中，小规模模型仍然难以达到最佳性能，尤其是在心理健康(MH)领域。

Method: 该方法包括对特定层的激活应用线性变换，利用steering vectors来引导模型的输出。

Result: 该方法在两个不同的任务中都取得了改进的结果：(1)识别Reddit帖子是否有助于检测抑郁症状(相关性预测任务)，(2)根据用户的Reddit帖子历史完成标准化心理筛查问卷(问卷完成任务)。

Conclusion: 结果表明，steering机制作为一种计算效率高的工具，在llm的MH领域适应方面具有未开发的潜力。

Abstract: The rapid evolution of Large Language Models (LLMs) is transforming AI,
opening new opportunities in sensitive and high-impact areas such as Mental
Health (MH). Yet, despite these advancements, recent evidence reveals that
smaller-scale models still struggle to deliver optimal performance in
domain-specific applications. In this study, we present a cost-efficient yet
powerful approach to improve MH assessment capabilities of an LLM, without
relying on any computationally intensive techniques. Our lightweight method
consists of a linear transformation applied to a specific layer's activations,
leveraging steering vectors to guide the model's output. Remarkably, this
intervention enables the model to achieve improved results across two distinct
tasks: (1) identifying whether a Reddit post is useful for detecting the
presence or absence of depressive symptoms (relevance prediction task), and (2)
completing a standardized psychological screening questionnaire for depression
based on users' Reddit post history (questionnaire completion task). Results
highlight the untapped potential of steering mechanisms as computationally
efficient tools for LLMs' MH domain adaptation.

</details>


### [17] [MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in Language Models, More than Outcomes](https://arxiv.org/abs/2510.16380)
*Yu Ying Chiu,Michael S. Lee,Rachel Calcott,Brandon Handoko,Paul de Font-Reaulx,Paula Rodriguez,Chen Bo Calvin Zhang,Ziwen Han,Udari Madhushani Sehwag,Yash Maurya,Christina Q Knight,Harry R. Lloyd,Florence Bacus,Mantas Mazeika,Bing Liu,Yejin Choi,Mitchell L Gordon,Sydney Levine*

Main category: cs.CL

TL;DR: 提出了MoReBench，一个包含1000个道德场景的基准，用于评估AI的道德推理能力。


<details>
  <summary>Details</summary>
Motivation: 确保AI决策与人类价值观对齐，需要理解AI如何做出决策。道德困境为过程评估提供了 отличную 试验场。

Method: 创建了MoReBench，包含道德场景和专家设计的评估标准。同时，创建了MoReBench-Theory，用于测试AI在五种主要规范伦理框架下的推理能力。

Result: 现有基准在数学、代码和科学推理任务上的表现无法预测模型在道德推理方面的能力。模型对特定道德框架存在偏好。

Conclusion: 这些基准推进了以过程为中心的推理评估，旨在实现更安全、更透明的AI。

Abstract: As AI systems progress, we rely more on them to make decisions with us and
for us. To ensure that such decisions are aligned with human values, it is
imperative for us to understand not only what decisions they make but also how
they come to those decisions. Reasoning language models, which provide both
final responses and (partially transparent) intermediate thinking traces,
present a timely opportunity to study AI procedural reasoning. Unlike math and
code problems which often have objectively correct answers, moral dilemmas are
an excellent testbed for process-focused evaluation because they allow for
multiple defensible conclusions. To do so, we present MoReBench: 1,000 moral
scenarios, each paired with a set of rubric criteria that experts consider
essential to include (or avoid) when reasoning about the scenarios. MoReBench
contains over 23 thousand criteria including identifying moral considerations,
weighing trade-offs, and giving actionable recommendations to cover cases on AI
advising humans moral decisions as well as making moral decisions autonomously.
Separately, we curate MoReBench-Theory: 150 examples to test whether AI can
reason under five major frameworks in normative ethics. Our results show that
scaling laws and existing benchmarks on math, code, and scientific reasoning
tasks fail to predict models' abilities to perform moral reasoning. Models also
show partiality towards specific moral frameworks (e.g., Benthamite Act
Utilitarianism and Kantian Deontology), which might be side effects of popular
training paradigms. Together, these benchmarks advance process-focused
reasoning evaluation towards safer and more transparent AI.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [18] [ESCA: Contextualizing Embodied Agents via Scene-Graph Generation](https://arxiv.org/abs/2510.15963)
*Jiani Huang,Amish Sethi,Matthew Kuo,Mayank Keoliya,Neelay Velingker,JungHo Jung,Ser-Nam Lim,Ziyang Li,Mayur Naik*

Main category: cs.CV

TL;DR: 本文提出了ESCA框架，通过结构化的时空理解来增强具身代理的上下文感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型训练pipeline缺乏像素级视觉内容和文本语义之间的细粒度对齐。

Method: 提出了一个名为SGClip的CLIP模型，该模型基于神经符号学习pipeline在87K+开放域视频上进行训练，无需人工标注场景图。

Result: SGClip在场景图生成和动作定位基准测试中表现出色。ESCA与SGClip结合，持续改进了开源和商业MLLM，并在两个具身环境中实现了最先进的性能。

Conclusion: ESCA显著减少了代理感知错误，并使开源模型超越了专有基线。

Abstract: Multi-modal large language models (MLLMs) are making rapid progress toward
general-purpose embodied agents. However, current training pipelines primarily
rely on high-level vision-sound-text pairs and lack fine-grained, structured
alignment between pixel-level visual content and textual semantics. To overcome
this challenge, we propose ESCA, a new framework for contextualizing embodied
agents through structured spatial-temporal understanding. At its core is
SGClip, a novel CLIP-based, open-domain, and promptable model for generating
scene graphs. SGClip is trained on 87K+ open-domain videos via a neurosymbolic
learning pipeline, which harnesses model-driven self-supervision from
video-caption pairs and structured reasoning, thereby eliminating the need for
human-labeled scene graph annotations. We demonstrate that SGClip supports both
prompt-based inference and task-specific fine-tuning, excelling in scene graph
generation and action localization benchmarks. ESCA with SGClip consistently
improves both open-source and commercial MLLMs, achieving state-of-the-art
performance across two embodied environments. Notably, it significantly reduces
agent perception errors and enables open-source models to surpass proprietary
baselines.

</details>


### [19] [CrossRay3D: Geometry and Distribution Guidance for Efficient Multimodal 3D Detection](https://arxiv.org/abs/2510.15991)
*Huiming Yang*

Main category: cs.CV

TL;DR: 提出了一种名为CrossRay3D的稀疏跨模态检测器，它在nuScenes基准测试中实现了最先进的性能，同时比其他领先方法运行速度快1.84倍。即使在LiDAR或相机数据部分或完全丢失的情况下，CrossRay3D也表现出强大的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的稀疏检测器忽略了token表示的质量，导致前景质量欠佳且性能有限。

Method: 提出了一个稀疏选择器（SS），其核心模块是射线感知监督（RAS）和类平衡监督，以及射线位置编码（Ray PE）。

Result: 在具有挑战性的nuScenes基准测试中，CrossRay3D实现了最先进的性能，mAP为72.4，NDS为74.7。

Conclusion: CrossRay3D在稀疏跨模态检测方面具有优越的性能和鲁棒性。

Abstract: The sparse cross-modality detector offers more advantages than its
counterpart, the Bird's-Eye-View (BEV) detector, particularly in terms of
adaptability for downstream tasks and computational cost savings. However,
existing sparse detectors overlook the quality of token representation, leaving
it with a sub-optimal foreground quality and limited performance. In this
paper, we identify that the geometric structure preserved and the class
distribution are the key to improving the performance of the sparse detector,
and propose a Sparse Selector (SS). The core module of SS is Ray-Aware
Supervision (RAS), which preserves rich geometric information during the
training stage, and Class-Balanced Supervision, which adaptively reweights the
salience of class semantics, ensuring that tokens associated with small objects
are retained during token sampling. Thereby, outperforming other sparse
multi-modal detectors in the representation of tokens. Additionally, we design
Ray Positional Encoding (Ray PE) to address the distribution differences
between the LiDAR modality and the image. Finally, we integrate the
aforementioned module into an end-to-end sparse multi-modality detector, dubbed
CrossRay3D. Experiments show that, on the challenging nuScenes benchmark,
CrossRay3D achieves state-of-the-art performance with 72.4 mAP and 74.7 NDS,
while running 1.84 faster than other leading methods. Moreover, CrossRay3D
demonstrates strong robustness even in scenarios where LiDAR or camera data are
partially or entirely missing.

</details>


### [20] [InfraGPT Smart Infrastructure: An End-to-End VLM-Based Framework for Detecting and Managing Urban Defects](https://arxiv.org/abs/2510.16017)
*Ibrahim Sheikh Mohamed,Abdullah Yahya Abdullah Omaisan*

Main category: cs.CV

TL;DR: 本文提出了一种综合性的管道，利用街道闭路电视流进行多重缺陷检测和分割，并使用视觉语言模型（VLM）生成结构化的行动计划。


<details>
  <summary>Details</summary>
Motivation: 人工检测成本高且危险，现有的自动系统通常只处理个别缺陷类型或提供非结构化输出，无法直接指导维护人员。

Method: 使用YOLO系列目标检测器进行多重缺陷检测和分割，并将检测结果传递给视觉语言模型（VLM）进行场景感知总结。

Result: 在公共数据集和捕获的闭路电视片段上的实验评估表明，该系统能够准确识别各种缺陷并生成连贯的摘要。

Conclusion: 讨论了将系统扩展到全市范围部署的挑战和方向。

Abstract: Infrastructure in smart cities is increasingly monitored by networks of
closed circuit television (CCTV) cameras. Roads, bridges and tunnels develop
cracks, potholes, and fluid leaks that threaten public safety and require
timely repair. Manual inspection is costly and hazardous, and existing
automatic systems typically address individual defect types or provide
unstructured outputs that cannot directly guide maintenance crews. This paper
proposes a comprehensive pipeline that leverages street CCTV streams for multi
defect detection and segmentation using the YOLO family of object detectors and
passes the detections to a vision language model (VLM) for scene aware
summarization. The VLM generates a structured action plan in JSON format that
includes incident descriptions, recommended tools, dimensions, repair plans,
and urgent alerts. We review literature on pothole, crack and leak detection,
highlight recent advances in large vision language models such as QwenVL and
LLaVA, and describe the design of our early prototype. Experimental evaluation
on public datasets and captured CCTV clips demonstrates that the system
accurately identifies diverse defects and produces coherent summaries. We
conclude by discussing challenges and directions for scaling the system to city
wide deployments.

</details>


### [21] [IAD-GPT: Advancing Visual Knowledge in Multimodal Large Language Model for Industrial Anomaly Detection](https://arxiv.org/abs/2510.16036)
*Zewen Li,Zitong Yu,Qilang Ye,Weicheng Xie,Wei Zhuo,Linlin Shen*

Main category: cs.CV

TL;DR: IAD-GPT: A new MLLM-based paradigm for industrial anomaly detection (IAD) that leverages detailed anomaly prompts, text-guided enhancement, and multi-mask fusion.


<details>
  <summary>Details</summary>
Motivation: Traditional IAD methods lack multi-turn dialogue and detailed descriptions. Existing large pre-trained models haven't fully utilized their anomaly detection capabilities.

Method: Proposes IAD-GPT with Abnormal Prompt Generator (APG), Text-Guided Enhancer, and Multi-Mask Fusion module to enhance anomaly detection and segmentation.

Result: Achieves state-of-the-art performance on MVTec-AD and VisA datasets for self-supervised and few-shot anomaly detection and segmentation.

Conclusion: IAD-GPT effectively combines text semantics with image information for improved industrial anomaly detection.

Abstract: The robust causal capability of Multimodal Large Language Models (MLLMs) hold
the potential of detecting defective objects in Industrial Anomaly Detection
(IAD). However, most traditional IAD methods lack the ability to provide
multi-turn human-machine dialogues and detailed descriptions, such as the color
of objects, the shape of an anomaly, or specific types of anomalies. At the
same time, methods based on large pre-trained models have not fully stimulated
the ability of large models in anomaly detection tasks. In this paper, we
explore the combination of rich text semantics with both image-level and
pixel-level information from images and propose IAD-GPT, a novel paradigm based
on MLLMs for IAD. We employ Abnormal Prompt Generator (APG) to generate
detailed anomaly prompts for specific objects. These specific prompts from the
large language model (LLM) are used to activate the detection and segmentation
functions of the pre-trained visual-language model (i.e., CLIP). To enhance the
visual grounding ability of MLLMs, we propose Text-Guided Enhancer, wherein
image features interact with normal and abnormal text prompts to dynamically
select enhancement pathways, which enables language models to focus on specific
aspects of visual data, enhancing their ability to accurately interpret and
respond to anomalies within images. Moreover, we design a Multi-Mask Fusion
module to incorporate mask as expert knowledge, which enhances the LLM's
perception of pixel-level anomalies. Extensive experiments on MVTec-AD and VisA
datasets demonstrate our state-of-the-art performance on self-supervised and
few-shot anomaly detection and segmentation tasks, such as MVTec-AD and VisA
datasets. The codes are available at
\href{https://github.com/LiZeWen1225/IAD-GPT}{https://github.com/LiZeWen1225/IAD-GPT}.

</details>


### [22] [Effect of Reporting Mode and Clinical Experience on Radiologists' Gaze and Image Analysis Behavior in Chest Radiography](https://arxiv.org/abs/2510.16070)
*Mahta Khoobi,Marc Sebastian von der Stueck,Felix Barajas Ordonez,Anca-Maria Iancu,Eric Corban,Julia Nowak,Aleksandar Kargaliev,Valeria Perelygina,Anna-Sophie Schott,Daniel Pinto dos Santos,Christiane Kuhl,Daniel Truhn,Sven Nebelung,Robert Siepmann*

Main category: cs.CV

TL;DR: 比较了三种报告模式（自由文本 FT、结构化报告 SR 和 AI 辅助结构化报告 AI-SR）对图像分析行为、诊断准确性、效率和用户体验的影响。


<details>
  <summary>Details</summary>
Motivation: 旨在研究结构化报告 (SR) 和人工智能 (AI) 如何改变放射科医生与影像学研究的交互方式。

Method: 招募了四名新手和四名非新手读者（放射科医生和医学生），每个人使用定制的阅读器和眼动追踪系统分析 35 张床边胸片。通过 Cohen's Kappa 值、报告时间、眼动追踪指标和问卷调查来评估诊断准确性、报告时间和用户体验。

Result: AI-SR 的诊断准确性更高，报告时间更短，并且更受用户欢迎。SR 通过引导视觉注意力来提高效率。

Conclusion: SR 提高了效率，AI 预填充的 SR 进一步提高了诊断准确性和用户满意度。

Abstract: Structured reporting (SR) and artificial intelligence (AI) may transform how
radiologists interact with imaging studies. This prospective study (July to
December 2024) evaluated the impact of three reporting modes: free-text (FT),
structured reporting (SR), and AI-assisted structured reporting (AI-SR), on
image analysis behavior, diagnostic accuracy, efficiency, and user experience.
Four novice and four non-novice readers (radiologists and medical students)
each analyzed 35 bedside chest radiographs per session using a customized
viewer and an eye-tracking system. Outcomes included diagnostic accuracy
(compared with expert consensus using Cohen's $\kappa$), reporting time per
radiograph, eye-tracking metrics, and questionnaire-based user experience.
Statistical analysis used generalized linear mixed models with Bonferroni
post-hoc tests with a significance level of ($P \le .01$). Diagnostic accuracy
was similar in FT ($\kappa = 0.58$) and SR ($\kappa = 0.60$) but higher in
AI-SR ($\kappa = 0.71$, $P < .001$). Reporting times decreased from $88 \pm 38$
s (FT) to $37 \pm 18$ s (SR) and $25 \pm 9$ s (AI-SR) ($P < .001$). Saccade
counts for the radiograph field ($205 \pm 135$ (FT), $123 \pm 88$ (SR), $97 \pm
58$ (AI-SR)) and total fixation duration for the report field ($11 \pm 5$ s
(FT), $5 \pm 3$ s (SR), $4 \pm 1$ s (AI-SR)) were lower with SR and AI-SR ($P <
.001$ each). Novice readers shifted gaze towards the radiograph in SR, while
non-novice readers maintained their focus on the radiograph. AI-SR was the
preferred mode. In conclusion, SR improves efficiency by guiding visual
attention toward the image, and AI-prefilled SR further enhances diagnostic
accuracy and user satisfaction.

</details>


### [23] [Data-Driven Analysis of Intersectional Bias in Image Classification: A Framework with Bias-Weighted Augmentation](https://arxiv.org/abs/2510.16072)
*Farjana Yesmin*

Main category: cs.CV

TL;DR: 这篇论文提出了一个用于分析和减轻图像分类中交叉偏差的框架。该框架结合了定量公平性指标和可解释性工具来识别模型预测中的偏差模式，并提出了一种新的数据增强策略，该策略可以根据子组分布统计信息调整转换强度。实验表明，该方法可以提高弱势群体的准确性并减少公平性指标的差距。


<details>
  <summary>Details</summary>
Motivation: 在不平衡数据集上训练的机器学习模型通常表现出交叉偏差，即由对象类别和环境条件等多个属性相互作用引起的系统性错误。

Method: 该论文介绍了交叉公平性评估框架 (IFEF)，该框架结合了定量公平性指标与可解释性工具，以系统地识别模型预测中的偏差模式。在此分析的基础上，该文提出了一种偏差加权增强 (BWA)，这是一种新的数据增强策略，可根据子组分布统计信息调整转换强度。

Result: 在具有五个对象类别的 Open Images V7 数据集上进行的实验表明，BWA 将代表性不足的类环境交叉点的准确率提高了高达 24 个百分点，同时将公平性指标的差距减少了 35%。

Conclusion: 该方法提供了一种可复制的方法，用于分析和解决图像分类系统中的交叉偏差。

Abstract: Machine learning models trained on imbalanced datasets often exhibit
intersectional biases-systematic errors arising from the interaction of
multiple attributes such as object class and environmental conditions. This
paper presents a data-driven framework for analyzing and mitigating such biases
in image classification. We introduce the Intersectional Fairness Evaluation
Framework (IFEF), which combines quantitative fairness metrics with
interpretability tools to systematically identify bias patterns in model
predictions. Building on this analysis, we propose Bias-Weighted Augmentation
(BWA), a novel data augmentation strategy that adapts transformation
intensities based on subgroup distribution statistics. Experiments on the Open
Images V7 dataset with five object classes demonstrate that BWA improves
accuracy for underrepresented class-environment intersections by up to 24
percentage points while reducing fairness metric disparities by 35%.
Statistical analysis across multiple independent runs confirms the significance
of improvements (p < 0.05). Our methodology provides a replicable approach for
analyzing and addressing intersectional biases in image classification systems.

</details>


### [24] [Differentiable, Bit-shifting, and Scalable Quantization without training neural network from scratch](https://arxiv.org/abs/2510.16088)
*Zia Badar*

Main category: cs.CV

TL;DR: 提出了一种可微分的量化方法，并证明了其收敛性。该方法支持权重和激活的对数量化，并在ImageNet数据集上取得了很好的结果。


<details>
  <summary>Details</summary>
Motivation: 之前的量化方法不可微，学习能力受限，且在权重和激活同时量化时精度较低。

Method: 提出了一种可微分的量化方法，支持n比特量化。

Result: 在ImageNet数据集上，使用ResNet18进行图像分类任务，仅使用权重进行量化时，精度损失小于1%。在权重和激活同时量化时，精度与SOTA方法相当。

Conclusion: 该方法在精度和效率之间取得了较好的平衡。

Abstract: Quantization of neural networks provides benefits of inference in less
compute and memory requirements. Previous work in quantization lack two
important aspects which this work provides. First almost all previous work in
quantization used a non-differentiable approach and for learning; the
derivative is usually set manually in backpropogation which make the learning
ability of algorithm questionable, our approach is not just differentiable, we
also provide proof of convergence of our approach to the optimal neural
network. Second previous work in shift/logrithmic quantization either have
avoided activation quantization along with weight quantization or achieved less
accuracy. Learning logrithmic quantize values of form $2^n$ requires the
quantization function can scale to more than 1 bit quantization which is
another benifit of our quantization that it provides $n$ bits quantization as
well. Our approach when tested with image classification task using imagenet
dataset, resnet18 and weight quantization only achieves less than 1 percent
accuracy compared to full precision accuracy while taking only 15 epochs to
train using shift bit quantization and achieves comparable to SOTA approaches
accuracy in both weight and activation quantization using shift bit
quantization in 15 training epochs with slightly higher(only higher cpu
instructions) inference cost compared to 1 bit quantization(without logrithmic
quantization) and not requiring any higher precision multiplication.

</details>


### [25] [StripRFNet: A Strip Receptive Field and Shape-Aware Network for Road Damage Detection](https://arxiv.org/abs/2510.16115)
*Jianhan Lin,Yuchu Qin,Shuai Gao,Yikang Rui,Jie Liu,Yanjie Lv*

Main category: cs.CV

TL;DR: StripRFNet: A new deep neural network for accurate road surface damage detection.


<details>
  <summary>Details</summary>
Motivation: Accurate road surface damage detection is crucial for traffic safety and sustainable urban development (SDG 11), but it is challenging due to diverse damage shapes, slender cracks, and high error rates in small-scale damage recognition.

Method: The paper proposes StripRFNet, which includes a Shape Perception Module (SPM) using large separable kernel attention, a Strip Receptive Field Module (SRFM) employing large strip convolutions and pooling, and a Small-Scale Enhancement Module (SSEM) leveraging a high-resolution feature map and dynamic upsampling.

Result: StripRFNet outperforms existing methods on the RDD2022 benchmark, improving F1-score, mAP50, and mAP50:95 on the Chinese subset. It achieves the highest F1-score of 80.33% on the full dataset while maintaining competitive inference speed.

Conclusion: StripRFNet achieves state-of-the-art accuracy and real-time efficiency, offering a promising tool for intelligent road maintenance and sustainable infrastructure management.

Abstract: Well-maintained road networks are crucial for achieving Sustainable
Development Goal (SDG) 11. Road surface damage not only threatens traffic
safety but also hinders sustainable urban development. Accurate detection,
however, remains challenging due to the diverse shapes of damages, the
difficulty of capturing slender cracks with high aspect ratios, and the high
error rates in small-scale damage recognition. To address these issues, we
propose StripRFNet, a novel deep neural network comprising three modules: (1) a
Shape Perception Module (SPM) that enhances shape discrimination via large
separable kernel attention (LSKA) in multi-scale feature aggregation; (2) a
Strip Receptive Field Module (SRFM) that employs large strip convolutions and
pooling to capture features of slender cracks; and (3) a Small-Scale
Enhancement Module (SSEM) that leverages a high-resolution P2 feature map, a
dedicated detection head, and dynamic upsampling to improve small-object
detection. Experiments on the RDD2022 benchmark show that StripRFNet surpasses
existing methods. On the Chinese subset, it improves F1-score, mAP50, and
mAP50:95 by 4.4, 2.9, and 3.4 percentage points over the baseline,
respectively. On the full dataset, it achieves the highest F1-score of 80.33%
compared with CRDDC'2022 participants and ORDDC'2024 Phase 2 results, while
maintaining competitive inference speed. These results demonstrate that
StripRFNet achieves state-of-the-art accuracy and real-time efficiency,
offering a promising tool for intelligent road maintenance and sustainable
infrastructure management.

</details>


### [26] [ObjectTransforms for Uncertainty Quantification and Reduction in Vision-Based Perception for Autonomous Vehicles](https://arxiv.org/abs/2510.16118)
*Nishad Sahu,Shounak Sural,Aditya Satish Patil,Ragunathan,Rajkumar*

Main category: cs.CV

TL;DR: 本文介绍了一种名为 ObjectTransforms 的技术，用于量化和减少基于视觉的对象检测中的不确定性。


<details>
  <summary>Details</summary>
Motivation: 基于视觉的对象检测神经网络容易受到数据偏差和分布偏移等问题引起的不确定性的影响。因此，可靠的感知是自动驾驶安全决策的基础。

Method: ObjectTransforms 在训练时对单个对象执行颜色空间扰动，提高对光照和颜色变化的鲁棒性，并使用扩散模型生成逼真、多样化的行人实例。在推理时，对检测到的对象应用对象扰动，并使用检测分数的方差来实时量化预测不确定性。

Result: 在 NuImages 10K 数据集上使用 YOLOv8 进行的实验表明，该方法在训练期间显着提高了所有对象类别的准确性和减少了不确定性，同时在推理期间预测的假阳性的不确定性值高于真阳性。

Conclusion: ObjectTransforms 是一种轻量级但有效的机制，可分别在训练和推理期间减少和量化基于视觉的感知中的不确定性。

Abstract: Reliable perception is fundamental for safety critical decision making in
autonomous driving. Yet, vision based object detector neural networks remain
vulnerable to uncertainty arising from issues such as data bias and
distributional shifts. In this paper, we introduce ObjectTransforms, a
technique for quantifying and reducing uncertainty in vision based object
detection through object specific transformations at both training and
inference times. At training time, ObjectTransforms perform color space
perturbations on individual objects, improving robustness to lighting and color
variations. ObjectTransforms also uses diffusion models to generate realistic,
diverse pedestrian instances. At inference time, object perturbations are
applied to detected objects and the variance of detection scores are used to
quantify predictive uncertainty in real time. This uncertainty signal is then
used to filter out false positives and also recover false negatives, improving
the overall precision recall curve. Experiments with YOLOv8 on the NuImages 10K
dataset demonstrate that our method yields notable accuracy improvements and
uncertainty reduction across all object classes during training, while
predicting desirably higher uncertainty values for false positives as compared
to true positives during inference. Our results highlight the potential of
ObjectTransforms as a lightweight yet effective mechanism for reducing and
quantifying uncertainty in vision-based perception during training and
inference respectively.

</details>


### [27] [Aria Gen 2 Pilot Dataset](https://arxiv.org/abs/2510.16134)
*Chen Kong,James Fort,Aria Kang,Jonathan Wittmer,Simon Green,Tianwei Shen,Yipu Zhao,Cheng Peng,Gustavo Solaira,Andrew Berkovich,Nikhil Raina,Vijay Baiyya,Evgeniy Oleinik,Eric Huang,Fan Zhang,Julian Straub,Mark Schwesinger,Luis Pesqueira,Xiaqing Pan,Jakob Julian Engel,Carl Ren,Mingfei Yan,Richard Newcombe*

Main category: cs.CV

TL;DR: Aria Gen 2 Pilot Dataset (A2PD)是一个使用Aria Gen 2眼镜捕获的以自我为中心的Multimodal开放数据集，逐步发布并不断增强。


<details>
  <summary>Details</summary>
Motivation: 为了促进及时访问A2PD数据集。

Method: 使用配备Aria Gen 2眼镜的Dia'ane及其朋友记录他们的日常活动，涵盖清洁、烹饪、饮食、玩耍和户外行走五个主要场景，提供全面的原始传感器数据和各种机器感知算法的输出数据。

Result: 展示了设备感知佩戴者、周围环境以及佩戴者与环境之间交互的能力，同时在不同的用户和条件下保持了强大的性能。

Conclusion: A2PD在projectaria.com公开提供，并在Project Aria Tools中提供开源工具和使用示例。

Abstract: The Aria Gen 2 Pilot Dataset (A2PD) is an egocentric multimodal open dataset
captured using the state-of-the-art Aria Gen 2 glasses. To facilitate timely
access, A2PD is released incrementally with ongoing dataset enhancements. The
initial release features Dia'ane, our primary subject, who records her daily
activities alongside friends, each equipped with Aria Gen 2 glasses. It
encompasses five primary scenarios: cleaning, cooking, eating, playing, and
outdoor walking. In each of the scenarios, we provide comprehensive raw sensor
data and output data from various machine perception algorithms. These data
illustrate the device's ability to perceive the wearer, the surrounding
environment, and interactions between the wearer and the environment, while
maintaining robust performance across diverse users and conditions. The A2PD is
publicly available at projectaria.com, with open-source tools and usage
examples provided in Project Aria Tools.

</details>


### [28] [GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer](https://arxiv.org/abs/2510.16136)
*Sayan Deb Sarkar,Sinisa Stekovic,Vincent Lepetit,Iro Armeni*

Main category: cs.CV

TL;DR: 本研究提出了一种新的外观迁移方法，可以将图像或文本的外观迁移到3D资产上，即使输入和外观对象之间的几何形状差异很大也能work。


<details>
  <summary>Details</summary>
Motivation: 现有方法在输入和外观对象几何形状差异显著时效果不佳，直接应用3D生成模型也无法产生吸引人的结果。

Method: 该方法受到通用引导的启发，利用预训练的校正流模型，通过周期性地添加引导来与采样过程交互。引导被建模为可微损失函数，并实验了两种不同类型的引导，包括用于外观和自相似性的部分感知损失。

Result: 实验表明，该方法成功地将纹理和几何细节转移到输入3D资产，在质量和数量上均优于基线方法。同时，传统指标不适合评估该任务，因此采用基于GPT的系统客观地对输出进行排序。

Conclusion: 该方法是通用的，可以扩展到不同类型的扩散模型和引导函数。

Abstract: Transferring appearance to 3D assets using different representations of the
appearance object - such as images or text - has garnered interest due to its
wide range of applications in industries like gaming, augmented reality, and
digital content creation. However, state-of-the-art methods still fail when the
geometry between the input and appearance objects is significantly different. A
straightforward approach is to directly apply a 3D generative model, but we
show that this ultimately fails to produce appealing results. Instead, we
propose a principled approach inspired by universal guidance. Given a
pretrained rectified flow model conditioned on image or text, our training-free
method interacts with the sampling process by periodically adding guidance.
This guidance can be modeled as a differentiable loss function, and we
experiment with two different types of guidance including part-aware losses for
appearance and self-similarity. Our experiments show that our approach
successfully transfers texture and geometric details to the input 3D asset,
outperforming baselines both qualitatively and quantitatively. We also show
that traditional metrics are not suitable for evaluating the task due to their
inability of focusing on local details and comparing dissimilar inputs, in
absence of ground truth data. We thus evaluate appearance transfer quality with
a GPT-based system objectively ranking outputs, ensuring robust and human-like
assessment, as further confirmed by our user study. Beyond showcased scenarios,
our method is general and could be extended to different types of diffusion
models and guidance functions.

</details>


### [29] [C-arm Guidance: A Self-supervised Approach To Automated Positioning During Stroke Thrombectomy](https://arxiv.org/abs/2510.16145)
*Ahmad Arrabi,Jay hwasung Jung,J Le,A Nguyen,J Reed,E Stahl,Nathan Franssen,Scott Raymond,Safwan Wshah*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的自监督框架，旨在提高血栓切除术的效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 血栓切除术是治疗缺血性卒中最有效的方法之一，但它需要大量的资源和人员。

Method: 使用基于回归的pretext task来分类各种骨骼landmark。

Result: 该模型在回归和分类任务中均优于现有方法，并且位置pretext task显著提高了下游分类性能。

Conclusion: 该框架可以扩展到完全自主的C型臂控制，从而优化中风血栓切除术过程中从骨盆到头部的轨迹。

Abstract: Thrombectomy is one of the most effective treatments for ischemic stroke, but
it is resource and personnel-intensive. We propose employing deep learning to
automate critical aspects of thrombectomy, thereby enhancing efficiency and
safety. In this work, we introduce a self-supervised framework that classifies
various skeletal landmarks using a regression-based pretext task. Our
experiments demonstrate that our model outperforms existing methods in both
regression and classification tasks. Notably, our results indicate that the
positional pretext task significantly enhances downstream classification
performance. Future work will focus on extending this framework toward fully
autonomous C-arm control, aiming to optimize trajectories from the pelvis to
the head during stroke thrombectomy procedures. All code used is available at
https://github.com/AhmadArrabi/C_arm_guidance

</details>


### [30] [DuetMatch: Harmonizing Semi-Supervised Brain MRI Segmentation via Decoupled Branch Optimization](https://arxiv.org/abs/2510.16146)
*Thanh-Huy Nguyen,Hoang-Thien Nguyen,Vi Vu,Ba-Thinh Lam,Phat Huynh,Tianyang Wang,Xingjian Li,Ulas Bagci,Min Xu*

Main category: cs.CV

TL;DR: DuetMatch: A dual-branch semi-supervised framework with asynchronous optimization for medical image segmentation.


<details>
  <summary>Details</summary>
Motivation: Jointly optimizing the entire network can hinder convergence and stability, especially in challenging scenarios with limited annotated data.

Method: A dual-branch semi-supervised framework with asynchronous optimization, Decoupled Dropout Perturbation, Pair-wise CutMix Cross-Guidance, and Consistency Matching.

Result: DuetMatch consistently outperforms state-of-the-art methods on benchmark brain MRI segmentation datasets.

Conclusion: DuetMatch demonstrates its effectiveness and robustness across diverse semi-supervised segmentation scenarios.

Abstract: The limited availability of annotated data in medical imaging makes
semi-supervised learning increasingly appealing for its ability to learn from
imperfect supervision. Recently, teacher-student frameworks have gained
popularity for their training benefits and robust performance. However, jointly
optimizing the entire network can hinder convergence and stability, especially
in challenging scenarios. To address this for medical image segmentation, we
propose DuetMatch, a novel dual-branch semi-supervised framework with
asynchronous optimization, where each branch optimizes either the encoder or
decoder while keeping the other frozen. To improve consistency under noisy
conditions, we introduce Decoupled Dropout Perturbation, enforcing
regularization across branches. We also design Pair-wise CutMix Cross-Guidance
to enhance model diversity by exchanging pseudo-labels through augmented input
pairs. To mitigate confirmation bias from noisy pseudo-labels, we propose
Consistency Matching, refining labels using stable predictions from frozen
teacher models. Extensive experiments on benchmark brain MRI segmentation
datasets, including ISLES2022 and BraTS, show that DuetMatch consistently
outperforms state-of-the-art methods, demonstrating its effectiveness and
robustness across diverse semi-supervised segmentation scenarios.

</details>


### [31] [Automated C-Arm Positioning via Conformal Landmark Localization](https://arxiv.org/abs/2510.16160)
*Ahmad Arrabi,Jay Hwasung Jung,Jax Luo,Nathan Franssen,Scott Raymond,Safwan Wshah*

Main category: cs.CV

TL;DR: 本文提出了一种自主导航C形臂的管线，利用X射线图像将C形臂导航到预定义的解剖标志物。


<details>
  <summary>Details</summary>
Motivation: 临床工作流程依赖于手动对齐，这会增加辐射暴露和手术延迟。

Method: 该模型预测一个朝向身体上每个目标标志物的3D位移向量。为了确保可靠的部署，我们捕获模型预测中的不确定性，并使用共形预测对其进行校准。训练框架将概率损失与骨骼姿势正则化相结合，以鼓励解剖学上合理的输出。

Result: 在DeepDRR生成的合成X射线数据集上验证了该方法。结果表明，不仅在多种架构上具有很强的定位精度，而且具有良好校准的预测界限。

Conclusion: 这些发现突出了该管线作为安全可靠的自主C形臂系统组件的潜力。

Abstract: Accurate and reliable C-arm positioning is essential for fluoroscopy-guided
interventions. However, clinical workflows rely on manual alignment that
increases radiation exposure and procedural delays. In this work, we present a
pipeline that autonomously navigates the C-arm to predefined anatomical
landmarks utilizing X-ray images. Given an input X-ray image from an arbitrary
starting location on the operating table, the model predicts a 3D displacement
vector toward each target landmark along the body. To ensure reliable
deployment, we capture both aleatoric and epistemic uncertainties in the
model's predictions and further calibrate them using conformal prediction. The
derived prediction regions are interpreted as 3D confidence regions around the
predicted landmark locations. The training framework combines a probabilistic
loss with skeletal pose regularization to encourage anatomically plausible
outputs. We validate our approach on a synthetic X-ray dataset generated from
DeepDRR. Results show not only strong localization accuracy across multiple
architectures but also well-calibrated prediction bounds. These findings
highlight the pipeline's potential as a component in safe and reliable
autonomous C-arm systems. Code is available at
https://github.com/AhmadArrabi/C_arm_guidance_APAH

</details>


### [32] [Cost Savings from Automatic Quality Assessment of Generated Images](https://arxiv.org/abs/2510.16179)
*Xavier Giro-i-Nieto,Nefeli Andreou,Anqi Liang,Manel Baradad,Francesc Moreno-Noguer,Aleix Martinez*

Main category: cs.CV

TL;DR: 提出了一种自动预过滤方法，以提高生成图像的质量，从而降低图像质量评估（IQA）的成本。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型的质量未达到传统摄影方法标准，导致IQA过程成本高昂。

Method: 提出一个公式来估计成本节省，该公式依赖于通用IQA引擎的精度和通过率。

Result: 在一个背景修复的用例中，使用简单的AutoML解决方案实现了51.61%的显著成本节省。

Conclusion: 通过引入自动预过滤阶段，可以显著降低获得高质量图像所需的平均成本。

Abstract: Deep generative models have shown impressive progress in recent years, making
it possible to produce high quality images with a simple text prompt or a
reference image. However, state of the art technology does not yet meet the
quality standards offered by traditional photographic methods. For this reason,
production pipelines that use generated images often include a manual stage of
image quality assessment (IQA). This process is slow and expensive, especially
because of the low yield of automatically generated images that pass the
quality bar. The IQA workload can be reduced by introducing an automatic
pre-filtering stage, that will increase the overall quality of the images sent
to review and, therefore, reduce the average cost required to obtain a high
quality image. We present a formula that estimates the cost savings depending
on the precision and pass yield of a generic IQA engine. This formula is
applied in a use case of background inpainting, showcasing a significant cost
saving of 51.61% obtained with a simple AutoML solution.

</details>


### [33] [Seeing Through the Brain: New Insights from Decoding Visual Stimuli with fMRI](https://arxiv.org/abs/2510.16196)
*Zheng Huang,Enpei Zhang,Yinghao Cai,Weikang Qiu,Carl Yang,Elynn Chen,Xiang Zhang,Rex Ying,Dawei Zhou,Yujun Yan*

Main category: cs.CV

TL;DR: 该论文提出了一种新的视觉刺激重建模型PRISM，该模型利用结构化文本空间作为中间表示，将fMRI信号转换为图像。


<details>
  <summary>Details</summary>
Motivation: 理解大脑如何编码视觉信息是神经科学和机器学习领域的核心挑战。利用fMRI信号重建视觉刺激是一种有前景的方法，但重建质量取决于潜在空间与神经活动结构的相似性以及生成模型生成图像的能力。

Method: 该论文提出PRISM模型，该模型将fMRI信号投影到结构化文本空间，并包含一个以对象为中心的扩散模块和一个属性关系搜索模块，以捕捉视觉刺激的组合性质。

Result: 在真实世界数据集上的大量实验表明，PRISM框架优于现有方法，感知损失降低了8%。

Conclusion: 该研究结果强调了使用结构化文本作为桥梁fMRI信号和图像重建的中间空间的重要性。

Abstract: Understanding how the brain encodes visual information is a central challenge
in neuroscience and machine learning. A promising approach is to reconstruct
visual stimuli, essentially images, from functional Magnetic Resonance Imaging
(fMRI) signals. This involves two stages: transforming fMRI signals into a
latent space and then using a pretrained generative model to reconstruct
images. The reconstruction quality depends on how similar the latent space is
to the structure of neural activity and how well the generative model produces
images from that space. Yet, it remains unclear which type of latent space best
supports this transformation and how it should be organized to represent visual
stimuli effectively. We present two key findings. First, fMRI signals are more
similar to the text space of a language model than to either a vision based
space or a joint text image space. Second, text representations and the
generative model should be adapted to capture the compositional nature of
visual stimuli, including objects, their detailed attributes, and
relationships. Building on these insights, we propose PRISM, a model that
Projects fMRI sIgnals into a Structured text space as an interMediate
representation for visual stimuli reconstruction. It includes an object centric
diffusion module that generates images by composing individual objects to
reduce object detection errors, and an attribute relationship search module
that automatically identifies key attributes and relationships that best align
with the neural activity. Extensive experiments on real world datasets
demonstrate that our framework outperforms existing methods, achieving up to an
8% reduction in perceptual loss. These results highlight the importance of
using structured text as the intermediate space to bridge fMRI signals and
image reconstruction.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [34] [VisuoAlign: Safety Alignment of LVLMs with Multimodal Tree Search](https://arxiv.org/abs/2510.15948)
*MingSheng Li,Guangze Zhao,Sichen Liu*

Main category: cs.AI

TL;DR: 本文提出了一种名为 VisuoAlign 的框架，用于多模态安全对齐，通过提示引导的树搜索来提高 LVLMs 的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有 LVLMs 在多模态感知和生成方面取得了显著进展，但其安全性对齐仍然是一个关键挑战。视觉输入引入了新的攻击面，推理链缺乏安全监督，并且对齐通常在模态融合下会降低。

Method: VisuoAlign 通过视觉-文本交互提示将安全约束嵌入到推理过程中，采用蒙特卡洛树搜索 (MCTS) 来系统地构建多样化的安全关键提示轨迹，并引入基于提示的缩放以确保实时风险检测和合规响应。

Result: 大量实验表明，VisuoAlign 主动暴露风险，实现全面的数据集生成，并显著提高了 LVLMs 抵抗复杂跨模态威胁的鲁棒性。

Conclusion: VisuoAlign 框架能够有效提高 LVLMs 的安全性，并为未来的研究提供了新的方向。

Abstract: Large Vision-Language Models (LVLMs) have achieved remarkable progress in
multimodal perception and generation, yet their safety alignment remains a
critical challenge.Existing defenses and vulnerable to multimodal jailbreaks,
as visual inputs introduce new attack surfaces, reasoning chains lack safety
supervision, and alignment often degrades under modality fusion.To overcome
these limitation, we propose VisuoAlign, a framework for multi-modal safety
alignment via prompt-guided tree search.VisuoAlign embeds safety constrains
into the reasoning process through visual-textual interactive prompts, employs
Monte Carlo Tree Search(MCTS) to systematically construct diverse
safety-critical prompt trajectories, and introduces prompt-based scaling to
ensure real-time risk detection and compliant responses.Extensive experiments
demonstrate that VisuoAlign proactively exposes risks, enables comprehensive
dataset generation, and significantly improves the robustness of LVLMs against
complex cross-modal threats.

</details>


### [35] [Executable Epistemology: The Structured Cognitive Loop as an Architecture of Intentional Understanding](https://arxiv.org/abs/2510.15952)
*Myung Ho Kim*

Main category: cs.AI

TL;DR: 大型语言模型缺乏真正的认知理解能力。本文提出了结构化认知循环（SCL）框架，旨在弥补这一差距。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型虽然表现出智能，但缺乏真正的认知理解。本文旨在通过构建认知架构来解决这个问题。

Method: 本文基于心灵哲学和认知现象学，结合过程哲学、具身认知和扩展心智理论，提出了结构化认知循环（SCL）框架。该框架将智能定义为一个持续的判断、记忆、控制、行动和调节的循环过程。

Result: SCL框架通过功能分离实现了比传统基于prompt的系统更连贯和可解释的行为。它将知识定义为在现象学连贯的循环中持续重建的过程。

Conclusion: 本文提出的SCL框架为心灵哲学、认识论和人工智能领域带来了影响。它强调认知架构的重要性，并认为真正的进展需要实现认知原则的结构，而不是更大的模型。

Abstract: Large language models exhibit intelligence without genuine epistemic
understanding, exposing a key gap: the absence of epistemic architecture. This
paper introduces the Structured Cognitive Loop (SCL) as an executable
epistemological framework for emergent intelligence. Unlike traditional AI
research asking "what is intelligence?" (ontological), SCL asks "under what
conditions does cognition emerge?" (epistemological). Grounded in philosophy of
mind and cognitive phenomenology, SCL bridges conceptual philosophy and
implementable cognition. Drawing on process philosophy, enactive cognition, and
extended mind theory, we define intelligence not as a property but as a
performed process -- a continuous loop of judgment, memory, control, action,
and regulation. SCL makes three contributions. First, it operationalizes
philosophical insights into computationally interpretable structures, enabling
"executable epistemology" -- philosophy as structural experiment. Second, it
shows that functional separation within cognitive architecture yields more
coherent and interpretable behavior than monolithic prompt based systems,
supported by agent evaluations. Third, it redefines intelligence: not
representational accuracy but the capacity to reconstruct its own epistemic
state through intentional understanding. This framework impacts philosophy of
mind, epistemology, and AI. For philosophy, it allows theories of cognition to
be enacted and tested. For AI, it grounds behavior in epistemic structure
rather than statistical regularity. For epistemology, it frames knowledge not
as truth possession but as continuous reconstruction within a
phenomenologically coherent loop. We situate SCL within debates on cognitive
phenomenology, emergence, normativity, and intentionality, arguing that real
progress requires not larger models but architectures that realize cognitive
principles structurally.

</details>


### [36] [Exploring the Potential of Citiverses for Regulatory Learning](https://arxiv.org/abs/2510.15959)
*Isabelle Hupont,Marisa Ponti,Sven Schade*

Main category: cs.AI

TL;DR: 探索citiverse作为监管学习的实验空间


<details>
  <summary>Details</summary>
Motivation: Citiverse可以通过提供沉浸式虚拟环境来支持监管学习，从而可以进行策略场景和技术实验。

Method: 通过与高级专家小组（包括欧盟委员会的政策制定者、国家政府科学顾问以及数字监管和虚拟世界领域的领先研究人员）协商，提出了一个科学政策议程。

Result: 确定了关键研究领域，包括可扩展性、实时反馈、复杂性建模、跨境合作、降低风险、公民参与、伦理考量以及新兴技术的整合。分析了一系列实验主题，涵盖交通运输、城市规划和环境/气候危机，这些主题可以在citiverse平台中进行测试，以推进这些领域的监管学习。

Conclusion: 为了将citiverse整合到更广泛的实验空间生态系统中，包括试验台、生活实验室和监管沙箱，探索了必要的初步步骤，并强调了以负责任的方式开发和使用citiverse。

Abstract: Citiverses hold the potential to support regulatory learning by offering
immersive, virtual environments for experimenting with policy scenarios and
technologies. This paper proposes a science-for-policy agenda to explore the
potential of citiverses as experimentation spaces for regulatory learning,
grounded in a consultation with a high-level panel of experts, including
policymakers from the European Commission, national government science advisers
and leading researchers in digital regulation and virtual worlds. It identifies
key research areas, including scalability, real-time feedback, complexity
modelling, cross-border collaboration, risk reduction, citizen participation,
ethical considerations and the integration of emerging technologies. In
addition, the paper analyses a set of experimental topics, spanning
transportation, urban planning and the environment/climate crisis, that could
be tested in citiverse platforms to advance regulatory learning in these areas.
The proposed work is designed to inform future research for policy and
emphasizes a responsible approach to developing and using citiverses. It
prioritizes careful consideration of the ethical, economic, ecological and
social dimensions of different regulations. The paper also explores essential
preliminary steps necessary for integrating citiverses into the broader
ecosystems of experimentation spaces, including test beds, living labs and
regulatory sandboxes

</details>


### [37] [PISA: A Pragmatic Psych-Inspired Unified Memory System for Enhanced AI Agency](https://arxiv.org/abs/2510.15966)
*Shian Jia,Ziyang Huang,Xinbo Wang,Haofei Zhang,Mingli Song*

Main category: cs.AI

TL;DR: 提出了一种新的记忆系统PISA，它具有适应性和建构性，可以提高检索的准确性和效率，并在LOCOMO和AggQA基准测试中取得了新的state-of-the-art。


<details>
  <summary>Details</summary>
Motivation: 现有的记忆系统缺乏对不同任务的适应性，并且忽略了AI agent记忆的建构性和面向任务的角色。

Method: PISA引入了一种三模态适应机制（即模式更新、模式演化和模式创建），该机制在支持灵活的记忆更新的同时，保持了连贯的组织。此外，还设计了一种混合记忆访问架构，该架构将符号推理与神经检索无缝集成。

Result: 在LOCOMO和新提出的AggQA基准测试中，PISA都达到了新的state-of-the-art，显著提高了适应性和长期知识保留能力。

Conclusion: PISA是一种有效的记忆系统，它可以提高AI agent的适应性和长期知识保留能力。

Abstract: Memory systems are fundamental to AI agents, yet existing work often lacks
adaptability to diverse tasks and overlooks the constructive and task-oriented
role of AI agent memory. Drawing from Piaget's theory of cognitive development,
we propose PISA, a pragmatic, psych-inspired unified memory system that
addresses these limitations by treating memory as a constructive and adaptive
process. To enable continuous learning and adaptability, PISA introduces a
trimodal adaptation mechanism (i.e., schema updation, schema evolution, and
schema creation) that preserves coherent organization while supporting flexible
memory updates. Building on these schema-grounded structures, we further design
a hybrid memory access architecture that seamlessly integrates symbolic
reasoning with neural retrieval, significantly improving retrieval accuracy and
efficiency. Our empirical evaluation, conducted on the existing LOCOMO
benchmark and our newly proposed AggQA benchmark for data analysis tasks,
confirms that PISA sets a new state-of-the-art by significantly enhancing
adaptability and long-term knowledge retention.

</details>


### [38] [DeepAnalyze: Agentic Large Language Models for Autonomous Data Science](https://arxiv.org/abs/2510.16872)
*Shaolei Zhang,Ju Fan,Meihao Fan,Guoliang Li,Xiaoyong Du*

Main category: cs.AI

TL;DR: DeepAnalyze-8B，首个为自动数据科学设计的Agentic LLM，能够自动完成从数据源到分析师级别的深度研究报告的端到端流程。


<details>
  <summary>Details</summary>
Motivation: 现有基于工作流的数据代理在实现完全自主的数据科学方面存在根本性限制，因为它们依赖于预定义的工作流程。因此，我们需要一个能够自主完成数据科学流程的Agentic LLM。

Method: 提出了一种基于课程的Agentic训练范例，该范例模仿了人类数据科学家的学习轨迹，使LLM能够在实际环境中逐步获取和整合多种能力。同时，引入了一个数据驱动的轨迹合成框架，用于构建高质量的训练数据。

Result: DeepAnalyze仅用8B参数，就胜过了之前构建在最先进的专有LLM之上的，基于工作流程的代理。

Conclusion: DeepAnalyze的模型、代码和训练数据已开源，为实现自主数据科学铺平了道路。

Abstract: Autonomous data science, from raw data sources to analyst-grade deep research
reports, has been a long-standing challenge, and is now becoming feasible with
the emergence of powerful large language models (LLMs). Recent workflow-based
data agents have shown promising results on specific data tasks but remain
fundamentally limited in achieving fully autonomous data science due to their
reliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B,
the first agentic LLM designed for autonomous data science, capable of
automatically completing the end-toend pipeline from data sources to
analyst-grade deep research reports. To tackle high-complexity data science
tasks, we propose a curriculum-based agentic training paradigm that emulates
the learning trajectory of human data scientists, enabling LLMs to
progressively acquire and integrate multiple capabilities in real-world
environments. We also introduce a data-grounded trajectory synthesis framework
that constructs high-quality training data. Through agentic training,
DeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data
question answering and specialized analytical tasks to open-ended data
research. Experiments demonstrate that, with only 8B parameters, DeepAnalyze
outperforms previous workflow-based agents built on most advanced proprietary
LLMs. The model, code, and training data of DeepAnalyze are open-sourced,
paving the way toward autonomous data science.

</details>


### [39] [Limits of Emergent Reasoning of Large Language Models in Agentic Frameworks for Deterministic Games](https://arxiv.org/abs/2510.15974)
*Chris Su,Harrison Li,Matheus Marques,George Flint,Kevin Zhu,Sunishchal Dev*

Main category: cs.AI

TL;DR: 大型推理模型在解决谜题时，性能会随着困惑度的增加而下降。本研究表明，即使为大型语言模型提供汉诺塔问题的环境接口，允许其通过工具调用进行移动、提供书面理由、观察结果状态空间并重新提示自己下一步行动，性能崩溃的现象仍然存在。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在解决谜题时性能崩溃的原因，并研究环境接口是否能改善这一问题。

Method: 为大型语言模型提供汉诺塔问题的环境接口，允许其通过工具调用进行移动、提供书面理由、观察结果状态空间并重新提示自己下一步行动。通过分析模型参数化的策略，研究其与最优策略和均匀随机策略的差异。

Result: 即使提供环境接口，大型语言模型的性能崩溃现象仍然存在。模型参数化的策略分析显示，模型在每个复杂度级别上都表现出类似模式的崩溃，并且性能取决于该模式是否反映了问题的正确解决方案。

Conclusion: 大型语言模型在解决谜题时存在性能崩溃现象，这可能与模型在每个复杂度级别上都表现出类似模式的崩溃有关。

Abstract: Recent work reports that Large Reasoning Models (LRMs) undergo a collapse in
performance on solving puzzles beyond certain perplexity thresholds. In
subsequent discourse, questions have arisen as to whether the nature of the
task muddles an evaluation of true reasoning. One potential confound is the
requirement that the model keep track of the state space on its own. We provide
a large language model (LLM) with an environment interface for Tower of Hanoi
problems, allowing it to make a move with a tool call, provide written
justification, observe the resulting state space, and reprompt itself for the
next move. We observe that access to an environment interface does not delay or
eradicate performance collapse. Furthermore, LLM-parameterized policy analysis
reveals increasing divergence from both optimal policies and uniformly random
policies, suggesting that the model exhibits mode-like collapse at each level
of complexity, and that performance is dependent upon whether the mode reflects
the correct solution for the problem. We suggest that a similar phenomena might
take place in LRMs.

</details>


### [40] [Cognitive Load Traces as Symbolic and Visual Accounts of Deep Model Cognition](https://arxiv.org/abs/2510.15980)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: 提出了认知负荷追踪(CLT)，一个受人类认知中认知负荷理论启发的深度模型的中级可解释性框架。


<details>
  <summary>Details</summary>
Motivation: 量化模型内部资源分配。

Method: CLT被表示为一个三组分的随机过程(IL_t, EL_t, GL_t)，对应于内在、外在和相关负荷。每个组件通过可测量的代理实例化，例如注意力熵，KV-cache未命中率，表示分散性和解码稳定性。

Result: CLT预测误差开始，揭示认知策略，并实现负载引导干预，在保持精度的同时，将推理效率提高15-30%。

Conclusion: CLT可以有效地提高推理效率。

Abstract: We propose \textbf{Cognitive Load Traces} (CLTs) as a mid-level
interpretability framework for deep models, inspired by Cognitive Load Theory
in human cognition. CLTs are defined as symbolic, temporally varying functions
that quantify model-internal resource allocation. Formally, we represent CLTs
as a three-component stochastic process $(\mathrm{IL}_t, \mathrm{EL}_t,
\mathrm{GL}_t)$, corresponding to \emph{Intrinsic}, \emph{Extraneous}, and
\emph{Germane} load. Each component is instantiated through measurable proxies
such as attention entropy, KV-cache miss ratio, representation dispersion, and
decoding stability. We propose both symbolic formulations and visualization
methods (load curves, simplex diagrams) that enable interpretable analysis of
reasoning dynamics. Experiments on reasoning and planning benchmarks show that
CLTs predict error-onset, reveal cognitive strategies, and enable load-guided
interventions that improve reasoning efficiency by 15-30\% while maintaining
accuracy.

</details>


### [41] [ProofFlow: A Dependency Graph Approach to Faithful Proof Autoformalization](https://arxiv.org/abs/2510.15981)
*Rafael Cabral,Tuan Manh Do,Xuejun Yu,Wai Ming Tai,Zijin Feng,Xin Shen*

Main category: cs.AI

TL;DR: ProofFlow是一个新的自动形式化pipeline，它优先考虑结构保真度，并使用DAG图和基于引理的方法来形式化证明步骤。


<details>
  <summary>Details</summary>
Motivation: 当前的方法无法保持语义意义和原始论证的逻辑结构。

Method: 首先构建一个有向无环图（DAG）来映射证明步骤之间的逻辑依赖关系。然后，它采用一种新的基于引理的方法来系统地将每个步骤形式化为中间引理，从而保留原始论证的逻辑结构。

Result: ProofFlow在自动形式化方面达到了新的state-of-the-art，ProofScore为0.545，大大超过了full-proof formalization (0.123) 和 step-proof formalization (0.072)。

Conclusion: ProofFlow pipeline，benchmark和评分指标已开源，以鼓励进一步发展。

Abstract: Proof autoformalization, the task of translating natural language theorems
and proofs into machine-verifiable code, is a critical step for integrating
large language models into rigorous mathematical workflows. Current approaches
focus on producing executable code, but they frequently fail to preserve the
semantic meaning and logical structure of the original human-written argument.
To address this, we introduce ProofFlow, a novel pipeline that treats
structural fidelity as a primary objective. ProofFlow first constructs a
directed acyclic graph (DAG) to map the logical dependencies between proof
steps. Then, it employs a novel lemma-based approach to systematically
formalize each step as an intermediate lemma, preserving the logical structure
of the original argument. To facilitate evaluation, we present a new benchmark
of 184 undergraduate-level problems, manually annotated with step-by-step
solutions and logical dependency graphs, and introduce ProofScore, a new
composite metric to evaluate syntactic correctness, semantic faithfulness, and
structural fidelity. Experimental results show our pipeline sets a new
state-of-the-art for autoformalization, achieving a ProofScore of 0.545,
substantially exceeding baselines like full-proof formalization (0.123), which
processes the entire proof at once, and step-proof formalization (0.072), which
handles each step independently. Our pipeline, benchmark, and score metric are
open-sourced to encourage further progress at
https://github.com/Huawei-AI4Math/ProofFlow.

</details>


### [42] [Ontologies in Motion: A BFO-Based Approach to Knowledge Graph Construction for Motor Performance Research Data in Sports Science](https://arxiv.org/abs/2510.15983)
*Sarah Rebecca Ondraszek,Jörg Waitelonis,Katja Keller,Claudia Niessner,Anna M. Jacyszyn,Harald Sack*

Main category: cs.AI

TL;DR: 本文介绍了卡尔斯鲁厄理工学院开发的运动研究 (MO|RE) 数据存储库，该存储库用于发布和存档运动科学领域的研究数据，特别是运动表现研究领域的数据。


<details>
  <summary>Details</summary>
Motivation: 为了评估和比较不同人群之间的身体和认知能力，测试与人类表现相关的各种因素至关重要。作为运动科学研究的核心部分，运动表现测试能够分析不同人口群体的身体健康状况，并使它们具有可比性。

Method: 本文提出了一种从 MO|RE 数据创建知识图谱的愿景。该方法以植根于基本形式本体的本体为中心，正式表示计划规范、特定过程和相关测量之间的相互关系。

Result: 本文旨在改变运动表现数据的建模和跨研究共享方式，使其标准化和机器可理解。

Conclusion: 本文提出的想法是在莱布尼茨科学园区“研究的数字化转型” (DiTraRe) 中开发的。

Abstract: An essential component for evaluating and comparing physical and cognitive
capabilities between populations is the testing of various factors related to
human performance. As a core part of sports science research, testing motor
performance enables the analysis of the physical health of different
demographic groups and makes them comparable.
  The Motor Research (MO|RE) data repository, developed at the Karlsruhe
Institute of Technology, is an infrastructure for publishing and archiving
research data in sports science, particularly in the field of motor performance
research. In this paper, we present our vision for creating a knowledge graph
from MO|RE data. With an ontology rooted in the Basic Formal Ontology, our
approach centers on formally representing the interrelation of plan
specifications, specific processes, and related measurements. Our goal is to
transform how motor performance data are modeled and shared across studies,
making it standardized and machine-understandable. The idea presented here is
developed within the Leibniz Science Campus ``Digital Transformation of
Research'' (DiTraRe).

</details>


### [43] [A Non-overlap-based Conflict Measure for Random Permutation Sets](https://arxiv.org/abs/2510.16001)
*Ruolan Cheng,Yong Deng,Enrique Herrera-Viedma*

Main category: cs.AI

TL;DR: 本文提出了一种新的冲突度量方法，用于随机置换集（RPS）中的不确定性推理，该方法基于排序偏差重叠（RBO）并考虑了Dempster-Shafer理论（DST）的扩展。


<details>
  <summary>Details</summary>
Motivation: 在order-structured不确定信息融合中，衡量由置换质量函数表示的两条证据之间的冲突是一个紧迫的研究问题。

Method: 1.  从随机有限集（RFS）和Dempster-Shafer理论（DST）两个不同的角度，对RPS中的冲突进行了详细的分析。
2.  定义了一个受排序偏差重叠（RBO）度量启发的置换间不一致性度量。
3.  提出了一个非重叠的冲突度量方法，用于RPS。
4.  将RPS理论（RPST）视为DST的扩展，其中焦点集中的顺序信息表示定性倾向，其特征是排名靠前的元素占据更关键的位置。

Result: 提出的方法具有自然的top-weightedness属性，可以有效地从DST的角度衡量RPS之间的冲突，并且为决策者提供了权重、参数和截断深度的灵活选择。

Conclusion: 本文提出了一种有效的RPS冲突度量方法，该方法具有top-weightedness属性，并且可以灵活地选择权重、参数和截断深度。

Abstract: Random permutation set (RPS) is a new formalism for reasoning with
uncertainty involving order information. Measuring the conflict between two
pieces of evidence represented by permutation mass functions remains an urgent
research topic in order-structured uncertain information fusion. In this paper,
a detailed analysis of conflicts in RPS is carried out from two different
perspectives: random finite set (RFS) and Dempster-Shafer theory (DST).
Starting from the observation of permutations, we first define an inconsistency
measure between permutations inspired by the rank-biased overlap(RBO) measure
and further propose a non-overlap-based conflict measure method for RPSs. This
paper regards RPS theory (RPST) as an extension of DST. The order information
newly added in focal sets indicates qualitative propensity, characterized by
top-ranked elements occupying a more critical position. Some numerical examples
are used to demonstrate the behavior and properties of the proposed conflict
measure. The proposed method not only has the natural top-weightedness property
and can effectively measure the conflict between RPSs from the DST view but
also provides decision-makers with a flexible selection of weights, parameters,
and truncated depths.

</details>


### [44] [PAINT: Parallel-in-time Neural Twins for Dynamical System Reconstruction](https://arxiv.org/abs/2510.16004)
*Andreas Radler,Vincent Seyfried,Stefan Pirker,Johannes Brandstetter,Thomas Lichtenegger*

Main category: cs.AI

TL;DR: 提出了Parallel-in-time Neural Twins (PAINT)，一种用于从测量数据中建模动态系统的方法。


<details>
  <summary>Details</summary>
Motivation: 神经代理在模拟动态系统方面表现出巨大的潜力，同时提供实时能力。神经孪生旨在创建真实系统的数字副本。神经孪生的一个关键特性是它们保持在轨道上的能力。

Method: PAINT训练一个生成神经网络来模拟状态在时间上的并行分布。在测试时，状态以滑动窗口的方式从测量数据中预测。

Result: 理论分析表明，PAINT是在轨道上的，而自回归模型通常不是。在二维湍流体动力学问题上的评估结果表明，PAINT保持在轨道上，并能以高保真度从稀疏测量数据中预测系统状态。

Conclusion: PAINT在开发保持在轨道上的神经孪生方面具有潜力，从而能够实现更准确的状态估计和决策。

Abstract: Neural surrogates have shown great potential in simulating dynamical systems,
while offering real-time capabilities. We envision Neural Twins as a
progression of neural surrogates, aiming to create digital replicas of real
systems. A neural twin consumes measurements at test time to update its state,
thereby enabling context-specific decision-making. A critical property of
neural twins is their ability to remain on-trajectory, i.e., to stay close to
the true system state over time. We introduce Parallel-in-time Neural Twins
(PAINT), an architecture-agnostic family of methods for modeling dynamical
systems from measurements. PAINT trains a generative neural network to model
the distribution of states parallel over time. At test time, states are
predicted from measurements in a sliding window fashion. Our theoretical
analysis shows that PAINT is on-trajectory, whereas autoregressive models
generally are not. Empirically, we evaluate our method on a challenging
two-dimensional turbulent fluid dynamics problem. The results demonstrate that
PAINT stays on-trajectory and predicts system states from sparse measurements
with high fidelity. These findings underscore PAINT's potential for developing
neural twins that stay on-trajectory, enabling more accurate state estimation
and decision-making.

</details>


### [45] [Global-focal Adaptation with Information Separation for Noise-robust Transfer Fault Diagnosis](https://arxiv.org/abs/2510.16033)
*Junyu Ren,Wensheng Gan,Guangyu Zhang,Wei Zhong,Philip S. Yu*

Main category: cs.AI

TL;DR: ISGFAN: A robust cross-domain fault diagnosis framework using information separation and global-focal adversarial learning to handle noise and domain shifts.


<details>
  <summary>Details</summary>
Motivation: Existing transfer fault diagnosis methods are limited by clean data or sufficient domain similarity, which is not realistic in industrial environments with noise and domain shifts.

Method: An information separation global-focal adversarial network (ISGFAN) is proposed, which decouples domain-invariant fault representation using adversarial learning and orthogonal loss. It also uses a global-focal domain-adversarial scheme to constrain conditional and marginal distributions.

Result: Experiments on three public datasets show that ISGFAN outperforms existing methods.

Conclusion: The proposed ISGFAN framework is superior for cross-domain fault diagnosis under noise conditions.

Abstract: Existing transfer fault diagnosis methods typically assume either clean data
or sufficient domain similarity, which limits their effectiveness in industrial
environments where severe noise interference and domain shifts coexist. To
address this challenge, we propose an information separation global-focal
adversarial network (ISGFAN), a robust framework for cross-domain fault
diagnosis under noise conditions. ISGFAN is built on an information separation
architecture that integrates adversarial learning with an improved orthogonal
loss to decouple domain-invariant fault representation, thereby isolating noise
interference and domain-specific characteristics. To further strengthen
transfer robustness, ISGFAN employs a global-focal domain-adversarial scheme
that constrains both the conditional and marginal distributions of the model.
Specifically, the focal domain-adversarial component mitigates
category-specific transfer obstacles caused by noise in unsupervised scenarios,
while the global domain classifier ensures alignment of the overall
distribution. Experiments conducted on three public benchmark datasets
demonstrate that the proposed method outperforms other prominent existing
approaches, confirming the superiority of the ISGFAN framework. Data and code
are available at https://github.com/JYREN-Source/ISGFAN

</details>


### [46] [Algorithms for dynamic scheduling in manufacturing, towards digital factories Improving Deadline Feasibility and Responsiveness via Temporal Networks](https://arxiv.org/abs/2510.16047)
*Ioan Hedea*

Main category: cs.AI

TL;DR: 这篇论文提出了一种混合方法，结合了离线约束编程（CP）优化和在线时间网络执行，以创建在最坏情况的不确定性下保持可行的调度方案。


<details>
  <summary>Details</summary>
Motivation: 传统确定性调度在实际情况偏离标称计划时会失效，导致代价高昂的临时修复。现代制造系统必须满足严格的交货期限，同时应对由过程噪声、设备可变性和人为干预引起的随机任务持续时间。

Method: 首先，构建了一个具有每项作业截止日期任务的灵活作业车间的CP模型，并插入一个最佳缓冲区 Δ* 以获得完全主动的基线。然后，将结果计划转换为具有不确定性的简单时间网络（STNU）并验证动态可控性，这保证了实时调度程序可以为每个有界持续时间实现重新定时活动，而不会违反资源或截止日期约束。

Result: 在Kacem 1-4基准测试套件上进行的大量蒙特卡罗模拟表明，该混合方法消除了最先进的元启发式调度中观察到的100％的截止日期冲突，同时仅增加了3-5％的makespan开销。可扩展性实验证实，在中等规模的实例上，CP求解时间和STNU检查保持在亚秒级。

Conclusion: 这项工作展示了时间网络推理如何弥合主动缓冲和动态鲁棒性之间的差距，从而使工业界朝着真正的数字化、自我校正工厂迈进了一步。

Abstract: Modern manufacturing systems must meet hard delivery deadlines while coping
with stochastic task durations caused by process noise, equipment variability,
and human intervention. Traditional deterministic schedules break down when
reality deviates from nominal plans, triggering costly last-minute repairs.
This thesis combines offline constraint-programming (CP) optimisation with
online temporal-network execution to create schedules that remain feasible
under worst-case uncertainty. First, we build a CP model of the flexible
job-shop with per-job deadline tasks and insert an optimal buffer $\Delta^*$ to
obtain a fully pro-active baseline. We then translate the resulting plan into a
Simple Temporal Network with Uncertainty (STNU) and verify dynamic
controllability, which guarantees that a real-time dispatcher can retime
activities for every bounded duration realisation without violating resource or
deadline constraints. Extensive Monte-Carlo simulations on the open Kacem~1--4
benchmark suite show that our hybrid approach eliminates 100\% of deadline
violations observed in state-of-the-art meta-heuristic schedules, while adding
only 3--5\% makespan overhead. Scalability experiments confirm that CP
solve-times and STNU checks remain sub-second on medium-size instances. The
work demonstrates how temporal-network reasoning can bridge the gap between
proactive buffering and dynamic robustness, moving industry a step closer to
truly digital, self-correcting factories.

</details>


### [47] [DTKG: Dual-Track Knowledge Graph-Verified Reasoning Framework for Multi-Hop QA](https://arxiv.org/abs/2510.16302)
*Changhao Wang,Yanfang Liu,Xinxin Fan,Anzhi Zhou,Lao Tian,Yunfeng Lu*

Main category: cs.AI

TL;DR: 提出了一种新的双轨知识图谱验证和推理框架DTKG，以解决多跳问答中并行事实验证和链式推理的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理并行事实验证和链式多跳推理方面存在局限性，影响了多跳问答任务的效率和准确性。

Method: 该框架分为分类阶段和分支处理阶段。

Result: 未提及

Conclusion: 未提及

Abstract: Multi-hop reasoning for question answering (QA) plays a critical role in
retrieval-augmented generation (RAG) for modern large language models (LLMs).
The accurate answer can be obtained through retrieving relational structure of
entities from knowledge graph (KG). Regarding the inherent relation-dependency
and reasoning pattern, multi-hop reasoning can be in general classified into
two categories: i) parallel fact-verification multi-hop reasoning question,
i.e., requiring simultaneous verifications of multiple independent
sub-questions; and ii) chained multi-hop reasoning questions, i.e., demanding
sequential multi-step inference with intermediate conclusions serving as
essential premises for subsequent reasoning. Currently, the multi-hop reasoning
approaches singly employ one of two techniques: LLM response-based fact
verification and KG path-based chain construction. Nevertheless, the former
excels at parallel fact-verification but underperforms on chained reasoning
tasks, while the latter demonstrates proficiency in chained multi-hop reasoning
but suffers from redundant path retrieval when handling parallel
fact-verification reasoning. These limitations deteriorate the efficiency and
accuracy for multi-hop QA tasks. To address this challenge, we propose a novel
dual-track KG verification and reasoning framework DTKG, which is inspired by
the Dual Process Theory in cognitive science. Specifically, DTKG comprises two
main stages: the Classification Stage and the Branch Processing Stage.

</details>


### [48] [Reliability of Large Language Model Generated Clinical Reasoning in Assisted Reproductive Technology: Blinded Comparative Evaluation Study](https://arxiv.org/abs/2510.16095)
*Dou Liu,Ying Long,Sophia Zuoqiu,Di Liu,Kang Li,Yiting Lin,Hanyi Liu,Rong Yin,Tian Tang*

Main category: cs.AI

TL;DR: 本研究评估了大型语言模型 (LLM) 生成的临床思维链 (CoT) 的可靠性，并探讨了提高其质量的提示策略。


<details>
  <summary>Details</summary>
Motivation: 在数据稀缺的情况下，创建高质量的临床思维链 (CoT) 对于可解释的医疗人工智能 (AI) 至关重要。虽然大型语言模型 (LLM) 可以合成医疗数据，但其临床可靠性仍未得到验证。

Method: 通过一项盲法对比研究，生殖辅助技术 (ART) 的高级临床医生评估了通过三种不同的策略生成的 CoT：零样本、随机少样本（使用浅层示例）和选择性少样本（使用多样化、高质量的示例）。这些专家评分与来自最先进的 AI 模型 (GPT-4o) 的评估进行了比较。

Result: 选择性少样本策略在所有人工评估指标上均显着优于其他策略 (p < .001)。随机少样本策略并没有比零样本基线提供显着改进，这表明低质量的示例与没有示例一样无效。选择性策略的成功归因于两个原则：“金标准深度”（推理质量）和“代表性多样性”（泛化）。

Conclusion: 合成 CoT 的临床可靠性取决于策略性提示的调整，而不仅仅是示例的存在。我们提出了一个“双重原则”框架，作为大规模生成可信数据的基本方法。这项工作为数据瓶颈提供了一个经过验证的解决方案，并证实了人类专业知识在评估高风险临床 AI 中不可或缺的作用。

Abstract: Creating high-quality clinical Chains-of-Thought (CoTs) is crucial for
explainable medical Artificial Intelligence (AI) while constrained by data
scarcity. Although Large Language Models (LLMs) can synthesize medical data,
their clinical reliability remains unverified. This study evaluates the
reliability of LLM-generated CoTs and investigates prompting strategies to
enhance their quality. In a blinded comparative study, senior clinicians in
Assisted Reproductive Technology (ART) evaluated CoTs generated via three
distinct strategies: Zero-shot, Random Few-shot (using shallow examples), and
Selective Few-shot (using diverse, high-quality examples). These expert ratings
were compared against evaluations from a state-of-the-art AI model (GPT-4o).
The Selective Few-shot strategy significantly outperformed other strategies
across all human evaluation metrics (p < .001). Critically, the Random Few-shot
strategy offered no significant improvement over the Zero-shot baseline,
demonstrating that low-quality examples are as ineffective as no examples. The
success of the Selective strategy is attributed to two principles:
"Gold-Standard Depth" (reasoning quality) and "Representative Diversity"
(generalization). Notably, the AI evaluator failed to discern these critical
performance differences. The clinical reliability of synthetic CoTs is dictated
by strategic prompt curation, not the mere presence of examples. We propose a
"Dual Principles" framework as a foundational methodology to generate
trustworthy data at scale. This work offers a validated solution to the data
bottleneck and confirms the indispensable role of human expertise in evaluating
high-stakes clinical AI.

</details>


### [49] [Operationalising Extended Cognition: Formal Metrics for Corporate Knowledge and Legal Accountability](https://arxiv.org/abs/2510.16193)
*Elija Perrier*

Main category: cs.AI

TL;DR: 本文提出了一种衡量企业知识的新方法，用于评估企业在使用人工智能时的责任。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能越来越多地参与企业决策，传统的企业责任概念受到挑战。

Method: 通过扩展认知理论，将企业知识重新定义为一种动态能力，并建立了一个形式模型来捕捉企业在使用人工智能或信息系统时的认知状态。

Result: 推导出一个阈值知识谓词和一个公司范围内的认知能力指标，并将这些定量指标映射到实际知识、推定知识、故意视而不见和鲁莽等法律标准。

Conclusion: 本文为创建可衡量和可审判的审计工件提供了一条途径，使企业在算法时代能够被追踪和负责。

Abstract: Corporate responsibility turns on notions of corporate \textit{mens rea},
traditionally imputed from human agents. Yet these assumptions are under
challenge as generative AI increasingly mediates enterprise decision-making.
Building on the theory of extended cognition, we argue that in response
corporate knowledge may be redefined as a dynamic capability, measurable by the
efficiency of its information-access procedures and the validated reliability
of their outputs. We develop a formal model that captures epistemic states of
corporations deploying sophisticated AI or information systems, introducing a
continuous organisational knowledge metric $S_S(\varphi)$ which integrates a
pipeline's computational cost and its statistically validated error rate. We
derive a thresholded knowledge predicate $\mathsf{K}_S$ to impute knowledge and
a firm-wide epistemic capacity index $\mathcal{K}_{S,t}$ to measure overall
capability. We then operationally map these quantitative metrics onto the legal
standards of actual knowledge, constructive knowledge, wilful blindness, and
recklessness. Our work provides a pathway towards creating measurable and
justiciable audit artefacts, that render the corporate mind tractable and
accountable in the algorithmic age.

</details>


### [50] [Towards Automatic Evaluation and Selection of PHI De-identification Models via Multi-Agent Collaboration](https://arxiv.org/abs/2510.16194)
*Guanchen Wu,Zuhui Chen,Yuzhang Xie,Carl Yang*

Main category: cs.AI

TL;DR: TEAM-PHI: 利用大型语言模型自动评估和选择PHI去标识模型，减少对人工标注的依赖。


<details>
  <summary>Details</summary>
Motivation: 评估和比较PHI去标识模型通常依赖于昂贵的小规模专家标注。

Method: 使用多代理评估框架TEAM-PHI，该框架使用大型语言模型自动评估去标识质量，并通过基于LLM的多数投票机制整合评估结果。

Result: 实验表明，TEAM-PHI能够产生一致且准确的排名，并且与人工评估结果高度吻合。

Conclusion: TEAM-PHI为PHI去标识的自动评估和最佳模型选择提供了一种实用、安全且经济高效的解决方案，即使在ground-truth标签有限的情况下也是如此。

Abstract: Protected health information (PHI) de-identification is critical for enabling
the safe reuse of clinical notes, yet evaluating and comparing PHI
de-identification models typically depends on costly, small-scale expert
annotations. We present TEAM-PHI, a multi-agent evaluation and selection
framework that uses large language models (LLMs) to automatically measure
de-identification quality and select the best-performing model without heavy
reliance on gold labels. TEAM-PHI deploys multiple Evaluation Agents, each
independently judging the correctness of PHI extractions and outputting
structured metrics. Their results are then consolidated through an LLM-based
majority voting mechanism that integrates diverse evaluator perspectives into a
single, stable, and reproducible ranking. Experiments on a real-world clinical
note corpus demonstrate that TEAM-PHI produces consistent and accurate
rankings: despite variation across individual evaluators, LLM-based voting
reliably converges on the same top-performing systems. Further comparison with
ground-truth annotations and human evaluation confirms that the framework's
automated rankings closely match supervised evaluation. By combining
independent evaluation agents with LLM majority voting, TEAM-PHI offers a
practical, secure, and cost-effective solution for automatic evaluation and
best-model selection in PHI de-identification, even when ground-truth labels
are limited.

</details>


### [51] [The Right to Be Remembered: Preserving Maximally Truthful Digital Memory in the Age of AI](https://arxiv.org/abs/2510.16206)
*Alex Zhavoronkov,Dominika Wilczok,Roman Yampolskiy*

Main category: cs.AI

TL;DR: 大型语言模型（LLM）在信息检索中被广泛使用，但它们可能会放大偏差和遗漏，从而不成比例地抑制某些叙述、个人或群体，同时不成比例地提升其他叙述、个人或群体。这可能会导致数字存在有限的人逐渐被抹去，而已经 prominent 的人则会被放大，从而重塑集体记忆。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在信息检索中的广泛使用可能会放大偏差和遗漏，从而不成比例地抑制某些叙述、个人或群体，同时不成比例地提升其他叙述、个人或群体，从而重塑集体记忆。

Method: 提出了一种“被记住的权利”（RTBR）的概念，该概念包括最大限度地降低人工智能驱动的信息遗漏的风险，拥抱公平对待的权利，同时确保生成的内容最大限度地真实。

Result: 提出了一个概念

Conclusion: 提出了“被记住的权利”（RTBR）的概念，旨在解决人工智能驱动的信息遗漏和偏差问题，确保公平和真实的生成内容。

Abstract: Since the rapid expansion of large language models (LLMs), people have begun
to rely on them for information retrieval. While traditional search engines
display ranked lists of sources shaped by search engine optimization (SEO),
advertising, and personalization, LLMs typically provide a synthesized response
that feels singular and authoritative. While both approaches carry risks of
bias and omission, LLMs may amplify the effect by collapsing multiple
perspectives into one answer, reducing users ability or inclination to compare
alternatives. This concentrates power over information in a few LLM vendors
whose systems effectively shape what is remembered and what is overlooked. As a
result, certain narratives, individuals or groups, may be disproportionately
suppressed, while others are disproportionately elevated. Over time, this
creates a new threat: the gradual erasure of those with limited digital
presence, and the amplification of those already prominent, reshaping
collective memory.To address these concerns, this paper presents a concept of
the Right To Be Remembered (RTBR) which encompasses minimizing the risk of
AI-driven information omission, embracing the right of fair treatment, while
ensuring that the generated content would be maximally truthful.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [52] [Unified Peripartum Database with Natural-Language-to-SQL Capabilities at Udine University Hospital: Design and Prototype](https://arxiv.org/abs/2510.16388)
*Doriana Armenise,Ginevra Battello,Andrea Brunello,Lorenza Driul,Angelo Montanari,Elisa Rizzante,Nicola Saccomanno,Andrea Salvador,Serena Xodo,Silvia Zermano*

Main category: cs.DB

TL;DR: 本研究提出了一个实用的蓝图，用于将异构的围产期记录转换为可计算、可查询的资产。


<details>
  <summary>Details</summary>
Motivation: 医院中产科信息在电子健康记录模块、设备存储库和实验室系统中的碎片化阻碍了分娩过程中的护理和可重复的研究。

Method: 通过在乌迪内大学医院妇产科诊所设计和原型化一个具有自然语言到SQL (NL2SQL)能力的统一围产期关系数据库。

Result: 该数据库集成了异构来源，以连接母亲的既往史和纵向病史、当前妊娠结果、分娩过程以及分娩和新生儿结局。

Conclusion: NL2SQL层使临床医生能够向系统提出自然语言查询，降低了审计和探索性分析的障碍。

Abstract: The fragmentation of obstetric information across electronic health record
modules, device repositories, and laboratory systems, as it is common in
hospitals, hinders both intrapartum care and reproducible research. In this
work, we present a practical blueprint for transforming heterogeneous
peripartum records into computable, queryable assets by designing and
prototyping a unified peripartum relational database with
natural-language-to-SQL (NL2SQL) capabilities at the Obstetrics Clinic of Udine
University Hospital. Requirements were co-defined with clinicians and
formalized as an Entity-Relationship diagram, from which the logical schema and
SQL implementation of the database were then derived. The latter integrates
heterogeneous sources to connect maternal anamnestic and longitudinal history,
current-pregnancy findings, intrapartum course, and delivery and neonatal
outcomes. The NL2SQL layer enables clinicians to pose natural-language queries
to the system, lowering barriers to audit and exploratory analysis.

</details>


### [53] [Declarative Techniques for NL Queries over Heterogeneous Data](https://arxiv.org/abs/2510.16470)
*Elham Khabiri,Jeffrey O. Kephart,Fenno F. Heath III,Srideepika Jayaraman,Fateh A. Tipu,Yingjie Li,Dhruv Shah,Achille Fokoue,Anu Bhamidipaty*

Main category: cs.DB

TL;DR: 大型语言模型可以将自然语言问题转换成API或数据库调用，然后执行它们，并将结果组合成合适的自然语言响应。但是，这些应用程序在实际的工业环境中仍然不实用，因为它们无法处理数据源异构性。本文通过引入需要组合数据库和API调用的Spider基准数据集的两个扩展来模拟真实工业环境的异构性。然后，我们引入了一种声明式方法来处理这种数据异构性，并表明它比基于LLM的代理或命令式代码生成系统更好地处理数据源异构性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型无法处理实际工业环境中的数据源异构性

Method: 引入Spider基准数据集的两个扩展来模拟真实工业环境的异构性，并引入声明式方法来处理这种数据异构性

Result: 声明式方法比基于LLM的代理或命令式代码生成系统更好地处理数据源异构性

Conclusion: 增强的基准测试可供研究社区使用

Abstract: In many industrial settings, users wish to ask questions in natural language,
the answers to which require assembling information from diverse structured
data sources. With the advent of Large Language Models (LLMs), applications can
now translate natural language questions into a set of API calls or database
calls, execute them, and combine the results into an appropriate natural
language response. However, these applications remain impractical in realistic
industrial settings because they do not cope with the data source heterogeneity
that typifies such environments. In this work, we simulate the heterogeneity of
real industry settings by introducing two extensions of the popular Spider
benchmark dataset that require a combination of database and API calls. Then,
we introduce a declarative approach to handling such data heterogeneity and
demonstrate that it copes with data source heterogeneity significantly better
than state-of-the-art LLM-based agentic or imperative code generation systems.
Our augmented benchmarks are available to the research community.

</details>


### [54] [AVOCADO: The Streaming Process Mining Challenge](https://arxiv.org/abs/2510.17089)
*Christian Imenkamp,Andrea Maldonado,Hendrik Reiter,Martin Werner,Wilhelm Hasselbring,Agnes Koschmider,Andrea Burattin*

Main category: cs.DB

TL;DR: 提出了一种用于流式过程挖掘的标准挑战框架AVOCADO，用于算法评估。


<details>
  <summary>Details</summary>
Motivation: 为了系统地解决流式过程挖掘领域的复杂性，提出AVOCADO框架。

Method: AVOCADO框架通过分离概念层和实例化层来构建挑战，并使用流式特定的指标（如准确性、MAE、RMSE、处理延迟和鲁棒性）评估算法。

Result: AVOCADO框架旨在促进创新和社区驱动的讨论，以推进流式过程挖掘领域的发展。

Conclusion: AVOCADO框架为流式过程挖掘领域奠定了基础，并邀请社区通过建议新的挑战来贡献其发展，例如集成系统吞吐量和内存消耗的指标，并扩大范围以解决现实世界的流复杂性（如乱序事件到达）。

Abstract: Streaming process mining deals with the real-time analysis of streaming data.
Event streams require algorithms capable of processing data incrementally. To
systematically address the complexities of this domain, we propose AVOCADO, a
standardized challenge framework that provides clear structural divisions:
separating the concept and instantiation layers of challenges in streaming
process mining for algorithm evaluation. The AVOCADO evaluates algorithms on
streaming-specific metrics like accuracy, Mean Absolute Error (MAE), Root Mean
Square Error (RMSE), Processing Latency, and robustness. This initiative seeks
to foster innovation and community-driven discussions to advance the field of
streaming process mining. We present this framework as a foundation and invite
the community to contribute to its evolution by suggesting new challenges, such
as integrating metrics for system throughput and memory consumption, and
expanding the scope to address real-world stream complexities like out-of-order
event arrival.

</details>


### [55] [Comprehending Spatio-temporal Data via Cinematic Storytelling using Large Language Models](https://arxiv.org/abs/2510.17301)
*Panos Kalnis. Shuo Shang,Christian S. Jensen*

Main category: cs.DB

TL;DR: MapMuse是一个基于故事讲述的框架，用于解释时空数据集，将其转化为引人入胜的叙事体验。


<details>
  <summary>Details</summary>
Motivation: 传统的可视化方法复杂，需要领域专业知识，并且通常不能引起更广泛受众的共鸣。因此，需要一种新的方法来解释时空数据。

Method: 利用大型语言模型，并采用检索增强生成（RAG）和基于代理的技术来生成全面的故事。借鉴电影故事讲述中常见的原则，强调清晰度、情感连接和以受众为中心的设计。

Result: 通过将地点描绘成角色，将运动描绘成情节，数据故事讲述能够驱动对时空信息的洞察、参与和行动。

Conclusion: 电影故事讲述技术作为一种有效的时空数据交流工具具有潜力，并描述了未来研究的开放问题和机会。

Abstract: Spatio-temporal data captures complex dynamics across both space and time,
yet traditional visualizations are complex, require domain expertise and often
fail to resonate with broader audiences. Here, we propose MapMuse, a
storytelling-based framework for interpreting spatio-temporal datasets,
transforming them into compelling, narrative-driven experiences. We utilize
large language models and employ retrieval augmented generation (RAG) and
agent-based techniques to generate comprehensive stories. Drawing on principles
common in cinematic storytelling, we emphasize clarity, emotional connection,
and audience-centric design. As a case study, we analyze a dataset of taxi
trajectories. Two perspectives are presented: a captivating story based on a
heat map that visualizes millions of taxi trip endpoints to uncover urban
mobility patterns; and a detailed narrative following a single long taxi
journey, enriched with city landmarks and temporal shifts. By portraying
locations as characters and movement as plot, we argue that data storytelling
drives insight, engagement, and action from spatio-temporal information. The
case study illustrates how MapMuse can bridge the gap between data complexity
and human understanding. The aim of this short paper is to provide a glimpse to
the potential of the cinematic storytelling technique as an effective
communication tool for spatio-temporal data, as well as to describe open
problems and opportunities for future research.

</details>


### [56] [Approximate Nearest Neighbor Search of Large Scale Vectors on Distributed Storage](https://arxiv.org/abs/2510.17326)
*Kun Yu,Jiabao Jin,Xiaoyao Zhong,Peng Cheng,Lei Chen,Zhitao Shen,Jingkuan Song,Hengtao Shen,Xuemin Lin*

Main category: cs.DB

TL;DR: 本文提出了一种名为 DSANN 的分布式存储近似最近邻搜索系统，旨在解决现有 ANNS 算法在处理大规模向量数据时面临的存储成本高、规模受限和单点故障等问题。


<details>
  <summary>Details</summary>
Motivation: 现有 ANNS 算法在处理高维空间中的近似最近邻搜索时，面临存储成本高、规模受限和单点故障等问题；分布式存储可以提供一种经济高效且稳健的解决方案，但缺乏有效的分布式存储索引算法。

Method: DSANN 采用图聚类混合索引和搜索系统，利用并发索引构建方法降低索引构建的复杂性，并应用点聚合图来优化存储效率和提高查询吞吐量。

Result: 实验结果表明，DSANN 能够高效地索引、存储和搜索分布式存储场景中的大规模向量数据集。

Conclusion: DSANN 是一种高效且有效的分布式存储近似最近邻搜索系统，能够处理大规模向量数据，并保证索引服务的高可用性。

Abstract: Approximate Nearest Neighbor Search (ANNS) in high-dimensional space is an
essential operator in many online services, such as information retrieval and
recommendation. Indices constructed by the state-of-the-art ANNS algorithms
must be stored in single machine's memory or disk for high recall rate and
throughput, suffering from substantial storage cost, constraint of limited
scale and single point of failure. While distributed storage can provide a
cost-effective and robust solution, there is no efficient and effective
algorithms for indexing vectors in distributed storage scenarios. In this
paper, we present a new graph-cluster hybrid indexing and search system which
supports Distributed Storage Approximate Nearest Neighbor Search, called DSANN.
DSANN can efficiently index, store, search billion-scale vector database in
distributed storage and guarantee the high availability of index service. DSANN
employs the concurrent index construction method to significantly reduces the
complexity of index building. Then, DSANN applies Point Aggregation Graph to
leverage the structural information of graph to aggregate similar vectors,
optimizing storage efficiency and improving query throughput via asynchronous
I/O in distributed storage. Through extensive experiments, we demonstrate DSANN
can efficiently and effectively index, store and search large-scale vector
datasets in distributed storage scenarios.

</details>


### [57] [DeepEye-SQL: A Software-Engineering-Inspired Text-to-SQL Framework](https://arxiv.org/abs/2510.17586)
*Boyan Li,Chong Chen,Zhujun Xue,Yinan Mei,Yuyu Luo*

Main category: cs.DB

TL;DR: DeepEye-SQL: a software-engineering-inspired framework that reframes Text-to-SQL as the development of a small software program, executed through a verifiable process guided by the Software Development Life Cycle (SDLC).


<details>
  <summary>Details</summary>
Motivation: Existing Text-to-SQL solutions lack structured orchestration, hindering system-level reliability.

Method: Integrates semantic value retrieval, robust schema linking, N-version SQL generation, deterministic verification via unit tests and LLM-guided revision, and confidence-aware selection.

Result: Achieves 73.5% execution accuracy on BIRD-Dev and 89.8% on Spider-Test using ~30B open-source LLMs without fine-tuning, outperforming state-of-the-art solutions.

Conclusion: Principled orchestration, rather than LLM scaling alone, is key to achieving system-level reliability in Text-to-SQL.

Abstract: Large language models (LLMs) have advanced Text-to-SQL, yet existing
solutions still fall short of system-level reliability. The limitation is not
merely in individual modules - e.g., schema linking, reasoning, and
verification - but more critically in the lack of structured orchestration that
enforces correctness across the entire workflow. This gap motivates a paradigm
shift: treating Text-to-SQL not as free-form language generation but as a
software-engineering problem that demands structured, verifiable orchestration.
We present DeepEye-SQL, a software-engineering-inspired framework that reframes
Text-to-SQL as the development of a small software program, executed through a
verifiable process guided by the Software Development Life Cycle (SDLC).
DeepEye-SQL integrates four synergistic stages: it grounds ambiguous user
intent through semantic value retrieval and robust schema linking; enhances
fault tolerance with N-version SQL generation using diverse reasoning
paradigms; ensures deterministic verification via a tool-chain of unit tests
and targeted LLM-guided revision; and introduces confidence-aware selection
that clusters execution results to estimate confidence and then takes a
high-confidence shortcut or runs unbalanced pairwise adjudication in
low-confidence cases, yielding a calibrated, quality-gated output. This
SDLC-aligned workflow transforms ad hoc query generation into a disciplined
engineering process. Using ~30B open-source LLMs without any fine-tuning,
DeepEye-SQL achieves 73.5% execution accuracy on BIRD-Dev and 89.8% on
Spider-Test, outperforming state-of-the-art solutions. This highlights that
principled orchestration, rather than LLM scaling alone, is key to achieving
system-level reliability in Text-to-SQL.

</details>


### [58] [This is Going to Sound Crazy, But What If We Used Large Language Models to Boost Automatic Database Tuning Algorithms By Leveraging Prior History? We Will Find Better Configurations More Quickly Than Retraining From Scratch!](https://arxiv.org/abs/2510.17748)
*William Zhang,Wan Shen Lim,Andrew Pavlo*

Main category: cs.DB

TL;DR: Booster框架通过利用查询级别的历史信息，辅助现有调优器适应环境变化，从而提升数据库管理系统(DBMS)的性能和调优效率。


<details>
  <summary>Details</summary>
Motivation: 现有的自动调优器在环境变化时难以适应和重新优化数据库管理系统(DBMS)。

Method: Booster框架将历史信息构建成查询-配置上下文，利用大型语言模型(LLMs)为每个查询建议配置，并通过beam search将查询级别的建议组合成整体配置。

Result: Booster框架辅助调优器发现性能提升高达74%的配置，并且比从历史配置继续调优的方法节省高达4.7倍的时间。

Conclusion: Booster框架能够有效地辅助现有调优器适应环境变化，提高数据库管理系统的性能和调优效率。

Abstract: Tuning database management systems (DBMSs) is challenging due to trillions of
possible configurations and evolving workloads. Recent advances in tuning have
led to breakthroughs in optimizing over the possible configurations. However,
due to their design and inability to leverage query-level historical insights,
existing automated tuners struggle to adapt and re-optimize the DBMS when the
environment changes (e.g., workload drift, schema transfer).
  This paper presents the Booster framework that assists existing tuners in
adapting to environment changes (e.g., drift, cross-schema transfer). Booster
structures historical artifacts into query-configuration contexts, prompts
large language models (LLMs) to suggest configurations for each query based on
relevant contexts, and then composes the query-level suggestions into a
holistic configuration with beam search. With multiple OLAP workloads, we
evaluate Booster's ability to assist different state-of-the-art tuners (e.g.,
cost-/machine learning-/LLM-based) in adapting to environment changes. By
composing recommendations derived from query-level insights, Booster assists
tuners in discovering configurations that are up to 74% better and in up to
4.7x less time than the alternative approach of continuing to tune from
historical configurations.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [59] [Investigating the Association Between Text-Based Indications of Foodborne Illness from Yelp Reviews and New York City Health Inspection Outcomes (2023)](https://arxiv.org/abs/2510.16334)
*Eden Shaveet,Crystal Su,Daniel Hsu,Luis Gravano*

Main category: cs.IR

TL;DR: 本研究利用Yelp评论中的信号来分析食物传播疾病与餐馆卫生检查结果之间的关系，使用了层级Sigmoid注意力网络（HSAN）分类器。


<details>
  <summary>Details</summary>
Motivation: 通过社交媒体平台可以获取及时的公众健康信号，尤其是在餐馆食物传播疾病爆发的情况下，但正式渠道的公共报告有限。

Method: 使用层级Sigmoid注意力网络（HSAN）分类器分析Yelp评论中的信号，并将其与纽约市卫生与精神卫生局（NYC DOHMH）发布的官方餐馆检查结果进行比较。

Result: 在人口普查区级别上，HSAN信号与检查分数之间的相关性极小，并且C等级餐馆数量没有显着差异。

Conclusion: 研究结果表明，Yelp评论信号与餐馆检查结果之间的相关性有限，需要在地址级别上进行进一步分析。

Abstract: Foodborne illnesses are gastrointestinal conditions caused by consuming
contaminated food. Restaurants are critical venues to investigate outbreaks
because they share sourcing, preparation, and distribution of foods. Public
reporting of illness via formal channels is limited, whereas social media
platforms host abundant user-generated content that can provide timely public
health signals. This paper analyzes signals from Yelp reviews produced by a
Hierarchical Sigmoid Attention Network (HSAN) classifier and compares them with
official restaurant inspection outcomes issued by the New York City Department
of Health and Mental Hygiene (NYC DOHMH) in 2023. We evaluate correlations at
the Census tract level, compare distributions of HSAN scores by prevalence of
C-graded restaurants, and map spatial patterns across NYC. We find minimal
correlation between HSAN signals and inspection scores at the tract level and
no significant differences by number of C-graded restaurants. We discuss
implications and outline next steps toward address-level analyses.

</details>


### [60] [Blending Learning to Rank and Dense Representations for Efficient and Effective Cascades](https://arxiv.org/abs/2510.16393)
*Franco Maria Nardini,Raffaele Perego,Nicola Tonellotto,Salvatore Trani*

Main category: cs.IR

TL;DR: 本文研究了如何利用词汇和神经相关性信号进行 ad-hoc 段落检索。


<details>
  <summary>Details</summary>
Motivation: 探索大规模训练数据集，其中 MS-MARCO 查询和段落的密集神经表示与从同一语料库中提取的 253 个手工制作的词汇特征进行补充和集成。

Method: 通过基于决策树森林的经典 Learning-to-Rank (LTR) 模型学习来自两组不同特征的相关性信号的混合。采用流水线架构，其中密集神经检索器作为第一阶段，并对文档的神经表示执行最近邻搜索。LTR 模型充当第二阶段，重新排列第一阶段检索到的候选集以提高有效性。

Result: 在公开可用的资源上使用最先进的密集检索器进行的可重复实验结果表明，所提出的解决方案显着提高了端到端排名性能，同时对效率的影响相对最小。nDCG@10 提高了 11%，平均查询延迟仅增加了 4.3%。

Conclusion: 证实了无缝结合两个不同信号系列的优势，这两个信号系列相互促进了检索效果。

Abstract: We investigate the exploitation of both lexical and neural relevance signals
for ad-hoc passage retrieval. Our exploration involves a large-scale training
dataset in which dense neural representations of MS-MARCO queries and passages
are complemented and integrated with 253 hand-crafted lexical features
extracted from the same corpus. Blending of the relevance signals from the two
different groups of features is learned by a classical Learning-to-Rank (LTR)
model based on a forest of decision trees. To evaluate our solution, we employ
a pipelined architecture where a dense neural retriever serves as the first
stage and performs a nearest-neighbor search over the neural representations of
the documents. Our LTR model acts instead as the second stage that re-ranks the
set of candidates retrieved by the first stage to enhance effectiveness. The
results of reproducible experiments conducted with state-of-the-art dense
retrievers on publicly available resources show that the proposed solution
significantly enhances the end-to-end ranking performance while relatively
minimally impacting efficiency. Specifically, we achieve a boost in nDCG@10 of
up to 11% with an increase in average query latency of only 4.3%. This confirms
the advantage of seamlessly combining two distinct families of signals that
mutually contribute to retrieval effectiveness.

</details>


### [61] [FRONTIER-RevRec: A Large-scale Dataset for Reviewer Recommendation](https://arxiv.org/abs/2510.16597)
*Qiyao Peng,Chen Wang,Yinghui Wang,Hongtao Liu,Xuan Guo,Wenjun Wang*

Main category: cs.IR

TL;DR: 本研究构建了一个大规模的审稿人推荐数据集FRONTIER-RevRec，旨在解决该领域缺乏高质量基准数据集的问题。


<details>
  <summary>Details</summary>
Motivation: 现有审稿人推荐研究受限于缺乏大规模、跨学科、包含对比分析的优质基准数据集。

Method: 构建了一个包含2007-2025年Frontiers开放获取出版平台数据的FRONTIER-RevRec数据集，并对内容方法和协同过滤方法进行了全面评估。

Result: 内容方法显著优于协同过滤，语言模型能有效捕捉论文内容和审稿人专业知识之间的语义对齐。

Conclusion: FRONTIER-RevRec可以作为一个全面的基准，以促进审稿人推荐的研究，并促进更有效的学术同行评审系统的发展。

Abstract: Reviewer recommendation is a critical task for enhancing the efficiency of
academic publishing workflows. However, research in this area has been
persistently hindered by the lack of high-quality benchmark datasets, which are
often limited in scale, disciplinary scope, and comparative analyses of
different methodologies. To address this gap, we introduce FRONTIER-RevRec, a
large-scale dataset constructed from authentic peer review records (2007-2025)
from the Frontiers open-access publishing platform
https://www.frontiersin.org/. The dataset contains 177941 distinct reviewers
and 478379 papers across 209 journals spanning multiple disciplines including
clinical medicine, biology, psychology, engineering, and social sciences. Our
comprehensive evaluation on this dataset reveals that content-based methods
significantly outperform collaborative filtering. This finding is explained by
our structural analysis, which uncovers fundamental differences between
academic recommendation and commercial domains. Notably, approaches leveraging
language models are particularly effective at capturing the semantic alignment
between a paper's content and a reviewer's expertise. Furthermore, our
experiments identify optimal aggregation strategies to enhance the
recommendation pipeline. FRONTIER-RevRec is intended to serve as a
comprehensive benchmark to advance research in reviewer recommendation and
facilitate the development of more effective academic peer review systems. The
FRONTIER-RevRec dataset is available at:
https://anonymous.4open.science/r/FRONTIER-RevRec-5D05.

</details>


### [62] [Right Answer at the Right Time - Temporal Retrieval-Augmented Generation via Graph Summarization](https://arxiv.org/abs/2510.16715)
*Zulun Zhu,Haoyu Liu,Mengke He,Siqiang Luo*

Main category: cs.IR

TL;DR: STAR-RAG: A temporal GraphRAG framework for question answering in temporal knowledge graphs, improving time-consistent retrieval.


<details>
  <summary>Details</summary>
Motivation: Existing RAG methods neglect explicit temporal constraints, leading to time-inconsistent answers and inflated token usage.

Method: Building a time-aligned rule graph and conducting propagation to narrow the search space and prioritize semantically relevant, time-consistent evidence.

Result: Improved answer accuracy and reduced token consumption compared to strong GraphRAG baselines.

Conclusion: STAR-RAG eliminates the need for heavy model training and fine-tuning, reducing computational cost and simplifying deployment.

Abstract: Question answering in temporal knowledge graphs requires retrieval that is
both time-consistent and efficient. Existing RAG methods are largely semantic
and typically neglect explicit temporal constraints, which leads to
time-inconsistent answers and inflated token usage. We propose STAR-RAG, a
temporal GraphRAG framework that relies on two key ideas: building a
time-aligned rule graph and conducting propagation on this graph to narrow the
search space and prioritize semantically relevant, time-consistent evidence.
This design enforces temporal proximity during retrieval, reduces the candidate
set of retrieval results, and lowers token consumption without sacrificing
accuracy. Compared with existing temporal RAG approaches, STAR-RAG eliminates
the need for heavy model training and fine-tuning, thereby reducing
computational cost and significantly simplifying deployment.Extensive
experiments on real-world temporal KG datasets show that our method achieves
improved answer accuracy while consuming fewer tokens than strong GraphRAG
baselines.

</details>


### [63] [Exact Nearest-Neighbor Search on Energy-Efficient FPGA Devices](https://arxiv.org/abs/2510.16736)
*Patrizio Dazzi,William Guglielmo,Franco Maria Nardini,Raffaele Perego,Salvatore Trani*

Main category: cs.IR

TL;DR: 本论文研究了使用 FPGA 设备在高度潜在空间中进行节能精确 kNN 搜索。


<details>
  <summary>Details</summary>
Motivation: 通过使基于神经编码器模型的大规模学习表示的采用更环保和更具包容性，来支持这种表示方法日益普及的趋势。

Method: 提出了两种不同的节能解决方案，它们采用相同的 FPGA 底层配置。第一种解决方案通过在不适合 FPGA 内存的流式数据集上并行处理批处理查询来最大化系统吞吐量。第二种解决方案通过在内存数据集上并行处理每个 kNN 传入查询来最大限度地减少延迟。

Result: 在公开可用的图像和文本数据集上进行的可重复实验表明，该解决方案在吞吐量、延迟和能耗方面优于最先进的基于 CPU 的竞争对手。实验表明，所提出的 FPGA 解决方案在每秒查询次数方面实现了最佳吞吐量，并实现了高达 16.6 倍的放大倍数的最佳观察延迟。在能源效率方面，结果表明，与强大的基于 CPU 的竞争对手相比，该解决方案可以实现高达 11.9 倍的节能效果。

Conclusion: 该论文表明，FPGA 解决方案在 kNN 搜索方面具有更高的效率。

Abstract: This paper investigates the usage of FPGA devices for energy-efficient exact
kNN search in high-dimension latent spaces. This work intercepts a relevant
trend that tries to support the increasing popularity of learned
representations based on neural encoder models by making their large-scale
adoption greener and more inclusive. The paper proposes two different
energy-efficient solutions adopting the same FPGA low-level configuration. The
first solution maximizes system throughput by processing the queries of a batch
in parallel over a streamed dataset not fitting into the FPGA memory. The
second minimizes latency by processing each kNN incoming query in parallel over
an in-memory dataset. Reproducible experiments on publicly available image and
text datasets show that our solution outperforms state-of-the-art CPU-based
competitors regarding throughput, latency, and energy consumption.
Specifically, experiments show that the proposed FPGA solutions achieve the
best throughput in terms of queries per second and the best-observed latency
with scale-up factors of up to 16.6X. Similar considerations can be made
regarding energy efficiency, where results show that our solutions can achieve
up to 11.9X energy saving w.r.t. strong CPU-based competitors.

</details>


### [64] [An Efficient Framework for Whole-Page Reranking via Single-Modal Supervision](https://arxiv.org/abs/2510.16803)
*Zishuai Zhang,Sihao Yu,Wenyi Xie,Ying Nie,Junfeng Wang,Zhiming Zheng,Dawei Yin,Hainan Zhang*

Main category: cs.IR

TL;DR: 提出了一种新的全页面重排序框架SMAR，该框架利用强大的单模态排序器来指导模态相关性对齐以进行有效的重排序，仅使用有限的全页面注释即可胜过完全注释的重排序模型。


<details>
  <summary>Details</summary>
Motivation: 现有的全页面重排序方法主要依赖于大规模的人工标注数据，获取成本高且耗时。全页面标注比单模态标注复杂得多，因为它需要评估整个结果页面，同时考虑跨模态相关性差异。因此，如何在提高全页面重排序性能的同时降低标注成本仍然是优化搜索引擎结果页面（SERP）的关键挑战。

Method: 首先，在特定于各自模态的数据上训练高质量的单模态排序器。然后，对于每个查询，我们选择其输出的子集以构建候选页面，并在页面级别执行人工注释。最后，我们使用这些有限的注释来训练整个页面重排序器，并强制与单模态偏好保持一致，以维持每个模态内的排序质量。

Result: 在麒麟和百度数据集上的实验表明，与基线相比，SMAR降低了约70-90％的注释成本，同时实现了显着的排名改进。在百度应用程序上进行的进一步离线和在线A / B测试也表明，标准排名指标和用户体验指标均获得了显着提升。

Conclusion: SMAR方法在实际搜索场景中具有有效性和实践价值。

Abstract: The whole-page reranking plays a critical role in shaping the user experience
of search engines, which integrates retrieval results from multiple modalities,
such as documents, images, videos, and LLM outputs. Existing methods mainly
rely on large-scale human-annotated data, which is costly to obtain and
time-consuming. This is because whole-page annotation is far more complex than
single-modal: it requires assessing the entire result page while accounting for
cross-modal relevance differences. Thus, how to improve whole-page reranking
performance while reducing annotation costs is still a key challenge in
optimizing search engine result pages(SERP). In this paper, we propose SMAR, a
novel whole-page reranking framework that leverages strong Single-modal rankers
to guide Modal-wise relevance Alignment for effective Reranking, using only
limited whole-page annotation to outperform fully-annotated reranking models.
Specifically, high-quality single-modal rankers are first trained on data
specific to their respective modalities. Then, for each query, we select a
subset of their outputs to construct candidate pages and perform human
annotation at the page level. Finally, we train the whole-page reranker using
these limited annotations and enforcing consistency with single-modal
preferences to maintain ranking quality within each modality. Experiments on
the Qilin and Baidu datasets demonstrate that SMAR reduces annotation costs by
about 70-90\% while achieving significant ranking improvements compared to
baselines. Further offline and online A/B testing on Baidu APPs also shows
notable gains in standard ranking metrics as well as user experience
indicators, fully validating the effectiveness and practical value of our
approach in real-world search scenarios.

</details>


### [65] [The Layout Is the Model: On Action-Item Coupling in Generative Recommendation](https://arxiv.org/abs/2510.16804)
*Xiaokai Wei,Jiajun Wu,Daiyao Yi,Reza Shirkavand,Michelle Gong*

Main category: cs.IR

TL;DR: 本文研究了生成式推荐 (GR) 模型中 token layout 的设计，旨在提高模型准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 探索 token layout 对 GR 模型性能的影响，现有 interleaved layout 存在序列过长的问题。

Method: 提出一种新的非 interleaved 方法，Lagged Action Conditioning (LAC)，并从最大化信号、保持条件关系和防止信息泄漏三个原则进行分析。

Result: LAC 方法在公共数据集和大规模生产日志上表现出与 interleaved 方法相当或更优的性能，同时显著降低了 FLOPs。

Conclusion: 研究结果为构建准确高效的 GR 系统提供了可操作的指导。

Abstract: Generative Recommendation (GR) models treat a user's interaction history as a
sequence to be autoregressively predicted. When both items and actions (e.g.,
watch time, purchase, comment) are modeled, the layout-the ordering and
visibility of item/action tokens-critically determines what information the
model can use and how it generalizes. We present a unified study of token
layouts for GR grounded in first principles: (P1) maximize item/action signal
in both input/output space, (P2) preserve the conditioning relationship "action
given item" and (P3) no information leakage.
  While interleaved layout (where item and action occupy separate tokens)
naturally satisfies these principles, it also bloats sequence length with
larger training/inference cost. On the non-interleaved front, we design a novel
and effective approach, Lagged Action Conditioning (LAC), which appears strange
on the surface but aligns well with the design principles to yield strong
accuracy. Comprehensive experiments on public datasets and large-scale
production logs evaluate different layout options and empirically verifies the
design principles. Our proposed non-interleaved method, LAC, achieves
competitive or superior quality at substantially lower FLOPs than interleaving.
Our findings offer actionable guidance for assembling GR systems that are both
accurate and efficient.

</details>


### [66] [Towards Context-aware Reasoning-enhanced Generative Searching in E-commerce](https://arxiv.org/abs/2510.16925)
*Zhiding Liu,Ben Chen,Mingyue Cheng,Enchong Chen,Li Li,Chenyi Lei,Wenwu Ou,Han Li,Kun Gai*

Main category: cs.IR

TL;DR: 提出了一种上下文感知的推理增强生成搜索框架，以更好地理解复杂的上下文，从而改进基于搜索的推荐。


<details>
  <summary>Details</summary>
Motivation: 现有的搜索方法在整合上下文信息方面存在局限性，阻碍了它们充分捕捉用户意图的能力。用户复杂的搜索上下文是他们决策制定的重要组成部分，反映了补充明确查询词的隐含偏好。建模这种丰富的上下文信号及其与候选项目的复杂关联仍然是一个关键挑战。

Method: 该框架首先将异构用户和项目上下文统一为文本表示或基于文本的语义标识符，并将它们对齐。为了克服缺乏显式推理轨迹的问题，我们引入了一种自我进化的后训练范式，该范式迭代地结合了监督微调和强化学习，以逐步提高模型的推理能力。此外，我们还在现有RL算法应用于搜索场景时识别出潜在的偏差，并提出了GRPO的去偏变体，以提高排序性能。

Result: 在从真实电子商务平台收集的搜索日志数据上进行的大量实验表明，我们的方法与强大的基线相比，取得了优异的性能，验证了其在基于搜索的推荐中的有效性。

Conclusion: 该方法在基于搜索的推荐中有效。

Abstract: Search-based recommendation is one of the most critical application scenarios
in e-commerce platforms. Users' complex search contexts--such as spatiotemporal
factors, historical interactions, and current query's information--constitute
an essential part of their decision-making, reflecting implicit preferences
that complement explicit query terms. Modeling such rich contextual signals and
their intricate associations with candidate items remains a key challenge.
Although numerous efforts have been devoted to building more effective search
methods, existing approaches still show limitations in integrating contextual
information, which hinders their ability to fully capture user intent.
  To address these challenges, we propose a context-aware reasoning-enhanced
generative search framework for better \textbf{understanding the complicated
context}. Specifically, the framework first unifies heterogeneous user and item
contexts into textual representations or text-based semantic identifiers and
aligns them. To overcome the lack of explicit reasoning trajectories, we
introduce a self-evolving post-training paradigm that iteratively combines
supervised fine-tuning and reinforcement learning to progressively enhance the
model's reasoning capability. In addition, we identify potential biases in
existing RL algorithms when applied to search scenarios and present a debiased
variant of GRPO to improve ranking performance. Extensive experiments on search
log data collected from a real-world e-commerce platform demonstrate that our
approach achieves superior performance compared with strong baselines,
validating its effectiveness for search-based recommendation.

</details>


### [67] [DSEBench: A Test Collection for Explainable Dataset Search with Examples](https://arxiv.org/abs/2510.17228)
*Qing Shi,Jing He,Qiaosheng Chen,Gong Cheng*

Main category: cs.IR

TL;DR: 本文研究了数据集搜索与示例（DSE）任务，并将其扩展到可解释的DSE，要求识别数据集的元数据和内容字段，这些字段表明其与查询的相关性和与目标数据集的相似性。


<details>
  <summary>Details</summary>
Motivation: 为了结合信息需求的规范，本文研究了更广义的DSE任务，并将其扩展到可解释的DSE。

Method: 我们构建了DSEBench，这是一个测试集合，提供高质量的数据集和字段级注释，以支持可解释的DSE的评估。我们还使用大型语言模型来生成大量注释以用于训练。我们通过调整和评估各种稀疏、密集和基于LLM的检索、重新排序和解释方法，在DSEBench上建立了广泛的基线。

Result: 我们通过调整和评估各种稀疏、密集和基于LLM的检索、重新排序和解释方法，在DSEBench上建立了广泛的基线。

Conclusion: 本文研究了数据集搜索与示例（DSE）任务，并将其扩展到可解释的DSE，要求识别数据集的元数据和内容字段，这些字段表明其与查询的相关性和与目标数据集的相似性。为了促进这项研究，我们构建了DSEBench，这是一个测试集合，提供高质量的数据集和字段级注释，以支持可解释的DSE的评估。我们还使用大型语言模型来生成大量注释以用于训练。

Abstract: Dataset search has been an established information retrieval task. Current
paradigms either retrieve datasets that are relevant to a keyword query or find
datasets that are similar to an input target dataset. To allow for their
combined specification of information needs, in this article, we investigate
the more generalized task of Dataset Search with Examples (DSE) and further
extend it to Explainable DSE that requires identifying the metadata and content
fields of a dataset that indicate its relevance to the query and similarity to
the target datasets. To facilitate this research, we construct DSEBench, a test
collection that provides high-quality dataset- and field-level annotations to
enable the evaluation of explainable DSE. We also employ a large language model
to generate numerous annotations to be used for training. We establish
extensive baselines on DSEBench by adapting and evaluating a variety of sparse,
dense, and LLM-based retrieval, reranking, and explanation methods.

</details>


### [68] [On Efficiency-Effectiveness Trade-off of Diffusion-based Recommenders](https://arxiv.org/abs/2510.17245)
*Wenyu Mao,Jiancan Wu,Guoqing Hu,Wei Ji,Xiang Wang*

Main category: cs.IR

TL;DR: 提出了一种名为TA-Rec的两阶段框架，旨在提高基于扩散模型的序列推荐的效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散模型的序列推荐依赖于多步去噪过程，存在离散化误差，需要在计算效率和推荐效果之间进行权衡。

Method: 该框架包括两个阶段：预训练阶段通过时间一致性正则化(TCR)平滑去噪函数，实现一步生成；微调阶段引入自适应偏好对齐(APA)，根据偏好对相似性和时间步长自适应地调整去噪过程与用户偏好。

Result: 大量实验证明，TA-Rec的两阶段目标有效地缓解了离散化误差引起的权衡，提高了基于扩散的推荐器的效率和效果。

Conclusion: TA-Rec通过平滑去噪函数和对齐用户偏好，成功地提高了基于扩散模型的序列推荐的效率和效果。

Abstract: Diffusion models have emerged as a powerful paradigm for generative
sequential recommendation, which typically generate next items to recommend
guided by user interaction histories with a multi-step denoising process.
However, the multi-step process relies on discrete approximations, introducing
discretization error that creates a trade-off between computational efficiency
and recommendation effectiveness. To address this trade-off, we propose TA-Rec,
a two-stage framework that achieves one-step generation by smoothing the
denoising function during pretraining while alleviating trajectory deviation by
aligning with user preferences during fine-tuning. Specifically, to improve the
efficiency without sacrificing the recommendation performance, TA-Rec pretrains
the denoising model with Temporal Consistency Regularization (TCR), enforcing
the consistency between the denoising results across adjacent steps. Thus, we
can smooth the denoising function to map the noise as oracle items in one step
with bounded error. To further enhance effectiveness, TA-Rec introduces
Adaptive Preference Alignment (APA) that aligns the denoising process with user
preference adaptively based on preference pair similarity and timesteps.
Extensive experiments prove that TA-Rec's two-stage objective effectively
mitigates the discretization errors-induced trade-off, enhancing both
efficiency and effectiveness of diffusion-based recommenders.

</details>


### [69] [How role-play shapes relevance judgment in zero-shot LLM rankers](https://arxiv.org/abs/2510.17535)
*Yumeng Wang,Jirui Qi,Catherine Chen,Panagiotis Eustratiadis,Suzan Verberne*

Main category: cs.IR

TL;DR: 角色扮演提示可以提高大型语言模型 (LLM) 作为零样本排序器的性能，但其机制尚未得到充分探索。本文研究了角色扮演变化如何影响零样本 LLM 排序器，并揭示了角色扮演信号主要编码在早期层中，并与中间层的任务指令进行通信，同时与查询或文档表示的交互有限。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 已成为有希望的零样本排序器，但它们的性能对提示公式非常敏感。角色扮演提示通常提供更强大和准确的相关性排序。然而，角色扮演效果的机制和多样性仍未得到充分探索，限制了有效使用和可解释性。

Method: 采用来自机制可解释性的因果干预技术来追踪角色扮演信息如何塑造 LLM 中的相关性判断。

Result: (1) 角色描述的仔细制定对 LLM 的排序质量有很大影响；(2) 角色扮演信号主要编码在早期层中，并与中间层的任务指令进行通信，同时与查询或文档表示的交互有限。具体来说，我们确定了一组注意力头，它们编码对角色条件相关性至关重要的信息。

Conclusion: 这些发现不仅揭示了 LLM 排序中角色扮演的内部运作，而且为在 IR 及其他领域设计更有效的提示提供了指导，从而为在零样本应用中利用角色扮演提供了更广泛的机会。

Abstract: Large Language Models (LLMs) have emerged as promising zero-shot rankers, but
their performance is highly sensitive to prompt formulation. In particular,
role-play prompts, where the model is assigned a functional role or identity,
often give more robust and accurate relevance rankings. However, the mechanisms
and diversity of role-play effects remain underexplored, limiting both
effective use and interpretability. In this work, we systematically examine how
role-play variations influence zero-shot LLM rankers. We employ causal
intervention techniques from mechanistic interpretability to trace how
role-play information shapes relevance judgments in LLMs. Our analysis reveals
that (1) careful formulation of role descriptions have a large effect on the
ranking quality of the LLM; (2) role-play signals are predominantly encoded in
early layers and communicate with task instructions in middle layers, while
receiving limited interaction with query or document representations.
Specifically, we identify a group of attention heads that encode information
critical for role-conditioned relevance. These findings not only shed light on
the inner workings of role-play in LLM ranking but also offer guidance for
designing more effective prompts in IR and beyond, pointing toward broader
opportunities for leveraging role-play in zero-shot applications.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [70] [Lean Finder: Semantic Search for Mathlib That Understands User Intents](https://arxiv.org/abs/2510.15940)
*Jialin Lu,Kye Emond,Kaiyu Yang,Swarat Chaudhuri,Weiran Sun,Wuyang Chen*

Main category: cs.LG

TL;DR: Lean Finder是一个语义搜索引擎，专为Lean和mathlib设计，能够理解数学家的意图。


<details>
  <summary>Details</summary>
Motivation: 形式化定理证明进展缓慢，原因在于难以找到相关定理以及Lean 4语言学习曲线陡峭。现有的Lean搜索引擎主要依赖于非形式化，忽略了与实际用户查询的不匹配。

Method: 通过分析和聚类公开的Lean讨论的语义，然后在模拟用户意图的合成查询上微调文本嵌入，从而实现以用户为中心的语义搜索。

Result: 在真实查询、非形式化陈述和证明状态的评估中，Lean Finder比以前的搜索引擎和GPT-4o提高了30%以上。

Conclusion: Lean Finder与基于LLM的定理证明器兼容，将检索与形式推理连接起来。

Abstract: We present Lean Finder, a semantic search engine for Lean and mathlib that
understands and aligns with the intents of mathematicians. Progress in formal
theorem proving is often hindered by the difficulty of locating relevant
theorems and the steep learning curve of the Lean 4 language, making
advancement slow and labor-intensive. Existing Lean search engines, though
helpful, rely primarily on informalizations (natural language translation of
the formal statements), while largely overlooking the mismatch with real-world
user queries. In contrast, we propose a user-centered semantic search tailored
to the needs of mathematicians. Our approach begins by analyzing and clustering
the semantics of public Lean discussions, then fine-tuning text embeddings on
synthesized queries that emulate user intents. We further align Lean Finder
with mathematicians' preferences using diverse feedback signals, encoding it
with a rich awareness of their goals from multiple perspectives. Evaluations on
real-world queries, informalized statements, and proof states demonstrate that
our Lean Finder achieves over $30\%$ relative improvement compared to previous
search engines and GPT-4o. In addition, Lean Finder is compatible with
LLM-based theorem provers, bridging retrieval with formal reasoning. Lean
Finder is available at: https://leanfinder.github.io

</details>


### [71] [Lyapunov-Stable Adaptive Control for Multimodal Concept Drift](https://arxiv.org/abs/2510.15944)
*Tianyu Bell Pan,Mengdi Zhu,Alexa Jordyn Cole,Ronald Wilson,Damon L. Woodard*

Main category: cs.LG

TL;DR: 本文提出了一种新的自适应控制框架LS-OGD，用于在概念漂移存在的情况下实现鲁棒的多模态学习。


<details>
  <summary>Details</summary>
Motivation: 多模态学习系统在非平稳环境中由于概念漂移而难以表现良好，模态特定的漂移和缺乏持续稳定适应的机制加剧了这一挑战。

Method: LS-OGD使用在线控制器，动态调整模型的学习率和不同数据模态之间的融合权重，以响应检测到的漂移和不断变化的预测误差。

Result: 证明了在有界漂移条件下，LS-OGD系统的预测误差是一致最终有界的，如果漂移停止，则收敛到零。自适应融合策略有效地隔离和减轻了严重模态特定漂移的影响，从而确保了系统的弹性和容错能力。

Conclusion: 这些理论保证为开发可靠和持续适应的多模态学习系统奠定了原则性基础。

Abstract: Multimodal learning systems often struggle in non-stationary environments due
to concept drift, where changing data distributions can degrade performance.
Modality-specific drifts and the lack of mechanisms for continuous, stable
adaptation compound this challenge. This paper introduces LS-OGD, a novel
adaptive control framework for robust multimodal learning in the presence of
concept drift. LS-OGD uses an online controller that dynamically adjusts the
model's learning rate and the fusion weights between different data modalities
in response to detected drift and evolving prediction errors. We prove that
under bounded drift conditions, the LS-OGD system's prediction error is
uniformly ultimately bounded and converges to zero if the drift ceases.
Additionally, we demonstrate that the adaptive fusion strategy effectively
isolates and mitigates the impact of severe modality-specific drift, thereby
ensuring system resilience and fault tolerance. These theoretical guarantees
establish a principled foundation for developing reliable and continuously
adapting multimodal learning systems.

</details>


### [72] [BEACON: Bayesian Optimal Stopping for Efficient LLM Sampling](https://arxiv.org/abs/2510.15945)
*Guangya Wan,Zixin Stephen Xu,Sasa Zorc,Manel Baucells,Mengxuan Hu,Hao Wang,Sheng Li*

Main category: cs.LG

TL;DR: 提出了一种基于贝叶斯学习的自适应采样框架BEACON，以在保证输出质量的同时，降低LLM的计算成本。


<details>
  <summary>Details</summary>
Motivation: 在提高LLM输出质量时，多重采样会带来额外的计算成本，关键挑战在于如何在准确性和效率之间找到平衡点，决定何时停止生成新的样本。

Method: BEACON框架基于贝叶斯学习的序列搜索，通过从策略LLM中序列地生成响应，实时更新奖励分布的后验信念，并根据预期收益与计算成本的权衡来决定何时停止采样。

Result: 实验表明，BEACON在保持响应质量的同时，平均减少了高达80%的采样。

Conclusion: BEACON为降低采样成本、同时保持LLM输出质量提供了一种有效方法，并展示了其在成本效益高的偏好数据生成方面的应用，为未来的研究提供了可操作的见解。

Abstract: Sampling multiple responses is a common way to improve LLM output quality,
but it comes at the cost of additional computation. The key challenge is
deciding when to stop generating new samples to balance accuracy gains against
efficiency. To address this, we introduce BEACON (Bayesian Efficient Adaptive
Criterion for Optimal N-stopping), a principled adaptive sampling framework
grounded in Sequential Search with Bayesian Learning. BEACON sequentially
generates responses from the policy LLM, updates posterior belief over reward
distributions in real time without further training, and determines when to
stop by weighing expected gains against computational cost. Sampling terminates
once the marginal utility of further exploration no longer justifies the
expense. We establish both theoretical optimality guarantees and practical
tractability, and show empirically that BEACON reduces average sampling by up
to 80% while maintaining response quality. We further demonstrate BEACON's
utility for cost-efficient preference data generation and outline practical
extensions, offering actionable insights for future researchers.

</details>


### [73] [Learning from Mistakes: Enhancing Harmful Meme Detection via Misjudgment Risk Patterns](https://arxiv.org/abs/2510.15946)
*Wenshuo Wang,Ziyou Jiang,Junjie Wang,Mingyang Li,Jie Huang,Yuekai Huang,Zhiyuan Chang,Feiyan Duan,Qing Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为PatMD的新方法，通过识别潜在的误判风险模式来改进有害模因的检测。


<details>
  <summary>Details</summary>
Motivation: 现有的检测方法难以检测通过讽刺和隐喻等微妙修辞手段传达有害观点的互联网模因，导致频繁的误判。

Method: PatMD构建了一个知识库，将每个模因分解为误判风险模式，并利用这些模式动态地指导MLLM的推理。

Result: 在包含6,626个模因的基准测试中，PatMD在5个有害检测任务中优于现有技术，F1-score平均提高了8.30%，准确率提高了7.71%。

Conclusion: PatMD具有很强的泛化能力，并提高了有害模因的检测能力。

Abstract: Internet memes have emerged as a popular multimodal medium, yet they are
increasingly weaponized to convey harmful opinions through subtle rhetorical
devices like irony and metaphor. Existing detection approaches, including
MLLM-based techniques, struggle with these implicit expressions, leading to
frequent misjudgments. This paper introduces PatMD, a novel approach that
improves harmful meme detection by learning from and proactively mitigating
these potential misjudgment risks. Our core idea is to move beyond superficial
content-level matching and instead identify the underlying misjudgment risk
patterns, proactively guiding the MLLMs to avoid known misjudgment pitfalls. We
first construct a knowledge base where each meme is deconstructed into a
misjudgment risk pattern explaining why it might be misjudged, either
overlooking harmful undertones (false negative) or overinterpreting benign
content (false positive). For a given target meme, PatMD retrieves relevant
patterns and utilizes them to dynamically guide the MLLM's reasoning.
Experiments on a benchmark of 6,626 memes across 5 harmful detection tasks show
that PatMD outperforms state-of-the-art baselines, achieving an average of
8.30\% improvement in F1-score and 7.71\% improvement in accuracy,
demonstrating strong generalizability and improved detection capability of
harmful memes.

</details>


### [74] [WaveNet's Precision in EEG Classification](https://arxiv.org/abs/2510.15947)
*Casper van Laar,Khubaib Ahmed*

Main category: cs.LG

TL;DR: 本研究提出了一种基于WaveNet的深度学习模型，用于自动将脑电信号分类为生理性、病理性、伪迹和噪声类别。


<details>
  <summary>Details</summary>
Motivation: 由于脑电图记录的复杂性和数量不断增加，依赖专家视觉检查的传统脑电图信号分类方法正变得越来越不切实际。

Method: 利用来自 Mayo Clinic 和 St. Anne's University Hospital 的公开带注释数据集，WaveNet 模型在 209,232 个样本上进行了训练、验证和测试，分割比例为 70/20/10。

Result: 该模型实现了超过以前基于 CNN 和 LSTM 的方法的分类精度，并以时间卷积网络 (TCN) 基线为基准。值得注意的是，该模型以高精度区分噪声和伪迹，尽管它揭示了生理和病理信号之间存在适度但可解释的错误分类，反映了固有的临床重叠。

Conclusion: WaveNet 的架构最初是为原始音频合成而开发的，由于其使用扩张的因果卷积和残差连接，使其非常适合脑电图数据，从而能够捕获细粒度和长程时间依赖性。该研究还详细介绍了预处理流程，包括支持模型泛化的动态数据集分区和标准化步骤。

Abstract: This study introduces a WaveNet-based deep learning model designed to
automate the classification of EEG signals into physiological, pathological,
artifact, and noise categories. Traditional methods for EEG signal
classification, which rely on expert visual review, are becoming increasingly
impractical due to the growing complexity and volume of EEG recordings.
Leveraging a publicly available annotated dataset from Mayo Clinic and St.
Anne's University Hospital, the WaveNet model was trained, validated, and
tested on 209,232 samples with a 70/20/10 percent split. The model achieved a
classification accuracy exceeding previous CNN and LSTM-based approaches, and
was benchmarked against a Temporal Convolutional Network (TCN) baseline.
Notably, the model distinguishes noise and artifacts with high precision,
although it reveals a modest but explainable degree of misclassification
between physiological and pathological signals, reflecting inherent clinical
overlap. WaveNet's architecture, originally developed for raw audio synthesis,
is well suited for EEG data due to its use of dilated causal convolutions and
residual connections, enabling it to capture both fine-grained and long-range
temporal dependencies. The research also details the preprocessing pipeline,
including dynamic dataset partitioning and normalization steps that support
model generalization.

</details>


### [75] [Cross-dataset Multivariate Time-series Model for Parkinson's Diagnosis via Keyboard Dynamics](https://arxiv.org/abs/2510.15950)
*Arianna Francesconi,Donato Cappetta,Fabio Rebecchi,Paolo Soda,Valerio Guarrasi,Rosa Sicilia*

Main category: cs.LG

TL;DR: 本研究提出了一种利用击键动力学作为生物标志物进行帕金森病远程筛查和远程监控的新流程。


<details>
  <summary>Details</summary>
Motivation: 传统临床评估在帕金森病早期诊断方面存在局限性，且运动症状出现较晚。

Method: 该方法包括三个阶段：数据预处理、预训练深度学习架构和微调。

Result: 混合卷积循环模型和基于 Transformer 的模型实现了强大的外部验证性能，AUC-ROC 分数超过 90%，F1-Score 超过 70%。

Conclusion: 击键动力学有潜力成为帕金森病可靠的数字生物标志物，为早期检测和持续监测提供有希望的途径。

Abstract: Parkinson's disease (PD) presents a growing global challenge, affecting over
10 million individuals, with prevalence expected to double by 2040. Early
diagnosis remains difficult due to the late emergence of motor symptoms and
limitations of traditional clinical assessments. In this study, we propose a
novel pipeline that leverages keystroke dynamics as a non-invasive and scalable
biomarker for remote PD screening and telemonitoring. Our methodology involves
three main stages: (i) preprocessing of data from four distinct datasets,
extracting four temporal signals and addressing class imbalance through the
comparison of three methods; (ii) pre-training eight state-of-the-art
deep-learning architectures on the two largest datasets, optimizing temporal
windowing, stride, and other hyperparameters; (iii) fine-tuning on an
intermediate-sized dataset and performing external validation on a fourth,
independent cohort. Our results demonstrate that hybrid convolutional-recurrent
and transformer-based models achieve strong external validation performance,
with AUC-ROC scores exceeding 90% and F1-Score over 70%. Notably, a temporal
convolutional model attains an AUC-ROC of 91.14% in external validation,
outperforming existing methods that rely solely on internal validation. These
findings underscore the potential of keystroke dynamics as a reliable digital
biomarker for PD, offering a promising avenue for early detection and
continuous monitoring.

</details>


### [76] [Fire-EnSF: Wildfire Spread Data Assimilation using Ensemble Score Filter](https://arxiv.org/abs/2510.15954)
*Hongzheng Shi,Yuhang Wang,Xiao Liu*

Main category: cs.LG

TL;DR: 本文研究了使用 Ensemble Score Filter (EnSF) 算法进行实时野火蔓延预测的数据同化问题。


<details>
  <summary>Details</summary>
Motivation: 有效管理野火需要准确、实时的火势蔓延预测。数据同化通过整合观测数据和数值模型预测来提高预测准确性。

Method: 本文应用了一种基于扩散模型的滤波算法 EnSF 来解决野火蔓延模型的数据同化问题。

Result: 数值研究表明，EnSF 具有更高的准确性、稳定性和计算效率。

Conclusion: EnSF 是一种稳健实用的野火数据同化方法，代码已公开。

Abstract: As wildfires become increasingly destructive and expensive to control,
effective management of active wildfires requires accurate, real-time fire
spread predictions. To enhance the forecasting accuracy of active fires, data
assimilation plays a vital role by integrating observations (such as
remote-sensing data) and fire predictions generated from numerical models. This
paper provides a comprehensive investigation on the application of a recently
proposed diffusion-model-based filtering algorithm -- the Ensemble Score Filter
(EnSF) -- to the data assimilation problem for real-time active wildfire spread
predictions. Leveraging a score-based generative diffusion model, EnSF has been
shown to have superior accuracy for high-dimensional nonlinear filtering
problems, making it an ideal candidate for the filtering problems of wildfire
spread models. Technical details are provided, and our numerical investigations
demonstrate that EnSF provides superior accuracy, stability, and computational
efficiency, establishing it as a robust and practical method for wildfire data
assimilation. Our code has been made publicly available.

</details>


### [77] [How Good Are LLMs at Processing Tool Outputs?](https://arxiv.org/abs/2510.15955)
*Kiran Kate,Yara Rizk,Poulami Ghosh,Ashu Gulati,Tathagata Chakraborti,Zidane Wright,Mayank Agarwal*

Main category: cs.LG

TL;DR: 大型语言模型在处理工具返回的复杂JSON响应时面临挑战，这影响了任务自动化。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型处理结构化（JSON）响应的能力，因为现实任务自动化问题需要它们调用工具并处理返回的复杂JSON响应。

Method: 创建了一个数据集，并使用多种prompting方法评估了15个开放和封闭权重模型。

Result: 即使对于前沿模型，JSON处理仍然是一项困难的任务，最佳响应处理策略取决于工具输出的性质和大小以及所需推理的复杂性。处理方法的差异可能导致3％到50％的性能差异。

Conclusion: 大型语言模型在处理JSON响应方面仍有提升空间，且最佳处理策略的选择取决于多种因素。

Abstract: Most realistic task automation problems require large language models (LLMs)
to call tools, which often return complex JSON responses. These responses must
be further processed to derive the information necessary for task completion.
The ability of LLMs to do so is under-studied. In this paper, we study the tool
response processing task and LLMs' abilities to process structured (JSON)
responses. We created a dataset for this task, and evaluated 15 open and closed
weight models using multiple prompting approaches. Our results show that JSON
processing remains a difficult task even for frontier models across multiple
prompting strategies. The optimal response processing strategy depends on both
the nature and size of the tool outputs, as well as the complexity of the
required reasoning. Variations in processing approaches can lead to performance
differences ranging from 3\% to 50\%.

</details>


### [78] [Hydrogen production from blended waste biomass: pyrolysis, thermodynamic-kinetic analysis and AI-based modelling](https://arxiv.org/abs/2510.15960)
*Sana Kordoghli,Abdelhakim Settar,Oumayma Belaati,Mohammad Alkhatib*

Main category: cs.LG

TL;DR: 本研究探讨了通过热解对食物生物质进行热化学转化，并强调了人工智能（AI）在提高过程建模准确性和优化效率方面的作用，以推进可持续能源和废物管理战略。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探索咖啡渣和枣核等未充分利用的生物质资源在可持续制氢方面的潜力，优化热解过程，并评估这些资源单独和混合时的性能。

Method: 对纯枣核、咖啡渣和混合物（75% 枣核 - 25% 咖啡渣，50% 枣核 - 50% 咖啡渣，25% 枣核 - 75% 咖啡渣）进行了近端分析、元素分析、纤维分析、TGA/DTG、动力学、热力学和 Py-Micro GC 分析。采用基于等转化率方法（KAS、FWO、Friedman）的动力学建模，并使用 LSTM 模型训练木质纤维素数据以预测 TGA 曲线。

Result: 混合物 3 具有优异的氢气产量潜力，但活化能最高（Ea: 313.24 kJ/mol），而混合物 1 表现出最佳的活化能值（Ea: 161.75 kJ/mol）。KAS 被确定为最准确的模型。LSTM 模型以极高的精度预测 TGA 曲线 (R^2: 0.9996-0.9998)。

Conclusion: 该研究详细了解了热解过程，特别强调了人工智能的整合。

Abstract: This work contributes to advancing sustainable energy and waste management
strategies by investigating the thermochemical conversion of food-based biomass
through pyrolysis, highlighting the role of artificial intelligence (AI) in
enhancing process modelling accuracy and optimization efficiency. The main
objective is to explore the potential of underutilized biomass resources, such
as spent coffee grounds (SCG) and date seeds (DS), for sustainable hydrogen
production. Specifically, it aims to optimize the pyrolysis process while
evaluating the performance of these resources both individually and as blends.
Proximate, ultimate, fibre, TGA/DTG, kinetic, thermodynamic, and Py-Micro GC
analyses were conducted for pure DS, SCG, and blends (75% DS - 25% SCG, 50% DS
- 50% SCG, 25% DS - 75% SCG). Blend 3 offered superior hydrogen yield potential
but had the highest activation energy (Ea: 313.24 kJ/mol), while Blend 1
exhibited the best activation energy value (Ea: 161.75 kJ/mol). The kinetic
modelling based on isoconversional methods (KAS, FWO, Friedman) identified KAS
as the most accurate. These approaches provide a detailed understanding of the
pyrolysis process, with particular emphasis on the integration of artificial
intelligence. An LSTM model trained with lignocellulosic data predicted TGA
curves with exceptional accuracy (R^2: 0.9996-0.9998).

</details>


### [79] [Interpretable Graph-Language Modeling for Detecting Youth Illicit Drug Use](https://arxiv.org/abs/2510.15961)
*Yiyang Li,Zehong Wang,Zhengqing Yuan,Zheyuan Zhang,Keerthiram Murugesan,Chuxu Zhang,Yanfang Ye*

Main category: cs.LG

TL;DR: 提出了一种新的图-语言联合建模框架LAMI，用于检测青少年和年轻人中的非法药物使用。


<details>
  <summary>Details</summary>
Motivation: 现有建模方法忽略了调查变量之间的潜在和相互关联的结构。

Method: 将个人反应表示为关系图，通过专门的图结构学习层学习潜在连接，并整合大型语言模型以生成基于图结构和调查语义的自然语言解释。

Result: 在 YRBS 和 NSDUH 数据集上的实验表明，LAMI 在预测准确性方面优于其他基线。

Conclusion: LAMI 揭示了有意义的行为子结构和心理社会途径，例如家庭动态、同伴影响和与学校相关的困扰，这些都与已确定的药物使用风险因素相一致。

Abstract: Illicit drug use among teenagers and young adults (TYAs) remains a pressing
public health concern, with rising prevalence and long-term impacts on health
and well-being. To detect illicit drug use among TYAs, researchers analyze
large-scale surveys such as the Youth Risk Behavior Survey (YRBS) and the
National Survey on Drug Use and Health (NSDUH), which preserve rich
demographic, psychological, and environmental factors related to substance use.
However, existing modeling methods treat survey variables independently,
overlooking latent and interconnected structures among them. To address this
limitation, we propose LAMI (LAtent relation Mining with bi-modal
Interpretability), a novel joint graph-language modeling framework for
detecting illicit drug use and interpreting behavioral risk factors among TYAs.
LAMI represents individual responses as relational graphs, learns latent
connections through a specialized graph structure learning layer, and
integrates a large language model to generate natural language explanations
grounded in both graph structures and survey semantics. Experiments on the YRBS
and NSDUH datasets show that LAMI outperforms competitive baselines in
predictive accuracy. Interpretability analyses further demonstrate that LAMI
reveals meaningful behavioral substructures and psychosocial pathways, such as
family dynamics, peer influence, and school-related distress, that align with
established risk factors for substance use.

</details>


### [80] [CTR-LoRA: Curvature-Aware and Trust-Region Guided Low-Rank Adaptation for Large Language Models](https://arxiv.org/abs/2510.15962)
*Zhuxuanzi Wang,Mingqiao Mo,Xi Xiao,Chen Liu,Chenrui Ma,Yunbei Zhang,Xiao Wang,Smita Krishnaswamy,Tianyang Wang*

Main category: cs.LG

TL;DR: CTR-LoRA: A parameter-efficient fine-tuning framework using curvature trust region for rank scheduling and stability-aware optimization.


<details>
  <summary>Details</summary>
Motivation: Existing PEFT methods decouple capacity allocation from update evolution during training, leading to inefficiencies.

Method: Introduces CTR-LoRA, which integrates rank scheduling with stability-aware optimization, allocating parameters based on marginal utility and constraining updates using a Fisher/Hessian-metric trust region.

Result: CTR-LoRA shows consistent improvements over strong PEFT baselines on multiple open-source backbones (7B-13B) in both in-distribution and out-of-distribution benchmarks. It also enhances training stability, reduces memory requirements, and achieves higher throughput.

Conclusion: CTR-LoRA provides a more robust and deployable approach to PEFT.

Abstract: Parameter-efficient fine-tuning (PEFT) has become the standard approach for
adapting large language models under limited compute and memory budgets.
Although previous methods improve efficiency through low-rank updates,
quantization, or heuristic budget reallocation, they often decouple the
allocation of capacity from the way updates evolve during training. In this
work, we introduce CTR-LoRA, a framework guided by curvature trust region that
integrates rank scheduling with stability-aware optimization. CTR-LoRA
allocates parameters based on marginal utility derived from lightweight
second-order proxies and constrains updates using a Fisher/Hessian-metric trust
region. Experiments on multiple open-source backbones (7B-13B), evaluated on
both in-distribution and out-of-distribution benchmarks, show consistent
improvements over strong PEFT baselines. In addition to increased accuracy,
CTR-LoRA enhances training stability, reduces memory requirements, and achieves
higher throughput, positioning it on the Pareto frontier of performance and
efficiency. These results highlight a principled path toward more robust and
deployable PEFT.

</details>


### [81] [BPL: Bias-adaptive Preference Distillation Learning for Recommender System](https://arxiv.org/abs/2510.16076)
*SeongKu Kang,Jianxun Lian,Dongha Lee,Wonbin Kweon,Sanghwan Jang,Jaehyun Lee,Jindong Wang,Xing Xie,Hwanjo Yu*

Main category: cs.LG

TL;DR: 提出了一个名为 Bias-adaptive Preference distillation Learning (BPL) 的新框架，以逐步揭示用户偏好，该框架在事实和反事实测试环境中均表现良好。


<details>
  <summary>Details</summary>
Motivation: 推荐系统存在偏差，导致收集到的反馈不完全反映用户偏好。以往的去偏学习主要集中在通过随机展示物品模拟的专门的反事实测试环境中，这会显著降低在基于实际用户-物品交互的典型的事实测试环境中的准确性。

Method: 引入双重蒸馏策略，从 biased 模型中进行 teacher-student 蒸馏，保留与收集到的反馈对齐的准确偏好知识。通过带有可靠性过滤的自蒸馏，迭代地细化模型在整个训练过程中的知识。

Result: 在事实和反事实测试中，综合实验验证了 BPL 的有效性。

Conclusion: BPL 框架能够提高推荐系统在不同测试环境下的性能，从而更好地满足用户需求。

Abstract: Recommender systems suffer from biases that cause the collected feedback to
incompletely reveal user preference. While debiasing learning has been
extensively studied, they mostly focused on the specialized (called
counterfactual) test environment simulated by random exposure of items,
significantly degrading accuracy in the typical (called factual) test
environment based on actual user-item interactions. In fact, each test
environment highlights the benefit of a different aspect: the counterfactual
test emphasizes user satisfaction in the long-terms, while the factual test
focuses on predicting subsequent user behaviors on platforms. Therefore, it is
desirable to have a model that performs well on both tests rather than only
one. In this work, we introduce a new learning framework, called Bias-adaptive
Preference distillation Learning (BPL), to gradually uncover user preferences
with dual distillation strategies. These distillation strategies are designed
to drive high performance in both factual and counterfactual test environments.
Employing a specialized form of teacher-student distillation from a biased
model, BPL retains accurate preference knowledge aligned with the collected
feedback, leading to high performance in the factual test. Furthermore, through
self-distillation with reliability filtering, BPL iteratively refines its
knowledge throughout the training process. This enables the model to produce
more accurate predictions across a broader range of user-item combinations,
thereby improving performance in the counterfactual test. Comprehensive
experiments validate the effectiveness of BPL in both factual and
counterfactual tests. Our implementation is accessible via:
https://github.com/SeongKu-Kang/BPL.

</details>


### [82] [Long Exposure: Accelerating Parameter-Efficient Fine-Tuning for LLMs under Shadowy Sparsity](https://arxiv.org/abs/2510.15964)
*Tuowei Wang,Kun Li,Zixu Hao,Donglin Bai,Ju Ren,Yaoxue Zhang,Ting Cao,Mao Yang*

Main category: cs.LG

TL;DR: 本文提出了一种名为 Long Exposure 的高效系统，用于加速大型语言模型 (LLM) 的参数高效微调 (PEFT)。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调 (PEFT) 技术效率低下，导致时间和运营成本方面的重大挑战。本文旨在解决这个问题。

Method: 本文首先介绍了一种细致的稀疏形式，称为 Shadowy Sparsity。然后，提出了 Long Exposure，它包含三个关键组件：Shadowy-sparsity Exposer、Sequence-oriented Predictor 和 Dynamic-aware Operator。

Result: Long Exposure 在端到端微调中实现了高达 2.49 倍的加速，优于现有技术。

Conclusion: Long Exposure 在加速 LLM 的 PEFT 方面提供了有希望的进步。

Abstract: The adaptation of pre-trained large language models (LLMs) to diverse
downstream tasks via fine-tuning is critical for numerous applications.
However, the inefficiency of parameter-efficient fine-tuning (PEFT) techniques
presents significant challenges in terms of time investments and operational
costs. In this paper, we first introduce a nuanced form of sparsity, termed
Shadowy Sparsity, which is distinctive in fine-tuning and has not been
adequately addressed for acceleration. Under Shadowy Sparsity, we propose Long
Exposure, an efficient system to accelerate PEFT for LLMs. Long Exposure
comprises three key components: Shadowy-sparsity Exposer employs a prolonged
sensing range to capture more sparsity details under shadowy sparsity;
Sequence-oriented Predictor provides efficient yet accurate predictions to
handle large sequence inputs and constantly-evolving parameters; and
Dynamic-aware Operator facilitates more structured computational patterns and
coalesced memory accesses, addressing dynamic sparse operations. Extensive
evaluations show that Long Exposure outperforms state-of-the-arts with up to a
$2.49\times$ speedup in end-to-end fine-tuning, offering promising advancements
in accelerating PEFT for LLMs.

</details>


### [83] [One Token Embedding Is Enough to Deadlock Your Large Reasoning Model](https://arxiv.org/abs/2510.15965)
*Mohan Zhang,Yihua Zhang,Jinghan Jia,Zhangyang Wang,Sijia Liu,Tianlong Chen*

Main category: cs.LG

TL;DR: 本文介绍了一种新的针对大型推理模型（LRM）的攻击方法，称为死锁攻击，该攻击通过诱导模型进入永久推理循环来耗尽资源。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过思维链（CoT）推理展示了令人印象深刻的多步问题解决能力。然而，这种迭代思维机制引入了一个新的脆弱性。

Method: 该方法通过训练恶意对抗嵌入来劫持LRM的生成控制流，该嵌入鼓励在推理步骤之后使用过渡token（例如，“Wait”，“But”），从而阻止模型得出答案。为了克服连续到离散的投影差距，引入了一种后门植入策略，通过特定的触发token实现可靠的激活。

Result: 该方法在四种先进的LRM（Phi-RM、Nemotron-Nano、R1-Qwen、R1-Llama）和三个数学推理基准测试中实现了100%的攻击成功率，迫使模型生成高达其最大token限制。

Conclusion: 研究结果揭示了从推理（不）效率的角度来看，LRM中一个关键且未被充分探索的安全漏洞。

Abstract: Modern large reasoning models (LRMs) exhibit impressive multi-step
problem-solving via chain-of-thought (CoT) reasoning. However, this iterative
thinking mechanism introduces a new vulnerability surface. We present the
Deadlock Attack, a resource exhaustion method that hijacks an LRM's generative
control flow by training a malicious adversarial embedding to induce perpetual
reasoning loops. Specifically, the optimized embedding encourages transitional
tokens (e.g., "Wait", "But") after reasoning steps, preventing the model from
concluding its answer. A key challenge we identify is the
continuous-to-discrete projection gap: na\"ive projections of adversarial
embeddings to token sequences nullify the attack. To overcome this, we
introduce a backdoor implantation strategy, enabling reliable activation
through specific trigger tokens. Our method achieves a 100% attack success rate
across four advanced LRMs (Phi-RM, Nemotron-Nano, R1-Qwen, R1-Llama) and three
math reasoning benchmarks, forcing models to generate up to their maximum token
limits. The attack is also stealthy (in terms of causing negligible utility
loss on benign user inputs) and remains robust against existing strategies
trying to mitigate the overthinking issue. Our findings expose a critical and
underexplored security vulnerability in LRMs from the perspective of reasoning
(in)efficiency.

</details>


### [84] [Resolution-Aware Retrieval Augmented Zero-Shot Forecasting](https://arxiv.org/abs/2510.16695)
*Iman Deznabi,Peeyush Kumar,Madalina Fiterau*

Main category: cs.LG

TL;DR: 本文提出了一种零样本预测模型，该模型通过利用空间相关性和时间频率特征来提高预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统预测方法在没有直接历史数据的情况下，预测先前未见条件的结果面临重大挑战。

Method: 该模型将信号分解为不同的频率分量，采用分辨率感知检索，其中较低频率分量依赖于更广泛的空间背景，而较高频率分量侧重于局部影响。这使得模型能够动态检索相关数据并适应具有最小历史背景的新位置。

Result: 该模型在微气候预测中显着优于传统预测方法、数值天气预报模型和现代基础时间序列模型，在 ERA5 数据集上实现的 MSE 比 HRRR 低 71%，比 Chronos 低 34%。

Conclusion: 研究结果强调了检索增强和分辨率感知策略的有效性，为微气候建模及其他领域的零样本预测提供了一种可扩展且数据高效的解决方案。

Abstract: Zero-shot forecasting aims to predict outcomes for previously unseen
conditions without direct historical data, posing a significant challenge for
traditional forecasting methods. We introduce a Resolution-Aware
Retrieval-Augmented Forecasting model that enhances predictive accuracy by
leveraging spatial correlations and temporal frequency characteristics. By
decomposing signals into different frequency components, our model employs
resolution-aware retrieval, where lower-frequency components rely on broader
spatial context, while higher-frequency components focus on local influences.
This allows the model to dynamically retrieve relevant data and adapt to new
locations with minimal historical context.
  Applied to microclimate forecasting, our model significantly outperforms
traditional forecasting methods, numerical weather prediction models, and
modern foundation time series models, achieving 71% lower MSE than HRRR and 34%
lower MSE than Chronos on the ERA5 dataset.
  Our results highlight the effectiveness of retrieval-augmented and
resolution-aware strategies, offering a scalable and data-efficient solution
for zero-shot forecasting in microclimate modeling and beyond.

</details>


### [85] [Gains: Fine-grained Federated Domain Adaptation in Open Set](https://arxiv.org/abs/2510.15967)
*Zhengyi Zhong,Wenzheng Jiang,Weidong Bao,Ji Wang,Cheems Wang,Guanbo Wang,Yongheng Deng,Ju Ren*

Main category: cs.LG

TL;DR: 本文提出了一种细粒度的联邦域适应方法，用于开放集场景，可以检测新知识并将其整合到全局模型中，同时保持源域性能。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习假设客户端数量固定，但实际场景中新客户端不断加入，引入新知识，需要检测新知识并整合到全局模型中。现有研究侧重于粗粒度的知识发现，并且常常牺牲源域性能和适应效率。

Method: 该方法将模型分解为编码器和分类器，发现编码器对域漂移敏感，而分类器参数对类别增量敏感。基于此，开发了细粒度的知识发现和贡献驱动的聚合技术来识别和整合新知识。此外，还设计了一种反遗忘机制来保持源域性能，确保平衡适应。

Result: 在跨三个典型数据漂移场景的多域数据集上的实验结果表明，Gains 在源域和目标域客户端的性能方面均显着优于其他基线。

Conclusion: Gains 是一种有效的细粒度联邦域适应方法，可以在开放集场景中实现知识发现和整合，同时保持源域性能。

Abstract: Conventional federated learning (FL) assumes a closed world with a fixed
total number of clients. In contrast, new clients continuously join the FL
process in real-world scenarios, introducing new knowledge. This raises two
critical demands: detecting new knowledge, i.e., knowledge discovery, and
integrating it into the global model, i.e., knowledge adaptation. Existing
research focuses on coarse-grained knowledge discovery, and often sacrifices
source domain performance and adaptation efficiency. To this end, we propose a
fine-grained federated domain adaptation approach in open set (Gains). Gains
splits the model into an encoder and a classifier, empirically revealing
features extracted by the encoder are sensitive to domain shifts while
classifier parameters are sensitive to class increments. Based on this, we
develop fine-grained knowledge discovery and contribution-driven aggregation
techniques to identify and incorporate new knowledge. Additionally, an
anti-forgetting mechanism is designed to preserve source domain performance,
ensuring balanced adaptation. Experimental results on multi-domain datasets
across three typical data-shift scenarios demonstrate that Gains significantly
outperforms other baselines in performance for both source-domain and
target-domain clients. Code is available at:
https://github.com/Zhong-Zhengyi/Gains.

</details>


### [86] [Self-Attention to Operator Learning-based 3D-IC Thermal Simulation](https://arxiv.org/abs/2510.15968)
*Zhen Huang,Hong Wang,Wenkai Yang,Muxi Tang,Depeng Xie,Ting-Jung Lin,Yu Zhang,Wei W. Xing,Lei He*

Main category: cs.LG

TL;DR: 提出了一种新的热管理框架SAU-FNO，用于解决3D集成电路中传统方法速度慢和机器学习方法丢失高频信息的问题。


<details>
  <summary>Details</summary>
Motivation: 由于功率密度较高，3D IC 中的热管理越来越具有挑战性。传统的基于 PDE 求解的方法虽然准确，但对于迭代设计来说太慢。像 FNO 这样的机器学习方法提供了更快的替代方案，但存在高频信息丢失和高保真数据依赖性问题。

Method: 结合了自注意力机制和 U-Net 与 FNO，以捕获远程依赖关系并有效地建模局部高频特征。采用迁移学习来微调低保真数据，从而最大限度地减少对大量高保真数据集的需求并加快训练。

Result: SAU-FNO 实现了最先进的热预测精度，并且比传统的 FEM 方法提供了 842 倍的加速。

Conclusion: SAU-FNO 是一种用于高级 3D IC 热仿真的高效工具。

Abstract: Thermal management in 3D ICs is increasingly challenging due to higher power
densities. Traditional PDE-solving-based methods, while accurate, are too slow
for iterative design. Machine learning approaches like FNO provide faster
alternatives but suffer from high-frequency information loss and high-fidelity
data dependency. We introduce Self-Attention U-Net Fourier Neural Operator
(SAU-FNO), a novel framework combining self-attention and U-Net with FNO to
capture long-range dependencies and model local high-frequency features
effectively. Transfer learning is employed to fine-tune low-fidelity data,
minimizing the need for extensive high-fidelity datasets and speeding up
training. Experiments demonstrate that SAU-FNO achieves state-of-the-art
thermal prediction accuracy and provides an 842x speedup over traditional FEM
methods, making it an efficient tool for advanced 3D IC thermal simulations.

</details>


### [87] [MemoryBench: A Benchmark for Memory and Continual Learning in LLM Systems](https://arxiv.org/abs/2510.17281)
*Qingyao Ai,Yichen Tang,Changyue Wang,Jianming Long,Weihang Su,Yiqun Liu*

Main category: cs.LG

TL;DR: 大型语言模型系统(LLMsys)通过扩大数据、参数和测试时计算来改进，但由于高质量数据的逐渐耗尽和更大的计算资源消耗带来的边际收益，其上限几乎达到。因此，为LLMsys构建记忆和持续学习框架已成为一个重要的研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有LLM记忆的基准通常侧重于评估系统在具有长格式输入的同质阅读理解任务上的能力，而不是测试它们从服务时间内累积的用户反馈中学习的能力。

Method: 我们提出了一个用户反馈模拟框架和一个综合基准，涵盖多个领域、语言和任务类型，以评估LLMsys的持续学习能力。

Result: 实验表明，最先进的基线的有效性和效率远不能令人满意。

Conclusion: 我们希望这个基准可以为未来LLM记忆和优化算法的研究铺平道路。

Abstract: Scaling up data, parameters, and test-time computation has been the
mainstream methods to improve LLM systems (LLMsys), but their upper bounds are
almost reached due to the gradual depletion of high-quality data and marginal
gains obtained from larger computational resource consumption. Inspired by the
abilities of human and traditional AI systems in learning from practice,
constructing memory and continual learning frameworks for LLMsys has become an
important and popular research direction in recent literature. Yet, existing
benchmarks for LLM memory often focus on evaluating the system on homogeneous
reading comprehension tasks with long-form inputs rather than testing their
abilities to learn from accumulated user feedback in service time. Therefore,
we propose a user feedback simulation framework and a comprehensive benchmark
covering multiple domains, languages, and types of tasks to evaluate the
continual learning abilities of LLMsys. Experiments show that the effectiveness
and efficiency of state-of-the-art baselines are far from satisfying, and we
hope this benchmark could pave the way for future studies on LLM memory and
optimization algorithms.

</details>
