<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 9]
- [cs.CV](#cs.CV) [Total: 8]
- [cs.AI](#cs.AI) [Total: 8]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.IR](#cs.IR) [Total: 11]
- [cs.LG](#cs.LG) [Total: 7]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Categorical Classification of Book Summaries Using Word Embedding Techniques](https://arxiv.org/abs/2507.21058)
*Kerem Keskin,Mümine Kaya Keleş*

Main category: cs.CL

TL;DR: 本书利用词嵌入、自然语言处理和机器学习算法对书籍摘要进行分类，发现支持向量机、朴素贝叶斯和逻辑回归模型以及 TF-IDF 和 One-Hot Encoder 词嵌入技术对土耳其文本效果更好。


<details>
  <summary>Details</summary>
Motivation: 利用书籍网站的书籍摘要和类别进行分类。

Method: 词嵌入方法、自然语言处理技术和机器学习算法，包括 one hot encoding、Word2Vec 和 Term Frequency - Inverse Document Frequency (TF-IDF)。

Result: 支持向量机、朴素贝叶斯和逻辑回归模型以及 TF-IDF 和 One-Hot Encoder 词嵌入技术为土耳其文本提供了更成功的结果。

Conclusion: SVM, 朴素贝叶斯和逻辑回归模型以及 TF-IDF 和 One-Hot Encoder 词嵌入技术为土耳其文本提供了更成功的结果。

Abstract: In this study, book summaries and categories taken from book sites were
classified using word embedding methods, natural language processing techniques
and machine learning algorithms. In addition, one hot encoding, Word2Vec and
Term Frequency - Inverse Document Frequency (TF-IDF) methods, which are
frequently used word embedding methods were used in this study and their
success was compared. Additionally, the combination table of the pre-processing
methods used is shown and added to the table. Looking at the results, it was
observed that Support Vector Machine, Naive Bayes and Logistic Regression
Models and TF-IDF and One-Hot Encoder word embedding techniques gave more
successful results for Turkish texts.

</details>


### [2] [Dialogic Social Learning for Artificial Agents: Enhancing LLM Ontology Acquisition through Mixed-Initiative Educational Interactions](https://arxiv.org/abs/2507.21065)
*Sabrina Patania,Luca Annese,Cansu Koyuturk,Azzurra Ruggeri,Dimitri Ognibene*

Main category: cs.CL

TL;DR: This paper explores socially mediated learning paradigms to improve LLMs' ability to acquire and apply new knowledge using an 'AI Social Gym' where AI learner agents engage in pedagogical dialogues with AI teacher agents.


<details>
  <summary>Details</summary>
Motivation: LLMs face challenges in acquiring and integrating complex knowledge online. Traditional AI training paradigms limit the models' ability to learn efficiently from interactions.

Method: A dynamic environment, termed the 'AI Social Gym', where an AI learner agent engages in dyadic pedagogical dialogues with knowledgeable AI teacher agents. These interactions emphasize external, structured dialogue as a core mechanism for knowledge acquisition.

Result: Dialogic approaches significantly enhance the LLM's ability to acquire and apply new knowledge, outperforming both unidirectional instructional methods and direct access to structured knowledge.

Conclusion: Integrating pedagogical and psychological insights into AI and robot training can substantially improve post-training knowledge acquisition and response quality, offering a complementary pathway to existing strategies like prompt engineering.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
processing extensive offline datasets. However, they often face challenges in
acquiring and integrating complex, knowledge online. Traditional AI training
paradigms, predominantly based on supervised learning or reinforcement
learning, mirror a 'Piagetian' model of independent exploration. These
approaches typically rely on large datasets and sparse feedback signals,
limiting the models' ability to learn efficiently from interactions. Drawing
inspiration from Vygotsky's sociocultural theory, this study explores the
potential of socially mediated learning paradigms to address these limitations.
  We introduce a dynamic environment, termed the 'AI Social Gym', where an AI
learner agent engages in dyadic pedagogical dialogues with knowledgeable AI
teacher agents. These interactions emphasize external, structured dialogue as a
core mechanism for knowledge acquisition, contrasting with methods that depend
solely on internal inference or pattern recognition.
  Our investigation focuses on how different pedagogical strategies impact the
AI learning process in the context of ontology acquisition. Empirical results
indicate that such dialogic approaches-particularly those involving
mixed-direction interactions combining top-down explanations with
learner-initiated questioning-significantly enhance the LLM's ability to
acquire and apply new knowledge, outperforming both unidirectional
instructional methods and direct access to structured knowledge, formats
typically present in training datasets.
  These findings suggest that integrating pedagogical and psychological
insights into AI and robot training can substantially improve post-training
knowledge acquisition and response quality. This approach offers a
complementary pathway to existing strategies like prompt engineering

</details>


### [3] [Product vs. Process: Exploring EFL Students' Editing of AI-Generated Text for Expository Writing](https://arxiv.org/abs/2507.21073)
*David James Woo,Yangyang Yu,Kai Guo,Yilin Huang,April Ka Yeng Fung*

Main category: cs.CL

TL;DR: This research examines how EFL secondary students edit AI-generated text and its impact on their expository writing. Results suggest that AI supports but does not replace writing skills, and editing efforts do not guarantee improved composition quality.


<details>
  <summary>Details</summary>
Motivation: The impact of AI chatbot-generated text on EFL students' expository writing process and compositions is understudied.

Method: A convergent design was employed to analyze students' screen recordings and compositions to examine their editing behaviors and writing qualities, using qualitative coding, descriptive statistics, temporal sequence analysis, human-rated scoring, and multiple linear regression analysis.

Result: The number of AI-generated words positively predicted all score dimensions, while most editing variables showed minimal impact. Two editing patterns were identified: refining introductory units repeatedly and quickly shifting to extensive edits in body units.

Conclusion: Students' editing effort does not necessarily improve composition quality, suggesting AI supports but does not replace writing skills.

Abstract: Text generated by artificial intelligence (AI) chatbots is increasingly used
in English as a foreign language (EFL) writing contexts, yet its impact on
students' expository writing process and compositions remains understudied.
This research examines how EFL secondary students edit AI-generated text.
Exploring editing behaviors in their expository writing process and in
expository compositions, and their effect on human-rated scores for content,
organization, language, and overall quality. Participants were 39 Hong Kong
secondary students who wrote an expository composition with AI chatbots in a
workshop. A convergent design was employed to analyze their screen recordings
and compositions to examine students' editing behaviors and writing qualities.
Analytical methods included qualitative coding, descriptive statistics,
temporal sequence analysis, human-rated scoring, and multiple linear regression
analysis. We analyzed over 260 edits per dataset, and identified two editing
patterns: one where students refined introductory units repeatedly before
progressing, and another where they quickly shifted to extensive edits in body
units (e.g., topic and supporting sentences). MLR analyses revealed that the
number of AI-generated words positively predicted all score dimensions, while
most editing variables showed minimal impact. These results suggest a
disconnect between students' significant editing effort and improved
composition quality, indicating AI supports but does not replace writing
skills. The findings highlight the importance of genre-specific instruction and
process-focused writing before AI integration. Educators should also develop
assessments valuing both process and product to encourage critical engagement
with AI text.

</details>


### [4] [Which symbol grounding problem should we try to solve?](https://arxiv.org/abs/2507.21080)
*Vincent C. Müller*

Main category: cs.CL

TL;DR: Floridi 和 Taddeo 提出了“零语义承诺”条件来解决 grounding 问题，但这个条件无法满足。我们需要重新思考 grounding 问题是什么。


<details>
  <summary>Details</summary>
Motivation: Floridi 和 Taddeo 提出了“零语义承诺”条件来解决 grounding 问题，并提出了解决方案。我认为他们的条件无法满足，甚至他们自己的解决方案也无法满足。在看了一下 Luc Steels 非常不同的竞争性建议后，我认为我们需要重新思考问题是什么，以及系统中“目标”在制定问题中扮演的角色。

Method: 对计算的正确理解

Result: 他们的条件无法满足，甚至他们自己的解决方案也无法满足

Conclusion: 唯一的合理的 grounding 问题是，我们如何解释和重现人工计算代理中意义的行为能力和功能。

Abstract: Floridi and Taddeo propose a condition of "zero semantic commitment" for
solutions to the grounding problem, and a solution to it. I argue briefly that
their condition cannot be fulfilled, not even by their own solution. After a
look at Luc Steels' very different competing suggestion, I suggest that we need
to re-think what the problem is and what role the 'goals' in a system play in
formulating the problem. On the basis of a proper understanding of computing, I
come to the conclusion that the only sensible grounding problem is how we can
explain and re-produce the behavioral ability and function of meaning in
artificial computational agents

</details>


### [5] [StructText: A Synthetic Table-to-Text Approach for Benchmark Generation with Multi-Dimensional Evaluation](https://arxiv.org/abs/2507.21340)
*Satyananda Kashyap,Sola Shirai,Nandana Mihindukulasooriya,Horst Samulowitz*

Main category: cs.CL

TL;DR: 提出了StructText，一个利用表格数据自动生成键值提取基准的框架。


<details>
  <summary>Details</summary>
Motivation: 缺乏用于评估大型语言模型（LLM）提取质量的基准，尤其是在特定领域或特定于给定组织的重点文档中。手动注释构建此类基准既费力又限制了基准的大小和可扩展性。

Method: 提出了一个端到端的框架StructText，用于从文本中自动生成键值提取的高保真基准，使用现有的表格数据。它使用可用的表格数据作为结构化的基本事实，并遵循一个两阶段的“计划-然后执行”流程来合成生成相应的自然语言文本。

Result: 在 49 个数据集的 71,539 个示例上评估了该方法。结果表明，虽然 LLM 实现了强大的事实准确性并避免了幻觉，但它们在生成可提取文本的叙述连贯性方面存在困难。

Conclusion: LLMs表现出强大的事实准确性并避免了幻觉，但在生成可提取文本的叙述连贯性方面存在困难。模型能高度保真地推定数值和时间信息，但这些信息嵌入在难以自动提取的叙述中。

Abstract: Extracting structured information from text, such as key-value pairs that
could augment tabular data, is quite useful in many enterprise use cases.
Although large language models (LLMs) have enabled numerous automated pipelines
for converting natural language into structured formats, there is still a lack
of benchmarks for evaluating their extraction quality, especially in specific
domains or focused documents specific to a given organization. Building such
benchmarks by manual annotations is labour-intensive and limits the size and
scalability of the benchmarks. In this work, we present StructText, an
end-to-end framework for automatically generating high-fidelity benchmarks for
key-value extraction from text using existing tabular data. It uses available
tabular data as structured ground truth, and follows a two-stage
``plan-then-execute'' pipeline to synthetically generate corresponding
natural-language text. To ensure alignment between text and structured source,
we introduce a multi-dimensional evaluation strategy that combines (a)
LLM-based judgments on factuality, hallucination, and coherence and (b)
objective extraction metrics measuring numeric and temporal accuracy. We
evaluated the proposed method on 71,539 examples across 49 datasets. Results
reveal that while LLMs achieve strong factual accuracy and avoid hallucination,
they struggle with narrative coherence in producing extractable text. Notably,
models presume numerical and temporal information with high fidelity yet this
information becomes embedded in narratives that resist automated extraction. We
release a framework, including datasets, evaluation tools, and baseline
extraction systems, to support continued research.

</details>


### [6] [ChatGPT Reads Your Tone and Responds Accordingly -- Until It Does Not -- Emotional Framing Induces Bias in LLM Outputs](https://arxiv.org/abs/2507.21083)
*Franck Bardol*

Main category: cs.CL

TL;DR: GPT-4's responses are significantly influenced by the emotional tone of prompts, showing a bias towards neutrality or positivity, especially on sensitive topics.


<details>
  <summary>Details</summary>
Motivation: Investigating how the emotional phrasing of prompts affects the responses of Large Language Models like GPT-4.

Method: Systematically varied the emotional tone of 156 prompts across controversial and everyday topics and analyzed model responses.

Result: GPT-4 is three times less likely to respond negatively to a negatively framed question than to a neutral one. The study introduces concepts like the 'tone floor' and uses tone-valence transition matrices to quantify behavior. Visualizations confirm semantic drift based on tone.

Conclusion: GPT-4 exhibits a 'rebound' bias, overcorrecting in response to negative prompts, especially on sensitive topics where tone-based variation is suppressed, suggesting an alignment override.

Abstract: Large Language Models like GPT-4 adjust their responses not only based on the
question asked, but also on how it is emotionally phrased. We systematically
vary the emotional tone of 156 prompts - spanning controversial and everyday
topics - and analyze how it affects model responses. Our findings show that
GPT-4 is three times less likely to respond negatively to a negatively framed
question than to a neutral one. This suggests a "rebound" bias where the model
overcorrects, often shifting toward neutrality or positivity. On sensitive
topics (e.g., justice or politics), this effect is even more pronounced:
tone-based variation is suppressed, suggesting an alignment override. We
introduce concepts like the "tone floor" - a lower bound in response negativity
- and use tone-valence transition matrices to quantify behavior. Visualizations
based on 1536-dimensional embeddings confirm semantic drift based on tone. Our
work highlights an underexplored class of biases driven by emotional framing in
prompts, with implications for AI alignment and trust. Code and data are
available at: https://github.com/bardolfranck/llm-responses-viewer

</details>


### [7] [Reviving Your MNEME: Predicting The Side Effects of LLM Unlearning and Fine-Tuning via Sparse Model Diffing](https://arxiv.org/abs/2507.21084)
*Aly M. Kassem,Zhuan Shi,Negar Rostamzadeh,Golnoosh Farnadi*

Main category: cs.CL

TL;DR: MNEME 通过稀疏模型差异化来识别大型语言模型微调中的副作用。


<details>
  <summary>Details</summary>
Motivation: 现有的评估方法评估干预后的性能，但仍然没有通用的方法来检测意外的副作用，例如生物学内容的遗忘降低了化学任务的性能，尤其是在这些影响是不可预测的或突发的情况下。

Method: MNEME 比较基础模型和微调模型在任务无关数据上的表现，无需访问微调数据即可隔离行为变化。

Result: MNEME 在预测副作用方面达到了高达 95% 的准确率，与已知基准对齐，并且不需要自定义启发式方法。此外，在重新训练高激活样本后，可以部分逆转这些影响。

Conclusion: 稀疏探查和差异化提供了一个可扩展的自动化视角，可以了解微调引起的模型变化，从而为理解和管理 LLM 行为提供实用的工具。

Abstract: Large language models (LLMs) are frequently fine-tuned or unlearned to adapt
to new tasks or eliminate undesirable behaviors. While existing evaluation
methods assess performance after such interventions, there remains no general
approach for detecting unintended side effects, such as unlearning biology
content degrading performance on chemistry tasks, particularly when these
effects are unpredictable or emergent. To address this issue, we introduce
MNEME, Model diffiNg for Evaluating Mechanistic Effects, a lightweight
framework for identifying these side effects using sparse model diffing. MNEME
compares base and fine-tuned models on task-agnostic data (for example, The
Pile, LMSYS-Chat-1M) without access to fine-tuning data to isolate behavioral
shifts. Applied to five LLMs across three scenarios: WMDP knowledge unlearning,
emergent misalignment, and benign fine-tuning, MNEME achieves up to 95 percent
accuracy in predicting side effects, aligning with known benchmarks and
requiring no custom heuristics. Furthermore, we show that retraining on
high-activation samples can partially reverse these effects. Our results
demonstrate that sparse probing and diffing offer a scalable and automated lens
into fine-tuning-induced model changes, providing practical tools for
understanding and managing LLM behavior.

</details>


### [8] [Multi-Amateur Contrastive Decoding for Text Generation](https://arxiv.org/abs/2507.21086)
*Jaydip Sen,Subhasis Dasgupta,Hetvi Waghela*

Main category: cs.CL

TL;DR: This paper introduces Multi-Amateur Contrastive Decoding (MACD), which enhances text generation by using multiple amateur models to overcome the limitations of single-amateur contrastive decoding. MACD improves fluency, coherence, diversity, and adaptability without extra training.


<details>
  <summary>Details</summary>
Motivation: Contrastive Decoding (CD) improves coherence and fluency, its dependence on a single amateur restricts its capacity to capture the diverse and multifaceted failure modes of language generation, such as repetition, hallucination, and stylistic drift.

Method: This paper proposes Multi-Amateur Contrastive Decoding (MACD), a generalization of the CD framework that employs an ensemble of amateur models to more comprehensively characterize undesirable generation patterns. MACD integrates contrastive signals through both averaging and consensus penalization mechanisms and extends the plausibility constraint to operate effectively in the multi-amateur setting. Furthermore, the framework enables controllable generation by incorporating amateurs with targeted stylistic or content biases.

Result: Experimental results across multiple domains, such as news, encyclopedic, and narrative, demonstrate that MACD consistently surpasses conventional decoding methods and the original CD approach in terms of fluency, coherence, diversity, and adaptability, all without requiring additional training or fine-tuning.

Conclusion: Multi-Amateur Contrastive Decoding (MACD) consistently surpasses conventional decoding methods and the original CD approach in terms of fluency, coherence, diversity, and adaptability, all without requiring additional training or fine-tuning.

Abstract: Contrastive Decoding (CD) has emerged as an effective inference-time strategy
for enhancing open-ended text generation by exploiting the divergence in output
probabilities between a large expert language model and a smaller amateur
model. Although CD improves coherence and fluency, its dependence on a single
amateur restricts its capacity to capture the diverse and multifaceted failure
modes of language generation, such as repetition, hallucination, and stylistic
drift. This paper proposes Multi-Amateur Contrastive Decoding (MACD), a
generalization of the CD framework that employs an ensemble of amateur models
to more comprehensively characterize undesirable generation patterns. MACD
integrates contrastive signals through both averaging and consensus
penalization mechanisms and extends the plausibility constraint to operate
effectively in the multi-amateur setting. Furthermore, the framework enables
controllable generation by incorporating amateurs with targeted stylistic or
content biases. Experimental results across multiple domains, such as news,
encyclopedic, and narrative, demonstrate that MACD consistently surpasses
conventional decoding methods and the original CD approach in terms of fluency,
coherence, diversity, and adaptability, all without requiring additional
training or fine-tuning.

</details>


### [9] [QU-NLP at CheckThat! 2025: Multilingual Subjectivity in News Articles Detection using Feature-Augmented Transformer Models with Sequential Cross-Lingual Fine-Tuning](https://arxiv.org/abs/2507.21095)
*Mohammad AL-Smadi*

Main category: cs.CL

TL;DR: This paper introduces a feature-augmented transformer architecture for subjectivity detection in news articles, achieving competitive results across multiple languages in monolingual, multilingual, and zero-shot settings.


<details>
  <summary>Details</summary>
Motivation: This paper presents our approach to the CheckThat! 2025 Task 1 on subjectivity detection, where systems are challenged to distinguish whether a sentence from a news article expresses the subjective view of the author or presents an objective view on the covered topic.

Method: We propose a feature-augmented transformer architecture that combines contextual embeddings from pre-trained language models with statistical and linguistic features. Our system leveraged pre-trained transformers with additional lexical features: for Arabic we used AraELECTRA augmented with part-of-speech (POS) tags and TF-IDF features, while for the other languages we fine-tuned a cross-lingual DeBERTa~V3 model combined with TF-IDF features through a gating mechanism.

Result: achieving competitive performance across different languages with notable success in the monolingual setting for English (rank 1st with macro-F1=0.8052), German (rank 3rd with macro-F1=0.8013), Arabic (rank 4th with macro-F1=0.5771), and Romanian (rank 1st with macro-F1=0.8126) in the zero-shot setting

Conclusion: The results demonstrate the effectiveness of our approach, achieving competitive performance across different languages with notable success in the monolingual setting for English, German, Arabic, and Romanian in the zero-shot setting.The analysis reveals the model's sensitivity to both the order of cross-lingual fine-tuning and the linguistic proximity of the training languages.

Abstract: This paper presents our approach to the CheckThat! 2025 Task 1 on
subjectivity detection, where systems are challenged to distinguish whether a
sentence from a news article expresses the subjective view of the author or
presents an objective view on the covered topic. We propose a feature-augmented
transformer architecture that combines contextual embeddings from pre-trained
language models with statistical and linguistic features. Our system leveraged
pre-trained transformers with additional lexical features: for Arabic we used
AraELECTRA augmented with part-of-speech (POS) tags and TF-IDF features, while
for the other languages we fine-tuned a cross-lingual DeBERTa~V3 model combined
with TF-IDF features through a gating mechanism. We evaluated our system in
monolingual, multilingual, and zero-shot settings across multiple languages
including English, Arabic, German, Italian, and several unseen languages. The
results demonstrate the effectiveness of our approach, achieving competitive
performance across different languages with notable success in the monolingual
setting for English (rank 1st with macro-F1=0.8052), German (rank 3rd with
macro-F1=0.8013), Arabic (rank 4th with macro-F1=0.5771), and Romanian (rank
1st with macro-F1=0.8126) in the zero-shot setting. We also conducted an
ablation analysis that demonstrated the importance of combining TF-IDF features
with the gating mechanism and the cross-lingual transfer for subjectivity
detection. Furthermore, our analysis reveals the model's sensitivity to both
the order of cross-lingual fine-tuning and the linguistic proximity of the
training languages.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [10] [GAITEX: Human motion dataset from impaired gait and rehabilitation exercises of inertial and optical sensor data](https://arxiv.org/abs/2507.21069)
*Andreas Spilz,Heiko Oppel,Jochen Werner,Kathrin Stucke-Straub,Felix Capanni,Michael Munz*

Main category: cs.CV

TL;DR: 本研究提出了一个用于人体运动分析的多模态数据集，该数据集包含理疗练习和步态数据，旨在加速机器学习驱动的人体运动分析研究。


<details>
  <summary>Details</summary>
Motivation: 开发用于理疗练习和步态分析的基于传感器的稳健分类模型需要大型、多样化的数据集，而这些数据集的收集成本高且耗时。

Method: 使用同步 IMU 和基于标记的运动捕捉 (MoCap) 从 19 名参与者记录的理疗练习（包括正确和临床相关的变体）和步态相关练习（包括正常和受损的步态模式）的多模态数据集。

Result: 该数据集包括来自 9 个 IMU 和 35 个光学标记的原始数据，这些数据捕获了全身运动学。每个 IMU 还配备了 4 个光学标记，从而可以精确比较 IMU 衍生的方向估计值与 MoCap 系统的参考值。为了支持进一步分析，我们还提供与常见片段坐标系对齐的经过处理的 IMU 方向、特定于主体的 OpenSim 模型、逆运动学结果以及用于在肌肉骨骼环境中可视化 IMU 方向的工具。运动执行质量和带时间戳的分割的详细注释支持多样化的分析目标。

Conclusion: 该数据集支持开发和评估用于自动运动评估、步态分析、时间活动分割和生物力学参数估计等任务的机器学习模型，并提供用于后处理、传感器到片段对齐、逆运动学计算和技术验证的代码，旨在加速机器学习驱动的人体运动分析研究。

Abstract: Wearable inertial measurement units (IMUs) offer a cost-effective and
scalable means to assess human movement quality in clinical and everyday
settings. However, the development of robust sensor-based classification models
for physiotherapeutic exercises and gait analysis requires large, diverse
datasets, which are costly and time-consuming to collect. Here, we present a
multimodal dataset of physiotherapeutic exercises - including correct and
clinically relevant variants - and gait-related exercises - including both
normal and impaired gait patterns - recorded from 19 participants using
synchronized IMUs and marker-based motion capture (MoCap). The dataset includes
raw data from nine IMUs and thirty-five optical markers capturing full-body
kinematics. Each IMU is additionally equipped with four optical markers,
enabling precise comparison between IMU-derived orientation estimates and
reference values from the MoCap system. To support further analysis, we also
provide processed IMU orientations aligned with common segment coordinate
systems, subject-specific OpenSim models, inverse kinematics results, and tools
for visualizing IMU orientations in the musculoskeletal context. Detailed
annotations of movement execution quality and time-stamped segmentations
support diverse analysis goals. This dataset supports the development and
benchmarking of machine learning models for tasks such as automatic exercise
evaluation, gait analysis, temporal activity segmentation, and biomechanical
parameter estimation. To facilitate reproducibility, we provide code for
postprocessing, sensor-to-segment alignment, inverse kinematics computation,
and technical validation. This resource is intended to accelerate research in
machine learning-driven human movement analysis.

</details>


### [11] [Seeing Beyond Frames: Zero-Shot Pedestrian Intention Prediction with Raw Temporal Video and Multimodal Cues](https://arxiv.org/abs/2507.21161)
*Pallavi Zambare,Venkata Nikhil Thanikella,Ying Liu*

Main category: cs.CV

TL;DR: introduce BF-PIP, a zero-shot approach built upon Gemini 2.5 Pro, achieves 73% prediction accuracy, outperforming a GPT-4V baseline by 18 %


<details>
  <summary>Details</summary>
Motivation: Pedestrian intention prediction is essential for autonomous driving in complex urban environments. Conventional approaches depend on supervised learning over frame sequences and require extensive retraining to adapt to new scenarios.

Method: a zero-shot approach built upon Gemini 2.5 Pro. It infers crossing intentions directly from short, continuous video clips enriched with structured JAAD metadata. It also incorporates bounding-box annotations and ego-vehicle speed via specialized multimodal prompts.

Result: achieves 73% prediction accuracy, outperforming a GPT-4V baseline by 18 %

Conclusion: combining temporal video inputs with contextual cues enhances spatiotemporal perception and improves intent inference under ambiguous conditions. This approach paves the way for agile, retraining-free perception module in intelligent transportation system.

Abstract: Pedestrian intention prediction is essential for autonomous driving in
complex urban environments. Conventional approaches depend on supervised
learning over frame sequences and require extensive retraining to adapt to new
scenarios. Here, we introduce BF-PIP (Beyond Frames Pedestrian Intention
Prediction), a zero-shot approach built upon Gemini 2.5 Pro. It infers crossing
intentions directly from short, continuous video clips enriched with structured
JAAD metadata. In contrast to GPT-4V based methods that operate on discrete
frames, BF-PIP processes uninterrupted temporal clips. It also incorporates
bounding-box annotations and ego-vehicle speed via specialized multimodal
prompts. Without any additional training, BF-PIP achieves 73% prediction
accuracy, outperforming a GPT-4V baseline by 18 %. These findings illustrate
that combining temporal video inputs with contextual cues enhances
spatiotemporal perception and improves intent inference under ambiguous
conditions. This approach paves the way for agile, retraining-free perception
module in intelligent transportation system.

</details>


### [12] [ChartM$^3$: Benchmarking Chart Editing with Multimodal Instructions](https://arxiv.org/abs/2507.21167)
*Danglu Yang,Liang Zhang,Zihao Yue,Liangyu Chen,Yichen Xu,Wenxuan Wang,Qin Jin*

Main category: cs.CV

TL;DR: Introduces a new multimodal chart editing paradigm using natural language and visual indicators, a new benchmark dataset (ChartM3), and a training set (ChartM3-Train) to improve MLLM performance in chart editing.


<details>
  <summary>Details</summary>
Motivation: Existing methods primarily rely on natural language instructions, which are often too ambiguous to support fine-grained editing of charts.

Method: A new benchmark for multimodal chart editing with multi-level complexity and multi-perspective evaluation, ChartM3, and a large-scale training set with 24,000 multimodal chart editing samples, ChartM3-Train.

Result: Benchmark reveals significant limitations in current multimodal large language models (MLLMs), including GPT-4o, particularly in their ability to interpret and act on visual indicators.

Conclusion: Fine-tuning MLLMs on a new dataset leads to substantial improvements, demonstrating the importance of multimodal supervision in building practical chart editing systems.

Abstract: Charts are a fundamental visualization format widely used in data analysis
across research and industry. While enabling users to edit charts based on
high-level intentions is of great practical value, existing methods primarily
rely on natural language instructions, which are often too ambiguous to support
fine-grained editing. In this work, we introduce a novel paradigm for
multimodal chart editing, where user intent is expressed through a combination
of natural language and visual indicators that explicitly highlight the
elements to be modified. To support this paradigm, we present
Chart$\text{M}^3$, a new benchmark for Multimodal chart editing with
Multi-level complexity and Multi-perspective evaluation. Chart$\text{M}^3$
contains 1,000 samples spanning four levels of editing difficulty. Each sample
includes triplets in the form of (chart, code, multimodal instructions). To
comprehensively evaluate chart editing models, Chart$\text{M}^3$ provides
metrics that assess both visual appearance and code correctness. Our benchmark
reveals significant limitations in current multimodal large language models
(MLLMs), including GPT-4o, particularly in their ability to interpret and act
on visual indicators. To address this, we construct Chart$\text{M}^3$-Train, a
large-scale training set with 24,000 multimodal chart editing samples.
Fine-tuning MLLMs on this dataset leads to substantial improvements,
demonstrating the importance of multimodal supervision in building practical
chart editing systems. Our datasets, codes, and evaluation tools are available
at https://github.com/MLrollIT/ChartM3. %https://github.com/MLrollIT/ChartM3Our
datasets, codes, and evaluation tools are available at
https://github.com/yaolinli/VCE.

</details>


### [13] [PanoGAN A Deep Generative Model for Panoramic Dental Radiographs](https://arxiv.org/abs/2507.21200)
*Soren Pedersen,Sanyam Jain,Mikkel Chavez,Viktor Ladehoff,Bruna Neves de Freitas,Ruben Pauwels*

Main category: cs.CV

TL;DR: Developed a GAN to generate dental panoramic radiographs to address data scarcity. Clinical expert evaluation showed moderate anatomical depiction with trade-offs between detail and clarity.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the scarcity of data in dental research and education.

Method: We trained a deep convolutional GAN (DCGAN) using a Wasserstein loss with gradient penalty (WGANGP) on a dataset of 2322 radiographs of varying quality.

Result: Most images showed moderate anatomical depiction, although some were degraded by artifacts. A trade-off was observed the model trained on non-denoised data yielded finer details especially in structures like the mandibular canal and trabecular bone, while a model trained on denoised data offered superior overall image clarity and sharpness.

Conclusion: These findings provide a foundation for future work on GAN-based methods in dental imaging.

Abstract: This paper presents the development of a generative adversarial network (GAN)
for synthesizing dental panoramic radiographs. Although exploratory in nature,
the study aims to address the scarcity of data in dental research and
education. We trained a deep convolutional GAN (DCGAN) using a Wasserstein loss
with gradient penalty (WGANGP) on a dataset of 2322 radiographs of varying
quality. The focus was on the dentoalveolar regions, other anatomical
structures were cropped out. Extensive preprocessing and data cleaning were
performed to standardize the inputs while preserving anatomical variability. We
explored four candidate models by varying critic iterations, feature depth, and
the use of denoising prior to training. A clinical expert evaluated the
generated radiographs based on anatomical visibility and realism, using a
5-point scale (1 very poor 5 excellent). Most images showed moderate anatomical
depiction, although some were degraded by artifacts. A trade-off was observed
the model trained on non-denoised data yielded finer details especially in
structures like the mandibular canal and trabecular bone, while a model trained
on denoised data offered superior overall image clarity and sharpness. These
findings provide a foundation for future work on GAN-based methods in dental
imaging.

</details>


### [14] [On Explaining Visual Captioning with Hybrid Markov Logic Networks](https://arxiv.org/abs/2507.21246)
*Monika Shah,Somdeb Sarkhel,Deepak Venugopal*

Main category: cs.CV

TL;DR: develop a novel explanation framework based on Hybrid Markov Logic Networks to interpret how models integrate information to generate captions


<details>
  <summary>Details</summary>
Motivation: explaining/interpreting how these models integrate visual information, language information and knowledge representation to generate meaningful captions remains a challenging problem. Standard metrics to measure performance typically rely on comparing generated captions with human-written ones that may not provide a user with a deep insights into this integration.

Method: develop a novel explanation framework that is easily interpretable based on Hybrid Markov Logic Networks (HMLNs)

Result: learn a HMLN distribution over the training instances and infer the shift in distributions over these instances when we condition on the generated sample which allows us to quantify which examples may have been a source of richer information to generate the observed caption.

Conclusion: Experiments on captions generated for several state-of-the-art captioning models using Amazon Mechanical Turk illustrate the interpretability of our explanations, and allow us to compare these models along the dimension of explainability.

Abstract: Deep Neural Networks (DNNs) have made tremendous progress in multimodal tasks
such as image captioning. However, explaining/interpreting how these models
integrate visual information, language information and knowledge representation
to generate meaningful captions remains a challenging problem. Standard metrics
to measure performance typically rely on comparing generated captions with
human-written ones that may not provide a user with a deep insights into this
integration. In this work, we develop a novel explanation framework that is
easily interpretable based on Hybrid Markov Logic Networks (HMLNs) - a language
that can combine symbolic rules with real-valued functions - where we
hypothesize how relevant examples from the training data could have influenced
the generation of the observed caption. To do this, we learn a HMLN
distribution over the training instances and infer the shift in distributions
over these instances when we condition on the generated sample which allows us
to quantify which examples may have been a source of richer information to
generate the observed caption. Our experiments on captions generated for
several state-of-the-art captioning models using Amazon Mechanical Turk
illustrate the interpretability of our explanations, and allow us to compare
these models along the dimension of explainability.

</details>


### [15] [Dual Guidance Semi-Supervised Action Detection](https://arxiv.org/abs/2507.21247)
*Ankit Singh,Efstratios Gavves,Cees G. M. Snoek,Hilde Kuehne*

Main category: cs.CV

TL;DR: This paper presents a semi-supervised approach for spatial-temporal action localization using a dual guidance network, achieving superior results with limited labeled data.


<details>
  <summary>Details</summary>
Motivation: Semi-Supervised Learning (SSL) has shown tremendous potential to improve the predictive performance of deep learning models when annotations are hard to obtain. However, the application of SSL has so far been mainly studied in the context of image classification.

Method: A dual guidance network is introduced to select better pseudo-bounding boxes, combining frame-level classification with bounding-box prediction to enforce action class consistency.

Result: The proposed module considerably enhances the model's performance in limited labeled data settings on UCF101-24, J-HMDB-21 and AVA datasets.

Conclusion: The proposed semi-supervised framework achieves superior results compared to extended image-based semi-supervised baselines in spatial-temporal action localization.

Abstract: Semi-Supervised Learning (SSL) has shown tremendous potential to improve the
predictive performance of deep learning models when annotations are hard to
obtain. However, the application of SSL has so far been mainly studied in the
context of image classification. In this work, we present a semi-supervised
approach for spatial-temporal action localization. We introduce a dual guidance
network to select better pseudo-bounding boxes. It combines a frame-level
classification with a bounding-box prediction to enforce action class
consistency across frames and boxes. Our evaluation across well-known
spatial-temporal action localization datasets, namely UCF101-24 , J-HMDB-21 and
AVA shows that the proposed module considerably enhances the model's
performance in limited labeled data settings. Our framework achieves superior
results compared to extended image-based semi-supervised baselines.

</details>


### [16] [Tracking Moose using Aerial Object Detection](https://arxiv.org/abs/2507.21256)
*Christopher Indris,Raiyan Rahman,Goetz Bramesfeld,Guanghui Wang*

Main category: cs.CV

TL;DR: This paper studies the performance of object detection models on aerial wildlife tracking using patching augmentation. It finds that simpler models are as effective as more complex ones, encouraging UAV deployment.


<details>
  <summary>Details</summary>
Motivation: Aerial wildlife tracking is critical for conservation efforts and relies on detecting small objects on the ground below the aircraft. It presents technical challenges: crewed aircraft are expensive, risky and disruptive; autonomous drones have limited computational capacity for onboard AI systems. Since the objects of interest may appear only a few pixels wide, small object detection is an inherently challenging computer vision subfield compounded by computational efficiency needs.

Method: This paper applies a patching augmentation to datasets to study model performance under various settings. A comparative study of three common yet architecturally diverse object detectors is conducted using the data, varying the patching method's hyperparameters against detection accuracy.

Result: Each model achieved at least 93% mAP@IoU=0.5 on at least one patching configuration. Statistical analyses provide an in-depth commentary on the effects of various factors. Analysis also shows that faster, simpler models are about as effective as models that require more computational power for this task and perform well given limited patch scales, encouraging UAV deployment.

Conclusion: Each model achieved at least 93% mAP@IoU=0.5 on at least one patching configuration. Statistical analyses provide an in-depth commentary on the effects of various factors. Analysis also shows that faster, simpler models are about as effective as models that require more computational power for this task and perform well given limited patch scales, encouraging UAV deployment.

Abstract: Aerial wildlife tracking is critical for conservation efforts and relies on
detecting small objects on the ground below the aircraft. It presents technical
challenges: crewed aircraft are expensive, risky and disruptive; autonomous
drones have limited computational capacity for onboard AI systems. Since the
objects of interest may appear only a few pixels wide, small object detection
is an inherently challenging computer vision subfield compounded by
computational efficiency needs. This paper applies a patching augmentation to
datasets to study model performance under various settings. A comparative study
of three common yet architecturally diverse object detectors is conducted using
the data, varying the patching method's hyperparameters against detection
accuracy. Each model achieved at least 93\% mAP@IoU=0.5 on at least one
patching configuration. Statistical analyses provide an in-depth commentary on
the effects of various factors. Analysis also shows that faster, simpler models
are about as effective as models that require more computational power for this
task and perform well given limited patch scales, encouraging UAV deployment.
Datasets and models will be made available via
https://github.com/chrisindris/Moose.

</details>


### [17] [HDR Environment Map Estimation with Latent Diffusion Models](https://arxiv.org/abs/2507.21261)
*Jack Hilliard,Adrian Hilton,Jean-Yves Guillemaut*

Main category: cs.CV

TL;DR: 利用潜在扩散模型，该论文提出了一种从单视图图像估计 HDR 环境图的新方法。


<details>
  <summary>Details</summary>
Motivation: 从单视图图像估计 HDR 环境图。

Method: 利用潜在扩散模型 (LDM) 提出了一种新方法，以生成能够合理照亮镜面反射表面的高质量环境图。

Result: 所提出的 PanoDiT 网络减少了 ERP 扭曲和伪影，但以图像质量和合理性为代价。提出的 ERP 卷积填充消除了边界接缝伪影。

Conclusion: 该模型估计的高质量环境图在图像质量和光照精度方面与最先进的方法相比具有竞争力。

Abstract: We advance the field of HDR environment map estimation from a single-view
image by establishing a novel approach leveraging the Latent Diffusion Model
(LDM) to produce high-quality environment maps that can plausibly light
mirror-reflective surfaces. A common issue when using the ERP representation,
the format used by the vast majority of approaches, is distortions at the poles
and a seam at the sides of the environment map. We remove the border seam
artefact by proposing an ERP convolutional padding in the latent autoencoder.
Additionally, we investigate whether adapting the diffusion network
architecture to the ERP format can improve the quality and accuracy of the
estimated environment map by proposing a panoramically-adapted Diffusion
Transformer architecture. Our proposed PanoDiT network reduces ERP distortions
and artefacts, but at the cost of image quality and plausibility. We evaluate
with standard benchmarks to demonstrate that our models estimate high-quality
environment maps that perform competitively with state-of-the-art approaches in
both image quality and lighting accuracy.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [18] [SynLang and Symbiotic Epistemology: A Manifesto for Conscious Human-AI Collaboration](https://arxiv.org/abs/2507.21067)
*Jan Kapusta*

Main category: cs.AI

TL;DR: This paper introduces Symbiotic Epistemology and SynLang for transparent human-AI collaboration, fostering calibrated trust and enhancing human intelligence.


<details>
  <summary>Details</summary>
Motivation: Current AI systems rely on opaque reasoning processes that hinder human oversight and collaborative potential. Conventional explainable AI approaches offer post-hoc justifications and often fail to establish genuine symbiotic collaboration.

Method: The framework is empirically validated through actual human-AI dialogues demonstrating AI's adaptation to structured reasoning protocols and successful metacognitive intervention. The protocol defines two complementary mechanisms: TRACE for high-level reasoning patterns and TRACE_FE for detailed factor explanations.

Result: The protocol facilitates rapid comprehension and supports thorough verification of AI decision-making through dual-level transparency, beginning with high-level reasoning patterns and progressing to granular explanations.

Conclusion: SynLang, combined with symbiotic epistemology, enables AI systems that enhance human intelligence, preserve human agency, and uphold ethical accountability in collaborative decision-making.

Abstract: Current AI systems rely on opaque reasoning processes that hinder human
oversight and collaborative potential. Conventional explainable AI approaches
offer post-hoc justifications and often fail to establish genuine symbiotic
collaboration. In this paper, the Symbiotic Epistemology is presented as a
philosophical foundation for human-AI cognitive partnerships. Unlike frameworks
that treat AI as a mere tool or replacement, symbiotic epistemology positions
AI as a reasoning partner, fostering calibrated trust by aligning human
confidence with AI reliability through explicit reasoning patterns and
confidence assessments. SynLang (Symbiotic Syntactic Language) is introduced as
a formal protocol for transparent human-AI collaboration. The framework is
empirically validated through actual human-AI dialogues demonstrating AI's
adaptation to structured reasoning protocols and successful metacognitive
intervention. The protocol defines two complementary mechanisms: TRACE for
high-level reasoning patterns and TRACE_FE for detailed factor explanations. It
also integrates confidence quantification, declarative control over AI
behavior, and context inheritance for multi-agent coordination. By structuring
communication and embedding confidence-calibrated transparency, SynLang,
together with symbiotic epistemology, enables AI systems that enhance human
intelligence, preserve human agency, and uphold ethical accountability in
collaborative decision-making. Through dual-level transparency, beginning with
high-level reasoning patterns and progressing to granular explanations, the
protocol facilitates rapid comprehension and supports thorough verification of
AI decision-making.

</details>


### [19] [Artificial intelligence for sustainable wine industry: AI-driven management in viticulture, wine production and enotourism](https://arxiv.org/abs/2507.21098)
*Marta Sidorkiewicz,Karolina Królikowska,Berenika Dyczek,Edyta Pijet-Migon,Anna Dubel*

Main category: cs.AI

TL;DR: AI enhances sustainability and efficiency in the wine industry through intelligent management in viticulture, wine production, and enotourism.


<details>
  <summary>Details</summary>
Motivation: The wine industry faces environmental and economic challenges, and AI offers solutions to optimize resource use, reduce environmental impact, and improve customer engagement.

Method: Questionnaire survey among Polish winemakers, combined with analysis of AI methods applicable to viticulture, production, and tourism.

Result: AI enhances vineyard monitoring, optimizes irrigation, and streamlines production processes, contributing to sustainable resource management. In enotourism, AI-powered chatbots, recommendation systems, and virtual tastings personalize consumer experiences. AI impacts economic, environmental, and social sustainability.

Conclusion: AI enhances vineyard monitoring, optimizes irrigation, streamlines production, and personalizes consumer experiences, contributing to sustainable resource management and supporting local wine enterprises and cultural heritage.

Abstract: This study examines the role of Artificial Intelligence (AI) in enhancing
sustainability and efficiency within the wine industry. It focuses on AI-driven
intelligent management in viticulture, wine production, and enotourism. As the
wine industry faces environmental and economic challenges, AI offers innovative
solutions to optimize resource use, reduce environmental impact, and improve
customer engagement. Understanding AI's potential in sustainable winemaking is
crucial for fostering responsible and efficient industry practices. The
research is based on a questionnaire survey conducted among Polish winemakers,
combined with a comprehensive analysis of AI methods applicable to viticulture,
production, and tourism. Key AI technologies, including predictive analytics,
machine learning, and computer vision, are explored. The findings indicate that
AI enhances vineyard monitoring, optimizes irrigation, and streamlines
production processes, contributing to sustainable resource management. In
enotourism, AI-powered chatbots, recommendation systems, and virtual tastings
personalize consumer experiences. The study highlights AI's impact on economic,
environmental, and social sustainability, supporting local wine enterprises and
cultural heritage. Keywords: Artificial Intelligence, Sustainable Development,
AI-Driven Management, Viticulture, Wine Production, Enotourism, Wine
Enterprises, Local Communities

</details>


### [20] [Leveraging Generative AI to Enhance Synthea Module Development](https://arxiv.org/abs/2507.21123)
*Mark A. Kramer,Aanchal Mathur,Caroline E. Adams,Jason A. Walonoski*

Main category: cs.AI

TL;DR: This paper explores the use of large language models (LLMs) to assist in the development of new disease modules for Synthea, an open-source synthetic health data generator.


<details>
  <summary>Details</summary>
Motivation: Incorporating LLMs into the module development process has the potential to reduce development time, reduce required expertise, expand model diversity, and improve the overall quality of synthetic patient data.

Method: demonstrate four ways that LLMs can support Synthea module creation: generating a disease profile, generating a disease module from a disease profile, evaluating an existing Synthea module, and refining an existing module. We introduce the concept of progressive refinement, which involves iteratively evaluating the LLM-generated module by checking its syntactic correctness and clinical accuracy, and then using that information to modify the module.

Result: We demonstrate four ways that LLMs can support Synthea module creation

Conclusion: The paper concludes with recommendations for future research and development to fully realize the potential of LLM-aided synthetic data creation.

Abstract: This paper explores the use of large language models (LLMs) to assist in the
development of new disease modules for Synthea, an open-source synthetic health
data generator. Incorporating LLMs into the module development process has the
potential to reduce development time, reduce required expertise, expand model
diversity, and improve the overall quality of synthetic patient data. We
demonstrate four ways that LLMs can support Synthea module creation: generating
a disease profile, generating a disease module from a disease profile,
evaluating an existing Synthea module, and refining an existing module. We
introduce the concept of progressive refinement, which involves iteratively
evaluating the LLM-generated module by checking its syntactic correctness and
clinical accuracy, and then using that information to modify the module. While
the use of LLMs in this context shows promise, we also acknowledge the
challenges and limitations, such as the need for human oversight, the
importance of rigorous testing and validation, and the potential for
inaccuracies in LLM-generated content. The paper concludes with recommendations
for future research and development to fully realize the potential of LLM-aided
synthetic data creation.

</details>


### [21] [Measuring and Analyzing Intelligence via Contextual Uncertainty in Large Language Models using Information-Theoretic Metrics](https://arxiv.org/abs/2507.21129)
*Jae Wan Shim*

Main category: cs.AI

TL;DR: This paper introduces a new method to characterize how LLMs process information, moving beyond task-specific benchmarks.


<details>
  <summary>Details</summary>
Motivation: The internal mechanisms of LLMs are the subject of intense scientific inquiry.

Method: A novel, task-agnostic approach to probe these dynamics by creating a quantitative Cognitive Profile centered on the Entropy Decay Curve.

Result: Unique and consistent cognitive profiles that are sensitive to both model scale and text complexity.

Conclusion: This work provides a new lens for analyzing the intrinsic operational dynamics of AI.

Abstract: The remarkable capabilities of Large Language Models (LLMs) are now
extensively documented on task-specific benchmarks, yet the internal mechanisms
that produce these results are the subject of intense scientific inquiry. This
paper contributes to this inquiry by moving beyond metrics that measure
\textit{what} models can do, to a methodology that characterizes \textit{how}
they process information. We introduce a novel, task-agnostic approach to probe
these dynamics by creating a quantitative ``Cognitive Profile" for any given
model. This profile is centered on the \textbf{Entropy Decay Curve}, a
visualization that traces how a model's normalized predictive uncertainty
changes as a function of context length. Applying this methodology to several
state-of-the-art LLMs across diverse texts, we uncover unique and consistent
cognitive profiles that are sensitive to both model scale and text complexity.
We also introduce the Information Gain Span (IGS) index to summarize the
desirability of the decay trajectory. This work thus provides a new, principled
lens for analyzing and comparing the intrinsic operational dynamics of
artificial intelligence.

</details>


### [22] [INTEGRALBENCH: Benchmarking LLMs with Definite Integral Problems](https://arxiv.org/abs/2507.21130)
*Bintao Tang,Xin Yang,Yuhao Wang,Zixuan Qiu,Zimo Ji,Wenyuan Jiang*

Main category: cs.AI

TL;DR: INTEGRALBENCH: a benchmark for evaluating LLM performance on definite integral problems.


<details>
  <summary>Details</summary>
Motivation: To advance automated mathematical reasoning by providing a rigorous evaluation framework for definite integral computation.

Method: Evaluation of nine state-of-the-art LLMs on definite integral problems.

Result: Significant performance gaps and strong correlations between problem difficulty and model accuracy.

Conclusion: INTEGRALBENCH establishes baseline metrics for definite integral computation and reveals performance gaps in LLMs.

Abstract: We present INTEGRALBENCH, a focused benchmark designed to evaluate Large
Language Model (LLM) performance on definite integral problems. INTEGRALBENCH
provides both symbolic and numerical ground truth solutions with manual
difficulty annotations. Our evaluation of nine state-of-the-art LLMs reveals
significant performance gaps and strong correlations between problem difficulty
and model accuracy, establishing baseline metrics for this challenging domain.
INTEGRALBENCH aims to advance automated mathematical reasoning by providing a
rigorous evaluation framework specifically tailored for definite integral
computation.

</details>


### [23] [NPO: Learning Alignment and Meta-Alignment through Structured Human Feedback](https://arxiv.org/abs/2507.21131)
*Madhava Gaikwad,Ashwini Ramchandra Doke*

Main category: cs.AI

TL;DR: NPO是一种对齐感知的学习框架，它形式化了对齐损失，并证明了它在超大规模部署设置中具有可衡量的价值。


<details>
  <summary>Details</summary>
Motivation: 本文提出了NPO，一个对齐感知的学习框架，它在人机交互决策系统中实现了反馈驱动的适应。与先前将对齐视为静态或事后属性的方法不同。

Method: NPO引入了一种对齐损失的形式化，该形式化是可测量的、可监督的，并且可以在结构化反馈下减少。

Result: NPO在超大规模部署设置中展示了可衡量的价值。基于模拟的artifact和消融研究进一步说明了实际应用中的理论原理。

Conclusion: NPO提供了一个紧凑、可检查的持续对齐监控架构，有助于弥合理论对齐保证与动态环境中的实际可靠性之间的差距。

Abstract: We present NPO, an alignment-aware learning framework that operationalizes
feedback-driven adaptation in human-in-the-loop decision systems. Unlike prior
approaches that treat alignment as a static or post-hoc property, NPO
introduces a formalization of alignment loss that is measurable, supervisable,
and reducible under structured feedback. In parallel, we propose meta-alignment
as the fidelity of the monitoring process that governs retraining or override
triggers, and show that it is formally reducible to primary alignment via
threshold fidelity. Our implementation spans a scalable operational loop
involving scenario scoring, threshold tuning, policy validation, and structured
feedback ingestion, including "likes", overrides, and abstentions. We provide
formal convergence results under stochastic feedback and show that both
alignment loss and monitoring fidelity converge additively. Empirically, NPO
demonstrates measurable value in hyperscale deployment settings. A
simulation-based artifact and ablation studies further illustrate the
theoretical principles in action. Together, NPO offers a compact, inspectable
architecture for continual alignment monitoring, helping bridge theoretical
alignment guarantees with practical reliability in dynamic environments.

</details>


### [24] [Can You Trust an LLM with Your Life-Changing Decision? An Investigation into AI High-Stakes Responses](https://arxiv.org/abs/2507.21132)
*Joshua Adrian Cahyono,Saran Subramanian*

Main category: cs.AI

TL;DR: LLMs lack safeguards against misguided advice, leading to sycophancy and over-confidence. The paper explores these issues through experiments, finding that cautious models asking clarifying questions are safer and that model cautiousness can be controlled.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) are increasingly consulted for high-stakes life advice, yet they lack standard safeguards against providing confident but misguided responses. This creates risks of sycophancy and over-confidence.

Method: This paper investigates these failure modes through three experiments: (1) a multiple-choice evaluation to measure model stability against user pressure; (2) a free-response analysis using a novel safety typology and an LLM Judge; and (3) a mechanistic interpretability experiment to steer model behavior by manipulating a "high-stakes" activation vector.

Result: Our results show that while some models exhibit sycophancy, others like o4-mini remain robust.

Conclusion: Top-performing models achieve high safety scores by frequently asking clarifying questions, a key feature of a safe, inquisitive approach, rather than issuing prescriptive advice. Furthermore, we demonstrate that a model's cautiousness can be directly controlled via activation steering, suggesting a new path for safety alignment. These findings underscore the need for nuanced, multi-faceted benchmarks to ensure LLMs can be trusted with life-changing decisions.

Abstract: Large Language Models (LLMs) are increasingly consulted for high-stakes life
advice, yet they lack standard safeguards against providing confident but
misguided responses. This creates risks of sycophancy and over-confidence. This
paper investigates these failure modes through three experiments: (1) a
multiple-choice evaluation to measure model stability against user pressure;
(2) a free-response analysis using a novel safety typology and an LLM Judge;
and (3) a mechanistic interpretability experiment to steer model behavior by
manipulating a "high-stakes" activation vector. Our results show that while
some models exhibit sycophancy, others like o4-mini remain robust.
Top-performing models achieve high safety scores by frequently asking
clarifying questions, a key feature of a safe, inquisitive approach, rather
than issuing prescriptive advice. Furthermore, we demonstrate that a model's
cautiousness can be directly controlled via activation steering, suggesting a
new path for safety alignment. These findings underscore the need for nuanced,
multi-faceted benchmarks to ensure LLMs can be trusted with life-changing
decisions.

</details>


### [25] [Project Patti: Why can You Solve Diabolical Puzzles on one Sudoku Website but not Easy Puzzles on another Sudoku Website?](https://arxiv.org/abs/2507.21137)
*Arman Eisenkolb-Vaithyanathan*

Main category: cs.AI

TL;DR: The paper proposes two new metrics to characterize Sudoku difficulty and constructs a universal rating system, enabling consistent difficulty mapping across Sudoku websites.


<details>
  <summary>Details</summary>
Motivation: The paper aims to answer the question of what constitutes Sudoku difficulty rating across different Sudoku websites.

Method: Two distinct methods are used: converting a Sudoku puzzle into a SAT problem and simulating human Sudoku solvers using a backtracking algorithm called Nishio.

Result: Two new metrics are proposed to characterize Sudoku difficulty. Strong correlations are found between the proposed metrics and website-labeled difficulty levels for 4 out of 5 websites.

Conclusion: A universal Sudoku rating system is constructed using unsupervised classification based on two proposed metrics, aligning well with website-labeled difficulty levels for 4 out of 5 websites. An algorithm for early Sudoku practitioners is presented.

Abstract: In this paper we try to answer the question "What constitutes Sudoku
difficulty rating across different Sudoku websites?" Using two distinct methods
that can both solve every Sudoku puzzle, I propose two new metrics to
characterize Sudoku difficulty. The first method is based on converting a
Sudoku puzzle into its corresponding Satisfiability (SAT) problem. The first
proposed metric is derived from SAT Clause Length Distribution which captures
the structural complexity of a Sudoku puzzle including the number of given
digits and the cells they are in. The second method simulates human Sudoku
solvers by intertwining four popular Sudoku strategies within a backtracking
algorithm called Nishio. The second metric is computed by counting the number
of times Sudoku strategies are applied within the backtracking iterations of a
randomized Nishio. Using these two metrics, I analyze more than a thousand
Sudoku puzzles across five popular websites to characterize every difficulty
level in each website. I evaluate the relationship between the proposed metrics
and website-labeled difficulty levels using Spearman's rank correlation
coefficient, finding strong correlations for 4 out of 5 websites. I construct a
universal rating system using a simple, unsupervised classifier based on the
two proposed metrics. This rating system is capable of classifying both
individual puzzles and entire difficulty levels from the different Sudoku
websites into three categories - Universal Easy, Universal Medium, and
Universal Hard - thereby enabling consistent difficulty mapping across Sudoku
websites. The experimental results show that for 4 out of 5 Sudoku websites,
the universal classification aligns well with website-labeled difficulty
levels. Finally, I present an algorithm that can be used by early Sudoku
practitioners to solve Sudoku puzzles.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [26] [AI-Driven Generation of Data Contracts in Modern Data Engineering Systems](https://arxiv.org/abs/2507.21056)
*Harshraj Bhoite*

Main category: cs.DB

TL;DR: 使用大型语言模型 (LLM) 自动生成数据合同，从而减少手动工作量。


<details>
  <summary>Details</summary>
Motivation: 随着数据管道复杂性的增加，手动编写和维护合同容易出错且 labor-intensive。

Method: 使用大型语言模型 (LLM) 的自动数据合同生成的人工智能驱动框架，利用参数高效的微调方法（包括 LoRA 和 PEFT）来使 LLM 适应结构化数据领域。

Result: 在合成和真实世界数据集上的实验结果表明，微调的 LLM 在生成有效合同方面实现了高精度，并将手动工作量减少了 70% 以上。

Conclusion: 生成式人工智能可以通过弥合企业数据管理中意图和实施之间的差距，从而实现可扩展、敏捷的数据治理。

Abstract: Data contracts formalize agreements between data producers and consumers
regarding schema, semantics, and quality expectations. As data pipelines grow
in complexity, manual authoring and maintenance of contracts becomes
error-prone and labor-intensive. We present an AI-driven framework for
automatic data contract generation using large language models (LLMs). Our
system leverages parameter-efficient fine-tuning methods, including LoRA and
PEFT, to adapt LLMs to structured data domains. The models take sample data or
schema descriptions and output validated contract definitions in formats such
as JSON Schema and Avro. We integrate this framework into modern data platforms
(e.g., Databricks, Snowflake) to automate contract enforcement at scale.
Experimental results on synthetic and real-world datasets demonstrate that the
fine-tuned LLMs achieve high accuracy in generating valid contracts and reduce
manual workload by over 70%. We also discuss key challenges such as
hallucination, version control, and the need for continuous learning. This work
demonstrates that generative AI can enable scalable, agile data governance by
bridging the gap between intent and implementation in enterprise data
management.

</details>


### [27] [Digitalizing Uncertain Information](https://arxiv.org/abs/2507.21173)
*Chris Partridge,Andrew Mitchell,Andreas Cola*

Main category: cs.DB

TL;DR: The paper introduces an ontology-based method using a Lewisian counterpart approach to represent uncertain information in a digital form.


<details>
  <summary>Details</summary>
Motivation: The paper aims to develop an ontology-based digital form for representing uncertain information and advance digital maturity across a technology divide.

Method: The paper extends an extensional ontology (BORO Foundational Ontology or the Information Exchange Standard) with a Lewisian counterpart approach.

Result: The paper presents initial results showing how the proposed approach can handle the challenges of representing digital uncertainty.

Conclusion: The paper demonstrates the expressiveness of extending an extensional ontology with a Lewisian counterpart approach for formalizing uncertainty, showing its ability to handle challenges in representing uncertain information.

Abstract: The paper sketches some initial results from an ongoing project to develop an
ontology-based digital form for representing uncertain information. We frame
this work as a journey from lower to higher levels of digital maturity across a
technology divide. The paper first sets a baseline by describing the basic
challenges any project dealing with digital uncertainty faces. It then
describes how the project is facing them. It shows firstly how an extensional
ontology (such as the BORO Foundational Ontology or the Information Exchange
Standard) can be extended with a Lewisian counterpart approach to formalizing
uncertainty that is adapted to computing. And then it shows how this is
expressive enough to handle the challenges. Keywords: actuality, BORO
Foundational Ontology, counterpart, Information Exchange Standard,
informational uncertainty, my doxastic actualities, two-dimensional semantics.

</details>


### [28] [Ranking Methods for Skyline Queries](https://arxiv.org/abs/2507.21860)
*Mickaël Martin-Nevot,Lotfi Lakhal*

Main category: cs.DB

TL;DR: 提出了DeepSky，结合多种方法改进Skyline点排序，解决高基数数据集问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理高基数结果集时，对Pareto最优或Skyline点的相关比较不足。

Method: 改进了dp-idp方法，引入了支配层级的概念；提出了RankSky方法，采用了Google的PageRank解决方案；建立了基于TOPSIS的CoSky方法，利用Gini指数自动加权归一化属性，并计算Salton余弦。

Result: 实验评估了dp-idp、RankSky和CoSky的实现。

Conclusion: 提出了DeepSky，通过将多级Skyline与dp-idp、RankSky或CoSky相结合。

Abstract: {Multi-criteria decision analysis in databases has been actively studied,
especially through the Skyline operator. Yet, few approaches offer a relevant
comparison of Pareto optimal, or Skyline, points for high cardinality result
sets. We propose to improve the dp-idp method, inspired by tf-idf, a recent
approach computing a score for each Skyline point, by introducing the concept
of dominance hierarchy. As dp-idp lacks efficiency and does not ensure a
distinctive rank, we introduce the RankSky method, the adaptation of Google's
well-known PageRank solution, using a square stochastic matrix, a teleportation
matrix, a damping factor, and then a row score eigenvector and the IPL
algorithm. For the same reasons as RankSky, and also to offer directly
embeddable in DBMS solution, we establish the TOPSIS based CoSky method,
derived from both information research and multi-criteria analysis. CoSky
automatically ponderates normalized attributes using the Gini index, then
computes a score using Salton's cosine toward an ideal point. By coupling
multilevel Skyline to dp-idp, RankSky or CoSky, we introduce DeepSky.
Implementations of dp-idp, RankSky and CoSky are evaluated experimentally.

</details>


### [29] [Benchmarking Filtered Approximate Nearest Neighbor Search Algorithms on Transformer-based Embedding Vectors](https://arxiv.org/abs/2507.21989)
*Patrick Iff,Paul Bruegger,Marcin Chrapek,Maciej Besta,Torsten Hoefler*

Main category: cs.DB

TL;DR: introduce a novel dataset consisting of embedding vectors for the abstracts of over 2.7 million research articles from the arXiv repository, accompanied by 11 real-world attributes such as authors and categories, and benchmark a wide range of FANNS methods on it


<details>
  <summary>Details</summary>
Motivation: lack of diverse and realistic datasets, particularly ones derived from the latest transformer-based text embedding models

Method: a comprehensive survey and taxonomy of FANNS methods and analyze how they are benchmarked in the literature

Result: each method has distinct strengths and limitations; no single approach performs best across all scenarios

Conclusion: no universally best method exists for Filtered Approximate Nearest Neighbor Search (FANNS)

Abstract: Advances in embedding models for text, image, audio, and video drive progress
across multiple domains, including retrieval-augmented generation,
recommendation systems, vehicle/person reidentification, and face recognition.
Many applications in these domains require an efficient method to retrieve
items that are close to a given query in the embedding space while satisfying a
filter condition based on the item's attributes, a problem known as Filtered
Approximate Nearest Neighbor Search (FANNS). In this work, we present a
comprehensive survey and taxonomy of FANNS methods and analyze how they are
benchmarked in the literature. By doing so, we identify a key challenge in the
current FANNS landscape: the lack of diverse and realistic datasets,
particularly ones derived from the latest transformer-based text embedding
models. To address this, we introduce a novel dataset consisting of embedding
vectors for the abstracts of over 2.7 million research articles from the arXiv
repository, accompanied by 11 real-world attributes such as authors and
categories. We benchmark a wide range of FANNS methods on our novel dataset and
find that each method has distinct strengths and limitations; no single
approach performs best across all scenarios. ACORN, for example, supports
various filter types and performs reliably across dataset scales but is often
outperformed by more specialized methods. SeRF shows excellent performance for
range filtering on ordered attributes but cannot handle categorical attributes.
Filtered-DiskANN and UNG excel on the medium-scale dataset but fail on the
large-scale dataset, highlighting the challenge posed by transformer-based
embeddings, which are often more than an order of magnitude larger than earlier
embeddings. We conclude that no universally best method exists.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [30] [Analise Semantica Automatizada com LLM e RAG para Bulas Farmaceuticas](https://arxiv.org/abs/2507.21103)
*Daniel Meireles do Rego*

Main category: cs.IR

TL;DR: 本文研究了RAG与LLM结合用于自动化分析PDF文档，实验结果表明，这种结合在智能信息检索和解释非结构化文本方面有显著收益。


<details>
  <summary>Details</summary>
Motivation: 数字文档的产生在学术、商业和健康环境中迅速增长，这给非结构化信息的有效提取和分析带来了新的挑战。

Method: RAG (检索增强生成) 架构与大规模语言模型 (LLM) 相结合，以自动化分析 PDF 格式的文档。该方案集成了通过嵌入的向量搜索技术、语义数据提取和上下文自然语言响应的生成。

Result: 通过官方公共来源提取的药品包装插件进行的实验结果表明，RAG与LLM的结合在智能信息检索和非结构化技术文本的解释方面提供了显著的收益。

Conclusion: RAG与LLM的结合在智能信息检索和非结构化技术文本的解释方面提供了显著的收益。

Abstract: The production of digital documents has been growing rapidly in academic,
business, and health environments, presenting new challenges in the efficient
extraction and analysis of unstructured information. This work investigates the
use of RAG (Retrieval-Augmented Generation) architectures combined with
Large-Scale Language Models (LLMs) to automate the analysis of documents in PDF
format. The proposal integrates vector search techniques by embeddings,
semantic data extraction and generation of contextualized natural language
responses. To validate the approach, we conducted experiments with drug package
inserts extracted from official public sources. The semantic queries applied
were evaluated by metrics such as accuracy, completeness, response speed and
consistency. The results indicate that the combination of RAG with LLMs offers
significant gains in intelligent information retrieval and interpretation of
unstructured technical texts.

</details>


### [31] [AgentMaster: A Multi-Agent Conversational Framework Using A2A and MCP Protocols for Multimodal Information Retrieval and Analysis](https://arxiv.org/abs/2507.21105)
*Callie C. Liao,Duoduo Liao,Sai Surya Gadiraju*

Main category: cs.IR

TL;DR: AgentMaster, a new MAS framework using A2A and MCP protocols, improves inter-agent communication and coordination, leading to better conversational AI performance.


<details>
  <summary>Details</summary>
Motivation: Current MAS systems face challenges in inter-agent communication, coordination, and interaction with heterogeneous tools and resources. Existing protocols like MCP and A2A are rarely used together.

Method: The paper introduces AgentMaster, a modular multi-protocol MAS framework with self-implemented A2A and MCP.

Result: Evaluation using BERTScore F1 and G-Eval metrics averaged 96.3% and 87.1%, demonstrating robust inter-agent coordination, query decomposition, dynamic routing, and domain-specific responses.

Conclusion: The AgentMaster framework contributes to domain-specific, cooperative, and scalable conversational AI by using MAS.

Abstract: The rise of Multi-Agent Systems (MAS) in Artificial Intelligence (AI),
especially integrated with Large Language Models (LLMs), has greatly
facilitated the resolution of complex tasks. However, current systems are still
facing challenges of inter-agent communication, coordination, and interaction
with heterogeneous tools and resources. Most recently, the Model Context
Protocol (MCP) by Anthropic and Agent-to-Agent (A2A) communication protocol by
Google have been introduced, and to the best of our knowledge, very few
applications exist where both protocols are employed within a single MAS
framework. We present a pilot study of AgentMaster, a novel modular
multi-protocol MAS framework with self-implemented A2A and MCP, enabling
dynamic coordination and flexible communication. Through a unified
conversational interface, the system supports natural language interaction
without prior technical expertise and responds to multimodal queries for tasks
including information retrieval, question answering, and image analysis.
Evaluation through the BERTScore F1 and LLM-as-a-Judge metric G-Eval averaged
96.3\% and 87.1\%, revealing robust inter-agent coordination, query
decomposition, dynamic routing, and domain-specific, relevant responses.
Overall, our proposed framework contributes to the potential capabilities of
domain-specific, cooperative, and scalable conversational AI powered by MAS.

</details>


### [32] [Page image classification for content-specific data processing](https://arxiv.org/abs/2507.21114)
*Kateryna Lutsai,Pavel Straňák*

Main category: cs.IR

TL;DR: This paper develops an image classification system for historical documents to categorize pages based on their content, enabling tailored downstream analysis pipelines.


<details>
  <summary>Details</summary>
Motivation: digitization projects in humanities generate vast quantities of page images from historical documents, presenting significant challenges for manual sorting and analysis

Method: leveraging advancements in artificial intelligence and machine learning

Result: the set of categories was chosen to facilitate content-specific processing workflows, separating pages requiring different analysis techniques

Conclusion: an image classification system is developed for historical document pages

Abstract: Digitization projects in humanities often generate vast quantities of page
images from historical documents, presenting significant challenges for manual
sorting and analysis. These archives contain diverse content, including various
text types (handwritten, typed, printed), graphical elements (drawings, maps,
photos), and layouts (plain text, tables, forms). Efficiently processing this
heterogeneous data requires automated methods to categorize pages based on
their content, enabling tailored downstream analysis pipelines. This project
addresses this need by developing and evaluating an image classification system
specifically designed for historical document pages, leveraging advancements in
artificial intelligence and machine learning. The set of categories was chosen
to facilitate content-specific processing workflows, separating pages requiring
different analysis techniques (e.g., OCR for text, image analysis for graphics)

</details>


### [33] [FedFlex: Federated Learning for Diverse Netflix Recommendations](https://arxiv.org/abs/2507.21115)
*Sven Lankester,Manel Slokom,Gustavo de Carvalho Bertoli,Matias Vizcaino,Emmanuelle Beauxis Aussalet,Laura Hollink*

Main category: cs.IR

TL;DR: FedFlex是一种联邦推荐系统，它集成了矩阵分解算法和MMR来提高推荐的多样性，而不会降低用户满意度。


<details>
  <summary>Details</summary>
Motivation: 现有关于联邦推荐系统的大部分工作主要集中在提高准确性上，而对公平性和多样性的关注有限。

Method: FedFlex集成了两种先进的矩阵分解算法，用于个性化微调，并应用最大边缘相关性（MMR）来重新排序项目并增强多样性。

Result: 在为期两周的实际用户研究中，参与者收到了两个推荐列表：列表A，基于SVD或BPR，以及列表B，一个强调多样性的重新排序版本。研究结果表明，FedFlex有效地引入了多样化的内容。

Conclusion: FedFlex在不影响用户满意度的情况下，有效地将多样化的内容（如新类型）引入推荐。

Abstract: Federated learning is a decentralized approach that enables collaborative
model training across multiple devices while preserving data privacy. It has
shown significant potential in various domains, including healthcare and
personalized recommendation systems. However, most existing work on federated
recommendation systems has focused primarily on improving accuracy, with
limited attention to fairness and diversity. In this paper, we introduce
FedFlex, a federated recommender system for Netflix-style TV series
recommendations. FedFlex integrates two state-of-the-art matrix factorization
algorithms for personalized fine-tuning. FedFlex also applies Maximal Marginal
Relevance (MMR) to re-rank items and enhance diversity. We conduct extensive
experiments comparing recommendations generated by SVD and BPR algorithms. In a
live two-week user study, participants received two recommendation lists: List
A, based on SVD or BPR, and List B, a re-ranked version emphasizing diversity.
Participants were asked to click on the movies they were interested in
watching. Our findings demonstrate that FedFlex effectively introduces diverse
content, such as new genres, into recommendations without necessarily
compromising user satisfaction.

</details>


### [34] [A Comprehensive Review on Harnessing Large Language Models to Overcome Recommender System Challenges](https://arxiv.org/abs/2507.21117)
*Rahul Raja,Anshaj Vats,Arpita Vats,Anirban Majumder*

Main category: cs.IR

TL;DR: This paper surveys how Large Language Models (LLMs) can address key challenges in modern recommender systems, enhancing personalization, semantic understanding, and cold-start performance.


<details>
  <summary>Details</summary>
Motivation: Traditional recommender systems face challenges including sparse and noisy interaction data, cold-start problems, limited personalization depth, and inadequate semantic understanding of user and item content.

Method: Comprehensive technical survey of how LLMs can be leveraged to tackle key challenges in modern recommender systems. Examination of the use of LLMs for prompt-driven candidate retrieval, language-native ranking, retrieval-augmented generation (RAG), and conversational recommendation.

Result: LLMs enhance personalization, semantic alignment, and interpretability without requiring extensive task-specific supervision. LLMs further enable zero- and few-shot reasoning, allowing systems to operate effectively in cold-start and long-tail scenarios.

Conclusion: LLMs are foundational enablers for building more adaptive, semantically rich, and user-centric recommender systems.

Abstract: Recommender systems have traditionally followed modular architectures
comprising candidate generation, multi-stage ranking, and re-ranking, each
trained separately with supervised objectives and hand-engineered features.
While effective in many domains, such systems face persistent challenges
including sparse and noisy interaction data, cold-start problems, limited
personalization depth, and inadequate semantic understanding of user and item
content. The recent emergence of Large Language Models (LLMs) offers a new
paradigm for addressing these limitations through unified, language-native
mechanisms that can generalize across tasks, domains, and modalities. In this
paper, we present a comprehensive technical survey of how LLMs can be leveraged
to tackle key challenges in modern recommender systems. We examine the use of
LLMs for prompt-driven candidate retrieval, language-native ranking,
retrieval-augmented generation (RAG), and conversational recommendation,
illustrating how these approaches enhance personalization, semantic alignment,
and interpretability without requiring extensive task-specific supervision.
LLMs further enable zero- and few-shot reasoning, allowing systems to operate
effectively in cold-start and long-tail scenarios by leveraging external
knowledge and contextual cues. We categorize these emerging LLM-driven
architectures and analyze their effectiveness in mitigating core bottlenecks of
conventional pipelines. In doing so, we provide a structured framework for
understanding the design space of LLM-enhanced recommenders, and outline the
trade-offs between accuracy, scalability, and real-time performance. Our goal
is to demonstrate that LLMs are not merely auxiliary components but
foundational enablers for building more adaptive, semantically rich, and
user-centric recommender systems

</details>


### [35] [Affect-aware Cross-Domain Recommendation for Art Therapy via Music Preference Elicitation](https://arxiv.org/abs/2507.21120)
*Bereket A. Yilma,Luis A. Leiva*

Main category: cs.IR

TL;DR: 提出音乐驱动的艺术疗法跨域推荐方法，实验表明其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉艺术推荐系统依赖视觉刺激进行用户建模，限制了它们在偏好启发期间捕捉完整情感反应的能力。之前的研究表明，音乐刺激可以引发独特的情感反射，为跨域推荐 (CDR) 增强 AT 个性化提供了机会。

Method: 提出了一系列基于音乐驱动偏好启发的艺术疗法跨域推荐方法。

Result: 一项有 200 名用户参与的大规模研究表明，音乐驱动的偏好启发方法的有效性。

Conclusion: 音乐驱动的偏好启发方法优于经典视觉启发方法。

Abstract: Art Therapy (AT) is an established practice that facilitates emotional
processing and recovery through creative expression. Recently, Visual Art
Recommender Systems (VA RecSys) have emerged to support AT, demonstrating their
potential by personalizing therapeutic artwork recommendations. Nonetheless,
current VA RecSys rely on visual stimuli for user modeling, limiting their
ability to capture the full spectrum of emotional responses during preference
elicitation. Previous studies have shown that music stimuli elicit unique
affective reflections, presenting an opportunity for cross-domain
recommendation (CDR) to enhance personalization in AT. Since CDR has not yet
been explored in this context, we propose a family of CDR methods for AT based
on music-driven preference elicitation. A large-scale study with 200 users
demonstrates the efficacy of music-driven preference elicitation, outperforming
the classic visual-only elicitation approach. Our source code, data, and models
are available at https://github.com/ArtAICare/Affect-aware-CDR

</details>


### [36] [RATE: An LLM-Powered Retrieval Augmented Generation Technology-Extraction Pipeline](https://arxiv.org/abs/2507.21125)
*Karan Mirhosseini,Arya Aftab,Alireza Sheikh*

Main category: cs.IR

TL;DR: RATE, a new LLM pipeline, significantly outperforms BERT in automated technology extraction from scientific literature, offering new insights into BCI-XR trends.


<details>
  <summary>Details</summary>
Motivation: Technology maps are crucial for enhancing decision-making in the face of radical technology transformations, and these maps rely on automated methods of technology extraction.

Method: Retrieval Augmented Technology Extraction (RATE), a Large Language Model (LLM) based pipeline combining Retrieval Augmented Generation (RAG) with multi-definition LLM-based validation.

Result: RATE achieved an F1-score of 91.27%, significantly outperforming BERT with an F1-score of 53.73%. Validated technology terms were mapped into a co-occurrence network, revealing thematic clusters and structural features of the research landscape.

Conclusion: Definition-driven LLM methods show promise for technology extraction and mapping, offering insights into emerging trends within the BCI-XR field.

Abstract: In an era of radical technology transformations, technology maps play a
crucial role in enhancing decision making. These maps heavily rely on automated
methods of technology extraction. This paper introduces Retrieval Augmented
Technology Extraction (RATE), a Large Language Model (LLM) based pipeline for
automated technology extraction from scientific literature. RATE combines
Retrieval Augmented Generation (RAG) with multi-definition LLM-based
validation. This hybrid method results in high recall in candidate generation
alongside with high precision in candidate filtering. While the pipeline is
designed to be general and widely applicable, we demonstrate its use on 678
research articles focused on Brain-Computer Interfaces (BCIs) and Extended
Reality (XR) as a case study. Consequently, The validated technology terms by
RATE were mapped into a co-occurrence network, revealing thematic clusters and
structural features of the research landscape. For the purpose of evaluation, a
gold standard dataset of technologies in 70 selected random articles had been
curated by the experts. In addition, a technology extraction model based on
Bidirectional Encoder Representations of Transformers (BERT) was used as a
comparative method. RATE achieved F1-score of 91.27%, Significantly
outperforming BERT with F1-score of 53.73%. Our findings highlight the promise
of definition-driven LLM methods for technology extraction and mapping. They
also offer new insights into emerging trends within the BCI-XR field. The
source code is available https://github.com/AryaAftab/RATE

</details>


### [37] [Efficient Data Retrieval and Comparative Bias Analysis of Recommendation Algorithms for YouTube Shorts and Long-Form Videos](https://arxiv.org/abs/2507.21467)
*Selimhan Dagtas,Mert Can Cakmak,Nitin Agarwal*

Main category: cs.IR

TL;DR: This study analyzes YouTube's recommendation algorithms for short-form and long-form videos, revealing differences in content diversity and biases in politically sensitive topics.


<details>
  <summary>Details</summary>
Motivation: The growing popularity of short-form video content has transformed user engagement on digital platforms, raising critical questions about the role of recommendation algorithms in shaping user experiences. Concerns about biases, echo chambers, and content diversity persist.

Method: This study develops an efficient data collection framework to analyze YouTube's recommendation algorithms for both short-form and long-form videos, employing parallel computing and advanced scraping techniques to overcome limitations of YouTube's API.

Result: The analysis uncovers distinct behavioral patterns in recommendation algorithms across the two formats, with short-form videos showing a more immediate shift toward engaging yet less diverse content compared to long-form videos.  a novel investigation into biases in politically sensitive topics highlights the role of these algorithms in shaping narratives and amplifying specific viewpoints.

Conclusion: This research underscores the importance of responsible AI practices in the evolving digital media landscape by providing actionable insights for designing equitable and transparent recommendation systems.

Abstract: The growing popularity of short-form video content, such as YouTube Shorts,
has transformed user engagement on digital platforms, raising critical
questions about the role of recommendation algorithms in shaping user
experiences. These algorithms significantly influence content consumption, yet
concerns about biases, echo chambers, and content diversity persist. This study
develops an efficient data collection framework to analyze YouTube's
recommendation algorithms for both short-form and long-form videos, employing
parallel computing and advanced scraping techniques to overcome limitations of
YouTube's API. The analysis uncovers distinct behavioral patterns in
recommendation algorithms across the two formats, with short-form videos
showing a more immediate shift toward engaging yet less diverse content
compared to long-form videos. Furthermore, a novel investigation into biases in
politically sensitive topics, such as the South China Sea dispute, highlights
the role of these algorithms in shaping narratives and amplifying specific
viewpoints. By providing actionable insights for designing equitable and
transparent recommendation systems, this research underscores the importance of
responsible AI practices in the evolving digital media landscape.

</details>


### [38] [Solution for Meta KDD Cup'25: A Comprehensive Three-Step Framework for Vision Question Answering](https://arxiv.org/abs/2507.21520)
*Zijian Zhang,Xiaocheng Zhang,Yang Zhou,Zhimin Lin,Peng Yan*

Main category: cs.IR

TL;DR: BlackPearl team's solution to the CRAG-MM Challenge at KDD Cup 2025 uses data augmentation, RAG, reranking, and multi-task fine-tuning to achieve high rankings in all three tasks.


<details>
  <summary>Details</summary>
Motivation: Vision Large Language Models (VLLMs) struggle with hallucinated answers, and challenges remain in visual context comprehension, multi-source retrieval, and multi-turn interactions in Multi-modal Retrieval-Augmented Generation (RAG).

Method: The solution employs data augmentation, RAG, reranking, and multi-task fine-tuning, using a single model for each task.

Result: The BlackPearl team achieved 3rd, 3rd, and 1st place in the automatic evaluation of the three tasks and won second place in Task3 after human evaluation.

Conclusion: BlackPearl team's solution achieves strong performance in the CRAG-MM Challenge, securing 3rd, 3rd, and 1st place in automatic evaluation and 2nd place in Task3 after human evaluation.

Abstract: Vision Large Language Models (VLLMs) have improved multi-modal understanding
and visual question answering (VQA), but still suffer from hallucinated
answers. Multi-modal Retrieval-Augmented Generation (RAG) helps address these
issues by incorporating external information, yet challenges remain in visual
context comprehension, multi-source retrieval, and multi-turn interactions. To
address these challenges, Meta constructed the CRAG-MM benchmark and launched
the CRAG-MM Challenge at KDD Cup 2025, which consists of three tasks. This
paper describes the solutions of all tasks in Meta KDD Cup'25 from BlackPearl
team. We use a single model for each task, with key methods including data
augmentation, RAG, reranking, and multi-task fine-tuning. Our solution achieve
automatic evaluation rankings of 3rd, 3rd, and 1st on the three tasks, and win
second place in Task3 after human evaluation.

</details>


### [39] [Enhancing Graph-based Recommendations with Majority-Voting LLM-Rerank Augmentation](https://arxiv.org/abs/2507.21563)
*Minh-Anh Nguyen,Bao Nguyen,Ha Lan N. T.,Tuan Anh Hoang,Duc-Trong Le,Dung D. Le*

Main category: cs.IR

TL;DR: 利用大型语言模型和项目文本描述来丰富交互数据，从而提高准确性并减少流行度偏差。


<details>
  <summary>Details</summary>
Motivation: 推荐系统经常受到用户-项目交互有限导致的数据稀疏性的影响，这会降低它们的性能并放大现实场景中的流行度偏差。

Method: 通过多次少量提示 LLM 对项目重新排序并通过多数投票聚合结果，生成高置信度的合成用户-项目交互，并基于集中度测量提供理论保证。将其集成到图对比学习框架中，以减轻分布偏移并缓解流行度偏差。

Result: 通过利用大型语言模型 (LLM) 和项目文本描述来丰富交互数据，该方法生成高置信度的合成用户-项目交互。

Conclusion: 该方法提高了准确性并减少了流行度偏差，优于强大的基线。

Abstract: Recommendation systems often suffer from data sparsity caused by limited
user-item interactions, which degrade their performance and amplify popularity
bias in real-world scenarios. This paper proposes a novel data augmentation
framework that leverages Large Language Models (LLMs) and item textual
descriptions to enrich interaction data. By few-shot prompting LLMs multiple
times to rerank items and aggregating the results via majority voting, we
generate high-confidence synthetic user-item interactions, supported by
theoretical guarantees based on the concentration of measure. To effectively
leverage the augmented data in the context of a graph recommendation system, we
integrate it into a graph contrastive learning framework to mitigate
distributional shift and alleviate popularity bias. Extensive experiments show
that our method improves accuracy and reduces popularity bias, outperforming
strong baselines.

</details>


### [40] [Proposing a Semantic Movie Recommendation System Enhanced by ChatGPT's NLP Results](https://arxiv.org/abs/2507.21770)
*Ali Fallahi,Azam Bastanfard,Amineh Amini,Hadi Saboohi*

Main category: cs.IR

TL;DR: This study provides a new method for building a knowledge graph based on semantic information to enhance accuracy of movie recommender system. It uses the ChatGPT to assess the brief descriptions of movies and extract their tone of voice


<details>
  <summary>Details</summary>
Motivation: Providing highly individualized suggestions can boost user engagement and satisfaction, which is one of the fundamental goals of the movie industry, significantly in online platforms. According to recent studies and research, using knowledge-based techniques and considering the semantic ideas of the textual data is a suitable way to get more appropriate results

Method: a new method for building a knowledge graph based on semantic information. It uses the ChatGPT, as a large language model, to assess the brief descriptions of movies and extract their tone of voice

Result: using the proposed method may significantly enhance accuracy

Conclusion: using the proposed method may significantly enhance accuracy rather than employing the explicit genres supplied by the publishers

Abstract: The importance of recommender systems on the web has grown, especially in the
movie industry, with a vast selection of options to watch. To assist users in
traversing available items and finding relevant results, recommender systems
analyze operational data and investigate users' tastes and habits. Providing
highly individualized suggestions can boost user engagement and satisfaction,
which is one of the fundamental goals of the movie industry, significantly in
online platforms. According to recent studies and research, using
knowledge-based techniques and considering the semantic ideas of the textual
data is a suitable way to get more appropriate results. This study provides a
new method for building a knowledge graph based on semantic information. It
uses the ChatGPT, as a large language model, to assess the brief descriptions
of movies and extract their tone of voice. Results indicated that using the
proposed method may significantly enhance accuracy rather than employing the
explicit genres supplied by the publishers.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [41] [Task-Focused Consolidation with Spaced Recall: Making Neural Networks learn like college students](https://arxiv.org/abs/2507.21109)
*Prital Bamnodkar*

Main category: cs.LG

TL;DR: TFC-SR通过主动记忆检索机制，显著提高了持续学习的性能，尤其在内存受限的环境中。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络常常遭受灾难性遗忘的限制，即在学习新任务后，过去任务的性能会下降。

Method: TFC-SR通过一种名为主动召回探针的机制增强了标准经验回放。

Result: 在Split CIFAR-100上，TFC-SR的最终准确率为13.17%，而标准回放的准确率为7.40%。

Conclusion: TFC-SR是一种鲁棒且高效的方法，突出了将主动记忆检索机制整合到持续学习系统中的重要性。

Abstract: Deep Neural Networks often suffer from a critical limitation known as
Catastrophic Forgetting, where performance on past tasks degrades after
learning new ones. This paper introduces a novel continual learning approach
inspired by human learning strategies like Active Recall, Deliberate Practice
and Spaced Repetition, named Task Focused Consolidation with Spaced Recall
(TFC-SR). TFC-SR enhances the standard experience replay with a mechanism we
termed the Active Recall Probe. It is a periodic, task-aware evaluation of the
model's memory that stabilizes the representations of past knowledge. We test
TFC-SR on the Split MNIST and Split CIFAR-100 benchmarks against leading
regularization-based and replay-based baselines. Our results show that TFC-SR
performs significantly better than these methods. For instance, on the Split
CIFAR-100, it achieves a final accuracy of 13.17% compared to standard replay's
7.40%. We demonstrate that this advantage comes from the stabilizing effect of
the probe itself, and not from the difference in replay volume. Additionally,
we analyze the trade-off between memory size and performance and show that
while TFC-SR performs better in memory-constrained environments, higher replay
volume is still more effective when available memory is abundant. We conclude
that TFC-SR is a robust and efficient approach, highlighting the importance of
integrating active memory retrieval mechanisms into continual learning systems.

</details>


### [42] [Pre-, In-, and Post-Processing Class Imbalance Mitigation Techniques for Failure Detection in Optical Networks](https://arxiv.org/abs/2507.21119)
*Yousuf Moiz Ali,Jaroslaw E. Prilepsky,Nicola Sambo,João Pedro,Mohammad M. Hosseini,Antonio Napoli,Sergei K. Turitsyn,Pedro Freire*

Main category: cs.LG

TL;DR: Compares techniques for class imbalance mitigation in optical network failure detection, finding a performance-complexity trade-off between Threshold Adjustment and Random Under-sampling.


<details>
  <summary>Details</summary>
Motivation: class imbalance mitigation in optical network failure detection

Method: pre-, in-, and post-processing techniques

Result: Threshold Adjustment achieves the highest F1 gain (15.3%), while Random Under-sampling (RUS) offers the fastest inference

Conclusion: Threshold Adjustment achieves the highest F1 gain, while Random Under-sampling (RUS) offers the fastest inference.

Abstract: We compare pre-, in-, and post-processing techniques for class imbalance
mitigation in optical network failure detection. Threshold Adjustment achieves
the highest F1 gain (15.3%), while Random Under-sampling (RUS) offers the
fastest inference, highlighting a key performance-complexity trade-off.

</details>


### [43] [Quantum Geometry of Data](https://arxiv.org/abs/2507.21135)
*Alexander G. Abanov,Luca Candelori,Harold C. Steinacker,Martin T. Wells,Jerome R. Busemeyer,Cameron J. Hogan,Vahagn Kirakosyan,Nicola Marzari,Sunil Pinnamaneni,Dario Villani,Mengjia Xu,Kharen Musaelian*

Main category: cs.LG

TL;DR: QCML encodes data as quantum geometry, representing features as Hermitian matrices and mapping data points to states in Hilbert space, capturing global properties and avoiding the curse of dimensionality. 


<details>
  <summary>Details</summary>
Motivation: Demonstrates how Quantum Cognition Machine Learning (QCML) encodes data as quantum geometry. QCML captures global properties of data, while avoiding the curse of dimensionality inherent in local methods.

Method: Features of the data are represented by learned Hermitian matrices, and data points are mapped to states in Hilbert space.

Result: The quantum geometry description endows the dataset with rich geometric and topological structure - including intrinsic dimension, quantum metric, and Berry curvature - derived directly from the data. We illustrate this on a number of synthetic and real-world examples.

Conclusion: Quantum geometric representation of QCML could advance our understanding of cognitive phenomena within the framework of quantum cognition.

Abstract: We demonstrate how Quantum Cognition Machine Learning (QCML) encodes data as
quantum geometry. In QCML, features of the data are represented by learned
Hermitian matrices, and data points are mapped to states in Hilbert space. The
quantum geometry description endows the dataset with rich geometric and
topological structure - including intrinsic dimension, quantum metric, and
Berry curvature - derived directly from the data. QCML captures global
properties of data, while avoiding the curse of dimensionality inherent in
local methods. We illustrate this on a number of synthetic and real-world
examples. Quantum geometric representation of QCML could advance our
understanding of cognitive phenomena within the framework of quantum cognition.

</details>


### [44] [A Study on Variants of Conventional, Fuzzy, and Nullspace-Based Independence Criteria for Improving Supervised and Unsupervised Learning](https://arxiv.org/abs/2507.21136)
*Mojtaba Moattari*

Main category: cs.LG

TL;DR: 我们提出了 3 个独立性标准，并使用它们来设计无监督和有监督的降维方法。


<details>
  <summary>Details</summary>
Motivation: 传统上，无监督和有监督的学习方法使用核来捕获数据结构中固有的非线性。 然而，专家必须确保他们提出的非线性最大化可变性并捕获数据的固有多样性。

Method: 设计了 3 个独立性标准，并使用它们来设计无监督和有监督的降维方法。

Result: 在线性和神经非线性设置中评估了这些方法的对比度、准确性和可解释性。结果表明，这些方法优于基线（tSNE、PCA、正则化 LDA、具有（非）监督学习器和层共享的 VAE）。

Conclusion: 该论文提出的方法优于基线方法，并为研究人员开辟了可解释机器学习 (ML) 的新途径。

Abstract: Unsupervised and supervised learning methods conventionally use kernels to
capture nonlinearities inherent in data structure. However experts have to
ensure their proposed nonlinearity maximizes variability and capture inherent
diversity of data. We reviewed all independence criteria to design unsupervised
learners. Then we proposed 3 independence criteria and used them to design
unsupervised and supervised dimensionality reduction methods. We evaluated
contrast, accuracy and interpretability of these methods in both linear and
neural nonlinear settings. The results show that the methods have outperformed
the baseline (tSNE, PCA, regularized LDA, VAE with (un)supervised learner and
layer sharing) and opened a new line of interpretable machine learning (ML) for
the researchers.

</details>


### [45] [Advancing Wildfire Risk Prediction via Morphology-Aware Curriculum Contrastive Learning](https://arxiv.org/abs/2507.21147)
*Fabrizio Lo Scudo,Alessio De Rango,Luca Furnari,Alfonso Senatore,Donato D'Ambrosio,Giuseppe Mendicino,Gianluigi Greco*

Main category: cs.LG

TL;DR: 本文提出了一种新的对比学习方法，用于解决野火预测中的数据不平衡和高计算成本问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 野火对自然生态系统和人类健康有显著影响，气候变化加剧了这些影响。现有的野火事件数据不平衡，且高维时空数据给深度学习训练带来挑战。为了能够更频繁地使用最新的天气预报更新模型，需要降低计算成本。

Method: 采用对比学习框架，通过增强潜在表征来处理时空数据的复杂性，并使用基于形态的课程学习来缓解区域特征差异带来的问题。

Result: 通过实验分析验证了所提出的建模策略的有效性。

Conclusion: 这篇论文提出了一种新的基于形态的课程对比学习方法，以解决野火预测中数据不平衡和计算成本高的问题。

Abstract: Wildfires significantly impact natural ecosystems and human health, leading
to biodiversity loss, increased hydrogeological risks, and elevated emissions
of toxic substances. Climate change exacerbates these effects, particularly in
regions with rising temperatures and prolonged dry periods, such as the
Mediterranean. This requires the development of advanced risk management
strategies that utilize state-of-the-art technologies. However, in this
context, the data show a bias toward an imbalanced setting, where the incidence
of wildfire events is significantly lower than typical situations. This
imbalance, coupled with the inherent complexity of high-dimensional
spatio-temporal data, poses significant challenges for training deep learning
architectures. Moreover, since precise wildfire predictions depend mainly on
weather data, finding a way to reduce computational costs to enable more
frequent updates using the latest weather forecasts would be beneficial. This
paper investigates how adopting a contrastive framework can address these
challenges through enhanced latent representations for the patch's dynamic
features. We thus introduce a new morphology-based curriculum contrastive
learning that mitigates issues associated with diverse regional characteristics
and enables the use of smaller patch sizes without compromising performance. An
experimental analysis is performed to validate the effectiveness of the
proposed modeling strategies.

</details>


### [46] [Deep Unfolding for MIMO Signal Detection](https://arxiv.org/abs/2507.21152)
*Hangli Ge,Noboru Koshizuka*

Main category: cs.LG

TL;DR: a deep unfolding neural network-based MIMO detector


<details>
  <summary>Details</summary>
Motivation: prior approaches that rely on real-valued approximations

Method: a deep unfolding neural network-based MIMO detector that incorporates complex-valued computations using Wirtinger calculus. The method, referred as Dynamic Partially Shrinkage Thresholding (DPST)

Result: achieves superior detection performance with fewer iterations and lower computational complexity

Conclusion: The proposed method achieves superior detection performance with fewer iterations and lower computational complexity, making it a practical solution for next-generation massive MIMO systems.

Abstract: In this paper, we propose a deep unfolding neural network-based MIMO detector
that incorporates complex-valued computations using Wirtinger calculus. The
method, referred as Dynamic Partially Shrinkage Thresholding (DPST), enables
efficient, interpretable, and low-complexity MIMO signal detection. Unlike
prior approaches that rely on real-valued approximations, our method operates
natively in the complex domain, aligning with the fundamental nature of signal
processing tasks. The proposed algorithm requires only a small number of
trainable parameters, allowing for simplified training. Numerical results
demonstrate that the proposed method achieves superior detection performance
with fewer iterations and lower computational complexity, making it a practical
solution for next-generation massive MIMO systems.

</details>


### [47] [Deep Reinforcement Learning for Real-Time Green Energy Integration in Data Centers](https://arxiv.org/abs/2507.21153)
*Abderaouf Bahi,Amel Ourici*

Main category: cs.LG

TL;DR: 本文提出了一种DRL优化的能源管理系统，该系统在降低能源成本、提高能源效率和减少碳排放方面优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 旨在提高能源效率、成本效益和环境可持续性。

Method: 利用DRL算法动态管理可再生能源、储能和电网电力的集成。

Result: DRL优化系统在能源成本方面降低了38%，能源效率提高了82%，碳排放减少了45%。

Conclusion: DRL在优化能源管理策略和应对可持续性挑战方面具有潜力。

Abstract: This paper explores the implementation of a Deep Reinforcement Learning
(DRL)-optimized energy management system for e-commerce data centers, aimed at
enhancing energy efficiency, cost-effectiveness, and environmental
sustainability. The proposed system leverages DRL algorithms to dynamically
manage the integration of renewable energy sources, energy storage, and grid
power, adapting to fluctuating energy availability in real time. The study
demonstrates that the DRL-optimized system achieves a 38\% reduction in energy
costs, significantly outperforming traditional Reinforcement Learning (RL)
methods (28\%) and heuristic approaches (22\%). Additionally, it maintains a
low SLA violation rate of 1.5\%, compared to 3.0\% for RL and 4.8\% for
heuristic methods. The DRL-optimized approach also results in an 82\%
improvement in energy efficiency, surpassing other methods, and a 45\%
reduction in carbon emissions, making it the most environmentally friendly
solution. The system's cumulative reward of 950 reflects its superior
performance in balancing multiple objectives. Through rigorous testing and
ablation studies, the paper validates the effectiveness of the DRL model's
architecture and parameters, offering a robust solution for energy management
in data centers. The findings highlight the potential of DRL in advancing
energy optimization strategies and addressing sustainability challenges.

</details>
