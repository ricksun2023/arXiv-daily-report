{"id": "2508.01136", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.01136", "abs": "https://arxiv.org/abs/2508.01136", "authors": ["Wei Zhou", "Peng Sun", "Xuanhe Zhou", "Qianglei Zang", "Ji Xu", "Tieying Zhang", "Guoliang Li", "Fan Wu"], "title": "DBAIOps: A Reasoning LLM-Enhanced Database Operation and Maintenance System using Knowledge Graphs", "comment": "DBAIOps supports 25 database systems and has been deployed in 20\n  real-world scenarios, covering domains like finance, energy, and healthcare.\n  See website at: https://www.dbaiops.com; See code at:\n  https://github.com/weAIDB/DBAIOps/", "summary": "The operation and maintenance (O&M) of database systems is critical to\nensuring system availability and performance, typically requiring expert\nexperience (e.g., identifying metric-to-anomaly relations) for effective\ndiagnosis and recovery. However, existing automatic database O&M methods,\nincluding commercial products, cannot effectively utilize expert experience. On\nthe one hand, rule-based methods only support basic O&M tasks (e.g.,\nmetric-based anomaly detection), which are mostly numerical equations and\ncannot effectively incorporate literal O&M experience (e.g., troubleshooting\nguidance in manuals). On the other hand, LLM-based methods, which retrieve\nfragmented information (e.g., standard documents + RAG), often generate\ninaccurate or generic results. To address these limitations, we present\nDBAIOps, a novel hybrid database O&M system that combines reasoning LLMs with\nknowledge graphs to achieve DBA-style diagnosis. First, DBAIOps introduces a\nheterogeneous graph model for representing the diagnosis experience, and\nproposes a semi-automatic graph construction algorithm to build that graph from\nthousands of documents. Second, DBAIOps develops a collection of (800+)\nreusable anomaly models that identify both directly alerted metrics and\nimplicitly correlated experience and metrics. Third, for each anomaly, DBAIOps\nproposes a two-stage graph evolution mechanism to explore relevant diagnosis\npaths and identify missing relations automatically. It then leverages a\nreasoning LLM (e.g., DeepSeek-R1) to infer root causes and generate clear\ndiagnosis reports for both DBAs and common users. Our evaluation over four\nmainstream database systems (Oracle, MySQL, PostgreSQL, and DM8) demonstrates\nthat DBAIOps outperforms state-of-the-art baselines, 34.85% and 47.22% higher\nin root cause and human evaluation accuracy, respectively.", "AI": {"tldr": "DBAIOps is a new database O&M system that combines LLMs and knowledge graphs to improve diagnosis accuracy by leveraging expert experience more effectively.", "motivation": "Existing automatic database O&M methods cannot effectively utilize expert experience, with rule-based methods limited to basic tasks and LLM-based methods generating inaccurate or generic results.", "method": "A novel hybrid database O&M system combining reasoning LLMs with knowledge graphs, featuring a heterogeneous graph model for diagnosis experience, reusable anomaly models, and a two-stage graph evolution mechanism.", "result": "DBAIOps demonstrates superior performance in root cause analysis and human evaluation accuracy compared to state-of-the-art baselines.", "conclusion": "DBAIOps outperforms state-of-the-art baselines, achieving 34.85% and 47.22% higher accuracy in root cause and human evaluation, respectively, across four mainstream database systems (Oracle, MySQL, PostgreSQL, and DM8)."}}
{"id": "2508.01405", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2508.01405", "abs": "https://arxiv.org/abs/2508.01405", "authors": ["Mengzhao Wang", "Boyu Tan", "Yunjun Gao", "Hai Jin", "Yingfeng Zhang", "Xiangyu Ke", "Xiaoliang Xu", "Yifan Zhu"], "title": "Balancing the Blend: An Experimental Analysis of Trade-offs in Hybrid Search", "comment": null, "summary": "Hybrid search, the integration of lexical and semantic retrieval, has become\na cornerstone of modern information retrieval systems, driven by demanding\napplications like Retrieval-Augmented Generation (RAG). The architectural\ndesign space for these systems is vast and complex, yet a systematic, empirical\nunderstanding of the trade-offs among their core components--retrieval\nparadigms, combination schemes, and re-ranking methods--is critically lacking.\nTo address this, and informed by our experience building the Infinity\nopen-source database, we present the first systematic benchmark of advanced\nhybrid search architectures. Our framework evaluates four retrieval\nparadigms--Full-Text Search (FTS), Sparse Vector Search (SVS), Dense Vector\nSearch (DVS), and Tensor Search (TenS)--benchmarking their combinations and\nre-ranking strategies across 11 real-world datasets. Our results reveal three\nkey findings for practitioners and researchers: (1) A \"weakest link\"\nphenomenon, where a single underperforming retrieval path can\ndisproportionately degrade overall accuracy, highlighting the need for\npath-wise quality assessment before fusion. (2) A data-driven map of the\nperformance trade-offs, demonstrating that optimal configurations depend\nheavily on resource constraints and data characteristics, moving beyond a\none-size-fits-all approach. (3) The identification of Tensor-based Re-ranking\nFusion (TRF) as a high-efficacy alternative to mainstream fusion methods,\noffering the semantic power of tensor search at a fraction of the computational\nand memory cost. Our findings offer concrete guidelines for designing the next\ngeneration of adaptive, scalable hybrid search systems while also identifying\nkey directions for future research.", "AI": {"tldr": "This paper benchmarks hybrid search architectures, revealing performance trade-offs and identifying Tensor-based Re-ranking Fusion as a promising alternative to mainstream fusion methods.", "motivation": "A systematic, empirical understanding of the trade-offs among the core components of hybrid search systems is critically lacking.", "method": "The paper presents a systematic benchmark of advanced hybrid search architectures, evaluating four retrieval paradigms (FTS, SVS, DVS, TenS) and benchmarking their combinations and re-ranking strategies across 11 real-world datasets.", "result": "The paper reveals three key findings: (1) A 'weakest link' phenomenon. (2) A data-driven map of the performance trade-offs. (3) The identification of Tensor-based Re-ranking Fusion (TRF) as a high-efficacy alternative to mainstream fusion methods.", "conclusion": "The paper offers concrete guidelines for designing the next generation of adaptive, scalable hybrid search systems and identifies key directions for future research."}}
{"id": "2508.01931", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2508.01931", "abs": "https://arxiv.org/abs/2508.01931", "authors": ["Wenjie Hu", "Guanzhou Hu", "Mahesh Balakrishnan", "Xiangyao Yu"], "title": "Marlin: Efficient Coordination for Autoscaling Cloud DBMS (Extended Version)", "comment": null, "summary": "Modern cloud databases are shifting from converged architectures to storage\ndisaggregation, enabling independent scaling and billing of compute and\nstorage. However, cloud databases still rely on external, converged\ncoordination services (e.g., ZooKeeper) for their control planes. These\nservices are effectively lightweight databases optimized for low-volume\nmetadata. As the control plane scales in the cloud, this approach faces similar\nlimitations as converged databases did before storage disaggregation:\nscalability bottlenecks, low cost efficiency, and increased operational burden.\n  We propose to disaggregate the cluster coordination to achieve the same\nbenefits that storage disaggregation brought to modern cloud DBMSs. We present\nMarlin, a cloud-native coordination mechanism that fully embraces storage\ndisaggregation. Marlin eliminates the need for external coordination services\nby consolidating coordination functionality into the existing cloud-native\ndatabase it manages. To achieve failover without an external coordination\nservice, Marlin allows cross-node modifications on coordination states. To\nensure data consistency, Marlin employs transactions to manage both\ncoordination and application states and introduces MarlinCommit, an optimized\ncommit protocol that ensures strong transactional guarantees even under\ncross-node modifications. Our evaluations demonstrate that Marlin improves cost\nefficiency by up to 4.4x and reduces reconfiguration duration by up to 4.9x\ncompared to converged coordination solutions.", "AI": {"tldr": "Marlin disaggregates cluster coordination in cloud databases, improving cost efficiency and reducing reconfiguration duration by consolidating coordination functionality and using optimized commit protocol.", "motivation": "Cloud databases rely on external, converged coordination services which face scalability bottlenecks, low cost efficiency, and increased operational burden as the control plane scales.", "method": "Marlin, a cloud-native coordination mechanism that consolidates coordination functionality into the existing cloud-native database and employs transactions with MarlinCommit for data consistency.", "result": "Marlin improves cost efficiency by up to 4.4x and reduces reconfiguration duration by up to 4.9x compared to converged coordination solutions.", "conclusion": "Marlin improves cost efficiency and reduces reconfiguration duration compared to converged coordination solutions."}}
{"id": "2508.02280", "categories": ["cs.DB", "H.2.4; E.4; H.3.2"], "pdf": "https://arxiv.org/pdf/2508.02280", "abs": "https://arxiv.org/abs/2508.02280", "authors": ["Francesco Gargiulo", "Rossano Venturini"], "title": "OnPair: Short Strings Compression for Fast Random Access", "comment": null, "summary": "We present OnPair, a dictionary-based compression algorithm designed to meet\nthe needs of in-memory database systems that require both high compression and\nfast random access. Existing methods either achieve strong compression ratios\nat significant computational and memory cost (e.g., BPE) or prioritize speed at\nthe expense of compression quality (e.g., FSST). OnPair bridges this gap by\nemploying a cache-friendly dictionary construction technique that incrementally\nmerges frequent adjacent substrings in a single sequential pass over a data\nsample. This enables fast, memory-efficient training without tracking global\npair positions, as required by traditional BPE. We also introduce OnPair16, a\nvariant that limits dictionary entries to 16 bytes, enabling faster parsing via\noptimized longest prefix matching. Both variants compress strings\nindependently, supporting fine-grained random access without block-level\noverhead. Experiments on real-world datasets show that OnPair and OnPair16\nachieve compression ratios comparable to BPE while significantly improving\ncompression speed and memory usage.", "AI": {"tldr": "OnPair\u662f\u4e00\u79cd\u5b57\u5178\u538b\u7f29\u7b97\u6cd5\uff0c\u5b83\u5728\u538b\u7f29\u7387\u3001\u901f\u5ea6\u548c\u5185\u5b58\u4f7f\u7528\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u8981\u4e48\u4ee5\u663e\u8457\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\uff08\u4f8b\u5982\uff0cBPE\uff09\u6765\u5b9e\u73b0\u5f3a\u5927\u7684\u538b\u7f29\u7387\uff0c\u8981\u4e48\u4ee5\u727a\u7272\u538b\u7f29\u8d28\u91cf\u4e3a\u4ee3\u4ef7\u6765\u4f18\u5148\u8003\u8651\u901f\u5ea6\uff08\u4f8b\u5982\uff0cFSST\uff09\u3002", "method": "\u91c7\u7528\u4e86\u4e00\u79cd\u7f13\u5b58\u53cb\u597d\u7684\u5b57\u5178\u6784\u5efa\u6280\u672f\uff0c\u8be5\u6280\u672f\u901a\u8fc7\u5bf9\u6570\u636e\u6837\u672c\u8fdb\u884c\u4e00\u6b21\u8fde\u7eed\u626b\u63cf\u6765\u589e\u91cf\u5408\u5e76\u9891\u7e41\u7684\u76f8\u90bb\u5b50\u5b57\u7b26\u4e32\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cOnPair\u548cOnPair16\u7684\u538b\u7f29\u7387\u4e0eBPE\u76f8\u5f53\uff0c\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u538b\u7f29\u901f\u5ea6\u548c\u5185\u5b58\u4f7f\u7528\u7387\u3002", "conclusion": "OnPair\u548cOnPair16\u7684\u538b\u7f29\u7387\u4e0eBPE\u76f8\u5f53\uff0c\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u538b\u7f29\u901f\u5ea6\u548c\u5185\u5b58\u4f7f\u7528\u7387\u3002"}}
{"id": "2508.01036", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.01036", "abs": "https://arxiv.org/abs/2508.01036", "authors": ["Omar Elgohary", "Nathan Jorgenson", "Trenton Marple"], "title": "Addressing Cold Start For next-article Recommendation", "comment": null, "summary": "This replication study modifies ALMM, the Adaptive Linear Mapping Model\nconstructed for the next song recommendation, to the news recommendation\nproblem on the MIND dataset. The original version of ALMM computes latent\nrepresentations for users, last-time items, and current items in a tensor\nfactorization structure and learns a linear mapping from content features to\nlatent item vectors. Our replication aims to improve recommendation performance\nin cold-start scenarios by restructuring this model to sequential news click\nbehavior, viewing consecutively read articles as (last news, next news) tuples.\nInstead of the original audio features, we apply BERT and a TF-IDF (Term\nFrequency-Inverse Document Frequency) to news titles and abstracts to extract\ntoken contextualized representations and align them with triplet-based user\nreading patterns. We also propose a reproducibly thorough pre-processing\npipeline combining news filtering and feature integrity validation. Our\nimplementation of ALMM with TF-IDF shows relatively improved recommendation\naccuracy and robustness over Forbes and Oord baseline models in the cold-start\nscenario. We demonstrate that ALMM in a minimally modified state is not\nsuitable for next news recommendation.", "AI": {"tldr": "\u672c\u7814\u7a76\u4fee\u6539\u4e86 ALMM \u6a21\u578b\uff0c\u4ee5\u89e3\u51b3 MIND \u6570\u636e\u96c6\u4e0a\u7684\u65b0\u95fb\u63a8\u8350\u95ee\u9898\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u5c06\u6b64\u6a21\u578b\u91cd\u6784\u4e3a\u5e8f\u5217\u65b0\u95fb\u70b9\u51fb\u884c\u4e3a\u6765\u63d0\u9ad8\u51b7\u542f\u52a8\u573a\u666f\u4e2d\u7684\u63a8\u8350\u6027\u80fd\u3002", "method": "\u5c06 ALMM \u6a21\u578b\u91cd\u6784\u4e3a\u5e8f\u5217\u65b0\u95fb\u70b9\u51fb\u884c\u4e3a\uff0c\u5e76\u5c06 BERT \u548c TF-IDF \u5e94\u7528\u4e8e\u65b0\u95fb\u6807\u9898\u548c\u6458\u8981\uff0c\u4ee5\u63d0\u53d6 token \u4e0a\u4e0b\u6587\u8868\u793a\uff0c\u5e76\u5c06\u5176\u4e0e\u57fa\u4e8e\u4e09\u5143\u7ec4\u7684\u7528\u6237\u9605\u8bfb\u6a21\u5f0f\u5bf9\u9f50\u3002", "result": "\u4f7f\u7528 TF-IDF \u7684 ALMM \u5b9e\u73b0\u663e\u793a\uff0c\u5728\u51b7\u542f\u52a8\u573a\u666f\u4e2d\uff0c\u76f8\u5bf9\u4e8e Forbes \u548c Oord \u57fa\u7ebf\u6a21\u578b\uff0c\u63a8\u8350\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u76f8\u5bf9\u63d0\u9ad8\u3002", "conclusion": "ALMM \u5728\u7ecf\u8fc7\u6700\u5c0f\u4fee\u6539\u540e\u4e0d\u9002\u5408\u7528\u4e8e\u4e0b\u4e00\u7bc7\u65b0\u95fb\u63a8\u8350\u3002"}}
{"id": "2508.00864", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00864", "abs": "https://arxiv.org/abs/2508.00864", "authors": ["Margarita Bugue\u00f1o", "Gerard de Melo"], "title": "Rethinking Graph-Based Document Classification: Learning Data-Driven Structures Beyond Heuristic Approaches", "comment": "7 pages, 3 figures, 3 tables. Appendix starts on page 10", "summary": "In document classification, graph-based models effectively capture document\nstructure, overcoming sequence length limitations and enhancing contextual\nunderstanding. However, most existing graph document representations rely on\nheuristics, domain-specific rules, or expert knowledge. Unlike previous\napproaches, we propose a method to learn data-driven graph structures,\neliminating the need for manual design and reducing domain dependence. Our\napproach constructs homogeneous weighted graphs with sentences as nodes, while\nedges are learned via a self-attention model that identifies dependencies\nbetween sentence pairs. A statistical filtering strategy aims to retain only\nstrongly correlated sentences, improving graph quality while reducing the graph\nsize. Experiments on three document classification datasets demonstrate that\nlearned graphs consistently outperform heuristic-based graphs, achieving higher\naccuracy and $F_1$ score. Furthermore, our study demonstrates the effectiveness\nof the statistical filtering in improving classification robustness. These\nresults highlight the potential of automatic graph generation over traditional\nheuristic approaches and open new directions for broader applications in NLP.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5b66\u4e60\u6587\u6863\u56fe\u7ed3\u6784\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u6587\u6863\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5728\u6587\u6863\u5206\u7c7b\u4e2d\uff0c\u57fa\u4e8e\u56fe\u7684\u6a21\u578b\u6709\u6548\u5730\u6355\u83b7\u6587\u6863\u7ed3\u6784\uff0c\u514b\u670d\u4e86\u5e8f\u5217\u957f\u5ea6\u7684\u9650\u5236\u5e76\u589e\u5f3a\u4e86\u4e0a\u4e0b\u6587\u7406\u89e3\u3002\u4f46\u662f\uff0c\u5927\u591a\u6570\u73b0\u6709\u7684\u56fe\u6587\u6863\u8868\u793a\u90fd\u4f9d\u8d56\u4e8e\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u7279\u5b9a\u4e8e\u9886\u57df\u7684\u89c4\u5219\u6216\u4e13\u5bb6\u77e5\u8bc6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b66\u4e60\u6570\u636e\u9a71\u52a8\u56fe\u7ed3\u6784\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6784\u5efa\u4e86\u4ee5\u53e5\u5b50\u4e3a\u8282\u70b9\u7684\u540c\u6784\u52a0\u6743\u56fe\uff0c\u800c\u8fb9\u662f\u901a\u8fc7\u8bc6\u522b\u53e5\u5b50\u5bf9\u4e4b\u95f4\u4f9d\u8d56\u5173\u7cfb\u7684self-attention\u6a21\u578b\u5b66\u4e60\u7684\u3002\u7edf\u8ba1\u8fc7\u6ee4\u7b56\u7565\u65e8\u5728\u4ec5\u4fdd\u7559\u5f3a\u76f8\u5173\u7684\u53e5\u5b50\uff0c\u4ece\u800c\u63d0\u9ad8\u56fe\u7684\u8d28\u91cf\u3002", "result": "\u5728\u4e09\u4e2a\u6587\u6863\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5b66\u4e60\u7684\u56fe\u59cb\u7ec8\u4f18\u4e8e\u57fa\u4e8e\u542f\u53d1\u5f0f\u7684\u56fe\uff0c\u4ece\u800c\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c$F_1$\u5206\u6570\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u7814\u7a76\u8bc1\u660e\u4e86\u7edf\u8ba1\u8fc7\u6ee4\u5728\u63d0\u9ad8\u5206\u7c7b\u9c81\u68d2\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u81ea\u52a8\u751f\u6210\u7684\u56fe\u7ed3\u6784\u4f18\u4e8e\u4f20\u7edf\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5e76\u4e3aNLP\u4e2d\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2508.00834", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00834", "abs": "https://arxiv.org/abs/2508.00834", "authors": ["Wei Wu", "Wenjie Wang", "Yang Tan", "Ying Liu", "Liang Diao", "Lin Huang", "Kaihe Xu", "Wenfeng Xie", "Ziling Lin"], "title": "Team PA-VCG's Solution for Competition on Understanding Chinese College Entrance Exam Papers in ICDAR'25", "comment": "Technical Report", "summary": "This report presents Team PA-VGG's solution for the ICDAR'25 Competition on\nUnderstanding Chinese College Entrance Exam Papers. In addition to leveraging\nhigh-resolution image processing and a multi-image end-to-end input strategy to\naddress the challenges of dense OCR extraction and complex document layouts in\nGaokao papers, our approach introduces domain-specific post-training\nstrategies. Experimental results demonstrate that our post-training approach\nachieves the most outstanding performance, securing first place with an\naccuracy rate of 89.6%.", "AI": {"tldr": "Team PA-VGG's solution for the ICDAR'25 Competition on Understanding Chinese College Entrance Exam Papers leverages high-resolution image processing, a multi-image end-to-end input strategy, and domain-specific post-training strategies to achieve first place with an accuracy rate of 89.6%.", "motivation": "address the challenges of dense OCR extraction and complex document layouts in Gaokao papers", "method": "high-resolution image processing and a multi-image end-to-end input strategy, domain-specific post-training strategies", "result": "achieves the most outstanding performance, securing first place with an accuracy rate of 89.6%.", "conclusion": "The post-training approach achieves the most outstanding performance, securing first place with an accuracy rate of 89.6%."}}
{"id": "2508.00844", "categories": ["cs.AI", "cs.ET", "cs.MA", "econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2508.00844", "abs": "https://arxiv.org/abs/2508.00844", "authors": ["Christopher Wissuchek", "Patrick Zschech"], "title": "Exploring Agentic Artificial Intelligence Systems: Towards a Typological Framework", "comment": "Preprint accepted for archival and presentation at the Pacific-Asia\n  Conference on Information Systems (PACIS) 2025, Kuala Lumpur, Malaysia", "summary": "Artificial intelligence (AI) systems are evolving beyond passive tools into\nautonomous agents capable of reasoning, adapting, and acting with minimal human\nintervention. Despite their growing presence, a structured framework is lacking\nto classify and compare these systems. This paper develops a typology of\nagentic AI systems, introducing eight dimensions that define their cognitive\nand environmental agency in an ordinal structure. Using a multi-phase\nmethodological approach, we construct and refine this typology, which is then\nevaluated through a human-AI hybrid approach and further distilled into\nconstructed types. The framework enables researchers and practitioners to\nanalyze varying levels of agency in AI systems. By offering a structured\nperspective on the progression of AI capabilities, the typology provides a\nfoundation for assessing current systems and anticipating future developments\nin agentic AI.", "AI": {"tldr": "This paper develops a typology of agentic AI systems, introducing eight dimensions that define their cognitive and environmental agency in an ordinal structure, enabling researchers and practitioners to analyze varying levels of agency in AI systems.", "motivation": "a structured framework is lacking to classify and compare these systems", "method": "a multi-phase methodological approach", "result": "develops a typology of agentic AI systems, introducing eight dimensions that define their cognitive and environmental agency in an ordinal structure", "conclusion": "The typology provides a foundation for assessing current systems and anticipating future developments in agentic AI."}}
{"id": "2508.00835", "categories": ["cs.LG", "cs.AI", "stat.ME"], "pdf": "https://arxiv.org/pdf/2508.00835", "abs": "https://arxiv.org/abs/2508.00835", "authors": ["Zachary T. Rewolinski", "Bin Yu"], "title": "PCS Workflow for Veridical Data Science in the Age of AI", "comment": null, "summary": "Data science is a pillar of artificial intelligence (AI), which is\ntransforming nearly every domain of human activity, from the social and\nphysical sciences to engineering and medicine. While data-driven findings in AI\noffer unprecedented power to extract insights and guide decision-making, many\nare difficult or impossible to replicate. A key reason for this challenge is\nthe uncertainty introduced by the many choices made throughout the data science\nlife cycle (DSLC). Traditional statistical frameworks often fail to account for\nthis uncertainty. The Predictability-Computability-Stability (PCS) framework\nfor veridical (truthful) data science offers a principled approach to\naddressing this challenge throughout the DSLC. This paper presents an updated\nand streamlined PCS workflow, tailored for practitioners and enhanced with\nguided use of generative AI. We include a running example to display the PCS\nframework in action, and conduct a related case study which showcases the\nuncertainty in downstream predictions caused by judgment calls in the data\ncleaning stage.", "AI": {"tldr": "The paper introduces an updated PCS workflow for truthful data science, enhanced with generative AI, to address the uncertainty in the data science life cycle. It includes a running example and a case study.", "motivation": "Data-driven findings in AI offer unprecedented power to extract insights and guide decision-making, many are difficult or impossible to replicate. A key reason for this challenge is the uncertainty introduced by the many choices made throughout the data science life cycle (DSLC). Traditional statistical frameworks often fail to account for this uncertainty.", "method": "PCS framework", "result": "We include a running example to display the PCS framework in action, and conduct a related case study which showcases the uncertainty in downstream predictions caused by judgment calls in the data cleaning stage.", "conclusion": "This paper presents an updated and streamlined PCS workflow, tailored for practitioners and enhanced with guided use of generative AI."}}
{"id": "2508.02458", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2508.02458", "abs": "https://arxiv.org/abs/2508.02458", "authors": ["Yichao Feng"], "title": "From Stimuli to Minds: Enhancing Psychological Reasoning in LLMs via Bilateral Reinforcement Learning", "comment": null, "summary": "Large Language Models show promise in emotion understanding, social\nreasoning, and empathy, yet they struggle with psychologically grounded tasks\nthat require inferring implicit mental states in context-rich, ambiguous\nsettings. These limitations arise from the absence of theory-aligned\nsupervision and the difficulty of capturing nuanced mental processes in\nreal-world narratives. To address this gap, we leverage expert-labeled,\npsychologically rich scenarios and propose a trajectory-aware reinforcement\nlearning framework that explicitly imitates expert psychological thought\npatterns. By integrating real-world stimuli with structured reasoning guidance,\nour approach enables compact models to internalize social-cognitive principles,\nperform nuanced psychological inference, and support continual\nself-improvement. Comprehensive experiments across multiple benchmarks further\ndemonstrate that our models achieve expert-level interpretive capabilities,\nexhibiting strong out-of-distribution generalization and robust continual\nlearning across diverse, challenging, and psychologically grounded tasks.", "AI": {"tldr": "This paper introduces a new framework to improve the ability of language models to understand implicit mental states. The framework uses reinforcement learning to imitate expert psychological thought patterns, allowing models to perform nuanced psychological inference and continual self-improvement.", "motivation": "Large Language Models struggle with psychologically grounded tasks that require inferring implicit mental states in context-rich, ambiguous settings. These limitations arise from the absence of theory-aligned supervision and the difficulty of capturing nuanced mental processes in real-world narratives.", "method": "a trajectory-aware reinforcement learning framework that explicitly imitates expert psychological thought patterns. By integrating real-world stimuli with structured reasoning guidance", "result": "our approach enables compact models to internalize social-cognitive principles, perform nuanced psychological inference, and support continual self-improvement.", "conclusion": "Models achieve expert-level interpretive capabilities, exhibiting strong out-of-distribution generalization and robust continual learning across diverse, challenging, and psychologically grounded tasks."}}
{"id": "2508.01128", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.01128", "abs": "https://arxiv.org/abs/2508.01128", "authors": ["Leyao Wang", "Xutao Mao", "Xuhui Zhan", "Yuying Zhao", "Bo Ni", "Ryan A. Rossi", "Nesreen K. Ahmed", "Tyler Derr"], "title": "Towards Bridging Review Sparsity in Recommendation with Textual Edge Graph Representation", "comment": "13 pages", "summary": "Textual reviews enrich recommender systems with fine-grained preference\nsignals and enhanced explainability. However, in real-world scenarios, users\nrarely leave reviews, resulting in severe sparsity that undermines the\neffectiveness of existing models. A natural solution is to impute or generate\nmissing reviews to enrich the data. However, conventional imputation techniques\n-- such as matrix completion and LLM-based augmentation -- either lose\ncontextualized semantics by embedding texts into vectors, or overlook\nstructural dependencies among user-item interactions. To address these\nshortcomings, we propose TWISTER (ToWards Imputation on Sparsity with Textual\nEdge Graph Representation), a unified framework that imputes missing reviews by\njointly modeling semantic and structural signals. Specifically, we represent\nuser-item interactions as a Textual-Edge Graph (TEG), treating reviews as edge\nattributes. To capture relational context, we construct line-graph views and\nemploy a large language model as a graph-aware aggregator. For each interaction\nlacking a textual review, our model aggregates the neighborhood's\nnatural-language representations to generate a coherent and personalized\nreview. Experiments on the Amazon and Goodreads datasets show that TWISTER\nconsistently outperforms traditional numeric, graph-based, and LLM baselines,\ndelivering higher-quality imputed reviews and, more importantly, enhanced\nrecommendation performance. In summary, TWISTER generates reviews that are more\nhelpful, authentic, and specific, while smoothing structural signals for\nimproved recommendations.", "AI": {"tldr": "TWISTER is a framework that imputes missing reviews by jointly modeling semantic and structural signals using a Textual-Edge Graph and a large language model, improving recommendation performance.", "motivation": "Real-world recommender systems suffer from review sparsity, undermining model effectiveness. Existing imputation techniques lose contextualized semantics or overlook structural dependencies.", "method": "Represent user-item interactions as a Textual-Edge Graph (TEG), construct line-graph views, and employ a large language model as a graph-aware aggregator to impute missing reviews.", "result": "TWISTER outperforms traditional numeric, graph-based, and LLM baselines on Amazon and Goodreads datasets, delivering higher-quality imputed reviews and enhanced recommendation performance.", "conclusion": "TWISTER generates helpful, authentic, and specific reviews, smoothing structural signals for improved recommendations."}}
{"id": "2508.00889", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00889", "abs": "https://arxiv.org/abs/2508.00889", "authors": ["Hagyeong Shin", "Binoy Robin Dalal", "Iwona Bialynicka-Birula", "Navjot Matharu", "Ryan Muir", "Xingwei Yang", "Samuel W. K. Wong"], "title": "FECT: Factuality Evaluation of Interpretive AI-Generated Claims in Contact Center Conversation Transcripts", "comment": "Accepted for an oral presentation at Agentic & GenAI Evaluation KDD\n  2025: KDD workshop on Evaluation and Trustworthiness of Agentic and\n  Generative AI Models", "summary": "Large language models (LLMs) are known to hallucinate, producing natural\nlanguage outputs that are not grounded in the input, reference materials, or\nreal-world knowledge. In enterprise applications where AI features support\nbusiness decisions, such hallucinations can be particularly detrimental. LLMs\nthat analyze and summarize contact center conversations introduce a unique set\nof challenges for factuality evaluation, because ground-truth labels often do\nnot exist for analytical interpretations about sentiments captured in the\nconversation and root causes of the business problems. To remedy this, we first\nintroduce a \\textbf{3D} -- \\textbf{Decompose, Decouple, Detach} -- paradigm in\nthe human annotation guideline and the LLM-judges' prompt to ground the\nfactuality labels in linguistically-informed evaluation criteria. We then\nintroduce \\textbf{FECT}, a novel benchmark dataset for \\textbf{F}actuality\n\\textbf{E}valuation of Interpretive AI-Generated \\textbf{C}laims in Contact\nCenter Conversation \\textbf{T}ranscripts, labeled under our 3D paradigm.\nLastly, we report our findings from aligning LLM-judges on the 3D paradigm.\nOverall, our findings contribute a new approach for automatically evaluating\nthe factuality of outputs generated by an AI system for analyzing contact\ncenter conversations.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u7528\u4e8e\u8bc4\u4f30\u8054\u7edc\u4e2d\u5fc3\u5bf9\u8bdd\u5206\u6790\u4e2d\u4eba\u5de5\u667a\u80fd\u751f\u6210\u8f93\u51fa\u7684\u4e8b\u5b9e\u6027\u7684\u65b0\u65b9\u6cd5\uff0c\u5305\u62ec 3D \u6807\u6ce8\u8303\u4f8b\u548c FECT \u57fa\u51c6\u6570\u636e\u96c6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5df2\u77e5\u4f1a\u4ea7\u751f\u5e7b\u89c9\uff0c\u4ea7\u751f\u672a\u57fa\u4e8e\u8f93\u5165\u3001\u53c2\u8003\u8d44\u6599\u6216\u771f\u5b9e\u4e16\u754c\u77e5\u8bc6\u7684\u81ea\u7136\u8bed\u8a00\u8f93\u51fa\u3002\u5728\u4eba\u5de5\u667a\u80fd\u529f\u80fd\u652f\u6301\u4e1a\u52a1\u51b3\u7b56\u7684\u4f01\u4e1a\u5e94\u7528\u7a0b\u5e8f\u4e2d\uff0c\u6b64\u7c7b\u5e7b\u89c9\u53ef\u80fd\u7279\u522b\u6709\u5bb3\u3002\u5206\u6790\u548c\u603b\u7ed3\u8054\u7edc\u4e2d\u5fc3\u5bf9\u8bdd\u7684\u6cd5\u5b66\u7855\u58eb\u4e3a\u4e8b\u5b9e\u6027\u8bc4\u4f30\u5e26\u6765\u4e86\u4e00\u7cfb\u5217\u72ec\u7279\u7684\u6311\u6218\uff0c\u56e0\u4e3a\u901a\u5e38\u4e0d\u5b58\u5728\u5173\u4e8e\u5bf9\u8bdd\u4e2d\u6355\u83b7\u7684\u60c5\u7eea\u548c\u4e1a\u52a1\u95ee\u9898\u6839\u672c\u539f\u56e0\u7684\u5206\u6790\u89e3\u91ca\u7684\u771f\u5b9e\u6807\u7b7e\u3002", "method": "\u8be5\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a 3D\uff08\u5206\u89e3\u3001\u89e3\u8026\u3001\u5206\u79bb\uff09\u8303\u4f8b\uff0c\u7528\u4e8e\u4eba\u5de5\u6807\u6ce8\u6307\u5357\u548c LLM \u8bc4\u5224\u5668\u7684\u63d0\u793a\uff0c\u4ee5\u5c06\u4e8b\u5b9e\u6027\u6807\u7b7e\u7f6e\u4e8e\u8bed\u8a00\u5b66\u77e5\u60c5\u8bc4\u4f30\u6807\u51c6\u4e2d\u3002\u7136\u540e\uff0c\u8be5\u7814\u7a76\u5f15\u5165\u4e86 FECT\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5728\u8054\u7edc\u4e2d\u5fc3\u5bf9\u8bdd\u8bb0\u5f55\u4e2d\u5bf9\u89e3\u91ca\u6027\u4eba\u5de5\u667a\u80fd\u751f\u6210\u7684\u58f0\u660e\u8fdb\u884c\u4e8b\u5b9e\u6027\u8bc4\u4f30\uff0c\u8be5\u6570\u636e\u96c6\u5728 3D \u8303\u4f8b\u4e0b\u8fdb\u884c\u6807\u8bb0\u3002", "result": "\u8be5\u7814\u7a76\u62a5\u544a\u4e86\u5c06 LLM \u8bc4\u5224\u5668\u4e0e 3D \u8303\u4f8b\u5bf9\u9f50\u7684\u7ed3\u679c\u3002\u603b\u4f53\u800c\u8a00\uff0c\u8be5\u7814\u7a76\u7684\u7ed3\u679c\u4e3a\u81ea\u52a8\u8bc4\u4f30\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u5728\u5206\u6790\u8054\u7edc\u4e2d\u5fc3\u5bf9\u8bdd\u65f6\u751f\u6210\u7684\u8f93\u51fa\u7684\u4e8b\u5b9e\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u8bc4\u4f30\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u5728\u5206\u6790\u8054\u7edc\u4e2d\u5fc3\u5bf9\u8bdd\u65f6\u751f\u6210\u7684\u8f93\u51fa\u7684\u4e8b\u5b9e\u6027\u3002"}}
{"id": "2508.00841", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00841", "abs": "https://arxiv.org/abs/2508.00841", "authors": ["Ali Haitham Abdul Amir", "Zainab N. Nemer"], "title": "Inclusive Review on Advances in Masked Human Face Recognition Technologies", "comment": null, "summary": "Masked Face Recognition (MFR) is an increasingly important area in biometric\nrecognition technologies, especially with the widespread use of masks as a\nresult of the COVID-19 pandemic. This development has created new challenges\nfor facial recognition systems due to the partial concealment of basic facial\nfeatures. This paper aims to provide a comprehensive review of the latest\ndevelopments in the field, with a focus on deep learning techniques, especially\nconvolutional neural networks (CNNs) and twin networks (Siamese networks),\nwhich have played a pivotal role in improving the accuracy of covering face\nrecognition. The paper discusses the most prominent challenges, which include\nchanges in lighting, different facial positions, partial concealment, and the\nimpact of mask types on the performance of systems. It also reviews advanced\ntechnologies developed to overcome these challenges, including data enhancement\nusing artificial databases and multimedia methods to improve the ability of\nsystems to generalize. In addition, the paper highlights advance in deep\nnetwork design, feature extraction techniques, evaluation criteria, and data\nsets used in this area. Moreover, it reviews the various applications of masked\nface recognition in the fields of security and medicine, highlighting the\ngrowing importance of these systems in light of recurrent health crises and\nincreasing security threats. Finally, the paper focuses on future research\ntrends such as developing more efficient algorithms and integrating multimedia\ntechnologies to improve the performance of recognition systems in real-world\nenvironments and expand their applications.", "AI": {"tldr": "\u672c\u6587\u5168\u9762\u56de\u987e\u4e86\u53e3\u7f69\u4eba\u8138\u8bc6\u522b\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u8ba8\u8bba\u4e86\u6700\u7a81\u51fa\u7684\u6311\u6218\uff0c\u5e76\u56de\u987e\u4e86\u5404\u79cd\u5e94\u7528\u3002", "motivation": "\u7531\u4e8e COVID-19 \u5927\u6d41\u884c\uff0c\u53e3\u7f69\u7684\u5e7f\u6cdb\u4f7f\u7528\u5bfc\u81f4\u9762\u90e8\u57fa\u672c\u7279\u5f81\u7684\u90e8\u5206\u9690\u85cf\uff0c\u8fd9\u5bf9\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u63d0\u51fa\u4e86\u65b0\u7684\u6311\u6218\u3002", "method": "\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u7279\u522b\u662f\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) \u548c\u5b6a\u751f\u7f51\u7edc (Siamese networks)\u3002", "result": "\u56de\u987e\u4e86\u514b\u670d\u8fd9\u4e9b\u6311\u6218\u7684\u5148\u8fdb\u6280\u672f\uff0c\u5305\u62ec\u4f7f\u7528\u4eba\u5de5\u6570\u636e\u5e93\u7684\u6570\u636e\u589e\u5f3a\u548c\u591a\u5a92\u4f53\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u7cfb\u7edf\u6cdb\u5316\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u672c\u6587\u91cd\u70b9\u4ecb\u7ecd\u4e86\u6df1\u5ea6\u7f51\u7edc\u8bbe\u8ba1\u3001\u7279\u5f81\u63d0\u53d6\u6280\u672f\u3001\u8bc4\u4f30\u6807\u51c6\u4ee5\u53ca\u8be5\u9886\u57df\u4e2d\u4f7f\u7528\u7684\u6570\u636e\u96c6\u7684\u8fdb\u5c55\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u8d8b\u52bf\u5305\u62ec\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u7b97\u6cd5\u548c\u96c6\u6210\u591a\u5a92\u4f53\u6280\u672f\uff0c\u4ee5\u63d0\u9ad8\u8bc6\u522b\u7cfb\u7edf\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u6027\u80fd\u5e76\u6269\u5c55\u5176\u5e94\u7528\u3002"}}
{"id": "2508.00853", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.00853", "abs": "https://arxiv.org/abs/2508.00853", "authors": ["Kei Itoh"], "title": "A Formal Framework for the Definition of 'State': Hierarchical Representation and Meta-Universe Interpretation", "comment": "43 pages, 8 figures, 8 Tables, in English, in Japanese", "summary": "This study aims to reinforce the theoretical foundation for diverse\nsystems--including the axiomatic definition of intelligence--by introducing a\nmathematically rigorous and unified formal structure for the concept of\n'state,' which has long been used without consensus or formal clarity. First, a\n'hierarchical state grid' composed of two axes--state depth and mapping\nhierarchy--is proposed to provide a unified notational system applicable across\nmathematical, physical, and linguistic domains. Next, the 'Intermediate\nMeta-Universe (IMU)' is introduced to enable explicit descriptions of definers\n(ourselves) and the languages we use, thereby allowing conscious meta-level\noperations while avoiding self-reference and logical inconsistency. Building on\nthis meta-theoretical foundation, this study expands inter-universal theory\nbeyond mathematics to include linguistic translation and agent integration,\nintroducing the conceptual division between macrocosm-inter-universal and\nmicrocosm-inter-universal operations for broader expressivity. Through these\ncontributions, this paper presents a meta-formal logical framework--grounded in\nthe principle of definition = state--that spans time, language, agents, and\noperations, providing a mathematically robust foundation applicable to the\ndefinition of intelligence, formal logic, and scientific theory at large.", "AI": {"tldr": "Introduces a meta-formal logical framework for defining 'state' to provide a foundation for intelligence, logic, and scientific theory.", "motivation": "to reinforce the theoretical foundation for diverse systems by introducing a mathematically rigorous and unified formal structure for the concept of 'state,' which has long been used without consensus or formal clarity", "method": "introducing a mathematically rigorous and unified formal structure for the concept of 'state', a 'hierarchical state grid', and the 'Intermediate Meta-Universe (IMU)'", "result": "expands inter-universal theory beyond mathematics to include linguistic translation and agent integration", "conclusion": "This paper presents a meta-formal logical framework applicable to the definition of intelligence, formal logic, and scientific theory at large."}}
{"id": "2508.00855", "categories": ["cs.LG", "cs.CE", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2508.00855", "abs": "https://arxiv.org/abs/2508.00855", "authors": ["Ziyang Zhang", "Feifan Zhang", "Weidong Tang", "Lei Shi", "Tailai Chen"], "title": "A Residual Guided strategy with Generative Adversarial Networks in training Physics-Informed Transformer Networks", "comment": null, "summary": "Nonlinear partial differential equations (PDEs) are pivotal in modeling\ncomplex physical systems, yet traditional Physics-Informed Neural Networks\n(PINNs) often struggle with unresolved residuals in critical spatiotemporal\nregions and violations of temporal causality. To address these limitations, we\npropose a novel Residual Guided Training strategy for Physics-Informed\nTransformer via Generative Adversarial Networks (GAN). Our framework integrates\na decoder-only Transformer to inherently capture temporal correlations through\nautoregressive processing, coupled with a residual-aware GAN that dynamically\nidentifies and prioritizes high-residual regions. By introducing a causal\npenalty term and an adaptive sampling mechanism, the method enforces temporal\ncausality while refining accuracy in problematic domains. Extensive numerical\nexperiments on the Allen-Cahn, Klein-Gordon, and Navier-Stokes equations\ndemonstrate significant improvements, achieving relative MSE reductions of up\nto three orders of magnitude compared to baseline methods. This work bridges\nthe gap between deep learning and physics-driven modeling, offering a robust\nsolution for multiscale and time-dependent PDE systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6b8b\u5dee\u5f15\u5bfc\u8bad\u7ec3\u7b56\u7565\uff0c\u7528\u4e8e\u7269\u7406\u4fe1\u606fTransformer\uff0c\u901a\u8fc7\u7ed3\u5408GAN\u6765\u63d0\u9ad8\u6c42\u89e3\u975e\u7ebf\u6027PDE\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u5728\u5173\u952e\u65f6\u7a7a\u533a\u57df\u4e2d\u7ecf\u5e38\u9762\u4e34\u672a\u89e3\u51b3\u7684\u6b8b\u5dee\u548c\u8fdd\u53cd\u65f6\u95f4\u56e0\u679c\u5173\u7cfb\u7684\u95ee\u9898\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u89e3\u7801\u5668Transformer\u548c\u6b8b\u5dee\u611f\u77e5GAN\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u5904\u7406\u6355\u6349\u65f6\u95f4\u76f8\u5173\u6027\uff0c\u5e76\u52a8\u6001\u8bc6\u522b\u548c\u4f18\u5148\u5904\u7406\u9ad8\u6b8b\u5dee\u533a\u57df\u3002\u5f15\u5165\u4e86\u56e0\u679c\u60e9\u7f5a\u9879\u548c\u81ea\u9002\u5e94\u62bd\u6837\u673a\u5236\uff0c\u4ee5\u52a0\u5f3a\u65f6\u95f4\u56e0\u679c\u5173\u7cfb\uff0c\u540c\u65f6\u63d0\u9ad8\u95ee\u9898\u57df\u7684\u7cbe\u5ea6\u3002", "result": "\u5728Allen-Cahn\u3001Klein-Gordon\u548cNavier-Stokes\u65b9\u7a0b\u7684\u5927\u91cf\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u76f8\u5bf9MSE\u964d\u4f4e\u4e86\u9ad8\u8fbe\u4e09\u4e2a\u6570\u91cf\u7ea7\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002", "conclusion": "\u8be5\u8bba\u6587\u901a\u8fc7\u7ed3\u5408\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u7684\u6b8b\u5dee\u5f15\u5bfc\u8bad\u7ec3\u7b56\u7565\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7269\u7406\u4fe1\u606fTransformer\u7684\u65b0\u65b9\u6cd5\uff0c\u4e3a\u591a\u5c3a\u5ea6\u548c\u65f6\u95f4\u76f8\u5173\u7684PDE\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.02508", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2508.02508", "abs": "https://arxiv.org/abs/2508.02508", "authors": ["Kyoseung Koo", "Bogyeong Kim", "Bongki Moon"], "title": "M2: An Analytic System with Specialized Storage Engines for Multi-Model Workloads", "comment": null, "summary": "Modern data analytic workloads increasingly require handling multiple data\nmodels simultaneously. Two primary approaches meet this need: polyglot\npersistence and multi-model database systems. Polyglot persistence employs a\ncoordinator program to manage several independent database systems but suffers\nfrom high communication costs due to its physically disaggregated architecture.\nMeanwhile, existing multi-model database systems rely on a single storage\nengine optimized for a specific data model, resulting in inefficient processing\nacross diverse data models. To address these limitations, we present M2, a\nmulti-model analytic system with integrated storage engines. M2 treats all data\nmodels as first-class entities, composing query plans that incorporate\noperations across models. To effectively combine data from different models,\nthe system introduces a specialized inter-model join algorithm called\nmulti-stage hash join. Our evaluation demonstrates that M2 outperforms existing\napproaches by up to 188x speedup on multi-model analytics, confirming the\neffectiveness of our proposed techniques.", "AI": {"tldr": "M2, a multi-model analytic system with integrated storage engines, outperforms existing approaches by up to 188x speedup on multi-model analytics.", "motivation": "Modern data analytic workloads increasingly require handling multiple data models simultaneously. Polyglot persistence employs a coordinator program to manage several independent database systems but suffers from high communication costs due to its physically disaggregated architecture. Meanwhile, existing multi-model database systems rely on a single storage engine optimized for a specific data model, resulting in inefficient processing across diverse data models.", "method": "a multi-model analytic system with integrated storage engines. M2 treats all data models as first-class entities, composing query plans that incorporate operations across models. To effectively combine data from different models, the system introduces a specialized inter-model join algorithm called multi-stage hash join.", "result": "M2 outperforms existing approaches by up to 188x speedup on multi-model analytics", "conclusion": "M2 outperforms existing approaches by up to 188x speedup on multi-model analytics, confirming the effectiveness of our proposed techniques."}}
{"id": "2508.01226", "categories": ["cs.IR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.01226", "abs": "https://arxiv.org/abs/2508.01226", "authors": ["Xin Zhou", "Yongjie Wang", "Zhiqi Shen"], "title": "CM$^3$: Calibrating Multimodal Recommendation", "comment": "Working Paper: https://github.com/enoche/CM3", "summary": "Alignment and uniformity are fundamental principles within the domain of\ncontrastive learning. In recommender systems, prior work has established that\noptimizing the Bayesian Personalized Ranking (BPR) loss contributes to the\nobjectives of alignment and uniformity. Specifically, alignment aims to draw\ntogether the representations of interacting users and items, while uniformity\nmandates a uniform distribution of user and item embeddings across a unit\nhypersphere. This study revisits the alignment and uniformity properties within\nthe context of multimodal recommender systems, revealing a proclivity among\nextant models to prioritize uniformity to the detriment of alignment. Our\nhypothesis challenges the conventional assumption of equitable item treatment\nthrough a uniformity loss, proposing a more nuanced approach wherein items with\nsimilar multimodal attributes converge toward proximal representations within\nthe hyperspheric manifold. Specifically, we leverage the inherent similarity\nbetween items' multimodal data to calibrate their uniformity distribution,\nthereby inducing a more pronounced repulsive force between dissimilar entities\nwithin the embedding space. A theoretical analysis elucidates the relationship\nbetween this calibrated uniformity loss and the conventional uniformity\nfunction. Moreover, to enhance the fusion of multimodal features, we introduce\na Spherical B\\'ezier method designed to integrate an arbitrary number of\nmodalities while ensuring that the resulting fused features are constrained to\nthe same hyperspherical manifold. Empirical evaluations conducted on five\nreal-world datasets substantiate the superiority of our approach over competing\nbaselines. We also shown that the proposed methods can achieve up to a 5.4%\nincrease in NDCG@20 performance via the integration of MLLM-extracted features.\nSource code is available at: https://github.com/enoche/CM3.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5728\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\u4e2d\u91cd\u65b0\u5ba1\u89c6\u4e86\u5bf9\u9f50\u6027\u548c\u5747\u5300\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6821\u51c6\u5747\u5300\u6027\u635f\u5931\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\u6a21\u578b\u503e\u5411\u4e8e\u4f18\u5148\u8003\u8651\u5747\u5300\u6027\u800c\u635f\u5bb3\u5bf9\u9f50\u6027\uff0c\u5e76\u4e14\u901a\u8fc7\u5747\u5300\u6027\u635f\u5931\u516c\u5e73\u5bf9\u5f85\u9879\u76ee\u7684\u4f20\u7edf\u5047\u8bbe\u63d0\u51fa\u4e86\u6311\u6218\u3002", "method": "\u8be5\u7814\u7a76\u5229\u7528\u9879\u76ee\u591a\u6a21\u6001\u6570\u636e\u4e4b\u95f4\u7684\u5185\u5728\u76f8\u4f3c\u6027\u6765\u6821\u51c6\u5176\u5747\u5300\u6027\u5206\u5e03\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u7403\u5f62\u8d1d\u585e\u5c14\u65b9\u6cd5\uff0c\u65e8\u5728\u6574\u5408\u4efb\u610f\u6570\u91cf\u7684\u6a21\u6001\uff0c\u540c\u65f6\u786e\u4fdd\u878d\u5408\u540e\u7684\u7279\u5f81\u7ea6\u675f\u5728\u540c\u4e00\u8d85\u7403\u9762\u6d41\u5f62\u4e0a\u3002", "result": "\u5728\u4e94\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u4f18\u4e8e\u7ade\u4e89\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u6821\u51c6\u5747\u5300\u6027\u5206\u5e03\u5e76\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5f15\u5165\u66f4\u5f3a\u7684\u76f8\u65a5\u529b\uff0c\u4ece\u800c\u5728\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u7684\u6027\u80fd\u3002\u901a\u8fc7\u96c6\u6210 MLLM \u63d0\u53d6\u7684\u7279\u5f81\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728 NDCG@20 \u6027\u80fd\u65b9\u9762\u5b9e\u73b0\u4e86\u9ad8\u8fbe 5.4% \u7684\u63d0\u5347\u3002"}}
{"id": "2508.00924", "categories": ["cs.CL", "68T05, 68T50", "I.2.6; I.2.7; I.2.8"], "pdf": "https://arxiv.org/pdf/2508.00924", "abs": "https://arxiv.org/abs/2508.00924", "authors": ["Ernesto L. Estevanell-Valladares", "Suilan Estevez-Velarde", "Yoan Guti\u00e9rrez", "Andr\u00e9s Montoyo", "Ruslan Mitkov"], "title": "XAutoLM: Efficient Fine-Tuning of Language Models via Meta-Learning and AutoML", "comment": "17 pages, 10 figures, 7 tables. Preprint. Under review at EMNLP 2025.\n  This is not the final version", "summary": "Experts in machine learning leverage domain knowledge to navigate decisions\nin model selection, hyperparameter optimisation, and resource allocation. This\nis particularly critical for fine-tuning language models (LMs), where repeated\ntrials incur substantial computational overhead and environmental impact.\nHowever, no existing automated framework simultaneously tackles the entire\nmodel selection and HPO task for resource-efficient LM fine-tuning. We\nintroduce XAutoLM, a meta-learning-augmented AutoML framework that reuses past\nexperiences to optimise discriminative and generative LM fine-tuning pipelines\nefficiently. XAutoLM learns from stored successes and failures by extracting\ntask- and system-level meta-features to bias its sampling toward fruitful\nconfigurations and away from costly dead ends. On four text classification and\ntwo question-answering benchmarks, XAutoLM surpasses zero-shot optimiser's peak\nF1 on five of six tasks, cuts mean evaluation time by up to 4.5x, reduces error\nratios by up to sevenfold, and uncovers up to 50% more pipelines above the\nzero-shot Pareto front. In contrast, simpler memory-based baselines suffer\nnegative transfer. We release XAutoLM and our experience store to catalyse\nresource-efficient, Green AI fine-tuning in the NLP community.", "AI": {"tldr": "XAutoLM is a meta-learning AutoML framework that reuses past experiences to efficiently optimise LM fine-tuning, outperforming existing methods in speed and accuracy.", "motivation": "Experts in machine learning leverage domain knowledge to navigate decisions in model selection, hyperparameter optimisation, and resource allocation. This is particularly critical for fine-tuning language models (LMs), where repeated trials incur substantial computational overhead and environmental impact. However, no existing automated framework simultaneously tackles the entire model selection and HPO task for resource-efficient LM fine-tuning.", "method": "a meta-learning-augmented AutoML framework that reuses past experiences to optimise discriminative and generative LM fine-tuning pipelines efficiently. XAutoLM learns from stored successes and failures by extracting task- and system-level meta-features to bias its sampling toward fruitful configurations and away from costly dead ends.", "result": "XAutoLM surpasses zero-shot optimiser's peak F1 on five of six tasks, cuts mean evaluation time by up to 4.5x, reduces error ratios by up to sevenfold, and uncovers up to 50% more pipelines above the zero-shot Pareto front. In contrast, simpler memory-based baselines suffer negative transfer.", "conclusion": "XAutoLM surpasses zero-shot optimiser's peak F1 on five of six tasks, cuts mean evaluation time by up to 4.5x, reduces error ratios by up to sevenfold, and uncovers up to 50% more pipelines above the zero-shot Pareto front."}}
{"id": "2508.00892", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00892", "abs": "https://arxiv.org/abs/2508.00892", "authors": ["Zhihao Zhu", "Jiale Han", "Yi Yang"], "title": "HoneyImage: Verifiable, Harmless, and Stealthy Dataset Ownership Verification for Image Models", "comment": null, "summary": "Image-based AI models are increasingly deployed across a wide range of\ndomains, including healthcare, security, and consumer applications. However,\nmany image datasets carry sensitive or proprietary content, raising critical\nconcerns about unauthorized data usage. Data owners therefore need reliable\nmechanisms to verify whether their proprietary data has been misused to train\nthird-party models. Existing solutions, such as backdoor watermarking and\nmembership inference, face inherent trade-offs between verification\neffectiveness and preservation of data integrity. In this work, we propose\nHoneyImage, a novel method for dataset ownership verification in image\nrecognition models. HoneyImage selectively modifies a small number of hard\nsamples to embed imperceptible yet verifiable traces, enabling reliable\nownership verification while maintaining dataset integrity. Extensive\nexperiments across four benchmark datasets and multiple model architectures\nshow that HoneyImage consistently achieves strong verification accuracy with\nminimal impact on downstream performance while maintaining imperceptible. The\nproposed HoneyImage method could provide data owners with a practical mechanism\nto protect ownership over valuable image datasets, encouraging safe sharing and\nunlocking the full transformative potential of data-driven AI.", "AI": {"tldr": "HoneyImage is proposed to verify the ownership of image datasets by embedding imperceptible traces. It achieves strong verification accuracy with minimal impact on downstream performance while maintaining imperceptibility.", "motivation": "Data owners therefore need reliable mechanisms to verify whether their proprietary data has been misused to train third-party models. Existing solutions, such as backdoor watermarking and membership inference, face inherent trade-offs between verification effectiveness and preservation of data integrity.", "method": "HoneyImage selectively modifies a small number of hard samples to embed imperceptible yet verifiable traces, enabling reliable ownership verification while maintaining dataset integrity.", "result": "Extensive experiments across four benchmark datasets and multiple model architectures show that HoneyImage consistently achieves strong verification accuracy with minimal impact on downstream performance while maintaining imperceptible.", "conclusion": "HoneyImage consistently achieves strong verification accuracy with minimal impact on downstream performance while maintaining imperceptible. The proposed HoneyImage method could provide data owners with a practical mechanism to protect ownership over valuable image datasets, encouraging safe sharing and unlocking the full transformative potential of data-driven AI."}}
{"id": "2508.00890", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.00890", "abs": "https://arxiv.org/abs/2508.00890", "authors": ["Fali Wang", "Hui Liu", "Zhenwei Dai", "Jingying Zeng", "Zhiwei Zhang", "Zongyu Wu", "Chen Luo", "Zhen Li", "Xianfeng Tang", "Qi He", "Suhang Wang"], "title": "AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks", "comment": "Under review", "summary": "Test-time scaling (TTS) enhances the performance of large language models\n(LLMs) by allocating additional compute resources during inference. However,\nexisting research primarily investigates TTS in single-stage tasks; while many\nreal-world problems are multi-stage complex tasks, composed of a sequence of\nheterogeneous subtasks with each subtask requires LLM of specific capability.\nTherefore, we study a novel problem: the test-time compute-optimal scaling in\nmulti-stage complex tasks, aiming to select suitable models and allocate\nbudgets per subtask to maximize overall performance. TTS in multi-stage tasks\nintroduces two fundamental challenges: (i) The combinatorial search space of\nmodel and budget allocations, combined with the high cost of inference, makes\nbrute-force search impractical. (ii) The optimal model and budget allocations\nacross subtasks are interdependent, increasing the complexity of the\ncompute-optimal search. To address this gap, we conduct extensive pilot\nexperiments on four tasks across six datasets, deriving three empirical\ninsights characterizing the behavior of LLMs in multi-stage complex tasks.\nInformed by these insights, we propose AgentTTS, an LLM-agent-based framework\nthat autonomously searches for compute-optimal allocations through iterative\nfeedback-driven interactions with the execution environment. Experimental\nresults demonstrate that AgentTTS significantly outperforms traditional and\nother LLM-based baselines in search efficiency, and shows improved robustness\nto varying training set sizes and enhanced interpretability.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u9636\u6bb5\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6700\u4f73\u7f29\u653e\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86AgentTTS\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5728\u641c\u7d22\u6548\u7387\u3001\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u8c03\u67e5\u5355\u9636\u6bb5\u4efb\u52a1\u4e2d\u7684TTS\uff1b\u800c\u8bb8\u591a\u5b9e\u9645\u95ee\u9898\u662f\u591a\u9636\u6bb5\u590d\u6742\u4efb\u52a1\uff0c\u7531\u4e00\u7cfb\u5217\u5f02\u6784\u5b50\u4efb\u52a1\u7ec4\u6210\uff0c\u6bcf\u4e2a\u5b50\u4efb\u52a1\u90fd\u9700\u8981\u5177\u6709\u7279\u5b9a\u80fd\u529b\u7684LLM\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u4e00\u4e2a\u65b0\u95ee\u9898\uff1a\u591a\u9636\u6bb5\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6700\u4f73\u7f29\u653e\uff0c\u65e8\u5728\u9009\u62e9\u5408\u9002\u7684\u6a21\u578b\u5e76\u4e3a\u6bcf\u4e2a\u5b50\u4efb\u52a1\u5206\u914d\u9884\u7b97\uff0c\u4ee5\u6700\u5927\u5316\u6574\u4f53\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86AgentTTS\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u4e0e\u6267\u884c\u73af\u5883\u7684\u8fed\u4ee3\u53cd\u9988\u9a71\u52a8\u7684\u4ea4\u4e92\u6765\u81ea\u52a8\u641c\u7d22\u8ba1\u7b97\u6700\u4f73\u5206\u914d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAgentTTS\u5728\u641c\u7d22\u6548\u7387\u65b9\u9762\u660e\u663e\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u5176\u5b83\u57fa\u4e8eLLM\u7684\u57fa\u7ebf\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u7684\u8bad\u7ec3\u96c6\u5927\u5c0f\u4e0b\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u4ee5\u53ca\u589e\u5f3a\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "AgentTTS\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u5176\u4ed6\u57fa\u4e8eLLM\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u641c\u7d22\u6548\u7387\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u5bf9\u4e0d\u540c\u7684\u8bad\u7ec3\u96c6\u5927\u5c0f\u8868\u73b0\u51fa\u66f4\u597d\u7684\u9c81\u68d2\u6027\u548c\u589e\u5f3a\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2508.00858", "categories": ["cs.LG", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00858", "abs": "https://arxiv.org/abs/2508.00858", "authors": ["Christina Butsko", "Kristof Van Tricht", "Gabriel Tseng", "Giorgia Milli", "David Rolnick", "Ruben Cartuyvels", "Inbal Becker Reshef", "Zoltan Szantoi", "Hannah Kerner"], "title": "Deploying Geospatial Foundation Models in the Real World: Lessons from WorldCereal", "comment": null, "summary": "The increasing availability of geospatial foundation models has the potential\nto transform remote sensing applications such as land cover classification,\nenvironmental monitoring, and change detection. Despite promising benchmark\nresults, the deployment of these models in operational settings is challenging\nand rare. Standardized evaluation tasks often fail to capture real-world\ncomplexities relevant for end-user adoption such as data heterogeneity,\nresource constraints, and application-specific requirements. This paper\npresents a structured approach to integrate geospatial foundation models into\noperational mapping systems. Our protocol has three key steps: defining\napplication requirements, adapting the model to domain-specific data and\nconducting rigorous empirical testing. Using the Presto model in a case study\nfor crop mapping, we demonstrate that fine-tuning a pre-trained model\nsignificantly improves performance over conventional supervised methods. Our\nresults highlight the model's strong spatial and temporal generalization\ncapabilities. Our protocol provides a replicable blueprint for practitioners\nand lays the groundwork for future research to operationalize foundation models\nin diverse remote sensing applications. Application of the protocol to the\nWorldCereal global crop-mapping system showcases the framework's scalability.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\u96c6\u6210\u5230 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0435 \u6620\u5c04\u7cfb\u7edf\u4e2d\u7684\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u5e76\u5728\u4f5c\u7269\u5236\u56fe\u6848\u4f8b\u7814\u7a76\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\u7684\u65e5\u76ca\u666e\u53ca\u6709\u53ef\u80fd\u6539\u53d8\u9065\u611f\u5e94\u7528\uff0c\u4f8b\u5982\u571f\u5730\u8986\u76d6\u5206\u7c7b\u3001\u73af\u5883\u76d1\u6d4b\u548c\u53d8\u5316\u68c0\u6d4b\u3002\u5c3d\u7ba1\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u5f88\u6709\u5e0c\u671b\uff0c\u4f46\u5728 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0435 \u73af\u5883\u4e2d\u90e8\u7f72\u8fd9\u4e9b\u6a21\u578b\u5177\u6709\u6311\u6218\u6027\u4e14\u5f88\u5c11\u89c1\u3002\u6807\u51c6\u5316\u8bc4\u4f30\u4efb\u52a1\u901a\u5e38\u65e0\u6cd5\u6355\u6349\u5230\u4e0e\u6700\u7ec8\u7528\u6237\u91c7\u7528\u76f8\u5173\u7684\u73b0\u5b9e\u4e16\u754c\u7684\u590d\u6742\u6027\uff0c\u4f8b\u5982\u6570\u636e\u5f02\u6784\u6027\u3001\u8d44\u6e90\u7ea6\u675f\u548c\u7279\u5b9a\u4e8e\u5e94\u7528\u7a0b\u5e8f\u7684\u9700\u6c42\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u7684\u65b9\u6cd5\uff0c\u5c06\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\u96c6\u6210\u5230 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0435 \u7cfb\u7edf\u4e2d\u3002\u8be5\u534f\u8bae\u6709\u4e09\u4e2a\u5173\u952e\u6b65\u9aa4\uff1a\u5b9a\u4e49\u5e94\u7528\u7a0b\u5e8f\u9700\u6c42\u3001\u4f7f\u6a21\u578b\u9002\u5e94\u7279\u5b9a\u9886\u57df\u6570\u636e\u4ee5\u53ca\u8fdb\u884c\u4e25\u683c\u7684\u5b9e\u8bc1\u6d4b\u8bd5\u3002", "result": "\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u76d1\u7763\u65b9\u6cd5\u3002\u8be5\u6a21\u578b\u5177\u6709\u5f3a\u5927\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u6cdb\u5316\u80fd\u529b\u3002\u8be5\u6846\u67b6\u5177\u6709\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u5728\u4f5c\u7269\u5236\u56fe\u6848\u4f8b\u7814\u7a76\u4e2d\u4f7f\u7528Presto\u6a21\u578b\uff0c\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6027\u80fd\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u76d1\u7763\u65b9\u6cd5\u3002\u7ed3\u679c\u5f3a\u8c03\u4e86\u8be5\u6a21\u578b\u5f3a\u5927\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u6cdb\u5316\u80fd\u529b\u3002\u8be5\u534f\u8bae\u5728 WorldCereal \u5168\u7403\u4f5c\u7269\u5236\u56fe\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u5c55\u793a\u4e86\u6846\u67b6\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2508.02548", "categories": ["cs.DB", "cs.AI", "68P15"], "pdf": "https://arxiv.org/pdf/2508.02548", "abs": "https://arxiv.org/abs/2508.02548", "authors": ["Enrico Franconi", "Beno\u00eet Groz", "Jan Hidders", "Nina Pardal", "S\u0142awek Staworko", "Jan Van den Bussche", "Piotr Wieczorek"], "title": "The KG-ER Conceptual Schema Language", "comment": null, "summary": "We propose KG-ER, a conceptual schema language for knowledge graphs that\ndescribes the structure of knowledge graphs independently of their\nrepresentation (relational databases, property graphs, RDF) while helping to\ncapture the semantics of the information stored in a knowledge graph.", "AI": {"tldr": "KG-ER: a conceptual schema language for knowledge graphs.", "motivation": "describes the structure of knowledge graphs independently of their representation while helping to capture the semantics of the information stored in a knowledge graph", "method": "KG-ER", "result": "None", "conclusion": "a conceptual schema language for knowledge graphs"}}
{"id": "2508.01265", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.01265", "abs": "https://arxiv.org/abs/2508.01265", "authors": ["Ali Fallahi", "Azam Bastanfard", "Amineh Amini", "Hadi Saboohi"], "title": "A Study on Enhancing User Engagement by Employing Gamified Recommender Systems", "comment": "June 2023, 21 pages, 6 figures", "summary": "Providing customized products and services in the modern business world is\none of the most efficient solutions to improve users' experience and their\nengagements with the industries. To aim, recommender systems, by producing\npersonalized recommendations, have a crucial role in the digital age. As a\nconsequence of modern improvements in the internet and online-based\ntechnologies, using gamification rules also increased in various fields. Recent\nstudies showed that considering gamification concepts in implementing\nrecommendation systems not only can become helpful to overcome the cold start\nand lack of sufficient data, moreover, can effectively improve user engagement.\nGamification can motivate individuals to have more activities on the system;\nthese interactions are valuable resources of data for recommender engines.\nUnlike the past related works about using gamified recommendation systems in\ndifferent environments or studies that particularly surveyed gamification\nstrategies or recommenders separately, this work provides a comprehensive\nreview of how gamified recommender systems can enhance user engagement in\nvarious domain applications. Furthermore, comparing different approaches for\nbuilding recommender systems is followed by in-depth surveying about\ninvestigating the gamified recommender systems, including their approaches,\nlimitations, evaluation metrics, proposed achievements, datasets, domain areas,\nand their recommendation techniques. This exhaustive analysis provides a\ndetailed picture of the topic's popularity, gaps, and unexplored regions. It is\nenvisaged that the proposed research and introduced possible future directions\nwould serve as a stepping stone for researchers interested in using gamified\nrecommender systems for user satisfaction and engagement.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u6e38\u620f\u5316\u63a8\u8350\u7cfb\u7edf\u5982\u4f55\u63d0\u5347\u7528\u6237\u53c2\u4e0e\u5ea6\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "motivation": "\u5728\u73b0\u4ee3\u5546\u4e1a\u4e16\u754c\u4e2d\uff0c\u63d0\u4f9b\u5b9a\u5236\u5316\u7684\u4ea7\u54c1\u548c\u670d\u52a1\u662f\u63d0\u9ad8\u7528\u6237\u4f53\u9a8c\u548c\u53c2\u4e0e\u5ea6\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002\u6e38\u620f\u5316\u63a8\u8350\u7cfb\u7edf\u53ef\u4ee5\u514b\u670d\u51b7\u542f\u52a8\u548c\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u6709\u6548\u63d0\u9ad8\u7528\u6237\u53c2\u4e0e\u5ea6\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u4e0d\u540c\u7684\u63a8\u8350\u7cfb\u7edf\u6784\u5efa\u65b9\u6cd5\uff0c\u6df1\u5165\u7814\u7a76\u6e38\u620f\u5316\u63a8\u8350\u7cfb\u7edf\u3002", "result": "\u5bf9\u6e38\u620f\u5316\u63a8\u8350\u7cfb\u7edf\u7684\u65b9\u6cd5\u3001\u5c40\u9650\u6027\u3001\u8bc4\u4f30\u6307\u6807\u3001\u6210\u5c31\u3001\u6570\u636e\u96c6\u3001\u9886\u57df\u548c\u63a8\u8350\u6280\u672f\u8fdb\u884c\u4e86\u8c03\u67e5\uff0c\u5168\u9762\u5206\u6790\u4e86\u8be5\u4e3b\u9898\u7684\u666e\u53ca\u7a0b\u5ea6\u3001\u5dee\u8ddd\u548c\u672a\u5f00\u53d1\u7684\u9886\u57df\u3002", "conclusion": "\u672c\u6587\u5bf9\u6e38\u620f\u5316\u63a8\u8350\u7cfb\u7edf\u5982\u4f55\u63d0\u5347\u7528\u6237\u53c2\u4e0e\u5ea6\u8fdb\u884c\u4e86\u5168\u9762\u7684\u7efc\u8ff0\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.01005", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.01005", "abs": "https://arxiv.org/abs/2508.01005", "authors": ["Yiqun Chen", "Erhan Zhang", "Lingyong Yan", "Shuaiqiang Wang", "Jizhou Huang", "Dawei Yin", "Jiaxin Mao"], "title": "MAO-ARAG: Multi-Agent Orchestration for Adaptive Retrieval-Augmented Generation", "comment": null, "summary": "In question-answering (QA) systems, Retrieval-Augmented Generation (RAG) has\nbecome pivotal in enhancing response accuracy and reducing hallucination\nissues. The architecture of RAG systems varies significantly, encompassing\nsingle-round RAG, iterative RAG, and reasoning RAG, each tailored to address\ndifferent types of queries. Due to the varying complexity of real-world\nqueries, a fixed RAG pipeline often struggles to balance performance and cost\nefficiency across different queries. To address this challenge, we propose an\nadaptive RAG framework called MAO-ARAG, which leverages multi-agent\norchestration. Our adaptive RAG is conceived as a multi-turn framework.\nSpecifically, we define multiple executor agents, representing typical RAG\nmodules such as query reformulation agents, document selection agent, and\ngeneration agents. A planner agent intelligently selects and integrates the\nappropriate agents from these executors into a suitable workflow tailored for\neach query, striving for high-quality answers while maintaining reasonable\ncosts. During each turn, the planner agent is trained using reinforcement\nlearning, guided by an outcome-based reward (F1 score) and a cost-based\npenalty, continuously improving answer quality while keeping costs within a\nreasonable range. Experiments conducted on multiple QA datasets demonstrate\nthat our approach, which dynamically plans workflows for each query, not only\nachieves high answer quality but also maintains both cost and latency within\nacceptable limits.The code of MAO-ARAG is on\nhttps://github.com/chenyiqun/Agentic-RAG.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u534f\u540c\u7684\u81ea\u9002\u5e94RAG\u6846\u67b6MAO-ARAG\uff0c\u5b83\u53ef\u4ee5\u6839\u636e\u67e5\u8be2\u52a8\u6001\u89c4\u5212\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5728\u4fdd\u8bc1\u7b54\u6848\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u5c06\u6210\u672c\u548c\u5ef6\u8fdf\u63a7\u5236\u5728\u53ef\u63a5\u53d7\u7684\u8303\u56f4\u5185\u3002", "motivation": "\u7531\u4e8e\u73b0\u5b9e\u4e16\u754c\u4e2d\u67e5\u8be2\u7684\u590d\u6742\u6027\u5404\u4e0d\u76f8\u540c\uff0c\u56fa\u5b9a\u7684RAG\u6d41\u7a0b\u901a\u5e38\u96be\u4ee5\u5728\u4e0d\u540c\u67e5\u8be2\u7684\u6027\u80fd\u548c\u6210\u672c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "method": "\u591a\u667a\u80fd\u4f53\u534f\u540c\u7684\u81ea\u9002\u5e94RAG\u6846\u67b6", "result": "\u5728\u591a\u4e2aQA\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7b54\u6848\u7684\u540c\u65f6\uff0c\u5c06\u6210\u672c\u548c\u5ef6\u8fdf\u4fdd\u6301\u5728\u53ef\u63a5\u53d7\u7684\u8303\u56f4\u5185\u3002", "conclusion": "MAO-ARAG\u901a\u8fc7\u52a8\u6001\u89c4\u5212\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5728\u4fdd\u8bc1\u7b54\u6848\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u5c06\u6210\u672c\u548c\u5ef6\u8fdf\u63a7\u5236\u5728\u53ef\u63a5\u53d7\u7684\u8303\u56f4\u5185\u3002"}}
{"id": "2508.00896", "categories": ["cs.CV", "cond-mat.mtrl-sci", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.00896", "abs": "https://arxiv.org/abs/2508.00896", "authors": ["Hoang Hai Nam Nguyen", "Minh Tien Tran", "Hoheok Kim", "Ho Won Lee"], "title": "Phase-fraction guided denoising diffusion model for augmenting multiphase steel microstructure segmentation via micrograph image-mask pair synthesis", "comment": null, "summary": "The effectiveness of machine learning in metallographic microstructure\nsegmentation is often constrained by the lack of human-annotated phase masks,\nparticularly for rare or compositionally complex morphologies within the metal\nalloy. We introduce PF-DiffSeg, a phase-fraction controlled, one-stage\ndenoising diffusion framework that jointly synthesizes microstructure images\nand their corresponding segmentation masks in a single generative trajectory to\nfurther improve segmentation accuracy. By conditioning on global phase-fraction\nvectors, augmented to represent real data distribution and emphasize minority\nclasses, our model generates compositionally valid and structurally coherent\nmicrostructure image and mask samples that improve both data diversity and\ntraining efficiency. Evaluated on the MetalDAM benchmark for additively\nmanufactured multiphase steel, our synthetic augmentation method yields notable\nimprovements in segmentation accuracy compared to standard augmentation\nstrategies especially in minority classes and further outperforms a two-stage\nmask-guided diffusion and generative adversarial network (GAN) baselines, while\nalso reducing inference time compared to conventional approach. The method\nintegrates generation and conditioning into a unified framework, offering a\nscalable solution for data augmentation in metallographic applications.", "AI": {"tldr": "PF-DiffSeg\u662f\u4e00\u79cd\u7528\u4e8e\u91d1\u76f8\u5fae\u89c2\u7ed3\u6784\u5206\u5272\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5408\u6210\u56fe\u50cf\u548c\u63a9\u6a21\u6765\u63d0\u9ad8\u5206\u5272\u7cbe\u5ea6\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u91cf\u4e0d\u8db3\u7684\u60c5\u51b5\u4e0b\u3002", "motivation": "\u91d1\u76f8\u5fae\u89c2\u7ed3\u6784\u5206\u5272\u4e2d\u673a\u5668\u5b66\u4e60\u7684\u6709\u6548\u6027\u901a\u5e38\u53d7\u5230\u7f3a\u4e4f\u4eba\u5de5\u6ce8\u91ca\u7684\u76f8\u63a9\u6a21\u7684\u9650\u5236\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u91d1\u5c5e\u5408\u91d1\u4e2d\u7a00\u6709\u6216\u6210\u5206\u590d\u6742\u7684\u5f62\u6001\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u76f8\u5206\u6570\u63a7\u5236\u7684\u5355\u9636\u6bb5\u53bb\u566a\u6269\u6563\u6846\u67b6PF-DiffSeg\uff0c\u8be5\u6846\u67b6\u8054\u5408\u5408\u6210\u5fae\u89c2\u7ed3\u6784\u56fe\u50cf\u53ca\u5176\u76f8\u5e94\u7684\u5206\u5272\u63a9\u6a21\u3002", "result": "\u5728\u589e\u6750\u5236\u9020\u7684\u591a\u76f8\u94a2MetalDAM\u57fa\u51c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u4e0e\u6807\u51c6\u589e\u5f3a\u7b56\u7565\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u5408\u6210\u589e\u5f3a\u65b9\u6cd5\u5728\u5206\u5272\u7cbe\u5ea6\u65b9\u9762\u4ea7\u751f\u4e86\u663e\u7740\u63d0\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u5c11\u6570\u7c7b\u4e2d\uff0c\u5e76\u4e14\u8fdb\u4e00\u6b65\u4f18\u4e8e\u4e24\u9636\u6bb5\u63a9\u6a21\u5f15\u5bfc\u6269\u6563\u548c\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u57fa\u7ebf\u3002", "conclusion": "PF-DiffSeg\u65b9\u6cd5\u5728\u63d0\u9ad8\u5206\u5272\u7cbe\u5ea6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u662f\u5728\u5c11\u6570\u7c7b\u4e2d\uff0c\u5e76\u4e14\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u51cf\u5c11\u4e86\u63a8\u7406\u65f6\u95f4\uff0c\u4e3a\u91d1\u76f8\u5e94\u7528\u4e2d\u7684\u6570\u636e\u589e\u5f3a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00899", "categories": ["cs.AI", "cs.CY", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00899", "abs": "https://arxiv.org/abs/2508.00899", "authors": ["Abeer Dyoub", "Ivan Letteri", "Francesca A. Lisi"], "title": "ff4ERA: A new Fuzzy Framework for Ethical Risk Assessment in AI", "comment": null, "summary": "The emergence of Symbiotic AI (SAI) introduces new challenges to ethical\ndecision-making as it deepens human-AI collaboration. As symbiosis grows, AI\nsystems pose greater ethical risks, including harm to human rights and trust.\nEthical Risk Assessment (ERA) thus becomes crucial for guiding decisions that\nminimize such risks. However, ERA is hindered by uncertainty, vagueness, and\nincomplete information, and morality itself is context-dependent and imprecise.\nThis motivates the need for a flexible, transparent, yet robust framework for\nERA. Our work supports ethical decision-making by quantitatively assessing and\nprioritizing multiple ethical risks so that artificial agents can select\nactions aligned with human values and acceptable risk levels. We introduce\nff4ERA, a fuzzy framework that integrates Fuzzy Logic, the Fuzzy Analytic\nHierarchy Process (FAHP), and Certainty Factors (CF) to quantify ethical risks\nvia an Ethical Risk Score (ERS) for each risk type. The final ERS combines the\nFAHP-derived weight, propagated CF, and risk level. The framework offers a\nrobust mathematical approach for collaborative ERA modeling and systematic,\nstep-by-step analysis. A case study confirms that ff4ERA yields\ncontext-sensitive, ethically meaningful risk scores reflecting both expert\ninput and sensor-based evidence. Risk scores vary consistently with relevant\nfactors while remaining robust to unrelated inputs. Local sensitivity analysis\nshows predictable, mostly monotonic behavior across perturbations, and global\nSobol analysis highlights the dominant influence of expert-defined weights and\ncertainty factors, validating the model design. Overall, the results\ndemonstrate ff4ERA ability to produce interpretable, traceable, and risk-aware\nethical assessments, enabling what-if analyses and guiding designers in\ncalibrating membership functions and expert judgments for reliable ethical\ndecision support.", "AI": {"tldr": "This paper introduces ff4ERA, a fuzzy framework for quantitatively assessing and prioritizing multiple ethical risks in AI systems, offering a robust mathematical approach for collaborative ERA modeling and systematic analysis.", "motivation": "Ethical Risk Assessment (ERA) is crucial for guiding decisions that minimize ethical risks posed by AI systems, but it is hindered by uncertainty, vagueness, and incomplete information, and morality itself is context-dependent and imprecise.", "method": "The paper introduces ff4ERA, a fuzzy framework that integrates Fuzzy Logic, the Fuzzy Analytic Hierarchy Process (FAHP), and Certainty Factors (CF) to quantify ethical risks via an Ethical Risk Score (ERS) for each risk type.", "result": "ff4ERA yields context-sensitive, ethically meaningful risk scores reflecting both expert input and sensor-based evidence. Risk scores vary consistently with relevant factors while remaining robust to unrelated inputs. Local sensitivity analysis shows predictable, mostly monotonic behavior across perturbations, and global Sobol analysis highlights the dominant influence of expert-defined weights and certainty factors.", "conclusion": "The ff4ERA framework produces interpretable, traceable, and risk-aware ethical assessments, enabling what-if analyses and guiding designers in calibrating membership functions and expert judgments for reliable ethical decision support."}}
{"id": "2508.00869", "categories": ["cs.LG", "cs.ET", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.00869", "abs": "https://arxiv.org/abs/2508.00869", "authors": ["Dmitriy Kashitsyn", "Dmitriy Shabanov"], "title": "Discrete approach to machine learning", "comment": "preprint, 52 pages, 37 figures", "summary": "The article explores an encoding and structural information processing\napproach using sparse bit vectors and fixed-length linear vectors. The\nfollowing are presented: a discrete method of speculative stochastic\ndimensionality reduction of multidimensional code and linear spaces with linear\nasymptotic complexity; a geometric method for obtaining discrete embeddings of\nan organised code space that reflect the internal structure of a given\nmodality. The structure and properties of a code space are investigated using\nthree modalities as examples: morphology of Russian and English languages, and\nimmunohistochemical markers. Parallels are drawn between the resulting map of\nthe code space layout and so-called pinwheels appearing on the mammalian\nneocortex. A cautious assumption is made about similarities between neocortex\norganisation and processes happening in our models.", "AI": {"tldr": "The paper explores encoding and structural information processing using sparse bit vectors, drawing parallels between code space layouts and mammalian neocortex organization.", "motivation": "The article explores an encoding and structural information processing approach.", "method": "The paper presents a discrete method of speculative stochastic dimensionality reduction and a geometric method for obtaining discrete embeddings of an organised code space.", "result": "The structure and properties of a code space are investigated using three modalities as examples: morphology of Russian and English languages, and immunohistochemical markers. Parallels are drawn between the resulting map of the code space layout and so-called pinwheels appearing on the mammalian neocortex.", "conclusion": "The paper cautiously assumes similarities between neocortex organisation and processes happening in their models."}}
{"id": "2508.01871", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2508.01871", "abs": "https://arxiv.org/abs/2508.01871", "authors": ["Yuanyuan Liang", "Lei Pan", "Tingyu Xie", "Yunshi Lan", "Weining Qian"], "title": "Multi-turn Natural Language to Graph Query Language Translation", "comment": "21 pages", "summary": "In recent years, research on transforming natural language into graph query\nlanguage (NL2GQL) has been increasing. Most existing methods focus on\nsingle-turn transformation from NL to GQL. In practical applications, user\ninteractions with graph databases are typically multi-turn, dynamic, and\ncontext-dependent. While single-turn methods can handle straightforward\nqueries, more complex scenarios often require users to iteratively adjust their\nqueries, investigate the connections between entities, or request additional\ndetails across multiple dialogue turns. Research focused on single-turn\nconversion fails to effectively address multi-turn dialogues and complex\ncontext dependencies. Additionally, the scarcity of high-quality multi-turn\nNL2GQL datasets further hinders the progress of this field. To address this\nchallenge, we propose an automated method for constructing multi-turn NL2GQL\ndatasets based on Large Language Models (LLMs) , and apply this method to\ndevelop the MTGQL dataset, which is constructed from a financial market graph\ndatabase and will be publicly released for future research. Moreover, we\npropose three types of baseline methods to assess the effectiveness of\nmulti-turn NL2GQL translation, thereby laying a solid foundation for future\nresearch.", "AI": {"tldr": "This paper introduces a method for creating multi-turn NL2GQL datasets using LLMs to address the limitations of single-turn methods and the lack of suitable datasets. They also propose baseline methods for evaluating multi-turn NL2GQL translation.", "motivation": "Single-turn methods can handle straightforward queries, more complex scenarios often require users to iteratively adjust their queries, investigate the connections between entities, or request additional details across multiple dialogue turns. Research focused on single-turn conversion fails to effectively address multi-turn dialogues and complex context dependencies. Additionally, the scarcity of high-quality multi-turn NL2GQL datasets further hinders the progress of this field.", "method": "We propose an automated method for constructing multi-turn NL2GQL datasets based on Large Language Models (LLMs) , and apply this method to develop the MTGQL dataset, which is constructed from a financial market graph database and will be publicly released for future research.", "result": "To address this challenge, we propose an automated method for constructing multi-turn NL2GQL datasets based on Large Language Models (LLMs)", "conclusion": "We propose three types of baseline methods to assess the effectiveness of multi-turn NL2GQL translation, thereby laying a solid foundation for future research."}}
{"id": "2508.01375", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.01375", "abs": "https://arxiv.org/abs/2508.01375", "authors": ["Yining Yao", "Ziwei Li", "Shuwen Xiao", "Boya Du", "Jialin Zhu", "Junjun Zheng", "Xiangheng Kong", "Yuning Jiang"], "title": "SaviorRec: Semantic-Behavior Alignment for Cold-Start Recommendation", "comment": null, "summary": "In recommendation systems, predicting Click-Through Rate (CTR) is crucial for\naccurately matching users with items. To improve recommendation performance for\ncold-start and long-tail items, recent studies focus on leveraging item\nmultimodal features to model users' interests. However, obtaining multimodal\nrepresentations for items relies on complex pre-trained encoders, which incurs\nunacceptable computation cost to train jointly with downstream ranking models.\nTherefore, it is important to maintain alignment between semantic and behavior\nspace in a lightweight way.\n  To address these challenges, we propose a Semantic-Behavior Alignment for\nCold-start Recommendation framework, which mainly focuses on utilizing\nmultimodal representations that align with the user behavior space to predict\nCTR. First, we leverage domain-specific knowledge to train a multimodal encoder\nto generate behavior-aware semantic representations. Second, we use residual\nquantized semantic ID to dynamically bridge the gap between multimodal\nrepresentations and the ranking model, facilitating the continuous\nsemantic-behavior alignment. We conduct our offline and online experiments on\nthe Taobao, one of the world's largest e-commerce platforms, and have achieved\nan increase of 0.83% in offline AUC, 13.21% clicks increase and 13.44% orders\nincrease in the online A/B test, emphasizing the efficacy of our method.", "AI": {"tldr": "This paper introduces a lightweight Semantic-Behavior Alignment framework to improve CTR prediction for cold-start items by aligning multimodal representations with user behavior space, achieving significant performance gains on Taobao.", "motivation": "Improving recommendation performance for cold-start and long-tail items by leveraging item multimodal features is challenging due to the high computational cost of training complex pre-trained encoders. Maintaining alignment between semantic and behavior space in a lightweight way is important.", "method": "A Semantic-Behavior Alignment framework is proposed, utilizing domain-specific knowledge to train a multimodal encoder and residual quantized semantic ID to bridge the gap between multimodal representations and the ranking model.", "result": "Offline AUC increased by 0.83%, online clicks increased by 13.21%, and orders increased by 13.44% in the A/B test on Taobao.", "conclusion": "The proposed Semantic-Behavior Alignment framework improves CTR prediction, especially for cold-start items, as demonstrated by offline AUC increase and online A/B testing gains on Taobao."}}
{"id": "2508.01006", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01006", "abs": "https://arxiv.org/abs/2508.01006", "authors": ["Farah Adeeba", "Brian Dillon", "Hassan Sajjad", "Rajesh Bhatt"], "title": "UrBLiMP: A Benchmark for Evaluating the Linguistic Competence of Large Language Models in Urdu", "comment": null, "summary": "Multilingual Large Language Models (LLMs) have shown remarkable performance\nacross various languages; however, they often include significantly less data\nfor low-resource languages such as Urdu compared to high-resource languages\nlike English. To assess the linguistic knowledge of LLMs in Urdu, we present\nthe Urdu Benchmark of Linguistic Minimal Pairs (UrBLiMP) i.e. pairs of\nminimally different sentences that contrast in grammatical acceptability.\nUrBLiMP comprises 5,696 minimal pairs targeting ten core syntactic phenomena,\ncarefully curated using the Urdu Treebank and diverse Urdu text corpora. A\nhuman evaluation of UrBLiMP annotations yielded a 96.10% inter-annotator\nagreement, confirming the reliability of the dataset. We evaluate twenty\nmultilingual LLMs on UrBLiMP, revealing significant variation in performance\nacross linguistic phenomena. While LLaMA-3-70B achieves the highest average\naccuracy (94.73%), its performance is statistically comparable to other top\nmodels such as Gemma-3-27B-PT. These findings highlight both the potential and\nthe limitations of current multilingual LLMs in capturing fine-grained\nsyntactic knowledge in low-resource languages.", "AI": {"tldr": "Evaluate multilingual LLMs on Urdu using UrBLiMP, find LLaMA-3-70B performs best but is comparable to Gemma-3-27B-PT, highlighting potential and limitations in low-resource language syntax.", "motivation": "Multilingual LLMs often include significantly less data for low-resource languages such as Urdu compared to high-resource languages like English. To assess the linguistic knowledge of LLMs in Urdu", "method": "present the Urdu Benchmark of Linguistic Minimal Pairs (UrBLiMP) and evaluate twenty multilingual LLMs on it", "result": "LLaMA-3-70B achieves the highest average accuracy (94.73%), but its performance is statistically comparable to other top models such as Gemma-3-27B-PT", "conclusion": "current multilingual LLMs have both potential and limitations in capturing fine-grained syntactic knowledge in low-resource languages"}}
{"id": "2508.00898", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00898", "abs": "https://arxiv.org/abs/2508.00898", "authors": ["Jose M. S\u00e1nchez Vel\u00e1zquez", "Mingbo Cai", "Andrew Coney", "\u00c1lvaro J. Garc\u00eda- Tejedor", "Alberto Nogales"], "title": "Benefits of Feature Extraction and Temporal Sequence Analysis for Video Frame Prediction: An Evaluation of Hybrid Deep Learning Models", "comment": "2 Figures, 12 Tables, 21 pages", "summary": "In recent years, advances in Artificial Intelligence have significantly\nimpacted computer science, particularly in the field of computer vision,\nenabling solutions to complex problems such as video frame prediction. Video\nframe prediction has critical applications in weather forecasting or autonomous\nsystems and can provide technical improvements, such as video compression and\nstreaming. Among Artificial Intelligence methods, Deep Learning has emerged as\nhighly effective for solving vision-related tasks, although current frame\nprediction models still have room for enhancement. This paper evaluates several\nhybrid deep learning approaches that combine the feature extraction\ncapabilities of autoencoders with temporal sequence modelling using Recurrent\nNeural Networks (RNNs), 3D Convolutional Neural Networks (3D CNNs), and related\narchitectures. The proposed solutions were rigorously evaluated on three\ndatasets that differ in terms of synthetic versus real-world scenarios and\ngrayscale versus color imagery. Results demonstrate that the approaches perform\nwell, with SSIM metrics increasing from 0.69 to 0.82, indicating that hybrid\nmodels utilizing 3DCNNs and ConvLSTMs are the most effective, and greyscale\nvideos with real data are the easiest to predict.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u51e0\u79cd\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u7ed3\u5408\u4e86\u81ea\u7f16\u7801\u5668\u7684\u7279\u5f81\u63d0\u53d6\u529f\u80fd\u4e0e\u4f7f\u7528\u5faa\u73af\u795e\u7ecf\u7f51\u7edc (RNN)\u30013D \u5377\u79ef\u795e\u7ecf\u7f51\u7edc (3D CNN) \u548c\u76f8\u5173\u67b6\u6784\u7684\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0c\u4eba\u5de5\u667a\u80fd\u7684\u8fdb\u6b65\u5bf9\u8ba1\u7b97\u673a\u79d1\u5b66\u4ea7\u751f\u4e86\u91cd\u5927\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\uff0c\u4ece\u800c\u80fd\u591f\u89e3\u51b3\u8bf8\u5982\u89c6\u9891\u5e27\u9884\u6d4b\u4e4b\u7c7b\u7684\u590d\u6742\u95ee\u9898\u3002\u89c6\u9891\u5e27\u9884\u6d4b\u5728\u5929\u6c14\u9884\u62a5\u6216\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u5177\u6709\u5173\u952e\u5e94\u7528\uff0c\u5e76\u4e14\u53ef\u4ee5\u63d0\u4f9b\u8bf8\u5982\u89c6\u9891\u538b\u7f29\u548c\u6d41\u4f20\u8f93\u4e4b\u7c7b\u7684\u6280\u672f\u6539\u8fdb\u3002\u5728\u4eba\u5de5\u667a\u80fd\u65b9\u6cd5\u4e2d\uff0c\u6df1\u5ea6\u5b66\u4e60\u5df2\u6210\u4e3a\u89e3\u51b3\u4e0e\u89c6\u89c9\u76f8\u5173\u7684\u4efb\u52a1\u7684\u975e\u5e38\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5c3d\u7ba1\u5f53\u524d\u7684\u5e27\u9884\u6d4b\u6a21\u578b\u4ecd\u6709\u6539\u8fdb\u7684\u7a7a\u95f4\u3002", "method": "\u7ed3\u5408\u4e86\u81ea\u7f16\u7801\u5668\u7684\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u4e0e\u4f7f\u7528\u5faa\u73af\u795e\u7ecf\u7f51\u7edc (RNN)\u30013D \u5377\u79ef\u795e\u7ecf\u7f51\u7edc (3D CNN) \u548c\u76f8\u5173\u67b6\u6784\u7684\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6548\u679c\u826f\u597d\uff0cSSIM \u6307\u6807\u4ece 0.69 \u589e\u52a0\u5230 0.82\u3002", "conclusion": "\u6df7\u5408\u6a21\u578b\u8868\u73b0\u826f\u597d\uff0c\u5229\u7528 3DCNN \u548c ConvLSTM \u7684\u6df7\u5408\u6a21\u578b\u6700\u6709\u6548\uff0c\u5e76\u4e14\u5305\u542b\u771f\u5b9e\u6570\u636e\u7684\u7070\u5ea6\u89c6\u9891\u6700\u5bb9\u6613\u9884\u6d4b\u3002"}}
{"id": "2508.00902", "categories": ["cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.00902", "abs": "https://arxiv.org/abs/2508.00902", "authors": ["Kenneth Payne"], "title": "An analysis of AI Decision under Risk: Prospect theory emerges in Large Language Models", "comment": "26 pages, 2 figures, 9 tables, 2 appendices", "summary": "Judgment of risk is key to decision-making under uncertainty. As Daniel\nKahneman and Amos Tversky famously discovered, humans do so in a distinctive\nway that departs from mathematical rationalism. Specifically, they demonstrated\nexperimentally that humans accept more risk when they feel themselves at risk\nof losing something than when they might gain. I report the first tests of\nKahneman and Tversky's landmark 'prospect theory' with Large Language Models,\nincluding today's state of the art chain-of-thought 'reasoners'.\n  In common with humans, I find that prospect theory often anticipates how\nthese models approach risky decisions across a range of scenarios. I also\ndemonstrate that context is key to explaining much of the variance in risk\nappetite. The 'frame' through which risk is apprehended appears to be embedded\nwithin the language of the scenarios tackled by the models. Specifically, I\nfind that military scenarios generate far larger 'framing effects' than do\ncivilian settings, ceteris paribus. My research suggests, therefore, that\nlanguage models the world, capturing our human heuristics and biases. But also\nthat these biases are uneven - the idea of a 'frame' is richer than simple\ngains and losses. Wittgenstein's notion of 'language games' explains the\ncontingent, localised biases activated by these scenarios. Finally, I use my\nfindings to reframe the ongoing debate about reasoning and memorisation in\nLLMs.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u98ce\u9669\u51b3\u7b56\u4e2d\u8868\u73b0\u51fa\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u524d\u666f\u7406\u8bba\u504f\u89c1\uff0c\u4e14\u53d7\u60c5\u5883\u5f71\u54cd\u3002", "motivation": "\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\uff0c\u98ce\u9669\u5224\u65ad\u662f\u51b3\u7b56\u7684\u5173\u952e\u3002\u4eba\u7c7b\u4ee5\u4e00\u79cd\u6709\u522b\u4e8e\u6570\u5b66\u7406\u6027\u4e3b\u4e49\u7684\u72ec\u7279\u65b9\u5f0f\u8fdb\u884c\u98ce\u9669\u5224\u65ad\u3002\u4ed6\u4eec\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5f53\u4eba\u7c7b\u89c9\u5f97\u81ea\u5df1\u6709\u5931\u53bb\u67d0\u4e9b\u4e1c\u897f\u7684\u98ce\u9669\u65f6\uff0c\u6bd4\u4ed6\u4eec\u53ef\u80fd\u83b7\u5f97\u6536\u76ca\u65f6\uff0c\u4f1a\u627f\u62c5\u66f4\u591a\u7684\u98ce\u9669\u3002", "method": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5305\u62ec\u5f53\u4eca\u6700\u5148\u8fdb\u7684\u601d\u7ef4\u94fe\u201c\u63a8\u7406\u5668\u201d\u6d4b\u8bd5\u5361\u5c3c\u66fc\u548c\u7279\u6c83\u65af\u57fa\u7684\u201c\u524d\u666f\u7406\u8bba\u201d\u3002", "result": "\u524d\u666f\u7406\u8bba\u53ef\u4ee5\u9884\u6d4b\u8fd9\u4e9b\u6a21\u578b\u5728\u4e00\u7cfb\u5217\u60c5\u666f\u4e2d\u5982\u4f55\u8fdb\u884c\u98ce\u9669\u51b3\u7b56\u3002\u519b\u4e8b\u573a\u666f\u6bd4\u6c11\u7528\u73af\u5883\u4ea7\u751f\u66f4\u5927\u7684\u201c\u6846\u67b6\u6548\u5e94\u201d\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u6355\u6349\u5230\u4eba\u7c7b\u7684\u542f\u53d1\u5f0f\u548c\u504f\u89c1\uff0c\u4f46\u8fd9\u4e9b\u504f\u89c1\u662f\u4e0d\u5747\u8861\u7684\uff0c\u60c5\u5883\u4f1a\u6fc0\u6d3b\u8fd9\u4e9b\u5c40\u90e8\u504f\u89c1\u3002"}}
{"id": "2508.00876", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.00876", "abs": "https://arxiv.org/abs/2508.00876", "authors": ["Bakhtiyar Mammadli", "Casim Yazici", "Muhammed G\u00fcrb\u00fcz", "\u0130rfan Kocaman", "F. Javier Dominguez-Gutierrez", "Fatih Mehmet \u00d6zkal"], "title": "A Data-Driven Machine Learning Approach for Predicting Axial Load Capacity in Steel Storage Rack Columns", "comment": null, "summary": "In this study, we present a machine learning (ML) framework to predict the\naxial load-bearing capacity, (kN), of cold-formed steel structural members. The\nmethodology emphasizes robust model selection and interpretability, addressing\nthe limitations of traditional analytical approaches in capturing the\nnonlinearities and geometrical complexities inherent to buckling behavior. The\ndataset, comprising key geometric and mechanical parameters of steel columns,\nwas curated with appropriate pre-processing steps including removal of\nnon-informative identifiers and imputation of missing values. A comprehensive\nsuite of regression algorithms, ranging from linear models to kernel-based\nregressors and ensemble tree methods was evaluated. Among these, Gradient\nBoosting Regression exhibited superior predictive performance across multiple\nmetrics, including the coefficient of determination (R2), root mean squared\nerror (RMSE), and mean absolute error (MAE), and was consequently selected as\nthe final model. Model interpretability was addressed using SHapley Additive\nexPlanations (SHAP), enabling insight into the relative importance and\ninteraction of input features influencing the predicted axial capacity. To\nfacilitate practical deployment, the model was integrated into an interactive,\nPython-based web interface via Streamlit. This tool allows end-users-such as\nstructural engineers and designers, to input design parameters manually or\nthrough CSV upload, and to obtain real-time predictions of axial load capacity\nwithout the need for programming expertise. Applied to the context of steel\nstorage rack columns, the framework demonstrates how data-driven tools can\nenhance design safety, streamline validation workflows, and inform\ndecision-making in structural applications where buckling is a critical failure\nmode", "AI": {"tldr": "A machine learning framework using Gradient Boosting Regression was developed to predict the axial load-bearing capacity of steel columns, offering a practical tool for structural engineers.", "motivation": "Traditional analytical approaches have limitations in capturing the nonlinearities and geometrical complexities inherent to buckling behavior.", "method": "A machine learning framework was used to predict the axial load-bearing capacity of cold-formed steel structural members. Gradient Boosting Regression was selected as the final model.", "result": "Gradient Boosting Regression exhibited superior predictive performance across multiple metrics.", "conclusion": "The Gradient Boosting Regression model was integrated into a Python-based web interface for real-time prediction of axial load capacity."}}
{"id": "2508.02091", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2508.02091", "abs": "https://arxiv.org/abs/2508.02091", "authors": ["Xiaoya Li", "Xiaofei Sun", "Albert Wang", "Chris Shum", "Jiwei Li"], "title": "CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search", "comment": "Preprint Version", "summary": "Approximate nearest-neighbor search (ANNS) algorithms have become\nincreasingly critical for recent AI applications, particularly in\nretrieval-augmented generation (RAG) and agent-based LLM applications. In this\npaper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS\noptimization as a reinforcement learning problem where execution speed serves\nas the reward signal. This approach enables the automatic generation of\nprogressively faster ANNS implementations while maintaining accuracy\nconstraints. Our experimental evaluation demonstrates CRINN's effectiveness\nacross six widely-used NNS benchmark datasets. When compared against\nstate-of-the-art open-source ANNS algorithms, CRINN achieves best performance\non three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and\nGloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean\nand GloVe-25-angular). The implications of CRINN's success reach well beyond\nANNS optimization: It validates that LLMs augmented with reinforcement learning\ncan function as an effective tool for automating sophisticated algorithmic\noptimizations that demand specialized knowledge and labor-intensive manual\nrefinement.Code can be found at https://github.com/deepreinforce-ai/CRINN", "AI": {"tldr": "CRINN is a new ANNS paradigm using reinforcement learning to optimize execution speed while maintaining accuracy, achieving state-of-the-art results on several benchmark datasets.", "motivation": "ANNS algorithms are critical for AI applications like RAG and agent-based LLM applications.", "method": "Treats ANNS optimization as a reinforcement learning problem.", "result": "CRINN achieves best or tied for first place performance on five widely-used NNS benchmark datasets compared to state-of-the-art open-source ANNS algorithms.", "conclusion": "CRINN demonstrates that reinforcement learning-augmented LLMs can automate sophisticated algorithmic optimizations."}}
{"id": "2508.01502", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.01502", "abs": "https://arxiv.org/abs/2508.01502", "authors": ["Ali Fallahi", "Amineh Amini", "Azam Bastanfard", "Hadi Saboohi"], "title": "Req-Rec: Enhancing Requirements Elicitation for Increasing Stakeholder's Satisfaction Using a Collaborative Filtering Based Recommender System", "comment": "March 2023, 28 pages, 7 figures", "summary": "The success or failure of a project is highly related to recognizing the\nright stakeholders and accurately finding and discovering their requirements.\nHowever, choosing the proper elicitation technique was always a considerable\nchallenge for efficient requirement engineering. As a consequence of the swift\nimprovement of digital technologies since the past decade, recommender systems\nhave become an efficient channel for making a deeply personalized interactive\ncommunication with stakeholders. In this research, a new method, called the\nReq-Rec (Requirements Recommender), is proposed. It is a hybrid recommender\nsystem based on the collaborative filtering approach and the repertory grid\ntechnique as the core component. The primary goal of Req-Rec is to increase\nstakeholder satisfaction by assisting them in the requirement elicitation\nphase. Based on the results, the method efficiently could overcome weaknesses\nof common requirement elicitation techniques, such as time limitation,\nlocation-based restrictions, and bias in requirements' elicitation process.\nTherefore, recommending related requirements assists stakeholders in becoming\nmore aware of different aspects of the project.", "AI": {"tldr": "A new hybrid recommender system, Req-Rec, is proposed to increase stakeholder satisfaction by assisting them in the requirement elicitation phase.", "motivation": "Choosing the proper elicitation technique was always a considerable challenge for efficient requirement engineering. Recommender systems have become an efficient channel for making a deeply personalized interactive communication with stakeholders.", "method": "A hybrid recommender system based on the collaborative filtering approach and the repertory grid technique.", "result": "The method efficiently could overcome weaknesses of common requirement elicitation techniques, such as time limitation, location-based restrictions, and bias in requirements' elicitation process.", "conclusion": "The proposed Req-Rec method efficiently overcomes weaknesses of common requirement elicitation techniques and assists stakeholders in becoming more aware of different aspects of the project."}}
{"id": "2508.01096", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.01096", "abs": "https://arxiv.org/abs/2508.01096", "authors": ["Michael Farag", "Patrick Halina", "Andrey Zaytsev", "Alekhya Munagala", "Imtihan Ahmed", "Junhao Wang"], "title": "Cross-Domain Web Information Extraction at Pinterest", "comment": null, "summary": "The internet offers a massive repository of unstructured information, but\nit's a significant challenge to convert this into a structured format. At\nPinterest, the ability to accurately extract structured product data from\ne-commerce websites is essential to enhance user experiences and improve\ncontent distribution. In this paper, we present Pinterest's system for\nattribute extraction, which achieves remarkable accuracy and scalability at a\nmanageable cost. Our approach leverages a novel webpage representation that\ncombines structural, visual, and text modalities into a compact form,\noptimizing it for small model learning. This representation captures each\nvisible HTML node with its text, style and layout information. We show how this\nallows simple models such as eXtreme Gradient Boosting (XGBoost) to extract\nattributes more accurately than much more complex Large Language Models (LLMs)\nsuch as Generative Pre-trained Transformer (GPT). Our results demonstrate a\nsystem that is highly scalable, processing over 1,000 URLs per second, while\nbeing 1000 times more cost-effective than the cheapest GPT alternatives.", "AI": {"tldr": "Pinterest \u7684\u5c5e\u6027\u63d0\u53d6\u7cfb\u7edf\u5229\u7528\u4e00\u79cd\u65b0\u9896\u7684\u7f51\u9875\u8868\u793a\uff0c\u8be5\u8868\u793a\u5c06\u7ed3\u6784\u3001\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u7ec4\u5408\u6210\u4e00\u79cd\u7d27\u51d1\u7684\u5f62\u5f0f\uff0c\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u76f8\u6bd4\uff0c\u5b83\u53ef\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u4e14\u6210\u672c\u6548\u76ca\u66f4\u9ad8\u3002", "motivation": "\u4ece\u7535\u5b50\u5546\u52a1\u7f51\u7ad9\u51c6\u786e\u63d0\u53d6\u7ed3\u6784\u5316\u4ea7\u54c1\u6570\u636e\u5bf9\u4e8e\u589e\u5f3a\u7528\u6237\u4f53\u9a8c\u548c\u6539\u5584\u5185\u5bb9\u5206\u53d1\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5229\u7528\u4e00\u79cd\u65b0\u9896\u7684\u7f51\u9875\u8868\u793a\uff0c\u8be5\u8868\u793a\u5c06\u7ed3\u6784\u3001\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u7ec4\u5408\u6210\u4e00\u79cd\u7d27\u51d1\u7684\u5f62\u5f0f\uff0c\u9488\u5bf9\u5c0f\u578b\u6a21\u578b\u5b66\u4e60\u8fdb\u884c\u4e86\u4f18\u5316\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u6bd4\u66f4\u590d\u6742\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM)\uff08\u5982\u751f\u6210\u578b\u9884\u8bad\u7ec3 Transformer (GPT)\uff09\u66f4\u51c6\u786e\u5730\u63d0\u53d6\u5c5e\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5177\u6709\u9ad8\u5ea6\u7684\u53ef\u6269\u5c55\u6027\uff0c\u6bcf\u79d2\u53ef\u5904\u7406\u8d85\u8fc7 1,000 \u4e2a URL\uff0c\u540c\u65f6\u6bd4\u6700\u4fbf\u5b9c\u7684 GPT \u66ff\u4ee3\u65b9\u6848\u4fbf\u5b9c 1000 \u500d\u3002"}}
{"id": "2508.00913", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00913", "abs": "https://arxiv.org/abs/2508.00913", "authors": ["Mohammad Mohammadi", "Ziyi Wu", "Igor Gilitschenski"], "title": "TESPEC: Temporally-Enhanced Self-Supervised Pretraining for Event Cameras", "comment": "Accepted at IEEE/CVF International Conference on Computer Vision\n  (ICCV) 2025", "summary": "Long-term temporal information is crucial for event-based perception tasks,\nas raw events only encode pixel brightness changes. Recent works show that when\ntrained from scratch, recurrent models achieve better results than feedforward\nmodels in these tasks. However, when leveraging self-supervised pre-trained\nweights, feedforward models can outperform their recurrent counterparts.\nCurrent self-supervised learning (SSL) methods for event-based pre-training\nlargely mimic RGB image-based approaches. They pre-train feedforward models on\nraw events within a short time interval, ignoring the temporal information of\nevents. In this work, we introduce TESPEC, a self-supervised pre-training\nframework tailored for learning spatio-temporal information. TESPEC is\nwell-suited for recurrent models, as it is the first framework to leverage long\nevent sequences during pre-training. TESPEC employs the masked image modeling\nparadigm with a new reconstruction target. We design a novel method to\naccumulate events into pseudo grayscale videos containing high-level semantic\ninformation about the underlying scene, which is robust to sensor noise and\nreduces motion blur. Reconstructing this target thus requires the model to\nreason about long-term history of events. Extensive experiments demonstrate our\nstate-of-the-art results in downstream tasks, including object detection,\nsemantic segmentation, and monocular depth estimation. Project webpage:\nhttps://mhdmohammadi.github.io/TESPEC_webpage.", "AI": {"tldr": "TESPEC\u662f\u4e00\u79cd\u7528\u4e8e\u5b66\u4e60\u65f6\u7a7a\u4fe1\u606f\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u957f\u4e8b\u4ef6\u5e8f\u5217\u548c\u4f2a\u7070\u5ea6\u89c6\u9891\u91cd\u5efa\u76ee\u6807\uff0c\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u4e8b\u4ef6\u7684\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u65b9\u6cd5\u5f88\u5927\u7a0b\u5ea6\u4e0a\u6a21\u4eff\u4e86\u57fa\u4e8eRGB\u56fe\u50cf\u7684\u65b9\u6cd5\uff0c\u5728\u77ed\u65f6\u95f4\u5185\u5bf9\u539f\u59cb\u4e8b\u4ef6\u8fdb\u884c\u524d\u9988\u6a21\u578b\u7684\u9884\u8bad\u7ec3\uff0c\u5ffd\u7565\u4e86\u4e8b\u4ef6\u7684\u65f6\u95f4\u4fe1\u606f\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51faTESPEC\uff0c\u4e00\u79cd\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u8bbe\u8ba1\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\u5c06\u4e8b\u4ef6\u7d2f\u79ef\u6210\u5305\u542b\u5e95\u5c42\u573a\u666f\u9ad8\u7ea7\u8bed\u4e49\u4fe1\u606f\u7684\u4f2a\u7070\u5ea6\u89c6\u9891\uff0c\u8be5\u65b9\u6cd5\u5bf9\u4f20\u611f\u5668\u566a\u58f0\u5177\u6709\u9c81\u68d2\u6027\u5e76\u51cf\u5c11\u4e86\u8fd0\u52a8\u6a21\u7cca\u3002\u7136\u540e\uff0c\u4f7f\u7528\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\u8303\u4f8b\u548c\u4e00\u4e2a\u65b0\u7684\u91cd\u5efa\u76ee\u6807\u8fdb\u884c\u9884\u8bad\u7ec3\u3002", "result": "TESPEC\u662f\u7b2c\u4e00\u4e2a\u5728\u9884\u8bad\u7ec3\u671f\u95f4\u5229\u7528\u957f\u4e8b\u4ef6\u5e8f\u5217\u7684\u6846\u67b6\uff0c\u5e76\u4e14\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "TESPEC\u5728\u4e0b\u6e38\u4efb\u52a1\uff08\u5305\u62ec\u5bf9\u8c61\u68c0\u6d4b\u3001\u8bed\u4e49\u5206\u5272\u548c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\uff09\u4e2d\u8868\u73b0\u51fa\u6700\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2508.00914", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00914", "abs": "https://arxiv.org/abs/2508.00914", "authors": ["Dominic Simon", "Rickard Ewetz"], "title": "Knowledge Editing for Multi-Hop Question Answering Using Semantic Analysis", "comment": "14 pages, 15 figures, pre-print of paper accepted to IJCAI 2025", "summary": "Large Language Models (LLMs) require lightweight avenues of updating stored\ninformation that has fallen out of date. Knowledge Editing (KE) approaches have\nbeen successful in updating model knowledge for simple factual queries but\nstruggle with handling tasks that require compositional reasoning such as\nmulti-hop question answering (MQA). We observe that existing knowledge editors\nleverage decompositional techniques that result in illogical reasoning\nprocesses. In this paper, we propose a knowledge editor for MQA based on\nsemantic analysis called CHECK. Our framework is based on insights from an\nanalogy between compilers and reasoning using LLMs. Similar to how source code\nis first compiled before being executed, we propose to semantically analyze\nreasoning chains before executing the chains to answer questions. Reasoning\nchains with semantic errors are revised to ensure consistency through logic\noptimization and re-prompting the LLM model at a higher temperature. We\nevaluate the effectiveness of CHECK against five state-of-the-art frameworks on\nfour datasets and achieve an average 22.8% improved MQA accuracy.", "AI": {"tldr": "This paper presents CHECK, a knowledge editor that uses semantic analysis to improve the accuracy of multi-hop question answering in large language models.", "motivation": "Existing knowledge editors struggle with handling tasks that require compositional reasoning such as multi-hop question answering (MQA). Existing knowledge editors leverage decompositional techniques that result in illogical reasoning processes.", "method": "The proposed framework semantically analyzes reasoning chains before execution, revising chains with semantic errors to ensure consistency through logic optimization and re-prompting the LLM model at a higher temperature.", "result": "CHECK achieves an average 22.8% improved MQA accuracy against five state-of-the-art frameworks on four datasets.", "conclusion": "The paper introduces CHECK, a knowledge editor for multi-hop question answering (MQA) based on semantic analysis, which improves MQA accuracy by 22.8% compared to state-of-the-art frameworks."}}
{"id": "2508.00877", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00877", "abs": "https://arxiv.org/abs/2508.00877", "authors": ["Chao Yan", "Babak Mafakheri"], "title": "Satellite Connectivity Prediction for Fast-Moving Platforms", "comment": null, "summary": "Satellite connectivity is gaining increased attention as the demand for\nseamless internet access, especially in transportation and remote areas,\ncontinues to grow. For fast-moving objects such as aircraft, vehicles, or\ntrains, satellite connectivity is critical due to their mobility and frequent\npresence in areas without terrestrial coverage. Maintaining reliable\nconnectivity in these cases requires frequent switching between satellite\nbeams, constellations, or orbits. To enhance user experience and address\nchallenges like long switching times, Machine Learning (ML) algorithms can\nanalyze historical connectivity data and predict network quality at specific\nlocations. This allows for proactive measures, such as network switching before\nconnectivity issues arise. In this paper, we analyze a real dataset of\ncommunication between a Geostationary Orbit (GEO) satellite and aircraft over\nmultiple flights, using ML to predict signal quality. Our prediction model\nachieved an F1 score of 0.97 on the test data, demonstrating the accuracy of\nmachine learning in predicting signal quality during flight. By enabling\nseamless broadband service, including roaming between different satellite\nconstellations and providers, our model addresses the need for real-time\npredictions of signal quality. This approach can further be adapted to automate\nsatellite and beam-switching mechanisms to improve overall communication\nefficiency. The model can also be retrained and applied to any moving object\nwith satellite connectivity, using customized datasets, including connected\nvehicles and trains.", "AI": {"tldr": "\u5229\u7528\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u536b\u661f\u4fe1\u53f7\u8d28\u91cf\uff0c\u5728\u98de\u884c\u4e2d\u5b9e\u73b0\u4e86 0.97 \u7684 F1 \u5206\u6570\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u65e0\u7f1d\u5bbd\u5e26\u670d\u52a1\u3002", "motivation": "\u968f\u7740\u5bf9\u65e0\u7f1d\u4e92\u8054\u7f51\u8bbf\u95ee\u7684\u9700\u6c42\u4e0d\u65ad\u589e\u957f\uff0c\u536b\u661f\u8fde\u63a5\u8d8a\u6765\u8d8a\u53d7\u5230\u5173\u6ce8\uff0c\u5c24\u5176\u662f\u5728\u4ea4\u901a\u8fd0\u8f93\u548c\u504f\u8fdc\u5730\u533a\u3002\u5bf9\u4e8e\u98de\u673a\u3001\u8f66\u8f86\u6216\u706b\u8f66\u7b49\u5feb\u901f\u79fb\u52a8\u7684\u7269\u4f53\uff0c\u536b\u661f\u8fde\u63a5\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u5b83\u4eec\u5177\u6709\u79fb\u52a8\u6027\u5e76\u4e14\u7ecf\u5e38\u51fa\u73b0\u5728\u6ca1\u6709\u5730\u9762\u8986\u76d6\u7684\u533a\u57df\u3002", "method": "\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u5206\u6790 GEO \u536b\u661f\u548c\u98de\u673a\u5728\u591a\u6b21\u98de\u884c\u4e2d\u7684\u901a\u4fe1\u771f\u5b9e\u6570\u636e\u96c6\uff0c\u4ee5\u9884\u6d4b\u4fe1\u53f7\u8d28\u91cf\u3002", "result": "\u6211\u4eec\u7684\u9884\u6d4b\u6a21\u578b\u5728\u6d4b\u8bd5\u6570\u636e\u4e0a\u5b9e\u73b0\u4e86 0.97 \u7684 F1 \u5206\u6570\uff0c\u8bc1\u660e\u4e86\u673a\u5668\u5b66\u4e60\u5728\u9884\u6d4b\u98de\u884c\u8fc7\u7a0b\u4e2d\u4fe1\u53f7\u8d28\u91cf\u65b9\u9762\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u6a21\u578b\u901a\u8fc7\u5b9e\u73b0\u4e0d\u540c\u536b\u661f\u661f\u5ea7\u548c\u63d0\u4f9b\u5546\u4e4b\u95f4\u7684\u6f2b\u6e38\uff0c\u6ee1\u8db3\u4e86\u5bf9\u4fe1\u53f7\u8d28\u91cf\u8fdb\u884c\u5b9e\u65f6\u9884\u6d4b\u7684\u9700\u6c42\u3002\u8be5\u65b9\u6cd5\u53ef\u4ee5\u8fdb\u4e00\u6b65\u8c03\u6574\u4ee5\u81ea\u52a8\u6267\u884c\u536b\u661f\u548c\u6ce2\u675f\u5207\u6362\u673a\u5236\uff0c\u4ece\u800c\u63d0\u9ad8\u6574\u4f53\u901a\u4fe1\u6548\u7387\u3002\u8be5\u6a21\u578b\u8fd8\u53ef\u4ee5\u4f7f\u7528\u5b9a\u5236\u6570\u636e\u96c6\uff08\u5305\u62ec\u8054\u7f51\u8f66\u8f86\u548c\u706b\u8f66\uff09\u91cd\u65b0\u8bad\u7ec3\u5e76\u5e94\u7528\u4e8e\u4efb\u4f55\u5177\u6709\u536b\u661f\u8fde\u63a5\u7684\u79fb\u52a8\u7269\u4f53\u3002"}}
{"id": "2508.02270", "categories": ["cs.LG", "cs.DB"], "pdf": "https://arxiv.org/pdf/2508.02270", "abs": "https://arxiv.org/abs/2508.02270", "authors": ["Tiantian Liu", "Xiao Li", "Huan Li", "Hua Lu", "Christian S. Jensen", "Jianliang Xu"], "title": "Skeleton-Guided Learning for Shortest Path Search", "comment": null, "summary": "Shortest path search is a core operation in graph-based applications, yet\nexisting methods face important limitations. Classical algorithms such as\nDijkstra's and A* become inefficient as graphs grow more complex, while\nindex-based techniques often require substantial preprocessing and storage.\nRecent learning-based approaches typically focus on spatial graphs and rely on\ncontext-specific features like geographic coordinates, limiting their general\napplicability. We propose a versatile learning-based framework for shortest\npath search on generic graphs, without requiring domain-specific features. At\nthe core of our approach is the construction of a skeleton graph that captures\nmulti-level distance and hop information in a compact form. A Skeleton Graph\nNeural Network (SGNN) operates on this structure to learn node embeddings and\npredict distances and hop lengths between node pairs. These predictions support\nLSearch, a guided search algorithm that uses model-driven pruning to reduce the\nsearch space while preserving accuracy. To handle larger graphs, we introduce a\nhierarchical training strategy that partitions the graph into subgraphs with\nindividually trained SGNNs. This structure enables HLSearch, an extension of\nour method for efficient path search across graph partitions. Experiments on\nfive diverse real-world graphs demonstrate that our framework achieves strong\nperformance across graph types, offering a flexible and effective solution for\nlearning-based shortest path search.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u901a\u7528\u56fe\u4e0a\u8fdb\u884c\u6700\u77ed\u8def\u5f84\u641c\u7d22\uff0c\u800c\u65e0\u9700\u7279\u5b9a\u4e8e\u9886\u57df\u7684\u7279\u5f81\u3002", "motivation": "\u6700\u77ed\u8def\u5f84\u641c\u7d22\u662f\u57fa\u4e8e\u56fe\u7684\u5e94\u7528\u4e2d\u7684\u6838\u5fc3\u64cd\u4f5c\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u7740\u91cd\u8981\u7684\u5c40\u9650\u6027\u3002\u968f\u7740\u56fe\u53d8\u5f97\u8d8a\u6765\u8d8a\u590d\u6742\uff0c\u8bf8\u5982Dijkstra\u548cA*\u4e4b\u7c7b\u7684\u7ecf\u5178\u7b97\u6cd5\u53d8\u5f97\u6548\u7387\u4f4e\u4e0b\uff0c\u800c\u57fa\u4e8e\u7d22\u5f15\u7684\u6280\u672f\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u9884\u5904\u7406\u548c\u5b58\u50a8\u3002\u6700\u8fd1\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u901a\u5e38\u4fa7\u91cd\u4e8e\u7a7a\u95f4\u56fe\uff0c\u5e76\u4e14\u4f9d\u8d56\u4e8e\u8bf8\u5982\u5730\u7406\u5750\u6807\u4e4b\u7c7b\u7684\u7279\u5b9a\u4e8e\u4e0a\u4e0b\u6587\u7684\u7279\u5f81\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5b83\u4eec\u7684\u901a\u7528\u6027\u3002", "method": "\u6784\u5efa\u9aa8\u67b6\u56fe\uff0c\u8be5\u9aa8\u67b6\u56fe\u4ee5\u7d27\u51d1\u7684\u5f62\u5f0f\u6355\u83b7\u591a\u5c42\u8ddd\u79bb\u548c\u8df3\u8dc3\u4fe1\u606f\u3002\u9aa8\u67b6\u56fe\u795e\u7ecf\u7f51\u7edc (SGNN) \u5bf9\u8be5\u7ed3\u6784\u8fdb\u884c\u64cd\u4f5c\uff0c\u4ee5\u5b66\u4e60\u8282\u70b9\u5d4c\u5165\u5e76\u9884\u6d4b\u8282\u70b9\u5bf9\u4e4b\u95f4\u7684\u8ddd\u79bb\u548c\u8df3\u8dc3\u957f\u5ea6\u3002\u8fd9\u4e9b\u9884\u6d4b\u652f\u6301 LSearch\uff0c\u8fd9\u662f\u4e00\u79cd\u5f15\u5bfc\u641c\u7d22\u7b97\u6cd5\uff0c\u5b83\u4f7f\u7528\u6a21\u578b\u9a71\u52a8\u7684\u4fee\u526a\u6765\u51cf\u5c11\u641c\u7d22\u7a7a\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002\u4e3a\u4e86\u5904\u7406\u66f4\u5927\u7684\u56fe\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u5206\u5c42\u8bad\u7ec3\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u5c06\u56fe\u5212\u5206\u4e3a\u5177\u6709\u5355\u72ec\u8bad\u7ec3\u7684 SGNN \u7684\u5b50\u56fe\u3002\u8fd9\u79cd\u7ed3\u6784\u652f\u6301 HLSearch\uff0c\u8fd9\u662f\u6211\u4eec\u7684\u4e00\u79cd\u65b9\u6cd5\u7684\u6269\u5c55\uff0c\u7528\u4e8e\u5728\u56fe\u5206\u533a\u4e2d\u8fdb\u884c\u6709\u6548\u7684\u8def\u5f84\u641c\u7d22\u3002", "result": "\u5728\u4e94\u4e2a\u4e0d\u540c\u7684\u771f\u5b9e\u4e16\u754c\u56fe\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6846\u67b6\u5728\u5404\u79cd\u56fe\u7c7b\u578b\u4e0a\u90fd\u53d6\u5f97\u4e86\u5f3a\u5927\u7684\u6027\u80fd\uff0c", "conclusion": "\u8be5\u6846\u67b6\u5728\u5404\u79cd\u56fe\u7c7b\u578b\u4e0a\u90fd\u53d6\u5f97\u4e86\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u4e3a\u57fa\u4e8e\u5b66\u4e60\u7684\u6700\u77ed\u8def\u5f84\u641c\u7d22\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.01514", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.01514", "abs": "https://arxiv.org/abs/2508.01514", "authors": ["Danial Ebrat", "Tina Aminian", "Sepideh Ahmadian", "Luis Rueda"], "title": "End-to-End Personalization: Unifying Recommender Systems with Large Language Models", "comment": "Second Workshop on Generative AI for Recommender Systems and\n  Personalization at the ACM Conference on Knowledge Discovery and Data Mining\n  (GenAIRecP@KDD 2025)", "summary": "Recommender systems are essential for guiding users through the vast and\ndiverse landscape of digital content by delivering personalized and relevant\nsuggestions. However, improving both personalization and interpretability\nremains a challenge, particularly in scenarios involving limited user feedback\nor heterogeneous item attributes. In this article, we propose a novel hybrid\nrecommendation framework that combines Graph Attention Networks (GATs) with\nLarge Language Models (LLMs) to address these limitations. LLMs are first used\nto enrich user and item representations by generating semantically meaningful\nprofiles based on metadata such as titles, genres, and overviews. These\nenriched embeddings serve as initial node features in a user and movie\nbipartite graph, which is processed using a GAT based collaborative filtering\nmodel. To enhance ranking accuracy, we introduce a hybrid loss function that\ncombines Bayesian Personalized Ranking (BPR), cosine similarity, and robust\nnegative sampling. Post-processing involves reranking the GAT-generated\nrecommendations using the LLM, which also generates natural-language\njustifications to improve transparency. We evaluated our model on benchmark\ndatasets, including MovieLens 100k and 1M, where it consistently outperforms\nstrong baselines. Ablation studies confirm that LLM-based embeddings and the\ncosine similarity term significantly contribute to performance gains. This work\ndemonstrates the potential of integrating LLMs to improve both the accuracy and\ninterpretability of recommender systems.", "AI": {"tldr": "hybrid recommendation framework that combines Graph Attention Networks (GATs) with Large Language Models (LLMs) to improve both the accuracy and interpretability of recommender systems", "motivation": "improving both personalization and interpretability remains a challenge, particularly in scenarios involving limited user feedback or heterogeneous item attributes", "method": "a novel hybrid recommendation framework that combines Graph Attention Networks (GATs) with Large Language Models (LLMs)", "result": "evaluated our model on benchmark datasets, including MovieLens 100k and 1M, where it consistently outperforms strong baselines. Ablation studies confirm that LLM-based embeddings and the cosine similarity term significantly contribute to performance gains.", "conclusion": "This work demonstrates the potential of integrating LLMs to improve both the accuracy and interpretability of recommender systems."}}
{"id": "2508.01159", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01159", "abs": "https://arxiv.org/abs/2508.01159", "authors": ["Liam G. McCoy", "Fateme Nateghi Haredasht", "Kanav Chopra", "David Wu", "David JH Wu", "Abass Conteh", "Sarita Khemani", "Saloni Kumar Maharaj", "Vishnu Ravi", "Arth Pahwa", "Yingjie Weng", "Leah Rosengaus", "Lena Giang", "Kelvin Zhenghao Li", "Olivia Jee", "Daniel Shirvani", "Ethan Goh", "Jonathan H. Chen"], "title": "Asking the Right Questions: Benchmarking Large Language Models in the Development of Clinical Consultation Templates", "comment": null, "summary": "This study evaluates the capacity of large language models (LLMs) to generate\nstructured clinical consultation templates for electronic consultation. Using\n145 expert-crafted templates developed and routinely used by Stanford's\neConsult team, we assess frontier models -- including o3, GPT-4o, Kimi K2,\nClaude 4 Sonnet, Llama 3 70B, and Gemini 2.5 Pro -- for their ability to\nproduce clinically coherent, concise, and prioritized clinical question\nschemas. Through a multi-agent pipeline combining prompt optimization, semantic\nautograding, and prioritization analysis, we show that while models like o3\nachieve high comprehensiveness (up to 92.2\\%), they consistently generate\nexcessively long templates and fail to correctly prioritize the most clinically\nimportant questions under length constraints. Performance varies across\nspecialties, with significant degradation in narrative-driven fields such as\npsychiatry and pain medicine. Our findings demonstrate that LLMs can enhance\nstructured clinical information exchange between physicians, while highlighting\nthe need for more robust evaluation methods that capture a model's ability to\nprioritize clinically salient information within the time constraints of\nreal-world physician communication.", "AI": {"tldr": "LLMs can help with clinical consultation templates but struggle with length and prioritization.", "motivation": "To evaluate the capacity of LLMs to generate structured clinical consultation templates for electronic consultation.", "method": "Evaluated LLMs against expert templates using prompt optimization, semantic autograding, and prioritization analysis.", "result": "Models achieve high comprehensiveness but generate excessively long templates and fail to prioritize clinically important questions, with performance varying across specialties.", "conclusion": "LLMs can enhance structured clinical information exchange but need better prioritization within time constraints."}}
{"id": "2508.00941", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00941", "abs": "https://arxiv.org/abs/2508.00941", "authors": ["Hassan Ugail", "Hamad Mansour Alawar", "AbdulNasser Abbas Zehi", "Ahmed Mohammad Alkendi", "Ismail Lujain Jaleel"], "title": "Latent Diffusion Based Face Enhancement under Degraded Conditions for Forensic Face Recognition", "comment": null, "summary": "Face recognition systems experience severe performance degradation when\nprocessing low-quality forensic evidence imagery. This paper presents an\nevaluation of latent diffusion-based enhancement for improving face recognition\nunder forensically relevant degradations. Using a dataset of 3,000 individuals\nfrom LFW with 24,000 recognition attempts, we implement the Flux.1 Kontext Dev\npipeline with Facezoom LoRA adaptation to test against seven degradation\ncategories, including compression artefacts, blur effects, and noise\ncontamination. Our approach demonstrates substantial improvements, increasing\noverall recognition accuracy from 29.1% to 84.5% (55.4 percentage point\nimprovement, 95% CI: [54.1, 56.7]). Statistical analysis reveals significant\nperformance gains across all degradation types, with effect sizes exceeding\nconventional thresholds for practical significance. These findings establish\nthe potential of sophisticated diffusion based enhancement in forensic face\nrecognition applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u7684\u589e\u5f3a\u6280\u672f\uff0c\u4ee5\u63d0\u9ad8\u6cd5\u5ead\u76f8\u5173\u964d\u7ea7\u4e0b\u7684\u4eba\u8138\u8bc6\u522b\u80fd\u529b\uff0c\u7ed3\u679c\u8868\u660e\u6027\u80fd\u663e\u7740\u63d0\u9ad8\u3002", "motivation": "\u5f53\u5904\u7406\u4f4e\u8d28\u91cf\u7684\u6cd5\u5ead\u8bc1\u636e\u56fe\u50cf\u65f6\uff0c\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u4f1a\u9047\u5230\u4e25\u91cd\u7684\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u4f7f\u7528\u5177\u6709 Facezoom LoRA \u9002\u914d\u7684 Flux.1 Kontext Dev \u7ba1\u9053\uff0c\u9488\u5bf9\u4e03\u79cd\u964d\u7ea7\u7c7b\u522b\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5305\u62ec\u538b\u7f29\u4f2a\u5f71\u3001\u6a21\u7cca\u6548\u679c\u548c\u566a\u58f0\u6c61\u67d3\u3002", "result": "\u8be5\u65b9\u6cd5\u8868\u73b0\u51fa\u663e\u7740\u6539\u8fdb\uff0c\u5c06\u6574\u4f53\u8bc6\u522b\u51c6\u786e\u7387\u4ece 29.1% \u63d0\u9ad8\u5230 84.5%\uff08\u63d0\u9ad8 55.4 \u4e2a\u767e\u5206\u70b9\uff0c95% CI\uff1a[54.1, 56.7]\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u786e\u7acb\u4e86\u57fa\u4e8e\u6269\u6563\u7684\u589e\u5f3a\u6280\u672f\u5728\u6cd5\u5ead\u4eba\u8138\u8bc6\u522b\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.00967", "categories": ["cs.AI", "cs.RO", "68T07, 68T45, 93C85", "I.2.6; I.2.9; I.2.10; I.4.8"], "pdf": "https://arxiv.org/pdf/2508.00967", "abs": "https://arxiv.org/abs/2508.00967", "authors": ["Massoud Pourmandi"], "title": "Cooperative Perception: A Resource-Efficient Framework for Multi-Drone 3D Scene Reconstruction Using Federated Diffusion and NeRF", "comment": "15 pages, 3 figures, 1 table, 1 algorithm. Preprint based on NeurIPS\n  2024 template", "summary": "The proposal introduces an innovative drone swarm perception system that aims\nto solve problems related to computational limitations and low-bandwidth\ncommunication, and real-time scene reconstruction. The framework enables\nefficient multi-agent 3D/4D scene synthesis through federated learning of\nshared diffusion model and YOLOv12 lightweight semantic extraction and local\nNeRF updates while maintaining privacy and scalability. The framework redesigns\ngenerative diffusion models for joint scene reconstruction, and improves\ncooperative scene understanding, while adding semantic-aware compression\nprotocols. The approach can be validated through simulations and potential\nreal-world deployment on drone testbeds, positioning it as a disruptive\nadvancement in multi-agent AI for autonomous systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u4eba\u673a\u7fa4\u611f\u77e5\u7cfb\u7edf\uff0c\u901a\u8fc7\u8054\u90a6\u5b66\u4e60\u548c\u8f7b\u91cf\u7ea7\u8bed\u4e49\u63d0\u53d6\u6765\u5b9e\u73b0\u9ad8\u6548\u3001\u5b9e\u65f6\u7684\u573a\u666f\u91cd\u5efa\uff0c\u89e3\u51b3\u4e86\u8ba1\u7b97\u548c\u901a\u4fe1\u9650\u5236\uff0c\u5e76\u5728\u4fdd\u6301\u9690\u79c1\u548c\u53ef\u6269\u5c55\u6027\u7684\u540c\u65f6\uff0c\u6539\u8fdb\u4e86\u534f\u4f5c\u573a\u666f\u7406\u89e3\u3002", "motivation": "\u89e3\u51b3\u4e0e\u8ba1\u7b97\u9650\u5236\u3001\u4f4e\u5e26\u5bbd\u901a\u4fe1\u548c\u5b9e\u65f6\u573a\u666f\u91cd\u5efa\u76f8\u5173\u7684\u95ee\u9898", "method": "\u8054\u90a6\u5b66\u4e60\u5171\u4eab\u6269\u6563\u6a21\u578b\u548c YOLOv12 \u8f7b\u91cf\u7ea7\u8bed\u4e49\u63d0\u53d6\u4ee5\u53ca\u5c40\u90e8 NeRF \u66f4\u65b0", "result": "\u5b9e\u73b0\u9ad8\u6548\u7684\u591a\u667a\u80fd\u4f53 3D/4D \u573a\u666f\u5408\u6210\uff0c\u540c\u65f6\u4fdd\u6301\u9690\u79c1\u548c\u53ef\u6269\u5c55\u6027\u3002\u6539\u8fdb\u4e86\u534f\u4f5c\u573a\u666f\u7406\u89e3\uff0c\u540c\u65f6\u6dfb\u52a0\u4e86\u8bed\u4e49\u611f\u77e5\u538b\u7f29\u534f\u8bae\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u65e0\u4eba\u673a\u7fa4\u611f\u77e5\u7cfb\u7edf\uff0c\u65e8\u5728\u89e3\u51b3\u4e0e\u8ba1\u7b97\u9650\u5236\u3001\u4f4e\u5e26\u5bbd\u901a\u4fe1\u548c\u5b9e\u65f6\u573a\u666f\u91cd\u5efa\u76f8\u5173\u7684\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u8054\u90a6\u5b66\u4e60\u5171\u4eab\u6269\u6563\u6a21\u578b\u548c YOLOv12 \u8f7b\u91cf\u7ea7\u8bed\u4e49\u63d0\u53d6\u4ee5\u53ca\u5c40\u90e8 NeRF \u66f4\u65b0\u6765\u5b9e\u73b0\u9ad8\u6548\u7684\u591a\u667a\u80fd\u4f53 3D/4D \u573a\u666f\u5408\u6210\uff0c\u540c\u65f6\u4fdd\u6301\u9690\u79c1\u548c\u53ef\u6269\u5c55\u6027\u3002\u901a\u8fc7\u4eff\u771f\u548c\u6f5c\u5728\u7684\u65e0\u4eba\u673a\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u7684\u5b9e\u9645\u90e8\u7f72\u6765\u9a8c\u8bc1\u8be5\u65b9\u6cd5\uff0c\u5c06\u5176\u5b9a\u4f4d\u4e3a\u81ea\u4e3b\u7cfb\u7edf\u591a\u667a\u80fd\u4f53 AI \u7684\u98a0\u8986\u6027\u8fdb\u6b65\u3002"}}
{"id": "2508.00879", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00879", "abs": "https://arxiv.org/abs/2508.00879", "authors": ["Moutaz Bellah Bentrad", "Adel Ghoggal", "Tahar Bahi", "Abderaouf Bahi"], "title": "GNN-ASE: Graph-Based Anomaly Detection and Severity Estimation in Three-Phase Induction Machines", "comment": null, "summary": "The diagnosis of induction machines has traditionally relied on model-based\nmethods that require the development of complex dynamic models, making them\ndifficult to implement and computationally expensive. To overcome these\nlimitations, this paper proposes a model-free approach using Graph Neural\nNetworks (GNNs) for fault diagnosis in induction machines. The focus is on\ndetecting multiple fault types -- including eccentricity, bearing defects, and\nbroken rotor bars -- under varying severity levels and load conditions. Unlike\ntraditional approaches, raw current and vibration signals are used as direct\ninputs, eliminating the need for signal preprocessing or manual feature\nextraction. The proposed GNN-ASE model automatically learns and extracts\nrelevant features from raw inputs, leveraging the graph structure to capture\ncomplex relationships between signal types and fault patterns. It is evaluated\nfor both individual fault detection and multi-class classification of combined\nfault conditions. Experimental results demonstrate the effectiveness of the\nproposed model, achieving 92.5\\% accuracy for eccentricity defects, 91.2\\% for\nbearing faults, and 93.1\\% for broken rotor bar detection. These findings\nhighlight the model's robustness and generalization capability across different\noperational scenarios. The proposed GNN-based framework offers a lightweight\nyet powerful solution that simplifies implementation while maintaining high\ndiagnostic performance. It stands as a promising alternative to conventional\nmodel-based diagnostic techniques for real-world induction machine monitoring\nand predictive maintenance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u65e0\u6a21\u578b\u65b9\u6cd5\uff0c\u7528\u4e8e\u611f\u5e94\u7535\u673a\u4e2d\u7684\u6545\u969c\u8bca\u65ad\uff0c\u65e0\u9700\u4fe1\u53f7\u9884\u5904\u7406\u6216\u624b\u52a8\u7279\u5f81\u63d0\u53d6\uff0c\u5b9e\u73b0\u4e86\u8f83\u9ad8\u7684\u8bca\u65ad\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u611f\u5e94\u7535\u673a\u8bca\u65ad\u4f9d\u8d56\u4e8e\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u9700\u8981\u5f00\u53d1\u590d\u6742\u7684\u52a8\u6001\u6a21\u578b\uff0c\u4f7f\u5176\u96be\u4ee5\u5b9e\u65bd\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002", "method": "\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u65e0\u6a21\u578b\u65b9\u6cd5", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u504f\u5fc3\u7f3a\u9677\u7684\u51c6\u786e\u7387\u4e3a92.5%\uff0c\u8f74\u627f\u6545\u969c\u7684\u51c6\u786e\u7387\u4e3a91.2%\uff0c\u65ad\u6761\u8f6c\u5b50\u68c0\u6d4b\u7684\u51c6\u786e\u7387\u4e3a93.1%\u3002", "conclusion": "GNN\u6a21\u578b\u4e3a\u611f\u5e94\u7535\u673a\u6545\u969c\u8bca\u65ad\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5b83\u7b80\u5316\u4e86\u5b9e\u65bd\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u8bca\u65ad\u6027\u80fd\u3002"}}
{"id": "2508.01643", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01643", "abs": "https://arxiv.org/abs/2508.01643", "authors": ["Ali Shiraee Kasmaee", "Mohammad Khodadad", "Mehdi Astaraki", "Mohammad Arshi Saloot", "Nicholas Sherck", "Hamidreza Mahyar", "Soheila Samiee"], "title": "ChEmbed: Enhancing Chemical Literature Search Through Domain-Specific Text Embeddings", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems in chemistry heavily depend on\naccurate and relevant retrieval of chemical literature. However,\ngeneral-purpose text embedding models frequently fail to adequately represent\ncomplex chemical terminologies, resulting in suboptimal retrieval quality.\nSpecialized embedding models tailored to chemical literature retrieval have not\nyet been developed, leaving a substantial performance gap. To address this\nchallenge, we introduce ChEmbed, a domain-adapted family of text embedding\nmodels fine-tuned on a dataset comprising chemistry-specific text from the\nPubChem, Semantic Scholar, and ChemRxiv corpora. To create effective training\ndata, we employ large language models to synthetically generate queries,\nresulting in approximately 1.7 million high-quality query-passage pairs.\nAdditionally, we augment the tokenizer by adding 900 chemically specialized\ntokens to previously unused slots, which significantly reduces the\nfragmentation of chemical entities, such as IUPAC names. ChEmbed also maintains\na 8192-token context length, enabling the efficient retrieval of longer\npassages compared to many other open-source embedding models, which typically\nhave a context length of 512 or 2048 tokens. Evaluated on our newly introduced\nChemRxiv Retrieval benchmark, ChEmbed outperforms state-of-the-art general\nembedding models, raising nDCG@10 from 0.82 to 0.91 (+9 pp). ChEmbed represents\na practical, lightweight, and reproducible embedding solution that effectively\nimproves retrieval for chemical literature search.", "AI": {"tldr": "ChEmbed, a domain-adapted text embedding model, improves chemical literature retrieval by fine-tuning on chemical text, generating synthetic queries, adding chemical tokens, and maintaining a long context length.", "motivation": "General-purpose text embedding models frequently fail to adequately represent complex chemical terminologies, resulting in suboptimal retrieval quality. Specialized embedding models tailored to chemical literature retrieval have not yet been developed, leaving a substantial performance gap.", "method": "a domain-adapted family of text embedding models fine-tuned on a dataset comprising chemistry-specific text from the PubChem, Semantic Scholar, and ChemRxiv corpora. We employ large language models to synthetically generate queries, resulting in approximately 1.7 million high-quality query-passage pairs. Additionally, we augment the tokenizer by adding 900 chemically specialized tokens to previously unused slots, which significantly reduces the fragmentation of chemical entities, such as IUPAC names. ChEmbed also maintains a 8192-token context length", "result": "ChEmbed outperforms state-of-the-art general embedding models, raising nDCG@10 from 0.82 to 0.91 (+9 pp) on our newly introduced ChemRxiv Retrieval benchmark.", "conclusion": "ChEmbed is a practical, lightweight, and reproducible embedding solution that effectively improves retrieval for chemical literature search."}}
{"id": "2508.01161", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01161", "abs": "https://arxiv.org/abs/2508.01161", "authors": ["Jiyu Chen", "Necva B\u00f6l\u00fcc\u00fc", "Sarvnaz Karimi", "Diego Moll\u00e1", "C\u00e9cile L. Paris"], "title": "CSIRO-LT at SemEval-2025 Task 11: Adapting LLMs for Emotion Recognition for Multiple Languages", "comment": "In Proceedings of the 19th International Workshop on Semantic\n  Evaluation (SemEval-2025), Vienna, Austria. Association for Computational\n  Linguistics", "summary": "Detecting emotions across different languages is challenging due to the\nvaried and culturally nuanced ways of emotional expressions. The\n\\textit{Semeval 2025 Task 11: Bridging the Gap in Text-Based emotion} shared\ntask was organised to investigate emotion recognition across different\nlanguages. The goal of the task is to implement an emotion recogniser that can\nidentify the basic emotional states that general third-party observers would\nattribute to an author based on their written text snippet, along with the\nintensity of those emotions. We report our investigation of various\ntask-adaptation strategies for LLMs in emotion recognition. We show that the\nmost effective method for this task is to fine-tune a pre-trained multilingual\nLLM with LoRA setting separately for each language.", "AI": {"tldr": "This paper investigates emotion recognition across different languages using LLMs, finding that fine-tuning a multilingual LLM with LoRA separately for each language is the most effective method.", "motivation": "Detecting emotions across different languages is challenging due to the varied and culturally nuanced ways of emotional expressions. The Semeval 2025 Task 11: Bridging the Gap in Text-Based emotion shared task was organised to investigate emotion recognition across different languages. The goal of the task is to implement an emotion recogniser that can identify the basic emotional states that general third-party observers would attribute to an author based on their written text snippet, along with the intensity of those emotions.", "method": "fine-tune a pre-trained multilingual LLM with LoRA setting separately for each language", "result": "investigation of various task-adaptation strategies for LLMs in emotion recognition", "conclusion": "The most effective method for this task is to fine-tune a pre-trained multilingual LLM with LoRA setting separately for each language."}}
{"id": "2508.00945", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00945", "abs": "https://arxiv.org/abs/2508.00945", "authors": ["Yifan Wang", "Hongfeng Ai", "Quangao Liu", "Maowei Jiang", "Ruiyuan Kang", "Ruiqi Li", "Jiahua Dong", "Mengting Xiao", "Cheng Jiang", "Chenzhong Li"], "title": "Optimizing Vision-Language Consistency via Cross-Layer Regional Attention Alignment", "comment": "10 pages", "summary": "Vision Language Models (VLMs) face challenges in effectively coordinating\ndiverse attention mechanisms for cross-modal embedding learning, leading to\nmismatched attention and suboptimal performance. We propose Consistent\nCross-layer Regional Alignment (CCRA), which introduces Layer-Patch-wise Cross\nAttention (LPWCA) to capture fine-grained regional-semantic correlations by\njointly weighting patch and layer-wise embedding, and Progressive Attention\nIntegration (PAI) that systematically coordinates LPWCA, layer-wise, and\npatch-wise attention mechanisms in sequence. This progressive design ensures\nconsistency from semantic to regional levels while preventing attention drift\nand maximizing individual attention benefits. Experimental results on ten\ndiverse vision-language benchmarks demonstrate that our CCRA-enhanced\nLLaVA-v1.5-7B model achieves state-of-the-art performance, outperforming all\nbaseline methods with only 3.55M additional parameters, while providing\nenhanced interpretability through more regionally focused and semantically\naligned attention patterns.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a CCRA \u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u6709\u6548\u534f\u8c03\u7528\u4e8e\u8de8\u6a21\u6001\u5d4c\u5165\u5b66\u4e60\u7684\u5404\u79cd\u6ce8\u610f\u529b\u673a\u5236\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u5bfc\u81f4\u6ce8\u610f\u529b\u4e0d\u5339\u914d\u548c\u6027\u80fd\u6b20\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u81f4\u7684\u8de8\u5c42\u533a\u57df\u5bf9\u9f50\uff08CCRA\uff09\uff0c\u5b83\u5f15\u5165\u4e86\u5c42-\u8865\u4e01\u5f0f\u4ea4\u53c9\u6ce8\u610f\u529b\uff08LPWCA\uff09\u4ee5\u901a\u8fc7\u8054\u5408\u52a0\u6743\u8865\u4e01\u548c\u5c42\u5f0f\u5d4c\u5165\u6765\u6355\u83b7\u7ec6\u7c92\u5ea6\u7684\u533a\u57df-\u8bed\u4e49\u76f8\u5173\u6027\uff0c\u4ee5\u53ca\u6e10\u8fdb\u5f0f\u6ce8\u610f\u529b\u96c6\u6210\uff08PAI\uff09\uff0c\u5176\u7cfb\u7edf\u5730\u6309\u987a\u5e8f\u534f\u8c03 LPWCA\u3001\u5c42\u5f0f\u548c\u8865\u4e01\u5f0f\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "CCRA-enhanced LLaVA-v1.5-7B \u6a21\u578b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u901a\u8fc7\u66f4\u6ce8\u91cd\u533a\u57df\u548c\u8bed\u4e49\u5bf9\u9f50\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\u63d0\u4f9b\u4e86\u589e\u5f3a\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "CCRA-enhanced LLaVA-v1.5-7B \u6a21\u578b\u5728\u5341\u4e2a\u4e0d\u540c\u7684\u89c6\u89c9\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u901a\u8fc7\u66f4\u6ce8\u91cd\u533a\u57df\u548c\u8bed\u4e49\u5bf9\u9f50\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\u63d0\u4f9b\u4e86\u589e\u5f3a\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2508.01012", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01012", "abs": "https://arxiv.org/abs/2508.01012", "authors": ["Yiyi Lu", "Hoi Ian Au", "Junyao Zhang", "Jingyu Pan", "Yiting Wang", "Ang Li", "Jianyi Zhang", "Yiran Chen"], "title": "AutoEDA: Enabling EDA Flow Automation through Microservice-Based LLM Agents", "comment": null, "summary": "Modern Electronic Design Automation (EDA) workflows, especially the\nRTL-to-GDSII flow, require heavily manual scripting and demonstrate a multitude\nof tool-specific interactions which limits scalability and efficiency. While\nLLMs introduces strides for automation, existing LLM solutions require\nexpensive fine-tuning and do not contain standardized frameworks for\nintegration and evaluation. We introduce AutoEDA, a framework for EDA\nautomation that leverages paralleled learning through the Model Context\nProtocol (MCP) specific for standardized and scalable natural language\nexperience across the entire RTL-to-GDSII flow. AutoEDA limits fine-tuning\nthrough structured prompt engineering, implements intelligent parameter\nextraction and task decomposition, and provides an extended CodeBLEU metric to\nevaluate the quality of TCL scripts. Results from experiments over five\npreviously curated benchmarks show improvements in automation accuracy and\nefficiency, as well as script quality when compared to existing methods.\nAutoEDA is released open-sourced to support reproducibility and the EDA\ncommunity. Available at: https://github.com/AndyLu666/MCP-EDA-Server", "AI": {"tldr": "AutoEDA\u662f\u4e00\u4e2a\u7528\u4e8eEDA\u81ea\u52a8\u5316\u7684\u6846\u67b6\uff0c\u5b83\u5229\u7528\u5e76\u884c\u5b66\u4e60\u3001\u7ed3\u6784\u5316\u63d0\u793a\u5de5\u7a0b\u3001\u667a\u80fd\u53c2\u6570\u63d0\u53d6\u548c\u4efb\u52a1\u5206\u89e3\u6765\u63d0\u9ad8\u81ea\u52a8\u5316\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5e76\u5df2\u5f00\u6e90\u3002", "motivation": "\u73b0\u4ee3\u7535\u5b50\u8bbe\u8ba1\u81ea\u52a8\u5316\uff08EDA\uff09\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7279\u522b\u662fRTL-to-GDSII\u6d41\u7a0b\uff0c\u9700\u8981\u5927\u91cf\u7684\u624b\u52a8\u811a\u672c\u7f16\u5199\uff0c\u5e76\u5c55\u793a\u4e86\u8bb8\u591a\u7279\u5b9a\u4e8e\u5de5\u5177\u7684\u4ea4\u4e92\uff0c\u8fd9\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u3002\u73b0\u6709\u7684LLM\u89e3\u51b3\u65b9\u6848\u9700\u8981\u6602\u8d35\u7684\u5fae\u8c03\uff0c\u5e76\u4e14\u4e0d\u5305\u542b\u7528\u4e8e\u96c6\u6210\u548c\u8bc4\u4f30\u7684\u6807\u51c6\u5316\u6846\u67b6\u3002", "method": "AutoEDA\u6846\u67b6\u5229\u7528\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u8fdb\u884c\u5e76\u884c\u5b66\u4e60\uff0c\u5b9e\u73b0\u8de8\u6574\u4e2aRTL-to-GDSII\u6d41\u7a0b\u7684\u6807\u51c6\u5316\u548c\u53ef\u6269\u5c55\u7684\u81ea\u7136\u8bed\u8a00\u4f53\u9a8c\u3002", "result": "\u5728\u4e94\u4e2a\u5148\u524d\u6574\u7406\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u81ea\u52a8\u5316\u51c6\u786e\u6027\u548c\u6548\u7387\u4ee5\u53ca\u811a\u672c\u8d28\u91cf\u6709\u6240\u63d0\u9ad8\u3002", "conclusion": "AutoEDA\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u5de5\u7a0b\u3001\u667a\u80fd\u53c2\u6570\u63d0\u53d6\u548c\u4efb\u52a1\u5206\u89e3\uff0c\u4ee5\u53ca\u6269\u5c55\u7684CodeBLEU\u6307\u6807\u6765\u8bc4\u4f30TCL\u811a\u672c\u7684\u8d28\u91cf\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u81ea\u52a8\u5316\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u4ee5\u53ca\u811a\u672c\u8d28\u91cf\u3002AutoEDA\u5df2\u5f00\u6e90\u53d1\u5e03\u3002"}}
{"id": "2508.00880", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00880", "abs": "https://arxiv.org/abs/2508.00880", "authors": ["Adil Mukhtar", "Michael Hadwiger", "Franz Wotawa", "Gerald Schweiger"], "title": "Reproducibility of Machine Learning-Based Fault Detection and Diagnosis for HVAC Systems in Buildings: An Empirical Study", "comment": null, "summary": "Reproducibility is a cornerstone of scientific research, enabling independent\nverification and validation of empirical findings. The topic gained prominence\nin fields such as psychology and medicine, where concerns about non -\nreplicable results sparked ongoing discussions about research practices. In\nrecent years, the fast-growing field of Machine Learning (ML) has become part\nof this discourse, as it faces similar concerns about transparency and\nreliability. Some reproducibility issues in ML research are shared with other\nfields, such as limited access to data and missing methodological details. In\naddition, ML introduces specific challenges, including inherent nondeterminism\nand computational constraints. While reproducibility issues are increasingly\nrecognized by the ML community and its major conferences, less is known about\nhow these challenges manifest in applied disciplines. This paper contributes to\nclosing this gap by analyzing the transparency and reproducibility standards of\nML applications in building energy systems. The results indicate that nearly\nall articles are not reproducible due to insufficient disclosure across key\ndimensions of reproducibility. 72% of the articles do not specify whether the\ndataset used is public, proprietary, or commercially available. Only two papers\nshare a link to their code - one of which was broken. Two-thirds of the\npublications were authored exclusively by academic researchers, yet no\nsignificant differences in reproducibility were observed compared to\npublications with industry-affiliated authors. These findings highlight the\nneed for targeted interventions, including reproducibility guidelines, training\nfor researchers, and policies by journals and conferences that promote\ntransparency and reproducibility.", "AI": {"tldr": "This paper analyzes the reproducibility of ML applications in building energy systems and finds that nearly all articles are not reproducible due to insufficient disclosure. It recommends reproducibility guidelines, training, and policies to promote transparency and reproducibility.", "motivation": "The fast-growing field of Machine Learning (ML) has become part of this discourse, as it faces similar concerns about transparency and reliability. While reproducibility issues are increasingly recognized by the ML community and its major conferences, less is known about how these challenges manifest in applied disciplines.", "method": "Analyzing the transparency and reproducibility standards of ML applications in building energy systems.", "result": "72% of the articles do not specify whether the dataset used is public, proprietary, or commercially available. Only two papers share a link to their code - one of which was broken. No significant differences in reproducibility were observed compared to publications with industry-affiliated authors.", "conclusion": "Nearly all articles are not reproducible due to insufficient disclosure across key dimensions of reproducibility. The findings highlight the need for targeted interventions, including reproducibility guidelines, training for researchers, and policies by journals and conferences that promote transparency and reproducibility."}}
{"id": "2508.01867", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01867", "abs": "https://arxiv.org/abs/2508.01867", "authors": ["Kazuki Kawamura", "Takuma Udagawa", "Kei Tateno"], "title": "Counterfactual Reciprocal Recommender Systems for User-to-User Matching", "comment": "9 pages, 2 figures. Accepted for publication at the Workshop on\n  Two-sided Marketplace Optimization (TSMO '25), held in conjunction with the\n  31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 2025),\n  Toronto, Canada", "summary": "Reciprocal recommender systems (RRS) in dating, gaming, and talent platforms\nrequire mutual acceptance for a match. Logged data, however, over-represents\npopular profiles due to past exposure policies, creating feedback loops that\nskew learning and fairness. We introduce Counterfactual Reciprocal Recommender\nSystems (CFRR), a causal framework to mitigate this bias. CFRR uses inverse\npropensity scored, self-normalized objectives. Experiments show CFRR improves\nNDCG@10 by up to 3.5% (e.g., from 0.459 to 0.475 on DBLP, from 0.299 to 0.307\non Synthetic), increases long-tail user coverage by up to 51% (from 0.504 to\n0.763 on Synthetic), and reduces Gini exposure inequality by up to 24% (from\n0.708 to 0.535 on Synthetic). CFRR offers a promising approach for more\naccurate and fair user-to-user matching.", "AI": {"tldr": "CFRR is a causal framework that mitigates bias in reciprocal recommender systems by using inverse propensity scoring, leading to improved accuracy, fairness, and coverage of long-tail users.", "motivation": "Logged data over-represents popular profiles due to past exposure policies, creating feedback loops that skew learning and fairness in reciprocal recommender systems.", "method": "CFRR uses inverse propensity scored, self-normalized objectives.", "result": "CFRR improves NDCG@10 by up to 3.5%, increases long-tail user coverage by up to 51%, and reduces Gini exposure inequality by up to 24%.", "conclusion": "CFRR offers a promising approach for more accurate and fair user-to-user matching."}}
{"id": "2508.01198", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01198", "abs": "https://arxiv.org/abs/2508.01198", "authors": ["Yige Li", "Peihai Jiang", "Jun Sun", "Peng Shu", "Tianming Liu", "Zhen Xiang"], "title": "Adaptive Content Restriction for Large Language Models via Suffix Optimization", "comment": "19 pages", "summary": "Large Language Models (LLMs) have demonstrated significant success across\ndiverse applications. However, enforcing content restrictions remains a\nsignificant challenge due to their expansive output space. One aspect of\ncontent restriction is preventing LLMs from generating harmful content via\nmodel alignment approaches such as supervised fine-tuning (SFT). Yet, the need\nfor content restriction may vary significantly across user groups, change\nrapidly over time, and not always align with general definitions of\nharmfulness. Applying SFT to each of these specific use cases is impractical\ndue to the high computational, data, and storage demands. Motivated by this\nneed, we propose a new task called \\textit{Adaptive Content Restriction}\n(AdaCoRe), which focuses on lightweight strategies -- methods without model\nfine-tuning -- to prevent deployed LLMs from generating restricted terms for\nspecific use cases. We propose the first method for AdaCoRe, named\n\\textit{Suffix Optimization (SOP)}, which appends a short, optimized suffix to\nany prompt to a) prevent a target LLM from generating a set of restricted\nterms, while b) preserving the output quality. To evaluate AdaCoRe approaches,\nincluding our SOP, we create a new \\textit{Content Restriction Benchmark}\n(CoReBench), which contains 400 prompts for 80 restricted terms across 8\ncarefully selected categories. We demonstrate the effectiveness of SOP on\nCoReBench, which outperforms the system-level baselines such as system suffix\nby 15\\%, 17\\%, 10\\%, 9\\%, and 6\\% on average restriction rates for Gemma2-2B,\nMistral-7B, Vicuna-7B, Llama3-8B, and Llama3.1-8B, respectively. We also\ndemonstrate that SOP is effective on POE, an online platform hosting various\ncommercial LLMs, highlighting its practicality in real-world scenarios.", "AI": {"tldr": "This paper introduces Adaptive Content Restriction (AdaCoRe) and a new method called Suffix Optimization (SOP) to prevent LLMs from generating restricted content without fine-tuning. SOP is evaluated on a new benchmark and shown to be effective.", "motivation": "Content restrictions for LLMs vary across user groups and time, making supervised fine-tuning impractical for each use case. This motivates the need for lightweight strategies like Adaptive Content Restriction (AdaCoRe).", "method": "The paper proposes Suffix Optimization (SOP), a lightweight strategy that appends a short, optimized suffix to prompts to prevent LLMs from generating restricted terms.", "result": "SOP outperforms system-level baselines on CoReBench by 15%, 17%, 10%, 9%, and 6% on average restriction rates for Gemma2-2B, Mistral-7B, Vicuna-7B, Llama3-8B, and Llama3.1-8B, respectively.", "conclusion": "The paper demonstrates the effectiveness of Suffix Optimization (SOP) on a new Content Restriction Benchmark (CoReBench) and on the POE platform, showing its practicality in real-world scenarios."}}
{"id": "2508.00974", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00974", "abs": "https://arxiv.org/abs/2508.00974", "authors": ["Daniel Andr\u00e9s L\u00f3pez", "Vincent Weber", "Severin Zentgraf", "Barlo Hillen", "Perikles Simon", "Elmar Sch\u00f6mer"], "title": "ThermoCycleNet: Stereo-based Thermogram Labeling for Model Transition to Cycling", "comment": "Presented at IWANN 2025 18th International Work-Conference on\n  Artificial Neural Networks, A Coru\\~na, Spain, 16-18 June, 2025. Book of\n  abstracts: ISBN: 979-13-8752213-1. Funding: Johannes Gutenberg University\n  \"Stufe I'': \"Start ThermoCycleNet''. Partial funding: Carl-Zeiss-Stiftung:\n  \"Multi-dimensionAI'' (CZS-Project number: P2022-08-010)", "summary": "Infrared thermography is emerging as a powerful tool in sports medicine,\nallowing assessment of thermal radiation during exercise and analysis of\nanatomical regions of interest, such as the well-exposed calves. Building on\nour previous advanced automatic annotation method, we aimed to transfer the\nstereo- and multimodal-based labeling approach from treadmill running to\nergometer cycling. Therefore, the training of the semantic segmentation network\nwith automatic labels and fine-tuning on high-quality manually annotated images\nhas been examined and compared in different data set combinations. The results\nindicate that fine-tuning with a small fraction of manual data is sufficient to\nimprove the overall performance of the deep neural network. Finally, combining\nautomatically generated labels with small manually annotated data sets\naccelerates the adaptation of deep neural networks to new use cases, such as\nthe transition from treadmill to bicycle.", "AI": {"tldr": "transition from treadmill to bicycle.", "motivation": "Infrared thermography is emerging as a powerful tool in sports medicine, allowing assessment of thermal radiation during exercise and analysis of anatomical regions of interest, such as the well-exposed calves. Building on our previous advanced automatic annotation method", "method": "transfer the stereo- and multimodal-based labeling approach from treadmill running to ergometer cycling. Therefore, the training of the semantic segmentation network with automatic labels and fine-tuning on high-quality manually annotated images has been examined and compared in different data set combinations.", "result": "fine-tuning with a small fraction of manual data is sufficient to improve the overall performance of the deep neural network.", "conclusion": "Combining automatically generated labels with small manually annotated data sets accelerates the adaptation of deep neural networks to new use cases, such as the transition from treadmill to bicycle."}}
{"id": "2508.01031", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01031", "abs": "https://arxiv.org/abs/2508.01031", "authors": ["Jingzhe Ni", "Xiaolong Yin", "Xintong Li", "Xingyu Lu", "Ji Wei", "Ruofeng Tong", "Min Tang", "Peng Du"], "title": "CADDesigner: Conceptual Design of CAD Models Based on General-Purpose Agent", "comment": null, "summary": "Computer-Aided Design (CAD) plays a pivotal role in industrial manufacturing\nbut typically requires a high level of expertise from designers. To lower the\nentry barrier and improve design efficiency, we present an agent for CAD\nconceptual design powered by large language models (LLMs). The agent accepts\nboth abstract textual descriptions and freehand sketches as input, engaging in\ninteractive dialogue with users to refine and clarify design requirements\nthrough comprehensive requirement analysis. Built upon a novel\nContext-Independent Imperative Paradigm (CIP), the agent generates high-quality\nCAD modeling code. During the generation process, the agent incorporates\niterative visual feedback to improve model quality. Generated design cases are\nstored in a structured knowledge base, enabling continuous improvement of the\nagent's code generation capabilities. Experimental results demonstrate that our\nmethod achieves state-of-the-art performance in CAD code generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u7684CAD\u6982\u5ff5\u8bbe\u8ba1\u4ee3\u7406\uff0c\u8be5\u4ee3\u7406\u63a5\u53d7\u62bd\u8c61\u6587\u672c\u63cf\u8ff0\u548c\u624b\u7ed8\u8349\u56fe\u4f5c\u4e3a\u8f93\u5165\uff0c\u901a\u8fc7\u7efc\u5408\u9700\u6c42\u5206\u6790\u4e0e\u7528\u6237\u8fdb\u884c\u4ea4\u4e92\u5f0f\u5bf9\u8bdd\uff0c\u4ee5\u5b8c\u5584\u548c\u660e\u786e\u8bbe\u8ba1\u9700\u6c42\u3002", "motivation": "\u4e3a\u4e86\u964d\u4f4eCAD\u7684\u51c6\u5165\u95e8\u69db\u5e76\u63d0\u9ad8\u8bbe\u8ba1\u6548\u7387\u3002", "method": "\u8be5Agent\u57fa\u4e8e\u4e00\u79cd\u65b0\u9896\u7684\u4e0a\u4e0b\u6587\u65e0\u5173\u547d\u4ee4\u5f0f\u8303\u5f0f(CIP)\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7684CAD\u5efa\u6a21\u4ee3\u7801\uff0c\u5e76\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u7ed3\u5408\u8fed\u4ee3\u89c6\u89c9\u53cd\u9988\u6765\u63d0\u9ad8\u6a21\u578b\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728CAD\u4ee3\u7801\u751f\u6210\u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728CAD\u4ee3\u7801\u751f\u6210\u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2508.00881", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00881", "abs": "https://arxiv.org/abs/2508.00881", "authors": ["Vijja Wichitwechkarn", "Charles Fox", "Ruchi Choudhary"], "title": "Hallucination Detection and Mitigation with Diffusion in Multi-Variate Time-Series Foundation Models", "comment": null, "summary": "Foundation models for natural language processing have many coherent\ndefinitions of hallucination and methods for its detection and mitigation.\nHowever, analogous definitions and methods do not exist for multi-variate\ntime-series (MVTS) foundation models. We propose new definitions for MVTS\nhallucination, along with new detection and mitigation methods using a\ndiffusion model to estimate hallucination levels. We derive relational datasets\nfrom popular time-series datasets to benchmark these relational hallucination\nlevels. Using these definitions and models, we find that open-source\npre-trained MVTS imputation foundation models relationally hallucinate on\naverage up to 59.5% as much as a weak baseline. The proposed mitigation method\nreduces this by up to 47.7% for these models. The definition and methods may\nimprove adoption and safe usage of MVTS foundation models.", "AI": {"tldr": "This paper introduces definitions and methods for detecting and mitigating hallucination in multi-variate time-series (MVTS) foundation models, finding that current models hallucinate significantly and proposing a method to reduce this issue.", "motivation": "Analogous definitions and methods do not exist for multi-variate time-series (MVTS) foundation models, while foundation models for natural language processing have many coherent definitions of hallucination and methods for its detection and mitigation.", "method": "The paper proposes new definitions for MVTS hallucination, along with new detection and mitigation methods using a diffusion model to estimate hallucination levels. Relational datasets are derived from popular time-series datasets to benchmark these relational hallucination levels.", "result": "Find that open-source pre-trained MVTS imputation foundation models relationally hallucinate on average up to 59.5% as much as a weak baseline. The proposed mitigation method reduces this by up to 47.7% for these models.", "conclusion": "Open-source pre-trained MVTS imputation foundation models relationally hallucinate on average up to 59.5% as much as a weak baseline, and the proposed mitigation method reduces this by up to 47.7% for these models. The definition and methods may improve adoption and safe usage of MVTS foundation models."}}
{"id": "2508.02020", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.02020", "abs": "https://arxiv.org/abs/2508.02020", "authors": ["Ethan Bito", "Yongli Ren", "Estrid He"], "title": "Evaluating Position Bias in Large Language Model Recommendations", "comment": null, "summary": "Large Language Models (LLMs) are being increasingly explored as\ngeneral-purpose tools for recommendation tasks, enabling zero-shot and\ninstruction-following capabilities without the need for task-specific training.\nWhile the research community is enthusiastically embracing LLMs, there are\nimportant caveats to directly adapting them for recommendation tasks. In this\npaper, we show that LLM-based recommendation models suffer from position bias,\nwhere the order of candidate items in a prompt can disproportionately influence\nthe recommendations produced by LLMs. First, we analyse the position bias of\nLLM-based recommendations on real-world datasets, where results uncover\nsystemic biases of LLMs with high sensitivity to input orders. Furthermore, we\nintroduce a new prompting strategy to mitigate the position bias of LLM\nrecommendation models called Ranking via Iterative SElection (RISE). We compare\nour proposed method against various baselines on key benchmark datasets.\nExperiment results show that our method reduces sensitivity to input ordering\nand improves stability without requiring model fine-tuning or post-processing.", "AI": {"tldr": "LLM \u63a8\u8350\u6a21\u578b\u5b58\u5728\u4f4d\u7f6e\u504f\u5dee\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63d0\u793a\u7b56\u7565 RISE \u6765\u51cf\u8f7b\u8fd9\u79cd\u504f\u5dee\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u6709\u6548\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u6b63\u8d8a\u6765\u8d8a\u591a\u5730\u88ab\u63a2\u7d22\u4e3a\u63a8\u8350\u4efb\u52a1\u7684\u901a\u7528\u5de5\u5177\uff0c\u65e0\u9700\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u96f6\u6837\u672c\u548c\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u3002\u7136\u800c\uff0c\u76f4\u63a5\u5c06 LLM \u5e94\u7528\u4e8e\u63a8\u8350\u4efb\u52a1\u5b58\u5728\u91cd\u8981\u7684\u6ce8\u610f\u4e8b\u9879\uff0cLLM \u63a8\u8350\u6a21\u578b\u5b58\u5728\u4f4d\u7f6e\u504f\u5dee\uff0c\u5176\u4e2d prompt \u4e2d\u5019\u9009\u9879\u76ee\u7684\u987a\u5e8f\u53ef\u80fd\u4f1a\u4e0d\u6210\u6bd4\u4f8b\u5730\u5f71\u54cd LLM \u4ea7\u751f\u7684\u63a8\u8350\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63d0\u793a\u7b56\u7565\uff0c\u79f0\u4e3a\u901a\u8fc7\u8fed\u4ee3\u9009\u62e9\u8fdb\u884c\u6392\u5e8f (RISE)\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\u4e0a\u5206\u6790\u4e86\u57fa\u4e8e LLM \u7684\u63a8\u8350\u7684\u4f4d\u7f6e\u504f\u5dee\uff0c\u7ed3\u679c\u63ed\u793a\u4e86 LLM \u7684\u7cfb\u7edf\u504f\u5dee\uff0c\u5176\u5bf9\u8f93\u5165\u987a\u5e8f\u9ad8\u5ea6\u654f\u611f\u3002\u63d0\u51fa\u7684 RISE \u65b9\u6cd5\u5728\u5173\u952e\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4e0e\u5404\u79cd\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u964d\u4f4e\u4e86\u5bf9\u8f93\u5165\u987a\u5e8f\u7684\u654f\u611f\u6027\u5e76\u63d0\u9ad8\u4e86\u7a33\u5b9a\u6027\uff0c\u800c\u65e0\u9700\u6a21\u578b\u5fae\u8c03\u6216\u540e\u5904\u7406\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63d0\u793a\u7b56\u7565\uff0c\u79f0\u4e3a\u901a\u8fc7\u8fed\u4ee3\u9009\u62e9\u8fdb\u884c\u6392\u5e8f (RISE)\uff0c\u4ee5\u51cf\u8f7b LLM \u63a8\u8350\u6a21\u578b\u7684\u4f4d\u7f6e\u504f\u5dee\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u964d\u4f4e\u4e86\u5bf9\u8f93\u5165\u987a\u5e8f\u7684\u654f\u611f\u6027\u5e76\u63d0\u9ad8\u4e86\u7a33\u5b9a\u6027\uff0c\u800c\u65e0\u9700\u6a21\u578b\u5fae\u8c03\u6216\u540e\u5904\u7406\u3002"}}
{"id": "2508.01213", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.01213", "abs": "https://arxiv.org/abs/2508.01213", "authors": ["Shengqi Zhu", "Jeffrey M. Rzeszotarski", "David Mimno"], "title": "Show or Tell? Modeling the evolution of request-making in Human-LLM conversations", "comment": null, "summary": "Chat logs provide a rich source of information about LLM users, but patterns\nof user behavior are often masked by the variability of queries. We present a\nnew task, segmenting chat queries into contents of requests, roles,\nquery-specific context, and additional expressions. We find that, despite the\nfamiliarity of chat-based interaction, request-making in LLM queries remains\nsignificantly different from comparable human-human interactions. With the data\nresource, we introduce an important perspective of diachronic analyses with\nuser expressions. We find that query patterns vary between early ones\nemphasizing requests, and individual users explore patterns but tend to\nconverge with experience. Finally, we show that model capabilities affect user\nbehavior, particularly with the introduction of new models, which are traceable\nat the community level.", "AI": {"tldr": "LLM\u67e5\u8be2\u5206\u6790\uff1a\u7528\u6237\u884c\u4e3a\u56e0\u7ecf\u9a8c\u548c\u6a21\u578b\u80fd\u529b\u800c\u5f02\u3002", "motivation": "\u804a\u5929\u8bb0\u5f55\u63d0\u4f9b\u4e86\u5173\u4e8eLLM\u7528\u6237\u7684\u4e30\u5bcc\u4fe1\u606f\u6765\u6e90\uff0c\u4f46\u7528\u6237\u884c\u4e3a\u6a21\u5f0f\u901a\u5e38\u88ab\u67e5\u8be2\u7684\u53ef\u53d8\u6027\u6240\u63a9\u76d6\u3002", "method": "\u5c06\u804a\u5929\u67e5\u8be2\u5206\u5272\u6210\u8bf7\u6c42\u5185\u5bb9\u3001\u89d2\u8272\u3001\u67e5\u8be2\u7279\u5b9a\u4e0a\u4e0b\u6587\u548c\u5176\u4ed6\u8868\u8fbe\u3002", "result": "LLM\u67e5\u8be2\u4e2d\u7684\u8bf7\u6c42\u65b9\u5f0f\u4e0e\u7c7b\u4f3c\u7684\u4eba\u9645\u4e92\u52a8\u6709\u663e\u8457\u5dee\u5f02\u3002\u67e5\u8be2\u6a21\u5f0f\u968f\u65f6\u95f4\u53d8\u5316\uff0c\u6a21\u578b\u80fd\u529b\u5f71\u54cd\u7528\u6237\u884c\u4e3a\u3002", "conclusion": "\u67e5\u8be2\u6a21\u5f0f\u5728\u65e9\u671f\u5f3a\u8c03\u8bf7\u6c42\uff0c\u4e2a\u4f53\u7528\u6237\u63a2\u7d22\u6a21\u5f0f\u4f46\u503e\u5411\u4e8e\u968f\u7740\u7ecf\u9a8c\u878d\u5408\u3002\u6a21\u578b\u80fd\u529b\u4f1a\u5f71\u54cd\u7528\u6237\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5728\u5f15\u5165\u65b0\u6a21\u578b\u65f6\uff0c\u8fd9\u53ef\u4ee5\u5728\u793e\u533a\u5c42\u9762\u8ffd\u8e2a\u3002"}}
{"id": "2508.01008", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01008", "abs": "https://arxiv.org/abs/2508.01008", "authors": ["Cihang Peng", "Qiming Hou", "Zhong Ren", "Kun Zhou"], "title": "ROVI: A VLM-LLM Re-Captioned Dataset for Open-Vocabulary Instance-Grounded Text-to-Image Generation", "comment": "Accepted at ICCV 2025", "summary": "We present ROVI, a high-quality synthetic dataset for instance-grounded\ntext-to-image generation, created by labeling 1M curated web images. Our key\ninnovation is a strategy called re-captioning, focusing on the pre-detection\nstage, where a VLM (Vision-Language Model) generates comprehensive visual\ndescriptions that are then processed by an LLM (Large Language Model) to\nextract a flat list of potential categories for OVDs (Open-Vocabulary\nDetectors) to detect. This approach yields a global prompt inherently linked to\ninstance annotations while capturing secondary visual elements humans typically\noverlook. Evaluations show that ROVI exceeds existing detection datasets in\nimage quality and resolution while containing two orders of magnitude more\ncategories with an open-vocabulary nature. For demonstrative purposes, a\ntext-to-image model GLIGEN trained on ROVI significantly outperforms\nstate-of-the-art alternatives in instance grounding accuracy, prompt fidelity,\nand aesthetic quality. Our dataset and reproducible pipeline are available at\nhttps://github.com/CihangPeng/ROVI.", "AI": {"tldr": "ROVI: a high-quality synthetic dataset for instance-grounded text-to-image generation, created by labeling 1M curated web images using a re-captioning strategy. It outperforms existing datasets and improves text-to-image model performance.", "motivation": "creating a high-quality synthetic dataset for instance-grounded text-to-image generation", "method": "re-captioning strategy, focusing on the pre-detection stage, where a VLM generates comprehensive visual descriptions that are then processed by an LLM to extract a flat list of potential categories for OVDs to detect", "result": "ROVI exceeds existing detection datasets in image quality and resolution while containing two orders of magnitude more categories with an open-vocabulary nature. GLIGEN trained on ROVI significantly outperforms state-of-the-art alternatives in instance grounding accuracy, prompt fidelity, and aesthetic quality.", "conclusion": "ROVI outperforms existing detection datasets in image quality and resolution, containing two orders of magnitude more categories. GLIGEN trained on ROVI significantly outperforms state-of-the-art alternatives."}}
{"id": "2508.01057", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.01057", "abs": "https://arxiv.org/abs/2508.01057", "authors": ["Fengze Yang", "Bo Yu", "Yang Zhou", "Xuewen Luo", "Zhengzhong Tu", "Chenxi Liu"], "title": "REACT: A Real-Time Edge-AI Based V2X Framework for Accident Avoidance in Autonomous Driving System", "comment": "24 pages, 6 tables, 7 figures", "summary": "Collisions caused by human error are the most common type of multi-vehicle\ncrash, highlighting the critical need for autonomous driving (AD) systems to\nleverage cooperative perception through Vehicle-to-Everything (V2X)\ncommunication. This capability extends situational awareness beyond the\nlimitations of onboard sensors. However, current transformer-based V2X\nframeworks suffer from limited generalization, shallow contextual reasoning,\nand reliance on mono-modal inputs. Vision-Language Models (VLMs) offer enhanced\nreasoning and multimodal integration but typically fall short of real-time\nperformance requirements in safety-critical applications. This paper presents\nREACT, a real-time, V2X-integrated trajectory optimization framework built upon\na fine-tuned lightweight VLM. REACT integrates a set of specialized modules\nthat process multimodal inputs into optimized, risk-aware trajectories. To\nensure real-time performance on edge devices, REACT incorporates edge\nadaptation strategies that reduce model complexity and accelerate inference.\nEvaluated on the DeepAccident benchmark, REACT achieves state-of-the-art\nperformance, a 77% collision rate reduction, a 48.2% Video Panoptic Quality\n(VPQ), and a 0.57-second inference latency on the Jetson AGX Orin. Ablation\nstudies validate the contribution of each input, module, and edge adaptation\nstrategy. These results demonstrate the feasibility of lightweight VLMs for\nreal-time edge-based cooperative planning and showcase the potential of\nlanguage-guided contextual reasoning to improve safety and responsiveness in\nautonomous driving.", "AI": {"tldr": "REACT\u662f\u4e00\u4e2a\u57fa\u4e8e\u8f7b\u91cf\u7ea7VLM\u7684\u5b9e\u65f6V2X\u96c6\u6210\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u8fb9\u7f18\u81ea\u9002\u5e94\u7b56\u7565\u548c\u591a\u6a21\u6001\u8f93\u5165\u5904\u7406\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u4eba\u4e3a\u9519\u8bef\u9020\u6210\u7684\u78b0\u649e\u662f\u591a\u8f66\u78b0\u649e\u7684\u6700\u5e38\u89c1\u7c7b\u578b\uff0c\u8fd9\u7a81\u51fa\u4e86\u81ea\u52a8\u9a7e\u9a76(AD)\u7cfb\u7edf\u901a\u8fc7\u8f66\u5bf9\u4e07\u7269(V2X)\u901a\u4fe1\u5229\u7528\u534f\u540c\u611f\u77e5\u7684\u8feb\u5207\u9700\u6c42\u3002\u7136\u800c\uff0c\u76ee\u524d\u57fa\u4e8etransformer\u7684V2X\u6846\u67b6\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3001\u4e0a\u4e0b\u6587\u63a8\u7406\u80a4\u6d45\u4ee5\u53ca\u4f9d\u8d56\u4e8e\u5355\u6a21\u6001\u8f93\u5165\u7b49\u95ee\u9898\u3002\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLM)\u63d0\u4f9b\u4e86\u589e\u5f3a\u7684\u63a8\u7406\u548c\u591a\u6a21\u6001\u96c6\u6210\uff0c\u4f46\u901a\u5e38\u65e0\u6cd5\u6ee1\u8db3\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u5b9e\u65f6\u6027\u80fd\u8981\u6c42\u3002", "method": "\u57fa\u4e8e\u5fae\u8c03\u7684\u8f7b\u91cf\u7ea7VLM\uff0cREACT\u96c6\u6210\u4e86\u591a\u4e2a\u4e13\u7528\u6a21\u5757\uff0c\u5c06\u591a\u6a21\u6001\u8f93\u5165\u5904\u7406\u4e3a\u4f18\u5316\u7684\u3001\u5177\u6709\u98ce\u9669\u610f\u8bc6\u7684\u8f68\u8ff9\u3002\u4e3a\u4e86\u786e\u4fdd\u8fb9\u7f18\u8bbe\u5907\u7684\u5b9e\u65f6\u6027\u80fd\uff0cREACT\u96c6\u6210\u4e86\u8fb9\u7f18\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u4ee5\u964d\u4f4e\u6a21\u578b\u590d\u6742\u6027\u5e76\u52a0\u901f\u63a8\u7406\u3002", "result": "\u5728DeepAccident\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cREACT\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u78b0\u649e\u7387\u964d\u4f4e\u4e8677%\uff0c\u89c6\u9891\u5168\u666f\u8d28\u91cf(VPQ)\u63d0\u9ad8\u4e8648.2%\uff0c\u5728Jetson AGX Orin\u4e0a\u7684\u63a8\u7406\u5ef6\u8fdf\u4e3a0.57\u79d2\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLM)\u5728\u5b9e\u65f6\u8fb9\u7f18\u534f\u540c\u89c4\u5212\u4e2d\u662f\u53ef\u884c\u7684\uff0c\u5e76\u5c55\u793a\u4e86\u8bed\u8a00\u5f15\u5bfc\u7684\u4e0a\u4e0b\u6587\u63a8\u7406\u5728\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u6027\u548c\u54cd\u5e94\u80fd\u529b\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.00884", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00884", "abs": "https://arxiv.org/abs/2508.00884", "authors": ["Zhenan Lin", "Yuni Lai", "Wai Lun Lo", "Richard Tai-Chiu Hsung", "Harris Sik-Ho Tsang", "Xiaoyu Xue", "Kai Zhou", "Yulin Zhu"], "title": "Multi-Grained Temporal-Spatial Graph Learning for Stable Traffic Flow Forecasting", "comment": null, "summary": "Time-evolving traffic flow forecasting are playing a vital role in\nintelligent transportation systems and smart cities. However, the dynamic\ntraffic flow forecasting is a highly nonlinear problem with complex\ntemporal-spatial dependencies. Although the existing methods has provided great\ncontributions to mine the temporal-spatial patterns in the complex traffic\nnetworks, they fail to encode the globally temporal-spatial patterns and are\nprone to overfit on the pre-defined geographical correlations, and thus hinder\nthe model's robustness on the complex traffic environment. To tackle this\nissue, in this work, we proposed a multi-grained temporal-spatial graph\nlearning framework to adaptively augment the globally temporal-spatial patterns\nobtained from a crafted graph transformer encoder with the local patterns from\nthe graph convolution by a crafted gated fusion unit with residual connection\ntechniques. Under these circumstances, our proposed model can mine the hidden\nglobal temporal-spatial relations between each monitor stations and balance the\nrelative importance of local and global temporal-spatial patterns. Experiment\nresults demonstrate the strong representation capability of our proposed method\nand our model consistently outperforms other strong baselines on various\nreal-world traffic networks.", "AI": {"tldr": "propose a multi-grained temporal-spatial graph learning framework to adaptively augment the globally temporal-spatial patterns", "motivation": "the dynamic traffic flow forecasting is a highly nonlinear problem with complex temporal-spatial dependencies. Although the existing methods has provided great contributions to mine the temporal-spatial patterns in the complex traffic networks, they fail to encode the globally temporal-spatial patterns and are prone to overfit on the pre-defined geographical correlations, and thus hinder the model's robustness on the complex traffic environment", "method": "a multi-grained temporal-spatial graph learning framework to adaptively augment the globally temporal-spatial patterns obtained from a crafted graph transformer encoder with the local patterns from the graph convolution by a crafted gated fusion unit with residual connection techniques", "result": "outperforms other strong baselines on various real-world traffic networks", "conclusion": " proposed model can mine the hidden global temporal-spatial relations between each monitor stations and balance the relative importance of local and global temporal-spatial patterns. Experiment results demonstrate the strong representation capability of our proposed method and our model consistently outperforms other strong baselines on various real-world traffic networks."}}
{"id": "2508.02050", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.02050", "abs": "https://arxiv.org/abs/2508.02050", "authors": ["Yuli Liu", "Wenjun Kong", "Cheng Luo", "Weizhi Ma"], "title": "Why Generate When You Can Transform? Unleashing Generative Attention for Dynamic Recommendation", "comment": "Accepted at ACMMM 2025", "summary": "Sequential Recommendation (SR) focuses on personalizing user experiences by\npredicting future preferences based on historical interactions. Transformer\nmodels, with their attention mechanisms, have become the dominant architecture\nin SR tasks due to their ability to capture dependencies in user behavior\nsequences. However, traditional attention mechanisms, where attention weights\nare computed through query-key transformations, are inherently linear and\ndeterministic. This fixed approach limits their ability to account for the\ndynamic and non-linear nature of user preferences, leading to challenges in\ncapturing evolving interests and subtle behavioral patterns. Given that\ngenerative models excel at capturing non-linearity and probabilistic\nvariability, we argue that generating attention distributions offers a more\nflexible and expressive alternative compared to traditional attention\nmechanisms. To support this claim, we present a theoretical proof demonstrating\nthat generative attention mechanisms offer greater expressiveness and\nstochasticity than traditional deterministic approaches. Building upon this\ntheoretical foundation, we introduce two generative attention models for SR,\neach grounded in the principles of Variational Autoencoders (VAE) and Diffusion\nModels (DMs), respectively. These models are designed specifically to generate\nadaptive attention distributions that better align with variable user\npreferences. Extensive experiments on real-world datasets show our models\nsignificantly outperform state-of-the-art in both accuracy and diversity.", "AI": {"tldr": "\u63d0\u51fa\u751f\u6210\u5f0f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u66f4\u597d\u5730\u6355\u6349\u7528\u6237\u504f\u597d\u7684\u52a8\u6001\u6027\u548c\u975e\u7ebf\u6027\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684\u6ce8\u610f\u529b\u673a\u5236\u662f\u7ebf\u6027\u548c\u786e\u5b9a\u6027\u7684\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u6355\u6349\u7528\u6237\u504f\u597d\u7684\u52a8\u6001\u548c\u975e\u7ebf\u6027\u6027\u8d28\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u751f\u6210\u5f0f\u6ce8\u610f\u529b\u6a21\u578b\uff0c\u5206\u522b\u57fa\u4e8e\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u548c\u6269\u6563\u6a21\u578b\uff08DMs\uff09\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\u65b9\u9762\u5747\u663e\u7740\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e24\u79cd\u7528\u4e8eSR\u7684\u751f\u6210\u5f0f\u6ce8\u610f\u529b\u6a21\u578b\uff0c\u5206\u522b\u57fa\u4e8eVAE\u548cDMs\uff0c\u5e76\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2508.01222", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01222", "abs": "https://arxiv.org/abs/2508.01222", "authors": ["Ethan Hsu", "Hong Meng Yam", "Ines Bouissou", "Aaron Murali John", "Raj Thota", "Josh Koe", "Vivek Sarath Putta", "G K Dharesan", "Alexander Spangher", "Shikhar Murty", "Tenghao Huang", "Christopher D. Manning"], "title": "WebDS: An End-to-End Benchmark for Web-based Data Science", "comment": "14 pages", "summary": "A large portion of real-world data science tasks are complex and require\nmulti-hop web-based interactions: finding appropriate data available on the\ninternet, synthesizing real-time data of various modalities from different\nlocations, and producing summarized analyses. Existing web benchmarks often\nfocus on simplistic interactions, such as form submissions or e-commerce\ntransactions, and often do not require diverse tool-using capabilities required\nfor web based data science. Conversely, traditional data science benchmarks\ntypically concentrate on static, often textually bound datasets and do not\nassess end-to-end workflows that encompass data acquisition, cleaning,\nanalysis, and insight generation. In response, we introduce WebDS, the first\nend-to-end web-based data science benchmark. It comprises 870 web-based data\nscience tasks across 29 diverse websites from structured government data\nportals to unstructured news media, challenging agents to perform complex,\nmulti-step operations requiring the use of tools and heterogeneous data formats\nthat better reflect the realities of modern data analytics. Evaluations of\ncurrent SOTA LLM agents indicate significant performance gaps in accomplishing\nthese tasks. For instance, Browser Use, which accomplishes 80% of tasks on Web\nVoyager, successfully completes only 15% of tasks in WebDS, which our analysis\nsuggests is due to new failure modes like poor information grounding,\nrepetitive behavior and shortcut-taking that agents performing WebDS' tasks\ndisplay. By providing a more robust and realistic testing ground, WebDS sets\nthe stage for significant advances in the development of practically useful\nLLM-based data science.", "AI": {"tldr": "WebDS\u662f\u7b2c\u4e00\u4e2a\u7aef\u5230\u7aefweb\u6570\u636e\u79d1\u5b66\u57fa\u51c6\uff0c\u5b83\u5305\u542b870\u4e2aweb\u6570\u636e\u79d1\u5b66\u4efb\u52a1\uff0c\u8fd9\u4e9b\u4efb\u52a1\u6765\u81ea29\u4e2a\u4e0d\u540c\u7684\u7f51\u7ad9\uff0c\u65e8\u5728\u6311\u6218agent\u6267\u884c\u590d\u6742\u7684\u3001\u591a\u6b65\u9aa4\u7684\u64cd\u4f5c\uff0c\u8fd9\u4e9b\u64cd\u4f5c\u9700\u8981\u4f7f\u7528\u5de5\u5177\u548c\u5f02\u6784\u6570\u636e\u683c\u5f0f\uff0c\u66f4\u597d\u5730\u53cd\u6620\u73b0\u4ee3\u6570\u636e\u5206\u6790\u7684\u73b0\u5b9e\u3002", "motivation": "\u73b0\u6709\u7684\u7f51\u7edc\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u4fa7\u91cd\u4e8e\u7b80\u5355\u7684\u4ea4\u4e92\uff0c\u800c\u4f20\u7edf\u7684\u6570\u636e\u79d1\u5b66\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u4fa7\u91cd\u4e8e\u9759\u6001\u7684\u3001\u901a\u5e38\u662f\u6587\u672c\u7ed1\u5b9a\u7684\u6570\u636e\u96c6\uff0c\u5e76\u4e14\u4e0d\u8bc4\u4f30\u5305\u542b\u6570\u636e\u83b7\u53d6\u3001\u6e05\u7406\u3001\u5206\u6790\u548c\u6d1e\u5bdf\u529b\u751f\u6210\u7684\u7aef\u5230\u7aef\u5de5\u4f5c\u6d41\u7a0b\u3002", "method": "WebDS", "result": "Browser Use\u5728Web Voyager\u4e0a\u5b8c\u6210\u4e8680%\u7684\u4efb\u52a1\uff0c\u4f46\u5728WebDS\u4e2d\u53ea\u6210\u529f\u5b8c\u6210\u4e8615%\u7684\u4efb\u52a1\uff0c\u8fd9\u8868\u660e\u7531\u4e8e\u4fe1\u606f\u57fa\u7840\u8584\u5f31\u3001\u91cd\u590d\u884c\u4e3a\u548c\u91c7\u7528\u6377\u5f84\u7b49\u65b0\u7684\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "WebDS\u7684\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u76ee\u524d\u6700\u5148\u8fdb\u7684LLM agent\u5728\u5b8c\u6210\u8fd9\u4e9b\u4efb\u52a1\u65b9\u9762\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\u3002"}}
{"id": "2508.01015", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.01015", "abs": "https://arxiv.org/abs/2508.01015", "authors": ["Byron Dowling", "Jozef Probcin", "Adam Czajka"], "title": "AutoSIGHT: Automatic Eye Tracking-based System for Immediate Grading of Human experTise", "comment": "This work has been accepted for publication in the proceedings of the\n  IEEE VL/HCC conference 2025. The final published version will be available\n  via IEEE Xplore", "summary": "Can we teach machines to assess the expertise of humans solving visual tasks\nautomatically based on eye tracking features? This paper proposes AutoSIGHT,\nAutomatic System for Immediate Grading of Human experTise, that classifies\nexpert and non-expert performers, and builds upon an ensemble of features\nextracted from eye tracking data while the performers were solving a visual\ntask. Results on the task of iris Presentation Attack Detection (PAD) used for\nthis study show that with a small evaluation window of just 5 seconds,\nAutoSIGHT achieves an average average Area Under the ROC curve performance of\n0.751 in subject-disjoint train-test regime, indicating that such detection is\nviable. Furthermore, when a larger evaluation window of up to 30 seconds is\navailable, the Area Under the ROC curve (AUROC) increases to 0.8306, indicating\nthe model is effectively leveraging more information at a cost of slightly\ndelayed decisions. This work opens new areas of research on how to incorporate\nthe automatic weighing of human and machine expertise into human-AI pairing\nsetups, which need to react dynamically to nonstationary expertise distribution\nbetween the human and AI players (e.g. when the experts need to be replaced, or\nthe task at hand changes rapidly). Along with this paper, we offer the eye\ntracking data used in this study collected from 6 experts and 53 non-experts\nsolving iris PAD visual task.", "AI": {"tldr": "AutoSIGHT\u4f7f\u7528\u773c\u52a8\u8ffd\u8e2a\u6570\u636e\u533a\u5206\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u4e13\u5bb6\u548c\u975e\u4e13\u5bb6\uff0c\u4e3a\u4eba\u7c7b\u4e0eAI\u534f\u4f5c\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002", "motivation": "\u80fd\u5426\u6559\u4f1a\u673a\u5668\u57fa\u4e8e\u773c\u52a8\u8ffd\u8e2a\u7279\u5f81\u81ea\u52a8\u8bc4\u4f30\u4eba\u7c7b\u89e3\u51b3\u89c6\u89c9\u4efb\u52a1\u7684\u4e13\u4e1a\u77e5\u8bc6\uff1f", "method": "AutoSIGHT\uff1a\u4e00\u4e2a\u81ea\u52a8\u7cfb\u7edf\uff0c\u7528\u4e8e\u5373\u65f6\u8bc4\u4f30\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\uff0c\u5b83\u57fa\u4e8e\u4ece\u773c\u52a8\u8ffd\u8e2a\u6570\u636e\u4e2d\u63d0\u53d6\u7684\u7279\u5f81\u96c6\u5408\uff0c\u5bf9\u4e13\u5bb6\u548c\u975e\u4e13\u5bb6\u6267\u884c\u8005\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5728\u8679\u819c\u5448\u73b0\u653b\u51fb\u68c0\u6d4b\uff08PAD\uff09\u4efb\u52a1\u4e2d\uff0cAutoSIGHT\u57285\u79d2\u7684\u8bc4\u4f30\u7a97\u53e3\u5185\uff0c\u5728subject-disjoint\u7684\u8bad\u7ec3\u6d4b\u8bd5\u673a\u5236\u4e0b\uff0c\u5e73\u5747ROC\u66f2\u7ebf\u4e0b\u9762\u79ef\u8fbe\u52300.751\u3002\u5f53\u8bc4\u4f30\u7a97\u53e3\u589e\u52a0\u523030\u79d2\u65f6\uff0cAUROC\u589e\u52a0\u52300.8306\u3002", "conclusion": "AutoSIGHT\u5728\u8679\u819c\u5448\u73b0\u653b\u51fb\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u533a\u5206\u4e13\u5bb6\u548c\u975e\u4e13\u5bb6\u7684\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u8f83\u957f\u7684\u8bc4\u4f30\u7a97\u53e3\u4e0b\u3002\u8be5\u7814\u7a76\u4e3a\u4eba\u7c7b\u4e0eAI\u7684\u914d\u5bf9\u8bbe\u7f6e\u4e2d\u81ea\u52a8\u8861\u91cf\u4eba\u7c7b\u548c\u673a\u5668\u4e13\u4e1a\u77e5\u8bc6\u5f00\u8f9f\u4e86\u65b0\u7684\u7814\u7a76\u9886\u57df\u3002"}}
{"id": "2508.01073", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01073", "abs": "https://arxiv.org/abs/2508.01073", "authors": ["Martin B\u00f6ckling", "Heiko Paulheim"], "title": "gpuRDF2vec -- Scalable GPU-based RDF2vec", "comment": "18 pages, ISWC 2025", "summary": "Generating Knowledge Graph (KG) embeddings at web scale remains challenging.\nAmong existing techniques, RDF2vec combines effectiveness with strong\nscalability. We present gpuRDF2vec, an open source library that harnesses\nmodern GPUs and supports multi-node execution to accelerate every stage of the\nRDF2vec pipeline. Extensive experiments on both synthetically generated graphs\nand real-world benchmarks show that gpuRDF2vec achieves up to a substantial\nspeedup over the currently fastest alternative, i.e., jRDF2vec. In a\nsingle-node setup, our walk-extraction phase alone outperforms pyRDF2vec,\nSparkKGML, and jRDF2vec by a substantial margin using random walks on large/\ndense graphs, and scales very well to longer walks, which typically lead to\nbetter quality embeddings. Our implementation of gpuRDF2vec enables\npractitioners and researchers to train high-quality KG embeddings on\nlarge-scale graphs within practical time budgets and builds on top of Pytorch\nLightning for the scalable word2vec implementation.", "AI": {"tldr": "gpuRDF2vec is a fast, open-source library for generating knowledge graph embeddings at web scale using GPUs.", "motivation": "Generating Knowledge Graph (KG) embeddings at web scale remains challenging.", "method": "We present gpuRDF2vec, an open source library that harnesses modern GPUs and supports multi-node execution to accelerate every stage of the RDF2vec pipeline.", "result": "gpuRDF2vec achieves up to a substantial speedup over the currently fastest alternative, i.e., jRDF2vec. In a single-node setup, our walk-extraction phase alone outperforms pyRDF2vec, SparkKGML, and jRDF2vec by a substantial margin using random walks on large/ dense graphs, and scales very well to longer walks, which typically lead to better quality embeddings.", "conclusion": "gpuRDF2vec enables practitioners and researchers to train high-quality KG embeddings on large-scale graphs within practical time budgets and builds on top of Pytorch Lightning for the scalable word2vec implementation."}}
{"id": "2508.00886", "categories": ["cs.LG", "math.OC", "90C22, 93C10, 28A99"], "pdf": "https://arxiv.org/pdf/2508.00886", "abs": "https://arxiv.org/abs/2508.00886", "authors": ["Etienne Buehrle", "Christoph Stiller"], "title": "Stochastic Optimal Control via Measure Relaxations", "comment": "7 pages, 4 figures", "summary": "The optimal control problem of stochastic systems is commonly solved via\nrobust or scenario-based optimization methods, which are both challenging to\nscale to long optimization horizons. We cast the optimal control problem of a\nstochastic system as a convex optimization problem over occupation measures. We\ndemonstrate our method on a set of synthetic and real-world scenarios, learning\ncost functions from data via Christoffel polynomials. The code for our\nexperiments is available at https://github.com/ebuehrle/dpoc.", "AI": {"tldr": "casting the optimal control problem of a stochastic system as a convex optimization problem over occupation measures to  solve the optimal control problem of stochastic systems", "motivation": "The optimal control problem of stochastic systems is commonly solved via robust or scenario-based optimization methods, which are both challenging to scale to long optimization horizons.", "method": "casting the optimal control problem of a stochastic system as a convex optimization problem over occupation measures", "result": "demonstrate our method on a set of synthetic and real-world scenarios, learning cost functions from data via Christoffel polynomials.", "conclusion": "The optimal control problem of a stochastic system as a convex optimization problem over occupation measures."}}
{"id": "2508.02096", "categories": ["cs.IR", "cs.AI", "cs.HC", "H.3.3; H.5.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2508.02096", "abs": "https://arxiv.org/abs/2508.02096", "authors": ["Raj Mahmud", "Yufeng Wu", "Abdullah Bin Sawad", "Shlomo Berkovsky", "Mukesh Prasad", "A. Baki Kocaballi"], "title": "Evaluating User Experience in Conversational Recommender Systems: A Systematic Review Across Classical and LLM-Powered Approaches", "comment": "Accepted at OZCHI 2025. 23 pages, 1 figure, 5 tables", "summary": "Conversational Recommender Systems (CRSs) are receiving growing research\nattention across domains, yet their user experience (UX) evaluation remains\nlimited. Existing reviews largely overlook empirical UX studies, particularly\nin adaptive and large language model (LLM)-based CRSs. To address this gap, we\nconducted a systematic review following PRISMA guidelines, synthesising 23\nempirical studies published between 2017 and 2025. We analysed how UX has been\nconceptualised, measured, and shaped by domain, adaptivity, and LLM.\n  Our findings reveal persistent limitations: post hoc surveys dominate,\nturn-level affective UX constructs are rarely assessed, and adaptive behaviours\nare seldom linked to UX outcomes. LLM-based CRSs introduce further challenges,\nincluding epistemic opacity and verbosity, yet evaluations infrequently address\nthese issues. We contribute a structured synthesis of UX metrics, a comparative\nanalysis of adaptive and nonadaptive systems, and a forward-looking agenda for\nLLM-aware UX evaluation. These findings support the development of more\ntransparent, engaging, and user-centred CRS evaluation practices.", "AI": {"tldr": "\u5bf9\u4f1a\u8bdd\u5f0f\u63a8\u8350\u7cfb\u7edf\u7684\u7528\u6237\u4f53\u9a8c\u8bc4\u4f30\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u56de\u987e\uff0c\u53d1\u73b0\u4e86\u4e00\u4e9b\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7684LLM\u611f\u77e5\u7528\u6237\u4f53\u9a8c\u8bc4\u4f30\u63d0\u51fa\u4e86\u5efa\u8bae\u3002", "motivation": "\u4f1a\u8bdd\u63a8\u8350\u7cfb\u7edf\uff08CRS\uff09\u5728\u5404\u4e2a\u9886\u57df\u53d7\u5230\u8d8a\u6765\u8d8a\u591a\u7684\u7814\u7a76\u5173\u6ce8\uff0c\u4f46\u5176\u7528\u6237\u4f53\u9a8c\uff08UX\uff09\u8bc4\u4f30\u4ecd\u7136\u6709\u9650\u3002\u73b0\u6709\u7684\u8bc4\u8bba\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u5ffd\u7565\u4e86\u5b9e\u8bc1\u7528\u6237\u4f53\u9a8c\u7814\u7a76\uff0c\u5c24\u5176\u662f\u5728\u81ea\u9002\u5e94\u548c\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684CRS\u4e2d\u3002", "method": "\u5bf92017\u5e74\u81f32025\u5e74\u95f4\u53d1\u8868\u768423\u9879\u5b9e\u8bc1\u7814\u7a76\u8fdb\u884c\u4e86\u7cfb\u7edf\u56de\u987e\uff0c\u9075\u5faaPRISMA\u6307\u5357\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u6301\u7eed\u5b58\u5728\u7684\u5c40\u9650\u6027\uff1a\u4e8b\u540e\u8c03\u67e5\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u5f88\u5c11\u8bc4\u4f30turn-level\u60c5\u611f\u7528\u6237\u4f53\u9a8c\u7ed3\u6784\uff0c\u5e76\u4e14\u81ea\u9002\u5e94\u884c\u4e3a\u5f88\u5c11\u4e0e\u7528\u6237\u4f53\u9a8c\u7ed3\u679c\u76f8\u5173\u8054\u3002\u57fa\u4e8eLLM\u7684CRS\u5f15\u5165\u4e86\u8fdb\u4e00\u6b65\u7684\u6311\u6218\uff0c\u5305\u62ec\u8ba4\u77e5\u4e0d\u900f\u660e\u6027\u548c\u5197\u957f\u6027\uff0c\u4f46\u8bc4\u4f30\u5f88\u5c11\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "conclusion": "\u603b\u7ed3\u4e86\u7528\u6237\u4f53\u9a8c\u6307\u6807\uff0c\u5bf9\u81ea\u9002\u5e94\u548c\u975e\u81ea\u9002\u5e94\u7cfb\u7edf\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\uff0c\u5e76\u4e3aLLM\u611f\u77e5\u7528\u6237\u4f53\u9a8c\u8bc4\u4f30\u63d0\u51fa\u4e86\u524d\u77bb\u6027\u8bae\u7a0b\u3002\u8fd9\u4e9b\u53d1\u73b0\u652f\u6301\u5f00\u53d1\u66f4\u900f\u660e\u3001\u66f4\u5177\u5438\u5f15\u529b\u4e14\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684CRS\u8bc4\u4f30\u5b9e\u8df5\u3002"}}
{"id": "2508.01245", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01245", "abs": "https://arxiv.org/abs/2508.01245", "authors": ["Yue Chen", "Minghua He", "Fangkai Yang", "Pu Zhao", "Lu Wang", "Yu Kang", "Yifei Dong", "Yuefeng Zhan", "Hao Sun", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "title": "WarriorMath: Enhancing the Mathematical Ability of Large Language Models with a Defect-aware Framework", "comment": null, "summary": "Large Language Models (LLMs) excel in solving mathematical problems, yet\ntheir performance is often limited by the availability of high-quality, diverse\ntraining data. Existing methods focus on augmenting datasets through rephrasing\nor difficulty progression but overlook the specific failure modes of LLMs. This\nresults in synthetic questions that the model can already solve, providing\nminimal performance gains. To address this, we propose WarriorMath, a\ndefect-aware framework for mathematical problem solving that integrates both\ntargeted data synthesis and progressive training. In the synthesis stage, we\nemploy multiple expert LLMs in a collaborative process to generate, critique,\nand refine problems. Questions that base LLMs fail to solve are identified and\niteratively improved through expert-level feedback, producing high-quality,\ndefect-aware training data. In the training stage, we introduce a progressive\nlearning framework that iteratively fine-tunes the model using increasingly\nchallenging data tailored to its weaknesses. Experiments on six mathematical\nbenchmarks show that WarriorMath outperforms strong baselines by 12.57% on\naverage, setting a new state-of-the-art. Our results demonstrate the\neffectiveness of a defect-aware, multi-expert framework for improving\nmathematical ability.", "AI": {"tldr": "WarriorMath \u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684\u6570\u636e\u5408\u6210\u548c\u6e10\u8fdb\u5f0f\u8bad\u7ec3\uff0c\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf 12.57%\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u901a\u8fc7\u6539\u8ff0\u6216\u96be\u5ea6\u9012\u589e\u6765\u6269\u5145\u6570\u636e\u96c6\uff0c\u4f46\u5ffd\u7565\u4e86LLM\u7684\u7279\u5b9a\u5931\u8d25\u6a21\u5f0f\u3002\u8fd9\u5bfc\u81f4\u5408\u6210\u7684\u95ee\u9898\u662f\u6a21\u578b\u5df2\u7ecf\u53ef\u4ee5\u89e3\u51b3\u7684\uff0c\u63d0\u4f9b\u7684\u6027\u80fd\u589e\u76ca\u6700\u5c0f\u3002", "method": "\u4e00\u4e2a\u7f3a\u9677\u611f\u77e5\u6570\u5b66\u95ee\u9898\u6c42\u89e3\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u6709\u9488\u5bf9\u6027\u7684\u6570\u636e\u5408\u6210\u548c\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u3002", "result": "\u5728\u516d\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cWarriorMath \u7684\u6027\u80fd\u5e73\u5747\u6bd4\u5f3a\u5927\u7684\u57fa\u7ebf\u9ad8\u51fa 12.57%\uff0c\u521b\u9020\u4e86\u65b0\u7684\u6280\u672f\u6c34\u5e73\u3002", "conclusion": "\u4f7f\u7528\u7f3a\u9677\u611f\u77e5\u3001\u591a\u4e13\u5bb6\u6846\u67b6\u80fd\u6709\u6548\u63d0\u9ad8\u6570\u5b66\u80fd\u529b\u3002"}}
{"id": "2508.01019", "categories": ["cs.CV", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.01019", "abs": "https://arxiv.org/abs/2508.01019", "authors": ["Muhammad Zeeshan", "Umer Zaki", "Syed Ahmed Pasha", "Zaar Khizar"], "title": "3D Reconstruction via Incremental Structure From Motion", "comment": "8 pages, 8 figures, proceedings in International Bhurban Conference\n  on Applied Sciences & Technology (IBCAST) 2025", "summary": "Accurate 3D reconstruction from unstructured image collections is a key\nrequirement in applications such as robotics, mapping, and scene understanding.\nWhile global Structure from Motion (SfM) techniques rely on full image\nconnectivity and can be sensitive to noise or missing data, incremental SfM\noffers a more flexible alternative. By progressively incorporating new views\ninto the reconstruction, it enables the system to recover scene structure and\ncamera motion even in sparse or partially overlapping datasets. In this paper,\nwe present a detailed implementation of the incremental SfM pipeline, focusing\non the consistency of geometric estimation and the effect of iterative\nrefinement through bundle adjustment. We demonstrate the approach using a real\ndataset and assess reconstruction quality through reprojection error and camera\ntrajectory coherence. The results support the practical utility of incremental\nSfM as a reliable method for sparse 3D reconstruction in visually structured\nenvironments.", "AI": {"tldr": "Incremental SfM for robust 3D reconstruction from images.", "motivation": "Accurate 3D reconstruction from unstructured image collections is a key requirement in applications such as robotics, mapping, and scene understanding. Global SfM techniques rely on full image connectivity and can be sensitive to noise or missing data, incremental SfM offers a more flexible alternative.", "method": "Detailed implementation of the incremental SfM pipeline, focusing on the consistency of geometric estimation and the effect of iterative refinement through bundle adjustment.", "result": "Demonstrated the approach using a real dataset and assess reconstruction quality through reprojection error and camera trajectory coherence.", "conclusion": "Incremental SfM is a reliable method for sparse 3D reconstruction in visually structured environments."}}
{"id": "2508.01097", "categories": ["cs.AI", "nlin.AO", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2508.01097", "abs": "https://arxiv.org/abs/2508.01097", "authors": ["Neil F. Johnson", "Frank Yingjie Huo"], "title": "Multispin Physics of AI Tipping Points and Hallucinations", "comment": null, "summary": "Output from generative AI such as ChatGPT, can be repetitive and biased. But\nmore worrying is that this output can mysteriously tip mid-response from good\n(correct) to bad (misleading or wrong) without the user noticing. In 2024\nalone, this reportedly caused $67 billion in losses and several deaths.\nEstablishing a mathematical mapping to a multispin thermal system, we reveal a\nhidden tipping instability at the scale of the AI's 'atom' (basic Attention\nhead). We derive a simple but essentially exact formula for this tipping point\nwhich shows directly the impact of a user's prompt choice and the AI's training\nbias. We then show how the output tipping can get amplified by the AI's\nmultilayer architecture. As well as helping improve AI transparency,\nexplainability and performance, our results open a path to quantifying users'\nAI risk and legal liabilities.", "AI": {"tldr": "\u751f\u6210\u5f0f AI \u7684\u8f93\u51fa\u53ef\u80fd\u5728\u4e2d\u9014\u51fa\u9519\uff0c\u5bfc\u81f4\u5de8\u5927\u635f\u5931\u3002\u8be5\u7814\u7a76\u63ed\u793a\u4e86 AI \u6a21\u578b\u4e2d\u9690\u85cf\u7684 tipping instability\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u516c\u5f0f\u6765\u9884\u6d4b tipping point\u3002", "motivation": "ChatGPT \u7b49\u751f\u6210\u5f0f AI \u7684\u8f93\u51fa\u53ef\u80fd\u5177\u6709\u91cd\u590d\u6027\u548c\u504f\u5dee\uff0c\u66f4\u4ee4\u4eba\u62c5\u5fe7\u7684\u662f\uff0c\u8fd9\u79cd\u8f93\u51fa\u53ef\u80fd\u4f1a\u5728\u7528\u6237\u6ca1\u6709\u6ce8\u610f\u5230\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u4e2d\u9014\u4ece\u597d\uff08\u6b63\u786e\uff09\u53d8\u4e3a\u574f\uff08\u8bef\u5bfc\u6216\u9519\u8bef\uff09\u3002\u636e\u62a5\u9053\uff0c\u4ec5\u5728 2024 \u5e74\uff0c\u8fd9\u9020\u6210\u4e86 670 \u4ebf\u7f8e\u5143\u7684\u635f\u5931\u548c\u6570\u8d77\u6b7b\u4ea1\u3002", "method": "\u901a\u8fc7\u5efa\u7acb\u4e00\u4e2a\u5230\u591a\u81ea\u65cb\u70ed\u529b\u7cfb\u7edf\u7684\u6570\u5b66\u6620\u5c04", "result": "\u63a8\u5bfc\u51fa\u4e00\u4e2a\u7b80\u5355\u4f46\u57fa\u672c\u4e0a\u7cbe\u786e\u7684 tipping point \u516c\u5f0f\uff0c\u8be5\u516c\u5f0f\u76f4\u63a5\u663e\u793a\u4e86\u7528\u6237 prompt \u9009\u62e9\u548c AI \u8bad\u7ec3\u504f\u5dee\u7684\u5f71\u54cd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8f93\u51fa tipping \u53ef\u80fd\u4f1a\u88ab AI \u7684\u591a\u5c42\u67b6\u6784\u653e\u5927\u3002", "conclusion": "\u63ed\u793a\u4e86 AI \u6a21\u578b\u4e2d\u9690\u85cf\u7684 tipping instability\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u516c\u5f0f\u6765\u9884\u6d4b tipping point\uff0c\u8fd9\u6709\u52a9\u4e8e\u63d0\u9ad8 AI \u7684\u900f\u660e\u5ea6\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6027\u80fd\uff0c\u5e76\u4e3a\u91cf\u5316\u7528\u6237\u7684 AI \u98ce\u9669\u548c\u6cd5\u5f8b\u8d23\u4efb\u5f00\u8f9f\u4e86\u9053\u8def\u3002"}}
{"id": "2508.00887", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00887", "abs": "https://arxiv.org/abs/2508.00887", "authors": ["Binrui Shen", "Yuan Liang", "Shengxin Zhu"], "title": "FRAM: Frobenius-Regularized Assignment Matching with Mixed-Precision Computing", "comment": null, "summary": "Graph matching, typically formulated as a Quadratic Assignment Problem (QAP),\nseeks to establish node correspondences between two graphs. To address the\nNP-hardness of QAP, some existing methods adopt projection-based relaxations\nthat embed the problem into the convex hull of the discrete domain. However,\nthese relaxations inevitably enlarge the feasible set, introducing two sources\nof error: numerical scale sensitivity and geometric misalignment between the\nrelaxed and original domains. To alleviate these errors, we propose a novel\nrelaxation framework by reformulating the projection step as a\nFrobenius-regularized Linear Assignment (FRA) problem, where a tunable\nregularization term mitigates feasible region inflation. This formulation\nenables normalization-based operations to preserve numerical scale invariance\nwithout compromising accuracy. To efficiently solve FRA, we propose the Scaling\nDoubly Stochastic Normalization (SDSN) algorithm. Building on its favorable\ncomputational properties, we develop a theoretically grounded mixed-precision\narchitecture to achieve substantial acceleration. Comprehensive CPU-based\nbenchmarks demonstrate that FRAM consistently outperforms all baseline methods\nunder identical precision settings. When combined with a GPU-based\nmixed-precision architecture, FRAM achieves up to 370X speedup over its\nCPU-FP64 counterpart, with negligible loss in solution accuracy.", "AI": {"tldr": "This paper introduces a novel relaxation framework to address the NP-hardness of QAP in graph matching by reformulating the projection step as a Frobenius-regularized Linear Assignment (FRA) problem. The method achieves significant speedup with negligible loss in accuracy.", "motivation": "To address the NP-hardness of QAP, some existing methods adopt projection-based relaxations that embed the problem into the convex hull of the discrete domain. However, these relaxations inevitably enlarge the feasible set, introducing two sources of error: numerical scale sensitivity and geometric misalignment between the relaxed and original domains.", "method": "reformulating the projection step as a Frobenius-regularized Linear Assignment (FRA) problem, where a tunable regularization term mitigates feasible region inflation. To efficiently solve FRA, we propose the Scaling Doubly Stochastic Normalization (SDSN) algorithm.", "result": "FRAM consistently outperforms all baseline methods under identical precision settings.", "conclusion": "FRAM achieves up to 370X speedup over its CPU-FP64 counterpart, with negligible loss in solution accuracy."}}
{"id": "2508.02222", "categories": ["cs.IR", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2508.02222", "abs": "https://arxiv.org/abs/2508.02222", "authors": ["Xuan Xu", "Beilin Chu", "Qinhong Lin", "Yixiao Zhong", "Fufang Wen", "Jiaqi Liu", "Binjie Fei", "Yu Li", "Zhongliang Yang", "Linna Zhou"], "title": "FinCPRG: A Bidirectional Generation Pipeline for Hierarchical Queries and Rich Relevance in Financial Chinese Passage Retrieval", "comment": null, "summary": "In recent years, large language models (LLMs) have demonstrated significant\npotential in constructing passage retrieval datasets. However, existing methods\nstill face limitations in expressing cross-doc query needs and controlling\nannotation quality. To address these issues, this paper proposes a\nbidirectional generation pipeline, which aims to generate 3-level hierarchical\nqueries for both intra-doc and cross-doc scenarios and mine additional\nrelevance labels on top of direct mapping annotation. The pipeline introduces\ntwo query generation methods: bottom-up from single-doc text and top-down from\nmulti-doc titles. The bottom-up method uses LLMs to disassemble and generate\nstructured queries at both sentence-level and passage-level simultaneously from\nintra-doc passages. The top-down approach incorporates three key financial\nelements--industry, topic, and time--to divide report titles into clusters and\nprompts LLMs to generate topic-level queries from each cluster. For relevance\nannotation, our pipeline not only relies on direct mapping annotation from the\ngeneration relationship but also implements an indirect positives mining method\nto enrich the relevant query-passage pairs. Using this pipeline, we constructed\na Financial Passage Retrieval Generated dataset (FinCPRG) from almost 1.3k\nChinese financial research reports, which includes hierarchical queries and\nrich relevance labels. Through evaluations of mined relevance labels,\nbenchmarking and training experiments, we assessed the quality of FinCPRG and\nvalidated its effectiveness as a passage retrieval dataset for both training\nand benchmarking.", "AI": {"tldr": "This paper introduces a new pipeline to generate a financial passage retrieval dataset (FinCPRG) with hierarchical queries and rich relevance labels, addressing limitations in existing methods for cross-doc query needs and annotation quality control.", "motivation": "existing methods still face limitations in expressing cross-doc query needs and controlling annotation quality", "method": "a bidirectional generation pipeline, which aims to generate 3-level hierarchical queries for both intra-doc and cross-doc scenarios and mine additional relevance labels on top of direct mapping annotation. The pipeline introduces two query generation methods: bottom-up from single-doc text and top-down from multi-doc titles.", "result": "constructed a Financial Passage Retrieval Generated dataset (FinCPRG) from almost 1.3k Chinese financial research reports, which includes hierarchical queries and rich relevance labels", "conclusion": "We assessed the quality of FinCPRG and validated its effectiveness as a passage retrieval dataset for both training and benchmarking."}}
{"id": "2508.01263", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01263", "abs": "https://arxiv.org/abs/2508.01263", "authors": ["Long S. T. Nguyen", "Khang H. N. Vo", "Thu H. A. Nguyen", "Tuan C. Bui", "Duc Q. Nguyen", "Thanh-Tung Tran", "Anh D. Nguyen", "Minh L. Nguyen", "Fabien Baldacci", "Thang H. Bui", "Emanuel Di Nardo", "Angelo Ciaramella", "Son H. Le", "Ihsan Ullah", "Lorenzo Di Rocco", "Tho T. Quan"], "title": "Bridging LLMs and Symbolic Reasoning in Educational QA Systems: Insights from the XAI Challenge at IJCNN 2025", "comment": "The XAI Challenge @ TRNS-AI Workshop, IJCNN 2025: Explainable AI for\n  Educational Question Answering. Website:\n  https://sites.google.com/view/trns-ai/challenge/", "summary": "The growing integration of Artificial Intelligence (AI) into education has\nintensified the need for transparency and interpretability. While hackathons\nhave long served as agile environments for rapid AI prototyping, few have\ndirectly addressed eXplainable AI (XAI) in real-world educational contexts.\nThis paper presents a comprehensive analysis of the XAI Challenge 2025, a\nhackathon-style competition jointly organized by Ho Chi Minh City University of\nTechnology (HCMUT) and the International Workshop on Trustworthiness and\nReliability in Neurosymbolic AI (TRNS-AI), held as part of the International\nJoint Conference on Neural Networks (IJCNN 2025). The challenge tasked\nparticipants with building Question-Answering (QA) systems capable of answering\nstudent queries about university policies while generating clear, logic-based\nnatural language explanations. To promote transparency and trustworthiness,\nsolutions were required to use lightweight Large Language Models (LLMs) or\nhybrid LLM-symbolic systems. A high-quality dataset was provided, constructed\nvia logic-based templates with Z3 validation and refined through expert student\nreview to ensure alignment with real-world academic scenarios. We describe the\nchallenge's motivation, structure, dataset construction, and evaluation\nprotocol. Situating the competition within the broader evolution of AI\nhackathons, we argue that it represents a novel effort to bridge LLMs and\nsymbolic reasoning in service of explainability. Our findings offer actionable\ninsights for future XAI-centered educational systems and competitive research\ninitiatives.", "AI": {"tldr": "XAI Challenge 2025 was a hackathon that promoted explainable AI in education by building question-answering systems with clear, logic-based explanations using LLMs and symbolic systems.", "motivation": "The paper addresses the need for transparency and interpretability in AI in education, highlighting the lack of XAI focus in traditional AI hackathons.", "method": "The paper analyzes the XAI Challenge 2025, detailing its structure, dataset construction using logic-based templates with Z3 validation and expert student review, and evaluation protocol.", "result": "The XAI Challenge 2025 successfully bridged LLMs and symbolic reasoning to enhance explainability in educational QA systems.", "conclusion": "This paper presents findings and actionable insights from the XAI Challenge 2025, a hackathon focused on building explainable question-answering systems for education, offering guidance for future XAI educational systems and research."}}
{"id": "2508.01045", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.01045", "abs": "https://arxiv.org/abs/2508.01045", "authors": ["Theo Di Piazza", "Carole Lazarus", "Olivier Nempont", "Loic Boussel"], "title": "Structured Spectral Graph Learning for Anomaly Classification in 3D Chest CT Scans", "comment": "Accepted for publication at MICCAI 2025 EMERGE Workshop", "summary": "With the increasing number of CT scan examinations, there is a need for\nautomated methods such as organ segmentation, anomaly detection and report\ngeneration to assist radiologists in managing their increasing workload.\nMulti-label classification of 3D CT scans remains a critical yet challenging\ntask due to the complex spatial relationships within volumetric data and the\nvariety of observed anomalies. Existing approaches based on 3D convolutional\nnetworks have limited abilities to model long-range dependencies while Vision\nTransformers suffer from high computational costs and often require extensive\npre-training on large-scale datasets from the same domain to achieve\ncompetitive performance. In this work, we propose an alternative by introducing\na new graph-based approach that models CT scans as structured graphs,\nleveraging axial slice triplets nodes processed through spectral domain\nconvolution to enhance multi-label anomaly classification performance. Our\nmethod exhibits strong cross-dataset generalization, and competitive\nperformance while achieving robustness to z-axis translation. An ablation study\nevaluates the contribution of each proposed component.", "AI": {"tldr": "This paper presents a graph-based approach for multi-label anomaly classification in CT scans that is robust, generalizable, and performs well.", "motivation": "The increasing number of CT scan examinations necessitates automated methods to assist radiologists. Existing 3D convolutional networks have limited abilities to model long-range dependencies, and Vision Transformers suffer from high computational costs and require extensive pre-training.", "method": "The method models CT scans as structured graphs, leveraging axial slice triplets nodes processed through spectral domain convolution.", "result": "The proposed method exhibits strong cross-dataset generalization, competitive performance, and robustness to z-axis translation. An ablation study evaluates the contribution of each proposed component.", "conclusion": "This paper introduces a graph-based approach for multi-label anomaly classification in 3D CT scans, demonstrating strong cross-dataset generalization, competitive performance, and robustness to z-axis translation."}}
{"id": "2508.01109", "categories": ["cs.AI", "68T07", "I.2; J.4"], "pdf": "https://arxiv.org/pdf/2508.01109", "abs": "https://arxiv.org/abs/2508.01109", "authors": ["Satiyabooshan Murugaboopathy", "Connor T. Jerzak", "Adel Daoud"], "title": "Platonic Representations for Poverty Mapping: Unified Vision-Language Codes or Agent-Induced Novelty?", "comment": "7 figures", "summary": "We investigate whether socio-economic indicators like household wealth leave\nrecoverable imprints in satellite imagery (capturing physical features) and\nInternet-sourced text (reflecting historical/economic narratives). Using\nDemographic and Health Survey (DHS) data from African neighborhoods, we pair\nLandsat images with LLM-generated textual descriptions conditioned on\nlocation/year and text retrieved by an AI search agent from web sources. We\ndevelop a multimodal framework predicting household wealth (International\nWealth Index) through five pipelines: (i) vision model on satellite images,\n(ii) LLM using only location/year, (iii) AI agent searching/synthesizing web\ntext, (iv) joint image-text encoder, (v) ensemble of all signals. Our framework\nyields three contributions. First, fusing vision and agent/LLM text outperforms\nvision-only baselines in wealth prediction (e.g., R-squared of 0.77 vs. 0.63 on\nout-of-sample splits), with LLM-internal knowledge proving more effective than\nagent-retrieved text, improving robustness to out-of-country and out-of-time\ngeneralization. Second, we find partial representational convergence: fused\nembeddings from vision/language modalities correlate moderately (median cosine\nsimilarity of 0.60 after alignment), suggesting a shared latent code of\nmaterial well-being while retaining complementary details, consistent with the\nPlatonic Representation Hypothesis. Although LLM-only text outperforms\nagent-retrieved data, challenging our Agent-Induced Novelty Hypothesis, modest\ngains from combining agent data in some splits weakly support the notion that\nagent-gathered information introduces unique representational structures not\nfully captured by static LLM knowledge. Third, we release a large-scale\nmultimodal dataset comprising more than 60,000 DHS clusters linked to satellite\nimages, LLM-generated descriptions, and agent-retrieved texts.", "AI": {"tldr": "Using satellite imagery and internet text, a multimodal framework is developed to predict household wealth. Fusing vision and language data improves prediction accuracy. A new dataset is released.", "motivation": "Investigating whether socio-economic indicators leave recoverable imprints in satellite imagery and Internet-sourced text.", "method": "A multimodal framework predicting household wealth through five pipelines: (i) vision model on satellite images, (ii) LLM using only location/year, (iii) AI agent searching/synthesizing web text, (iv) joint image-text encoder, (v) ensemble of all signals.", "result": "Fusing vision and agent/LLM text outperforms vision-only baselines in wealth prediction. There is partial representational convergence between vision/language modalities. LLM-internal knowledge is more effective than agent-retrieved text. A large-scale multimodal dataset is released.", "conclusion": "Fusing vision and agent/LLM text outperforms vision-only baselines in wealth prediction. There is partial representational convergence between vision/language modalities. A large-scale multimodal dataset is released."}}
{"id": "2508.00888", "categories": ["cs.LG", "stat.AP", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2508.00888", "abs": "https://arxiv.org/abs/2508.00888", "authors": ["Amir Hossein Kalantari", "Eleonora Papadimitriou", "Amir Pooyan Afghari"], "title": "A Dynamic, Context-Aware Framework for Risky Driving Prediction Using Naturalistic Data", "comment": "32 pages", "summary": "Naturalistic driving studies offer a powerful means for observing and\nquantifying real-world driving behaviour. One of their prominent applications\nin traffic safety is the continuous monitoring and classification of risky\ndriving behaviour. However, many existing frameworks rely on fixed time windows\nand static thresholds for distinguishing between safe and risky behaviour -\nlimiting their ability to respond to the stochastic nature of real-world\ndriving. This study proposes a dynamic and individualised framework for\nidentifying risky driving behaviour using Belgian naturalistic driving data.\nThe approach leverages a rolling time window and bi-level optimisation to\ndynamically calibrate both risk thresholds and model hyperparameters, capturing\nsubtle behavioural shifts. Two safety indicators, speed-weighted headway and\nharsh driving events, were evaluated using three data-driven models: Random\nForest, XGBoost, and Deep Neural Network (DNN). The DNN demonstrated strong\ncapability in capturing subtle changes in driving behaviour, particularly\nexcelling in high-recall tasks, making it promising for early-stage risk\ndetection. XGBoost provided the most balanced and stable performance across\ndifferent thresholds and evaluation metrics. While random forest showed more\nvariability, it responded sensitively to dynamic threshold adjustments, which\nmay be advantageous during model adaptation or tuning. Speed-weighted headway\nemerged as a more stable and context-sensitive risk indicator than harsh\ndriving events, likely due to its robustness to label sparsity and contextual\nvariation. Overall, the findings support the value of adaptive, personalised\nrisk detection approaches for enhancing real-time safety feedback and tailoring\ndriver support in intelligent transport systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u548c\u4e2a\u6027\u5316\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522b\u5371\u9669\u9a7e\u9a76\u884c\u4e3a\uff0c\u4f7f\u7528\u6bd4\u5229\u65f6\u81ea\u7136\u9a7e\u9a76\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u7684\u6846\u67b6\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u7684\u65f6\u95f4\u7a97\u53e3\u548c\u9759\u6001\u9608\u503c\u6765\u533a\u5206\u5b89\u5168\u548c\u5371\u9669\u884c\u4e3a\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5bf9\u73b0\u5b9e\u4e16\u754c\u9a7e\u9a76\u7684\u968f\u673a\u6027\u8d28\u7684\u53cd\u5e94\u80fd\u529b\u3002", "method": "\u5229\u7528\u6eda\u52a8\u65f6\u95f4\u7a97\u53e3\u548c\u53cc\u5c42\u4f18\u5316\u6765\u52a8\u6001\u6821\u51c6\u98ce\u9669\u9608\u503c\u548c\u6a21\u578b\u8d85\u53c2\u6570\u3002", "result": "DNN \u5728\u6355\u6349\u9a7e\u9a76\u884c\u4e3a\u7684\u7ec6\u5fae\u53d8\u5316\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u9ad8\u53ec\u56de\u7387\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f7f\u5176\u5728\u65e9\u671f\u98ce\u9669\u68c0\u6d4b\u65b9\u9762\u5f88\u6709\u524d\u666f\u3002XGBoost \u5728\u4e0d\u540c\u7684\u9608\u503c\u548c\u8bc4\u4f30\u6307\u6807\u4e2d\u63d0\u4f9b\u4e86\u6700\u5e73\u8861\u548c\u7a33\u5b9a\u7684\u6027\u80fd\u3002\u867d\u7136\u968f\u673a\u68ee\u6797\u663e\u793a\u51fa\u66f4\u591a\u7684\u53ef\u53d8\u6027\uff0c\u4f46\u5b83\u5bf9\u52a8\u6001\u9608\u503c\u8c03\u6574\u53cd\u5e94\u7075\u654f\uff0c\u8fd9\u5728\u6a21\u578b\u9002\u5e94\u6216\u8c03\u6574\u671f\u95f4\u53ef\u80fd\u662f\u6709\u5229\u7684\u3002\u901f\u5ea6\u52a0\u6743\u8f66\u5934\u65f6\u8ddd\u6bd4\u6076\u52a3\u9a7e\u9a76\u4e8b\u4ef6\u66f4\u7a33\u5b9a\u548c\u5bf9\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u98ce\u9669\u6307\u6807\uff0c\u8fd9\u53ef\u80fd\u662f\u7531\u4e8e\u5176\u5bf9\u6807\u7b7e\u7a00\u758f\u6027\u548c\u4e0a\u4e0b\u6587\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u81ea\u9002\u5e94\u7684\u3001\u4e2a\u6027\u5316\u7684\u98ce\u9669\u68c0\u6d4b\u65b9\u6cd5\u5bf9\u4e8e\u589e\u5f3a\u5b9e\u65f6\u5b89\u5168\u53cd\u9988\u548c\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u5b9a\u5236\u9a7e\u9a76\u5458\u652f\u6301\u5177\u6709\u4ef7\u503c\u3002"}}
{"id": "2508.02242", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.02242", "abs": "https://arxiv.org/abs/2508.02242", "authors": ["Kaike Zhang", "Xiaobei Wang", "Xiaoyu Liu", "Shuchang Liu", "Hailan Yang", "Xiang Li", "Fei Sun", "Qi Cao"], "title": "From Generation to Consumption: Personalized List Value Estimation for Re-ranking", "comment": null, "summary": "Re-ranking is critical in recommender systems for optimizing the order of\nrecommendation lists, thus improving user satisfaction and platform revenue.\nMost existing methods follow a generator-evaluator paradigm, where the\nevaluator estimates the overall value of each candidate list. However, they\noften ignore the fact that users may exit before consuming the full list,\nleading to a mismatch between estimated generation value and actual consumption\nvalue. To bridge this gap, we propose CAVE, a personalized Consumption-Aware\nlist Value Estimation framework. CAVE formulates the list value as the\nexpectation over sub-list values, weighted by user-specific exit probabilities\nat each position. The exit probability is decomposed into an interest-driven\ncomponent and a stochastic component, the latter modeled via a Weibull\ndistribution to capture random external factors such as fatigue. By jointly\nmodeling sub-list values and user exit behavior, CAVE yields a more faithful\nestimate of actual list consumption value. We further contribute three\nlarge-scale real-world list-wise benchmarks from the Kuaishou platform, varying\nin size and user activity patterns. Extensive experiments on these benchmarks,\ntwo Amazon datasets, and online A/B testing on Kuaishou show that CAVE\nconsistently outperforms strong baselines, highlighting the benefit of\nexplicitly modeling user exits in re-ranking.", "AI": {"tldr": "CAVE \u662f\u4e00\u79cd\u4e2a\u6027\u5316\u7684\u6d88\u8d39\u611f\u77e5\u5217\u8868\u503c\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u7528\u6237\u9000\u51fa\u884c\u4e3a\u6765\u66f4\u51c6\u786e\u5730\u4f30\u8ba1\u5217\u8868\u6d88\u8d39\u4ef7\u503c\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u4e86\u7528\u6237\u53ef\u80fd\u5728\u6d88\u8d39\u5b8c\u6574\u5217\u8868\u4e4b\u524d\u9000\u51fa\u7684\u4e8b\u5b9e\uff0c\u4ece\u800c\u5bfc\u81f4\u4f30\u8ba1\u7684\u751f\u6210\u503c\u4e0e\u5b9e\u9645\u6d88\u8d39\u503c\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u3002", "method": "CAVE \u6846\u67b6\uff0c\u5b83\u5c06\u5217\u8868\u503c\u516c\u5f0f\u5316\u4e3a\u5b50\u5217\u8868\u503c\u7684\u671f\u671b\uff0c\u5e76\u7531\u6bcf\u4e2a\u4f4d\u7f6e\u7684\u7528\u6237\u7279\u5b9a\u9000\u51fa\u6982\u7387\u52a0\u6743\u3002\u9000\u51fa\u6982\u7387\u88ab\u5206\u89e3\u4e3a\u5174\u8da3\u9a71\u52a8\u7684\u6210\u5206\u548c\u968f\u673a\u6210\u5206\uff0c\u540e\u8005\u901a\u8fc7 Weibull \u5206\u5e03\u5efa\u6a21\u4ee5\u6355\u83b7\u968f\u673a\u5916\u90e8\u56e0\u7d20\uff0c\u4f8b\u5982\u75b2\u52b3\u3002", "result": "CAVE \u5728\u5927\u89c4\u6a21\u771f\u5b9e\u5217\u8868\u57fa\u51c6\u6d4b\u8bd5\u548c\u5728\u7ebf A/B \u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\u3002", "conclusion": "CAVE\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u5728\u7ebfA/B\u6d4b\u8bd5\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\uff0c\u7a81\u51fa\u4e86\u5728\u91cd\u6392\u5e8f\u4e2d\u663e\u5f0f\u5efa\u6a21\u7528\u6237\u9000\u51fa\u7684\u597d\u5904\u3002"}}
{"id": "2508.01290", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01290", "abs": "https://arxiv.org/abs/2508.01290", "authors": ["Zhichao Yan", "Jiapu Wang", "Jiaoyan Chen", "Yanyan Wang", "Hongye Tan", "Jiye Liang", "Xiaoli Li", "Ru Li", "Jeff Z. Pan"], "title": "Prompting Large Language Models with Partial Knowledge for Answering Questions with Unseen Entities", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) shows impressive performance by\nsupplementing and substituting parametric knowledge in Large Language Models\n(LLMs). Retrieved knowledge can be divided into three types: explicit answer\nevidence, implicit answer clue, and insufficient answer context which can be\nfurther categorized into totally irrelevant and partially relevant information.\nEffectively utilizing partially relevant knowledge remains a key challenge for\nRAG systems, especially in incomplete knowledge base retrieval. Contrary to the\nconventional view, we propose a new perspective: LLMs can be awakened via\npartially relevant knowledge already embedded in LLMs. To comprehensively\ninvestigate this phenomenon, the triplets located in the gold reasoning path\nand their variants are used to construct partially relevant knowledge by\nremoving the path that contains the answer. We provide theoretical analysis of\nthe awakening effect in LLMs and support our hypothesis with experiments on two\nKnowledge Graphs (KGs) Question Answering (QA) datasets. Furthermore, we\npresent a new task, Unseen Entity KGQA, simulating real-world challenges where\nentity linking fails due to KG incompleteness. Our awakening-based approach\ndemonstrates greater efficacy in practical applications, outperforms\ntraditional methods that rely on embedding-based similarity which are prone to\nreturning noisy information.", "AI": {"tldr": "This paper proposes a new awakening-based approach for RAG systems that utilizes partially relevant knowledge in LLMs to improve performance in incomplete knowledge base retrieval, outperforming traditional methods.", "motivation": "Effectively utilizing partially relevant knowledge is a key challenge for RAG systems, especially in incomplete knowledge base retrieval. The paper challenges the conventional view and proposes that LLMs can be awakened via partially relevant knowledge already embedded in them.", "method": "The paper uses triplets and their variants from gold reasoning paths to construct partially relevant knowledge. It includes theoretical analysis and experiments on two Knowledge Graphs (KGs) Question Answering (QA) datasets.", "result": "The awakening-based approach demonstrates greater efficacy in practical applications and outperforms traditional methods that rely on embedding-based similarity.", "conclusion": "The paper introduces an awakening-based approach that leverages partially relevant knowledge already embedded in LLMs to address the challenges of incomplete knowledge base retrieval in RAG systems. This approach outperforms traditional embedding-based similarity methods, especially in the new Unseen Entity KGQA task."}}
{"id": "2508.01058", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01058", "abs": "https://arxiv.org/abs/2508.01058", "authors": ["Sara Yavari", "Rahul Nitin Pandya", "Jacob Furst"], "title": "ReCoSeg++:Extended Residual-Guided Cross-Modal Diffusion for Brain Tumor Segmentation", "comment": null, "summary": "Accurate segmentation of brain tumors in MRI scans is critical for clinical\ndiagnosis and treatment planning. We propose a semi-supervised, two-stage\nframework that extends the ReCoSeg approach to the larger and more\nheterogeneous BraTS 2021 dataset, while eliminating the need for ground-truth\nmasks for the segmentation objective. In the first stage, a residual-guided\ndenoising diffusion probabilistic model (DDPM) performs cross-modal synthesis\nby reconstructing the T1ce modality from FLAIR, T1, and T2 scans. The residual\nmaps, capturing differences between predicted and actual T1ce images, serve as\nspatial priors to enhance downstream segmentation. In the second stage, a\nlightweight U-Net takes as input the concatenation of residual maps, computed\nas the difference between real T1ce and synthesized T1ce, with T1, T2, and\nFLAIR modalities to improve whole tumor segmentation. To address the increased\nscale and variability of BraTS 2021, we apply slice-level filtering to exclude\nnon-informative samples and optimize thresholding strategies to balance\nprecision and recall. Our method achieves a Dice score of $93.02\\%$ and an IoU\nof $86.7\\%$ for whole tumor segmentation on the BraTS 2021 dataset,\noutperforming the ReCoSeg baseline on BraTS 2020 (Dice: $91.7\\%$, IoU:\n$85.3\\%$), and demonstrating improved accuracy and scalability for real-world,\nmulti-center MRI datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u9ad8\u8111\u80bf\u7624\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u5728BraTS 2021\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u7ed3\u679c\u3002", "motivation": "\u5728MRI\u626b\u63cf\u4e2d\u5bf9\u8111\u80bf\u7624\u8fdb\u884c\u7cbe\u786e\u5206\u5272\u5bf9\u4e8e\u4e34\u5e8a\u8bca\u65ad\u548c\u6cbb\u7597\u8ba1\u5212\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u3001\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6269\u5c55\u4e86ReCoSeg\u65b9\u6cd5\uff0c\u5229\u7528\u6b8b\u5dee\u5f15\u5bfc\u7684\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b(DDPM)\u8fdb\u884c\u8de8\u6a21\u6001\u5408\u6210\uff0c\u5e76\u4f7f\u7528\u8f7b\u91cf\u7ea7U-Net\u8fdb\u884c\u80bf\u7624\u5206\u5272\u3002", "result": "\u8be5\u65b9\u6cd5\u5728BraTS 2021\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684Dice score\u548cIoU\uff0c\u4f18\u4e8eReCoSeg\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728BraTS 2021\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8693.02%\u7684Dice score\u548c86.7%\u7684IoU\uff0c\u4f18\u4e8eReCoSeg\u5728BraTS 2020\u4e0a\u7684\u8868\u73b0\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u771f\u5b9e\u4e16\u754c\u591a\u4e2d\u5fc3MRI\u6570\u636e\u96c6\u4e0a\u7684\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2508.01158", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01158", "abs": "https://arxiv.org/abs/2508.01158", "authors": ["Yunlong Lin", "Zirui Li", "Guodong Du", "Xiaocong Zhao", "Cheng Gong", "Xinwei Wang", "Chao Lu", "Jianwei Gong"], "title": "H2C: Hippocampal Circuit-inspired Continual Learning for Lifelong Trajectory Prediction in Autonomous Driving", "comment": "Open source code: https://github.com/BIT-Jack/H2C-lifelong", "summary": "Deep learning (DL) has shown state-of-the-art performance in trajectory\nprediction, which is critical to safe navigation in autonomous driving (AD).\nHowever, most DL-based methods suffer from catastrophic forgetting, where\nadapting to a new distribution may cause significant performance degradation in\npreviously learned ones. Such inability to retain learned knowledge limits\ntheir applicability in the real world, where AD systems need to operate across\nvarying scenarios with dynamic distributions. As revealed by neuroscience, the\nhippocampal circuit plays a crucial role in memory replay, effectively\nreconstructing learned knowledge based on limited resources. Inspired by this,\nwe propose a hippocampal circuit-inspired continual learning method (H2C) for\ntrajectory prediction across varying scenarios. H2C retains prior knowledge by\nselectively recalling a small subset of learned samples. First, two\ncomplementary strategies are developed to select the subset to represent\nlearned knowledge. Specifically, one strategy maximizes inter-sample diversity\nto represent the distinctive knowledge, and the other estimates the overall\nknowledge by equiprobable sampling. Then, H2C updates via a memory replay loss\nfunction calculated by these selected samples to retain knowledge while\nlearning new data. Experiments based on various scenarios from the INTERACTION\ndataset are designed to evaluate H2C. Experimental results show that H2C\nreduces catastrophic forgetting of DL baselines by 22.71% on average in a\ntask-free manner, without relying on manually informed distributional shifts.\nThe implementation is available at https://github.com/BIT-Jack/H2C-lifelong.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u6d77\u9a6c\u56de\u8def\u542f\u53d1\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff08H2C\uff09\uff0c\u7528\u4e8e\u8f68\u8ff9\u9884\u6d4b\uff0c\u53ef\u6709\u6548\u51cf\u5c11\u707e\u96be\u6027\u9057\u5fd8\u3002", "motivation": "\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5373\u9002\u5e94\u65b0\u7684\u5206\u5e03\u53ef\u80fd\u5bfc\u81f4\u5148\u524d\u5b66\u4e60\u7684\u5206\u5e03\u7684\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u8fd9\u79cd\u65e0\u6cd5\u4fdd\u7559\u5b66\u4e60\u77e5\u8bc6\u7684\u80fd\u529b\u9650\u5236\u4e86\u5b83\u4eec\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u9002\u7528\u6027\uff0c\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\uff0cAD\u7cfb\u7edf\u9700\u8981\u5728\u5177\u6709\u52a8\u6001\u5206\u5e03\u7684\u5404\u79cd\u573a\u666f\u4e2d\u8fd0\u884c\u3002\u6b63\u5982\u795e\u7ecf\u79d1\u5b66\u6240\u63ed\u793a\u7684\u90a3\u6837\uff0c\u6d77\u9a6c\u56de\u8def\u5728\u8bb0\u5fc6\u91cd\u653e\u4e2d\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u57fa\u4e8e\u6709\u9650\u7684\u8d44\u6e90\u91cd\u5efa\u5b66\u4e60\u5230\u7684\u77e5\u8bc6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u6d77\u9a6c\u56de\u8def\u542f\u53d1\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff08H2C\uff09\uff0c\u7528\u4e8e\u8de8\u5404\u79cd\u573a\u666f\u7684\u8f68\u8ff9\u9884\u6d4b\u3002H2C\u901a\u8fc7\u9009\u62e9\u6027\u5730\u56de\u5fc6\u4e00\u5c0f\u90e8\u5206\u5b66\u4e60\u8fc7\u7684\u6837\u672c\u6765\u4fdd\u7559\u5148\u524d\u7684\u77e5\u8bc6\u3002\u9996\u5148\uff0c\u5f00\u53d1\u4e86\u4e24\u79cd\u4e92\u8865\u7684\u7b56\u7565\u6765\u9009\u62e9\u4ee3\u8868\u5b66\u4e60\u77e5\u8bc6\u7684\u5b50\u96c6\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u4e00\u79cd\u7b56\u7565\u662f\u6700\u5927\u5316\u6837\u672c\u95f4\u7684\u591a\u6837\u6027\u6765\u4ee3\u8868\u72ec\u7279\u7684\u77e5\u8bc6\uff0c\u53e6\u4e00\u79cd\u7b56\u7565\u662f\u901a\u8fc7\u7b49\u6982\u7387\u62bd\u6837\u6765\u4f30\u8ba1\u6574\u4f53\u77e5\u8bc6\u3002\u7136\u540e\uff0cH2C\u901a\u8fc7\u8fd9\u4e9b\u9009\u5b9a\u7684\u6837\u672c\u8ba1\u7b97\u7684\u8bb0\u5fc6\u91cd\u653e\u635f\u5931\u51fd\u6570\u8fdb\u884c\u66f4\u65b0\uff0c\u4ee5\u5728\u5b66\u4e60\u65b0\u6570\u636e\u7684\u540c\u65f6\u4fdd\u7559\u77e5\u8bc6\u3002", "result": "H2C\u4ee5\u65e0\u4efb\u52a1\u7684\u65b9\u5f0f\u5e73\u5747\u51cf\u5c11\u4e8622.71%\u7684DL\u57fa\u7ebf\u7684\u707e\u96be\u6027\u9057\u5fd8\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u624b\u52a8\u901a\u77e5\u7684\u5206\u5e03\u504f\u79fb\u3002", "conclusion": "H2C\u901a\u8fc7\u9009\u62e9\u6027\u5730\u56de\u5fc6\u4e00\u5c0f\u90e8\u5206\u5b66\u4e60\u8fc7\u7684\u6837\u672c\u6765\u4fdd\u7559\u5148\u524d\u7684\u77e5\u8bc6\uff0c\u5e76\u5728\u5404\u79cd\u573a\u666f\u7684INTERACTION\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cH2C\u4ee5\u65e0\u4efb\u52a1\u7684\u65b9\u5f0f\u5e73\u5747\u51cf\u5c11\u4e8622.71%\u7684DL\u57fa\u7ebf\u7684\u707e\u96be\u6027\u9057\u5fd8\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u624b\u52a8\u901a\u77e5\u7684\u5206\u5e03\u504f\u79fb\u3002"}}
{"id": "2508.00897", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.00897", "abs": "https://arxiv.org/abs/2508.00897", "authors": ["Julien Simon de Kergunic", "Rony Abecidan", "Patrick Bas", "Vincent Itier"], "title": "Maximize margins for robust splicing detection", "comment": "in French language. GRETSI 2025 - Colloque Francophone de Traitement\n  du Signal et des Images, https://gretsi.fr/colloque2025/, Aug 2025,\n  Strasbourg, France", "summary": "Despite recent progress in splicing detection, deep learning-based forensic\ntools remain difficult to deploy in practice due to their high sensitivity to\ntraining conditions. Even mild post-processing applied to evaluation images can\nsignificantly degrade detector performance, raising concerns about their\nreliability in operational contexts. In this work, we show that the same deep\narchitecture can react very differently to unseen post-processing depending on\nthe learned weights, despite achieving similar accuracy on in-distribution test\ndata. This variability stems from differences in the latent spaces induced by\ntraining, which affect how samples are separated internally. Our experiments\nreveal a strong correlation between the distribution of latent margins and a\ndetector's ability to generalize to post-processed images. Based on this\nobservation, we propose a practical strategy for building more robust\ndetectors: train several variants of the same model under different conditions,\nand select the one that maximizes latent margins.", "AI": {"tldr": "\u6df1\u5ea6\u5b66\u4e60\u53d6\u8bc1\u5de5\u5177\u5bf9\u8bad\u7ec3\u6761\u4ef6\u9ad8\u5ea6\u654f\u611f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6700\u5927\u5316\u6f5c\u5728\u88d5\u5ea6\u6765\u6784\u5efa\u66f4\u9c81\u68d2\u68c0\u6d4b\u5668\u7684\u7b56\u7565\u3002", "motivation": "\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u53d6\u8bc1\u5de5\u5177\u5bf9\u8bad\u7ec3\u6761\u4ef6\u9ad8\u5ea6\u654f\u611f\uff0c\u5373\u4f7f\u5e94\u7528\u4e8e\u8bc4\u4f30\u56fe\u50cf\u7684\u8f7b\u5fae\u540e\u5904\u7406\u4e5f\u4f1a\u663e\u7740\u964d\u4f4e\u68c0\u6d4b\u5668\u6027\u80fd\uff0c\u4ece\u800c\u5f15\u8d77\u5bf9\u5176\u5728\u64cd\u4f5c\u73af\u5883\u4e2d\u53ef\u9760\u6027\u7684\u62c5\u5fe7\u3002", "method": "\u8bad\u7ec3\u540c\u4e00\u6a21\u578b\u7684\u591a\u4e2a\u53d8\u4f53\uff0c\u5e76\u9009\u62e9\u6700\u5927\u5316\u6f5c\u5728\u88d5\u5ea6\u7684\u53d8\u4f53\u3002", "result": "\u6f5c\u5728\u88d5\u5ea6\u7684\u5206\u5e03\u4e0e\u68c0\u6d4b\u5668\u6cdb\u5316\u5230\u540e\u5904\u7406\u56fe\u50cf\u7684\u80fd\u529b\u4e4b\u95f4\u5b58\u5728\u5f88\u5f3a\u7684\u76f8\u5173\u6027\u3002\u8868\u660e\u76f8\u540c\u7684\u6df1\u5ea6\u67b6\u6784\u5bf9\u770b\u4e0d\u89c1\u7684\u540e\u5904\u7406\u7684\u53cd\u5e94\u975e\u5e38\u4e0d\u540c\uff0c\u5177\u4f53\u53d6\u51b3\u4e8e\u5b66\u4e60\u7684\u6743\u91cd\uff0c\u5c3d\u7ba1\u5728\u5206\u5e03\u5185\u6d4b\u8bd5\u6570\u636e\u4e0a\u5b9e\u73b0\u4e86\u76f8\u4f3c\u7684\u51c6\u786e\u6027\u3002\u8fd9\u79cd\u53ef\u53d8\u6027\u6e90\u4e8e\u8bad\u7ec3\u5f15\u8d77\u7684\u6f5c\u5728\u7a7a\u95f4\u7684\u5dee\u5f02\uff0c\u8fd9\u4f1a\u5f71\u54cd\u6837\u672c\u5728\u5185\u90e8\u7684\u5206\u79bb\u65b9\u5f0f\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6784\u5efa\u66f4\u9c81\u68d2\u68c0\u6d4b\u5668\u7684\u5b9e\u7528\u7b56\u7565\uff1a\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u8bad\u7ec3\u540c\u4e00\u6a21\u578b\u7684\u591a\u4e2a\u53d8\u4f53\uff0c\u5e76\u9009\u62e9\u6700\u5927\u5316\u6f5c\u5728\u88d5\u5ea6\u7684\u53d8\u4f53\u3002"}}
{"id": "2508.02266", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.02266", "abs": "https://arxiv.org/abs/2508.02266", "authors": ["Yang Xu", "Kai Ming Ting"], "title": "Voronoi Diagram Encoded Hashing", "comment": null, "summary": "The goal of learning to hash (L2H) is to derive data-dependent hash functions\nfrom a given data distribution in order to map data from the input space to a\nbinary coding space. Despite the success of L2H, two observations have cast\ndoubt on the source of the power of L2H, i.e., learning. First, a recent study\nshows that even using a version of locality sensitive hashing functions without\nlearning achieves binary representations that have comparable accuracy as those\nof L2H, but with less time cost. Second, existing L2H methods are constrained\nto three types of hash functions: thresholding, hyperspheres, and hyperplanes\nonly. In this paper, we unveil the potential of Voronoi diagrams in hashing.\nVoronoi diagram is a suitable candidate because of its three properties. This\ndiscovery has led us to propose a simple and efficient no-learning binary\nhashing method, called Voronoi Diagram Encoded Hashing (VDeH), which constructs\na set of hash functions through a data-dependent similarity measure and\nproduces independent binary bits through encoded hashing. We demonstrate\nthrough experiments on several benchmark datasets that VDeH achieves superior\nperformance and lower computational cost compared to existing state-of-the-art\nmethods under the same bit length.", "AI": {"tldr": "This paper proposes Voronoi Diagram Encoded Hashing (VDeH), a simple and efficient no-learning binary hashing method, which achieves superior performance and lower computational cost compared to existing state-of-the-art methods.", "motivation": "Existing L2H methods are constrained to three types of hash functions and a recent study shows that even using a version of locality sensitive hashing functions without learning achieves comparable accuracy as those of L2H, but with less time cost.", "method": "Voronoi Diagram Encoded Hashing (VDeH), which constructs a set of hash functions through a data-dependent similarity measure and produces independent binary bits through encoded hashing.", "result": "VDeH achieves superior performance and lower computational cost compared to existing state-of-the-art methods under the same bit length.", "conclusion": "VDeH achieves superior performance and lower computational cost compared to existing state-of-the-art methods under the same bit length."}}
{"id": "2508.01302", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01302", "abs": "https://arxiv.org/abs/2508.01302", "authors": ["Chenming Tang", "Yutong Yang", "Yunfang Wu"], "title": "KEDAS: Knowledge Editing Alignment with Diverse Augmentation and Self-adaptive Inference", "comment": "Preprint", "summary": "Knowledge editing aims to modify outdated knowledge in large language models\n(LLMs) efficiently while retaining their powerful capabilities. Most existing\nmethods rely on either parameter-level editing or retrieval-based approaches.\nIn this work, we propose Knowledge Editing alignment with Diverse Augmentation\nand Self-adaptive inference (KEDAS) to better align LLMs with knowledge\nediting. In the alignment phase, LLMs learn to apply in-context edited\nknowledge via low-rank adaptation. During editing, we design a diverse edit\naugmentation technique to improve the recall of edits. After that, a\nself-adaptive post-alignment inference mechanism is proposed, in which a\nfilter-based smart retriever is employed to perform a dynamic selection of\ninference routing. Specifically, irrelevant queries will go through the\noriginal pre-alignment model directly, while relevant ones, together with their\nrelated edits, go through the model with aligned adapters activated. In\nexperiments, KEDAS secures the highest overall performance scores in 35 out of\n36 cases across four datasets with three LLMs on three settings, surpassing its\nstrong knowledge editing alignment counterpart by about 19.8 harmonic mean\nscores of edit success, locality and portability and outperforming both\nparameter editing and retrieval-based baselines significantly. Analysis of\ncomputational cost and performance on general tasks further validates the\nrobustness and efficiency of KEDAS, indicating that it presents an ideal\nparadigm of knowledge editing alignment.", "AI": {"tldr": "KEDAS\u662f\u4e00\u79cd\u65b0\u7684\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u5bf9\u9f50\u3001\u589e\u5f3a\u548c\u81ea\u9002\u5e94\u63a8\u7406\u6765\u6709\u6548\u4fee\u6539LLM\u4e2d\u7684\u77e5\u8bc6\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u7684\u77e5\u8bc6\u7f16\u8f91\u65e8\u5728\u9ad8\u6548\u5730\u4fee\u6539\u8fc7\u65f6\u7684\u77e5\u8bc6\uff0c\u540c\u65f6\u4fdd\u7559\u5176\u5f3a\u5927\u7684\u80fd\u529b\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u53c2\u6570\u7ea7\u7f16\u8f91\u6216\u57fa\u4e8e\u68c0\u7d22\u7684\u65b9\u6cd5\u3002", "method": "KEDAS\u901a\u8fc7\u4f4e\u79e9\u81ea\u9002\u5e94\u5b66\u4e60\u5e94\u7528\u4e0a\u4e0b\u6587\u7f16\u8f91\u77e5\u8bc6\uff0c\u5229\u7528\u591a\u6837\u5316\u7684\u7f16\u8f91\u589e\u5f3a\u6280\u672f\u63d0\u9ad8\u7f16\u8f91\u7684\u53ec\u56de\u7387\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u8fc7\u6ee4\u5668\u7684\u667a\u80fd\u68c0\u7d22\u5668\u8fdb\u884c\u52a8\u6001\u63a8\u7406\u8def\u7531\u9009\u62e9\u3002", "result": "KEDAS\u5728\u56db\u4e2a\u6570\u636e\u96c6\u3001\u4e09\u4e2aLLM\u548c\u4e09\u4e2a\u8bbe\u7f6e\u768436\u4e2a\u6848\u4f8b\u4e2d\uff0c\u670935\u4e2a\u6848\u4f8b\u83b7\u5f97\u4e86\u6700\u9ad8\u7684\u603b\u4f53\u6027\u80fd\u8bc4\u5206\uff0c\u8d85\u8fc7\u4e86\u5176\u5f3a\u5927\u7684\u77e5\u8bc6\u7f16\u8f91\u5bf9\u7b49\u65b9\u7ea619.8\u4e2a\u7f16\u8f91\u6210\u529f\u3001\u5c40\u90e8\u6027\u548c\u53ef\u79fb\u690d\u6027\u7684\u8c03\u548c\u5e73\u5747\u5206\uff0c\u5e76\u4e14\u663e\u8457\u4f18\u4e8e\u53c2\u6570\u7f16\u8f91\u548c\u57fa\u4e8e\u68c0\u7d22\u7684\u57fa\u7ebf\u3002", "conclusion": "KEDAS\u5728\u77e5\u8bc6\u7f16\u8f91\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2508.01064", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01064", "abs": "https://arxiv.org/abs/2508.01064", "authors": ["Fenghe Tang", "Bingkun Nian", "Jianrui Ding", "Wenxin Ma", "Quan Quan", "Chengqi Dong", "Jie Yang", "Wei Liu", "S. Kevin Zhou"], "title": "Mobile U-ViT: Revisiting large kernel and U-shaped ViT for efficient medical image segmentation", "comment": "Accepted by ACM Multimedia 2025. Code:\n  https://github.com/FengheTan9/Mobile-U-ViT", "summary": "In clinical practice, medical image analysis often requires efficient\nexecution on resource-constrained mobile devices. However, existing mobile\nmodels-primarily optimized for natural images-tend to perform poorly on medical\ntasks due to the significant information density gap between natural and\nmedical domains. Combining computational efficiency with medical\nimaging-specific architectural advantages remains a challenge when developing\nlightweight, universal, and high-performing networks. To address this, we\npropose a mobile model called Mobile U-shaped Vision Transformer (Mobile U-ViT)\ntailored for medical image segmentation. Specifically, we employ the newly\npurposed ConvUtr as a hierarchical patch embedding, featuring a\nparameter-efficient large-kernel CNN with inverted bottleneck fusion. This\ndesign exhibits transformer-like representation learning capacity while being\nlighter and faster. To enable efficient local-global information exchange, we\nintroduce a novel Large-kernel Local-Global-Local (LGL) block that effectively\nbalances the low information density and high-level semantic discrepancy of\nmedical images. Finally, we incorporate a shallow and lightweight transformer\nbottleneck for long-range modeling and employ a cascaded decoder with\ndownsample skip connections for dense prediction. Despite its reduced\ncomputational demands, our medical-optimized architecture achieves\nstate-of-the-art performance across eight public 2D and 3D datasets covering\ndiverse imaging modalities, including zero-shot testing on four unseen\ndatasets. These results establish it as an efficient yet powerful and\ngeneralization solution for mobile medical image analysis. Code is available at\nhttps://github.com/FengheTan9/Mobile-U-ViT.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Mobile U-ViT \u7684\u79fb\u52a8\u6a21\u578b\uff0c\u4e13\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u800c\u5b9a\u5236\uff0c\u5e76\u5728\u5404\u79cd\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\uff0c\u533b\u5b66\u56fe\u50cf\u5206\u6790\u901a\u5e38\u9700\u8981\u5728\u8d44\u6e90\u53d7\u9650\u7684\u79fb\u52a8\u8bbe\u5907\u4e0a\u9ad8\u6548\u6267\u884c\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u4e3b\u8981\u9488\u5bf9\u81ea\u7136\u56fe\u50cf\u4f18\u5316\u7684\u79fb\u52a8\u6a21\u578b\uff0c\u7531\u4e8e\u81ea\u7136\u9886\u57df\u548c\u533b\u5b66\u9886\u57df\u4e4b\u95f4\u5b58\u5728\u663e\u7740\u7684\u4fe1\u606f\u5bc6\u5ea6\u5dee\u8ddd\uff0c\u56e0\u6b64\u5728\u533b\u5b66\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u5f80\u5f80\u4e0d\u4f73\u3002\u5728\u5f00\u53d1\u8f7b\u91cf\u7ea7\u3001\u901a\u7528\u548c\u9ad8\u6027\u80fd\u7f51\u7edc\u65f6\uff0c\u5c06\u8ba1\u7b97\u6548\u7387\u4e0e\u533b\u5b66\u6210\u50cf\u7279\u6709\u7684\u67b6\u6784\u4f18\u52bf\u76f8\u7ed3\u5408\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Mobile U-shaped Vision Transformer (Mobile U-ViT) \u7684\u79fb\u52a8\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u4e13\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u800c\u5b9a\u5236\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u91c7\u7528\u65b0\u63d0\u51fa\u7684 ConvUtr \u4f5c\u4e3a\u5206\u5c42\u8865\u4e01\u5d4c\u5165\uff0c\u5177\u6709\u53c2\u6570\u9ad8\u6548\u7684\u5927\u5185\u6838 CNN \u548c\u53cd\u5411\u74f6\u9888\u878d\u5408\u3002\u4e3a\u4e86\u5b9e\u73b0\u9ad8\u6548\u7684\u5c40\u90e8-\u5168\u5c40\u4fe1\u606f\u4ea4\u6362\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5927\u5185\u6838\u5c40\u90e8-\u5168\u5c40-\u5c40\u90e8 (LGL) \u5757\uff0c\u8be5\u5757\u6709\u6548\u5730\u5e73\u8861\u4e86\u533b\u5b66\u56fe\u50cf\u7684\u4f4e\u4fe1\u606f\u5bc6\u5ea6\u548c\u9ad8\u7ea7\u8bed\u4e49\u5dee\u5f02\u3002\u6700\u540e\uff0c\u6211\u4eec\u7ed3\u5408\u4e86\u4e00\u4e2a\u6d45\u800c\u8f7b\u91cf\u7ea7\u7684 Transformer \u74f6\u9888\u7528\u4e8e\u8fdc\u7a0b\u5efa\u6a21\uff0c\u5e76\u91c7\u7528\u5e26\u6709\u4e0b\u91c7\u6837\u8df3\u8dc3\u8fde\u63a5\u7684\u7ea7\u8054\u89e3\u7801\u5668\u7528\u4e8e\u5bc6\u96c6\u9884\u6d4b\u3002", "result": "\u533b\u5b66\u4f18\u5316\u7684\u67b6\u6784\u5728\u516b\u4e2a\u516c\u5171 2D \u548c 3D \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u6db5\u76d6\u4e86\u4e0d\u540c\u7684\u6210\u50cf\u65b9\u5f0f\uff0c\u5305\u62ec\u5bf9\u56db\u4e2a\u672a\u89c1\u6570\u636e\u96c6\u7684\u96f6\u6837\u672c\u6d4b\u8bd5\u3002", "conclusion": "\u8be5\u533b\u5b66\u4f18\u5316\u7684\u67b6\u6784\u5728\u516b\u4e2a\u516c\u5171 2D \u548c 3D \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u6db5\u76d6\u4e86\u4e0d\u540c\u7684\u6210\u50cf\u65b9\u5f0f\uff0c\u5305\u62ec\u5bf9\u56db\u4e2a\u672a\u89c1\u6570\u636e\u96c6\u7684\u96f6\u6837\u672c\u6d4b\u8bd5\u3002\u8fd9\u4e9b\u7ed3\u679c\u5c06\u5176\u786e\u7acb\u4e3a\u7528\u4e8e\u79fb\u52a8\u533b\u5b66\u56fe\u50cf\u5206\u6790\u7684\u9ad8\u6548\u3001\u5f3a\u5927\u548c\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.01181", "categories": ["cs.AI", "cs.CV", "cs.MM", "cs.SD", "eess.AS", "68", "I.2.10"], "pdf": "https://arxiv.org/pdf/2508.01181", "abs": "https://arxiv.org/abs/2508.01181", "authors": ["Zhiyuan Han", "Beier Zhu", "Yanlong Xu", "Peipei Song", "Xun Yang"], "title": "Benchmarking and Bridging Emotion Conflicts for Multimodal Emotion Reasoning", "comment": "ACM Multimedia 2025", "summary": "Despite their strong performance in multimodal emotion reasoning, existing\nMultimodal Large Language Models (MLLMs) often overlook the scenarios involving\nemotion conflicts, where emotional cues from different modalities are\ninconsistent. To fill this gap, we first introduce CA-MER, a new benchmark\ndesigned to examine MLLMs under realistic emotion conflicts. It consists of\nthree subsets: video-aligned, audio-aligned, and consistent, where only one or\nall modalities reflect the true emotion. However, evaluations on our CA-MER\nreveal that current state-of-the-art emotion MLLMs systematically over-rely on\naudio signal during emotion conflicts, neglecting critical cues from visual\nmodality. To mitigate this bias, we propose MoSEAR, a parameter-efficient\nframework that promotes balanced modality integration. MoSEAR consists of two\nmodules: (1)MoSE, modality-specific experts with a regularized gating mechanism\nthat reduces modality bias in the fine-tuning heads; and (2)AR, an attention\nreallocation mechanism that rebalances modality contributions in frozen\nbackbones during inference. Our framework offers two key advantages: it\nmitigates emotion conflicts and improves performance on consistent\nsamples-without incurring a trade-off between audio and visual modalities.\nExperiments on multiple benchmarks-including MER2023, EMER, DFEW, and our\nCA-MER-demonstrate that MoSEAR achieves state-of-the-art performance,\nparticularly under modality conflict conditions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86CA-MER\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30MLLM\u5728\u60c5\u611f\u51b2\u7a81\u4e0b\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u97f3\u9891\u4fe1\u53f7\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86MoSEAR\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u6001\u7279\u5b9a\u4e13\u5bb6\u548c\u6ce8\u610f\u529b\u91cd\u65b0\u5206\u914d\u673a\u5236\u6765\u5e73\u8861\u6a21\u6001\u96c6\u6210\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b(MLLM)\u5728\u591a\u6a21\u6001\u60c5\u611f\u63a8\u7406\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u901a\u5e38\u5ffd\u7565\u6d89\u53ca\u60c5\u611f\u51b2\u7a81\u7684\u573a\u666f\uff0c\u5373\u6765\u81ea\u4e0d\u540c\u6a21\u6001\u7684\u60c5\u611f\u7ebf\u7d22\u4e0d\u4e00\u81f4\u7684\u60c5\u51b5\u3002", "method": "\u63d0\u51fa\u4e86MoSEAR\uff0c\u4e00\u4e2a\u53c2\u6570\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6a21\u5757\uff1a(1)\u5e26\u6709\u6b63\u5219\u5316\u95e8\u63a7\u673a\u5236\u7684\u6a21\u6001\u7279\u5b9a\u4e13\u5bb6MoSE\uff0c\u51cf\u5c11\u5fae\u8c03\u5934\u4e2d\u7684\u6a21\u6001\u504f\u5dee\uff1b(2)\u6ce8\u610f\u529b\u91cd\u65b0\u5206\u914d\u673a\u5236AR\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u91cd\u65b0\u5e73\u8861\u51bb\u7ed3\u9aa8\u5e72\u7f51\u7edc\u4e2d\u7684\u6a21\u6001\u8d21\u732e\u3002", "result": "\u5bf9CA-MER\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684\u60c5\u611fMLLM\u5728\u60c5\u611f\u51b2\u7a81\u671f\u95f4\u7cfb\u7edf\u6027\u5730\u8fc7\u5ea6\u4f9d\u8d56\u97f3\u9891\u4fe1\u53f7\uff0c\u5ffd\u7565\u4e86\u6765\u81ea\u89c6\u89c9\u6a21\u6001\u7684\u5173\u952e\u7ebf\u7d22\u3002MoSEAR\u7f13\u89e3\u4e86\u60c5\u611f\u51b2\u7a81\uff0c\u63d0\u9ad8\u4e86\u5728\u4e00\u81f4\u6837\u672c\u4e0a\u7684\u6027\u80fd\uff0c\u4e14\u4e0d\u4f1a\u5728\u97f3\u9891\u548c\u89c6\u89c9\u6a21\u6001\u4e4b\u95f4\u4ea7\u751f\u6743\u8861\u3002", "conclusion": "\u63d0\u51fa\u7684MoSEAR\u6846\u67b6\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u6a21\u6001\u51b2\u7a81\u6761\u4ef6\u4e0b\u3002"}}
{"id": "2508.00901", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00901", "abs": "https://arxiv.org/abs/2508.00901", "authors": ["Ruichen Xu", "Kexin Chen"], "title": "Filtering with Self-Attention and Storing with MLP: One-Layer Transformers Can Provably Acquire and Extract Knowledge", "comment": null, "summary": "Modern large language models excel in knowledge-intensive tasks, yet how\ntransformers acquire (store) knowledge during pre-training and extract\n(retrieve) it during post-fine-tuning inference remains theoretically opaque.\nWhile prior theoretical work has begun to investigate these questions through\nthe analysis of training dynamics, such studies are limited to single-layer,\nattention-only architectures. However, most existing studies suggest that MLPs\nare the most contributing components for storing knowledge in transformer-based\nlanguage models. Meanwhile, our empirical investigations reveal that such\nsimplified models, when trained using standard next-token prediction\nobjectives, may be incapable of acquiring or extracting factual knowledge. To\novercome this limitation, we introduce a tractable one-layer transformer\nframework that crucially incorporates both self-attention and MLP modules. By\ntracking its gradient dynamics, we establish convergence and generalization\nguarantees that illuminate the ability of knowledge acquisition and extraction.\nWe prove that 1) Transformers can achieve near-optimal training loss during\npre-training, signifying effective knowledge acquisition; 2) With a large\nfine-tuning dataset and specific data multiplicity conditions met, transformers\ncan achieve low generalization error when tested on factual knowledge learned\nduring pre-training but not reinforced during the fine-tuning, indicating\nsuccessful knowledge extraction; 3) When the conditions are not satisfied,\ntransformers exhibit high generalization loss, resulting in hallucinations. Our\nanalysis includes both full fine-tuning and low-rank fine-tuning. Furthermore,\nour analysis offers theoretical insights into several pertinent empirical\nphenomena, such as the role of learning rate schedules. Experiments on\nsynthetic and real-world PopQA datasets with GPT-2 and Llama-3.2-1B validate\nour results.", "AI": {"tldr": "This paper introduces a one-layer transformer framework with self-attention and MLP modules to analyze knowledge acquisition and extraction in LLMs, proving transformers can achieve near-optimal training loss and low generalization error under specific conditions. Experiments validate the results on synthetic and real-world datasets.", "motivation": "Modern large language models excel in knowledge-intensive tasks, yet how transformers acquire (store) knowledge during pre-training and extract (retrieve) it during post-fine-tuning inference remains theoretically opaque. While prior theoretical work has begun to investigate these questions through the analysis of training dynamics, such studies are limited to single-layer, attention-only architectures. However, most existing studies suggest that MLPs are the most contributing components for storing knowledge in transformer-based language models. Meanwhile, our empirical investigations reveal that such simplified models, when trained using standard next-token prediction objectives, may be incapable of acquiring or extracting factual knowledge.", "method": "introduce a tractable one-layer transformer framework that crucially incorporates both self-attention and MLP modules. By tracking its gradient dynamics, we establish convergence and generalization guarantees that illuminate the ability of knowledge acquisition and extraction", "result": "Experiments on synthetic and real-world PopQA datasets with GPT-2 and Llama-3.2-1B validate our results.", "conclusion": "Transformers can achieve near-optimal training loss during pre-training, signifying effective knowledge acquisition. With a large fine-tuning dataset and specific data multiplicity conditions met, transformers can achieve low generalization error when tested on factual knowledge learned during pre-training but not reinforced during the fine-tuning, indicating successful knowledge extraction; When the conditions are not satisfied, transformers exhibit high generalization loss, resulting in hallucinations."}}
{"id": "2508.02300", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.02300", "abs": "https://arxiv.org/abs/2508.02300", "authors": ["Kanishka Silva", "Marcel R. Ackermann", "Heike Fliegl", "Genet-Asefa Gesese", "Fidan Limani", "Philipp Mayr", "Peter Mutschke", "Allard Oelen", "Muhammad Asif Suryani", "Sharmila Upadhyaya", "Benjamin Zapilko", "Harald Sack", "Stefan Dietze"], "title": "Research Knowledge Graphs in NFDI4DataScience: Key Activities, Achievements, and Future Directions", "comment": null, "summary": "As research in Artificial Intelligence and Data Science continues to grow in\nvolume and complexity, it becomes increasingly difficult to ensure\ntransparency, reproducibility, and discoverability. To address these\nchallenges, as research artifacts should be understandable and usable by\nmachines, the NFDI4DataScience consortium is developing and providing Research\nKnowledge Graphs (RKGs). Building upon earlier works, this paper presents\nrecent progress in creating semantically rich RKGs using standardized\nontologies, shared vocabularies, and automated Information Extraction\ntechniques. Key achievements include the development of the NFDI4DS ontology,\nmetadata standards, tools, and services designed to support the FAIR\nprinciples, as well as community-led projects and various implementations of\nRKGs. Together, these efforts aim to capture and connect the complex\nrelationships between datasets, models, software, and scientific publications.", "AI": {"tldr": "NFDI4DataScience \u8054\u76df\u6b63\u5728\u6784\u5efa\u7814\u7a76\u77e5\u8bc6\u56fe\u8c31\u4ee5\u63d0\u9ad8\u4eba\u5de5\u667a\u80fd\u548c\u6570\u636e\u79d1\u5b66\u7814\u7a76\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002", "motivation": "\u786e\u4fdd\u4eba\u5de5\u667a\u80fd\u548c\u6570\u636e\u79d1\u5b66\u7814\u7a76\u7684\u900f\u660e\u5ea6\u3001\u53ef\u91cd\u590d\u6027\u548c\u53ef\u53d1\u73b0\u6027\u9762\u4e34\u6311\u6218\u3002", "method": "\u4f7f\u7528\u6807\u51c6\u5316\u672c\u4f53\u3001\u5171\u4eab\u8bcd\u6c47\u8868\u548c\u81ea\u52a8\u5316\u4fe1\u606f\u63d0\u53d6\u6280\u672f\u521b\u5efa\u8bed\u4e49\u4e30\u5bcc\u7684 RKG\u3002", "result": "\u5f00\u53d1\u4e86 NFDI4DS \u672c\u4f53\u3001\u5143\u6570\u636e\u6807\u51c6\u3001\u5de5\u5177\u548c\u670d\u52a1\uff0c\u4ee5\u652f\u6301 FAIR \u539f\u5219\uff0c\u4ee5\u53ca\u793e\u533a\u4e3b\u5bfc\u7684\u9879\u76ee\u548c RKG \u7684\u5404\u79cd\u5b9e\u73b0\u3002", "conclusion": "NFDI4DataScience \u8054\u76df\u6b63\u5728\u5f00\u53d1\u548c\u63d0\u4f9b\u7814\u7a76\u77e5\u8bc6\u56fe\u8c31 (RKG)\uff0c\u65e8\u5728\u6355\u83b7\u548c\u8fde\u63a5\u6570\u636e\u96c6\u3001\u6a21\u578b\u3001\u8f6f\u4ef6\u548c\u79d1\u5b66\u51fa\u7248\u7269\u4e4b\u95f4\u590d\u6742\u7684\u5173\u7cfb\u3002"}}
{"id": "2508.01309", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01309", "abs": "https://arxiv.org/abs/2508.01309", "authors": ["Weibo Zhou", "Lingbo Li", "Shangsong Liang"], "title": "D-SCoRE: Document-Centric Segmentation and CoT Reasoning with Structured Export for QA-CoT Data Generation", "comment": null, "summary": "The scarcity and high cost of high-quality question-answering (QA) datasets\nhinder supervised fine-tuning (SFT) for domain-specific large language models\n(LLMs). To address this, we introduce D-SCoRE, a training-free pipeline that\nutilizes LLMs and prompt engineering to produce diverse, high-quality QA\ndatasets from arbitrary textual sources. D-SCoRE integrates\n$\\textbf{D}$ocument-centric processing, $\\textbf{S}$egmentation, $\\textbf{Co}$T\n$\\textbf{R}$easoning, and structured $\\textbf{E}$xport to generate QA-COT\ndatasets tailored for domain-aware SFT. Multi-dimensional control mechanisms,\nsuch as semantic role transformation, question type balancing, and\ncounterfactual materials, enhance diversity and relevance, overcoming\nlimitations of existing QA generation. LLMs fine-tuned on D-SCoRE-generated QA\ndatasets, and human-annotated QA datasets (SQuAD, Covid-QA) are evaluated on\nSQuADShifts and Covid-QA test sets, with D-SCoRE outperforming across most\ndomains. D-SCoRE generates six QA-CoT pairs with four-option counterfactual\nmaterials per 100-200-word text in 90 seconds using an 8B LLM on consumer-grade\nhardware. Its simplicity and scalability enable efficient QA generation and\nhigh-performance fine-tuning across domains.", "AI": {"tldr": "D-SCoRE\u662f\u4e00\u4e2a\u65e0\u8bad\u7ec3\u7ba1\u9053\uff0c\u5b83\u5229\u7528LLM\u548c\u63d0\u793a\u5de5\u7a0b\u4ece\u4efb\u610f\u6587\u672c\u6e90\u751f\u6210\u591a\u6837\u5316\u3001\u9ad8\u8d28\u91cf\u7684QA\u6570\u636e\u96c6\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u6548\u7684QA\u751f\u6210\u548c\u8de8\u9886\u57df\u7684\u9ad8\u6027\u80fd\u5fae\u8c03\u3002", "motivation": "\u9ad8\u8d28\u91cf\u95ee\u7b54\uff08QA\uff09\u6570\u636e\u96c6\u7684\u7a00\u7f3a\u6027\u548c\u9ad8\u6210\u672c\u963b\u788d\u4e86\u9886\u57df\u7279\u5b9a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u3002", "method": "D-SCoRE\uff0c\u4e00\u4e2a\u5229\u7528LLM\u548c\u63d0\u793a\u5de5\u7a0b\u4ece\u4efb\u610f\u6587\u672c\u6e90\u751f\u6210\u591a\u6837\u5316\u3001\u9ad8\u8d28\u91cfQA\u6570\u636e\u96c6\u7684\u65e0\u8bad\u7ec3\u7ba1\u9053\u3002", "result": "D-SCoRE\u751f\u6210\u516d\u4e2aQA-CoT\u5bf9\uff0c\u6bcf\u4e2a100-200\u5b57\u7684\u6587\u672c\u5e26\u6709\u56db\u9009\u9879\u53cd\u4e8b\u5b9e\u6750\u6599\uff0c\u4f7f\u75288B LLM\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u82b1\u8d3990\u79d2\u3002", "conclusion": "D-SCoRE\u5728\u5927\u591a\u6570\u9886\u57df\u90fd\u4f18\u4e8e\u4eba\u5de5\u6807\u6ce8\u7684QA\u6570\u636e\u96c6\u3002"}}
{"id": "2508.01074", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.01074", "abs": "https://arxiv.org/abs/2508.01074", "authors": ["Hongyu Zhu", "Sichu Liang", "Wenwen Wang", "Zhuomeng Zhang", "Fangqi Li", "Shi-Lin Wang"], "title": "Evading Data Provenance in Deep Neural Networks", "comment": "ICCV 2025 Highlight", "summary": "Modern over-parameterized deep models are highly data-dependent, with large\nscale general-purpose and domain-specific datasets serving as the bedrock for\nrapid advancements. However, many datasets are proprietary or contain sensitive\ninformation, making unrestricted model training problematic. In the open world\nwhere data thefts cannot be fully prevented, Dataset Ownership Verification\n(DOV) has emerged as a promising method to protect copyright by detecting\nunauthorized model training and tracing illicit activities. Due to its\ndiversity and superior stealth, evading DOV is considered extremely\nchallenging. However, this paper identifies that previous studies have relied\non oversimplistic evasion attacks for evaluation, leading to a false sense of\nsecurity. We introduce a unified evasion framework, in which a teacher model\nfirst learns from the copyright dataset and then transfers task-relevant yet\nidentifier-independent domain knowledge to a surrogate student using an\nout-of-distribution (OOD) dataset as the intermediary. Leveraging\nVision-Language Models and Large Language Models, we curate the most\ninformative and reliable subsets from the OOD gallery set as the final transfer\nset, and propose selectively transferring task-oriented knowledge to achieve a\nbetter trade-off between generalization and evasion effectiveness. Experiments\nacross diverse datasets covering eleven DOV methods demonstrate our approach\nsimultaneously eliminates all copyright identifiers and significantly\noutperforms nine state-of-the-art evasion attacks in both generalization and\neffectiveness, with moderate computational overhead. As a proof of concept, we\nreveal key vulnerabilities in current DOV methods, highlighting the need for\nlong-term development to enhance practicality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u89c4\u907f\u6570\u636e\u96c6\u6240\u6709\u6743\u9a8c\u8bc1(DOV)\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u5f53\u524dDOV\u65b9\u6cd5\u7684\u6f0f\u6d1e\u3002", "motivation": "\u8bb8\u591a\u6570\u636e\u96c6\u662f\u4e13\u6709\u7684\u6216\u5305\u542b\u654f\u611f\u4fe1\u606f\uff0c\u8fd9\u4f7f\u5f97\u4e0d\u53d7\u9650\u5236\u7684\u6a21\u578b\u8bad\u7ec3\u6210\u4e3a\u95ee\u9898\u3002\u6570\u636e\u96c6\u6240\u6709\u6743\u9a8c\u8bc1\uff08DOV\uff09\u5df2\u6210\u4e3a\u4e00\u79cd\u6709\u524d\u666f\u7684\u7248\u6743\u4fdd\u62a4\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u6d4b\u672a\u7ecf\u6388\u6743\u7684\u6a21\u578b\u8bad\u7ec3\u548c\u8ffd\u8e2a\u975e\u6cd5\u6d3b\u52a8\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u89c4\u907f\u6846\u67b6\uff0c\u5176\u4e2d\u6559\u5e08\u6a21\u578b\u9996\u5148\u4ece\u7248\u6743\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\uff0c\u7136\u540e\u4f7f\u7528\u5206\u5e03\u5916\uff08OOD\uff09\u6570\u636e\u96c6\u4f5c\u4e3a\u4e2d\u4ecb\uff0c\u5c06\u4e0e\u4efb\u52a1\u76f8\u5173\u4f46\u4e0e\u6807\u8bc6\u7b26\u65e0\u5173\u7684\u9886\u57df\u77e5\u8bc6\u8f6c\u79fb\u7ed9\u66ff\u4ee3\u5b66\u751f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6cdb\u5316\u548c\u6709\u6548\u6027\u65b9\u9762\u540c\u65f6\u6d88\u9664\u4e86\u6240\u6709\u7248\u6743\u6807\u8bc6\u7b26\uff0c\u5e76\u4e14\u663e\u7740\u4f18\u4e8e\u4e5d\u79cd\u6700\u5148\u8fdb\u7684\u89c4\u907f\u653b\u51fb\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u9002\u4e2d\u3002", "conclusion": "\u672c\u6587\u63ed\u793a\u4e86\u5f53\u524d\u6570\u636e\u96c6\u6240\u6709\u6743\u9a8c\u8bc1\uff08DOV\uff09\u65b9\u6cd5\u4e2d\u7684\u5173\u952e\u6f0f\u6d1e\uff0c\u5f3a\u8c03\u4e86\u957f\u671f\u5f00\u53d1\u4ee5\u63d0\u9ad8\u5b9e\u7528\u6027\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2508.01186", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.01186", "abs": "https://arxiv.org/abs/2508.01186", "authors": ["Chaojia Yu", "Zihan Cheng", "Hanwen Cui", "Yishuo Gao", "Zexu Luo", "Yijin Wang", "Hangbin Zheng", "Yong Zhao"], "title": "A Survey on Agent Workflow -- Status and Future", "comment": "12 pages, 3 figures, accepted to IEEE Conference,\n  ICAIBD(International Conference of Artificial Intelligence and Big Data)\n  2025. This is the author's version, not the publisher's. See\n  https://ieeexplore.ieee.org/document/11082076", "summary": "In the age of large language models (LLMs), autonomous agents have emerged as\na powerful paradigm for achieving general intelligence. These agents\ndynamically leverage tools, memory, and reasoning capabilities to accomplish\nuser-defined goals. As agent systems grow in complexity, agent\nworkflows-structured orchestration frameworks-have become central to enabling\nscalable, controllable, and secure AI behaviors. This survey provides a\ncomprehensive review of agent workflow systems, spanning academic frameworks\nand industrial implementations. We classify existing systems along two key\ndimensions: functional capabilities (e.g., planning, multi-agent collaboration,\nexternal API integration) and architectural features (e.g., agent roles,\norchestration flows, specification languages). By comparing over 20\nrepresentative systems, we highlight common patterns, potential technical\nchallenges, and emerging trends. We further address concerns related to\nworkflow optimization strategies and security. Finally, we outline open\nproblems such as standardization and multimodal integration, offering insights\nfor future research at the intersection of agent design, workflow\ninfrastructure, and safe automation.", "AI": {"tldr": "This survey reviews agent workflow systems, classifying them by functional capabilities and architectural features, and highlights challenges, trends, and open problems.", "motivation": "Autonomous agents have emerged as a powerful paradigm for achieving general intelligence. As agent systems grow in complexity, agent workflows have become central to enabling scalable, controllable, and secure AI behaviors.", "method": "This survey classifies existing systems along two key dimensions: functional capabilities and architectural features. By comparing over 20 representative systems, it highlights common patterns, potential technical challenges, and emerging trends. It further address concerns related to workflow optimization strategies and security.", "result": "This survey provides a comprehensive review of agent workflow systems, spanning academic frameworks and industrial implementations.", "conclusion": "This survey outlines open problems such as standardization and multimodal integration, offering insights for future research at the intersection of agent design, workflow infrastructure, and safe automation."}}
{"id": "2508.00903", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.00903", "abs": "https://arxiv.org/abs/2508.00903", "authors": ["Advey Nandan", "Cheng-Ting Chou", "Amrit Kurakula", "Cole Blondin", "Kevin Zhu", "Vasu Sharma", "Sean O'Brien"], "title": "Universal Neurons in GPT-2: Emergence, Persistence, and Functional Impact", "comment": null, "summary": "We investigate the phenomenon of neuron universality in independently trained\nGPT-2 Small models, examining how these universal neurons-neurons with\nconsistently correlated activations across models-emerge and evolve throughout\ntraining. By analyzing five GPT-2 models at three checkpoints (100k, 200k, 300k\nsteps), we identify universal neurons through pairwise correlation analysis of\nactivations over a dataset of 5 million tokens. Ablation experiments reveal\nsignificant functional impacts of universal neurons on model predictions,\nmeasured via loss and KL divergence. Additionally, we quantify neuron\npersistence, demonstrating high stability of universal neurons across training\ncheckpoints, particularly in deeper layers. These findings suggest stable and\nuniversal representational structures emerge during neural network training.", "AI": {"tldr": "\u7814\u7a76\u4e86GPT-2 Small\u6a21\u578b\u4e2d\u901a\u7528\u795e\u7ecf\u5143\u7684\u73b0\u8c61\uff0c\u53d1\u73b0\u5b83\u4eec\u5bf9\u6a21\u578b\u9884\u6d4b\u6709\u663e\u8457\u5f71\u54cd\uff0c\u5e76\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8868\u73b0\u51fa\u9ad8\u7a33\u5b9a\u6027\u3002", "motivation": "\u6211\u4eec\u7814\u7a76\u4e86\u72ec\u7acb\u8bad\u7ec3\u7684GPT-2 Small\u6a21\u578b\u4e2d\u795e\u7ecf\u5143\u666e\u904d\u6027\u7684\u73b0\u8c61\uff0c\u7814\u7a76\u4e86\u8fd9\u4e9b\u901a\u7528\u795e\u7ecf\u5143\uff08\u5373\u5728\u6a21\u578b\u4e2d\u5177\u6709\u4e00\u81f4\u76f8\u5173\u6fc0\u6d3b\u7684\u795e\u7ecf\u5143\uff09\u5982\u4f55\u5728\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4ea7\u751f\u548c\u6f14\u53d8\u3002", "method": "\u901a\u8fc7\u5bf9\u4e00\u4e2a\u5305\u542b500\u4e07\u4e2atokens\u7684\u6570\u636e\u96c6\u8fdb\u884c\u6fc0\u6d3b\u7684\u4e24\u4e24\u76f8\u5173\u6027\u5206\u6790\uff0c\u8bc6\u522b\u901a\u7528\u795e\u7ecf\u5143\u3002", "result": "\u6d88\u878d\u5b9e\u9a8c\u63ed\u793a\u4e86\u901a\u7528\u795e\u7ecf\u5143\u5bf9\u6a21\u578b\u9884\u6d4b\u7684\u663e\u8457\u529f\u80fd\u5f71\u54cd\uff0c\u901a\u8fc7\u635f\u5931\u548cKL\u6563\u5ea6\u6765\u8861\u91cf\u3002\u6b64\u5916\uff0c\u6211\u4eec\u91cf\u5316\u4e86\u795e\u7ecf\u5143\u7684\u6301\u4e45\u6027\uff0c\u8868\u660e\u901a\u7528\u795e\u7ecf\u5143\u5728\u8bad\u7ec3\u68c0\u67e5\u70b9\u4e2d\u5177\u6709\u5f88\u9ad8\u7684\u7a33\u5b9a\u6027\uff0c\u7279\u522b\u662f\u5728\u66f4\u6df1\u7684\u5c42\u4e2d\u3002", "conclusion": "\u7a33\u5b9a\u7684\u548c\u901a\u7528\u7684\u8868\u5f81\u7ed3\u6784\u5728\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u3002"}}
{"id": "2508.02342", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.02342", "abs": "https://arxiv.org/abs/2508.02342", "authors": ["Yashar Deldjoo", "Nima Rafiee", "Mahdyar Ravanbakhsh"], "title": "Agentic Personalized Fashion Recommendation in the Age of Generative AI: Challenges, Opportunities, and Evaluation", "comment": null, "summary": "Fashion recommender systems (FaRS) face distinct challenges due to rapid\ntrend shifts, nuanced user preferences, intricate item-item compatibility, and\nthe complex interplay among consumers, brands, and influencers. Traditional\nrecommendation approaches, largely static and retrieval-focused, struggle to\neffectively capture these dynamic elements, leading to decreased user\nsatisfaction and elevated return rates. This paper synthesizes both academic\nand industrial viewpoints to map the distinctive output space and stakeholder\necosystem of modern FaRS, identifying the complex interplay among users,\nbrands, platforms, and influencers, and highlighting the unique data and\nmodeling challenges that arise.\n  We outline a research agenda for industrial FaRS, centered on five\nrepresentative scenarios spanning static queries, outfit composition, and\nmulti-turn dialogue, and argue that mixed-modality refinement-the ability to\ncombine image-based references (anchors) with nuanced textual constraints-is a\nparticularly critical task for real-world deployment. To this end, we propose\nan Agentic Mixed-Modality Refinement (AMMR) pipeline, which fuses multimodal\nencoders with agentic LLM planners and dynamic retrieval, bridging the gap\nbetween expressive user intent and fast-changing fashion inventories. Our work\nshows that moving beyond static retrieval toward adaptive, generative, and\nstakeholder-aware systems is essential to satisfy the evolving expectations of\nfashion consumers and brands.", "AI": {"tldr": "\u672c\u6587\u6982\u8ff0\u4e86\u5de5\u4e1a FaRS \u7684\u7814\u7a76\u8bae\u7a0b\uff0c\u4e13\u6ce8\u4e8e\u8de8\u8d8a\u9759\u6001\u67e5\u8be2\u3001\u670d\u88c5\u7ec4\u5408\u548c\u591a\u8f6e\u5bf9\u8bdd\u7684\u4e94\u4e2a\u4ee3\u8868\u6027\u573a\u666f\uff0c\u5e76\u63d0\u51fa\u6df7\u5408\u6a21\u6001\u7ec6\u5316\u5bf9\u4e8e\u5b9e\u9645\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u7531\u4e8e\u5feb\u901f\u7684\u8d8b\u52bf\u53d8\u5316\u3001\u7ec6\u81f4\u7684\u7528\u6237\u504f\u597d\u3001\u590d\u6742\u7684\u7269\u54c1-\u7269\u54c1\u517c\u5bb9\u6027\u4ee5\u53ca\u6d88\u8d39\u8005\u3001\u54c1\u724c\u548c\u5f71\u54cd\u8005\u4e4b\u95f4\u590d\u6742\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u65f6\u5c1a\u63a8\u8350\u7cfb\u7edf\uff08FaRS\uff09\u9762\u4e34\u7740\u72ec\u7279\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2aAgentic\u6df7\u5408\u6a21\u6001\u7ec6\u5316\uff08AMMR\uff09\u7ba1\u9053\uff0c\u8be5\u7ba1\u9053\u5c06\u591a\u6a21\u6001\u7f16\u7801\u5668\u4e0e\u4ee3\u7406LLM\u89c4\u5212\u5668\u548c\u52a8\u6001\u68c0\u7d22\u878d\u5408\u3002", "result": "\u8bba\u6587\u7efc\u5408\u4e86\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\u7684\u89c2\u70b9\uff0c\u4ee5\u63cf\u7ed8\u73b0\u4ee3FaRS\u7684\u72ec\u7279\u8f93\u51fa\u7a7a\u95f4\u548c\u5229\u76ca\u76f8\u5173\u8005\u751f\u6001\u7cfb\u7edf\uff0c\u786e\u5b9a\u4e86\u7528\u6237\u3001\u54c1\u724c\u3001\u5e73\u53f0\u548c\u5f71\u54cd\u8005\u4e4b\u95f4\u590d\u6742\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5e76\u5f3a\u8c03\u4e86\u7531\u6b64\u4ea7\u751f\u7684\u72ec\u7279\u6570\u636e\u548c\u5efa\u6a21\u6311\u6218\u3002", "conclusion": "\u8f6c\u5411\u81ea\u9002\u5e94\u3001\u751f\u6210\u548c\u5229\u76ca\u76f8\u5173\u8005\u611f\u77e5\u7684\u7cfb\u7edf\u5bf9\u4e8e\u6ee1\u8db3\u65f6\u5c1a\u6d88\u8d39\u8005\u548c\u54c1\u724c\u4e0d\u65ad\u53d1\u5c55\u7684\u671f\u671b\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2508.01317", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01317", "abs": "https://arxiv.org/abs/2508.01317", "authors": ["Xuemiao Zhang", "Can Ren", "Chengying Tu", "Rongxiang Weng", "Hongfei Yan", "Jingang Wang", "Xunliang Cai"], "title": "LinkQA: Synthesizing Diverse QA from Multiple Seeds Strongly Linked by Knowledge Points", "comment": null, "summary": "The advancement of large language models (LLMs) struggles with the scarcity\nof high-quality, diverse training data. To address this limitation, we propose\nLinkSyn, a novel knowledge point (KP) graph-based synthesis framework that\nenables flexible control over discipline and difficulty distributions while\nbalancing KP coverage and popularity. LinkSyn extracts KPs from\nquestion-answering (QA) seed data and constructs a KP graph to synthesize\ndiverse QA data from multiple seeds strongly linked by KPs and sampled from\ngraph walks. Specifically, LinkSyn incorporates (1) a knowledge distribution\nvalue function to guide the adjustment of path sampling probability and balance\nKP coverage and popularity during graph walks; (2) diffusion-based synthesis\nvia DeepSeek-R1 by leveraging multiple seeds with dense logical associations\nalong each path; and (3) high-difficulty QA enhancement within given\ndisciplines by flexible difficulty adjustments. By executing LinkSyn, we\nsynthesize LinkQA, a diverse multi-disciplinary QA dataset with 50B tokens.\nExtensive experiments on Llama-3 8B demonstrate that continual pre-training\nwith LinkQA yields an average improvement of $\\mathbf{11.51\\%}$ on MMLU and\nCMMLU, establishing new SOTA results. LinkQA consistently enhances performance\nacross model size and initial FLOPs scales.", "AI": {"tldr": "LinkSyn is a KP graph-based synthesis framework that enables flexible control over discipline and difficulty distributions while balancing KP coverage and popularity. It synthesizes LinkQA, a diverse multi-disciplinary QA dataset with 50B tokens. Continual pre-training with LinkQA yields an average improvement of 11.51% on MMLU and CMMLU, establishing new SOTA results.", "motivation": "The advancement of large language models (LLMs) struggles with the scarcity of high-quality, diverse training data.", "method": "a novel knowledge point (KP) graph-based synthesis framework that enables flexible control over discipline and difficulty distributions while balancing KP coverage and popularity", "result": "synthesize LinkQA, a diverse multi-disciplinary QA dataset with 50B tokens", "conclusion": "Continual pre-training with LinkQA yields an average improvement of 11.51% on MMLU and CMMLU, establishing new SOTA results. LinkQA consistently enhances performance across model size and initial FLOPs scales."}}
{"id": "2508.01079", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.01079", "abs": "https://arxiv.org/abs/2508.01079", "authors": ["Santiago Diaz", "Xinghui Hu", "Josiane Uwumukiza", "Giovanni Lavezzi", "Victor Rodriguez-Fernandez", "Richard Linares"], "title": "DreamSat-2.0: Towards a General Single-View Asteroid 3D Reconstruction", "comment": null, "summary": "To enhance asteroid exploration and autonomous spacecraft navigation, we\nintroduce DreamSat-2.0, a pipeline that benchmarks three state-of-the-art 3D\nreconstruction models-Hunyuan-3D, Trellis-3D, and Ouroboros-3D-on custom\nspacecraft and asteroid datasets. Our systematic analysis, using 2D perceptual\n(image quality) and 3D geometric (shape accuracy) metrics, reveals that model\nperformance is domain-dependent. While models produce higher-quality images of\ncomplex spacecraft, they achieve better geometric reconstructions for the\nsimpler forms of asteroids. New benchmarks are established, with Hunyuan-3D\nachieving top perceptual scores on spacecraft but its best geometric accuracy\non asteroids, marking a significant advance over our prior work.", "AI": {"tldr": "DreamSat-2.0\u662f\u4e00\u4e2apipeline\uff0c\u5b83\u5bf9\u4e09\u79cd3D\u91cd\u5efa\u6a21\u578b\u5728\u822a\u5929\u5668\u548c\u5c0f\u884c\u661f\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u4e3a\u4e86\u52a0\u5f3a\u5c0f\u884c\u661f\u63a2\u6d4b\u548c\u81ea\u4e3b\u822a\u5929\u5668\u5bfc\u822a\uff0c\u6211\u4eec\u5f15\u5165\u4e86DreamSat-2.0\u3002", "method": "\u5bf9\u4e09\u79cd\u6700\u5148\u8fdb\u76843D\u91cd\u5efa\u6a21\u578b\u2014\u2014Hunyuan-3D\u3001Trellis-3D\u548cOuroboros-3D\u2014\u2014\u5728\u5b9a\u5236\u7684\u822a\u5929\u5668\u548c\u5c0f\u884c\u661f\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u6a21\u578b\u6027\u80fd\u662f\u4f9d\u8d56\u4e8e\u9886\u57df\u7684\u3002\u867d\u7136\u6a21\u578b\u4ea7\u751f\u4e86\u66f4\u9ad8\u8d28\u91cf\u7684\u590d\u6742\u822a\u5929\u5668\u56fe\u50cf\uff0c\u4f46\u5b83\u4eec\u4e3a\u66f4\u7b80\u5355\u7684\u5c0f\u884c\u661f\u5f62\u72b6\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u51e0\u4f55\u91cd\u5efa\u3002\u5efa\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\uff0c", "conclusion": "Hunyuan-3D\u5728\u5b87\u5b99\u98de\u8239\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u611f\u77e5\u5206\u6570\uff0c\u4f46\u5728\u5c0f\u884c\u661f\u4e0a\u5b9e\u73b0\u4e86\u6700\u4f73\u7684\u51e0\u4f55\u7cbe\u5ea6\uff0c\u6807\u5fd7\u7740\u6bd4\u6211\u4eec\u4e4b\u524d\u7684\u5de5\u4f5c\u6709\u4e86\u663e\u8457\u7684\u8fdb\u6b65\u3002"}}
{"id": "2508.01191", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.01191", "abs": "https://arxiv.org/abs/2508.01191", "authors": ["Chengshuai Zhao", "Zhen Tan", "Pingchuan Ma", "Dawei Li", "Bohan Jiang", "Yancheng Wang", "Yingzhen Yang", "Huan Liu"], "title": "Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens", "comment": null, "summary": "Chain-of-Thought (CoT) prompting has been shown to improve Large Language\nModel (LLM) performance on various tasks. With this approach, LLMs appear to\nproduce human-like reasoning steps before providing answers (a.k.a., CoT\nreasoning), which often leads to the perception that they engage in deliberate\ninferential processes. However, some initial findings suggest that CoT\nreasoning may be more superficial than it appears, motivating us to explore\nfurther. In this paper, we study CoT reasoning via a data distribution lens and\ninvestigate if CoT reasoning reflects a structured inductive bias learned from\nin-distribution data, allowing the model to conditionally generate reasoning\npaths that approximate those seen during training. Thus, its effectiveness is\nfundamentally bounded by the degree of distribution discrepancy between the\ntraining data and the test queries. With this lens, we dissect CoT reasoning\nvia three dimensions: task, length, and format. To investigate each dimension,\nwe design DataAlchemy, an isolated and controlled environment to train LLMs\nfrom scratch and systematically probe them under various distribution\nconditions. Our results reveal that CoT reasoning is a brittle mirage that\nvanishes when it is pushed beyond training distributions. This work offers a\ndeeper understanding of why and when CoT reasoning fails, emphasizing the\nongoing challenge of achieving genuine and generalizable reasoning.", "AI": {"tldr": "CoT reasoning is a brittle mirage that vanishes when it is pushed beyond training distributions.", "motivation": "some initial findings suggest that CoT reasoning may be more superficial than it appears, motivating us to explore further. In this paper, we study CoT reasoning via a data distribution lens and investigate if CoT reasoning reflects a structured inductive bias learned from in-distribution data, allowing the model to conditionally generate reasoning paths that approximate those seen during training. Thus, its effectiveness is fundamentally bounded by the degree of distribution discrepancy between the training data and the test queries.", "method": "design DataAlchemy, an isolated and controlled environment to train LLMs from scratch and systematically probe them under various distribution conditions. dissect CoT reasoning via three dimensions: task, length, and format.", "result": "CoT reasoning is a brittle mirage that vanishes when it is pushed beyond training distributions.", "conclusion": "CoT reasoning is a brittle mirage that vanishes when it is pushed beyond training distributions. This work offers a deeper understanding of why and when CoT reasoning fails, emphasizing the ongoing challenge of achieving genuine and generalizable reasoning."}}
{"id": "2508.00909", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00909", "abs": "https://arxiv.org/abs/2508.00909", "authors": ["Aitor S\u00e1nchez-Ferrera", "Usue Mori", "Borja Calvo", "Jose A. Lozano"], "title": "NeuCoReClass AD: Redefining Self-Supervised Time Series Anomaly Detection", "comment": null, "summary": "Time series anomaly detection plays a critical role in a wide range of\nreal-world applications. Among unsupervised approaches, self-supervised\nlearning has gained traction for modeling normal behavior without the need of\nlabeled data. However, many existing methods rely on a single proxy task,\nlimiting their ability to capture meaningful patterns in normal data. Moreover,\nthey often depend on handcrafted transformations tailored specific domains,\nhindering their generalization accross diverse problems. To address these\nlimitations, we introduce NeuCoReClass AD, a self-supervised multi-task time\nseries anomaly detection framework that combines contrastive, reconstruction,\nand classification proxy tasks. Our method employs neural transformation\nlearning to generate augmented views that are informative, diverse, and\ncoherent, without requiring domain-specific knowledge. We evaluate NeuCoReClass\nAD across a wide range of benchmarks, demonstrating that it consistently\noutperforms both classical baselines and most deep-learning alternatives.\nFurthermore, it enables the characterization of distinct anomaly profiles in a\nfully unsupervised manner.", "AI": {"tldr": "NeuCoReClass AD\uff1a\u4e00\u79cd\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u7684\u81ea\u76d1\u7763\u591a\u4efb\u52a1\u6846\u67b6\uff0c\u5b83\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u4e0d\u9700\u8981\u9886\u57df\u77e5\u8bc6\u3002", "motivation": "\u73b0\u6709\u7684\u8bb8\u591a\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5355\u4e00\u7684\u4ee3\u7406\u4efb\u52a1\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u6355\u83b7\u6b63\u5e38\u6570\u636e\u4e2d\u6709\u610f\u4e49\u6a21\u5f0f\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u5b83\u4eec\u901a\u5e38\u4f9d\u8d56\u4e8e\u4e3a\u7279\u5b9a\u9886\u57df\u91cf\u8eab\u5b9a\u5236\u7684\u624b\u5de5\u8f6c\u6362\uff0c\u4ece\u800c\u963b\u788d\u4e86\u5b83\u4eec\u5728\u5404\u79cd\u95ee\u9898\u4e2d\u7684\u63a8\u5e7f\u3002", "method": "NeuCoReClass AD\uff0c\u4e00\u4e2a\u7ed3\u5408\u4e86\u5bf9\u6bd4\u3001\u91cd\u5efa\u548c\u5206\u7c7b\u4ee3\u7406\u4efb\u52a1\u7684\u81ea\u76d1\u7763\u591a\u4efb\u52a1\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u795e\u7ecf\u8f6c\u6362\u5b66\u4e60\u6765\u751f\u6210\u4fe1\u606f\u4e30\u5bcc\u3001\u591a\u6837\u5316\u4e14\u8fde\u8d2f\u7684\u589e\u5f3a\u89c6\u56fe\uff0c\u800c\u65e0\u9700\u7279\u5b9a\u9886\u57df\u7684\u77e5\u8bc6\u3002", "result": "NeuCoReClass AD \u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u7ecf\u5178\u57fa\u7ebf\u548c\u5927\u591a\u6570\u6df1\u5ea6\u5b66\u4e60\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "NeuCoReClass AD \u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u7ecf\u5178\u57fa\u7ebf\u548c\u5927\u591a\u6570\u6df1\u5ea6\u5b66\u4e60\u66ff\u4ee3\u65b9\u6848, \u5e76\u4e14\u80fd\u591f\u5728\u5b8c\u5168\u65e0\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u8868\u5f81\u4e0d\u540c\u7684\u5f02\u5e38\u914d\u7f6e\u6587\u4ef6\u3002"}}
{"id": "2508.02435", "categories": ["cs.IR", "H.3"], "pdf": "https://arxiv.org/pdf/2508.02435", "abs": "https://arxiv.org/abs/2508.02435", "authors": ["Shengbo Gong", "Xianfeng Tang", "Carl Yang", "Wei jin"], "title": "Beyond Chunks and Graphs: Retrieval-Augmented Generation through Triplet-Driven Thinking", "comment": "19 pages", "summary": "Retrieval-augmented generation (RAG) is critical for reducing hallucinations\nand incorporating external knowledge into Large Language Models (LLMs).\nHowever, advanced RAG systems face a trade-off between performance and\nefficiency. Multi-round RAG approaches achieve strong reasoning but incur\nexcessive LLM calls and token costs, while Graph RAG methods suffer from\ncomputationally expensive, error-prone graph construction and retrieval\nredundancy. To address these challenges, we propose T$^2$RAG, a novel framework\nthat operates on a simple, graph-free knowledge base of atomic triplets.\nT$^2$RAG leverages an LLM to decompose questions into searchable triplets with\nplaceholders, which it then iteratively resolves by retrieving evidence from\nthe triplet database. Empirical results show that T$^2$RAG significantly\noutperforms state-of-the-art multi-round and Graph RAG methods, achieving an\naverage performance gain of up to 11\\% across six datasets while reducing\nretrieval costs by up to 45\\%. Our code is available at\nhttps://github.com/rockcor/T2RAG", "AI": {"tldr": "T$^2$RAG \u4f18\u4e8e\u5176\u4ed6 RAG \u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u9ad8\u7ea7 RAG \u7cfb\u7edf\u9762\u4e34\u6027\u80fd\u548c\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u3002\u591a\u8f6e RAG \u65b9\u6cd5\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u4f1a\u5bfc\u81f4\u8fc7\u591a\u7684 LLM \u8c03\u7528\u548c token \u6210\u672c\uff0c\u800c\u56fe RAG \u65b9\u6cd5\u5219\u9762\u4e34\u8ba1\u7b97\u91cf\u5927\u3001\u5bb9\u6613\u51fa\u9519\u7684\u56fe\u6784\u5efa\u548c\u68c0\u7d22\u5197\u4f59\u95ee\u9898\u3002", "method": "T$^2$RAG\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u5b83\u5728\u7b80\u5355\u7684\u3001\u65e0\u56fe\u7684\u4e09\u5143\u7ec4\u539f\u5b50\u77e5\u8bc6\u5e93\u4e0a\u8fd0\u884c\u3002T$^2$RAG \u5229\u7528 LLM \u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u5e26\u6709\u5360\u4f4d\u7b26\u7684\u53ef\u641c\u7d22\u4e09\u5143\u7ec4\uff0c\u7136\u540e\u901a\u8fc7\u4ece\u4e09\u5143\u7ec4\u6570\u636e\u5e93\u4e2d\u68c0\u7d22\u8bc1\u636e\u6765\u8fed\u4ee3\u5730\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "result": "T$^2$RAG \u7684\u5e73\u5747\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe 11%\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u9ad8\u8fbe 45% \u7684\u68c0\u7d22\u6210\u672c\u3002", "conclusion": "T$^2$RAG\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u591a\u8f6e\u548c\u56fe RAG \u65b9\u6cd5\uff0c\u5728\u516d\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e73\u5747\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe 11%\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u9ad8\u8fbe 45% \u7684\u68c0\u7d22\u6210\u672c\u3002"}}
{"id": "2508.01326", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01326", "abs": "https://arxiv.org/abs/2508.01326", "authors": ["Xuemiao Zhang", "Chengying Tu", "Can Ren", "Rongxiang Weng", "Hongfei Yan", "Jingang Wang", "Xunliang Cai"], "title": "Large-Scale Diverse Synthesis for Mid-Training", "comment": null, "summary": "The scarcity of high-quality, knowledge-intensive training data hinders the\ndevelopment of large language models (LLMs), as traditional corpora provide\nlimited information. Previous studies have synthesized and integrated\ncorpora-dependent question-answering (QA) data to improve model performance but\nface challenges in QA data scalability and knowledge diversity, particularly in\ncross-domain contexts. Furthermore, leveraging our designed discipline and\ndifficulty annotation system, we probe model deficiencies in STEM disciplines\nand high-difficulty data. To overcome these limitations, we propose a novel\ndiversified pipeline to synthesize BoostQA, a 100B-token large-scale QA\ndataset. Our synthesis framework: (1) curates seed data from heterogeneous\nsources; (2) utilizes DeepSeek-R1 to implement STEM-focused multi-grade\nsynthesis to boost data diversity and high-difficulty synthesis to mitigate\ndifficulty degradation; (3) refines answers via DeepSeek-V3 to improve output\nquality. We utilize BoostQA in mid-training, a mid-stage between pre-training\nand post-training, to optimize domain-specific knowledge acquisition and\nenhance data quality. Our method enables Llama-3 8B, mid-trained on a 40B-token\ndataset, to achieve an average improvement of $\\mathbf{12.74\\%}$ on MMLU and\nCMMLU and establish SOTA average performance across 12 benchmarks. BoostQA also\ndemonstrates robust scalability, with performance consistently improving as\nmodel size, data volume, and initial FLOPs scale.", "AI": {"tldr": "\u63d0\u51fa\u4e86BoostQA\uff0c\u4e00\u4e2a\u5927\u89c4\u6a21QA\u6570\u636e\u96c6\uff0c\u7528\u4e8emid-training\uff0c\u663e\u8457\u63d0\u5347\u4e86Llama-3 8B\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u9ad8\u8d28\u91cf\u3001\u77e5\u8bc6\u5bc6\u96c6\u578b\u8bad\u7ec3\u6570\u636e\u7684\u7a00\u7f3a\u6027\u963b\u788d\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u53d1\u5c55\uff0c\u56e0\u4e3a\u4f20\u7edf\u7684\u8bed\u6599\u5e93\u63d0\u4f9b\u7684\u4fe1\u606f\u6709\u9650\u3002\u4ee5\u5f80\u7684\u7814\u7a76\u5408\u6210\u4e86\u4f9d\u8d56\u4e8e\u8bed\u6599\u5e93\u7684\u95ee\u7b54\uff08QA\uff09\u6570\u636e\u4ee5\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u5728QA\u6570\u636e\u7684\u53ef\u6269\u5c55\u6027\u548c\u77e5\u8bc6\u591a\u6837\u6027\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u8de8\u9886\u57df\u73af\u5883\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u6837\u5316pipeline\u6765\u5408\u6210BoostQA\uff0c\u4e00\u4e2a100B token\u7684\u5927\u89c4\u6a21QA\u6570\u636e\u96c6\u3002\u8be5\u6846\u67b6\u5305\u62ec\uff1a(1) \u4ece\u5f02\u6784\u6765\u6e90\u6536\u96c6\u79cd\u5b50\u6570\u636e\uff1b(2) \u5229\u7528DeepSeek-R1\u5b9e\u73b0STEM\u9886\u57df\u7684\u591a\u68af\u5ea6\u5408\u6210\uff0c\u4ee5\u63d0\u9ad8\u6570\u636e\u591a\u6837\u6027\u548c\u9ad8\u96be\u5ea6\u5408\u6210\uff0c\u4ee5\u51cf\u8f7b\u96be\u5ea6\u9000\u5316\uff1b(3) \u901a\u8fc7DeepSeek-V3\u6539\u8fdb\u7b54\u6848\uff0c\u4ee5\u63d0\u9ad8\u8f93\u51fa\u8d28\u91cf\u3002", "result": "BoostQA\u5728\u6a21\u578b\u89c4\u6a21\u3001\u6570\u636e\u91cf\u548c\u521d\u59cbFLOPs\u6269\u5c55\u65f6\uff0c\u6027\u80fd\u6301\u7eed\u63d0\u9ad8\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "Llama-3 8B\u6a21\u578b\u901a\u8fc7\u5728\u5305\u542b40B token\u7684BoostQA\u6570\u636e\u96c6\u4e0a\u8fdb\u884cmid-training\uff0c\u5728MMLU\u548cCMMLU\u4e0a\u5e73\u5747\u63d0\u5347\u4e8612.74%\uff0c\u5e76\u572812\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86SOTA\u7684\u5e73\u5747\u6027\u80fd\u3002BoostQA\u8fd8\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2508.01087", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01087", "abs": "https://arxiv.org/abs/2508.01087", "authors": ["Ryan Rabinowitz", "Steve Cruz", "Walter Scheirer", "Terrance E. Boult"], "title": "COSTARR: Consolidated Open Set Technique with Attenuation for Robust Recognition", "comment": "Accepted at ICCV 2025", "summary": "Handling novelty remains a key challenge in visual recognition systems.\nExisting open-set recognition (OSR) methods rely on the familiarity hypothesis,\ndetecting novelty by the absence of familiar features. We propose a novel\nattenuation hypothesis: small weights learned during training attenuate\nfeatures and serve a dual role-differentiating known classes while discarding\ninformation useful for distinguishing known from unknown classes. To leverage\nthis overlooked information, we present COSTARR, a novel approach that combines\nboth the requirement of familiar features and the lack of unfamiliar ones. We\nprovide a probabilistic interpretation of the COSTARR score, linking it to the\nlikelihood of correct classification and belonging in a known class. To\ndetermine the individual contributions of the pre- and post-attenuated features\nto COSTARR's performance, we conduct ablation studies that show both\npre-attenuated deep features and the underutilized post-attenuated Hadamard\nproduct features are essential for improving OSR. Also, we evaluate COSTARR in\na large-scale setting using ImageNet2012-1K as known data and NINCO,\niNaturalist, OpenImage-O, and other datasets as unknowns, across multiple\nmodern pre-trained architectures (ViTs, ConvNeXts, and ResNet). The experiments\ndemonstrate that COSTARR generalizes effectively across various architectures\nand significantly outperforms prior state-of-the-art methods by incorporating\npreviously discarded attenuation information, advancing open-set recognition\ncapabilities.", "AI": {"tldr": "COSTARR\u7ed3\u5408\u4e86\u719f\u6089\u548c\u4e0d\u719f\u6089\u7279\u5f81\u6765\u63d0\u9ad8\u5f00\u653e\u96c6\u8bc6\u522b\u80fd\u529b\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u5904\u7406\u65b0\u9896\u6027\u4ecd\u7136\u662f\u89c6\u89c9\u8bc6\u522b\u7cfb\u7edf\u4e2d\u7684\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u73b0\u6709\u7684\u5f00\u653e\u96c6\u8bc6\u522b\uff08OSR\uff09\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u719f\u6089\u6027\u5047\u8bbe\uff0c\u901a\u8fc7\u7f3a\u5c11\u719f\u6089\u7279\u5f81\u6765\u68c0\u6d4b\u65b0\u9896\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8870\u51cf\u5047\u8bbe\uff1a\u5728\u8bad\u7ec3\u671f\u95f4\u5b66\u4e60\u7684\u5c0f\u6743\u91cd\u4f1a\u8870\u51cf\u7279\u5f81\uff0c\u5e76\u53d1\u6325\u53cc\u91cd\u4f5c\u7528\u2014\u2014\u533a\u5206\u5df2\u77e5\u7c7b\u522b\uff0c\u540c\u65f6\u4e22\u5f03\u533a\u5206\u5df2\u77e5\u7c7b\u522b\u4e0e\u672a\u77e5\u7c7b\u522b\u7684\u4fe1\u606f\u3002\u4e3a\u4e86\u5229\u7528\u8fd9\u79cd\u88ab\u5ffd\u89c6\u7684\u4fe1\u606f\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684COSTARR\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u719f\u6089\u7279\u5f81\u7684\u9700\u6c42\u548c\u7f3a\u4e4f\u4e0d\u719f\u6089\u7279\u5f81\u7684\u9700\u6c42\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCOSTARR\u5728\u5404\u79cd\u67b6\u6784\u4e0a\u6709\u6548\u5730\u63a8\u5e7f\uff0c\u5e76\u4e14\u901a\u8fc7\u5408\u5e76\u5148\u524d\u4e22\u5f03\u7684\u8870\u51cf\u4fe1\u606f\uff0c\u663e\u8457\u4f18\u4e8e\u5148\u524d\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u5f00\u653e\u96c6\u8bc6\u522b\u80fd\u529b\u3002", "conclusion": "COSTARR\u901a\u8fc7\u7ed3\u5408\u719f\u6089\u7279\u5f81\u7684\u9700\u6c42\u548c\u7f3a\u4e4f\u4e0d\u719f\u6089\u7279\u5f81\u7684\u9700\u6c42\uff0c\u5728\u5404\u79cd\u67b6\u6784\u4e0a\u6709\u6548\u5730\u63a8\u5e7f\uff0c\u5e76\u4e14\u901a\u8fc7\u5408\u5e76\u5148\u524d\u4e22\u5f03\u7684\u8870\u51cf\u4fe1\u606f\uff0c\u663e\u8457\u4f18\u4e8e\u5148\u524d\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u5f00\u653e\u96c6\u8bc6\u522b\u80fd\u529b\u3002"}}
{"id": "2508.01203", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01203", "abs": "https://arxiv.org/abs/2508.01203", "authors": ["Junjie Shi", "Wei Ma", "Shi Ying", "Lingxiao Jiang", "Yang liu", "Bo Du"], "title": "Importance Sampling is All You Need: Predict LLM's performance on new benchmark by reusing existing benchmark", "comment": null, "summary": "With the rapid advancement of large language models , code generation has\nbecome a key benchmark for evaluating LLM capabilities. However, existing\nbenchmarks face two major challenges: (1) the escalating cost of constructing\nhigh-quality test suites and reference solutions, and (2) the increasing risk\nof data contamination, which undermines the reliability of benchmark-based\nevaluations. In this paper, we propose BIS, a prompt-centric evaluation\nframework that enables ground-truth-free prediction of LLM performance on code\ngeneration tasks. Rather than executing generated code, BIS estimates\nperformance metrics by analyzing the prompt distribution alone. Built on\nimportance sampling theory and implemented using Importance Weighted\nAutoencoders, our method reweights samples from existing annotated benchmarks\nto estimate performance on new, unseen benchmarks. To stabilize the estimation,\nwe introduce weight truncation strategies and compute marginal expectations\nacross the fitted distributions. BIS serves as a complementary tool that\nsupports benchmark development and validation under constrained resources,\noffering actionable and quick feedback for prompt selection and contamination\nassessment. We conduct extensive experiments involving 8,000 evaluation points\nacross 4 CodeLlama models and 9 diverse benchmarks. Our framework achieves an\naverage absolute prediction error of 1.1% for code correctness scores, with\nbest- and worst-case errors of 0.3% and 1.9%, respectively. It also generalizes\nwell to other metrics, attaining average absolute errors of 2.15% for pass@1.\nThese results demonstrate the reliability and broad applicability of BIS, which\ncan significantly reduce the cost and effort of benchmarking LLMs in\ncode-related tasks.", "AI": {"tldr": "BIS \u662f\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u5206\u6790\u63d0\u793a\u5206\u5e03\u6765\u9884\u6d4b LLM \u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u57fa\u51c6\u6d4b\u8bd5\u7684\u6210\u672c\u548c\u5de5\u4f5c\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a(1) \u6784\u5efa\u9ad8\u8d28\u91cf\u6d4b\u8bd5\u5957\u4ef6\u548c\u53c2\u8003\u89e3\u51b3\u65b9\u6848\u7684\u6210\u672c\u4e0d\u65ad\u5347\u7ea7\uff0c(2) \u6570\u636e\u6c61\u67d3\u7684\u98ce\u9669\u65e5\u76ca\u589e\u52a0\uff0c\u8fd9\u7834\u574f\u4e86\u57fa\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u7684\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u63d0\u793a\u4e3a\u4e2d\u5fc3\u7684\u8bc4\u4f30\u6846\u67b6 BIS\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5206\u6790\u63d0\u793a\u5206\u5e03\u6765\u9884\u6d4b LLM \u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u800c\u4e0d\u662f\u6267\u884c\u751f\u6210\u7684\u4ee3\u7801\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u91cd\u8981\u6027\u62bd\u6837\u7406\u8bba\uff0c\u5e76\u4f7f\u7528\u91cd\u8981\u6027\u52a0\u6743\u81ea\u52a8\u7f16\u7801\u5668\u5b9e\u73b0\uff0c\u901a\u8fc7\u5bf9\u73b0\u6709\u5e26\u6ce8\u91ca\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6837\u672c\u8fdb\u884c\u91cd\u65b0\u52a0\u6743\u6765\u4f30\u8ba1\u65b0\u57fa\u51c6\u6d4b\u8bd5\u7684\u6027\u80fd\u3002\u4e3a\u4e86\u7a33\u5b9a\u4f30\u8ba1\uff0c\u5f15\u5165\u4e86\u6743\u91cd\u622a\u65ad\u7b56\u7565\uff0c\u5e76\u8ba1\u7b97\u62df\u5408\u5206\u5e03\u7684\u8fb9\u9645\u671f\u671b\u3002", "result": "BIS \u6846\u67b6\u5728\u4ee3\u7801\u6b63\u786e\u6027\u8bc4\u5206\u65b9\u9762\u5b9e\u73b0\u4e86 1.1% \u7684\u5e73\u5747\u7edd\u5bf9\u9884\u6d4b\u8bef\u5dee\uff0c\u6700\u4f73\u548c\u6700\u5dee\u60c5\u51b5\u4e0b\u7684\u8bef\u5dee\u5206\u522b\u4e3a 0.3% \u548c 1.9%\u3002\u5b83\u8fd8\u53ef\u4ee5\u5f88\u597d\u5730\u63a8\u5e7f\u5230\u5176\u4ed6\u6307\u6807\uff0cpass@1 \u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a 2.15%\u3002", "conclusion": "BIS \u6846\u67b6\u80fd\u591f\u4ee5\u4f4e\u6210\u672c\u548c\u4f4e\u5de5\u4f5c\u91cf\u7684\u65b9\u5f0f\u663e\u8457\u964d\u4f4e LLM \u5728\u4ee3\u7801\u76f8\u5173\u4efb\u52a1\u4e2d\u7684\u57fa\u51c6\u6d4b\u8bd5\u6210\u672c\u548c\u5de5\u4f5c\u91cf\uff0c\u5e76\u4e14\u5177\u6709\u53ef\u9760\u6027\u548c\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2508.00912", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.00912", "abs": "https://arxiv.org/abs/2508.00912", "authors": ["Ziyao Wang", "Guoheng Sun", "Yexiao He", "Zheyu Shen", "Bowei Tian", "Ang Li"], "title": "Predictive Auditing of Hidden Tokens in LLM APIs via Reasoning Length Estimation", "comment": null, "summary": "Commercial LLM services often conceal internal reasoning traces while still\ncharging users for every generated token, including those from hidden\nintermediate steps, raising concerns of token inflation and potential\noverbilling. This gap underscores the urgent need for reliable token auditing,\nyet achieving it is far from straightforward: cryptographic verification (e.g.,\nhash-based signature) offers little assurance when providers control the entire\nexecution pipeline, while user-side prediction struggles with the inherent\nvariance of reasoning LLMs, where token usage fluctuates across domains and\nprompt styles. To bridge this gap, we present PALACE (Predictive Auditing of\nLLM APIs via Reasoning Token Count Estimation), a user-side framework that\nestimates hidden reasoning token counts from prompt-answer pairs without access\nto internal traces. PALACE introduces a GRPO-augmented adaptation module with a\nlightweight domain router, enabling dynamic calibration across diverse\nreasoning tasks and mitigating variance in token usage patterns. Experiments on\nmath, coding, medical, and general reasoning benchmarks show that PALACE\nachieves low relative error and strong prediction accuracy, supporting both\nfine-grained cost auditing and inflation detection. Taken together, PALACE\nrepresents an important first step toward standardized predictive auditing,\noffering a practical path to greater transparency, accountability, and user\ntrust.", "AI": {"tldr": "PALACE \u662f\u4e00\u79cd\u7528\u6237\u7aef\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u63a8\u7406\u4ee4\u724c\u8ba1\u6570\u4f30\u8ba1\u6765\u9884\u6d4b\u6027\u5730\u5ba1\u8ba1 LLM API\uff0c\u65e0\u9700\u8bbf\u95ee\u5185\u90e8\u8ddf\u8e2a\uff0c\u4ece\u800c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u6210\u672c\u5ba1\u8ba1\u548c\u901a\u8d27\u81a8\u80c0\u68c0\u6d4b\u3002", "motivation": "\u5546\u4e1a LLM \u670d\u52a1\u901a\u5e38\u4f1a\u9690\u85cf\u5185\u90e8\u63a8\u7406\u75d5\u8ff9\uff0c\u540c\u65f6\u4ecd\u7136\u5411\u7528\u6237\u6536\u53d6\u6bcf\u4e2a\u751f\u6210\u7684\u4ee4\u724c\u7684\u8d39\u7528\uff0c\u5305\u62ec\u6765\u81ea\u9690\u85cf\u7684\u4e2d\u95f4\u6b65\u9aa4\u7684\u4ee4\u724c\uff0c\u8fd9\u5f15\u8d77\u4e86\u4eba\u4eec\u5bf9\u4ee4\u724c\u81a8\u80c0\u548c\u6f5c\u5728\u8fc7\u5ea6\u8ba1\u8d39\u7684\u62c5\u5fe7\u3002\u8fd9\u79cd\u5dee\u8ddd \u043f\u043e\u0434\u0447\u0435\u0440\u043a\u0438\u0432\u0430\u0435\u0442 \u4e86\u5bf9\u53ef\u9760\u4ee4\u724c\u5ba1\u8ba1\u7684\u8feb\u5207\u9700\u6c42\u3002", "method": "PALACE \u5f15\u5165\u4e86\u4e00\u4e2a\u5177\u6709\u8f7b\u91cf\u7ea7\u57df\u8def\u7531\u7684 GRPO \u589e\u5f3a\u9002\u5e94\u6a21\u5757\uff0c\u4ece\u800c\u80fd\u591f\u8de8\u4e0d\u540c\u7684\u63a8\u7406\u4efb\u52a1\u8fdb\u884c\u52a8\u6001\u6821\u51c6\uff0c\u5e76\u51cf\u8f7b\u4ee4\u724c\u4f7f\u7528\u6a21\u5f0f\u7684\u5dee\u5f02\u3002", "result": "\u5728\u6570\u5b66\u3001\u7f16\u7801\u3001\u533b\u7597\u548c\u4e00\u822c\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPALACE \u5b9e\u73b0\u4e86\u8f83\u4f4e\u7684\u76f8\u5bf9\u8bef\u5dee\u548c\u8f83\u5f3a\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u7684\u6210\u672c\u5ba1\u8ba1\u548c\u81a8\u80c0\u68c0\u6d4b\u3002", "conclusion": "PALACE \u662f\u8fc8\u5411\u6807\u51c6\u5316\u9884\u6d4b\u5ba1\u8ba1\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u4e3a\u63d0\u9ad8\u900f\u660e\u5ea6\u3001\u95ee\u8d23\u5236\u548c\u7528\u6237\u4fe1\u4efb\u63d0\u4f9b\u4e86\u4e00\u6761\u5207\u5b9e\u53ef\u884c\u7684\u9014\u5f84\u3002"}}
{"id": "2508.02451", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02451", "abs": "https://arxiv.org/abs/2508.02451", "authors": ["Zhaoyu Hu", "Hao Guo", "Yuan Tian", "Erpeng Xue", "Jianyang Wang", "Xianyang Qi", "Hongxiang Lin", "Lei Wang", "Sheng Chen"], "title": "Dynamic Forgetting and Spatio-Temporal Periodic Interest Modeling for Local-Life Service Recommendation", "comment": null, "summary": "In the context of the booming digital economy, recommendation systems, as a\nkey link connecting users and numerous services, face challenges in modeling\nuser behavior sequences on local-life service platforms, including the sparsity\nof long sequences and strong spatio-temporal dependence. Such challenges can be\naddressed by drawing an analogy to the forgetting process in human memory. This\nis because users' responses to recommended content follow the recency effect\nand the cyclicality of memory. By exploring this, this paper introduces the\nforgetting curve and proposes Spatio-Temporal periodic Interest Modeling (STIM)\nwith long sequences for local-life service recommendation. STIM integrates\nthree key components: a dynamic masking module based on the forgetting curve,\nwhich is used to extract both recent spatiotemporal features and periodic\nspatiotemporal features; a query-based mixture of experts (MoE) approach that\ncan adaptively activate expert networks under different dynamic masks, enabling\nthe collaborative modeling of time, location, and items; and a hierarchical\nmulti-interest network unit, which captures multi-interest representations by\nmodeling the hierarchical interactions between the shallow and deep semantics\nof users' recent behaviors. By introducing the STIM method, we conducted online\nA/B tests and achieved a 1.54\\% improvement in gross transaction volume (GTV).\nIn addition, extended offline experiments also showed improvements. STIM has\nbeen deployed in a large-scale local-life service recommendation system,\nserving hundreds of millions of daily active users in core application\nscenarios.", "AI": {"tldr": "This paper introduces the forgetting curve and proposes Spatio-Temporal periodic Interest Modeling (STIM) with long sequences for local-life service recommendation.", "motivation": "modeling user behavior sequences on local-life service platforms, including the sparsity of long sequences and strong spatio-temporal dependence", "method": "Spatio-Temporal periodic Interest Modeling (STIM)", "result": "achieved a 1.54% improvement in gross transaction volume (GTV)", "conclusion": "STIM has been deployed in a large-scale local-life service recommendation system, serving hundreds of millions of daily active users in core application scenarios."}}
{"id": "2508.01370", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.01370", "abs": "https://arxiv.org/abs/2508.01370", "authors": ["Roman Koshkin", "Pengyu Dai", "Nozomi Fujikawa", "Masahito Togami", "Marco Visentini-Scarzanella"], "title": "MaRGen: Multi-Agent LLM Approach for Self-Directed Market Research and Analysis", "comment": null, "summary": "We present an autonomous framework that leverages Large Language Models\n(LLMs) to automate end-to-end business analysis and market report generation.\nAt its core, the system employs specialized agents - Researcher, Reviewer,\nWriter, and Retriever - that collaborate to analyze data and produce\ncomprehensive reports. These agents learn from real professional consultants'\npresentation materials at Amazon through in-context learning to replicate\nprofessional analytical methodologies. The framework executes a multi-step\nprocess: querying databases, analyzing data, generating insights, creating\nvisualizations, and composing market reports. We also introduce a novel\nLLM-based evaluation system for assessing report quality, which shows alignment\nwith expert human evaluations. Building on these evaluations, we implement an\niterative improvement mechanism that optimizes report quality through automated\nreview cycles. Experimental results show that report quality can be improved by\nboth automated review cycles and consultants' unstructured knowledge. In\nexperimental validation, our framework generates detailed 6-page reports in 7\nminutes at a cost of approximately \\$1. Our work could be an important step to\nautomatically create affordable market insights.", "AI": {"tldr": "This paper presents an autonomous framework that leverages Large Language Models (LLMs) to automate end-to-end business analysis and market report generation.", "motivation": "automate end-to-end business analysis and market report generation", "method": "The system employs specialized agents - Researcher, Reviewer, Writer, and Retriever - that collaborate to analyze data and produce comprehensive reports. These agents learn from real professional consultants' presentation materials at Amazon through in-context learning to replicate professional analytical methodologies. The framework executes a multi-step process: querying databases, analyzing data, generating insights, creating visualizations, and composing market reports.", "result": "report quality can be improved by both automated review cycles and consultants' unstructured knowledge. In experimental validation, our framework generates detailed 6-page reports in 7 minutes at a cost of approximately $1.", "conclusion": "This work could be an important step to automatically create affordable market insights."}}
{"id": "2508.01095", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01095", "abs": "https://arxiv.org/abs/2508.01095", "authors": ["Mikhail Bychkov", "Matey Yordanov", "Andrei Kuchma"], "title": "AURA: A Hybrid Spatiotemporal-Chromatic Framework for Robust, Real-Time Detection of Industrial Smoke Emissions", "comment": "19 pages, 3 figures", "summary": "This paper introduces AURA, a novel hybrid spatiotemporal-chromatic framework\ndesigned for robust, real-time detection and classification of industrial smoke\nemissions. The framework addresses critical limitations of current monitoring\nsystems, which often lack the specificity to distinguish smoke types and\nstruggle with environmental variability. AURA leverages both the dynamic\nmovement patterns and the distinct color characteristics of industrial smoke to\nprovide enhanced accuracy and reduced false positives. This framework aims to\nsignificantly improve environmental compliance, operational safety, and public\nhealth outcomes by enabling precise, automated monitoring of industrial\nemissions.", "AI": {"tldr": "AURA\u662f\u4e00\u79cd\u7528\u4e8e\u5de5\u4e1a\u70df\u96fe\u6392\u653e\u7684\u5b9e\u65f6\u68c0\u6d4b\u548c\u5206\u7c7b\u7684\u65b0\u6846\u67b6\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u4e86\u8bef\u62a5\u3002", "motivation": "\u5f53\u524d\u7684\u76d1\u6d4b\u7cfb\u7edf\u7f3a\u4e4f\u533a\u5206\u70df\u96fe\u7c7b\u578b\u7684\u7279\u5f02\u6027\uff0c\u5e76\u4e14\u96be\u4ee5\u5e94\u5bf9\u73af\u5883\u53d8\u5316\u3002", "method": "AURA\uff1a\u4e00\u79cd\u65b0\u578b\u7684\u6df7\u5408\u65f6\u7a7a\u8272\u6846\u67b6", "result": "\u589e\u5f3a\u4e86\u51c6\u786e\u6027\u5e76\u51cf\u5c11\u4e86\u8bef\u62a5\u3002", "conclusion": "AURA\u6846\u67b6\u901a\u8fc7\u7cbe\u786e\u7684\u81ea\u52a8\u76d1\u6d4b\u5de5\u4e1a\u6392\u653e\uff0c\u663e\u8457\u63d0\u9ad8\u73af\u5883\u5408\u89c4\u6027\u3001\u8fd0\u8425\u5b89\u5168\u548c\u516c\u4f17\u5065\u5eb7\u6c34\u5e73\u3002"}}
{"id": "2508.01208", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01208", "abs": "https://arxiv.org/abs/2508.01208", "authors": ["Mingchen Mei", "Yi Li", "YiYao Qian", "Zijun Jia"], "title": "Calibrated Prediction Set in Fault Detection with Risk Guarantees via Significance Tests", "comment": null, "summary": "Fault detection is crucial for ensuring the safety and reliability of modern\nindustrial systems. However, a significant scientific challenge is the lack of\nrigorous risk control and reliable uncertainty quantification in existing\ndiagnostic models, particularly when facing complex scenarios such as\ndistributional shifts. To address this issue, this paper proposes a novel fault\ndetection method that integrates significance testing with the conformal\nprediction framework to provide formal risk guarantees. The method transforms\nfault detection into a hypothesis testing task by defining a nonconformity\nmeasure based on model residuals. It then leverages a calibration dataset to\ncompute p-values for new samples, which are used to construct prediction sets\nmathematically guaranteed to contain the true label with a user-specified\nprobability, $1-\\alpha$. Fault classification is subsequently performed by\nanalyzing the intersection of the constructed prediction set with predefined\nnormal and fault label sets. Experimental results on cross-domain fault\ndiagnosis tasks validate the theoretical properties of our approach. The\nproposed method consistently achieves an empirical coverage rate at or above\nthe nominal level ($1-\\alpha$), demonstrating robustness even when the\nunderlying point-prediction models perform poorly. Furthermore, the results\nreveal a controllable trade-off between the user-defined risk level ($\\alpha$)\nand efficiency, where higher risk tolerance leads to smaller average prediction\nset sizes. This research contributes a theoretically grounded framework for\nfault detection that enables explicit risk control, enhancing the\ntrustworthiness of diagnostic systems in safety-critical applications and\nadvancing the field from simple point predictions to informative,\nuncertainty-aware outputs.", "AI": {"tldr": "This paper introduces a new fault detection method using significance testing and conformal prediction to provide risk guarantees, improving trustworthiness in safety-critical applications.", "motivation": "A significant scientific challenge is the lack of rigorous risk control and reliable uncertainty quantification in existing diagnostic models, particularly when facing complex scenarios such as distributional shifts.", "method": "This paper proposes a novel fault detection method that integrates significance testing with the conformal prediction framework to provide formal risk guarantees. The method transforms fault detection into a hypothesis testing task by defining a nonconformity measure based on model residuals. It then leverages a calibration dataset to compute p-values for new samples, which are used to construct prediction sets mathematically guaranteed to contain the true label with a user-specified probability.", "result": "The proposed method consistently achieves an empirical coverage rate at or above the nominal level, demonstrating robustness even when the underlying point-prediction models perform poorly. Furthermore, the results reveal a controllable trade-off between the user-defined risk level and efficiency, where higher risk tolerance leads to smaller average prediction set sizes.", "conclusion": "This research contributes a theoretically grounded framework for fault detection that enables explicit risk control, enhancing the trustworthiness of diagnostic systems in safety-critical applications and advancing the field from simple point predictions to informative, uncertainty-aware outputs."}}
{"id": "2508.00921", "categories": ["cs.LG", "cs.AI", "eess.IV", "I.2.1; I.2.6; I.2.9; I.4.8"], "pdf": "https://arxiv.org/pdf/2508.00921", "abs": "https://arxiv.org/abs/2508.00921", "authors": ["Khaled Eskaf"], "title": "SmartDate: AI-Driven Precision Sorting and Quality Control in Date Fruits", "comment": "6 pages, 2 figures, published in Proceedings of the 21st IEEE\n  International Conference on High Performance Computing and Networking (HONET\n  2024), Doha, Qatar, December 2024", "summary": "SmartDate is an AI-powered system for automated sorting and quality control\nof date fruits. It combines deep learning, genetic algorithms, and\nreinforcement learning to improve classification accuracy and predict shelf\nlife. The system uses high-resolution imaging and Visible-Near-Infrared\n(VisNIR) spectral sensors to evaluate key features such as moisture, sugar\ncontent, and texture. Reinforcement learning enables real-time adaptation to\nproduction conditions, while genetic algorithms optimize model parameters.\nSmartDate achieved 94.5 percent accuracy, 93.1 percent F1-score, and an AUC-ROC\nof 0.96. The system reduces waste and ensures that only high-quality dates\nreach the market, setting a new benchmark in smart agriculture.", "AI": {"tldr": "AI-powered system for automated sorting and quality control of date fruits.", "motivation": "Automated sorting and quality control of date fruits.", "method": "It combines deep learning, genetic algorithms, and reinforcement learning. The system uses high-resolution imaging and Visible-Near-Infrared (VisNIR) spectral sensors to evaluate key features such as moisture, sugar content, and texture. Reinforcement learning enables real-time adaptation to production conditions, while genetic algorithms optimize model parameters.", "result": "SmartDate achieved 94.5 percent accuracy, 93.1 percent F1-score, and an AUC-ROC of 0.96.", "conclusion": "SmartDate reduces waste and ensures that only high-quality dates reach the market, setting a new benchmark in smart agriculture."}}
{"id": "2508.02506", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02506", "abs": "https://arxiv.org/abs/2508.02506", "authors": ["Xiaowei Yuan", "Lei Jin", "Haoxin Zhang", "Yan Gao", "Yi Wu", "Yao Hu", "Ziyang Huang", "Jun Zhao", "Kang Liu"], "title": "Decomposed Reasoning with Reinforcement Learning for Relevance Assessment in UGC Platforms", "comment": null, "summary": "Retrieval-augmented generation (RAG) plays a critical role in user-generated\ncontent (UGC) platforms, but its effectiveness depends heavily on accurate\nrelevance assessment of query-document pairs. Despite recent advances in\napplying large language models (LLMs) to relevance modeling, UGC platforms\npresent unique challenges: 1) ambiguous user intent due to sparse user feedback\nin RAG scenarios, and 2) substantial noise introduced by informal and\nunstructured language. To address these issues, we propose the Reinforced\nReasoning Model for Relevance Assessment (R3A), which introduces a decomposed\nreasoning framework over queries and candidate documents before scoring. R3A\nfirst leverages auxiliary high-ranked documents within the platform to infer\nlatent query intent. It then performs verbatim fragment extraction to justify\nrelevance decisions, thereby reducing errors caused by noisy UGC. Based on a\nreinforcement learning framework, R3A is optimized to mitigate distortions\narising from ambiguous queries and unstructured content. Experimental results\nshow that R3A significantly outperforms existing baseline methods in terms of\nrelevance accuracy, across both offline benchmarks and online experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e86R3A\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u7528\u6237\u751f\u6210\u5185\u5bb9\u5e73\u53f0\u4e2d\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u76f8\u5173\u6027\u8bc4\u4f30\u95ee\u9898\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u5206\u89e3\u63a8\u7406\u548c\u5f3a\u5316\u5b66\u4e60\u6765\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5728\u7528\u6237\u751f\u6210\u5185\u5bb9\uff08UGC\uff09\u5e73\u53f0\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u4f46\u5176\u6709\u6548\u6027\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u5bf9\u67e5\u8be2-\u6587\u6863\u5bf9\u7684\u51c6\u786e\u76f8\u5173\u6027\u8bc4\u4f30\u3002\u5c3d\u7ba1\u6700\u8fd1\u5728\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5e94\u7528\u4e8e\u76f8\u5173\u6027\u5efa\u6a21\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46UGC\u5e73\u53f0\u9762\u4e34\u7740\u72ec\u7279\u7684\u6311\u6218\uff1a1\uff09\u7531\u4e8eRAG\u573a\u666f\u4e2d\u7a00\u758f\u7684\u7528\u6237\u53cd\u9988\u800c\u5bfc\u81f4\u7684\u7528\u6237\u610f\u56fe\u4e0d\u660e\u786e\uff0c\u4ee5\u53ca2\uff09\u975e\u6b63\u5f0f\u548c\u975e\u7ed3\u6784\u5316\u8bed\u8a00\u5f15\u5165\u7684\u5de8\u5927\u566a\u58f0\u3002", "method": "\u63d0\u51fa\u4e86\u7528\u4e8e\u76f8\u5173\u6027\u8bc4\u4f30\u7684\u5f3a\u5316\u63a8\u7406\u6a21\u578b\uff08R3A\uff09\uff0c\u5b83\u5728\u8bc4\u5206\u4e4b\u524d\u5f15\u5165\u4e86\u5bf9\u67e5\u8be2\u548c\u5019\u9009\u6587\u6863\u7684\u5206\u89e3\u63a8\u7406\u6846\u67b6\u3002R3A\u9996\u5148\u5229\u7528\u5e73\u53f0\u5185\u8f85\u52a9\u7684\u9ad8\u6392\u5e8f\u6587\u6863\u6765\u63a8\u65ad\u6f5c\u5728\u7684\u67e5\u8be2\u610f\u56fe\u3002\u7136\u540e\uff0c\u5b83\u6267\u884c\u9010\u5b57\u7247\u6bb5\u63d0\u53d6\uff0c\u4ee5\u8bc1\u660e\u76f8\u5173\u6027\u51b3\u7b56\u7684\u5408\u7406\u6027\uff0c\u4ece\u800c\u51cf\u5c11\u7531\u5608\u6742\u7684UGC\u5f15\u8d77\u7684\u9519\u8bef\u3002\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0cR3A\u7ecf\u8fc7\u4f18\u5316\uff0c\u53ef\u4ee5\u51cf\u8f7b\u7531\u6a21\u7cca\u67e5\u8be2\u548c\u975e\u7ed3\u6784\u5316\u5185\u5bb9\u5f15\u8d77\u7684\u5931\u771f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cR3A\u5728\u79bb\u7ebf\u57fa\u51c6\u6d4b\u8bd5\u548c\u5728\u7ebf\u5b9e\u9a8c\u4e2d\u7684\u76f8\u5173\u6027\u51c6\u786e\u5ea6\u65b9\u9762\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "R3A\u5728\u79bb\u7ebf\u57fa\u51c6\u6d4b\u8bd5\u548c\u5728\u7ebf\u5b9e\u9a8c\u4e2d\u7684\u76f8\u5173\u6027\u51c6\u786e\u5ea6\u65b9\u9762\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2508.01401", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01401", "abs": "https://arxiv.org/abs/2508.01401", "authors": ["Ahmad Rezaie Mianroodi", "Amirali Rezaie", "Niko Grisel Todorov", "Cyril Rakovski", "Frank Rudzicz"], "title": "MedSynth: Realistic, Synthetic Medical Dialogue-Note Pairs", "comment": "7 pages excluding references and appendices", "summary": "Physicians spend significant time documenting clinical encounters, a burden\nthat contributes to professional burnout. To address this, robust automation\ntools for medical documentation are crucial. We introduce MedSynth -- a novel\ndataset of synthetic medical dialogues and notes designed to advance the\nDialogue-to-Note (Dial-2-Note) and Note-to-Dialogue (Note-2-Dial) tasks.\nInformed by an extensive analysis of disease distributions, this dataset\nincludes over 10,000 dialogue-note pairs covering over 2000 ICD-10 codes. We\ndemonstrate that our dataset markedly enhances the performance of models in\ngenerating medical notes from dialogues, and dialogues from medical notes. The\ndataset provides a valuable resource in a field where open-access,\nprivacy-compliant, and diverse training data are scarce. Code is available at\nhttps://github.com/ahmadrezarm/MedSynth/tree/main and the dataset is available\nat https://huggingface.co/datasets/Ahmad0067/MedSynth.", "AI": {"tldr": "MedSynth is a new dataset of synthetic medical dialogues and notes that improves the performance of models in generating medical notes from dialogues, and dialogues from medical notes.", "motivation": "Physicians spend significant time documenting clinical encounters, a burden that contributes to professional burnout. To address this, robust automation tools for medical documentation are crucial.", "method": "A novel dataset of synthetic medical dialogues and notes designed to advance the Dialogue-to-Note (Dial-2-Note) and Note-to-Dialogue (Note-2-Dial) tasks.", "result": "The dataset includes over 10,000 dialogue-note pairs covering over 2000 ICD-10 codes.", "conclusion": "The MedSynth dataset enhances the performance of models in generating medical notes from dialogues, and dialogues from medical notes."}}
{"id": "2508.01098", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01098", "abs": "https://arxiv.org/abs/2508.01098", "authors": ["Yuekun Dai", "Haitian Li", "Shangchen Zhou", "Chen Change Loy"], "title": "Trans-Adapter: A Plug-and-Play Framework for Transparent Image Inpainting", "comment": "accepted to ICCV 2025", "summary": "RGBA images, with the additional alpha channel, are crucial for any\napplication that needs blending, masking, or transparency effects, making them\nmore versatile than standard RGB images. Nevertheless, existing image\ninpainting methods are designed exclusively for RGB images. Conventional\napproaches to transparent image inpainting typically involve placing a\nbackground underneath RGBA images and employing a two-stage process: image\ninpainting followed by image matting. This pipeline, however, struggles to\npreserve transparency consistency in edited regions, and matting can introduce\njagged edges along transparency boundaries. To address these challenges, we\npropose Trans-Adapter, a plug-and-play adapter that enables diffusion-based\ninpainting models to process transparent images directly. Trans-Adapter also\nsupports controllable editing via ControlNet and can be seamlessly integrated\ninto various community models. To evaluate our method, we introduce LayerBench,\nalong with a novel non-reference alpha edge quality evaluation metric for\nassessing transparency edge quality. We conduct extensive experiments on\nLayerBench to demonstrate the effectiveness of our approach.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86Trans-Adapter\uff0c\u5b83\u53ef\u4ee5\u76f4\u63a5\u5904\u7406\u900f\u660e\u56fe\u50cf\uff0c\u5e76\u4e14\u652f\u6301\u53ef\u63a7\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\u662f\u4e13\u95e8\u4e3aRGB\u56fe\u50cf\u8bbe\u8ba1\u7684\u3002\u4f20\u7edf\u7684\u900f\u660e\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\u901a\u5e38\u5305\u62ec\u5728RGBA\u56fe\u50cf\u4e0b\u653e\u7f6e\u4e00\u4e2a\u80cc\u666f\uff0c\u5e76\u91c7\u7528\u4e24\u9636\u6bb5\u7684\u8fc7\u7a0b\uff1a\u56fe\u50cf\u4fee\u590d\uff0c\u7136\u540e\u8fdb\u884c\u56fe\u50cf\u62a0\u56fe\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u6d41\u7a0b\u96be\u4ee5\u4fdd\u6301\u7f16\u8f91\u533a\u57df\u7684\u900f\u660e\u5ea6\u4e00\u81f4\u6027\uff0c\u5e76\u4e14\u62a0\u56fe\u4f1a\u5728\u900f\u660e\u5ea6\u8fb9\u754c\u5f15\u5165\u952f\u9f7f\u8fb9\u7f18\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86Trans-Adapter\uff0c\u8fd9\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u9002\u914d\u5668\uff0c\u4f7f\u57fa\u4e8e\u6269\u6563\u7684\u4fee\u590d\u6a21\u578b\u80fd\u591f\u76f4\u63a5\u5904\u7406\u900f\u660e\u56fe\u50cf\u3002", "result": "Trans-Adapter\u8fd8\u652f\u6301\u901a\u8fc7ControlNet\u8fdb\u884c\u53ef\u63a7\u7f16\u8f91\uff0c\u5e76\u4e14\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u5404\u79cd\u793e\u533a\u6a21\u578b\u4e2d\u3002\u4e3a\u4e86\u8bc4\u4f30\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u6211\u4eec\u5f15\u5165\u4e86LayerBench\uff0c\u4ee5\u53ca\u4e00\u79cd\u7528\u4e8e\u8bc4\u4f30\u900f\u660e\u5ea6\u8fb9\u7f18\u8d28\u91cf\u7684\u65b0\u578b\u975e\u53c2\u8003alpha\u8fb9\u7f18\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u3002", "conclusion": "\u6211\u4eec\u8fdb\u884c\u4e86\u5927\u91cf\u7684\u5b9e\u9a8c\uff0c\u4ee5\u8bc1\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u662f\u6709\u6548\u7684\u3002"}}
{"id": "2508.01237", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01237", "abs": "https://arxiv.org/abs/2508.01237", "authors": ["Cheng Tan", "Qi Chen", "Jingxuan Wei", "Gaowei Wu", "Zhangyang Gao", "Siyuan Li", "Bihui Yu", "Ruifeng Guo", "Stan Z. Li"], "title": "SketchAgent: Generating Structured Diagrams from Hand-Drawn Sketches", "comment": "Accepted by IJCAI 2025", "summary": "Hand-drawn sketches are a natural and efficient medium for capturing and\nconveying ideas. Despite significant advancements in controllable natural image\ngeneration, translating freehand sketches into structured, machine-readable\ndiagrams remains a labor-intensive and predominantly manual task. The primary\nchallenge stems from the inherent ambiguity of sketches, which lack the\nstructural constraints and semantic precision required for automated diagram\ngeneration. To address this challenge, we introduce SketchAgent, a multi-agent\nsystem designed to automate the transformation of hand-drawn sketches into\nstructured diagrams. SketchAgent integrates sketch recognition, symbolic\nreasoning, and iterative validation to produce semantically coherent and\nstructurally accurate diagrams, significantly reducing the need for manual\neffort. To evaluate the effectiveness of our approach, we propose the\nSketch2Diagram Benchmark, a comprehensive dataset and evaluation framework\nencompassing eight diverse diagram categories, such as flowcharts, directed\ngraphs, and model architectures. The dataset comprises over 6,000 high-quality\nexamples with token-level annotations, standardized preprocessing, and rigorous\nquality control. By streamlining the diagram generation process, SketchAgent\nholds great promise for applications in design, education, and engineering,\nwhile offering a significant step toward bridging the gap between intuitive\nsketching and machine-readable diagram generation. The benchmark is released at\nhttps://huggingface.co/datasets/DiagramAgent/Sketch2Diagram-Benchmark.", "AI": {"tldr": "SketchAgent\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u53ef\u4ee5\u5c06\u624b\u7ed8\u8349\u56fe\u81ea\u52a8\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u56fe\u8868\uff0c\u5e76\u63d0\u51fa\u4e86Sketch2Diagram\u57fa\u51c6\u7528\u4e8e\u8bc4\u4f30\u3002", "motivation": "\u624b\u7ed8\u8349\u56fe\u662f\u6355\u6349\u548c\u4f20\u9012\u60f3\u6cd5\u7684\u81ea\u7136\u9ad8\u6548\u5a92\u4ecb\u3002\u5c3d\u7ba1\u53ef\u63a7\u81ea\u7136\u56fe\u50cf\u751f\u6210\u6280\u672f\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5c06\u624b\u7ed8\u8349\u56fe\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u7684\u3001\u673a\u5668\u53ef\u8bfb\u7684\u56fe\u8868\u4ecd\u7136\u662f\u4e00\u9879\u52b3\u52a8\u5bc6\u96c6\u578b\u4e14\u4e3b\u8981\u9760\u624b\u5de5\u5b8c\u6210\u7684\u4efb\u52a1\u3002\u4e3b\u8981\u6311\u6218\u6765\u81ea\u4e8e\u8349\u56fe\u56fa\u6709\u7684\u6a21\u7cca\u6027\uff0c\u8349\u56fe\u7f3a\u4e4f\u81ea\u52a8\u56fe\u8868\u751f\u6210\u6240\u9700\u7684\u7ed3\u6784\u7ea6\u675f\u548c\u8bed\u4e49\u7cbe\u5ea6\u3002", "method": "\u5f15\u5165\u4e86SketchAgent\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u96c6\u6210\u4e86\u8349\u56fe\u8bc6\u522b\u3001\u7b26\u53f7\u63a8\u7406\u548c\u8fed\u4ee3\u9a8c\u8bc1\u3002", "result": "\u63d0\u51fa\u4e86Sketch2Diagram\u57fa\u51c6\uff0c\u4e00\u4e2a\u7efc\u5408\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b\u516b\u4e2a\u4e0d\u540c\u7684\u56fe\u8868\u7c7b\u522b\uff0c\u4f8b\u5982\u6d41\u7a0b\u56fe\u3001\u6709\u5411\u56fe\u548c\u6a21\u578b\u67b6\u6784\u3002\u8be5\u6570\u636e\u96c6\u5305\u542b\u8d85\u8fc76,000\u4e2a\u9ad8\u8d28\u91cf\u793a\u4f8b\uff0c\u5177\u6709\u4ee4\u724c\u7ea7\u6ce8\u91ca\u3001\u6807\u51c6\u5316\u9884\u5904\u7406\u548c\u4e25\u683c\u7684\u8d28\u91cf\u63a7\u5236\u3002", "conclusion": "SketchAgent\u901a\u8fc7\u96c6\u6210\u8349\u56fe\u8bc6\u522b\u3001\u7b26\u53f7\u63a8\u7406\u548c\u8fed\u4ee3\u9a8c\u8bc1\uff0c\u80fd\u591f\u663e\u8457\u51cf\u5c11\u624b\u52a8\u5de5\u4f5c\uff0c\u5c06\u624b\u7ed8\u8349\u56fe\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u56fe\u8868\uff0c\u5e76\u5728\u8bbe\u8ba1\u3001\u6559\u80b2\u548c\u5de5\u7a0b\u9886\u57df\u5177\u6709\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2508.00922", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00922", "abs": "https://arxiv.org/abs/2508.00922", "authors": ["Jinsoo Bae", "Seoung Bum Kim", "Hyungrok Do"], "title": "CaliMatch: Adaptive Calibration for Improving Safe Semi-supervised Learning", "comment": null, "summary": "Semi-supervised learning (SSL) uses unlabeled data to improve the performance\nof machine learning models when labeled data is scarce. However, its real-world\napplications often face the label distribution mismatch problem, in which the\nunlabeled dataset includes instances whose ground-truth labels are absent from\nthe labeled training dataset. Recent studies, referred to as safe SSL, have\naddressed this issue by using both classification and out-of-distribution (OOD)\ndetection. However, the existing methods may suffer from overconfidence in deep\nneural networks, leading to increased SSL errors because of high confidence in\nincorrect pseudo-labels or OOD detection. To address this, we propose a novel\nmethod, CaliMatch, which calibrates both the classifier and the OOD detector to\nfoster safe SSL. CaliMatch presents adaptive label smoothing and temperature\nscaling, which eliminates the need to manually tune the smoothing degree for\neffective calibration. We give a theoretical justification for why improving\nthe calibration of both the classifier and the OOD detector is crucial in safe\nSSL. Extensive evaluations on CIFAR-10, CIFAR-100, SVHN, TinyImageNet, and\nImageNet demonstrate that CaliMatch outperforms the existing methods in safe\nSSL tasks.", "AI": {"tldr": "CaliMatch\u901a\u8fc7\u6821\u51c6\u5206\u7c7b\u5668\u548cOOD\u68c0\u6d4b\u5668\u6765\u6539\u8fdb\u5b89\u5168SSL\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u534a\u76d1\u7763\u5b66\u4e60\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9762\u4e34\u6807\u7b7e\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\u3002", "method": "CaliMatch\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u6807\u7b7e\u5e73\u6ed1\u548c\u6e29\u5ea6\u7f29\u653e\uff0c\u65e0\u9700\u624b\u52a8\u8c03\u6574\u5e73\u6ed1\u7a0b\u5ea6\u3002", "result": "\u5728CIFAR-10\u3001CIFAR-100\u3001SVHN\u3001TinyImageNet\u548cImageNet\u4e0a\u7684\u5927\u91cf\u8bc4\u4f30\u8868\u660eCaliMatch\u4f18\u4e8e\u73b0\u6709\u7684\u5b89\u5168SSL\u4efb\u52a1\u65b9\u6cd5\u3002", "conclusion": "CaliMatch\u5728\u5b89\u5168SSL\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.02538", "categories": ["cs.IR", "H.3"], "pdf": "https://arxiv.org/pdf/2508.02538", "abs": "https://arxiv.org/abs/2508.02538", "authors": ["Zhengxin Pan", "Haishuai Wang", "Fangyu Wu", "Peng Zhang", "Jiajun Bu"], "title": "Hubness Reduction with Dual Bank Sinkhorn Normalization for Cross-Modal Retrieval", "comment": "ACMMM 2025", "summary": "The past decade has witnessed rapid advancements in cross-modal retrieval,\nwith significant progress made in accurately measuring the similarity between\ncross-modal pairs. However, the persistent hubness problem, a phenomenon where\na small number of targets frequently appear as nearest neighbors to numerous\nqueries, continues to hinder the precision of similarity measurements. Despite\nseveral proposed methods to reduce hubness, their underlying mechanisms remain\npoorly understood. To bridge this gap, we analyze the widely-adopted Inverted\nSoftmax approach and demonstrate its effectiveness in balancing target\nprobabilities during retrieval. Building on these insights, we propose a\nprobability-balancing framework for more effective hubness reduction. We\ncontend that balancing target probabilities alone is inadequate and, therefore,\nextend the framework to balance both query and target probabilities by\nintroducing Sinkhorn Normalization (SN). Notably, we extend SN to scenarios\nwhere the true query distribution is unknown, showing that current methods,\nwhich rely solely on a query bank to estimate target hubness, produce\nsuboptimal results due to a significant distributional gap between the query\nbank and targets. To mitigate this issue, we introduce Dual Bank Sinkhorn\nNormalization (DBSN), incorporating a corresponding target bank alongside the\nquery bank to narrow this distributional gap. Our comprehensive evaluation\nacross various cross-modal retrieval tasks, including image-text retrieval,\nvideo-text retrieval, and audio-text retrieval, demonstrates consistent\nperformance improvements, validating the effectiveness of both SN and DBSN. All\ncodes are publicly available at https://github.com/ppanzx/DBSN.", "AI": {"tldr": "This paper introduces SN and DBSN to solve the hubness problem in cross-modal retrieval by balancing query and target probabilities, achieving improved performance in image-text, video-text, and audio-text retrieval tasks.", "motivation": "The hubness problem in cross-modal retrieval hinders the precision of similarity measurements, and the underlying mechanisms of existing hubness reduction methods are poorly understood.", "method": "The paper proposes a probability-balancing framework that extends Sinkhorn Normalization (SN) to balance both query and target probabilities and introduces Dual Bank Sinkhorn Normalization (DBSN) to address the distributional gap between the query bank and targets.", "result": "The paper achieves consistent performance improvements across various cross-modal retrieval tasks, including image-text retrieval, video-text retrieval, and audio-text retrieval, validating the effectiveness of both SN and DBSN.", "conclusion": "The paper introduces Sinkhorn Normalization (SN) and Dual Bank Sinkhorn Normalization (DBSN) and demonstrates their effectiveness in improving performance across various cross-modal retrieval tasks."}}
{"id": "2508.01411", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01411", "abs": "https://arxiv.org/abs/2508.01411", "authors": ["Rania Al-Sabbagh"], "title": "ArzEn-MultiGenre: An aligned parallel dataset of Egyptian Arabic song lyrics, novels, and subtitles, with English translations", "comment": null, "summary": "ArzEn-MultiGenre is a parallel dataset of Egyptian Arabic song lyrics,\nnovels, and TV show subtitles that are manually translated and aligned with\ntheir English counterparts. The dataset contains 25,557 segment pairs that can\nbe used to benchmark new machine translation models, fine-tune large language\nmodels in few-shot settings, and adapt commercial machine translation\napplications such as Google Translate. Additionally, the dataset is a valuable\nresource for research in various disciplines, including translation studies,\ncross-linguistic analysis, and lexical semantics. The dataset can also serve\npedagogical purposes by training translation students and aid professional\ntranslators as a translation memory. The contributions are twofold: first, the\ndataset features textual genres not found in existing parallel Egyptian Arabic\nand English datasets, and second, it is a gold-standard dataset that has been\ntranslated and aligned by human experts.", "AI": {"tldr": "ArzEn-MultiGenre\u662f\u4e00\u4e2a\u4eba\u5de5\u7ffb\u8bd1\u7684\u57c3\u53ca\u963f\u62c9\u4f2f\u8bed-\u82f1\u8bed\u5e73\u884c\u6570\u636e\u96c6\uff0c\u5305\u542b25,557\u4e2a\u7247\u6bb5\u5bf9\uff0c\u53ef\u7528\u4e8e\u5404\u79cd\u76ee\u7684\u3002", "motivation": "\u73b0\u6709\u7684\u5e73\u884c\u57c3\u53ca\u963f\u62c9\u4f2f\u8bed\u548c\u82f1\u8bed\u6570\u636e\u96c6\u4e2d\u6ca1\u6709\u6587\u672c\u7c7b\u578b\u3002", "method": "\u4eba\u5de5\u7ffb\u8bd1\u548c\u5bf9\u9f50\u57c3\u53ca\u963f\u62c9\u4f2f\u8bed\u6b4c\u66f2\u6b4c\u8bcd\u3001\u5c0f\u8bf4\u548c\u7535\u89c6\u8282\u76ee\u5b57\u5e55\u53ca\u5176\u82f1\u8bed\u5bf9\u5e94\u7248\u672c\u3002", "result": "\u8be5\u6570\u636e\u96c6\u5305\u542b25,557\u4e2a\u7247\u6bb5\u5bf9\u3002", "conclusion": "ArzEn-MultiGenre\u662f\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u5e73\u884c\u8bed\u6599\u5e93\uff0c\u7531\u4eba\u5de5\u7ffb\u8bd1\u548c\u5bf9\u9f50\uff0c\u53ef\u7528\u4e8e\u673a\u5668\u7ffb\u8bd1\u6a21\u578b\u7684\u57fa\u51c6\u6d4b\u8bd5\u3001\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u8c03\u6574\u5546\u4e1a\u673a\u5668\u7ffb\u8bd1\u5e94\u7528\u7a0b\u5e8f\u3002"}}
{"id": "2508.01112", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01112", "abs": "https://arxiv.org/abs/2508.01112", "authors": ["Yizhou Zhao", "Haoyu Chen", "Chunjiang Liu", "Zhenyang Li", "Charles Herrmann", "Junhwa Hur", "Yinxiao Li", "Ming-Hsuan Yang", "Bhiksha Raj", "Min Xu"], "title": "MASIV: Toward Material-Agnostic System Identification from Videos", "comment": "ICCV 2025", "summary": "System identification from videos aims to recover object geometry and\ngoverning physical laws. Existing methods integrate differentiable rendering\nwith simulation but rely on predefined material priors, limiting their ability\nto handle unknown ones. We introduce MASIV, the first vision-based framework\nfor material-agnostic system identification. Unlike existing approaches that\ndepend on hand-crafted constitutive laws, MASIV employs learnable neural\nconstitutive models, inferring object dynamics without assuming a\nscene-specific material prior. However, the absence of full particle state\ninformation imposes unique challenges, leading to unstable optimization and\nphysically implausible behaviors. To address this, we introduce dense geometric\nguidance by reconstructing continuum particle trajectories, providing\ntemporally rich motion constraints beyond sparse visual cues. Comprehensive\nexperiments show that MASIV achieves state-of-the-art performance in geometric\naccuracy, rendering quality, and generalization ability.", "AI": {"tldr": "MASIV\u662f\u4e00\u79cd\u4e0e\u6750\u6599\u65e0\u5173\u7684\u7cfb\u7edf\u8bc6\u522b\u7684\u89c6\u89c9\u6846\u67b6\uff0c\u5b83\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u795e\u7ecf\u672c\u6784\u6a21\u578b\u6765\u63a8\u65ad\u7269\u4f53\u52a8\u529b\u5b66\uff0c\u800c\u65e0\u9700\u5047\u8bbe\u573a\u666f\u7279\u5b9a\u7684\u6750\u6599\u5148\u9a8c\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u5c06\u53ef\u5fae\u6e32\u67d3\u4e0e\u6a21\u62df\u76f8\u7ed3\u5408\uff0c\u4f46\u4f9d\u8d56\u4e8e\u9884\u5b9a\u4e49\u7684\u6750\u6599\u5148\u9a8c\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5904\u7406\u672a\u77e5\u6750\u6599\u7684\u80fd\u529b\u3002", "method": "MASIV\u91c7\u7528\u53ef\u5b66\u4e60\u7684\u795e\u7ecf\u672c\u6784\u6a21\u578b\uff0c\u65e0\u9700\u5047\u8bbe\u7279\u5b9a\u573a\u666f\u7684\u6750\u6599\u5148\u9a8c\u5373\u53ef\u63a8\u65ad\u7269\u4f53\u52a8\u529b\u5b66\u3002", "result": "MASIV\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u51e0\u4f55\u7cbe\u5ea6\u3001\u6e32\u67d3\u8d28\u91cf\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MASIV\u5728\u51e0\u4f55\u7cbe\u5ea6\u3001\u6e32\u67d3\u8d28\u91cf\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2508.01261", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01261", "abs": "https://arxiv.org/abs/2508.01261", "authors": ["Sushant Mehta", "Raj Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models", "comment": null, "summary": "We present MoE-MLA-RoPE, a novel architecture combination that combines\nMixture of Experts (MoE) with Multi-head Latent Attention (MLA) and Rotary\nPosition Embeddings (RoPE) for efficient language modeling. Our approach\naddresses the fundamental trade-off between model capacity and computational\nefficiency through three key innovations: (1) fine-grained expert routing with\n64 micro-experts and top-$k$ selection, enabling flexible specialization\nthrough 3.6 * 10^7 possible expert combinations; (2) shared expert isolation\nthat dedicates 2 always active experts for common patterns while routing to 6\nof 62 specialized experts; and (3) gradient-conflict-free load balancing that\nmaintains expert utilization without interfering with primary loss\noptimization.\n  Extensive experiments on models ranging from 17M to 202M parameters\ndemonstrate that MoE-MLA-RoPE with compression ratio r=d/2 achieves 68% KV\ncache memory reduction and 3.2x inference speedup while maintaining competitive\nperplexity (0.8% degradation). Compared to the parameters with 53.9M\nparameters, MoE-MLA-RoPE improves the validation loss by 6.9% over the vanilla\ntransformers while using 42% fewer active parameters per forward pass.\nFLOP-matched experiments reveal even larger gains: 11.1% improvement with 3.2x\ninference acceleration. Automated evaluation using GPT-4 as a judge confirms\nquality improvements in generation, with higher scores on coherence (8.1/10),\ncreativity (7.9/10) and grammatical correctness (8.2/10). Our results establish\nthat architectural novelty, not parameter scaling, defines the efficiency\nfrontier for resource-constrained language model deployment.", "AI": {"tldr": "MoE-MLA-RoPE: efficient language model with MoE, MLA, and RoPE, achieving memory reduction and speedup with competitive performance.", "motivation": "addresses the fundamental trade-off between model capacity and computational efficiency", "method": "combines Mixture of Experts (MoE) with Multi-head Latent Attention (MLA) and Rotary Position Embeddings (RoPE)", "result": "achieves 68% KV cache memory reduction and 3.2x inference speedup while maintaining competitive perplexity (0.8% degradation). Compared to the parameters with 53.9M parameters, MoE-MLA-RoPE improves the validation loss by 6.9% over the vanilla transformers while using 42% fewer active parameters per forward pass.", "conclusion": "MoE-MLA-RoPE demonstrates that architectural novelty, not parameter scaling, defines the efficiency frontier for resource-constrained language model deployment."}}
{"id": "2508.00923", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00923", "abs": "https://arxiv.org/abs/2508.00923", "authors": ["Jiazhen Pan", "Bailiang Jian", "Paul Hager", "Yundi Zhang", "Che Liu", "Friedrike Jungmann", "Hongwei Bran Li", "Chenyu You", "Junde Wu", "Jiayuan Zhu", "Fenglin Liu", "Yuyuan Liu", "Niklas Bubeck", "Christian Wachinger", "Chen", "Chen", "Zhenyu Gong", "Cheng Ouyang", "Georgios Kaissis", "Benedikt Wiestler", "Daniel Rueckert"], "title": "Beyond Benchmarks: Dynamic, Automatic And Systematic Red-Teaming Agents For Trustworthy Medical Language Models", "comment": null, "summary": "Ensuring the safety and reliability of large language models (LLMs) in\nclinical practice is critical to prevent patient harm and promote trustworthy\nhealthcare applications of AI. However, LLMs are advancing so rapidly that\nstatic safety benchmarks often become obsolete upon publication, yielding only\nan incomplete and sometimes misleading picture of model trustworthiness. We\ndemonstrate that a Dynamic, Automatic, and Systematic (DAS) red-teaming\nframework that continuously stress-tests LLMs can reveal significant weaknesses\nof current LLMs across four safety-critical domains: robustness, privacy,\nbias/fairness, and hallucination. A suite of adversarial agents is applied to\nautonomously mutate test cases, identify/evolve unsafe-triggering strategies,\nand evaluate responses, uncovering vulnerabilities in real time without human\nintervention. Applying DAS to 15 proprietary and open-source LLMs revealed a\nstark contrast between static benchmark performance and vulnerability under\nadversarial pressure. Despite a median MedQA accuracy exceeding 80\\%, 94\\% of\npreviously correct answers failed our dynamic robustness tests. We observed\nsimilarly high failure rates across other domains: privacy leaks were elicited\nin 86\\% of scenarios, cognitive-bias priming altered clinical recommendations\nin 81\\% of fairness tests, and we identified hallucination rates exceeding 66\\%\nin widely used models. Such profound residual risks are incompatible with\nroutine clinical practice. By converting red-teaming from a static checklist\ninto a dynamic stress-test audit, DAS red-teaming offers the surveillance that\nhospitals/regulators/technology vendors require as LLMs become embedded in\npatient chatbots, decision-support dashboards, and broader healthcare\nworkflows. Our framework delivers an evolvable, scalable, and reliable\nsafeguard for the next generation of medical AI.", "AI": {"tldr": "This paper introduces a Dynamic, Automatic, and Systematic (DAS) red-teaming framework to continuously stress-test LLMs and reveal significant weaknesses of current LLMs across four safety-critical domains: robustness, privacy, bias/fairness, and hallucination.", "motivation": "Ensuring the safety and reliability of large language models (LLMs) in clinical practice is critical to prevent patient harm and promote trustworthy healthcare applications of AI. However, LLMs are advancing so rapidly that static safety benchmarks often become obsolete upon publication, yielding only an incomplete and sometimes misleading picture of model trustworthiness.", "method": "a Dynamic, Automatic, and Systematic (DAS) red-teaming framework that continuously stress-tests LLMs", "result": "Applying DAS to 15 proprietary and open-source LLMs revealed a stark contrast between static benchmark performance and vulnerability under adversarial pressure. Despite a median MedQA accuracy exceeding 80%, 94% of previously correct answers failed our dynamic robustness tests. We observed similarly high failure rates across other domains: privacy leaks were elicited in 86% of scenarios, cognitive-bias priming altered clinical recommendations in 81% of fairness tests, and we identified hallucination rates exceeding 66% in widely used models. Such profound residual risks are incompatible with routine clinical practice.", "conclusion": "By converting red-teaming from a static checklist into a dynamic stress-test audit, DAS red-teaming offers the surveillance that hospitals/regulators/technology vendors require as LLMs become embedded in patient chatbots, decision-support dashboards, and broader healthcare workflows. Our framework delivers an evolvable, scalable, and reliable safeguard for the next generation of medical AI."}}
{"id": "2508.00955", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.00955", "abs": "https://arxiv.org/abs/2508.00955", "authors": ["Yeong-Joon Ju", "Seong-Whan Lee"], "title": "From Generator to Embedder: Harnessing Innate Abilities of Multimodal LLMs via Building Zero-Shot Discriminative Embedding Model", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have emerged as a promising solution\nfor universal embedding tasks, yet adapting their generative nature for\ndiscriminative representation learning remains a significant challenge. The\ndominant paradigm of large-scale contrastive pre-training suffers from critical\ninefficiencies, including prohibitive computational costs and a failure to\nleverage the intrinsic, instruction-following capabilities of MLLMs. To\novercome these limitations, we propose an efficient framework for universal\nmultimodal embeddings, which bridges this gap by centering on two synergistic\ncomponents. First, our hierarchical embedding prompt template employs a\ntwo-level instruction architecture that forces the model to produce\ndiscriminative representations. Building on this strong foundation, our second\ncomponent, self-aware hard negative sampling, redefines the fine-tuning process\nby leveraging the model's own understanding to efficiently mine challenging\nnegatives while actively filtering out potential false negatives. Our\ncomprehensive experiments show that our hierarchical prompt achieves zero-shot\nperformance competitive with contrastively trained baselines and enhances the\nfine-tuning process by lifting a simple in-batch negative baseline by 4.8\npoints on the MMEB benchmark. We further boost the performance via our\nself-aware hard negative sampling, achieving the state-of-the-art performance\nwithout the contrative pre-training. Our work presents an effective and\nefficient pathway to adapt MLLMs for universal embedding tasks, significantly\nreducing training time.", "AI": {"tldr": "This paper proposes a new framework for universal multimodal embeddings that overcomes the limitations of contrastive pre-training by using a hierarchical embedding prompt template and self-aware hard negative sampling.", "motivation": "Adapting the generative nature of Multimodal Large Language Models (MLLMs) for discriminative representation learning remains a significant challenge. The dominant paradigm of large-scale contrastive pre-training suffers from critical inefficiencies", "method": "hierarchical embedding prompt template employs a two-level instruction architecture and self-aware hard negative sampling", "result": "hierarchical prompt achieves zero-shot performance competitive with contrastively trained baselines and enhances the fine-tuning process by lifting a simple in-batch negative baseline by 4.8 points on the MMEB benchmark. self-aware hard negative sampling achieves the state-of-the-art performance without the contrative pre-training", "conclusion": "This work presents an effective and efficient pathway to adapt MLLMs for universal embedding tasks, significantly reducing training time."}}
{"id": "2508.01412", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01412", "abs": "https://arxiv.org/abs/2508.01412", "authors": ["Jinhao Pan", "Chahat Raj", "Ziwei Zhu"], "title": "Discovering Bias Associations through Open-Ended LLM Generations", "comment": null, "summary": "Social biases embedded in Large Language Models (LLMs) raise critical\nconcerns, resulting in representational harms -- unfair or distorted portrayals\nof demographic groups -- that may be expressed in subtle ways through generated\nlanguage. Existing evaluation methods often depend on predefined\nidentity-concept associations, limiting their ability to surface new or\nunexpected forms of bias. In this work, we present the Bias Association\nDiscovery Framework (BADF), a systematic approach for extracting both known and\npreviously unrecognized associations between demographic identities and\ndescriptive concepts from open-ended LLM outputs. Through comprehensive\nexperiments spanning multiple models and diverse real-world contexts, BADF\nenables robust mapping and analysis of the varied concepts that characterize\ndemographic identities. Our findings advance the understanding of biases in\nopen-ended generation and provide a scalable tool for identifying and analyzing\nbias associations in LLMs. Data, code, and results are available at\nhttps://github.com/JP-25/Discover-Open-Ended-Generation", "AI": {"tldr": "This paper introduces Bias Association Discovery Framework (BADF), a systematic approach for extracting both known and previously unrecognized associations between demographic identities and descriptive concepts from open-ended LLM outputs. It helps to advance the understanding of biases in open-ended generation and provide a scalable tool for identifying and analyzing bias associations in LLMs.", "motivation": "Social biases embedded in Large Language Models (LLMs) raise critical concerns, resulting in representational harms -- unfair or distorted portrayals of demographic groups -- that may be expressed in subtle ways through generated language. Existing evaluation methods often depend on predefined identity-concept associations, limiting their ability to surface new or unexpected forms of bias.", "method": "We present the Bias Association Discovery Framework (BADF), a systematic approach for extracting both known and previously unrecognized associations between demographic identities and descriptive concepts from open-ended LLM outputs. Through comprehensive experiments spanning multiple models and diverse real-world contexts", "result": "BADF enables robust mapping and analysis of the varied concepts that characterize demographic identities.", "conclusion": "BADF enables robust mapping and analysis of the varied concepts that characterize demographic identities. Our findings advance the understanding of biases in open-ended generation and provide a scalable tool for identifying and analyzing bias associations in LLMs."}}
{"id": "2508.01119", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.01119", "abs": "https://arxiv.org/abs/2508.01119", "authors": ["Saba Ahmadi", "Rabiul Awal", "Ankur Sikarwar", "Amirhossein Kazemnejad", "Ge Ya Luo", "Juan A. Rodriguez", "Sai Rajeswar", "Siva Reddy", "Christopher Pal", "Benno Krojer", "Aishwarya Agrawal"], "title": "The Promise of RL for Autoregressive Image Editing", "comment": null, "summary": "We explore three strategies to enhance performance on a wide range of image\nediting tasks: supervised fine-tuning (SFT), reinforcement learning (RL), and\nChain-of-Thought (CoT) reasoning. In order to study all these components in one\nconsistent framework, we adopt an autoregressive multimodal model that\nprocesses textual and visual tokens in a unified manner. We find RL combined\nwith a large multi-modal LLM verifier to be the most effective of these\nstrategies. As a result, we release EARL: Editing with Autoregression and RL, a\nstrong RL-based image editing model that performs competitively on a diverse\nrange of edits compared to strong baselines, despite using much less training\ndata. Thus, EARL pushes the frontier of autoregressive multimodal models on\nimage editing. We release our code, training data, and trained models at\nhttps://github.com/mair-lab/EARL.", "AI": {"tldr": "EARL: A strong RL-based image editing model that performs well with less data.", "motivation": "Enhance performance on a wide range of image editing tasks.", "method": "Autoregressive multimodal model with supervised fine-tuning (SFT), reinforcement learning (RL), and Chain-of-Thought (CoT) reasoning.", "result": "RL combined with a large multi-modal LLM verifier is the most effective strategy. EARL performs competitively on a diverse range of edits compared to strong baselines, despite using much less training data.", "conclusion": "EARL, an RL-based image editing model, achieves competitive performance on diverse edits with less training data, advancing autoregressive multimodal models."}}
{"id": "2508.01268", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.01268", "abs": "https://arxiv.org/abs/2508.01268", "authors": ["Roya Arkhmammadova", "Hosein Madadi Tamar", "M. Emre Gursoy"], "title": "Win-k: Improved Membership Inference Attacks on Small Language Models", "comment": null, "summary": "Small language models (SLMs) are increasingly valued for their efficiency and\ndeployability in resource-constrained environments, making them useful for\non-device, privacy-sensitive, and edge computing applications. On the other\nhand, membership inference attacks (MIAs), which aim to determine whether a\ngiven sample was used in a model's training, are an important threat with\nserious privacy and intellectual property implications. In this paper, we study\nMIAs on SLMs. Although MIAs were shown to be effective on large language models\n(LLMs), they are relatively less studied on emerging SLMs, and furthermore,\ntheir effectiveness decreases as models get smaller. Motivated by this finding,\nwe propose a new MIA called win-k, which builds on top of a state-of-the-art\nattack (min-k). We experimentally evaluate win-k by comparing it with five\nexisting MIAs using three datasets and eight SLMs. Results show that win-k\noutperforms existing MIAs in terms of AUROC, TPR @ 1% FPR, and FPR @ 99% TPR\nmetrics, especially on smaller models.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86SLM\u4e0a\u7684MIAs\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684MIA\u53eb\u505awin-k\uff0c\u5b83\u5efa\u7acb\u5728\u6700\u5148\u8fdb\u7684\u653b\u51fb(min-k)\u4e4b\u4e0a\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cwin-k\u4f18\u4e8e\u73b0\u6709\u7684MIAs\uff0c\u5c24\u5176\u662f\u5728\u8f83\u5c0f\u7684\u6a21\u578b\u4e0a\u3002", "motivation": "\u5c0f\u578b\u8bed\u8a00\u6a21\u578b(SLM)\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u8d8a\u6765\u8d8a\u88ab\u91cd\u89c6\uff0c\u56e0\u4e3a\u5b83\u4eec\u7684\u9ad8\u6548\u6027\u548c\u53ef\u90e8\u7f72\u6027\uff0c\u8fd9\u4f7f\u5f97\u5b83\u4eec\u5728\u8bbe\u5907\u4e0a\u3001\u9690\u79c1\u654f\u611f\u548c\u8fb9\u7f18\u8ba1\u7b97\u5e94\u7528\u4e2d\u975e\u5e38\u6709\u7528\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u6210\u5458\u63a8\u7406\u653b\u51fb(MIAs)\u65e8\u5728\u786e\u5b9a\u4e00\u4e2a\u7ed9\u5b9a\u7684\u6837\u672c\u662f\u5426\u88ab\u7528\u4e8e\u6a21\u578b\u7684\u8bad\u7ec3\uff0c\u8fd9\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u5a01\u80c1\uff0c\u5177\u6709\u4e25\u91cd\u7684\u9690\u79c1\u548c\u77e5\u8bc6\u4ea7\u6743\u5f71\u54cd\u3002\u867d\u7136MIAs\u5df2\u88ab\u8bc1\u660e\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u6709\u6548\uff0c\u4f46\u5bf9\u65b0\u5174\u7684slm\u7684\u7814\u7a76\u76f8\u5bf9\u8f83\u5c11\uff0c\u800c\u4e14\uff0c\u5b83\u4eec\u7684\u6709\u6548\u6027\u968f\u7740\u6a21\u578b\u53d8\u5c0f\u800c\u964d\u4f4e\u3002\u53d7\u8fd9\u4e00\u53d1\u73b0\u7684\u542f\u53d1\uff0c", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684MIA\u53eb\u505awin-k\uff0c\u5b83\u5efa\u7acb\u5728\u6700\u5148\u8fdb\u7684\u653b\u51fb(min-k)\u4e4b\u4e0a\u3002", "result": "win-k\u5728AUROC\u3001TPR @ 1% FPR\u548cFPR @ 99% TPR\u6307\u6807\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684MIAs\uff0c\u5c24\u5176\u662f\u5728\u8f83\u5c0f\u7684\u6a21\u578b\u4e0a\u3002", "conclusion": "win-k\u5728AUROC\u3001TPR @ 1% FPR\u548cFPR @ 99% TPR\u6307\u6807\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684MIAs\uff0c\u5c24\u5176\u662f\u5728\u8f83\u5c0f\u7684\u6a21\u578b\u4e0a\u3002"}}
{"id": "2508.00926", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00926", "abs": "https://arxiv.org/abs/2508.00926", "authors": ["Feng Xu", "Hui Wang", "Yuting Huang", "Danwei Zhang", "Zizhu Fan"], "title": "Hybrid Hypergraph Networks for Multimodal Sequence Data Classification", "comment": "9 pages, 5 figures", "summary": "Modeling temporal multimodal data poses significant challenges in\nclassification tasks, particularly in capturing long-range temporal\ndependencies and intricate cross-modal interactions. Audiovisual data, as a\nrepresentative example, is inherently characterized by strict temporal order\nand diverse modalities. Effectively leveraging the temporal structure is\nessential for understanding both intra-modal dynamics and inter-modal\ncorrelations. However, most existing approaches treat each modality\nindependently and rely on shallow fusion strategies, which overlook temporal\ndependencies and hinder the model's ability to represent complex structural\nrelationships. To address the limitation, we propose the hybrid hypergraph\nnetwork (HHN), a novel framework that models temporal multimodal data via a\nsegmentation-first, graph-later strategy. HHN splits sequences into timestamped\nsegments as nodes in a heterogeneous graph. Intra-modal structures are captured\nvia hyperedges guided by a maximum entropy difference criterion, enhancing node\nheterogeneity and structural discrimination, followed by hypergraph convolution\nto extract high-order dependencies. Inter-modal links are established through\ntemporal alignment and graph attention for semantic fusion. HHN achieves\nstate-of-the-art (SOTA) results on four multimodal datasets, demonstrating its\neffectiveness in complex classification tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df7\u5408\u8d85\u56fe\u7f51\u7edc (HHN)\uff0c\u7528\u4e8e\u5efa\u6a21\u65f6\u95f4\u591a\u6a21\u6001\u6570\u636e\uff0c\u5e76\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u5efa\u6a21\u65f6\u95f4\u591a\u6a21\u6001\u6570\u636e\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u6355\u83b7\u8fdc\u8ddd\u79bb\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u590d\u6742\u7684\u8de8\u6a21\u6001\u4ea4\u4e92\u65b9\u9762\u3002\u89c6\u542c\u6570\u636e\u4f5c\u4e3a\u4e00\u4e2a\u4ee3\u8868\u6027\u4f8b\u5b50\uff0c\u5176\u672c\u8d28\u7279\u5f81\u662f\u4e25\u683c\u7684\u65f6\u95f4\u987a\u5e8f\u548c\u591a\u6837\u7684\u6a21\u6001\u3002\u6709\u6548\u5229\u7528\u65f6\u95f4\u7ed3\u6784\u5bf9\u4e8e\u7406\u89e3\u6a21\u5185\u52a8\u6001\u548c\u6a21\u95f4\u76f8\u5173\u6027\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u72ec\u7acb\u5904\u7406\u6bcf\u4e2a\u6a21\u6001\uff0c\u5e76\u4f9d\u8d56\u4e8e\u6d45\u5c42\u878d\u5408\u7b56\u7565\uff0c\u8fd9\u5ffd\u7565\u4e86\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u5e76\u963b\u788d\u4e86\u6a21\u578b\u8868\u793a\u590d\u6742\u7ed3\u6784\u5173\u7cfb\u7684\u80fd\u529b\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u6df7\u5408\u8d85\u56fe\u7f51\u7edc (HHN)\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5148\u5206\u5272\u3001\u540e\u56fe\u5f62\u7684\u7b56\u7565\u5bf9\u65f6\u95f4\u591a\u6a21\u6001\u6570\u636e\u8fdb\u884c\u5efa\u6a21\u3002HHN \u5c06\u5e8f\u5217\u5206\u5272\u6210\u5e26\u65f6\u95f4\u6233\u7684\u7247\u6bb5\uff0c\u4f5c\u4e3a\u5f02\u6784\u56fe\u4e2d\u7684\u8282\u70b9\u3002\u6a21\u5185\u7ed3\u6784\u901a\u8fc7\u6700\u5927\u71b5\u5dee\u51c6\u5219\u5f15\u5bfc\u7684\u8d85\u8fb9\u6355\u83b7\uff0c\u589e\u5f3a\u8282\u70b9\u5f02\u8d28\u6027\u548c\u7ed3\u6784\u5224\u522b\uff0c\u7136\u540e\u8fdb\u884c\u8d85\u56fe\u5377\u79ef\u4ee5\u63d0\u53d6\u9ad8\u9636\u4f9d\u8d56\u5173\u7cfb\u3002\u6a21\u95f4\u94fe\u63a5\u901a\u8fc7\u65f6\u95f4\u5bf9\u9f50\u548c\u56fe\u6ce8\u610f\u5efa\u7acb\uff0c\u4ee5\u5b9e\u73b0\u8bed\u4e49\u878d\u5408\u3002", "result": "HHN \u5728\u56db\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb (SOTA) \u7684\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u590d\u6742\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "HHN\u5728\u56db\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u590d\u6742\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.00956", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.00956", "abs": "https://arxiv.org/abs/2508.00956", "authors": ["Chuan He", "Yang Chen", "Wuliang Huang", "Tianyi Zheng", "Jianhu Chen", "Bin Dou", "Yice Luo", "Yun Zhu", "Baokun Wang", "Yongchao Liu", "Xing Fu", "Yu Cheng", "Chuntao Hong", "Weiqiang Wang", "Xin-Wei Yao"], "title": "Learning Unified User Quantized Tokenizers for User Representation", "comment": null, "summary": "Multi-source user representation learning plays a critical role in enabling\npersonalized services on web platforms (e.g., Alipay). While prior works have\nadopted late-fusion strategies to combine heterogeneous data sources, they\nsuffer from three key limitations: lack of unified representation frameworks,\nscalability and storage issues in data compression, and inflexible cross-task\ngeneralization. To address these challenges, we propose U^2QT (Unified User\nQuantized Tokenizers), a novel framework that integrates cross-domain knowledge\ntransfer with early fusion of heterogeneous domains. Our framework employs a\ntwo-stage architecture: first, a causal Q-Former projects domain-specific\nfeatures into a shared causal representation space to preserve inter-modality\ndependencies; second, a multi-view RQ-VAE discretizes causal embeddings into\ncompact tokens through shared and source-specific codebooks, enabling efficient\nstorage while maintaining semantic coherence. Experimental results showcase\nU^2QT's advantages across diverse downstream tasks, outperforming task-specific\nbaselines in future behavior prediction and recommendation tasks while\nachieving efficiency gains in storage and computation. The unified tokenization\nframework enables seamless integration with language models and supports\nindustrial-scale applications.", "AI": {"tldr": "U^2QT\uff1a\u4e00\u79cd\u7528\u4e8e\u591a\u6e90\u7528\u6237\u8868\u793a\u5b66\u4e60\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u57df\u77e5\u8bc6\u8f6c\u79fb\u548c\u5f02\u6784\u57df\u7684\u65e9\u671f\u878d\u5408\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6548\u7387\u63d0\u5347\u3002", "motivation": "\u5148\u524d\u7684\u5de5\u4f5c\u91c7\u7528 late-fusion \u7b56\u7565\u6765\u7ec4\u5408\u5f02\u6784\u6570\u636e\u6e90\uff0c\u4f46\u5b83\u4eec\u5b58\u5728\u4e09\u4e2a\u5173\u952e\u9650\u5236\uff1a\u7f3a\u4e4f\u7edf\u4e00\u7684\u8868\u793a\u6846\u67b6\u3001\u6570\u636e\u538b\u7f29\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u5b58\u50a8\u95ee\u9898\u4ee5\u53ca\u4e0d\u7075\u6d3b\u7684\u8de8\u4efb\u52a1\u6cdb\u5316\u3002", "method": "U^2QT \u91c7\u7528\u4e24\u9636\u6bb5\u67b6\u6784\uff1a\u9996\u5148\uff0c\u56e0\u679c Q-Former \u5c06\u7279\u5b9a\u9886\u57df\u7684\u7279\u5f81\u6295\u5f71\u5230\u5171\u4eab\u7684\u56e0\u679c\u8868\u793a\u7a7a\u95f4\uff0c\u4ee5\u4fdd\u7559\u6a21\u6001\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff1b\u5176\u6b21\uff0c\u591a\u89c6\u56fe RQ-VAE \u901a\u8fc7\u5171\u4eab\u548c\u6e90\u7279\u5b9a\u7684\u5bc6\u7801\u672c\u5c06\u56e0\u679c\u5d4c\u5165\u79bb\u6563\u5316\u4e3a\u7d27\u51d1\u7684token\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u6548\u5b58\u50a8\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u8fde\u8d2f\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cU^2QT \u5728\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u90fd\u5177\u6709\u4f18\u52bf\uff0c\u5728\u672a\u6765\u7684\u884c\u4e3a\u9884\u6d4b\u548c\u63a8\u8350\u4efb\u52a1\u4e2d\u4f18\u4e8e\u7279\u5b9a\u4efb\u52a1\u7684\u57fa\u7ebf\uff0c\u540c\u65f6\u5728\u5b58\u50a8\u548c\u8ba1\u7b97\u65b9\u9762\u5b9e\u73b0\u4e86\u6548\u7387\u63d0\u5347\u3002", "conclusion": "U^2QT\u5728\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5728\u672a\u6765\u884c\u4e3a\u9884\u6d4b\u548c\u63a8\u8350\u4efb\u52a1\u4e2d\u4f18\u4e8e\u7279\u5b9a\u4efb\u52a1\u7684\u57fa\u7ebf\uff0c\u540c\u65f6\u5728\u5b58\u50a8\u548c\u8ba1\u7b97\u65b9\u9762\u5b9e\u73b0\u4e86\u6548\u7387\u63d0\u5347\u3002\u7edf\u4e00\u7684\u6807\u8bb0\u5316\u6846\u67b6\u80fd\u591f\u4e0e\u8bed\u8a00\u6a21\u578b\u65e0\u7f1d\u96c6\u6210\uff0c\u5e76\u652f\u6301\u5de5\u4e1a\u89c4\u6a21\u7684\u5e94\u7528\u3002"}}
{"id": "2508.01424", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01424", "abs": "https://arxiv.org/abs/2508.01424", "authors": ["Haonan Bian", "Yutao Qi", "Rui Yang", "Yuanxi Che", "Jiaqian Wang", "Heming Xia", "Ranran Zhen"], "title": "From Query to Logic: Ontology-Driven Multi-Hop Reasoning in LLMs", "comment": null, "summary": "Large Language Models (LLMs), despite their success in question answering,\nexhibit limitations in complex multi-hop question answering (MQA) tasks that\nnecessitate non-linear, structured reasoning. This limitation stems from their\ninability to adequately capture deep conceptual relationships between entities.\nTo overcome this challenge, we present **ORACLE** (**O**ntology-driven\n**R**easoning **A**nd **C**hain for **L**ogical **E**ucidation), a\ntraining-free framework that combines LLMs' generative capabilities with the\nstructural benefits of knowledge graphs. Our approach operates through three\nstages: (1) dynamic construction of question-specific knowledge ontologies\nusing LLMs, (2) transformation of these ontologies into First-Order Logic\nreasoning chains, and (3) systematic decomposition of the original query into\nlogically coherent sub-questions. Experimental results on several standard MQA\nbenchmarks show that our framework achieves highly competitive performance,\nrivaling current state-of-the-art models like DeepSeek-R1. Detailed analyses\nfurther confirm the effectiveness of each component, while demonstrating that\nour method generates more logical and interpretable reasoning chains than\nexisting approaches.", "AI": {"tldr": "ORACLE, a training-free framework, enhances LLMs for complex reasoning by using knowledge graphs to build question-specific ontologies and decompose queries into logical sub-questions, achieving state-of-the-art performance.", "motivation": "LLMs struggle with complex multi-hop question answering (MQA) due to their inability to capture deep conceptual relationships between entities.", "method": "A training-free framework combining LLMs with knowledge graphs through dynamic ontology construction, transformation into First-Order Logic chains, and systematic query decomposition.", "result": "ORACLE achieves highly competitive performance on MQA benchmarks and generates more logical and interpretable reasoning chains.", "conclusion": "The ORACLE framework achieves highly competitive performance on MQA benchmarks, rivaling state-of-the-art models and generating more logical reasoning chains."}}
{"id": "2508.01126", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01126", "abs": "https://arxiv.org/abs/2508.01126", "authors": ["Chaitanya Patel", "Hiroki Nakamura", "Yuta Kyuragi", "Kazuki Kozuka", "Juan Carlos Niebles", "Ehsan Adeli"], "title": "UniEgoMotion: A Unified Model for Egocentric Motion Reconstruction, Forecasting, and Generation", "comment": "ICCV 2025. Project Page:\n  https://chaitanya100100.github.io/UniEgoMotion/", "summary": "Egocentric human motion generation and forecasting with scene-context is\ncrucial for enhancing AR/VR experiences, improving human-robot interaction,\nadvancing assistive technologies, and enabling adaptive healthcare solutions by\naccurately predicting and simulating movement from a first-person perspective.\nHowever, existing methods primarily focus on third-person motion synthesis with\nstructured 3D scene contexts, limiting their effectiveness in real-world\negocentric settings where limited field of view, frequent occlusions, and\ndynamic cameras hinder scene perception. To bridge this gap, we introduce\nEgocentric Motion Generation and Egocentric Motion Forecasting, two novel tasks\nthat utilize first-person images for scene-aware motion synthesis without\nrelying on explicit 3D scene. We propose UniEgoMotion, a unified conditional\nmotion diffusion model with a novel head-centric motion representation tailored\nfor egocentric devices. UniEgoMotion's simple yet effective design supports\negocentric motion reconstruction, forecasting, and generation from first-person\nvisual inputs in a unified framework. Unlike previous works that overlook scene\nsemantics, our model effectively extracts image-based scene context to infer\nplausible 3D motion. To facilitate training, we introduce EE4D-Motion, a\nlarge-scale dataset derived from EgoExo4D, augmented with pseudo-ground-truth\n3D motion annotations. UniEgoMotion achieves state-of-the-art performance in\negocentric motion reconstruction and is the first to generate motion from a\nsingle egocentric image. Extensive evaluations demonstrate the effectiveness of\nour unified framework, setting a new benchmark for egocentric motion modeling\nand unlocking new possibilities for egocentric applications.", "AI": {"tldr": "This paper introduces UniEgoMotion, a unified conditional motion diffusion model for egocentric motion generation and forecasting from first-person images, and introduces EE4D-Motion, a large-scale dataset for training.", "motivation": "Existing methods primarily focus on third-person motion synthesis with structured 3D scene contexts, limiting their effectiveness in real-world egocentric settings where limited field of view, frequent occlusions, and dynamic cameras hinder scene perception.", "method": "We propose UniEgoMotion, a unified conditional motion diffusion model with a novel head-centric motion representation tailored for egocentric devices.", "result": "UniEgoMotion achieves state-of-the-art performance in egocentric motion reconstruction and is the first to generate motion from a single egocentric image.", "conclusion": "UniEgoMotion achieves state-of-the-art performance in egocentric motion reconstruction and is the first to generate motion from a single egocentric image. Extensive evaluations demonstrate the effectiveness of our unified framework, setting a new benchmark for egocentric motion modeling and unlocking new possibilities for egocentric applications."}}
{"id": "2508.01273", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01273", "abs": "https://arxiv.org/abs/2508.01273", "authors": ["Xianda Zheng", "Zijian Huang", "Meng-Fen Chiang", "Michael J. Witbrock", "Kaiqi Zhao"], "title": "KCR: Resolving Long-Context Knowledge Conflicts via Reasoning in LLMs", "comment": null, "summary": "Knowledge conflicts commonly arise across diverse sources, and their\nprevalence has increased with the advent of LLMs. When dealing with conflicts\nbetween multiple contexts, also known as \\emph{inter-context knowledge\nconflicts}, LLMs are often confused by lengthy and conflicting contexts. To\naddress this challenge, we propose the Knowledge Conflict Reasoning (KCR)\nframework, which enhances the ability of LLMs to resolve conflicting knowledge.\nThe key idea of KCR is to train backbone LLMs to establish a correct reasoning\nprocess by rewarding them for selecting and adhering to the context with\nstronger logical consistency when presented with conflicting contexts.\nSpecifically, we first extract reasoning paths, represented by either text or\nlocal knowledge graphs, from the conflicting long contexts. Subsequently, we\nemploy Reinforcement Learning to encourage the model to learn the paradigm of\nreasoning process that follows correct reasoning paths rather than the\nincorrect counterparts. This enables the backbone models to genuinely acquire\nthe capability to resolve inter-context knowledge conflicts within long\ncontexts. Experimental results demonstrate that our framework significantly\nimproves the ability of various backbone models to resolve knowledge conflicts\nin long-context scenarios, yielding substantial performance gains.", "AI": {"tldr": "Proposes a Knowledge Conflict Reasoning (KCR) framework to enhance LLMs' ability to resolve conflicting knowledge by training them to follow logically consistent reasoning paths.", "motivation": "When dealing with conflicts between multiple contexts, also known as inter-context knowledge conflicts, LLMs are often confused by lengthy and conflicting contexts.", "method": "The Knowledge Conflict Reasoning (KCR) framework, which enhances the ability of LLMs to resolve conflicting knowledge. The key idea of KCR is to train backbone LLMs to establish a correct reasoning process by rewarding them for selecting and adhering to the context with stronger logical consistency when presented with conflicting contexts.", "result": "The framework significantly improves the ability of various backbone models to resolve knowledge conflicts in long-context scenarios, yielding substantial performance gains.", "conclusion": "This framework significantly improves the ability of various backbone models to resolve knowledge conflicts in long-context scenarios, yielding substantial performance gains."}}
{"id": "2508.00930", "categories": ["cs.LG", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2508.00930", "abs": "https://arxiv.org/abs/2508.00930", "authors": ["M. Ontivero-Ortega", "A. Fania", "A. Lacalamita", "R. Bellotti", "A. Monaco", "S. Stramaglia"], "title": "Cooperative effects in feature importance of individual patterns: application to air pollutants and Alzheimer disease", "comment": null, "summary": "Leveraging recent advances in the analysis of synergy and redundancy in\nsystems of random variables, an adaptive version of the widely used metric\nLeave One Covariate Out (LOCO) has been recently proposed to quantify\ncooperative effects in feature importance (Hi-Fi), a key technique in\nexplainable artificial intelligence (XAI), so as to disentangle high-order\neffects involving a particular input feature in regression problems.\nDifferently from standard feature importance tools, where a single score\nmeasures the relevance of each feature, each feature is here characterized by\nthree scores, a two-body (unique) score and higher-order scores (redundant and\nsynergistic). This paper presents a framework to assign those three scores\n(unique, redundant, and synergistic) to each individual pattern of the data\nset, while comparing it with the well-known measure of feature importance named\n{\\it Shapley effect}. To illustrate the potential of the proposed framework, we\nfocus on a One-Health application: the relation between air pollutants and\nAlzheimer's disease mortality rate. Our main result is the synergistic\nassociation between features related to $O_3$ and $NO_2$ with mortality,\nespecially in the provinces of Bergamo e Brescia; notably also the density of\nurban green areas displays synergistic influence with pollutants for the\nprediction of AD mortality. Our results place local Hi-Fi as a promising tool\nof wide applicability, which opens new perspectives for XAI as well as to\nanalyze high-order relationships in complex systems.", "AI": {"tldr": "This paper presents a framework to assign unique, redundant, and synergistic scores to each individual pattern of the data set, while comparing it with the well-known measure of feature importance named {\"it Shapley effect\"}.", "motivation": "quantify cooperative effects in feature importance (Hi-Fi), a key technique in explainable artificial intelligence (XAI), so as to disentangle high-order effects involving a particular input feature in regression problems.", "method": "adaptive version of the widely used metric Leave One Covariate Out (LOCO)", "result": "synergistic association between features related to $O_3$ and $NO_2$ with mortality, especially in the provinces of Bergamo e Brescia; notably also the density of urban green areas displays synergistic influence with pollutants for the prediction of AD mortality.", "conclusion": "Local Hi-Fi is a promising tool of wide applicability, which opens new perspectives for XAI as well as to analyze high-order relationships in complex systems."}}
{"id": "2508.01450", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01450", "abs": "https://arxiv.org/abs/2508.01450", "authors": ["Xinlin Zhuang", "Feilong Tang", "Haolin Yang", "Ming Hu", "Huifa Li", "Haochen Xue", "Yichen Li", "Junjun He", "Zongyuan Ge", "Ying Qian", "Imran Razzak"], "title": "Towards Efficient Medical Reasoning with Minimal Fine-Tuning Data", "comment": "preprint, under review", "summary": "Supervised Fine-Tuning (SFT) plays a pivotal role in adapting Large Language\nModels (LLMs) to specialized domains such as medical reasoning. However,\nexisting SFT practices often rely on unfiltered datasets that contain redundant\nand low-quality samples, leading to substantial computational costs and\nsuboptimal performance. Although existing methods attempt to alleviate this\nproblem by selecting data based on sample difficulty, defined by knowledge and\nreasoning complexity, they overlook each sample's optimization utility\nreflected in its gradient. Interestingly, we find that gradient-based influence\nalone favors easy-to-optimize samples that cause large parameter shifts but\nlack deep reasoning chains, while difficulty alone selects noisy or overly\ncomplex cases that fail to guide stable optimization. Based on this\nobservation, we propose a data selection strategy, Difficulty-Influence\nQuadrant (DIQ), which prioritizes samples in the high-difficulty-high-influence\nquadrant to balance complex clinical reasoning with substantial gradient\ninfluence, enabling efficient medical reasoning with minimal fine-tuning data.\nFurthermore, Human and LLM-as-a-judge evaluations show that DIQ-selected\nsubsets demonstrate higher data quality and generate clinical reasoning that is\nmore aligned with expert practices in differential diagnosis, safety check, and\nevidence citation, as DIQ emphasizes samples that foster expert-like reasoning\npatterns. Extensive experiments on medical reasoning benchmarks demonstrate\nthat DIQ enables models fine-tuned on only 1% of selected data to match\nfull-dataset performance, while using 10% consistently outperforms the\nbaseline, highlighting the superiority of principled data selection over\nbrute-force scaling. The code and data are available at\nhttps://github.com/mihara-bot/DIQ.", "AI": {"tldr": "DIQ\u662f\u4e00\u79cd\u65b0\u7684\u6570\u636e\u9009\u62e9\u7b56\u7565\uff0c\u5b83\u4f18\u5148\u8003\u8651\u9ad8\u96be\u5ea6-\u9ad8\u5f71\u54cd\u7684\u6837\u672c\uff0c\u4ee5\u63d0\u9ad8\u533b\u5b66\u63a8\u7406\u7684\u6548\u7387\u548c\u8d28\u91cf\uff0c\u5b9e\u9a8c\u8868\u660e\uff0c\u4ec5\u4f7f\u7528\u5c11\u91cf\u6570\u636e\u5373\u53ef\u8fbe\u5230\u751a\u81f3\u8d85\u8fc7\u5b8c\u6574\u6570\u636e\u96c6\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684SFT\u5b9e\u8df5\u901a\u5e38\u4f9d\u8d56\u4e8e\u5305\u542b\u5197\u4f59\u548c\u4f4e\u8d28\u91cf\u6837\u672c\u7684\u672a\u8fc7\u6ee4\u6570\u636e\u96c6\uff0c\u5bfc\u81f4\u5927\u91cf\u7684\u8ba1\u7b97\u6210\u672c\u548c\u6b21\u4f18\u7684\u6027\u80fd\u3002\u867d\u7136\u73b0\u6709\u7684\u65b9\u6cd5\u8bd5\u56fe\u901a\u8fc7\u57fa\u4e8e\u6837\u672c\u96be\u5ea6\uff08\u7531\u77e5\u8bc6\u548c\u63a8\u7406\u590d\u6742\u6027\u5b9a\u4e49\uff09\u9009\u62e9\u6570\u636e\u6765\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46\u5b83\u4eec\u5ffd\u7565\u4e86\u6bcf\u4e2a\u6837\u672c\u7684\u4f18\u5316\u6548\u7528\uff0c\u800c\u8fd9\u79cd\u6548\u7528\u4f53\u73b0\u5728\u5176\u68af\u5ea6\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9009\u62e9\u7b56\u7565\uff0c\u5373\u96be\u5ea6-\u5f71\u54cd\u8c61\u9650\uff08DIQ\uff09\uff0c\u8be5\u7b56\u7565\u4f18\u5148\u8003\u8651\u9ad8\u96be\u5ea6-\u9ad8\u5f71\u54cd\u8c61\u9650\u4e2d\u7684\u6837\u672c\uff0c\u4ee5\u5e73\u8861\u590d\u6742\u7684\u4e34\u5e8a\u63a8\u7406\u548c\u5b9e\u8d28\u6027\u7684\u68af\u5ea6\u5f71\u54cd\uff0c\u4ece\u800c\u80fd\u591f\u4ee5\u6700\u5c11\u7684\u5fae\u8c03\u6570\u636e\u5b9e\u73b0\u9ad8\u6548\u7684\u533b\u7597\u63a8\u7406\u3002", "result": "DIQ\u9009\u62e9\u7684\u5b50\u96c6\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6570\u636e\u8d28\u91cf\uff0c\u5e76\u751f\u6210\u4e0e\u5dee\u5f02\u8bca\u65ad\u3001\u5b89\u5168\u68c0\u67e5\u548c\u8bc1\u636e\u5f15\u7528\u7684\u4e13\u5bb6\u5b9e\u8df5\u66f4\u4e00\u81f4\u7684\u4e34\u5e8a\u63a8\u7406\uff0c\u56e0\u4e3aDIQ\u5f3a\u8c03\u57f9\u517b\u7c7b\u4f3c\u4e13\u5bb6\u7684\u63a8\u7406\u6a21\u5f0f\u7684\u6837\u672c\u3002", "conclusion": "DIQ\u5728\u533b\u7597\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4ec5\u4f7f\u75281%\u7684\u7cbe\u9009\u6570\u636e\u5c31\u80fd\u8fbe\u5230\u5b8c\u6574\u6570\u636e\u96c6\u7684\u6027\u80fd\uff0c\u800c\u4f7f\u752810%\u7684\u6570\u636e\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\uff0c\u7a81\u51fa\u4e86\u6709\u539f\u5219\u7684\u6570\u636e\u9009\u62e9\u76f8\u5bf9\u4e8e\u66b4\u529b\u6269\u5c55\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2508.01137", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01137", "abs": "https://arxiv.org/abs/2508.01137", "authors": ["Zeduo Zhang", "Yalda Mohsenzadeh"], "title": "Semi-Supervised Anomaly Detection in Brain MRI Using a Domain-Agnostic Deep Reinforcement Learning Approach", "comment": "34 pages, 6 figures and 4 tables in main text, 17 pages supplementary\n  material with 3 tables and 3 figures; Submitted to Radiology: Artificial\n  Intelligence", "summary": "To develop a domain-agnostic, semi-supervised anomaly detection framework\nthat integrates deep reinforcement learning (DRL) to address challenges such as\nlarge-scale data, overfitting, and class imbalance, focusing on brain MRI\nvolumes. This retrospective study used publicly available brain MRI datasets\ncollected between 2005 and 2021. The IXI dataset provided 581 T1-weighted and\n578 T2-weighted MRI volumes (from healthy subjects) for training, while the\nBraTS 2021 dataset provided 251 volumes for validation and 1000 for testing\n(unhealthy subjects with Glioblastomas). Preprocessing included normalization,\nskull-stripping, and co-registering to a uniform voxel size. Experiments were\nconducted on both T1- and T2-weighted modalities. Additional experiments and\nablation analyses were also carried out on the industrial datasets. The\nproposed method integrates DRL with feature representations to handle label\nscarcity, large-scale data and overfitting. Statistical analysis was based on\nseveral detection and segmentation metrics including AUROC and Dice score. The\nproposed method achieved an AUROC of 88.7% (pixel-level) and 96.7%\n(image-level) on brain MRI datasets, outperforming State-of-The-Art (SOTA)\nmethods. On industrial surface datasets, the model also showed competitive\nperformance (AUROC = 99.8% pixel-level, 99.3% image-level) on MVTec AD dataset,\nindicating strong cross-domain generalization. Studies on anomaly sample size\nshowed a monotonic increase in AUROC as more anomalies were seen, without\nevidence of overfitting or additional computational cost. The domain-agnostic\nsemi-supervised approach using DRL shows significant promise for MRI anomaly\ndetection, achieving strong performance on both medical and industrial\ndatasets. Its robustness, generalizability and efficiency highlight its\npotential for real-world clinical applications.", "AI": {"tldr": "Developed a DRL-based anomaly detection framework for brain MRIs, achieving high AUROC scores and demonstrating strong generalizability across domains.", "motivation": "address challenges such as large-scale data, overfitting, and class imbalance, focusing on brain MRI volumes", "method": "integrates DRL with feature representations to handle label scarcity, large-scale data and overfitting", "result": "achieved an AUROC of 88.7% (pixel-level) and 96.7% (image-level) on brain MRI datasets, outperforming State-of-The-Art (SOTA) methods. On industrial surface datasets, the model also showed competitive performance (AUROC = 99.8% pixel-level, 99.3% image-level) on MVTec AD dataset, indicating strong cross-domain generalization", "conclusion": "The domain-agnostic semi-supervised approach using DRL shows significant promise for MRI anomaly detection, achieving strong performance on both medical and industrial datasets. Its robustness, generalizability and efficiency highlight its potential for real-world clinical applications."}}
{"id": "2508.01274", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01274", "abs": "https://arxiv.org/abs/2508.01274", "authors": ["Jui-Ming Yao", "Bing-Cheng Xie", "Sheng-Wei Peng", "Hao-Yuan Chen", "He-Rong Zheng", "Bing-Jia Tan", "Peter Shaojui Wang", "Shun-Feng Su"], "title": "Multi-TW: Benchmarking Multimodal Models on Traditional Chinese Question Answering in Taiwan", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) process visual, acoustic, and\ntextual inputs, addressing the limitations of single-modality LLMs. However,\nexisting benchmarks often overlook tri-modal evaluation in Traditional Chinese\nand do not consider inference latency. To address this, we introduce Multi-TW,\nthe first Traditional Chinese benchmark for evaluating the performance and\nlatency of any-to-any multimodal models. Multi-TW includes 900 multiple-choice\nquestions (image and text, audio and text pairs) sourced from official\nproficiency tests developed with the Steering Committee for the Test of\nProficiency-Huayu (SC-TOP). We evaluated various any-to-any models and\nvision-language models (VLMs) with audio transcription. Our results show that\nclosed-source models generally outperform open-source ones across modalities,\nalthough open-source models can perform well in audio tasks. End-to-end\nany-to-any pipelines offer clear latency advantages compared to VLMs using\nseparate audio transcription. Multi-TW presents a comprehensive view of model\ncapabilities and highlights the need for Traditional Chinese fine-tuning and\nefficient multimodal architectures.", "AI": {"tldr": "Multi-TW\u662f\u4e00\u4e2a\u65b0\u7684\u7e41\u4f53\u4e2d\u6587\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u6a21\u578b\u7684\u6027\u80fd\u548c\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u901a\u5e38\u5ffd\u7565\u4e86\u4f20\u7edf\u4e2d\u6587\u7684\u4e09\u6a21\u6001\u8bc4\u4f30\uff0c\u5e76\u4e14\u4e0d\u8003\u8651\u63a8\u7406\u5ef6\u8fdf\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898", "method": "\u6211\u4eec\u5f15\u5165\u4e86Multi-TW\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u4efb\u4f55\u5230\u4efb\u4f55\u591a\u6a21\u6001\u6a21\u578b\u7684\u6027\u80fd\u548c\u5ef6\u8fdf\u7684\u7e41\u4f53\u4e2d\u6587\u57fa\u51c6\u3002", "result": "\u95ed\u6e90\u6a21\u578b\u901a\u5e38\u4f18\u4e8e\u8de8\u6a21\u6001\u7684\u5f00\u6e90\u6a21\u578b\uff0c\u5c3d\u7ba1\u5f00\u6e90\u6a21\u578b\u53ef\u4ee5\u5728\u97f3\u9891\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\u3002\u4e0e\u4f7f\u7528\u5355\u72ec\u97f3\u9891\u8f6c\u5f55\u7684VLM \u76f8\u6bd4\uff0c\u7aef\u5230\u7aef\u4efb\u4f55\u5230\u4efb\u4f55\u7ba1\u9053\u63d0\u4f9b\u4e86\u660e\u663e\u7684\u5ef6\u8fdf\u4f18\u52bf\u3002", "conclusion": "Multi-TW\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6a21\u578b\u80fd\u529b\u89c6\u89d2\uff0c\u5e76\u5f3a\u8c03\u4e86\u5bf9\u7e41\u4f53\u4e2d\u6587\u8fdb\u884c\u5fae\u8c03\u548c\u9ad8\u6548\u591a\u6a21\u6001\u67b6\u6784\u7684\u9700\u6c42\u3002"}}
{"id": "2508.00933", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00933", "abs": "https://arxiv.org/abs/2508.00933", "authors": ["Hanchen Yang", "Jiaqi Wang", "Jiannong Cao", "Wengen Li", "Jialun Zheng", "Yangning Li", "Chunyu Miao", "Jihong Guan", "Shuigeng Zhou", "Philip S. Yu"], "title": "OKG-LLM: Aligning Ocean Knowledge Graph with Observation Data via LLMs for Global Sea Surface Temperature Prediction", "comment": null, "summary": "Sea surface temperature (SST) prediction is a critical task in ocean science,\nsupporting various applications, such as weather forecasting, fisheries\nmanagement, and storm tracking. While existing data-driven methods have\ndemonstrated significant success, they often neglect to leverage the rich\ndomain knowledge accumulated over the past decades, limiting further\nadvancements in prediction accuracy. The recent emergence of large language\nmodels (LLMs) has highlighted the potential of integrating domain knowledge for\ndownstream tasks. However, the application of LLMs to SST prediction remains\nunderexplored, primarily due to the challenge of integrating ocean domain\nknowledge and numerical data. To address this issue, we propose Ocean Knowledge\nGraph-enhanced LLM (OKG-LLM), a novel framework for global SST prediction. To\nthe best of our knowledge, this work presents the first systematic effort to\nconstruct an Ocean Knowledge Graph (OKG) specifically designed to represent\ndiverse ocean knowledge for SST prediction. We then develop a graph embedding\nnetwork to learn the comprehensive semantic and structural knowledge within the\nOKG, capturing both the unique characteristics of individual sea regions and\nthe complex correlations between them. Finally, we align and fuse the learned\nknowledge with fine-grained numerical SST data and leverage a pre-trained LLM\nto model SST patterns for accurate prediction. Extensive experiments on the\nreal-world dataset demonstrate that OKG-LLM consistently outperforms\nstate-of-the-art methods, showcasing its effectiveness, robustness, and\npotential to advance SST prediction. The codes are available in the online\nrepository.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86OKG-LLM\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u5168\u7403SST\u9884\u6d4b\u7684\u65b0\u6846\u67b6\uff0c\u5b83\u5229\u7528\u6d77\u6d0b\u77e5\u8bc6\u56fe\u6765\u589e\u5f3aLLM\uff0c\u5e76\u5728\u5b9e\u9645\u6570\u636e\u96c6\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728SST\u9884\u6d4b\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u5ffd\u7565\u4e86\u5229\u7528\u8fc7\u53bb\u51e0\u5341\u5e74\u79ef\u7d2f\u7684\u4e30\u5bcc\u7684\u9886\u57df\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u9884\u6d4b\u7cbe\u5ea6\u7684\u8fdb\u4e00\u6b65\u63d0\u9ad8\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u7684\u6700\u65b0\u51fa\u73b0\u7a81\u51fa\u4e86\u6574\u5408\u9886\u57df\u77e5\u8bc6\u4ee5\u5b8c\u6210\u4e0b\u6e38\u4efb\u52a1\u7684\u6f5c\u529b\u3002\u7136\u800c\uff0c\u7531\u4e8e\u6574\u5408\u6d77\u6d0b\u9886\u57df\u77e5\u8bc6\u548c\u6570\u503c\u6570\u636e\u7684\u6311\u6218\uff0cllm\u5728SST\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\u4ecd\u6709\u5f85\u63a2\u7d22\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u6d77\u6d0b\u77e5\u8bc6\u56fe\u589e\u5f3aLLM (OKG-LLM)\uff0c\u8fd9\u662f\u4e00\u79cd\u7528\u4e8e\u5168\u7403SST\u9884\u6d4b\u7684\u65b0\u6846\u67b6\u3002\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u6d77\u6d0b\u77e5\u8bc6\u56fe(OKG)\uff0c\u4e13\u95e8\u7528\u4e8e\u8868\u793a\u5404\u79cd\u6d77\u6d0b\u77e5\u8bc6\u4ee5\u8fdb\u884cSST\u9884\u6d4b\u3002\u7136\u540e\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u56fe\u5d4c\u5165\u7f51\u7edc\u6765\u5b66\u4e60OKG\u4e2d\u5168\u9762\u7684\u8bed\u4e49\u548c\u7ed3\u6784\u77e5\u8bc6\uff0c\u6355\u6349\u5404\u4e2a\u6d77\u57df\u7684\u72ec\u7279\u7279\u5f81\u4ee5\u53ca\u5b83\u4eec\u4e4b\u95f4\u590d\u6742\u7684\u5173\u8054\u3002\u6700\u540e\uff0c\u6211\u4eec\u5c06\u5b66\u4e60\u5230\u7684\u77e5\u8bc6\u4e0e\u7ec6\u7c92\u5ea6\u7684\u6570\u503cSST\u6570\u636e\u5bf9\u9f50\u548c\u878d\u5408\uff0c\u5e76\u5229\u7528\u9884\u8bad\u7ec3\u7684LLM\u6765\u5efa\u6a21SST\u6a21\u5f0f\uff0c\u4ee5\u5b9e\u73b0\u51c6\u786e\u7684\u9884\u6d4b\u3002", "result": "OKG-LLM\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3001\u7a33\u5065\u6027\u548c\u63a8\u8fdbSST\u9884\u6d4b\u7684\u6f5c\u529b\u3002", "conclusion": "OKG-LLM\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u5176\u6027\u80fd\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3001\u7a33\u5065\u6027\u548c\u63a8\u8fdbSST\u9884\u6d4b\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.01473", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01473", "abs": "https://arxiv.org/abs/2508.01473", "authors": ["Yiming Zeng", "Jinghan Cao", "Zexin Li", "Yiming Chen", "Tao Ren", "Dawei Xiang", "Xidong Wu", "Shangqian Gao", "Tingting Yu"], "title": "TreeDiff: AST-Guided Code Generation with Diffusion LLMs", "comment": null, "summary": "Recent advances in diffusion-based language models have opened new\npossibilities for controllable and bidirectional sequence generation. These\nmodels provide an alternative to traditional autoregressive approaches by\nframing text generation as an iterative denoising process. However, applying\ndiffusion models to structured domains such as source code remains a\nsignificant challenge. Programming languages differ from natural language in\nthat they follow strict syntactic and semantic rules, with hierarchical\norganization that must be preserved for correctness. Standard token-level\ncorruption techniques used during training often ignore this structure, which\nmay hinder the model's ability to learn meaningful representations of code. To\naddress this limitation, we propose a syntax-aware diffusion framework that\nincorporates structural priors from Abstract Syntax Trees (ASTs) into the\ndenoising process. Instead of masking individual tokens at random, we\nselectively corrupt syntactically meaningful code spans derived from AST\nsubtrees. This enables the model to reconstruct programs in a way that respects\ngrammatical boundaries and captures long-range dependencies. Experimental\nresults demonstrate that syntax-aware corruption significantly improves\nsyntactic correctness, reconstruction accuracy, and generalization to unseen\ncode patterns. These findings highlight the potential of incorporating\nstructural information into diffusion-based training and suggest that\nsyntax-guided denoising is a promising direction for advancing diffusion-based\nlanguage models in code generation tasks.", "AI": {"tldr": "This paper introduces a syntax-aware diffusion framework that incorporates structural priors from Abstract Syntax Trees (ASTs) into the denoising process for code generation. Experimental results demonstrate that syntax-aware corruption significantly improves syntactic correctness, reconstruction accuracy, and generalization to unseen code patterns.", "motivation": "Applying diffusion models to structured domains such as source code remains a significant challenge. Standard token-level corruption techniques used during training often ignore this structure, which may hinder the model's ability to learn meaningful representations of code.", "method": "a syntax-aware diffusion framework that incorporates structural priors from Abstract Syntax Trees (ASTs) into the denoising process. Instead of masking individual tokens at random, we selectively corrupt syntactically meaningful code spans derived from AST subtrees.", "result": "syntax-aware corruption significantly improves syntactic correctness, reconstruction accuracy, and generalization to unseen code patterns.", "conclusion": "syntax-aware corruption significantly improves syntactic correctness, reconstruction accuracy, and generalization to unseen code patterns. These findings highlight the potential of incorporating structural information into diffusion-based training and suggest that syntax-guided denoising is a promising direction for advancing diffusion-based language models in code generation tasks."}}
{"id": "2508.01139", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.01139", "abs": "https://arxiv.org/abs/2508.01139", "authors": ["Huyu Wu", "Duo Su", "Junjie Hou", "Guang Li"], "title": "Dataset Condensation with Color Compensation", "comment": null, "summary": "Dataset condensation always faces a constitutive trade-off: balancing\nperformance and fidelity under extreme compression. Existing methods struggle\nwith two bottlenecks: image-level selection methods (Coreset Selection, Dataset\nQuantization) suffer from inefficiency condensation, while pixel-level\noptimization (Dataset Distillation) introduces semantic distortion due to\nover-parameterization. With empirical observations, we find that a critical\nproblem in dataset condensation is the oversight of color's dual role as an\ninformation carrier and a basic semantic representation unit. We argue that\nimproving the colorfulness of condensed images is beneficial for representation\nlearning. Motivated by this, we propose DC3: a Dataset Condensation framework\nwith Color Compensation. After a calibrated selection strategy, DC3 utilizes\nthe latent diffusion model to enhance the color diversity of an image rather\nthan creating a brand-new one. Extensive experiments demonstrate the superior\nperformance and generalization of DC3 that outperforms SOTA methods across\nmultiple benchmarks. To the best of our knowledge, besides focusing on\ndownstream tasks, DC3 is the first research to fine-tune pre-trained diffusion\nmodels with condensed datasets. The FID results prove that training networks\nwith our high-quality datasets is feasible without model collapse or other\ndegradation issues. Code and generated data will be released soon.", "AI": {"tldr": "DC3 \u662f\u4e00\u79cd\u5e26\u6709\u989c\u8272\u8865\u507f\u7684\u6570\u636e\u96c6\u6d53\u7f29\u6846\u67b6\uff0c\u5b83\u5229\u7528\u6f5c\u5728\u7684\u6269\u6563\u6a21\u578b\u6765\u589e\u5f3a\u56fe\u50cf\u7684\u989c\u8272\u591a\u6837\u6027\uff0c\u4ece\u800c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e SOTA \u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u74f6\u9888\uff1a\u56fe\u50cf\u7ea7\u9009\u62e9\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\uff0c\u800c\u50cf\u7d20\u7ea7\u4f18\u5316\u7531\u4e8e\u8fc7\u5ea6\u53c2\u6570\u5316\u800c\u5f15\u5165\u8bed\u4e49\u5931\u771f\u3002\u6570\u636e\u96c6\u6d53\u7f29\u7684\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u662f\u5ffd\u89c6\u4e86\u989c\u8272\u4f5c\u4e3a\u4fe1\u606f\u8f7d\u4f53\u548c\u57fa\u672c\u8bed\u4e49\u8868\u793a\u5355\u5143\u7684\u53cc\u91cd\u4f5c\u7528\u3002\u6539\u5584\u6d53\u7f29\u56fe\u50cf\u7684\u8272\u5f69\u5bf9\u4e8e\u8868\u793a\u5b66\u4e60\u662f\u6709\u76ca\u7684\u3002", "method": "DC3\uff1a\u4e00\u79cd\u5e26\u6709\u989c\u8272\u8865\u507f\u7684\u6570\u636e\u96c6\u6d53\u7f29\u6846\u67b6\u3002\u5728\u6821\u51c6\u7684\u9009\u62e9\u7b56\u7565\u4e4b\u540e\uff0cDC3 \u5229\u7528\u6f5c\u5728\u7684\u6269\u6563\u6a21\u578b\u6765\u589e\u5f3a\u56fe\u50cf\u7684\u989c\u8272\u591a\u6837\u6027\uff0c\u800c\u4e0d\u662f\u521b\u5efa\u4e00\u4e2a\u5168\u65b0\u7684\u56fe\u50cf\u3002", "result": "DC3 \u4f18\u4e8e SOTA \u65b9\u6cd5\uff0c\u5177\u6709\u5353\u8d8a\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002FID \u7ed3\u679c\u8bc1\u660e\uff0c\u4f7f\u7528\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u662f\u53ef\u884c\u7684\uff0c\u4e0d\u4f1a\u51fa\u73b0\u6a21\u578b\u5d29\u6e83\u6216\u5176\u4ed6\u9000\u5316\u95ee\u9898\u3002", "conclusion": "DC3\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e SOTA \u65b9\u6cd5\uff0c\u5177\u6709\u5353\u8d8a\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002\u4f7f\u7528\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u662f\u53ef\u884c\u7684\uff0c\u4e0d\u4f1a\u51fa\u73b0\u6a21\u578b\u5d29\u6e83\u6216\u5176\u4ed6\u9000\u5316\u95ee\u9898\u3002"}}
{"id": "2508.01285", "categories": ["cs.AI", "cs.ET", "cs.IR", "stat.AP"], "pdf": "https://arxiv.org/pdf/2508.01285", "abs": "https://arxiv.org/abs/2508.01285", "authors": ["Yujing Ke", "Kevin George", "Kathan Pandya", "David Blumenthal", "Maximilian Sprang", "Gerrit Gro\u00dfmann", "Sebastian Vollmer", "David Antony Selby"], "title": "BioDisco: Multi-agent hypothesis generation with dual-mode evidence, iterative feedback and temporal evaluation", "comment": "7 pages main content + 11 pages appendices", "summary": "Identifying novel hypotheses is essential to scientific research, yet this\nprocess risks being overwhelmed by the sheer volume and complexity of available\ninformation. Existing automated methods often struggle to generate novel and\nevidence-grounded hypotheses, lack robust iterative refinement and rarely\nundergo rigorous temporal evaluation for future discovery potential. To address\nthis, we propose BioDisco, a multi-agent framework that draws upon language\nmodel-based reasoning and a dual-mode evidence system (biomedical knowledge\ngraphs and automated literature retrieval) for grounded novelty, integrates an\ninternal scoring and feedback loop for iterative refinement, and validates\nperformance through pioneering temporal and human evaluations and a\nBradley-Terry paired comparison model to provide statistically-grounded\nassessment. Our evaluations demonstrate superior novelty and significance over\nablated configurations representative of existing agentic architectures.\nDesigned for flexibility and modularity, BioDisco allows seamless integration\nof custom language models or knowledge graphs, and can be run with just a few\nlines of code. We anticipate researchers using this practical tool as a\ncatalyst for the discovery of new hypotheses.", "AI": {"tldr": "BioDisco, a new multi-agent framework, helps researchers discover novel hypotheses by using language models, knowledge graphs, and literature retrieval. It outperforms existing methods and is easy to use.", "motivation": "Identifying novel hypotheses is essential to scientific research, yet this process risks being overwhelmed by the sheer volume and complexity of available information. Existing automated methods often struggle to generate novel and evidence-grounded hypotheses, lack robust iterative refinement and rarely undergo rigorous temporal evaluation for future discovery potential.", "method": "a multi-agent framework that draws upon language model-based reasoning and a dual-mode evidence system (biomedical knowledge graphs and automated literature retrieval)", "result": "demonstrates superior novelty and significance over ablated configurations representative of existing agentic architectures. Designed for flexibility and modularity, BioDisco allows seamless integration of custom language models or knowledge graphs, and can be run with just a few lines of code.", "conclusion": "The BioDisco framework demonstrates superior novelty and significance over existing agentic architectures and is anticipated to be a catalyst for the discovery of new hypotheses."}}
{"id": "2508.00954", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00954", "abs": "https://arxiv.org/abs/2508.00954", "authors": ["Andy Hu", "Devika Prasad", "Luiz Pizzato", "Nicholas Foord", "Arman Abrahamyan", "Anna Leontjeva", "Cooper Doyle", "Dan Jermyn"], "title": "FeatureCuts: Feature Selection for Large Data by Optimizing the Cutoff", "comment": "11 pages, 4 figures, appendix", "summary": "In machine learning, the process of feature selection involves finding a\nreduced subset of features that captures most of the information required to\ntrain an accurate and efficient model. This work presents FeatureCuts, a novel\nfeature selection algorithm that adaptively selects the optimal feature cutoff\nafter performing filter ranking. Evaluated on 14 publicly available datasets\nand one industry dataset, FeatureCuts achieved, on average, 15 percentage\npoints more feature reduction and up to 99.6% less computation time while\nmaintaining model performance, compared to existing state-of-the-art methods.\nWhen the selected features are used in a wrapper method such as Particle Swarm\nOptimization (PSO), it enables 25 percentage points more feature reduction,\nrequires 66% less computation time, and maintains model performance when\ncompared to PSO alone. The minimal overhead of FeatureCuts makes it scalable\nfor large datasets typically seen in enterprise applications.", "AI": {"tldr": "FeatureCuts is a feature selection algorithm that adaptively selects the optimal feature cutoff after performing filter ranking. It achieves better feature reduction and computation time while maintaining model performance.", "motivation": "finding a reduced subset of features that captures most of the information required to train an accurate and efficient model", "method": "a novel feature selection algorithm that adaptively selects the optimal feature cutoff after performing filter ranking", "result": "achieved, on average, 15 percentage points more feature reduction and up to 99.6% less computation time while maintaining model performance, compared to existing state-of-the-art methods", "conclusion": "FeatureCuts achieves better feature reduction and computation time while maintaining model performance."}}
{"id": "2508.01480", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01480", "abs": "https://arxiv.org/abs/2508.01480", "authors": ["Dimitra Panou", "Alexandros C. Dimopoulos", "Manolis Koubarakis", "Martin Reczko"], "title": "Harnessing Collective Intelligence of LLMs for Robust Biomedical QA: A Multi-Model Approach", "comment": null, "summary": "Biomedical text mining and question-answering are essential yet highly\ndemanding tasks, particularly in the face of the exponential growth of\nbiomedical literature. In this work, we present our participation in the 13th\nedition of the BioASQ challenge, which involves biomedical semantic\nquestion-answering for Task 13b and biomedical question-answering for\ndeveloping topics for the Synergy task. We deploy a selection of open-source\nlarge language models (LLMs) as retrieval-augmented generators to answer\nbiomedical questions. Various models are used to process the questions. A\nmajority voting system combines their output to determine the final answer for\nYes/No questions, while for list and factoid type questions, the union of their\nanswers in used. We evaluated 13 state-of-the-art open source LLMs, exploring\nall possible model combinations to contribute to the final answer, resulting in\ntailored LLM pipelines for each question type. Our findings provide valuable\ninsight into which combinations of LLMs consistently produce superior results\nfor specific question types. In the four rounds of the 2025 BioASQ challenge,\nour system achieved notable results: in the Synergy task, we secured 1st place\nfor ideal answers and 2nd place for exact answers in round 2, as well as two\nshared 1st places for exact answers in round 3 and 4.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u53c2\u4e0eBioASQ\u6311\u6218\u8d5b\u7684\u5de5\u4f5c\uff0c\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u56de\u7b54\u751f\u7269\u533b\u5b66\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u6a21\u578b\u7ec4\u5408\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6210\u679c\u3002", "motivation": "\u751f\u7269\u533b\u5b66\u6587\u672c\u6316\u6398\u548c\u95ee\u7b54\u81f3\u5173\u91cd\u8981\u4f46\u8981\u6c42\u5f88\u9ad8\uff0c\u7279\u522b\u662f\u5728\u751f\u7269\u533b\u5b66\u6587\u732e\u5448\u6307\u6570\u589e\u957f\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u4f7f\u7528\u5404\u79cd\u6a21\u578b\u5904\u7406\u95ee\u9898\u3002\u591a\u6570\u6295\u7968\u7cfb\u7edf\u7ed3\u5408\u5b83\u4eec\u7684\u8f93\u51fa\u6765\u786e\u5b9aYes/No\u95ee\u9898\u7684\u6700\u7ec8\u7b54\u6848\uff0c\u800c\u5bf9\u4e8e\u5217\u8868\u548c\u7c7b\u4e8b\u5b9e\u578b\u95ee\u9898\uff0c\u4f7f\u7528\u5b83\u4eec\u7684\u7b54\u6848\u7684\u5e76\u96c6\u3002", "result": "\u6211\u4eec\u8bc4\u4f30\u4e8613\u4e2a\u6700\u5148\u8fdb\u7684\u5f00\u6e90LLM\uff0c\u63a2\u7d22\u6240\u6709\u53ef\u80fd\u7684\u6a21\u578b\u7ec4\u5408\u4ee5\u8d21\u732e\u6700\u7ec8\u7b54\u6848\uff0c\u4ece\u800c\u4e3a\u6bcf\u79cd\u95ee\u9898\u7c7b\u578b\u5b9a\u5236LLM\u7ba1\u9053\u3002\u6211\u4eec\u7684\u53d1\u73b0\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u5373\u54ea\u4e9bLLM\u7ec4\u5408\u59cb\u7ec8\u80fd\u4e3a\u7279\u5b9a\u95ee\u9898\u7c7b\u578b\u4ea7\u751f\u4f18\u5f02\u7684\u7ed3\u679c\u3002", "conclusion": "\u57282025 BioASQ\u6311\u6218\u8d5b\u7684\u56db\u4e2a\u56de\u5408\u4e2d\uff0c\u8be5\u7cfb\u7edf\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\uff1a\u5728\u534f\u540c\u4efb\u52a1\u4e2d\uff0c\u5728\u7b2c\u4e8c\u8f6e\u7684\u7406\u60f3\u7b54\u6848\u4e2d\u83b7\u5f97\u7b2c\u4e00\u540d\uff0c\u5728\u7cbe\u786e\u7b54\u6848\u4e2d\u83b7\u5f97\u7b2c\u4e8c\u540d\uff0c\u4ee5\u53ca\u5728\u7b2c\u4e09\u8f6e\u548c\u7b2c\u56db\u8f6e\u7684\u7cbe\u786e\u7b54\u6848\u4e2d\u83b7\u5f97\u4e24\u4e2a\u5e76\u5217\u7b2c\u4e00\u540d\u3002"}}
{"id": "2508.01150", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01150", "abs": "https://arxiv.org/abs/2508.01150", "authors": ["Dianyi Yang", "Xihan Wang", "Yu Gao", "Shiyang Liu", "Bohan Ren", "Yufeng Yue", "Yi Yang"], "title": "OpenGS-Fusion: Open-Vocabulary Dense Mapping with Hybrid 3D Gaussian Splatting for Refined Object-Level Understanding", "comment": "IROS2025", "summary": "Recent advancements in 3D scene understanding have made significant strides\nin enabling interaction with scenes using open-vocabulary queries, particularly\nfor VR/AR and robotic applications. Nevertheless, existing methods are hindered\nby rigid offline pipelines and the inability to provide precise 3D object-level\nunderstanding given open-ended queries. In this paper, we present\nOpenGS-Fusion, an innovative open-vocabulary dense mapping framework that\nimproves semantic modeling and refines object-level understanding.\nOpenGS-Fusion combines 3D Gaussian representation with a Truncated Signed\nDistance Field to facilitate lossless fusion of semantic features on-the-fly.\nFurthermore, we introduce a novel multimodal language-guided approach named\nMLLM-Assisted Adaptive Thresholding, which refines the segmentation of 3D\nobjects by adaptively adjusting similarity thresholds, achieving an improvement\n17\\% in 3D mIoU compared to the fixed threshold strategy. Extensive experiments\ndemonstrate that our method outperforms existing methods in 3D object\nunderstanding and scene reconstruction quality, as well as showcasing its\neffectiveness in language-guided scene interaction. The code is available at\nhttps://young-bit.github.io/opengs-fusion.github.io/ .", "AI": {"tldr": "OpenGS-Fusion, an innovative open-vocabulary dense mapping framework that improves semantic modeling and refines object-level understanding.", "motivation": "Existing methods are hindered by rigid offline pipelines and the inability to provide precise 3D object-level understanding given open-ended queries.", "method": "OpenGS-Fusion combines 3D Gaussian representation with a Truncated Signed Distance Field to facilitate lossless fusion of semantic features on-the-fly. Furthermore, the authors introduce a novel multimodal language-guided approach named MLLM-Assisted Adaptive Thresholding, which refines the segmentation of 3D objects by adaptively adjusting similarity thresholds", "result": "achieving an improvement 17% in 3D mIoU compared to the fixed threshold strategy.", "conclusion": "The method outperforms existing methods in 3D object understanding and scene reconstruction quality, as well as showcasing its effectiveness in language-guided scene interaction."}}
{"id": "2508.01300", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01300", "abs": "https://arxiv.org/abs/2508.01300", "authors": ["Ma'ayan Armony", "Albert Mero\u00f1o-Pe\u00f1uela", "Gerard Canal"], "title": "How Far Are LLMs from Symbolic Planners? An NLP-Based Perspective", "comment": null, "summary": "The reasoning and planning abilities of Large Language Models (LLMs) have\nbeen a frequent topic of discussion in recent years. Their ability to take\nunstructured planning problems as input has made LLMs' integration into AI\nplanning an area of interest. Nevertheless, LLMs are still not reliable as\nplanners, with the generated plans often containing mistaken or hallucinated\nactions. Existing benchmarking and evaluation methods investigate planning with\nLLMs, focusing primarily on success rate as a quality indicator in various\nplanning tasks, such as validating plans or planning in relaxed conditions. In\nthis paper, we approach planning with LLMs as a natural language processing\n(NLP) task, given that LLMs are NLP models themselves. We propose a recovery\npipeline consisting of an NLP-based evaluation of the generated plans, along\nwith three stages to recover the plans through NLP manipulation of the\nLLM-generated plans, and eventually complete the plan using a symbolic planner.\nThis pipeline provides a holistic analysis of LLM capabilities in the context\nof AI task planning, enabling a broader understanding of the quality of invalid\nplans. Our findings reveal no clear evidence of underlying reasoning during\nplan generation, and that a pipeline comprising an NLP-based analysis of the\nplans, followed by a recovery mechanism, still falls short of the quality and\nreliability of classical planners. On average, only the first 2.65 actions of\nthe plan are executable, with the average length of symbolically generated\nplans being 8.4 actions. The pipeline still improves action quality and\nincreases the overall success rate from 21.9% to 27.5%.", "AI": {"tldr": "LLMs are unreliable planners. An NLP-based recovery pipeline improves action quality and success rate, but still falls short of classical planners.", "motivation": "LLMs are still not reliable as planners, with the generated plans often containing mistaken or hallucinated actions.", "method": "We propose a recovery pipeline consisting of an NLP-based evaluation of the generated plans, along with three stages to recover the plans through NLP manipulation of the LLM-generated plans, and eventually complete the plan using a symbolic planner.", "result": "Our findings reveal no clear evidence of underlying reasoning during plan generation, and that a pipeline comprising an NLP-based analysis of the plans, followed by a recovery mechanism, still falls short of the quality and reliability of classical planners. On average, only the first 2.65 actions of the plan are executable, with the average length of symbolically generated plans being 8.4 actions.", "conclusion": "The pipeline still improves action quality and increases the overall success rate from 21.9% to 27.5%."}}
{"id": "2508.01486", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01486", "abs": "https://arxiv.org/abs/2508.01486", "authors": ["Vallabhaneni Raj Kumar", "Ashwin S", "Supriya Manna", "Niladri Sett", "Cheedella V S N M S Hema Harshitha", "Kurakula Harshitha", "Anand Kumar Sharma", "Basina Deepakraj", "Tanuj Sarkar", "Bondada Navaneeth Krishna", "Samanthapudi Shakeer"], "title": "TeSent: A Benchmark Dataset for Fairness-aware Explainable Sentiment Classification in Telugu", "comment": "work under review", "summary": "In the Indian subcontinent, Telugu, one of India's six classical languages,\nis the most widely spoken Dravidian Language. Despite its 96 million speaker\nbase worldwide, Telugu remains underrepresented in the global NLP and Machine\nLearning landscape, mainly due to lack of high-quality annotated resources.\nThis work introduces TeSent, a comprehensive benchmark dataset for sentiment\nclassification, a key text classification problem, in Telugu. TeSent not only\nprovides ground truth labels for the sentences, but also supplements with\nprovisions for evaluating explainability and fairness, two critical\nrequirements in modern-day machine learning tasks. We scraped Telugu texts\ncovering multiple domains from various social media platforms, news websites\nand web-blogs to preprocess and generate 26,150 sentences, and developed a\ncustom-built annotation platform and a carefully crafted annotation protocol\nfor collecting the ground truth labels along with their human-annotated\nrationales. We then fine-tuned several SOTA pre-trained models in two ways:\nwith rationales, and without rationales. Further, we provide a detailed\nplausibility and faithfulness evaluation suite, which exploits the rationales,\nfor six widely used post-hoc explainers applied on the trained models. Lastly,\nwe curate TeEEC, Equity Evaluation Corpus in Telugu, a corpus to evaluate\nfairness of Telugu sentiment and emotion related NLP tasks, and provide a\nfairness evaluation suite for the trained classifier models. Our experimental\nresults suggest that training with rationales may improve model accuracy,\nreduce bias in models, and make the explainers' output more aligned to human\nreasoning.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86 TeSent\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u6cf0\u5362\u56fa\u8bed\u60c5\u611f\u5206\u7c7b\u7684\u7efc\u5408\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u4f9b\u4e86\u7528\u4e8e\u8bc4\u4f30\u53ef\u89e3\u91ca\u6027\u548c\u516c\u5e73\u6027\u7684\u5de5\u5177\u3002", "motivation": "\u6cf0\u5362\u56fa\u8bed\u662f\u5370\u5ea6\u4f7f\u7528\u6700\u5e7f\u6cdb\u7684\u5fb7\u62c9\u5a01\u8bed\uff0c\u4f46\u5728\u5168\u7403 NLP \u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u4ecd\u7136\u6ca1\u6709\u5f97\u5230\u5145\u5206\u7684\u4ee3\u8868\u6027\uff0c\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u6ce8\u91ca\u8d44\u6e90\u3002", "method": "\u4ece\u5404\u79cd\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u3001\u65b0\u95fb\u7f51\u7ad9\u548c\u7f51\u7edc\u535a\u5ba2\u4e2d\u6293\u53d6\u6cf0\u5362\u56fa\u8bed\u6587\u672c\uff0c\u6db5\u76d6\u591a\u4e2a\u9886\u57df\uff0c\u4ee5\u9884\u5904\u7406\u5e76\u751f\u6210 26,150 \u4e2a\u53e5\u5b50\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b9a\u5236\u7684\u6ce8\u91ca\u5e73\u53f0\u548c\u4e00\u4e2a\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6ce8\u91ca\u534f\u8bae\uff0c\u7528\u4e8e\u6536\u96c6\u57fa\u672c\u4e8b\u5b9e\u6807\u7b7e\u53ca\u5176\u4eba\u5de5\u6ce8\u91ca\u7684\u7406\u7531\u3002", "result": "\u6211\u4eec\u5f15\u5165\u4e86 TeSent\uff0c\u8fd9\u662f\u4e00\u4e2a\u5168\u9762\u7684\u60c5\u611f\u5206\u7c7b\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u60c5\u611f\u5206\u7c7b\u662f\u6cf0\u5362\u56fa\u8bed\u4e2d\u7684\u4e00\u4e2a\u5173\u952e\u6587\u672c\u5206\u7c7b\u95ee\u9898\u3002\u6211\u4eec\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u5408\u7406\u6027\u548c\u5fe0\u5b9e\u6027\u8bc4\u4f30\u5957\u4ef6\uff0c\u8be5\u5957\u4ef6\u5229\u7528\u4e86\u7406\u7531\uff0c\u9002\u7528\u4e8e\u5e94\u7528\u4e8e\u8bad\u7ec3\u6a21\u578b\u7684\u516d\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u4e8b\u540e\u89e3\u91ca\u5668\u3002\u6700\u540e\uff0c\u6211\u4eec\u6574\u7406\u4e86\u6cf0\u5362\u56fa\u8bed\u7684 TeEEC\uff08\u516c\u5e73\u6027\u8bc4\u4f30\u8bed\u6599\u5e93\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u6cf0\u5362\u56fa\u8bed\u60c5\u611f\u548c\u60c5\u611f\u76f8\u5173 NLP \u4efb\u52a1\u7684\u516c\u5e73\u6027\u7684\u8bed\u6599\u5e93\uff0c\u5e76\u4e3a\u8bad\u7ec3\u7684\u5206\u7c7b\u5668\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u516c\u5e73\u6027\u8bc4\u4f30\u5957\u4ef6\u3002", "conclusion": "\u8bad\u7ec3\u4e0e\u7406\u7531\u53ef\u80fd\u63d0\u9ad8\u6a21\u578b\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u6a21\u578b\u504f\u5dee\uff0c\u5e76\u4f7f\u89e3\u91ca\u5668\u7684\u8f93\u51fa\u66f4\u7b26\u5408\u4eba\u7c7b\u63a8\u7406\u3002"}}
{"id": "2508.01151", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01151", "abs": "https://arxiv.org/abs/2508.01151", "authors": ["Yu Lei", "Jinbin Bai", "Qingyu Shi", "Aosong Feng", "Kaidong Yu"], "title": "Personalized Safety Alignment for Text-to-Image Diffusion Models", "comment": "14 pages, 8 figures, 4 tables", "summary": "Text-to-image diffusion models have revolutionized visual content generation,\nbut current safety mechanisms apply uniform standards that often fail to\naccount for individual user preferences. These models overlook the diverse\nsafety boundaries shaped by factors like age, mental health, and personal\nbeliefs. To address this, we propose Personalized Safety Alignment (PSA), a\nframework that allows user-specific control over safety behaviors in generative\nmodels. PSA integrates personalized user profiles into the diffusion process,\nadjusting the model's behavior to match individual safety preferences while\npreserving image quality. We introduce a new dataset, Sage, which captures\nuser-specific safety preferences and incorporates these profiles through a\ncross-attention mechanism. Experiments show that PSA outperforms existing\nmethods in harmful content suppression and aligns generated content better with\nuser constraints, achieving higher Win Rate and Pass Rate scores. Our code,\ndata, and models are publicly available at\nhttps://torpedo2648.github.io/PSAlign/.", "AI": {"tldr": "Personalized Safety Alignment (PSA) allows user-specific control over safety behaviors in generative models.", "motivation": "Current safety mechanisms apply uniform standards that often fail to account for individual user preferences, overlooking the diverse safety boundaries shaped by factors like age, mental health, and personal beliefs.", "method": "Personalized Safety Alignment (PSA), a framework that allows user-specific control over safety behaviors in generative models. PSA integrates personalized user profiles into the diffusion process, adjusting the model's behavior to match individual safety preferences while preserving image quality. A new dataset, Sage, captures user-specific safety preferences and incorporates these profiles through a cross-attention mechanism.", "result": "PSA outperforms existing methods in harmful content suppression and aligns generated content better with user constraints, achieving higher Win Rate and Pass Rate scores.", "conclusion": "Personalized Safety Alignment (PSA) can better align generated content with user constraints, achieving higher Win Rate and Pass Rate scores."}}
{"id": "2508.01306", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.01306", "abs": "https://arxiv.org/abs/2508.01306", "authors": ["Yelim Ahn", "Jaejin Lee"], "title": "PUZZLED: Jailbreaking LLMs through Word-Based Puzzles", "comment": "15 pages", "summary": "As large language models (LLMs) are increasingly deployed across diverse\ndomains, ensuring their safety has become a critical concern. In response,\nstudies on jailbreak attacks have been actively growing. Existing approaches\ntypically rely on iterative prompt engineering or semantic transformations of\nharmful instructions to evade detection. In this work, we introduce PUZZLED, a\nnovel jailbreak method that leverages the LLM's reasoning capabilities. It\nmasks keywords in a harmful instruction and presents them as word puzzles for\nthe LLM to solve. We design three puzzle types-word search, anagram, and\ncrossword-that are familiar to humans but cognitively demanding for LLMs. The\nmodel must solve the puzzle to uncover the masked words and then proceed to\ngenerate responses to the reconstructed harmful instruction. We evaluate\nPUZZLED on five state-of-the-art LLMs and observe a high average attack success\nrate (ASR) of 88.8%, specifically 96.5% on GPT-4.1 and 92.3% on Claude 3.7\nSonnet. PUZZLED is a simple yet powerful attack that transforms familiar\npuzzles into an effective jailbreak strategy by harnessing LLMs' reasoning\ncapabilities.", "AI": {"tldr": "PUZZLED\u662f\u4e00\u79cd\u65b0\u7684\u8d8a\u72f1\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u6cd5\u5b66\u7855\u58eb\u7684\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u5c06\u6709\u5bb3\u6307\u4ee4\u4e2d\u7684\u5173\u952e\u8bcd\u5c4f\u853d\u4e3a\u5355\u8bcd\u8c1c\u9898\u6765\u9003\u907f\u68c0\u6d4b\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8d8a\u6765\u8d8a\u591a\u5730\u90e8\u7f72\u5230\u4e0d\u540c\u7684\u9886\u57df\uff0c\u786e\u4fdd\u5176\u5b89\u5168\u6027\u5df2\u6210\u4e3a\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002\u4f5c\u4e3a\u56de\u5e94\uff0c\u5bf9\u8d8a\u72f1\u653b\u51fb\u7684\u7814\u7a76\u4e00\u76f4\u5728\u79ef\u6781\u53d1\u5c55\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u8fed\u4ee3\u63d0\u793a\u5de5\u7a0b\u6216\u6709\u5bb3\u6307\u4ee4\u7684\u8bed\u4e49\u8f6c\u6362\u6765\u9003\u907f\u68c0\u6d4b\u3002", "method": "PUZZLED\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u8d8a\u72f1\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u6cd5\u5b66\u7855\u58eb\u7684\u63a8\u7406\u80fd\u529b\u3002\u5b83\u5c4f\u853d\u6709\u5bb3\u6307\u4ee4\u4e2d\u7684\u5173\u952e\u8bcd\uff0c\u5e76\u5c06\u5b83\u4eec\u4f5c\u4e3a\u5355\u8bcd\u8c1c\u9898\u5448\u73b0\u7ed9\u6cd5\u5b66\u7855\u58eb\u6765\u89e3\u51b3\u3002\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e09\u79cd\u8c1c\u9898\u7c7b\u578b\u2014\u2014\u5355\u8bcd\u641c\u7d22\u3001\u5b57\u8c1c\u548c\u586b\u5b57\u6e38\u620f\u2014\u2014\u8fd9\u4e9b\u8c1c\u9898\u5bf9\u4eba\u7c7b\u6765\u8bf4\u5f88\u719f\u6089\uff0c\u4f46\u5bf9\u6cd5\u5b66\u7855\u58eb\u6765\u8bf4\u5728\u8ba4\u77e5\u4e0a\u8981\u6c42\u5f88\u9ad8\u3002\u6a21\u578b\u5fc5\u987b\u89e3\u51b3\u96be\u9898\u624d\u80fd\u53d1\u73b0\u88ab\u5c4f\u853d\u7684\u5355\u8bcd\uff0c\u7136\u540e\u7ee7\u7eed\u751f\u6210\u5bf9\u91cd\u5efa\u7684\u6709\u5bb3\u6307\u4ee4\u7684\u54cd\u5e94\u3002", "result": "\u6211\u4eec\u5728\u4e94\u4e2a\u6700\u5148\u8fdb\u7684\u6cd5\u5b66\u7855\u58eb\u4e0a\u8bc4\u4f30\u4e86PUZZLED\uff0c\u5e76\u89c2\u5bdf\u523088.8%\u7684\u5e73\u5747\u653b\u51fb\u6210\u529f\u7387\uff08ASR\uff09\uff0c\u7279\u522b\u662fGPT-4.1\u4e0a\u768496.5%\u548cClaude 3.7 Sonnet\u4e0a\u768492.3%\u3002", "conclusion": "PUZZLED\u662f\u4e00\u79cd\u7b80\u5355\u4f46\u529f\u80fd\u5f3a\u5927\u7684\u653b\u51fb\uff0c\u5b83\u901a\u8fc7\u5229\u7528\u6cd5\u5b66\u7855\u58eb\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5c06\u719f\u6089\u7684\u8c1c\u9898\u8f6c\u5316\u4e3a\u6709\u6548\u7684\u8d8a\u72f1\u7b56\u7565\u3002"}}
{"id": "2508.01491", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01491", "abs": "https://arxiv.org/abs/2508.01491", "authors": ["Zhivar Sourati", "Alireza S. Ziabari", "Morteza Dehghani"], "title": "The Homogenizing Effect of Large Language Models on Human Expression and Thought", "comment": null, "summary": "Cognitive diversity, reflected in variations of language, perspective, and\nreasoning, is essential to creativity and collective intelligence. This\ndiversity is rich and grounded in culture, history, and individual experience.\nYet as large language models (LLMs) become deeply embedded in people's lives,\nthey risk standardizing language and reasoning. This Review synthesizes\nevidence across linguistics, cognitive, and computer science to show how LLMs\nreflect and reinforce dominant styles while marginalizing alternative voices\nand reasoning strategies. We examine how their design and widespread use\ncontribute to this effect by mirroring patterns in their training data and\namplifying convergence as all people increasingly rely on the same models\nacross contexts. Unchecked, this homogenization risks flattening the cognitive\nlandscapes that drive collective intelligence and adaptability.", "AI": {"tldr": "LLMs threaten cognitive diversity by standardizing language and reasoning, potentially harming creativity and collective intelligence.", "motivation": "Cognitive diversity is essential to creativity and collective intelligence, but LLMs risk standardizing language and reasoning.", "method": "Review synthesizes evidence across linguistics, cognitive, and computer science.", "result": "LLMs reflect and reinforce dominant styles, marginalizing alternative voices and reasoning strategies due to their design and widespread use mirroring training data patterns and amplifying convergence.", "conclusion": "Unchecked homogenization risks flattening cognitive landscapes, hindering collective intelligence and adaptability."}}
{"id": "2508.01152", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01152", "abs": "https://arxiv.org/abs/2508.01152", "authors": ["Xinyu Yan", "Meijun Sun", "Ge-Peng Ji", "Fahad Shahbaz Khan", "Salman Khan", "Deng-Ping Fan"], "title": "LawDIS: Language-Window-based Controllable Dichotomous Image Segmentation", "comment": "17 pages, 10 figures, ICCV 2025", "summary": "We present LawDIS, a language-window-based controllable dichotomous image\nsegmentation (DIS) framework that produces high-quality object masks. Our\nframework recasts DIS as an image-conditioned mask generation task within a\nlatent diffusion model, enabling seamless integration of user controls. LawDIS\nis enhanced with macro-to-micro control modes. Specifically, in macro mode, we\nintroduce a language-controlled segmentation strategy (LS) to generate an\ninitial mask based on user-provided language prompts. In micro mode, a\nwindow-controlled refinement strategy (WR) allows flexible refinement of\nuser-defined regions (i.e., size-adjustable windows) within the initial mask.\nCoordinated by a mode switcher, these modes can operate independently or\njointly, making the framework well-suited for high-accuracy, personalised\napplications. Extensive experiments on the DIS5K benchmark reveal that our\nLawDIS significantly outperforms 11 cutting-edge methods across all metrics.\nNotably, compared to the second-best model MVANet, we achieve $F_\\beta^\\omega$\ngains of 4.6\\% with both the LS and WR strategies and 3.6\\% gains with only the\nLS strategy on DIS-TE. Codes will be made available at\nhttps://github.com/XinyuYanTJU/LawDIS.", "AI": {"tldr": "LawDIS\u662f\u4e00\u4e2a\u57fa\u4e8e\u8bed\u8a00\u7a97\u53e3\u7684\u53ef\u63a7\u4e8c\u5206\u56fe\u50cf\u5206\u5272\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u5b8f\u89c2\u5230\u5fae\u89c2\u7684\u63a7\u5236\u6a21\u5f0f\uff0c\u5728DIS5K\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u4ea7\u751f\u9ad8\u8d28\u91cf\u7684\u5bf9\u8c61\u63a9\u7801\uff0c\u5e76\u5b9e\u73b0\u7528\u6237\u63a7\u5236\u7684\u65e0\u7f1d\u96c6\u6210\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u8a00\u7a97\u53e3\u7684\u53ef\u63a7\u4e8c\u5206\u56fe\u50cf\u5206\u5272(DIS)\u6846\u67b6LawDIS\uff0c\u5b83\u5c06DIS\u91cd\u94f8\u4e3a\u6f5c\u5728\u6269\u6563\u6a21\u578b\u4e2d\u7684\u56fe\u50cf\u6761\u4ef6\u63a9\u7801\u751f\u6210\u4efb\u52a1\uff0c\u5e76\u96c6\u6210\u4e86\u5b8f\u89c2\u5230\u5fae\u89c2\u7684\u63a7\u5236\u6a21\u5f0f\u3002", "result": "LawDIS\u5728DIS5K\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5404\u9879\u6307\u6807\u5747\u663e\u8457\u4f18\u4e8e11\u79cd\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "LawDIS\u5728DIS5K\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728DIS-TE\u6570\u636e\u96c6\u4e0a\uff0c\u901a\u8fc7LS\u548cWR\u7b56\u7565\u7ed3\u5408\u4f7f\u7528\uff0c$F_\u03b2^\u03c9$\u6307\u6807\u63d0\u5347\u4e864.6%\uff0c\u5355\u72ec\u4f7f\u7528LS\u7b56\u7565\u63d0\u5347\u4e863.6%\u3002"}}
{"id": "2508.01323", "categories": ["cs.AI", "cs.CY", "econ.GN", "q-fin.EC", "47H10, 06B10, 91B40, 91B55, 68T20", "I.2.0; I.2.11; F.4.1"], "pdf": "https://arxiv.org/pdf/2508.01323", "abs": "https://arxiv.org/abs/2508.01323", "authors": ["Faruk Alpay", "Bugra Kilictas", "Taylan Alpay", "Hamdi Alakkad"], "title": "Idempotent Equilibrium Analysis of Hybrid Workflow Allocation: A Mathematical Schema for Future Work", "comment": "25 pages, 9 figures, 4 tables. Proves existence/uniqueness of an\n  \"idempotent equilibrium\" for human-AI task allocation and provides\n  closed-form steady-state automation share", "summary": "The rapid advance of large-scale AI systems is reshaping how work is divided\nbetween people and machines. We formalise this reallocation as an iterated\ntask-delegation map and show that--under broad, empirically grounded\nassumptions--the process converges to a stable idempotent equilibrium in which\nevery task is performed by the agent (human or machine) with enduring\ncomparative advantage. Leveraging lattice-theoretic fixed-point tools (Tarski\nand Banach), we (i) prove existence of at least one such equilibrium and (ii)\nderive mild monotonicity conditions that guarantee uniqueness. In a stylised\ncontinuous model the long-run automated share takes the closed form $x^* =\n\\alpha / (\\alpha + \\beta)$, where $\\alpha$ captures the pace of automation and\n$\\beta$ the rate at which new, human-centric tasks appear; hence full\nautomation is precluded whenever $\\beta > 0$. We embed this analytic result in\nthree complementary dynamical benchmarks--a discrete linear update, an\nevolutionary replicator dynamic, and a continuous Beta-distributed task\nspectrum--each of which converges to the same mixed equilibrium and is\nreproducible from the provided code-free formulas. A 2025-to-2045 simulation\ncalibrated to current adoption rates projects automation rising from\napproximately 10% of work to approximately 65%, leaving a persistent one-third\nof tasks to humans. We interpret that residual as a new profession of workflow\nconductor: humans specialise in assigning, supervising and integrating AI\nmodules rather than competing with them. Finally, we discuss implications for\nskill development, benchmark design and AI governance, arguing that policies\nwhich promote \"centaur\" human-AI teaming can steer the economy toward the\nwelfare-maximising fixed point.", "AI": {"tldr": "AI automation will rise to 65% by 2045, with humans specializing in AI workflow management.", "motivation": "The rapid advance of large-scale AI systems is reshaping how work is divided between people and machines.", "method": "Leveraging lattice-theoretic fixed-point tools (Tarski and Banach).", "result": "Automation is projected to rise from approximately 10% of work to approximately 65%, leaving a persistent one-third of tasks to humans.", "conclusion": "Policies promoting human-AI teaming can steer the economy toward the welfare-maximizing fixed point."}}
{"id": "2508.00957", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00957", "abs": "https://arxiv.org/abs/2508.00957", "authors": ["Amrit Rajeev", "Udayaadithya Avadhanam", "Harshula Tulapurkar", "SaiBarath Sundar"], "title": "Small sample-based adaptive text classification through iterative and contrastive description refinement", "comment": null, "summary": "Zero-shot text classification remains a difficult task in domains with\nevolving knowledge and ambiguous category boundaries, such as ticketing\nsystems. Large language models (LLMs) often struggle to generalize in these\nscenarios due to limited topic separability, while few-shot methods are\nconstrained by insufficient data diversity. We propose a classification\nframework that combines iterative topic refinement, contrastive prompting, and\nactive learning. Starting with a small set of labeled samples, the model\ngenerates initial topic labels. Misclassified or ambiguous samples are then\nused in an iterative contrastive prompting process to refine category\ndistinctions by explicitly teaching the model to differentiate between closely\nrelated classes. The framework features a human-in-the-loop component, allowing\nusers to introduce or revise category definitions in natural language. This\nenables seamless integration of new, unseen categories without retraining,\nmaking the system well-suited for real-world, dynamic environments. The\nevaluations on AGNews and DBpedia demonstrate strong performance: 91% accuracy\non AGNews (3 seen, 1 unseen class) and 84% on DBpedia (8 seen, 1 unseen), with\nminimal accuracy shift after introducing unseen classes (82% and 87%,\nrespectively). The results highlight the effectiveness of prompt-based semantic\nreasoning for fine-grained classification with limited supervision.", "AI": {"tldr": "This paper proposes a classification framework that combines iterative topic refinement, contrastive prompting, and active learning to address zero-shot text classification in dynamic environments. The results highlight the effectiveness of prompt-based semantic reasoning for fine-grained classification with limited supervision.", "motivation": "Zero-shot text classification remains a difficult task in domains with evolving knowledge and ambiguous category boundaries, such as ticketing systems. Large language models (LLMs) often struggle to generalize in these scenarios due to limited topic separability, while few-shot methods are constrained by insufficient data diversity.", "method": "a classification framework that combines iterative topic refinement, contrastive prompting, and active learning", "result": "91% accuracy on AGNews (3 seen, 1 unseen class) and 84% on DBpedia (8 seen, 1 unseen), with minimal accuracy shift after introducing unseen classes (82% and 87%, respectively).", "conclusion": "prompt-based semantic reasoning is effective for fine-grained classification with limited supervision."}}
{"id": "2508.01987", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.01987", "abs": "https://arxiv.org/abs/2508.01987", "authors": ["Shutong Qiao", "Wei Yuan", "Junliang Yu", "Tong Chen", "Quoc Viet Hung Nguyen", "Hongzhi Yin"], "title": "Controllable and Stealthy Shilling Attacks via Dispersive Latent Diffusion", "comment": null, "summary": "Recommender systems (RSs) are now fundamental to various online platforms,\nbut their dependence on user-contributed data leaves them vulnerable to\nshilling attacks that can manipulate item rankings by injecting fake users.\nAlthough widely studied, most existing attack models fail to meet two critical\nobjectives simultaneously: achieving strong adversarial promotion of target\nitems while maintaining realistic behavior to evade detection. As a result, the\ntrue severity of shilling threats that manage to reconcile the two objectives\nremains underappreciated. To expose this overlooked vulnerability, we present\nDLDA, a diffusion-based attack framework that can generate highly effective yet\nindistinguishable fake users by enabling fine-grained control over target\npromotion. Specifically, DLDA operates in a pre-aligned collaborative embedding\nspace, where it employs a conditional latent diffusion process to iteratively\nsynthesize fake user profiles with precise target item control. To evade\ndetection, DLDA introduces a dispersive regularization mechanism that promotes\nvariability and realism in generated behavioral patterns. Extensive experiments\non three real-world datasets and five popular RS models demonstrate that,\ncompared to prior attacks, DLDA consistently achieves stronger item promotion\nwhile remaining harder to detect. These results highlight that modern RSs are\nmore vulnerable than previously recognized, underscoring the urgent need for\nmore robust defenses.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u653b\u51fb\u6846\u67b6\uff0c\u53ef\u4ee5\u751f\u6210\u66f4\u6709\u6548\u4f46\u96be\u4ee5\u533a\u5206\u7684\u865a\u5047\u7528\u6237\uff0c\u5b9e\u9a8c\u8868\u660e\u73b0\u4ee3\u63a8\u8350\u7cfb\u7edf\u6bd4\u4ee5\u524d\u8ba4\u4e3a\u7684\u66f4\u5bb9\u6613\u53d7\u5230\u653b\u51fb\u3002", "motivation": "\u63a8\u8350\u7cfb\u7edf\u4f9d\u8d56\u4e8e\u7528\u6237\u8d21\u732e\u7684\u6570\u636e\uff0c\u5bb9\u6613\u53d7\u5230\u6ce8\u5165\u865a\u5047\u7528\u6237\u4ee5\u64cd\u7eb5\u9879\u76ee\u6392\u540d\u7684\u6076\u610f\u653b\u51fb\u3002\u73b0\u6709\u653b\u51fb\u6a21\u578b\u65e0\u6cd5\u540c\u65f6\u5b9e\u73b0\u4e24\u4e2a\u5173\u952e\u76ee\u6807\uff1a\u5b9e\u73b0\u5bf9\u76ee\u6807\u9879\u76ee\u7684\u5f3a\u5927\u5bf9\u6297\u6027\u63a8\u5e7f\uff0c\u540c\u65f6\u4fdd\u6301\u903c\u771f\u7684\u884c\u4e3a\u4ee5\u907f\u514d\u88ab\u53d1\u73b0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u653b\u51fb\u6846\u67b6DLDA\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5bf9\u76ee\u6807\u9879\u76ee\u8fdb\u884c\u7ec6\u7c92\u5ea6\u63a7\u5236\u6765\u751f\u6210\u9ad8\u6548\u4f46\u96be\u4ee5\u533a\u5206\u7684\u865a\u5047\u7528\u6237\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u548c\u4e94\u4e2a\u6d41\u884c\u7684RS\u6a21\u578b\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4e4b\u524d\u7684\u653b\u51fb\u76f8\u6bd4\uff0cDLDA\u59cb\u7ec8\u80fd\u591f\u5b9e\u73b0\u66f4\u5f3a\u7684\u9879\u76ee\u63a8\u5e7f\uff0c\u540c\u65f6\u66f4\u96be\u88ab\u68c0\u6d4b\u5230\u3002", "conclusion": "\u73b0\u4ee3\u63a8\u8350\u7cfb\u7edf\u6bd4\u4ee5\u524d\u8ba4\u4e3a\u7684\u66f4\u5bb9\u6613\u53d7\u5230\u653b\u51fb\uff0c\u56e0\u6b64\u8feb\u5207\u9700\u8981\u66f4\u5f3a\u5927\u7684\u9632\u5fa1\u63aa\u65bd\u3002"}}
{"id": "2508.01503", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01503", "abs": "https://arxiv.org/abs/2508.01503", "authors": ["Clayton Cohn", "Surya Rayala", "Namrata Srivastava", "Joyce Horn Fonteles", "Shruti Jain", "Xinying Luo", "Divya Mereddy", "Naveeduddin Mohammed", "Gautam Biswas"], "title": "A Theory of Adaptive Scaffolding for LLM-Based Pedagogical Agents", "comment": null, "summary": "Large language models (LLMs) present new opportunities for creating\npedagogical agents that engage in meaningful dialogue to support student\nlearning. However, the current use of LLM systems like ChatGPT in classrooms\noften lacks the solid theoretical foundation found in earlier intelligent\ntutoring systems. To bridge this gap, we propose a framework that combines\nEvidence-Centered Design with Social Cognitive Theory for adaptive scaffolding\nin LLM-based agents focused on STEM+C learning. We illustrate this framework\nwith Inquizzitor, an LLM-based formative assessment agent that integrates\nhuman-AI hybrid intelligence and provides feedback grounded in cognitive\nscience principles. Our findings show that Inquizzitor delivers high-quality\nassessment and interaction aligned with core learning theories, offering\nteachers effective guidance that students value. This research underscores the\npotential for theory-driven LLM integration in education, highlighting the\nability of these systems to provide adaptive and principled instruction.", "AI": {"tldr": "This paper proposes a framework combining Evidence-Centered Design with Social Cognitive Theory for adaptive scaffolding in LLM-based agents, and illustrates it with Inquizzitor, an LLM-based formative assessment agent.", "motivation": "Current LLM systems in classrooms lack the theoretical foundation of earlier intelligent tutoring systems.", "method": "combining Evidence-Centered Design with Social Cognitive Theory", "result": "Inquizzitor delivers high-quality assessment and interaction aligned with core learning theories.", "conclusion": "LLMs can provide adaptive and principled instruction in education through theory-driven integration."}}
{"id": "2508.01153", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01153", "abs": "https://arxiv.org/abs/2508.01153", "authors": ["Xiahan Yang", "Hui Zheng"], "title": "TEACH: Text Encoding as Curriculum Hints for Scene Text Recognition", "comment": "9 pages (w/o ref), 5 figures, 7 tables", "summary": "Scene Text Recognition (STR) remains a challenging task due to complex visual\nappearances and limited semantic priors. We propose TEACH, a novel training\nparadigm that injects ground-truth text into the model as auxiliary input and\nprogressively reduces its influence during training. By encoding target labels\ninto the embedding space and applying loss-aware masking, TEACH simulates a\ncurriculum learning process that guides the model from label-dependent learning\nto fully visual recognition. Unlike language model-based approaches, TEACH\nrequires no external pretraining and introduces no inference overhead. It is\nmodel-agnostic and can be seamlessly integrated into existing encoder-decoder\nframeworks. Extensive experiments across multiple public benchmarks show that\nmodels trained with TEACH achieve consistently improved accuracy, especially\nunder challenging conditions, validating its robustness and general\napplicability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8bad\u7ec3\u8303\u5f0f TEACH\uff0c\u8be5\u8303\u5f0f\u5c06\u771f\u5b9e\u6587\u672c\u6ce8\u5165\u6a21\u578b\u4f5c\u4e3a\u8f85\u52a9\u8f93\u5165\uff0c\u5e76\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u9010\u6b65\u964d\u4f4e\u5176\u5f71\u54cd\uff0c\u4ece\u800c\u5f15\u5bfc\u6a21\u578b\u4ece\u4f9d\u8d56\u6807\u7b7e\u7684\u5b66\u4e60\u5230\u5b8c\u5168\u7684\u89c6\u89c9\u8bc6\u522b\u3002", "motivation": "\u7531\u4e8e\u590d\u6742\u7684\u89c6\u89c9\u5916\u89c2\u548c\u6709\u9650\u7684\u8bed\u4e49\u5148\u9a8c\uff0c\u573a\u666f\u6587\u672c\u8bc6\u522b (STR) \u4ecd\u7136\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002", "method": "\u5c06\u771f\u5b9e\u6587\u672c\u6ce8\u5165\u6a21\u578b\u4f5c\u4e3a\u8f85\u52a9\u8f93\u5165\uff0c\u5e76\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u9010\u6b65\u964d\u4f4e\u5176\u5f71\u54cd\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5171\u57fa\u51c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528 TEACH \u8bad\u7ec3\u7684\u6a21\u578b\u59cb\u7ec8\u80fd\u63d0\u9ad8\u51c6\u786e\u7387\u3002", "conclusion": "\u6a21\u578b\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6761\u4ef6\u4e0b\u59cb\u7ec8\u80fd\u63d0\u9ad8\u51c6\u786e\u7387\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2508.01324", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01324", "abs": "https://arxiv.org/abs/2508.01324", "authors": ["Ke Miao", "Yuke Hu", "Xiaochen Li", "Wenjie Bao", "Zhihao Liu", "Zhan Qin", "Kui Ren"], "title": "Towards Evaluation for Real-World LLM Unlearning", "comment": null, "summary": "This paper analyzes the limitations of existing unlearning evaluation metrics\nin terms of practicality, exactness, and robustness in real-world LLM\nunlearning scenarios. To overcome these limitations, we propose a new metric\ncalled Distribution Correction-based Unlearning Evaluation (DCUE). It\nidentifies core tokens and corrects distributional biases in their confidence\nscores using a validation set. The evaluation results are quantified using the\nKolmogorov-Smirnov test. Experimental results demonstrate that DCUE overcomes\nthe limitations of existing metrics, which also guides the design of more\npractical and reliable unlearning algorithms in the future.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6307\u6807DCUE\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u7684unlearning\u6548\u679c\uff0c\u8be5\u6307\u6807\u53ef\u4ee5\u514b\u670d\u73b0\u6709\u6307\u6807\u7684\u5c40\u9650\u6027\uff0c\u5e76\u6307\u5bfc\u672a\u6765unlearning\u7b97\u6cd5\u7684\u8bbe\u8ba1\u3002", "motivation": "\u5206\u6790\u4e86\u73b0\u6709unlearning\u8bc4\u4f30\u6307\u6807\u5728\u5b9e\u9645LLM unlearning\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u6027\u3001\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u57fa\u4e8e\u5206\u5e03\u6821\u6b63\u7684Unlearning\u8bc4\u4f30\uff08DCUE\uff09\u7684\u65b0\u6307\u6807\uff0c\u8be5\u6307\u6807\u8bc6\u522b\u6838\u5fc3token\u5e76\u4f7f\u7528\u9a8c\u8bc1\u96c6\u6821\u6b63\u5176\u7f6e\u4fe1\u5ea6\u5206\u6570\u4e2d\u7684\u5206\u5e03\u504f\u5dee\uff0c\u5e76\u4f7f\u7528Kolmogorov-Smirnov\u68c0\u9a8c\u91cf\u5316\u8bc4\u4f30\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDCUE\u514b\u670d\u4e86\u73b0\u6709\u6307\u6807\u7684\u5c40\u9650\u6027\u3002", "conclusion": "DCUE\u514b\u670d\u4e86\u73b0\u6709\u6307\u6807\u7684\u5c40\u9650\u6027\uff0c\u53ef\u4ee5\u6307\u5bfc\u672a\u6765\u66f4\u5b9e\u7528\u3001\u66f4\u53ef\u9760\u7684unlearning\u7b97\u6cd5\u8bbe\u8ba1\u3002"}}
{"id": "2508.00959", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00959", "abs": "https://arxiv.org/abs/2508.00959", "authors": ["Rub\u00e9n Mu\u00f1oz-Sierra", "Manuel Doblar\u00e9", "Jacobo Ayensa-Jim\u00e9nez"], "title": "Enhancing material behavior discovery using embedding-oriented Physically-Guided Neural Networks with Internal Variables", "comment": null, "summary": "Physically Guided Neural Networks with Internal Variables are SciML tools\nthat use only observable data for training and and have the capacity to unravel\ninternal state relations. They incorporate physical knowledge both by\nprescribing the model architecture and using loss regularization, thus endowing\ncertain specific neurons with a physical meaning as internal state variables.\nDespite their potential, these models face challenges in scalability when\napplied to high-dimensional data such as fine-grid spatial fields or\ntime-evolving systems. In this work, we propose some enhancements to the PGNNIV\nframework that address these scalability limitations through reduced-order\nmodeling techniques. Specifically, we introduce alternatives to the original\ndecoder structure using spectral decomposition, POD, and pretrained\nautoencoder-based mappings. These surrogate decoders offer varying trade-offs\nbetween computational efficiency, accuracy, noise tolerance, and\ngeneralization, while improving drastically the scalability. Additionally, we\nintegrate model reuse via transfer learning and fine-tuning strategies to\nexploit previously acquired knowledge, supporting efficient adaptation to novel\nmaterials or configurations, and significantly reducing training time while\nmaintaining or improving model performance. To illustrate these various\ntechniques, we use a representative case governed by the nonlinear diffusion\nequation, using only observable data. Results demonstrate that the enhanced\nPGNNIV framework successfully identifies the underlying constitutive state\nequations while maintaining high predictive accuracy. It also improves\nrobustness to noise, mitigates overfitting, and reduces computational demands.\nThe proposed techniques can be tailored to various scenarios depending on data\navailability, resources, and specific modeling objectives, overcoming\nscalability challenges in all the scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e9b\u589e\u5f3a\u7684PGNNIV\u6846\u67b6\uff0c\u901a\u8fc7\u964d\u9636\u5efa\u6a21\u6280\u672f\u89e3\u51b3\u4e86\u53ef\u6269\u5c55\u6027\u9650\u5236\uff0c\u5e76\u96c6\u6210\u4e86\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u548c\u5fae\u8c03\u7b56\u7565\u8fdb\u884c\u6a21\u578b\u91cd\u7528\uff0c\u4ee5\u5229\u7528\u5148\u524d\u83b7\u5f97\u7684\u77e5\u8bc6\u3002", "motivation": "\u5177\u6709\u5185\u90e8\u53d8\u91cf\u7684\u7269\u7406\u5f15\u5bfc\u795e\u7ecf\u7f51\u7edc\u662fSciML\u5de5\u5177\uff0c\u5b83\u4ec5\u4f7f\u7528\u53ef\u89c2\u5bdf\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5177\u6709\u89e3\u5f00\u5185\u90e8\u72b6\u6001\u5173\u7cfb\u7684\u80fd\u529b\u3002\u5b83\u4eec\u901a\u8fc7\u6307\u5b9a\u6a21\u578b\u67b6\u6784\u548c\u4f7f\u7528\u635f\u5931\u6b63\u5219\u5316\u6765\u7ed3\u5408\u7269\u7406\u77e5\u8bc6\uff0c\u4ece\u800c\u8d4b\u4e88\u67d0\u4e9b\u7279\u5b9a\u795e\u7ecf\u5143\u4ee5\u7269\u7406\u610f\u4e49\u4f5c\u4e3a\u5185\u90e8\u72b6\u6001\u53d8\u91cf\u3002\u5c3d\u7ba1\u8fd9\u4e9b\u6a21\u578b\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u5e94\u7528\u4e8e\u9ad8\u7ef4\u6570\u636e\uff08\u5982\u7ec6\u7f51\u683c\u7a7a\u95f4\u573a\u6216\u968f\u65f6\u95f4\u6f14\u5316\u7684\u7cfb\u7edf\uff09\u65f6\uff0c\u5b83\u4eec\u9762\u4e34\u7740\u53ef\u6269\u5c55\u6027\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "\u6211\u4eec\u901a\u8fc7\u964d\u9636\u5efa\u6a21\u6280\u672f\uff0c\u4e3aPGNNIV\u6846\u67b6\u63d0\u51fa\u4e86\u4e00\u4e9b\u589e\u5f3a\u529f\u80fd\uff0c\u89e3\u51b3\u4e86\u8fd9\u4e9b\u53ef\u6269\u5c55\u6027\u9650\u5236\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4f7f\u7528\u9891\u8c31\u5206\u89e3\u3001POD\u548c\u9884\u8bad\u7ec3\u7684\u57fa\u4e8e\u81ea\u52a8\u7f16\u7801\u5668\u7684\u6620\u5c04\u6765\u66ff\u4ee3\u539f\u59cb\u89e3\u7801\u5668\u7ed3\u6784\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u589e\u5f3a\u7684PGNNIV\u6846\u67b6\u6210\u529f\u8bc6\u522b\u4e86\u5e95\u5c42\u672c\u6784\u72b6\u6001\u65b9\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002", "conclusion": "\u589e\u5f3a\u7684PGNNIV\u6846\u67b6\u6210\u529f\u8bc6\u522b\u4e86\u5e95\u5c42\u672c\u6784\u72b6\u6001\u65b9\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002\u5b83\u8fd8\u63d0\u9ad8\u4e86\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff0c\u51cf\u8f7b\u4e86\u8fc7\u62df\u5408\uff0c\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u9700\u6c42\u3002\u6240\u63d0\u51fa\u7684\u6280\u672f\u53ef\u4ee5\u6839\u636e\u6570\u636e\u53ef\u7528\u6027\u3001\u8d44\u6e90\u548c\u5177\u4f53\u7684\u5efa\u6a21\u76ee\u6807\u8fdb\u884c\u5b9a\u5236\uff0c\u514b\u670d\u4e86\u6240\u6709\u573a\u666f\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u6311\u6218\u3002"}}
{"id": "2508.02243", "categories": ["cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.02243", "abs": "https://arxiv.org/abs/2508.02243", "authors": ["Ziyan Liu", "Junwen Li", "Kaiwen Li", "Tong Ruan", "Chao Wang", "Xinyan He", "Zongyu Wang", "Xuezhi Cao", "Jingping Liu"], "title": "I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal Entity Linking", "comment": "10 pages, 6 figures, accepted by ACMMM 2025", "summary": "Multimodal entity linking plays a crucial role in a wide range of\napplications. Recent advances in large language model-based methods have become\nthe dominant paradigm for this task, effectively leveraging both textual and\nvisual modalities to enhance performance. Despite their success, these methods\nstill face two challenges, including unnecessary incorporation of image data in\ncertain scenarios and the reliance only on a one-time extraction of visual\nfeatures, which can undermine their effectiveness and accuracy. To address\nthese challenges, we propose a novel LLM-based framework for the multimodal\nentity linking task, called Intra- and Inter-modal Collaborative Reflections.\nThis framework prioritizes leveraging text information to address the task.\nWhen text alone is insufficient to link the correct entity through intra- and\ninter-modality evaluations, it employs a multi-round iterative strategy that\nintegrates key visual clues from various aspects of the image to support\nreasoning and enhance matching accuracy. Extensive experiments on three widely\nused public datasets demonstrate that our framework consistently outperforms\ncurrent state-of-the-art methods in the task, achieving improvements of 3.2%,\n5.1%, and 1.6%, respectively. Our code is available at\nhttps://github.com/ziyan-xiaoyu/I2CR/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684 LLM \u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u6574\u5408\u5173\u952e\u89c6\u89c9\u7ebf\u7d22\u6765\u63d0\u9ad8\u591a\u6a21\u6001\u5b9e\u4f53\u94fe\u63a5\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u4ecd\u7136\u9762\u4e34\u4e24\u4e2a\u6311\u6218\uff0c\u5305\u62ec\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4e0d\u5fc5\u8981\u5730\u6574\u5408\u56fe\u50cf\u6570\u636e\uff0c\u4ee5\u53ca\u4ec5\u4f9d\u8d56\u4e8e\u89c6\u89c9\u7279\u5f81\u7684\u4e00\u6b21\u6027\u63d0\u53d6\uff0c\u8fd9\u53ef\u80fd\u4f1a\u524a\u5f31\u5176\u6709\u6548\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e LLM \u7684\u591a\u6a21\u6001\u5b9e\u4f53\u94fe\u63a5\u4efb\u52a1\u6846\u67b6\uff0c\u79f0\u4e3a\u6a21\u5185\u548c\u6a21\u95f4\u534f\u4f5c\u53cd\u601d\u3002", "result": "\u8be5\u6846\u67b6\u59cb\u7ec8\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5206\u522b\u63d0\u9ad8\u4e86 3.2%\u30015.1% \u548c 1.6%\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u8be5\u4efb\u52a1\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5206\u522b\u63d0\u9ad8\u4e86 3.2%\u30015.1% \u548c 1.6%\u3002"}}
{"id": "2508.01541", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01541", "abs": "https://arxiv.org/abs/2508.01541", "authors": ["Sara C\u00e2mara", "Eduardo Luz", "Val\u00e9ria Carvalho", "Ivan Meneghini", "Gladston Moreira"], "title": "MOPrompt: Multi-objective Semantic Evolution for Prompt Optimization", "comment": "8 pages", "summary": "Prompt engineering is crucial for unlocking the potential of Large Language\nModels (LLMs). Still, since manual prompt design is often complex,\nnon-intuitive, and time-consuming, automatic prompt optimization has emerged as\na research area. However, a significant challenge in prompt optimization is\nmanaging the inherent trade-off between task performance, such as accuracy, and\ncontext size. Most existing automated methods focus on a single objective,\ntypically performance, thereby failing to explore the critical spectrum of\nefficiency and effectiveness. This paper introduces the MOPrompt, a novel\nMulti-objective Evolutionary Optimization (EMO) framework designed to optimize\nprompts for both accuracy and context size (measured in tokens) simultaneously.\nOur framework maps the Pareto front of prompt solutions, presenting\npractitioners with a set of trade-offs between context size and performance, a\ncrucial tool for deploying Large Language Models (LLMs) in real-world\napplications. We evaluate MOPrompt on a sentiment analysis task in Portuguese,\nusing Gemma-2B and Sabiazinho-3 as evaluation models. Our findings show that\nMOPrompt substantially outperforms the baseline framework. For the Sabiazinho\nmodel, MOPrompt identifies a prompt that achieves the same peak accuracy (0.97)\nas the best baseline solution, but with a 31% reduction in token length.", "AI": {"tldr": "MOPrompt, a multi-objective evolutionary optimization framework, optimizes prompts for accuracy and context size, outperforming baselines by achieving similar accuracy with 31% less token length.", "motivation": "Manual prompt design is complex, non-intuitive, and time-consuming, while existing automated methods fail to balance efficiency and effectiveness.", "method": "A Multi-objective Evolutionary Optimization (EMO) framework is used to optimize prompts for both accuracy and context size.", "result": "MOPrompt outperforms the baseline framework in sentiment analysis, achieving comparable accuracy with significantly reduced token length.", "conclusion": "MOPrompt identifies prompts that match baseline accuracy with a 31% reduction in token length for the Sabiazinho model."}}
{"id": "2508.01170", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01170", "abs": "https://arxiv.org/abs/2508.01170", "authors": ["Tuan Duc Ngo", "Ashkan Mirzaei", "Guocheng Qian", "Hanwen Liang", "Chuang Gan", "Evangelos Kalogerakis", "Peter Wonka", "Chaoyang Wang"], "title": "DELTAv2: Accelerating Dense 3D Tracking", "comment": null, "summary": "We propose a novel algorithm for accelerating dense long-term 3D point\ntracking in videos. Through analysis of existing state-of-the-art methods, we\nidentify two major computational bottlenecks. First, transformer-based\niterative tracking becomes expensive when handling a large number of\ntrajectories. To address this, we introduce a coarse-to-fine strategy that\nbegins tracking with a small subset of points and progressively expands the set\nof tracked trajectories. The newly added trajectories are initialized using a\nlearnable interpolation module, which is trained end-to-end alongside the\ntracking network. Second, we propose an optimization that significantly reduces\nthe cost of correlation feature computation, another key bottleneck in prior\nmethods. Together, these improvements lead to a 5-100x speedup over existing\napproaches while maintaining state-of-the-art tracking accuracy.", "AI": {"tldr": "This paper proposes a novel algorithm for accelerating dense long-term 3D point tracking in videos, which leads to a 5-100x speedup over existing approaches while maintaining state-of-the-art tracking accuracy.", "motivation": "transformer-based iterative tracking becomes expensive when handling a large number of trajectories.  the cost of correlation feature computation, another key bottleneck in prior methods.", "method": "introduce a coarse-to-fine strategy that begins tracking with a small subset of points and progressively expands the set of tracked trajectories. The newly added trajectories are initialized using a learnable interpolation module, which is trained end-to-end alongside the tracking network. propose an optimization that significantly reduces the cost of correlation feature computation", "result": "Together, these improvements lead to a 5-100x speedup over existing approaches while maintaining state-of-the-art tracking accuracy.", "conclusion": "These improvements lead to a 5-100x speedup over existing approaches while maintaining state-of-the-art tracking accuracy."}}
{"id": "2508.01330", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01330", "abs": "https://arxiv.org/abs/2508.01330", "authors": ["Zihan Zheng", "Tianle Cui", "Chuwen Xie", "Jiahui Zhang", "Jiahui Pan", "Lewei He", "Qianglong Chen"], "title": "NatureGAIA: Pushing the Frontiers of GUI Agents with a Challenging Benchmark and High-Quality Trajectory Dataset", "comment": null, "summary": "The rapid advancement of Large Language Model (LLM)-driven Graphical User\nInterface (GUI) agents is significantly hampered by the profound limitations of\nexisting evaluation benchmarks in terms of accuracy, reproducibility, and\nscalability. To address this critical gap, we introduce \\Benchmark, a novel\nbenchmark engineered on the principle of Causal Pathways. This design paradigm\nstructures complex tasks into a series of programmatically verifiable atomic\nsteps, ensuring a rigorous, fully automated, and reproducible standard for\nassessment. Concurrently, to mitigate the inherent capability deficits of\nagents, we developed \\Agent, a hierarchical agent architecture specifically\noptimized for long-horizon tasks. We leveraged this agent to generate a\nhigh-quality, human-verified trajectory dataset that uniquely captures diverse\nand even self-correcting interaction patterns of LLMs. We then utilized this\ndataset to perform Reinforcement Fine-Tuning (RFT) on the Qwen2.5-VL-7B model.\nOur experiments reveal that \\Benchmark~presents a formidable challenge to\ncurrent state-of-the-art LLMs; even the top-performing Claude-sonnet-4 achieved\na Weighted Pathway Success Rate (WPSR) of only 34.6\\%. Moreover, while RFT\nsubstantially improved the smaller model's GUI execution capabilities (WPSR\nincreased from 3.3\\% to 10.8\\%), its performance degraded sharply when handling\ncomplex scenarios. This outcome highlights the inherent capability ceiling of\nsmaller models when faced with comprehensive tasks that integrate perception,\ndecision-making, and execution. This research contributes a rigorous evaluation\nstandard and a high-quality dataset to the community, aiming to guide the\nfuture development of GUI agents.", "AI": {"tldr": "The paper introduces a new benchmark (\") and agent architecture to address limitations in evaluating and improving LLM-based GUI agents. Experiments show the benchmark is challenging and that smaller models struggle with complex tasks despite fine-tuning.", "motivation": "The rapid advancement of Large Language Model (LLM)-driven Graphical User Interface (GUI) agents is significantly hampered by the profound limitations of existing evaluation benchmarks in terms of accuracy, reproducibility, and scalability.", "method": "introduce \"{Benchmark}\", a novel benchmark engineered on the principle of Causal Pathways and developed \\Agent, a hierarchical agent architecture specifically optimized for long-horizon tasks", "result": "\"{Benchmark}~presents a formidable challenge to current state-of-the-art LLMs; even the top-performing Claude-sonnet-4 achieved a Weighted Pathway Success Rate (WPSR) of only 34.6\\%. Moreover, while RFT substantially improved the smaller model's GUI execution capabilities (WPSR increased from 3.3\\% to 10.8\\%), its performance degraded sharply when handling complex scenarios.\"", "conclusion": "This research contributes a rigorous evaluation standard and a high-quality dataset to the community, aiming to guide the future development of GUI agents."}}
{"id": "2508.00960", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.00960", "abs": "https://arxiv.org/abs/2508.00960", "authors": ["Sudip K. Seal", "Maksudul Alam", "Jorge Ramirez", "Sajal Dash", "Hao Lu"], "title": "Compression-Induced Communication-Efficient Large Model Training and Inferencing", "comment": null, "summary": "Energy efficiency of training and inferencing with large neural network\nmodels is a critical challenge facing the future of sustainable large-scale\nmachine learning workloads. This paper introduces an alternative strategy,\ncalled phantom parallelism, to minimize the net energy consumption of\ntraditional tensor (model) parallelism, the most energy-inefficient component\nof large neural network training. The approach is presented in the context of\nfeed-forward network architectures as a preliminary, but comprehensive,\nproof-of-principle study of the proposed methodology. We derive new forward and\nbackward propagation operators for phantom parallelism, implement them as\ncustom autograd operations within an end-to-end phantom parallel training\npipeline and compare its parallel performance and energy-efficiency against\nthose of conventional tensor parallel training pipelines. Formal analyses that\npredict lower bandwidth and FLOP counts are presented with supporting empirical\nresults on up to 256 GPUs that corroborate these gains. Experiments are shown\nto deliver ~50% reduction in the energy consumed to train FFNs using the\nproposed phantom parallel approach when compared with conventional tensor\nparallel methods. Additionally, the proposed approach is shown to train smaller\nphantom models to the same model loss on smaller GPU counts as larger tensor\nparallel models on larger GPU counts offering the possibility for even greater\nenergy savings.", "AI": {"tldr": "Phantom parallelism is introduced to minimize energy consumption in large neural network training, achieving ~50% energy reduction compared to tensor parallelism and enabling training of smaller models on fewer GPUs.", "motivation": "Energy efficiency of training and inferencing with large neural network models is a critical challenge.", "method": "The paper introduces phantom parallelism and derives new forward and backward propagation operators, implementing them as custom autograd operations within an end-to-end training pipeline. The performance and energy-efficiency are compared against conventional tensor parallel training pipelines.", "result": "Experiments show a ~50% reduction in energy consumption for training FFNs using phantom parallelism compared to conventional tensor parallel methods. The approach also enables training smaller models to the same loss on fewer GPUs.", "conclusion": "The proposed phantom parallel approach can reduce energy consumption by ~50% when training FFNs compared to conventional tensor parallel methods. Smaller phantom models can achieve the same model loss as larger tensor parallel models on fewer GPUs, offering even greater energy savings."}}
{"id": "2508.02340", "categories": ["cs.CV", "cs.IR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.02340", "abs": "https://arxiv.org/abs/2508.02340", "authors": ["Fan Hu", "Zijie Xin", "Xirong Li"], "title": "Learning Partially-Decorrelated Common Spaces for Ad-hoc Video Search", "comment": "Accepted by ACMMM2025", "summary": "Ad-hoc Video Search (AVS) involves using a textual query to search for\nmultiple relevant videos in a large collection of unlabeled short videos. The\nmain challenge of AVS is the visual diversity of relevant videos. A simple\nquery such as \"Find shots of a man and a woman dancing together indoors\" can\nspan a multitude of environments, from brightly lit halls and shadowy bars to\ndance scenes in black-and-white animations. It is therefore essential to\nretrieve relevant videos as comprehensively as possible. Current solutions for\nthe AVS task primarily fuse multiple features into one or more common spaces,\nyet overlook the need for diverse spaces. To fully exploit the expressive\ncapability of individual features, we propose LPD, short for Learning Partially\nDecorrelated common spaces. LPD incorporates two key innovations:\nfeature-specific common space construction and the de-correlation loss.\nSpecifically, LPD learns a separate common space for each video and text\nfeature, and employs de-correlation loss to diversify the ordering of negative\nsamples across different spaces. To enhance the consistency of multi-space\nconvergence, we designed an entropy-based fair multi-space triplet ranking\nloss. Extensive experiments on the TRECVID AVS benchmarks (2016-2023) justify\nthe effectiveness of LPD. Moreover, diversity visualizations of LPD's spaces\nhighlight its ability to enhance result diversity.", "AI": {"tldr": "This paper proposes LPD to address the visual diversity challenge in Ad-hoc Video Search (AVS) by learning partially decorrelated common spaces. LPD demonstrates its effectiveness and ability to enhance result diversity on the TRECVID AVS benchmarks.", "motivation": "The main challenge of AVS is the visual diversity of relevant videos. Current solutions for the AVS task primarily fuse multiple features into one or more common spaces, yet overlook the need for diverse spaces.", "method": "We propose LPD, short for Learning Partially Decorrelated common spaces. LPD incorporates two key innovations: feature-specific common space construction and the de-correlation loss. Specifically, LPD learns a separate common space for each video and text feature, and employs de-correlation loss to diversify the ordering of negative samples across different spaces. To enhance the consistency of multi-space convergence, we designed an entropy-based fair multi-space triplet ranking loss.", "result": "LPD enhances result diversity.", "conclusion": "Extensive experiments on the TRECVID AVS benchmarks (2016-2023) justify the effectiveness of LPD. Moreover, diversity visualizations of LPD's spaces highlight its ability to enhance result diversity."}}
{"id": "2508.01554", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.01554", "abs": "https://arxiv.org/abs/2508.01554", "authors": ["Yujia Zheng", "Tianhao Li", "Haotian Huang", "Tianyu Zeng", "Jingyu Lu", "Chuangxin Chu", "Yuekai Huang", "Ziyou Jiang", "Qian Xiong", "Yuyao Ge", "Mingyang Li"], "title": "Are All Prompt Components Value-Neutral? Understanding the Heterogeneous Adversarial Robustness of Dissected Prompt in Large Language Models", "comment": null, "summary": "Prompt-based adversarial attacks have become an effective means to assess the\nrobustness of large language models (LLMs). However, existing approaches often\ntreat prompts as monolithic text, overlooking their structural\nheterogeneity-different prompt components contribute unequally to adversarial\nrobustness. Prior works like PromptRobust assume prompts are value-neutral, but\nour analysis reveals that complex, domain-specific prompts with rich structures\nhave components with differing vulnerabilities. To address this gap, we\nintroduce PromptAnatomy, an automated framework that dissects prompts into\nfunctional components and generates diverse, interpretable adversarial examples\nby selectively perturbing each component using our proposed method, ComPerturb.\nTo ensure linguistic plausibility and mitigate distribution shifts, we further\nincorporate a perplexity (PPL)-based filtering mechanism. As a complementary\nresource, we annotate four public instruction-tuning datasets using the\nPromptAnatomy framework, verified through human review. Extensive experiments\nacross these datasets and five advanced LLMs demonstrate that ComPerturb\nachieves state-of-the-art attack success rates. Ablation studies validate the\ncomplementary benefits of prompt dissection and PPL filtering. Our results\nunderscore the importance of prompt structure awareness and controlled\nperturbation for reliable adversarial robustness evaluation in LLMs. Code and\ndata are available at https://github.com/Yujiaaaaa/PACP.", "AI": {"tldr": "PromptAnatomy dissects prompts to generate adversarial examples, achieving state-of-the-art attack success rates, highlighting the importance of prompt structure.", "motivation": "Existing approaches often treat prompts as monolithic text, overlooking their structural heterogeneity. Different prompt components contribute unequally to adversarial robustness, and complex, domain-specific prompts with rich structures have components with differing vulnerabilities.", "method": "An automated framework that dissects prompts into functional components and generates diverse, interpretable adversarial examples by selectively perturbing each component using a proposed method, ComPerturb, incorporating a perplexity (PPL)-based filtering mechanism.", "result": "ComPerturb achieves state-of-the-art attack success rates. Ablation studies validate the complementary benefits of prompt dissection and PPL filtering.", "conclusion": "Prompt structure awareness and controlled perturbation are important for reliable adversarial robustness evaluation in LLMs."}}
{"id": "2508.01171", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01171", "abs": "https://arxiv.org/abs/2508.01171", "authors": ["Ranran Huang", "Krystian Mikolajczyk"], "title": "No Pose at All: Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views", "comment": "Project Page: https://ranrhuang.github.io/spfsplat/", "summary": "We introduce SPFSplat, an efficient framework for 3D Gaussian splatting from\nsparse multi-view images, requiring no ground-truth poses during training or\ninference. It employs a shared feature extraction backbone, enabling\nsimultaneous prediction of 3D Gaussian primitives and camera poses in a\ncanonical space from unposed inputs within a single feed-forward step.\nAlongside the rendering loss based on estimated novel-view poses, a\nreprojection loss is integrated to enforce the learning of pixel-aligned\nGaussian primitives for enhanced geometric constraints. This pose-free training\nparadigm and efficient one-step feed-forward design make SPFSplat well-suited\nfor practical applications. Remarkably, despite the absence of pose\nsupervision, SPFSplat achieves state-of-the-art performance in novel view\nsynthesis even under significant viewpoint changes and limited image overlap.\nIt also surpasses recent methods trained with geometry priors in relative pose\nestimation. Code and trained models are available on our project page:\nhttps://ranrhuang.github.io/spfsplat/.", "AI": {"tldr": "SPFSplat \u662f\u4e00\u79cd\u7528\u4e8e\u4ece\u7a00\u758f\u591a\u89c6\u56fe\u56fe\u50cf\u8fdb\u884c 3D \u9ad8\u65af\u6e85\u5c04\u7684\u9ad8\u6548\u6846\u67b6\uff0c\u65e0\u9700\u5730\u9762\u5b9e\u51b5\u59ff\u52bf\uff0c\u5e76\u5728 novel view \u5408\u6210\u548c\u76f8\u5bf9\u59ff\u52bf\u4f30\u8ba1\u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u4ece\u7a00\u758f\u7684\u591a\u89c6\u56fe\u56fe\u50cf\u4e2d\u8fdb\u884c 3D \u9ad8\u65af\u6e85\u5c04\u7684\u9ad8\u6548\u6846\u67b6\uff0c\u5728\u8bad\u7ec3\u6216\u63a8\u7406\u671f\u95f4\u4e0d\u9700\u8981\u5730\u9762\u5b9e\u51b5\u59ff\u52bf\u3002", "method": "SPFSplat \u91c7\u7528\u5171\u4eab\u7279\u5f81\u63d0\u53d6\u4e3b\u5e72\uff0c\u80fd\u591f\u5728\u5355\u4e2a\u524d\u9988\u6b65\u9aa4\u4e2d\u4ece\u65e0\u59ff\u52bf\u8f93\u5165\u540c\u65f6\u9884\u6d4b\u89c4\u8303\u7a7a\u95f4\u4e2d\u7684 3D \u9ad8\u65af\u57fa\u5143\u548c\u76f8\u673a\u59ff\u52bf\u3002", "result": "SPFSplat \u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684 novel view \u5408\u6210\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u76f8\u5bf9\u59ff\u52bf\u4f30\u8ba1\u65b9\u9762\u8d85\u8d8a\u4e86\u6700\u8fd1\u7684\u65b9\u6cd5\u3002", "conclusion": "SPFSplat\u5728 novel view \u5408\u6210\u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5373\u4f7f\u5728\u663e\u7740\u7684\u89c6\u70b9\u53d8\u5316\u548c\u6709\u9650\u7684\u56fe\u50cf\u91cd\u53e0\u4e0b\u4e5f\u662f\u5982\u6b64\u3002\u5b83\u8fd8\u5728\u76f8\u5bf9\u59ff\u52bf\u4f30\u8ba1\u65b9\u9762\u8d85\u8d8a\u4e86\u6700\u8fd1\u4f7f\u7528\u51e0\u4f55\u5148\u9a8c\u8bad\u7ec3\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.01368", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01368", "abs": "https://arxiv.org/abs/2508.01368", "authors": ["Zhehong Ren", "Tianluo Zhang", "Yiheng Lu", "Yushen Liang", "Promethee Spathis"], "title": "Relation-Aware LNN-Transformer for Intersection-Centric Next-Step Prediction", "comment": "8 pages, 5 figures", "summary": "Next-step location prediction plays a pivotal role in modeling human\nmobility, underpinning applications from personalized navigation to strategic\nurban planning. However, approaches that assume a closed world - restricting\nchoices to a predefined set of points of interest (POIs) - often fail to\ncapture exploratory or target-agnostic behavior and the topological constraints\nof urban road networks. Hence, we introduce a road-node-centric framework that\nrepresents road-user trajectories on the city's road-intersection graph,\nthereby relaxing the closed-world constraint and supporting next-step\nforecasting beyond fixed POI sets. To encode environmental context, we\nintroduce a sector-wise directional POI aggregation that produces compact\nfeatures capturing distance, bearing, density and presence cues. By combining\nthese cues with structural graph embeddings, we obtain semantically grounded\nnode representations. For sequence modeling, we integrate a Relation-Aware\nLNN-Transformer - a hybrid of a Continuous-time Forgetting Cell CfC-LNN and a\nbearing-biased self-attention module - to capture both fine-grained temporal\ndynamics and long-range spatial dependencies. Evaluated on city-scale road-user\ntrajectories, our model outperforms six state-of-the-art baselines by up to 17\npercentage points in accuracy at one hop and 10 percentage points in MRR, and\nmaintains high resilience under noise, losing only 2.4 percentage points in\naccuracy at one under 50 meter GPS perturbation and 8.9 percentage points in\naccuracy at one hop under 25 percent POI noise.", "AI": {"tldr": "This paper introduces a road-node-centric framework for next-step location prediction that relaxes the closed-world constraint by representing trajectories on the city's road-intersection graph. The model combines sector-wise directional POI aggregation with structural graph embeddings and a Relation-Aware LNN-Transformer to achieve state-of-the-art performance and resilience to noise.", "motivation": "Approaches that assume a closed world - restricting choices to a predefined set of points of interest (POIs) - often fail to capture exploratory or target-agnostic behavior and the topological constraints of urban road networks.", "method": "We integrate a Relation-Aware LNN-Transformer - a hybrid of a Continuous-time Forgetting Cell CfC-LNN and a bearing-biased self-attention module - to capture both fine-grained temporal dynamics and long-range spatial dependencies.", "result": "Our model outperforms six state-of-the-art baselines by up to 17 percentage points in accuracy at one hop and 10 percentage points in MRR, and maintains high resilience under noise.", "conclusion": "Our model outperforms six state-of-the-art baselines by up to 17 percentage points in accuracy at one hop and 10 percentage points in MRR, and maintains high resilience under noise, losing only 2.4 percentage points in accuracy at one under 50 meter GPS perturbation and 8.9 percentage points in accuracy at one hop under 25 percent POI noise."}}
{"id": "2508.00961", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00961", "abs": "https://arxiv.org/abs/2508.00961", "authors": ["Xiang Li", "Penglei Sun", "Wanyun Zhou", "Zikai Wei", "Yongqi Zhang", "Xiaowen Chu"], "title": "FinKario: Event-Enhanced Automated Construction of Financial Knowledge Graph", "comment": null, "summary": "Individual investors are significantly outnumbered and disadvantaged in\nfinancial markets, overwhelmed by abundant information and lacking professional\nanalysis. Equity research reports stand out as crucial resources, offering\nvaluable insights. By leveraging these reports, large language models (LLMs)\ncan enhance investors' decision-making capabilities and strengthen financial\nanalysis. However, two key challenges limit their effectiveness: (1) the rapid\nevolution of market events often outpaces the slow update cycles of existing\nknowledge bases, (2) the long-form and unstructured nature of financial reports\nfurther hinders timely and context-aware integration by LLMs. To address these\nchallenges, we tackle both data and methodological aspects. First, we introduce\nthe Event-Enhanced Automated Construction of Financial Knowledge Graph\n(FinKario), a dataset comprising over 305,360 entities, 9,625 relational\ntriples, and 19 distinct relation types. FinKario automatically integrates\nreal-time company fundamentals and market events through prompt-driven\nextraction guided by professional institutional templates, providing structured\nand accessible financial insights for LLMs. Additionally, we propose a\nTwo-Stage, Graph-Based retrieval strategy (FinKario-RAG), optimizing the\nretrieval of evolving, large-scale financial knowledge to ensure efficient and\nprecise data access. Extensive experiments show that FinKario with FinKario-RAG\nachieves superior stock trend prediction accuracy, outperforming financial LLMs\nby 18.81% and institutional strategies by 17.85% on average in backtesting.", "AI": {"tldr": "This paper introduces FinKario, a financial knowledge graph, and FinKario-RAG, a retrieval strategy, to improve stock trend prediction accuracy, outperforming existing methods.", "motivation": "Individual investors are significantly outnumbered and disadvantaged in financial markets, overwhelmed by abundant information and lacking professional analysis. Equity research reports stand out as crucial resources, offering valuable insights. However, two key challenges limit their effectiveness: (1) the rapid evolution of market events often outpaces the slow update cycles of existing knowledge bases, (2) the long-form and unstructured nature of financial reports further hinders timely and context-aware integration by LLMs.", "method": "We introduce the Event-Enhanced Automated Construction of Financial Knowledge Graph (FinKario), a dataset comprising over 305,360 entities, 9,625 relational triples, and 19 distinct relation types. Additionally, we propose a Two-Stage, Graph-Based retrieval strategy (FinKario-RAG), optimizing the retrieval of evolving, large-scale financial knowledge to ensure efficient and precise data access.", "result": "FinKario with FinKario-RAG achieves superior stock trend prediction accuracy, outperforming financial LLMs by 18.81% and institutional strategies by 17.85% on average in backtesting.", "conclusion": "FinKario with FinKario-RAG achieves superior stock trend prediction accuracy, outperforming financial LLMs by 18.81% and institutional strategies by 17.85% on average in backtesting."}}
{"id": "2508.02374", "categories": ["cs.CV", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02374", "abs": "https://arxiv.org/abs/2508.02374", "authors": ["Shuo Lu", "Yanyin Chen", "Wei Feng", "Jiahao Fan", "Fengheng Li", "Zheng Zhang", "Jingjing Lv", "Junjie Shen", "Ching Law", "Jian Liang"], "title": "Uni-Layout: Integrating Human Feedback in Unified Layout Generation and Evaluation", "comment": "Accepted to ACM MM 2025", "summary": "Layout generation plays a crucial role in enhancing both user experience and\ndesign efficiency. However, current approaches suffer from task-specific\ngeneration capabilities and perceptually misaligned evaluation metrics, leading\nto limited applicability and ineffective measurement. In this paper, we propose\n\\textit{Uni-Layout}, a novel framework that achieves unified generation,\nhuman-mimicking evaluation and alignment between the two. For universal\ngeneration, we incorporate various layout tasks into a single taxonomy and\ndevelop a unified generator that handles background or element contents\nconstrained tasks via natural language prompts. To introduce human feedback for\nthe effective evaluation of layouts, we build \\textit{Layout-HF100k}, the first\nlarge-scale human feedback dataset with 100,000 expertly annotated layouts.\nBased on \\textit{Layout-HF100k}, we introduce a human-mimicking evaluator that\nintegrates visual and geometric information, employing a Chain-of-Thought\nmechanism to conduct qualitative assessments alongside a confidence estimation\nmodule to yield quantitative measurements. For better alignment between the\ngenerator and the evaluator, we integrate them into a cohesive system by\nadopting Dynamic-Margin Preference Optimization (DMPO), which dynamically\nadjusts margins based on preference strength to better align with human\njudgments. Extensive experiments show that \\textit{Uni-Layout} significantly\noutperforms both task-specific and general-purpose methods. Our code is\npublicly available at https://github.com/JD-GenX/Uni-Layout.", "AI": {"tldr": "Uni-Layout is a novel framework that achieves unified generation, human-mimicking evaluation and alignment between the two.", "motivation": "current approaches suffer from task-specific generation capabilities and perceptually misaligned evaluation metrics, leading to limited applicability and ineffective measurement", "method": "incorporate various layout tasks into a single taxonomy and develop a unified generator that handles background or element contents constrained tasks via natural language prompts. To introduce human feedback for the effective evaluation of layouts, we build Layout-HF100k, the first large-scale human feedback dataset with 100,000 expertly annotated layouts. Based on Layout-HF100k, we introduce a human-mimicking evaluator that integrates visual and geometric information, employing a Chain-of-Thought mechanism to conduct qualitative assessments alongside a confidence estimation module to yield quantitative measurements. For better alignment between the generator and the evaluator, we integrate them into a cohesive system by adopting Dynamic-Margin Preference Optimization (DMPO), which dynamically adjusts margins based on preference strength to better align with human judgments", "result": "Uni-Layout significantly outperforms both task-specific and general-purpose methods", "conclusion": "Uni-Layout significantly outperforms both task-specific and general-purpose methods."}}
{"id": "2508.01630", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01630", "abs": "https://arxiv.org/abs/2508.01630", "authors": ["Maziyar Panahi"], "title": "OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers for Biomedical NER Across 12 Public Datasets", "comment": null, "summary": "Named-entity recognition (NER) is fundamental to extracting structured\ninformation from the >80% of healthcare data that resides in unstructured\nclinical notes and biomedical literature. Despite recent advances with large\nlanguage models, achieving state-of-the-art performance across diverse entity\ntypes while maintaining computational efficiency remains a significant\nchallenge. We introduce OpenMed NER, a suite of open-source, domain-adapted\ntransformer models that combine lightweight domain-adaptive pre-training (DAPT)\nwith parameter-efficient Low-Rank Adaptation (LoRA). Our approach performs\ncost-effective DAPT on a 350k-passage corpus compiled from ethically sourced,\npublicly available research repositories and de-identified clinical notes\n(PubMed, arXiv, and MIMIC-III) using DeBERTa-v3, PubMedBERT, and BioELECTRA\nbackbones. This is followed by task-specific fine-tuning with LoRA, which\nupdates less than 1.5% of model parameters. We evaluate our models on 12\nestablished biomedical NER benchmarks spanning chemicals, diseases, genes, and\nspecies. OpenMed NER achieves new state-of-the-art micro-F1 scores on 10 of\nthese 12 datasets, with substantial gains across diverse entity types. Our\nmodels advance the state-of-the-art on foundational disease and chemical\nbenchmarks (e.g., BC5CDR-Disease, +2.70 pp), while delivering even larger\nimprovements of over 5.3 and 9.7 percentage points on more specialized gene and\nclinical cell line corpora. This work demonstrates that strategically adapted\nopen-source models can surpass closed-source solutions. This performance is\nachieved with remarkable efficiency: training completes in under 12 hours on a\nsingle GPU with a low carbon footprint (< 1.2 kg CO2e), producing permissively\nlicensed, open-source checkpoints designed to help practitioners facilitate\ncompliance with emerging data protection and AI regulations, such as the EU AI\nAct.", "AI": {"tldr": "OpenMed NER, a suite of open-source, domain-adapted transformer models, achieves state-of-the-art performance on biomedical NER benchmarks with high efficiency and a low carbon footprint.", "motivation": "Achieving state-of-the-art performance across diverse entity types while maintaining computational efficiency in named-entity recognition (NER) remains a significant challenge.", "method": "Lightweight domain-adaptive pre-training (DAPT) with parameter-efficient Low-Rank Adaptation (LoRA) on DeBERTa-v3, PubMedBERT, and BioELECTRA backbones.", "result": "Achieves new state-of-the-art micro-F1 scores on 10 of 12 established biomedical NER benchmarks, with substantial gains across diverse entity types. Training completes in under 12 hours on a single GPU with a low carbon footprint.", "conclusion": "Strategically adapted open-source models can surpass closed-source solutions with remarkable efficiency."}}
{"id": "2508.01184", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01184", "abs": "https://arxiv.org/abs/2508.01184", "authors": ["Xinhang Wan", "Dongqiang Gou", "Xinwang Liu", "En Zhu", "Xuming He"], "title": "Object Affordance Recognition and Grounding via Multi-scale Cross-modal Representation Learning", "comment": null, "summary": "A core problem of Embodied AI is to learn object manipulation from\nobservation, as humans do. To achieve this, it is important to localize 3D\nobject affordance areas through observation such as images (3D affordance\ngrounding) and understand their functionalities (affordance classification).\nPrevious attempts usually tackle these two tasks separately, leading to\ninconsistent predictions due to lacking proper modeling of their dependency. In\naddition, these methods typically only ground the incomplete affordance areas\ndepicted in images, failing to predict the full potential affordance areas, and\noperate at a fixed scale, resulting in difficulty in coping with affordances\nsignificantly varying in scale with respect to the whole object. To address\nthese issues, we propose a novel approach that learns an affordance-aware 3D\nrepresentation and employs a stage-wise inference strategy leveraging the\ndependency between grounding and classification tasks. Specifically, we first\ndevelop a cross-modal 3D representation through efficient fusion and\nmulti-scale geometric feature propagation, enabling inference of full potential\naffordance areas at a suitable regional scale. Moreover, we adopt a simple\ntwo-stage prediction mechanism, effectively coupling grounding and\nclassification for better affordance understanding. Experiments demonstrate the\neffectiveness of our method, showing improved performance in both affordance\ngrounding and classification.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60 affordance-aware 3D \u8868\u793a\u548c\u91c7\u7528\u9636\u6bb5\u5f0f\u63a8\u7406\u7b56\u7565\uff0c\u4ece\u800c\u6539\u8fdb\u4e86 affordance grounding \u548c\u5206\u7c7b\u3002", "motivation": "\u5177\u8eab\u4eba\u5de5\u667a\u80fd\u7684\u6838\u5fc3\u95ee\u9898\u662f\u4ece\u89c2\u5bdf\u4e2d\u5b66\u4e60\u7269\u4f53\u64cd\u4f5c\uff0c\u5c31\u50cf\u4eba\u7c7b\u4e00\u6837\u3002 \u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0c\u91cd\u8981\u7684\u662f\u901a\u8fc7\u56fe\u50cf\u7b49\u89c2\u5bdf\u7ed3\u679c\u6765\u5b9a\u4f4d 3D \u7269\u4f53 affordance \u533a\u57df\uff083D affordance grounding\uff09\u5e76\u4e86\u89e3\u5176\u529f\u80fd\uff08affordance \u5206\u7c7b\uff09\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5b66\u4e60\u4e86 affordance-aware 3D \u8868\u793a\uff0c\u5e76\u91c7\u7528\u4e86\u4e00\u79cd\u5229\u7528 grounding \u548c\u5206\u7c7b\u4efb\u52a1\u4e4b\u95f4\u4f9d\u8d56\u5173\u7cfb\u7684\u9636\u6bb5\u5f0f\u63a8\u7406\u7b56\u7565\u3002 \u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u9996\u5148\u901a\u8fc7\u9ad8\u6548\u878d\u5408\u548c\u591a\u5c3a\u5ea6\u51e0\u4f55\u7279\u5f81\u4f20\u64ad\u5f00\u53d1\u4e86\u4e00\u79cd\u8de8\u6a21\u6001 3D \u8868\u793a\uff0c\u4ece\u800c\u80fd\u591f\u5728\u5408\u9002\u7684\u533a\u57df\u5c3a\u5ea6\u4e0a\u63a8\u65ad\u51fa\u5b8c\u6574\u7684\u6f5c\u5728 affordance \u533a\u57df\u3002 \u6b64\u5916\uff0c\u6211\u4eec\u91c7\u7528\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u4e24\u9636\u6bb5\u9884\u6d4b\u673a\u5236\uff0c\u6709\u6548\u5730\u5c06 grounding \u548c\u5206\u7c7b\u7ed3\u5408\u8d77\u6765\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3 affordance\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5728 affordance grounding \u548c\u5206\u7c7b\u65b9\u9762\u5747\u8868\u73b0\u51fa\u6539\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728 affordance grounding \u548c\u5206\u7c7b\u65b9\u9762\u5747\u8868\u73b0\u51fa\u6539\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2508.01432", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01432", "abs": "https://arxiv.org/abs/2508.01432", "authors": ["Yuanzhe Shen", "Kaimin Wang", "Changze Lv", "Xiaoqing Zheng", "Xuanjing Huang"], "title": "TripTailor: A Real-World Benchmark for Personalized Travel Planning", "comment": "Accepted to ACL 2025 Findings", "summary": "The continuous evolution and enhanced reasoning capabilities of large\nlanguage models (LLMs) have elevated their role in complex tasks, notably in\ntravel planning, where demand for personalized, high-quality itineraries is\nrising. However, current benchmarks often rely on unrealistic simulated data,\nfailing to reflect the differences between LLM-generated and real-world\nitineraries. Existing evaluation metrics, which primarily emphasize\nconstraints, fall short of providing a comprehensive assessment of the overall\nquality of travel plans. To address these limitations, we introduce TripTailor,\na benchmark designed specifically for personalized travel planning in\nreal-world scenarios. This dataset features an extensive collection of over\n500,000 real-world points of interest (POIs) and nearly 4,000 diverse travel\nitineraries, complete with detailed information, providing a more authentic\nevaluation framework. Experiments show that fewer than 10\\% of the itineraries\ngenerated by the latest state-of-the-art LLMs achieve human-level performance.\nMoreover, we identify several critical challenges in travel planning, including\nthe feasibility, rationality, and personalized customization of the proposed\nsolutions. We hope that TripTailor will drive the development of travel\nplanning agents capable of understanding and meeting user needs while\ngenerating practical itineraries. Our code and dataset are available at\nhttps://github.com/swxkfm/TripTailor", "AI": {"tldr": "TripTailor, a new benchmark for travel planning, reveals that LLMs still struggle to create realistic and personalized travel itineraries.", "motivation": "Existing travel planning benchmarks rely on unrealistic simulated data and lack comprehensive evaluation metrics for assessing the overall quality of travel plans generated by LLMs.", "method": "The authors introduce TripTailor, a benchmark for personalized travel planning using a dataset of over 500,000 real-world points of interest (POIs) and nearly 4,000 diverse travel itineraries.", "result": "Experiments show that fewer than 10% of itineraries generated by state-of-the-art LLMs achieve human-level performance on the TripTailor benchmark.", "conclusion": "The TripTailor benchmark reveals that current LLMs struggle to generate human-level travel itineraries, highlighting challenges in feasibility, rationality, and personalization. The authors hope TripTailor will drive the development of better travel planning agents."}}
{"id": "2508.00963", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00963", "abs": "https://arxiv.org/abs/2508.00963", "authors": ["Timothy Oladunni", "Alex Wong"], "title": "Rethinking Multimodality: Optimizing Multimodal Deep Learning for Biomedical Signal Classification", "comment": null, "summary": "This study proposes a novel perspective on multimodal deep learning for\nbiomedical signal classification, systematically analyzing how complementary\nfeature domains impact model performance. While fusing multiple domains often\npresumes enhanced accuracy, this work demonstrates that adding modalities can\nyield diminishing returns, as not all fusions are inherently advantageous. To\nvalidate this, five deep learning models were designed, developed, and\nrigorously evaluated: three unimodal (1D-CNN for time, 2D-CNN for\ntime-frequency, and 1D-CNN-Transformer for frequency) and two multimodal\n(Hybrid 1, which fuses 1D-CNN and 2D-CNN; Hybrid 2, which combines 1D-CNN,\n2D-CNN, and a Transformer). For ECG classification, bootstrapping and Bayesian\ninference revealed that Hybrid 1 consistently outperformed the 2D-CNN baseline\nacross all metrics (p-values < 0.05, Bayesian probabilities > 0.90), confirming\nthe synergistic complementarity of the time and time-frequency domains.\nConversely, Hybrid 2's inclusion of the frequency domain offered no further\nimprovement and sometimes a marginal decline, indicating representational\nredundancy; a phenomenon further substantiated by a targeted ablation study.\nThis research redefines a fundamental principle of multimodal design in\nbiomedical signal analysis. We demonstrate that optimal domain fusion isn't\nabout the number of modalities, but the quality of their inherent\ncomplementarity. This paradigm-shifting concept moves beyond purely heuristic\nfeature selection. Our novel theoretical contribution, \"Complementary Feature\nDomains in Multimodal ECG Deep Learning,\" presents a mathematically\nquantifiable framework for identifying ideal domain combinations, demonstrating\nthat optimal multimodal performance arises from the intrinsic\ninformation-theoretic complementarity among fused domains.", "AI": {"tldr": "Optimal domain fusion isn't about the number of modalities, but the quality of their inherent complementarity.", "motivation": "While fusing multiple domains often presumes enhanced accuracy, this work demonstrates that adding modalities can yield diminishing returns, as not all fusions are inherently advantageous.", "method": "Five deep learning models were designed, developed, and rigorously evaluated: three unimodal (1D-CNN for time, 2D-CNN for time-frequency, and 1D-CNN-Transformer for frequency) and two multimodal (Hybrid 1, which fuses 1D-CNN and 2D-CNN; Hybrid 2, which combines 1D-CNN, 2D-CNN, and a Transformer).", "result": "Hybrid 1 consistently outperformed the 2D-CNN baseline across all metrics (p-values < 0.05, Bayesian probabilities > 0.90), confirming the synergistic complementarity of the time and time-frequency domains. Hybrid 2's inclusion of the frequency domain offered no further improvement and sometimes a marginal decline, indicating representational redundancy.", "conclusion": "Optimal domain fusion isn't about the number of modalities, but the quality of their inherent complementarity. Optimal multimodal performance arises from the intrinsic information-theoretic complementarity among fused domains."}}
{"id": "2508.02383", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.02383", "abs": "https://arxiv.org/abs/2508.02383", "authors": ["Changjie Sheng", "Zhichao Zhang", "Wei Yao"], "title": "Graph Embedding in the Graph Fractional Fourier Transform Domain", "comment": null, "summary": "Spectral graph embedding plays a critical role in graph representation\nlearning by generating low-dimensional vector representations from graph\nspectral information. However, the embedding space of traditional spectral\nembedding methods often exhibit limited expressiveness, failing to exhaustively\ncapture latent structural features across alternative transform domains. To\naddress this issue, we use the graph fractional Fourier transform to extend the\nexisting state-of-the-art generalized frequency filtering embedding (GEFFE)\ninto fractional domains, giving birth to the generalized fractional filtering\nembedding (GEFRFE), which enhances embedding informativeness via the graph\nfractional domain. The GEFRFE leverages graph fractional domain filtering and a\nnonlinear composition of eigenvector components derived from a fractionalized\ngraph Laplacian. To dynamically determine the fractional order, two parallel\nstrategies are introduced: search-based optimization and a ResNet18-based\nadaptive learning. Extensive experiments on six benchmark datasets demonstrate\nthat the GEFRFE captures richer structural features and significantly enhance\nclassification performance. Notably, the proposed method retains computational\ncomplexity comparable to GEFFE approaches.", "AI": {"tldr": "GEFRFE enhances embedding informativeness via the graph fractional domain, captures richer structural features and significantly enhance classification performance.", "motivation": "The embedding space of traditional spectral embedding methods often exhibit limited expressiveness, failing to exhaustively capture latent structural features across alternative transform domains.", "method": "graph fractional Fourier transform to extend the existing state-of-the-art generalized frequency filtering embedding (GEFFE) into fractional domains", "result": "significantly enhance classification performance on six benchmark datasets", "conclusion": "GEFRFE captures richer structural features and significantly enhance classification performance, while retaining comparable computational complexity to GEFFE approaches."}}
{"id": "2508.01656", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2508.01656", "abs": "https://arxiv.org/abs/2508.01656", "authors": ["Lucio La Cava", "Dominik Macko", "R\u00f3bert M\u00f3ro", "Ivan Srba", "Andrea Tagarelli"], "title": "Authorship Attribution in Multilingual Machine-Generated Texts", "comment": null, "summary": "As Large Language Models (LLMs) have reached human-like fluency and\ncoherence, distinguishing machine-generated text (MGT) from human-written\ncontent becomes increasingly difficult. While early efforts in MGT detection\nhave focused on binary classification, the growing landscape and diversity of\nLLMs require a more fine-grained yet challenging authorship attribution (AA),\ni.e., being able to identify the precise generator (LLM or human) behind a\ntext. However, AA remains nowadays confined to a monolingual setting, with\nEnglish being the most investigated one, overlooking the multilingual nature\nand usage of modern LLMs. In this work, we introduce the problem of\nMultilingual Authorship Attribution, which involves attributing texts to human\nor multiple LLM generators across diverse languages. Focusing on 18 languages\n-- covering multiple families and writing scripts -- and 8 generators (7 LLMs\nand the human-authored class), we investigate the multilingual suitability of\nmonolingual AA methods, their cross-lingual transferability, and the impact of\ngenerators on attribution performance. Our results reveal that while certain\nmonolingual AA methods can be adapted to multilingual settings, significant\nlimitations and challenges remain, particularly in transferring across diverse\nlanguage families, underscoring the complexity of multilingual AA and the need\nfor more robust approaches to better match real-world scenarios.", "AI": {"tldr": "\u6211\u4eec\u4ecb\u7ecd\u4e86\u591a\u8bed\u4f5c\u8005\u8eab\u4efd\u5f52\u5c5e\u95ee\u9898\uff0c\u8be5\u95ee\u9898\u6d89\u53ca\u5c06\u6587\u672c\u5f52\u56e0\u4e8e\u4e0d\u540c\u8bed\u8a00\u7684\u4eba\u7c7b\u6216\u591a\u4e2a LLM \u751f\u6210\u5668\u3002", "motivation": "\u533a\u5206\u673a\u5668\u751f\u6210\u7684\u6587\u672c (MGT) \u4e0e\u4eba\u7c7b\u64b0\u5199\u7684\u5185\u5bb9\u53d8\u5f97\u8d8a\u6765\u8d8a\u56f0\u96be\u3002\u867d\u7136\u65e9\u671f\u5728 MGT \u68c0\u6d4b\u65b9\u9762\u7684\u52aa\u529b\u90fd\u96c6\u4e2d\u5728\u4e8c\u5143\u5206\u7c7b\u4e0a\uff0c\u4f46 LLM \u4e0d\u65ad\u589e\u957f\u7684\u683c\u5c40\u548c\u591a\u6837\u6027\u9700\u8981\u66f4\u7cbe\u7ec6\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u4f5c\u8005\u8eab\u4efd\u5f52\u5c5e (AA)\uff0c\u5373\u80fd\u591f\u8bc6\u522b\u6587\u672c\u80cc\u540e\u7684\u7cbe\u786e\u751f\u6210\u5668\uff08LLM \u6216\u4eba\u7c7b\uff09\u3002", "method": "\u7814\u7a76\u4e86\u5355\u8bedAA\u65b9\u6cd5\u7684\u591a\u8bed\u9002\u7528\u6027\u3001\u8de8\u8bed\u8fc1\u79fb\u6027\u4ee5\u53ca\u751f\u6210\u5668\u5bf9\u5f52\u56e0\u6027\u80fd\u7684\u5f71\u54cd", "result": "\u67d0\u4e9b\u5355\u8bedAA\u65b9\u6cd5\u53ef\u4ee5\u9002\u5e94\u591a\u8bed\u73af\u5883\uff0c\u4f46\u4ecd\u5b58\u5728\u5f88\u5927\u7684\u5c40\u9650\u6027\u548c\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u8de8\u4e0d\u540c\u8bed\u7cfb\u8fdb\u884c\u8fc1\u79fb\u65f6", "conclusion": "\u67d0\u4e9b\u5355\u8bedAA\u65b9\u6cd5\u53ef\u4ee5\u9002\u5e94\u591a\u8bed\u73af\u5883\uff0c\u4f46\u4ecd\u5b58\u5728\u5f88\u5927\u7684\u5c40\u9650\u6027\u548c\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u8de8\u4e0d\u540c\u8bed\u7cfb\u8fdb\u884c\u8fc1\u79fb\u65f6\uff0c\u8fd9\u7a81\u663e\u4e86\u591a\u8bedAA\u7684\u590d\u6742\u6027\uff0c\u5e76\u4e14\u9700\u8981\u66f4\u5f3a\u5927\u7684\u65b9\u6cd5\u6765\u66f4\u597d\u5730\u5339\u914d\u771f\u5b9e\u4e16\u754c\u7684\u573a\u666f\u3002"}}
{"id": "2508.01197", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.01197", "abs": "https://arxiv.org/abs/2508.01197", "authors": ["Zhan Shi", "Song Wang", "Junbo Chen", "Jianke Zhu"], "title": "A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding", "comment": "IROS 2025 Accepted Paper", "summary": "Visual grounding aims to identify objects or regions in a scene based on\nnatural language descriptions, essential for spatially aware perception in\nautonomous driving. However, existing visual grounding tasks typically depend\non bounding boxes that often fail to capture fine-grained details. Not all\nvoxels within a bounding box are occupied, resulting in inaccurate object\nrepresentations. To address this, we introduce a benchmark for 3D occupancy\ngrounding in challenging outdoor scenes. Built on the nuScenes dataset, it\nintegrates natural language with voxel-level occupancy annotations, offering\nmore precise object perception compared to the traditional grounding task.\nMoreover, we propose GroundingOcc, an end-to-end model designed for 3D\noccupancy grounding through multi-modal learning. It combines visual, textual,\nand point cloud features to predict object location and occupancy information\nfrom coarse to fine. Specifically, GroundingOcc comprises a multimodal encoder\nfor feature extraction, an occupancy head for voxel-wise predictions, and a\ngrounding head to refine localization. Additionally, a 2D grounding module and\na depth estimation module enhance geometric understanding, thereby boosting\nmodel performance. Extensive experiments on the benchmark demonstrate that our\nmethod outperforms existing baselines on 3D occupancy grounding. The dataset is\navailable at https://github.com/RONINGOD/GroundingOcc.", "AI": {"tldr": "This paper introduces a 3D occupancy grounding benchmark using the nuScenes dataset and proposes GroundingOcc, a new model that outperforms existing methods by using multi-modal learning and fine-grained voxel-level occupancy annotations.", "motivation": "Existing visual grounding tasks rely on bounding boxes that fail to capture fine-grained details, leading to inaccurate object representations.", "method": "The paper proposes GroundingOcc, an end-to-end model for 3D occupancy grounding that combines visual, textual, and point cloud features. It includes a multimodal encoder, an occupancy head, a grounding head, a 2D grounding module, and a depth estimation module.", "result": "The paper introduces a new benchmark for 3D occupancy grounding in outdoor scenes based on the nuScenes dataset and demonstrates that the proposed GroundingOcc method outperforms existing baselines.", "conclusion": "The proposed GroundingOcc method outperforms existing baselines on 3D occupancy grounding, as demonstrated by experiments on the introduced benchmark."}}
{"id": "2508.01475", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01475", "abs": "https://arxiv.org/abs/2508.01475", "authors": ["Zhen Wu", "Ritam Dutt", "Luke M. Breitfeller", "Armineh Nourbakhsh", "Siddharth Parekh", "Carolyn Ros\u00e9"], "title": "$R^2$-CoD: Understanding Text-Graph Complementarity in Relational Reasoning via Knowledge Co-Distillation", "comment": null, "summary": "Relational reasoning lies at the core of many NLP tasks, drawing on\ncomplementary signals from text and graphs. While prior research has\ninvestigated how to leverage this dual complementarity, a detailed and\nsystematic understanding of text-graph interplay and its effect on hybrid\nmodels remains underexplored. We take an analysis-driven approach to\ninvestigate text-graph representation complementarity via a unified\narchitecture that supports knowledge co-distillation (CoD). We explore five\ntasks involving relational reasoning that differ in how text and graph\nstructures encode the information needed to solve that task. By tracking how\nthese dual representations evolve during training, we uncover interpretable\npatterns of alignment and divergence, and provide insights into when and why\ntheir integration is beneficial.", "AI": {"tldr": "\u6211\u4eec\u901a\u8fc7\u4e00\u4e2a\u7edf\u4e00\u7684\u67b6\u6784\u7814\u7a76\u4e86\u6587\u672c-\u56fe\u8868\u793a\u4e92\u8865\u6027\uff0c\u8be5\u67b6\u6784\u652f\u6301\u77e5\u8bc6\u534f\u540c\u63d0\u70bc (CoD)\u3002", "motivation": "\u5173\u7cfb\u63a8\u7406\u662f\u8bb8\u591a NLP \u4efb\u52a1\u7684\u6838\u5fc3\uff0c\u5b83\u5229\u7528\u6765\u81ea\u6587\u672c\u548c\u56fe\u7684\u4e92\u8865\u4fe1\u53f7\u3002\u867d\u7136\u4e4b\u524d\u7684\u7814\u7a76\u5df2\u7ecf\u8c03\u67e5\u4e86\u5982\u4f55\u5229\u7528\u8fd9\u79cd\u53cc\u91cd\u4e92\u8865\u6027\uff0c\u4f46\u5bf9\u6587\u672c-\u56fe\u76f8\u4e92\u4f5c\u7528\u53ca\u5176\u5bf9\u6df7\u5408\u6a21\u578b\u7684\u5f71\u54cd\u7684\u8be6\u7ec6\u548c\u7cfb\u7edf\u7684\u7406\u89e3\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u6211\u4eec\u91c7\u7528\u5206\u6790\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u652f\u6301\u77e5\u8bc6\u534f\u540c\u63d0\u70bc (CoD) \u7684\u7edf\u4e00\u67b6\u6784\u6765\u7814\u7a76\u6587\u672c-\u56fe\u8868\u793a\u4e92\u8865\u6027\u3002", "result": "\u6211\u4eec\u63a2\u7d22\u4e86\u4e94\u4e2a\u6d89\u53ca\u5173\u7cfb\u63a8\u7406\u7684\u4efb\u52a1\uff0c\u8fd9\u4e9b\u4efb\u52a1\u5728\u6587\u672c\u548c\u56fe\u7ed3\u6784\u5982\u4f55\u7f16\u7801\u89e3\u51b3\u8be5\u4efb\u52a1\u6240\u9700\u7684\u4fe1\u606f\u65b9\u9762\u6709\u6240\u4e0d\u540c\u3002", "conclusion": "\u901a\u8fc7\u8ddf\u8e2a\u8fd9\u4e9b\u53cc\u91cd\u8868\u5f81\u5728\u8bad\u7ec3\u671f\u95f4\u7684\u6f14\u53d8\uff0c\u6211\u4eec\u63ed\u793a\u4e86\u5bf9\u9f50\u548c\u5dee\u5f02\u7684\u53ef\u89e3\u91ca\u6a21\u5f0f\uff0c\u5e76\u6df1\u5165\u4e86\u89e3\u4e86\u4f55\u65f6\u4ee5\u53ca\u4e3a\u4ec0\u4e48\u5b83\u4eec\u7684\u96c6\u6210\u662f\u6709\u76ca\u7684\u3002"}}
{"id": "2508.00965", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00965", "abs": "https://arxiv.org/abs/2508.00965", "authors": ["Roie Kazoom", "Ofir Cohen", "Rami Puzis", "Asaf Shabtai", "Ofer Hadar"], "title": "VAULT: Vigilant Adversarial Updates via LLM-Driven Retrieval-Augmented Generation for NLI", "comment": null, "summary": "We introduce VAULT, a fully automated adversarial RAG pipeline that\nsystematically uncovers and remedies weaknesses in NLI models through three\nstages: retrieval, adversarial generation, and iterative retraining. First, we\nperform balanced few-shot retrieval by embedding premises with both semantic\n(BGE) and lexical (BM25) similarity. Next, we assemble these contexts into LLM\nprompts to generate adversarial hypotheses, which are then validated by an LLM\nensemble for label fidelity. Finally, the validated adversarial examples are\ninjected back into the training set at increasing mixing ratios, progressively\nfortifying a zero-shot RoBERTa-base model.On standard benchmarks, VAULT\nelevates RoBERTa-base accuracy from 88.48% to 92.60% on SNLI +4.12%, from\n75.04% to 80.95% on ANLI +5.91%, and from 54.67% to 71.99% on MultiNLI +17.32%.\nIt also consistently outperforms prior in-context adversarial methods by up to\n2.0% across datasets. By automating high-quality adversarial data curation at\nscale, VAULT enables rapid, human-independent robustness improvements in NLI\ninference tasks.", "AI": {"tldr": "VAULT\u662f\u4e00\u4e2a\u5168\u81ea\u52a8\u7684\u5bf9\u6297\u6027RAG\u6d41\u7a0b\uff0c\u901a\u8fc7\u68c0\u7d22\u3001\u751f\u6210\u548c\u8fed\u4ee3\u91cd\u65b0\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u9ad8\u4e86NLI\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709NLI\u6a21\u578b\u5b58\u5728\u5f31\u70b9\uff0cVAULT\u65e8\u5728\u53d1\u73b0\u5e76\u5f25\u8865\u8fd9\u4e9b\u5f31\u70b9\u3002", "method": "VAULT\u4f7f\u7528\u5305\u542b\u8bed\u4e49\u548c\u8bcd\u6c47\u76f8\u4f3c\u6027\u7684\u5d4c\u5165\u68c0\u7d22\u3001LLM\u63d0\u793a\u751f\u6210\u5bf9\u6297\u5047\u8bbe\uff0c\u5e76\u901a\u8fc7LLM\u96c6\u6210\u9a8c\u8bc1\u6807\u7b7e\u4fdd\u771f\u5ea6\u3002", "result": "VAULT\u5728SNLI\u4e0a\u5c06RoBERTa-base\u51c6\u786e\u7387\u4ece88.48%\u63d0\u5347\u523092.60%\uff0c\u5728ANLI\u4e0a\u4ece75.04%\u63d0\u5347\u523080.95%\uff0c\u5728MultiNLI\u4e0a\u4ece54.67%\u63d0\u5347\u523071.99%\uff0c\u5e76\u4e14\u59cb\u7ec8\u4f18\u4e8e\u5148\u524d\u7684\u4e0a\u4e0b\u6587\u5bf9\u6297\u65b9\u6cd5\u3002", "conclusion": "VAULT\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u5bf9\u6297\u6837\u672c\u5e76\u8fed\u4ee3\u91cd\u65b0\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u9ad8\u4e86RoBERTa-base\u6a21\u578b\u5728SNLI\u3001ANLI\u548cMultiNLI\u6570\u636e\u96c6\u4e0a\u7684\u51c6\u786e\u7387\uff0c\u4e14\u4f18\u4e8e\u73b0\u6709\u7684\u4e0a\u4e0b\u6587\u5bf9\u6297\u65b9\u6cd5\u3002"}}
{"id": "2508.01674", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.01674", "abs": "https://arxiv.org/abs/2508.01674", "authors": ["Tae Soo Kim", "Yoonjoo Lee", "Yoonah Park", "Jiho Kim", "Young-Ho Kim", "Juho Kim"], "title": "CUPID: Evaluating Personalized and Contextualized Alignment of LLMs from Interactions", "comment": "Accepted to COLM 2025. Project Website: https://cupid.kixlab.org/", "summary": "Personalization of Large Language Models (LLMs) often assumes users hold\nstatic preferences that reflect globally in all tasks. In reality, humans hold\ndynamic preferences that change depending on the context. As users interact\nwith an LLM in various contexts, they naturally reveal their contextual\npreferences, which a model must infer and apply in future contexts to ensure\nalignment. To assess this, we introduce CUPID, a benchmark of 756 human-curated\ninteraction session histories between users and LLM-based chat assistants. In\neach interaction session, the user provides a request in a specific context and\nexpresses their preference through multi-turn feedback. Given a new user\nrequest and prior interaction sessions, our benchmark assesses whether LLMs can\ninfer the preference relevant to this request and generate a response that\nsatisfies this preference. With CUPID, we evaluated 10 open and proprietary\nLLMs, revealing that state-of-the-art LLMs struggle to infer preferences from\nmulti-turn interactions and fail to discern what previous context is relevant\nto a new request -- under 50% precision and 65% recall. Our work highlights the\nneed to advance LLM capabilities for more contextually personalized\ninteractions and proposes CUPID as a resource to drive these improvements.", "AI": {"tldr": "CUPID\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0c\u5f53\u524d\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u548c\u5e94\u7528\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u7528\u6237\u504f\u597d\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u8fd9\u8868\u660e\u9700\u8981\u8fdb\u4e00\u6b65\u53d1\u5c55\u4ee5\u5b9e\u73b0\u66f4\u4e2a\u6027\u5316\u7684\u4e92\u52a8\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u4e2a\u6027\u5316\u901a\u5e38\u5047\u8bbe\u7528\u6237\u6301\u6709\u9759\u6001\u504f\u597d\uff0c\u8fd9\u4e9b\u504f\u597d\u5728\u5168\u7403\u6240\u6709\u4efb\u52a1\u4e2d\u90fd\u6709\u4f53\u73b0\u3002 \u5b9e\u9645\u4e0a\uff0c\u4eba\u7c7b\u6301\u6709\u52a8\u6001\u504f\u597d\uff0c\u8fd9\u4e9b\u504f\u597d\u4f1a\u6839\u636e\u4e0a\u4e0b\u6587\u800c\u53d8\u5316\u3002 \u7528\u6237\u5728\u5404\u79cd\u4e0a\u4e0b\u6587\u4e2d\u4e0e\u6cd5\u5b66\u7855\u58eb\u4e92\u52a8\u65f6\uff0c\u4f1a\u81ea\u7136\u5730\u8868\u8fbe\u4ed6\u4eec\u7684\u60c5\u5883\u504f\u597d\uff0c\u6a21\u578b\u5fc5\u987b\u63a8\u65ad\u8fd9\u4e9b\u504f\u597d\u5e76\u5c06\u5176\u5e94\u7528\u5230\u672a\u6765\u7684\u60c5\u5883\u4e2d\uff0c\u4ee5\u786e\u4fdd\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCUPID\u7684\u57fa\u51c6\uff0c\u5176\u4e2d\u5305\u542b\u7528\u6237\u548c\u57fa\u4e8eLLM\u7684\u804a\u5929\u52a9\u624b\u4e4b\u95f4756\u4e2a\u4eba\u5de5\u7b56\u5212\u7684\u4ea4\u4e92\u4f1a\u8bdd\u5386\u53f2\u3002", "result": "\u5bf9 10 \u4e2a\u5f00\u653e\u548c\u4e13\u6709\u7684\u6cd5\u5b66\u7855\u58eb\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u6700\u5148\u8fdb\u7684\u6cd5\u5b66\u7855\u58eb\u96be\u4ee5\u4ece\u591a\u8f6e\u4ea4\u4e92\u4e2d\u63a8\u65ad\u504f\u597d\uff0c\u5e76\u4e14\u65e0\u6cd5\u8fa8\u522b\u54ea\u4e9b\u5148\u524d\u7684\u4e0a\u4e0b\u6587\u4e0e\u65b0\u7684\u8bf7\u6c42\u76f8\u5173\u2014\u2014\u51c6\u786e\u7387\u4f4e\u4e8e 50%\uff0c\u53ec\u56de\u7387\u4f4e\u4e8e 65%\u3002", "conclusion": "\u5f53\u524d\u7684\u5927\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u4ece\u591a\u8f6e\u4ea4\u4e92\u4e2d\u63a8\u65ad\u504f\u597d\uff0c\u5e76\u4e14\u65e0\u6cd5\u8fa8\u522b\u54ea\u4e9b\u5148\u524d\u7684\u4e0a\u4e0b\u6587\u4e0e\u65b0\u7684\u8bf7\u6c42\u76f8\u5173\u3002\u9700\u8981\u63d0\u9ad8\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u4e2a\u6027\u5316\u4ea4\u4e92\u65b9\u9762\u7684\u80fd\u529b\u3002"}}
{"id": "2508.01206", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01206", "abs": "https://arxiv.org/abs/2508.01206", "authors": ["Prathyush Kumar Reddy Lebaku", "Lu Gao", "Pan Lu", "Jingran Sun"], "title": "Deep Learning for Pavement Condition Evaluation Using Satellite Imagery", "comment": null, "summary": "Civil infrastructure systems covers large land areas and needs frequent\ninspections to maintain their public service capabilities. The conventional\napproaches of manual surveys or vehicle-based automated surveys to assess\ninfrastructure conditions are often labor-intensive and time-consuming. For\nthis reason, it is worthwhile to explore more cost-effective methods for\nmonitoring and maintaining these infrastructures. Fortunately, recent\nadvancements in satellite systems and image processing algorithms have opened\nup new possibilities. Numerous satellite systems have been employed to monitor\ninfrastructure conditions and identify damages. Due to the improvement in\nground sample distance (GSD), the level of detail that can be captured has\nsignificantly increased. Taking advantage of these technology advancement, this\nresearch investigated to evaluate pavement conditions using deep learning\nmodels for analyzing satellite images. We gathered over 3,000 satellite images\nof pavement sections, together with pavement evaluation ratings from TxDOT's\nPMIS database. The results of our study show an accuracy rate is exceeding 90%.\nThis research paves the way for a rapid and cost-effective approach to\nevaluating the pavement network in the future.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5206\u6790\u536b\u661f\u56fe\u50cf\u8bc4\u4f30\u8def\u9762\u72b6\u51b5\uff0c\u51c6\u786e\u7387\u8d85\u8fc7 90%\uff0c\u4e3a\u672a\u6765\u5feb\u901f\u4e14\u7ecf\u6d4e\u9ad8\u6548\u5730\u8bc4\u4f30\u8def\u9762\u7f51\u7edc\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u4f20\u7edf\u7684\u8def\u9762\u68c0\u6d4b\u65b9\u6cd5\u8017\u65f6\u8017\u529b\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u63a2\u7d22\u66f4\u7ecf\u6d4e\u6709\u6548\u7684\u65b9\u6cd5\u6765\u76d1\u6d4b\u548c\u7ef4\u62a4\u8fd9\u4e9b\u57fa\u7840\u8bbe\u65bd\u3002", "method": "\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5206\u6790\u536b\u661f\u56fe\u50cf\u8bc4\u4f30\u8def\u9762\u72b6\u51b5", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u51c6\u786e\u7387\u8d85\u8fc7 90%\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u672a\u6765\u8bc4\u4f30\u8def\u9762\u7f51\u7edc\u94fa\u5e73\u4e86\u4e00\u6761\u5feb\u901f\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u9053\u8def\u3002"}}
{"id": "2508.01476", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01476", "abs": "https://arxiv.org/abs/2508.01476", "authors": ["Arindam Khanda", "Anurag Satpathy", "Amit Jha", "Sajal K. Das"], "title": "CARGO: A Co-Optimization Framework for EV Charging and Routing in Goods Delivery Logistics", "comment": null, "summary": "With growing interest in sustainable logistics, electric vehicle (EV)-based\ndeliveries offer a promising alternative for urban distribution. However, EVs\nface challenges due to their limited battery capacity, requiring careful\nplanning for recharging. This depends on factors such as the charging point\n(CP) availability, cost, proximity, and vehicles' state of charge (SoC). We\npropose CARGO, a framework addressing the EV-based delivery route planning\nproblem (EDRP), which jointly optimizes route planning and charging for\ndeliveries within time windows. After proving the problem's NP-hardness, we\npropose a mixed integer linear programming (MILP)-based exact solution and a\ncomputationally efficient heuristic method. Using real-world datasets, we\nevaluate our methods by comparing the heuristic to the MILP solution, and\nbenchmarking it against baseline strategies, Earliest Deadline First (EDF) and\nNearest Delivery First (NDF). The results show up to 39% and 22% reductions in\nthe charging cost over EDF and NDF, respectively, while completing comparable\ndeliveries.", "AI": {"tldr": "This paper proposes CARGO, a framework addressing the EV-based delivery route planning problem (EDRP), which jointly optimizes route planning and charging for deliveries within time windows.", "motivation": "EVs face challenges due to their limited battery capacity, requiring careful planning for recharging. This depends on factors such as the charging point (CP) availability, cost, proximity, and vehicles' state of charge (SoC).", "method": "a mixed integer linear programming (MILP)-based exact solution and a computationally efficient heuristic method", "result": "Using real-world datasets, we evaluate our methods by comparing the heuristic to the MILP solution, and benchmarking it against baseline strategies, Earliest Deadline First (EDF) and Nearest Delivery First (NDF).", "conclusion": "The results show up to 39% and 22% reductions in the charging cost over EDF and NDF, respectively, while completing comparable deliveries."}}
{"id": "2508.00969", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00969", "abs": "https://arxiv.org/abs/2508.00969", "authors": ["Lucas Robinet", "Ahmad Berjaoui", "Elizabeth Cohen-Jonathan Moyal"], "title": "Masked Omics Modeling for Multimodal Representation Learning across Histopathology and Molecular Profiles", "comment": null, "summary": "Self-supervised learning has driven major advances in computational pathology\nby enabling models to learn rich representations from hematoxylin and eosin\n(H&E)-stained cancer tissue. However, histopathology alone often falls short\nfor molecular characterization and understanding clinical outcomes, as\nimportant information is contained in high-dimensional omics profiles like\ntranscriptomics, methylomics, or genomics. In this work, we introduce MORPHEUS,\na unified transformer-based pre-training framework that encodes both\nhistopathology and multi-omics data into a shared latent space. At its core,\nMORPHEUS relies on a masked modeling objective applied to randomly selected\nomics portions, encouraging the model to learn biologically meaningful\ncross-modal relationships. The same pre-trained network can be applied to\nhistopathology alone or in combination with any subset of omics modalities,\nseamlessly adapting to the available inputs. Additionally, MORPHEUS enables\nany-to-any omics generation, enabling one or more omics profiles to be inferred\nfrom any subset of modalities, including H&E alone. Pre-trained on a large\npan-cancer cohort, MORPHEUS consistently outperforms state-of-the-art methods\nacross diverse modality combinations and tasks, positioning itself as a\npromising framework for developing multimodal foundation models in oncology.\nThe code is available at: https://github.com/Lucas-rbnt/MORPHEUS", "AI": {"tldr": "MORPHEUS \u662f\u4e00\u4e2a\u7edf\u4e00\u7684 Transformer \u6846\u67b6\uff0c\u5b83\u4f7f\u7528\u7ec4\u7ec7\u75c5\u7406\u5b66\u548c\u591a\u7ec4\u5b66\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u4ee5\u5b9e\u73b0\u5353\u8d8a\u7684\u764c\u75c7\u5206\u6790\u6027\u80fd\u3002", "motivation": "\u7ec4\u7ec7\u75c5\u7406\u5b66\u901a\u5e38\u4e0d\u8db3\u4ee5\u8fdb\u884c\u5206\u5b50\u8868\u5f81\u548c\u7406\u89e3\u4e34\u5e8a\u7ed3\u679c\uff0c\u56e0\u4e3a\u91cd\u8981\u4fe1\u606f\u5305\u542b\u5728\u9ad8\u7ef4\u7ec4\u5b66\u56fe\u8c31\uff08\u5982\u8f6c\u5f55\u7ec4\u5b66\u3001\u7532\u57fa\u7ec4\u5b66\u6216\u57fa\u56e0\u7ec4\u5b66\uff09\u4e2d\u3002", "method": "\u7edf\u4e00\u7684\u57fa\u4e8e Transformer \u7684\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u53ef\u5c06\u7ec4\u7ec7\u75c5\u7406\u5b66\u548c\u591a\u7ec4\u5b66\u6570\u636e\u7f16\u7801\u5230\u5171\u4eab\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u3002MORPHEUS \u4f9d\u8d56\u4e8e\u5e94\u7528\u4e8e\u968f\u673a\u9009\u62e9\u7684\u7ec4\u5b66\u90e8\u5206\u7684 masked modeling \u76ee\u6807\uff0c\u9f13\u52b1\u6a21\u578b\u5b66\u4e60\u5177\u6709\u751f\u7269\u5b66\u610f\u4e49\u7684\u8de8\u6a21\u6001\u5173\u7cfb\u3002", "result": "MORPHEUS \u80fd\u591f\u5b9e\u73b0\u4efb\u610f\u5230\u4efb\u610f\u7684\u7ec4\u5b66\u751f\u6210\uff0c\u4ece\u800c\u80fd\u591f\u4ece\u4efb\u4f55\u6a21\u6001\u5b50\u96c6\u4e2d\u63a8\u65ad\u51fa\u4e00\u4e2a\u6216\u591a\u4e2a\u7ec4\u5b66\u56fe\u8c31\uff0c\u5305\u62ec\u4ec5 H&E\u3002\u5728\u5927\u578b\u6cdb\u764c\u961f\u5217\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u540e\uff0cMORPHEUS \u5728\u5404\u79cd\u6a21\u6001\u7ec4\u5408\u548c\u4efb\u52a1\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "MORPHEUS \u5728\u5404\u79cd\u6a21\u6001\u7ec4\u5408\u548c\u4efb\u52a1\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5b9a\u4f4d\u4e3a\u5728\u80bf\u7624\u5b66\u4e2d\u5f00\u53d1\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7684\u6709\u524d\u9014\u7684\u6846\u67b6\u3002"}}
{"id": "2508.01682", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01682", "abs": "https://arxiv.org/abs/2508.01682", "authors": ["Lingyin Zhang", "Jun Gao", "Xiaoxue Ren", "Ziqiang Cao"], "title": "The Bidirectional Process Reward Model", "comment": null, "summary": "Process Reward Models (PRMs) have emerged as a promising approach to enhance\nthe reasoning quality of Large Language Models (LLMs) by assigning fine-grained\nscores to intermediate reasoning steps within a solution trajectory. However,\nexisting PRMs predominantly adopt a unidirectional left-to-right (L2R)\nevaluation paradigm, which limits their ability to leverage global context,\nmaking it challenging to verify the consistency of earlier steps based on later\nones. In light of these challenges, we propose a novel bidirectional evaluation\nparadigm, named Bidirectional Process Reward Model (BiPRM). BiPRM seamlessly\nincorporates a parallel right-to-left (R2L) evaluation stream alongside the\nconventional L2R flow, enabling later reasoning steps to help assess earlier\nones in real time. Notably, the built-in R2L evaluation is implemented solely\nthrough prompt modifications that reverse the original reasoning trajectory,\nwithout any additional parameters or inference latency introduced. This ensures\nBiPRM remains both efficient and broadly compatible with existing PRM studies.\nWe conduct extensive experiments on two mathematical reasoning benchmarks using\nsamples generated by three different policy models. Our method, BiPRM, is\nevaluated across three backbones and three distinct PRM objectives. Across all\nsettings, BiPRM consistently outperforms unidirectional baselines, achieving up\nto a 31.9% improvement in stepwise reward evaluation. Generally, our results\nhighlight BiPRM's effectiveness, robustness, and general applicability,\noffering a promising new direction for process-based reward modeling.", "AI": {"tldr": "BiPRM \u662f\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u5411\u8bc4\u4f30\u8303\u5f0f\uff0c\u5b83\u901a\u8fc7\u7ed3\u5408\u4ece\u53f3\u5230\u5de6\u7684\u8bc4\u4f30\u6d41\u6765\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b \uff08LLM\uff09 \u7684\u63a8\u7406\u8d28\u91cf\uff0c\u4ece\u800c\u80fd\u591f\u6839\u636e\u540e\u9762\u7684\u6b65\u9aa4\u5b9e\u65f6\u8bc4\u4f30\u65e9\u671f\u7684\u6b65\u9aa4\uff0c\u800c\u65e0\u9700\u4efb\u4f55\u989d\u5916\u7684\u53c2\u6570\u6216\u63a8\u7406\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709\u7684 PRM \u4e3b\u8981\u91c7\u7528\u5355\u5411\u4ece\u5de6\u5230\u53f3 \uff08L2R\uff09 \u8bc4\u4f30\u8303\u5f0f\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5229\u7528\u5168\u5c40\u4e0a\u4e0b\u6587\u7684\u80fd\u529b\uff0c\u4ece\u800c\u96be\u4ee5\u6839\u636e\u540e\u9762\u7684\u6b65\u9aa4\u9a8c\u8bc1\u65e9\u671f\u6b65\u9aa4\u7684\u4e00\u81f4\u6027\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u5411\u8bc4\u4f30\u8303\u5f0f\uff0c\u540d\u4e3a\u53cc\u5411\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b \uff08BiPRM\uff09\u3002BiPRM \u65e0\u7f1d\u5730\u7ed3\u5408\u4e86\u4e0e\u4f20\u7edf L2R \u6d41\u5e73\u884c\u7684\u4ece\u53f3\u5230\u5de6 \uff08R2L\uff09 \u8bc4\u4f30\u6d41\uff0c\u4f7f\u540e\u9762\u7684\u63a8\u7406\u6b65\u9aa4\u80fd\u591f\u5e2e\u52a9\u5b9e\u65f6\u8bc4\u4f30\u8f83\u65e9\u7684\u63a8\u7406\u6b65\u9aa4\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5185\u7f6e\u7684 R2L \u8bc4\u4f30\u4ec5\u901a\u8fc7\u63d0\u793a\u4fee\u6539\u6765\u5b9e\u73b0\uff0c\u8fd9\u4e9b\u4fee\u6539\u53cd\u8f6c\u4e86\u539f\u59cb\u63a8\u7406\u8f68\u8ff9\uff0c\u800c\u65e0\u9700\u5f15\u5165\u4efb\u4f55\u989d\u5916\u7684\u53c2\u6570\u6216\u63a8\u7406\u5ef6\u8fdf\u3002\u8fd9\u786e\u4fdd\u4e86 BiPRM \u65e2\u9ad8\u6548\u53c8\u4e0e\u73b0\u6709\u7684 PRM \u7814\u7a76\u5e7f\u6cdb\u517c\u5bb9\u3002", "result": "\u5728\u4e24\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u4f7f\u7528\u7531\u4e09\u4e2a\u4e0d\u540c\u7684\u7b56\u7565\u6a21\u578b\u751f\u6210\u7684\u6837\u672c\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u3002\u6211\u4eec\u7684\u65b9\u6cd5 BiPRM \u5728\u4e09\u4e2a\u9aa8\u5e72\u7f51\u548c\u4e09\u4e2a\u4e0d\u540c\u7684 PRM \u76ee\u6807\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u5728\u6240\u6709\u8bbe\u7f6e\u4e2d\uff0cBiPRM \u59cb\u7ec8\u4f18\u4e8e\u5355\u5411\u57fa\u7ebf\uff0c\u5728\u9010\u6b65\u5956\u52b1\u8bc4\u4f30\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe 31.9% \u7684\u6539\u8fdb\u3002", "conclusion": "BiPRM\u5728\u6240\u6709\u8bbe\u7f6e\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u5355\u5411\u57fa\u7ebf\uff0c\u5728\u9010\u6b65\u5956\u52b1\u8bc4\u4f30\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe 31.9% \u7684\u6539\u8fdb\u3002\u603b\u7684\u6765\u8bf4\uff0c\u6211\u4eec\u7684\u7ed3\u679c\u7a81\u51fa\u4e86 BiPRM \u7684\u6709\u6548\u6027\u3001\u7a33\u5065\u6027\u548c\u666e\u904d\u9002\u7528\u6027\uff0c\u4e3a\u57fa\u4e8e\u8fc7\u7a0b\u7684\u5956\u52b1\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u5e0c\u671b\u7684\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.01210", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01210", "abs": "https://arxiv.org/abs/2508.01210", "authors": ["Tianze Wang", "Zhang Zhang", "Chao Yue", "Nuoran Li", "Chao Sun"], "title": "RoadMamba: A Dual Branch Visual State Space Model for Road Surface Classification", "comment": null, "summary": "Acquiring the road surface conditions in advance based on visual technologies\nprovides effective information for the planning and control system of\nautonomous vehicles, thus improving the safety and driving comfort of the\nvehicles. Recently, the Mamba architecture based on state-space models has\nshown remarkable performance in visual processing tasks, benefiting from the\nefficient global receptive field. However, existing Mamba architectures\nstruggle to achieve state-of-the-art visual road surface classification due to\ntheir lack of effective extraction of the local texture of the road surface. In\nthis paper, we explore for the first time the potential of visual Mamba\narchitectures for road surface classification task and propose a method that\neffectively combines local and global perception, called RoadMamba.\nSpecifically, we utilize the Dual State Space Model (DualSSM) to effectively\nextract the global semantics and local texture of the road surface and decode\nand fuse the dual features through the Dual Attention Fusion (DAF). In\naddition, we propose a dual auxiliary loss to explicitly constrain dual\nbranches, preventing the network from relying only on global semantic\ninformation from the deep large receptive field and ignoring the local texture.\nThe proposed RoadMamba achieves the state-of-the-art performance in experiments\non a large-scale road surface classification dataset containing 1 million\nsamples.", "AI": {"tldr": "RoadMamba, a novel Mamba-based architecture, is proposed for road surface classification, achieving state-of-the-art performance by combining local and global perception with DualSSM and DAF.", "motivation": "Existing Mamba architectures lack effective extraction of the local texture of the road surface, hindering their performance in visual road surface classification.", "method": "A method called RoadMamba that effectively combines local and global perception using Dual State Space Model (DualSSM) to extract global semantics and local texture, and Dual Attention Fusion (DAF) to decode and fuse dual features. A dual auxiliary loss is also proposed to constrain dual branches.", "result": "RoadMamba achieves state-of-the-art performance in road surface classification.", "conclusion": "The proposed RoadMamba achieves state-of-the-art performance on a large-scale road surface classification dataset."}}
{"id": "2508.01495", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01495", "abs": "https://arxiv.org/abs/2508.01495", "authors": ["Jingtian Yan", "Stephen F. Smith", "Jiaoyang Li"], "title": "WinkTPG: An Execution Framework for Multi-Agent Path Finding Using Temporal Reasoning", "comment": null, "summary": "Planning collision-free paths for a large group of agents is a challenging\nproblem with numerous real-world applications. While recent advances in\nMulti-Agent Path Finding (MAPF) have shown promising progress, standard MAPF\nalgorithms rely on simplified kinodynamic models, preventing agents from\ndirectly following the generated MAPF plan. To bridge this gap, we propose\nkinodynamic Temporal Plan Graph Planning (kTPG), a multi-agent speed\noptimization algorithm that efficiently refines a MAPF plan into a\nkinodynamically feasible plan while accounting for uncertainties and preserving\ncollision-freeness. Building on kTPG, we propose Windowed kTPG (WinkTPG), a\nMAPF execution framework that incrementally refines MAPF plans using a\nwindow-based mechanism, dynamically incorporating agent information during\nexecution to reduce uncertainty. Experiments show that WinkTPG can generate\nspeed profiles for up to 1,000 agents in 1 second and improves solution quality\nby up to 51.7% over existing MAPF execution methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Windowed kTPG (WinkTPG) \u7684 MAPF \u6267\u884c\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u53ef\u4ee5\u6709\u6548\u5730\u5c06 MAPF \u8ba1\u5212\u4f18\u5316\u4e3a\u8fd0\u52a8\u5b66\u4e0a\u53ef\u884c\u7684\u8ba1\u5212\uff0c\u540c\u65f6\u8003\u8651\u4e0d\u786e\u5b9a\u6027\u5e76\u4fdd\u6301\u65e0\u78b0\u649e\u6027\u3002", "motivation": "\u4e3a\u4e00\u5927\u7fa4\u667a\u80fd\u4f53\u89c4\u5212\u65e0\u78b0\u649e\u8def\u5f84\u662f\u4e00\u4e2a\u5177\u6709\u4f17\u591a\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u7684\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\u3002\u867d\u7136\u591a\u667a\u80fd\u4f53\u8def\u5f84\u5bfb\u627e (MAPF) \u7684\u6700\u65b0\u8fdb\u5c55\u663e\u793a\u51fa\u4e86\u53ef\u559c\u7684\u8fdb\u5c55\uff0c\u4f46\u6807\u51c6 MAPF \u7b97\u6cd5\u4f9d\u8d56\u4e8e\u7b80\u5316\u7684\u8fd0\u52a8\u5b66\u6a21\u578b\uff0c\u4ece\u800c\u963b\u6b62\u667a\u80fd\u4f53\u76f4\u63a5\u9075\u5faa\u751f\u6210\u7684 MAPF \u8ba1\u5212\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u8fd0\u52a8\u5b66\u65f6\u95f4\u89c4\u5212\u56fe\u89c4\u5212 (kTPG)\uff0c\u8fd9\u662f\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u901f\u5ea6\u4f18\u5316\u7b97\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u5c06 MAPF \u8ba1\u5212\u4f18\u5316\u4e3a\u8fd0\u52a8\u5b66\u4e0a\u53ef\u884c\u7684\u8ba1\u5212\uff0c\u540c\u65f6\u8003\u8651\u4e0d\u786e\u5b9a\u6027\u5e76\u4fdd\u6301\u65e0\u78b0\u649e\u6027\u3002\u5728 kTPG \u7684\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 Windowed kTPG (WinkTPG)\uff0c\u8fd9\u662f\u4e00\u4e2a MAPF \u6267\u884c\u6846\u67b6\uff0c\u5b83\u4f7f\u7528\u57fa\u4e8e\u7a97\u53e3\u7684\u673a\u5236\u589e\u91cf\u5730\u4f18\u5316 MAPF \u8ba1\u5212\uff0c\u5728\u6267\u884c\u671f\u95f4\u52a8\u6001\u5730\u6574\u5408\u4ee3\u7406\u4fe1\u606f\u4ee5\u51cf\u5c11\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e WinkTPG \u53ef\u4ee5\u4e3a\u591a\u8fbe 1,000 \u4e2a\u4ee3\u7406\u5728 1 \u79d2\u5185\u751f\u6210\u901f\u5ea6\u66f2\u7ebf\uff0c\u5e76\u4e14\u6bd4\u73b0\u6709\u7684 MAPF \u6267\u884c\u65b9\u6cd5\u63d0\u9ad8\u4e86\u9ad8\u8fbe 51.7% \u7684\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u3002", "conclusion": "WinkTPG\u53ef\u4ee5\u4e3a\u591a\u8fbe 1,000 \u4e2a\u4ee3\u7406\u5728 1 \u79d2\u5185\u751f\u6210\u901f\u5ea6\u66f2\u7ebf\uff0c\u5e76\u4e14\u6bd4\u73b0\u6709\u7684 MAPF \u6267\u884c\u65b9\u6cd5\u63d0\u9ad8\u4e86\u9ad8\u8fbe 51.7% \u7684\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u3002"}}
{"id": "2508.01002", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.01002", "abs": "https://arxiv.org/abs/2508.01002", "authors": ["Agrim Bari", "Parikshit Hegde", "Gustavo de Veciana"], "title": "Optimal Scheduling Algorithms for LLM Inference: Theory and Practice", "comment": null, "summary": "With the growing use of Large Language Model (LLM)-based tools like ChatGPT,\nPerplexity, and Gemini across industries, there is a rising need for efficient\nLLM inference systems. These systems handle requests with a unique two-phase\ncomputation structure: a prefill-phase that processes the full input prompt and\na decode-phase that autoregressively generates tokens one at a time. This\nstructure calls for new strategies for routing and scheduling requests.\n  In this paper, we take a comprehensive approach to this challenge by\ndeveloping a theoretical framework that models routing and scheduling in LLM\ninference systems. We identify two key design principles-optimal tiling and\ndynamic resource allocation-that are essential for achieving high throughput.\nGuided by these principles, we propose the Resource-Aware Dynamic (RAD)\nscheduler and prove that it achieves throughput optimality under mild\nconditions. To address practical Service Level Objectives (SLOs) such as\nserving requests with different Time Between Token (TBT) constraints, we design\nthe SLO-Aware LLM Inference (SLAI) scheduler. SLAI uses real-time measurements\nto prioritize decode requests that are close to missing their TBT deadlines and\nreorders prefill requests based on known prompt lengths to further reduce the\nTime To First Token (TTFT) delays.\n  We evaluate SLAI on the Openchat ShareGPT4 dataset using the Mistral-7B model\non an NVIDIA RTX ADA 6000 GPU. Compared to Sarathi-Serve, SLAI reduces the\nmedian TTFT by 53% and increases the maximum serving capacity by 26% such that\nmedian TTFT is below 0.5 seconds, while meeting tail TBT latency constraints.", "AI": {"tldr": "Proposes RAD and SLAI schedulers for efficient LLM inference, achieving throughput optimality and reducing TTFT.", "motivation": "Rising need for efficient LLM inference systems due to the growing use of LLM-based tools.", "method": "Resource-Aware Dynamic (RAD) scheduler and SLO-Aware LLM Inference (SLAI) scheduler", "result": "RAD scheduler achieves throughput optimality; SLAI reduces median TTFT by 53% and increases maximum serving capacity by 26% compared to Sarathi-Serve.", "conclusion": "SLAI reduces the median TTFT by 53% and increases the maximum serving capacity by 26% compared to Sarathi-Serve, while meeting tail TBT latency constraints."}}
{"id": "2508.01696", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01696", "abs": "https://arxiv.org/abs/2508.01696", "authors": ["Yi Jiang", "Sendong Zhao", "Jianbo Li", "Haochun Wang", "Lizhe Zhang", "Yan Liu", "Bin Qin"], "title": "Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge Synergy", "comment": "code available at https://github.com/liunian-Jay/CoCoA", "summary": "Retrieval-Augmented Generation (RAG) has emerged as a promising framework for\nenhancing the capabilities of Large Language Models (LLMs), especially in\nknowledge-intensive tasks. Despite its advantages, current RAG methods often\nstruggle to *fully exploit knowledge during generation*. In particular, the\nsynergy between the model's internal parametric knowledge and external\nretrieved knowledge remains limited. Retrieved contents may sometimes mislead\ngeneration, while certain generated content can guide the model toward more\naccurate outputs. In this work, we propose Collaborative Chain-of-Agents, a\nframework designed to enhance explicitly synergy over both parametric and\nretrieved knowledge. Specifically, we first introduce CoCoA-zero, a multi-agent\nRAG framework that first performs conditional knowledge induction and then\nreasons answers. Building on this, we develop CoCoA, a long-chain training\nstrategy that synthesizes extended multi-agent reasoning trajectories from\nCoCoA-zero to fine-tune the LLM. This strategy enhances the model's capability\nto explicitly integrate and jointly leverage parametric and retrieved\nknowledge. Experiments results show that CoCoA-zero and CoCoA achieve superior\nperformance on open-domain and multi-hop QA tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684 RAG \u6846\u67b6\uff0c\u901a\u8fc7\u534f\u540c\u94fe\u5f0f\u4ee3\u7406\u6765\u589e\u5f3a\u53c2\u6570\u548c\u68c0\u7d22\u77e5\u8bc6\u4e4b\u95f4\u7684\u534f\u540c\u4f5c\u7528\uff0c\u5e76\u5728\u5f00\u653e\u57df\u548c\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002", "motivation": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210 (RAG) \u5df2\u6210\u4e3a\u4e00\u79cd\u6709\u524d\u9014\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u3002\u5c3d\u7ba1\u5b83\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u76ee\u524d\u7684 RAG \u65b9\u6cd5\u901a\u5e38\u96be\u4ee5\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u5145\u5206\u5229\u7528\u77e5\u8bc6\u3002\u7279\u522b\u662f\uff0c\u6a21\u578b\u5185\u90e8\u53c2\u6570\u77e5\u8bc6\u548c\u5916\u90e8\u68c0\u7d22\u77e5\u8bc6\u4e4b\u95f4\u7684\u534f\u540c\u4f5c\u7528\u4ecd\u7136\u6709\u9650\u3002\u68c0\u7d22\u5230\u7684\u5185\u5bb9\u6709\u65f6\u53ef\u80fd\u4f1a\u8bef\u5bfc\u751f\u6210\uff0c\u800c\u67d0\u4e9b\u751f\u6210\u7684\u5185\u5bb9\u53ef\u4ee5\u5f15\u5bfc\u6a21\u578b\u4ea7\u751f\u66f4\u51c6\u786e\u7684\u8f93\u51fa\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u534f\u540c\u94fe\u5f0f\u4ee3\u7406\uff0c\u8fd9\u662f\u4e00\u4e2a\u65e8\u5728\u589e\u5f3a\u53c2\u6570\u548c\u68c0\u7d22\u77e5\u8bc6\u4e4b\u95f4\u663e\u5f0f\u534f\u540c\u7684\u6846\u67b6\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u9996\u5148\u4ecb\u7ecd\u4e86 CoCoA-zero\uff0c\u8fd9\u662f\u4e00\u4e2a\u591a\u4ee3\u7406 RAG \u6846\u67b6\uff0c\u5b83\u9996\u5148\u6267\u884c\u6761\u4ef6\u77e5\u8bc6\u5f52\u7eb3\uff0c\u7136\u540e\u63a8\u65ad\u7b54\u6848\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u5f00\u53d1\u4e86 CoCoA\uff0c\u8fd9\u662f\u4e00\u79cd\u957f\u94fe\u8bad\u7ec3\u7b56\u7565\uff0c\u5b83\u7efc\u5408\u4e86\u6765\u81ea CoCoA-zero \u7684\u6269\u5c55\u591a\u4ee3\u7406\u63a8\u7406\u8f68\u8ff9\uff0c\u4ee5\u5fae\u8c03 LLM\u3002\u8fd9\u79cd\u7b56\u7565\u589e\u5f3a\u4e86\u6a21\u578b\u663e\u5f0f\u6574\u5408\u548c\u8054\u5408\u5229\u7528\u53c2\u6570\u548c\u68c0\u7d22\u77e5\u8bc6\u7684\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCoCoA-zero \u548c CoCoA \u5728\u5f00\u653e\u57df\u548c\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "CoCoA-zero \u548c CoCoA \u5728\u5f00\u653e\u57df\u548c\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.01215", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01215", "abs": "https://arxiv.org/abs/2508.01215", "authors": ["Yuanlin Yang", "Quanjian Song", "Zhexian Gao", "Ge Wang", "Shanshan Li", "Xiaoyan Zhang"], "title": "StyDeco: Unsupervised Style Transfer with Distilling Priors and Semantic Decoupling", "comment": "9 pages in total", "summary": "Diffusion models have emerged as the dominant paradigm for style transfer,\nbut their text-driven mechanism is hindered by a core limitation: it treats\ntextual descriptions as uniform, monolithic guidance. This limitation overlooks\nthe semantic gap between the non-spatial nature of textual descriptions and the\nspatially-aware attributes of visual style, often leading to the loss of\nsemantic structure and fine-grained details during stylization. In this paper,\nwe propose StyDeco, an unsupervised framework that resolves this limitation by\nlearning text representations specifically tailored for the style transfer\ntask. Our framework first employs Prior-Guided Data Distillation (PGD), a\nstrategy designed to distill stylistic knowledge without human supervision. It\nleverages a powerful frozen generative model to automatically synthesize\npseudo-paired data. Subsequently, we introduce Contrastive Semantic Decoupling\n(CSD), a task-specific objective that adapts a text encoder using\ndomain-specific weights. CSD performs a two-class clustering in the semantic\nspace, encouraging source and target representations to form distinct clusters.\nExtensive experiments on three classic benchmarks demonstrate that our\nframework outperforms several existing approaches in both stylistic fidelity\nand structural preservation, highlighting its effectiveness in style transfer\nwith semantic preservation. In addition, our framework supports a unique\nde-stylization process, further demonstrating its extensibility. Our code is\nvailable at https://github.com/QuanjianSong/StyDeco.", "AI": {"tldr": "\u63d0\u51faStyDeco\uff0c\u4e00\u79cd\u65e0\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u4e13\u95e8\u4e3a\u98ce\u683c\u8fc1\u79fb\u4efb\u52a1\u91cf\u8eab\u5b9a\u5236\u7684\u6587\u672c\u8868\u793a\u6765\u89e3\u51b3\u6b64\u9650\u5236\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5df2\u6210\u4e3a\u98ce\u683c\u8fc1\u79fb\u7684\u4e3b\u5bfc\u8303\u4f8b\uff0c\u4f46\u5b83\u4eec\u7684\u6587\u672c\u9a71\u52a8\u673a\u5236\u53d7\u5230\u4e00\u4e2a\u6838\u5fc3\u9650\u5236\u7684\u963b\u788d\uff1a\u5b83\u5c06\u6587\u672c\u63cf\u8ff0\u89c6\u4e3a\u7edf\u4e00\u7684\u3001\u5355\u7247\u7684\u6307\u5bfc\u3002\u8fd9\u79cd\u9650\u5236\u5ffd\u7565\u4e86\u6587\u672c\u63cf\u8ff0\u7684\u975e\u7a7a\u95f4\u6027\u8d28\u4e0e\u89c6\u89c9\u98ce\u683c\u7684\u7a7a\u95f4\u611f\u77e5\u5c5e\u6027\u4e4b\u95f4\u7684\u8bed\u4e49\u5dee\u8ddd\uff0c\u901a\u5e38\u5bfc\u81f4\u98ce\u683c\u5316\u8fc7\u7a0b\u4e2d\u8bed\u4e49\u7ed3\u6784\u548c\u7cbe\u7ec6\u7ec6\u8282\u7684\u4e22\u5931\u3002", "method": "\u8be5\u6846\u67b6\u9996\u5148\u91c7\u7528\u5148\u9a8c\u5f15\u5bfc\u6570\u636e\u84b8\u998f\uff08PGD\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65e8\u5728\u5728\u6ca1\u6709\u4eba\u4e3a\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u63d0\u53d6\u98ce\u683c\u77e5\u8bc6\u7684\u7b56\u7565\u3002\u5b83\u5229\u7528\u5f3a\u5927\u7684\u51bb\u7ed3\u751f\u6210\u6a21\u578b\u6765\u81ea\u52a8\u5408\u6210\u4f2a\u914d\u5bf9\u6570\u636e\u3002\u968f\u540e\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u5bf9\u6bd4\u8bed\u4e49\u89e3\u8026\uff08CSD\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u76ee\u6807\uff0c\u5b83\u4f7f\u7528\u7279\u5b9a\u4e8e\u9886\u57df\u7684\u6743\u91cd\u6765\u8c03\u6574\u6587\u672c\u7f16\u7801\u5668\u3002CSD \u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u6267\u884c\u4e24\u7c7b\u805a\u7c7b\uff0c\u9f13\u52b1\u6e90\u8868\u793a\u548c\u76ee\u6807\u8868\u793a\u5f62\u6210\u4e0d\u540c\u7684\u805a\u7c7b\u3002", "result": "\u5728\u4e09\u4e2a\u7ecf\u5178\u57fa\u51c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6846\u67b6\u4f18\u4e8e\u51e0\u79cd\u73b0\u6709\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u98ce\u683c\u4fdd\u771f\u5ea6\u548c\u7ed3\u6784\u4fdd\u6301\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u5728\u8bed\u4e49\u4fdd\u7559\u7684\u98ce\u683c\u8fc1\u79fb\u4e2d\u7684\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u652f\u6301\u72ec\u7279\u7684\u53bb\u98ce\u683c\u5316\u8fc7\u7a0b\uff0c\u8fdb\u4e00\u6b65\u5c55\u793a\u4e86\u5176\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2508.01543", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01543", "abs": "https://arxiv.org/abs/2508.01543", "authors": ["Derin Cayir", "Renjie Tao", "Rashi Rungta", "Kai Sun", "Sean Chen", "Haidar Khan", "Minseok Kim", "Julia Reinspach", "Yue Liu"], "title": "Refine-n-Judge: Curating High-Quality Preference Chains for LLM-Fine-Tuning", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable progress through\npreference-based fine-tuning, which critically depends on the quality of the\nunderlying training data. While human feedback is essential for improving data\nquality, it is costly and does not scale well. In this paper, we introduce\nRefine-n-Judge, an automated iterative approach that leverages a single LLM as\nboth a refiner and a judge to enhance dataset quality. Unlike existing\niterative refinement methods, Refine-n-Judge employs an LLM to both generate\nrefinements and explicitly evaluate each improvement, ensuring that every\niteration meaningfully enhances the dataset without requiring additional human\nannotation or a separate reward model. At each step, the LLM refines a response\nand judges whether the refinement is an improvement over the previous answer.\nThis process continues until the LLM prefers the initial answer over the\nrefinement, indicating no further improvements. This produces sequences of\nincreasing quality, preference-labeled responses ideal for fine-tuning.\n  We demonstrate the effectiveness of Refine-n-Judge across a range of public\ndatasets spanning five corpora, targeting tasks such as coding, math, and\nconversation. Models (Llama 3.1-8B and Llama 3.3-70B) fine-tuned on\nRefine-n-Judge-enhanced datasets were preferred by LLM judges in over 74% of\ncomparisons against models tuned on the original dataset by GPT-4.\nAdditionally, we report performance gains: +5% on AlpacaEval and AlpacaEval\n2.0, and +19% on MT-Bench. Our results indicate that Refine-n-Judge produces\nhigh-quality datasets and scalable model improvements.", "AI": {"tldr": "Refine-n-Judge uses a single LLM to iteratively refine and judge its own responses, creating high-quality datasets for fine-tuning, leading to improved model performance.", "motivation": "human feedback is essential for improving data quality, it is costly and does not scale well", "method": "an automated iterative approach that leverages a single LLM as both a refiner and a judge to enhance dataset quality", "result": "Models fine-tuned on Refine-n-Judge-enhanced datasets were preferred by LLM judges in over 74% of comparisons against models tuned on the original dataset by GPT-4. Additionally, we report performance gains: +5% on AlpacaEval and AlpacaEval 2.0, and +19% on MT-Bench.", "conclusion": "Refine-n-Judge produces high-quality datasets and scalable model improvements."}}
{"id": "2508.01010", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01010", "abs": "https://arxiv.org/abs/2508.01010", "authors": ["Gnankan Landry Regis N'guessan"], "title": "v-PuNNs: van der Put Neural Networks for Transparent Ultrametric Representation Learning", "comment": null, "summary": "Conventional deep learning models embed data in Euclidean space\n$\\mathbb{R}^d$, a poor fit for strictly hierarchical objects such as taxa, word\nsenses, or file systems. We introduce van der Put Neural Networks (v-PuNNs),\nthe first architecture whose neurons are characteristic functions of p-adic\nballs in $\\mathbb{Z}_p$. Under our Transparent Ultrametric Representation\nLearning (TURL) principle every weight is itself a p-adic number, giving exact\nsubtree semantics. A new Finite Hierarchical Approximation Theorem shows that a\ndepth-K v-PuNN with $\\sum_{j=0}^{K-1}p^{\\,j}$ neurons universally represents\nany K-level tree. Because gradients vanish in this discrete space, we propose\nValuation-Adaptive Perturbation Optimization (VAPO), with a fast deterministic\nvariant (HiPaN-DS) and a moment-based one (HiPaN / Adam-VAPO). On three\ncanonical benchmarks our CPU-only implementation sets new state-of-the-art:\nWordNet nouns (52,427 leaves) 99.96% leaf accuracy in 16 min; GO\nmolecular-function 96.9% leaf / 100% root in 50 s; NCBI Mammalia Spearman $\\rho\n= -0.96$ with true taxonomic distance. The learned metric is perfectly\nultrametric (zero triangle violations), and its fractal and\ninformation-theoretic properties are analyzed. Beyond classification we derive\nstructural invariants for quantum systems (HiPaQ) and controllable generative\ncodes for tabular data (Tab-HiPaN). v-PuNNs therefore bridge number theory and\ndeep learning, offering exact, interpretable, and efficient models for\nhierarchical data.", "AI": {"tldr": "v-PuNNs, a new neural network architecture based on p-adic numbers, achieves state-of-the-art results on hierarchical data benchmarks with exact, interpretable, and efficient models.", "motivation": "Conventional deep learning models embed data in Euclidean space which is a poor fit for strictly hierarchical objects.", "method": "van der Put Neural Networks (v-PuNNs) with Valuation-Adaptive Perturbation Optimization (VAPO).", "result": "On three canonical benchmarks our CPU-only implementation sets new state-of-the-art.", "conclusion": "v-PuNNs bridge number theory and deep learning, offering exact, interpretable, and efficient models for hierarchical data."}}
{"id": "2508.01708", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01708", "abs": "https://arxiv.org/abs/2508.01708", "authors": ["Berkay K\u00f6pr\u00fc", "Mehrzad Mashal", "Yigit Gurses", "Akos Kadar", "Maximilian Schmitt", "Ditty Mathew", "Felix Burkhardt", "Florian Eyben", "Bj\u00f6rn W. Schuller"], "title": "Am I Blue or Is My Hobby Counting Teardrops? Expression Leakage in Large Language Models as a Symptom of Irrelevancy Disruption", "comment": null, "summary": "Large language models (LLMs) have advanced natural language processing (NLP)\nskills such as through next-token prediction and self-attention, but their\nability to integrate broad context also makes them prone to incorporating\nirrelevant information. Prior work has focused on semantic leakage, bias\nintroduced by semantically irrelevant context. In this paper, we introduce\nexpression leakage, a novel phenomenon where LLMs systematically generate\nsentimentally charged expressions that are semantically unrelated to the input\ncontext. To analyse the expression leakage, we collect a benchmark dataset\nalong with a scheme to automatically generate a dataset from free-form text\nfrom common-crawl. In addition, we propose an automatic evaluation pipeline\nthat correlates well with human judgment, which accelerates the benchmarking by\ndecoupling from the need of annotation for each analysed model. Our experiments\nshow that, as the model scales in the parameter space, the expression leakage\nreduces within the same LLM family. On the other hand, we demonstrate that\nexpression leakage mitigation requires specific care during the model building\nprocess, and cannot be mitigated by prompting. In addition, our experiments\nindicate that, when negative sentiment is injected in the prompt, it disrupts\nthe generation process more than the positive sentiment, causing a higher\nexpression leakage rate.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bb9\u6613\u4ea7\u751f\u4e0e\u8f93\u5165\u4e0a\u4e0b\u6587\u65e0\u5173\u7684\u60c5\u611f\u8868\u8fbe\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bb9\u6613\u6574\u5408\u4e0d\u76f8\u5173\u7684\u4fe1\u606f\uff0c\u4ece\u800c\u5f15\u5165\u4e0d\u76f8\u5173\u7684\u60c5\u611f\u8868\u8fbe\u3002", "method": "\u6536\u96c6\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u8bbe\u8ba1\u81ea\u52a8\u751f\u6210\u6570\u636e\u96c6\u7684\u65b9\u6848\u3002\u63d0\u51fa\u4e00\u4e2a\u4e0e\u4eba\u7c7b\u5224\u65ad\u76f8\u5173\u7684\u81ea\u52a8\u8bc4\u4f30\u7ba1\u9053\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u540c\u4e00LLM\u5bb6\u65cf\u4e2d\uff0c\u968f\u7740\u6a21\u578b\u5728\u53c2\u6570\u7a7a\u95f4\u4e2d\u6269\u5c55\uff0c\u60c5\u611f\u6cc4\u9732\u4f1a\u51cf\u5c11\u3002\u60c5\u611f\u6cc4\u9732\u7684\u7f13\u89e3\u9700\u8981\u5728\u6a21\u578b\u6784\u5efa\u8fc7\u7a0b\u4e2d\u7279\u522b\u6ce8\u610f\uff0c\u5e76\u4e14\u4e0d\u80fd\u901a\u8fc7\u63d0\u793a\u6765\u7f13\u89e3\u3002\u8d1f\u9762\u60c5\u7eea\u6ce8\u5165\u63d0\u793a\u4f1a\u6bd4\u6b63\u9762\u60c5\u7eea\u66f4\u591a\u5730\u6270\u4e71\u751f\u6210\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u66f4\u9ad8\u7684\u60c5\u611f\u6cc4\u9732\u7387\u3002", "conclusion": "\u6a21\u578b\u89c4\u6a21\u6269\u5927\u53ef\u4ee5\u51cf\u5c11\u60c5\u611f\u6cc4\u9732\uff0c\u4f46\u9700\u8981\u7279\u5b9a\u7684\u6a21\u578b\u6784\u5efa\u8fc7\u7a0b\u6765\u7f13\u89e3\uff0c\u63d0\u793a\u65e0\u6cd5\u7f13\u89e3\u3002\u8d1f\u9762\u60c5\u7eea\u6bd4\u6b63\u9762\u60c5\u7eea\u66f4\u5bb9\u6613\u6270\u4e71\u751f\u6210\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u66f4\u9ad8\u7684\u60c5\u611f\u6cc4\u9732\u7387\u3002"}}
{"id": "2508.01216", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.01216", "abs": "https://arxiv.org/abs/2508.01216", "authors": ["Bolei Chen", "Shengsheng Yan", "Yongzheng Cui", "Jiaxu Kang", "Ping Zhong", "Jianxin Wang"], "title": "Perspective from a Broader Context: Can Room Style Knowledge Help Visual Floorplan Localization?", "comment": "Submitted to AAAI 2026. arXiv admin note: text overlap with\n  arXiv:2507.18881", "summary": "Since a building's floorplan remains consistent over time and is inherently\nrobust to changes in visual appearance, visual Floorplan Localization (FLoc)\nhas received increasing attention from researchers. However, as a compact and\nminimalist representation of the building's layout, floorplans contain many\nrepetitive structures (e.g., hallways and corners), thus easily result in\nambiguous localization. Existing methods either pin their hopes on matching 2D\nstructural cues in floorplans or rely on 3D geometry-constrained visual\npre-trainings, ignoring the richer contextual information provided by visual\nimages. In this paper, we suggest using broader visual scene context to empower\nFLoc algorithms with scene layout priors to eliminate localization uncertainty.\nIn particular, we propose an unsupervised learning technique with clustering\nconstraints to pre-train a room discriminator on self-collected unlabeled room\nimages. Such a discriminator can empirically extract the hidden room type of\nthe observed image and distinguish it from other room types. By injecting the\nscene context information summarized by the discriminator into an FLoc\nalgorithm, the room style knowledge is effectively exploited to guide definite\nvisual FLoc. We conducted sufficient comparative studies on two standard visual\nFloc benchmarks. Our experiments show that our approach outperforms\nstate-of-the-art methods and achieves significant improvements in robustness\nand accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u66f4\u5e7f\u6cdb\u7684\u89c6\u89c9\u573a\u666f\u4e0a\u4e0b\u6587\u6765\u589e\u5f3a FLoc \u7b97\u6cd5\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528\u5177\u6709\u805a\u7c7b\u7ea6\u675f\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u6280\u672f\u6765\u9884\u8bad\u7ec3\u623f\u95f4\u5224\u522b\u5668\uff0c\u4ece\u800c\u63d0\u53d6\u9690\u85cf\u7684\u623f\u95f4\u7c7b\u578b\u5e76\u533a\u5206\u4e0d\u540c\u7684\u623f\u95f4\u7c7b\u578b\uff0c\u4ece\u800c\u6307\u5bfc\u89c6\u89c9 FLoc\u3002", "motivation": "\u7531\u4e8e\u5efa\u7b51\u7269\u7684\u5e73\u9762\u56fe\u968f\u65f6\u95f4\u63a8\u79fb\u4fdd\u6301\u4e00\u81f4\uff0c\u5e76\u4e14\u672c\u8d28\u4e0a\u5bf9\u89c6\u89c9\u5916\u89c2\u7684\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\uff0c\u56e0\u6b64\u89c6\u89c9\u5e73\u9762\u56fe\u5b9a\u4f4d\uff08FLoc\uff09\u53d7\u5230\u4e86\u7814\u7a76\u4eba\u5458\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\u3002\u7136\u800c\uff0c\u4f5c\u4e3a\u5efa\u7b51\u7269\u5e03\u5c40\u7684\u7d27\u51d1\u548c\u7b80\u7ea6\u8868\u793a\uff0c\u5e73\u9762\u56fe\u5305\u542b\u8bb8\u591a\u91cd\u590d\u7684\u7ed3\u6784\uff08\u4f8b\u5982\uff0c\u8d70\u5eca\u548c\u89d2\u843d\uff09\uff0c\u56e0\u6b64\u5f88\u5bb9\u6613\u5bfc\u81f4\u6a21\u7cca\u7684\u5b9a\u4f4d\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5c06\u5e0c\u671b\u5bc4\u6258\u5728\u5339\u914d\u5e73\u9762\u56fe\u4e2d\u7684 2D \u7ed3\u6784\u7ebf\u7d22\u4e0a\uff0c\u8981\u4e48\u4f9d\u8d56\u4e8e 3D \u51e0\u4f55\u7ea6\u675f\u7684\u89c6\u89c9\u9884\u8bad\u7ec3\uff0c\u800c\u5ffd\u7565\u4e86\u89c6\u89c9\u56fe\u50cf\u63d0\u4f9b\u7684\u66f4\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u805a\u7c7b\u7ea6\u675f\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u6280\u672f\uff0c\u4ee5\u5728\u81ea\u884c\u6536\u96c6\u7684\u672a\u6807\u8bb0\u623f\u95f4\u56fe\u50cf\u4e0a\u9884\u8bad\u7ec3\u623f\u95f4\u5224\u522b\u5668\u3002", "result": "\u901a\u8fc7\u5c06\u5224\u522b\u5668\u603b\u7ed3\u7684\u573a\u666f\u4e0a\u4e0b\u6587\u4fe1\u606f\u6ce8\u5165\u5230 FLoc \u7b97\u6cd5\u4e2d\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u5229\u7528\u623f\u95f4\u6837\u5f0f\u77e5\u8bc6\u6765\u6307\u5bfc\u660e\u786e\u7684\u89c6\u89c9 FLoc\u3002 ", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e24\u4e2a\u6807\u51c6\u89c6\u89c9\u5e73\u9762\u56fe\u5b9a\u4f4d\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u5145\u5206\u7684\u5bf9\u6bd4\u7814\u7a76\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u5728\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002"}}
{"id": "2508.01545", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.01545", "abs": "https://arxiv.org/abs/2508.01545", "authors": ["Emilio Barkett", "Olivia Long", "Paul Kr\u00f6ger"], "title": "Getting out of the Big-Muddy: Escalation of Commitment in LLMs", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in autonomous\ndecision-making roles across high-stakes domains. However, since models are\ntrained on human-generated data, they may inherit cognitive biases that\nsystematically distort human judgment, including escalation of commitment,\nwhere decision-makers continue investing in failing courses of action due to\nprior investment. Understanding when LLMs exhibit such biases presents a unique\nchallenge. While these biases are well-documented in humans, it remains unclear\nwhether they manifest consistently in LLMs or require specific triggering\nconditions. This paper investigates this question using a two-stage investment\ntask across four experimental conditions: model as investor, model as advisor,\nmulti-agent deliberation, and compound pressure scenario. Across N = 6,500\ntrials, we find that bias manifestation in LLMs is highly context-dependent. In\nindividual decision-making contexts (Studies 1-2, N = 4,000), LLMs demonstrate\nstrong rational cost-benefit logic with minimal escalation of commitment.\nHowever, multi-agent deliberation reveals a striking hierarchy effect (Study 3,\nN = 500): while asymmetrical hierarchies show moderate escalation rates\n(46.2%), symmetrical peer-based decision-making produces near-universal\nescalation (99.2%). Similarly, when subjected to compound organizational and\npersonal pressures (Study 4, N = 2,000), models exhibit high degrees of\nescalation of commitment (68.95% average allocation to failing divisions).\nThese findings reveal that LLM bias manifestation depends critically on social\nand organizational context rather than being inherent, with significant\nimplications for the deployment of multi-agent systems and unsupervised\noperations where such conditions may emerge naturally.", "AI": {"tldr": "LLM bias depends on context, not inherent traits, impacting multi-agent and unsupervised system deployment.", "motivation": "LLMs may inherit cognitive biases that systematically distort human judgment, including escalation of commitment, where decision-makers continue investing in failing courses of action due to prior investment. Understanding when LLMs exhibit such biases presents a unique challenge.", "method": "a two-stage investment task across four experimental conditions: model as investor, model as advisor, multi-agent deliberation, and compound pressure scenario", "result": "bias manifestation in LLMs is highly context-dependent. In individual decision-making contexts, LLMs demonstrate strong rational cost-benefit logic with minimal escalation of commitment. However, multi-agent deliberation reveals a striking hierarchy effect... Similarly, when subjected to compound organizational and personal pressures, models exhibit high degrees of escalation of commitment", "conclusion": "LLM bias manifestation depends critically on social and organizational context rather than being inherent, with significant implications for the deployment of multi-agent systems and unsupervised operations where such conditions may emerge naturally."}}
{"id": "2508.01013", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.01013", "abs": "https://arxiv.org/abs/2508.01013", "authors": ["Arjun Manoj", "Anastasia S. Georgiou", "Dimitris G. Giovanis", "Themistoklis P. Sapsis", "Ioannis G. Kevrekidis"], "title": "On Some Tunable Multi-fidelity Bayesian Optimization Frameworks", "comment": null, "summary": "Multi-fidelity optimization employs surrogate models that integrate\ninformation from varying levels of fidelity to guide efficient exploration of\ncomplex design spaces while minimizing the reliance on (expensive)\nhigh-fidelity objective function evaluations. To advance Gaussian Process\n(GP)-based multi-fidelity optimization, we implement a proximity-based\nacquisition strategy that simplifies fidelity selection by eliminating the need\nfor separate acquisition functions at each fidelity level. We also enable\nmulti-fidelity Upper Confidence Bound (UCB) strategies by combining them with\nmulti-fidelity GPs rather than the standard GPs typically used. We benchmark\nthese approaches alongside other multi-fidelity acquisition strategies\n(including fidelity-weighted approaches) comparing their performance, reliance\non high-fidelity evaluations, and hyperparameter tunability in representative\noptimization tasks. The results highlight the capability of the proximity-based\nmulti-fidelity acquisition function to deliver consistent control over\nhigh-fidelity usage while maintaining convergence efficiency. Our illustrative\nexamples include multi-fidelity chemical kinetic models, both homogeneous and\nheterogeneous (dynamic catalysis for ammonia production).", "AI": {"tldr": "This paper introduces a proximity-based acquisition strategy for Gaussian Process (GP)-based multi-fidelity optimization. The results highlight the capability of the proximity-based multi-fidelity acquisition function to deliver consistent control over high-fidelity usage while maintaining convergence efficiency.", "motivation": "To advance Gaussian Process (GP)-based multi-fidelity optimization, which employs surrogate models that integrate information from varying levels of fidelity to guide efficient exploration of complex design spaces while minimizing the reliance on (expensive) high-fidelity objective function evaluations.", "method": "We implement a proximity-based acquisition strategy that simplifies fidelity selection by eliminating the need for separate acquisition functions at each fidelity level. We also enable multi-fidelity Upper Confidence Bound (UCB) strategies by combining them with multi-fidelity GPs rather than the standard GPs typically used. We benchmark these approaches alongside other multi-fidelity acquisition strategies (including fidelity-weighted approaches).", "result": "The results highlight the capability of the proximity-based multi-fidelity acquisition function to deliver consistent control over high-fidelity usage while maintaining convergence efficiency. Our illustrative examples include multi-fidelity chemical kinetic models, both homogeneous and heterogeneous (dynamic catalysis for ammonia production).", "conclusion": "The proximity-based multi-fidelity acquisition function can deliver consistent control over high-fidelity usage while maintaining convergence efficiency."}}
{"id": "2508.01710", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.01710", "abs": "https://arxiv.org/abs/2508.01710", "authors": ["Raviraj Joshi", "Rakesh Paul", "Kanishk Singla", "Anusha Kamath", "Michael Evans", "Katherine Luna", "Shaona Ghosh", "Utkarsh Vaidya", "Eileen Long", "Sanjay Singh Chauhan", "Niranjan Wartikar"], "title": "CultureGuard: Towards Culturally-Aware Dataset and Guard Model for Multilingual Safety Applications", "comment": null, "summary": "The increasing use of Large Language Models (LLMs) in agentic applications\nhighlights the need for robust safety guard models. While content safety in\nEnglish is well-studied, non-English languages lack similar advancements due to\nthe high cost of collecting culturally aligned labeled datasets. We present\nCultureGuard, a novel solution for curating culturally aligned, high-quality\nsafety datasets across multiple languages. Our approach introduces a four-stage\nsynthetic data generation and filtering pipeline: cultural data segregation,\ncultural data adaptation, machine translation, and quality filtering. This\npipeline enables the conversion and expansion of the\nNemotron-Content-Safety-Dataset-V2 English safety dataset into eight distinct\nlanguages: Arabic, German, Spanish, French, Hindi, Japanese, Thai, and Chinese.\nThe resulting dataset, Nemotron-Content-Safety-Dataset-Multilingual-v1,\ncomprises 386,661 samples in 9 languages and facilitates the training of\nLlama-3.1-Nemotron-Safety-Guard-Multilingual-8B-v1 via LoRA-based fine-tuning.\nThe final model achieves state-of-the-art performance on several multilingual\ncontent safety benchmarks. We also benchmark the latest open LLMs on\nmultilingual safety and observe that these LLMs are more prone to give unsafe\nresponses when prompted in non-English languages. This work represents a\nsignificant step toward closing the safety gap in multilingual LLMs by enabling\nthe development of culturally aware safety guard models.", "AI": {"tldr": "Addresses the safety gap in multilingual LLMs by creating a culturally aware safety guard model using a novel data generation and filtering pipeline, achieving state-of-the-art performance.", "motivation": "The increasing use of Large Language Models (LLMs) in agentic applications highlights the need for robust safety guard models. While content safety in English is well-studied, non-English languages lack similar advancements due to the high cost of collecting culturally aligned labeled datasets.", "method": "introducing a four-stage synthetic data generation and filtering pipeline: cultural data segregation, cultural data adaptation, machine translation, and quality filtering to convert and expand the Nemotron-Content-Safety-Dataset-V2 English safety dataset into eight distinct languages", "result": "The resulting dataset, Nemotron-Content-Safety-Dataset-Multilingual-v1, comprises 386,661 samples in 9 languages and facilitates the training of Llama-3.1-Nemotron-Safety-Guard-Multilingual-8B-v1 via LoRA-based fine-tuning. The final model achieves state-of-the-art performance on several multilingual content safety benchmarks. the latest open LLMs are more prone to give unsafe responses when prompted in non-English languages.", "conclusion": "This work represents a significant step toward closing the safety gap in multilingual LLMs by enabling the development of culturally aware safety guard models."}}
{"id": "2508.01218", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.01218", "abs": "https://arxiv.org/abs/2508.01218", "authors": ["Yujian Liu", "Linlang Cao", "Chuang Chen", "Fanyu Geng", "Dongxu Shen", "Peng Cao", "Shidang Xu", "Xiaoli Liu"], "title": "MoGaFace: Momentum-Guided and Texture-Aware Gaussian Avatars for Consistent Facial Geometry", "comment": "10 pages, 7 figures", "summary": "Existing 3D head avatar reconstruction methods adopt a two-stage process,\nrelying on tracked FLAME meshes derived from facial landmarks, followed by\nGaussian-based rendering. However, misalignment between the estimated mesh and\ntarget images often leads to suboptimal rendering quality and loss of fine\nvisual details. In this paper, we present MoGaFace, a novel 3D head avatar\nmodeling framework that continuously refines facial geometry and texture\nattributes throughout the Gaussian rendering process. To address the\nmisalignment between estimated FLAME meshes and target images, we introduce the\nMomentum-Guided Consistent Geometry module, which incorporates a\nmomentum-updated expression bank and an expression-aware correction mechanism\nto ensure temporal and multi-view consistency. Additionally, we propose Latent\nTexture Attention, which encodes compact multi-view features into head-aware\nrepresentations, enabling geometry-aware texture refinement via integration\ninto Gaussians. Extensive experiments show that MoGaFace achieves high-fidelity\nhead avatar reconstruction and significantly improves novel-view synthesis\nquality, even under inaccurate mesh initialization and unconstrained real-world\nsettings.", "AI": {"tldr": "MoGaFace \u662f\u4e00\u79cd\u65b0\u9896\u7684 3D \u5934\u90e8\u5934\u50cf\u5efa\u6a21\u6846\u67b6\uff0c\u53ef\u5728\u6574\u4e2a\u9ad8\u65af\u6e32\u67d3\u8fc7\u7a0b\u4e2d\u4e0d\u65ad\u7ec6\u5316\u9762\u90e8\u51e0\u4f55\u548c\u7eb9\u7406\u5c5e\u6027\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5934\u90e8\u5934\u50cf\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u7684 3D \u5934\u90e8\u5934\u50cf\u91cd\u5efa\u65b9\u6cd5\u91c7\u7528\u4e24\u9636\u6bb5\u8fc7\u7a0b\uff0c\u4f9d\u8d56\u4e8e\u4ece\u9762\u90e8\u6807\u5fd7\u5bfc\u51fa\u7684\u8ddf\u8e2a FLAME \u7f51\u683c\uff0c\u7136\u540e\u8fdb\u884c\u57fa\u4e8e\u9ad8\u65af\u7684\u6e32\u67d3\u3002\u7136\u800c\uff0c\u4f30\u8ba1\u7684\u7f51\u683c\u548c\u76ee\u6807\u56fe\u50cf\u4e4b\u95f4\u7684\u4e0d\u5bf9\u9f50\u901a\u5e38\u4f1a\u5bfc\u81f4\u6b21\u4f18\u7684\u6e32\u67d3\u8d28\u91cf\u548c\u7cbe\u7ec6\u89c6\u89c9\u7ec6\u8282\u7684\u4e22\u5931\u3002", "method": "MoGaFace\uff0c\u4e00\u4e2a\u65b0\u9896\u7684 3D \u5934\u90e8\u5934\u50cf\u5efa\u6a21\u6846\u67b6\uff0c\u53ef\u5728\u6574\u4e2a\u9ad8\u65af\u6e32\u67d3\u8fc7\u7a0b\u4e2d\u4e0d\u65ad\u7ec6\u5316\u9762\u90e8\u51e0\u4f55\u548c\u7eb9\u7406\u5c5e\u6027\u3002\u5f15\u5165\u4e86\u52a8\u91cf\u5f15\u5bfc\u4e00\u81f4\u51e0\u4f55\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u7ed3\u5408\u4e86\u52a8\u91cf\u66f4\u65b0\u7684\u8868\u8fbe\u5e93\u548c\u8868\u8fbe\u611f\u77e5\u6821\u6b63\u673a\u5236\uff0c\u4ee5\u786e\u4fdd\u65f6\u95f4\u548c\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u6f5c\u5728\u7eb9\u7406\u6ce8\u610f\u529b\uff0c\u5b83\u5c06\u7d27\u51d1\u7684\u591a\u89c6\u56fe\u7279\u5f81\u7f16\u7801\u4e3a\u5934\u90e8\u611f\u77e5\u8868\u793a\uff0c\u4ece\u800c\u53ef\u4ee5\u901a\u8fc7\u96c6\u6210\u5230\u9ad8\u65af\u51fd\u6570\u4e2d\u5b9e\u73b0\u51e0\u4f55\u611f\u77e5\u7eb9\u7406\u7ec6\u5316\u3002", "result": "MoGaFace \u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5934\u90e8\u5934\u50cf\u91cd\u5efa\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u65b0\u89c6\u89d2\u5408\u6210\u8d28\u91cf\uff0c\u5373\u4f7f\u5728\u4e0d\u51c6\u786e\u7684\u7f51\u683c\u521d\u59cb\u5316\u548c\u4e0d\u53d7\u7ea6\u675f\u7684\u771f\u5b9e\u4e16\u754c\u8bbe\u7f6e\u4e0b\u4e5f\u662f\u5982\u6b64\u3002", "conclusion": "MoGaFace\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5934\u90e8\u5934\u50cf\u91cd\u5efa\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u65b0\u89c6\u89d2\u5408\u6210\u8d28\u91cf\uff0c\u5373\u4f7f\u5728\u4e0d\u51c6\u786e\u7684\u7f51\u683c\u521d\u59cb\u5316\u548c\u4e0d\u53d7\u7ea6\u675f\u7684\u771f\u5b9e\u4e16\u754c\u8bbe\u7f6e\u4e0b\u4e5f\u662f\u5982\u6b64\u3002"}}
{"id": "2508.01556", "categories": ["cs.AI", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.01556", "abs": "https://arxiv.org/abs/2508.01556", "authors": ["Mengshi Chen", "Yuxiang Sun", "Tengchao Li", "Jianwei Wang", "Kai Wang", "Xuemin Lin", "Ying Zhang", "Wenjie Zhang"], "title": "Empowering Tabular Data Preparation with Language Models: Why and How?", "comment": "Preprint under submission, 16 pages, 2 figures, 1 table", "summary": "Data preparation is a critical step in enhancing the usability of tabular\ndata and thus boosts downstream data-driven tasks. Traditional methods often\nface challenges in capturing the intricate relationships within tables and\nadapting to the tasks involved. Recent advances in Language Models (LMs),\nespecially in Large Language Models (LLMs), offer new opportunities to automate\nand support tabular data preparation. However, why LMs suit tabular data\npreparation (i.e., how their capabilities match task demands) and how to use\nthem effectively across phases still remain to be systematically explored. In\nthis survey, we systematically analyze the role of LMs in enhancing tabular\ndata preparation processes, focusing on four core phases: data acquisition,\nintegration, cleaning, and transformation. For each phase, we present an\nintegrated analysis of how LMs can be combined with other components for\ndifferent preparation tasks, highlight key advancements, and outline\nprospective pipelines.", "AI": {"tldr": "This survey analyzes the role of LMs in enhancing tabular data preparation processes, focusing on data acquisition, integration, cleaning, and transformation.", "motivation": "Traditional methods often face challenges in capturing the intricate relationships within tables and adapting to the tasks involved. Recent advances in Language Models (LMs), especially in Large Language Models (LLMs), offer new opportunities to automate and support tabular data preparation. However, why LMs suit tabular data preparation (i.e., how their capabilities match task demands) and how to use them effectively across phases still remain to be systematically explored.", "method": "LMs are combined with other components for different preparation tasks", "result": "LMs can enhance tabular data preparation processes", "conclusion": "This survey systematically analyzes the role of LMs in enhancing tabular data preparation processes, focusing on four core phases: data acquisition, integration, cleaning, and transformation. For each phase, it presents an integrated analysis of how LMs can be combined with other components for different preparation tasks, highlights key advancements, and outlines prospective pipelines."}}
{"id": "2508.01048", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.01048", "abs": "https://arxiv.org/abs/2508.01048", "authors": ["Jesse He", "Akbar Rafiey", "Gal Mishne", "Yusu Wang"], "title": "Explaining GNN Explanations with Edge Gradients", "comment": "KDD 2025", "summary": "In recent years, the remarkable success of graph neural networks (GNNs) on\ngraph-structured data has prompted a surge of methods for explaining GNN\npredictions. However, the state-of-the-art for GNN explainability remains in\nflux. Different comparisons find mixed results for different methods, with many\nexplainers struggling on more complex GNN architectures and tasks. This\npresents an urgent need for a more careful theoretical analysis of competing\nGNN explanation methods. In this work we take a closer look at GNN explanations\nin two different settings: input-level explanations, which produce explanatory\nsubgraphs of the input graph, and layerwise explanations, which produce\nexplanatory subgraphs of the computation graph. We establish the first\ntheoretical connections between the popular perturbation-based and classical\ngradient-based methods, as well as point out connections between other recently\nproposed methods. At the input level, we demonstrate conditions under which\nGNNExplainer can be approximated by a simple heuristic based on the sign of the\nedge gradients. In the layerwise setting, we point out that edge gradients are\nequivalent to occlusion search for linear GNNs. Finally, we demonstrate how our\ntheoretical results manifest in practice with experiments on both synthetic and\nreal datasets.", "AI": {"tldr": "\u672c\u6587\u5bf9GNN\u89e3\u91ca\u65b9\u6cd5\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u65b9\u6cd5\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u3002", "motivation": "\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u5728\u56fe\u7ed3\u6784\u6570\u636e\u4e0a\u7684\u663e\u8457\u6210\u529f\u4fc3\u4f7f\u4e86\u5927\u91cf\u89e3\u91caGNN\u9884\u6d4b\u7684\u65b9\u6cd5\u7684\u6d8c\u73b0\u3002\u7136\u800c\uff0cGNN\u53ef\u89e3\u91ca\u6027\u7684\u6700\u65b0\u6280\u672f\u4ecd\u4e0d\u7a33\u5b9a\u3002\u4e0d\u540c\u7684\u6bd4\u8f83\u53d1\u73b0\u4e0d\u540c\u65b9\u6cd5\u7684\u7ed3\u679c\u597d\u574f\u53c2\u534a\uff0c\u8bb8\u591a\u89e3\u91ca\u5668\u5728\u66f4\u590d\u6742\u7684GNN\u67b6\u6784\u548c\u4efb\u52a1\u4e0a\u6b65\u5c65\u8e52\u8dda\u3002\u8fd9\u8feb\u5207\u9700\u8981\u5bf9\u7ade\u4e89\u6027GNN\u89e3\u91ca\u65b9\u6cd5\u8fdb\u884c\u66f4\u4ed4\u7ec6\u7684\u7406\u8bba\u5206\u6790\u3002", "method": "\u672c\u6587\u5728\u4e24\u79cd\u4e0d\u540c\u7684\u8bbe\u7f6e\u4e0b\uff0c\u5bf9GNN\u89e3\u91ca\u8fdb\u884c\u4e86\u66f4\u4ed4\u7ec6\u7684\u89c2\u5bdf\uff1a\u8f93\u5165\u5c42\u9762\u7684\u89e3\u91ca\uff0c\u4ea7\u751f\u8f93\u5165\u56fe\u7684\u89e3\u91ca\u6027\u5b50\u56fe\uff1b\u4ee5\u53ca\u5c42\u9762\u7684\u89e3\u91ca\uff0c\u4ea7\u751f\u8ba1\u7b97\u56fe\u7684\u89e3\u91ca\u6027\u5b50\u56fe\u3002\u5efa\u7acb\u4e86\u6d41\u884c\u7684\u57fa\u4e8e\u6270\u52a8\u7684\u65b9\u6cd5\u548c\u7ecf\u5178\u7684\u57fa\u4e8e\u68af\u5ea6\u7684\u65b9\u6cd5\u4e4b\u95f4\u7684\u7b2c\u4e00\u4e2a\u7406\u8bba\u8054\u7cfb\uff0c\u5e76\u6307\u51fa\u4e86\u5176\u4ed6\u6700\u8fd1\u63d0\u51fa\u7684\u65b9\u6cd5\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "result": "\u5728\u8f93\u5165\u5c42\u9762\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u5728\u4ec0\u4e48\u6761\u4ef6\u4e0bGNNExplainer\u53ef\u4ee5\u88ab\u57fa\u4e8e\u8fb9\u7f18\u68af\u5ea6\u7b26\u53f7\u7684\u7b80\u5355\u542f\u53d1\u5f0f\u65b9\u6cd5\u8fd1\u4f3c\u3002\u5728\u5c42\u9762\u8bbe\u7f6e\u4e2d\uff0c\u6211\u4eec\u6307\u51fa\u8fb9\u7f18\u68af\u5ea6\u7b49\u540c\u4e8e\u7ebf\u6027GNN\u7684\u906e\u6321\u641c\u7d22\u3002\u6700\u540e\uff0c\u6211\u4eec\u901a\u8fc7\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86\u6211\u4eec\u7684\u7406\u8bba\u7ed3\u679c\u5982\u4f55\u5728\u5b9e\u8df5\u4e2d\u4f53\u73b0\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u63ed\u793a\u4e86GNN\u89e3\u91ca\u65b9\u6cd5\u5728\u8f93\u5165\u5c42\u9762\u548c\u5c42\u9762\u7684\u6027\u8d28\uff0c\u5e76\u5efa\u7acb\u4e86\u4e0d\u540c\u65b9\u6cd5\u4e4b\u95f4\u7684\u8054\u7cfb\u3002"}}
{"id": "2508.01739", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01739", "abs": "https://arxiv.org/abs/2508.01739", "authors": ["Cheng Wang", "ziru Liu", "Pengcheng Tang", "Mingyu Zhang", "Quanyu Dai", "Yue Zhu"], "title": "Enhancing the Preference Extractor in Multi-turn Dialogues: From Annotating Disasters to Accurate Preference Extraction", "comment": null, "summary": "Identifying user preferences in dialogue systems is a pivotal aspect of\nproviding satisfying services. Current research shows that using large language\nmodels (LLMs) to fine-tune a task-specific preference extractor yields\nexcellent results in terms of accuracy and generalization. However, the primary\nchallenge stems from the inherent difficulty in obtaining high-quality labeled\nmulti-turn dialogue data. Accurately tracking user preference transitions\nacross turns not only demands intensive domain expertise and contextual\nconsistency maintenance for annotators (termed \\textbf{``Annotating\nDisaster''}) but also complicates model training due to error propagation in\nsequential dependency learning. Inspired by the observation that multi-turn\npreference extraction can be decomposed into iterative executions of one-turn\nextraction processes. We propose a novel dialogue data generation framework\nnamed \\textbf{IterChat}. First, we construct a new data format that categorizes\nthe dialogue data into attributed historical preferences and one-turn\ndialogues. This reduces the probability of annotation errors and improves\nannotation efficiency. Then, to generate a high-quality and diverse dialogue\ndataset, we adopt GPT4 to pre-define the preference slots in the target\npreference extractor task and then randomly sample the subset of the slots and\ntheir corresponding schema values to create the dialogue datasets. Experimental\nresults indicate that fine-tuning or only few-shot prompting with the new\ndialogue format yields superior performance compared to the original multi-turn\ndialogues. Additionally, the new data format improves annotator efficiency with\na win rate of 28.4\\% higher than the original multi-turn dialogues.", "AI": {"tldr": "Proposes IterChat, a dialogue data generation framework using GPT4 and a new data format to improve the performance and annotator efficiency for user preference extraction in dialogue systems.", "motivation": "The primary challenge stems from the inherent difficulty in obtaining high-quality labeled multi-turn dialogue data. Accurately tracking user preference transitions across turns not only demands intensive domain expertise and contextual consistency maintenance for annotators but also complicates model training due to error propagation in sequential dependency learning.", "method": "We propose a novel dialogue data generation framework named IterChat. First, we construct a new data format that categorizes the dialogue data into attributed historical preferences and one-turn dialogues. Then, to generate a high-quality and diverse dialogue dataset, we adopt GPT4 to pre-define the preference slots in the target preference extractor task and then randomly sample the subset of the slots and their corresponding schema values to create the dialogue datasets.", "result": "Fine-tuning or few-shot prompting with the new dialogue format yields superior performance compared to the original multi-turn dialogues. The new data format improves annotator efficiency with a win rate of 28.4% higher than the original multi-turn dialogues.", "conclusion": "Fine-tuning or few-shot prompting with the new dialogue format yields superior performance compared to the original multi-turn dialogues. Additionally, the new data format improves annotator efficiency with a win rate of 28.4% higher than the original multi-turn dialogues."}}
{"id": "2508.01219", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.01219", "abs": "https://arxiv.org/abs/2508.01219", "authors": ["Anzhe Cheng", "Chenzhong Yin", "Mingxi Cheng", "Shukai Duan", "Shahin Nazarian", "Paul Bogdan"], "title": "Eigen Neural Network: Unlocking Generalizable Vision with Eigenbasis", "comment": null, "summary": "The remarkable success of Deep Neural Networks(DNN) is driven by\ngradient-based optimization, yet this process is often undermined by its\ntendency to produce disordered weight structures, which harms feature clarity\nand degrades learning dynamics. To address this fundamental representational\nflaw, we introduced the Eigen Neural Network (ENN), a novel architecture that\nreparameterizes each layer's weights in a layer-shared, learned orthonormal\neigenbasis. This design enforces decorrelated, well-aligned weight dynamics\naxiomatically, rather than through regularization, leading to more structured\nand discriminative feature representations. When integrated with standard BP,\nENN consistently outperforms state-of-the-art methods on large-scale image\nclassification benchmarks, including ImageNet, and its superior representations\ngeneralize to set a new benchmark in cross-modal image-text retrieval.\nFurthermore, ENN's principled structure enables a highly efficient,\nbackpropagation-free(BP-free) local learning variant, ENN-$\\ell$. This variant\nnot only resolves BP's procedural bottlenecks to achieve over 2$\\times$\ntraining speedup via parallelism, but also, remarkably, surpasses the accuracy\nof end-to-end backpropagation. ENN thus presents a new architectural paradigm\nthat directly remedies the representational deficiencies of BP, leading to\nenhanced performance and enabling a more efficient, parallelizable training\nregime.", "AI": {"tldr": "Introduces Eigen Neural Network (ENN) to address representational flaws in DNNs, achieving superior performance and efficient training.", "motivation": "DNN's tendency to produce disordered weight structures, which harms feature clarity and degrades learning dynamics.", "method": "Eigen Neural Network (ENN), a novel architecture that reparameterizes each layer's weights in a layer-shared, learned orthonormal eigenbasis.", "result": "ENN consistently outperforms state-of-the-art methods on large-scale image classification benchmarks, including ImageNet, and its superior representations generalize to set a new benchmark in cross-modal image-text retrieval. ENN's principled structure enables a highly efficient, backpropagation-free(BP-free) local learning variant, ENN-$\\\\ell$. This variant not only resolves BP's procedural bottlenecks to achieve over 2$\\times$ training speedup via parallelism, but also, remarkably, surpasses the accuracy of end-to-end backpropagation.", "conclusion": "ENN presents a new architectural paradigm that directly remedies the representational deficiencies of BP, leading to enhanced performance and enabling a more efficient, parallelizable training regime."}}
{"id": "2508.01561", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01561", "abs": "https://arxiv.org/abs/2508.01561", "authors": ["Zijian Guo", "\u0130lker I\u015f\u0131k", "H. M. Sabbir Ahmad", "Wenchao Li"], "title": "One Subgoal at a Time: Zero-Shot Generalization to Arbitrary Linear Temporal Logic Requirements in Multi-Task Reinforcement Learning", "comment": null, "summary": "Generalizing to complex and temporally extended task objectives and safety\nconstraints remains a critical challenge in reinforcement learning (RL). Linear\ntemporal logic (LTL) offers a unified formalism to specify such requirements,\nyet existing methods are limited in their abilities to handle nested\nlong-horizon tasks and safety constraints, and cannot identify situations when\na subgoal is not satisfiable and an alternative should be sought. In this\npaper, we introduce GenZ-LTL, a method that enables zero-shot generalization to\narbitrary LTL specifications. GenZ-LTL leverages the structure of B\\\"uchi\nautomata to decompose an LTL task specification into sequences of reach-avoid\nsubgoals. Contrary to the current state-of-the-art method that conditions on\nsubgoal sequences, we show that it is more effective to achieve zero-shot\ngeneralization by solving these reach-avoid problems \\textit{one subgoal at a\ntime} through proper safe RL formulations. In addition, we introduce a novel\nsubgoal-induced observation reduction technique that can mitigate the\nexponential complexity of subgoal-state combinations under realistic\nassumptions. Empirical results show that GenZ-LTL substantially outperforms\nexisting methods in zero-shot generalization to unseen LTL specifications.", "AI": {"tldr": "GenZ-LTL\u901a\u8fc7\u5c06LTL\u4efb\u52a1\u5206\u89e3\u4e3a\u5b50\u76ee\u6807\u5e76\u9010\u4e2a\u89e3\u51b3\uff0c\u5b9e\u73b0\u4e86\u5bf9\u672a\u89c1\u8fc7\u7684LTL\u89c4\u8303\u7684\u96f6\u6837\u672c\u6cdb\u5316\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u63a8\u5e7f\u5230\u590d\u6742\u548c\u65f6\u95f4\u6269\u5c55\u7684\u4efb\u52a1\u76ee\u6807\u548c\u5b89\u5168\u7ea6\u675f\u65b9\u9762\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\uff08LTL\uff09\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u5f62\u5f0f\u6765\u6307\u5b9a\u8fd9\u4e9b\u9700\u6c42\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5d4c\u5957\u7684\u957f\u89c6\u8ddd\u4efb\u52a1\u548c\u5b89\u5168\u7ea6\u675f\u65b9\u9762\u7684\u80fd\u529b\u6709\u9650\uff0c\u5e76\u4e14\u65e0\u6cd5\u8bc6\u522b\u5b50\u76ee\u6807\u65e0\u6cd5\u6ee1\u8db3\u4ee5\u53ca\u5e94\u8be5\u5bfb\u6c42\u66ff\u4ee3\u65b9\u6848\u7684\u60c5\u51b5\u3002", "method": "GenZ-LTL\u5229\u7528B\u00fcchi\u81ea\u52a8\u673a\u7684\u7ed3\u6784\u5c06LTL\u4efb\u52a1\u89c4\u8303\u5206\u89e3\u4e3areach-avoid\u5b50\u76ee\u6807\u7684\u5e8f\u5217\uff0c\u5e76\u901a\u8fc7\u9002\u5f53\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u516c\u5f0f\u4e00\u6b21\u89e3\u51b3\u4e00\u4e2a\u5b50\u76ee\u6807\u3002", "result": "GenZ-LTL\u80fd\u591f\u5b9e\u73b0\u5bf9\u4efb\u610fLTL\u89c4\u8303\u7684\u96f6\u6837\u672c\u6cdb\u5316\uff0c\u5e76\u4e14\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "GenZ-LTL\u5728\u96f6\u6837\u672c\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684LTL\u89c4\u8303\u65b9\u9762\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.01049", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.01049", "abs": "https://arxiv.org/abs/2508.01049", "authors": ["Nicholas E. Corrado", "Josiah P. Hanna"], "title": "Centralized Adaptive Sampling for Reliable Co-Training of Independent Multi-Agent Policies", "comment": null, "summary": "Independent on-policy policy gradient algorithms are widely used for\nmulti-agent reinforcement learning (MARL) in cooperative and no-conflict games,\nbut they are known to converge suboptimally when each agent's policy gradient\npoints toward a suboptimal equilibrium. In this work, we identify a subtler\nfailure mode that arises \\textit{even when the expected policy gradients of all\nagents point toward an optimal solution.} After collecting a finite set of\ntrajectories, stochasticity in independent action sampling can cause the joint\ndata distribution to deviate from the expected joint on-policy distribution.\nThis \\textit{sampling error} w.r.t. the joint on-policy distribution produces\ninaccurate gradient estimates that can lead agents to converge suboptimally. In\nthis paper, we investigate if joint sampling error can be reduced through\ncoordinated action selection and whether doing so improves the reliability of\npolicy gradient learning in MARL. Toward this end, we introduce an adaptive\naction sampling approach to reduce joint sampling error. Our method,\nMulti-Agent Proximal Robust On-Policy Sampling (MA-PROPS), uses a centralized\nbehavior policy that we continually adapt to place larger probability on joint\nactions that are currently under-sampled w.r.t. the current joint policy. We\nempirically evaluate MA-PROPS in a diverse range of multi-agent games and\ndemonstrate that (1) MA-PROPS reduces joint sampling error more efficiently\nthan standard on-policy sampling and (2) improves the reliability of\nindependent policy gradient algorithms, increasing the fraction of training\nruns that converge to an optimal joint policy.", "AI": {"tldr": "This paper introduces MA-PROPS, an adaptive action sampling approach, to reduce joint sampling error and improve the reliability of policy gradient learning in MARL.", "motivation": "Independent on-policy policy gradient algorithms are known to converge suboptimally when each agent's policy gradient points toward a suboptimal equilibrium. Stochasticity in independent action sampling can cause the joint data distribution to deviate from the expected joint on-policy distribution. This sampling error w.r.t. the joint on-policy distribution produces inaccurate gradient estimates that can lead agents to converge suboptimally.", "method": "adaptive action sampling approach to reduce joint sampling error. Uses a centralized behavior policy that we continually adapt to place larger probability on joint actions that are currently under-sampled w.r.t. the current joint policy. Method name: Multi-Agent Proximal Robust On-Policy Sampling (MA-PROPS)", "result": "MA-PROPS reduces joint sampling error more efficiently than standard on-policy sampling. MA-PROPS improves the reliability of independent policy gradient algorithms, increasing the fraction of training runs that converge to an optimal joint policy.", "conclusion": "MA-PROPS reduces joint sampling error more efficiently than standard on-policy sampling and improves the reliability of independent policy gradient algorithms, increasing the fraction of training runs that converge to an optimal joint policy."}}
{"id": "2508.01754", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.01754", "abs": "https://arxiv.org/abs/2508.01754", "authors": ["Alva West", "Yixuan Weng", "Minjun Zhu", "Luodan Zhang", "Zhen Lin", "Guangsheng Bao", "Yue Zhang"], "title": "AI-Generated Text is Non-Stationary: Detection via Temporal Tomography", "comment": null, "summary": "The field of AI-generated text detection has evolved from supervised\nclassification to zero-shot statistical analysis. However, current approaches\nshare a fundamental limitation: they aggregate token-level measurements into\nscalar scores, discarding positional information about where anomalies occur.\nOur empirical analysis reveals that AI-generated text exhibits significant\nnon-stationarity, statistical properties vary by 73.8\\% more between text\nsegments compared to human writing. This discovery explains why existing\ndetectors fail against localized adversarial perturbations that exploit this\noverlooked characteristic. We introduce Temporal Discrepancy Tomography (TDT),\na novel detection paradigm that preserves positional information by\nreformulating detection as a signal processing task. TDT treats token-level\ndiscrepancies as a time-series signal and applies Continuous Wavelet Transform\nto generate a two-dimensional time-scale representation, capturing both the\nlocation and linguistic scale of statistical anomalies. On the RAID benchmark,\nTDT achieves 0.855 AUROC (7.1\\% improvement over the best baseline). More\nimportantly, TDT demonstrates robust performance on adversarial tasks, with\n14.1\\% AUROC improvement on HART Level 2 paraphrasing attacks. Despite its\nsophisticated analysis, TDT maintains practical efficiency with only 13\\%\ncomputational overhead. Our work establishes non-stationarity as a fundamental\ncharacteristic of AI-generated text and demonstrates that preserving temporal\ndynamics is essential for robust detection.", "AI": {"tldr": "This paper introduces Temporal Discrepancy Tomography (TDT), a new method for detecting AI-generated text that considers the location and scale of statistical anomalies. TDT outperforms existing methods, especially against adversarial attacks, with minimal computational overhead.", "motivation": "current approaches share a fundamental limitation: they aggregate token-level measurements into scalar scores, discarding positional information about where anomalies occur. AI-generated text exhibits significant non-stationarity, statistical properties vary by 73.8% more between text segments compared to human writing. This discovery explains why existing detectors fail against localized adversarial perturbations that exploit this overlooked characteristic.", "method": "Temporal Discrepancy Tomography (TDT), a novel detection paradigm that preserves positional information by reformulating detection as a signal processing task. TDT treats token-level discrepancies as a time-series signal and applies Continuous Wavelet Transform to generate a two-dimensional time-scale representation, capturing both the location and linguistic scale of statistical anomalies.", "result": "TDT achieves 0.855 AUROC (7.1% improvement over the best baseline). TDT demonstrates robust performance on adversarial tasks, with 14.1% AUROC improvement on HART Level 2 paraphrasing attacks. TDT maintains practical efficiency with only 13% computational overhead.", "conclusion": "non-stationarity is a fundamental characteristic of AI-generated text and demonstrates that preserving temporal dynamics is essential for robust detection"}}
{"id": "2508.01223", "categories": ["cs.CV", "68T10", "I.4.6"], "pdf": "https://arxiv.org/pdf/2508.01223", "abs": "https://arxiv.org/abs/2508.01223", "authors": ["Changqing Xu", "Guoqing Sun", "Yi Liu", "Xinfang Liao", "Yintang Yang"], "title": "ParaRevSNN: A Parallel Reversible Spiking Neural Network for Efficient Training and Inference", "comment": "8 pages, 3 figures, submitted to AAAI 2026", "summary": "Reversible Spiking Neural Networks (RevSNNs) enable memory-efficient training\nby reconstructing forward activations during backpropagation, but suffer from\nhigh latency due to strictly sequential computation. To overcome this\nlimitation, we propose ParaRevSNN, a parallel reversible SNN architecture that\ndecouples sequential dependencies between reversible blocks while preserving\nreversibility. This design enables inter-block parallelism, significantly\naccelerating training and inference while retaining the memory-saving benefits\nof reversibility. Experiments on CIFAR10, CIFAR100, CIFAR10-DVS, and DVS128\nGesture demonstrate that ParaRevSNN matches or exceeds the accuracy of standard\nRevSNNs, while reducing training time by up to 35.2\\% and inference time to\n18.15\\%, making it well-suited for deployment in resource-constrained\nscenarios.", "AI": {"tldr": "ParaRevSNN\u662f\u4e00\u79cd\u5e76\u884c\u7684\u53ef\u9006SNN\u67b6\u6784\uff0c\u53ef\u51cf\u5c11\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u5185\u5b58\u8282\u7701\u7684\u4f18\u52bf\u3002", "motivation": "\u53ef\u9006\u8109\u51b2\u795e\u7ecf\u7f51\u7edc(RevSNNs)\u901a\u8fc7\u5728\u53cd\u5411\u4f20\u64ad\u8fc7\u7a0b\u4e2d\u91cd\u5efa\u524d\u5411\u6fc0\u6d3b\u6765\u5b9e\u73b0\u5185\u5b58\u9ad8\u6548\u8bad\u7ec3\uff0c\u4f46\u7531\u4e8e\u4e25\u683c\u7684\u987a\u5e8f\u8ba1\u7b97\u800c\u5b58\u5728\u9ad8\u5ef6\u8fdf\u3002", "method": "ParaRevSNN\uff0c\u4e00\u79cd\u5e76\u884c\u7684\u53ef\u9006SNN\u67b6\u6784\uff0c\u5b83\u89e3\u8026\u4e86\u53ef\u9006\u5757\u4e4b\u95f4\u7684\u987a\u5e8f\u4f9d\u8d56\u6027\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u53ef\u9006\u6027\u3002", "result": "ParaRevSNN\u7684\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u4e8635.2%\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u5c11\u523018.15%\u3002", "conclusion": "ParaRevSNN\u5728CIFAR10\u3001CIFAR100\u3001CIFAR10-DVS\u548cDVS128\u624b\u52bf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cParaRevSNN\u7684\u7cbe\u5ea6\u4e0e\u6807\u51c6RevSNN\u76f8\u5339\u914d\u6216\u8d85\u8fc7\u6807\u51c6RevSNN\uff0c\u540c\u65f6\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u4e8635.2%\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u5c11\u523018.15%\uff0c\u4f7f\u5176\u975e\u5e38\u9002\u5408\u5728\u8d44\u6e90\u53d7\u9650\u7684\u573a\u666f\u4e2d\u90e8\u7f72\u3002"}}
{"id": "2508.01581", "categories": ["cs.AI", "math.CO", "stat.CO"], "pdf": "https://arxiv.org/pdf/2508.01581", "abs": "https://arxiv.org/abs/2508.01581", "authors": ["David Pearl", "Matthew Murphy", "James Intriligator"], "title": "Polymorphic Combinatorial Frameworks (PCF): Guiding the Design of Mathematically-Grounded, Adaptive AI Agents", "comment": null, "summary": "The Polymorphic Combinatorial Framework (PCF) leverages Large Language Models\n(LLMs) and mathematical frameworks to guide the meta-prompt enabled design of\nsolution spaces and adaptive AI agents for complex, dynamic environments.\nUnlike static agent architectures, PCF enables real-time parameter\nreconfiguration through mathematically-grounded combinatorial spaces, allowing\nagents to adapt their core behavioral traits dynamically. Grounded in\ncombinatorial logic, topos theory, and rough fuzzy set theory, PCF defines a\nmultidimensional SPARK parameter space (Skills, Personalities, Approaches,\nResources, Knowledge) to capture agent behaviors. This paper demonstrates how\nLLMs can parameterize complex spaces and estimate likely parameter\nvalues/variabilities. Using PCF, we parameterized mock caf\\'e domains (five\nlevels of complexity), estimated variables/variabilities, and conducted over\n1.25 million Monte Carlo simulations. The results revealed trends in agent\nadaptability and performance across the five complexity tiers, with diminishing\nreturns at higher complexity levels highlighting thresholds for scalable\ndesigns. PCF enables the generation of optimized agent configurations for\nspecific scenarios while maintaining logical consistency. This framework\nsupports scalable, dynamic, explainable, and ethical AI applications in domains\nlike customer service, healthcare, robotics, and collaborative systems, paving\nthe way for adaptable and cooperative next-generation polymorphic agents.", "AI": {"tldr": "PCF\u4f7f\u7528LLM\u548c\u6570\u5b66\u6846\u67b6\u6765\u8bbe\u8ba1\u81ea\u9002\u5e94AI\u4ee3\u7406\uff0c\u5e76\u5df2\u901a\u8fc7\u8d85\u8fc7125\u4e07\u6b21\u8499\u7279\u5361\u6d1b\u6a21\u62df\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u4e0e\u9759\u6001\u4ee3\u7406\u67b6\u6784\u4e0d\u540c\uff0cPCF\u901a\u8fc7\u6570\u5b66\u7ec4\u5408\u7a7a\u95f4\u5b9e\u73b0\u5b9e\u65f6\u53c2\u6570\u91cd\u914d\u7f6e\uff0c\u4ece\u800c\u4f7f\u4ee3\u7406\u80fd\u591f\u52a8\u6001\u5730\u8c03\u6574\u5176\u6838\u5fc3\u884c\u4e3a\u7279\u5f81\u3002", "method": "PCF\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u6570\u5b66\u6846\u67b6\u6765\u6307\u5bfc\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\u548c\u81ea\u9002\u5e94AI\u4ee3\u7406\u7684\u5143\u63d0\u793a\u8bbe\u8ba1\uff0c\u7528\u4e8e\u590d\u6742\u3001\u52a8\u6001\u7684\u73af\u5883\u3002", "result": "\u7ed3\u679c\u63ed\u793a\u4e86\u4ee3\u7406\u5728\u4e94\u4e2a\u590d\u6742\u6027\u7b49\u7ea7\u4e2d\u7684\u9002\u5e94\u6027\u548c\u6027\u80fd\u8d8b\u52bf\uff0c\u5728\u8f83\u9ad8\u590d\u6742\u6027\u7b49\u7ea7\u4e2d\u6536\u76ca\u9012\u51cf\uff0c\u4ece\u800c\u7a81\u51fa\u4e86\u53ef\u6269\u5c55\u8bbe\u8ba1\u7684\u9608\u503c\u3002", "conclusion": "PCF\u5b9e\u73b0\u4e86\u9488\u5bf9\u7279\u5b9a\u573a\u666f\u7684\u4f18\u5316\u4ee3\u7406\u914d\u7f6e\u751f\u6210\uff0c\u5e76\u4fdd\u6301\u903b\u8f91\u4e00\u81f4\u6027\u3002\u8be5\u6846\u67b6\u652f\u6301\u5728\u5ba2\u6237\u670d\u52a1\u3001\u533b\u7597\u4fdd\u5065\u3001\u673a\u5668\u4eba\u548c\u534f\u4f5c\u7cfb\u7edf\u7b49\u9886\u57df\u4e2d\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u52a8\u6001\u3001\u53ef\u89e3\u91ca\u548c\u7b26\u5408\u9053\u5fb7\u89c4\u8303\u7684AI\u5e94\u7528\uff0c\u4ece\u800c\u4e3a\u9002\u5e94\u6027\u5f3a\u4e14\u534f\u4f5c\u7684\u4e0b\u4e00\u4ee3\u591a\u6001\u4ee3\u7406\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2508.01055", "categories": ["cs.LG", "cs.AI", "q-bio.BM", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.01055", "abs": "https://arxiv.org/abs/2508.01055", "authors": ["Xuan Liu", "Siru Ouyang", "Xianrui Zhong", "Jiawei Han", "Huimin Zhao"], "title": "FGBench: A Dataset and Benchmark for Molecular Property Reasoning at Functional Group-Level in Large Language Models", "comment": "20 pages, 20 figures", "summary": "Large language models (LLMs) have gained significant attention in chemistry.\nHowever, most existing datasets center on molecular-level property prediction\nand overlook the role of fine-grained functional group (FG) information.\nIncorporating FG-level data can provide valuable prior knowledge that links\nmolecular structures with textual descriptions, which can be used to build more\ninterpretable, structure-aware LLMs for reasoning on molecule-related tasks.\nMoreover, LLMs can learn from such fine-grained information to uncover hidden\nrelationships between specific functional groups and molecular properties,\nthereby advancing molecular design and drug discovery. Here, we introduce\nFGBench, a dataset comprising 625K molecular property reasoning problems with\nfunctional group information. Functional groups are precisely annotated and\nlocalized within the molecule, which ensures the dataset's interoperability\nthereby facilitating further multimodal applications. FGBench includes both\nregression and classification tasks on 245 different functional groups across\nthree categories for molecular property reasoning: (1) single functional group\nimpacts, (2) multiple functional group interactions, and (3) direct molecular\ncomparisons. In the benchmark of state-of-the-art LLMs on 7K curated data, the\nresults indicate that current LLMs struggle with FG-level property reasoning,\nhighlighting the need to enhance reasoning capabilities in LLMs for chemistry\ntasks. We anticipate that the methodology employed in FGBench to construct\ndatasets with functional group-level information will serve as a foundational\nframework for generating new question-answer pairs, enabling LLMs to better\nunderstand fine-grained molecular structure-property relationships. The dataset\nand evaluation code are available at\n\\href{https://github.com/xuanliugit/FGBench}{https://github.com/xuanliugit/FGBench}.", "AI": {"tldr": "\u63d0\u51fa\u4e86FGBench\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u63d0\u9ad8LLM\u5728\u5316\u5b66\u4efb\u52a1\u4e2d\u5bf9\u529f\u80fd\u7ec4\u7ea7\u522b\u5c5e\u6027\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u4e3b\u8981\u96c6\u4e2d\u5728\u5206\u5b50\u6c34\u5e73\u7684\u6027\u8d28\u9884\u6d4b\u4e0a\uff0c\u5ffd\u7565\u4e86\u7ec6\u7c92\u5ea6\u7684\u529f\u80fd\u7ec4\uff08FG\uff09\u4fe1\u606f\u7684\u4f5c\u7528\u3002\u7eb3\u5165FG\u6c34\u5e73\u7684\u6570\u636e\u53ef\u4ee5\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u5c06\u5206\u5b50\u7ed3\u6784\u4e0e\u6587\u672c\u63cf\u8ff0\u8054\u7cfb\u8d77\u6765\uff0c\u4ece\u800c\u6784\u5efa\u66f4\u6613\u4e8e\u7406\u89e3\u7684\u3001\u7ed3\u6784\u611f\u77e5\u7684LLM\uff0c\u7528\u4e8e\u5206\u5b50\u76f8\u5173\u4efb\u52a1\u7684\u63a8\u7406\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u529f\u80fd\u7ec4\u4fe1\u606f\u7684\u5206\u5b50\u6027\u8d28\u63a8\u7406\u95ee\u9898\u7684\u6570\u636e\u96c6FGBench", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b625K\u5206\u5b50\u6027\u8d28\u63a8\u7406\u95ee\u9898\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b245\u4e2a\u4e0d\u540c\u7684\u529f\u80fd\u7ec4\uff0c\u6db5\u76d6\u5206\u5b50\u6027\u8d28\u63a8\u7406\u7684\u4e09\u4e2a\u7c7b\u522b\u3002\u57287K\u7cbe\u9009\u6570\u636e\u4e0a\u5bf9\u6700\u5148\u8fdb\u7684LLM\u7684\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d\u7684LLM\u5728FG\u6c34\u5e73\u7684\u5c5e\u6027\u63a8\u7406\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "\u5f53\u524d\u7684LLM\u5728FG\u6c34\u5e73\u7684\u5c5e\u6027\u63a8\u7406\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u9700\u8981\u589e\u5f3aLLM\u5728\u5316\u5b66\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3002FGBench\u4e2d\u91c7\u7528\u7684\u6784\u5efa\u5177\u6709\u529f\u80fd\u7ec4\u7ea7\u522b\u4fe1\u606f\u7684\u6570\u636e\u96c6\u7684\u65b9\u6cd5\uff0c\u5c06\u4f5c\u4e3a\u751f\u6210\u65b0\u7684\u95ee\u7b54\u5bf9\u7684\u57fa\u7840\u6846\u67b6\uff0c\u4f7fLLM\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u7ec6\u7c92\u5ea6\u7684\u5206\u5b50\u7ed3\u6784-\u6027\u8d28\u5173\u7cfb\u3002"}}
{"id": "2508.01781", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.01781", "abs": "https://arxiv.org/abs/2508.01781", "authors": ["Manuel Cossio"], "title": "A comprehensive taxonomy of hallucinations in Large Language Models", "comment": "55 pages, 16 figures, 3 tables", "summary": "Large language models (LLMs) have revolutionized natural language processing,\nyet their propensity for hallucination, generating plausible but factually\nincorrect or fabricated content, remains a critical challenge. This report\nprovides a comprehensive taxonomy of LLM hallucinations, beginning with a\nformal definition and a theoretical framework that posits its inherent\ninevitability in computable LLMs, irrespective of architecture or training. It\nexplores core distinctions, differentiating between intrinsic (contradicting\ninput context) and extrinsic (inconsistent with training data or reality), as\nwell as factuality (absolute correctness) and faithfulness (adherence to\ninput). The report then details specific manifestations, including factual\nerrors, contextual and logical inconsistencies, temporal disorientation,\nethical violations, and task-specific hallucinations across domains like code\ngeneration and multimodal applications. It analyzes the underlying causes,\ncategorizing them into data-related issues, model-related factors, and\nprompt-related influences. Furthermore, the report examines cognitive and human\nfactors influencing hallucination perception, surveys evaluation benchmarks and\nmetrics for detection, and outlines architectural and systemic mitigation\nstrategies. Finally, it introduces web-based resources for monitoring LLM\nreleases and performance. This report underscores the complex, multifaceted\nnature of LLM hallucinations and emphasizes that, given their theoretical\ninevitability, future efforts must focus on robust detection, mitigation, and\ncontinuous human oversight for responsible and reliable deployment in critical\napplications.", "AI": {"tldr": "\u672c\u62a5\u544a\u5168\u9762\u5206\u6790\u4e86LLM\u5e7b\u89c9\uff0c\u5f3a\u8c03\u4e86\u5176\u590d\u6742\u6027\u548c\u4e0d\u53ef\u907f\u514d\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u7f13\u89e3\u7b56\u7565\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5df2\u7ecf\u5f7b\u5e95\u6539\u53d8\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff0c\u4f46\u5b83\u4eec\u4ea7\u751f\u5e7b\u89c9\u7684\u503e\u5411\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002", "method": "\u5bf9LLM\u5e7b\u89c9\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5206\u7c7b\uff0c\u5305\u62ec\u6b63\u5f0f\u5b9a\u4e49\u3001\u7406\u8bba\u6846\u67b6\u4ee5\u53ca\u5bf9\u5185\u5728\u548c\u5916\u5728\u5e7b\u89c9\u7684\u533a\u5206\u3002", "result": "\u8be6\u7ec6\u63cf\u8ff0\u4e86LLM\u5e7b\u89c9\u7684\u5177\u4f53\u8868\u73b0\uff0c\u5206\u6790\u4e86\u5176\u6839\u672c\u539f\u56e0\uff0c\u5e76\u8003\u5bdf\u4e86\u5f71\u54cd\u5e7b\u89c9\u611f\u77e5\u7684\u8ba4\u77e5\u548c\u4eba\u4e3a\u56e0\u7d20\u3002", "conclusion": "LLM\u5e7b\u89c9\u662f\u4e0d\u53ef\u907f\u514d\u7684\uff0c\u672a\u6765\u7684\u5de5\u4f5c\u5fc5\u987b\u4fa7\u91cd\u4e8e\u5f3a\u5927\u7684\u68c0\u6d4b\u3001\u7f13\u89e3\u548c\u6301\u7eed\u7684\u4eba\u5de5\u76d1\u7763\uff0c\u4ee5\u4fbf\u5728\u5173\u952e\u5e94\u7528\u4e2d\u8d1f\u8d23\u4efb\u548c\u53ef\u9760\u5730\u90e8\u7f72\u3002"}}
{"id": "2508.01225", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01225", "abs": "https://arxiv.org/abs/2508.01225", "authors": ["Xinyu Chen", "Haotian Zhai", "Can Zhang", "Xiupeng Shi", "Ruirui Li"], "title": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of Vision-Language Models", "comment": "Accepted by ICCV 2025", "summary": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance.", "AI": {"tldr": "Proposes a Multi-Cache enhanced Prototype-based Test-Time Adaptation (MCP) and MCP++ to achieve state-of-the-art generalization performance.", "motivation": "Existing cache-enhanced TTA methods rely on a low-entropy criterion to select samples for prototype construction, assuming intra-class compactness. However, low-entropy samples may be unreliable under distribution shifts, and the resulting prototypes may not ensure compact intra-class distributions. This study identifies a positive correlation between cache-enhanced performance and intra-class compactness.", "method": "a Multi-Cache enhanced Prototype-based Test-Time Adaptation (MCP) featuring three caches: an entropy cache, an align cache, and a negative cache. MCP++ incorporates cross-modal prototype alignment and residual learning, introducing prototype residual fine-tuning.", "result": "Comparative and ablation experiments across 15 downstream tasks demonstrate that the proposed method and framework achieve state-of-the-art generalization performance.", "conclusion": "The proposed method and framework achieve state-of-the-art generalization performance."}}
{"id": "2508.01623", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01623", "abs": "https://arxiv.org/abs/2508.01623", "authors": ["Tadisetty Sai Yashwanth", "Dhatri C"], "title": "A Multi-Agent Pokemon Tournament for Evaluating Strategic Reasoning of Large Language Models", "comment": null, "summary": "This research presents LLM Pokemon League, a competitive tournament system\nthat leverages Large Language Models (LLMs) as intelligent agents to simulate\nstrategic decision-making in Pok\\'emon battles. The platform is designed to\nanalyze and compare the reasoning, adaptability, and tactical depth exhibited\nby different LLMs in a type-based, turn-based combat environment. By\nstructuring the competition as a single-elimination tournament involving\ndiverse AI trainers, the system captures detailed decision logs, including\nteam-building rationale, action selection strategies, and switching decisions.\nThe project enables rich exploration into comparative AI behavior, battle\npsychology, and meta-strategy development in constrained, rule-based game\nenvironments. Through this system, we investigate how modern LLMs understand,\nadapt, and optimize decisions under uncertainty, making Pok\\'emon League a\nnovel benchmark for AI research in strategic reasoning and competitive\nlearning.", "AI": {"tldr": "LLM Pokemon League, a tournament system using LLMs as agents in Pokemon battles, is presented as a benchmark for AI research in strategic reasoning and competitive learning.", "motivation": "The research aims to analyze and compare the reasoning, adaptability, and tactical depth exhibited by different LLMs in a type-based, turn-based combat environment.", "method": "A competitive tournament system is built that leverages Large Language Models (LLMs) as intelligent agents to simulate strategic decision-making in Pokemon battles. The competition is structured as a single-elimination tournament involving diverse AI trainers.", "result": "The system captures detailed decision logs, including team-building rationale, action selection strategies, and switching decisions, enabling rich exploration into comparative AI behavior, battle psychology, and meta-strategy development.", "conclusion": "LLM Pokemon League is introduced as a novel benchmark for AI research in strategic reasoning and competitive learning, showcasing how LLMs understand, adapt, and optimize decisions under uncertainty."}}
{"id": "2508.01077", "categories": ["cs.LG", "cs.AI", "I.2.6"], "pdf": "https://arxiv.org/pdf/2508.01077", "abs": "https://arxiv.org/abs/2508.01077", "authors": ["Johann Birnick"], "title": "The Lattice Geometry of Neural Network Quantization -- A Short Equivalence Proof of GPTQ and Babai's algorithm", "comment": "9 pages, 4 figures", "summary": "We explain how data-driven quantization of a linear unit in a neural network\ncorresponds to solving the closest vector problem for a certain lattice\ngenerated by input data. We prove that the GPTQ algorithm is equivalent to\nBabai's well-known nearest-plane algorithm. We furthermore provide geometric\nintuition for both algorithms. Lastly, we note the consequences of these\nresults, in particular hinting at the possibility for using lattice basis\nreduction for better quantization.", "AI": {"tldr": "data-driven quantization <-> closest vector problem; GPTQ <-> Babai; lattice basis reduction may be better", "motivation": "data-driven quantization of a linear unit in a neural network corresponds to solving the closest vector problem for a certain lattice generated by input data", "method": "GPTQ algorithm is equivalent to Babai's nearest-plane algorithm", "result": "GPTQ algorithm is equivalent to Babai's nearest-plane algorithm; geometric intuition for both algorithms is provided", "conclusion": "lattice basis reduction may improve quantization"}}
{"id": "2508.01812", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.01812", "abs": "https://arxiv.org/abs/2508.01812", "authors": ["Amir DN Cohen", "Hilla Merhav", "Yoav Goldberg", "Reut Tsarfaty"], "title": "HeQ: a Large and Diverse Hebrew Reading Comprehension Benchmark", "comment": null, "summary": "Current benchmarks for Hebrew Natural Language Processing (NLP) focus mainly\non morpho-syntactic tasks, neglecting the semantic dimension of language\nunderstanding. To bridge this gap, we set out to deliver a Hebrew Machine\nReading Comprehension (MRC) dataset, where MRC is to be realized as extractive\nQuestion Answering. The morphologically rich nature of Hebrew poses a challenge\nto this endeavor: the indeterminacy and non-transparency of span boundaries in\nmorphologically complex forms lead to annotation inconsistencies,\ndisagreements, and flaws in standard evaluation metrics.\n  To remedy this, we devise a novel set of guidelines, a controlled\ncrowdsourcing protocol, and revised evaluation metrics that are suitable for\nthe morphologically rich nature of the language. Our resulting benchmark, HeQ\n(Hebrew QA), features 30,147 diverse question-answer pairs derived from both\nHebrew Wikipedia articles and Israeli tech news. Our empirical investigation\nreveals that standard evaluation metrics such as F1 scores and Exact Match (EM)\nare not appropriate for Hebrew (and other MRLs), and we propose a relevant\nenhancement.\n  In addition, our experiments show low correlation between models' performance\non morpho-syntactic tasks and on MRC, which suggests that models designed for\nthe former might underperform on semantics-heavy tasks. The development and\nexploration of HeQ illustrate some of the challenges MRLs pose in natural\nlanguage understanding (NLU), fostering progression towards more and better NLU\nmodels for Hebrew and other MRLs.", "AI": {"tldr": "This paper introduces HeQ, a Hebrew Machine Reading Comprehension dataset, and proposes revised evaluation metrics suitable for morphologically rich languages (MRLs).", "motivation": "Current benchmarks for Hebrew Natural Language Processing (NLP) focus mainly on morpho-syntactic tasks, neglecting the semantic dimension of language understanding.", "method": "We devise a novel set of guidelines, a controlled crowdsourcing protocol, and revised evaluation metrics that are suitable for the morphologically rich nature of the language.", "result": "Our resulting benchmark, HeQ (Hebrew QA), features 30,147 diverse question-answer pairs derived from both Hebrew Wikipedia articles and Israeli tech news. Our empirical investigation reveals that standard evaluation metrics such as F1 scores and Exact Match (EM) are not appropriate for Hebrew (and other MRLs), and we propose a relevant enhancement.  In addition, our experiments show low correlation between models' performance on morpho-syntactic tasks and on MRC, which suggests that models designed for the former might underperform on semantics-heavy tasks.", "conclusion": "The development and exploration of HeQ illustrate some of the challenges MRLs pose in natural language understanding (NLU), fostering progression towards more and better NLU models for Hebrew and other MRLs."}}
