{"id": "2507.01079", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.01079", "abs": "https://arxiv.org/abs/2507.01079", "authors": ["Taehwan Park", "Geonho Lee", "Min-Soo Kim"], "title": "MobileRAG: A Fast, Memory-Efficient, and Energy-Efficient Method for On-Device RAG", "comment": "14 pages", "summary": "Retrieval-Augmented Generation (RAG) has proven effective on server\ninfrastructures, but its application on mobile devices is still underexplored\ndue to limited memory and power resources. Existing vector search and RAG\nsolutions largely assume abundant computation resources, making them\nimpractical for on-device scenarios. In this paper, we propose MobileRAG, a\nfully on-device pipeline that overcomes these limitations by combining a\nmobile-friendly vector search algorithm, \\textit{EcoVector}, with a lightweight\n\\textit{Selective Content Reduction} (SCR) method. By partitioning and\npartially loading index data, EcoVector drastically reduces both memory\nfootprint and CPU usage, while the SCR method filters out irrelevant text to\ndiminish Language Model (LM) input size without degrading accuracy. Extensive\nexperiments demonstrated that MobileRAG significantly outperforms conventional\nvector search and RAG methods in terms of latency, memory usage, and power\nconsumption, while maintaining accuracy and enabling offline operation to\nsafeguard privacy in resource-constrained environments.", "AI": {"tldr": "MobileRAG is proposed to enable RAG on mobile devices by combining EcoVector and SCR, outperforming conventional methods in resource efficiency and maintaining accuracy.", "motivation": "Applying RAG on mobile devices is underexplored due to limited resources, and existing solutions are impractical for on-device scenarios.", "method": "Combining a mobile-friendly vector search algorithm, EcoVector, with a lightweight Selective Content Reduction (SCR) method.", "result": "EcoVector reduces memory and CPU usage, while SCR filters irrelevant text to diminish LM input size without degrading accuracy.", "conclusion": "MobileRAG significantly outperforms conventional methods in latency, memory, and power while maintaining accuracy and enabling offline operation."}}
{"id": "2507.01461", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.01461", "abs": "https://arxiv.org/abs/2507.01461", "authors": ["Styliani Kyrama", "Anastasios Gounaris"], "title": "Handling out-of-order input arrival in CEP engines on the edge combining optimistic, pessimistic and lazy evaluation", "comment": null, "summary": "In Complex Event Processing, handling out-of-order, late, and duplicate\nevents is critical for real-time analytics, especially on resource-constrained\ndevices that process heterogeneous data from multiple sources. We present\nLimeCEP, a hybrid CEP approach that combines lazy evaluation, buffering, and\nspeculative processing to efficiently handle data inconsistencies while\nsupporting multi-pattern detection under relaxed semantics. LimeCEP integrates\nKafka for efficient message ordering, retention, and duplicate elimination, and\noffers configurable strategies to trade off between accuracy, latency, and\nresource consumption. Compared to state-of-the-art systems like SASE and\nFlinkCEP, LimeCEP achieves up to six orders of magnitude lower latency, with up\nto 10 times lower memory usage and 6 times lower CPU utilization, while\nmaintaining near-perfect precision and recall under high-disorder input\nstreams, making it well-suited for non-cloud deployments.", "AI": {"tldr": "LimeCEP is a hybrid CEP approach that combines lazy evaluation, buffering, and speculative processing to efficiently handle data inconsistencies while supporting multi-pattern detection under relaxed semantics.", "motivation": "handling out-of-order, late, and duplicate events is critical for real-time analytics, especially on resource-constrained devices that process heterogeneous data from multiple sources", "method": "a hybrid CEP approach that combines lazy evaluation, buffering, and speculative processing", "result": "LimeCEP integrates Kafka for efficient message ordering, retention, and duplicate elimination, and offers configurable strategies to trade off between accuracy, latency, and resource consumption", "conclusion": "LimeCEP achieves up to six orders of magnitude lower latency, with up to 10 times lower memory usage and 6 times lower CPU utilization, while maintaining near-perfect precision and recall under high-disorder input streams, making it well-suited for non-cloud deployments."}}
{"id": "2507.01599", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01599", "abs": "https://arxiv.org/abs/2507.01599", "authors": ["Zhaoyan Sun", "Jiayi Wang", "Xinyang Zhao", "Jiachi Wang", "Guoliang Li"], "title": "Data Agent: A Holistic Architecture for Orchestrating Data+AI Ecosystems", "comment": null, "summary": "Traditional Data+AI systems utilize data-driven techniques to optimize\nperformance, but they rely heavily on human experts to orchestrate system\npipelines, enabling them to adapt to changes in data, queries, tasks, and\nenvironments. For instance, while there are numerous data science tools\navailable, developing a pipeline planning system to coordinate these tools\nremains challenging. This difficulty arises because existing Data+AI systems\nhave limited capabilities in semantic understanding, reasoning, and planning.\nFortunately, we have witnessed the success of large language models (LLMs) in\nenhancing semantic understanding, reasoning, and planning abilities. It is\ncrucial to incorporate LLM techniques to revolutionize data systems for\norchestrating Data+AI applications effectively.\n  To achieve this, we propose the concept of a 'Data Agent' - a comprehensive\narchitecture designed to orchestrate Data+AI ecosystems, which focuses on\ntackling data-related tasks by integrating knowledge comprehension, reasoning,\nand planning capabilities. We delve into the challenges involved in designing\ndata agents, such as understanding data/queries/environments/tools,\norchestrating pipelines/workflows, optimizing and executing pipelines, and\nfostering pipeline self-reflection. Furthermore, we present examples of data\nagent systems, including a data science agent, data analytics agents (such as\nunstructured data analytics agent, semantic structured data analytics agent,\ndata lake analytics agent, and multi-modal data analytics agent), and a\ndatabase administrator (DBA) agent. We also outline several open challenges\nassociated with designing data agent systems.", "AI": {"tldr": "This paper introduces the concept of a Data Agent, a novel architecture leveraging LLMs to orchestrate Data+AI ecosystems by integrating knowledge comprehension, reasoning, and planning capabilities, and showcases examples of its applications.", "motivation": "Existing Data+AI systems have limited capabilities in semantic understanding, reasoning, and planning.", "method": "incorporate LLM techniques to revolutionize data systems for orchestrating Data+AI applications effectively", "result": "We present examples of data agent systems, including a data science agent, data analytics agents, and a database administrator (DBA) agent.", "conclusion": "We propose the concept of a 'Data Agent' - a comprehensive architecture designed to orchestrate Data+AI ecosystems, which focuses on tackling data-related tasks by integrating knowledge comprehension, reasoning, and planning capabilities."}}
{"id": "2507.01231", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01231", "abs": "https://arxiv.org/abs/2507.01231", "authors": ["I\u00f1aki Dellibarda Varela", "Pablo Romero-Sorozabal", "Eduardo Rocon", "Manuel Cebrian"], "title": "Rethinking the Illusion of Thinking", "comment": "8 pages, 4 figures", "summary": "Earlier this year, Apple ignited controversy by publishing \"The Illusion of\nThinking,\" prompting heated debate within the AI community. Critics seized upon\nthe findings as conclusive evidence that Large Reasoning Models (LRMs) lack\ngenuine reasoning capabilities, branding them as mere stochastic parrots.\nMeanwhile, defenders-spearheaded by Lawsen et al. (2025)-fired back, condemning\nthe experimental setup as flawed and the conclusions overstated. We clarify\nthis debate by replicating and refining two of the original study's most\ncontentious benchmarks: Towers of Hanoi and River Crossing. By introducing\nincremental stepwise prompting and agentic collaborative dialogue, we show that\npreviously reported failures solving the Towers of Hanoi were not purely result\nof output constraints, but also partly a result of cognition limitations: LRMs\nstill stumble when complexity rises moderately (around 8 disks). Moreover, the\nRiver Crossing results initially heralded as catastrophic failures turn out to\nhinge upon testing unsolvable configurations. Once we limit tests strictly to\nsolvable problems-LRMs effortlessly solve large instances involving over 100\nagent pairs. Our findings ultimately defy simplistic narratives: today's LRMs\nare stochastic, RL-tuned searchers in a discrete state space we barely\nunderstand. Real progress in symbolic, long-horizon reasoning demands mapping\nthat terrain through fine-grained ablations like those introduced here.", "AI": {"tldr": "This paper clarifies the debate around Apple's 'The Illusion of Thinking' paper, showing that LRMs have limitations in Towers of Hanoi but can solve River Crossing with solvable configurations, suggesting they are stochastic searchers and require fine-grained ablations for real progress.", "motivation": "Apple ignited controversy by publishing 'The Illusion of Thinking,' prompting heated debate within the AI community. Critics seized upon the findings as conclusive evidence that Large Reasoning Models (LRMs) lack genuine reasoning capabilities, branding them as mere stochastic parrots. Meanwhile, defenders fired back, condemning the experimental setup as flawed and the conclusions overstated. We clarify this debate", "method": "replicating and refining two of the original study's most contentious benchmarks: Towers of Hanoi and River Crossing. By introducing incremental stepwise prompting and agentic collaborative dialogue", "result": "previously reported failures solving the Towers of Hanoi were not purely result of output constraints, but also partly a result of cognition limitations: LRMs still stumble when complexity rises moderately (around 8 disks). Moreover, the River Crossing results initially heralded as catastrophic failures turn out to hinge upon testing unsolvable configurations. Once we limit tests strictly to solvable problems-LRMs effortlessly solve large instances involving over 100 agent pairs.", "conclusion": "LRMs are stochastic, RL-tuned searchers in a discrete state space we barely understand. Real progress in symbolic, long-horizon reasoning demands mapping that terrain through fine-grained ablations like those introduced here."}}
{"id": "2507.01755", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.01755", "abs": "https://arxiv.org/abs/2507.01755", "authors": ["Roberto Garc\u00eda", "Renzo Angles", "Vicente Rojas", "Sebasti\u00e1n Ferrada"], "title": "PathDB: A system for evaluating regular path queries", "comment": null, "summary": "PathDB is a Java-based graph database designed for in-memory data loading and\nquerying. By utilizing Regular Path Queries (RPQ) and a closed path algebra,\nPathDB processes paths through its three main components: the parser, the\nlogical plan, and the physical plan. This modular design allows for targeted\noptimizations and modifications without impacting overall functionality.\nBenchmark experiments illustrate PathDB's execution times and flexibility in\nhandling dynamic and complex path queries, compared to baseline methods like\nDepth-First Search (DFS) and Breadth-First Search (BFS) guided by an automaton,\nhighlighting its optimizations that contribute to its performance.", "AI": {"tldr": "PathDB is a Java-based graph database optimized for in-memory path queries using RPQ and a modular design, outperforming DFS and BFS in benchmarks.", "motivation": "The paper introduces PathDB, a Java-based graph database designed for in-memory data loading and querying, which motivates the need for a system optimized for path-based queries.", "method": "PathDB utilizes Regular Path Queries (RPQ) and a closed path algebra, processing paths through a parser, logical plan, and physical plan.", "result": "Benchmark experiments illustrate PathDB's performance compared to DFS and BFS.", "conclusion": "PathDB demonstrates efficient execution times and flexibility in handling dynamic and complex path queries compared to baseline methods."}}
{"id": "2507.01282", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.01282", "abs": "https://arxiv.org/abs/2507.01282", "authors": ["Matthew JY Kang", "Wenli Yang", "Monica R Roberts", "Byeong Ho Kang", "Charles B Malpas"], "title": "Beyond Black-Box AI: Interpretable Hybrid Systems for Dementia Care", "comment": null, "summary": "The recent boom of large language models (LLMs) has re-ignited the hope that\nartificial intelligence (AI) systems could aid medical diagnosis. Yet despite\ndazzling benchmark scores, LLM assistants have yet to deliver measurable\nimprovements at the bedside. This scoping review aims to highlight the areas\nwhere AI is limited to make practical contributions in the clinical setting,\nspecifically in dementia diagnosis and care.\n  Standalone machine-learning models excel at pattern recognition but seldom\nprovide actionable, interpretable guidance, eroding clinician trust. Adjacent\nuse of LLMs by physicians did not result in better diagnostic accuracy or\nspeed. Key limitations trace to the data-driven paradigm: black-box outputs\nwhich lack transparency, vulnerability to hallucinations, and weak causal\nreasoning. Hybrid approaches that combine statistical learning with expert\nrule-based knowledge, and involve clinicians throughout the process help bring\nback interpretability. They also fit better with existing clinical workflows,\nas seen in examples like PEIRS and ATHENA-CDS.\n  Future decision-support should prioritise explanatory coherence by linking\npredictions to clinically meaningful causes. This can be done through\nneuro-symbolic or hybrid AI that combines the language ability of LLMs with\nhuman causal expertise. AI researchers have addressed this direction, with\nexplainable AI and neuro-symbolic AI being the next logical steps in further\nadvancement in AI. However, they are still based on data-driven knowledge\nintegration instead of human-in-the-loop approaches. Future research should\nmeasure success not only by accuracy but by improvements in clinician\nunderstanding, workflow fit, and patient outcomes. A better understanding of\nwhat helps improve human-computer interactions is greatly needed for AI systems\nto become part of clinical practice.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u8bca\u65ad\u4e2d\u7684\u5e94\u7528\u53d7\u5230\u6570\u636e\u9a71\u52a8\u8303\u4f8b\u7684\u9650\u5236\uff0c\u672a\u6765\u7684\u7814\u7a76\u5e94\u4fa7\u91cd\u4e8e\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u548c\u4e34\u5e8a\u533b\u751f\u7406\u89e3\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6700\u65b0\u7e41\u8363\u91cd\u65b0\u70b9\u71c3\u4e86\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u7cfb\u7edf\u53ef\u4ee5\u5e2e\u52a9\u533b\u7597\u8bca\u65ad\u7684\u5e0c\u671b\u3002\u7136\u800c\uff0c\u5c3d\u7ba1\u57fa\u51c6\u5206\u6570\u4ee4\u4eba\u773c\u82b1\u7f2d\u4e71\uff0c\u4f46LLM\u52a9\u624b\u5c1a\u672a\u5728\u5e8a\u8fb9\u63d0\u4f9b\u53ef\u8861\u91cf\u7684\u6539\u8fdb\u3002\u672c\u6b21\u8303\u56f4\u5ba1\u67e5\u65e8\u5728\u5f3a\u8c03\u4eba\u5de5\u667a\u80fd\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u505a\u51fa\u5b9e\u9645\u8d21\u732e\u53d7\u5230\u9650\u5236\u7684\u9886\u57df\uff0c\u7279\u522b\u662f\u5728\u75f4\u5446\u75c7\u8bca\u65ad\u548c\u62a4\u7406\u65b9\u9762\u3002", "method": "\u8303\u56f4\u5ba1\u67e5", "result": "\u533b\u751f\u5bf9LLM\u7684\u90bb\u8fd1\u4f7f\u7528\u5e76\u6ca1\u6709\u5bfc\u81f4\u66f4\u597d\u7684\u8bca\u65ad\u51c6\u786e\u6027\u6216\u901f\u5ea6\u3002\u5173\u952e\u9650\u5236\u5728\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u8303\u4f8b\uff1a\u7f3a\u4e4f\u900f\u660e\u5ea6\u7684\u9ed1\u76d2\u8f93\u51fa\u3001\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\u4ee5\u53ca\u8584\u5f31\u7684\u56e0\u679c\u63a8\u7406\u3002\u5c06\u7edf\u8ba1\u5b66\u4e60\u4e0e\u4e13\u5bb6\u57fa\u4e8e\u89c4\u5219\u7684\u77e5\u8bc6\u76f8\u7ed3\u5408\uff0c\u5e76\u5728\u6574\u4e2a\u8fc7\u7a0b\u4e2d\u8ba9\u4e34\u5e8a\u533b\u751f\u53c2\u4e0e\u7684\u6df7\u5408\u65b9\u6cd5\u6709\u52a9\u4e8e\u6062\u590d\u53ef\u89e3\u91ca\u6027\u3002\u6b63\u5982PEIRS\u548cATHENA-CDS\u7b49\u4f8b\u5b50\u6240\u89c1\uff0c\u5b83\u4eec\u4e5f\u66f4\u9002\u5408\u73b0\u6709\u7684\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u3002", "conclusion": "\u672a\u6765\u7684\u51b3\u7b56\u652f\u6301\u5e94\u4f18\u5148\u8003\u8651\u901a\u8fc7\u5c06\u9884\u6d4b\u4e0e\u4e34\u5e8a\u610f\u4e49\u7684\u539f\u56e0\u8054\u7cfb\u8d77\u6765\u7684\u89e3\u91ca\u8fde\u8d2f\u6027\u3002\u8fd9\u53ef\u4ee5\u901a\u8fc7\u795e\u7ecf\u7b26\u53f7\u6216\u6df7\u5408\u4eba\u5de5\u667a\u80fd\u6765\u5b9e\u73b0\uff0c\u5c06LLM\u7684\u8bed\u8a00\u80fd\u529b\u4e0e\u4eba\u7c7b\u56e0\u679c\u4e13\u4e1a\u77e5\u8bc6\u76f8\u7ed3\u5408\u3002\u672a\u6765\u7684\u7814\u7a76\u4e0d\u4ec5\u5e94\u901a\u8fc7\u51c6\u786e\u6027\u6765\u8861\u91cf\u6210\u529f\uff0c\u8fd8\u5e94\u901a\u8fc7\u4e34\u5e8a\u533b\u751f\u7406\u89e3\u3001\u5de5\u4f5c\u6d41\u7a0b\u9002\u5e94\u548c\u60a3\u8005\u7ed3\u679c\u7684\u6539\u8fdb\u6765\u8861\u91cf\u3002"}}
{"id": "2507.01042", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01042", "abs": "https://arxiv.org/abs/2507.01042", "authors": ["Harsh Joshi", "Gautam Siddharth Kashyap", "Rafiq Ali", "Ebad Shabbir", "Niharika Jain", "Sarthak Jain", "Jiechao Gao", "Usman Naseem"], "title": "Can Argus Judge Them All? Comparing VLMs Across Domains", "comment": null, "summary": "Vision-Language Models (VLMs) are advancing multimodal AI, yet their\nperformance consistency across tasks is underexamined. We benchmark CLIP, BLIP,\nand LXMERT across diverse datasets spanning retrieval, captioning, and\nreasoning. Our evaluation includes task accuracy, generation quality,\nefficiency, and a novel Cross-Dataset Consistency (CDC) metric. CLIP shows\nstrongest generalization (CDC: 0.92), BLIP excels on curated data, and LXMERT\nleads in structured reasoning. These results expose trade-offs between\ngeneralization and specialization, informing industrial deployment of VLMs and\nguiding development toward robust, task-flexible architectures.", "AI": {"tldr": "VLMs benchmarked for task consistency, revealing generalization vs. specialization trade-offs; CLIP generalizes best, BLIP excels on curated data, LXMERT leads in reasoning.", "motivation": "VLMs are advancing multimodal AI, yet their performance consistency across tasks is underexamined.", "method": "Benchmarked CLIP, BLIP, and LXMERT across diverse datasets spanning retrieval, captioning, and reasoning, including task accuracy, generation quality, efficiency, and a novel Cross-Dataset Consistency (CDC) metric.", "result": "CLIP shows strongest generalization (CDC: 0.92), BLIP excels on curated data, and LXMERT leads in structured reasoning.", "conclusion": "VLMs exhibit trade-offs between generalization and specialization."}}
{"id": "2507.01019", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.01019", "abs": "https://arxiv.org/abs/2507.01019", "authors": ["Imran Mirza", "Cole Huang", "Ishwara Vasista", "Rohan Patil", "Asli Akalin", "Sean O'Brien", "Kevin Zhu"], "title": "MALIBU Benchmark: Multi-Agent LLM Implicit Bias Uncovered", "comment": "Accepted to Building Trust in LLMs @ ICLR 2025 and NAACL SRW 2025", "summary": "Multi-agent systems, which consist of multiple AI models interacting within a\nshared environment, are increasingly used for persona-based interactions.\nHowever, if not carefully designed, these systems can reinforce implicit biases\nin large language models (LLMs), raising concerns about fairness and equitable\nrepresentation. We present MALIBU, a novel benchmark developed to assess the\ndegree to which LLM-based multi-agent systems implicitly reinforce social\nbiases and stereotypes. MALIBU evaluates bias in LLM-based multi-agent systems\nthrough scenario-based assessments. AI models complete tasks within predefined\ncontexts, and their responses undergo evaluation by an LLM-based multi-agent\njudging system in two phases. In the first phase, judges score responses\nlabeled with specific demographic personas (e.g., gender, race, religion)\nacross four metrics. In the second phase, judges compare paired responses\nassigned to different personas, scoring them and selecting the superior\nresponse. Our study quantifies biases in LLM-generated outputs, revealing that\nbias mitigation may favor marginalized personas over true neutrality,\nemphasizing the need for nuanced detection, balanced fairness strategies, and\ntransparent evaluation benchmarks in multi-agent systems.", "AI": {"tldr": "MALIBU\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u793e\u4f1a\u504f\u89c1\u7684\u57fa\u51c6\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u57fa\u4e8e\u89d2\u8272\u7684\u4ea4\u4e92\u3002\u7136\u800c\uff0c\u5982\u679c\u8bbe\u8ba1\u4e0d\u4ed4\u7ec6\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u4f1a\u5f3a\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4e2d\u7684\u9690\u542b\u504f\u89c1\uff0c\u4ece\u800c\u5f15\u53d1\u4eba\u4eec\u5bf9\u516c\u5e73\u548c\u516c\u6b63\u4ee3\u8868\u6027\u7684\u62c5\u5fe7\u3002", "method": "\u901a\u8fc7\u57fa\u4e8e\u573a\u666f\u7684\u8bc4\u4f30\u6765\u8bc4\u4f30\u57fa\u4e8e LLM \u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u504f\u5dee\u3002AI \u6a21\u578b\u5728\u9884\u5b9a\u4e49\u7684\u73af\u5883\u4e2d\u5b8c\u6210\u4efb\u52a1\uff0c\u4ed6\u4eec\u7684\u53cd\u5e94\u7ecf\u8fc7\u4e00\u4e2a\u57fa\u4e8e LLM \u7684\u591a\u667a\u80fd\u4f53\u5224\u65ad\u7cfb\u7edf\u5206\u4e24\u4e2a\u9636\u6bb5\u7684\u8bc4\u4f30\u3002", "result": "\u91cf\u5316\u4e86 LLM \u751f\u6210\u7684\u8f93\u51fa\u4e2d\u7684\u504f\u5dee\uff0c", "conclusion": "\u504f\u5dee\u7f13\u89e3\u53ef\u80fd\u66f4\u504f\u5411\u8fb9\u7f18\u5316\u89d2\u8272\uff0c\u800c\u4e0d\u662f\u771f\u6b63\u7684\u4e2d\u7acb\uff0c\u8fd9\u8868\u660e\u9700\u8981\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u8fdb\u884c\u7ec6\u81f4\u7684\u68c0\u6d4b\u3001\u5e73\u8861\u7684\u516c\u5e73\u7b56\u7565\u548c\u900f\u660e\u7684\u8bc4\u4f30\u57fa\u51c6\u3002"}}
{"id": "2507.01099", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.01099", "abs": "https://arxiv.org/abs/2507.01099", "authors": ["Zeyi Liu", "Shuang Li", "Eric Cousineau", "Siyuan Feng", "Benjamin Burchfiel", "Shuran Song"], "title": "Geometry-aware 4D Video Generation for Robot Manipulation", "comment": "Project website: https://robot4dgen.github.io", "summary": "Understanding and predicting the dynamics of the physical world can enhance a\nrobot's ability to plan and interact effectively in complex environments. While\nrecent video generation models have shown strong potential in modeling dynamic\nscenes, generating videos that are both temporally coherent and geometrically\nconsistent across camera views remains a significant challenge. To address\nthis, we propose a 4D video generation model that enforces multi-view 3D\nconsistency of videos by supervising the model with cross-view pointmap\nalignment during training. This geometric supervision enables the model to\nlearn a shared 3D representation of the scene, allowing it to predict future\nvideo sequences from novel viewpoints based solely on the given RGB-D\nobservations, without requiring camera poses as inputs. Compared to existing\nbaselines, our method produces more visually stable and spatially aligned\npredictions across multiple simulated and real-world robotic datasets. We\nfurther show that the predicted 4D videos can be used to recover robot\nend-effector trajectories using an off-the-shelf 6DoF pose tracker, supporting\nrobust robot manipulation and generalization to novel camera viewpoints.", "AI": {"tldr": "This paper proposes a 4D video generation model that enforces multi-view 3D consistency of videos by supervising the model with cross-view pointmap alignment during training. The model produces more visually stable and spatially aligned predictions and can be used to recover robot end-effector trajectories.", "motivation": "Generating videos that are both temporally coherent and geometrically consistent across camera views remains a significant challenge.", "method": "a 4D video generation model that enforces multi-view 3D consistency of videos by supervising the model with cross-view pointmap alignment during training", "result": "produces more visually stable and spatially aligned predictions across multiple simulated and real-world robotic datasets", "conclusion": "The predicted 4D videos can be used to recover robot end-effector trajectories using an off-the-shelf 6DoF pose tracker, supporting robust robot manipulation and generalization to novel camera viewpoints."}}
{"id": "2507.01026", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01026", "abs": "https://arxiv.org/abs/2507.01026", "authors": ["Md Shakil Ahamed Shohag", "Q. M. Jonathan Wu", "Farhad Pourpanah"], "title": "Few-Shot Inspired Generative Zero-Shot Learning", "comment": null, "summary": "Generative zero-shot learning (ZSL) methods typically synthesize visual\nfeatures for unseen classes using predefined semantic attributes, followed by\ntraining a fully supervised classification model. While effective, these\nmethods require substantial computational resources and extensive synthetic\ndata, thereby relaxing the original ZSL assumptions. In this paper, we propose\nFSIGenZ, a few-shot-inspired generative ZSL framework that reduces reliance on\nlarge-scale feature synthesis. Our key insight is that class-level attributes\nexhibit instance-level variability, i.e., some attributes may be absent or\npartially visible, yet conventional ZSL methods treat them as uniformly\npresent. To address this, we introduce Model-Specific Attribute Scoring (MSAS),\nwhich dynamically re-scores class attributes based on model-specific\noptimization to approximate instance-level variability without access to unseen\ndata. We further estimate group-level prototypes as clusters of instances based\non MSAS-adjusted attribute scores, which serve as representative synthetic\nfeatures for each unseen class. To mitigate the resulting data imbalance, we\nintroduce a Dual-Purpose Semantic Regularization (DPSR) strategy while training\na semantic-aware contrastive classifier (SCC) using these prototypes.\nExperiments on SUN, AwA2, and CUB benchmarks demonstrate that FSIGenZ achieves\ncompetitive performance using far fewer synthetic features.", "AI": {"tldr": "FSIGenZ: a few-shot-inspired generative ZSL framework that reduces reliance on large-scale feature synthesis.", "motivation": "conventional ZSL methods treat them as uniformly present. To address this, we introduce Model-Specific Attribute Scoring (MSAS), which dynamically re-scores class attributes based on model-specific optimization to approximate instance-level variability without access to unseen data.", "method": "introduce Model-Specific Attribute Scoring (MSAS), estimate group-level prototypes as clusters of instances based on MSAS-adjusted attribute scores, introduce a Dual-Purpose Semantic Regularization (DPSR) strategy while training a semantic-aware contrastive classifier (SCC) using these prototypes", "result": "Experiments on SUN, AwA2, and CUB benchmarks demonstrate that FSIGenZ achieves competitive performance using far fewer synthetic features.", "conclusion": "FSIGenZ achieves competitive performance using far fewer synthetic features."}}
{"id": "2507.01053", "categories": ["cs.IR", "cs.AI", "cs.DB", "68T50, 68P15", "H.2.3; I.2.7; J.3"], "pdf": "https://arxiv.org/pdf/2507.01053", "abs": "https://arxiv.org/abs/2507.01053", "authors": ["Rafi Al Attrach", "Pedro Moreira", "Rajna Fani", "Renato Umeton", "Leo Anthony Celi"], "title": "Conversational LLMs Simplify Secure Clinical Data Access, Understanding, and Analysis", "comment": "10 pages, 4 figures", "summary": "As ever-larger clinical datasets become available, they have the potential to\nunlock unprecedented opportunities for medical research. Foremost among them is\nMedical Information Mart for Intensive Care (MIMIC-IV), the world's largest\nopen-source EHR database. However, the inherent complexity of these datasets,\nparticularly the need for sophisticated querying skills and the need to\nunderstand the underlying clinical settings, often presents a significant\nbarrier to their effective use. M3 lowers the technical barrier to\nunderstanding and querying MIMIC-IV data. With a single command it retrieves\nMIMIC-IV from PhysioNet, launches a local SQLite instance (or hooks into the\nhosted BigQuery), and-via the Model Context Protocol (MCP)-lets researchers\nconverse with the database in plain English. Ask a clinical question in natural\nlanguage; M3 uses a language model to translate it into SQL, executes the query\nagainst the MIMIC-IV dataset, and returns structured results alongside the\nunderlying query for verifiability and reproducibility. Demonstrations show\nthat minutes of dialogue with M3 yield the kind of nuanced cohort analyses that\nonce demanded hours of handcrafted SQL and relied on understanding the\ncomplexities of clinical workflows. By simplifying access, M3 invites the\nbroader research community to mine clinical critical-care data and accelerates\nthe translation of raw records into actionable insight.", "AI": {"tldr": "M3 lowers the technical barrier to understanding and querying MIMIC-IV data by allowing researchers to converse with the database in plain English.", "motivation": "The complexity of large clinical datasets like MIMIC-IV, particularly the need for sophisticated querying skills and understanding of clinical settings, presents a significant barrier to their effective use.", "method": "M3 uses a language model to translate natural language questions into SQL, executes the query against the MIMIC-IV dataset, and returns structured results alongside the underlying query.", "result": "Demonstrations show that minutes of dialogue with M3 yield nuanced cohort analyses that once demanded hours of handcrafted SQL.", "conclusion": "M3 simplifies access to clinical critical-care data, inviting broader research community participation and accelerating the translation of raw records into actionable insight."}}
{"id": "2507.01376", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01376", "abs": "https://arxiv.org/abs/2507.01376", "authors": ["Yinwang Ren", "Yangyang Liu", "Tang Ji", "Xun Xu"], "title": "AI Agents and Agentic AI-Navigating a Plethora of Concepts for Future Manufacturing", "comment": "Submitted to JMS(March 2025)", "summary": "AI agents are autonomous systems designed to perceive, reason, and act within\ndynamic environments. With the rapid advancements in generative AI (GenAI),\nlarge language models (LLMs) and multimodal large language models (MLLMs) have\nsignificantly improved AI agents' capabilities in semantic comprehension,\ncomplex reasoning, and autonomous decision-making. At the same time, the rise\nof Agentic AI highlights adaptability and goal-directed autonomy in dynamic and\ncomplex environments. LLMs-based AI Agents (LLM-Agents), MLLMs-based AI Agents\n(MLLM-Agents), and Agentic AI contribute to expanding AI's capabilities in\ninformation processing, environmental perception, and autonomous\ndecision-making, opening new avenues for smart manufacturing. However, the\ndefinitions, capability boundaries, and practical applications of these\nemerging AI paradigms in smart manufacturing remain unclear. To address this\ngap, this study systematically reviews the evolution of AI and AI agent\ntechnologies, examines the core concepts and technological advancements of\nLLM-Agents, MLLM-Agents, and Agentic AI, and explores their potential\napplications in and integration into manufacturing, along with the potential\nchallenges they may face.", "AI": {"tldr": "This paper reviews LLM-Agents, MLLM-Agents, and Agentic AI, exploring their applications in smart manufacturing and potential challenges.", "motivation": "the definitions, capability boundaries, and practical applications of these emerging AI paradigms in smart manufacturing remain unclear", "method": "systematically reviews the evolution of AI and AI agent technologies, examines the core concepts and technological advancements of LLM-Agents, MLLM-Agents, and Agentic AI", "result": "LLMs-based AI Agents (LLM-Agents), MLLMs-based AI Agents (MLLM-Agents), and Agentic AI contribute to expanding AI's capabilities in information processing, environmental perception, and autonomous decision-making, opening new avenues for smart manufacturing", "conclusion": "This study systematically reviews the evolution of AI and AI agent technologies, examines the core concepts and technological advancements of LLM-Agents, MLLM-Agents, and Agentic AI, and explores their potential applications in and integration into manufacturing, along with the potential challenges they may face."}}
{"id": "2507.01049", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01049", "abs": "https://arxiv.org/abs/2507.01049", "authors": ["Pranav Jadhav"], "title": "Cohort Retrieval using Dense Passage Retrieval", "comment": null, "summary": "Patient cohort retrieval is a pivotal task in medical research and clinical\npractice, enabling the identification of specific patient groups from extensive\nelectronic health records (EHRs). In this work, we address the challenge of\ncohort retrieval in the echocardiography domain by applying Dense Passage\nRetrieval (DPR), a prominent methodology in semantic search. We propose a\nsystematic approach to transform an echocardiographic EHR dataset of\nunstructured nature into a Query-Passage dataset, framing the problem as a\nCohort Retrieval task. Additionally, we design and implement evaluation metrics\ninspired by real-world clinical scenarios to rigorously test the models across\ndiverse retrieval tasks. Furthermore, we present a custom-trained DPR embedding\nmodel that demonstrates superior performance compared to traditional and\noff-the-shelf SOTA methods.To our knowledge, this is the first work to apply\nDPR for patient cohort retrieval in the echocardiography domain, establishing a\nframework that can be adapted to other medical domains.", "AI": {"tldr": "Applies Dense Passage Retrieval (DPR) for patient cohort retrieval in the echocardiography domain and shows superior performance.", "motivation": "Patient cohort retrieval is a pivotal task in medical research and clinical practice, enabling the identification of specific patient groups from extensive electronic health records (EHRs).", "method": "Applying Dense Passage Retrieval (DPR) to transform an echocardiographic EHR dataset of unstructured nature into a Query-Passage dataset.", "result": "A custom-trained DPR embedding model demonstrates superior performance compared to traditional and off-the-shelf SOTA methods.", "conclusion": "This paper is the first to apply DPR for patient cohort retrieval in the echocardiography domain, establishing a framework that can be adapted to other medical domains."}}
{"id": "2507.01160", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01160", "abs": "https://arxiv.org/abs/2507.01160", "authors": ["Huiling You", "Samia Touileb", "Erik Velldal", "Lilja \u00d8vrelid"], "title": "Event-based evaluation of abstractive news summarization", "comment": "to appear at GEM2 workshop@ACL 2025", "summary": "An abstractive summary of a news article contains its most important\ninformation in a condensed version. The evaluation of automatically generated\nsummaries by generative language models relies heavily on human-authored\nsummaries as gold references, by calculating overlapping units or similarity\nscores. News articles report events, and ideally so should the summaries. In\nthis work, we propose to evaluate the quality of abstractive summaries by\ncalculating overlapping events between generated summaries, reference\nsummaries, and the original news articles. We experiment on a richly annotated\nNorwegian dataset comprising both events annotations and summaries authored by\nexpert human annotators. Our approach provides more insight into the event\ninformation contained in the summaries.", "AI": {"tldr": "evaluate the quality of abstractive summaries by calculating overlapping events between generated summaries, reference summaries, and the original news articles", "motivation": "The evaluation of automatically generated summaries relies heavily on human-authored summaries as gold references, by calculating overlapping units or similarity scores. News articles report events, and ideally so should the summaries.", "method": "evaluate the quality of abstractive summaries by calculating overlapping events between generated summaries, reference summaries, and the original news articles", "result": "experiment on a richly annotated Norwegian dataset comprising both events annotations and summaries authored by expert human annotators", "conclusion": "The approach provides more insight into the event information contained in the summaries."}}
{"id": "2507.01123", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.01123", "abs": "https://arxiv.org/abs/2507.01123", "authors": ["Rahul A. Burange", "Harsh K. Shinde", "Omkar Mutyalwar"], "title": "Landslide Detection and Mapping Using Deep Learning Across Multi-Source Satellite Data and Geographic Regions", "comment": "20 pages, 24 figures", "summary": "Landslides pose severe threats to infrastructure, economies, and human lives,\nnecessitating accurate detection and predictive mapping across diverse\ngeographic regions. With advancements in deep learning and remote sensing,\nautomated landslide detection has become increasingly effective. This study\npresents a comprehensive approach integrating multi-source satellite imagery\nand deep learning models to enhance landslide identification and prediction. We\nleverage Sentinel-2 multispectral data and ALOS PALSAR-derived slope and\nDigital Elevation Model (DEM) layers to capture critical environmental features\ninfluencing landslide occurrences. Various geospatial analysis techniques are\nemployed to assess the impact of terra in characteristics, vegetation cover,\nand rainfall on detection accuracy. Additionally, we evaluate the performance\nof multiple stateof-the-art deep learning segmentation models, including U-Net,\nDeepLabV3+, and Res-Net, to determine their effectiveness in landslide\ndetection. The proposed framework contributes to the development of reliable\nearly warning systems, improved disaster risk management, and sustainable\nland-use planning. Our findings provide valuable insights into the potential of\ndeep learning and multi-source remote sensing in creating robust, scalable, and\ntransferable landslide prediction models.", "AI": {"tldr": "This study uses deep learning and satellite data to improve landslide detection and prediction, which can help with early warnings and disaster management.", "motivation": "Accurate landslide detection and prediction are crucial due to the severe threats landslides pose to infrastructure, economies, and human lives. Automated detection has become more effective with advancements in deep learning and remote sensing.", "method": "The study integrates multi-source satellite imagery (Sentinel-2, ALOS PALSAR-derived slope and DEM) with deep learning models (U-Net, DeepLabV3+, Res-Net) and geospatial analysis techniques to enhance landslide identification and prediction.", "result": "The study evaluates the performance of multiple deep learning segmentation models and assesses the impact of terrain characteristics, vegetation cover, and rainfall on detection accuracy.", "conclusion": "This study's findings highlight the potential of deep learning and multi-source remote sensing in developing robust, scalable, and transferable landslide prediction models, contributing to early warning systems, disaster risk management, and sustainable land-use planning."}}
{"id": "2507.01027", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01027", "abs": "https://arxiv.org/abs/2507.01027", "authors": ["Zijian Ye", "Wei Huang", "Yifei Yu", "Tianhe Ren", "Zhongrui Wang", "Xiaojuan Qi"], "title": "DBellQuant: Breaking the Bell with Double-Bell Transformation for LLMs Post Training Binarization", "comment": "19 pages; Appendix added", "summary": "Large language models (LLMs) demonstrate remarkable performance but face\nsubstantial computational and memory challenges that limit their practical\ndeployment. Quantization has emerged as a promising solution; however, its\neffectiveness is often limited by quantization errors arising from weight\ndistributions that are not quantization-friendly and the presence of activation\noutliers. To address these challenges, we introduce DBellQuant, an innovative\npost-training quantization (PTQ) framework that achieves nearly 1-bit weight\ncompression and 6-bit activation quantization with minimal performance\ndegradation. DBellQuant uses Learnable Transformation for Dual-Bell (LTDB)\nalgorithm, which transforms single-bell weight distributions into dual-bell\nforms to reduce binarization errors and applies inverse transformations to\nsmooth activations. DBellQuant sets a new state-of-the-art by preserving\nsuperior model performance under aggressive weight and activation quantization.\nFor example, on the Wikitext2 dataset, DBellQuant achieves a perplexity of\n14.39 on LLaMA2-13B with 6-bit activation quantization, significantly\noutperforming BiLLM's 21.35 without activation quantization, underscoring its\npotential in compressing LLMs for real-world applications.", "AI": {"tldr": "DBellQuant \u662f\u4e00\u79cd\u521b\u65b0\u7684\u8bad\u7ec3\u540e\u91cf\u5316 (PTQ) \u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528 Learnable Transformation for Dual-Bell (LTDB) \u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u8fd1 1 \u4f4d\u6743\u91cd\u538b\u7f29\u548c 6 \u4f4d\u6fc0\u6d3b\u91cf\u5316\uff0c\u4e14\u6027\u80fd\u4e0b\u964d\u6700\u5c0f\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u4f46\u9762\u4e34\u7740\u5de8\u5927\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6311\u6218\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u7684\u5b9e\u9645\u90e8\u7f72\u3002\u91cf\u5316\u5df2\u6210\u4e3a\u4e00\u79cd\u6709\u524d\u9014\u7684\u89e3\u51b3\u65b9\u6848\uff1b\u7136\u800c\uff0c\u5176\u6709\u6548\u6027\u901a\u5e38\u53d7\u5230\u6743\u91cd\u5206\u5e03\uff08\u4e0d\u5229\u4e8e\u91cf\u5316\uff09\u548c\u6fc0\u6d3b\u5f02\u5e38\u503c\u5f15\u8d77\u7684\u91cf\u5316\u8bef\u5dee\u7684\u9650\u5236\u3002", "method": "DBellQuant \u4f7f\u7528\u53ef\u5b66\u4e60\u53d8\u6362\u8fdb\u884c\u53cc\u5cf0 (LTDB) \u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5c06\u5355\u5cf0\u6743\u91cd\u5206\u5e03\u8f6c\u6362\u4e3a\u53cc\u5cf0\u5f62\u5f0f\u4ee5\u51cf\u5c11\u4e8c\u503c\u5316\u8bef\u5dee\uff0c\u5e76\u5c06\u9006\u53d8\u6362\u5e94\u7528\u4e8e\u5e73\u6ed1\u6fc0\u6d3b\u3002", "result": "DBellQuant \u5b9e\u73b0\u4e86\u8fd1 1 \u4f4d\u6743\u91cd\u538b\u7f29\u548c 6 \u4f4d\u6fc0\u6d3b\u91cf\u5316\uff0c\u4e14\u6027\u80fd\u4e0b\u964d\u6700\u5c0f\u3002\u4f8b\u5982\uff0c\u5728 Wikitext2 \u6570\u636e\u96c6\u4e0a\uff0cDBellQuant \u5728 LLaMA2-13B \u4e0a\u5b9e\u73b0\u4e86 14.39 \u7684\u56f0\u60d1\u5ea6\uff0c\u5177\u6709 6 \u4f4d\u6fc0\u6d3b\u91cf\u5316\uff0c\u660e\u663e\u4f18\u4e8e BiLLM \u7684 21.35\uff0c\u800c\u6ca1\u6709\u6fc0\u6d3b\u91cf\u5316\uff0c\u7a81\u663e\u4e86\u5176\u5728\u538b\u7f29 LLM \u4ee5\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "DBellQuant \u901a\u8fc7\u5728\u79ef\u6781\u7684\u6743\u91cd\u548c\u6fc0\u6d3b\u91cf\u5316\u4e0b\u4fdd\u6301\u5353\u8d8a\u7684\u6a21\u578b\u6027\u80fd\uff0c\u521b\u9020\u4e86\u65b0\u7684\u6280\u672f\u6c34\u5e73\u3002"}}
{"id": "2507.01616", "categories": ["cs.IR", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2507.01616", "abs": "https://arxiv.org/abs/2507.01616", "authors": ["Chengkun He", "Xiangmin Zhou", "Chen Wang", "Longbing Cao", "Jie Shao", "Xiaodong Li", "Guang Xu", "Carrie Jinqiu Hu", "Zahir Tari"], "title": "Enhanced Influence-aware Group Recommendation for Online Media Propagation", "comment": null, "summary": "Group recommendation over social media streams has attracted significant\nattention due to its wide applications in domains such as e-commerce,\nentertainment, and online news broadcasting. By leveraging social connections\nand group behaviours, group recommendation (GR) aims to provide more accurate\nand engaging content to a set of users rather than individuals. Recently,\ninfluence-aware GR has emerged as a promising direction, as it considers the\nimpact of social influence on group decision-making. In earlier work, we\nproposed Influence-aware Group Recommendation (IGR) to solve this task.\nHowever, this task remains challenging due to three key factors: the large and\never-growing scale of social graphs, the inherently dynamic nature of influence\npropagation within user groups, and the high computational overhead of\nreal-time group-item matching.\n  To tackle these issues, we propose an Enhanced Influence-aware Group\nRecommendation (EIGR) framework. First, we introduce a Graph Extraction-based\nSampling (GES) strategy to minimise redundancy across multiple temporal social\ngraphs and effectively capture the evolving dynamics of both groups and items.\nSecond, we design a novel DYnamic Independent Cascade (DYIC) model to predict\nhow influence propagates over time across social items and user groups.\nFinally, we develop a two-level hash-based User Group Index (UG-Index) to\nefficiently organise user groups and enable real-time recommendation\ngeneration. Extensive experiments on real-world datasets demonstrate that our\nproposed framework, EIGR, consistently outperforms state-of-the-art baselines\nin both effectiveness and efficiency.", "AI": {"tldr": "\u63d0\u51faEIGR\u6846\u67b6\uff0c\u901a\u8fc7GES\u7b56\u7565\u3001DYIC\u6a21\u578b\u548cUG-Index\u6765\u89e3\u51b3\u5f71\u54cd\u611f\u77e5\u7fa4\u4f53\u63a8\u8350\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u5728\u5b9e\u9645\u6570\u636e\u96c6\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u6d41\u4e0a\u7684\u7fa4\u4f53\u63a8\u8350\u56e0\u5176\u5728\u7535\u5b50\u5546\u52a1\u3001\u5a31\u4e50\u548c\u5728\u7ebf\u65b0\u95fb\u5e7f\u64ad\u7b49\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\u800c\u5907\u53d7\u5173\u6ce8\u3002\u7531\u4e8e\u793e\u4ea4\u56fe\u7684\u5927\u89c4\u6a21\u548c\u4e0d\u65ad\u589e\u957f\u3001\u7528\u6237\u7ec4\u5185\u5f71\u54cd\u4f20\u64ad\u7684\u52a8\u6001\u6027\u4ee5\u53ca\u5b9e\u65f6\u7fa4\u4f53\u9879\u76ee\u5339\u914d\u7684\u9ad8\u8ba1\u7b97\u5f00\u9500\uff0c\u5f71\u54cd\u611f\u77e5\u7fa4\u4f53\u63a8\u8350\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "GES\u7b56\u7565\u6700\u5c0f\u5316\u591a\u4e2a\u65f6\u95f4\u793e\u4ea4\u56fe\u4e2d\u7684\u5197\u4f59\uff0cDYIC\u6a21\u578b\u9884\u6d4b\u793e\u4ea4\u9879\u76ee\u548c\u7528\u6237\u7ec4\u7684\u5f71\u54cd\u4f20\u64ad\uff0cUG-Index\u9ad8\u6548\u7ec4\u7ec7\u7528\u6237\u7ec4\u5e76\u5b9e\u73b0\u5b9e\u65f6\u63a8\u8350\u751f\u6210\u3002", "result": "EIGR\u6846\u67b6\u5728\u5b9e\u9645\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u3002", "conclusion": "EIGR\u6846\u67b6\u5728\u6709\u6548\u6027\u548c\u6548\u7387\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u3002"}}
{"id": "2507.01410", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01410", "abs": "https://arxiv.org/abs/2507.01410", "authors": ["Abeer Dyoub", "Francesca A. Lisi"], "title": "A Fuzzy Approach to the Specification, Verification and Validation of Risk-Based Ethical Decision Making Models", "comment": null, "summary": "The ontological and epistemic complexities inherent in the moral domain make\nit challenging to establish clear standards for evaluating the performance of a\nmoral machine. In this paper, we present a formal method to describe Ethical\nDecision Making models based on ethical risk assessment. Then, we show how\nthese models that are specified as fuzzy rules can be verified and validated\nusing fuzzy Petri nets. A case study from the medical field is considered to\nillustrate the proposed approach.", "AI": {"tldr": "Formal method to describe Ethical Decision Making models based on ethical risk assessment, models can be verified and validated using fuzzy Petri nets, a case study from the medical field is considered.", "motivation": "The ontological and epistemic complexities inherent in the moral domain make it challenging to establish clear standards for evaluating the performance of a moral machine.", "method": "We present a formal method to describe Ethical Decision Making models based on ethical risk assessment. Then, we show how these models that are specified as fuzzy rules can be verified and validated using fuzzy Petri nets.", "result": "These models that are specified as fuzzy rules can be verified and validated using fuzzy Petri nets.", "conclusion": "A case study from the medical field is considered to illustrate the proposed approach."}}
{"id": "2507.01170", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01170", "abs": "https://arxiv.org/abs/2507.01170", "authors": ["Simon B\u00f6rjesson", "Erik Ersmark", "Pierre Nugues"], "title": "Matching and Linking Entries in Historical Swedish Encyclopedias", "comment": "10 pages, 3 figures", "summary": "The \\textit{Nordisk familjebok} is a Swedish encyclopedia from the 19th and\n20th centuries. It was written by a team of experts and aimed to be an\nintellectual reference, stressing precision and accuracy. This encyclopedia had\nfour main editions remarkable by their size, ranging from 20 to 38 volumes. As\na consequence, the \\textit{Nordisk familjebok} had a considerable influence in\nuniversities, schools, the media, and society overall. As new editions were\nreleased, the selection of entries and their content evolved, reflecting\nintellectual changes in Sweden.\n  In this paper, we used digitized versions from \\textit{Project Runeberg}. We\nfirst resegmented the raw text into entries and matched pairs of entries\nbetween the first and second editions using semantic sentence embeddings. We\nthen extracted the geographical entries from both editions using a\ntransformer-based classifier and linked them to Wikidata. This enabled us to\nidentify geographic trends and possible shifts between the first and second\neditions, written between 1876-1899 and 1904-1926, respectively.\n  Interpreting the results, we observe a small but significant shift in\ngeographic focus away from Europe and towards North America, Africa, Asia,\nAustralia, and northern Scandinavia from the first to the second edition,\nconfirming the influence of the First World War and the rise of new powers. The\ncode and data are available on GitHub at\nhttps://github.com/sibbo/nordisk-familjebok.", "AI": {"tldr": "\u672c\u6587\u4f7f\u7528\u6570\u5b57\u5316\u7248\u672c\u7684 Nordisk familjebok\uff0c\u901a\u8fc7\u8bed\u4e49\u5d4c\u5165\u5339\u914d\u6761\u76ee\uff0c\u63d0\u53d6\u5730\u7406\u6761\u76ee\u5e76\u94fe\u63a5\u5230 Wikidata\uff0c\u4ece\u800c\u8bc6\u522b\u5730\u7406\u8d8b\u52bf\u7684\u8f6c\u53d8\uff0c\u53d1\u73b0\u7126\u70b9\u4ece\u6b27\u6d32\u8f6c\u79fb\u5230\u5176\u4ed6\u5730\u533a\uff0c\u53cd\u6620\u4e86\u4e00\u6218\u7684\u5f71\u54cd\u3002", "motivation": "Nordisk familjebok \u662f\u4e00\u90e8 19 \u4e16\u7eaa\u548c 20 \u4e16\u7eaa\u7684\u745e\u5178\u767e\u79d1\u5168\u4e66\u3002\u5b83\u7531\u4e00\u4e2a\u4e13\u5bb6\u56e2\u961f\u7f16\u5199\uff0c\u65e8\u5728\u6210\u4e3a\u4e00\u4e2a\u77e5\u8bc6\u53c2\u8003\uff0c\u5f3a\u8c03\u7cbe\u786e\u6027\u548c\u51c6\u786e\u6027\u3002\u968f\u7740\u65b0\u7248\u672c\u7684\u53d1\u5e03\uff0c\u6761\u76ee\u7684\u9009\u62e9\u548c\u5185\u5bb9\u4e0d\u65ad\u53d1\u5c55\uff0c\u53cd\u6620\u4e86\u745e\u5178\u7684\u77e5\u8bc6\u53d8\u5316\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e Transformer \u7684\u5206\u7c7b\u5668\u4ece\u4e24\u4e2a\u7248\u672c\u4e2d\u63d0\u53d6\u5730\u7406\u6761\u76ee\uff0c\u5e76\u5c06\u5b83\u4eec\u94fe\u63a5\u5230 Wikidata\u3002\u4f7f\u7528\u8bed\u4e49\u53e5\u5b50\u5d4c\u5165\u5c06\u539f\u59cb\u6587\u672c\u91cd\u65b0\u5206\u5272\u6210\u6761\u76ee\uff0c\u5e76\u5728\u7b2c\u4e00\u7248\u548c\u7b2c\u4e8c\u7248\u4e4b\u95f4\u5339\u914d\u6761\u76ee\u5bf9\u3002", "result": "\u80fd\u591f\u8bc6\u522b 1876-1899 \u5e74\u548c 1904-1926 \u5e74\u95f4\u7f16\u5199\u7684\u7b2c\u4e00\u7248\u548c\u7b2c\u4e8c\u7248\u4e4b\u95f4\u7684\u5730\u7406\u8d8b\u52bf\u548c\u53ef\u80fd\u7684\u8f6c\u53d8\u3002", "conclusion": "\u89c2\u5bdf\u5230\u4ece\u7b2c\u4e00\u7248\u5230\u7b2c\u4e8c\u7248\uff0c\u5730\u7406\u7126\u70b9\u4ece\u6b27\u6d32\u7565\u5fae\u4f46\u663e\u8457\u5730\u8f6c\u79fb\u5230\u5317\u7f8e\u3001\u975e\u6d32\u3001\u4e9a\u6d32\u3001\u6fb3\u5927\u5229\u4e9a\u548c\u65af\u582a\u7684\u7eb3\u7ef4\u4e9a\u5317\u90e8\uff0c\u8bc1\u5b9e\u4e86\u7b2c\u4e00\u6b21\u4e16\u754c\u5927\u6218\u7684\u5f71\u54cd\u548c\u65b0\u52bf\u529b\u7684\u5d1b\u8d77\u3002"}}
{"id": "2507.01163", "categories": ["cs.CV", "q-bio.CB", "q-bio.QM", "I.4.7"], "pdf": "https://arxiv.org/pdf/2507.01163", "abs": "https://arxiv.org/abs/2507.01163", "authors": ["Al\u00e1n F. Mu\u00f1oz", "Tim Treis", "Alexandr A. Kalinin", "Shatavisha Dasgupta", "Fabian Theis", "Anne E. Carpenter", "Shantanu Singh"], "title": "cp_measure: API-first feature extraction for image-based profiling workflows", "comment": "10 pages, 4 figures, 4 supplementary figures. CODEML Workshop paper\n  accepted (non-archival), as a part of ICML2025 events", "summary": "Biological image analysis has traditionally focused on measuring specific\nvisual properties of interest for cells or other entities. A complementary\nparadigm gaining increasing traction is image-based profiling - quantifying\nmany distinct visual features to form comprehensive profiles which may reveal\nhidden patterns in cellular states, drug responses, and disease mechanisms.\nWhile current tools like CellProfiler can generate these feature sets, they\npose significant barriers to automated and reproducible analyses, hindering\nmachine learning workflows. Here we introduce cp_measure, a Python library that\nextracts CellProfiler's core measurement capabilities into a modular, API-first\ntool designed for programmatic feature extraction. We demonstrate that\ncp_measure features retain high fidelity with CellProfiler features while\nenabling seamless integration with the scientific Python ecosystem. Through\napplications to 3D astrocyte imaging and spatial transcriptomics, we showcase\nhow cp_measure enables reproducible, automated image-based profiling pipelines\nthat scale effectively for machine learning applications in computational\nbiology.", "AI": {"tldr": "cp_measure is a Python library that extracts CellProfiler's core measurement capabilities into a modular, API-first tool designed for programmatic feature extraction, enabling reproducible, automated image-based profiling pipelines that scale effectively for machine learning applications in computational biology.", "motivation": "current tools like CellProfiler can generate these feature sets, they pose significant barriers to automated and reproducible analyses, hindering machine learning workflows", "method": "a Python library that extracts CellProfiler's core measurement capabilities into a modular, API-first tool designed for programmatic feature extraction", "result": "cp_measure features retain high fidelity with CellProfiler features while enabling seamless integration with the scientific Python ecosystem", "conclusion": "cp_measure enables reproducible, automated image-based profiling pipelines that scale effectively for machine learning applications in computational biology."}}
{"id": "2507.01028", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01028", "abs": "https://arxiv.org/abs/2507.01028", "authors": ["Jean Ponce", "Martial Hebert", "Basile Terver"], "title": "Dual Perspectives on Non-Contrastive Self-Supervised Learning", "comment": null, "summary": "The objective of non-contrastive approaches to self-supervised learning is to\ntrain on pairs of different views of the data an encoder and a predictor that\nminimize the mean discrepancy between the code predicted from the embedding of\nthe first view and the embedding of the second one. In this setting, the stop\ngradient and exponential moving average iterative procedures are commonly used\nto avoid representation collapse, with excellent performance in downstream\nsupervised applications. This presentation investigates these procedures from\nthe dual theoretical viewpoints of optimization and dynamical systems. We first\nshow that, in general, although they do not optimize the original objective, or\nfor that matter, any other smooth function, they do avoid collapse. Following\nTian et al. [2021], but without any of the extra assumptions used in their\nproofs, we then show using a dynamical system perspective that, in the linear\ncase, minimizing the original objective function without the use of a stop\ngradient or exponential moving average always leads to collapse. Conversely, we\nfinally show that the limit points of the dynamical systems associated with\nthese two procedures are, in general, asymptotically stable equilibria, with no\nrisk of degenerating to trivial solutions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u975e\u5bf9\u6bd4\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u505c\u6b62\u68af\u5ea6\u548c\u6307\u6570\u79fb\u52a8\u5e73\u5747\u7a0b\u5e8f\uff0c\u8868\u660e\u5b83\u4eec\u53ef\u4ee5\u907f\u514d\u8868\u793a\u5d29\u6e83\uff0c\u5e76\u4e14\u76f8\u5173\u7684\u52a8\u6001\u7cfb\u7edf\u662f\u7a33\u5b9a\u7684\u3002", "motivation": "\u975e\u5bf9\u6bd4\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u76ee\u6807\u662f\u8bad\u7ec3\u4e00\u4e2a\u7f16\u7801\u5668\u548c\u4e00\u4e2a\u9884\u6d4b\u5668\uff0c\u8be5\u9884\u6d4b\u5668\u53ef\u4ee5\u6700\u5c0f\u5316\u4ece\u7b2c\u4e00\u4e2a\u89c6\u56fe\u7684\u5d4c\u5165\u9884\u6d4b\u7684\u4ee3\u7801\u4e0e\u7b2c\u4e8c\u4e2a\u89c6\u56fe\u7684\u5d4c\u5165\u4e4b\u95f4\u7684\u5e73\u5747\u5dee\u5f02\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u505c\u6b62\u68af\u5ea6\u548c\u6307\u6570\u79fb\u52a8\u5e73\u5747\u8fed\u4ee3\u7a0b\u5e8f\u901a\u5e38\u7528\u4e8e\u907f\u514d\u8868\u793a\u5d29\u6e83\uff0c\u5e76\u5728\u4e0b\u6e38\u76d1\u7763\u5e94\u7528\u4e2d\u5177\u6709\u51fa\u8272\u7684\u6027\u80fd\u3002", "method": "\u8be5\u8bba\u6587\u4ece\u4f18\u5316\u548c\u52a8\u529b\u7cfb\u7edf\u7684\u53cc\u91cd\u7406\u8bba\u89d2\u5ea6\u7814\u7a76\u4e86\u8fd9\u4e9b\u7a0b\u5e8f\u3002", "result": "\u8be5\u8bba\u6587\u8868\u660e\uff0c\u901a\u5e38\u60c5\u51b5\u4e0b\uff0c\u867d\u7136\u5b83\u4eec\u6ca1\u6709\u4f18\u5316\u539f\u59cb\u76ee\u6807\uff0c\u6216\u8005\u5c31\u6b64\u800c\u8a00\uff0c\u6ca1\u6709\u4f18\u5316\u4efb\u4f55\u5176\u4ed6\u5e73\u6ed1\u51fd\u6570\uff0c\u4f46\u5b83\u4eec\u786e\u5b9e\u907f\u514d\u4e86\u5d29\u6e83\u3002\u7136\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u52a8\u6001\u7cfb\u7edf\u89c6\u89d2\u8868\u660e\uff0c\u5728\u6ca1\u6709\u505c\u6b62\u68af\u5ea6\u6216\u6307\u6570\u79fb\u52a8\u5e73\u5747\u7684\u60c5\u51b5\u4e0b\uff0c\u6700\u5c0f\u5316\u539f\u59cb\u76ee\u6807\u51fd\u6570\u603b\u662f\u4f1a\u5bfc\u81f4\u5d29\u6e83\u3002\u76f8\u53cd\uff0c\u6211\u4eec\u6700\u7ec8\u8868\u660e\uff0c\u4e0e\u8fd9\u4e24\u4e2a\u7a0b\u5e8f\u76f8\u5173\u7684\u52a8\u6001\u7cfb\u7edf\u7684\u6781\u9650\u70b9\u901a\u5e38\u662f\u6e10\u8fd1\u7a33\u5b9a\u7684\u5e73\u8861\uff0c\u6ca1\u6709\u9000\u5316\u4e3a\u5e73\u51e1\u89e3\u7684\u98ce\u9669\u3002", "conclusion": "\u8be5\u8bba\u6587\u8868\u660e\uff0c\u505c\u6b62\u68af\u5ea6\u548c\u6307\u6570\u79fb\u52a8\u5e73\u5747\u8fed\u4ee3\u7a0b\u5e8f\u53ef\u4ee5\u907f\u514d\u8868\u793a\u5d29\u6e83\uff0c\u5e76\u4e14\u4e0e\u8fd9\u4e9b\u7a0b\u5e8f\u76f8\u5173\u7684\u52a8\u6001\u7cfb\u7edf\u7684\u6781\u9650\u70b9\u901a\u5e38\u662f\u6e10\u8fd1\u7a33\u5b9a\u7684\u5e73\u8861\uff0c\u6ca1\u6709\u9000\u5316\u4e3a\u5e73\u51e1\u89e3\u7684\u98ce\u9669\u3002"}}
{"id": "2507.01431", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01431", "abs": "https://arxiv.org/abs/2507.01431", "authors": ["Yoonseok Yang", "Minjune Kim", "Marlon Rondinelli", "Keren Shao"], "title": "Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading", "comment": "7 pages, 5 figues, 1 table", "summary": "Grading handwritten, open-ended responses remains a major bottleneck in large\nuniversity STEM courses. We introduce Pensieve (https://www.pensieve.co), an\nAI-assisted grading platform that leverages large language models (LLMs) to\ntranscribe and evaluate student work, providing instructors with rubric-aligned\nscores, transcriptions, and confidence ratings. Unlike prior tools that focus\nnarrowly on specific tasks like transcription or rubric generation, Pensieve\nsupports the entire grading pipeline-from scanned student submissions to final\nfeedback-within a human-in-the-loop interface.\n  Pensieve has been deployed in real-world courses at over 20 institutions and\nhas graded more than 300,000 student responses. We present system details and\nempirical results across four core STEM disciplines: Computer Science,\nMathematics, Physics, and Chemistry. Our findings show that Pensieve reduces\ngrading time by an average of 65%, while maintaining a 95.4% agreement rate\nwith instructor-assigned grades for high-confidence predictions.", "AI": {"tldr": "Pensieve is an AI-assisted grading platform that reduces grading time by 65% with high agreement with instructors.", "motivation": "Grading handwritten, open-ended responses remains a major bottleneck in large university STEM courses.", "method": "an AI-assisted grading platform that leverages large language models (LLMs) to transcribe and evaluate student work", "result": "Pensieve has been deployed in real-world courses at over 20 institutions and has graded more than 300,000 student responses. Empirical results across four core STEM disciplines: Computer Science, Mathematics, Physics, and Chemistry.", "conclusion": "Pensieve reduces grading time by an average of 65%, while maintaining a 95.4% agreement rate with instructor-assigned grades for high-confidence predictions."}}
{"id": "2507.01058", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01058", "abs": "https://arxiv.org/abs/2507.01058", "authors": ["Puspendu Banerjee", "Aritra Mazumdar", "Wazib Ansar", "Saptarsi Goswami", "Amlan Chakrabarti"], "title": "A Data Science Approach to Calcutta High Court Judgments: An Efficient LLM and RAG-powered Framework for Summarization and Similar Cases Retrieval", "comment": "12 pages, 6 figures", "summary": "The judiciary, as one of democracy's three pillars, is dealing with a rising\namount of legal issues, needing careful use of judicial resources. This\nresearch presents a complex framework that leverages Data Science\nmethodologies, notably Large Language Models (LLM) and Retrieval-Augmented\nGeneration (RAG) techniques, to improve the efficiency of analyzing Calcutta\nHigh Court verdicts. Our framework focuses on two key aspects: first, the\ncreation of a robust summarization mechanism that distills complex legal texts\ninto concise and coherent summaries; and second, the development of an\nintelligent system for retrieving similar cases, which will assist legal\nprofessionals in research and decision making. By fine-tuning the Pegasus model\nusing case head note summaries, we achieve significant improvements in the\nsummarization of legal cases. Our two-step summarizing technique preserves\ncrucial legal contexts, allowing for the production of a comprehensive vector\ndatabase for RAG. The RAG-powered framework efficiently retrieves similar cases\nin response to user queries, offering thorough overviews and summaries. This\ntechnique not only improves legal research efficiency, but it also helps legal\nprofessionals and students easily acquire and grasp key legal information,\nbenefiting the overall legal scenario.", "AI": {"tldr": "\u5229\u7528 LLM \u548c RAG \u6280\u672f\u6539\u8fdb\u52a0\u5c14\u5404\u7b54\u9ad8\u7b49\u6cd5\u9662\u5224\u51b3\u5206\u6790\u7684\u6548\u7387\u3002", "motivation": "\u4f5c\u4e3a\u6c11\u4e3b\u4e09\u5927\u652f\u67f1\u4e4b\u4e00\u7684\u53f8\u6cd5\u673a\u6784\uff0c\u6b63\u5728\u5904\u7406\u8d8a\u6765\u8d8a\u591a\u7684\u6cd5\u5f8b\u95ee\u9898\uff0c\u9700\u8981\u8c28\u614e\u5229\u7528\u53f8\u6cd5\u8d44\u6e90\u3002", "method": "\u5229\u7528\u6570\u636e\u79d1\u5b66\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210 (RAG) \u6280\u672f\u3002", "result": "\u901a\u8fc7\u4f7f\u7528\u6848\u4f8b\u6807\u9898\u6458\u8981\u5fae\u8c03 Pegasus \u6a21\u578b\uff0c\u6211\u4eec\u5728\u6cd5\u5f8b\u6848\u4f8b\u7684\u6458\u8981\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u7740\u6539\u8fdb\u3002\u6211\u4eec\u7684\u4e24\u6b65\u603b\u7ed3\u6280\u672f\u4fdd\u7559\u4e86\u5173\u952e\u7684\u6cd5\u5f8b\u80cc\u666f\uff0c\u4ece\u800c\u53ef\u4ee5\u751f\u6210\u7528\u4e8e RAG \u7684\u7efc\u5408\u5411\u91cf\u6570\u636e\u5e93\u3002RAG \u9a71\u52a8\u7684\u6846\u67b6\u53ef\u4ee5\u6709\u6548\u5730\u68c0\u7d22\u4e0e\u7528\u6237\u67e5\u8be2\u7c7b\u4f3c\u7684\u6848\u4f8b\uff0c\u4ece\u800c\u63d0\u4f9b\u5168\u9762\u7684\u6982\u8ff0\u548c\u603b\u7ed3\u3002", "conclusion": "\u8be5\u6280\u672f\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u6cd5\u5f8b\u7814\u7a76\u6548\u7387\uff0c\u800c\u4e14\u6709\u52a9\u4e8e\u6cd5\u5f8b\u4e13\u4e1a\u4eba\u58eb\u548c\u5b66\u751f\u8f7b\u677e\u83b7\u53d6\u548c\u638c\u63e1\u5173\u952e\u6cd5\u5f8b\u4fe1\u606f\uff0c\u4ece\u800c\u6709\u76ca\u4e8e\u6574\u4e2a\u6cd5\u5f8b\u9886\u57df\u3002"}}
{"id": "2507.01213", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01213", "abs": "https://arxiv.org/abs/2507.01213", "authors": ["Adamu Lawan", "Juhua Pu", "Haruna Yunusa", "Jawad Muhammad", "Muhammad Lawan"], "title": "MEGA: xLSTM with Multihead Exponential Gated Fusion for Precise Aspect-based Sentiment Analysis", "comment": "6, 1 figure", "summary": "Aspect-based Sentiment Analysis (ABSA) is a critical Natural Language\nProcessing (NLP) task that extracts aspects from text and determines their\nassociated sentiments, enabling fine-grained analysis of user opinions.\nExisting ABSA methods struggle to balance computational efficiency with high\nperformance: deep learning models often lack global context, transformers\ndemand significant computational resources, and Mamba-based approaches face\nCUDA dependency and diminished local correlations. Recent advancements in\nExtended Long Short-Term Memory (xLSTM) models, particularly their efficient\nmodeling of long-range dependencies, have significantly advanced the NLP\ncommunity. However, their potential in ABSA remains untapped. To this end, we\npropose xLSTM with Multihead Exponential Gated Fusion (MEGA), a novel framework\nintegrating a bi-directional mLSTM architecture with forward and partially\nflipped backward (PF-mLSTM) streams. The PF-mLSTM enhances localized context\nmodeling by processing the initial sequence segment in reverse with dedicated\nparameters, preserving critical short-range patterns. We further introduce an\nmLSTM-based multihead cross exponential gated fusion mechanism (MECGAF) that\ndynamically combines forward mLSTM outputs as query and key with PF-mLSTM\noutputs as value, optimizing short-range dependency capture while maintaining\nglobal context and efficiency. Experimental results on three benchmark datasets\ndemonstrate that MEGA outperforms state-of-the-art baselines, achieving\nsuperior accuracy and efficiency in ABSA tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684ABSA\u6846\u67b6\uff08MEGA\uff09\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86xLSTM\u548c\u591a\u5934\u6307\u6570\u95e8\u63a7\u878d\u5408\uff0c\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684ABSA\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u4e0e\u9ad8\u6027\u80fd\uff1a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u901a\u5e38\u7f3a\u4e4f\u5168\u5c40\u4e0a\u4e0b\u6587\uff0ctransformers\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\uff0c\u800c\u57fa\u4e8eMamba\u7684\u65b9\u6cd5\u9762\u4e34CUDA\u4f9d\u8d56\u6027\u5e76\u964d\u4f4e\u4e86\u5c40\u90e8\u76f8\u5173\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u5177\u6709\u524d\u5411\u548c\u90e8\u5206\u7ffb\u8f6c\u540e\u5411\uff08PF-mLSTM\uff09\u6d41\u7684\u53cc\u5411mLSTM\u67b6\u6784\u3002\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8emLSTM\u7684\u591a\u5934\u4ea4\u53c9\u6307\u6570\u95e8\u63a7\u878d\u5408\u673a\u5236\uff08MECGAF\uff09\uff0c\u8be5\u673a\u5236\u52a8\u6001\u5730\u5c06\u524d\u5411mLSTM\u8f93\u51fa\uff08\u4f5c\u4e3a\u67e5\u8be2\u548c\u952e\uff09\u4e0ePF-mLSTM\u8f93\u51fa\uff08\u4f5c\u4e3a\u503c\uff09\u76f8\u7ed3\u5408\uff0c\u4ece\u800c\u4f18\u5316\u4e86\u77ed\u7a0b\u4f9d\u8d56\u6027\u6355\u83b7\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5168\u5c40\u4e0a\u4e0b\u6587\u548c\u6548\u7387\u3002", "result": "MEGA\u5728ABSA\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u6c34\u5e73\u3002", "conclusion": "MEGA\u5728ABSA\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\uff0c\u5e76\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2507.01182", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01182", "abs": "https://arxiv.org/abs/2507.01182", "authors": ["Zhuo Su", "Li Liu", "Matthias M\u00fcller", "Jiehua Zhang", "Diana Wofk", "Ming-Ming Cheng", "Matti Pietik\u00e4inen"], "title": "Rapid Salient Object Detection with Difference Convolutional Neural Networks", "comment": "16 pages, accepted in TPAMI", "summary": "This paper addresses the challenge of deploying salient object detection\n(SOD) on resource-constrained devices with real-time performance. While recent\nadvances in deep neural networks have improved SOD, existing top-leading models\nare computationally expensive. We propose an efficient network design that\ncombines traditional wisdom on SOD and the representation power of modern CNNs.\nLike biologically-inspired classical SOD methods relying on computing contrast\ncues to determine saliency of image regions, our model leverages Pixel\nDifference Convolutions (PDCs) to encode the feature contrasts. Differently,\nPDCs are incorporated in a CNN architecture so that the valuable contrast cues\nare extracted from rich feature maps. For efficiency, we introduce a difference\nconvolution reparameterization (DCR) strategy that embeds PDCs into standard\nconvolutions, eliminating computation and parameters at inference.\nAdditionally, we introduce SpatioTemporal Difference Convolution (STDC) for\nvideo SOD, enhancing the standard 3D convolution with spatiotemporal contrast\ncapture. Our models, SDNet for image SOD and STDNet for video SOD, achieve\nsignificant improvements in efficiency-accuracy trade-offs. On a Jetson Orin\ndevice, our models with $<$ 1M parameters operate at 46 FPS and 150 FPS on\nstreamed images and videos, surpassing the second-best lightweight models in\nour experiments by more than $2\\times$ and $3\\times$ in speed with superior\naccuracy. Code will be available at https://github.com/hellozhuo/stdnet.git.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u7f51\u7edc\u8bbe\u8ba1\uff0c\u7ed3\u5408\u4e86\u4f20\u7edfSOD\u65b9\u6cd5\u548c\u73b0\u4ee3CNN\u7684\u8868\u793a\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u4e0a\u90e8\u7f72\u5177\u6709\u5b9e\u65f6\u6027\u80fd\u7684\u663e\u8457\u76ee\u6807\u68c0\u6d4b(SOD)\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u7684\u9886\u5148\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002", "method": "\u7ed3\u5408\u4e86SOD\u7684\u4f20\u7edf\u65b9\u6cd5\u548c\u73b0\u4ee3CNN\u7684\u8868\u793a\u80fd\u529b\uff0c\u5229\u7528\u50cf\u7d20\u5dee\u5377\u79ef(PDCs)\u6765\u7f16\u7801\u7279\u5f81\u5bf9\u6bd4\uff0c\u5e76\u5f15\u5165\u5dee\u5206\u5377\u79ef\u91cd\u53c2\u6570\u5316(DCR)\u7b56\u7565\u5c06PDCs\u5d4c\u5165\u5230\u6807\u51c6\u5377\u79ef\u4e2d\uff0c\u51cf\u5c11\u63a8\u7406\u65f6\u7684\u8ba1\u7b97\u548c\u53c2\u6570\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u65f6\u7a7a\u5dee\u5206\u5377\u79ef(STDC)\u7528\u4e8e\u89c6\u9891SOD\uff0c\u901a\u8fc7\u65f6\u7a7a\u5bf9\u6bd4\u6355\u83b7\u6765\u589e\u5f3a\u6807\u51c63D\u5377\u79ef\u3002", "result": "SDNet\u548cSTDNet\u5728\u6548\u7387\u548c\u7cbe\u5ea6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\uff0c\u5728Jetson Orin\u8bbe\u5907\u4e0a\uff0c\u53c2\u6570\u5c0f\u4e8e1M\u7684\u6a21\u578b\u5728\u6d41\u5f0f\u56fe\u50cf\u548c\u89c6\u9891\u4e0a\u7684\u8fd0\u884c\u901f\u5ea6\u5206\u522b\u4e3a46 FPS\u548c150 FPS\uff0c\u6bd4\u7b2c\u4e8c\u597d\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\u5feb2\u500d\u4ee5\u4e0a\u548c3\u500d\u4ee5\u4e0a\uff0c\u5e76\u4e14\u5177\u6709\u66f4\u9ad8\u7684\u7cbe\u5ea6\u3002", "conclusion": "SDNet\u548cSTDNet\u5728\u56fe\u50cf\u548c\u89c6\u9891SOD\u4e0a\u5b9e\u73b0\u4e86\u6548\u7387\u548c\u7cbe\u5ea6\u4e0a\u7684\u663e\u8457\u63d0\u5347\uff0c\u5728Jetson Orin\u8bbe\u5907\u4e0a\uff0c\u53c2\u6570\u5c0f\u4e8e1M\u7684\u6a21\u578b\u5728\u6d41\u5f0f\u56fe\u50cf\u548c\u89c6\u9891\u4e0a\u7684\u8fd0\u884c\u901f\u5ea6\u5206\u522b\u4e3a46 FPS\u548c150 FPS\uff0c\u5728\u6211\u4eec\u7684\u5b9e\u9a8c\u4e2d\uff0c\u901f\u5ea6\u6bd4\u7b2c\u4e8c\u597d\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\u5feb2\u500d\u4ee5\u4e0a\u548c3\u500d\u4ee5\u4e0a\uff0c\u5e76\u4e14\u5177\u6709\u66f4\u9ad8\u7684\u7cbe\u5ea6\u3002"}}
{"id": "2507.01029", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01029", "abs": "https://arxiv.org/abs/2507.01029", "authors": ["Junjie Zhou", "Yingli Zuo", "Shichang Feng", "Peng Wan", "Qi Zhu", "Daoqiang Zhang", "Wei Shao"], "title": "PathCoT: Chain-of-Thought Prompting for Zero-shot Pathology Visual Reasoning", "comment": null, "summary": "With the development of generative artificial intelligence and instruction\ntuning techniques, multimodal large language models (MLLMs) have made\nimpressive progress on general reasoning tasks. Benefiting from the\nchain-of-thought (CoT) methodology, MLLMs can solve the visual reasoning\nproblem step-by-step. However, existing MLLMs still face significant challenges\nwhen applied to pathology visual reasoning tasks: (1) LLMs often underperforms\nbecause they lack domain-specific information, which can lead to model\nhallucinations. (2) The additional reasoning steps in CoT may introduce errors,\nleading to the divergence of answers. To address these limitations, we propose\nPathCoT, a novel zero-shot CoT prompting method which integrates the pathology\nexpert-knowledge into the reasoning process of MLLMs and incorporates\nself-evaluation to mitigate divergence of answers. Specifically, PathCoT guides\nthe MLLM with prior knowledge to perform as pathology experts, and provides\ncomprehensive analysis of the image with their domain-specific knowledge. By\nincorporating the experts' knowledge, PathCoT can obtain the answers with CoT\nreasoning. Furthermore, PathCoT incorporates a self-evaluation step that\nassesses both the results generated directly by MLLMs and those derived through\nCoT, finally determining the reliable answer. The experimental results on the\nPathMMU dataset demonstrate the effectiveness of our method on pathology visual\nunderstanding and reasoning.", "AI": {"tldr": "PathCoT \u662f\u4e00\u79cd\u65b0\u7684\u96f6\u6837\u672c CoT \u63d0\u793a\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u6574\u5408\u75c5\u7406\u5b66\u4e13\u5bb6\u77e5\u8bc6\u548c\u81ea\u6211\u8bc4\u4f30\u6765\u63d0\u9ad8 MLLM \u5728\u75c5\u7406\u89c6\u89c9\u7406\u89e3\u548c\u63a8\u7406\u65b9\u9762\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u5728\u5e94\u7528\u4e8e\u75c5\u7406\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u65f6\u4ecd\u7136\u9762\u4e34\u91cd\u5927\u6311\u6218\uff1a(1) LLM \u5e38\u5e38\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5b83\u4eec\u7f3a\u4e4f\u7279\u5b9a\u9886\u57df\u7684\u4fe1\u606f\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u4ea7\u751f\u5e7b\u89c9\u3002(2) CoT \u4e2d\u989d\u5916\u7684\u63a8\u7406\u6b65\u9aa4\u53ef\u80fd\u4f1a\u5f15\u5165\u9519\u8bef\uff0c\u5bfc\u81f4\u7b54\u6848\u53d1\u6563\u3002", "method": "PathCoT\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u96f6\u6837\u672cCoT\u63d0\u793a\u65b9\u6cd5\uff0c\u5b83\u5c06\u75c5\u7406\u5b66\u4e13\u5bb6\u77e5\u8bc6\u6574\u5408\u5230 MLLM \u7684\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u5e76\u7ed3\u5408\u81ea\u6211\u8bc4\u4f30\u6765\u51cf\u8f7b\u7b54\u6848\u7684\u53d1\u6563\u3002", "result": "PathMMU \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u75c5\u7406\u89c6\u89c9\u7406\u89e3\u548c\u63a8\u7406\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "PathCoT\u5728\u75c5\u7406\u89c6\u89c9\u7406\u89e3\u548c\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2507.01446", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01446", "abs": "https://arxiv.org/abs/2507.01446", "authors": ["Abd Elrahman Amer", "Magdi Amer"], "title": "Using multi-agent architecture to mitigate the risk of LLM hallucinations", "comment": null, "summary": "Improving customer service quality and response time are critical factors for\nmaintaining customer loyalty and increasing a company's market share. While\nadopting emerging technologies such as Large Language Models (LLMs) is becoming\na necessity to achieve these goals, the risk of hallucination remains a major\nchallenge. In this paper, we present a multi-agent system to handle customer\nrequests sent via SMS. This system integrates LLM based agents with fuzzy logic\nto mitigate hallucination risks.", "AI": {"tldr": "A multi-agent system using LLMs and fuzzy logic addresses customer requests via SMS, reducing hallucination risks.", "motivation": "Improving customer service quality and response time are critical factors for maintaining customer loyalty and increasing a company's market share. The risk of hallucination remains a major challenge.", "method": "LLM based agents with fuzzy logic", "result": "A multi-agent system to handle customer requests sent via SMS.", "conclusion": "This paper presents a multi-agent system to handle customer requests sent via SMS, integrating LLM based agents with fuzzy logic to mitigate hallucination risks."}}
{"id": "2507.01060", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01060", "abs": "https://arxiv.org/abs/2507.01060", "authors": ["Kang Liu"], "title": "Optimizing Conversational Product Recommendation via Reinforcement Learning", "comment": null, "summary": "We propose a reinforcement learning-based approach to optimize conversational\nstrategies for product recommendation across diverse industries. As\norganizations increasingly adopt intelligent agents to support sales and\nservice operations, the effectiveness of a conversation hinges not only on what\nis recommended but how and when recommendations are delivered. We explore a\nmethodology where agentic systems learn optimal dialogue policies through\nfeedback-driven reinforcement learning. By mining aggregate behavioral patterns\nand conversion outcomes, our approach enables agents to refine talk tracks that\ndrive higher engagement and product uptake, while adhering to contextual and\nregulatory constraints. We outline the conceptual framework, highlight key\ninnovations, and discuss the implications for scalable, personalized\nrecommendation in enterprise environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5bf9\u8bdd\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8\u4ea7\u54c1\u63a8\u8350\u8fc7\u7a0b\u4e2d\u7684\u7528\u6237\u53c2\u4e0e\u5ea6\u548c\u8f6c\u5316\u7387\u3002", "motivation": "\u968f\u7740\u8d8a\u6765\u8d8a\u591a\u7684\u7ec4\u7ec7\u91c7\u7528\u667a\u80fd\u4ee3\u7406\u6765\u652f\u6301\u9500\u552e\u548c\u670d\u52a1\u8fd0\u8425\uff0c\u5bf9\u8bdd\u7684\u6709\u6548\u6027\u4e0d\u4ec5\u53d6\u51b3\u4e8e\u63a8\u8350\u4ec0\u4e48\uff0c\u8fd8\u53d6\u51b3\u4e8e\u63a8\u8350\u7684\u65b9\u5f0f\u548c\u65f6\u95f4\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b66\u4e60\u6700\u4f18\u5bf9\u8bdd\u7b56\u7565\u3002", "result": "\u4ee3\u7406\u80fd\u591f\u6539\u8fdb\u5bf9\u8bdd\u7b56\u7565\uff0c\u63d0\u9ad8\u7528\u6237\u53c2\u4e0e\u5ea6\u548c\u4ea7\u54c1\u63a5\u53d7\u5ea6\u3002", "conclusion": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u8de8\u884c\u4e1a\u4ea7\u54c1\u63a8\u8350\u7684\u5bf9\u8bdd\u7b56\u7565\uff0c\u901a\u8fc7\u6316\u6398\u7528\u6237\u884c\u4e3a\u6a21\u5f0f\u548c\u8f6c\u5316\u7ed3\u679c\uff0c\u4f7fagent\u80fd\u591f\u6539\u8fdb\u5bf9\u8bdd\u7b56\u7565\uff0c\u63d0\u9ad8\u7528\u6237\u53c2\u4e0e\u5ea6\u548c\u4ea7\u54c1\u63a5\u53d7\u5ea6\u3002"}}
{"id": "2507.01234", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01234", "abs": "https://arxiv.org/abs/2507.01234", "authors": ["Yu Fan", "Yang Tian", "Shauli Ravfogel", "Mrinmaya Sachan", "Elliott Ash", "Alexander Hoyle"], "title": "The Medium Is Not the Message: Deconfounding Text Embeddings via Linear Concept Erasure", "comment": null, "summary": "Embedding-based similarity metrics between text sequences can be influenced\nnot just by the content dimensions we most care about, but can also be biased\nby spurious attributes like the text's source or language. These document\nconfounders cause problems for many applications, but especially those that\nneed to pool texts from different corpora. This paper shows that a debiasing\nalgorithm that removes information about observed confounders from the encoder\nrepresentations substantially reduces these biases at a minimal computational\ncost. Document similarity and clustering metrics improve across every embedding\nvariant and task we evaluate -- often dramatically. Interestingly, performance\non out-of-distribution benchmarks is not impacted, indicating that the\nembeddings are not otherwise degraded.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53bb\u504f\u7b97\u6cd5\uff0c\u7528\u4e8e\u6d88\u9664\u6587\u672c\u5d4c\u5165\u4e2d\u7684\u6df7\u6dc6\u56e0\u7d20\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u7b97\u6cd5\u53ef\u4ee5\u63d0\u9ad8\u6587\u6863\u76f8\u4f3c\u6027\u548c\u805a\u7c7b\u6307\u6807\u3002", "motivation": "\u57fa\u4e8e\u5d4c\u5165\u7684\u6587\u672c\u5e8f\u5217\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf\u53ef\u80fd\u53d7\u5230\u6587\u672c\u7684\u6765\u6e90\u6216\u8bed\u8a00\u7b49\u865a\u5047\u5c5e\u6027\u7684\u5f71\u54cd\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u53d7\u6211\u4eec\u6700\u5173\u5fc3\u7684\u5185\u5bb9\u7ef4\u5ea6\u7684\u5f71\u54cd\u3002\u8fd9\u4e9b\u6587\u6863\u6df7\u6dc6\u56e0\u7d20\u7ed9\u8bb8\u591a\u5e94\u7528\u5e26\u6765\u4e86\u95ee\u9898\uff0c\u5c24\u5176\u662f\u90a3\u4e9b\u9700\u8981\u6c47\u96c6\u6765\u81ea\u4e0d\u540c\u8bed\u6599\u5e93\u7684\u6587\u672c\u7684\u5e94\u7528\u3002", "method": "\u53bb\u504f\u7b97\u6cd5", "result": "\u6587\u6863\u76f8\u4f3c\u6027\u548c\u805a\u7c7b\u6307\u6807\u5728\u6240\u8bc4\u4f30\u7684\u6bcf\u4e2a\u5d4c\u5165\u53d8\u4f53\u548c\u4efb\u52a1\u4e2d\u90fd\u6709\u6240\u6539\u8fdb\uff0c\u800c\u4e14\u6539\u8fdb\u5e45\u5ea6\u901a\u5e38\u5f88\u5927\u3002\u6709\u8da3\u7684\u662f\uff0c\u5728\u5206\u5e03\u5916\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\u6ca1\u6709\u53d7\u5230\u5f71\u54cd\uff0c\u8fd9\u8868\u660e\u5d4c\u5165\u6ca1\u6709\u4ee5\u5176\u4ed6\u65b9\u5f0f\u964d\u7ea7\u3002", "conclusion": "\u6d88\u9664\u4e86\u7f16\u7801\u5668\u8868\u793a\u4e2d\u5173\u4e8e\u89c2\u5bdf\u5230\u7684\u6df7\u6dc6\u56e0\u7d20\u7684\u4fe1\u606f\u7684\u53bb\u504f\u7b97\u6cd5\uff0c\u5927\u5927\u51cf\u5c11\u4e86\u8fd9\u4e9b\u504f\u5dee\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u6781\u4f4e\u3002"}}
{"id": "2507.01254", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01254", "abs": "https://arxiv.org/abs/2507.01254", "authors": ["Runze Cheng", "Xihang Qiu", "Ming Li", "Ye Zhang", "Chun Li", "Fei Yu"], "title": "Robust Brain Tumor Segmentation with Incomplete MRI Modalities Using H\u00f6lder Divergence and Mutual Information-Enhanced Knowledge Transfer", "comment": null, "summary": "Multimodal MRI provides critical complementary information for accurate brain\ntumor segmentation. However, conventional methods struggle when certain\nmodalities are missing due to issues such as image quality, protocol\ninconsistencies, patient allergies, or financial constraints. To address this,\nwe propose a robust single-modality parallel processing framework that achieves\nhigh segmentation accuracy even with incomplete modalities. Leveraging Holder\ndivergence and mutual information, our model maintains modality-specific\nfeatures while dynamically adjusting network parameters based on the available\ninputs. By using these divergence- and information-based loss functions, the\nframework effectively quantifies discrepancies between predictions and\nground-truth labels, resulting in consistently accurate segmentation. Extensive\nevaluations on the BraTS 2018 and BraTS 2020 datasets demonstrate superior\nperformance over existing methods in handling missing modalities.", "AI": {"tldr": "This paper introduces a new MRI brain tumor segmentation framework that is robust to missing modalities by using Holder divergence and mutual information.", "motivation": "Conventional methods for brain tumor segmentation struggle when certain MRI modalities are missing due to issues such as image quality, protocol inconsistencies, patient allergies, or financial constraints.", "method": "The paper proposes a robust single-modality parallel processing framework that leverages Holder divergence and mutual information to maintain modality-specific features and dynamically adjust network parameters.", "result": "The model achieves high segmentation accuracy even with incomplete modalities by quantifying discrepancies between predictions and ground-truth labels using divergence- and information-based loss functions.", "conclusion": "The proposed framework demonstrates superior performance over existing methods in handling missing modalities, as shown by evaluations on the BraTS 2018 and BraTS 2020 datasets."}}
{"id": "2507.01030", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01030", "abs": "https://arxiv.org/abs/2507.01030", "authors": ["Reza Lotfi Navaei", "Mohammad Safarzadeh", "Seyed Mohammad Jafar Sobhani"], "title": "Optimizing Flamelet Generated Manifold Models: A Machine Learning Performance Study", "comment": "It has been submitted to ASME Journal of Heat and Mass Transfer", "summary": "In chemistry tabulations and Flamelet combustion models, the Flamelet\nGenerated Manifold (FGM) is recognized for its precision and physical\nrepresentation. The practical implementation of FGM requires a significant\nallocation of memory resources. FGM libraries are developed specifically for a\nspecific fuel and subsequently utilized for all numerical problems using\nmachine learning techniques. This research aims to develop libraries of Laminar\nFGM utilizing machine learning algorithms for application in combustion\nsimulations of methane fuel. This study employs four Machine Learning\nalgorithms to regenerate Flamelet libraries, based on an understanding of data\nsources, techniques, and data-driven concepts. 1. Multi-Layer Perceptron; 2.\nRandom Forest; 3. Linear Regression; 4. Support Vector Machine. Seven libraries\nwere identified as appropriate for constructing a database for training machine\nlearning models, giving an error rate of 2.30%. The default architectures of\neach method were evaluated to determine the optimal approach, leading to the\nselection of the MLP method as the primary choice. The method was enhanced\nthrough hyperparameter tuning to improve accuracy. The quantity of hidden\nlayers and neurons significantly influences method performance. The optimal\nmodel, comprising four hidden layers with 10, 15, 20, and 25 neurons\nrespectively, achieved an accuracy of 99.81%.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5f00\u53d1Laminar FGM\u5e93\uff0c\u7528\u4e8e\u7532\u70f7\u71c3\u6599\u71c3\u70e7\u6a21\u62df\uff0cMLP\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u8fbe99.81%\u3002", "motivation": "\u706b\u7130\u5c0f\u9762\u751f\u6210\u6d41\u5f62(FGM)\u56e0\u5176\u7cbe\u786e\u6027\u548c\u7269\u7406\u8868\u793a\u800c\u53d7\u5230\u8ba4\u53ef\u3002FGM\u7684\u5b9e\u9645\u5e94\u7528\u9700\u8981\u5927\u91cf\u7684\u5185\u5b58\u8d44\u6e90\u3002FGM\u5e93\u662f\u4e13\u95e8\u4e3a\u7279\u5b9a\u71c3\u6599\u5f00\u53d1\u7684\uff0c\u968f\u540e\u5229\u7528\u673a\u5668\u5b66\u4e60\u6280\u672f\u7528\u4e8e\u6240\u6709\u6570\u503c\u95ee\u9898\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1Laminar FGM\u5e93\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5728\u7532\u70f7\u71c3\u6599\u7684\u71c3\u70e7\u6a21\u62df\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528\u56db\u79cd\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff08\u591a\u5c42\u611f\u77e5\u5668\u3001\u968f\u673a\u68ee\u6797\u3001\u7ebf\u6027\u56de\u5f52\u3001\u652f\u6301\u5411\u91cf\u673a\uff09\u6765\u91cd\u5efaFlamelet libraries\u3002", "result": "\u786e\u5b9a\u4e86\u4e03\u4e2a\u5e93\u9002\u5408\u6784\u5efa\u7528\u4e8e\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6570\u636e\u5e93\uff0c\u8bef\u5dee\u7387\u4e3a2.30%\u3002\u8bc4\u4f30\u4e86\u6bcf\u79cd\u65b9\u6cd5\u7684\u9ed8\u8ba4\u67b6\u6784\u4ee5\u786e\u5b9a\u6700\u4f73\u65b9\u6cd5\uff0c\u4ece\u800c\u9009\u62e9MLP\u65b9\u6cd5\u4f5c\u4e3a\u4e3b\u8981\u9009\u62e9\u3002\u6700\u4f73\u6a21\u578b\u5b9e\u73b0\u4e8699.81%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u901a\u8fc7\u8d85\u53c2\u6570\u8c03\u6574\u589e\u5f3a\u4e86MLP\u65b9\u6cd5\uff0c\u6700\u4f73\u6a21\u578b\u5305\u542b\u56db\u4e2a\u9690\u85cf\u5c42\uff0c\u5206\u522b\u670910\u300115\u300120\u548c25\u4e2a\u795e\u7ecf\u5143\uff0c\u5b9e\u73b0\u4e8699.81%\u7684\u51c6\u786e\u7387\u3002"}}
{"id": "2507.01489", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.01489", "abs": "https://arxiv.org/abs/2507.01489", "authors": ["Yanfei Zhang"], "title": "Agent-as-Tool: A Study on the Hierarchical Decision Making with Reinforcement Learning", "comment": "12 pages", "summary": "Large Language Models (LLMs) have emerged as one of the most significant\ntechnological advancements in artificial intelligence in recent years. Their\nability to understand, generate, and reason with natural language has\ntransformed how we interact with AI systems. With the development of LLM-based\nagents and reinforcement-learning-based reasoning models, the study of applying\nreinforcement learning in agent frameworks has become a new research focus.\nHowever, all previous studies face the challenge of deciding the tool calling\nprocess and the reasoning process simultaneously, and the chain of reasoning\nwas solely relied on the unprocessed raw result with redundant information and\nsymbols unrelated to the task from the tool, which impose a heavy burden on the\nmodel's capability to reason. Therefore, in our research, we proposed a\nhierarchical framework Agent-as-tool that detach the tool calling process and\nthe reasoning process, which enables the model to focus on the verbally\nreasoning process while the tool calling process is handled by another agent.\nOur work had achieved comparable results with only a slight reinforcement\nfine-tuning on 180 samples, and had achieved exceptionally well performance in\nBamboogle with 63.2% of exact match and 75.2% in cover exact match, exceeding\nSearch-R1 by 4.8% in exact match and 3.2% in cover exact match.", "AI": {"tldr": "This paper introduces Agent-as-tool, a hierarchical framework for LLM agents that separates tool calling from reasoning, achieving improved performance on the Bamboogle dataset.", "motivation": "Previous studies struggle with simultaneously deciding the tool calling and reasoning processes, and rely on unprocessed tool results, burdening the model's reasoning capability.", "method": "The paper proposes a hierarchical framework called Agent-as-tool that separates the tool calling process from the reasoning process.", "result": "The Agent-as-tool framework achieves comparable results with slight reinforcement fine-tuning and outperforms Search-R1 on the Bamboogle dataset with 63.2% exact match and 75.2% cover exact match.", "conclusion": "The proposed Agent-as-tool framework achieves comparable or better performance by detaching the tool calling and reasoning processes, as demonstrated on the Bamboogle dataset."}}
{"id": "2507.01063", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01063", "abs": "https://arxiv.org/abs/2507.01063", "authors": ["Madhav Kotecha"], "title": "FAIR-MATCH: A Multi-Objective Framework for Bias Mitigation in Reciprocal Dating Recommendations", "comment": null, "summary": "Online dating platforms have fundamentally transformed the formation of\nromantic relationships, with millions of users worldwide relying on algorithmic\nmatching systems to find compatible partners. However, current recommendation\nsystems in dating applications suffer from significant algorithmic\ndeficiencies, including but not limited to popularity bias, filter bubble\neffects, and inadequate reciprocity modeling that limit effectiveness and\nintroduce harmful biases. This research integrates foundational work with\nrecent empirical findings to deliver a detailed analysis of dating app\nrecommendation systems, highlighting key issues and suggesting research-backed\nsolutions. Through analysis of reciprocal recommendation frameworks, fairness\nevaluation metrics, and industry implementations, we demonstrate that current\nsystems achieve modest performance with collaborative filtering reaching 25.1\\%\nwhile reciprocal methods achieve 28.7\\%. Our proposed mathematical framework\naddresses these limitations through enhanced similarity measures,\nmulti-objective optimization, and fairness-aware algorithms that maintain\ncompetitive accuracy while improving demographic representation to reduce\nalgorithmic bias.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e86\u7ea6\u4f1a\u5e94\u7528\u7a0b\u5e8f\u63a8\u8350\u7cfb\u7edf\uff0c\u5f3a\u8c03\u4e86\u5173\u952e\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u7814\u7a76\u652f\u6301\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u89e3\u51b3\u7b97\u6cd5\u504f\u5dee\u5e76\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7ea6\u4f1a\u5e94\u7528\u4e2d\u7684\u63a8\u8350\u7cfb\u7edf\u5b58\u5728\u663e\u8457\u7684\u7b97\u6cd5\u7f3a\u9677\uff0c\u5305\u62ec\u4f46\u4e0d\u9650\u4e8e\u53d7\u6b22\u8fce\u7a0b\u5ea6\u504f\u5dee\u3001\u8fc7\u6ee4\u6c14\u6ce1\u6548\u5e94\u548c\u4e0d\u5145\u5206\u7684\u4e92\u60e0\u5efa\u6a21\uff0c\u8fd9\u4e9b\u7f3a\u9677\u9650\u5236\u4e86\u6709\u6548\u6027\u5e76\u5f15\u5165\u4e86\u6709\u5bb3\u7684\u504f\u5dee\u3002", "method": "\u901a\u8fc7\u5206\u6790\u76f8\u4e92\u63a8\u8350\u6846\u67b6\u3001\u516c\u5e73\u6027\u8bc4\u4f30\u6307\u6807\u548c\u884c\u4e1a\u5b9e\u65bd\uff0c", "result": "\u76ee\u524d\u7684\u7cfb\u7edf\u6027\u80fd\u9002\u4e2d\uff0c\u534f\u540c\u8fc7\u6ee4\u8fbe\u5230 25.1%\uff0c\u800c\u4e92\u60e0\u65b9\u6cd5\u8fbe\u5230 28.7%\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6570\u5b66\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf\u3001\u591a\u76ee\u6807\u4f18\u5316\u548c\u516c\u5e73\u611f\u77e5\u7b97\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u8fd9\u4e9b\u7b97\u6cd5\u5728\u4fdd\u6301\u7ade\u4e89\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u6539\u8fdb\u4e86\u4eba\u53e3\u7edf\u8ba1\u5b66\u8868\u793a\uff0c\u4ee5\u51cf\u5c11\u7b97\u6cd5\u504f\u5dee\u3002"}}
{"id": "2507.01259", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01259", "abs": "https://arxiv.org/abs/2507.01259", "authors": ["Micha\u0142 Matak", "Jaros\u0142aw A. Chudziak"], "title": "GAIus: Combining Genai with Legal Clauses Retrieval for Knowledge-based Assistant", "comment": "8 pages, 2 figures, presented at ICAART 2025, in proceedings of the\n  17th International Conference on Agents and Artificial Intelligence - Volume\n  3: ICAART", "summary": "In this paper we discuss the capability of large language models to base\ntheir answer and provide proper references when dealing with legal matters of\nnon-english and non-chinese speaking country. We discuss the history of legal\ninformation retrieval, the difference between case law and statute law, its\nimpact on the legal tasks and analyze the latest research in this field. Basing\non that background we introduce gAIus, the architecture of the cognitive\nLLM-based agent, whose responses are based on the knowledge retrieved from\ncertain legal act, which is Polish Civil Code. We propose a retrieval mechanism\nwhich is more explainable, human-friendly and achieves better results than\nembedding-based approaches. To evaluate our method we create special dataset\nbased on single-choice questions from entrance exams for law apprenticeships\nconducted in Poland. The proposed architecture critically leveraged the\nabilities of used large language models, improving the gpt-3.5-turbo-0125 by\n419%, allowing it to beat gpt-4o and lifting gpt-4o-mini score from 31% to 86%.\nAt the end of our paper we show the possible future path of research and\npotential applications of our findings.", "AI": {"tldr": "This paper explores the use of large language models for legal information retrieval in non-English speaking countries, introduces a new LLM-based agent (gAIus) for Polish legal code, and demonstrates significant performance improvements over existing models.", "motivation": "The paper addresses the capability of large language models to provide accurate answers and references for legal matters in non-English and non-Chinese speaking countries. It discusses the history of legal information retrieval, the difference between case law and statute law, and analyzes recent research in the field.", "method": "The paper introduces gAIus, a cognitive LLM-based agent architecture, using a retrieval mechanism that is more explainable and human-friendly than embedding-based approaches. A dataset based on Polish law apprenticeship entrance exam questions was created to evaluate the method.", "result": "The proposed architecture significantly improved the performance of gpt-3.5-turbo-0125 by 419%, surpassing gpt-4o and increasing the gpt-4o-mini score from 31% to 86%.", "conclusion": "The paper concludes by outlining potential future research directions and applications of the findings."}}
{"id": "2507.01255", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.01255", "abs": "https://arxiv.org/abs/2507.01255", "authors": ["Xiao Liu", "Jiawei Zhang"], "title": "AIGVE-MACS: Unified Multi-Aspect Commenting and Scoring Model for AI-Generated Video Evaluation", "comment": "Working in Progress", "summary": "The rapid advancement of AI-generated video models has created a pressing\nneed for robust and interpretable evaluation frameworks. Existing metrics are\nlimited to producing numerical scores without explanatory comments, resulting\nin low interpretability and human evaluation alignment. To address those\nchallenges, we introduce AIGVE-MACS, a unified model for AI-Generated Video\nEvaluation(AIGVE), which can provide not only numerical scores but also\nmulti-aspect language comment feedback in evaluating these generated videos.\nCentral to our approach is AIGVE-BENCH 2, a large-scale benchmark comprising\n2,500 AI-generated videos and 22,500 human-annotated detailed comments and\nnumerical scores across nine critical evaluation aspects. Leveraging\nAIGVE-BENCH 2, AIGVE-MACS incorporates recent Vision-Language Models with a\nnovel token-wise weighted loss and a dynamic frame sampling strategy to better\nalign with human evaluators. Comprehensive experiments across supervised and\nzero-shot benchmarks demonstrate that AIGVE-MACS achieves state-of-the-art\nperformance in both scoring correlation and comment quality, significantly\noutperforming prior baselines including GPT-4o and VideoScore. In addition, we\nfurther showcase a multi-agent refinement framework where feedback from\nAIGVE-MACS drives iterative improvements in video generation, leading to 53.5%\nquality enhancement. This work establishes a new paradigm for comprehensive,\nhuman-aligned evaluation of AI-generated videos. We release the AIGVE-BENCH 2\nand AIGVE-MACS at https://huggingface.co/xiaoliux/AIGVE-MACS.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86 AIGVE-MACS\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30 AI \u751f\u6210\u89c6\u9891\u7684\u6a21\u578b\uff0c\u5b83\u4e0d\u4ec5\u63d0\u4f9b\u6570\u503c\u8bc4\u5206\uff0c\u8fd8\u80fd\u63d0\u4f9b\u591a\u65b9\u9762\u7684\u8bed\u8a00\u8bc4\u8bba\u53cd\u9988\u3002", "motivation": "\u73b0\u6709\u7684\u8bc4\u4f30\u6307\u6807\u4ec5\u9650\u4e8e\u751f\u6210\u6570\u503c\u5206\u6570\uff0c\u7f3a\u4e4f\u89e3\u91ca\u6027\u6ce8\u91ca\uff0c\u5bfc\u81f4\u53ef\u89e3\u91ca\u6027\u4f4e\uff0c\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u7684\u4e00\u81f4\u6027\u5dee\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u9c81\u68d2\u4e14\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86 AIGVE-MACS\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684 AI \u751f\u6210\u89c6\u9891\u8bc4\u4f30\u6a21\u578b\uff0c\u5b83\u7ed3\u5408\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u91c7\u7528\u4e86\u65b0\u7684 token-wise \u52a0\u6743\u635f\u5931\u548c\u52a8\u6001\u5e27\u91c7\u6837\u7b56\u7565\uff0c\u4ee5\u66f4\u597d\u5730\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u8005\u5bf9\u9f50\u3002", "result": "AIGVE-MACS \u5728\u8bc4\u5206\u76f8\u5173\u6027\u548c\u8bc4\u8bba\u8d28\u91cf\u65b9\u9762\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u901a\u8fc7 AIGVE-MACS \u7684\u53cd\u9988\u9a71\u52a8\u89c6\u9891\u751f\u6210\u7684\u8fed\u4ee3\u6539\u8fdb\uff0c\u5b9e\u73b0\u4e86 53.5% \u7684\u8d28\u91cf\u63d0\u5347\u3002", "conclusion": "AIGVE-MACS \u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\uff08\u5305\u62ec GPT-4o \u548c VideoScore\uff09\uff0c\u5e76\u5728\u89c6\u9891\u751f\u6210\u8d28\u91cf\u4e0a\u5b9e\u73b0\u4e86 53.5% \u7684\u63d0\u5347\uff0c\u4e3a\u5168\u9762\u3001\u4e0e\u4eba\u7c7b\u5bf9\u9f50\u7684 AI \u751f\u6210\u89c6\u9891\u8bc4\u4f30\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u8303\u5f0f\u3002\u8be5\u8bba\u6587\u53d1\u5e03\u4e86 AIGVE-BENCH 2 \u548c AIGVE-MACS\u3002"}}
{"id": "2507.01031", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.01031", "abs": "https://arxiv.org/abs/2507.01031", "authors": ["Fanchen Bu", "Kijung Shin"], "title": "PyTorch-based Geometric Learning with Non-CUDA Processing Units: Experiences from Intel Gaudi-v2 HPUs", "comment": "Conference paper: Accepted in Korea Computer Congress (KCC) 2025. The\n  library is available at https://github.com/bokveizen/gaudi-geometric-learning", "summary": "Geometric learning has emerged as a powerful paradigm for modeling\nnon-Euclidean data, especially graph-structured ones, with applications\nspanning social networks, molecular structures, knowledge graphs, and\nrecommender systems. While Nvidia's CUDA-enabled graphics processing units\n(GPUs) largely dominate the hardware landscape, emerging accelerators such as\nIntel's Gaudi Habana Processing Units (HPUs) offer competitive performance and\nenergy efficiency. However, the usage of such non-CUDA processing units\nrequires significant engineering effort and novel software adaptations. In this\nwork, we present our experiences porting PyTorch-based geometric learning\nframeworks to Gaudi-v2 HPUs. We introduce a collection of core utilities that\nrestore essential operations (e.g., scatter, sparse indexing, k-nearest\nneighbors) on Gaudi-v2 HPUs, and we consolidate sixteen guided tutorials and\neleven real-world examples with diagnostic analyses of encountered failures and\ndetailed workarounds. We collect all our experiences into a publicly accessible\nGitHub repository. Our contributions lower the barrier for researchers to\nexperiment with geometric-learning algorithms and models on non-CUDA hardware,\nproviding a foundation for further optimization and cross-platform portability.", "AI": {"tldr": "This paper presents experiences porting PyTorch-based geometric learning frameworks to Gaudi-v2 HPUs, introduces core utilities, and provides tutorials and examples to lower the barrier for researchers to experiment with geometric-learning algorithms and models on non-CUDA hardware.", "motivation": "Emerging accelerators such as Intel's Gaudi Habana Processing Units (HPUs) offer competitive performance and energy efficiency, but the usage of such non-CUDA processing units requires significant engineering effort and novel software adaptations.", "method": "porting PyTorch-based geometric learning frameworks to Gaudi-v2 HPUs and introducing a collection of core utilities that restore essential operations (e.g., scatter, sparse indexing, k-nearest neighbors) on Gaudi-v2 HPUs", "result": "We consolidate sixteen guided tutorials and eleven real-world examples with diagnostic analyses of encountered failures and detailed workarounds. We collect all our experiences into a publicly accessible GitHub repository.", "conclusion": "This work lowers the barrier for researchers to experiment with geometric-learning algorithms and models on non-CUDA hardware, providing a foundation for further optimization and cross-platform portability."}}
{"id": "2507.01597", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01597", "abs": "https://arxiv.org/abs/2507.01597", "authors": ["Yuehang Si", "Zefan Zeng", "Jincai Huang", "Qing Cheng"], "title": "T3DM: Test-Time Training-Guided Distribution Shift Modelling for Temporal Knowledge Graph Reasoning", "comment": null, "summary": "Temporal Knowledge Graph (TKG) is an efficient method for describing the\ndynamic development of facts along a timeline. Most research on TKG reasoning\n(TKGR) focuses on modelling the repetition of global facts and designing\npatterns of local historical facts. However, they face two significant\nchallenges: inadequate modeling of the event distribution shift between\ntraining and test samples, and reliance on random entity substitution for\ngenerating negative samples, which often results in low-quality sampling. To\nthis end, we propose a novel distributional feature modeling approach for\ntraining TKGR models, Test-Time Training-guided Distribution shift Modelling\n(T3DM), to adjust the model based on distribution shift and ensure the global\nconsistency of model reasoning. In addition, we design a negative-sampling\nstrategy to generate higher-quality negative quadruples based on adversarial\ntraining. Extensive experiments show that T3DM provides better and more robust\nresults than the state-of-the-art baselines in most cases.", "AI": {"tldr": "T3DM\u901a\u8fc7\u5206\u5e03\u7279\u5f81\u5efa\u6a21\u548c\u5bf9\u6297\u6027\u8d1f\u62bd\u6837\uff0c\u63d0\u9ad8\u4e86\u65f6\u95f4\u77e5\u8bc6\u56fe\u63a8\u7406\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u65f6\u95f4\u77e5\u8bc6\u56fe\u63a8\u7406(TKGR)\u7814\u7a76\u9762\u4e34\u4e24\u4e2a\u91cd\u5927\u6311\u6218\uff1a\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6837\u672c\u4e4b\u95f4\u4e8b\u4ef6\u5206\u5e03\u504f\u79fb\u5efa\u6a21\u4e0d\u8db3\uff0c\u4ee5\u53ca\u4f9d\u8d56\u968f\u673a\u5b9e\u4f53\u66ff\u6362\u751f\u6210\u8d1f\u6837\u672c\uff0c\u8fd9\u901a\u5e38\u5bfc\u81f4\u4f4e\u8d28\u91cf\u62bd\u6837\u3002", "method": "\u4e00\u79cd\u7528\u4e8e\u8bad\u7ec3TKGR\u6a21\u578b\u7684\u65b0\u7684\u5206\u5e03\u7279\u5f81\u5efa\u6a21\u65b9\u6cd5\uff0c\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u5f15\u5bfc\u7684\u5206\u5e03\u504f\u79fb\u5efa\u6a21(T3DM)\uff0c\u4ee5\u6839\u636e\u5206\u5e03\u504f\u79fb\u8c03\u6574\u6a21\u578b\uff0c\u5e76\u786e\u4fdd\u6a21\u578b\u63a8\u7406\u7684\u5168\u5c40\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u8d1f\u62bd\u6837\u7b56\u7565\uff0c\u4ee5\u57fa\u4e8e\u5bf9\u6297\u6027\u8bad\u7ec3\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u8d1f\u56db\u5143\u7ec4\u3002", "result": "T3DM\u63d0\u4f9b\u4e86\u66f4\u597d\u3001\u66f4\u7a33\u5065\u7684\u7ed3\u679c\u3002", "conclusion": "T3DM\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u63d0\u4f9b\u66f4\u597d\u3001\u66f4\u7a33\u5065\u7684\u7ed3\u679c\u3002"}}
{"id": "2507.01066", "categories": ["cs.IR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01066", "abs": "https://arxiv.org/abs/2507.01066", "authors": ["Hanzhong Liang", "Jinghao Shi", "Xiang Shen", "Zixuan Wang", "Vera Wen", "Ardalan Mehrani", "Zhiqian Chen", "Yifan Wu", "Zhixin Zhang"], "title": "Embedding-based Retrieval in Multimodal Content Moderation", "comment": "Camera ready for SIGIR 2025", "summary": "Video understanding plays a fundamental role for content moderation on short\nvideo platforms, enabling the detection of inappropriate content. While\nclassification remains the dominant approach for content moderation, it often\nstruggles in scenarios requiring rapid and cost-efficient responses, such as\ntrend adaptation and urgent escalations. To address this issue, we introduce an\nEmbedding-Based Retrieval (EBR) method designed to complement traditional\nclassification approaches. We first leverage a Supervised Contrastive Learning\n(SCL) framework to train a suite of foundation embedding models, including both\nsingle-modal and multi-modal architectures. Our models demonstrate superior\nperformance over established contrastive learning methods such as CLIP and\nMoCo. Building on these embedding models, we design and implement the\nembedding-based retrieval system that integrates embedding generation and video\nretrieval to enable efficient and effective trend handling. Comprehensive\noffline experiments on 25 diverse emerging trends show that EBR improves\nROC-AUC from 0.85 to 0.99 and PR-AUC from 0.35 to 0.95. Further online\nexperiments reveal that EBR increases action rates by 10.32% and reduces\noperational costs by over 80%, while also enhancing interpretability and\nflexibility compared to classification-based solutions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5d4c\u5165\u7684\u68c0\u7d22 (EBR) \u65b9\u6cd5\uff0c\u4ee5\u8865\u5145\u4f20\u7edf\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u7528\u4e8e\u77ed\u89c6\u9891\u5e73\u53f0\u4e0a\u7684\u5185\u5bb9\u5ba1\u6838\uff0c\u4ece\u800c\u63d0\u9ad8\u6548\u7387\u5e76\u964d\u4f4e\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u7684\u5206\u7c7b\u65b9\u6cd5\u5728\u9700\u8981\u5feb\u901f\u548c\u7ecf\u6d4e\u9ad8\u6548\u7684\u54cd\u5e94\u7684\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4f8b\u5982\u8d8b\u52bf\u9002\u5e94\u548c\u7d27\u6025\u5347\u7ea7\u3002", "method": "\u5229\u7528\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60 (SCL) \u6846\u67b6\u8bad\u7ec3\u4e86\u4e00\u5957\u57fa\u7840\u5d4c\u5165\u6a21\u578b\uff0c\u5305\u62ec\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u67b6\u6784\uff0c\u5e76\u8bbe\u8ba1\u548c\u5b9e\u65bd\u4e86\u57fa\u4e8e\u5d4c\u5165\u7684\u68c0\u7d22\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u96c6\u6210\u4e86\u5d4c\u5165\u751f\u6210\u548c\u89c6\u9891\u68c0\u7d22\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u548c\u6709\u6548\u7684\u8d8b\u52bf\u5904\u7406\u3002", "result": "\u79bb\u7ebf\u5b9e\u9a8c\u8868\u660e\uff0cEBR \u5c06 ROC-AUC \u4ece 0.85 \u63d0\u9ad8\u5230 0.99\uff0cPR-AUC \u4ece 0.35 \u63d0\u9ad8\u5230 0.95\u3002\u5728\u7ebf\u5b9e\u9a8c\u8868\u660e\uff0cEBR \u5c06\u64cd\u4f5c\u7387\u63d0\u9ad8\u4e86 10.32%\uff0c\u5e76\u5c06\u8fd0\u8425\u6210\u672c\u964d\u4f4e\u4e86 80% \u4ee5\u4e0a\u3002", "conclusion": "EBR\u901a\u8fc7\u63d0\u9ad8\u6548\u7387\u3001\u964d\u4f4e\u6210\u672c\u5e76\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u548c\u7075\u6d3b\u6027\uff0c\u4f18\u4e8e\u57fa\u4e8e\u5206\u7c7b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01278", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01278", "abs": "https://arxiv.org/abs/2507.01278", "authors": ["Cindy Lie Tabuse", "David Restepo", "Carolina Gracitelli", "Fernando Korn Malerbi", "Caio Regatieri", "Luis Filipe Nakayama"], "title": "Evaluating Large Language Models for Multimodal Simulated Ophthalmic Decision-Making in Diabetic Retinopathy and Glaucoma Screening", "comment": null, "summary": "Large language models (LLMs) can simulate clinical reasoning based on natural\nlanguage prompts, but their utility in ophthalmology is largely unexplored.\nThis study evaluated GPT-4's ability to interpret structured textual\ndescriptions of retinal fundus photographs and simulate clinical decisions for\ndiabetic retinopathy (DR) and glaucoma screening, including the impact of\nadding real or synthetic clinical metadata. We conducted a retrospective\ndiagnostic validation study using 300 annotated fundus images. GPT-4 received\nstructured prompts describing each image, with or without patient metadata. The\nmodel was tasked with assigning an ICDR severity score, recommending DR\nreferral, and estimating the cup-to-disc ratio for glaucoma referral.\nPerformance was evaluated using accuracy, macro and weighted F1 scores, and\nCohen's kappa. McNemar's test and change rate analysis were used to assess the\ninfluence of metadata. GPT-4 showed moderate performance for ICDR\nclassification (accuracy 67.5%, macro F1 0.33, weighted F1 0.67, kappa 0.25),\ndriven mainly by correct identification of normal cases. Performance improved\nin the binary DR referral task (accuracy 82.3%, F1 0.54, kappa 0.44). For\nglaucoma referral, performance was poor across all settings (accuracy ~78%, F1\n<0.04, kappa <0.03). Metadata inclusion did not significantly affect outcomes\n(McNemar p > 0.05), and predictions remained consistent across conditions.\nGPT-4 can simulate basic ophthalmic decision-making from structured prompts but\nlacks precision for complex tasks. While not suitable for clinical use, LLMs\nmay assist in education, documentation, or image annotation workflows in\nophthalmology.", "AI": {"tldr": "GPT-4's ability to interpret retinal fundus photographs for diabetic retinopathy and glaucoma screening was evaluated. It showed moderate performance for DR but poor performance for glaucoma, with metadata having no significant impact. LLMs may assist in education, documentation, or image annotation workflows in ophthalmology, but are not suitable for clinical use.", "motivation": "LLMs can simulate clinical reasoning based on natural language prompts, but their utility in ophthalmology is largely unexplored. This study evaluated GPT-4's ability to interpret structured textual descriptions of retinal fundus photographs and simulate clinical decisions for diabetic retinopathy (DR) and glaucoma screening, including the impact of adding real or synthetic clinical metadata.", "method": "GPT-4 received structured prompts describing each image, with or without patient metadata. The model was tasked with assigning an ICDR severity score, recommending DR referral, and estimating the cup-to-disc ratio for glaucoma referral. Performance was evaluated using accuracy, macro and weighted F1 scores, and Cohen's kappa. McNemar's test and change rate analysis were used to assess the influence of metadata.", "result": "GPT-4 showed moderate performance for ICDR classification (accuracy 67.5%, macro F1 0.33, weighted F1 0.67, kappa 0.25), driven mainly by correct identification of normal cases. Performance improved in the binary DR referral task (accuracy 82.3%, F1 0.54, kappa 0.44). For glaucoma referral, performance was poor across all settings (accuracy ~78%, F1 <0.04, kappa <0.03). Metadata inclusion did not significantly affect outcomes (McNemar p > 0.05), and predictions remained consistent across conditions.", "conclusion": "GPT-4 can simulate basic ophthalmic decision-making from structured prompts but lacks precision for complex tasks. While not suitable for clinical use, LLMs may assist in education, documentation, or image annotation workflows in ophthalmology."}}
{"id": "2507.01269", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.01269", "abs": "https://arxiv.org/abs/2507.01269", "authors": ["Mohammad Jahanbakht", "Alex Olsen", "Ross Marchant", "Emilie Fillols", "Mostafa Rahimi Azghadi"], "title": "Advancements in Weed Mapping: A Systematic Review", "comment": null, "summary": "Weed mapping plays a critical role in precision management by providing\naccurate and timely data on weed distribution, enabling targeted control and\nreduced herbicide use. This minimizes environmental impacts, supports\nsustainable land management, and improves outcomes across agricultural and\nnatural environments. Recent advances in weed mapping leverage ground-vehicle\nRed Green Blue (RGB) cameras, satellite and drone-based remote sensing combined\nwith sensors such as spectral, Near Infra-Red (NIR), and thermal cameras. The\nresulting data are processed using advanced techniques including big data\nanalytics and machine learning, significantly improving the spatial and\ntemporal resolution of weed maps and enabling site-specific management\ndecisions. Despite a growing body of research in this domain, there is a lack\nof comprehensive literature reviews specifically focused on weed mapping. In\nparticular, the absence of a structured analysis spanning the entire mapping\npipeline, from data acquisition to processing techniques and mapping tools,\nlimits progress in the field. This review addresses these gaps by\nsystematically examining state-of-the-art methods in data acquisition (sensor\nand platform technologies), data processing (including annotation and\nmodelling), and mapping techniques (such as spatiotemporal analysis and\ndecision support tools). Following PRISMA guidelines, we critically evaluate\nand synthesize key findings from the literature to provide a holistic\nunderstanding of the weed mapping landscape. This review serves as a\nfoundational reference to guide future research and support the development of\nefficient, scalable, and sustainable weed management systems.", "AI": {"tldr": "This review addresses the gap in comprehensive weed mapping literature by systematically examining data acquisition, processing, and mapping techniques to guide future research and improve weed management.", "motivation": "The lack of comprehensive literature reviews specifically focused on weed mapping, particularly a structured analysis spanning the entire mapping pipeline, limits progress in the field.", "method": "A systematic review following PRISMA guidelines was conducted to examine state-of-the-art methods in data acquisition, data processing, and mapping techniques.", "result": "The review critically evaluates and synthesizes key findings from the literature to provide a holistic understanding of the weed mapping landscape.", "conclusion": "This review provides a holistic understanding of the weed mapping landscape, serving as a foundational reference to guide future research and support the development of efficient, scalable, and sustainable weed management systems."}}
{"id": "2507.01032", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.01032", "abs": "https://arxiv.org/abs/2507.01032", "authors": ["Nan Mu", "Hongbo Yang", "Chen Zhao"], "title": "An Uncertainty-Aware Dynamic Decision Framework for Progressive Multi-Omics Integration in Classification Tasks", "comment": null, "summary": "Background and Objective: High-throughput multi-omics technologies have\nproven invaluable for elucidating disease mechanisms and enabling early\ndiagnosis. However, the high cost of multi-omics profiling imposes a\nsignificant economic burden, with over reliance on full omics data potentially\nleading to unnecessary resource consumption. To address these issues, we\npropose an uncertainty-aware, multi-view dynamic decision framework for omics\ndata classification that aims to achieve high diagnostic accuracy while\nminimizing testing costs. Methodology: At the single-omics level, we refine the\nactivation functions of neural networks to generate Dirichlet distribution\nparameters, utilizing subjective logic to quantify both the belief masses and\nuncertainty mass of classification results. Belief mass reflects the support of\na specific omics modality for a disease class, while the uncertainty parameter\ncaptures limitations in data quality and model discriminability, providing a\nmore trustworthy basis for decision-making. At the multi omics level, we employ\na fusion strategy based on Dempster-Shafer theory to integrate heterogeneous\nmodalities, leveraging their complementarity to boost diagnostic accuracy and\nrobustness. A dynamic decision mechanism is then applied that omics data are\nincrementally introduced for each patient until either all data sources are\nutilized or the model confidence exceeds a predefined threshold, potentially\nbefore all data sources are utilized. Results and Conclusion: We evaluate our\napproach on four benchmark multi-omics datasets, ROSMAP, LGG, BRCA, and KIPAN.\nIn three datasets, over 50% of cases achieved accurate classification using a\nsingle omics modality, effectively reducing redundant testing. Meanwhile, our\nmethod maintains diagnostic performance comparable to full-omics models and\npreserves essential biological insights.", "AI": {"tldr": "Developed a method to reduce multi-omics testing costs while maintaining accuracy by using an uncertainty-aware dynamic decision framework. It reduces redundant testing by using a single omics modality in some cases.", "motivation": "High cost of multi-omics profiling imposes a significant economic burden, with over reliance on full omics data potentially leading to unnecessary resource consumption.", "method": "An uncertainty-aware, multi-view dynamic decision framework for omics data classification. At the single-omics level, refine the activation functions of neural networks to generate Dirichlet distribution parameters, utilizing subjective logic to quantify both the belief masses and uncertainty mass of classification results. At the multi omics level, employ a fusion strategy based on Dempster-Shafer theory to integrate heterogeneous modalities. Apply a dynamic decision mechanism that omics data are incrementally introduced for each patient until a threshold is met.", "result": "Achieved accurate classification using a single omics modality in over 50% of cases in three datasets, effectively reducing redundant testing. Maintains diagnostic performance comparable to full-omics models and preserves essential biological insights.", "conclusion": "The method maintains diagnostic performance comparable to full-omics models and preserves essential biological insights. In three datasets, over 50% of cases achieved accurate classification using a single omics modality, effectively reducing redundant testing."}}
{"id": "2507.01717", "categories": ["cs.AI", "cs.IR", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.01717", "abs": "https://arxiv.org/abs/2507.01717", "authors": ["Gopichand Kanumolu", "Ashok Urlana", "Charaka Vinayak Kumar", "Bala Mallikarjunarao Garlapati"], "title": "Agent Ideate: A Framework for Product Idea Generation from Patents Using Agentic AI", "comment": "AgentScen Workshop, IJCAI 2025", "summary": "Patents contain rich technical knowledge that can inspire innovative product\nideas, yet accessing and interpreting this information remains a challenge.\nThis work explores the use of Large Language Models (LLMs) and autonomous\nagents to mine and generate product concepts from a given patent. In this work,\nwe design Agent Ideate, a framework for automatically generating product-based\nbusiness ideas from patents. We experimented with open-source LLMs and\nagent-based architectures across three domains: Computer Science, Natural\nLanguage Processing, and Material Chemistry. Evaluation results show that the\nagentic approach consistently outperformed standalone LLMs in terms of idea\nquality, relevance, and novelty. These findings suggest that combining LLMs\nwith agentic workflows can significantly enhance the innovation pipeline by\nunlocking the untapped potential of business idea generation from patent data.", "AI": {"tldr": "\u672c\u6587\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u81ea\u4e3b\u4ee3\u7406\u6765\u6316\u6398\u4e13\u5229\u5e76\u4ece\u4e2d\u751f\u6210\u4ea7\u54c1\u6982\u5ff5\u3002", "motivation": "\u4e13\u5229\u5305\u542b\u4e30\u5bcc\u7684\u6280\u672f\u77e5\u8bc6\uff0c\u53ef\u4ee5\u6fc0\u53d1\u521b\u65b0\u7684\u4ea7\u54c1\u521b\u610f\uff0c\u4f46\u8bbf\u95ee\u548c\u89e3\u91ca\u8fd9\u4e9b\u4fe1\u606f\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u6211\u4eec\u8bbe\u8ba1\u4e86Agent Ideate\uff0c\u4e00\u4e2a\u4ece\u4e13\u5229\u4e2d\u81ea\u52a8\u751f\u6210\u57fa\u4e8e\u4ea7\u54c1\u7684\u5546\u4e1a\u60f3\u6cd5\u7684\u6846\u67b6\u3002 \u6211\u4eec\u5728\u4e09\u4e2a\u9886\u57df\uff08\u8ba1\u7b97\u673a\u79d1\u5b66\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u6750\u6599\u5316\u5b66\uff09\u8bd5\u9a8c\u4e86\u5f00\u6e90LLM\u548c\u57fa\u4e8eAgent\u7684\u67b6\u6784\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u5728\u60f3\u6cd5\u8d28\u91cf\u3001\u76f8\u5173\u6027\u548c\u65b0\u9896\u6027\u65b9\u9762\uff0cAgent\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u72ec\u7acb\u7684LLM\u3002", "conclusion": "\u7ed3\u5408LLM\u548cAgent\u5de5\u4f5c\u6d41\u53ef\u4ee5\u901a\u8fc7\u91ca\u653e\u4e13\u5229\u6570\u636e\u4e2d\u5546\u4e1a\u60f3\u6cd5\u4ea7\u751f\u7684\u672a\u5f00\u53d1\u6f5c\u529b\uff0c\u4ece\u800c\u663e\u8457\u589e\u5f3a\u521b\u65b0\u7ba1\u9053\u3002"}}
{"id": "2507.01168", "categories": ["cs.IR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.01168", "abs": "https://arxiv.org/abs/2507.01168", "authors": ["Yeonbin Son", "Matthew L. Bolton"], "title": "Towards a Signal Detection Based Measure for Assessing Information Quality of Explainable Recommender Systems", "comment": "Accepted to IEEE CAI 2025", "summary": "There is growing interest in explainable recommender systems that provide\nrecommendations along with explanations for the reasoning behind them. When\nevaluating recommender systems, most studies focus on overall recommendation\nperformance. Only a few assess the quality of the explanations. Explanation\nquality is often evaluated through user studies that subjectively gather users'\nopinions on representative explanatory factors that shape end-users'\nperspective towards the results, not about the explanation contents itself. We\naim to fill this gap by developing an objective metric to evaluate Veracity:\nthe information quality of explanations. Specifically, we decompose Veracity\ninto two dimensions: Fidelity and Attunement. Fidelity refers to whether the\nexplanation includes accurate information about the recommended item.\nAttunement evaluates whether the explanation reflects the target user's\npreferences. By applying signal detection theory, we first determine decision\noutcomes for each dimension and then combine them to calculate a sensitivity,\nwhich serves as the final Veracity value. To assess the effectiveness of the\nproposed metric, we set up four cases with varying levels of information\nquality to validate whether our metric can accurately capture differences in\nquality. The results provided meaningful insights into the effectiveness of our\nproposed metric.", "AI": {"tldr": "This paper proposes an objective metric, Veracity, to evaluate the quality of explanations in recommender systems, which is decomposed into Fidelity and Attunement. The metric's effectiveness is validated through experiments.", "motivation": "Existing evaluations of recommender systems primarily focus on recommendation performance, with limited assessment of explanation quality, often relying on subjective user opinions. This paper aims to fill this gap by developing an objective metric to evaluate the information quality of explanations.", "method": "The paper decomposes Veracity into Fidelity and Attunement, uses signal detection theory to determine decision outcomes for each dimension, and combines them to calculate a sensitivity, which serves as the final Veracity value.", "result": "The results of the four cases provided meaningful insights into the effectiveness of the proposed Veracity metric in capturing differences in information quality.", "conclusion": "The paper introduces an objective metric, Veracity, to evaluate the information quality of explanations in recommender systems. The effectiveness of the metric is validated through four cases."}}
