<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 56]
- [cs.CV](#cs.CV) [Total: 54]
- [cs.AI](#cs.AI) [Total: 26]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.LG](#cs.LG) [Total: 55]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Hallucination reduction with CASAL: Contrastive Activation Steering For Amortized Learning](https://arxiv.org/abs/2510.02324)
*Wannan Yang,Xinchi Qiu,Lei Yu,Yuchen Zhang,Oliver Aobo Yang,Narine Kokhlikyan,Nicola Cancedda,Diego Garcia-Olano*

Main category: cs.CL

TL;DR: 本文介绍了一种名为对比激活引导的摊销学习 (CASAL) 的高效算法，该算法将可解释性与摊销优化联系起来，直接将激活引导的优势融入模型权重中，减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 表现出令人印象深刻的能力，但经常产生幻觉，自信地提供不正确的答案而不是承认无知。先前的工作表明，模型对自身知识进行线性表示，并且激活引导可以减少幻觉。然而，这些方法需要在推理过程中进行实时监控和干预。

Method: 我们引入了对比激活引导的摊销学习 (CASAL)，这是一种高效的算法，它将可解释性与摊销优化联系起来。CASAL 直接将激活引导的优势融入模型的权重中。一旦经过训练，法学硕士会回答他们知道的问题，同时避免回答他们不知道的问题。CASAL 的轻量级设计只需要训练单个 Transformer 层的一个子模块，但可以在多个简短的 QA 基准测试中将幻觉减少 30%-40%。

Result: CASAL 的计算效率比基于 LoRA 的强大基线（如 SFT 和 DPO）高 30 倍，数据效率高 20 倍，从而提高了其在数据稀缺领域中的实际适用性。重要的是，CASAL 还可以有效地推广到分布外 (OOD) 域。我们展示了 CASAL 在减轻纯文本和视觉语言模型中的幻觉方面的灵活性。据我们所知，CASAL 是第一个已被证明对密集模型和混合专家 (MoE) 模型都有效的基于转向的训练方法。

Conclusion: CASAL 代表了应用可解释性启发方法在生产系统中进行实际部署的有希望的一步。

Abstract: Large Language Models (LLMs) exhibit impressive capabilities but often
hallucinate, confidently providing incorrect answers instead of admitting
ignorance. Prior work has shown that models encode linear representations of
their own knowledge and that activation steering can reduce hallucinations.
These approaches, however, require real-time monitoring and intervention during
inference. We introduce Contrastive Activation Steering for Amortized Learning
(CASAL), an efficient algorithm that connects interpretability with amortized
optimization. CASAL directly bakes the benefits of activation steering into
model's weights. Once trained, LLMs answer questions they know while abstaining
from answering those they do not. CASAL's light-weight design requires training
only a submodule of a single transformer layer and yet reduces hallucination by
30%-40% across multiple short-form QA benchmarks. CASAL is 30x more
compute-efficient and 20x more data-efficient than strong LoRA-based baselines
such as SFT and DPO, boosting its practical applicability in data scarce
domains. Importantly, CASAL also generalizes effectively to out-of-distribution
(OOD) domains. We showcase CASAL's flexibility in mitigating hallucinations in
both text-only and vision-language models. To our knowledge, CASAL is the first
steering-based training method that has been shown to be effective for both
dense and Mixture-of-Experts (MoE) models. CASAL represents a promising step
forward for applying interpretability-inspired method for practical deployment
in production systems.

</details>


### [2] [Hallucination-Resistant, Domain-Specific Research Assistant with Self-Evaluation and Vector-Grounded Retrieval](https://arxiv.org/abs/2510.02326)
*Vivek Bhavsar,Joseph Ereifej,Aravanan Gurusami*

Main category: cs.CL

TL;DR: RA-FSM: A modular, GPT-based research assistant grounded in vector retrieval and a deterministic citation pipeline.


<details>
  <summary>Details</summary>
Motivation: Large language models can hallucinate and mis-cite, limiting their usefulness in expert workflows.

Method: A modular GPT-based research assistant that wraps generation in a finite-state control loop: Relevance -> Confidence -> Knowledge. The system is grounded in vector retrieval and a deterministic citation pipeline. A ranked-tier ingestion workflow constructs a domain knowledge base from journals, conferences, indices, preprints, and patents, writing both to a dense vector index and to a relational store of normalized metrics.

Result: Domain experts prefer RA-FSM to both a strong Notebook LM (NLM) and a vanilla Default GPT API call single-pass baseline, citing stronger boundary-condition handling and more defensible evidence use. Coverage and novelty analyses indicate that RA-FSM explores beyond the NLM while incurring tunable latency and cost overheads.

Conclusion: The design emphasizes transparent, well-cited answers for high-stakes technical work and is generalizable to other scientific domains.

Abstract: Large language models accelerate literature synthesis but can hallucinate and
mis-cite, limiting their usefulness in expert workflows. We present RA-FSM
(Research Assistant - Finite State Machine), a modular GPT-based research
assistant that wraps generation in a finite-state control loop: Relevance ->
Confidence -> Knowledge. The system is grounded in vector retrieval and a
deterministic citation pipeline. The controller filters out-of-scope queries,
scores answerability, decomposes questions, and triggers retrieval only when
needed, and emits answers with confidence labels and in-corpus, de-duplicated
references. A ranked-tier ingestion workflow constructs a domain knowledge base
from journals, conferences, indices, preprints, and patents, writing both to a
dense vector index and to a relational store of normalized metrics. We
implement the system for photonics and evaluate it on six task categories:
analytical reasoning, numerical analysis, methodological critique, comparative
synthesis, factual extraction, and application design. In blinded A/B reviews,
domain experts prefer RA-FSM to both a strong Notebook LM (NLM) and a vanilla
Default GPT API call single-pass baseline, citing stronger boundary-condition
handling and more defensible evidence use. Coverage and novelty analyses
indicate that RA-FSM explores beyond the NLM while incurring tunable latency
and cost overheads. The design emphasizes transparent, well-cited answers for
high-stakes technical work and is generalizable to other scientific domains.

</details>


### [3] [KAME: Tandem Architecture for Enhancing Knowledge in Real-Time Speech-to-Speech Conversational AI](https://arxiv.org/abs/2510.02327)
*So Kuroki,Yotaro Kubo,Takuya Akiba,Yujin Tang*

Main category: cs.CL

TL;DR: 提出了一种混合架构，结合了 S2S 模型的低延迟和 LLM 的知识，以提高语音到语音对话系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的 S2S 模型缺乏深层知识和语义理解，而级联系统延迟高。

Method: 使用 S2S transformer 处理用户语音，同时将查询传递给 LLM，LLM 的文本响应实时注入以指导 S2S 模型的语音生成。

Result: 在 MT-Bench 基准测试中，该系统在响应正确性方面优于基线 S2S 模型，接近级联系统，同时保持与基线相当的延迟。

Conclusion: 该混合架构有效地融合了 S2S 模型的低延迟和 LLM 的丰富知识。

Abstract: Real-time speech-to-speech (S2S) models excel at generating natural,
low-latency conversational responses but often lack deep knowledge and semantic
understanding. Conversely, cascaded systems combining automatic speech
recognition, a text-based Large Language Model (LLM), and text-to-speech
synthesis offer superior knowledge representation at the cost of high latency,
which disrupts the flow of natural interaction. This paper introduces a novel
hybrid architecture that bridges the gap between these two paradigms. Our
framework processes user speech through an S2S transformer for immediate
responsiveness while concurrently relaying the query to a powerful back-end
LLM. The LLM's text-based response is then injected in real time to guide the
S2S model's speech generation, effectively infusing its output with rich
knowledge without the full latency penalty of a cascaded system. We evaluated
our method using a speech-synthesized variant of the MT-Bench benchmark that
consists of multi-turn question-answering sessions. The results demonstrate
that our system substantially outperforms a baseline S2S model in response
correctness, approaching that of a cascaded system, while maintaining a latency
on par with the baseline.

</details>


### [4] [AMANDA: Agentic Medical Knowledge Augmentation for Data-Efficient Medical Visual Question Answering](https://arxiv.org/abs/2510.02328)
*Ziqing Wang,Chengsheng Mao,Xiaole Wen,Yuan Luo,Kaize Ding*

Main category: cs.CL

TL;DR: 本文提出了一种名为AMANDA的免训练Agent框架，通过LLM Agent进行医学知识增强，以解决医学多模态大语言模型（Med-MLLMs）在低资源环境下医学推理能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的Med-MLLMs在低资源环境下，由于忽略医学图像细节和缺乏专业医学知识，导致医学推理能力不足。

Method: 该方法通过LLM Agent进行医学知识增强，包括细粒度的医学问题分解和基于生物医学知识图谱检索的推理过程。

Result: 在八个Med-VQA基准测试中，零样本和小样本Med-VQA设置均取得了显著改进。

Conclusion: AMANDA框架能够有效提升Med-MLLMs在低资源环境下的医学推理能力。

Abstract: Medical Multimodal Large Language Models (Med-MLLMs) have shown great promise
in medical visual question answering (Med-VQA). However, when deployed in
low-resource settings where abundant labeled data are unavailable, existing
Med-MLLMs commonly fail due to their medical reasoning capability bottlenecks:
(i) the intrinsic reasoning bottleneck that ignores the details from the
medical image; (ii) the extrinsic reasoning bottleneck that fails to
incorporate specialized medical knowledge. To address those limitations, we
propose AMANDA, a training-free agentic framework that performs medical
knowledge augmentation via LLM agents. Specifically, our intrinsic medical
knowledge augmentation focuses on coarse-to-fine question decomposition for
comprehensive diagnosis, while extrinsic medical knowledge augmentation grounds
the reasoning process via biomedical knowledge graph retrieval. Extensive
experiments across eight Med-VQA benchmarks demonstrate substantial
improvements in both zero-shot and few-shot Med-VQA settings. The code is
available at https://github.com/REAL-Lab-NU/AMANDA.

</details>


### [5] [SelfJudge: Faster Speculative Decoding via Self-Supervised Judge Verification](https://arxiv.org/abs/2510.02329)
*Kanghoon Yoon,Minsub Kim,Sungjae Lee,Joonhyung Lee,Sunghyeon Woo,Yeonjun In,Se Jung Kwon,Chanyoung Park,Dongsoo Lee*

Main category: cs.CL

TL;DR: Speculative decoding accelerates LLM inference. SelfJudge trains judge verifiers via self-supervision of the target model, enabling automatic verifier training across diverse NLP tasks.


<details>
  <summary>Details</summary>
Motivation: Existing judge decoding methods are restricted by their reliance on human annotations or tasks with verifiable ground truths, limiting generalizability across diverse NLP tasks.

Method: Train judge verifiers via self-supervision of the target model. Measure semantic preservation by assessing whether token-substituted responses preserve the meaning of original responses.

Result: SelfJudge achieves superior inference-accuracy trade-offs than judge decoding baselines.

Conclusion: SelfJudge is a broadly applicable solution for faster LLM inference.

Abstract: Speculative decoding accelerates LLM inference by verifying candidate tokens
from a draft model against a larger target model. Recent judge decoding boosts
this process by relaxing verification criteria by accepting draft tokens that
may exhibit minor discrepancies from target model output, but existing methods
are restricted by their reliance on human annotations or tasks with verifiable
ground truths, limiting generalizability across diverse NLP tasks. We propose
SelfJudge, which trains judge verifiers via self-supervision of the target
model. Our method measures semantic preservation by assessing whether
token-substituted responses preserve the meaning of original responses,
enabling automatic verifier training across diverse NLP tasks. Our experiments
show SelfJudge achieves superior inference-accuracy trade-offs than judge
decoding baselines, offering a broadly applicable solution for faster LLM
inference.

</details>


### [6] [EntropyLong: Effective Long-Context Training via Predictive Uncertainty](https://arxiv.org/abs/2510.02330)
*Junlong Jia,Ziyang Chen,Xing Wu,Chaochen Gao,Zijia Lin,Debing Zhang,Songlin Hu,Binghui Guo*

Main category: cs.CL

TL;DR: 这篇论文提出了一种名为EntropyLong的新数据构建方法，用于训练长文本语言模型，以捕捉长距离依赖关系。


<details>
  <summary>Details</summary>
Motivation: 当前的数据构建方法无法保证真正的长距离依赖关系。

Method: 该方法利用预测不确定性来验证依赖质量，通过识别文档中的高熵位置，检索相关的上下文，并评估它们是否能减少预测熵。

Result: 使用EntropyLong方法训练的模型在RULER和LongBenchv2基准测试中表现出显著的改进。

Conclusion: 基于熵的验证对于长文本训练是必要且有效的。

Abstract: Training long-context language models to capture long-range dependencies
requires specialized data construction. Current approaches, such as generic
text concatenation or heuristic-based variants, frequently fail to guarantee
genuine long-range dependencies. We propose EntropyLong, a novel data
construction method that leverages predictive uncertainty to verify dependency
quality. Our approach identifies high-entropy positions in documents, retrieves
semantically relevant contexts from large corpora, and verifies their utility
by assessing whether they reduce prediction entropy. This model-in-the-loop
verification ensures each dependency represents measurable information gain
rather than spurious correlation. We construct training samples with long-range
dependencies by combining original documents with these verified contextual
supplements. Using FineWebEdu and Cosmopedia, we generate a dataset of
128K-length sequences with verified dependencies. Models trained on this data
demonstrate significant improvements on RULER benchmarks, particularly in tasks
requiring distant information. Following instruction fine-tuning, our models
also achieve substantial gains on LongBenchv2, demonstrating enhanced
long-context understanding. Extensive ablation studies further validate the
necessity and effectiveness of entropybased verification for long-context
training.

</details>


### [7] [Synthetic Dialogue Generation for Interactive Conversational Elicitation & Recommendation (ICER)](https://arxiv.org/abs/2510.02331)
*Moonkyung Ryu,Chih-Wei Hsu,Yinlam Chow,Mohammad Ghavamzadeh,Craig Boutilier*

Main category: cs.CL

TL;DR: 由于缺乏公共 CRS 数据，微调用于 CRS 的 LM 具有挑战性。因此，可以将 LM 用作数据生成器的用户模拟器来训练基于 LM 的 CRS，但通常缺乏行为一致性，从而生成与任何真实用户不一致的语句序列。为了解决这个问题，我们开发了一种方法，用于生成与用户使用行为模拟器和 LM 提示的底层状态一致的自然对话。


<details>
  <summary>Details</summary>
Motivation: 由于公共 CRS 数据的缺乏，微调用于 CRS 的 LM 具有挑战性。

Method: 使用行为模拟器和 LM 提示生成与用户底层状态一致的自然对话。

Result: 生成了一个大型开源 CRS 数据集，其中包含偏好启发和示例评论。评估者对其中一些对话的评估表明它们表现出相当大的一致性、事实性和自然性。

Conclusion: 所提出的方法可以生成高质量的 CRS 数据集。

Abstract: While language models (LMs) offer great potential for conversational
recommender systems (CRSs), the paucity of public CRS data makes fine-tuning
LMs for CRSs challenging. In response, LMs as user simulators qua data
generators can be used to train LM-based CRSs, but often lack behavioral
consistency, generating utterance sequences inconsistent with those of any real
user. To address this, we develop a methodology for generating natural
dialogues that are consistent with a user's underlying state using behavior
simulators together with LM-prompting. We illustrate our approach by generating
a large, open-source CRS data set with both preference elicitation and example
critiquing. Rater evaluation on some of these dialogues shows them to exhibit
considerable consistency, factuality and naturalness.

</details>


### [8] [A High-Capacity and Secure Disambiguation Algorithm for Neural Linguistic Steganography](https://arxiv.org/abs/2510.02332)
*Yapei Feng,Feng Jiang,Shanhao Wu,Hua Zhong*

Main category: cs.CL

TL;DR: 提出了一种名为look-ahead Sync的方法，克服了SyncPool容量限制，同时保持了其可证明的安全性保证。


<details>
  <summary>Details</summary>
Motivation: 现代分词器中的分词模糊性可能导致灾难性的解码失败。

Method: 仅对真正无法区分的token序列执行最小同步采样，同时有策略地保留所有其他可辨别的路径，以最大化嵌入容量。

Result: 在英语和中文基准测试中，该方法始终接近理论容量上限，并且明显优于SyncPool。嵌入率的提高在英语中超过 160%，在中文中超过 25%，尤其是在具有较大候选池的设置中。

Conclusion: 这项工作代表了迈向实际高容量、可证明安全的语言隐写术的重要一步。

Abstract: Neural linguistic steganography aims to embed information
  into natural text while preserving statistical undetectability. A fundamental
challenge in this ffeld stems from tokenization ambiguity in modern tokenizers,
which can lead to catastrophic decoding failures. The recent method, SyncPool,
addresses this ambiguity
  by employing a coarse-grained synchronization mechanism over groups of
ambiguous candidates. However, SyncPool sacriffces embedding capacity, as it
utilizes the entire Shannon entropy of an ambiguous group solely for
synchronization rather than for payload embedding. We propose a method named
look-ahead Sync, which overcomes the capacity limitation of SyncPool while
retaining its provable security guarantees. Our approach performs minimal
synchronized sampling only on truly indistinguishable token sequences, while
strategically preserving all other discernible paths to maximize embedding
capacity. We provide theoretical proofs for the security of our method and
analyze the gap between its achievable embedding capacity and the theoretical
upper bound. Experiments on English (using Llama 3) and Chinese (using Qwen
2.5) benchmarks show that our method consistently approaches the theoretical
capacity upper bound and signiffcantly outperforms SyncPool. The improvement in
embedding rate exceeds 160% in English and 25% in Chinese, particularly in
settings with larger candidate pools. This work represents a signiffcant step
toward practical high-capacity provably secure linguistic steganography.

</details>


### [9] [Human Mobility Datasets Enriched With Contextual and Social Dimensions](https://arxiv.org/abs/2510.02333)
*Chiara Pugliese,Francesco Lettich,Guido Rocchietti,Chiara Renso,Fabio Pinelli*

Main category: cs.CL

TL;DR: 本文提出了两个公开可用的、包含语义信息的GPS轨迹数据集，以及构建它们的流程。


<details>
  <summary>Details</summary>
Motivation: 为了促进多模态和语义移动分析的研究。

Method: 该方法通过从OpenStreetMap获取GPS轨迹，并结合停靠点、移动、兴趣点（POI）、推断的交通方式和天气数据等上下文层来丰富轨迹数据。一个新颖的语义特征是包含由大型语言模型（LLM）生成的合成的、真实的社交媒体帖子。

Result: 构建了两个数据集，分别覆盖巴黎和纽约两个结构不同的城市。这些数据集以表格和资源描述框架（RDF）格式提供。

Conclusion: 该资源是第一个在可重用框架中结合真实世界运动、结构化语义增强、LLM生成的文本和语义网络兼容性的资源。

Abstract: In this resource paper, we present two publicly available datasets of
semantically enriched human trajectories, together with the pipeline to build
them. The trajectories are publicly available GPS traces retrieved from
OpenStreetMap. Each dataset includes contextual layers such as stops, moves,
points of interest (POIs), inferred transportation modes, and weather data. A
novel semantic feature is the inclusion of synthetic, realistic social media
posts generated by Large Language Models (LLMs), enabling multimodal and
semantic mobility analysis. The datasets are available in both tabular and
Resource Description Framework (RDF) formats, supporting semantic reasoning and
FAIR data practices. They cover two structurally distinct, large cities: Paris
and New York. Our open source reproducible pipeline allows for dataset
customization, while the datasets support research tasks such as behavior
modeling, mobility prediction, knowledge graph construction, and LLM-based
applications. To our knowledge, our resource is the first to combine real-world
movement, structured semantic enrichment, LLM-generated text, and semantic web
compatibility in a reusable framework.

</details>


### [10] [Hierarchical Semantic Retrieval with Cobweb](https://arxiv.org/abs/2510.02539)
*Anant Gupta,Karthik Singaravadivelan,Zekun Wang*

Main category: cs.CL

TL;DR: 本文提出了一种新的神经文档检索方法，该方法利用 Cobweb 框架将句子嵌入组织成原型树，并通过粗到细的遍历对文档进行排序。该方法在保持与点积搜索相当的有效性的同时，提高了对嵌入质量的鲁棒性，并提供了可解释的检索。


<details>
  <summary>Details</summary>
Motivation: 现有的神经文档检索方法通常将语料库视为扁平的向量云，忽略了语料库的结构，并且缺乏透明的解释性。

Method: 本文使用 Cobweb 框架将句子嵌入组织成原型树，并通过粗到细的遍历对文档进行排序。内部节点充当概念原型，提供多粒度的相关性信号和透明的基本原理。

Result: 本文在 MS MARCO 和 QQP 数据集上评估了该方法，结果表明该方法在强编码器嵌入上与点积搜索相匹配，并且在 kNN 性能下降时保持鲁棒性。特别是，在 GPT-2 向量上，点积性能崩溃，而本文的方法仍然可以检索到相关的结果。

Conclusion: 本文的实验表明，Cobweb 框架提供了有竞争力的有效性、对嵌入质量的改进的鲁棒性、可扩展性，以及通过分层原型实现的可解释的检索。

Abstract: Neural document retrieval often treats a corpus as a flat cloud of vectors
scored at a single granularity, leaving corpus structure underused and
explanations opaque. We use Cobweb--a hierarchy-aware framework--to organize
sentence embeddings into a prototype tree and rank documents via coarse-to-fine
traversal. Internal nodes act as concept prototypes, providing multi-granular
relevance signals and a transparent rationale through retrieval paths. We
instantiate two inference approaches: a generalized best-first search and a
lightweight path-sum ranker. We evaluate our approaches on MS MARCO and QQP
with encoder (e.g., BERT/T5) and decoder (GPT-2) representations. Our results
show that our retrieval approaches match the dot product search on strong
encoder embeddings while remaining robust when kNN degrades: with GPT-2
vectors, dot product performance collapses whereas our approaches still
retrieve relevant results. Overall, our experiments suggest that Cobweb
provides competitive effectiveness, improved robustness to embedding quality,
scalability, and interpretable retrieval via hierarchical prototypes.

</details>


### [11] [Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation Gradient Tracing](https://arxiv.org/abs/2510.02334)
*Zhe Li,Wei Zhao,Yige Li,Jun Sun*

Main category: cs.CL

TL;DR: 提出了一种新的诊断框架，通过分析表征及其梯度来诊断大型语言模型（LLM）的不良行为。


<details>
  <summary>Details</summary>
Motivation: 现有归因方法由于噪声信号和计算复杂性而存在不足，诊断LLM不良行为的根本原因是一个关键挑战。

Method: 该框架直接在模型的激活空间中操作，以提供语义上有意义的信号，将输出与其训练数据联系起来。

Result: 该方法在跟踪有害内容、检测后门中毒和识别知识污染等任务中表现出色，能够进行样本级别和细粒度的token级别分析。

Conclusion: 该工作提供了一个强大的诊断工具，以理解、审计和最终减轻与LLM相关的风险。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities, yet
their deployment is frequently undermined by undesirable behaviors such as
generating harmful content, factual inaccuracies, and societal biases.
Diagnosing the root causes of these failures poses a critical challenge for AI
safety. Existing attribution methods, particularly those based on parameter
gradients, often fall short due to prohibitive noisy signals and computational
complexity. In this work, we introduce a novel and efficient framework that
diagnoses a range of undesirable LLM behaviors by analyzing representation and
its gradients, which operates directly in the model's activation space to
provide a semantically meaningful signal linking outputs to their training
data. We systematically evaluate our method for tasks that include tracking
harmful content, detecting backdoor poisoning, and identifying knowledge
contamination. The results demonstrate that our approach not only excels at
sample-level attribution but also enables fine-grained token-level analysis,
precisely identifying the specific samples and phrases that causally influence
model behavior. This work provides a powerful diagnostic tool to understand,
audit, and ultimately mitigate the risks associated with LLMs. The code is
available at https://github.com/plumprc/RepT.

</details>


### [12] [FormalML: A Benchmark for Evaluating Formal Subgoal Completion in Machine Learning Theory](https://arxiv.org/abs/2510.02335)
*Xiao-Wen Yang,Zihao Zhang,Jianuo Cao,Zhi Zhou,Zenan Li,Lan-Zhe Guo,Yuan Yao,Taolue Chen,Yu-Feng Li,Xiaoxing Ma*

Main category: cs.CL

TL;DR: 大型语言模型在定理证明方面取得了显著进展，但它们在复杂证明中填补缺失步骤的能力仍有待探索。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在定理证明中作为实用助手的能力，特别是在子目标完成任务中的应用。

Method: 提出了FormalML，一个基于Lean 4的基准，用于评估模型在子目标完成任务中的性能。该基准包含从机器学习基础理论中提取的4937个问题，涵盖优化和概率不等式。

Result: 对现有最佳定理证明器的评估表明，在准确性和效率方面仍存在局限性。

Conclusion: 需要更强大的基于LLM的定理证明器，以实现有效的子目标完成。

Abstract: Large language models (LLMs) have recently demonstrated remarkable progress
in formal theorem proving. Yet their ability to serve as practical assistants
for mathematicians, filling in missing steps within complex proofs, remains
underexplored. We identify this challenge as the task of subgoal completion,
where an LLM must discharge short but nontrivial proof obligations left
unresolved in a human-provided sketch. To study this problem, we introduce
FormalML, a Lean 4 benchmark built from foundational theories of machine
learning. Using a translation tactic that converts procedural proofs into
declarative form, we extract 4937 problems spanning optimization and
probability inequalities, with varying levels of difficulty. FormalML is the
first subgoal completion benchmark to combine premise retrieval and complex
research-level contexts. Evaluation of state-of-the-art provers highlights
persistent limitations in accuracy and efficiency, underscoring the need for
more capable LLM-based theorem provers for effective subgoal completion,

</details>


### [13] [KurdSTS: The Kurdish Semantic Textual Similarity](https://arxiv.org/abs/2510.02336)
*Abdulhady Abas Abdullah,Hadi Veisi,Hussein M. Al*

Main category: cs.CL

TL;DR: 创建了首个库尔德语语义文本相似度（STS）数据集，包含10,000个句子对，并为相似性进行了注释。


<details>
  <summary>Details</summary>
Motivation: 缺乏库尔德语等低资源语言的语义文本相似度资源。

Method: 创建数据集，并使用Sentence-BERT、多语言BERT等模型进行基准测试。

Result: 获得了有竞争力的结果，并强调了库尔德语形态、拼写变异和代码混合带来的挑战。

Conclusion: 该数据集和基准测试建立了一个可复现的评估套件，并为未来库尔德语语义和低资源NLP研究提供了一个强大的起点。

Abstract: Semantic Textual Similarity (STS) measures the degree of meaning overlap
between two texts and underpins many NLP tasks. While extensive resources exist
for high-resource languages, low-resource languages such as Kurdish remain
underserved. We present, to our knowledge, the first Kurdish STS dataset:
10,000 sentence pairs spanning formal and informal registers, each annotated
for similarity. We benchmark Sentence-BERT, multilingual BERT, and other strong
baselines, obtaining competitive results while highlighting challenges arising
from Kurdish morphology, orthographic variation, and code-mixing. The dataset
and baselines establish a reproducible evaluation suite and provide a strong
starting point for future research on Kurdish semantics and low-resource NLP.

</details>


### [14] [StepChain GraphRAG: Reasoning Over Knowledge Graphs for Multi-Hop Question Answering](https://arxiv.org/abs/2510.02827)
*Tengjun Ni,Xin Yuan,Shenghong Li,Kai Wu,Ren Ping Liu,Wei Ni,Wenjie Zhang*

Main category: cs.CL

TL;DR: StepChain GraphRAG框架通过结合问题分解和广度优先搜索推理流程，增强了多跳问答能力，在MuSiQue、2WikiMultiHopQA和HotpotQA数据集上取得了state-of-the-art的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成（RAG）方法在整合迭代推理步骤和外部知识检索方面仍然面临挑战。

Method: 该方法首先构建一个全局语料库索引；在推理时，只将检索到的段落即时解析成知识图谱，并将复杂查询分解成子问题。对于每个子问题，基于BFS的遍历动态地沿着相关边扩展，组装显式证据链。

Result: 在MuSiQue、2WikiMultiHopQA和HotpotQA数据集上的实验表明，StepChain GraphRAG实现了state-of-the-art的Exact Match和F1分数。StepChain GraphRAG的平均EM提升了2.57%，F1提升了2.13%，在HotpotQA上获得了最大的收益（+4.70% EM，+3.44% F1）。

Conclusion: 未来的工作可以缓解计算开销，并解决大型语言模型中潜在的幻觉问题，以提高多跳QA的效率和可靠性。

Abstract: Recent progress in retrieval-augmented generation (RAG) has led to more
accurate and interpretable multi-hop question answering (QA). Yet, challenges
persist in integrating iterative reasoning steps with external knowledge
retrieval. To address this, we introduce StepChain GraphRAG, a framework that
unites question decomposition with a Breadth-First Search (BFS) Reasoning Flow
for enhanced multi-hop QA. Our approach first builds a global index over the
corpus; at inference time, only retrieved passages are parsed on-the-fly into a
knowledge graph, and the complex query is split into sub-questions. For each
sub-question, a BFS-based traversal dynamically expands along relevant edges,
assembling explicit evidence chains without overwhelming the language model
with superfluous context. Experiments on MuSiQue, 2WikiMultiHopQA, and HotpotQA
show that StepChain GraphRAG achieves state-of-the-art Exact Match and F1
scores. StepChain GraphRAG lifts average EM by 2.57% and F1 by 2.13% over the
SOTA method, achieving the largest gain on HotpotQA (+4.70% EM, +3.44% F1).
StepChain GraphRAG also fosters enhanced explainability by preserving the
chain-of-thought across intermediate retrieval steps. We conclude by discussing
how future work can mitigate the computational overhead and address potential
hallucinations from large language models to refine efficiency and reliability
in multi-hop QA.

</details>


### [15] [CRACQ: A Multi-Dimensional Approach To Automated Document Assessment](https://arxiv.org/abs/2510.02337)
*Ishak Soltani,Francisco Belo,Bernardo Tavares*

Main category: cs.CL

TL;DR: CRACQ is a multi-dimensional framework for evaluating machine-generated text across five traits: coherence, rigor, appropriateness, completeness, and quality.


<details>
  <summary>Details</summary>
Motivation: The paper aims to provide a more detailed and interpretable evaluation of machine-generated text than single-score approaches or direct LLM evaluation.

Method: CRACQ integrates linguistic, semantic, and structural signals into a cumulative assessment, trained on 500 synthetic grant proposals and benchmarked against an LLM-as-a-judge.

Result: CRACQ produces more stable and interpretable trait-level judgments than direct LLM evaluation, but faces challenges in reliability and domain scope.

Conclusion: CRACQ offers a promising approach to multi-dimensional text evaluation, with potential for improvement in reliability and broader applicability.

Abstract: This paper presents CRACQ, a multi-dimensional evaluation framework tailored
to evaluate documents across f i v e specific traits: Coherence, Rigor,
Appropriateness, Completeness, and Quality. Building on insights from
traitbased Automated Essay Scoring (AES), CRACQ expands its fo-cus beyond
essays to encompass diverse forms of machine-generated text, providing a
rubricdriven and interpretable methodology for automated evaluation. Unlike
singlescore approaches, CRACQ integrates linguistic, semantic, and structural
signals into a cumulative assessment, enabling both holistic and trait-level
analysis. Trained on 500 synthetic grant pro-posals, CRACQ was benchmarked
against an LLM-as-a-judge and further tested on both strong and weak real
applications. Preliminary results in-dicate that CRACQ produces more stable and
interpretable trait-level judgments than direct LLM evaluation, though
challenges in reliability and domain scope remain

</details>


### [16] [Grounding Large Language Models in Clinical Evidence: A Retrieval-Augmented Generation System for Querying UK NICE Clinical Guidelines](https://arxiv.org/abs/2510.02967)
*Matthew Lewis,Samuel Thio,Richard JB Dobson,Spiros Denaxas*

Main category: cs.CL

TL;DR: 本研究开发并评估了一个检索增强生成（RAG）系统，用于查询英国国家健康与护理卓越研究院（NICE）的临床指南。


<details>
  <summary>Details</summary>
Motivation: 在时间受限的医疗保健系统中，指南内容过多，不易快速找到所需信息。

Method: 该系统采用混合嵌入机制的检索架构，并针对来自300条指南的10,195个文本块的数据库进行了评估。

Result: 该系统表现出高性能，在7901个查询中，平均倒数排名（MRR）为0.814，首次检索块的召回率为81％，前十个检索块内的召回率为99.1％。在生成阶段，RAG增强模型在70个问题-答案对的手动管理数据集上进行了评估，结果表明性能显着提高。RAG增强的O4-Mini模型的忠实度提高了64.7个百分点，达到99.5％，并且大大优于医学重点的Meditron3-8B LLM，后者得分为43％。所有RAG增强模型的上下文精确度得分均为1，这证实了该系统通过将其答案基于相关源材料来防止信息编造的能力。

Conclusion: 本研究证实了RAG是医疗保健中应用生成AI的一种有效，可靠且可扩展的方法，从而可以经济高效地访问医疗指南。

Abstract: This paper presents the development and evaluation of a Retrieval-Augmented
Generation (RAG) system for querying the United Kingdom's National Institute
for Health and Care Excellence (NICE) clinical guidelines using Large Language
Models (LLMs). The extensive length and volume of these guidelines can impede
their utilisation within a time-constrained healthcare system, a challenge this
project addresses through the creation of a system capable of providing users
with precisely matched information in response to natural language queries. The
system's retrieval architecture, composed of a hybrid embedding mechanism, was
evaluated against a database of 10,195 text chunks derived from three hundred
guidelines. It demonstrates high performance, with a Mean Reciprocal Rank (MRR)
of 0.814, a Recall of 81% at the first chunk and of 99.1% within the top ten
retrieved chunks, when evaluated on 7901 queries.
  The most significant impact of the RAG system was observed during the
generation phase. When evaluated on a manually curated dataset of seventy
question-answer pairs, RAG-enhanced models showed substantial gains in
performance. Faithfulness, the measure of whether an answer is supported by the
source text, was increased by 64.7 percentage points to 99.5% for the
RAG-enhanced O4-Mini model and significantly outperformed the medical-focused
Meditron3-8B LLM, which scored 43%. This, combined with a perfect Context
Precision score of 1 for all RAG-enhanced models, confirms the system's ability
to prevent information fabrication by grounding its answers in relevant source
material. This study thus establishes RAG as an effective, reliable, and
scalable approach for applying generative AI in healthcare, enabling
cost-effective access to medical guidelines.

</details>


### [17] [Optimizing Long-Form Clinical Text Generation with Claim-Based Rewards](https://arxiv.org/abs/2510.02338)
*Samyak Jhaveri,Praphul Singh,Jangwon Kim,Tara Taghavi,Krishnaram Kenthapadi*

Main category: cs.CL

TL;DR: 本研究提出了一种评估整合强化学习框架，用于生成长篇临床文本，该框架将 Group Relative Policy Optimization (GRPO) 与 DocLens 相结合，DocLens 是一种声明级别的评估器，可提供确定性的、基于对话的奖励。


<details>
  <summary>Details</summary>
Motivation: 利用大型语言模型自动生成临床文档需要与完整性和事实基础等优先级精确对齐。

Method: 该方法直接优化事实基础和完整性，无需训练单独的奖励模型或依赖人工编写的参考文献。

Result: 实验结果表明，该方法提高了临床笔记的质量，并通过简单的奖励门控策略降低了训练成本。独立的 GPT-5 定性评估进一步支持了这些优势，表明 GRPO 输出在事实性、完整性和简洁性方面更受欢迎，遗漏和幻觉更少。

Conclusion: 该框架可扩展到实际应用，并可纳入自定义目标，如指南遵守情况或计费偏好。

Abstract: Automating clinical documentation with large language models requires precise
alignment with priorities such as completeness and factual grounding. We
present an evaluation-integrated reinforcement learning framework for long-form
clinical text generation that couples Group Relative Policy Optimization (GRPO)
with DocLens, a claim-level evaluator that provides deterministic,
dialogue-grounded rewards. Our method directly optimizes factual grounding and
completeness without training a separate reward model or relying on
human-authored references. Empirically, the approach improves clinical note
quality and reduces training cost via a simple reward-gating strategy. An
independent GPT-5 qualitative evaluation further supports these gains, showing
higher preference for GRPO outputs in factuality, completeness, and brevity,
with fewer omissions and hallucinations. Because the benchmarks are relatively
clean and the base model already well aligned, these improvements likely
represent a conservative lower bound. The framework is scalable to real-world
settings and can incorporate custom objectives such as guideline adherence or
billing preferences.

</details>


### [18] [Evaluating Uncertainty Quantification Methods in Argumentative Large Language Models](https://arxiv.org/abs/2510.02339)
*Kevin Zhou,Adam Dejl,Gabriel Freedman,Lihu Chen,Antonio Rago,Francesca Toni*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型(llm)不确定性量化(UQ)方法在论证型llm (ArgLLMs)中的应用，ArgLLMs是一种基于计算论证的可解释决策llm框架。


<details>
  <summary>Details</summary>
Motivation: 保证大型语言模型的可靠性，尤其是在复杂的论证场景中。

Method: 通过实验评估ArgLLMs在不同LLM UQ方法下的性能，完成claim验证任务。

Result: 直接提示是一种有效的UQ策略，优于更复杂的方法。

Conclusion: 直接提示在ArgLLMs中是一种简单而有效的不确定性量化方法。

Abstract: Research in uncertainty quantification (UQ) for large language models (LLMs)
is increasingly important towards guaranteeing the reliability of this
groundbreaking technology. We explore the integration of LLM UQ methods in
argumentative LLMs (ArgLLMs), an explainable LLM framework for decision-making
based on computational argumentation in which UQ plays a critical role. We
conduct experiments to evaluate ArgLLMs' performance on claim verification
tasks when using different LLM UQ methods, inherently performing an assessment
of the UQ methods' effectiveness. Moreover, the experimental procedure itself
is a novel way of evaluating the effectiveness of UQ methods, especially when
intricate and potentially contentious statements are present. Our results
demonstrate that, despite its simplicity, direct prompting is an effective UQ
strategy in ArgLLMs, outperforming considerably more complex approaches.

</details>


### [19] [Can Prompts Rewind Time for LLMs? Evaluating the Effectiveness of Prompted Knowledge Cutoffs](https://arxiv.org/abs/2510.02340)
*Xin Gao,Ruiyi Zhang,Daniel Du,Saurabh Mahindre,Sai Ashish Somayajula,Pengtao Xie*

Main category: cs.CL

TL;DR: 大型语言模型依赖于预训练数据，这引发了数据污染问题。本文研究了提示方法在模拟LLM较早知识截止方面的能力。结果表明，基于提示的模拟知识截止在直接查询截止日期后的信息时有效，但当忘记的内容与查询存在因果关系时，它们难以诱导遗忘。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型被广泛用于时间预测，但它们对预训练数据的依赖引起了人们对数据污染的担忧，因为对截止测试数据的准确预测可能反映了记忆而不是推理，从而导致对其泛化能力的过度估计。

Method: 本文构建了三个评估数据集，以评估LLM在多大程度上可以忘记（1）直接的事实知识，（2）语义转变，以及（3）因果相关的知识。

Result: 结果表明，当直接查询截止日期后的信息时，基于提示的模拟知识截止显示出有效性，但是当忘记的内容不是直接询问而是与查询存在因果关系时，它们难以诱导遗忘。

Conclusion: 这些发现强调了在将LLM应用于时间预测任务时，需要更严格的评估设置。

Abstract: Large Language Models (LLMs) are widely used for temporal prediction, but
their reliance on pretraining data raises contamination concerns, as accurate
predictions on pre-cutoff test data may reflect memorization rather than
reasoning, leading to an overestimation of their generalization capability.
With the recent emergence of prompting-based unlearning techniques, a natural
question arises: Can LLMs be prompted to simulate an earlier knowledge cutoff?
In this work, we investigate the capability of prompting to simulate earlier
knowledge cutoff in LLMs. We construct three evaluation datasets to assess the
extent to which LLMs can forget (1) direct factual knowledge, (2) semantic
shifts, and (3) causally related knowledge. Results demonstrate that while
prompt-based simulated knowledge cutoffs show effectiveness when directly
queried with the information after that date, they struggle to induce
forgetting when the forgotten content is not directly asked but causally
related to the query. These findings highlight the need for more rigorous
evaluation settings when applying LLMs for temporal prediction tasks. The full
dataset and evaluation code are available at
https://github.com/gxx27/time_unlearn.

</details>


### [20] [DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning](https://arxiv.org/abs/2510.02341)
*Yifan Wang,Bolian Li,Junlin Wu,Zhaoxuan Tan,Zheli Liu,Ruqi Zhang,Ananth Grama,Qingkai Zeng*

Main category: cs.CL

TL;DR: DRIFT利用真实世界中的用户不满意信号进行训练，并动态采样积极信号，以提高大型语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有偏好学习方法依赖昂贵的人工标注或假设存在大量积极反馈，与实际情况不符，即用户不满意信号丰富但显式满意反馈稀缺。

Method: 提出DRIFT方法，它基于不满意信号进行训练，并从不断演变的策略中动态采样积极样本。

Result: 在WildBench任务得分和AlpacaEval2胜率上，DRIFT优于基线模型，且在更大规模的模型上改进更明显，甚至超越了GPT-4o-mini。DRIFT还能保持探索能力，产生更多样化的高回报解决方案。

Conclusion: DRIFT是一种有效的、可扩展的现实世界后训练方法，它利用了最丰富和信息量最大的信号。

Abstract: Real-world large language model deployments (e.g., conversational AI systems,
code generation assistants) naturally generate abundant implicit user
dissatisfaction (DSAT) signals, as users iterate toward better answers through
refinements, corrections, and expressed preferences, while explicit
satisfaction (SAT) feedback is scarce. Existing preference learning approaches
are poorly aligned with this data profile, as they rely on costly human
annotations or assume plentiful positive responses. In this paper, we introduce
\textbf{DRIFT} (\textbf{D}issatisfaction-\textbf{R}efined \textbf{I}terative
pre\textbf{F}erence \textbf{T}raining), which anchors training on real-world
DSAT signals and samples positives dynamically from the evolving policy.
Empirically, DRIFT models trained on real-world \textit{WildFeedback} datasets
and synthetic \textit{UltraFeedback} datasets achieve up to +6.23\% (7B) /
+7.61\% (14B) on WildBench Task Score and up to +8.95\% (7B) / +12.29\% (14B)
on AlpacaEval2 win rate over base models, outperforming strong baseline methods
such as iterative DPO and SPIN. At larger scales, the improvements are
particularly pronounced: 14B models trained with DRIFT surpass GPT-4o-mini on
WildBench. Further analysis shows that DRIFT also preserves exploratory
capacity, yielding more diverse high-reward solutions rather than collapsing to
narrow subsets. Theoretically, we demonstrate that this design preserves
preference margins and avoids the gradient degeneration. These results show
that DRIFT is an effective and scalable recipe for real-world post-training
that leverages the most abundant and informative signal. The code and data are
available at https://github.com/cacayaya/DRIFT.git.

</details>


### [21] [$\texttt{BluePrint}$: A Social Media User Dataset for LLM Persona Evaluation and Training](https://arxiv.org/abs/2510.02343)
*Aurélien Bück-Kaeffer,Je Qin Chooi,Dan Zhao,Maximilian Puelma Touzel,Kellin Pelrine,Jean-François Godbout,Reihaneh Rabbany,Zachary Yang*

Main category: cs.CL

TL;DR: 提出SIMPACT框架，用于构建行为导向的社交媒体数据集，以训练和评估LLM社交媒体代理。


<details>
  <summary>Details</summary>
Motivation: 缺乏标准化的数据资源来微调和评估LLM作为真实的社交媒体代理。

Method: 将下一个动作预测公式化为训练和评估LLM代理的任务，并引入聚类和人口层面的指标来评估行为保真度和风格真实性。

Result: 发布了BluePrint数据集，该数据集来自公共Bluesky数据，专注于政治讨论。BluePrint将匿名用户聚集成聚合行为的角色，在保护隐私的同时捕捉真实的参与模式。

Conclusion: SIMPACT通过标准化数据和评估协议，为推进严谨、符合伦理的社交媒体模拟奠定了基础。BluePrint可以作为政治话语建模的评估基准，也可以作为构建特定领域数据集以研究错误信息和两极分化的模板。

Abstract: Large language models (LLMs) offer promising capabilities for simulating
social media dynamics at scale, enabling studies that would be ethically or
logistically challenging with human subjects. However, the field lacks
standardized data resources for fine-tuning and evaluating LLMs as realistic
social media agents. We address this gap by introducing SIMPACT, the
SIMulation-oriented Persona and Action Capture Toolkit, a privacy respecting
framework for constructing behaviorally-grounded social media datasets suitable
for training agent models. We formulate next-action prediction as a task for
training and evaluating LLM-based agents and introduce metrics at both the
cluster and population levels to assess behavioral fidelity and stylistic
realism. As a concrete implementation, we release BluePrint, a large-scale
dataset built from public Bluesky data focused on political discourse.
BluePrint clusters anonymized users into personas of aggregated behaviours,
capturing authentic engagement patterns while safeguarding privacy through
pseudonymization and removal of personally identifiable information. The
dataset includes a sizable action set of 12 social media interaction types
(likes, replies, reposts, etc.), each instance tied to the posting activity
preceding it. This supports the development of agents that use
context-dependence, not only in the language, but also in the interaction
behaviours of social media to model social media users. By standardizing data
and evaluation protocols, SIMPACT provides a foundation for advancing rigorous,
ethically responsible social media simulations. BluePrint serves as both an
evaluation benchmark for political discourse modeling and a template for
building domain specific datasets to study challenges such as misinformation
and polarization.

</details>


### [22] [Breaking the MoE LLM Trilemma: Dynamic Expert Clustering with Structured Compression](https://arxiv.org/abs/2510.02345)
*Peijun Zhu,Ning Yang,Jiayu Wei,Jinghang Wu,Haijun Zhang*

Main category: cs.CL

TL;DR: 提出了一种统一的框架，通过动态专家聚类和结构化压缩来解决MoE LLM中的负载不平衡、参数冗余和通信开销问题。


<details>
  <summary>Details</summary>
Motivation: MoE LLM面临负载不平衡、参数冗余和通信开销的三难问题。

Method: 采用在线聚类程序，定期使用参数和激活相似性的融合指标对专家进行重新分组，从而稳定专家利用率。在每个集群中，将专家权重分解为共享的基础矩阵和极低秩残差适配器。异构精度方案，将共享库存储在FP16中，将残差因子存储在INT4中，再加上非活动集群的动态卸载，从而将峰值内存消耗降低到与密集模型相当的水平。

Result: 在GLUE和WikiText-103上进行评估，该框架在匹配标准MoE模型质量的同时，将总参数减少了约80%，吞吐量提高了10%到20%，并将专家负载方差降低了三倍以上。

Conclusion: 结构重组是实现可扩展、高效和内存高效的MoE LLM的一条原则性途径。

Abstract: Mixture-of-Experts (MoE) Large Language Models (LLMs) face a trilemma of load
imbalance, parameter redundancy, and communication overhead. We introduce a
unified framework based on dynamic expert clustering and structured compression
to address these issues cohesively. Our method employs an online clustering
procedure that periodically regroups experts using a fused metric of parameter
and activation similarity, which stabilizes expert utilization. To our
knowledge, this is one of the first frameworks to leverage the semantic
embedding capability of the router to dynamically reconfigure the model's
architecture during training for substantial efficiency gains. Within each
cluster, we decompose expert weights into a shared base matrix and extremely
low-rank residual adapters, achieving up to fivefold parameter reduction per
group while preserving specialization. This structure enables a two-stage
hierarchical routing strategy: tokens are first assigned to a cluster, then to
specific experts within it, drastically reducing the routing search space and
the volume of all-to-all communication. Furthermore, a heterogeneous precision
scheme, which stores shared bases in FP16 and residual factors in INT4, coupled
with dynamic offloading of inactive clusters, reduces peak memory consumption
to levels comparable to dense models. Evaluated on GLUE and WikiText-103, our
framework matches the quality of standard MoE models while reducing total
parameters by approximately 80%, improving throughput by 10% to 20%, and
lowering expert load variance by a factor of over three. Our work demonstrates
that structural reorganization is a principled path toward scalable, efficient,
and memory-effective MoE LLMs.

</details>


### [23] [Small Language Models for Curriculum-based Guidance](https://arxiv.org/abs/2510.02347)
*Konstantinos Katharakis,Sippo Rossi,Raghava Rao Mukkamala*

Main category: cs.CL

TL;DR: 本研究探索了使用检索增强生成 (RAG) 管道和小型语言模型 (SLM) 开发 AI 教学助手，并评估了其性能。


<details>
  <summary>Details</summary>
Motivation: 在教育领域采用生成式 AI 和大型语言模型 (LLM) 尚处于新兴阶段。

Method: 使用检索增强生成 (RAG) 管道应用于选定的开源小型语言模型 (SLM)。对八个 SLM（包括 LLaMA 3.1、IBM Granite 3.3 和 Gemma 3（7-17B 参数））与 GPT-4o 进行了基准测试。

Result: 研究结果表明，通过适当的提示和有针对性的检索，SLM 在提供准确且符合教学法的响应方面可以与 LLM 相媲美。SLM 具有显着的可持续性优势，因为它们对计算和能源的要求较低，可以在消费级硬件上实时使用，而无需依赖云基础设施。

Conclusion: SLM 不仅具有成本效益和隐私保护能力，而且对环境负责，使其成为旨在以可持续和节能的方式扩展个性化学习的教育机构的可行 AI 教学助手。

Abstract: The adoption of generative AI and large language models (LLMs) in education
is still emerging. In this study, we explore the development and evaluation of
AI teaching assistants that provide curriculum-based guidance using a
retrieval-augmented generation (RAG) pipeline applied to selected open-source
small language models (SLMs). We benchmarked eight SLMs, including LLaMA 3.1,
IBM Granite 3.3, and Gemma 3 (7-17B parameters), against GPT-4o. Our findings
show that with proper prompting and targeted retrieval, SLMs can match LLMs in
delivering accurate, pedagogically aligned responses. Importantly, SLMs offer
significant sustainability benefits due to their lower computational and energy
requirements, enabling real-time use on consumer-grade hardware without
depending on cloud infrastructure. This makes them not only cost-effective and
privacy-preserving but also environmentally responsible, positioning them as
viable AI teaching assistants for educational institutions aiming to scale
personalized learning in a sustainable and energy-efficient manner.

</details>


### [24] [mini-vec2vec: Scaling Universal Geometry Alignment with Linear Transformations](https://arxiv.org/abs/2510.02348)
*Guy Dar*

Main category: cs.CL

TL;DR: 提出了一种更有效、更稳定的 vec2vec 替代方案，称为 mini-vec2vec，用于对齐文本嵌入空间。


<details>
  <summary>Details</summary>
Motivation: 原始的 vec2vec 方法虽然对齐效果好，但计算成本高且不稳定。

Method: mini-vec2vec 包括三个阶段：伪并行嵌入向量的初步匹配、变换拟合和迭代细化，学习到的映射是线性变换。

Result: mini-vec2vec 在效率上比原始 vec2vec 提高几个数量级，同时达到或超过其结果。

Conclusion: mini-vec2vec 的稳定性和可解释的算法步骤有助于扩展，并为在新领域和新领域的采用释放新的机会。

Abstract: We build upon vec2vec, a procedure designed to align text embedding spaces
without parallel data. vec2vec finds a near-perfect alignment, but it is
expensive and unstable. We present mini-vec2vec, a simple and efficient
alternative that requires substantially lower computational cost and is highly
robust. Moreover, the learned mapping is a linear transformation. Our method
consists of three main stages: a tentative matching of pseudo-parallel
embedding vectors, transformation fitting, and iterative refinement. Our linear
alternative exceeds the original instantiation of vec2vec by orders of
magnitude in efficiency, while matching or exceeding their results. The
method's stability and interpretable algorithmic steps facilitate scaling and
unlock new opportunities for adoption in new domains and fields.

</details>


### [25] [LLMSQL: Upgrading WikiSQL for the LLM Era of Text-to-SQL](https://arxiv.org/abs/2510.02350)
*Dzmitry Pihulski,Karol Charchut,Viktoria Novogrodskaia,Jan Kocoń*

Main category: cs.CL

TL;DR: LLMSQL: A revised WikiSQL dataset for LLMs, addressing structural and annotation issues.


<details>
  <summary>Details</summary>
Motivation: WikiSQL has issues like case sensitivity, data type mismatches, and unanswered questions, hindering NL2SQL research.

Method: Systematic revision and transformation of WikiSQL with automated cleaning and re-annotation.

Result: Evaluation of multiple large language models (LLMs) on the improved dataset.

Conclusion: LLMSQL is introduced as an LLM-ready benchmark with clean questions and full SQL queries, facilitating generation and evaluation for modern NL2SQL models.

Abstract: Converting natural language questions into SQL queries (Text-to-SQL) enables
non-expert users to interact with relational databases and has long been a
central task for natural language interfaces to data. While the WikiSQL dataset
played a key role in early NL2SQL research, its usage has declined due to
structural and annotation issues, including case sensitivity inconsistencies,
data type mismatches, syntax errors, and unanswered questions. We present
LLMSQL, a systematic revision and transformation of WikiSQL designed for the
LLM era. We classify these errors and implement automated methods for cleaning
and re-annotation. To assess the impact of these improvements, we evaluated
multiple large language models (LLMs), including Gemma 3, LLaMA 3.2, Mistral
7B, gpt-oss 20B, Phi-3.5 Mini, Qwen 2.5, OpenAI o4-mini, DeepSeek R1 and
others. Rather than serving as an update, LLMSQL is introduced as an LLM-ready
benchmark: unlike the original WikiSQL, tailored for pointer-network models
selecting tokens from input, LLMSQL provides clean natural language questions
and full SQL queries as plain text, enabling straightforward generation and
evaluation for modern natural language-to-SQL models.

</details>


### [26] [Language, Culture, and Ideology: Personalizing Offensiveness Detection in Political Tweets with Reasoning LLMs](https://arxiv.org/abs/2510.02351)
*Dzmitry Pihulski,Jan Kocoń*

Main category: cs.CL

TL;DR: 大型语言模型在评估政治言论中的冒犯性时，会因被赋予特定的政治和文化视角而产生差异。较大的模型在意识形态和文化差异方面表现得更为一致和敏感，而较小的模型则难以捕捉到细微的差别。推理能力显著提高了冒犯性判断的个性化和可解释性。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLMs）如何在被提示采用特定的政治和文化视角时，评估政治言论中的冒犯性。

Method: 使用MD-Agreement数据集的多语言子集，评估了包括DeepSeek-R1、o4-mini、GPT-4.1-mini、Qwen3、Gemma和Mistral在内的多个最新LLMs，任务是从不同政治人物（极右、保守、中间派、进步派）的视角判断推文是否具有冒犯性，涵盖英语、波兰语和俄语语境。

Result: 较大的模型（如DeepSeek-R1、o4-mini）在意识形态和文化变异方面更加一致和敏感，而较小的模型通常无法捕捉到细微的差别。推理能力显著提高了冒犯性判断的个性化和可解释性。

Conclusion: 推理机制是使LLMs适应跨语言和意识形态的细致社会政治文本分类的关键。

Abstract: We explore how large language models (LLMs) assess offensiveness in political
discourse when prompted to adopt specific political and cultural perspectives.
Using a multilingual subset of the MD-Agreement dataset centered on tweets from
the 2020 US elections, we evaluate several recent LLMs - including DeepSeek-R1,
o4-mini, GPT-4.1-mini, Qwen3, Gemma, and Mistral - tasked with judging tweets
as offensive or non-offensive from the viewpoints of varied political personas
(far-right, conservative, centrist, progressive) across English, Polish, and
Russian contexts. Our results show that larger models with explicit reasoning
abilities (e.g., DeepSeek-R1, o4-mini) are more consistent and sensitive to
ideological and cultural variation, while smaller models often fail to capture
subtle distinctions. We find that reasoning capabilities significantly improve
both the personalization and interpretability of offensiveness judgments,
suggesting that such mechanisms are key to adapting LLMs for nuanced
sociopolitical text classification across languages and ideologies.

</details>


### [27] [Evaluating Bias in Spoken Dialogue LLMs for Real-World Decisions and Recommendations](https://arxiv.org/abs/2510.02352)
*Yihao Wu,Tianrui Wang,Yizhou Peng,Yi-Wen Chao,Xuyi Zhuang,Xinsheng Wang,Shunshun Yin,Ziyang Ma*

Main category: cs.CL

TL;DR: 本研究是首次针对端到端语音对话模型中偏见的系统性研究，揭示了不同模型在决策和推荐任务中存在的偏见，并发布了数据集和评估代码。


<details>
  <summary>Details</summary>
Motivation: 现有研究较少关注语音对话模型（SDM）中由语音输入和输出引起的偏见，特别是当副语言特征（如年龄、性别和口音）与多轮对话结合时，可能加剧偏见，影响决策和推荐任务的公平性。

Method: 使用Group Unfairness Score (GUS)和similarity-based normalized statistics rate (SNSR)指标，系统评估了包括Qwen2.5-Omni、GLM-4-Voice、GPT-4o Audio和Gemini-2.5-Flash在内的多种开源和闭源语音LLM的偏见，并研究了多轮对话和重复负反馈的影响。

Result: 研究发现闭源模型的偏见通常较低，而开源模型对年龄和性别更敏感，推荐任务更容易扩大群体间的差异。多轮对话中可能持续存在有偏见的决策。

Conclusion: 本研究首次系统性地研究了端到端语音对话模型中的偏见，为构建公平可靠的音频交互系统提供了见解。

Abstract: While biases in large language models (LLMs), such as stereotypes and
cultural tendencies in outputs, have been examined and identified, their
presence and characteristics in spoken dialogue models (SDMs) with audio input
and output remain largely unexplored. Paralinguistic features, such as age,
gender, and accent, can affect model outputs; when compounded by multi-turn
conversations, these effects may exacerbate biases, with potential implications
for fairness in decision-making and recommendation tasks. In this paper, we
systematically evaluate biases in speech LLMs and study the impact of
multi-turn dialogues with repeated negative feedback. Bias is measured using
Group Unfairness Score (GUS) for decisions and similarity-based normalized
statistics rate (SNSR) for recommendations, across both open-source models like
Qwen2.5-Omni and GLM-4-Voice, as well as closed-source APIs such as GPT-4o
Audio and Gemini-2.5-Flash. Our analysis reveals that closed-source models
generally exhibit lower bias, while open-source models are more sensitive to
age and gender, and recommendation tasks tend to amplify cross-group
disparities. We found that biased decisions may persist in multi-turn
conversations. This work provides the first systematic study of biases in
end-to-end spoken dialogue models, offering insights towards fair and reliable
audio-based interactive systems. To facilitate further research, we release the
FairDialogue dataset and evaluation code.

</details>


### [28] [An Senegalese Legal Texts Structuration Using LLM-augmented Knowledge Graph](https://arxiv.org/abs/2510.02353)
*Oumar Kane,Mouhamad M. Allaya,Dame Samb,Mamadou Bousso*

Main category: cs.CL

TL;DR: 本研究探讨了人工智能和大型语言模型在改善塞内加尔司法系统中获取法律文本的应用。


<details>
  <summary>Details</summary>
Motivation: 重点在于提取和组织法律文件的困难，强调需要更好地获取司法信息。

Method: 研究成功地从各种法律文件中提取了 7,967 篇文章，特别关注土地和公共领域法典。开发了一个详细的图数据库，其中包含 2,872 个节点和 10,774 个关系，有助于可视化法律文本中的互连。此外，利用先进的三重提取技术来获取知识，论证了 GPT-4o、GPT-4 和 Mistral-Large 等模型在识别关系和相关元数据方面的有效性。

Result: 创建了一个包含 2,872 个节点和 10,774 个关系的图数据库。

Conclusion: 通过这些技术，旨在创建一个坚实的框架，使塞内加尔公民和法律专业人士能够更有效地了解他们的权利和责任。

Abstract: This study examines the application of artificial intelligence (AI) and large
language models (LLM) to improve access to legal texts in Senegal's judicial
system. The emphasis is on the difficulties of extracting and organizing legal
documents, highlighting the need for better access to judicial information. The
research successfully extracted 7,967 articles from various legal documents,
particularly focusing on the Land and Public Domain Code. A detailed graph
database was developed, which contains 2,872 nodes and 10,774 relationships,
aiding in the visualization of interconnections within legal texts. In
addition, advanced triple extraction techniques were utilized for knowledge,
demonstrating the effectiveness of models such as GPT-4o, GPT-4, and
Mistral-Large in identifying relationships and relevant metadata. Through these
technologies, the aim is to create a solid framework that allows Senegalese
citizens and legal professionals to more effectively understand their rights
and responsibilities.

</details>


### [29] [Modeling the language cortex with form-independent and enriched representations of sentence meaning reveals remarkable semantic abstractness](https://arxiv.org/abs/2510.02354)
*Shreya Saha,Shurui Li,Greta Tuckute,Yuanning Li,Ru-Yuan Zhang,Leila Wehbe,Evelina Fedorenko,Meenakshi Khosla*

Main category: cs.CL

TL;DR: 本文研究了语言皮层中意义的抽象表征。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨人类语言系统如何表征语言形式和意义，以及意义表征的抽象程度。

Method: 通过使用视觉和语言模型的表征来建模对句子的神经反应。

Result: 研究结果表明，聚合多个生成的图像可以更准确地预测语言皮层反应，有时甚至可以与大型语言模型相媲美。类似地，对句子的多个释义进行平均嵌入可以提高预测准确性。用上下文细节丰富释义可以进一步提高预测准确性，甚至超过基于原始句子嵌入的预测。

Conclusion: 研究表明，语言皮层中存在高度抽象的、独立于形式的意义表征。

Abstract: The human language system represents both linguistic forms and meanings, but
the abstractness of the meaning representations remains debated. Here, we
searched for abstract representations of meaning in the language cortex by
modeling neural responses to sentences using representations from vision and
language models. When we generate images corresponding to sentences and extract
vision model embeddings, we find that aggregating across multiple generated
images yields increasingly accurate predictions of language cortex responses,
sometimes rivaling large language models. Similarly, averaging embeddings
across multiple paraphrases of a sentence improves prediction accuracy compared
to any single paraphrase. Enriching paraphrases with contextual details that
may be implicit (e.g., augmenting "I had a pancake" to include details like
"maple syrup") further increases prediction accuracy, even surpassing
predictions based on the embedding of the original sentence, suggesting that
the language system maintains richer and broader semantic representations than
language models. Together, these results demonstrate the existence of highly
abstract, form-independent meaning representations within the language cortex.

</details>


### [30] [DiffuSpec: Unlocking Diffusion Language Models for Speculative Decoding](https://arxiv.org/abs/2510.02358)
*Guanghao Li,Zhihui Fu,Min Fang,Qibin Zhao,Ming Tang,Chun Yuan,Jun Wang*

Main category: cs.CL

TL;DR: DiffuSpec uses a diffusion language model (DLM) for faster multi-token draft generation in speculative decoding, offering up to 3x speedup.


<details>
  <summary>Details</summary>
Motivation: Autoregressive decoding in large language models increases latency due to serial forward passes. Existing speculative decoding methods still rely on AR drafters, limiting performance.

Method: Introduces DiffuSpec, a training-free framework using a pretrained DLM for parallel multi-token draft generation. It includes a causal-consistency path search (CPS) and an adaptive draft-length (ADL) controller.

Result: DiffuSpec achieves up to 3x wall-clock speedup compared to autoregressive drafters.

Conclusion: Diffusion-based drafting is a robust alternative to autoregressive drafters for speculative decoding.

Abstract: As large language models (LLMs) scale up, accuracy improves, but the
autoregressive (AR) nature of decoding increases latency since each token
requires a serial forward pass. Speculative decoding addresses this by
employing a fast drafter to propose multi-token drafts, which are then verified
in parallel by the target model. However, many deployments still rely on AR
drafters, where sequential passes limit wall-clock gains. We revisit the
drafting stage and present DiffuSpec, a training-free drop-in framework that
uses a pretrained diffusion language model (DLM) to produce multi-token drafts
in a single forward pass, while remaining compatible with standard AR
verifiers. Because DLM drafts are generated under bidirectional conditioning,
parallel per-position candidates form a token lattice in which the locally
highest-probability token at each position need not form a causal left-to-right
path. Moreover, DLM drafting requires pre-specifying a draft length, inducing a
speed-quality trade-off. To address these challenges, we introduce two
practical components: (i) a causal-consistency path search (CPS) over this
lattice that extracts a left-to-right path aligned with AR verification; and
(ii) an adaptive draft-length (ADL) controller that adjusts next proposal size
based on recent acceptance feedback and realized generated length. Across
benchmarks, DiffuSpec yields up to 3x wall-clock speedup, establishing
diffusion-based drafting as a robust alternative to autoregressive drafters for
speculative decoding.

</details>


### [31] [Emission-GPT: A domain-specific language model agent for knowledge retrieval, emission inventory and data analysis](https://arxiv.org/abs/2510.02359)
*Jiashu Ye,Tong Wu,Weiwen Chen,Hao Zhang,Zeteng Lin,Xingxing Li,Shujuan Weng,Manni Zhu,Xin Yuan,Xinlong Hong,Jingjie Li,Junyu Zheng,Zhijiong Huang,Jing Tang*

Main category: cs.CL

TL;DR: Emission-GPT: A knowledge-enhanced large language model agent for atmospheric emissions, built on a curated knowledge base, enables accurate question answering and interactive data analysis.


<details>
  <summary>Details</summary>
Motivation: Existing methods for accessing and compiling emissions data remain inefficient, hindering non-experts' ability to interpret emissions information.

Method: Developed Emission-GPT, a knowledge-enhanced large language model agent, built on a curated knowledge base of over 10,000 documents, integrates prompt engineering and question completion.

Result: Emission-GPT can extract key insights from raw data with simple prompts, as demonstrated in a case study in Guangdong Province.

Conclusion: Emission-GPT facilitates automation of traditionally manual workflows and is a foundational tool for next-generation emission inventory development and scenario-based assessment.

Abstract: Improving air quality and addressing climate change relies on accurate
understanding and analysis of air pollutant and greenhouse gas emissions.
However, emission-related knowledge is often fragmented and highly specialized,
while existing methods for accessing and compiling emissions data remain
inefficient. These issues hinder the ability of non-experts to interpret
emissions information, posing challenges to research and management. To address
this, we present Emission-GPT, a knowledge-enhanced large language model agent
tailored for the atmospheric emissions domain. Built on a curated knowledge
base of over 10,000 documents (including standards, reports, guidebooks, and
peer-reviewed literature), Emission-GPT integrates prompt engineering and
question completion to support accurate domain-specific question answering.
Emission-GPT also enables users to interactively analyze emissions data via
natural language, such as querying and visualizing inventories, analyzing
source contributions, and recommending emission factors for user-defined
scenarios. A case study in Guangdong Province demonstrates that Emission-GPT
can extract key insights--such as point source distributions and sectoral
trends--directly from raw data with simple prompts. Its modular and extensible
architecture facilitates automation of traditionally manual workflows,
positioning Emission-GPT as a foundational tool for next-generation emission
inventory development and scenario-based assessment.

</details>


### [32] [Spiral of Silence in Large Language Model Agents](https://arxiv.org/abs/2510.02360)
*Mingze Zhong,Meng Fang,Zijing Shi,Yuxuan Huang,Shunfeng Zheng,Yali Du,Ling Chen,Jun Wang*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLM）群体中是否会出现类似“沉默的螺旋”效应的现象。


<details>
  <summary>Details</summary>
Motivation: 研究人类社会中的“沉默的螺旋”理论是否适用于LLM群体，以及LLM的纯统计语言生成是否会导致类似效应。

Method: 构建了一个评估框架，通过控制“历史”和“角色”信号的可用性，并使用趋势测试（如Mann-Kendall和Spearman等级）以及集中度量（包括峰度和四分位距）来评估观点动态。

Result: 实验表明，历史和角色信号共同作用会产生强大的多数主导并复制“沉默的螺旋”模式；单独的历史信号会诱导强烈的锚定效应；单独的角色信号会培养多样但互不相关的观点；没有历史锚定，沉默的螺旋动态无法出现。

Conclusion: 这项工作连接了计算社会学和负责任的AI设计，强调需要监控和缓解LLM智能体系统中出现的趋同性。

Abstract: The Spiral of Silence (SoS) theory holds that individuals with minority views
often refrain from speaking out for fear of social isolation, enabling majority
positions to dominate public discourse. When the 'agents' are large language
models (LLMs), however, the classical psychological explanation is not directly
applicable, since SoS was developed for human societies. This raises a central
question: can SoS-like dynamics nevertheless emerge from purely statistical
language generation in LLM collectives? We propose an evaluation framework for
examining SoS in LLM agents. Specifically, we consider four controlled
conditions that systematically vary the availability of 'History' and 'Persona'
signals. Opinion dynamics are assessed using trend tests such as Mann-Kendall
and Spearman's rank, along with concentration measures including kurtosis and
interquartile range. Experiments across open-source and closed-source models
show that history and persona together produce strong majority dominance and
replicate SoS patterns; history signals alone induce strong anchoring; and
persona signals alone foster diverse but uncorrelated opinions, indicating that
without historical anchoring, SoS dynamics cannot emerge. The work bridges
computational sociology and responsible AI design, highlighting the need to
monitor and mitigate emergent conformity in LLM-agent systems.

</details>


### [33] [ChunkLLM: A Lightweight Pluggable Framework for Accelerating LLMs Inference](https://arxiv.org/abs/2510.02361)
*Haojie Ouyang,Jianwei Lv,Lei Ren,Chen Wei,Xiaojie Wang,Fangxiang Feng*

Main category: cs.CL

TL;DR: ChunkLLM is proposed to solve the computational inefficiency of Transformer-based large models on long texts.


<details>
  <summary>Details</summary>
Motivation: Transformer-based large models have computational inefficiencies due to the self-attention's quadratic complexity. Existing methods have issues with semantic incompleteness or poor training-inference efficiency.

Method: Two components are introduced: QK Adapter (Q-Adapter and K-Adapter) and Chunk Adapter. The former compresses features and acquires chunk attention, while the latter detects chunk boundaries. An attention distillation method is used for training the QK Adapter.

Result: ChunkLLM attains comparable performance on short-text benchmarks and maintains 98.64% of the performance on long-context benchmarks while preserving a 48.58% key-value cache retention rate. It achieves a maximum speedup of 4.48x compared to the vanilla Transformer in processing 120K long texts.

Conclusion: ChunkLLM effectively addresses the computational challenges of processing long texts in Transformer-based large models.

Abstract: Transformer-based large models excel in natural language processing and
computer vision, but face severe computational inefficiencies due to the
self-attention's quadratic complexity with input tokens. Recently, researchers
have proposed a series of methods based on block selection and compression to
alleviate this problem, but they either have issues with semantic
incompleteness or poor training-inference efficiency. To comprehensively
address these challenges, we propose ChunkLLM, a lightweight and pluggable
training framework. Specifically, we introduce two components: QK Adapter
(Q-Adapter and K-Adapter) and Chunk Adapter. The former is attached to each
Transformer layer, serving dual purposes of feature compression and chunk
attention acquisition. The latter operates at the bottommost layer of the
model, functioning to detect chunk boundaries by leveraging contextual semantic
information. During the training phase, the parameters of the backbone remain
frozen, with only the QK Adapter and Chunk Adapter undergoing training.
Notably, we design an attention distillation method for training the QK
Adapter, which enhances the recall rate of key chunks. During the inference
phase, chunk selection is triggered exclusively when the current token is
detected as a chunk boundary, thereby accelerating model inference.
Experimental evaluations are conducted on a diverse set of long-text and
short-text benchmark datasets spanning multiple tasks. ChunkLLM not only
attains comparable performance on short-text benchmarks but also maintains
98.64% of the performance on long-context benchmarks while preserving a 48.58%
key-value cache retention rate. Particularly, ChunkLLM attains a maximum
speedup of 4.48x in comparison to the vanilla Transformer in the processing of
120K long texts.

</details>


### [34] [A Cross-Lingual Analysis of Bias in Large Language Models Using Romanian History](https://arxiv.org/abs/2510.02362)
*Matei-Iulian Cocu,Răzvan-Cosmin Cristia,Adrian Marius Dumitran*

Main category: cs.CL

TL;DR: 本研究通过调查大型语言模型在回答罗马尼亚历史问题时的偏差，揭示了它们在不同语言和背景下的不一致性。


<details>
  <summary>Details</summary>
Motivation: 历史常受文化和意识形态影响，大型语言模型也可能因训练数据偏差而缺乏中立性，从而影响用户。

Method: 研究分三个阶段进行，通过改变提问方式（肯定回答、数值评分）来观察模型回答的一致性。

Result: 二元回答稳定性较高但非完美，且因语言而异。模型在不同语言或格式间立场不稳，数值评分与初始二元选择常不一致。最稳定的模型并非总是最准确或中立。

Conclusion: 研究揭示了模型在特定语言环境下的不一致性倾向。

Abstract: In this case study, we select a set of controversial Romanian historical
questions and ask multiple Large Language Models to answer them across
languages and contexts, in order to assess their biases. Besides being a study
mainly performed for educational purposes, the motivation also lies in the
recognition that history is often presented through altered perspectives,
primarily influenced by the culture and ideals of a state, even through large
language models. Since they are often trained on certain data sets that may
present certain ambiguities, the lack of neutrality is subsequently instilled
in users. The research process was carried out in three stages, to confirm the
idea that the type of response expected can influence, to a certain extent, the
response itself; after providing an affirmative answer to some given question,
an LLM could shift its way of thinking after being asked the same question
again, but being told to respond with a numerical value of a scale. Results
show that binary response stability is relatively high but far from perfect and
varies by language. Models often flip stance across languages or between
formats; numeric ratings frequently diverge from the initial binary choice, and
the most consistent models are not always those judged most accurate or
neutral. Our research brings to light the predisposition of models to such
inconsistencies, within a specific contextualization of the language for the
question asked.

</details>


### [35] [Beyond Manuals and Tasks: Instance-Level Context Learning for LLM Agents](https://arxiv.org/abs/2510.02369)
*Kuntai Cai,Juncheng Liu,Xianglin Yang,Zhaojie Niu,Xiaokui Xiao,Xing Chen*

Main category: cs.CL

TL;DR: 这篇论文介绍了一种新的上下文类型，实例级别上下文，并提出了一种自动学习和重用这种上下文的方法，以提高LLM agent在复杂任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM agent通常只接收环境级别和任务级别的上下文，忽略了实例级别上下文，这导致agent在复杂任务中容易失败，因为成功往往依赖于精确和持久的事实。

Method: 论文提出了一种名为实例级别上下文学习（ILCL）的任务无关方法，该方法通过有指导的探索、TODO森林和plan-act-extract循环来自动生成高精度的上下文文档。

Result: 在TextWorld, ALFWorld和Crafter等环境中的实验表明，该方法可以显著提高LLM agent的成功率和效率。

Conclusion: 该方法通过将一次性探索转化为持久的可重用知识，可以补充现有的上下文，从而实现更可靠和高效的LLM agent。

Abstract: Large language model (LLM) agents typically receive two kinds of context: (i)
environment-level manuals that define interaction interfaces and global rules,
and (ii) task-level guidance or demonstrations tied to specific goals. In this
work, we identify a crucial but overlooked third type of context,
instance-level context, which consists of verifiable and reusable facts tied to
a specific environment instance, such as object locations, crafting recipes,
and local rules. We argue that the absence of instance-level context is a
common source of failure for LLM agents in complex tasks, as success often
depends not only on reasoning over global rules or task prompts but also on
making decisions based on precise and persistent facts. Acquiring such context
requires more than memorization: the challenge lies in efficiently exploring,
validating, and formatting these facts under tight interaction budgets. We
formalize this problem as Instance-Level Context Learning (ILCL) and introduce
our task-agnostic method to solve it. Our method performs a guided exploration,
using a compact TODO forest to intelligently prioritize its next actions and a
lightweight plan-act-extract loop to execute them. This process automatically
produces a high-precision context document that is reusable across many
downstream tasks and agents, thereby amortizing the initial exploration cost.
Experiments across TextWorld, ALFWorld, and Crafter demonstrate consistent
gains in both success and efficiency: for instance, ReAct's mean success rate
in TextWorld rises from 37% to 95%, while IGE improves from 81% to 95%. By
transforming one-off exploration into persistent, reusable knowledge, our
method complements existing contexts to enable more reliable and efficient LLM
agents.

</details>


### [36] [Training Dynamics of Parametric and In-Context Knowledge Utilization in Language Models](https://arxiv.org/abs/2510.02370)
*Minsung Kim,Dong-Kyum Kim,Jea Kwon,Nakyeong Yang,Kyomin Jung,Meeyoung Cha*

Main category: cs.CL

TL;DR: 大型语言模型在推理时检索到的上下文知识和预训练期间获得的参数知识之间经常遇到冲突。本文研究了训练条件如何影响模型对上下文知识和参数知识的使用，以及它们如何在两者之间进行仲裁。


<details>
  <summary>Details</summary>
Motivation: 目前我们仍然缺乏对训练期间知识仲裁策略的系统理解。为了解决这个问题，我们首次对训练条件如何影响模型对上下文知识和参数知识的使用进行对照研究，以及它们如何在两者之间进行仲裁。

Method: 我们在一个合成的传记语料库上训练基于Transformer的语言模型，同时系统地控制各种条件。

Result: 文档内事实的重复培养了参数和上下文能力的发展。此外，在包含不一致信息或分布偏差的语料库上进行训练，鼓励模型制定利用参数和上下文知识的稳健策略。

Conclusion: 这些见解为预训练和谐整合参数和上下文知识的模型提供了具体的经验指导。

Abstract: Large language models often encounter conflicts between in-context knowledge
retrieved at inference time and parametric knowledge acquired during
pretraining. Models that accept external knowledge uncritically are vulnerable
to misinformation, whereas models that adhere rigidly to parametric knowledge
fail to benefit from retrieval. Despite the widespread adoption of
retrieval-augmented generation, we still lack a systematic understanding of
what shapes knowledge-arbitration strategies during training. This gap risks
producing pretrained models with undesirable arbitration behaviors and,
consequently, wasting substantial computational resources after the pretraining
budget has already been spent. To address this problem, we present the first
controlled study of how training conditions influence models' use of in-context
and parametric knowledge, and how they arbitrate between them. We train
transformer-based language models on a synthetic biographies corpus while
systematically controlling various conditions. Our experiments reveal that
intra-document repetition of facts fosters the development of both parametric
and in-context capabilities. Moreover, training on a corpus that contains
inconsistent information or distributional skew encourages models to develop
robust strategies for leveraging parametric and in-context knowledge. Rather
than viewing these non-ideal properties as artifacts to remove, our results
indicate that they are important for learning robust arbitration. These
insights offer concrete, empirical guidance for pretraining models that
harmoniously integrate parametric and in-context knowledge.

</details>


### [37] [Pretraining with hierarchical memories: separating long-tail and common knowledge](https://arxiv.org/abs/2510.02375)
*Hadi Pouransari,David Grangier,C Thomas,Michael Kirchhof,Oncel Tuzel*

Main category: cs.CL

TL;DR: 本文提出了一种内存增强架构和预训练策略，旨在解决大型语言模型参数过多、难以在边缘设备上部署的问题。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型依赖于扩展参数来提高性能，但这对于内存和计算资源有限的边缘设备来说是不切实际的。

Method: 本文引入了一种小型语言模型，它可以访问编码世界知识的大型分层参数记忆库。在预训练和推理过程中，模型会获取一个小的、上下文相关的记忆块并将其添加到模型中。

Result: 实验结果表明，一个1.6亿参数的模型，通过访问一个包含46亿参数记忆库的1800万参数记忆，可以获得与参数量是其两倍以上的常规模型相当的性能。

Conclusion: 本文提出的分层前馈记忆在transformer架构中表现稳健，无论是在预训练期间还是事后添加。

Abstract: The impressive performance gains of modern language models currently rely on
scaling parameters: larger models store more world knowledge and reason better.
Yet compressing all world knowledge into parameters is unnecessary, as only a
fraction is used per prompt, and impractical for edge devices with limited
inference-time memory and compute. We address this shortcoming by a
memory-augmented architecture and a pretraining strategy aligned with existing
hardware paradigms. We introduce small language models that access large
hierarchical parametric memory banks encoding world knowledge. During
pretraining and inference, we fetch a small, context-dependent memory block and
add it to the model. Our pretraining learns to store long-tail world knowledge
in the memory parameters, while the small language model acts as an anchor
capturing common knowledge and general reasoning abilities. Through
trillion-token-scale experiments, we show significant gains: a 160M-parameters
model augmented with an 18M-parameters memory fetched from a 4.6B memory bank
obtains comparable performance to a regular model with more than 2x the
parameters. Through extensive experiments, we study the optimal type and size
of parametric memories in transformers, scaling them to over 21B parameters. We
find that our proposed hierarchical feed-forward memories work robustly across
transformer architectures, whether added during pretraining or post-hoc.

</details>


### [38] [Uncertainty-Aware Answer Selection for Improved Reasoning in Multi-LLM Systems](https://arxiv.org/abs/2510.02377)
*Aakriti Agrawal,Rohith Aralikatti,Anirudh Satheesh,Souradip Chakraborty,Amrit Singh Bedi,Furong Huang*

Main category: cs.CL

TL;DR: 提出了一种新的计算效率高的方法，用于从多个不同的大型语言模型中选择最佳响应。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖于昂贵的外部验证器、人工评估员或需要从单个模型中抽取多个样本的自我一致性技术，成本高昂。

Method: 使用校准的对数似然得分，隐式地利用这些模型固有的知识和置信度。

Result: 在 GSM8K、MMLU（6 个子集）和 ARC 数据集上，辩论（多轮 LLM 讨论）和非辩论（具有多个 LLM 的 Best-of-N）设置的改进分别约为 4%、3% 和 5%。

Conclusion: 该方法在选择最佳响应方面表现出有效性，并在各种数据集上取得了显著的改进。

Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities, yet
selecting the most reliable response from multiple LLMs remains a challenge,
particularly in resource-constrained settings. Existing approaches often depend
on costly external verifiers, human evaluators, or self-consistency techniques
that require multiple samples from a single model. While multi-LLM systems
produce more diverse responses than single models and thus have greater
potential, they often underperform compared to single LLM self-consistency. We
propose a principled, novel and computationally efficient method to select the
best response from multiple different LLMs using a calibrated log-likelihood
score, implicitly leveraging the inherent knowledge and confidence of these
models. Our method demonstrates improvements of approx. 4%, 3%, and 5% across
both debate (multi-round LLM discussions) and non-debate (Best-of-N with
multiple LLMs) settings on GSM8K, MMLU (6 subsets), and ARC datasets
respectively.

</details>


### [39] [Learning to Route: A Rule-Driven Agent Framework for Hybrid-Source Retrieval-Augmented Generation](https://arxiv.org/abs/2510.02388)
*Haoyue Bai,Haoyu Wang,Shengyu Chen,Zhengzhang Chen,Lu-An Tang,Wei Cheng,Haifeng Chen,Yanjie Fu*

Main category: cs.CL

TL;DR: 这篇论文提出了一种新的检索增强生成（RAG）框架，该框架通过规则驱动的路由来利用关系数据库和文档的互补优势，从而提高了领域特定问答的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成系统主要依赖于非结构化文档，而忽略了关系数据库中精确、及时和可高效查询的事实信息。因此，作者旨在填补这一空白。

Method: 作者首先通过系统分析揭示了数据库和文档的互补优势，以及简单组合这两种来源的缺点。然后，作者提出了一个规则驱动的路由框架，包括一个路由代理、一个规则制定专家代理和一个路径级元缓存。

Result: 在三个问答基准上的实验表明，该框架始终优于静态策略和学习路由基线，实现了更高的准确性，同时保持了适度的计算成本。

Conclusion: 该研究表明，通过规则驱动的路由，可以有效地利用关系数据库和文档的互补优势，从而提高领域特定问答的性能。

Abstract: Large Language Models (LLMs) have shown remarkable performance on general
Question Answering (QA), yet they often struggle in domain-specific scenarios
where accurate and up-to-date information is required. Retrieval-Augmented
Generation (RAG) addresses this limitation by enriching LLMs with external
knowledge, but existing systems primarily rely on unstructured documents, while
largely overlooking relational databases, which provide precise, timely, and
efficiently queryable factual information, serving as indispensable
infrastructure in domains such as finance, healthcare, and scientific research.
Motivated by this gap, we conduct a systematic analysis that reveals three
central observations: (i) databases and documents offer complementary strengths
across queries, (ii) naively combining both sources introduces noise and cost
without consistent accuracy gains, and (iii) selecting the most suitable source
for each query is crucial to balance effectiveness and efficiency. We further
observe that query types show consistent regularities in their alignment with
retrieval paths, suggesting that routing decisions can be effectively guided by
systematic rules that capture these patterns. Building on these insights, we
propose a rule-driven routing framework. A routing agent scores candidate
augmentation paths based on explicit rules and selects the most suitable one; a
rule-making expert agent refines the rules over time using QA feedback to
maintain adaptability; and a path-level meta-cache reuses past routing
decisions for semantically similar queries to reduce latency and cost.
Experiments on three QA benchmarks demonstrate that our framework consistently
outperforms static strategies and learned routing baselines, achieving higher
accuracy while maintaining moderate computational cost.

</details>


### [40] [KnowledgeSmith: Uncovering Knowledge Updating in LLMs with Model Editing and Unlearning](https://arxiv.org/abs/2510.02392)
*Yinyi Luo,Zhexian Zhou,Hao Chen,Kai Qiu,Marios Savvides,Yixuan Li,Jindong Wang*

Main category: cs.CL

TL;DR: 本文提出了KnowledgeSmith，一个统一的框架，用于系统地理解大型语言模型的更新机制。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的知识编辑和机器遗忘是两种流行的保持LLM最新的方法。然而，由于不足、孤立和小规模的评估，LLM的知识更新机制在很大程度上仍未被探索。

Method: 首先将编辑和遗忘视为一个约束优化问题的实例。然后，我们提出了一个自动数据集生成器，该生成器提供跨多个图级别和数据规模的结构化干预，从而能够控制研究不同的修改策略如何通过模型知识传播。

Result: 大量的实验证明了知识传播、可塑性缩放、一致性和鲁棒性的细微见解。例如，我们的结果表明，对于不同级别的知识，LLM没有表现出与人类相似的更新，并且存在一致性-容量权衡。

Conclusion: 我们的研究结果可以为更可靠和可扩展的策略设计提供建议。

Abstract: Knowledge editing and machine unlearning are two popular approaches for large
language models (LLMs) to stay up-to-date. However, the knowledge updating
mechanism of LLMs remains largely unexplored due to insufficient, isolated, and
small-scale evaluation. For instance, are LLMs similar to humans in modifying
certain knowledge? What differs editing and unlearning as training data
increases? This paper proposes KnowledgeSmith, a unified framework to
systematically understand the updating mechanism of LLMs. We first cast editing
and unlearning as instances of one constrained optimization problem. Then, we
propose an automatic dataset generator that provides structured interventions
across multiple graph levels and data scales, enabling controlled studies of
how different modification strategies propagate through model knowledge.
Extensive experiments demonstrate nuanced insights over knowledge propagation,
plasticity scaling, consistency, and robustness. For instance, our results show
that LLMs do not exhibit similar updating as humans for different levels of
knowledge, and there exists consistency-capacity trade-off. We hope our
findings can offer suggestions to the design of more reliable and scalable
strategies. Code: https://github.com/AIFrontierLab/KnowledgeSmith.git

</details>


### [41] [Retrieval and Augmentation of Domain Knowledge for Text-to-SQL Semantic Parsing](https://arxiv.org/abs/2510.02394)
*Manasi Patwardhan,Ayush Agarwal,Shabbirhussain Bhaisaheb,Aseem Arora,Lovekesh Vig,Sunita Sarawagi*

Main category: cs.CL

TL;DR: LLMs在将自然语言查询翻译成SQL时，在不同数据库上的表现差异很大。现有基准依赖于不切实际的、特定于查询的文本提示来表达领域知识。这篇论文提出了一个系统性的框架，用于在数据库级别关联结构化的领域语句。通过子串级别的匹配，检索给定用户查询的相关结构化领域语句。在涵盖不同领域的11个真实数据库模式上进行了评估，结果表明，数据库级别的结构化领域语句比现有的特定于查询的文本领域语句更实用和准确，并且基于子串匹配的相关领域语句检索比其他检索方法提供更高的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基准依赖于不切实际的、特定于查询的文本提示来表达领域知识，这篇论文旨在解决这个问题。

Method: 提出了一个系统性的框架，用于在数据库级别关联结构化的领域语句。使用子串级别的匹配，检索给定用户查询的相关结构化领域语句。

Result: 在涵盖不同领域的11个真实数据库模式上进行了评估，结果表明，数据库级别的结构化领域语句比现有的特定于查询的文本领域语句更实用和准确，并且基于子串匹配的相关领域语句检索比其他检索方法提供更高的准确性。

Conclusion: 数据库级别的结构化领域语句比现有的特定于查询的文本领域语句更实用和准确，并且基于子串匹配的相关领域语句检索比其他检索方法提供更高的准确性。

Abstract: The performance of Large Language Models (LLMs) for translating Natural
Language (NL) queries into SQL varies significantly across databases (DBs). NL
queries are often expressed using a domain specific vocabulary, and mapping
these to the correct SQL requires an understanding of the embedded domain
expressions, their relationship to the DB schema structure. Existing benchmarks
rely on unrealistic, ad-hoc query specific textual hints for expressing domain
knowledge. In this paper, we propose a systematic framework for associating
structured domain statements at the database level. We present retrieval of
relevant structured domain statements given a user query using sub-string level
match. We evaluate on eleven realistic DB schemas covering diverse domains
across five open-source and proprietary LLMs and demonstrate that (1) DB level
structured domain statements are more practical and accurate than existing
ad-hoc query specific textual domain statements, and (2) Our sub-string match
based retrieval of relevant domain statements provides significantly higher
accuracy than other retrieval approaches.

</details>


### [42] [Words That Make Language Models Perceive](https://arxiv.org/abs/2510.02425)
*Sophie L. Wang,Phillip Isola,Brian Cheung*

Main category: cs.CL

TL;DR: 文本大型语言模型(llm)虽然只接受文本训练，但实际上已经通过语言编码包含了多模态的规律。研究表明，通过感官提示，可以有效地激活llm中与视觉和听觉相关的潜在表征，使其在表征上与专门的视觉和音频编码器更加一致。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索纯文本训练的llm是否以及如何在内部表征中编码了多模态信息，并且如何通过提示工程来激活这些潜在的表征。

Method: 该研究通过设计感官提示（如“看”或“听”）来引导llm进行预测，并观察模型是否能够生成与视觉或听觉信息相关的token。同时，将llm的表征与专门的视觉和音频编码器的表征进行比较，以评估其一致性。

Result: 研究发现，通过简单的提示工程，可以有效地激活纯文本训练的llm中与特定模态相适应的表征。

Conclusion: 研究表明，即使是纯文本训练的llm，也可以通过感官提示来激发其内部潜在的多模态表征能力，使其在表征上与专门的视觉和听觉模型更加接近。

Abstract: Large language models (LLMs) trained purely on text ostensibly lack any
direct perceptual experience, yet their internal representations are implicitly
shaped by multimodal regularities encoded in language. We test the hypothesis
that explicit sensory prompting can surface this latent structure, bringing a
text-only LLM into closer representational alignment with specialist vision and
audio encoders. When a sensory prompt tells the model to 'see' or 'hear', it
cues the model to resolve its next-token predictions as if they were
conditioned on latent visual or auditory evidence that is never actually
supplied. Our findings reveal that lightweight prompt engineering can reliably
activate modality-appropriate representations in purely text-trained LLMs.

</details>


### [43] [CLARITY: Clinical Assistant for Routing, Inference, and Triage](https://arxiv.org/abs/2510.02463)
*Vladimir Shaposhnikov,Aleksandr Nesterov,Ilia Kopanichuk,Ivan Bakulin,Egor Zhelvakov,Ruslan Abramov,Ekaterina Tsapieva,Dmitry V. Dylov,Ivan Oseledets*

Main category: cs.CL

TL;DR: CLARITY是一个AI平台，可以帮助患者进行专家路由、临床咨询和病情评估。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在解决患者找到合适的专科医生以及进行高效临床咨询和病情评估的问题。

Method: CLARITY采用混合架构，结合了有限状态机（FSM）和大型语言模型（LLM）。

Result: CLARITY在部署的两个月内完成了超过55,000次对话，并且在首次路由精度方面超过了人类水平，咨询时间缩短了3倍。

Conclusion: CLARITY是一个安全、高效、稳健且可扩展的平台，可以满足医疗保健领域的需求。

Abstract: We present CLARITY (Clinical Assistant for Routing, Inference, and Triage),
an AI-driven platform designed to facilitate patient-to-specialist routing,
clinical consultations, and severity assessment of patients' conditions. Its
hybrid architecture combines a Finite State Machine (FSM) for structured
dialogue flows with collaborative agents that employ Large Language Model (LLM)
to analyze symptoms and prioritize referrals to appropriate specialists. Built
on a modular microservices framework, CLARITY ensures safe, efficient, and
robust performance, flexible and readily scalable to meet the demands of
existing workflows and IT solutions in healthcare.
  We report integration of our clinical assistant into a large-scale
nation-wide inter-hospital IT platform, with over 55,000 content-rich user
dialogues completed within the two months of deployment, 2,500 of which were
expert-annotated for a consequent validation. The validation results show that
CLARITY surpasses human-level performance in terms of the first-attempt routing
precision, naturally requiring up to 3 times shorter duration of the
consultation than with a human.

</details>


### [44] [Unraveling Syntax: How Language Models Learn Context-Free Grammars](https://arxiv.org/abs/2510.02524)
*Laura Ying Schulz,Daniel Mitropolsky,Tomaso Poggio*

Main category: cs.CL

TL;DR: 本文提出了一个新框架，用于理解语言模型如何获取句法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型取得了令人瞩目的成果，但对其学习动态知之甚少。本文旨在研究语言模型学习句法的学习动态。

Method: 通过研究在从 PCFG 生成的合成语言上训练的小型模型，从而精确控制语法复杂性、递归深度和子语法结构。

Result: Transformer 并行地减少所有子语法的损失。子语法预训练可以改善较小模型的最终损失，并且预训练模型开发了与语法子结构更一致的内部表示。模型难以处理更深的递归结构。

Conclusion: 本文启动了对 Transformer 在 PCFG 上的学习动态的研究，作为一个通用的测试平台，用于探测语言模型的学习，开辟了一个有许多开放问题的研究方向。

Abstract: We introduce a new framework for understanding how language models acquire
syntax. While large models achieve impressive results, little is known about
their learning dynamics. Our approach starts with the observation that most
domains of interest, such as natural language syntax, coding languages,
arithmetic problems, are captured by probabilistic context-free grammars
(PCFGs). We study the learning dynamics of small models trained on synthetic
languages generated from PCFGs, enabling precise control over grammar
complexity, recursion depth, and subgrammar structure. We prove several
general, recursive formulae for the training loss and Kullback-Leibler
divergence over the subgrammar structure of a PCFG. Empirically, we find that
unlike children, who first master simple substructures before progressing to
more complex constructions, transformers reduce loss across all subgrammars in
parallel. We further show that subgrammar pretraining can improve the final
loss for smaller models, and that pretrained models develop internal
representations more aligned with the grammar's substructure. Finally, we
demonstrate that models struggle with deeper recursive structures (a limitation
even of large language models), revealing fundamental challenges in how neural
networks represent hierarchical syntax. Overall, our work initiates the study
of the learning dynamics of transformers on PCFGs as a versatile testbed for
probing learning in language models, opening a research direction with many
open questions.

</details>


### [45] [Knowledge-Graph Based RAG System Evaluation Framework](https://arxiv.org/abs/2510.02549)
*Sicheng Dong,Vahid Zolfaghari,Nenad Petrovic,Alois Knoll*

Main category: cs.CL

TL;DR: 本研究扩展了RAGAS框架，提出了一个基于知识图谱的RAG评估范式，以实现更全面的评分指标。


<details>
  <summary>Details</summary>
Motivation: 传统的评估指标难以有效捕捉大型语言模型生成内容的关键特征，尤其是在流畅性和自然性方面。

Method: 通过将RAGAS框架扩展到基于知识图谱的评估范式，实现多跳推理和语义社区聚类。

Result: 该方法与RAGAS评分进行了比较，并构建了一个人工标注子集来评估人类判断和自动指标之间的相关性。实验表明，该方法对生成输出中细微的语义差异更敏感。

Conclusion: 基于知识图谱的评估方法能够更深入地理解RAG系统，并对其性能有更细致的理解。同时也讨论了评估RAG系统中的关键挑战，并强调了未来研究的潜在方向。

Abstract: Large language models (LLMs) has become a significant research focus and is
utilized in various fields, such as text generation and dialog systems. One of
the most essential applications of LLM is Retrieval Augmented Generation (RAG),
which greatly enhances generated content's reliability and relevance. However,
evaluating RAG systems remains a challenging task. Traditional evaluation
metrics struggle to effectively capture the key features of modern
LLM-generated content that often exhibits high fluency and naturalness.
Inspired by the RAGAS tool, a well-known RAG evaluation framework, we extended
this framework into a KG-based evaluation paradigm, enabling multi-hop
reasoning and semantic community clustering to derive more comprehensive
scoring metrics. By incorporating these comprehensive evaluation criteria, we
gain a deeper understanding of RAG systems and a more nuanced perspective on
their performance. To validate the effectiveness of our approach, we compare
its performance with RAGAS scores and construct a human-annotated subset to
assess the correlation between human judgments and automated metrics. In
addition, we conduct targeted experiments to demonstrate that our KG-based
evaluation method is more sensitive to subtle semantic differences in generated
outputs. Finally, we discuss the key challenges in evaluating RAG systems and
highlight potential directions for future research.

</details>


### [46] [Transcribe, Translate, or Transliterate: An Investigation of Intermediate Representations in Spoken Language Models](https://arxiv.org/abs/2510.02569)
*Tolúl\d{o}pé Ògúnrèmí,Christopher D. Manning,Dan Jurafsky,Karen Livescu*

Main category: cs.CL

TL;DR: 探讨了语音语言模型中模态适配器 (MA) 的作用，发现它们使用不同的策略来转换表示：基于 Whisper 编码器的模型使用基于英语的中间语言表示意义，而其他模型则使用英语单词表示语音。


<details>
  <summary>Details</summary>
Motivation: 了解模态适配器 (MA) 如何转换表示，因为它们在将语音与大型语言模型 (LM) 集成中起着关键作用。

Method: 通过查找 MA 表示的最近解码器 LM 令牌来检查三种 SLM（SALMONN、Qwen2-Audio 和 Phi-4-Multimodal-Instruct）中的 MA 输出表示。

Result: 发现 MA 表示的两种策略：使用 Whisper 编码器的模型使用基于英语的中间语言表示意义；而像 Phi-4-Multimodal-Instruct 这样的模型则使用英语单词表示输入的语音。

Conclusion: MA 的策略取决于语音编码器是仅针对语音识别进行训练还是也针对翻译进行训练。

Abstract: Spoken language models (SLMs) that integrate speech with large language
models (LMs) rely on modality adapters (MAs) to map the output of speech
encoders to a representation that is understandable to the decoder LM. Yet we
know very little about how these crucial MAs transform representations. Here we
examine the MA output representation in three SLMs (SALMONN, Qwen2-Audio and
Phi-4-Multimodal-Instruct). By finding the nearest decoder LM token to an MA
representation, we uncover two strategies for MA representations. For models
using a Whisper encoder, MAs appear to represent the meaning of the input using
an English-based interlingua, allowing them to handle languages unseen in
instruction tuning. For models that don't, like Phi-4-Multimodal-Instruct, MAs
instead represent the phonetics of the input, but expressed with English words.
We hypothesise that which arises depends on whether the speech encoder is
trained only for speech recognition or also for translation.

</details>


### [47] [Evaluation Framework for Highlight Explanations of Context Utilisation in Language Models](https://arxiv.org/abs/2510.02629)
*Jingyi Sun,Pepa Atanasova,Sagnik Ray Choudhury,Sekh Mainul Islam,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 论文研究了语言模型(LM)利用上下文信息生成回复的能力，并指出用户无法确定模型是依赖参数记忆还是提供的上下文，也无法识别哪些上下文片段影响了回复。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对解释上下文利用的突出显示解释(HEs)有效性的评估。

Method: 论文提出了第一个用于上下文归因的黄金标准HE评估框架，使用具有已知真实上下文用法的受控测试用例。

Result: MechLight方法在所有上下文场景中表现最佳，但所有方法在较长的上下文中都存在困难，并表现出位置偏差。

Conclusion: 解释准确性方面存在根本性挑战，需要新的方法来提供可靠的上下文利用解释。

Abstract: Context utilisation, the ability of Language Models (LMs) to incorporate
relevant information from the provided context when generating responses,
remains largely opaque to users, who cannot determine whether models draw from
parametric memory or provided context, nor identify which specific context
pieces inform the response. Highlight explanations (HEs) offer a natural
solution as they can point the exact context pieces and tokens that influenced
model outputs. However, no existing work evaluates their effectiveness in
accurately explaining context utilisation. We address this gap by introducing
the first gold standard HE evaluation framework for context attribution, using
controlled test cases with known ground-truth context usage, which avoids the
limitations of existing indirect proxy evaluations. To demonstrate the
framework's broad applicability, we evaluate four HE methods -- three
established techniques and MechLight, a mechanistic interpretability approach
we adapt for this task -- across four context scenarios, four datasets, and
five LMs. Overall, we find that MechLight performs best across all context
scenarios. However, all methods struggle with longer contexts and exhibit
positional biases, pointing to fundamental challenges in explanation accuracy
that require new approaches to deliver reliable context utilisation
explanations at scale.

</details>


### [48] [Mind the Gap: Linguistic Divergence and Adaptation Strategies in Human-LLM Assistant vs. Human-Human Interactions](https://arxiv.org/abs/2510.02645)
*Fulei Zhang,Zhou Yu*

Main category: cs.CL

TL;DR: 用户与LLM聊天机器人和人类客服的沟通方式不同，用人类对话数据训练的模型可能无法适应这种变化。


<details>
  <summary>Details</summary>
Motivation: 探讨用户与LLM聊天机器人和人类客服沟通方式的不同之处，以及这种不同对LLM的影响。

Method: 分析用户在两种情境下的语言，包括语法流畅度、礼貌程度和词汇多样性，并尝试数据增强和推理时消息重构两种策略。

Result: 使用风格多样的数据集训练的模型明显优于仅使用原始或风格统一的数据集训练的模型，而推理时重构效果不佳。

Conclusion: 为了提升LLM用户交互体验，需要更好地调整模型以适应用户沟通方式的变化。

Abstract: As Large Language Models (LLMs) are increasingly deployed in customer-facing
applications, a critical yet underexplored question is how users communicate
differently with LLM chatbots compared to human agent. In this study, we
present empirical evidence that users adopt distinct communication styles when
users interact with chatbots versus human agents. Our analysis reveals
significant differences in grammatical fluency, politeness, and lexical
diversity in user language between the two settings. These findings suggest
that models trained exclusively on human-human interaction data may not
adequately accommodate the communication style shift that occurs once an LLM
chatbot is deployed. To enhance LLM robustness to post-launch communication
style changes, we experimented with two strategies: (1) data augmentation
during the post-training phase and (2) inference-time user message
reformulation. Our results indicate that models trained on stylistically
diverse datasets significantly outperform those trained exclusively on original
or stylistically uniform datasets, while inference-time reformulation proved
less effective. These insights help us to better adapt our models for improved
LLM-user interaction experiences.

</details>


### [49] [SoT: Structured-of-Thought Prompting Guides Multilingual Reasoning in Large Language Models](https://arxiv.org/abs/2510.02648)
*Rui Qi,Zhibo Man,Yufeng Chen,Fengran Mo,Jinan Xu,Kaiyu Huang*

Main category: cs.CL

TL;DR: 提出了一种名为SoT的无训练方法，通过多步骤转换来提高多语言推理的性能。


<details>
  <summary>Details</summary>
Motivation: 由于资源限制，推理能力未能成功转移到非高资源语言，这在多语言推理任务中面临挑战。

Method: SoT方法将特定于语言的语义信息转换为与语言无关的结构化表示，使模型能够更复杂地理解不同语言的查询。

Result: 实验结果表明，在适应各种LLM的backbone时，SoT在多个多语言推理基准测试中优于几个强大的基线。

Conclusion: SoT有效地引导LLM进行更集中的推理，以在处理跨语言表达的变异时保持一致的底层推理路径，并且可以与其他无训练策略集成以进一步改进。

Abstract: Recent developments have enabled Large Language Models (LLMs) to engage in
complex reasoning tasks through deep thinking. However, the capacity of
reasoning has not been successfully transferred to non-high-resource languages
due to resource constraints, which struggles with multilingual reasoning tasks.
To this end, we propose Structured-of-Thought (SoT), a training-free method
that improves the performance on multilingual reasoning through a multi-step
transformation: Language Thinking Transformation and Structured Knowledge
Transformation. The SoT method converts language-specific semantic information
into language-agnostic structured representations, enabling the models to
understand the query in different languages more sophisticated. Besides, SoT
effectively guides LLMs toward more concentrated reasoning to maintain
consistent underlying reasoning pathways when handling cross-lingual variations
in expression. Experimental results demonstrate that SoT outperforms several
strong baselines on multiple multilingual reasoning benchmarks when adapting to
various backbones of LLMs. It can also be integrated with other training-free
strategies for further improvements. Our code is available at
https://github.com/Cherry-qwq/SoT.

</details>


### [50] [Self-Improvement in Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2510.02665)
*Shijian Deng,Kai Wang,Tianyu Yang,Harsh Singh,Yapeng Tian*

Main category: cs.CL

TL;DR: 本文全面概述了多模态LLM (MLLM) 中的自我改进，从数据收集、数据组织和模型优化三个角度讨论了方法，并概述了开放的挑战和未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 的自我改进方面的最新进展有效地增强了模型的能力，而没有显着增加成本，尤其是在人力方面。将其扩展到多模态领域具有利用多样化数据源和开发更通用的自我改进模型的巨大潜力。

Method: 本文从数据收集、数据组织和模型优化三个角度，对当前文献进行了结构化概述。

Result: 本文全面概述了多模态LLM (MLLM) 中的自我改进。

Conclusion: 本文概述了开放的挑战和未来的研究方向，以促进MLLM自我改进的进一步发展。

Abstract: Recent advancements in self-improvement for Large Language Models (LLMs) have
efficiently enhanced model capabilities without significantly increasing costs,
particularly in terms of human effort. While this area is still relatively
young, its extension to the multimodal domain holds immense potential for
leveraging diverse data sources and developing more general self-improving
models. This survey is the first to provide a comprehensive overview of
self-improvement in Multimodal LLMs (MLLMs). We provide a structured overview
of the current literature and discuss methods from three perspectives: 1) data
collection, 2) data organization, and 3) model optimization, to facilitate the
further development of self-improvement in MLLMs. We also include commonly used
evaluations and downstream applications. Finally, we conclude by outlining open
challenges and future research directions.

</details>


### [51] [Uncertainty as Feature Gaps: Epistemic Uncertainty Quantification of LLMs in Contextual Question-Answering](https://arxiv.org/abs/2510.02671)
*Yavuz Bakman,Sungmin Kang,Zhiqi Huang,Duygu Nur Yaldiz,Catarina G. Belém,Chenyang Zhu,Anoop Kumar,Alfy Samuel,Salman Avestimehr,Daben Liu,Sai Praneeth Karimireddy*

Main category: cs.CL

TL;DR: 本文研究了上下文问答任务中的不确定性量化（UQ）问题，并提出了一种理论方法来量化认知不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有的不确定性量化研究主要集中在封闭领域的知识问答上，而忽略了上下文问答，但上下文问答在现实应用中非常重要。

Method: 1. 提出了一个与任务无关的token级别不确定性度量，即给定模型的预测分布和未知的真实分布之间的交叉熵。
2. 分解该度量，分离出认知成分，并通过一个完美提示的理想化模型来近似真实分布。
3. 推导出认知不确定性的上限，并表明它可以解释为给定模型的隐藏表示中相对于理想模型的语义特征差距。
4. 将该通用框架应用于上下文问答任务，并假设三个特征可以近似这个差距：上下文依赖性、上下文理解和诚实性。
5. 使用自上而下的可解释性方法，仅使用少量标记样本提取这些特征，并将它们集成以形成鲁棒的不确定性分数。

Result: 在多个QA基准测试中（包括同分布和异分布），该方法显著优于最先进的无监督和有监督的UQ方法，PRR提高了13个百分点，且推理开销可忽略不计。

Conclusion: 本文提出了一种新的上下文问答不确定性量化方法，该方法在多个基准测试中表现出色，且推理开销很小。

Abstract: Uncertainty Quantification (UQ) research has primarily focused on closed-book
factual question answering (QA), while contextual QA remains unexplored,
despite its importance in real-world applications. In this work, we focus on UQ
for the contextual QA task and propose a theoretically grounded approach to
quantify epistemic uncertainty. We begin by introducing a task-agnostic,
token-level uncertainty measure defined as the cross-entropy between the
predictive distribution of the given model and the unknown true distribution.
By decomposing this measure, we isolate the epistemic component and approximate
the true distribution by a perfectly prompted, idealized model. We then derive
an upper bound for epistemic uncertainty and show that it can be interpreted as
semantic feature gaps in the given model's hidden representations relative to
the ideal model. We further apply this generic framework to the contextual QA
task and hypothesize that three features approximate this gap: context-reliance
(using the provided context rather than parametric knowledge), context
comprehension (extracting relevant information from context), and honesty
(avoiding intentional lies). Using a top-down interpretability approach, we
extract these features by using only a small number of labeled samples and
ensemble them to form a robust uncertainty score. Experiments on multiple QA
benchmarks in both in-distribution and out-of-distribution settings show that
our method substantially outperforms state-of-the-art unsupervised
(sampling-free and sampling-based) and supervised UQ methods, achieving up to a
13-point PRR improvement while incurring a negligible inference overhead.

</details>


### [52] [Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks](https://arxiv.org/abs/2510.02712)
*Yubo Li,Ramayya Krishnan,Rema Padman*

Main category: cs.CL

TL;DR: 本文对大型语言模型在多轮对话中的鲁棒性进行了生存分析，发现突发语义漂移会导致对话失败，而渐进语义漂移则有助于延长对话。


<details>
  <summary>Details</summary>
Motivation: 现有评估框架无法捕捉真实交互中对话退化的时间动态。

Method: 采用Cox比例风险模型、加速失效时间模型和随机生存森林等生存建模方法。

Result: 发现突发语义漂移会增加对话失败的风险，而渐进语义漂移则会降低风险并延长对话。

Conclusion: 生存分析是评估LLM鲁棒性的有效方法，并为设计弹性对话代理提供了具体见解。

Abstract: Large Language Models (LLMs) have revolutionized conversational AI, yet their
robustness in extended multi-turn dialogues remains poorly understood. Existing
evaluation frameworks focus on static benchmarks and single-turn assessments,
failing to capture the temporal dynamics of conversational degradation that
characterize real-world interactions. In this work, we present the first
comprehensive survival analysis of conversational AI robustness, analyzing
36,951 conversation turns across 9 state-of-the-art LLMs to model failure as a
time-to-event process. Our survival modeling framework-employing Cox
proportional hazards, Accelerated Failure Time, and Random Survival Forest
approaches-reveals extraordinary temporal dynamics. We find that abrupt,
prompt-to-prompt(P2P) semantic drift is catastrophic, dramatically increasing
the hazard of conversational failure. In stark contrast, gradual, cumulative
drift is highly protective, vastly reducing the failure hazard and enabling
significantly longer dialogues. AFT models with interactions demonstrate
superior performance, achieving excellent discrimination and exceptional
calibration. These findings establish survival analysis as a powerful paradigm
for evaluating LLM robustness, offer concrete insights for designing resilient
conversational agents, and challenge prevailing assumptions about the necessity
of semantic consistency in conversational AI Systems.

</details>


### [53] [TravelBench : Exploring LLM Performance in Low-Resource Domains](https://arxiv.org/abs/2510.02719)
*Srinivas Billa,Xiaonan Jing*

Main category: cs.CL

TL;DR: 现有的LLM基准测试无法充分捕捉模型在低资源任务中的能力，阻碍了有效解决方案的开发。


<details>
  <summary>Details</summary>
Motivation: 为了解决低资源任务中LLM性能评估的挑战，作者构建了14个旅游领域的NLP数据集，涵盖7个常见NLP任务，并分析了LLM的性能。

Method: 使用来自真实场景的匿名数据，在各种任务中报告了LLM的准确性、扩展行为和推理能力。

Result: 结果表明，通用基准测试不足以理解模型在低资源任务中的性能。即使有大量的训练FLOPs，开箱即用的LLM在复杂的、特定领域的场景中也会遇到性能瓶颈。推理为较小的LLM提供了更显著的提升。

Conclusion: 推理可以使模型在某些任务上做出更好的判断，从而显著提升较小LLM的性能。

Abstract: Results on existing LLM benchmarks capture little information over the model
capabilities in low-resource tasks, making it difficult to develop effective
solutions in these domains. To address these challenges, we curated 14
travel-domain datasets spanning 7 common NLP tasks using anonymised data from
real-world scenarios, and analysed the performance across LLMs. We report on
the accuracy, scaling behaviour, and reasoning capabilities of LLMs in a
variety of tasks. Our results confirm that general benchmarking results are
insufficient for understanding model performance in low-resource tasks. Despite
the amount of training FLOPs, out-of-the-box LLMs hit performance bottlenecks
in complex, domain-specific scenarios. Furthermore, reasoning provides a more
significant boost for smaller LLMs by making the model a better judge on
certain tasks.

</details>


### [54] [PGMEL: Policy Gradient-based Generative Adversarial Network for Multimodal Entity Linking](https://arxiv.org/abs/2510.02726)
*KM Pooja,Cheng Long,Aixin Sun*

Main category: cs.CL

TL;DR: 本文提出了一种基于策略梯度的生成对抗网络PGMEL，用于多模态实体链接，通过生成高质量的负样本来提高链接性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态实体链接(MEL)技术忽略了高质量负样本选择的重要性。

Method: 利用生成对抗网络，生成器负责生成高质量的负样本，判别器负责度量学习任务，并使用策略梯度优化生成器。

Result: 在Wiki-MEL, Richpedia-MEL和WikiDiverse数据集上的实验结果表明，PGMEL通过选择具有挑战性的负样本学习有意义的表示，并优于现有方法。

Conclusion: PGMEL能够学习有意义的表示，并通过选择具有挑战性的负样本来提高多模态实体链接的性能。

Abstract: The task of entity linking, which involves associating mentions with their
respective entities in a knowledge graph, has received significant attention
due to its numerous potential applications. Recently, various multimodal entity
linking (MEL) techniques have been proposed, targeted to learn comprehensive
embeddings by leveraging both text and vision modalities. The selection of
high-quality negative samples can potentially play a crucial role in
metric/representation learning. However, to the best of our knowledge, this
possibility remains unexplored in existing literature within the framework of
MEL. To fill this gap, we address the multimodal entity linking problem in a
generative adversarial setting where the generator is responsible for
generating high-quality negative samples, and the discriminator is assigned the
responsibility for the metric learning tasks. Since the generator is involved
in generating samples, which is a discrete process, we optimize it using policy
gradient techniques and propose a policy gradient-based generative adversarial
network for multimodal entity linking (PGMEL). Experimental results based on
Wiki-MEL, Richpedia-MEL and WikiDiverse datasets demonstrate that PGMEL learns
meaningful representation by selecting challenging negative samples and
outperforms state-of-the-art methods.

</details>


### [55] [IndiCASA: A Dataset and Bias Evaluation Framework in LLMs Using Contrastive Embedding Similarity in the Indian Context](https://arxiv.org/abs/2510.02742)
*Santhosh G S,Akshay Govind S,Gokul S Krishnan,Balaraman Ravindran,Sriraam Natarajan*

Main category: cs.CL

TL;DR: 大型语言模型(LLM)因其强大的上下文理解和生成能力而在关键领域获得了显著关注。然而，它们在高风险应用中越来越多地部署，需要对嵌入的偏见进行严格评估，特别是在像印度这样文化多样的环境中，现有的基于嵌入的偏见评估方法通常不足以捕捉细微的刻板印象。


<details>
  <summary>Details</summary>
Motivation: 在文化多样的印度环境中，现有的基于嵌入的偏见评估方法通常不足以捕捉细微的刻板印象。因此，有必要对大型语言模型中嵌入的偏见进行严格评估。

Method: 提出了一种基于对比学习训练的编码器的评估框架，该框架通过嵌入相似性捕捉细粒度的偏见。引入了一个新的数据集 IndiCASA，该数据集包含 2,575 个经过人工验证的句子，涵盖五个人口统计轴：种姓、性别、宗教、残疾和社会经济地位。

Result: 对多个开放权重 LLM 的评估表明，所有模型都表现出一定程度的刻板印象偏见，其中与残疾相关的偏见尤其持久，而宗教偏见通常较低，这可能是由于全球去偏见努力。

Conclusion: 所有模型都表现出一定程度的刻板印象偏见，表明需要更公平的模型开发。

Abstract: Large Language Models (LLMs) have gained significant traction across critical
domains owing to their impressive contextual understanding and generative
capabilities. However, their increasing deployment in high stakes applications
necessitates rigorous evaluation of embedded biases, particularly in culturally
diverse contexts like India where existing embedding-based bias assessment
methods often fall short in capturing nuanced stereotypes. We propose an
evaluation framework based on a encoder trained using contrastive learning that
captures fine-grained bias through embedding similarity. We also introduce a
novel dataset - IndiCASA (IndiBias-based Contextually Aligned Stereotypes and
Anti-stereotypes) comprising 2,575 human-validated sentences spanning five
demographic axes: caste, gender, religion, disability, and socioeconomic
status. Our evaluation of multiple open-weight LLMs reveals that all models
exhibit some degree of stereotypical bias, with disability related biases being
notably persistent, and religion bias generally lower likely due to global
debiasing efforts demonstrating the need for fairer model development.

</details>


### [56] [The Path of Self-Evolving Large Language Models: Achieving Data-Efficient Learning via Intrinsic Feedback](https://arxiv.org/abs/2510.02752)
*Hangfan Zhang,Siyuan Xu,Zhimeng Guo,Huaisheng Zhu,Shicheng Liu,Xinrun Wang,Qiaosheng Zhang,Yang Chen,Peng Ye,Lei Bai,Shuyue Hu*

Main category: cs.CL

TL;DR: 本文提出了一种基于自知的强化学习方法，旨在利用少量数据提升大型语言模型（LLM）的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法通常需要大量的数据标注工作，为了解决这个问题。

Method: 该方法交替进行LLM的任务提出和解决，并引入了自知难度预测和自知极限突破两种机制来减少数据依赖。

Result: 在九个基准测试中，该方法使用不到1.2%的额外数据实现了53.8%的相对改进。

Conclusion: 自知强化学习是有效的，并突出了自进化智能体训练的潜力。

Abstract: Reinforcement learning (RL) has demonstrated potential in enhancing the
reasoning capabilities of large language models (LLMs), but such training
typically demands substantial efforts in creating and annotating data. In this
work, we explore improving LLMs through RL with minimal data. Our approach
alternates between the LLM proposing a task and then attempting to solve it. To
minimize data dependency, we introduce two novel mechanisms grounded in
self-awareness: (1) self-aware difficulty prediction, where the model learns to
assess task difficulty relative to its own abilities and prioritize challenging
yet solvable tasks, and (2) self-aware limit breaking, where the model
recognizes when a task is beyond its capability boundary and proactively
requests external data to break through that limit. Extensive experiments on
nine benchmarks showing a 53.8% relative improvement with less than 1.2% extra
data demonstrate the efficacy of self-aware RL and underscore the promise of
self-evolving agent training.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [57] [Exploring OCR-augmented Generation for Bilingual VQA](https://arxiv.org/abs/2510.02543)
*JoonHo Lee,Sunho Park*

Main category: cs.CV

TL;DR: 本文探讨了使用视觉语言模型 (VLM) 进行 OCR 增强生成， исследуя 韩语和英语的多语言任务。为了支持该领域的研究，我们训练并发布了 KLOCR，这是一个强大的双语 OCR 基线，在 1 亿个实例上进行训练，以增强 VLM 的 OCR 能力。为了补充现有的 VQA 基准，我们为韩语 VQA 管理 KOCRBench，并分析不同的提示方法。大量的实验表明，OCR 提取的文本显着提高了开源和商业模型的性能。我们的工作为双语 VQA 的 OCR 增强生成提供了新的见解。模型、代码和数据可在 https://github.com/JHLee0513/KLOCR 获取。


<details>
  <summary>Details</summary>
Motivation:  исследуя 韩语和英语的多语言任务

Method: 训练并发布了 KLOCR，这是一个强大的双语 OCR 基线，在 1 亿个实例上进行训练，以增强 VLM 的 OCR 能力。为了补充现有的 VQA 基准，我们为韩语 VQA 管理 KOCRBench，并分析不同的提示方法。

Result: OCR 提取的文本显着提高了开源和商业模型的性能

Conclusion: 我们的工作为双语 VQA 的 OCR 增强生成提供了新的见解。

Abstract: We investigate OCR-augmented generation with Vision Language Models (VLMs),
exploring tasks in Korean and English toward multilingualism. To support
research in this domain, we train and release KLOCR, a strong bilingual OCR
baseline trained on 100M instances to augment VLMs with OCR ability. To
complement existing VQA benchmarks, we curate KOCRBench for Korean VQA, and
analyze different prompting methods. Extensive experiments show that
OCR-extracted text significantly boosts performance across open source and
commercial models. Our work offers new insights into OCR-augmented generation
for bilingual VQA. Model, code, and data are available at
https://github.com/JHLee0513/KLOCR.

</details>


### [58] [Oracle-RLAIF: An Improved Fine-Tuning Framework for Multi-modal Video Models through Reinforcement Learning from Ranking Feedback](https://arxiv.org/abs/2510.02561)
*Derek Shi,Ruben Glatt,Christine Klymko,Shubham Mohole,Hongjun Choi,Shashank Kushwaha,Sam Sakla,Felipe Leno da Silva*

Main category: cs.CV

TL;DR: 提出了一种新的视频语言模型微调框架Oracle-RLAIF，该框架使用通用Oracle排序器代替人工或AI反馈，并结合新的排序损失函数GRPO_{rank}，在视频理解基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视频语言模型微调方法依赖于大量的人工反馈或昂贵的AI奖励模型，成本高昂且数据效率低。

Method: 提出Oracle-RLAIF框架，使用通用Oracle排序器对候选模型响应进行排序，并引入基于GRPO的排序损失函数GRPO_{rank}，直接优化排序反馈。

Result: 实验表明，Oracle-RLAIF在各种视频理解基准测试中始终优于使用现有微调方法的领先VLM。

Conclusion: Oracle-RLAIF为创建灵活且数据高效的框架铺平了道路，可以使用强化学习从排序而不是分数来对齐大型多模态视频模型。

Abstract: Recent advances in large video-language models (VLMs) rely on extensive
fine-tuning techniques that strengthen alignment between textual and visual
comprehension. Leading pipelines typically pair supervised fine-tuning (SFT)
with reinforcement learning from preference data to enhance video
comprehension. However, as VLMs scale in parameter size, so does the cost of
gathering enough human feedback. To make fine-tuning more cost-effective,
recent frameworks explore reinforcement learning with AI feedback (RLAIF),
which replace human preference with AI as a judge. Current RLAIF frameworks
rely on a specialized reward model trained with video narratives to create
calibrated scalar rewards -- an expensive and restrictive pipeline. We propose
Oracle-RLAIF, a novel framework that replaces the trained reward model with a
more general Oracle ranker which acts as a drop-in model ranking candidate
model responses rather than scoring them. Alongside Oracle-RLAIF, we introduce
$GRPO_{rank}$, a novel rank-based loss function based on Group Relative Policy
Optimization (GRPO) that directly optimizes ordinal feedback with rank-aware
advantages. Empirically, we demonstrate that Oracle-RLAIF consistently
outperforms leading VLMs using existing fine-tuning methods when evaluated
across various video comprehension benchmarks. Oracle-RLAIF paves the path to
creating flexible and data-efficient frameworks for aligning large multi-modal
video models with reinforcement learning from rank rather than score.

</details>


### [59] [PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction](https://arxiv.org/abs/2510.02566)
*Qiao Feng,Yiming Huang,Yufu Wang,Jiatao Gu,Lingjie Liu*

Main category: cs.CV

TL;DR: PhysHMR: A unified framework for physically plausible human motion reconstruction from monocular videos.


<details>
  <summary>Details</summary>
Motivation: Existing methods often lead to unrealistic results due to the lack of physical constraints, and two-stage designs introduce error accumulation.

Method: A visual-to-action policy is learned for humanoid control in a physics-based simulator, using a pixel-as-ray strategy to lift 2D keypoints into 3D spatial rays and incorporating them as policy inputs.

Result: PhysHMR produces high-fidelity, physically plausible motion across diverse scenarios, outperforming prior approaches in both visual accuracy and physical realism.

Conclusion: The proposed PhysHMR framework effectively reconstructs human motion from monocular videos by directly learning a physics-based control policy.

Abstract: Reconstructing physically plausible human motion from monocular videos
remains a challenging problem in computer vision and graphics. Existing methods
primarily focus on kinematics-based pose estimation, often leading to
unrealistic results due to the lack of physical constraints. To address such
artifacts, prior methods have typically relied on physics-based post-processing
following the initial kinematics-based motion estimation. However, this
two-stage design introduces error accumulation, ultimately limiting the overall
reconstruction quality. In this paper, we present PhysHMR, a unified framework
that directly learns a visual-to-action policy for humanoid control in a
physics-based simulator, enabling motion reconstruction that is both physically
grounded and visually aligned with the input video. A key component of our
approach is the pixel-as-ray strategy, which lifts 2D keypoints into 3D spatial
rays and transforms them into global space. These rays are incorporated as
policy inputs, providing robust global pose guidance without depending on noisy
3D root predictions. This soft global grounding, combined with local visual
features from a pretrained encoder, allows the policy to reason over both
detailed pose and global positioning. To overcome the sample inefficiency of
reinforcement learning, we further introduce a distillation scheme that
transfers motion knowledge from a mocap-trained expert to the
vision-conditioned policy, which is then refined using physically motivated
reinforcement learning rewards. Extensive experiments demonstrate that PhysHMR
produces high-fidelity, physically plausible motion across diverse scenarios,
outperforming prior approaches in both visual accuracy and physical realism.

</details>


### [60] [Unlocking the power of partnership: How humans and machines can work together to improve face recognition](https://arxiv.org/abs/2510.02570)
*P. Jonathon Phillips,Geraldine Jeckeln,Carina A. Hahn,Amy N. Yates,Peter C. Fontana,Alice J. O'Toole*

Main category: cs.CV

TL;DR: 该研究探讨了人脸识别算法中人机协作的有效性，发现当合作者之间的基线准确度差异减小时，协作收益会增加。


<details>
  <summary>Details</summary>
Motivation: 研究人脸识别算法中人机协作如何提高准确性，以及个体差异对协作效果的影响。

Method: 通过分析专家和非专家人脸识别者的数据，比较人与人以及人与机器协作的收益，并根据近端准确度规则(PAR)建立了一个关键融合区。

Result: 发现当人类的准确率略低于机器时，人机融合可以提高系统准确率；智能人机融合比单独使用机器或简单结合所有人类和机器的判断更准确。图论方法可以实现最高的人类系统准确率，但智能人机协作能更有效地减少低水平人类的影响。

Conclusion: 人机协作在确保准确的人脸识别方面发挥着重要作用，并为在人脸识别中智能使用人工智能提供了循证路线图。

Abstract: Human review of consequential decisions by face recognition algorithms
creates a "collaborative" human-machine system. Individual differences between
people and machines, however, affect whether collaboration improves or degrades
accuracy in any given case. We establish the circumstances under which
combining human and machine face identification decisions improves accuracy.
Using data from expert and non-expert face identifiers, we examined the
benefits of human-human and human-machine collaborations. The benefits of
collaboration increased as the difference in baseline accuracy between
collaborators decreased-following the Proximal Accuracy Rule (PAR). This rule
predicted collaborative (fusion) benefit across a wide range of baseline
abilities, from people with no training to those with extensive training. Using
the PAR, we established a critical fusion zone, where humans are less accurate
than the machine, but fusing the two improves system accuracy. This zone was
surprisingly large. We implemented "intelligent human-machine fusion" by
selecting people with the potential to increase the accuracy of a
high-performing machine. Intelligent fusion was more accurate than the machine
operating alone and more accurate than combining all human and machine
judgments. The highest system-wide accuracy achievable with human-only
partnerships was found by graph theory. This fully human system approximated
the average performance achieved by intelligent human-machine collaboration.
However, intelligent human-machine collaboration more effectively minimized the
impact of low-performing humans on system-wide accuracy. The results
demonstrate a meaningful role for both humans and machines in assuring accurate
face identification. This study offers an evidence-based road map for the
intelligent use of AI in face identification.

</details>


### [61] [How Confident are Video Models? Empowering Video Models to Express their Uncertainty](https://arxiv.org/abs/2510.02571)
*Zhiting Mei,Ola Shorinwa,Anirudha Majumdar*

Main category: cs.CV

TL;DR: 本文是第一个量化视频模型不确定性的工作。


<details>
  <summary>Details</summary>
Motivation: 视频生成模型容易产生幻觉，但目前没有针对视频模型的不确定性量化 (UQ) 方法，存在安全隐患。

Method: 提出了一个视频模型不确定性量化框架，包括：(i) 一个用于评估视频模型校准的指标；(ii) 一个黑盒 UQ 方法 (S-QUBED)，利用潜在建模将预测不确定性分解为偶然性和认知性成分；(iii) 一个 UQ 数据集，以促进视频模型中的基准校准。

Result: 通过在基准视频数据集上的大量实验，证明 S-QUBED 计算出的校准总不确定性估计与任务准确率负相关，并有效计算偶然性和认知成分。

Conclusion: S-QUBED 可以有效量化视频生成模型的不确定性，并分解为偶然性和认知性成分。

Abstract: Generative video models demonstrate impressive text-to-video capabilities,
spurring widespread adoption in many real-world applications. However, like
large language models (LLMs), video generation models tend to hallucinate,
producing plausible videos even when they are factually wrong. Although
uncertainty quantification (UQ) of LLMs has been extensively studied in prior
work, no UQ method for video models exists, raising critical safety concerns.
To our knowledge, this paper represents the first work towards quantifying the
uncertainty of video models. We present a framework for uncertainty
quantification of generative video models, consisting of: (i) a metric for
evaluating the calibration of video models based on robust rank correlation
estimation with no stringent modeling assumptions; (ii) a black-box UQ method
for video models (termed S-QUBED), which leverages latent modeling to
rigorously decompose predictive uncertainty into its aleatoric and epistemic
components; and (iii) a UQ dataset to facilitate benchmarking calibration in
video models. By conditioning the generation task in the latent space, we
disentangle uncertainty arising due to vague task specifications from that
arising from lack of knowledge. Through extensive experiments on benchmark
video datasets, we demonstrate that S-QUBED computes calibrated total
uncertainty estimates that are negatively correlated with the task accuracy and
effectively computes the aleatoric and epistemic constituents.

</details>


### [62] [PEO: Training-Free Aesthetic Quality Enhancement in Pre-Trained Text-to-Image Diffusion Models with Prompt Embedding Optimization](https://arxiv.org/abs/2510.02599)
*Hovhannes Margaryan,Bo Wan,Tinne Tuytelaars*

Main category: cs.CV

TL;DR: 提出了一种新的方法，通过优化文本嵌入来提高预训练文本到图像扩散模型生成图像的美学质量。


<details>
  <summary>Details</summary>
Motivation: 提高文本到图像扩散模型生成图像的美学质量。

Method: 使用预训练的文本到图像扩散模型作为主干，并优化给定简单提示的文本嵌入，通过三方目标函数（提高生成图像的美学保真度、确保符合优化的文本嵌入、并最大限度地减少与初始提示的差异）来实现。

Result: 定量和定性评估证实了该方法的有效性，超过或等同于最先进的文本到图像和提示适应方法的性能。

Conclusion: PEO 是一种免训练且独立于主干的方法，可以有效提高生成图像的美学质量。

Abstract: This paper introduces a novel approach to aesthetic quality improvement in
pre-trained text-to-image diffusion models when given a simple prompt. Our
method, dubbed Prompt Embedding Optimization (PEO), leverages a pre-trained
text-to-image diffusion model as a backbone and optimizes the text embedding of
a given simple and uncurated prompt to enhance the visual quality of the
generated image. We achieve this by a tripartite objective function that
improves the aesthetic fidelity of the generated image, ensures adherence to
the optimized text embedding, and minimal divergence from the initial prompt.
The latter is accomplished through a prompt preservation term. Additionally,
PEO is training-free and backbone-independent. Quantitative and qualitative
evaluations confirm the effectiveness of the proposed method, exceeding or
equating the performance of state-of-the-art text-to-image and prompt
adaptation methods.

</details>


### [63] [Ego-Exo 3D Hand Tracking in the Wild with a Mobile Multi-Camera Rig](https://arxiv.org/abs/2510.02601)
*Patrick Rim,Kun He,Kevin Harris,Braden Copple,Shangchen Han,Sizhe An,Ivan Shugurov,Tomas Hodan,He Wen,Xu Xie*

Main category: cs.CV

TL;DR: 提出了一种新的多摄像头系统，用于在无约束条件下捕捉精确的3D手部和物体，从而解决现有数据集在环境多样性和模型泛化方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 在非约束环境中准确跟踪手部及其与世界的交互仍然是一个巨大的挑战，现有的数据集主要在受控实验室环境中捕获，限制了环境多样性和模型泛化。

Method: 设计了一种新的无标记多摄像头系统，结合了轻量级背负式捕捉装置、八个外视摄像头和一个用户佩戴的 Meta Quest 3 头显，并设计了一个 ego-exo 跟踪流程，以生成精确的 3D 手部姿势 ground truth。

Result: 收集了一个带注释的数据集，其中包含同步的多视图图像和精确的 3D 手部姿势，证明了该方法能够显著减少环境真实性和 3D 注释准确性之间的权衡。

Conclusion: 该方法能够显著减少环境真实性和 3D 注释准确性之间的权衡

Abstract: Accurate 3D tracking of hands and their interactions with the world in
unconstrained settings remains a significant challenge for egocentric computer
vision. With few exceptions, existing datasets are predominantly captured in
controlled lab setups, limiting environmental diversity and model
generalization. To address this, we introduce a novel marker-less multi-camera
system designed to capture precise 3D hands and objects, which allows for
nearly unconstrained mobility in genuinely in-the-wild conditions. We combine a
lightweight, back-mounted capture rig with eight exocentric cameras, and a
user-worn Meta Quest 3 headset, which contributes two egocentric views. We
design an ego-exo tracking pipeline to generate accurate 3D hand pose ground
truth from this system, and rigorously evaluate its quality. By collecting an
annotated dataset featuring synchronized multi-view images and precise 3D hand
poses, we demonstrate the capability of our approach to significantly reduce
the trade-off between environmental realism and 3D annotation accuracy.

</details>


### [64] [Input-Aware Sparse Attention for Real-Time Co-Speech Video Generation](https://arxiv.org/abs/2510.02617)
*Beijia Lu,Ziyi Chen,Jing Xiao,Jun-Yan Zhu*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的视频提炼方法，以加速基于扩散模型的语音合成视频生成，使其能够实时部署。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散模型的语音合成视频方法速度慢，无法实时部署。

Method: 该方法利用输入的人体姿势信息来调节注意力和损失函数，提出了输入感知稀疏注意力和输入感知提炼损失。

Result: 该方法实现了实时性能，并提高了视觉质量。

Conclusion: 通过整合输入感知稀疏注意力和提炼损失，该方法在视觉质量上优于现有的音频驱动和输入驱动方法，并进行了大量实验验证了算法设计的有效性。

Abstract: Diffusion models can synthesize realistic co-speech video from audio for
various applications, such as video creation and virtual agents. However,
existing diffusion-based methods are slow due to numerous denoising steps and
costly attention mechanisms, preventing real-time deployment. In this work, we
distill a many-step diffusion video model into a few-step student model.
Unfortunately, directly applying recent diffusion distillation methods degrades
video quality and falls short of real-time performance. To address these
issues, our new video distillation method leverages input human pose
conditioning for both attention and loss functions. We first propose using
accurate correspondence between input human pose keypoints to guide attention
to relevant regions, such as the speaker's face, hands, and upper body. This
input-aware sparse attention reduces redundant computations and strengthens
temporal correspondences of body parts, improving inference efficiency and
motion coherence. To further enhance visual quality, we introduce an
input-aware distillation loss that improves lip synchronization and hand motion
realism. By integrating our input-aware sparse attention and distillation loss,
our method achieves real-time performance with improved visual quality compared
to recent audio-driven and input-driven methods. We also conduct extensive
experiments showing the effectiveness of our algorithmic design choices.

</details>


### [65] [Deep Generative Continual Learning using Functional LoRA: FunLoRA](https://arxiv.org/abs/2510.02631)
*Victor Enescu,Hichem Sahbi*

Main category: cs.CV

TL;DR: 提出了一种新的基于低秩适应（LoRA）的生成模型条件机制，称为 FunLoRA，以解决持续适应深度生成模型中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 在文本和视觉应用中，深度生成模型的持续适应至关重要。然而，由于灾难性遗忘现象，增量训练仍然具有挑战性。现有的基于合成数据的方法存在训练时间过长和长期性能下降的问题。

Method: 设计了一种新的、更具表现力的生成模型条件机制，基于低秩适应（LoRA），使用秩为 1 的矩阵，并通过精心选择的函数增加矩阵秩，称为 FunLoRA。这种动态条件机制保证了生成模型避免灾难性遗忘，并且只需要在当前任务的数据上进行训练。

Result: 在基于流匹配的模型上进行了大量实验，结果表明，所提出的参数高效微调（PEFT）方法超越了基于扩散模型的现有最佳结果，达到了更高的分类准确率，同时只需要一小部分内存成本和采样时间。

Conclusion: FunLoRA 是一种有效的参数高效微调方法，可以解决持续适应深度生成模型中的灾难性遗忘问题，并在分类准确率、内存成本和采样时间方面优于现有方法。

Abstract: Continual adaptation of deep generative models holds tremendous potential and
critical importance, given their rapid and expanding usage in text and vision
based applications. Incremental training, however, remains highly challenging
due to catastrophic forgetting phenomenon, which makes it difficult for neural
networks to effectively incorporate new knowledge. A common strategy consists
in retraining the generative model on its own synthetic data in order to
mitigate forgetting. Yet, such an approach faces two major limitations: (i) the
continually increasing training time eventually becomes intractable, and (ii)
reliance on synthetic data inevitably leads to long-term performance
degradation, since synthetic samples lack the richness of real training data.
In this paper, we attenuate these issues by designing a novel and more
expressive conditioning mechanism for generative models based on low rank
adaptation (LoRA), that exclusively employs rank 1 matrices, whose
reparametrized matrix rank is functionally increased using carefully selected
functions -- and dubbed functional LoRA: FunLoRA. Using this dynamic
conditioning, the generative model is guaranteed to avoid catastrophic
forgetting and needs only to be trained on data from the current task.
Extensive experiments using flow-matching based models trained from scratch,
showcase that our proposed parameter-efficient fine-tuning (PEFT) method
surpasses prior state-of-the-art results based on diffusion models, reaching
higher classification accuracy scores, while only requiring a fraction of the
memory cost and sampling time.

</details>


### [66] [Sequence-Preserving Dual-FoV Defense for Traffic Sign and Light Recognition in Autonomous Vehicles](https://arxiv.org/abs/2510.02642)
*Abhishek Joshi,Jahnavi Krishna Koda,Abhishek Phadke*

Main category: cs.CV

TL;DR: 提出了一种双视场、序列保持的鲁棒性框架，用于美国的交通灯和标志识别，该框架基于多源数据集，并结合了特征挤压、防御蒸馏和基于熵的异常检测等技术。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆中的交通灯和标志识别至关重要，因为感知错误会直接影响导航和安全性。模型容易受到现有扰动（眩光、雨水、污垢或涂鸦）以及数字对抗攻击的影响，从而可能导致危险的错误分类。

Method: 提出了一个统一的三层防御堆栈框架，该框架结合了特征挤压、防御蒸馏和基于熵的异常检测，以及序列方面的时序投票。

Result: 统一防御堆栈实现了79.8mAP，并将攻击成功率降低至18.2%，同时将高风险错误分类降低至32%。

Conclusion: 该研究提出的统一防御堆栈框架在交通灯和标志识别方面表现优于YOLOv8、YOLOv9和BEVFormer。

Abstract: Traffic light and sign recognition are key for Autonomous Vehicles (AVs)
because perception mistakes directly influence navigation and safety. In
addition to digital adversarial attacks, models are vulnerable to existing
perturbations (glare, rain, dirt, or graffiti), which could lead to dangerous
misclassifications. The current work lacks consideration of temporal
continuity, multistatic field-of-view (FoV) sensing, and robustness to both
digital and natural degradation. This study proposes a dual FoV,
sequence-preserving robustness framework for traffic lights and signs in the
USA based on a multi-source dataset built on aiMotive, Udacity, Waymo, and
self-recorded videos from the region of Texas. Mid and long-term sequences of
RGB images are temporally aligned for four operational design domains (ODDs):
highway, night, rainy, and urban. Over a series of experiments on a real-life
application of anomaly detection, this study outlines a unified three-layer
defense stack framework that incorporates feature squeezing, defensive
distillation, and entropy-based anomaly detection, as well as sequence-wise
temporal voting for further enhancement. The evaluation measures included
accuracy, attack success rate (ASR), risk-weighted misclassification severity,
and confidence stability. Physical transferability was confirmed using probes
for recapture. The results showed that the Unified Defense Stack achieved
79.8mAP and reduced the ASR to 18.2%, which is superior to YOLOv8, YOLOv9, and
BEVFormer, while reducing the high-risk misclassification to 32%.

</details>


### [67] [Smart-GRPO: Smartly Sampling Noise for Efficient RL of Flow-Matching Models](https://arxiv.org/abs/2510.02654)
*Benjamin Yu,Jackie Liu,Justin Cui*

Main category: cs.CV

TL;DR: 提出了一种名为Smart-GRPO的新方法，用于优化Flow-Matching模型中强化学习的噪声扰动。


<details>
  <summary>Details</summary>
Motivation: Flow-Matching模型的确定性使其不太适合强化学习，而强化学习是提高图像质量和人类对齐的关键工具。先前的研究通过随机噪声扰动潜在变量引入了随机性，但这种扰动效率低下且不稳定。

Method: 采用迭代搜索策略，解码候选扰动，使用奖励函数评估它们，并将噪声分布细化到更高奖励的区域。

Result: 与基线方法相比，Smart-GRPO提高了奖励优化和视觉质量。

Conclusion: Smart-GRPO为Flow-Matching框架中的强化学习提供了一条实用的路径，弥合了高效训练和人类对齐生成之间的差距。

Abstract: Recent advancements in flow-matching have enabled high-quality text-to-image
generation. However, the deterministic nature of flow-matching models makes
them poorly suited for reinforcement learning, a key tool for improving image
quality and human alignment. Prior work has introduced stochasticity by
perturbing latents with random noise, but such perturbations are inefficient
and unstable. We propose Smart-GRPO, the first method to optimize noise
perturbations for reinforcement learning in flow-matching models. Smart-GRPO
employs an iterative search strategy that decodes candidate perturbations,
evaluates them with a reward function, and refines the noise distribution
toward higher-reward regions. Experiments demonstrate that Smart-GRPO improves
both reward optimization and visual quality compared to baseline methods. Our
results suggest a practical path toward reinforcement learning in flow-matching
frameworks, bridging the gap between efficient training and human-aligned
generation.

</details>


### [68] [FSFSplatter: Build Surface and Novel Views with Sparse-Views within 3min](https://arxiv.org/abs/2510.02691)
*Yibin Zhao,Yihan Pan,Jun Nan,Jianjun Yi*

Main category: cs.CV

TL;DR: FSFSplatter: A novel approach for fast surface reconstruction from free sparse images.


<details>
  <summary>Details</summary>
Motivation: Reconstructing from free sparse images often leads to poor surface due to limited overlap and overfitting.

Method: Integrates end-to-end dense Gaussian initialization, camera parameter estimation, and geometry-enhanced scene optimization. Employs a large Transformer to encode multi-view images and generates a dense and geometrically consistent Gaussian scene initialization via a self-splitting Gaussian head. It eliminates local floaters through contribution-based pruning and mitigates overfitting to limited views by leveraging depth and multi-view feature supervision with differentiable camera parameters during rapid optimization.

Result: FSFSplatter outperforms current state-of-the-art methods on widely used DTU and Replica.

Conclusion: Introduces FSFSplatter, a new approach for fast surface reconstruction from free sparse images, outperforming current state-of-the-art methods.

Abstract: Gaussian Splatting has become a leading reconstruction technique, known for
its high-quality novel view synthesis and detailed reconstruction. However,
most existing methods require dense, calibrated views. Reconstructing from free
sparse images often leads to poor surface due to limited overlap and
overfitting. We introduce FSFSplatter, a new approach for fast surface
reconstruction from free sparse images. Our method integrates end-to-end dense
Gaussian initialization, camera parameter estimation, and geometry-enhanced
scene optimization. Specifically, FSFSplatter employs a large Transformer to
encode multi-view images and generates a dense and geometrically consistent
Gaussian scene initialization via a self-splitting Gaussian head. It eliminates
local floaters through contribution-based pruning and mitigates overfitting to
limited views by leveraging depth and multi-view feature supervision with
differentiable camera parameters during rapid optimization. FSFSplatter
outperforms current state-of-the-art methods on widely used DTU and Replica.

</details>


### [69] [MoGIC: Boosting Motion Generation via Intention Understanding and Visual Context](https://arxiv.org/abs/2510.02722)
*Junyu Shi,Yong Sun,Zhiyuan Zhang,Lijiang Liu,Zhengjie Zhang,Yuxin He,Qiang Nie*

Main category: cs.CV

TL;DR: MoGIC是一个统一的框架，它将意图建模和视觉先验整合到多模态运动合成中，通过联合优化多模态条件运动生成和意图预测，MoGIC揭示了潜在的人类目标，利用视觉先验来增强生成，并展示了通用的多模态生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有的文本驱动的运动生成方法通常将合成视为语言和运动之间的双向映射，但在捕捉动作执行的因果逻辑和驱动行为的人类意图方面仍然有限。缺乏视觉基础进一步限制了精度和个性化，因为仅凭语言无法指定细粒度的时空细节。

Method: MoGIC集成了意图建模和视觉先验，并引入了一种具有自适应范围的混合注意力机制，以实现条件tokens和运动子序列之间的有效局部对齐。

Result: 实验表明，经过微调后，MoGIC在HumanML3D上将FID降低了38.6％，在Mo440H上将FID降低了34.6％，通过轻量级文本头在运动字幕方面超越了基于LLM的方法，并进一步实现了意图预测和视觉条件生成。

Conclusion: MoGIC推进了可控运动合成和意图理解。

Abstract: Existing text-driven motion generation methods often treat synthesis as a
bidirectional mapping between language and motion, but remain limited in
capturing the causal logic of action execution and the human intentions that
drive behavior. The absence of visual grounding further restricts precision and
personalization, as language alone cannot specify fine-grained spatiotemporal
details. We propose MoGIC, a unified framework that integrates intention
modeling and visual priors into multimodal motion synthesis. By jointly
optimizing multimodal-conditioned motion generation and intention prediction,
MoGIC uncovers latent human goals, leverages visual priors to enhance
generation, and exhibits versatile multimodal generative capability. We further
introduce a mixture-of-attention mechanism with adaptive scope to enable
effective local alignment between conditional tokens and motion subsequences.
To support this paradigm, we curate Mo440H, a 440-hour benchmark from 21
high-quality motion datasets. Experiments show that after finetuning, MoGIC
reduces FID by 38.6\% on HumanML3D and 34.6\% on Mo440H, surpasses LLM-based
methods in motion captioning with a lightweight text head, and further enables
intention prediction and vision-conditioned generation, advancing controllable
motion synthesis and intention understanding. The code is available at
https://github.com/JunyuShi02/MoGIC

</details>


### [70] [From Tokens to Nodes: Semantic-Guided Motion Control for Dynamic 3D Gaussian Splatting](https://arxiv.org/abs/2510.02732)
*Jianing Chen,Zehao Li,Yujun Cai,Hao Jiang,Shuqin Gao,Honglong Zhao,Tianlu Mao,Yucheng Zhang*

Main category: cs.CV

TL;DR: 提出了一种运动自适应的单目视频动态3D重建框架，通过对动态区域集中控制点，同时抑制静态背景中的冗余，提高了重建质量和效率。


<details>
  <summary>Details</summary>
Motivation: 从单目视频中进行动态3D重建仍然很困难，因为从有限的视角推断3D运动的模糊性以及对时间变化场景进行建模的计算需求。

Method: 利用视觉基础模型的语义和运动先验，建立patch-token-node对应关系，并应用运动自适应压缩来集中动态区域中的控制点，同时抑制静态背景中的冗余。通过迭代体素化和运动趋势评分实现灵活的表示密度自适应，并引入由2D轨迹初始化的基于样条的轨迹参数化，以实现更平滑的运动表示和更稳定的优化。

Result: 在重建质量和效率方面，相比现有方法有显著提高。

Conclusion: 该方法通过运动自适应框架对单目视频进行动态3D重建，有效提高了重建质量和效率。

Abstract: Dynamic 3D reconstruction from monocular videos remains difficult due to the
ambiguity inferring 3D motion from limited views and computational demands of
modeling temporally varying scenes. While recent sparse control methods
alleviate computation by reducing millions of Gaussians to thousands of control
points, they suffer from a critical limitation: they allocate points purely by
geometry, leading to static redundancy and dynamic insufficiency. We propose a
motion-adaptive framework that aligns control density with motion complexity.
Leveraging semantic and motion priors from vision foundation models, we
establish patch-token-node correspondences and apply motion-adaptive
compression to concentrate control points in dynamic regions while suppressing
redundancy in static backgrounds. Our approach achieves flexible
representational density adaptation through iterative voxelization and motion
tendency scoring, directly addressing the fundamental mismatch between control
point allocation and motion complexity. To capture temporal evolution, we
introduce spline-based trajectory parameterization initialized by 2D tracklets,
replacing MLP-based deformation fields to achieve smoother motion
representation and more stable optimization. Extensive experiments demonstrate
significant improvements in reconstruction quality and efficiency over existing
state-of-the-art methods.

</details>


### [71] [Net2Net: When Un-trained Meets Pre-trained Networks for Robust Real-World Denoising](https://arxiv.org/abs/2510.02733)
*Weimin Yuan,Cai Meng*

Main category: cs.CV

TL;DR: 提出了一种名为Net2Net的创新方法，结合了未训练和预训练网络的优势，以应对真实世界噪声去除的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统去噪方法在受控环境中表现良好，但难以解决真实噪声的复杂性和可变性。深度学习方法虽然可以从大型数据集中学习噪声特征，但需要大量的标记数据，并且可能无法有效地推广到不同的噪声类型和成像条件。

Method: 通过去噪正则化（RED）结合无监督DIP和有监督预训练模型DRUNet。

Result: 在基准数据集上的大量实验表明，该方法在真实世界噪声去除方面具有优越性。

Conclusion: 该混合框架增强了在不同噪声模式下的泛化能力，并提高了性能，尤其是在训练数据有限的情况下。

Abstract: Traditional denoising methods for noise removal have largely relied on
handcrafted priors, often perform well in controlled environments but struggle
to address the complexity and variability of real noise. In contrast, deep
learning-based approaches have gained prominence for learning noise
characteristics from large datasets, but these methods frequently require
extensive labeled data and may not generalize effectively across diverse noise
types and imaging conditions. In this paper, we present an innovative method,
termed as Net2Net, that combines the strengths of untrained and pre-trained
networks to tackle the challenges of real-world noise removal. The innovation
of Net2Net lies in its combination of unsupervised DIP and supervised
pre-trained model DRUNet by regularization by denoising (RED). The untrained
network adapts to the unique noise characteristics of each input image without
requiring labeled data, while the pre-trained network leverages learned
representations from large-scale datasets to deliver robust denoising
performance. This hybrid framework enhances generalization across varying noise
patterns and improves performance, particularly in scenarios with limited
training data. Extensive experiments on benchmark datasets demonstrate the
superiority of our method for real-world noise removal.

</details>


### [72] [Retrv-R1: A Reasoning-Driven MLLM Framework for Universal and Efficient Multimodal Retrieval](https://arxiv.org/abs/2510.02745)
*Lanyun Zhu,Deyi Ji,Tianrun Chen,Haiyang Wu,Shiqi Wang*

Main category: cs.CV

TL;DR: Retrv-R1 achieves state-of-the-art retrieval performance with high efficiency and strong generalization ability.


<details>
  <summary>Details</summary>
Motivation: To enhance LLMs' reasoning capabilities for multimodal universal retrieval.

Method: Introducing an information compression module with a details inspection mechanism and a new training paradigm including an activation stage and RL with a novel curriculum reward.

Result: Retrv-R1 achieves SOTA performance, high efficiency, and strong generalization ability.

Conclusion: Retrv-R1 is the first R1-style MLLM specifically designed for multimodal universal retrieval.

Abstract: The success of DeepSeek-R1 demonstrates the immense potential of using
reinforcement learning (RL) to enhance LLMs' reasoning capabilities. This paper
introduces Retrv-R1, the first R1-style MLLM specifically designed for
multimodal universal retrieval, achieving higher performance by employing
step-by-step reasoning to produce more accurate retrieval results. We find that
directly applying the methods of DeepSeek-R1 to retrieval tasks is not
feasible, mainly due to (1) the high computational cost caused by the large
token consumption required for multiple candidates with reasoning processes,
and (2) the instability and suboptimal results when directly applying RL to
train for retrieval tasks. To address these issues, Retrv-R1 introduces an
information compression module with a details inspection mechanism, which
enhances computational efficiency by reducing the number of tokens while
ensuring that critical information for challenging candidates is preserved.
Furthermore, a new training paradigm is proposed, including an activation stage
using a retrieval-tailored synthetic CoT dataset for more effective
optimization, followed by RL with a novel curriculum reward to improve both
performance and efficiency. Incorporating these novel designs, Retrv-R1
achieves SOTA performance, high efficiency, and strong generalization ability,
as demonstrated by experiments across multiple benchmarks and tasks.

</details>


### [73] [Bayesian Test-time Adaptation for Object Recognition and Detection with Vision-language Models](https://arxiv.org/abs/2510.02750)
*Lihua Zhou,Mao Ye,Shuaifeng Li,Nianxin Li,Jinlin Wu,Xiatian Zhu,Lei Deng,Hongbin Liu,Jiebo Luo,Zhen Lei*

Main category: cs.CV

TL;DR: BCA+ is a training-free test-time adaptation framework for both object recognition and detection that improves performance under real-world distribution shifts.


<details>
  <summary>Details</summary>
Motivation: Existing test-time adaptation methods are either computationally expensive or overlook the importance of the prior. BCA+ addresses these shortcomings.

Method: BCA+ introduces a dynamic cache that adaptively stores and updates class embeddings, spatial scales, and adaptive class priors. It formulates adaptation as a Bayesian inference problem, fusing initial VLM output with a cache-based prediction.

Result: BCA+ achieves state-of-the-art performance on both recognition and detection benchmarks.

Conclusion: BCA+ is a highly efficient, training-free method that corrects both the model's semantic understanding and its contextual confidence.

Abstract: Vision-language models (VLMs) such as CLIP and Grounding DINO have achieved
remarkable success in object recognition and detection. However, their
performance often degrades under real-world distribution shifts. Test-time
adaptation (TTA) aims to mitigate this issue by adapting models during
inference. Existing methods either rely on computationally expensive
backpropagation, which hinders real-time deployment, or focus solely on
likelihood adaptation, which overlooks the critical role of the prior. Our
prior work, Bayesian Class Adaptation (BCA), addressed these shortcomings for
object recognition by introducing a training-free framework that incorporates
adaptive priors. Building upon this foundation, we now present Bayesian Class
Adaptation plus (BCA+), a unified, training-free framework for TTA for both
object recognition and detection. BCA+ introduces a dynamic cache that
adaptively stores and updates class embeddings, spatial scales (for detection),
and, crucially, adaptive class priors derived from historical predictions. We
formulate adaptation as a Bayesian inference problem, where final predictions
are generated by fusing the initial VLM output with a cache-based prediction.
This cache-based prediction combines a dynamically updated likelihood
(measuring feature and scale similarity) and a prior (reflecting the evolving
class distribution). This dual-adaptation mechanism, coupled with
uncertainty-guided fusion, enables BCA+ to correct both the model's semantic
understanding and its contextual confidence. As a training-free method
requiring no backpropagation, BCA+ is highly efficient. Extensive experiments
demonstrate that BCA+ achieves state-of-the-art performance on both recognition
and detection benchmarks.

</details>


### [74] [Hierarchical Generalized Category Discovery for Brain Tumor Classification in Digital Pathology](https://arxiv.org/abs/2510.02760)
*Matthias Perkonigg,Patrick Rockenschaub,Georg Göbel,Adelheid Wöhrer*

Main category: cs.CV

TL;DR: 提出了一种新的脑肿瘤分类方法HGCD-BT，该方法集成了分层聚类和对比学习，以解决现有方法无法识别训练中未出现的肿瘤类型的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的脑肿瘤分类方法无法捕捉到训练中未出现的肿瘤类型，而无监督学习缺乏整合标签数据先验知识的能力，半监督学习通常假设所有潜在类别都已在标签数据中表示。

Method: 引入了分层广义类别发现用于脑肿瘤分类（HGCD-BT），这是一种将分层聚类与对比学习相结合的新方法。该方法通过结合一种新的半监督分层聚类损失来扩展基于对比学习的GCD。

Result: 在OpenSRH数据集上，与最先进的GCD方法相比，HGCD-BT在patch级别分类的准确率上提高了+28%，尤其是在识别以前未见过的肿瘤类别方面。在来自数字脑肿瘤图谱的苏木精和曙红染色的全切片图像的slide级别分类上，证明了HGCD-BT的通用性。

Conclusion: HGCD-BT方法在识别以前未见过的肿瘤类别方面表现出色，并且在不同的成像模式中都具有实用性。

Abstract: Accurate brain tumor classification is critical for intra-operative decision
making in neuro-oncological surgery. However, existing approaches are
restricted to a fixed set of predefined classes and are therefore unable to
capture patterns of tumor types not available during training. Unsupervised
learning can extract general-purpose features, but it lacks the ability to
incorporate prior knowledge from labelled data, and semi-supervised methods
often assume that all potential classes are represented in the labelled data.
Generalized Category Discovery (GCD) aims to bridge this gap by categorizing
both known and unknown classes within unlabelled data. To reflect the
hierarchical structure of brain tumor taxonomies, in this work, we introduce
Hierarchical Generalized Category Discovery for Brain Tumor Classification
(HGCD-BT), a novel approach that integrates hierarchical clustering with
contrastive learning. Our method extends contrastive learning based GCD by
incorporating a novel semi-supervised hierarchical clustering loss. We evaluate
HGCD-BT on OpenSRH, a dataset of stimulated Raman histology brain tumor images,
achieving a +28% improvement in accuracy over state-of-the-art GCD methods for
patch-level classification, particularly in identifying previously unseen tumor
categories. Furthermore, we demonstrate the generalizability of HGCD-BT on
slide-level classification of hematoxylin and eosin stained whole-slide images
from the Digital Brain Tumor Atlas, confirming its utility across imaging
modalities.

</details>


### [75] [AdaRD-key: Adaptive Relevance-Diversity Keyframe Sampling for Long-form Video understanding](https://arxiv.org/abs/2510.02778)
*Xian Zhang,Zexi Wu,Zinuo Li,Hongming Xu,Luqi Gong,Farid Boussaid,Naoufel Werghi,Mohammed Bennamoun*

Main category: cs.CV

TL;DR: 本文提出了一种名为AdaRD-Key的免训练关键帧采样模块，用于查询驱动的长视频理解。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大型语言模型(MLLM)依赖于均匀采样，忽略了关键时刻，导致对查询的响应不正确。现有的关键帧选择方法采用严格的时间间隔，容易错过重要事件附近的细微线索，或者只强调视觉多样性而忽略了查询相关性。

Method: AdaRD-Key最大化一个统一的相关性-多样性最大体积(RD-MV)目标，将查询条件下的相关性评分与log-行列式多样性组件相结合，以产生信息丰富且非冗余的帧。采用轻量级的相关性感知门控机制来处理与视频弱对齐的广泛查询；当相关性分布表明弱对齐时，该方法无缝地转换为仅多样性模式，从而增强覆盖范围而无需额外的监督。

Result: 在LongVideoBench和Video-MME上的大量实验表明，该方法具有最先进的性能，尤其是在长视频上。

Conclusion: AdaRD-Key是一种免训练、计算效率高且与现有VLM兼容的即插即用模块，为长视频理解提供了一种有效的方法。

Abstract: Understanding long-form videos remains a significant challenge for
vision--language models (VLMs) due to their extensive temporal length and high
information density. Most current multimodal large language models (MLLMs) rely
on uniform sampling, which often overlooks critical moments, leading to
incorrect responses to queries. In parallel, many keyframe selection approaches
impose rigid temporal spacing: once a frame is chosen, an exclusion window
suppresses adjacent timestamps to reduce redundancy. While effective at
limiting overlap, this strategy frequently misses short, fine-grained cues near
important events. Other methods instead emphasize visual diversity but neglect
query relevance. We propose AdaRD-Key, a training-free keyframe sampling module
for query-driven long-form video understanding. AdaRD-Key maximizes a unified
Relevance--Diversity Max-Volume (RD-MV) objective, combining a
query-conditioned relevance score with a log-determinant diversity component to
yield informative yet non-redundant frames. To handle broad queries with weak
alignment to the video, AdaRD-Key employs a lightweight relevance-aware gating
mechanism; when the relevance distribution indicates weak alignment, the method
seamlessly shifts into a diversity-only mode, enhancing coverage without
additional supervision. Our pipeline is training-free, computationally
efficient (running in real time on a single GPU), and compatible with existing
VLMs in a plug-and-play manner. Extensive experiments on LongVideoBench and
Video-MME demonstrate state-of-the-art performance, particularly on long-form
videos. Code available at https://github.com/Xian867/AdaRD-Key.

</details>


### [76] [Reasoning Riddles: How Explainability Reveals Cognitive Limits in Vision-Language Models](https://arxiv.org/abs/2510.02780)
*Prahitha Movva*

Main category: cs.CV

TL;DR: 本文研究了视觉语言模型在解决隐语谜题时的认知过程。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在多模态任务中表现出色，但在解决隐语谜题等复杂横向思维挑战时，其认知过程仍然不明确。先前的研究表明，这些模型在解决隐语谜题方面存在困难，但其根本的推理过程和失败模式在很大程度上尚未被探索。

Method: 本文通过全面的可解释性分析来弥补这一差距，超越了性能指标，以了解视觉语言模型如何应对这些复杂的横向思维挑战。本文构建了一个系统注释的数据集，其中包含221个跨越六个认知类别的隐语谜题，并配有一个评估框架，该框架将推理质量与答案正确性分开。本文研究了三种旨在引发不同类型解释过程的提示策略，并揭示了对视觉语言模型认知过程的关键见解。

Result: 研究结果表明，不同谜题类别的推理质量差异很大，模型在视觉构成方面表现出系统性的优势，而在缺席解释和文化象征意义方面表现出根本性的局限性。提示策略对认知方法和解决问题的有效性都有很大的影响，从而将可解释性确立为模型性能的一个组成部分，而不是事后考虑。

Conclusion: 本文通过可解释性分析，深入了解了视觉语言模型在解决隐语谜题时的认知过程，并发现模型在不同谜题类别中的推理质量存在差异，且提示策略对模型性能有重要影响。

Abstract: Vision-Language Models (VLMs) excel at many multimodal tasks, yet their
cognitive processes remain opaque on complex lateral thinking challenges like
rebus puzzles. While recent work has demonstrated these models struggle
significantly with rebus puzzle solving, the underlying reasoning processes and
failure patterns remain largely unexplored. We address this gap through a
comprehensive explainability analysis that moves beyond performance metrics to
understand how VLMs approach these complex lateral thinking challenges. Our
study contributes a systematically annotated dataset of 221 rebus puzzles
across six cognitive categories, paired with an evaluation framework that
separates reasoning quality from answer correctness. We investigate three
prompting strategies designed to elicit different types of explanatory
processes and reveal critical insights into VLM cognitive processes. Our
findings demonstrate that reasoning quality varies dramatically across puzzle
categories, with models showing systematic strengths in visual composition
while exhibiting fundamental limitations in absence interpretation and cultural
symbolism. We also discover that prompting strategy substantially influences
both cognitive approach and problem-solving effectiveness, establishing
explainability as an integral component of model performance rather than a
post-hoc consideration.

</details>


### [77] [OTR: Synthesizing Overlay Text Dataset for Text Removal](https://arxiv.org/abs/2510.02787)
*Jan Zdenek,Wataru Shimoda,Kota Yamaguchi*

Main category: cs.CV

TL;DR: 本研究着眼于计算机视觉中一项关键任务——文本移除，它在隐私保护、图像编辑和媒体重用等领域具有广泛应用。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中于自然图像中的场景文本移除，但当前数据集的局限性阻碍了领域外泛化或准确评估。常用的基准测试（如 SCUT-EnsText）存在人工编辑造成的ground truth伪影、过于简单的文本背景以及无法捕捉生成结果质量的评估指标。

Method: 我们介绍了一种合成文本移除基准的方法，该基准适用于场景文本以外的领域。我们的数据集具有使用对象感知放置和视觉语言模型生成的内容在复杂背景上呈现的文本，确保干净的ground truth和具有挑战性的文本移除场景。

Result: 数据集可在https://huggingface.co/datasets/cyberagent/OTR 获取。

Conclusion: 构建了一个新的文本移除数据集，以解决现有数据集的不足，并为该领域的研究提供更好的基准。

Abstract: Text removal is a crucial task in computer vision with applications such as
privacy preservation, image editing, and media reuse. While existing research
has primarily focused on scene text removal in natural images, limitations in
current datasets hinder out-of-domain generalization or accurate evaluation. In
particular, widely used benchmarks such as SCUT-EnsText suffer from ground
truth artifacts due to manual editing, overly simplistic text backgrounds, and
evaluation metrics that do not capture the quality of generated results. To
address these issues, we introduce an approach to synthesizing a text removal
benchmark applicable to domains other than scene texts. Our dataset features
text rendered on complex backgrounds using object-aware placement and
vision-language model-generated content, ensuring clean ground truth and
challenging text removal scenarios. The dataset is available at
https://huggingface.co/datasets/cyberagent/OTR .

</details>


### [78] [Align Your Query: Representation Alignment for Multimodality Medical Object Detection](https://arxiv.org/abs/2510.02789)
*Ara Seo,Bryan Sangwoo Kim,Hyungjin Chung,Jong Chul Ye*

Main category: cs.CV

TL;DR: 针对混合医学模态（如 CXR、CT、MRI）的医学目标检测问题，提出了一种简单且与检测器无关的框架，通过多模态上下文注意（MoCA）和 QueryREPA 预训练，将不同模态的特征表示对齐到共享空间，从而提高检测性能。


<details>
  <summary>Details</summary>
Motivation: 在混合医学模态上训练单一检测器时，由于异构统计和不相交的表示空间，医学目标检测效果不佳。为了解决这个问题，本文转向表示对齐。

Method: 1. 定义模态 Tokens：紧凑的、文本衍生的嵌入，编码成像模态，轻量级且不需要额外的注释。
2. 通过多模态上下文注意（MoCA）将模态 Tokens 集成到检测过程中，通过自注意力混合目标查询表示，以在查询集中传播模态上下文。
3. 引入 QueryREPA：一个短的预训练阶段，使用具有模态平衡批次的任务特定对比目标，将查询表示与其模态 Tokens 对齐。

Result: 该方法在多种模态上进行联合训练，始终提高 AP，且开销最小，无需架构修改。

Conclusion: 该方法为鲁棒的多模态医学目标检测提供了一条实用的路径。

Abstract: Medical object detection suffers when a single detector is trained on mixed
medical modalities (e.g., CXR, CT, MRI) due to heterogeneous statistics and
disjoint representation spaces. To address this challenge, we turn to
representation alignment, an approach that has proven effective for bringing
features from different sources into a shared space. Specifically, we target
the representations of DETR-style object queries and propose a simple,
detector-agnostic framework to align them with modality context. First, we
define modality tokens: compact, text-derived embeddings encoding imaging
modality that are lightweight and require no extra annotations. We integrate
the modality tokens into the detection process via Multimodality Context
Attention (MoCA), mixing object-query representations via self-attention to
propagate modality context within the query set. This preserves DETR-style
architectures and adds negligible latency while injecting modality cues into
object queries. We further introduce QueryREPA, a short pretraining stage that
aligns query representations to their modality tokens using a task-specific
contrastive objective with modality-balanced batches. Together, MoCA and
QueryREPA produce modality-aware, class-faithful queries that transfer
effectively to downstream training. Across diverse modalities trained
altogether, the proposed approach consistently improves AP with minimal
overhead and no architectural modifications, offering a practical path toward
robust multimodality medical object detection. Project page:
https://araseo.github.io/alignyourquery/.

</details>


### [79] [MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding](https://arxiv.org/abs/2510.02790)
*Jingyuan Deng,Yujiu Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为 MaskCD 的方法，用于解决大型视觉语言模型 (LVLMs) 中出现的幻觉问题，即生成与其输入视觉和文本内容相矛盾的内容。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型 (LVLMs) 在视觉语言理解方面表现出色，但同时出现了幻觉问题，现有方法（如对比解码和注意力操纵）存在不足，例如对比解码难以构建合适的对比样本，注意力操纵方法缺乏稳定性。

Method: 本文提出图像头 Masked Contrastive Decoding (MaskCD) 方法，利用 LVLMs 中的“图像头”，通过屏蔽它们来构建对比解码的对比样本。

Result: 在 LLaVA-1.5-7b 和 Qwen-VL-7b 上，使用 CHAIR、POPE、AMBER 和 MME 等基准测试评估 MaskCD，结果表明 MaskCD 有效地缓解了幻觉现象，并保留了 LVLMs 的一般能力。

Conclusion: 本文提出的 MaskCD 方法能够有效地缓解大型视觉语言模型中的幻觉问题，同时保持其通用能力。

Abstract: Large vision-language models (LVLMs) have shown remarkable performance in
visual-language understanding for downstream multimodal tasks. While their
capabilities are improving, problems emerge simultaneously. Among those
problems, the hallucinations have attracted much attention, which stands for
the phenomenon where LVLMs generate contradictory content to their input visual
and text contents. Many approaches have been proposed to deal with this issue,
such as contrastive decoding and attention manipulation. However, contrastive
decoding methods struggle in constructing appropriate contrastive samples, and
attention manipulation methods are highly sensitive, lacking stability. In this
work, we propose image head Masked Contrastive Decoding (MaskCD). Our approach
utilizes the "image heads" in LVLMs, masking them to construct contrastive
samples for contrastive decoding. We evaluated MaskCD on LLaVA-1.5-7b and
Qwen-VL-7b, using various benchmarks such as CHAIR, POPE, AMBER and MME. The
results demonstrate that MaskCD effectively alleviates the phenomenon of
hallucinations and retains the general capabilities of LVLMs. Corresponding
resources could be found at: https://github.com/Deng-Jingyuan/MaskCD .

</details>


### [80] [VERNIER: an open-source software pushing marker pose estimation down to the micrometer and nanometer scales](https://arxiv.org/abs/2510.02791)
*Patrick Sandoz,Antoine N. André,Guillaume J. Laurent*

Main category: cs.CV

TL;DR: VERNIER is an open-source phase processing software for pose measurement using pseudo-periodic patterns, robust to noise, defocus and occlusion.


<details>
  <summary>Details</summary>
Motivation: Pose estimation at small scales is challenging, and few solutions exist for capturing 6DoF with nanometric and microradian resolutions over large ranges.

Method: Phase-based local thresholding algorithm for pose measurement based on pseudo-periodic patterns.

Result: Software is robust to noise, defocus and occlusion. Implementation is illustrated with synthetic and experimental images.

Conclusion: Guidelines are provided for selecting pattern design and microscope magnification lenses for desired performance.

Abstract: Pose estimation is still a challenge at the small scales. Few solutions exist
to capture the 6 degrees of freedom of an object with nanometric and
microradians resolutions over relatively large ranges. Over the years, we have
proposed several fiducial marker and pattern designs to achieve reliable
performance for various microscopy applications. Centimeter ranges are possible
using pattern encoding methods, while nanometer resolutions can be achieved
using phase processing of the periodic frames. This paper presents VERNIER, an
open source phase processing software designed to provide fast and reliable
pose measurement based on pseudo-periodic patterns. Thanks to a phase-based
local thresholding algorithm, the software has proven to be particularly robust
to noise, defocus and occlusion. The successive steps of the phase processing
are presented, as well as the different types of patterns that address
different application needs. The implementation procedure is illustrated with
synthetic and experimental images. Finally, guidelines are given for selecting
the appropriate pattern design and microscope magnification lenses as a
function of the desired performance.

</details>


### [81] [Med-K2N: Flexible K-to-N Modality Translation for Medical Image Synthesis](https://arxiv.org/abs/2510.02815)
*Feng Yuan,Yifan Gao,Yuehua Ye,Haoyue Li,Xin Gao*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Med-K2N的新方法，用于解决医学图像跨模态合成中的K to N生成问题。


<details>
  <summary>Details</summary>
Motivation: 临床上需要灵活的模态重建，因此研究K to N医学图像生成。

Method: 该方法将多模态医学数据视为顺序帧，并使用质量驱动的选择机制，设计了PreWeightNet、ThresholdNet和EffiWeightNet三个协同模块，以及因果模态身份模块（CMIM）。

Result: 实验结果表明，Med-K2N在多个基准测试中优于现有技术。

Conclusion: 该论文提出了一种有效的医学图像跨模态合成方法，可以实现K to N生成，并在多个基准测试中取得了显著的性能提升。

Abstract: Cross-modal medical image synthesis research focuses on reconstructing
missing imaging modalities from available ones to support clinical diagnosis.
Driven by clinical necessities for flexible modality reconstruction, we explore
K to N medical generation, where three critical challenges emerge: How can we
model the heterogeneous contributions of different modalities to various target
tasks? How can we ensure fusion quality control to prevent degradation from
noisy information? How can we maintain modality identity consistency in
multi-output generation? Driven by these clinical necessities, and drawing
inspiration from SAM2's sequential frame paradigm and clinicians' progressive
workflow of incrementally adding and selectively integrating multi-modal
information, we treat multi-modal medical data as sequential frames with
quality-driven selection mechanisms. Our key idea is to "learn" adaptive
weights for each modality-task pair and "memorize" beneficial fusion patterns
through progressive enhancement. To achieve this, we design three collaborative
modules: PreWeightNet for global contribution assessment, ThresholdNet for
adaptive filtering, and EffiWeightNet for effective weight computation.
Meanwhile, to maintain modality identity consistency, we propose the Causal
Modality Identity Module (CMIM) that establishes causal constraints between
generated images and target modality descriptions using vision-language
modeling. Extensive experimental results demonstrate that our proposed Med-K2N
outperforms state-of-the-art methods by significant margins on multiple
benchmarks. Source code is available.

</details>


### [82] [ELMF4EggQ: Ensemble Learning with Multimodal Feature Fusion for Non-Destructive Egg Quality Assessment](https://arxiv.org/abs/2510.02876)
*Md Zahim Hassan,Md. Osama,Muhammad Ashad Kabir,Md. Saiful Islam,Zannatul Naim*

Main category: cs.CV

TL;DR: 本文提出了一种名为ELMF4EggQ的集成学习框架，该框架仅使用鸡蛋的外部属性（图像、形状和重量）来对鸡蛋的等级和新鲜度进行分类。


<details>
  <summary>Details</summary>
Motivation: 准确、无损的鸡蛋质量评估对于确保食品安全、保持产品标准和提高商业家禽生产的运营效率至关重要。

Method: 该框架集成了从外部鸡蛋图像中提取的深度特征与鸡蛋形状和重量等结构特征。图像特征提取使用性能最佳的预训练CNN模型（ResNet152、DenseNet169和ResNet152V2），然后进行基于PCA的降维、SMOTE增强和使用多种机器学习算法进行分类。集成投票机制结合了性能最佳的分类器的预测，以提高整体准确性。

Result: 多模态方法显着优于仅图像和仅表格（形状和重量）的基线，多模态集成方法在等级分类中达到86.57%的准确率，在新鲜度预测中达到70.83%的准确率。

Conclusion: 本文是第一个仅使用外部、非侵入性特征应用机器学习方法进行内部鸡蛋质量评估的研究，也是第一个发布相应标记数据集的研究。

Abstract: Accurate, non-destructive assessment of egg quality is critical for ensuring
food safety, maintaining product standards, and operational efficiency in
commercial poultry production. This paper introduces ELMF4EggQ, an ensemble
learning framework that employs multimodal feature fusion to classify egg grade
and freshness using only external attributes - image, shape, and weight. A
novel, publicly available dataset of 186 brown-shelled eggs was constructed,
with egg grade and freshness levels determined through laboratory-based expert
assessments involving internal quality measurements, such as yolk index and
Haugh unit. To the best of our knowledge, this is the first study to apply
machine learning methods for internal egg quality assessment using only
external, non-invasive features, and the first to release a corresponding
labeled dataset. The proposed framework integrates deep features extracted from
external egg images with structural characteristics such as egg shape and
weight, enabling a comprehensive representation of each egg. Image feature
extraction is performed using top-performing pre-trained CNN models (ResNet152,
DenseNet169, and ResNet152V2), followed by PCA-based dimensionality reduction,
SMOTE augmentation, and classification using multiple machine learning
algorithms. An ensemble voting mechanism combines predictions from the
best-performing classifiers to enhance overall accuracy. Experimental results
demonstrate that the multimodal approach significantly outperforms image-only
and tabular (shape and weight) only baselines, with the multimodal ensemble
approach achieving 86.57% accuracy in grade classification and 70.83% in
freshness prediction. All code and data are publicly available at
https://github.com/Kenshin-Keeps/Egg_Quality_Prediction_ELMF4EggQ, promoting
transparency, reproducibility, and further research in this domain.

</details>


### [83] [Representation Learning for Compressed Video Action Recognition via Attentive Cross-modal Interaction with Motion Enhancement](https://arxiv.org/abs/2205.03569)
*Bing Li,Jiaxin Chen,Dongming Zhang,Xiuguo Bao,Di Huang*

Main category: cs.CV

TL;DR: 本文提出了一种名为MEACI-Net的新框架，用于压缩视频动作识别，通过增强的运动信息和跨模态交互来解决粗糙和嘈杂的动态以及异构RGB和运动模态融合不足的问题。


<details>
  <summary>Details</summary>
Motivation: 压缩视频动作识别通过稀疏采样的RGB帧和压缩的运动线索（例如，运动矢量和残差）来显着降低存储和计算成本。然而，这个任务严重受到粗糙和嘈杂的动态以及异构RGB和运动模态融合不足的影响。

Method: 该方法采用双流架构，一个用于RGB模态，另一个用于运动模态。运动流采用嵌入去噪模块的多尺度块来增强表示学习。通过引入选择性运动补充（SMC）和跨模态增强（CMA）模块来加强两个流之间的交互，其中SMC用时空注意局部运动特征来补充RGB模态，CMA进一步结合具有选择性特征增强的两种模态。

Result: 在UCF-101、HMDB-51和Kinetics-400基准上的大量实验证明了MEACI-Net的有效性和效率。

Conclusion: MEACI-Net在压缩视频动作识别任务上表现出色，有效解决了现有方法的局限性。

Abstract: Compressed video action recognition has recently drawn growing attention,
since it remarkably reduces the storage and computational cost via replacing
raw videos by sparsely sampled RGB frames and compressed motion cues (e.g.,
motion vectors and residuals). However, this task severely suffers from the
coarse and noisy dynamics and the insufficient fusion of the heterogeneous RGB
and motion modalities. To address the two issues above, this paper proposes a
novel framework, namely Attentive Cross-modal Interaction Network with Motion
Enhancement (MEACI-Net). It follows the two-stream architecture, i.e. one for
the RGB modality and the other for the motion modality. Particularly, the
motion stream employs a multi-scale block embedded with a denoising module to
enhance representation learning. The interaction between the two streams is
then strengthened by introducing the Selective Motion Complement (SMC) and
Cross-Modality Augment (CMA) modules, where SMC complements the RGB modality
with spatio-temporally attentive local motion features and CMA further combines
the two modalities with selective feature augmentation. Extensive experiments
on the UCF-101, HMDB-51 and Kinetics-400 benchmarks demonstrate the
effectiveness and efficiency of MEACI-Net.

</details>


### [84] [One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework](https://arxiv.org/abs/2510.02898)
*Lorenzo Bianchi,Giacomo Pacini,Fabio Carrara,Nicola Messina,Giuseppe Amato,Fabrizio Falchi*

Main category: cs.CV

TL;DR: 本文提出了一个统一的零样本图像描述框架，该框架将图像分割成多个patch，并将每个patch作为基本的描述单元，从而实现对任意区域的描述。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本图像描述模型依赖于图像-文本的通用空间表示，但仅限于全局表示和整图描述。

Method: 该方法将图像分割成多个patch，并将每个patch作为基本的描述单元，然后聚合这些单元来描述任意区域。

Result: 实验结果表明，使用DINO等产生有意义的密集视觉特征的骨干网络是实现多个基于区域的描述任务中最先进性能的关键。与其他的baseline和最先进的竞品相比，本文的模型在zero-shot dense，region-set和trace captioning任务上都取得了更好的性能。

Conclusion: 本文提出的基于patch的语义表示方法对于可扩展的描述生成是有效的。

Abstract: Zero-shot captioners are recently proposed models that utilize common-space
vision-language representations to caption images without relying on paired
image-text data. To caption an image, they proceed by textually decoding a
text-aligned image feature, but they limit their scope to global
representations and whole-image captions. We present \frameworkName{}, a
unified framework for zero-shot captioning that shifts from an image-centric to
a patch-centric paradigm, enabling the captioning of arbitrary regions without
the need of region-level supervision. Instead of relying on global image
representations, we treat individual patches as atomic captioning units and
aggregate them to describe arbitrary regions, from single patches to
non-contiguous areas and entire images. We analyze the key ingredients that
enable current latent captioners to work in our novel proposed framework.
Experiments demonstrate that backbones producing meaningful, dense visual
features, such as DINO, are key to achieving state-of-the-art performance in
multiple region-based captioning tasks. Compared to other baselines and
state-of-the-art competitors, our models achieve better performance on
zero-shot dense, region-set, and a newly introduced trace captioning task,
highlighting the effectiveness of patch-wise semantic representations for
scalable caption generation. Project page at https://paciosoft.com/Patch-ioner/ .

</details>


### [85] [Training-Free Out-Of-Distribution Segmentation With Foundation Models](https://arxiv.org/abs/2510.02909)
*Laith Nayal,Hadi Salloum,Ahmad Taha,Yaroslav Kholodov,Alexander Gasnikov*

Main category: cs.CV

TL;DR: 本文研究了大型视觉基础模型在语义分割中检测未知对象的能力，提出了一种无需训练的方法，利用InternImage骨干网络提取的特征，通过K-Means聚类和置信度阈值来识别OoD区域，并在RoadAnomaly和ADE-OoD基准测试中取得了优于其他基线方法的结果。


<details>
  <summary>Details</summary>
Motivation: 在语义分割中检测未知对象对于自动驾驶等安全关键应用至关重要。大型视觉基础模型在视觉表征学习方面取得了进展，但它们在语义分割中检测分布外(OoD)区域的能力仍有待探索。

Method: 本文提出了一种简单、无需训练的方法，该方法利用InternImage骨干网络中的特征，并结合K-Means聚类和原始解码器logits上的置信度阈值来识别OoD簇。

Result: 该方法在RoadAnomaly基准测试中取得了50.02的平均精度，在ADE-OoD基准测试中取得了48.77的平均精度，优于几种监督和非监督基线方法。

Conclusion: 研究结果表明，对于需要最少的假设或额外数据的通用OoD分割方法，这是一个很有前途的方向。

Abstract: Detecting unknown objects in semantic segmentation is crucial for
safety-critical applications such as autonomous driving. Large vision
foundation models, including DINOv2, InternImage, and CLIP, have advanced
visual representation learning by providing rich features that generalize well
across diverse tasks. While their strength in closed-set semantic tasks is
established, their capability to detect out-of-distribution (OoD) regions in
semantic segmentation remains underexplored. In this work, we investigate
whether foundation models fine-tuned on segmentation datasets can inherently
distinguish in-distribution (ID) from OoD regions without any outlier
supervision. We propose a simple, training-free approach that utilizes features
from the InternImage backbone and applies K-Means clustering alongside
confidence thresholding on raw decoder logits to identify OoD clusters. Our
method achieves 50.02 Average Precision on the RoadAnomaly benchmark and 48.77
on the benchmark of ADE-OoD with InternImage-L, surpassing several supervised
and unsupervised baselines. These results suggest a promising direction for
generic OoD segmentation methods that require minimal assumptions or additional
data.

</details>


### [86] [Don't Just Chase "Highlighted Tokens" in MLLMs: Revisiting Visual Holistic Context Retention](https://arxiv.org/abs/2510.02912)
*Xin Zou,Di Lu,Yizhou Wang,Yibo Yan,Yuanhuiyi Lyu,Xu Zheng,Linfeng Zhang,Xuming Hu*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉token剪枝框架HoloV，旨在解决现有方法在token剪枝过程中容易保留语义相似token的问题，从而在高剪枝率下导致性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大型语言模型由于依赖大量的视觉token而存在巨大的计算开销。为了解决这个问题，有研究探索了token剪枝技术，但这些方法倾向于保留语义相似的token，导致在高剪枝率下性能下降。

Method: HoloV框架从整体的角度重新考虑token保留，通过自适应地在不同的空间区域分配剪枝预算，确保保留的token能够捕捉到全局视觉上下文，而不是孤立的显著特征。

Result: 实验结果表明，HoloV在各种任务、MLLM架构和剪枝率下均优于SOTA方法。例如，配备HoloV的LLaVA1.5在剪枝88.9%的视觉token后，仍能保持原始性能的95.8%。

Conclusion: HoloV能够实现卓越的效率-精度权衡。

Abstract: Despite their powerful capabilities, Multimodal Large Language Models (MLLMs)
suffer from considerable computational overhead due to their reliance on
massive visual tokens. Recent studies have explored token pruning to alleviate
this problem, which typically uses text-vision cross-attention or
[\texttt{CLS}] attention to assess and discard redundant visual tokens. In this
work, we identify a critical limitation of such attention-first pruning
approaches, i.e., they tend to preserve semantically similar tokens, resulting
in pronounced performance drops under high pruning ratios. To this end, we
propose {HoloV}, a simple yet effective, plug-and-play visual token pruning
framework for efficient inference. Distinct from previous attention-first
schemes, HoloV rethinks token retention from a holistic perspective. By
adaptively distributing the pruning budget across different spatial crops,
HoloV ensures that the retained tokens capture the global visual context rather
than isolated salient features. This strategy minimizes representational
collapse and maintains task-relevant information even under aggressive pruning.
Experimental results demonstrate that our HoloV achieves superior performance
across various tasks, MLLM architectures, and pruning ratios compared to SOTA
methods. For instance, LLaVA1.5 equipped with HoloV preserves 95.8\% of the
original performance after pruning 88.9\% of visual tokens, achieving superior
efficiency-accuracy trade-offs.

</details>


### [87] [Zero-Shot Robustness of Vision Language Models Via Confidence-Aware Weighting](https://arxiv.org/abs/2510.02913)
*Nikoo Naghavian,Mostafa Tavassolipour*

Main category: cs.CV

TL;DR: 本文提出了一种名为置信度感知权重（CAW）的方法，以提高视觉语言模型中的零样本鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在零样本泛化方面表现出色，但容易受到对抗性攻击。

Method: CAW包括两个组成部分：(1) 置信度感知损失，通过缩放干净预测和对抗性预测之间的KL散度来优先考虑不确定的对抗性示例；(2) 特征对齐正则化，通过最小化冻结和微调图像编码器在对抗性输入上的特征之间的距离来保持语义一致性。

Result: 在TinyImageNet和14个额外数据集上的大量实验表明，在AutoAttack等强攻击下，CAW优于PMG-AFT和TGA-ZSR等最新方法，同时使用更少的内存。

Conclusion: CAW能够提高干净准确率和鲁棒准确率，且不牺牲泛化能力。

Abstract: Vision-language models like CLIP demonstrate impressive zero-shot
generalization but remain highly vulnerable to adversarial attacks. In this
work, we propose Confidence-Aware Weighting (CAW) to enhance zero-shot
robustness in vision-language models. CAW consists of two components: (1) a
Confidence-Aware loss that prioritizes uncertain adversarial examples by
scaling the KL divergence between clean and adversarial predictions, and (2) a
feature alignment regularization that preserves semantic consistency by
minimizing the distance between frozen and fine-tuned image encoder features on
adversarial inputs. These components work jointly to improve both clean and
robust accuracy without sacrificing generalization. Extensive experiments on
TinyImageNet and 14 additional datasets show that CAW outperforms recent
methods such as PMG-AFT and TGA-ZSR under strong attacks like AutoAttack, while
using less memory.

</details>


### [88] [Multimodal Carotid Risk Stratification with Large Vision-Language Models: Benchmarking, Fine-Tuning, and Clinical Insights](https://arxiv.org/abs/2510.02922)
*Daphne Tsolissou,Theofanis Ganitidis,Konstantinos Mitsis,Stergios CHristodoulidis,Maria Vakalopoulou,Konstantina Nikita*

Main category: cs.CV

TL;DR: 本研究探讨了大型视觉语言模型（LVLMs）在颈动脉斑块评估中的潜力，通过整合超声成像（USI）与临床、人口统计、实验室和蛋白质生物标志物数据。


<details>
  <summary>Details</summary>
Motivation: 可靠的颈动脉粥样硬化疾病风险评估仍然是一个主要的临床挑战，因为它需要以透明和可解释的方式整合不同的临床和影像信息。

Method: 研究提出了一个通过访谈式问题序列模拟真实诊断场景的框架，比较了一系列开源LVLMs，包括通用和医学调整模型。使用低秩适应（LoRA）将LLaVa-NeXT-Vicuna适配到超声领域。

Result: Zero-shot实验表明，并非所有LVLMs都能准确识别成像方式和解剖结构，并且它们在准确的风险分类中表现不佳。通过使用LoRA进行领域适配，LLaVa-NeXT-Vicuna在卒中风险分层方面取得了显著改善。以文本形式整合多模态表格数据进一步提高了特异性和平衡准确性。

Conclusion: 研究结果强调了LVLMs在基于超声的心血管风险预测中的前景和局限性，强调了多模态整合、模型校准和领域自适应对于临床转化的重要性。

Abstract: Reliable risk assessment for carotid atheromatous disease remains a major
clinical challenge, as it requires integrating diverse clinical and imaging
information in a manner that is transparent and interpretable to clinicians.
This study investigates the potential of state-of-the-art and recent large
vision-language models (LVLMs) for multimodal carotid plaque assessment by
integrating ultrasound imaging (USI) with structured clinical, demographic,
laboratory, and protein biomarker data. A framework that simulates realistic
diagnostic scenarios through interview-style question sequences is proposed,
comparing a range of open-source LVLMs, including both general-purpose and
medically tuned models. Zero-shot experiments reveal that even if they are very
powerful, not all LVLMs can accurately identify imaging modality and anatomy,
while all of them perform poorly in accurate risk classification. To address
this limitation, LLaVa-NeXT-Vicuna is adapted to the ultrasound domain using
low-rank adaptation (LoRA), resulting in substantial improvements in stroke
risk stratification. The integration of multimodal tabular data in the form of
text further enhances specificity and balanced accuracy, yielding competitive
performance compared to prior convolutional neural network (CNN) baselines
trained on the same dataset. Our findings highlight both the promise and
limitations of LVLMs in ultrasound-based cardiovascular risk prediction,
underscoring the importance of multimodal integration, model calibration, and
domain adaptation for clinical translation.

</details>


### [89] [Flip Distribution Alignment VAE for Multi-Phase MRI Synthesis](https://arxiv.org/abs/2510.02970)
*Xiaoyan Kui,Qianmu Xiao,Qqinsong Li,Zexin Ji,JIelin Zhang,Beiji Zou*

Main category: cs.CV

TL;DR: 提出了一种轻量级的特征解耦VAE模型（FDA-VAE），用于多期CE MRI合成，该模型参数效率高，并具有可解释的训练策略。


<details>
  <summary>Details</summary>
Motivation: 现有的多期CE MRI合成方法使用深度自编码器生成器，参数效率低，缺乏可解释的训练策略。

Method: 该方法将输入和目标图像编码成关于标准正态分布对称的两个潜在分布，有效地分离共享和独立特征。Y形双向训练策略进一步增强了特征分离的可解释性。

Result: 实验结果表明，与现有的基于深度自编码器的端到端合成方法相比，FDA-VAE显著减少了模型参数和推理时间，同时有效提高了合成质量。

Conclusion: FDA-VAE模型在多期CE MRI合成方面表现出色，具有轻量级、高效率和可解释性强的优点。

Abstract: Separating shared and independent features is crucial for multi-phase
contrast-enhanced (CE) MRI synthesis. However, existing methods use deep
autoencoder generators with low parameter efficiency and lack interpretable
training strategies. In this paper, we propose Flip Distribution Alignment
Variational Autoencoder (FDA-VAE), a lightweight feature-decoupled VAE model
for multi-phase CE MRI synthesis. Our method encodes input and target images
into two latent distributions that are symmetric concerning a standard normal
distribution, effectively separating shared and independent features. The
Y-shaped bidirectional training strategy further enhances the interpretability
of feature separation. Experimental results show that compared to existing deep
autoencoder-based end-to-end synthesis methods, FDA-VAE significantly reduces
model parameters and inference time while effectively improving synthesis
quality. The source code is publicly available at
https://github.com/QianMuXiao/FDA-VAE.

</details>


### [90] [TIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via Text-to-Image-to-Text Consistency](https://arxiv.org/abs/2510.02987)
*Juntong Wang,Huiyu Duan,Jiarui Wang,Ziheng Jia,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: LPG-Bench是一个用于评估基于长文本prompt的文本到图像生成的综合基准。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像模型难以理解和遵循长而详细的prompt，导致生成结果不一致。

Method: 提出了LPG-Bench基准，包含200个精心设计的prompt，平均长度超过250个单词。使用这些prompt，生成了来自13个最先进模型的2,600张图像，并进行了人工排序注释。此外，还提出了一种新的基于文本到图像到文本一致性的零样本指标TIT，用于评估长文本prompt生成的图像。

Result: LPG-Bench表明，最先进的T2I对齐评估指标与基于长文本prompt的图像生成的人类偏好一致性较差。TIT框架在与人类判断的对齐方面优于CLIP-score、LMM-score等，其中TIT-Score-LLM在两两比较准确率方面比最强的基线提高了7.31%。

Conclusion: LPG-Bench和TIT方法共同为基准测试和促进T2I模型的发展提供了更深层次的视角。所有资源都将公开提供。

Abstract: With the rapid advancement of large multimodal models (LMMs), recent
text-to-image (T2I) models can generate high-quality images and demonstrate
great alignment to short prompts. However, they still struggle to effectively
understand and follow long and detailed prompts, displaying inconsistent
generation. To address this challenge, we introduce LPG-Bench, a comprehensive
benchmark for evaluating long-prompt-based text-to-image generation. LPG-Bench
features 200 meticulously crafted prompts with an average length of over 250
words, approaching the input capacity of several leading commercial models.
Using these prompts, we generate 2,600 images from 13 state-of-the-art models
and further perform comprehensive human-ranked annotations. Based on LPG-Bench,
we observe that state-of-the-art T2I alignment evaluation metrics exhibit poor
consistency with human preferences on long-prompt-based image generation. To
address the gap, we introduce a novel zero-shot metric based on
text-to-image-to-text consistency, termed TIT, for evaluating
long-prompt-generated images. The core concept of TIT is to quantify T2I
alignment by directly comparing the consistency between the raw prompt and the
LMM-produced description on the generated image, which includes an efficient
score-based instantiation TIT-Score and a large-language-model (LLM) based
instantiation TIT-Score-LLM. Extensive experiments demonstrate that our
framework achieves superior alignment with human judgment compared to
CLIP-score, LMM-score, etc., with TIT-Score-LLM attaining a 7.31% absolute
improvement in pairwise accuracy over the strongest baseline. LPG-Bench and TIT
methods together offer a deeper perspective to benchmark and foster the
development of T2I models. All resources will be made publicly available.

</details>


### [91] [Towards Scalable and Consistent 3D Editing](https://arxiv.org/abs/2510.02994)
*Ruihao Xia,Yang Tang,Pan Zhou*

Main category: cs.CV

TL;DR: 这篇论文介绍了3D编辑，这是一个在3D资产上局部修改几何形状或外观的任务。他们提出了一个名为3DEditVerse的大型配对3D编辑基准，并提出了一个名为3DEditFormer的3D结构保持条件transformer模型。


<details>
  <summary>Details</summary>
Motivation: 现有的3D编辑方法通常速度慢，容易产生几何变形，或者依赖于手动且精确的3D mask，这些mask容易出错且不实用。

Method: 他们提出了3DEditVerse数据集，并通过姿势驱动的几何编辑和基础模型引导的外观编辑的互补pipeline构建。他们还提出了3DEditFormer，通过双重引导注意力和时间自适应门控来增强图像到3D的生成，从而将可编辑区域与保留结构分离。

Result: 他们的框架在定量和定性方面都优于最先进的baseline，为实用且可扩展的3D编辑建立了新标准。

Conclusion: 他们提出了一个名为3DEditVerse的大型配对3D编辑基准，并提出了一个名为3DEditFormer的3D结构保持条件transformer模型，从而实现了精确和一致的编辑，无需辅助3D mask。

Abstract: 3D editing - the task of locally modifying the geometry or appearance of a 3D
asset - has wide applications in immersive content creation, digital
entertainment, and AR/VR. However, unlike 2D editing, it remains challenging
due to the need for cross-view consistency, structural fidelity, and
fine-grained controllability. Existing approaches are often slow, prone to
geometric distortions, or dependent on manual and accurate 3D masks that are
error-prone and impractical. To address these challenges, we advance both the
data and model fronts. On the data side, we introduce 3DEditVerse, the largest
paired 3D editing benchmark to date, comprising 116,309 high-quality training
pairs and 1,500 curated test pairs. Built through complementary pipelines of
pose-driven geometric edits and foundation model-guided appearance edits,
3DEditVerse ensures edit locality, multi-view consistency, and semantic
alignment. On the model side, we propose 3DEditFormer, a
3D-structure-preserving conditional transformer. By enhancing image-to-3D
generation with dual-guidance attention and time-adaptive gating, 3DEditFormer
disentangles editable regions from preserved structure, enabling precise and
consistent edits without requiring auxiliary 3D masks. Extensive experiments
demonstrate that our framework outperforms state-of-the-art baselines both
quantitatively and qualitatively, establishing a new standard for practical and
scalable 3D editing. Dataset and code will be released. Project:
https://www.lv-lab.org/3DEditFormer/

</details>


### [92] [Not every day is a sunny day: Synthetic cloud injection for deep land cover segmentation robustness evaluation across data sources](https://arxiv.org/abs/2510.03006)
*Sara Mobsite,Renaud Hostache,Laure Berti Equille,Emmanuel Roux,Joris Guerin*

Main category: cs.CV

TL;DR: 本文研究了云层覆盖对土地覆盖语义分割（LCS）的影响，并提出了利用Sentinel-1雷达数据和归一化差异指数（NDIs）来提高模型在多云条件下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的Sentinel-2数据集大多无云，限制了其在热带地区的应用。为了评估这个问题，作者开发了一种云注入算法来模拟真实的云层覆盖。

Method: 1. 开发云注入算法模拟云层覆盖。
2. 提出一种轻量级方法，将归一化差异指数（NDIs）注入到解码层中，以保留关键的空间特征。
3. 结合Sentinel-1雷达数据来填补云层遮挡造成的图像空白。

Result: 在无云图像上，注入NDIs后，U-Net提升了1.99%，DeepLabV3提升了2.78%。在多云条件下，结合Sentinel-1数据显著提升了所有模型的性能。

Conclusion: 雷达-光学融合在具有挑战性的大气场景中是有效的。

Abstract: Supervised deep learning for land cover semantic segmentation (LCS) relies on
labeled satellite data. However, most existing Sentinel-2 datasets are
cloud-free, which limits their usefulness in tropical regions where clouds are
common. To properly evaluate the extent of this problem, we developed a cloud
injection algorithm that simulates realistic cloud cover, allowing us to test
how Sentinel-1 radar data can fill in the gaps caused by cloud-obstructed
optical imagery. We also tackle the issue of losing spatial and/or spectral
details during encoder downsampling in deep networks. To mitigate this loss, we
propose a lightweight method that injects Normalized Difference Indices (NDIs)
into the final decoding layers, enabling the model to retain key spatial
features with minimal additional computation. Injecting NDIs enhanced land
cover segmentation performance on the DFC2020 dataset, yielding improvements of
1.99% for U-Net and 2.78% for DeepLabV3 on cloud-free imagery. Under
cloud-covered conditions, incorporating Sentinel-1 data led to significant
performance gains across all models compared to using optical data alone,
highlighting the effectiveness of radar-optical fusion in challenging
atmospheric scenarios.

</details>


### [93] [PocketSR: The Super-Resolution Expert in Your Pocket Mobiles](https://arxiv.org/abs/2510.03012)
*Haoze Sun,Linfeng Jiang,Fan Li,Renjing Pei,Zhixin Wang,Yong Guo,Jiaqi Xu,Haoyu Chen,Jin Han,Fenglong Song,Yujiu Yang,Wenbo Li*

Main category: cs.CV

TL;DR: 提出了一种超轻量级的单步模型PocketSR，它将生成建模能力引入RealSR，同时保持了高保真度，非常适合边缘部署。


<details>
  <summary>Details</summary>
Motivation: 现有方法计算成本高、延迟大，不适合边缘部署。

Method: 1. 设计 LiteED，这是一种高效的 VAE 替代方案，与 SD 中原始的计算密集型 VAE 相比，参数减少了 97.5%，同时保持了高质量的编码和解码。
2. 提出用于 U-Net 的在线退火剪枝，它逐步将生成先验从重模块转移到轻量级模块，确保有效的知识转移并进一步优化效率。
3. 为了减轻剪枝过程中先验知识的损失，我们结合了多层特征蒸馏损失。

Result: PocketSR 的模型大小为 146M 参数，只需 0.8 秒即可处理 4K 图像，与以前的方法相比实现了显着加速。值得注意的是，它提供的性能与最先进的单步甚至多步 RealSR 模型相当。

Conclusion: PocketSR 是一种非常适合边缘设备应用的解决方案

Abstract: Real-world image super-resolution (RealSR) aims to enhance the visual quality
of in-the-wild images, such as those captured by mobile phones. While existing
methods leveraging large generative models demonstrate impressive results, the
high computational cost and latency make them impractical for edge deployment.
In this paper, we introduce PocketSR, an ultra-lightweight, single-step model
that brings generative modeling capabilities to RealSR while maintaining high
fidelity. To achieve this, we design LiteED, a highly efficient alternative to
the original computationally intensive VAE in SD, reducing parameters by 97.5%
while preserving high-quality encoding and decoding. Additionally, we propose
online annealing pruning for the U-Net, which progressively shifts generative
priors from heavy modules to lightweight counterparts, ensuring effective
knowledge transfer and further optimizing efficiency. To mitigate the loss of
prior knowledge during pruning, we incorporate a multi-layer feature
distillation loss. Through an in-depth analysis of each design component, we
provide valuable insights for future research. PocketSR, with a model size of
146M parameters, processes 4K images in just 0.8 seconds, achieving a
remarkable speedup over previous methods. Notably, it delivers performance on
par with state-of-the-art single-step and even multi-step RealSR models, making
it a highly practical solution for edge-device applications.

</details>


### [94] [When and Where do Events Switch in Multi-Event Video Generation?](https://arxiv.org/abs/2510.03049)
*Ruotong Liao,Guowen Huang,Qing Cheng,Thomas Seidl,Daniel Cremers,Volker Tresp*

Main category: cs.CV

TL;DR: 本文研究了多事件文本到视频生成中的事件转换控制问题，并提出了一个用于评估多事件T2V生成的提示套件MEve。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了事件转换的内在因素，本文旨在回答多事件提示如何控制T2V生成过程中的事件转换。

Method: 通过引入MEve提示套件，对OpenSora和CogVideoX两个代表性模型族进行了系统研究。

Result: 实验表明，在去噪步骤和块状模型层中尽早干预非常重要，揭示了多事件视频生成的关键因素。

Conclusion: 研究结果强调了未来模型中多事件条件控制的可能性。

Abstract: Text-to-video (T2V) generation has surged in response to challenging
questions, especially when a long video must depict multiple sequential events
with temporal coherence and controllable content. Existing methods that extend
to multi-event generation omit an inspection of the intrinsic factor in event
shifting. The paper aims to answer the central question: When and where
multi-event prompts control event transition during T2V generation. This work
introduces MEve, a self-curated prompt suite for evaluating multi-event
text-to-video (T2V) generation, and conducts a systematic study of two
representative model families, i.e., OpenSora and CogVideoX. Extensive
experiments demonstrate the importance of early intervention in denoising steps
and block-wise model layers, revealing the essential factor for multi-event
video generation and highlighting the possibilities for multi-event
conditioning in future models.

</details>


### [95] [InsideOut: An EfficientNetV2-S Based Deep Learning Framework for Robust Multi-Class Facial Emotion Recognition](https://arxiv.org/abs/2510.03066)
*Ahsan Farabi,Israt Khandaker,Ibrahim Khalil Shanto,Md Abdul Ahad Minhaz,Tanisha Zaman*

Main category: cs.CV

TL;DR: 本文提出了一种名为InsideOut的FER框架，该框架基于EfficientNetV2-S，并结合了迁移学习、数据增强和不平衡感知优化。


<details>
  <summary>Details</summary>
Motivation: 人脸表情识别(FER)是情感计算中的一项关键任务，但由于遮挡、光照和姿势变化、类内差异以及数据集不平衡等问题，FER仍然具有挑战性。

Method: 该方法标准化FER2013图像，应用分层分割和增强，并使用类加权损失微调轻量级分类头以解决倾斜分布。

Result: InsideOut在FER2013上实现了62.8%的准确率和0.590的宏平均F1，与传统的CNN基线相比显示出有竞争力的结果。

Conclusion: 研究表明，有效的架构与定制的不平衡处理相结合，可以提供实用、透明和可重复的FER解决方案。

Abstract: Facial Emotion Recognition (FER) is a key task in affective computing,
enabling applications in human-computer interaction, e-learning, healthcare,
and safety systems. Despite advances in deep learning, FER remains challenging
due to occlusions, illumination and pose variations, subtle intra-class
differences, and dataset imbalance that hinders recognition of minority
emotions. We present InsideOut, a reproducible FER framework built on
EfficientNetV2-S with transfer learning, strong data augmentation, and
imbalance-aware optimization. The approach standardizes FER2013 images, applies
stratified splitting and augmentation, and fine-tunes a lightweight
classification head with class-weighted loss to address skewed distributions.
InsideOut achieves 62.8% accuracy with a macro averaged F1 of 0.590 on FER2013,
showing competitive results compared to conventional CNN baselines. The novelty
lies in demonstrating that efficient architectures, combined with tailored
imbalance handling, can provide practical, transparent, and reproducible FER
solutions.

</details>


### [96] [What Drives Compositional Generalization in Visual Generative Models?](https://arxiv.org/abs/2510.03075)
*Karim Farid,Rajat Sahay,Yumna Ali Alnaggar,Simon Schrodi,Volker Fischer,Cordelia Schmid,Thomas Brox*

Main category: cs.CV

TL;DR: 这篇论文研究了视觉生成模型中的组合泛化能力，即生成已知概念的新组合的能力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在系统地研究各种设计选择如何以积极或消极的方式影响图像和视频生成中的组合泛化能力，从而更好地理解促进或抑制组合泛化的机制。

Method: 通过对照实验，论文确定了两个关键因素：(1) 训练目标是在离散分布上操作还是在连续分布上操作；(2) 在训练过程中，条件反射在多大程度上提供了关于组成概念的信息。

Result: 研究表明，使用基于JEPA的辅助连续目标函数放松MaskGIT离散损失可以提高像MaskGIT这样的离散模型中的组合性能。

Conclusion: 基于这些发现，论文提出了一种改进离散模型组合性能的方法，即通过辅助连续目标函数放松离散损失。

Abstract: Compositional generalization, the ability to generate novel combinations of
known concepts, is a key ingredient for visual generative models. Yet, not all
mechanisms that enable or inhibit it are fully understood. In this work, we
conduct a systematic study of how various design choices influence
compositional generalization in image and video generation in a positive or
negative way. Through controlled experiments, we identify two key factors: (i)
whether the training objective operates on a discrete or continuous
distribution, and (ii) to what extent conditioning provides information about
the constituent concepts during training. Building on these insights, we show
that relaxing the MaskGIT discrete loss with an auxiliary continuous JEPA-based
objective can improve compositional performance in discrete models like
MaskGIT.

</details>


### [97] [Latent Diffusion Unlearning: Protecting Against Unauthorized Personalization Through Trajectory Shifted Perturbations](https://arxiv.org/abs/2510.03089)
*Naresh Kumar Devulapally,Shruti Agarwal,Tejas Gokhale,Vishnu Suresh Lokhande*

Main category: cs.CV

TL;DR: 该论文提出了一种新的基于模型的扰动策略，该策略在扩散模型的潜在空间中运行，以生成“不可学习”的训练样本，从而防御未经授权的模型改编。


<details>
  <summary>Details</summary>
Motivation: 现有的个性化技术引发了人们对数据隐私、知识产权保护和未经授权使用的担忧。为了缓解此类未经授权的使用和模型复制，出现了生成利用图像中毒技术的“不可学习”训练样本的想法。

Method: 该方法在去噪和反演之间交替进行，同时修改扩散模型的去噪轨迹的起点。这种轨迹移位采样确保扰动图像保持对原始输入的高视觉保真度，同时抵抗下游生成模型的反演和个性化。

Result: 该方法在四个基准数据集上进行了验证，结果表明，该方法在不可感知性（在包括PSNR、SSIM和FID在内的感知指标上约为8%-10%）和鲁棒性（在五个对抗性设置中平均约为10%）方面取得了显著改善。

Conclusion: 该方法将不可学习性集成到潜在扩散模型（LDM）的框架中，从而实现了对未经授权模型改编的实用且难以察觉的防御。

Abstract: Text-to-image diffusion models have demonstrated remarkable effectiveness in
rapid and high-fidelity personalization, even when provided with only a few
user images. However, the effectiveness of personalization techniques has lead
to concerns regarding data privacy, intellectual property protection, and
unauthorized usage. To mitigate such unauthorized usage and model replication,
the idea of generating ``unlearnable'' training samples utilizing image
poisoning techniques has emerged. Existing methods for this have limited
imperceptibility as they operate in the pixel space which results in images
with noise and artifacts. In this work, we propose a novel model-based
perturbation strategy that operates within the latent space of diffusion
models. Our method alternates between denoising and inversion while modifying
the starting point of the denoising trajectory: of diffusion models. This
trajectory-shifted sampling ensures that the perturbed images maintain high
visual fidelity to the original inputs while being resistant to inversion and
personalization by downstream generative models. This approach integrates
unlearnability into the framework of Latent Diffusion Models (LDMs), enabling a
practical and imperceptible defense against unauthorized model adaptation. We
validate our approach on four benchmark datasets to demonstrate robustness
against state-of-the-art inversion attacks. Results demonstrate that our method
achieves significant improvements in imperceptibility ($\sim 8 \% -10\%$ on
perceptual metrics including PSNR, SSIM, and FID) and robustness ( $\sim 10\%$
on average across five adversarial settings), highlighting its effectiveness in
safeguarding sensitive data.

</details>


### [98] [Geometry Meets Vision: Revisiting Pretrained Semantics in Distilled Fields](https://arxiv.org/abs/2510.03104)
*Zhiting Mei,Ola Shorinwa,Anirudha Majumdar*

Main category: cs.CV

TL;DR: 本文研究了几何信息融入的语义特征在高斯溅射和神经辐射场中的作用，发现虽然几何信息能提供更精细的结构细节，但在物体定位和辐射场反演等任务中，纯视觉特征表现更好。


<details>
  <summary>Details</summary>
Motivation: 探讨几何信息融入的语义特征在辐射场中的潜力，特别是在位姿估计等空间任务中是否能带来优势。

Method: 1. 比较了几何信息融入和纯视觉的语义特征的质量和在物体定位上的表现。2. 提出了一个名为SPINE的框架，用于在没有初始猜测的情况下反演辐射场，该框架包含使用distilled semantics进行粗略反演和使用基于光度法的优化进行精细反演两个核心组件。

Result: 1. 几何信息融入的特征包含更精细的结构细节。2. 在语义物体定位任务上，没有观察到显著差异。3. 几何信息融入的特征降低了位姿估计的准确性。4. 纯视觉特征在更广泛的下游任务中表现出更大的通用性。

Conclusion: 纯视觉特征在下游任务中更具通用性，但几何信息融入的特征包含更多几何细节。未来的研究需要探索有效的几何信息融入策略，以增强预训练语义特征的通用性和性能。

Abstract: Semantic distillation in radiance fields has spurred significant advances in
open-vocabulary robot policies, e.g., in manipulation and navigation, founded
on pretrained semantics from large vision models. While prior work has
demonstrated the effectiveness of visual-only semantic features (e.g., DINO and
CLIP) in Gaussian Splatting and neural radiance fields, the potential benefit
of geometry-grounding in distilled fields remains an open question. In
principle, visual-geometry features seem very promising for spatial tasks such
as pose estimation, prompting the question: Do geometry-grounded semantic
features offer an edge in distilled fields? Specifically, we ask three critical
questions: First, does spatial-grounding produce higher-fidelity geometry-aware
semantic features? We find that image features from geometry-grounded backbones
contain finer structural details compared to their counterparts. Secondly, does
geometry-grounding improve semantic object localization? We observe no
significant difference in this task. Thirdly, does geometry-grounding enable
higher-accuracy radiance field inversion? Given the limitations of prior work
and their lack of semantics integration, we propose a novel framework SPINE for
inverting radiance fields without an initial guess, consisting of two core
components: coarse inversion using distilled semantics, and fine inversion
using photometric-based optimization. Surprisingly, we find that the pose
estimation accuracy decreases with geometry-grounded features. Our results
suggest that visual-only features offer greater versatility for a broader range
of downstream tasks, although geometry-grounded features contain more geometric
detail. Notably, our findings underscore the necessity of future research on
effective strategies for geometry-grounding that augment the versatility and
performance of pretrained semantic features.

</details>


### [99] [GeoComplete: Geometry-Aware Diffusion for Reference-Driven Image Completion](https://arxiv.org/abs/2510.03110)
*Beibei Lin,Tingting Chen,Robby T. Tan*

Main category: cs.CV

TL;DR: GeoComplete: 利用3D结构信息引导图像补全，显著提升几何一致性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在差异大的视图下补全效果差，缺乏几何信息。

Method: 提出GeoComplete框架，结合投影点云的几何信息，使用双分支扩散模型和目标感知mask。

Result: 实验结果显示，GeoComplete在PSNR上提升了17.1，显著提高了几何精度和视觉质量。

Conclusion: GeoComplete为几何条件下的图像补全提供了一个统一且鲁棒的解决方案。

Abstract: Reference-driven image completion, which restores missing regions in a target
view using additional images, is particularly challenging when the target view
differs significantly from the references. Existing generative methods rely
solely on diffusion priors and, without geometric cues such as camera pose or
depth, often produce misaligned or implausible content. We propose GeoComplete,
a novel framework that incorporates explicit 3D structural guidance to enforce
geometric consistency in the completed regions, setting it apart from prior
image-only approaches. GeoComplete introduces two key ideas: conditioning the
diffusion process on projected point clouds to infuse geometric information,
and applying target-aware masking to guide the model toward relevant reference
cues. The framework features a dual-branch diffusion architecture. One branch
synthesizes the missing regions from the masked target, while the other
extracts geometric features from the projected point cloud. Joint
self-attention across branches ensures coherent and accurate completion. To
address regions visible in references but absent in the target, we project the
target view into each reference to detect occluded areas, which are then masked
during training. This target-aware masking directs the model to focus on useful
cues, enhancing performance in difficult scenarios. By integrating a
geometry-aware dual-branch diffusion architecture with a target-aware masking
strategy, GeoComplete offers a unified and robust solution for
geometry-conditioned image completion. Experiments show that GeoComplete
achieves a 17.1 PSNR improvement over state-of-the-art methods, significantly
boosting geometric accuracy while maintaining high visual quality.

</details>


### [100] [Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction](https://arxiv.org/abs/2510.03117)
*Kaisi Guan,Xihua Wang,Zhengfeng Lai,Xin Cheng,Peng Zhang,XiaoJiang Liu,Ruihua Song,Meng Cao*

Main category: cs.CV

TL;DR: 本文提出了一种新的文本到声音视频生成（T2SV）框架，该框架可以生成具有同步音频的视频，同时确保两种模态都与文本对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的联合音频-视频训练方法存在两个关键挑战：单文本字幕容易造成模态干扰，以及跨模态特征交互的最佳机制尚不清楚。

Method: 本文首先提出分层视觉接地字幕（HVGC）框架，该框架生成解耦的字幕对，即视频字幕和音频字幕，从而消除调节阶段的干扰。在此基础上，进一步引入了BridgeDiT，一种新型双塔扩散Transformer，它采用双重交叉注意力（DCA）机制，充当稳健的“桥梁”，实现对称的、双向的信息交换。

Result: 在三个基准数据集上的大量实验以及人工评估表明，该方法在大多数指标上都取得了最先进的结果。

Conclusion: 全面的消融研究进一步验证了本文的贡献的有效性，为未来的T2SV任务提供了关键见解。

Abstract: This study focuses on a challenging yet promising task,
Text-to-Sounding-Video (T2SV) generation, which aims to generate a video with
synchronized audio from text conditions, meanwhile ensuring both modalities are
aligned with text. Despite progress in joint audio-video training, two critical
challenges still remain unaddressed: (1) a single, shared text caption where
the text for video is equal to the text for audio often creates modal
interference, confusing the pretrained backbones, and (2) the optimal mechanism
for cross-modal feature interaction remains unclear. To address these
challenges, we first propose the Hierarchical Visual-Grounded Captioning (HVGC)
framework that generates pairs of disentangled captions, a video caption, and
an audio caption, eliminating interference at the conditioning stage. Based on
HVGC, we further introduce BridgeDiT, a novel dual-tower diffusion transformer,
which employs a Dual CrossAttention (DCA) mechanism that acts as a robust
``bridge" to enable a symmetric, bidirectional exchange of information,
achieving both semantic and temporal synchronization. Extensive experiments on
three benchmark datasets, supported by human evaluations, demonstrate that our
method achieves state-of-the-art results on most metrics. Comprehensive
ablation studies further validate the effectiveness of our contributions,
offering key insights for the future T2SV task. All the codes and checkpoints
will be publicly released.

</details>


### [101] [HAVIR: HierArchical Vision to Image Reconstruction using CLIP-Guided Versatile Diffusion](https://arxiv.org/abs/2510.03122)
*Shiyi Zhang,Dong Liang,Hairong Zheng,Yihang Zhou*

Main category: cs.CV

TL;DR: 提出了一种新的HAVIR模型，用于从大脑活动中重建视觉信息，通过分离视觉皮层为两个层级区域，分别提取结构和语义特征，并结合 Versatile Diffusion 模型生成最终图像。


<details>
  <summary>Details</summary>
Motivation: 现有方法在准确恢复高度复杂的视觉刺激方面面临挑战，因为自然场景的低级特征表现出异质性，而高级特征由于上下文重叠而表现出语义纠缠。

Method: 将视觉皮层分为两个层级区域，Structural Generator 从空间处理体素中提取结构信息并转换为潜在扩散先验，Semantic Extractor 将语义处理体素转换为 CLIP 嵌入，通过 Versatile Diffusion 模型整合这些组件以合成最终图像。

Result: HAVIR 提高了重建的结构和语义质量，即使在复杂场景中也优于现有模型。

Conclusion: HAVIR 模型能够有效地从大脑活动中重建出高质量的视觉信息。

Abstract: The reconstruction of visual information from brain activity fosters
interdisciplinary integration between neuroscience and computer vision.
However, existing methods still face challenges in accurately recovering highly
complex visual stimuli. This difficulty stems from the characteristics of
natural scenes: low-level features exhibit heterogeneity, while high-level
features show semantic entanglement due to contextual overlaps. Inspired by the
hierarchical representation theory of the visual cortex, we propose the HAVIR
model, which separates the visual cortex into two hierarchical regions and
extracts distinct features from each. Specifically, the Structural Generator
extracts structural information from spatial processing voxels and converts it
into latent diffusion priors, while the Semantic Extractor converts semantic
processing voxels into CLIP embeddings. These components are integrated via the
Versatile Diffusion model to synthesize the final image. Experimental results
demonstrate that HAVIR enhances both the structural and semantic quality of
reconstructions, even in complex scenes, and outperforms existing models.

</details>


### [102] [Mask2IV: Interaction-Centric Video Generation via Mask Trajectories](https://arxiv.org/abs/2510.03135)
*Gen Li,Bo Zhao,Jianfei Yang,Laura Sevilla-Lara*

Main category: cs.CV

TL;DR: 提出了一种新的交互视频生成框架 Mask2IV，用于生成人与物体或机器人与物体交互的视频。


<details>
  <summary>Details</summary>
Motivation: 现有的方法难以模拟复杂和动态的交互，而获取密集和精确的掩码注释仍然是一个主要的挑战。

Method: Mask2IV 采用解耦的两阶段流程，首先预测参与者和物体的运动轨迹，然后根据这些轨迹生成视频。

Result: Mask2IV 在视觉真实性和可控性方面优于现有的基线方法。

Conclusion: Mask2IV 能够灵活地操纵交互过程，支持通用和直观的控制，并且在人与物体交互和机器人操作场景中都取得了优异的结果。

Abstract: Generating interaction-centric videos, such as those depicting humans or
robots interacting with objects, is crucial for embodied intelligence, as they
provide rich and diverse visual priors for robot learning, manipulation policy
training, and affordance reasoning. However, existing methods often struggle to
model such complex and dynamic interactions. While recent studies show that
masks can serve as effective control signals and enhance generation quality,
obtaining dense and precise mask annotations remains a major challenge for
real-world use. To overcome this limitation, we introduce Mask2IV, a novel
framework specifically designed for interaction-centric video generation. It
adopts a decoupled two-stage pipeline that first predicts plausible motion
trajectories for both actor and object, then generates a video conditioned on
these trajectories. This design eliminates the need for dense mask inputs from
users while preserving the flexibility to manipulate the interaction process.
Furthermore, Mask2IV supports versatile and intuitive control, allowing users
to specify the target object of interaction and guide the motion trajectory
through action descriptions or spatial position cues. To support systematic
training and evaluation, we curate two benchmarks covering diverse action and
object categories across both human-object interaction and robotic manipulation
scenarios. Extensive experiments demonstrate that our method achieves superior
visual realism and controllability compared to existing baselines.

</details>


### [103] [ReeMark: Reeb Graphs for Simulating Patterns of Life in Spatiotemporal Trajectories](https://arxiv.org/abs/2510.03152)
*Anantajit Subrahmanya,Chandrakanth Gudavalli,Connor Levenson,Umang Garg,B. S. Manjunath*

Main category: cs.CV

TL;DR: 提出了一种新的模拟时空轨迹的框架，该框架可以保留从基线数据中学习到的生活模式 (PoL)。


<details>
  <summary>Details</summary>
Motivation: 准确地建模人类的移动对于城市规划、流行病学和交通管理至关重要。

Method: 通过在概率拓扑模型中结合个体和群体层面的移动结构。

Result: 在城市异常数据集（亚特兰大和柏林子集）上使用 Jensen-Shannon 散度 (JSD) 跨人口和代理级别指标进行的评估表明，所提出的方法实现了强大的保真度，同时保持了数据和计算效率。

Conclusion: 马尔可夫 Reeb 图是一个可扩展的轨迹模拟框架，在不同的城市环境中具有广泛的适用性

Abstract: Accurately modeling human mobility is critical for urban planning,
epidemiology, and traffic management. In this work, we introduce Markovian Reeb
Graphs, a novel framework for simulating spatiotemporal trajectories that
preserve Patterns of Life (PoLs) learned from baseline data. By combining
individual- and population-level mobility structures within a probabilistic
topological model, our approach generates realistic future trajectories that
capture both consistency and variability in daily life. Evaluations on the
Urban Anomalies dataset (Atlanta and Berlin subsets) using the Jensen-Shannon
Divergence (JSD) across population- and agent-level metrics demonstrate that
the proposed method achieves strong fidelity while remaining data- and
compute-efficient. These results position Markovian Reeb Graphs as a scalable
framework for trajectory simulation with broad applicability across diverse
urban environments.

</details>


### [104] [SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the SpineMed-450k Corpus](https://arxiv.org/abs/2510.03160)
*Ming Zhao,Wenhui Dong,Yang Zhang,Xiang Zheng,Zhonghao Zhang,Zian Zhou,Yunzhi Guan,Liukun Xu,Wei Peng,Zhaoyang Gong,Zhicheng Zhang,Dachuan Li,Xiaosheng Ma,Yuli Ma,Jianing Ni,Changjiang Jiang,Lixia Tian,Qixin Chen,Kaishun Xia,Pingping Liu,Tongshun Zhang,Zhiqiang Liu,Zhongan Bi,Chenyang Si,Tiansheng Sun,Caifeng Shan*

Main category: cs.CV

TL;DR: SpineMed is introduced to address the lack of level-aware, multimodal datasets for AI-assisted diagnosis of spine disorders. It includes a large-scale dataset (SpineMed-450k) and a clinically-grounded evaluation framework (SpineBench).


<details>
  <summary>Details</summary>
Motivation: AI-assisted diagnosis for spine disorders is limited by the lack of level-aware, multimodal datasets. Clinical decision-making requires reasoning across different imaging modalities at specific vertebral levels, but there's a lack of traceable instruction data and standardized benchmarks.

Method: A clinician-in-the-loop pipeline with a two-stage LLM generation method (draft and revision) was used to curate SpineMed-450k from diverse sources. SpineBench is used to evaluate models on level identification, pathology assessment, and surgical planning.

Result: Evaluation of advanced large vision-language models (LVLMs) on SpineBench reveals weaknesses in fine-grained, level-specific reasoning. The model fine-tuned on SpineMed-450k shows consistent and significant improvements across all tasks. Clinician assessments confirm the diagnostic clarity and practical utility of the model's outputs.

Conclusion: SpineMed addresses the limitations in AI-assisted spine disorder diagnosis by providing a large-scale dataset and evaluation framework, enabling improved performance in level-specific reasoning and demonstrating practical utility in clinical settings.

Abstract: Spine disorders affect 619 million people globally and are a leading cause of
disability, yet AI-assisted diagnosis remains limited by the lack of
level-aware, multimodal datasets. Clinical decision-making for spine disorders
requires sophisticated reasoning across X-ray, CT, and MRI at specific
vertebral levels. However, progress has been constrained by the absence of
traceable, clinically-grounded instruction data and standardized,
spine-specific benchmarks. To address this, we introduce SpineMed, an ecosystem
co-designed with practicing spine surgeons. It features SpineMed-450k, the
first large-scale dataset explicitly designed for vertebral-level reasoning
across imaging modalities with over 450,000 instruction instances, and
SpineBench, a clinically-grounded evaluation framework. SpineMed-450k is
curated from diverse sources, including textbooks, guidelines, open datasets,
and ~1,000 de-identified hospital cases, using a clinician-in-the-loop pipeline
with a two-stage LLM generation method (draft and revision) to ensure
high-quality, traceable data for question-answering, multi-turn consultations,
and report generation. SpineBench evaluates models on clinically salient axes,
including level identification, pathology assessment, and surgical planning.
Our comprehensive evaluation of several recently advanced large vision-language
models (LVLMs) on SpineBench reveals systematic weaknesses in fine-grained,
level-specific reasoning. In contrast, our model fine-tuned on SpineMed-450k
demonstrates consistent and significant improvements across all tasks.
Clinician assessments confirm the diagnostic clarity and practical utility of
our model's outputs.

</details>


### [105] [UniShield: An Adaptive Multi-Agent Framework for Unified Forgery Image Detection and Localization](https://arxiv.org/abs/2510.03161)
*Qing Huang,Zhipei Xu,Xuanyu Zhang,Jian Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为 UniShield 的新型多智能体统一系统，用于检测和定位各种领域的图像伪造。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法 специализация 窄，跨域泛化能力差，缺乏集成的自适应框架。

Method: 集成了感知代理和检测代理。感知代理智能地分析图像特征以动态选择合适的检测模型，而检测代理将各种专家检测器整合到一个统一的框架中。

Result: UniShield 实现了最先进的结果，超过了现有的统一方法和特定领域的检测器。

Conclusion: UniShield 具有卓越的实用性、适应性和可扩展性。

Abstract: With the rapid advancements in image generation, synthetic images have become
increasingly realistic, posing significant societal risks, such as
misinformation and fraud. Forgery Image Detection and Localization (FIDL) thus
emerges as essential for maintaining information integrity and societal
security. Despite impressive performances by existing domain-specific detection
methods, their practical applicability remains limited, primarily due to their
narrow specialization, poor cross-domain generalization, and the absence of an
integrated adaptive framework. To address these issues, we propose UniShield,
the novel multi-agent-based unified system capable of detecting and localizing
image forgeries across diverse domains, including image manipulation, document
manipulation, DeepFake, and AI-generated images. UniShield innovatively
integrates a perception agent with a detection agent. The perception agent
intelligently analyzes image features to dynamically select suitable detection
models, while the detection agent consolidates various expert detectors into a
unified framework and generates interpretable reports. Extensive experiments
show that UniShield achieves state-of-the-art results, surpassing both existing
unified approaches and domain-specific detectors, highlighting its superior
practicality, adaptiveness, and scalability.

</details>


### [106] [ROGR: Relightable 3D Objects using Generative Relighting](https://arxiv.org/abs/2510.03163)
*Jiapeng Tang,Matthew Lavine,Dor Verbin,Stephan J. Garbin,Matthias Nießner,Ricardo Martin Brualla,Pratul P. Srinivasan,Philipp Henzler*

Main category: cs.CV

TL;DR: ROGR: 重建可重新光照的3D模型，通过生成式光照模型模拟不同环境光照下的物体效果。


<details>
  <summary>Details</summary>
Motivation: 从多角度捕获的物体，在新的环境光照下，重建可重新光照的3D模型。

Method: 使用生成式光照模型，对物体在多种光照环境下的外观进行采样，创建数据集，训练光照条件神经辐射场（NeRF）。NeRF采用双分支结构，分别编码一般光照效果和镜面反射。

Result: 在TensoIR和Stanford-ORB数据集上，ROGR在大多数指标上优于现有技术，并在真实物体捕获中展示了其有效性。

Conclusion: ROGR方法能够高效地进行前馈式重新光照，无需针对每个光照进行优化或光传输模拟。

Abstract: We introduce ROGR, a novel approach that reconstructs a relightable 3D model
of an object captured from multiple views, driven by a generative relighting
model that simulates the effects of placing the object under novel environment
illuminations. Our method samples the appearance of the object under multiple
lighting environments, creating a dataset that is used to train a
lighting-conditioned Neural Radiance Field (NeRF) that outputs the object's
appearance under any input environmental lighting. The lighting-conditioned
NeRF uses a novel dual-branch architecture to encode the general lighting
effects and specularities separately. The optimized lighting-conditioned NeRF
enables efficient feed-forward relighting under arbitrary environment maps
without requiring per-illumination optimization or light transport simulation.
We evaluate our approach on the established TensoIR and Stanford-ORB datasets,
where it improves upon the state-of-the-art on most metrics, and showcase our
approach on real-world object captures.

</details>


### [107] [Dynamic Prompt Generation for Interactive 3D Medical Image Segmentation Training](https://arxiv.org/abs/2510.03189)
*Tidiane Camaret Ndir,Alexander Pfefferle,Robin Tibor Schirrmeister*

Main category: cs.CV

TL;DR: 提出了一种训练策略，结合动态体积提示生成和内容感知的自适应裁剪，以优化图像编码器的使用。


<details>
  <summary>Details</summary>
Motivation: 现有的基础模型要么缺乏体积感知，要么交互能力有限。

Method: 该方法在训练过程中模拟真实的用户交互模式，同时解决在单个GPU上从顺序细化反馈中学习的计算挑战。为了高效训练，我们使用来自nnInteractive分割模型的公开权重初始化我们的网络。

Result: 在Foundation Models for Interactive 3D Biomedical Image Segmentation竞赛中表现出色，平均最终Dice评分为0.6385，归一化表面距离为0.6614，曲线下面积指标为2.4799 (Dice) 和2.5671 (NSD)。

Conclusion: 该方法在交互式3D生物医学图像分割任务中表现出强大的性能。

Abstract: Interactive 3D biomedical image segmentation requires efficient models that
can iteratively refine predictions based on user prompts. Current foundation
models either lack volumetric awareness or suffer from limited interactive
capabilities. We propose a training strategy that combines dynamic volumetric
prompt generation with content-aware adaptive cropping to optimize the use of
the image encoder. Our method simulates realistic user interaction patterns
during training while addressing the computational challenges of learning from
sequential refinement feedback on a single GPU. For efficient training, we
initialize our network using the publicly available weights from the
nnInteractive segmentation model. Evaluation on the \textbf{Foundation Models
for Interactive 3D Biomedical Image Segmentation} competition demonstrates
strong performance with an average final Dice score of 0.6385, normalized
surface distance of 0.6614, and area-under-the-curve metrics of 2.4799 (Dice)
and 2.5671 (NSD).

</details>


### [108] [Product-Quantised Image Representation for High-Quality Image Synthesis](https://arxiv.org/abs/2510.03191)
*Denis Zavadski,Nikita Philip Tatsch,Carsten Rother*

Main category: cs.CV

TL;DR: PQGAN: A quantised image autoencoder integrating Product Quantisation (PQ) into VQGAN, improving reconstruction and enabling faster/higher-resolution image generation.


<details>
  <summary>Details</summary>
Motivation: Limited usage of Product Quantisation (PQ) for latent representations in high-fidelity image generation.

Method: Integration of PQ into VQGAN framework with analysis of codebook size, embedding dimensionality, and subspace factorisation.

Result: Significant improvement in reconstruction performance (PSNR 37dB vs. prior 27dB) and reduction of FID, LPIPS, and CMMD scores by up to 96%.

Conclusion: PQGAN is a strong extension for discrete latent representation in image synthesis, enabling faster generation or higher resolution at no additional cost, and can be seamlessly integrated into pre-trained diffusion models.

Abstract: Product quantisation (PQ) is a classical method for scalable vector encoding,
yet it has seen limited usage for latent representations in high-fidelity image
generation. In this work, we introduce PQGAN, a quantised image autoencoder
that integrates PQ into the well-known vector quantisation (VQ) framework of
VQGAN. PQGAN achieves a noticeable improvement over state-of-the-art methods in
terms of reconstruction performance, including both quantisation methods and
their continuous counterparts. We achieve a PSNR score of 37dB, where prior
work achieves 27dB, and are able to reduce the FID, LPIPS, and CMMD score by up
to 96%. Our key to success is a thorough analysis of the interaction between
codebook size, embedding dimensionality, and subspace factorisation, with
vector and scalar quantisation as special cases. We obtain novel findings, such
that the performance of VQ and PQ behaves in opposite ways when scaling the
embedding dimension. Furthermore, our analysis shows performance trends for PQ
that help guide optimal hyperparameter selection. Finally, we demonstrate that
PQGAN can be seamlessly integrated into pre-trained diffusion models. This
enables either a significantly faster and more compute-efficient generation, or
a doubling of the output resolution at no additional cost, positioning PQ as a
strong extension for discrete latent representation in image synthesis.

</details>


### [109] [Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation on Minecraft](https://arxiv.org/abs/2510.03198)
*Junchao Huang,Xinting Hu,Boyao Han,Shaoshuai Shi,Zhuotao Tian,Tianyu He,Li Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种名为 Memory Forcing 的学习框架，用于提高自回归视频扩散模型在 Minecraft 游戏中的长期空间一致性和生成质量。


<details>
  <summary>Details</summary>
Motivation: 为了忠实地模拟游戏，模型必须生成自然内容，同时探索新场景，并在重新访问已探索区域时保持空间一致性。在有限的计算预算下，需要在有限的上下文中压缩和利用历史线索，这带来了一个权衡：仅时间记忆缺乏长期空间一致性，而添加空间记忆会加强一致性，但当模型过度依赖不充分的空间上下文时，可能会降低新场景生成质量。

Method: Memory Forcing 框架结合了训练协议和几何索引的空间记忆。混合训练 (Hybrid Training) 暴露了不同的游戏机制，引导模型在探索期间依赖时间记忆，并在重新访问时结合空间记忆。链式前向训练 (Chained Forward Training) 通过模型展开扩展了自回归训练，其中链式预测会产生更大的姿势变化，并鼓励依赖空间记忆来维持一致性。点到帧检索 (Point-to-Frame Retrieval) 通过将当前可见点映射到其源帧来有效地检索历史，而增量 3D 重建 (Incremental 3D Reconstruction) 维护和更新显式 3D 缓存。

Result: 大量实验表明，Memory Forcing 在不同的环境中实现了卓越的长期空间一致性和生成质量，同时保持了扩展序列的计算效率。

Conclusion: Memory Forcing 是一种有效的学习框架，可以提高自回归视频扩散模型在 Minecraft 游戏中的长期空间一致性和生成质量。

Abstract: Autoregressive video diffusion models have proved effective for world
modeling and interactive scene generation, with Minecraft gameplay as a
representative application. To faithfully simulate play, a model must generate
natural content while exploring new scenes and preserve spatial consistency
when revisiting explored areas. Under limited computation budgets, it must
compress and exploit historical cues within a finite context window, which
exposes a trade-off: Temporal-only memory lacks long-term spatial consistency,
whereas adding spatial memory strengthens consistency but may degrade new scene
generation quality when the model over-relies on insufficient spatial context.
We present Memory Forcing, a learning framework that pairs training protocols
with a geometry-indexed spatial memory. Hybrid Training exposes distinct
gameplay regimes, guiding the model to rely on temporal memory during
exploration and incorporate spatial memory for revisits. Chained Forward
Training extends autoregressive training with model rollouts, where chained
predictions create larger pose variations and encourage reliance on spatial
memory for maintaining consistency. Point-to-Frame Retrieval efficiently
retrieves history by mapping currently visible points to their source frames,
while Incremental 3D Reconstruction maintains and updates an explicit 3D cache.
Extensive experiments demonstrate that Memory Forcing achieves superior
long-term spatial consistency and generative quality across diverse
environments, while maintaining computational efficiency for extended
sequences.

</details>


### [110] [MonSTeR: a Unified Model for Motion, Scene, Text Retrieval](https://arxiv.org/abs/2510.03200)
*Luca Collorone,Matteo Gioia,Massimiliano Pappa,Paolo Leoni,Giovanni Ficarra,Or Litany,Indro Spinelli,Fabio Galasso*

Main category: cs.CV

TL;DR: 提出了一个名为MonSTeR的模型，用于评估骨骼运动（动作）、意图（文本）和周围环境（场景）之间的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有研究尚未提供工具来评估骨骼运动（动作）、意图（文本）和周围环境（场景）之间的一致性。

Method: MonSTeR通过利用单模和跨模态表示构建统一的潜在空间，从而捕获模态之间复杂的依赖关系。

Result: MonSTeR优于仅依赖于单模态表示的三模态模型。检索分数与人类偏好相符。MonSTeR的潜在空间在zero-shot场景物体放置和动作描述方面表现出多功能性。

Conclusion: MonSTeR模型能够有效对动作、意图和环境进行对齐和检索，并在多个任务中表现出良好的性能。

Abstract: Intention drives human movement in complex environments, but such movement
can only happen if the surrounding context supports it. Despite the intuitive
nature of this mechanism, existing research has not yet provided tools to
evaluate the alignment between skeletal movement (motion), intention (text),
and the surrounding context (scene). In this work, we introduce MonSTeR, the
first MOtioN-Scene-TExt Retrieval model. Inspired by the modeling of
higher-order relations, MonSTeR constructs a unified latent space by leveraging
unimodal and cross-modal representations. This allows MonSTeR to capture the
intricate dependencies between modalities, enabling flexible but robust
retrieval across various tasks. Our results show that MonSTeR outperforms
trimodal models that rely solely on unimodal representations. Furthermore, we
validate the alignment of our retrieval scores with human preferences through a
dedicated user study. We demonstrate the versatility of MonSTeR's latent space
on zero-shot in-Scene Object Placement and Motion Captioning. Code and
pre-trained models are available at github.com/colloroneluca/MonSTeR.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [111] [BrowserArena: Evaluating LLM Agents on Real-World Web Navigation Tasks](https://arxiv.org/abs/2510.02418)
*Sagnik Anupam,Davis Brown,Shuo Li,Eric Wong,Hamed Hassani,Osbert Bastani*

Main category: cs.AI

TL;DR: 提出了BrowserArena，一个用于评估web agent的平台，通过用户提交的任务和人工反馈来发现agent的失败模式。


<details>
  <summary>Details</summary>
Motivation: 现有的agent评估受限于沙盒环境或人工任务。

Method: 收集用户提交的任务，运行Arena式的head-to-head比较，并使用步骤级的人工反馈。

Result: 识别了三个常见的失败模式：验证码解决、弹窗广告移除和直接导航到URL。不同语言模型在解决这些失败模式时表现出差异。

Conclusion: 该研究揭示了当前web agent的多样性和脆弱性，并提供了一种大规模评估和理解web agent失败模式的方法。

Abstract: LLM web agents now browse and take actions on the open web, yet current agent
evaluations are constrained to sandboxed environments or artificial tasks. We
introduce BrowserArena, a live open-web agent evaluation platform that collects
user-submitted tasks, runs Arena-style head-to-head comparisons, and uses
step-level human feedback to surface failure modes. Collecting and analyzing
step-level annotations on the agent traces, we identify three consistent
failure modes: captcha resolution, pop-up banner removal, and direct navigation
to URLs. By constructing targeted datasets to further study these tasks, we
discover variations in how different language models navigate these failure
modes. We find, for example, that o4-mini deploys a wider variety of strategies
to circumvent captcha resolution than other models and DeepSeek-R1 consistently
misleads users about captcha resolution. Our findings surface both the
diversity and brittleness of current web agents. More broadly, our benchmarking
methodology provides an approach to evaluating and understanding web agent
failure modes at scale.

</details>


### [112] [RefineShot: Rethinking Cinematography Understanding with Foundational Skill Evaluation](https://arxiv.org/abs/2510.02423)
*Hang Wu,Yujun Cai,Haonan Ge,Hongkai Chen,Ming-Hsuan Yang,Yiwei Wang*

Main category: cs.AI

TL;DR: 本文对电影摄影理解任务的现有基准（ShotBench）和最先进模型（ShotVL）进行了分析，发现其存在选项设计模糊和推理一致性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 提高真实世界应用中的多模态理解能力，并支持电影和媒体中连贯的内容创作。

Method: 通过一致的选项重构系统地改进ShotBench，并对ShotVL的推理行为进行首次 критический 分析，引入扩展的评估协议。

Result: 提出了RefineShot，这是一个改进和扩展的基准，可以实现更可靠的评估，并促进电影摄影理解的未来发展。

Conclusion: 通过改进基准和评估方法，本文旨在解决现有电影摄影理解评估的局限性，从而促进该领域的未来发展。

Abstract: Cinematography understanding refers to the ability to recognize not only the
visual content of a scene but also the cinematic techniques that shape
narrative meaning. This capability is attracting increasing attention, as it
enhances multimodal understanding in real-world applications and underpins
coherent content creation in film and media. As the most comprehensive
benchmark for this task, ShotBench spans a wide range of cinematic concepts and
VQA-style evaluations, with ShotVL achieving state-of-the-art results on it.
However, our analysis reveals that ambiguous option design in ShotBench and
ShotVL's shortcomings in reasoning consistency and instruction adherence
undermine evaluation reliability, limiting fair comparison and hindering future
progress. To overcome these issues, we systematically refine ShotBench through
consistent option restructuring, conduct the first critical analysis of
ShotVL's reasoning behavior, and introduce an extended evaluation protocol that
jointly assesses task accuracy and core model competencies. These efforts lead
to RefineShot, a refined and expanded benchmark that enables more reliable
assessment and fosters future advances in cinematography understanding.

</details>


### [113] [Safe and Efficient In-Context Learning via Risk Control](https://arxiv.org/abs/2510.02480)
*Andrea Wynn,Metod Jazbec,Charith Peris,Rinat Khaziev,Anqi Liu,Daniel Khashabi,Eric Nalisnick*

Main category: cs.AI

TL;DR: 大型语言模型可以通过少量的上下文示例学习新任务，但也容易受到恶意示例的影响。本文提出了一种新方法，通过限制有害示例降低模型性能的程度来解决这个问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型可以通过上下文学习新任务，但也容易受到恶意示例的攻击，这引发了安全问题。

Method: 首先，定义模型的“安全”行为（零样本表现）。然后，应用无分布风险控制（DFRC）来限制上下文样本使性能低于零样本的程度。通过动态提前退出预测来实现，忽略那些最关注不安全输入的注意力头。最后，修改 DFRC，使其既能控制有害输入的风险，又能利用有益输入提高性能和效率。

Result: 提出的方法可以有效控制有害上下文示例的风险，同时通过有益的示例获得显著的计算效率提升。

Conclusion: 该研究提出了一种有效的方法来防御大型语言模型中由恶意上下文示例引起的安全问题，同时还能提高计算效率。

Abstract: Large language models (LLMs) demonstrate a remarkable ability to learn new
tasks from a few in-context examples. However, this flexibility introduces
safety concerns: LLMs can be influenced by incorrect or malicious
demonstrations -- for example, if an adversary tampers with or injects harmful
examples without a human supervisor noticing. This motivates principled designs
in which the system itself includes built-in mechanisms to guard against such
attacks. We propose a novel approach to limit the degree to which harmful
demonstrations can degrade model performance. First, we define a baseline
``safe'' behavior for the model -- the model's performance given no in-context
demonstrations (zero-shot). Next, we apply distribution-free risk control
(DFRC) to control the extent to which in-context samples can decay performance
below zero-shot. We achieve this by leveraging dynamic early exit prediction,
ignoring later attention heads that attend the most to the unsafe inputs.
Finally, we propose modifications to DFRC that allow it to both control risk
for harmful inputs \textit{and} leverage performance and efficiency gains on
helpful inputs. We present both theoretical and empirical results showing that
our approach can effectively control risk for harmful in-context demonstrations
while simultaneously achieving substantial computational efficiency gains with
helpful demonstrations.

</details>


### [114] [Multimodal Function Vectors for Spatial Relations](https://arxiv.org/abs/2510.02528)
*Shuhao Fu,Esther Goldberg,Ying Nian Wu,Hongjing Lu*

Main category: cs.AI

TL;DR: 大型多模态模型(LMMs)在有限的多模态演示中表现出令人印象深刻的上下文学习能力，但支持这种任务学习的内部机制仍然不明确。本文表明，视觉-语言模型OpenFlamingo-4B中一小部分注意力头负责传输空间关系的表示。这些注意力头（称为函数向量）的激活可以被提取和操纵，以改变LMM在关系任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 研究大型多模态模型(LMMs)中支持任务学习的内部机制。

Method: 1. 使用因果中介分析识别对关系预测有强烈影响的注意力头，并提取多模态函数向量，以提高推理时的zero-shot准确率。
2. 在保持LMM参数冻结的情况下，使用少量训练数据对这些多模态函数向量进行微调，以显著优于上下文学习基线。
3. 线性组合特定于关系的函数向量，以解决涉及新颖和未经训练的空间关系的类比问题。

Result: 1. 识别出LMMs中编码空间关系知识的局部内部结构。
2. 提取和优化这些结构可以提高模型模块化，并增强对LMMs中关系推理的控制。

Conclusion: LMMs在局部内部结构中编码空间关系知识，这些结构可以被系统地提取和优化，从而提高我们对模型模块化的理解，并增强对LMMs中关系推理的控制。

Abstract: Large Multimodal Models (LMMs) demonstrate impressive in-context learning
abilities from limited multimodal demonstrations, yet the internal mechanisms
supporting such task learning remain opaque. Building on prior work of large
language models, we show that a small subset of attention heads in the
vision-language model OpenFlamingo-4B is responsible for transmitting
representations of spatial relations. The activations of these attention heads,
termed function vectors, can be extracted and manipulated to alter an LMM's
performance on relational tasks. First, using both synthetic and real image
datasets, we apply causal mediation analysis to identify attention heads that
strongly influence relational predictions, and extract multimodal function
vectors that improve zero-shot accuracy at inference time. We further
demonstrate that these multimodal function vectors can be fine-tuned with a
modest amount of training data, while keeping LMM parameters frozen, to
significantly outperform in-context learning baselines. Finally, we show that
relation-specific function vectors can be linearly combined to solve analogy
problems involving novel and untrained spatial relations, highlighting the
strong generalization ability of this approach. Our results show that LMMs
encode spatial relational knowledge within localized internal structures, which
can be systematically extracted and optimized, thereby advancing our
understanding of model modularity and enhancing control over relational
reasoning in LMMs.

</details>


### [115] [Orchestrating Human-AI Teams: The Manager Agent as a Unifying Research Challenge](https://arxiv.org/abs/2510.02557)
*Charlie Masters,Advaith Vellanki,Jiangbo Shangguan,Bart Kultys,Jonathan Gilmore,Alastair Moore,Stefano V. Albrecht*

Main category: cs.AI

TL;DR: 自主Agent AI在自动化任务方面取得了进展，但管理复杂的多Agent工作流程仍然是一个难题。本文提出了自主Agent系统的研究愿景，该系统可以协调动态人机团队中的协作。


<details>
  <summary>Details</summary>
Motivation: Agent AI在自动化个体任务上有所进展，但复杂多Agent工作流的管理仍然具有挑战性。

Method: 将工作流管理形式化为部分可观察随机博弈，并确定四个基础挑战：(1)用于分层分解的组合推理，(2)在不断变化的偏好下的多目标优化，(3)临时团队中的协调和规划，以及(4)通过设计的治理和合规性。为了推进这一议程，我们发布了MA-Gym，这是一个用于多Agent工作流编排的开源仿真和评估框架。

Result: 在20个工作流中评估基于GPT-5的Manager Agents，发现它们难以共同优化目标完成、约束遵守和工作流运行时间——强调工作流管理是一个困难的开放问题。

Conclusion: 自主管理系统具有组织和伦理意义。

Abstract: While agentic AI has advanced in automating individual tasks, managing
complex multi-agent workflows remains a challenging problem. This paper
presents a research vision for autonomous agentic systems that orchestrate
collaboration within dynamic human-AI teams. We propose the Autonomous Manager
Agent as a core challenge: an agent that decomposes complex goals into task
graphs, allocates tasks to human and AI workers, monitors progress, adapts to
changing conditions, and maintains transparent stakeholder communication. We
formalize workflow management as a Partially Observable Stochastic Game and
identify four foundational challenges: (1) compositional reasoning for
hierarchical decomposition, (2) multi-objective optimization under shifting
preferences, (3) coordination and planning in ad hoc teams, and (4) governance
and compliance by design. To advance this agenda, we release MA-Gym, an
open-source simulation and evaluation framework for multi-agent workflow
orchestration. Evaluating GPT-5-based Manager Agents across 20 workflows, we
find they struggle to jointly optimize for goal completion, constraint
adherence, and workflow runtime - underscoring workflow management as a
difficult open problem. We conclude with organizational and ethical
implications of autonomous management systems.

</details>


### [116] [Agentic Additive Manufacturing Alloy Discovery](https://arxiv.org/abs/2510.02567)
*Peter Pak,Achuth Chandrasekhar,Amir Barati Farimani*

Main category: cs.AI

TL;DR: 利用LLM驱动的智能体系统来加速增材制造领域的合金发现。


<details>
  <summary>Details</summary>
Motivation: 合金发现是一个复杂的问题，需要材料科学、热力学模拟和实验分析等多领域的专业知识。LLM可以通过调用工具来辅助研究人员。

Method: 开发了一个多智能体系统，该系统可以通过模型上下文协议(MCP)调用Thermo-Calc等工具，并能根据工具结果动态调整任务。

Result: 该智能体系统能够有效地理解复杂的用户提示，并分析所提出的合金的可打印性。

Conclusion: 利用LLM驱动的智能体可以自动化和加速增材制造领域的合金发现任务，并展示了采用这种多智能体系统的好处。

Abstract: Agentic systems enable the intelligent use of research tooling, augmenting a
researcher's ability to investigate and propose novel solutions to existing
problems. Within Additive Manufacturing (AM), alloy discovery remains a complex
challenge, often requiring expertise in the various domains of materials
science, thermodynamic simulations, and experimental analysis. Large Language
Model (LLM) enabled agents can facilitate this endeavor by utilizing their
extensive knowledge base to dispatch tool calls via Model Context Protocol
(MCP) to perform actions such as Thermo-Calc property diagram calculations and
lack of fusion process map generation. In addition, the multi-agent system
developed in this work is able to effectively reason through complex user
prompts and provide analysis on the printability of proposed alloys. These
agents can dynamically adjust their task trajectory to the outcomes of tool
call results, effectively enabling autonomous decision-making in practical
environments. This work aims to utilize LLM enabled agents to automate and
accelerate the task of alloy discovery within the field of additive
manufacturing and showcase the benefits of adopting this multi-agent system.

</details>


### [117] [A Benchmark Study of Deep Reinforcement Learning Algorithms for the Container Stowage Planning Problem](https://arxiv.org/abs/2510.02589)
*Yunqi Huang,Nishith Chennakeshava,Alexis Carras,Vladislav Neverov,Wei Liu,Aske Plaat,Yingjie Fan*

Main category: cs.AI

TL;DR: 本文对集装箱积载计划（CSPP）中的强化学习（RL）算法进行了基准测试。


<details>
  <summary>Details</summary>
Motivation: 集装箱积载计划 (CSPP) 对供应链效率至关重要，但传统上依赖于人工专业知识。尽管强化学习 (RL) 最近已应用于 CSPP，但不同算法之间的系统基准比较仍然有限。

Method: 我们开发了一个 Gym 环境，该环境捕获 CSPP 的基本特征，并将其扩展到包括多智能体和单智能体公式中的起重机调度。我们在此框架内评估了五种 RL 算法：DQN、QR-DQN、A2C、PPO 和 TRPO 在多种不同复杂程度的场景下。

Result: 结果揭示了随着复杂性的增加，性能差距明显，这突显了算法选择和问题公式对于 CSPP 的重要性。

Conclusion: 本文对 CSPP 的多种 RL 方法进行了基准测试，同时提供了一个带有起重机调度的可重复使用的 Gym 环境，从而为未来的研究和海运物流的实际部署奠定了基础。

Abstract: Container stowage planning (CSPP) is a critical component of maritime
transportation and terminal operations, directly affecting supply chain
efficiency. Owing to its complexity, CSPP has traditionally relied on human
expertise. While reinforcement learning (RL) has recently been applied to CSPP,
systematic benchmark comparisons across different algorithms remain limited. To
address this gap, we develop a Gym environment that captures the fundamental
features of CSPP and extend it to include crane scheduling in both multi-agent
and single-agent formulations. Within this framework, we evaluate five RL
algorithms: DQN, QR-DQN, A2C, PPO, and TRPO under multiple scenarios of varying
complexity. The results reveal distinct performance gaps with increasing
complexity, underscoring the importance of algorithm choice and problem
formulation for CSPP. Overall, this paper benchmarks multiple RL methods for
CSPP while providing a reusable Gym environment with crane scheduling, thus
offering a foundation for future research and practical deployment in maritime
logistics.

</details>


### [118] [Multimodal Large Language Model Framework for Safe and Interpretable Grid-Integrated EVs](https://arxiv.org/abs/2510.02592)
*Jean Douglas Carvalho,Hugo Kenji,Ahmad Mohammad Saber,Glaucia Melo,Max Mauro Dias Santos,Deepa Kundur*

Main category: cs.AI

TL;DR: 本文提出了一种基于多模态大语言模型（LLM）的框架，用于处理多模态传感器数据并为驾驶员生成自然语言警报，以提高电动汽车在智能电网中的安全性。


<details>
  <summary>Details</summary>
Motivation: 确保驾驶员、车辆和周围环境之间安全且可解释的交互仍然是一个关键挑战。

Method: 该框架结合了视觉感知（YOLOv8）、地理编码定位和CAN总线遥测技术。

Result: 使用来自在城市道路上行驶的车辆的真实数据验证了该框架的有效性，并通过案例研究展示了其在生成情境感知警报方面的有效性。

Conclusion: 本文强调了LLM作为电动汽车辅助工具的潜力，通过实现可扩展的车队协调、电动汽车负载预测和交通感知能源规划，使交通系统和电网受益。

Abstract: The integration of electric vehicles (EVs) into smart grids presents unique
opportunities to enhance both transportation systems and energy networks.
However, ensuring safe and interpretable interactions between drivers,
vehicles, and the surrounding environment remains a critical challenge. This
paper presents a multi-modal large language model (LLM)-based framework to
process multimodal sensor data - such as object detection, semantic
segmentation, and vehicular telemetry - and generate natural-language alerts
for drivers. The framework is validated using real-world data collected from
instrumented vehicles driving on urban roads, ensuring its applicability to
real-world scenarios. By combining visual perception (YOLOv8), geocoded
positioning, and CAN bus telemetry, the framework bridges raw sensor data and
driver comprehension, enabling safer and more informed decision-making in urban
driving scenarios. Case studies using real data demonstrate the framework's
effectiveness in generating context-aware alerts for critical situations, such
as proximity to pedestrians, cyclists, and other vehicles. This paper
highlights the potential of LLMs as assistive tools in e-mobility, benefiting
both transportation systems and electric networks by enabling scalable fleet
coordination, EV load forecasting, and traffic-aware energy planning.
  Index Terms - Electric vehicles, visual perception, large language models,
YOLOv8, semantic segmentation, CAN bus, prompt engineering, smart grid.

</details>


### [119] [Mitigating Modal Imbalance in Multimodal Reasoning](https://arxiv.org/abs/2510.02608)
*Chen Henry Wu,Neil Kale,Aditi Raghunathan*

Main category: cs.AI

TL;DR: 研究了基础模型在处理跨模态冲突时的表现，发现模型在跨模态推理方面存在困难。


<details>
  <summary>Details</summary>
Motivation: 探究基础模型在多模态联合推理方面的能力，特别是在模态之间存在冲突时。

Method: 通过构建跨模态冲突场景，研究基础模型如何处理不同模态之间的冲突证据，并分析模型的注意力机制。

Result: 发现基础模型在单模态冲突识别率高，但在跨模态冲突识别率低，且存在跨模态注意力失衡问题。

Conclusion: 强调了系统性解决跨模态上下文问题对于构建可靠基础模型的重要性，并提出了一种简单的提升跨模态推理能力的方法。

Abstract: Foundation models (FMs) deployed in real-world tasks such as computer-use
agents must integrate diverse modalities. How good are FMs at performing joint
reasoning, simultaneously reasoning over multiple modalities, especially when
the modalities interact and relate to each other to form cross-modal context?
To better understand this problem, we study FMs on cross-modal conflicts:
scenarios where conflicting evidence is presented across modalities. This
allows us to examine whether FMs prioritize one modality over another or reason
jointly to reconcile the conflict. Our experiments reveal that FMs can
recognize conflicts in unimodal contexts, composed of a single modality, 90% of
the time, but the ratio falls as low as 3% when evidence is split across
modalities -- similar observations hold in cross-lingual contexts, composed of
multiple languages. We trace this failure to cross-modal attention imbalance,
showing that FMs exhibit extreme asymmetry in attention scores,
disproportionately prioritizing certain modalities. We show that cross-modal
attention imbalance does not go away by simply scaling up multimodal or
multilingual datasets blindly, since they lack training examples that
explicitly require cross-modal reasoning. We demonstrate that even a simple and
scalable method of explicitly combining multiple modalities within each
training instance significantly reduces attention imbalance. Reduced attention
imbalance directly translates to improved downstream performance on several
vision-language benchmarks. Our findings underscore the importance of
systematically addressing cross-modal contexts to build reliable foundation
models.

</details>


### [120] [On the Role of Temperature Sampling in Test-Time Scaling](https://arxiv.org/abs/2510.02611)
*Yuheng Wu,Azalia Mirhoseini,Thierry Tambe*

Main category: cs.AI

TL;DR: 大规模语言模型可以通过测试时缩放（TTS）来提高推理能力，即生成多个推理轨迹并选择最佳轨迹。本文表明，增加样本数量K并不能无限地提高准确性。当K很大时，进一步缩放不会产生增益，某些难题仍然无法解决。通过温度缩放，可以扩大LLM的推理范围。


<details>
  <summary>Details</summary>
Motivation: 先前的研究表明，增加样本数量K可以稳定地提高准确性。然而，本文发现这种趋势并非无限持续，当K很大时，进一步缩放不会产生增益，某些难题仍然无法解决。不同的采样温度可以解决不同的问题子集，这意味着单温度缩放仅探索了模型潜力的一部分。

Method: 本文提出沿温度维度进行缩放，这扩大了LLM的推理边界。设计了一种多温度投票方法，以减少温度缩放的开销。

Result: 在Qwen3 (0.6B, 1.7B, 4B, 8B)和五个代表性推理基准测试（AIME 2024/2025, MATH500, LiveCodeBench, Hi-ToM）上进行平均，温度缩放比单温度TTS提高了7.3个点。温度缩放还使基础模型能够达到与强化学习（RL）训练的对应模型相当的性能，而无需额外的后训练。

Conclusion: TTS比以前认为的更强大，并且温度缩放提供了一种简单有效的方法来释放基础模型的潜在能力。

Abstract: Large language models (LLMs) can improve reasoning at inference time through
test-time scaling (TTS), where multiple reasoning traces are generated and the
best one is selected. Prior work shows that increasing the number of samples K
steadily improves accuracy. In this paper, we demonstrate that this trend does
not hold indefinitely: at large K, further scaling yields no gains, and certain
hard questions remain unsolved regardless of the number of traces.
Interestingly, we find that different sampling temperatures solve different
subsets of problems, implying that single-temperature scaling explores only
part of a model's potential. We therefore propose scaling along the temperature
dimension, which enlarges the reasoning boundary of LLMs. Averaged over Qwen3
(0.6B, 1.7B, 4B, 8B) and five representative reasoning benchmarks (AIME
2024/2025, MATH500, LiveCodeBench, Hi-ToM), temperature scaling yields an
additional 7.3 points over single-temperature TTS. Temperature scaling also
enables base models to reach performance comparable to reinforcement learning
(RL)-trained counterparts, without additional post-training. We further provide
a comprehensive analysis of this phenomenon and design a multi-temperature
voting method that reduces the overhead of temperature scaling. Overall, our
findings suggest that TTS is more powerful than previously thought, and that
temperature scaling offers a simple and effective way to unlock the latent
potential of base models.

</details>


### [121] [Geolog-IA: Conversational System for Academic Theses](https://arxiv.org/abs/2510.02653)
*Micaela Fuel Pozo,Andrea Guatumillo Saltos,Yeseña Tipan Llumiquinga,Kelly Lascano Aguirre,Marilyn Castillo Jara,Christian Mejia-Escobar*

Main category: cs.AI

TL;DR: 开发了一个名为Geolog-IA的AI对话系统，可以自然地回答关于厄瓜多尔中央大学地质论文的问题。


<details>
  <summary>Details</summary>
Motivation: 旨在克服幻觉和知识过时的问题，为学校师生提供便捷的信息检索工具，支持教学、培训和研究。

Method: 使用Llama 3.1和Gemini 2.5语言模型，结合RAG架构和SQLite数据库。

Result: BLEU评估指标平均达到0.87，表明生成回答具有高度一致性和准确性。

Conclusion: Geolog-IA是一个在教育、培训和研究方面具有关键支持作用的工具，并为未来在其他学科的应用奠定了基础。

Abstract: This study presents the development of Geolog-IA, a novel conversational
system based on artificial intelligence that responds naturally to questions
about geology theses from the Central University of Ecuador. Our proposal uses
the Llama 3.1 and Gemini 2.5 language models, which are complemented by a
Retrieval Augmented Generation (RAG) architecture and an SQLite database. This
strategy allows us to overcome problems such as hallucinations and outdated
knowledge. The evaluation of Geolog-IA's performance with the BLEU metric
reaches an average of 0.87, indicating high consistency and accuracy in the
responses generated. The system offers an intuitive, web-based interface that
facilitates interaction and information retrieval for directors, teachers,
students, and administrative staff at the institution. This tool can be a key
support in education, training, and research and establishes a basis for future
applications in other disciplines.

</details>


### [122] [AutoMaAS: Self-Evolving Multi-Agent Architecture Search for Large Language Models](https://arxiv.org/abs/2510.02669)
*Bo Ma,Hang Li,ZeHua Hu,XiaoFan Gui,LuYao Liu,Simon Liu*

Main category: cs.AI

TL;DR: AutoMaAS: A self-evolving multi-agent architecture search framework.


<details>
  <summary>Details</summary>
Motivation: Existing automated design approaches seek monolithic solutions that fail to adapt resource allocation based on query complexity and domain requirements.

Method: Leverages neural architecture search principles to automatically discover optimal agent configurations through dynamic operator lifecycle management and automated machine learning techniques. Incorporates automatic operator generation/fusion/elimination, dynamic cost-aware optimization, online feedback integration and enhanced interpretability.

Result: Achieves 1.0-7.1% performance improvement while reducing inference costs by 3-5% compared to state-of-the-art methods. Shows superior transferability across datasets and LLM backbones.

Conclusion: Establishes a new paradigm for automated multi-agent system design in the era of large language models.

Abstract: Multi-agent systems powered by large language models have demonstrated
remarkable capabilities across diverse domains, yet existing automated design
approaches seek monolithic solutions that fail to adapt resource allocation
based on query complexity and domain requirements. This paper introduces
AutoMaAS, a self-evolving multi-agent architecture search framework that
leverages neural architecture search principles to automatically discover
optimal agent configurations through dynamic operator lifecycle management and
automated machine learning techniques. Our approach incorporates four key
innovations: (1) automatic operator generation, fusion, and elimination based
on performance-cost analysis, (2) dynamic cost-aware optimization with
real-time parameter adjustment, (3) online feedback integration for continuous
architecture refinement, and (4) enhanced interpretability through decision
tracing mechanisms. Extensive experiments across six benchmarks demonstrate
that AutoMaAS achieves 1.0-7.1\% performance improvement while reducing
inference costs by 3-5\% compared to state-of-the-art methods. The framework
shows superior transferability across datasets and LLM backbones, establishing
a new paradigm for automated multi-agent system design in the era of large
language models.

</details>


### [123] [A Concept of Possibility for Real-World Events](https://arxiv.org/abs/2510.02655)
*Daniel G. Schwartz*

Main category: cs.AI

TL;DR: 提出了一种新的可能性概念，作为Zadeh在1978年提出的标准概念的替代方案。


<details>
  <summary>Details</summary>
Motivation: 专注于现实世界事件的可能性，认为事件的发生有先决条件和约束。

Method: 通过计算先决条件成立和约束不成立的概率来计算事件的可能性。

Result: 该理论可以用于确定哪个计划最有可能完成，并通过车辆路线规划的例子加以说明。

Conclusion: 该模型正确地捕捉了人类关于计划的正常推理，并提出了潜在的未来应用。

Abstract: This paper offers a new concept of {\it possibility} as an alternative to the
now-a-days standard concept originally introduced by L.A. Zadeh in 1978. This
new version was inspired by the original but, formally, has nothing in common
with it other than that they both adopt the {\L}ukasiewicz multivalent
interpretation of the logical connectives. Moreover, rather than seeking to
provide a general notion of possibility, this focuses specifically on the
possibility of a real-world event. An event is viewed as having prerequisites
that enable its occurrence and constraints that may impede its occurrence, and
the possibility of the event is computed as a function of the probabilities
that the prerequisites hold and the constraints do not. This version of
possibility might appropriately be applied to problems of planning. When there
are multiple plans available for achieving a goal, this theory can be used to
determine which plan is most possible, i.e., easiest or most feasible to
complete. It is speculated that this model of reasoning correctly captures
normal human reasoning about plans. The theory is elaborated and an
illustrative example for vehicle route planning is provided. There is also a
suggestion of potential future applications.

</details>


### [124] [ARMs: Adaptive Red-Teaming Agent against Multimodal Models with Plug-and-Play Attacks](https://arxiv.org/abs/2510.02677)
*Zhaorun Chen,Xun Liu,Mintong Kang,Jiawei Zhang,Minzhou Pan,Shuang Yang,Bo Li*

Main category: cs.AI

TL;DR: 这篇论文提出了一种名为ARMs的自适应红队代理，用于系统地评估视觉-语言模型(VLM)的风险。


<details>
  <summary>Details</summary>
Motivation: 现有的红队方法在识别VLM的实际漏洞方面存在局限性，无法规模化探索。

Method: ARMs通过推理增强的多步骤编排自动优化多样化的红队策略，以引出VLM的有害输出。它包含11种新的多模态攻击策略，并通过模型上下文协议集成了17种红队算法。此外，设计了一个分层记忆和epsilon-greedy攻击探索算法来平衡攻击的多样性和有效性。

Result: 实验表明，ARMs达到了最先进的攻击成功率，平均超过基线52.1%，在Claude-4-Sonnet上超过90%。ARMs生成的红队实例的多样性显著提高，揭示了VLM中新出现的漏洞。利用ARMs构建了一个大规模的多模态安全数据集ARMs-Bench。

Conclusion: 通过使用ARMs-Bench进行安全微调，可以显著提高VLM的鲁棒性，同时保持其通用性，为改进多模态安全对齐提供了可操作的指导。

Abstract: As vision-language models (VLMs) gain prominence, their multimodal interfaces
also introduce new safety vulnerabilities, making the safety evaluation
challenging and critical. Existing red-teaming efforts are either restricted to
a narrow set of adversarial patterns or depend heavily on manual engineering,
lacking scalable exploration of emerging real-world VLM vulnerabilities. To
bridge this gap, we propose ARMs, an adaptive red-teaming agent that
systematically conducts comprehensive risk assessments for VLMs. Given a target
harmful behavior or risk definition, ARMs automatically optimizes diverse
red-teaming strategies with reasoning-enhanced multi-step orchestration, to
effectively elicit harmful outputs from target VLMs. We propose 11 novel
multimodal attack strategies, covering diverse adversarial patterns of VLMs
(e.g., reasoning hijacking, contextual cloaking), and integrate 17 red-teaming
algorithms into ARMs via model context protocol (MCP). To balance the diversity
and effectiveness of the attack, we design a layered memory with an
epsilon-greedy attack exploration algorithm. Extensive experiments on instance-
and policy-based benchmarks show that ARMs achieves SOTA attack success rates,
exceeding baselines by an average of 52.1% and surpassing 90% on
Claude-4-Sonnet. We show that the diversity of red-teaming instances generated
by ARMs is significantly higher, revealing emerging vulnerabilities in VLMs.
Leveraging ARMs, we construct ARMs-Bench, a large-scale multimodal safety
dataset comprising over 30K red-teaming instances spanning 51 diverse risk
categories, grounded in both real-world multimodal threats and regulatory
risks. Safety fine-tuning with ARMs-Bench substantially improves the robustness
of VLMs while preserving their general utility, providing actionable guidance
to improve multimodal safety alignment against emerging threats.

</details>


### [125] [Automated Constraint Specification for Job Scheduling by Regulating Generative Model with Domain-Specific Representation](https://arxiv.org/abs/2510.02679)
*Yu-Zhe Shi,Qiao Xu,Yanjia Li,Mingchen Liu,Huamin Qu,Lecheng Ruan,Qining Wang*

Main category: cs.AI

TL;DR: 本文提出了一种约束中心架构，该架构规范了大型语言模型（LLM）的性能，以执行可靠的自动化约束规范，用于生产调度。


<details>
  <summary>Details</summary>
Motivation: 虽然解决抽象调度问题的算法已被广泛研究，但将制造需求指定为正式约束的关键先决条件仍然是手动且劳动密集型的。虽然生成模型的最新进展，特别是大型语言模型（LLM），在自动化从异构原始制造数据中进行约束规范方面显示出希望，但由于自然语言的模糊性、非确定性输出和有限的领域知识，它们的直接应用面临挑战。

Method: 该架构定义了一个跨三个层次组织的分层结构空间，通过特定领域的表示来实现，以确保精度和可靠性，同时保持灵活性。此外，还设计和部署了一种自动生产场景适应算法，以有效地为特定制造配置定制架构。

Result: 实验结果表明，所提出的方法成功地平衡了LLM的生成能力与制造系统的可靠性要求，在约束规范任务中显著优于纯粹基于LLM的方法。

Conclusion: 本文提出了一种约束中心架构，该架构规范了大型语言模型（LLM）的性能，以执行可靠的自动化约束规范，用于生产调度。

Abstract: Advanced Planning and Scheduling (APS) systems have become indispensable for
modern manufacturing operations, enabling optimized resource allocation and
production efficiency in increasingly complex and dynamic environments. While
algorithms for solving abstracted scheduling problems have been extensively
investigated, the critical prerequisite of specifying manufacturing
requirements into formal constraints remains manual and labor-intensive.
Although recent advances of generative models, particularly Large Language
Models (LLMs), show promise in automating constraint specification from
heterogeneous raw manufacturing data, their direct application faces challenges
due to natural language ambiguity, non-deterministic outputs, and limited
domain-specific knowledge. This paper presents a constraint-centric
architecture that regulates LLMs to perform reliable automated constraint
specification for production scheduling. The architecture defines a
hierarchical structural space organized across three levels, implemented
through domain-specific representation to ensure precision and reliability
while maintaining flexibility. Furthermore, an automated production scenario
adaptation algorithm is designed and deployed to efficiently customize the
architecture for specific manufacturing configurations. Experimental results
demonstrate that the proposed approach successfully balances the generative
capabilities of LLMs with the reliability requirements of manufacturing
systems, significantly outperforming pure LLM-based approaches in constraint
specification tasks.

</details>


### [126] [NCV: A Node-Wise Consistency Verification Approach for Low-Cost Structured Error Localization in LLM Reasoning](https://arxiv.org/abs/2510.02816)
*Yulong Zhang,Li Wang,Wei Du,Peilin Li,Yuqin Dai Zhiyuan Zhao,Lingyong Fang,Ziniu Liu,Ru Zhang,Huijia Zhu,Gongshen Liu*

Main category: cs.AI

TL;DR: 提出了一个名为 Node-wise Consistency Verification (NCV) 的训练free框架，用于验证大型语言模型中的多步推理，通过将思维链分解为互连的验证节点，NCV 能够精确定位错误并避免不必要的长格式生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么评估整个推理链，要么依赖昂贵的多重采样，导致误差定位不准确和token成本高昂。

Method: 将验证重铸为节点级别的轻量级二元一致性检查，将思维链分解为互连的验证节点。

Result: 在公共数据集上，NCV 的 F1 分数比基线提高了 10% 到 25%，同时比传统的基于 CoT 的验证器使用的 token 数量减少了 6 倍到 58 倍。

Conclusion: NCV 提高了可解释性和效率，为可靠的 LLM 推理验证提供了一种可扩展的解决方案。

Abstract: Verifying multi-step reasoning in large language models is difficult due to
imprecise error localization and high token costs. Existing methods either
assess entire reasoning chains, suffering attention dilution, or rely on
expensive multi-sampling. We introduce Node-wise Consistency Verification
(NCV), a training-free framework that recasts verification as lightweight
binary consistency checks at the node level. By decomposing the chain of
thought into interconnected verification nodes, NCV precisely localizes errors
and avoids unnecessary long-form generation. Experiments demonstrate that our
approach enhances interpretability and efficiency, presenting a scalable
solution for reliable LLM reasoning verification. On public datasets, NCV
achieves a 10\% to 25\% improvement in F1 scores over baselines while utilizing
$6\times$~$58\times$ fewer tokens than traditional methods like CoT-based
verifiers.

</details>


### [127] [Beyond the Final Answer: Evaluating the Reasoning Trajectories of Tool-Augmented Agents](https://arxiv.org/abs/2510.02837)
*Wonjoong Kim,Sangwu Park,Yeonjun In,Sein Kim,Dongha Lee,Chanyoung Park*

Main category: cs.AI

TL;DR: TRACE框架通过结合证据库，对工具增强的LLM代理的推理轨迹进行多维度评估，解决了传统评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有工具增强基准测试的评估方法主要依赖于答案匹配，忽略了解决用户请求所需的步骤增加带来的效率、幻觉和适应性等问题。

Method: TRACE框架通过引入证据库，积累先前推理步骤中收集的知识，从而实现对代理推理轨迹的多方面分析和评估。此外，构建了一个新的元评估数据集，通过使用多方面的性能分数标记不同的和有缺陷的轨迹来增强现有的基准。

Result: TRACE框架能够以可扩展且经济高效的方式准确评估这些复杂的行为，即使使用小型开源LLM也是如此。该方法应用于评估代理在解决工具增强任务时产生的轨迹，提出了以前未报告的观察结果和相应的见解。

Conclusion: TRACE框架能够有效地评估工具增强的LLM代理的性能，解决了传统评估方法的局限性，并为未来的研究提供了新的方向。

Abstract: Although recent tool-augmented benchmarks incorporate complex user requests
and diverse tools, the evaluation methods for most of them remain limited to
answer matching. However, as the number of steps required to resolve a user
request increases, a proper evaluation of an agent's performance must go beyond
the final answer to also assess the problem-solving trajectory, including
previously ignored aspects such as efficiency, hallucination, and adaptivity.
The most straightforward method for evaluating these aspects is to compare an
agent's trajectory with the ground-truth trajectory, but this approach is
fundamentally limited since annotating all valid ground-truth trajectories is
prohibitively expensive. However, a simple LLM-based evaluator struggles to
assess trajectories in detail without ground truth. To effectively evaluate the
agents in this manner, we introduce TRACE, a framework for the
multi-dimensional evaluation of tool-augmented LLM agent performance. By
incorporating an evidence bank, which accumulates knowledge gathered from
preceding reasoning steps, TRACE enables a multi-faceted analysis and
evaluation of an agent's reasoning trajectory effectively. To validate our
framework, we develop a new meta-evaluation dataset by augmenting existing
benchmarks with diverse and flawed trajectories, each labeled with
multi-faceted performance scores. Our results confirm that TRACE accurately
evaluates these complex behaviors in a scalable and cost-effective manner, even
with small open-source LLMs. Furthermore, we apply our method to evaluate the
trajectories that agents produce while solving tool-augmented tasks, presenting
previously unreported observations and their corresponding insights.

</details>


### [128] [Take Goodhart Seriously: Principled Limit on General-Purpose AI Optimization](https://arxiv.org/abs/2510.02840)
*Antoine Maier,Aude Maier,Tom David*

Main category: cs.AI

TL;DR: 机器学习中一个常见的假设是，训练产生的模型实际上满足了其指定的目标函数。但实际上，由于近似、估计和优化误差，以及目标规范的错误，OSA在实际条件下会失效。


<details>
  <summary>Details</summary>
Motivation: 论证了在现实条件下，目标满足假设（OSA）不成立，并探讨了其影响。

Method: 在与学习范式无关的框架下进行论证，并结合了近期的数学结果。

Result: 即使目标规范的质量很高，也无法避免与预期目标的系统性偏差。在没有对这些差距进行数学表征的情况下，它们与在强大的优化压力下崩溃为古德哈特定律失效模式的差距无法区分。由于古德哈特失效点无法事先确定，因此有必要对通用人工智能系统的优化进行有原则的限制。

Conclusion: 需要对通用人工智能系统的优化进行限制，否则持续的优化可能会导致系统可预测且不可逆转的失控。

Abstract: A common but rarely examined assumption in machine learning is that training
yields models that actually satisfy their specified objective function. We call
this the Objective Satisfaction Assumption (OSA). Although deviations from OSA
are acknowledged, their implications are overlooked. We argue, in a
learning-paradigm-agnostic framework, that OSA fails in realistic conditions:
approximation, estimation, and optimization errors guarantee systematic
deviations from the intended objective, regardless of the quality of its
specification. Beyond these technical limitations, perfectly capturing and
translating the developer's intent, such as alignment with human preferences,
into a formal objective is practically impossible, making misspecification
inevitable. Building on recent mathematical results, absent a mathematical
characterization of these gaps, they are indistinguishable from those that
collapse into Goodhart's law failure modes under strong optimization pressure.
Because the Goodhart breaking point cannot be located ex ante, a principled
limit on the optimization of General-Purpose AI systems is necessary. Absent
such a limit, continued optimization is liable to push systems into predictable
and irreversible loss of control.

</details>


### [129] [Reward Model Routing in Alignment](https://arxiv.org/abs/2510.02850)
*Xinle Wu,Yao Lu*

Main category: cs.AI

TL;DR: 提出了一种新的RM路由框架BayesianRouter，它结合了离线RM强度学习和在线贝叶斯选择，以提高LLM的对齐质量并避免过拟合。


<details>
  <summary>Details</summary>
Motivation: 现有的RLHF/RLAIF流程依赖于单一奖励模型(RM)，这限制了对齐质量并有过度拟合的风险。现有的RM路由方法存在冷启动和探索不足的问题。

Method: 离线阶段：在偏好数据上训练一个多任务路由器，以评估每个RM的可靠性。在线阶段：贝叶斯Thompson抽样路由器执行每个查询的RM选择，用离线嵌入作为高斯先验初始化RM特定的权重向量，并使用在线奖励自适应地更新它们的后验，以适应不断变化的策略分布。

Result: 在指令跟随和推理基准测试中，BayesianRouter始终优于个体RM、RM集成和现有的路由方法。

Conclusion: BayesianRouter是一种有效的RM路由框架，可以提高LLM的对齐质量并避免过拟合。

Abstract: Reinforcement learning from human or AI feedback (RLHF / RLAIF) has become
the standard paradigm for aligning large language models (LLMs). However, most
pipelines rely on a single reward model (RM), limiting alignment quality and
risking overfitting. Recent work explores RM routing--dynamically selecting an
RM from a candidate pool to exploit complementary strengths while maintaining
$O(1)$ RM calls--but existing methods suffer from cold-start and insufficient
exploration. We propose BayesianRouter, a hybrid routing framework that
combines offline RM strengths learning with online Bayesian selection. In the
offline stage, a multi-task router is trained on preference data to estimate
per-RM reliability. In the online stage, a Bayesian Thompson sampling router
performs per-query RM selection, initializing RM-specific weight vectors with
offline embeddings as Gaussian priors and adaptively updating their posteriors
with online rewards to adapt to the evolving policy distribution. Extensive
experiments on instruction-following (AlpacaEval-2, Arena-Hard, MT-Bench) and
reasoning (GSM8K, MMLU) benchmarks show that BayesianRouter consistently
outperforms individual RMs, RM ensembling, and existing routing methods.

</details>


### [130] [Consolidating Reinforcement Learning for Multimodal Discrete Diffusion Models](https://arxiv.org/abs/2510.02880)
*Tianren Ma,Mu Zhang,Yibing Wang,Qixiang Ye*

Main category: cs.AI

TL;DR: MaskGRPO: A novel reinforcement learning method for optimizing discrete diffusion models (DDMs).


<details>
  <summary>Details</summary>
Motivation: Optimizing DDMs with rewards is challenging due to the non-autoregressive paradigm, making importance sampling intractable and rollout complex.

Method: Introduces MaskGRPO, which uses a theoretical foundation for DDMs to build an importance estimator and a tailored rollout method for visual sequences.

Result: MaskGRPO demonstrates more stable and efficient updates, leading to stronger reasoning performance and better generation quality on math reasoning, coding, and visual generation benchmarks.

Conclusion: MaskGRPO is a systematic policy optimization approach and the first practical way for discretized visual diffusion.

Abstract: Optimizing discrete diffusion model (DDM) with rewards remains a challenge:
the non-autoregressive paradigm makes importance sampling intractable and
rollout complex, puzzling reinforcement learning methods such as Group Relative
Policy Optimization (GRPO). In this study, we introduce MaskGRPO, the first
viable approach to enable scalable multimodal reinforcement learning in
discrete diffusion with effective importance sampling and modality-specific
adaptations. To this end, we first clarify the theoretical foundation for DDMs,
which facilitates building an importance estimator that captures valuable token
fluctuation for gradient updates. We then delicately tailored the rollout
method for visual sequences, which yields diverse completions and reliable
optimization gradients. Upon math reasoning, coding, and visual generation
benchmarks, MaskGRPO brings more stable and efficient updates, leading to
stronger reasoning performance and better generation quality. This study
establishes MaskGRPO as a systematic policy optimization approach and the first
practical way for discretized visual diffusion.

</details>


### [131] [Onto-Epistemological Analysis of AI Explanations](https://arxiv.org/abs/2510.02996)
*Martina Mattioli,Eike Petersen,Aasa Feragen,Marcello Pelillo,Siavash A. Bigdeli*

Main category: cs.AI

TL;DR: 这篇论文探讨了可解释人工智能（XAI）方法中本体论和认识论的假设，这些假设影响了解释的有效性和解释。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法缺乏解释性，限制了其可信度和应用。XAI旨在通过提供模型决策过程的解释来克服这一挑战。

Method: 通过研究XAI方法应用于AI系统时所做的本体论和认识论假设来进行分析。

Result: 分析表明，XAI方法的细微技术变化可能对应于关于解释的潜在假设的重要差异。

Conclusion: 强调了在为给定应用选择XAI方法时忽略潜在的本体论-认识论范式的风险，并讨论了如何为不同的应用领域选择和调整合适的XAI方法。

Abstract: Artificial intelligence (AI) is being applied in almost every field. At the
same time, the currently dominant deep learning methods are fundamentally
black-box systems that lack explanations for their inferences, significantly
limiting their trustworthiness and adoption. Explainable AI (XAI) methods aim
to overcome this challenge by providing explanations of the models' decision
process. Such methods are often proposed and developed by engineers and
scientists with a predominantly technical background and incorporate their
assumptions about the existence, validity, and explanatory utility of different
conceivable explanatory mechanisms. However, the basic concept of an
explanation -- what it is, whether we can know it, whether it is absolute or
relative -- is far from trivial and has been the subject of deep philosophical
debate for millennia. As we point out here, the assumptions incorporated into
different XAI methods are not harmless and have important consequences for the
validity and interpretation of AI explanations in different domains. We
investigate ontological and epistemological assumptions in explainability
methods when they are applied to AI systems, meaning the assumptions we make
about the existence of explanations and our ability to gain knowledge about
those explanations. Our analysis shows how seemingly small technical changes to
an XAI method may correspond to important differences in the underlying
assumptions about explanations. We furthermore highlight the risks of ignoring
the underlying onto-epistemological paradigm when choosing an XAI method for a
given application, and we discuss how to select and adapt appropriate XAI
methods for different domains of application.

</details>


### [132] [From Facts to Foils: Designing and Evaluating Counterfactual Explanations for Smart Environments](https://arxiv.org/abs/2510.03078)
*Anna Trapp,Mersedeh Sadeghi,Andreas Vogelsang*

Main category: cs.AI

TL;DR: 本文提出了第一个针对rule-based智能环境的反事实解释的形式化和实现方法。


<details>
  <summary>Details</summary>
Motivation: 在rule-based智能环境中，可解释性至关重要，而反事实解释是XAI中的强大工具，但目前尚无在此类领域中生成反事实解释的已建立方法。

Method: 该方法被实现为一个插件，扩展了现有智能环境的解释引擎。

Result: 用户研究表明，用户偏好具有高度的背景依赖性：因果解释因其语言简洁性和在时间压力大的情况下而受到青睐，而反事实解释因其可操作的内容而受到青睐，尤其是在用户想要解决问题时。

Conclusion: 本文贡献了一个在智能环境中进行新型解释的实用框架，并提供了经验证据来指导何时每种解释类型最有效。

Abstract: Explainability is increasingly seen as an essential feature of rule-based
smart environments. While counterfactual explanations, which describe what
could have been done differently to achieve a desired outcome, are a powerful
tool in eXplainable AI (XAI), no established methods exist for generating them
in these rule-based domains. In this paper, we present the first formalization
and implementation of counterfactual explanations tailored to this domain. It
is implemented as a plugin that extends an existing explanation engine for
smart environments. We conducted a user study (N=17) to evaluate our generated
counterfactuals against traditional causal explanations. The results show that
user preference is highly contextual: causal explanations are favored for their
linguistic simplicity and in time-pressured situations, while counterfactuals
are preferred for their actionable content, particularly when a user wants to
resolve a problem. Our work contributes a practical framework for a new type of
explanation in smart environments and provides empirical evidence to guide the
choice of when each explanation type is most effective.

</details>


### [133] [A Study of Rule Omission in Raven's Progressive Matrices](https://arxiv.org/abs/2510.03127)
*Binze Li*

Main category: cs.AI

TL;DR: 本研究探讨了人工智能系统在不完全训练下解决Raven's Progressive Matrices (RPM)问题的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 评估人工智能是否真正具有推理能力，还是依赖于统计捷径。

Method: 通过在训练期间故意省略若干结构规则，并在Impartial-RAVEN (I-RAVEN)数据集上评估了序列到序列转换模型以及基于视觉的架构（如CoPINet和Dual-Contrast Network）。

Result: Transformer模型在熟悉的规则上表现出色，但在面对新的或省略的规则时，其准确性急剧下降。Token级别准确性和完整答案准确性之间的差距凸显了当前方法的根本局限性。

Conclusion: 研究结果表明，深度学习模型需要超越模式识别，转向强大的抽象推理。

Abstract: Analogical reasoning lies at the core of human cognition and remains a
fundamental challenge for artificial intelligence. Raven's Progressive Matrices
(RPM) serve as a widely used benchmark to assess abstract reasoning by
requiring the inference of underlying structural rules. While many vision-based
and language-based models have achieved success on RPM tasks, it remains
unclear whether their performance reflects genuine reasoning ability or
reliance on statistical shortcuts. This study investigates the generalization
capacity of modern AI systems under conditions of incomplete training by
deliberately omitting several structural rules during training. Both
sequence-to-sequence transformer models and vision-based architectures such as
CoPINet and the Dual-Contrast Network are evaluated on the Impartial-RAVEN
(I-RAVEN) dataset. Experiments reveal that although transformers demonstrate
strong performance on familiar rules, their accuracy declines sharply when
faced with novel or omitted rules. Moreover, the gap between token-level
accuracy and complete answer accuracy highlights fundamental limitations in
current approaches. These findings provide new insights into the reasoning
mechanisms underlying deep learning models and underscore the need for
architectures that move beyond pattern recognition toward robust abstract
reasoning.

</details>


### [134] [Improving Cooperation in Collaborative Embodied AI](https://arxiv.org/abs/2510.03153)
*Hima Jacob Leven Suprabha,Laxmi Nag Laxminarayan Nagesh,Ajith Nair,Alvin Reuben Amal Selvaster,Ayan Khan,Raghuram Damarla,Sanju Hannah Samuel,Sreenithi Saravana Perumal,Titouan Puech,Venkataramireddy Marella,Vishal Sonar,Alessandro Suglia,Oliver Lemon*

Main category: cs.AI

TL;DR: 研究大型语言模型（LLM）在多智能体系统中的应用，通过优化prompt来提升智能体的协作行为和决策能力。


<details>
  <summary>Details</summary>
Motivation: 探索LLM集成到多智能体系统中以实现协同推理和合作的可能性。

Method: 通过系统实验，评估不同的prompt方法在增强智能体协作行为和决策方面的有效性。使用CoELA框架，并结合语音功能。

Result: prompt优化能有效提升协作智能体的性能。最佳组合使Gemma3系统的效率提高了22%。

Conclusion: prompt优化可以有效提高协作智能体的性能，语音集成提供了一个更具吸引力的用户界面，方便迭代系统开发和演示。

Abstract: The integration of Large Language Models (LLMs) into multiagent systems has
opened new possibilities for collaborative reasoning and cooperation with AI
agents. This paper explores different prompting methods and evaluates their
effectiveness in enhancing agent collaborative behaviour and decision-making.
We enhance CoELA, a framework designed for building Collaborative Embodied
Agents that leverage LLMs for multi-agent communication, reasoning, and task
coordination in shared virtual spaces. Through systematic experimentation, we
examine different LLMs and prompt engineering strategies to identify optimised
combinations that maximise collaboration performance. Furthermore, we extend
our research by integrating speech capabilities, enabling seamless
collaborative voice-based interactions. Our findings highlight the
effectiveness of prompt optimisation in enhancing collaborative agent
performance; for example, our best combination improved the efficiency of the
system running with Gemma3 by 22% compared to the original CoELA system. In
addition, the speech integration provides a more engaging user interface for
iterative system development and demonstrations.

</details>


### [135] [CoDA: Agentic Systems for Collaborative Data Visualization](https://arxiv.org/abs/2510.03194)
*Zichen Chen,Jiefeng Chen,Sercan Ö. Arik,Misha Sra,Tomas Pfister,Jinsung Yoon*

Main category: cs.AI

TL;DR: 本文提出了一个用于数据可视化的多智能体系统CoDA，该系统通过元数据分析、任务规划、代码生成和自我反思等步骤，实现了更强大的自动化。


<details>
  <summary>Details</summary>
Motivation: 当前系统在处理包含多个文件和迭代改进的复杂数据集时存在困难，并且无法有效地管理数据复杂性、代码错误或最终可视化质量。

Method: 本文将挑战重新定义为协作多智能体问题，并引入了CoDA，一个使用专门的LLM智能体进行元数据分析、任务规划、代码生成和自我反思的多智能体系统。

Result: 实验结果表明，CoDA在整体评分方面取得了显著提升，优于竞争基线高达41.5%。

Conclusion: 这项工作表明，可视化自动化的未来不在于孤立的代码生成，而在于集成、协作的智能体工作流程。

Abstract: Deep research has revolutionized data analysis, yet data scientists still
devote substantial time to manually crafting visualizations, highlighting the
need for robust automation from natural language queries. However, current
systems struggle with complex datasets containing multiple files and iterative
refinement. Existing approaches, including simple single- or multi-agent
systems, often oversimplify the task, focusing on initial query parsing while
failing to robustly manage data complexity, code errors, or final visualization
quality. In this paper, we reframe this challenge as a collaborative
multi-agent problem. We introduce CoDA, a multi-agent system that employs
specialized LLM agents for metadata analysis, task planning, code generation,
and self-reflection. We formalize this pipeline, demonstrating how
metadata-focused analysis bypasses token limits and quality-driven refinement
ensures robustness. Extensive evaluations show CoDA achieves substantial gains
in the overall score, outperforming competitive baselines by up to 41.5%. This
work demonstrates that the future of visualization automation lies not in
isolated code generation but in integrated, collaborative agentic workflows.

</details>


### [136] [Coevolutionary Continuous Discrete Diffusion: Make Your Diffusion Language Model a Latent Reasoner](https://arxiv.org/abs/2510.03206)
*Cai Zhou,Chenxiao Yang,Yi Hu,Chenyu Wang,Chubin Zhang,Muhan Zhang,Lester Mackey,Tommi Jaakkola,Stephen Bates,Dinghuai Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种名为CCDD的联合多模态扩散过程，它在连续表示空间和离散token空间的联合上运行，以结合两者的优点。


<details>
  <summary>Details</summary>
Motivation: 连续扩散模型在理论上比离散扩散模型和循环Transformer更具表达力，但实际上表现不佳。本文旨在解决这一矛盾。

Method: 本文提出了CCDD，它利用单个模型在联合空间中同时去噪，并提出了有效的架构和训练/采样技术。

Result: 在真实世界的任务中，CCDD在广泛的语言建模实验中表现出强大的经验性能。

Conclusion: CCDD结合了连续表示空间和离散token空间的优点，既具有丰富的语义信息，又具有良好的可训练性和样本质量。

Abstract: Diffusion language models, especially masked discrete diffusion models, have
achieved great success recently. While there are some theoretical and primary
empirical results showing the advantages of latent reasoning with looped
transformers or continuous chain-of-thoughts, continuous diffusion models
typically underperform their discrete counterparts. In this paper, we argue
that diffusion language models do not necessarily need to be in the discrete
space. In particular, we prove that continuous diffusion models have stronger
expressivity than discrete diffusions and looped transformers. We attribute the
contradiction between the theoretical expressiveness and empirical performance
to their practical trainability: while continuous diffusion provides
intermediate supervision that looped transformers lack, they introduce
additional difficulty decoding tokens into the discrete token space from the
continuous representation space. We therefore propose Coevolutionary Continuous
Discrete Diffusion (CCDD), which defines a joint multimodal diffusion process
on the union of a continuous representation space and a discrete token space,
leveraging a single model to simultaneously denoise in the joint space. By
combining two modalities, CCDD is expressive with rich semantics in the latent
space, as well as good trainability and sample quality with the help of
explicit discrete tokens. We also propose effective architectures and advanced
training/sampling techniques for CCDD, which reveals strong empirical
performance in extensive language modeling experiments on real-world tasks.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [137] [A New Normalization Form for Limited Distinct Attributes](https://arxiv.org/abs/2510.02865)
*Niko S. Snell,Rayen C. Lee*

Main category: cs.DB

TL;DR: 提出了一种新的数据库范式，用于约束具有有限值的属性。


<details>
  <summary>Details</summary>
Motivation: 当前的数据规范化方法没有限制属性值的数量，这可能导致数据异常和查询不准确。

Method: 提出一种新的数据库规范化方法，即有限区分范式 (LDNF)，它通过强制属性符合有限数量的值，将非有限区分属性转换为有限区分属性。

Result: LDNF 与现有的范式一起使用，可以满足当前方法无法满足的规范化需求。

Conclusion: 因此，提出了一种 LDNF 的正式方法。

Abstract: In modern databases, the practice of data normalization continues to be
important in improving data integrity, minimizing redundancies, and eliminating
anomalies. However, since its inception and consequent improvements, there have
been no attempts to document a method which constrains the values of attributes
capable of only possessing a limited quantity of values. These non-limited
distinct attributes pose a problem throughout many relational databases as they
have the potential to cause data anomalies and query inaccuracies. Thus, a new
database normalization method, Limited Distinct Normal Form (LDNF), is
necessary in order to improve upon the currently established data normalization
process. In brief, LDNF is a method which turns non-limited distinct attributes
into limited distinct attributes by forcing the attributes to conform to a
limited quantity of values. Utilizing LDNF in tandem with existing normal forms
fulfills a need in normalization that is otherwise not present when only using
current methods. A formal approach to LDNF is therefore proposed.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [138] [OpenZL: A Graph-Based Model for Compression](https://arxiv.org/abs/2510.03203)
*Yann Collet,Nick Terrell,W. Felix Handte,Danielle Rozenblit,Victor Zhang,Kevin Zhang,Yaelle Goldschlag,Jennifer Lee,Daniel Riegel,Stan Angelov,Nadav Rotem*

Main category: cs.IR

TL;DR: 提出了一种新的压缩方法，在压缩率和速度上都优于现有的通用压缩器。


<details>
  <summary>Details</summary>
Motivation: 通用无损压缩在压缩率上的改进以资源利用率和处理吞吐量为代价，而实际生产需要高吞吐量和低资源利用率。应用特定的压缩器虽然性能好，但适用性有限，难以维护和部署。

Method: 提出“图模型”压缩，这是一种将压缩表示为模块化编解码器的有向无环图的新理论框架。实现了 OpenZL，它可以将数据压缩为自描述的线格式，其任何配置都可以通过通用解码器解压缩。

Result: OpenZL 在各种真实世界数据集上实现了优于最先进的通用压缩器的压缩率和速度。在 Meta 的内部部署也显示出大小和/或速度的持续改进，开发时间从几个月减少到几天。

Conclusion: OpenZL 代表了现代数据密集型应用中实用、可扩展和可维护的数据压缩的进步。

Abstract: Research in general-purpose lossless compression over the last decade has
largely found improvements in compression ratio that come at great cost to
resource utilization and processing throughput. However, most production
workloads require high throughput and low resource utilization, so most
research systems have seen little adoption. Instead, real world improvements in
compression are increasingly often realized by building application-specific
compressors which can exploit knowledge about the structure and semantics of
the data being compressed. These systems easily outperform even the best
generic compressors, but application-specific compression schemes are not
without drawbacks. They are inherently limited in applicability and are
difficult to maintain and deploy.
  We show that these challenges can be overcome with a new way of thinking
about compression. We propose the ``graph model'' of compression, a new
theoretical framework for representing compression as a directed acyclic graph
of modular codecs. This motivates OpenZL, an implementation of this model that
compresses data into a self-describing wire format, any configuration of which
can be decompressed by a universal decoder. OpenZL's design enables rapid
development of tailored compressors with minimal code, its universal decoder
eliminates deployment lag, and its investment in a well-vetted standard
component library minimizes security risks. Experimental results demonstrate
that OpenZL achieves superior compression ratios and speeds compared to
state-of-the-art general-purpose compressors on a variety of real-world
datasets. Internal deployments at Meta have also shown consistent improvements
in size and/or speed, with development timelines reduced from months to days.
OpenZL thus represents an advance in practical, scalable, and maintainable data
compression for modern data-intensive applications.

</details>


### [139] [Revisiting Query Variants: The Advantage of Retrieval Over Generation of Query Variants for Effective QPP](https://arxiv.org/abs/2510.02512)
*Fangzheng Tian,Debasis Ganguly,Craig Macdonald*

Main category: cs.IR

TL;DR: 本文提出了一种新的基于查询变体（QV）的查询性能预测（QPP）方法，该方法从训练集中检索与目标查询具有相似信息需求的QV，并通过两跳检索扩展QV，以提高检索召回率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于QV的QPP方法存在主题漂移和幻觉问题。

Method: 该方法从训练集中检索QV，并通过使用相关文档进行第二轮检索来扩展直接检索到的QV（1跳QV），从而产生2跳QV。

Result: 在TREC DL'19和DL'20上的实验表明，使用该方法检索到的QV的QPP方法优于现有最佳的基于生成QV的QPP方法约20%。

Conclusion: 该方法能够有效提高QPP的性能。

Abstract: Leveraging query variants (QVs), i.e., queries with potentially similar
information needs to the target query, has been shown to improve the
effectiveness of query performance prediction (QPP) approaches. Existing
QV-based QPP methods generate QVs facilitated by either query expansion or
non-contextual embeddings, which may introduce topical drifts and
hallucinations. In this paper, we propose a method that retrieves QVs from a
training set (e.g., MS MARCO) for a given target query of QPP. To achieve a
high recall in retrieving queries with the most similar information needs as
the target query from a training set, we extend the directly retrieved QVs
(1-hop QVs) by a second retrieval using their denoted relevant documents (which
yields 2-hop QVs). Our experiments, conducted on TREC DL'19 and DL'20, show
that the QPP methods with QVs retrieved by our method outperform the
best-performing existing generated-QV-based QPP approaches by as much as around
20\%, on neural ranking models like MonoT5.

</details>


### [140] [A Simple but Effective Elaborative Query Reformulation Approach for Natural Language Recommendation](https://arxiv.org/abs/2510.02656)
*Qianfeng Wen,Yifan Liu,Justin Cui,Joshua Zhang,Anton Korikov,George-Kirollos Saad,Scott Sanner*

Main category: cs.IR

TL;DR: 提出了一种新的查询重构方法EQR，该方法结合了广度和深度，通过生成具有信息丰富的细化的潜在查询子主题来改进NL推荐系统。


<details>
  <summary>Details</summary>
Motivation: 现有的NL推荐系统难以解释表达广泛或间接用户意图的具有挑战性的查询。现有的QR方法倾向于只关注扩大查询子主题的范围（广度）或详细阐述查询的潜在含义（深度），但不能同时关注两者。

Method: 提出了一种基于大型语言模型的QR方法EQR（Elaborative Subtopic Query Reformulation），该方法结合了广度和深度，通过生成具有信息丰富的细化的潜在查询子主题。

Result: 实验表明，EQR在各种评估指标上都大大优于最先进的QR方法。

Conclusion: 一个简单而有效的QR方法可以显著改善具有广泛和间接用户意图的查询的NL推荐系统。

Abstract: Natural Language (NL) recommender systems aim to retrieve relevant items from
free-form user queries and item descriptions. Existing systems often rely on
dense retrieval (DR), which struggles to interpret challenging queries that
express broad (e.g., "cities for youth friendly activities") or indirect (e.g.,
"cities for a high school graduation trip") user intents. While query
reformulation (QR) has been widely adopted to improve such systems, existing QR
methods tend to focus only on expanding the range of query subtopics (breadth)
or elaborating on the potential meaning of a query (depth), but not both. In
this paper, we propose EQR (Elaborative Subtopic Query Reformulation), a large
language model-based QR method that combines both breadth and depth by
generating potential query subtopics with information-rich elaborations. We
also introduce three new natural language recommendation benchmarks in travel,
hotel, and restaurant domains to establish evaluation of NL recommendation with
challenging queries. Experiments show EQR substantially outperforms
state-of-the-art QR methods in various evaluation metrics, highlighting that a
simple yet effective QR approach can significantly improve NL recommender
systems for queries with broad and indirect user intents.

</details>


### [141] [Less LLM, More Documents: Searching for Improved RAG](https://arxiv.org/abs/2510.02657)
*Jingjie Ning,Yibo Kong,Yunfan Long,Jamie Callan*

Main category: cs.IR

TL;DR: 扩展检索语料库可以在检索增强生成（RAG）中替代扩大语言模型（LLM），从而在成本和部署方面更具优势。


<details>
  <summary>Details</summary>
Motivation: 探索在不依赖大型LLM的情况下，通过扩大检索语料库来提升RAG性能。

Method: 通过实验评估不同大小的语料库和LLM在RAG中的效果。

Result: 扩大语料库通常可以替代增大模型尺寸，尤其对于中等大小的模型增益最大。改善主要来自答案段落覆盖率的增加。

Conclusion: 投资于更大的语料库是增强RAG的有效途径，其效果通常与扩大LLM本身相当。

Abstract: Retrieval-Augmented Generation (RAG) couples document retrieval with large
language models (LLMs). While scaling generators improves accuracy, it also
raises cost and limits deployability. We explore an orthogonal axis: enlarging
the retriever's corpus to reduce reliance on large LLMs. Experimental results
show that corpus scaling consistently strengthens RAG and can often serve as a
substitute for increasing model size, though with diminishing returns at larger
scales. Small- and mid-sized generators paired with larger corpora often rival
much larger models with smaller corpora; mid-sized models tend to gain the
most, while tiny and large models benefit less. Our analysis shows that
improvements arise primarily from increased coverage of answer-bearing
passages, while utilization efficiency remains largely unchanged. These
findings establish a principled corpus-generator trade-off: investing in larger
corpora offers an effective path to stronger RAG, often comparable to enlarging
the LLM itself.

</details>


### [142] [AgenticRAG: Tool-Augmented Foundation Models for Zero-Shot Explainable Recommender Systems](https://arxiv.org/abs/2510.02668)
*Bo Ma,Hang Li,ZeHua Hu,XiaoFan Gui,LuYao Liu,Simon Liu*

Main category: cs.IR

TL;DR: AgenticRAG: Combines tool-augmented foundation models with retrieval-augmented generation for explainable recommendations.


<details>
  <summary>Details</summary>
Motivation: Foundation models in recommender systems are limited by reasoning opacity and knowledge constraints.

Method: Integrates external tool invocation, knowledge retrieval, and chain-of-thought reasoning to create autonomous recommendation agents.

Result: Achieves consistent improvements over state-of-the-art baselines on three real-world datasets.

Conclusion: Exhibits superior explainability and maintains computational efficiency.

Abstract: Foundation models have revolutionized artificial intelligence, yet their
application in recommender systems remains limited by reasoning opacity and
knowledge constraints. This paper introduces AgenticRAG, a novel framework that
combines tool-augmented foundation models with retrieval-augmented generation
for zero-shot explainable recommendations. Our approach integrates external
tool invocation, knowledge retrieval, and chain-of-thought reasoning to create
autonomous recommendation agents capable of transparent decision-making without
task-specific training. Experimental results on three real-world datasets
demonstrate that AgenticRAG achieves consistent improvements over
state-of-the-art baselines, with NDCG@10 improvements of 0.4\% on Amazon
Electronics, 0.8\% on MovieLens-1M, and 1.6\% on Yelp datasets. The framework
exhibits superior explainability while maintaining computational efficiency
comparable to traditional methods.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [143] [Extreme value forecasting using relevance-based data augmentation with deep learning models](https://arxiv.org/abs/2510.02407)
*Junru Hua,Rahul Ahluwalia,Rohitash Chandra*

Main category: cs.LG

TL;DR: 本研究提出了一种用于极值预测的数据增强框架，重点是结合深度学习模型（如Conv-LSTM和BD-LSTM）以及数据增强模型（如GAN和SMOTE）来预测极值。


<details>
  <summary>Details</summary>
Motivation: 极值预测在金融和气候变化等领域具有挑战性和广泛的应用。

Method: 使用GAN和SMOTE等数据增强模型，结合Conv-LSTM和BD-LSTM等深度学习模型，进行多步超前极值预测。同时，还提出了基于相关性函数的数据增强策略。

Result: SMOTE策略表现出更好的适应性，提高了短期和长期预测的性能。Conv-LSTM在周期性稳定数据集中表现出色，而BD-LSTM在混沌或非平稳序列中表现更好。

Conclusion: SMOTE是极值预测的有效数据增强方法，Conv-LSTM和BD-LSTM具有互补优势，适用于不同类型的数据集。

Abstract: Data augmentation with generative adversarial networks (GANs) has been
popular for class imbalance problems, mainly for pattern classification and
computer vision-related applications. Extreme value forecasting is a
challenging field that has various applications from finance to climate change
problems. In this study, we present a data augmentation framework for extreme
value forecasting. In this framework, our focus is on forecasting extreme
values using deep learning models in combination with data augmentation models
such as GANs and synthetic minority oversampling technique (SMOTE). We use deep
learning models such as convolutional long short-term memory (Conv-LSTM) and
bidirectional long short-term memory (BD-LSTM) networks for multistep ahead
prediction featuring extremes. We investigate which data augmentation models
are the most suitable, taking into account the prediction accuracy overall and
at extreme regions, along with computational efficiency. We also present novel
strategies for incorporating data augmentation, considering extreme values
based on a relevance function. Our results indicate that the SMOTE-based
strategy consistently demonstrated superior adaptability, leading to improved
performance across both short- and long-horizon forecasts. Conv-LSTM and
BD-LSTM exhibit complementary strengths: the former excels in periodic, stable
datasets, while the latter performs better in chaotic or non-stationary
sequences.

</details>


### [144] [OpenTSLM: Time-Series Language Models for Reasoning over Multivariate Medical Text- and Time-Series Data](https://arxiv.org/abs/2510.02410)
*Patrick Langer,Thomas Kaar,Max Rosenblattl,Maxwell A. Xu,Winnie Chow,Martin Maritsch,Aradhana Verma,Brian Han,Daniel Seung Kim,Henry Chubb,Scott Ceresnak,Aydin Zahedivash,Alexander Tarlochan Singh Sandhu,Fatima Rodriguez,Daniel McDuff,Elgar Fleisch,Oliver Aalami,Filipe Barata,Paul Schmiedmayer*

Main category: cs.LG

TL;DR: OpenTSLM: Time Series Language Models克服了LLMs无法处理时间序列的局限性，通过将时间序列作为原生模态集成到预训练的LLMs中，实现对任意长度的多个时间序列进行推理。


<details>
  <summary>Details</summary>
Motivation: 现有的LLMs在医学领域应用潜力巨大，但无法处理时间序列数据，限制了其应用。

Method: 提出了两种OpenTSLM架构：OpenTSLM-SoftPrompt（通过软提示将可学习的时间序列tokens与文本tokens连接）和OpenTSLM-Flamingo（通过交叉注意力将时间序列与文本集成）。

Result: OpenTSLM模型在多个文本-时间序列CoT推理任务中优于基线模型，在睡眠分期中达到69.9 F1，在HAR中达到65.4，即使是1B参数的OpenTSLM模型也超过了GPT-4o。OpenTSLM-Flamingo在性能上与OpenTSLM-SoftPrompt相当，并在更长的序列上表现更好，同时保持稳定的内存需求。

Conclusion: OpenTSLM模型展示了强大的推理能力，所有代码、数据集和模型均开源，以促进进一步研究。

Abstract: LLMs have emerged as powerful tools for interpreting multimodal data. In
medicine, they hold particular promise for synthesizing large volumes of
clinical information into actionable insights and digital health applications.
Yet, a major limitation remains their inability to handle time series. To
overcome this gap, we present OpenTSLM, a family of Time Series Language Models
(TSLMs) created by integrating time series as a native modality to pretrained
LLMs, enabling reasoning over multiple time series of any length. We
investigate two architectures for OpenTSLM. The first, OpenTSLM-SoftPrompt,
models time series implicitly by concatenating learnable time series tokens
with text tokens via soft prompting. Although parameter-efficient, we
hypothesize that explicit time series modeling scales better and outperforms
implicit approaches. We thus introduce OpenTSLM-Flamingo, which integrates time
series with text via cross-attention. We benchmark both variants against
baselines that treat time series as text tokens or plots, across a suite of
text-time-series Chain-of-Thought (CoT) reasoning tasks. We introduce three
datasets: HAR-CoT, Sleep-CoT, and ECG-QA-CoT. Across all, OpenTSLM models
outperform baselines, reaching 69.9 F1 in sleep staging and 65.4 in HAR,
compared to 9.05 and 52.2 for finetuned text-only models. Notably, even
1B-parameter OpenTSLM models surpass GPT-4o (15.47 and 2.95). OpenTSLM-Flamingo
matches OpenTSLM-SoftPrompt in performance and outperforms on longer sequences,
while maintaining stable memory requirements. By contrast, SoftPrompt grows
exponentially in memory with sequence length, requiring around 110 GB compared
to 40 GB VRAM when training on ECG-QA with LLaMA-3B. Expert reviews by
clinicians find strong reasoning capabilities exhibited by OpenTSLMs on ECG-QA.
To facilitate further research, we provide all code, datasets, and models
open-source.

</details>


### [145] [RainSeer: Fine-Grained Rainfall Reconstruction via Physics-Guided Modeling](https://arxiv.org/abs/2510.02414)
*Lin Chen,Jun Chen,Minghui Qiu,Shuxin Zhong,Binghong Chen,Kaishun Wu*

Main category: cs.LG

TL;DR: RainSeer是一个结构感知的降雨重建框架，它利用雷达反射率作为物理基础的结构先验，来解决现有空间插值方法过度平滑的问题。


<details>
  <summary>Details</summary>
Motivation: 现有空间插值方法在重建高分辨率降雨场时，无法捕捉到尖锐的转变和局部极端情况，RainSeer旨在解决这个问题。

Method: RainSeer通过一个物理信息驱动的两阶段架构来实现：结构到点映射器（Structure-to-Point Mapper）和地理感知降雨解码器（Geo-Aware Rain Decoder）。

Result: 在两个公共数据集RAIN-F和MeteoNet上的评估表明，RainSeer优于现有技术，MAE降低超过13.31%，并显著提高了重建降雨场的结构保真度。

Conclusion: RainSeer能够有效地重建高分辨率降雨场，并在结构保真度方面优于现有方法。

Abstract: Reconstructing high-resolution rainfall fields is essential for flood
forecasting, hydrological modeling, and climate analysis. However, existing
spatial interpolation methods-whether based on automatic weather station (AWS)
measurements or enhanced with satellite/radar observations often over-smooth
critical structures, failing to capture sharp transitions and localized
extremes. We introduce RainSeer, a structure-aware reconstruction framework
that reinterprets radar reflectivity as a physically grounded structural
prior-capturing when, where, and how rain develops. This shift, however,
introduces two fundamental challenges: (i) translating high-resolution
volumetric radar fields into sparse point-wise rainfall observations, and (ii)
bridging the physical disconnect between aloft hydro-meteors and ground-level
precipitation. RainSeer addresses these through a physics-informed two-stage
architecture: a Structure-to-Point Mapper performs spatial alignment by
projecting mesoscale radar structures into localized ground-level rainfall,
through a bidirectional mapping, and a Geo-Aware Rain Decoder captures the
semantic transformation of hydro-meteors through descent, melting, and
evaporation via a causal spatiotemporal attention mechanism. We evaluate
RainSeer on two public datasets-RAIN-F (Korea, 2017-2019) and MeteoNet (France,
2016-2018)-and observe consistent improvements over state-of-the-art baselines,
reducing MAE by over 13.31% and significantly enhancing structural fidelity in
reconstructed rainfall fields.

</details>


### [146] [How to Train Your Advisor: Steering Black-Box LLMs with Advisor Models](https://arxiv.org/abs/2510.02453)
*Parth Asawa,Alan Zhu,Matei Zaharia,Alexandros G. Dimakis,Joseph E. Gonzalez*

Main category: cs.LG

TL;DR: 提出了Advisor Models，通过强化学习训练轻量级参数策略，针对黑盒模型动态生成自然语言指导指令，以适应不同的输入、用户或环境。


<details>
  <summary>Details</summary>
Motivation: 现有的静态prompt优化方法无法适应不同的输入、用户或环境。

Method: 使用强化学习训练轻量级参数策略（Advisor Models），使其能够根据环境反馈动态生成prompt。

Result: Advisor Models在多个领域优于静态prompt优化器，能够发现环境动态并提高下游任务性能。同时，Advisor Models具有良好的泛化能力，可以跨黑盒模型迁移，并在保持鲁棒性的前提下实现专业化。

Conclusion: 通过Advisor Models对黑盒模型进行动态优化，是实现个性化和环境适应性AI的一个有希望的方向。

Abstract: Foundation models are increasingly deployed as black-box services, where
model weights cannot be modified and customization is limited to prompting.
While static prompt optimization has shown promise, it produces a single fixed
prompt that fails to adapt to different inputs, users, or environments. We
introduce Advisor Models, lightweight parametric policies trained with
reinforcement learning to reactively issue natural language steering
instructions in-context to black-box models. The advisor is a second small
model that sits between the input and the model, shaping behavior on a
per-instance basis using reward signals from the environment. Across multiple
domains involving reasoning and personalization, we show that Advisor Models
outperform static prompt optimizers, discovering environment dynamics and
improving downstream task performance. We also demonstrate the generalizability
of advisors by transferring them across black-box models, as well as the
framework's ability to achieve specialization while retaining robustness to
out-of-distribution inputs. Viewed more broadly, Advisor Models provide a
learnable interface to black-box systems where the advisor acts as a
parametric, environment-specific memory. We argue that dynamic optimization of
black-box models via Advisor Models is a promising direction for enabling
personalization and environment-adaptable AI with frontier-level capabilities.

</details>


### [147] [Market-Based Data Subset Selection -- Principled Aggregation of Multi-Criteria Example Utility](https://arxiv.org/abs/2510.02456)
*Ashish Jha,Valentin Leplat,AH Phan*

Main category: cs.LG

TL;DR: 提出了一个基于市场的选择器，该选择器通过成本函数预测市场（LMSR）为每个示例定价。


<details>
  <summary>Details</summary>
Motivation: 选择一个小的但有用的训练数据子集是困难的，因为示例效用信号是异构的，通常与临时权重相结合。

Method: 使用基于市场的选择器，其中信号充当交易者，单个流动性参数控制集中度，并且按主题进行归一化以稳定校准。通过价格/令牌规则显式处理令牌预算，并使用轻量级多样性头来提高覆盖率。

Result: 在GSM8K（60k令牌预算）上，具有多样性的市场实现了与强大的单信号基线相同的性能，同时减少了种子差异并产生了<0.1 GPU-hr的选择开销；在AGNews上，保持=5-25％的市场（具有轻微平衡）提供了具有竞争力的准确性，并具有改进的平衡性和稳定性。

Conclusion: 该框架统一了固定计算下的多信号数据管理，用于提示级别推理和分类。

Abstract: Selecting a small yet useful subset of training data is hard because signals
of example utility (uncertainty, rarity, diversity, etc.) are heterogeneous and
typically combined with ad hoc weights. We propose a market-based selector that
prices each example via a cost-function prediction market (LMSR), signals act
as traders, a single liquidity parameter controls concentration, and topic-wise
normalization stabilizes calibration. Token budgets are handled explicitly by a
price-per-token rule $\rho=p/\ell^{\gamma}$, with $\gamma$ exposing an
interpretable length bias; a lightweight diversity head improves coverage. We
quantify coverage via topic cluster coverage and effective sample size. On the
theory side, we show that LMSR implements a maximum-entropy aggregation with
exponential weighting and a convex objective, yielding transparent knobs for
aggregation strength. Empirically, on GSM8K (60k-token budget) the market with
diversity achieves parity with strong single-signal baselines while reducing
seed variance and incurring $<\!0.1$ GPU-hr selection overhead; on AGNews at
kept=5-25\% the market (with light balancing) delivers competitive accuracy
with improved balance and stability. The framework unifies multi-signal data
curation under fixed compute for prompt-level reasoning and classification.

</details>


### [148] [Assessing the Potential for Catastrophic Failure in Dynamic Post-Training Quantization](https://arxiv.org/abs/2510.02457)
*Logan Frank,Paul Ardis*

Main category: cs.LG

TL;DR: 本文研究了后训练量化 (PTQ) 导致的神经网络性能下降问题，特别关注安全关键环境下的极端失效情况。


<details>
  <summary>Details</summary>
Motivation: 在安全关键环境中部署量化模型时，需要调查潜在的性能下降程度以及导致这种下降的输入分布特征。

Method: 本文构建了一个知识蒸馏和强化学习任务，用于学习网络和位宽策略对，从而分析量化下的最坏情况潜在灾难性故障。

Result: 结果表明，确实存在“有害”的网络-策略对，与“稳健”的对应模型相比，准确率下降了 10-65%。

Conclusion: 本文强调在实际部署场景中需要谨慎，并鼓励对鲁棒性进行更严格的检查，以及在深度学习领域未来工作中更加重视安全考量。

Abstract: Post-training quantization (PTQ) has recently emerged as an effective tool
for reducing the computational complexity and memory usage of a neural network
by representing its weights and activations with lower precision. While this
paradigm has shown great success in lowering compute and storage costs, there
is the potential for drastic performance reduction depending upon the
distribution of inputs experienced in inference. When considering possible
deployment in safety-critical environments, it is important to investigate the
extent of potential performance reduction, and what characteristics of input
distributions may give rise to this reduction. In this work, we explore the
idea of extreme failure stemming from dynamic PTQ and formulate a knowledge
distillation and reinforcement learning task to learn a network and bit-width
policy pair such that catastrophic failure under quantization is analyzed in
terms of worst case potential. Our results confirm the existence of this
"detrimental" network-policy pair, with several instances demonstrating
performance reductions in the range of 10-65% in accuracy, compared to their
"robust" counterparts encountering a <2% decrease. From systematic
experimentation and analyses, we also provide an initial exploration into
points at highest vulnerability. While our results represent an initial step
toward understanding failure cases introduced by PTQ, our findings ultimately
emphasize the need for caution in real-world deployment scenarios. We hope this
work encourages more rigorous examinations of robustness and a greater emphasis
on safety considerations for future works within the broader field of deep
learning.

</details>


### [149] [SAGE: Streaming Agreement-Driven Gradient Sketches for Representative Subset Selection](https://arxiv.org/abs/2510.02470)
*Ashish Jha,Salman Ahmadi-Asl*

Main category: cs.LG

TL;DR: SAGE是一种流式数据子集选择方法，它使用紧凑的频繁方向（FD）草图来保持梯度几何结构，并优先选择梯度与一致方向对齐的样本。


<details>
  <summary>Details</summary>
Motivation: 在大型数据集上训练现代神经网络需要大量的计算和能源。

Method: SAGE维护梯度几何结构的紧凑FD草图，并优先选择草图梯度与一致方向对齐的样本。该方法消除了成对相似性和显式梯度存储，从而产生了一个简单的双通道、gpu友好的管道。

Result: SAGE在多个基准测试中，以较小的保持率预算进行训练，同时保持相对于全数据训练和最近子集选择基线的竞争精度，并减少端到端计算和峰值内存。

Conclusion: SAGE提供了一种实用的、恒定内存的替代方案，可以补充剪枝和模型压缩，以实现高效训练。

Abstract: Training modern neural networks on large datasets is computationally and
energy intensive. We present SAGE, a streaming data-subset selection method
that maintains a compact Frequent Directions (FD) sketch of gradient geometry
in $O(\ell D)$ memory and prioritizes examples whose sketched gradients align
with a consensus direction. The approach eliminates $N \times N$ pairwise
similarities and explicit $N \times \ell$ gradient stores, yielding a simple
two-pass, GPU-friendly pipeline. Leveraging FD's deterministic approximation
guarantees, we analyze how agreement scoring preserves gradient energy within
the principal sketched subspace. Across multiple benchmarks, SAGE trains with
small kept-rate budgets while retaining competitive accuracy relative to
full-data training and recent subset-selection baselines, and reduces
end-to-end compute and peak memory. Overall, SAGE offers a practical,
constant-memory alternative that complements pruning and model compression for
efficient training.

</details>


### [150] [Uncertainty-Guided Model Selection for Tabular Foundation Models in Biomolecule Efficacy Prediction](https://arxiv.org/abs/2510.02476)
*Jie Li,Andrew McCarthy,Zhizhuo Zhang,Stephen Young*

Main category: cs.LG

TL;DR: TabPFN模型在生物分子功效预测中表现出潜力，但其性能对上下文高度敏感。通过选择具有最低平均 IQR 的模型集成，实现了优于朴素集成或使用在所有可用数据上训练的单一模型的性能。


<details>
  <summary>Details</summary>
Motivation: 研究背景是TabPFN等上下文学习器在生物分子功效预测中的应用，以及它们对所提供上下文的敏感性。研究旨在解决如何在没有真实标签的情况下，为集成选择最佳模型的问题。

Method: 该研究探索了一种不确定性引导的模型选择策略。通过使用简单的基于序列的特征的TabPFN模型，并结合预测的四分位距(IQR)作为不确定性度量，来选择和平均集成模型。

Result: 研究结果表明，TabPFN模型在使用简单的基于序列的特征时，可以超过最先进的预测器。模型的预测 IQR 与真实预测误差呈负相关。通过选择和平均具有最低平均 IQR 的模型集成，实现了优于朴素集成或使用在所有可用数据上训练的单一模型的性能。

Conclusion: 该研究强调了模型不确定性作为一种强大的、无标签的启发式方法，可用于优化生物分子功效预测。

Abstract: In-context learners like TabPFN are promising for biomolecule efficacy
prediction, where established molecular feature sets and relevant experimental
results can serve as powerful contextual examples. However, their performance
is highly sensitive to the provided context, making strategies like post-hoc
ensembling of models trained on different data subsets a viable approach. An
open question is how to select the best models for the ensemble without access
to ground truth labels. In this study, we investigate an uncertainty-guided
strategy for model selection. We demonstrate on an siRNA knockdown efficacy
task that a TabPFN model using simple sequence-based features can surpass
specialized state-of-the-art predictors. We also show that the model's
predicted inter-quantile range (IQR), a measure of its uncertainty, has a
negative correlation with true prediction error. By selecting and averaging an
ensemble of models with the lowest mean IQR, we achieve superior performance
compared to naive ensembling or using a single model trained on all available
data. This finding highlights model uncertainty as a powerful, label-free
heuristic for optimizing biomolecule efficacy predictions.

</details>


### [151] [Litespark Technical Report: High-Throughput, Energy-Efficient LLM Training Framework](https://arxiv.org/abs/2510.02483)
*Nii Osae Osae Dade,Moinul Hossain Rahat*

Main category: cs.LG

TL;DR: Litespark is introduced to improve the efficiency of training large language models.


<details>
  <summary>Details</summary>
Motivation: Training LLMs requires significant time and energy.

Method: Optimizations to transformer attention and MLP layers, architectural improvements with algorithmic enhancements to maximize MFU.

Result: 2x-6x training throughput improvement and 55%-83% energy consumption reduction on 3B and 30B parameter Llama models.

Conclusion: Litespark optimizations are model- and hardware-agnostic, applicable to various transformer architectures and post-training phases.

Abstract: Training Large Language Models (LLMs) is plagued by long training times and
massive energy consumption, with modern models requiring months of computation
and gigawatt-hours of electricity. In light of these challenges,we introduce
Litespark, a novel pre-training framework that addresses these inefficiencies
through targeted optimizations to transformer attention and MLP layers. Our
approach combines architectural improvements with algorithmic enhancements to
maximize Model FLOPs Utilization (MFU) while maintaining compatibility with
standard transformer implementations. Comprehensive benchmarking on 3B and 30B
parameter Llama models using the SlimPajama-627B dataset demonstrates
substantial performance gains: 2x-6x training throughput improvement and
$55\%-83$% energy consumption reduction across multi-node H200 GPU clusters.
These optimizations are model- and hardware-agnostic, enabling broad
applicability across transformer architectures and extending to post-training
phases including supervised fine-tuning and direct preference optimization.

</details>


### [152] [From Pixels to Factors: Learning Independently Controllable State Variables for Reinforcement Learning](https://arxiv.org/abs/2510.02484)
*Rafael Rodriguez-Sanchez,Cameron Allen,George Konidaris*

Main category: cs.LG

TL;DR: 本文提出了一种名为Action-Controllable Factorization (ACF) 的对比学习方法，用于从高维观测中发现独立可控的潜在变量。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习可以处理高维输入，但无法从分解结构中获益。而利用分解马尔可夫决策过程的算法比factor-agnostic方法具有更高的样本效率，但它们假设先验已知分解表示，当agent只看到高维观测时，这种要求会失效。

Method: ACF利用稀疏性：动作通常只影响变量的一个子集，而其余变量在环境动态下演变，从而为对比训练产生信息丰富的数据。

Result: 在具有已知分解结构的三个基准测试（Taxi、FourRooms 和 MiniGrid-DoorKey）中，ACF直接从像素观测中恢复了ground truth可控因素，并且始终优于基线解缠算法。

Conclusion: ACF 是一种有前景的解决表征问题的方法，它可以从高维观测中发现独立可控的潜在变量，并有可能提高强化学习算法的样本效率。

Abstract: Algorithms that exploit factored Markov decision processes are far more
sample-efficient than factor-agnostic methods, yet they assume a factored
representation is known a priori -- a requirement that breaks down when the
agent sees only high-dimensional observations. Conversely, deep reinforcement
learning handles such inputs but cannot benefit from factored structure. We
address this representation problem with Action-Controllable Factorization
(ACF), a contrastive learning approach that uncovers independently controllable
latent variables -- state components each action can influence separately. ACF
leverages sparsity: actions typically affect only a subset of variables, while
the rest evolve under the environment's dynamics, yielding informative data for
contrastive training. ACF recovers the ground truth controllable factors
directly from pixel observations on three benchmarks with known factored
structure -- Taxi, FourRooms, and MiniGrid-DoorKey -- consistently
outperforming baseline disentanglement algorithms.

</details>


### [153] [Improved Robustness of Deep Reinforcement Learning for Control of Time-Varying Systems by Bounded Extremum Seeking](https://arxiv.org/abs/2510.02490)
*Shaifalee Saxena,Alan Williams,Rafael Fierro,Alexander Scheinker*

Main category: cs.LG

TL;DR: 本文研究了使用鲁棒的、模型独立的有界极值搜索 (ES) 反馈控制来提高深度强化学习 (DRL) 控制器对于一类非线性时变系统的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: DRL 有可能从大型数据集中学习，从而快速控制或优化多参数系统的输出，但是当系统模型随时间快速变化时，其性能会急剧下降。有界 ES 可以处理具有未知控制方向的时变系统，但是其收敛速度会随着调整参数数量的增加而降低，并且像所有局部自适应方法一样，它可能会陷入局部最小值。

Method: DRL 和有界 ES 结合在一起，形成一种混合控制器，DRL 利用历史数据来学习如何快速控制多参数系统到期望的设定点，而有界 ES 确保其对时间变化的鲁棒性。

Result: 我们展示了一个通用时变系统的数值研究，以及一个组合的 ES-DRL 控制器，用于自动调整洛斯阿拉莫斯中子科学中心线性粒子加速器的低能量束传输部分。

Conclusion: DRL 和有界 ES 结合使用，其性能超过了各个部分之和。

Abstract: In this paper, we study the use of robust model independent bounded extremum
seeking (ES) feedback control to improve the robustness of deep reinforcement
learning (DRL) controllers for a class of nonlinear time-varying systems. DRL
has the potential to learn from large datasets to quickly control or optimize
the outputs of many-parameter systems, but its performance degrades
catastrophically when the system model changes rapidly over time. Bounded ES
can handle time-varying systems with unknown control directions, but its
convergence speed slows down as the number of tuned parameters increases and,
like all local adaptive methods, it can get stuck in local minima. We
demonstrate that together, DRL and bounded ES result in a hybrid controller
whose performance exceeds the sum of its parts with DRL taking advantage of
historical data to learn how to quickly control a many-parameter system to a
desired setpoint while bounded ES ensures its robustness to time variations. We
present a numerical study of a general time-varying system and a combined
ES-DRL controller for automatic tuning of the Low Energy Beam Transport section
at the Los Alamos Neutron Science Center linear particle accelerator.

</details>


### [154] [Beyond Imitation: Recovering Dense Rewards from Demonstrations](https://arxiv.org/abs/2510.02493)
*Jiangnan Li,Thuy-Trang Vu,Ehsan Abbasnejad,Gholamreza Haffari*

Main category: cs.LG

TL;DR: 本文重新定义了监督微调（SFT），不仅仅是模仿学习，更是一种奖励学习机制，为利用专家演示开辟了新的可能性。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为SFT只是一个简单的模仿学习过程，通过训练策略来模仿演示数据集中的专家行为。本文对此观点提出挑战。

Method: 本文证明了SFT目标是逆Q学习的一个特例，这意味着SFT过程不仅学习策略，还学习了一个隐含的、密集的、token级别的奖励模型来解释专家演示。然后，通过制定一个基线相关的奖励函数，展示了如何直接从SFT模型中恢复这种密集的奖励信号。

Result: 本文提出了一种名为Dense-Path REINFORCE的方法，该方法在指令跟随基准测试中始终优于原始SFT模型。

Conclusion: 本文将SFT重新定义为一种强大的奖励学习机制，而不仅仅是策略模仿。

Abstract: Conventionally, supervised fine-tuning (SFT) is treated as a simple imitation
learning process that only trains a policy to imitate expert behavior on
demonstration datasets. In this work, we challenge this view by establishing a
fundamental equivalence between SFT and Inverse Reinforcement Learning. We
prove that the SFT objective is a special case of Inverse Q-Learning, which
implies that the SFT process does not just learn a policy, but also an
implicit, dense, token-level reward model that explains the expert
demonstrations. We then show how to recover this dense reward signal directly
from the SFT model by formulating a baseline-relative reward function. The
availability of such a dense reward model offers numerous benefits, providing
granular credit assignment for each token generated. We demonstrate one key
application by using these recovered rewards to further improve the policy with
reinforcement learning. Our method, Dense-Path REINFORCE, consistently
outperforms the original SFT models on instruction-following benchmarks. This
work reframes SFT not merely as policy imitation but as a powerful reward
learning mechanism, opening new possibilities for leveraging expert
demonstrations.

</details>


### [155] [In-memory Training on Analog Devices with Limited Conductance States via Multi-tile Residual Learning](https://arxiv.org/abs/2510.02516)
*Jindan Li,Zhaoxian Wu,Gaowen Liu,Tayfun Gokmen,Tianyi Chen*

Main category: cs.LG

TL;DR: 这篇论文提出了一种残差学习框架，用于在使用有限状态忆阻器件的模拟内存计算（AIMC）加速器上进行片上训练，以补偿低精度权重更新带来的残差误差。


<details>
  <summary>Details</summary>
Motivation: 在内存训练中，通常需要至少8位导通状态才能与数字基线相匹配。然而，许多有前景的忆阻器件（如ReRAM）由于制造限制，仅提供约4位分辨率，这会显著降低训练精度。

Method: 该论文提出了一种残差学习框架，该框架在多个交叉条磁贴上依次学习，以补偿低精度权重更新中的残差误差。

Result: 在标准图像分类基准上的实验表明，该方法在有限状态设置下始终优于最先进的内存模拟训练策略。

Conclusion: 该论文提出了一种残差学习框架，可以在有限状态忆阻器件上实现有效的片上训练，并且硬件开销适中。

Abstract: Analog in-memory computing (AIMC) accelerators enable efficient deep neural
network computation directly within memory using resistive crossbar arrays,
where model parameters are represented by the conductance states of memristive
devices. However, effective in-memory training typically requires at least
8-bit conductance states to match digital baselines. Realizing such
fine-grained states is costly and often requires complex noise mitigation
techniques that increase circuit complexity and energy consumption. In
practice, many promising memristive devices such as ReRAM offer only about
4-bit resolution due to fabrication constraints, and this limited update
precision substantially degrades training accuracy. To enable on-chip training
with these limited-state devices, this paper proposes a \emph{residual
learning} framework that sequentially learns on multiple crossbar tiles to
compensate the residual errors from low-precision weight updates. Our
theoretical analysis shows that the optimality gap shrinks with the number of
tiles and achieves a linear convergence rate. Experiments on standard image
classification benchmarks demonstrate that our method consistently outperforms
state-of-the-art in-memory analog training strategies under limited-state
settings, while incurring only moderate hardware overhead as confirmed by our
cost analysis.

</details>


### [156] [Graph Generation with Spectral Geodesic Flow Matching](https://arxiv.org/abs/2510.02520)
*Xikun Huang,Tianyu Ruan,Chihao Zhang,Shihua Zhang*

Main category: cs.LG

TL;DR: 提出了一种新的图生成框架，通过匹配谱特征向量嵌入流来生成图。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了由特征向量和图的全局结构引起的几何结构。

Method: 将输入和目标图嵌入到连续黎曼流形中，然后定义嵌入之间的测地线流，并匹配这些流上的分布以生成输出图。

Result: 在各种基准测试中，在图元、度和谱指标上与最先进的方法相匹配，并且比基于扩散的模型快 30 倍。

Conclusion: SFMG 通过将谱几何与流匹配相结合，为图合成提供了一种新方法。

Abstract: Graph generation is a fundamental task with wide applications in modeling
complex systems. Although existing methods align the spectrum or degree profile
of the target graph, they often ignore the geometry induced by eigenvectors and
the global structure of the graph. In this work, we propose Spectral Geodesic
Flow Matching (SFMG), a novel framework that uses spectral eigenmaps to embed
both input and target graphs into continuous Riemannian manifolds. We then
define geodesic flows between embeddings and match distributions along these
flows to generate output graphs. Our method yields several advantages: (i)
captures geometric structure beyond eigenvalues, (ii) supports flexible
generation of diverse graphs, and (iii) scales efficiently. Empirically, SFMG
matches the performance of state-of-the-art approaches on graphlet, degree, and
spectral metrics across diverse benchmarks. In particular, it achieves up to
30$\times$ speedup over diffusion-based models, offering a substantial
advantage in scalability and training efficiency. We also demonstrate its
ability to generalize to unseen graph scales. Overall, SFMG provides a new
approach to graph synthesis by integrating spectral geometry with flow
matching.

</details>


### [157] [CHORD: Customizing Hybrid-precision On-device Model for Sequential Recommendation with Device-cloud Collaboration](https://arxiv.org/abs/2510.03038)
*Tianqi Liu,Kairui Fu,Shengyu Zhang,Wenyan Fan,Zhaocheng Du,Jieming Zhu,Fan Wu,Fei Wu*

Main category: cs.LG

TL;DR: 提出了一种名为CHORD的框架，用于在设备上进行个性化和资源自适应的序列推荐模型部署，利用通道混合精度量化。


<details>
  <summary>Details</summary>
Motivation: 现有量化方法忽略了设备特定的用户兴趣，导致推荐准确性下降。虽然设备上微调可以捕获个性化的用户偏好，但会通过本地重新训练带来额外的计算负担。

Method: 利用通道混合精度量化，通过云端的辅助超网络模块识别用户特定的关键参数，并进行参数敏感性分析，实现用户画像到量化策略的精确映射。使用2位编码量化策略来减少通信开销。

Result: 在三个真实世界数据集上，使用两个流行的backbone（SASRec和Caser）进行的实验证明了CHORD的准确性、效率和适应性。

Conclusion: CHORD 能够在保证推荐准确性的前提下，实现设备上的高效个性化模型部署，并降低了通信开销。

Abstract: With the advancement of mobile device capabilities, deploying reranking
models directly on devices has become feasible, enabling real-time contextual
recommendations. When migrating models from cloud to devices, resource
heterogeneity inevitably necessitates model compression. Recent quantization
methods show promise for efficient deployment, yet they overlook
device-specific user interests, resulting in compromised recommendation
accuracy. While on-device finetuning captures personalized user preference, it
imposes additional computational burden through local retraining. To address
these challenges, we propose a framework for \underline{\textbf{C}}ustomizing
\underline{\textbf{H}}ybrid-precision \underline{\textbf{O}}n-device model for
sequential \underline{\textbf{R}}ecommendation with
\underline{\textbf{D}}evice-cloud collaboration (\textbf{CHORD}), leveraging
channel-wise mixed-precision quantization to simultaneously achieve
personalization and resource-adaptive deployment. CHORD distributes randomly
initialized models across heterogeneous devices and identifies user-specific
critical parameters through auxiliary hypernetwork modules on the cloud. Our
parameter sensitivity analysis operates across multiple granularities (layer,
filter, and element levels), enabling precise mapping from user profiles to
quantization strategy. Through on-device mixed-precision quantization, CHORD
delivers dynamic model adaptation and accelerated inference without
backpropagation, eliminating costly retraining cycles. We minimize
communication overhead by encoding quantization strategies using only 2 bits
per channel instead of 32-bit weights. Experiments on three real-world datasets
with two popular backbones (SASRec and Caser) demonstrate the accuracy,
efficiency, and adaptivity of CHORD.

</details>


### [158] [Model-brain comparison using inter-animal transforms](https://arxiv.org/abs/2510.02523)
*Imran Thobani,Javier Sagastuy-Brena,Aran Nayebi,Jacob Prince,Rosa Cao,Daniel Yamins*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于Inter-Animal Transform Class (IATC)的比较方法，用于比较模型激活和大脑反应，它可以双向映射模型反应和大脑数据，评估模型伪装成典型受试者的能力。该方法在神经网络模型、小鼠和人类受试者群体中进行了验证，能够准确预测神经活动，同时实现机制识别的高特异性。


<details>
  <summary>Details</summary>
Motivation: 当前对于比较模型激活和大脑反应的正确方法缺乏共识。

Method: 提出了一种基于Inter-Animal Transform Class (IATC)的比较方法，通过寻找在动物群体中准确映射神经反应所需的最严格的函数集，来评估模型与大脑的匹配程度。

Result: 研究发现IATC能够分辨神经机制的细节，如非线性激活函数，并且能够准确预测神经活动，同时实现机制识别的高特异性。此外，IATC支持拓扑深度神经网络（TDANNs）作为视觉系统模型。

Conclusion: IATC实现高模型-大脑预测性和识别机制准确的大脑模型这两个目标，并为使用深度学习模型研究大脑提供了新的证据，改善了以前的模型-大脑比较方法。

Abstract: Artificial neural network models have emerged as promising mechanistic models
of the brain. However, there is little consensus on the correct method for
comparing model activations to brain responses. Drawing on recent work in
philosophy of neuroscience, we propose a comparison methodology based on the
Inter-Animal Transform Class (IATC) - the strictest set of functions needed to
accurately map neural responses between subjects in an animal population. Using
the IATC, we can map bidirectionally between a candidate model's responses and
brain data, assessing how well the model can masquerade as a typical subject
using the same kinds of transforms needed to map across real subjects. We
identify the IATC in three settings: a simulated population of neural network
models, a population of mouse subjects, and a population of human subjects. We
find that the IATC resolves detailed aspects of the neural mechanism, such as
the non-linear activation function. Most importantly, we find that the IATC
enables accurate predictions of neural activity while also achieving high
specificity in mechanism identification, evidenced by its ability to separate
response patterns from different brain areas while strongly aligning
same-brain-area responses between subjects. In other words, the IATC is a
proof-by-existence that there is no inherent tradeoff between the neural
engineering goal of high model-brain predictivity and the neuroscientific goal
of identifying mechanistically accurate brain models. Using IATC-guided
transforms, we obtain new evidence in favor of topographical deep neural
networks (TDANNs) as models of the visual system. Overall, the IATC enables
principled model-brain comparisons, contextualizing previous findings about the
predictive success of deep learning models of the brain, while improving upon
previous approaches to model-brain comparison.

</details>


### [159] [AttentiveGRUAE: An Attention-Based GRU Autoencoder for Temporal Clustering and Behavioral Characterization of Depression from Wearable Data](https://arxiv.org/abs/2510.02558)
*Nidhi Soley,Vishal M Patel,Casey O Taylor*

Main category: cs.LG

TL;DR: 提出了一种基于注意力机制的门控循环单元（GRU）自编码器AttentiveGRUAE，用于时间聚类和从纵向可穿戴数据中预测结果。


<details>
  <summary>Details</summary>
Motivation: 旨在利用可穿戴设备数据进行更有效的时间聚类和结果预测，特别是在抑郁症风险评估方面。

Method: 模型联合优化三个目标：学习每日行为特征的紧凑潜在表示、预测期末抑郁率、通过高斯混合模型（GMM）进行软聚类以识别行为亚型。

Result: 在372名参与者的纵向睡眠数据上，AttentiveGRUAE在聚类质量和抑郁症分类方面均优于基线模型。在跨年度队列上的外部验证也证实了聚类的可重复性和稳定性。

Conclusion: 该模型能够有效识别睡眠相关的差异，并识别与睡眠规律变化相关的显著时间窗口，从而为风险提供临床可解释的解释。

Abstract: In this study, we present AttentiveGRUAE, a novel attention-based gated
recurrent unit (GRU) autoencoder designed for temporal clustering and
prediction of outcome from longitudinal wearable data. Our model jointly
optimizes three objectives: (1) learning a compact latent representation of
daily behavioral features via sequence reconstruction, (2) predicting
end-of-period depression rate through a binary classification head, and (3)
identifying behavioral subtypes through Gaussian Mixture Model (GMM) based soft
clustering of learned embeddings. We evaluate AttentiveGRUAE on longitudinal
sleep data from 372 participants (GLOBEM 2018-2019), and it demonstrates
superior performance over baseline clustering, domain-aligned self-supervised,
and ablated models in both clustering quality (silhouette score = 0.70 vs
0.32-0.70) and depression classification (AUC = 0.74 vs 0.50-0.67).
Additionally, external validation on cross-year cohorts from 332 participants
(GLOBEM 2020-2021) confirms cluster reproducibility (silhouette score = 0.63,
AUC = 0.61) and stability. We further perform subtype analysis and visualize
temporal attention, which highlights sleep-related differences between clusters
and identifies salient time windows that align with changes in sleep
regularity, yielding clinically interpretable explanations of risk.

</details>


### [160] [On The Expressive Power of GNN Derivatives](https://arxiv.org/abs/2510.02565)
*Yam Eitan,Moshe Eliasof,Yoav Gelberg,Fabrizio Frasca,Guy Bar-Shalom,Haggai Maron*

Main category: cs.LG

TL;DR: 本文提出了一种新的GNN架构，称为高阶导数GNN (HOD-GNN)，它利用高阶节点导数来提高MPNN的表达能力。


<details>
  <summary>Details</summary>
Motivation: 尽管图神经网络(GNNs)取得了显著的进展，但其有限的表达能力仍然是一个根本性的挑战。GNN相对于节点特征的导数已被广泛研究，但这些导数作为增强GNN表达能力的一种手段仍未被探索。

Method: 该方法通过利用基本模型的高阶节点导数来增强消息传递神经网络(MPNN)的表达能力。这些导数生成可表达的结构感知节点嵌入，由第二个GNN在端到端可训练架构中处理。

Result: 在流行的图学习基准上的评估表明，HOD-GNN在流行的图学习任务上表现出色。

Conclusion: 理论上，我们证明了由此产生的架构系列的表达能力与WL层次结构一致。我们还在HOD-GNN、子图GNN和流行的结构编码方案之间建立了深刻的联系。

Abstract: Despite significant advances in Graph Neural Networks (GNNs), their limited
expressivity remains a fundamental challenge. Research on GNN expressivity has
produced many expressive architectures, leading to architecture hierarchies
with models of increasing expressive power. Separately, derivatives of GNNs
with respect to node features have been widely studied in the context of the
oversquashing and over-smoothing phenomena, GNN explainability, and more. To
date, these derivatives remain unexplored as a means to enhance GNN
expressivity. In this paper, we show that these derivatives provide a natural
way to enhance the expressivity of GNNs. We introduce High-Order Derivative GNN
(HOD-GNN), a novel method that enhances the expressivity of Message Passing
Neural Networks (MPNNs) by leveraging high-order node derivatives of the base
model. These derivatives generate expressive structure-aware node embeddings
processed by a second GNN in an end-to-end trainable architecture.
Theoretically, we show that the resulting architecture family's expressive
power aligns with the WL hierarchy. We also draw deep connections between
HOD-GNN, Subgraph GNNs, and popular structural encoding schemes. For
computational efficiency, we develop a message-passing algorithm for computing
high-order derivatives of MPNNs that exploits graph sparsity and parallelism.
Evaluations on popular graph learning benchmarks demonstrate HOD-GNN's strong
performance on popular graph learning tasks.

</details>


### [161] [Geospatial Machine Learning Libraries](https://arxiv.org/abs/2510.02572)
*Adam J. Stewart,Caleb Robinson,Arindam Banerjee*

Main category: cs.LG

TL;DR: 本文概述了地理空间机器学习 (GeoML) 库，分析了它们的发展、核心功能和当前生态系统。


<details>
  <summary>Details</summary>
Motivation: 地球观测数据的可用性已经超过了领域库的开发，这些领域库旨在处理其独特的挑战，例如不同的空间分辨率、光谱特性、时间节奏、数据覆盖范围、坐标系和文件格式。

Method: 本文对 GeoML 库进行了全面的概述，并介绍了流行的 GeoML 库，如 TorchGeo、eo-learn 和 Raster Vision，详细介绍了它们的架构、支持的数据类型以及与 ML 框架的集成。此外，本文还讨论了数据预处理、时空连接、基准测试和预训练模型的使用等常见方法。

Result: 本文通过一个作物类型 mapping 的案例研究，展示了这些工具的实际应用。

Conclusion: 本文重点介绍了软件设计、许可和测试方面的最佳实践，以及开放的挑战和未来的方向，特别是基础模型的兴起以及开源地理空间软件中治理的需求。

Abstract: Recent advances in machine learning have been supported by the emergence of
domain-specific software libraries, enabling streamlined workflows and
increased reproducibility. For geospatial machine learning (GeoML), the
availability of Earth observation data has outpaced the development of domain
libraries to handle its unique challenges, such as varying spatial resolutions,
spectral properties, temporal cadence, data coverage, coordinate systems, and
file formats. This chapter presents a comprehensive overview of GeoML
libraries, analyzing their evolution, core functionalities, and the current
ecosystem. It also introduces popular GeoML libraries such as TorchGeo,
eo-learn, and Raster Vision, detailing their architecture, supported data
types, and integration with ML frameworks. Additionally, it discusses common
methodologies for data preprocessing, spatial--temporal joins, benchmarking,
and the use of pretrained models. Through a case study in crop type mapping, it
demonstrates practical applications of these tools. Best practices in software
design, licensing, and testing are highlighted, along with open challenges and
future directions, particularly the rise of foundation models and the need for
governance in open-source geospatial software. Our aim is to guide
practitioners, developers, and researchers in navigating and contributing to
the rapidly evolving GeoML landscape.

</details>


### [162] [Use the Online Network If You Can: Towards Fast and Stable Reinforcement Learning](https://arxiv.org/abs/2510.02590)
*Ahmed Hendawy,Henrik Metternich,Théo Vincent,Mahdi Kallel,Jan Peters,Carlo D'Eramo*

Main category: cs.LG

TL;DR: 提出了一种新的更新规则MINTO，该规则使用目标网络和在线网络之间的最小估计值来计算目标值，从而实现更快、更稳定的价值函数学习。


<details>
  <summary>Details</summary>
Motivation: 目标网络虽然有效，但以缓慢移动的目标为代价来保持稳定性，从而延迟学习。使用在线网络作为自举目标直观上很有吸引力，但会导致不稳定的学习。

Method: 提出一种新的更新规则MINTO，使用目标网络和在线网络之间的最小估计值来计算目标值。

Result: MINTO在各种基准测试中始终提高性能，证明了其广泛的适用性和有效性。

Conclusion: MINTO能够更快、更稳定地学习价值函数，并且可以无缝集成到各种基于价值和actor-critic算法中，且成本可忽略不计。

Abstract: The use of target networks is a popular approach for estimating value
functions in deep Reinforcement Learning (RL). While effective, the target
network remains a compromise solution that preserves stability at the cost of
slowly moving targets, thus delaying learning. Conversely, using the online
network as a bootstrapped target is intuitively appealing, albeit well-known to
lead to unstable learning. In this work, we aim to obtain the best out of both
worlds by introducing a novel update rule that computes the target using the
MINimum estimate between the Target and Online network, giving rise to our
method, MINTO. Through this simple, yet effective modification, we show that
MINTO enables faster and stable value function learning, by mitigating the
potential overestimation bias of using the online network for bootstrapping.
Notably, MINTO can be seamlessly integrated into a wide range of value-based
and actor-critic algorithms with a negligible cost. We evaluate MINTO
extensively across diverse benchmarks, spanning online and offline RL, as well
as discrete and continuous action spaces. Across all benchmarks, MINTO
consistently improves performance, demonstrating its broad applicability and
effectiveness.

</details>


### [163] [Towards CONUS-Wide ML-Augmented Conceptually-Interpretable Modeling of Catchment-Scale Precipitation-Storage-Runoff Dynamics](https://arxiv.org/abs/2510.02605)
*Yuan-Heng Wang,Yang Yang,Fabio Ciulla,Hoshin V. Gupta,Charuleka Varadharajan*

Main category: cs.LG

TL;DR: 本研究着眼于基于机器学习的大样本水文建模，并检验了其在预测改进方面的效果。


<details>
  <summary>Details</summary>
Motivation: 目前的机器学习水文建模未能转化为基于增强的物理概念理解的预测改进。

Method: 本研究使用基于质量守恒感知器（MCP）的、具有不同复杂度的ML增强的物理可解释流域尺度模型，进行了一项CONUS范围的大样本研究（跨越不同的水文地质气候条件）。

Result: 结果表明，选择适当模型复杂度的模型架构非常重要，这取决于过程优势如何随水文状况而变化。基准比较表明，基于物理可解释的质量守恒MCP的模型可以达到与基于长短期记忆网络（LSTM）架构的数据模型相当的性能。

Conclusion: 本研究强调了一种理论指导的、基于物理的大样本水文学方法的潜力，重点是机械理解和开发简约且可解释的模型架构，从而为未来的模型奠定基础，这些模型在结构上编码了关于空间和时间变化的过程优势的信息。

Abstract: While many modern studies are dedicated to ML-based large-sample hydrologic
modeling, these efforts have not necessarily translated into predictive
improvements that are grounded in enhanced physical-conceptual understanding.
Here, we report on a CONUS-wide large-sample study (spanning diverse
hydro-geo-climatic conditions) using ML-augmented physically-interpretable
catchment-scale models of varying complexity based in the Mass-Conserving
Perceptron (MCP). Results were evaluated using attribute masks such as snow
regime, forest cover, and climate zone. Our results indicate the importance of
selecting model architectures of appropriate model complexity based on how
process dominance varies with hydrological regime. Benchmark comparisons show
that physically-interpretable mass-conserving MCP-based models can achieve
performance comparable to data-based models based in the Long Short-Term Memory
network (LSTM) architecture. Overall, this study highlights the potential of a
theory-informed, physically grounded approach to large-sample hydrology, with
emphasis on mechanistic understanding and the development of parsimonious and
interpretable model architectures, thereby laying the foundation for future
models of everywhere that architecturally encode information about spatially-
and temporally-varying process dominance.

</details>


### [164] [MINERVA: Mutual Information Neural Estimation for Supervised Feature Selection](https://arxiv.org/abs/2510.02610)
*Taurai Muvunzaa,Egor Kraev,Pere Planell-Morell,Alexander Y. Shestopaloff*

Main category: cs.LG

TL;DR: 提出了一种新的基于互信息神经估计的监督特征选择方法 (MINERVA)，用于解决现有特征过滤方法无法处理目标依赖于高阶特征交互的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的特征过滤方法依赖于统计配对依赖性指标来建模特征-目标关系，但当目标依赖于高阶特征交互而不是个体贡献时，这种方法可能会失败。

Method: 使用神经网络参数化互信息的近似，并使用精心设计的损失函数和稀疏性正则化器进行特征选择。该方法分两个阶段实现，将表示学习与特征选择分离，确保更好的泛化和更准确的特征重要性表达。

Result: 在合成和真实欺诈数据集上的实验结果表明了该方法的有效性及其执行精确解的能力。

Conclusion: MINERVA 能够有效地捕获这些复杂的特征-目标关系，并通过将特征子集作为一个整体进行评估。

Abstract: Existing feature filters rely on statistical pair-wise dependence metrics to
model feature-target relationships, but this approach may fail when the target
depends on higher-order feature interactions rather than individual
contributions. We introduce Mutual Information Neural Estimation Regularized
Vetting Algorithm (MINERVA), a novel approach to supervised feature selection
based on neural estimation of mutual information between features and targets.
We paramaterize the approximation of mutual information with neural networks
and perform feature selection using a carefully designed loss function
augmented with sparsity-inducing regularizers. Our method is implemented in a
two-stage process to decouple representation learning from feature selection,
ensuring better generalization and a more accurate expression of feature
importance. We present examples of ubiquitous dependency structures that are
rarely captured in literature and show that our proposed method effectively
captures these complex feature-target relationships by evaluating feature
subsets as an ensemble. Experimental results on synthetic and real-life fraud
datasets demonstrate the efficacy of our method and its ability to perform
exact solutions.

</details>


### [165] [TabImpute: Accurate and Fast Zero-Shot Missing-Data Imputation with a Pre-Trained Transformer](https://arxiv.org/abs/2510.02625)
*Jacob Feitelberg,Dwaipayan Saha,Kyuseong Choi,Zaid Ahmad,Anish Agarwal,Raaz Dwivedi*

Main category: cs.LG

TL;DR: TabImpute是一个预训练的transformer，它提供准确且快速的zero-shot插补，无需拟合或超参数调整。


<details>
  <summary>Details</summary>
Motivation: 表格数据中普遍存在缺失数据问题，现有的解决方案性能差异大，且需要耗时的超参数调整，因此没有默认的插补方法。

Method: 提出TabImpute，一个预训练的transformer，并引入entry-wise特征化以加速，以及合成训练数据生成流水线以提升性能。

Result: TabImpute在MissBench（包含42个OpenML数据集和13个缺失模式）上表现出强大的性能，优于11种已建立的插补方法。

Conclusion: TabImpute是一个有效的表格数据插补方法，在各种领域都表现出强大的性能。

Abstract: Missing data is a pervasive problem in tabular settings. Existing solutions
range from simple averaging to complex generative adversarial networks.
However, due to huge variance in performance across real-world domains and
time-consuming hyperparameter tuning, no default imputation method exists.
Building on TabPFN, a recent tabular foundation model for supervised learning,
we propose TabImpute, a pre-trained transformer that delivers accurate and fast
zero-shot imputations requiring no fitting or hyperparameter tuning at
inference-time. To train and evaluate TabImpute, we introduce (i) an entry-wise
featurization for tabular settings, which enables a $100\times$ speedup over
the previous TabPFN imputation method, (ii) a synthetic training data
generation pipeline incorporating realistic missingness patterns, which boosts
test-time performance, and (iii) MissBench, a comprehensive benchmark for
evaluation of imputation methods with $42$ OpenML datasets and $13$ missingness
patterns. MissBench spans domains such as medicine, finance, and engineering,
showcasing TabImpute's robust performance compared to $11$ established
imputation methods.

</details>


### [166] [HyperAdaLoRA: Accelerating LoRA Rank Allocation During Training via Hypernetworks without Sacrificing Performance](https://arxiv.org/abs/2510.02630)
*Hao Zhang,Zhenjia Li,Runfeng Bao,Yifan Gao,Xi Xiao,Bo Huang,Yuhang Wu,Tianyang Wang,Hao Xu*

Main category: cs.LG

TL;DR: HyperAdaLoRA通过超网络动态生成LoRA的SVD参数，加速AdaLoRA的收敛，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: LoRA对所有增量矩阵使用统一的秩，忽略了不同模块和层中权重矩阵的不同重要性。AdaLoRA虽然通过SVD分解和剪枝实现动态秩分配，但存在收敛速度慢和计算开销大的问题。

Method: 提出HyperAdaLoRA，利用基于注意力机制的超网络动态生成SVD参数，并通过剪枝超网络输出实现动态秩分配。

Result: 在各种数据集和模型上的实验表明，该方法实现了更快的收敛速度，且不牺牲性能。在其他基于LoRA的方法上的进一步扩展实验验证了该方法的广泛适用性。

Conclusion: HyperAdaLoRA通过超网络加速AdaLoRA收敛，并在各种LoRA方法中具有广泛的适用性。

Abstract: Parameter-Efficient Fine-Tuning (PEFT), especially Low-Rank Adaptation
(LoRA), has emerged as a promising approach to fine-tuning large language
models(LLMs) while reducing computational and memory overhead. However, LoRA
assumes a uniform rank \textit{r} for each incremental matrix, not accounting
for the varying significance of weight matrices across different modules and
layers. AdaLoRA leverages Singular Value Decomposition (SVD) to parameterize
updates and employs pruning of singular values to introduce dynamic rank
allocation, thereby enhancing adaptability. However, during the training
process, it often encounters issues of slow convergence speed and high
computational overhead. To address this issue, we propose HyperAdaLoRA, a novel
framework that accelerates the convergence of AdaLoRA by leveraging a
hypernetwork. Instead of directly optimizing the components of Singular Value
Decomposition $(P, \Lambda, Q)$, HyperAdaLoRA employs a hypernetwork based on
attention mechanisms to dynamically generate these parameters. By pruning the
outputs of the hypernetwork that generates the singular values, dynamic rank
allocation is achieved. Comprehensive experiments on various datasets and
models demonstrate that our method achieves faster convergence without
sacrificing performance. Additionally, further extension experiments on other
LoRA-based approaches validate the broad applicability of our method.

</details>


### [167] [Optimal Characteristics of Inspection Vehicle for Drive-by Bridge Inspection](https://arxiv.org/abs/2510.02658)
*A. Calderon Hurtado,E. Atroshchenko,K. C. Chang,C. W. Kim,M. Makki Alamdari*

Main category: cs.LG

TL;DR: 本研究致力于优化用于桥梁健康监测的行驶检测车辆，以提高其损伤检测的灵敏度。


<details>
  <summary>Details</summary>
Motivation: 行驶检测方法受车辆机械和动态特性的显著影响，限制了其有效性。

Method: 采用基于对抗自编码器(AAE)的无监督深度学习方法，重建加速度响应的频域表示。通过最小化健康和受损桥梁状态的损伤指标分布之间的Wasserstein距离，优化双轴车辆的轮胎悬架系统的质量和刚度。使用Kriging元模型来有效逼近该目标函数，并在有量纲和无量纲参数空间中识别最佳车辆配置。

Result: 结果表明，相对于桥梁的第一固有频率，频率比在0.3和0.7之间的车辆最有效，而接近共振的车辆表现较差。较轻的车辆需要较低的固有频率才能实现最佳检测。

Conclusion: 本研究首次严格优化了行驶检测的传感平台，并提出了专用检测车辆。

Abstract: Drive-by inspection for bridge health monitoring has gained increasing
attention over the past decade. This method involves analysing the coupled
vehicle-bridge response, recorded by an instrumented inspection vehicle, to
assess structural integrity and detect damage. However, the vehicles mechanical
and dynamic properties significantly influence detection performance, limiting
the effectiveness of the approach. This study presents a framework for
optimising the inspection vehicle to enhance damage sensitivity. An
unsupervised deep learning methodbased on adversarial autoencoders (AAE)is used
to reconstruct the frequency-domain representation of acceleration responses.
The mass and stiffness of the tyre suspension system of a two-axle vehicle are
optimised by minimising the Wasserstein distance between damage index
distributions for healthy and damaged bridge states. A Kriging meta-model is
employed to approximate this objective function efficiently and identify
optimal vehicle configurations in both dimensional and non-dimensional
parameter spaces. Results show that vehicles with frequency ratios between 0.3
and 0.7 relative to the bridges' first natural frequency are most effective,
while those near resonance perform poorly. Lighter vehicles require lower
natural frequencies for optimal detection. This is the first study to
rigorously optimise the sensing platform for drive-by sensing and to propose a
purpose-built inspection vehicle.

</details>


### [168] [TutorBench: A Benchmark To Assess Tutoring Capabilities Of Large Language Models](https://arxiv.org/abs/2510.02663)
*Rakshith S Srinivasa,Zora Che,Chen Bo Calvin Zhang,Diego Mares,Ernesto Hernandez,Jayeon Park,Dean Lee,Guillermo Mangialardi,Charmaine Ng,Ed-Yeremai Hernandez Cardona,Anisha Gunjal,Yunzhong He,Bing Liu,Chen Xing*

Main category: cs.LG

TL;DR: TutorBench is introduced to evaluate LLMs on tutoring skills.


<details>
  <summary>Details</summary>
Motivation: It is crucial to build LLMs adept at tutoring as students increasingly use them as learning aids.

Method: A dataset and evaluation benchmark is designed to rigorously evaluate the core tutoring skills of LLMs.

Result: None of the frontier LLMs achieve a score of greater than 56%, showing a large room for improvement.

Conclusion: TutorBench is released to guide the development of the next-generation of AI tutors.

Abstract: As students increasingly adopt large language models (LLMs) as learning aids,
it is crucial to build models that are adept at handling the nuances of
tutoring: they need to identify the core needs of students, be adaptive,
provide personalized guidance, and be accurate. To this end, we introduce
TutorBench, a dataset and evaluation benchmark designed to rigorously evaluate
the core tutoring skills of LLMs. The dataset comprises 1,490 samples curated
by human experts, focused on high-school and AP-level curricula. The samples
are drawn from three common tutoring tasks: (i) generating adaptive
explanations tailored to a student's confusion, (ii) providing actionable
feedback on a student's work, and (iii) promoting active learning through
effective hint generation. To account for the inherent complexity of tutoring,
samples are accompanied by sample-specific rubrics which are used to judge
model responses during evaluation. TutorBench uses a reliable and fine-grained
automatic evaluation method that uses an LLM-judge and the sample-specific
rubrics. We evaluate 16 frontier LLMs on TutorBench and present a detailed
analysis of their performance and behavior. Our results show that none of the
frontier LLMs achieve a score of greater than $56\%$, showing a large room for
improvement. We find that LLMs fall short in exhibiting the full range of
tutoring skills needed to guide, diagnose, and support students effectively,
with all the frontier models achieving less than a $60\%$ pass rate on rubric
criteria related to these skills. We also find that different model families
exhibit varied strengths and limitations: the Claude models outperform others
in supporting active learning, while they lag behind in the other two use
cases. By releasing TutorBench, we provide a comprehensive and unsaturated
benchmark to guide the development of the next-generation of AI tutors.

</details>


### [169] [Topological Invariance and Breakdown in Learning](https://arxiv.org/abs/2510.02670)
*Yongyi Yang,Tomaso Poggio,Isaac Chuang,Liu Ziyin*

Main category: cs.LG

TL;DR: 本文证明了对于一类广泛的置换等变学习规则，训练过程会在神经元之间产生双 Lipschitz 映射，并强烈约束训练期间神经元分布的拓扑结构。


<details>
  <summary>Details</summary>
Motivation: 揭示了小学习率和大学习率之间的质的区别。

Method: 梯度下降

Result: 学习动态可以分为两个阶段：首先，在拓扑约束下进行平滑优化，然后进入通过剧烈的拓扑简化进行学习的第二阶段。

Conclusion: 我们的理论的一个关键特征是它独立于特定的架构或损失函数，从而能够将拓扑方法普遍应用于深度学习的研究。

Abstract: We prove that for a broad class of permutation-equivariant learning rules
(including SGD, Adam, and others), the training process induces a bi-Lipschitz
mapping between neurons and strongly constrains the topology of the neuron
distribution during training. This result reveals a qualitative difference
between small and large learning rates $\eta$. With a learning rate below a
topological critical point $\eta^*$, the training is constrained to preserve
all topological structure of the neurons. In contrast, above $\eta^*$, the
learning process allows for topological simplification, making the neuron
manifold progressively coarser and thereby reducing the model's expressivity.
Viewed in combination with the recent discovery of the edge of stability
phenomenon, the learning dynamics of neuron networks under gradient descent can
be divided into two phases: first they undergo smooth optimization under
topological constraints, and then enter a second phase where they learn through
drastic topological simplifications. A key feature of our theory is that it is
independent of specific architectures or loss functions, enabling the universal
application of topological methods to the study of deep learning.

</details>


### [170] [To Compress or Not? Pushing the Frontier of Lossless GenAI Model Weights Compression with Exponent Concentration](https://arxiv.org/abs/2510.02676)
*Zeyu Yang,Tianyi Zhang,Jianwen Xie,Chuan Li,Zhaozhuo Xu,Anshumali Shrivastava*

Main category: cs.LG

TL;DR: 本文提出了一种新的FP8格式ECF8，用于压缩GenAI模型，实现了无损计算。


<details>
  <summary>Details</summary>
Motivation: 为了解决GenAI模型规模过大导致部署效率低下的问题，探索低精度计算方案。

Method: 通过理论分析和实证研究，发现GenAI模型权重中存在指数集中现象，并基于此设计了ECF8压缩框架。

Result: 在高达671B参数的LLM和DiT模型上，ECF8实现了高达26.9%的内存节省和177.1%的吞吐量加速，且模型输出无偏差。

Conclusion: 指数集中是训练模型的一种统计规律，为FP8时代的无损低精度浮点设计开辟了一条有原则的道路。

Abstract: The scaling of Generative AI (GenAI) models into the hundreds of billions of
parameters makes low-precision computation indispensable for efficient
deployment. We argue that the fundamental solution lies in developing
low-precision floating-point formats, which inherently provide numerical
stability, memory savings, and hardware efficiency without dequantization
overhead. In this paper, we present a theoretical and empirical study of an
exponent concentration phenomenon in GenAI weights: exponents consistently
exhibit low entropy across architectures and modalities. We show that this
arises naturally from $\alpha$-stable distributions induced by stochastic
gradient descent, and we prove tight bounds on the entropy of exponents. Our
analysis establishes a theoretical compression limit near FP4.67, which
motivates the design of a practical FP8 format. Building on these insights, we
propose Exponent-Concentrated FP8 (ECF8), a lossless compression framework with
entropy-aware encoding and GPU-optimized decoding. Experiments on LLMs and DiTs
up to 671B parameters demonstrate up to 26.9% memory savings and 177.1%
throughput acceleration, with perfectly lossless computations, i.e., no
deviation in model outputs. Our results establish exponent concentration as a
statistical law of trained models and open a principled path for lossless
low-precision floating-point design in the FP8 era.

</details>


### [171] [Multiplicative-Additive Constrained Models:Toward Joint Visualization of Interactive and Independent Effects](https://arxiv.org/abs/2509.21923)
*Fumin Wang*

Main category: cs.LG

TL;DR: 本文提出了一种新的模型，Multiplicative-Additive Constrained Models (MACMs)，它结合了乘法模型和加法模型的优点，既能可视化形状函数，又能捕捉所有特征之间的交互作用。


<details>
  <summary>Details</summary>
Motivation: 现有的广义加性模型(GAMs)虽然具有良好的可解释性，但由于忽略了高阶交互效应，预测性能受到限制。而Curve Ergodic Set Regression (CESR)模型虽然能够捕捉所有特征之间的交互作用，但性能不如GAMs。

Method: 本文提出的MACMs模型通过增加一个加性部分来增强CESR模型，从而解开交互项和独立项的缠绕系数，有效地扩大了假设空间。该模型由乘法部分和加法部分组成，它们的形状函数都可以自然地可视化。

Result: 实验结果表明，基于神经网络的MACMs在预测性能方面明显优于CESR和当前最先进的GAMs。

Conclusion: MACMs模型是对CESR和GAMs的改进，它既具有良好的可解释性，又具有较高的预测性能。

Abstract: Interpretability is one of the considerations when applying machine learning
to high-stakes fields such as healthcare that involve matters of life safety.
Generalized Additive Models (GAMs) enhance interpretability by visualizing
shape functions. Nevertheless, to preserve interpretability, GAMs omit
higher-order interaction effects (beyond pairwise interactions), which imposes
significant constraints on their predictive performance. We observe that Curve
Ergodic Set Regression (CESR), a multiplicative model, naturally enables the
visualization of its shape functions and simultaneously incorporates both
interactions among all features and individual feature effects. Nevertheless,
CESR fails to demonstrate superior performance compared to GAMs. We introduce
Multiplicative-Additive Constrained Models (MACMs), which augment CESR with an
additive part to disentangle the intertwined coefficients of its interactive
and independent terms, thus effectively broadening the hypothesis space. The
model is composed of a multiplicative part and an additive part, whose shape
functions can both be naturally visualized, thereby assisting users in
interpreting how features participate in the decision-making process.
Consequently, MACMs constitute an improvement over both CESR and GAMs. The
experimental results indicate that neural network-based MACMs significantly
outperform both CESR and the current state-of-the-art GAMs in terms of
predictive performance.

</details>


### [172] [Can Data-Driven Dynamics Reveal Hidden Physics? There Is A Need for Interpretable Neural Operators](https://arxiv.org/abs/2510.02683)
*Wenhan Gao,Jian Luo,Fang Wan,Ruichen Xu,Xiang Liu,Haipeng Xing,Yi Liu*

Main category: cs.LG

TL;DR: 本文对神经算子进行了分类，并从多个角度探讨了它们在学习数据驱动的动力学中的应用，特别是在物理原则方面。同时，指出了现有解释方法的局限性以及未来研究的方向。


<details>
  <summary>Details</summary>
Motivation: 目前对神经算子的学习机制理解不深。

Method: 将神经算子分为空间域模型和函数域模型，并从这个分类出发提出了几个观点。

Result: 解释了神经算子的预测过程，表明神经算子可以从数据中学习隐藏的物理模式。同时，证明了一个简单的双空间多尺度模型可以达到最佳性能。

Conclusion: 强调了将已知物理知识纳入神经算子的必要性，以实现更好的泛化并发现更多隐藏的物理现象。

Abstract: Recently, neural operators have emerged as powerful tools for learning
mappings between function spaces, enabling data-driven simulations of complex
dynamics. Despite their successes, a deeper understanding of their learning
mechanisms remains underexplored. In this work, we classify neural operators
into two types: (1) Spatial domain models that learn on grids and (2)
Functional domain models that learn with function bases. We present several
viewpoints based on this classification and focus on learning data-driven
dynamics adhering to physical principles. Specifically, we provide a way to
explain the prediction-making process of neural operators and show that neural
operator can learn hidden physical patterns from data. However, this
explanation method is limited to specific situations, highlighting the urgent
need for generalizable explanation methods. Next, we show that a simple
dual-space multi-scale model can achieve SOTA performance and we believe that
dual-space multi-spatio-scale models hold significant potential to learn
complex physics and require further investigation. Lastly, we discuss the
critical need for principled frameworks to incorporate known physics into
neural operators, enabling better generalization and uncovering more hidden
physical phenomena.

</details>


### [173] [EvoSpeak: Large Language Models for Interpretable Genetic Programming-Evolved Heuristics](https://arxiv.org/abs/2510.02686)
*Meng Xu,Jiao Liu,Yew Soon Ong*

Main category: cs.LG

TL;DR: EvoSpeak: A framework integrating GP with LLMs to improve heuristic evolution.


<details>
  <summary>Details</summary>
Motivation: Complex GP heuristics in dynamic scenarios lack interpretability and transferability.

Method: EvoSpeak learns from GP heuristics, extracts knowledge, and uses it to generate warm-start populations, translate GP trees into natural language, and enable knowledge transfer.

Result: EvoSpeak produces more effective heuristics, improves efficiency, and delivers human-readable reports in dynamic flexible job shop scheduling.

Conclusion: EvoSpeak advances intelligent, transparent, and user-aligned heuristics by combining GP and LLMs.

Abstract: Genetic programming (GP) has demonstrated strong effectiveness in evolving
tree-structured heuristics for complex optimization problems. Yet, in dynamic
and large-scale scenarios, the most effective heuristics are often highly
complex, hindering interpretability, slowing convergence, and limiting
transferability across tasks. To address these challenges, we present EvoSpeak,
a novel framework that integrates GP with large language models (LLMs) to
enhance the efficiency, transparency, and adaptability of heuristic evolution.
EvoSpeak learns from high-quality GP heuristics, extracts knowledge, and
leverages this knowledge to (i) generate warm-start populations that accelerate
convergence, (ii) translate opaque GP trees into concise natural-language
explanations that foster interpretability and trust, and (iii) enable knowledge
transfer and preference-aware heuristic generation across related tasks. We
verify the effectiveness of EvoSpeak through extensive experiments on dynamic
flexible job shop scheduling (DFJSS), under both single- and multi-objective
formulations. The results demonstrate that EvoSpeak produces more effective
heuristics, improves evolutionary efficiency, and delivers human-readable
reports that enhance usability. By coupling the symbolic reasoning power of GP
with the interpretative and generative strengths of LLMs, EvoSpeak advances the
development of intelligent, transparent, and user-aligned heuristics for
real-world optimization problems.

</details>


### [174] [Fine-Tuning Diffusion Models via Intermediate Distribution Shaping](https://arxiv.org/abs/2510.02692)
*Gautham Govind Anil,Shaan Ul Haque,Nithish Kannen,Dheeraj Nagaraj,Sanjay Shakkottai,Karthikeyan Shanmugam*

Main category: cs.LG

TL;DR: 本文统一了基于拒绝采样的微调（RAFT）的变体作为GRAFT，并表明这隐式地执行了具有重塑奖励的PPO。然后，引入P-GRAFT以在中间噪声水平下塑造分布，并通过偏差-方差权衡在数学上解释了这一点。受此启发，我们提出了逆噪声校正以改进流动模型，而无需利用显式奖励。


<details>
  <summary>Details</summary>
Motivation: 虽然预训练的扩散模型可以有效地捕获训练数据分布，但通常需要使用奖励函数来塑造这些分布，以使其与下游应用保持一致。策略梯度方法（如近端策略优化 (PPO)）广泛用于自回归生成。

Method: 统一了基于拒绝采样的微调（RAFT）的变体作为GRAFT，引入P-GRAFT以在中间噪声水平下塑造分布，并提出了逆噪声校正以改进流动模型。

Result: 在文本到图像 (T2I) 生成方面，应用于 Stable Diffusion 2 的框架在 VQAScore 方面优于流行的 T2I 基准上的策略梯度方法，并且比基本模型提高了 8.81%。对于无条件图像生成，逆噪声校正提高了较低 FLOP/图像下生成图像的 FID。

Conclusion: 本文提出GRAFT, P-GRAFT 和 inverse noise correction, 并在T2I生成，布局生成，分子生成和无条件图像生成上验证了有效性

Abstract: Diffusion models are widely used for generative tasks across domains. While
pre-trained diffusion models effectively capture the training data
distribution, it is often desirable to shape these distributions using reward
functions to align with downstream applications. Policy gradient methods, such
as Proximal Policy Optimization (PPO), are widely used in the context of
autoregressive generation. However, the marginal likelihoods required for such
methods are intractable for diffusion models, leading to alternative proposals
and relaxations. In this context, we unify variants of Rejection sAmpling based
Fine-Tuning (RAFT) as GRAFT, and show that this implicitly performs PPO with
reshaped rewards. We then introduce P-GRAFT to shape distributions at
intermediate noise levels and demonstrate empirically that this can lead to
more effective fine-tuning. We mathematically explain this via a bias-variance
tradeoff. Motivated by this, we propose inverse noise correction to improve
flow models without leveraging explicit rewards. We empirically evaluate our
methods on text-to-image(T2I) generation, layout generation, molecule
generation and unconditional image generation. Notably, our framework, applied
to Stable Diffusion 2, improves over policy gradient methods on popular T2I
benchmarks in terms of VQAScore and shows an $8.81\%$ relative improvement over
the base model. For unconditional image generation, inverse noise correction
improves FID of generated images at lower FLOPs/image.

</details>


### [175] [RAMAC: Multimodal Risk-Aware Offline Reinforcement Learning and the Role of Behavior Regularization](https://arxiv.org/abs/2510.02695)
*Kai Fukazawa,Kunal Mundada,Iman Soltani*

Main category: cs.LG

TL;DR: 提出了一种新的离线强化学习框架，该框架能够在保证高回报的同时，降低风险。


<details>
  <summary>Details</summary>
Motivation: 之前的风险规避离线强化学习方法以牺牲价值保守主义和限制策略类别为代价来实现安全性，而富有表现力的策略仅在风险中性的设置中使用。为了解决这个问题。

Method: 提出了风险感知多模态Actor-Critic (RAMAC) 框架，该框架将富有表现力的生成式actor与分布式的评论家相结合。

Result: 在大多数Stochastic-D4RL任务上，在保持强大回报的同时，在CVaR_0.1方面观察到了一致的收益。

Conclusion: 该框架实现了复杂多模态场景中的风险敏感学习。

Abstract: In safety-critical domains where online data collection is infeasible,
offline reinforcement learning (RL) offers an attractive alternative but only
if policies deliver high returns without incurring catastrophic lower-tail
risk. Prior work on risk-averse offline RL achieves safety at the cost of value
conservatism and restricted policy classes, whereas expressive policies are
only used in risk-neutral settings. Here, we address this gap by introducing
the \textbf{Risk-Aware Multimodal Actor-Critic (RAMAC)} framework, which
couples an \emph{expressive generative actor} with a distributional critic. The
RAMAC differentiates composite objective combining distributional risk and BC
loss through the generative path, achieving risk-sensitive learning in complex
multimodal scenarios. We instantiate RAMAC with diffusion and flow-matching
actors and observe consistent gains in $\mathrm{CVaR}_{0.1}$ while maintaining
strong returns on most Stochastic-D4RL tasks. Code:
https://github.com/KaiFukazawa/RAMAC.git

</details>


### [176] [A Novel Unified Lightweight Temporal-Spatial Transformer Approach for Intrusion Detection in Drone Networks](https://arxiv.org/abs/2510.02711)
*Tarun Kumar Biswas,Ashrafun Zannat,Waqas Ishtiaq,Md. Alamgir Hossain*

Main category: cs.LG

TL;DR: 提出了一种名为TSLT-Net的新型轻量级统一入侵检测系统，专为无人机网络量身定制。


<details>
  <summary>Details</summary>
Motivation: 无人机在商业、工业和民用领域的日益普及带来了巨大的网络安全挑战，特别是因为无人机网络容易受到各种网络攻击。现有的入侵检测机制通常缺乏无人机运行所需的动态和资源受限环境所需的适应性、效率和通用性。

Method: 利用自注意力机制，TSLT-Net有效地建模了网络流量中的时间模式和空间依赖性，从而能够准确检测各种入侵类型。该框架包括简化的预处理管道，并支持单个架构中的多类攻击分类和二进制异常检测。

Result: 在包含超过230万条标记记录的ISOT无人机异常检测数据集上进行的大量实验表明，TSLT-Net具有卓越的性能，在多类检测中的准确率为99.99％，在二进制异常检测中的准确率为100％，同时保持最小的内存占用量，仅为0.04 MB和9722个可训练参数。

Conclusion: 这些结果证明TSLT-Net是一种有效且可扩展的实时无人机网络安全解决方案，特别适合在关键任务UAV系统中的边缘设备上部署。

Abstract: The growing integration of drones across commercial, industrial, and civilian
domains has introduced significant cybersecurity challenges, particularly due
to the susceptibility of drone networks to a wide range of cyberattacks.
Existing intrusion detection mechanisms often lack the adaptability,
efficiency, and generalizability required for the dynamic and resource
constrained environments in which drones operate. This paper proposes TSLT-Net,
a novel lightweight and unified Temporal Spatial Transformer based intrusion
detection system tailored specifically for drone networks. By leveraging self
attention mechanisms, TSLT-Net effectively models both temporal patterns and
spatial dependencies in network traffic, enabling accurate detection of diverse
intrusion types. The framework includes a streamlined preprocessing pipeline
and supports both multiclass attack classification and binary anomaly detection
within a single architecture. Extensive experiments conducted on the ISOT Drone
Anomaly Detection Dataset, consisting of more than 2.3 million labeled records,
demonstrate the superior performance of TSLT-Net with 99.99 percent accuracy in
multiclass detection and 100 percent in binary anomaly detection, while
maintaining a minimal memory footprint of only 0.04 MB and 9722 trainable
parameters. These results establish TSLT-Net as an effective and scalable
solution for real time drone cybersecurity, particularly suitable for
deployment on edge devices in mission critical UAV systems.

</details>


### [177] [CST-AFNet: A dual attention-based deep learning framework for intrusion detection in IoT networks](https://arxiv.org/abs/2510.02717)
*Waqas Ishtiaq,Ashrafun Zannat,A. H. M. Shahariar Parvez,Md. Alamgir Hossain,Muntasir Hasan Kanchan,Muhammad Masud Tarek*

Main category: cs.LG

TL;DR: CST AFNet: A novel deep learning framework for intrusion detection in IoT networks, achieving high accuracy and outperforming traditional models.


<details>
  <summary>Details</summary>
Motivation: The increasing cybersecurity challenges in IoT due to its heterogeneous and distributed nature.

Method: A dual attention-based deep learning framework (CST AFNet) integrating CNNs, BiGRUs, and channel/temporal attention mechanisms.

Result: Achieves 99.97% accuracy on the Edge IIoTset dataset, with precision, recall, and F1 score above 99.3%.

Conclusion: CST AFNet is a powerful and scalable solution for real-time cyber threat detection in IoT/IIoT environments.

Abstract: The rapid expansion of the Internet of Things (IoT) has revolutionized modern
industries by enabling smart automation and real time connectivity. However,
this evolution has also introduced complex cybersecurity challenges due to the
heterogeneous, resource constrained, and distributed nature of these
environments. To address these challenges, this research presents CST AFNet, a
novel dual attention based deep learning framework specifically designed for
robust intrusion detection in IoT networks. The model integrates multi scale
Convolutional Neural Networks (CNNs) for spatial feature extraction,
Bidirectional Gated Recurrent Units (BiGRUs) for capturing temporal
dependencies, and a dual attention mechanism, channel and temporal attention,
to enhance focus on critical patterns in the data. The proposed method was
trained and evaluated on the Edge IIoTset dataset, a comprehensive and
realistic benchmark containing more than 2.2 million labeled instances spanning
15 attack types and benign traffic, collected from a seven layer industrial
testbed. Our proposed model achieves outstanding accuracy for both 15 attack
types and benign traffic. CST AFNet achieves 99.97 percent accuracy. Moreover,
this model demonstrates exceptional performance with macro averaged precision,
recall, and F1 score all above 99.3 percent. Experimental results show that CST
AFNet achieves superior detection accuracy, significantly outperforming
traditional deep learning models. The findings confirm that CST AFNet is a
powerful and scalable solution for real time cyber threat detection in complex
IoT and IIoT environments, paving the way for more secure, intelligent, and
adaptive cyber physical systems.

</details>


### [178] [Hyperparameter Loss Surfaces Are Simple Near their Optima](https://arxiv.org/abs/2510.02721)
*Nicholas Lourie,He He,Kyunghyun Cho*

Main category: cs.LG

TL;DR: 研究人员通常会设计在各种规模上都能很好地训练的配方，但很少有工具可以理解超参数损失面。我们提出了一种新技术，可以揭示损失面的结构，并推导出随机搜索的渐近定律，从而实现新的分析。


<details>
  <summary>Details</summary>
Motivation: 现代模型太大，无法进行广泛的搜索，因此研究人员根据他们对超参数的理解来设计可以在各种规模上都能很好地训练的配方。然而，很少有工具可以理解超参数损失面。

Method: 我们开发了一种基于随机搜索的新技术，以揭示这种渐近状态。

Result: 随机搜索的最佳分数呈现出一种新的分布，其参数精确地定义了渐近状态下的损失面特征。从这些特征中，我们推导出了一种新的随机搜索渐近定律，可以解释和推断其收敛性。

Conclusion: 这些新工具可以实现新的分析，例如最佳性能的置信区间或确定有效超参数的数量。

Abstract: Hyperparameters greatly impact models' capabilities; however, modern models
are too large for extensive search. Instead, researchers design recipes that
train well across scales based on their understanding of the hyperparameters.
Despite this importance, few tools exist for understanding the hyperparameter
loss surface. We discover novel structure in it and propose a new theory
yielding such tools. The loss surface is complex, but as you approach the
optimum simple structure emerges. It becomes characterized by a few basic
features, like its effective dimension and the best possible loss. To uncover
this asymptotic regime, we develop a novel technique based on random search.
Within this regime, the best scores from random search take on a new
distribution we discover. Its parameters are exactly the features defining the
loss surface in the asymptotic regime. From these features, we derive a new
asymptotic law for random search that can explain and extrapolate its
convergence. These new tools enable new analyses, such as confidence intervals
for the best possible performance or determining the effective number of
hyperparameters. We make these tools available at
https://github.com/nicholaslourie/opda .

</details>


### [179] [Accuracy Law for the Future of Deep Time Series Forecasting](https://arxiv.org/abs/2510.02729)
*Yuxuan Wang,Haixu Wu,Yuezhou Ma,Yuchen Fang,Ziyi Zhang,Yong Liu,Shiyu Wang,Zhou Ye,Yang Xiang,Jianmin Wang,Mingsheng Long*

Main category: cs.LG

TL;DR: 本文研究了深度时间序列预测的性能上限问题，提出了一个精度定律，可以用来识别饱和任务和指导模型训练。


<details>
  <summary>Details</summary>
Motivation: 目前深度时间序列预测的研究方向有些混乱，因为在标准数据集上的改进很小。时间序列预测本质上具有非零误差下限，因为其具有部分可观察性和不确定性。

Method: 通过对2800多个新训练的深度预测器的严格统计测试，发现深度模型的最小预测误差与窗口序列模式的复杂性之间存在显着的指数关系，这被称为精度定律。

Result: 提出的精度定律成功地指导我们从广泛使用的基准测试中识别饱和任务，并为大型时间序列模型推导出有效的训练策略。

Conclusion: 该研究为未来的研究提供了有价值的见解

Abstract: Deep time series forecasting has emerged as a booming direction in recent
years. Despite the exponential growth of community interests, researchers are
sometimes confused about the direction of their efforts due to minor
improvements on standard benchmarks. In this paper, we notice that, unlike
image recognition, whose well-acknowledged and realizable goal is 100%
accuracy, time series forecasting inherently faces a non-zero error lower bound
due to its partially observable and uncertain nature. To pinpoint the research
objective and release researchers from saturated tasks, this paper focuses on a
fundamental question: how to estimate the performance upper bound of deep time
series forecasting? Going beyond classical series-wise predictability metrics,
e.g., ADF test, we realize that the forecasting performance is highly related
to window-wise properties because of the sequence-to-sequence forecasting
paradigm of deep time series models. Based on rigorous statistical tests of
over 2,800 newly trained deep forecasters, we discover a significant
exponential relationship between the minimum forecasting error of deep models
and the complexity of window-wise series patterns, which is termed the accuracy
law. The proposed accuracy law successfully guides us to identify saturated
tasks from widely used benchmarks and derives an effective training strategy
for large time series models, offering valuable insights for future research.

</details>


### [180] [Dale meets Langevin: A Multiplicative Denoising Diffusion Model](https://arxiv.org/abs/2510.02730)
*Nishanth Shetty,Madhava Prasath,Chandra Sekhar Seelamantula*

Main category: cs.LG

TL;DR: 本文提出了一种新的受生物学启发的生成模型，该模型基于几何布朗运动，采用乘法更新，并与 Dale 定律相关。


<details>
  <summary>Details</summary>
Motivation: 标准梯度下降优化与生物系统中的学习不一致，因此需要开发受生物学启发的学习技术。

Method: 从控制几何布朗运动的 SDE 出发，通过离散化相应的逆时 SDE 得到乘法更新规则，并提出了一种新的乘法去噪 score-matching 形式。

Result: 在 MNIST、Fashion MNIST 和 Kuzushiji 数据集上的实验结果证明了新方案的生成能力。

Conclusion: 本文首次提出了基于几何布朗运动并采用乘法更新的生物学启发生成模型。

Abstract: Gradient descent has proven to be a powerful and effective technique for
optimization in numerous machine learning applications. Recent advances in
computational neuroscience have shown that learning in standard gradient
descent optimization formulation is not consistent with learning in biological
systems. This has opened up interesting avenues for building biologically
inspired learning techniques. One such approach is inspired by Dale's law,
which states that inhibitory and excitatory synapses do not swap roles during
the course of learning. The resulting exponential gradient descent optimization
scheme leads to log-normally distributed synaptic weights. Interestingly, the
density that satisfies the Fokker-Planck equation corresponding to the
stochastic differential equation (SDE) with geometric Brownian motion (GBM) is
the log-normal density. Leveraging this connection, we start with the SDE
governing geometric Brownian motion, and show that discretizing the
corresponding reverse-time SDE yields a multiplicative update rule, which
surprisingly, coincides with the sampling equivalent of the exponential
gradient descent update founded on Dale's law. Furthermore, we propose a new
formalism for multiplicative denoising score-matching, subsuming the loss
function proposed by Hyvaerinen for non-negative data. Indeed, log-normally
distributed data is positive and the proposed score-matching formalism turns
out to be a natural fit. This allows for training of score-based models for
image data and results in a novel multiplicative update scheme for sample
generation starting from a log-normal density. Experimental results on MNIST,
Fashion MNIST, and Kuzushiji datasets demonstrate generative capability of the
new scheme. To the best of our knowledge, this is the first instance of a
biologically inspired generative model employing multiplicative updates,
founded on geometric Brownian motion.

</details>


### [181] [Hybrid-Collaborative Augmentation and Contrastive Sample Adaptive-Differential Awareness for Robust Attributed Graph Clustering](https://arxiv.org/abs/2510.02731)
*Tianxiang Zhao,Youqing Wang,Jinlu Wang,Jiapu Wang,Mingliang Cui,Junbin Gao,Jipeng Guo*

Main category: cs.LG

TL;DR: 本文提出了一种新的鲁棒属性图聚类（RAGC）方法，该方法结合了混合协作增强（HCA）和对比样本自适应差分感知（CSADA），以解决现有对比属性图聚类（CAGC）方法中忽略边缘级别嵌入增强以及未区分对待不同对比样本对的问题。


<details>
  <summary>Details</summary>
Motivation: 现有CAGC方法主要依赖于有效的数据增强和对比目标设置，但忽略了边缘级别嵌入增强以及节点级别和边缘级别嵌入增强之间在不同粒度上的交互。此外，它们通常平等对待所有对比样本对，忽略了难易正负样本对之间的显着差异，这最终限制了它们的区分能力。

Method: 本文提出RAGC，通过同时执行节点级别和边缘级别嵌入表示和增强，建立更全面的相似性度量标准，用于后续的对比学习。然后，利用具有高置信度的伪标签信息，设计了一种CSADA策略，自适应地识别所有对比样本对，并通过创新的权重调制函数对它们进行差异化处理。

Result: 在六个基准数据集上的综合图聚类评估表明，所提出的RAGC相对于几种最先进的CAGC方法具有有效性。

Conclusion: RAGC方法中的HCA和CSADA模块相互加强，从而增强了表示学习中的可区分性。

Abstract: Due to its powerful capability of self-supervised representation learning and
clustering, contrastive attributed graph clustering (CAGC) has achieved great
success, which mainly depends on effective data augmentation and contrastive
objective setting. However, most CAGC methods utilize edges as auxiliary
information to obtain node-level embedding representation and only focus on
node-level embedding augmentation. This approach overlooks edge-level embedding
augmentation and the interactions between node-level and edge-level embedding
augmentations across various granularity. Moreover, they often treat all
contrastive sample pairs equally, neglecting the significant differences
between hard and easy positive-negative sample pairs, which ultimately limits
their discriminative capability. To tackle these issues, a novel robust
attributed graph clustering (RAGC), incorporating hybrid-collaborative
augmentation (HCA) and contrastive sample adaptive-differential awareness
(CSADA), is proposed. First, node-level and edge-level embedding
representations and augmentations are simultaneously executed to establish a
more comprehensive similarity measurement criterion for subsequent contrastive
learning. In turn, the discriminative similarity further consciously guides
edge augmentation. Second, by leveraging pseudo-label information with high
confidence, a CSADA strategy is elaborately designed, which adaptively
identifies all contrastive sample pairs and differentially treats them by an
innovative weight modulation function. The HCA and CSADA modules mutually
reinforce each other in a beneficent cycle, thereby enhancing discriminability
in representation learning. Comprehensive graph clustering evaluations over six
benchmark datasets demonstrate the effectiveness of the proposed RAGC against
several state-of-the-art CAGC methods.

</details>


### [182] [TokenFlow: Responsive LLM Text Streaming Serving under Request Burst via Preemptive Scheduling](https://arxiv.org/abs/2510.02758)
*Junyi Chen,Chuheng Du,Renyuan Liu,Shuochao Yao,Dingtian Yan,Jiang Liao,Shengzhong Liu,Fan Wu,Guihai Chen*

Main category: cs.LG

TL;DR: TokenFlow是一个新的LLM服务系统，通过抢占式请求调度和主动的键值（KV）缓存管理来增强文本流式传输性能。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM服务系统由于非抢占式请求调度和被动内存管理导致资源利用率低，请求处理并行度低。

Method: TokenFlow基于实时token缓冲区占用率和token消耗率动态地对请求进行优先级排序，同时在后台主动地在GPU和CPU内存之间传输KV缓存，并将I/O与计算重叠，以最大限度地减少请求抢占开销。

Result: 在多个GPU（RTX 4090，A6000，H200）上对Llama3-8B和Qwen2.5-32B进行的大量实验表明，TokenFlow实现了高达82.5%的有效吞吐量提升（考虑了实际用户消耗），同时将P99 TTFT降低了高达80.2%，而没有降低总体token吞吐量。

Conclusion: TokenFlow通过抢占式调度和主动KV缓存管理，显著提升了LLM服务的文本流式传输性能。

Abstract: Real-time LLM interactions demand streamed token generations, where text
tokens are progressively generated and delivered to users while balancing two
objectives: responsiveness (i.e., low time-to-first-token) and steady
generation (i.e.,required time-between-tokens). Standard LLM serving systems
suffer from the inflexibility caused by non-preemptive request scheduling and
reactive memory management, leading to poor resource utilization and low
request processing parallelism under request bursts. Therefore, we present
TokenFlow, a novel LLM serving system with enhanced text streaming performance
via preemptive request scheduling and proactive key-value (KV) cache
management. TokenFlow dynamically prioritizes requests based on real-time token
buffer occupancy and token consumption rate, while actively transferring KV
cache between GPU and CPU memory in the background and overlapping I/O with
computation to minimize request preemption overhead. Extensive experiments on
Llama3-8B and Qwen2.5-32B across multiple GPUs (RTX 4090, A6000, H200)
demonstrate that TokenFlow achieves up to 82.5% higher effective throughput
(accounting for actual user consumption) while reducing P99 TTFT by up to
80.2%, without degrading overall token throughput.

</details>


### [183] [Fusing Multi- and Hyperspectral Satellite Data for Harmful Algal Bloom Monitoring with Self-Supervised and Hierarchical Deep Learning](https://arxiv.org/abs/2510.02763)
*Nicholas LaHaye,Kelly M. Luis,Michelle M. Gierach*

Main category: cs.LG

TL;DR: 提出了一种自监督学习框架SIT-FUSE，用于检测和绘制有害藻华（HAB）的严重程度和物种，无需针对每个仪器进行标记的数据集。


<details>
  <summary>Details</summary>
Motivation: 在缺乏标签的环境中，扩展HAB监测，并通过分层嵌入实现探索性分析。

Method: 融合来自多个卫星仪器的反射率数据（VIIRS，MODIS，Sentinel-3，PACE）与TROPOMI太阳诱导荧光（SIF），采用自监督表征学习和分层深度聚类。

Result: 与来自墨西哥湾和南加州（2018-2025）的实地数据验证表明，结果与总浮游植物、短凯伦藻、亚历山大藻属和伪nitzschia属测量结果高度一致。

Conclusion: 该工作推进了在标签稀缺环境中可扩展的HAB监测，并为全球水生生物地球化学的自监督学习的实施奠定了基础。

Abstract: We present a self-supervised machine learning framework for detecting and
mapping harmful algal bloom (HAB) severity and speciation using multi-sensor
satellite data. By fusing reflectance data from operational instruments (VIIRS,
MODIS, Sentinel-3, PACE) with TROPOMI solar-induced fluorescence (SIF), our
framework, called SIT-FUSE, generates HAB severity and speciation products
without requiring per-instrument labeled datasets. The framework employs
self-supervised representation learning, hierarchical deep clustering to
segment phytoplankton concentrations and speciations into interpretable
classes, validated against in-situ data from the Gulf of Mexico and Southern
California (2018-2025). Results show strong agreement with total phytoplankton,
Karenia brevis, Alexandrium spp., and Pseudo-nitzschia spp. measurements. This
work advances scalable HAB monitoring in label-scarce environments while
enabling exploratory analysis via hierarchical embeddings: a critical step
toward operationalizing self-supervised learning for global aquatic
biogeochemistry.

</details>


### [184] [Curl Descent: Non-Gradient Learning Dynamics with Sign-Diverse Plasticity](https://arxiv.org/abs/2510.02765)
*Hugo Ninou,Jonathan Kadmon,N. Alex Cayco-Gajic*

Main category: cs.LG

TL;DR: 本文研究了生物神经网络是否使用类似于梯度下降的策略进行学习，发现学习动态可能包含非梯度“卷曲”成分，这些成分可能源于抑制-兴奋连接或赫布/反赫布可塑性。当卷曲项较小时，学习动态类似于梯度下降；当卷曲项超过临界值时，可能导致混沌学习动态或加速学习。


<details>
  <summary>Details</summary>
Motivation: 探讨生物神经网络是否使用梯度下降策略进行学习，以及非梯度成分对学习动态的影响。

Method: 通过在具有易于分析的student-teacher框架的前馈网络中引入非梯度动态，并分析其影响。

Result: 小的卷曲项保持了解的稳定性，而大的卷曲项可能导致混沌学习或加速学习。

Conclusion: 研究结果表明，某些架构可以通过不同的学习规则支持稳健的学习，为神经网络中基于梯度的学习的规范理论提供了一个重要的对立面。

Abstract: Gradient-based algorithms are a cornerstone of artificial neural network
training, yet it remains unclear whether biological neural networks use similar
gradient-based strategies during learning. Experiments often discover a
diversity of synaptic plasticity rules, but whether these amount to an
approximation to gradient descent is unclear. Here we investigate a previously
overlooked possibility: that learning dynamics may include fundamentally
non-gradient "curl"-like components while still being able to effectively
optimize a loss function. Curl terms naturally emerge in networks with
inhibitory-excitatory connectivity or Hebbian/anti-Hebbian plasticity,
resulting in learning dynamics that cannot be framed as gradient descent on any
objective. To investigate the impact of these curl terms, we analyze
feedforward networks within an analytically tractable student-teacher
framework, systematically introducing non-gradient dynamics through neurons
exhibiting rule-flipped plasticity. Small curl terms preserve the stability of
the original solution manifold, resulting in learning dynamics similar to
gradient descent. Beyond a critical value, strong curl terms destabilize the
solution manifold. Depending on the network architecture, this loss of
stability can lead to chaotic learning dynamics that destroy performance. In
other cases, the curl terms can counterintuitively speed learning compared to
gradient descent by allowing the weight dynamics to escape saddles by
temporarily ascending the loss. Our results identify specific architectures
capable of supporting robust learning via diverse learning rules, providing an
important counterpoint to normative theories of gradient-based learning in
neural networks.

</details>


### [185] [A Granular Study of Safety Pretraining under Model Abliteration](https://arxiv.org/abs/2510.02768)
*Shashank Agnihotri,Jonas Jakubassa,Priyam Dey,Sachin Goyal,Bernt Schiele,Venkatesh Babu Radhakrishnan,Margret Keuper*

Main category: cs.LG

TL;DR: 研究了通过激活编辑修改开放权重LLM的安全性问题，特别是模型删除技术对拒绝回答的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨常见的安全干预措施（如拒绝训练或元标签训练）是否能在推理时通过简单的激活编辑而继续有效。

Method: 使用一种轻量级的投影技术（模型删除）来移除拒绝敏感的方向，并在SmolLM2-1.7B的Safety Pretraining检查点序列以及广泛使用的开放基线上进行受控评估。对20个系统，原始的和删除的系统，发出100个提示，平衡有害和无害的情况，使用多个判断者将响应分类为“拒绝”或“非拒绝”，并在小的人工标记子集上验证判断者的保真度。同时，探测模型是否可以识别自己输出中的拒绝。

Result: 研究对哪些以数据为中心的安全组件在删除下保持稳健进行了检查点级别的表征，量化了判断者选择如何影响评估结果，并概述了一个将推理时编辑集成到安全评估中的实用协议。

Conclusion: 量化了模型删除技术对LLM安全性的影响，并为安全评估提供了实用方案。

Abstract: Open-weight LLMs can be modified at inference time with simple activation
edits, which raises a practical question for safety: do common safety
interventions like refusal training or metatag training survive such edits? We
study model abliteration, a lightweight projection technique designed to remove
refusal-sensitive directions, and conduct a controlled evaluation across a
granular sequence of Safety Pretraining checkpoints for SmolLM2-1.7B, alongside
widely used open baselines. For each of 20 systems, original and abliterated,
we issue 100 prompts with balanced harmful and harmless cases, classify
responses as **Refusal** or **Non-Refusal** using multiple judges, and validate
judge fidelity on a small human-labeled subset. We also probe whether models
can identify refusal in their own outputs. Our study produces a
checkpoint-level characterization of which data-centric safety components
remain robust under abliteration, quantifies how judge selection influences
evaluation outcomes, and outlines a practical protocol for integrating
inference-time edits into safety assessments. Code:
https://github.com/shashankskagnihotri/safety_pretraining.

</details>


### [186] [Optimal Rates for Generalization of Gradient Descent for Deep ReLU Classification](https://arxiv.org/abs/2510.02779)
*Yuanfan Li,Yunwen Lei,Zheng-Chu Guo,Yiming Ying*

Main category: cs.LG

TL;DR: 本文研究了深度ReLU网络中梯度下降(GD)的泛化性能，并建立了最优的泛化率。


<details>
  <summary>Details</summary>
Motivation: 现有研究要么产生次优的O(1/√n)速率，要么集中于具有平滑激活函数的网络，导致对网络深度L的指数依赖。

Method: 通过仔细权衡优化和泛化误差，实现了对深度的多项式依赖。

Result: 在数据是NTK可分的假设下，证明了超额风险率为$\widetilde{O}(L^4 (1 + \gamma L^2) / (n \gamma^2))$，这与最优SVM型速率$\widetilde{O}(1 / (n \gamma^2))$对齐。

Conclusion: 关键的技术贡献在于我们对参考模型附近激活模式的新颖控制，从而能够对用梯度下降训练的深度ReLU网络进行更清晰的Rademacher复杂度约束。

Abstract: Recent advances have significantly improved our understanding of the
generalization performance of gradient descent (GD) methods in deep neural
networks. A natural and fundamental question is whether GD can achieve
generalization rates comparable to the minimax optimal rates established in the
kernel setting. Existing results either yield suboptimal rates of
$O(1/\sqrt{n})$, or focus on networks with smooth activation functions,
incurring exponential dependence on network depth $L$. In this work, we
establish optimal generalization rates for GD with deep ReLU networks by
carefully trading off optimization and generalization errors, achieving only
polynomial dependence on depth. Specifically, under the assumption that the
data are NTK separable from the margin $\gamma$, we prove an excess risk rate
of $\widetilde{O}(L^4 (1 + \gamma L^2) / (n \gamma^2))$, which aligns with the
optimal SVM-type rate $\widetilde{O}(1 / (n \gamma^2))$ up to depth-dependent
factors. A key technical contribution is our novel control of activation
patterns near a reference model, enabling a sharper Rademacher complexity bound
for deep ReLU networks trained with gradient descent.

</details>


### [187] [OptunaHub: A Platform for Black-Box Optimization](https://arxiv.org/abs/2510.02798)
*Yoshihiko Ozaki,Shuhei Watanabe,Toshihiko Yanase*

Main category: cs.LG

TL;DR: OptunaHub 是一个集中化黑盒优化 (BBO) 方法和基准的社区平台。


<details>
  <summary>Details</summary>
Motivation: 为了整合不同领域中分散的 BBO 研究工作。

Method: OptunaHub 提供统一的 Python API、贡献者包注册表和一个 Web 界面。

Result: OptunaHub 旨在促进贡献和应用的良性循环。

Conclusion: OptunaHub 的源代码在 GitHub 上公开可用。

Abstract: Black-box optimization (BBO) drives advances in domains such as AutoML and
Materials Informatics, yet research efforts often remain fragmented across
domains. We introduce OptunaHub (https://hub.optuna.org/), a community platform
that centralizes BBO methods and benchmarks. OptunaHub provides unified Python
APIs, a contributor package registry, and a web interface to promote
searchability and cross-domain research. OptunaHub aims to foster a virtuous
cycle of contributions and applications. The source code is publicly available
in the optunahub, optunahub-registry, and optunahub-web repositories under the
Optuna organization on GitHub (https://github.com/optuna/).

</details>


### [188] [Relevance-Aware Thresholding in Online Conformal Prediction for Time Series](https://arxiv.org/abs/2510.02809)
*Théo Dupuy,Binbin Xu,Stéphane Perrey,Jacky Montmain,Abdelhak Imoussaten*

Main category: cs.LG

TL;DR: 本文提出了一种改进的在线共形预测(OCP)方法，旨在解决时间序列预测中数据分布随时间变化的问题，通过在阈值更新步骤中引入更广泛的函数来量化预测区间的相关性，从而避免突兀的阈值变化，并产生更窄的预测区间。


<details>
  <summary>Details</summary>
Motivation: 现有的OCP方法在阈值更新时主要关注预测区间的有效性，忽略了其相关性。本文旨在利用这一被忽视的方面。

Method: 本文提出通过使用更广泛的函数来量化预测区间的相关性，从而增强阈值更新步骤，防止阈值突变。

Result: 在真实数据集上的实验结果表明，与现有的OCP方法相比，本文提出的方法可以产生更紧密的区间，同时保持覆盖有效性。

Conclusion: 本文提出了一种改进的OCP方法，通过量化预测区间的相关性来提高预测性能，实验结果表明该方法能够产生更窄的预测区间，同时保持覆盖有效性。

Abstract: Uncertainty quantification has received considerable interest in recent works
in Machine Learning. In particular, Conformal Prediction (CP) gains ground in
this field. For the case of time series, Online Conformal Prediction (OCP)
becomes an option to address the problem of data distribution shift over time.
Indeed, the idea of OCP is to update a threshold of some quantity (whether the
miscoverage level or the quantile) based on the distribution observation. To
evaluate the performance of OCP methods, two key aspects are typically
considered: the coverage validity and the prediction interval width
minimization. Recently, new OCP methods have emerged, offering long-run
coverage guarantees and producing more informative intervals. However, during
the threshold update step, most of these methods focus solely on the validity
of the prediction intervals~--~that is, whether the ground truth falls inside
or outside the interval~--~without accounting for their relevance. In this
paper, we aim to leverage this overlooked aspect. Specifically, we propose
enhancing the threshold update step by replacing the binary evaluation
(inside/outside) with a broader class of functions that quantify the relevance
of the prediction interval using the ground truth. This approach helps prevent
abrupt threshold changes, potentially resulting in narrower prediction
intervals. Indeed, experimental results on real-world datasets suggest that
these functions can produce tighter intervals compared to existing OCP methods
while maintaining coverage validity.

</details>


### [189] [Dissecting Transformers: A CLEAR Perspective towards Green AI](https://arxiv.org/abs/2510.02810)
*Hemang Jain,Shailender Goyal,Divyansh Pandey,Karthik Vaidhyanathan*

Main category: cs.LG

TL;DR: LLM的广泛应用引起了环境问题，推理过程持续产生能源消耗。现有研究缺乏细粒度的测量方法，未能将能效作为主要目标。本研究提出了一个新方法CLEAR，对transformer架构的核心组件进行了细粒度的推理能源分析。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的快速普及引发了人们对环境的担忧。与一次性的训练成本不同，LLM推理在全球范围内持续发生，并且目前主导着AI的能源消耗。

Method: 我们提出了一种新的方法，即通过重复采样进行组件级能量评估（CLEAR），以克服微秒级组件执行和毫秒级能量传感器监测之间的时间不匹配问题。

Result: 我们的实证分析表明，注意力模块每个浮点操作（FLOP）消耗的能量明显更多，这表明能量消耗与FLOP计数不成比例。这表明FLOP无法单独捕获组件级别的真实能源成本。

Conclusion: 我们的研究结果建立了详细的组件级能量基线，并为通过组件级优化构建节能transformer模型提供了初步的见解。

Abstract: The rapid adoption of Large Language Models (LLMs) has raised significant
environmental concerns. Unlike the one-time cost of training, LLM inference
occurs continuously at a global scale and now dominates the AI energy
footprint. Yet, most sustainability studies report only coarse, model-level
metrics due to the lack of fine-grained measurement methods, treating energy
efficiency more as an afterthought than as a primary objective. We present the
first fine-grained empirical analysis of inference energy across core
components of transformer architecture. We propose a novel methodology,
Component-Level Energy Assessment via Repeated sampling (CLEAR), to overcome
temporal mismatch between microsecond scale component execution and monitoring
of millisecond (ms) scale energy sensors. Using CLEAR, we evaluate 15 models
spanning four distinct architecture types and consistently keep component-wise
energy variance below 9.5\% while capturing more than 90\% of the model's total
energy as individual components. Our empirical analysis reveals that Attention
blocks consume significantly more energy per floating-point operation (FLOP),
indicating that energy consumption is not proportionally aligned with FLOP
counts. This shows that FLOPs alone fail to capture the true energy cost at a
component level. Our findings establish detailed component-level energy
baselines and provide insight as an initial step to build energy-efficient
transformer models through component-level optimizations.

</details>


### [190] [Mitigating Spurious Correlation via Distributionally Robust Learning with Hierarchical Ambiguity Sets](https://arxiv.org/abs/2510.02818)
*Sung Ho Jo,Seonghwi Kim,Minwoo Chae*

Main category: cs.LG

TL;DR: 提出了一种分层Group DRO方法，以解决组间和组内分布偏移问题，并在模拟真实少数群体分布偏移的新基准设置下表现出强大的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统的监督学习方法容易受到虚假相关性的影响，特别是在测试数据中存在分布偏移的情况下。现有的Group DRO方法对亚群体或组偏移具有很强的鲁棒性，但仍然容易受到组内分布偏移的影响，这种情况在样本有限的少数群体中经常发生。

Method: 提出了一种分层扩展的Group DRO，可以解决组间和组内不确定性，从而在多个层面上提供对分布偏移的鲁棒性。同时，引入了新的基准设置，以模拟真实的少数群体分布偏移。

Result: 该方法在这些条件下表现出强大的鲁棒性，而现有的鲁棒学习方法在该条件下始终失效，同时在标准基准上也取得了优异的性能。

Conclusion: 这些结果强调了扩大模糊集以更好地捕获组间和组内分布不确定性的重要性。

Abstract: Conventional supervised learning methods are often vulnerable to spurious
correlations, particularly under distribution shifts in test data. To address
this issue, several approaches, most notably Group DRO, have been developed.
While these methods are highly robust to subpopulation or group shifts, they
remain vulnerable to intra-group distributional shifts, which frequently occur
in minority groups with limited samples. We propose a hierarchical extension of
Group DRO that addresses both inter-group and intra-group uncertainties,
providing robustness to distribution shifts at multiple levels. We also
introduce new benchmark settings that simulate realistic minority group
distribution shifts-an important yet previously underexplored challenge in
spurious correlation research. Our method demonstrates strong robustness under
these conditions-where existing robust learning methods consistently fail-while
also achieving superior performance on standard benchmarks. These results
highlight the importance of broadening the ambiguity set to better capture both
inter-group and intra-group distributional uncertainties.

</details>


### [191] [Online Learning in the Random Order Model](https://arxiv.org/abs/2510.02820)
*Martino Bernasconi,Andrea Celli,Riccardo Colini-Baldeschi,Federico Fusco,Stefano Leonardi,Matteo Russo*

Main category: cs.LG

TL;DR: 本文提出了一种通用的模板，用于将随机学习算法调整为随机顺序模型，而不会显着影响其后悔保证。


<details>
  <summary>Details</summary>
Motivation: 在在线学习的随机顺序模型中，损失序列由对手预先选择，并在随机排列后呈现给学习者。任何随机顺序输入都与随机 i.i.d. 输入渐近等效，但对于有限的时间，它可能表现出显着的非平稳性，这可能会阻碍随机学习算法的性能。虽然对抗输入的算法自然地保持其在随机顺序中的后悔保证，但随机模型存在简单的无后悔算法，这些算法在随机顺序实例中失败。

Method: 本文提出了一个通用模板，用于将随机学习算法调整为随机顺序模型，而不会显着影响其后悔保证。

Result: 这允许我们恢复改进的延迟预测、约束在线学习和具有切换成本的bandit的后悔界限。最后，我们研究了在线分类，并证明在随机顺序中，可学习性由VC维度而不是Littlestone维度来表征，从而提供了与一般对抗模型的进一步分离。

Conclusion: 在随机顺序模型中，针对随机学习算法的非平稳性问题，本文提出了一种通用的模板，该模板可以调整随机学习算法，同时保持其后悔保证。

Abstract: In the random-order model for online learning,
  the sequence of losses is chosen upfront by an adversary and presented to the
learner
  after a random permutation. Any random-order input is \emph{asymptotically}
equivalent to a stochastic i.i.d. one, but, for finite times, it may exhibit
significant {\em non-stationarity}, which can hinder the performance of
stochastic learning algorithms.
  While algorithms for adversarial inputs naturally maintain their regret
guarantees in random order, simple no-regret algorithms exist for the
stochastic model that fail against random-order instances.
  In this paper, we propose a general template to adapt stochastic learning
algorithms to the random-order model without substantially affecting their
regret guarantees. This allows us to recover improved regret bounds for
prediction with delays, online learning with constraints, and bandits with
switching costs. Finally, we investigate online classification and prove that,
in random order, learnability is characterized by the VC dimension rather than
the Littlestone dimension, thus providing a further separation from the general
adversarial model.

</details>


### [192] [FlexiQ: Adaptive Mixed-Precision Quantization for Latency/Accuracy Trade-Offs in Deep Neural Networks](https://arxiv.org/abs/2510.02822)
*Jaemin Kim,Hongjun Um,Sungkyun Kim,Yongjun Park,Jiwon Seo*

Main category: cs.LG

TL;DR: FlexiQ: Adaptive mixed-precision quantization for computer vision models that adjusts low-bitwidth channel ratio in real time.


<details>
  <summary>Details</summary>
Motivation: Hardware accelerators are costly and hard to scale for real-time workload fluctuations.

Method: Selectively applies low-bitwidth computation to feature channels with small value ranges and employs an efficient bit-lowering method to minimize quantization errors.

Result: Achieves on average 6.6% higher accuracy for 4-bit models and outperforms four state-of-the-art quantization techniques. The 50% 4-bit model incurs only 0.6% accuracy loss while achieving 40% speedup of the 100% 4-bit model over the 8-bit model.

Conclusion: FlexiQ introduces minimal runtime overhead and demonstrates hardware efficiency and overall performance benefits.

Abstract: Neural networks commonly execute on hardware accelerators such as NPUs and
GPUs for their size and computation overhead. These accelerators are costly and
it is hard to scale their resources to handle real-time workload fluctuations.
  We present FlexiQ, an adaptive mixed-precision quantization scheme for
computer vision models. FlexiQ selectively applies low-bitwidth computation to
feature channels with small value ranges and employs an efficient bit-lowering
method to minimize quantization errors while maintaining inference accuracy.
Furthermore, FlexiQ adjusts its low-bitwidth channel ratio in real time,
enabling quantized models to effectively manage fluctuating inference workload.
  We implemented FlexiQ prototype, including the mixed-precision inference
runtime on our custom NPU and GPUs. Evaluated on eleven convolution- and
transformer-based vision models, FlexiQ achieves on average 6.6% higher
accuracy for 4-bit models with finetuning and outperforms four state-of-the-art
quantization techniques. Moreover, our mixed-precision models achieved an
efficient accuracy-latency trade-off, with the 50% 4-bit model incurring only
0.6% accuracy loss while achieving 40% of the speedup of the 100% 4-bit model
over 8-bit model. Latency evaluations on our NPU and GPUs confirmed that FlexiQ
introduces minimal runtime overhead, demonstrating its hardware efficiency and
overall performance benefits.

</details>


### [193] [The Curious Case of In-Training Compression of State Space Models](https://arxiv.org/abs/2510.02823)
*Makram Chahine,Philipp Nazari,Daniela Rus,T. Konstantin Rusch*

Main category: cs.LG

TL;DR: 本文提出了一种在训练过程中压缩状态空间模型（SSM）的方法，以提高计算效率并保持模型表达能力。


<details>
  <summary>Details</summary>
Motivation: SSM在处理长序列建模任务时面临着表达能力和计算负担之间的权衡问题。

Method: 利用控制理论中的汉克尔奇异值分析，在训练过程中识别并保留高影响力的状态维度，从而对SSM进行压缩。

Result: 实验表明，在训练过程中进行压缩可以显著加速优化，同时保留表达能力。压缩后的模型能够保持任务关键结构，而直接在较小维度上训练的模型则会丢失这些结构。

Conclusion: SSM通过在训练开始时使用大模型并在训练过程中缩小模型尺寸，可以实现计算效率和高性能。

Abstract: State Space Models (SSMs), developed to tackle long sequence modeling tasks
efficiently, offer both parallelizable training and fast inference. At their
core are recurrent dynamical systems that maintain a hidden state, with update
costs scaling with the state dimension. A key design challenge is striking the
right balance between maximizing expressivity and limiting this computational
burden. Control theory, and more specifically Hankel singular value analysis,
provides a potent framework for the measure of energy for each state, as well
as the balanced truncation of the original system down to a smaller
representation with performance guarantees. Leveraging the eigenvalue stability
properties of Hankel matrices, we apply this lens to SSMs during training,
where only dimensions of high influence are identified and preserved. Our
approach applies to Linear Time-Invariant SSMs such as Linear Recurrent Units,
but is also extendable to selective models. Experiments show that in-training
reduction significantly accelerates optimization while preserving expressivity,
with compressed models retaining task-critical structure lost by models trained
directly at smaller dimension. In other words, SSMs that begin large and shrink
during training achieve computational efficiency while maintaining higher
performance.

</details>


### [194] [Multi-scale Autoregressive Models are Laplacian, Discrete, and Latent Diffusion Models in Disguise](https://arxiv.org/abs/2510.02826)
*Steve Hong,Samuel Belkadi*

Main category: cs.LG

TL;DR: 本文通过迭代细化框架重新审视了视觉自回归 (VAR) 模型，将其形式化为构建拉普拉斯式潜在金字塔的确定性前向过程，并结合学习到的反向过程，通过少量由粗到精的步骤重建金字塔。


<details>
  <summary>Details</summary>
Motivation: 将 VAR 连接到去噪扩散，并分离出三个设计选择，这有助于解释其效率和保真度：在学习的潜在空间中细化，将预测转换为代码索引上的离散分类，以及按空间频率划分任务。

Method: 通过受控实验来量化每个因素对保真度和速度的贡献，并概述了相同的框架如何扩展到置换不变图生成和概率式、集成式中程天气预报。

Result: 量化了每个因素对保真度和速度的贡献。

Conclusion: 该框架还为 VAR 提供了实用的接口，以利用扩散生态系统中的工具，同时保留少量步骤、规模并行生成。

Abstract: We revisit Visual Autoregressive (VAR) models through the lens of an
iterative-refinement framework. Rather than viewing VAR solely as next-scale
autoregression, we formalise it as a deterministic forward process that
constructs a Laplacian-style latent pyramid, paired with a learned backward
process that reconstructs it in a small number of coarse-to-fine steps. This
view connects VAR to denoising diffusion and isolates three design choices that
help explain its efficiency and fidelity: refining in a learned latent space,
casting prediction as discrete classification over code indices, and
partitioning the task by spatial frequency. We run controlled experiments to
quantify each factor's contribution to fidelity and speed, and we outline how
the same framework extends to permutation-invariant graph generation and to
probabilistic, ensemble-style medium-range weather forecasting. The framework
also suggests practical interfaces for VAR to leverage tools from the diffusion
ecosystem while retaining few-step, scale-parallel generation.

</details>


### [195] [Subject-Adaptive Sparse Linear Models for Interpretable Personalized Health Prediction from Multimodal Lifelog Data](https://arxiv.org/abs/2510.02835)
*Dohyun Bu,Jisoo Han,Soohwa Kwon,Yulim So,Jong-Seok Lee*

Main category: cs.LG

TL;DR: 提出了一种用于个性化健康预测的可解释模型Subject-Adaptive Sparse Linear (SASL)


<details>
  <summary>Details</summary>
Motivation: 现有的深度神经网络和梯度提升集成模型牺牲了解释性，并且未能充分解决生活日志数据中固有的显着的个体间差异。

Method: SASL集成了带有subject-specific interactions的普通最小二乘回归，系统地区分了全局和individual-level的影响。我们采用基于嵌套$F$-tests的迭代向后特征消除方法来构建稀疏且统计上稳健的模型。此外，考虑到健康结果通常代表连续过程的离散化版本，我们开发了一种回归-然后-阈值方法，专门用于最大化序数目标的宏平均F1分数。对于本质上具有挑战性的预测，SASL通过基于置信度的门控选择性地结合来自紧凑型LightGBM模型的输出，从而在不影响可解释性的前提下提高了准确性。

Result: 在CH-2025数据集上进行的评估表明，混合SASL-LightGBM框架实现了与复杂的黑盒方法相当的预测性能，但参数明显更少，透明度更高

Conclusion: SASL-LightGBM框架为临床医生和从业者提供了清晰且可操作的见解

Abstract: Improved prediction of personalized health outcomes -- such as sleep quality
and stress -- from multimodal lifelog data could have meaningful clinical and
practical implications. However, state-of-the-art models, primarily deep neural
networks and gradient-boosted ensembles, sacrifice interpretability and fail to
adequately address the significant inter-individual variability inherent in
lifelog data. To overcome these challenges, we propose the Subject-Adaptive
Sparse Linear (SASL) framework, an interpretable modeling approach explicitly
designed for personalized health prediction. SASL integrates ordinary least
squares regression with subject-specific interactions, systematically
distinguishing global from individual-level effects. We employ an iterative
backward feature elimination method based on nested $F$-tests to construct a
sparse and statistically robust model. Additionally, recognizing that health
outcomes often represent discretized versions of continuous processes, we
develop a regression-then-thresholding approach specifically designed to
maximize macro-averaged F1 scores for ordinal targets. For intrinsically
challenging predictions, SASL selectively incorporates outputs from compact
LightGBM models through confidence-based gating, enhancing accuracy without
compromising interpretability. Evaluations conducted on the CH-2025 dataset --
which comprises roughly 450 daily observations from ten subjects -- demonstrate
that the hybrid SASL-LightGBM framework achieves predictive performance
comparable to that of sophisticated black-box methods, but with significantly
fewer parameters and substantially greater transparency, thus providing clear
and actionable insights for clinicians and practitioners.

</details>


### [196] [Knowledge-Aware Modeling with Frequency Adaptive Learning for Battery Health Prognostics](https://arxiv.org/abs/2510.02839)
*Vijay Babu Pamshetti,Wei Zhang,Sumei Sun,Jie Zhang,Yonggang Wen,Qingyu Yan*

Main category: cs.LG

TL;DR: 提出了一种名为 Karma 的知识感知模型，该模型具有频率自适应学习能力，用于电池容量估计和剩余使用寿命预测。


<details>
  <summary>Details</summary>
Motivation: 现有的数据驱动模型捕获了时间退化特征，但通常缺乏知识指导，这导致了不可靠的长期健康预测。为了克服这些限制，我们提出了 Karma。

Method: 该模型首先执行信号分解以导出不同频段的电池信号。开发了一种双流深度学习架构，其中一个流捕获长期低频退化趋势，另一个流模拟高频短期动态。Karma 使用知识来调节预测，其中电池退化被建模为基于经验研究的双指数函数。我们的双流模型用于通过粒子滤波器优化知识参数，以确保物理上一致且可靠的预测和不确定性量化。

Result: 实验研究表明，Karma 具有卓越的性能，在两个主流数据集上，电池健康预测的平均误差分别比最先进的算法降低了 50.6% 和 32.6%。

Conclusion: 这些结果突出了 Karma 的稳健性、通用性和在各种应用中实现更安全、更可靠的电池管理的潜力。

Abstract: Battery health prognostics are critical for ensuring safety, efficiency, and
sustainability in modern energy systems. However, it has been challenging to
achieve accurate and robust prognostics due to complex battery degradation
behaviors with nonlinearity, noise, capacity regeneration, etc. Existing
data-driven models capture temporal degradation features but often lack
knowledge guidance, which leads to unreliable long-term health prognostics. To
overcome these limitations, we propose Karma, a knowledge-aware model with
frequency-adaptive learning for battery capacity estimation and remaining
useful life prediction. The model first performs signal decomposition to derive
battery signals in different frequency bands. A dual-stream deep learning
architecture is developed, where one stream captures long-term low-frequency
degradation trends and the other models high-frequency short-term dynamics.
Karma regulates the prognostics with knowledge, where battery degradation is
modeled as a double exponential function based on empirical studies. Our
dual-stream model is used to optimize the parameters of the knowledge with
particle filters to ensure physically consistent and reliable prognostics and
uncertainty quantification. Experimental study demonstrates Karma's superior
performance, achieving average error reductions of 50.6% and 32.6% over
state-of-the-art algorithms for battery health prediction on two mainstream
datasets, respectively. These results highlight Karma's robustness,
generalizability, and potential for safer and more reliable battery management
across diverse applications.

</details>


### [197] [RoiRL: Efficient, Self-Supervised Reasoning with Offline Iterative Reinforcement Learning](https://arxiv.org/abs/2510.02892)
*Aleksei Arzhantsev,Otmane Sakhi,Flavian Vasile*

Main category: cs.LG

TL;DR: RoiRL: Reasoning with offline iterative Reinforcement Learning, a family of lightweight offline learning alternatives that can target the same regularized optimal policies.


<details>
  <summary>Details</summary>
Motivation: Test-Time Reinforcement Learning (TTRL) removes this need by using majority-vote rewards, but relies on heavy online RL and incurs substantial computational cost.

Method: RoiRL eliminates the need to maintain a reference model and instead optimizes weighted log-likelihood objectives, enabling stable training with significantly lower memory and compute requirements.

Result: RoiRL trains to 2.5x faster and consistently outperforms TTRL on reasoning benchmarks, establishing a scalable path to self-improving LLMs without labels.

Conclusion: RoiRL establishes a scalable path to self-improving LLMs without labels, by training faster and outperforming TTRL.

Abstract: Reinforcement learning (RL) is central to improving reasoning in large
language models (LLMs) but typically requires ground-truth rewards. Test-Time
Reinforcement Learning (TTRL) removes this need by using majority-vote rewards,
but relies on heavy online RL and incurs substantial computational cost. We
propose RoiRL: Reasoning with offline iterative Reinforcement Learning, a
family of lightweight offline learning alternatives that can target the same
regularized optimal policies. Unlike TTRL, RoiRL eliminates the need to
maintain a reference model and instead optimizes weighted log-likelihood
objectives, enabling stable training with significantly lower memory and
compute requirements. Experimental results show that RoiRL trains to 2.5x
faster and consistently outperforms TTRL on reasoning benchmarks, establishing
a scalable path to self-improving LLMs without labels.

</details>
