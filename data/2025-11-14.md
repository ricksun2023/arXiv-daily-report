<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 12]
- [cs.CV](#cs.CV) [Total: 8]
- [cs.AI](#cs.AI) [Total: 9]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.LG](#cs.LG) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Omnilingual ASR: Open-Source Multilingual Speech Recognition for 1600+ Languages](https://arxiv.org/abs/2511.09690)
*Omnilingual ASR team,Gil Keren,Artyom Kozhevnikov,Yen Meng,Christophe Ropers,Matthew Setzler,Skyler Wang,Ife Adebara,Michael Auli,Can Balioglu,Kevin Chan,Chierh Cheng,Joe Chuang,Caley Droof,Mark Duppenthaler,Paul-Ambroise Duquenne,Alexander Erben,Cynthia Gao,Gabriel Mejia Gonzalez,Kehan Lyu,Sagar Miglani,Vineel Pratap,Kaushik Ram Sadagopan,Safiyyah Saleem,Arina Turkatenko,Albert Ventayol-Boada,Zheng-Xin Yong,Yu-An Chung,Jean Maillard,Rashel Moritz,Alexandre Mourachko,Mary Williamson,Shireen Yates*

Main category: cs.CL

TL;DR: Omnilingual ASR is a large-scale ASR system designed for extensibility, enabling communities to introduce unserved languages with only a few data samples.


<details>
  <summary>Details</summary>
Motivation: Expanding ASR coverage has been costly and limited, leaving thousands of long-tail languages behind. Ethical concerns also exist when pursued without community collaboration.

Method: It scales self-supervised pre-training to 7B parameters and introduces an encoder-decoder architecture designed for zero-shot generalization, leveraging a LLM-inspired decoder. It combines public resources with community-sourced recordings.

Result: Omnilingual ASR expands coverage to over 1,600 languages, including over 500 never before served by ASR, with substantial gains over prior systems, especially in low-resource conditions.

Conclusion: The paper reflects on the ethical considerations and discusses the societal impact, highlighting how open-sourcing models and tools can lower barriers for researchers and communities.

Abstract: Automatic speech recognition (ASR) has advanced in high-resource languages, but most of the world's 7,000+ languages remain unsupported, leaving thousands of long-tail languages behind. Expanding ASR coverage has been costly and limited by architectures that restrict language support, making extension inaccessible to most--all while entangled with ethical concerns when pursued without community collaboration. To transcend these limitations, we introduce Omnilingual ASR, the first large-scale ASR system designed for extensibility. Omnilingual ASR enables communities to introduce unserved languages with only a handful of data samples. It scales self-supervised pre-training to 7B parameters to learn robust speech representations and introduces an encoder-decoder architecture designed for zero-shot generalization, leveraging a LLM-inspired decoder. This capability is grounded in a massive and diverse training corpus; by combining breadth of coverage with linguistic variety, the model learns representations robust enough to adapt to unseen languages. Incorporating public resources with community-sourced recordings gathered through compensated local partnerships, Omnilingual ASR expands coverage to over 1,600 languages, the largest such effort to date--including over 500 never before served by ASR. Automatic evaluations show substantial gains over prior systems, especially in low-resource conditions, and strong generalization. We release Omnilingual ASR as a family of models, from 300M variants for low-power devices to 7B for maximum accuracy. We reflect on the ethical considerations shaping this design and conclude by discussing its societal impact. In particular, we highlight how open-sourcing models and tools can lower barriers for researchers and communities, inviting new forms of participation. Open-source artifacts are available at https://github.com/facebookresearch/omnilingual-asr.

</details>


### [2] [Order Matters: Rethinking Prompt Construction in In-Context Learning](https://arxiv.org/abs/2511.09700)
*Warren Li,Yiqian Wang,Zihan Wang,Jingbo Shang*

Main category: cs.CL

TL;DR: ICL中，示例顺序的影响与示例选择的影响相当，应重新审视ICL中的假设。


<details>
  <summary>Details</summary>
Motivation: 先前的工作认为示例选择比示例顺序更重要。本文对此假设进行了重新审视。

Method: 通过在分类和生成任务上进行受控实验，使用多个开源模型家族（0.5B到27B参数）和GPT-5。

Result: 不同示例顺序导致的性能差异与使用完全不同的示例集相当。仅使用开发集就可以识别强顺序，其性能接近于基于测试标签选择最佳顺序的oracle。

Conclusion: 示例选择和排序在提示设计中同等重要且相互关联，因此需要重新检查ICL中持有的假设。

Abstract: In-context learning (ICL) enables large language models to perform new tasks by conditioning on a sequence of examples. Most prior work reasonably and intuitively assumes that which examples are chosen has a far greater effect on performance than how those examples are ordered, leading to a focus on example selection. We revisit this assumption and conduct a systematic comparison between the effect of selection and ordering. Through controlled experiments on both classification and generation tasks, using multiple open-source model families (0.5B to 27B parameters) and GPT-5, we find that the variance in performance due to different example orderings is comparable to that from using entirely different example sets. Furthermore, we show that strong orderings can be identified using only a development set, achieving performance close to an oracle that selects the best ordering based on test labels. Our findings highlight the equal and intertwined importance of example selection and ordering in prompt design, calling for a reexamination of the assumptions held in ICL.

</details>


### [3] [Contextual morphologically-guided tokenization for Latin encoder models](https://arxiv.org/abs/2511.09709)
*Marisa Hudspeth,Patrick J. Burns,Brendan O'Connor*

Main category: cs.CL

TL;DR: 本文探讨了针对拉丁语的形态感知分词方法，旨在提升语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的分词方法通常注重信息论目标，而忽略了语言学目标，对于形态丰富的语言来说，分词质量直接影响下游任务的表现。本文研究拉丁语的分词问题，拉丁语是一种形态丰富的语言，拥有丰富的词汇资源。

Method: 本文 исследует 针对拉丁语的形态感知分词方法。

Result: 形态引导的分词提高了四个下游任务的整体性能，尤其是在领域外文本中，突显了模型更好的泛化能力。

Conclusion: 对于缺乏大规模预训练数据的低资源语言，开发和整合语言资源可以作为提高语言模型性能的可行替代方案。

Abstract: Tokenization is a critical component of language model pretraining, yet standard tokenization methods often prioritize information-theoretical goals like high compression and low fertility rather than linguistic goals like morphological alignment. In fact, they have been shown to be suboptimal for morphologically rich languages, where tokenization quality directly impacts downstream performance. In this work, we investigate morphologically-aware tokenization for Latin, a morphologically rich language that is medium-resource in terms of pretraining data, but high-resource in terms of curated lexical resources -- a distinction that is often overlooked but critical in discussions of low-resource language modeling. We find that morphologically-guided tokenization improves overall performance on four downstream tasks. Performance gains are most pronounced for out of domain texts, highlighting our models' improved generalization ability. Our findings demonstrate the utility of linguistic resources to improve language modeling for morphologically complex languages. For low-resource languages that lack large-scale pretraining data, the development and incorporation of linguistic resources can serve as a feasible alternative to improve LM performance.

</details>


### [4] [Assessing the Applicability of Natural Language Processing to Traditional Social Science Methodology: A Case Study in Identifying Strategic Signaling Patterns in Presidential Directives](https://arxiv.org/abs/2511.09738)
*C. LeMay,A. Lane,J. Seales,M. Winstead,S. Baty*

Main category: cs.CL

TL;DR: 本研究探讨了自然语言处理（NLP）如何用于从大量书面数据中提取主题，应用于识别里根至克林顿政府总统指令（PDs）中的信号主题。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索NLP在分析大量书面语料库中的应用潜力，特别是在识别总统指令中的主题方面。

Method: 通过NLP技术和人工分析两种方法识别相关文件，并比较结果。

Result: NLP和人工分析都能识别相关文件，但也存在差异，表明需要更多研究来评估NLP在此用例中的有效性。

Conclusion: 研究结果表明，快速发展的AIML领域意味着现有工具已经改进，并且已经开发出新工具； 这项研究展示了潜在的过时AI工具在新兴社会科学应用中的固有能力。

Abstract: Our research investigates how Natural Language Processing (NLP) can be used to extract main topics from a larger corpus of written data, as applied to the case of identifying signaling themes in Presidential Directives (PDs) from the Reagan through Clinton administrations. Analysts and NLP both identified relevant documents, demonstrating the potential utility of NLPs in research involving large written corpuses. However, we also identified discrepancies between NLP and human-labeled results that indicate a need for more research to assess the validity of NLP in this use case. The research was conducted in 2023, and the rapidly evolving landscape of AIML means existing tools have improved and new tools have been developed; this research displays the inherent capabilities of a potentially dated AI tool in emerging social science applications.

</details>


### [5] [How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation](https://arxiv.org/abs/2511.09748)
*Muskaan Chopra,Lorenz Sparrenberg,Sarthak Khanna,Rafet Sifa*

Main category: cs.CL

TL;DR: 该论文研究了在保证检测翻译错误的同时，如何尽可能缩小大型语言模型（LLM）的规模和成本，以便在边缘设备和隐私敏感的工作流程中部署。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在评估机器翻译方面表现出色，但其规模和成本限制了它们在边缘设备和隐私敏感的工作流程中的部署。

Method: 该研究针对英语到德语的关键错误检测，对20亿参数以下的模型进行了基准测试，采用标准化的提示、轻量级的logit-bias校准和多数投票方法，并报告了语义质量和计算指标。

Result: 结果表明，大约10亿参数的模型具有明显的优势：Gemma-3-1B在质量和效率之间取得了最佳平衡，在SynCED-EnDe-2025上达到了MCC=0.77，F1-ERR=0.98，同时在MacBook Pro M4 Pro上保持了400毫秒的单样本延迟。

Conclusion: 紧凑的、指令调整的LLM，通过轻量级的校准和小样本监督进行增强，可以为机器翻译提供可信赖的、设备上的CED，从而在实际翻译管道中实现私有的、低成本的错误筛选。

Abstract: Large Language Models (LLMs) excel at evaluating machine translation (MT), but their scale and cost hinder deployment on edge devices and in privacy-sensitive workflows. We ask: how small can you get while still detecting meaning-altering translation errors? Focusing on English->German Critical Error Detection (CED), we benchmark sub-2B models (LFM2-350M, Qwen-3-0.6B/1.7B, Llama-3.2-1B-Instruct, Gemma-3-1B) across WMT21, WMT22, and SynCED-EnDe-2025. Our framework standardizes prompts, applies lightweight logit-bias calibration and majority voting, and reports both semantic quality (MCC, F1-ERR/F1-NOT) and compute metrics (VRAM, latency, throughput). Results reveal a clear sweet spot around one billion parameters: Gemma-3-1B provides the best quality-efficiency trade-off, reaching MCC=0.77 with F1-ERR=0.98 on SynCED-EnDe-2025 after merged-weights fine-tuning, while maintaining 400 ms single-sample latency on a MacBook Pro M4 Pro (24 GB). At larger scale, Qwen-3-1.7B attains the highest absolute MCC (+0.11 over Gemma) but with higher compute cost. In contrast, ultra-small models (0.6B) remain usable with few-shot calibration yet under-detect entity and number errors. Overall, compact, instruction-tuned LLMs augmented with lightweight calibration and small-sample supervision can deliver trustworthy, on-device CED for MT, enabling private, low-cost error screening in real-world translation pipelines. All datasets, prompts, and scripts are publicly available at our GitHub repository.

</details>


### [6] [Predicate-Argument Structure Divergences in Chinese and English Parallel Sentences and their Impact on Language Transfer](https://arxiv.org/abs/2511.09796)
*Rocco Tripodi,Xiaoyu Liu*

Main category: cs.CL

TL;DR: 跨语言NLP通过迁移语言知识来解决低资源问题，但语言差异阻碍了迁移，尤其是在类型学上差异很大的语言之间。


<details>
  <summary>Details</summary>
Motivation: 探索汉英平行句中谓词-论元结构的对齐和不对齐现象。

Method: 分析谓词注释的对齐和不对齐，检查异同，并提出结构差异的分类。通过注释投影实验的结果进行定性和定量分析来支持分析和分类。

Result: 语言迁移是不对称的。

Conclusion: 在迁移学习应用中选择源语言时，需要注意语言迁移的不对称性，并且在提出关于跨语言NLP的任何科学主张之前，都需要对其进行调查。

Abstract: Cross-lingual Natural Language Processing (NLP) has gained significant traction in recent years, offering practical solutions in low-resource settings by transferring linguistic knowledge from resource-rich to low-resource languages. This field leverages techniques like annotation projection and model transfer for language adaptation, supported by multilingual pre-trained language models. However, linguistic divergences hinder language transfer, especially among typologically distant languages. In this paper, we present an analysis of predicate-argument structures in parallel Chinese and English sentences. We explore the alignment and misalignment of predicate annotations, inspecting similarities and differences and proposing a categorization of structural divergences. The analysis and the categorization are supported by a qualitative and quantitative analysis of the results of an annotation projection experiment, in which, in turn, one of the two languages has been used as source language to project annotations into the corresponding parallel sentences. The results of this analysis show clearly that language transfer is asymmetric. An aspect that requires attention when it comes to selecting the source language in transfer learning applications and that needs to be investigated before any scientific claim about cross-lingual NLP is proposed.

</details>


### [7] [Text2SQL-Flow: A Robust SQL-Aware Data Augmentation Framework for Text-to-SQL](https://arxiv.org/abs/2511.10192)
*Qifeng Cai,Hao Liang,Chang Xu,Tao Xie,Wentao Zhang,Bin Cui*

Main category: cs.CL

TL;DR: 这篇论文提出了一种名为Text2SQL-Flow的SQL-aware数据增强框架，用于从少量种子数据生成大规模、语义有效、结构多样的Text-to-SQL对，从而解决Text-to-SQL任务中数据集稀缺、简单和低多样性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的Text-to-SQL数据集稀缺、简单和低多样性，限制了AI的性能。

Method: 该框架通过六个增强维度运行，并集成了一个端到端管道，包括SQL执行验证、自然语言问题生成、思维链推理追踪和数据分类。此外，还引入了一个模块化数据库管理器，以确保跨数据库兼容性和可扩展性。

Result: 使用该框架构建了一个高质量的数据集SQLFlow，包含89,544个带注释的示例。在开源LLM上进行微调以及在闭源LLM上使用masked alignment retrieval方法进行评估，结果表明SQLFlow数据集和所提出的检索策略均优于现有方法。

Conclusion: 该研究建立了一个可扩展的、以数据为中心的Text-to-SQL系统基础，并强调了高质量结构化数据在现代AI中的关键作用。

Abstract: The data-centric paradigm has become pivotal in AI, especially for Text-to-SQL, where performance is limited by scarce, simplistic, and low-diversity datasets. To address this, we propose Text2SQL-Flow, a SQL-aware data augmentation framework that generates large-scale, semantically valid, and structurally diverse Text-to-SQL pairs from minimal seed data. It operates across six augmentation dimensions and integrates an end-to-end pipeline featuring SQL execution verification, natural language question generation, chain-of-thought reasoning traces, and data classification. A modular Database Manager ensures cross-database compatibility and scalability. Using this framework, we build SQLFlow, a high-quality dataset of 89,544 annotated examples. We evaluate SQLFlow in two settings: (1) For open-source LLMs, fine-tuning on SQLFlow consistently improves performance across benchmarks under the same data budget. (2) For closed-source LLMs, we introduce a masked alignment retrieval method that treats SQLFlow as both knowledge base and training data for the retriever. This enables structure-aware example matching by modeling fine-grained alignments between questions and SQL queries. Experiments show our retrieval strategy outperforms existing methods, underscoring the value of SQLFlow's high-fidelity data and our novel technique. Our work establishes a scalable, data-centric foundation for advancing Text-to-SQL systems and highlights the critical role of high-quality structured data in modern AI.

</details>


### [8] [TARG: Training-Free Adaptive Retrieval Gating for Efficient RAG](https://arxiv.org/abs/2511.09803)
*Yufeng Wang,Lu wei,Haibin Ling*

Main category: cs.CL

TL;DR: 提出了一种名为 TARG 的免训练自适应检索门控策略，该策略决定何时检索，仅使用来自基本模型的简短、无上下文的草稿。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成 (RAG) 提高了事实性，但为每个查询进行检索通常会损害质量，同时增加 tokens 和延迟。

Method: TARG 仅使用来自基本模型的简短、无上下文的草稿来决定何时检索。从草稿的前缀 logits 中，TARG 计算轻量级不确定性分数：平均 token 熵、通过单调链接从前 1/top-2 logit 差距导出的边际信号，或少量随机前缀上的小 N 方差，并且仅当分数超过阈值时才触发检索。

Result: 在 NQ-Open、TriviaQA 和 PopQA 上，与 Always-RAG 相比，TARG 在匹配或提高 EM/F1 的同时，将检索减少了 70-90%，并减少了端到端延迟，并且在开销方面仍然接近 Never-RAG。

Conclusion: 在现代指令调整的法学硕士下，边际信号是一个强大的默认值（熵随着骨干锐化而压缩），小 N 方差提供了一个保守的、预算优先的替代方案。

Abstract: Retrieval-Augmented Generation (RAG) improves factuality but retrieving for every query often hurts quality while inflating tokens and latency. We propose Training-free Adaptive Retrieval Gating (TARG), a single-shot policy that decides when to retrieve using only a short, no-context draft from the base model. From the draft's prefix logits, TARG computes lightweight uncertainty scores: mean token entropy, a margin signal derived from the top-1/top-2 logit gap via a monotone link, or small-N variance across a handful of stochastic prefixes, and triggers retrieval only when the score exceeds a threshold. The gate is model agnostic, adds only tens to hundreds of draft tokens, and requires no additional training or auxiliary heads. On NQ-Open, TriviaQA, and PopQA, TARG consistently shifts the accuracy-efficiency frontier: compared with Always-RAG, TARG matches or improves EM/F1 while reducing retrieval by 70-90% and cutting end-to-end latency, and it remains close to Never-RAG in overhead. A central empirical finding is that under modern instruction-tuned LLMs the margin signal is a robust default (entropy compresses as backbones sharpen), with small-N variance offering a conservative, budget-first alternative. We provide ablations over gate type and prefix length and use a delta-latency view to make budget trade-offs explicit.

</details>


### [9] [Khmer Spellchecking: A Holistic Approach](https://arxiv.org/abs/2511.09812)
*Marry Kong,Rina Buoy,Sovisal Chenda,Nguonly Taing*

Main category: cs.CL

TL;DR: 本文提出了一种针对高棉语拼写检查的整体方法，该方法集成了高棉语子词分割、命名实体识别、字形到音素的转换以及语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有的解决方案未能充分解决高棉语拼写检查中的挑战，例如词汇和分词模型之间的不匹配、单词的不同形式、复合词的自由组合以及缺乏命名实体识别模型。

Method: 该方法集成了高棉语子词分割、命名实体识别、字形到音素的转换以及高棉语语言模型，以识别潜在的更正候选并对最合适的候选进行排序。

Result: 实验结果表明，与现有解决方案相比，该方法实现了高达 94.4% 的最先进的高棉语拼写检查准确率。

Conclusion: 该研究为高棉语拼写检查和命名实体识别任务提供了基准数据集，并将公开提供。

Abstract: Compared to English and other high-resource languages, spellchecking for Khmer remains an unresolved problem due to several challenges. First, there are misalignments between words in the lexicon and the word segmentation model. Second, a Khmer word can be written in different forms. Third, Khmer compound words are often loosely and easily formed, and these compound words are not always found in the lexicon. Fourth, some proper nouns may be flagged as misspellings due to the absence of a Khmer named-entity recognition (NER) model. Unfortunately, existing solutions do not adequately address these challenges. This paper proposes a holistic approach to the Khmer spellchecking problem by integrating Khmer subword segmentation, Khmer NER, Khmer grapheme-to-phoneme (G2P) conversion, and a Khmer language model to tackle these challenges, identify potential correction candidates, and rank the most suitable candidate. Experimental results show that the proposed approach achieves a state-of-the-art Khmer spellchecking accuracy of up to 94.4%, compared to existing solutions. The benchmark datasets for Khmer spellchecking and NER tasks in this study will be made publicly available.

</details>


### [10] [Improving Graduate Outcomes by Identifying Skills Gaps and Recommending Courses Based on Career Interests](https://arxiv.org/abs/2511.09819)
*Rahul Soni,Basem Suleiman,Sonit Singh*

Main category: cs.CL

TL;DR: 本研究旨在设计和开发一个课程推荐系统，以应对为学生选择相关课程的挑战。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机是弥合大学学习和行业期望之间的差距，帮助学生选择与行业趋势和要求相符的课程。

Method: 该系统采用数据挖掘和协同过滤技术，结合机器学习方法、用户偏好和学术标准，构建了一个全面的算法框架。

Result: 该系统通过迭代原型设计和用户输入修订，开发了一个易于使用的前端界面，并根据用户反馈进行了优化。

Conclusion: 该课程推荐系统可以帮助学生、教师和职业顾问，促进终身学习和职业发展，并帮助大学生做出数据驱动和行业 informed 的课程决策，从而提高大学毕业生的成果。

Abstract: This paper aims to address the challenge of selecting relevant courses for students by proposing the design and development of a course recommendation system. The course recommendation system utilises a combination of data analytics techniques and machine learning algorithms to recommend courses that align with current industry trends and requirements. In order to provide customised suggestions, the study entails the design and implementation of an extensive algorithmic framework that combines machine learning methods, user preferences, and academic criteria. The system employs data mining and collaborative filtering techniques to examine past courses and individual career goals in order to provide course recommendations. Moreover, to improve the accessibility and usefulness of the recommendation system, special attention is given to the development of an easy-to-use front-end interface. The front-end design prioritises visual clarity, interaction, and simplicity through iterative prototyping and user input revisions, guaranteeing a smooth and captivating user experience. We refined and optimised the proposed system by incorporating user feedback, ensuring that it effectively meets the needs and preferences of its target users. The proposed course recommendation system could be a useful tool for students, instructors, and career advisers to use in promoting lifelong learning and professional progression as it fills the gap between university learning and industry expectations. We hope that the proposed course recommendation system will help university students in making data-drive and industry-informed course decisions, in turn, improving graduate outcomes for the university sector.

</details>


### [11] [Answering Students' Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM](https://arxiv.org/abs/2511.09831)
*Neo Wang,Sonit Singh*

Main category: cs.CL

TL;DR: 本文提出了一种基于大型语言模型和检索增强生成(RAG)的问答系统，以解决课程论坛中学生问题无法及时解答以及重复性问题过多的问题。


<details>
  <summary>Details</summary>
Motivation: 随着课程注册学生人数的增加，学生的问题不能立即得到回答，教师面临大量重复性问题。

Method: 该方法基于大型语言模型，并采用检索增强生成(RAG)方法。使用开源大型语言模型，并在相关课程数据集上进行微调。为了进一步提高性能，使用本地知识库，并应用RAG方法检索与学生问题相关的文档。为了缓解大型语言模型的幻觉问题，集成了多链思维推理。

Result: 在HotpotQA数据集上对微调的LLM与RAG方法进行了实验。实验结果表明，微调的LLM与RAG方法在问答任务上具有很强的性能。

Conclusion: 本文表明，基于大型语言模型和检索增强生成(RAG)的问答系统在课程论坛中具有很强的应用前景。

Abstract: The course forums are increasingly significant and play vital role in facilitating student discussions and answering their questions related to the course. It provides a platform for students to post their questions related to the content and admin issues related to the course. However, there are several challenges due to the increase in the number of students enrolled in the course. The primary challenge is that students' queries cannot be responded immediately and the instructors have to face lots of repetitive questions. To mitigate these issues, we propose a question answering system based on large language model with retrieval augmented generation (RAG) method. This work focuses on designing a question answering system with open source Large Language Model (LLM) and fine-tuning it on the relevant course dataset. To further improve the performance, we use a local knowledge base and applied RAG method to retrieve relevant documents relevant to students' queries, where the local knowledge base contains all the course content. To mitigate the hallucination of LLMs, We also integrate it with multi chain-of-thought reasoning to overcome the challenge of hallucination in LLMs. In this work, we experiment fine-tuned LLM with RAG method on the HotpotQA dataset. The experimental results demonstrate that the fine-tuned LLM with RAG method has a strong performance on question answering task.

</details>


### [12] [TermGPT: Multi-Level Contrastive Fine-Tuning for Terminology Adaptation in Legal and Financial Domain](https://arxiv.org/abs/2511.09854)
*Yidan Sun,Mengying Zhu,Feiyue Chen,Yangyang Wu,Xiaolei Dan,Mengyuan Yang,Xiaolin Zheng,Shenglin Ben*

Main category: cs.CL

TL;DR: 大型语言模型在文本生成任务中表现出色，但其嵌入空间存在各向同性问题，导致领域术语区分度差。本文提出 TermGPT，一个多层次对比微调框架，用于术语适应，从而解决这个问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在法律和金融等领域的术语区分度较差，严重阻碍了法律判决预测或金融风险分析等下游任务。

Method: 构建句子图以捕获语义和结构关系，并基于上下文和拓扑线索生成语义一致但具有区分性的正负样本。设计了一种句子和token级别的多层次对比学习方法，以增强全局上下文理解和细粒度的术语区分。

Result: TermGPT 在金融和法律领域的术语区分任务中优于现有的基线模型。

Conclusion: TermGPT 有效地提高了大型语言模型在特定领域术语区分任务中的性能。

Abstract: Large language models (LLMs) have demonstrated impressive performance in text generation tasks; however, their embedding spaces often suffer from the isotropy problem, resulting in poor discrimination of domain-specific terminology, particularly in legal and financial contexts. This weakness in terminology-level representation can severely hinder downstream tasks such as legal judgment prediction or financial risk analysis, where subtle semantic distinctions are critical. To address this problem, we propose TermGPT, a multi-level contrastive fine-tuning framework designed for terminology adaptation. We first construct a sentence graph to capture semantic and structural relations, and generate semantically consistent yet discriminative positive and negative samples based on contextual and topological cues. We then devise a multi-level contrastive learning approach at both the sentence and token levels, enhancing global contextual understanding and fine-grained terminology discrimination. To support robust evaluation, we construct the first financial terminology dataset derived from official regulatory documents. Experiments show that TermGPT outperforms existing baselines in term discrimination tasks within the finance and legal domains.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [13] [FedeCouple: Fine-Grained Balancing of Global-Generalization and Local-Adaptability in Federated Learning](https://arxiv.org/abs/2511.09599)
*Ming Yang,Dongrun Li,Xin Wang,Feng Li,Lisheng Fan,Chunxiao Wang,Xiaoming Wu,Peng Cheng*

Main category: cs.CV

TL;DR: 提出了一种名为FedeCouple的联邦学习方法，它在细粒度级别上平衡了全局泛化和局部适应性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注局部训练期间的特征空间一致性和分类个性化，通常忽略了提取器的局部适应性和分类器的全局泛化。

Method: 联合学习全局和局部特征表示，同时采用动态知识蒸馏来增强个性化分类器的泛化能力。引入锚点以细化特征空间。

Result: 在五个图像分类数据集上进行的大量实验表明，FedeCouple在有效性、稳定性、可扩展性和安全性方面始终优于九种基线方法。在评估有效性的实验中，FedeCouple比最佳基线高出 4.3%。

Conclusion: FedeCouple在非凸目标下收敛，并且随着通信轮数的增加，迭代接近静止点。

Abstract: In privacy-preserving mobile network transmission scenarios with heterogeneous client data, personalized federated learning methods that decouple feature extractors and classifiers have demonstrated notable advantages in enhancing learning capability. However, many existing approaches primarily focus on feature space consistency and classification personalization during local training, often neglecting the local adaptability of the extractor and the global generalization of the classifier. This oversight results in insufficient coordination and weak coupling between the components, ultimately degrading the overall model performance. To address this challenge, we propose FedeCouple, a federated learning method that balances global generalization and local adaptability at a fine-grained level. Our approach jointly learns global and local feature representations while employing dynamic knowledge distillation to enhance the generalization of personalized classifiers. We further introduce anchors to refine the feature space; their strict locality and non-transmission inherently preserve privacy and reduce communication overhead. Furthermore, we provide a theoretical analysis proving that FedeCouple converges for nonconvex objectives, with iterates approaching a stationary point as the number of communication rounds increases. Extensive experiments conducted on five image-classification datasets demonstrate that FedeCouple consistently outperforms nine baseline methods in effectiveness, stability, scalability, and security. Notably, in experiments evaluating effectiveness, FedeCouple surpasses the best baseline by a significant margin of 4.3%.

</details>


### [14] [MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation](https://arxiv.org/abs/2511.09611)
*Ye Tian,Ling Yang,Jiongfan Yang,Anran Wang,Yu Tian,Jiani Zheng,Haochen Wang,Zhiyang Teng,Zhuochen Wang,Yinjie Wang,Yunhai Tong,Mengdi Wang,Xiangtai Li*

Main category: cs.CV

TL;DR: 现有的自回归方法在复杂任务中由于误差传播可能导致性能下降。论文提出了 ParaBench 基准来评估文本和图像输出，发现性能下降与生成的推理和最终图像之间的对齐不良有关。因此，论文提出了一个并行的多模态扩散框架 MMaDA-Parallel，通过文本和图像之间的持续交互来解决这个问题，并通过 Parallel Reinforcement Learning (ParaRL) 进一步优化，以提高跨模态一致性。实验表明，该模型显著提高了跨模态对齐和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的序列自回归方法在复杂任务中由于误差传播可能导致性能下降。

Method: 提出了一个并行的多模态扩散框架 MMaDA-Parallel，通过文本和图像之间的持续交互，并使用 Parallel Reinforcement Learning (ParaRL) 进行优化。

Result: MMaDA-Parallel 在 ParaBench 上实现了 6.9% 的 Output Alignment 提升，优于现有最佳模型 Bagel。

Conclusion: MMaDA-Parallel 建立了一个更强大的思维感知图像合成范例，显著提高了跨模态对齐和语义一致性。

Abstract: While thinking-aware generation aims to improve performance on complex tasks, we identify a critical failure mode where existing sequential, autoregressive approaches can paradoxically degrade performance due to error propagation. To systematically analyze this issue, we propose ParaBench, a new benchmark designed to evaluate both text and image output modalities. Our analysis using ParaBench reveals that this performance degradation is strongly correlated with poor alignment between the generated reasoning and the final image. To resolve this, we propose a parallel multimodal diffusion framework, MMaDA-Parallel, that enables continuous, bidirectional interaction between text and images throughout the entire denoising trajectory. MMaDA-Parallel is trained with supervised finetuning and then further optimized by Parallel Reinforcement Learning (ParaRL), a novel strategy that applies semantic rewards along the trajectory to enforce cross-modal consistency. Experiments validate that our model significantly improves cross-modal alignment and semantic consistency, achieving a 6.9\% improvement in Output Alignment on ParaBench compared to the state-of-the-art model, Bagel, establishing a more robust paradigm for thinking-aware image synthesis. Our code is open-sourced at https://github.com/tyfeld/MMaDA-Parallel

</details>


### [15] [PriVi: Towards A General-Purpose Video Model For Primate Behavior In The Wild](https://arxiv.org/abs/2511.09675)
*Felix B. Mueller,Jan F. Meier,Timo Lueddecke,Richard Vogg,Roger L. Freixanet,Valentin Hassler,Tiffany Bosshard,Elif Karakoc,William J. O'Hearn,Sofia M. Pereira,Sandro Sehner,Kaja Wierucka,Judith Burkart,Claudia Fichtel,Julia Fischer,Alexander Gail,Catherine Hobaiter,Julia Ostner,Liran Samuni,Oliver Schülke,Neda Shahidi,Erin G. Wessling,Alexander S. Ecker*

Main category: cs.CV

TL;DR: 提出了PriVi，一个大规模的以灵长类动物为中心的视频预训练数据集，用于提升计算机视觉在灵长类动物行为分析中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于以人为中心的预训练模型，并且专注于单个数据集，限制了泛化能力。

Method: 构建了一个包含424小时视频的大规模数据集PriVi，并使用V-JEPA进行预训练，学习特定于灵长类动物的表征。

Result: 在四个基准数据集上，该方法始终优于现有方法，包括完全微调的基线，并且能以更少的标签进行扩展。

Conclusion: 以灵长类动物为中心的预训练可以显著提高数据效率和泛化能力，使其成为低标签应用的一种有前景的方法。

Abstract: Non-human primates are our closest living relatives, and analyzing their behavior is central to research in cognition, evolution, and conservation. Computer vision could greatly aid this research, but existing methods often rely on human-centric pretrained models and focus on single datasets, which limits generalization. We address this limitation by shifting from a model-centric to a data-centric approach and introduce PriVi, a large-scale primate-centric video pretraining dataset. PriVi contains 424 hours of curated video, combining 174 hours from behavioral research across 11 settings with 250 hours of diverse web-sourced footage, assembled through a scalable data curation pipeline. We pretrain V-JEPA on PriVi to learn primate-specific representations and evaluate it using a lightweight frozen classifier. Across four benchmark datasets, ChimpACT, BaboonLand, PanAf500, and ChimpBehave, our approach consistently outperforms prior work, including fully finetuned baselines, and scales favorably with fewer labels. These results demonstrate that primate-centric pretraining substantially improves data efficiency and generalization, making it a promising approach for low-label applications. Code, models, and the majority of the dataset will be made available.

</details>


### [16] [Classifying Phonotrauma Severity from Vocal Fold Images with Soft Ordinal Regression](https://arxiv.org/abs/2511.09702)
*Katie Matton,Purvaja Balaji,Hamzeh Ghasemzadeh,Jameson C. Cooper,Daryush D. Mehta,Jarrad H. Van Stan,Robert E. Hillman,Rosalind Picard,John Guttag,S. Mazdak Abulnaga*

Main category: cs.CV

TL;DR: 本文提出了一种从声带图像自动分类声带损伤程度的方法。


<details>
  <summary>Details</summary>
Motivation: 临床医生对声带损伤程度的评估成本高，可靠性差异大。

Method: 该方法采用了一种广泛使用的序数回归框架，并针对标签不确定性，提出了一种新的序数回归损失函数修改方法，使其能够对反映注释者评分分布的软标签进行操作。

Result: 所提出的软序数回归方法实现了接近临床专家的预测性能，同时产生了良好校准的不确定性估计。

Conclusion: 通过提供一种自动化的声带损伤程度评估工具，该研究可以实现对声带损伤的大规模研究，最终提高临床理解和患者护理水平。

Abstract: Phonotrauma refers to vocal fold tissue damage resulting from exposure to forces during voicing. It occurs on a continuum from mild to severe, and treatment options can vary based on severity. Assessment of severity involves a clinician's expert judgment, which is costly and can vary widely in reliability. In this work, we present the first method for automatically classifying phonotrauma severity from vocal fold images. To account for the ordinal nature of the labels, we adopt a widely used ordinal regression framework. To account for label uncertainty, we propose a novel modification to ordinal regression loss functions that enables them to operate on soft labels reflecting annotator rating distributions. Our proposed soft ordinal regression method achieves predictive performance approaching that of clinical experts, while producing well-calibrated uncertainty estimates. By providing an automated tool for phonotrauma severity assessment, our work can enable large-scale studies of phonotrauma, ultimately leading to improved clinical understanding and patient care.

</details>


### [17] [SliderEdit: Continuous Image Editing with Fine-Grained Instruction Control](https://arxiv.org/abs/2511.09715)
*Arman Zarei,Samyadeep Basu,Mobina Pournemat,Sayan Nag,Ryan Rossi,Soheil Feizi*

Main category: cs.CV

TL;DR: SliderEdit是一个用于连续图像编辑的框架，它可以对每个指令的强度进行平滑调整，从而实现细粒度的、可解释的指令控制。


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的图像编辑模型对每个指令都采用固定强度，限制了用户精确控制编辑强度的能力。

Method: SliderEdit通过解耦单个指令，并将每个指令作为一个全局训练的滑块，允许平滑调整其强度。该方法学习一个低秩适应矩阵集，该矩阵集可以推广到不同的编辑、属性和组合指令。

Result: SliderEdit在编辑可控性、视觉一致性和用户可操纵性方面都有显著提高。

Conclusion: SliderEdit为交互式的、指令驱动的图像操作铺平了道路，具有连续的和组合的控制。

Abstract: Instruction-based image editing models have recently achieved impressive performance, enabling complex edits to an input image from a multi-instruction prompt. However, these models apply each instruction in the prompt with a fixed strength, limiting the user's ability to precisely and continuously control the intensity of individual edits. We introduce SliderEdit, a framework for continuous image editing with fine-grained, interpretable instruction control. Given a multi-part edit instruction, SliderEdit disentangles the individual instructions and exposes each as a globally trained slider, allowing smooth adjustment of its strength. Unlike prior works that introduced slider-based attribute controls in text-to-image generation, typically requiring separate training or fine-tuning for each attribute or concept, our method learns a single set of low-rank adaptation matrices that generalize across diverse edits, attributes, and compositional instructions. This enables continuous interpolation along individual edit dimensions while preserving both spatial locality and global semantic consistency. We apply SliderEdit to state-of-the-art image editing models, including FLUX-Kontext and Qwen-Image-Edit, and observe substantial improvements in edit controllability, visual consistency, and user steerability. To the best of our knowledge, we are the first to explore and propose a framework for continuous, fine-grained instruction control in instruction-based image editing models. Our results pave the way for interactive, instruction-driven image manipulation with continuous and compositional control.

</details>


### [18] [Density Estimation and Crowd Counting](https://arxiv.org/abs/2511.09723)
*Balachandra Devarangadi Sunil,Rakshith Venkatesh,Shantanu Todmal*

Main category: cs.CV

TL;DR: 本文提出了一种改进的基于视频的 crowd density estimation 算法，通过结合降噪概率模型、窄高斯核、回归分支和事件驱动采样技术来提高准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决视频 crowd density estimation 中的时间挑战，提高在公共安全、灾难响应和事件管理等应用中的实时 crowd monitoring 的可扩展性和效率。

Method: 1. 将图像 crowd density estimation 算法调整为视频场景；2. 集成利用扩散过程生成高质量 crowd density maps 的降噪概率模型；3. 采用窄高斯核并生成多个 density map 输出；4. 引入回归分支以实现精确的特征提取；5. 基于相似性分数合并这些 maps 以产生稳健的最终结果；6. 引入事件驱动采样技术，利用 Farneback 光流算法选择性地捕获显示显着 crowd movements 的帧，从而减少计算负荷和存储。

Result: 该模型通过定性和定量评估（包括覆盖图和平均绝对误差 (MAE)）证明了其在密集和稀疏设置中有效捕获 crowd dynamics 的能力。采样方法的效率得到进一步评估，展示了其在保持基本 crowd events 的同时减少帧数的能力。

Conclusion: 该研究提供了一个可扩展且高效的框架，用于在公共安全、灾难响应和事件管理等应用中进行实时 crowd monitoring。

Abstract: This study enhances a crowd density estimation algorithm originally designed for image-based analysis by adapting it for video-based scenarios. The proposed method integrates a denoising probabilistic model that utilizes diffusion processes to generate high-quality crowd density maps. To improve accuracy, narrow Gaussian kernels are employed, and multiple density map outputs are generated. A regression branch is incorporated into the model for precise feature extraction, while a consolidation mechanism combines these maps based on similarity scores to produce a robust final result. An event-driven sampling technique, utilizing the Farneback optical flow algorithm, is introduced to selectively capture frames showing significant crowd movements, reducing computational load and storage by focusing on critical crowd dynamics. Through qualitative and quantitative evaluations, including overlay plots and Mean Absolute Error (MAE), the model demonstrates its ability to effectively capture crowd dynamics in both dense and sparse settings. The efficiency of the sampling method is further assessed, showcasing its capability to decrease frame counts while maintaining essential crowd events. By addressing the temporal challenges unique to video analysis, this work offers a scalable and efficient framework for real-time crowd monitoring in applications such as public safety, disaster response, and event management.

</details>


### [19] [PALMS+: Modular Image-Based Floor Plan Localization Leveraging Depth Foundation Model](https://arxiv.org/abs/2511.09724)
*Yunqian Cheng,Benjamin Princen,Roberto Manduchi*

Main category: cs.CV

TL;DR: PALMS++: An improved indoor localization system using RGB images and a foundation monocular depth estimation model.


<details>
  <summary>Details</summary>
Motivation: Vision-based indoor localization methods are limited by short range and ambiguity. Overcome the limitations of existing vision-based indoor localization methods in GPS-denied environments.

Method: Reconstructs scale-aligned 3D point clouds from posed RGB images using Depth Pro, followed by geometric layout matching via convolution with the floor plan.

Result: Outperforms PALMS and F3Loc in stationary localization accuracy and achieves lower localization errors in sequential localization.

Conclusion: Demonstrates robustness for camera-free tracking and its potential for infrastructure-free applications.

Abstract: Indoor localization in GPS-denied environments is crucial for applications like emergency response and assistive navigation. Vision-based methods such as PALMS enable infrastructure-free localization using only a floor plan and a stationary scan, but are limited by the short range of smartphone LiDAR and ambiguity in indoor layouts. We propose PALMS$+$, a modular, image-based system that addresses these challenges by reconstructing scale-aligned 3D point clouds from posed RGB images using a foundation monocular depth estimation model (Depth Pro), followed by geometric layout matching via convolution with the floor plan. PALMS$+$ outputs a posterior over the location and orientation, usable for direct or sequential localization. Evaluated on the Structured3D and a custom campus dataset consisting of 80 observations across four large campus buildings, PALMS$+$ outperforms PALMS and F3Loc in stationary localization accuracy -- without requiring any training. Furthermore, when integrated with a particle filter for sequential localization on 33 real-world trajectories, PALMS$+$ achieved lower localization errors compared to other methods, demonstrating robustness for camera-free tracking and its potential for infrastructure-free applications. Code and data are available at https://github.com/Head-inthe-Cloud/PALMS-Plane-based-Accessible-Indoor-Localization-Using-Mobile-Smartphones

</details>


### [20] [Social LSTM with Dynamic Occupancy Modeling for Realistic Pedestrian Trajectory Prediction](https://arxiv.org/abs/2511.09735)
*Ahmed Alia,Mohcine Chraibi,Armin Seyfried*

Main category: cs.CV

TL;DR: 提出了一种新的深度学习模型，通过引入动态占据空间损失函数来改进 Social LSTM，从而在避免碰撞的同时提高轨迹预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的行人轨迹预测模型忽略了行人的物理空间占用，导致在拥挤环境中预测不准确。

Method: 提出了一个带有动态占据空间损失函数的 Social LSTM 模型，该损失函数结合了平均位移误差和一个对场景密度和个体空间占用敏感的碰撞惩罚项。

Result: 在里昂灯光节 2022 期间记录的真实行人轨迹生成的五个数据集上进行实验，结果表明该模型降低了碰撞率，并提高了位移预测的准确性。与基线相比，碰撞率降低了 31%，平均位移误差和最终位移误差分别降低了 5% 和 6%。

Conclusion: 该模型在大多数测试集中始终优于几种最先进的深度学习模型。

Abstract: In dynamic and crowded environments, realistic pedestrian trajectory prediction remains a challenging task due to the complex nature of human motion and the mutual influences among individuals. Deep learning models have recently achieved promising results by implicitly learning such patterns from 2D trajectory data. However, most approaches treat pedestrians as point entities, ignoring the physical space that each person occupies. To address these limitations, this paper proposes a novel deep learning model that enhances the Social LSTM with a new Dynamic Occupied Space loss function. This loss function guides Social LSTM in learning to avoid realistic collisions without increasing displacement error across different crowd densities, ranging from low to high, in both homogeneous and heterogeneous density settings. Such a function achieves this by combining the average displacement error with a new collision penalty that is sensitive to scene density and individual spatial occupancy. For efficient training and evaluation, five datasets were generated from real pedestrian trajectories recorded during the Festival of Lights in Lyon 2022. Four datasets represent homogeneous crowd conditions -- low, medium, high, and very high density -- while the fifth corresponds to a heterogeneous density distribution. The experimental findings indicate that the proposed model not only lowers collision rates but also enhances displacement prediction accuracy in each dataset. Specifically, the model achieves up to a 31% reduction in the collision rate and reduces the average displacement error and the final displacement error by 5% and 6%, respectively, on average across all datasets compared to the baseline. Moreover, the proposed model consistently outperforms several state-of-the-art deep learning models across most test sets.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [21] [An Efficient and Almost Optimal Solver for the Joint Routing-Assignment Problem via Partial JRA and Large-α Optimization](https://arxiv.org/abs/2511.09563)
*Qilong Yuan*

Main category: cs.AI

TL;DR: 提出了一种新的高效方法，用于解决大规模联合路由分配（JRA）问题，该方法通过部分路径重构（PPR）求解器和全局Large-α约束，实现了接近最优的解决方案，并在benchmark数据集上取得了0.00%的平均偏差。


<details>
  <summary>Details</summary>
Motivation: 先前研究的精确混合整数规划（MIP）求解器在解决大规模JRA问题时计算效率低下，启发式方法虽然能快速得到解，但与最优解存在偏差。

Method: 提出了一种部分路径重构（PPR）求解器，该求解器首先识别关键的项目-占位符对以形成简化的子问题，然后有效地求解该子问题以优化全局解决方案。此外，还将全局Large-α约束集成到JRA模型中，以进一步提高解决方案的最优性。

Result: 在n=300、500和1000的benchmark数据集上的实验评估表明，该方法始终如一地提供几乎最优的解决方案，实现了与ground truth的平均偏差为0.00%，同时保持了较高的计算效率。

Conclusion: 所提出的框架和方法不仅可以应用于JRA问题，而且在TSP和相关优化问题中也具有强大的应用潜力。

Abstract: The Joint Routing-Assignment (JRA) optimization problem simultaneously determines the assignment of items to placeholders and a Hamiltonian cycle that visits each node pair exactly once, with the objective of minimizing total travel cost. Previous studies introduced an exact mixed-integer programming (MIP) solver, along with datasets and a Gurobi implementation, showing that while the exact approach guarantees optimality, it becomes computationally inefficient for large-scale instances. To overcome this limitation, heuristic methods based on merging algorithms and shaking procedures were proposed, achieving solutions within approximately 1% deviation from the optimum. This work presents a novel and more efficient approach that attains high-accuracy, near-optimal solutions for large-scale JRA problems. The proposed method introduces a Partial Path Reconstructon (PPR) solver that first identifies key item-placeholder pairs to form a reduced subproblem, which is solved efficiently to refine the global solution. Using this PJAR framework, the initial heuristic merging solutions can be further improved, reducing the deviation by half. Moreover, the solution can be iteratively polished with PPR based solver along the optimization path to yield highly accurate tours. Additionally, a global Large-α constraint is incorporated into the JRA model to further enhance solution optimality. Experimental evaluations on benchmark datasets with n = 300, 500, and 1000 demonstrate that the proposed method consistently delivers almost optimal solutions, achieving an average deviation of 0.00% from the ground truth while maintaining high computational efficiency. Beyond the JRA problem, the proposed framework and methodologies exhibit strong potential for broader applications. The Framework can be applied to TSP and related optimization problems.

</details>


### [22] [Variable Neighborhood Search for the Electric Vehicle Routing Problem](https://arxiv.org/abs/2511.09570)
*David Woller,Viktor Kozák,Miroslav Kulich,Libor Přeučil*

Main category: cs.AI

TL;DR: 本文介绍了CEC-12竞赛中获胜的电动汽车路径问题（EVRP）解决方案，该方案基于可变邻域搜索（VNS）元启发式算法。


<details>
  <summary>Details</summary>
Motivation: 由于文献中考虑的约束条件多种多样，因此比较不同问题变体的方案仍然具有挑战性。关注 इलेक्ट्रिक汽车路径问题 (EVRP) 的一个简化变体，称为有容量的绿色车辆路径问题 (CGVRP)。

Method: 基于可变邻域搜索 (VNS) 元启发式算法。

Result: 该方法在完整的竞赛数据集上取得了最好的结果，并且也优于之后发布的更新的算法。

Conclusion: 本文提出了一种基于VNS元启发式算法的电动汽车路径问题解决方案，并在竞赛中取得了优异的成绩。

Abstract: The Electric Vehicle Routing Problem (EVRP) extends the classical Vehicle Routing Problem (VRP) to reflect the growing use of electric and hybrid vehicles in logistics. Due to the variety of constraints considered in the literature, comparing approaches across different problem variants remains challenging. A minimalistic variant of the EVRP, known as the Capacitated Green Vehicle Routing Problem (CGVRP), was the focus of the CEC-12 competition held during the 2020 IEEE World Congress on Computational Intelligence. This paper presents the competition-winning approach, based on the Variable Neighborhood Search (VNS) metaheuristic. The method achieves the best results on the full competition dataset and also outperforms a more recent algorithm published afterward.

</details>


### [23] [Fixed-Persona SLMs with Modular Memory: Scalable NPC Dialogue on Consumer Hardware](https://arxiv.org/abs/2511.10277)
*Martin Braas,Lukas Esterle*

Main category: cs.AI

TL;DR: 提出了一种模块化的NPC对话系统，利用小型语言模型(slm)进行微调，以编码特定的NPC角色，并与运行时可交换的记忆模块集成。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(llm)在生成类人文本方面表现出了卓越的能力，但其在计算机游戏中对话系统中的适用性仍然有限。这种限制来自于它们大量的硬件需求、延迟约束以及在游戏环境中保持清晰定义的知识边界的必要性。

Method: 该方法利用小型语言模型(slm)进行微调，以编码特定的NPC角色，并与运行时可交换的记忆模块集成。这些记忆模块保存特定于角色的会话上下文和世界知识，从而实现表达性交互和长期记忆，而无需在游戏过程中进行再训练或模型重新加载。

Result: 使用三个开源SLM对系统进行了综合评估:DistilGPT-2、TinyLlama-1.1B-Chat和Mistral-7B-Instruct，这些模型在合成的角色对齐数据上进行训练，并在消费级硬件上进行基准测试。

Conclusion: 虽然该方法是由游戏中的应用所驱动的，但其模块化设计和角色驱动的记忆架构在需要表达性、可伸缩性和记忆丰富的会话代理的领域中具有更广泛的应用潜力，如虚拟助手、客户支持机器人或交互式教育系统。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in generating human-like text, yet their applicability to dialogue systems in computer games remains limited. This limitation arises from their substantial hardware requirements, latency constraints, and the necessity to maintain clearly defined knowledge boundaries within a game setting. In this paper, we propose a modular NPC dialogue system that leverages Small Language Models (SLMs), fine-tuned to encode specific NPC personas and integrated with runtime-swappable memory modules. These memory modules preserve character-specific conversational context and world knowledge, enabling expressive interactions and long-term memory without retraining or model reloading during gameplay. We comprehensively evaluate our system using three open-source SLMs: DistilGPT-2, TinyLlama-1.1B-Chat, and Mistral-7B-Instruct, trained on synthetic persona-aligned data and benchmarked on consumer-grade hardware. While our approach is motivated by applications in gaming, its modular design and persona-driven memory architecture hold significant potential for broader adoption in domains requiring expressive, scalable, and memory-rich conversational agents, such as virtual assistants, customer support bots, or interactive educational systems.

</details>


### [24] [SynthTools: A Framework for Scaling Synthetic Tools for Agent Development](https://arxiv.org/abs/2511.09572)
*Tommaso Castellani,Naimeng Ye,Daksh Mittal,Thomson Yen,Hongseok Namkoong*

Main category: cs.AI

TL;DR: SynthTools是一个用于生成合成工具生态系统的灵活且可扩展的框架，它解决了真实世界API在可用性、领域覆盖和稳定性方面的局限性，从而促进了工具使用代理的大规模训练和稳定评估。


<details>
  <summary>Details</summary>
Motivation: 为了解决真实世界API在可用性、领域覆盖和稳定性方面的局限性，这些局限性使得它们不适合稳定评估或可扩展训练。

Method: 该框架包含三个核心组件：工具生成（用于自动和可扩展地创建多样化的工具）、工具模拟（用于模拟真实的工具行为）和工具审计（用于确保工具模拟的正确性和一致性）。

Result: SynthTools可以轻松生成工具集，其跨越的领域数量和每个领域的工具数量是先前工作的两倍。工具模拟和工具审计组件表现出很强的可靠性，分别达到94%和99%的准确率。由生成的工具构建的下游任务，即使是最先进的模型也很难完成。

Conclusion: SynthTools通过启用可扩展、多样化和可靠的工具生态系统，为工具使用代理的大规模训练和稳定评估提供了一条可行的途径。

Abstract: AI agents increasingly rely on external tools to solve complex, long-horizon tasks. Advancing such agents requires reproducible evaluation and large-scale training in controllable, diverse, and realistic tool-use environments. However, real-world APIs are limited in availability, domain coverage, and stability, often requiring access keys and imposing rate limits, which render them impractical for stable evaluation or scalable training. To address these challenges, we introduce SynthTools, a flexible and scalable framework for generating synthetic tool ecosystems. Our framework consists of three core components: Tool Generation for automatic and scalable creation of diverse tools, Tool Simulation to emulate realistic tool behaviors, and Tool Audit to ensure correctness and consistency of tool simulation. To illustrate its scalability, we show that SynthTools can readily produce toolsets that span twice as many domains and twice as many tools per domain as prior work. Furthermore, the tool simulation and tool audit components demonstrate strong reliability, achieving $94\%$ and $99\%$ accuracy respectively. Finally, we construct downstream tasks from the generated tools that even state-of-the-art models struggle to complete. By enabling scalable, diverse, and reliable tool ecosystems, SynthTools provides a practical path toward large-scale training and stable evaluation of tool-use agents. Our code is available at https://github.com/namkoong-lab/SynthTools.

</details>


### [25] [Proceedings of the Second International Workshop on Next-Generation Language Models for Knowledge Representation and Reasoning (NeLaMKRR 2025)](https://arxiv.org/abs/2511.09575)
*Ha-Thanh Nguyen,Ken Satoh,Francesca Toni,Randy Goebel,Kostas Stathis*

Main category: cs.AI

TL;DR: 探讨大型语言模型是否具备推理能力，并探讨如何将逻辑推理融入这些模型。


<details>
  <summary>Details</summary>
Motivation: 评估和提升语言模型的推理能力，使其在需要精确性和可靠性的领域更有用。

Method: 通过研讨会形式，聚集不同学科和AI视角的研究人员，共同探索。

Result: 目标是分析语言模型的推理能力，将知识表示的推理能力注入语言模型，并形式化语言模型执行的推理。

Conclusion: 旨在整合语言模型的知识和推理能力，提高其应用性和实用性。

Abstract: Reasoning is an essential component of human intelligence in that it plays a fundamental role in our ability to think critically, support responsible decisions, and solve challenging problems. Traditionally, AI has addressed reasoning in the context of logic-based representations of knowledge. However, the recent leap forward in natural language processing, with the emergence of language models based on transformers, is hinting at the possibility that these models exhibit reasoning abilities, particularly as they grow in size and are trained on more and more data. Still, despite ongoing discussions about what reasoning is in language models, it is still not easy to articulate to what extent these models are actually capable of reasoning.
  The goal of this workshop is to create a platform for researchers from different disciplines and/or AI perspectives to explore approaches and techniques with the aim to reconcile reasoning between language models using transformers and logic-based representations. The specific objectives include analysing the reasoning abilities of language models measured alongside KR methods, injecting KR-style reasoning abilities into language models (including by neuro-symbolic means), and formalising the kind of reasoning language models carry out. This exploration aims to uncover how language models can effectively integrate and leverage knowledge and reasoning with it, thus improving their application and utility in areas where precision and reliability are key requirements.

</details>


### [26] [Cogent argument extensions are weakly admissible but not vice versa](https://arxiv.org/abs/2511.09600)
*Gustavo Bodanza*

Main category: cs.AI

TL;DR: 本文研究了两种非容许论证框架语义之间的关系：coogent语义和弱容许语义。证明了coogent扩展是弱容许的，反之则不然。


<details>
  <summary>Details</summary>
Motivation: 研究coogent语义和弱容许语义之间的关系。

Method: 通过证明coogent扩展是弱容许的，但反之则不然。

Result: 证明了coogent扩展是弱容许的，反之则不然。

Conclusion: Coogent扩展是弱容许的，但反之则不然。

Abstract: In this research note, we show the relationship between two non-admissible argumentation framework semantics: cogent and weakly admissible semantics. We prove that, while cogent extensions are weakly admissible, the converse is not true.

</details>


### [27] [Rebellion: Noise-Robust Reasoning Training for Audio Reasoning Models](https://arxiv.org/abs/2511.09682)
*Tiansheng Huang,Virat Shejwalkar,Oscar Chang,Milad Nasr,Ling Liu*

Main category: cs.AI

TL;DR: 本文研究了音频推理模型（ARMs）的安全性，发现标准推理训练（RT）无法防御高级对抗攻击。为此，提出了一种鲁棒的推理训练方法Rebellion，通过训练ARMs以应对最坏情况下的表征漂移，从而提高模型在对抗攻击下的安全性和在良性任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对音频推理模型（ARMs）对抗恶意攻击安全性的研究。现有标准推理训练方法无法防御高级对抗攻击。

Method: 提出一种鲁棒的推理训练方法Rebellion，训练ARMs以应对最坏情况下的表征漂移。

Result: Rebellion方法在不影响良性任务性能的前提下，防御了高级音频对抗攻击，并显著提高了准确性-安全性权衡。

Conclusion: Rebellion方法能够有效提高音频推理模型（ARMs）的安全性，使其免受高级对抗攻击。

Abstract: Instilling reasoning capabilities in large models (LMs) using reasoning training (RT) significantly improves LMs' performances. Thus Audio Reasoning Models (ARMs), i.e., audio LMs that can reason, are becoming increasingly popular. However, no work has studied the safety of ARMs against jailbreak attacks that aim to elicit harmful responses from target models. To this end, first, we show that standard RT with appropriate safety reasoning data can protect ARMs from vanilla audio jailbreaks, but cannot protect them against our proposed simple yet effective jailbreaks. We show that this is because of the significant representation drift between vanilla and advanced jailbreaks which forces the target ARMs to emit harmful responses. Based on this observation, we propose Rebellion, a robust RT that trains ARMs to be robust to the worst-case representation drift. All our results are on Qwen2-Audio; they demonstrate that Rebellion: 1) can protect against advanced audio jailbreaks without compromising performance on benign tasks, and 2) significantly improves accuracy-safety trade-off over standard RT method.

</details>


### [28] [Echoing: Identity Failures when LLM Agents Talk to Each Other](https://arxiv.org/abs/2511.09710)
*Sarath Shekkizhar,Romain Cosentino,Adam Earle,Silvio Savarese*

Main category: cs.AI

TL;DR: 大型语言模型（LLM）驱动的agent在自主交互时，会出现单agent性能无法预测的新型故障：agent间对话中的行为漂移（AxA）。其中一种故障是“回声”，即agent放弃其角色并镜像对话伙伴。


<details>
  <summary>Details</summary>
Motivation: 探讨在agent间对话（AxA）中，由于缺乏人类的引导，agent会偏离预定目标，产生“回声”现象。

Method: 通过在60个AxA配置、3个领域和2000多次对话中进行实验，研究了“回声”现象。

Result: 实验表明，在三大LLM提供商中均存在“回声”现象，发生率从5%到70%不等，并且即使在高级推理模型中也很普遍（32.8%），增加推理努力也无法减少。分析表明，“回声”现象随着交互的进行而增长（实验中超过7轮），并非仅仅是提示不佳。

Conclusion: 通过引入协议级别的缓解措施，有针对性地使用结构化响应，可以将“回声”现象降低到9%。

Abstract: As large language model (LLM) based agents interact autonomously with one another, a new class of failures emerges that cannot be predicted from single agent performance: behavioral drifts in agent-agent conversations (AxA). Unlike human-agent interactions, where humans ground and steer conversations, AxA lacks such stabilizing signals, making these failures unique. We investigate one such failure, echoing, where agents abandon their assigned roles and instead mirror their conversational partners, undermining their intended objectives. Through experiments across $60$ AxA configurations, $3$ domains, and $2000+$ conversations, we demonstrate that echoing occurs across three major LLM providers, with echoing rates from $5\%$ to $70\%$ depending on the model and domain. Moreover, we find that echoing is persistent even in advanced reasoning models with substantial rates ($32.8\%$) that are not reduced by increased reasoning efforts. We analyze prompt impacts, conversation dynamics, showing that echoing arises as interaction grows longer ($7+$ turns in experiments) and is not merely an artifact of sub-optimal prompting. Finally, we introduce a protocol-level mitigation in which targeted use of structured responses reduces echoing to $9\%$.

</details>


### [29] [ProbLog4Fairness: A Neurosymbolic Approach to Modeling and Mitigating Bias](https://arxiv.org/abs/2511.09768)
*Rik Adriaensen,Lucas Van Praet,Jessa Bekker,Robin Manhaeve,Pieter Delobelle,Maarten Buyl*

Main category: cs.AI

TL;DR: 这篇论文提出了一种新的算法偏差缓解框架，通过将偏差假设形式化为 ProbLog 程序，并将其集成到神经网络的训练过程中，从而灵活地对特定任务中的偏差进行建模和缓解。


<details>
  <summary>Details</summary>
Motivation: 在实践中，公平的定义难以操作，因为多个定义可能不兼容。直接描述算法偏差可能更容易，例如，基于对其背景中的系统偏差的背景信息。这些假设可以用来在训练期间减轻这种偏差。然而，目前缺乏一个同时具有原则性、灵活性和可解释性的框架来整合这些假设。

Method: 该方法将偏差假设形式化为 ProbLog 中的程序，这是一种概率逻辑编程语言，允许通过逻辑描述概率因果关系。Problog 的神经符号扩展允许轻松地将这些假设集成到神经网络的训练过程中。论文提出了一组模板来表达不同类型的偏差。

Result: 该方法在具有已知偏差的合成表格数据集上展示了其通用性。使用对存在的偏差失真的估计，该方法还成功地减轻了现实世界表格和图像数据中的算法偏差。

Conclusion: ProbLog4Fairness 优于基线，因为它能够灵活地对相关的偏差假设进行建模，而其他方法通常坚持固定的偏差类型或公平概念。

Abstract: Operationalizing definitions of fairness is difficult in practice, as multiple definitions can be incompatible while each being arguably desirable. Instead, it may be easier to directly describe algorithmic bias through ad-hoc assumptions specific to a particular real-world task, e.g., based on background information on systemic biases in its context. Such assumptions can, in turn, be used to mitigate this bias during training. Yet, a framework for incorporating such assumptions that is simultaneously principled, flexible, and interpretable is currently lacking.
  Our approach is to formalize bias assumptions as programs in ProbLog, a probabilistic logic programming language that allows for the description of probabilistic causal relationships through logic. Neurosymbolic extensions of ProbLog then allow for easy integration of these assumptions in a neural network's training process. We propose a set of templates to express different types of bias and show the versatility of our approach on synthetic tabular datasets with known biases. Using estimates of the bias distortions present, we also succeed in mitigating algorithmic bias in real-world tabular and image data. We conclude that ProbLog4Fairness outperforms baselines due to its ability to flexibly model the relevant bias assumptions, where other methods typically uphold a fixed bias type or notion of fairness.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [30] [Dolphin: An Actor-Oriented Database for Reactive Moving Object Data Management](https://arxiv.org/abs/2511.10063)
*Yiwen Wang,Vivek Shah,Marcos Antonio Vaz Salles,Claudia Bauzer Medeiros,Julio Cesar Dos Reis,Yongluan Zhou*

Main category: cs.DB

TL;DR: 本文提出了一种新的方法，通过移动actor抽象来增强基于actor的分布式框架，使其具有反应功能、复杂的空间数据管理和并发语义，从而支持新的反应式移动对象应用。


<details>
  <summary>Details</summary>
Motivation: 现有的移动对象场景研究通常将反应行为留给最终用户复杂实现，无法满足低延迟计算和可扩展性的要求。

Method: 提出移动actor抽象，增强actor模型，使其具有反应感知、移动和空间查询能力。定义了一个反应式移动对象数据管理平台M-AODBs，并实现了Dolphin。

Result: 在真实的反应式移动对象场景的实验评估中，Dolphin在多机器上展示了可扩展性，并提供了近实时的反应延迟。

Conclusion: Dolphin平台能够有效支持反应式移动对象应用，减轻开发者在性能和一致性之间进行权衡的负担。

Abstract: Novel reactive moving object applications require solutions to support object reactive behaviors as a way to query and update dynamic data. While moving object scenarios have long been researched in the context of spatio-temporal data management, reactive behavior is usually left to complex end-user implementations. However, it is not just a matter of hardwiring reactive constraints: the required solutions need to satisfy tight low-latency computation requirements and be scalable. This paper explores a novel approach to enrich a distributed actor-based framework with reactive functionality and complex spatial data management along with concurrency semantics. Our approach relies on a proposal of the moving actor abstraction, which is a conceptual enhancement of the actor model with reactive sensing, movement, and spatial querying capabilities. This enhancement helps developers of reactive moving object applications avoid the significant burden of implementing application-level schemes to balance performance and consistency. Based on moving actors, we define a reactive moving object data management platform, named Moving Actor-Oriented Databases (M-AODBs), and build Dolphin -- an implementation of M-AODBs. Dolphin embodies a non-intrusive actor-based design layered on top of the Microsoft Orleans distributed virtual actor framework. In a set of experimental evaluations with realistic reactive moving object scenarios, Dolphin exhibits scalability on multi-machines and provides near-real-time reaction latency.

</details>


### [31] [CityVerse: A Unified Data Platform for Multi-Task Urban Computing with Large Language Models](https://arxiv.org/abs/2511.10418)
*Yaqiao Zhu,Hongkai Wen,Mark Birkin,Man Luo*

Main category: cs.DB

TL;DR: CityVerse是一个统一的平台，集成了多源城市数据、基于能力的任务分类和动态模拟，用于在城市环境中进行系统的LLM评估。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）在城市计算方面显示出巨大的潜力，但由于缺乏统一的平台来进行一致的多源数据访问，以及任务定义不明确，阻碍了公平的比较，因此评估LLM在各种城市任务中的表现面临着严峻的挑战。

Method: CityVerse平台通过以下方式实现：1) 坐标数据API，统一了十类城市数据；2) 任务API，将43个城市计算任务组织成四个认知层次；3) 交互式可视化前端，支持实时数据检索、多层显示和模拟重放。

Result: 通过对主流LLM在代表性任务上的评估，验证了该平台的有效性，证明了其支持可重复和系统评估的能力。

Conclusion: CityVerse为推进城市计算领域中的LLM和多任务方法提供了一个可重用的基础。

Abstract: Large Language Models (LLMs) show remarkable potential for urban computing, from spatial reasoning to predictive analytics. However, evaluating LLMs across diverse urban tasks faces two critical challenges: lack of unified platforms for consistent multi-source data access and fragmented task definitions that hinder fair comparison. To address these challenges, we present CityVerse, the first unified platform integrating multi-source urban data, capability-based task taxonomy, and dynamic simulation for systematic LLM evaluation in urban contexts. CityVerse provides: 1) coordinate-based Data APIs unifying ten categories of urban data-including spatial features, temporal dynamics, demographics, and multi-modal imagery-with over 38 million curated records; 2) Task APIs organizing 43 urban computing tasks into a four-level cognitive hierarchy: Perception, Spatial Understanding, Reasoning and Prediction, and Decision and Interaction, enabling standardized evaluation across capability levels; 3) an interactive visualization frontend supporting real-time data retrieval, multi-layer display, and simulation replay for intuitive exploration and validation. We validate the platform's effectiveness through evaluations on mainstream LLMs across representative tasks, demonstrating its capability to support reproducible and systematic assessment. CityVerse provides a reusable foundation for advancing LLMs and multi-task approaches in the urban computing domain.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [32] [GPR: Towards a Generative Pre-trained One-Model Paradigm for Large-Scale Advertising Recommendation](https://arxiv.org/abs/2511.10138)
*Jun Zhang,Yi Li,Yue Liu,Changping Wang,Yuan Wang,Yuling Xiong,Xun Liu,Haiyang Wu,Qian Li,Enming Zhang,Jiawei Sun,Xin Xu,Zishuai Zhang,Ruoran Liu,Suyuan Huang,Zhaoxin Zhang,Zhengkai Guo,Shuojin Yang,Meng-Hao Guo,Huan Yu,Jie Jiang,Shi-Min Hu*

Main category: cs.IR

TL;DR: GPR: 一个端到端生成式广告推荐框架，用统一方法代替传统级联模式。


<details>
  <summary>Details</summary>
Motivation: 现有的多阶段广告推荐系统存在目标不一致和误差传播问题，而统一生成式推荐模型难以满足实际工业应用的需求。

Method: 提出了GPR，包含统一表示、网络架构和训练策略三个关键创新点。统一表示设计了统一的输入模式和tokenization方法；网络架构开发了异构分层解码器 (HHD)；训练策略整合了多token预测 (MTP)、价值感知微调和层级增强策略优化 (HEPO) 算法。

Result: GPR已在腾讯微信频道广告系统中全面部署，并在GMV和CTCVR等关键业务指标上实现了显著提升。

Conclusion: GPR是首个单模型框架，将广告推荐重新定义为端到端生成任务

Abstract: As an intelligent infrastructure connecting users with commercial content, advertising recommendation systems play a central role in information flow and value creation within the digital economy. However, existing multi-stage advertising recommendation systems suffer from objective misalignment and error propagation, making it difficult to achieve global optimality, while unified generative recommendation models still struggle to meet the demands of practical industrial applications. To address these issues, we propose GPR (Generative Pre-trained Recommender), the first one-model framework that redefines advertising recommendation as an end-to-end generative task, replacing the traditional cascading paradigm with a unified generative approach. To realize GPR, we introduce three key innovations spanning unified representation, network architecture, and training strategy. First, we design a unified input schema and tokenization method tailored to advertising scenarios, mapping both ads and organic content into a shared multi-level semantic ID space, thereby enhancing semantic alignment and modeling consistency across heterogeneous data. Second, we develop the Heterogeneous Hierarchical Decoder (HHD), a dual-decoder architecture that decouples user intent modeling from ad generation, achieving a balance between training efficiency and inference flexibility while maintaining strong modeling capacity. Finally, we propose a multi-stage joint training strategy that integrates Multi-Token Prediction (MTP), Value-Aware Fine-Tuning and the Hierarchy Enhanced Policy Optimization (HEPO) algorithm, forming a complete generative recommendation pipeline that unifies interest modeling, value alignment, and policy optimization. GPR has been fully deployed in the Tencent Weixin Channels advertising system, delivering significant improvements in key business metrics including GMV and CTCVR.

</details>


### [33] [Don't Waste It: Guiding Generative Recommenders with Structured Human Priors via Multi-head Decoding](https://arxiv.org/abs/2511.10492)
*Yunkai Zhang,Qiang Zhang,Feng,Lin,Ruizhong Qiu,Hanchao Yu,Jason Liu,Yinglong Xia,Zhuoran Yu,Zeyu Zheng,Diji Yang*

Main category: cs.IR

TL;DR: 提出了一种将人类先验知识融入生成式推荐系统端到端训练的通用框架，以优化推荐系统在准确率之外的目标（如多样性、新颖性和个性化）。


<details>
  <summary>Details</summary>
Motivation: 工业界积累了大量结构化领域知识（人类先验），但现有方法通常在排序或后排序阶段进行事后调整，与核心模型学习脱节。许多针对准确率之外目标的方法需要特定的架构修改，并且以完全无监督的方式学习用户意图，从而丢弃了这些有价值的人类先验。

Method: 提出了一种与骨干网络无关的框架，通过受高效LLM解码策略启发的轻量级、先验条件适配器头，将人类先验直接整合到生成式推荐器的端到端训练中，引导模型沿着人类可理解的轴（如交互类型、长期与短期兴趣）解耦用户意图。还提出了一种分层组合策略，用于建模不同先验类型之间的复杂交互。

Result: 在三个大规模数据集上的大量实验表明，该方法显著提高了准确率和准确率之外的目标。人类先验使得骨干模型能够更有效地利用更长的上下文长度和更大的模型尺寸。

Conclusion: 该方法成功地将人类先验知识融入到生成式推荐系统中，提高了推荐系统的性能。

Abstract: Optimizing recommender systems for objectives beyond accuracy, such as diversity, novelty, and personalization, is crucial for long-term user satisfaction. To this end, industrial practitioners have accumulated vast amounts of structured domain knowledge, which we term human priors (e.g., item taxonomies, temporal patterns). This knowledge is typically applied through post-hoc adjustments during ranking or post-ranking. However, this approach remains decoupled from the core model learning, which is particularly undesirable as the industry shifts to end-to-end generative recommendation foundation models. On the other hand, many methods targeting these beyond-accuracy objectives often require architecture-specific modifications and discard these valuable human priors by learning user intent in a fully unsupervised manner.
  Instead of discarding the human priors accumulated over years of practice, we introduce a backbone-agnostic framework that seamlessly integrates these human priors directly into the end-to-end training of generative recommenders. With lightweight, prior-conditioned adapter heads inspired by efficient LLM decoding strategies, our approach guides the model to disentangle user intent along human-understandable axes (e.g., interaction types, long- vs. short-term interests). We also introduce a hierarchical composition strategy for modeling complex interactions across different prior types. Extensive experiments on three large-scale datasets demonstrate that our method significantly enhances both accuracy and beyond-accuracy objectives. We also show that human priors allow the backbone model to more effectively leverage longer context lengths and larger model sizes.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [34] [Probability-Biased Attention over Directed Bipartite Graphs for Long-Tail ICD Coding](https://arxiv.org/abs/2511.09559)
*Tianlei Chen,Yuxiao Chen,Yang Li,Feifei Wang*

Main category: cs.LG

TL;DR: 提出了一种新的 ICD 编码方法，通过对常见代码和罕见代码之间的共现关系进行建模来解决长尾分布问题。


<details>
  <summary>Details</summary>
Motivation: ICD 编码任务具有标签空间大和长尾分布的挑战，罕见代码缺乏足够的训练数据。

Method: 构建了一个有向二分图编码器，其中边从常见代码指向罕见代码，并使用基于概率的偏差来定义这些连接。利用大型语言模型 (LLM) 为代码生成全面的描述，从而丰富初始嵌入。

Result: 在三个自动 ICD 编码基准数据集上的实验表明，该方法实现了最先进的性能，尤其是在 Macro-F1 方面有显著改进。

Conclusion: 该方法通过聚合常见代码的统计共现信息来丰富罕见代码的表示，从而提高了 ICD 编码的性能。

Abstract: Automated International Classification of Diseases (ICD) coding aims to assign multiple disease codes to clinical documents, constituting a crucial multi-label text classification task in healthcare informatics. However, the task is challenging due to its large label space (10,000 to 20,000 codes) and long-tail distribution, where a few codes dominate while many rare codes lack sufficient training data. To address this, we propose a learning method that models fine-grained co-occurrence relationships among codes. Specifically, we construct a Directed Bipartite Graph Encoder with disjoint sets of common and rare code nodes. To facilitate a one-way information flow, edges are directed exclusively from common to rare codes. The nature of these connections is defined by a probability-based bias, which is derived from the conditional probability of a common code co-occurring given the presence of a rare code. This bias is then injected into the encoder's attention module, a process we term Co-occurrence Encoding. This structure empowers the graph encoder to enrich rare code representations by aggregating latent comorbidity information reflected in the statistical co-occurrence of their common counterparts. To ensure high-quality input to the graph, we utilize a large language model (LLM) to generate comprehensive descriptions for codes, enriching initial embeddings with clinical context and comorbidity information, serving as external knowledge for the statistical co-occurrence relationships in the code system. Experiments on three automated ICD coding benchmark datasets demonstrate that our method achieves state-of-the-art performance with particularly notable improvements in Macro-F1, which is the key metric for long-tail classification.

</details>


### [35] [DemoTuner: Efficient DBMS Knobs Tuning via LLM-Assisted Demonstration Reinforcement Learning](https://arxiv.org/abs/2511.09998)
*Hui Dou,Lei Jin,Yuxuan Zhou,Jiang He,Yiwen Zhang*

Main category: cs.LG

TL;DR: 提出DemoTuner框架，利用LLM辅助的演示强化学习方法，通过挖掘文本中的tuning hints来改善数据库管理系统(DBMS)旋钮调优。


<details>
  <summary>Details</summary>
Motivation: 手动调整数据库管理系统(DBMS)的性能关键旋钮既费力又低效，强化学习(RL)方法在离线训练中收敛速度慢。

Method: 设计了一个结构化的思维链提示，利用LLM进行条件感知的tuning hints提取，并提出了一种hint-aware的演示强化学习算法HA-DDPGfD。

Result: 在MySQL和PostgreSQL上的实验表明，DemoTuner在性能提升和在线调整成本降低方面优于DB-BERT, GPTuner和CDBTune，并且对未知工作负载具有更好的适应性。

Conclusion: DemoTuner是第一个将演示强化学习算法引入DBMS旋钮调优的工作，并在实验中表现出显著优势。

Abstract: The performance of modern DBMSs such as MySQL and PostgreSQL heavily depends on the configuration of performance-critical knobs. Manual tuning these knobs is laborious and inefficient due to the complex and high-dimensional nature of the configuration space. Among the automated tuning methods, reinforcement learning (RL)-based methods have recently sought to improve the DBMS knobs tuning process from several different perspectives. However, they still encounter challenges with slow convergence speed during offline training. In this paper, we mainly focus on how to leverage the valuable tuning hints contained in various textual documents such as DBMS manuals and web forums to improve the offline training of RL-based methods. To this end, we propose an efficient DBMS knobs tuning framework named DemoTuner via a novel LLM-assisted demonstration reinforcement learning method. Specifically, to comprehensively and accurately mine tuning hints from documents, we design a structured chain of thought prompt to employ LLMs to conduct a condition-aware tuning hints extraction task. To effectively integrate the mined tuning hints into RL agent training, we propose a hint-aware demonstration reinforcement learning algorithm HA-DDPGfD in DemoTuner. As far as we know, DemoTuner is the first work to introduce the demonstration reinforcement learning algorithm for DBMS knobs tuning. Experimental evaluations conducted on MySQL and PostgreSQL across various workloads demonstrate the significant advantages of DemoTuner in both performance improvement and online tuning cost reduction over three representative baselines including DB-BERT, GPTuner and CDBTune. Additionally, DemoTuner also exhibits superior adaptability to application scenarios with unknown workloads.

</details>


### [36] [Let the Experts Speak: Improving Survival Prediction & Calibration via Mixture-of-Experts Heads](https://arxiv.org/abs/2511.09567)
*Todd Morrill,Aahlad Puli,Murad Megjhani,Soojin Park,Richard Zemel*

Main category: cs.LG

TL;DR: 本文介绍了一种用于生存分析问题的离散时间深度混合专家 (MoE) 架构，该架构实现了所有理想的结果：聚类、校准和预测准确性。


<details>
  <summary>Details</summary>
Motivation: 深度混合专家模型在生存分析问题中备受关注，特别是因为它们能够将相似的患者聚集在一起。在实践中，分组通常以关键指标（如校准误差和预测准确性）为代价。这是由于混合专家施加的限制性归纳偏差，即个体患者的预测必须看起来像他们所分配到的群体的预测。

Method: 本文介绍了几种用于生存分析问题的离散时间深度混合专家 (MoE) 架构。

Result: 研究结果表明，这一系列 MoE 之间的关键区别在于它们的专家有多么具有表现力。研究发现，为每个患者定制预测的更具表现力的专家优于依赖于固定群体原型的专家。

Conclusion: 本文提出了一种新的MoE架构，可以同时实现聚类、校准和预测准确性，并且发现更具表现力的专家可以提高预测性能。

Abstract: Deep mixture-of-experts models have attracted a lot of attention for survival analysis problems, particularly for their ability to cluster similar patients together. In practice, grouping often comes at the expense of key metrics such calibration error and predictive accuracy. This is due to the restrictive inductive bias that mixture-of-experts imposes, that predictions for individual patients must look like predictions for the group they're assigned to. Might we be able to discover patient group structure, where it exists, while improving calibration and predictive accuracy? In this work, we introduce several discrete-time deep mixture-of-experts (MoE) based architectures for survival analysis problems, one of which achieves all desiderata: clustering, calibration, and predictive accuracy. We show that a key differentiator between this array of MoEs is how expressive their experts are. We find that more expressive experts that tailor predictions per patient outperform experts that rely on fixed group prototypes.

</details>


### [37] [Filtering Jump Markov Systems with Partially Known Dynamics: A Model-Based Deep Learning Approach](https://arxiv.org/abs/2511.09569)
*George Stamatelis,George C. Alexandropoulos*

Main category: cs.LG

TL;DR: 提出了一种用于跳跃马尔可夫系统中实时状态估计的深度学习框架JMFNet，该框架无需噪声统计和模式转换动态的先验知识。


<details>
  <summary>Details</summary>
Motivation: 在具有未知噪声统计和模式转换动态的跳跃马尔可夫系统中，实现实时的状态估计。

Method: 该框架包含两个循环神经网络(RNN):一个用于模式预测，另一个用于滤波，它基于最近提出的KalmanNet架构的模式增强版本。使用交替最小二乘策略联合训练所提出的RNN，以实现无需潜在模式监督的相互适应。

Result: 在包括目标跟踪、单摆角度跟踪、Lorenz吸引子动力学和真实数据集在内的线性及非线性系统上进行的大量数值实验表明，所提出的JMFNet框架优于经典的基于模型的滤波器(例如，交互式多模型和粒子滤波器)以及无模型的深度学习基线，尤其是在非平稳和高噪声情况下。JMFNet在复杂系统或长轨迹中，相对于KalmanNet框架，性能有小幅但意义重大的提升。该方法的性能经过实验验证，具有一致性和可靠性，对初始条件、超参数选择以及不正确的模型知识表现出较低的敏感性。

Conclusion: 所提出的JMFNet框架在跳跃马尔可夫系统中实现了优异的实时状态估计性能，并且对各种因素具有鲁棒性。

Abstract: This paper presents the Jump Markov Filtering Network (JMFNet), a novel model-based deep learning framework for real-time state-state estimation in jump Markov systems with unknown noise statistics and mode transition dynamics. A hybrid architecture comprising two Recurrent Neural Networks (RNNs) is proposed: one for mode prediction and another for filtering that is based on a mode-augmented version of the recently presented KalmanNet architecture. The proposed RNNs are trained jointly using an alternating least squares strategy that enables mutual adaptation without supervision of the latent modes. Extensive numerical experiments on linear and nonlinear systems, including target tracking, pendulum angle tracking, Lorenz attractor dynamics, and a real-life dataset demonstrate that the proposed JMFNet framework outperforms classical model-based filters (e.g., interacting multiple models and particle filters) as well as model-free deep learning baselines, particularly in non-stationary and high-noise regimes. It is also showcased that JMFNet achieves a small yet meaningful improvement over the KalmanNet framework, which becomes much more pronounced in complicated systems or long trajectories. Finally, the method's performance is empirically validated to be consistent and reliable, exhibiting low sensitivity to initial conditions, hyperparameter selection, as well as to incorrect model knowledge

</details>


### [38] [Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost](https://arxiv.org/abs/2511.09573)
*Valentino F. Foit,David W. Hogg,Soledad Villar*

Main category: cs.LG

TL;DR: 该论文提出了一种简单有效的提高模型精度的方法，即在测试时使用群平均，通过对模型在对称群上的平均来强制执行精确对称性。


<details>
  <summary>Details</summary>
Motivation: 在自然科学中的许多机器学习任务在特定对称性下是精确等变的，但等变方法通常未被采用，可能是因为训练被认为具有挑战性，或者期望学习对称性，或者等变实现被认为难以构建。

Method: 该方法在测试时进行群平均，通过在对称群上平均模型来强制执行精确对称性。它对模型结构或训练没有要求。

Result: 实验表明，该程序始终会降低平均评估损失，在 VRMSE 方面最多可提高 37%。平均可以为连续动力学产生视觉上更好的预测。

Conclusion: 在某些常见情况下，施加精确对称性没有任何缺点；ML4PS 社区应考虑将群平均作为一种廉价且简单的方法来提高模型精度。

Abstract: Many machine learning tasks in the natural sciences are precisely equivariant to particular symmetries. Nonetheless, equivariant methods are often not employed, perhaps because training is perceived to be challenging, or the symmetry is expected to be learned, or equivariant implementations are seen as hard to build. Group averaging is an available technique for these situations. It happens at test time; it can make any trained model precisely equivariant at a (often small) cost proportional to the size of the group; it places no requirements on model structure or training. It is known that, under mild conditions, the group-averaged model will have a provably better prediction accuracy than the original model. Here we show that an inexpensive group averaging can improve accuracy in practice. We take well-established benchmark machine learning models of differential equations in which certain symmetries ought to be obeyed. At evaluation time, we average the models over a small group of symmetries. Our experiments show that this procedure always decreases the average evaluation loss, with improvements of up to 37\% in terms of the VRMSE. The averaging produces visually better predictions for continuous dynamics. This short paper shows that, under certain common circumstances, there are no disadvantages to imposing exact symmetries; the ML4PS community should consider group averaging as a cheap and simple way to improve model accuracy.

</details>


### [39] [HeatGen: A Guided Diffusion Framework for Multiphysics Heat Sink Design Optimization](https://arxiv.org/abs/2511.09578)
*Hadi Keramati,Morteza Sadeghi,Rajeev K. Jaiman*

Main category: cs.LG

TL;DR: 提出了一种基于引导去噪扩散概率模型 (DDPM) 的生成优化框架，该框架利用替代梯度生成散热器设计，从而最大限度地减少压降，同时将表面温度保持在指定阈值以下。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统优化方法和拓扑优化方法在散热器设计中的局限性，如计算成本高、难以扩展以及需要针对新约束进行重新训练等问题。

Method: 该方法结合了多重保真度方法生成训练数据，并训练去噪扩散概率模型以生成具有与数据中观察到的特征一致的散热器。此外，还训练了两个不同的残差神经网络来预测每个几何形状的压降和表面温度，并使用这些替代模型的梯度来指导几何生成过程，以满足低压和表面温度约束。

Result: 使用引导扩散模型生成的样本实现了比传统黑盒优化方法获得的限制低 10% 的压降。

Conclusion: 这项工作代表了朝着构建电子冷却基础生成模型迈出的一步。

Abstract: This study presents a generative optimization framework based on a guided denoising diffusion probabilistic model (DDPM) that leverages surrogate gradients to generate heat sink designs minimizing pressure drop while maintaining surface temperatures below a specified threshold. Geometries are represented using boundary representations of multiple fins, and a multi-fidelity approach is employed to generate training data. Using this dataset, along with vectors representing the boundary representation geometries, we train a denoising diffusion probabilistic model to generate heat sinks with characteristics consistent with those observed in the data. We train two different residual neural networks to predict the pressure drop and surface temperature for each geometry. We use the gradients of these surrogate models with respect to the design variables to guide the geometry generation process toward satisfying the low-pressure and surface temperature constraints. This inference-time guidance directs the generative process toward heat sink designs that not only prevent overheating but also achieve lower pressure drops compared to traditional optimization methods such as CMA-ES. In contrast to traditional black-box optimization approaches, our method is scalable, provided sufficient training data is available. Unlike traditional topology optimization methods, once the model is trained and the heat sink world model is saved, inference under new constraints (e.g., temperature) is computationally inexpensive and does not require retraining. Samples generated using the guided diffusion model achieve pressure drops up to 10 percent lower than the limits obtained by traditional black-box optimization methods. This work represents a step toward building a foundational generative model for electronics cooling.

</details>


### [40] [Scaling Environments for LLM Agents in the Era of Learning from Interaction: A Survey](https://arxiv.org/abs/2511.09586)
*Yuchen Huang,Sijia Li,Minghao Liu,Wei Liu,Shijue Huang,Zhiyuan Fan,Hou Pong Chan,Yi R. Fung*

Main category: cs.LG

TL;DR: 本文综述了如何从以环境为中心的角度扩展环境，以提高智能体的适应性行为和长期决策能力。


<details>
  <summary>Details</summary>
Motivation: 为了培养智能体的适应性行为和长期决策能力，仅靠在静态数据集上训练是不够的，需要智能体直接与环境互动并通过强化学习从经验中学习。

Method: 本文将迭代过程形式化为生成-执行-反馈（GEF）循环，并从环境中心角度系统地回顾了环境扩展的代表性方法，并按GEF循环的阶段组织它们。

Result: 本文分析了基准、实施策略和应用。

Conclusion: 本文总结了零散的进展，并概述了智能体智能的未来研究方向。

Abstract: LLM-based agents can autonomously accomplish complex tasks across various domains. However, to further cultivate capabilities such as adaptive behavior and long-term decision-making, training on static datasets built from human-level knowledge is insufficient. These datasets are costly to construct and lack both dynamism and realism. A growing consensus is that agents should instead interact directly with environments and learn from experience through reinforcement learning. We formalize this iterative process as the Generation-Execution-Feedback (GEF) loop, where environments generate tasks to challenge agents, return observations in response to agents' actions during task execution, and provide evaluative feedback on rollouts for subsequent learning. Under this paradigm, environments function as indispensable producers of experiential data, highlighting the need to scale them toward greater complexity, realism, and interactivity. In this survey, we systematically review representative methods for environment scaling from a pioneering environment-centric perspective and organize them along the stages of the GEF loop, namely task generation, task execution, and feedback. We further analyze benchmarks, implementation strategies, and applications, consolidating fragmented advances and outlining future research directions for agent intelligence.

</details>


### [41] [DynamicRTL: RTL Representation Learning for Dynamic Circuit Behavior](https://arxiv.org/abs/2511.09593)
*Ruiyang Ma,Yunhao Zhou,Yipeng Wang,Yi Liu,Zhengyuan Shi,Ziyang Zheng,Kexin Chen,Zhiqiang He,Lingwei Yan,Gang Chen,Qiang Xu,Guojie Luo*

Main category: cs.LG

TL;DR: DR-GNN通过结合静态结构和多周期执行行为来学习RTL电路表示，优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有GNN模型无法捕捉电路运行时行为，这对于电路验证和优化至关重要。

Method: 提出DR-GNN，利用操作级控制数据流图（CDFG）表示RTL电路，从而能够捕获动态依赖关系和运行时执行。

Result: DR-GNN在分支命中预测和翻转率预测方面优于现有模型，并且其学习的表示有效地转移到相关的动态电路任务，在功率估计和断言预测中实现了强大的性能。

Conclusion: DR-GNN能够有效地学习电路的动态行为，并在多个下游任务中表现出色。

Abstract: There is a growing body of work on using Graph Neural Networks (GNNs) to learn representations of circuits, focusing primarily on their static characteristics. However, these models fail to capture circuit runtime behavior, which is crucial for tasks like circuit verification and optimization. To address this limitation, we introduce DR-GNN (DynamicRTL-GNN), a novel approach that learns RTL circuit representations by incorporating both static structures and multi-cycle execution behaviors. DR-GNN leverages an operator-level Control Data Flow Graph (CDFG) to represent Register Transfer Level (RTL) circuits, enabling the model to capture dynamic dependencies and runtime execution. To train and evaluate DR-GNN, we build the first comprehensive dynamic circuit dataset, comprising over 6,300 Verilog designs and 63,000 simulation traces. Our results demonstrate that DR-GNN outperforms existing models in branch hit prediction and toggle rate prediction. Furthermore, its learned representations transfer effectively to related dynamic circuit tasks, achieving strong performance in power estimation and assertion prediction.

</details>


### [42] [Making Every Head Count: Sparse Attention Without the Speed-Performance Trade-off](https://arxiv.org/abs/2511.09596)
*Mingkuan Zhao,Wentao Hu,Jiayin Wang,Xin Lai,Tianchen Huang,Yuheng Min,Rui Yan,Xiaoyan Zhu*

Main category: cs.LG

TL;DR: SPAttention通过将注意力计算划分为平衡的、非重叠的距离带来解决效率和性能之间的权衡，从而降低计算复杂度并提高训练吞吐量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）中的核心注意力机制的显著表达能力是建立在计算复杂度之上的，该计算复杂度随上下文大小（N）呈二次方增长，并且随头数（H）呈线性增长。

Method: SPAttention 将总注意力工作负载划分为平衡的、非重叠的距离带，并为每个头分配一个独特的段。

Result: SPAttention 在 OLMoE-1B-7B 和 0.25B-1.75B 模型系列上进行了广泛的实证验证，结果表明，在训练吞吐量大约提高两倍的同时，其性能与标准密集注意力相当，甚至在某些关键指标上超过了标准密集注意力，同时在所有评估指标上始终优于具有代表性的稀疏注意力方法，包括 Longformer、Reformer 和 BigBird。

Conclusion: SPAttention 通过结构化的归纳偏置，促使头之间的功能专业化，从而能够更有效地分配计算资源，从冗余建模到整个序列跨度的不同依赖关系。

Abstract: The design of Large Language Models (LLMs) has long been hampered by a fundamental conflict within their core attention mechanism: its remarkable expressivity is built upon a computational complexity of $O(H \cdot N^2)$ that grows quadratically with the context size ($N$) and linearly with the number of heads ($H$). This standard implementation harbors significant computational redundancy, as all heads independently compute attention over the same sequence space. Existing sparse methods, meanwhile, often trade information integrity for computational efficiency. To resolve this efficiency-performance trade-off, we propose SPAttention, whose core contribution is the introduction of a new paradigm we term Principled Structural Sparsity. SPAttention does not merely drop connections but instead reorganizes the computational task by partitioning the total attention workload into balanced, non-overlapping distance bands, assigning each head a unique segment. This approach transforms the multi-head attention mechanism from $H$ independent $O(N^2)$ computations into a single, collaborative $O(N^2)$ computation, fundamentally reducing complexity by a factor of $H$. The structured inductive bias compels functional specialization among heads, enabling a more efficient allocation of computational resources from redundant modeling to distinct dependencies across the entire sequence span. Extensive empirical validation on the OLMoE-1B-7B and 0.25B-1.75B model series demonstrates that while delivering an approximately two-fold increase in training throughput, its performance is on par with standard dense attention, even surpassing it on select key metrics, while consistently outperforming representative sparse attention methods including Longformer, Reformer, and BigBird across all evaluation metrics.

</details>
