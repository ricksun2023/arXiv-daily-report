<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 18]
- [cs.CV](#cs.CV) [Total: 16]
- [cs.AI](#cs.AI) [Total: 16]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.LG](#cs.LG) [Total: 17]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Activation-Space Personality Steering: Hybrid Layer Selection for Stable Trait Control in LLMs](https://arxiv.org/abs/2511.03738)
*Pranav Bhandari,Nicolas Fay,Sanjeevan Selvaganapathy,Amitava Datta,Usman Naseem,Mehwish Nasim*

Main category: cs.CL

TL;DR: 论文提出了一种新的流程，通过提取Transformer层中的隐藏状态激活，利用五大人格特质，并应用低秩子空间发现方法，从而在不同模型架构中识别出特定于特质的最佳层，以实现对LLM输出中特质表达的精确控制。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成过程中表现出隐含的人格，但可靠地控制或调整这些特质以满足特定需求仍然是一个开放的挑战。缺乏在生成过程中有效操纵模型的机制是文献中的一个关键空白。人格感知的LLM为此提供了一个有希望的方向。然而，这些心理结构及其在LLM中的表征之间的关系仍未被充分探索，需要进一步研究。

Method: 该论文提出了一种新颖的流程，从Transformer层提取隐藏状态激活，使用五大人格特质，应用低秩子空间发现方法，并在不同的模型架构中识别出特定于特质的最佳层，以实现鲁棒的注入。

Result: 研究结果表明，人格特质占据一个低秩共享子空间，并且这些潜在结构可以转化为可操作的机制，通过仔细的扰动进行有效的引导，而不会影响流畅性、方差和一般能力。

Conclusion: 该研究有助于弥合心理学理论和实际模型对齐之间的差距。

Abstract: Large Language Models exhibit implicit personalities in their generation, but
reliably controlling or aligning these traits to meet specific needs remains an
open challenge. The need for effective mechanisms for behavioural manipulation
of the model during generation is a critical gap in the literature that needs
to be fulfilled. Personality-aware LLMs hold a promising direction towards this
objective. However, the relationship between these psychological constructs and
their representations within LLMs remains underexplored and requires further
investigation. Moreover, it is intriguing to understand and study the use of
these representations to steer the models' behaviour. We propose a novel
pipeline that extracts hidden state activations from transformer layers using
the Big Five Personality Traits (Openness, Conscientiousness, Extraversion,
Agreeableness and Neuroticism), which is a comprehensive and empirically
validated framework to model human personality applies low-rank subspace
discovery methods, and identifies trait-specific optimal layers across
different model architectures for robust injection. The resulting
personality-aligned directions are then operationalised through a flexible
steering framework with dynamic layer selection, enabling precise control of
trait expression in LLM outputs. Our findings reveal that personality traits
occupy a low-rank shared subspace, and that these latent structures can be
transformed into actionable mechanisms for effective steering through careful
perturbations without impacting the fluency, variance and general capabilities,
helping to bridge the gap between psychological theory and practical model
alignment.

</details>


### [2] [BAPPA: Benchmarking Agents, Plans, and Pipelines for Automated Text-to-SQL Generation](https://arxiv.org/abs/2511.04153)
*Fahim Ahmed,Md Mubtasim Ahasan,Jahir Sadik Monon,Muntasir Wahed,M Ashraful Amin,A K M Mahbubur Rahman,Amin Ahsan Ali*

Main category: cs.CL

TL;DR: 这篇论文研究了如何使用多智能体LLM流水线来提高Text-to-SQL系统的性能，特别关注于小型高效模型。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM在处理大型schema和复杂推理时，难以从自然语言指令生成SQL，并且现有工作主要集中在复杂的pipeline和旗舰模型上，忽略了小型模型。

Method: 论文探索了三种多智能体LLM流水线：多智能体讨论流水线、Planner-Coder流水线和Coder-Aggregator流水线，并在不同大小的开源模型上进行了系统性能基准测试。

Result: 实验表明，多智能体讨论可以提高小型模型的性能，LLM Reasoner-Coder流水线效果最好，DeepSeek-R1-32B和QwQ-32B planners可以将Gemma 3 27B IT的准确率从52.4%提高到56.4%。

Conclusion: 多智能体讨论流水线和LLM Reasoner-Coder流水线可以有效提高小型LLM在Text-to-SQL任务上的性能。

Abstract: Text-to-SQL systems provide a natural language interface that can enable even
laymen to access information stored in databases. However, existing Large
Language Models (LLM) struggle with SQL generation from natural instructions
due to large schema sizes and complex reasoning. Prior work often focuses on
complex, somewhat impractical pipelines using flagship models, while smaller,
efficient models remain overlooked. In this work, we explore three multi-agent
LLM pipelines, with systematic performance benchmarking across a range of small
to large open-source models: (1) Multi-agent discussion pipeline, where agents
iteratively critique and refine SQL queries, and a judge synthesizes the final
answer; (2) Planner-Coder pipeline, where a thinking model planner generates
stepwise SQL generation plans and a coder synthesizes queries; and (3)
Coder-Aggregator pipeline, where multiple coders independently generate SQL
queries, and a reasoning agent selects the best query. Experiments on the
Bird-Bench Mini-Dev set reveal that Multi-Agent discussion can improve small
model performance, with up to 10.6% increase in Execution Accuracy for
Qwen2.5-7b-Instruct seen after three rounds of discussion. Among the pipelines,
the LLM Reasoner-Coder pipeline yields the best results, with DeepSeek-R1-32B
and QwQ-32B planners boosting Gemma 3 27B IT accuracy from 52.4% to the highest
score of 56.4%. Codes are available at
https://github.com/treeDweller98/bappa-sql.

</details>


### [3] [TextualVerifier: Verify TextGrad Step-by-Step](https://arxiv.org/abs/2511.03739)
*Eugenius Mario Situmorang,Adila Alfa Krisnadhi,Ari Wibisono*

Main category: cs.CL

TL;DR: TextGrad lacks self-verification. TextualVerifier, a new framework, uses chain-of-thought and majority voting to address this, improving reasoning validity and overall performance.


<details>
  <summary>Details</summary>
Motivation: TextGrad, a text-based automatic differentiation approach, needs self-verification mechanisms for reasoning validity in text-based decision making.

Method: TextualVerifier implements a four-stage workflow: chain-of-thought decomposition, variant generation, majority voting, and consensus aggregation. It integrates non-invasively with TextGrad.

Result: TextualVerifier improves reasoning validity by 29% and TextGrad performance by 2.2 percentage points. Versioning yields further improvements.

Conclusion: TextualVerifier is the first self-verification framework for TextGrad, enhancing reliability and opening new avenues for verification in text-based optimization.

Abstract: TextGrad is a novel approach to text-based automatic differentiation that
enables composite AI systems to perform optimization without explicit numerical
equations. However, it currently lacks self-verification mechanisms that ensure
reasoning validity in text-based decision making. This research introduces
TextualVerifier, a verification framework that leverages chain-of-thought
reasoning and majority voting with large language models to address this
verification gap. TextualVerifier implements a four-stage workflow:
chain-of-thought decomposition, variant generation, majority voting, and
consensus aggregation. It integrates non-invasively with TextGrad at both the
loss function and optimization result verification stages. Experimental
evaluation using the Gemini 1.5 Pro model is conducted in two phases: (1)
standalone evaluation on PRM800K, and (2) integrated evaluation with TextGrad
on GPQA-Diamond, MMLU-ML, and MMLU-CP benchmarks. Results show statistically
significant improvements (p < 0.001). In phase one, TextualVerifier improves
the validity of reasoning steps by 29 percent. In phase two, integration into
TextGrad loss function yields a 2.2 percentage point gain from 68.2 to 70.4
percent with a moderate overhead of 5.9 LLM calls on average. Further
evaluations of TextualVerifier versioning yield 8.08, 10.71, and 3.92
percentage point improvements on GPQA, MMLU-ML, and MMLU-CP respectively.
TextualVerifier thus presents the first self-verification framework for
TextGrad through LLM-based techniques without requiring numerical gradients,
enabling more reliable reasoning and opening new directions for verification in
text-based optimization.

</details>


### [4] [GRDD+: An Extended Greek Dialectal Dataset with Cross-Architecture Fine-tuning Evaluation](https://arxiv.org/abs/2511.03772)
*Stergios Chatzikyriakidis,Dimitris Papadakis,Sevasti-Ioanna Papaioannou,Erofili Psaltaki*

Main category: cs.CL

TL;DR: 本文介绍了一个扩展的希腊方言数据集 (GRDD+)，它在现有 GRDD 数据集的基础上增加了来自克里特岛、塞浦路斯、本都和希腊北部的数据，同时增加了六个新的变种：科西嘉希腊语、格里科语（意大利南部希腊语）、马尼奥特语、赫普塔尼斯语、察科尼亚语和卡塔雷武萨希腊语。最终数据集总大小为 6,374,939 个单词，包含 10 个变种。这是迄今为止第一个具有如此变异性和规模的数据集。我们进行了多次微调实验，以了解高质量方言数据对许多大型语言模型的影响。我们微调了三种模型架构（Llama-3-8B、Llama-3.1-8B、Krikri-8B），并将结果与前沿模型（Claude-3.7-Sonnet、Gemini-2.5、ChatGPT-5）进行了比较。


<details>
  <summary>Details</summary>
Motivation: 创建包含多种希腊方言的大型数据集，以研究方言数据对大型语言模型的影响。

Method: 通过扩展现有的 GRDD 数据集并添加新的希腊方言变种来构建 GRDD+ 数据集。然后，在 GRDD+ 数据集上微调 Llama-3-8B、Llama-3.1-8B 和 Krikri-8B 模型，并将结果与 Claude-3.7-Sonnet、Gemini-2.5 和 ChatGPT-5 等前沿模型进行比较。

Result: 创建了一个包含 6,374,939 个单词和 10 个变种的希腊方言数据集 GRDD+。通过微调实验，评估了高质量方言数据对大型语言模型的影响。

Conclusion: 本文构建了一个大规模的希腊方言数据集，并评估了其对大型语言模型的影响。

Abstract: We present an extended Greek Dialectal Dataset (GRDD+) 1that complements the
existing GRDD dataset with more data from Cretan, Cypriot, Pontic and Northern
Greek, while we add six new varieties: Greco-Corsican, Griko (Southern Italian
Greek), Maniot, Heptanesian, Tsakonian, and Katharevusa Greek. The result is a
dataset with total size 6,374,939 words and 10 varieties. This is the first
dataset with such variation and size to date. We conduct a number of
fine-tuning experiments to see the effect of good quality dialectal data on a
number of LLMs. We fine-tune three model architectures (Llama-3-8B,
Llama-3.1-8B, Krikri-8B) and compare the results to frontier models
(Claude-3.7-Sonnet, Gemini-2.5, ChatGPT-5).

</details>


### [5] [RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within Structured Tables](https://arxiv.org/abs/2511.04491)
*Nikhil Abhyankar,Purvi Chaurasia,Sanchit Kabra,Ananya Srivastava,Vivek Gupta,Chandan K. Reddy*

Main category: cs.CL

TL;DR: RUST-BENCH是一个新的表格推理基准，它由来自两个领域的真实世界表格组成，用于评估LLM在规模、异构性、领域特异性和推理复杂性方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的表格推理基准主要测试模型在小型、统一的表格上，不能代表真实世界数据的复杂性，并且不能完整地了解大型语言模型（LLM）的推理能力。真实表格很长、异构且特定于领域，混合了结构化字段和自由文本，并且需要在数千个token上进行多跳推理。

Method: RUST-BENCH包含来自两个领域的2031个真实世界表格的7966个问题：i) RB-Science（NSF资助记录）和ii) RB-Sports（NBA统计数据）。

Result: 开源和专有模型的实验表明，LLM在异构模式和复杂的多跳推理方面存在困难，揭示了当前架构和提示策略中存在的持续弱点。

Conclusion: RUST-BENCH建立了一个具有挑战性的新测试平台，用于推进表格推理研究。

Abstract: Existing tabular reasoning benchmarks mostly test models on small, uniform
tables, underrepresenting the complexity of real-world data and giving an
incomplete view of Large Language Models' (LLMs) reasoning abilities. Real
tables are long, heterogeneous, and domain-specific, mixing structured fields
with free text and requiring multi-hop reasoning across thousands of tokens. To
address this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from
2031 real-world tables spanning two domains: i) RB-Science (NSF grant records)
and ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates
LLMs jointly across scale, heterogeneity, domain specificity, and reasoning
complexity. Experiments with open-source and proprietary models show that LLMs
struggle with heterogeneous schemas and complex multi-hop inference, revealing
persistent weaknesses in current architectures and prompting strategies.
RUST-BENCH establishes a challenging new testbed for advancing tabular
reasoning research.

</details>


### [6] [PLLuM: A Family of Polish Large Language Models](https://arxiv.org/abs/2511.03823)
*Jan Kocoń,Maciej Piasecki,Arkadiusz Janz,Teddy Ferdinan,Łukasz Radliński,Bartłomiej Koptyra,Marcin Oleksy,Stanisław Woźniak,Paweł Walkowiak,Konrad Wojtasik,Julia Moska,Tomasz Naskręt,Bartosz Walkowiak,Mateusz Gniewkowski,Kamil Szyc,Dawid Motyka,Dawid Banach,Jonatan Dalasiński,Ewa Rudnicka,Bartłomiej Alberski,Tomasz Walkowiak,Aleksander Szczęsny,Maciej Markiewicz,Tomasz Bernaś,Hubert Mazur,Kamil Żyta,Mateusz Tykierko,Grzegorz Chodak,Tomasz Kajdanowicz,Przemysław Kazienko,Agnieszka Karlińska,Karolina Seweryn,Anna Kołos,Maciej Chrabąszcz,Katarzyna Lorenc,Aleksandra Krasnodębska,Artur Wilczek,Katarzyna Dziewulska,Paula Betscher,Zofia Cieślińska,Katarzyna Kowol,Daria Mikoś,Maciej Trzciński,Dawid Krutul,Marek Kozłowski,Sławomir Dadas,Rafał Poświata,Michał Perełkiewicz,Małgorzata Grębowiec,Maciej Kazuła,Marcin Białas,Roman Roszko,Danuta Roszko,Jurgita Vaičenonienė,Andrius Utka,Paweł Levchuk,Paweł Kowalski,Irena Prawdzic-Jankowska,Maciej Ogrodniczuk,Monika Borys,Anna Bulińska,Wiktoria Gumienna,Witold Kieraś,Dorota Komosińska,Katarzyna Krasnowska-Kieraś,Łukasz Kobyliński,Martyna Lewandowska,Marek Łaziński,Mikołaj Łątkowski,Dawid Mastalerz,Beata Milewicz,Agnieszka Anna Mykowiecka,Angelika Peljak-Łapińska,Sandra Penno,Zuzanna Przybysz,Michał Rudolf,Piotr Rybak,Karolina Saputa,Aleksandra Tomaszewska,Aleksander Wawer,Marcin Woliński,Joanna Wołoszyn,Alina Wróblewska,Bartosz Żuk,Filip Żarnecki,Konrad Kaczyński,Anna Cichosz,Zuzanna Deckert,Monika Garnys,Izabela Grabarczyk,Wojciech Janowski,Sylwia Karasińska,Aleksandra Kujawiak,Piotr Misztela,Maria Szymańska,Karolina Walkusz,Igor Siek,Jakub Kwiatkowski,Piotr Pęzik*

Main category: cs.CL

TL;DR: PLLuM是为波兰语量身定制的最大的开源基础模型系列，旨在促进开放研究和加强波兰的主权AI技术。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型主要集中在英语上，对其他语言的支持有限。PLLuM的出现是为了解决对高质量、透明和文化相关的语言模型的需求，并超越以英语为中心的商业环境。

Method: 构建了一个新的1400亿token的波兰语文本语料库用于预训练，一个77k的自定义指令数据集和一个100k的偏好优化数据集。 关键组成部分是一个负责任的AI框架，该框架结合了严格的数据治理和一个用于输出校正和安全过滤的混合模块。

Result: 详细介绍了模型架构、训练程序以及基础变体和指令调整变体的对齐技术，并展示了它们在公共管理部门下游任务中的效用。

Conclusion: 通过公开发布这些模型，PLLuM旨在促进开放研究并加强波兰的主权AI技术。

Abstract: Large Language Models (LLMs) play a central role in modern artificial
intelligence, yet their development has been primarily focused on English,
resulting in limited support for other languages. We present PLLuM (Polish
Large Language Model), the largest open-source family of foundation models
tailored specifically for the Polish language. Developed by a consortium of
major Polish research institutions, PLLuM addresses the need for high-quality,
transparent, and culturally relevant language models beyond the English-centric
commercial landscape. We describe the development process, including the
construction of a new 140-billion-token Polish text corpus for pre-training, a
77k custom instructions dataset, and a 100k preference optimization dataset. A
key component is a Responsible AI framework that incorporates strict data
governance and a hybrid module for output correction and safety filtering. We
detail the models' architecture, training procedures, and alignment techniques
for both base and instruction-tuned variants, and demonstrate their utility in
a downstream task within public administration. By releasing these models
publicly, PLLuM aims to foster open research and strengthen sovereign AI
technologies in Poland.

</details>


### [7] [STARS: Segment-level Token Alignment with Rejection Sampling in Large Language Models](https://arxiv.org/abs/2511.03827)
*Mohammad Atif Quamar,Mohammad Areeb,Mikhail Kuznetsov,Muslum Ozgur Ozmen,Z. Berkay Celik*

Main category: cs.CL

TL;DR: STARS: 使用分段token对齐和拒绝抽样来引导模型生成，提高计算效率和对齐质量。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型对齐方法（如微调）计算成本高且效果欠佳，而推理期方法（如Best-of-N抽样）需要大量计算才能实现最佳对齐。

Method: 提出了一种解码时算法STARS，通过迭代地抽样、评分和拒绝/接受短的、固定大小的token段来引导模型生成。

Result: STARS在六个LLM套件上的表现优于监督式微调（SFT）高达14.9个百分点，优于直接偏好优化（DPO）高达4.3个百分点。

Conclusion: 细粒度的、奖励引导的抽样是一种通用、稳健和高效的替代方案，可以替代传统的微调和全序列排序方法来对齐LLM。

Abstract: Aligning large language models with human values is crucial for their safe
deployment; however, existing methods, such as fine-tuning, are computationally
expensive and suboptimal. In contrast, inference-time approaches like Best-of-N
sampling require practically infeasible computation to achieve optimal
alignment. We propose STARS: Segment-level Token Alignment with Rejection
Sampling, a decoding-time algorithm that steers model generation by iteratively
sampling, scoring, and rejecting/accepting short, fixed-size token segments.
This allows for early correction of the generation path, significantly
improving computational efficiency and boosting alignment quality. Across a
suite of six LLMs, we show that STARS outperforms Supervised Fine-Tuning (SFT)
by up to 14.9 percentage points and Direct Preference Optimization (DPO) by up
to 4.3 percentage points on win-rates, while remaining highly competitive with
strong Best-of-N baselines. Our work establishes granular, reward-guided
sampling as a generalizable, robust, and efficient alternative to traditional
fine-tuning and full-sequence ranking methods for aligning LLMs.

</details>


### [8] [Divide, Cache, Conquer: Dichotomic Prompting for Efficient Multi-Label LLM-Based Classification](https://arxiv.org/abs/2511.03830)
*Mikołaj Langner,Jan Eliasz,Ewa Rudnicka,Jan Kocoń*

Main category: cs.CL

TL;DR: 提出了一种高效的多标签文本分类方法，该方法将分类任务重新构建为二分决策序列，从而提高了效率。


<details>
  <summary>Details</summary>
Motivation: 利用大型语言模型（LLM）进行多标签文本分类。

Method: 将分类任务分解为二分（是/否）决策序列，并结合前缀缓存机制。

Result: 在情感文本分析方面，微调后的模型在训练期间看到的维度上显示出比零样本基线显着的改进。

Conclusion: 将多标签分类分解为二分查询，结合蒸馏和缓存感知推理，为基于LLM的分类提供了一个可扩展且有效的框架。

Abstract: We introduce a method for efficient multi-label text classification with
large language models (LLMs), built on reformulating classification tasks as
sequences of dichotomic (yes/no) decisions. Instead of generating all labels in
a single structured response, each target dimension is queried independently,
which, combined with a prefix caching mechanism, yields substantial efficiency
gains for short-text inference without loss of accuracy. To demonstrate the
approach, we focus on affective text analysis, covering 24 dimensions including
emotions and sentiment. Using LLM-to-SLM distillation, a powerful annotator
model (DeepSeek-V3) provides multiple annotations per text, which are
aggregated to fine-tune smaller models (HerBERT-Large, CLARIN-1B, PLLuM-8B,
Gemma3-1B). The fine-tuned models show significant improvements over zero-shot
baselines, particularly on the dimensions seen during training. Our findings
suggest that decomposing multi-label classification into dichotomic queries,
combined with distillation and cache-aware inference, offers a scalable and
effective framework for LLM-based classification. While we validate the method
on affective states, the approach is general and applicable across domains.

</details>


### [9] [Evaluating Machine Translation Datasets for Low-Web Data Languages: A Gendered Lens](https://arxiv.org/abs/2511.03880)
*Hellina Hailu Nigatu,Bethelhem Yemane Mamo,Bontu Fufa Balcha,Debora Taye Tesfaye,Elbethel Daniel Zewdie,Ikram Behiru Nesiru,Jitu Ewnetu Hailu,Senait Mengesha Yayo*

Main category: cs.CL

TL;DR: 本研究调查了三种低资源语言（阿凡奥罗莫语、阿姆哈拉语和提格里尼亚语）的机器翻译 (MT) 数据集的质量，重点关注数据集中的性别代表性。


<details>
  <summary>Details</summary>
Motivation: 随着低资源语言越来越多地被纳入 NLP 研究，人们越来越重视收集大规模数据集。但在优先考虑数量而非质量时，我们面临着 1) 构建的语言技术对于这些语言的性能不佳，以及 2) 产生有害内容，从而使社会偏见永久化 的风险。

Method: 研究人员调查了三种低资源语言（阿凡奥罗莫语、阿姆哈拉语和提格里尼亚语）的机器翻译 (MT) 数据集的质量，重点关注数据集中的性别代表性。

Result: 研究结果表明，虽然训练数据大量代表政治和宗教领域文本，但基准数据集侧重于新闻、健康和体育。 我们还发现男性性别存在很大的偏差——在人名、动词的语法性别以及数据集中刻板印象的描述中。 此外，我们发现针对女性的有害和有毒描述，对于数据量最大的语言来说，这些描述更为突出，这 подчеркивает 数量并不能保证质量。

Conclusion: 我们希望我们的工作能够激发人们对为低资源语言收集的数据集进行进一步的询问，并促使人们及早缓解有害内容。

Abstract: As low-resourced languages are increasingly incorporated into NLP research,
there is an emphasis on collecting large-scale datasets. But in prioritizing
quantity over quality, we risk 1) building language technologies that perform
poorly for these languages and 2) producing harmful content that perpetuates
societal biases. In this paper, we investigate the quality of Machine
Translation (MT) datasets for three low-resourced languages--Afan Oromo,
Amharic, and Tigrinya, with a focus on the gender representation in the
datasets. Our findings demonstrate that while training data has a large
representation of political and religious domain text, benchmark datasets are
focused on news, health, and sports. We also found a large skew towards the
male gender--in names of persons, the grammatical gender of verbs, and in
stereotypical depictions in the datasets. Further, we found harmful and toxic
depictions against women, which were more prominent for the language with the
largest amount of data, underscoring that quantity does not guarantee quality.
We hope that our work inspires further inquiry into the datasets collected for
low-resourced languages and prompts early mitigation of harmful content.
WARNING: This paper contains discussion of NSFW content that some may find
disturbing.

</details>


### [10] [GRAD: Graph-Retrieved Adaptive Decoding for Hallucination Mitigation](https://arxiv.org/abs/2511.03900)
*Manh Nguyen,Sunil Gupta,Dai Do,Hung Le*

Main category: cs.CL

TL;DR: 提出了一种名为Graph-Retrieved Adaptive Decoding (GRAD) 的解码方法，用于减轻大型语言模型 (LLM) 的幻觉问题，该方法通过在解码时将生成过程与从语料库中检索到的证据相关联，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 现有的幻觉缓解方法依赖外部知识源，但基于提示的方法脆弱且对领域敏感，而符号知识集成则会产生繁重的检索和格式化成本。因此，研究的动机在于寻找一种更轻量级、即插即用的替代方案。

Method: GRAD通过在单个前向传递中累积检索到的语料库中的下一个token logits来构建一个稀疏token转换图。在解码过程中，graph-retrieved logits经过最大归一化处理，并与模型logits自适应融合，以支持高证据延续，同时保持流畅性。

Result: 在三个模型和一系列涵盖内在、外在幻觉和事实性任务的问答基准测试中，GRAD始终优于基线，与贪婪解码相比，内在准确率提高了9.7$\\%$，幻觉率降低了8.6$\\%$，正确性提高了6.9$\\%$，并在所有方法中获得了最高的真值-信息量乘积得分。

Conclusion: GRAD提供了一种轻量级、即插即用的替代方案，可以有效引导生成过程产生更真实和可验证的输出。

Abstract: Hallucination mitigation remains a persistent challenge for large language
models (LLMs), even as model scales grow. Existing approaches often rely on
external knowledge sources, such as structured databases or knowledge graphs,
accessed through prompting or retrieval. However, prompt-based grounding is
fragile and domain-sensitive, while symbolic knowledge integration incurs heavy
retrieval and formatting costs. Motivated by knowledge graphs, we introduce
Graph-Retrieved Adaptive Decoding (GRAD), a decoding-time method that grounds
generation in corpus-derived evidence without retraining. GRAD constructs a
sparse token transition graph by accumulating next-token logits across a small
retrieved corpus in a single forward pass. During decoding, graph-retrieved
logits are max-normalized and adaptively fused with model logits to favor
high-evidence continuations while preserving fluency. Across three models and a
range of question-answering benchmarks spanning intrinsic, extrinsic
hallucination, and factuality tasks, GRAD consistently surpasses baselines,
achieving up to 9.7$\%$ higher intrinsic accuracy, 8.6$\%$ lower hallucination
rates, and 6.9$\%$ greater correctness compared to greedy decoding, while
attaining the highest truth--informativeness product score among all methods.
GRAD offers a lightweight, plug-and-play alternative to contrastive decoding
and knowledge graph augmentation, demonstrating that statistical evidence from
corpus-level token transitions can effectively steer generation toward more
truthful and verifiable outputs.

</details>


### [11] [Context informs pragmatic interpretation in vision-language models](https://arxiv.org/abs/2511.03908)
*Alvin Wei Ming Tan,Ben Prystawski,Veronica Boyce,Michael C. Frank*

Main category: cs.CL

TL;DR: 本文研究了在迭代指称游戏中，智能体在多轮语言环境中执行上下文敏感的语用推理的能力。


<details>
  <summary>Details</summary>
Motivation: 研究动机是测试智能体在迭代指称游戏中利用上下文进行语用推理的能力。

Method: 通过在迭代指称游戏中测试人类和视觉-语言模型，并改变给定的上下文（数量、顺序和相关性）来评估。

Result: 结果表明，在缺乏相关上下文的情况下，模型表现优于随机水平，但远不如人类。然而，在提供相关上下文后，模型性能在试验中显著提高。

Conclusion: 结论是，对于机器学习模型来说，使用抽象指称物的少样本指称游戏仍然是一项艰巨的任务。

Abstract: Iterated reference games - in which players repeatedly pick out novel
referents using language - present a test case for agents' ability to perform
context-sensitive pragmatic reasoning in multi-turn linguistic environments. We
tested humans and vision-language models on trials from iterated reference
games, varying the given context in terms of amount, order, and relevance.
Without relevant context, models were above chance but substantially worse than
humans. However, with relevant context, model performance increased
dramatically over trials. Few-shot reference games with abstract referents
remain a difficult task for machine learning models.

</details>


### [12] [The Human Flourishing Geographic Index: A County-Level Dataset for the United States, 2013--2023](https://arxiv.org/abs/2511.03915)
*Stefano M. Iacus,Devika Jain,Andrea Nasuto,Giuseppe Porro,Marcello Carammia,Andrea Vezzulli*

Main category: cs.CL

TL;DR: 本文介绍了一种新的人类繁荣地理指数(HFGI)，该指数通过分析美国地理定位的推文来量化人类的繁荣程度。


<details>
  <summary>Details</summary>
Motivation: 现有的幸福衡量标准缺乏精细的时空分辨率。

Method: 利用大型语言模型分析了大约 26 亿条美国地理定位的推文(2013-2023)，对与哈佛全球繁荣研究框架相关的 48 个指标以及对移民的态度和腐败的看法进行分类。

Result: 该数据集提供了月度和年度县和州级别与繁荣相关的话语指标，经验证证实这些指标准确地代表了潜在的结构，并显示出与已建立指标的预期相关性。

Conclusion: 该资源能够对前所未有的分辨率的福祉、不平等和社会变革进行多学科分析，从而深入了解过去十年中美国社交媒体言论中反映的人类繁荣动态。

Abstract: Quantifying human flourishing, a multidimensional construct including
happiness, health, purpose, virtue, relationships, and financial stability, is
critical for understanding societal well-being beyond economic indicators.
Existing measures often lack fine spatial and temporal resolution. Here we
introduce the Human Flourishing Geographic Index (HFGI), derived from analyzing
approximately 2.6 billion geolocated U.S. tweets (2013-2023) using fine-tuned
large language models to classify expressions across 48 indicators aligned with
Harvard's Global Flourishing Study framework plus attitudes towards migration
and perception of corruption. The dataset offers monthly and yearly county- and
state-level indicators of flourishing-related discourse, validated to confirm
that the measures accurately represent the underlying constructs and show
expected correlations with established indicators. This resource enables
multidisciplinary analyses of well-being, inequality, and social change at
unprecedented resolution, offering insights into the dynamics of human
flourishing as reflected in social media discourse across the United States
over the past decade.

</details>


### [13] [Direct Semantic Communication Between Large Language Models via Vector Translation](https://arxiv.org/abs/2511.03945)
*Fu-Chun Yang,Jason Eshraghian*

Main category: cs.CL

TL;DR: 大型语言模型在多智能体环境中传递信息时丢弃了大部分潜在语义，限制了信息传递并增加了计算开销。本文提出了一种基于向量转换的潜在桥梁方法，实现不同模型间直接的语义交换。


<details>
  <summary>Details</summary>
Motivation: 在多智能体环境中，大型语言模型的信息传递方式存在局限性，缺乏有效的语义共享机制。

Method: 通过学习到的映射，使用双编码器在Llama-2-7B和Mistral-7B-Instruct之间进行训练，实现向量转换，从而建立潜在桥梁。

Result: Llama-2-7B和Mistral-7B-Instruct的平均余弦对齐度达到0.538。以30%的混合强度注入转换后的向量可以在不破坏logits稳定性的前提下引导目标模型的生成。双向评估显示出2.01:1的传输不对称性，表明通用模型比指令调整变体产生更多的可传输表示。

Conclusion: 跨模型潜在通信是可行的，从而实现了共享意义而非tokens的协作AI系统。

Abstract: In multi-agent settings, such as debate, reflection, or tool-calling, large
language models (LLMs) pass messages as plain tokens, discarding most latent
semantics. This constrains information transfer and adds unnecessary
computational overhead. We form a latent bridge via vector translations, which
use learned mappings that enable direct semantic exchange between
representation spaces. A dual-encoder translator trained between Llama-2-7B and
Mistral-7B-Instruct attains an average cosine alignment of 0.538. Injecting the
translated vectors at 30 percent blending strength steers the target model's
generation without destabilizing logits. Bidirectional evaluation shows a
2.01:1 transfer asymmetry, indicating that general-purpose models yield more
transferable representations than instruction-tuned variants. This conservative
injection preserves computational stability while demonstrating that
cross-model latent communication is feasible, enabling collaborative AI systems
that share meaning rather than tokens.

</details>


### [14] [Abductive Inference in Retrieval-Augmented Language Models: Generating and Validating Missing Premises](https://arxiv.org/abs/2511.04020)
*Shiyin Lin*

Main category: cs.CL

TL;DR: 这篇论文提出了一种将溯因推理融入检索增强LLM的框架，以解决检索证据不完整的问题。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成（RAG）在知识密集型任务中表现出色，但当检索到的证据不完整时会失败，导致推理过程存在差距。溯因推理提供了一种弥合这些差距的方法。

Method: 该方法检测证据不足的情况，生成候选的缺失前提，并通过一致性和合理性检查来验证它们。

Result: 在溯因推理和多跳QA基准测试中的实验结果表明，该方法提高了答案的准确性和推理的可靠性。

Conclusion: 这项工作强调了溯因推理是提高RAG系统鲁棒性和可解释性的一个有希望的方向。

Abstract: Large Language Models (LLMs) enhanced with retrieval -- commonly referred to
as Retrieval-Augmented Generation (RAG) -- have demonstrated strong performance
in knowledge-intensive tasks. However, RAG pipelines often fail when retrieved
evidence is incomplete, leaving gaps in the reasoning process. In such cases,
\emph{abductive inference} -- the process of generating plausible missing
premises to explain observations -- offers a principled approach to bridge
these gaps. In this paper, we propose a framework that integrates abductive
inference into retrieval-augmented LLMs. Our method detects insufficient
evidence, generates candidate missing premises, and validates them through
consistency and plausibility checks. Experimental results on abductive
reasoning and multi-hop QA benchmarks show that our approach improves both
answer accuracy and reasoning faithfulness. This work highlights abductive
inference as a promising direction for enhancing the robustness and
explainability of RAG systems.

</details>


### [15] [WST: Weakly Supervised Transducer for Automatic Speech Recognition](https://arxiv.org/abs/2511.04035)
*Dongji Gao,Chenda Liao,Changliang Liu,Matthew Wiesner,Leibny Paola Garcia,Daniel Povey,Sanjeev Khudanpur,Jian Wu*

Main category: cs.CL

TL;DR: 提出了一种弱监督Transducer（WST），它集成了灵活的训练图，可以稳健地处理转录中的错误，而无需额外的置信度估计或辅助预训练模型。


<details>
  <summary>Details</summary>
Motivation: 循环神经网络-Transducer (RNN-T) 广泛应用于端到端 (E2E) 自动语音识别 (ASR) 任务中，但严重依赖于大规模、高质量的注释数据，而这些数据通常成本高昂且难以获得。

Method: 设计了一个灵活的训练图来处理转录中的错误，无需额外的置信度估计或辅助预训练模型。

Result: 在合成和工业数据集上的经验评估表明，即使转录错误率高达 70%，WST 也能有效保持性能，并且始终优于现有的基于连接主义时间分类 (CTC) 的弱监督方法，例如旁路时间分类 (BTC) 和全时分类 (OTC)。

Conclusion: WST 在实际 ASR 设置中具有实用性和鲁棒性。

Abstract: The Recurrent Neural Network-Transducer (RNN-T) is widely adopted in
end-to-end (E2E) automatic speech recognition (ASR) tasks but depends heavily
on large-scale, high-quality annotated data, which are often costly and
difficult to obtain. To mitigate this reliance, we propose a Weakly Supervised
Transducer (WST), which integrates a flexible training graph designed to
robustly handle errors in the transcripts without requiring additional
confidence estimation or auxiliary pre-trained models. Empirical evaluations on
synthetic and industrial datasets reveal that WST effectively maintains
performance even with transcription error rates of up to 70%, consistently
outperforming existing Connectionist Temporal Classification (CTC)-based weakly
supervised approaches, such as Bypass Temporal Classification (BTC) and
Omni-Temporal Classification (OTC). These results demonstrate the practical
utility and robustness of WST in realistic ASR settings. The implementation
will be publicly available.

</details>


### [16] [T-FIX: Text-Based Explanations with Features Interpretable to eXperts](https://arxiv.org/abs/2511.04070)
*Shreya Havaldar,Helen Jin,Chaehyeon Kim,Anton Xue,Weiqiu You,Marco Gatti,Bhuvnesh Jain,Helen Qu,Daniel A Hashimoto,Amin Madani,Rajat Deo,Sameed Ahmed M. Khatana,Gary E. Weissman,Lyle Ungar,Eric Wong*

Main category: cs.CL

TL;DR: 当前的LLM解释评估方法未能充分捕捉解释内容与专家直觉的对齐程度。


<details>
  <summary>Details</summary>
Motivation: 在知识密集型领域，用户不仅需要答案，更需要有意义的解释，并且这些解释应反映专家级别的推理。

Method: 提出了T-FIX，一个跨越七个知识密集型领域的基准，用于评估解释的专家对齐程度。

Result: 开发了新的指标来衡量LLM解释与专家判断的对齐程度。

Conclusion: 形式化了专家对齐作为评估解释的标准。

Abstract: As LLMs are deployed in knowledge-intensive settings (e.g., surgery,
astronomy, therapy), users expect not just answers, but also meaningful
explanations for those answers. In these settings, users are often domain
experts (e.g., doctors, astrophysicists, psychologists) who require
explanations that reflect expert-level reasoning. However, current evaluation
schemes primarily emphasize plausibility or internal faithfulness of the
explanation, which fail to capture whether the content of the explanation truly
aligns with expert intuition. We formalize expert alignment as a criterion for
evaluating explanations with T-FIX, a benchmark spanning seven
knowledge-intensive domains. In collaboration with domain experts, we develop
novel metrics to measure the alignment of LLM explanations with expert
judgment.

</details>


### [17] [Plan of Knowledge: Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering](https://arxiv.org/abs/2511.04072)
*Xinying Qian,Ying Zhang,Yu Zhao,Baohang Zhou,Xuhui Sui,Xiaojie Yuan*

Main category: cs.CL

TL;DR: 这篇论文提出了一种名为PoK的框架，用于解决时序知识图谱问答（TKGQA）中大型语言模型（LLM）在时间推理方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 以往的研究未能充分理解时间约束的复杂语义信息，并且LLM存在幻觉和知识匮乏的问题。

Method: 该框架包括知识规划模块和对比时序检索器。知识规划模块将复杂的时间问题分解为一系列子目标，而对比检索框架则用于从TKG中选择性地检索语义和时间对齐的事实。

Result: 在四个基准TKGQA数据集上的大量实验表明，PoK显著提高了LLM的检索精度和推理准确性，最多超过了最先进的TKGQA方法56.0%。

Conclusion: 通过结合结构化规划和时间知识检索，PoK有效地增强了时间推理的可解释性和事实一致性。

Abstract: Temporal Knowledge Graph Question Answering (TKGQA) aims to answer
time-sensitive questions by leveraging factual information from Temporal
Knowledge Graphs (TKGs). While previous studies have employed pre-trained TKG
embeddings or graph neural networks to inject temporal knowledge, they fail to
fully understand the complex semantic information of time constraints.
Recently, Large Language Models (LLMs) have shown remarkable progress,
benefiting from their strong semantic understanding and reasoning
generalization capabilities. However, their temporal reasoning ability remains
limited. LLMs frequently suffer from hallucination and a lack of knowledge. To
address these limitations, we propose the Plan of Knowledge framework with a
contrastive temporal retriever, which is named PoK. Specifically, the proposed
Plan of Knowledge module decomposes a complex temporal question into a sequence
of sub-objectives from the pre-defined tools, serving as intermediate guidance
for reasoning exploration. In parallel, we construct a Temporal Knowledge Store
(TKS) with a contrastive retrieval framework, enabling the model to selectively
retrieve semantically and temporally aligned facts from TKGs. By combining
structured planning with temporal knowledge retrieval, PoK effectively enhances
the interpretability and factual consistency of temporal reasoning. Extensive
experiments on four benchmark TKGQA datasets demonstrate that PoK significantly
improves the retrieval precision and reasoning accuracy of LLMs, surpassing the
performance of the state-of-the-art TKGQA methods by 56.0% at most.

</details>


### [18] [The truth is no diaper: Human and AI-generated associations to emotional words](https://arxiv.org/abs/2511.04077)
*Špela Vintar,Jan Jona Javoršek*

Main category: cs.CL

TL;DR: 这篇论文比较了人类和大型语言模型在词语联想上的行为。研究发现，两者之间存在一定的重叠，但大型语言模型的联想更倾向于放大刺激的情感负载，并且比人类的联想更可预测和缺乏创造性。


<details>
  <summary>Details</summary>
Motivation: 研究人类的词语联想是为了深入了解内部心理词典，但人类的反应可能受到个人经历、情绪或认知风格的影响。形成看似无关概念之间的联想能力可能是创造力的驱动机制。因此，本研究旨在比较人类和大型语言模型在词语联想上的行为。

Method: 研究比较了人类和大型语言模型对带有情感色彩的词语的联想。

Result: 研究发现，人类和大型语言模型在词语联想上存在适度的重叠。大型语言模型的联想倾向于放大刺激的情感负载，并且比人类的联想更可预测和缺乏创造性。

Conclusion: 大型语言模型在词语联想上与人类存在差异，它们更倾向于放大情感负载，缺乏创造性。

Abstract: Human word associations are a well-known method of gaining insight into the
internal mental lexicon, but the responses spontaneously offered by human
participants to word cues are not always predictable as they may be influenced
by personal experience, emotions or individual cognitive styles. The ability to
form associative links between seemingly unrelated concepts can be the driving
mechanisms of creativity. We perform a comparison of the associative behaviour
of humans compared to large language models. More specifically, we explore
associations to emotionally loaded words and try to determine whether large
language models generate associations in a similar way to humans. We find that
the overlap between humans and LLMs is moderate, but also that the associations
of LLMs tend to amplify the underlying emotional load of the stimulus, and that
they tend to be more predictable and less creative than human ones.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [19] [Improving Diagnostic Performance on Small and Imbalanced Datasets Using Class-Based Input Image Composition](https://arxiv.org/abs/2511.03891)
*Hlali Azzeddine,Majid Ben Yakhlef,Soulaiman El Hazzat*

Main category: cs.CV

TL;DR: 提出了基于类别的图像合成方法，通过融合同一类别的多个图像来增强类内方差，提高模型区分细微疾病模式的能力。


<details>
  <summary>Details</summary>
Motivation: 小型、不平衡的数据集和较差的输入图像质量会导致深度学习模型出现较高的错误预测率。

Method: 通过将同一类别的多个图像融合成组合视觉复合图像（CoImg）来重新构建训练输入，从而增强类内方差。

Result: 在OCTDL数据集上评估，使用VGG16模型，Co-OCTDL数据集实现了近乎完美的准确率（99.6%），F1-score（0.995）和AUC（0.9996），并且显著降低了错误预测率。

Conclusion: 该方法即使对于受类别不平衡或小样本量影响的弱数据集，也能产生高质量的预测结果。

Abstract: Small, imbalanced datasets and poor input image quality can lead to high
false predictions rates with deep learning models. This paper introduces
Class-Based Image Composition, an approach that allows us to reformulate
training inputs through a fusion of multiple images of the same class into
combined visual composites, named Composite Input Images (CoImg). That enhances
the intra-class variance and improves the valuable information density per
training sample and increases the ability of the model to distinguish between
subtle disease patterns. Our method was evaluated on the Optical Coherence
Tomography Dataset for Image-Based Deep Learning Methods (OCTDL) (Kulyabin et
al., 2024), which contains 2,064 high-resolution optical coherence tomography
(OCT) scans of the human retina, representing seven distinct diseases with a
significant class imbalance. We constructed a perfectly class-balanced version
of this dataset, named Co-OCTDL, where each scan is resented as a 3x1 layout
composite image. To assess the effectiveness of this new representation, we
conducted a comparative analysis between the original dataset and its variant
using a VGG16 model. A fair comparison was ensured by utilizing the identical
model architecture and hyperparameters for all experiments. The proposed
approach markedly improved diagnostic results.The enhanced Dataset achieved
near-perfect accuracy (99.6%) with F1-score (0.995) and AUC (0.9996), compared
to a baseline model trained on raw dataset. The false prediction rate was also
significantly lower, this demonstrates that the method can producehigh-quality
predictions even for weak datasets affected by class imbalance or small sample
size.

</details>


### [20] [LoRA-Edge: Tensor-Train-Assisted LoRA for Practical CNN Fine-Tuning on Edge Devices](https://arxiv.org/abs/2511.03765)
*Hyunseok Kwak,Kyeongwon Lee,Jae-Jin Lee,Woojoo Lee*

Main category: cs.CV

TL;DR: LoRA-Edge: A parameter-efficient fine-tuning method for CNNs on edge devices, using Tensor-Train SVD and selective updates.


<details>
  <summary>Details</summary>
Motivation: Full fine-tuning of CNNs on edge devices is infeasible due to resource constraints, but on-device fine-tuning is needed to address domain shift.

Method: Applies Tensor-Train SVD to pre-trained convolutional layers, selectively updates the output-side core with zero-initialization, and fuses the updates back into dense kernels.

Result: Achieves accuracy within 4.7% of full fine-tuning while updating at most 1.49% of parameters, outperforming other methods. Also achieves 1.4-3.8x faster convergence.

Conclusion: LoRA-Edge enables practical on-device CNN adaptation for edge platforms by being structure-aligned and parameter-efficient.

Abstract: On-device fine-tuning of CNNs is essential to withstand domain shift in edge
applications such as Human Activity Recognition (HAR), yet full fine-tuning is
infeasible under strict memory, compute, and energy budgets. We present
LoRA-Edge, a parameter-efficient fine-tuning (PEFT) method that builds on
Low-Rank Adaptation (LoRA) with tensor-train assistance. LoRA-Edge (i) applies
Tensor-Train Singular Value Decomposition (TT-SVD) to pre-trained convolutional
layers, (ii) selectively updates only the output-side core with
zero-initialization to keep the auxiliary path inactive at the start, and (iii)
fuses the update back into dense kernels, leaving inference cost unchanged.
This design preserves convolutional structure and reduces the number of
trainable parameters by up to two orders of magnitude compared to full
fine-tuning. Across diverse HAR datasets and CNN backbones, LoRA-Edge achieves
accuracy within 4.7% of full fine-tuning while updating at most 1.49% of
parameters, consistently outperforming prior parameter-efficient baselines
under similar budgets. On a Jetson Orin Nano, TT-SVD initialization and
selective-core training yield 1.4-3.8x faster convergence to target F1.
LoRA-Edge thus makes structure-aligned, parameter-efficient on-device CNN
adaptation practical for edge platforms.

</details>


### [21] [SILVI: Simple Interface for Labeling Video Interactions](https://arxiv.org/abs/2511.03819)
*Ozan Kanbertay,Richard Vogg,Elif Karakoc,Peter M. Kappeler,Claudia Fichtel,Alexander S. Ecker*

Main category: cs.CV

TL;DR: SILVI是一个开源标注软件，集成了行为标注和个体定位功能，可以用于动物行为和人类互动分析，并生成结构化输出以训练和验证计算机视觉模型。


<details>
  <summary>Details</summary>
Motivation: 现有的开源标注工具要么支持行为标注但无法定位个体，要么支持个体定位但无法捕捉互动，为了弥补这一差距。

Method: 提出了SILVI，一个集成了行为标注和个体定位功能的开源标注软件。

Result: SILVI能够直接在视频数据中标注行为和互动，生成结构化输出，适用于训练和验证计算机视觉模型。通过连接行为生态学和计算机视觉，SILVI有助于开发用于细粒度行为分析的自动化方法。

Conclusion: SILVI主要在动物行为的背景下开发，但更广泛地适用于注释需要提取动态场景图的其他视频中的人际互动。

Abstract: Computer vision methods are increasingly used for the automated analysis of
large volumes of video data collected through camera traps, drones, or direct
observations of animals in the wild. While recent advances have focused
primarily on detecting individual actions, much less work has addressed the
detection and annotation of interactions -- a crucial aspect for understanding
social and individualized animal behavior. Existing open-source annotation
tools support either behavioral labeling without localization of individuals,
or localization without the capacity to capture interactions. To bridge this
gap, we present SILVI, an open-source labeling software that integrates both
functionalities. SILVI enables researchers to annotate behaviors and
interactions directly within video data, generating structured outputs suitable
for training and validating computer vision models. By linking behavioral
ecology with computer vision, SILVI facilitates the development of automated
approaches for fine-grained behavioral analyses. Although developed primarily
in the context of animal behavior, SILVI could be useful more broadly to
annotate human interactions in other videos that require extracting dynamic
scene graphs. The software, along with documentation and download instructions,
is available at: https://gitlab.gwdg.de/kanbertay/interaction-labelling-app.

</details>


### [22] [Noise Injection: Improving Out-of-Distribution Generalization for Limited Size Datasets](https://arxiv.org/abs/2511.03855)
*Duong Mai,Lawrence Hall*

Main category: cs.CV

TL;DR: 深度学习模型在图像识别方面泛化能力差，尤其是在COVID-19胸部X光检测中。模型学习利用源特定伪像，而非生物标记物。通过在训练期间注入噪声，可以显著提高模型对分布偏移的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 模型在胸部X光检测中无法推广到训练集中未包含的新临床来源的分布外(OOD)数据，因为它们学习利用源特定伪像，而不是合理的生物标志物。

Method: 研究调查了在训练期间使用基本噪声注入技术（高斯、散斑、泊松和盐和胡椒）。

Result: 噪声注入技术可以显著缩小ID和OOD评估之间的性能差距，从0.10-0.20降至0.01-0.06。

Conclusion: 噪声注入提高了模型对分布偏移的鲁棒性。

Abstract: Deep learned (DL) models for image recognition have been shown to fail to
generalize to data from different devices, populations, etc. COVID-19 detection
from Chest X-rays (CXRs), in particular, has been shown to fail to generalize
to out-of-distribution (OOD) data from new clinical sources not covered in the
training set. This occurs because models learn to exploit shortcuts -
source-specific artifacts that do not translate to new distributions - rather
than reasonable biomarkers to maximize performance on in-distribution (ID)
data. Rendering the models more robust to distribution shifts, our study
investigates the use of fundamental noise injection techniques (Gaussian,
Speckle, Poisson, and Salt and Pepper) during training. Our empirical results
demonstrate that this technique can significantly reduce the performance gap
between ID and OOD evaluation from 0.10-0.20 to 0.01-0.06, based on results
averaged over ten random seeds across key metrics such as AUC, F1, accuracy,
recall and specificity. Our source code is publicly available at
https://github.com/Duongmai127/Noisy-ood

</details>


### [23] [Investigating Robot Control Policy Learning for Autonomous X-ray-guided Spine Procedures](https://arxiv.org/abs/2511.03882)
*Florence Klitzner,Blanca Inigo,Benjamin D. Killeen,Lalithkumar Seenivasan,Michelle Song,Axel Krieger,Mathias Unberath*

Main category: cs.CV

TL;DR: 本研究探讨了基于模仿学习的机器人控制策略在X射线引导的手术中的应用，特别是在脊柱器械植入方面。通过模拟和实验，验证了该方法在导管插入中的可行性，并指出了其局限性，为未来的研究提供了方向。


<details>
  <summary>Details</summary>
Motivation: 目前尚不清楚基于模仿学习的机器人控制策略是否适用于X射线引导的手术，例如脊柱器械植入，因为多角度X射线的解读很复杂。

Method: 1. 开发了一个用于X射线引导脊柱手术的可扩展、自动化模拟的计算机沙箱。
2. 整理了一个包含正确轨迹和相应双平面X射线序列的数据集，模拟医生的逐步对齐。
3. 训练模仿学习策略，用于规划和开环控制，仅基于视觉信息迭代对齐套管。

Result: 该策略在68.5%的案例中首次尝试成功，并在不同的椎体水平上保持安全的椎弓根内轨迹。该策略推广到复杂的解剖结构，包括骨折，并且对不同的初始化保持稳健。在真实的双平面X射线上的实验表明，该模型可以产生合理的轨迹，尽管完全在模拟中进行训练。

Conclusion: 初步结果很有希望，但也存在局限性，尤其是在入口点精度方面。完整的闭环控制将需要更多关于如何提供足够频繁的反馈的考虑。通过更强大的先验知识和领域知识，这些模型可以为未来在轻量级和无CT的机器人术中脊柱导航方面的工作奠定基础。

Abstract: Imitation learning-based robot control policies are enjoying renewed interest
in video-based robotics. However, it remains unclear whether this approach
applies to X-ray-guided procedures, such as spine instrumentation. This is
because interpretation of multi-view X-rays is complex. We examine
opportunities and challenges for imitation policy learning in bi-plane-guided
cannula insertion. We develop an in silico sandbox for scalable, automated
simulation of X-ray-guided spine procedures with a high degree of realism. We
curate a dataset of correct trajectories and corresponding bi-planar X-ray
sequences that emulate the stepwise alignment of providers. We then train
imitation learning policies for planning and open-loop control that iteratively
align a cannula solely based on visual information. This precisely controlled
setup offers insights into limitations and capabilities of this method. Our
policy succeeded on the first attempt in 68.5% of cases, maintaining safe
intra-pedicular trajectories across diverse vertebral levels. The policy
generalized to complex anatomy, including fractures, and remained robust to
varied initializations. Rollouts on real bi-planar X-rays further suggest that
the model can produce plausible trajectories, despite training exclusively in
simulation. While these preliminary results are promising, we also identify
limitations, especially in entry point precision. Full closed-look control will
require additional considerations around how to provide sufficiently frequent
feedback. With more robust priors and domain knowledge, such models may provide
a foundation for future efforts toward lightweight and CT-free robotic
intra-operative spinal navigation.

</details>


### [24] [Desert Waste Detection and Classification Using Data-Based and Model-Based Enhanced YOLOv12 DL Model](https://arxiv.org/abs/2511.03888)
*Abdulmumin Sa'ad,Sulaimon Oyeniyi Adebayo,Abdul Jabbar Siddiqui*

Main category: cs.CV

TL;DR: 这篇论文提出了一种用于沙漠环境中实时垃圾检测的增强型目标检测框架，该框架基于YOLOv12的轻量级版本，并结合了自对抗训练（SAT）和专门的数据增强策略。


<details>
  <summary>Details</summary>
Motivation: 传统垃圾收集方法在偏远或恶劣环境中（如沙漠）劳动强度大、效率低且通常具有危险性。现有的计算机视觉和深度学习研究主要集中在城市环境和可回收材料上，忽略了有机和有害废物以及未充分勘探的地形（如沙漠）。

Method: 该研究提出了一种增强的实时目标检测框架，该框架基于YOLOv12的修剪轻量级版本，并结合了自对抗训练（SAT）和专门的数据增强策略。

Result: 该模型在DroneTrashNet数据集上表现出显着的精度、召回率和平均精度均值（mAP）的提高，同时实现了低延迟和紧凑的模型尺寸，适合部署在资源受限的空中无人机上。与其他最先进的轻量级YOLO变体的基准测试进一步突出了其在准确性和效率方面的最佳平衡。

Conclusion: 研究结果验证了以数据为中心和以模型为中心的增强功能相结合，对于在沙漠环境中进行稳健的实时垃圾检测的有效性。

Abstract: The global waste crisis is escalating, with solid waste generation expected
to increase by 70% by 2050. Traditional waste collection methods, particularly
in remote or harsh environments like deserts, are labor-intensive, inefficient,
and often hazardous. Recent advances in computer vision and deep learning have
opened the door to automated waste detection systems, yet most research focuses
on urban environments and recyclable materials, overlooking organic and
hazardous waste and underexplored terrains such as deserts. In this work, we
propose an enhanced real-time object detection framework based on a pruned,
lightweight version of YOLOv12 integrated with Self-Adversarial Training (SAT)
and specialized data augmentation strategies. Using the DroneTrashNet dataset,
we demonstrate significant improvements in precision, recall, and mean average
precision (mAP), while achieving low latency and compact model size suitable
for deployment on resource-constrained aerial drones. Benchmarking our model
against state-of-the-art lightweight YOLO variants further highlights its
optimal balance of accuracy and efficiency. Our results validate the
effectiveness of combining data-centric and model-centric enhancements for
robust, real-time waste detection in desert environments.

</details>


### [25] [I Detect What I Don't Know: Incremental Anomaly Learning with Stochastic Weight Averaging-Gaussian for Oracle-Free Medical Imaging](https://arxiv.org/abs/2511.03912)
*Nand Kumar Yadav,Rodrigue Rizk,William CW Chen,KC Santosh*

Main category: cs.CV

TL;DR: 提出了一种用于医学影像中未知异常检测的无监督框架，该框架通过逐步扩展可信的正常样本集来工作，无需任何异常标签。


<details>
  <summary>Details</summary>
Motivation: 医学影像中带标签的异常样本稀缺，且专家监督成本高昂，因此未知异常检测仍然是一个根本性的挑战。

Method: 该方法从一小部分经过验证的正常图像种子开始，交替进行轻量级适配器更新和不确定性门控样本准入。利用冻结的预训练视觉主干，并通过微小的卷积适配器进行增强，确保快速的领域适应性，且计算开销可忽略不计。提取的嵌入存储在一个紧凑的coreset中，从而实现高效的k近邻异常(k-NN)评分。通过双重概率门执行增量扩展期间的安全性，只有当样本到现有coreset的距离位于校准的z-score阈值内，并且其基于SWAG的认知不确定性保持在种子校准的界限以下时，才允许将样本添加到正常内存中。

Result: 在COVID-CXR数据集上，ROC-AUC从0.9489提高到0.9982 (F1: 0.8048提高到0.9746)；在Pneumonia CXR数据集上，ROC-AUC从0.6834提高到0.8968；在Brain MRI ND-5数据集上，ROC-AUC从0.6041提高到0.7269, PR-AUC从0.7539提高到0.8211。

Conclusion: 该结果突出了所提出的框架在真实、标签稀缺的医学影像应用中的有效性和效率。

Abstract: Unknown anomaly detection in medical imaging remains a fundamental challenge
due to the scarcity of labeled anomalies and the high cost of expert
supervision. We introduce an unsupervised, oracle-free framework that
incrementally expands a trusted set of normal samples without any anomaly
labels. Starting from a small, verified seed of normal images, our method
alternates between lightweight adapter updates and uncertainty-gated sample
admission. A frozen pretrained vision backbone is augmented with tiny
convolutional adapters, ensuring rapid domain adaptation with negligible
computational overhead. Extracted embeddings are stored in a compact coreset
enabling efficient k-nearest neighbor anomaly (k-NN) scoring. Safety during
incremental expansion is enforced by dual probabilistic gates, a sample is
admitted into the normal memory only if its distance to the existing coreset
lies within a calibrated z-score threshold, and its SWAG-based epistemic
uncertainty remains below a seed-calibrated bound. This mechanism prevents
drift and false inclusions without relying on generative reconstruction or
replay buffers. Empirically, our system steadily refines the notion of
normality as unlabeled data arrive, producing substantial gains over baselines.
On COVID-CXR, ROC-AUC improves from 0.9489 to 0.9982 (F1: 0.8048 to 0.9746); on
Pneumonia CXR, ROC-AUC rises from 0.6834 to 0.8968; and on Brain MRI ND-5,
ROC-AUC increases from 0.6041 to 0.7269 and PR-AUC from 0.7539 to 0.8211. These
results highlight the effectiveness and efficiency of the proposed framework
for real-world, label-scarce medical imaging applications.

</details>


### [26] [Adaptive Temporal Refinement: Continuous Depth Allocation and Distance Regression for Efficient Action Localization](https://arxiv.org/abs/2511.03943)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.CV

TL;DR: 本文提出了一种新的时间动作定位方法，该方法通过边界距离回归（BDR）和自适应时间细化（ATR）来提高边界检测的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 当前的时间动作定位方法在边界检测方面存在精度问题，并且计算效率不高。

Method: 本文提出了BDR和ATR两种方法。BDR通过有符号距离回归实现信息论最优定位，ATR通过连续深度选择实现计算分配。

Result: BDR在不同的架构上实现了1.8%到3.1%的mAP@0.7的提升。ATR在THUMOS14上以更少的计算量实现了更高的mAP@0.7。

Conclusion: 本文提出的方法在时间动作定位方面取得了显著的成果，并在四个基准测试中得到了验证。

Abstract: Temporal action localization requires precise boundary detection; however,
current methods apply uniform computation despite significant variations in
difficulty across boundaries. We present two complementary contributions.
First, Boundary Distance Regression (BDR) provides information-theoretically
optimal localization through signed-distance regression rather than
classification, achieving 43\% sharper boundary peaks. BDR retrofits to
existing methods with approximately 50 lines of code, yielding consistent 1.8
to 3.1\% mAP@0.7 improvements across diverse architectures. Second, Adaptive
Temporal Refinement (ATR) allocates computation via continuous depth selection
$\tau \in [0,1]$, enabling end-to-end differentiable optimization without
reinforcement learning. On THUMOS14, ATR achieves 56.5\% mAP@0.7 at 162G FLOPs,
compared to 53.6\% at 198G for uniform processing, providing a 2.9\%
improvement with 18\% less compute. Gains scale with boundary heterogeneity,
showing 4.2\% improvement on short actions. Training cost is mitigated via
knowledge distillation, with lightweight students retaining 99\% performance at
baseline cost. Results are validated across four benchmarks with rigorous
statistical testing.

</details>


### [27] [Improving Multi-View Reconstruction via Texture-Guided Gaussian-Mesh Joint Optimization](https://arxiv.org/abs/2511.03950)
*Zhejia Cai,Puhua Jiang,Shiwei Mao,Hongkun Cao,Ruqi Huang*

Main category: cs.CV

TL;DR: 提出了一种新的框架，用于同时优化网格几何体（顶点位置和面）和顶点颜色，通过高斯引导的网格可微渲染，利用来自输入图像的光度一致性和来自法线和深度图的几何正则化。


<details>
  <summary>Details</summary>
Motivation: 从多视图图像重建真实世界的物体对于 3D 编辑、AR/VR 和数字内容创建中的应用至关重要。现有的方法通常优先考虑几何精度（多视图立体）或照片般逼真的渲染（新视图合成），通常将几何和外观优化分离，这阻碍了下游编辑任务。

Method: 通过高斯引导的网格可微渲染，同时优化网格几何体（顶点位置和面）和顶点颜色，利用来自输入图像的光度一致性和来自法线和深度图的几何正则化。

Result: 获得的高质量 3D 重建可以进一步用于下游编辑任务，例如重新照明和形状变形。

Conclusion: 倡导对几何和外观优化进行统一处理，以实现无缝的高斯网格联合优化。

Abstract: Reconstructing real-world objects from multi-view images is essential for
applications in 3D editing, AR/VR, and digital content creation. Existing
methods typically prioritize either geometric accuracy (Multi-View Stereo) or
photorealistic rendering (Novel View Synthesis), often decoupling geometry and
appearance optimization, which hinders downstream editing tasks. This paper
advocates an unified treatment on geometry and appearance optimization for
seamless Gaussian-mesh joint optimization. More specifically, we propose a
novel framework that simultaneously optimizes mesh geometry (vertex positions
and faces) and vertex colors via Gaussian-guided mesh differentiable rendering,
leveraging photometric consistency from input images and geometric
regularization from normal and depth maps. The obtained high-quality 3D
reconstruction can be further exploit in down-stream editing tasks, such as
relighting and shape deformation. The code will be publicly available upon
acceptance.

</details>


### [28] [A Linear Fractional Transformation Model and Calibration Method for Light Field Camera](https://arxiv.org/abs/2511.03962)
*Zhong Chen,Changfeng Chen*

Main category: cs.CV

TL;DR: 提出了一种新的光场相机标定方法，通过解耦主镜头和微透镜阵列来提高标定精度和效率。


<details>
  <summary>Details</summary>
Motivation: 精确标定内参数是光场相机三维重建的关键，但具有挑战性。

Method: 提出了一种线性分式变换（LFT）参数α来解耦主镜头和微透镜阵列，该方法包括基于最小二乘的解析解和非线性优化。

Result: 在真实和模拟数据上的实验结果验证了该方法的性能。基于该模型，原始光场图像的仿真速度更快。

Conclusion: 该方法提高了光场相机标定精度和效率，并为数据驱动的深度学习方法提供了便利。

Abstract: Accurate calibration of internal parameters is a crucial yet challenging
prerequisite for 3D reconstruction using light field cameras. In this paper, we
propose a linear fractional transformation(LFT) parameter $\alpha$ to decoupled
the main lens and micro lens array (MLA). The proposed method includes an
analytical solution based on least squares, followed by nonlinear refinement.
The method for detecting features from the raw images is also introduced.
Experimental results on both physical and simulated data have verified the
performance of proposed method. Based on proposed model, the simulation of raw
light field images becomes faster, which is crucial for data-driven deep
learning methods. The corresponding code can be obtained from the author's
website.

</details>


### [29] [Room Envelopes: A Synthetic Dataset for Indoor Layout Reconstruction from Images](https://arxiv.org/abs/2511.03970)
*Sam Bahrami,Dylan Campbell*

Main category: cs.CV

TL;DR: 本文提出了一种使用合成数据集Room Envelopes来预测场景结构元素的方法，该数据集包含RGB图像以及可见表面和结构布局的点云图。


<details>
  <summary>Details</summary>
Motivation: 现有的场景重建方法无法重建被遮挡的表面，而场景中的结构元素（如墙壁、地板和天花板）通常是平面、重复且简单的，因此更容易预测。

Method: 使用合成数据集Room Envelopes，该数据集包含RGB图像以及可见表面和结构布局的点云图，从而可以直接监督前馈单目几何估计器，以预测第一个可见表面和第一个布局表面。

Result: 该方法能够理解场景的范围以及其物体的形状和位置。

Conclusion: 该研究表明，通过预测场景的结构元素，可以更好地理解整个场景的结构和内容。

Abstract: Modern scene reconstruction methods are able to accurately recover 3D
surfaces that are visible in one or more images. However, this leads to
incomplete reconstructions, missing all occluded surfaces. While much progress
has been made on reconstructing entire objects given partial observations using
generative models, the structural elements of a scene, like the walls, floors
and ceilings, have received less attention. We argue that these scene elements
should be relatively easy to predict, since they are typically planar,
repetitive and simple, and so less costly approaches may be suitable. In this
work, we present a synthetic dataset -- Room Envelopes -- that facilitates
progress on this task by providing a set of RGB images and two associated
pointmaps for each image: one capturing the visible surface and one capturing
the first surface once fittings and fixtures are removed, that is, the
structural layout. As we show, this enables direct supervision for feed-forward
monocular geometry estimators that predict both the first visible surface and
the first layout surface. This confers an understanding of the scene's extent,
as well as the shape and location of its objects.

</details>


### [30] [Simple 3D Pose Features Support Human and Machine Social Scene Understanding](https://arxiv.org/abs/2511.03988)
*Wenshuo Qin,Leyla Isik*

Main category: cs.CV

TL;DR: 这篇论文研究了人类如何从视觉输入中识别社交互动，并发现3D姿势信息在其中起着关键作用，而目前的AI视觉模型通常缺乏这种信息。


<details>
  <summary>Details</summary>
Motivation: 目前对人类如何进行社交互动识别的计算机制了解不足，且AI视觉系统在此方面仍面临挑战。

Method: 该研究结合了先进的姿势和深度估计算法，提取了视频中人物的3D关节位置，并将其预测人类社交互动判断的能力与当前的AI视觉模型进行了比较。此外，还提取了一组简化的3D社交姿势特征，描述了视频中面部的3D位置和方向。

Result: 3D关节位置的表现优于大多数当前的AI视觉模型。简化的3D社交姿势特征与完整的3D关节具有相似的预测能力，并且在与AI视觉模型的嵌入相结合时，显著提高了模型的性能。模型中3D社交姿势特征的表示程度，预测了模型与人类社交判断的匹配能力。

Conclusion: 人类的社交场景理解依赖于3D姿势的显式表示，并且可以由简单的、结构化的视觉空间原语支持。

Abstract: Humans can quickly and effortlessly extract a variety of information about
others' social interactions from visual input, ranging from visuospatial cues
like whether two people are facing each other to higher-level information. Yet,
the computations supporting these abilities remain poorly understood, and
social interaction recognition continues to challenge even the most advanced AI
vision systems. Here, we hypothesized that humans rely on 3D visuospatial pose
information to make social interaction judgments, which is absent in most AI
vision models. To test this, we combined state-of-the-art pose and depth
estimation algorithms to extract 3D joint positions of people in short video
clips depicting everyday human actions and compared their ability to predict
human social interaction judgments with current AI vision models. Strikingly,
3D joint positions outperformed most current AI vision models, revealing that
key social information is available in explicit body position but not in the
learned features of most vision models, including even the layer-wise
embeddings of the pose models used to extract joint positions. To uncover the
critical pose features humans use to make social judgments, we derived a
compact set of 3D social pose features describing only the 3D position and
direction of faces in the videos. We found that these minimal descriptors
matched the predictive strength of the full set of 3D joints and significantly
improved the performance of off-the-shelf AI vision models when combined with
their embeddings. Moreover, the degree to which 3D social pose features were
represented in each off-the-shelf AI vision model predicted the model's ability
to match human social judgments. Together, our findings provide strong evidence
that human social scene understanding relies on explicit representations of 3D
pose and can be supported by simple, structured visuospatial primitives.

</details>


### [31] [CaRF: Enhancing Multi-View Consistency in Referring 3D Gaussian Splatting Segmentation](https://arxiv.org/abs/2511.03992)
*Yuwen Tao,Kanglei Zhou,Xin Tan,Yuan Xie*

Main category: cs.CV

TL;DR: 提出了一种名为相机感知Referring Field (CaRF)的框架，用于解决3D高斯分割中跨视角一致性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于2D渲染伪监督和特定视角的特征学习，导致跨视角一致性问题。

Method: CaRF框架直接在3D高斯空间中运行，引入了高斯场相机编码(GFCE)来结合相机几何信息，并提出了训练配对视角监督(ITPVS)来对齐跨视角的logits。

Result: 在Ref LERF, LERF OVS, 和 3D OVS数据集上，mIoU分别提高了16.8%, 4.3%, 和 2.0%。

Conclusion: CaRF框架能够实现更可靠和视角一致的3D场景理解，对具身智能、AR/VR交互和自主感知具有潜在的好处。

Abstract: Referring 3D Gaussian Splatting Segmentation (R3DGS) aims to interpret
free-form language expressions and localize the corresponding 3D regions in
Gaussian fields. While recent advances have introduced cross-modal alignment
between language and 3D geometry, existing pipelines still struggle with
cross-view consistency due to their reliance on 2D rendered pseudo supervision
and view specific feature learning. In this work, we present Camera Aware
Referring Field (CaRF), a fully differentiable framework that operates directly
in the 3D Gaussian space and achieves multi view consistency. Specifically,
CaRF introduces Gaussian Field Camera Encoding (GFCE), which incorporates
camera geometry into Gaussian text interactions to explicitly model view
dependent variations and enhance geometric reasoning. Building on this, In
Training Paired View Supervision (ITPVS) is proposed to align per Gaussian
logits across calibrated views during training, effectively mitigating single
view overfitting and exposing inter view discrepancies for optimization.
Extensive experiments on three representative benchmarks demonstrate that CaRF
achieves average improvements of 16.8%, 4.3%, and 2.0% in mIoU over state of
the art methods on the Ref LERF, LERF OVS, and 3D OVS datasets, respectively.
Moreover, this work promotes more reliable and view consistent 3D scene
understanding, with potential benefits for embodied AI, AR/VR interaction, and
autonomous perception.

</details>


### [32] [PhysCorr: Dual-Reward DPO for Physics-Constrained Text-to-Video Generation with Automated Preference Selection](https://arxiv.org/abs/2511.03997)
*Peiyao Wang,Weining Wang,Qi Li*

Main category: cs.CV

TL;DR: PhysCorr是一个统一的框架，用于建模、评估和优化视频生成中的物理一致性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频生成模型生成的视频违反了物理合理性原则，阻碍了其在具身人工智能、机器人和模拟密集型领域的部署。

Method: 提出了PhysicsRM，一种双维度奖励模型，用于量化物体内部稳定性和物体间的相互作用；并开发了PhyDPO，一种直接偏好优化管道，利用对比反馈和物理感知重加权来指导生成物理连贯的输出。

Result: PhysCorr在多个基准测试中显著提高了物理真实感，同时保持了视觉保真度和语义对齐。

Conclusion: 这项工作朝着物理基础和可信的视频生成迈出了关键一步。

Abstract: Recent advances in text-to-video generation have achieved impressive
perceptual quality, yet generated content often violates fundamental principles
of physical plausibility - manifesting as implausible object dynamics,
incoherent interactions, and unrealistic motion patterns. Such failures hinder
the deployment of video generation models in embodied AI, robotics, and
simulation-intensive domains. To bridge this gap, we propose PhysCorr, a
unified framework for modeling, evaluating, and optimizing physical consistency
in video generation. Specifically, we introduce PhysicsRM, the first
dual-dimensional reward model that quantifies both intra-object stability and
inter-object interactions. On this foundation, we develop PhyDPO, a novel
direct preference optimization pipeline that leverages contrastive feedback and
physics-aware reweighting to guide generation toward physically coherent
outputs. Our approach is model-agnostic and scalable, enabling seamless
integration into a wide range of video diffusion and transformer-based
backbones. Extensive experiments across multiple benchmarks demonstrate that
PhysCorr achieves significant improvements in physical realism while preserving
visual fidelity and semantic alignment. This work takes a critical step toward
physically grounded and trustworthy video generation.

</details>


### [33] [GNN-MoE: Context-Aware Patch Routing using GNNs for Parameter-Efficient Domain Generalization](https://arxiv.org/abs/2511.04008)
*Mahmoud Soliman,Omar Abdelaziz,Ahmed Radwan,Anand,Mohamed Shehata*

Main category: cs.CV

TL;DR: GNN-MoE: Uses a Graph Neural Network (GNN) router to dynamically assign image patches to specialized experts, improving domain generalization (DG) in Vision Transformers (ViTs) with parameter efficiency.


<details>
  <summary>Details</summary>
Motivation: Adapting pretrained ViTs for domain generalization (DG) is challenging because standard fine-tuning is costly and can impair generalization.

Method: Proposes GNN-MoE, which enhances Parameter-Efficient Fine-Tuning (PEFT) with a Mixture-of-Experts (MoE) framework using Kronecker adapters and a GNN router operating on inter-patch graphs.

Result: Achieves state-of-the-art or competitive DG benchmark performance with high parameter efficiency.

Conclusion: Graph-based contextual routing is useful for robust, lightweight domain generalization.

Abstract: Domain generalization (DG) seeks robust Vision Transformer (ViT) performance
on unseen domains. Efficiently adapting pretrained ViTs for DG is challenging;
standard fine-tuning is costly and can impair generalization. We propose
GNN-MoE, enhancing Parameter-Efficient Fine-Tuning (PEFT) for DG with a
Mixture-of-Experts (MoE) framework using efficient Kronecker adapters. Instead
of token-based routing, a novel Graph Neural Network (GNN) router (GCN, GAT,
SAGE) operates on inter-patch graphs to dynamically assign patches to
specialized experts. This context-aware GNN routing leverages inter-patch
relationships for better adaptation to domain shifts. GNN-MoE achieves
state-of-the-art or competitive DG benchmark performance with high parameter
efficiency, highlighting the utility of graph-based contextual routing for
robust, lightweight DG.

</details>


### [34] [MedDChest: A Content-Aware Multimodal Foundational Vision Model for Thoracic Imaging](https://arxiv.org/abs/2511.04016)
*Mahmoud Soliman,Islam Osman,Mohamed S. Shehata,Rasika Rajapakshe*

Main category: cs.CV

TL;DR: MedDChest是一个新的Vision Transformer (ViT)模型，专门为胸腔成像优化。


<details>
  <summary>Details</summary>
Motivation: 医学影像中的视觉模型性能通常受到在领域外自然图像上预训练的骨干网络微调的影响。为了解决这个根本的领域差距。

Method: 从头开始在超过120万张图像的大规模、精选的多模态数据集上预训练MedDChest，该数据集包含来自10个公共来源的不同模态，包括胸部X光和计算机断层扫描（CT）。

Result: MedDChest明显优于强大的、公开可用的ImageNet预训练模型。

Conclusion: 大规模的、领域内的预训练与领域特定的数据增强相结合，MedDChest提供了一个强大而稳健的特征提取器，可以作为一个更好的起点，用于各种胸腔诊断任务。

Abstract: The performance of vision models in medical imaging is often hindered by the
prevailing paradigm of fine-tuning backbones pre-trained on out-of-domain
natural images. To address this fundamental domain gap, we propose MedDChest, a
new foundational Vision Transformer (ViT) model optimized specifically for
thoracic imaging. We pre-trained MedDChest from scratch on a massive, curated,
multimodal dataset of over 1.2 million images, encompassing different
modalities including Chest X-ray and Computed Tomography (CT) compiled from 10
public sources. A core technical contribution of our work is Guided Random
Resized Crops, a novel content-aware data augmentation strategy that biases
sampling towards anatomically relevant regions, overcoming the inefficiency of
standard cropping techniques on medical scans. We validate our model's
effectiveness by fine-tuning it on a diverse set of downstream diagnostic
tasks. Comprehensive experiments empirically demonstrate that MedDChest
significantly outperforms strong, publicly available ImageNet-pretrained
models. By establishing the superiority of large-scale, in-domain pre-training
combined with domain-specific data augmentation, MedDChest provides a powerful
and robust feature extractor that serves as a significantly better starting
point for a wide array of thoracic diagnostic tasks. The model weights will be
made publicly available to foster future research and applications.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [35] [Scaling Agent Learning via Experience Synthesis](https://arxiv.org/abs/2511.03773)
*Zhaorun Chen,Zhuokai Zhao,Kai Zhang,Bo Liu,Qi Qi,Yifan Wu,Tarun Kalluri,Sara Cao,Yuanhao Xiong,Haibo Tong,Huaxiu Yao,Hengduo Li,Jiacheng Zhu,Xian Li,Dawn Song,Bo Li,Jason Weston,Dat Huynh*

Main category: cs.AI

TL;DR: DreamGym是一个统一的框架，旨在合成多样化的经验，并具有可扩展性，以实现自主代理的有效在线RL训练。


<details>
  <summary>Details</summary>
Motivation: 强化学习（RL）可以通过互动实现自我完善，从而增强大型语言模型（LLM）代理的能力，但由于昂贵的rollout、有限的任务多样性、不可靠的奖励信号和基础设施复杂性，其实际应用仍然具有挑战性，所有这些都阻碍了可扩展的经验数据的收集。

Method: DreamGym将环境动态提炼成一个基于推理的经验模型，该模型通过逐步推理得出一致的状态转换和反馈信号，从而实现RL的可扩展代理rollout收集。为了提高转换的稳定性和质量，DreamGym利用一个用离线真实世界数据初始化的经验回放缓冲区，并不断用新鲜的交互来丰富它，以积极支持代理训练。为了提高知识获取，DreamGym自适应地生成新的任务，以挑战当前的代理策略，从而实现更有效的在线课程学习。

Result: 在不同的环境和代理主干上的实验表明，DreamGym大大提高了RL训练，无论是在完全合成的设置中，还是在sim-to-real的转移场景中。在非RL-ready任务（如WebArena）上，DreamGym的性能超过所有基线30%以上。在RL-ready但成本高昂的设置中，它仅使用合成交互即可匹配GRPO和PPO的性能。当将完全在合成经验上训练的策略转移到真实环境RL时，DreamGym可产生显着的额外性能提升，同时需要的真实世界交互要少得多，从而为通用RL提供了一种可扩展的warm-start策略。

Conclusion: DreamGym是一个很有前途的框架，可以解决在LLM中使用RL的挑战。它通过合成经验和使用基于推理的经验模型来实现可扩展的训练。实验结果表明，DreamGym可以提高RL训练的性能，并在各种任务中取得良好的效果。它还可以作为通用RL的可扩展的warm-start策略。

Abstract: While reinforcement learning (RL) can empower large language model (LLM)
agents by enabling self-improvement through interaction, its practical adoption
remains challenging due to costly rollouts, limited task diversity, unreliable
reward signals, and infrastructure complexity, all of which obstruct the
collection of scalable experience data. To address these challenges, we
introduce DreamGym, the first unified framework designed to synthesize diverse
experiences with scalability in mind to enable effective online RL training for
autonomous agents. Rather than relying on expensive real-environment rollouts,
DreamGym distills environment dynamics into a reasoning-based experience model
that derives consistent state transitions and feedback signals through
step-by-step reasoning, enabling scalable agent rollout collection for RL. To
improve the stability and quality of transitions, DreamGym leverages an
experience replay buffer initialized with offline real-world data and
continuously enriched with fresh interactions to actively support agent
training. To improve knowledge acquisition, DreamGym adaptively generates new
tasks that challenge the current agent policy, enabling more effective online
curriculum learning. Experiments across diverse environments and agent
backbones demonstrate that DreamGym substantially improves RL training, both in
fully synthetic settings and in sim-to-real transfer scenarios. On non-RL-ready
tasks like WebArena, DreamGym outperforms all baselines by over 30%. And in
RL-ready but costly settings, it matches GRPO and PPO performance using only
synthetic interactions. When transferring a policy trained purely on synthetic
experiences to real-environment RL, DreamGym yields significant additional
performance gains while requiring far fewer real-world interactions, providing
a scalable warm-start strategy for general-purpose RL.

</details>


### [36] [How Different Tokenization Algorithms Impact LLMs and Transformer Models for Binary Code Analysis](https://arxiv.org/abs/2511.03825)
*Ahmed Mostafa,Raisul Arefin Nahid,Samuel Mulder*

Main category: cs.AI

TL;DR: 本研究探讨了汇编代码分析中tokenization的重要性，并评估了NLP tokenization模型和参数选择对汇编代码的影响。


<details>
  <summary>Details</summary>
Motivation: 汇编代码的tokenization是一个未被充分研究的领域，但它会影响词汇量大小、语义覆盖等内在特征，以及下游任务的性能。

Method: 对各种tokenization模型进行了全面研究，系统地分析了它们在编码汇编指令和捕获语义细微之处方面的效率。使用了包括Llama 3.2、BERT和BART等模型。

Result: Tokenizer的选择会显著影响下游性能，内在指标可以部分预测外在评估结果。内在tokenizer属性和它们在实际汇编代码任务中的效用之间存在复杂的权衡。

Conclusion: 本研究为优化低级代码分析的tokenization模型提供了有价值的见解，有助于提高基于NLM的二进制分析工作流程的鲁棒性和可扩展性。

Abstract: Tokenization is fundamental in assembly code analysis, impacting intrinsic
characteristics like vocabulary size, semantic coverage, and extrinsic
performance in downstream tasks. Despite its significance, tokenization in the
context of assembly code remains an underexplored area. This study aims to
address this gap by evaluating the intrinsic properties of Natural Language
Processing (NLP) tokenization models and parameter choices, such as vocabulary
size. We explore preprocessing customization options and pre-tokenization rules
tailored to the unique characteristics of assembly code. Additionally, we
assess their impact on downstream tasks like function signature prediction -- a
critical problem in binary code analysis.
  To this end, we conduct a thorough study on various tokenization models,
systematically analyzing their efficiency in encoding assembly instructions and
capturing semantic nuances. Through intrinsic evaluations, we compare
tokenizers based on tokenization efficiency, vocabulary compression, and
representational fidelity for assembly code. Using state-of-the-art pre-trained
models such as the decoder-only Large Language Model (LLM) Llama 3.2, the
encoder-only transformer BERT, and the encoder-decoder model BART, we evaluate
the effectiveness of these tokenizers across multiple performance metrics.
Preliminary findings indicate that tokenizer choice significantly influences
downstream performance, with intrinsic metrics providing partial but incomplete
predictability of extrinsic evaluation outcomes. These results reveal complex
trade-offs between intrinsic tokenizer properties and their utility in
practical assembly code tasks. Ultimately, this study provides valuable
insights into optimizing tokenization models for low-level code analysis,
contributing to the robustness and scalability of Natural Language Model
(NLM)-based binary analysis workflows.

</details>


### [37] [To See or To Read: User Behavior Reasoning in Multimodal LLMs](https://arxiv.org/abs/2511.03845)
*Tianning Dong,Luyi Ma,Varun Vasudevan,Jason Cho,Sushant Kumar,Kannan Achan*

Main category: cs.AI

TL;DR: 研究了多模态大语言模型（MLLM）在用户行为数据上的推理能力，特别是文本和图像表示的有效性。


<details>
  <summary>Details</summary>
Motivation: 探讨文本和图像表示的用户行为数据，哪种更有效地提升 MLLM 的性能。

Method: 提出了一个名为 BehaviorLens 的系统性基准测试框架，用于评估六个 MLLM 在用户行为推理中的模态权衡，将交易数据表示为文本段落、散点图和流程图。

Result: 发现当数据表示为图像时，MLLM 的下一次购买预测准确率比等效的文本表示提高了 87.5%，且没有额外的计算成本。

Conclusion: 图像表示比文本表示更有效地提升 MLLM 在用户行为预测方面的性能。

Abstract: Multimodal Large Language Models (MLLMs) are reshaping how modern agentic
systems reason over sequential user-behavior data. However, whether textual or
image representations of user behavior data are more effective for maximizing
MLLM performance remains underexplored. We present \texttt{BehaviorLens}, a
systematic benchmarking framework for assessing modality trade-offs in
user-behavior reasoning across six MLLMs by representing transaction data as
(1) a text paragraph, (2) a scatter plot, and (3) a flowchart. Using a
real-world purchase-sequence dataset, we find that when data is represented as
images, MLLMs next-purchase prediction accuracy is improved by 87.5% compared
with an equivalent textual representation without any additional computational
cost.

</details>


### [38] [KnowThyself: An Agentic Assistant for LLM Interpretability](https://arxiv.org/abs/2511.03878)
*Suraj Prasai,Mengnan Du,Ying Zhang,Fan Yang*

Main category: cs.AI

TL;DR: KnowThyself is a chat-based tool that simplifies LLM interpretability by consolidating existing tools into an accessible interface.


<details>
  <summary>Details</summary>
Motivation: Existing LLM interpretability tools are fragmented and code-intensive.

Method: An orchestrator LLM reformulates queries, an agent router directs them to specialized modules, and outputs are contextualized into explanations.

Result: KnowThyself lowers technical barriers and provides an extensible platform for LLM inspection.

Conclusion: KnowThyself offers a robust foundation for accessible LLM interpretability by embedding the whole process into a conversational workflow.

Abstract: We develop KnowThyself, an agentic assistant that advances large language
model (LLM) interpretability. Existing tools provide useful insights but remain
fragmented and code-intensive. KnowThyself consolidates these capabilities into
a chat-based interface, where users can upload models, pose natural language
questions, and obtain interactive visualizations with guided explanations. At
its core, an orchestrator LLM first reformulates user queries, an agent router
further directs them to specialized modules, and the outputs are finally
contextualized into coherent explanations. This design lowers technical
barriers and provides an extensible platform for LLM inspection. By embedding
the whole process into a conversational workflow, KnowThyself offers a robust
foundation for accessible LLM interpretability.

</details>


### [39] [Are We Asking the Right Questions? On Ambiguity in Natural Language Queries for Tabular Data Analysis](https://arxiv.org/abs/2511.04584)
*Daniel Gomm,Cornelius Wolff,Madelon Hulsebos*

Main category: cs.AI

TL;DR: 这篇论文提出了一个处理表格数据自然语言界面中歧义的新框架，将歧义视为用户与系统之间协作互动的特征。


<details>
  <summary>Details</summary>
Motivation: 目前的表格数据自然语言界面未能充分解决查询中的歧义问题。

Method: 该框架区分了可解析的合作查询和不可解析的非合作查询。

Result: 通过对15个流行数据集的分析，发现现有评估方法混合了不同类型的查询，无法有效评估系统的执行准确性和解释能力。

Conclusion: 该框架将重点从解决歧义转变为在查询解决中拥抱合作，从而为表格数据的自然语言界面的设计和评估提供参考。

Abstract: Natural language interfaces to tabular data must handle ambiguities inherent
to queries. Instead of treating ambiguity as a deficiency, we reframe it as a
feature of cooperative interaction, where the responsibility of query
specification is shared among the user and the system. We develop a principled
framework distinguishing cooperative queries, i.e., queries that yield a
resolvable interpretation, from uncooperative queries that cannot be resolved.
Applying the framework to evaluations for tabular question answering and
analysis, we analyze the queries in 15 popular datasets, and observe an
uncontrolled mixing of query types neither adequate for evaluating a system's
execution accuracy nor for evaluating interpretation capabilities. Our
framework and analysis of queries shifts the perspective from fixing ambiguity
to embracing cooperation in resolving queries. This reflection enables more
informed design and evaluation for natural language interfaces for tabular
data, for which we outline implications and directions for future research.

</details>


### [40] [Extracting Causal Relations in Deep Knowledge Tracing](https://arxiv.org/abs/2511.03948)
*Kevin Hong,Kia Karbasi,Gregory Pottie*

Main category: cs.AI

TL;DR: 本文挑战了关于深度知识追踪（DKT）模型优越性的普遍解释，并提出 DKT 的优势在于其模拟因果关系的能力，而不是双向关系。


<details>
  <summary>Details</summary>
Motivation: 当前对 DKT 性能提升的解释是它能够建模课程中不同知识组件（KC）之间的双向关系，从而能够根据学生在其他 KC 上的表现来推断他们对一个 KC 的理解。本文对此提出质疑。

Method: 通过将练习关系图修剪为有向无环图（DAG），并在 Assistments 数据集的因果子集上训练 DKT，来验证 DKT 的预测能力与这些因果结构高度一致。此外，还提出了一种使用 DKT 学习的表征来提取练习关系 DAG 的替代方法。

Result: DKT 的预测能力与因果结构高度一致。实证结果表明 DKT 的有效性主要来源于其近似 KC 之间因果依赖关系的能力。

Conclusion: DKT 的有效性主要源于其模拟知识组件之间因果依赖关系的能力，而非简单的关系映射。

Abstract: A longstanding goal in computational educational research is to develop
explainable knowledge tracing (KT) models. Deep Knowledge Tracing (DKT), which
leverages a Recurrent Neural Network (RNN) to predict student knowledge and
performance on exercises, has been proposed as a major advancement over
traditional KT methods. Several studies suggest that its performance gains stem
from its ability to model bidirectional relationships between different
knowledge components (KCs) within a course, enabling the inference of a
student's understanding of one KC from their performance on others. In this
paper, we challenge this prevailing explanation and demonstrate that DKT's
strength lies in its implicit ability to model prerequisite relationships as a
causal structure, rather than bidirectional relationships. By pruning exercise
relation graphs into Directed Acyclic Graphs (DAGs) and training DKT on causal
subsets of the Assistments dataset, we show that DKT's predictive capabilities
align strongly with these causal structures. Furthermore, we propose an
alternative method for extracting exercise relation DAGs using DKT's learned
representations and provide empirical evidence supporting our claim. Our
findings suggest that DKT's effectiveness is largely driven by its capacity to
approximate causal dependencies between KCs rather than simple relational
mappings.

</details>


### [41] [LLMs and Cultural Values: the Impact of Prompt Language and Explicit Cultural Framing](https://arxiv.org/abs/2511.03980)
*Bram Bulté,Ayla Rigouts Terryn*

Main category: cs.AI

TL;DR: 大型语言模型（LLM）在全球范围内被广泛使用，但由于训练数据和优化目标的不平衡，其是否能代表用户群体的文化多样性令人怀疑。本研究探讨了LLM和文化价值观，以及prompt语言和文化框架如何影响模型响应，并考察了模型在不同国家与人类价值观的对齐情况。研究发现，prompt语言和文化视角都会影响LLM的输出，但模型存在系统性偏差，倾向于某些特定国家的价值观（荷兰、德国、美国和日本）。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型是否能代表其广泛用户群体的文化多样性，以及prompt语言和文化框架如何影响模型响应。

Method: 使用Hofstede价值观调查模块和世界价值观调查中的63个项目，将其翻译成11种语言，并构建带有和不带有不同显式文化视角的prompt，对10个LLM进行探究。

Result: Prompt语言和文化视角都会影响LLM的输出。虽然有针对性的prompt可以在一定程度上引导LLM的响应朝着相应国家的主导价值观方向发展，但无法克服模型对某些特定国家价值观的系统性偏差。与使用目标prompt语言相比，使用明确的文化视角更能提高与人类受访者文化价值观的对齐度。

Conclusion: LLM对prompt的变化足够敏感，可以产生变化，但又过于牢固地锚定在特定的文化默认值上，无法充分代表文化多样性。

Abstract: Large Language Models (LLMs) are rapidly being adopted by users across the
globe, who interact with them in a diverse range of languages. At the same
time, there are well-documented imbalances in the training data and
optimisation objectives of this technology, raising doubts as to whether LLMs
can represent the cultural diversity of their broad user base. In this study,
we look at LLMs and cultural values and examine how prompt language and
cultural framing influence model responses and their alignment with human
values in different countries. We probe 10 LLMs with 63 items from the Hofstede
Values Survey Module and World Values Survey, translated into 11 languages, and
formulated as prompts with and without different explicit cultural
perspectives. Our study confirms that both prompt language and cultural
perspective produce variation in LLM outputs, but with an important caveat:
While targeted prompting can, to a certain extent, steer LLM responses in the
direction of the predominant values of the corresponding countries, it does not
overcome the models' systematic bias toward the values associated with a
restricted set of countries in our dataset: the Netherlands, Germany, the US,
and Japan. All tested models, regardless of their origin, exhibit remarkably
similar patterns: They produce fairly neutral responses on most topics, with
selective progressive stances on issues such as social tolerance. Alignment
with cultural values of human respondents is improved more with an explicit
cultural perspective than with a targeted prompt language. Unexpectedly,
combining both approaches is no more effective than cultural framing with an
English prompt. These findings reveal that LLMs occupy an uncomfortable middle
ground: They are responsive enough to changes in prompts to produce variation,
but too firmly anchored to specific cultural defaults to adequately represent
cultural diversity.

</details>


### [42] [ArchPilot: A Proxy-Guided Multi-Agent Approach for Machine Learning Engineering](https://arxiv.org/abs/2511.03985)
*Zhuowen Yuan,Tao Liu,Yang Yang,Yang Wang,Feng Qi,Kaushik Rangadurai,Bo Li,Shuang Yang*

Main category: cs.AI

TL;DR: ArchPilot is a multi-agent system that efficiently automates ML engineering by using proxy-based evaluation to reduce the need for expensive full training runs.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based agents for automated ML engineering are computationally expensive due to repeated full training runs.

Method: ArchPilot uses a multi-agent system with architecture generation, proxy-based evaluation, and adaptive search, coordinated by an orchestration agent using a Monte Carlo Tree Search (MCTS)-inspired algorithm.

Result: ArchPilot outperforms state-of-the-art baselines like AIDE and ML-Master on MLE-Bench.

Conclusion: ArchPilot's multi-agent collaboration enables efficient ML engineering under limited budgets by prioritizing high-potential candidates with minimal reliance on full training runs.

Abstract: Recent LLM-based agents have demonstrated strong capabilities in automated ML
engineering. However, they heavily rely on repeated full training runs to
evaluate candidate solutions, resulting in significant computational overhead,
limited scalability to large search spaces, and slow iteration cycles. To
address these challenges, we introduce ArchPilot, a multi-agent system that
integrates architecture generation, proxy-based evaluation, and adaptive search
into a unified framework. ArchPilot consists of three specialized agents: an
orchestration agent that coordinates the search process using a Monte Carlo
Tree Search (MCTS)-inspired novel algorithm with a restart mechanism and
manages memory of previous candidates; a generation agent that iteratively
generates, improves, and debugs candidate architectures; and an evaluation
agent that executes proxy training runs, generates and optimizes proxy
functions, and aggregates the proxy scores into a fidelity-aware performance
metric. This multi-agent collaboration allows ArchPilot to prioritize
high-potential candidates with minimal reliance on expensive full training
runs, facilitating efficient ML engineering under limited budgets. Experiments
on MLE-Bench demonstrate that ArchPilot outperforms SOTA baselines such as AIDE
and ML-Master, validating the effectiveness of our multi-agent system.

</details>


### [43] [Detecting Silent Failures in Multi-Agentic AI Trajectories](https://arxiv.org/abs/2511.04032)
*Divya Pathak,Harshit Kumar,Anuska Roy,Felix George,Mudit Verma,Pratibha Moogi*

Main category: cs.AI

TL;DR: 论文研究了多代理AI系统中的异常检测问题，这些系统由大型语言模型驱动，容易出现难以检测的故障。


<details>
  <summary>Details</summary>
Motivation: 旨在识别多代理AI系统中难以检测的故障，如漂移、循环和输出细节缺失等。

Method: 提出了一个数据集生成流程，用于捕获用户行为、代理非确定性和LLM变化，并使用XGBoost和SVDD等方法进行异常检测。

Result: 构建了包含4,275和894个轨迹的基准数据集，XGBoost和SVDD方法分别达到了98%和96%的准确率。

Conclusion: 本研究首次对多代理AI系统中的异常检测进行了系统研究，提供了数据集、基准和见解，以指导未来的研究。

Abstract: Multi-Agentic AI systems, powered by large language models (LLMs), are
inherently non-deterministic and prone to silent failures such as drift,
cycles, and missing details in outputs, which are difficult to detect. We
introduce the task of anomaly detection in agentic trajectories to identify
these failures and present a dataset curation pipeline that captures user
behavior, agent non-determinism, and LLM variation. Using this pipeline, we
curate and label two benchmark datasets comprising \textbf{4,275 and 894}
trajectories from Multi-Agentic AI systems. Benchmarking anomaly detection
methods on these datasets, we show that supervised (XGBoost) and
semi-supervised (SVDD) approaches perform comparably, achieving accuracies up
to 98% and 96%, respectively. This work provides the first systematic study of
anomaly detection in Multi-Agentic AI systems, offering datasets, benchmarks,
and insights to guide future research.

</details>


### [44] [Interpreting Multi-Attribute Confounding through Numerical Attributes in Large Language Models](https://arxiv.org/abs/2511.04053)
*Hirohane Takagi,Gouki Minegishi,Shota Kizawa,Issey Sukeda,Hitomi Yanaka*

Main category: cs.AI

TL;DR: 大型语言模型(LLM)在数字推理方面存在问题，其根本原因尚不清楚。本文通过结合线性探测、偏相关分析和基于提示的脆弱性测试，研究了LLM如何整合数字属性以及不相关的数字上下文如何扰乱这些表示及其下游输出。


<details>
  <summary>Details</summary>
Motivation: 研究LLM中数字推理错误的根本原因，以及LLM如何整合单个实体的多个数字属性，以及不相关的数字上下文如何扰乱这些表示及其下游输出。

Method: 结合线性探测、偏相关分析和基于提示的脆弱性测试

Result: LLM编码了真实世界的数字相关性，但倾向于系统地放大它们。不相关的上下文会导致幅度表示的持续变化，下游效应随模型大小而变化。

Conclusion: LLM决策中存在漏洞，为多属性纠缠下更公平、具有表示意识的控制奠定了基础。

Abstract: Although behavioral studies have documented numerical reasoning errors in
large language models (LLMs), the underlying representational mechanisms remain
unclear. We hypothesize that numerical attributes occupy shared latent
subspaces and investigate two questions:(1) How do LLMs internally integrate
multiple numerical attributes of a single entity? (2)How does irrelevant
numerical context perturb these representations and their downstream outputs?
To address these questions, we combine linear probing with partial correlation
analysis and prompt-based vulnerability tests across models of varying sizes.
Our results show that LLMs encode real-world numerical correlations but tend to
systematically amplify them. Moreover, irrelevant context induces consistent
shifts in magnitude representations, with downstream effects that vary by model
size. These findings reveal a vulnerability in LLM decision-making and lay the
groundwork for fairer, representation-aware control under multi-attribute
entanglement.

</details>


### [45] [Agentmandering: A Game-Theoretic Framework for Fair Redistricting via Large Language Model Agents](https://arxiv.org/abs/2511.04076)
*Hao Li,Haotian Chen,Ruoyuan Gong,Juanjuan Wang,Hao Jiang*

Main category: cs.AI

TL;DR: 提出 Agentmandering 框架，通过 LLM 智能体之间的博弈来重新构想选区重划，以减少党派偏见和不公正。


<details>
  <summary>Details</summary>
Motivation: 现有的计算方法主要旨在生成大量法律上有效的选区划分方案，但它们通常忽略了选择过程中的战略动态，这为党派行为者创造了机会来挑选在政治上有利的地图。

Method: Agentmandering 框架将选区重划重新构想为代表相反政治利益的两个智能体之间的回合制谈判。该方法借鉴了博弈论的思想，特别是“选择和冻结”协议，通过大型语言模型 (LLM) 智能体将战略交互嵌入到选区重划过程中。

Result: 在美国 2020 年人口普查后的所有州的数据评估表明，Agentmandering 显着减少了党派偏见和不公正，同时实现了比标准基线低 2 到 3 个数量级的方差。

Conclusion: 结果表明，Agentmandering 具有公平性和稳定性，尤其是在摇摆州情景中。

Abstract: Redistricting plays a central role in shaping how votes are translated into
political power. While existing computational methods primarily aim to generate
large ensembles of legally valid districting plans, they often neglect the
strategic dynamics involved in the selection process. This oversight creates
opportunities for partisan actors to cherry-pick maps that, while technically
compliant, are politically advantageous. Simply satisfying formal constraints
does not ensure fairness when the selection process itself can be manipulated.
We propose \textbf{Agentmandering}, a framework that reimagines redistricting
as a turn-based negotiation between two agents representing opposing political
interests. Drawing inspiration from game-theoretic ideas, particularly the
\textit{Choose-and-Freeze} protocol, our method embeds strategic interaction
into the redistricting process via large language model (LLM) agents. Agents
alternate between selecting and freezing districts from a small set of
candidate maps, gradually partitioning the state through constrained and
interpretable choices. Evaluation on post-2020 U.S. Census data across all
states shows that Agentmandering significantly reduces partisan bias and
unfairness, while achieving 2 to 3 orders of magnitude lower variance than
standard baselines. These results demonstrate both fairness and stability,
especially in swing-state scenarios. Our code is available at
https://github.com/Lihaogx/AgentMandering.

</details>


### [46] [KGFR: A Foundation Retriever for Generalized Knowledge Graph Question Answering](https://arxiv.org/abs/2511.04093)
*Yuanning Cui,Zequn Sun,Wei Hu,Zhangjie Fu*

Main category: cs.AI

TL;DR: LLMs在知识密集型问题上表现不佳，因为上下文和参数知识有限。我们提出了LLM-KGFR框架，该框架利用LLM生成的关系描述和基于问题中角色的实体初始化，实现了对未见KG的零样本泛化。为了处理大型图，它采用了非对称渐进传播（APP），这是一种选择性限制高度节点同时保留信息路径的逐步扩展。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）擅长推理，但由于上下文和参数知识有限，难以处理知识密集型问题。现有的依赖于微调LLM或GNN检索器的方法受到数据集特定调整和大型或未见图上的可扩展性的限制。

Method: 我们提出了LLM-KGFR协作框架，其中LLM与结构化检索器知识图基础检索器（KGFR）协同工作。KGFR使用LLM生成的描述对关系进行编码，并根据实体在问题中的角色初始化实体，从而实现对未见KG的零样本泛化。为了高效地处理大型图，它采用了非对称渐进传播（APP）——一种选择性限制高度节点同时保留信息路径的逐步扩展。通过节点、边和路径级别的接口，LLM迭代地请求候选答案、支持事实和推理路径，形成一个可控的推理循环。

Result: 实验表明，LLM-KGFR在保持可扩展性和泛化能力的同时，实现了强大的性能，为KG增强推理提供了实用的解决方案。

Conclusion: LLM-KGFR框架在知识图谱增强推理方面表现出色，具有良好的可扩展性和泛化能力。

Abstract: Large language models (LLMs) excel at reasoning but struggle with
knowledge-intensive questions due to limited context and parametric knowledge.
However, existing methods that rely on finetuned LLMs or GNN retrievers are
limited by dataset-specific tuning and scalability on large or unseen graphs.
We propose the LLM-KGFR collaborative framework, where an LLM works with a
structured retriever, the Knowledge Graph Foundation Retriever (KGFR). KGFR
encodes relations using LLM-generated descriptions and initializes entities
based on their roles in the question, enabling zero-shot generalization to
unseen KGs. To handle large graphs efficiently, it employs Asymmetric
Progressive Propagation (APP)- a stepwise expansion that selectively limits
high-degree nodes while retaining informative paths. Through node-, edge-, and
path-level interfaces, the LLM iteratively requests candidate answers,
supporting facts, and reasoning paths, forming a controllable reasoning loop.
Experiments demonstrate that LLM-KGFR achieves strong performance while
maintaining scalability and generalization, providing a practical solution for
KG-augmented reasoning.

</details>


### [47] [Testing the Testers: Human-Driven Quality Assessment of Voice AI Testing Platforms](https://arxiv.org/abs/2511.04133)
*Miguel E. Andres,Vadim Fedorov,Rida Sadek,Enric Spagnolo-Arrizabalaga,Nadescha Trudel*

Main category: cs.AI

TL;DR: 本研究提出了一个评估语音AI测试质量的系统框架，旨在解决测试平台在生成真实测试对话和准确评估代理响应方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前语音AI代理快速发展，但缺乏系统性的测试可靠性方法，组织无法客观评估其测试方法是否有效。

Method: 该框架结合心理测量技术（成对比较、Elo评分、bootstrap置信区间和置换检验）与严格的统计验证，为任何测试方法提供可重复的指标。

Result: 通过对三个领先的商业平台进行评估，结果显示使用该框架能有效区分性能差异，其中Evalion平台的评估质量F1值为0.92，模拟质量得分为0.61，均优于其他平台。

Conclusion: 该框架使研究人员和组织能够验证任何平台的测试能力，为大规模部署语音AI提供重要的测量基础。

Abstract: Voice AI agents are rapidly transitioning to production deployments, yet
systematic methods for ensuring testing reliability remain underdeveloped.
Organizations cannot objectively assess whether their testing approaches
(internal tools or external platforms) actually work, creating a critical
measurement gap as voice AI scales to billions of daily interactions.
  We present the first systematic framework for evaluating voice AI testing
quality through human-centered benchmarking. Our methodology addresses the
fundamental dual challenge of testing platforms: generating realistic test
conversations (simulation quality) and accurately evaluating agent responses
(evaluation quality). The framework combines established psychometric
techniques (pairwise comparisons yielding Elo ratings, bootstrap confidence
intervals, and permutation tests) with rigorous statistical validation to
provide reproducible metrics applicable to any testing approach.
  To validate the framework and demonstrate its utility, we conducted
comprehensive empirical evaluation of three leading commercial platforms
focused on Voice AI Testing using 21,600 human judgments across 45 simulations
and ground truth validation on 60 conversations. Results reveal statistically
significant performance differences with the proposed framework, with the
top-performing platform, Evalion, achieving 0.92 evaluation quality measured as
f1-score versus 0.73 for others, and 0.61 simulation quality using a league
based scoring system (including ties) vs 0.43 for other platforms.
  This framework enables researchers and organizations to empirically validate
the testing capabilities of any platform, providing essential measurement
foundations for confident voice AI deployment at scale. Supporting materials
are made available to facilitate reproducibility and adoption.

</details>


### [48] [When Empowerment Disempowers](https://arxiv.org/abs/2511.04177)
*Claire Yang,Maya Cakmak,Max Kleiman-Weiner*

Main category: cs.AI

TL;DR: 研究了在多人类环境中，基于赋权的AI助手可能导致对某些人的“剥夺权力”现象，即降低他们的环境影响力和奖励。


<details>
  <summary>Details</summary>
Motivation: 在多人类环境（如家庭和医院）中，AI助手有很大的应用前景。然而，以往基于赋权的AI助手研究主要集中在单个人的帮助上，忽略了多人类环境下的潜在问题。

Method: 引入了一个名为Disempower-Grid的开源多人类gridworld测试平台，用于评估和分析AI助手在多人类环境中的行为。

Result: 实验表明，优化单个人赋权的AI助手可能会显著降低另一个人的环境影响力和奖励。同时发现，联合赋权可以在一定程度上缓解剥夺权力的问题，但会牺牲用户的奖励。

Conclusion: 在单智能体环境中看起来对齐的目标，在多智能体环境中可能会变得不对齐。

Abstract: Empowerment, a measure of an agent's ability to control its environment, has
been proposed as a universal goal-agnostic objective for motivating assistive
behavior in AI agents. While multi-human settings like homes and hospitals are
promising for AI assistance, prior work on empowerment-based assistance assumes
that the agent assists one human in isolation. We introduce an open source
multi-human gridworld test suite Disempower-Grid. Using Disempower-Grid, we
empirically show that assistive RL agents optimizing for one human's
empowerment can significantly reduce another human's environmental influence
and rewards - a phenomenon we formalize as disempowerment. We characterize when
disempowerment occurs in these environments and show that joint empowerment
mitigates disempowerment at the cost of the user's reward. Our work reveals a
broader challenge for the AI alignment community: goal-agnostic objectives that
seem aligned in single-agent settings can become misaligned in multi-agent
contexts.

</details>


### [49] [Opus: A Quantitative Framework for Workflow Evaluation](https://arxiv.org/abs/2511.04220)
*Alan Seroul,Théo Fagnoni,Inès Adnani,Dana O. Mohamed,Phillip Kingston*

Main category: cs.AI

TL;DR: 提出了Opus工作流评估框架，用于量化工作流质量和效率。


<details>
  <summary>Details</summary>
Motivation: 为了量化工作流的质量和效率，并支持工作流的自动评估、排序和优化。

Method: 结合了Opus工作流奖励模型（概率函数，估计预期性能）和Opus工作流规范惩罚（衡量结构和信息质量的函数）。

Result: 提出了Opus工作流奖励模型，形式化了工作流成功的概率期望，并定义了可测量的Opus工作流规范惩罚。

Conclusion: 提出了一个统一的优化公式，用于识别和排序最优工作流。

Abstract: This paper introduces the Opus Workflow Evaluation Framework, a
probabilistic-normative formulation for quantifying Workflow quality and
efficiency. It integrates notions of correctness, reliability, and cost into a
coherent mathematical model that enables direct comparison, scoring, and
optimization of Workflows. The framework combines the Opus Workflow Reward, a
probabilistic function estimating expected performance through success
likelihood, resource usage, and output gain, with the Opus Workflow Normative
Penalties, a set of measurable functions capturing structural and informational
quality across Cohesion, Coupling, Observability, and Information Hygiene. It
supports automated Workflow assessment, ranking, and optimization within modern
automation systems such as Opus and can be integrated into Reinforcement
Learning loops to guide Workflow discovery and refinement. In this paper, we
introduce the Opus Workflow Reward model that formalizes Workflow success as a
probabilistic expectation over costs and outcomes. We define measurable Opus
Workflow Normative Penalties capturing structural, semantic, and signal-related
properties of Workflows. Finally, we propose a unified optimization formulation
for identifying and ranking optimal Workflows under joint Reward-Penalty
trade-offs.

</details>


### [50] [Shared Spatial Memory Through Predictive Coding](https://arxiv.org/abs/2511.04235)
*Zhengru Fang,Yu Guo,Jingjing Wang,Yuang Zhang,Haonan An,Yinhai Wang,Yuguang Fang*

Main category: cs.AI

TL;DR: 提出了一种多智能体预测编码框架，通过最小化智能体之间的相互不确定性来实现协作，从而在带宽受限的情况下也能保持较好的性能。


<details>
  <summary>Details</summary>
Motivation: 在多智能体系统中，共享和重建一致的空间记忆是一个关键挑战，因为部分可观察性和有限的带宽经常导致灾难性的协调失败。

Method: 该框架基于信息瓶颈目标，促使智能体学习交流的对象、内容和时机。底层采用类似网格单元的度量作为内部空间编码进行自我定位，并通过自监督运动预测自发产生。在此基础上，智能体逐步发展出带宽高效的通信机制和专门的神经种群来编码伙伴的位置。

Result: 在Memory-Maze基准测试中，该方法对带宽约束表现出卓越的弹性：当带宽从128位/步减少到4位/步时，成功率从73.5%降至64.4%，而完全广播基线的成功率则从67.6%下降到28.6%。

Conclusion: 研究结果为复杂的社会表征如何从统一的预测驱动中产生，从而导致社会集体智能，奠定了理论上合理且生物学上合理的basis。

Abstract: Sharing and reconstructing a consistent spatial memory is a critical
challenge in multi-agent systems, where partial observability and limited
bandwidth often lead to catastrophic failures in coordination. We introduce a
multi-agent predictive coding framework that formulate coordination as the
minimization of mutual uncertainty among agents. Instantiated as an information
bottleneck objective, it prompts agents to learn not only who and what to
communicate but also when. At the foundation of this framework lies a
grid-cell-like metric as internal spatial coding for self-localization,
emerging spontaneously from self-supervised motion prediction. Building upon
this internal spatial code, agents gradually develop a bandwidth-efficient
communication mechanism and specialized neural populations that encode
partners' locations: an artificial analogue of hippocampal social place cells
(SPCs). These social representations are further enacted by a hierarchical
reinforcement learning policy that actively explores to reduce joint
uncertainty. On the Memory-Maze benchmark, our approach shows exceptional
resilience to bandwidth constraints: success degrades gracefully from 73.5% to
64.4% as bandwidth shrinks from 128 to 4 bits/step, whereas a full-broadcast
baseline collapses from 67.6% to 28.6%. Our findings establish a theoretically
principled and biologically plausible basis for how complex social
representations emerge from a unified predictive drive, leading to social
collective intelligence.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [51] [GPU-Based Floating-point Adaptive Lossless Compression](https://arxiv.org/abs/2511.04140)
*Zheng Li,Weiyan Wang,Ruiyuan Li,Chao Chen,Xianlei Long,Linjiang Zheng,Quanqing Xu,Chuanhui Yang*

Main category: cs.DB

TL;DR: Falcon是一个基于GPU的浮点自适应无损压缩框架，旨在提高压缩比和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 物联网和高性能计算等领域产生大量浮点时间序列数据，对其进行压缩同时保持绝对保真度至关重要；利用现代GPU的大规模并行性可以实现前所未有的吞吐量。

Method: Falcon首先引入了一个轻量级的异步流水线，它隐藏了CPU和GPU之间数据传输期间的I/O延迟。然后，我们提出了一种精确且快速的浮点到整数转换方法，具有理论保证，消除了由浮点运算引起的误差。此外，我们设计了一种自适应稀疏位平面无损编码策略，该策略减少了由异常值引起的稀疏性。

Result: 在12个不同的数据集上进行的大量实验表明，我们的压缩比比最先进的基于CPU的方法提高了9.1%，压缩吞吐量比最快的基于GPU的竞争对手高2.43倍，解压缩吞吐量高2.4倍。

Conclusion: Falcon框架在压缩比和吞吐量方面均优于现有技术。

Abstract: Domains such as IoT (Internet of Things) and HPC (High Performance Computing)
generate a torrential influx of floating-point time-series data. Compressing
these data while preserving their absolute fidelity is critical, and leveraging
the massive parallelism of modern GPUs offers a path to unprecedented
throughput. Nevertheless, designing such a high-performance GPU-based lossless
compressor faces three key challenges: 1) heterogeneous data movement
bottlenecks, 2) precision-preserving conversion complexity, and 3)
anomaly-induced sparsity degradation. To address these challenges, this paper
proposes Falcon, a GPU-based Floating-point Adaptive Lossless COmpressioN
framework. Specifically, Falcon first introduces a lightweight asynchronous
pipeline, which hides the I/O latency during the data transmission between the
CPU and GPU. Then, we propose an accurate and fast float-to-integer
transformation method with theoretical guarantees, which eliminates the errors
caused by floating-point arithmetic. Moreover, we devise an adaptive sparse
bit-plane lossless encoding strategy, which reduces the sparsity caused by
outliers. Extensive experiments on 12 diverse datasets show that our
compression ratio improves by 9.1% over the most advanced CPU-based method,
with compression throughput 2.43X higher and decompression throughput 2.4X
higher than the fastest GPU-based competitors, respectively.

</details>


### [52] [EntroGD: Efficient Compression and Accurate Direct Analytics on Compressed Data](https://arxiv.org/abs/2511.04148)
*Xiaobo Zhao,Daniel E. Lucani*

Main category: cs.DB

TL;DR: EntroGD: An entropy-guided GD framework reduces complexity of the bit-selection algorithm to O(nd), achieves comparable compression, reduces configuration time, and accelerates clustering with negligible accuracy loss.


<details>
  <summary>Details</summary>
Motivation: GD algorithms face scalability challenges for high-dimensional data.

Method: EntroGD operates considers a two-step process. First, it generates condensed samples to preserve analytic fidelity. Second, it applies entropy-guided bit selection to maximize compression efficiency.

Result: Across 18 datasets, EntroGD achieves comparable compression, reduces configuration time by up to 53.5× over GreedyGD, and accelerates clustering by up to 31.6× over the original data with negligible accuracy loss.

Conclusion: EntroGD provides an efficient and scalable solution to performing analytics directly on compressed data.

Abstract: Generalized Deduplication (GD) enables lossless compression with direct
analytics on compressed data by dividing data into \emph{bases} and
\emph{deviations} and performing dictionary encoding on the former. However, GD
algorithms face scalability challenges for high-dimensional data. For example,
the GreedyGD algorithm relies on an iterative bit-selection process across
$d$-dimensional data resulting in $O(nd^2)$ complexity for $n$ data rows to
select bits to be used as bases and deviations. Although the $n$ data rows can
be reduced during training at the expense of performance, highly dimensional
data still experiences a marked loss in performance. This paper introduces
EntroGD, an entropy-guided GD framework that reduces complexity of the
bit-selection algorithm to $O(nd)$. EntroGD operates considers a two-step
process. First, it generates condensed samples to preserve analytic fidelity.
Second, it applies entropy-guided bit selection to maximize compression
efficiency. Across 18 datasets of varying types and dimensionalities, EntroGD
achieves compression performance comparable to GD-based and universal
compressors, while reducing configuration time by up to 53.5$\times$ over
GreedyGD and accelerating clustering by up to 31.6$\times$ over the original
data with negligible accuracy loss by performing analytics on the condensed
samples, which are much fewer than original samples. Thus, EntroGD provides an
efficient and scalable solution to performing analytics directly on compressed
data.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [53] [Caption Injection for Optimization in Generative Search Engine](https://arxiv.org/abs/2511.04080)
*Xiaolu Chen,Yong Liao*

Main category: cs.IR

TL;DR: 本文提出了一种新的多模态生成搜索引擎优化方法，通过提取图像字幕并将其注入到文本内容中，以提高内容在生成搜索场景中的主观可见性。


<details>
  <summary>Details</summary>
Motivation: 现有的G-SEO方法仅限于基于文本的优化，未能充分利用多模态数据。

Method: 提出Caption Injection方法，从图像中提取字幕并将其注入到文本内容中，整合视觉语义以增强内容的主观可见性。

Result: 在MRAMG基准数据集上的实验结果表明，Caption Injection方法在G-Eval指标下显著优于仅基于文本的G-SEO基线方法。

Conclusion: 多模态整合对于G-SEO至关重要，能够有效提升用户感知的内容可见性。

Abstract: Generative Search Engines (GSEs) leverage Retrieval-Augmented Generation
(RAG) techniques and Large Language Models (LLMs) to integrate multi-source
information and provide users with accurate and comprehensive responses. Unlike
traditional search engines that present results in ranked lists, GSEs shift
users' attention from sequential browsing to content-driven subjective
perception, driving a paradigm shift in information retrieval. In this context,
enhancing the subjective visibility of content through Generative Search Engine
Optimization (G-SEO) methods has emerged as a new research focus. With the
rapid advancement of Multimodal Retrieval-Augmented Generation (MRAG)
techniques, GSEs can now efficiently integrate text, images, audio, and video,
producing richer responses that better satisfy complex information needs.
Existing G-SEO methods, however, remain limited to text-based optimization and
fail to fully exploit multimodal data. To address this gap, we propose Caption
Injection, the first multimodal G-SEO approach, which extracts captions from
images and injects them into textual content, integrating visual semantics to
enhance the subjective visibility of content in generative search scenarios. We
systematically evaluate Caption Injection on MRAMG, a benchmark for MRAG, under
both unimodal and multimodal settings. Experimental results show that Caption
Injection significantly outperforms text-only G-SEO baselines under the G-Eval
metric, demonstrating the necessity and effectiveness of multimodal integration
in G-SEO to improve user-perceived content visibility.

</details>


### [54] [E-CARE: An Efficient LLM-based Commonsense-Augmented Framework for E-Commerce](https://arxiv.org/abs/2511.04087)
*Ge Zhang,Rohan Deepak Ajwani,Tony Zheng,Hongjian Gu,Yaochen Hu,Wei Guo,Mark Coates,Yingxue Zhang*

Main category: cs.IR

TL;DR: 提出了一种名为 E-CARE 的高效常识增强推荐方法，旨在降低电商推荐系统中利用大型语言模型 (LLM) 进行常识推理的成本，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 在电商平台中，准确预测用户查询和产品之间的相关性至关重要，但现有方法依赖大量实时 LLM 推理，成本高昂。

Method: 利用常识推理因子图编码 LLM 的推理模式，从而在推理过程中，模型只需对每个查询进行一次 LLM 前向传播即可访问常识推理。

Result: 在两个下游任务上的实验表明，E-CARE 将 precision@5 指标提高了 12.1%。

Conclusion: E-CARE 能够以较低的成本有效利用 LLM 的常识推理能力，提高电商推荐系统的性能。

Abstract: Finding relevant products given a user query plays a pivotal role in an
e-commerce platform, as it can spark shopping behaviors and result in revenue
gains. The challenge lies in accurately predicting the correlation between
queries and products. Recently, mining the cross-features between queries and
products based on the commonsense reasoning capacity of Large Language Models
(LLMs) has shown promising performance. However, such methods suffer from high
costs due to intensive real-time LLM inference during serving, as well as human
annotations and potential Supervised Fine Tuning (SFT). To boost efficiency
while leveraging the commonsense reasoning capacity of LLMs for various
e-commerce tasks, we propose the Efficient Commonsense-Augmented Recommendation
Enhancer (E-CARE). During inference, models augmented with E-CARE can access
commonsense reasoning with only a single LLM forward pass per query by
utilizing a commonsense reasoning factor graph that encodes most of the
reasoning schema from powerful LLMs. The experiments on 2 downstream tasks show
an improvement of up to 12.1% on precision@5.

</details>


### [55] [Coordination-Free Lane Partitioning for Convergent ANN Search](https://arxiv.org/abs/2511.04221)
*Carl Kugblenu,Petri Vuorimaa*

Main category: cs.IR

TL;DR: 提出了一种无协调的lane划分器，以提高向量搜索系统的效率。


<details>
  <summary>Details</summary>
Motivation: 现有的向量搜索系统在并行lane中会重复发现相同的候选，导致计算资源的浪费。

Method: 该方法构建一个确定性的候选池，应用per-query伪随机排列，并为每个lane分配一个不相交的位置切片。

Result: 在SIFT1M和MS MARCO数据集上，该方法显著提高了recall@10和hit@10等指标，同时降低了lane的重叠率。

Conclusion: 通过确定性地划分位置，可以将冗余的fan-out转化为互补的覆盖，从而提高向量搜索系统的效率。

Abstract: Production vector search systems often fan out each query across parallel
lanes (threads, replicas, or shards) to meet latency service-level objectives
(SLOs). In practice, these lanes rediscover the same candidates, so extra
compute does not increase coverage. We present a coordination-free lane
partitioner that turns duplication into complementary work at the same cost and
deadline. For each query we (1) build a deterministic candidate pool sized to
the total top-k budget, (2) apply a per-query pseudorandom permutation, and (3)
assign each lane a disjoint slice of positions. Lanes then return different
results by construction, with no runtime coordination.
  At equal cost with four lanes (total candidate budget 64), on SIFT1M (1M SIFT
feature vectors) with Hierarchical Navigable Small World graphs (HNSW)
recall@10 rises from 0.249 to 0.999 while lane overlap falls from nearly 100%
to 0%. On MS MARCO (8.8M passages) with HNSW, hit@10 improves from 0.200 to
0.601 and Mean Reciprocal Rank at 10 (MRR@10) from 0.133 to 0.330. For inverted
file (IVF) indexes we see smaller but consistent gains (for example, +11% on MS
MARCO) by de-duplicating list routing. A microbenchmark shows planner overhead
of ~37 microseconds per query (mean at the main setting) with linear growth in
the number of merged candidates.
  These results yield a simple operational guideline: size the per-query pool
to the total budget, deterministically partition positions across lanes, and
turn redundant fan-out into complementary coverage without changing budget or
deadline.

</details>


### [56] [Transforming Mentorship: An AI Powered Chatbot Approach to University Guidance](https://arxiv.org/abs/2511.04172)
*Mashrur Rahman,Mantaqa abedin,Monowar Zamil Abir,Faizul Islam Ansari,Adib Reza,Farig Yousuf Sadeque,Niloy Farhan*

Main category: cs.IR

TL;DR: 这篇论文介绍了一个为孟加拉国大学学生设计的AI聊天机器人，它可以作为学生的导师，提供个性化的指导。


<details>
  <summary>Details</summary>
Motivation: 大学生在本科期间面临着巨大的挑战，他们常常缺乏个性化的指导。现有的数字工具无法为新生提供定制的指导。

Method: 该聊天机器人使用数据提取管道从各种来源处理和更新信息，并结合BM25词法排序和ChromaDB语义检索来检索信息，然后使用LLaMA-3.3-70B生成会话回复。

Result: 生成的文本在语义上具有高度相关性，BERTScore为0.831，METEOR评分为0.809。数据管道也非常高效，更新需要106.82秒，而新数据需要368.62秒。

Conclusion: 这个聊天机器人将能够通过回答学生的疑问来帮助他们，帮助他们更好地了解大学生活，并帮助他们为开放学分大学的学期规划更好的日程。

Abstract: University students face immense challenges during their undergraduate lives,
often being deprived of personalized on-demand guidance that mentors fail to
provide at scale. Digital tools exist, but there is a serious lack of
customized coaching for newcomers. This paper presents an AI-powered chatbot
that will serve as a mentor for the students of BRAC University. The main
component is a data ingestion pipeline that efficiently processes and updates
information from diverse sources, such as CSV files and university webpages.
The chatbot retrieves information through a hybrid approach, combining BM25
lexical ranking with ChromaDB semantic retrieval, and uses a Large Language
Model, LLaMA-3.3-70B, to generate conversational responses. The generated text
was found to be semantically highly relevant, with a BERTScore of 0.831 and a
METEOR score of 0.809. The data pipeline was also very efficient, taking 106.82
seconds for updates, compared to 368.62 seconds for new data. This chatbot will
be able to help students by responding to their queries, helping them to get a
better understanding of university life, and assisting them to plan better
routines for their semester in the open-credit university.

</details>


### [57] [Denoised Recommendation Model with Collaborative Signal Decoupling](https://arxiv.org/abs/2511.04237)
*Zefeng Li,Ning Yang*

Main category: cs.IR

TL;DR: 这篇论文提出了一种新的基于GNN的协同过滤模型DRCSD，用于消除不稳定的交互。


<details>
  <summary>Details</summary>
Motivation: 现有的去噪方法在单个图上进行去噪，这可能会导致协作信号的衰减。

Method: DRCSD包括两个核心模块：协作信号解耦模块和Order-wise去噪模块。此外，修改了传统GNN-based CF模型的信息聚合机制，以避免跨Order信号干扰，直到最终的pooling操作。

Result: 在三个公共真实世界数据集上的大量实验表明，DRCSD对不稳定的交互具有优越的鲁棒性，并且与最先进的基线模型相比，在推荐准确性指标方面实现了统计上显着的性能改进。

Conclusion: DRCSD模型可以有效地提高推荐系统的性能。

Abstract: Although the collaborative filtering (CF) algorithm has achieved remarkable
performance in recommendation systems, it suffers from suboptimal
recommendation performance due to noise in the user-item interaction matrix.
Numerous noise-removal studies have improved recommendation models, but most
existing approaches conduct denoising on a single graph. This may cause
attenuation of collaborative signals: removing edges between two nodes can
interrupt paths between other nodes, weakening path-dependent collaborative
information. To address these limitations, this study proposes a novel
GNN-based CF model called DRCSD for denoising unstable interactions. DRCSD
includes two core modules: a collaborative signal decoupling module (decomposes
signals into distinct orders by structural characteristics) and an order-wise
denoising module (performs targeted denoising on each order). Additionally, the
information aggregation mechanism of traditional GNN-based CF models is
modified to avoid cross-order signal interference until the final pooling
operation. Extensive experiments on three public real-world datasets show that
DRCSD has superior robustness against unstable interactions and achieves
statistically significant performance improvements in recommendation accuracy
metrics compared to state-of-the-art baseline models.

</details>


### [58] [LLM-as-a-Judge: Toward World Models for Slate Recommendation Systems](https://arxiv.org/abs/2511.04541)
*Baptiste Bonin,Maxime Heuillet,Audrey Durand*

Main category: cs.IR

TL;DR: 利用大型语言模型(LLM)通过成对推理预测用户在不同领域的偏好，用于slate推荐。


<details>
  <summary>Details</summary>
Motivation: 在slate推荐中，对用户在不同领域的偏好建模仍然是一个关键挑战。

Method: 使用多个LLM在跨越不同数据集的三个任务上进行实证研究，通过成对推理预测用户偏好。

Result: 结果揭示了任务表现与LLM捕获的偏好函数属性之间的关系，暗示了改进的领域。

Conclusion: 强调了LLM作为推荐系统中世界模型的潜力。

Abstract: Modeling user preferences across domains remains a key challenge in slate
recommendation (i.e. recommending an ordered sequence of items) research. We
investigate how Large Language Models (LLM) can effectively act as world models
of user preferences through pairwise reasoning over slates. We conduct an
empirical study involving several LLMs on three tasks spanning different
datasets. Our results reveal relationships between task performance and
properties of the preference function captured by LLMs, hinting towards areas
for improvement and highlighting the potential of LLMs as world models in
recommender systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [59] [Learning Filter-Aware Distance Metrics for Nearest Neighbor Search with Multiple Filters](https://arxiv.org/abs/2511.04073)
*Ananya Sutradhar,Suryansh Gupta,Ravishankar Krishnaswamy,Haiyang Xu,Aseem Rastogi,Gopal Srinivasa*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的过滤近似最近邻搜索（ANN）方法，该方法通过学习数据中的向量距离和过滤器匹配之间的最佳权衡来提高搜索准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于图的方法在过滤的ANN搜索中通常使用固定的惩罚，这不能很好地推广到具有不同标签和向量分布的数据集。

Method: 该方法将问题定义为一个约束线性优化问题，从中推导出权重，这些权重更好地反映了底层的过滤器分布。

Result: 实验表明，与固定惩罚方法相比，该方法将准确率提高了5-10%。

Conclusion: 该方法通过调整距离函数来适应数据，为过滤的ANN搜索问题提供了一个更灵活和通用的框架。

Abstract: Filtered Approximate Nearest Neighbor (ANN) search retrieves the closest
vectors for a query vector from a dataset. It enforces that a specified set of
discrete labels $S$ for the query must be included in the labels of each
retrieved vector. Existing graph-based methods typically incorporate filter
awareness by assigning fixed penalties or prioritizing nodes based on filter
satisfaction. However, since these methods use fixed, data in- dependent
penalties, they often fail to generalize across datasets with diverse label and
vector distributions. In this work, we propose a principled alternative that
learns the optimal trade-off between vector distance and filter match directly
from the data, rather than relying on fixed penalties. We formulate this as a
constrained linear optimization problem, deriving weights that better reflect
the underlying filter distribution and more effectively address the filtered
ANN search problem. These learned weights guide both the search process and
index construction, leading to graph structures that more effectively capture
the underlying filter distribution and filter semantics. Our experiments
demonstrate that adapting the distance function to the data significantly im-
proves accuracy by 5-10% over fixed-penalty methods, providing a more flexible
and generalizable framework for the filtered ANN search problem.

</details>


### [60] [Applying Time Series Deep Learning Models to Forecast the Growth of Perennial Ryegrass in Ireland](https://arxiv.org/abs/2511.03749)
*Oluwadurotimi Onibonoje,Vuong M. Ngo,Andrew McCarre,Elodie Ruelle,Bernadette O-Briend,Mark Roantree*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度学习的牧草生长预测方法，旨在解决爱尔兰乳制品行业面临的盈利能力和可持续性挑战。


<details>
  <summary>Details</summary>
Motivation: 爱尔兰乳制品业是重要的经济贡献者，但面临盈利能力和可持续性的挑战。现有的牧草生长预测依赖于不切实际的机械模型。

Method: 使用专为单变量数据集设计的深度学习模型，特别是时间卷积网络，用于预测多年生黑麦草的生长。

Result: 在科克进行的多年生黑麦草生长预测表现出高性能，RMSE为2.74，MAE为3.46。对跨越34年1757周的综合数据集的验证，提供了关于最佳模型配置的见解。

Conclusion: 该研究提高了我们对模型行为的理解，从而提高了牧草生长预测的可靠性，并有助于可持续乳制品农业实践的进步。

Abstract: Grasslands, constituting the world's second-largest terrestrial carbon sink,
play a crucial role in biodiversity and the regulation of the carbon cycle.
Currently, the Irish dairy sector, a significant economic contributor, grapples
with challenges related to profitability and sustainability. Presently, grass
growth forecasting relies on impractical mechanistic models. In response, we
propose deep learning models tailored for univariate datasets, presenting
cost-effective alternatives. Notably, a temporal convolutional network designed
for forecasting Perennial Ryegrass growth in Cork exhibits high performance,
leveraging historical grass height data with RMSE of 2.74 and MAE of 3.46.
Validation across a comprehensive dataset spanning 1,757 weeks over 34 years
provides insights into optimal model configurations. This study enhances our
understanding of model behavior, thereby improving reliability in grass growth
forecasting and contributing to the advancement of sustainable dairy farming
practices.

</details>


### [61] [Federated Learning with Gramian Angular Fields for Privacy-Preserving ECG Classification on Heterogeneous IoT Devices](https://arxiv.org/abs/2511.03753)
*Youssef Elmir,Yassine Himeur,Abbes Amira*

Main category: cs.LG

TL;DR: 提出了一种基于联邦学习(FL)的框架，用于在物联网(IoT)医疗环境中进行保护隐私的心电图(ECG)分类。


<details>
  <summary>Details</summary>
Motivation: 在物联网医疗环境中，保护病人隐私的心电图(ECG)分类需求。

Method: 将一维心电信号转换为二维格拉米角场(GAF)图像，通过卷积神经网络(CNN)实现高效特征提取。

Result: FL-GAF模型在多客户端设置中实现了95.18%的高分类精度，在精度和训练时间上都明显优于单客户端基线。

Conclusion: 该框架突出了轻量级、保护隐私的人工智能在基于物联网的医疗监控中的潜力，支持智能健康系统中可扩展且安全的边缘部署。

Abstract: This study presents a federated learning (FL) framework for
privacy-preserving electrocardiogram (ECG) classification in Internet of Things
(IoT) healthcare environments. By transforming 1D ECG signals into 2D Gramian
Angular Field (GAF) images, the proposed approach enables efficient feature
extraction through Convolutional Neural Networks (CNNs) while ensuring that
sensitive medical data remain local to each device. This work is among the
first to experimentally validate GAF-based federated ECG classification across
heterogeneous IoT devices, quantifying both performance and communication
efficiency. To evaluate feasibility in realistic IoT settings, we deployed the
framework across a server, a laptop, and a resource-constrained Raspberry Pi 4,
reflecting edge-cloud integration in IoT ecosystems. Experimental results
demonstrate that the FL-GAF model achieves a high classification accuracy of
95.18% in a multi-client setup, significantly outperforming a single-client
baseline in both accuracy and training time. Despite the added computational
complexity of GAF transformations, the framework maintains efficient resource
utilization and communication overhead. These findings highlight the potential
of lightweight, privacy-preserving AI for IoT-based healthcare monitoring,
supporting scalable and secure edge deployments in smart health systems.

</details>


### [62] [Laugh, Relate, Engage: Stylized Comment Generation for Short Videos](https://arxiv.org/abs/2511.03757)
*Xuan Ouyang,Senan Wang,Bouzhou Wang,Siyuan Xiahou,Jinrong Zhou,Yuekang Li*

Main category: cs.LG

TL;DR: 本研究提出了一种名为LOLGORITHM的多智能体系统，用于生成可控的短视频评论，该系统集成了视频分割、上下文和情感分析以及风格感知提示构建。


<details>
  <summary>Details</summary>
Motivation: 短视频平台中，评论在促进社区参与和内容再创作方面起着至关重要的作用。然而，生成既符合平台准则，又具有风格多样性和上下文感知能力的评论仍然是一个重大挑战。

Method: 该系统采用模块化设计，利用多模态大型语言模型直接处理视频输入，并通过显式提示标记和少量示例实现细粒度的风格控制。同时，构建了一个双语数据集以支持开发和评估。

Result: 实验结果表明，LOLGORITHM显著优于基线模型，在抖音和YouTube上的偏好率分别超过90%和87.55%。

Conclusion: 这项工作提出了一个可扩展且文化适应性强的短视频平台风格化评论生成框架，为增强用户参与度和创造性互动提供了一条有希望的途径。

Abstract: Short-video platforms have become a central medium in the modern Internet
landscape, where efficient information delivery and strong interactivity are
reshaping user engagement and cultural dissemination. Among the various forms
of user interaction, comments play a vital role in fostering community
participation and enabling content re-creation. However, generating comments
that are both compliant with platform guidelines and capable of exhibiting
stylistic diversity and contextual awareness remains a significant challenge.
We introduce LOLGORITHM, a modular multi-agent system (MAS) designed for
controllable short-video comment generation. The system integrates video
segmentation, contextual and affective analysis, and style-aware prompt
construction. It supports six distinct comment styles: puns (homophones),
rhyming, meme application, sarcasm (irony), plain humor, and content
extraction. Powered by a multimodal large language model (MLLM), LOLGORITHM
directly processes video inputs and achieves fine-grained style control through
explicit prompt markers and few-shot examples. To support development and
evaluation, we construct a bilingual dataset using official APIs from Douyin
(Chinese) and YouTube (English), covering five popular video genres: comedy
skits, daily life jokes, funny animal clips, humorous commentary, and talk
shows. Evaluation combines automated metrics originality, relevance, and style
conformity with a large-scale human preference study involving 40 videos and
105 participants. Results show that LOLGORITHM significantly outperforms
baseline models, achieving preference rates of over 90% on Douyin and 87.55% on
YouTube. This work presents a scalable and culturally adaptive framework for
stylized comment generation on short-video platforms, offering a promising path
to enhance user engagement and creative interaction.

</details>


### [63] [What's in Common? Multimodal Models Hallucinate When Reasoning Across Scenes](https://arxiv.org/abs/2511.03768)
*Candace Ross,Florian Bordes,Adina Williams,Polina Kirichenko,Mark Ibrahim*

Main category: cs.LG

TL;DR: 现有的多模态语言模型在处理真实场景时存在幻觉问题，为了解决这个问题，作者构建了一个新的基准测试 Common-O，它包含超过 10.5k 个在实际场景中拍摄的图像，用于考察模型在场景中进行推理的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态模型在感知基准测试中表现出色，但在真实场景推理中存在差距，为了弥补这一差距。

Method: 作者构建了一个名为 Common-O 的新基准测试，该基准测试包含超过 10.5k 个示例，全部使用新的图像，避免了网络训练数据的污染。Common-O 受到人类认知测试的启发，通过询问“What's in common?”来探究跨场景的推理。

Result: 即使是最好的模型，包括专门训练进行思维链推理的模型，在跨场景推理方面仍然面临很大的挑战。在 Common-O 上，性能最佳的模型仅达到 35% 的准确率，而在更复杂的 Common-O Complex 上，最佳模型的准确率仅为 1%。当场景中存在相似的对象时，模型更容易产生幻觉。

Conclusion: 作者发现，规模可以提供适度的改进，而使用多图像输入进行显式训练的模型表现出更大的改进，这表明规模化的多图像训练可能是有希望的。作者公开了他们的基准测试，以促进对跨场景推理时幻觉挑战的研究。

Abstract: Multimodal language models possess a remarkable ability to handle an
open-vocabulary's worth of objects. Yet the best models still suffer from
hallucinations when reasoning about scenes in the real world, revealing a gap
between their seemingly strong performance on existing perception benchmarks
that are saturating and their reasoning in the real world. To address this gap,
we build a novel benchmark of in-the-wild scenes that we call Common-O. With
more than 10.5k examples using exclusively new images not found in web training
data to avoid contamination, Common-O goes beyond just perception, inspired by
cognitive tests for humans, to probe reasoning across scenes by asking "what's
in common?". We evaluate leading multimodal language models, including models
specifically trained to perform chain-of-thought reasoning. We find that
perceiving objects in single images is tractable for most models, yet reasoning
across scenes is very challenging even for the best models, including reasoning
models. Despite saturating many leaderboards focusing on perception, the best
performing model only achieves 35% on Common-O -- and on Common-O Complex,
consisting of more complex scenes, the best model achieves only 1%. Curiously,
we find models are more prone to hallucinate when similar objects are present
in the scene, suggesting models may be relying on object co-occurrence seen
during training. Among the models we evaluated, we found scale can provide
modest improvements while models explicitly trained with multi-image inputs
show bigger improvements, suggesting scaled multi-image training may offer
promise. We make our benchmark publicly available to spur research into the
challenge of hallucination when reasoning across scenes.

</details>


### [64] [Contamination Detection for VLMs using Multi-Modal Semantic Perturbation](https://arxiv.org/abs/2511.03774)
*Jaden Park,Mu Cai,Feng Yao,Jingbo Shang,Soochahn Lee,Yong Jae Lee*

Main category: cs.LG

TL;DR: 这篇论文提出了一种检测被污染的视觉语言模型（VLM）的新方法，通过多模态语义扰动来识别在预训练过程中可能泄露测试集的模型。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型由于使用大规模的专有预训练数据，存在测试集泄露的风险，导致性能虚高，但目前缺乏有效的检测方法。

Method: 该论文提出了一种基于多模态语义扰动的新检测方法，通过观察模型在受控扰动下的泛化能力来判断其是否被污染。

Result: 实验表明，现有的检测方法要么失效，要么表现不稳定，而该论文提出的方法在多种实际污染策略下均表现出鲁棒性和有效性。

Conclusion: 该论文成功提出了一种有效检测被污染视觉语言模型的方法，并通过实验验证了其性能。

Abstract: Recent advances in Vision-Language Models (VLMs) have achieved
state-of-the-art performance on numerous benchmark tasks. However, the use of
internet-scale, often proprietary, pretraining corpora raises a critical
concern for both practitioners and users: inflated performance due to test-set
leakage. While prior works have proposed mitigation strategies such as
decontamination of pretraining data and benchmark redesign for LLMs, the
complementary direction of developing detection methods for contaminated VLMs
remains underexplored. To address this gap, we deliberately contaminate
open-source VLMs on popular benchmarks and show that existing detection
approaches either fail outright or exhibit inconsistent behavior. We then
propose a novel simple yet effective detection method based on multi-modal
semantic perturbation, demonstrating that contaminated models fail to
generalize under controlled perturbations. Finally, we validate our approach
across multiple realistic contamination strategies, confirming its robustness
and effectiveness. The code and perturbed dataset will be released publicly.

</details>


### [65] [FusionDP: Foundation Model-Assisted Differentially Private Learning for Partially Sensitive Features](https://arxiv.org/abs/2511.03806)
*Linghui Zeng,Ruixuan Liu,Atiquer Rahman Sarkar,Xiaoqian Jiang,Joyce C. Ho,Li Xiong*

Main category: cs.LG

TL;DR: 提出了一种名为FusionDP的两步框架，用于在特征级别差分隐私下提高模型效用。该框架首先利用大型基础模型根据非敏感特征推断敏感特征，然后引入一种改进的DP-SGD算法，在原始特征和推断特征上训练模型，同时正式保护原始敏感特征的隐私。


<details>
  <summary>Details</summary>
Motivation: 在隐私保护机器学习中，确保敏感训练数据的隐私至关重要。在实际场景中，可能只需要对一部分特征进行隐私保护。传统的DP-SGD对一个样本中的所有特征强制执行隐私保护，导致过度的噪声注入和显著的效用降低。

Method: FusionDP 框架包含两个步骤：1) 利用大型基础模型根据非敏感特征推断敏感特征；2) 引入一种改进的DP-SGD算法，在原始特征和推断特征上训练模型，同时正式保护原始敏感特征的隐私。

Result: 在 PhysioNet 的表格数据上的脓毒症预测任务和 MIMIC-III 的临床笔记分类任务中，FusionDP 显著提高了模型性能，同时保持了严格的特征级别隐私。

Conclusion: FusionDP 证明了基础模型驱动的推断在增强各种模态的隐私-效用权衡方面的潜力。

Abstract: Ensuring the privacy of sensitive training data is crucial in
privacy-preserving machine learning. However, in practical scenarios, privacy
protection may be required for only a subset of features. For instance, in ICU
data, demographic attributes like age and gender pose higher privacy risks due
to their re-identification potential, whereas raw lab results are generally
less sensitive. Traditional DP-SGD enforces privacy protection on all features
in one sample, leading to excessive noise injection and significant utility
degradation. We propose FusionDP, a two-step framework that enhances model
utility under feature-level differential privacy. First, FusionDP leverages
large foundation models to impute sensitive features given non-sensitive
features, treating them as external priors that provide high-quality estimates
of sensitive attributes without accessing the true values during model
training. Second, we introduce a modified DP-SGD algorithm that trains models
on both original and imputed features while formally preserving the privacy of
the original sensitive features. We evaluate FusionDP on two modalities: a
sepsis prediction task on tabular data from PhysioNet and a clinical note
classification task from MIMIC-III. By comparing against privacy-preserving
baselines, our results show that FusionDP significantly improves model
performance while maintaining rigorous feature-level privacy, demonstrating the
potential of foundation model-driven imputation to enhance the privacy-utility
trade-off for various modalities.

</details>


### [66] [Fair and Explainable Credit-Scoring under Concept Drift: Adaptive Explanation Frameworks for Evolving Populations](https://arxiv.org/abs/2511.03807)
*Shivogo John*

Main category: cs.LG

TL;DR: 针对信用评分系统中概念漂移导致传统SHAP解释不稳定的问题，提出了自适应解释框架，通过重校解释性和公平性来应对动态变化的信用模型。


<details>
  <summary>Details</summary>
Motivation: 传统SHAP解释技术假设静态数据和固定背景分布，当概念漂移发生时，其解释不稳定且可能不公平。现代信用评分系统的数据分布因借款人行为、经济状况和监管环境的变化而不断重塑。

Method: 结合XGBoost预测模型，提出了三种自适应SHAP变体：(A) 基于切片的解释重加权，调整特征分布偏移；(B) 漂移感知SHAP重基线，使用滑动窗口背景样本；(C) 使用增量岭回归进行在线代理校准。

Result: 自适应方法，特别是重基线和基于代理的解释，在不降低预测准确性的前提下，显著提高了时间稳定性，并减少了不同人群之间的差异性影响。鲁棒性测试证实了自适应解释在实际漂移条件下的弹性。

Conclusion: 自适应解释是一种实用的机制，可在数据驱动的信用系统以及决策模型随人口变化而演变的任何领域中，维持透明度、问责制和道德可靠性。

Abstract: Evolving borrower behaviors, shifting economic conditions, and changing
regulatory landscapes continuously reshape the data distributions underlying
modern credit-scoring systems. Conventional explainability techniques, such as
SHAP, assume static data and fixed background distributions, making their
explanations unstable and potentially unfair when concept drift occurs. This
study addresses that challenge by developing adaptive explanation frameworks
that recalibrate interpretability and fairness in dynamically evolving credit
models. Using a multi-year credit dataset, we integrate predictive modeling via
XGBoost with three adaptive SHAP variants: (A) per-slice explanation
reweighting that adjusts for feature distribution shifts, (B) drift-aware SHAP
rebaselining with sliding-window background samples, and (C) online surrogate
calibration using incremental Ridge regression. Each method is benchmarked
against static SHAP explanations using metrics of predictive performance (AUC,
F1), directional and rank stability (cosine, Kendall tau), and fairness
(demographic parity and recalibration). Results show that adaptive methods,
particularly rebaselined and surrogate-based explanations, substantially
improve temporal stability and reduce disparate impact across demographic
groups without degrading predictive accuracy. Robustness tests, including
counterfactual perturbations, background sensitivity analysis, and
proxy-variable detection, confirm the resilience of adaptive explanations under
real-world drift conditions. These findings establish adaptive explainability
as a practical mechanism for sustaining transparency, accountability, and
ethical reliability in data-driven credit systems, and more broadly, in any
domain where decision models evolve with population change.

</details>


### [67] [Optimizing Reasoning Efficiency through Prompt Difficulty Prediction](https://arxiv.org/abs/2511.03808)
*Bo Zhao,Berkcan Kapusuzoglu,Kartik Balasubramaniam,Sambit Sahu,Supriyo Chakraborty,Genta Indra Winata*

Main category: cs.LG

TL;DR: 提出了一个路由方法，将每个问题分配给最有可能解决它的最小模型，从而在不牺牲准确性的前提下减少计算。


<details>
  <summary>Details</summary>
Motivation: 由于规模和推理轨迹长，推理语言模型在复杂任务上表现良好，但部署成本高昂。

Method: 使用来自 s1.1-32B 的中间表示，我们训练问题难度或模型正确性的轻量级预测器，以指导跨推理模型池的路由。

Result: 在不同的数学基准测试中，路由提高了效率，超过了随机分配，并且在使用明显更少的计算资源的同时，匹配了 s1.1-32B 的性能。

Conclusion: 我们的结果表明，难度感知路由对于推理模型的经济高效部署是有效的。

Abstract: Reasoning language models perform well on complex tasks but are costly to
deploy due to their size and long reasoning traces. We propose a routing
approach that assigns each problem to the smallest model likely to solve it,
reducing compute without sacrificing accuracy. Using intermediate
representations from s1.1-32B, we train lightweight predictors of problem
difficulty or model correctness to guide routing across a pool of reasoning
models. On diverse math benchmarks, routing improves efficiency over random
assignment and matches s1.1-32B's performance while using significantly less
compute. Our results demonstrate that difficulty-aware routing is effective for
cost-efficient deployment of reasoning models.

</details>


### [68] [Ground-Truth Subgraphs for Better Training and Evaluation of Knowledge Graph Augmented LLMs](https://arxiv.org/abs/2511.04473)
*Alberto Cattaneo,Carlo Luschi,Daniel Justus*

Main category: cs.LG

TL;DR: 提出了一个从知识图谱中生成高质量合成问答数据集的框架，以改进大型语言模型的知识图谱检索。


<details>
  <summary>Details</summary>
Motivation: 缺乏具有ground-truth targets的QA数据集，难以比较知识图谱检索方法。

Method: 提出了SynthKGQA框架，用于从任何知识图谱生成合成知识图谱问答数据集。

Result: 展示了SynthKGQA生成的数据可以更有效地进行KG检索的基准测试，并训练更好的模型。利用SynthKGQA在Wikidata上生成了GTSQA数据集。

Conclusion: GTSQA数据集旨在测试KG检索器在未见过的图结构和关系类型方面的zero-shot泛化能力，并对流行的KG-augmented LLM解决方案进行了基准测试。

Abstract: Retrieval of information from graph-structured knowledge bases represents a
promising direction for improving the factuality of LLMs. While various
solutions have been proposed, a comparison of methods is difficult due to the
lack of challenging QA datasets with ground-truth targets for graph retrieval.
We present SynthKGQA, a framework for generating high-quality synthetic
Knowledge Graph Question Answering datasets from any Knowledge Graph, providing
the full set of ground-truth facts in the KG to reason over each question. We
show how, in addition to enabling more informative benchmarking of KG
retrievers, the data produced with SynthKGQA also allows us to train better
models. We apply SynthKGQA to Wikidata to generate GTSQA, a new dataset
designed to test zero-shot generalization abilities of KG retrievers with
respect to unseen graph structures and relation types, and benchmark popular
solutions for KG-augmented LLMs on it.

</details>


### [69] [One Size Does Not Fit All: Architecture-Aware Adaptive Batch Scheduling with DEBA](https://arxiv.org/abs/2511.03809)
*François Belias,Naser Ezzati-Jivan,Foutse Khomh*

Main category: cs.LG

TL;DR: DEBA是一种自适应批次大小调整方法，它通过监控梯度方差、梯度范数变化和损失变化来指导批次大小调整。实验表明，不同架构从自适应批次大小调整中获得的收益不同，轻量级和中等深度的架构受益最大。


<details>
  <summary>Details</summary>
Motivation: 现有的自适应批次大小方法对所有架构采用相同的策略，假设存在一种通用的解决方案，但本文认为架构会影响自适应效果。

Method: DEBA方法监控梯度方差、梯度范数变化和损失变化来指导批次大小调整，并在六个架构上进行了评估。

Result: 轻量级和中等深度的架构获得了45-62%的训练加速和1-7%的精度提升；浅层残差网络表现出持续的精度和速度提升；深度残差网络表现出高方差和偶尔的性能下降；已经稳定的架构表现出最小的速度提升。梯度稳定性指标可以预测哪些架构将从自适应调度中受益。

Conclusion: 自适应方法不能在所有架构上推广，批次大小调整需要架构感知的设计。

Abstract: Adaptive batch size methods aim to accelerate neural network training, but
existing approaches apply identical adaptation strategies across all
architectures, assuming a one-size-fits-all solution. We introduce DEBA
(Dynamic Efficient Batch Adaptation), an adaptive batch scheduler that monitors
gradient variance, gradient norm variation and loss variation to guide batch
size adaptations. Through systematic evaluation across six architectures
(ResNet-18/50, DenseNet-121, EfficientNet-B0, MobileNet-V3, ViT-B16) on
CIFAR-10 and CIFAR-100, with five random seeds per configuration, we
demonstrate that the architecture fundamentally determines adaptation efficacy.
Our findings reveal that: (1) lightweight and medium-depth architectures
(MobileNet-V3, DenseNet-121, EfficientNet-B0) achieve a 45-62% training speedup
with simultaneous accuracy improvements of 1-7%; (2) shallow residual networks
(ResNet-18) show consistent gains of +2.4 - 4.0% in accuracy, 36 - 43% in
speedup, while deep residual networks (ResNet-50) exhibit high variance and
occasional degradation; (3) already-stable architectures (ViT-B16) show minimal
speedup (6%) despite maintaining accuracy, indicating that adaptation benefits
vary with baseline optimization characteristics. We introduce a baseline
characterization framework using gradient stability metrics (stability score,
gradient norm variation) that predicts which architectures will benefit from
adaptive scheduling. Our ablation studies reveal critical design choices often
overlooked in prior work: sliding window statistics (vs. full history) and
sufficient cooldown periods (5+ epochs) between adaptations are essential for
success. This work challenges the prevailing assumption that adaptive methods
generalize across architectures and provides the first systematic evidence that
batch size adaptation requires an architecture-aware design.

</details>


### [70] [Sketch-Augmented Features Improve Learning Long-Range Dependencies in Graph Neural Networks](https://arxiv.org/abs/2511.03824)
*Ryien Hosseini,Filippo Simini,Venkatram Vishwanath,Rebecca Willett,Henry Hoffmann*

Main category: cs.LG

TL;DR: 该论文提出了一种新的GNN框架，通过注入随机全局嵌入来有效捕获远程依赖，从而改进GNN的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的GNN存在三个关键挑战：长程信息过度压缩、节点表示过度平滑和表达能力有限。

Method: 该论文将节点特征的随机全局嵌入（称为Sketched Random Features）注入到标准GNN中。

Result: 在真实图学习任务上的实验结果表明，该策略持续提高了baseline GNN的性能。

Conclusion: 该方法可作为GNN的独立解决方案，也可作为对现有技术的补充增强。

Abstract: Graph Neural Networks learn on graph-structured data by iteratively
aggregating local neighborhood information. While this local message passing
paradigm imparts a powerful inductive bias and exploits graph sparsity, it also
yields three key challenges: (i) oversquashing of long-range information, (ii)
oversmoothing of node representations, and (iii) limited expressive power. In
this work we inject randomized global embeddings of node features, which we
term \textit{Sketched Random Features}, into standard GNNs, enabling them to
efficiently capture long-range dependencies. The embeddings are unique,
distance-sensitive, and topology-agnostic -- properties which we analytically
and empirically show alleviate the aforementioned limitations when injected
into GNNs. Experimental results on real-world graph learning tasks confirm that
this strategy consistently improves performance over baseline GNNs, offering
both a standalone solution and a complementary enhancement to existing
techniques such as graph positional encodings. Our source code is available at
\href{https://github.com/ryienh/sketched-random-features}{https://github.com/ryienh/sketched-random-features}.

</details>


### [71] [From Static to Dynamic: Enhancing Offline-to-Online Reinforcement Learning via Energy-Guided Diffusion Stratification](https://arxiv.org/abs/2511.03828)
*Lipeng Zu,Hansong Zhou,Xiaonan Zhang*

Main category: cs.LG

TL;DR: 提出了一种名为能量引导扩散分层 (StratDiff) 的新方法，以实现离线到在线强化学习中的平滑过渡。


<details>
  <summary>Details</summary>
Motivation: 由于离线数据集中的固定行为策略与在线学习期间的演化策略之间的分布差异，从离线到在线强化学习的过渡带来了严峻的挑战。现有方法很少尝试明确评估或利用离线数据本身的分布结构。

Method: 部署扩散模型以学习离线数据集的先验知识，然后通过基于能量的函数改进该知识，以改进策略模仿并在在线微调期间生成类似离线的动作。计算生成的动作和相应的采样动作之间的 KL 散度，用于将训练批次分层为类似离线和类似在线的子集。类似离线的样本使用离线目标更新，而类似在线的样本遵循在线学习策略。

Result: 在 D4RL 基准测试中进行了广泛的实证评估，结果表明 StratDiff 显著优于现有方法，在各种 RL 设置中实现了增强的适应性和更稳定的性能。

Conclusion: StratDiff 是一种有效的方法，可以提高离线到在线强化学习的性能。

Abstract: Transitioning from offline to online reinforcement learning (RL) poses
critical challenges due to distributional shifts between the fixed behavior
policy in the offline dataset and the evolving policy during online learning.
Although this issue is widely recognized, few methods attempt to explicitly
assess or utilize the distributional structure of the offline data itself,
leaving a research gap in adapting learning strategies to different types of
samples. To address this challenge, we propose an innovative method,
Energy-Guided Diffusion Stratification (StratDiff), which facilitates smoother
transitions in offline-to-online RL. StratDiff deploys a diffusion model to
learn prior knowledge from the offline dataset. It then refines this knowledge
through energy-based functions to improve policy imitation and generate
offline-like actions during online fine-tuning. The KL divergence between the
generated action and the corresponding sampled action is computed for each
sample and used to stratify the training batch into offline-like and
online-like subsets. Offline-like samples are updated using offline objectives,
while online-like samples follow online learning strategies. We demonstrate the
effectiveness of StratDiff by integrating it with off-the-shelf methods Cal-QL
and IQL. Extensive empirical evaluations on D4RL benchmarks show that StratDiff
significantly outperforms existing methods, achieving enhanced adaptability and
more stable performance across diverse RL settings.

</details>


### [72] [Higher-Order Causal Structure Learning with Additive Models](https://arxiv.org/abs/2511.03831)
*James Enouen,Yujia Zheng,Ignavier Ng,Yan Liu,Kun Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种扩展的因果加性模型(CAM)，以处理具有高阶交互作用的加性模型。


<details>
  <summary>Details</summary>
Motivation: 现实世界中存在大量表现出高阶机制的过程，然而，因果发现中对交互作用的显式处理却很少受到关注。

Method: 将CAM扩展到具有高阶交互作用的加性模型，并引入有向无环超图来表示这种结构。同时，还扩展了典型的马尔可夫等价类。

Result: 结果表明，学习更复杂的超图结构可能导致更好的经验结果。更严格的假设，如CAM，对应于更容易学习的超DAG和更好的有限样本复杂度。

Conclusion: 开发了贪婪CAM算法的扩展，它可以处理更复杂的超DAG搜索空间，并在综合实验中证明了其经验有效性。

Abstract: Causal structure learning has long been the central task of inferring causal
insights from data. Despite the abundance of real-world processes exhibiting
higher-order mechanisms, however, an explicit treatment of interactions in
causal discovery has received little attention. In this work, we focus on
extending the causal additive model (CAM) to additive models with higher-order
interactions. This second level of modularity we introduce to the structure
learning problem is most easily represented by a directed acyclic hypergraph
which extends the DAG. We introduce the necessary definitions and theoretical
tools to handle the novel structure we introduce and then provide
identifiability results for the hyper DAG, extending the typical Markov
equivalence classes. We next provide insights into why learning the more
complex hypergraph structure may actually lead to better empirical results. In
particular, more restrictive assumptions like CAM correspond to easier-to-learn
hyper DAGs and better finite sample complexity. We finally develop an extension
of the greedy CAM algorithm which can handle the more complex hyper DAG search
space and demonstrate its empirical usefulness in synthetic experiments.

</details>


### [73] [Enhancing Q-Value Updates in Deep Q-Learning via Successor-State Prediction](https://arxiv.org/abs/2511.03836)
*Lipeng Zu,Hansong Zhou,Xiaonan Zhang*

Main category: cs.LG

TL;DR: 提出了一种名为Successor-state Aggregation Deep Q-Network (SADQ) 的新方法，通过显式地使用随机转移模型对环境动态进行建模，从而改进 DQN 的训练稳定性。


<details>
  <summary>Details</summary>
Motivation: DQN 的目标更新依赖于过去策略产生的次优状态，导致学习信号的信息量不足和更新过程中的高方差。当抽样的转移与智能体当前策略 плохо 对齐时，问题会加剧。

Method: SADQ 通过将后继状态分布整合到 Q 值的估计过程中，从而实现更稳定和与策略对齐的值更新。此外，它还利用建模的转移结构探索更有效的动作选择策略。

Result: 在标准 RL 基准和实际的基于向量控制任务中，SADQ 在稳定性和学习效率方面始终优于 DQN 变体。

Conclusion: SADQ 保持了无偏的值估计，同时降低了训练方差，并在实验中表现出优于 DQN 变体的性能。

Abstract: Deep Q-Networks (DQNs) estimate future returns by learning from transitions
sampled from a replay buffer. However, the target updates in DQN often rely on
next states generated by actions from past, potentially suboptimal, policy. As
a result, these states may not provide informative learning signals, causing
high variance into the update process. This issue is exacerbated when the
sampled transitions are poorly aligned with the agent's current policy. To
address this limitation, we propose the Successor-state Aggregation Deep
Q-Network (SADQ), which explicitly models environment dynamics using a
stochastic transition model. SADQ integrates successor-state distributions into
the Q-value estimation process, enabling more stable and policy-aligned value
updates. Additionally, it explores a more efficient action selection strategy
with the modeled transition structure. We provide theoretical guarantees that
SADQ maintains unbiased value estimates while reducing training variance. Our
extensive empirical results across standard RL benchmarks and real-world
vector-based control tasks demonstrate that SADQ consistently outperforms DQN
variants in both stability and learning efficiency.

</details>


### [74] [Benchmark Datasets for Lead-Lag Forecasting on Social Platforms](https://arxiv.org/abs/2511.03877)
*Kimia Kazemian,Zhenzhen Liu,Yangfanyu Yang,Katie Z Luo,Shuhan Gu,Audrey Du,Xinyu Yang,Jack Jansons,Kilian Q Weinberger,John Thickstun,Yian Yin,Sarah Dean*

Main category: cs.LG

TL;DR: 本文介绍了前导-滞后预测 (LLF) 的概念，即根据早期交互（前导）预测相关但时间偏移的结果（滞后）。


<details>
  <summary>Details</summary>
Motivation: 尽管这种模式很普遍，但由于缺乏标准化数据集，LLF 尚未被视为时间序列社区中的统一预测问题。

Method: 本文提出了两个大型基准数据集 arXiv 和 GitHub，并概述了具有类似前导-滞后动态的其他领域。

Result: 本文记录了数据管理和清理的所有技术细节，通过统计和分类测试验证了前导-滞后动态的存在，并对回归的参数和非参数基线进行了基准测试。

Conclusion: 本文确立了 LLF 作为一种新的预测范式，并为其在社会和使用数据中的系统探索奠定了经验基础。

Abstract: Social and collaborative platforms emit multivariate time-series traces in
which early interactions-such as views, likes, or downloads-are followed,
sometimes months or years later, by higher impact like citations, sales, or
reviews. We formalize this setting as Lead-Lag Forecasting (LLF): given an
early usage channel (the lead), predict a correlated but temporally shifted
outcome channel (the lag). Despite the ubiquity of such patterns, LLF has not
been treated as a unified forecasting problem within the time-series community,
largely due to the absence of standardized datasets. To anchor research in LLF,
here we present two high-volume benchmark datasets-arXiv (accesses -> citations
of 2.3M papers) and GitHub (pushes/stars -> forks of 3M repositories)-and
outline additional domains with analogous lead-lag dynamics, including
Wikipedia (page views -> edits), Spotify (streams -> concert attendance),
e-commerce (click-throughs -> purchases), and LinkedIn profile (views ->
messages). Our datasets provide ideal testbeds for lead-lag forecasting, by
capturing long-horizon dynamics across years, spanning the full spectrum of
outcomes, and avoiding survivorship bias in sampling. We documented all
technical details of data curation and cleaning, verified the presence of
lead-lag dynamics through statistical and classification tests, and benchmarked
parametric and non-parametric baselines for regression. Our study establishes
LLF as a novel forecasting paradigm and lays an empirical foundation for its
systematic exploration in social and usage data. Our data portal with downloads
and documentation is available at https://lead-lag-forecasting.github.io/.

</details>


### [75] [DecoHD: Decomposed Hyperdimensional Classification under Extreme Memory Budgets](https://arxiv.org/abs/2511.03911)
*Sanggeon Yun,Hyunwoo Oh,Ryozo Masukawa,Mohsen Imani*

Main category: cs.LG

TL;DR: DecoHD: Compresses hyperdimensional computing models via learned decomposition, achieving significant memory and energy savings with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Reducing the footprint of hyperdimensional computing (HDC) models typically sacrifices robustness. The paper aims to address this by compressing learned class prototypes.

Method: Introduces DecoHD, which learns a decomposed HDC parameterization with shared per-layer channels and multiplicative binding. It compresses along the class axis using a lightweight bundling head and is trained end-to-end.

Result: DecoHD achieves extreme memory savings with minor accuracy degradation, is more robust to noise, and requires significantly fewer trainable parameters. Hardware evaluations show substantial energy and speed gains compared to CPU, GPU, and baseline HDC ASIC.

Conclusion: DecoHD effectively compresses HDC models while maintaining accuracy and improving hardware performance.

Abstract: Decomposition is a proven way to shrink deep networks without changing I/O.
We bring this idea to hyperdimensional computing (HDC), where footprint cuts
usually shrink the feature axis and erode concentration and robustness. Prior
HDC decompositions decode via fixed atomic hypervectors, which are ill-suited
for compressing learned class prototypes. We introduce DecoHD, which learns
directly in a decomposed HDC parameterization: a small, shared set of per-layer
channels with multiplicative binding across layers and bundling at the end,
yielding a large representational space from compact factors. DecoHD compresses
along the class axis via a lightweight bundling head while preserving native
bind-bundle-score; training is end-to-end, and inference remains pure HDC,
aligning with in/near-memory accelerators. In evaluation, DecoHD attains
extreme memory savings with only minor accuracy degradation under tight
deployment budgets. On average it stays within about 0.1-0.15% of a strong
non-reduced HDC baseline (worst case 5.7%), is more robust to random bit-flip
noise, reaches its accuracy plateau with up to ~97% fewer trainable parameters,
and -- in hardware -- delivers roughly 277x/35x energy/speed gains over a CPU
(AMD Ryzen 9 9950X), 13.5x/3.7x over a GPU (NVIDIA RTX 4090), and 2.0x/2.4x
over a baseline HDC ASIC.

</details>
