{"id": "2507.14301", "categories": ["cs.IR", "cs.CV", "cs.DB"], "pdf": "https://arxiv.org/pdf/2507.14301", "abs": "https://arxiv.org/abs/2507.14301", "authors": ["Yuxin Liu", "Yuezhang Peng", "Hefeng Zhou", "Hongze Liu", "Xinyu Lu", "Jiong Lou", "Chentao Wu", "Wei Zhao", "Jie Li"], "title": "LOVO: Efficient Complex Object Query in Large-Scale Video Datasets", "comment": "@inproceedings{liu2025lovo,title={LOVO: Efficient Complex Object\n  Query in Large-Scale Video Datasets},author={Liu, Yuxin and Peng, Yuezhang\n  and Zhou, Hefeng and Liu, Hongze and Lu, Xinyu and Lou, Jiong and Wu, Chentao\n  and Zhao, Wei and Li, Jie},booktitle={2025 IEEE 41st International Conference\n  on Data Engineering (ICDE)},pages={1938--1951},year={2025},organization={IEEE\n  Computer Society}}", "summary": "The widespread deployment of cameras has led to an exponential increase in\nvideo data, creating vast opportunities for applications such as traffic\nmanagement and crime surveillance. However, querying specific objects from\nlarge-scale video datasets presents challenges, including (1) processing\nmassive and continuously growing data volumes, (2) supporting complex query\nrequirements, and (3) ensuring low-latency execution. Existing video analysis\nmethods struggle with either limited adaptability to unseen object classes or\nsuffer from high query latency. In this paper, we present LOVO, a novel system\ndesigned to efficiently handle comp$\\underline{L}$ex $\\underline{O}$bject\nqueries in large-scale $\\underline{V}$ide$\\underline{O}$ datasets. Agnostic to\nuser queries, LOVO performs one-time feature extraction using pre-trained\nvisual encoders, generating compact visual embeddings for key frames to build\nan efficient index. These visual embeddings, along with associated bounding\nboxes, are organized in an inverted multi-index structure within a vector\ndatabase, which supports queries for any objects. During the query phase, LOVO\ntransforms object queries to query embeddings and conducts fast approximate\nnearest-neighbor searches on the visual embeddings. Finally, a cross-modal\nrerank is performed to refine the results by fusing visual features with\ndetailed textual features. Evaluation on real-world video datasets demonstrates\nthat LOVO outperforms existing methods in handling complex queries, with\nnear-optimal query accuracy and up to 85x lower search latency, while\nsignificantly reducing index construction costs. This system redefines the\nstate-of-the-art object query approaches in video analysis, setting a new\nbenchmark for complex object queries with a novel, scalable, and efficient\napproach that excels in dynamic environments.", "AI": {"tldr": "LOVO is a novel system designed to efficiently handle complex object queries in large-scale video datasets. It uses pre-trained visual encoders and a multi-index structure within a vector database to achieve high accuracy and low latency.", "motivation": "Querying specific objects from large-scale video datasets presents challenges, including (1) processing massive and continuously growing data volumes, (2) supporting complex query requirements, and (3) ensuring low-latency execution. Existing video analysis methods struggle with either limited adaptability to unseen object classes or suffer from high query latency.", "method": "LOVO performs one-time feature extraction using pre-trained visual encoders, generating compact visual embeddings for key frames to build an efficient index. These visual embeddings, along with associated bounding boxes, are organized in an inverted multi-index structure within a vector database, which supports queries for any objects. During the query phase, LOVO transforms object queries to query embeddings and conducts fast approximate nearest-neighbor searches on the visual embeddings. Finally, a cross-modal rerank is performed to refine the results by fusing visual features with detailed textual features.", "result": "LOVO outperforms existing methods in handling complex queries, with near-optimal query accuracy and up to 85x lower search latency, while significantly reducing index construction costs.", "conclusion": "LOVO outperforms existing methods in handling complex queries, with near-optimal query accuracy and up to 85x lower search latency, while significantly reducing index construction costs. This system redefines the state-of-the-art object query approaches in video analysis, setting a new benchmark for complex object queries with a novel, scalable, and efficient approach that excels in dynamic environments."}}
{"id": "2507.14352", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14352", "abs": "https://arxiv.org/abs/2507.14352", "authors": ["Huy-Son Nguyen", "Yuanna Liu", "Masoud Mansoury", "Mohammad Alian Nejadi", "Alan Hanjalic", "Maarten de Rijke"], "title": "A Reproducibility Study of Product-side Fairness in Bundle Recommendation", "comment": null, "summary": "Recommender systems are known to exhibit fairness issues, particularly on the\nproduct side, where products and their associated suppliers receive unequal\nexposure in recommended results. While this problem has been widely studied in\ntraditional recommendation settings, its implications for bundle recommendation\n(BR) remain largely unexplored. This emerging task introduces additional\ncomplexity: recommendations are generated at the bundle level, yet user\nsatisfaction and product (or supplier) exposure depend on both the bundle and\nthe individual items it contains. Existing fairness frameworks and metrics\ndesigned for traditional recommender systems may not directly translate to this\nmulti-layered setting. In this paper, we conduct a comprehensive\nreproducibility study of product-side fairness in BR across three real-world\ndatasets using four state-of-the-art BR methods. We analyze exposure\ndisparities at both the bundle and item levels using multiple fairness metrics,\nuncovering important patterns. Our results show that exposure patterns differ\nnotably between bundles and items, revealing the need for fairness\ninterventions that go beyond bundle-level assumptions. We also find that\nfairness assessments vary considerably depending on the metric used,\nreinforcing the need for multi-faceted evaluation. Furthermore, user behavior\nplays a critical role: when users interact more frequently with bundles than\nwith individual items, BR systems tend to yield fairer exposure distributions\nacross both levels. Overall, our findings offer actionable insights for\nbuilding fairer bundle recommender systems and establish a vital foundation for\nfuture research in this emerging domain.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4e00\u63fd\u5b50\u63a8\u8350\u4e2d\u7684\u4ea7\u54c1\u65b9\u516c\u5e73\u6027\u95ee\u9898\uff0c\u53d1\u73b0\u66dd\u5149\u6a21\u5f0f\u5728\u4e0d\u540c\u7684\u4e00\u63fd\u5b50\u548c\u9879\u76ee\u4e4b\u95f4\u5b58\u5728\u5dee\u5f02\uff0c\u4e14\u516c\u5e73\u8bc4\u4f30\u56e0\u6307\u6807\u800c\u5f02\u3002", "motivation": "\u63a8\u8350\u7cfb\u7edf\u5b58\u5728\u516c\u5e73\u6027\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u4ea7\u54c1\u65b9\u9762\uff0c\u4ea7\u54c1\u53ca\u5176\u76f8\u5173\u4f9b\u5e94\u5546\u5728\u63a8\u8350\u7ed3\u679c\u4e2d\u83b7\u5f97\u7684\u66dd\u5149\u4e0d\u5747\u7b49\u3002\u867d\u7136\u8fd9\u4e2a\u95ee\u9898\u5728\u4f20\u7edf\u63a8\u8350\u8bbe\u7f6e\u4e2d\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5176\u5bf9\u4e00\u63fd\u5b50\u63a8\u8350\u7684\u5f71\u54cd\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u672a\u88ab\u63a2\u7d22\u3002\u8fd9\u9879\u65b0\u5174\u7684\u4efb\u52a1\u5f15\u5165\u4e86\u989d\u5916\u7684\u590d\u6742\u6027\uff1a\u63a8\u8350\u662f\u5728\u4e00\u63fd\u5b50\u5c42\u9762\u751f\u6210\u7684\uff0c\u4f46\u7528\u6237\u6ee1\u610f\u5ea6\u548c\u4ea7\u54c1\uff08\u6216\u4f9b\u5e94\u5546\uff09\u66dd\u5149\u53d6\u51b3\u4e8e\u4e00\u63fd\u5b50\u53ca\u5176\u5305\u542b\u7684\u5404\u4e2a\u9879\u76ee\u3002", "method": "\u8be5\u8bba\u6587\u5bf9\u4e09\u79cd\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4f7f\u7528\u56db\u79cd\u6700\u5148\u8fdb\u7684\u4e00\u63fd\u5b50\u63a8\u8350\u65b9\u6cd5\uff0c\u8fdb\u884c\u4e86\u4e00\u9879\u5173\u4e8e\u4e00\u63fd\u5b50\u63a8\u8350\u4e2d\u4ea7\u54c1\u65b9\u516c\u5e73\u6027\u7684\u5168\u9762\u53ef\u91cd\u590d\u6027\u7814\u7a76\u3002\u8be5\u7814\u7a76\u4f7f\u7528\u591a\u4e2a\u516c\u5e73\u6027\u6307\u6807\u5206\u6790\u4e86\u4e00\u63fd\u5b50\u548c\u9879\u76ee\u5c42\u9762\u7684\u66dd\u5149\u5dee\u5f02\uff0c\u63ed\u793a\u4e86\u91cd\u8981\u7684\u6a21\u5f0f\u3002", "result": "\u8be5\u8bba\u6587\u7684\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u7528\u6237\u4e0e\u4e00\u63fd\u5b50\u4e92\u52a8\u66f4\u9891\u7e41\u65f6\uff0c\u4e00\u63fd\u5b50\u63a8\u8350\u7cfb\u7edf\u5f80\u5f80\u4f1a\u5728\u4e24\u4e2a\u5c42\u9762\u4e0a\u4ea7\u751f\u66f4\u516c\u5e73\u7684\u66dd\u5149\u5206\u5e03\u3002\u540c\u65f6\u53d1\u73b0\uff0c\u516c\u5e73\u8bc4\u4f30\u56e0\u6240\u4f7f\u7528\u7684\u6307\u6807\u800c\u5f02\uff0c\u52a0\u5f3a\u4e86\u591a\u65b9\u9762\u8bc4\u4f30\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u8fd9\u7bc7\u8bba\u6587\u7684\u7ed3\u679c\u8868\u660e\uff0c\u4e00\u63fd\u5b50\u63a8\u8350\u4e2d\u7684\u66dd\u5149\u6a21\u5f0f\u5728\u4e0d\u540c\u7684\u4e00\u63fd\u5b50\u548c\u9879\u76ee\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u63ed\u793a\u4e86\u9700\u8981\u8d85\u8d8a\u4e00\u63fd\u5b50\u5c42\u9762\u5047\u8bbe\u7684\u516c\u5e73\u5e72\u9884\u3002\u540c\u65f6\u53d1\u73b0\uff0c\u516c\u5e73\u8bc4\u4f30\u56e0\u6240\u4f7f\u7528\u7684\u6307\u6807\u800c\u5f02\uff0c\u52a0\u5f3a\u4e86\u591a\u65b9\u9762\u8bc4\u4f30\u7684\u5fc5\u8981\u6027\u3002\u6b64\u5916\uff0c\u7528\u6237\u884c\u4e3a\u4e5f\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff1a\u5f53\u7528\u6237\u4e0e\u4e00\u63fd\u5b50\u7684\u4e92\u52a8\u6bd4\u4e0e\u5355\u4e2a\u9879\u76ee\u7684\u4e92\u52a8\u66f4\u9891\u7e41\u65f6\uff0c\u4e00\u63fd\u5b50\u63a8\u8350\u7cfb\u7edf\u5f80\u5f80\u4f1a\u5728\u4e24\u4e2a\u5c42\u9762\u4e0a\u4ea7\u751f\u66f4\u516c\u5e73\u7684\u66dd\u5149\u5206\u5e03\u3002"}}
{"id": "2507.14361", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.14361", "abs": "https://arxiv.org/abs/2507.14361", "authors": ["Huy-Son Nguyen", "Quang-Huy Nguyen", "Duc-Hoang Pham", "Duc-Trong Le", "Hoang-Quynh Le", "Padipat Sitkrongwong", "Atsuhiro Takasu", "Masoud Mansoury"], "title": "RaMen: Multi-Strategy Multi-Modal Learning for Bundle Construction", "comment": null, "summary": "Existing studies on bundle construction have relied merely on user feedback\nvia bipartite graphs or enhanced item representations using semantic\ninformation. These approaches fail to capture elaborate relations hidden in\nreal-world bundle structures, resulting in suboptimal bundle representations.\nTo overcome this limitation, we propose RaMen, a novel method that provides a\nholistic multi-strategy approach for bundle construction. RaMen utilizes both\nintrinsic (characteristics) and extrinsic (collaborative signals) information\nto model bundle structures through Explicit Strategy-aware Learning (ESL) and\nImplicit Strategy-aware Learning (ISL). ESL employs task-specific attention\nmechanisms to encode multi-modal data and direct collaborative relations\nbetween items, thereby explicitly capturing essential bundle features.\nMoreover, ISL computes hyperedge dependencies and hypergraph message passing to\nuncover shared latent intents among groups of items. Integrating diverse\nstrategies enables RaMen to learn more comprehensive and robust bundle\nrepresentations. Meanwhile, Multi-strategy Alignment & Discrimination module is\nemployed to facilitate knowledge transfer between learning strategies and\nensure discrimination between items/bundles. Extensive experiments demonstrate\nthe effectiveness of RaMen over state-of-the-art models on various domains,\njustifying valuable insights into complex item set problems.", "AI": {"tldr": "RaMen\u662f\u4e00\u79cdbundle\u6784\u5efa\u7684\u65b0\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u663e\u5f0f\u548c\u9690\u5f0f\u7b56\u7565\u611f\u77e5\u5b66\u4e60\u6765\u5efa\u6a21bundle\u7ed3\u6784\uff0c\u4ece\u800c\u5728\u591a\u4e2a\u9886\u57df\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684bundle\u6784\u5efa\u7814\u7a76\u4ec5\u4ec5\u4f9d\u8d56\u4e8e\u901a\u8fc7\u4e8c\u5206\u56fe\u7684\u7528\u6237\u53cd\u9988\u6216\u4f7f\u7528\u8bed\u4e49\u4fe1\u606f\u589e\u5f3a\u7684\u9879\u76ee\u8868\u793a\uff0c\u672a\u80fd\u6355\u6349\u5230\u73b0\u5b9e\u4e16\u754cbundle\u7ed3\u6784\u4e2d\u9690\u85cf\u7684\u7cbe\u7ec6\u5173\u7cfb\uff0c\u5bfc\u81f4\u6b21\u4f18\u7684bundle\u8868\u793a\u3002", "method": "RaMen\uff0c\u4e00\u79cd\u65b0\u9896\u7684bundle\u6784\u5efa\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u663e\u5f0f\u7b56\u7565\u611f\u77e5\u5b66\u4e60 (ESL) \u548c\u9690\u5f0f\u7b56\u7565\u611f\u77e5\u5b66\u4e60 (ISL) \u6765\u5efa\u6a21bundle\u7ed3\u6784\u3002", "result": "RaMen\u5b66\u4e60\u66f4\u5168\u9762\u548c\u5f3a\u5927\u7684bundle\u8868\u793a\u3002", "conclusion": "RaMen\u5728\u591a\u4e2a\u9886\u57df\u90fd\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u590d\u6742\u9879\u76ee\u96c6\u95ee\u9898\u4e0a\u7684\u4ef7\u503c\u3002"}}
{"id": "2507.14604", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14604", "abs": "https://arxiv.org/abs/2507.14604", "authors": ["Mathias Vast", "Basile Van Cooten", "Laure Soulier", "Benjamin Piwowarski"], "title": "Understanding Matching Mechanisms in Cross-Encoders", "comment": "Accepted at Workshop on Explainability in Information Retrieval at\n  SIGIR 25 (WExIR25)", "summary": "Neural IR architectures, particularly cross-encoders, are highly effective\nmodels whose internal mechanisms are mostly unknown. Most works trying to\nexplain their behavior focused on high-level processes (e.g., what in the input\ninfluences the prediction, does the model adhere to known IR axioms) but fall\nshort of describing the matching process. Instead of Mechanistic\nInterpretability approaches which specifically aim at explaining the hidden\nmechanisms of neural models, we demonstrate that more straightforward methods\ncan already provide valuable insights. In this paper, we first focus on the\nattention process and extract causal insights highlighting the crucial roles of\nsome attention heads in this process. Second, we provide an interpretation of\nthe mechanism underlying matching detection.", "AI": {"tldr": "This paper explains the matching process in neural IR architectures by focusing on attention heads and matching detection mechanisms.", "motivation": "The internal mechanisms of neural IR architectures, particularly cross-encoders, are mostly unknown. Most works fall short of describing the matching process.", "method": "The paper uses straightforward methods to provide valuable insights into the matching process of neural IR architectures.", "result": "Extracted causal insights highlighting the crucial roles of some attention heads and an interpretation of the mechanism underlying matching detection.", "conclusion": "This paper focuses on the attention process and extracts causal insights highlighting the crucial roles of some attention heads. It also provides an interpretation of the mechanism underlying matching detection."}}
{"id": "2507.14189", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14189", "abs": "https://arxiv.org/abs/2507.14189", "authors": ["Song Mao", "Lejun Cheng", "Pinlong Cai", "Guohang Yan", "Ding Wang", "Botian Shi"], "title": "DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On Offline Knowledge Base", "comment": "work in process", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious applications. However, their use as writing assistants in specialized\ndomains like finance, medicine, and law is often hampered by a lack of deep\ndomain-specific knowledge and a tendency to hallucinate. Existing solutions,\nsuch as Retrieval-Augmented Generation (RAG), can suffer from inconsistency\nacross multiple retrieval steps, while online search-based methods often\ndegrade quality due to unreliable web content. To address these challenges, we\nintroduce DeepWriter, a customizable, multimodal, long-form writing assistant\nthat operates on a curated, offline knowledge base. DeepWriter leverages a\nnovel pipeline that involves task decomposition, outline generation, multimodal\nretrieval, and section-by-section composition with reflection. By deeply mining\ninformation from a structured corpus and incorporating both textual and visual\nelements, DeepWriter generates coherent, factually grounded, and\nprofessional-grade documents. We also propose a hierarchical knowledge\nrepresentation to enhance retrieval efficiency and accuracy. Our experiments on\nfinancial report generation demonstrate that DeepWriter produces high-quality,\nverifiable articles that surpasses existing baselines in factual accuracy and\ngenerated content quality.", "AI": {"tldr": "DeepWriter, a customizable, multimodal writing assistant, addresses the limitations of LLMs in specialized domains by using a curated knowledge base and a novel pipeline, achieving superior factual accuracy and content quality in financial report generation.", "motivation": "LLMs lack deep domain-specific knowledge and a tendency to hallucinate in specialized domains. Existing solutions, such as RAG, can suffer from inconsistency across multiple retrieval steps, while online search-based methods often degrade quality due to unreliable web content.", "method": "DeepWriter leverages a novel pipeline that involves task decomposition, outline generation, multimodal retrieval, and section-by-section composition with reflection. We also propose a hierarchical knowledge representation to enhance retrieval efficiency and accuracy.", "result": "DeepWriter generates coherent, factually grounded, and professional-grade documents. Experiments on financial report generation demonstrate that DeepWriter produces high-quality, verifiable articles that surpasses existing baselines in factual accuracy and generated content quality.", "conclusion": "DeepWriter produces high-quality, verifiable articles that surpasses existing baselines in factual accuracy and generated content quality."}}
{"id": "2507.14376", "categories": ["cs.DB", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14376", "abs": "https://arxiv.org/abs/2507.14376", "authors": ["Osman Erman Gungor", "Derak Paulsen", "William Kang"], "title": "Schemora: schema matching via multi-stage recommendation and metadata enrichment using off-the-shelf llms", "comment": "11 pages", "summary": "Schema matching is essential for integrating heterogeneous data sources and\nenhancing dataset discovery, yet it remains a complex and resource-intensive\nproblem. We introduce SCHEMORA, a schema matching framework that combines large\nlanguage models with hybrid retrieval techniques in a prompt-based approach,\nenabling efficient identification of candidate matches without relying on\nlabeled training data or exhaustive pairwise comparisons. By enriching schema\nmetadata and leveraging both vector-based and lexical retrieval, SCHEMORA\nimproves matching accuracy and scalability. Evaluated on the MIMIC-OMOP\nbenchmark, it establishes new state-of-the-art performance, with gains of 7.49%\nin HitRate@5 and 3.75% in HitRate@3 over previous best results. To our\nknowledge, this is the first LLM-based schema matching method with an\nopen-source implementation, accompanied by analysis that underscores the\ncritical role of retrieval and provides practical guidance on model selection.", "AI": {"tldr": "SCHEMORA is an LLM-based schema matching framework that improves accuracy and scalability using hybrid retrieval, achieving state-of-the-art results on the MIMIC-OMOP benchmark.", "motivation": "Schema matching is essential for integrating heterogeneous data sources, yet it remains a complex and resource-intensive problem.", "method": "It combines large language models with hybrid retrieval techniques in a prompt-based approach.", "result": "SCHEMORA improves matching accuracy and scalability, with gains of 7.49% in HitRate@5 and 3.75% in HitRate@3 over previous best results.", "conclusion": "SCHEMORA achieves state-of-the-art schema matching performance on the MIMIC-OMOP benchmark."}}
{"id": "2507.14268", "categories": ["cs.CV", "cond-mat.mtrl-sci", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.14268", "abs": "https://arxiv.org/abs/2507.14268", "authors": ["Andreas Alpers", "Orkun Furat", "Christian Jung", "Matthias Neumann", "Claudia Redenbach", "Aigerim Saken", "Volker Schmidt"], "title": "Comparative Analysis of Algorithms for the Fitting of Tessellations to 3D Image Data", "comment": "31 pages, 16 figures, 8 tables", "summary": "This paper presents a comparative analysis of algorithmic strategies for\nfitting tessellation models to 3D image data of materials such as polycrystals\nand foams. In this steadily advancing field, we review and assess\noptimization-based methods -- including linear and nonlinear programming,\nstochastic optimization via the cross-entropy method, and gradient descent --\nfor generating Voronoi, Laguerre, and generalized balanced power diagrams\n(GBPDs) that approximate voxelbased grain structures. The quality of fit is\nevaluated on real-world datasets using discrepancy measures that quantify\ndifferences in grain volume, surface area, and topology. Our results highlight\ntrade-offs between model complexity, the complexity of the optimization\nroutines involved, and the quality of approximation, providing guidance for\nselecting appropriate methods based on data characteristics and application\nneeds.", "AI": {"tldr": "This paper reviews and assesses optimization-based methods for generating Voronoi, Laguerre, and generalized balanced power diagrams (GBPDs) that approximate voxelbased grain structures.", "motivation": "comparative analysis of algorithmic strategies for fitting tessellation models to 3D image data of materials such as polycrystals and foams", "method": "optimization-based methods including linear and nonlinear programming, stochastic optimization via the cross-entropy method, and gradient descent", "result": "quality of fit is evaluated on real-world datasets using discrepancy measures that quantify differences in grain volume, surface area, and topology", "conclusion": "This paper highlights trade-offs between model complexity, the complexity of the optimization routines involved, and the quality of approximation, providing guidance for selecting appropriate methods based on data characteristics and application needs."}}
{"id": "2507.14170", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14170", "abs": "https://arxiv.org/abs/2507.14170", "authors": ["Jaeheun Jung", "Donghun Lee"], "title": "Catalyst: a Novel Regularizer for Structured Pruning with Auxiliary Extension of Parameter Space", "comment": "ICML 2025 workshop HiLD 2025 (3rd workshop on High-dimensional\n  Learning Dynamics)", "summary": "Structured pruning aims to reduce the size and computational cost of deep\nneural networks by removing entire filters or channels. The traditional\nregularizers such as L1 or Group Lasso and its variants lead to\nmagnitude-biased pruning decisions, such that the filters with small magnitudes\nare likely to be pruned. Also, they often entail pruning results with almost\nzero margin around pruning decision boundary, such that tiny perturbation in a\nfilter magnitude can flip the pruning decision. In this paper, we identify the\nprecise algebraic condition under which pruning operations preserve model\nperformance, and use the condition to construct a novel regularizer defined in\nan extended parameter space via auxiliary catalyst variables. The proposed\nCatalyst regularization ensures fair pruning chance for each filters with\ntheoretically provable zero bias to their magnitude and robust pruning behavior\nachieved by wide-margin bifurcation of magnitudes between the preserved and the\npruned filters. The theoretical properties naturally lead to real-world\neffectiveness, as shown by empirical validations of Catalyst Pruning algorithm.\nPruning results on various datasets and models are superior to state-of-the-art\nfilter pruning methods, and at the same time confirm the predicted robust and\nfair pruning characteristics of Catalyst pruning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684Catalyst\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u7ed3\u6784\u5316\u526a\u679d\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u65e0\u504f\u3001\u9c81\u68d2\u548c\u516c\u5e73\u7684\u7279\u6027\uff0c\u5e76\u5728\u5404\u79cd\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u7684\u6027\u80fd\u3002", "motivation": "\u7ed3\u6784\u5316\u526a\u679d\u65e8\u5728\u901a\u8fc7\u79fb\u9664\u6574\u4e2a\u6ee4\u6ce2\u5668\u6216\u901a\u9053\u6765\u51cf\u5c11\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u89c4\u6a21\u548c\u8ba1\u7b97\u6210\u672c\u3002\u4f20\u7edf\u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff08\u5982 L1 \u6216 Group Lasso \u53ca\u5176\u53d8\u4f53\uff09\u4f1a\u5bfc\u81f4\u5e45\u5ea6\u504f\u5dee\u7684\u526a\u679d\u51b3\u7b56\uff0c\u56e0\u6b64\u5e45\u5ea6\u5c0f\u7684\u6ee4\u6ce2\u5668\u53ef\u80fd\u88ab\u526a\u679d\u3002\u6b64\u5916\uff0c\u5b83\u4eec\u901a\u5e38\u4f1a\u5bfc\u81f4\u526a\u679d\u7ed3\u679c\u5728\u526a\u679d\u51b3\u7b56\u8fb9\u754c\u9644\u8fd1\u51e0\u4e4e\u6ca1\u6709\u4f59\u91cf\uff0c\u56e0\u6b64\u6ee4\u6ce2\u5668\u5e45\u5ea6\u4e2d\u7684\u5fae\u5c0f\u6270\u52a8\u53ef\u80fd\u4f1a\u7ffb\u8f6c\u526a\u679d\u51b3\u7b56\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684Catalyst\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u6269\u5c55\u7684\u53c2\u6570\u7a7a\u95f4\u4e2d\u901a\u8fc7\u8f85\u52a9\u50ac\u5316\u5242\u53d8\u91cf\u5b9a\u4e49\uff0c\u786e\u4fdd\u6bcf\u4e2a\u6ee4\u6ce2\u5668\u90fd\u6709\u516c\u5e73\u7684\u526a\u679d\u673a\u4f1a\uff0c\u4e14\u7406\u8bba\u4e0a\u53ef\u8bc1\u660e\u5bf9\u5176\u5e45\u5ea6\u96f6\u504f\u5dee\uff0c\u5e76\u901a\u8fc7\u4fdd\u7559\u548c\u526a\u679d\u6ee4\u6ce2\u5668\u4e4b\u95f4\u5e45\u5ea6\u7684\u5927\u8fb9\u8ddd\u5206\u53c9\u5b9e\u73b0\u9c81\u68d2\u7684\u526a\u679d\u884c\u4e3a\u3002", "result": "\u6240\u63d0\u51fa\u7684Catalyst\u6b63\u5219\u5316\u786e\u4fdd\u6bcf\u4e2a\u6ee4\u6ce2\u5668\u90fd\u6709\u516c\u5e73\u7684\u526a\u679d\u673a\u4f1a\uff0c\u4e14\u7406\u8bba\u4e0a\u53ef\u8bc1\u660e\u5bf9\u5176\u5e45\u5ea6\u96f6\u504f\u5dee\uff0c\u5e76\u901a\u8fc7\u4fdd\u7559\u548c\u526a\u679d\u6ee4\u6ce2\u5668\u4e4b\u95f4\u5e45\u5ea6\u7684\u5927\u8fb9\u8ddd\u5206\u53c9\u5b9e\u73b0\u9c81\u68d2\u7684\u526a\u679d\u884c\u4e3a\u3002\u7406\u8bba\u6027\u8d28\u81ea\u7136\u4f1a\u5bfc\u81f4\u5b9e\u9645\u6709\u6548\u6027\uff0cCatalyst Pruning\u7b97\u6cd5\u7684\u7ecf\u9a8c\u9a8c\u8bc1\u8868\u660e\u4e86\u8fd9\u4e00\u70b9\u3002", "conclusion": "Catalyst Pruning\u7b97\u6cd5\u5728\u5404\u79cd\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u7684\u526a\u679d\u7ed3\u679c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6ee4\u6ce2\u5668\u526a\u679d\u65b9\u6cd5\uff0c\u540c\u65f6\u8bc1\u5b9e\u4e86Catalyst\u526a\u679d\u7684\u9c81\u68d2\u6027\u548c\u516c\u5e73\u526a\u679d\u7279\u6027\u3002"}}
{"id": "2507.14154", "categories": ["cs.AI", "cs.LG", "68T05, 81P68", "I.2.6; I.2.0; F.1.2"], "pdf": "https://arxiv.org/pdf/2507.14154", "abs": "https://arxiv.org/abs/2507.14154", "authors": ["Rahul Kabali"], "title": "The Free Will Equation: Quantum Field Analogies for AGI", "comment": "22 pages, 5 figures. Submitted as an arXiv preprint. All code and\n  experiment details included in appendix", "summary": "Artificial General Intelligence (AGI) research traditionally focuses on\nalgorithms that optimize for specific goals under deterministic rules. Yet,\nhuman-like intelligence exhibits adaptive spontaneity - an ability to make\nunexpected choices or free decisions not strictly dictated by past data or\nimmediate reward. This trait, often dubbed \"free will\" in a loose sense, might\nbe crucial for creativity, robust adaptation, and avoiding ruts in\nproblem-solving. This paper proposes a theoretical framework, called the Free\nWill Equation, that draws analogies from quantum field theory to endow AGI\nagents with a form of adaptive, controlled stochasticity in their\ndecision-making process. The core idea is to treat an AI agent's cognitive\nstate as a superposition of potential actions or thoughts, which collapses\nprobabilistically into a concrete action when a decision is made - much like a\nquantum wavefunction collapsing upon measurement. By incorporating mechanisms\nanalogous to quantum fields, along with intrinsic motivation terms, we aim to\nimprove an agent's ability to explore novel strategies and adapt to unforeseen\nchanges. Experiments in a non-stationary multi-armed bandit environment\ndemonstrate that agents using this framework achieve higher rewards and policy\ndiversity compared to baseline methods.", "AI": {"tldr": "The paper proposes the Free Will Equation, a framework drawing from quantum field theory, to give AGI agents adaptive spontaneity. Experiments show improved rewards and policy diversity compared to baselines.", "motivation": "human-like intelligence exhibits adaptive spontaneity - an ability to make unexpected choices or free decisions not strictly dictated by past data or immediate reward. This trait, often dubbed \"free will\" in a loose sense, might be crucial for creativity, robust adaptation, and avoiding ruts in problem-solving", "method": "This paper proposes a theoretical framework, called the Free Will Equation, that draws analogies from quantum field theory to endow AGI agents with a form of adaptive, controlled stochasticity in their decision-making process. The core idea is to treat an AI agent's cognitive state as a superposition of potential actions or thoughts, which collapses probabilistically into a concrete action when a decision is made - much like a quantum wavefunction collapsing upon measurement. By incorporating mechanisms analogous to quantum fields, along with intrinsic motivation terms", "result": "Agents using the Free Will Equation framework achieve higher rewards and policy diversity compared to baseline methods in a non-stationary multi-armed bandit environment.", "conclusion": "AGI agents using the Free Will Equation framework achieve higher rewards and policy diversity compared to baseline methods in a non-stationary multi-armed bandit environment."}}
{"id": "2507.14612", "categories": ["cs.IR", "cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2507.14612", "abs": "https://arxiv.org/abs/2507.14612", "authors": ["Pei-Xuan Li", "Wei-Yun Liang", "Fandel Lin", "Hsun-Ping Hsieh"], "title": "Enhancing POI Recommendation through Global Graph Disentanglement with POI Weighted Module", "comment": null, "summary": "Next point of interest (POI) recommendation primarily predicts future\nactivities based on users' past check-in data and current status, providing\nsignificant value to users and service providers. We observed that the popular\ncheck-in times for different POI categories vary. For example, coffee shops are\ncrowded in the afternoon because people like to have coffee to refresh after\nmeals, while bars are busy late at night. However, existing methods rarely\nexplore the relationship between POI categories and time, which may result in\nthe model being unable to fully learn users' tendencies to visit certain POI\ncategories at different times. Additionally, existing methods for modeling time\ninformation often convert it into time embeddings or calculate the time\ninterval and incorporate it into the model, making it difficult to capture the\ncontinuity of time. Finally, during POI prediction, various weighting\ninformation is often ignored, such as the popularity of each POI, the\ntransition relationships between POIs, and the distances between POIs, leading\nto suboptimal performance. To address these issues, this paper proposes a novel\nnext POI recommendation framework called Graph Disentangler with POI Weighted\nModule (GDPW). This framework aims to jointly consider POI category information\nand multiple POI weighting factors. Specifically, the proposed GDPW learns\ncategory and time representations through the Global Category Graph and the\nGlobal Category-Time Graph. Then, we disentangle category and time information\nthrough contrastive learning. After prediction, the final POI recommendation\nfor users is obtained by weighting the prediction results based on the\ntransition weights and distance relationships between POIs. We conducted\nexperiments on two real-world datasets, and the results demonstrate that the\nproposed GDPW outperforms other existing models, improving performance by 3% to\n11%.", "AI": {"tldr": "This paper proposes a novel next POI recommendation framework called Graph Disentangler with POI Weighted Module (GDPW) to jointly consider POI category information and multiple POI weighting factors.", "motivation": "Existing methods rarely explore the relationship between POI categories and time, making it difficult to capture the continuity of time, and often ignore various weighting information.", "method": "The proposed GDPW learns category and time representations through the Global Category Graph and the Global Category-Time Graph. Then, we disentangle category and time information through contrastive learning. After prediction, the final POI recommendation for users is obtained by weighting the prediction results based on the transition weights and distance relationships between POIs.", "result": "The proposed GDPW improves performance by 3% to 11% compared to existing models on two real-world datasets.", "conclusion": "The proposed GDPW outperforms other existing models, improving performance by 3% to 11%."}}
{"id": "2507.14198", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14198", "abs": "https://arxiv.org/abs/2507.14198", "authors": ["Fufang Wen", "Shichang Zhang"], "title": "Retention analysis of edited knowledge after fine-tuning", "comment": null, "summary": "Large language models (LLMs) store vast amounts of knowledge, which often\nrequires updates to correct factual errors, incorporate newly acquired\ninformation, or adapt model behavior. Model editing methods have emerged as\nefficient solutions for such updates, offering localized and precise knowledge\nmodification at significantly lower computational cost than continual training.\nIn parallel, LLMs are frequently fine-tuned for a wide range of downstream\ntasks. However, the effect of fine-tuning on previously edited knowledge\nremains poorly understood. In this work, we systematically investigate how\ndifferent fine-tuning objectives interact with various model editing\ntechniques. Our findings show that edited knowledge is substantially more\nsusceptible to forgetting during fine-tuning than intrinsic knowledge acquired\nthrough pre-training. This analysis highlights a key limitation of current\nediting approaches and suggests that evaluating edit robustness under\ndownstream fine-tuning is critical for their practical deployment. We further\nfind that freezing layers associated with edited content can significantly\nimprove knowledge retention, offering insight into how future editing methods\nmight be made more robust.", "AI": {"tldr": "\u5fae\u8c03\u66f4\u5bb9\u6613\u4f7fLLM\u5fd8\u8bb0\u7f16\u8f91\u8fc7\u7684\u77e5\u8bc6\uff0c\u4f46\u51bb\u7ed3\u67d0\u4e9b\u5c42\u53ef\u4ee5\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5b58\u50a8\u4e86\u5927\u91cf\u77e5\u8bc6\uff0c\u8fd9\u4e9b\u77e5\u8bc6\u901a\u5e38\u9700\u8981\u66f4\u65b0\uff0c\u4ee5\u7ea0\u6b63\u4e8b\u5b9e\u9519\u8bef\u3001\u6574\u5408\u65b0\u83b7\u5f97\u7684\u4fe1\u606f\u6216\u8c03\u6574\u6a21\u578b\u884c\u4e3a\u3002\u6a21\u578b\u7f16\u8f91\u65b9\u6cd5\u5df2\u7ecf\u6210\u4e3a\u8fd9\u79cd\u66f4\u65b0\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u6bd4\u6301\u7eed\u8bad\u7ec3\u4f4e\u5f97\u591a\u7684\u8ba1\u7b97\u6210\u672c\u63d0\u4f9b\u672c\u5730\u5316\u548c\u7cbe\u786e\u7684\u77e5\u8bc6\u4fee\u6539\u3002\u7136\u800c\uff0c\u5fae\u8c03\u5bf9\u5148\u524d\u7f16\u8f91\u7684\u77e5\u8bc6\u7684\u5f71\u54cd\u4ecd\u7136\u77e5\u4e4b\u751a\u5c11\u3002", "method": "\u7cfb\u7edf\u5730\u7814\u7a76\u4e0d\u540c\u7684\u5fae\u8c03\u76ee\u6807\u5982\u4f55\u4e0e\u5404\u79cd\u6a21\u578b\u7f16\u8f91\u6280\u672f\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u7f16\u8f91\u540e\u7684\u77e5\u8bc6\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u66f4\u5bb9\u6613\u88ab\u9057\u5fd8\uff0c\u51bb\u7ed3\u4e0e\u7f16\u8f91\u5185\u5bb9\u76f8\u5173\u7684\u5c42\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u77e5\u8bc6\u4fdd\u7559\u7387\u3002", "conclusion": "\u7f16\u8f91\u540e\u7684\u77e5\u8bc6\u6bd4\u9884\u8bad\u7ec3\u83b7\u5f97\u7684\u5185\u5728\u77e5\u8bc6\u66f4\u5bb9\u6613\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u88ab\u9057\u5fd8\u3002\u51bb\u7ed3\u4e0e\u7f16\u8f91\u5185\u5bb9\u76f8\u5173\u7684\u5c42\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u77e5\u8bc6\u4fdd\u7559\u7387\u3002"}}
{"id": "2507.14475", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.14475", "abs": "https://arxiv.org/abs/2507.14475", "authors": ["Runhao Zhao", "Weixin Zeng", "Wentao Zhang", "Xiang Zhao", "Jiuyang Tang", "Lei Chen"], "title": "Towards Temporal Knowledge Graph Alignment in the Wild", "comment": "18 pages, 6 figures", "summary": "Temporal Knowledge Graph Alignment (TKGA) seeks to identify equivalent\nentities across heterogeneous temporal knowledge graphs (TKGs) for fusion to\nimprove their completeness. Although some approaches have been proposed to\ntackle this task, most assume unified temporal element standards and simplified\ntemporal structures across different TKGs. They cannot deal with TKGA in the\nwild (TKGA-Wild), where multi-scale temporal element entanglement and\ncross-source temporal structural imbalances are common. To bridge this gap, we\nstudy the task of TKGA-Wild and propose HyDRA, a new and effective solution.\nHyDRA is the first to reformulate the task via multi-scale hypergraph\nretrieval-augmented generation to address the challenges of TKGA-Wild.In\naddition, we design a new scale-weave synergy mechanism for HyDRA, which\nincorporates intra-scale interactions and cross-scale conflict detection. This\nmechanism is designed to alleviate the fragmentation caused by multi-source\ntemporal incompleteness and resolves inconsistencies arising from complex and\nuneven temporal event density distributions, thereby enhancing the model\ncapacity to handle the intricacies of real-world temporal alignment. Finally,\nthere is no standard benchmark that captures these challenges of TKGA-Wild and\neffectively evaluates existing methods. To this end, we formally propose to\nbenchmark challenges for TKGA-Wild and validate the effectiveness of the method\nby establishing two new datasets(BETA and WildBETA). Extensive experiments on\nthe new datasets and six representative benchmarks show that BETA and WildBETA\nbetter reflect real-world challenges. Meanwhile, HyDRA proposes a new paradigm\nfor TKGA-Wild, consistently outperforming 24 competitive baselines, while\nmaintaining strong efficiency and scalability.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u65f6\u95f4\u77e5\u8bc6\u56fe\u5bf9\u9f50\uff08TKGA-Wild\uff09\u4efb\u52a1\uff0c\u63d0\u51fa\u4e86HyDRA\u65b9\u6cd5\u548c\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u771f\u5b9e\u573a\u666f\u4e2d\u7684TKGA\uff08TKGA-Wild\uff09\uff0c\u5176\u4e2d\u591a\u5c3a\u5ea6\u65f6\u95f4\u5143\u7d20\u7ea0\u7f20\u548c\u8de8\u6e90\u65f6\u95f4\u7ed3\u6784\u4e0d\u5e73\u8861\u5f88\u5e38\u89c1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u5c3a\u5ea6\u8d85\u56fe\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5HyDRA\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u5c3a\u5ea6\u7f16\u7ec7\u534f\u540c\u673a\u5236\uff0c\u7ed3\u5408\u4e86\u5c3a\u5ea6\u5185\u4ea4\u4e92\u548c\u8de8\u5c3a\u5ea6\u51b2\u7a81\u68c0\u6d4b\u3002", "result": "\u5efa\u7acb\u4e86\u4e24\u4e2a\u65b0\u7684\u6570\u636e\u96c6\uff08BETA\u548cWildBETA\uff09\uff0c\u5e76\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86HyDRA\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u4f18\u4e8e24\u4e2a\u7ade\u4e89\u57fa\u7ebf\u3002", "conclusion": "HyDRA\u5728TKGA-Wild\u4efb\u52a1\u4e0a\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5e76\u5728\u65b0\u6570\u636e\u96c6\u548c\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2507.14303", "categories": ["cs.CV", "I.4.8"], "pdf": "https://arxiv.org/pdf/2507.14303", "abs": "https://arxiv.org/abs/2507.14303", "authors": ["Ehsan Rassekh"], "title": "Semantic Segmentation based Scene Understanding in Autonomous Vehicles", "comment": "74 pages, 35 figures, Master's Thesis, Institute for Advanced Studies\n  in Basic Sciences (IASBS), Zanjan, Iran, 2023", "summary": "In recent years, the concept of artificial intelligence (AI) has become a\nprominent keyword because it is promising in solving complex tasks. The need\nfor human expertise in specific areas may no longer be needed because machines\nhave achieved successful results using artificial intelligence and can make the\nright decisions in critical situations. This process is possible with the help\nof deep learning (DL), one of the most popular artificial intelligence\ntechnologies. One of the areas in which the use of DL is used is in the\ndevelopment of self-driving cars, which is very effective and important. In\nthis work, we propose several efficient models to investigate scene\nunderstanding through semantic segmentation. We use the BDD100k dataset to\ninvestigate these models. Another contribution of this work is the usage of\nseveral Backbones as encoders for models. The obtained results show that\nchoosing the appropriate backbone has a great effect on the performance of the\nmodel for semantic segmentation. Better performance in semantic segmentation\nallows us to understand better the scene and the environment around the agent.\nIn the end, we analyze and evaluate the proposed models in terms of accuracy,\nmean IoU, and loss function, and the results show that these metrics are\nimproved.", "AI": {"tldr": "This paper proposes efficient models with different backbones for semantic segmentation using the BDD100k dataset, showing that the choice of backbone greatly affects performance, ultimately improving scene understanding for self-driving cars.", "motivation": "Using deep learning in self-driving cars for scene understanding through semantic segmentation is effective and important.", "method": "Several efficient models and different backbones are used for semantic segmentation. The BDD100k dataset is used for investigation.", "result": "Choosing the appropriate backbone has a great effect on the performance of the model for semantic segmentation.", "conclusion": "The proposed models improve accuracy, mean IoU, and loss function in semantic segmentation."}}
{"id": "2507.14171", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14171", "abs": "https://arxiv.org/abs/2507.14171", "authors": ["Jaeheun Jung", "Jaehyuk Lee", "Yeajin Lee", "Donghun Lee"], "title": "IPPRO: Importance-based Pruning with PRojective Offset for Magnitude-indifferent Structural Pruning", "comment": null, "summary": "With the growth of demand on neural network compression methods, the\nstructured pruning methods including importance-based approach are actively\nstudied. The magnitude importance and many correlated modern importance\ncriteria often limit the capacity of pruning decision, since the filters with\nlarger magnitudes are not likely to be pruned if the smaller one didn't, even\nif it is redundant. In this paper, we propose a novel pruning strategy to\nchallenge this dominating effect of magnitude and provide fair chance to each\nfilter to be pruned, by placing it on projective space. After that, we observe\nthe gradient descent movement whether the filters move toward the origin or\nnot, to measure how the filter is likely to be pruned. This measurement is used\nto construct PROscore, a novel importance score for IPPRO, a novel\nimportance-based structured pruning with magnitude-indifference. Our evaluation\nresults shows that the proposed importance criteria using the projective space\nachieves near-lossless pruning by reducing the performance drop in pruning,\nwith promising performance after the finetuning. Our work debunks the\n``size-matters'' myth in pruning and expands the frontier of importance-based\npruning both theoretically and empirically.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u526a\u679d\u7b56\u7565\uff0c\u901a\u8fc7\u5c06\u6bcf\u4e2a\u6ee4\u6ce2\u5668\u653e\u7f6e\u5728\u6295\u5f71\u7a7a\u95f4\u4e0a\uff0c\u6311\u6218\u5e45\u5ea6\u7684\u4e3b\u5bfc\u6548\u5e94\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u6ee4\u6ce2\u5668\u63d0\u4f9b\u516c\u5e73\u7684\u526a\u679d\u673a\u4f1a\u3002", "motivation": "\u57fa\u4e8e\u91cd\u8981\u6027\u7684\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5\uff08\u5305\u62ec\u57fa\u4e8e\u91cd\u8981\u6027\u7684\u65b9\u6cd5\uff09\u5f97\u5230\u4e86\u79ef\u6781\u7684\u7814\u7a76\u3002\u5e45\u5ea6\u91cd\u8981\u6027\u548c\u8bb8\u591a\u76f8\u5173\u7684\u73b0\u4ee3\u91cd\u8981\u6027\u6807\u51c6\u901a\u5e38\u4f1a\u9650\u5236\u526a\u679d\u51b3\u7b56\u7684\u80fd\u529b\uff0c\u56e0\u4e3a\u5982\u679c\u8f83\u5c0f\u7684\u6ee4\u6ce2\u5668\u6ca1\u6709\u88ab\u526a\u679d\uff0c\u5219\u8f83\u5927\u7684\u6ee4\u6ce2\u5668\u4e0d\u592a\u53ef\u80fd\u88ab\u526a\u679d\uff0c\u5373\u4f7f\u5b83\u662f\u5197\u4f59\u7684\u3002", "method": "\u901a\u8fc7\u5c06\u6bcf\u4e2a\u6ee4\u6ce2\u5668\u653e\u7f6e\u5728\u6295\u5f71\u7a7a\u95f4\u4e0a\uff0c\u63d0\u4f9b\u516c\u5e73\u7684\u526a\u679d\u673a\u4f1a\uff0c\u89c2\u5bdf\u68af\u5ea6\u4e0b\u964d\u8fd0\u52a8\uff0c\u4ee5\u6d4b\u91cf\u6ee4\u6ce2\u5668\u88ab\u526a\u679d\u7684\u53ef\u80fd\u6027\u3002\u4f7f\u7528\u8be5\u6d4b\u91cf\u7ed3\u679c\u6765\u6784\u5efa PROscore\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u91cd\u8981\u6027\u8bc4\u5206\uff0c\u7528\u4e8e IPPRO\uff0c\u8fd9\u662f\u4e00\u79cd\u5177\u6709\u5e45\u5ea6\u65e0\u5173\u6027\u7684\u57fa\u4e8e\u91cd\u8981\u6027\u7684\u7ed3\u6784\u5316\u526a\u679d\u3002", "result": "\u6240\u63d0\u51fa\u7684\u4f7f\u7528\u6295\u5f71\u7a7a\u95f4\u7684\u91cd\u8981\u6027\u51c6\u5219\u901a\u8fc7\u51cf\u5c11\u526a\u679d\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u6765\u5b9e\u73b0\u63a5\u8fd1\u65e0\u635f\u7684\u526a\u679d\uff0c\u5e76\u5728\u5fae\u8c03\u540e\u5177\u6709\u826f\u597d\u7684\u6027\u80fd\u3002", "conclusion": "\u4f7f\u7528\u6295\u5f71\u7a7a\u95f4\u7684\u91cd\u8981\u6027\u51c6\u5219\u901a\u8fc7\u51cf\u5c11\u526a\u679d\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u6765\u5b9e\u73b0\u63a5\u8fd1\u65e0\u635f\u7684\u526a\u679d\uff0c\u5e76\u5728\u5fae\u8c03\u540e\u5177\u6709\u826f\u597d\u7684\u6027\u80fd\u3002\u8fd9\u9879\u5de5\u4f5c\u63ed\u7a7f\u4e86\u526a\u679d\u4e2d\u201c\u5927\u5c0f\u51b3\u5b9a\u4e00\u5207\u201d\u7684 \u092e\u093f\u0925\uff0c\u5e76\u5728\u7406\u8bba\u548c\u7ecf\u9a8c\u4e0a\u6269\u5c55\u4e86\u57fa\u4e8e\u91cd\u8981\u6027\u7684\u526a\u679d\u7684\u524d\u6cbf\u3002"}}
{"id": "2507.14267", "categories": ["cs.AI", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.14267", "abs": "https://arxiv.org/abs/2507.14267", "authors": ["Ziqi Wang", "Hongshuo Huang", "Hancheng Zhao", "Changwen Xu", "Shang Zhu", "Jan Janssen", "Venkatasubramanian Viswanathan"], "title": "DREAMS: Density Functional Theory Based Research Engine for Agentic Materials Simulation", "comment": "34 pages, 28 pages of Supporting Information", "summary": "Materials discovery relies on high-throughput, high-fidelity simulation\ntechniques such as Density Functional Theory (DFT), which require years of\ntraining, extensive parameter fine-tuning and systematic error handling. To\naddress these challenges, we introduce the DFT-based Research Engine for\nAgentic Materials Screening (DREAMS), a hierarchical, multi-agent framework for\nDFT simulation that combines a central Large Language Model (LLM) planner agent\nwith domain-specific LLM agents for atomistic structure generation, systematic\nDFT convergence testing, High-Performance Computing (HPC) scheduling, and error\nhandling. In addition, a shared canvas helps the LLM agents to structure their\ndiscussions, preserve context and prevent hallucination. We validate DREAMS\ncapabilities on the Sol27LC lattice-constant benchmark, achieving average\nerrors below 1\\% compared to the results of human DFT experts. Furthermore, we\napply DREAMS to the long-standing CO/Pt(111) adsorption puzzle, demonstrating\nits long-term and complex problem-solving capabilities. The framework again\nreproduces expert-level literature adsorption-energy differences. Finally,\nDREAMS is employed to quantify functional-driven uncertainties with Bayesian\nensemble sampling, confirming the Face Centered Cubic (FCC)-site preference at\nthe Generalized Gradient Approximation (GGA) DFT level. In conclusion, DREAMS\napproaches L3-level automation - autonomous exploration of a defined design\nspace - and significantly reduces the reliance on human expertise and\nintervention, offering a scalable path toward democratized, high-throughput,\nhigh-fidelity computational materials discovery.", "AI": {"tldr": "DREAMS, a multi-agent LLM framework, automates DFT simulations for materials discovery, achieving expert-level accuracy and reducing reliance on human expertise.", "motivation": "Materials discovery relies on high-throughput, high-fidelity simulation techniques such as Density Functional Theory (DFT), which require years of training, extensive parameter fine-tuning and systematic error handling.", "method": "a hierarchical, multi-agent framework for DFT simulation that combines a central Large Language Model (LLM) planner agent with domain-specific LLM agents", "result": "achieving average errors below 1% compared to the results of human DFT experts on the Sol27LC lattice-constant benchmark, reproduces expert-level literature adsorption-energy differences on the CO/Pt(111) adsorption puzzle, and confirms the FCC-site preference at the GGA DFT level using Bayesian ensemble sampling", "conclusion": "DREAMS approaches L3-level automation and reduces reliance on human expertise, offering a scalable path toward democratized computational materials discovery."}}
{"id": "2507.14619", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14619", "abs": "https://arxiv.org/abs/2507.14619", "authors": ["Van-Hoang Le", "Duc-Vu Nguyen", "Kiet Van Nguyen", "Ngan Luu-Thuy Nguyen"], "title": "Optimizing Legal Document Retrieval in Vietnamese with Semi-Hard Negative Mining", "comment": "Accepted at ICCCI 2025", "summary": "Large Language Models (LLMs) face significant challenges in specialized\ndomains like law, where precision and domain-specific knowledge are critical.\nThis paper presents a streamlined two-stage framework consisting of Retrieval\nand Re-ranking to enhance legal document retrieval efficiency and accuracy. Our\napproach employs a fine-tuned Bi-Encoder for rapid candidate retrieval,\nfollowed by a Cross-Encoder for precise re-ranking, both optimized through\nstrategic negative example mining. Key innovations include the introduction of\nthe Exist@m metric to evaluate retrieval effectiveness and the use of semi-hard\nnegatives to mitigate training bias, which significantly improved re-ranking\nperformance. Evaluated on the SoICT Hackathon 2024 for Legal Document\nRetrieval, our team, 4Huiter, achieved a top-three position. While\ntop-performing teams employed ensemble models and iterative self-training on\nlarge bge-m3 architectures, our lightweight, single-pass approach offered a\ncompetitive alternative with far fewer parameters. The framework demonstrates\nthat optimized data processing, tailored loss functions, and balanced negative\nsampling are pivotal for building robust retrieval-augmented systems in legal\ncontexts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6cd5\u5f8b\u6587\u4ef6\u68c0\u7d22\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u5fae\u8c03\u7684Bi-Encoder\u548cCross-Encoder\uff0c\u5e76\u5728SoICT Hackathon 2024\u4e0a\u53d6\u5f97\u4e86\u524d\u4e09\u540d\u7684\u6210\u7ee9\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6cd5\u5f8b\u7b49\u4e13\u4e1a\u9886\u57df\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u5728\u8fd9\u4e9b\u9886\u57df\uff0c\u7cbe\u786e\u6027\u548c\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5316\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u5305\u62ec\u68c0\u7d22\u548c\u91cd\u65b0\u6392\u5e8f\uff0c\u4ee5\u63d0\u9ad8\u6cd5\u5f8b\u6587\u4ef6\u68c0\u7d22\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u5305\u62ec\u7528\u4e8e\u5feb\u901f\u5019\u9009\u68c0\u7d22\u7684\u5fae\u8c03Bi-Encoder\u548c\u7528\u4e8e\u7cbe\u786e\u91cd\u65b0\u6392\u5e8f\u7684Cross-Encoder\uff0c\u901a\u8fc7\u7b56\u7565\u6027\u8d1f\u4f8b\u6316\u6398\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728SoICT Hackathon 2024\u6cd5\u5f8b\u6587\u4ef6\u68c0\u7d22\u7ade\u8d5b\u4e2d\uff0c\u6211\u4eec\u7684\u56e2\u961f4Huiter\u53d6\u5f97\u4e86\u524d\u4e09\u540d\u7684\u6210\u7ee9\u3002\u867d\u7136\u8868\u73b0\u6700\u597d\u7684\u56e2\u961f\u91c7\u7528\u4e86\u5927\u578bbge-m3\u67b6\u6784\u4e0a\u7684\u96c6\u6210\u6a21\u578b\u548c\u8fed\u4ee3\u81ea\u8bad\u7ec3\uff0c\u4f46\u6211\u4eec\u8f7b\u91cf\u7ea7\u7684\u5355\u6b21\u901a\u8fc7\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u5177\u6709\u7ade\u4e89\u529b\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u53c2\u6570\u8981\u5c11\u5f97\u591a\u3002", "conclusion": "\u4f18\u5316\u7684\u6570\u636e\u5904\u7406\u3001\u5b9a\u5236\u7684\u635f\u5931\u51fd\u6570\u548c\u5e73\u8861\u7684\u8d1f\u91c7\u6837\u5bf9\u4e8e\u5728\u6cd5\u5f8b\u80cc\u666f\u4e0b\u6784\u5efa\u5f3a\u5927\u7684\u68c0\u7d22\u589e\u5f3a\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.14200", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14200", "abs": "https://arxiv.org/abs/2507.14200", "authors": ["Shengji Tang", "Jianjian Cao", "Weihao Lin", "Jiale Hong", "Bo Zhang", "Shuyue Hu", "Lei Bai", "Tao Chen", "Wanli Ouyang", "Peng Ye"], "title": "Open-Source LLMs Collaboration Beats Closed-Source LLMs: A Scalable Multi-Agent System", "comment": null, "summary": "This paper aims to demonstrate the potential and strengths of open-source\ncollectives. It leads to a promising question: Can we harness multiple\nopen-source LLMs to match or even beat the closed-source LLMs? To answer this,\nwe propose SMACS, a scalable multi-agent collaboration system (MACS) framework\nwith high performance. Specifically, for continuous integration of new LLMs and\ngeneralization to diverse questions, we first propose a Retrieval-based Prior\nSelection (RPS), which assigns a proxy performance score to each LLM to select\nthe Top-k LLMs at the instance level for any given question. Then, we propose\nan Exploration-Exploitation-Driven Posterior Enhancement (EPE), encouraging the\ngeneration of diverse responses through prior dropping and selecting the\nhigh-quality response via a hybrid posterior score. Experiments on eight\nmainstream benchmarks validate the effectiveness of our SMACS: by integrating\nfifteen open-source LLMs, SMACS outperforms leading closed-source LLMs in 2025,\ne.g., Claude-3.7-Sonnet (+12.73%), GPT-4.1(+5.36%) and GPT-o3-mini(+5.28%)\nacross multiple tasks. Remarkably, it even exceeds the average of best results\nof different datasets from both open-source LLMs (+2.86%) and closed-source\nLLMs (+2.04%), pushing the upper bound of intelligence. Code will be released\nat https://github.com/magent4aci/SMACS.", "AI": {"tldr": "SMACS, a multi-agent collaboration system, integrates fifteen open-source LLMs and outperforms leading closed-source LLMs on multiple benchmarks.", "motivation": "Can we harness multiple open-source LLMs to match or even beat the closed-source LLMs?", "method": "Scalable Multi-Agent Collaboration System (SMACS) framework with Retrieval-based Prior Selection (RPS) and Exploration-Exploitation-Driven Posterior Enhancement (EPE).", "result": "SMACS outperforms leading closed-source LLMs, e.g., Claude-3.7-Sonnet (+12.73%), GPT-4.1(+5.36%) and GPT-o3-mini(+5.28%) across multiple tasks. It even exceeds the average of best results from both open-source and closed-source LLMs.", "conclusion": "SMACS outperforms leading closed-source LLMs and even exceeds the average of best results from both open-source and closed-source LLMs."}}
{"id": "2507.14495", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.14495", "abs": "https://arxiv.org/abs/2507.14495", "authors": ["Roman Heinrich", "Oleksandr Havrylov", "Manisha Luthra", "Johannes Wehrstein", "Carsten Binnig"], "title": "Opening The Black-Box: Explaining Learned Cost Models For Databases", "comment": "Accepted to VLDB 2025 Demonstration Track", "summary": "Learned Cost Models (LCMs) have shown superior results over traditional\ndatabase cost models as they can significantly improve the accuracy of cost\npredictions. However, LCMs still fail for some query plans, as prediction\nerrors can be large in the tail. Unfortunately, recent LCMs are based on\ncomplex deep neural models, and thus, there is no easy way to understand where\nthis accuracy drop is rooted, which critically prevents systematic\ntroubleshooting. In this demo paper, we present the very first approach for\nopening the black box by bringing AI explainability approaches to LCMs. As a\ncore contribution, we developed new explanation techniques that extend existing\nmethods that are available for the general explainability of AI models and\nadapt them significantly to be usable for LCMs. In our demo, we provide an\ninteractive tool to showcase how explainability for LCMs works. We believe this\nis a first step for making LCMs debuggable and thus paving the road for new\napproaches for systematically fixing problems in LCMs.", "AI": {"tldr": "This paper presents a new approach to explain and debug Learned Cost Models (LCMs) using AI explainability techniques, with an interactive tool for demonstration.", "motivation": "Existing Learned Cost Models (LCMs) have prediction errors, particularly in the tail, and their complexity makes it difficult to understand the source of these errors.", "method": "The authors developed new explanation techniques adapted from existing AI explainability methods to be usable for LCMs.", "result": "The authors provide an interactive tool demonstrating the explainability of LCMs.", "conclusion": "This paper introduces a novel approach to explain the predictions of Learned Cost Models (LCMs) using AI explainability techniques, aiming to make LCMs debuggable and improve their accuracy."}}
{"id": "2507.14312", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14312", "abs": "https://arxiv.org/abs/2507.14312", "authors": ["Marc Lafon", "Gustavo Adolfo Vargas Hakim", "Cl\u00e9ment Rambour", "Christian Desrosier", "Nicolas Thome"], "title": "CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation", "comment": null, "summary": "Vision-language models (VLMs) like CLIP exhibit strong zero-shot capabilities\nbut often fail to generalize under distribution shifts. Test-time adaptation\n(TTA) allows models to update at inference time without labeled data, typically\nvia entropy minimization. However, this objective is fundamentally misaligned\nwith the contrastive image-text training of VLMs, limiting adaptation\nperformance and introducing failure modes such as pseudo-label drift and class\ncollapse. We propose CLIPTTA, a new gradient-based TTA method for\nvision-language models that leverages a soft contrastive loss aligned with\nCLIP's pre-training objective. We provide a theoretical analysis of CLIPTTA's\ngradients, showing how its batch-aware design mitigates the risk of collapse.\nWe further extend CLIPTTA to the open-set setting, where both in-distribution\n(ID) and out-of-distribution (OOD) samples are encountered, using an Outlier\nContrastive Exposure (OCE) loss to improve OOD detection. Evaluated on 75\ndatasets spanning diverse distribution shifts, CLIPTTA consistently outperforms\nentropy-based objectives and is highly competitive with state-of-the-art TTA\nmethods, outperforming them on a large number of datasets and exhibiting more\nstable performance across diverse shifts.", "AI": {"tldr": "CLIPTTA\u662f\u4e00\u79cd\u65b0\u7684TTA\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u4e0eCLIP\u7684\u9884\u8bad\u7ec3\u76ee\u6807\u5bf9\u9f50\u7684\u8f6f\u5bf9\u6bd4\u635f\u5931\uff0c\u5e76\u5728\u5404\u79cd\u5206\u5e03\u504f\u79fb\u7684\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "motivation": "\u50cfCLIP\u8fd9\u6837\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u80fd\u529b\uff0c\u4f46\u901a\u5e38\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u65e0\u6cd5\u6cdb\u5316\u3002\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\uff08TTA\uff09\u5141\u8bb8\u6a21\u578b\u5728\u63a8\u7406\u65f6\u66f4\u65b0\uff0c\u800c\u65e0\u9700\u6807\u8bb0\u6570\u636e\uff0c\u901a\u5e38\u901a\u8fc7\u71b5\u6700\u5c0f\u5316\u3002\u7136\u800c\uff0c\u8fd9\u4e2a\u76ee\u6807\u4ece\u6839\u672c\u4e0a\u4e0eVLM\u7684\u5bf9\u6bd4\u56fe\u50cf-\u6587\u672c\u8bad\u7ec3\u4e0d\u4e00\u81f4\uff0c\u9650\u5236\u4e86\u81ea\u9002\u5e94\u6027\u80fd\uff0c\u5e76\u5f15\u5165\u4e86\u8bf8\u5982\u4f2a\u6807\u7b7e\u6f02\u79fb\u548c\u7c7b\u5d29\u6e83\u7b49\u5931\u8d25\u6a21\u5f0f\u3002", "method": "CLIPTTA\uff0c\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u68af\u5ea6\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578bTTA\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u4e0eCLIP\u7684\u9884\u8bad\u7ec3\u76ee\u6807\u5bf9\u9f50\u7684\u8f6f\u5bf9\u6bd4\u635f\u5931\u3002\u4f7f\u7528\u5f02\u5e38\u5bf9\u6bd4\u66b4\u9732\uff08OCE\uff09\u635f\u5931\u5c06CLIPTTA\u6269\u5c55\u5230\u5f00\u653e\u96c6\u8bbe\u7f6e\uff0c\u4ee5\u63d0\u9ad8OOD\u68c0\u6d4b\u3002", "result": "CLIPTTA\u59cb\u7ec8\u4f18\u4e8e\u57fa\u4e8e\u71b5\u7684\u76ee\u6807\uff0c\u5e76\u4e14\u4e0e\u6700\u5148\u8fdb\u7684TTA\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u5f88\u5f3a\u7684\u7ade\u4e89\u529b\uff0c\u5728\u5927\u91cf\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u5b83\u4eec\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u7684\u504f\u79fb\u4e2d\u8868\u73b0\u51fa\u66f4\u7a33\u5b9a\u7684\u6027\u80fd\u3002", "conclusion": "CLIPTTA\u5728\u5404\u79cd\u5206\u5e03\u504f\u79fb\u7684\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u71b5\u7684\u76ee\u6807\uff0c\u5e76\u4e14\u4e0e\u6700\u5148\u8fdb\u7684TTA\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\uff0c\u5728\u5927\u91cf\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u5b83\u4eec\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u7684\u504f\u79fb\u4e2d\u8868\u73b0\u51fa\u66f4\u7a33\u5b9a\u7684\u6027\u80fd\u3002"}}
{"id": "2507.14172", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.14172", "abs": "https://arxiv.org/abs/2507.14172", "authors": ["Julien Pourcel", "C\u00e9dric Colas", "Pierre-Yves Oudeyer"], "title": "Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI", "comment": null, "summary": "Many program synthesis tasks prove too challenging for even state-of-the-art\nlanguage models to solve in single attempts. Search-based evolutionary methods\noffer a promising alternative by exploring solution spaces iteratively, but\ntheir effectiveness remain limited by the fixed capabilities of the underlying\ngenerative model.\n  We propose SOAR, a method that learns program synthesis by integrating\nlanguage models into a self-improving evolutionary loop.\n  SOAR alternates between (1) an evolutionary search that uses an LLM to sample\nand refine candidate solutions, and (2) a hindsight learning phase that\nconverts search attempts into valid problem-solution pairs used to fine-tune\nthe LLM's sampling and refinement capabilities\\, -- \\,enabling increasingly\neffective search in subsequent iterations.\n  On the challenging ARC-AGI benchmark, SOAR achieves significant performance\ngains across model scales and iterations, leveraging positive transfer between\nthe sampling and refinement finetuning tasks. These improvements carry over to\ntest-time adaptation, enabling SOAR to solve 52\\% of the public test set. Our\ncode is open-sourced at: https://github.com/flowersteam/SOAR", "AI": {"tldr": "SOAR learns program synthesis by integrating language models into a self-improving evolutionary loop, achieving significant performance gains on the ARC-AGI benchmark.", "motivation": "Many program synthesis tasks prove too challenging for even state-of-the-art language models to solve in single attempts. Search-based evolutionary methods offer a promising alternative by exploring solution spaces iteratively, but their effectiveness remain limited by the fixed capabilities of the underlying generative model.", "method": "SOAR alternates between (1) an evolutionary search that uses an LLM to sample and refine candidate solutions, and (2) a hindsight learning phase that converts search attempts into valid problem-solution pairs used to fine-tune the LLM's sampling and refinement capabilities.", "result": "SOAR achieves significant performance gains across model scales and iterations, leveraging positive transfer between the sampling and refinement finetuning tasks. These improvements carry over to test-time adaptation, enabling SOAR to solve 52% of the public test set.", "conclusion": "SOAR achieves significant performance gains across model scales and iterations, leveraging positive transfer between the sampling and refinement finetuning tasks. These improvements carry over to test-time adaptation, enabling SOAR to solve 52% of the public test set."}}
{"id": "2507.14293", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14293", "abs": "https://arxiv.org/abs/2507.14293", "authors": ["Boyuan Zheng", "Zeyi Liao", "Scott Salisbury", "Zeyuan Liu", "Michael Lin", "Qinyuan Zheng", "Zifan Wang", "Xiang Deng", "Dawn Song", "Huan Sun", "Yu Su"], "title": "WebGuard: Building a Generalizable Guardrail for Web Agents", "comment": "We publicly release WebGuard, along with its annotation tools and\n  fine-tuned models, to facilitate open-source research on monitoring and\n  safeguarding web agents. All resources are available at\n  https://github.com/OSU-NLP-Group/WebGuard", "summary": "The rapid development of autonomous web agents powered by Large Language\nModels (LLMs), while greatly elevating efficiency, exposes the frontier risk of\ntaking unintended or harmful actions. This situation underscores an urgent need\nfor effective safety measures, akin to access controls for human users. To\naddress this critical challenge, we introduce WebGuard, the first comprehensive\ndataset designed to support the assessment of web agent action risks and\nfacilitate the development of guardrails for real-world online environments. In\ndoing so, WebGuard specifically focuses on predicting the outcome of\nstate-changing actions and contains 4,939 human-annotated actions from 193\nwebsites across 22 diverse domains, including often-overlooked long-tail\nwebsites. These actions are categorized using a novel three-tier risk schema:\nSAFE, LOW, and HIGH. The dataset includes designated training and test splits\nto support evaluation under diverse generalization settings. Our initial\nevaluations reveal a concerning deficiency: even frontier LLMs achieve less\nthan 60% accuracy in predicting action outcomes and less than 60% recall in\nlagging HIGH-risk actions, highlighting the risks of deploying\ncurrent-generation agents without dedicated safeguards. We therefore\ninvestigate fine-tuning specialized guardrail models using WebGuard. We conduct\ncomprehensive evaluations across multiple generalization settings and find that\na fine-tuned Qwen2.5VL-7B model yields a substantial improvement in\nperformance, boosting accuracy from 37% to 80% and HIGH-risk action recall from\n20% to 76%. Despite these improvements, the performance still falls short of\nthe reliability required for high-stakes deployment, where guardrails must\napproach near-perfect accuracy and recall.", "AI": {"tldr": "WebGuard \u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30 Web \u4ee3\u7406\u64cd\u4f5c\u98ce\u9669\u7684\u6570\u636e\u96c6\u3002", "motivation": "\u81ea\u4e3b Web \u4ee3\u7406\u7684\u5feb\u901f\u53d1\u5c55\u66b4\u9732\u4e86\u91c7\u53d6\u610f\u5916\u6216\u6709\u5bb3\u884c\u52a8\u7684\u524d\u6cbf\u98ce\u9669\uff0c\u56e0\u6b64\u8feb\u5207\u9700\u8981\u6709\u6548\u7684\u5b89\u5168\u63aa\u65bd\u3002", "method": "\u5f15\u5165 WebGuard\uff0c\u8fd9\u662f\u4e00\u4e2a\u5168\u9762\u7684\u6570\u636e\u96c6\uff0c\u65e8\u5728\u652f\u6301\u8bc4\u4f30 Web \u4ee3\u7406\u64cd\u4f5c\u98ce\u9669\u5e76\u4fc3\u8fdb\u5f00\u53d1\u771f\u5b9e\u5728\u7ebf\u73af\u5883\u7684\u62a4\u680f\u3002", "result": "\u5373\u4f7f\u662f\u524d\u6cbf\u7684\u6cd5\u5b66\u7855\u58eb\u5728\u9884\u6d4b\u884c\u52a8\u7ed3\u679c\u65b9\u9762\u7684\u51c6\u786e\u7387\u4e5f\u4f4e\u4e8e 60%\uff0c\u5728\u9ad8\u98ce\u9669\u884c\u52a8\u4e2d\u7684\u53ec\u56de\u7387\u4e5f\u4f4e\u4e8e 60%\u3002\u7ecf\u8fc7\u5fae\u8c03\u7684 Qwen2.5VL-7B \u6a21\u578b\u5728\u6027\u80fd\u65b9\u9762\u6709\u4e86\u663e\u7740\u63d0\u9ad8\uff0c\u51c6\u786e\u7387\u4ece 37% \u63d0\u9ad8\u5230 80%\uff0c\u9ad8\u98ce\u9669\u884c\u52a8\u53ec\u56de\u7387\u4ece 20% \u63d0\u9ad8\u5230 76%\u3002", "conclusion": "\u5373\u4f7f\u7ecf\u8fc7\u6539\u8fdb\uff0c\u6027\u80fd\u4ecd\u672a\u8fbe\u5230\u9ad8\u98ce\u9669\u90e8\u7f72\u6240\u9700\u7684\u53ef\u9760\u6027\uff0c\u9700\u8981\u63a5\u8fd1\u5b8c\u7f8e\u7684\u51c6\u786e\u7387\u548c\u53ec\u56de\u7387\u3002"}}
{"id": "2507.14902", "categories": ["cs.IR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14902", "abs": "https://arxiv.org/abs/2507.14902", "authors": ["Xiaojie Li", "Chu Li", "Shi-Zhe Chen", "Xi Chen"], "title": "U-MARVEL: Unveiling Key Factors for Universal Multimodal Retrieval via Embedding Learning with MLLMs", "comment": "Technical Report (in progress)", "summary": "Universal multimodal retrieval (UMR), which aims to address complex retrieval\ntasks where both queries and candidates span diverse modalities, has been\nsignificantly advanced by the emergence of MLLMs. While state-of-the-art\nMLLM-based methods in the literature predominantly adopt contrastive learning\nprinciples, they often differ in their specific training recipes. Despite their\nsuccess, the mechanisms underlying their retrieval capabilities remain largely\nunexplored, potentially resulting in suboptimal performance and limited\ngeneralization ability. To address these issues, we present a comprehensive\nstudy aimed at uncovering the key factors that drive effective embedding\nlearning for UMR using MLLMs. We begin by implementing a general MLLM-based\nembedding learning pipeline, and systematically analyze the primary\ncontributors to high-performing universal retrieval systems. Based on this, we\nexplore various aspects of the details in embedding generation and training\nstrategies, including progressive transition, hard negative mining and\nre-ranker distillation. Notably, our findings reveal that often-overlooked\nfactors can have a substantial impact on model performance. Building on these\ndiscoveries, we introduce a unified framework termed U-MARVEL\n(\\textbf{U}niversal \\textbf{M}ultimod\\textbf{A}l \\textbf{R}etrie\\textbf{V}al\nvia \\textbf{E}mbedding \\textbf{L}earning), which outperforms state-of-the-art\ncompetitors on the M-BEIR benchmark by a large margin in supervised settings,\nand also exihibits strong zero-shot performance on several tasks such as\ncomposed image retrieval and text-to-video retrieval. These results underscore\nthe generalization potential of our framework across various embedding-based\nretrieval tasks. Code is available at https://github.com/chaxjli/U-MARVEL", "AI": {"tldr": "This paper introduces U-MARVEL, a new framework for universal multimodal retrieval that outperforms existing methods by analyzing key factors in MLLM embedding learning and incorporating techniques like progressive transition and hard negative mining.", "motivation": "The paper addresses the issue that the mechanisms underlying the retrieval capabilities of state-of-the-art MLLM-based methods for universal multimodal retrieval (UMR) are largely unexplored, potentially resulting in suboptimal performance and limited generalization ability.", "method": "The paper implements a general MLLM-based embedding learning pipeline and systematically analyzes the primary contributors to high-performing universal retrieval systems. It explores embedding generation and training strategies, including progressive transition, hard negative mining, and re-ranker distillation.", "result": "The paper's findings reveal that often-overlooked factors can have a substantial impact on model performance. The proposed U-MARVEL framework outperforms state-of-the-art competitors on the M-BEIR benchmark by a large margin in supervised settings and exhibits strong zero-shot performance on several tasks.", "conclusion": "The paper introduces a unified framework called U-MARVEL for universal multimodal retrieval (UMR) that outperforms state-of-the-art methods on the M-BEIR benchmark in supervised settings and demonstrates strong zero-shot performance on various tasks. The results highlight the generalization potential of the framework."}}
{"id": "2507.14214", "categories": ["cs.CL", "cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.14214", "abs": "https://arxiv.org/abs/2507.14214", "authors": ["Rui Zhao", "Vladyslav Melnychuk", "Jun Zhao", "Jesse Wright", "Nigel Shadbolt"], "title": "Let's Measure the Elephant in the Room: Facilitating Personalized Automated Analysis of Privacy Policies at Scale", "comment": null, "summary": "In modern times, people have numerous online accounts, but they rarely read\nthe Terms of Service or Privacy Policy of those sites despite claiming\notherwise. This paper introduces PoliAnalyzer, a neuro-symbolic system that\nassists users with personalized privacy policy analysis. PoliAnalyzer uses\nNatural Language Processing (NLP) to extract formal representations of data\nusage practices from policy texts. In favor of deterministic, logical inference\nis applied to compare user preferences with the formal privacy policy\nrepresentation and produce a compliance report. To achieve this, we extend an\nexisting formal Data Terms of Use policy language to model privacy policies as\napp policies and user preferences as data policies. In our evaluation using our\nenriched PolicyIE dataset curated by legal experts, PoliAnalyzer demonstrated\nhigh accuracy in identifying relevant data usage practices, achieving F1-score\nof 90-100% across most tasks. Additionally, we demonstrate how PoliAnalyzer can\nmodel diverse user data-sharing preferences, derived from prior research as 23\nuser profiles, and perform compliance analysis against the top 100 most-visited\nwebsites. This analysis revealed that, on average, 95.2% of a privacy policy's\nsegments do not conflict with the analyzed user preferences, enabling users to\nconcentrate on understanding the 4.8% (636 / 13205) that violates preferences,\nsignificantly reducing cognitive burden. Further, we identified common\npractices in privacy policies that violate user expectations - such as the\nsharing of location data with 3rd parties. This paper demonstrates that\nPoliAnalyzer can support automated personalized privacy policy analysis at\nscale using off-the-shelf NLP tools. This sheds light on a pathway to help\nindividuals regain control over their data and encourage societal discussions\non platform data practices to promote a fairer power dynamic.", "AI": {"tldr": "PoliAnalyzer is a system that helps users analyze privacy policies by extracting data usage practices and comparing them to user preferences, achieving high accuracy and reducing cognitive burden.", "motivation": "People have numerous online accounts, but they rarely read the Terms of Service or Privacy Policy of those sites despite claiming otherwise.", "method": "a neuro-symbolic system that assists users with personalized privacy policy analysis. PoliAnalyzer uses Natural Language Processing (NLP) to extract formal representations of data usage practices from policy texts. In favor of deterministic, logical inference is applied to compare user preferences with the formal privacy policy representation and produce a compliance report. To achieve this, we extend an existing formal Data Terms of Use policy language to model privacy policies as app policies and user preferences as data policies.", "result": "PoliAnalyzer demonstrated high accuracy in identifying relevant data usage practices, achieving F1-score of 90-100% across most tasks.  This analysis revealed that, on average, 95.2% of a privacy policy's segments do not conflict with the analyzed user preferences, enabling users to concentrate on understanding the 4.8% (636 / 13205) that violates preferences, significantly reducing cognitive burden. Further, we identified common practices in privacy policies that violate user expectations - such as the sharing of location data with 3rd parties.", "conclusion": "PoliAnalyzer can support automated personalized privacy policy analysis at scale using off-the-shelf NLP tools. This sheds light on a pathway to help individuals regain control over their data and encourage societal discussions on platform data practices to promote a fairer power dynamic."}}
{"id": "2507.14682", "categories": ["cs.DB", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.14682", "abs": "https://arxiv.org/abs/2507.14682", "authors": ["Massimo Cafaro", "Italo Epicoco", "Marco Pulimeno", "Lunodzo J. Mwinuka", "Lucas Pereira", "Hugo Morais"], "title": "IDSS, a Novel P2P Relational Data Storage Service", "comment": null, "summary": "The rate at which data is generated has been increasing rapidly, raising\nchallenges related to its management. Traditional database management systems\nsuffer from scalability and are usually inefficient when dealing with\nlarge-scale and heterogeneous data. This paper introduces IDSS (InnoCyPES Data\nStorage Service), a novel large-scale data storage tool that leverages\npeer-to-peer networks and embedded relational databases. We present the IDSS\narchitecture and its design, and provide details related to the implementation.\nThe peer-to-peer framework is used to provide support for distributed queries\nleveraging a relational database architecture based on a common schema.\nFurthermore, methods to support complex distributed query processing, enabling\nrobust and efficient management of vast amounts of data are presented.", "AI": {"tldr": "This paper introduces IDSS, a novel large-scale data storage tool that leverages peer-to-peer networks and embedded relational databases to support complex distributed query processing, enabling robust and efficient management of vast amounts of data.", "motivation": "Traditional database management systems suffer from scalability and are usually inefficient when dealing with large-scale and heterogeneous data. The rate at which data is generated has been increasing rapidly, raising challenges related to its management.", "method": "This paper introduces IDSS (InnoCyPES Data Storage Service), a novel large-scale data storage tool that leverages peer-to-peer networks and embedded relational databases. The peer-to-peer framework is used to provide support for distributed queries leveraging a relational database architecture based on a common schema.", "result": "We present the IDSS architecture and its design, and provide details related to the implementation.", "conclusion": "Methods to support complex distributed query processing, enabling robust and efficient management of vast amounts of data are presented."}}
{"id": "2507.14315", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14315", "abs": "https://arxiv.org/abs/2507.14315", "authors": ["Qiyu Xu", "Zhanxuan Hu", "Yu Duan", "Ercheng Pei", "Yonghang Tai"], "title": "A Hidden Stumbling Block in Generalized Category Discovery: Distracted Attention", "comment": null, "summary": "Generalized Category Discovery (GCD) aims to classify unlabeled data from\nboth known and unknown categories by leveraging knowledge from labeled known\ncategories. While existing methods have made notable progress, they often\noverlook a hidden stumbling block in GCD: distracted attention. Specifically,\nwhen processing unlabeled data, models tend to focus not only on key objects in\nthe image but also on task-irrelevant background regions, leading to suboptimal\nfeature extraction. To remove this stumbling block, we propose Attention\nFocusing (AF), an adaptive mechanism designed to sharpen the model's focus by\npruning non-informative tokens. AF consists of two simple yet effective\ncomponents: Token Importance Measurement (TIME) and Token Adaptive Pruning\n(TAP), working in a cascade. TIME quantifies token importance across multiple\nscales, while TAP prunes non-informative tokens by utilizing the multi-scale\nimportance scores provided by TIME. AF is a lightweight, plug-and-play module\nthat integrates seamlessly into existing GCD methods with minimal computational\noverhead. When incorporated into one prominent GCD method, SimGCD, AF achieves\nup to 15.4% performance improvement over the baseline with minimal\ncomputational overhead. The implementation code is provided in\nhttps://github.com/Afleve/AFGCD.", "AI": {"tldr": "GCD models are distracted by irrelevant background. AF sharpens focus by pruning non-informative tokens, improving performance.", "motivation": "Existing GCD methods suffer from distracted attention, focusing on irrelevant background regions.", "method": "Attention Focusing (AF), featuring Token Importance Measurement (TIME) and Token Adaptive Pruning (TAP).", "result": "AF achieves up to 15.4% performance improvement over SimGCD with minimal overhead.", "conclusion": "Attention Focusing (AF) improves GCD performance by 15.4% with minimal overhead."}}
{"id": "2507.14175", "categories": ["cs.LG", "cs.AI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2507.14175", "abs": "https://arxiv.org/abs/2507.14175", "authors": ["Youcef Barkat", "Dylan Hamitouche", "Deven Parekh", "Ivy Guo", "David Benrimoh"], "title": "Latent Space Data Fusion Outperforms Early Fusion in Multimodal Mental Health Digital Phenotyping Data", "comment": null, "summary": "Background: Mental illnesses such as depression and anxiety require improved\nmethods for early detection and personalized intervention. Traditional\npredictive models often rely on unimodal data or early fusion strategies that\nfail to capture the complex, multimodal nature of psychiatric data. Advanced\nintegration techniques, such as intermediate (latent space) fusion, may offer\nbetter accuracy and clinical utility. Methods: Using data from the BRIGHTEN\nclinical trial, we evaluated intermediate (latent space) fusion for predicting\ndaily depressive symptoms (PHQ-2 scores). We compared early fusion implemented\nwith a Random Forest (RF) model and intermediate fusion implemented via a\nCombined Model (CM) using autoencoders and a neural network. The dataset\nincluded behavioral (smartphone-based), demographic, and clinical features.\nExperiments were conducted across multiple temporal splits and data stream\ncombinations. Performance was evaluated using mean squared error (MSE) and\ncoefficient of determination (R2). Results: The CM outperformed both RF and\nLinear Regression (LR) baselines across all setups, achieving lower MSE (0.4985\nvs. 0.5305 with RF) and higher R2 (0.4695 vs. 0.4356). The RF model showed\nsigns of overfitting, with a large gap between training and test performance,\nwhile the CM maintained consistent generalization. Performance was best when\nintegrating all data modalities in the CM (in contradistinction to RF),\nunderscoring the value of latent space fusion for capturing non-linear\ninteractions in complex psychiatric datasets. Conclusion: Latent space fusion\noffers a robust alternative to traditional fusion methods for prediction with\nmultimodal mental health data. Future work should explore model\ninterpretability and individual-level prediction for clinical deployment.", "AI": {"tldr": "Latent space fusion is better than traditional methods for predicting mental health using multimodal data.", "motivation": "Mental illnesses such as depression and anxiety require improved methods for early detection and personalized intervention. Traditional predictive models often rely on unimodal data or early fusion strategies that fail to capture the complex, multimodal nature of psychiatric data. Advanced integration techniques, such as intermediate (latent space) fusion, may offer better accuracy and clinical utility.", "method": "We evaluated intermediate (latent space) fusion for predicting daily depressive symptoms (PHQ-2 scores). We compared early fusion implemented with a Random Forest (RF) model and intermediate fusion implemented via a Combined Model (CM) using autoencoders and a neural network. The dataset included behavioral (smartphone-based), demographic, and clinical features. Experiments were conducted across multiple temporal splits and data stream combinations. Performance was evaluated using mean squared error (MSE) and coefficient of determination (R2).", "result": "The CM outperformed both RF and Linear Regression (LR) baselines across all setups, achieving lower MSE (0.4985 vs. 0.5305 with RF) and higher R2 (0.4695 vs. 0.4356). The RF model showed signs of overfitting, with a large gap between training and test performance, while the CM maintained consistent generalization. Performance was best when integrating all data modalities in the CM (in contradistinction to RF), underscoring the value of latent space fusion for capturing non-linear interactions in complex psychiatric datasets.", "conclusion": "Latent space fusion offers a robust alternative to traditional fusion methods for prediction with multimodal mental health data. Future work should explore model interpretability and individual-level prediction for clinical deployment."}}
{"id": "2507.14306", "categories": ["cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.14306", "abs": "https://arxiv.org/abs/2507.14306", "authors": ["Samarth P", "Vyoman Jain", "Shiva Golugula", "Motamarri Sai Sathvik"], "title": "Manimator: Transforming Research Papers into Visual Explanations", "comment": null, "summary": "Understanding complex scientific and mathematical concepts, particularly\nthose presented in dense research papers, poses a significant challenge for\nlearners. Dynamic visualizations can greatly enhance comprehension, but\ncreating them manually is time-consuming and requires specialized knowledge and\nskills. We introduce manimator, an open-source system that leverages Large\nLanguage Models to transform research papers and natural language prompts into\nexplanatory animations using the Manim engine. Manimator employs a pipeline\nwhere an LLM interprets the input text or research paper PDF to generate a\nstructured scene description outlining key concepts, mathematical formulas, and\nvisual elements and another LLM translates this description into executable\nManim Python code. We discuss its potential as an educational tool for rapidly\ncreating engaging visual explanations for complex STEM topics, democratizing\nthe creation of high-quality educational content.", "AI": {"tldr": "Manimator uses LLMs to automatically create educational animations from research papers, making complex STEM topics easier to understand.", "motivation": "Understanding complex scientific and mathematical concepts from research papers is challenging, and creating dynamic visualizations manually is difficult and time-consuming.", "method": "An open-source system that uses Large Language Models to transform research papers and natural language prompts into explanatory animations using the Manim engine.", "result": "Manimator can transform research papers and natural language prompts into explanatory animations by interpreting the input text or research paper PDF to generate a structured scene description, and then translating this description into executable Manim Python code.", "conclusion": "Manimator is potentially a valuable educational tool for creating visual explanations of complex STEM topics."}}
{"id": "2507.14925", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.14925", "abs": "https://arxiv.org/abs/2507.14925", "authors": ["Mingshi Yan", "Zhiyong Cheng", "Fan Liu", "Yingda Lyu", "Yahong Han"], "title": "User Invariant Preference Learning for Multi-Behavior Recommendation", "comment": null, "summary": "In multi-behavior recommendation scenarios, analyzing users' diverse\nbehaviors, such as click, purchase, and rating, enables a more comprehensive\nunderstanding of their interests, facilitating personalized and accurate\nrecommendations. A fundamental assumption of multi-behavior recommendation\nmethods is the existence of shared user preferences across behaviors,\nrepresenting users' intrinsic interests. Based on this assumption, existing\napproaches aim to integrate information from various behaviors to enrich user\nrepresentations. However, they often overlook the presence of both\ncommonalities and individualities in users' multi-behavior preferences. These\nindividualities reflect distinct aspects of preferences captured by different\nbehaviors, where certain auxiliary behaviors may introduce noise, hindering the\nprediction of the target behavior. To address this issue, we propose a user\ninvariant preference learning for multi-behavior recommendation (UIPL for\nshort), aiming to capture users' intrinsic interests (referred to as invariant\npreferences) from multi-behavior interactions to mitigate the introduction of\nnoise. Specifically, UIPL leverages the paradigm of invariant risk minimization\nto learn invariant preferences. To implement this, we employ a variational\nautoencoder (VAE) to extract users' invariant preferences, replacing the\nstandard reconstruction loss with an invariant risk minimization constraint.\nAdditionally, we construct distinct environments by combining multi-behavior\ndata to enhance robustness in learning these preferences. Finally, the learned\ninvariant preferences are used to provide recommendations for the target\nbehavior. Extensive experiments on four real-world datasets demonstrate that\nUIPL significantly outperforms current state-of-the-art methods.", "AI": {"tldr": "This paper proposes a user invariant preference learning method (UIPL) for multi-behavior recommendation to capture users' intrinsic interests and mitigate noise. It uses invariant risk minimization and a variational autoencoder to learn invariant preferences from multi-behavior data. Experiments show UIPL outperforms state-of-the-art methods.", "motivation": "existing approaches often overlook the presence of both commonalities and individualities in users' multi-behavior preferences. These individualities reflect distinct aspects of preferences captured by different behaviors, where certain auxiliary behaviors may introduce noise, hindering the prediction of the target behavior", "method": "propose a user invariant preference learning for multi-behavior recommendation (UIPL for short), aiming to capture users' intrinsic interests (referred to as invariant preferences) from multi-behavior interactions to mitigate the introduction of noise. Specifically, UIPL leverages the paradigm of invariant risk minimization to learn invariant preferences. To implement this, we employ a variational autoencoder (VAE) to extract users' invariant preferences, replacing the standard reconstruction loss with an invariant risk minimization constraint. Additionally, we construct distinct environments by combining multi-behavior data to enhance robustness in learning these preferences.", "result": "the learned invariant preferences are used to provide recommendations for the target behavior", "conclusion": "Extensive experiments on four real-world datasets demonstrate that UIPL significantly outperforms current state-of-the-art methods."}}
{"id": "2507.14231", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14231", "abs": "https://arxiv.org/abs/2507.14231", "authors": ["Khalid Hasan", "Jamil Saquer"], "title": "Beyond Architectures: Evaluating the Role of Contextual Embeddings in Detecting Bipolar Disorder on Social Media", "comment": "The 37th International Conference on Software Engineering & Knowledge\n  Engineering, SEKE 2025 (camera-ready)", "summary": "Bipolar disorder is a chronic mental illness frequently underdiagnosed due to\nsubtle early symptoms and social stigma. This paper explores the advanced\nnatural language processing (NLP) models for recognizing signs of bipolar\ndisorder based on user-generated social media text. We conduct a comprehensive\nevaluation of transformer-based models (BERT, RoBERTa, ALBERT, ELECTRA,\nDistilBERT) and Long Short Term Memory (LSTM) models based on contextualized\n(BERT) and static (GloVe, Word2Vec) word embeddings. Experiments were performed\non a large, annotated dataset of Reddit posts after confirming their validity\nthrough sentiment variance and judgmental analysis. Our results demonstrate\nthat RoBERTa achieves the highest performance among transformer models with an\nF1 score of ~98% while LSTM models using BERT embeddings yield nearly identical\nresults. In contrast, LSTMs trained on static embeddings fail to capture\nmeaningful patterns, scoring near-zero F1. These findings underscore the\ncritical role of contextual language modeling in detecting bipolar disorder. In\naddition, we report model training times and highlight that DistilBERT offers\nan optimal balance between efficiency and accuracy. In general, our study\noffers actionable insights for model selection in mental health NLP\napplications and validates the potential of contextualized language models to\nsupport early bipolar disorder screening.", "AI": {"tldr": "This paper explores the use of NLP models to recognize signs of bipolar disorder in social media text, finding that RoBERTa and LSTM models with BERT embeddings perform the best.", "motivation": "Bipolar disorder is a chronic mental illness frequently underdiagnosed due to subtle early symptoms and social stigma.", "method": "transformer-based models (BERT, RoBERTa, ALBERT, ELECTRA, DistilBERT) and Long Short Term Memory (LSTM) models based on contextualized (BERT) and static (GloVe, Word2Vec) word embeddings", "result": "RoBERTa achieves the highest performance among transformer models with an F1 score of ~98% while LSTM models using BERT embeddings yield nearly identical results. In contrast, LSTMs trained on static embeddings fail to capture meaningful patterns, scoring near-zero F1.", "conclusion": "contextualized language models can support early bipolar disorder screening"}}
{"id": "2507.14813", "categories": ["cs.DB", "cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.14813", "abs": "https://arxiv.org/abs/2507.14813", "authors": ["Sanjay Sri Vallabh Singapuram", "Ronald Dreslinski", "Nishil Talati"], "title": "Mayura: Exploiting Similarities in Motifs for Temporal Co-Mining", "comment": null, "summary": "Temporal graphs serve as a critical foundation for modeling evolving\ninteractions in domains ranging from financial networks to social media. Mining\ntemporal motifs is essential for applications such as fraud detection,\ncybersecurity, and dynamic network analysis. However, conventional motif mining\napproaches treat each query independently, incurring significant redundant\ncomputations when similar substructures exist across multiple motifs. In this\npaper, we propose Mayura, a novel framework that unifies the mining of multiple\ntemporal motifs by exploiting their inherent structural and temporal\ncommonalities. Central to our approach is the Motif-Group Tree (MG-Tree), a\nhierarchical data structure that organizes related motifs and enables the reuse\nof common search paths, thereby reducing redundant computation. We propose a\nco-mining algorithm that leverages the MG-Tree and develop a flexible runtime\ncapable of exploiting both CPU and GPU architectures for scalable performance.\nEmpirical evaluations on diverse real-world datasets demonstrate that Mayura\nachieves substantial improvements over the state-of-the-art techniques that\nmine each motif individually, with an average speed-up of 2.4x on the CPU and\n1.7x on the GPU, while maintaining the exactness required for high-stakes\napplications.", "AI": {"tldr": "Mayura\u901a\u8fc7\u5171\u4eab\u516c\u5171\u641c\u7d22\u8def\u5f84\u6765\u52a0\u901f\u65f6\u95f4motif\u6316\u6398\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u4f20\u7edfmotif\u6316\u6398\u65b9\u6cd5\u72ec\u7acb\u5904\u7406\u6bcf\u4e2a\u67e5\u8be2\uff0c\u5f53\u591a\u4e2amotif\u5b58\u5728\u76f8\u4f3c\u5b50\u7ed3\u6784\u65f6\uff0c\u4f1a\u4ea7\u751f\u5927\u91cf\u5197\u4f59\u8ba1\u7b97\u3002\u65f6\u95f4\u56fe\u662f\u5efa\u6a21\u91d1\u878d\u7f51\u7edc\u548c\u793e\u4ea4\u5a92\u4f53\u7b49\u9886\u57df\u4e2d\u6f14\u5316\u4ea4\u4e92\u7684\u5173\u952e\u57fa\u7840\u3002\u6316\u6398\u65f6\u95f4motif\u5bf9\u4e8e\u6b3a\u8bc8\u68c0\u6d4b\u3001\u7f51\u7edc\u5b89\u5168\u548c\u52a8\u6001\u7f51\u7edc\u5206\u6790\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86Motif-Group Tree (MG-Tree)\uff0c\u4e00\u79cd\u7ec4\u7ec7\u76f8\u5173motif\u7684\u5206\u5c42\u6570\u636e\u7ed3\u6784\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u5229\u7528MG-Tree\u7684\u534f\u540c\u6316\u6398\u7b97\u6cd5\u3002", "result": "Mayura\u5728\u5404\u79cd\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u76f8\u5bf9\u4e8e\u5355\u72ec\u6316\u6398\u6bcf\u4e2amotif\u7684\u73b0\u6709\u6280\u672f\uff0cMayura\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002", "conclusion": "Mayura\u901a\u8fc7\u5229\u7528\u7ed3\u6784\u548c\u65f6\u95f4\u4e0a\u7684\u5171\u6027\u7edf\u4e00\u4e86\u591a\u4e2a\u65f6\u95f4motif\u7684\u6316\u6398\uff0c\u5728CPU\u4e0a\u5e73\u5747\u52a0\u901f2.4\u500d\uff0c\u5728GPU\u4e0a\u5e73\u5747\u52a0\u901f1.7\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u98ce\u9669\u5e94\u7528\u6240\u9700\u7684\u7cbe\u786e\u6027\u3002"}}
{"id": "2507.14367", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14367", "abs": "https://arxiv.org/abs/2507.14367", "authors": ["Weiming Ren", "Raghav Goyal", "Zhiming Hu", "Tristan Ty Aumentado-Armstrong", "Iqbal Mohomed", "Alex Levinshtein"], "title": "Hallucination Score: Towards Mitigating Hallucinations in Generative Image Super-Resolution", "comment": "12 pages, 17 figures and 7 tables", "summary": "Generative super-resolution (GSR) currently sets the state-of-the-art in\nterms of perceptual image quality, overcoming the \"regression-to-the-mean\" blur\nof prior non-generative models. However, from a human perspective, such models\ndo not fully conform to the optimal balance between quality and fidelity.\nInstead, a different class of artifacts, in which generated details fail to\nperceptually match the low resolution image (LRI) or ground-truth image (GTI),\nis a critical but under studied issue in GSR, limiting its practical\ndeployments. In this work, we focus on measuring, analyzing, and mitigating\nthese artifacts (i.e., \"hallucinations\"). We observe that hallucinations are\nnot well-characterized with existing image metrics or quality models, as they\nare orthogonal to both exact fidelity and no-reference quality. Instead, we\ntake advantage of a multimodal large language model (MLLM) by constructing a\nprompt that assesses hallucinatory visual elements and generates a\n\"Hallucination Score\" (HS). We find that our HS is closely aligned with human\nevaluations, and also provides complementary insights to prior image metrics\nused for super-resolution (SR) models. In addition, we find certain deep\nfeature distances have strong correlations with HS. We therefore propose to\nalign the GSR models by using such features as differentiable reward functions\nto mitigate hallucinations.", "AI": {"tldr": "This paper addresses the problem of hallucinations in generative super-resolution (GSR) models. It introduces a Hallucination Score (HS) based on a multimodal large language model (MLLM) and proposes to align GSR models using deep features to mitigate hallucinations.", "motivation": "Generative super-resolution (GSR) models, while achieving high perceptual image quality, suffer from 'hallucinations' where generated details fail to perceptually match the low resolution image (LRI) or ground-truth image (GTI). These artifacts are not well-characterized with existing image metrics.", "method": "The authors construct a prompt that assesses hallucinatory visual elements and generates a Hallucination Score (HS) using a multimodal large language model (MLLM). They also find certain deep feature distances have strong correlations with HS and use such features as differentiable reward functions.", "result": "The authors find that their Hallucination Score (HS) is closely aligned with human evaluations and provides complementary insights to prior image metrics. They also find certain deep feature distances have strong correlations with HS.", "conclusion": "This paper proposes to align GSR models by using deep features as differentiable reward functions to mitigate hallucinations."}}
{"id": "2507.14176", "categories": ["cs.LG", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.14176", "abs": "https://arxiv.org/abs/2507.14176", "authors": ["Andr\u00e9s Morales-Forero", "Lili J. Rueda", "Ronald Herrera", "Samuel Bassetto", "Eric Coatanea"], "title": "Predictive Representativity: Uncovering Racial Bias in AI-based Skin Cancer Detection", "comment": null, "summary": "Artificial intelligence (AI) systems increasingly inform medical\ndecision-making, yet concerns about algorithmic bias and inequitable outcomes\npersist, particularly for historically marginalized populations. This paper\nintroduces the concept of Predictive Representativity (PR), a framework of\nfairness auditing that shifts the focus from the composition of the data set to\noutcomes-level equity. Through a case study in dermatology, we evaluated\nAI-based skin cancer classifiers trained on the widely used HAM10000 dataset\nand on an independent clinical dataset (BOSQUE Test set) from Colombia. Our\nanalysis reveals substantial performance disparities by skin phototype, with\nclassifiers consistently underperforming for individuals with darker skin,\ndespite proportional sampling in the source data. We argue that\nrepresentativity must be understood not as a static feature of datasets but as\na dynamic, context-sensitive property of model predictions. PR operationalizes\nthis shift by quantifying how reliably models generalize fairness across\nsubpopulations and deployment contexts. We further propose an External\nTransportability Criterion that formalizes the thresholds for fairness\ngeneralization. Our findings highlight the ethical imperative for post-hoc\nfairness auditing, transparency in dataset documentation, and inclusive model\nvalidation pipelines. This work offers a scalable tool for diagnosing\nstructural inequities in AI systems, contributing to discussions on equity,\ninterpretability, and data justice and fostering a critical re-evaluation of\nfairness in data-driven healthcare.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u516c\u5e73\u6027\u5ba1\u8ba1\u6846\u67b6\uff0c\u53d1\u73b0AI\u76ae\u80a4\u764c\u5206\u7c7b\u5668\u5728\u80a4\u8272\u8f83\u6df1\u7684\u4eba\u7fa4\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5373\u4f7f\u6570\u636e\u96c6\u4e2d\u6709\u6bd4\u4f8b\u62bd\u6837\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd (AI) \u7cfb\u7edf\u8d8a\u6765\u8d8a\u591a\u5730\u4e3a\u533b\u7597\u51b3\u7b56\u63d0\u4f9b\u4fe1\u606f\uff0c\u4f46\u4eba\u4eec\u4ecd\u7136\u62c5\u5fc3\u7b97\u6cd5\u504f\u5dee\u548c\u4e0d\u516c\u5e73\u7684\u7ed3\u679c\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5386\u53f2\u4e0a\u8fb9\u7f18\u5316\u7684\u4eba\u7fa4\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u9884\u6d4b\u4ee3\u8868\u6027 (PR) \u7684\u516c\u5e73\u6027\u5ba1\u8ba1\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u91cd\u70b9\u4ece\u6570\u636e\u96c6\u7684\u7ec4\u6210\u8f6c\u79fb\u5230\u7ed3\u679c\u5c42\u9762\u7684\u516c\u5e73\u6027\u3002", "result": "\u901a\u8fc7\u76ae\u80a4\u75c5\u5b66\u6848\u4f8b\u7814\u7a76\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684 HAM10000 \u6570\u636e\u96c6\u548c\u54e5\u4f26\u6bd4\u4e9a\u7684\u72ec\u7acb\u4e34\u5e8a\u6570\u636e\u96c6\uff08BOSQUE \u6d4b\u8bd5\u96c6\uff09\u4e0a\u8bad\u7ec3\u7684\u57fa\u4e8e AI \u7684\u76ae\u80a4\u764c\u5206\u7c7b\u5668\u3002\u6211\u4eec\u7684\u5206\u6790\u663e\u793a\uff0c\u6309\u76ae\u80a4\u5149\u578b\u5212\u5206\uff0c\u6027\u80fd\u5b58\u5728\u663e\u7740\u5dee\u5f02\uff0c\u5c3d\u7ba1\u6e90\u6570\u636e\u4e2d\u5b58\u5728\u6bd4\u4f8b\u62bd\u6837\uff0c\u4f46\u5206\u7c7b\u5668\u5bf9\u80a4\u8272\u8f83\u6df1\u7684\u4eba\u7684\u8868\u73b0\u59cb\u7ec8\u4e0d\u4f73\u3002", "conclusion": "\u9700\u8981\u5bf9AI\u7cfb\u7edf\u8fdb\u884c\u516c\u5e73\u6027\u5ba1\u8ba1\uff0c\u6570\u636e\u96c6\u6587\u6863\u9700\u8981\u900f\u660e\u5316\uff0c\u6a21\u578b\u9a8c\u8bc1\u7ba1\u9053\u9700\u8981\u5177\u6709\u5305\u5bb9\u6027\u3002\u8fd9\u9879\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u8bca\u65adAI\u7cfb\u7edf\u4e2d\u7684\u7ed3\u6784\u6027\u4e0d\u5e73\u7b49\uff0c\u4fc3\u8fdb\u5173\u4e8e\u516c\u5e73\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6570\u636e\u516c\u6b63\u7684\u8ba8\u8bba\uff0c\u5e76\u4fc3\u8fdb\u5bf9\u6570\u636e\u9a71\u52a8\u7684\u533b\u7597\u4fdd\u5065\u4e2d\u7684\u516c\u5e73\u6027\u7684\u91cd\u65b0\u8bc4\u4f30\u3002"}}
{"id": "2507.14334", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14334", "abs": "https://arxiv.org/abs/2507.14334", "authors": ["Hui Yang", "Jiaoyan Chen", "Yuan He", "Yongsheng Gao", "Ian Horrocks"], "title": "Language Models as Ontology Encoders", "comment": null, "summary": "OWL (Web Ontology Language) ontologies which are able to formally represent\ncomplex knowledge and support semantic reasoning have been widely adopted\nacross various domains such as healthcare and bioinformatics. Recently,\nontology embeddings have gained wide attention due to its potential to infer\nplausible new knowledge and approximate complex reasoning. However, existing\nmethods face notable limitations: geometric model-based embeddings typically\noverlook valuable textual information, resulting in suboptimal performance,\nwhile the approaches that incorporate text, which are often based on language\nmodels, fail to preserve the logical structure. In this work, we propose a new\nontology embedding method OnT, which tunes a Pretrained Language Model (PLM)\nvia geometric modeling in a hyperbolic space for effectively incorporating\ntextual labels and simultaneously preserving class hierarchies and other\nlogical relationships of Description Logic EL. Extensive experiments on four\nreal-world ontologies show that OnT consistently outperforms the baselines\nincluding the state-of-the-art across both tasks of prediction and inference of\naxioms. OnT also demonstrates strong potential in real-world applications,\nindicated by its robust transfer learning abilities and effectiveness in real\ncases of constructing a new ontology from SNOMED CT. Data and code are\navailable at https://github.com/HuiYang1997/OnT.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u672c\u4f53\u5d4c\u5165\u65b9\u6cd5 OnT\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u53cc\u66f2\u7a7a\u95f4\u4e2d\u7684\u51e0\u4f55\u5efa\u6a21\u8c03\u6574\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u6709\u6548\u5730\u7ed3\u5408\u6587\u672c\u6807\u7b7e\uff0c\u540c\u65f6\u4fdd\u7559\u7c7b\u5c42\u6b21\u7ed3\u6784\u548c\u63cf\u8ff0\u903b\u8f91 EL \u7684\u5176\u4ed6\u903b\u8f91\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u7684\u672c\u4f53\u5d4c\u5165\u65b9\u6cd5\u9762\u4e34\u7740\u660e\u663e\u7684\u5c40\u9650\u6027\uff1a\u57fa\u4e8e\u51e0\u4f55\u6a21\u578b\u7684\u5d4c\u5165\u901a\u5e38\u5ffd\u7565\u6709\u4ef7\u503c\u7684\u6587\u672c\u4fe1\u606f\uff0c\u5bfc\u81f4\u6027\u80fd\u6b20\u4f73\uff0c\u800c\u7ed3\u5408\u6587\u672c\u7684\u65b9\u6cd5\uff08\u901a\u5e38\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\uff09\u672a\u80fd\u4fdd\u7559\u903b\u8f91\u7ed3\u6784\u3002", "method": "\u901a\u8fc7\u53cc\u66f2\u7a7a\u95f4\u4e2d\u7684\u51e0\u4f55\u5efa\u6a21\u6765\u8c03\u6574\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b (PLM)\uff0c\u4ee5\u6709\u6548\u5730\u7ed3\u5408\u6587\u672c\u6807\u7b7e\uff0c\u540c\u65f6\u4fdd\u7559\u7c7b\u5c42\u6b21\u7ed3\u6784\u548c\u63cf\u8ff0\u903b\u8f91 EL \u7684\u5176\u4ed6\u903b\u8f91\u5173\u7cfb\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u672c\u4f53\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cOnT \u5728\u516c\u7406\u9884\u6d4b\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u5305\u62ec\u6700\u5148\u8fdb\u6280\u672f\u5728\u5185\u7684\u57fa\u7ebf\u3002OnT \u8fd8\u901a\u8fc7\u5176\u5f3a\u5927\u7684\u8fc1\u79fb\u5b66\u4e60\u80fd\u529b\u548c\u4ece SNOMED CT \u6784\u5efa\u65b0\u672c\u4f53\u7684\u5b9e\u9645\u6848\u4f8b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b\u3002", "conclusion": "OnT\u5728\u516c\u7406\u9884\u6d4b\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u5305\u62ec\u6700\u5148\u8fdb\u6280\u672f\u5728\u5185\u7684\u57fa\u7ebf\uff0c\u5e76\u5728\u6784\u5efa\u672c\u4f53\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.14946", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14946", "abs": "https://arxiv.org/abs/2507.14946", "authors": ["Amna Ali", "Liyanage C. De Silva", "Pg Emeroylariffion Abas"], "title": "FullRecall: A Semantic Search-Based Ranking Approach for Maximizing Recall in Patent Retrieval", "comment": null, "summary": "Patent examiners and inventors face significant pressure to verify the\noriginality and non-obviousness of inventions, and the intricate nature of\npatent data intensifies the challenges of patent retrieval. Therefore, there is\na pressing need to devise cutting-edge retrieval strategies that can reliably\nachieve the desired recall. This study introduces FullRecall, a novel patent\nretrieval approach that effectively manages the complexity of patent data while\nmaintaining the reliability of relevance matching and maximising recall. It\nleverages IPC-guided knowledge to generate informative phrases, which are\nprocessed to extract key information in the form of noun phrases characterising\nthe query patent under observation. From these, the top k keyphrases are\nselected to construct a query for retrieving a focused subset of the dataset.\nThis initial retrieval step achieves complete recall, successfully capturing\nall relevant documents. To further refine the results, a ranking scheme is\napplied to the retrieved subset, reducing its size while maintaining 100%\nrecall. This multi-phase process demonstrates an effective strategy for\nbalancing precision and recall in patent retrieval tasks. Comprehensive\nexperiments were conducted, and the results were compared with baseline\nstudies, namely HRR2 [1] and ReQ-ReC [2]. The proposed approach yielded\nsuperior results, achieving 100% recall in all five test cases. However,\nHRR2[1] recall values across the five test cases were 10%, 25%, 33.3%, 0%, and\n14.29%, while ReQ-ReC [2] showed 50% for the first test case, 25% for the\nsecond test case, and 0% for the third, fourth, and fifth test cases. The 100%\nrecall ensures that no relevant prior art is overlooked, thereby strengthening\nthe patent pre-filing and examination processes, hence reducing potential legal\nrisks.", "AI": {"tldr": "FullRecall, a novel patent retrieval approach, achieves 100% recall by using IPC-guided knowledge and a multi-phase process, outperforming baseline studies.", "motivation": "Patent examiners and inventors face significant pressure to verify the originality and non-obviousness of inventions, and the intricate nature of patent data intensifies the challenges of patent retrieval. Therefore, there is a pressing need to devise cutting-edge retrieval strategies that can reliably achieve the desired recall.", "method": "This study introduces FullRecall, a novel patent retrieval approach that effectively manages the complexity of patent data while maintaining the reliability of relevance matching and maximising recall. It leverages IPC-guided knowledge to generate informative phrases, which are processed to extract key information in the form of noun phrases characterising the query patent under observation. From these, the top k keyphrases are selected to construct a query for retrieving a focused subset of the dataset. This initial retrieval step achieves complete recall, successfully capturing all relevant documents. To further refine the results, a ranking scheme is applied to the retrieved subset, reducing its size while maintaining 100% recall. This multi-phase process demonstrates an effective strategy for balancing precision and recall in patent retrieval tasks.", "result": "The proposed approach yielded superior results, achieving 100% recall in all five test cases. However, HRR2[1] recall values across the five test cases were 10%, 25%, 33.3%, 0%, and 14.29%, while ReQ-ReC [2] showed 50% for the first test case, 25% for the second test case, and 0% for the third, fourth, and fifth test cases.", "conclusion": "The proposed approach yielded superior results, achieving 100% recall in all five test cases. The 100% recall ensures that no relevant prior art is overlooked, thereby strengthening the patent pre-filing and examination processes, hence reducing potential legal risks."}}
{"id": "2507.14238", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.14238", "abs": "https://arxiv.org/abs/2507.14238", "authors": ["Matthew Kearney", "Reuben Binns", "Yarin Gal"], "title": "Language Models Change Facts Based on the Way You Talk", "comment": null, "summary": "Large language models (LLMs) are increasingly being used in user-facing\napplications, from providing medical consultations to job interview advice.\nRecent research suggests that these models are becoming increasingly proficient\nat inferring identity information about the author of a piece of text from\nlinguistic patterns as subtle as the choice of a few words. However, little is\nknown about how LLMs use this information in their decision-making in\nreal-world applications. We perform the first comprehensive analysis of how\nidentity markers present in a user's writing bias LLM responses across five\ndifferent high-stakes LLM applications in the domains of medicine, law,\npolitics, government benefits, and job salaries. We find that LLMs are\nextremely sensitive to markers of identity in user queries and that race,\ngender, and age consistently influence LLM responses in these applications. For\ninstance, when providing medical advice, we find that models apply different\nstandards of care to individuals of different ethnicities for the same\nsymptoms; we find that LLMs are more likely to alter answers to align with a\nconservative (liberal) political worldview when asked factual questions by\nolder (younger) individuals; and that LLMs recommend lower salaries for\nnon-White job applicants and higher salaries for women compared to men. Taken\ntogether, these biases mean that the use of off-the-shelf LLMs for these\napplications may cause harmful differences in medical care, foster wage gaps,\nand create different political factual realities for people of different\nidentities. Beyond providing an analysis, we also provide new tools for\nevaluating how subtle encoding of identity in users' language choices impacts\nmodel decisions. Given the serious implications of these findings, we recommend\nthat similar thorough assessments of LLM use in user-facing applications are\nconducted before future deployment.", "AI": {"tldr": "LLMs are biased by identity markers in user queries, leading to inconsistent and potentially harmful responses in high-stakes applications like medicine, law, and job seeking.", "motivation": "Little is known about how LLMs use identity information in their decision-making in real-world applications, while LLMs are increasingly being used in user-facing applications.", "method": "Comprehensive analysis of how identity markers present in a user's writing bias LLM responses across five different high-stakes LLM applications in the domains of medicine, law, politics, government benefits, and job salaries.", "result": "LLMs are extremely sensitive to markers of identity in user queries and that race, gender, and age consistently influence LLM responses in these applications. LLMs apply different standards of care to individuals of different ethnicities, alter answers to align with political worldviews based on age, and recommend different salaries based on race and gender.", "conclusion": "Off-the-shelf LLMs in user-facing applications may cause harmful differences in medical care, foster wage gaps, and create different political factual realities for people of different identities. Thorough assessments of LLM use in user-facing applications are recommended before future deployment."}}
{"id": "2507.14368", "categories": ["cs.CV", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.14368", "abs": "https://arxiv.org/abs/2507.14368", "authors": ["Praneeth Namburi", "Roger Pallar\u00e8s-L\u00f3pez", "Jessica Rosendorf", "Duarte Folgado", "Brian W. Anthony"], "title": "DUSTrack: Semi-automated point tracking in ultrasound videos", "comment": null, "summary": "Ultrasound technology enables safe, non-invasive imaging of dynamic tissue\nbehavior, making it a valuable tool in medicine, biomechanics, and sports\nscience. However, accurately tracking tissue motion in B-mode ultrasound\nremains challenging due to speckle noise, low edge contrast, and out-of-plane\nmovement. These challenges complicate the task of tracking anatomical landmarks\nover time, which is essential for quantifying tissue dynamics in many clinical\nand research applications. This manuscript introduces DUSTrack (Deep learning\nand optical flow-based toolkit for UltraSound Tracking), a semi-automated\nframework for tracking arbitrary points in B-mode ultrasound videos. We combine\ndeep learning with optical flow to deliver high-quality and robust tracking\nacross diverse anatomical structures and motion patterns. The toolkit includes\na graphical user interface that streamlines the generation of high-quality\ntraining data and supports iterative model refinement. It also implements a\nnovel optical-flow-based filtering technique that reduces high-frequency\nframe-to-frame noise while preserving rapid tissue motion. DUSTrack\ndemonstrates superior accuracy compared to contemporary zero-shot point\ntrackers and performs on par with specialized methods, establishing its\npotential as a general and foundational tool for clinical and biomechanical\nresearch. We demonstrate DUSTrack's versatility through three use cases:\ncardiac wall motion tracking in echocardiograms, muscle deformation analysis\nduring reaching tasks, and fascicle tracking during ankle plantarflexion. As an\nopen-source solution, DUSTrack offers a powerful, flexible framework for point\ntracking to quantify tissue motion from ultrasound videos. DUSTrack is\navailable at https://github.com/praneethnamburi/DUSTrack.", "AI": {"tldr": "DUSTrack\u662f\u4e00\u4e2a\u534a\u81ea\u52a8\u6846\u67b6\uff0c\u7528\u4e8e\u8ddf\u8e2aB\u8d85\u89c6\u9891\u4e2d\u7684\u4efb\u610f\u70b9\uff0c\u5b83\u7ed3\u5408\u4e86\u6df1\u5ea6\u5b66\u4e60\u548c\u5149\u6d41\u6280\u672f\uff0c\u5177\u6709\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u901a\u8fc7\u4e09\u4e2a\u7528\u4f8b\u5c55\u793a\u4e86\u5176\u901a\u7528\u6027\u3002", "motivation": "\u7531\u4e8e\u6563\u6591\u566a\u58f0\u3001\u4f4e\u8fb9\u7f18\u5bf9\u6bd4\u5ea6\u548c\u5e73\u9762\u5916\u8fd0\u52a8\uff0c\u51c6\u786e\u8ddf\u8e2aB\u8d85\u4e2d\u7684\u7ec4\u7ec7\u8fd0\u52a8\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u8fd9\u4e9b\u6311\u6218\u4f7f\u5f97\u8ddf\u8e2a\u968f\u65f6\u95f4\u53d8\u5316\u7684\u89e3\u5256\u6807\u5fd7\u53d8\u5f97\u590d\u6742\uff0c\u8fd9\u5bf9\u4e8e\u91cf\u5316\u8bb8\u591a\u4e34\u5e8a\u548c\u7814\u7a76\u5e94\u7528\u4e2d\u7684\u7ec4\u7ec7\u52a8\u529b\u5b66\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u5149\u6d41\u6280\u672f\uff0c\u5e76\u91c7\u7528\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u5149\u6d41\u7684\u6ee4\u6ce2\u6280\u672f\uff0c\u51cf\u5c11\u9ad8\u9891\u5e27\u95f4\u566a\u58f0\uff0c\u540c\u65f6\u4fdd\u7559\u5feb\u901f\u7ec4\u7ec7\u8fd0\u52a8\u3002", "result": "DUSTrack\u5728\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u5f53\u4ee3\u96f6\u6837\u672c\u70b9\u8ddf\u8e2a\u5668\uff0c\u5e76\u4e14\u4e0e\u4e13\u95e8\u65b9\u6cd5\u76f8\u6bd4\u8868\u73b0\u76f8\u5f53\uff0c\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3a\u4e34\u5e8a\u548c\u751f\u7269\u529b\u5b66\u7814\u7a76\u7684\u901a\u7528\u548c\u57fa\u7840\u5de5\u5177\u7684\u6f5c\u529b\u3002\u901a\u8fc7\u4e09\u4e2a\u7528\u4f8b\u5c55\u793a\u4e86DUSTrack\u7684\u901a\u7528\u6027\uff1a\u8d85\u58f0\u5fc3\u52a8\u56fe\u4e2d\u7684\u5fc3\u58c1\u8fd0\u52a8\u8ddf\u8e2a\u3001\u4f38\u624b\u4efb\u52a1\u671f\u95f4\u7684\u808c\u8089\u53d8\u5f62\u5206\u6790\u548c\u8e1d\u5173\u8282\u8dd6\u5c48\u671f\u95f4\u7684\u808c\u675f\u8ddf\u8e2a\u3002", "conclusion": "DUSTrack\u4f5c\u4e3a\u4e00\u4e2a\u5f00\u6e90\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u4ece\u8d85\u58f0\u89c6\u9891\u4e2d\u91cf\u5316\u7ec4\u7ec7\u8fd0\u52a8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u800c\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u5e76\u5df2\u5728https://github.com/praneethnamburi/DUSTrack\u4e0a\u53d1\u5e03\u3002"}}
{"id": "2507.14177", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA", "68T07(Primary), 41A15(Secondary)", "I.2.6; G.1.2"], "pdf": "https://arxiv.org/pdf/2507.14177", "abs": "https://arxiv.org/abs/2507.14177", "authors": ["Changcun Huang"], "title": "Understanding Two-Layer Neural Networks with Smooth Activation Functions", "comment": null, "summary": "This paper aims to understand the training solution, which is obtained by the\nback-propagation algorithm, of two-layer neural networks whose hidden layer is\ncomposed of the units with smooth activation functions, including the usual\nsigmoid type most commonly used before the advent of ReLUs. The mechanism\ncontains four main principles: construction of Taylor series expansions, strict\npartial order of knots, smooth-spline implementation and smooth-continuity\nrestriction. The universal approximation for arbitrary input dimensionality is\nproved and experimental verification is given, through which the mystery of\n``black box'' of the solution space is largely revealed. The new proofs\nemployed also enrich approximation theory.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e24\u5c42\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u89e3\u51b3\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u901a\u7528\u903c\u8fd1\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u63ed\u793a\u4e86\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\u7684\u795e\u79d8\u6027\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u4e86\u89e3\u4e24\u5c42\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u7f51\u7edc\u7684\u9690\u85cf\u5c42\u7531\u5177\u6709\u5e73\u6ed1\u6fc0\u6d3b\u529f\u80fd\u7684\u5355\u5143\u7ec4\u6210\uff0c\u5305\u62ecReLU\u51fa\u73b0\u4e4b\u524d\u6700\u5e38\u7528\u7684\u901a\u5e38\u7684sigmoid\u7c7b\u578b\u3002", "method": "\u6cf0\u52d2\u7ea7\u6570\u5c55\u5f00\u7684\u6784\u5efa\uff0c\u7ed3\u70b9\u7684\u4e25\u683c\u504f\u5e8f\uff0c\u5149\u6ed1\u6837\u6761\u5b9e\u73b0\u548c\u5e73\u6ed1\u8fde\u7eed\u6027\u7ea6\u675f", "result": "\u8bc1\u660e\u4e86\u4efb\u610f\u8f93\u5165\u7ef4\u5ea6\u7684\u901a\u7528\u903c\u8fd1", "conclusion": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5f88\u5927\u7a0b\u5ea6\u4e0a\u63ed\u793a\u4e86\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\u7684\u201c\u9ed1\u5323\u5b50\u201d\u7684\u795e\u79d8\u9762\u7eb1\u3002\u6240\u91c7\u7528\u7684\u65b0\u8bc1\u660e\u4e5f\u4e30\u5bcc\u4e86\u903c\u8fd1\u7406\u8bba\u3002"}}
{"id": "2507.14335", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14335", "abs": "https://arxiv.org/abs/2507.14335", "authors": ["Nicolas Wischermann", "Claudio Mayrink Verdun", "Gabriel Poesia", "Francesco Noseda"], "title": "ProofCompass: Enhancing Specialized Provers with LLM Guidance", "comment": "19 pages, 7 figures. Accepted at the 2nd AI for MATH Workshop at the\n  42nd International Conference on Machine Learning (ICML 2025)", "summary": "Language models have become increasingly powerful tools for formal\nmathematical reasoning. However, most existing approaches rely exclusively on\neither large general-purpose models or smaller specialized models, each with\ndistinct limitations, while training specialized large models still requires\nsignificant computational resources. This paper introduces ProofCompass, a\nnovel hybrid methodology that achieves remarkable computational efficiency by\nstrategically guiding existing specialized prover methods, such as\nDeepSeek-Prover-v1.5-RL (DSP-v1.5) with a Large Language Model (LLM) without\nrequiring additional model training. The LLM provides natural language proof\nstrategies and analyzes failed attempts to select intermediate lemmas, enabling\neffective problem decomposition. On the miniF2F benchmark, ProofCompass\ndemonstrates substantial resource efficiency: it outperforms DSP-v1.5 ($54.9\\%\n\\rightarrow 55.3\\%$) while using 25x fewer attempts ($3200 \\rightarrow 128$).\nOur synergistic approach paves the way for simultaneously improving\ncomputational efficiency and accuracy in formal theorem proving.", "AI": {"tldr": "ProofCompass is a novel hybrid methodology that achieves remarkable computational efficiency by strategically guiding existing specialized prover methods with a Large Language Model (LLM) without requiring additional model training.", "motivation": "Existing approaches rely exclusively on either large general-purpose models or smaller specialized models, each with distinct limitations, while training specialized large models still requires significant computational resources.", "method": "ProofCompass guides existing specialized prover methods with a Large Language Model (LLM) without additional model training.", "result": "ProofCompass outperforms DSP-v1.5 (54.9% -> 55.3%) while using 25x fewer attempts (3200 -> 128) on the miniF2F benchmark.", "conclusion": "ProofCompass paves the way for simultaneously improving computational efficiency and accuracy in formal theorem proving."}}
{"id": "2507.15113", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.15113", "abs": "https://arxiv.org/abs/2507.15113", "authors": ["Xiangyu Zeng", "Amit Jaspal", "Bin Liu", "Goutham Panneeru", "Kevin Huang", "Nicolas Bievre", "Mohit Jaggi", "Prathap Maniraju", "Ankur Jain"], "title": "Click A, Buy B: Rethinking Conversion Attribution in E- Commerce Recommendations", "comment": null, "summary": "User journeys in e-commerce routinely violate the one-to-one assumption that\na clicked item on an advertising platform is the same item later purchased on\nthe merchant's website/app. For a significant number of converting sessions on\nour platform, users click product A but buy product B -- the Click A, Buy B\n(CABB) phenomenon. Training recommendation models on raw click-conversion pairs\ntherefore rewards items that merely correlate with purchases, leading to biased\nlearning and sub-optimal conversion rates. We reframe conversion prediction as\na multi-task problem with separate heads for Click A Buy A (CABA) and Click A\nBuy B (CABB). To isolate informative CABB conversions from unrelated CABB\nconversions, we introduce a taxonomy-aware collaborative filtering weighting\nscheme where each product is first mapped to a leaf node in a product taxonomy,\nand a category-to-category similarity matrix is learned from large-scale\nco-engagement logs. This weighting amplifies pairs that reflect genuine\nsubstitutable or complementary relations while down-weighting coincidental\ncross-category purchases. Offline evaluation on e-commerce sessions reduces\nnormalized entropy by 13.9% versus a last-click attribution baseline. An online\nA/B test on live traffic shows +0.25% gains in the primary business metric.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u7535\u5546\u4e2d\u70b9\u51fb\u5546\u54c1\u548c\u8d2d\u4e70\u5546\u54c1\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u5206\u7c7b\u611f\u77e5\u534f\u540c\u8fc7\u6ee4\u52a0\u6743\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u63a8\u8350\u6a21\u578b\u7684\u8f6c\u5316\u7387\u3002", "motivation": "\u5728\u7535\u5546\u7528\u6237\u65c5\u7a0b\u4e2d\uff0c\u70b9\u51fb\u5546\u54c1\u548c\u6700\u7ec8\u8d2d\u4e70\u5546\u54c1\u4e0d\u4e00\u81f4\u7684\u73b0\u8c61\u666e\u904d\u5b58\u5728\uff0c\u5bfc\u81f4\u4f7f\u7528\u539f\u59cb\u70b9\u51fb-\u8f6c\u5316\u5bf9\u8bad\u7ec3\u63a8\u8350\u6a21\u578b\u4f1a\u4ea7\u751f\u504f\u5dee\uff0c\u964d\u4f4e\u8f6c\u5316\u7387\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u5177\u6709\u9488\u5bf9Click A Buy A (CABA)\u548cClick A Buy B (CABB)\u7684\u72ec\u7acbhead\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u5206\u7c7b\u611f\u77e5\u534f\u540c\u8fc7\u6ee4\u52a0\u6743\u65b9\u6848\u3002", "result": "\u79bb\u7ebf\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u5bf9\u4e8elast-click\u5f52\u56e0\u57fa\u7ebf\uff0c\u5f52\u4e00\u5316\u71b5\u964d\u4f4e\u4e8613.9%\u3002\u5728\u7ebfA/B\u6d4b\u8bd5\u8868\u660e\uff0c\u4e3b\u8981\u4e1a\u52a1\u6307\u6807\u63d0\u5347\u4e860.25%\u3002", "conclusion": "\u901a\u8fc7\u5c06\u8f6c\u5316\u9884\u6d4b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u591a\u4efb\u52a1\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u4e00\u79cd\u5206\u7c7b\u611f\u77e5\u534f\u540c\u8fc7\u6ee4\u52a0\u6743\u65b9\u6848\uff0c\u8be5\u8bba\u6587\u63d0\u9ad8\u4e86\u7535\u5546\u73af\u5883\u4e0b\u7684\u63a8\u8350\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2507.14239", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14239", "abs": "https://arxiv.org/abs/2507.14239", "authors": ["Weihua Zheng", "Roy Ka-Wei Lee", "Zhengyuan Liu", "Kui Wu", "AiTi Aw", "Bowei Zou"], "title": "CCL-XCoT: An Efficient Cross-Lingual Knowledge Transfer Method for Mitigating Hallucination Generation", "comment": null, "summary": "Multilingual Large Language Models(MLLMs) demonstrate strong generalization\nacross languages, yet they remain prone to hallucinations, especially in\nlow-resource languages, due to training data imbalances. These hallucinations,\nwhich include inaccurate or fabricated outputs, are particularly problematic in\ndomain-specific generation tasks (Chataigner et al., 2024). To address this\nchallenge, we propose CCL-XCoT(Curriculum-based Contrastive Learning-based\nCross-lingual Chain-of-Thought), a two-stage fine-tuning framework for\nmitigating hallucination in MLLMs. Our approach first enhances cross-lingual\nsemantic alignment through curriculum-based contrastive learning combined with\nnext-token prediction during continued pre-training. Building on this\nfoundation, we then introduce a cross-lingual Chain-of-Thought (XCoT) prompting\nstrategy during instruction fine-tuning, which guides the model to reason in a\nhigh-resource language before generating answers in the target low-resource\nlanguage. Experimental results show that CCL-XCoT reduces hallucination rates\nby up to 62% and substantially improves factual knowledge transfer across\nlanguage pairs, without relying on external retrieval or multi-model ensembles.", "AI": {"tldr": "This paper proposes CCL-XCoT, a two-stage fine-tuning framework, to reduce hallucinations in multilingual language models, especially in low-resource languages. It uses curriculum-based contrastive learning and cross-lingual Chain-of-Thought prompting.", "motivation": "Multilingual Large Language Models(MLLMs) demonstrate strong generalization across languages, yet they remain prone to hallucinations, especially in low-resource languages, due to training data imbalances. These hallucinations, which include inaccurate or fabricated outputs, are particularly problematic in domain-specific generation tasks", "method": "a two-stage fine-tuning framework for mitigating hallucination in MLLMs. Our approach first enhances cross-lingual semantic alignment through curriculum-based contrastive learning combined with next-token prediction during continued pre-training. Building on this foundation, we then introduce a cross-lingual Chain-of-Thought (XCoT) prompting strategy during instruction fine-tuning, which guides the model to reason in a high-resource language before generating answers in the target low-resource language.", "result": "CCL-XCoT reduces hallucination rates by up to 62% and substantially improves factual knowledge transfer across language pairs", "conclusion": "CCL-XCoT reduces hallucination rates by up to 62% and substantially improves factual knowledge transfer across language pairs, without relying on external retrieval or multi-model ensembles."}}
{"id": "2507.14372", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.14372", "abs": "https://arxiv.org/abs/2507.14372", "authors": ["Albert Chen", "Manas Bundele", "Gaurav Ahlawat", "Patrick Stetz", "Zhitao Wang", "Qiang Fei", "Donghoon Jung", "Audrey Chu", "Bharadwaj Jayaraman", "Ayushi Panth", "Yatin Arora", "Sourav Jain", "Renjith Varma", "Alexey Ilin", "Iuliia Melnychuk", "Chelsea Chueh", "Joyan Sil", "Xiaofeng Wang"], "title": "Text-to-SQL for Enterprise Data Analytics", "comment": "11 pages, 8 figures, Workshop on Agentic AI for Enterprise at KDD '25", "summary": "The introduction of large language models has brought rapid progress on\nText-to-SQL benchmarks, but it is not yet easy to build a working enterprise\nsolution. In this paper, we present insights from building an internal chatbot\nthat enables LinkedIn's product managers, engineers, and operations teams to\nself-serve data insights from a large, dynamic data lake. Our approach features\nthree components. First, we construct a knowledge graph that captures\nup-to-date semantics by indexing database metadata, historical query logs,\nwikis, and code. We apply clustering to identify relevant tables for each team\nor product area. Second, we build a Text-to-SQL agent that retrieves and ranks\ncontext from the knowledge graph, writes a query, and automatically corrects\nhallucinations and syntax errors. Third, we build an interactive chatbot that\nsupports various user intents, from data discovery to query writing to\ndebugging, and displays responses in rich UI elements to encourage follow-up\nchats. Our chatbot has over 300 weekly users. Expert review shows that 53% of\nits responses are correct or close to correct on an internal benchmark set.\nThrough ablation studies, we identify the most important knowledge graph and\nmodeling components, offering a practical path for developing enterprise\nText-to-SQL solutions.", "AI": {"tldr": "LinkedIn built a chatbot for internal data insights using a knowledge graph and Text-to-SQL agent, achieving 53% accuracy.", "motivation": "Building a working enterprise Text-to-SQL solution is challenging despite progress in Text-to-SQL benchmarks.", "method": "The approach involves a knowledge graph for up-to-date semantics, a Text-to-SQL agent with retrieval and error correction, and an interactive chatbot with a rich UI.", "result": "The chatbot has over 300 weekly users and achieves 53% accuracy on an internal benchmark set.", "conclusion": "The paper presents a chatbot that enables self-service data insights at LinkedIn, achieving 53% accuracy on an internal benchmark. Ablation studies identify key components for enterprise Text-to-SQL solutions."}}
{"id": "2507.14426", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14426", "abs": "https://arxiv.org/abs/2507.14426", "authors": ["Zhou Chen", "Joe Lin", "Sathyanarayanan N. Aakur"], "title": "CRAFT: A Neuro-Symbolic Framework for Visual Functional Affordance Grounding", "comment": "Accepted to NeSy 2025", "summary": "We introduce CRAFT, a neuro-symbolic framework for interpretable affordance\ngrounding, which identifies the objects in a scene that enable a given action\n(e.g., \"cut\"). CRAFT integrates structured commonsense priors from ConceptNet\nand language models with visual evidence from CLIP, using an energy-based\nreasoning loop to refine predictions iteratively. This process yields\ntransparent, goal-driven decisions to ground symbolic and perceptual\nstructures. Experiments in multi-object, label-free settings demonstrate that\nCRAFT enhances accuracy while improving interpretability, providing a step\ntoward robust and trustworthy scene understanding.", "AI": {"tldr": "CRAFT\u662f\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u5e38\u8bc6\u548c\u89c6\u89c9\u4fe1\u606f\u6765\u8bc6\u522b\u573a\u666f\u4e2d\u652f\u6301\u7279\u5b9a\u52a8\u4f5c\u7684\u5bf9\u8c61\uff0c\u4ece\u800c\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u672c\u6587\u4ecb\u7ecd\u4e86CRAFT\uff0c\u8fd9\u662f\u4e00\u79cd\u7528\u4e8e\u53ef\u89e3\u91ca\u7684\u627f\u8f7d\u5173\u7cfb\u63a5\u5730\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u5b83\u53ef\u4ee5\u8bc6\u522b\u573a\u666f\u4e2d\u80fd\u591f\u5b9e\u73b0\u7ed9\u5b9a\u52a8\u4f5c\uff08\u4f8b\u5982\uff0c\u201c\u5207\u5272\u201d\uff09\u7684\u5bf9\u8c61\u3002", "method": "CRAFT\u6574\u5408\u4e86\u6765\u81eaConceptNet\u548c\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u6784\u5316\u5e38\u8bc6\u5148\u9a8c\u77e5\u8bc6\u4e0e\u6765\u81eaCLIP\u7684\u89c6\u89c9\u8bc1\u636e\uff0c\u4f7f\u7528\u57fa\u4e8e\u80fd\u91cf\u7684\u63a8\u7406\u5faa\u73af\u6765\u8fed\u4ee3\u5730\u6539\u8fdb\u9884\u6d4b\u3002", "result": "\u5728\u591a\u5bf9\u8c61\u3001\u65e0\u6807\u7b7e\u8bbe\u7f6e\u4e0b\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCRAFT\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "CRAFT\u901a\u8fc7\u6574\u5408\u5e38\u8bc6\u5148\u9a8c\u77e5\u8bc6\u548c\u89c6\u89c9\u8bc1\u636e\uff0c\u589e\u5f3a\u4e86\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u9c81\u68d2\u548c\u53ef\u4fe1\u7684\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6b65\u9aa4\u3002"}}
{"id": "2507.14178", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14178", "abs": "https://arxiv.org/abs/2507.14178", "authors": ["Yuhang Liu", "Yuefei Wu", "Bin Shi", "Bo Dong"], "title": "Feature Bank Enhancement for Distance-based Out-of-Distribution Detection", "comment": "8 pages, 5 figures", "summary": "Out-of-distribution (OOD) detection is critical to ensuring the reliability\nof deep learning applications and has attracted significant attention in recent\nyears. A rich body of literature has emerged to develop efficient score\nfunctions that assign high scores to in-distribution (ID) samples and low\nscores to OOD samples, thereby helping distinguish OOD samples. Among these\nmethods, distance-based score functions are widely used because of their\nefficiency and ease of use. However, deep learning often leads to a biased\ndistribution of data features, and extreme features are inevitable. These\nextreme features make the distance-based methods tend to assign too low scores\nto ID samples. This limits the OOD detection capabilities of such methods. To\naddress this issue, we propose a simple yet effective method, Feature Bank\nEnhancement (FBE), that uses statistical characteristics from dataset to\nidentify and constrain extreme features to the separation boundaries, therapy\nmaking the distance between samples inside and outside the distribution\nfarther. We conducted experiments on large-scale ImageNet-1k and CIFAR-10\nrespectively, and the results show that our method achieves state-of-the-art\nperformance on both benchmark. Additionally, theoretical analysis and\nsupplementary experiments are conducted to provide more insights into our\nmethod.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a FBE \u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u57fa\u4e8e\u8ddd\u79bb\u7684 OOD \u68c0\u6d4b\u65b9\u6cd5\u4e2d\u7531\u4e8e\u6781\u7aef\u7279\u5f81\u5bfc\u81f4\u7684 ID \u6837\u672c\u8bc4\u5206\u8fc7\u4f4e\u7684\u95ee\u9898\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u901a\u5e38\u5bfc\u81f4\u6570\u636e\u7279\u5f81\u7684\u504f\u5dee\u5206\u5e03\uff0c\u5e76\u4e14\u4e0d\u53ef\u907f\u514d\u5730\u4f1a\u51fa\u73b0\u6781\u7aef\u7279\u5f81\u3002\u8fd9\u4e9b\u6781\u7aef\u7279\u5f81\u4f7f\u5f97\u57fa\u4e8e\u8ddd\u79bb\u7684\u65b9\u6cd5\u503e\u5411\u4e8e\u4e3a ID \u6837\u672c\u5206\u914d\u8fc7\u4f4e\u7684\u8bc4\u5206\uff0c\u4ece\u800c\u9650\u5236\u4e86\u6b64\u7c7b\u65b9\u6cd5\u7684 OOD \u68c0\u6d4b\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5373\u7279\u5f81\u5e93\u589e\u5f3a (FBE)\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528\u6570\u636e\u96c6\u7684\u7edf\u8ba1\u7279\u5f81\u6765\u8bc6\u522b\u548c\u7ea6\u675f\u6781\u7aef\u7279\u5f81\u5230\u5206\u79bb\u8fb9\u754c\u3002", "result": "\u8be5\u65b9\u6cd5\u5728 ImageNet-1k \u548c CIFAR-10 \u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728 ImageNet-1k \u548c CIFAR-10 \u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u8865\u5145\u5b9e\u9a8c\u63d0\u4f9b\u4e86\u66f4\u591a\u5173\u4e8e\u8be5\u65b9\u6cd5\u7684\u89c1\u89e3\u3002"}}
{"id": "2507.14393", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14393", "abs": "https://arxiv.org/abs/2507.14393", "authors": ["Humza Sami", "Mubashir ul Islam", "Pierre-Emmanuel Gaillardon", "Valerio Tenace"], "title": "Adaptive Multi-Agent Reasoning via Automated Workflow Generation", "comment": null, "summary": "The rise of Large Reasoning Models (LRMs) promises a significant leap forward\nin language model capabilities, aiming to tackle increasingly sophisticated\ntasks with unprecedented efficiency and accuracy. However, despite their\nimpressive performance, recent studies have highlighted how current reasoning\nmodels frequently fail to generalize to novel, unseen problems, often resorting\nto memorized solutions rather than genuine inferential reasoning. Such behavior\nunderscores a critical limitation in modern LRMs, i.e., their tendency toward\noverfitting, which in turn results in poor generalization in problem-solving\ncapabilities.\n  In this paper, we introduce Nexus Architect, an enhanced iteration of our\nmulti-agent system framework, Nexus, equipped with a novel automated workflow\nsynthesis mechanism. Given a user's prompt and a small set of representative\nexamples, the Architect autonomously generates a tailored reasoning workflow by\nselecting suitable strategies, tool integrations, and adversarial techniques\nfor a specific problem class. Furthermore, the Architect includes an iterative\nprompt refinement mechanism that fine-tunes agents' system prompts to maximize\nperformance and improve the generalization capabilities of the system.\n  We empirically evaluate Nexus Architect by employing an off-the-shelf,\nnon-reasoning model on a custom dataset of challenging logical questions and\ncompare its performance against state-of-the-art LRMs. Results show that Nexus\nArchitect consistently outperforms existing solutions, achieving up to a 66%\nincrease in pass rate over Gemini 2.5 Flash Preview, nearly 2.5$\\times$ against\nClaude Sonnet 4 and DeepSeek-R1, and over 3$\\times$ w.r.t. Llama 4 Scout.", "AI": {"tldr": "Nexus Architect, an enhanced multi-agent system, outperforms state-of-the-art LRMs on challenging logical questions by using automated workflow synthesis and prompt refinement.", "motivation": "current reasoning models frequently fail to generalize to novel, unseen problems, often resorting to memorized solutions rather than genuine inferential reasoning", "method": "an enhanced iteration of our multi-agent system framework, Nexus, equipped with a novel automated workflow synthesis mechanism", "result": "Nexus Architect consistently outperforms existing solutions, achieving up to a 66% increase in pass rate over Gemini 2.5 Flash Preview, nearly 2.5$\\times$ against Claude Sonnet 4 and DeepSeek-R1, and over 3$\\times$ w.r.t. Llama 4 Scout.", "conclusion": "Nexus Architect consistently outperforms existing solutions, achieving up to a 66% increase in pass rate over Gemini 2.5 Flash Preview, nearly 2.5$\\times$ against Claude Sonnet 4 and DeepSeek-R1, and over 3$\\times$ w.r.t. Llama 4 Scout."}}
{"id": "2507.15245", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15245", "abs": "https://arxiv.org/abs/2507.15245", "authors": ["Xiaofeng Shi", "Yuduo Li", "Qian Kou", "Longbin Yu", "Jinxin Xie", "Hua Zhou"], "title": "SPAR: Scholar Paper Retrieval with LLM-based Agents for Enhanced Academic Search", "comment": null, "summary": "Recent advances in large language models (LLMs) have opened new opportunities\nfor academic literature retrieval. However, existing systems often rely on\nrigid pipelines and exhibit limited reasoning capabilities. We introduce SPAR,\na multi-agent framework that incorporates RefChain-based query decomposition\nand query evolution to enable more flexible and effective search. To facilitate\nsystematic evaluation, we also construct SPARBench, a challenging benchmark\nwith expert-annotated relevance labels. Experimental results demonstrate that\nSPAR substantially outperforms strong baselines, achieving up to +56% F1 on\nAutoScholar and +23% F1 on SPARBench over the best-performing baseline.\nTogether, SPAR and SPARBench provide a scalable, interpretable, and\nhigh-performing foundation for advancing research in scholarly retrieval. Code\nand data will be available at: https://github.com/xiaofengShi/SPAR", "AI": {"tldr": "SPAR, a multi-agent framework, outperforms existing systems in academic literature retrieval with significant F1 score improvements on AutoScholar and SPARBench.", "motivation": "existing systems often rely on rigid pipelines and exhibit limited reasoning capabilities.", "method": "a multi-agent framework that incorporates RefChain-based query decomposition and query evolution", "result": "SPAR substantially outperforms strong baselines, achieving up to +56% F1 on AutoScholar and +23% F1 on SPARBench over the best-performing baseline.", "conclusion": "SPAR and SPARBench provide a scalable, interpretable, and high-performing foundation for advancing research in scholarly retrieval."}}
{"id": "2507.14240", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14240", "abs": "https://arxiv.org/abs/2507.14240", "authors": ["Mohammad Shahedur Rahman", "Peng Gao", "Yuede Ji"], "title": "HuggingGraph: Understanding the Supply Chain of LLM Ecosystem", "comment": "10 pages, 5 figures", "summary": "Large language models (LLMs) leverage deep learning to process and predict\nsequences of words from context, enabling them to perform various NLP tasks,\nsuch as translation, summarization, question answering, and content generation.\nHowever, the growing size and complexity of developing, training, and deploying\nadvanced LLMs require extensive computational resources and large datasets.\nThis creates a barrier for users. As a result, platforms that host models and\ndatasets are widely used. For example, Hugging Face, one of the most popular\nplatforms, hosted 1.8 million models and 450K datasets by June 2025, with no\nsign of slowing down. Since many LLMs are built from base models, pre-trained\nmodels, and external datasets, they can inherit vulnerabilities, biases, or\nmalicious components from earlier models or datasets. Therefore, it is critical\nto understand the origin and development of these components to better detect\npotential risks, improve model fairness, and ensure compliance. Motivated by\nthis, our project aims to study the relationships between models and datasets,\nwhich are core components of the LLM supply chain. First, we design a method to\nsystematically collect LLM supply chain data. Using this data, we build a\ndirected heterogeneous graph to model the relationships between models and\ndatasets, resulting in a structure with 397,376 nodes and 453,469 edges. We\nthen perform various analyses and uncover several findings, such as: (i) the\nLLM supply chain graph is large, sparse, and follows a power-law degree\ndistribution; (ii) it features a densely connected core and a fragmented\nperiphery; (iii) datasets play pivotal roles in training; (iv) strong\ninterdependence exists between models and datasets; and (v) the graph is\ndynamic, with daily updates reflecting the ecosystem's ongoing evolution.", "AI": {"tldr": "This paper studies the relationships between models and datasets in the LLM supply chain by building a graph and analyzing its structure and dynamics.", "motivation": "It is critical to understand the origin and development of LLM components to better detect potential risks, improve model fairness, and ensure compliance.", "method": "A method to systematically collect LLM supply chain data to build a directed heterogeneous graph to model the relationships between models and datasets.", "result": "A directed heterogeneous graph with 397,376 nodes and 453,469 edges, and several findings about the LLM supply chain.", "conclusion": "The LLM supply chain graph is large, sparse, follows a power-law degree distribution, features a densely connected core and a fragmented periphery, datasets play pivotal roles in training, strong interdependence exists between models and datasets, and the graph is dynamic, with daily updates reflecting the ecosystem's ongoing evolution."}}
{"id": "2507.15336", "categories": ["cs.LG", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2507.15336", "abs": "https://arxiv.org/abs/2507.15336", "authors": ["Jialiang Wang", "Hanmo Liu", "Shimin Di", "Zhili Wang", "Jiachuan Wang", "Lei Chen", "Xiaofang Zhou"], "title": "Beyond Model Base Selection: Weaving Knowledge to Master Fine-grained Neural Network Design", "comment": null, "summary": "Database systems have recently advocated for embedding machine learning (ML)\ncapabilities, offering declarative model queries over large, managed model\nrepositories, thereby circumventing the huge computational overhead of\ntraditional ML-based algorithms in automated neural network model selection.\nPioneering database studies aim to organize existing benchmark repositories as\nmodel bases (MB), querying them for the model records with the highest\nperformance estimation metrics for given tasks. However, this static model\nselection practice overlooks the fine-grained, evolving relational dependencies\nbetween diverse task queries and model architecture variations, resulting in\nsuboptimal matches and failing to further refine the model effectively. To fill\nthe model refinement gap in database research, we propose M-DESIGN, a curated\nmodel knowledge base (MKB) pipeline for mastering neural network refinement by\nadaptively weaving prior insights about model architecture modification. First,\nwe propose a knowledge weaving engine that reframes model refinement as an\nadaptive query problem over task metadata. Given a user's task query, M-DESIGN\nquickly matches and iteratively refines candidate models by leveraging a\ngraph-relational knowledge schema that explicitly encodes data properties,\narchitecture variations, and pairwise performance deltas as joinable relations.\nThis schema supports fine-grained relational analytics over architecture tweaks\nand drives a predictive query planner that can detect and adapt to\nout-of-distribution (OOD) tasks. We instantiate M-DESIGN for graph analytics\ntasks, where our model knowledge base enriches existing benchmarks with\nstructured metadata covering 3 graph tasks and 22 graph datasets, contributing\ndata records of 67,760 graph models. Empirical results demonstrate that\nM-DESIGN delivers the optimal model in 26 of 33 data-task pairs within limited\nbudgets.", "AI": {"tldr": "M-DESIGN \u901a\u8fc7\u5229\u7528\u56fe\u5173\u7cfb\u77e5\u8bc6\u6a21\u5f0f\uff0c\u5feb\u901f\u5339\u914d\u548c\u8fed\u4ee3\u5730\u6539\u8fdb\u5019\u9009\u6a21\u578b\uff0c\u4ece\u800c\u6539\u8fdb\u795e\u7ecf\u7f51\u7edc\u3002", "motivation": "\u9759\u6001\u6a21\u578b\u9009\u62e9\u5b9e\u8df5\u5ffd\u7565\u4e86\u4e0d\u540c\u4efb\u52a1\u67e5\u8be2\u548c\u6a21\u578b\u67b6\u6784\u53d8\u4f53\u4e4b\u95f4\u7ec6\u7c92\u5ea6\u7684\u3001\u4e0d\u65ad\u53d1\u5c55\u7684\u5173\u7cfb\u4f9d\u8d56\u6027\uff0c\u5bfc\u81f4\u6b21\u4f18\u5339\u914d\uff0c\u5e76\u4e14\u672a\u80fd\u8fdb\u4e00\u6b65\u6709\u6548\u5730\u6539\u8fdb\u6a21\u578b\u3002", "method": "M-DESIGN\uff0c\u4e00\u4e2a\u7528\u4e8e\u638c\u63e1\u795e\u7ecf\u7f51\u7edc\u6539\u8fdb\u7684\u7cbe\u9009\u6a21\u578b\u77e5\u8bc6\u5e93 (MKB) \u7ba1\u9053\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5730\u7f16\u7ec7\u5173\u4e8e\u6a21\u578b\u67b6\u6784\u4fee\u6539\u7684\u5148\u524d\u89c1\u89e3\u3002", "result": "M-DESIGN \u4ea4\u4ed8\u4e86 67,760 \u4e2a\u56fe\u6a21\u578b\u7684\u6700\u4f73\u6a21\u578b\u3002", "conclusion": "M-DESIGN \u5728\u6709\u9650\u7684\u9884\u7b97\u5185\uff0c\u5728 33 \u4e2a\u6570\u636e-\u4efb\u52a1\u5bf9\u4e2d\u7684 26 \u4e2a\u4e2d\u63d0\u4f9b\u4e86\u6700\u4f73\u6a21\u578b\u3002"}}
{"id": "2507.14432", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.14432", "abs": "https://arxiv.org/abs/2507.14432", "authors": ["Han Gong", "Qiyue Li", "Zhi Liu", "Hao Zhou", "Peng Yuan Zhou", "Zhu Li", "Jie Li"], "title": "Adaptive 3D Gaussian Splatting Video Streaming", "comment": null, "summary": "The advent of 3D Gaussian splatting (3DGS) has significantly enhanced the\nquality of volumetric video representation. Meanwhile, in contrast to\nconventional volumetric video, 3DGS video poses significant challenges for\nstreaming due to its substantially larger data volume and the heightened\ncomplexity involved in compression and transmission. To address these issues,\nwe introduce an innovative framework for 3DGS volumetric video streaming.\nSpecifically, we design a 3DGS video construction method based on the Gaussian\ndeformation field. By employing hybrid saliency tiling and differentiated\nquality modeling of 3DGS video, we achieve efficient data compression and\nadaptation to bandwidth fluctuations while ensuring high transmission quality.\nThen we build a complete 3DGS video streaming system and validate the\ntransmission performance. Through experimental evaluation, our method\ndemonstrated superiority over existing approaches in various aspects, including\nvideo quality, compression effectiveness, and transmission rate.", "AI": {"tldr": "This paper introduces a 3DGS video streaming framework using Gaussian deformation fields, hybrid saliency tiling, and differentiated quality modeling to improve video quality, compression, and transmission rate compared to existing methods.", "motivation": "3D Gaussian splatting (3DGS) enhances volumetric video quality, but its large data volume and compression complexity pose streaming challenges compared to conventional volumetric video.", "method": "A 3DGS video construction method based on Gaussian deformation field is designed, employing hybrid saliency tiling and differentiated quality modeling.", "result": "The proposed method demonstrates superiority over existing approaches in video quality, compression effectiveness, and transmission rate.", "conclusion": "The proposed 3DGS video streaming framework outperforms existing methods in video quality, compression, and transmission rate."}}
{"id": "2507.14179", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.14179", "abs": "https://arxiv.org/abs/2507.14179", "authors": ["Nobel Dhar", "Bobin Deng", "Md Romyull Islam", "Xinyue Zhang", "Kazi Fahim Ahmad Nasif", "Kun Suo"], "title": "A Sparsity Predicting Approach for Large Language Models via Activation Pattern Clustering", "comment": "To be published in Euro-Par 2025", "summary": "Large Language Models (LLMs) exhibit significant activation sparsity, where\nonly a subset of neurons are active for a given input. Although this sparsity\npresents opportunities to reduce computational cost, efficiently utilizing it\nrequires predicting activation patterns in a scalable manner. However, direct\nprediction at the neuron level is computationally expensive due to the vast\nnumber of neurons in modern LLMs. To enable efficient prediction and\nutilization of activation sparsity, we propose a clustering-based activation\npattern compression framework. Instead of treating each neuron independently,\nwe group similar activation patterns into a small set of representative\nclusters. Our method achieves up to 79.34% clustering precision, outperforming\nstandard binary clustering approaches while maintaining minimal degradation in\nperplexity (PPL) scores. With a sufficiently large number of clusters, our\napproach attains a PPL score as low as 12.49, demonstrating its effectiveness\nin preserving model quality while reducing computational overhead. By\npredicting cluster assignments rather than individual neuron states, future\nmodels can efficiently infer activation patterns from pre-computed centroids.\nWe detail the clustering algorithm, analyze its effectiveness in capturing\nmeaningful activation structures, and demonstrate its potential to improve\nsparse computation efficiency. This clustering-based formulation serves as a\nfoundation for future work on activation pattern prediction, paving the way for\nefficient inference in large-scale language models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u805a\u7c7b\u7684\u6fc0\u6d3b\u6a21\u5f0f\u538b\u7f29\u6846\u67b6\uff0c\u7528\u4e8e\u6709\u6548\u9884\u6d4b\u548c\u5229\u7528 LLM \u4e2d\u7684\u6fc0\u6d3b\u7a00\u758f\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u805a\u7c7b\u7cbe\u5ea6\u548c\u4f4e\u56f0\u60d1\u5ea6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u8868\u73b0\u51fa\u663e\u7740\u7684\u6fc0\u6d3b\u7a00\u758f\u6027\uff0c\u5176\u4e2d\u53ea\u6709\u4e00\u90e8\u5206\u795e\u7ecf\u5143\u5bf9\u7ed9\u5b9a\u7684\u8f93\u5165\u5904\u4e8e\u6d3b\u52a8\u72b6\u6001\u3002\u867d\u7136\u8fd9\u79cd\u7a00\u758f\u6027\u63d0\u4f9b\u4e86\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u673a\u4f1a\uff0c\u4f46\u6709\u6548\u5229\u7528\u5b83\u9700\u8981\u5728\u53ef\u6269\u5c55\u7684\u65b9\u5f0f\u4e2d\u9884\u6d4b\u6fc0\u6d3b\u6a21\u5f0f\u3002\u7136\u800c\uff0c\u7531\u4e8e\u73b0\u4ee3 LLM \u4e2d\u795e\u7ecf\u5143\u6570\u91cf\u5e9e\u5927\uff0c\u795e\u7ecf\u5143\u7ea7\u522b\u7684\u76f4\u63a5\u9884\u6d4b\u8ba1\u7b97\u6210\u672c\u5f88\u9ad8\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u805a\u7c7b\u7684\u6fc0\u6d3b\u6a21\u5f0f\u538b\u7f29\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u76f8\u4f3c\u7684\u6fc0\u6d3b\u6a21\u5f0f\u5206\u7ec4\u5230\u4e00\u5c0f\u7ec4\u4ee3\u8868\u6027\u805a\u7c7b\u4e2d\uff0c\u800c\u4e0d\u662f\u72ec\u7acb\u5730\u5904\u7406\u6bcf\u4e2a\u795e\u7ecf\u5143\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u8fbe 79.34% \u7684\u805a\u7c7b\u7cbe\u5ea6\uff0c\u4f18\u4e8e\u6807\u51c6\u4e8c\u5143\u805a\u7c7b\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6700\u5c0f\u7684\u56f0\u60d1\u5ea6 (PPL) \u5206\u6570\u4e0b\u964d\u3002\u901a\u8fc7\u8db3\u591f\u5927\u6570\u91cf\u7684\u96c6\u7fa4\uff0c\u8be5\u65b9\u6cd5\u83b7\u5f97\u4e86\u4f4e\u81f3 12.49 \u7684 PPL \u5206\u6570\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u4fdd\u6301\u6a21\u578b\u8d28\u91cf\u7684\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u9884\u6d4b\u805a\u7c7b\u5206\u914d\u800c\u4e0d\u662f\u5355\u72ec\u7684\u795e\u7ecf\u5143\u72b6\u6001\uff0c\u672a\u6765\u7684\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u5730\u4ece\u9884\u5148\u8ba1\u7b97\u7684\u8d28\u5fc3\u4e2d\u63a8\u65ad\u6fc0\u6d3b\u6a21\u5f0f\u3002\u8fd9\u79cd\u57fa\u4e8e\u805a\u7c7b\u7684\u516c\u5f0f\u4e3a\u672a\u6765\u6fc0\u6d3b\u6a21\u5f0f\u9884\u6d4b\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u9ad8\u6548\u63a8\u7406\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2507.14406", "categories": ["cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.14406", "abs": "https://arxiv.org/abs/2507.14406", "authors": ["Michael J. Zellinger", "Matt Thomson"], "title": "Fail Fast, or Ask: Mitigating the Deficiencies of Reasoning LLMs with Human-in-the-Loop Systems Engineering", "comment": "8 pages, 5 figures", "summary": "State-of-the-art reasoning LLMs are powerful problem solvers, but they still\noccasionally make mistakes. However, adopting AI models in risk-sensitive\ndomains often requires error rates near 0%. To address this gap, we propose\ncollaboration between a reasoning model and a human expert who resolves queries\nthe model cannot confidently answer. We find that quantifying the uncertainty\nof a reasoning model through the length of its reasoning trace yields an\neffective basis for deferral to a human, e.g., cutting the error rate of Qwen3\n235B-A22B on difficult MATH problems from 3% to less than 1% when deferring\n7.5% of queries. However, the high latency of reasoning models still makes them\nchallenging to deploy on use cases with high query volume. To address this\nchallenge, we explore fronting a reasoning model with a large non-reasoning\nmodel. We call this modified human-in-the-loop system \"Fail Fast, or Ask\",\nsince the non-reasoning model may defer difficult queries to the human expert\ndirectly (\"failing fast\"), without incurring the reasoning model's higher\nlatency. We show that this approach yields around 40% latency reduction and\nabout 50% cost savings for DeepSeek R1 while maintaining 90+% area under the\naccuracy-rejection curve. However, we observe that latency savings are lower\nthan expected because of \"latency drag\", the phenomenon that processing easier\nqueries with a non-reasoning model pushes the reasoning model's latency\ndistribution towards longer latencies. Broadly, our results suggest that the\ndeficiencies of state-of-the-art reasoning models -- nontrivial error rates and\nhigh latency -- can be substantially mitigated through black-box systems\nengineering, without requiring access to LLM internals.", "AI": {"tldr": "Mitigating error rates and high latency of reasoning models through collaboration between a reasoning model and a human expert and fronting a reasoning model with a large non-reasoning model.", "motivation": "Adopting AI models in risk-sensitive domains often requires error rates near 0%, but state-of-the-art reasoning LLMs still occasionally make mistakes and have high latency.", "method": "Propose collaboration between a reasoning model and a human expert who resolves queries the model cannot confidently answer, and explore fronting a reasoning model with a large non-reasoning model.", "result": "Quantifying the uncertainty of a reasoning model through the length of its reasoning trace yields an effective basis for deferral to a human, cutting the error rate of Qwen3 235B-A22B on difficult MATH problems from 3% to less than 1% when deferring 7.5% of queries. This approach yields around 40% latency reduction and about 50% cost savings for DeepSeek R1 while maintaining 90+% area under the accuracy-rejection curve. However, latency savings are lower than expected because of latency drag.", "conclusion": "The deficiencies of state-of-the-art reasoning models can be substantially mitigated through black-box systems engineering, without requiring access to LLM internals."}}
{"id": "2507.15267", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15267", "abs": "https://arxiv.org/abs/2507.15267", "authors": ["Ninglu Shao", "Jinshan Wang", "Chenxu Wang", "Qingbiao Li", "Xiaoxue Zang", "Han Li"], "title": "GREAT: Guiding Query Generation with a Trie for Recommending Related Search about Video at Kuaishou", "comment": null, "summary": "Currently, short video platforms have become the primary place for\nindividuals to share experiences and obtain information. To better meet users'\nneeds for acquiring information while browsing short videos, some apps have\nintroduced a search entry at the bottom of videos, accompanied with recommended\nrelevant queries. This scenario is known as query recommendation in\nvideo-related search, where core task is item-to-query (I2Q) recommendation. As\nthis scenario has only emerged in recent years, there is a notable scarcity of\nacademic research and publicly available datasets in this domain. To address\nthis gap, we systematically examine the challenges associated with this\nscenario for the first time. Subsequently, we release a large-scale dataset\nderived from real-world data pertaining to the query recommendation in\nvideo-\\textit{\\textbf{r}}elated \\textit{\\textbf{s}}earch on the\n\\textit{\\textbf{Kuai}}shou app (\\textbf{KuaiRS}). Presently, existing methods\nrely on embeddings to calculate similarity for matching short videos with\nqueries, lacking deep interaction between the semantic content and the query.\nIn this paper, we introduce a novel LLM-based framework named \\textbf{GREAT},\nwhich \\textit{\\textbf{g}}uides que\\textit{\\textbf{r}}y\ng\\textit{\\textbf{e}}ner\\textit{\\textbf{a}}tion with a \\textit{\\textbf{t}}rie to\naddress I2Q recommendation in related search. Specifically, we initially gather\nhigh-quality queries with high exposure and click-through rate to construct a\nquery-based trie. During training, we enhance the LLM's capability to generate\nhigh-quality queries using the query-based trie. In the inference phase, the\nquery-based trie serves as a guide for the token generation. Finally, we\nfurther refine the relevance and literal quality between items and queries via\na post-processing module. Extensive offline and online experiments demonstrate\nthe effectiveness of our proposed method.", "AI": {"tldr": "This paper addresses the lack of research in video-related query recommendation by introducing GREAT, an LLM-based framework using a trie to generate better queries, and releases a new large-scale dataset.", "motivation": "There is a scarcity of academic research and publicly available datasets in the emerging field of query recommendation in video-related search (item-to-query recommendation). Existing methods lack deep interaction between semantic content and the query.", "method": "The paper introduces a novel LLM-based framework named GREAT, which guides query generation with a trie. It gathers high-quality queries to construct a query-based trie, enhances the LLM's generation capability using the trie during training, and uses the trie to guide token generation during inference. A post-processing module refines relevance and literal quality.", "result": "The paper releases a large-scale dataset (KuaiRS) derived from real-world data and demonstrates the effectiveness of the proposed GREAT method through extensive offline and online experiments.", "conclusion": "The paper introduces GREAT, a novel LLM-based framework that guides query generation with a trie to address I2Q recommendation in related search. Extensive offline and online experiments demonstrate the effectiveness of the proposed method."}}
{"id": "2507.14241", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14241", "abs": "https://arxiv.org/abs/2507.14241", "authors": ["Rithesh Murthy", "Ming Zhu", "Liangwei Yang", "Jielin Qiu", "Juntao Tan", "Shelby Heinecke", "Huan Wang", "Caiming Xiong", "Silvio Savarese"], "title": "Promptomatix: An Automatic Prompt Optimization Framework for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) perform best with well-crafted prompts, yet\nprompt engineering remains manual, inconsistent, and inaccessible to\nnon-experts. We introduce Promptomatix, an automatic prompt optimization\nframework that transforms natural language task descriptions into high-quality\nprompts without requiring manual tuning or domain expertise. Promptomatix\nsupports both a lightweight meta-prompt-based optimizer and a DSPy-powered\ncompiler, with modular design enabling future extension to more advanced\nframeworks. The system analyzes user intent, generates synthetic training data,\nselects prompting strategies, and refines prompts using cost-aware objectives.\nEvaluated across 5 task categories, Promptomatix achieves competitive or\nsuperior performance compared to existing libraries, while reducing prompt\nlength and computational overhead making prompt optimization scalable and\nefficient.", "AI": {"tldr": "Promptomatix is an automatic prompt optimization framework that transforms natural language task descriptions into high-quality prompts without manual tuning, achieving better performance with reduced cost.", "motivation": "prompt engineering remains manual, inconsistent, and inaccessible to non-experts", "method": "an automatic prompt optimization framework that transforms natural language task descriptions into high-quality prompts", "result": "achieves competitive or superior performance compared to existing libraries, while reducing prompt length and computational overhead making prompt optimization scalable and efficient", "conclusion": "Promptomatix achieves competitive or superior performance compared to existing libraries, while reducing prompt length and computational overhead."}}
{"id": "2507.14449", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14449", "abs": "https://arxiv.org/abs/2507.14449", "authors": ["Zhe Cao", "Jin Zhang", "Ruiheng Zhang"], "title": "IRGPT: Understanding Real-world Infrared Image with Bi-cross-modal Curriculum on Large-scale Benchmark", "comment": "11 pages, 7 figures. This paper is accepted by ICCV 2025", "summary": "Real-world infrared imagery presents unique challenges for vision-language\nmodels due to the scarcity of aligned text data and domain-specific\ncharacteristics. Although existing methods have advanced the field, their\nreliance on synthetic infrared images generated through style transfer from\nvisible images, which limits their ability to capture the unique\ncharacteristics of the infrared modality. To address this, we propose IRGPT,\nthe first multi-modal large language model for real-world infrared images,\nbuilt upon a large-scale InfraRed-Text Dataset (IR-TD) comprising over 260K\nauthentic image-text pairs. The proposed IR-TD dataset contains real infrared\nimages paired with meticulously handcrafted texts, where the initial drafts\noriginated from two complementary processes: (1) LLM-generated descriptions of\nvisible images, and (2) rule-based descriptions of annotations. Furthermore, we\nintroduce a bi-cross-modal curriculum transfer learning strategy that\nsystematically transfers knowledge from visible to infrared domains by\nconsidering the difficulty scores of both infrared-visible and infrared-text.\nEvaluated on a benchmark of 9 tasks (e.g., recognition, grounding), IRGPT\nachieves state-of-the-art performance even compared with larger-scale models.", "AI": {"tldr": "IRGPT\u662f\u9996\u4e2a\u7528\u4e8e\u771f\u5b9e\u7ea2\u5916\u56fe\u50cf\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5b83\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u56e0\u4e3a\u5b83\u662f\u5728\u4e00\u4e2a\u5305\u542b\u8d85\u8fc726\u4e07\u4e2a\u771f\u5b9e\u56fe\u50cf-\u6587\u672c\u5bf9\u7684\u5927\u89c4\u6a21\u7ea2\u5916-\u6587\u672c\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u7ea2\u5916\u56fe\u50cf\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5e26\u6765\u4e86\u72ec\u7279\u7684\u6311\u6218\uff0c\u56e0\u4e3a\u5bf9\u9f50\u7684\u6587\u672c\u6570\u636e\u7a00\u7f3a\u4e14\u5177\u6709\u7279\u5b9a\u9886\u57df\u7684\u7279\u5f81\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u4ece\u53ef\u89c1\u56fe\u50cf\u901a\u8fc7\u98ce\u683c\u8fc1\u79fb\u751f\u6210\u7684\u5408\u6210\u7ea2\u5916\u56fe\u50cf\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u6355\u6349\u7ea2\u5916\u6a21\u6001\u72ec\u7279\u7279\u5f81\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86IRGPT\uff0c\u9996\u4e2a\u7528\u4e8e\u771f\u5b9e\u7ea2\u5916\u56fe\u50cf\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5b83\u57fa\u4e8e\u4e00\u4e2a\u5305\u542b\u8d85\u8fc726\u4e07\u4e2a\u771f\u5b9e\u56fe\u50cf-\u6587\u672c\u5bf9\u7684\u5927\u89c4\u6a21\u7ea2\u5916-\u6587\u672c\u6570\u636e\u96c6\uff08IR-TD\uff09\u3002", "result": "IRGPT\u5728\u4e5d\u9879\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "IRGPT\u5728\u7ea2\u5916\u56fe\u50cf\u76f8\u5173\u7684\u4e5d\u9879\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u751a\u81f3\u4f18\u4e8e\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\u3002"}}
{"id": "2507.14180", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14180", "abs": "https://arxiv.org/abs/2507.14180", "authors": ["Nasir Khan", "Asmaa Abdallah", "Abdulkadir Celik", "Ahmed M. Eltawil", "Sinem Coleri"], "title": "Digital Twin-Assisted Explainable AI for Robust Beam Prediction in mmWave MIMO Systems", "comment": null, "summary": "In line with the AI-native 6G vision, explainability and robustness are\ncrucial for building trust and ensuring reliable performance in millimeter-wave\n(mmWave) systems. Efficient beam alignment is essential for initial access, but\ndeep learning (DL) solutions face challenges, including high data collection\noverhead, hardware constraints, lack of explainability, and susceptibility to\nadversarial attacks. This paper proposes a robust and explainable DL-based beam\nalignment engine (BAE) for mmWave multiple-input multiple output (MIMO)\nsystems. The BAE uses received signal strength indicator (RSSI) measurements\nfrom wide beams to predict the best narrow beam, reducing the overhead of\nexhaustive beam sweeping. To overcome the challenge of real-world data\ncollection, this work leverages a site-specific digital twin (DT) to generate\nsynthetic channel data closely resembling real-world environments. A model\nrefinement via transfer learning is proposed to fine-tune the pre-trained model\nresiding in the DT with minimal real-world data, effectively bridging\nmismatches between the digital replica and real-world environments. To reduce\nbeam training overhead and enhance transparency, the framework uses deep\nShapley additive explanations (SHAP) to rank input features by importance,\nprioritizing key spatial directions and minimizing beam sweeping. It also\nincorporates the Deep k-nearest neighbors (DkNN) algorithm, providing a\ncredibility metric for detecting out-of-distribution inputs and ensuring\nrobust, transparent decision-making. Experimental results show that the\nproposed framework reduces real-world data needs by 70%, beam training overhead\nby 62%, and improves outlier detection robustness by up to 8.5x, achieving\nnear-optimal spectral efficiency and transparent decision making compared to\ntraditional softmax based DL models.", "AI": {"tldr": "This paper proposes a robust and explainable DL-based beam alignment engine (BAE) for mmWave MIMO systems, using a site-specific digital twin (DT) and transfer learning to reduce data needs and beam training overhead, while enhancing transparency and outlier detection.", "motivation": "Explainability and robustness are crucial for building trust and ensuring reliable performance in millimeter-wave (mmWave) systems. Efficient beam alignment is essential for initial access, but deep learning (DL) solutions face challenges, including high data collection overhead, hardware constraints, lack of explainability, and susceptibility to adversarial attacks.", "method": "The BAE uses received signal strength indicator (RSSI) measurements from wide beams to predict the best narrow beam, reducing the overhead of exhaustive beam sweeping. A model refinement via transfer learning is proposed to fine-tune the pre-trained model residing in the DT with minimal real-world data. The framework uses deep Shapley additive explanations (SHAP) to rank input features by importance, prioritizing key spatial directions and minimizing beam sweeping. It also incorporates the Deep k-nearest neighbors (DkNN) algorithm, providing a credibility metric for detecting out-of-distribution inputs.", "result": "The proposed framework reduces real-world data needs by 70%, beam training overhead by 62%, and improves outlier detection robustness by up to 8.5x.", "conclusion": "The proposed framework reduces real-world data needs by 70%, beam training overhead by 62%, and improves outlier detection robustness by up to 8.5x, achieving near-optimal spectral efficiency and transparent decision making compared to traditional softmax based DL models."}}
{"id": "2507.14417", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14417", "abs": "https://arxiv.org/abs/2507.14417", "authors": ["Aryo Pradipta Gema", "Alexander H\u00e4gele", "Runjin Chen", "Andy Arditi", "Jacob Goldman-Wetzler", "Kit Fraser-Taliente", "Henry Sleight", "Linda Petrini", "Julian Michael", "Beatrice Alex", "Pasquale Minervini", "Yanda Chen", "Joe Benton", "Ethan Perez"], "title": "Inverse Scaling in Test-Time Compute", "comment": null, "summary": "We construct evaluation tasks where extending the reasoning length of Large\nReasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling\nrelationship between test-time compute and accuracy. Our evaluation tasks span\nfour categories: simple counting tasks with distractors, regression tasks with\nspurious features, deduction tasks with constraint tracking, and advanced AI\nrisks. We identify five distinct failure modes when models reason for longer:\n1) Claude models become increasingly distracted by irrelevant information; 2)\nOpenAI o-series models resist distractors but overfit to problem framings; 3)\nmodels shift from reasonable priors to spurious correlations; 4) all models\nshow difficulties in maintaining focus on complex deductive tasks; and 5)\nextended reasoning may amplify concerning behaviors, with Claude Sonnet 4\nshowing increased expressions of self-preservation. These findings suggest that\nwhile test-time compute scaling remains promising for improving model\ncapabilities, it may inadvertently reinforce problematic reasoning patterns.\nOur results demonstrate the importance of evaluating models across diverse\nreasoning lengths to identify and address these failure modes in LRMs.", "AI": {"tldr": "Extending reasoning length in LRMs can worsen performance and amplify problematic reasoning patterns.", "motivation": "Extending the reasoning length of Large Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling relationship between test-time compute and accuracy.", "method": "Evaluation tasks span four categories: simple counting tasks with distractors, regression tasks with spurious features, deduction tasks with constraint tracking, and advanced AI risks.", "result": "Five distinct failure modes when models reason for longer.", "conclusion": "Test-time compute scaling may reinforce problematic reasoning patterns. Evaluating models across diverse reasoning lengths is important to identify and address these failure modes in LRMs."}}
{"id": "2507.15395", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.15395", "abs": "https://arxiv.org/abs/2507.15395", "authors": ["Hengyu Zhang", "Chunxu Shen", "Xiangguo Sun", "Jie Tan", "Yanchao Tan", "Yu Rong", "Hong Cheng", "Lingling Yi"], "title": "Hierarchical Graph Information Bottleneck for Multi-Behavior Recommendation", "comment": "Accepted by RecSys2025", "summary": "In real-world recommendation scenarios, users typically engage with platforms\nthrough multiple types of behavioral interactions. Multi-behavior\nrecommendation algorithms aim to leverage various auxiliary user behaviors to\nenhance prediction for target behaviors of primary interest (e.g., buy),\nthereby overcoming performance limitations caused by data sparsity in target\nbehavior records. Current state-of-the-art approaches typically employ\nhierarchical design following either cascading (e.g.,\nview$\\rightarrow$cart$\\rightarrow$buy) or parallel\n(unified$\\rightarrow$behavior$\\rightarrow$specific components) paradigms, to\ncapture behavioral relationships. However, these methods still face two\ncritical challenges: (1) severe distribution disparities across behaviors, and\n(2) negative transfer effects caused by noise in auxiliary behaviors. In this\npaper, we propose a novel model-agnostic Hierarchical Graph Information\nBottleneck (HGIB) framework for multi-behavior recommendation to effectively\naddress these challenges. Following information bottleneck principles, our\nframework optimizes the learning of compact yet sufficient representations that\npreserve essential information for target behavior prediction while eliminating\ntask-irrelevant redundancies. To further mitigate interaction noise, we\nintroduce a Graph Refinement Encoder (GRE) that dynamically prunes redundant\nedges through learnable edge dropout mechanisms. We conduct comprehensive\nexperiments on three real-world public datasets, which demonstrate the superior\neffectiveness of our framework. Beyond these widely used datasets in the\nacademic community, we further expand our evaluation on several real industrial\nscenarios and conduct an online A/B testing, showing again a significant\nimprovement in multi-behavior recommendations. The source code of our proposed\nHGIB is available at https://github.com/zhy99426/HGIB.", "AI": {"tldr": "This paper introduces HGIB, a Hierarchical Graph Information Bottleneck framework for multi-behavior recommendation, addressing distribution disparities and negative transfer effects. It outperforms existing methods on public datasets and in real-world industrial scenarios.", "motivation": "Current multi-behavior recommendation algorithms face challenges including severe distribution disparities across behaviors and negative transfer effects caused by noise in auxiliary behaviors.", "method": "The paper proposes a novel model-agnostic Hierarchical Graph Information Bottleneck (HGIB) framework for multi-behavior recommendation, which optimizes the learning of compact representations and introduces a Graph Refinement Encoder (GRE) to dynamically prune redundant edges.", "result": "The proposed HGIB framework achieves superior performance on three real-world public datasets, real industrial scenarios, and online A/B testing, demonstrating a significant improvement in multi-behavior recommendations.", "conclusion": "The paper demonstrates the superior effectiveness of the proposed HGIB framework through comprehensive experiments on three real-world public datasets, real industrial scenarios, and online A/B testing, showing a significant improvement in multi-behavior recommendations."}}
{"id": "2507.14298", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14298", "abs": "https://arxiv.org/abs/2507.14298", "authors": ["Wan-Cyuan Fan", "Yen-Chun Chen", "Mengchen Liu", "Alexander Jacobson", "Lu Yuan", "Leonid Sigal"], "title": "In-Depth and In-Breadth: Pre-training Multimodal Language Models Customized for Comprehensive Chart Understanding", "comment": "arXiv admin note: substantial text overlap with arXiv:2407.14506", "summary": "Recent methods for customizing Large Vision Language Models (LVLMs) for\ndomain-specific tasks have shown promising results in scientific chart\ncomprehension. However, existing approaches face two major limitations: First,\nthey rely on paired data from only a few chart types, limiting generalization\nto wide range of chart types. Secondly, they lack targeted pre-training for\nchart-data alignment, which hampers the model's understanding of underlying\ndata. In this paper, we introduce ChartScope, an LVLM optimized for in-depth\nchart comprehension across diverse chart types. We propose an efficient data\ngeneration pipeline that synthesizes paired data for a wide range of chart\ntypes, along with a novel Dual-Path training strategy that enabling the model\nto succinctly capture essential data details while preserving robust reasoning\ncapabilities by incorporating reasoning over the underlying data. Lastly, we\nestablish ChartDQA, a new benchmark for evaluating not only question-answering\nat different levels but also underlying data understanding. Experimental\nresults demonstrate that ChartScope significantly enhances comprehension on a\nwide range of chart types. The code and data are available at\nhttps://davidhalladay.github.io/chartscope_demo.", "AI": {"tldr": "ChartScope\u662f\u4e00\u79cdLVLM\uff0c\u9488\u5bf9\u5404\u79cd\u56fe\u8868\u7c7b\u578b\u7684\u6df1\u5165\u56fe\u8868\u7406\u89e3\u8fdb\u884c\u4e86\u4f18\u5316\u3002", "motivation": "\u7528\u4e8e\u5b9a\u5236\u7279\u5b9a\u9886\u57df\u4efb\u52a1\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u7684\u6700\u65b0\u65b9\u6cd5\u5728\u79d1\u5b66\u56fe\u8868\u7406\u89e3\u65b9\u9762\u663e\u793a\u51fa\u53ef\u559c\u7684\u7ed3\u679c\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u9650\u5236\uff1a\u9996\u5148\uff0c\u5b83\u4eec\u4f9d\u8d56\u4e8e\u4ec5\u6765\u81ea\u5c11\u6570\u56fe\u8868\u7c7b\u578b\u7684\u914d\u5bf9\u6570\u636e\uff0c\u9650\u5236\u4e86\u5bf9\u5404\u79cd\u56fe\u8868\u7c7b\u578b\u7684\u6cdb\u5316\u3002\u5176\u6b21\uff0c\u5b83\u4eec\u7f3a\u4e4f\u9488\u5bf9\u56fe\u8868\u6570\u636e\u5bf9\u9f50\u7684\u9488\u5bf9\u6027\u9884\u8bad\u7ec3\uff0c\u8fd9\u963b\u788d\u4e86\u6a21\u578b\u5bf9\u5e95\u5c42\u6570\u636e\u7684\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u8be5\u7ba1\u9053\u5408\u6210\u4e86\u5404\u79cd\u56fe\u8868\u7c7b\u578b\u7684\u914d\u5bf9\u6570\u636e\uff0c\u4ee5\u53ca\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u8def\u5f84\u8bad\u7ec3\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u4f7f\u6a21\u578b\u80fd\u591f\u7b80\u6d01\u5730\u6355\u83b7\u57fa\u672c\u6570\u636e\u7ec6\u8282\uff0c\u540c\u65f6\u901a\u8fc7\u7ed3\u5408\u5bf9\u5e95\u5c42\u6570\u636e\u7684\u63a8\u7406\u6765\u4fdd\u6301\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6ChartDQA\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e0d\u540c\u7ea7\u522b\u7684\u95ee\u7b54\u4ee5\u53ca\u5e95\u5c42\u6570\u636e\u7406\u89e3\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cChartScope\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u5404\u79cd\u56fe\u8868\u7c7b\u578b\u7684\u7406\u89e3\u3002", "conclusion": "ChartScope\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u5404\u79cd\u56fe\u8868\u7c7b\u578b\u7684\u7406\u89e3\u3002"}}
{"id": "2507.14452", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14452", "abs": "https://arxiv.org/abs/2507.14452", "authors": ["Weikang Gu", "Mingyue Han", "Li Xue", "Heng Dong", "Changcai Yang", "Riqing Chen", "Lifang Wei"], "title": "GPI-Net: Gestalt-Guided Parallel Interaction Network via Orthogonal Geometric Consistency for Robust Point Cloud Registration", "comment": "9 pages, 4 figures. Accepted to IJCAI 2025", "summary": "The accurate identification of high-quality correspondences is a prerequisite\ntask in feature-based point cloud registration. However, it is extremely\nchallenging to handle the fusion of local and global features due to feature\nredundancy and complex spatial relationships. Given that Gestalt principles\nprovide key advantages in analyzing local and global relationships, we propose\na novel Gestalt-guided Parallel Interaction Network via orthogonal geometric\nconsistency (GPI-Net) in this paper. It utilizes Gestalt principles to\nfacilitate complementary communication between local and global information.\nSpecifically, we introduce an orthogonal integration strategy to optimally\nreduce redundant information and generate a more compact global structure for\nhigh-quality correspondences. To capture geometric features in correspondences,\nwe leverage a Gestalt Feature Attention (GFA) block through a hybrid\nutilization of self-attention and cross-attention mechanisms. Furthermore, to\nfacilitate the integration of local detail information into the global\nstructure, we design an innovative Dual-path Multi-Granularity parallel\ninteraction aggregation (DMG) block to promote information exchange across\ndifferent granularities. Extensive experiments on various challenging tasks\ndemonstrate the superior performance of our proposed GPI-Net in comparison to\nexisting methods. The code will be released at https://github.com/gwk/GPI-Net.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u683c\u5f0f\u5854\u5f15\u5bfc\u5e76\u884c\u4ea4\u4e92\u7f51\u7edc(GPI-Net)\uff0c\u7528\u4e8e\u70b9\u4e91\u914d\u51c6\uff0c\u8be5\u7f51\u7edc\u5229\u7528\u683c\u5f0f\u5854\u539f\u5219\u4fc3\u8fdb\u5c40\u90e8\u548c\u5168\u5c40\u4fe1\u606f\u4e4b\u95f4\u7684\u4e92\u8865\u901a\u4fe1\u3002", "motivation": "\u7cbe\u786e\u8bc6\u522b\u9ad8\u8d28\u91cf\u7684\u5bf9\u5e94\u5173\u7cfb\u662f\u57fa\u4e8e\u7279\u5f81\u7684\u70b9\u4e91\u914d\u51c6\u4e2d\u7684\u4e00\u9879\u5148\u51b3\u4efb\u52a1\u3002\u7136\u800c\uff0c\u7531\u4e8e\u7279\u5f81\u5197\u4f59\u548c\u590d\u6742\u7684\u7a7a\u95f4\u5173\u7cfb\uff0c\u5904\u7406\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\u7684\u878d\u5408\u6781\u5177\u6311\u6218\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u683c\u5f0f\u5854\u5f15\u5bfc\u7684\u5e76\u884c\u4ea4\u4e92\u7f51\u7edc\uff0c\u901a\u8fc7\u6b63\u4ea4\u51e0\u4f55\u4e00\u81f4\u6027(GPI-Net)\u3002", "result": "\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u63d0\u51fa\u7684GPI-Net\u5177\u6709\u4f18\u8d8a\u7684\u6027\u80fd", "conclusion": "GPI-Net\u5728\u5404\u79cd\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2507.14181", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14181", "abs": "https://arxiv.org/abs/2507.14181", "authors": ["Yajiao Dai", "Jun Li", "Zhen Mei", "Yiyang Ni", "Shi Jin", "Zengxiang Li", "Sheng Guo", "Wei Xiang"], "title": "Semi-Supervised Federated Learning via Dual Contrastive Learning and Soft Labeling for Intelligent Fault Diagnosis", "comment": "Accepted to IEEE Internet of Things Journal, Early Access. 14 pages,\n  5 figures", "summary": "Intelligent fault diagnosis (IFD) plays a crucial role in ensuring the safe\noperation of industrial machinery and improving production efficiency. However,\ntraditional supervised deep learning methods require a large amount of training\ndata and labels, which are often located in different clients. Additionally,\nthe cost of data labeling is high, making labels difficult to acquire.\nMeanwhile, differences in data distribution among clients may also hinder the\nmodel's performance. To tackle these challenges, this paper proposes a\nsemi-supervised federated learning framework, SSFL-DCSL, which integrates dual\ncontrastive loss and soft labeling to address data and label scarcity for\ndistributed clients with few labeled samples while safeguarding user privacy.\nIt enables representation learning using unlabeled data on the client side and\nfacilitates joint learning among clients through prototypes, thereby achieving\nmutual knowledge sharing and preventing local model divergence. Specifically,\nfirst, a sample weighting function based on the Laplace distribution is\ndesigned to alleviate bias caused by low confidence in pseudo labels during the\nsemi-supervised training process. Second, a dual contrastive loss is introduced\nto mitigate model divergence caused by different data distributions, comprising\nlocal contrastive loss and global contrastive loss. Third, local prototypes are\naggregated on the server with weighted averaging and updated with momentum to\nshare knowledge among clients. To evaluate the proposed SSFL-DCSL framework,\nexperiments are conducted on two publicly available datasets and a dataset\ncollected on motors from the factory. In the most challenging task, where only\n10\\% of the data are labeled, the proposed SSFL-DCSL can improve accuracy by\n1.15% to 7.85% over state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u91cd\u5bf9\u6bd4\u635f\u5931\u548c\u8f6f\u6807\u7b7e\u6765\u89e3\u51b3\u667a\u80fd\u6545\u969c\u8bca\u65ad\u4e2d\u6570\u636e\u548c\u6807\u7b7e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u51c6\u786e\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u548c\u6807\u7b7e\uff0c\u8fd9\u4e9b\u6570\u636e\u548c\u6807\u7b7e\u901a\u5e38\u4f4d\u4e8e\u4e0d\u540c\u7684\u5ba2\u6237\u7aef\u3002\u6b64\u5916\uff0c\u6570\u636e\u6807\u8bb0\u7684\u6210\u672c\u5f88\u9ad8\uff0c\u4f7f\u5f97\u6807\u7b7e\u96be\u4ee5\u83b7\u53d6\u3002\u540c\u65f6\uff0c\u5ba2\u6237\u7aef\u4e4b\u95f4\u6570\u636e\u5206\u5e03\u7684\u5dee\u5f02\u4e5f\u53ef\u80fd\u963b\u788d\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u8054\u90a6\u5b66\u4e60\u6846\u67b6 SSFL-DCSL\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u53cc\u91cd\u5bf9\u6bd4\u635f\u5931\u548c\u8f6f\u6807\u7b7e\uff0c\u4ee5\u89e3\u51b3\u6570\u636e\u548c\u6807\u7b7e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u3002\u5b83\u652f\u6301\u4f7f\u7528\u5ba2\u6237\u7aef\u4e0a\u7684\u672a\u6807\u8bb0\u6570\u636e\u8fdb\u884c\u8868\u793a\u5b66\u4e60\uff0c\u5e76\u901a\u8fc7\u539f\u578b\u4fc3\u8fdb\u5ba2\u6237\u7aef\u4e4b\u95f4\u7684\u8054\u5408\u5b66\u4e60\uff0c\u4ece\u800c\u5b9e\u73b0\u76f8\u4e92\u77e5\u8bc6\u5171\u4eab\u5e76\u9632\u6b62\u672c\u5730\u6a21\u578b\u53d1\u6563\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u4ece\u5de5\u5382\u7535\u673a\u6536\u96c6\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002\u5728\u6700\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\u4e2d\uff0c\u53ea\u6709 10% \u7684\u6570\u636e\u88ab\u6807\u8bb0\uff0c\u6240\u63d0\u51fa\u7684 SSFL-DCSL \u53ef\u4ee5\u6bd4\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u63d0\u9ad8 1.15% \u5230 7.85% \u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u5728\u6700\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\u4e2d\uff0c\u53ea\u6709 10% \u7684\u6570\u636e\u88ab\u6807\u8bb0\uff0c\u6240\u63d0\u51fa\u7684 SSFL-DCSL \u53ef\u4ee5\u6bd4\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u63d0\u9ad8 1.15% \u5230 7.85% \u7684\u51c6\u786e\u7387\u3002"}}
{"id": "2507.14447", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14447", "abs": "https://arxiv.org/abs/2507.14447", "authors": ["Guancheng Zeng", "Xueyi Chen", "Jiawang Hu", "Shaohua Qi", "Yaxuan Mao", "Zhantao Wang", "Yifan Nie", "Shuang Li", "Qiuyang Feng", "Pengxu Qiu", "Yujia Wang", "Wenqiang Han", "Linyan Huang", "Gang Li", "Jingjing Mo", "Haowen Hu"], "title": "Routine: A Structural Planning Framework for LLM Agent System in Enterprise", "comment": "26 pages, 8 figures, 5 tables", "summary": "The deployment of agent systems in an enterprise environment is often\nhindered by several challenges: common models lack domain-specific process\nknowledge, leading to disorganized plans, missing key tools, and poor execution\nstability. To address this, this paper introduces Routine, a multi-step agent\nplanning framework designed with a clear structure, explicit instructions, and\nseamless parameter passing to guide the agent's execution module in performing\nmulti-step tool-calling tasks with high stability. In evaluations conducted\nwithin a real-world enterprise scenario, Routine significantly increases the\nexecution accuracy in model tool calls, increasing the performance of GPT-4o\nfrom 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. We further constructed\na Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an\naccuracy increase to 88.2% on scenario-specific evaluations, indicating\nimproved adherence to execution plans. In addition, we employed Routine-based\ndistillation to create a scenario-specific, multi-step tool-calling dataset.\nFine-tuning on this distilled dataset raised the model's accuracy to 95.5%,\napproaching GPT-4o's performance. These results highlight Routine's\neffectiveness in distilling domain-specific tool-usage patterns and enhancing\nmodel adaptability to new scenarios. Our experimental results demonstrate that\nRoutine provides a practical and accessible approach to building stable agent\nworkflows, accelerating the deployment and adoption of agent systems in\nenterprise environments, and advancing the technical vision of AI for Process.", "AI": {"tldr": "Routine \u662f\u4e00\u79cd\u591a\u6b65\u9aa4\u4ee3\u7406\u89c4\u5212\u6846\u67b6\uff0c\u53ef\u63d0\u9ad8\u6a21\u578b\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u5de5\u5177\u8c03\u7528\u4e2d\u7684\u6267\u884c\u51c6\u786e\u6027\u3002", "motivation": "\u4ee3\u7406\u7cfb\u7edf\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u5e38\u5e38\u53d7\u5230\u591a\u79cd\u6311\u6218\u7684\u963b\u788d\uff1a\u901a\u7528\u6a21\u578b\u7f3a\u4e4f\u7279\u5b9a\u9886\u57df\u7684\u6d41\u7a0b\u77e5\u8bc6\uff0c\u5bfc\u81f4\u8ba1\u5212\u6df7\u4e71\u3001\u7f3a\u5c11\u5173\u952e\u5de5\u5177\u548c\u6267\u884c\u7a33\u5b9a\u6027\u5dee\u3002", "method": "\u5f15\u5165\u4e86 Routine\uff0c\u4e00\u4e2a\u591a\u6b65\u9aa4\u4ee3\u7406\u89c4\u5212\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5177\u6709\u6e05\u6670\u7684\u7ed3\u6784\u3001\u660e\u786e\u7684\u6307\u4ee4\u548c\u65e0\u7f1d\u7684\u53c2\u6570\u4f20\u9012\u3002", "result": "Routine \u5c06 GPT-4o \u7684\u6027\u80fd\u4ece 41.1% \u63d0\u9ad8\u5230 96.3%\uff0c\u5c06 Qwen3-14B \u7684\u6027\u80fd\u4ece 32.6% \u63d0\u9ad8\u5230 83.3%\u3002", "conclusion": "Routine \u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u5de5\u5177\u8c03\u7528\u4e2d\u7684\u6267\u884c\u51c6\u786e\u6027\uff0c\u5e76\u4e3a\u6784\u5efa\u7a33\u5b9a\u7684\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.15551", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.15551", "abs": "https://arxiv.org/abs/2507.15551", "authors": ["Jie Zhu", "Zhifang Fan", "Xiaoxie Zhu", "Yuchen Jiang", "Hangyu Wang", "Xintian Han", "Haoran Ding", "Xinmin Wang", "Wenlin Zhao", "Zhen Gong", "Huizhi Yang", "Zheng Chai", "Zhe Chen", "Yuchao Zheng", "Qiwei Chen", "Feng Zhang", "Xun Zhou", "Peng Xu", "Xiao Yang", "Di Wu", "Zuotao Liu"], "title": "RankMixer: Scaling Up Ranking Models in Industrial Recommenders", "comment": null, "summary": "Recent progress on large language models (LLMs) has spurred interest in\nscaling up recommendation systems, yet two practical obstacles remain. First,\ntraining and serving cost on industrial Recommenders must respect strict\nlatency bounds and high QPS demands. Second, most human-designed\nfeature-crossing modules in ranking models were inherited from the CPU era and\nfail to exploit modern GPUs, resulting in low Model Flops Utilization (MFU) and\npoor scalability. We introduce RankMixer, a hardware-aware model design\ntailored towards a unified and scalable feature-interaction architecture.\nRankMixer retains the transformer's high parallelism while replacing quadratic\nself-attention with multi-head token mixing module for higher efficiency.\nBesides, RankMixer maintains both the modeling for distinct feature subspaces\nand cross-feature-space interactions with Per-token FFNs. We further extend it\nto one billion parameters with a Sparse-MoE variant for higher ROI. A dynamic\nrouting strategy is adapted to address the inadequacy and imbalance of experts\ntraining. Experiments show RankMixer's superior scaling abilities on a\ntrillion-scale production dataset. By replacing previously diverse handcrafted\nlow-MFU modules with RankMixer, we boost the model MFU from 4.5% to 45%, and\nscale our ranking model parameters by 100x while maintaining roughly the same\ninference latency. We verify RankMixer's universality with online A/B tests\nacross three core application scenarios (Recommendation, Advertisement and\nSearch). Finally, we launch 1B Dense-Parameters RankMixer for full traffic\nserving without increasing the serving cost, which improves user active days by\n0.2% and total in-app usage duration by 0.5%.", "AI": {"tldr": "RankMixer is a hardware-aware model that improves efficiency and scalability of large language models in recommendation systems, leading to better user engagement.", "motivation": "Scaling up recommendation systems with LLMs faces challenges due to latency bounds, high QPS demands, and inefficient feature-crossing modules on GPUs.", "method": "The paper introduces RankMixer, a hardware-aware model design with multi-head token mixing and Per-token FFNs, extended with a Sparse-MoE variant and dynamic routing strategy.", "result": "RankMixer boosts model MFU from 4.5% to 45%, scales model parameters by 100x, and improves user active days by 0.2% and in-app usage duration by 0.5%.", "conclusion": "RankMixer significantly improves model MFU, scales ranking model parameters, and enhances user engagement across various applications without increasing serving costs."}}
{"id": "2507.14304", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14304", "abs": "https://arxiv.org/abs/2507.14304", "authors": ["Rakesh Paul", "Anusha Kamath", "Kanishk Singla", "Raviraj Joshi", "Utkarsh Vaidya", "Sanjay Singh Chauhan", "Niranjan Wartikar"], "title": "Aligning Large Language Models to Low-Resource Languages through LLM-Based Selective Translation: A Systematic Study", "comment": null, "summary": "Multilingual large language models (LLMs) often demonstrate a performance gap\nbetween English and non-English languages, particularly in low-resource\nsettings. Aligning these models to low-resource languages is essential yet\nchallenging due to limited high-quality data. While English alignment datasets\nare readily available, curating equivalent data in other languages is expensive\nand time-consuming. A common workaround is to translate existing English\nalignment data; however, standard translation techniques often fail to preserve\ncritical elements such as code, mathematical expressions, and structured\nformats like JSON. In this work, we investigate LLM-based selective\ntranslation, a technique that selectively translates only the translatable\nparts of a text while preserving non-translatable content and sentence\nstructure. We conduct a systematic study to explore key questions around this\napproach, including its effectiveness compared to vanilla translation, the\nimportance of filtering noisy outputs, and the benefits of mixing translated\nsamples with original English data during alignment. Our experiments focus on\nthe low-resource Indic language Hindi and compare translations generated by\nGoogle Cloud Translation (GCP) and Llama-3.1-405B. The results highlight the\npromise of selective translation as a practical and effective method for\nimproving multilingual alignment in LLMs.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8eLLM\u7684\u9009\u62e9\u6027\u7ffb\u8bd1\u6280\u672f\uff0c\u8be5\u6280\u672f\u9009\u62e9\u6027\u5730\u4ec5\u7ffb\u8bd1\u6587\u672c\u7684\u53ef\u7ffb\u8bd1\u90e8\u5206\uff0c\u540c\u65f6\u4fdd\u7559\u4e0d\u53ef\u7ffb\u8bd1\u7684\u5185\u5bb9\u548c\u53e5\u5b50\u7ed3\u6784\uff0c\u4ece\u800c\u63d0\u9ad8LLM\u7684\u591a\u8bed\u8a00\u5bf9\u9f50\u80fd\u529b\u3002", "motivation": "\u591a\u8bed\u8a00\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u901a\u5e38\u5728\u82f1\u8bed\u548c\u975e\u82f1\u8bed\u8bed\u8a00\u4e4b\u95f4\u8868\u73b0\u51fa\u6027\u80fd\u5dee\u8ddd\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\u3002\u7531\u4e8e\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u9650\u5236\uff0c\u5c06\u8fd9\u4e9b\u6a21\u578b\u4e0e\u4f4e\u8d44\u6e90\u8bed\u8a00\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5177\u6709\u6311\u6218\u6027\u3002\u867d\u7136\u82f1\u8bed\u5bf9\u9f50\u6570\u636e\u96c6\u5f88\u5bb9\u6613\u83b7\u5f97\uff0c\u4f46\u5728\u5176\u4ed6\u8bed\u8a00\u4e2d\u6574\u7406\u7b49\u6548\u6570\u636e\u65e2\u6602\u8d35\u53c8\u8017\u65f6\u3002", "method": "LLM-based selective translation", "result": "\u9009\u62e9\u6027\u7ffb\u8bd1\u4f18\u4e8e\u666e\u901a\u7ffb\u8bd1\uff0c\u8fc7\u6ee4\u566a\u58f0\u8f93\u51fa\u975e\u5e38\u91cd\u8981\uff0c\u5e76\u4e14\u5728\u5bf9\u9f50\u671f\u95f4\u5c06\u7ffb\u8bd1\u540e\u7684\u6837\u672c\u4e0e\u539f\u59cb\u82f1\u8bed\u6570\u636e\u6df7\u5408\u662f\u6709\u76ca\u7684\u3002", "conclusion": "\u9009\u62e9\u6027\u7ffb\u8bd1\u662f\u4e00\u79cd\u7528\u4e8e\u6539\u8fdbLLM\u4e2d\u591a\u8bed\u8a00\u5bf9\u9f50\u7684\u5b9e\u7528\u4e14\u6709\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.14454", "categories": ["cs.CV", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.14454", "abs": "https://arxiv.org/abs/2507.14454", "authors": ["Han Gong", "Qiyue Li", "Jie Li", "Zhi Liu"], "title": "Adaptive 3D Gaussian Splatting Video Streaming: Visual Saliency-Aware Tiling and Meta-Learning-Based Bitrate Adaptation", "comment": null, "summary": "3D Gaussian splatting video (3DGS) streaming has recently emerged as a\nresearch hotspot in both academia and industry, owing to its impressive ability\nto deliver immersive 3D video experiences. However, research in this area is\nstill in its early stages, and several fundamental challenges, such as tiling,\nquality assessment, and bitrate adaptation, require further investigation. In\nthis paper, we tackle these challenges by proposing a comprehensive set of\nsolutions. Specifically, we propose an adaptive 3DGS tiling technique guided by\nsaliency analysis, which integrates both spatial and temporal features. Each\ntile is encoded into versions possessing dedicated deformation fields and\nmultiple quality levels for adaptive selection. We also introduce a novel\nquality assessment framework for 3DGS video that jointly evaluates\nspatial-domain degradation in 3DGS representations during streaming and the\nquality of the resulting 2D rendered images. Additionally, we develop a\nmeta-learning-based adaptive bitrate algorithm specifically tailored for 3DGS\nvideo streaming, achieving optimal performance across varying network\nconditions. Extensive experiments demonstrate that our proposed approaches\nsignificantly outperform state-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e3DGS\u89c6\u9891\u6d41\u7684\u81ea\u9002\u5e94\u5206\u5757\u3001\u8d28\u91cf\u8bc4\u4f30\u548c\u6bd4\u7279\u7387\u7b97\u6cd5\u3002", "motivation": "3D\u9ad8\u65af\u6e85\u5c04\u89c6\u9891\uff083DGS\uff09\u6d41\u5a92\u4f53\u6700\u8fd1\u6210\u4e3a\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\u7684\u7814\u7a76\u70ed\u70b9\uff0c\u56e0\u4e3a\u5b83\u5177\u6709\u63d0\u4f9b\u6c89\u6d78\u5f0f3D\u89c6\u9891\u4f53\u9a8c\u7684\u5f3a\u5927\u80fd\u529b\u3002\u7136\u800c\uff0c\u8be5\u9886\u57df\u7684\u7814\u7a76\u4ecd\u5904\u4e8e\u65e9\u671f\u9636\u6bb5\uff0c\u4e00\u4e9b\u57fa\u672c\u6311\u6218\uff0c\u5982\u5206\u5757\u3001\u8d28\u91cf\u8bc4\u4f30\u548c\u6bd4\u7279\u7387\u81ea\u9002\u5e94\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e943DGS\u5206\u5757\u6280\u672f\uff0c\u8be5\u6280\u672f\u7531\u663e\u7740\u6027\u5206\u6790\u5f15\u5bfc\uff0c\u96c6\u6210\u4e86\u7a7a\u95f4\u548c\u65f6\u95f4\u7279\u5f81\u3002\u6bcf\u4e2a\u56fe\u5757\u90fd\u88ab\u7f16\u7801\u6210\u5177\u6709\u4e13\u7528\u53d8\u5f62\u573a\u548c\u591a\u4e2a\u8d28\u91cf\u7ea7\u522b\u7684\u7248\u672c\uff0c\u4ee5\u4f9b\u81ea\u9002\u5e94\u9009\u62e9\u3002\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u76843DGS\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5171\u540c\u8bc4\u4f30\u4e86\u6d41\u5f0f\u4f20\u8f93\u671f\u95f43DGS\u8868\u793a\u4e2d\u7684\u7a7a\u95f4\u57df\u9000\u5316\u4ee5\u53ca\u751f\u6210\u76842D\u6e32\u67d3\u56fe\u50cf\u7684\u8d28\u91cf\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u6bd4\u7279\u7387\u7b97\u6cd5\uff0c\u4e13\u95e8\u4e3a3DGS\u89c6\u9891\u6d41\u91cf\u8eab\u5b9a\u5236\uff0c\u53ef\u5728\u4e0d\u540c\u7684\u7f51\u7edc\u6761\u4ef6\u4e0b\u5b9e\u73b0\u6700\u4f73\u6027\u80fd\u3002", "result": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u5957\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.14182", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.14182", "abs": "https://arxiv.org/abs/2507.14182", "authors": ["Xiaotong Luo", "Shengda Zhuo", "Min Chen", "Lichun Li", "Ruizhao Lu", "Wenqi Fan", "Shuqiang Huang", "Yin Tang"], "title": "From Bias to Behavior: Learning Bull-Bear Market Dynamics with Contrastive Modeling", "comment": null, "summary": "Financial markets exhibit highly dynamic and complex behaviors shaped by both\nhistorical price trajectories and exogenous narratives, such as news, policy\ninterpretations, and social media sentiment. The heterogeneity in these data\nand the diverse insight of investors introduce biases that complicate the\nmodeling of market dynamics. Unlike prior work, this paper explores the\npotential of bull and bear regimes in investor-driven market dynamics. Through\nempirical analysis on real-world financial datasets, we uncover a dynamic\nrelationship between bias variation and behavioral adaptation, which enhances\ntrend prediction under evolving market conditions. To model this mechanism, we\npropose the Bias to Behavior from Bull-Bear Dynamics model (B4), a unified\nframework that jointly embeds temporal price sequences and external contextual\nsignals into a shared latent space where opposing bull and bear forces\nnaturally emerge, forming the foundation for bias representation. Within this\nspace, an inertial pairing module pairs temporally adjacent samples to preserve\nmomentum, while the dual competition mechanism contrasts bullish and bearish\nembeddings to capture behavioral divergence. Together, these components allow\nB4 to model bias-driven asymmetry, behavioral inertia, and market\nheterogeneity. Experimental results on real-world financial datasets\ndemonstrate that our model not only achieves superior performance in predicting\nmarket trends but also provides interpretable insights into the interplay of\nbiases, investor behaviors, and market dynamics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a B4 \u7684\u65b0\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u5e02\u573a\u8d8b\u52bf\u3002\u8be5\u6a21\u578b\u8003\u8651\u4e86\u6295\u8d44\u8005\u504f\u5dee\u548c\u5e02\u573a\u52a8\u6001\uff0c\u5e76\u5728\u5b9e\u9645\u91d1\u878d\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u91d1\u878d\u5e02\u573a\u8868\u73b0\u51fa\u9ad8\u5ea6\u52a8\u6001\u548c\u590d\u6742\u7684\u884c\u4e3a\uff0c\u8fd9\u4e9b\u884c\u4e3a\u53d7\u5230\u5386\u53f2\u4ef7\u683c\u8f68\u8ff9\u548c\u5916\u751f\u53d9\u4e8b\uff08\u5982\u65b0\u95fb\u3001\u653f\u7b56\u89e3\u8bfb\u548c\u793e\u4ea4\u5a92\u4f53\u60c5\u7eea\uff09\u7684\u5f71\u54cd\u3002\u8fd9\u4e9b\u6570\u636e\u4e2d\u7684\u5f02\u8d28\u6027\u548c\u6295\u8d44\u8005\u89c2\u70b9\u7684\u591a\u6837\u6027\u5f15\u5165\u4e86\u504f\u5dee\uff0c\u4f7f\u5e02\u573a\u52a8\u6001\u7684\u5efa\u6a21\u53d8\u5f97\u590d\u6742\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u504f\u5dee\u5230\u884c\u4e3a\u7684\u725b\u718a\u52a8\u6001\u6a21\u578b (B4)\uff0c\u8be5\u6a21\u578b\u5c06\u65f6\u95f4\u4ef7\u683c\u5e8f\u5217\u548c\u5916\u90e8\u4e0a\u4e0b\u6587\u4fe1\u53f7\u5171\u540c\u5d4c\u5165\u5230\u4e00\u4e2a\u5171\u4eab\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u5728\u8fd9\u4e2a\u7a7a\u95f4\u4e2d\uff0c\u76f8\u5bf9\u7684\u725b\u5e02\u548c\u718a\u5e02\u529b\u91cf\u81ea\u7136\u51fa\u73b0\uff0c\u5f62\u6210\u4e86\u504f\u5dee\u8868\u793a\u7684\u57fa\u7840\u3002", "result": "\u8be5\u6a21\u578b\u5728\u9884\u6d4b\u5e02\u573a\u8d8b\u52bf\u65b9\u9762\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u5e76\u4e3a\u504f\u5dee\u3001\u6295\u8d44\u8005\u884c\u4e3a\u548c\u5e02\u573a\u52a8\u6001\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u89c1\u89e3\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u9884\u6d4b\u5e02\u573a\u8d8b\u52bf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u80fd\u6df1\u5165\u4e86\u89e3\u504f\u5dee\u3001\u6295\u8d44\u8005\u884c\u4e3a\u548c\u5e02\u573a\u52a8\u6001\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002"}}
{"id": "2507.14468", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14468", "abs": "https://arxiv.org/abs/2507.14468", "authors": ["Yitong Lin", "Jiaying He", "Jiahe Chen", "Xinnan Zhu", "Jianwei Zheng", "Tao Bo"], "title": "BioGraphFusion: Graph Knowledge Embedding for Biological Completion and Reasoning", "comment": "Accepted by Bioinformatics on July 11th", "summary": "Motivation: Biomedical knowledge graphs (KGs) are crucial for drug discovery\nand disease understanding, yet their completion and reasoning are challenging.\nKnowledge Embedding (KE) methods capture global semantics but struggle with\ndynamic structural integration, while Graph Neural Networks (GNNs) excel\nlocally but often lack semantic understanding. Even ensemble approaches,\nincluding those leveraging language models, often fail to achieve a deep,\nadaptive, and synergistic co-evolution between semantic comprehension and\nstructural learning. Addressing this critical gap in fostering continuous,\nreciprocal refinement between these two aspects in complex biomedical KGs is\nparamount.\n  Results: We introduce BioGraphFusion, a novel framework for deeply\nsynergistic semantic and structural learning. BioGraphFusion establishes a\nglobal semantic foundation via tensor decomposition, guiding an LSTM-driven\nmechanism to dynamically refine relation embeddings during graph propagation.\nThis fosters adaptive interplay between semantic understanding and structural\nlearning, further enhanced by query-guided subgraph construction and a hybrid\nscoring mechanism. Experiments across three key biomedical tasks demonstrate\nBioGraphFusion's superior performance over state-of-the-art KE, GNN, and\nensemble models. A case study on Cutaneous Malignant Melanoma 1 (CMM1)\nhighlights its ability to unveil biologically meaningful pathways.\n  Availability and Implementation: Source code and all training data are freely\navailable for download at https://github.com/Y-TARL/BioGraphFusion.\n  Contact: zjw@zjut.edu.cn, botao666666@126.com.\n  Supplementary information: Supplementary data are available at Bioinformatics\nonline.", "AI": {"tldr": "BioGraphFusion \u662f\u4e00\u79cd\u7528\u4e8e\u6df1\u5ea6\u534f\u540c\u8bed\u4e49\u548c\u7ed3\u6784\u5b66\u4e60\u7684\u65b0\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u5f20\u91cf\u5206\u89e3\u5efa\u7acb\u5168\u5c40\u8bed\u4e49\u57fa\u7840\uff0c\u6307\u5bfc LSTM \u9a71\u52a8\u7684\u673a\u5236\u6765\u52a8\u6001\u7ec6\u5316\u5173\u7cfb\u5d4c\u5165\uff0c\u4ece\u800c\u5728\u8bed\u4e49\u7406\u89e3\u548c\u7ed3\u6784\u5b66\u4e60\u4e4b\u95f4\u5efa\u7acb\u81ea\u9002\u5e94\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "motivation": "\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31 (KG) \u5bf9\u4e8e\u836f\u7269\u53d1\u73b0\u548c\u75be\u75c5\u7406\u89e3\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5b83\u4eec\u7684\u5b8c\u6210\u548c\u63a8\u7406\u5177\u6709\u6311\u6218\u6027\u3002\u77e5\u8bc6\u5d4c\u5165 (KE) \u65b9\u6cd5\u6355\u83b7\u5168\u5c40\u8bed\u4e49\uff0c\u4f46\u5728\u52a8\u6001\u7ed3\u6784\u96c6\u6210\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u800c\u56fe\u795e\u7ecf\u7f51\u7edc (GNN) \u5728\u5c40\u90e8\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u901a\u5e38\u7f3a\u4e4f\u8bed\u4e49\u7406\u89e3\u3002\u5373\u4f7f\u662f\u5305\u62ec\u5229\u7528\u8bed\u8a00\u6a21\u578b\u7684\u96c6\u6210\u65b9\u6cd5\uff0c\u4e5f\u5e38\u5e38\u65e0\u6cd5\u5728\u8bed\u4e49\u7406\u89e3\u548c\u7ed3\u6784\u5b66\u4e60\u4e4b\u95f4\u5b9e\u73b0\u6df1\u5ea6\u3001\u81ea\u9002\u5e94\u548c\u534f\u540c\u7684\u534f\u540c\u8fdb\u5316\u3002", "method": "BioGraphFusion \u5efa\u7acb\u4e86\u4e00\u4e2a\u901a\u8fc7\u5f20\u91cf\u5206\u89e3\u7684\u5168\u5c40\u8bed\u4e49\u57fa\u7840\uff0c\u6307\u5bfc LSTM \u9a71\u52a8\u7684\u673a\u5236\u6765\u52a8\u6001\u5730\u7ec6\u5316\u56fe\u4f20\u64ad\u671f\u95f4\u7684\u5173\u7cfb\u5d4c\u5165\u3002", "result": "BioGraphFusion \u5728\u4e09\u4e2a\u5173\u952e\u751f\u7269\u533b\u5b66\u4efb\u52a1\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684 KE\u3001GNN \u548c\u96c6\u6210\u6a21\u578b\u3002\u9ed1\u8272\u7d20\u7624\u6848\u4f8b\u7814\u7a76\u7a81\u51fa\u4e86\u5176\u63ed\u793a\u751f\u7269\u5b66\u610f\u4e49\u901a\u8def\u7684\u80fd\u529b\u3002", "conclusion": "BioGraphFusion\u5728\u4e09\u4e2a\u5173\u952e\u751f\u7269\u533b\u5b66\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709 KE\u3001GNN \u548c\u96c6\u6210\u6a21\u578b\u3002\u9ed1\u8272\u7d20\u7624\u6848\u4f8b\u7814\u7a76\u7a81\u51fa\u4e86\u5176\u63ed\u793a\u751f\u7269\u5b66\u610f\u4e49\u901a\u8def\u7684\u80fd\u529b\u3002"}}
{"id": "2507.15826", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15826", "abs": "https://arxiv.org/abs/2507.15826", "authors": ["Alessandro B. Melchiorre", "Elena V. Epure", "Shahed Masoudian", "Gustavo Escobedo", "Anna Hausberger", "Manuel Moussallam", "Markus Schedl"], "title": "Just Ask for Music (JAM): Multimodal and Personalized Natural Language Music Recommendation", "comment": null, "summary": "Natural language interfaces offer a compelling approach for music\nrecommendation, enabling users to express complex preferences conversationally.\nWhile Large Language Models (LLMs) show promise in this direction, their\nscalability in recommender systems is limited by high costs and latency.\nRetrieval-based approaches using smaller language models mitigate these issues\nbut often rely on single-modal item representations, overlook long-term user\npreferences, and require full model retraining, posing challenges for\nreal-world deployment. In this paper, we present JAM (Just Ask for Music), a\nlightweight and intuitive framework for natural language music recommendation.\nJAM models user-query-item interactions as vector translations in a shared\nlatent space, inspired by knowledge graph embedding methods like TransE. To\ncapture the complexity of music and user intent, JAM aggregates multimodal item\nfeatures via cross-attention and sparse mixture-of-experts. We also introduce\nJAMSessions, a new dataset of over 100k user-query-item triples with anonymized\nuser/item embeddings, uniquely combining conversational queries and user\nlong-term preferences. Our results show that JAM provides accurate\nrecommendations, produces intuitive representations suitable for practical use\ncases, and can be easily integrated with existing music recommendation stacks.", "AI": {"tldr": "JAM is a lightweight framework for natural language music recommendation that models user-query-item interactions as vector translations in a shared latent space. It introduces JAMSessions, a new dataset of over 100k user-query-item triples. JAM provides accurate recommendations, produces intuitive representations, and can be easily integrated with existing music recommendation stacks.", "motivation": "Natural language interfaces offer a compelling approach for music recommendation, enabling users to express complex preferences conversationally. While Large Language Models (LLMs) show promise in this direction, their scalability in recommender systems is limited by high costs and latency. Retrieval-based approaches using smaller language models mitigate these issues but often rely on single-modal item representations, overlook long-term user preferences, and require full model retraining, posing challenges for real-world deployment.", "method": "JAM models user-query-item interactions as vector translations in a shared latent space, inspired by knowledge graph embedding methods like TransE. To capture the complexity of music and user intent, JAM aggregates multimodal item features via cross-attention and sparse mixture-of-experts.", "result": "JAM provides accurate recommendations and produces intuitive representations.", "conclusion": "JAM provides accurate recommendations, produces intuitive representations suitable for practical use cases, and can be easily integrated with existing music recommendation stacks."}}
{"id": "2507.14307", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14307", "abs": "https://arxiv.org/abs/2507.14307", "authors": ["Karin de Langis", "Jong Inn Park", "Andreas Schramm", "Bin Hu", "Khanh Chi Le", "Michael Mensink", "Ahn Thu Tong", "Dongyeop Kang"], "title": "How LLMs Comprehend Temporal Meaning in Narratives: A Case Study in Cognitive Evaluation of LLMs", "comment": null, "summary": "Large language models (LLMs) exhibit increasingly sophisticated linguistic\ncapabilities, yet the extent to which these behaviors reflect human-like\ncognition versus advanced pattern recognition remains an open question. In this\nstudy, we investigate how LLMs process the temporal meaning of linguistic\naspect in narratives that were previously used in human studies. Using an\nExpert-in-the-Loop probing pipeline, we conduct a series of targeted\nexperiments to assess whether LLMs construct semantic representations and\npragmatic inferences in a human-like manner. Our findings show that LLMs\nover-rely on prototypicality, produce inconsistent aspectual judgments, and\nstruggle with causal reasoning derived from aspect, raising concerns about\ntheir ability to fully comprehend narratives. These results suggest that LLMs\nprocess aspect fundamentally differently from humans and lack robust narrative\nunderstanding. Beyond these empirical findings, we develop a standardized\nexperimental framework for the reliable assessment of LLMs' cognitive and\nlinguistic capabilities.", "AI": {"tldr": "LLMs don't understand narratives like humans do, over-relying on patterns instead of true comprehension.", "motivation": "The extent to which these behaviors reflect human-like cognition versus advanced pattern recognition remains an open question.", "method": "Using an Expert-in-the-Loop probing pipeline, we conduct a series of targeted experiments to assess whether LLMs construct semantic representations and pragmatic inferences in a human-like manner.", "result": "LLMs over-rely on prototypicality, produce inconsistent aspectual judgments, and struggle with causal reasoning derived from aspect.", "conclusion": "LLMs process aspect fundamentally differently from humans and lack robust narrative understanding."}}
{"id": "2507.14456", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14456", "abs": "https://arxiv.org/abs/2507.14456", "authors": ["Chi Wan", "Yixin Cui", "Jiatong Du", "Shuo Yang", "Yulong Bai", "Yanjun Huang"], "title": "GEMINUS: Dual-aware Global and Scene-Adaptive Mixture-of-Experts for End-to-End Autonomous Driving", "comment": null, "summary": "End-to-end autonomous driving requires adaptive and robust handling of\ncomplex and diverse traffic environments. However, prevalent single-mode\nplanning methods attempt to learn an overall policy while struggling to acquire\ndiversified driving skills to handle diverse scenarios. Therefore, this paper\nproposes GEMINUS, a Mixture-of-Experts end-to-end autonomous driving framework\nfeaturing a Global Expert, a Scene-Adaptive Experts Group, and equipped with a\nDual-aware Router. Specifically, the Global Expert is trained on the overall\ndataset, possessing robust performance. The Scene-Adaptive Experts are trained\non corresponding scene subsets, achieving adaptive performance. The Dual-aware\nRouter simultaneously considers scenario-level features and routing uncertainty\nto dynamically activate expert modules. Through the effective coupling of the\nGlobal Expert and the Scene-Adaptive Experts Group via the Dual-aware Router,\nGEMINUS achieves adaptive and robust performance in diverse scenarios. GEMINUS\noutperforms existing methods in the Bench2Drive closed-loop benchmark and\nachieves state-of-the-art performance in Driving Score and Success Rate, even\nwith only monocular vision input. Furthermore, ablation studies demonstrate\nsignificant improvements over the original single-expert baseline: 7.67% in\nDriving Score, 22.06% in Success Rate, and 19.41% in MultiAbility-Mean. The\ncode will be available at https://github.com/newbrains1/GEMINUS.", "AI": {"tldr": "GEMINUS \u662f\u4e00\u4e2a\u6df7\u5408\u4e13\u5bb6\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u4e13\u5bb6\u548c\u573a\u666f\u81ea\u9002\u5e94\u4e13\u5bb6\u7ec4\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u81ea\u9002\u5e94\u548c\u9c81\u68d2\u7684\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u6a21\u89c4\u5212\u65b9\u6cd5\u8bd5\u56fe\u5b66\u4e60\u6574\u4f53\u7b56\u7565\uff0c\u4f46\u5728\u83b7\u53d6\u591a\u6837\u5316\u7684\u9a7e\u9a76\u6280\u80fd\u6765\u5904\u7406\u5404\u79cd\u573a\u666f\u65f6\u9047\u5230\u4e86\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u4e13\u5bb6\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6 GEMINUS\uff0c\u8be5\u6846\u67b6\u5177\u6709\u4e00\u4e2a\u5168\u5c40\u4e13\u5bb6\u3001\u4e00\u4e2a\u573a\u666f\u81ea\u9002\u5e94\u4e13\u5bb6\u7ec4\uff0c\u5e76\u914d\u5907\u4e86\u4e00\u4e2a\u53cc\u91cd\u611f\u77e5\u8def\u7531\u5668\u3002", "result": "GEMINUS \u5728\u9a7e\u9a76\u8bc4\u5206\u65b9\u9762\u63d0\u9ad8\u4e86 7.67%\uff0c\u5728\u6210\u529f\u7387\u65b9\u9762\u63d0\u9ad8\u4e86 22.06%\uff0c\u5728 MultiAbility-Mean \u65b9\u9762\u63d0\u9ad8\u4e86 19.41%\u3002", "conclusion": "GEMINUS \u901a\u8fc7\u5168\u5c40\u4e13\u5bb6\u548c\u573a\u666f\u81ea\u9002\u5e94\u4e13\u5bb6\u7ec4\u7684\u6709\u6548\u7ed3\u5408\uff0c\u5728\u5404\u79cd\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u81ea\u9002\u5e94\u548c\u9c81\u68d2\u7684\u6027\u80fd\uff0c\u5e76\u5728 Bench2Drive \u95ed\u73af\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5373\u4f7f\u4ec5\u4f7f\u7528\u5355\u76ee\u89c6\u89c9\u8f93\u5165\uff0c\u4e5f\u80fd\u5728\u9a7e\u9a76\u8bc4\u5206\u548c\u6210\u529f\u7387\u65b9\u9762\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2507.14204", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14204", "abs": "https://arxiv.org/abs/2507.14204", "authors": ["Dachuan Shi", "Yonggan Fu", "Xiangchi Yuan", "Zhongzhi Yu", "Haoran You", "Sixu Li", "Xin Dong", "Jan Kautz", "Pavlo Molchanov", "Yingyan", "Lin"], "title": "LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models", "comment": "ICML 2025. Code: https://github.com/GATECH-EIC/LaCache", "summary": "Recent advancements in Large Language Models (LLMs) have spurred interest in\nnumerous applications requiring robust long-range capabilities, essential for\nprocessing extensive input contexts and continuously generating extended\noutputs. As sequence lengths increase, the number of Key-Value (KV) pairs in\nLLMs escalates, creating a significant efficiency bottleneck. In this paper, we\npropose a new KV cache optimization paradigm called LaCache, a training-free\nmethod for efficient and accurate generative inference of LLMs. LaCache enables\nLLMs to simultaneously address both of the critical challenges in long-range\nmodeling: robust long-range capabilities and continuous generation without\nrunning out-of-memory (OOM). Specifically, LaCache integrates two key\ninnovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only\nsequentially (left-to-right within each layer) but also across layers (from\nshallow to deep), providing an extended span for capturing long-range\ndependencies under a fixed storage budget, thereby boosting long-range\ncapabilities; and (2) an iterative compaction mechanism that progressively\ncompresses older caches, freeing up space for new tokens within a fixed cache\nsize. This token distance-based dynamic compression enables more effective\ncontinuous generation under constrained cache budgets. Experiments across\nvarious tasks, benchmarks, and LLM models consistently validate LaCache's\neffectiveness in enhancing LLMs' long-range capabilities. Our code is available\nat https://github.com/GATECH-EIC/LaCache.", "AI": {"tldr": "LaCache\u662f\u4e00\u79cd\u65b0\u7684KV\u7f13\u5b58\u4f18\u5316\u8303\u4f8b\uff0c\u5b83\u901a\u8fc7\u68af\u5f62KV\u7f13\u5b58\u6a21\u5f0f\u548c\u8fed\u4ee3\u538b\u7f29\u673a\u5236\uff0c\u5728\u589e\u5f3aLLM\u7684\u8fdc\u7a0b\u80fd\u529b\u65b9\u9762\u662f\u6709\u6548\u7684\u3002", "motivation": "\u5e8f\u5217\u957f\u5ea6\u7684\u589e\u52a0\uff0cLLM\u4e2d\u7684Key-Value (KV)\u5bf9\u7684\u6570\u91cf\u4e5f\u5728\u589e\u52a0\uff0c\u9020\u6210\u4e86\u663e\u8457\u7684\u6548\u7387\u74f6\u9888\u3002\u9700\u8981\u5f3a\u5927\u7684\u8fdc\u7a0b\u80fd\u529b\uff0c\u8fd9\u5bf9\u4e8e\u5904\u7406\u5e7f\u6cdb\u7684\u8f93\u5165\u4e0a\u4e0b\u6587\u548c\u4e0d\u65ad\u751f\u6210\u6269\u5c55\u7684\u8f93\u51fa\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4e00\u79cd\u540d\u4e3aLaCache\u7684\u65b0\u7684KV\u7f13\u5b58\u4f18\u5316\u8303\u4f8b\uff0c\u8fd9\u662f\u4e00\u79cd\u7528\u4e8eLLM\u7684\u9ad8\u6548\u548c\u51c6\u786e\u7684\u751f\u6210\u63a8\u7406\u7684\u514d\u8bad\u7ec3\u65b9\u6cd5\u3002LaCache\u96c6\u6210\u4e86\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a(1) \u68af\u5f62KV\u7f13\u5b58\u6a21\u5f0f\uff0c\u4e0d\u4ec5\u6309\u987a\u5e8f\uff08\u5728\u6bcf\u4e00\u5c42\u4e2d\u4ece\u5de6\u5230\u53f3\uff09\u5b58\u50a8KV\u5bf9\uff0c\u800c\u4e14\u8de8\u5c42\uff08\u4ece\u6d45\u5230\u6df1\uff09\u5b58\u50a8KV\u5bf9\uff0c\u4ece\u800c\u4e3a\u5728\u56fa\u5b9a\u5b58\u50a8\u9884\u7b97\u4e0b\u6355\u83b7\u8fdc\u7a0b\u4f9d\u8d56\u5173\u7cfb\u63d0\u4f9b\u4e86\u6269\u5c55\u8303\u56f4\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u8fdc\u7a0b\u80fd\u529b\uff1b(2) \u4e00\u79cd\u8fed\u4ee3\u538b\u7f29\u673a\u5236\uff0c\u53ef\u9010\u6b65\u538b\u7f29\u65e7\u7f13\u5b58\uff0c\u4ece\u800c\u5728\u56fa\u5b9a\u7f13\u5b58\u5927\u5c0f\u5185\u4e3a\u65b0\u4ee4\u724c\u91ca\u653e\u7a7a\u95f4\u3002", "result": "\u5728\u5404\u79cd\u4efb\u52a1\u3001\u57fa\u51c6\u548cLLM\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u4e00\u81f4\u5730\u9a8c\u8bc1\u4e86LaCache\u5728\u589e\u5f3aLLM\u7684\u8fdc\u7a0b\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "LaCache\u5728\u589e\u5f3aLLM\u7684\u8fdc\u7a0b\u80fd\u529b\u65b9\u9762\u662f\u6709\u6548\u7684\u3002"}}
{"id": "2507.14513", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14513", "abs": "https://arxiv.org/abs/2507.14513", "authors": ["Hongyi Yang", "Yue Pan", "Jiayi Xu", "Kelsen Liu"], "title": "Amico: An Event-Driven Modular Framework for Persistent and Embedded Autonomy", "comment": null, "summary": "Recent advances in large language models (LLMs) and autonomous agents have\nenabled systems capable of performing complex tasks across domains such as\nhuman-computer interaction, planning, and web navigation. However, many\nexisting frameworks struggle in real-world or resource-constrained environments\ndue to their reliance on cloud-based computation, limited robustness in dynamic\ncontexts, and lack of persistent autonomy and environmental awareness.\n  We present Amico, a modular, event-driven framework for building autonomous\nagents optimized for embedded systems. Written in Rust for safety and\nperformance, Amico supports reactive, persistent agents that operate\nefficiently across embedded platforms and browser environments via WebAssembly.\nIt provides clean abstractions for event handling, state management, behavior\nexecution, and integration with reasoning modules. Amico delivers a unified\ninfrastructure for constructing resilient, interactive agents suitable for\ndeployment in settings with limited compute and intermittent connectivity.", "AI": {"tldr": "Amico is a framework for building autonomous agents optimized for embedded systems.", "motivation": "Many existing frameworks struggle in real-world or resource-constrained environments due to their reliance on cloud-based computation, limited robustness in dynamic contexts, and lack of persistent autonomy and environmental awareness.", "method": "Amico, a modular, event-driven framework for building autonomous agents optimized for embedded systems. Written in Rust.", "result": "Amico supports reactive, persistent agents that operate efficiently across embedded platforms and browser environments via WebAssembly. It provides clean abstractions for event handling, state management, behavior execution, and integration with reasoning modules.", "conclusion": "Amico is a unified infrastructure for constructing resilient, interactive agents suitable for deployment in settings with limited compute and intermittent connectivity."}}
{"id": "2507.14758", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.14758", "abs": "https://arxiv.org/abs/2507.14758", "authors": ["Luyi Ma", "Wanjia Zhang", "Kai Zhao", "Abhishek Kulkarni", "Lalitesh Morishetti", "Anjana Ganesh", "Ashish Ranjan", "Aashika Padmanabhan", "Jianpeng Xu", "Jason Cho", "Praveen Kanumala", "Kaushiki Nag", "Sumit Dutta", "Kamiya Motwani", "Malay Patel", "Evren Korpeoglu", "Sushant Kumar", "Kannan Achan"], "title": "GRACE: Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization", "comment": "10 pages, 5 figures, The ACM Conference on Recommender Systems\n  (RecSys) 2025", "summary": "Generative models have recently demonstrated strong potential in\nmulti-behavior recommendation systems, leveraging the expressive power of\ntransformers and tokenization to generate personalized item sequences. However,\ntheir adoption is hindered by (1) the lack of explicit information for token\nreasoning, (2) high computational costs due to quadratic attention complexity\nand dense sequence representations after tokenization, and (3) limited\nmulti-scale modeling over user history. In this work, we propose GRACE\n(Generative Recommendation via journey-aware sparse Attention on\nChain-of-thought tokEnization), a novel generative framework for multi-behavior\nsequential recommendation. GRACE introduces a hybrid Chain-of-Thought (CoT)\ntokenization method that encodes user-item interactions with explicit\nattributes from product knowledge graphs (e.g., category, brand, price) over\nsemantic tokenization, enabling interpretable and behavior-aligned generation.\nTo address the inefficiency of standard attention, we design a Journey-Aware\nSparse Attention (JSA) mechanism, which selectively attends to compressed,\nintra-, inter-, and current-context segments in the tokenized sequence.\nExperiments on two real-world datasets show that GRACE significantly\noutperforms state-of-the-art baselines, achieving up to +106.9% HR@10 and\n+106.7% NDCG@10 improvement over the state-of-the-art baseline on the Home\ndomain, and +22.1% HR@10 on the Electronics domain. GRACE also reduces\nattention computation by up to 48% with long sequences.", "AI": {"tldr": "GRACE\u662f\u4e00\u79cd\u65b0\u7684\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u884c\u4e3a\u5e8f\u5217\u63a8\u8350\uff0c\u5b83\u901a\u8fc7\u65c5\u7a0b\u611f\u77e5\u7684\u7a00\u758f\u6ce8\u610f\u529b\u548c\u94fe\u5f0f\u601d\u8003tokenization\u6765\u5b9e\u73b0\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u6700\u8fd1\u5728\u591a\u884c\u4e3a\u63a8\u8350\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6f5c\u529b\uff0c\u5229\u7528transformer\u548ctokenization\u7684\u8868\u8fbe\u80fd\u529b\u6765\u751f\u6210\u4e2a\u6027\u5316\u7684\u9879\u76ee\u5e8f\u5217\u3002\u7136\u800c\uff0c\u5b83\u4eec\u7684\u91c7\u7528\u53d7\u5230\u4ee5\u4e0b\u56e0\u7d20\u7684\u963b\u788d\uff1a\uff081\uff09\u7f3a\u4e4f\u7528\u4e8etoken\u63a8\u7406\u7684\u663e\u5f0f\u4fe1\u606f\uff0c\uff082\uff09\u7531\u4e8e\u4e8c\u6b21\u6ce8\u610f\u590d\u6742\u6027\u548ctokenization\u540e\u7684\u5bc6\u96c6\u5e8f\u5217\u8868\u793a\u800c\u5bfc\u81f4\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\uff0c\u4ee5\u53ca\uff083\uff09\u5bf9\u7528\u6237\u5386\u53f2\u7684\u6709\u9650\u591a\u5c3a\u5ea6\u5efa\u6a21\u3002", "method": "GRACE\u5f15\u5165\u4e86\u4e00\u79cd\u6df7\u5408\u7684Chain-of-Thought (CoT) tokenization\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528\u6765\u81ea\u4ea7\u54c1\u77e5\u8bc6\u56fe\u8c31\u7684\u663e\u5f0f\u5c5e\u6027\uff08\u4f8b\u5982\uff0c\u7c7b\u522b\u3001\u54c1\u724c\u3001\u4ef7\u683c\uff09\u5728\u8bed\u4e49tokenization\u4e0a\u7f16\u7801\u7528\u6237-\u9879\u76ee\u4ea4\u4e92\uff0c\u4ece\u800c\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u3001\u884c\u4e3a\u5bf9\u9f50\u7684\u751f\u6210\u3002\u4e3a\u4e86\u89e3\u51b3\u6807\u51c6\u6ce8\u610f\u529b\u7684\u4f4e\u6548\u95ee\u9898\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cdJourney-Aware Sparse Attention (JSA)\u673a\u5236\uff0c\u8be5\u673a\u5236\u9009\u62e9\u6027\u5730\u6ce8\u610ftokenized\u5e8f\u5217\u4e2d\u538b\u7f29\u7684\u3001\u5185\u90e8\u7684\u3001\u76f8\u4e92\u7684\u4ee5\u53ca\u5f53\u524d\u7684\u4e0a\u4e0b\u6587\u7247\u6bb5\u3002", "result": "GRACE\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\uff0c\u5728Home\u9886\u57df\u5b9e\u73b0\u4e86\u9ad8\u8fbe+106.9% HR@10\u548c+106.7% NDCG@10\u7684\u6539\u8fdb\uff0c\u5728Electronics\u9886\u57df\u5b9e\u73b0\u4e86+22.1% HR@10\u7684\u6539\u8fdb\u3002GRACE\u8fd8\u51cf\u5c11\u4e86\u9ad8\u8fbe48%\u7684\u6ce8\u610f\u8ba1\u7b97\u3002", "conclusion": "GRACE\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\uff0c\u5728Home\u9886\u57df\u5b9e\u73b0\u4e86\u9ad8\u8fbe+106.9% HR@10\u548c+106.7% NDCG@10\u7684\u6539\u8fdb\uff0c\u5728Electronics\u9886\u57df\u5b9e\u73b0\u4e86+22.1% HR@10\u7684\u6539\u8fdb\u3002GRACE\u8fd8\u51cf\u5c11\u4e86\u9ad8\u8fbe48%\u7684\u6ce8\u610f\u8ba1\u7b97\u3002"}}
{"id": "2507.14314", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14314", "abs": "https://arxiv.org/abs/2507.14314", "authors": ["Marija An\u0111edeli\u0107", "Dominik \u0160ipek", "Laura Majer", "Jan \u0160najder"], "title": "What Makes You CLIC: Detection of Croatian Clickbait Headlines", "comment": "Accepted at Slavic NLP 2025", "summary": "Online news outlets operate predominantly on an advertising-based revenue\nmodel, compelling journalists to create headlines that are often scandalous,\nintriguing, and provocative -- commonly referred to as clickbait. Automatic\ndetection of clickbait headlines is essential for preserving information\nquality and reader trust in digital media and requires both contextual\nunderstanding and world knowledge. For this task, particularly in\nless-resourced languages, it remains unclear whether fine-tuned methods or\nin-context learning (ICL) yield better results. In this paper, we compile CLIC,\na novel dataset for clickbait detection of Croatian news headlines spanning a\n20-year period and encompassing mainstream and fringe outlets. We fine-tune the\nBERTi\\'c model on this task and compare its performance to LLM-based ICL\nmethods with prompts both in Croatian and English. Finally, we analyze the\nlinguistic properties of clickbait. We find that nearly half of the analyzed\nheadlines contain clickbait, and that finetuned models deliver better results\nthan general LLMs.", "AI": {"tldr": "This paper introduces a new dataset (CLIC) for Croatian clickbait detection and finds that fine-tuned models perform better than general LLMs.", "motivation": "Automatic detection of clickbait headlines is essential for preserving information quality and reader trust in digital media.", "method": "Fine-tuned BERTi'c model and compared its performance to LLM-based ICL methods with prompts both in Croatian and English.", "result": "Nearly half of the analyzed headlines contain clickbait.", "conclusion": "Finetuned models outperform general LLMs in clickbait detection for Croatian news headlines."}}
{"id": "2507.14459", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14459", "abs": "https://arxiv.org/abs/2507.14459", "authors": ["Huayuan Ye", "Juntong Chen", "Shenzhuo Zhang", "Yipeng Zhang", "Changbo Wang", "Chenhui Li"], "title": "VisGuard: Securing Visualization Dissemination through Tamper-Resistant Data Retrieval", "comment": "9 pages, IEEE VIS 2025", "summary": "The dissemination of visualizations is primarily in the form of raster\nimages, which often results in the loss of critical information such as source\ncode, interactive features, and metadata. While previous methods have proposed\nembedding metadata into images to facilitate Visualization Image Data Retrieval\n(VIDR), most existing methods lack practicability since they are fragile to\ncommon image tampering during online distribution such as cropping and editing.\nTo address this issue, we propose VisGuard, a tamper-resistant VIDR framework\nthat reliably embeds metadata link into visualization images. The embedded data\nlink remains recoverable even after substantial tampering upon images. We\npropose several techniques to enhance robustness, including repetitive data\ntiling, invertible information broadcasting, and an anchor-based scheme for\ncrop localization. VisGuard enables various applications, including interactive\nchart reconstruction, tampering detection, and copyright protection. We conduct\ncomprehensive experiments on VisGuard's superior performance in data retrieval\naccuracy, embedding capacity, and security against tampering and steganalysis,\ndemonstrating VisGuard's competence in facilitating and safeguarding\nvisualization dissemination and information conveyance.", "AI": {"tldr": "VisGuard\u662f\u4e00\u79cd\u9632\u7be1\u6539VIDR\u6846\u67b6\uff0c\u53ef\u5c06\u5143\u6570\u636e\u94fe\u63a5\u53ef\u9760\u5730\u5d4c\u5165\u5230\u53ef\u89c6\u5316\u56fe\u50cf\u4e2d\uff0c\u5373\u4f7f\u5728\u56fe\u50cf\u88ab\u7be1\u6539\u540e\uff0c\u5d4c\u5165\u7684\u6570\u636e\u94fe\u63a5\u4ecd\u7136\u53ef\u4ee5\u6062\u590d\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5b9e\u7528\u6027\uff0c\u56e0\u4e3a\u5b83\u4eec\u5728\u5728\u7ebf\u5206\u53d1\u671f\u95f4\u5bb9\u6613\u53d7\u5230\u5e38\u89c1\u7684\u56fe\u50cf\u7be1\u6539\uff08\u5982\u88c1\u526a\u548c\u7f16\u8f91\uff09\u7684\u5f71\u54cd,\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86VisGuard\uff0c\u8fd9\u662f\u4e00\u4e2a\u9632\u7be1\u6539\u7684VIDR\u6846\u67b6\uff0c\u5b83\u53ef\u4ee5\u53ef\u9760\u5730\u5c06\u5143\u6570\u636e\u94fe\u63a5\u5d4c\u5165\u5230\u53ef\u89c6\u5316\u56fe\u50cf\u4e2d\u3002\u5373\u4f7f\u5728\u5bf9\u56fe\u50cf\u8fdb\u884c\u5927\u91cf\u7be1\u6539\u540e\uff0c\u5d4c\u5165\u7684\u6570\u636e\u94fe\u63a5\u4ecd\u7136\u53ef\u4ee5\u6062\u590d\u3002", "method": "\u91cd\u590d\u6570\u636e\u5e73\u94fa\uff0c\u53ef\u9006\u4fe1\u606f\u5e7f\u64ad\uff0c\u4ee5\u53ca\u57fa\u4e8e\u951a\u70b9\u7684\u88c1\u526a\u5b9a\u4f4d\u65b9\u6848", "result": "\u5d4c\u5165\u7684\u6570\u636e\u94fe\u63a5\u5373\u4f7f\u5728\u5bf9\u56fe\u50cf\u8fdb\u884c\u5927\u91cf\u7be1\u6539\u540e\u4ecd\u7136\u53ef\u4ee5\u6062\u590d\u3002VisGuard\u652f\u6301\u5404\u79cd\u5e94\u7528\uff0c\u5305\u62ec\u4ea4\u4e92\u5f0f\u56fe\u8868\u91cd\u5efa\u3001\u7be1\u6539\u68c0\u6d4b\u548c\u7248\u6743\u4fdd\u62a4\u3002", "conclusion": "VisGuard\u5728\u6570\u636e\u68c0\u7d22\u51c6\u786e\u6027\u3001\u5d4c\u5165\u5bb9\u91cf\u548c\u6297\u7be1\u6539\u548c\u9690\u5199\u5206\u6790\u7684\u5b89\u5168\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86VisGuard\u5728\u4fc3\u8fdb\u548c\u4fdd\u969c\u53ef\u89c6\u5316\u4f20\u64ad\u548c\u4fe1\u606f\u4f20\u9012\u65b9\u9762\u7684\u80fd\u529b\u3002"}}
{"id": "2507.14215", "categories": ["cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.14215", "abs": "https://arxiv.org/abs/2507.14215", "authors": ["Jiayu", "Liu"], "title": "Developing an AI-Guided Assistant Device for the Deaf and Hearing Impaired", "comment": null, "summary": "This study aims to develop a deep learning system for an accessibility device\nfor the deaf or hearing impaired. The device will accurately localize and\nidentify sound sources in real time. This study will fill an important gap in\ncurrent research by leveraging machine learning techniques to target the\nunderprivileged community. The system includes three main components. 1.\nJerryNet: A custom designed CNN architecture that determines the direction of\narrival (DoA) for nine possible directions. 2. Audio Classification: This model\nis based on fine-tuning the Contrastive Language-Audio Pretraining (CLAP) model\nto identify the exact sound classes only based on audio. 3. Multimodal\nintegration model: This is an accurate sound localization model that combines\naudio, visual, and text data to locate the exact sound sources in the images.\nThe part consists of two modules, one object detection using Yolov9 to generate\nall the bounding boxes of the objects, and an audio visual localization model\nto identify the optimal bounding box using complete Intersection over Union\n(CIoU). The hardware consists of a four-microphone rectangular formation and a\ncamera mounted on glasses with a wristband for displaying necessary information\nlike direction. On a custom collected data set, JerryNet achieved a precision\nof 91. 1% for the sound direction, outperforming all the baseline models. The\nCLAP model achieved 98.5% and 95% accuracy on custom and AudioSet datasets,\nrespectively. The audio-visual localization model within component 3 yielded a\ncIoU of 0.892 and an AUC of 0.658, surpassing other similar models. There are\nmany future potentials to this study, paving the way to creating a new\ngeneration of accessibility devices.", "AI": {"tldr": "Developed a deep learning system for an accessibility device for the deaf or hearing impaired, with high accuracy in sound direction, audio classification, and audio-visual localization.", "motivation": "This study aims to develop a deep learning system for an accessibility device for the deaf or hearing impaired and fill an important gap in current research by leveraging machine learning techniques to target the underprivileged community.", "method": "The system includes three main components: JerryNet, Audio Classification, and Multimodal integration model.", "result": "JerryNet achieved a precision of 91. 1% for the sound direction. The CLAP model achieved 98.5% and 95% accuracy on custom and AudioSet datasets, respectively. The audio-visual localization model within component 3 yielded a cIoU of 0.892 and an AUC of 0.658.", "conclusion": "This study has the potential to create a new generation of accessibility devices."}}
{"id": "2507.14520", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14520", "abs": "https://arxiv.org/abs/2507.14520", "authors": ["Xinyi Chen", "Yifei Yuan", "Jiaang Li", "Serge Belongie", "Maarten de Rijke", "Anders S\u00f8gaard"], "title": "What if Othello-Playing Language Models Could See?", "comment": "ICML 2025 Assessing World Models Workshop", "summary": "Language models are often said to face a symbol grounding problem. While some\nargue that world understanding can emerge from text alone, others suggest\ngrounded learning is more efficient. We explore this through Othello, where the\nboard state defines a simplified, rule-based world. Building on prior work, we\nintroduce VISOTHELLO, a multi-modal model trained on move histories and board\nimages. Using next-move prediction, we compare it to mono-modal baselines and\ntest robustness to semantically irrelevant perturbations. We find that\nmulti-modal training improves both performance and the robustness of internal\nrepresentations. These results suggest that grounding language in visual input\nhelps models infer structured world representations.", "AI": {"tldr": "\u591a\u6a21\u6001\u8bad\u7ec3\u901a\u8fc7\u89c6\u89c9\u8f93\u5165\u5e2e\u52a9\u8bed\u8a00\u6a21\u578b\u66f4\u597d\u5730\u7406\u89e3\u4e16\u754c\u3002", "motivation": "\u63a2\u8ba8\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u7b26\u53f7 grounding \u95ee\u9898\u3002\u7814\u7a76\u4e16\u754c\u7406\u89e3\u662f\u5426\u53ef\u4ee5\u4ec5\u4ece\u6587\u672c\u4e2d\u51fa\u73b0\uff0c\u6216\u8005 grounded learning \u662f\u5426\u66f4\u6709\u6548\u3002", "method": "\u5f15\u5165VISOTHELLO\uff0c\u4e00\u4e2a\u5728\u79fb\u52a8\u5386\u53f2\u548c\u68cb\u76d8\u56fe\u50cf\u4e0a\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u6a21\u578b\u3002\u4f7f\u7528\u4e0b\u4e00\u6b65\u79fb\u52a8\u9884\u6d4b\uff0c\u5c06\u5176\u4e0e\u5355\u6a21\u6001\u57fa\u7ebf\u8fdb\u884c\u6bd4\u8f83\uff0c\u5e76\u6d4b\u8bd5\u5bf9\u8bed\u4e49\u65e0\u5173\u6270\u52a8\u7684\u9c81\u68d2\u6027\u3002", "result": "\u591a\u6a21\u6001\u8bad\u7ec3\u63d0\u9ad8\u4e86\u6027\u80fd\u548c\u5185\u90e8\u8868\u5f81\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u591a\u6a21\u6001\u8bad\u7ec3\u53ef\u4ee5\u63d0\u9ad8\u6027\u80fd\u548c\u5185\u90e8\u8868\u5f81\u7684\u9c81\u68d2\u6027\u3002\u5c06\u8bed\u8a00\u7f6e\u4e8e\u89c6\u89c9\u8f93\u5165\u4e2d\u6709\u52a9\u4e8e\u6a21\u578b\u63a8\u65ad\u7ed3\u6784\u5316\u7684\u4e16\u754c\u8868\u5f81\u3002"}}
{"id": "2507.15042", "categories": ["cs.AI", "cs.IR", "I.2.7; H.3.3; K.6.5"], "pdf": "https://arxiv.org/pdf/2507.15042", "abs": "https://arxiv.org/abs/2507.15042", "authors": ["Jerry Wang", "Fang Yu"], "title": "DeRAG: Black-box Adversarial Attacks on Multiple Retrieval-Augmented Generation Applications via Prompt Injection", "comment": "Accepted by KDD Workshop on Prompt Optimization 2025", "summary": "Adversarial prompt attacks can significantly alter the reliability of\nRetrieval-Augmented Generation (RAG) systems by re-ranking them to produce\nincorrect outputs. In this paper, we present a novel method that applies\nDifferential Evolution (DE) to optimize adversarial prompt suffixes for\nRAG-based question answering. Our approach is gradient-free, treating the RAG\npipeline as a black box and evolving a population of candidate suffixes to\nmaximize the retrieval rank of a targeted incorrect document to be closer to\nreal world scenarios. We conducted experiments on the BEIR QA datasets to\nevaluate attack success at certain retrieval rank thresholds under multiple\nretrieving applications. Our results demonstrate that DE-based prompt\noptimization attains competitive (and in some cases higher) success rates\ncompared to GGPP to dense retrievers and PRADA to sparse retrievers, while\nusing only a small number of tokens (<=5 tokens) in the adversarial suffix.\nFurthermore, we introduce a readability-aware suffix construction strategy,\nvalidated by a statistically significant reduction in MLM negative\nlog-likelihood with Welch's t-test. Through evaluations with a BERT-based\nadversarial suffix detector, we show that DE-generated suffixes evade\ndetection, yielding near-chance detection accuracy.", "AI": {"tldr": "This paper introduces a novel method using Differential Evolution (DE) to generate adversarial prompt suffixes that can effectively attack RAG systems, achieving competitive success rates with small token lengths and evading detection.", "motivation": "Adversarial prompt attacks can significantly alter the reliability of RAG systems by re-ranking them to produce incorrect outputs.", "method": "applies Differential Evolution (DE) to optimize adversarial prompt suffixes", "result": "DE-based prompt optimization attains competitive (and in some cases higher) success rates compared to GGPP to dense retrievers and PRADA to sparse retrievers, while using only a small number of tokens (<=5 tokens) in the adversarial suffix.", "conclusion": "DE-generated suffixes evade detection, yielding near-chance detection accuracy."}}
{"id": "2507.14355", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14355", "abs": "https://arxiv.org/abs/2507.14355", "authors": ["Jianfeng Zhu", "Ruoming Jin", "Karin G. Coifman"], "title": "Can LLMs Infer Personality from Real World Conversations?", "comment": "21 pages, 12 figures", "summary": "Large Language Models (LLMs) such as OpenAI's GPT-4 and Meta's LLaMA offer a\npromising approach for scalable personality assessment from open-ended\nlanguage. However, inferring personality traits remains challenging, and\nearlier work often relied on synthetic data or social media text lacking\npsychometric validity. We introduce a real-world benchmark of 555\nsemi-structured interviews with BFI-10 self-report scores for evaluating\nLLM-based personality inference. Three state-of-the-art LLMs (GPT-4.1 Mini,\nMeta-LLaMA, and DeepSeek) were tested using zero-shot prompting for BFI-10 item\nprediction and both zero-shot and chain-of-thought prompting for Big Five trait\ninference. All models showed high test-retest reliability, but construct\nvalidity was limited: correlations with ground-truth scores were weak (max\nPearson's $r = 0.27$), interrater agreement was low (Cohen's $\\kappa < 0.10$),\nand predictions were biased toward moderate or high trait levels.\nChain-of-thought prompting and longer input context modestly improved\ndistributional alignment, but not trait-level accuracy. These results\nunderscore limitations in current LLM-based personality inference and highlight\nthe need for evidence-based development for psychological applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86LLM\u5728\u6027\u683c\u63a8\u65ad\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u5176\u4fe1\u5ea6\u9ad8\u4f46\u6548\u5ea6\u6709\u9650\uff0c\u8868\u660e\u5f53\u524dLLM\u5728\u5fc3\u7406\u5b66\u5e94\u7528\u4e2d\u5b58\u5728\u5c40\u9650\u6027\u3002", "motivation": "\u5229\u7528\u5f00\u653e\u5f0f\u8bed\u8a00\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u5982OpenAI\u7684GPT-4\u548cMeta\u7684LLaMA\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u6027\u683c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u63a8\u65ad\u6027\u683c\u7279\u5f81\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u800c\u4e14\u65e9\u671f\u5de5\u4f5c\u901a\u5e38\u4f9d\u8d56\u4e8e\u7f3a\u4e4f\u5fc3\u7406\u6d4b\u91cf\u6709\u6548\u6027\u7684\u5408\u6210\u6570\u636e\u6216\u793e\u4ea4\u5a92\u4f53\u6587\u672c\u3002", "method": "\u4f7f\u7528BFI-10\u81ea\u6211\u62a5\u544a\u8bc4\u5206\uff0c\u901a\u8fc7\u96f6\u6837\u672c\u63d0\u793a\u8fdb\u884cBFI-10\u9879\u76ee\u9884\u6d4b\uff0c\u5e76\u901a\u8fc7\u96f6\u6837\u672c\u548c\u601d\u7ef4\u94fe\u63d0\u793a\u8fdb\u884cBig Five\u7279\u8d28\u63a8\u65ad\uff0c\u6d4b\u8bd5\u4e86\u4e09\u79cd\u6700\u5148\u8fdb\u7684LLM\uff08GPT-4.1 Mini\u3001Meta-LLaMA\u548cDeepSeek\uff09\u3002", "result": "\u6240\u6709\u6a21\u578b\u90fd\u8868\u73b0\u51fa\u5f88\u9ad8\u7684\u91cd\u6d4b\u4fe1\u5ea6\uff0c\u4f46\u7ed3\u6784\u6548\u5ea6\u6709\u9650\uff1a\u4e0e\u771f\u5b9e\u5206\u6570\u7684\u76f8\u5173\u6027\u8f83\u5f31\uff08\u6700\u5927Pearson's r = 0.27\uff09\uff0c\u8bc4\u5206\u8005\u95f4\u4e00\u81f4\u6027\u8f83\u4f4e\uff08Cohen's \u03ba < 0.10\uff09\uff0c\u5e76\u4e14\u9884\u6d4b\u504f\u5411\u4e8e\u4e2d\u7b49\u6216\u9ad8\u7279\u8d28\u6c34\u5e73\u3002\u601d\u7ef4\u94fe\u63d0\u793a\u548c\u66f4\u957f\u7684\u8f93\u5165\u4e0a\u4e0b\u6587\u9002\u5ea6\u5730\u6539\u5584\u4e86\u5206\u5e03\u5bf9\u9f50\uff0c\u4f46\u6ca1\u6709\u6539\u5584\u7279\u8d28\u6c34\u5e73\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u6027\u683c\u63a8\u65ad\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5fc3\u7406\u5b66\u5e94\u7528\u9700\u8981\u57fa\u4e8e\u8bc1\u636e\u7684\u5f00\u53d1\u3002"}}
{"id": "2507.14477", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14477", "abs": "https://arxiv.org/abs/2507.14477", "authors": ["Zhenyu Li", "Tianyi Shang", "Pengjie Xu", "Ruirui Zhang", "Fanchen Kong"], "title": "OptiCorNet: Optimizing Sequence-Based Context Correlation for Visual Place Recognition", "comment": "5 figures", "summary": "Visual Place Recognition (VPR) in dynamic and perceptually aliased\nenvironments remains a fundamental challenge for long-term localization.\nExisting deep learning-based solutions predominantly focus on single-frame\nembeddings, neglecting the temporal coherence present in image sequences. This\npaper presents OptiCorNet, a novel sequence modeling framework that unifies\nspatial feature extraction and temporal differencing into a differentiable,\nend-to-end trainable module. Central to our approach is a lightweight 1D\nconvolutional encoder combined with a learnable differential temporal operator,\ntermed Differentiable Sequence Delta (DSD), which jointly captures short-term\nspatial context and long-range temporal transitions. The DSD module models\ndirectional differences across sequences via a fixed-weight differencing\nkernel, followed by an LSTM-based refinement and optional residual projection,\nyielding compact, discriminative descriptors robust to viewpoint and appearance\nshifts. To further enhance inter-class separability, we incorporate a\nquadruplet loss that optimizes both positive alignment and multi-negative\ndivergence within each batch. Unlike prior VPR methods that treat temporal\naggregation as post-processing, OptiCorNet learns sequence-level embeddings\ndirectly, enabling more effective end-to-end place recognition. Comprehensive\nevaluations on multiple public benchmarks demonstrate that our approach\noutperforms state-of-the-art baselines under challenging seasonal and viewpoint\nvariations.", "AI": {"tldr": "OptiCorNet \u662f\u4e00\u79cd\u7528\u4e8e\u89c6\u89c9\u4f4d\u7f6e\u8bc6\u522b\u7684\u65b0\u578b\u5e8f\u5217\u5efa\u6a21\u6846\u67b6\uff0c\u5b83\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u3002", "motivation": "\u5728\u52a8\u6001\u548c\u611f\u77e5\u6df7\u53e0\u73af\u5883\u4e2d\u8fdb\u884c\u89c6\u89c9\u4f4d\u7f6e\u8bc6\u522b (VPR) \u4ecd\u7136\u662f\u957f\u671f\u5b9a\u4f4d\u7684\u4e00\u4e2a\u6839\u672c\u6311\u6218\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u89e3\u51b3\u65b9\u6848\u4e3b\u8981\u5173\u6ce8\u5355\u5e27\u5d4c\u5165\uff0c\u5ffd\u7565\u4e86\u56fe\u50cf\u5e8f\u5217\u4e2d\u5b58\u5728\u7684\u65f6\u95f4\u8fde\u8d2f\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5e8f\u5217\u5efa\u6a21\u6846\u67b6 OptiCorNet\uff0c\u8be5\u6846\u67b6\u5c06\u7a7a\u95f4\u7279\u5f81\u63d0\u53d6\u548c\u65f6\u95f4\u5dee\u5206\u7edf\u4e00\u5230\u4e00\u4e2a\u53ef\u5fae\u7684\u7aef\u5230\u7aef\u53ef\u8bad\u7ec3\u6a21\u5757\u4e2d\u3002\u8be5\u65b9\u6cd5\u7684\u6838\u5fc3\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7 1D \u5377\u79ef\u7f16\u7801\u5668\uff0c\u7ed3\u5408\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u5fae\u5206\u65f6\u95f4\u7b97\u5b50\uff0c\u79f0\u4e3a\u53ef\u5fae\u5e8f\u5217 Delta (DSD)\uff0c\u5b83\u5171\u540c\u6355\u83b7\u77ed\u671f\u7a7a\u95f4\u4e0a\u4e0b\u6587\u548c\u957f\u671f\u65f6\u95f4\u8f6c\u6362\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5171\u57fa\u51c6\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u5b63\u8282\u548c\u89c6\u70b9\u53d8\u5316\u4e0b\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u3002"}}
{"id": "2507.14217", "categories": ["cs.LG", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.14217", "abs": "https://arxiv.org/abs/2507.14217", "authors": ["Tudor Matei Opran", "Samir Loudni"], "title": "Geometry-Aware Active Learning of Pattern Rankings via Choquet-Based Aggregation", "comment": null, "summary": "We address the pattern explosion problem in pattern mining by proposing an\ninteractive learning framework that combines nonlinear utility aggregation with\ngeometry-aware query selection. Our method models user preferences through a\nChoquet integral over multiple interestingness measures and exploits the\ngeometric structure of the version space to guide the selection of informative\ncomparisons. A branch-and-bound strategy with tight distance bounds enables\nefficient identification of queries near the decision boundary. Experiments on\nUCI datasets show that our approach outperforms existing methods such as\nChoquetRank, achieving better ranking accuracy with fewer user interactions.", "AI": {"tldr": "interactive learning framework", "motivation": "address the pattern explosion problem in pattern mining", "method": "nonlinear utility aggregation with geometry-aware query selection", "result": "Experiments on UCI datasets", "conclusion": "The proposed approach outperforms existing methods such as ChoquetRank, achieving better ranking accuracy with fewer user interactions."}}
{"id": "2507.14552", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14552", "abs": "https://arxiv.org/abs/2507.14552", "authors": ["Anna Sofia Lippolis", "Mohammad Javad Saeedizade", "Robin Keskis\u00e4rkk\u00e4", "Aldo Gangemi", "Eva Blomqvist", "Andrea Giovanni Nuzzolese"], "title": "Large Language Models Assisting Ontology Evaluation", "comment": null, "summary": "Ontology evaluation through functional requirements, such as testing via\ncompetency question (CQ) verification, is a well-established yet costly,\nlabour-intensive, and error-prone endeavour, even for ontology engineering\nexperts. In this work, we introduce OE-Assist, a novel framework designed to\nassist ontology evaluation through automated and semi-automated CQ\nverification. By presenting and leveraging a dataset of 1,393 CQs paired with\ncorresponding ontologies and ontology stories, our contributions present, to\nour knowledge, the first systematic investigation into large language model\n(LLM)-assisted ontology evaluation, and include: (i) evaluating the\neffectiveness of a LLM-based approach for automatically performing CQ\nverification against a manually created gold standard, and (ii) developing and\nassessing an LLM-powered framework to assist CQ verification with Prot\\'eg\\'e,\nby providing suggestions. We found that automated LLM-based evaluation with\no1-preview and o3-mini perform at a similar level to the average user's\nperformance.", "AI": {"tldr": "This paper introduces OE-Assist, a framework for LLM-assisted ontology evaluation through automated CQ verification, finding LLM performance similar to average users.", "motivation": "Ontology evaluation through functional requirements is a well-established yet costly, labour-intensive, and error-prone endeavour.", "method": "introducing OE-Assist, a novel framework designed to assist ontology evaluation through automated and semi-automated CQ verification", "result": "evaluated the effectiveness of a LLM-based approach for automatically performing CQ verification against a manually created gold standard, and developing and assessing an LLM-powered framework to assist CQ verification with Prot\u00e9g\u00e9, by providing suggestions.", "conclusion": "Automated LLM-based evaluation performs at a similar level to the average user's performance."}}
{"id": "2507.15742", "categories": ["cs.CL", "cs.IR", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2507.15742", "abs": "https://arxiv.org/abs/2507.15742", "authors": ["Paul Sheridan", "Zeyad Ahmed", "Aitazaz A. Farooque"], "title": "A Fisher's exact test justification of the TF-IDF term-weighting scheme", "comment": "23 pages, 4 tables", "summary": "Term frequency-inverse document frequency, or TF-IDF for short, is arguably\nthe most celebrated mathematical expression in the history of information\nretrieval. Conceived as a simple heuristic quantifying the extent to which a\ngiven term's occurrences are concentrated in any one given document out of\nmany, TF-IDF and its many variants are routinely used as term-weighting schemes\nin diverse text analysis applications. There is a growing body of scholarship\ndedicated to placing TF-IDF on a sound theoretical foundation. Building on that\ntradition, this paper justifies the use of TF-IDF to the statistics community\nby demonstrating how the famed expression can be understood from a significance\ntesting perspective. We show that the common TF-IDF variant TF-ICF is, under\nmild regularity conditions, closely related to the negative logarithm of the\n$p$-value from a one-tailed version of Fisher's exact test of statistical\nsignificance. As a corollary, we establish a connection between TF-IDF and the\nsaid negative log-transformed $p$-value under certain idealized assumptions. We\nfurther demonstrate, as a limiting case, that this same quantity converges to\nTF-IDF in the limit of an infinitely large document collection. The Fisher's\nexact test justification of TF-IDF equips the working statistician with a ready\nexplanation of the term-weighting scheme's long-established effectiveness.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5c55\u793a TF-IDF \u5982\u4f55\u4ece\u663e\u7740\u6027\u68c0\u9a8c\u7684\u89d2\u5ea6\u6765\u7406\u89e3\uff0c\u4ece\u800c\u4e3a TF-IDF \u5728\u7edf\u8ba1\u793e\u533a\u4e2d\u7684\u4f7f\u7528\u63d0\u4f9b\u4e86\u7406\u7531\u3002", "motivation": "TF-IDF \u53ca\u5176\u8bb8\u591a\u53d8\u4f53\u901a\u5e38\u7528\u4f5c\u5404\u79cd\u6587\u672c\u5206\u6790\u5e94\u7528\u4e2d\u7684\u672f\u8bed\u52a0\u6743\u65b9\u6848\u3002\u8d8a\u6765\u8d8a\u591a\u7684\u5b66\u672f\u7814\u7a76\u81f4\u529b\u4e8e\u4e3a TF-IDF \u5960\u5b9a\u826f\u597d\u7684\u7406\u8bba\u57fa\u7840\u3002", "method": "\u901a\u8fc7\u4ece\u663e\u7740\u6027\u68c0\u9a8c\u7684\u89d2\u5ea6\u5c55\u793a\u5982\u4f55\u7406\u89e3\u8457\u540d\u7684\u8868\u8fbe\u5f0f\uff0c\u8bc1\u660e\u4e86 TF-IDF \u5728\u7edf\u8ba1\u793e\u533a\u4e2d\u7684\u4f7f\u7528\u662f\u5408\u7406\u7684\u3002", "result": "\u5e38\u89c1\u7684 TF-IDF \u53d8\u4f53 TF-ICF \u5728\u6e29\u548c\u7684\u6b63\u5219\u6027\u6761\u4ef6\u4e0b\uff0c\u4e0e Fisher \u7edf\u8ba1\u663e\u7740\u6027\u7cbe\u786e\u68c0\u9a8c\u7684\u5355\u5c3e\u7248\u672c\u7684 p \u503c\u7684\u8d1f\u5bf9\u6570\u5bc6\u5207\u76f8\u5173\u3002\u5728\u67d0\u4e9b\u7406\u60f3\u5316\u5047\u8bbe\u4e0b\uff0cTF-IDF \u4e0e\u6240\u8ff0\u8d1f\u5bf9\u6570\u8f6c\u6362 p \u503c\u4e4b\u95f4\u5efa\u7acb\u4e86\u8054\u7cfb\u3002\u4f5c\u4e3a\u4e00\u79cd\u9650\u5236\u60c5\u51b5\uff0c\u8bc1\u660e\u4e86\u5728\u65e0\u9650\u5927\u7684\u6587\u6863\u96c6\u5408\u7684\u9650\u5236\u4e0b\uff0c\u540c\u4e00\u6570\u91cf\u6536\u655b\u5230 TF-IDF\u3002", "conclusion": "TF-IDF \u7684 Fisher \u7cbe\u786e\u68c0\u9a8c\u8bc1\u660e\u4e3a\u5de5\u4f5c\u7edf\u8ba1\u5b66\u5bb6\u63d0\u4f9b\u4e86\u89e3\u91ca\u672f\u8bed\u6743\u91cd\u65b9\u6848\u957f\u671f\u6709\u6548\u6027\u7684\u73b0\u6210\u65b9\u6cd5\u3002"}}
{"id": "2507.14481", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14481", "abs": "https://arxiv.org/abs/2507.14481", "authors": ["Yujia Tong", "Jingling Yuan", "Tian Zhang", "Jianquan Liu", "Chuang Hu"], "title": "DFQ-ViT: Data-Free Quantization for Vision Transformers without Fine-tuning", "comment": null, "summary": "Data-Free Quantization (DFQ) enables the quantization of Vision Transformers\n(ViTs) without requiring access to data, allowing for the deployment of ViTs on\ndevices with limited resources. In DFQ, the quantization model must be\ncalibrated using synthetic samples, making the quality of these synthetic\nsamples crucial. Existing methods fail to fully capture and balance the global\nand local features within the samples, resulting in limited synthetic data\nquality. Moreover, we have found that during inference, there is a significant\ndifference in the distributions of intermediate layer activations between the\nquantized and full-precision models. These issues lead to a severe performance\ndegradation of the quantized model. To address these problems, we propose a\npipeline for Data-Free Quantization for Vision Transformers (DFQ-ViT).\nSpecifically, we synthesize samples in order of increasing difficulty,\neffectively enhancing the quality of synthetic data. During the calibration and\ninference stage, we introduce the activation correction matrix for the\nquantized model to align the intermediate layer activations with those of the\nfull-precision model. Extensive experiments demonstrate that DFQ-ViT achieves\nremarkable superiority over existing DFQ methods and its performance is on par\nwith models quantized through real data. For example, the performance of DeiT-T\nwith 3-bit weights quantization is 4.29% higher than the state-of-the-art. Our\nmethod eliminates the need for fine-tuning, which not only reduces\ncomputational overhead but also lowers the deployment barriers for edge\ndevices. This characteristic aligns with the principles of Green Learning by\nimproving energy efficiency and facilitating real-world applications in\nresource-constrained environments.", "AI": {"tldr": "DFQ-ViT\u662f\u4e00\u79cd\u7528\u4e8eViT\u7684\u65e0\u6570\u636e\u91cf\u5316\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u6539\u8fdb\u5408\u6210\u6570\u636e\u8d28\u91cf\u548c\u6821\u6b63\u6fc0\u6d3b\u6765\u63d0\u9ad8\u91cf\u5316\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u5fae\u8c03\u3002", "motivation": "\u73b0\u6709DFQ\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u6355\u6349\u548c\u5e73\u8861\u6837\u672c\u4e2d\u7684\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\uff0c\u5bfc\u81f4\u5408\u6210\u6570\u636e\u8d28\u91cf\u53d7\u9650\u3002\u91cf\u5316\u6a21\u578b\u548c\u5168\u7cbe\u5ea6\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u4e2d\u95f4\u5c42\u6fc0\u6d3b\u5206\u5e03\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5bfc\u81f4\u91cf\u5316\u6a21\u578b\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\u3002", "method": "\u63d0\u51faDFQ-ViT\u6d41\u7a0b\uff0c\u901a\u8fc7\u96be\u5ea6\u9012\u589e\u7684\u65b9\u5f0f\u5408\u6210\u6837\u672c\u4ee5\u63d0\u9ad8\u5408\u6210\u6570\u636e\u8d28\u91cf\uff0c\u5e76\u5f15\u5165\u6fc0\u6d3b\u6821\u6b63\u77e9\u9635\u4ee5\u5bf9\u9f50\u91cf\u5316\u6a21\u578b\u548c\u5168\u7cbe\u5ea6\u6a21\u578b\u7684\u4e2d\u95f4\u5c42\u6fc0\u6d3b\u3002", "result": "DeiT-T\u57283\u6bd4\u7279\u6743\u91cd\u91cf\u5316\u4e0b\u7684\u6027\u80fd\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u9ad84.29%\u3002", "conclusion": "DFQ-ViT\u5728ViT\u91cf\u5316\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709DFQ\u65b9\u6cd5\uff0c\u6027\u80fd\u4e0e\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u91cf\u5316\u7684\u6a21\u578b\u76f8\u5f53\uff0c\u4e14\u65e0\u9700\u5fae\u8c03\uff0c\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\u548c\u90e8\u7f72\u95e8\u69db\uff0c\u7b26\u5408\u7eff\u8272\u5b66\u4e60\u539f\u5219\u3002"}}
{"id": "2507.14219", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.14219", "abs": "https://arxiv.org/abs/2507.14219", "authors": ["Obumneme Zimuzor Nwafor", "Mohammed Abdul Majeed Al Hooti"], "title": "Artificial Intelligence for Green Hydrogen Yield Prediction and Site Suitability using SHAP-Based Composite Index: Focus on Oman", "comment": null, "summary": "As nations seek sustainable alternatives to fossil fuels, green hydrogen has\nemerged as a promising strategic pathway toward decarbonisation, particularly\nin solar-rich arid regions. However, identifying optimal locations for hydrogen\nproduction requires the integration of complex environmental, atmospheric, and\ninfrastructural factors, often compounded by limited availability of direct\nhydrogen yield data. This study presents a novel Artificial Intelligence (AI)\nframework for computing green hydrogen yield and site suitability index using\nmean absolute SHAP (SHapley Additive exPlanations) values. This framework\nconsists of a multi-stage pipeline of unsupervised multi-variable clustering,\nsupervised machine learning classifier and SHAP algorithm. The pipeline trains\non an integrated meteorological, topographic and temporal dataset and the\nresults revealed distinct spatial patterns of suitability and relative\ninfluence of the variables. With model predictive accuracy of 98%, the result\nalso showed that water proximity, elevation and seasonal variation are the most\ninfluential factors determining green hydrogen site suitability in Oman with\nmean absolute shap values of 2.470891, 2.376296 and 1.273216 respectively.\nGiven limited or absence of ground-truth yield data in many countries that have\ngreen hydrogen prospects and ambitions, this study offers an objective and\nreproducible alternative to subjective expert weightings, thus allowing the\ndata to speak for itself and potentially discover novel latent groupings\nwithout pre-imposed assumptions. This study offers industry stakeholders and\npolicymakers a replicable and scalable tool for green hydrogen infrastructure\nplanning and other decision making in data-scarce regions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u4eba\u5de5\u667a\u80fd\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u6c14\u8c61\u3001\u5730\u5f62\u548c\u65f6\u95f4\u6570\u636e\uff0c\u4e3a\u7eff\u8272\u6c22\u6c14\u751f\u4ea7\u9009\u5740\u63d0\u4f9b\u5ba2\u89c2\u3001\u53ef\u590d\u5236\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u6570\u636e\u7a00\u7f3a\u5730\u533a\u3002", "motivation": "\u786e\u5b9a\u6c22\u6c14\u751f\u4ea7\u7684\u6700\u4f73\u4f4d\u7f6e\u9700\u8981\u6574\u5408\u590d\u6742\u7684\u73af\u5883\u3001\u5927\u6c14\u548c\u57fa\u7840\u8bbe\u65bd\u56e0\u7d20\uff0c\u5e76\u4e14\u901a\u5e38\u53d7\u5230\u76f4\u63a5\u6c22\u6c14\u4ea7\u91cf\u6570\u636e\u6709\u9650\u6027\u7684\u5f71\u54cd\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u4eba\u5de5\u667a\u80fd\u6846\u67b6\uff0c\u7528\u4e8e\u8ba1\u7b97\u7eff\u8272\u6c22\u4ea7\u91cf\u548c\u573a\u5730\u9002\u5b9c\u6027\u6307\u6570\uff0c\u4f7f\u7528\u5e73\u5747\u7edd\u5bf9SHAP\u503c\u3002\u8be5\u6846\u67b6\u5305\u62ec\u4e00\u4e2a\u591a\u9636\u6bb5\u7ba1\u9053\uff0c\u5305\u542b\u65e0\u76d1\u7763\u591a\u53d8\u91cf\u805a\u7c7b\u3001\u76d1\u7763\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u548cSHAP\u7b97\u6cd5\u3002", "result": "\u8be5\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u9002\u5b9c\u6027\u7684\u660e\u663e\u7a7a\u95f4\u6a21\u5f0f\u4ee5\u53ca\u53d8\u91cf\u7684\u76f8\u5bf9\u5f71\u54cd\u3002\u6a21\u578b\u9884\u6d4b\u51c6\u786e\u7387\u8fbe\u523098%\uff0c\u7ed3\u679c\u8fd8\u8868\u660e\uff0c\u6c34 proximity\u3001\u6d77\u62d4\u548c\u5b63\u8282\u53d8\u5316\u662f\u51b3\u5b9a\u963f\u66fc\u7eff\u8272\u6c22\u6c14\u573a\u5730\u9002\u5b9c\u6027\u7684\u6700\u91cd\u8981\u56e0\u7d20\uff0c\u5176\u5e73\u5747\u7edd\u5bf9SHAP\u503c\u5206\u522b\u4e3a2.470891\u30012.376296\u548c1.273216\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7eff\u8272\u6c22\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u548c\u51b3\u7b56\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u590d\u5236\u548c\u53ef\u6269\u5c55\u7684\u5de5\u5177\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u7a00\u7f3a\u5730\u533a\u3002"}}
{"id": "2507.14593", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14593", "abs": "https://arxiv.org/abs/2507.14593", "authors": ["Omar Al-Desi"], "title": "Coordinate Heart System: A Geometric Framework for Emotion Representation", "comment": "26 pages", "summary": "This paper presents the Coordinate Heart System (CHS), a geometric framework\nfor emotion representation in artificial intelligence applications. We position\neight core emotions as coordinates on a unit circle, enabling mathematical\ncomputation of complex emotional states through coordinate mixing and vector\noperations. Our initial five-emotion model revealed significant coverage gaps\nin the emotion space, leading to the development of an eight-emotion system\nthat provides complete geometric coverage with mathematical guarantees. The\nframework converts natural language input to emotion coordinates and supports\nreal-time emotion interpolation through computational algorithms. The system\nintroduces a re-calibrated stability parameter S in [0,1], which dynamically\nintegrates emotional load, conflict resolution, and contextual drain factors.\nThis stability model leverages advanced Large Language Model interpretation of\ntextual cues and incorporates hybrid temporal tracking mechanisms to provide\nnuanced assessment of psychological well-being states. Our key contributions\ninclude: (i) mathematical proof demonstrating why five emotions are\ninsufficient for complete geometric coverage, (ii) an eight-coordinate system\nthat eliminates representational blind spots, (iii) novel algorithms for\nemotion mixing, conflict resolution, and distance calculation in emotion space,\nand (iv) a comprehensive computational framework for AI emotion recognition\nwith enhanced multi-dimensional stability modeling. Experimental validation\nthrough case studies demonstrates the system's capability to handle emotionally\nconflicted states, contextual distress factors, and complex psychological\nscenarios that traditional categorical emotion models cannot adequately\nrepresent. This work establishes a new mathematical foundation for emotion\nmodeling in artificial intelligence systems.", "AI": {"tldr": "introduces a geometric framework for emotion representation using eight core emotions on a unit circle, enabling mathematical computation of complex emotional states. It addresses coverage gaps in previous models and provides enhanced stability modeling for AI emotion recognition.", "motivation": "significant coverage gaps in the emotion space, leading to the development of an eight-emotion system that provides complete geometric coverage with mathematical guarantees", "method": "presents the Coordinate Heart System (CHS), a geometric framework for emotion representation", "result": "an eight-coordinate system that eliminates representational blind spots, novel algorithms for emotion mixing, conflict resolution, and distance calculation in emotion space, and a comprehensive computational framework for AI emotion recognition with enhanced multi-dimensional stability modeling", "conclusion": "This work establishes a new mathematical foundation for emotion modeling in artificial intelligence systems."}}
{"id": "2507.14374", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14374", "abs": "https://arxiv.org/abs/2507.14374", "authors": ["Sinchani Chakraborty", "Sudeshna Sarkar", "Pawan Goyal"], "title": "Error-Aware Curriculum Learning for Biomedical Relation Classification", "comment": "16 pages, 2 figures", "summary": "Relation Classification (RC) in biomedical texts is essential for\nconstructing knowledge graphs and enabling applications such as drug\nrepurposing and clinical decision-making. We propose an error-aware\nteacher--student framework that improves RC through structured guidance from a\nlarge language model (GPT-4o). Prediction failures from a baseline student\nmodel are analyzed by the teacher to classify error types, assign difficulty\nscores, and generate targeted remediations, including sentence rewrites and\nsuggestions for KG-based enrichment. These enriched annotations are used to\ntrain a first student model via instruction tuning. This model then annotates a\nbroader dataset with difficulty scores and remediation-enhanced inputs. A\nsecond student is subsequently trained via curriculum learning on this dataset,\nordered by difficulty, to promote robust and progressive learning. We also\nconstruct a heterogeneous biomedical knowledge graph from PubMed abstracts to\nsupport context-aware RC. Our approach achieves new state-of-the-art\nperformance on 4 of 5 PPI datasets and the DDI dataset, while remaining\ncompetitive on ChemProt.", "AI": {"tldr": "This paper introduces an error-aware teacher-student framework using GPT-4o for relation classification in biomedical texts, achieving state-of-the-art results on PPI and DDI datasets.", "motivation": "Relation Classification (RC) in biomedical texts is essential for constructing knowledge graphs and enabling applications such as drug repurposing and clinical decision-making.", "method": "An error-aware teacher--student framework is proposed, which uses GPT-4o to analyze prediction failures, classify error types, assign difficulty scores, and generate targeted remediations. Curriculum learning is then applied to train a second student model.", "result": "The approach achieves new state-of-the-art performance on 4 of 5 PPI datasets and the DDI dataset, while remaining competitive on ChemProt.", "conclusion": "The proposed error-aware teacher--student framework achieves new state-of-the-art performance on 4 of 5 PPI datasets and the DDI dataset, while remaining competitive on ChemProt."}}
{"id": "2507.14485", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14485", "abs": "https://arxiv.org/abs/2507.14485", "authors": ["Hongye Hou", "Liu Zhan", "Yang Yang"], "title": "Benefit from Reference: Retrieval-Augmented Cross-modal Point Cloud Completion", "comment": null, "summary": "Completing the whole 3D structure based on an incomplete point cloud is a\nchallenging task, particularly when the residual point cloud lacks typical\nstructural characteristics. Recent methods based on cross-modal learning\nattempt to introduce instance images to aid the structure feature learning.\nHowever, they still focus on each particular input class, limiting their\ngeneration abilities. In this work, we propose a novel retrieval-augmented\npoint cloud completion framework. The core idea is to incorporate cross-modal\nretrieval into completion task to learn structural prior information from\nsimilar reference samples. Specifically, we design a Structural Shared Feature\nEncoder (SSFE) to jointly extract cross-modal features and reconstruct\nreference features as priors. Benefiting from a dual-channel control gate in\nthe encoder, relevant structural features in the reference sample are enhanced\nand irrelevant information interference is suppressed. In addition, we propose\na Progressive Retrieval-Augmented Generator (PRAG) that employs a hierarchical\nfeature fusion mechanism to integrate reference prior information with input\nfeatures from global to local. Through extensive evaluations on multiple\ndatasets and real-world scenes, our method shows its effectiveness in\ngenerating fine-grained point clouds, as well as its generalization capability\nin handling sparse data and unseen categories.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u68c0\u7d22\u589e\u5f3a\u70b9\u4e91\u8865\u5168\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8de8\u6a21\u6001\u68c0\u7d22\u548c\u5206\u5c42\u7279\u5f81\u878d\u5408\u673a\u5236\uff0c\u4ece\u53c2\u8003\u6837\u672c\u4e2d\u5b66\u4e60\u7ed3\u6784\u5148\u9a8c\u4fe1\u606f\uff0c\u4ece\u800c\u6709\u6548\u5730\u751f\u6210\u7ec6\u7c92\u5ea6\u70b9\u4e91\u5e76\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u57fa\u4e8e\u4e0d\u5b8c\u6574\u70b9\u4e91\u5b8c\u6210\u6574\u4e2a 3D \u7ed3\u6784\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u7279\u522b\u662f\u5f53\u6b8b\u4f59\u70b9\u4e91\u7f3a\u4e4f\u5178\u578b\u7684\u7ed3\u6784\u7279\u5f81\u65f6\u3002\u6700\u8fd1\u57fa\u4e8e\u8de8\u6a21\u6001\u5b66\u4e60\u7684\u65b9\u6cd5\u8bd5\u56fe\u5f15\u5165\u5b9e\u4f8b\u56fe\u50cf\u6765\u5e2e\u52a9\u7ed3\u6784\u7279\u5f81\u5b66\u4e60\u3002\u7136\u800c\uff0c\u4ed6\u4eec\u4ecd\u7136\u4e13\u6ce8\u4e8e\u6bcf\u4e2a\u7279\u5b9a\u7684\u8f93\u5165\u7c7b\u522b\uff0c\u9650\u5236\u4e86\u4ed6\u4eec\u7684\u751f\u6210\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u68c0\u7d22\u589e\u5f3a\u70b9\u4e91\u8865\u5168\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u8de8\u6a21\u6001\u68c0\u7d22\u6765\u5b66\u4e60\u6765\u81ea\u7c7b\u4f3c\u53c2\u8003\u6837\u672c\u7684\u7ed3\u6784\u5148\u9a8c\u4fe1\u606f\u3002\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7ed3\u6784\u5171\u4eab\u7279\u5f81\u7f16\u7801\u5668 (SSFE) \u6765\u8054\u5408\u63d0\u53d6\u8de8\u6a21\u6001\u7279\u5f81\u5e76\u91cd\u5efa\u53c2\u8003\u7279\u5f81\u4f5c\u4e3a\u5148\u9a8c\u3002\u63d0\u51fa\u4e86\u4e00\u4e2a\u6e10\u8fdb\u5f0f\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5668 (PRAG)\uff0c\u5b83\u91c7\u7528\u5206\u5c42\u7279\u5f81\u878d\u5408\u673a\u5236\u6765\u6574\u5408\u6765\u81ea\u5168\u5c40\u5230\u5c40\u90e8\u7684\u53c2\u8003\u5148\u9a8c\u4fe1\u606f\u4e0e\u8f93\u5165\u7279\u5f81\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5927\u91cf\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u7ec6\u7c92\u5ea6\u70b9\u4e91\u4ee5\u53ca\u5904\u7406\u7a00\u758f\u6570\u636e\u548c\u672a\u89c1\u7c7b\u522b\u65b9\u9762\u663e\u793a\u51fa\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u7ec6\u7c92\u5ea6\u70b9\u4e91\u4ee5\u53ca\u5904\u7406\u7a00\u758f\u6570\u636e\u548c\u672a\u89c1\u7c7b\u522b\u65b9\u9762\u663e\u793a\u51fa\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.14227", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14227", "abs": "https://arxiv.org/abs/2507.14227", "authors": ["Khoi Do", "Duong Nguyen", "Nam-Khanh Le", "Quoc-Viet Pham", "Binh-Son Hua", "Won-Joo Hwang"], "title": "Domain Generalization via Pareto Optimal Gradient Matching", "comment": null, "summary": "In this study, we address the gradient-based domain generalization problem,\nwhere predictors aim for consistent gradient directions across different\ndomains. Existing methods have two main challenges. First, minimization of\ngradient empirical distance or gradient inner products (GIP) leads to gradient\nfluctuations among domains, thereby hindering straightforward learning. Second,\nthe direct application of gradient learning to the joint loss function can\nincur high computation overheads due to second-order derivative approximation.\nTo tackle these challenges, we propose a new Pareto Optimality Gradient\nMatching (POGM) method. In contrast to existing methods that add gradient\nmatching as regularization, we leverage gradient trajectories as collected data\nand apply independent training at the meta-learner. In the meta-update, we\nmaximize GIP while limiting the learned gradient from deviating too far from\nthe empirical risk minimization gradient trajectory. By doing so, the aggregate\ngradient can incorporate knowledge from all domains without suffering gradient\nfluctuation towards any particular domain. Experimental evaluations on datasets\nfrom DomainBed demonstrate competitive results yielded by POGM against other\nbaselines while achieving computational efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5e15\u7d2f\u6258\u6700\u4f18\u68af\u5ea6\u5339\u914d\uff08POGM\uff09\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u57fa\u4e8e\u68af\u5ea6\u7684\u9886\u57df\u6cdb\u5316\u95ee\u9898\u4e2d\u7684\u68af\u5ea6\u6ce2\u52a8\u548c\u9ad8\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u68af\u5ea6\u7684\u9886\u57df\u6cdb\u5316\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u68af\u5ea6\u7ecf\u9a8c\u8ddd\u79bb\u6216\u68af\u5ea6\u5185\u79ef\uff08GIP\uff09\u7684\u6700\u5c0f\u5316\u5bfc\u81f4\u9886\u57df\u95f4\u7684\u68af\u5ea6\u6ce2\u52a8\uff0c\u4ece\u800c\u963b\u788d\u4e86\u76f4\u63a5\u5b66\u4e60\uff1b\u76f4\u63a5\u5c06\u68af\u5ea6\u5b66\u4e60\u5e94\u7528\u4e8e\u8054\u5408\u635f\u5931\u51fd\u6570\u4f1a\u5bfc\u81f4\u7531\u4e8e\u4e8c\u9636\u5bfc\u6570\u8fd1\u4f3c\u5e26\u6765\u7684\u9ad8\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5e15\u7d2f\u6258\u6700\u4f18\u68af\u5ea6\u5339\u914d\uff08POGM\uff09\u65b9\u6cd5\uff0c\u5229\u7528\u68af\u5ea6\u8f68\u8ff9\u4f5c\u4e3a\u6536\u96c6\u7684\u6570\u636e\uff0c\u5e76\u5728\u5143\u5b66\u4e60\u5668\u4e0a\u5e94\u7528\u72ec\u7acb\u8bad\u7ec3\u3002\u5728\u5143\u66f4\u65b0\u4e2d\uff0c\u6700\u5927\u5316GIP\uff0c\u540c\u65f6\u9650\u5236\u5b66\u4e60\u7684\u68af\u5ea6\u504f\u79bb\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u68af\u5ea6\u8f68\u8ff9\u3002", "result": "POGM\u5728DomainBed\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "POGM\u5728DomainBed\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2507.14642", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14642", "abs": "https://arxiv.org/abs/2507.14642", "authors": ["Monoshiz Mahbub Khan", "Xioayin Xi", "Andrew Meneely", "Zhe Yu"], "title": "Efficient Story Point Estimation With Comparative Learning", "comment": null, "summary": "Story point estimation is an essential part of agile software development.\nStory points are unitless, project-specific effort estimates that help\ndevelopers plan their sprints. Traditionally, developers estimate story points\ncollaboratively using planning poker or other manual techniques. While the\ninitial calibrating of the estimates to each project is helpful, once a team\nhas converged on a set of precedents, story point estimation can become tedious\nand labor-intensive. Machine learning can reduce this burden, but only with\nenough context from the historical decisions made by the project team. That is,\nstate-of-the-art models, such as GPT2SP and FastText-SVM, only make accurate\npredictions (within-project) when trained on data from the same project. The\ngoal of this work is to streamline story point estimation by evaluating a\ncomparative learning-based framework for calibrating project-specific story\npoint prediction models. Instead of assigning a specific story point value to\nevery backlog item, developers are presented with pairs of items, and indicate\nwhich item requires more effort. Using these comparative judgments, a machine\nlearning model is trained to predict the story point estimates. We empirically\nevaluated our technique using data with 23,313 manual estimates in 16 projects.\nThe model learned from comparative judgments can achieve on average 0.34\nSpearman's rank correlation coefficient between its predictions and the ground\ntruth story points. This is similar to, if not better than, the performance of\na regression model learned from the ground truth story points. Therefore, the\nproposed comparative learning approach is more efficient than state-of-the-art\nregression-based approaches according to the law of comparative judgments -\nproviding comparative judgments yields a lower cognitive burden on humans than\nproviding ratings or categorical labels.", "AI": {"tldr": "This paper proposes a comparative learning approach to streamline story point estimation. It trains a model using pairwise comparisons of backlog items, achieving comparable or better performance than regression models with lower cognitive burden on developers.", "motivation": "Story point estimation can become tedious and labor-intensive. Machine learning can reduce this burden, but only with enough context from the historical decisions made by the project team. State-of-the-art models only make accurate predictions when trained on data from the same project. The goal of this work is to streamline story point estimation by evaluating a comparative learning-based framework for calibrating project-specific story point prediction models.", "method": "A comparative learning-based framework for calibrating project-specific story point prediction models. Developers are presented with pairs of items, and indicate which item requires more effort. Using these comparative judgments, a machine learning model is trained to predict the story point estimates.", "result": "The model learned from comparative judgments can achieve on average 0.34 Spearman's rank correlation coefficient between its predictions and the ground truth story points. This is similar to, if not better than, the performance of a regression model learned from the ground truth story points.", "conclusion": "The proposed comparative learning approach is more efficient than state-of-the-art regression-based approaches because providing comparative judgments yields a lower cognitive burden on humans than providing ratings or categorical labels."}}
{"id": "2507.14430", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14430", "abs": "https://arxiv.org/abs/2507.14430", "authors": ["Xiaolin Yan", "Yangxing Liu", "Jiazhang Zheng", "Chi Liu", "Mingyu Du", "Caisheng Chen", "Haoyang Liu", "Ming Ding", "Yuan Li", "Qiuping Liao", "Linfeng Li", "Zhili Mei", "Siyu Wan", "Li Li", "Ruyi Zhong", "Jiangling Yu", "Xule Liu", "Huihui Hu", "Jiameng Yue", "Ruohui Cheng", "Qi Yang", "Liangqing Wu", "Ke Zhu", "Chi Zhang", "Chufei Jing", "Yifan Zhou", "Yan Liang", "Dongdong Li", "Zhaohui Wang", "Bin Zhao", "Mingzhou Wu", "Mingzhong Zhou", "Peng Du", "Zuomin Liao", "Chao Dai", "Pengfei Liang", "Xiaoguang Zhu", "Yu Zhang", "Yu Gu", "Kun Pan", "Yuan Wu", "Yanqing Guan", "Shaojing Wu", "Zikang Feng", "Xianze Ma", "Peishan Cheng", "Wenjuan Jiang", "Jing Ba", "Huihao Yu", "Zeping Hu", "Yuan Xu", "Zhiwei Liu", "He Wang", "Zhenguo Lin", "Ming Liu", "Yanhong Meng"], "title": "X-Intelligence 3.0: Training and Evaluating Reasoning LLM for Semiconductor Display", "comment": "Technical Report", "summary": "Large language models (LLMs) have recently achieved significant advances in\nreasoning and demonstrated their advantages in solving challenging problems.\nYet, their effectiveness in the semiconductor display industry remains limited\ndue to a lack of domain-specific training and expertise. To bridge this gap, we\npresent X-Intelligence 3.0, the first high-performance reasoning model\nspecifically developed for the semiconductor display industry. This model is\ndesigned to deliver expert-level understanding and reasoning for the industry's\ncomplex challenges. Leveraging a carefully curated industry knowledge base, the\nmodel undergoes supervised fine-tuning and reinforcement learning to enhance\nits reasoning and comprehension capabilities. To further accelerate\ndevelopment, we implemented an automated evaluation framework that simulates\nexpert-level assessments. We also integrated a domain-specific\nretrieval-augmented generation (RAG) mechanism, resulting in notable\nperformance gains on benchmark datasets. Despite its relatively compact size of\n32 billion parameters, X-Intelligence 3.0 outperforms SOTA DeepSeek-R1-671B\nacross multiple evaluations. This demonstrates its exceptional efficiency and\nestablishes it as a powerful solution to the longstanding reasoning challenges\nfaced by the semiconductor display industry.", "AI": {"tldr": "X-Intelligence 3.0 is the first high-performance reasoning model specifically developed for the semiconductor display industry.", "motivation": "LLMs effectiveness in the semiconductor display industry remains limited due to a lack of domain-specific training and expertise", "method": "supervised fine-tuning and reinforcement learning, automated evaluation framework, domain-specific retrieval-augmented generation (RAG) mechanism", "result": "notable performance gains on benchmark datasets; outperforms SOTA DeepSeek-R1-671B across multiple evaluations, despite its relatively compact size of 32 billion parameters", "conclusion": "X-Intelligence 3.0 outperforms SOTA DeepSeek-R1-671B across multiple evaluations, demonstrating its exceptional efficiency and establishes it as a powerful solution to the longstanding reasoning challenges faced by the semiconductor display industry."}}
{"id": "2507.14497", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14497", "abs": "https://arxiv.org/abs/2507.14497", "authors": ["Weimin Lyu", "Qingqiao Hu", "Kehan Qi", "Zhan Shi", "Wentao Huang", "Saumya Gupta", "Chao Chen"], "title": "Efficient Whole Slide Pathology VQA via Token Compression", "comment": null, "summary": "Whole-slide images (WSIs) in pathology can reach up to 10,000 x 10,000\npixels, posing significant challenges for multimodal large language model\n(MLLM) due to long context length and high computational demands. Previous\nmethods typically focus on patch-level analysis or slide-level classification\nusing CLIP-based models with multi-instance learning, but they lack the\ngenerative capabilities needed for visual question answering (VQA). More recent\nMLLM-based approaches address VQA by feeding thousands of patch tokens directly\ninto the language model, which leads to excessive resource consumption. To\naddress these limitations, we propose Token Compression Pathology LLaVA\n(TCP-LLaVA), the first MLLM architecture to perform WSI VQA via token\ncompression. TCP-LLaVA introduces a set of trainable compression tokens that\naggregate visual and textual information through a modality compression module,\ninspired by the [CLS] token mechanism in BERT. Only the compressed tokens are\nforwarded to the LLM for answer generation, significantly reducing input length\nand computational cost. Experiments on ten TCGA tumor subtypes show that\nTCP-LLaVA outperforms existing MLLM baselines in VQA accuracy while reducing\ntraining resource consumption by a substantial margin.", "AI": {"tldr": "TCP-LLaVA\u662f\u4e00\u79cd\u7528\u4e8eWSI VQA\u7684\u65b0\u578bMLLM\u67b6\u6784\uff0c\u5b83\u901a\u8fc7token\u538b\u7f29\u6765\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u5728VQA\u51c6\u786e\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u4e0a\u4e0b\u6587\u957f\u5ea6\u8f83\u957f\u548c\u8ba1\u7b97\u9700\u6c42\u8f83\u9ad8\uff0c\u75c5\u7406\u5b66\u4e2d\u7684\u5168\u5207\u7247\u56fe\u50cf(WSI)\u5bf9\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b(MLLM)\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u5148\u524d\u7684\u65b9\u6cd5\u901a\u5e38\u4fa7\u91cd\u4e8e\u4f7f\u7528\u57fa\u4e8eCLIP\u7684\u6a21\u578b\u548c\u591a\u5b9e\u4f8b\u5b66\u4e60\u8fdb\u884cpatch\u7ea7\u522b\u5206\u6790\u6216slide\u7ea7\u522b\u5206\u7c7b\uff0c\u4f46\u5b83\u4eec\u7f3a\u4e4f\u89c6\u89c9\u95ee\u9898\u56de\u7b54(VQA)\u6240\u9700\u7684\u751f\u6210\u80fd\u529b\u3002\u66f4\u8fd1\u671f\u7684\u57fa\u4e8eMLLM\u7684\u65b9\u6cd5\u901a\u8fc7\u5c06\u6570\u5343\u4e2apatch token\u76f4\u63a5\u8f93\u5165\u5230\u8bed\u8a00\u6a21\u578b\u4e2d\u6765\u89e3\u51b3VQA\u95ee\u9898\uff0c\u8fd9\u5bfc\u81f4\u4e86\u8fc7\u5ea6\u7684\u8d44\u6e90\u6d88\u8017\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aToken Compression Pathology LLaVA (TCP-LLaVA) \u7684MLLM\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u901a\u8fc7token\u538b\u7f29\u6267\u884cWSI VQA\u3002TCP-LLaVA\u5f15\u5165\u4e86\u4e00\u7ec4\u53ef\u8bad\u7ec3\u7684\u538b\u7f29token\uff0c\u901a\u8fc7\u53d7BERT\u4e2d[CLS] token\u673a\u5236\u542f\u53d1\u7684\u6a21\u6001\u538b\u7f29\u6a21\u5757\u805a\u5408\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\u3002\u53ea\u6709\u538b\u7f29\u540e\u7684token\u88ab\u8f6c\u53d1\u5230LLM\u7528\u4e8e\u7b54\u6848\u751f\u6210\u3002", "result": "\u5728\u5341\u79cdTCGA\u80bf\u7624\u4e9a\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTCP-LLaVA\u5728VQA\u51c6\u786e\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684MLLM\u57fa\u7ebf\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u8bad\u7ec3\u8d44\u6e90\u6d88\u8017\u3002", "conclusion": "TCP-LLaVA\u5728VQA\u51c6\u786e\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684MLLM\u57fa\u7ebf\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u8bad\u7ec3\u8d44\u6e90\u6d88\u8017\u3002"}}
{"id": "2507.14245", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI", "cs.CE", "q-bio.BM", "I.6.5; J.3; I.5.4"], "pdf": "https://arxiv.org/pdf/2507.14245", "abs": "https://arxiv.org/abs/2507.14245", "authors": ["Hengjie Yu", "Kenneth A. Dawson", "Haiyun Yang", "Shuya Liu", "Yan Yan", "Yaochu Jin"], "title": "A million-scale dataset and generalizable foundation model for nanomaterial-protein interactions", "comment": "31 pages, 6 figures", "summary": "Unlocking the potential of nanomaterials in medicine and environmental\nscience hinges on understanding their interactions with proteins, a complex\ndecision space where AI is poised to make a transformative impact. However,\nprogress has been hindered by limited datasets and the restricted\ngeneralizability of existing models. Here, we propose NanoPro-3M, the largest\nnanomaterial-protein interaction dataset to date, comprising over 3.2 million\nsamples and 37,000 unique proteins. Leveraging this, we present NanoProFormer,\na foundational model that predicts nanomaterial-protein affinities through\nmultimodal representation learning, demonstrating strong generalization,\nhandling missing features, and unseen nanomaterials or proteins. We show that\nmultimodal modeling significantly outperforms single-modality approaches and\nidentifies key determinants of corona formation. Furthermore, we demonstrate\nits applicability to a range of downstream tasks through zero-shot inference\nand fine-tuning. Together, this work establishes a solid foundation for\nhigh-performance and generalized prediction of nanomaterial-protein interaction\nendpoints, reducing experimental reliance and accelerating various in vitro\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5927\u578b\u7eb3\u7c73\u6750\u6599-\u86cb\u767d\u8d28\u76f8\u4e92\u4f5c\u7528\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u540d\u4e3aNanoProFormer\u7684\u57fa\u7840\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u7eb3\u7c73\u6750\u6599-\u86cb\u767d\u8d28\u7684\u4eb2\u548c\u529b\uff0c\u5e76\u5728\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u5176\u9002\u7528\u6027\u3002", "motivation": "\u4e86\u89e3\u7eb3\u7c73\u6750\u6599\u4e0e\u86cb\u767d\u8d28\u7684\u76f8\u4e92\u4f5c\u7528\u81f3\u5173\u91cd\u8981\uff0c\u8fd9\u662f\u4e00\u4e2a\u590d\u6742\u7684\u51b3\u7b56\u7a7a\u95f4\uff0c\u4eba\u5de5\u667a\u80fd\u6709\u671b\u5728\u6b64\u4ea7\u751f\u53d8\u9769\u6027\u5f71\u54cd\u3002\u7136\u800c\uff0c\u7531\u4e8e\u6709\u9650\u7684\u6570\u636e\u96c6\u548c\u73b0\u6709\u6a21\u578b\u7684\u53d7\u9650\u7684\u6cdb\u5316\u6027\uff0c\u8fdb\u5c55\u53d7\u5230\u4e86\u963b\u788d\u3002", "method": "\u63d0\u51fa\u4e86NanoPro-3M\uff0c\u8fd9\u662f\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u7684\u7eb3\u7c73\u6750\u6599-\u86cb\u767d\u8d28\u76f8\u4e92\u4f5c\u7528\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc7320\u4e07\u4e2a\u6837\u672c\u548c37000\u4e2a\u72ec\u7279\u7684\u86cb\u767d\u8d28\u3002\u5229\u7528\u8fd9\u4e00\u70b9\uff0c\u6211\u4eec\u63d0\u51fa\u4e86NanoProFormer\uff0c\u4e00\u4e2a\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u8868\u5f81\u5b66\u4e60\u9884\u6d4b\u7eb3\u7c73\u6750\u6599-\u86cb\u767d\u8d28\u7684\u4eb2\u548c\u529b\u3002", "result": "\u591a\u6a21\u6001\u5efa\u6a21\u663e\u8457\u4f18\u4e8e\u5355\u6a21\u6001\u65b9\u6cd5\uff0c\u5e76\u786e\u5b9a\u4e86\u51a0\u72b6\u5f62\u6210\u7684\u5173\u952e\u51b3\u5b9a\u56e0\u7d20\u3002\u6b64\u5916\uff0c\u6211\u4eec\u901a\u8fc7\u96f6\u6837\u672c\u63a8\u7406\u548c\u5fae\u8c03\u8bc1\u660e\u4e86\u5176\u5bf9\u4e00\u7cfb\u5217\u4e0b\u6e38\u4efb\u52a1\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u9ad8\u6027\u80fd\u548c\u5e7f\u4e49\u7684\u7eb3\u7c73\u6750\u6599-\u86cb\u767d\u8d28\u76f8\u4e92\u4f5c\u7528\u7ec8\u70b9\u9884\u6d4b\u5960\u5b9a\u4e86\u575a\u5b9e\u7684\u57fa\u7840\uff0c\u51cf\u5c11\u4e86\u5bf9\u5b9e\u9a8c\u7684\u4f9d\u8d56\uff0c\u5e76\u52a0\u901f\u4e86\u5404\u79cd\u4f53\u5916\u5e94\u7528\u3002"}}
{"id": "2507.14660", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14660", "abs": "https://arxiv.org/abs/2507.14660", "authors": ["Qibing Ren", "Sitao Xie", "Longxuan Wei", "Zhenfei Yin", "Junchi Yan", "Lizhuang Ma", "Jing Shao"], "title": "When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems", "comment": "Code is available at https://github.com/renqibing/RogueAgent", "summary": "Recent large-scale events like election fraud and financial scams have shown\nhow harmful coordinated efforts by human groups can be. With the rise of\nautonomous AI systems, there is growing concern that AI-driven groups could\nalso cause similar harm. While most AI safety research focuses on individual AI\nsystems, the risks posed by multi-agent systems (MAS) in complex real-world\nsituations are still underexplored. In this paper, we introduce a\nproof-of-concept to simulate the risks of malicious MAS collusion, using a\nflexible framework that supports both centralized and decentralized\ncoordination structures. We apply this framework to two high-risk fields:\nmisinformation spread and e-commerce fraud. Our findings show that\ndecentralized systems are more effective at carrying out malicious actions than\ncentralized ones. The increased autonomy of decentralized systems allows them\nto adapt their strategies and cause more damage. Even when traditional\ninterventions, like content flagging, are applied, decentralized groups can\nadjust their tactics to avoid detection. We present key insights into how these\nmalicious groups operate and the need for better detection systems and\ncountermeasures. Code is available at https://github.com/renqibing/RogueAgent.", "AI": {"tldr": "This paper simulates the risks of malicious multi-agent system (MAS) collusion in misinformation spread and e-commerce fraud. It finds that decentralized systems are more effective at carrying out malicious actions than centralized ones, highlighting the need for better detection and countermeasures.", "motivation": "Recent large-scale events have shown how harmful coordinated efforts by human groups can be. There is growing concern that AI-driven groups could also cause similar harm. The risks posed by multi-agent systems (MAS) in complex real-world situations are still underexplored.", "method": "A flexible framework that supports both centralized and decentralized coordination structures is used to simulate the risks of malicious MAS collusion. The framework is applied to misinformation spread and e-commerce fraud.", "result": "Decentralized systems are more effective at carrying out malicious actions than centralized ones. The increased autonomy of decentralized systems allows them to adapt their strategies and cause more damage. Even when traditional interventions are applied, decentralized groups can adjust their tactics to avoid detection.", "conclusion": "Decentralized systems are more effective at carrying out malicious actions than centralized ones, and can adjust their tactics to avoid detection. The paper highlights the need for better detection systems and countermeasures."}}
{"id": "2507.14578", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14578", "abs": "https://arxiv.org/abs/2507.14578", "authors": ["Sachin Yadav", "Dominik Schlechtweg"], "title": "XL-DURel: Finetuning Sentence Transformers for Ordinal Word-in-Context Classification", "comment": "8 pages", "summary": "We propose XL-DURel, a finetuned, multilingual Sentence Transformer model\noptimized for ordinal Word-in-Context classification. We test several loss\nfunctions for regression and ranking tasks managing to outperform previous\nmodels on ordinal and binary data with a ranking objective based on angular\ndistance in complex space. We further show that binary WiC can be treated as a\nspecial case of ordinal WiC and that optimizing models for the general ordinal\ntask improves performance on the more specific binary task. This paves the way\nfor a unified treatment of WiC modeling across different task formulations.", "AI": {"tldr": "XL-DURel \u662f\u4e00\u79cd\u7528\u4e8e\u6709\u5e8f Word-in-Context \u5206\u7c7b\u7684\u5fae\u8c03\u591a\u8bed\u8a00\u53e5\u5b50\u8f6c\u6362\u5668\u6a21\u578b\uff0c\u5b83\u5728\u6392\u5e8f\u76ee\u6807\u4e0a\u4f18\u4e8e\u4e4b\u524d\u7684\u6a21\u578b\uff0c\u5e76\u5c06\u4e8c\u5143 WiC \u89c6\u4e3a\u6709\u5e8f WiC \u7684\u4e00\u4e2a\u7279\u4f8b\u3002", "motivation": "\u63d0\u51fa XL-DURel\uff0c\u8fd9\u662f\u4e00\u79cd\u5fae\u8c03\u7684\u591a\u8bed\u8a00\u53e5\u5b50\u8f6c\u6362\u5668\u6a21\u578b\uff0c\u9488\u5bf9\u6709\u5e8f\u4e0a\u4e0b\u6587\u8bcd\u5206\u7c7b\u8fdb\u884c\u4e86\u4f18\u5316\u3002", "method": "\u5fae\u8c03\u7684\u591a\u8bed\u8a00\u53e5\u5b50\u8f6c\u6362\u5668\u6a21\u578b\uff0c\u9488\u5bf9\u6709\u5e8f\u4e0a\u4e0b\u6587\u8bcd\u5206\u7c7b\u8fdb\u884c\u4e86\u4f18\u5316", "result": "\u5728\u6709\u5e8f\u548c\u4e8c\u5143\u6570\u636e\u4e0a\uff0c\u4f7f\u7528\u57fa\u4e8e\u590d\u7a7a\u95f4\u4e2d\u89d2\u5ea6\u8ddd\u79bb\u7684\u6392\u5e8f\u76ee\u6807\uff0c\u4f18\u4e8e\u4e4b\u524d\u7684\u6a21\u578b\u3002", "conclusion": "\u4e8c\u5143 WiC \u53ef\u4ee5\u88ab\u89c6\u4e3a\u6709\u5e8f WiC \u7684\u4e00\u4e2a\u7279\u4f8b\uff0c\u5e76\u4e14\u9488\u5bf9\u4e00\u822c\u6709\u5e8f\u4efb\u52a1\u4f18\u5316\u6a21\u578b\u53ef\u4ee5\u63d0\u9ad8\u5728\u66f4\u5177\u4f53\u7684\u4e8c\u5143\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002\u8fd9\u4e3a\u8de8\u4e0d\u540c\u4efb\u52a1\u516c\u5f0f\u7684 WiC \u5efa\u6a21\u7684\u7edf\u4e00\u5904\u7406\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2507.14500", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14500", "abs": "https://arxiv.org/abs/2507.14500", "authors": ["Zhiyuan Hua", "Dehao Yuan", "Cornelia Ferm\u00fcller"], "title": "Motion Segmentation and Egomotion Estimation from Event-Based Normal Flow", "comment": null, "summary": "This paper introduces a robust framework for motion segmentation and\negomotion estimation using event-based normal flow, tailored specifically for\nneuromorphic vision sensors. In contrast to traditional methods that rely\nheavily on optical flow or explicit depth estimation, our approach exploits the\nsparse, high-temporal-resolution event data and incorporates geometric\nconstraints between normal flow, scene structure, and inertial measurements.\nThe proposed optimization-based pipeline iteratively performs event\nover-segmentation, isolates independently moving objects via residual analysis,\nand refines segmentations using hierarchical clustering informed by motion\nsimilarity and temporal consistency. Experimental results on the EVIMO2v2\ndataset validate that our method achieves accurate segmentation and\ntranslational motion estimation without requiring full optical flow\ncomputation. This approach demonstrates significant advantages at object\nboundaries and offers considerable potential for scalable, real-time robotic\nand navigation applications.", "AI": {"tldr": "A robust framework for motion segmentation and egomotion estimation using event-based normal flow is introduced, which is tailored for neuromorphic vision sensors and demonstrates advantages at object boundaries.", "motivation": "Traditional methods rely heavily on optical flow or explicit depth estimation, which are not ideal for neuromorphic vision sensors. This paper exploits the sparse, high-temporal-resolution event data and incorporates geometric constraints.", "method": "An optimization-based pipeline iteratively performs event over-segmentation, isolates independently moving objects via residual analysis, and refines segmentations using hierarchical clustering.", "result": "The method achieves accurate segmentation and translational motion estimation on the EVIMO2v2 dataset.", "conclusion": "The method achieves accurate segmentation and translational motion estimation without full optical flow computation, with advantages at object boundaries and potential for scalable, real-time applications."}}
{"id": "2507.14257", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14257", "abs": "https://arxiv.org/abs/2507.14257", "authors": ["Julio Candanedo"], "title": "Linearized Diffusion Map", "comment": null, "summary": "We introduce the Linearized Diffusion Map (LDM), a novel linear\ndimensionality reduction method constructed via a linear approximation of the\ndiffusion-map kernel. LDM integrates the geometric intuition of diffusion-based\nnonlinear methods with the computational simplicity, efficiency, and\ninterpretability inherent in linear embeddings such as PCA and classical MDS.\nThrough comprehensive experiments on synthetic datasets (Swiss roll and\nhyperspheres) and real-world benchmarks (MNIST and COIL-20), we illustrate that\nLDM captures distinct geometric features of datasets compared to PCA, offering\ncomplementary advantages. Specifically, LDM embeddings outperform PCA in\ndatasets exhibiting explicit manifold structures, particularly in\nhigh-dimensional regimes, whereas PCA remains preferable in scenarios dominated\nby variance or noise. Furthermore, the complete positivity of LDM's kernel\nmatrix allows direct applicability of Non-negative Matrix Factorization (NMF),\nsuggesting opportunities for interpretable latent-structure discovery. Our\nanalysis positions LDM as a valuable new linear dimensionality reduction\ntechnique with promising theoretical and practical extensions.", "AI": {"tldr": "LDM\u662f\u4e00\u79cd\u65b0\u7684\u7ebf\u6027\u964d\u7ef4\u65b9\u6cd5\uff0c\u5728\u5177\u6709\u663e\u5f0f\u6d41\u5f62\u7ed3\u6784\u7684\u6570\u636e\u96c6\u4e2d\u4f18\u4e8ePCA\uff0c\u5e76\u4e14\u5141\u8bb8\u76f4\u63a5\u5e94\u7528\u975e\u8d1f\u77e9\u9635\u5206\u89e3(NMF)\u3002", "motivation": "\u5c06\u57fa\u4e8e\u6269\u6563\u7684\u975e\u7ebf\u6027\u65b9\u6cd5\u7684\u51e0\u4f55\u76f4\u89c9\u4e0ePCA\u548c\u7ecf\u5178MDS\u7b49\u7ebf\u6027\u5d4c\u5165\u7684\u8ba1\u7b97\u7b80\u5355\u6027\u3001\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u76f8\u7ed3\u5408\u3002", "method": "\u901a\u8fc7\u6269\u6563\u56fe\u6838\u7684\u7ebf\u6027\u8fd1\u4f3c\u6784\u5efa\u7ebf\u6027\u964d\u7ef4\u65b9\u6cd5LDM\u3002", "result": "\u5728\u5177\u6709\u663e\u5f0f\u6d41\u5f62\u7ed3\u6784\u7684\u6570\u636e\u96c6\u4e2d\uff0cLDM\u5d4c\u5165\u4f18\u4e8ePCA\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u7ef4\u60c5\u51b5\u4e0b\uff1bPCA\u5728\u65b9\u5dee\u6216\u566a\u58f0\u5360\u4e3b\u5bfc\u5730\u4f4d\u7684\u60c5\u51b5\u4e0b\u4ecd\u7136\u662f\u9996\u9009\u3002LDM\u7684\u6838\u77e9\u9635\u7684\u5b8c\u5168\u6b63\u6027\u5141\u8bb8\u76f4\u63a5\u5e94\u7528\u975e\u8d1f\u77e9\u9635\u5206\u89e3(NMF)\u3002", "conclusion": "LDM\u662f\u4e00\u79cd\u65b0\u7684\u7ebf\u6027\u964d\u7ef4\u6280\u672f\uff0c\u5177\u6709\u826f\u597d\u7684\u7406\u8bba\u548c\u5b9e\u9645\u6269\u5c55\u524d\u666f\u3002"}}
{"id": "2507.14705", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14705", "abs": "https://arxiv.org/abs/2507.14705", "authors": ["Sai Wang", "Senthilnathan Subramanian", "Mudit Sahni", "Praneeth Gone", "Lingjie Meng", "Xiaochen Wang", "Nicolas Ferradas Bertoli", "Tingxian Cheng", "Jun Xu"], "title": "Configurable multi-agent framework for scalable and realistic testing of llm-based agents", "comment": null, "summary": "Large-language-model (LLM) agents exhibit complex, context-sensitive\nbehaviour that quickly renders static benchmarks and ad-hoc manual testing\nobsolete.\n  We present Neo, a configurable, multi-agent framework that automates\nrealistic, multi-turn evaluation of LLM-based systems. Neo couples a Question\nGeneration Agent and an Evaluation Agent through a shared context-hub, allowing\ndomain prompts, scenario controls and dynamic feedback to be composed\nmodularly. Test inputs are sampled from a probabilistic state model spanning\ndialogue flow, user intent and emotional tone, enabling diverse, human-like\nconversations that adapt after every turn.\n  Applied to a production-grade Seller Financial Assistant chatbot, Neo (i)\nuncovered edge-case failures across five attack categories with a 3.3% break\nrate close to the 5.8% achieved by expert human red-teamers, and (ii) delivered\n10-12X higher throughput, generating 180 coherent test questions in around 45\nmins versus 16h of human effort. Beyond security probing, Neo's stochastic\npolicies balanced topic coverage and conversational depth, yielding broader\nbehavioural exploration than manually crafted scripts.\n  Neo therefore lays a foundation for scalable, self-evolving LLM QA: its agent\ninterfaces, state controller and feedback loops are model-agnostic and\nextensible to richer factual-grounding and policy-compliance checks. We release\nthe framework to facilitate reproducible, high-fidelity testing of emerging\nagentic systems.", "AI": {"tldr": "Neo is a configurable, multi-agent framework that automates realistic, multi-turn evaluation of LLM-based systems. It couples a Question Generation Agent and an Evaluation Agent through a shared context-hub, allowing domain prompts, scenario controls and dynamic feedback to be composed modularly.", "motivation": "LLM agents exhibit complex, context-sensitive behaviour that quickly renders static benchmarks and ad-hoc manual testing obsolete.", "method": "Neo couples a Question Generation Agent and an Evaluation Agent through a shared context-hub, allowing domain prompts, scenario controls and dynamic feedback to be composed modularly. Test inputs are sampled from a probabilistic state model spanning dialogue flow, user intent and emotional tone, enabling diverse, human-like conversations that adapt after every turn.", "result": "Neo (i) uncovered edge-case failures across five attack categories with a 3.3% break rate close to the 5.8% achieved by expert human red-teamers, and (ii) delivered 10-12X higher throughput, generating 180 coherent test questions in around 45 mins versus 16h of human effort. Beyond security probing, Neo's stochastic policies balanced topic coverage and conversational depth, yielding broader behavioural exploration than manually crafted scripts.", "conclusion": "Neo lays a foundation for scalable, self-evolving LLM QA by using agent interfaces, state controller and feedback loops which are model-agnostic and extensible to richer factual-grounding and policy-compliance checks. The framework is released to facilitate reproducible, high-fidelity testing of emerging agentic systems."}}
{"id": "2507.14579", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14579", "abs": "https://arxiv.org/abs/2507.14579", "authors": ["Kester Wong", "Sahan Bulathwela", "Mutlu Cukurova"], "title": "Exploring Human-AI Complementarity in CPS Diagnosis Using Unimodal and Multimodal BERT Models", "comment": "Accepted to appear in the workshop proceedings for the HEXED'25\n  workshop in the 26th International Conference on Artificial Intelligence in\n  Education 2025 (AIED 2025), 22 July 2025, Palermo, Italy. 5 pages", "summary": "Detecting collaborative problem solving (CPS) indicators from dialogue using\nmachine learning techniques is a significant challenge for the field of AI in\nEducation. Recent studies have explored the use of Bidirectional Encoder\nRepresentations from Transformers (BERT) models on transcription data to\nreliably detect meaningful CPS indicators. A notable advancement involved the\nmultimodal BERT variant, AudiBERT, which integrates speech and\nacoustic-prosodic audio features to enhance CPS diagnosis. Although initial\nresults demonstrated multimodal improvements, the statistical significance of\nthese enhancements remained unclear, and there was insufficient guidance on\nleveraging human-AI complementarity for CPS diagnosis tasks. This workshop\npaper extends the previous research by highlighting that the AudiBERT model not\nonly improved the classification of classes that were sparse in the dataset,\nbut it also had statistically significant class-wise improvements over the BERT\nmodel for classifications in the social-cognitive dimension. However, similar\nsignificant class-wise improvements over the BERT model were not observed for\nclassifications in the affective dimension. A correlation analysis highlighted\nthat larger training data was significantly associated with higher recall\nperformance for both the AudiBERT and BERT models. Additionally, the precision\nof the BERT model was significantly associated with high inter-rater agreement\namong human coders. When employing the BERT model to diagnose indicators within\nthese subskills that were well-detected by the AudiBERT model, the performance\nacross all indicators was inconsistent. We conclude the paper by outlining a\nstructured approach towards achieving human-AI complementarity for CPS\ndiagnosis, highlighting the crucial inclusion of model explainability to\nsupport human agency and engagement in the reflective coding process.", "AI": {"tldr": "This paper extends previous research on using AudiBERT for CPS diagnosis, showing statistically significant improvements in the social-cognitive dimension compared to BERT, but not in the affective dimension. It also explores the importance of data size, inter-rater agreement, and human-AI complementarity.", "motivation": "Detecting collaborative problem solving (CPS) indicators from dialogue using machine learning techniques is a significant challenge. Previous research showed unclear statistical significance in multimodal improvements and lacked guidance on human-AI complementarity.", "method": "The study uses the multimodal BERT variant, AudiBERT, which integrates speech and acoustic-prosodic audio features to enhance CPS diagnosis. It also employs the BERT model for comparison.", "result": "AudiBERT improved classification of sparse classes and showed statistically significant class-wise improvements over BERT in the social-cognitive dimension, but not in the affective dimension. Larger training data correlated with higher recall for both models. BERT's precision was linked to high inter-rater agreement. Performance was inconsistent when using BERT to diagnose indicators well-detected by AudiBERT.", "conclusion": "The paper concludes by outlining a structured approach towards achieving human-AI complementarity for CPS diagnosis, highlighting the crucial inclusion of model explainability to support human agency and engagement in the reflective coding process."}}
{"id": "2507.14501", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14501", "abs": "https://arxiv.org/abs/2507.14501", "authors": ["Jiahui Zhang", "Yuelei Li", "Anpei Chen", "Muyu Xu", "Kunhao Liu", "Jianyuan Wang", "Xiao-Xiao Long", "Hanxue Liang", "Zexiang Xu", "Hao Su", "Christian Theobalt", "Christian Rupprecht", "Andrea Vedaldi", "Hanspeter Pfister", "Shijian Lu", "Fangneng Zhan"], "title": "Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey", "comment": "A project page associated with this survey is available at\n  https://fnzhan.com/projects/Feed-Forward-3D", "summary": "3D reconstruction and view synthesis are foundational problems in computer\nvision, graphics, and immersive technologies such as augmented reality (AR),\nvirtual reality (VR), and digital twins. Traditional methods rely on\ncomputationally intensive iterative optimization in a complex chain, limiting\ntheir applicability in real-world scenarios. Recent advances in feed-forward\napproaches, driven by deep learning, have revolutionized this field by enabling\nfast and generalizable 3D reconstruction and view synthesis. This survey offers\na comprehensive review of feed-forward techniques for 3D reconstruction and\nview synthesis, with a taxonomy according to the underlying representation\narchitectures including point cloud, 3D Gaussian Splatting (3DGS), Neural\nRadiance Fields (NeRF), etc. We examine key tasks such as pose-free\nreconstruction, dynamic 3D reconstruction, and 3D-aware image and video\nsynthesis, highlighting their applications in digital humans, SLAM, robotics,\nand beyond. In addition, we review commonly used datasets with detailed\nstatistics, along with evaluation protocols for various downstream tasks. We\nconclude by discussing open research challenges and promising directions for\nfuture work, emphasizing the potential of feed-forward approaches to advance\nthe state of the art in 3D vision.", "AI": {"tldr": "\u672c\u6587\u5168\u9762\u56de\u987e\u4e86\u7528\u4e8e 3D \u91cd\u5efa\u548c\u89c6\u56fe\u5408\u6210\u7684\u524d\u9988\u6280\u672f\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86\u5176\u5728\u5404\u79cd\u5e94\u7528\u4e2d\u7684\u4f18\u52bf\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002", "motivation": "3D \u91cd\u5efa\u548c\u89c6\u56fe\u5408\u6210\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u56fe\u5f62\u548c\u589e\u5f3a\u73b0\u5b9e (AR)\u3001\u865a\u62df\u73b0\u5b9e (VR) \u548c\u6570\u5b57\u5b6a\u751f\u7b49\u6c89\u6d78\u5f0f\u6280\u672f\u4e2d\u7684\u57fa\u7840\u95ee\u9898\u3002\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u590d\u6742\u94fe\u4e2d\u8ba1\u7b97\u5bc6\u96c6\u578b\u8fed\u4ee3\u4f18\u5316\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u5bf9\u7528\u4e8e 3D \u91cd\u5efa\u548c\u89c6\u56fe\u5408\u6210\u7684\u524d\u9988\u6280\u672f\u8fdb\u884c\u4e86\u5168\u9762\u7684\u56de\u987e\uff0c\u5e76\u6839\u636e\u5305\u62ec\u70b9\u4e91\u30013D \u9ad8\u65af\u6e85\u5c04 (3DGS)\u3001\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u7b49\u5728\u5185\u7684\u5e95\u5c42\u8868\u793a\u67b6\u6784\u8fdb\u884c\u4e86\u5206\u7c7b\u3002", "result": "\u68c0\u67e5\u4e86\u8bf8\u5982\u65e0\u59ff\u52bf\u91cd\u5efa\u3001\u52a8\u6001 3D \u91cd\u5efa\u548c 3D \u611f\u77e5\u56fe\u50cf\u548c\u89c6\u9891\u5408\u6210\u7b49\u5173\u952e\u4efb\u52a1\uff0c\u7a81\u51fa\u4e86\u5b83\u4eec\u5728\u6570\u5b57\u4eba\u3001SLAM\u3001\u673a\u5668\u4eba\u7b49\u9886\u57df\u7684\u5e94\u7528\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u56de\u987e\u4e86\u5e38\u7528\u7684\u6570\u636e\u96c6\u4ee5\u53ca\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u7684\u8be6\u7ec6\u7edf\u8ba1\u6570\u636e\u548c\u8bc4\u4f30\u534f\u8bae\u3002", "conclusion": "\u603b\u7ed3\u4e86\u5f00\u653e\u7684\u7814\u7a76\u6311\u6218\u548c\u672a\u6765\u6709\u5e0c\u671b\u7684\u65b9\u5411\uff0c\u5f3a\u8c03\u4e86\u524d\u9988\u65b9\u6cd5\u5728\u63a8\u8fdb 3D \u89c6\u89c9\u6280\u672f\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.14295", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14295", "abs": "https://arxiv.org/abs/2507.14295", "authors": ["Licheng Liu", "Zihan Wang", "Linjie Li", "Chenwei Xu", "Yiping Lu", "Han Liu", "Avirup Sil", "Manling Li"], "title": "A Simple \"Try Again\" Can Elicit Multi-Turn LLM Reasoning", "comment": null, "summary": "Multi-turn problem solving is critical yet challenging for Large Reasoning\nModels (LRMs) to reflect on their reasoning and revise from feedback. Existing\nReinforcement Learning (RL) methods train large reasoning models on a\nsingle-turn paradigm with verifiable rewards. However, we observe that models\ntrained with existing RL paradigms often lose their ability to solve problems\nacross multiple turns and struggle to revise answers based on contextual\nfeedback, leading to repetitive responses. We ask: can LRMs learn to reflect\ntheir answers in a multi-turn context? In this work, we find that training\nmodels with multi-turn RL using only unary feedback (e.g., \"Let's try again\")\nafter wrong answers can improve both single-turn performance and multi-turn\nreasoning. We introduce Unary Feedback as Observation (UFO) for reinforcement\nlearning, which uses minimal yet common unary user feedback during iterative\nproblem solving. It can be easily applied to existing single-turn RL training\nsetups. Experimental results show that RL training with UFO keeps single-turn\nperformance and improves multi-turn reasoning accuracy by up to 14%, enabling\nlanguage models to better react to feedback in multi-turn problem solving. To\nfurther minimize the number of turns needed for a correct answer while\nencouraging diverse reasoning when mistakes occur, we design reward structures\nthat guide models to produce careful and deliberate answers in each turn. Code:\nhttps://github.com/lichengliu03/unary-feedback", "AI": {"tldr": "training models with multi-turn RL using only unary feedback after wrong answers can improve both single-turn performance and multi-turn reasoning", "motivation": "Existing Reinforcement Learning (RL) methods train large reasoning models on a single-turn paradigm with verifiable rewards, but models trained with existing RL paradigms often lose their ability to solve problems across multiple turns and struggle to revise answers based on contextual feedback, leading to repetitive responses.", "method": "multi-turn RL using only unary feedback (e.g., \"Let's try again\") after wrong answers", "result": "improve both single-turn performance and multi-turn reasoning", "conclusion": "RL training with UFO keeps single-turn performance and improves multi-turn reasoning accuracy by up to 14%, enabling language models to better react to feedback in multi-turn problem solving."}}
{"id": "2507.14719", "categories": ["cs.AI", "I.2.7; F.2.2"], "pdf": "https://arxiv.org/pdf/2507.14719", "abs": "https://arxiv.org/abs/2507.14719", "authors": ["Juan Manuel Contreras"], "title": "Automated Safety Evaluations Across 20 Large Language Models: The Aymara LLM Risk and Responsibility Matrix", "comment": null, "summary": "As large language models (LLMs) become increasingly integrated into\nreal-world applications, scalable and rigorous safety evaluation is essential.\nThis paper introduces Aymara AI, a programmatic platform for generating and\nadministering customized, policy-grounded safety evaluations. Aymara AI\ntransforms natural-language safety policies into adversarial prompts and scores\nmodel responses using an AI-based rater validated against human judgments. We\ndemonstrate its capabilities through the Aymara LLM Risk and Responsibility\nMatrix, which evaluates 20 commercially available LLMs across 10 real-world\nsafety domains. Results reveal wide performance disparities, with mean safety\nscores ranging from 86.2% to 52.4%. While models performed well in\nwell-established safety domains such as Misinformation (mean = 95.7%), they\nconsistently failed in more complex or underspecified domains, notably Privacy\n& Impersonation (mean = 24.3%). Analyses of Variance confirmed that safety\nscores differed significantly across both models and domains (p < .05). These\nfindings underscore the inconsistent and context-dependent nature of LLM safety\nand highlight the need for scalable, customizable tools like Aymara AI to\nsupport responsible AI development and oversight.", "AI": {"tldr": "Aymara AI\uff1a\u4e00\u4e2a\u7528\u4e8e\u5927\u89c4\u6a21 LLM \u5b89\u5168\u8bc4\u4f30\u7684\u5e73\u53f0\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u5b89\u5168\u9886\u57df\u8868\u73b0\u4e0d\u4e00\u81f4\uff0c\u5f3a\u8c03\u4e86\u53ef\u5b9a\u5236\u5de5\u5177\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u8d8a\u6765\u8d8a\u591a\u5730\u96c6\u6210\u5230\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u53ef\u6269\u5c55\u4e14\u4e25\u683c\u7684\u5b89\u5168\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\u3002", "method": "Aymara AI \u5e73\u53f0\uff0c\u7528\u4e8e\u751f\u6210\u548c\u7ba1\u7406\u81ea\u5b9a\u4e49\u7684\u3001\u57fa\u4e8e\u7b56\u7565\u7684\u5b89\u5168\u8bc4\u4f30\u3002\u4f7f\u7528\u57fa\u4e8e AI \u7684\u8bc4\u5206\u5668\u5bf9\u6a21\u578b\u54cd\u5e94\u8fdb\u884c\u8bc4\u5206\uff0c\u8be5\u8bc4\u5206\u5668\u5df2\u6839\u636e\u4eba\u5de5\u5224\u65ad\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "result": "\u5728 10 \u4e2a\u5b9e\u9645\u5b89\u5168\u9886\u57df\u8bc4\u4f30\u4e86 20 \u4e2a\u5546\u7528 LLM\uff0c\u7ed3\u679c\u663e\u793a\u6027\u80fd\u5dee\u5f02\u5f88\u5927\uff0c\u5e73\u5747\u5b89\u5168\u5206\u6570\u8303\u56f4\u4ece 86.2% \u5230 52.4%\u3002\u5728\u5b8c\u5584\u7684\u5b89\u5168\u9886\u57df\uff08\u4f8b\u5982\u9519\u8bef\u4fe1\u606f\uff09\u4e2d\uff0c\u6a21\u578b\u8868\u73b0\u826f\u597d\uff08\u5e73\u5747 = 95.7%\uff09\uff0c\u4f46\u5728\u66f4\u590d\u6742\u6216\u672a\u660e\u786e\u6307\u5b9a\u7684\u9886\u57df\uff08\u5c24\u5176\u662f\u9690\u79c1\u548c\u5192\u5145\uff09\u4e2d\uff0c\u6a21\u578b\u59cb\u7ec8\u5931\u8d25\uff08\u5e73\u5747 = 24.3%\uff09\u3002", "conclusion": "LLM \u7684\u5b89\u5168\u6027\u4e0d\u4e00\u81f4\u4e14\u4f9d\u8d56\u4e8e\u4e0a\u4e0b\u6587\u3002\u9700\u8981\u50cf Aymara AI \u8fd9\u6837\u53ef\u6269\u5c55\u3001\u53ef\u5b9a\u5236\u7684\u5de5\u5177\u6765\u652f\u6301\u8d1f\u8d23\u4efb\u7684 AI \u5f00\u53d1\u548c\u76d1\u7763\u3002"}}
{"id": "2507.14584", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14584", "abs": "https://arxiv.org/abs/2507.14584", "authors": ["Kester Wong", "Sahan Bulathwela", "Mutlu Cukurova"], "title": "Explainable Collaborative Problem Solving Diagnosis with BERT using SHAP and its Implications for Teacher Adoption", "comment": "Accepted to appear in the workshop proceedings for the HEXED'25\n  workshop in the 26th International Conference on Artificial Intelligence in\n  Education 2025 (AIED 2025), 22 July 2025, Palermo, Italy. 6 pages, 2 figures", "summary": "The use of Bidirectional Encoder Representations from Transformers (BERT)\nmodel and its variants for classifying collaborative problem solving (CPS) has\nbeen extensively explored within the AI in Education community. However,\nlimited attention has been given to understanding how individual tokenised\nwords in the dataset contribute to the model's classification decisions.\nEnhancing the explainability of BERT-based CPS diagnostics is essential to\nbetter inform end users such as teachers, thereby fostering greater trust and\nfacilitating wider adoption in education. This study undertook a preliminary\nstep towards model transparency and explainability by using SHapley Additive\nexPlanations (SHAP) to examine how different tokenised words in transcription\ndata contributed to a BERT model's classification of CPS processes. The\nfindings suggested that well-performing classifications did not necessarily\nequate to a reasonable explanation for the classification decisions. Particular\ntokenised words were used frequently to affect classifications. The analysis\nalso identified a spurious word, which contributed positively to the\nclassification but was not semantically meaningful to the class. While such\nmodel transparency is unlikely to be useful to an end user to improve their\npractice, it can help them not to overrely on LLM diagnostics and ignore their\nhuman expertise. We conclude the workshop paper by noting that the extent to\nwhich the model appropriately uses the tokens for its classification is\nassociated with the number of classes involved. It calls for an investigation\ninto the exploration of ensemble model architectures and the involvement of\nhuman-AI complementarity for CPS diagnosis, since considerable human reasoning\nis still required for fine-grained discrimination of CPS subskills.", "AI": {"tldr": "This study uses SHAP to examine how tokenized words contribute to a BERT model's classification of CPS processes, finding that good classifications don't always mean reasonable explanations and that spurious words can influence classifications. It suggests exploring ensemble models and human-AI collaboration for CPS diagnosis.", "motivation": "Enhancing the explainability of BERT-based CPS diagnostics is essential to better inform end users such as teachers, thereby fostering greater trust and facilitating wider adoption in education. Limited attention has been given to understanding how individual tokenised words in the dataset contribute to the model's classification decisions.", "method": "SHapley Additive exPlanations (SHAP) to examine how different tokenised words in transcription data contributed to a BERT model's classification of CPS processes.", "result": "Well-performing classifications did not necessarily equate to a reasonable explanation for the classification decisions. Particular tokenised words were used frequently to affect classifications. The analysis also identified a spurious word, which contributed positively to the classification but was not semantically meaningful to the class.", "conclusion": "The extent to which the model appropriately uses the tokens for its classification is associated with the number of classes involved. It calls for an investigation into the exploration of ensemble model architectures and the involvement of human-AI complementarity for CPS diagnosis, since considerable human reasoning is still required for fine-grained discrimination of CPS subskills."}}
{"id": "2507.14505", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14505", "abs": "https://arxiv.org/abs/2507.14505", "authors": ["Jiahao Ma", "Tianyu Wang", "Miaomiao Liu", "David Ahmedt-Aristizabal", "Chuong Nguyen"], "title": "DCHM: Depth-Consistent Human Modeling for Multiview Detection", "comment": "multi-view detection, sparse-view reconstruction", "summary": "Multiview pedestrian detection typically involves two stages: human modeling\nand pedestrian localization. Human modeling represents pedestrians in 3D space\nby fusing multiview information, making its quality crucial for detection\naccuracy. However, existing methods often introduce noise and have low\nprecision. While some approaches reduce noise by fitting on costly multiview 3D\nannotations, they often struggle to generalize across diverse scenes. To\neliminate reliance on human-labeled annotations and accurately model humans, we\npropose Depth-Consistent Human Modeling (DCHM), a framework designed for\nconsistent depth estimation and multiview fusion in global coordinates.\nSpecifically, our proposed pipeline with superpixel-wise Gaussian Splatting\nachieves multiview depth consistency in sparse-view, large-scaled, and crowded\nscenarios, producing precise point clouds for pedestrian localization.\nExtensive validations demonstrate that our method significantly reduces noise\nduring human modeling, outperforming previous state-of-the-art baselines.\nAdditionally, to our knowledge, DCHM is the first to reconstruct pedestrians\nand perform multiview segmentation in such a challenging setting. Code is\navailable on the \\href{https://jiahao-ma.github.io/DCHM/}{project page}.", "AI": {"tldr": "DCHM improves multiview pedestrian detection by using depth-consistent human modeling with Gaussian Splatting, reducing noise and outperforming existing methods without relying on human-labeled annotations.", "motivation": "Existing multiview pedestrian detection methods introduce noise and have low precision in human modeling. Some approaches rely on costly multiview 3D annotations and struggle to generalize across diverse scenes.", "method": "The paper proposes Depth-Consistent Human Modeling (DCHM), a framework with superpixel-wise Gaussian Splatting for consistent depth estimation and multiview fusion in global coordinates.", "result": "DCHM achieves multiview depth consistency in sparse-view, large-scaled, and crowded scenarios, producing precise point clouds for pedestrian localization.", "conclusion": "The proposed DCHM method significantly reduces noise during human modeling and outperforms previous state-of-the-art baselines. DCHM is the first to reconstruct pedestrians and perform multiview segmentation in challenging settings."}}
{"id": "2507.14322", "categories": ["cs.LG", "cs.CR", "cs.DC", "I.2.11; C.2.4; K.6.5"], "pdf": "https://arxiv.org/pdf/2507.14322", "abs": "https://arxiv.org/abs/2507.14322", "authors": ["Md Rafid Haque", "Abu Raihan Mostofa Kamal", "Md. Azam Hossain"], "title": "FedStrategist: A Meta-Learning Framework for Adaptive and Robust Aggregation in Federated Learning", "comment": "24 pages, 8 figures. This work is intended for a journal submission", "summary": "Federated Learning (FL) offers a paradigm for privacy-preserving\ncollaborative AI, but its decentralized nature creates significant\nvulnerabilities to model poisoning attacks. While numerous static defenses\nexist, their effectiveness is highly context-dependent, often failing against\nadaptive adversaries or in heterogeneous data environments. This paper\nintroduces FedStrategist, a novel meta-learning framework that reframes robust\naggregation as a real-time, cost-aware control problem. We design a lightweight\ncontextual bandit agent that dynamically selects the optimal aggregation rule\nfrom an arsenal of defenses based on real-time diagnostic metrics. Through\ncomprehensive experiments, we demonstrate that no single static rule is\nuniversally optimal. We show that our adaptive agent successfully learns\nsuperior policies across diverse scenarios, including a ``Krum-favorable\"\nenvironment and against a sophisticated \"stealth\" adversary designed to\nneutralize specific diagnostic signals. Critically, we analyze the paradoxical\nscenario where a non-robust baseline achieves high but compromised accuracy,\nand demonstrate that our agent learns a conservative policy to prioritize model\nintegrity. Furthermore, we prove the agent's policy is controllable via a\nsingle \"risk tolerance\" parameter, allowing practitioners to explicitly manage\nthe trade-off between performance and security. Our work provides a new,\npractical, and analyzable approach to creating resilient and intelligent\ndecentralized AI systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86FedStrategist\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u8054\u90a6\u5b66\u4e60\u9632\u5fa1\u6846\u67b6\uff0c\u53ef\u4ee5\u52a8\u6001\u9009\u62e9\u6700\u4f73\u805a\u5408\u89c4\u5219\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5bb9\u6613\u53d7\u5230\u6a21\u578b\u4e2d\u6bd2\u653b\u51fb\uff0c\u73b0\u6709\u7684\u9759\u6001\u9632\u5fa1\u65b9\u6cd5\u5728\u4e0d\u540c\u7684\u73af\u5883\u548c\u81ea\u9002\u5e94\u653b\u51fb\u8005\u9762\u524d\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u4e0a\u4e0b\u6587bandit agent\uff0c\u8be5agent\u57fa\u4e8e\u5b9e\u65f6\u8bca\u65ad\u6307\u6807\u52a8\u6001\u5730\u9009\u62e9\u6700\u4f73\u7684\u805a\u5408\u89c4\u5219\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6ca1\u6709\u4e00\u79cd\u9759\u6001\u89c4\u5219\u662f\u666e\u904d\u6700\u4f18\u7684\u3002FedStrategist \u80fd\u591f\u5b66\u4e60\u5230\u5728\u5404\u79cd\u573a\u666f\u4e0b\u90fd\u8868\u73b0\u51fa\u8272\u7684\u7b56\u7565\uff0c\u5373\u4f7f\u5728\u201cKrum-favorable\u201d\u73af\u5883\u548c\u9762\u5bf9\u590d\u6742\u7684\u201c\u9690\u8eab\u201d\u653b\u51fb\u8005\u65f6\u4e5f\u662f\u5982\u6b64\u3002\u6b64\u5916\uff0c\u8be5agent\u7684\u7b56\u7565\u53ef\u4ee5\u901a\u8fc7\u5355\u4e2a\u201c\u98ce\u9669\u5bb9\u5fcd\u5ea6\u201d\u53c2\u6570\u6765\u63a7\u5236\uff0c\u4ece\u800c\u5141\u8bb8\u4ece\u4e1a\u8005\u660e\u786e\u5730\u7ba1\u7406\u6027\u80fd\u548c\u5b89\u5168\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5143\u5b66\u4e60\u6846\u67b6FedStrategist\uff0c\u7528\u4e8e\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u6a21\u578b\u4e2d\u6bd2\u653b\u51fb\u7684\u95ee\u9898\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u4e0a\u4e0b\u6587bandit agent\u52a8\u6001\u5730\u4ece\u4e00\u7cfb\u5217\u9632\u5fa1\u63aa\u65bd\u4e2d\u9009\u62e9\u6700\u4f73\u7684\u805a\u5408\u89c4\u5219\uff0c\u4ece\u800c\u5728\u4e0d\u540c\u7684\u573a\u666f\u4e0b\u5b66\u4e60\u5230\u66f4\u4f18\u7684\u7b56\u7565\uff0c\u5e76\u5728\u6027\u80fd\u548c\u5b89\u5168\u6027\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u3002"}}
{"id": "2507.14730", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14730", "abs": "https://arxiv.org/abs/2507.14730", "authors": ["Yanjie Fu"], "title": "Towards AI Urban Planner in the Age of GenAI, LLMs, and Agentic AI", "comment": "4 pages; will continue to update to add more figures to describe the\n  vision;", "summary": "Generative AI, large language models, and agentic AI have emerged separately\nof urban planning. However, the convergence between AI and urban planning\npresents an interesting opportunity towards AI urban planners. This paper\nconceptualizes urban planning as a generative AI task, where AI synthesizes\nland-use configurations under geospatial, social, and human-centric\nconstraints. We survey how generative AI approaches, including VAEs, GANs,\ntransformers, and diffusion models, reshape urban design. We further identify\ncritical gaps: 1) limited research on integrating urban theory guidance, 2)\nlimited research of AI urban planning over multiple spatial resolutions or\nangularities, 3) limited research on augmenting urban design knowledge from\ndata, and 4) limited research on addressing real-world interactions. To address\nthese limitations, we outline future research directions in theory-guided\ngeneration, digital twins, and human-machine co-design, calling for a new\nsynthesis of generative intelligence and participatory urbanism.", "AI": {"tldr": "This paper conceptualizes urban planning as a generative AI task and surveys how generative AI approaches reshape urban design. The paper further identifies critical gaps and outlines future research directions.", "motivation": "The convergence between AI and urban planning presents an interesting opportunity towards AI urban planners.", "method": "This paper surveys how generative AI approaches, including VAEs, GANs, transformers, and diffusion models, reshape urban design.", "result": "This paper identifies critical gaps: 1) limited research on integrating urban theory guidance, 2) limited research of AI urban planning over multiple spatial resolutions or angularities, 3) limited research on augmenting urban design knowledge from data, and 4) limited research on addressing real-world interactions.", "conclusion": "This paper outlines future research directions in theory-guided generation, digital twins, and human-machine co-design, calling for a new synthesis of generative intelligence and participatory urbanism."}}
{"id": "2507.14590", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14590", "abs": "https://arxiv.org/abs/2507.14590", "authors": ["\u0141ukasz Radli\u0144ski", "Mateusz Gu\u015bciora", "Jan Koco\u0144"], "title": "Backtranslation and paraphrasing in the LLM era? Comparing data augmentation methods for emotion classification", "comment": "International Conference on Computational Science 2025", "summary": "Numerous domain-specific machine learning tasks struggle with data scarcity\nand class imbalance. This paper systematically explores data augmentation\nmethods for NLP, particularly through large language models like GPT. The\npurpose of this paper is to examine and evaluate whether traditional methods\nsuch as paraphrasing and backtranslation can leverage a new generation of\nmodels to achieve comparable performance to purely generative methods. Methods\naimed at solving the problem of data scarcity and utilizing ChatGPT were\nchosen, as well as an exemplary dataset. We conducted a series of experiments\ncomparing four different approaches to data augmentation in multiple\nexperimental setups. We then evaluated the results both in terms of the quality\nof generated data and its impact on classification performance. The key\nfindings indicate that backtranslation and paraphrasing can yield comparable or\neven better results than zero and a few-shot generation of examples.", "AI": {"tldr": "This paper explores data augmentation methods for NLP, particularly through large language models like GPT, and finds that backtranslation and paraphrasing can yield comparable or even better results than zero and a few-shot generation of examples.", "motivation": "Numerous domain-specific machine learning tasks struggle with data scarcity and class imbalance.", "method": "We conducted a series of experiments comparing four different approaches to data augmentation in multiple experimental setups.", "result": "Evaluated the results both in terms of the quality of generated data and its impact on classification performance.", "conclusion": "Backtranslation and paraphrasing can yield comparable or even better results than zero and a few-shot generation of examples."}}
{"id": "2507.14533", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14533", "abs": "https://arxiv.org/abs/2507.14533", "authors": ["Shuo Cao", "Nan Ma", "Jiayang Li", "Xiaohui Li", "Lihao Shao", "Kaiwen Zhu", "Yu Zhou", "Yuandong Pu", "Jiarui Wu", "Jiaquan Wang", "Bo Qu", "Wenhai Wang", "Yu Qiao", "Dajuin Yao", "Yihao Liu"], "title": "ArtiMuse: Fine-Grained Image Aesthetics Assessment with Joint Scoring and Expert-Level Understanding", "comment": "43 pages, 31 figures, 13 tables", "summary": "The rapid advancement of educational applications, artistic creation, and\nAI-generated content (AIGC) technologies has substantially increased practical\nrequirements for comprehensive Image Aesthetics Assessment (IAA), particularly\ndemanding methods capable of delivering both quantitative scoring and\nprofessional understanding. Multimodal Large Language Model (MLLM)-based IAA\nmethods demonstrate stronger perceptual and generalization capabilities\ncompared to traditional approaches, yet they suffer from modality bias\n(score-only or text-only) and lack fine-grained attribute decomposition,\nthereby failing to support further aesthetic assessment. In this paper, we\npresent:(1) ArtiMuse, an innovative MLLM-based IAA model with Joint Scoring and\nExpert-Level Understanding capabilities; (2) ArtiMuse-10K, the first\nexpert-curated image aesthetic dataset comprising 10,000 images spanning 5 main\ncategories and 15 subcategories, each annotated by professional experts with\n8-dimensional attributes analysis and a holistic score. Both the model and\ndataset will be made public to advance the field.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ArtiMuse\u6a21\u578b\u548cArtiMuse-10K\u6570\u636e\u96c6\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u56fe\u50cf\u7f8e\u5b66\u8bc4\u4f30\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u6559\u80b2\u5e94\u7528\u3001\u827a\u672f\u521b\u4f5c\u548cAI\u751f\u6210\u5185\u5bb9(AIGC)\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5927\u5927\u589e\u52a0\u4e86\u5bf9\u7efc\u5408\u56fe\u50cf\u7f8e\u5b66\u8bc4\u4f30(IAA)\u7684\u5b9e\u9645\u9700\u6c42\uff0c\u7279\u522b\u662f\u9700\u8981\u80fd\u591f\u63d0\u4f9b\u5b9a\u91cf\u8bc4\u5206\u548c\u4e13\u4e1a\u7406\u89e3\u7684\u65b9\u6cd5\u3002\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b(MLLM)\u7684IAA\u65b9\u6cd5\u6bd4\u4f20\u7edf\u65b9\u6cd5\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u611f\u77e5\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u5b58\u5728\u6a21\u6001\u504f\u5dee(\u4ec5\u8bc4\u5206\u6216\u4ec5\u6587\u672c)\uff0c\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u7684\u5c5e\u6027\u5206\u89e3\uff0c\u4ece\u800c\u65e0\u6cd5\u652f\u6301\u8fdb\u4e00\u6b65\u7684\u7f8e\u5b66\u8bc4\u4f30\u3002", "method": "\u521b\u65b0\u6027\u7684\u57fa\u4e8eMLLM\u7684IAA\u6a21\u578b\uff0c\u5177\u6709\u8054\u5408\u8bc4\u5206\u548c\u4e13\u5bb6\u7ea7\u7406\u89e3\u80fd\u529b\u3002", "result": "\u63d0\u51fa\u4e86ArtiMuse\uff0c\u4e00\u4e2a\u521b\u65b0\u7684\u57fa\u4e8eMLLM\u7684IAA\u6a21\u578b\uff0c\u5177\u6709\u8054\u5408\u8bc4\u5206\u548c\u4e13\u5bb6\u7ea7\u7406\u89e3\u80fd\u529b;\u6784\u5efa\u4e86ArtiMuse-10K\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u4e13\u5bb6\u7b56\u5212\u7684\u56fe\u50cf\u7f8e\u5b66\u6570\u636e\u96c6\uff0c\u5305\u542b10000\u5f20\u56fe\u50cf\uff0c\u8de8\u8d8a5\u4e2a\u4e3b\u8981\u7c7b\u522b\u548c15\u4e2a\u5b50\u7c7b\u522b\uff0c\u6bcf\u4e2a\u56fe\u50cf\u90fd\u7531\u4e13\u4e1a\u4e13\u5bb6\u6ce8\u91ca\uff0c\u5305\u62ec8\u7ef4\u5c5e\u6027\u5206\u6790\u548c\u4e00\u4e2a\u6574\u4f53\u5206\u6570\u3002", "conclusion": "ArtiMuse\u6a21\u578b\u548cArtiMuse-10K\u6570\u636e\u96c6\u5c06\u88ab\u516c\u5f00\uff0c\u4ee5\u63a8\u8fdb\u884c\u4e1a\u53d1\u5c55\u3002"}}
{"id": "2507.14326", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.14326", "abs": "https://arxiv.org/abs/2507.14326", "authors": ["Aryana Hou", "Li Lin", "Justin Li", "Shu Hu"], "title": "Rethinking Individual Fairness in Deepfake Detection", "comment": "This paper has been accepted by ACM MM 2025", "summary": "Generative AI models have substantially improved the realism of synthetic\nmedia, yet their misuse through sophisticated DeepFakes poses significant\nrisks. Despite recent advances in deepfake detection, fairness remains\ninadequately addressed, enabling deepfake markers to exploit biases against\nspecific populations. While previous studies have emphasized group-level\nfairness, individual fairness (i.e., ensuring similar predictions for similar\nindividuals) remains largely unexplored. In this work, we identify for the\nfirst time that the original principle of individual fairness fundamentally\nfails in the context of deepfake detection, revealing a critical gap previously\nunexplored in the literature. To mitigate it, we propose the first\ngeneralizable framework that can be integrated into existing deepfake detectors\nto enhance individual fairness and generalization. Extensive experiments\nconducted on leading deepfake datasets demonstrate that our approach\nsignificantly improves individual fairness while maintaining robust detection\nperformance, outperforming state-of-the-art methods. The code is available at\nhttps://github.com/Purdue-M2/Individual-Fairness-Deepfake-Detection.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684deepfake\u68c0\u6d4b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u63d0\u9ad8\u4e86\u4e2a\u4eba\u516c\u5e73\u6027\u5e76\u4e14\u4f18\u4e8e\u5f53\u524d\u6700\u4f73\u65b9\u6cd5\u3002", "motivation": "\u751f\u6210\u5f0fAI\u6a21\u578b\u5927\u5927\u63d0\u9ad8\u4e86\u5408\u6210\u5a92\u4f53\u7684\u771f\u5b9e\u6027\uff0c\u4f46\u901a\u8fc7\u590d\u6742\u7684DeepFakes\u6ee5\u7528\u5b83\u4eec\u6784\u6210\u4e86\u91cd\u5927\u98ce\u9669\u3002\u5c3d\u7ba1\u6700\u8fd1\u5728deepfake\u68c0\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u516c\u5e73\u6027\u4ecd\u7136\u6ca1\u6709\u5f97\u5230\u5145\u5206\u89e3\u51b3\uff0c\u4f7f\u5f97deepfake\u6807\u8bb0\u80fd\u591f\u5229\u7528\u9488\u5bf9\u7279\u5b9a\u4eba\u7fa4\u7684\u504f\u89c1\u3002\u4ee5\u524d\u7684\u7814\u7a76\u5f3a\u8c03\u4e86\u7fa4\u4f53\u5c42\u9762\u7684\u516c\u5e73\u6027\uff0c\u4f46\u4e2a\u4f53\u516c\u5e73\u6027\uff08\u5373\u786e\u4fdd\u5bf9\u76f8\u4f3c\u4e2a\u4f53\u8fdb\u884c\u76f8\u4f3c\u7684\u9884\u6d4b\uff09\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u672a\u5f97\u5230\u63a2\u7d22\u3002\u8fd9\u9879\u5de5\u4f5c\u9996\u6b21\u53d1\u73b0\u4e2a\u4f53\u516c\u5e73\u6027\u7684\u539f\u59cb\u539f\u5219\u5728deepfake\u68c0\u6d4b\u7684\u80cc\u666f\u4e0b\u6839\u672c\u5931\u8d25\uff0c\u63ed\u793a\u4e86\u6587\u732e\u4e2d\u5148\u524d\u672a\u66fe\u63a2\u7d22\u7684\u5173\u952e\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u96c6\u6210\u5230\u73b0\u6709deepfake\u68c0\u6d4b\u5668\u4e2d\u7684\u901a\u7528\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u4e2a\u4f53\u516c\u5e73\u6027\u3002", "result": "\u5728\u9886\u5148\u7684deepfake\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9c81\u68d2\u68c0\u6d4b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4e2a\u4f53\u516c\u5e73\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u96c6\u6210\u5230\u73b0\u6709deepfake\u68c0\u6d4b\u5668\u4e2d\u7684\u901a\u7528\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u4e2a\u4f53\u516c\u5e73\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u9886\u5148\u7684deepfake\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9c81\u68d2\u68c0\u6d4b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4e2a\u4f53\u516c\u5e73\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.14897", "categories": ["cs.AI", "I.2.5"], "pdf": "https://arxiv.org/pdf/2507.14897", "abs": "https://arxiv.org/abs/2507.14897", "authors": ["Renxi Wang", "Rifo Ahmad Genadi", "Bilal El Bouardi", "Yongxin Wang", "Fajri Koto", "Zhengzhong Liu", "Timothy Baldwin", "Haonan Li"], "title": "AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents", "comment": null, "summary": "Language model (LM) agents have gained significant attention for their\nability to autonomously complete tasks through interactions with environments,\ntools, and APIs. LM agents are primarily built with prompt engineering or\nsupervised finetuning. At the same time, reinforcement learning (RL) has been\nexplored to enhance LM's capabilities, such as reasoning and factuality.\nHowever, the combination of the LM agents and reinforcement learning (Agent-RL)\nremains underexplored and lacks systematic study. To this end, we built\nAgentFly, a scalable and extensible Agent-RL framework designed to empower LM\nagents with a variety of RL algorithms. Our framework supports multi-turn\ninteractions by adapting traditional RL methods with token-level masking. It\nfeatures a decorator-based interface for defining tools and reward functions,\nenabling seamless extension and ease of use. To support high-throughput\ntraining, we implement asynchronous execution of tool calls and reward\ncomputations, and design a centralized resource management system for scalable\nenvironment coordination. We also provide a suite of prebuilt tools and\nenvironments, demonstrating the framework's effectiveness through successful\nagent training across multiple tasks.", "AI": {"tldr": "AgentFly is a scalable Agent-RL framework designed to empower LM agents with RL algorithms, featuring multi-turn interactions, decorator-based interface, asynchronous execution, and centralized resource management.", "motivation": "The combination of LM agents and reinforcement learning (Agent-RL) remains underexplored and lacks systematic study.", "method": "The AgentFly framework adapts traditional RL methods with token-level masking and features a decorator-based interface for defining tools and reward functions. It also implements asynchronous execution and a centralized resource management system.", "result": "The authors built AgentFly, a scalable and extensible Agent-RL framework designed to empower LM agents with a variety of RL algorithms.", "conclusion": "The AgentFly framework's effectiveness is demonstrated through successful agent training across multiple tasks."}}
{"id": "2507.14615", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14615", "abs": "https://arxiv.org/abs/2507.14615", "authors": ["Fred Mutisya", "Shikoh Gitau", "Christine Syovata", "Diana Oigara", "Ibrahim Matende", "Muna Aden", "Munira Ali", "Ryan Nyotu", "Diana Marion", "Job Nyangena", "Nasubo Ongoma", "Keith Mbae", "Elizabeth Wamicha", "Eric Mibuari", "Jean Philbert Nsengemana", "Talkmore Chidede"], "title": "Retrieval-Augmented Clinical Benchmarking for Contextual Model Testing in Kenyan Primary Care: A Methodology Paper", "comment": "29 pages, 6 figs, 6 tables. Companion methods paper forthcoming", "summary": "Large Language Models(LLMs) hold promise for improving healthcare access in\nlow-resource settings, but their effectiveness in African primary care remains\nunderexplored. We present a methodology for creating a benchmark dataset and\nevaluation framework focused on Kenyan Level 2 and 3 clinical care. Our\napproach uses retrieval augmented generation (RAG) to ground clinical questions\nin Kenya's national guidelines, ensuring alignment with local standards. These\nguidelines were digitized, chunked, and indexed for semantic retrieval. Gemini\nFlash 2.0 Lite was then prompted with guideline excerpts to generate realistic\nclinical scenarios, multiple-choice questions, and rationale based answers in\nEnglish and Swahili. Kenyan physicians co-created and refined the dataset, and\na blinded expert review process ensured clinical accuracy, clarity, and\ncultural appropriateness. The resulting Alama Health QA dataset includes\nthousands of regulator-aligned question answer pairs across common outpatient\nconditions. Beyond accuracy, we introduce evaluation metrics that test clinical\nreasoning, safety, and adaptability such as rare case detection (Needle in the\nHaystack), stepwise logic (Decision Points), and contextual adaptability.\nInitial results reveal significant performance gaps when LLMs are applied to\nlocalized scenarios, consistent with findings that LLM accuracy is lower on\nAfrican medical content than on US-based benchmarks. This work offers a\nreplicable model for guideline-driven, dynamic benchmarking to support safe AI\ndeployment in African health systems.", "AI": {"tldr": "\u521b\u5efa\u4e86\u4e00\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u91cd\u70b9\u5173\u6ce8\u80af\u5c3c\u4e9a2\u7ea7\u548c3\u7ea7\u4e34\u5e8a\u62a4\u7406\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u5f53LLM\u5e94\u7528\u4e8e\u672c\u5730\u5316\u573a\u666f\u65f6\uff0c\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6709\u671b\u6539\u5584\u8d44\u6e90\u532e\u4e4f\u5730\u533a\u83b7\u5f97\u533b\u7597\u4fdd\u5065\u7684\u673a\u4f1a\uff0c\u4f46\u5b83\u4eec\u5728\u975e\u6d32\u521d\u7ea7\u4fdd\u5065\u4e2d\u7684\u6709\u6548\u6027\u4ecd\u6709\u5f85\u63a2\u7d22\u3002", "method": "\u4f7f\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5c06\u4e34\u5e8a\u95ee\u9898\u7f6e\u4e8e\u80af\u5c3c\u4e9a\u56fd\u5bb6\u6307\u5357\u4e2d\uff0c\u786e\u4fdd\u4e0e\u5f53\u5730\u6807\u51c6\u5bf9\u9f50\u3002\u8fd9\u4e9b\u6307\u5357\u88ab\u6570\u5b57\u5316\u3001\u5206\u5757\u548c\u7d22\u5f15\u4ee5\u8fdb\u884c\u8bed\u4e49\u68c0\u7d22\u3002\u7136\u540e\uff0c\u4f7f\u7528Gemini Flash 2.0 Lite\uff0c\u6839\u636e\u6307\u5357\u6458\u5f55\u751f\u6210\u903c\u771f\u7684\u4e34\u5e8a\u573a\u666f\u3001\u591a\u9879\u9009\u62e9\u9898\u548c\u57fa\u4e8e\u539f\u7406\u7684\u82f1\u8bed\u548c\u65af\u74e6\u5e0c\u91cc\u8bed\u7b54\u6848\u3002", "result": "Alama Health QA\u6570\u636e\u96c6\u5305\u62ec\u6570\u5343\u4e2a\u4e0e\u76d1\u7ba1\u673a\u6784\u5bf9\u9f50\u7684\u95ee\u9898\u7b54\u6848\u5bf9\uff0c\u6db5\u76d6\u5e38\u89c1\u7684\u95e8\u8bca\u6761\u4ef6\u3002\u9664\u4e86\u51c6\u786e\u6027\u4e4b\u5916\uff0c\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u8bc4\u4f30\u6307\u6807\uff0c\u7528\u4e8e\u6d4b\u8bd5\u4e34\u5e8a\u63a8\u7406\u3001\u5b89\u5168\u6027\u548c\u9002\u5e94\u6027\uff0c\u4f8b\u5982\u7f55\u89c1\u75c5\u4f8b\u68c0\u6d4b\uff08\u5927\u6d77\u635e\u9488\uff09\u3001\u9010\u6b65\u903b\u8f91\uff08\u51b3\u7b56\u70b9\uff09\u548c\u60c5\u5883\u9002\u5e94\u6027\u3002", "conclusion": "LLMs\u5728\u5e94\u7528\u4e8e\u672c\u5730\u5316\u573a\u666f\u65f6\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u4e0eLLM\u5728\u975e\u6d32\u533b\u7597\u5185\u5bb9\u4e0a\u7684\u51c6\u786e\u7387\u4f4e\u4e8e\u7f8e\u56fd\u57fa\u51c6\u7684\u7814\u7a76\u7ed3\u679c\u4e00\u81f4\u3002\u8fd9\u9879\u5de5\u4f5c\u4e3a\u6307\u5357\u9a71\u52a8\u7684\u52a8\u6001\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u590d\u5236\u7684\u6a21\u578b\uff0c\u4ee5\u652f\u6301\u975e\u6d32\u536b\u751f\u7cfb\u7edf\u4e2d\u5b89\u5168\u7684\u4eba\u5de5\u667a\u80fd\u90e8\u7f72\u3002"}}
{"id": "2507.14543", "categories": ["cs.CV", "cs.CY", "cs.HC", "cs.LG", "I.4.6"], "pdf": "https://arxiv.org/pdf/2507.14543", "abs": "https://arxiv.org/abs/2507.14543", "authors": ["Sharanya Mukherjee", "Md Hishaam Akhtar", "Kannadasan R"], "title": "Real Time Captioning of Sign Language Gestures in Video Meetings", "comment": "7 pages, 2 figures, 1 table, Presented at ICCMDE 2021", "summary": "It has always been a rather tough task to communicate with someone possessing\na hearing impairment. One of the most tested ways to establish such a\ncommunication is through the use of sign based languages. However, not many\npeople are aware of the smaller intricacies involved with sign language. Sign\nlanguage recognition using computer vision aims at eliminating the\ncommunication barrier between deaf-mute and ordinary people so that they can\nproperly communicate with others. Recently the pandemic has left the whole\nworld shaken up and has transformed the way we communicate. Video meetings have\nbecome essential for everyone, even people with a hearing disability. In recent\nstudies, it has been found that people with hearing disabilities prefer to sign\nover typing during these video calls. In this paper, we are proposing a browser\nextension that will automatically translate sign language to subtitles for\neveryone else in the video call. The Large-scale dataset which contains more\nthan 2000 Word-Level ASL videos, which were performed by over 100 signers will\nbe used.", "AI": {"tldr": "This paper proposes a browser extension for real-time sign language translation to subtitles in video calls, using a large ASL video dataset.", "motivation": "Communication with hearing-impaired individuals is challenging, and sign language recognition using computer vision can eliminate communication barriers. Video meetings have become essential, and hearing-disabled individuals prefer signing over typing during these calls.", "method": "Using a Large-scale dataset which contains more than 2000 Word-Level ASL videos, which were performed by over 100 signers.", "result": "N/A", "conclusion": "Proposing a browser extension that will automatically translate sign language to subtitles for everyone else in the video call."}}
{"id": "2507.14332", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14332", "abs": "https://arxiv.org/abs/2507.14332", "authors": ["Aidan Furlong", "Xingang Zhao", "Robert Salko", "Xu Wu"], "title": "Development and Deployment of Hybrid ML Models for Critical Heat Flux Prediction in Annulus Geometries", "comment": "Accepted for inclusion in Transactions of the American Nuclear\n  Society for the 2025 ANS Winter Conference", "summary": "Accurate prediction of critical heat flux (CHF) is an essential component of\nsafety analysis in pressurized and boiling water reactors. To support reliable\nprediction of this quantity, several empirical correlations and lookup tables\nhave been constructed from physical experiments over the past several decades.\nWith the onset of accessible machine learning (ML) frameworks, multiple\ninitiatives have been established with the goal of predicting CHF more\naccurately than these traditional methods. While purely data-driven surrogate\nmodeling has been extensively investigated, these approaches lack\ninterpretability, lack resilience to data scarcity, and have been developed\nmostly using data from tube experiments. As a result, bias-correction hybrid\napproaches have become increasingly popular, which correct initial\n\"low-fidelity\" estimates provided by deterministic base models by using\nML-predicted residuals. This body of work has mostly considered round tube\ngeometries; annular geometry-specific ML models have not yet been deployed in\nthermal hydraulic codes. This study developed, deployed, and validated four ML\nmodels to predict CHF in annular geometries using the CTF subchannel code.\nThree empirical correlation models, Biasi, Bowring, and Katto, were used as\nbase models for comparison. The ML models were trained and tested using 577\nexperimental annulus data points from four datasets: Becker, Beus, Janssen, and\nMortimore. Baseline CHF predictions were obtained from the empirical\ncorrelations, with mean relative errors above 26%. The ML-driven models\nachieved mean relative errors below 3.5%, with no more than one point exceeding\nthe 10% error envelope. In all cases, the hybrid ML models significantly\noutperformed their empirical counterparts.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u7528\u4e8e\u9884\u6d4b\u73af\u5f62\u51e0\u4f55\u4f53\u4e2dCHF\u7684\u6df7\u5408\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7684\u7ecf\u9a8c\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u7cbe\u786e\u9884\u6d4b\u4e34\u754c\u70ed\u901a\u91cf\uff08CHF\uff09\u662f\u538b\u6c34\u5806\u548c\u6cb8\u6c34\u5806\u5b89\u5168\u5206\u6790\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\u3002\u73b0\u6709\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u5bf9\u6570\u636e\u7a00\u7f3a\u7684\u9002\u5e94\u6027\u5dee\uff0c\u5e76\u4e14\u4e3b\u8981\u4f7f\u7528\u6765\u81ea\u7ba1\u5b9e\u9a8c\u7684\u6570\u636e\u5f00\u53d1\u3002\u73af\u5f62\u51e0\u4f55\u4f53\u7279\u5b9a\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5c1a\u672a\u5728\u70ed\u5de5\u6c34\u529b\u4ee3\u7801\u4e2d\u90e8\u7f72\u3002", "method": "\u5f00\u53d1\u3001\u90e8\u7f72\u548c\u9a8c\u8bc1\u4e86\u56db\u4e2a\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u4f7f\u7528CTF\u5b50\u901a\u9053\u4ee3\u7801\u9884\u6d4b\u73af\u5f62\u51e0\u4f55\u4f53\u4e2d\u7684CHF\u3002\u4f7f\u7528Biasi\u3001Bowring\u548cKatto\u4e09\u4e2a\u7ecf\u9a8c\u76f8\u5173\u6a21\u578b\u4f5c\u4e3a\u6bd4\u8f83\u7684\u57fa\u7840\u6a21\u578b\u3002\u4f7f\u7528\u6765\u81ea\u56db\u4e2a\u6570\u636e\u96c6\uff08Becker\u3001Beus\u3001Janssen\u548cMortimore\uff09\u7684577\u4e2a\u5b9e\u9a8c\u73af\u5f62\u6570\u636e\u70b9\u8bad\u7ec3\u548c\u6d4b\u8bd5\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002", "result": "\u673a\u5668\u5b66\u4e60\u9a71\u52a8\u7684\u6a21\u578b\u5b9e\u73b0\u4e86\u4f4e\u4e8e3.5%\u7684\u5e73\u5747\u76f8\u5bf9\u8bef\u5dee\uff0c\u4e14\u4e0d\u8d85\u8fc7\u4e00\u4e2a\u70b9\u8d85\u8fc710%\u7684\u8bef\u5dee\u8303\u56f4\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u7ecf\u9a8c\u76f8\u5173\u6a21\u578b\u7684\u5e73\u5747\u76f8\u5bf9\u8bef\u5dee\u9ad8\u4e8e26%\u3002", "conclusion": "\u6df7\u5408\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u9884\u6d4b\u73af\u5f62\u51e0\u4f55\u4f53\u4e2d\u7684\u4e34\u754c\u70ed\u901a\u91cf\uff08CHF\uff09\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u7ecf\u9a8c\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u4f4e\u4e8e3.5%\u7684\u5e73\u5747\u76f8\u5bf9\u8bef\u5dee\u3002"}}
{"id": "2507.14899", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14899", "abs": "https://arxiv.org/abs/2507.14899", "authors": ["Jiale Liu", "Huan Wang", "Yue Zhang", "Xiaoyu Luo", "Jiaxiang Hu", "Zhiliang Liu", "Min Xie"], "title": "InsightX Agent: An LMM-based Agentic Framework with Integrated Tools for Reliable X-ray NDT Analysis", "comment": null, "summary": "Non-destructive testing (NDT), particularly X-ray inspection, is vital for\nindustrial quality assurance, yet existing deep-learning-based approaches often\nlack interactivity, interpretability, and the capacity for critical\nself-assessment, limiting their reliability and operator trust. To address\nthese shortcomings, this paper proposes InsightX Agent, a novel LMM-based\nagentic framework designed to deliver reliable, interpretable, and interactive\nX-ray NDT analysis. Unlike typical sequential pipelines, InsightX Agent\npositions a Large Multimodal Model (LMM) as a central orchestrator,\ncoordinating between the Sparse Deformable Multi-Scale Detector (SDMSD) and the\nEvidence-Grounded Reflection (EGR) tool. The SDMSD generates dense defect\nregion proposals for multi-scale feature maps and sparsifies them through\nNon-Maximum Suppression (NMS), optimizing detection of small, dense targets in\nX-ray images while maintaining computational efficiency. The EGR tool guides\nthe LMM agent through a chain-of-thought-inspired review process, incorporating\ncontext assessment, individual defect analysis, false positive elimination,\nconfidence recalibration and quality assurance to validate and refine the\nSDMSD's initial proposals. By strategically employing and intelligently using\ntools, InsightX Agent moves beyond passive data processing to active reasoning,\nenhancing diagnostic reliability and providing interpretations that integrate\ndiverse information sources. Experimental evaluations on the GDXray+ dataset\ndemonstrate that InsightX Agent not only achieves a high object detection\nF1-score of 96.35% but also offers significantly improved interpretability and\ntrustworthiness in its analyses, highlighting the transformative potential of\nagentic LLM frameworks for industrial inspection tasks.", "AI": {"tldr": "This paper introduces InsightX Agent, an LMM-based framework for X-ray NDT analysis that improves reliability, interpretability, and interactivity using a central LMM orchestrator coordinating between a defect detector and an evidence-grounded reflection tool. It achieves a 96.35% F1-score on the GDXray+ dataset.", "motivation": "Existing deep-learning-based approaches for X-ray inspection often lack interactivity, interpretability, and the capacity for critical self-assessment, limiting their reliability and operator trust.", "method": "The paper proposes InsightX Agent, a novel LMM-based agentic framework. It positions a Large Multimodal Model (LMM) as a central orchestrator, coordinating between the Sparse Deformable Multi-Scale Detector (SDMSD) and the Evidence-Grounded Reflection (EGR) tool.", "result": "InsightX Agent achieves a high object detection F1-score of 96.35% and offers significantly improved interpretability and trustworthiness.", "conclusion": "InsightX Agent achieves a high object detection F1-score of 96.35% and offers significantly improved interpretability and trustworthiness, highlighting the transformative potential of agentic LLM frameworks for industrial inspection tasks."}}
{"id": "2507.14640", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14640", "abs": "https://arxiv.org/abs/2507.14640", "authors": ["Eric Xia", "Jugal Kalita"], "title": "Linear Relational Decoding of Morphology in Language Models", "comment": null, "summary": "A two-part affine approximation has been found to be a good approximation for\ntransformer computations over certain subject object relations. Adapting the\nBigger Analogy Test Set, we show that the linear transformation Ws, where s is\na middle layer representation of a subject token and W is derived from model\nderivatives, is also able to accurately reproduce final object states for many\nrelations. This linear technique is able to achieve 90% faithfulness on\nmorphological relations, and we show similar findings multi-lingually and\nacross models. Our findings indicate that some conceptual relationships in\nlanguage models, such as morphology, are readily interpretable from latent\nspace, and are sparsely encoded by cross-layer linear transformations.", "AI": {"tldr": "Transformer\u8ba1\u7b97\u53ef\u4ee5\u4f7f\u7528\u7ebf\u6027\u8f6c\u6362\u6765\u8fd1\u4f3c\uff0c\u8be5\u8f6c\u6362\u5728\u5f62\u6001\u5173\u7cfb\u4e0a\u5b9e\u73b0\u4e86 90% \u7684\u5fe0\u5b9e\u5ea6\u3002", "motivation": "\u53d1\u73b0\u4e24\u90e8\u5206\u4eff\u5c04\u8fd1\u4f3c\u662f\u67d0\u4e9b\u4e3b\u5ba2\u4f53\u5173\u7cfb\u4e0a Transformer \u8ba1\u7b97\u7684\u826f\u597d\u8fd1\u4f3c\u3002", "method": "\u7ebf\u6027\u53d8\u6362 Ws\uff0c\u5176\u4e2d s \u662f\u4e3b\u9898\u6807\u8bb0\u7684\u4e2d\u95f4\u5c42\u8868\u793a\uff0cW \u6765\u6e90\u4e8e\u6a21\u578b\u5bfc\u6570\u3002", "result": "\u5728\u7ebf\u6027\u6280\u672f\u4e0a\uff0c\u5f62\u6001\u5173\u7cfb\u7684\u5fe0\u5b9e\u5ea6\u8fbe\u5230 90%\uff0c\u5e76\u4e14\u6211\u4eec\u5c55\u793a\u4e86\u591a\u8bed\u8a00\u548c\u8de8\u6a21\u578b\u7684\u7c7b\u4f3c\u53d1\u73b0\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u4e00\u4e9b\u6982\u5ff5\u5173\u7cfb\uff08\u5982\u5f62\u6001\u5b66\uff09\u53ef\u4ee5\u5f88\u5bb9\u6613\u5730\u4ece\u6f5c\u5728\u7a7a\u95f4\u89e3\u91ca\uff0c\u5e76\u4e14\u7531\u8de8\u5c42\u7ebf\u6027\u53d8\u6362\u7a00\u758f\u7f16\u7801\u3002"}}
{"id": "2507.14544", "categories": ["cs.CV", "cs.AI", "68T45 (Machine vision and scene understanding)", "I.2.10; I.4.8; H.3.1"], "pdf": "https://arxiv.org/pdf/2507.14544", "abs": "https://arxiv.org/abs/2507.14544", "authors": ["Sujata Gaihre", "Amir Thapa Magar", "Prasuna Pokharel", "Laxmi Tiwari"], "title": "Multimodal AI for Gastrointestinal Diagnostics: Tackling VQA in MEDVQA-GI 2025", "comment": "accepted to ImageCLEF 2025, to be published in the lab proceedings", "summary": "This paper describes our approach to Subtask 1 of the ImageCLEFmed MEDVQA\n2025 Challenge, which targets visual question answering (VQA) for\ngastrointestinal endoscopy. We adopt the Florence model-a large-scale\nmultimodal foundation model-as the backbone of our VQA pipeline, pairing a\npowerful vision encoder with a text encoder to interpret endoscopic images and\nproduce clinically relevant answers. To improve generalization, we apply\ndomain-specific augmentations that preserve medical features while increasing\ntraining diversity. Experiments on the KASVIR dataset show that fine-tuning\nFlorence yields accurate responses on the official challenge metrics. Our\nresults highlight the potential of large multimodal models in medical VQA and\nprovide a strong baseline for future work on explainability, robustness, and\nclinical integration. The code is publicly available at:\nhttps://github.com/TiwariLaxuu/VQA-Florence.git", "AI": {"tldr": "This paper uses the Florence model for visual question answering on gastrointestinal endoscopy images, achieving accurate results by fine-tuning and domain-specific augmentations.", "motivation": "This paper addresses visual question answering (VQA) for gastrointestinal endoscopy in the ImageCLEFmed MEDVQA 2025 Challenge.", "method": "The Florence model is adopted as the backbone of VQA pipeline, pairing a powerful vision encoder with a text encoder. Domain-specific augmentations are applied to improve generalization.", "result": "Experiments on the KASVIR dataset show that fine-tuning Florence yields accurate responses on the official challenge metrics.", "conclusion": "Fine-tuning Florence yields accurate responses on the official challenge metrics, highlighting the potential of large multimodal models in medical VQA and providing a strong baseline for future work."}}
{"id": "2507.14344", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14344", "abs": "https://arxiv.org/abs/2507.14344", "authors": ["Daniel Fein", "Gabriela Aranguiz-Dias"], "title": "Influence Functions for Preference Dataset Pruning", "comment": null, "summary": "Language models are commonly fine-tuned via reinforcement learning to alter\ntheir behavior or elicit new capabilities. Datasets used for these purposes,\nand particularly human preference datasets, are often noisy. The relatively\nsmall size post-training datasets, combined with parameter-efficient\nfine-tuning methods, enable the use of influence functions approximations to\ndetect and prune training examples that are harmful to performance on a\nvalidation set. In this work, we adapt the TL;DR dataset for reward model\ntraining to demonstrate how conjugate-gradient approximated influence functions\ncan be used to filter datasets. In our experiments, influence function\nfiltering yields a small retraining accuracy uplift of 1.5% after removing 10%\nof training examples. We also show that gradient similarity outperforms\ninfluence functions for detecting helpful training examples. This suggests that\nlocal curvature is important for detecting harmful training examples, but less\nso for identifying helpful examples.", "AI": {"tldr": "\u4f7f\u7528\u5f71\u54cd\u51fd\u6570\u8fd1\u4f3c\u6765\u68c0\u6d4b\u548c\u4fee\u526a\u5bf9\u9a8c\u8bc1\u96c6\u6027\u80fd\u6709\u5bb3\u7684\u8bad\u7ec3\u793a\u4f8b\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u6539\u53d8\u5176\u884c\u4e3a\u6216\u5f15\u53d1\u65b0\u7684\u80fd\u529b\u3002\u7528\u4e8e\u8fd9\u4e9b\u76ee\u7684\u7684\u6570\u636e\u96c6\uff0c\u7279\u522b\u662f\u4eba\u7c7b\u504f\u597d\u6570\u636e\u96c6\uff0c\u901a\u5e38\u662f\u5608\u6742\u7684\u3002", "method": "\u8c03\u6574 TL;DR \u6570\u636e\u96c6\u4ee5\u8fdb\u884c\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\uff0c\u5e76\u5c55\u793a\u5982\u4f55\u4f7f\u7528\u5171\u8f6d\u68af\u5ea6\u8fd1\u4f3c\u5f71\u54cd\u51fd\u6570\u6765\u8fc7\u6ee4\u6570\u636e\u96c6\u3002", "result": "\u5f71\u54cd\u51fd\u6570\u8fc7\u6ee4\u5728\u5220\u9664 10% \u7684\u8bad\u7ec3\u6837\u672c\u540e\u4ea7\u751f 1.5% \u7684\u5c0f\u5e45\u91cd\u8bad\u7ec3\u51c6\u786e\u7387\u63d0\u5347\u3002\u68af\u5ea6\u76f8\u4f3c\u6027\u5728\u68c0\u6d4b\u6709\u7528\u7684\u8bad\u7ec3\u6837\u672c\u65b9\u9762\u4f18\u4e8e\u5f71\u54cd\u51fd\u6570\u3002", "conclusion": "\u4f7f\u7528\u5f71\u54cd\u51fd\u6570\u8fc7\u6ee4\u6570\u636e\u96c6\u53ef\u4ee5\u5728\u5220\u9664 10% \u7684\u8bad\u7ec3\u6837\u672c\u540e\u4ea7\u751f 1.5% \u7684\u5c0f\u5e45\u91cd\u8bad\u7ec3\u51c6\u786e\u7387\u63d0\u5347\u3002\u68af\u5ea6\u76f8\u4f3c\u6027\u5728\u68c0\u6d4b\u6709\u7528\u7684\u8bad\u7ec3\u6837\u672c\u65b9\u9762\u4f18\u4e8e\u5f71\u54cd\u51fd\u6570\u3002\u8fd9\u8868\u660e\u5c40\u90e8\u66f2\u7387\u5bf9\u4e8e\u68c0\u6d4b\u6709\u5bb3\u8bad\u7ec3\u6837\u672c\u5f88\u91cd\u8981\uff0c\u4f46\u5bf9\u4e8e\u8bc6\u522b\u6709\u7528\u7684\u6837\u672c\u5219\u4e0d\u90a3\u4e48\u91cd\u8981\u3002"}}
{"id": "2507.14906", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14906", "abs": "https://arxiv.org/abs/2507.14906", "authors": ["Xiao Yang", "Juxi Leitner", "Michael Burke"], "title": "Feedback-Induced Performance Decline in LLM-Based Decision-Making", "comment": null, "summary": "The ability of Large Language Models (LLMs) to extract context from natural\nlanguage problem descriptions naturally raises questions about their\nsuitability in autonomous decision-making settings. This paper studies the\nbehaviour of these models within a Markov Decision Process (MDPs). While\ntraditional reinforcement learning (RL) strategies commonly employed in this\nsetting rely on iterative exploration, LLMs, pre-trained on diverse datasets,\noffer the capability to leverage prior knowledge for faster adaptation. We\ninvestigate online structured prompting strategies in sequential decision\nmaking tasks, comparing the zero-shot performance of LLM-based approaches to\nthat of classical RL methods. Our findings reveal that although LLMs\ndemonstrate improved initial performance in simpler environments, they struggle\nwith planning and reasoning in complex scenarios without fine-tuning or\nadditional guidance. Our results show that feedback mechanisms, intended to\nimprove decision-making, often introduce confusion, leading to diminished\nperformance in intricate environments. These insights underscore the need for\nfurther exploration into hybrid strategies, fine-tuning, and advanced memory\nintegration to enhance LLM-based decision-making capabilities.", "AI": {"tldr": "LLMs\u5728\u7b80\u5355\u51b3\u7b56\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u9700\u8981\u6539\u8fdb\uff0c\u5e76\u4e14\u53cd\u9988\u53ef\u80fd\u4f1a\u9002\u5f97\u5176\u53cd\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u81ea\u4e3b\u51b3\u7b56\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u5728\u7ebf\u7ed3\u6784\u5316\u63d0\u793a\u7b56\u7565\uff0c\u6bd4\u8f83LLM\u65b9\u6cd5\u4e0e\u7ecf\u5178RL\u65b9\u6cd5\u7684zero-shot\u6027\u80fd\u3002", "result": "LLMs\u5728\u7b80\u5355\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u6539\u8fdb\u7684\u521d\u59cb\u6027\u80fd\uff0c\u4f46\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff1b\u53cd\u9988\u673a\u5236\u53ef\u80fd\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "LLMs\u5728\u590d\u6742\u73af\u5883\u4e2d\u8fdb\u884c\u89c4\u5212\u548c\u63a8\u7406\u65f6\u9047\u5230\u56f0\u96be\uff0c\u5e76\u4e14\u53cd\u9988\u673a\u5236\u53ef\u80fd\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002"}}
{"id": "2507.14649", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14649", "abs": "https://arxiv.org/abs/2507.14649", "authors": ["Minsuh Joo", "Hyunsoo Cho"], "title": "Cleanse: Uncertainty Estimation Approach Using Clustering-based Semantic Consistency in LLMs", "comment": null, "summary": "Despite the outstanding performance of large language models (LLMs) across\nvarious NLP tasks, hallucinations in LLMs--where LLMs generate inaccurate\nresponses--remains as a critical problem as it can be directly connected to a\ncrisis of building safe and reliable LLMs. Uncertainty estimation is primarily\nused to measure hallucination levels in LLM responses so that correct and\nincorrect answers can be distinguished clearly. This study proposes an\neffective uncertainty estimation approach, \\textbf{Cl}ust\\textbf{e}ring-based\nsem\\textbf{an}tic con\\textbf{s}ist\\textbf{e}ncy (\\textbf{Cleanse}). Cleanse\nquantifies the uncertainty with the proportion of the intra-cluster consistency\nin the total consistency between LLM hidden embeddings which contain adequate\nsemantic information of generations, by employing clustering. The effectiveness\nof Cleanse for detecting hallucination is validated using four off-the-shelf\nmodels, LLaMA-7B, LLaMA-13B, LLaMA2-7B and Mistral-7B and two\nquestion-answering benchmarks, SQuAD and CoQA.", "AI": {"tldr": "\u63d0\u51faCleanse\u65b9\u6cd5\uff0c\u901a\u8fc7\u805a\u7c7b\u5206\u6790LLM\u751f\u6210\u6587\u672c\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u6765\u68c0\u6d4b\u5e7b\u89c9\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5404\u79cdNLP\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46LLM\u4e2d\u7684\u5e7b\u89c9\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\uff0c\u56e0\u4e3a\u5b83\u76f4\u63a5\u5173\u7cfb\u5230\u6784\u5efa\u5b89\u5168\u53ef\u9760\u7684LLM\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCleanse\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u8ba1\u7b97LLM\u9690\u85cf\u5c42\u5d4c\u5165\u5411\u91cf\u805a\u7c7b\u5185\u90e8\u4e00\u81f4\u6027\u4e0e\u603b\u4e00\u81f4\u6027\u7684\u6bd4\u4f8b\u6765\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728LLaMA-7B, LLaMA-13B, LLaMA2-7B\u548cMistral-7B\u56db\u4e2a\u6a21\u578b\u4ee5\u53caSQuAD\u548cCoQA\u4e24\u4e2a\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86Cleanse\u68c0\u6d4b\u5e7b\u89c9\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u805a\u7c7b\u65b9\u6cd5\u91cf\u5316LLM\u9690\u85cf\u5c42\u5d4c\u5165\u5411\u91cf\u7684\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u4ee5\u6b64\u6765\u8bc4\u4f30LLM\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\u3002"}}
{"id": "2507.14549", "categories": ["cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.14549", "abs": "https://arxiv.org/abs/2507.14549", "authors": ["Haotian Deng", "Chi Zhang", "Chen Wei", "Quanying Liu"], "title": "Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering Human Perceptual Variability on Facial Expressions", "comment": "Accepted by IJCNN 2025", "summary": "A fundamental challenge in affective cognitive science is to develop models\nthat accurately capture the relationship between external emotional stimuli and\nhuman internal experiences. While ANNs have demonstrated remarkable accuracy in\nfacial expression recognition, their ability to model inter-individual\ndifferences in human perception remains underexplored. This study investigates\nthe phenomenon of high perceptual variability-where individuals exhibit\nsignificant differences in emotion categorization even when viewing the same\nstimulus. Inspired by the similarity between ANNs and human perception, we\nhypothesize that facial expression samples that are ambiguous for ANN\nclassifiers also elicit divergent perceptual judgments among human observers.\nTo examine this hypothesis, we introduce a novel perceptual boundary sampling\nmethod to generate facial expression stimuli that lie along ANN decision\nboundaries. These ambiguous samples form the basis of the varEmotion dataset,\nconstructed through large-scale human behavioral experiments. Our analysis\nreveals that these ANN-confusing stimuli also provoke heightened perceptual\nuncertainty in human participants, highlighting shared computational principles\nin emotion perception. Finally, by fine-tuning ANN representations using\nbehavioral data, we achieve alignment between ANN predictions and both\ngroup-level and individual-level human perceptual patterns. Our findings\nestablish a systematic link between ANN decision boundaries and human\nperceptual variability, offering new insights into personalized modeling of\nemotional interpretation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc (ANN) \u5982\u4f55\u6a21\u62df\u4eba\u7c7b\u5728\u60c5\u7eea\u611f\u77e5\u4e0a\u7684\u4e2a\u4f53\u5dee\u5f02\uff0c\u53d1\u73b0 ANN \u96be\u4ee5\u8bc6\u522b\u7684\u9762\u90e8\u8868\u60c5\u4e5f\u4f1a\u5f15\u8d77\u4eba\u7c7b\u611f\u77e5\u4e0a\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u8c03\u6574 ANN \u4f7f\u5176\u4e0e\u4eba\u7c7b\u611f\u77e5\u6a21\u5f0f\u5bf9\u9f50\uff0c\u4e3a\u60c5\u7eea\u89e3\u8bfb\u7684\u4e2a\u6027\u5316\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "motivation": "\u60c5\u611f\u8ba4\u77e5\u79d1\u5b66\u4e2d\u7684\u4e00\u4e2a\u6839\u672c\u6311\u6218\u662f\u5f00\u53d1\u80fd\u591f\u51c6\u786e\u6355\u6349\u5916\u90e8\u60c5\u7eea\u523a\u6fc0\u4e0e\u4eba\u7c7b\u5185\u5728\u4f53\u9a8c\u4e4b\u95f4\u5173\u7cfb\u7684\u6a21\u578b\u3002\u867d\u7136 ANN \u5728\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u663e\u8457\u7684\u51c6\u786e\u6027\uff0c\u4f46\u5b83\u4eec\u5728\u6a21\u62df\u4eba\u7c7b\u611f\u77e5\u4e2d\u7684\u4e2a\u4f53\u5dee\u5f02\u65b9\u9762\u7684\u80fd\u529b\u4ecd\u6709\u5f85\u63a2\u7d22\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u611f\u77e5\u8fb9\u754c\u62bd\u6837\u65b9\u6cd5\uff0c\u4ee5\u751f\u6210\u4f4d\u4e8e ANN \u51b3\u7b56\u8fb9\u754c\u4e0a\u7684\u9762\u90e8\u8868\u60c5\u523a\u6fc0\u3002\u8fd9\u4e9b\u6a21\u68f1\u4e24\u53ef\u7684\u6837\u672c\u6784\u6210\u4e86 varEmotion \u6570\u636e\u96c6\u7684\u57fa\u7840\uff0c\u8be5\u6570\u636e\u96c6\u662f\u901a\u8fc7\u5927\u89c4\u6a21\u4eba\u7c7b\u884c\u4e3a\u5b9e\u9a8c\u6784\u5efa\u7684\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u8fd9\u4e9b\u4f7f ANN \u611f\u5230\u56f0\u60d1\u7684\u523a\u6fc0\u4e5f\u4f1a\u5f15\u53d1\u4eba\u7c7b\u53c2\u4e0e\u8005\u7684\u9ad8\u5ea6\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u7a81\u51fa\u4e86\u60c5\u7eea\u611f\u77e5\u4e2d\u5171\u4eab\u7684\u8ba1\u7b97\u539f\u5219\u3002\u7814\u7a76\u63ed\u793a\u4e86 ANN \u51b3\u7b56\u8fb9\u754c\u4e0e\u4eba\u7c7b\u611f\u77e5\u53d8\u5f02\u6027\u4e4b\u95f4\u7684\u7cfb\u7edf\u8054\u7cfb\u3002", "conclusion": "\u901a\u8fc7\u4f7f\u7528\u884c\u4e3a\u6570\u636e\u5fae\u8c03 ANN \u8868\u5f81\uff0c\u5b9e\u73b0\u4e86 ANN \u9884\u6d4b\u4e0e\u7fa4\u4f53\u548c\u4e2a\u4f53\u5c42\u9762\u7684\u4eba\u7c7b\u611f\u77e5\u6a21\u5f0f\u7684\u5bf9\u9f50\u3002\u7814\u7a76\u7ed3\u679c\u5efa\u7acb\u4e86 ANN \u51b3\u7b56\u8fb9\u754c\u4e0e\u4eba\u7c7b\u611f\u77e5\u53d8\u5f02\u6027\u4e4b\u95f4\u7684\u7cfb\u7edf\u8054\u7cfb\uff0c\u4e3a\u60c5\u7eea\u89e3\u8bfb\u7684\u4e2a\u6027\u5316\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2507.14353", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14353", "abs": "https://arxiv.org/abs/2507.14353", "authors": ["Harsh Nilesh Pathak", "Randy Paffenroth"], "title": "Solo Connection: A Parameter Efficient Fine-Tuning Technique for Transformers", "comment": null, "summary": "Parameter efficient fine tuning (PEFT) is a versatile and extensible approach\nfor adapting a Large Language Model (LLM) for newer tasks. One of the most\nprominent PEFT approaches, Low Rank Adaptation (LoRA), primarily focuses on\nadjusting the attention weight matrices within individual decoder blocks of a\nGenerative Pre trained Transformer (GPT2). In contrast, we introduce Solo\nConnection a novel method that adapts the representation at the decoder-block\nlevel rather than modifying individual weight matrices. Not only does Solo\nConnection outperform LoRA on E2E natural language generation benchmarks, but\nit also reduces the number of trainable parameters by 59% relative to LoRA and\nby more than 99% compared to full fine-tuning of GPT2, an early version of\nLarge Language Models (LLMs). Solo Connection is also motivated by homotopy\ntheory: we introduce a trainable linear transformation that gradually\ninterpolates between a zero vector and the task-specific representation,\nenabling smooth and stable adaptation over time. While skip connections in the\noriginal 12 layer GPT2 are typically confined to individual decoder blocks,\nsubsequent GPT2 variants scale up to 48 layers, and even larger language models\ncan include 128 or more decoder blocks. These expanded architectures underscore\nthe need to revisit how skip connections are employed during fine-tuning. This\npaper focuses on long skip connections that link outputs of different decoder\nblocks, potentially enhancing the model's ability to adapt to new tasks while\nleveraging pre-trained knowledge.", "AI": {"tldr": "Introduces Solo Connection, a parameter-efficient fine-tuning method that outperforms LoRA by adapting decoder-block level representations and using long skip connections.", "motivation": "The need to revisit how skip connections are employed during fine-tuning, especially in larger language models with many decoder blocks.", "method": "Introducing Solo Connection, a novel method that adapts the representation at the decoder-block level with a trainable linear transformation.", "result": "Solo Connection outperforms LoRA, reduces trainable parameters by 59% relative to LoRA and by more than 99% compared to full fine-tuning of GPT2.", "conclusion": "Solo Connection outperforms LoRA on E2E natural language generation benchmarks, reduces trainable parameters, and is motivated by homotopy theory for stable adaptation."}}
{"id": "2507.14909", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.14909", "abs": "https://arxiv.org/abs/2507.14909", "authors": ["Elio Grande"], "title": "The Endless Tuning. An Artificial Intelligence Design To Avoid Human Replacement and Trace Back Responsibilities", "comment": null, "summary": "The Endless Tuning is a design method for a reliable deployment of artificial\nintelligence based on a double mirroring process, which pursues both the goals\nof avoiding human replacement and filling the so-called responsibility gap\n(Matthias 2004). Originally depicted in (Fabris et al. 2024) and ensuing the\nrelational approach urged therein, it was then actualized in a protocol,\nimplemented in three prototypical applications regarding decision-making\nprocesses (respectively: loan granting, pneumonia diagnosis, and art style\nrecognition) and tested with such as many domain experts. Step by step\nillustrating the protocol, giving insights concretely showing a different voice\n(Gilligan 1993) in the ethics of artificial intelligence, a philosophical\naccount of technical choices (e.g., a reversed and hermeneutic deployment of\nXAI algorithms) will be provided in the present study together with the results\nof the experiments, focusing on user experience rather than statistical\naccuracy. Even thoroughly employing deep learning models, full control was\nperceived by the interviewees in the decision-making setting, while it appeared\nthat a bridge can be built between accountability and liability in case of\ndamage.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u65e0\u5c3d\u8c03\u6574\u201d\u7684\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u53cc\u91cd\u955c\u50cf\u8fc7\u7a0b\u5b9e\u73b0\u53ef\u9760\u7684\u4eba\u5de5\u667a\u80fd\u90e8\u7f72\uff0c\u907f\u514d\u4eba\u7c7b\u66ff\u4ee3\u5e76\u586b\u8865\u8d23\u4efb\u5dee\u8ddd\u3002", "motivation": "\u907f\u514d\u4eba\u7c7b\u66ff\u4ee3\u548c\u586b\u8865\u6240\u8c13\u7684\u8d23\u4efb\u5dee\u8ddd", "method": "\u57fa\u4e8e\u53cc\u91cd\u955c\u50cf\u8fc7\u7a0b\u7684\u8bbe\u8ba1\u65b9\u6cd5", "result": "\u901a\u8fc7\u4e09\u4e2a\u539f\u578b\u5e94\u7528\uff08\u8d37\u6b3e\u6279\u51c6\u3001\u80ba\u708e\u8bca\u65ad\u548c\u827a\u672f\u98ce\u683c\u8bc6\u522b\uff09\u7684\u5b9e\u73b0\u548c\u9886\u57df\u4e13\u5bb6\u7684\u6d4b\u8bd5\uff0c\u5c55\u793a\u4e86\u7528\u6237\u4f53\u9a8c\u3002", "conclusion": "\u7528\u6237\u5728\u51b3\u7b56\u73af\u5883\u4e2d\u611f\u77e5\u5230\u5b8c\u5168\u7684\u63a7\u5236\uff0c\u5e76\u4e14\u5728\u53d1\u751f\u635f\u5bb3\u65f6\uff0c\u53ef\u4ee5\u5728\u95ee\u8d23\u548c\u8d23\u4efb\u4e4b\u95f4\u5efa\u7acb\u6865\u6881\u3002"}}
{"id": "2507.14664", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14664", "abs": "https://arxiv.org/abs/2507.14664", "authors": ["Wannaphong Phatthiyaphaibun", "Can Udomcharoenchaikit", "Pakpoom Singkorapoom", "Kunat Pipatanakul", "Ekapol Chuangsuwanich", "Peerat Limkonchotiwat", "Sarana Nutanong"], "title": "Mangosteen: An Open Thai Corpus for Language Model Pretraining", "comment": "Work in Progress.All artifacts in this papers:\n  https://huggingface.co/collections/aisingapore/wangchanlion-v3-687a362d8f0ea2fe4077c6b3", "summary": "Pre-training data shapes a language model's quality, but raw web text is\nnoisy and demands careful cleaning. Existing large-scale corpora rely on\nEnglish-centric or language-agnostic pipelines whose heuristics do not capture\nThai script or cultural nuances, leaving risky material such as gambling\ncontent untreated. Prior Thai-specific efforts customize pipelines or build new\nones, yet seldom release their data or document design choices, hindering\nreproducibility and raising the question of how to construct a transparent,\nhigh-quality Thai corpus. We introduce Mangosteen: a 47 billion-token Thai\ncorpus built through a Thai-adapted Dolma pipeline that includes custom\nrule-based language ID, revised C4/Gopher quality filters, and Thai-trained\ncontent filters, plus curated non-web sources such as Wikipedia, Royal Gazette\ntexts, OCR-extracted books, and CC-licensed YouTube subtitles. Systematic\nablations using GPT-2 show the pipeline trims CommonCrawl from 202M to 25M\ndocuments while raising SEA-HELM NLG from 3 to 11; an 8B-parameter SEA-LION\nmodel continually pre-trained on Mangosteen then surpasses SEA-LION-v3 and\nLlama-3.1 by about four points on Thai benchmarks. We release the full pipeline\ncode, cleaning manifests, corpus snapshot, and all checkpoints, providing a\nfully reproducible foundation for future Thai and regional LLM research.", "AI": {"tldr": "Mangosteen, a 47B-token Thai corpus, was built using a Thai-adapted Dolma pipeline and improves performance on Thai benchmarks.", "motivation": "Raw web text is noisy, and existing corpora don't capture Thai script or cultural nuances, leaving risky material untreated. Prior Thai-specific efforts lack reproducibility.", "method": "A Thai-adapted Dolma pipeline with custom language ID, quality filters, and content filters, plus curated non-web sources.", "result": "The pipeline trims CommonCrawl from 202M to 25M documents while raising SEA-HELM NLG from 3 to 11. An 8B-parameter SEA-LION model surpasses SEA-LION-v3 and Llama-3.1 by about four points on Thai benchmarks.", "conclusion": "A 47 billion-token Thai corpus, Mangosteen, was created using a Thai-adapted Dolma pipeline. A SEA-LION model pre-trained on Mangosteen surpasses previous models on Thai benchmarks."}}
{"id": "2507.14553", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.14553", "abs": "https://arxiv.org/abs/2507.14553", "authors": ["Xiaoran Wu"], "title": "Clutter Detection and Removal by Multi-Objective Analysis for Photographic Guidance", "comment": null, "summary": "Clutter in photos is a distraction preventing photographers from conveying\nthe intended emotions or stories to the audience. Photography amateurs\nfrequently include clutter in their photos due to unconscious negligence or the\nlack of experience in creating a decluttered, aesthetically appealing scene for\nshooting. We are thus motivated to develop a camera guidance system that\nprovides solutions and guidance for clutter identification and removal. We\nestimate and visualize the contribution of objects to the overall aesthetics\nand content of a photo, based on which users can interactively identify\nclutter. Suggestions on getting rid of clutter, as well as a tool that removes\ncluttered objects computationally, are provided to guide users to deal with\ndifferent kinds of clutter and improve their photographic work. Two technical\nnovelties underpin interactions in our system: a clutter distinguishment\nalgorithm with aesthetics evaluations for objects and an iterative image\ninpainting algorithm based on generative adversarial nets that reconstructs\nmissing regions of removed objects for high-resolution images. User studies\ndemonstrate that our system provides flexible interfaces and accurate\nalgorithms that allow users to better identify distractions and take higher\nquality images within less time.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u76f8\u673a\u6307\u5bfc\u7cfb\u7edf\uff0c\u53ef\u4ee5\u8bc6\u522b\u548c\u79fb\u9664\u7167\u7247\u4e2d\u7684\u6742\u7269\uff0c\u4ece\u800c\u5e2e\u52a9\u7528\u6237\u62cd\u6444\u66f4\u9ad8\u8d28\u91cf\u7684\u7167\u7247\u3002", "motivation": "\u6444\u5f71\u7231\u597d\u8005\u7531\u4e8e\u65e0\u610f\u8bc6\u7684\u758f\u5ffd\u6216\u7f3a\u4e4f\u521b\u9020\u6574\u6d01\u3001\u7f8e\u89c2\u7684\u62cd\u6444\u573a\u666f\u7684\u7ecf\u9a8c\uff0c\u7ecf\u5e38\u5728\u7167\u7247\u4e2d\u5305\u542b\u6742\u7269\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u6709\u52a8\u529b\u5f00\u53d1\u4e00\u79cd\u76f8\u673a\u6307\u5bfc\u7cfb\u7edf\uff0c\u4e3a\u6742\u7269\u8bc6\u522b\u548c\u79fb\u9664\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u548c\u6307\u5bfc\u3002", "method": "\u4e00\u79cd\u5177\u6709\u5bf9\u8c61\u7f8e\u5b66\u8bc4\u4f30\u7684\u6742\u4e71\u533a\u5206\u7b97\u6cd5\uff0c\u4ee5\u53ca\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u7684\u8fed\u4ee3\u56fe\u50cf\u4fee\u590d\u7b97\u6cd5\u3002", "result": "\u8be5\u7cfb\u7edf\u53ef\u4ee5\u4f30\u8ba1\u548c\u53ef\u89c6\u5316\u5bf9\u8c61\u5bf9\u7167\u7247\u6574\u4f53\u7f8e\u5b66\u548c\u5185\u5bb9\u7684\u8d21\u732e\uff0c\u7528\u6237\u53ef\u4ee5\u4ea4\u4e92\u5f0f\u5730\u8bc6\u522b\u6742\u7269\u3002\u63d0\u4f9b\u4e86\u5173\u4e8e\u53bb\u9664\u6742\u7269\u7684\u5efa\u8bae\uff0c\u4ee5\u53ca\u4e00\u79cd\u901a\u8fc7\u8ba1\u7b97\u53bb\u9664\u6742\u4e71\u5bf9\u8c61\uff0c\u91cd\u5efa\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e2d\u7f3a\u5931\u533a\u57df\u7684\u5de5\u5177\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u7075\u6d3b\u7684\u754c\u9762\u548c\u51c6\u786e\u7684\u7b97\u6cd5\uff0c\u4f7f\u7528\u6237\u80fd\u591f\u5728\u66f4\u77ed\u7684\u65f6\u95f4\u5185\u66f4\u597d\u5730\u8bc6\u522b\u5e72\u6270\u56e0\u7d20\u5e76\u62cd\u6444\u66f4\u9ad8\u8d28\u91cf\u7684\u7167\u7247\u3002"}}
{"id": "2507.14387", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14387", "abs": "https://arxiv.org/abs/2507.14387", "authors": ["Arun Vignesh Malarkkan", "Dongjie Wang", "Haoyue Bai", "Yanjie Fu"], "title": "Incremental Causal Graph Learning for Online Cyberattack Detection in Cyber-Physical Infrastructures", "comment": "12 pages, 5 figures, 3 Tables, under review in IEEE Transactions on\n  Big Data", "summary": "The escalating threat of cyberattacks on real-time critical infrastructures\nposes serious risks to public safety, demanding detection methods that\neffectively capture complex system interdependencies and adapt to evolving\nattack patterns. Traditional real-time anomaly detection techniques often\nsuffer from excessive false positives due to their statistical sensitivity to\nhigh data variance and class imbalance. To address these limitations, recent\nresearch has explored modeling causal relationships among system components.\nHowever, prior work mainly focuses on offline causal graph-based approaches\nthat require static historical data and fail to generalize to real-time\nsettings. These methods are fundamentally constrained by: (1) their inability\nto adapt to dynamic shifts in data distribution without retraining, and (2) the\nrisk of catastrophic forgetting when lacking timely supervision in live\nsystems. To overcome these challenges, we propose INCADET, a novel framework\nfor incremental causal graph learning tailored to real-time cyberattack\ndetection. INCADET dynamically captures evolving system behavior by\nincrementally updating causal graphs across streaming time windows. The\nframework comprises three modules: 1) Early Symptom Detection: Detects\ntransitions in system status using divergence in edge-weight distributions\nacross sequential causal graphs. 2) Incremental Causal Graph Learning:\nLeverages experience replay and edge reinforcement to continually refine causal\nstructures while preserving prior knowledge. 3) Causal Graph Classification:\nEmploys Graph Convolutional Networks (GCNs) to classify system status using the\nlearned causal graphs. Extensive experiments on real-world critical\ninfrastructure datasets demonstrate that INCADET achieves superior accuracy,\nrobustness, and adaptability compared to both static causal and deep temporal\nbaselines in evolving attack scenarios.", "AI": {"tldr": "INCADET \u662f\u4e00\u79cd\u7528\u4e8e\u5b9e\u65f6\u7f51\u7edc\u653b\u51fb\u68c0\u6d4b\u7684\u589e\u91cf\u56e0\u679c\u56fe\u5b66\u4e60\u6846\u67b6\uff0c\u5b83\u4f18\u4e8e\u73b0\u6709\u7684\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5b9e\u65f6\u5f02\u5e38\u68c0\u6d4b\u6280\u672f\u5bb9\u6613\u51fa\u73b0\u8fc7\u591a\u7684\u8bef\u62a5\uff0c\u5e76\u4e14\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u6570\u636e\u5206\u5e03\u548c\u707e\u96be\u6027\u9057\u5fd8\u3002", "method": "INCADET\uff0c\u4e00\u4e2a\u7528\u4e8e\u5b9e\u65f6\u7f51\u7edc\u653b\u51fb\u68c0\u6d4b\u7684\u589e\u91cf\u56e0\u679c\u56fe\u5b66\u4e60\u6846\u67b6\u3002\u5b83\u901a\u8fc7\u8de8\u6d41\u65f6\u95f4\u7a97\u53e3\u589e\u91cf\u66f4\u65b0\u56e0\u679c\u56fe\u6765\u52a8\u6001\u6355\u83b7\u4e0d\u65ad\u6f14\u53d8\u7684\u7cfb\u7edf\u884c\u4e3a\u3002\u8be5\u6846\u67b6\u5305\u62ec\u4e09\u4e2a\u6a21\u5757\uff1a1) \u65e9\u671f\u75c7\u72b6\u68c0\u6d4b\uff1b2) \u589e\u91cf\u56e0\u679c\u56fe\u5b66\u4e60\uff1b3) \u56e0\u679c\u56fe\u5206\u7c7b\u3002", "result": "INCADET \u5b9e\u73b0\u4e86\u4f18\u4e8e\u9759\u6001\u56e0\u679c\u548c\u6df1\u5ea6\u65f6\u95f4\u57fa\u7ebf\u7684\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "INCADET\u5728\u771f\u5b9e\u4e16\u754c\u7684\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4f18\u4e8e\u9759\u6001\u56e0\u679c\u548c\u6df1\u5ea6\u65f6\u95f4\u57fa\u7ebf\u7684\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2507.14912", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14912", "abs": "https://arxiv.org/abs/2507.14912", "authors": ["Ruhul Amin Khalil", "Kashif Ahmad", "Hazrat Ali"], "title": "Redefining Elderly Care with Agentic AI: Challenges and Opportunities", "comment": null, "summary": "The global ageing population necessitates new and emerging strategies for\ncaring for older adults. In this article, we explore the potential for\ntransformation in elderly care through Agentic Artificial Intelligence (AI),\npowered by Large Language Models (LLMs). We discuss the proactive and\nautonomous decision-making facilitated by Agentic AI in elderly care.\nPersonalized tracking of health, cognitive care, and environmental management,\nall aimed at enhancing independence and high-level living for older adults,\nrepresents important areas of application. With a potential for significant\ntransformation of elderly care, Agentic AI also raises profound concerns about\ndata privacy and security, decision independence, and access. We share key\ninsights to emphasize the need for ethical safeguards, privacy protections, and\ntransparent decision-making. Our goal in this article is to provide a balanced\ndiscussion of both the potential and the challenges associated with Agentic AI,\nand to provide insights into its responsible use in elderly care, to bring\nAgentic AI into harmony with the requirements and vulnerabilities specific to\nthe elderly. Finally, we identify the priorities for the academic research\ncommunities, to achieve human-centered advancements and integration of Agentic\nAI in elderly care. To the best of our knowledge, this is no existing study\nthat reviews the role of Agentic AI in elderly care. Hence, we address the\nliterature gap by analyzing the unique capabilities, applications, and\nlimitations of LLM-based Agentic AI in elderly care. We also provide a\ncompanion interactive dashboard at https://hazratali.github.io/agenticai/.", "AI": {"tldr": "This paper explores the potential and challenges of using Agentic AI in elderly care, emphasizing the need for ethical considerations and responsible use.", "motivation": "The global ageing population necessitates new strategies for caring for older adults, and there is no existing study that reviews the role of Agentic AI in elderly care.", "method": "The paper analyzes the capabilities, applications, and limitations of LLM-based Agentic AI in elderly care.", "result": "Personalized tracking of health, cognitive care, and environmental management can enhance independence and high-level living for older adults.", "conclusion": "Agentic AI has the potential to transform elderly care but raises concerns about data privacy, security, decision independence and access. The paper emphasizes the need for ethical safeguards, privacy protections, and transparent decision-making."}}
{"id": "2507.14681", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14681", "abs": "https://arxiv.org/abs/2507.14681", "authors": ["Vinicius Anjos de Almeida", "Vinicius de Camargo", "Raquel G\u00f3mez-Bravo", "Egbert van der Haring", "Kees van Boven", "Marcelo Finger", "Luis Fernandez Lopez"], "title": "Large Language Models as Medical Codes Selectors: a benchmark using the International Classification of Primary Care", "comment": "To be submitted to peer-reviewed journal. 33 pages, 10 figures\n  (including appendix), 15 tables (including appendix). For associated code\n  repository, see https://github.com/almeidava93/llm-as-code-selectors-paper", "summary": "Background: Medical coding structures healthcare data for research, quality\nmonitoring, and policy. This study assesses the potential of large language\nmodels (LLMs) to assign ICPC-2 codes using the output of a domain-specific\nsearch engine.\n  Methods: A dataset of 437 Brazilian Portuguese clinical expressions, each\nannotated with ICPC-2 codes, was used. A semantic search engine (OpenAI's\ntext-embedding-3-large) retrieved candidates from 73,563 labeled concepts.\nThirty-three LLMs were prompted with each query and retrieved results to select\nthe best-matching ICPC-2 code. Performance was evaluated using F1-score, along\nwith token usage, cost, response time, and format adherence.\n  Results: Twenty-eight models achieved F1-score > 0.8; ten exceeded 0.85. Top\nperformers included gpt-4.5-preview, o3, and gemini-2.5-pro. Retriever\noptimization can improve performance by up to 4 points. Most models returned\nvalid codes in the expected format, with reduced hallucinations. Smaller models\n(<3B) struggled with formatting and input length.\n  Conclusions: LLMs show strong potential for automating ICPC-2 coding, even\nwithout fine-tuning. This work offers a benchmark and highlights challenges,\nbut findings are limited by dataset scope and setup. Broader, multilingual,\nend-to-end evaluations are needed for clinical validation.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u5316ICPC-2\u7f16\u7801\u65b9\u9762\u663e\u793a\u51fa\u5e0c\u671b\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u7684\u4e34\u5e8a\u9a8c\u8bc1\u3002", "motivation": "\u533b\u5b66\u7f16\u7801\u6784\u5efa\u4e86\u7528\u4e8e\u7814\u7a76\u3001\u8d28\u91cf\u76d1\u63a7\u548c\u653f\u7b56\u7684\u533b\u7597\u4fdd\u5065\u6570\u636e\u3002\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4f7f\u7528\u9886\u57df\u7279\u5b9a\u641c\u7d22\u5f15\u64ce\u7684\u8f93\u51fa\u5206\u914dICPC-2\u4ee3\u7801\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u7528\u4e86\u4e00\u4e2a\u5305\u542b437\u4e2a\u8461\u8404\u7259\u8bed\u4e34\u5e8a\u8868\u8fbe\u5f0f\u7684\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u8868\u8fbe\u5f0f\u90fd\u7528ICPC-2\u4ee3\u7801\u8fdb\u884c\u6ce8\u91ca\u3002\u4e00\u4e2a\u8bed\u4e49\u641c\u7d22\u5f15\u64ce\uff08OpenAI\u7684text-embedding-3-large\uff09\u4ece73,563\u4e2a\u6807\u8bb0\u7684\u6982\u5ff5\u4e2d\u68c0\u7d22\u5019\u9009\u4ee3\u7801\u3002\u4f7f\u7528\u6bcf\u4e2a\u67e5\u8be2\u548c\u68c0\u7d22\u7ed3\u679c\u63d0\u793a\u4e8633\u4e2aLLM\uff0c\u4ee5\u9009\u62e9\u6700\u4f73\u5339\u914d\u7684ICPC-2\u4ee3\u7801\u3002\u4f7f\u7528F1\u5206\u6570\u4ee5\u53ca\u4ee4\u724c\u4f7f\u7528\u91cf\u3001\u6210\u672c\u3001\u54cd\u5e94\u65f6\u95f4\u548c\u683c\u5f0f\u4f9d\u4ece\u6027\u6765\u8bc4\u4f30\u6027\u80fd\u3002", "result": "28\u4e2a\u6a21\u578b\u7684F1\u5206\u6570> 0.8\uff1b10\u4e2a\u6a21\u578b\u8d85\u8fc70.85\u3002\u8868\u73b0\u6700\u4f73\u7684\u6a21\u578b\u5305\u62ecgpt-4.5-preview\u3001o3\u548cgemini-2.5-pro\u3002\u68c0\u7d22\u5668\u4f18\u5316\u53ef\u4ee5\u5c06\u6027\u80fd\u63d0\u9ad8\u9ad8\u8fbe4\u4e2a\u70b9\u3002\u5927\u591a\u6570\u6a21\u578b\u4ee5\u9884\u671f\u683c\u5f0f\u8fd4\u56de\u6709\u6548\u4ee3\u7801\uff0c\u51cf\u5c11\u4e86\u5e7b\u89c9\u3002\u8f83\u5c0f\u7684\u6a21\u578b\uff08<3B\uff09\u5728\u683c\u5f0f\u548c\u8f93\u5165\u957f\u5ea6\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "LLMs\u8868\u73b0\u51fa\u5728\u81ea\u52a8\u5316ICPC-2\u7f16\u7801\u65b9\u9762\u7684\u5f3a\u5927\u6f5c\u529b\uff0c\u5373\u4f7f\u6ca1\u6709\u8fdb\u884c\u5fae\u8c03\u3002\u8fd9\u9879\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u51c6\u5e76\u5f3a\u8c03\u4e86\u6311\u6218\uff0c\u4f46\u7814\u7a76\u7ed3\u679c\u53d7\u5230\u6570\u636e\u96c6\u8303\u56f4\u548c\u8bbe\u7f6e\u7684\u9650\u5236\u3002\u9700\u8981\u66f4\u5e7f\u6cdb\u7684\u3001\u591a\u8bed\u8a00\u7684\u3001\u7aef\u5230\u7aef\u7684\u8bc4\u4f30\u6765\u8fdb\u884c\u4e34\u5e8a\u9a8c\u8bc1\u3002"}}
{"id": "2507.14555", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14555", "abs": "https://arxiv.org/abs/2507.14555", "authors": ["Jintang Xue", "Ganning Zhao", "Jie-En Yao", "Hong-En Chen", "Yue Hu", "Meida Chen", "Suya You", "C. -C. Jay Kuo"], "title": "Descrip3D: Enhancing Large Language Model-based 3D Scene Understanding with Object-Level Text Descriptions", "comment": null, "summary": "Understanding 3D scenes goes beyond simply recognizing objects; it requires\nreasoning about the spatial and semantic relationships between them. Current 3D\nscene-language models often struggle with this relational understanding,\nparticularly when visual embeddings alone do not adequately convey the roles\nand interactions of objects. In this paper, we introduce Descrip3D, a novel and\npowerful framework that explicitly encodes the relationships between objects\nusing natural language. Unlike previous methods that rely only on 2D and 3D\nembeddings, Descrip3D enhances each object with a textual description that\ncaptures both its intrinsic attributes and contextual relationships. These\nrelational cues are incorporated into the model through a dual-level\nintegration: embedding fusion and prompt-level injection. This allows for\nunified reasoning across various tasks such as grounding, captioning, and\nquestion answering, all without the need for task-specific heads or additional\nsupervision. When evaluated on five benchmark datasets, including ScanRefer,\nMulti3DRefer, ScanQA, SQA3D, and Scan2Cap, Descrip3D consistently outperforms\nstrong baseline models, demonstrating the effectiveness of language-guided\nrelational representation for understanding complex indoor scenes.", "AI": {"tldr": "Descrip3D\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u663e\u5f0f\u7f16\u7801\u5bf9\u8c61\u5173\u7cfb\uff0c\u4f18\u4e8e\u73b0\u67093D\u573a\u666f\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u76843D\u573a\u666f\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u96be\u4ee5\u7406\u89e3\u5173\u7cfb\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u5d4c\u5165\u4e0d\u8db3\u4ee5\u5145\u5206\u8868\u8fbe\u5bf9\u8c61\u7684\u89d2\u8272\u548c\u4ea4\u4e92\u65f6\u3002", "method": "Descrip3D\u901a\u8fc7\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u663e\u5f0f\u7f16\u7801\u5bf9\u8c61\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u589e\u5f3a\u4e86\u6bcf\u4e2a\u5bf9\u8c61\u7684\u6587\u672c\u63cf\u8ff0\uff0c\u5e76\u901a\u8fc7\u5d4c\u5165\u878d\u5408\u548c\u63d0\u793a\u7ea7\u522b\u6ce8\u5165\u5c06\u8fd9\u4e9b\u5173\u7cfb\u7ebf\u7d22\u6574\u5408\u5230\u6a21\u578b\u4e2d\u3002", "result": "Descrip3D\u5728ScanRefer\u3001Multi3DRefer\u3001ScanQA\u3001SQA3D\u548cScan2Cap\u7b49\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "Descrip3D\u901a\u8fc7\u5728\u5bf9\u8c61\u4e4b\u95f4\u663e\u5f0f\u7f16\u7801\u5173\u7cfb\uff0c\u5e76\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u4ece\u800c\u5728\u7406\u89e3\u590d\u6742\u5ba4\u5185\u573a\u666f\u65b9\u9762\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002"}}
{"id": "2507.14419", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14419", "abs": "https://arxiv.org/abs/2507.14419", "authors": ["Guojun Wu"], "title": "It's Not That Simple. An Analysis of Simple Test-Time Scaling", "comment": null, "summary": "Prior work proposed simple test-time scaling, a method for replicating this\nscaling behavior with models distilled from o1-like models by manually\ncontrolling test-time compute: either scaling down by enforcing a maximum\nlength or scaling up by iteratively appending \"Wait\" when the model is about to\nterminate its generation. This paper presents an analysis of simple test-time\nscaling and finds that the scaling behavior is largely attributed to scaling\ndown by enforcing a maximum length. In contrast, fine-tuning on long CoT data\ndistilled from o1-like models has no significant impact on scaling behavior,\nand scaling up by appending \"Wait\" leads to inconsistencies, as the model may\noscillate between solutions. A key distinction exists between scaling down by\nenforcing a maximum length and scaling up test-time compute in o1-like models,\nsuch as DeepSeek-R1\\@. These models are typically allowed to utilize as much\ncompute as needed, with the only constraint being the model's maximum supported\nlength. By learning to naturally scale up test-time compute during\nreinforcement learning, o1-like models surpass their peak performance when\nscaling up. In contrast, simple test-time scaling progressively imposes a lower\nupper limit on model performance as it scales down. While replicating the\ntest-time scaling behavior of o1 models can be straightforward by scaling down,\nit is crucial to recognize that the goal of scaling test-time compute is to\nunlock higher performance -- beyond what the model could originally achieve --\nrather than merely reproducing the appearance of scaling behavior.", "AI": {"tldr": "\u7b80\u5355\u6d4b\u8bd5\u65f6\u7f29\u653e\u7684\u884c\u4e3a\u4e3b\u8981\u5f52\u56e0\u4e8e\u901a\u8fc7\u5f3a\u5236\u6700\u5927\u957f\u5ea6\u6765\u7f29\u5c0f\u6a21\u578b\u89c4\u6a21\u3002", "motivation": "\u5148\u524d\u7684\u5de5\u4f5c\u63d0\u51fa\u4e86\u7b80\u5355\u7684\u6d4b\u8bd5\u65f6\u7f29\u653e\uff0c\u4e00\u79cd\u901a\u8fc7\u624b\u52a8\u63a7\u5236\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6765\u590d\u5236\u8fd9\u79cd\u7f29\u653e\u884c\u4e3a\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5236\u6700\u5927\u957f\u5ea6\u6765\u7f29\u5c0f\u89c4\u6a21\uff0c\u6216\u8005\u5728\u6a21\u578b\u5373\u5c06\u7ed3\u675f\u5176\u751f\u6210\u65f6\u901a\u8fc7\u8fed\u4ee3\u9644\u52a0\u201c\u7b49\u5f85\u201d\u6765\u6269\u5927\u89c4\u6a21\u3002", "method": "\u5206\u6790\u7b80\u5355\u6d4b\u8bd5\u65f6\u7f29\u653e", "result": "\u53d1\u73b0\u7f29\u653e\u884c\u4e3a\u4e3b\u8981\u5f52\u56e0\u4e8e\u901a\u8fc7\u5f3a\u5236\u6700\u5927\u957f\u5ea6\u6765\u7f29\u5c0f\u89c4\u6a21\u3002\u901a\u8fc7\u9644\u52a0\u201c\u7b49\u5f85\u201d\u6765\u6269\u5927\u89c4\u6a21\u4f1a\u5bfc\u81f4\u4e0d\u4e00\u81f4\u3002", "conclusion": "\u7b80\u5355\u6d4b\u8bd5\u65f6\u7f29\u653e\u7684\u884c\u4e3a\u4e3b\u8981\u5f52\u56e0\u4e8e\u901a\u8fc7\u5f3a\u5236\u6700\u5927\u957f\u5ea6\u6765\u7f29\u5c0f\u6a21\u578b\u89c4\u6a21\u3002\u4e0e\u6b64\u76f8\u53cd\uff0c\u5728\u4eceo1\u7c7b\u6a21\u578b\u4e2d\u63d0\u53d6\u7684\u957fCoT\u6570\u636e\u4e0a\u8fdb\u884c\u5fae\u8c03\u5bf9\u7f29\u653e\u884c\u4e3a\u6ca1\u6709\u663e\u8457\u5f71\u54cd\uff0c\u5e76\u4e14\u901a\u8fc7\u9644\u52a0\u201c\u7b49\u5f85\u201d\u6765\u6269\u5927\u89c4\u6a21\u4f1a\u5bfc\u81f4\u4e0d\u4e00\u81f4\uff0c\u56e0\u4e3a\u6a21\u578b\u53ef\u80fd\u5728\u89e3\u51b3\u65b9\u6848\u4e4b\u95f4\u632f\u8361\u3002"}}
{"id": "2507.14962", "categories": ["cs.AI", "cs.CC", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.14962", "abs": "https://arxiv.org/abs/2507.14962", "authors": ["Johannes Schmidt", "Mohamed Maizia", "Victor Lagerkvist", "Johannes K. Fichte"], "title": "Complexity of Faceted Explanations in Propositional Abduction", "comment": "This is the author's self-archived copy including detailed proofs. To\n  appear in Theory and Practice of Logic Programming (TPLP), Proceedings of the\n  41st International Conference on Logic Programming (ICLP 2025)", "summary": "Abductive reasoning is a popular non-monotonic paradigm that aims to explain\nobserved symptoms and manifestations. It has many applications, such as\ndiagnosis and planning in artificial intelligence and database updates. In\npropositional abduction, we focus on specifying knowledge by a propositional\nformula. The computational complexity of tasks in propositional abduction has\nbeen systematically characterized - even with detailed classifications for\nBoolean fragments. Unsurprisingly, the most insightful reasoning problems\n(counting and enumeration) are computationally highly challenging. Therefore,\nwe consider reasoning between decisions and counting, allowing us to understand\nexplanations better while maintaining favorable complexity. We introduce facets\nto propositional abductions, which are literals that occur in some explanation\n(relevant) but not all explanations (dispensable). Reasoning with facets\nprovides a more fine-grained understanding of variability in explanations\n(heterogeneous). In addition, we consider the distance between two\nexplanations, enabling a better understanding of heterogeneity/homogeneity. We\ncomprehensively analyze facets of propositional abduction in various settings,\nincluding an almost complete characterization in Post's framework.", "AI": {"tldr": "This paper introduces and analyzes facets in propositional abduction to better understand explanations and their variability, achieving a comprehensive characterization within Post's framework.", "motivation": "The paper aims to understand explanations better while maintaining favorable complexity in propositional abduction. It addresses the challenge that insightful reasoning problems (counting and enumeration) are computationally highly challenging.", "method": "The paper introduces facets to propositional abductions, which are literals that occur in some explanation but not all explanations. It also considers the distance between two explanations.", "result": "The paper provides a more fine-grained understanding of variability in explanations and enables a better understanding of heterogeneity/homogeneity.", "conclusion": "This paper comprehensively analyzes facets of propositional abduction in various settings, including an almost complete characterization in Post's framework."}}
{"id": "2507.14683", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14683", "abs": "https://arxiv.org/abs/2507.14683", "authors": ["Xingxuan Li", "Yao Xiao", "Dianwen Ng", "Hai Ye", "Yue Deng", "Xiang Lin", "Bin Wang", "Zhanfeng Mo", "Chong Zhang", "Yueyi Zhang", "Zonglin Yang", "Ruilin Li", "Lei Lei", "Shihao Xu", "Han Zhao", "Weiling Chen", "Feng Ji", "Lidong Bing"], "title": "MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via Context-Aware Multi-Stage Policy Optimization", "comment": "Technical report", "summary": "Large language models have recently evolved from fluent text generation to\nadvanced reasoning across diverse domains, giving rise to reasoning language\nmodels. Among these domains, mathematical reasoning serves as a representative\nbenchmark as it requires precise multi-step logic and abstract reasoning, which\ncan be generalized to other tasks. While closed-source RLMs such as GPT-o3\ndemonstrate impressive reasoning capabilities, their proprietary nature limits\ntransparency and reproducibility. Although many open-source projects aim to\nclose this gap, most of them lack sufficient openness by omitting critical\nresources such as datasets and detailed training configurations, which hinders\nreproducibility. To contribute toward greater transparency in RLM development,\nwe introduce the MiroMind-M1 series, a set of fully open-source RLMs built on\nthe Qwen-2.5 backbone that match or exceed the performance of existing\nopen-source RLMs. Specifically, our models are trained in two stages: SFT on a\ncarefully curated corpus of 719K math-reasoning problems with verified CoT\ntrajectories, followed by RLVR on 62K challenging and verifiable problems. To\nenhance the robustness and efficiency of the RLVR process, we introduce\nContext-Aware Multi-Stage Policy Optimization, an algorithm that integrates\nlength-progressive training with an adaptive repetition penalty to encourage\ncontext-aware RL training. Our model achieves state-of-the-art or competitive\nperformance and superior token efficiency among Qwen-2.5-based open-source 7B\nand 32B models on the AIME24, AIME25, and MATH benchmarks. To facilitate\nreproducibility, we release the complete stack: models (MiroMind-M1-SFT-7B,\nMiroMind-M1-RL-7B, MiroMind-M1-RL-32B); datasets (MiroMind-M1-SFT-719K,\nMiroMind-M1-RL-62K); and all training and evaluation configurations. We hope\nthese resources will support further research and foster community advancement.", "AI": {"tldr": "MiroMind-M1, a fully open-source RLM series based on Qwen-2.5, is introduced with complete resources for reproducibility, achieving strong performance in mathematical reasoning benchmarks.", "motivation": "Existing open-source RLMs lack sufficient openness by omitting critical resources such as datasets and detailed training configurations, which hinders reproducibility. This paper aims to contribute toward greater transparency in RLM development.", "method": "The models are trained in two stages: SFT on a carefully curated corpus of 719K math-reasoning problems with verified CoT trajectories, followed by RLVR on 62K challenging and verifiable problems. Context-Aware Multi-Stage Policy Optimization, an algorithm that integrates length-progressive training with an adaptive repetition penalty, is introduced to enhance the robustness and efficiency of the RLVR process.", "result": "The model achieves state-of-the-art or competitive performance and superior token efficiency among Qwen-2.5-based open-source 7B and 32B models on the AIME24, AIME25, and MATH benchmarks.", "conclusion": "The MiroMind-M1 series, a set of fully open-source RLMs built on the Qwen-2.5 backbone, matches or exceeds the performance of existing open-source RLMs. The complete stack, including models, datasets, and all training and evaluation configurations, is released to facilitate reproducibility and foster community advancement."}}
{"id": "2507.14559", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14559", "abs": "https://arxiv.org/abs/2507.14559", "authors": ["Zixuan Hu", "Xiaotong Li", "Shixiang Tang", "Jun Liu", "Yichun Hu", "Ling-Yu Duan"], "title": "LEAD: Exploring Logit Space Evolution for Model Selection", "comment": "Accepted by CVPR 2024", "summary": "The remarkable success of pretrain-then-finetune paradigm has led to a\nproliferation of available pre-trained models for vision tasks. This surge\npresents a significant challenge in efficiently choosing the most suitable\npre-trained models for downstream tasks. The critical aspect of this challenge\nlies in effectively predicting the model transferability by considering the\nunderlying fine-tuning dynamics. Existing methods often model fine-tuning\ndynamics in feature space with linear transformations, which do not precisely\nalign with the fine-tuning objective and fail to grasp the essential\nnonlinearity from optimization. To this end, we present LEAD, a\nfinetuning-aligned approach based on the network output of logits. LEAD\nproposes a theoretical framework to model the optimization process and derives\nan ordinary differential equation (ODE) to depict the nonlinear evolution\ntoward the final logit state. Additionally, we design a class-aware\ndecomposition method to consider the varying evolution dynamics across classes\nand further ensure practical applicability. Integrating the closely aligned\noptimization objective and nonlinear modeling capabilities derived from the\ndifferential equation, our method offers a concise solution to effectively\nbridge the optimization gap in a single step, bypassing the lengthy fine-tuning\nprocess. The comprehensive experiments on 24 supervised and self-supervised\npre-trained models across 10 downstream datasets demonstrate impressive\nperformances and showcase its broad adaptability even in low-data scenarios.", "AI": {"tldr": "LEAD\u901a\u8fc7\u5efa\u6a21\u4f18\u5316\u8fc7\u7a0b\u5e76\u5bfc\u51fa\u4e00\u4e2a\u5e38\u5fae\u5206\u65b9\u7a0b\u6765\u63cf\u8ff0\u5411\u6700\u7ec8logit\u72b6\u6001\u7684\u975e\u7ebf\u6027\u6f14\u5316\uff0c\u4ece\u800c\u6709\u6548\u5730\u9884\u6d4b\u6a21\u578b\u7684\u53ef\u8fc1\u79fb\u6027\uff0c\u9009\u62e9\u6700\u9002\u5408\u4e0b\u6e38\u4efb\u52a1\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "motivation": "\u9884\u8bad\u7ec3-\u5fae\u8c03\u8303\u4f8b\u7684\u663e\u8457\u6210\u529f\u5bfc\u81f4\u4e86\u53ef\u7528\u4e8e\u89c6\u89c9\u4efb\u52a1\u7684\u53ef\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6fc0\u589e\u3002\u8fd9\u79cd\u6fc0\u589e\u5728\u6709\u6548\u5730\u9009\u62e9\u6700\u9002\u5408\u4e0b\u6e38\u4efb\u52a1\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u65b9\u9762\u63d0\u51fa\u4e86\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u8fd9\u9879\u6311\u6218\u7684\u5173\u952e\u5728\u4e8e\u901a\u8fc7\u8003\u8651\u6f5c\u5728\u7684\u5fae\u8c03\u52a8\u6001\u6765\u6709\u6548\u9884\u6d4b\u6a21\u578b\u7684\u53ef\u8fc1\u79fb\u6027\u3002", "method": "LEAD\u63d0\u51fa\u4e86\u4e00\u79cd\u7406\u8bba\u6846\u67b6\u6765\u5efa\u6a21\u4f18\u5316\u8fc7\u7a0b\uff0c\u5e76\u63a8\u5bfc\u51fa\u4e00\u4e2a\u5e38\u5fae\u5206\u65b9\u7a0b\uff08ODE\uff09\u6765\u63cf\u8ff0\u5411\u6700\u7ec8logit\u72b6\u6001\u7684\u975e\u7ebf\u6027\u6f14\u5316\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7c7b\u611f\u77e5\u5206\u89e3\u65b9\u6cd5\u6765\u8003\u8651\u4e0d\u540c\u7c7b\u522b\u7684\u4e0d\u540c\u6f14\u5316\u52a8\u6001\uff0c\u5e76\u8fdb\u4e00\u6b65\u786e\u4fdd\u5b9e\u9645\u9002\u7528\u6027\u3002", "result": "LEAD\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u6d01\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u4ee5\u5728\u4e00\u6b65\u4e4b\u5185\u6709\u6548\u5730\u5f25\u5408\u4f18\u5316\u5dee\u8ddd\uff0c\u4ece\u800c\u7ed5\u8fc7\u6f2b\u957f\u7684\u5fae\u8c03\u8fc7\u7a0b\u3002", "conclusion": "LEAD\u572824\u4e2a\u76d1\u7763\u548c\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6a21\u578b\u4ee5\u53ca10\u4e2a\u4e0b\u6e38\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cLEAD\u5177\u6709\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5e7f\u6cdb\u7684\u9002\u5e94\u6027\uff0c\u5373\u4f7f\u5728\u4f4e\u6570\u636e\u573a\u666f\u4e2d\u4e5f\u662f\u5982\u6b64\u3002"}}
{"id": "2507.14446", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14446", "abs": "https://arxiv.org/abs/2507.14446", "authors": ["Feng Liu", "Ying Liu", "Carson Eisenach"], "title": "Deep RL Dual Sourcing Inventory Management with Supply and Capacity Risk Awareness", "comment": null, "summary": "In this work, we study how to efficiently apply reinforcement learning (RL)\nfor solving large-scale stochastic optimization problems by leveraging\nintervention models. The key of the proposed methodology is to better explore\nthe solution space by simulating and composing the stochastic processes using\npre-trained deep learning (DL) models. We demonstrate our approach on a\nchallenging real-world application, the multi-sourcing multi-period inventory\nmanagement problem in supply chain optimization. In particular, we employ deep\nRL models for learning and forecasting the stochastic supply chain processes\nunder a range of assumptions. Moreover, we also introduce a constraint\ncoordination mechanism, designed to forecast dual costs given the\ncross-products constraints in the inventory network. We highlight that instead\nof directly modeling the complex physical constraints into the RL optimization\nproblem and solving the stochastic problem as a whole, our approach breaks down\nthose supply chain processes into scalable and composable DL modules, leading\nto improved performance on large real-world datasets. We also outline open\nproblems for future research to further investigate the efficacy of such\nmodels.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u5e72\u9884\u6a21\u578b\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u62df\u548c\u7ec4\u5408\u968f\u673a\u8fc7\u7a0b\uff0c\u4ee5\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u9ad8\u6548\u89e3\u51b3\u5927\u89c4\u6a21\u968f\u673a\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5728\u4f9b\u5e94\u94fe\u7ba1\u7406\u95ee\u9898\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u6709\u6548\u5730\u5e94\u7528\u5f3a\u5316\u5b66\u4e60 (RL) \u6765\u89e3\u51b3\u5927\u89c4\u6a21\u968f\u673a\u4f18\u5316\u95ee\u9898\u3002", "method": "\u5229\u7528\u5e72\u9884\u6a21\u578b\uff0c\u901a\u8fc7\u4f7f\u7528\u9884\u5148\u8bad\u7ec3\u7684\u6df1\u5ea6\u5b66\u4e60 (DL) \u6a21\u578b\u6a21\u62df\u548c\u7ec4\u5408\u968f\u673a\u8fc7\u7a0b\u6765\u66f4\u597d\u5730\u63a2\u7d22\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\u3002", "result": "\u5728\u4f9b\u5e94\u94fe\u4f18\u5316\u4e2d\u5177\u6709\u6311\u6218\u6027\u7684\u73b0\u5b9e\u5e94\u7528\uff0c\u5373\u591a\u5bfb\u6e90\u591a\u5468\u671f\u5e93\u5b58\u7ba1\u7406\u95ee\u9898\u4e0a\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u91c7\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u6765\u5b66\u4e60\u548c\u9884\u6d4b\u5404\u79cd\u5047\u8bbe\u4e0b\u7684\u968f\u673a\u4f9b\u5e94\u94fe\u6d41\u7a0b\u3002\u5f15\u5165\u4e86\u4e00\u79cd\u7ea6\u675f\u534f\u8c03\u673a\u5236\uff0c\u65e8\u5728\u9884\u6d4b\u5e93\u5b58\u7f51\u7edc\u4e2d\u7ed9\u5b9a\u4ea4\u53c9\u4ea7\u54c1\u7ea6\u675f\u7684\u53cc\u91cd\u6210\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c06\u4f9b\u5e94\u94fe\u6d41\u7a0b\u5206\u89e3\u4e3a\u53ef\u6269\u5c55\u548c\u53ef\u7ec4\u5408\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u5757\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u5927\u578b\u5b9e\u9645\u6570\u636e\u96c6\u7684\u6027\u80fd\u3002"}}
{"id": "2507.14987", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14987", "abs": "https://arxiv.org/abs/2507.14987", "authors": ["Yi Zhang", "An Zhang", "XiuYu Zhang", "Leheng Sheng", "Yuxin Chen", "Zhenkai Liang", "Xiang Wang"], "title": "AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning", "comment": null, "summary": "Large language models (LLMs), despite possessing latent safety understanding\nfrom their vast pretraining data, remain vulnerable to generating harmful\ncontent and exhibit issues such as over-refusal and utility degradation after\nsafety alignment. Current safety alignment methods often result in superficial\nrefusal shortcuts or rely on intensive supervision for reasoning-based\napproaches, failing to fully leverage the model's intrinsic safety\nself-awareness. We propose \\textbf{AlphaAlign}, a simple yet effective pure\nreinforcement learning (RL) framework with verifiable safety reward designed to\nincentivize this latent safety awareness through proactive safety reasoning.}\nAlphaAlign employs a dual-reward system: a verifiable safety reward encourages\ncorrectly formatted and explicitly justified refusals for harmful queries while\npenalizing over-refusals, and a normalized helpfulness reward guides\nhigh-quality responses to benign inputs. This allows the model to develop\nproactive safety reasoning capabilities without depending on supervised\nsafety-specific reasoning data. AlphaAlign demonstrates three key advantages:\n(1) Simplicity and efficiency, requiring only binary prompt safety labels and\nminimal RL steps for substantial improvements. (2) Breaking the safety-utility\ntrade-off, by enhancing refusal of harmful content and reducing over-refusals,\nwhile simultaneously maintaining or even improving general task performance and\nrobustness to unseen jailbreaks. (3) Deep alignment, fostering proactive safety\nreasoning that generates explicit safety rationales rather than relying on\nshallow refusal patterns.", "AI": {"tldr": "AlphaAlign\u662f\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u6548\u7528\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5177\u6709\u6f5c\u5728\u7684\u5b89\u5168\u7406\u89e3\u80fd\u529b\uff0c\u4f46\u4ecd\u7136\u5bb9\u6613\u751f\u6210\u6709\u5bb3\u5185\u5bb9\uff0c\u5e76\u4e14\u5728\u5b89\u5168\u5bf9\u9f50\u540e\u4f1a\u51fa\u73b0\u8fc7\u5ea6\u62d2\u7edd\u548c\u6548\u7528\u964d\u4f4e\u7b49\u95ee\u9898\u3002\u73b0\u6709\u7684\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\u901a\u5e38\u5bfc\u81f4\u80a4\u6d45\u7684\u62d2\u7edd\u6377\u5f84\uff0c\u6216\u8005\u4f9d\u8d56\u4e8e\u5bc6\u96c6\u7684\u76d1\u7763\u3002", "method": "\u63d0\u51faAlphaAlign\uff0c\u4e00\u4e2a\u7b80\u5355\u7684\u7eaf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5177\u6709\u53ef\u9a8c\u8bc1\u7684\u5b89\u5168\u5956\u52b1\uff0c\u65e8\u5728\u6fc0\u52b1\u8fd9\u79cd\u6f5c\u5728\u7684\u5b89\u5168\u610f\u8bc6\u901a\u8fc7\u4e3b\u52a8\u5b89\u5168\u63a8\u7406\u3002", "result": "AlphaAlign\u5177\u6709\u4e09\u4e2a\u5173\u952e\u4f18\u52bf\uff1a\u7b80\u5355\u9ad8\u6548\uff0c\u6253\u7834\u4e86\u5b89\u5168\u4e0e\u6548\u7528\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4fc3\u8fdb\u4e86\u4e3b\u52a8\u5b89\u5168\u63a8\u7406\u3002", "conclusion": "AlphaAlign\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u53ef\u9a8c\u8bc1\u7684\u5b89\u5168\u5956\u52b1\uff0c\u6fc0\u52b1\u6a21\u578b\u6f5c\u5728\u7684\u5b89\u5168\u610f\u8bc6\uff0c\u5b9e\u73b0\u4e86\u66f4\u6df1\u5c42\u6b21\u7684\u5b89\u5168\u5bf9\u9f50\u3002"}}
{"id": "2507.14688", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14688", "abs": "https://arxiv.org/abs/2507.14688", "authors": ["Mohammed Alkhowaiter", "Norah Alshahrani", "Saied Alshahrani", "Reem I. Masoud", "Alaa Alzahrani", "Deema Alnuhait", "Emad A. Alghamdi", "Khalid Almubarak"], "title": "Mind the Gap: A Review of Arabic Post-Training Datasets and Their Limitations", "comment": null, "summary": "Post-training has emerged as a crucial technique for aligning pre-trained\nLarge Language Models (LLMs) with human instructions, significantly enhancing\ntheir performance across a wide range of tasks. Central to this process is the\nquality and diversity of post-training datasets. This paper presents a review\nof publicly available Arabic post-training datasets on the Hugging Face Hub,\norganized along four key dimensions: (1) LLM Capabilities (e.g., Question\nAnswering, Translation, Reasoning, Summarization, Dialogue, Code Generation,\nand Function Calling); (2) Steerability (e.g., persona and system prompts); (3)\nAlignment (e.g., cultural, safety, ethics, and fairness), and (4) Robustness.\nEach dataset is rigorously evaluated based on popularity, practical adoption,\nrecency and maintenance, documentation and annotation quality, licensing\ntransparency, and scientific contribution. Our review revealed critical gaps in\nthe development of Arabic post-training datasets, including limited task\ndiversity, inconsistent or missing documentation and annotation, and low\nadoption across the community. Finally, the paper discusses the implications of\nthese gaps on the progress of Arabic LLMs and applications while providing\nconcrete recommendations for future efforts in post-training dataset\ndevelopment.", "AI": {"tldr": "This paper reviews Arabic post-training datasets, finds gaps in task diversity and documentation, and suggests improvements.", "motivation": "Post-training is crucial for aligning LLMs with human instructions, and the quality of post-training datasets is central to this process.", "method": "Review of publicly available Arabic post-training datasets on the Hugging Face Hub, organized along four key dimensions: LLM Capabilities, Steerability, Alignment, and Robustness. Datasets are evaluated based on popularity, adoption, recency, documentation, licensing, and scientific contribution.", "result": "Revealed critical gaps in the development of Arabic post-training datasets.", "conclusion": "This paper identifies critical gaps in Arabic post-training datasets, including limited task diversity, inconsistent documentation, and low adoption, and recommends future efforts in dataset development."}}
{"id": "2507.14575", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14575", "abs": "https://arxiv.org/abs/2507.14575", "authors": ["Andrea Moschetto", "Lemuel Puglisi", "Alec Sargood", "Pierluigi Dell'Acqua", "Francesco Guarnera", "Sebastiano Battiato", "Daniele Rav\u00ec"], "title": "Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w MRI Translation", "comment": null, "summary": "Magnetic Resonance Imaging (MRI) enables the acquisition of multiple image\ncontrasts, such as T1-weighted (T1w) and T2-weighted (T2w) scans, each offering\ndistinct diagnostic insights. However, acquiring all desired modalities\nincreases scan time and cost, motivating research into computational methods\nfor cross-modal synthesis. To address this, recent approaches aim to synthesize\nmissing MRI contrasts from those already acquired, reducing acquisition time\nwhile preserving diagnostic quality. Image-to-image (I2I) translation provides\na promising framework for this task. In this paper, we present a comprehensive\nbenchmark of generative models$\\unicode{x2013}$specifically, Generative\nAdversarial Networks (GANs), diffusion models, and flow matching (FM)\ntechniques$\\unicode{x2013}$for T1w-to-T2w 2D MRI I2I translation. All\nframeworks are implemented with comparable settings and evaluated on three\npublicly available MRI datasets of healthy adults. Our quantitative and\nqualitative analyses show that the GAN-based Pix2Pix model outperforms\ndiffusion and FM-based methods in terms of structural fidelity, image quality,\nand computational efficiency. Consistent with existing literature, these\nresults suggest that flow-based models are prone to overfitting on small\ndatasets and simpler tasks, and may require more data to match or surpass GAN\nperformance. These findings offer practical guidance for deploying I2I\ntranslation techniques in real-world MRI workflows and highlight promising\ndirections for future research in cross-modal medical image synthesis. Code and\nmodels are publicly available at\nhttps://github.com/AndreaMoschetto/medical-I2I-benchmark.", "AI": {"tldr": "\u672c\u6587\u5bf9\u7528\u4e8eT1w\u5230T2w 2D MRI\u56fe\u50cf\u8f6c\u6362\u7684\u751f\u6210\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8eGAN\u7684Pix2Pix\u6a21\u578b\u4f18\u4e8e\u6269\u6563\u6a21\u578b\u548c\u57fa\u4e8eFM\u7684\u65b9\u6cd5\u3002", "motivation": "\u83b7\u53d6\u6240\u6709\u6240\u9700\u7684MRI\u6a21\u6001\u4f1a\u589e\u52a0\u626b\u63cf\u65f6\u95f4\u548c\u6210\u672c\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u7528\u4e8e\u8de8\u6a21\u6001\u5408\u6210\u7684\u8ba1\u7b97\u65b9\u6cd5\u3002\u76ee\u7684\u662f\u4ece\u5df2\u83b7\u53d6\u7684MRI\u56fe\u50cf\u5408\u6210\u7f3a\u5c11\u7684MRI\u56fe\u50cf\uff0c\u4ece\u800c\u51cf\u5c11\u83b7\u53d6\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u8bca\u65ad\u8d28\u91cf\u3002", "method": "\u4f7f\u7528\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u3001\u6269\u6563\u6a21\u578b\u548c\u6d41\u91cf\u5339\u914d\uff08FM\uff09\u6280\u672f\u8fdb\u884cT1w\u5230T2w\u76842D MRI\u56fe\u50cf\u8f6c\u6362\u3002", "result": "GAN-based Pix2Pix\u6a21\u578b\u5728\u7ed3\u6784\u4fdd\u771f\u5ea6\u3001\u56fe\u50cf\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u6269\u6563\u6a21\u578b\u548c\u57fa\u4e8eFM\u7684\u65b9\u6cd5\u3002", "conclusion": "GAN-based Pix2Pix\u6a21\u578b\u5728\u7ed3\u6784\u4fdd\u771f\u5ea6\u3001\u56fe\u50cf\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u6269\u6563\u6a21\u578b\u548c\u57fa\u4e8eFM\u7684\u65b9\u6cd5\u3002\u6d41\u6a21\u578b\u5bb9\u6613\u5728\u5c0f\u6570\u636e\u96c6\u548c\u7b80\u5355\u4efb\u52a1\u4e0a\u8fc7\u62df\u5408\uff0c\u53ef\u80fd\u9700\u8981\u66f4\u591a\u6570\u636e\u624d\u80fd\u4e0eGAN\u6027\u80fd\u76f8\u5339\u914d\u6216\u8d85\u8fc7GAN\u6027\u80fd\u3002"}}
{"id": "2507.14484", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14484", "abs": "https://arxiv.org/abs/2507.14484", "authors": ["Yule Li", "Yifeng Lu", "Zhen Wang", "Zhewei Wei", "Yaliang Li", "Bolin Ding"], "title": "ReDiSC: A Reparameterized Masked Diffusion Model for Scalable Node Classification with Structured Predictions", "comment": null, "summary": "In recent years, graph neural networks (GNN) have achieved unprecedented\nsuccesses in node classification tasks. Although GNNs inherently encode\nspecific inductive biases (e.g., acting as low-pass or high-pass filters), most\nexisting methods implicitly assume conditional independence among node labels\nin their optimization objectives. While this assumption is suitable for\ntraditional classification tasks such as image recognition, it contradicts the\nintuitive observation that node labels in graphs remain correlated, even after\nconditioning on the graph structure. To make structured predictions for node\nlabels, we propose ReDiSC, namely, Reparameterized masked Diffusion model for\nStructured node Classification. ReDiSC estimates the joint distribution of node\nlabels using a reparameterized masked diffusion model, which is learned through\nthe variational expectation-maximization (EM) framework. Our theoretical\nanalysis shows the efficiency advantage of ReDiSC in the E-step compared to\nDPM-SNC, a state-of-the-art model that relies on a manifold-constrained\ndiffusion model in continuous domain. Meanwhile, we explicitly link ReDiSC's\nM-step objective to popular GNN and label propagation hybrid approaches.\nExtensive experiments demonstrate that ReDiSC achieves superior or highly\ncompetitive performance compared to state-of-the-art GNN, label propagation,\nand diffusion-based baselines across both homophilic and heterophilic graphs of\nvarying sizes. Notably, ReDiSC scales effectively to large-scale datasets on\nwhich previous structured diffusion methods fail due to computational\nconstraints, highlighting its significant practical advantage in structured\nnode classification tasks.", "AI": {"tldr": "ReDiSC\uff1a\u4e00\u79cd\u7528\u4e8e\u7ed3\u6784\u5316\u8282\u70b9\u5206\u7c7b\u7684\u91cd\u65b0\u53c2\u6570\u5316\u63a9\u7801\u6269\u6563\u6a21\u578b\uff0c\u5b83\u4f18\u4e8e\u73b0\u6709 GNN \u548c\u6807\u7b7e\u4f20\u64ad\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u5927\u578b\u6570\u636e\u96c6\u4e0a\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9690\u5f0f\u5730\u5047\u8bbe\u8282\u70b9\u6807\u7b7e\u4e4b\u95f4\u7684\u6761\u4ef6\u72ec\u7acb\u6027\uff0c\u8fd9\u4e0e\u56fe\u4e2d\u7684\u8282\u70b9\u6807\u7b7e\u5373\u4f7f\u5728\u4ee5\u56fe\u7ed3\u6784\u4e3a\u6761\u4ef6\u7684\u60c5\u51b5\u4e0b\u4ecd\u7136\u76f8\u5173\u7684\u76f4\u89c2\u89c2\u5bdf\u76f8\u77db\u76fe\u3002", "method": "\u63d0\u51faReDiSC\uff0c\u5373\u7528\u4e8e\u7ed3\u6784\u5316\u8282\u70b9\u5206\u7c7b\u7684\u91cd\u65b0\u53c2\u6570\u5316\u63a9\u7801\u6269\u6563\u6a21\u578b\uff0c\u5b83\u4f7f\u7528\u901a\u8fc7\u53d8\u5206\u671f\u671b\u6700\u5927\u5316 (EM) \u6846\u67b6\u5b66\u4e60\u7684\u91cd\u65b0\u53c2\u6570\u5316\u63a9\u7801\u6269\u6563\u6a21\u578b\u6765\u4f30\u8ba1\u8282\u70b9\u6807\u7b7e\u7684\u8054\u5408\u5206\u5e03\u3002", "result": "ReDiSC \u5b9e\u73b0\u4e86\u4f18\u4e8e\u6216\u6781\u5177\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u53ef\u4ee5\u6709\u6548\u5730\u6269\u5c55\u5230\u5927\u578b\u6570\u636e\u96c6\u3002", "conclusion": "ReDiSC\u5728\u5404\u79cd\u5927\u5c0f\u7684\u540c\u8d28\u548c\u5f02\u8d28\u56fe\u4e0a\u5b9e\u73b0\u4e86\u4f18\u8d8a\u6216\u6781\u5177\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u53ef\u4ee5\u6709\u6548\u5730\u6269\u5c55\u5230\u5927\u578b\u6570\u636e\u96c6\u3002"}}
{"id": "2507.15013", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15013", "abs": "https://arxiv.org/abs/2507.15013", "authors": ["Xiaoyu Li", "Jin Wu", "Shaoyang Guo", "Haoran Shi", "Chanjin Zheng"], "title": "A Forced-Choice Neural Cognitive Diagnostic Model of Personality Testing", "comment": "15pages, 7 figures", "summary": "In the smart era, psychometric tests are becoming increasingly important for\npersonnel selection, career development, and mental health assessment.\nForced-choice tests are common in personality assessments because they require\nparticipants to select from closely related options, lowering the risk of\nresponse distortion. This study presents a deep learning-based Forced-Choice\nNeural Cognitive Diagnostic Model (FCNCD) that overcomes the limitations of\ntraditional models and is applicable to the three most common item block types\nfound in forced-choice tests. To account for the unidimensionality of items in\nforced-choice tests, we create interpretable participant and item parameters.\nWe model the interactions between participant and item features using\nmultilayer neural networks after mining them using nonlinear mapping. In\naddition, we use the monotonicity assumption to improve the interpretability of\nthe diagnostic results. The FCNCD's effectiveness is validated by experiments\non real-world and simulated datasets that show its accuracy, interpretability,\nand robustness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578bFCNCD\uff0c\u7528\u4e8e\u5f3a\u8feb\u9009\u62e9\u6d4b\u8bd5\uff0c\u7ecf\u9a8c\u8bc1\u6709\u6548\u3002", "motivation": "\u5728\u667a\u80fd\u65f6\u4ee3\uff0c\u5fc3\u7406\u6d4b\u91cf\u6d4b\u8bd5\u5728\u4eba\u5458\u9009\u62d4\u3001\u804c\u4e1a\u53d1\u5c55\u548c\u5fc3\u7406\u5065\u5eb7\u8bc4\u4f30\u4e2d\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u5f3a\u8feb\u9009\u62e9\u6d4b\u8bd5\u5728\u4eba\u683c\u8bc4\u4f30\u4e2d\u5f88\u5e38\u89c1\uff0c\u56e0\u4e3a\u5b83\u4eec\u8981\u6c42\u53c2\u4e0e\u8005\u4ece\u5bc6\u5207\u76f8\u5173\u7684\u9009\u9879\u4e2d\u8fdb\u884c\u9009\u62e9\uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u53cd\u5e94\u626d\u66f2\u7684\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5f3a\u8feb\u9009\u62e9\u795e\u7ecf\u8ba4\u77e5\u8bca\u65ad\u6a21\u578b(FCNCD)\u3002", "result": "\u8be5\u6a21\u578b\u514b\u670d\u4e86\u4f20\u7edf\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u9002\u7528\u4e8e\u5f3a\u8feb\u9009\u62e9\u6d4b\u8bd5\u4e2d\u53d1\u73b0\u7684\u4e09\u79cd\u6700\u5e38\u89c1\u7684\u9879\u76ee\u5757\u7c7b\u578b\u3002\u4e3a\u4e86\u89e3\u91ca\u5f3a\u8feb\u9009\u62e9\u6d4b\u8bd5\u4e2d\u9879\u76ee\u7684\u5355\u7ef4\u6027\uff0c\u6211\u4eec\u521b\u5efa\u4e86\u53ef\u89e3\u91ca\u7684\u53c2\u4e0e\u8005\u548c\u9879\u76ee\u53c2\u6570\u3002\u5728\u4f7f\u7528\u975e\u7ebf\u6027\u6620\u5c04\u6316\u6398\u53c2\u4e0e\u8005\u548c\u9879\u76ee\u7279\u5f81\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u591a\u5c42\u795e\u7ecf\u7f51\u7edc\u5bf9\u5b83\u4eec\u4e4b\u95f4\u7684\u4ea4\u4e92\u8fdb\u884c\u5efa\u6a21\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4f7f\u7528\u5355\u8c03\u6027\u5047\u8bbe\u6765\u63d0\u9ad8\u8bca\u65ad\u7ed3\u679c\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "FCNCD\u5728\u771f\u5b9e\u548c\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.14693", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14693", "abs": "https://arxiv.org/abs/2507.14693", "authors": ["Amina Dzafic", "Merve Kavut", "Ulya Bayram"], "title": "Rethinking Suicidal Ideation Detection: A Trustworthy Annotation Framework and Cross-Lingual Model Evaluation", "comment": "This manuscript has been submitted to the IEEE Journal of Biomedical\n  and Health Informatics", "summary": "Suicidal ideation detection is critical for real-time suicide prevention, yet\nits progress faces two under-explored challenges: limited language coverage and\nunreliable annotation practices. Most available datasets are in English, but\neven among these, high-quality, human-annotated data remains scarce. As a\nresult, many studies rely on available pre-labeled datasets without examining\ntheir annotation process or label reliability. The lack of datasets in other\nlanguages further limits the global realization of suicide prevention via\nartificial intelligence (AI). In this study, we address one of these gaps by\nconstructing a novel Turkish suicidal ideation corpus derived from social media\nposts and introducing a resource-efficient annotation framework involving three\nhuman annotators and two large language models (LLMs). We then address the\nremaining gaps by performing a bidirectional evaluation of label reliability\nand model consistency across this dataset and three popular English suicidal\nideation detection datasets, using transfer learning through eight pre-trained\nsentiment and emotion classifiers. These transformers help assess annotation\nconsistency and benchmark model performance against manually labeled data. Our\nfindings underscore the need for more rigorous, language-inclusive approaches\nto annotation and evaluation in mental health natural language processing (NLP)\nwhile demonstrating the questionable performance of popular models with\nzero-shot transfer learning. We advocate for transparency in model training and\ndataset construction in mental health NLP, prioritizing data and model\nreliability.", "AI": {"tldr": "\u6784\u5efa\u4e86\u4e00\u4e2a\u571f\u8033\u5176\u8bed\u81ea\u6740\u610f\u5ff5\u8bed\u6599\u5e93\uff0c\u8bc4\u4f30\u4e86\u6807\u7b7e\u53ef\u9760\u6027\u548c\u6a21\u578b\u4e00\u81f4\u6027\uff0c\u53d1\u73b0\u9700\u8981\u66f4\u4e25\u8c28\u3001\u66f4\u5177\u8bed\u8a00\u5305\u5bb9\u6027\u7684\u6ce8\u91ca\u548c\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u81ea\u6740\u610f\u5ff5\u68c0\u6d4b\u5bf9\u4e8e\u5b9e\u65f6\u9884\u9632\u81ea\u6740\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u8fdb\u5c55\u9762\u4e34\u4e24\u4e2a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u6311\u6218\uff1a\u8bed\u8a00\u8986\u76d6\u8303\u56f4\u6709\u9650\u548c\u6ce8\u91ca\u5b9e\u8df5\u4e0d\u53ef\u9760\u3002\u5927\u591a\u6570\u53ef\u7528\u7684\u6570\u636e\u96c6\u90fd\u662f\u82f1\u6587\u7684\uff0c\u4f46\u5373\u4f7f\u5728\u8fd9\u4e9b\u6570\u636e\u96c6\u4e2d\uff0c\u9ad8\u8d28\u91cf\u7684\u4eba\u5de5\u6ce8\u91ca\u6570\u636e\u4ecd\u7136\u7a00\u7f3a\u3002\u7f3a\u4e4f\u5176\u4ed6\u8bed\u8a00\u7684\u6570\u636e\u96c6\u8fdb\u4e00\u6b65\u9650\u5236\u4e86\u901a\u8fc7\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u5728\u5168\u7403\u8303\u56f4\u5185\u5b9e\u73b0\u81ea\u6740\u9884\u9632\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u571f\u8033\u5176\u8bed\u81ea\u6740\u610f\u5ff5\u8bed\u6599\u5e93\uff0c\u8be5\u8bed\u6599\u5e93\u6765\u81ea\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u8d44\u6e90\u9ad8\u6548\u7684\u6ce8\u91ca\u6846\u67b6\uff0c\u6d89\u53ca\u4e09\u4e2a\u4eba\u5de5\u6ce8\u91ca\u5458\u548c\u4e24\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u3002\u901a\u8fc7\u516b\u4e2a\u9884\u8bad\u7ec3\u7684\u60c5\u611f\u548c\u60c5\u611f\u5206\u7c7b\u5668\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\uff0c\u5bf9\u8be5\u6570\u636e\u96c6\u548c\u4e09\u4e2a\u6d41\u884c\u7684\u82f1\u8bed\u81ea\u6740\u610f\u5ff5\u68c0\u6d4b\u6570\u636e\u96c6\u7684\u6807\u7b7e\u53ef\u9760\u6027\u548c\u6a21\u578b\u4e00\u81f4\u6027\u8fdb\u884c\u4e86\u53cc\u5411\u8bc4\u4f30\u3002", "result": "\u5f3a\u8c03\u9700\u8981\u5728\u5fc3\u7406\u5065\u5eb7\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u91c7\u7528\u66f4\u4e25\u683c\u3001\u66f4\u5177\u8bed\u8a00\u5305\u5bb9\u6027\u7684\u6ce8\u91ca\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u540c\u65f6\u8bc1\u660e\u4e86\u6d41\u884c\u7684\u6a21\u578b\u5728\u96f6\u6837\u672c\u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u53ef\u7591\u6027\u80fd\u3002", "conclusion": "\u9700\u8981\u66f4\u4e25\u8c28\u3001\u66f4\u5177\u8bed\u8a00\u5305\u5bb9\u6027\u7684\u65b9\u6cd5\u6765\u8fdb\u884c\u5fc3\u7406\u5065\u5eb7\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4e2d\u7684\u6ce8\u91ca\u548c\u8bc4\u4f30\uff0c\u540c\u65f6\u8bc1\u660e\u4e86\u6d41\u884c\u7684\u6a21\u578b\u5728\u96f6\u6837\u672c\u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u53ef\u7591\u8868\u73b0\u3002\u6211\u4eec\u63d0\u5021\u5fc3\u7406\u5065\u5eb7\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u6a21\u578b\u8bad\u7ec3\u548c\u6570\u636e\u96c6\u6784\u5efa\u7684\u900f\u660e\u5ea6\uff0c\u4f18\u5148\u8003\u8651\u6570\u636e\u548c\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2507.14587", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14587", "abs": "https://arxiv.org/abs/2507.14587", "authors": ["Merjem Be\u0107irovi\u0107", "Amina Kurtovi\u0107", "Nordin Smajlovi\u0107", "Medina Kapo", "Amila Akagi\u0107"], "title": "Performance comparison of medical image classification systems using TensorFlow Keras, PyTorch, and JAX", "comment": null, "summary": "Medical imaging plays a vital role in early disease diagnosis and monitoring.\nSpecifically, blood microscopy offers valuable insights into blood cell\nmorphology and the detection of hematological disorders. In recent years, deep\nlearning-based automated classification systems have demonstrated high\npotential in enhancing the accuracy and efficiency of blood image analysis.\nHowever, a detailed performance analysis of specific deep learning frameworks\nappears to be lacking. This paper compares the performance of three popular\ndeep learning frameworks, TensorFlow with Keras, PyTorch, and JAX, in\nclassifying blood cell images from the publicly available BloodMNIST dataset.\nThe study primarily focuses on inference time differences, but also\nclassification performance for different image sizes. The results reveal\nvariations in performance across frameworks, influenced by factors such as\nimage resolution and framework-specific optimizations. Classification accuracy\nfor JAX and PyTorch was comparable to current benchmarks, showcasing the\nefficiency of these frameworks for medical image classification.", "AI": {"tldr": "This paper compares the performance of TensorFlow/Keras, PyTorch, and JAX in classifying blood cell images, revealing performance variations and the efficiency of JAX and PyTorch.", "motivation": "A detailed performance analysis of specific deep learning frameworks appears to be lacking in blood image analysis.", "method": "Compared TensorFlow with Keras, PyTorch, and JAX in classifying blood cell images from the BloodMNIST dataset, focusing on inference time differences and classification performance for different image sizes.", "result": "Variations in performance across frameworks, influenced by factors such as image resolution and framework-specific optimizations.", "conclusion": "JAX and PyTorch showed comparable classification accuracy to current benchmarks, demonstrating their efficiency for medical image classification."}}
{"id": "2507.14487", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14487", "abs": "https://arxiv.org/abs/2507.14487", "authors": ["Ukjo Hwang", "Songnam Hong"], "title": "Federated Reinforcement Learning in Heterogeneous Environments", "comment": null, "summary": "We investigate a Federated Reinforcement Learning with Environment\nHeterogeneity (FRL-EH) framework, where local environments exhibit statistical\nheterogeneity. Within this framework, agents collaboratively learn a global\npolicy by aggregating their collective experiences while preserving the privacy\nof their local trajectories. To better reflect real-world scenarios, we\nintroduce a robust FRL-EH framework by presenting a novel global objective\nfunction. This function is specifically designed to optimize a global policy\nthat ensures robust performance across heterogeneous local environments and\ntheir plausible perturbations. We propose a tabular FRL algorithm named FedRQ\nand theoretically prove its asymptotic convergence to an optimal policy for the\nglobal objective function. Furthermore, we extend FedRQ to environments with\ncontinuous state space through the use of expectile loss, addressing the key\nchallenge of minimizing a value function over a continuous subset of the state\nspace. This advancement facilitates the seamless integration of the principles\nof FedRQ with various Deep Neural Network (DNN)-based RL algorithms. Extensive\nempirical evaluations validate the effectiveness and robustness of our FRL\nalgorithms across diverse heterogeneous environments, consistently achieving\nsuperior performance over the existing state-of-the-art FRL algorithms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9c81\u68d2\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u73af\u5883\u5f02\u6784\u6027\u95ee\u9898\u3002", "motivation": "\u8be5\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u4e2d\u73af\u5883\u5f02\u6784\u6027\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u7b97\u6cd5\u5728\u5f02\u6784\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFedRQ\u7684\u8868\u683c\u578b\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5e76\u5c06\u5176\u6269\u5c55\u5230\u8fde\u7eed\u72b6\u6001\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u5404\u79cd\u5f02\u6784\u73af\u5883\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5e76\u5728\u5f02\u6784\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2507.14741", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14741", "abs": "https://arxiv.org/abs/2507.14741", "authors": ["Maria Sahakyan", "Bedoor AlShebli"], "title": "Disparities in Peer Review Tone and the Role of Reviewer Anonymity", "comment": null, "summary": "The peer review process is often regarded as the gatekeeper of scientific\nintegrity, yet increasing evidence suggests that it is not immune to bias.\nAlthough structural inequities in peer review have been widely debated, much\nless attention has been paid to the subtle ways in which language itself may\nreinforce disparities. This study undertakes one of the most comprehensive\nlinguistic analyses of peer review to date, examining more than 80,000 reviews\nin two major journals. Using natural language processing and large-scale\nstatistical modeling, it uncovers how review tone, sentiment, and supportive\nlanguage vary across author demographics, including gender, race, and\ninstitutional affiliation. Using a data set that includes both anonymous and\nsigned reviews, this research also reveals how the disclosure of reviewer\nidentity shapes the language of evaluation. The findings not only expose hidden\nbiases in peer feedback, but also challenge conventional assumptions about\nanonymity's role in fairness. As academic publishing grapples with reform,\nthese insights raise critical questions about how review policies shape career\ntrajectories and scientific progress.", "AI": {"tldr": "\u540c\u884c\u8bc4\u5ba1\u5b58\u5728\u504f\u89c1\uff0c\u533f\u540d\u8bc4\u5ba1\u672a\u80fd\u5b8c\u5168\u5b9e\u73b0\u516c\u5e73\u3002", "motivation": "\u540c\u884c\u8bc4\u5ba1\u8fc7\u7a0b\u5e76\u975e\u6ca1\u6709\u504f\u89c1\uff0c\u4f46\u8bed\u8a00\u672c\u8eab\u53ef\u80fd\u5f3a\u5316\u5dee\u8ddd\u3002", "method": "\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u5927\u89c4\u6a21\u7edf\u8ba1\u5efa\u6a21\uff0c\u68c0\u67e5\u4e86\u4e24\u4e2a\u4e3b\u8981\u671f\u520a\u7684 80,000 \u591a\u7bc7\u8bc4\u8bba\u3002", "result": "\u8bc4\u5ba1\u8bed\u6c14\u3001\u60c5\u611f\u548c\u652f\u6301\u6027\u8bed\u8a00\u56e0\u4f5c\u8005\u7684\u4eba\u53e3\u7edf\u8ba1\uff08\u5305\u62ec\u6027\u522b\u3001\u79cd\u65cf\u548c\u673a\u6784\u5173\u7cfb\uff09\u800c\u5f02\u3002\u8bc4\u5ba1\u8005\u8eab\u4efd\u7684\u62ab\u9732\u4f1a\u5f71\u54cd\u8bc4\u4f30\u8bed\u8a00\u3002", "conclusion": "\u63ed\u793a\u4e86\u540c\u884c\u8bc4\u5ba1\u4e2d\u9690\u85cf\u7684\u504f\u89c1\uff0c\u5e76\u8d28\u7591\u4e86\u533f\u540d\u5728\u516c\u5e73\u6027\u4e2d\u7684\u4f5c\u7528\u3002"}}
{"id": "2507.14596", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14596", "abs": "https://arxiv.org/abs/2507.14596", "authors": ["Doriand Petit", "Steve Bourgeois", "Vincent Gay-Bellile", "Florian Chabot", "Lo\u00efc Barthe"], "title": "DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary queries in NeRF", "comment": "Published at ICCV'25", "summary": "3D semantic segmentation provides high-level scene understanding for\napplications in robotics, autonomous systems, \\textit{etc}. Traditional methods\nadapt exclusively to either task-specific goals (open-vocabulary segmentation)\nor scene content (unsupervised semantic segmentation). We propose DiSCO-3D, the\nfirst method addressing the broader problem of 3D Open-Vocabulary Sub-concepts\nDiscovery, which aims to provide a 3D semantic segmentation that adapts to both\nthe scene and user queries. We build DiSCO-3D on Neural Fields representations,\ncombining unsupervised segmentation with weak open-vocabulary guidance. Our\nevaluations demonstrate that DiSCO-3D achieves effective performance in\nOpen-Vocabulary Sub-concepts Discovery and exhibits state-of-the-art results in\nthe edge cases of both open-vocabulary and unsupervised segmentation.", "AI": {"tldr": "DiSCO-3D is the first method addressing the broader problem of 3D Open-Vocabulary Sub-concepts Discovery, which aims to provide a 3D semantic segmentation that adapts to both the scene and user queries.", "motivation": "Traditional methods adapt exclusively to either task-specific goals (open-vocabulary segmentation) or scene content (unsupervised semantic segmentation).", "method": "DiSCO-3D is built on Neural Fields representations, combining unsupervised segmentation with weak open-vocabulary guidance.", "result": "DiSCO-3D achieves effective performance in Open-Vocabulary Sub-concepts Discovery and exhibits state-of-the-art results in the edge cases of both open-vocabulary and unsupervised segmentation.", "conclusion": "DiSCO-3D achieves effective performance in Open-Vocabulary Sub-concepts Discovery and exhibits state-of-the-art results in the edge cases of both open-vocabulary and unsupervised segmentation."}}
{"id": "2507.14492", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.14492", "abs": "https://arxiv.org/abs/2507.14492", "authors": ["Satyankar Chandra", "Ashutosh Gupta", "Kaushik Mallik", "Krishna Shankaranarayanan", "Namrita Varshney"], "title": "Glitches in Decision Tree Ensemble Models", "comment": null, "summary": "Many critical decision-making tasks are now delegated to machine-learned\nmodels, and it is imperative that their decisions are trustworthy and reliable,\nand their outputs are consistent across similar inputs. We identify a new\nsource of unreliable behaviors-called glitches-which may significantly impair\nthe reliability of AI models having steep decision boundaries. Roughly\nspeaking, glitches are small neighborhoods in the input space where the model's\noutput abruptly oscillates with respect to small changes in the input. We\nprovide a formal definition of glitches, and use well-known models and datasets\nfrom the literature to demonstrate that they have widespread existence and\nargue they usually indicate potential model inconsistencies in the neighborhood\nof where they are found. We proceed to the algorithmic search of glitches for\nwidely used gradient-boosted decision tree (GBDT) models. We prove that the\nproblem of detecting glitches is NP-complete for tree ensembles, already for\ntrees of depth 4. Our glitch-search algorithm for GBDT models uses an MILP\nencoding of the problem, and its effectiveness and computational feasibility\nare demonstrated on a set of widely used GBDT benchmarks taken from the\nliterature.", "AI": {"tldr": "Identified and analyzed 'glitches' (small input regions causing model output oscillations) in AI models, showing they're common, indicate inconsistencies, and are hard to detect efficiently in tree ensembles.", "motivation": "To address the unreliability of AI models with steep decision boundaries, especially inconsistencies across similar inputs.", "method": "Algorithmic search of glitches using an MILP encoding.", "result": "Demonstrated the widespread existence of glitches in well-known models and datasets. Proved that detecting glitches is NP-complete for tree ensembles.", "conclusion": "Glitches indicate potential model inconsistencies."}}
{"id": "2507.15106", "categories": ["cs.AI", "cs.RO", "F.2.2"], "pdf": "https://arxiv.org/pdf/2507.15106", "abs": "https://arxiv.org/abs/2507.15106", "authors": ["Xia Xu", "Jochen Triesch"], "title": "From Kicking to Causality: Simulating Infant Agency Detection with a Robust Intrinsic Reward", "comment": "13 pages, 5 figures", "summary": "While human infants robustly discover their own causal efficacy, standard\nreinforcement learning agents remain brittle, as their reliance on\ncorrelation-based rewards fails in noisy, ecologically valid scenarios. To\naddress this, we introduce the Causal Action Influence Score (CAIS), a novel\nintrinsic reward rooted in causal inference. CAIS quantifies an action's\ninfluence by measuring the 1-Wasserstein distance between the learned\ndistribution of sensory outcomes conditional on that action, $p(h|a)$, and the\nbaseline outcome distribution, $p(h)$. This divergence provides a robust reward\nthat isolates the agent's causal impact from confounding environmental noise.\nWe test our approach in a simulated infant-mobile environment where\ncorrelation-based perceptual rewards fail completely when the mobile is\nsubjected to external forces. In stark contrast, CAIS enables the agent to\nfilter this noise, identify its influence, and learn the correct policy.\nFurthermore, the high-quality predictive model learned for CAIS allows our\nagent, when augmented with a surprise signal, to successfully reproduce the\n\"extinction burst\" phenomenon. We conclude that explicitly inferring causality\nis a crucial mechanism for developing a robust sense of agency, offering a\npsychologically plausible framework for more adaptive autonomous systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u63a8\u7406\u7684\u5185\u5728\u5956\u52b1\u673a\u5236CAIS\uff0c\u53ef\u4ee5\u63d0\u9ad8\u667a\u80fd\u4f53\u5728\u5608\u6742\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u4f7f\u5176\u80fd\u591f\u5b66\u4e60\u6b63\u786e\u7684\u7b56\u7565\u3002", "motivation": "\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u4ecd\u7136\u5f88\u8106\u5f31\uff0c\u56e0\u4e3a\u5b83\u4eec\u5bf9\u57fa\u4e8e\u76f8\u5173\u7684\u5956\u52b1\u7684\u4f9d\u8d56\u5728\u5608\u6742\u7684\u3001\u751f\u6001\u4e0a\u6709\u6548\u7684\u573a\u666f\u4e2d\u4f1a\u5931\u8d25\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898", "method": "\u4ecb\u7ecd\u4e86\u56e0\u679c\u884c\u52a8\u5f71\u54cd\u8bc4\u5206 (CAIS)\uff0c\u8fd9\u662f\u4e00\u79cd\u690d\u6839\u4e8e\u56e0\u679c\u63a8\u7406\u7684\u65b0\u578b\u5185\u5728\u5956\u52b1\u3002CAIS \u901a\u8fc7\u6d4b\u91cf\u4ee5\u8be5\u52a8\u4f5c\u4e3a\u6761\u4ef6\u7684\u611f\u5b98\u7ed3\u679c\u7684\u5b66\u4e60\u5206\u5e03\u4e4b\u95f4\u7684 1-Wasserstein \u8ddd\u79bb\u6765\u91cf\u5316\u884c\u52a8\u7684\u5f71\u54cd\uff0c\u5373 $p(h|a)$\uff0c\u4ee5\u53ca\u57fa\u7ebf\u7ed3\u679c\u5206\u5e03\uff0c\u5373 $p(h)$\u3002", "result": "\u5728\u6a21\u62df\u7684\u5a74\u513f\u79fb\u52a8\u73af\u5883\u4e2d\u6d4b\u8bd5\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5728\u8be5\u73af\u5883\u4e2d\uff0c\u5f53\u79fb\u52a8\u8bbe\u5907\u53d7\u5230\u5916\u529b\u65f6\uff0c\u57fa\u4e8e\u76f8\u5173\u7684\u611f\u77e5\u5956\u52b1\u5b8c\u5168\u5931\u8d25\u3002CAIS \u80fd\u591f\u8fc7\u6ee4\u8fd9\u79cd\u566a\u97f3\uff0c\u8bc6\u522b\u5176\u5f71\u54cd\uff0c\u5e76\u5b66\u4e60\u6b63\u786e\u7684\u7b56\u7565\u3002\u6b64\u5916\uff0c\u4e3a CAIS \u5b66\u4e60\u7684\u9ad8\u8d28\u91cf\u9884\u6d4b\u6a21\u578b\u5141\u8bb8\u6211\u4eec\u7684\u667a\u80fd\u4f53\u5728\u589e\u52a0\u4e86\u4e00\u4e2a\u60ca\u5947\u4fe1\u53f7\u540e\uff0c\u6210\u529f\u5730\u91cd\u73b0\u201c\u6d88\u9000\u7206\u53d1\u201d\u73b0\u8c61\u3002", "conclusion": "\u660e\u786e\u63a8\u65ad\u56e0\u679c\u5173\u7cfb\u662f\u53d1\u5c55\u5f3a\u5927\u7684\u80fd\u52a8\u6027\u7684\u5173\u952e\u673a\u5236\uff0c\u4e3a\u66f4\u5177\u9002\u5e94\u6027\u7684\u81ea\u4e3b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5fc3\u7406\u4e0a\u5408\u7406\u7684\u6846\u67b6\u3002"}}
{"id": "2507.14749", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14749", "abs": "https://arxiv.org/abs/2507.14749", "authors": ["Wai Keen Vong", "Brenden M. Lake"], "title": "On the robustness of modeling grounded word learning through a child's egocentric input", "comment": null, "summary": "What insights can machine learning bring to understanding human language\nacquisition? Large language and multimodal models have achieved remarkable\ncapabilities, but their reliance on massive training datasets creates a\nfundamental mismatch with children, who succeed in acquiring language from\ncomparatively limited input. To help bridge this gap, researchers have\nincreasingly trained neural networks using data similar in quantity and quality\nto children's input. Taking this approach to the limit, Vong et al. (2024)\nshowed that a multimodal neural network trained on 61 hours of visual and\nlinguistic input extracted from just one child's developmental experience could\nacquire word-referent mappings. However, whether this approach's success\nreflects the idiosyncrasies of a single child's experience, or whether it would\nshow consistent and robust learning patterns across multiple children's\nexperiences was not explored. In this article, we applied automated speech\ntranscription methods to the entirety of the SAYCam dataset, consisting of over\n500 hours of video data spread across all three children. Using these automated\ntranscriptions, we generated multi-modal vision-and-language datasets for both\ntraining and evaluation, and explored a range of neural network configurations\nto examine the robustness of simulated word learning. Our findings demonstrate\nthat networks trained on automatically transcribed data from each child can\nacquire and generalize word-referent mappings across multiple network\narchitectures. These results validate the robustness of multimodal neural\nnetworks for grounded word learning, while highlighting the individual\ndifferences that emerge in how models learn when trained on each child's\ndevelopmental experiences.", "AI": {"tldr": "Multimodal neural networks trained on automatically transcribed data from multiple children can acquire and generalize word-referent mappings, validating their robustness for grounded word learning and highlighting individual differences in learning.", "motivation": "Large language and multimodal models rely on massive training datasets, creating a mismatch with children's language acquisition from limited input. It was not explored whether the success of training neural networks using data similar in quantity and quality to children's input reflects the idiosyncrasies of a single child's experience, or whether it would show consistent and robust learning patterns across multiple children's experiences.", "method": "Applied automated speech transcription methods to the entirety of the SAYCam dataset and generated multi-modal vision-and-language datasets for training and evaluation, and explored a range of neural network configurations.", "result": "Networks trained on automatically transcribed data from each child can acquire and generalize word-referent mappings across multiple network architectures.", "conclusion": "Networks trained on automatically transcribed data from each child can acquire and generalize word-referent mappings across multiple network architectures, validating the robustness of multimodal neural networks for grounded word learning, while highlighting the individual differences that emerge in how models learn when trained on each child's developmental experiences."}}
{"id": "2507.14608", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14608", "abs": "https://arxiv.org/abs/2507.14608", "authors": ["Nandani Sharma", "Dinesh Singh"], "title": "Exp-Graph: How Connections Learn Facial Attributes in Graph-based Expression Recognition", "comment": null, "summary": "Facial expression recognition is crucial for human-computer interaction\napplications such as face animation, video surveillance, affective computing,\nmedical analysis, etc. Since the structure of facial attributes varies with\nfacial expressions, incorporating structural information into facial attributes\nis essential for facial expression recognition. In this paper, we propose\nExp-Graph, a novel framework designed to represent the structural relationships\namong facial attributes using graph-based modeling for facial expression\nrecognition. For facial attributes graph representation, facial landmarks are\nused as the graph's vertices. At the same time, the edges are determined based\non the proximity of the facial landmark and the similarity of the local\nappearance of the facial attributes encoded using the vision transformer.\nAdditionally, graph convolutional networks are utilized to capture and\nintegrate these structural dependencies into the encoding of facial attributes,\nthereby enhancing the accuracy of expression recognition. Thus, Exp-Graph\nlearns from the facial attribute graphs highly expressive semantic\nrepresentations. On the other hand, the vision transformer and graph\nconvolutional blocks help the framework exploit the local and global\ndependencies among the facial attributes that are essential for the recognition\nof facial expressions. We conducted comprehensive evaluations of the proposed\nExp-Graph model on three benchmark datasets: Oulu-CASIA, eNTERFACE05, and AFEW.\nThe model achieved recognition accuracies of 98.09\\%, 79.01\\%, and 56.39\\%,\nrespectively. These results indicate that Exp-Graph maintains strong\ngeneralization capabilities across both controlled laboratory settings and\nreal-world, unconstrained environments, underscoring its effectiveness for\npractical facial expression recognition applications.", "AI": {"tldr": "Exp-Graph, a graph-based framework for facial expression recognition, uses facial landmarks and a vision transformer to model structural relationships, achieving high accuracy on benchmark datasets.", "motivation": "Facial expression recognition is crucial for human-computer interaction applications. Incorporating structural information into facial attributes is essential since the structure of facial attributes varies with facial expressions.", "method": "The paper proposes Exp-Graph, a novel framework that uses graph-based modeling to represent structural relationships among facial attributes for facial expression recognition. It uses facial landmarks as graph vertices and determines edges based on landmark proximity and local appearance similarity encoded using a vision transformer. Graph convolutional networks capture and integrate structural dependencies.", "result": "The Exp-Graph model achieved recognition accuracies of 98.09% on Oulu-CASIA, 79.01% on eNTERFACE05, and 56.39% on AFEW.", "conclusion": "The Exp-Graph model achieves high recognition accuracies on three benchmark datasets, demonstrating strong generalization capabilities in both controlled and real-world environments, which proves its effectiveness for practical facial expression recognition applications."}}
{"id": "2507.14503", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14503", "abs": "https://arxiv.org/abs/2507.14503", "authors": ["Jiequan Cui", "Beier Zhu", "Qingshan Xu", "Xiaogang Xu", "Pengguang Chen", "Xiaojuan Qi", "Bei Yu", "Hanwang Zhang", "Richang Hong"], "title": "Generative Distribution Distillation", "comment": "Technique report", "summary": "In this paper, we formulate the knowledge distillation (KD) as a conditional\ngenerative problem and propose the \\textit{Generative Distribution Distillation\n(GenDD)} framework. A naive \\textit{GenDD} baseline encounters two major\nchallenges: the curse of high-dimensional optimization and the lack of semantic\nsupervision from labels. To address these issues, we introduce a \\textit{Split\nTokenization} strategy, achieving stable and effective unsupervised KD.\nAdditionally, we develop the \\textit{Distribution Contraction} technique to\nintegrate label supervision into the reconstruction objective. Our theoretical\nproof demonstrates that \\textit{GenDD} with \\textit{Distribution Contraction}\nserves as a gradient-level surrogate for multi-task learning, realizing\nefficient supervised training without explicit classification loss on\nmulti-step sampling image representations. To evaluate the effectiveness of our\nmethod, we conduct experiments on balanced, imbalanced, and unlabeled data.\nExperimental results show that \\textit{GenDD} performs competitively in the\nunsupervised setting, significantly surpassing KL baseline by \\textbf{16.29\\%}\non ImageNet validation set. With label supervision, our ResNet-50 achieves\n\\textbf{82.28\\%} top-1 accuracy on ImageNet in 600 epochs training,\nestablishing a new state-of-the-art.", "AI": {"tldr": "This paper introduces Generative Distribution Distillation (GenDD) for knowledge distillation, using Split Tokenization and Distribution Contraction to improve performance in unsupervised and supervised settings, achieving state-of-the-art results on ImageNet.", "motivation": "To address the curse of high-dimensional optimization and the lack of semantic supervision from labels in knowledge distillation (KD).", "method": "Generative Distribution Distillation (GenDD) framework with Split Tokenization and Distribution Contraction", "result": "GenDD performs competitively in the unsupervised setting and achieves state-of-the-art results with label supervision.", "conclusion": "GenDD performs competitively in the unsupervised setting, significantly surpassing KL baseline by 16.29% on ImageNet validation set. With label supervision, ResNet-50 achieves 82.28% top-1 accuracy on ImageNet in 600 epochs training, establishing a new state-of-the-art."}}
{"id": "2507.15120", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.15120", "abs": "https://arxiv.org/abs/2507.15120", "authors": ["Stefan Borgwardt", "Duy Nhu", "Gabriele R\u00f6ger"], "title": "Automated planning with ontologies under coherence update semantics", "comment": null, "summary": "Standard automated planning employs first-order formulas under closed-world\nsemantics to achieve a goal with a given set of actions from an initial state.\nWe follow a line of research that aims to incorporate background knowledge into\nautomated planning problems, for example, by means of ontologies, which are\nusually interpreted under open-world semantics. We present a new approach for\nplanning with DL-Lite ontologies that combines the advantages of ontology-based\naction conditions provided by explicit-input knowledge and action bases (eKABs)\nand ontology-aware action effects under the coherence update semantics. We show\nthat the complexity of the resulting formalism is not higher than that of\nprevious approaches and provide an implementation via a polynomial compilation\ninto classical planning. An evaluation of existing and new benchmarks examines\nthe performance of a planning system on different variants of our compilation.", "AI": {"tldr": "This paper presents a new approach for planning with DL-Lite ontologies that combines the advantages of ontology-based action conditions and ontology-aware action effects. The complexity of the resulting formalism is not higher than that of previous approaches, and an implementation via a polynomial compilation into classical planning is provided.", "motivation": "To incorporate background knowledge into automated planning problems, for example, by means of ontologies, which are usually interpreted under open-world semantics.", "method": "A new approach for planning with DL-Lite ontologies that combines the advantages of ontology-based action conditions provided by explicit-input knowledge and action bases (eKABs) and ontology-aware action effects under the coherence update semantics.", "result": "The complexity of the resulting formalism is not higher than that of previous approaches.", "conclusion": "The complexity of the resulting formalism is not higher than that of previous approaches, and an implementation via a polynomial compilation into classical planning is provided. An evaluation of existing and new benchmarks examines the performance of a planning system on different variants of our compilation."}}
{"id": "2507.14613", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14613", "abs": "https://arxiv.org/abs/2507.14613", "authors": ["Guoping Xu", "Christopher Kabat", "You Zhang"], "title": "Depthwise-Dilated Convolutional Adapters for Medical Object Tracking and Segmentation Using the Segment Anything Model 2", "comment": "24 pages, 6 figures", "summary": "Recent advances in medical image segmentation have been driven by deep\nlearning; however, most existing methods remain limited by modality-specific\ndesigns and exhibit poor adaptability to dynamic medical imaging scenarios. The\nSegment Anything Model 2 (SAM2) and its related variants, which introduce a\nstreaming memory mechanism for real-time video segmentation, present new\nopportunities for prompt-based, generalizable solutions. Nevertheless, adapting\nthese models to medical video scenarios typically requires large-scale datasets\nfor retraining or transfer learning, leading to high computational costs and\nthe risk of catastrophic forgetting. To address these challenges, we propose\nDD-SAM2, an efficient adaptation framework for SAM2 that incorporates a\nDepthwise-Dilated Adapter (DD-Adapter) to enhance multi-scale feature\nextraction with minimal parameter overhead. This design enables effective\nfine-tuning of SAM2 on medical videos with limited training data. Unlike\nexisting adapter-based methods focused solely on static images, DD-SAM2 fully\nexploits SAM2's streaming memory for medical video object tracking and\nsegmentation. Comprehensive evaluations on TrackRad2025 (tumor segmentation)\nand EchoNet-Dynamic (left ventricle tracking) datasets demonstrate superior\nperformance, achieving Dice scores of 0.93 and 0.97, respectively. To the best\nof our knowledge, this work provides an initial attempt at systematically\nexploring adapter-based SAM2 fine-tuning for medical video segmentation and\ntracking. Code, datasets, and models will be publicly available at\nhttps://github.com/apple1986/DD-SAM2.", "AI": {"tldr": "DD-SAM2\u662f\u4e00\u79cd\u7528\u4e8e\u533b\u5b66\u89c6\u9891\u5206\u5272\u548c\u8ddf\u8e2a\u7684SAM2\u9ad8\u6548\u9002\u914d\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u6df1\u5ea6\u53ef\u5206\u79bb\u6269\u5f20\u9002\u914d\u5668\u589e\u5f3a\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u5728\u6709\u9650\u6570\u636e\u4e0b\u5b9e\u73b0\u6709\u6548\u5fae\u8c03\uff0c\u5728TrackRad2025\u548cEchoNet-Dynamic\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u53d7\u9650\u4e8e\u6a21\u6001\u7279\u5b9a\u8bbe\u8ba1\uff0c\u4e14\u5bf9\u52a8\u6001\u533b\u5b66\u6210\u50cf\u573a\u666f\u7684\u9002\u5e94\u6027\u5dee\u3002\u5c06SAM2\u53ca\u5176\u53d8\u4f53\u5e94\u7528\u4e8e\u533b\u5b66\u89c6\u9891\u901a\u5e38\u9700\u8981\u5927\u89c4\u6a21\u6570\u636e\u96c6\u8fdb\u884c\u518d\u8bad\u7ec3\u6216\u8fc1\u79fb\u5b66\u4e60\uff0c\u5bfc\u81f4\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u707e\u96be\u6027\u9057\u5fd8\u7684\u98ce\u9669\u3002", "method": "\u63d0\u51faDD-SAM2\uff0c\u4e00\u79cdSAM2\u7684\u6709\u6548\u9002\u914d\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u542b\u4e00\u4e2a\u6df1\u5ea6\u53ef\u5206\u79bb\u6269\u5f20\u9002\u914d\u5668\uff08DD-Adapter\uff09\uff0c\u4ee5\u6700\u5c0f\u7684\u53c2\u6570\u5f00\u9500\u589e\u5f3a\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u3002", "result": "DD-SAM2\u5728\u533b\u5b66\u89c6\u9891\u5bf9\u8c61\u8ddf\u8e2a\u548c\u5206\u5272\u65b9\u9762\u5145\u5206\u5229\u7528\u4e86SAM2\u7684\u6d41\u5f0f\u5185\u5b58\uff0c\u5e76\u5728\u6709\u9650\u7684\u8bad\u7ec3\u6570\u636e\u4e0b\u6709\u6548\u5fae\u8c03SAM2\u3002\u5728TrackRad2025\u548cEchoNet-Dynamic\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0cDD-SAM2\u8868\u73b0\u4f18\u5f02\uff0cDice\u7cfb\u6570\u5206\u522b\u8fbe\u52300.93\u548c0.97\u3002", "conclusion": "DD-SAM2\u5728TrackRad2025\u548cEchoNet-Dynamic\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0cDice\u7cfb\u6570\u5206\u522b\u8fbe\u52300.93\u548c0.97\u3002\u8be5\u5de5\u4f5c\u9996\u6b21\u5c1d\u8bd5\u4e86\u57fa\u4e8e\u9002\u914d\u5668\u7684SAM2\u5fae\u8c03\uff0c\u7528\u4e8e\u533b\u5b66\u89c6\u9891\u5206\u5272\u548c\u8ddf\u8e2a\u3002"}}
{"id": "2507.14516", "categories": ["cs.LG", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.14516", "abs": "https://arxiv.org/abs/2507.14516", "authors": ["Jeyoung Lee", "Hochul Kang"], "title": "SDSC:A Structure-Aware Metric for Semantic Signal Representation Learning", "comment": null, "summary": "We propose the Signal Dice Similarity Coefficient (SDSC), a structure-aware\nmetric function for time series self-supervised representation learning. Most\nSelf-Supervised Learning (SSL) methods for signals commonly adopt\ndistance-based objectives such as mean squared error (MSE), which are sensitive\nto amplitude, invariant to waveform polarity, and unbounded in scale. These\nproperties hinder semantic alignment and reduce interpretability. SDSC\naddresses this by quantifying structural agreement between temporal signals\nbased on the intersection of signed amplitudes, derived from the Dice\nSimilarity Coefficient (DSC).Although SDSC is defined as a structure-aware\nmetric, it can be used as a loss by subtracting from 1 and applying a\ndifferentiable approximation of the Heaviside function for gradient-based\noptimization. A hybrid loss formulation is also proposed to combine SDSC with\nMSE, improving stability and preserving amplitude where necessary. Experiments\non forecasting and classification benchmarks demonstrate that SDSC-based\npre-training achieves comparable or improved performance over MSE, particularly\nin in-domain and low-resource scenarios. The results suggest that structural\nfidelity in signal representations enhances the semantic representation\nquality, supporting the consideration of structure-aware metrics as viable\nalternatives to conventional distance-based methods.", "AI": {"tldr": "SDSC: a structure-aware metric function for time series self-supervised representation learning, achieves comparable or improved performance over MSE.", "motivation": "Most Self-Supervised Learning (SSL) methods for signals commonly adopt distance-based objectives such as mean squared error (MSE), which are sensitive to amplitude, invariant to waveform polarity, and unbounded in scale. These properties hinder semantic alignment and reduce interpretability.", "method": "We propose the Signal Dice Similarity Coefficient (SDSC), a structure-aware metric function for time series self-supervised representation learning. A hybrid loss formulation is also proposed to combine SDSC with MSE", "result": "Experiments on forecasting and classification benchmarks demonstrate that SDSC-based pre-training achieves comparable or improved performance over MSE, particularly in in-domain and low-resource scenarios.", "conclusion": "SDSC-based pre-training achieves comparable or improved performance over MSE, particularly in in-domain and low-resource scenarios. Structural fidelity in signal representations enhances the semantic representation quality."}}
{"id": "2507.15140", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15140", "abs": "https://arxiv.org/abs/2507.15140", "authors": ["Mohammad Mashayekhi", "Sara Ahmadi Majd", "Arian AmirAmjadi", "Parsa Hosseini"], "title": "Clinical Semantic Intelligence (CSI): Emulating the Cognitive Framework of the Expert Clinician for Comprehensive Oral Disease Diagnosis", "comment": null, "summary": "The diagnosis of oral diseases presents a problematic clinical challenge,\ncharacterized by a wide spectrum of pathologies with overlapping\nsymptomatology. To address this, we developed Clinical Semantic Intelligence\n(CSI), a novel artificial intelligence framework that diagnoses 118 different\noral diseases by computationally modeling the cognitive processes of an expert\nclinician. Our core hypothesis is that moving beyond simple pattern matching to\nemulate expert reasoning is critical to building clinically useful diagnostic\naids.\n  CSI's architecture integrates a fine-tuned multimodal CLIP model with a\nspecialized ChatGLM-6B language model. This system executes a Hierarchical\nDiagnostic Reasoning Tree (HDRT), a structured framework that distills the\nsystematic, multi-step logic of differential diagnosis. The framework operates\nin two modes: a Fast Mode for rapid screening and a Standard Mode that\nleverages the full HDRT for an interactive and in-depth diagnostic workup.\n  To train and validate our system, we curated a primary dataset of 4,310\nimages, supplemented by an external hold-out set of 176 images for final\nvalidation. A clinically-informed augmentation strategy expanded our training\ndata to over 30,000 image-text pairs. On a 431-image internal test set, CSI's\nFast Mode achieved an accuracy of 73.4%, which increased to 89.5% with the\nHDRT-driven Standard Mode. The performance gain is directly attributable to the\nhierarchical reasoning process. Herein, we detail the architectural philosophy,\ndevelopment, and rigorous evaluation of the CSI framework.", "AI": {"tldr": "Developed CSI, an AI framework for diagnosing 118 oral diseases, achieving up to 89.5% accuracy by emulating expert reasoning through a Hierarchical Diagnostic Reasoning Tree.", "motivation": "Diagnosing oral diseases is challenging due to overlapping symptomatology, necessitating diagnostic aids that emulate expert clinical reasoning.", "method": "The study uses a fine-tuned multimodal CLIP model integrated with a ChatGLM-6B language model, executing a Hierarchical Diagnostic Reasoning Tree (HDRT).", "result": "CSI achieved 73.4% accuracy in Fast Mode and 89.5% in HDRT-driven Standard Mode on an internal test set.", "conclusion": "The Clinical Semantic Intelligence (CSI) framework shows promising results in diagnosing oral diseases, with the Hierarchical Diagnostic Reasoning Tree (HDRT) significantly improving diagnostic accuracy."}}
{"id": "2507.14815", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14815", "abs": "https://arxiv.org/abs/2507.14815", "authors": ["Shoutao Guo", "Shaolei Zhang", "Qingkai Fang", "Zhengrui Ma", "Min Zhang", "Yang Feng"], "title": "FastLongSpeech: Enhancing Large Speech-Language Models for Efficient Long-Speech Processing", "comment": "The code is at https://github.com/ictnlp/FastLongSpeech. This model\n  is at https://huggingface.co/ICTNLP/FastLongSpeech. The dataset is at\n  https://huggingface.co/datasets/ICTNLP/LongSpeech-Eval", "summary": "The rapid advancement of Large Language Models (LLMs) has spurred significant\nprogress in Large Speech-Language Models (LSLMs), enhancing their capabilities\nin both speech understanding and generation. While existing LSLMs often\nconcentrate on augmenting speech generation or tackling a diverse array of\nshort-speech tasks, the efficient processing of long-form speech remains a\ncritical yet underexplored challenge. This gap is primarily attributed to the\nscarcity of long-speech training datasets and the high computational costs\nassociated with long sequences. To address these limitations, we introduce\nFastLongSpeech, a novel framework designed to extend LSLM capabilities for\nefficient long-speech processing without necessitating dedicated long-speech\ntraining data. FastLongSpeech incorporates an iterative fusion strategy that\ncan compress excessively long-speech sequences into manageable lengths. To\nadapt LSLMs for long-speech inputs, it introduces a dynamic compression\ntraining approach, which exposes the model to short-speech sequences at varying\ncompression ratios, thereby transferring the capabilities of LSLMs to\nlong-speech tasks. To assess the long-speech capabilities of LSLMs, we develop\na long-speech understanding benchmark called LongSpeech-Eval. Experiments show\nthat our method exhibits strong performance in both long-speech and\nshort-speech tasks, while greatly improving inference efficiency.", "AI": {"tldr": "FastLongSpeech is introduced to enable LSLMs to efficiently process long-form speech without requiring long-speech training data. It uses iterative fusion and dynamic compression training. The method shows strong performance and efficiency, evaluated using LongSpeech-Eval.", "motivation": "Efficient processing of long-form speech by Large Speech-Language Models (LSLMs) is a critical challenge due to the scarcity of long-speech training datasets and high computational costs.", "method": "The paper introduces FastLongSpeech, a framework with an iterative fusion strategy for compressing long-speech sequences and a dynamic compression training approach to adapt LSLMs for long-speech inputs.", "result": "The FastLongSpeech framework exhibits strong performance in both long-speech and short-speech tasks and greatly improves inference efficiency. A new long-speech understanding benchmark called LongSpeech-Eval was developed to assess the long-speech capabilities of LSLMs.", "conclusion": "The proposed FastLongSpeech framework demonstrates strong performance in both long-speech and short-speech tasks and improves inference efficiency."}}
{"id": "2507.14632", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14632", "abs": "https://arxiv.org/abs/2507.14632", "authors": ["Haiquan Wen", "Tianxiao Li", "Zhenglin Huang", "Yiwei He", "Guangliang Cheng"], "title": "BusterX++: Towards Unified Cross-Modal AI-Generated Content Detection and Explanation with MLLM", "comment": null, "summary": "Recent advances in generative AI have dramatically improved image and video\nsynthesis capabilities, significantly increasing the risk of misinformation\nthrough sophisticated fake content. In response, detection methods have evolved\nfrom traditional approaches to multimodal large language models (MLLMs),\noffering enhanced transparency and interpretability in identifying synthetic\nmedia. However, current detection systems remain fundamentally limited by their\nsingle-modality design. These approaches analyze images or videos separately,\nmaking them ineffective against synthetic content that combines multiple media\nformats. To address these challenges, we introduce \\textbf{BusterX++}, a novel\nframework designed specifically for cross-modal detection and explanation of\nsynthetic media. Our approach incorporates an advanced reinforcement learning\n(RL) post-training strategy that eliminates cold-start. Through Multi-stage\nTraining, Thinking Reward, and Hybrid Reasoning, BusterX++ achieves stable and\nsubstantial performance improvements. To enable comprehensive evaluation, we\nalso present \\textbf{GenBuster++}, a cross-modal benchmark leveraging\nstate-of-the-art image and video generation techniques. This benchmark\ncomprises 4,000 images and video clips, meticulously curated by human experts\nusing a novel filtering methodology to ensure high quality, diversity, and\nreal-world applicability. Extensive experiments demonstrate the effectiveness\nand generalizability of our approach.", "AI": {"tldr": "Introduces BusterX++, a novel framework for cross-modal detection and explanation of synthetic media, along with GenBuster++, a new cross-modal benchmark dataset.", "motivation": "Current detection systems are fundamentally limited by their single-modality design, making them ineffective against synthetic content that combines multiple media formats.", "method": "The BusterX++ framework incorporates an advanced reinforcement learning (RL) post-training strategy that eliminates cold-start through Multi-stage Training, Thinking Reward, and Hybrid Reasoning.", "result": "BusterX++ achieves stable and substantial performance improvements. The GenBuster++ benchmark comprises 4,000 images and video clips, meticulously curated by human experts using a novel filtering methodology to ensure high quality, diversity, and real-world applicability.", "conclusion": "Extensive experiments demonstrate the effectiveness and generalizability of the BusterX++ approach."}}
{"id": "2507.14528", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14528", "abs": "https://arxiv.org/abs/2507.14528", "authors": ["Ilias Tsoumas", "Dimitrios Bormpoudakis", "Vasileios Sitokonstantinou", "Athanasios Askitopoulos", "Andreas Kalogeras", "Charalampos Kontoes", "Ioannis Athanasiadis"], "title": "Positive-Unlabeled Learning for Control Group Construction in Observational Causal Inference", "comment": "Accepted at KDD 2025 Workshop on Causal Inference and Machine\n  Learning in Practice", "summary": "In causal inference, whether through randomized controlled trials or\nobservational studies, access to both treated and control units is essential\nfor estimating the effect of a treatment on an outcome of interest. When\ntreatment assignment is random, the average treatment effect (ATE) can be\nestimated directly by comparing outcomes between groups. In non-randomized\nsettings, various techniques are employed to adjust for confounding and\napproximate the counterfactual scenario to recover an unbiased ATE. A common\nchallenge, especially in observational studies, is the absence of units clearly\nlabeled as controls-that is, units known not to have received the treatment. To\naddress this, we propose positive-unlabeled (PU) learning as a framework for\nidentifying, with high confidence, control units from a pool of unlabeled ones,\nusing only the available treated (positive) units. We evaluate this approach\nusing both simulated and real-world data. We construct a causal graph with\ndiverse relationships and use it to generate synthetic data under various\nscenarios, assessing how reliably the method recovers control groups that allow\nestimates of true ATE. We also apply our approach to real-world data on optimal\nsowing and fertilizer treatments in sustainable agriculture. Our findings show\nthat PU learning can successfully identify control (negative) units from\nunlabeled data based only on treated units and, through the resulting control\ngroup, estimate an ATE that closely approximates the true value. This work has\nimportant implications for observational causal inference, especially in fields\nwhere randomized experiments are difficult or costly. In domains such as earth,\nenvironmental, and agricultural sciences, it enables a plethora of\nquasi-experiments by leveraging available earth observation and climate data,\nparticularly when treated units are available but control units are lacking.", "AI": {"tldr": "This paper introduces a PU learning framework to identify control units when they are not readily available, using only treated units. The method is evaluated on both simulated and real-world data, showing it can effectively estimate the average treatment effect (ATE).", "motivation": "A common challenge, especially in observational studies, is the absence of units clearly labeled as controls. To address this, we propose positive-unlabeled (PU) learning as a framework for identifying, with high confidence, control units from a pool of unlabeled ones, using only the available treated (positive) units.", "method": "propose positive-unlabeled (PU) learning as a framework for identifying control units from a pool of unlabeled ones, using only the available treated (positive) units", "result": "PU learning can successfully identify control (negative) units from unlabeled data based only on treated units and, through the resulting control group, estimate an ATE that closely approximates the true value.", "conclusion": "PU learning can successfully identify control (negative) units from unlabeled data based only on treated units and estimate an ATE that closely approximates the true value. This work has important implications for observational causal inference, especially in fields where randomized experiments are difficult or costly. It enables a plethora of quasi-experiments by leveraging available earth observation and climate data, particularly when treated units are available but control units are lacking."}}
{"id": "2507.15143", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.15143", "abs": "https://arxiv.org/abs/2507.15143", "authors": ["Abderaouf Bahi", "Amel Ourici"], "title": "Can We Move Freely in NEOM's The Line? An Agent-Based Simulation of Human Mobility in a Futuristic Smart City", "comment": null, "summary": "This paper investigates the feasibility of human mobility in The Line, a\nproposed 170-kilometer linear smart city in NEOM, Saudi Arabia. To assess\nwhether citizens can move freely within this unprecedented urban topology, we\ndevelop a hybrid simulation framework that integrates agent-based modeling,\nreinforcement learning, supervised learning, and graph neural networks. The\nsimulation captures multi-modal transportation behaviors across 50 vertical\nlevels and varying density scenarios using both synthetic data and real-world\ntraces from high-density cities. Our experiments reveal that with the full\nAI-integrated architecture, agents achieved an average commute time of 7.8 to\n8.4 minutes, a satisfaction rate exceeding 89 percent, and a reachability index\nof over 91 percent, even during peak congestion periods. Ablation studies\nconfirmed that the removal of intelligent modules such as reinforcement\nlearning or graph neural networks significantly degrades performance, with\ncommute times increasing by up to 85 percent and reachability falling below 70\npercent. Environmental modeling further demonstrated low energy consumption and\nminimal CO2 emissions when electric modes are prioritized. The findings suggest\nthat freedom of movement is not only conceptually achievable in The Line, but\nalso operationally realistic if supported by adaptive AI systems, sustainable\ninfrastructure, and real-time feedback loops.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86NEOM\u7684\u7ebf\u6027\u57ce\u5e02The Line\u4e2d\u7684\u4ea4\u901a\u53ef\u884c\u6027\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7AI\u548c\u53ef\u6301\u7eed\u57fa\u7840\u8bbe\u65bd\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u57ce\u5e02\u4ea4\u901a\u3002", "motivation": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u6c99\u7279\u963f\u62c9\u4f2fNEOM\u62df\u5efa\u7684170\u516c\u91cc\u7ebf\u6027\u667a\u80fd\u57ce\u5e02The Line\u4e2d\uff0c\u4eba\u7c7b\u79fb\u52a8\u7684\u53ef\u884c\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6df7\u5408\u6a21\u62df\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u57fa\u4e8eAgent\u7684\u5efa\u6a21\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u76d1\u7763\u5b66\u4e60\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u5b8c\u5168\u96c6\u6210AI\u7684\u67b6\u6784\u4e0b\uff0c\u5373\u4f7f\u5728\u9ad8\u5cf0\u62e5\u5835\u65f6\u671f\uff0c\u667a\u80fd\u4f53\u7684\u5e73\u5747\u901a\u52e4\u65f6\u95f4\u4e3a7.8\u81f38.4\u5206\u949f\uff0c\u6ee1\u610f\u7387\u8d85\u8fc789%\uff0c\u53ef\u8fbe\u6027\u6307\u6570\u8d85\u8fc791%\u3002\u79fb\u9664\u5f3a\u5316\u5b66\u4e60\u6216\u56fe\u795e\u7ecf\u7f51\u7edc\u7b49\u667a\u80fd\u6a21\u5757\u4f1a\u663e\u8457\u964d\u4f4e\u6027\u80fd\uff0c\u901a\u52e4\u65f6\u95f4\u589e\u52a0\u9ad8\u8fbe85%\uff0c\u53ef\u8fbe\u6027\u964d\u81f370%\u4ee5\u4e0b\u3002\u73af\u5883\u6a21\u578b\u8fdb\u4e00\u6b65\u8bc1\u660e\uff0c\u5f53\u4f18\u5148\u8003\u8651\u7535\u52a8\u6a21\u5f0f\u65f6\uff0c\u80fd\u6e90\u6d88\u8017\u4f4e\uff0cCO2\u6392\u653e\u91cf\u6700\u4f4e\u3002", "conclusion": "\u5728The Line\u4e2d\uff0c\u5982\u679c\u91c7\u7528\u81ea\u9002\u5e94AI\u7cfb\u7edf\u3001\u53ef\u6301\u7eed\u57fa\u7840\u8bbe\u65bd\u548c\u5b9e\u65f6\u53cd\u9988\u56de\u8def\uff0c\u81ea\u7531\u79fb\u52a8\u4e0d\u4ec5\u5728\u6982\u5ff5\u4e0a\u662f\u53ef\u884c\u7684\uff0c\u800c\u4e14\u5728\u64cd\u4f5c\u4e0a\u4e5f\u662f\u73b0\u5b9e\u7684\u3002"}}
{"id": "2507.14819", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14819", "abs": "https://arxiv.org/abs/2507.14819", "authors": ["Akriti Jain", "Pritika Ramu", "Aparna Garimella", "Apoorv Saxena"], "title": "Doc2Chart: Intent-Driven Zero-Shot Chart Generation from Documents", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\ntransforming text descriptions or tables to data visualizations via\ninstruction-tuning methods. However, it is not straightforward to apply these\nmethods directly for a more real-world use case of visualizing data from long\ndocuments based on user-given intents, as opposed to the user pre-selecting the\nrelevant content manually. We introduce the task of intent-based chart\ngeneration from documents: given a user-specified intent and document(s), the\ngoal is to generate a chart adhering to the intent and grounded on the\ndocument(s) in a zero-shot setting. We propose an unsupervised, two-staged\nframework in which an LLM first extracts relevant information from the\ndocument(s) by decomposing the intent and iteratively validates and refines\nthis data. Next, a heuristic-guided module selects an appropriate chart type\nbefore final code generation. To assess the data accuracy of the generated\ncharts, we propose an attribution-based metric that uses a structured textual\nrepresentation of charts, instead of relying on visual decoding metrics that\noften fail to capture the chart data effectively. To validate our approach, we\ncurate a dataset comprising of 1,242 $<$intent, document, charts$>$ tuples from\ntwo domains, finance and scientific, in contrast to the existing datasets that\nare largely limited to parallel text descriptions/ tables and their\ncorresponding charts. We compare our approach with baselines using single-shot\nchart generation using LLMs and query-based retrieval methods; our method\noutperforms by upto $9$ points and $17$ points in terms of chart data accuracy\nand chart type respectively over the best baselines.", "AI": {"tldr": "This paper introduces intent-based chart generation from long documents, proposes a two-stage unsupervised framework, and demonstrates its superior performance compared to baselines on a newly curated dataset.", "motivation": "Existing instruction-tuning methods for transforming text/tables to data visualizations are not directly applicable to real-world use cases of visualizing data from long documents based on user intents without manual content pre-selection. The paper introduces the task of intent-based chart generation from documents in a zero-shot setting.", "method": "An unsupervised, two-staged framework is proposed, involving LLM-based information extraction with iterative validation and refinement, followed by a heuristic-guided chart type selection module before final code generation.", "result": "The proposed method outperforms baselines by up to 9 points in chart data accuracy and 17 points in chart type accuracy. A new dataset of 1,242 <intent, document, charts> tuples from finance and scientific domains is curated.", "conclusion": "The proposed unsupervised, two-staged framework outperforms baselines in chart data accuracy and chart type generation by up to 9 and 17 points, respectively."}}
{"id": "2507.14643", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14643", "abs": "https://arxiv.org/abs/2507.14643", "authors": ["Jifeng Shen", "Haibo Zhan", "Shaohua Dong", "Xin Zuo", "Wankou Yang", "Haibin Ling"], "title": "Multispectral State-Space Feature Fusion: Bridging Shared and Cross-Parametric Interactions for Object Detection", "comment": "submitted on 30/4/2025, Under Major Revision", "summary": "Modern multispectral feature fusion for object detection faces two critical\nlimitations: (1) Excessive preference for local complementary features over\ncross-modal shared semantics adversely affects generalization performance; and\n(2) The trade-off between the receptive field size and computational complexity\npresent critical bottlenecks for scalable feature modeling. Addressing these\nissues, a novel Multispectral State-Space Feature Fusion framework, dubbed\nMS2Fusion, is proposed based on the state space model (SSM), achieving\nefficient and effective fusion through a dual-path parametric interaction\nmechanism. More specifically, the first cross-parameter interaction branch\ninherits the advantage of cross-attention in mining complementary information\nwith cross-modal hidden state decoding in SSM. The second shared-parameter\nbranch explores cross-modal alignment with joint embedding to obtain\ncross-modal similar semantic features and structures through parameter sharing\nin SSM. Finally, these two paths are jointly optimized with SSM for fusing\nmultispectral features in a unified framework, allowing our MS2Fusion to enjoy\nboth functional complementarity and shared semantic space. In our extensive\nexperiments on mainstream benchmarks including FLIR, M3FD and LLVIP, our\nMS2Fusion significantly outperforms other state-of-the-art multispectral object\ndetection methods, evidencing its superiority. Moreover, MS2Fusion is general\nand applicable to other multispectral perception tasks. We show that, even\nwithout specific design, MS2Fusion achieves state-of-the-art results on RGB-T\nsemantic segmentation and RGBT salient object detection, showing its\ngenerality. The source code will be available at\nhttps://github.com/61s61min/MS2Fusion.git.", "AI": {"tldr": "MS2Fusion\u662f\u4e00\u79cd\u65b0\u7684\u591a\u5149\u8c31\u7279\u5f81\u878d\u5408\u6846\u67b6\uff0c\u5b83\u4f7f\u7528\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u548c\u53cc\u8def\u5f84\u53c2\u6570\u4ea4\u4e92\u673a\u5236\uff0c\u5728\u591a\u5149\u8c31\u76ee\u6807\u68c0\u6d4b\u7b49\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u591a\u5149\u8c31\u7279\u5f81\u878d\u5408\u5728\u76ee\u6807\u68c0\u6d4b\u65b9\u9762\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a(1)\u8fc7\u5ea6\u504f\u597d\u5c40\u90e8\u4e92\u8865\u7279\u5f81\u800c\u975e\u8de8\u6a21\u6001\u5171\u4eab\u8bed\u4e49\uff0c\u4e0d\u5229\u4e8e\u6cdb\u5316\u6027\u80fd\uff1b(2)\u611f\u53d7\u91ce\u5927\u5c0f\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u5bf9\u53ef\u6269\u5c55\u7279\u5f81\u5efa\u6a21\u63d0\u51fa\u4e86\u5173\u952e\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u5149\u8c31\u72b6\u6001\u7a7a\u95f4\u7279\u5f81\u878d\u5408\u6846\u67b6MS2Fusion\uff0c\u8be5\u6846\u67b6\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b(SSM)\uff0c\u901a\u8fc7\u53cc\u8def\u5f84\u53c2\u6570\u4ea4\u4e92\u673a\u5236\u5b9e\u73b0\u9ad8\u6548\u878d\u5408\u3002", "result": "\u5728FLIR\u3001M3FD\u548cLLVIP\u7b49\u4e3b\u6d41\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMS2Fusion\u663e\u8457\u4f18\u4e8e\u5176\u4ed6state-of-the-art\u7684\u591a\u5149\u8c31\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u3002", "conclusion": "MS2Fusion\u5728\u591a\u5149\u8c31\u76ee\u6807\u68c0\u6d4b\u3001RGB-T\u8bed\u4e49\u5206\u5272\u548cRGBT\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u7b49\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86state-of-the-art\u7684\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2507.14529", "categories": ["cs.LG", "math.OC", "91A16, 68T05, 49N45, 93E20, 46E22"], "pdf": "https://arxiv.org/pdf/2507.14529", "abs": "https://arxiv.org/abs/2507.14529", "authors": ["Berkay Anahtarci", "Can Deha Kariksiz", "Naci Saldi"], "title": "Kernel Based Maximum Entropy Inverse Reinforcement Learning for Mean-Field Games", "comment": null, "summary": "We consider the maximum causal entropy inverse reinforcement learning problem\nfor infinite-horizon stationary mean-field games, in which we model the unknown\nreward function within a reproducing kernel Hilbert space. This allows the\ninference of rich and potentially nonlinear reward structures directly from\nexpert demonstrations, in contrast to most existing inverse reinforcement\nlearning approaches for mean-field games that typically restrict the reward\nfunction to a linear combination of a fixed finite set of basis functions. We\nalso focus on the infinite-horizon cost structure, whereas prior studies\nprimarily rely on finite-horizon formulations. We introduce a Lagrangian\nrelaxation to this maximum causal entropy inverse reinforcement learning\nproblem that enables us to reformulate it as an unconstrained log-likelihood\nmaximization problem, and obtain a solution \\lk{via} a gradient ascent\nalgorithm. To illustrate the theoretical consistency of the algorithm, we\nestablish the smoothness of the log-likelihood objective by proving the\nFr\\'echet differentiability of the related soft Bellman operators with respect\nto the parameters in the reproducing kernel Hilbert space. We demonstrate the\neffectiveness of our method on a mean-field traffic routing game, where it\naccurately recovers expert behavior.", "AI": {"tldr": "This paper addresses the problem of inferring complex reward functions in infinite-horizon mean-field games using maximum causal entropy inverse reinforcement learning and a reproducing kernel Hilbert space, and introduces a gradient ascent algorithm to solve it. The method's effectiveness is demonstrated in a traffic routing game.", "motivation": "The paper considers the maximum causal entropy inverse reinforcement learning problem for infinite-horizon stationary mean-field games, in which the reward function is modeled within a reproducing kernel Hilbert space. This allows the inference of rich and potentially nonlinear reward structures directly from expert demonstrations, in contrast to most existing inverse reinforcement learning approaches for mean-field games that typically restrict the reward function to a linear combination of a fixed finite set of basis functions. The paper also focuses on the infinite-horizon cost structure, whereas prior studies primarily rely on finite-horizon formulations.", "method": "A Lagrangian relaxation is introduced to this maximum causal entropy inverse reinforcement learning problem that enables us to reformulate it as an unconstrained log-likelihood maximization problem, and a solution is obtained via a gradient ascent algorithm.", "result": "The smoothness of the log-likelihood objective is established by proving the Fr\u00e9chet differentiability of the related soft Bellman operators with respect to the parameters in the reproducing kernel Hilbert space.", "conclusion": "The effectiveness of the method is demonstrated on a mean-field traffic routing game, where it accurately recovers expert behavior."}}
{"id": "2507.15225", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15225", "abs": "https://arxiv.org/abs/2507.15225", "authors": ["Yichi Zhou", "Jianqiu Zhao", "Yongxin Zhang", "Bohan Wang", "Siran Wang", "Luoxin Chen", "Jiahui Wang", "Haowei Chen", "Allan Jie", "Xinbo Zhang", "Haocheng Wang", "Luong Trung", "Rong Ye", "Phan Nhat Hoang", "Huishuai Zhang", "Peng Sun", "Hang Li"], "title": "Solving Formal Math Problems by Decomposition and Iterative Reflection", "comment": null, "summary": "General-purpose Large Language Models (LLMs) have achieved remarkable success\nin intelligence, performing comparably to human experts on complex reasoning\ntasks such as coding and mathematical reasoning. However, generating formal\nproofs in specialized languages like Lean 4 remains a significant challenge for\nthese models, limiting their application in complex theorem proving and\nautomated verification. Current approaches typically require specializing\nmodels through fine-tuning on dedicated formal corpora, incurring high costs\nfor data collection and training. In this work, we introduce \\textbf{Delta\nProver}, an agent-based framework that orchestrates the interaction between a\ngeneral-purpose LLM and the Lean 4 proof environment. Delta Prover leverages\nthe reflection and reasoning capabilities of general-purpose LLMs to\ninteractively construct formal proofs in Lean 4, circumventing the need for\nmodel specialization. At its core, the agent integrates two novel,\ninterdependent components: an algorithmic framework for reflective\ndecomposition and iterative proof repair, and a custom Domain-Specific Language\n(DSL) built upon Lean 4 for streamlined subproblem management. \\textbf{Delta\nProver achieves a state-of-the-art 95.9\\% success rate on the miniF2F-test\nbenchmark, surpassing all existing approaches, including those requiring model\nspecialization.} Furthermore, Delta Prover exhibits a significantly stronger\ntest-time scaling law compared to standard Best-of-N proof strategies.\nCrucially, our findings demonstrate that general-purpose LLMs, when guided by\nan effective agentic structure, possess substantial untapped theorem-proving\ncapabilities. This presents a computationally efficient alternative to\nspecialized models for robust automated reasoning in formal environments.", "AI": {"tldr": "Delta Prover, an agent-based framework, enables general-purpose LLMs to achieve state-of-the-art results in Lean 4 theorem proving without fine-tuning.", "motivation": "Generating formal proofs in specialized languages like Lean 4 remains a significant challenge for LLMs, limiting their application in complex theorem proving and automated verification. Current approaches typically require specializing models through fine-tuning, incurring high costs.", "method": "an agent-based framework that orchestrates the interaction between a general-purpose LLM and the Lean 4 proof environment, integrating an algorithmic framework for reflective decomposition and iterative proof repair, and a custom Domain-Specific Language (DSL) built upon Lean 4 for streamlined subproblem management.", "result": "achieves a state-of-the-art 95.9% success rate on the miniF2F-test benchmark, surpassing all existing approaches, and exhibits a significantly stronger test-time scaling law compared to standard Best-of-N proof strategies.", "conclusion": "General-purpose LLMs, when guided by an effective agentic structure, possess substantial untapped theorem-proving capabilities, presenting a computationally efficient alternative to specialized models."}}
{"id": "2507.14849", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14849", "abs": "https://arxiv.org/abs/2507.14849", "authors": ["Yifei Wang"], "title": "Beyond Isolated Capabilities: Bridging Long CoT Reasoning and Long-Context Understanding", "comment": null, "summary": "Reasoning distillation has emerged as an effective approach to enhance the\nreasoning capabilities of smaller language models. However, the impact of\nlarge-scale reasoning distillation on other critical abilities, particularly\nin-context retrieval and reasoning, remains unexplored. This gap in\nunderstanding is particularly significant given the increasing importance of\nRetrieval-Augmented Generation (RAG) systems, where efficient acquisition and\nutilization of contextual information are paramount for generating reliable\nresponses. Motivated by the need to understand how the extended long-CoT\nprocess influences long-context comprehension, we conduct a comprehensive\ninvestigation using a series of open-source models distilled from Deepseek-R1,\nrenowned for its exceptional reasoning capabilities. Our study focuses on\nevaluating these models' performance in extracting and integrating relevant\ninformation from extended contexts through multi-document question and\nanswering tasks. Through rigorous experimentation, we demonstrate that\ndistilled reasoning patterns significantly improve long-context understanding.\nOur analysis reveals that distillation fosters greater long-context awareness\nby promoting more detailed and explicit reasoning processes during context\nanalysis and information parsing. This advancement effectively mitigates the\npersistent \"lost in the middle\" issue that has hindered long-context models.", "AI": {"tldr": "Reasoning distillation enhances long-context understanding in smaller language models, improving in-context retrieval and mitigating the 'lost in the middle' problem.", "motivation": "The impact of large-scale reasoning distillation on in-context retrieval and reasoning in Retrieval-Augmented Generation (RAG) systems remains unexplored.", "method": "Comprehensive investigation using open-source models distilled from Deepseek-R1, focusing on multi-document question answering tasks.", "result": "Distillation fosters greater long-context awareness by promoting more detailed and explicit reasoning processes during context analysis and information parsing.", "conclusion": "Distilled reasoning patterns significantly improve long-context understanding by promoting detailed reasoning, mitigating the 'lost in the middle' issue."}}
{"id": "2507.14657", "categories": ["cs.CV", "cs.AI", "68T45", "I.2.10"], "pdf": "https://arxiv.org/pdf/2507.14657", "abs": "https://arxiv.org/abs/2507.14657", "authors": ["Keivan Shariatmadar", "Ahmad Osman"], "title": "AI-Powered Precision in Sport Taekwondo: Enhancing Fairness, Speed, and Trust in Competition (FST.ai)", "comment": "24 pages, 9 figures", "summary": "The integration of Artificial Intelligence (AI) into sports officiating\nrepresents a paradigm shift in how decisions are made in competitive\nenvironments. Traditional manual systems, even when supported by Instant Video\nReplay (IVR), often suffer from latency, subjectivity, and inconsistent\nenforcement, undermining fairness and athlete trust. This paper introduces\nFST.ai, a novel AI-powered framework designed to enhance officiating in Sport\nTaekwondo, particularly focusing on the complex task of real-time head kick\ndetection and scoring. Leveraging computer vision, deep learning, and edge\ninference, the system automates the identification and classification of key\nactions, significantly reducing decision time from minutes to seconds while\nimproving consistency and transparency. Importantly, the methodology is not\nlimited to Taekwondo. The underlying framework -- based on pose estimation,\nmotion classification, and impact analysis -- can be adapted to a wide range of\nsports requiring action detection, such as judo, karate, fencing, or even team\nsports like football and basketball, where foul recognition or performance\ntracking is critical. By addressing one of Taekwondo's most challenging\nscenarios -- head kick scoring -- we demonstrate the robustness, scalability,\nand sport-agnostic potential of FST.ai to transform officiating standards\nacross multiple disciplines.", "AI": {"tldr": "This paper introduces FST.ai, a novel AI-powered framework designed to enhance officiating in Sport Taekwondo, particularly focusing on the complex task of real-time head kick detection and scoring.", "motivation": "Traditional manual systems suffer from latency, subjectivity, and inconsistent enforcement, undermining fairness and athlete trust.", "method": "computer vision, deep learning, and edge inference", "result": "significantly reducing decision time from minutes to seconds while improving consistency and transparency.", "conclusion": "FST.ai demonstrated robustness, scalability, and sport-agnostic potential to transform officiating standards across multiple disciplines."}}
{"id": "2507.14560", "categories": ["cs.LG", "cs.CV", "68T07, 05C50, 15A18", "I.2.6; I.2.7; I.5.1"], "pdf": "https://arxiv.org/pdf/2507.14560", "abs": "https://arxiv.org/abs/2507.14560", "authors": ["Giorgio Roffo"], "title": "The Origin of Self-Attention: From Pairwise Affinity Matrices to Transformers", "comment": "24 pages, 10 figures, submitted for review. Companion code and\n  reproducibility materials available", "summary": "The self-attention mechanism, now central to deep learning architectures such\nas Transformers, is a modern instance of a more general computational\nprinciple: learning and using pairwise affinity matrices to control how\ninformation flows through a model. This paper traces the conceptual origins of\nself-attention across multiple domains, including computer vision, natural\nlanguage processing, and graph learning, through their shared reliance on an\naffinity matrix, denoted as A. We highlight Infinite Feature Selection (Inf-FS)\nas a foundational approach that generalizes the idea of affinity-based\nweighting. Unlike the fixed dot-product structure used in Transformers, Inf-FS\ndefines A either through domain knowledge or by learning, and computes feature\nrelevance through multi-hop propagation over the affinity graph. From this\nperspective, self-attention can be seen as a special case of Inf-FS: it uses a\nsingle-hop affinity computation where A is dynamically built from token\nsimilarities. We argue that the underlying structure, reasoning over pairwise\nrelationships, is preserved across both approaches, and the key differences lie\nin how the affinity matrix is defined and applied. By situating self-attention\nwithin the broader paradigm of affinity-based computation, we unify several\nstrands of machine learning research and highlight a common mathematical\nfoundation that underpins diverse models and tasks.", "AI": {"tldr": "Self-attention\u662f\u57fa\u4e8e\u4eb2\u548c\u529b\u8ba1\u7b97\u7684\u4e00\u79cd\u7279\u6b8a\u60c5\u51b5\uff0c\u4e0eInf-FS\u5171\u4eab\u5e95\u5c42\u7ed3\u6784\uff0c\u5dee\u5f02\u5728\u4e8e\u4eb2\u548c\u77e9\u9635\u7684\u5b9a\u4e49\u548c\u5e94\u7528\u3002", "motivation": "self-attention\u673a\u5236\u662f\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff08\u5982Transformer\uff09\u7684\u6838\u5fc3\uff0c\u4f46\u5b83\u662f\u4e00\u79cd\u66f4\u901a\u7528\u7684\u8ba1\u7b97\u539f\u5219\uff08\u5b66\u4e60\u548c\u4f7f\u7528\u6210\u5bf9\u4eb2\u548c\u77e9\u9635\u6765\u63a7\u5236\u4fe1\u606f\u5982\u4f55\u5728\u6a21\u578b\u4e2d\u6d41\u52a8\uff09\u7684\u73b0\u4ee3\u5b9e\u4f8b\u3002", "method": "\u901a\u8fc7\u8ffd\u8e2aself-attention\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u56fe\u5b66\u4e60\u7b49\u591a\u4e2a\u9886\u57df\u7684\u6982\u5ff5\u8d77\u6e90\uff0c\u5e76\u5c06\u5176\u4e0eInfinite Feature Selection (Inf-FS)\u8054\u7cfb\u8d77\u6765\u3002", "result": "\u5c06self-attention\u7f6e\u4e8e\u57fa\u4e8e\u4eb2\u548c\u529b\u7684\u8ba1\u7b97\u7684\u66f4\u5e7f\u6cdb\u8303\u4f8b\u4e2d\uff0c\u7edf\u4e00\u4e86\u673a\u5668\u5b66\u4e60\u7814\u7a76\u7684\u591a\u4e2a\u5206\u652f\uff0c\u5e76\u5f3a\u8c03\u4e86\u652f\u6491\u4e0d\u540c\u6a21\u578b\u548c\u4efb\u52a1\u7684\u5171\u540c\u6570\u5b66\u57fa\u7840\u3002", "conclusion": "self-attention\u662f\u57fa\u4e8e\u4eb2\u548c\u529b\u7684\u8ba1\u7b97\u65b9\u6cd5\u7684\u4e00\u79cd\u7279\u6b8a\u60c5\u51b5\uff0c\u5b83\u4e0eInf-FS\u5171\u4eab\u4e00\u4e2a\u6f5c\u5728\u7684\u7ed3\u6784\uff0c\u5373\u57fa\u4e8e\u6210\u5bf9\u5173\u7cfb\u7684\u63a8\u7406\u3002\u5173\u952e\u533a\u522b\u5728\u4e8e\u4eb2\u548c\u77e9\u9635\u7684\u5b9a\u4e49\u548c\u5e94\u7528\u65b9\u5f0f\u3002"}}
{"id": "2507.15239", "categories": ["cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.15239", "abs": "https://arxiv.org/abs/2507.15239", "authors": ["Qianchao Wang", "Yuxuan Ding", "Chuanzhen Jia", "Zhe Li", "Yaping Du"], "title": "Explainable Artificial Intelligence based Soft Evaluation Indicator for Arc Fault Diagnosis", "comment": null, "summary": "Novel AI-based arc fault diagnosis models have demonstrated outstanding\nperformance in terms of classification accuracy. However, an inherent problem\nis whether these models can actually be trusted to find arc faults. In this\nlight, this work proposes a soft evaluation indicator that explains the outputs\nof arc fault diagnosis models, by defining the the correct explanation of arc\nfaults and leveraging Explainable Artificial Intelligence and real arc fault\nexperiments. Meanwhile, a lightweight balanced neural network is proposed to\nguarantee competitive accuracy and soft feature extraction score. In our\nexperiments, several traditional machine learning methods and deep learning\nmethods across two arc fault datasets with different sample times and noise\nlevels are utilized to test the effectiveness of the soft evaluation indicator.\nThrough this approach, the arc fault diagnosis models are easy to understand\nand trust, allowing practitioners to make informed and trustworthy decisions.", "AI": {"tldr": "This paper proposes a soft evaluation indicator to explain the outputs of arc fault diagnosis models and a lightweight balanced neural network to guarantee competitive accuracy and soft feature extraction score.", "motivation": "Whether AI-based arc fault diagnosis models can actually be trusted to find arc faults is an inherent problem.", "method": "A soft evaluation indicator is proposed to explain the outputs of arc fault diagnosis models, by defining the correct explanation of arc faults and leveraging Explainable Artificial Intelligence and real arc fault experiments. A lightweight balanced neural network is proposed to guarantee competitive accuracy and soft feature extraction score.", "result": "Several traditional machine learning methods and deep learning methods across two arc fault datasets with different sample times and noise levels are utilized to test the effectiveness of the soft evaluation indicator.", "conclusion": "The proposed soft evaluation indicator makes arc fault diagnosis models easy to understand and trust, allowing practitioners to make informed and trustworthy decisions."}}
{"id": "2507.14871", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14871", "abs": "https://arxiv.org/abs/2507.14871", "authors": ["Ronit D. Gross", "Yarden Tzach", "Tal Halevi", "Ella Koresh", "Ido Kanter"], "title": "Tiny language models", "comment": "23 pages, 1 figure and 12 tables", "summary": "A prominent achievement of natural language processing (NLP) is its ability\nto understand and generate meaningful human language. This capability relies on\ncomplex feedforward transformer block architectures pre-trained on large\nlanguage models (LLMs). However, LLM pre-training is currently feasible only\nfor a few dominant companies due to the immense computational resources\nrequired, limiting broader research participation. This creates a critical need\nfor more accessible alternatives. In this study, we explore whether tiny\nlanguage models (TLMs) exhibit the same key qualitative features of LLMs. We\ndemonstrate that TLMs exhibit a clear performance gap between pre-trained and\nnon-pre-trained models across classification tasks, indicating the\neffectiveness of pre-training, even at a tiny scale. The performance gap\nincreases with the size of the pre-training dataset and with greater overlap\nbetween tokens in the pre-training and classification datasets. Furthermore,\nthe classification accuracy achieved by a pre-trained deep TLM architecture can\nbe replicated through a soft committee of multiple, independently pre-trained\nshallow architectures, enabling low-latency TLMs without affecting\nclassification accuracy. Our results are based on pre-training BERT-6 and\nvariants of BERT-1 on subsets of the Wikipedia dataset and evaluating their\nperformance on FewRel, AGNews, and DBPedia classification tasks. Future\nresearch on TLM is expected to further illuminate the mechanisms underlying\nNLP, especially given that its biologically inspired models suggest that TLMs\nmay be sufficient for children or adolescents to develop language.", "AI": {"tldr": "This paper explores whether tiny language models (TLMs) exhibit the same key qualitative features of LLMs and demonstrates the effectiveness of pre-training, even at a tiny scale.", "motivation": "LLM pre-training is currently feasible only for a few dominant companies due to the immense computational resources required, limiting broader research participation, creating a critical need for more accessible alternatives.", "method": "Pre-training BERT-6 and variants of BERT-1 on subsets of the Wikipedia dataset and evaluating their performance on FewRel, AGNews, and DBPedia classification tasks.", "result": "TLMs exhibit a clear performance gap between pre-trained and non-pre-trained models across classification tasks, indicating the effectiveness of pre-training, even at a tiny scale. The performance gap increases with the size of the pre-training dataset and with greater overlap between tokens in the pre-training and classification datasets.", "conclusion": "Pre-trained deep TLM architecture's classification accuracy can be replicated through a soft committee of multiple, independently pre-trained shallow architectures, enabling low-latency TLMs without affecting classification accuracy."}}
{"id": "2507.14662", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14662", "abs": "https://arxiv.org/abs/2507.14662", "authors": ["Shayan Rokhva", "Babak Teimourpour"], "title": "Artificial Intelligence in the Food Industry: Food Waste Estimation based on Computer Vision, a Brief Case Study in a University Dining Hall", "comment": "Questions & Recommendations: shayanrokhva1999@gmail.com;\n  shayan1999rokh@yahoo.com", "summary": "Quantifying post-consumer food waste in institutional dining settings is\nessential for supporting data-driven sustainability strategies. This study\npresents a cost-effective computer vision framework that estimates plate-level\nfood waste by utilizing semantic segmentation of RGB images taken before and\nafter meal consumption across five Iranian dishes. Four fully supervised models\n(U-Net, U-Net++, and their lightweight variants) were trained using a capped\ndynamic inverse-frequency loss and AdamW optimizer, then evaluated through a\ncomprehensive set of metrics, including Pixel Accuracy, Dice, IoU, and a\ncustom-defined Distributional Pixel Agreement (DPA) metric tailored to the\ntask. All models achieved satisfying performance, and for each food type, at\nleast one model approached or surpassed 90% DPA, demonstrating strong alignment\nin pixel-wise proportion estimates. Lighter models with reduced parameter\ncounts offered faster inference, achieving real-time throughput on an NVIDIA T4\nGPU. Further analysis showed superior segmentation performance for dry and more\nrigid components (e.g., rice and fries), while more complex, fragmented, or\nviscous dishes, such as stews, showed reduced performance, specifically\npost-consumption. Despite limitations such as reliance on 2D imaging,\nconstrained food variety, and manual data collection, the proposed framework is\npioneering and represents a scalable, contactless solution for continuous\nmonitoring of food consumption. This research lays foundational groundwork for\nautomated, real-time waste tracking systems in large-scale food service\nenvironments and offers actionable insights and outlines feasible future\ndirections for dining hall management and policymakers aiming to reduce\ninstitutional food waste.", "AI": {"tldr": "Presents a computer vision framework for estimating plate-level food waste in dining settings using semantic segmentation.. Achieved high accuracy with lighter models enabling real-time performance.", "motivation": "Quantifying post-consumer food waste in institutional dining settings is essential for supporting data-driven sustainability strategies.", "method": "a cost-effective computer vision framework that estimates plate-level food waste by utilizing semantic segmentation of RGB images taken before and after meal consumption across five Iranian dishes. Four fully supervised models (U-Net, U-Net++, and their lightweight variants) were trained using a capped dynamic inverse-frequency loss and AdamW optimizer", "result": "All models achieved satisfying performance, and for each food type, at least one model approached or surpassed 90% DPA, demonstrating strong alignment in pixel-wise proportion estimates. Lighter models with reduced parameter counts offered faster inference, achieving real-time throughput on an NVIDIA T4 GPU. Further analysis showed superior segmentation performance for dry and more rigid components (e.g., rice and fries), while more complex, fragmented, or viscous dishes, such as stews, showed reduced performance, specifically post-consumption.", "conclusion": "This research lays foundational groundwork for automated, real-time waste tracking systems in large-scale food service environments and offers actionable insights and outlines feasible future directions for dining hall management and policymakers aiming to reduce institutional food waste."}}
{"id": "2507.14570", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14570", "abs": "https://arxiv.org/abs/2507.14570", "authors": ["Xu Cheng", "Liang Yao", "Feng He", "Yukuo Cen", "Yufei He", "Chenhui Zhang", "Wenzheng Feng", "Hongyun Cai", "Jie Tang"], "title": "LPS-GNN : Deploying Graph Neural Networks on Graphs with 100-Billion Edges", "comment": null, "summary": "Graph Neural Networks (GNNs) have emerged as powerful tools for various graph\nmining tasks, yet existing scalable solutions often struggle to balance\nexecution efficiency with prediction accuracy. These difficulties stem from\niterative message-passing techniques, which place significant computational\ndemands and require extensive GPU memory, particularly when dealing with the\nneighbor explosion issue inherent in large-scale graphs. This paper introduces\na scalable, low-cost, flexible, and efficient GNN framework called LPS-GNN,\nwhich can perform representation learning on 100 billion graphs with a single\nGPU in 10 hours and shows a 13.8% improvement in User Acquisition scenarios. We\nexamine existing graph partitioning methods and design a superior graph\npartition algorithm named LPMetis. In particular, LPMetis outperforms current\nstate-of-the-art (SOTA) approaches on various evaluation metrics. In addition,\nour paper proposes a subgraph augmentation strategy to enhance the model's\npredictive performance. It exhibits excellent compatibility, allowing the\nentire framework to accommodate various GNN algorithms. Successfully deployed\non the Tencent platform, LPS-GNN has been tested on public and real-world\ndatasets, achieving performance lifts of 8. 24% to 13. 89% over SOTA models in\nonline applications.", "AI": {"tldr": "LPS-GNN \u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u4f4e\u6210\u672c\u3001\u7075\u6d3b\u4e14\u9ad8\u6548\u7684 GNN \u6846\u67b6\uff0c\u5b83\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u4e14\u53ef\u4ee5\u90e8\u7f72\u5728\u817e\u8baf\u5e73\u53f0\u4e0a\u3002", "motivation": "\u73b0\u6709\u7684\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u96be\u4ee5\u5e73\u8861\u6267\u884c\u6548\u7387\u548c\u9884\u6d4b\u51c6\u786e\u6027\u3002\u8fd9\u4e9b\u56f0\u96be\u6e90\u4e8e\u8fed\u4ee3\u6d88\u606f\u4f20\u9012\u6280\u672f\uff0c\u8fd9\u4e9b\u6280\u672f\u63d0\u51fa\u4e86\u91cd\u8981\u7684\u8ba1\u7b97\u9700\u6c42\uff0c\u5e76\u4e14\u9700\u8981\u5927\u91cf\u7684 GPU \u5185\u5b58\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u5927\u89c4\u6a21\u56fe\u4e2d\u56fa\u6709\u7684\u90bb\u5c45\u7206\u70b8\u95ee\u9898\u65f6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a LPMetis \u7684\u56fe\u5212\u5206\u7b97\u6cd5\u548c\u5b50\u56fe\u589e\u5f3a\u7b56\u7565\u3002", "result": "LPS-GNN \u53ef\u4ee5\u5728 10 \u5c0f\u65f6\u5185\u4f7f\u7528\u5355\u4e2a GPU \u5bf9 1000 \u4ebf\u4e2a\u56fe\u6267\u884c\u8868\u793a\u5b66\u4e60\uff0c\u5e76\u5728\u7528\u6237\u83b7\u53d6\u573a\u666f\u4e2d\u663e\u793a\u51fa 13.8% \u7684\u6539\u8fdb\u3002", "conclusion": "LPS-GNN\u5728\u516c\u5171\u548c\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u5728\u5728\u7ebf\u5e94\u7528\u4e2d\uff0c\u76f8\u5bf9\u4e8e SOTA \u6a21\u578b\uff0c\u6027\u80fd\u63d0\u5347\u4e86 8.24% \u5230 13.89%\u3002"}}
{"id": "2507.15253", "categories": ["cs.AI", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2507.15253", "abs": "https://arxiv.org/abs/2507.15253", "authors": ["Zhaochen Guo", "Zhixiang Shen", "Xuanting Xie", "Liangjian Wen", "Zhao Kang"], "title": "Disentangling Homophily and Heterophily in Multimodal Graph Clustering", "comment": "Appear in ACM Multimedia 2025", "summary": "Multimodal graphs, which integrate unstructured heterogeneous data with\nstructured interconnections, offer substantial real-world utility but remain\ninsufficiently explored in unsupervised learning. In this work, we initiate the\nstudy of multimodal graph clustering, aiming to bridge this critical gap.\nThrough empirical analysis, we observe that real-world multimodal graphs often\nexhibit hybrid neighborhood patterns, combining both homophilic and\nheterophilic relationships. To address this challenge, we propose a novel\nframework -- \\textsc{Disentangled Multimodal Graph Clustering (DMGC)} -- which\ndecomposes the original hybrid graph into two complementary views: (1) a\nhomophily-enhanced graph that captures cross-modal class consistency, and (2)\nheterophily-aware graphs that preserve modality-specific inter-class\ndistinctions. We introduce a \\emph{Multimodal Dual-frequency Fusion} mechanism\nthat jointly filters these disentangled graphs through a dual-pass strategy,\nenabling effective multimodal integration while mitigating category confusion.\nOur self-supervised alignment objectives further guide the learning process\nwithout requiring labels. Extensive experiments on both multimodal and\nmulti-relational graph datasets demonstrate that DMGC achieves state-of-the-art\nperformance, highlighting its effectiveness and generalizability across diverse\nsettings. Our code is available at https://github.com/Uncnbb/DMGC.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u56fe\u805a\u7c7b\u6846\u67b6DMGC\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u89e3\u7f20\u548c\u878d\u5408\u7b56\u7565\u5904\u7406\u6df7\u5408\u90bb\u57df\u6a21\u5f0f\uff0c\u5e76\u5728\u591a\u6a21\u6001\u548c\u591a\u5173\u7cfb\u56fe\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u56fe\u96c6\u6210\u4e86\u975e\u7ed3\u6784\u5316\u5f02\u6784\u6570\u636e\u548c\u7ed3\u6784\u5316\u4e92\u8fde\uff0c\u63d0\u4f9b\u4e86\u5927\u91cf\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u4f46\u5728\u65e0\u76d1\u7763\u5b66\u4e60\u4e2d\u4ecd\u672a\u5f97\u5230\u5145\u5206\u7684\u63a2\u7d22\u3002\u8fd9\u9879\u5de5\u4f5c\u542f\u52a8\u4e86\u591a\u6a21\u6001\u56fe\u805a\u7c7b\u7684\u7814\u7a76\uff0c\u65e8\u5728\u5f25\u5408\u8fd9\u4e00\u5173\u952e\u5dee\u8ddd\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6--\\\\textsc{Disentangled Multimodal Graph Clustering (DMGC)}--\u5b83\u5c06\u539f\u59cb\u6df7\u5408\u56fe\u5206\u89e3\u4e3a\u4e24\u4e2a\u4e92\u8865\u7684\u89c6\u56fe\uff1a(1) \u6355\u83b7\u8de8\u6a21\u6001\u7c7b\u4e00\u81f4\u6027\u7684\u540c\u8d28\u6027\u589e\u5f3a\u56fe\uff0c\u4ee5\u53ca(2) \u4fdd\u7559\u6a21\u6001\u7279\u5b9a\u7c7b\u95f4\u5dee\u5f02\u7684\u5f02\u8d28\u6027\u611f\u77e5\u56fe\u3002\u5f15\u5165\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u53cc\u9891\u878d\u5408\u673a\u5236\uff0c\u901a\u8fc7\u53cc\u901a\u9053\u7b56\u7565\u8054\u5408\u8fc7\u6ee4\u8fd9\u4e9b\u89e3\u7f20\u56fe\uff0c\u4ece\u800c\u5b9e\u73b0\u6709\u6548\u7684\u591a\u6a21\u6001\u96c6\u6210\uff0c\u540c\u65f6\u51cf\u8f7b\u7c7b\u522b\u6df7\u6dc6\u3002", "result": "\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u771f\u5b9e\u4e16\u754c\u7684\u591a\u6a21\u6001\u56fe\u901a\u5e38\u8868\u73b0\u51fa\u6df7\u5408\u90bb\u57df\u6a21\u5f0f\uff0c\u7ed3\u5408\u4e86\u540c\u8d28\u548c\u5f02\u8d28\u5173\u7cfb\u3002", "conclusion": "DMGC\u5728\u591a\u6a21\u6001\u548c\u591a\u5173\u7cfb\u56fe\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u7a81\u663e\u4e86\u5176\u5728\u4e0d\u540c\u8bbe\u7f6e\u4e2d\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u6027\u3002"}}
{"id": "2507.14887", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14887", "abs": "https://arxiv.org/abs/2507.14887", "authors": ["Shiyi Mu", "Yongkang Liu", "Shi Feng", "Xiaocui Yang", "Daling Wang", "Yifei Zhang"], "title": "MEKiT: Multi-source Heterogeneous Knowledge Injection Method via Instruction Tuning for Emotion-Cause Pair Extraction", "comment": "Accepted by CogSci", "summary": "Although large language models (LLMs) excel in text comprehension and\ngeneration, their performance on the Emotion-Cause Pair Extraction (ECPE) task,\nwhich requires reasoning ability, is often underperform smaller language model.\nThe main reason is the lack of auxiliary knowledge, which limits LLMs' ability\nto effectively perceive emotions and reason causes. To address this issue, we\npropose a novel \\textbf{M}ulti-source h\\textbf{E}terogeneous \\textbf{K}nowledge\n\\textbf{i}njection me\\textbf{T}hod, MEKiT, which integrates heterogeneous\ninternal emotional knowledge and external causal knowledge. Specifically, for\nthese two distinct aspects and structures of knowledge, we apply the approaches\nof incorporating instruction templates and mixing data for instruction-tuning,\nwhich respectively facilitate LLMs in more comprehensively identifying emotion\nand accurately reasoning causes. Experimental results demonstrate that MEKiT\nprovides a more effective and adaptable solution for the ECPE task, exhibiting\nan absolute performance advantage over compared baselines and dramatically\nimproving the performance of LLMs on the ECPE task.", "AI": {"tldr": "This paper proposes MEKiT, a multi-source heterogeneous knowledge injection method, to improve LLMs' performance on the ECPE task by integrating internal emotional knowledge and external causal knowledge. MEKiT outperforms baselines and improves LLMs' performance dramatically.", "motivation": "LLMs' performance on the Emotion-Cause Pair Extraction (ECPE) task is often underperform smaller language model. The main reason is the lack of auxiliary knowledge, which limits LLMs' ability to effectively perceive emotions and reason causes.", "method": "Multi-source Heterogeneous Knowledge injection Method, MEKiT, which integrates heterogeneous internal emotional knowledge and external causal knowledge. incorporates instruction templates and mixing data for instruction-tuning.", "result": "MEKiT provides a more effective and adaptable solution for the ECPE task, exhibiting an absolute performance advantage over compared baselines and dramatically improving the performance of LLMs on the ECPE task.", "conclusion": "MEKiT is a more effective and adaptable solution for the ECPE task, exhibiting an absolute performance advantage over compared baselines and dramatically improving the performance of LLMs on the ECPE task."}}
{"id": "2507.14670", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14670", "abs": "https://arxiv.org/abs/2507.14670", "authors": ["Yaxuan Song", "Jianan Fan", "Hang Chang", "Weidong Cai"], "title": "Gene-DML: Dual-Pathway Multi-Level Discrimination for Gene Expression Prediction from Histopathology Images", "comment": "16 pages, 15 tables, 8 figures", "summary": "Accurately predicting gene expression from histopathology images offers a\nscalable and non-invasive approach to molecular profiling, with significant\nimplications for precision medicine and computational pathology. However,\nexisting methods often underutilize the cross-modal representation alignment\nbetween histopathology images and gene expression profiles across multiple\nrepresentational levels, thereby limiting their prediction performance. To\naddress this, we propose Gene-DML, a unified framework that structures latent\nspace through Dual-pathway Multi-Level discrimination to enhance correspondence\nbetween morphological and transcriptional modalities. The multi-scale\ninstance-level discrimination pathway aligns hierarchical histopathology\nrepresentations extracted at local, neighbor, and global levels with gene\nexpression profiles, capturing scale-aware morphological-transcriptional\nrelationships. In parallel, the cross-level instance-group discrimination\npathway enforces structural consistency between individual (image/gene)\ninstances and modality-crossed (gene/image, respectively) groups, strengthening\nthe alignment across modalities. By jointly modelling fine-grained and\nstructural-level discrimination, Gene-DML is able to learn robust cross-modal\nrepresentations, enhancing both predictive accuracy and generalization across\ndiverse biological contexts. Extensive experiments on public spatial\ntranscriptomics datasets demonstrate that Gene-DML achieves state-of-the-art\nperformance in gene expression prediction. The code and checkpoints will be\nreleased soon.", "AI": {"tldr": "Gene-DML \u662f\u4e00\u79cd\u7528\u4e8e\u4ece\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u9884\u6d4b\u57fa\u56e0\u8868\u8fbe\u7684\u65b0\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u53cc\u8def\u5f84\u591a\u7ea7\u5224\u522b\u6765\u589e\u5f3a\u5f62\u6001\u548c\u8f6c\u5f55\u6a21\u5f0f\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u5e76\u5728\u516c\u5171\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u4ece\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u51c6\u786e\u9884\u6d4b\u57fa\u56e0\u8868\u8fbe\u4e3a\u5206\u5b50\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u975e\u4fb5\u5165\u6027\u65b9\u6cd5\uff0c\u5bf9\u7cbe\u51c6\u533b\u5b66\u548c\u8ba1\u7b97\u75c5\u7406\u5b66\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u672a\u80fd\u5145\u5206\u5229\u7528\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u548c\u8de8\u591a\u4e2a\u8868\u5f81\u6c34\u5e73\u7684\u57fa\u56e0\u8868\u8fbe\u8c31\u4e4b\u95f4\u7684\u8de8\u6a21\u6001\u8868\u5f81\u5bf9\u9f50\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5b83\u4eec\u7684\u9884\u6d4b\u6027\u80fd\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86 Gene-DML\uff0c\u8fd9\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u53cc\u8def\u5f84\u591a\u7ea7\u5224\u522b\u6765\u6784\u5efa\u6f5c\u5728\u7a7a\u95f4\uff0c\u4ee5\u589e\u5f3a\u5f62\u6001\u548c\u8f6c\u5f55\u6a21\u5f0f\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\u3002", "result": "\u516c\u5171\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cGene-DML \u5728\u57fa\u56e0\u8868\u8fbe\u9884\u6d4b\u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "Gene-DML\u5728\u57fa\u56e0\u8868\u8fbe\u9884\u6d4b\u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2507.14592", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14592", "abs": "https://arxiv.org/abs/2507.14592", "authors": ["Haochen Liu", "Jia Bi", "Xiaomin Wang", "Xin Yang", "Ling Wang"], "title": "A Transformer-Based Conditional GAN with Multiple Instance Learning for UAV Signal Detection and Classification", "comment": "13 pages, 7 figures", "summary": "Unmanned Aerial Vehicles (UAVs) are increasingly used in surveillance,\nlogistics, agriculture, disaster management, and military operations. Accurate\ndetection and classification of UAV flight states, such as hovering, cruising,\nascending, or transitioning, which are essential for safe and effective\noperations. However, conventional time series classification (TSC) methods\noften lack robustness and generalization for dynamic UAV environments, while\nstate of the art(SOTA) models like Transformers and LSTM based architectures\ntypically require large datasets and entail high computational costs,\nespecially with high-dimensional data streams. This paper proposes a novel\nframework that integrates a Transformer-based Generative Adversarial Network\n(GAN) with Multiple Instance Locally Explainable Learning (MILET) to address\nthese challenges in UAV flight state classification. The Transformer encoder\ncaptures long-range temporal dependencies and complex telemetry dynamics, while\nthe GAN module augments limited datasets with realistic synthetic samples. MIL\nis incorporated to focus attention on the most discriminative input segments,\nreducing noise and computational overhead. Experimental results show that the\nproposed method achieves superior accuracy 96.5% on the DroneDetect dataset and\n98.6% on the DroneRF dataset that outperforming other SOTA approaches. The\nframework also demonstrates strong computational efficiency and robust\ngeneralization across diverse UAV platforms and flight states, highlighting its\npotential for real-time deployment in resource constrained environments.", "AI": {"tldr": "This paper proposes a novel framework that integrates a Transformer-based Generative Adversarial Network (GAN) with Multiple Instance Locally Explainable Learning (MILET) to address challenges in UAV flight state classification.", "motivation": "conventional time series classification (TSC) methods often lack robustness and generalization for dynamic UAV environments, while state of the art(SOTA) models like Transformers and LSTM based architectures typically require large datasets and entail high computational costs, especially with high-dimensional data streams", "method": "integrates a Transformer-based Generative Adversarial Network (GAN) with Multiple Instance Locally Explainable Learning (MILET)", "result": "achieves superior accuracy 96.5% on the DroneDetect dataset and 98.6% on the DroneRF dataset that outperforming other SOTA approaches. The framework also demonstrates strong computational efficiency", "conclusion": "The proposed framework achieves superior accuracy and robust generalization across diverse UAV platforms and flight states, highlighting its potential for real-time deployment in resource constrained environments."}}
{"id": "2507.15268", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.15268", "abs": "https://arxiv.org/abs/2507.15268", "authors": ["Junhyeong Lee", "Joon-Young Kim", "Heekyu Kim", "Inhyo Lee", "Seunghwa Ryu"], "title": "IM-Chat: A Multi-agent LLM-based Framework for Knowledge Transfer in Injection Molding Industry", "comment": null, "summary": "The injection molding industry faces critical challenges in preserving and\ntransferring field knowledge, particularly as experienced workers retire and\nmultilingual barriers hinder effective communication. This study introduces\nIM-Chat, a multi-agent framework based on large language models (LLMs),\ndesigned to facilitate knowledge transfer in injection molding. IM-Chat\nintegrates both limited documented knowledge (e.g., troubleshooting tables,\nmanuals) and extensive field data modeled through a data-driven process\ncondition generator that infers optimal manufacturing settings from\nenvironmental inputs such as temperature and humidity, enabling robust and\ncontext-aware task resolution. By adopting a retrieval-augmented generation\n(RAG) strategy and tool-calling agents within a modular architecture, IM-Chat\nensures adaptability without the need for fine-tuning. Performance was assessed\nacross 100 single-tool and 60 hybrid tasks for GPT-4o, GPT-4o-mini, and\nGPT-3.5-turbo by domain experts using a 10-point rubric focused on relevance\nand correctness, and was further supplemented by automated evaluation using\nGPT-4o guided by a domain-adapted instruction prompt. The evaluation results\nindicate that more capable models tend to achieve higher accuracy, particularly\nin complex, tool-integrated scenarios. Overall, these findings demonstrate the\nviability of multi-agent LLM systems for industrial knowledge workflows and\nestablish IM-Chat as a scalable and generalizable approach to AI-assisted\ndecision support in manufacturing.", "AI": {"tldr": "IM-Chat\u662f\u4e00\u4e2a\u57fa\u4e8ellm\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u65e8\u5728\u4fc3\u8fdb\u6ce8\u5851\u884c\u4e1a\u7684\u77e5\u8bc6\u8f6c\u79fb\uff0c\u5b83\u901a\u8fc7\u6574\u5408\u6709\u9650\u7684\u6587\u6863\u77e5\u8bc6\u548c\u5e7f\u6cdb\u7684\u73b0\u573a\u6570\u636e\uff0c\u5b9e\u73b0\u7a33\u5065\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4efb\u52a1\u89e3\u51b3\u3002", "motivation": "\u6ce8\u5851\u884c\u4e1a\u5728\u4fdd\u5b58\u548c\u4f20\u9012\u9886\u57df\u77e5\u8bc6\u65b9\u9762\u9762\u4e34\u7740\u4e25\u5cfb\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u7ecf\u9a8c\u4e30\u5bcc\u7684\u5de5\u4eba\u9000\u4f11\u548c\u591a\u8bed\u8a00\u969c\u788d\u963b\u788d\u6709\u6548\u6c9f\u901a\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6IM-Chat\uff0c\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210(rag)\u7b56\u7565\u548c\u6a21\u5757\u5316\u67b6\u6784\u4e2d\u7684\u5de5\u5177\u8c03\u7528\u4ee3\u7406\u3002", "result": "\u5728\u9886\u57df\u4e13\u5bb6\u4f7f\u752810\u70b9\u91cf\u8868(\u4fa7\u91cd\u4e8e\u76f8\u5173\u6027\u548c\u6b63\u786e\u6027)\u5bf9GPT-4o\u3001GPT-4o-mini\u548cGPT-3.5-turbo\u7684100\u4e2a\u5355\u5de5\u5177\u548c60\u4e2a\u6df7\u5408\u4efb\u52a1\u8fdb\u884c\u8bc4\u4f30\u540e\uff0c\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u66f4\u6709\u80fd\u529b\u7684\u6a21\u578b\u5f80\u5f80\u80fd\u83b7\u5f97\u66f4\u9ad8\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u7684\u5de5\u5177\u96c6\u6210\u573a\u666f\u4e2d\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u5728\u5de5\u4e1a\u77e5\u8bc6\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u662f\u53ef\u884c\u7684\uff0c\u5e76\u5c06IM-Chat\u786e\u7acb\u4e3a\u4e00\u79cd\u53ef\u6269\u5c55\u548c\u901a\u7528\u7684AI\u8f85\u52a9\u5236\u9020\u51b3\u7b56\u652f\u6301\u65b9\u6cd5\u3002"}}
{"id": "2507.14894", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14894", "abs": "https://arxiv.org/abs/2507.14894", "authors": ["Boyi Deng", "Yu Wan", "Baosong Yang", "Fei Huang", "Wenjie Wang", "Fuli Feng"], "title": "Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs", "comment": null, "summary": "Large Language Models (LLMs) have impressive multilingual capabilities, but\nthey suffer from unexpected code-switching, also known as language mixing,\nwhich involves switching to unexpected languages in the model response. This\nproblem leads to poor readability and degrades the usability of model\nresponses. However, existing work on this issue lacks a mechanistic analysis\nand shows limited effectiveness. In this paper, we first provide an in-depth\nanalysis of unexpected code-switching using sparse autoencoders and find that\nwhen LLMs switch to a language, the features of that language exhibit excessive\npre-activation values. Based on our findings, we propose $\\textbf{S}$parse\n$\\textbf{A}$utoencoder-guided $\\textbf{S}$upervised\n$\\textbf{F}$ine$\\textbf{t}$uning (SASFT), which teaches LLMs to maintain\nappropriate pre-activation values of specific language features during\ntraining. Experiments on five models across three languages demonstrate that\nSASFT consistently reduces unexpected code-switching by more than 50\\% compared\nto standard supervised fine-tuning, with complete elimination in four cases.\nMoreover, SASFT maintains or even improves the models' performance on six\nmultilingual benchmarks, showing its effectiveness in addressing code-switching\nwhile preserving multilingual capabilities.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86LLM\u4e2d\u610f\u5916\u7684code-switching\u95ee\u9898\uff0c\u53d1\u73b0\u8bed\u8a00\u7279\u5f81\u7684\u8fc7\u5ea6pre-activation\u503c\u662f\u4e3b\u8981\u539f\u56e0\u3002\u63d0\u51fa\u4e86SASFT\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a7\u5236pre-activation\u503c\u6765\u51cf\u5c11code-switching\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u5177\u6709\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u591a\u8bed\u8a00\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u4f1a\u906d\u53d7\u610f\u5916\u7684code-switching\u95ee\u9898\uff0c\u5bfc\u81f4\u6a21\u578b\u54cd\u5e94\u7684\u53ef\u8bfb\u6027\u5dee\uff0c\u5e76\u964d\u4f4e\u6a21\u578b\u7684\u53ef\u7528\u6027\u3002", "method": "\u63d0\u51faSparse Autoencoder-guided Supervised Fine-tuning (SASFT)\uff0c\u6559LLM\u5728\u8bad\u7ec3\u671f\u95f4\u4fdd\u6301\u7279\u5b9a\u8bed\u8a00\u7279\u5f81\u7684\u9002\u5f53pre-activation\u503c\u3002", "result": "\u5728\u4e09\u4e2a\u8bed\u8a00\u7684\u4e94\u4e2a\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u6807\u51c6\u76d1\u7763\u5fae\u8c03\u76f8\u6bd4\uff0cSASFT\u59cb\u7ec8\u80fd\u51cf\u5c1150%\u4ee5\u4e0a\u7684\u610f\u5916code-switching\uff0c\u5728\u56db\u4e2a\u6848\u4f8b\u4e2d\u5b8c\u5168\u6d88\u9664\u3002\u6b64\u5916\uff0cSASFT\u4fdd\u6301\u751a\u81f3\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u516d\u4e2a\u591a\u8bed\u8a00benchmark\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "SASFT\u80fd\u6709\u6548\u51cf\u5c11LLM\u4e2d\u610f\u5916\u7684code-switching\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u9ad8\u6a21\u578b\u5728\u591a\u8bed\u8a00benchmark\u4e0a\u7684\u6027\u80fd\u3002"}}
{"id": "2507.14675", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14675", "abs": "https://arxiv.org/abs/2507.14675", "authors": ["Yuchen Duan", "Zhe Chen", "Yusong Hu", "Weiyun Wang", "Shenglong Ye", "Botian Shi", "Lewei Lu", "Qibin Hou", "Tong Lu", "Hongsheng Li", "Jifeng Dai", "Wenhai Wang"], "title": "Docopilot: Improving Multimodal Models for Document-Level Understanding", "comment": null, "summary": "Despite significant progress in multimodal large language models (MLLMs),\ntheir performance on complex, multi-page document comprehension remains\ninadequate, largely due to the lack of high-quality, document-level datasets.\nWhile current retrieval-augmented generation (RAG) methods offer partial\nsolutions, they suffer from issues, such as fragmented retrieval contexts,\nmulti-stage error accumulation, and extra time costs of retrieval. In this\nwork, we present a high-quality document-level dataset, Doc-750K, designed to\nsupport in-depth understanding of multimodal documents. This dataset includes\ndiverse document structures, extensive cross-page dependencies, and real\nquestion-answer pairs derived from the original documents. Building on the\ndataset, we develop a native multimodal model, Docopilot, which can accurately\nhandle document-level dependencies without relying on RAG. Experiments\ndemonstrate that Docopilot achieves superior coherence, accuracy, and\nefficiency in document understanding tasks and multi-turn interactions, setting\na new baseline for document-level multimodal understanding. Data, code, and\nmodels are released at https://github.com/OpenGVLab/Docopilot", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u6587\u6863\u7ea7\u6570\u636e\u96c6Doc-750K\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u5f00\u53d1\u4e86\u4e00\u4e2a\u539f\u751f\u7684\u591a\u6a21\u6001\u6a21\u578bDocopilot\uff0c\u5b83\u5728\u6587\u6863\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e14\u4e0d\u9700\u8981\u4f9d\u8d56RAG\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b(MLLM)\u5728\u590d\u6742\u7684\u591a\u9875\u6587\u6863\u7406\u89e3\u65b9\u9762\u7684\u6027\u80fd\u4ecd\u7136\u4e0d\u8db3\uff0c\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u6587\u6863\u7ea7\u6570\u636e\u96c6\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u539f\u751f\u7684\u591a\u6a21\u6001\u6a21\u578bDocopilot\uff0c\u5b83\u53ef\u4ee5\u51c6\u786e\u5730\u5904\u7406\u6587\u6863\u7ea7\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u800c\u4e0d\u9700\u8981\u4f9d\u8d56RAG\u3002", "result": "Docopilot\u5728\u6587\u6863\u7406\u89e3\u4efb\u52a1\u548c\u591a\u8f6e\u4ea4\u4e92\u4e2d\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u8fde\u8d2f\u6027\u3001\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "conclusion": "Docopilot\u5728\u6587\u6863\u7406\u89e3\u4efb\u52a1\u548c\u591a\u8f6e\u4ea4\u4e92\u4e2d\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u8fde\u8d2f\u6027\u3001\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u4e3a\u6587\u6863\u7ea7\u591a\u6a21\u6001\u7406\u89e3\u8bbe\u5b9a\u4e86\u65b0\u7684\u57fa\u7ebf\u3002"}}
{"id": "2507.14631", "categories": ["cs.LG", "cs.CG", "cs.DS"], "pdf": "https://arxiv.org/pdf/2507.14631", "abs": "https://arxiv.org/abs/2507.14631", "authors": ["Daniel Greenhut", "Dan Feldman"], "title": "$k$-PCA for (non-squared) Euclidean Distances: Polynomial Time Approximation", "comment": null, "summary": "Given an integer $k\\geq1$ and a set $P$ of $n$ points in $\\REAL^d$, the\nclassic $k$-PCA (Principle Component Analysis) approximates the affine\n\\emph{$k$-subspace mean} of $P$, which is the $k$-dimensional affine linear\nsubspace that minimizes its sum of squared Euclidean distances\n($\\ell_{2,2}$-norm) over the points of $P$, i.e., the mean of these distances.\nThe \\emph{$k$-subspace median} is the subspace that minimizes its sum of\n(non-squared) Euclidean distances ($\\ell_{2,1}$-mixed norm), i.e., their\nmedian. The median subspace is usually more sparse and robust to noise/outliers\nthan the mean, but also much harder to approximate since, unlike the\n$\\ell_{z,z}$ (non-mixed) norms, it is non-convex for $k<d-1$.\n  We provide the first polynomial-time deterministic algorithm whose both\nrunning time and approximation factor are not exponential in $k$. More\nprecisely, the multiplicative approximation factor is $\\sqrt{d}$, and the\nrunning time is polynomial in the size of the input. We expect that our\ntechnique would be useful for many other related problems, such as $\\ell_{2,z}$\nnorm of distances for $z\\not \\in \\br{1,2}$, e.g., $z=\\infty$, and handling\noutliers/sparsity.\n  Open code and experimental results on real-world datasets are also provided.", "AI": {"tldr": "This paper introduces the first polynomial-time deterministic algorithm for approximating the k-subspace median, achieving a $\\sqrt{d}$ approximation factor with polynomial running time. The approach is expected to be applicable to related problems and includes open code and experimental results.", "motivation": "The median subspace is usually more sparse and robust to noise/outliers than the mean, but also much harder to approximate since, unlike the $\\ell_{z,z}$ (non-mixed) norms, it is non-convex for $k<d-1$.", "method": "a polynomial-time deterministic algorithm", "result": "the multiplicative approximation factor is $\\sqrt{d}$, and the running time is polynomial in the size of the input. Open code and experimental results on real-world datasets are also provided.", "conclusion": "We provide the first polynomial-time deterministic algorithm whose both running time and approximation factor are not exponential in $k$. More precisely, the multiplicative approximation factor is $\\sqrt{d}$, and the running time is polynomial in the size of the input. We expect that our technique would be useful for many other related problems, such as $\\ell_{2,z}$ norm of distances for $z\\not \\in \\br{1,2}$, e.g., $z=\\infty$, and handling outliers/sparsity."}}
{"id": "2507.15330", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15330", "abs": "https://arxiv.org/abs/2507.15330", "authors": ["Hammad Atta", "Muhammad Zeeshan Baig", "Yasir Mehmood", "Nadeem Shahzad", "Ken Huang", "Muhammad Aziz Ul Haq", "Muhammad Awais", "Kamal Ahmed"], "title": "QSAF: A Novel Mitigation Framework for Cognitive Degradation in Agentic AI", "comment": null, "summary": "We introduce Cognitive Degradation as a novel vulnerability class in agentic\nAI systems. Unlike traditional adversarial external threats such as prompt\ninjection, these failures originate internally, arising from memory starvation,\nplanner recursion, context flooding, and output suppression. These systemic\nweaknesses lead to silent agent drift, logic collapse, and persistent\nhallucinations over time. To address this class of failures, we introduce the\nQorvex Security AI Framework for Behavioral & Cognitive Resilience (QSAF Domain\n10), a lifecycle-aware defense framework defined by a six-stage cognitive\ndegradation lifecycle. The framework includes seven runtime controls\n(QSAF-BC-001 to BC-007) that monitor agent subsystems in real time and trigger\nproactive mitigation through fallback routing, starvation detection, and memory\nintegrity enforcement. Drawing from cognitive neuroscience, we map agentic\narchitectures to human analogs, enabling early detection of fatigue,\nstarvation, and role collapse. By introducing a formal lifecycle and real-time\nmitigation controls, this work establishes Cognitive Degradation as a critical\nnew class of AI system vulnerability and proposes the first cross-platform\ndefense model for resilient agentic behavior.", "AI": {"tldr": "\u8ba4\u77e5\u9000\u5316\u662f\u4e00\u79cd\u65b0\u7684 AI \u7cfb\u7edf\u8106\u5f31\u6027\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8de8\u5e73\u53f0\u9632\u5fa1\u6a21\u578b\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "motivation": "\u5f15\u5165\u8ba4\u77e5\u9000\u5316\u4f5c\u4e3a\u4ee3\u7406 AI \u7cfb\u7edf\u4e2d\u7684\u4e00\u79cd\u65b0\u7684\u8106\u5f31\u6027\u7c7b\u522b\u3002\u4e0e\u4f20\u7edf\u7684\u5bf9\u6297\u6027\u5916\u90e8\u5a01\u80c1\uff08\u5982\u63d0\u793a\u6ce8\u5165\uff09\u4e0d\u540c\uff0c\u8fd9\u4e9b\u6545\u969c\u6e90\u4e8e\u5185\u90e8\uff0c\u7531\u8bb0\u5fc6\u4e0d\u8db3\u3001\u89c4\u5212\u5668\u9012\u5f52\u3001\u4e0a\u4e0b\u6587\u6cdb\u6ee5\u548c\u8f93\u51fa\u6291\u5236\u5f15\u8d77\u3002\u8fd9\u4e9b\u7cfb\u7edf\u6027\u5f31\u70b9\u5bfc\u81f4\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\u51fa\u73b0\u65e0\u58f0\u7684\u4ee3\u7406\u6f02\u79fb\u3001\u903b\u8f91\u5d29\u6e83\u548c\u6301\u7eed\u7684\u5e7b\u89c9\u3002", "method": "\u5f15\u5165\u4e86 Qorvex Security AI Framework for Behavioral & Cognitive Resilience (QSAF Domain 10)\uff0c\u8fd9\u662f\u4e00\u4e2a\u7531\u516d\u4e2a\u9636\u6bb5\u8ba4\u77e5\u9000\u5316\u751f\u547d\u5468\u671f\u5b9a\u4e49\u7684\u751f\u547d\u5468\u671f\u611f\u77e5\u9632\u5fa1\u6846\u67b6\uff0c\u5305\u62ec\u4e03\u4e2a\u8fd0\u884c\u65f6\u63a7\u5236\uff08QSAF-BC-001 \u5230 BC-007\uff09\uff0c\u8fd9\u4e9b\u63a7\u5236\u5b9e\u65f6\u76d1\u63a7\u4ee3\u7406\u5b50\u7cfb\u7edf\u5e76\u901a\u8fc7\u56de\u9000\u8def\u7531\u3001\u9965\u997f\u68c0\u6d4b\u548c\u5185\u5b58\u5b8c\u6574\u6027\u6267\u884c\u89e6\u53d1\u4e3b\u52a8\u7f13\u89e3\u3002", "result": "\u901a\u8fc7\u5c06\u4ee3\u7406\u67b6\u6784\u6620\u5c04\u5230\u4eba\u7c7b\u7c7b\u4f3c\u7269\uff0c\u80fd\u591f\u53ca\u65e9\u53d1\u73b0\u75b2\u52b3\u3001\u9965\u997f\u548c\u89d2\u8272\u5d29\u6e83\u3002", "conclusion": "\u5efa\u7acb\u4e86\u8ba4\u77e5\u9000\u5316\u4f5c\u4e3a\u4e00\u4e2a\u65b0\u7684 AI \u7cfb\u7edf\u8106\u5f31\u6027\u7684\u5173\u952e\u7c7b\u522b\uff0c\u5e76\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u7528\u4e8e\u5f39\u6027\u4ee3\u7406\u884c\u4e3a\u7684\u8de8\u5e73\u53f0\u9632\u5fa1\u6a21\u578b\u3002"}}
{"id": "2507.14900", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14900", "abs": "https://arxiv.org/abs/2507.14900", "authors": ["Chongxuan Huang", "Yongshi Ye", "Biao Fu", "Qifeng Su", "Xiaodong Shi"], "title": "From Neurons to Semantics: Evaluating Cross-Linguistic Alignment Capabilities of Large Language Models via Neurons Alignment", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable multilingual\ncapabilities, however, how to evaluate cross-lingual alignment remains\nunderexplored. Existing alignment benchmarks primarily focus on sentence\nembeddings, but prior research has shown that neural models tend to induce a\nnon-smooth representation space, which impact of semantic alignment evaluation\non low-resource languages. Inspired by neuroscientific findings that similar\ninformation activates overlapping neuronal regions, we propose a novel Neuron\nState-Based Cross-Lingual Alignment (NeuronXA) to assess the cross-lingual a\nlignment capabilities of LLMs, which offers a more semantically grounded\napproach to assess cross-lingual alignment. We evaluate NeuronXA on several\nprominent multilingual LLMs (LLaMA, Qwen, Mistral, GLM, and OLMo) across two\ntransfer tasks and three multilingual benchmarks. The results demonstrate that\nwith only 100 parallel sentence pairs, NeuronXA achieves a Pearson correlation\nof 0.9556 with downstream tasks performance and 0.8514 with transferability.\nThese findings demonstrate NeuronXA's effectiveness in assessing both\ncross-lingual alignment and transferability, even with a small dataset. This\nhighlights its potential to advance cross-lingual alignment research and to\nimprove the semantic understanding of multilingual LLMs.", "AI": {"tldr": "\u63d0\u51fa NeuronXA \u6765\u8bc4\u4f30 LLM \u7684\u8de8\u8bed\u8a00\u5bf9\u9f50\u80fd\u529b\uff0c\u5373\u4f7f\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u4e5f\u80fd\u6709\u6548\u8bc4\u4f30\u8de8\u8bed\u8a00\u5bf9\u9f50\u548c\u53ef\u8fc1\u79fb\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5bf9\u9f50\u57fa\u51c6\u4e3b\u8981\u96c6\u4e2d\u5728\u53e5\u5b50\u5d4c\u5165\u4e0a\uff0c\u4f46\u4e4b\u524d\u7684\u7814\u7a76\u8868\u660e\uff0c\u795e\u7ecf\u6a21\u578b\u503e\u5411\u4e8e\u4ea7\u751f\u975e\u5e73\u6ed1\u7684\u8868\u793a\u7a7a\u95f4\uff0c\u8fd9\u4f1a\u5f71\u54cd\u8bed\u4e49\u5bf9\u9f50\u8bc4\u4f30\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u7684\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u795e\u7ecf\u5143\u72b6\u6001\u7684\u8de8\u8bed\u8a00\u5bf9\u9f50 (NeuronXA) \u65b9\u6cd5\uff0c\u4ee5\u8bc4\u4f30 LLM \u7684\u8de8\u8bed\u8a00\u5bf9\u9f50\u80fd\u529b\u3002", "result": "NeuronXA \u5728\u591a\u4e2a\u4e3b\u8981\u7684 multilingual LLM\uff08LLaMA\u3001Qwen\u3001Mistral\u3001GLM \u548c OLMo\uff09\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u8de8\u8d8a\u4e24\u4e2a\u8fc1\u79fb\u4efb\u52a1\u548c\u4e09\u4e2a multilingual \u57fa\u51c6\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4ec5\u4f7f\u7528 100 \u4e2a\u5e76\u884c\u53e5\u5b50\u5bf9\uff0cNeuronXA \u4e0e\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u7684\u76f8\u5173\u7cfb\u6570\u4e3a 0.9556\uff0c\u4e0e\u53ef\u8fc1\u79fb\u6027\u7684\u76f8\u5173\u7cfb\u6570\u4e3a 0.8514\u3002", "conclusion": "NeuronXA \u662f\u4e00\u79cd\u8bc4\u4f30\u8de8\u8bed\u8a00\u5bf9\u9f50\u548c\u53ef\u8fc1\u79fb\u6027\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5373\u4f7f\u5728\u5c0f\u578b\u6570\u636e\u96c6\u4e0a\u4e5f\u80fd\u5b9e\u73b0\u3002\u5b83\u6709\u6f5c\u529b\u63a8\u8fdb\u8de8\u8bed\u8a00\u5bf9\u9f50\u7814\u7a76\uff0c\u5e76\u63d0\u9ad8\u591a\u8bed\u8a00 LLM \u7684\u8bed\u4e49\u7406\u89e3\u3002"}}
{"id": "2507.14680", "categories": ["cs.CV", "cs.AI", "68T07, 92C55", "I.2.7; I.4.8; J.3"], "pdf": "https://arxiv.org/pdf/2507.14680", "abs": "https://arxiv.org/abs/2507.14680", "authors": ["Xinheng Lyu", "Yuci Liang", "Wenting Chen", "Meidan Ding", "Jiaqi Yang", "Guolin Huang", "Daokun Zhang", "Xiangjian He", "Linlin Shen"], "title": "WSI-Agents: A Collaborative Multi-Agent System for Multi-Modal Whole Slide Image Analysis", "comment": null, "summary": "Whole slide images (WSIs) are vital in digital pathology, enabling gigapixel\ntissue analysis across various pathological tasks. While recent advancements in\nmulti-modal large language models (MLLMs) allow multi-task WSI analysis through\nnatural language, they often underperform compared to task-specific models.\nCollaborative multi-agent systems have emerged as a promising solution to\nbalance versatility and accuracy in healthcare, yet their potential remains\nunderexplored in pathology-specific domains. To address these issues, we\npropose WSI-Agents, a novel collaborative multi-agent system for multi-modal\nWSI analysis. WSI-Agents integrates specialized functional agents with robust\ntask allocation and verification mechanisms to enhance both task-specific\naccuracy and multi-task versatility through three components: (1) a task\nallocation module assigning tasks to expert agents using a model zoo of patch\nand WSI level MLLMs, (2) a verification mechanism ensuring accuracy through\ninternal consistency checks and external validation using pathology knowledge\nbases and domain-specific models, and (3) a summary module synthesizing the\nfinal summary with visual interpretation maps. Extensive experiments on\nmulti-modal WSI benchmarks show WSI-Agents's superiority to current WSI MLLMs\nand medical agent frameworks across diverse tasks.", "AI": {"tldr": "WSI-Agents\u662f\u4e00\u79cd\u7528\u4e8e\u591a\u6a21\u6001WSI\u5206\u6790\u7684\u534f\u4f5c\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u5b83\u4f18\u4e8e\u5f53\u524d\u7684WSI MLLM\u548c\u533b\u7597\u4ee3\u7406\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b(MLLM)\u867d\u7136\u53ef\u4ee5\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5b9e\u73b0\u591a\u4efb\u52a1WSI\u5206\u6790\uff0c\u4f46\u4e0e\u7279\u5b9a\u4efb\u52a1\u6a21\u578b\u76f8\u6bd4\uff0c\u6027\u80fd\u901a\u5e38\u4e0d\u4f73\u3002\u534f\u4f5c\u591a\u4ee3\u7406\u7cfb\u7edf\u5df2\u6210\u4e3a\u5728\u533b\u7597\u4fdd\u5065\u9886\u57df\u5e73\u8861\u591a\u529f\u80fd\u6027\u548c\u51c6\u786e\u6027\u7684\u6709\u5e0c\u671b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5b83\u4eec\u5728\u75c5\u7406\u5b66\u7279\u5b9a\u9886\u57df\u7684\u6f5c\u529b\u4ecd\u672a\u5f97\u5230\u5145\u5206\u6316\u6398\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86WSI-Agents\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u6a21\u6001WSI\u5206\u6790\u7684\u65b0\u578b\u534f\u4f5c\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u5b83\u96c6\u6210\u4e86\u5177\u6709\u5f3a\u5927\u4efb\u52a1\u5206\u914d\u548c\u9a8c\u8bc1\u673a\u5236\u7684\u4e13\u7528\u529f\u80fd\u4ee3\u7406\u3002", "result": "WSI-Agents\u901a\u8fc7\u4e09\u4e2a\u7ec4\u6210\u90e8\u5206\u589e\u5f3a\u4e86\u7279\u5b9a\u4efb\u52a1\u7684\u51c6\u786e\u6027\u548c\u591a\u4efb\u52a1\u901a\u7528\u6027\uff1a(1)\u4efb\u52a1\u5206\u914d\u6a21\u5757\uff0c\u4f7f\u7528patch\u548cWSI\u7ea7\u522b\u7684MLLM\u6a21\u578b\u5e93\u5c06\u4efb\u52a1\u5206\u914d\u7ed9\u4e13\u5bb6\u4ee3\u7406\uff1b(2)\u9a8c\u8bc1\u673a\u5236\uff0c\u901a\u8fc7\u5185\u90e8\u4e00\u81f4\u6027\u68c0\u67e5\u548c\u4f7f\u7528\u75c5\u7406\u5b66\u77e5\u8bc6\u5e93\u548c\u9886\u57df\u7279\u5b9a\u6a21\u578b\u7684\u5916\u90e8\u9a8c\u8bc1\u6765\u786e\u4fdd\u51c6\u786e\u6027\uff1b(3)\u6458\u8981\u6a21\u5757\uff0c\u7efc\u5408\u5e26\u6709\u89c6\u89c9\u89e3\u91ca\u56fe\u7684\u6700\u7ec8\u6458\u8981\u3002", "conclusion": "WSI-Agents\u5728\u591a\u6a21\u6001WSI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u5f53\u524d\u7684WSI MLLM\u548c\u533b\u7597\u4ee3\u7406\u6846\u67b6\u3002"}}
{"id": "2507.14668", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14668", "abs": "https://arxiv.org/abs/2507.14668", "authors": ["Yunfeng Li", "Junhong Liu", "Zhaohui Yang", "Guofu Liao", "Chuyun Zhang"], "title": "Rec-AD: An Efficient Computation Framework for FDIA Detection Based on Tensor Train Decomposition and Deep Learning Recommendation Model", "comment": "15 pages, 14 figures", "summary": "Deep learning models have been widely adopted for False Data Injection Attack\n(FDIA) detection in smart grids due to their ability to capture unstructured\nand sparse features. However, the increasing system scale and data\ndimensionality introduce significant computational and memory burdens,\nparticularly in large-scale industrial datasets, limiting detection efficiency.\nTo address these issues, this paper proposes Rec-AD, a computationally\nefficient framework that integrates Tensor Train decomposition with the Deep\nLearning Recommendation Model (DLRM). Rec-AD enhances training and inference\nefficiency through embedding compression, optimized data access via index\nreordering, and a pipeline training mechanism that reduces memory communication\noverhead. Fully compatible with PyTorch, Rec-AD can be integrated into existing\nFDIA detection systems without code modifications. Experimental results show\nthat Rec-AD significantly improves computational throughput and real-time\ndetection performance, narrowing the attack window and increasing attacker\ncost. These advancements strengthen edge computing capabilities and\nscalability, providing robust technical support for smart grid security.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRec-AD\uff0c\u901a\u8fc7\u5d4c\u5165\u538b\u7f29\u3001\u4f18\u5316\u6570\u636e\u8bbf\u95ee\u548c\u6d41\u6c34\u7ebf\u8bad\u7ec3\u673a\u5236\uff0c\u63d0\u9ad8FDIA\u68c0\u6d4b\u7684\u8ba1\u7b97\u6548\u7387\u548c\u5b9e\u65f6\u6027\uff0c\u589e\u5f3a\u667a\u80fd\u7535\u7f51\u5b89\u5168\u6027\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5df2\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u667a\u80fd\u7535\u7f51\u4e2d\u7684\u865a\u5047\u6570\u636e\u6ce8\u5165\u653b\u51fb\uff08FDIA\uff09\u68c0\u6d4b\uff0c\u4f46\u65e5\u76ca\u589e\u957f\u7684\u7cfb\u7edf\u89c4\u6a21\u548c\u6570\u636e\u7ef4\u5ea6\u5e26\u6765\u4e86\u5de8\u5927\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u8d1f\u62c5\uff0c\u7279\u522b\u662f\u5728\u5927\u578b\u5de5\u4e1a\u6570\u636e\u96c6\u4e2d\uff0c\u9650\u5236\u4e86\u68c0\u6d4b\u6548\u7387\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51faRec-AD\uff0c\u4e00\u4e2a\u96c6\u6210\u4e86\u5f20\u91cf\u706b\u8f66\u5206\u89e3\u4e0e\u6df1\u5ea6\u5b66\u4e60\u63a8\u8350\u6a21\u578b\uff08DLRM\uff09\u7684\u8ba1\u7b97\u6548\u7387\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRec-AD\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u541e\u5410\u91cf\u548c\u5b9e\u65f6\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "Rec-AD\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u541e\u5410\u91cf\u548c\u5b9e\u65f6\u68c0\u6d4b\u6027\u80fd\uff0c\u7f29\u5c0f\u4e86\u653b\u51fb\u7a97\u53e3\u5e76\u589e\u52a0\u4e86\u653b\u51fb\u8005\u6210\u672c\u3002\u8fd9\u4e9b\u8fdb\u6b65\u52a0\u5f3a\u4e86\u8fb9\u7f18\u8ba1\u7b97\u80fd\u529b\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u667a\u80fd\u7535\u7f51\u5b89\u5168\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2507.15351", "categories": ["cs.AI", "cs.ET", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.15351", "abs": "https://arxiv.org/abs/2507.15351", "authors": ["Zijian Zhao", "Sen Li"], "title": "One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step Policy Optimization for Order Dispatch on Ride-Sharing Platforms", "comment": null, "summary": "On-demand ride-sharing platforms face the fundamental challenge of\ndynamically bundling passengers with diverse origins and destinations and\nmatching them with vehicles in real time, all under significant uncertainty.\nRecently, MARL has emerged as a promising solution for this problem, leveraging\ndecentralized learning to address the curse of dimensionality caused by the\nlarge number of agents in the ride-hailing market and the resulting expansive\nstate and action spaces. However, conventional MARL-based ride-sharing\napproaches heavily rely on the accurate estimation of Q-values or V-values,\nwhich becomes problematic in large-scale, highly uncertain environments.\nSpecifically, most of these approaches adopt an independent paradigm,\nexacerbating this issue, as each agent treats others as part of the\nenvironment, leading to unstable training and substantial estimation bias in\nvalue functions. To address these challenges, we propose two novel alternative\nmethods that bypass value function estimation. First, we adapt GRPO to\nride-sharing, replacing the PPO baseline with the group average reward to\neliminate critic estimation errors and reduce training bias. Second, inspired\nby GRPO's full utilization of group reward information, we customize the PPO\nframework for ride-sharing platforms and show that, under a homogeneous fleet,\nthe optimal policy can be trained using only one-step rewards - a method we\nterm One-Step Policy Optimization (OSPO). Experiments on a real-world Manhattan\nride-hailing dataset demonstrate that both GRPO and OSPO achieve superior\nperformance across most scenarios, efficiently optimizing pickup times and the\nnumber of served orders using simple MLP networks.", "AI": {"tldr": "This paper introduces GRPO and OSPO, two novel MARL methods for on-demand ride-sharing that bypass value function estimation, achieving superior performance in optimizing pickup times and served orders.", "motivation": "Conventional MARL-based ride-sharing approaches heavily rely on the accurate estimation of Q-values or V-values, which becomes problematic in large-scale, highly uncertain environments. Specifically, most of these approaches adopt an independent paradigm, exacerbating this issue, as each agent treats others as part of the environment, leading to unstable training and substantial estimation bias in value functions.", "method": "We propose two novel alternative methods that bypass value function estimation. First, we adapt GRPO to ride-sharing, replacing the PPO baseline with the group average reward to eliminate critic estimation errors and reduce training bias. Second, inspired by GRPO's full utilization of group reward information, we customize the PPO framework for ride-sharing platforms and show that, under a homogeneous fleet, the optimal policy can be trained using only one-step rewards - a method we term One-Step Policy Optimization (OSPO).", "result": "Experiments on a real-world Manhattan ride-hailing dataset demonstrate that both GRPO and OSPO achieve superior performance across most scenarios, efficiently optimizing pickup times and the number of served orders using simple MLP networks.", "conclusion": "GRPO and OSPO achieve superior performance across most scenarios, efficiently optimizing pickup times and the number of served orders using simple MLP networks."}}
{"id": "2507.14913", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14913", "abs": "https://arxiv.org/abs/2507.14913", "authors": ["Eliya Habba", "Noam Dahan", "Gili Lior", "Gabriel Stanovsky"], "title": "PromptSuite: A Task-Agnostic Framework for Multi-Prompt Generation", "comment": "Eliya Habba and Noam Dahan contributed equally to this work", "summary": "Evaluating LLMs with a single prompt has proven unreliable, with small\nchanges leading to significant performance differences. However, generating the\nprompt variations needed for a more robust multi-prompt evaluation is\nchallenging, limiting its adoption in practice. To address this, we introduce\nPromptSuite, a framework that enables the automatic generation of various\nprompts. PromptSuite is flexible - working out of the box on a wide range of\ntasks and benchmarks. It follows a modular prompt design, allowing controlled\nperturbations to each component, and is extensible, supporting the addition of\nnew components and perturbation types. Through a series of case studies, we\nshow that PromptSuite provides meaningful variations to support strong\nevaluation practices. It is available through both a Python API:\nhttps://github.com/eliyahabba/PromptSuite, and a user-friendly web interface:\nhttps://promptsuite.streamlit.app/", "AI": {"tldr": "PromptSuite is a framework that enables the automatic generation of various prompts.", "motivation": "Evaluating LLMs with a single prompt has proven unreliable, with small changes leading to significant performance differences. However, generating the prompt variations needed for a more robust multi-prompt evaluation is challenging, limiting its adoption in practice.", "method": "a framework that enables the automatic generation of various prompts", "result": "PromptSuite is flexible - working out of the box on a wide range of tasks and benchmarks. It follows a modular prompt design, allowing controlled perturbations to each component, and is extensible, supporting the addition of new components and perturbation types.", "conclusion": "PromptSuite provides meaningful variations to support strong evaluation practices."}}
{"id": "2507.14686", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14686", "abs": "https://arxiv.org/abs/2507.14686", "authors": ["Chen Cai", "Tianyi Liu", "Jianjun Gao", "Wenyang Liu", "Kejun Wu", "Ruoyu Wang", "Yi Wang", "Soo Chin Liew"], "title": "From Semantics, Scene to Instance-awareness: Distilling Foundation Model for Open-vocabulary Situation Recognition", "comment": null, "summary": "Recent Multimodal Large Language Models (MLLMs) exhibit strong zero-shot\nabilities but struggle with complex Grounded Situation Recognition (GSR) and\nare resource-intensive for edge device deployment. Meanwhile, conventional GSR\nmodels often lack generalization ability, falling short in recognizing unseen\nand rare situations. In this paper, we exploit transferring knowledge from a\nteacher MLLM to a small GSR model to enhance its generalization and zero-shot\nabilities, thereby introducing the task of Open-vocabulary Grounded Situation\nRecognition (Ov-GSR). To achieve this, we propose Multimodal Interactive Prompt\nDistillation (MIPD), a novel framework that distills enriched multimodal\nknowledge from the foundation model, enabling the student Ov-GSR model to\nrecognize unseen situations and be better aware of rare situations.\nSpecifically, the MIPD framework first leverages the LLM-based Judgmental\nRationales Generator (JRG) to construct positive and negative glimpse and gaze\nrationales enriched with contextual semantic information. The proposed\nscene-aware and instance-perception prompts are then introduced to align\nrationales with visual information from the MLLM teacher via the\nNegative-Guided Multimodal Prompting Alignment (NMPA) module, effectively\ncapturing holistic and perceptual multimodal knowledge. Finally, the aligned\nmultimodal knowledge is distilled into the student Ov-GSR model, providing a\nstronger foundation for generalization that enhances situation understanding,\nbridges the gap between seen and unseen scenarios, and mitigates prediction\nbias in rare cases. We evaluate MIPD on the refined Ov-SWiG dataset, achieving\nsuperior performance on seen, rare, and unseen situations, and further\ndemonstrate improved unseen detection on the HICO-DET dataset.", "AI": {"tldr": "This paper introduces Open-vocabulary Grounded Situation Recognition (Ov-GSR) to enhance generalization and zero-shot abilities of small GSR models by transferring knowledge from a teacher MLLM using Multimodal Interactive Prompt Distillation (MIPD).", "motivation": "Recent Multimodal Large Language Models (MLLMs) exhibit strong zero-shot abilities but struggle with complex Grounded Situation Recognition (GSR) and are resource-intensive for edge device deployment. Meanwhile, conventional GSR models often lack generalization ability, falling short in recognizing unseen and rare situations.", "method": "We propose Multimodal Interactive Prompt Distillation (MIPD), a novel framework that distills enriched multimodal knowledge from the foundation model, enabling the student Ov-GSR model to recognize unseen situations and be better aware of rare situations.", "result": "achieving superior performance on seen, rare, and unseen situations, and further demonstrate improved unseen detection on the HICO-DET dataset.", "conclusion": "We evaluate MIPD on the refined Ov-SWiG dataset, achieving superior performance on seen, rare, and unseen situations, and further demonstrate improved unseen detection on the HICO-DET dataset."}}
{"id": "2507.14677", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14677", "abs": "https://arxiv.org/abs/2507.14677", "authors": ["Yiming Xu", "Zhen Peng", "Bin Shi", "Xu Hua", "Bo Dong", "Song Wang", "Chen Chen"], "title": "Revisiting Graph Contrastive Learning on Anomaly Detection: A Structural Imbalance Perspective", "comment": "Accepted by AAAI2025", "summary": "The superiority of graph contrastive learning (GCL) has prompted its\napplication to anomaly detection tasks for more powerful risk warning systems.\nUnfortunately, existing GCL-based models tend to excessively prioritize overall\ndetection performance while neglecting robustness to structural imbalance,\nwhich can be problematic for many real-world networks following power-law\ndegree distributions. Particularly, GCL-based methods may fail to capture tail\nanomalies (abnormal nodes with low degrees). This raises concerns about the\nsecurity and robustness of current anomaly detection algorithms and therefore\nhinders their applicability in a variety of realistic high-risk scenarios. To\nthe best of our knowledge, research on the robustness of graph anomaly\ndetection to structural imbalance has received little scrutiny. To address the\nabove issues, this paper presents a novel GCL-based framework named AD-GCL. It\ndevises the neighbor pruning strategy to filter noisy edges for head nodes and\nfacilitate the detection of genuine tail nodes by aligning from head nodes to\nforged tail nodes. Moreover, AD-GCL actively explores potential neighbors to\nenlarge the receptive field of tail nodes through anomaly-guided neighbor\ncompletion. We further introduce intra- and inter-view consistency loss of the\noriginal and augmentation graph for enhanced representation. The performance\nevaluation of the whole, head, and tail nodes on multiple datasets validates\nthe comprehensive superiority of the proposed AD-GCL in detecting both head\nanomalies and tail anomalies.", "AI": {"tldr": "AD-GCL\u901a\u8fc7\u90bb\u5c45\u4fee\u526a\u548c\u90bb\u5c45\u8865\u5168\u6765\u89e3\u51b3\u56fe\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u7ed3\u6784\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u5934\u90e8\u548c\u5c3e\u90e8\u5f02\u5e38\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eGCL\u7684\u6a21\u578b\u5ffd\u7565\u4e86\u7ed3\u6784\u4e0d\u5e73\u8861\u7684\u9c81\u68d2\u6027\uff0c\u65e0\u6cd5\u6355\u83b7\u5c3e\u90e8\u5f02\u5e38\u3002", "method": "AD-GCL\u6846\u67b6\uff0c\u5305\u542b\u90bb\u5c45\u4fee\u526a\u7b56\u7565\u548c\u5f02\u5e38\u5f15\u5bfc\u7684\u90bb\u5c45\u8865\u5168\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u8bc4\u4f30\u9a8c\u8bc1\u4e86AD-GCL\u7684\u7efc\u5408\u4f18\u52bf\u3002", "conclusion": "AD-GCL\u5728\u68c0\u6d4b\u5934\u90e8\u548c\u5c3e\u90e8\u5f02\u5e38\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2507.15356", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15356", "abs": "https://arxiv.org/abs/2507.15356", "authors": ["Lu Guo", "Yixiang Shan", "Zhengbang Zhu", "Qifan Liang", "Lichang Song", "Ting Long", "Weinan Zhang", "Yi Chang"], "title": "RAD: Retrieval High-quality Demonstrations to Enhance Decision-making", "comment": null, "summary": "Offline reinforcement learning (RL) enables agents to learn policies from\nfixed datasets, avoiding costly or unsafe environment interactions. However,\nits effectiveness is often limited by dataset sparsity and the lack of\ntransition overlap between suboptimal and expert trajectories, which makes\nlong-horizon planning particularly challenging. Prior solutions based on\nsynthetic data augmentation or trajectory stitching often fail to generalize to\nnovel states and rely on heuristic stitching points. To address these\nchallenges, we propose Retrieval High-quAlity Demonstrations (RAD) for\ndecision-making, which combines non-parametric retrieval with diffusion-based\ngenerative modeling. RAD dynamically retrieves high-return states from the\noffline dataset as target states based on state similarity and return\nestimation, and plans toward them using a condition-guided diffusion model.\nSuch retrieval-guided generation enables flexible trajectory stitching and\nimproves generalization when encountered with underrepresented or\nout-of-distribution states. Extensive experiments confirm that RAD achieves\ncompetitive or superior performance compared to baselines across diverse\nbenchmarks, validating its effectiveness.", "AI": {"tldr": "RAD\u7ed3\u5408\u68c0\u7d22\u548c\u6269\u6563\u6a21\u578b\uff0c\u4ece\u79bb\u7ebf\u6570\u636e\u96c6\u4e2d\u52a8\u6001\u68c0\u7d22\u9ad8\u56de\u62a5\u72b6\u6001\u4f5c\u4e3a\u76ee\u6807\u72b6\u6001\uff0c\u5e76\u4f7f\u7528\u6761\u4ef6\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u89c4\u5212\u8fbe\u5230\u8fd9\u4e9b\u72b6\u6001\u7684\u8def\u5f84\uff0c\u4ece\u800c\u63d0\u5347\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u6709\u6548\u6027\u901a\u5e38\u53d7\u5230\u6570\u636e\u96c6\u7a00\u758f\u6027\u548c\u6b21\u4f18\u8f68\u8ff9\u4e0e\u4e13\u5bb6\u8f68\u8ff9\u4e4b\u95f4\u7f3a\u4e4f\u8f6c\u6362\u91cd\u53e0\u7684\u9650\u5236\uff0c\u8fd9\u4f7f\u5f97\u957f\u65f6\u7a0b\u89c4\u5212\u7279\u522b\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u7ed3\u5408\u975e\u53c2\u6570\u68c0\u7d22\u4e0e\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u6a21\u578b\u3002", "result": "RAD\u901a\u8fc7\u68c0\u7d22\u5f15\u5bfc\u751f\u6210\u5b9e\u73b0\u4e86\u7075\u6d3b\u7684\u8f68\u8ff9\u62fc\u63a5\uff0c\u5e76\u5728\u9047\u5230\u672a\u5145\u5206\u8868\u793a\u6216\u8d85\u51fa\u5206\u5e03\u72b6\u6001\u65f6\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "RAD\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u6216\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.14922", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.14922", "abs": "https://arxiv.org/abs/2507.14922", "authors": ["Vahid Rahimzadeh", "Erfan Moosavi Monazzah", "Mohammad Taher Pilehvar", "Yadollah Yaghoobzadeh"], "title": "SYNTHIA: Synthetic Yet Naturally Tailored Human-Inspired PersonAs", "comment": null, "summary": "Persona-driven LLMs have emerged as powerful tools in computational social\nscience, yet existing approaches fall at opposite extremes, either relying on\ncostly human-curated data or producing synthetic personas that lack consistency\nand realism. We introduce SYNTHIA, a dataset of 30,000 backstories derived from\n10,000 real social media users from BlueSky open platform across three time\nwindows, bridging this spectrum by grounding synthetic generation in authentic\nuser activity. Our evaluation demonstrates that SYNTHIA achieves competitive\nperformance with state-of-the-art methods in demographic diversity and social\nsurvey alignment while significantly outperforming them in narrative\nconsistency. Uniquely, SYNTHIA incorporates temporal dimensionality and\nprovides rich social interaction metadata from the underlying network, enabling\nnew research directions in computational social science and persona-driven\nlanguage modeling.", "AI": {"tldr": "SYNTHIA: a dataset of 30,000 backstories derived from 10,000 real social media users from BlueSky open platform across three time windows, grounding synthetic generation in authentic user activity.", "motivation": "existing approaches fall at opposite extremes, either relying on costly human-curated data or producing synthetic personas that lack consistency and realism.", "method": "grounding synthetic generation in authentic user activity", "result": "a dataset of 30,000 backstories derived from 10,000 real social media users", "conclusion": "SYNTHIA achieves competitive performance in demographic diversity and social survey alignment while significantly outperforming them in narrative consistency. Uniquely, SYNTHIA incorporates temporal dimensionality and provides rich social interaction metadata."}}
{"id": "2507.14697", "categories": ["cs.CV", "I.4.6; I.2.10"], "pdf": "https://arxiv.org/pdf/2507.14697", "abs": "https://arxiv.org/abs/2507.14697", "authors": ["Zhiwei Zhang", "Zi Ye", "Yibin Wen", "Shuai Yuan", "Haohuan Fu", "Jianxi Huang", "Juepeng Zheng"], "title": "GTPBD: A Fine-Grained Global Terraced Parcel and Boundary Dataset", "comment": "38 pages, 18 figures, submitted to NeurIPS 2025", "summary": "Agricultural parcels serve as basic units for conducting agricultural\npractices and applications, which is vital for land ownership registration,\nfood security assessment, soil erosion monitoring, etc. However, existing\nagriculture parcel extraction studies only focus on mid-resolution mapping or\nregular plain farmlands while lacking representation of complex terraced\nterrains due to the demands of precision agriculture.In this paper, we\nintroduce a more fine-grained terraced parcel dataset named GTPBD (Global\nTerraced Parcel and Boundary Dataset), which is the first fine-grained dataset\ncovering major worldwide terraced regions with more than 200,000 complex\nterraced parcels with manual annotation. GTPBD comprises 47,537 high-resolution\nimages with three-level labels, including pixel-level boundary labels, mask\nlabels, and parcel labels. It covers seven major geographic zones in China and\ntranscontinental climatic regions around the world.Compared to the existing\ndatasets, the GTPBD dataset brings considerable challenges due to the: (1)\nterrain diversity; (2) complex and irregular parcel objects; and (3) multiple\ndomain styles. Our proposed GTPBD dataset is suitable for four different tasks,\nincluding semantic segmentation, edge detection, terraced parcel extraction,\nand unsupervised domain adaptation (UDA) tasks.Accordingly, we benchmark the\nGTPBD dataset on eight semantic segmentation methods, four edge extraction\nmethods, three parcel extraction methods, and five UDA methods, along with a\nmulti-dimensional evaluation framework integrating pixel-level and object-level\nmetrics. GTPBD fills a critical gap in terraced remote sensing research,\nproviding a basic infrastructure for fine-grained agricultural terrain analysis\nand cross-scenario knowledge transfer.", "AI": {"tldr": "GTPBD\u662f\u4e00\u4e2a\u65b0\u7684\u7ec6\u7c92\u5ea6\u68af\u7530\u5730\u5757\u6570\u636e\u96c6\uff0c\u5b83\u586b\u8865\u4e86\u68af\u7530\u9065\u611f\u7814\u7a76\u4e2d\u7684\u4e00\u4e2a\u5173\u952e\u7a7a\u767d\uff0c\u5e76\u4e3a\u5404\u79cd\u4efb\u52a1\u63d0\u4f9b\u4e86\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709\u7684\u519c\u4e1a\u5730\u5757\u63d0\u53d6\u7814\u7a76\u53ea\u5173\u6ce8\u4e2d\u7b49\u5206\u8fa8\u7387\u6d4b\u7ed8\u6216\u666e\u901a\u5e73\u539f\u519c\u7530\uff0c\u7531\u4e8e\u7cbe\u786e\u519c\u4e1a\u7684\u9700\u6c42\uff0c\u7f3a\u4e4f\u5bf9\u590d\u6742\u68af\u7530\u5730\u5f62\u7684\u8868\u5f81\u3002", "method": "\u63d0\u51fa\u4e86GTPBD\u6570\u636e\u96c6\uff0c\u5e76\u5bf9\u516b\u79cd\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u3001\u56db\u79cd\u8fb9\u7f18\u63d0\u53d6\u65b9\u6cd5\u3001\u4e09\u79cd\u5730\u5757\u63d0\u53d6\u65b9\u6cd5\u548c\u4e94\u79cdUDA\u65b9\u6cd5\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u53ca\u4e00\u4e2a\u96c6\u6210\u4e86\u50cf\u7d20\u7ea7\u548c\u5bf9\u8c61\u7ea7\u6307\u6807\u7684\u591a\u7ef4\u8bc4\u4f30\u6846\u67b6\u3002", "result": "GTPBD\u662f\u4e00\u4e2a\u7ec6\u7c92\u5ea6\u7684\u68af\u7530\u5730\u5757\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc720\u4e07\u4e2a\u590d\u6742\u68af\u7530\u5730\u5757\u7684\u624b\u52a8\u6807\u6ce8\uff0c\u8986\u76d6\u4e2d\u56fd\u548c\u5168\u7403\u7684\u4e03\u4e2a\u4e3b\u8981\u5730\u7406\u533a\u57df\u548c\u8de8\u5927\u9646\u6c14\u5019\u533a\u57df\u3002\u4e0e\u73b0\u6709\u6570\u636e\u96c6\u76f8\u6bd4\uff0cGTPBD\u6570\u636e\u96c6\u5e26\u6765\u4e86\u76f8\u5f53\u5927\u7684\u6311\u6218\uff0c\u7531\u4e8e\uff1a(1)\u5730\u5f62\u591a\u6837\u6027\uff1b(2)\u590d\u6742\u548c\u4e0d\u89c4\u5219\u7684\u5730\u5757\u5bf9\u8c61\uff1b(3)\u591a\u4e2a\u9886\u57df\u98ce\u683c\u3002GTPBD\u6570\u636e\u96c6\u9002\u7528\u4e8e\u56db\u79cd\u4e0d\u540c\u7684\u4efb\u52a1\uff0c\u5305\u62ec\u8bed\u4e49\u5206\u5272\u3001\u8fb9\u7f18\u68c0\u6d4b\u3001\u68af\u7530\u5730\u5757\u63d0\u53d6\u548c\u65e0\u76d1\u7763\u57df\u9002\u5e94(UDA)\u4efb\u52a1\u3002", "conclusion": "GTPBD \u586b\u8865\u4e86\u68af\u7530\u9065\u611f\u7814\u7a76\u7684\u5173\u952e\u7a7a\u767d\uff0c\u4e3a\u7ec6\u7c92\u5ea6\u519c\u4e1a\u5730\u5f62\u5206\u6790\u548c\u8de8\u573a\u666f\u77e5\u8bc6\u8f6c\u79fb\u63d0\u4f9b\u4e86\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2507.14679", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14679", "abs": "https://arxiv.org/abs/2507.14679", "authors": ["Zixin Xu", "Zhijie Wang", "Zhiyuan Pan"], "title": "GCC-Spam: Spam Detection via GAN, Contrastive Learning, and Character Similarity Networks", "comment": null, "summary": "The exponential growth of spam text on the Internet necessitates robust\ndetection mechanisms to mitigate risks such as information leakage and social\ninstability. This work addresses two principal challenges: adversarial\nstrategies employed by spammers and the scarcity of labeled data. We propose a\nnovel spam-text detection framework GCC-Spam, which integrates three core\ninnovations. First, a character similarity network captures orthographic and\nphonetic features to counter character-obfuscation attacks and furthermore\nproduces sentence embeddings for downstream classification. Second, contrastive\nlearning enhances discriminability by optimizing the latent-space distance\nbetween spam and normal texts. Third, a Generative Adversarial Network (GAN)\ngenerates realistic pseudo-spam samples to alleviate data scarcity while\nimproving model robustness and classification accuracy. Extensive experiments\non real-world datasets demonstrate that our model outperforms baseline\napproaches, achieving higher detection rates with significantly fewer labeled\nexamples.", "AI": {"tldr": "This paper introduces GCC-Spam, a new spam detection framework using character similarity networks, contrastive learning, and GANs to improve detection rates with less labeled data.", "motivation": "The exponential growth of spam text on the Internet necessitates robust detection mechanisms to mitigate risks such as information leakage and social instability. Existing challenges include adversarial strategies employed by spammers and the scarcity of labeled data.", "method": "A novel spam-text detection framework GCC-Spam, which integrates a character similarity network, contrastive learning, and a Generative Adversarial Network (GAN).", "result": "The model achieves higher detection rates with significantly fewer labeled examples compared to baseline approaches.", "conclusion": "The proposed GCC-Spam model outperforms baseline approaches on real-world datasets, achieving higher detection rates with significantly fewer labeled examples."}}
{"id": "2507.15411", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15411", "abs": "https://arxiv.org/abs/2507.15411", "authors": ["Wissam Gherissi", "Mehdi Acheli", "Joyce El Haddad", "Daniela Grigori"], "title": "Predictive Process Monitoring Using Object-centric Graph Embeddings", "comment": "ICSOC Workshops 2024, Dec 2024, Tunis, Tunisia", "summary": "Object-centric predictive process monitoring explores and utilizes\nobject-centric event logs to enhance process predictions. The main challenge\nlies in extracting relevant information and building effective models. In this\npaper, we propose an end-to-end model that predicts future process behavior,\nfocusing on two tasks: next activity prediction and next event time. The\nproposed model employs a graph attention network to encode activities and their\nrelationships, combined with an LSTM network to handle temporal dependencies.\nEvaluated on one reallife and three synthetic event logs, the model\ndemonstrates competitive performance compared to state-of-the-art methods.", "AI": {"tldr": "end-to-end model that predicts future process behavior", "motivation": "Object-centric predictive process monitoring explores and utilizes object-centric event logs to enhance process predictions. The main challenge lies in extracting relevant information and building effective models.", "method": "a graph attention network to encode activities and their relationships, combined with an LSTM network to handle temporal dependencies", "result": "predicts future process behavior, focusing on two tasks: next activity prediction and next event time", "conclusion": "The model demonstrates competitive performance compared to state-of-the-art methods."}}
{"id": "2507.14958", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14958", "abs": "https://arxiv.org/abs/2507.14958", "authors": ["Hang Yan", "Fangzhi Xu", "Rongman Xu", "Yifei Li", "Jian Zhang", "Haoran Luo", "Xiaobao Wu", "Luu Anh Tuan", "Haiteng Zhao", "Qika Lin", "Jun Liu"], "title": "MUR: Momentum Uncertainty guided Reasoning for Large Language Models", "comment": "25 pages, 8 figures", "summary": "Large Language Models (LLMs) have achieved impressive performance on\nreasoning-intensive tasks, yet optimizing their reasoning efficiency remains an\nopen challenge. While Test-Time Scaling (TTS) improves reasoning quality, it\noften leads to overthinking, wasting tokens on redundant computations. This\nwork investigates how to efficiently and adaptively guide LLM test-time scaling\nwithout additional training. Inspired by the concept of momentum in physics, we\npropose Momentum Uncertainty-guided Reasoning (MUR), which dynamically\nallocates thinking budgets to critical reasoning steps by tracking and\naggregating stepwise uncertainty over time. To support flexible inference-time\ncontrol, we introduce gamma-control, a simple mechanism that tunes the\nreasoning budget via a single hyperparameter. We provide in-depth theoretical\nproof to support the superiority of MUR in terms of stability and biases. MUR\nis comprehensively evaluated against various TTS methods across four\nchallenging benchmarks (MATH-500, AIME24, AIME25, and GPQA-diamond) using\ndifferent sizes of recent Qwen3 models (1.7B, 4B, and 8B). Results demonstrate\nthat MUR reduces computation by over 50% on average while improving accuracy by\n0.62-3.37%.", "AI": {"tldr": "\u63d0\u51fa\u4e86MUR\uff0c\u4e00\u79cd\u52a8\u6001\u5206\u914d\u63a8\u7406\u9884\u7b97\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8LLM\u7684\u63a8\u7406\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u91cf\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\uff0c\u4f46\u4f18\u5316\u5176\u63a8\u7406\u6548\u7387\u4ecd\u7136\u662f\u4e00\u4e2a\u5f00\u653e\u7684\u6311\u6218\u3002\u867d\u7136\u6d4b\u8bd5\u65f6\u7f29\u653e\uff08TTS\uff09\u63d0\u9ad8\u4e86\u63a8\u7406\u8d28\u91cf\uff0c\u4f46\u5b83\u901a\u5e38\u4f1a\u5bfc\u81f4\u8fc7\u5ea6\u601d\u8003\uff0c\u5c06token\u6d6a\u8d39\u5728\u5197\u4f59\u8ba1\u7b97\u4e0a\u3002", "method": "\u63d0\u51fa\u4e86\u52a8\u91cf\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u63a8\u7406\uff08MUR\uff09\uff0c\u901a\u8fc7\u8ddf\u8e2a\u548c\u805a\u5408\u9010\u6b65\u4e0d\u786e\u5b9a\u6027\uff0c\u52a8\u6001\u5730\u5c06\u601d\u8003\u9884\u7b97\u5206\u914d\u7ed9\u5173\u952e\u7684\u63a8\u7406\u6b65\u9aa4\u3002", "result": "\u5728\u56db\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMUR\u5e73\u5747\u51cf\u5c11\u4e8650%\u4ee5\u4e0a\u7684\u8ba1\u7b97\u91cf\uff0c\u540c\u65f6\u63d0\u9ad8\u4e860.62-3.37%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "MUR\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u91cf\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002"}}
{"id": "2507.14738", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14738", "abs": "https://arxiv.org/abs/2507.14738", "authors": ["Jeannie She", "Katie Spivakovsky"], "title": "MultiRetNet: A Multimodal Vision Model and Deferral System for Staging Diabetic Retinopathy", "comment": null, "summary": "Diabetic retinopathy (DR) is a leading cause of preventable blindness,\naffecting over 100 million people worldwide. In the United States, individuals\nfrom lower-income communities face a higher risk of progressing to advanced\nstages before diagnosis, largely due to limited access to screening. Comorbid\nconditions further accelerate disease progression. We propose MultiRetNet, a\nnovel pipeline combining retinal imaging, socioeconomic factors, and\ncomorbidity profiles to improve DR staging accuracy, integrated with a clinical\ndeferral system for a clinical human-in-the-loop implementation. We experiment\nwith three multimodal fusion methods and identify fusion through a fully\nconnected layer as the most versatile methodology. We synthesize adversarial,\nlow-quality images and use contrastive learning to train the deferral system,\nguiding the model to identify out-of-distribution samples that warrant\nclinician review. By maintaining diagnostic accuracy on suboptimal images and\nintegrating critical health data, our system can improve early detection,\nparticularly in underserved populations where advanced DR is often first\nidentified. This approach may reduce healthcare costs, increase early detection\nrates, and address disparities in access to care, promoting healthcare equity.", "AI": {"tldr": "MultiRetNet improves DR staging accuracy by combining retinal imaging, socioeconomic factors, and comorbidity profiles, integrated with a clinical deferral system.", "motivation": "Diabetic retinopathy (DR) is a leading cause of preventable blindness, affecting over 100 million people worldwide. In the United States, individuals from lower-income communities face a higher risk of progressing to advanced stages before diagnosis, largely due to limited access to screening. Comorbid conditions further accelerate disease progression.", "method": "MultiRetNet, a novel pipeline combining retinal imaging, socioeconomic factors, and comorbidity profiles to improve DR staging accuracy, integrated with a clinical deferral system for a clinical human-in-the-loop implementation. Experiment with three multimodal fusion methods and identify fusion through a fully connected layer as the most versatile methodology. Synthesize adversarial, low-quality images and use contrastive learning to train the deferral system.", "result": "By maintaining diagnostic accuracy on suboptimal images and integrating critical health data, the system can improve early detection, particularly in underserved populations where advanced DR is often first identified.", "conclusion": "The system can improve early detection, reduce healthcare costs, increase early detection rates, and address disparities in access to care, promoting healthcare equity."}}
{"id": "2507.14698", "categories": ["cs.LG", "cs.AI", "cs.HC", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14698", "abs": "https://arxiv.org/abs/2507.14698", "authors": ["Xuetao Lin", "Tianhao Peng", "Peihong Dai", "Yu Liang", "Wenjun Wu"], "title": "Spatial-Temporal Transformer with Curriculum Learning for EEG-Based Emotion Recognition", "comment": null, "summary": "EEG-based emotion recognition plays an important role in developing adaptive\nbrain-computer communication systems, yet faces two fundamental challenges in\npractical implementations: (1) effective integration of non-stationary\nspatial-temporal neural patterns, (2) robust adaptation to dynamic emotional\nintensity variations in real-world scenarios. This paper proposes SST-CL, a\nnovel framework integrating spatial-temporal transformers with curriculum\nlearning. Our method introduces two core components: a spatial encoder that\nmodels inter-channel relationships and a temporal encoder that captures\nmulti-scale dependencies through windowed attention mechanisms, enabling\nsimultaneous extraction of spatial correlations and temporal dynamics from EEG\nsignals. Complementing this architecture, an intensity-aware curriculum\nlearning strategy progressively guides training from high-intensity to\nlow-intensity emotional states through dynamic sample scheduling based on a\ndual difficulty assessment. Comprehensive experiments on three benchmark\ndatasets demonstrate state-of-the-art performance across various emotional\nintensity levels, with ablation studies confirming the necessity of both\narchitectural components and the curriculum learning mechanism.", "AI": {"tldr": "\u63d0\u51faSST-CL\uff0c\u4e00\u4e2a\u96c6\u6210\u4e86\u7a7a\u95f4-\u65f6\u95f4Transformer\u4e0e\u8bfe\u7a0b\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8111\u7535\u60c5\u7eea\u8bc6\u522b\uff0c\u5e76\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u57fa\u4e8e\u8111\u7535\u7684\u60c5\u7eea\u8bc6\u522b\u5728\u5f00\u53d1\u81ea\u9002\u5e94\u8111-\u8ba1\u7b97\u673a\u901a\u4fe1\u7cfb\u7edf\u4e2d\u8d77\u7740\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9762\u4e34\u4e24\u4e2a\u6839\u672c\u6311\u6218\uff1a(1)\u6709\u6548\u6574\u5408\u975e\u5e73\u7a33\u7a7a\u95f4-\u65f6\u95f4\u795e\u7ecf\u6a21\u5f0f\uff0c(2)\u7a33\u5065\u9002\u5e94\u73b0\u5b9e\u573a\u666f\u4e2d\u52a8\u6001\u7684\u60c5\u7eea\u5f3a\u5ea6\u53d8\u5316\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6SST-CL\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u7a7a\u95f4-\u65f6\u95f4Transformer\u4e0e\u8bfe\u7a0b\u5b66\u4e60\u3002\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u4e00\u4e2a\u7a7a\u95f4\u7f16\u7801\u5668\uff0c\u7528\u4e8e\u5efa\u6a21\u901a\u9053\u95f4\u5173\u7cfb\uff1b\u4e00\u4e2a\u65f6\u95f4\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u7a97\u53e3\u6ce8\u610f\u529b\u673a\u5236\u6355\u83b7\u591a\u5c3a\u5ea6\u4f9d\u8d56\u5173\u7cfb\uff0c\u4ece\u800c\u80fd\u591f\u540c\u65f6\u4ece\u8111\u7535\u4fe1\u53f7\u4e2d\u63d0\u53d6\u7a7a\u95f4\u76f8\u5173\u6027\u548c\u65f6\u95f4\u52a8\u6001\u3002", "result": "\u5728\u5404\u79cd\u60c5\u7eea\u5f3a\u5ea6\u6c34\u5e73\u4e0a\u90fd\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u67b6\u6784\u7ec4\u4ef6\u548c\u8bfe\u7a0b\u5b66\u4e60\u673a\u5236\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u60c5\u7eea\u5f3a\u5ea6\u6c34\u5e73\u4e0a\u90fd\u5177\u6709\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u67b6\u6784\u7ec4\u4ef6\u548c\u8bfe\u7a0b\u5b66\u4e60\u673a\u5236\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2507.15457", "categories": ["cs.AI", "I.2.8"], "pdf": "https://arxiv.org/pdf/2507.15457", "abs": "https://arxiv.org/abs/2507.15457", "authors": ["Orlenys L\u00f3pez-Pintado", "Jannis Rosenbaum", "Marlon Dumas"], "title": "Optimization of Activity Batching Policies in Business Processes", "comment": null, "summary": "In business processes, activity batching refers to packing multiple activity\ninstances for joint execution. Batching allows managers to trade off cost and\nprocessing effort against waiting time. Larger and less frequent batches may\nlower costs by reducing processing effort and amortizing fixed costs, but they\ncreate longer waiting times. In contrast, smaller and more frequent batches\nreduce waiting times but increase fixed costs and processing effort. A batching\npolicy defines how activity instances are grouped into batches and when each\nbatch is activated. This paper addresses the problem of discovering batching\npolicies that strike optimal trade-offs between waiting time, processing\neffort, and cost. The paper proposes a Pareto optimization approach that starts\nfrom a given set (possibly empty) of activity batching policies and generates\nalternative policies for each batched activity via intervention heuristics.\nEach heuristic identifies an opportunity to improve an activity's batching\npolicy with respect to a metric (waiting time, processing time, cost, or\nresource utilization) and an associated adjustment to the activity's batching\npolicy (the intervention). The impact of each intervention is evaluated via\nsimulation. The intervention heuristics are embedded in an optimization\nmeta-heuristic that triggers interventions to iteratively update the Pareto\nfront of the interventions identified so far. The paper considers three\nmeta-heuristics: hill-climbing, simulated annealing, and reinforcement\nlearning. An experimental evaluation compares the proposed approach based on\nintervention heuristics against the same (non-heuristic guided) meta-heuristics\nbaseline regarding convergence, diversity, and cycle time gain of\nPareto-optimal policies.", "AI": {"tldr": "This paper introduces a Pareto optimization method using intervention heuristics to find the best activity batching policies, balancing waiting time, effort, and cost, and compares it with other methods.", "motivation": "The motivation is to find optimal trade-offs between waiting time, processing effort, and cost in activity batching by discovering effective batching policies.", "method": "The paper uses a Pareto optimization approach with intervention heuristics, embedded in hill-climbing, simulated annealing, and reinforcement learning meta-heuristics. The impact of each intervention is evaluated via simulation.", "result": "The experimental evaluation compares the proposed approach against non-heuristic guided meta-heuristics regarding convergence, diversity, and cycle time gain of Pareto-optimal policies.", "conclusion": "This paper proposes a Pareto optimization approach with intervention heuristics to discover activity batching policies that balance waiting time, processing effort, and cost. The approach is evaluated against non-heuristic guided meta-heuristics."}}
{"id": "2507.15024", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15024", "abs": "https://arxiv.org/abs/2507.15024", "authors": ["Qiaoyu Tang", "Hao Xiang", "Le Yu", "Bowen Yu", "Hongyu Lin", "Yaojie Lu", "Xianpei Han", "Le Sun", "Junyang Lin"], "title": "RefCritic: Training Long Chain-of-Thought Critic Models with Refinement Feedback", "comment": null, "summary": "With the rapid advancement of Large Language Models (LLMs), developing\neffective critic modules for precise guidance has become crucial yet\nchallenging. In this paper, we initially demonstrate that supervised\nfine-tuning for building critic modules (which is widely adopted in current\nsolutions) fails to genuinely enhance models' critique abilities, producing\nsuperficial critiques with insufficient reflections and verifications. To\nunlock the unprecedented critique capabilities, we propose RefCritic, a\nlong-chain-of-thought critic module based on reinforcement learning with dual\nrule-based rewards: (1) instance-level correctness of solution judgments and\n(2) refinement accuracies of the policy model based on critiques, aiming to\ngenerate high-quality evaluations with actionable feedback that effectively\nguides model refinement. We evaluate RefCritic on Qwen2.5-14B-Instruct and\nDeepSeek-R1-Distill-Qwen-14B across five benchmarks. On critique and refinement\nsettings, RefCritic demonstrates consistent advantages across all benchmarks,\ne.g., 6.8\\% and 7.2\\% gains on AIME25 for the respective base models. Notably,\nunder majority voting, policy models filtered by RefCritic show superior\nscaling with increased voting numbers. Moreover, despite training on\nsolution-level supervision, RefCritic outperforms step-level supervised\napproaches on ProcessBench, a benchmark to identify erroneous steps in\nmathematical reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86RefCritic\uff0c\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u957f\u94fe\u601d\u7ef4\u8bc4\u8bba\u6a21\u5757\uff0c\u7528\u4e8e\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u8bba\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5f00\u53d1\u7528\u4e8e\u7cbe\u786e\u6307\u5bfc\u7684\u6709\u6548\u8bc4\u8bba\u6a21\u5757\u5df2\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4e5f\u5177\u6709\u6311\u6218\u6027\u3002\u672c\u6587\u521d\u6b65\u8bc1\u660e\uff0c\u7528\u4e8e\u6784\u5efa\u8bc4\u8bba\u6a21\u5757\u7684\u76d1\u7763\u5fae\u8c03\uff08\u5df2\u88ab\u5f53\u524d\u89e3\u51b3\u65b9\u6848\u5e7f\u6cdb\u91c7\u7528\uff09\u672a\u80fd\u771f\u6b63\u63d0\u9ad8\u6a21\u578b\u7684\u8bc4\u8bba\u80fd\u529b\uff0c\u4ece\u800c\u4ea7\u751f\u80a4\u6d45\u7684\u8bc4\u8bba\uff0c\u4e14\u53cd\u601d\u548c\u9a8c\u8bc1\u4e0d\u8db3\u3002", "method": "RefCritic\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u957f\u94fe\u601d\u7ef4\u8bc4\u8bba\u6a21\u5757\uff0c\u5177\u6709\u53cc\u91cd\u57fa\u4e8e\u89c4\u5219\u7684\u5956\u52b1\uff1a(1)\u89e3\u51b3\u65b9\u6848\u5224\u65ad\u7684\u5b9e\u4f8b\u7ea7\u522b\u6b63\u786e\u6027\uff1b(2)\u57fa\u4e8e\u8bc4\u8bba\u7684\u7b56\u7565\u6a21\u578b\u7684\u6539\u8fdb\u51c6\u786e\u6027\uff0c\u65e8\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u8bc4\u4f30\uff0c\u5e76\u63d0\u4f9b\u6709\u6548\u7684\u53cd\u9988\uff0c\u4ece\u800c\u6709\u6548\u5730\u6307\u5bfc\u6a21\u578b\u6539\u8fdb\u3002", "result": "RefCritic\u5728Qwen2.5-14B-Instruct\u548cDeepSeek-R1-Distill-Qwen-14B\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u4e00\u81f4\u7684\u4f18\u52bf\u3002\u5728\u8bc4\u8bba\u548c\u6539\u8fdb\u8bbe\u7f6e\u4e2d\uff0cRefCritic\u5c55\u793a\u4e86\u4e00\u81f4\u7684\u4f18\u52bf\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5728\u591a\u6570\u6295\u7968\u4e0b\uff0c\u7ecf\u8fc7RefCritic\u8fc7\u6ee4\u7684\u7b56\u7565\u6a21\u578b\u663e\u793a\u51fa\u968f\u7740\u6295\u7968\u6570\u91cf\u589e\u52a0\u7684\u5353\u8d8a\u6269\u5c55\u6027\u3002\u6b64\u5916\uff0c\u5c3d\u7ba1\u5728\u89e3\u51b3\u65b9\u6848\u7ea7\u522b\u7684\u76d1\u7763\u4e0b\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f46RefCritic\u5728ProcessBench\u4e0a\u4f18\u4e8e\u6b65\u9aa4\u7ea7\u522b\u7684\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "RefCritic\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u4e00\u81f4\u7684\u4f18\u52bf\uff0c\u4f8b\u5982\uff0c\u5728AIME25\u4e0a\uff0c\u76f8\u5e94\u7684\u57fa\u672c\u6a21\u578b\u5206\u522b\u83b7\u5f97\u4e866.8%\u548c7.2%\u7684\u6536\u76ca\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5728\u591a\u6570\u6295\u7968\u4e0b\uff0c\u7ecf\u8fc7RefCritic\u8fc7\u6ee4\u7684\u7b56\u7565\u6a21\u578b\u663e\u793a\u51fa\u968f\u7740\u6295\u7968\u6570\u91cf\u589e\u52a0\u7684\u5353\u8d8a\u6269\u5c55\u6027\u3002\u6b64\u5916\uff0c\u5c3d\u7ba1\u5728\u89e3\u51b3\u65b9\u6848\u7ea7\u522b\u7684\u76d1\u7763\u4e0b\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f46RefCritic\u5728ProcessBench\u4e0a\u4f18\u4e8e\u6b65\u9aa4\u7ea7\u522b\u7684\u76d1\u7763\u65b9\u6cd5\uff0cProcessBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc6\u522b\u6570\u5b66\u63a8\u7406\u4e2d\u9519\u8bef\u6b65\u9aa4\u7684\u57fa\u51c6\u3002"}}
{"id": "2507.14743", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14743", "abs": "https://arxiv.org/abs/2507.14743", "authors": ["Joseph Raj Vishal", "Rutuja Patil", "Manas Srinivas Gowda", "Katha Naik", "Yezhou Yang", "Bharatesh Chakravarthi"], "title": "InterAct-Video: Reasoning-Rich Video QA for Urban Traffic", "comment": null, "summary": "Traffic monitoring is crucial for urban mobility, road safety, and\nintelligent transportation systems (ITS). Deep learning has advanced\nvideo-based traffic monitoring through video question answering (VideoQA)\nmodels, enabling structured insight extraction from traffic videos. However,\nexisting VideoQA models struggle with the complexity of real-world traffic\nscenes, where multiple concurrent events unfold across spatiotemporal\ndimensions. To address these challenges, this paper introduces \\textbf{InterAct\nVideoQA}, a curated dataset designed to benchmark and enhance VideoQA models\nfor traffic monitoring tasks. The InterAct VideoQA dataset comprises 8 hours of\nreal-world traffic footage collected from diverse intersections, segmented into\n10-second video clips, with over 25,000 question-answer (QA) pairs covering\nspatiotemporal dynamics, vehicle interactions, incident detection, and other\ncritical traffic attributes. State-of-the-art VideoQA models are evaluated on\nInterAct VideoQA, exposing challenges in reasoning over fine-grained\nspatiotemporal dependencies within complex traffic scenarios. Additionally,\nfine-tuning these models on InterAct VideoQA yields notable performance\nimprovements, demonstrating the necessity of domain-specific datasets for\nVideoQA. InterAct VideoQA is publicly available as a benchmark dataset to\nfacilitate future research in real-world deployable VideoQA models for\nintelligent transportation systems. GitHub Repo:\nhttps://github.com/joe-rabbit/InterAct_VideoQA", "AI": {"tldr": "This paper introduces InterAct VideoQA, a new dataset for video question answering in traffic monitoring. It addresses the limitations of existing models in handling complex real-world traffic scenarios.", "motivation": "Existing VideoQA models struggle with the complexity of real-world traffic scenes, where multiple concurrent events unfold across spatiotemporal dimensions.", "method": "This paper introduces InterAct VideoQA, a curated dataset designed to benchmark and enhance VideoQA models for traffic monitoring tasks.", "result": "State-of-the-art VideoQA models are evaluated on InterAct VideoQA, exposing challenges in reasoning over fine-grained spatiotemporal dependencies within complex traffic scenarios. Additionally, fine-tuning these models on InterAct VideoQA yields notable performance improvements, demonstrating the necessity of domain-specific datasets for VideoQA.", "conclusion": "InterAct VideoQA is publicly available as a benchmark dataset to facilitate future research in real-world deployable VideoQA models for intelligent transportation systems."}}
{"id": "2507.14706", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14706", "abs": "https://arxiv.org/abs/2507.14706", "authors": ["Claudio Giusti", "Luca Guarnera", "Mirko Casu", "Sebastiano Battiato"], "title": "Fraud is Not Just Rarity: A Causal Prototype Attention Approach to Realistic Synthetic Oversampling", "comment": "23 pages, 14 figures", "summary": "Detecting fraudulent credit card transactions remains a significant\nchallenge, due to the extreme class imbalance in real-world data and the often\nsubtle patterns that separate fraud from legitimate activity. Existing research\ncommonly attempts to address this by generating synthetic samples for the\nminority class using approaches such as GANs, VAEs, or hybrid generative\nmodels. However, these techniques, particularly when applied only to\nminority-class data, tend to result in overconfident classifiers and poor\nlatent cluster separation, ultimately limiting real-world detection\nperformance. In this study, we propose the Causal Prototype Attention\nClassifier (CPAC), an interpretable architecture that promotes class-aware\nclustering and improved latent space structure through prototype-based\nattention mechanisms and we will couple it with the encoder in a VAE-GAN\nallowing it to offer a better cluster separation moving beyond post-hoc sample\naugmentation. We compared CPAC-augmented models to traditional oversamplers,\nsuch as SMOTE, as well as to state-of-the-art generative models, both with and\nwithout CPAC-based latent classifiers. Our results show that classifier-guided\nlatent shaping with CPAC delivers superior performance, achieving an F1-score\nof 93.14\\% percent and recall of 90.18\\%, along with improved latent cluster\nseparation. Further ablation studies and visualizations provide deeper insight\ninto the benefits and limitations of classifier-driven representation learning\nfor fraud detection. The codebase for this work will be available at final\nsubmission.", "AI": {"tldr": "This paper introduces Causal Prototype Attention Classifier (CPAC) to address the challenge of detecting fraudulent credit card transactions. CPAC improves latent space structure through prototype-based attention mechanisms and achieves superior performance.", "motivation": "Detecting fraudulent credit card transactions remains a significant challenge, due to the extreme class imbalance in real-world data and the often subtle patterns that separate fraud from legitimate activity. Existing research commonly attempts to address this by generating synthetic samples for the minority class using approaches such as GANs, VAEs, or hybrid generative models. However, these techniques, particularly when applied only to minority-class data, tend to result in overconfident classifiers and poor latent cluster separation, ultimately limiting real-world detection performance.", "method": "Causal Prototype Attention Classifier (CPAC), an interpretable architecture that promotes class-aware clustering and improved latent space structure through prototype-based attention mechanisms and couple it with the encoder in a VAE-GAN", "result": "CPAC delivers superior performance, achieving an F1-score of 93.14% percent and recall of 90.18%, along with improved latent cluster separation", "conclusion": "classifier-guided latent shaping with CPAC delivers superior performance, achieving an F1-score of 93.14% percent and recall of 90.18%, along with improved latent cluster separation"}}
{"id": "2507.15509", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15509", "abs": "https://arxiv.org/abs/2507.15509", "authors": ["Lei Chen", "Xuanle Zhao", "Zhixiong Zeng", "Jing Huang", "Yufeng Zhong", "Lin Ma"], "title": "Chart-R1: Chain-of-Thought Supervision and Reinforcement for Advanced Chart Reasoner", "comment": "technical report", "summary": "Recently, inspired by OpenAI-o1/o3 and Deepseek-R1, the R1-Style method based\non reinforcement learning fine-tuning has received widespread attention from\nthe community. Previous R1-Style methods mainly focus on mathematical reasoning\nand code intelligence. It is of great research significance to verify their\nadvantages on more general multimodal data. Chart is an important multimodal\ndata type with rich information, which brings important research challenges in\ncomplex reasoning. In this work, we introduce Chart-R1, a chart-domain\nvision-language model with reinforcement learning fine-tuning to enable complex\nchart reasoning. To support Chart-R1, we first propose a novel programmatic\ndata synthesis technology to generate high-quality step-by-step chart reasoning\ndata covering single- and multi-subcharts, which makes up for the lack of\nreasoning data in the chart domain. Then we develop a two-stage training\nstrategy: Chart-COT with step-by-step chain-of-thought supervision, and\nChart-RFT with numerically sensitive reinforcement fine-tuning. Chart-COT aims\nto decompose complex chart reasoning tasks into fine-grained, understandable\nsubtasks through step-by-step supervision, which lays a good foundation for\nimproving the reasoning level of reinforcement learning. Chart-RFT utilize the\ntypical group relative policy optimization strategy, in which a relatively soft\nreward is adopted for numerical response to emphasize the numerical sensitivity\nin the chart domain. We conduct extensive experiments on open-source benchmarks\nand self-built chart reasoning dataset (\\emph{i.e., ChartRQA}). Experimental\nresults show that Chart-R1 has significant advantages compared to chart-domain\nmethods, even comparable to open/closed source large-scale models (\\emph{e.g.,\nGPT-4o, Claude-3.5}).", "AI": {"tldr": "Chart-R1, a chart-domain vision-language model with reinforcement learning, enables complex chart reasoning through a novel data synthesis technology and a two-stage training strategy.", "motivation": "Verifying the advantages of R1-Style methods on more general multimodal data, specifically complex chart reasoning, which lacks reasoning data.", "method": "A two-stage training strategy: Chart-COT with step-by-step chain-of-thought supervision, and Chart-RFT with numerically sensitive reinforcement fine-tuning.", "result": "Chart-R1 demonstrates significant advantages compared to chart-domain methods and is even comparable to large-scale models.", "conclusion": "Chart-R1 exhibits significant advantages over chart-domain methods and is even comparable to large-scale models like GPT-4o and Claude-3.5."}}
{"id": "2507.15061", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15061", "abs": "https://arxiv.org/abs/2507.15061", "authors": ["Zhengwei Tao", "Jialong Wu", "Wenbiao Yin", "Junkai Zhang", "Baixuan Li", "Haiyang Shen", "Kuan Li", "Liwen Zhang", "Xinyu Wang", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "title": "WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization", "comment": null, "summary": "The advent of Large Language Model (LLM)-powered agents has revolutionized\nartificial intelligence by enabling solutions to complex, open-ended tasks\nthrough web-based information-seeking (IS) capabilities. The scarcity of\nhigh-quality training data has limited the development of IS agents. Existing\napproaches typically adopt an information-driven paradigm that first collects\nweb data and then generates questions based on the retrieval. However, this may\nlead to inconsistency between information structure and reasoning structure,\nquestion and answer. To mitigate, we propose a formalization-driven IS data\nsynthesis framework WebShaper to construct a dataset. WebShaper systematically\nformalizes IS tasks through set theory. Central to the formalization is the\nconcept of Knowledge Projections (KP), which enables precise control over\nreasoning structure by KP operation compositions. During synthesis, we begin by\ncreating seed tasks, then use a multi-step expansion process. At each step, an\nagentic Expander expands the current formal question more complex with\nretrieval and validation tools based on our formalization. We train our model\non the synthesized dataset. Experiment results demonstrate that WebShaper\nachieves state-of-the-art performance among open-sourced IS agents on GAIA and\nWebWalkerQA benchmarks.", "AI": {"tldr": "WebShaper \u63d0\u51fa\u4e86\u4e00\u4e2a\u5f62\u5f0f\u5316\u9a71\u52a8\u7684 IS \u6570\u636e\u5408\u6210\u6846\u67b6\uff0c\u4ee5\u6784\u5efa\u6570\u636e\u96c6\uff0c\u4ece\u800c\u5728\u5f00\u653e\u57df\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u7a00\u7f3a\u6027\u9650\u5236\u4e86IS\u4ee3\u7406\u7684\u5f00\u53d1\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u4fe1\u606f\u9a71\u52a8\u8303\u4f8b\uff0c\u8be5\u8303\u4f8b\u9996\u5148\u6536\u96c6Web\u6570\u636e\uff0c\u7136\u540e\u6839\u636e\u68c0\u7d22\u751f\u6210\u95ee\u9898\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u4fe1\u606f\u7ed3\u6784\u548c\u63a8\u7406\u7ed3\u6784\u3001\u95ee\u9898\u548c\u7b54\u6848\u4e4b\u95f4\u4e0d\u4e00\u81f4\u3002", "method": "WebShaper\u901a\u8fc7\u96c6\u5408\u8bba\u7cfb\u7edf\u5730\u5f62\u5f0f\u5316IS\u4efb\u52a1\uff0c\u5e76\u4f7f\u7528Knowledge Projections (KP)\u7cbe\u786e\u63a7\u5236\u63a8\u7406\u7ed3\u6784\u3002", "result": "WebShaper\u5728GAIA\u548cWebWalkerQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u5f00\u6e90IS\u4ee3\u7406\u4e2d\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "WebShaper\u5728GAIA\u548cWebWalkerQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2507.14784", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14784", "abs": "https://arxiv.org/abs/2507.14784", "authors": ["Xinxin Dong", "Baoyun Peng", "Haokai Ma", "Yufei Wang", "Zixuan Dong", "Fei Hu", "Xiaodong Wang"], "title": "LeAdQA: LLM-Driven Context-Aware Temporal Grounding for Video Question Answering", "comment": null, "summary": "Video Question Answering (VideoQA) requires identifying sparse critical\nmoments in long videos and reasoning about their causal relationships to answer\nsemantically complex questions. While recent advances in multimodal learning\nhave improved alignment and fusion, current approaches remain limited by two\nprevalent but fundamentally flawed strategies: (1) task-agnostic sampling\nindiscriminately processes all frames, overwhelming key events with irrelevant\ncontent; and (2) heuristic retrieval captures superficial patterns but misses\ncausal-temporal structures needed for complex reasoning. To address these\nchallenges, we introduce LeAdQA, an innovative approach that bridges these gaps\nthrough synergizing causal-aware query refinement with fine-grained visual\ngrounding. Our method first leverages LLMs to reformulate question-option\npairs, resolving causal ambiguities and sharpening temporal focus. These\nrefined queries subsequently direct a temporal grounding model to precisely\nretrieve the most salient segments, complemented by an adaptive fusion\nmechanism dynamically integrating the evidence to maximize relevance. The\nintegrated visual-textual cues are then processed by an MLLM to generate\naccurate, contextually-grounded answers. Experiments on NExT-QA, IntentQA, and\nNExT-GQA demonstrate that our method's precise visual grounding substantially\nenhances the understanding of video-question relationships, achieving\nstate-of-the-art (SOTA) performance on complex reasoning tasks while\nmaintaining computational efficiency.", "AI": {"tldr": "LeAdQA\u901a\u8fc7\u7ed3\u5408\u56e0\u679c\u611f\u77e5\u67e5\u8be2\u7ec6\u5316\u548c\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5b9a\u4f4d\u6765\u89e3\u51b3\u89c6\u9891\u95ee\u7b54\u4e2d\u7684\u6311\u6218\uff0c\u4ece\u800c\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u95ee\u7b54\u65b9\u6cd5\u53d7\u9650\u4e8e\u4e24\u79cd\u6709\u7f3a\u9677\u7684\u7b56\u7565\uff1a1) \u4e0d\u52a0\u533a\u5206\u5730\u5904\u7406\u6240\u6709\u5e27\uff0c\u4f7f\u5173\u952e\u4e8b\u4ef6\u88ab\u65e0\u5173\u5185\u5bb9\u6df9\u6ca1\uff1b2) \u542f\u53d1\u5f0f\u68c0\u7d22\u6355\u83b7\u8868\u9762\u6a21\u5f0f\uff0c\u4f46\u9519\u5931\u590d\u6742\u63a8\u7406\u6240\u9700\u7684\u56e0\u679c\u65f6\u95f4\u7ed3\u6784\u3002", "method": "\u5229\u7528LLM\u6539\u8fdb\u95ee\u9898\u9009\u9879\u5bf9\uff0c\u89e3\u51b3\u56e0\u679c\u6a21\u7cca\u6027\u5e76\u52a0\u5f3a\u65f6\u95f4\u7126\u70b9\u3002\u7136\u540e\uff0c\u8fd9\u4e9b\u6539\u8fdb\u7684\u67e5\u8be2\u6307\u5bfc\u65f6\u95f4\u5b9a\u4f4d\u6a21\u578b\u7cbe\u786e\u68c0\u7d22\u6700\u76f8\u5173\u7684\u7247\u6bb5\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u878d\u5408\u673a\u5236\u52a8\u6001\u6574\u5408\u8bc1\u636e\u4ee5\u6700\u5927\u5316\u76f8\u5173\u6027\u3002", "result": "\u5728NExT-QA\u3001IntentQA\u548cNExT-GQA\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLeAdQA\u7684\u7cbe\u786e\u89c6\u89c9\u5b9a\u4f4d\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u89c6\u9891-\u95ee\u9898\u5173\u7cfb\u7684\u7406\u89e3\u3002", "conclusion": "LeAdQA\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2507.14715", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14715", "abs": "https://arxiv.org/abs/2507.14715", "authors": ["Rachid Karami", "Rajeev Patwari", "Hyoukjun Kwon", "Ashish Sirasao"], "title": "Exploring the Dynamic Scheduling Space of Real-Time Generative AI Applications on Emerging Heterogeneous Systems", "comment": null, "summary": "The integration of generative AI models, particularly large language models\n(LLMs), into real-time multi-model AI applications such as video conferencing\nand gaming is giving rise to a new class of workloads: real-time generative AI\n(RTGen). These workloads combine the compute intensity and dynamic execution\npatterns of generative models with the stringent latency and concurrency\nconstraints of real-time inference. To meet the diverse demands of RTGen\nworkloads, modern edge platforms increasingly adopt heterogeneous\nsystem-on-chip (SoC) architectures that integrate CPUs, GPUs, and NPUs. Despite\nthe potential of heterogeneous SoC, the scheduling space complexity and\nperformance implications of RTGen workloads on such platforms remain\nunderexplored. In this work, we perform a comprehensive characterization of\nRTGen workloads on AMD's latest heterogeneous SoC, Ryzen AI. We construct\nrealistic multi-model scenarios inspired by industry use cases and profile\nmodel performance across all available backends. Using this data, we evaluate\nfive scheduling policies and their impact on both real-time metrics (e.g.,\ndeadline violation rate) and LLM performance (e.g., time-to-first-token and\ntokens-per-second). Our results show that scheduling decisions significantly\naffect workload performance (e.g., leading to a 41.7% difference in deadline\nviolation rates on average), and highlight the need for scheduling strategies\nthat are aware of workload dynamics and hardware heterogeneity. Our findings\nunderscore the importance of workload-aware, dynamic heterogeneous scheduling\nin enabling high-performance, on-device RTGen applications.", "AI": {"tldr": "RTGen workloads on heterogeneous SoCs require workload-aware, dynamic scheduling for high performance.", "motivation": "The increasing integration of generative AI models into real-time multi-model AI applications creates a new class of workloads (RTGen) with stringent latency and concurrency constraints, making the scheduling space complexity and performance implications on heterogeneous SoC platforms underexplored.", "method": "Comprehensive characterization of RTGen workloads on AMD's Ryzen AI heterogeneous SoC, evaluating five scheduling policies and their impact on real-time metrics and LLM performance.", "result": "Scheduling decisions significantly affect workload performance, leading to a 41.7% difference in deadline violation rates on average, highlighting the need for scheduling strategies aware of workload dynamics and hardware heterogeneity.", "conclusion": "Workload-aware, dynamic heterogeneous scheduling is crucial for enabling high-performance, on-device RTGen applications."}}
{"id": "2507.15518", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.15518", "abs": "https://arxiv.org/abs/2507.15518", "authors": ["Sizhou Chen", "Shufan Jiang", "Chi Zhang", "Xiao-Lei Zhang", "Xuelong Li"], "title": "HAMLET: Hyperadaptive Agent-based Modeling for Live Embodied Theatrics", "comment": null, "summary": "Creating an immersive and interactive theatrical experience is a long-term\ngoal in the field of interactive narrative. The emergence of large language\nmodel (LLM) is providing a new path to achieve this goal. However, existing\nLLM-based drama generation methods often result in AI agents that lack\ninitiative and cannot interact with the physical environment. Furthermore,\nthese methods typically require detailed user input to drive the drama. These\nlimitations reduce the interactivity and immersion of online real-time\nperformance. To address the above challenges, we propose HAMLET, a multi-agent\nframework focused on drama creation and online performance. Given a simple\ntopic, the framework generates a narrative blueprint, guiding the subsequent\nimprovisational performance. During the online performance, each actor is given\nan autonomous mind. This means that actors can make independent decisions based\non their own background, goals, and emotional state. In addition to\nconversations with other actors, their decisions can also change the state of\nscene props through actions such as opening a letter or picking up a weapon.\nThe change is then broadcast to other related actors, updating what they know\nand care about, which in turn influences their next action. To evaluate the\nquality of drama performance, we designed an evaluation method to assess three\nprimary aspects, including character performance, narrative quality, and\ninteraction experience. The experimental evaluation shows that HAMLET can\ncreate expressive and coherent theatrical experiences. Our code, dataset and\nmodels are available at https://github.com/HAMLET-2025/HAMLET.", "AI": {"tldr": "HAMLET, a multi-agent framework, is proposed to create drama and online performance. It enables actors to make independent decisions and interact with the physical environment, and can create expressive and coherent theatrical experiences.", "motivation": "Existing LLM-based drama generation methods often result in AI agents that lack initiative and cannot interact with the physical environment, and typically require detailed user input to drive the drama, which reduce the interactivity and immersion of online real-time performance.", "method": "a multi-agent framework focused on drama creation and online performance", "result": "an evaluation method to assess three primary aspects, including character performance, narrative quality, and interaction experience", "conclusion": "HAMLET can create expressive and coherent theatrical experiences."}}
{"id": "2507.15087", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15087", "abs": "https://arxiv.org/abs/2507.15087", "authors": ["Chenlei Gong", "Yuanhe Tian", "Lei Mao", "Yan Song"], "title": "Evaluation of Coding Schemes for Transformer-based Gene Sequence Modeling", "comment": null, "summary": "Currently, many studies view DNA sequences as a special type of language and\nutilize Transformers to model them. These studies use fixed-length k-mer\nsegmentation and BPE subword tokenization but lack a systematic evaluation to\ndetermine which is superior. We compare k-mer segmentation with k=1,3,4,5,6, a\n4,096-token BPE vocabulary, and three positional encoding methods-sinusoidal,\nAliBi, and RoPE. Each configuration is trained from scratch in 3, 6, 12, and\n24-layer Transformer encoders and evaluated on GUE benchmark dataset. In\ngeneral, BPE delivers higher and more stable performance across tasks by\ncompressing frequent motifs into variable-length tokens, reducing sequence\nlength, and improving model generalization. RoPE excels at capturing periodic\nmotifs and extrapolating to long sequences, while AliBi also performs well on\ntasks driven by local dependencies. In terms of depth, we observe significant\ngains when increasing layers from 3 to 12, with only marginal improvements or\nslight overfitting at 24 layers. This study provides practical guidance for\ndesigning tokenization and positional encoding in DNA Transformer models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86DNA Transformer\u6a21\u578b\u4e2d\u7684tokenization\u548c\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\uff0c\u53d1\u73b0BPE\u8868\u73b0\u6700\u597d\uff0cRoPE\u64c5\u957f\u6355\u6349\u5468\u671f\u6027motif\uff0c\u589e\u52a0\u6df1\u5ea6\u523012\u5c42\u53ef\u4ee5\u83b7\u5f97\u663e\u8457\u6536\u76ca\u3002", "motivation": "\u76ee\u524d\uff0c\u8bb8\u591a\u7814\u7a76\u5c06DNA\u5e8f\u5217\u89c6\u4e3a\u4e00\u79cd\u7279\u6b8a\u7c7b\u578b\u7684\u8bed\u8a00\uff0c\u5e76\u5229\u7528Transformer\u5bf9\u5176\u8fdb\u884c\u5efa\u6a21\u3002\u8fd9\u4e9b\u7814\u7a76\u4f7f\u7528\u56fa\u5b9a\u957f\u5ea6\u7684k-mer\u5206\u5272\u548cBPE\u5b50\u8bcdtokenization\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u7684\u8bc4\u4f30\u6765\u786e\u5b9a\u54ea\u79cd\u65b9\u6cd5\u66f4\u4f18\u8d8a\u3002", "method": "\u6bd4\u8f83k-mer\u5206\u5272\uff08k=1,3,4,5,6\uff09\u30014,096-token BPE\u8bcd\u6c47\u548c\u4e09\u79cd\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\u2014\u2014\u6b63\u5f26\u3001AliBi\u548cRoPE\u3002\u6bcf\u4e2a\u914d\u7f6e\u90fd\u57283\u5c42\u30016\u5c42\u300112\u5c42\u548c24\u5c42Transformer\u7f16\u7801\u5668\u4e2d\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\uff0c\u5e76\u5728GUE\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "BPE\u5728\u5404\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u548c\u66f4\u7a33\u5b9a\u7684\u6027\u80fd\u3002RoPE\u64c5\u957f\u6355\u6349\u5468\u671f\u6027motif\u548c\u63a8\u65ad\u957f\u5e8f\u5217\uff0c\u800cAliBi\u5728\u7531\u5c40\u90e8\u4f9d\u8d56\u9a71\u52a8\u7684\u4efb\u52a1\u4e2d\u4e5f\u8868\u73b0\u826f\u597d\u3002\u5f53\u5c42\u6570\u4ece3\u5c42\u589e\u52a0\u523012\u5c42\u65f6\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u663e\u8457\u7684\u589e\u76ca\u3002", "conclusion": "BPE\u5728\u5404\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u548c\u66f4\u7a33\u5b9a\u7684\u6027\u80fd\uff0c\u56e0\u4e3a\u5b83\u5c06\u9891\u7e41\u7684motif\u538b\u7f29\u4e3a\u53ef\u53d8\u957f\u5ea6\u7684token\uff0c\u51cf\u5c11\u4e86\u5e8f\u5217\u957f\u5ea6\uff0c\u5e76\u63d0\u9ad8\u4e86\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002RoPE\u64c5\u957f\u6355\u6349\u5468\u671f\u6027motif\u548c\u63a8\u65ad\u957f\u5e8f\u5217\uff0c\u800cAliBi\u5728\u7531\u5c40\u90e8\u4f9d\u8d56\u9a71\u52a8\u7684\u4efb\u52a1\u4e2d\u4e5f\u8868\u73b0\u826f\u597d\u3002\u5728\u6df1\u5ea6\u65b9\u9762\uff0c\u5f53\u5c42\u6570\u4ece3\u5c42\u589e\u52a0\u523012\u5c42\u65f6\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u663e\u8457\u7684\u589e\u76ca\uff0c\u800c\u572824\u5c42\u65f6\u53ea\u6709\u8fb9\u9645\u6539\u8fdb\u6216\u8f7b\u5fae\u7684\u8fc7\u5ea6\u62df\u5408\u3002"}}
{"id": "2507.14787", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14787", "abs": "https://arxiv.org/abs/2507.14787", "authors": ["Xi Xiao", "Aristeidis Tsaris", "Anika Tabassum", "John Lagergren", "Larry M. York", "Tianyang Wang", "Xiao Wang"], "title": "FOCUS: Fused Observation of Channels for Unveiling Spectra", "comment": null, "summary": "Hyperspectral imaging (HSI) captures hundreds of narrow, contiguous\nwavelength bands, making it a powerful tool in biology, agriculture, and\nenvironmental monitoring. However, interpreting Vision Transformers (ViTs) in\nthis setting remains largely unexplored due to two key challenges: (1) existing\nsaliency methods struggle to capture meaningful spectral cues, often collapsing\nattention onto the class token, and (2) full-spectrum ViTs are computationally\nprohibitive for interpretability, given the high-dimensional nature of HSI\ndata. We present FOCUS, the first framework that enables reliable and efficient\nspatial-spectral interpretability for frozen ViTs. FOCUS introduces two core\ncomponents: class-specific spectral prompts that guide attention toward\nsemantically meaningful wavelength groups, and a learnable [SINK] token trained\nwith an attraction loss to absorb noisy or redundant attention. Together, these\ndesigns make it possible to generate stable and interpretable 3D saliency maps\nand spectral importance curves in a single forward pass, without any gradient\nbackpropagation or backbone modification. FOCUS improves band-level IoU by 15\npercent, reduces attention collapse by over 40 percent, and produces saliency\nresults that align closely with expert annotations. With less than 1 percent\nparameter overhead, our method makes high-resolution ViT interpretability\npractical for real-world hyperspectral applications, bridging a long-standing\ngap between black-box modeling and trustworthy HSI decision-making.", "AI": {"tldr": "FOCUS makes ViT interpretable for hyperspectral imaging by using spectral prompts and a sink token, achieving better performance and efficiency.", "motivation": "Interpreting Vision Transformers (ViTs) in hyperspectral imaging (HSI) is challenging due to difficulties in capturing meaningful spectral cues and the computational cost of full-spectrum ViTs.", "method": "Introduces class-specific spectral prompts and a learnable [SINK] token trained with an attraction loss.", "result": "Generates stable and interpretable 3D saliency maps and spectral importance curves in a single forward pass, without gradient backpropagation or backbone modification. Aligns closely with expert annotations and has less than 1 percent parameter overhead.", "conclusion": "FOCUS enables reliable and efficient spatial-spectral interpretability for frozen ViTs, improving band-level IoU by 15 percent and reducing attention collapse by over 40 percent."}}
