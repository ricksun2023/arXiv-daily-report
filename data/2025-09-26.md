<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 18]
- [cs.CV](#cs.CV) [Total: 12]
- [cs.AI](#cs.AI) [Total: 12]
- [cs.IR](#cs.IR) [Total: 9]
- [cs.LG](#cs.LG) [Total: 12]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Interpreting Public Sentiment in Diplomacy Events: A Counterfactual Analysis Framework Using Large Language Models](https://arxiv.org/abs/2509.20367)
*Leyi Ouyang*

Main category: cs.CL

TL;DR: 本文提出了一种新框架，通过修改外交事件的叙述方式来转变公众情绪，使其从负面变为中性或正面。


<details>
  <summary>Details</summary>
Motivation: 公众情绪在外交中起着关键作用，良好的公众情绪为政策实施提供重要支持，有助于解决国际问题，塑造国家形象。传统的情绪衡量方法耗时、费力，且缺乏前瞻性分析能力。

Method: 1. 训练语言模型来预测公众对外交事件的反应。2. 构建包含外交事件描述及其相关公众讨论的数据集。3. 确定几个用于修改的文本特征，确保任何改变都能改变事件的叙事框架，同时保留其核心事实。4. 开发一种反事实生成算法，该算法使用大型语言模型来系统地生成原始文本的修改版本。

Result: 该框架成功地将公众情绪转变为更有利的状态，成功率达70%。

Conclusion: 该框架可以作为外交官、政策制定者和传播专家的实用工具，提供数据驱动的见解，说明如何构建外交举措或报告事件，以培养更理想的公众情绪。

Abstract: Diplomatic events consistently prompt widespread public discussion and
debate. Public sentiment plays a critical role in diplomacy, as a good
sentiment provides vital support for policy implementation, helps resolve
international issues, and shapes a nation's international image. Traditional
methods for gauging public sentiment, such as large-scale surveys or manual
content analysis of media, are typically time-consuming, labor-intensive, and
lack the capacity for forward-looking analysis. We propose a novel framework
that identifies specific modifications for diplomatic event narratives to shift
public sentiment from negative to neutral or positive. First, we train a
language model to predict public reaction towards diplomatic events. To this
end, we construct a dataset comprising descriptions of diplomatic events and
their associated public discussions. Second, guided by communication theories
and in collaboration with domain experts, we predetermined several textual
features for modification, ensuring that any alterations changed the event's
narrative framing while preserving its core facts.We develop a counterfactual
generation algorithm that employs a large language model to systematically
produce modified versions of an original text. The results show that this
framework successfully shifted public sentiment to a more favorable state with
a 70\% success rate. This framework can therefore serve as a practical tool for
diplomats, policymakers, and communication specialists, offering data-driven
insights on how to frame diplomatic initiatives or report on events to foster a
more desirable public sentiment.

</details>


### [2] [Speaker Style-Aware Phoneme Anchoring for Improved Cross-Lingual Speech Emotion Recognition](https://arxiv.org/abs/2509.20373)
*Shreya G. Upadhyay,Carlos Busso,Chi-Chun Lee*

Main category: cs.CL

TL;DR: 本文提出了一种跨语言语音情感识别（SER）框架，该框架通过在音素和说话人层面进行对齐，以解决不同语言在语音变异性和说话人表达风格上的差异带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 跨语言语音情感识别（SER）由于不同语言在语音变异性和说话人表达风格上的差异而仍然是一项具有挑战性的任务。需要在这样的不同条件下有效地捕捉情感，这需要一个能够对齐不同说话人和语言的情感外化的框架。

Method: 我们提出了一个说话人风格感知的音素锚定框架，该框架在音素和说话人层面进行情感表达的对齐。我们的方法通过基于图的聚类构建特定于情感的说话人社区，以捕捉共享的说话人特征。利用这些群体，我们在说话人和语音空间中应用双空间锚定，以实现跨语言的更好的情感转移。

Result: 在MSP-Podcast（英语）和BIIC-Podcast（台湾普通话）语料库上的评估表明，与竞争基线相比，泛化能力有所提高，并为跨语言情感表示的共性提供了宝贵的见解。

Conclusion: 本文提出了一个说话人风格感知的音素锚定框架，该框架在音素和说话人层面进行情感表达的对齐，并通过实验验证了其有效性。

Abstract: Cross-lingual speech emotion recognition (SER) remains a challenging task due
to differences in phonetic variability and speaker-specific expressive styles
across languages. Effectively capturing emotion under such diverse conditions
requires a framework that can align the externalization of emotions across
different speakers and languages. To address this problem, we propose a
speaker-style aware phoneme anchoring framework that aligns emotional
expression at the phonetic and speaker levels. Our method builds
emotion-specific speaker communities via graph-based clustering to capture
shared speaker traits. Using these groups, we apply dual-space anchoring in
speaker and phonetic spaces to enable better emotion transfer across languages.
Evaluations on the MSP-Podcast (English) and BIIC-Podcast (Taiwanese Mandarin)
corpora demonstrate improved generalization over competitive baselines and
provide valuable insights into the commonalities in cross-lingual emotion
representation.

</details>


### [3] [CFD-LLMBench: A Benchmark Suite for Evaluating Large Language Models in Computational Fluid Dynamics](https://arxiv.org/abs/2509.20374)
*Nithin Somasekharan,Ling Yue,Yadi Cao,Weichao Li,Patrick Emami,Pochinapeddi Sai Bhargav,Anurag Acharya,Xingyu Xie,Shaowu Pan*

Main category: cs.CL

TL;DR: 这篇论文介绍了一个名为CFDLLMBench的基准测试套件，用于评估大型语言模型（LLM）在计算流体动力学（CFD）领域的性能。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在自动化复杂物理系统数值实验中的应用，特别是在计算流体动力学（CFD）这一关键且劳动密集型领域中的潜力。

Method: 该基准测试套件包含三个互补组件：CFDQuery、CFDCodeBench和FoamBench，用于全面评估LLM在CFD知识、数值和物理推理以及CFD工作流程的上下文相关实施方面的性能。

Result: 该基准测试结合了详细的任务分类和严格的评估框架，以提供可重现的结果，并量化LLM在代码可执行性、解决方案准确性和数值收敛行为方面的性能。

Conclusion: CFDLLMBench为开发和评估LLM驱动的复杂物理系统数值实验自动化奠定了坚实的基础。

Abstract: Large Language Models (LLMs) have demonstrated strong performance across
general NLP tasks, but their utility in automating numerical experiments of
complex physical system -- a critical and labor-intensive component -- remains
underexplored. As the major workhorse of computational science over the past
decades, Computational Fluid Dynamics (CFD) offers a uniquely challenging
testbed for evaluating the scientific capabilities of LLMs. We introduce
CFDLLMBench, a benchmark suite comprising three complementary components --
CFDQuery, CFDCodeBench, and FoamBench -- designed to holistically evaluate LLM
performance across three key competencies: graduate-level CFD knowledge,
numerical and physical reasoning of CFD, and context-dependent implementation
of CFD workflows. Grounded in real-world CFD practices, our benchmark combines
a detailed task taxonomy with a rigorous evaluation framework to deliver
reproducible results and quantify LLM performance across code executability,
solution accuracy, and numerical convergence behavior. CFDLLMBench establishes
a solid foundation for the development and evaluation of LLM-driven automation
of numerical experiments for complex physical systems. Code and data are
available at https://github.com/NREL-Theseus/cfdllmbench/.

</details>


### [4] [Assessing Classical Machine Learning and Transformer-based Approaches for Detecting AI-Generated Research Text](https://arxiv.org/abs/2509.20375)
*Sharanya Parimanoharan,Ruwan D. Nawarathna*

Main category: cs.CL

TL;DR: 探讨了使用机器学习方法检测ChatGPT生成的文本与人类撰写文本的区别，旨在解决学术诚信、知识产权和错误信息传播等问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（如ChatGPT）的快速普及模糊了人类和AI生成文本之间的界限，引发了对学术诚信、知识产权和错误信息传播的担忧。因此，需要可靠的AI文本检测技术来进行公平评估，维护人类原创性，并在数字通信中培养信任。

Method: 研究使用了包含250对来自不同研究主题的摘要的标记数据集，测试并比较了经典机器学习方法（逻辑回归与词袋模型、词性标注和TF-IDF特征）和基于Transformer的方法（BERT与N-grams、DistilBERT、带有轻量级自定义分类器的BERT和基于LSTM的N-gram模型）。

Result: DistilBERT实现了最佳的总体性能，而逻辑回归和BERT-Custom提供了可靠且平衡的替代方案；LSTM和BERT-N-gram方法则相对落后。最佳模型的最大投票集成未能超过DistilBERT本身。

Conclusion: 该研究全面评估了各种AI文本检测方法的优缺点，为构建更强大的Transformer框架奠定了基础，以便利用更大、更丰富的数据集跟上不断改进的生成式AI模型。

Abstract: The rapid adoption of large language models (LLMs) such as ChatGPT has
blurred the line between human and AI-generated texts, raising urgent questions
about academic integrity, intellectual property, and the spread of
misinformation. Thus, reliable AI-text detection is needed for fair assessment
to safeguard human authenticity and cultivate trust in digital communication.
In this study, we investigate how well current machine learning (ML) approaches
can distinguish ChatGPT-3.5-generated texts from human-written texts employing
a labeled data set of 250 pairs of abstracts from a wide range of research
topics. We test and compare both classical (Logistic Regression armed with
classical Bag-of-Words, POS, and TF-IDF features) and transformer-based (BERT
augmented with N-grams, DistilBERT, BERT with a lightweight custom classifier,
and LSTM-based N-gram models) ML detection techniques. As we aim to assess each
model's performance in detecting AI-generated research texts, we also aim to
test whether an ensemble of these models can outperform any single detector.
Results show DistilBERT achieves the overall best performance, while Logistic
Regression and BERT-Custom offer solid, balanced alternatives; LSTM- and
BERT-N-gram approaches lag. The max voting ensemble of the three best models
fails to surpass DistilBERT itself, highlighting the primacy of a single
transformer-based representation over mere model diversity. By comprehensively
assessing the strengths and weaknesses of these AI-text detection approaches,
this work lays a foundation for more robust transformer frameworks with larger,
richer datasets to keep pace with ever-improving generative AI models.

</details>


### [5] [ConceptViz: A Visual Analytics Approach for Exploring Concepts in Large Language Models](https://arxiv.org/abs/2509.20376)
*Haoxuan Li,Zhen Wen,Qiqi Jiang,Chenxiao Li,Yuwei Wu,Yuchen Yang,Yiyao Wang,Xiuqi Huang,Minfeng Zhu,Wei Chen*

Main category: cs.CL

TL;DR: ConceptViz is a visual analytics system designed for exploring concepts in LLMs, enabling users to query SAEs using concepts of interest, interactively explore concept-to-feature alignments, and validate the correspondences through model behavior verification.


<details>
  <summary>Details</summary>
Motivation: Understanding how LLMs internally represent knowledge remains a significant challenge. Sparse Autoencoders (SAEs) have emerged as a promising technique for extracting interpretable features from LLMs, but SAE features do not inherently align with human-understandable concepts, making their interpretation cumbersome and labor-intensive.

Method: ConceptViz implements a novel Identification => Interpretation => Validation pipeline, enabling users to query SAEs using concepts of interest, interactively explore concept-to-feature alignments, and validate the correspondences through model behavior verification.

Result: ConceptViz enhances interpretability research by streamlining the discovery and validation of meaningful concept representations in LLMs, ultimately aiding researchers in building more accurate mental models of LLM features.The effectiveness of ConceptViz is demonstrated through two usage scenarios and a user study.

Conclusion: ConceptViz is effective in streamlining the discovery and validation of meaningful concept representations in LLMs, ultimately aiding researchers in building more accurate mental models of LLM features.

Abstract: Large language models (LLMs) have achieved remarkable performance across a
wide range of natural language tasks. Understanding how LLMs internally
represent knowledge remains a significant challenge. Despite Sparse
Autoencoders (SAEs) have emerged as a promising technique for extracting
interpretable features from LLMs, SAE features do not inherently align with
human-understandable concepts, making their interpretation cumbersome and
labor-intensive. To bridge the gap between SAE features and human concepts, we
present ConceptViz, a visual analytics system designed for exploring concepts
in LLMs. ConceptViz implements a novel dentification => Interpretation =>
Validation pipeline, enabling users to query SAEs using concepts of interest,
interactively explore concept-to-feature alignments, and validate the
correspondences through model behavior verification. We demonstrate the
effectiveness of ConceptViz through two usage scenarios and a user study. Our
results show that ConceptViz enhances interpretability research by streamlining
the discovery and validation of meaningful concept representations in LLMs,
ultimately aiding researchers in building more accurate mental models of LLM
features. Our code and user guide are publicly available at
https://github.com/Happy-Hippo209/ConceptViz.

</details>


### [6] [SKILL-RAG: Self-Knowledge Induced Learning and Filtering for Retrieval-Augmented Generation](https://arxiv.org/abs/2509.20377)
*Tomoaki Isoda*

Main category: cs.CL

TL;DR: 这篇论文提出了一种名为SKILL-RAG的新方法，该方法利用模型的自我知识来判断哪些检索到的文档对回答给定的问题是有益的。


<details>
  <summary>Details</summary>
Motivation: 检索系统可能会返回不相关的内容，将这些信息纳入模型通常会导致幻觉。因此，识别和过滤掉无用的检索内容是提高RAG性能的关键挑战。

Method: 该方法设计了一个基于强化学习的训练框架，以明确地从模型中提取自我知识，并采用句子级的粒度来过滤掉不相关的内容，同时保留有用的知识。

Result: 在多个问答基准上使用Llama2-7B和Qwen3-8B评估SKILL-RAG。实验结果表明，SKILL-RAG不仅提高了生成质量，而且显著减少了输入文档的数量。

Conclusion: 验证了自我知识在指导选择高质量检索结果中的重要性。

Abstract: Retrieval-Augmented Generation (RAG) has significantly improved the
performance of large language models (LLMs) on knowledge-intensive tasks in
recent years. However, since retrieval systems may return irrelevant content,
incorporating such information into the model often leads to hallucinations.
Thus, identifying and filtering out unhelpful retrieved content is a key
challenge for improving RAG performance.To better integrate the internal
knowledge of the model with external knowledge from retrieval, it is essential
to understand what the model "knows" and "does not know" (which is also called
"self-knowledge"). Based on this insight, we propose SKILL-RAG (Self-Knowledge
Induced Learning and Filtering for RAG), a novel method that leverages the
model's self-knowledge to determine which retrieved documents are beneficial
for answering a given query. We design a reinforcement learning-based training
framework to explicitly elicit self-knowledge from the model and employs
sentence-level granularity to filter out irrelevant content while preserving
useful knowledge.We evaluate SKILL-RAG using Llama2-7B and Qwen3-8B on several
question answering benchmarks. Experimental results demonstrate that SKILL-RAG
not only improves generation quality but also significantly reduces the number
of input documents, validating the importance of self-knowledge in guiding the
selection of high-quality retrievals.

</details>


### [7] [SwasthLLM: a Unified Cross-Lingual, Multi-Task, and Meta-Learning Zero-Shot Framework for Medical Diagnosis Using Contrastive Representations](https://arxiv.org/abs/2509.20567)
*Ayan Sar,Pranav Singh Puri,Sumit Aich,Tanupriya Choudhury,Abhijit Kumar*

Main category: cs.CL

TL;DR: SwasthLLM: A multilingual, zero-shot, multi-task learning framework for medical diagnosis.


<details>
  <summary>Details</summary>
Motivation: Automatic disease diagnosis from clinical text in multilingual healthcare is challenging due to data scarcity in low-resource languages and linguistic variability.

Method: Utilizes XLM-RoBERTa with language-aware attention, Siamese contrastive learning, translation consistency, contrastive projection head, and MAML for rapid adaptation.

Result: Achieves 97.22% accuracy and 97.17% F1-score in supervised settings, and 92.78% accuracy in Hindi and 73.33% in Bengali in zero-shot scenarios.

Conclusion: SwasthLLM demonstrates strong generalization in low-resource contexts for medical diagnosis.

Abstract: In multilingual healthcare environments, automatic disease diagnosis from
clinical text remains a challenging task due to the scarcity of annotated
medical data in low-resource languages and the linguistic variability across
populations. This paper proposes SwasthLLM, a unified, zero-shot,
cross-lingual, and multi-task learning framework for medical diagnosis that
operates effectively across English, Hindi, and Bengali without requiring
language-specific fine-tuning. At its core, SwasthLLM leverages the
multilingual XLM-RoBERTa encoder augmented with a language-aware attention
mechanism and a disease classification head, enabling the model to extract
medically relevant information regardless of the language structure. To align
semantic representations across languages, a Siamese contrastive learning
module is introduced, ensuring that equivalent medical texts in different
languages produce similar embeddings. Further, a translation consistency module
and a contrastive projection head reinforce language-invariant representation
learning. SwasthLLM is trained using a multi-task learning strategy, jointly
optimizing disease classification, translation alignment, and contrastive
learning objectives. Additionally, we employ Model-Agnostic Meta-Learning
(MAML) to equip the model with rapid adaptation capabilities for unseen
languages or tasks with minimal data. Our phased training pipeline emphasizes
robust representation alignment before task-specific fine-tuning. Extensive
evaluation shows that SwasthLLM achieves high diagnostic performance, with a
test accuracy of 97.22% and an F1-score of 97.17% in supervised settings.
Crucially, in zero-shot scenarios, it attains 92.78% accuracy on Hindi and
73.33% accuracy on Bengali medical text, demonstrating strong generalization in
low-resource contexts.

</details>


### [8] [Beyond Global Emotion: Fine-Grained Emotional Speech Synthesis with Dynamic Word-Level Modulation](https://arxiv.org/abs/2509.20378)
*Sirui Wang,Andong Chen,Tiejun Zhao*

Main category: cs.CL

TL;DR: Emo-FiLM: A novel framework for fine-grained emotion control in LLM-based TTS, enabling word-level emotion modulation using emotion2vec and FiLM layers.


<details>
  <summary>Details</summary>
Motivation: Existing E-TTS systems lack the ability to capture dynamic emotion shifts within a sentence, relying on global emotion control.

Method: Emo-FiLM aligns frame-level features from emotion2vec to words for word-level emotion annotations, and uses a Feature-wise Linear Modulation (FiLM) layer to modulate text embeddings for word-level emotion control. A new dataset, FEDD, was created for evaluation.

Result: Emo-FiLM outperforms existing methods on both global and fine-grained emotion tasks.

Conclusion: Emo-FiLM is effective and general for expressive speech synthesis.

Abstract: Emotional text-to-speech (E-TTS) is central to creating natural and
trustworthy human-computer interaction. Existing systems typically rely on
sentence-level control through predefined labels, reference audio, or natural
language prompts. While effective for global emotion expression, these
approaches fail to capture dynamic shifts within a sentence. To address this
limitation, we introduce Emo-FiLM, a fine-grained emotion modeling framework
for LLM-based TTS. Emo-FiLM aligns frame-level features from emotion2vec to
words to obtain word-level emotion annotations, and maps them through a
Feature-wise Linear Modulation (FiLM) layer, enabling word-level emotion
control by directly modulating text embeddings. To support evaluation, we
construct the Fine-grained Emotion Dynamics Dataset (FEDD) with detailed
annotations of emotional transitions. Experiments show that Emo-FiLM
outperforms existing approaches on both global and fine-grained tasks,
demonstrating its effectiveness and generality for expressive speech synthesis.

</details>


### [9] [Dynamic Reasoning Chains through Depth-Specialized Mixture-of-Experts in Transformer Architectures](https://arxiv.org/abs/2509.20577)
*Sampurna Roy,Ayan Sar,Anurag Kaushish,Kanav Gupta,Tanupriya Choudhury,Abhijit Kumar*

Main category: cs.CL

TL;DR: 提出了一个名为DS-MoE的框架，该框架通过深度专业化的混合专家来创建动态推理链，从而克服了现有transformer架构的低效问题。


<details>
  <summary>Details</summary>
Motivation: 现有transformer架构对所有输入应用相同的处理深度，导致效率低下并限制了推理质量。简单的查询和复杂的逻辑问题受到相同的多层计算，浪费资源。

Method: 提出了深度专业化的混合专家(DS-MoE)框架，该框架将混合专家范式从基于宽度的计算扩展到深度专业化的计算。DS-MoE引入了针对不同推理深度优化的专家模块，并通过学习到的路由网络动态组装自定义推理链。

Result: DS-MoE实现了高达16%的计算节省和35%的推理速度提升，同时在复杂的多步骤推理基准测试中提供了2.8%的更高准确率。

Conclusion: DS-MoE是自适应神经架构的一个重大进步，证明了深度专业化的模块化处理可以同时提高大规模语言模型的效率、推理质量和可解释性。

Abstract: Contemporary transformer architectures apply identical processing depth to
all inputs, creating inefficiencies and limiting reasoning quality. Simple
factual queries are subjected to the same multilayered computation as complex
logical problems, wasting resources while constraining deep inference. To
overcome this, we came up with a concept of Dynamic Reasoning Chains through
Depth Specialised Mixture of Experts (DS-MoE), a modular framework that extends
the Mixture of Experts paradigm from width-based to depth specialised
computation. DS-MoE introduces expert modules optimised for distinct reasoning
depths, shallow pattern recognition, compositional reasoning, logical
inference, memory integration, and meta-cognitive supervision. A learned
routing network dynamically assembles custom reasoning chains, activating only
the necessary experts to match input complexity. The dataset on which we
trained and evaluated DS-MoE is on The Pile, an 800GB corpus covering diverse
domains such as scientific papers, legal texts, programming code, and web
content, enabling systematic assessment across reasoning depths. Experimental
results demonstrate that DS-MoE achieves up to 16 per cent computational
savings and 35 per cent faster inference compared to uniform-depth
transformers, while delivering 2.8 per cent higher accuracy on complex
multi-step reasoning benchmarks. Furthermore, routing decisions yield
interpretable reasoning chains, enhancing transparency and scalability. These
findings establish DS-MoE as a significant advancement in adaptive neural
architectures, demonstrating that depth-specialised modular processing can
simultaneously improve efficiency, reasoning quality, and interpretability in
large-scale language models.

</details>


### [10] [USB-Rec: An Effective Framework for Improving Conversational Recommendation Capability of Large Language Model](https://arxiv.org/abs/2509.20381)
*Jianyu Wen,Jingyun Wang,Cilin Yan,Jiayin Cai,Xiaolong Jiang,Ying Zhang*

Main category: cs.CL

TL;DR: 提出了一种用于改进LLM在对话推荐中性能的集成训练-推理框架（USB-Rec）。


<details>
  <summary>Details</summary>
Motivation: 现有的基于LLM的方法主要集中在如何利用LLM的总结和分析能力，而忽略了训练问题。

Method: 设计了一个基于LLM的偏好优化（PO）数据集构建策略用于RL训练，并提出了一个在推理阶段的自增强策略（SES）。

Result: 在各种数据集上的大量实验表明，该方法始终优于以前的最先进方法。

Conclusion: 该研究提出了一种有效的训练-推理框架，可以提高LLM在对话推荐中的性能。

Abstract: Recently, Large Language Models (LLMs) have been widely employed in
Conversational Recommender Systems (CRSs). Unlike traditional language model
approaches that focus on training, all existing LLMs-based approaches are
mainly centered around how to leverage the summarization and analysis
capabilities of LLMs while ignoring the issue of training. Therefore, in this
work, we propose an integrated training-inference framework,
User-Simulator-Based framework (USB-Rec), for improving the performance of LLMs
in conversational recommendation at the model level. Firstly, we design a
LLM-based Preference Optimization (PO) dataset construction strategy for RL
training, which helps the LLMs understand the strategies and methods in
conversational recommendation. Secondly, we propose a Self-Enhancement Strategy
(SES) at the inference stage to further exploit the conversational
recommendation potential obtained from RL training. Extensive experiments on
various datasets demonstrate that our method consistently outperforms previous
state-of-the-art methods.

</details>


### [11] [Hierarchical Resolution Transformers: A Wavelet-Inspired Architecture for Multi-Scale Language Understanding](https://arxiv.org/abs/2509.20581)
*Ayan Sar,Sampurna Roy,Kanav Gupta,Anurag Kaushish,Tanupriya Choudhury,Abhijit Kumar*

Main category: cs.CL

TL;DR: 提出了分层分辨率Transformer（HRT），一种受小波启发的神经架构，可以在多个分辨率上同时处理语言。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在自然语言任务中取得了最先进的性能，但它们从根本上错误地表示了人类语言的层次结构，将文本处理为扁平的token序列，导致二次计算成本、弱组合泛化和不充分的篇章级建模。

Method: HRT构建了一个多分辨率注意力，实现了自下而上的组合和自上而下的语境化。通过在不同尺度上采用指数序列缩减，HRT实现了O(nlogn)的复杂度。

Result: 在GLUE、SuperGLUE、Long Range Arena和WikiText-103等多个基准测试中，HRT的性能优于标准Transformer基线，在GLUE上平均提高+3.8%，在SuperGLUE上提高+4.5%，在Long Range Arena上提高+6.1%，同时与类似参数计数的BERT和GPT风格模型相比，内存使用量减少了42%，推理延迟减少了37%。

Conclusion: HRT是第一个将计算结构与人类语言的层次组织对齐的架构，证明了多尺度、受小波启发的处理可以产生理论上的效率提升和语言理解方面的实际改进。

Abstract: Transformer architectures have achieved state-of-the-art performance across
natural language tasks, yet they fundamentally misrepresent the hierarchical
nature of human language by processing text as flat token sequences. This
results in quadratic computational cost, weak computational cost, weak
compositional generalization, and inadequate discourse-level modeling. We
propose Hierarchical Resolution Transformer (HRT), a novel wavelet-inspired
neural architecture that processes language simultaneously across multiple
resolutions, from characters to discourse-level units. HRT constructs a
multi-resolution attention, enabling bottom-up composition and top-down
contextualization. By employing exponential sequence reduction across scales,
HRT achieves O(nlogn) complexity, offering significant efficiency improvements
over standard transformers. We evaluated HRT on a diverse suite of benchmarks,
including GLUE, SuperGLUE, Long Range Arena, and WikiText-103, and results
demonstrated that HRT outperforms standard transformer baselines by an average
of +3.8% on GLUE, +4.5% on SuperGLUE, and +6.1% on Long Range Arena, while
reducing memory usage by 42% and inference latency by 37% compared to BERT and
GPT style models of similar parameter count. Ablation studies confirm the
effectiveness of cross-resolution attention and scale-specialized modules,
showing that each contributes independently to both efficiency and accuracy.
Our findings establish HRT as the first architecture to align computational
structure with the hierarchical organization of human language, demonstrating
that multi-scale, wavelet-inspired processing yields both theoretical
efficiency gains and practical improvements in language understanding.

</details>


### [12] [Document Summarization with Conformal Importance Guarantees](https://arxiv.org/abs/2509.20461)
*Bruce Kuwahara,Chen-Yuan Lin,Xiao Shi Huang,Kin Kwan Leung,Jullian Arta Yapeter,Ilya Stanevich,Felipe Perez,Jesse C. Cresswell*

Main category: cs.CL

TL;DR: 本文介绍了一种名为 Conformal Importance Summarization 的框架，用于生成保留重要性的摘要，该框架使用 conformal prediction 来提供严格的、无分布的覆盖保证。


<details>
  <summary>Details</summary>
Motivation: 现有的自动摘要系统，尤其是基于大型语言模型（LLM）的系统，在医疗保健、法律和金融等高风险领域，缺乏对关键内容包含的可靠保证。

Method: 通过校准句子级别重要性分数的阈值，实现可提取文档摘要，并具有用户指定的覆盖率和关键内容召回率。该方法是模型无关的，只需要一个小的校准集，并且可以与现有的黑盒 LLM 无缝集成。

Result: 在已建立的摘要基准上的实验表明，Conformal Importance Summarization 实现了理论上保证的信息覆盖率。

Conclusion: Conformal Importance Summarization 可以与现有技术相结合，以实现可靠、可控的自动摘要，为在关键应用中更安全地部署 AI 摘要工具铺平道路。

Abstract: Automatic summarization systems have advanced rapidly with large language
models (LLMs), yet they still lack reliable guarantees on inclusion of critical
content in high-stakes domains like healthcare, law, and finance. In this work,
we introduce Conformal Importance Summarization, the first framework for
importance-preserving summary generation which uses conformal prediction to
provide rigorous, distribution-free coverage guarantees. By calibrating
thresholds on sentence-level importance scores, we enable extractive document
summarization with user-specified coverage and recall rates over critical
content. Our method is model-agnostic, requires only a small calibration set,
and seamlessly integrates with existing black-box LLMs. Experiments on
established summarization benchmarks demonstrate that Conformal Importance
Summarization achieves the theoretically assured information coverage rate. Our
work suggests that Conformal Importance Summarization can be combined with
existing techniques to achieve reliable, controllable automatic summarization,
paving the way for safer deployment of AI summarization tools in critical
applications. Code is available at
https://github.com/layer6ai-labs/conformal-importance-summarization.

</details>


### [13] [Few-Shot and Training-Free Review Generation via Conversational Prompting](https://arxiv.org/abs/2509.20805)
*Genki Kusano*

Main category: cs.CL

TL;DR: 提出了一种名为对话提示的轻量级方法，用于在少样本和无训练的情况下生成个性化评论。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设目标用户有大量的评论历史或需要额外的模型训练，但在实际应用中，用户评论较少且难以进行微调。

Method: 将用户评论重新构建为多轮对话，包括简单对话提示（SCP）和对比对话提示（CCP），后者通过插入其他用户的评论或LLM生成的评论作为错误回复，鼓励模型以用户的风格生成文本。

Result: 实验表明，传统的非对话提示生成的评论与随机用户相似，而SCP和CCP生成的评论更接近目标用户，尤其是在用户只有少量评论时。CCP在有高质量负样本时效果更好，而SCP在缺乏此类数据时仍具有竞争力。

Conclusion: 对话提示为少样本和无训练约束下的评论生成提供了一种实用的解决方案。

Abstract: Personalized review generation helps businesses understand user preferences,
yet most existing approaches assume extensive review histories of the target
user or require additional model training. Real-world applications often face
few-shot and training-free situations, where only a few user reviews are
available and fine-tuning is infeasible. It is well known that large language
models (LLMs) can address such low-resource settings, but their effectiveness
depends on prompt engineering. In this paper, we propose Conversational
Prompting, a lightweight method that reformulates user reviews as multi-turn
conversations. Its simple variant, Simple Conversational Prompting (SCP),
relies solely on the user's own reviews, while the contrastive variant,
Contrastive Conversational Prompting (CCP), inserts reviews from other users or
LLMs as incorrect replies and then asks the model to correct them, encouraging
the model to produce text in the user's style. Experiments on eight product
domains and five LLMs showed that the conventional non-conversational prompt
often produced reviews similar to those written by random users, based on
text-based metrics such as ROUGE-L and BERTScore, and application-oriented
tasks like user identity matching and sentiment analysis. In contrast, both SCP
and CCP produced reviews much closer to those of the target user, even when
each user had only two reviews. CCP brings further improvements when
high-quality negative examples are available, whereas SCP remains competitive
when such data cannot be collected. These results suggest that conversational
prompting offers a practical solution for review generation under few-shot and
training-free constraints.

</details>


### [14] [ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos](https://arxiv.org/abs/2509.20467)
*Henrik Vatndal,Vinay Setty*

Main category: cs.CL

TL;DR: ShortCheck是一个自动识别值得人工审核的短视频的pipeline，集成了语音转录、OCR、物体和deepfake检测、视频文本摘要和声明验证。


<details>
  <summary>Details</summary>
Motivation: 短视频平台上的错误信息检测具有挑战性，因为内容是多模态、动态和嘈杂的。

Method: ShortCheck是一个模块化的、仅推理的pipeline，具有用户友好的界面。

Result: 该pipeline在多语言环境下的TikTok视频数据集上进行了验证，F1-weighted score超过70%。

Conclusion: ShortCheck取得了有希望的结果。

Abstract: Short-form video platforms like TikTok present unique challenges for
misinformation detection due to their multimodal, dynamic, and noisy content.
We present ShortCheck, a modular, inference-only pipeline with a user-friendly
interface that automatically identifies checkworthy short-form videos to help
human fact-checkers. The system integrates speech transcription, OCR, object
and deepfake detection, video-to-text summarization, and claim verification.
ShortCheck is validated by evaluating it on two manually annotated datasets
with TikTok videos in a multilingual setting. The pipeline achieves promising
results with F1-weighted score over 70\%.

</details>


### [15] [BESPOKE: Benchmark for Search-Augmented Large Language Model Personalization via Diagnostic Feedback](https://arxiv.org/abs/2509.21106)
*Hyunseo Kim,Sangam Lee,Kwangwook Seo,Dongha Lee*

Main category: cs.CL

TL;DR: 本文提出了BESPOKE，一个用于评估搜索增强型LLM中个性化的现实基准。


<details>
  <summary>Details</summary>
Motivation: 现有的搜索增强型LLM在满足不同用户需求方面仍有不足，因为它们未能充分识别同一查询背后不同用户的意图，并且不能以用户偏好的形式传递信息。对ChatGPT和Gemini等系统的个性化能力缺乏系统的评估。

Method: 本文通过收集来自人类的真实聊天和搜索历史来设计BESPOKE基准，并通过将响应与细粒度的偏好分数和反馈配对来实现诊断性评估。

Result: 本文利用BESPOKE进行了系统分析，揭示了在信息搜寻任务中实现有效个性化的关键需求，为个性化搜索增强型LLM的细粒度评估奠定了基础。

Conclusion: BESPOKE是一个现实的基准，可以有效评估搜索增强型LLM中的个性化能力，并为未来的研究提供方向。

Abstract: Search-augmented large language models (LLMs) have advanced
information-seeking tasks by integrating retrieval into generation, reducing
users' cognitive burden compared to traditional search systems. Yet they remain
insufficient for fully addressing diverse user needs, which requires
recognizing how the same query can reflect different intents across users and
delivering information in preferred forms. While recent systems such as ChatGPT
and Gemini attempt personalization by leveraging user histories, systematic
evaluation of such personalization is under-explored. To address this gap, we
propose BESPOKE, the realistic benchmark for evaluating personalization in
search-augmented LLMs. BESPOKE is designed to be both realistic, by collecting
authentic chat and search histories directly from humans, and diagnostic, by
pairing responses with fine-grained preference scores and feedback. The
benchmark is constructed through long-term, deeply engaged human annotation,
where human annotators contributed their own histories, authored queries with
detailed information needs, and evaluated responses with scores and diagnostic
feedback. Leveraging BESPOKE, we conduct systematic analyses that reveal key
requirements for effective personalization in information-seeking tasks,
providing a foundation for fine-grained evaluation of personalized
search-augmented LLMs. Our code and data are available at
https://augustinlib.github.io/BESPOKE/.

</details>


### [16] [MARS: toward more efficient multi-agent collaboration for LLM reasoning](https://arxiv.org/abs/2509.20502)
*Xiao Wang,Jia Wang,Yijie Wang,Pengtao Dang,Sha Cao,Chi Zhang*

Main category: cs.CL

TL;DR: 这篇论文提出了一种新的多智能体协作框架，名为MARS，旨在提高大型语言模型（LLMs）的推理能力，同时降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体辩论（MAD）方法虽然有效，但由于智能体数量众多和频繁通信，计算开销巨大。本文旨在解决这个问题。

Method: 该方法设计了一个基于角色的协作框架MARS，其中作者智能体生成初始解决方案，评审者智能体独立提供决策和评论，元评审者整合反馈以做出最终决策并指导进一步修订。

Result: 实验结果表明，MARS在多个基准测试中与MAD的准确率相匹配，同时token使用量和推理时间减少了约50%。

Conclusion: MARS能够在提高推理质量的同时，有效控制token消耗和推理时间。

Abstract: Large language models (LLMs) have achieved impressive results in natural
language understanding, yet their reasoning capabilities remain limited when
operating as single agents. Multi-Agent Debate (MAD) has been proposed to
address this limitation by enabling collaborative reasoning among multiple
models in a round-table debate manner. While effective, MAD introduces
substantial computational overhead due to the number of agents involved and the
frequent communication required. In this paper, we propose MARS (Multi-Agent
Review System), a role-based collaboration framework inspired by the review
process. In MARS, an author agent generates an initial solution, reviewer
agents provide decisions and comments independently, and a meta-reviewer
integrates the feedback to make the final decision and guide further revision.
This design enhances reasoning quality while avoiding costly
reviewer-to-reviewer interactions, thereby controlling token consumption and
inference time. We compared MARS with both MAD and other state-of-the-art
reasoning strategies across multiple benchmarks. Extensive experiments with
different LLMs show that MARS matches the accuracy of MAD while reducing both
token usage and inference time by approximately 50\%. Code is available at
https://github.com/xwang97/MARS.

</details>


### [17] [Retrieval over Classification: Integrating Relation Semantics for Multimodal Relation Extraction](https://arxiv.org/abs/2509.21151)
*Lei Hei,Tingjing Liao,Yingxin Pei,Yiyang Qi,Jiaqi Wang,Ruiting Li,Feiliang Ren*

Main category: cs.CL

TL;DR: 提出了一种新的多模态关系抽取框架ROC，它将关系抽取任务转化为检索任务，并通过对比学习对齐实体-关系对。


<details>
  <summary>Details</summary>
Motivation: 传统的多模态关系抽取方法忽略了结构约束（如实体类型和位置信息），并且缺乏对细粒度关系理解的语义表达能力。

Method: 提出了一个名为检索优于分类(ROC)的框架。该框架通过多模态编码器整合实体类型和位置信息，使用大型语言模型将关系标签扩展为自然语言描述，并通过基于语义相似度的对比学习对齐实体-关系对。

Result: 在基准数据集MNRE和MORE上实现了state-of-the-art的性能，并表现出更强的鲁棒性和可解释性。

Conclusion: 该方法在多模态关系抽取任务上具有有效性。

Abstract: Relation extraction (RE) aims to identify semantic relations between entities
in unstructured text. Although recent work extends traditional RE to multimodal
scenarios, most approaches still adopt classification-based paradigms with
fused multimodal features, representing relations as discrete labels. This
paradigm has two significant limitations: (1) it overlooks structural
constraints like entity types and positional cues, and (2) it lacks semantic
expressiveness for fine-grained relation understanding. We propose
\underline{R}etrieval \underline{O}ver \underline{C}lassification (ROC), a
novel framework that reformulates multimodal RE as a retrieval task driven by
relation semantics. ROC integrates entity type and positional information
through a multimodal encoder, expands relation labels into natural language
descriptions using a large language model, and aligns entity-relation pairs via
semantic similarity-based contrastive learning. Experiments show that our
method achieves state-of-the-art performance on the benchmark datasets MNRE and
MORE and exhibits stronger robustness and interpretability.

</details>


### [18] [SiniticMTError: A Machine Translation Dataset with Error Annotations for Sinitic Languages](https://arxiv.org/abs/2509.20557)
*Hannah Liu,Junghyun Min,Ethan Yue Heng Cheung,Shou-Yi Hung,Syed Mekael Wasti,Runtong Liang,Shiyao Qian,Shizhao Zheng,Elsie Chan,Ka Ieng Charlotte Lo,Wing Yu Yip,Richard Tzong-Han Tsai,En-Shiun Annie Lee*

Main category: cs.CL

TL;DR: SiniticMTError是一个新的数据集，它建立在现有的平行语料库之上，以提供从英语到普通话、粤语和吴语的机器翻译示例中的错误跨度、错误类型和错误严重程度注释。


<details>
  <summary>Details</summary>
Motivation: 近年来，机器翻译（MT）取得了重大进展，但对于许多缺乏大规模训练数据和语言资源的低资源语言来说，进展仍然有限。粤语和吴语是汉藏语系的两个例子，尽管每种语言在世界各地都有超过 8000 万的使用者。

Method: 我们介绍了 SiniticMTError，这是一个新的数据集，它建立在现有的平行语料库之上，以提供机器翻译示例中的错误跨度、错误类型和错误严重程度注释，从英语到普通话、粤语和吴语。

Result: 我们报告了母语人士的严格注释过程，并分析了注释者间协议、迭代反馈以及错误类型和严重程度的模式。

Conclusion: 我们的数据集可以作为 MT 社区的资源，用于微调具有错误检测能力的模型，支持翻译质量估计、错误感知生成和低资源语言评估的研究。

Abstract: Despite major advances in machine translation (MT) in recent years, progress
remains limited for many low-resource languages that lack large-scale training
data and linguistic resources. Cantonese and Wu Chinese are two Sinitic
examples, although each enjoys more than 80 million speakers around the world.
In this paper, we introduce SiniticMTError, a novel dataset that builds on
existing parallel corpora to provide error span, error type, and error severity
annotations in machine-translated examples from English to Mandarin, Cantonese,
and Wu Chinese. Our dataset serves as a resource for the MT community to
utilize in fine-tuning models with error detection capabilities, supporting
research on translation quality estimation, error-aware generation, and
low-resource language evaluation. We report our rigorous annotation process by
native speakers, with analyses on inter-annotator agreement, iterative
feedback, and patterns in error type and severity.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [19] [Leveraging NTPs for Efficient Hallucination Detection in VLMs](https://arxiv.org/abs/2509.20379)
*Ofir Azachi,Kfir Eliyahu,Eyal El Ani,Rom Himelstein,Roi Reichart,Yuval Pinter,Nitay Calderon*

Main category: cs.CV

TL;DR: 这篇论文提出了一种基于视觉语言模型 (VLM) 下一token概率 (NTP) 的轻量级幻觉检测方法，旨在提高 VLM 的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的幻觉检测方法计算密集且增加模型延迟，因此需要更高效的方法。

Method: 该方法训练传统的机器学习模型，利用VLM的NTPs作为信号，并结合语言NTPs和VLM的幻觉预测分数。

Result: 实验结果表明，基于NTP的特征可以有效地预测幻觉，并且该方法的性能与强大的VLM相当。结合语言NTPs和VLM的幻觉预测分数可以进一步提高性能。

Conclusion: 这项研究为简单、轻量级的解决方案铺平了道路，从而增强了VLM的可靠性。

Abstract: Hallucinations of vision-language models (VLMs), which are misalignments
between visual content and generated text, undermine the reliability of VLMs.
One common approach for detecting them employs the same VLM, or a different
one, to assess generated outputs. This process is computationally intensive and
increases model latency. In this paper, we explore an efficient on-the-fly
method for hallucination detection by training traditional ML models over
signals based on the VLM's next-token probabilities (NTPs). NTPs provide a
direct quantification of model uncertainty. We hypothesize that high
uncertainty (i.e., a low NTP value) is strongly associated with hallucinations.
To test this, we introduce a dataset of 1,400 human-annotated statements
derived from VLM-generated content, each labeled as hallucinated or not, and
use it to test our NTP-based lightweight method. Our results demonstrate that
NTP-based features are valuable predictors of hallucinations, enabling fast and
simple ML models to achieve performance comparable to that of strong VLMs.
Furthermore, augmenting these NTPs with linguistic NTPs, computed by feeding
only the generated text back into the VLM, enhances hallucination detection
performance. Finally, integrating hallucination prediction scores from VLMs
into the NTP-based models led to better performance than using either VLMs or
NTPs alone. We hope this study paves the way for simple, lightweight solutions
that enhance the reliability of VLMs.

</details>


### [20] [Quasi-Synthetic Riemannian Data Generation for Writer-Independent Offline Signature Verification](https://arxiv.org/abs/2509.20420)
*Elias N. Zois,Moises Diaz,Salem Said,Miguel A. Ferrer*

Main category: cs.CV

TL;DR: 提出了一种准合成数据生成框架，利用对称正定矩阵 (SPD) 的黎曼几何，通过黎曼高斯混合模型生成正负样本，用于离线手写签名验证。


<details>
  <summary>Details</summary>
Motivation: 离线手写签名验证在作者独立的环境中仍然具有挑战性，过去的或现在的手工或数据驱动的方法通常依赖于真实世界的签名数据集进行分类器训练。

Method: 利用 SPD 空间的黎曼几何，以 SPD 空间中的一小组真实样本作为种子，通过黎曼高斯混合模型识别黎曼中心作为合成作者，并以方差作为其属性。在每个中心上进行黎曼高斯抽样，生成正的和负的合成 SPD 群体。利用度量学习框架，使用成对的相似和不相似的 SPD 点进行训练，然后在真实世界的数据集上进行测试。

Result: 在包含西方和亚洲书写风格的两个流行的签名数据集上进行的实验表明，该方法在内部和交叉数据集评估协议下均有效。结果表明，该准合成方法实现了低错误率。

Conclusion: 在黎曼空间中生成合成数据对于作者独立的签名验证系统具有潜力。

Abstract: Offline handwritten signature verification remains a challenging task,
particularly in writer-independent settings where models must generalize across
unseen individuals. Recent developments have highlighted the advantage of
geometrically inspired representations, such as covariance descriptors on
Riemannian manifolds. However, past or present, handcrafted or data-driven
methods usually depend on real-world signature datasets for classifier
training. We introduce a quasi-synthetic data generation framework leveraging
the Riemannian geometry of Symmetric Positive Definite matrices (SPD). A small
set of genuine samples in the SPD space is the seed to a Riemannian Gaussian
Mixture which identifies Riemannian centers as synthetic writers and variances
as their properties. Riemannian Gaussian sampling on each center generates
positive as well as negative synthetic SPD populations. A metric learning
framework utilizes pairs of similar and dissimilar SPD points, subsequently
testing it over on real-world datasets. Experiments conducted on two popular
signature datasets, encompassing Western and Asian writing styles, demonstrate
the efficacy of the proposed approach under both intra- and cross- dataset
evaluation protocols. The results indicate that our quasi-synthetic approach
achieves low error rates, highlighting the potential of generating synthetic
data in Riemannian spaces for writer-independent signature verification
systems.

</details>


### [21] [Seedream 4.0: Toward Next-generation Multimodal Image Generation](https://arxiv.org/abs/2509.20427)
*Team Seedream,Yunpeng Chen,Yu Gao,Lixue Gong,Meng Guo,Qiushan Guo,Zhiyao Guo,Xiaoxia Hou,Weilin Huang,Yixuan Huang,Xiaowen Jian,Huafeng Kuang,Zhichao Lai,Fanshi Li,Liang Li,Xiaochen Lian,Chao Liao,Liyang Liu,Wei Liu,Yanzuo Lu,Zhengxiong Luo,Tongtong Ou,Guang Shi,Yichun Shi,Shiqi Sun,Yu Tian,Zhi Tian,Peng Wang,Rui Wang,Xun Wang,Ye Wang,Guofeng Wu,Jie Wu,Wenxu Wu,Yonghui Wu,Xin Xia,Xuefeng Xiao,Shuang Xu,Xin Yan,Ceyuan Yang,Jianchao Yang,Zhonghua Zhai,Chenlin Zhang,Heng Zhang,Qi Zhang,Xinyu Zhang,Yuwei Zhang,Shijia Zhao,Wenliang Zhao,Wenjia Zhu*

Main category: cs.CV

TL;DR: Seedream 4.0是一个高效、高性能的多模态图像生成系统，统一了文本到图像合成、图像编辑和多图像组合。


<details>
  <summary>Details</summary>
Motivation: 为了提升图像生成和编辑的效率和质量，并扩展到更复杂的多模态任务。

Method: 开发了高效的扩散Transformer和强大的VAE，并结合多模态后训练、对抗蒸馏、分布匹配、量化和推测解码等技术。

Result: 实现了快速生成原生高分辨率图像（如1K-4K），并在T2I和多模态图像编辑方面取得了最先进的结果。

Conclusion: Seedream 4.0将传统的T2I系统扩展为更具交互性和多维度的创意工具，推动了生成人工智能在创意和专业应用方面的界限。

Abstract: We introduce Seedream 4.0, an efficient and high-performance multimodal image
generation system that unifies text-to-image (T2I) synthesis, image editing,
and multi-image composition within a single framework. We develop a highly
efficient diffusion transformer with a powerful VAE which also can reduce the
number of image tokens considerably. This allows for efficient training of our
model, and enables it to fast generate native high-resolution images (e.g.,
1K-4K). Seedream 4.0 is pretrained on billions of text-image pairs spanning
diverse taxonomies and knowledge-centric concepts. Comprehensive data
collection across hundreds of vertical scenarios, coupled with optimized
strategies, ensures stable and large-scale training, with strong
generalization. By incorporating a carefully fine-tuned VLM model, we perform
multi-modal post-training for training both T2I and image editing tasks
jointly. For inference acceleration, we integrate adversarial distillation,
distribution matching, and quantization, as well as speculative decoding. It
achieves an inference time of up to 1.8 seconds for generating a 2K image
(without a LLM/VLM as PE model). Comprehensive evaluations reveal that Seedream
4.0 can achieve state-of-the-art results on both T2I and multimodal image
editing. In particular, it demonstrates exceptional multimodal capabilities in
complex tasks, including precise image editing and in-context reasoning, and
also allows for multi-image reference, and can generate multiple output images.
This extends traditional T2I systems into an more interactive and
multidimensional creative tool, pushing the boundary of generative AI for both
creativity and professional applications. Seedream 4.0 is now accessible on
https://www.volcengine.com/experience/ark?launch=seedream.

</details>


### [22] [A Contrastive Learning Framework for Breast Cancer Detection](https://arxiv.org/abs/2509.20474)
*Samia Saeed,Khuram Naveed*

Main category: cs.CV

TL;DR: 本文提出了一种基于对比学习(CL)的框架，以解决深度学习方法在乳腺癌检测中因标记数据集有限而导致的准确性问题。


<details>
  <summary>Details</summary>
Motivation: 早期检测对于改善乳腺癌治疗效果至关重要，但深度学习方法由于缺乏大型标记数据集而难以保证准确性。

Method: 使用半监督对比学习方法训练Resnet-50，利用大量未标记的乳房X光片数据和相似性指标。

Result: 在基准数据集INbreast和MIAS上，乳腺癌检测准确率达到96.7%，优于现有技术水平。

Conclusion: 对比学习框架在小数据集上表现出色，能够有效提高乳腺癌检测的准确性。

Abstract: Breast cancer, the second leading cause of cancer-related deaths globally,
accounts for a quarter of all cancer cases [1]. To lower this death rate, it is
crucial to detect tumors early, as early-stage detection significantly improves
treatment outcomes. Advances in non-invasive imaging techniques have made early
detection possible through computer-aided detection (CAD) systems which rely on
traditional image analysis to identify malignancies. However, there is a
growing shift towards deep learning methods due to their superior
effectiveness. Despite their potential, deep learning methods often struggle
with accuracy due to the limited availability of large-labeled datasets for
training. To address this issue, our study introduces a Contrastive Learning
(CL) framework, which excels with smaller labeled datasets. In this regard, we
train Resnet-50 in semi supervised CL approach using similarity index on a
large amount of unlabeled mammogram data. In this regard, we use various
augmentation and transformations which help improve the performance of our
approach. Finally, we tune our model on a small set of labelled data that
outperforms the existing state of the art. Specifically, we observed a 96.7%
accuracy in detecting breast cancer on benchmark datasets INbreast and MIAS.

</details>


### [23] [Are Foundation Models Ready for Industrial Defect Recognition? A Reality Check on Real-World Data](https://arxiv.org/abs/2509.20479)
*Simon Baeuerle,Pratik Khanna,Nils Friederich,Angelo Jovin Yamachui Sitcheu,Damir Shakirov,Andreas Steimer,Ralf Mikut*

Main category: cs.CV

TL;DR: 大型模型在文本和图像处理任务中表现出色，但在实际工业图像数据上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 在系列制造中，使用大型模型进行自动质量检测可以节省大量工作，并且可以替代繁琐的标注任务。

Method: 在定制的真实工业图像数据和公共图像数据上测试多个最新的大型模型。

Result: 所有模型在真实数据上均失败，但在公共基准数据集上表现良好。

Conclusion: 大型模型在实际工业图像数据上的zero-shot泛化能力不足。

Abstract: Foundation Models (FMs) have shown impressive performance on various text and
image processing tasks. They can generalize across domains and datasets in a
zero-shot setting. This could make them suitable for automated quality
inspection during series manufacturing, where various types of images are being
evaluated for many different products. Replacing tedious labeling tasks with a
simple text prompt to describe anomalies and utilizing the same models across
many products would save significant efforts during model setup and
implementation. This is a strong advantage over supervised Artificial
Intelligence (AI) models, which are trained for individual applications and
require labeled training data. We test multiple recent FMs on both custom
real-world industrial image data and public image data. We show that all of
those models fail on our real-world data, while the very same models perform
well on public benchmark datasets.

</details>


### [24] [Shared Neural Space: Unified Precomputed Feature Encoding for Multi-Task and Cross Domain Vision](https://arxiv.org/abs/2509.20481)
*Jing Li,Oskar Bartosz,Chengyu Wang,Michal Wnuczynski,Dilshan Godaliyadda,Michael Polley*

Main category: cs.CV

TL;DR: 提出了一种通用的神经空间（NS），其中编码器-解码器框架预先计算视觉和图像任务的特征。


<details>
  <summary>Details</summary>
Motivation: 针对大多数成像和视觉中的AI模型被定制为执行特定的高精度任务，但这种策略对于具有一系列模块化任务的应用程序来说效率低下，因为每个任务都需要映射到不同的潜在域。

Method: 使用编码器-解码器框架预先计算视觉和图像任务的特征。编码器学习具有转换意识的、可泛化的表示，这使得多个下游AI模块可以共享相同的特征空间。该backbone是轻量级的和基于CNN的。

Result: 成像和视觉模块，如demosaicing、denoising、深度估计和语义分割可以在NS中有效地执行。

Conclusion: 该架构减少了冗余，提高了跨域转移的泛化能力，并为高效的多任务视觉管道奠定了基础。

Abstract: The majority of AI models in imaging and vision are customized to perform on
specific high-precision task. However, this strategy is inefficient for
applications with a series of modular tasks, since each requires a mapping into
a disparate latent domain. To address this inefficiency, we proposed a
universal Neural Space (NS), where an encoder-decoder framework pre-computes
features across vision and imaging tasks. Our encoder learns transformation
aware, generalizable representations, which enable multiple downstream AI
modules to share the same feature space. This architecture reduces redundancy,
improves generalization across domain shift, and establishes a foundation for
effecient multi-task vision pipelines. Furthermore, as opposed to larger
transformer backbones, our backbone is lightweight and CNN-based, allowing for
wider across hardware. We furthur demonstrate that imaging and vision modules,
such as demosaicing, denoising, depth estimation and semantic segmentation can
be performed efficiently in the NS.

</details>


### [25] [Data-Efficient Stream-Based Active Distillation for Scalable Edge Model Deployment](https://arxiv.org/abs/2509.20484)
*Dani Manjah,Tim Bary,Benoît Gérin,Benoît Macq,Christophe de Vleeschouwer*

Main category: cs.CV

TL;DR: 本文研究了如何选择最有用的图像来训练模型，以最大限度地提高模型质量，同时降低传输成本。


<details>
  <summary>Details</summary>
Motivation: 边缘相机系统不断扩展，面临着不断发展的环境，需要定期更新模型。通常，复杂的教师模型在中央服务器上运行以注释数据，然后用于训练针对计算能力有限的边缘设备量身定制的较小模型。

Method: 高置信度流式策略与基于多样性的方法相结合。

Result: 对于类似的训练负载（即迭代），该方法能够以最少的数据集查询生成高质量的模型。

Conclusion: 高置信度流式策略与基于多样性的方法相结合，能够以最少的数据集查询生成高质量的模型。

Abstract: Edge camera-based systems are continuously expanding, facing ever-evolving
environments that require regular model updates. In practice, complex teacher
models are run on a central server to annotate data, which is then used to
train smaller models tailored to the edge devices with limited computational
power. This work explores how to select the most useful images for training to
maximize model quality while keeping transmission costs low. Our work shows
that, for a similar training load (i.e., iterations), a high-confidence
stream-based strategy coupled with a diversity-based approach produces a
high-quality model with minimal dataset queries.

</details>


### [26] [InstructVTON: Optimal Auto-Masking and Natural-Language-Guided Interactive Style Control for Inpainting-Based Virtual Try-On](https://arxiv.org/abs/2509.20524)
*Julien Han,Shuwen Qiu,Qi Li,Xingzi Xu,Mehmet Saygin Seyfioglu,Kavosh Asadi,Karim Bouyarmane*

Main category: cs.CV

TL;DR: InstructVTON是一个通过自然语言指令控制的交互式虚拟试穿系统，可以对单件或多件服装进行细粒度和复杂的风格控制。


<details>
  <summary>Details</summary>
Motivation: 现有的基于掩码的虚拟试穿模型存在生成理想结果困难、需要背景知识、可能依赖模型以及某些情况下无法实现的问题（例如，在穿着长袖衬衫的人身上试穿卷起袖子的长袖衬衫）。

Method: InstructVTON利用视觉语言模型（VLMs）和图像分割模型自动生成二元掩码，这些掩码基于用户提供的图像和自由文本风格指令生成。

Result: InstructVTON简化了最终用户体验，无需精确绘制掩码，并自动执行多轮图像生成，以实现仅使用基于掩码的虚拟试穿模型无法实现的试穿场景。InstructVTON可与现有的虚拟试穿模型互操作，以实现具有风格控制的最先进的结果。

Conclusion: InstructVTON通过指令控制，简化了虚拟试穿流程，并提高了试穿效果。

Abstract: We present InstructVTON, an instruction-following interactive virtual try-on
system that allows fine-grained and complex styling control of the resulting
generation, guided by natural language, on single or multiple garments. A
computationally efficient and scalable formulation of virtual try-on formulates
the problem as an image-guided or image-conditioned inpainting task. These
inpainting-based virtual try-on models commonly use a binary mask to control
the generation layout. Producing a mask that yields desirable result is
difficult, requires background knowledge, might be model dependent, and in some
cases impossible with the masking-based approach (e.g. trying on a long-sleeve
shirt with "sleeves rolled up" styling on a person wearing long-sleeve shirt
with sleeves down, where the mask will necessarily cover the entire sleeve).
InstructVTON leverages Vision Language Models (VLMs) and image segmentation
models for automated binary mask generation. These masks are generated based on
user-provided images and free-text style instructions. InstructVTON simplifies
the end-user experience by removing the necessity of a precisely drawn mask,
and by automating execution of multiple rounds of image generation for try-on
scenarios that cannot be achieved with masking-based virtual try-on models
alone. We show that InstructVTON is interoperable with existing virtual try-on
models to achieve state-of-the-art results with styling control.

</details>


### [27] [Innovative Deep Learning Architecture for Enhanced Altered Fingerprint Recognition](https://arxiv.org/abs/2509.20537)
*Dana A Abdullah,Dana Rasul Hamad,Bishar Rasheed Ibrahim,Sirwan Abdulwahid Aula,Aso Khaleel Ameen,Sabat Salih Hamadamin*

Main category: cs.CV

TL;DR: DeepAFRNet是一种用于识别扭曲指纹样本的深度学习模型，它使用VGG16提取特征并通过余弦相似度进行比较。


<details>
  <summary>Details</summary>
Motivation: 在边境控制、取证和财政许可等应用中，改变的指纹识别(AFR)具有挑战性。对抗者可以故意修改脊线模式以逃避检测，因此对改变的指纹进行鲁棒识别至关重要。

Method: 该方法使用VGG16骨干网络提取高维特征，并使用余弦相似度来比较嵌入。

Result: 在严格的阈值下，DeepAFRNet在三个难度级别(简单、中等、困难)上分别实现了96.7%、98.76%和99.54%的准确率。阈值敏感性研究表明，将阈值从0.92放宽到0.72会使准确率急剧下降到7.86%、27.05%和29.51%，这 подчеркивает 了生物识别系统中阈值选择的重要性。

Conclusion: 通过使用真实的改变样本并报告每个级别的指标，DeepAFRNet解决了基于合成改变或有限验证协议的先前工作的局限性，并表明已准备好在安全性和识别弹性都至关重要的实际部署中应用。

Abstract: Altered fingerprint recognition (AFR) is challenging for biometric
verification in applications such as border control, forensics, and fiscal
admission. Adversaries can deliberately modify ridge patterns to evade
detection, so robust recognition of altered prints is essential. We present
DeepAFRNet, a deep learning recognition model that matches and recognizes
distorted fingerprint samples. The approach uses a VGG16 backbone to extract
high-dimensional features and cosine similarity to compare embeddings. We
evaluate on the SOCOFing Real-Altered subset with three difficulty levels
(Easy, Medium, Hard). With strict thresholds, DeepAFRNet achieves accuracies of
96.7 percent, 98.76 percent, and 99.54 percent for the three levels. A
threshold-sensitivity study shows that relaxing the threshold from 0.92 to 0.72
sharply degrades accuracy to 7.86 percent, 27.05 percent, and 29.51 percent,
underscoring the importance of threshold selection in biometric systems. By
using real altered samples and reporting per-level metrics, DeepAFRNet
addresses limitations of prior work based on synthetic alterations or limited
verification protocols, and indicates readiness for real-world deployments
where both security and recognition resilience are critical.

</details>


### [28] [Large Pre-Trained Models for Bimanual Manipulation in 3D](https://arxiv.org/abs/2509.20579)
*Hanna Yurchyk,Wei-Di Chang,Gregory Dudek,David Meger*

Main category: cs.CV

TL;DR: 本文研究了将预训练 Vision Transformer 的注意力图整合到体素表示中，以增强双臂机器人操作。


<details>
  <summary>Details</summary>
Motivation: 利用 DINOv2 的注意力图作为像素级显著性得分，提升双臂操作性能。

Method: 提取 DINOv2 的注意力图，将其转换为体素网格中的语义线索，并整合到行为克隆策略中。

Result: 在 RLBench 双臂基准测试中，注意力引导的特征化方法在所有任务上的平均绝对改进为 8.2%，相对增益为 21.9%。

Conclusion: 将预训练 ViT 的注意力图融入体素表示可以有效提升双臂机器人操作的性能。

Abstract: We investigate the integration of attention maps from a pre-trained Vision
Transformer into voxel representations to enhance bimanual robotic
manipulation. Specifically, we extract attention maps from DINOv2, a
self-supervised ViT model, and interpret them as pixel-level saliency scores
over RGB images. These maps are lifted into a 3D voxel grid, resulting in
voxel-level semantic cues that are incorporated into a behavior cloning policy.
When integrated into a state-of-the-art voxel-based policy, our
attention-guided featurization yields an average absolute improvement of 8.2%
and a relative gain of 21.9% across all tasks in the RLBench bimanual
benchmark.

</details>


### [29] [A Comparative Benchmark of Real-time Detectors for Blueberry Detection towards Precision Orchard Management](https://arxiv.org/abs/2509.20580)
*Xinyang Mu,Yuzhen Lu,Boyang Deng*

Main category: cs.CV

TL;DR: 本研究针对自然环境下蓝莓检测的挑战，建立了一个包含各种光照、遮挡和成熟阶段的大规模蓝莓数据集，并对 YOLO 和 RT-DETR 系列的 36 个实时目标检测模型进行了基准测试和比较分析。此外，还探索了使用无偏 Mean Teacher 半监督学习 (SSL) 对模型进行微调，以进一步提高检测性能。


<details>
  <summary>Details</summary>
Motivation: 在自然环境中检测蓝莓具有挑战性，因为存在可变的光照、遮挡和运动模糊等问题。基于深度学习的目标检测器有潜力解决这些问题，但需要一个大规模、多样化的数据集。同时，在实际部署这些模型时，需要在准确性、速度和内存之间进行权衡。

Method: 1. 创建了一个包含 661 张冠层图像的蓝莓数据集，这些图像是在 2022-2023 季使用智能手机拍摄的，包含 85,879 个标记实例。2. 对 YOLO (v8-v12) 和 RT-DETR (v1-v2) 系列的 36 个模型变体进行了基准测试和比较分析。3. 使用无偏 Mean Teacher 半监督学习 (SSL) 对所有模型进行了微调。

Result: YOLOv12m 的 mAP@50 达到 93.3%，RT-DETRv2-X 的 mAP@50 达到 93.6%。通过使用 SSL 进行微调，RT-DETR-v2-X 的 mAP@50 进一步提高到 94.8%。

Conclusion: 本研究建立了一个大规模蓝莓数据集，并对 YOLO 和 RT-DETR 系列的实时目标检测模型进行了全面的基准测试和分析。研究结果表明，中等规模的模型在准确性和速度之间取得了良好的平衡，并且可以通过 SSL 微调进一步提高检测性能。

Abstract: Blueberry detection in natural environments remains challenging due to
variable lighting, occlusions, and motion blur due to environmental factors and
imaging devices. Deep learning-based object detectors promise to address these
challenges, but they demand a large-scale, diverse dataset that captures the
real-world complexities. Moreover, deploying these models in practical
scenarios often requires the right accuracy/speed/memory trade-off in model
selection. This study presents a novel comparative benchmark analysis of
advanced real-time object detectors, including YOLO (You Only Look Once)
(v8-v12) and RT-DETR (Real-Time Detection Transformers) (v1-v2) families,
consisting of 36 model variants, evaluated on a newly curated dataset for
blueberry detection. This dataset comprises 661 canopy images collected with
smartphones during the 2022-2023 seasons, consisting of 85,879 labelled
instances (including 36,256 ripe and 49,623 unripe blueberries) across a wide
range of lighting conditions, occlusions, and fruit maturity stages. Among the
YOLO models, YOLOv12m achieved the best accuracy with a mAP@50 of 93.3%, while
RT-DETRv2-X obtained a mAP@50 of 93.6%, the highest among all the RT-DETR
variants. The inference time varied with the model scale and complexity, and
the mid-sized models appeared to offer a good accuracy-speed balance. To
further enhance detection performance, all the models were fine-tuned using
Unbiased Mean Teacher-based semi-supervised learning (SSL) on a separate set of
1,035 unlabeled images acquired by a ground-based machine vision platform in
2024. This resulted in accuracy gains ranging from -1.4% to 2.9%, with
RT-DETR-v2-X achieving the best mAP@50 of 94.8%. More in-depth research into
SSL is needed to better leverage cross-domain unlabeled data. Both the dataset
and software programs of this study are made publicly available to support
further research.

</details>


### [30] [Region-of-Interest Augmentation for Mammography Classification under Patient-Level Cross-Validation](https://arxiv.org/abs/2509.20585)
*Farbod Bigdeli,Mohsen Mohammadagha,Ali Bigdeli*

Main category: cs.CV

TL;DR: 这篇论文提出了一种轻量级的ROI增强策略，用于提高乳腺癌筛查的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习在乳腺癌钼靶图像判读中表现出潜力，但受限于数据集分辨率和样本量。

Method: 该方法在训练期间，将完整图像替换为从预先计算的无标签边界框库中抽样的随机ROI裁剪，并可选择抖动以增加变异性。

Result: 在Mini-DDSM数据集上，ROI增强在ROC-AUC方面略有提升，PR-AUC基本持平或略有下降。

Conclusion: 简单的数据驱动的ROI策略可以在受限环境中增强乳腺钼靶图像分类，无需额外标签或架构修改。

Abstract: Breast cancer screening with mammography remains central to early detection
and mortality reduction. Deep learning has shown strong potential for
automating mammogram interpretation, yet limited-resolution datasets and small
sample sizes continue to restrict performance. We revisit the Mini-DDSM dataset
(9,684 images; 2,414 patients) and introduce a lightweight region-of-interest
(ROI) augmentation strategy. During training, full images are probabilistically
replaced with random ROI crops sampled from a precomputed, label-free
bounding-box bank, with optional jitter to increase variability. We evaluate
under strict patient-level cross-validation and report ROC-AUC, PR-AUC, and
training-time efficiency metrics (throughput and GPU memory). Because ROI
augmentation is training-only, inference-time cost remains unchanged. On
Mini-DDSM, ROI augmentation (best: p_roi = 0.10, alpha = 0.10) yields modest
average ROC-AUC gains, with performance varying across folds; PR-AUC is flat to
slightly lower. These results demonstrate that simple, data-centric ROI
strategies can enhance mammography classification in constrained settings
without requiring additional labels or architectural modifications.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [31] [An Approach to Checking Correctness for Agentic Systems](https://arxiv.org/abs/2509.20364)
*Thomas J Sheffler*

Main category: cs.AI

TL;DR: 提出了一种用于监控AI agent行为的时序表达式语言，以系统地检测基于LLM的agent系统的错误。


<details>
  <summary>Details</summary>
Motivation: 当前错误检测方法主要依赖于输入和输出的文本匹配，由于LLM响应中固有的自然语言可变性，这种方法显得脆弱。本文方法侧重于agent动作序列，允许独立于特定文本输出验证系统行为。

Method: 使用时序逻辑技术，监控agent工具调用和状态转换的执行轨迹，以检测与预期行为模式的偏差。

Result: 当使用大型模型时，所有时序断言在许多测试运行中都得到满足。当较小的模型被替换时，执行违反了行为断言，主要是由于不正确的工具排序和失败的协调切换。时序表达式成功地标记了这些异常。

Conclusion: 该方法为系统地监控AI agent可靠性奠定了基础，因为这些系统越来越多地部署在关键应用中。

Abstract: This paper presents a temporal expression language for monitoring AI agent
behavior, enabling systematic error-detection of LLM-based agentic systems that
exhibit variable outputs due to stochastic generation processes. Drawing from
temporal logic techniques used in hardware verification, this approach monitors
execution traces of agent tool calls and state transitions to detect deviations
from expected behavioral patterns. Current error-detection approaches rely
primarily on text matching of inputs and outputs, which proves fragile due to
the natural language variability inherent in LLM responses. The proposed method
instead focuses on the sequence of agent actions -- such as tool invocations
and inter-agent communications -- allowing verification of system behavior
independent of specific textual outputs. The temporal expression language
provides assertions that capture correct behavioral patterns across multiple
execution scenarios. These assertions serve dual purposes: validating prompt
engineering and guardrail effectiveness during development, and providing
regression testing when agents are updated with new LLMs or modified logic. The
approach is demonstrated using a three-agent system, where agents coordinate to
solve multi-step reasoning tasks. When powered by large, capable models, all
temporal assertions were satisfied across many test runs. However, when smaller
models were substituted in two of the three agents, executions violated
behavioral assertions, primarily due to improper tool sequencing and failed
coordination handoffs. The temporal expressions successfully flagged these
anomalies, demonstrating the method's effectiveness for detecting behavioral
regressions in production agentic systems. This approach provides a foundation
for systematic monitoring of AI agent reliability as these systems become
increasingly deployed in critical applications.

</details>


### [32] [LATTS: Locally Adaptive Test-Time Scaling](https://arxiv.org/abs/2509.20368)
*Theo Uscidda,Matthew Trager,Michael Kleinman,Aditya Chattopadhyay,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: 提出了一种名为 Locally Adaptive Test-Time Scaling (LATTS) 的方法，该方法根据验证器模型得出的局部难度来调整每步的计算量，从而在精度和计算效率之间取得更好的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型改进下游任务性能的方法通常在所有样本和生成步骤中统一增加计算量，导致资源使用效率低下。

Method: LATTS 在每个生成步骤采用基于验证器的接受标准，以决定是否重新采样、回溯、重启或停止生成过程。该标准根据验证器模型得出的局部难度来有效调整每步的计算量。

Result: 实验结果表明，与标准的基于验证器的方法相比，LATTS 在精度和计算效率之间取得了显著的优势。

Conclusion: LATTS 能够显著提高大语言模型在下游任务上的性能，同时优化计算资源的利用率。

Abstract: One common strategy for improving the performance of Large Language Models
(LLMs) on downstream tasks involves using a \emph{verifier model} to either
select the best answer from a pool of candidates or to steer the
auto-regressive generation process towards better outputs. This class of
methods typically results in improved accuracy at the cost of increased
computation at test-time, a paradigm known as \emph{test-time scaling}.
However, most existing approaches increase computation uniformly across all
samples and generation steps, without considering the complexity of individual
instances, leading to inefficient resource use. We address this limitation by
proposing an approach, called \emph{Locally Adaptive Test-Time Scaling
(LATTS)}, that allocates variable compute across generation steps.
Specifically, at each generation step, LATTS employs a verifier-based
acceptance criterion to decide whether to resample, backtrack, restart, or stop
the generation process. This criterion effectively adjusts the per-step
computational effort based on a precise notion of \emph{local difficulty}
derived from the verifier model. Empirical results show that LATTS achieves
significantly superior accuracy--compute tradeoffs compared to standard
verifier-based methods.

</details>


### [33] [Philosophy-informed Machine Learning](https://arxiv.org/abs/2509.20370)
*MZ Naser*

Main category: cs.AI

TL;DR: 本文综述了哲学指导的机器学习(PhIML)，它将分析哲学的核心思想直接注入到ML模型架构、目标和评估协议中。


<details>
  <summary>Details</summary>
Motivation: 通过尊重设计的哲学概念和价值的模型，PhIML有望实现新的功能。本文旨在从概念基础的角度，展示哲学上的收益和一致性。

Method: 本文回顾了概念基础，并通过案例研究展示了ML用户/设计者如何采用PhIML作为不可知的事后工具或将其内在构建到ML模型架构中。

Result: 本文重点介绍了开放的技术障碍，以及哲学、实践和治理方面的挑战，并概述了通向安全、具有哲学意识和对伦理负责的PhIML的研究路线图。

Conclusion: PhIML通过将哲学思想融入机器学习模型，有望在安全和伦理方面实现新的突破，但仍面临技术、哲学和实践等多重挑战。

Abstract: Philosophy-informed machine learning (PhIML) directly infuses core ideas from
analytic philosophy into ML model architectures, objectives, and evaluation
protocols. Therefore, PhIML promises new capabilities through models that
respect philosophical concepts and values by design. From this lens, this paper
reviews conceptual foundations to demonstrate philosophical gains and
alignment. In addition, we present case studies on how ML users/designers can
adopt PhIML as an agnostic post-hoc tool or intrinsically build it into ML
model architectures. Finally, this paper sheds light on open technical barriers
alongside philosophical, practical, and governance challenges and outlines a
research roadmap toward safe, philosophy-aware, and ethically responsible
PhIML.

</details>


### [34] [InsightGUIDE: An Opinionated AI Assistant for Guided Critical Reading of Scientific Literature](https://arxiv.org/abs/2509.20493)
*Paris Koloveas,Serafeim Chatzopoulos,Thanasis Vergoulis,Christos Tryfonopoulos*

Main category: cs.AI

TL;DR: InsightGUIDE is a new AI tool that provides concise, structured insights to help researchers understand scientific papers, acting as a reading assistant rather than a replacement.


<details>
  <summary>Details</summary>
Motivation: The increasing amount of scientific literature makes it challenging for researchers to stay informed. Existing LLMs provide summaries that are too verbose.

Method: The paper introduces InsightGUIDE, an AI-powered tool with a prompt-driven methodology. It embeds an expert's reading methodology into its core AI logic to provide structured insights.

Result: InsightGUIDE produces more structured and actionable guidance compared to general-purpose LLMs.

Conclusion: InsightGUIDE is a more effective tool for researchers by providing concise and structured insights, acting as a reading assistant.

Abstract: The proliferation of scientific literature presents an increasingly
significant challenge for researchers. While Large Language Models (LLMs) offer
promise, existing tools often provide verbose summaries that risk replacing,
rather than assisting, the reading of the source material. This paper
introduces InsightGUIDE, a novel AI-powered tool designed to function as a
reading assistant, not a replacement. Our system provides concise, structured
insights that act as a "map" to a paper's key elements by embedding an expert's
reading methodology directly into its core AI logic. We present the system's
architecture, its prompt-driven methodology, and a qualitative case study
comparing its output to a general-purpose LLM. The results demonstrate that
InsightGUIDE produces more structured and actionable guidance, serving as a
more effective tool for the modern researcher.

</details>


### [35] [Reconstruction-Based Adaptive Scheduling Using AI Inferences in Safety-Critical Systems](https://arxiv.org/abs/2509.20513)
*Samer Alshaer,Ala Khalifeh,Roman Obermaisser*

Main category: cs.AI

TL;DR: 本研究提出了一种新的重构框架，用于动态验证和组装调度，以确保时间触发系统（TTS）在动态操作环境中的可靠性和安全性。


<details>
  <summary>Details</summary>
Motivation: 在动态操作环境中，时间触发系统（TTS）的调度框架面临消息冲突、不正确的优先级处理导致的锁死循环以及生成不完整或无效的调度等挑战，这些都会损害系统安全和性能。

Method: 该框架通过系统地将AI生成或启发式导出的调度优先级转换为完全可执行的调度来运行，确保符合关键系统约束，如优先级规则和无冲突通信。它结合了强大的安全检查、高效的分配算法和恢复机制来处理意外的上下文事件，包括硬件故障和模式转换。

Result: 实验结果表明，该框架显著提高了系统的适应性、操作完整性和运行时性能，同时保持了计算效率。

Conclusion: 该研究为安全关键型TTS中的安全调度生成问题提供了一种实用且可扩展的解决方案，即使在高度动态和不确定的操作条件下也能实现可靠和灵活的实时调度。

Abstract: Adaptive scheduling is crucial for ensuring the reliability and safety of
time-triggered systems (TTS) in dynamic operational environments. Scheduling
frameworks face significant challenges, including message collisions, locked
loops from incorrect precedence handling, and the generation of incomplete or
invalid schedules, which can compromise system safety and performance. To
address these challenges, this paper presents a novel reconstruction framework
designed to dynamically validate and assemble schedules. The proposed
reconstruction models operate by systematically transforming AI-generated or
heuristically derived scheduling priorities into fully executable schedules,
ensuring adherence to critical system constraints such as precedence rules and
collision-free communication. It incorporates robust safety checks, efficient
allocation algorithms, and recovery mechanisms to handle unexpected context
events, including hardware failures and mode transitions. Comprehensive
experiments were conducted across multiple performance profiles, including
makespan minimisation, workload balancing, and energy efficiency, to validate
the operational effectiveness of the reconstruction models. Results demonstrate
that the proposed framework significantly enhances system adaptability,
operational integrity, and runtime performance while maintaining computational
efficiency. Overall, this work contributes a practical and scalable solution to
the problem of safe schedule generation in safety-critical TTS, enabling
reliable and flexible real-time scheduling even under highly dynamic and
uncertain operational conditions.

</details>


### [36] [Adaptive Approach to Enhance Machine Learning Scheduling Algorithms During Runtime Using Reinforcement Learning in Metascheduling Applications](https://arxiv.org/abs/2509.20520)
*Samer Alshaer,Ala Khalifeh,Roman Obermaisser*

Main category: cs.AI

TL;DR: 这篇论文提出了一种集成在元调度器中的自适应在线学习单元，以增强实时性能，解决离线训练AI调度推理的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统的离线训练方法难以构建包含所有可能场景的完整多调度图（MSG），尤其是在考虑硬件故障、松弛变化或模式改变等上下文事件时，生成MSG的资源消耗大且常常不可行。

Method: 该方法在在线模式下利用强化学习（RL）持续探索和发现新的调度方案，从而扩展MSG并提高系统性能。

Result: 通过实时训练不断改进AI推理，系统保持灵活性，能够满足不断变化的需求。

Conclusion: 该方法确保了大型安全关键环境中的鲁棒性和效率。

Abstract: Metascheduling in time-triggered architectures has been crucial in adapting
to dynamic and unpredictable environments, ensuring the reliability and
efficiency of task execution. However, traditional approaches face significant
challenges when training Artificial Intelligence (AI) scheduling inferences
offline, particularly due to the complexities involved in constructing a
comprehensive Multi-Schedule Graph (MSG) that accounts for all possible
scenarios. The process of generating an MSG that captures the vast probability
space, especially when considering context events like hardware failures, slack
variations, or mode changes, is resource-intensive and often infeasible. To
address these challenges, we propose an adaptive online learning unit
integrated within the metascheduler to enhance performance in real-time. The
primary motivation for developing this unit stems from the limitations of
offline training, where the MSG created is inherently a subset of the complete
space, focusing only on the most probable and critical context events. In the
online mode, Reinforcement Learning (RL) plays a pivotal role by continuously
exploring and discovering new scheduling solutions, thus expanding the MSG and
enhancing system performance over time. This dynamic adaptation allows the
system to handle unexpected events and complex scheduling scenarios more
effectively. Several RL models were implemented within the online learning
unit, each designed to address specific challenges in scheduling. These models
not only facilitate the discovery of new solutions but also optimize existing
schedulers, particularly when stricter deadlines or new performance criteria
are introduced. By continuously refining the AI inferences through real-time
training, the system remains flexible and capable of meeting evolving demands,
thus ensuring robustness and efficiency in large-scale, safety-critical
environments.

</details>


### [37] [A Compound Classification System Based on Fuzzy Relations Applied to the Noise-Tolerant Control of a Bionic Hand via EMG Signal Recognition](https://arxiv.org/abs/2509.20523)
*Pawel Trajdos,Marek Kurzynski*

Main category: cs.AI

TL;DR: 本文提出了一种新的肌电（EMG）控制手部假肢识别系统，该系统能够检测受污染的生物信号，以减轻污染的负面影响。


<details>
  <summary>Details</summary>
Motivation: 肌电生物信号易受污染，这会降低识别系统的分类质量。

Method: 该系统由两个集成组成：单类分类器（OCC）集合，用于评估各个通道的污染程度；K近邻（KNN）分类器集合，用于识别患者的意图。为所有识别系统开发了一个原创的、连贯的模糊模型，该模型允许在整个识别过程中使用统一的软（模糊）决策方案。

Result: 对所开发方法的参数和程序进行了实验比较分析，并将其与文献中描述的类似系统进行了比较。

Conclusion: 提出的模糊识别系统能够检测受污染的生物信号，从而提高肌电控制手部假肢的性能。

Abstract: Modern anthropomorphic upper limb bioprostheses are typically controlled by
electromyographic (EMG) biosignals using a pattern recognition scheme.
Unfortunately, there are many factors originating from the human source of
objects to be classified and from the human-prosthesis interface that make it
difficult to obtain an acceptable classification quality. One of these factors
is the high susceptibility of biosignals to contamination, which can
considerably reduce the quality of classification of a recognition system.
  In the paper, the authors propose a new recognition system intended for EMG
based control of the hand prosthesis with detection of contaminated biosignals
in order to mitigate the adverse effect of contaminations. The system consists
of two ensembles: the set of one-class classifiers (OCC) to assess the degree
of contamination of individual channels and the ensemble of K-nearest
neighbours (KNN) classifier to recognise the patient's intent. For all
recognition systems, an original, coherent fuzzy model was developed, which
allows the use of a uniform soft (fuzzy) decision scheme throughout the
recognition process. The experimental evaluation was conducted using real
biosignals from a public repository. The goal was to provide an experimental
comparative analysis of the parameters and procedures of the developed method
on which the quality of the recognition system depends. The proposed fuzzy
recognition system was also compared with similar systems described in the
literature.

</details>


### [38] [SAMULE: Self-Learning Agents Enhanced by Multi-level Reflection](https://arxiv.org/abs/2509.20562)
*Yubin Ge,Salvatore Romeo,Jason Cai,Monica Sunkara,Yi Zhang*

Main category: cs.AI

TL;DR: 这篇论文提出了一种名为SAMULE的自学习框架，用于解决LLM Agent在复杂任务中生成有意义的反思的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM Agent由于错误分析不足和依赖罕见的成功轨迹，难以生成有意义的反思，尤其是在复杂任务中。

Method: 该框架基于多层次反思合成训练回顾性语言模型，包括单轨迹学习（微观层面）、任务内学习（中观层面）和任务间学习（宏观层面）。此外，还通过基于前瞻的反思机制扩展到交互式设置。

Result: 在TravelPlanner、NATURAL PLAN和Tau-bench三个基准测试中，SAMULE显著优于基于反思的基线方法。

Conclusion: 研究结果表明，精心设计的反思合成和以失败为中心的学习在构建自我完善的LLM Agent中起着关键作用。

Abstract: Despite the rapid advancements in LLM agents, they still face the challenge
of generating meaningful reflections due to inadequate error analysis and a
reliance on rare successful trajectories, especially in complex tasks. In this
work, we propose SAMULE, a new framework for self-learning agents powered by a
retrospective language model that is trained based on Multi-Level Reflection
Synthesis. It first synthesizes high-quality reflections across three
complementary levels: Single-Trajectory Learning (micro-level) for detailed
error correction; Intra-Task Learning (meso-level) to build error taxonomies
across multiple trials of the same task, and Inter-Task Learning (macro-level)
to extract transferable insights based on same typed errors from diverse task
failures. Then we fine-tune a language model serving as the retrospective model
to generate reflections during inference. We further extend our framework to
interactive settings through a foresight-based reflection mechanism, enabling
agents to proactively reflect and adapt during user interactions by comparing
predicted and actual responses. Extensive experiments on three challenging
benchmarks - TravelPlanner, NATURAL PLAN, and Tau-bench - demonstrate that our
approach significantly outperforms reflection-based baselines. Our results
highlight the critical role of well-designed reflection synthesis and
failure-centric learning in building self-improving LLM agents.

</details>


### [39] [Adaptive Cybersecurity Architecture for Digital Product Ecosystems Using Agentic AI](https://arxiv.org/abs/2509.20640)
*Oluwakemi T. Olayinka,Sumeet Jeswani,Divine Iloh*

Main category: cs.AI

TL;DR: 提出了一个基于agentic AI的自适应网络安全架构，以解决传统静态模型在可扩展性、实时检测和上下文响应方面的不足。


<details>
  <summary>Details</summary>
Motivation: 传统的静态网络安全模型难以适应当前数字产品生态系统（包括云服务、API、移动平台和边缘设备）的可扩展性、实时检测和上下文响应需求。

Method: 构建了一个由自主目标驱动的agent，该agent能够进行动态学习和上下文感知决策，并将其集成到自适应网络安全架构的关键生态层中，实现自主威胁缓解、主动策略执行和实时异常检测。该架构具有行为基线、去中心化风险评分和联邦威胁情报共享等重要功能。

Result: 通过原生云模拟，系统展示了识别零日攻击和动态修改访问策略的能力。评估结果表明，该架构提高了适应性，降低了响应延迟，并提高了检测准确性。

Conclusion: 该架构为保护复杂的数字基础设施提供了一个智能且可扩展的蓝图，并且与零信任模型兼容，从而支持遵守国际网络安全法规。

Abstract: Traditional static cybersecurity models often struggle with scalability,
real-time detection, and contextual responsiveness in the current digital
product ecosystems which include cloud services, application programming
interfaces (APIs), mobile platforms, and edge devices. This study introduces
autonomous goal driven agents capable of dynamic learning and context-aware
decision making as part of an adaptive cybersecurity architecture driven by
agentic artificial intelligence (AI). To facilitate autonomous threat
mitigation, proactive policy enforcement, and real-time anomaly detection, this
framework integrates agentic AI across the key ecosystem layers. Behavioral
baselining, decentralized risk scoring, and federated threat intelligence
sharing are important features. The capacity of the system to identify zero-day
attacks and dynamically modify access policies was demonstrated through native
cloud simulations. The evaluation results show increased adaptability,
decreased response latency, and improved detection accuracy. The architecture
provides an intelligent and scalable blueprint for safeguarding complex digital
infrastructure and is compatible with zero-trust models, thereby supporting the
adherence to international cybersecurity regulations.

</details>


### [40] [Accelerate Creation of Product Claims Using Generative AI](https://arxiv.org/abs/2509.20652)
*Po-Yu Liang,Yong Zhang,Tatiana Hwa,Aaron Byers*

Main category: cs.AI

TL;DR: 本研究开发了一个名为 Claim Advisor 的网络应用程序，旨在利用大型语言模型加速产品声明的创建过程。


<details>
  <summary>Details</summary>
Motivation: 产品声明是影响消费者购买行为的关键因素，但创建产品声明需要大量的时间和资金投入。

Method: 该应用通过上下文学习和大型语言模型的微调来实现，包含语义搜索、声明生成与优化、以及使用合成消费者进行模拟排序三个功能。

Result: 在一家消费品公司中的应用显示出非常有希望的结果。

Conclusion: 这项技术具有广泛的应用前景，我们分享研究成果以鼓励生成式人工智能在不同行业中的研究和应用。

Abstract: The benefit claims of a product is a critical driver of consumers' purchase
behavior. Creating product claims is an intense task that requires substantial
time and funding. We have developed the $\textbf{Claim Advisor}$ web
application to accelerate claim creations using in-context learning and
fine-tuning of large language models (LLM). $\textbf{Claim Advisor}$ was
designed to disrupt the speed and economics of claim search, generation,
optimization, and simulation. It has three functions: (1) semantically
searching and identifying existing claims and/or visuals that resonate with the
voice of consumers; (2) generating and/or optimizing claims based on a product
description and a consumer profile; and (3) ranking generated and/or manually
created claims using simulations via synthetic consumers. Applications in a
consumer packaged goods (CPG) company have shown very promising results. We
believe that this capability is broadly useful and applicable across product
categories and industries. We share our learning to encourage the research and
application of generative AI in different industries.

</details>


### [41] [An Automated Retrieval-Augmented Generation LLaMA-4 109B-based System for Evaluating Radiotherapy Treatment Plans](https://arxiv.org/abs/2509.20707)
*Junjie Cui,Peilong Wang,Jason Holmes,Leshan Sun,Michael L. Hinni,Barbara A. Pockaj,Sujay A. Vora,Terence T. Sio,William W. Wong,Nathan Y. Yu,Steven E. Schild,Joshua R. Niska,Sameer R. Keole,Jean-Claude M. Rwigema,Samir H. Patel,Lisa A. McGee,Carlos A. Vargas,Wei Liu*

Main category: cs.AI

TL;DR: 开发一个基于LLaMA-4 109B的检索增强生成（RAG）系统，用于自动、协议感知和可解释的放射治疗计划评估。


<details>
  <summary>Details</summary>
Motivation: 在放射治疗计划评估中，实现自动化、协议感知和可解释性。

Method: 构建包含标准化剂量指标和协议定义约束的知识库，并集成检索引擎、百分位预测组件和临床约束检查器，通过多步骤提示驱动的推理流程，由大型语言模型（LLM）指导，生成简洁、有根据的评估。

Result: RAG系统在百分位估计和约束识别方面与独立检索和约束检查模块计算的值达成100%的一致，确认了所有检索、预测和检查步骤的可靠执行。

Conclusion: 该研究表明，将基于结构化人群的评分与模块化工具增强推理相结合，可实现透明、可扩展的放射治疗计划评估。

Abstract: Purpose: To develop a retrieval-augmented generation (RAG) system powered by
LLaMA-4 109B for automated, protocol-aware, and interpretable evaluation of
radiotherapy treatment plans.
  Methods and Materials: We curated a multi-protocol dataset of 614
radiotherapy plans across four disease sites and constructed a knowledge base
containing normalized dose metrics and protocol-defined constraints. The RAG
system integrates three core modules: a retrieval engine optimized across five
SentenceTransformer backbones, a percentile prediction component based on
cohort similarity, and a clinical constraint checker. These tools are directed
by a large language model (LLM) using a multi-step prompt-driven reasoning
pipeline to produce concise, grounded evaluations.
  Results: Retrieval hyperparameters were optimized using Gaussian Process on a
scalarized loss function combining root mean squared error (RMSE), mean
absolute error (MAE), and clinically motivated accuracy thresholds. The best
configuration, based on all-MiniLM-L6-v2, achieved perfect nearest-neighbor
accuracy within a 5-percentile-point margin and a sub-2pt MAE. When tested
end-to-end, the RAG system achieved 100% agreement with the computed values by
standalone retrieval and constraint-checking modules on both percentile
estimates and constraint identification, confirming reliable execution of all
retrieval, prediction and checking steps.
  Conclusion: Our findings highlight the feasibility of combining structured
population-based scoring with modular tool-augmented reasoning for transparent,
scalable plan evaluation in radiation therapy. The system offers traceable
outputs, minimizes hallucination, and demonstrates robustness across protocols.
Future directions include clinician-led validation, and improved domain-adapted
retrieval models to enhance real-world integration.

</details>


### [42] [Fairy: Interactive Mobile Assistant to Real-world Tasks via LMM-based Multi-agent](https://arxiv.org/abs/2509.20729)
*Jiazheng Sun,Te Yang,Jiayang Niu,Mingxuan Li,Yongyong Lu,Ruimeng Yang,Xin Peng*

Main category: cs.AI

TL;DR: Fairy, an interactive multi-agent mobile assistant, continuously learns and evolves app knowledge during usage, enabling cross-app collaboration and interactive execution.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with diverse app interfaces and evolving user needs in real-world scenarios, often failing on long-tail apps and lacking user interaction.

Method: Fairy uses a Global Task Planner, an App-Level Executor with dual-loop agents, and a Self-Learner to decompose tasks, refine sub-tasks, and consolidate experience.

Result: Fairy with GPT-4o outperforms previous state-of-the-art methods, improving user requirement completion by 33.7% and reducing redundant steps by 58.5%.

Conclusion: Fairy's interaction and self-learning mechanisms are effective in real-world mobile app scenarios, as demonstrated by the RealMobile-Eval benchmark.

Abstract: Large multi-modal models (LMMs) have advanced mobile GUI agents. However,
existing methods struggle with real-world scenarios involving diverse app
interfaces and evolving user needs. End-to-end methods relying on model's
commonsense often fail on long-tail apps, and agents without user interaction
act unilaterally, harming user experience. To address these limitations, we
propose Fairy, an interactive multi-agent mobile assistant capable of
continuously accumulating app knowledge and self-evolving during usage. Fairy
enables cross-app collaboration, interactive execution, and continual learning
through three core modules:(i) a Global Task Planner that decomposes user tasks
into sub-tasks from a cross-app view; (ii) an App-Level Executor that refines
sub-tasks into steps and actions based on long- and short-term memory,
achieving precise execution and user interaction via four core agents operating
in dual loops; and (iii) a Self-Learner that consolidates execution experience
into App Map and Tricks. To evaluate Fairy, we introduce RealMobile-Eval, a
real-world benchmark with a comprehensive metric suite, and LMM-based agents
for automated scoring. Experiments show that Fairy with GPT-4o backbone
outperforms the previous SoTA by improving user requirement completion by 33.7%
and reducing redundant steps by 58.5%, showing the effectiveness of its
interaction and self-learning.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [43] [DELM: a Python toolkit for Data Extraction with Language Models](https://arxiv.org/abs/2509.20617)
*Eric Fithian,Kirill Skobelev*

Main category: cs.IR

TL;DR: DELM是一个用于LLM数据提取流程的Python工具包，旨在提高可重复性、鲁棒性和系统评估。


<details>
  <summary>Details</summary>
Motivation: 现有工作流程依赖于临时脚本，导致可重复性、鲁棒性和系统评估存在困难。

Method: DELM提供了一个模块化框架，具有结构化输出、内置验证、灵活的数据加载和评分策略以及高效的批处理。

Result: 通过两个案例研究展示了DELM的功能：一个展示了新的prompt优化算法，另一个展示了DELM如何量化成本和覆盖率之间的权衡。

Conclusion: DELM是一个开源工具包，可用于快速实验迭代LLM数据提取pipeline并量化它们之间的权衡。

Abstract: Large Language Models (LLMs) have become powerful tools for annotating
unstructured data. However, most existing workflows rely on ad hoc scripts,
making reproducibility, robustness, and systematic evaluation difficult. To
address these challenges, we introduce DELM (Data Extraction with Language
Models), an open-source Python toolkit designed for rapid experimental
iteration of LLM-based data extraction pipelines and for quantifying the
trade-offs between them. DELM minimizes boilerplate code and offers a modular
framework with structured outputs, built-in validation, flexible data-loading
and scoring strategies, and efficient batch processing. It also includes robust
support for working with LLM APIs, featuring retry logic, result caching,
detailed cost tracking, and comprehensive configuration management. We showcase
DELM's capabilities through two case studies: one featuring a novel prompt
optimization algorithm, and another illustrating how DELM quantifies trade-offs
between cost and coverage when selecting keywords to decide which paragraphs to
pass to an LLM. DELM is available at
\href{https://github.com/Center-for-Applied-AI/delm}{\texttt{github.com/Center-for-Applied-AI/delm}}.

</details>


### [44] [Provenance Analysis of Archaeological Artifacts via Multimodal RAG Systems](https://arxiv.org/abs/2509.20769)
*Tuo Zhang,Yuechun Sun,Ruiliang Liu*

Main category: cs.IR

TL;DR: 本文提出了一个基于RAG的系统，用于考古文物溯源分析，通过整合多模态检索和大型视觉语言模型（VLM）来支持专家推理。


<details>
  <summary>Details</summary>
Motivation: 该系统旨在通过整合多模态检索和大型视觉语言模型，减轻专家在处理大量比较语料库时的认知负担，并为分析提供具体的起点。

Method: 该系统构建了一个来自参考文本和图像的双模态知识库，支持原始视觉、边缘增强和语义检索，以识别风格相似的物体。检索到的候选对象由VLM合成，以生成结构化的推论，包括年代、地理和文化归属，以及解释性理由。

Result: 在对大英博物馆的欧亚东部青铜时代文物进行评估后，专家评估表明，该系统产生了有意义且可解释的输出。

Conclusion: 该系统为学者提供了具体的分析起点，并显著减轻了导航大型比较语料库的认知负担。

Abstract: In this work, we present a retrieval-augmented generation (RAG)-based system
for provenance analysis of archaeological artifacts, designed to support expert
reasoning by integrating multimodal retrieval and large vision-language models
(VLMs). The system constructs a dual-modal knowledge base from reference texts
and images, enabling raw visual, edge-enhanced, and semantic retrieval to
identify stylistically similar objects. Retrieved candidates are synthesized by
the VLM to generate structured inferences, including chronological,
geographical, and cultural attributions, alongside interpretive justifications.
We evaluate the system on a set of Eastern Eurasian Bronze Age artifacts from
the British Museum. Expert evaluation demonstrates that the system produces
meaningful and interpretable outputs, offering scholars concrete starting
points for analysis and significantly alleviating the cognitive burden of
navigating vast comparative corpora.

</details>


### [45] [Performance Consistency of Learning Methods for Information Retrieval Tasks](https://arxiv.org/abs/2509.20804)
*Meng Yuan,Justin Zobel*

Main category: cs.IR

TL;DR: Transformer模型在不同种子下的表现差异很大，这引发了对其可靠性的质疑。


<details>
  <summary>Details</summary>
Motivation: 评估信息检索（IR）方法性能的准确性和鲁棒性。

Method: 使用随机种子评估传统统计学习模型和基于Transformer的学习模型在三个不同的IR任务上的性能变化。

Result: 统计模型表现稳定，但Transformer模型在改变种子时表现出巨大差异。在11个案例中，有9个案例的F1分数标准差超过0.075，7个案例的精确度标准差超过0.125。

Conclusion: Transformer模型容易受到训练不稳定性的影响，之前的研究结果可能存在可靠性问题，因此需要更严格的评估方法。

Abstract: A range of approaches have been proposed for estimating the accuracy or
robustness of the measured performance of IR methods. One is to use
bootstrapping of test sets, which, as we confirm, provides an estimate of
variation in performance. For IR methods that rely on a seed, such as those
that involve machine learning, another approach is to use a random set of seeds
to examine performance variation. Using three different IR tasks we have used
such randomness to examine a range of traditional statistical learning models
and transformer-based learning models. While the statistical models are stable,
the transformer models show huge variation as seeds are changed. In 9 of 11
cases the F1-scores (in the range 0.0--1.0) had a standard deviation of over
0.075; while 7 of 11 precision values (also in the range 0.0--1.0) had a
standard deviation of over 0.125. This is in a context where differences of
less than 0.02 have been used as evidence of method improvement. Our findings
highlight the vulnerability of transformer models to training instabilities and
moreover raise questions about the reliability of previous results, thus
underscoring the need for rigorous evaluation practices.

</details>


### [46] [RecIS: Sparse to Dense, A Unified Training Framework for Recommendation Models](https://arxiv.org/abs/2509.20883)
*Hua Zong,Qingtao Zeng,Zhengxiong Zhou,Zhihua Han,Zhensong Yan,Mingjie Liu,Hechen Sun,Jiawei Liu,Yiwen Hu,Qi Wang,YiHan Xian,Wenjie Guo,Houyuan Xiang,Zhiyuan Zeng,Xiangrong Sheng,Bencheng Yan,Nan Hu,Yuheng Huang,Jinqing Lian,Ziru Xu,Yan Zhang,Ju Huang,Siran Yang,Huimin Yi,Jiamang Wang,Pengjie Wang,Han Zhu,Jian Wu,Dan Ou,Jian Xu,Haihong Tang,Yuning Jiang,Bo Zheng,Lin Qu*

Main category: cs.IR

TL;DR: RecIS is a unified sparse-dense training framework for industrial-grade recommendation models.


<details>
  <summary>Details</summary>
Motivation: To create a unified sparse-dense training framework and optimize the sparse component for better efficiency.

Method: A unified sparse-dense training framework based on PyTorch.

Result: RecIS is used in Alibaba for large-model enhanced recommendation training tasks and some traditional sparse models.

Conclusion: RecIS meets the training needs of industrial-grade recommendation models and offers superior efficiency.

Abstract: In this paper, we propose RecIS, a unified Sparse-Dense training framework
designed to achieve two primary goals: 1. Unified Framework To create a Unified
sparse-dense training framework based on the PyTorch ecosystem that meets the
training needs of industrial-grade recommendation models that integrated with
large models. 2.System Optimization To optimize the sparse component, offering
superior efficiency over the TensorFlow-based recommendation models. The dense
component, meanwhile, leverages existing optimization technologies within the
PyTorch ecosystem. Currently, RecIS is being used in Alibaba for numerous
large-model enhanced recommendation training tasks, and some traditional sparse
models have also begun training in it.

</details>


### [47] [FORGE: Forming Semantic Identifiers for Generative Retrieval in Industrial Datasets](https://arxiv.org/abs/2509.20904)
*Kairui Fu,Tao Zhang,Shuwen Xiao,Ziyang Wang,Xinming Zhang,Chenchi Zhang,Yuliang Yan,Junjun Zheng,Yu Li,Zhihong Chen,Jian Wu,Xiangheng Kong,Shengyu Zhang,Kun Kuang,Yuning Jiang,Bo Zheng*

Main category: cs.IR

TL;DR: 本研究提出了FORGE，一个用于生成检索中语义标识符的综合基准，它包含一个来自淘宝的大规模数据集，并探索了SID构建的优化方法。


<details>
  <summary>Details</summary>
Motivation: 现有语义标识符研究面临大规模数据集缺失、SID生成优化策略研究不足以及在线收敛速度慢等挑战。

Method: FORGE配备了一个包含140亿用户交互数据和2.5亿商品多模态特征的数据集，并探索了多种优化方法来增强SID的构建。

Result: 离线实验验证了FORGE的有效性，在线分析显示交易数量增加了0.35%。提出了两种与推荐性能正相关的SID新指标，无需GR训练即可进行评估。离线预训练模式将在线收敛速度降低了一半。

Conclusion: FORGE通过提供大规模数据集、优化SID构建方法和加速在线收敛，为生成检索中的语义标识符研究做出了重要贡献。

Abstract: Semantic identifiers (SIDs) have gained increasing attention in generative
retrieval (GR) due to their meaningful semantic discriminability. However,
current research on SIDs faces three main challenges: (1) the absence of
large-scale public datasets with multimodal features, (2) limited investigation
into optimization strategies for SID generation, which typically rely on costly
GR training for evaluation, and (3) slow online convergence in industrial
deployment. To address these challenges, we propose FORGE, a comprehensive
benchmark for FOrming semantic identifieR in Generative rEtrieval with
industrial datasets. Specifically, FORGE is equipped with a dataset comprising
14 billion user interactions and multimodal features of 250 million items
sampled from Taobao, one of the biggest e-commerce platforms in China.
Leveraging this dataset, FORGE explores several optimizations to enhance the
SID construction and validates their effectiveness via offline experiments
across different settings and tasks. Further online analysis conducted on our
platform, which serves over 300 million users daily, reveals a 0.35% increase
in transaction count, highlighting the practical impact of our method.
Regarding the expensive SID validation accompanied by the full training of GRs,
we propose two novel metrics of SID that correlate positively with
recommendation performance, enabling convenient evaluations without any GR
training. For real-world applications, FORGE introduces an offline pretraining
schema that reduces online convergence by half. The code and data are available
at https://github.com/selous123/al_sid.

</details>


### [48] [Markup Language Modeling for Web Document Understanding](https://arxiv.org/abs/2509.20940)
*Su Liu,Bin Bi,Jan Bakus,Paritosh Kumar Velalam,Vijay Yella,Vinod Hegde*

Main category: cs.IR

TL;DR: 本文研究了如何从购物评论网站提取详细的产品信息，以构建最新的产品数据库。


<details>
  <summary>Details</summary>
Motivation: 电子商务系统中，网络信息提取在客户分析和产品推荐等任务中扮演着重要角色。本文着眼于通过从购物评论网站提取详细信息来构建最新的产品数据库。

Method: 本文在从不同规模的评论网站收集的产品数据上微调了MarkupLM，并开发了一个名为MarkupLM++的变体，该变体将预测扩展到DOM树的内部节点。

Result: 实验表明，使用更大、更多样化的训练集可以提高整体提取精度。研究还发现，包含内部节点有助于某些产品属性的提取，但会导致整体性能略有下降。最终模型的精确率为0.906，召回率为0.724，F1得分为0.805。

Conclusion: 本文表明，使用更大、更多样化的训练集可以提高整体提取精度，包含内部节点有助于某些产品属性的提取。

Abstract: Web information extraction (WIE) is an important part of many e-commerce
systems, supporting tasks like customer analysis and product recommendation. In
this work, we look at the problem of building up-to-date product databases by
extracting detailed information from shopping review websites. We fine-tuned
MarkupLM on product data gathered from review sites of different sizes and then
developed a variant we call MarkupLM++, which extends predictions to internal
nodes of the DOM tree. Our experiments show that using larger and more diverse
training sets improves extraction accuracy overall. We also find that including
internal nodes helps with some product attributes, although it leads to a
slight drop in overall performance. The final model reached a precision of
0.906, recall of 0.724, and an F1 score of 0.805.

</details>


### [49] [Rejuvenating Cross-Entropy Loss in Knowledge Distillation for Recommender Systems](https://arxiv.org/abs/2509.20989)
*Zhangchi Zhu,Wei Zhang*

Main category: cs.IR

TL;DR: 本文分析了推荐系统中知识蒸馏的交叉熵（CE）损失，并揭示了CE损失与NDCG之间的联系。


<details>
  <summary>Details</summary>
Motivation: 知识蒸馏旨在提炼排序，尤其是在最有可能被偏好的项目之间，并且只能在项目的小子集上计算。作者发现学生模型top项目和老师模型top项目之间存在巨大差距。

Method: 提出了用于知识蒸馏的复兴交叉熵（RCE-KD）。它根据教师给出的top项目是否被学生高度排序，将top项目分成两个子集。对于违反条件的子集，设计了一种抽样策略，以使用师生协作来近似我们的封闭假设。我们还自适应地结合了两个子集上的损失。

Result: 大量实验表明了该方法的有效性。

Conclusion: 本文提出了一种新的知识蒸馏方法，可以有效地提高推荐系统的性能。

Abstract: This paper analyzes Cross-Entropy (CE) loss in knowledge distillation (KD)
for recommender systems. KD for recommender systems targets at distilling
rankings, especially among items most likely to be preferred, and can only be
computed on a small subset of items. Considering these features, we reveal the
connection between CE loss and NDCG in the field of KD. We prove that when
performing KD on an item subset, minimizing CE loss maximizes the lower bound
of NDCG, only if an assumption of closure is satisfied. It requires that the
item subset consists of the student's top items. However, this contradicts our
goal of distilling rankings of the teacher's top items. We empirically
demonstrate the vast gap between these two kinds of top items. To bridge the
gap between our goal and theoretical support, we propose Rejuvenated
Cross-Entropy for Knowledge Distillation (RCE-KD). It splits the top items
given by the teacher into two subsets based on whether they are highly ranked
by the student. For the subset that defies the condition, a sampling strategy
is devised to use teacher-student collaboration to approximate our assumption
of closure. We also combine the losses on the two subsets adaptively. Extensive
experiments demonstrate the effectiveness of our method. Our code is available
at https://anonymous.4open.science/r/RCE-KD.

</details>


### [50] [IntSR: An Integrated Generative Framework for Search and Recommendation](https://arxiv.org/abs/2509.21179)
*Huimin Yan,Longfei Xu,Junjie Sun,Ni Ou,Wei Luo,Xing Tan,Ran Cheng,Kaikui Liu,Xiangxiang Chu*

Main category: cs.IR

TL;DR: 提出了一种用于搜索和推荐的集成生成框架 IntSR，该框架在 Amap 的各种场景中成功部署，并在数字资产的 GMV、POI 推荐的 CTR 和出行方式建议的 ACC 方面取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有系统主要侧重于统一检索和排序，而忽略了搜索和推荐 (S&R) 任务的集成。搜索使用显式用户请求，而推荐依赖于隐式用户兴趣。检索与排序的区别在于查询是否为目标项目本身。

Method: IntSR 使用不同的查询模态集成了这些不同的任务。它还解决了与集成 S&R 行为相关的计算复杂性增加以及动态变化的语料库引入的错误模式学习问题。

Result: IntSR 已在 Amap 的各种场景中成功部署，从而显着提高了数字资产的 GMV (+3.02%)、POI 推荐的 CTR (+2.76%) 和出行方式建议的 ACC (+5.13%)。

Conclusion: IntSR 是一种用于 S&R 的集成生成框架，它使用不同的查询模态集成不同的任务，并解决了与集成 S&R 行为相关的计算复杂性增加以及动态变化的语料库引入的错误模式学习问题。

Abstract: Generative recommendation has emerged as a promising paradigm, demonstrating
remarkable results in both academic benchmarks and industrial applications.
However, existing systems predominantly focus on unifying retrieval and ranking
while neglecting the integration of search and recommendation (S&R) tasks. What
makes search and recommendation different is how queries are formed: search
uses explicit user requests, while recommendation relies on implicit user
interests. As for retrieval versus ranking, the distinction comes down to
whether the queries are the target items themselves. Recognizing the query as
central element, we propose IntSR, an integrated generative framework for S&R.
IntSR integrates these disparate tasks using distinct query modalities. It also
addresses the increased computational complexity associated with integrated S&R
behaviors and the erroneous pattern learning introduced by a dynamically
changing corpus. IntSR has been successfully deployed across various scenarios
in Amap, leading to substantial improvements in digital asset's GMV(+3.02%),
POI recommendation's CTR(+2.76%), and travel mode suggestion's ACC(+5.13%).

</details>


### [51] [Interactive Recommendation Agent with Active User Commands](https://arxiv.org/abs/2509.21317)
*Jiakai Tang,Yujie Luo,Xunke Xi,Fei Sun,Xueyang Feng,Sunhao Dai,Chao Yi,Dian Chen,Zhujin Gao,Yang Li,Xu Chen,Wen Chen,Jian Wu,Yuning Jiang,Bo Zheng*

Main category: cs.IR

TL;DR: 传统的推荐系统依赖于被动的反馈机制，无法捕捉用户细致的行为动机和意图。为了解决这些局限性，我们引入了交互式推荐Feed（IRF），它支持在主流推荐Feed中使用自然语言命令。我们开发了一个双代理架构RecBot，Parser Agent将语言表达转换为结构化偏好，Planner Agent动态地协调自适应工具链以进行即时策略调整。通过广泛的离线和长期在线实验，RecBot在用户满意度和业务成果方面均显示出显着改善。


<details>
  <summary>Details</summary>
Motivation: 传统的推荐系统依赖于被动的反馈机制，无法捕捉用户细致的行为动机和意图，导致用户意图和系统解释之间存在差距，最终损害用户满意度和系统有效性。

Method: 我们引入了交互式推荐Feed（IRF），它支持在主流推荐Feed中使用自然语言命令。我们开发了一个双代理架构RecBot，Parser Agent将语言表达转换为结构化偏好，Planner Agent动态地协调自适应工具链以进行即时策略调整。为了实现实际部署，我们采用模拟增强知识蒸馏，以实现高效的性能，同时保持强大的推理能力。

Result: 通过广泛的离线和长期在线实验，RecBot在用户满意度和业务成果方面均显示出显着改善。

Conclusion: 交互式推荐Feed（IRF）是一种有前景的范例，可以有效提高用户满意度和业务成果。

Abstract: Traditional recommender systems rely on passive feedback mechanisms that
limit users to simple choices such as like and dislike. However, these
coarse-grained signals fail to capture users' nuanced behavior motivations and
intentions. In turn, current systems cannot also distinguish which specific
item attributes drive user satisfaction or dissatisfaction, resulting in
inaccurate preference modeling. These fundamental limitations create a
persistent gap between user intentions and system interpretations, ultimately
undermining user satisfaction and harming system effectiveness.
  To address these limitations, we introduce the Interactive Recommendation
Feed (IRF), a pioneering paradigm that enables natural language commands within
mainstream recommendation feeds. Unlike traditional systems that confine users
to passive implicit behavioral influence, IRF empowers active explicit control
over recommendation policies through real-time linguistic commands. To support
this paradigm, we develop RecBot, a dual-agent architecture where a Parser
Agent transforms linguistic expressions into structured preferences and a
Planner Agent dynamically orchestrates adaptive tool chains for on-the-fly
policy adjustment. To enable practical deployment, we employ
simulation-augmented knowledge distillation to achieve efficient performance
while maintaining strong reasoning capabilities. Through extensive offline and
long-term online experiments, RecBot shows significant improvements in both
user satisfaction and business outcomes.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [52] [Sig2Model: A Boosting-Driven Model for Updatable Learned Indexes](https://arxiv.org/abs/2509.20781)
*Alireza Heidari,Amirhossein Ahmad,Wei Zhang,Ying Xiong*

Main category: cs.LG

TL;DR: Sig2Model: An efficient and adaptive learned index that minimizes retraining cost through sigmoid boosting, proactive update training, and neural joint optimization.


<details>
  <summary>Details</summary>
Motivation: Learned Indexes (LIs) achieve remarkable efficiency for static datasets, their performance degrades under dynamic updates: maintaining the CDF invariant requires global model retraining, which blocks queries and limits the queries-per-second (QPS) metric. Current approaches fail to address these retraining costs effectively, rendering them unsuitable for real-world workloads with frequent updates.

Method: a sigmoid boosting approximation technique that dynamically adjusts the index model by approximating update-induced shifts in data distribution with localized sigmoid functions while preserving bounded error guarantees and deferring full retraining; proactive update training via Gaussian mixture models (GMMs) that identifies high-update-probability regions for strategic placeholder allocation to speed up updates; and a neural joint optimization framework that continuously refines both the sigmoid ensemble and GMM parameters via gradient-based learning.

Result: Sig2Model reduces retraining cost by up to 20x, achieves up to 3x higher QPS, and uses up to 1000x less memory.

Conclusion: Sig2Model is an efficient and adaptive learned index that minimizes retraining cost and improves performance on dynamic datasets.

Abstract: Learned Indexes (LIs) represent a paradigm shift from traditional index
structures by employing machine learning models to approximate the cumulative
distribution function (CDF) of sorted data. While LIs achieve remarkable
efficiency for static datasets, their performance degrades under dynamic
updates: maintaining the CDF invariant (sum of F(k) equals 1) requires global
model retraining, which blocks queries and limits the queries-per-second (QPS)
metric. Current approaches fail to address these retraining costs effectively,
rendering them unsuitable for real-world workloads with frequent updates. In
this paper, we present Sig2Model, an efficient and adaptive learned index that
minimizes retraining cost through three key techniques: (1) a sigmoid boosting
approximation technique that dynamically adjusts the index model by
approximating update-induced shifts in data distribution with localized sigmoid
functions while preserving bounded error guarantees and deferring full
retraining; (2) proactive update training via Gaussian mixture models (GMMs)
that identifies high-update-probability regions for strategic placeholder
allocation to speed up updates; and (3) a neural joint optimization framework
that continuously refines both the sigmoid ensemble and GMM parameters via
gradient-based learning. We evaluate Sig2Model against state-of-the-art
updatable learned indexes on real-world and synthetic workloads, and show that
Sig2Model reduces retraining cost by up to 20x, achieves up to 3x higher QPS,
and uses up to 1000x less memory.

</details>


### [53] [A Theory of Multi-Agent Generative Flow Networks](https://arxiv.org/abs/2509.20408)
*Leo Maxime Brunswic,Haozhi Wang,Shuang Luo,Jianye Hao,Amir Rasouli,Yinchuan Li*

Main category: cs.LG

TL;DR: 本文提出了多智能体生成流网络（MA-GFlowNets）的理论框架，用于多个智能体协作生成对象。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对多智能体生成流网络的理论框架研究。

Method: 提出了四种算法：集中式流网络、独立流网络、联合流网络及其条件版本。联合流训练基于局部-全局原则。

Result: 实验结果表明，所提出的框架优于强化学习和基于MCMC的方法。

Conclusion: 本文为多智能体生成对象提供了一种新的理论框架和算法。

Abstract: Generative flow networks utilize a flow-matching loss to learn a stochastic
policy for generating objects from a sequence of actions, such that the
probability of generating a pattern can be proportional to the corresponding
given reward. However, a theoretical framework for multi-agent generative flow
networks (MA-GFlowNets) has not yet been proposed. In this paper, we propose
the theory framework of MA-GFlowNets, which can be applied to multiple agents
to generate objects collaboratively through a series of joint actions. We
further propose four algorithms: a centralized flow network for centralized
training of MA-GFlowNets, an independent flow network for decentralized
execution, a joint flow network for achieving centralized training with
decentralized execution, and its updated conditional version. Joint Flow
training is based on a local-global principle allowing to train a collection of
(local) GFN as a unique (global) GFN. This principle provides a loss of
reasonable complexity and allows to leverage usual results on GFN to provide
theoretical guarantees that the independent policies generate samples with
probability proportional to the reward function. Experimental results
demonstrate the superiority of the proposed framework compared to reinforcement
learning and MCMC-based methods.

</details>


### [54] [FastEagle: Cascaded Drafting for Accelerating Speculative Decoding](https://arxiv.org/abs/2509.20416)
*Haiduo Huang,Jiangcheng Song,Wenzhe Zhao,Pengju Ren*

Main category: cs.LG

TL;DR: FastEagle是一个非自回归的草稿器，它可以在一个前向过程中生成整个草稿，从而加速生成过程。


<details>
  <summary>Details</summary>
Motivation: 现有的草稿器需要N个连续的步骤来生成N个token，效率较低。

Method: FastEagle使用轻量级的层叠结构代替时间步，并通过层级的监督训练来减少误差累积。

Result: FastEagle在多个LLM和任务中，始终优于EAGLE-3，并且具有相当的平均接受长度。

Conclusion: 在草稿中移除顺序依赖性是实现无损LLM推理加速的有效途径。

Abstract: Speculative decoding accelerates generation by drafting candidates and
verifying them in parallel, yet state-of-the-art drafters (e.g., EAGLE) still
require N sequential passes to propose N tokens. We present FastEagle, a
non-autoregressive cascaded drafter that emits an entire draft in a single
forward pass. FastEagle replaces temporal steps with a lightweight layer
cascade and trains with layer-wise supervision to mitigate error accumulation.
Coupled with a constrained draft tree that preserves lossless verification
cost, FastEagle delivers substantial wall-clock speedups over strong
autoregressive drafters while maintaining competitive acceptance behavior.
Across multiple LLMs (Vicuna-13B, LLaMA-Instruct 3.x, and
DeepSeek-R1-Distill-LLaMA) and tasks (MT-Bench, HumanEval, GSM8K, CNN/DM,
Alpaca), FastEagle consistently outperforms EAGLE-3 in speedup under both
greedy and stochastic decoding, with comparable average acceptance lengths.
These results indicate that removing sequential dependencies in drafting is a
practical path toward lossless LLM inference acceleration.

</details>


### [55] [mloz: A Highly Efficient Machine Learning-Based Ozone Parameterization for Climate Sensitivity Simulations](https://arxiv.org/abs/2509.20422)
*Yiling Ma,Nathan Luke Abraham,Stefan Versick,Roland Ruhnke,Andrea Schneidereit,Ulrike Niemeier,Felix Back,Peter Braesicke,Peer Nowack*

Main category: cs.LG

TL;DR: 提出了一种机器学习参数化方法（mloz），用于在气候模型中模拟臭氧变化和趋势。


<details>
  <summary>Details</summary>
Motivation: 目前气候模型由于大气化学方案的高计算成本，通常缺乏对臭氧的交互表示。

Method: 使用机器学习参数化方法（mloz），以大气温度剖面信息作为唯一输入，模拟对流层和平流层的臭氧变化和趋势。

Result: mloz的预测速度比UKESM中的化学方案快约31倍，且在气候模型总运行时间中贡献小于4%。

Conclusion: mloz具有高保真度、灵活性和可移植性，有潜力在缺乏交互化学的CMIP级别气候模型中广泛应用。

Abstract: Atmospheric ozone is a crucial absorber of solar radiation and an important
greenhouse gas. However, most climate models participating in the Coupled Model
Intercomparison Project (CMIP) still lack an interactive representation of
ozone due to the high computational costs of atmospheric chemistry schemes.
Here, we introduce a machine learning parameterization (mloz) to interactively
model daily ozone variability and trends across the troposphere and
stratosphere in standard climate sensitivity simulations, including two-way
interactions of ozone with the Quasi-Biennial Oscillation. We demonstrate its
high fidelity on decadal timescales and its flexible use online across two
different climate models -- the UK Earth System Model (UKESM) and the German
ICOsahedral Nonhydrostatic (ICON) model. With atmospheric temperature profile
information as the only input, mloz produces stable ozone predictions around 31
times faster than the chemistry scheme in UKESM, contributing less than 4
percent of the respective total climate model runtimes. In particular, we also
demonstrate its transferability to different climate models without chemistry
schemes by transferring the parameterization from UKESM to ICON. This
highlights the potential for widespread adoption in CMIP-level climate models
that lack interactive chemistry for future climate change assessments,
particularly when focusing on climate sensitivity simulations, where ozone
trends and variability are known to significantly modulate atmospheric feedback
processes.

</details>


### [56] [Bridging Privacy and Utility: Synthesizing anonymized EEG with constraining utility functions](https://arxiv.org/abs/2509.20454)
*Kay Fuhrmeister,Arne Pelzer,Fabian Radke,Julia Lechinger,Mahzad Gharleghi,Thomas Köllmer,Insa Wolf*

Main category: cs.LG

TL;DR: 提出了一个基于transformer的自编码器，用于创建脑电数据，该数据不允许主体重新识别，同时仍保持其用于特定机器学习任务的效用。


<details>
  <summary>Details</summary>
Motivation: 脑电图(EEG)被广泛用于记录大脑活动，并在机器学习中有许多应用，例如检测睡眠阶段和神经系统疾病。一些研究成功地表明了脑电数据在重新识别和泄露其他个人信息方面的潜力。因此，脑电消费设备的日益普及引起了人们对用户隐私的担忧，促使我们研究如何在保护这种敏感数据，同时保持其在脑电应用中的效用。

Method: 我们提出了一个基于transformer的自编码器来创建脑电数据，该数据不允许主体重新识别，同时仍保持其用于特定机器学习任务的效用。我们将我们的方法应用于自动睡眠分期，通过评估匿名化前后脑电数据的重新识别和效用潜力。

Result: 结果表明，脑电信号的可重新识别性可以大大降低，同时保持其在机器学习中的效用。

Conclusion: 脑电信号的可重新识别性可以大大降低，同时保持其在机器学习中的效用。

Abstract: Electroencephalography (EEG) is widely used for recording brain activity and
has seen numerous applications in machine learning, such as detecting sleep
stages and neurological disorders. Several studies have successfully shown the
potential of EEG data for re-identification and leakage of other personal
information. Therefore, the increasing availability of EEG consumer devices
raises concerns about user privacy, motivating us to investigate how to
safeguard this sensitive data while retaining its utility for EEG applications.
To address this challenge, we propose a transformer-based autoencoder to create
EEG data that does not allow for subject re-identification while still
retaining its utility for specific machine learning tasks. We apply our
approach to automatic sleep staging by evaluating the re-identification and
utility potential of EEG data before and after anonymization. The results show
that the re-identifiability of the EEG signal can be substantially reduced
while preserving its utility for machine learning.

</details>


### [57] [Efficiently Attacking Memorization Scores](https://arxiv.org/abs/2509.20463)
*Tue Do,Varun Chandrasekaran,Daniel Alabi*

Main category: cs.LG

TL;DR: 本文研究了基于记忆的Influence Estimation方法是否可以被对抗攻击，发现即使是最先进的代理也容易受到目标分数操纵。


<details>
  <summary>Details</summary>
Motivation: 在数据估值和负责任的机器学习中的应用提出了一个问题：这些分数本身是否可以被对抗性操纵？

Method: 我们提出了一种系统的研究，研究攻击基于记忆的Influence Estimator的可行性。我们的攻击只需要对模型输出进行黑盒访问，并且计算开销适中(计算输入的伪逆)。

Result: 我们通过大量的图像分类任务验证了我们的攻击，表明即使是最先进的代理也容易受到目标分数操纵。此外，我们还对对抗扰动下记忆分数的稳定性进行了理论分析，揭示了影响估计本质上是脆弱的条件。

Conclusion: 我们的研究结果突出了基于影响的归因中的关键漏洞，并建议需要强大的防御。

Abstract: Influence estimation tools -- such as memorization scores -- are widely used
to understand model behavior, attribute training data, and inform dataset
curation. However, recent applications in data valuation and responsible
machine learning raise the question: can these scores themselves be
adversarially manipulated? In this work, we present a systematic study of the
feasibility of attacking memorization-based influence estimators. We
characterize attacks for producing highly memorized samples as highly sensitive
queries in the regime where a trained algorithm is accurate. Our attack
(calculating the pseudoinverse of the input) is practical, requiring only
black-box access to model outputs and incur modest computational overhead. We
empirically validate our attack across a wide suite of image classification
tasks, showing that even state-of-the-art proxies are vulnerable to targeted
score manipulations. In addition, we provide a theoretical analysis of the
stability of memorization scores under adversarial perturbations, revealing
conditions under which influence estimates are inherently fragile. Our findings
highlight critical vulnerabilities in influence-based attribution and suggest
the need for robust defenses. All code can be found at
https://anonymous.4open.science/r/MemAttack-5413/

</details>


### [58] [Offline Goal-conditioned Reinforcement Learning with Quasimetric Representations](https://arxiv.org/abs/2509.20478)
*Vivek Myers,Bill Chunyuan Zheng,Benjamin Eysenbach,Sergey Levine*

Main category: cs.LG

TL;DR: 本研究提出了一种新的目标条件强化学习（GCRL）方法，该方法统一了对比表示和时间距离两种框架，利用拟度量表示空间的结构来学习能够实现最佳目标到达的后继表示。


<details>
  <summary>Details</summary>
Motivation: 现有的GCRL方法通常使用学习到的状态表示来提取目标到达策略。对比表示和时间距离是两种有效的表示结构框架，但前者在拼接任务中表现不佳，后者在高维噪声环境中表现不佳。

Method: 该方法利用拟度量表示空间的结构（三角不等式），并施加额外的约束，以学习后继表示，从而实现最佳目标到达。

Result: 该方法能够在次优数据和随机环境中学习最佳目标到达距离，并且在对比学习难以处理的拼接任务和拟度量网络难以处理的高维噪声环境中，性能均有所提高。

Conclusion: 该方法结合了蒙特卡洛对比RL方法的稳定性和长时程能力，以及拟度量网络参数化的自由拼接能力，从而实现了更好的GCRL性能。

Abstract: Approaches for goal-conditioned reinforcement learning (GCRL) often use
learned state representations to extract goal-reaching policies. Two frameworks
for representation structure have yielded particularly effective GCRL
algorithms: (1) *contrastive representations*, in which methods learn
"successor features" with a contrastive objective that performs inference over
future outcomes, and (2) *temporal distances*, which link the (quasimetric)
distance in representation space to the transit time from states to goals. We
propose an approach that unifies these two frameworks, using the structure of a
quasimetric representation space (triangle inequality) with the right
additional constraints to learn successor representations that enable optimal
goal-reaching. Unlike past work, our approach is able to exploit a
**quasimetric** distance parameterization to learn **optimal** goal-reaching
distances, even with **suboptimal** data and in **stochastic** environments.
This gives us the best of both worlds: we retain the stability and long-horizon
capabilities of Monte Carlo contrastive RL methods, while getting the free
stitching capabilities of quasimetric network parameterizations. On existing
offline GCRL benchmarks, our representation learning objective improves
performance on stitching tasks where methods based on contrastive learning
struggle, and on noisy, high-dimensional environments where methods based on
quasimetric networks struggle.

</details>


### [59] [CoSupFormer : A Contrastive Supervised learning approach for EEG signal Classification](https://arxiv.org/abs/2509.20489)
*D. Darankoum,C. Habermacher,J. Volle,S. Grudinin*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的端到端深度学习框架，用于处理脑电信号中的噪声和通道变异性，并提取有意义的特征。


<details>
  <summary>Details</summary>
Motivation: 脑电信号包含丰富的多尺度信息，对于理解大脑状态至关重要，并在诊断和推进药物开发领域具有潜在应用。然而，从原始脑电信号中提取有意义的特征，同时处理噪声和通道变异性仍然是一个主要的挑战。

Method: 该框架包含一个能够显式捕获多尺度频率振荡的编码器；一个基于注意力的编码器，可以同时学习脑电通道之间和局部通道内的相互作用；一个门控网络，用于动态过滤掉噪声和非信息通道；以及一种利用监督和对比学习的新型损失函数。

Result: 该方法在多种应用中得到了验证，包括多种中枢神经系统（CNS）疾病治疗效果的分类以及帕金森病和阿尔茨海默病的诊断。

Conclusion: 结果表明，所提出的学习范式可以从不同物种的原始脑电信号中提取具有生物学意义的模式，自主选择高质量通道，并通过创新的架构和损失设计实现强大的泛化能力。

Abstract: Electroencephalography signals (EEGs) contain rich multi-scale information
crucial for understanding brain states, with potential applications in
diagnosing and advancing the drug development landscape. However, extracting
meaningful features from raw EEG signals while handling noise and channel
variability remains a major challenge. This work proposes a novel end-to-end
deep-learning framework that addresses these issues through several key
innovations. First, we designed an encoder capable of explicitly capturing
multi-scale frequency oscillations covering a wide range of features for
different EEG-related tasks. Secondly, to model complex dependencies and handle
the high temporal resolution of EEGs, we introduced an attention-based encoder
that simultaneously learns interactions across EEG channels and within
localized {\em patches} of individual channels. We integrated a dedicated
gating network on top of the attention encoder to dynamically filter out noisy
and non-informative channels, enhancing the reliability of EEG data. The entire
encoding process is guided by a novel loss function, which leverages supervised
and contrastive learning, significantly improving model generalization. We
validated our approach in multiple applications, ranging from the
classification of effects across multiple Central Nervous System (CNS)
disorders treatments to the diagnosis of Parkinson's and Alzheimer's disease.
Our results demonstrate that the proposed learning paradigm can extract
biologically meaningful patterns from raw EEG signals across different species,
autonomously select high-quality channels, and achieve robust generalization
through innovative architectural and loss design.

</details>


### [60] [Beyond Visual Similarity: Rule-Guided Multimodal Clustering with explicit domain rules](https://arxiv.org/abs/2509.20501)
*Kishor Datta Gupta,Mohd Ariful Haque,Marufa Kamal,Ahmed Rafi Hasan,Md. Mahfuzur Rahman,Roy George*

Main category: cs.LG

TL;DR: DARTVAE: A rule-guided multimodal clustering framework incorporating domain-specific constraints into representation learning.


<details>
  <summary>Details</summary>
Motivation: Traditional clustering techniques lack the ability to capture structural or semantic constraints.

Method: Extends VAE architecture by embedding explicit rules, semantic representations, and data-driven features into a unified latent space, enforcing constraint compliance through rule consistency and violation penalties in the loss function. Rules are generated by LLMs and structured into knowledge graphs.

Result: Rule-guided clustering produces more operationally meaningful and interpretable clusters, improving traditional clustering metrics on aircraft and automotive datasets.

Conclusion: DARTVAE achieves more meaningful and consistent clustering outcomes than purely data-driven models, highlighting the utility of constraint-guided multimodal clustering for complex, knowledge-intensive settings. Challenges include LLM hallucination, rule conflicts, overfitting, and scaling difficulties.

Abstract: Traditional clustering techniques often rely solely on similarity in the
input data, limiting their ability to capture structural or semantic
constraints that are critical in many domains. We introduce the Domain Aware
Rule Triggered Variational Autoencoder (DARTVAE), a rule guided multimodal
clustering framework that incorporates domain specific constraints directly
into the representation learning process. DARTVAE extends the VAE architecture
by embedding explicit rules, semantic representations, and data driven features
into a unified latent space, while enforcing constraint compliance through rule
consistency and violation penalties in the loss function. Unlike conventional
clustering methods that rely only on visual similarity or apply rules as post
hoc filters, DARTVAE treats rules as first class learning signals. The rules
are generated by LLMs, structured into knowledge graphs, and enforced through a
loss function combining reconstruction, KL divergence, consistency, and
violation penalties. Experiments on aircraft and automotive datasets
demonstrate that rule guided clustering produces more operationally meaningful
and interpretable clusters for example, isolating UAVs, unifying stealth
aircraft, or separating SUVs from sedans while improving traditional clustering
metrics. However, the framework faces challenges: LLM generated rules may
hallucinate or conflict, excessive rules risk overfitting, and scaling to
complex domains increases computational and consistency difficulties. By
combining rule encodings with learned representations, DARTVAE achieves more
meaningful and consistent clustering outcomes than purely data driven models,
highlighting the utility of constraint guided multimodal clustering for
complex, knowledge intensive settings.

</details>


### [61] [Myosotis: structured computation for attention like layer](https://arxiv.org/abs/2509.20503)
*Evgenii Egorov,Hanno Ackermann,Markus Nagel,Hong Cai*

Main category: cs.LG

TL;DR: 提出了一种新的算法，该算法结合了稀疏性和循环依赖性的优点，以解决注意力层中内存和计算随序列长度二次增长的问题。


<details>
  <summary>Details</summary>
Motivation: 注意力层的参数取决于输入元素的成对交互，但如果没有结构假设，内存和计算会随序列长度二次增长。为了缓解这个问题，要么引入稀疏性，要么引入循环依赖性，但这两种方法都有缺点。

Method: 该算法基于树结构矩阵的有效反演。

Result: 论文提出了一种新的算法

Conclusion: 该算法结合了稀疏性和循环依赖性的优点。

Abstract: Attention layers apply a sequence-to-sequence mapping whose parameters depend
on the pairwise interactions of the input elements. However, without any
structural assumptions, memory and compute scale quadratically with the
sequence length. The two main ways to mitigate this are to introduce sparsity
by ignoring a sufficient amount of pairwise interactions or to introduce
recurrent dependence along them, as SSM does. Although both approaches are
reasonable, they both have disadvantages. We propose a novel algorithm that
combines the advantages of both concepts. Our idea is based on the efficient
inversion of tree-structured matrices.

</details>


### [62] [Auto-Regressive U-Net for Full-Field Prediction of Shrinkage-Induced Damage in Concrete](https://arxiv.org/abs/2509.20507)
*Liya Gaynutdinova,Petr Havlásek,Ondřej Rokoš,Fleur Hendriks,Martin Doškář*

Main category: cs.LG

TL;DR: 提出了一种基于深度学习的方法来预测混凝土中随时间变化的完整损伤场。


<details>
  <summary>Details</summary>
Motivation: 研究目的是预测混凝土中随时间变化的完整损伤场，并优化混凝土的混合物设计，从而提高耐久性并减少内部损伤。

Method: 使用自回归 U-Net 模型预测单元格中标量损伤场的演变，并使用卷积神经网络 (CNN) 利用损伤估计来预测关键的机械性能，包括观测到的收缩率和残余刚度。

Result: 所提出的双网络架构在合成数据集上表现出很高的计算效率和强大的预测性能。该方法减少了与全场损伤评估相关的计算负荷。

Conclusion: 该方法可以帮助优化混凝土混合物设计，从而提高耐久性并减少内部损伤。

Abstract: This paper introduces a deep learning approach for predicting time-dependent
full-field damage in concrete. The study uses an auto-regressive U-Net model to
predict the evolution of the scalar damage field in a unit cell given
microstructural geometry and evolution of an imposed shrinkage profile. By
sequentially using the predicted damage output as input for subsequent
predictions, the model facilitates the continuous assessment of damage
progression. Complementarily, a convolutional neural network (CNN) utilises the
damage estimations to forecast key mechanical properties, including observed
shrinkage and residual stiffness. The proposed dual-network architecture
demonstrates high computational efficiency and robust predictive performance on
the synthesised datasets. The approach reduces the computational load
traditionally associated with full-field damage evaluations and is used to gain
insights into the relationship between aggregate properties, such as shape,
size, and distribution, and the effective shrinkage and reduction in stiffness.
Ultimately, this can help to optimize concrete mix designs, leading to improved
durability and reduced internal damage.

</details>


### [63] [Complexity-Driven Policy Optimization](https://arxiv.org/abs/2509.20509)
*Luca Serfilippi,Giorgio Franceschelli,Antonio Corradi,Mirco Musolesi*

Main category: cs.LG

TL;DR: 本文提出了一种新的强化学习算法，CDPO，它使用复杂性奖励代替熵奖励，以平衡探索和利用，鼓励智能体发现结构化且适应性强的策略。


<details>
  <summary>Details</summary>
Motivation: 最大化熵会使策略趋向于均匀随机分布，这代表着一种非结构化且有时效率低下的探索策略。为了解决这个问题。

Method: 用香农熵和不平衡的乘积来衡量复杂性，不平衡量化了与均匀分布的距离。用复杂性代替熵，提出CDPO算法。

Result: 在离散动作空间任务中，CDPO 比 PPO 对复杂性系数的选择更稳健，尤其是在需要更大探索的环境中。

Conclusion: CDPO 是一种更稳健的强化学习算法，它可以通过平衡随机性和结构来鼓励智能体发现结构化且适应性强的策略。

Abstract: Policy gradient methods often balance exploitation and exploration via
entropy maximization. However, maximizing entropy pushes the policy towards a
uniform random distribution, which represents an unstructured and sometimes
inefficient exploration strategy. In this work, we propose replacing the
entropy bonus with a more robust complexity bonus. In particular, we adopt a
measure of complexity, defined as the product of Shannon entropy and
disequilibrium, where the latter quantifies the distance from the uniform
distribution. This regularizer encourages policies that balance stochasticity
(high entropy) with structure (high disequilibrium), guiding agents toward
regimes where useful, non-trivial behaviors can emerge. Such behaviors arise
because the regularizer suppresses both extremes, e.g., maximal disorder and
complete order, creating pressure for agents to discover structured yet
adaptable strategies. Starting from Proximal Policy Optimization (PPO), we
introduce Complexity-Driven Policy Optimization (CDPO), a new learning
algorithm that replaces entropy with complexity. We show empirically across a
range of discrete action space tasks that CDPO is more robust to the choice of
the complexity coefficient than PPO is with the entropy coefficient, especially
in environments requiring greater exploration.

</details>
